XXX -X-XXXX -XXXX -X/XX/$XX.00 ©20XX IEEE  F4D: Factorized 4D Convolutional Neural Network 
for Efficient Video -level Representation Learning  
 
Mohammad Al -Saad, Lakshmish  Ramaswamy and Suchendra Bhandarkar  
School of Computing  
The University of Georgia  
Athens, GA  
{mohammad.alsaad, laksmr, suchi}@uga.edu
  
 
  
 
Abstract— Recent studies have shown that video -level 
representation learning is crucial to the capture and 
understanding of the long -range temporal structure for video 
action recognition . Most existing 3D convolutional neural network 
(CNN) -based methods for video -level representation learning are 
clip-based and focus only on short -term motion and appearances. 
These CNN -based methods lack the capacity to incorporate and 
model the long -rang e spatiotemporal representation of the 
underlying video and ignore the long -rang e video -level context 
during training. In this study, we propose a factorized 4D CNN 
architecture with attention (F4D) that is capable of learning more 
effective, finer -grained, long -term spatiotemporal video 
representations. We demonstrate that the propos ed F4D 
architecture yields significant performance improvements over 
the conventional 2D, and 3D CNN architectures proposed in the 
literature. Experiment evaluation on five action recognition 
benchmark datasets, i.e., Something -Something -v1, Something -
Some thing -v2, Kinetics -400, UCF101, and HMDB51 demonstrate 
the effectiveness of the proposed F4D network  architecture for 
video -level action recognition.  
Keywords —video -level action recognition, factorized 
convolutional neural network, temporal attention, spatio -temporal 
attention, channel attention, 3D CNN, 4D CNN.  
I. INTRODUCTION  
In an era dominated by digital mediums, the increasing 
number  of large -scale videos has transformed the way 
information is conveyed and consumed. From  autonomous 
vehicles and intelligent surveillance systems  to online 
streaming services  and social media  platforms , videos have 
emerged  as a pervasive and rich  source of data that captures the 
essence of human experiences and surrounding  environment. 
Nevertheless, the complexity and sheer volume of these huge 
videos present  the demand  for effective video understanding.  
The initial step of the video understanding is actio n recognition 
which aims to interpret and understand human actions, gestures, 
and movements .  
Many  2D and 3D Convolutional Neural Network (CNN) 
architectures have been proposed for the problem of video -based 
human action recognition. A straightforward CNN -based 
approach to this problem uses the entire video as an input to the 
CNN followed by a fully con volutional inference [1]. However, the data volume in videos are huge which could result in a very 
high memory footprint  and processing power  as trying to run a 
fully convolutional inference is well above the capabilities of 
modern GPUs [2].  
To substantially reduce the memory footprint and 
computational cost, most existing deep learning (DL) models for 
video representation learning incorporate clip -level feature 
learning This allows these DL models to apply deep networks 
over video clips of fi xed temporal length focusing on short -term 
object appearances and motion, thus, learning from video clips 
instead of the entire video. The clip -based learning methods 
sample short video clips comprising of 10 -32 frames per clip, 
and compute the prediction scores for each clip independently 
[3]. Finally, the individual results from all the clips are pulled 
together to generate a final video -level prediction.  
In general, clip -based models often ignore long -range 
spatiotemporal dependencies and the global video -level 
structure during training.  The temporal dependency problem in 
vision -based human action recognition refers to the challenge of 
correctly capturing and modeling the dynamic and sequential 
nature of human actions over time. It identifies that actions are 
not separated events but unfold as a sequence of distinctive 
motion patterns, each pattern contribut es to the overall 
understanding of the action bein g performed. Temporal 
dependency holds the notion that the duration , timing, and order  
of these motion patterns are critical for interpreting and 
recognizing actions correctly.  Capturing the temporal aspect is 
essential for distinguishing between actions that may share 
similar visual appearance but vary in their execution timing  or 
sequence . In many cases, partial observation of the underlying 
video makes it very difficult to recognize an action correctly.  
Additionally, relying on the average of the predict ion scores 
from individual clips is considered to result in a sub -optimal 
inference.  
 To learn from an entire video efficiently, the Temporal 
Segment Network (TSN) architecture has been proposed [4]. 
The TSN represents the contents of the entire video by operating 
on a sequence of multiple short clips (snippets) sampled from 
the entire vid eo. In the final TSN stage, a segmental consensus 
function is used to aggregate the predictions from the sampled 
snippets, thereby enabling the TSN to model long -range temporal structures. However, the fact that inter -clip interactions 
and video -level fusion are performed in the final TSN stage 
limits the ability of the TSN to capture fine temporal structures. 
To overcome this limitation, the V4D CNN model [5] 
incorporat ed the 4D CNN architecture. The 4D convolution 
operation has the capacity to model long -range dependencies 
and capture inter -clip interactions for efficient video -level 
representation learning. To capture finer temporal structures, the 
V4D CNN residual blo cks are placed at earlier stages in the 
network. Nevertheless, the 4D convolution operation in the V4D 
CNN model is complex and introduces many more parameters 
thereby making the model vulnerable to overfitting. 
Furthermore, the V4D CNN architecture does n ot incorporate an 
attention mechanism to focus on the regions of interest (ROIs) 
that evolve over time.   
Inspired by the above observations of the state of the art in 
video -level representation learning, we propose an effective yet 
simple framework for video level representation learning termed 
as the Factorized 4D (F4D) architecture , to model both short -
range motion and long -range temporal dependency within a 
large -scale video sequence. This paper has two main objectives ; 
the first objective  is to enhanc e accuracy  and to decreas e the 
complexity of the 4D convolution operation introduced in the 
V4D CNN framework. We start by factorization of the 4D 
convolution operation which renders the proposed F4D CNN 
model capable of representing more complex functions by 
capturing more complex inter-clip interactions and finer 
temporal structures. Furthermore, the proposed factorization 
improves the optimization procedure during both training and 
testing, yielding lower training and testing errors. The second 
objective  is to implemen t an attention mechanism that focuses 
on an ROI within the video and enhances the power of the 
resulting representation. We design two attention mechanisms, 
namely the temporal attention (TA) module and spatio -temporal 
attention (STA) module. These modules  will focus on the 
different inter -clip motion patterns that evolve over time and on 
the spatio -temporal discriminative features by focusing on the 
ROIs that evolve over time . We insert the proposed factorized 
4D CNN followed by the attention modules to fo rm a block 
named F4D residual block . The F4D residual  blocks can be 
easily inserted into standard  ResNet [ 6] architecture  to form the 
F4D architecture. The main contributions of our work can be 
summarized as follows:  
• We propose a Factorized 4D CNN that can capture 
more complex long-range temporal dependency  
and inter -clip interactions with lowered training 
and testing errors compared to the 4D CNN.  
• We propose a temporal attention module (TA) and 
a spatio -temporal attention module (STA) that 
guide the network to focus  on ROI s within the 
video and improves the resulting representation  
with negligible computation cost.  
• An effective  yet simple  network referred as F4D 
architecture is proposed with our F4D residual 
blocks  that consist of the proposed F4D CNN 
followed by the proposed attention modules , 
which can be easily integrated  into standard 
ResNet architecture.  •  Extensive experiments demonstrate  the 
effectiveness of the proposed F4D  architecture on 
five action recognition benchmark datasets 
including Something -Something -v1 and v2 [ 7], 
Kinetics -400 [ 8], UCF101 [ 9] and HMDB51 [ 10].  
II. RELATED WORKS  
 Two -Stream 2D CNN.  The two -stream CNN architecture 
represents a very practical approach to video -level 
representation learning. The earliest two -stream CNN 
architecture was introduced in [ 11] where one CNN learns from 
a stream of RGB frames and the other CNN from a stream 
comprising of stacks of 10 computed optical flow frames. In the 
later stages, the results of both streams are averaged to yield the 
final prediction.  
 Although the two -stream CNN architecture has been shown 
to yield impressive results, the extraction of spatial and temporal 
features is performed independently, and it is easy to ignore their 
intrinsic connection, which can influence the final prediction. 
Another limitation of two -stream networks is the excessive 
demands of optical flow comput ation where parallel 
optimization is difficult to implement. Some related works have 
explored the idea of enhancing the optical flow computations 
[12,13,14,15] in this regard.  
3D CNN.  Since 3D CNN s incorporate spatio -temporal 
filters, they represent a natural approach to video modelling. 
The biggest advantage of 3D CNNs is their ability to create 
hierarchical representations of spatio -temporal data. 3D CNN s 
have been explored in several works cited  in the literature. Ji et 
al. [16]  pioneered  the use of the 3D CNN for human action 
recognition by applying 3D convolution operation  in both the 
spatial and temporal domain s.  
Tran et al . [17] propose the C3D model and show its 
effectiveness when trained on large -scale video datasets. They  
conducted a systematic study to show that 3D CNN is better 
than 2D CNN in learn ing appearance and motion information. 
Moreover, they show  that using 3 ×3×3 convolution kernels for 
all layers work s best amongst the explored architectures. The 
work in [18] improves upon the C3D model by employing  
neural architecture search across multiple dimensions and  3D 
residual networks  that allow for use of  deeper networks that can 
be trained on large -scale video datasets.  
The two-stream 3D CNN  architecture  has been explored by  
Carreira et al . [19] with the goal of successful ly incorporating  
2D image classification models into a 3D CNN by inflating all 
the filters and pooling kernels by adding an extra temporal 
dimension. The authors use a pre -trained Inception framework 
as the architectural backbone with one stream trained on RGB 
inputs and another stream trained on optical flow . Recent wor k 
in [20] improves the 3D residual architecture by decoupling the 
3D convolutional kernel  and also presents the design  of a 3D 
attention mechanism to decrease  the model’s sensitiv ity to 
changes in the background environment.  
There are s everal disadvantages associated with the 3D 
CNN architecture . First, t he number of 3D CNN model 
parameters  increase s more rapidly compared to the 2D CNN. 
Second, the 3D C NN is hard to train and the resulting training information hard to transfer, and  its inference process very slow 
compared to other approaches. Third, in some cases, the 3D 
convolution operation cannot distinguish between the human 
action features and the background features making the model 
vulnerable to environmental factors.  
Mapping from 2D to 3D CNN.  Several research papers 
have explored  techniques to  transfer  the benefits of pre -trained 
2D CNNs  to 3D CNN architectures. In [21], the authors  
consider the 2D Resnet and replace  all its 2D convolutional 
filters with 3D convolutional kernels to arrive at the ResNet3D  
architecture . They assum e that a combination of  large -scale 
datasets  and deep 3D CNNs are capable of replicating  the 
success  of 2D CNNs on the ImageNet  dataset . Inspired by 
ResNeXt architecture [22] , Chen et al . [23]  propos e a multi -
fiber architecture that divides a complex neural network into an 
ensemble of lightweight networks thereby reduc ing the Identify  
the computational cost and simultaneously coordinat ing the  
information flow. Motivated by the SENet [24], the STCNet  
architecture  [25] incorporates channel -wise information within 
a 3D block  to capture the correlation information between the 
temporal and spatial channel s throughout  the network.  
Unifying 2D and 3D CNN.  3D CNNs have witnessed great 
success in recognizing human action in videos. However, the 
high complexity of training the 3D convolution kernels and the 
need for large quantities of training videos  limits their 
applicability . To reduce the complexity of 3D CNN  training, 
the P3D [26] and R(2+1)D [3] architectures explore the idea of 
3D factorization  wherein a 3D kernel is factorized into two 
separate operations, a 2D spatial convolution and a 1D temporal 
convolution . Trajectory convolu tion [27] is based on a  similar 
concept but utilizes deformable convolution for the temporal 
component to better deal with motion. A different approach of 
simplifying 3D CNNs is to  integrate  2D and 3D convolutions 
within  a single  network. MiCTNet  [28] integrates 2D and 3D 
CNNs to generate richer, deeper, and more informative feature 
maps  by decreasing the complexity of training in each round of 
spatial -temporal fusion. ARTNet [29] establishes a relation and 
appearance network by using a novel building block  compris ing 
of a spatial branch using 2D CNNs  and a relation branch using 
3D CNN s. S3D [30] and ECO [31] combine the advantages of 
the aforementioned models by adopt ing a top -heavy network to 
achieve online video understanding.  
Long -term Video Modelling Framework s. In their 
seminal work, Wang et al . [4], propose a simple, flexible, and 
general framework for learning action models in videos. 
Temporal segment networks  (TSN s) are designed by 
performing sparse sampling of a long video to  extract short 
snippets  followed by a segmental consensus function to 
aggregate information from the sampled snippets.  This allows 
the TSN to model long -range temporal structures within the 
entire  video. The Temporal Relational Reasoning Network  
(TRN) [32] enable s temporal relational reasonin g over videos 
by describ ing the temporal relations between observations in 
videos. While the TRN is shown to be capable of discover y and 
learn ing of  potential  temporal relations at multiple time scales  
within a video, it lacks the capacity to capture  finer temporal 
structure.  For efficient video understanding, Liu et al. [33] introduce a Temporal Shift Module  (TSM) that extends the shift 
operation to design a temporal module to capture temporal 
relations. The STM arc hitecture [34] incorporates two channel -
wise modules, one to represent motion features and the other to 
encode spatio -temporal features. Inspired by the approach in 
[24], the TEA architecture [35] improves the motion pattern 
representation by using the mot ion features to calibrate the 
spatio -temporal features.  
4D CNN . The V4D CNN architecture proposed by  Zhang 
et al. [5] tackles the analysis of RGB videos by incorporating a 
video-level sampling strategy to cover the holistic duration of a 
given video. A novel 4D residual block  is proposed  which 
allow s the casting of  3D CNNs into 4D CNNs for learning long -
range interactions of the 3D features, resulting in a “time of 
time” video -level representation. The proposed V4D 
architecture  has achieved excellent results compared to its 3D 
counterparts.  
III. F4D ARCHITECTURE  
A. Segment Based Sampling  
To model the long range spatio -temporal dependency, we use 
segment -based sampling  described in  [4]. Formally, given a 
whole video  𝑉, we divide it into 𝑈 sections of equal duration s 
and select a snippet , termed as an action unit , that is randomly 
sampled from each section to represent a short -term action 
pattern  within that section. The holistic action in the video is 
represented by a sequence of action units  {𝐴1,𝐴2,…,𝐴𝑈}, 
where 𝐴𝑖∈ ℝ𝐶×𝑇×𝐻×𝑊is the action unit obtained from the ith 
section, 𝐶 is the number of channels, 𝑇,𝐻,𝑊 are the temporal 
length , height, and width  . During the t raining phase, each 
action unit 𝐴𝑖 is randomly selected from each of the 𝑈 sections. 
During testing, the center of each 𝐴𝑖 is located exactly at the 
center of the corresponding section.  
B. Overview of 4D CNN  
In recent years, the 3D CNN has been shown to be a powerful 
approach for modelling short -term spatio -temporal features in 
video. However, the receptive fields of 3D kernels are usually 
deficient owing to the compact sizes of kernels, and hence 
pooling oper ations are applied to enlarge the receptive fields. In 
contrast, 4D convolution operations have been implemented to 
simultaneously model short -term and long -term spatio -
temporal representations since they have the capacity to model 
long-range dependencies and capture inter -clip interactions for 
efficient video -level representation learning.  
The input to a 4D convolution can be denoted as a tensor V of 
size (𝐶,𝑈,𝑇,𝐻,𝑊), where  𝑈 is the number of action units ( the 
4th dimension) . The batch dimension has been excluded  for 
simplicity.  Formally, a 4D convolution operation can be viewed 
as follows:  
𝑜𝑗𝑢𝑡ℎ𝑤= 𝑏𝑗+ ∑∑∑∑∑𝑊𝑗𝑐𝑠𝑝𝑞𝑟𝑣𝑐(𝑢+𝑠)(𝑡+𝑝)(ℎ+𝑞)(𝑤+𝑟)𝑅−1
𝑟=0𝑄−1
𝑞=0𝑃−1
𝑝=0𝑆−1
𝑠=0𝐶𝑖𝑛
𝑐(1) 
where 𝑜𝑗𝑢𝑡ℎ𝑤 is a pixel at position (𝑢,𝑡,ℎ,𝑤) of the 𝑗𝑡ℎ channel 
in the output following the annotation in [36], 𝑏𝑗 is a bias term,    
Fig. 1. F4D Residual Block.  
 
𝑐 is one of the 𝐶𝑖𝑛 input channels of the feature maps, 
𝑆 ×𝑃 × 𝑄× 𝑅 is the shape of 4D convolutional kernel, 
𝑊𝑗𝑐𝑠𝑝𝑞𝑟 is the weight at the position (𝑠,𝑝,𝑞,𝑟) of the kernel, 
corresponding to the 𝑐𝑡ℎchannel of the input feature maps and  
𝑗𝑡ℎchannel of the output feature maps.  Since deep learning 
libraries do  not provide an implementation for 4D convolutions, 
eqn. (1)  can be modified to generate  eqn. (2) which allow s the 
implementation of 4D convolutions using 3D convolutions. 
Eqn. (2)  can be formulated as follows:  
𝑜𝑗𝑢𝑡ℎ𝑤= 𝑏𝑗+ ∑(∑∑∑∑𝑊𝑗𝑐𝑠𝑝𝑞𝑟𝑣𝑐(𝑢+𝑠)(𝑡+𝑝)(ℎ+𝑞)(𝑤+𝑟)𝑅−1
𝑟=0)𝑄−1
𝑞=0𝑃−1
𝑝=0𝐶𝑖𝑛
𝑐𝑆−1
𝑠=0        (2) 
where the expression in the parentheses can be implemented by 
3D convolutions. Within the 4D space, the 4D convolution 
kernel has the ability to model both  the short -term 3D features 
of each action unit and the long -term temporal evolution of 
several action units at the same time. Thus, the 4D convolutions 
have the power to learn more complicated interactions of a 
long-range 3D spatio -temporal representation . 
C. F4D: Factorization of 4D CNN  
In this section, we design a network block termed as  F4D to 
improve upon the 4D convolution discussed in the previous 
section. We follow the work in [3] to approximate the 4D 
convolution by a 3D convolution followed by a 1D convolution, 
thereby decomposing the spatial modeling and the temporal 
modeling  for action units into two separate steps. The (3+1)D 
block replace s the 𝑁𝑖 4D convolutional filters of size 𝑁𝑖−1× 
𝑢×𝑡×ℎ×𝑤, with 𝑀𝑖 3D convolutional filters of size 
𝑁𝑖−1×𝑢×1×ℎ×𝑤  and 𝑁𝑖 temporal convolution filters of 
size 𝑀𝑖×1×𝑡×1×1. The hyperparameter 𝑀𝑖decides the 
dimensionality of the intermediate subspace where the signal is 
projected between the spatial convolution and the  temporal 
convolution. In order to have a (3+1)D block with the number 
of parameters  approximately equal to the number of parameters 
in the  implem entation of a  full 4D convolution layer, we set 
𝑀𝑖= ⌊𝑢 𝑡 ℎ 𝑤 𝑁𝑖−1 𝑁𝑖
𝑢 ℎ 𝑤  𝑁𝑖−1 + 𝑡𝑁𝑖⌋. 
  
The (3+1)D decomposition provide s advantages over the full 
4D convolution. First, although the number of parameters is 
approximately the same, the number of nonlinearities in the 
F4D network will increase due to the additional ReLU between 
the 3D and the 1D convolution in each block. Adding more 
nonlinearities results  in increased  complexity of functions that  
can be represented. This has been noted in VGG [37] and 
R(2+1)D networks which approximate the effect of a big filter 
by applying several smaller filt ers with additional nonlinearities  
introduced between  them . Second, forcing the 4D convolution 
into separate spatial and temporal modules can render the 
optimization easier , resulting in lower training error compared 
to the 4D convolution of the same size and capacity. Hence, for 
the same number of layers and parameters, the (3+1)D block 
will have lower training error and lower testing error compared 
to the V4D  network . Despite the fact that (3+1)D is a simple r 
architecture, experimental results show that it significantly 
outperforms the V4D  network.  
D. F4D block Integration  
This section discusses the ability of integrating the F4D 
blocks into existing state -of-the-art 3D CNN  frameworks for 
action recognition. As in  [5], we design a factorized 4D 
convolution in the residual structure [6], which shows the 
efficacy  of combining the short -term 3D features and the long -
term spatiotemporal representations for video action  
recognition.  We start by  defin ing a permutation  
functio n  ℘(di,dj ): 𝐴𝑑1×…×𝑑𝑖×…×𝑑𝑗×…𝑑𝑛⟼
𝐴𝑑1×…×𝑑𝑗×…×𝑑𝑖×…𝑑𝑛, which permutes the dimensions 𝑑𝑖 and 𝑑𝑗 
of a tensor 𝐴∈ ℝ𝑑1×….×𝑑𝑛. Formally, the residual factorized 4D 
convolution block can be formulated as:  
𝒴3𝐷= 𝒳3𝐷+ ℘(𝑈,𝐶)(ℱ3𝐷+ℱ1𝐷(℘(𝐶,𝑈)(𝒳3𝐷); 𝒲3𝐷+𝒲1𝐷))  (3) 
where ℱ3𝐷+ℱ1𝐷(𝒳; 𝒲3𝐷+𝒲1𝐷) is the factorized 4D 
convolution operation, and  𝒴3𝐷,𝒳3𝐷 ∈ ℝ𝑈×𝐶×𝑇×𝐻×𝑊. In 
order to process 𝒳3𝐷,𝒴3𝐷 using  standard 3D CNN s, 𝑈 is 
merged into the batch dimension  whereas i n order to process 
𝒳3𝐷 using  the factorized 4D convolution, we utilize the 
permutation function ℘ to permute the dimensions of 
𝒳3𝐷 from 𝑈×𝐶×𝑇×𝐻×𝑊 to 𝐶×𝑈×𝑇×𝐻×𝑊. Thus, 
the output of the factorized 4D convolution can be permuted 
back to the 3D form so that the output dimensions are 
consistent. The factorized 4D convolution is followed by a 
batch normalization layer  [38], ReLU activation and a dropout 
layer. In theory, any 3D CNN architecture can be recast  as a 
factorized 4D convolution using the proposed residual block . 
E. Attention in F4D Blocks  
Inspired by CBAM network [39], we implement  two 
attention modules and embed  it within the F4D block to learn 
better and more refined long -term spatiotemporal 
representations  with negligible computation overhead . The 
proposed attention has  three major components: temporal 
attention map over all action units, the channel attention map, 
and the spatio -temporal attention map. We arrange the attention 
modules by placing the temporal attention map in the 4D space, 
and both the channel attention map and the spatio -temporal 
attention map after permuting back to the 3D dimensio n. 
Temporal Attention (TA) Map.  In order to concentrate on 
the long -term temporal evolution of all action units, we design 
a temporal attention map that focus es on the different inter -clip 
motion patterns that evolve over time. Given an intermediate 
feature map 𝐹∈ ℝ𝐶×𝑈×𝑇×𝐻×𝑊 as input, we infer a temporal 
attention map 𝑀𝑇∈ℝ1×𝑈×𝑇×1×1 by utilizing both average 
pooling and max pooling along the channel and spatial 
dimensions to obtain two feature descriptors 𝐹𝑎𝑣𝑔𝑇and 𝐹𝑚𝑎𝑥𝑇. 
Although CBAM network adopts a  filter size of 7×7  which is 
considered a design  choice that has low computation cost in 2D 
image -related tasks , using a convolutional operation with such 
a large filter size in 3D or 4D  space  incurs a significant 
computational cost in our model.  To obtain substantial 
computational cost savings, we use the dilated convolution . We 
adopt a two-path 1D dilated temporal convolution  [40]. The 
first path has a temporal dilated convolution with a dilation 
factor = 2  (skipping 1 pixe l). The second path has a temporal 
dilated convolution with a dilation factor = 3  (skipping 2 
pixels) . The two paths model the multiscale global temporal 
interdependency between all action units.  The temporal 
attention map is computed as follows:  
𝑀𝑇(𝐹)= σ(𝐶𝑜𝑛𝑣 1𝐷 ([𝐴𝑣𝑔𝑃𝑜𝑜𝑙 (𝐹)+(𝑀𝑎𝑥𝑃𝑜𝑜𝑙 (𝐹)]))       (4) 
= σ(𝐶𝑜𝑛𝑣 1𝐷 ([(𝐹𝑎𝑣𝑔𝑇+𝐹𝑚𝑎𝑥𝑇]))) (5) 
Where σ denote s the sigmoid function,  and 𝐶𝑜𝑛𝑣 1𝐷 denotes 
the multipath dilated temporal convolution layer. The refined 
feature map after the temporal attention module is computed as:  
 
                        𝐹𝑇𝐴= 𝑀𝑇⨂𝐹+𝐹                                 (6) 
Where ⨂ denotes the element -wise multiplication, + denotes 
the inner residual connection and 𝐹𝑇𝐴 the refined feature map.  
In the original implementation of CBAM, feature refinement is 
attained by multiplying the attention maps with the input 
feature map. However, it does not take into consideration the 
preservation of the original feature map. We use inner residual 
connections  in all attention modules to preserve the original    
 
Fig. 2. Temporal Attention Modul e 
 
information. This helps to avoid any  unrelated features or 
background noise in the current layers.  
Channel Attention (CA) Map.  As in the CBAM  network , 
the channel attention map is produced by exploiting the inter -
channel relationship of features. Given an intermediate feature 
map 𝐹′∈ ℝ𝑈×𝐶×𝑇×𝐻×𝑊 , we compute the channel attention 
map by using both , the max-pooled features and average pooled 
features at the same time generating two different descriptors. 
Subsequently , both descriptors are fed to a multi -layer 
perceptron with one hidden layer  with an activation size of 
ℝ𝐶/𝑟×1×1×1×1 , where 𝑟 is the reduction ratio (we set 𝑟 = 16). 
The output feature vectors are then  combined using element -
wise summation. The entire process can be summarized as 
follows:  
𝑀𝑐(𝐹′)= σ(𝑀𝐿𝑃 (𝐴𝑣𝑔𝑃𝑜𝑜𝑙 (𝐹′)+𝑀𝐿𝑃 (𝑀𝑎𝑥𝑃𝑜𝑜𝑙 (𝐹′)))      (7) 
=σ(𝑊1((𝑊0(𝐹′𝑚𝑎𝑥𝐶))+𝑊1((𝑊0(𝐹′𝑚𝑎𝑥𝐶))) (8) 
Where 𝑊0∈ ℝ𝐶/𝑟×𝑐 and  𝑊1∈ ℝ𝐶×𝐶∕𝑟. In this case, 
the weights, 𝑊0 and 𝑊1 are shared by both inputs and the 
ReLU  activation function is followed by  weighting by  𝑊0. The 
channel attention map can be summarized as follows:  
𝐹𝐶= 𝑀𝐶⨂𝐹′+𝐹′(9) 
During multiplication, the channel attention values are copied 
along the spatial dimension  and the temporal dimension .  
 
Spatio -temporal Attention (STA) M ap. This module is 
designed to focus on the spatio -temporal discriminative 
features by concentrating on the ROIs  that evolve over time.  
The spatio -temporal attention map is generated by exploiting 
the inter-spatial relationship of features. Given an intermediate 
feature map 𝐹′∈ ℝ𝑈×𝐶×𝑇×𝐻×𝑊, we compute the spatio -
temporal attention map by first applying  both, the max-pooled  
operations  𝐹𝑚𝑎𝑥′𝑆𝑇∈ ℝ1×1×𝑇×𝐻×𝑊 and the average pooled 
operations 𝐹′𝑎𝑣𝑔𝑆𝑇∈ ℝ1×1×𝑇×𝐻×𝑊 along the channel axis and 
concatenate them to generate a refined and efficient feature 
descriptor 𝑚𝑆𝑇 . Subsequently,  we forward 𝑚𝑆𝑇 to a two -path 
2D dilated convolution layer (with skipping 1 -pixel and 
skipping 2 -pixels) and two -path 1D dilated temporal 
convolution layer (with skipping 1 -pixel and skipping 2 -
pixels).  These two layers  are designed to explore multiscale 
spatial relation ships  and local temporal interdependen cies 
respectively.   
 
 
 
 
 
Fig. 3. Spatio -Temporal Attention Module  
 
In summary, the spatio-temporal  attention is computed as:   
𝑚𝑆𝑇=𝐶𝑜𝑛𝑐𝑎𝑡𝑒𝑛𝑎𝑡𝑒 [𝐹′𝑎𝑣𝑔𝑆𝑇,𝐹𝑚𝑎𝑥′𝑆𝑇] (10) 
𝑀𝑆𝑇 = 𝜎(𝐶𝑜𝑛𝑣 1𝐷(𝑅𝑒𝐿𝑈 (𝐶𝑜𝑛𝑣 2𝐷(𝑚𝑆𝑇)) (11) 
where 𝐶𝑜𝑛𝑣 2𝐷 represents the two path 2D convolution layer. 
The refined feature map is computed as:  
𝐹𝑆𝑇= 𝑀𝑆𝑇⨂𝐹𝐶+𝐹𝐶 (12) 
where 𝐹𝑆𝑇 is the refined feature map.  
IV. EXPERIMENTS  
A. Datasets  
Five benchmark datasets have been used for experimental 
evaluation of the proposed F4D convolution block:  Something -
Something -v1, Something -Something -v2 [7], Kinetics -400 [8], 
UCF101  [9], HMDB51  [10]. Something -Something -v1 is a 
dataset that contains labeled video clips of humans performing 
predefined actions. It consists of 108,499 videos, with 86,017 
in the training set, 11,522 in the validation set and 10,960 in the 
testing set  comprising of 174 action classes. Something -
Something -v2 is an extension of the first version with a 
collection of 2 20,847 videos incorporating several 
enhancements such as higher video resolution, and reduced 
label noise.  The Kinetics 400 dataset covers 400 action classes 
with ≈400 video clips for each action. The v ideo clips  are 
obtained  from different YouTube videos with each video  clip 
lasting ≈10 seconds. The actions are human focused, and the 
action classes include a wide range of  human -human and 
human -object interactions. The UCF101 dataset consists of 
13320 video clips with 101 action classes. This dataset includ es 
several variation s arising from multi -viewpoint s, camera 
motion, object appearance, cluttered background, and 
illumination conditions. The HMDB51  dataset  has 51 action 
classes distributed across 6849 video clips collected from 
different sources and public databases such as YouTube, 
Google and the Prelinger archive.  
B. Implementation Details  
We perform our initial evaluation on Something -Something 
datasets , using the training split for training and  the validation 
split for testing. To learn the network parameters, we use the 
mini batch stochastic gradient descent (SGD) as the 
optimization algorithm. The batch size is set to 128 and the 
momentum to 0.9. Initially, the learning rate is set to 0.01, and    
Fig. 4. Training and Testing errors for V4D (left) and F4D (right)  
drops by a factor of 10  at epoch s 20, 40, and 60. Model t raining 
is concluded at 80 epochs. Batch normalization is applied to all 
convolutional layers. We follow each F4D convolutional block 
with batch normalization, ReLU activation and a dropout layer. 
To speed up training, we utilize the data parallelism strategy 
implemented using the torch.nn.DataParallel module in Pytorch 
to split the mini -batch of samples into multiple smaller mini -
batches and perform the computation over four Tesla P100 -
PCIE -16GB GPUs . Data augmentation plays an important role 
in enhancing the performance of deep learning architecture s. 
During training, we use random left -right flipping, location 
jittering , scale jittering  and corner cropping . 
C. Results on Motion -Focused Datasets  
In this section, we evaluate our proposed approach with the 
state-of-the-art approaches on motion -focused datasets 
including Something -Something -v1 and Something -
Something -v2. Both datasets focus on modelling motion and 
temporal information where the motio n of actions is more 
complicated compared to that in the Kinetics -400 dataset albeit 
with a clearer background. Videos in both datasets contain one 
continuous action with clear start and end points along the 
temporal dimension. To prepare the videos for tr aining, we use 
the segment -based sampling technique explained in Section 
3.1. We segment the holistic duration of a video into 𝑈 sections 
of equal durations in their temporal order  and for each section, 
we randomly select a snippet composed of 32 frames. To form 
an action unit, we take each snippet and use the sampling 
strategy mentioned in [2] to sample 8 frames with a fixed stride 
of 4.  We also experiment with the number of frames in the 
snippet set to 16  with the frame size fixed at 256×256 pixels . 
After applying the data augmentation techniques mentioned in 
the previous section, we resize the cropped region to 224×224 
pixels . We fix  𝑈=4 in all  of experiments. For fair comparison, 
we use the ResNet50 CNN as the backbone for proposed F4D 
network . 
For inference, we follow the approach in [2, 41]  using fully 
spatial convolutional testing. From  the entire  duration of a 
video, we sample 10 action units ( 𝑈=10)  of equal duration, 
scale  up the smaller  spatial image dimension to 256 pixels and 
take 3 crops of 256 ×256 pixels to spatially  cover the entire  
frame for each action unit, and then resize th e crops to 224×224 
pixels . Finally, the final prediction is produced via global 
average pooling over the sequence of all action units.  
Fig.4 highlights the training error and testing error for V4D 
CNN and F4D architecture. It is illustrated that for the same 
network backbone (ResNet 50) and approximately the same 
number of parameters, the F4D architecture achieves lower 
training error and  lower testing error. This shows that the 
factorization of the 4D CNN renders the optimization easier and 
achieves better resulting representation.  
Fig. 5 and Fig. 6 show the results of our approach compared 
to the state -of-the-art approaches on the Something -Something 
datasets. Compared with the baseline approach that uses a TSN 
with 8 frames, the proposed F4D approach with 8 frames 
achieves a 35.2% improvement with top-1 accuracy of 54.9 
with 8 frames on the Something -Something -v1 dataset when 
pretrained on ImageNet [42]. When the proposed F4D model is 
pretrained on ImageNet and Kinetics -400, the model achieves 
57.5 top -1 accuracy, an improvement of 36.8%. On Something -
Something -v2, the F4D model yields a 66.3 and 69.8 in top -1 
accuracy with an improvement of 39.5% when pretrained on 
ImageNet and 43% improvement in top -1 accuracy when 
pretrained on ImageNet and Kinetics -400 respectively .  
When the F4D  model is trained on ImageNet  and Kinetics -
400 using  16 frames on Something -Something -v1, the F4D 
model achieves a 58.4 top-1 accuracy. This shows a 7 .7% (50.7 
vs 58.4)  and 6.1% (52.3 vs 58.4)  improvement  in accuracy 
when compared with STM [34] and TEA [35] respectively . The 
above results show that the F4D model is capable of learning 
strong temporal  relationships in the videos in these datasets. 
When the F4D  model is compared to V4D using 8 frames on 
Something -Something -v1, the F4D  model shows a 4.5%  (50.4 
vs 54.9)  and 7.1%  (50.4 vs 57.5)  improvement in top -1 
accuracy when  pretrained on ImageNet  alone and, on ImageNet  
and Kinetics -400 respectively. This shows that the 4D 
factorization and the attention modules added in the residual 
block of the F4D model can capture more complex inter -clip 
interactions and finer long -range temporal structures  in the 
underlying video . 
 
Fig. 5. Performance of F4D on Something -Something v1 compared with state -
of-the-art approaches.   
Fig. 6. Performance of F4D on Something -Something v2 compared with state -
of-the-art approaches.  
D. Results on Scene -Focused Datasets  
In this section, we compare the proposed F4D approach with 
the state -of-the-art approaches on scene -focused datasets 
including Kinetics -400, UCF101 and HMDB51. The videos 
representing most actions in these datasets are short and can be 
recognized by static appearance without considering temporal 
relationships. Furthermore, the background information 
contributes heavily towards deciding the action class in most of 
these videos.  
Fig. 7 shows the results of the F4D model and other 
approaches on the Kinetics -400 dataset. When comparing  
the F4D model with STM [34] and TEA [35], F4D model shows 
a performance improvement of 7.5% and 5.1% respectively.  
Moreover, it outperforms MSNET [43] by 4.8% and V4D by 
3.8%.  Although the F4D model is designed specifically for 
temporal focused action recognition, it shows competitive 
results when compared to state -of-the-art methods.  
Fig. 8 highlights  the results on the UCF -101 and HMDB51 
datasets. We follow [4] in adopting the three training/testing 
splits for evaluation. The F4D model was pretrained on 
ImageNet and Kinetics -400. In both experiments, we set 𝑈=4 
and use 16 frames during training.  Our F4D model achieves 
98.2 and 84.3 accuracy on UCF101 and HMDB51 datasets 
respectively.  
 
 
Fig. 7. Performance of the F4D model on Kinetics -400. 
 
 
Fig. 8. Performance of the F4d model on UCF101 and HMDB51  
E. Runtime Analysis  
In this section, we compare the proposed F4D architecture 
with the V4D CNN. Our F4D architecture achieves better 
results than the V4D CNN on several benchmark datasets. 
Table 1 shows the model complexity and accuracy of F4D and 
V4D on Something -Something v 1 dataset. We follow [ 34] to 
evaluate the FLOPs and speed of our architecture. We equally 
sample 8 or 16 frames from a video and then apply the center 
crop. Moreover, for speed we use a batch size of 16. All 
evaluations are conducted using two Tesla P100 -PCIE-16GB 
GPU . As seen in Table 1 , F4D improves the accuracy by 7.1% 
while achieving 2.3x less FLOPs (72G vs 167G). Moreover, our 
F4D gains more accuracy with 1.37x faster speed. These results 
demonstrate the effectiveness of the proposed factorization and 
attention modules in learning be tter and refined long -range 
spatiotemporal representation with less FLOPs, more speed, 
and a very limited increase in the number of parameters.  
F. Ablation Study  
In this section, we evaluate our F4D model on the 
Something -Something datasets given different scenarios. All 
models used in this section are pretrained on ImageNet and 
Kinetics -400. 
Location of F4D Blocks.  In this experiment, we study the 
impact of adding the F4D residual block in different positions 
within the F4D network. In these experiments, we fix  𝑈=4 and 
use 8 frames during training. As shown in Table 2, adding F4D  
blocks at conv 2, conv 3, conv 4 or conv 5 layers yields better top-
1 accuracy. Adding a n F4D residual block at the conv 1 layer 
does not have a big impact which means that the short -long 
term features need to be refined by the earlier layers first to  
yield  more meaningful representations.  We found that adding 
F4D block s from conv 2 to conv5  yields the best results . 
 
TABLE I.   Model complexity of F4D compared to V4D using single crop.  
  
Approach  Frames  Top1  FLOPs  Speed  # of 
param  
V4D [5]  8 50.4 167G 38.1 V/s  36.2M  
F4D 8 57.5 72G 52.3 V/s  36.8M  
F4D  16 58.4 143G  27.5 V/s  36.8M  
 
 TABLE II.   LOCATION OF F4D RESIDUAL BLOCKS  
 
Location  v1 top -1 accuracy  v2 top -1 accuracy  
conv1  45.3 55.1 
conv2  49.9 57.3 
conv3  51.6 61.2 
conv4  52.8 63.2 
conv5  53.2 63.9 
conv2 -3 54.2 64.3 
conv3 -4 56.4 67.5 
conv2 -5 57.5 69.8 
 
TABLE III.  IMPACT OF NUMBER OF ACTION UNITS FOR TRAINING  
 
𝑈𝑡𝑟𝑎𝑖𝑛 V1 top -1 accuracy  V2 top -1 accuracy  
3 56.8 69.1 
4 57.5 69.8 
5 57.9 70.3 
6 58.3 70.5 
7 58.5 70.7 
 
Number of action units 𝑼 used for training.  In this  
experiment, we observe the change in the value of 𝑈 during 
training and we found that the value of 𝑈 have a significant 
impact on overall performance.  Although we anticipated 
obtaining higher performance  figures , the videos in Something -
Something datasets  are relatively short and  have one single and 
continuous action, and the action does not involve many stages. 
We argue that the effect of higher 𝑈 values will be more visible 
when using longer untrimmed videos during trainin g. 
 
Impact of Attention Modules. In this experiment, we study 
and verify the contributions of each attention module added in 
the proposed F4D model . We compare the results of each 
individual attention module and the various combination s of 
these attention modules. As seen in Table  4, TA+CA+STA 
achieves the best top -1 accuracy and outperforms the model 
that has no attention by 5.8% on Something -Something v1 and  
 
TABLE IV.   IMPACT OF ATTENTION MODULES  
 
Modules  v1 top -1 accuracy  v2 top -1 accuracy  
No Attention  51.7 60.2 
CA 52.5 61.3 
STA 53.8 63.2 
CA+STA  54.2 65.0 
TA 54.0 64.6 
TA+CA  55.3 65.4 
TA+CA+STA  57.5 69.8 
 
TABLE V.   COMPARISON WITH OTHER ATTENTION MODULES  
 
Modules  v1 top-1 accuracy  v2 top -1 accuracy  
SE [24] 52.1 60.9 
CBAM  [39] 52.9 62.1 
STM Block [34]  53.9 64.8 
TEA Block [35]  54.3 65.5 
TA+ CA+ STA 57.5 69.8 
9.6% on something -something v2. By c ombining all the 
attention modules , the F4D model was able to  learn richer 
short -long term motion and spatiotemporal features.   
Comparison with other attention modules. We compare the 
proposed TA and STA attention modules with two state -of-the-
art attention modules namely SE  [24] and CBAM [39]. Both 
attention modules can improve the performance by making the 
network focus on the distinctive object feature s by 
incorporating  finer channel -wise attention, and the spatial 
module in CBAM can make the model concentrate on the 
spatial  ROIs . First, we remove the proposed TA, CA, and STA 
modules in the F4D model and insert the SE module in the 3D 
space and compute the top -1 accuracy for both Something -
Something datasets.  In the second trial, we insert the CBAM 
instead and observe the improvement over the SE module. As 
illustrated in Table 5, our proposed combination of TA, CA and 
STA modules improve s the performance significantly as both 
proposed attention modules exploit short term and long -term 
temporal relationships  unlike SE and CBAM modules that do  
not take temporal modelling  into account.  
V. CONCLUSION  
      In this paper, we presented an effective yet simple 
framework for video level representation learning namely F4D, 
to model both short -range motion and long -range temporal 
dependency at a large scale. We add the F4D residual blocks 
within the ResNet a rchitecture to build the F4D pipeline. An 
F4D residual block performs the factorized 4D convolutional 
neural network which learns complex inter -clip interactions and 
finer temporal structures. Furthermore, it applies the two 
proposed attention modules to t he intermediate feature maps to 
learn richer and refined short -long term motion and 
spatiotemporal features. Extensive experiments have been 
conducted to verify the effectiveness of F4D on five action 
recognition benchmark datasets, where our proposed F4D 
achieved state -of-the-art results.  
REFERENCES  
[1] S. Yu, Y. Cheng, L. Xie, and S.Z. Li. Fully convolutional networks for 
action recognition. IET Computer Vision, 11(8), 744 -749, 2012.  
[2] C. Feichtenhofer, H. Fan, H, J. Malik, and K. He. Slowfast networks for 
video recognition. In Proceedings of the IEEE/CVF international 
conference on computer vision (pp. 6202 -6211), 2019.  
[3] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri. A closer 
look at spatiotemporal convolutions for action recognition. In 
Proceedings of the IEEE conference on Computer Vision and Pattern 
Recognition (pp. 6450 -6459), 2018.  
[4] L. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., & Van 
Gool, L. (2018). Temporal segment networks for action recognition in 
videos. IEEE transactions on pattern analysis and machine intelligence, 
41(11), 2740 -2755.  
[5] S. Zhang, S. Guo, W. Huang, M. R. Scott, and L.  Wang. V4d: 4d 
convolutional neural networks for video -level representation learning. In 
proceedings of International Conference on Learning Representations, 
2020  
[6] K. He, X. Zhang, S.  Ren, and J. Sun. Deep residual learning for image 
recognition. In Proceedings of the IEEE conference on computer vision 
and pattern recognition (pp. 770 -778), 2016  
[7] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, 
H. Kim, ... and R. Memisevic. The" something something" video database 
for learning and evaluating visual common sense. In Proceedings of the 
IEEE international conference on computer vision (pp. 5842 -5850), 2017  [8] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. 
Vijayanarasimhan, ... and A. Zisserman. The kinetics human action video 
dataset. arXiv preprint arXiv:1705.06950, 2017  
[9] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset of 101 human 
actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 
2012  
[10] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a 
large video database for human motion recognition. In 2011 International 
conference on computer vision (pp. 2556 -2563). IEEE, 2011  
[11]  K. Simonyan, & A. Zisserman. Two -stream convolutional networks for     
action recognition in videos. Advances in neural information processing 
systems, 27, 2014  
[12] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, ... 
and T. Brox. Flownet: Learning optical flow with convolutional networks. 
In Proceedings of the IEEE international conference on computer vision 
(pp. 2758 -2766), 2015  
[13] S. Sun, Z. Kuang, L. Sheng, W. Ouyang, and W. Zhang. Optical flow 
guided feature: A fast and robust motion representation for video action 
recognition. In Proceedings of the IEEE conference on computer vision 
and pattern recognition (pp. 1390 -1399), 2018  
[14] B. Zhang, L. Wang, Z. Wang, Y. Qiao, and H. Wang. Real -time action 
recognition with enhanced motion vector CNNs. In Proceedings of the 
IEEE conference on computer vision and pattern recognition (pp. 2718 -
2726), 2016  
[15] A. J. Piergiovanni, and M. S. Ryoo, M. S. Representation flow for action 
recognition. In Proceedings of the IEEE/CVF conference on computer 
vision and pattern recognition (pp. 9945 -9953), 2019  
[16] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural networks for 
human action recognition. IEEE transactions on pattern analysis and 
machine intelligence, 35(1), 221 -231, 2012  
[17] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning 
spatiotemporal features with 3d convolutional networks. In Proceedings 
of the IEEE international conference on computer vision (pp. 4489 -4497), 
2015  
[18] D. Tran, J.  Ray, Z.  Shou, S. F. Chang, and M. Paluri. Convnet 
architecture search for spatiotemporal feature learning. arXiv preprint 
arXiv:1708.05038, 2017  
[19] J. Carreira, and A. Zisserman. Quo vadis, action recognition? a new model 
and the kinetics dataset. In proceedings of the IEEE Conference on 
Computer Vision and Pattern Recognition (pp. 6299 -6308), 2017  
[20] M. Dong, Z. Fang, Y. Li, S. Bi, and J. Chen. AR3D: attention residual 3D 
network for human action recognition. Sensors, 21(5), 1656. 2021  
[21] K. Hara, H. Kataoka, and Y. Satoh. Can spatiotemporal 3d cnns retrace 
the history of 2d cnns and imagenet?. In Proceedings of the IEEE 
conference on Computer Vision and Pattern Recognition (pp. 6546 -
6555), 2018  
[22] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual 
transformations for deep neural networks. In Proceedings of the IEEE 
conference on computer vision and pattern recognition (pp. 1492 -1500), 
2017  
[23] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng. Multi -fiber networks 
for video recognition. In Proceedings of the european conference on 
computer vision (ECCV) (pp. 352 -367), 2018  
[24] J. Hu, L. Shen, and G. Sun, G. Squeeze -and-excitation networks. In 
Proceedings of the IEEE conference on computer vision and pattern 
recognition (pp. 7132 -7141), 2018  
[25] A. Diba, M. Fayyaz, V. Sharma, M. M Arzani, R. Yousefzadeh, J. Gall, 
and L. Van Gool. Spatio -temporal channel correlation networks for action 
classification. In Proceedings of the European Conference on Computer 
Vision (ECCV) (pp. 284 -299), 2018  
[26] Z. Qiu, T. Yao, and T. Mei. Learning spatio -temporal representation with 
pseudo -3d residual networks. In proceedings of the IEEE International 
Conference on Computer Vision (pp. 5533 -5541), 2017  
[27] Y. Zhao, Y. Xiong, and D. Lin. Trajectory convolution for action 
recognition. Advances in neural information processing systems, 31, 2018  
[28] Y. Zhou, X. Sun, Z. J Zha, and W. Zeng. Mict: Mixed 3d/2d convolutional 
tube for human action recognition. In Proceedings of the IEEE conference 
on computer vision and pattern recognition (pp. 449 -458), 2018  
[29] L. Wang, W. Li, W. Li, and L. Van Gool, L. Appearance -and-relation 
networks for video classification. In Proceedings of the IEEE conference 
on computer vision and pattern recognition (pp. 1430 -1439), 2018  [30] S. Xie, C. Sun, J.  Huang, Z. Tu, and K. Murphy. Rethinking 
spatiotemporal feature learning: Speed -accuracy trade -offs in video 
classification. In Proceedings of the European conference on computer 
vision (ECCV) (pp. 305 -321), 2018  
[31] M. Zolfaghari, K. Singh, and T. Brox. Eco: Efficient convolutional 
network for online video understanding. In Proceedings of the European 
conference on computer vision (ECCV) (pp. 695 -712), 2018  
[32] B. Zhou, A. Andonian, A. Oliva, and A. Torralba. Temporal relational 
reasoning in videos. In Proceedings of the European conference on 
computer vision (ECCV) (pp. 803 -818), 2018  
[33] J. Lin, C. Gan, and S. Han. TSM: Temporal Shift Module for Efficient 
Video Understanding. In The IEEE International Conference on 
Computer Vision (ICCV), 2019.  
[34] B. Jiang, M. Wang, W. Gan, W. Wu, and J. Yan, J. Stm: Spatiotemporal 
and motion encoding for action recognition. In Proceedings of the 
IEEE/CVF International Conference on Computer Vision (pp. 2000 -
2009), 2019  
[35] Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, and L. Wang. Tea: Temporal 
excitation and aggregation for action recognition. In Proceedings of the 
IEEE/CVF conference on computer vision and pattern recognition (pp. 
909-918), 2020  
[36] S. Ji, W. Xu, M. Yang, and K. Yu. 3D convolutional neural networks for 
human action recognition. IEEE transactions on pattern analysis and 
machine intelligence, 35(1), 221 -231, 2012  
[37] K. Simonyan, and A. Zisserman, A. Very deep convolutional networks 
for large -scale image recognition. arXiv preprint arXiv:1409.1556, 2014.  
[38] S. Ioffe, and C. Szegedy. Batch normalization: Accelerating deep network 
training by reducing internal covariate shift. In International conference 
on machine learning (pp. 448 -456). PMLR., 2015.  
[39] S. Woo, J. Park, J. Y Lee, and I. S Kweon. Cbam: Convolutional block 
attention module. In Proceedings of the European conference on computer 
vision (ECCV) (pp. 3 -19), 2018  
[40] N. Cheema, S.  Hosseini, J. Sprenger, E. Herrmann, H. Du, K. Fischer, 
and P. Slusallek. Dilated temporal fully convolutional network for 
semantic segmentation of motion capture data. In Proceedings of the 
Conference on Computer Animation, 2018  
[41] X. Wang, R. Girshick, A. Gupta, and K.  He. Non -local neural networks. 
In Proceedings of the IEEE conference on computer vision and pattern 
recognition (pp. 7794 -7803), 2018  
[42] J. Deng, W. Dong, R. Socher, L. J.  Li, K.  Li, & L. Fei -Fei. Imagenet: A 
large -scale hierarchical image database. In 2009 IEEE conference on 
computer vision and pattern recognition (pp. 248 -255). IEEE, 2009  
[43] H. Kwon, M. Kim, S. Kwak, & M. Cho. Motionsqueeze: Neural motion 
feature learning for video understanding. In Computer Vision –ECCV 
2020: 16th European Conference, Glasgow, UK, August 23 –28, 2020, 
Proceedings, Part XVI 16 (pp. 345 -362). Springer Internation al 
Publishing.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 