IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 1
DeepCPG Policies for Robot Locomotion
Aditya M. Deshpande, Eric Hurd, Ali A. Minai, and Manish Kumar
Abstract—Central Pattern Generators (CPGs) form the neural
basis of the observed rhythmic behaviors for locomotion in legged
animals. The CPG dynamics organized into networks allow the
emergence of complex locomotor behaviors. In this work, we take
this inspiration for developing walking behaviors in multi-legged
robots. We present novel DeepCPG policies that embed CPGs as a
layer in a larger neural network and facilitate end-to-end learning
of locomotion behaviors in deep reinforcement learning (DRL)
setup. We demonstrate the effectiveness of this approach on
physics engine-based insectoid robots. We show that, compared to
traditional approaches, DeepCPG policies allow sample-efﬁcient
end-to-end learning of effective locomotion strategies even in the
case of high-dimensional sensor spaces (vision). We scale the
DeepCPG policies using a modular robot conﬁguration and multi-
agent DRL. Our results suggest that gradual complexiﬁcation
with embedded priors of these policies in a modular fashion
could achieve non-trivial sensor and motor integration on a robot
platform. These results also indicate the efﬁcacy of bootstrapping
more complex intelligent systems from simpler ones based on
biological principles. Finally, we present the experimental results
for a proof-of-concept insectoid robot system for which DeepCPG
learned policies initially using the simulation engine and these
were afterwards transferred to real-world robots without any
additionalﬁne-tuning.
Index Terms—Developmental robotics, Central pattern genera-
tor, Locomotion, Reinforcement learning; Deep neural networks
I. I NTRODUCTION
Biologically inspired robots often have many degrees of
freedom. Locomotion in such legged robots involves the
interaction of a multi-body system with the surrounding
environments through multiple contact points. This presents a
formidable challenge for traditional control approaches. Self-
organization of complex behaviors is seen as a much more
viable approach for these. While it may be possible to solve
some of the challenges of autonomous locomotion in controlled
environments, traditional approaches are not useful for the real
world situations where multi-task generalization of the system
is required.
Biologically, it is known that Central Pattern Generators
(CPGs) are the neural modules primarily responsible for gen-
erating rhythmic responses that result in oscillatory functions
[1]–[4]. CPGs are used in a number of biological functions
like walking, swimming,ﬂying, etc. [5] and have inspired
development of elegant biomimetic control approaches for
locomotion of legged robots [6]–[8]. Furthermore, sensory
feedback also plays an important role in regulating the
oscillatory behaviors of CPGs [9]. It has been reported that
CPGs, sensory information and descending brain inputs interact
with each other to orchestrate coordinated movement of the
A. M. Deshpande, E. Hurd, A. A. Minai and M. Kumar were with University
of Cincinnati, Cincinnati, Ohio, 45221 USA e-mail: deshpaad@mail.uc.edu;
hurdeg@mail.uc.edu; ali.minai@uc.edu; manish.kumar@uc.edu.six legs of a walking insect [10], [11]. DeAngelis et al. [12]
observed that sensory perturbations in walking Drosophila
are responsible for altering their periodic walking gaits. Their
ﬁndings suggested the variablity in Drosophila walking gaits
could be a result of low-dimensional control architecture, which
provides a framework for understanding the neural circuits that
regulate hexapod legged locomotion
CPG networks are capable of generative encoding, and have
inherentﬂexibility in combining phase coupling with traditional
kinematic control to produce a variety of coordinated behaviors
[13]–[18]. The incorporation of sensory feedback into CPGs
has also been investigated, but this often requires extensive
engineering. Thus, such methods have been developed for
controlled scenarios including salamander-inspired Pleurobot
[15], quadruped robots [19], worm-like robots [20], stick-insect
robots [21], and dung beetle-like robot [22].
With advances in robotics, deep learning, and neuroscience,
we are starting to see real-world robots that not only look
like but also interact with the environment just as living
creatures do [15], [23]–[26]. However, most of these need
hand-tuned parameter sets and are only evaluated in the
constrained or supervised setting. The models used in these
robots are simpliﬁed further with various assumptions to
reduce the number of tunable parameters. Although such
methods have beautifully demonstrated how higher-level neural
modulations in cortical centers could enable the emergence
of various locomotion strategies [15], there has been limited
work on how to extend these models for actively using
high-dimensional complex observations from various on-board
sensors to modulate the cortical signals.
Previous studies have demonstrated the usefulness of
biomimetic movement templates such as movement primitives
and CPGs in robots [27]–[34]. The Hodgkin-Huxley model
of action potential generation in single neurons [35], [36] has
been used for developing locomotion strategies in snake-like
[37], [38] and quadruped robots [39]. The Matsuoka model
[40] has been used to produce robot locomotion [17], [41]. The
Van der Pol oscillator [42] and the Hopf oscillator [43] were
used in swinging robots [44] and for generating walking gaits
[45], [46]. The Kuramoto model of coupled phase oscillators is
one of the most abstract and simple phenomenological models
[47], [48] and is widely used in the robotics community to
develop locomotion strategies [15], [17], [49], [50]. Most
of these approaches have been developed with a behavior-
speciﬁc objective for robots and employed parameter tuning for
behavioral policies either using manual effort or auto-tuning
with a genetic algorithm. For high-dimensional sensor and
action spaces, deep learning methods embedded with movement
primitive have been investigated in the context of imitation
learning or supervised learning [51], [52]. Model-based control
guided imitation learning has also been proposed for learningIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 2
������������������������������
��������������������
�����������������
������
Fig. 1. Kuramoto model response for state and parameter perturbations: CPG
0and CPG 0�are initialized at identical states. Plot contains CPG outputs
y0andy0�. (cpg 0) CPG states φ0, a0, b0were perturbed to random values
at iterations 500 and1500 . (cpg 0�) CPG parameters ω0�, A0�, B0�were
perturbed to random values at iterations 500and1500 . Refer equations (3)-(6)
for details.
various quadruped robot gaits [53]. The end-to-end learning of
behavioral control policies based on high-dimensional sensor
feedback along with movement priors has received limited
attention.
In this work, we focus on the problem of generality in
controlling legged robots with high-dimensional environment
sensing (observation space) and interaction modalities (action
space) using CPG-inspired movement priors. We propose to
bring together current ideas in reinforcement learning and
deep learning, and expressive parameterizations resulting from
CPGs in developing novel models that are capable of learning
expressive behaviors for walking robots.
Brooks et al. [54] presented one of theﬁrst works that
demonstrated feedback and reactive control laws alone can
generate the full locomotive behaviors. Reinforcement learning
(RL), on the other hand, offers a framework for artiﬁcial
agents tolearnsuch decision-making in their environments by
trial and error using the feedback from their experiences [55].
The paradigm of modern deep reinforcement learning (DRL)
algorithms has emerged with promising approaches to teach
robots complex end-to-end control tasks such as locomotion
[56], [57]. Typically, an RL agent (i.e. control policy) is trained
to predict in raw action spaces and outputs actions in terms
of motor torques or joint angles that may lead to non-smooth
trajectories. It has been suggested previously by various works
[58]–[60] that if we model the policy to predict actions in the
trajectory space of the system, its response could be constrained
to remain smooth. For example, in the case of a robotic system,
if its control policy predicts the actions as trajectories of its
various motor joints, the system response remains smooth even
when these actions change (see cpg 0�in Fig. 1). We propose
to use this principle in training the locomotion policies for the
walking robots.
Various DRL algorithms have been proposed for learning
legged robot locomotion where the control policies are trained
from scratch. The work in [61] presented an attention-based
recurrent encoder network for learning robust quadruped
walking in challenging terrain. Authors in [62] presented a meta-
RL approach that could adapt to different quadruped designs.
Using locomotion priors with DRL has also been investigated
in the past. Structured control networks proposed separation
of linear and nonlinear parts of the control policy and using
sinusoidal activations for the nonlinear part of the policy while
training the locomotion agents [63]. Although these approaches
showed improved performance compared to conventional DRLtraining, the policy architecture did not preserve the structure
of the dynamical system deﬁned by the CPG models. The
work in [63] was extended in [64] to use recurrent neural
networks. Authors in [64]ﬁne-tuned locomotion priors using
an evolutionary approach. Authors in [41], [65] treated CPG
modules as part of the environment making it a black-box while
training the policy, and pre-tuned their CPG modules either
manually or by genetic algorithm. Although the work in [66]
presented a similar approach, the policy architecture requires
this method to execute the actions every time-step of the control
task, and this study was also limited to a single-legged system
for a hopping task.
In this work, we propose to address the limitations of
previous works by developing a hierarchical locomotion policy
architecture that embeds a CPG model (refer Fig. 2). We
embed the locomotion policies, that we call DeepCPG, with
Kuramoto oscillators [47] representing the CPG models. In
this hierarchical setting, the artiﬁcial neural network predicts
the parameters that deﬁne the CPG speciﬁcations, while the
recurrent CPG layer outputs the action commands to the
robots. We show the effectiveness of the proposed approach
for developing the end-to-end control strategies for walking
robots in high-dimensional observation and action spaces.
The rest of this paper is outlined as follows: Section II
brieﬂy discusses the RL basics. Section III provides the
details of the proposed hierarchical policy architecture followed
by Section IV describing the scaling strategy proposed for
DeepCPG. Section V describes the training and deployment
details for the proposed policy. Section VI presents the results
and discussions. The conclusions and future directions are
discussed in Section VII.
II. B ACKGROUND
The standard continuous control RL setting is adopted for
the work reported in this paper and the terminology is adopted
from the standard RL textbook by Sutton et al. [55]. The RL
agent interacts with an environment according to a behavior
policy π. The environment produces state (or observation)
stfor each step t. The agent samples the action ut∼πand
applies it to the environment. For every action, the environment
yields a reward rt. The aim of the RL agent is given by Eq.
(1):
π∗= arg max
πEτ∼π,p s0�
R(τ)�
= arg max
πEτ∼π,p s0��∞
t=0γtrt|π�
(1)
where τ= (s 0, u0, s1, u1, . . .) is the state-action trajectory
sampled using policy π, the initial state is sampled from a
ﬁxed distribution s0∼p s0and0≤γ<1 is the discount factor.
R(τ)is the return of the agent over the complete episode.
One may efﬁciently learn a good policy from state-action-
reward transitions collected by the RL agent by interaction with
the environment. Temporal difference learning, a model-free
RL approach, provides the framework to learn a control policy
based on these collected interactions and by bootstrapping from
the current estimation of the value function [55].
In our approach, we make use of the popular Twin Delayed
Deep Deterministic Policy Gradient (TD3) algorithm [67] forIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 3
Motor CommandsCPG Policy 
 CPG Hidden States 
CPG Goals Neural Policy 
 Primitive Controller 
 Goal Observation Joint Goals Joint States 
External ModulationsDeepCPG Policy
Environment
Neural ModuleCPG Module
Primitive Control ModuleFig. 2. Overview of Hierarchical Policy architecture used for Quadruped Robot
learning in a continuous action space. This algorithm is a
variant of Deep Deterministic Policy Gradient (DDPG) [68].
Using DDPG, a policy πθ(parameterized by θ) and the state-
action value function Qκ(parameterized by κ) are concurrently
learned. The Q-function represents the value of being in a
state stand taking action ut, and it is trained to minimize the
Bellman error over all the sampled transitions from the collected
data given by Eq. (2). The policy πθis trained to maximize
theQκ(st,πθ(st))over all the observed states collected by
environment interactions.
L(s t, ut, st+1)=(Q κ(st, ut)−r t−γQ κ(st+1,πθ(st+1)))2(2)
Although TD3 and DDPG train a deterministic policy, ex-
perience replay is collected by augmenting the actions with
Gaussian Noise. We note that TD3 makes several modiﬁcations
to the DDPG algorithm to yield a robust and stable policy
learning procedure. These modiﬁcations include (1) ensemble
learning over Q-functions (two Q-functions are used), (2) policy
and target networks update less frequently than Q-functions
and (3) addition of noise to target actions, to make it harder for
the control policy to exploit Q-function errors by smoothing
out Q-values along with changes in action. Out of the two
Q-functions Qκ1andQκ2used for training the control policy
in TD3, minimum of the two is chosen as a target value to
prevent the over estimation [69]. For further details regarding
TD3, we refer the readers to [67].
III. A PPROACH
In this work, we propose a hierarchical policy architecture
to connect the DRL-based policies to CPG models. Figure
2 provides an overview of the control architecture using this
policy. In this three-level hierarchy, the high-level policy is a
neural network πθparameterized by θ. The policy πθgenerates
goals for the mid-level CPG policy πCPGthat is based on the
Kuramoto oscillator model [15]. The Kuramoto Model was a
design choice but any other CPG models can be chosen without
loss of generality. Finally, πCPGgenerates goals for the low-
level primitive controller πPCthat generates motor commands
for the robot motor joints. The proposed method enables two
objectives: (1) The neural network enables acting in the CPGparameter space, so the trajectories generated by CPGs remain
smooth and can direct lower-level controllers generating joint
motor commands for the robot safely as illustrated in Fig.1.
(2) The CPG models allow embedding intrinsic primitive
behaviors in the policy, enabling faster learning of the goal-
directed behaviors.
A. Neural Policyπ θ
There are complex interactions between the central nervous
system and the peripheral nervous system in animals. One of
the basic and most important functions of the central nervous
system is processing sensory feedback. Observations sensed
from the environment need to be processed along with the
agent’s desired goals to generate appropriate responses. For the
presented approach, this process is simulated using the neural
policy πθ. This policy takes in the sensor feedback from the
robot and the desired goal to generate the response gCPGthat
is relayed to the CPG policy πCPG, where gCPGrepresents the
parameters that drive the CPG behavior (see Fig. 2). Section
III-B provides further details regarding this part of the system.
B. Central Pattern Generators for Motor Trajectories
We simulate the central pattern generator for motor dynamics
using the Kuramoto Model. Equations (3)-(6)describe this
dynamical system. Although simple, this CPG network model
enables replication of many behaviors observed in vertebrates. It
also allows modeling of inter-oscillator couplings and assumes
the presence of underlying oscillatory mechanisms. Simple
modulation of CPG parameters can lead to emergence of
various useful behaviors as shown in several studies [15]–[17],
[70]. In this work, a single CPG controls a single joint on the
robot.
˙φi=α ωωi+�
i��=iai�αwwii�sin(φ i�−φ i−α ϕϕii�)(3)
¨ai=α a(βa(αAAi−a i)−˙a i)(4)
¨bi=α b(βb(αBBi−bi)−˙bi)(5)
yi=b i+a isin(φ i) (6)IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 4
In Equations (3)-(6), for CPG i,φiis the phase, ωiis the natural
frequency, Aiis the desired amplitude and Biis the desired
offset, aiis the current amplitude and biis the current offset.
wii�≥0 represents the coupling weight between the incoming
connection from CPG i�to CPG i, and ϕii�is the connection
phase bias between the CPGs iandi�. Parameters (αa,βa)
and(αb,βb)are constants, whereas ˙φi,˙ai,˙birepresents the
ﬁrst time derivatives and ¨ai,¨bithe second time derivatives
of respective variables. The external modulation constants
αx≥0∀x∈{w,ϕ,ω, A, B} account for possibly user-deﬁned
external inﬂuence on the parameters that drive the πCPGpolicy.
For instance, αAandαBcould depend on the motor joint limits
on the robot and could be set to match these limits. Parameters
αw,αϕandαωinﬂuence the rate of change of joint angles. We
kept these values constant for the robot in our implementation.
Theπθgenerates gCPG≡{w,ϕ,ω, A, B} to govern the CPG
behavior for the robot. Thus, Ai∈A is then scaled by αA
depending on the system requirement after a prediction from
πθ. The output of the CPGiis given byy i.
The CPG network produces the desired joint trajecto-
ries for all the motor joints on the robot. The parameters
(wii�,ϕii�,ωi, Ai, Bi)deﬁne the behavior for CPG node iin
this network. We assume that each node in the CPG network
inﬂuences every other node symmetrically. Therefore, for a
robot with Nmotor joints, we have a symmetric weight matrix
w={w ii�}∀i, i�∈{1, . . . , N} isN×N with zero diagonal
elements, a phase bias matrix ϕ={ϕ ii�}∀i, i�∈{1, . . . , N}
that is N×N skew-symmetric, and natural frequencies ω,
desired amplitudes A, and desired offsets Beach forming
N-dimensional vectors. The zero diagonals of matrices ϕand
wsignify that CPG nodes do not have recurrent connections
to themselves and the network formed by these nodes is a
bidirectional network. In this bidirectional CPG network, each
CPG node iinﬂuences its neighboring CPG node i�proportional
to the weight wii�. Node ibeing out of phase to node i�by
ϕii�, makes node i�out of phase to node iby−ϕ ii�as a result
of skew-symmetric matrix ϕ. Additionally, the assumption of
a symmetric wand skew-symmetric ϕhalves the total number
of neural outputs predicted byπ θ.
In Fig. 2, the set of parameters {w,ϕ,ω, A, B} are also
referred to as CPG Goals gCPG. The value of gCPGis predicted
using neural policy πθ. Together, gCPGand the Equations (3)-
(6)form the CPG Policy πCPG. This πCPGconsists of time
dependent differential equations and maintains its hidden state
htfor each step tthat consists of {φt,˙φt, at,˙at,¨at, bt,˙bt,¨bt}.
The CPG network outputs the desired motor joint states
gj=y(t) (referred as Joint Goals in Fig. 2) that is a vector
ofNdimensions for a robot with Nmotor joints at each
step t. To reduce notation, we refer to the vector formed by
concatenation of goal and robot observations (g, o) asounless
otherwise stated. It should be noted that gCPGgoverns the
entire architecture of CPG network formed by πCPGbased on
the observed robot state.
C. Primitive Controllerπ PC
As Fig. 2 shows, the outputs action gjofπCPGform the
desired states of the robot joints. The value of gjis relayedto the lower level primitive controller πPCthat generates the
motor commands. Without loss of generality, πPCcan be any
type of controller such as a Proportional-Integral-Derivative
(PID) controller or another neural policy. Policy πPCcan be
designed as an inverse controller similar to the one described
in [60]. To keep our implementation simple, we chose the
PD-controller as πPC. The proportional kpand derivative kd
gains of the PD controller were manually tuned and kept equal
for all the robot joints in all the experiments.
IV. S CALING DEEPCPG P OLICIES
To enable scalability of DeepCPG policies, we propose
to train these policies using a multi-agent RL setup [71].
In this setup, we deﬁne the modular robot control policy
as a set of modular agents thatcooperativelysolve the
desired task. A similar strategy for training multi-legged robot
locomotion policy has been proposed in [18], [72]. The primary
difference between these works and our approach lies in the
trained policy where we learn the neural policy to predict
CPG model parameters. Therefore, a fully cooperative multi-
agent task is formulated such that a team of modular agents
forming the control policy interact with the same environment
(modular robot) to achieve a common goal. For modular
robot conﬁguration, we consider nagents corresponding to
nmodules in the system. Figure 10 visually illustrates a
single robot divided in multiple modules. In this Markov
game, s∈S describes the true state of the modular robot.
Each module m∈{1, . . . , n} consists of its corresponding
action set Umand (partial) observation set Om. A module
muses a policy πθm(stochastic or deterministic) to select
a continuous (or discrete) action um∈U mat each step.
This produces a next state s�according to state transition
function P(s�|s,u) :S×U×S�→[0,1] , where joint action
u∈U≡{U m}∀m∈{1, . . . , n} . Modular agents earn a
joint reward r(s,u) for every action u≡{u 1, . . . , u n}taken
in state s. Thus, this Markov Decision Process (MDP) can
be summarized in a tuple �n,S,U,P, r,O,γ,p S0�. The set
of observations corresponding to each module is contained
inO≡{O m}∀m∈{1, . . . , n} . Each modular agent in the
robot learns policies πθmconditioned on its local observation
correlated with the true state om=O(s, m) where om∈Om
ands∈S (we will use πminstead of πθmto avoid notation
clutter). The distribution of initial states of the system is
pS0:S�→[0,1] . The overall objective is to maximize the
sum of discounted return Rt=�∞
t�=0γt�rt+t�where γis the
discount factor.
Figure 3 provides a schematic overview of this setup for a
modular robot with nmodules. Each module m∈{1, . . . , n}
consists of its corresponding action set Umand (partial)
observation set Om. The module actor πθmcorresponds to
them-th robot module. The output of πθmis sent to πCPG,m
that generates action commands um=g j,mfor the Module-
mof the robot. To enable emergence of coordination in the
modular robot, we deﬁne the observation vector perceived by
each module mwith three distinct components. These three
components comprise of global contextual information og, local
contextual information o(m)private to module m, and inter-
modular contextual information {o(m,m�)}∀m�∈{1, . . . , n} .IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 5
(a) (b)Module Actor Global Critic (c)Module  
Robot BrainModule Module Legends
Inter-module information channels 
Global information channels
Local observations private to module Set of all Inter-module observations from module   to module  Joint goals generated from Fig. 3. (a) Schematic of information sharing across various modules in the modular robot; (b) Actor or Policy for m-th module parameterized by θm; (c)
Centralized critic or action value function form-th module parameterized byκ m
The global information ogconsists of information about the
desired goal of the system.
To train this modular system, a centralized training with
decentralized execution setup is adopted [73], [74]. The
independent training of modular policies may lead to poor
performance [75]. The major issue leading to such performance
degradation is the non-stationarity of environments that prevents
the use of the trajectory history of the agents during training.
Thus, in the regime of centralized training, policy learning can
be stabilized by the use of extra global information during
training, but during execution, each modular agent uses only
its local action-observation history. For centralized training,
the joint policy, denoted by Π, induces a joint action-value
function QΠ
κm(s,u) =E s��
r(s,u) +γE u�∼Π[QΠ
κm(s�,u�)]�
(henceforth, we will replace Qκmwith Qmto avoid notation
clutter). It should be noted that this joint policy is a set
Π≡{π 1, . . . ,π n}. The primary advantage of this action-value
function is that, if we know the actions taken by all modules
along with their true states during training, the environment
is stationary even as the individual policies πmare being
updated [74]. The joint action value function QΠ
mis trained by
minimizing the a mean-squared Bellman loss function, which
estimates how close QΠ
mcomes to satisfying the Bellman
equation. This loss function is given by Eq.(7):
L(κ m)=ER��
QΠ
m(s,u)−�
rm(s,u)+γQΠtarg
m(s�,u�)��2�
(7)
where rm(s,u) is the reward received by module m. In this
case, the collaborative task of all the modules corresponds to a
uniﬁed reward r(s,u) measuring the performance of the robot
as a whole. Thus, it is not necessary to consider a different
reward function for each modular agent. The set of target
policies Πtargwith delayed parameters {θtarg
1, . . . ,θtarg
n}
produces u�≡{u�
1, . . . , u�
n}.QΠtarg
m corresponds to the target
critic with delayed parameters κtarg
m.Ris the replay buffer
containing the transition tuples (s,u,s�, r). We can evaluate
the policy gradient for each modulemaccording to Eq. (8):
∇θmJ(π m) =E R�
∇θmπm(om)∇umQΠ
m(s,u)�
(8)
where QΠ
m(s,u) is a centralized action value function. Action
umis obtained from its current policy πmand remaining
{um�}∀m��=mare obtained from the replay bufferR.Readers should note that we provide the explanation for n
modules in a modular system for the sake of completeness.
In the experimental section, we restrict ourselves to 2agents
(refer to Fig. 10) to demonstrate the proof-of-concept using the
DeepCPG policies in this proposed multi-agent RL framework.
Additionally, for locomotion tasks, the velocity direction in
the body reference frame of Module-1 was used as ogin
observation. The local modular observation of the two modules
considered in the modular robot in Fig. 10 is given by sets o1=�
og, o(1),{o(1,1), o(1,2)}�
ando2=�
og, o(2),{o(2,1), o(2,2)}�
.
We keepo (1,1)=o (2,1)ando (1,2)=o (2,2).
V. T RAINING DEEPCPG P OLICIES
As described in Section II, we use the TD3 algorithm for
training the control policy. To train the DeepCPG policies using
this approach, the policy gradient calculated in TD3 must back-
propagate through actions u=g jgenerated by πCPGfollowed
byπθ. To back propagate, πCPGmust be differentiable in
practice. While the Kuramoto model based CPG network is
differentiable, toﬁnd the analytical solution of the set of
differential equations 3-6, the integration is implemented with
discrete steps of length δt. For backpropagation to work, it can
be shown that, the derivatives of these equations with respect
to the parameters predicted byπ θin Eq. (9) exist:
∂yi
∂wii�,∂yi
∂ϕii�,∂yi
∂ωi,∂yi
∂Ai,∂yi
∂Bi(9)
The complete derivation for Eq. (9) is provided in Appendix.
Given that the derivatives of the πCPGoutputs with respect
to the parameters gCPG∼π θexist, it is possible to train a DRL
policy in end-to-end fashion to predict CPG actions given the
observations from the environment. As discussed in Section II,
we use the TD3 algorithm to train πθby propagating the policy
gradients through πCPG. However, a different RL algorithm
for training the policy can also be used.
As shown in Fig. 4, policy πθis modeled with multiple
heads each corresponding to the CPG parameters gCPG ⊂
{wij,ϕii�,ωi, Ai, Bi}. It should be noted that it is possible to
train πθto predict (1) all the CPG parameters gCPGor (2)
only a subset of gCPGwith the remaining parameters preset
manually to speciﬁc values.
In this actor-critic method, two state-action value functions
Qκ1parameterized by κ1andQκ2parameterized by κ2areIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 6
(a)Layer Layer CPG Weight Head
CPG Phase Bias
HeadCPG Natural
Frequency HeadCPG Offset Head
CPG Amplitude
HeadGoalObservation
(b)Layer Layer Layer 
QLayer Joint Goals (CPG actions)GoalObservationConcatenate 
Fig. 4. Schematic of Neural Network architectures: (a) Neural Policy πθ; (b) Critics Qκ1andQκ2. In the article, the observation vector (g, o) is represented
asoto reduce notation unless otherwise stated. Note: TheHeadsinπ θare the layers in the neural network.
learned as critics. The job of these critics is to criticize the
actions taken by the policy in the given states. The critique or
the values given by the critics takes the form of a Temporal
Difference error (see Eq. 2). The output signals of the critics
drive all learning in both actor (policy) and critic. For critic
architectures, it is possible to estimate the state-action values
using the actions gCPGby higher-level policy πθor actions
gjgenerated by the CPG network πCPG. Based on our
experiments, we observed that a better policy is trained when
we use gjfor action-value prediction. This may be because
the behavior of πCPGis modiﬁed for each step of higher-level
policy πθ. This changing behavior of the lower-level policy
πCPGcreates anon-stationaryenvironment for the higher-level
policy πθ, and old off-policy experiences may exhibit different
transitions conditioned on the same goals. However, critiquing
the actions gjmay alleviate this problem as critics are able to
observe the complete behavior of the actor that consists of πθ
andπ CPG.
As discussed in Section III-B , the weight matrix wis
constrained to be symmetric with a zero diagonal. and the
phase bias matrix ϕis constrained to be skew symmetric for
the CPG network πCPG. Thus, to maintain these constraints,
the actor πθis designed to predict only the off-diagonal (upper
triangular) elements with an offset of 1, i.e., the diagonal
elements are not included in the prediction vector. Thus, for
robot with NCPG nodes in πCPG, the weight and phase bias
predictions areN(N−1)
2dimensional in size. These prediction
vectors are then converted to corresponding wandϕmatrices
of shape N×N inside πCPG. To keep the outputs of πθ
bounded, we use the ‘ tanh ’ activation function for all the
output heads.
y= 0.5�
x(ymax−y min) + (y max+y min)�
(10)
The afﬁne transformation shown in Eq. 10 is applied topredictions of πθto transform them within the respective
bounds, i.e., 0≤w ii�≤1,−1≤ϕ ii�≤1,0≤ω i≤1,0≤
Ai≤1,−1≤B i≤1before passing inπ CPG.
We provide the complete pseudo-code for training the
DeepCPG policy using DRL in Appendix. The input to policy
πθis a sequence of states of length τo, i.e., in Fig. 2 and Fig.
4,s=s t−τ o:t. The CPG policy acts on the environment for
τcsteps for each step ofπ θ.
Scaling DeepCPG policies
Based on the setup described in Section IV, we apply a
multi-agent RL algorithm to learn a scalable control policy for
a modular robot. To that end, we use the DDPG algorithm
customized for a multi-agent setup [74]. In the multi-agent
setup, we chose DDPG over TD3 for training to reduce
the compute requirement. We train the modular robot by
bootstrapping the policies learned during a simpler design
stage of the system, and use it on the modular system with
added design complexity as a result of Module-2 attached to
Module-1. Readers should note that the complexity in these
modular systems is deﬁned in terms of their degrees of freedom.
The system with a higher count of motor joints is considered a
comparatively more complex system than a system with a lower
number of motor joints. An explicit increase in complexity in
terms of motor joints also results in an increase of phenotypic
complexity, where the robot body itself changes as a result
of additional module. The scaling of policy and the critic in
terms of this modular setup is given in Fig. 5.
Deployment
The proposed method uses trained policy πθonce per τcsteps
in robot environment. This achieves speedup during inference
as fewer forward passes are required for a DeepCPG policyIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 7
(a)CPG Parameter
Heads(b)   
     
    Legends
Aggregation junction
CPG Parameter
HeadsFig. 5. Neural Network scaling: (a) Scaling of πθ1when Robot Module-2 is attached to Module-1; (b) Scaling of centralized critic of Module-1 when Robot
Module-2 is attached to Module-1. The same architecture is also used for Module-2 policy and centralized critic. Note: Am
lis a layer lof policy corresponding
to modulemandCm
lis layerlof critic corresponding to modulem.
as compared to a feed-forward multi-layered perceptron based
policy. Given the limited computational power of the real
robot, this skip of τc-steps also facilitates real-time processing.
The pseudo-code for the policy deployment is provided in
Appendix. The two critics learned during the training process
are not needed during the deployment for inferring from the
policy, further lowering the computational cost.
VI. R ESULTS AND DISCUSSION
Experiments were carried out using a physics-based simula-
tion environment as well as physical real-world robots. This
section presents the experimental setup and results for the
evaluation of DeepCPG policies. We evaluate the CPG policies
on intrinsically motivated locomotion task ( VI-B ), Go-to-Goal
task (VI-C) and visual feedback task (VI-F).
A. Experimental Setup - Physics Based Engine
For developing a modular system using legged robots, we
chose the Bullet Physics engine [76]. The quadruped robot
simulation was developed to perform experiments with the
proposed algorithm. Each joint in this robot is a revolute joint.
Figure 6 shows an image and the corresponding schematic of
the quadruped robot used in the evaluation of the proposed
work. This robot is a 12-degree-of-freedom system ( 12active
joints) as indicated in thisﬁgure.
The simulated environments used in the experiments were
wrapped with the OpenAI gym interface [77]. Different
features of the environments developed for testing the proposed
approach are also shown in Fig. 6. PyTorch was used as the
deep learning framework [78]. The simulations were performed
on a machine with an Intel-i7 processor, 16GB RAM, and
NVIDIA RTX 2070. The training of each experiment took
approximately 6hours for observations without images. For
experiments that used visual feedback for the robot, each run
took approximately23hours.To evaluate the proposed approach, we provide the com-
parison with a baseline of feed-forward policy trained using
the TD3-algorithm [67]. The base architectures of both, feed-
forward policy and DeepCPG policy, were kept identical (Note:
base architecture refers to the network architecture before the
output layer). For the feed-forward policy, the output layer
consisted of a fully connected layer of neurons as opposed
to the one in the DeepCPG policy that consisted of the CPG
model. The dimensions of this layer were kept equal to the
action-space dimensions of the robot. Further details about the
hyperparameters used for training the RL policies are given
in Table I. Each experiment was performedﬁve times with
different random seeds for a random number generator. The
resultant statistics are provided in the plots.
Figure 4 provides the architectural details of the neural
networks corresponding to πθ. For the feed-forward neural
network, layer A1consisted of 1024 neurons with ReLU
activation and A2consisted of 512 neurons with ReLU
activation. CPG parameter heads for w,ϕ,ω, A, B are layers
with 512 neurons and ReLU activation. The input dimensions
of the network depend on the observation space dimensions of
the robot and the output dimensions depend on the architecture
ofπCPGand the action space of the robot as discussed in
Section V. For the Critic architecture, C1andC2each consisted
of 1024 neurons with ReLU activation, C3consisted of 512
neurons with ReLU activation, followed by a linear layer C4
with 512 neurons. The critics output a scalar value. It should be
noted that for training the actor with visual feedback (images
as observations), actor and critic architectures were modiﬁed to
include 2 convolutional layers before A1andC1, respectively.
The image size used during training was 32×32 . These images
were converted to grayscale before feeding them to the neural
networks. The images for τosteps were stacked and passed
through the two convolutional layers and thenﬂattened and
concatenated with other observations (if any) to pass through
theﬁrst linear layer of 1024 neurons, shown in Fig. 4, forIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 8
SegmentTorsoActive Joint
Fixed Joint
Hip Knee Ankle Foot Shin Thigh
Robot VisionFront
Broken Leg
(a) (b) (c) (d)
Fig. 6. Quadruped robot: (a) Bullet Physics GUI; (b) Schematic; (c) Fault in front-left limb; (d) Uneven terrain and with simulated visual feedback.
the actor and both the critics. Theﬁrst 2D-convolutional layer
consisted of τoinput channels and 10output channels. For the
second 2D-convolutional layer, 10input channels, 15output
channels were used. The kernel size of 3and stride of 1were
used for both the layers.
TABLE I
HYPERPARAMETERS FOR TRAINING DEEPCPG POLICIES
Parameter Value
Learning rateηfor actor and critics 2×10−4
Neural network optimizer Adam [79]
Adam parameters(β 1,β2) (0.9,0.999)
Discount factorγ 0.95
Max. episode lengthT 2000
Batch size|B| 64
Replay buffer size|R| 1×106
Gradient norm clipping threshold 2.0
Babbling stepsτ b 10000
Update stepsτ update 1
Policy update delayτ delay 2
CPG Policy stepsτ c 5
Observation stepsτ o 5
{αw,αϕ,αω,αA,αB} {600,π,20,0.8,0.2}
B. Intrinsic Motivation Task
Good exploration enables the policy to discover various
behaviors. In practice, to enable this, the learning agent should
leverage information gained from environment dynamics. To
encourage this outcome, we deﬁne an intrinsic reward function
in equation (11) that encourages the policy to learn the
locomotion behavior. The agent is rewarded for successfully
moving the robot inX−Yplane.
rt=cv||v(x,y),t||2−cϑ||ϑt||2−cz||zt||2−cj||sjointt||2+cb(11)
In Eq.11, v(x,y),t denotes the current velocity of the robot
along xandydirection. ϑtis the angular velocity of the robot.
sjointtrepresents the vector of joint angles of the robot at time
t.ztrepresents the robot height above the ground at time t. The
term with sjointtimplicitly regularizes actions by imposing a
penalty on the robot joint angles. The last term cbcontributes to
the bonus value for the robot for staying alive. The coefﬁcients
c(·)>0∀{v,ϑ, z, b, j} . It should be noted that Eq.11 does not
contribute to a goal directed behavior of any kind in the learned
policy. In the experiments, cv= 2.0 ,cb= 4.0 ,cϑ= 0.5 ,
cz= 5.0 ,cj= 10−3. Figure 7-(a) provides the plot of episodereturns over the training of the robot. From this plot, it can be
observed that the DeepCPG policy performance is on par with
the feedforward policy trained using TD3. The advantage of
the CPG priors in the locomotion is evident from the results
in Section VI-C , where these policies areﬁne-tuned for the
goal-directed task.
C. Go-to-Goal Task
The policies that were trained in Section VI-B wereﬁne-
tuned to learn a goal-reaching behavior with the modiﬁcation
to the reward in Eq. (11). The weights that were learned for the
policies for the task in Section VI-B were transferred for this
downstream task. Equation (12) provides the updated reward
function used for learning a go-to-goal behavior.
rt=c e(v(x,y),t ·ˆeg) +c v||v(x,y),t||2−cϑ||ϑt||2
−cz||zt||2−cj||sjointt||2+cb(12)
In Equation (12),ˆegis the unit vector pointing towards
goal (xg, yg)from the robot position (xt, yt)and c(·)>
0∀{e, v,ϑ, z, j, b} . For the experiments, ce= 5.0 . The values
of the remaining coefﬁcients are provided in Section VI-B.
The advantage of the DeepCPG policy is visible from the
training plots in Fig. 7-(b) in terms of sample efﬁciency. It
was able to learn the go-to-goal behavior faster as compared to
the baseline policy. This could be attributed to the behavioral
priors in the DeepCPG policy as a result ofπ CPG.
D. Ablation Study: Fault Tolerance
To investigate the fault-tolerant behavior of the proposed
policy, we introduced a fault in the robot. This environment
change is shown in Fig. 6-(c). One of the legs in the robot
was broken. The reward function was updated from Eq. (11)
to Eq. (13). In Eq. (13), the robot was evaluated based on its
performance to walk along the x-axis, so its velocity along
x-axis v(x),twas rewarded. All the coefﬁcient values were kept
as deﬁned in Section VI-B.
rt=cv||v(x),t||2−cϑ||ϑt||2−cz||zt||2−cj||sjointt||2+cb(13)
Figure 7-(c) shows the episode return plots for the policies from
Section VI-B ﬁne-tuned for the system with the broken-leg. The
DeepCPG policy shows comparatively better performance than
the feed-forward policy. As compared to the system without anyIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 10
oscillators in Kuramoto CPG nodes contained in πCPG. The
plot shows a corresponding response of the trained DeepCPG
policy with the variation of this value. The policy was trained
using αw= 600 . As the CPG values are not able to synchronize
when α<600 , the policy performance was observed to degrade.
Forα>600 , the trained DeepCPG policy showed robustness
and its performance did not suffer.
Figure 8-(c) shows the results for variation of αϕin Eq. (3).
The trained policy performance was observed to be sensitive
to variation of αϕ. When αϕ= 0, the phase bias of each CPG
node of the πCPGbecomes zero, the robot was not able to
perform well.
Figure 8-(d) shows the results for variation of αAin Eq. (4).
It should be noted that we maintained a constraint of αA+
αB= 1 to ensure max(α AAi) + max(α BBi)≤max(s joint i)
where max(s joint i)corresponds to the limits of joint ion the
robot and AiandBi, respectively, correspond to the desired
amplitude and desired offset of the CPG node corresponding to
that joint. This constrains the joint movement resulting from a
CPG node to be within the joint’s physical limits. From the plot
shown in Fig. 8-(d), the robot achieved less reward for lower
values of αA. This was the result of the smaller amplitudes of
a robot’s gait. On the other hand, an interesting observation
is for αA= 1 where the robot performance degrades. We
observed that the offset is an important component in DeepCPG
Policy. When αA= 1,αB= 0 resulting in the performance
degradation.
Figure 8-(e) shows the results for variation of αωin Eq.
(3). This parameter inﬂuences the oscillation frequency of the
CPG nodes in DeepCPG policies. The policy showed robust
behavior to some variation of αω. The higher the value of
αω, the higher the oscillation frequency of CPG nodes. This
increase in frequency is reﬂected in higher robot speeds and a
resultant increase in episode return. At high values, αω= 100
andαω= 120 , the policy performance was degraded. This
may be associated with the non-stationarity in the observation
space with high αω. Although the trained policy was robust to
some variations of this parameter, the degrading performance
could be the result of distribution drift in the observations
space whenα ωincreases.
Overall, it can be observed from the parameter study
conducted in this section that the DeepCPG policy is robust to
a wide range of parameter changes during deployment even
when these parameters are not shown to the policy during the
training phase.
F . Visual Feedback Task
We also evaluated the DeepCPG policy on the environment
designed with high-dimensional image observations. Two cases
were tested for DeepCPG and feed-forward policies each: (1)
The robot was provided with visual feedback only, (2) The
robot was provided with visual feedback along with input
from the proprioceptive touch sensors on its feet. The sample
image of the environment is shown in Fig. 6. The terrain
was randomized in every episode. This randomization was
introduced to enable the generalization of the quadruped to
walk on rough terrain. The terrain height was randomly sampledfrom the range [0,0.1×h r]where hrrepresents the robot
height.
The robot was trained with the reward function deﬁned in Eq.
(13) to walk along the x-axis. Figure 9-(a) shows the results
of the experiments with visual feedback to the robot. From
these results, it can be observed that the learned DeepCPG
policy was able achieve comparatively higher reward than the
feed-forward policy.
In general, it was observed that the feed-forward actor
showed comparatively poor performance than DeepCPG for any
environment perturbation. The feed-forward actor consistently
demonstrated lower asymptotic performance, especially in the
case of visual feedback with no proprioceptive feet touch
sensors (see Fig. 9-(a)).
G. Energy Analysis
This section presents the results for the energy analysis of the
walking robot described in the previous sections. We compare
the results generated from the feed-forward MLP policy with
the DeepCPG policy. Both the policies were trained with the
reward function Eq. (11) in Section VI-B.
Figure 9-(b) provides the joint trajectories for the robot
trained using these policies recorded for a span of 3 seconds
arbitrarily from the simulation. It should be noted that these
trajectories are plotted using the joint angles of the robot
simulation and do not correspond to the motor commands sent
from the neural network. From thisﬁgure, it can be observed
that the DeepCPG policy enables the generation of smooth joint
trajectories. This is attributed to the CPG dynamical system
embedded as an output neural layer. This layer can predict in
the trajectory space of the Kuramoto Model of CPGs and thus
the joint trajectories generated from these commands appear
smooth. On the other hand, for Feed-forward MLP, the neural
network predicts the values of the motor commands directly for
every iteration. As a result of this, the joint trajectories appear
non-smooth. The average energy consumed by all the motor
TABLE II
ENERGY EXPENDED BY THE QUADRUPED ROBOT JOINTS
Policy Value t T
FF-MLP 486.78±68.92J 43±13.24ms 48±13s
DeepCPG 253.75±20.16J 46±26.19ms 41±22s
joints on the robot in the simulation is shown in Table II where
Column tdenotes time taken by the policies for each iteration
(in milliseconds) and Column Tdenotes the time taken for task
completion (in seconds). We estimate these values by using the
equation Work=�
joint�
tTorque joint t·∂s jointt, where
Torque joint tis the torque applied on a motor joint at timet
and∂sjointtis the change of joint state from time ttot+ 1 .
This statistic was evaluated based on the data gathered over
the complete episode of 2,500 steps for 5episodes. Given
the non-smooth nature of the joint trajectories generated from
Feed-Forward MLP policy, the energy expended in driving
the robot motors is almost twice the energy consumed by the
DeepCPG policy. Additionally, it can be observed that the
trajectories produced by DeepCPG are smoother (see Fig. 9-
(b)). In the case of the real robots, this could also contribute toIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 11
0.000.250.500.751.001.251.501.752.00
Iterations 1e620020406080Episode Returns1e2
DeepCPG+vision
DeepCPG+vision+feet
TD3+vision
TD3+vision+feet0.00.51.01.52.02.53.020020
Joint angles (deg)Hip-0
Hip-1
Hip-2
Hip-3
0.00.51.01.52.02.53.0Time (sec.)201001020
Joint angles (deg)(a)(b)
Fig. 9. (a) Training plots for the tasks with visual feedback: (vision) Robot has access to only visual feedback, (vision+feet) Robot gets feedback from vision
and feet touch sensors, (TD3) Feed-Forward policy trained using TD3, (DeepCPG) DeepCPG policy; (b) Trajectory of the Hip Joints of the quadruped robot:
(Top) Trajectories observed for the Feed-forward MLP policy, (Bottom) Trajectories observed for the DeepCPG policy. Hip-0 is Front Right, Hip-1 is Front
Left, Hip-2 is Back Right and Hip-3 is Back left.
Module-1Module-2
(a) (c)
(b)(d)Fig. 10. (a) Physics engine based model: (left) a single robot, (right) a robot with two modules; (b) CAD model: (left) Robot used in Sim2Real experiment,
(middle) Single leg along with servos, (right) Leg attached to robot body; (c) Individual modules of real robot; (d) Connected modules of real robot.
robot safety since DeepCPG policies would not perform any
unbounded actions that could damage the robot actuators.
H. Scalability and Simulation to Real-World Transfer of
DeepCPG Policies
In this section, results of the scalability study and simulation
of the real-world (sim2real) transfer of the DeepCPG policies
are presented. Figure 10 shows the snapshots of the simulation
model and the corresponding experimental setup used for the
sim2real experiments.
The images of the CAD model of the robot used in the
simulation-to-real-world transfer experiments with the trainedDeepCPG policies are shown in Fig. 10-(b). These robots are
custom built and the experiments were performed under a
motion capture system. Each of these quadrupeds had two
motor joints per leg. Thus, a single quadruped module have
a total of 8degrees of freedom. Raspberry-Pi 4 Model B is
used as the on-board computing platform for these robots
[80]. The robot joints are connected to a PWM breakout
board that handles sending commands to multiple servo motors
simultaneously. This is all housed within a shell that gives
the robot a turtle-like appearance. These robots can connect
and disconnect with each other using an electromagnet. A hall
effect sensor is used to detect the connection of one moduleIEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 12
to another. The connectors are designed with a conical shape
to allow for tolerance while connecting (highlighted in yellow
in Fig. 10-(b)-(left)). The corresponding 3D-Printed quadruped
robot modules using this CAD model are shown in Fig. 10-(c)
and Fig. 10-(d).
For the DeepCPG policies trained for sim2real experiments,
we did not use touch sensors on the robot feet. The feedback
available to the robot consisted of the robot joint angles, joint
angle velocities, linear velocity and angular velocity. The inter-
modular information shared across each module consisted of
values of joint angles measured for each module. As there are
eight joints on each module, this information consists of eight
joint angle values. The global contextual information shared
with Module- 2from Module- 1consisted of robot velocity
direction in body frame of reference.
The training plots for the modular system in Fig. 10-(a)-
(right) with 16degrees of freedom are provided in Fig. 11-(a).
The policies were trained using the reward function in Eq. (13).
This plot compares three training routines. It should be noted
that, for a fair comparison, the architectures of the control
policies were kept identical in each training routine and only
the neural network πθm∀m∈{1,2} parameter initialization
strategies were varied:
Routine-1 : Training the system with Module-1 and Module-
2 from scratch where the modular policies were initialized
randomly and trained together as a single giant policy. This is
referred to as “Not-Mod" in the Fig.11-(a).
Routine-2 : First, a policy for Module-1 was trained using
the reward function Eq. (13) and then transferred to the
corresponding module in the new system formed by connecting
Module-1 and Module-2. The Module-2 policy weights were
initialized randomly. The multi-agent RL setup described in
Section V was used to train these control policies. This is
referred to as “Mod-Rand" in the Fig.11-(a).
Routine-3 : First, a policy for Module-1 was trained using the
reward function Eq. (13) and then it was used to initialize
the weights of each module in the new system formed by
connecting Module-1 and Module-2. The multi-agent RL setup
described in Section V was used to train these control policies.
This is referred to as “Mod" in the Fig.11-(a).
Based on the comparison of all these training routines, it was
observed that Routine-3 is the most sample efﬁcient. In this
routine, the modular policies were able to take advantage of
two priors when the transfer from an individual module stage
to a connected module stage occured. Theﬁrst prior was as a
result of movement primitives embedded in DeepCPG policies.
The second prior is due to trained quadruped policy used for
parameter initialization in the design complexiﬁcation stage. It
can be observed clearly that modular design complexiﬁcation
with transfer of weights and additive learning enabled effective
and efﬁcient scaling of the DeepCPG policies to robots of
increasing complexity.
Given the Kuramoto model of CPG embedded in the network,
the DeepCPG policies were able to successfully transfer to
the real robot without requiring any furtherﬁne-tuning of the
neural network weights. We performed two experiments with
the provided setup:
Experiment 1 - A DeepCPG policy was trained for a singlequadruped robot for waypoint navigation using the reward
function in Eq. (12). The policy was trained in simulation and
transferred to the real robot. The trajectory followed by the
real robot Module-1 is shown in Fig. 11-(b).
Experiment 2 - In this experiment, the robot Module-1 and
Module-2 were connected together. The DeepCPG policy was
trained to let the connected modules walk in a straight line.
The connection between Module-2 and Module-1 is detected
by a hall-effect sensor. When the connection is detected, the
modular policy trained using Routine-3 described above was
activated.
Based on these experiments, it was also observed that the
learned control policies both for individual robot and the
modular robots were able to successfully work in the real
world. Additionally, the modular policies also enabled smooth
switching of behavior from a single module to a system with
two modules and effectively synchronize the leg movements
after the connection as decribed in Experiment 2 . DeepCPG
policy also ensured smooth joint trajectories in the robots. This
implicitly constrains the robots to follow a smooth and safe
trajectories using its actuators.
VII. C ONCLUSIONS AND FUTURE WORK
We presented a developmental RL-based approach for learn-
ing robot locomotion that allows the scalable construction of
more complex robots by a process of modular complexiﬁcation
and transfer of prior learning. This approach was tested in
various simulated environments as well as on a real-world
experimental platform. The results show the advantages offered
by the behavioral priors introduced in the actor-networks as
central pattern generator models. DeepCPG policies were able
to show sample efﬁcient learning of various behavioral policies
with different sensor modalities. As demonstrated by the results
presented for various sensor modalities, DeepCPG policies
enable end-to-end learning even in the case of high-dimensional
sensor spaces such as visual feedback.
The hierarchical DeepCPG policy also incorporates ideas
from thedual process theoryof human intelligence [81],
[82]. In the DeepCPG policy, the lower level CPG policy
πCPG can be thought of as System1 which is a fast,
unconscious, and automatic mode of behavior, while the higher-
level policy πθcan be considered as System2 which is a slow,
conscious and rule-based model of reasoning developed as a
result of knowledge gained by the robot from its surrounding
environment. In other words, the DeepCPG actor employs the
idea ofthinking fast and slowwhile learning a behavioral
policy.
Closely related work to the proposed work includes the
work by Schilling et al. [18] investigating learning of various
decentralized control policies for aﬁxed robot morphology.
Huang et al. [83] also presented a very interesting message-
passing approach to train a generalized decentralized control
policy for a wide variety of agent morphologies. This work
showed that an identical modular network could be shared
across various actuators in the robot morphology to generate
locomotion behaviors. The results using DeepCPG also align
with the conclusions drawn from the studies in [18], [83]IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 13
1.000.750.500.250.000.250.50
x (meters)0.80.60.40.20.00.20.40.6
y (meters)
trajectory
waypoint0.50.00.51.0x (meters)1.51.00.50.00.51.0
y (meters)
Module-2
Module-1start start
(a)(b)(c)
0.00.10.20.30.40.5
Iterations 1e620020406080100120Episode Returns1e2Mod
Mod-Rand
Not-Mod
Fig. 11. (a)Training plots for a modular walking robot when extended from having 4 legs to 8 legs: [Not-Mod] Policy was trained for 8-legged robot from
random initialization; [Mod-Rand] Policy was initialized randomly for the added legs when the 4-legged robot was extended to an 8-legged robot; [Mod]
Policy was initialized with the weights from the quadruped module when the robot was extended from 4-legged to 8-legged, (b) Trajectory followed by Robot
Module-1 for waypoint navigation task, (c) Trajectories followed by Module-1 and Module-2 while walking in the straight line after connecting with each other.
that decentralized training and biological inspiration are quite
helpful for learning. To summarize, decentralized training of
DeepCPG policies corroborates well with the observations from
these works that modular policies could be trained faster. In
addition to that, our work also showed that the interaction
of sensory modalities and movement primitives enables faster
learning of robust locomotion for a high degree of freedom
system.
From the perspective of developmental robotics, the ap-
proach proposed to train the behavioral policies for modular
systems also shows the efﬁcacy of bootstrapping more complex
intelligent systems from simpler ones based on biological
principles. The gradual complexiﬁcation with modular policies
along with phenotypic development in the robot enables non-
trivial sensorimotor integration on a real robotic platform.
We used the Kuramoto model as the basis of CPGs. One
can replace this model with other models without the loss of
generality. The addition of different nonlinear terms in the CPG
models could also enable the emergence of diverse behaviors
like jumping with appropriate environmental scenarios. The
proposed DeepCPG based policies could also be used with
complex robot architectures like humanoid robots [84] and
soft robots [85] for learning different behaviors. We believe
these could be interesting directions to explore as part of future
work.
REFERENCES
[1]D. M. Wilson, “The central nervous control ofﬂight in a locust,”Journal
of Experimental Biology, vol. 38, no. 2, pp. 471–490, 1961.
[2]P. S. Stein, D. G. Stuart, S. Grillner, and A. I. Selverston,Neurons,
networks, and motor behavior. MIT press, 1999.
[3] H. Hultborn and J. B. Nielsen, “Spinal control of locomotion–from cat
to man,”Acta Physiologica, vol. 189, no. 2, pp. 111–121, 2007.
[4]I. R. Popescu and W. N. Frost, “Highly dissimilar behaviors mediated
by a multifunctional network in the marine mollusk tritonia diomedea,”
Journal of Neuroscience, vol. 22, no. 5, pp. 1985–1993, 2002.
[5]E. Marder and D. Bucher, “Central pattern generators and the control of
rhythmic movements,”Current biology, vol. 11, no. 23, pp. R986–R996,
2001.
[6]V . Edgerton, S. Grillner, A. Sjöström, and P. Zangger, “Central generation
of locomotion in vertebrates,” inNeural control of locomotion. Springer,
1976, pp. 439–464.[7]D. E. Koditschek, R. J. Full, and M. Buehler, “Mechanical aspects of
legged locomotion control,”Arthropod structure & development, vol. 33,
no. 3, pp. 251–272, 2004.
[8]C. F. Herreid,Locomotion and energetics in arthropods. Springer
Science & Business Media, 2012.
[9]M. H. Dickinson, C. T. Farley, R. J. Full, M. A. R. Koehl,
R. Kram, and S. Lehman, “How animals move: An integrative view,”
Science, vol. 288, no. 5463, pp. 100–106, 2000. [Online]. Available:
https://www.science.org/doi/abs/10.1126/science.288.5463.100
[10] K. Pearson, “Central programming and reﬂex control of walking in the
cockroach,”Journal of experimental biology, vol. 56, no. 1, pp. 173–193,
1972.
[11] S. S. Bidaye, T. Bockemühl, and A. Büschges, “Six-legged walking in
insects: How cpgs, peripheral feedback, and descending signals generate
coordinated and adaptive motor rhythms,”Journal of Neurophysiology,
vol. 119, no. 2, pp. 459–475, 2018.
[12] B. D. DeAngelis, J. A. Zavatone-Veth, and D. A. Clark, “The manifold
structure of limb coordination in walking drosophila,”Elife, vol. 8, p.
e46409, 2019.
[13] T. G. Brown, “The intrinsic factors in the act of progression in the
mammal,”Proceedings of the Royal Society of London. Series B,
containing papers of a biological character, vol. 84, no. 572, pp. 308–319,
1911.
[14] T. G. Brown, “The factors in rhythmic activity of the nervous system,”
Proceedings of the Royal Society of London. Series B, Containing Papers
of a Biological Character, vol. 85, no. 579, pp. 278–289, 1912.
[15] A. J. Ijspeert, A. Crespi, D. Ryczko, and J.-M. Cabelguen, “From
swimming to walking with a salamander robot driven by a spinal cord
model,”science, vol. 315, no. 5817, pp. 1416–1420, 2007.
[16] P. Liljebäck, K. Y. Pettersen, Ø. Stavdahl, and J. T. Gravdahl,Snake
robots: modelling, mechatronics, and control. Springer Science &
Business Media, 2012.
[17] R. Thandiackal, K. Melo, L. Paez, J. Herault, T. Kano, K. Akiyama,
F. Boyer, D. Ryczko, A. Ishiguro, and A. J. Ijspeert, “Emergence of
robust self-organized undulatory swimming based on local hydrodynamic
force sensing,”Science Robotics, vol. 6, no. 57, 2021. [Online].
Available: https://robotics.sciencemag.org/content/6/57/eabf6354
[18] M. Schilling, A. Melnik, F. W. Ohl, H. J. Ritter, and B. Hammer,
“Decentralized control and local information for robust and adaptive
decentralized deep reinforcement learning,”Neural Networks, vol. 144,
pp. 699–725, 2021.
[19] P. Eckert, A. Spröwitz, H. Witte, and A. J. Ijspeert, “Comparing the effect
of different spine and leg designs for a small bounding quadruped robot,”
in2015 IEEE International Conference on Robotics and Automation
(ICRA). IEEE, 2015, pp. 3128–3133.
[20] F. Herrero-Carrón, F. Rodríguez, and P. Varona, “Bio-inspired design
strategies for central pattern generator control in modular robotics,”
Bioinspiration & Biomimetics, vol. 6, no. 1, p. 016006, 2011.
[21] A. Schneider, J. Paskarbeit, M. Schaeffersmann, and J. Schmitz, “Hector,
a new hexapod robot platform with increased mobility-control approach,IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 14
design and communication,” inAdvances in Autonomous Mini Robots.
Springer, 2012, pp. 249–264.
[22] P. Billeschou, N. N. Bijma, L. B. Larsen, S. N. Gorb, J. C. Larsen, and
P. Manoonpong, “Framework for developing bio-inspired morphologies
for walking robots,”Applied Sciences, vol. 10, no. 19, p. 6986, 2020.
[23] R. Siddall, G. Byrnes, R. J. Full, and A. Jusuﬁ, “Tails stabilize landing
of gliding geckos crashing head-ﬁrst into tree trunks,”Communications
Biology, vol. 4, no. 1, p. 1020, Sep 2021. [Online]. Available:
https://doi.org/10.1038/s42003-021-02378-6
[24] A. Ijspeert, J. Nakanishi, and S. Schaal, “Learning attractor landscapes for
learning motor primitives,” inAdvances in Neural Information Processing
Systems, S. Becker, S. Thrun, and K. Obermayer, Eds., vol. 15. MIT
Press, 2003.
[25] J. Nordmoen, T. F. Nygaard, K. O. Ellefsen, and K. Glette, “Evolved
embodied phase coordination enables robust quadruped robot locomotion,”
inProceedings of the Genetic and Evolutionary Computation Conference,
2019, pp. 133–141.
[26] E. D. Tytell and J. H. Long, “Biorobotic insights into neuromechanical
coordination of undulatory swimming,”Science Robotics, vol. 6, no. 57,
2021.
[27] A. Pikovsky, M. Rosenblum, and J. Kurths,Synchronization: A Universal
Concept in Nonlinear Sciences, ser. Cambridge Nonlinear Science Series.
Cambridge University Press, 2001.
[28] G. L. Liu, M. K. Habib, K. Watanabe, and K. Izumi, “Central pattern
generators based on matsuoka oscillators for the locomotion of biped
robots,”Artiﬁcial Life and Robotics, vol. 12, no. 1-2, pp. 264–269, 2008.
[29] B. Lim, J. Lee, J. Kim, M. Lee, H. Kwak, S. Kwon, H. Lee, W. Kwon,
and K. Roh, “Optimal gait primitives for dynamic bipedal locomotion,”
in2012 IEEE/RSJ International Conference on Intelligent Robots and
Systems. IEEE, 2012, pp. 4013–4018.
[30] D. Clever, M. Harant, K. Mombaur, M. Naveau, O. Stasse, and D. Endres,
“COCoMoPL: A novel approach for humanoid walking generation
combining optimal control, movement primitives and learning and its
transfer to the real robot hrp-2,”IEEE Robotics and Automation Letters,
vol. 2, no. 2, pp. 977–984, 2017.
[31] C. Mastalli, I. Havoutis, A. W. Winkler, D. G. Caldwell, and C. Semini,
“On-line and on-board planning and perception for quadrupedal loco-
motion,” in2015 IEEE International Conference on Technologies for
Practical Robot Applications (TePRA). IEEE, 2015, pp. 1–7.
[32] M. Duarte, J. Gomes, S. M. Oliveira, and A. L. Christensen, “Evolution
of repertoire-based control for robots with complex locomotor systems,”
IEEE Transactions on Evolutionary Computation, vol. 22, no. 2, pp.
314–328, 2017.
[33] C. Yang, C. Chen, N. Wang, Z. Ju, J. Fu, and M. Wang, “Biologically
inspired motion modeling and neural control for robot learning from
demonstrations,”IEEE Transactions on Cognitive and Developmental
Systems, vol. 11, no. 2, pp. 281–291, 2019.
[34] G. Bellegarda and A. Ijspeert, “CPG-RL: Learning central pattern
generators for quadruped locomotion,”IEEE Robotics and Automation
Letters, vol. 7, no. 4, pp. 12 547–12 554, 2022.
[35] A. L. Hodgkin and A. F. Huxley, “A quantitative description of membrane
current and its application to conduction and excitation in nerve,”The
Journal of physiology, vol. 117, no. 4, pp. 500–544, 1952.
[36] L. Abbott and T. B. Kepler, “Model neurons: from Hodgkin-Huxley to
Hopﬁeld,” inStatistical mechanics of neural networks. Springer, 1990,
pp. 5–18.
[37] P. Wen, X. Linsen, F. Baolin, and W. Zhong, “CPG control model of
snake-like robot parameters of optimization based on GA,” in2015 IEEE
International Conference on Robotics and Biomimetics (ROBIO). IEEE,
2015, pp. 1944–1949.
[38] D. Blanchard, K. Aihara, and T. Levi, “Snake robot controlled by
biomimetic cpgs,”Journal of Robotics, Networking and Artiﬁcial Life,
vol. 5, no. 4, pp. 253–256, 2019.
[39] A. Hunt, M. Schmidt, M. Fischer, and R. D. Quinn, “Neuromechanical
simulation of an inter-leg controller for tetrapod coordination,” in
Conference on Biomimetic and Biohybrid Systems. Springer, 2014,
pp. 142–153.
[40] K. Matsuoka, “Analysis of a neural oscillator,”Biological Cybernetics,
vol. 104, no. 4-5, pp. 297–304, 2011.
[41] X. Liu, R. Gasoto, Z. Jiang, C. Onal, and J. Fu, “Learning to locomote
with artiﬁcial neural-network and CPG-based control in a soft snake
robot,” in2020 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). IEEE, 2020, pp. 7758–7765.
[42] M. Cartwbight, “Balthazar van der pol,”Journal of the London Mathe-
matical Society, vol. 1, no. 3, pp. 367–376, 1960.[43] A. H. Nayfeh and B. Balachandran,Applied nonlinear dynamics:
analytical, computational, and experimental methods. John Wiley
& Sons, 2008.
[44] P. Veskos and Y. Demiris, “Developmental acquisition of entrainment
skills in robot swinging using van der Pol oscillators,” inFifth
International Workshop on Epigenetic Robotics: Modeling Cognitive
Development in Robotic Systems. Lund University Cognitive Studies,
2005, pp. 87–93.
[45] G. Wang, X. Chen, and S.-K. Han, “Central pattern generator and
feedforward neural network-based self-adaptive gait control for a crab-
like robot locomoting on complex terrain under two reﬂex mechanisms,”
International Journal of Advanced Robotic Systems, vol. 14, no. 4, p.
1729881417723440, 2017.
[46] Y. Zhu, Y. Wu, Q. Liu, T. Guo, R. Qin, and J. Hui, “A backward
control based on σ-Hopf oscillator with decoupled parameters for smooth
locomotion of bio-inspired legged robot,”Robotics and Autonomous
Systems, vol. 106, pp. 165–178, 2018.
[47] Y. Kuramoto,Chemical oscillations, waves, and turbulence. Courier
Corporation, 2003.
[48] M. Breakspear, S. Heitmann, and A. Daffertshofer, “Generative models
of cortical oscillations: neurobiological implications of the kuramoto
model,”Frontiers in human neuroscience, vol. 4, p. 190, 2010.
[49] C. Liu, Y . Chen, J. Zhang, and Q. Chen, “CPG driven locomotion control
of quadruped robot,” in2009 IEEE International Conference on Systems,
Man and Cybernetics. IEEE, 2009, pp. 2368–2373.
[50] M. Schilling, T. Hoinville, J. Schmitz, and H. Cruse, “Walknet, a bio-
inspired controller for hexapod walking,”Biological cybernetics, vol.
107, no. 4, pp. 397–419, 2013.
[51] S. Schaal, J. Peters, J. Nakanishi, and A. Ijspeert, “Control, planning,
learning, and imitation with dynamic movement primitives,” inWorkshop
on Bilateral Paradigms on Humans and Humanoids: IEEE International
Conference on Intelligent Robots and Systems (IROS 2003), 2003, pp.
1–21.
[52] J. Ding, X. Xiao, N. Tsagarakis, and Y. Huang, “Robust gait synthesis
combining constrained optimization and imitation learning,” in2020
IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS). IEEE, 2020, pp. 3473–3480.
[53] A. Reske, J. Carius, Y. Ma, F. Farshidian, and M. Hutter, “Imitation
learning from mpc for quadrupedal multi-gait control,” in2021 IEEE
International Conference on Robotics and Automation (ICRA). IEEE,
2021, pp. 5014–5020.
[54] R. A. Brooks, “A robot that walks; emergent behaviors from a carefully
evolved network,”Neural computation, vol. 1, no. 2, pp. 253–262, 1989.
[55] R. S. Sutton and A. G. Barto,Reinforcement learning: An introduction.
MIT press, 2018.
[56] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner,
B. Upcroft, P. Abbeel, W. Burgard, M. Milfordet al., “The limits and
potentials of deep learning for robotics,”The International Journal of
Robotics Research, vol. 37, no. 4-5, pp. 405–420, 2018.
[57] J. Ibarz, J. Tan, C. Finn, M. Kalakrishnan, P. Pastor, and S. Levine, “How
to train your robot with deep reinforcement learning: lessons we have
learned,”The International Journal of Robotics Research, vol. 40, no.
4-5, pp. 698–721, 2021.
[58] S. Schaal, “Dynamic movement primitives – a framework for motor
control in humans and humanoid robotics,” inAdaptive motion of animals
and machines. Springer, 2006, pp. 261–280.
[59] A. Gams, A. Ude, J. Morimotoet al., “Deep encoder-decoder networks
for mapping raw images to dynamic movement primitives,” in2018 IEEE
International Conference on Robotics and Automation (ICRA). IEEE,
2018, pp. 5863–5868.
[60] S. Bahl, M. Mukadam, A. Gupta, and D. Pathak, “Neural dynamic
policies for end-to-end sensorimotor learning,” inNeurIPS, 2020.
[61] T. Miki, J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,
“Learning robust perceptive locomotion for quadrupedal robots in the
wild,”Science Robotics, vol. 7, no. 62, p. eabk2822, 2022.
[62] Á. Belmonte-Baeza, J. Lee, G. Valsecchi, and M. Hutter, “Meta
reinforcement learning for optimal design of legged robots,”IEEE
Robotics and Automation Letters, 2022.
[63] M. Srouji, J. Zhang, and R. Salakhutdinov, “Structured control nets for
deep reinforcement learning,” inInternational Conference on Machine
Learning. PMLR, 2018, pp. 4742–4751.
[64] V. Liu, A. Adeniji, N. Lee, and J. Zhao, “Recurrent control nets as
central pattern generators for deep reinforcement learning,”SURJ: The
Stanford Undergraduate Research Journal, vol. 18, no. 1, pp. 51–55,
2019.
[65] G. Endo, J. Morimoto, T. Matsubara, J. Nakanishi, and G. Cheng,
“Learning CPG-based biped locomotion with a policy gradient method:IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 15
Application to a humanoid robot,”The International Journal of Robotics
Research, vol. 27, no. 2, pp. 213–228, 2008.
[66] L. Campanaro, S. Gangapurwala, D. De Martini, W. Merkt, and
I. Havoutis, “Cpg-actor: Reinforcement learning for central pattern
generators,”arXiv preprint arXiv:2102.12891, 2021.
[67] S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation
error in actor-critic methods,” inInternational Conference on Machine
Learning. PMLR, 2018, pp. 1587–1596.
[68] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa, D. Silver,
and D. Wierstra, “Continuous control with deep reinforcement learning,”
arXiv preprint arXiv:1509.02971, 2015.
[69] S. Thrun and A. Schwartz, “Issues in using function approximation
for reinforcement learning,” inProceedings of the 1993 Connectionist
Models Summer School Hillsdale, NJ. Lawrence Erlbaum, vol. 6, 1993,
pp. 1–9.
[70] T. Geijtenbeek, M. Van De Panne, and A. F. Van Der Stappen, “Flexible
muscle-based locomotion for bipedal creatures,”ACM Transactions on
Graphics (TOG), vol. 32, no. 6, pp. 1–11, 2013.
[71] M. L. Littman, “Markov games as a framework for multi-agent rein-
forcement learning,” inMachine learning proceedings 1994. Elsevier,
1994, pp. 157–163.
[72] M. Schilling, K. Konen, F. W. Ohl, and T. Korthals, “Decentralized deep
reinforcement learning for a distributed and adaptive locomotion con-
troller of a hexapod robot,” in2020 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 5335–5342.
[73] F. A. Oliehoek, M. T. Spaan, and N. Vlassis, “Optimal and approximate
Q-value functions for decentralized POMDPs,”Journal of Artiﬁcial
Intelligence Research, vol. 32, pp. 289–353, 2008.
[74] R. Lowe, Y . WU, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
“Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environ-
ments,”Advances in Neural Information Processing Systems, vol. 30, pp.
6379–6390, 2017.
[75] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooperative
agents,” inProceedings of the tenth international conference on machine
learning, 1993, pp. 330–337.
[76] E. Coumans and Y . Bai, “Pybullet, a python module for physics simulation
for games, robotics and machine learning,” http://pybullet.org, 2016–2019.
[77] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,
J. Tang, and W. Zaremba, “OpenAI Gym,” 2016.
[78] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-
performance deep learning library,” inAdvances in Neural Information
Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alché-Buc, E. Fox, and R. Garnett, Eds. Curran Associates, Inc.,
2019, pp. 8024–8035.
[79] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980, 2014.
[80] Raspberry Pi Foundation, “Raspberry Pi 4 Model B,” 2021, accessed:
September 18, 2021. [Online]. Available: https://www.raspberrypi.org/
products/raspberry-pi-4-model-b/
[81] D. Kahneman,Thinking, fast and slow. Macmillan, 2011.
[82] S. B. Kaufman,Beyond general intelligence: The dual-process theory of
human intelligence. Yale University, 2009.
[83] W. Huang, I. Mordatch, and D. Pathak, “One policy to control them
all: Shared modular policies for agent-agnostic control,” inInternational
Conference on Machine Learning. PMLR, 2020, pp. 4455–4464.
[84] P. Atooﬁ, F. H. Hamker, and J. Nassour, “Learning of central pattern
generator coordination in robot drawing,”Frontiers in Neurorobotics,
p. 44, 2018.
[85] M. Ishige, T. Umedachi, T. Taniguchi, and Y. Kawahara, “Exploring
Behaviors of Caterpillar-Like Soft Robots with a Central Pattern
Generator-Based Controller and Reinforcement Learning,”Soft robotics,
vol. 6, no. 5, pp. 579–594, 2019.APPENDIX : DEEPCPG P OLICIES FOR ROBOT LOCOMOTION
Aditya M. Deshpande, Eric Hurd, Ali A. Minai, Manish Kumar
University of Cincinnati
Cincinnati, OH 45221
deshpaad@mail.uc.edu; hurdeg@mail.uc.edu; ali.minai@uc.edu; manish.kumar@uc.edu
A Derivatives of Kuramoto Model based CPG Network
After incorporating Central Pattern Generators based on Kuramoto Model of oscillations in the neural actor, it remains
differentiable. The equations 1-4 describe the model of central pattern generators (CPGs).
˙φi=ω i+�
j�=iajwijsin(φ j−φ i−ϕ ij), (1)
¨ai=α a(βa(Ai−a i)−˙a i), (2)
¨bi=α b(βb(Bi−bi)−˙bi), (3)
yi=b i+a isin(φ i) (4)
Here for a CPG i,φiis the rate of change of phase, ωiis the natural frequency, Aiis the desired amplitude and Biis
the desired offset. wijrepresents the connection weight between the incoming connection from CPG jto CPG i.ϕijis
the connection phase bias between the CPGs iandj.(αa,βa)and(αb,βb)are the constants. The target trajectory or
action command sent to the robot environment by CPGiis given byy i.
We discretize equations 1-4 at timetfor analytically solution. Equations 5-6 represent the discrete form of Eq. 1.
˙φt
i=ω i+�
j�=iat−1
jwijsin(φt−1
j−φt−1
i−ϕ ij), (5)
φt
i=φt−1
i+˙φt−1
iδt (6)
Equations 7-9 represent the discrete form of Eq. 2.
¨at
i=α a(βa(Ai−at−1
i)−˙at−1
i), (7)
˙at
i=˙at−1
i+ ¨at−1
iδt, (8)
at
i=at−1
i+˙at−1
iδt, (9)
Equations 10-12 represent the discrete form of Eq. 3.
¨bt
i=α b(βb(Bi−bt−1
i)−˙bt−1
i), (10)
˙bt
i=˙bt−1
i+¨bt−1
iδt, (11)
bt
i=bt−1
i+˙bt−1
iδt, (12)
Similarly, Eq. 13 represent the discrete form of Eq. 4.yt
i=bt
i+at
isin(φt
i) (13)
Using 6-13, a recurrent relationship can be derived for yiand its derivatives with respect to CPG parameters
{ϕij, wij,ωi, Ai, Bi}∀i, j∈{1, .., N} . This relationship is similar to the one discussed for discrete dynamic movement
primitives in [Gams et al., 2018].
Let there be a cost function L(yt
i)which is dependent on the CPG output yt
iat time t. This loss function can be
considered as a negative reward function. Thus, to backpropagate through this policy using the loss function, we
need the partial derivatives with respect to the parameters that are outputs of the neural policy, i.e., CPG parameters
{ϕij, wij,ωi, Ai, Bi}∀i, j∈{1, .., N}. We can derive these partial derivatives using the Chain Rule in calculus.
A.1 Phase Biasϕ
∂L(yt
i)
∂ϕij=∂L(yt
i)
∂yt
i∂yt
i
∂φt
i∂φt
i
∂ϕij(14)
Here, the term∂φt
i
∂ϕijcan be found using Eq. 6:
∂φt
i
∂ϕij=∂φt−1
i
∂ϕij+∂˙φt−1
i
∂ϕijδt (15)
And using Eq. 5, to evalute∂˙φt−1
i
∂ϕijas follows:
∂˙φt−1
i
∂ϕij=∂�
k�=iat−2
jwiksin(φt−2
k−φt−2
i−ϕ ik)
∂ϕij(16)
=∂�
· · ·+at−2
jwijsin(φt−2
j−φt−2
j−ϕ ij) +· · ·�
∂ϕij(17)
=at−2
jwij�∂φt−2
j
∂ϕij−∂φt−2
i
∂ϕij−1�
cos(φt−2
j−φt−2
i−ϕ ij)(18)
But,φt−2
jdepends onϕ jiand not onϕ ij. We can rewrite Eq. 18 as Eq. 19
∂˙φt−1
i
∂ϕij=at−2
jwijcos(φt−2
j−φt−2
i−ϕ ij)�
−∂φt−2
i
∂ϕij−1�
(19)
By substituting Eq. 19 in Eq. 15
∂φt
i
∂ϕij=∂φt−1
i
∂ϕij+at−2
jwijcos(φt−2
j−φt−2
i−ϕ ij)�
−∂φt−2
i
∂ϕij−1�
δt(20)
The loss functionL(yt
i)can be given similar treatment with respect tow ijandω i.
A.2 CPG Connection Weightsw
∂φt
i
∂wij=∂φt−1
i
∂wij+at−2
jsin(φt−2
j−φt−2
i−ϕ ij)δt+at−2
jwijcos(φt−2
j−φt−2
i−ϕ ij)�
−∂φt−2
i
∂wij�
δt(21)
2A.3 CPG Natural Frequencyω
∂φt
i
∂ωi=∂φt−1
i
∂ωi+δt+at−2
jwijsin(φt−2
j−φt−2
i−ϕ ij)�
−∂φt−2
i
∂ωi�
δt(22)
A.4 CPG Desired AmplitudeA
Similarly, taking partial derivative of loss functionL(yt
i)with respect toA i:
∂L(yt
i)
∂Ai=∂L(yt
i)
∂yt
i∂yt
i
∂at
i∂at
i
∂Ai(23)
Using Eq. 9 toﬁnd the value of∂at
i
∂Aiin Eq. 23:
∂at
i
∂Ai=∂at−1
i
∂Ai+∂˙at−1
i
∂Aiδt (24)
Using Eq. 8 we can evaluate∂˙at−1
i
∂Ai:
∂˙at−1
i
∂Ai=∂˙at−2
i
∂Ai+∂¨at−2
i
∂Aiδt (25)
Similarly, Eq. 7 toﬁnd∂¨at−1
i
∂Ai:
∂¨at−1
i
∂Ai=α a�
βa(1−∂at−2
i
∂Ai)−∂˙at−2
i
∂Ai�
(26)
By substitution for Eq. 25 in Eq. 24,
∂at
i
∂Ai=∂at−1
i
∂Ai+�∂˙at−2
i
∂Ai+∂¨at−2
i
∂Aiδt�
δt (27)
By substitution Eq. 26 in Eq. 27,
∂at
i
∂Ai=∂at−1
i
∂Ai+�∂˙at−2
i
∂Ai+α a�
βa(1−∂at−3
i
∂Ai)−∂˙at−3
i
∂Ai�
δt�
δt(28)
A.5 CPG Desired OffsetB
Now, taking partial derivative of loss functionL(yt
i)with respect toB iand again using the Chain Rule:
∂L(yt
i)
∂Bi=∂L(yt
i)
∂yt
i∂yt
i
∂bt
i∂Bt
i
∂Bi(29)
Now, we can derive the similar expression as Eq. 28 forB iin Eq. 30,
∂bt
i
∂Bi=∂bt−1
i
∂Bi+�∂˙bt−2
i
∂Bi+α b�
βb(1−∂bt−3
i
∂Bi)−∂˙bt−3
i
∂Bi�
δt�
δt(30)
Therefore, equations 20, 21, 22, 28, 30 show that the dynamical system deﬁned by Central Pattern Generators based on
Kuramoto model of oscillation is differentiable with respect toϕ ij, wij,ωi, Ai, Birespectively.
3B Pseudo code
Algorithm 1:Training DeepCPG Policies
Input:Maximum iterationsNmax, Babbling stepsτ b, Update stepsτ update , Policy update delayτ delay, CPG
policy stepsτ c, Observation stepsτ o, Discount factorγ, Polyak averaging constantρ, Learning rateη,
Noise clip limitc, Policy exploration noiseσ
NOTE: We usex −τ(·):≡x t−τ (·):tandx :τ(·)≡x t:t+τ (·)to avoid notation clutter
Randomly initializeπ θ, Q-functionsQ κ1andQ κ2
Set target network parameters equal to main parameters:θtarg←θ,κtarg
1←κ 1andκtarg
2←κ 2
Initialize replay bufferR
whilek < Nmaxdo
Observe states −τo:, Goalg
ifk <τ bthen
Select CPG goalsg CPGrandomly
else
Select CPG goalsg CPG∼π θ(s−τo:, g)
end
forn={1, . . . ,τ c}do
Observe CPG hidden stateh
Select CPG actionsˆg j, h�∼π CPG(gCPG, h)
gj=clip�
ˆgj+�, g jmin, gjmax�
where�∼N(0,σ); Executeg jin environment
Observe next states�, rewardrand environment terminal signaldindicating ifs�is terminal state
Collect(s, g j, s�, r, d, h, h�, gCPG, g)inRwheres=s t
k=k+ 1
end
Ifs�is terminal state, reset environment, reset goalgand reset CPG hidden stateh
ifk >τ bthen
fori={1, . . . ,τ update }do
Randomly sample batches from episodes stored inR
B={(s −τo:, gj,:τc, s�
:τc, r:τc, dt+τ c, ht, h�
t+τ c, gCPG t, gt, gt+τ c)}
Computeg�
CPG∼π θtarg(s�
t+τ c−τo:t+τ c, gt+τ c)
h�=h�
t+τ c
fort={1, . . . ,τ c}do
Get CPG hidden stateh�
Select target CPG actionsˆg�
j,t, h��∼π CPG(g�
CPG, h�)
Collectg�
j,t=clip�
ˆg�
j,t+clip(�,−c, c), g jmin, gjmax�
where�∼N(0,σ)
end
Compute targets:
y(r:τc, s�
t+τ c−τo:t+τ c, gt+τ c, dt+τ c) =
�r:τc+γ(1−d t+τ c) min
i=1,2�
Qκtarg
i�
s�
t+τ c−τo:t+τ c, gt+τ c, g�
j,:τc��
Update Q-functions by one-step gradient descent using:
∇κi1
|B|�
B�
Qκi(s−τo:, gt, gj,:τc)−y(r :τc, s�
t+τ c−τo:t+τ c, gt+τ c, dt+τ c)�2fori={1,2}
ifimodτ delay= 0then
Update policy by one-step gradient ascent∇ θ1
|B|�
BQκ1(s−τo:, gt, gj,:τc)
Update target networks:
θtarg←ρθtarg+ (1−ρ)θ;κtarg
i←ρκtarg
i+ (1−ρ)κ ifori={1,2}
end
end
end
end
4Algorithm 2:Deployment of trained DeepCPG Policy in Robot Environment
Input:Trained policyπ θ, CPG policy stepsτ c, Observation stepsτ o, Goalg
NOTE: We usex −τ(·):≡x t−τ (·):tandx :τ(·)≡x t:t+τ (·)to avoid notation clutter
Initialize CPG hidden stateh
whileRobot is activedo
Observe states −τo:
Select CPG goalsg CPG∼π θ(s−τo:, g)
forn={1, . . . ,τ c}do
Observe CPG hidden stateh
Select CPG actionsg j, h�∼π CPG(gCPG, h)
Executeg jin environment
Observe next states�, rewardr
end
end
C Schematic of DeepCPG Policy
CPG Policy RobotNeural policyCPG parameters
Fixed joint
Active joint
Primitive
controller
Figure 1: Example schematic illustrating CPG Policy and dimensions of CPG parameter outputs from neural policy πθ
for a robot with four CPG nodes. In CPG Policy, each circled number represents a CPG node corresponding to the
active joint on the robot with that number. It should be noted that this is just an illustration. In the case of the actual
robots considered for the experiments, the number of joints on the robot is more and the corresponding diagram of CPG
Policy would have far more connections. We chose to show an example with only four nodes in the CPG policy to
avoid clutter. Please refer Figure 2 and Section-III in the paper for the meaning of various symbols used here.
5D Additional Results
�����������
�������������
���
� ��������
���
�������
Figure 2: Here we show the trajectories followed by trained policies for the go-to-goal task. In both cases, the robot
starts in an identical state. The target waypoints are shown to the robot using a receding horizon strategy. Section
VI-C of the paper provides further details of this task. The episode length was 3000 timesteps. A new waypoint was
introduced every 100 timesteps and a total of 26 waypoints were given to the robot. Legends: (Start) Start location of
the robot, (End) Final target waypoint, (Waypoints) True target waypoints given to the robot, (TD3) Path traced by
Feed-forward policy trained using TD3, (DeepCPG): Path traced by trained DeepCPG policy.
References
[Gams et al., 2018] Gams, A., Ude, A., Morimoto, J., et al. (2018). Deep encoder-decoder networks for mapping raw
images to dynamic movement primitives. In2018 IEEE International Conference on Robotics and Automation
(ICRA), pages 5863–5868. IEEE.
6