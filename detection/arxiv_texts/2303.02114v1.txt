arXiv:2303.02114v1  [math.ST]  3 Mar 2023LAG SELECTION AND ESTIMATION OF STABLE PARAMETERS FOR
MULTIPLE AUTOREGRESSIVE PROCESSES THROUGH
CONVEX PROGRAMMING
BYSOMNATH CHAKRABORTYa, JOHANNES LEDERERbAND RAINER VON SACHSc
1Fakultät für Mathematik, Ruhr-Universität Bochum, 44801 B ochum, Deutschland,
asomnath.chakraborty@ruhr-uni-bochum.de;bjohannes.lederer@ruhr-uni-bochum.de
2Institut de Statistique, Biostatistique et Sciences Actua rielles, LIDAM, UCLouvain, 1348 Louvain-la-Neuve, Belgiu m,
crainer.vonsachs@uclouvain.be
Motivated by a variety of applications, high-dimensional t ime series have
become an active topic of research. In particular, several m ethods and ﬁnite-
sample theories for individual stable autoregressive proc esses with known lag
have become available very recently. We, instead, consider multiple stable au-
toregressive processes that share an unknown lag. We use inf ormation across
the different processes to simultaneously select the lag an d estimate the pa-
rameters. We prove that the estimated process is stable, and we establish rates
for the forecasting error that can outmatch the known rate in our setting. Our
insights on the lag selection and the stability are also of in terest for the case
of individual autoregressive processes.
1. Introduction. Today’s world of acquisition of complex data in areas such di verse as
macroeconomics and ﬁnance, everyday weather predictions, brain imaging, and many more,
has called for intelligent model approaches that avoid need ing to use potentially a (too) high
number of model parameters per available sample size. Moreo ver, often these data are of high
dimensionality - as they arise together in a panel or in the fo rm of a multivariate vector. These
stylized facts render the purpose of predicting the evoluti on of these data into the (near) fu-
ture really challenging. To face this challenge choosing a d ata generating model that assumes
some common underlying structure relating the different components of the observed multi-
variate data set will not only turn out to be advantageous but reﬂects the observation that the
different series do not behave independently from each othe r - they might actually be driven
by latent (i.e. unobservable) mechanism (such as a leading e conomic indicator, or a global
climate trend, etc, often modelled by a latent factor model) . Moreover, we almost always ob-
serve serial correlation between present and past observations, which traditionall y has been
modelled by assuming some sort of weak dependence over time ( translating into dynamic
latent factor models, e.g., Forni et al. (2000).
In this context, as factor modelling does not necessarily al low for component-wise predic-
tion, the approach of (parametric) vector autoregression ( V AR) has already for a long time
become an overly prominent tool for modeling such multivari ate time series - with in particu-
lar the idea that the common serial dependence is limited by t he existence of a common max-
imal lag-order for all components. However, as the number of component series is increased,
V AR models have the known tendency to become overparametriz ed. In the virtue of having
to do with a high-dimensional parameter estimation problem , more recent possibilities to ad-
dress this issue are regularized approaches, such as the LASSO for estimating the parameters
of these models (essentially by some kind of regularised lea st-squares approach, see, for ex-
ample, Nardi and Rinaldo (2011)). This is in contrast to more traditional approaches (based
MSC2020 subject classiﬁcations: Primary 62M10, 62L99; secondary 49M29.
Keywords and phrases: regularised least square, LASSO, autoregressive process, hierarchical-group norm,
dual norm, stability, sample complexity.
12
mostly on information criteria for lag-order selection suc h as AIC, BIC, etc.) which address
overparametrization by selecting a low lag order, based on t he assumption of short range de-
pendence, assuming that a universal lag order applies to all components. For a good forecast
performance in a high-dimensional context, these approach es turned out to fall behind the
LASSO - which, until recently, did however not incorporate t he notion of lag order selection.
It has been only the recent work by Nicholson et al. (2020) tha t proposed a class of hierar-
chical lag structures that embed the notion of lag selection into a convex regularizer. The key
modeling tool has been a group LASSO with nested groups which guarantees that the sparsity
pattern of lag coefﬁcients honors the V AR’s ordered structu re. For more details on the litera-
ture on dimension reduction methods which address the V AR’s overparametrization problem
we refer to Section 2 of the mentioned work by Nicholson et al. (2020). A clear shortcoming,
however, of this approach is the necessity to model all compo nents of the observed multi-
variate time series to be of the same data length, a constrain t in classical V AR-modelling that
cannot be circumvented.
Motivated by the approach of Nicholson et al. (2020), in this paper, we propose a method
to analyse multiple stable autoregressive processes of (po tentially) different lengths in the
framework of regularized LASSO, where the regularization i s achieved via an overlapping
group-norm that induces sparsity at the group level. Moreov er, we show that, even in absence
of any information on the maximum lag of the processes, the pr oposed framework estimates
the true lag and the coefﬁcients of the AR model. Finally, we s how that the model ﬁtted with
the AR coefﬁcients returned by this proposed method is stabl e. As our results on statistical
guarantees are essentially of non-asymptotic nature - inte resting even in the context of ob-
serving a single time series - we ﬁrst review the (sparse) lit erature on those non-asymptotic
results in a time series context, before we turn in more detai l to the similarities and differences
between our and the approach of Nicholson et al. (2020).
Most of the research in time series analysis — until recently — focused on deriving asymp-
totic behaviour of the predictors. This severely restricts applicability of these results, espe-
cially in the reign of low sample-to-predictable ratio. Pop ular approaches to overcome this
nuisance of dependency have been using the assumption of sta bility (leading to stationar-
ity) of the data-generating process. For example, both Nega hban and Wainwright (2011) and
Loh and Wainwright (2012) used stability in deriving the gua rantees in small sample regimes;
however, these works established these results under the co ndition that the coefﬁcient matri-
ces are severely norm-bounded (namely, the sum of the operat or-norms of the coefﬁcient
matrices is smaller than 1), which is much stronger than stab ility of the process determined
by those coefﬁcients. Recently, Basu and Michailidis (2015 ) made a big stride towards under-
standing the effect of temporal and cross-sectional depend ency in the small sample regime.
The underlying hypothesis in that work was that data be amena ble to modelling via stable
vector autoregressive process; they tracked the restricti ons on the spectral domain — as en-
forced by stability of the process — and derived non-asympto tic prediction guarantees for
high dimensional vector autoregressive process with Gauss ian white noise innovation. Sev-
eral follow-up works (see e.g.Wong et al. (2020), Masini et-al (2022), and the references
there-in) then extended their results to the case where the i nnovations are heavy-tailed, and
moreover, they derived guarantees assuming only the statio narity and ﬁniteness of second
moment of the underlying process — conditions weaker than st ability.
In the remainder of this Introduction we go now into more deta ils about the relation of our
approach to the one of Nicholson et al. (2020). Essentially, the latter contributed by ﬁnding
out that algorithmically the method by Mairal et al. (2011) ( and its computationally faster
amendment by Tseng (2009)) can be used in such a setting to add ress, in the presence of
prior information on an upper bound Lof the true unknown lag order, estimation of order
Lvector-autoregressive models of dimension M, abbreviated V AR M(L)in the sequel. OurESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 3
work can now be seen as ﬁtting a common (i.e. “diagonal vector ”) autoregressive model to a
multivariate time series of dimension M, i.e. a panel of Mobserved (univariate) time series
ofin general not equal lengthsnm,1≤m≤M. We address the challenging question on how
to choose, solely from the information available in the mode l for the observed data, a common
appropriate lag order Lthat allows us to phrase and solve our problem via a penalised LASSO
approach. We derive non-asymptotic bounds on the multivari ate one-step ahead prediction er-
ror and estimate the collection of the autoregressive coefﬁ cientsβ··=(βm
1,...,βm
L)1≤m≤M
under the paradigm of sparseness. Assuming that there is a co mmon true unknown lag or-
derL0that generated our Mtime series, our algorithm, akin Nicholson et al. (2020) (an d
Mairal et al. (2011)) for ﬁtting a common model is based on a mo diﬁcation of a hierarchical
group LASSO approach: We ﬁrst determine an appropriate (min imal) upper bound L≥L0
depending essentially on the sample size nminof the shortest observed time series component
(and onM, of course) from a thorough analysis of the theoretical comp lexity of our group-
LASSO based approach. With this appropriate L, necessary to embed our autoregression
problem into the framework of high-dimensional multivaria te regression, we transfer exist-
ing technology on LASSO estimation (with overlapping hiera rchically constructed groups)
to our problem. We derive statistical guarantees for the est imatorsˆβ, solution of our afore-
mentioned learning algorithm, and for the estimator ˆL0(essentially taken from the support of
ˆβ). More speciﬁcally we deliver non-asymptotic bounds on the multivariate one-step ahead
prediction error, on the estimation error of β, on the false discoveries for the support of β,
and quite innovatively on the stability of the ﬁtted model. F or the latter, we show that the
ﬁtted autoregressive model of order ˆL0with estimated coefﬁcients ˆβfulﬁls the conditions
of the true model for stability (via a more explicit concept o fε-stability that we introduce to
asses the difﬁculty of the statistical estimation problem) .
In the following paragraph we are even more explicit about th e exact nature of our con-
tributions motivated from the existing limitations of the c urrent approaches we found in the
literature.
Current limitations and our contributions. Some of the questions that are not sufﬁciently
addressed in recent existing work on non-asymptotic time se ries analysis are as follows.
L1 Nicholson et al. (2020) illustrate that an approach based on LASSO with overlapping
groups (such as in Mairal et al. (2011)) can determine the com ponent-wise lag orders Lm
of stable, high-dimensional vector autoregressive proces ses (including the "cross-over"
lag order Lijof the dependence of the i−th component on the j−th component of a V AR
model). Their formulation based on (dual) convex programmi ng is computationally attrac-
tive and intriguing more generally, but it requires a unifor m upper-bound on the Li,jas
a parameter both in their theoretical bounds and in practice . The current literature either
ignores this issue altogether or sets those bounds based on m odel selection such as AIC or
BIC or via Bayesian shrinkage, whose suitability is unclear here.
C1 We mimic Nicholson et al. (2020)’s methodological approa ch, but we establish a suitable
upper bound for the lag orders. Our choice on this minimal upp er bound guarantees the
following:
1. the smallness of the empirical prediction risk (see Theor em 4.3 and the discussion fol-
lowing the theorem),
2. a resulting tuning parameter that is not too large (see Cor ollary 4.5), and
3. the restricted eigenvalue property of the data matrix to h old (Theorem 4.6 and the Corol-
lary 4.7 immediately after that).
In this sense, our upper bound on the lag order is optimal for o ur setup.
L2 Real world applications often involve multiple univaria te, decoupled time series. This
would translate to V AR modeling with a diagonal coefﬁcient m atrix. However, purely4
transferring existing results from a V AR M(L)modeling approach can be cumbersome in
practice for situations in which the number of available sam ples for each individual com-
ponent time series is not the same: obviously needing to chop off the samples in order to
work with the minimal individual sample size could result in possibly weaker theoretical
guarantees and practical performances (see below).
C2 Applying a hierarchical group norm enables us to derive pr ediction guarantees that de-
pend on the average number of samples per component (instead of the minimum numb er
of samples per model, as would be the case had we translated na ively to a M-dimensional
V AR model). More speciﬁcally, availability of perfect info rmation on the true lag L0and
n1=L0+T1,···,nm=L0+TM(respective) number of samples for the Mindividual
components yields the following: the V AR M(L)translation would result in the following
provable error (see Nicholson et al. (2020)), stating that w ith high probability
(1) ||ˆβ−β||2=O
/radicaligg
log(M2L0)
nminM
,
whereas the error from the algorithm we describe is of the ord er (see equation (33) below)
(2) ||ˆβ−β||2=O/parenleftigg/radicalbigg
log(ML)
D/parenrightigg
.
HereD=T1+···+TMis the total number of "postsamples" ( Tm··=nm−L0). Fur-
thermore, our strategy results in weaker dependency of the e rror on the behaviour of the
reverse characteristic polynomial on the unit disk, unlike in the relevant V AR M(L)model
translation (compare with Basu and Michailidis (2015)).
L3 Recall that a (univariate) autoregressive process Xt=a1Xt−1+···+aLXt−L+Utis
stable if the “reverse characteristics polynomial” 1−a1z−···−aLzLhas no complex
roots on the closed unit disk; it is known that stability impl ies stationarity (see Section 2
below). But what about stability for ﬁtted autoregressive models? While this question has
an afﬁrmative answer in the special case of Yule-Walker esti mation of the coefﬁcients
of a univariate autoregressive process (known however to be less efﬁcient), the question
of stability or stationarity of the process reconstructed f rom the parameters estimated by
LASSO-based approaches does not seem to have been addressed in the literature. However,
starting with a stable process to have generated the input ob servations of these devised
algorithms, it is reasonable to expect that the reconstruct ed process be stable (and thus,
multi-step predictions be reliable as well).
C3 We show, in Theorem 4.10, that the process reconstructed f rom the parameters returned
by our algorithm is stable when the samples available as inpu t are generated by stable
processes. As Basu and Michailidis (2015) showed, a measure of stability for an autore-
gressive process is, equivalently, a boundedness criteria on the spectral density, and the
boundedness in the Fourier domain translates in a sense to ‘s moothness’ of the process
in the temporal domain. Thus, as, intuitively, the stable au toregressive processes form a
‘smooth’ subclass, it is desirable that any algorithm for le arning the parameters of pro-
cesses from this smooth subclass should return estimators l ying in this subclass; in this
paper, this is ensured by Theorem 4.10.
It is important to mention here that the results in this paper demonstrate that our over-
lapping group-lasso approach yields a stable process when t he underlying process is stable
as well. The aim of the paper is not to choose the most optimal t uning parameters or some
absolute constants, but rather to show existence of these pr oposing reasonable candidates
for such parameters/constants.ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 5
Organization of the paper. The paper is organized as follows. In Section 2, we recall rel e-
vant deﬁnitions from existing literature, and we set notati ons. In Section 3, (1) we specify the
model and formulate the learning problem as a regularized gr oup-LASSO with overlapping
group norm, where the data matrix is a block-diagonal matrix — each block of which consists
of the data matrix that treats a least-squares problem corre sponding to the associated compo-
nent time series; (2) we present the learning algorithm base d on the group-LASSO problem.
Section 4 contains the bulk of the technical contents, in par ticular, the proof of the statements
of the main results, already presented at the end of Section 3 . This Section 4 is divided into
four subsections: Subsection 4.1 presents an oracle inequa lity bounding the one-step-ahead
prediction error (Theorem 4.2), as well as a high-probabili ty bound on the effective noise
of the model (Theorem 4.3). Subsection 4.2 starts with restr icted eigenvalue bounds (Propo-
sition 4.6) for the blocks of the data matrix, and goes on to in tegrate the blockwise results
to ﬁnally arrive at an estimate (Theorem 4.9) of the error in e stimating the AR-coefﬁcients.
Finally, combining the results from these subsections, sta bility of the estimated AR model
(Theorem 4.10) has been established in Subsection 4.3. All p roofs of auxiliary results are
deferred to a series of Appendices.
2. Preliminaries.
2.1. General notations.
•Md(F)denotes the ring of d×dmatrices with entries in the ﬁeld F∈ {R,C}, and for
M∈Md(F), we write M⊤for the transpose; if F=C, thenM⋆denotes the conjugate
transpose.
•Dis the complex closed unit disk D··={z∈C:|z|≤1}, and its boundary is ∂D··={z∈
C:|z|=1}.
• For any integer d>0, ifx∈Rd, then||x||2=√
x⊤x, andSd−1··={x∈Rd:||x||2=1}; in
particular, under standard identiﬁcation C=R2, we have S1=∂D.
• For integer n>0, we will denote the set {1,2,···,n}by[n].
• For a vector ˆβm=(ˆβm,1,···,ˆβm,L)and an integer 0<L0≤L, we write
(3) ˆβm(L0)··=(ˆβm,1,···,ˆβm,L0).
2.2. Notations for autoregressive process.
Conventions: We useX,Y,Z,... to denote random variables, and X,Y,Z,... to denote
random vectors. On the other hand, we use a,b,c,... to denote real (or complex) con-
stants, and a,b,c,... for vector-valued constants.
Notations: • Ford-dimensional autoregressive process
Xt··=A1Xt−1+···+ALXt−L+Ut, (4)
andz∈C, we write
Az··=I−A1z−···−ALzL. (5)
•Lwill denote an initially determined “ad-hoc" upper-bound o n the true lag of the pro-
cess in (4), and L0the true lag.
DEFINITION 2.1 (Weak stationarity). A d-dimensional time series {Xt}t∈Zis said to be
weakly stationary if the following holds: a)E[||Xt||2
2]<∞for allt∈Z,b)E[Xt]=µfor all
t∈Z, andc)E[XtX⊤
t−h]=Γ(h)for allt,h∈Z.6
DEFINITION 2.2 (Strong stationarity). A d-dimensional time series {Xt}t∈Zis said to be
strongly stationary if for each integer n>0, and all integers t1,···,tn,h, the distributions
of the vectors (Xt1,···,Xtn)and(Xt1+h,···,Xtn+h)are identical.
DEFINITION 2.3 (Autoregressive time series). A d-dimensional time series {Xt}t∈Zis
autoregressive of lag at most L>0if there are d×dmatricesA1,···,ALsuch that
Xt=A1Xt−1+···+ALXt−L+Ut. (6)
holds for all t∈Z, for some random white noise process {Ut}t∈Z.
Associated to each d-dimensional lag- Lautoregressive process {Xt}t∈Z— as in equation
(6) — is the associated order-1 process Xt=AXt−1+Ut, where
A··=/parenleftbigg
A1→L,AL
IdL−d,0/parenrightbigg
,Ut··=/parenleftbigg
Ut
0/parenrightbigg
, (7)
andA1→Lis the block matrix (A1···AL−1).
DEFINITION 2.4 (Stability). A d-dimensional lag- Lautoregressive process {Xt}t∈Z—
as in equation 6 — is said to be stable if det(I−Az)/\e}atio\slash= 0 for|z| ≤1. Equivalently, the
process is stable if det(Az)/\e}atio\slash=0for|z|≤1.
DEFINITION 2.5 (Reverse characteristic polynomial). The polynomial det(Az)is called
thereverse characteristic polynomial of the process in equation 6.
We note the equality det(I−Az)=det(Az). In particular, the process in equation (6) is
stable if and only if every eigenvalue of Ais inside the open unit disk.
DEFINITION 2.6 (ǫ-stability). A stable autoregressive process, as in equati on (6), is said
to beǫ-stable for an ǫ∈(0,1)if the following holds:
ǫ≤min
|z|=1|det(Az)|≤max
|z|=1|det(Az)|≤ǫ−1. (8)
REMARK . By maximum modulus principle, this is equivalent to saying that
ǫ≤min
|z|≤1|det(Az)|≤max
|z|≤1|det(Az)|≤ǫ−1.
The lower-bound here is a convenient quantiﬁcation of the no tion of stability, which demands
thatmin|z|≤1|det(Az)|>0. Note that we also require the upper bound to derive our stati sti-
cal guarantees.
A well-known fact about autoregressive processes is the fol lowing: see Lütkepohl (2005,
proposition 2.1) for details.
LEMMA 2.7 (Stability implies weak stationarity). A stable autoregressive process is
weakly stationary.
3. Statistical Model and Estimator. This section introduces our statistical model and
estimator, and presents an algorithm to learn the parameter s of the model from observed
samples.ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 7
3.1. Statistical Model. We start with the model. Suppose that we observe time-sample s
generated by Munivariate autoregressive process, for which we know a unif orm upper-
boundLof the true lag-order. Then, we can aggregate these Munivariate lag at most L
autoregressive processes
X1
t=β1
1X1
t−1+···+β1
LX1
t−L+U1
t;
... (9)
XM
t=βM
1XM
t−1+···+βM
LXM
t−L+UM
t.
In this paper, we work under the simpliﬁed assumption that th e true lag of all the Mcom-
ponent processes is identical, namely, L0, and that L≥L0is generic; neither L0norLis
known a priori . Additionally, we assume mean-zero, Gaussian white-noise innovations; that
is, for each m∈[M]the set{Um
t}t∈Zconsists of independent mean-zero, univariate Gaus-
sians with coordinate-wise standard deviation σm∈(0,∞). Additionally, we assume that for
eacht∈Z, the noise variables U1
t,...,UM
tare independent. We summarize the parameters
of the model in a matrix Θ∈RM×LviaΘml··=βm
lto refer to groups of parameters more
easily later. Our goal is 1. to estimate the parameters of allmodels simultaneously; and 2. to
assess the lag L0, which is assumed to be the same over all Mprocesses.
We make the following assumption on the absolute value of the smallestβ-coefﬁcient,
which is widely known as β-min assumption in the LASSO literature; see, for example,
Bunea (2008). We note that this assumption will only be neede d to achieve the bound in The-
orem 3.1 after λ-thresholding; in particular, when no thresholding is empl oyed, the analysis
in this paper does not require the assumption.
ASSUMPTION 1 (β-min assumption ).There is an absolute constant cβ>0such that the
true autoregressive coefﬁcient vector βsatisﬁes
(10) βm
j/\e}atio\slash=0⇒βm
j≥cβ.
Broadly speaking, this assumption ensures that the non-zer o coefﬁcients can be detected in
the ﬁrst place. We now set out to deﬁne a regularizer. Let njdenote the total number of
samples from
Xj
t=βj
1Xj
t−1+···+βj
LXj
t−L+Uj
t,
andTj··=nj−L. The main idea is as follows. Suppose that L≥L0is some integer, and for
eacht∈{−L+1,...,1,...,Tm}andm∈{1,...,M}, we have an observation xm
tofXm
t.
Denoteβm··=(βm
1,...,βm
L)⊤for eachm∈[M], and letβ··= (β⊤
1,...,β⊤
m)⊤. We deﬁne
G1,...,GL⊂SML··={1,2,...,M}×{1,2,...,L}by
(11) Gl··={1,2,...,M}×/braceleftbig
l,l+1...,L/bracerightbig
for alll∈{1,...,L}. The groups are nested: G1⊃···⊃G L. LetβGl∈RM×(L−l+1)be the
submatrix of Θ, consisting of columns having index larger or equal to l. We set the group
norm to be
||β||G··=L/summationdisplay
l=1/radicalbig
M(L−l+1)||βGl||F, (12)
where ||βGl||F··=/radicaltp/radicalvertex/radicalvertex/radicalbtM/summationdisplay
m=1L/summationdisplay
j=l|βm
j|2.8
is the Frobenius norm of βGl. We will alternatively write N(β)for the group norm ||β||G, for
the sake of notational ease.
The overall post-sample size is denoted
D··=T1+···+TM.
In order to estimate the coefﬁcient vector β, we propose solving the following constrained
convex program
minimize1
DM/summationdisplay
m=1Tm/summationdisplay
t=1/parenleftbig
xm
t−βm
1xm
t−1−···−βm
Lxm
t−L/parenrightbig2+λ||β||G, (13)
with an appropriate tuning parameter λ>0.
The objective function can be put in a concise form. For this, we deﬁne the vector y∈RD,
the matrix X∈RD×(ML), and the parameter β∈RML, as follows:
y··=(x1
1,...,x1
T1,x2
1,...,x2
T2,···,xM
1,...,xM
TM)⊤;
X··=
x1
1−1,...,x1
1−L...
x1
T1−1,...,x1
T1−L
x2
1−1,...,x2
1−L...
x2
T2−1,...,x2
T2−L
...
xM
1−1,...,xM
1−L...
xM
TM−1,...,xM
TM−L
; (14)
β··=(β1
1,...,β1
L,β2
1,...,β2
L,···,···,βM
1,...,βM
L)⊤.
It is immediate that
M/summationdisplay
m=1Tm/summationdisplay
t=1/parenleftbig
xm
t−βm
1xm
t−1−···−βm
Lxm
t−L/parenrightbig2=||y−Xβ||2
2.
In conclusion, the above estimation program is equivalent t o
/hatwideβ∈argmin
β∈RML/braceleftbigg1
D||y−Xβ||2
2+λ||β||G/bracerightbigg
. (15)
Hence, the estimator can be cast as a modiﬁed group-lasso est imator, which means that we
can use established group-lasso algorithms that allow for o verlapping groups (Mairal et al.,
2011). In essence, the above estimator generalizes the elementwise estimator HLagEin
Nicholson et al. (2020) to multiple time series. Note that, t he ordinary LASSO estimator—
as well as any group-LASSO estimators with non-overlapping groups—enforces sparsity by
setting coefﬁcients to zero without paying heed to the fact t hat when only an upper-bound to
the true-lag L0is an input to the regression— allcoefﬁcients indexed between L0+1andL
are supposed to be zero before any coefﬁcient with index smal ler or equal to L0; however,
the penalty obtained via the chained groups G1⊇···⊇G Lprecisely achieves this feat.ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 9
EXAMPLE (Regularizer). We consider the case of two univariate lag (a t most) three au-
toregressive processes; that is, M=2andL=3. The corresponding groups are the follow-
ing:
G1={(1,1),(1,2),(1,3),(2,1),(2,2),(2,3)};
G2={(1,2),(1,3),(2,2),(2,3)};
G3={(1,3),(2,3)}.
Thus, the group norm of β∈R2×3is
||β||G=√
6/radicalig
(β1
1)2+(β2
1)2+(β1
2)2+(β2
2)2+(β1
3)2+(β2
3)2
+√
4/radicalig
(β1
2)2+(β2
2)2+(β1
3)2+(β2
3)2
+√
2/radicalig
(β1
3)2+(β2
3)2.
Notice that, when the (regularized) LASSO sets a certain gro up (say the second group above)
to0, it automatically sets all the following groups to 0as well. More speciﬁcally, when the
(regularized) LASSO sets βGl0, then the hierarchical structure Gl⊇Gl+1⊇··· means that
for allr>0, each coordinate of βGl+rcomes as a coordinate of βGl, thus ensuring βGl+r=0
for eachr≥0.
In what follows, we use the following notations. For m∈[M]andl∈[L], andt≤Tm, we
write
(16)U(m)··=(Um
1,···,Um
Tm)⊤;
X(m,l)··=(Xm
1−l,...,Xm
Tm−l)⊤;
X(m)
t··=(Xm
t−1,...,Xm
t−L);
X(m)··=(X(m,1),...,X(m,L)).
Moreover, we will write X(m)
<jto denote any of the variables Xm
j′forj′<j.
3.2. Estimation Pipeline. Learning autoregressive coefﬁcients of multiple time seri es of
potentially different lengths and identical true lag is mor e complex than just the usual group-
LASSO problem, where the groups form a partition of the index set. From a methodologi-
cal perspective, some immediate technical challenges are 1 . deciding on what Lshould be
taken in the formulation of the convex problem (13), 2. how to disentangle the dual of the
group norm (in order to apply Hölder’s inequality to derive o racle prediction guarantees as
in subsection 4.1 below); from a practical perspective, the challenge lies in incorporating the
varying number of samples into the convex problem.
We now give a high-level overview of our estimation pipeline (described below) that
takes as input the multiple time series, and forms the approp riate convex problem in the
form of (15), and solves this convex problem via stochastic p roximal gradient method as in
Nicholson et al. (2020). In essence, the idea is to start look ing into the data set to ﬁrst ﬁnd
the samples corresponding to the component which has the min imum number of samples
(breaking ties arbitrarily). We then use this component to ﬁ nd the initial input lag, L, as de-
scribed in (38). In the next step, we solve the regularized le ast-squares problem in (13) —
where the penalty function is the group norm N, discussed further below (see (12)). The rate
of convergence of the procedure is quadratic in number of com putational steps as discussed
in Nicholson et al. (2020).10
Algorithm 1: AR Coefﬁcient Estimation Pipeline
Input : samples from the component stable processes, conﬁdence par ametersA≥1andδ >0, stability
parameter ǫ∈(0,1)
Output: estimated lag ˆL0, and estimated autoregression coefﬁcient vector ˜β
1. Letnmin be the minimum number of samples from the components. Solve f orL(see equation (38)):
nmin=L+84Aeζ−2Llog/parenleftbiggML
δ/parenrightbigg
.
2. Use the learning algorithm in Nicholson et al. (2020) as su broutine to solve the convex problem in (15) —
withLas above; call the output ˆβ.
3. Ifβm=βm′for allm,m′∈[M](equivalently, the component time series are from identica l AR process),
returnˆL0··=max{j:|(ˆβ1)j|>λ}and˜β··=(ˆβ′
0,···,ˆβ′
0), where (refer Equation (3) for notation)
ˆβ′
0··=1
MM/summationdisplay
m=1ˆβm(ˆL0);
else, return
ˆL0··=max{j:|ˆβm
j|>λfor some m∈[M]},
and ˜β··=(ˆβ1(ˆL0),···,ˆβm(ˆL0)).
The following is the main theorem on the theoretical propert ies of the output of the esti-
mation pipeline. This theorem is essentially a summary of th e results contained in Section 4,
and will be discussed in all detail in subsections 4.2.2 and 4 .3, as indicated below.
THEOREM 3.1 (Main Theorem). Letˆβbe the output of the group-LASSO in (15) above,
with
λ=24(84Ae)1
2ζ−1σ2
maxC♯3
2(1+ǫ−2+ǫ−4)/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM,
whereζ=6−3ǫ4, andL≥L0satisfying
nmin=L+84Aeζ−2Llog/parenleftbiggML
δ/parenrightbigg
.
Suppose that the β-min condition (10) holds with cβ=λ. If the total number Dof post-
samples satisﬁes
D≥39·(84Ae)C5
♯(1+ǫ−2+ǫ−4)2/parenleftbiggσmax
σmin/parenrightbigg4
(ǫ3ζ)−2MaL2
0L3log/parenleftbiggML
δ/parenrightbigg
log(2L),
and ifTmin≥84eAζ−2L0logL, then the following holds with high probability:
1.the estimation error is given by
||ˆβ−β||2≤81(84Ae)1
2LL0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2/radicaligg
log/parenleftbigML
δ/parenrightbig
D;
2.ifSλ··={j:|ˆβj|>λ}, then the false discovery is bounded by the following inequa lity:
|Sλ\supp(β)| ≤243(84Ae)1
2LL3
2
0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2λ/radicaligg
log/parenleftbigML
δ/parenrightbig
D.ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 11
3.the AR-models — ﬁtted with coefﬁcients ˆβ0returned by the Algorithm 1 ("AR Coefﬁcient
Estimation Pipeline"). — are stable, with high probability .
PROOF OF THEOREM 3.1. Subject to the stated β-min condition (10), this follows imme-
diately from Theorem (4.9) in combination with Theorem (4.1 0).
In the pipeline above, it is necessary to consider two distin ct stability parameters A≥1and
δ, as it is not possible to integrate them into a single paramet er — due mainly to the different
number of samples from the component processes in our set-up . Also, we separately mention
the two cases (of βm’s being identical (or not) for all m) in order to speciﬁcally emphasize
that in the ﬁrst case, our algorithm requires a smaller numbe r of samples than in the later case
(which allows savings of a factor of Min the sample complexity).
Decent algorithms like the proximal gradient method used in the subroutine above might
produce small non-zero valued parameters as numerical arti facts. One could consider other
types of algorithms instead, but the required number of comp utational steps could be much
higher. Moreover, those artifacts, and statistical false p ositives more generally, can also be
controlled by standard λ-thresholding as mentioned in the algorithm.
4. Statistical Guarantees. This section contains the main theoretical results of this p a-
per. We begin with deriving bounds for the one-step ahead pre diction error, ﬁrst formulated
by an oracle inequality (see Theorem 4.2), which depends on t he tuning parameter λof our
least-squares penalisation approach. Then we control this tuning parameter by controlling
the effective noise of our Lasso-optimisation problem. Bot h things together will ﬁnally yield
more explicit rates of our one-step ahead prediction error. The second part of this section
treats control of the estimated autoregressive coefﬁcient s, including control of false discovery
for their support (Theorem 4.9). In the end we present our res ult on stability of the estimated
AR-model (Theorem 4.10).
To start with, we brieﬂy recall the notion of the dual norm of o ur group-LASSO norm N
deﬁned above.
Our underlying space is RML. Observe that
β/ma√sto→N(β)··=/summationdisplay/radicalbig
|Gl|·||βGl||2
is a norm for any G={G1,···,Gl}that covers [ML]. However, this norm is singular at any
point where all the coordinates in a group vanish. Moreover, all the coordinates in all of the
smaller-sized groups will vanish, which is important in our analysis, since we will basically
care about sparse solutions. Thus, we can not appeal to diffe rential techniques to get bounds
on the norm, but rather need to appeal to the dual norm approac h.
DEFINITION 4.1 (Dual norm). For a norm NonRML, the dual norm N⋆(α)ofα∈
RMLis the optimum solution of the following convex program:
maximize /a\}bracketle{tα,β/a\}bracketri}ht
subject to N(β)≤1.
The dual norm is used to encapsulate the effective noise of LA SSO. More explicitly, the
dual norm shows up in the form N⋆(X⊤U), called the effective noise of the LASSO in equa-
tion (15). Recall that the effective noise vector X⊤Ucan be thought of as the “projection”
of the noise vector Uon the column space of X, and thus, N⋆(X⊤U)is a measure of the
“true” noise present in the data.
In Appendix D, we obtain a generic bound on the dual norm, whic h will be used to obtain
the statistical guarantees of this paper.12
4.1. Prediction Error. Here we now give the announced theoretical results on non-
asymptotic bounds for the one-step ahead prediction error, ﬁrst formulated by an oracle in-
equality (see Theorem 4.2), then in the following subsectio n including control of the tuning
parameter λby control of the effective noise of our Lasso-optimisation problem. This enables
us to formulate concrete rates of the prediction error. Note that this approach delivers an ex-
plicit way of how to select L(via equation (24), and subsequently (38)) the input parame ter
for Step 2 of Algorithm 1 ("AR Coefﬁcient Estimation Pipelin e").
4.1.1. Oracle Prediction Error. The problem (15) has a solution because, given any spe-
ciﬁc realization of the time series (thus, effectively, ﬁxi ngyandX) and any λ >0, the
convex function
fλ(β)=1
D||y−Xβ||2
2+λ||β||G (17)
is continuous, and
lim
||β||2→∞fλ(β)=∞.
This also shows that we can consider this as a convex program o n a compact domain, because
we should (at least in theory) be able to restrict the domain o f minimization to be an ℓ2-ball
of suitable radius, say Rr,X,y∈(0,∞). Letˆβbe a solution of this convex program.
Now write the time series observations in the matrix form as X. We are interested in
an estimate of the ’risk’, equivalently, the in-sample, one -step-ahead mean squared forecast
errorE[||y−Xˆβ||2
2/D|X]. In the derivations below, we follow a well-known approach f or
its control, as appeared (for example) in (Lederer, 2021, Ch apter 6). We defer the proof to
Appendix A.
THEOREM 4.2 (Prediction Guarantee). Suppose that λ≥2
DN⋆(X⊤U), whereN(α)=/summationtextL
l=1/radicalbig
|Gl|·||α(≥l)||2as in Proposition D.3. Write σ2=D−1(T1σ2
1+···+TMσ2
M); then
1
DE/bracketleftig
||y−Xˆβ||2
2|X/bracketrightig
≤σ2+ min
α∈RML/parenleftbigg1
D||X(β−α)||2
2+2λN(α)/parenrightbigg
,
In particular, the following inequality holds:
E/bracketleftig
||y−Xˆβ||2
2|X/bracketrightig
−E/bracketleftbig
||y−Xβ||2
2|X/bracketrightbig
≤min
α∈RML/parenleftbig
||X(β−α)||2
2+2DλN(α)/parenrightbig
.
If, moreover, λ≥4
DN⋆(X⊤U), then
1
DE/bracketleftig
||y−Xˆβ||2
2|X/bracketrightig
≤σ2+λ
2min/braceleftig
3N(β−ˆβ),3N(β)−N(ˆβ)/bracerightig
. (18)
Consequently,
1
DE/bracketleftig
||y−Xˆβ||2
2|X/bracketrightig
≤σ2+2λL0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||β−ˆβ||Gℓ. (19)
This result is in the form of standard oracle inequalities in high-dimensional statistics
(Lederer, 2021, Chapter 6). It shows that the estimator mini mizes the one-step-ahead mean-
squared forecast risk up to a complexity term that is linear i n the tuning parameter and the
model complexity. Hence, the above bound yields an upper bou nd for the rate convergence
once we can control the tuning parameter λvia an upper bound on the effective noise.
Thanks to Theorem 4.2 and Proposition D.3, in order to ﬁnd the smallest tuning param-
eterλfulﬁlling λ≥4N⋆(X⊤U)/Dit sufﬁces to derive a high-probability upper-bound on
D−1L−1
2||X⊤U||∞(which is precisely the bound on the dual norm of X⊤U).ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 13
4.1.2. Control of the Effective Noise for bounding the tuning param eter . We can prove
the following high-probability bound on the tuning paramet er (equivalently, on the effective
noise). Again, its proof appears in Appendix B. Note that thi s is a major inequality, and
while the arguments are well-known, we have applied those ar guments to the case where the
sparsity enforcing regularizer is induced by the overlappi ng group norm.
THEOREM 4.3 (Bound on the Effective Noise). LetC♯··=Tmax/Tmin. For any η >0
satisfying
(20) η≥8C♯σ2
max(1+ǫ−2+ǫ−4)
M,
and anyδ>0, ifD=T1+···+Tmsatisﬁes
(21) D≥8σ2
max(1+ǫ−2+ǫ−4)
c0ηlog/parenleftbiggML
δ/parenrightbigg
,
then the following inequality holds:
(22) P/bracketleftbigg2
DN⋆(X⊤U)≥3η
2√
L/bracketrightbigg
≤δ.
Herec0>0is the absolute constant from the Gaussian concentration in equality in proposi-
tion E.1.
The interpretation of the above result is that for the LASSO o racle inequality (Theorem 4.2)
to hold with high probability, it is sufﬁcient to choose λto be just as large as (3η)/(2√
L),
but it is not necessary to take it larger. Indeed, equation (2 2) shows that the probability that
2/DN⋆(XTU)is larger than 3η/2√
Lis small; thus, we may assume 2/DN⋆(XTU)<
3η/2√
L, to hold with probability 1−δ.
The above result bounds the tails of the effective noise. For anyA≥1, we will now set
η=C0√
ACǫC3
2
♯σ2
max/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM,
for some absolute constant C0>0and parameter Cǫ>0that depends only on the stability
parameter ǫ. Henceforth, we take
(23) η··=8(84Ae)1
2ζ−1(1+ǫ−2+ǫ−4)C3
2
♯σ2
max/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM,
which is obtained by plugging-in values of C0andCǫ— as in the proof of Lemma 4.4 below
— into Equation (23); we do not attempt to optimize these cons tants.
LEMMA 4.4 (Data-dependent selection of L).There is an absolute constant C∈(0,∞)
and a parameter Cǫ>0that depends only on the stability parameter ǫ>0such that if ηis
as in (23) and
(24) Tmin≤84Aeζ−2Llog/parenleftbiggML
δ/parenrightbigg
,
then the inequality (20) holds.
This results shows that there is a suitable ηfor our theories to hold. The question of ﬁnding
such anηin practice will need to be discussed in more applied future w ork.14
PROOF . We setC0··=(12)3√
84e, and
Cǫ··=ǫ−4(1+ǫ−2+ǫ−4).
Note that
(25) C♯Tmin=Tmax≥D
M.
Now, with ηas in (23), the inequality (20) is ensured by the following se quence of inequali-
ties:
(84Ae)1
2ζ−1/radicalig
Llog/parenleftbigML
δ/parenrightbig
≥/radicalbig
Tmin
⇒ (84Ae)1
2ζ−1C1
2
♯/radicalig
Llog/parenleftbigML
δ/parenrightbig
≥/radicalbigg
D
M
⇒8(84Ae)1
2ζ−1(1+ǫ−2+ǫ−4)C3
2
♯σ2
max/radicalig
Llog(ML
δ)
DM≥8C♯σ2
max(1+ǫ−2+ǫ−4)
M.
The inequality (24) provides for the theoretical support of our choice of Lin formulating
the penalized least squares program in (15).
The explicit nature of the parameters in the proof above is cr ucial here, in order to satisfy
both (20) and (21) above, as well as remaining compatible wit h the requirements involving
the restricted eigenvalue property (as in Corollary 4.7) be low. Note that the current LASSO-
based literature often ignores combining the requirements coming from standard oracle in-
equality type result (Theorem 4.2) and the restricted eigen value type results.
REMARK (On choosing Lvia equation (24)). The use of such an upper-bound Lcon-
forms with the (by now) well-known restricted isometry prop erty of sub-sampled Gaussian
matrices (that is, matrices whose entries are iid Gaussian) in the compressed-sensing litera-
ture; for more details, see (for example) (Candes and Tao, 20 06, section 1.E) and the refer-
ences therein.
ForDas in (21), this gets us the following bound:
P
N⋆/parenleftbigg2
DX⊤U/parenrightbigg
>12(84Ae)1
2ζ−1σ2
maxC♯3
2(1+ǫ−2+ǫ−4)/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM
≤δ,
In view of Theorem 4.3, we correspondingly set
(26) λ=24(84Ae)1
2ζ−1σ2
maxC♯3
2(1+ǫ−2+ǫ−4)/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM
to force (18) to be true.
REMARK . From the above, we note that λdecreases when any of M,D increases (as the
functionx−1logx→0whenx→∞ ). This is intuitive since larger Mrequires that LASSO
must not set too many coefﬁcients to 0, and larger the Dbetter the non-regularized estimator
is as approximation of the true coefﬁcients.
Together with Theorem 4.2, the above yields the following bo und:ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 15
COROLLARY 4.5 (Concrete rates of Prediction Error ). Suppose that
η= 8(84Ae)1
2ζ−1(1+ǫ−2+ǫ−4)C3
2
♯σ2
max/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM,
as set in Equation (23) above, and
D≥8σ2
max(1+ǫ−2+ǫ−4)
c0ηlog/parenleftbiggML
δ/parenrightbigg
.
Then, the following inequality holds with probability at le ast1−δ:
E/bracketleftbigg1
D||y−Xˆβ||2
2|X/bracketrightbigg
−σ2
≤min
α∈RML
1
D||X(β−α)||2
2+24
ζ(84Ae)1
2σ2
maxC♯3
2(1+ǫ−2+ǫ−4)/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DMN(α)
.
4.2. Estimating error of parameters, false discoveries. We now deliver the treatment of
assertions 1. and 2. on estimation error and support (via con trol of false discoveries) of our
Main Theorem 3.1. For this we need to cope with the Restricted Eigenvalue Property as it
typically arises in LASSO analysis (see Basu and Michailidi s (2015)).
4.2.1. Control of the restricted eigenvalue property. Our prediction guarantees stated so
far did not impose restrictions on the minimum number of samp les from each component. For
estimation and stability guarantees, however, we need to de mand a minimum number of those
samples. In particular, we will need the following proposit ion, on the restricted eigenvalue
property of the Gram matrix X⊤X. The proof presented in Appendix C follows the same
lines of arguments as in Basu and Michailidis (2015). Becaus e of the non-uniform nature of
the block dimensions of the data matrix in (14), as well as the individual weights assigned
to the components of the coefﬁcient β, we can only hope for a block-wise result as stated
below.
The following proposition delivers a lower bound on the erro r-expression ||Xmvm||2
2with
vmthem−th component of ˆβ−β.
PROPOSITION 4.6 (Bound on the Restricted Eigenvalue). For any conﬁdence parameter
δ>0and all vectors v=(v⊤
1,...,v⊤
M)⊤∈(RL)M, the following inequality holds with ζ··=
ǫ4
216andsmeven positive integers:
P/bracketleftbigg
∀m∈[M] inf
vm∈RL/parenleftbigg
||Xmvm||2
2−Tmσ2
mǫ2
2/parenleftbigg
||vm||2
2−2
sm||vm||2
1/parenrightbigg/parenrightbigg
≥0/bracketrightbigg
≥1−2M/summationdisplay
m=1exp/parenleftbigg
−Tmmin{ζ,ζ2}
2+smmin{logL,log(21eL/sm)}/parenrightbigg
. (27)
A proof of Proposition 4.6 appears in Appendix C.
Setting
(28) sm··=2/floorleftigTmζ2
8logL/floorrightig
,
we get the following immediate corollary to Proposition 4.6 :16
COROLLARY 4.7 (Bound on the Restricted Eigenvalue for speciﬁc sm).If
(29) Tmin≥84eζ−2logL,
whereζ=6−3ǫ4, then the following holds:
P/bracketleftbigg
∀m∈[M] inf
vm∈RL/parenleftbigg
||Xmvm||2
2−Tmσ2
mǫ2
2||vm||2
2/parenleftbigg
1−8LlogL
Tmζ2/parenrightbigg/parenrightbigg
≥0/bracketrightbigg
≥1−2M/summationdisplay
m=1e−Tmζ2
4. (30)
PROOF . We use ||vm||2
1≤L||vm||2
2in Proposition 4.6.
4.2.2. Bounds on estimation error of the autoregressive coefﬁcien ts, and false discovery.
This section contains the bound on the error of estimation of the autoregressive coefﬁcients,
and the false discovery.
The following is a compact notation used in the proof of the th eorem below (also in the
proof of Theorem 4.2).
DEFINITION 4.8 (Notation).
(31) N≤L0(v)··=L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||v||Gℓ.
The theorem below bounds the ℓ2-error in estimating the autoregressive coefﬁcients β, as
well as the size of the set of false positives, using the penal ized LASSO formulation as in
(15).
THEOREM 4.9 (Bounds on the Estimation Error and False Discovery). Letˆβbe the
solution to the group-regularized LASSO in (15) withλas in Equation (26) above — namely,
λ=24(84Ae)1
2ζ−1σ2
maxC♯3
2(1+ǫ−2+ǫ−4)/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM
whereζ=6−3ǫ4; if
(32) α··= min
m∈[M]Tmσ2
m
D,
and
D≥8σ2
max(1+ǫ−2+ǫ−4)
c0ηlog/parenleftbiggML
δ/parenrightbigg
,
then, for any conﬁdence parameter δ>0, the inequality
(33) ||ˆβ−β||2≤81(84Ae)1
2LL0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2/radicaligg
log/parenleftbigML
δ/parenrightbig
DESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 17
holds with probability at least
(34) (1−δ)/parenleftigg
1−2M/summationdisplay
m=1L−Tm
4logL/parenrightigg
,
provided Tmin≥84eAζ−2L0logLwithζ=6−3ǫ4.
Moreover, introducing the notation Sλ··={j:|ˆβj|> λ}, then the false discovery is
bounded by the following inequality holding with probabili ty as in (34) above:
(35)|Sλ\supp(β)| ≤243(84Ae)1
2LL3
2
0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2λ/radicaligg
log/parenleftbigML
δ/parenrightbig
D.
REMARK . In the above, the term inside the square root decreases asym ptotically, and the
terms outside may change or stay ﬁxed (depending on how the nu mber of samples for each
component increases).
PROOF OF THEOREM 4.9. For ease of notation, write
v··=ˆβ−β.
By Proposition 4.6 above (which can be applied since Tmin≥84eζ−2logL), one has
1
D||Xv||2
2≥1
DM/summationdisplay
m=1||Xmvm||2
2
≥ǫ2
2M/summationdisplay
m=1σ2
mTm
D||vm||2
2/parenleftbigg
1−8LlogL
Tmζ2/parenrightbigg
>4αǫ2
9||v||2
2.
To this, we now apply (see (45) in Appendix A) the inequality
1
D||X(β−ˆβ)||2
2≤3λN≤L0(β−ˆβ),
to obtain
||v||2
2≤27λ
8αǫ2N≤L0(v)
≤27λL0√
ML
8αǫ2||v||2
⇒ || v||2≤27λL0√
ML
8αǫ2
With the choice of λas in equation (26), namely,
λ=24(84Ae)1
2ζ−1σ2
maxC♯3
2(1+ǫ−2+ǫ−4)/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM,
it now follows that
||v||2≤81(84Ae)1
2LL0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2/radicaligg
log/parenleftbigML
δ/parenrightbig
D,18
as claimed. To get the bound on the false discovery, write S··=supp(β); the bound on the
false discovery can be obtained as follows:
|Sλ\S|=/summationdisplay
j∈[ML]\S1|ˆβj|>λ(ˆβ)
=/summationdisplay
j∈[ML]\S1|ˆvj|>λ(ˆv)
≤1
λ/summationdisplay
j∈[ML]\S|ˆvj|
by remark (A) ≤3
λ/summationdisplay
j∈S|ˆvj|
≤3√L0
λ||ˆv||2
which yields the bound in (35) by inequality (33).
REMARK . For the explicit choice of λas in equation (26), namely,
λ=24(84Ae)1
2ζ−1σ2
maxC♯3
2(1+ǫ−2+ǫ−4)/radicaligg
Llog/parenleftbigML
δ/parenrightbig
DM
withζ=6−3ǫ4, the inequality (35) becomes
(36) |Sλ\supp(β)| ≤81(ML)1
2L3
2
0
8αǫ2.
4.3. Stability of the estimated AR model. We ﬁnally prove that the coefﬁcients esti-
mated as per the overlapping-group-LASSO in (15), with λas in (26), lie in the region for
stability of univariate lag- Lautoregressive processes, even when the number of post-sam ples
is ’not too large’. More speciﬁcally, we have the following t heorem.
THEOREM 4.10 (Stability Guarantee). LetA≥1be a conﬁdence parameter. Let ˆβbe
the output of the group-LASSO in (15) above, using Dpost-samples, where
(37)
D≥39·(84Ae)C5
♯(1+ǫ−2+ǫ−4)2/parenleftbiggσmax
σmin/parenrightbigg4
(ǫ3ζ)−2MaL2
0L3log/parenleftbiggML
δ/parenrightbigg
log(2L).
IfTmin≥84eAζ−2L0logL, andLsatisﬁes
(38) nmin=L+84Aeζ−2Llog/parenleftbiggML
δ/parenrightbigg
,
withζ= 6−3ǫ4, then the AR-models — ﬁtted with coefﬁcients ˆβ0returned by Algorithm 1
("AR Coefﬁcient Estimation Pipeline") — are stable, with pr obability as in (34) — in the
following scenarios:
1.when all the time series are different realizations of a uniq ue underlying stochastic pro-
cess,a≥1, and
ˆβ0··=1
MM/summationdisplay
m=1ˆβm;ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 19
2.when all the time series are realizations of different under lying stochastic processes
(equivalently, all the βm’s are different), a≥2, andˆβ0··=ˆβ.
The lower bound on Tminaligns with the upper bound in Equation (24) as L≥L0. The
stated value of nminchooses the smallest value of Lto satisfy both the upper and lower bound
onTmin.
PROOF OF THEOREM 4.10. 1. We start with the ﬁrst case. The idea of the proof is
as follows. All the component ˆβm’s ofˆβare approximations of the same underly-
ingβ0··=βmfor allm∈[M]. Therefore, by the bound in Proposition 4.9 above and
by convexity of the square function, their mean must be ℓ2-close to β0, withD=
O(ML2
0L3log(ML)log(2L))many samples. Moreover, the 0.5L−1ǫperturbation of the
coefﬁcients preserves stability of the ǫ-stable process.
More explicitly, we have
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftiggM/summationdisplay
m=11
Mˆβm/parenrightigg
−β0/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
2=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
MM/summationdisplay
m=1(ˆβm−βm)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
2
convexity ⇒ ≤1
MM/summationdisplay
m=1||ˆβm−βm||2
2
=1
M||ˆβ−β||2
2,
and by Theorem 4.9, this yields
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftiggM/summationdisplay
m=11
Mˆβm/parenrightigg
−β/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
2≤1√
M||ˆβ−β||2
≤81(84Ae)1
2LL0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2/radicaligg
log/parenleftbigML
δ/parenrightbig
MD(39)
with high probability. Note that
(40) α= min
m∈[M]Tmσ2
m
D≥Tminσ2
min
D≥Tminσ2
min
MTmax≥σ2
min
MC♯.
When
D≥39·(84Ae)C5
♯(1+ǫ−2+ǫ−4)2/parenleftbiggσmax
σmin/parenrightbigg4
(ǫ3ζ)−2ML2
0L3log/parenleftbiggML
δ/parenrightbigg
log(2L),
this yields
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenleftiggM/summationdisplay
m=11
Mˆβm/parenrightigg
−β/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
2≤81(84Ae)1
2LL0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2/radicaligg
log/parenleftbigML
δ/parenrightbig
MD
≤ǫσ2
min√
3C♯Mα/radicaligg
1
Llog(2L)
(40)⇒ <ǫ√
3/radicaligg
1
Llog(2L).20
Sinceǫ∈(0,1), the stability follows by the triangle inequality. More exp licitly, if ˆβ0··=
M−1(ˆβ1+···+ˆβM), then for every z∈D, we have :
|fˆβ0(z)|=|1−ˆβ0·(z,z2,···,zL)|
=|1−β0·(z,z2,···,zL)−(ˆβ0−β0)·(z,z2,···,zL)|
≥|fβ0(z)|−||ˆβ0−β0||2·||(z,z2,···,zL)||2
>ǫ−ǫ√
3/radicaligg
1
log(2L)
>0.
Evidently, this shows that the autoregressive process with ˆβ0coefﬁcients is stable.
2. The arguments are similar in the second case. Since
D≥39·(84Ae)C5
♯(1+ǫ−2+ǫ−4)2/parenleftbiggσmax
σmin/parenrightbigg4
(ǫ3ζ)−2M2L2
0L3log/parenleftbiggML
δ/parenrightbigg
log(2L),
by Theorem 4.9, for any m∈[M]we have
||ˆβm−βm||2≤||ˆβ−β||2
≤81(84Ae)1
2LL0σ2
maxC3
2
♯(1+ǫ−2+ǫ−4)
ζαǫ2/radicaligg
log/parenleftbigML
δ/parenrightbig
D
(40)⇒ <ǫ√
3/radicaligg
1
Llog(2L).
As in the ﬁrst case, this implies stability for each of the com ponents.
5. Algorithmic Aspects. In this section we brieﬂy introduce the main gazette of our al -
gorithm — namely, the proximal operator — to understand the p rocedure of solving LASSO
as was done in Nicholson et al. (2020). To sketch the outline o f the standard procedure for
solving regularized LASSO penalized with an overlapping gr oup-norm, we start with the
following deﬁnition (see Mairal et al. (2011), for example) .
DEFINITION 5.1 (Proximal operator). Given a norm NonRML, and a tuning parameter
λ, the associated proximal operator ProxN,λis deﬁned for every α∈RMLas the optimum
value of the following convex problem:
Prox
N,λ(α)··=argmin
β/braceleftbigg1
2||β−α||2
2+λN(β)/bracerightbigg
.
By Zhao et al (2009), the proximal operator ProxN,λ— forNas deﬁned in (12) — is the
composition
Prox
G1,λ◦···◦Prox
GL,λ
of the proximal operators for the individual groups and can b e computed inductively — start-
ing from ProxGL,λ, which is the well-known soft-thresholding — in O(ML)computationalESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 21
steps. By Combettes and Wajs (2005, Proposition 3.1(iii)b) , the solutions to the regularized
LASSO problem in (15) are precisely the ﬁxed points of the ope rator
(41) β/ma√sto−→Prox
N,λ/parenleftbigg
β+2λ
DX⊤(y−Xβ)/parenrightbigg
.
Here we use an appropriate λ(as given in (26) above). As Nicholson et al. (2020) ob-
served, the proximal operator can be evaluated via duality. The proximal gradient method
of Mairal et al. (2011) then ﬁnds the ﬁxed point (which exists and is unique by convexity
of (15)) of this proximal operator. For pseudo-code of this p rocedure, see Nicholson et al.
(2020), where an accelerated version of the proximal descen t method was employed for
achieving quadratic convergence rate.
6. Conclusion. We have established a set-up (see Equation (15) and the discu ssion pre-
ceding this equation) in which LASSO — regularized with a hie rarchical group norm —
can be used to derive statistical guarantees in terms of the o ne-step ahead prediction error
(Theorem 4.2) in the realms of multiple ǫ-stable (Deﬁnition (2.6)) univariate autoregressive
processes of different lengths but identical true lag L0. The results presented here assume no
prior knowledge of the true lag (or any upper-bound of the tru e lag); in fact, we show that
the sample size itself suggests a certain lag ˆLto be used for the group-LASSO, and given
an appropriately large sample size, such that ˆLwill be an upper-bound of the true lag L0.
Moreover, this ˆLwill be of the order that is required for our theoretical guar antees to hold.
We proved that the group-LASSO formulated with a suitable tu ning parameter λestimates
the AR coefﬁcients with an arbitrarily high degree of accura cy. We also showed the support
of the estimated coefﬁcient-set approximately matches the support of the original parameters
(Theorem 4.9). Finally, we proved that the ﬁtted models with coefﬁcients as estimated by the
group-LASSO are ǫ-stable (Theorem 4.10), a property that is known in the liter ature solely
for Yule-Walker estimates of the parameters of univariate a utoregressive processes.
From a theoretical perspective, it will be interesting to in vestigate adaptations of the group-
LASSO method to the case of multiple decoupled AR processes w ith multivariate compo-
nents. We expect that this will require, among others, (1) ad ditional techniques to deal with
the group-norm, and (2) integrating the stability issues an d the restricted eigenvalue issues;
these will be technically far more demanding in the multivar iate components settings. On the
practical front, the most important question is to get a bett er hold on the tuning parameter
λ(see Equation (26)), which requires better constant/param eters than, for example, those
appearing in the proof of Lemma 4.4. A better control on these will enable a more realistic
estimate of ˆL— to be used by the group-LASSO as an upper-bound on the true la gL0.
Funding. S.C. and J.L. acknowledge funding from the Deutsche Forschu ngsgemein-
schaft (DFG) under grant number 451920280. S.C. was partial ly supported by a ‘Re-
search Assistantship Fellowship for Early Postdocs’ from R uhr-Universität Bochum Re-
search School by means of the German Academic Exchange Servi ce (DAAD) STIBET funds.
REFERENCES
Sumanta Basu and George Michailidis: Regularized estimati on in sparse high-dimensional time series models.
Ann. Statist. , 43(4): 1535–1567, 2015.
Florentina Bunea: Honest variable selection in linear and l ogistic regression models via ℓ1andℓ1+ℓ2penaliza-
tion. Electron. J. Stat. 2 , 2008.
Stephen Boyd, Lieven Vandenberghe: Convex Optimization. Cambridge University Press , Cambridge, 2004.
Emmanuel Candes, Terence Tao: Near-optimal signal recover y from random projections: universal encoding
strategies? IEEE Trans. Inform. Theory 52, no. 12, 200622
Patrick Combettes, Valérie Wajs: Signal recovery by proxim al forward-backward splitting. Multiscale Model.
Simul. 4, no. 4, 1168–1200, 2005
Mario Forni, Marc Hallin, Marco Lippi, Lucrezia Reichlin: T he Generalized Dynamic-Factor Model: Identiﬁca-
tion and Estimation. The Review of Economics and Statistics , 82, no. 4, 540–554, 2000.
Johannes Lederer: Fundamentals of High-dimensional Statistics—with exerci ses and R labs . Springer Texts in
Statistics. Springer, Cham, 2021.
Po-Ling Loh and Martin J. Wainwright: High-dimensional reg ression with noisy and missing data: provable guar-
antees with nonconvexity. Ann. Statist. , 40(3):1637–1664, 2012.
Helmut Lütkepohl: New Introduction to Multiple Time Series Analysis . Springer-Verlag, Berlin, 2005.
Julien Mairal, Rodolphe Jenatton, Guillaume Obozinski, an d Francis Bach: Convex and network ﬂow optimiza-
tion for structured sparsity. J. Mach. Learn. Res. , 12:2681–2720, 2011.
Ricardo Masini, Marcelo Medeiros, and Eduardo Mendes: Regu larized estimation of high-dimensional vector
autoregressions with weakly dependent innovations. J. Time Ser. Anal. , 43: 532–557, 2011.
Yuval Nardi and Alessandro Rinald: Autoregressive process modeling via the Lasso procedure. J. Multivariate
Anal. 102, no. 3, 528–549, 2011
Sahand Negahban and Martin J. Wainwright: Estimation of (ne ar) low-rank matrices with noise and high-
dimensional scaling. Ann. Statist. , 39(2):1069–1097, 2011.
William B. Nicholson, Ines Wilms, Jacob Bien, and David S. Ma tteson: High dimensional forecasting via inter-
pretable vector autoregression. J. Mach. Learn. Res. , 21:Paper No. 166, 52p, 2020.
Paul Tseng: On accelerated proximal gradient methods for co nvex-concave optimization. 2008. available at
http://www. math. washington. edu/˜ tseng/papers/apgm. p df.
Roman Vershynin: High-dimensional probability . An Introduction with Applications in Data Science. With a
foreword by Sara van de Geer. Cambridge Series in Statistica l and Probabilistic Mathematics, 47. Cambridge
University Press, Cambridge, 2018.
Kam Chung Wong, Zifan Li, and Ambuj Tewari: Lasso guarantees forβ-mixing heavy-tailed time series. Ann.
Statist. , 48(2), 2020.
Peng Zhao, Guilherme Rocha, Bin Yu: The composite absolute p enalties family for grouped and hierarchical
variable selection. Ann. Statist. 37, no. 6A, 3468–3497, 2009
APPENDIX A: PROOF OF THEOREM 4.2
PROOF . Denote the data by X. Deﬁne the random vector U∈RDas follows:
U··=(U1
1,...,U1
T1,U2
1,...,U2
T2,···,UM
1,...,UM
TM)⊤.
We note the following series of inequalities, all of which fo llow from linearity of expectation:
here, we write βfor the true coefﬁcient vector of the model. One has
E/bracketleftig
||y−Xˆβ||2
2|X/bracketrightig
=E/bracketleftbig
||y−Xβ||2
2|X/bracketrightbig
+E/bracketleftig
||X(β−ˆβ)||2
2|X/bracketrightig
+2E/bracketleftig
/a\}bracketle{ty−Xβ,Xβ−Xˆβ/a\}bracketri}ht|X/bracketrightig
=E/bracketleftbig
||U||2
2|X/bracketrightbig
+E/bracketleftig
||X(β−ˆβ)||2
2|X/bracketrightig
+2E/bracketleftig
/a\}bracketle{tU,Xβ−Xˆβ/a\}bracketri}ht|X/bracketrightig
=T1σ2
1+···+TMσ2
M+E/bracketleftig
||X(β−ˆβ)||2
2|X/bracketrightig
+2/a\}bracketle{tE[U|X],Xβ−Xˆβ/a\}bracketri}ht
=T1σ2
1+···+TMσ2
M+||X(β−ˆβ)||2
2,ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 23
where the last equality follows from E[U|X]=U, andU⊥(Xβ−Xˆβ). Therefore, we
need to derive an estimate of the error ||X(β−ˆβ)||2
2. We notice that
||X(β−ˆβ)||2
2=M/summationdisplay
m=1Tm/summationdisplay
t=1/parenleftiggL/summationdisplay
l=1/parenleftig
ˆβm
l−βm
l/parenrightig
xm
t−l/parenrightigg2
,
where{βm
l}denotes the true-parameters of the models. Using the fact th atˆβis a minimizer
of1
D||y−Xα||2
2+λ/summationtextL
l=1N(α), overα∈RML, one derives
1
D||y−Xˆβ||2
2+λN(ˆβ)≤1
D||y−Xα||2
2+λN(α)
⇒1
D||X(β−ˆβ)||2
2≤1
D||X(β−α)||2
2+2
D/a\}bracketle{tU,X(ˆβ−α)/a\}bracketri}ht+λN(α)−λN(ˆβ)
≤1
D||X(β−α)||2
2+2
D/a\}bracketle{tX⊤U,ˆβ−α/a\}bracketri}ht+λN(α)−λN(ˆβ)
for allα∈RML. By the Cauchy-Schwarz inequality, this implies
1
D||X(β−ˆβ)||2
2≤1
D||X(β−α)||2
2+2
DN⋆(X⊤U)N(ˆβ−α)+λ(N(α)−N(ˆβ))
≤1
D||X(β−α)||2
2+λN(ˆβ−α)+λ(N(α)−N(ˆβ))
sinceλ≥2
DN⋆(X⊤U). Finally, the triangle inequality N(ˆβ−α)≤N(ˆβ)+N(α)estab-
lishes the ﬁrst part of the theorem.
In the following, we will choose a slightly different thresh old for the tuning parameter λ,
as proposed in the statement of theorem. Let v··=β+ˆβ, and notice that the true lag L0
satisﬁes
L0=max{ℓ∈[L]:||β||Gℓ/\e}atio\slash=0}.
We have
N(β)−N(ˆβ)=N(β)−N(β+v)
=L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||β||Gℓ+L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||✁✁ ✕0
β||Gℓ
−L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||β+v||Gℓ−L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||✁✁ ✕0
β+v||Gℓ
=L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||β||Gℓ−L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||β+v||Gℓ−L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||v||Gℓ
≤L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||v||Gℓ−L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||v||Gℓ, (42)
where the last step is due to the triangle inequality: ||β+v||Gℓ≥ ||β||Gℓ−||v||Gℓ. Suppose
thatλ≥4
DN⋆(X⊤U). Using the fact that ˆβis a minimizer of the objective1
D||y−Xα||2
2+
λN(α), overα∈RML, one derives
1
D||y−Xˆβ||2
2+λN(ˆβ)≤1
D||y−Xβ||2
2+λN(β)
⇒1
D||X(β−ˆβ)||2
2≤2
D/a\}bracketle{tX⊤U,ˆβ−β/a\}bracketri}ht+λN(β)−λN(ˆβ).24
By the Cauchy-Schwarz inequality, this implies
1
D||X(β−ˆβ)||2
2≤2
DN⋆(X⊤U)N(ˆβ−β)+λ(N(β)−N(ˆβ))
≤λ
2N(ˆβ−β)+λ(N(β)−N(ˆβ))∵λ≥4
DN⋆(X⊤U)
≤λ
2min/braceleftig
3N(β−ˆβ),3N(β)−N(ˆβ)/bracerightig
. (43)
The inequalities
0≤2
λD||X(β−ˆβ)||2
2≤N(ˆβ−β)+2(N(β)−N(ˆβ)),
and
N(β)−N(ˆβ)≤L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||v||Gℓ−L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||v||Gℓ
yield the following:
0≤N(ˆβ−β)+2(N(β)−N(ˆβ))
≤N(ˆv)+2L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||v||Gℓ−2L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||v||Gℓ
=3L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||v||Gℓ−L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||v||Gℓ.
That is, we have
L/summationdisplay
ℓ=L0+1/radicalbig
|Gℓ|·||β−ˆβ||Gℓ≤3L0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||β−ˆβ||Gℓ. (44)
By equation (43), we then have
1
D||X(β−ˆβ)||2
2≤2λL0/summationdisplay
ℓ=1/radicalbig
|Gℓ|·||β−ˆβ||Gℓ (45)
=2λN≤L0(β−ˆβ).
REMARK (A). If, instead of N, we take standard ℓ2inRML(which correspond to the
largest group in N), the argument leading to (44) yields
L/summationdisplay
ℓ=L0+1|ˆβm
ℓ|=L/summationdisplay
ℓ=L0+1|βm
ℓ−ˆβm
ℓ|
≤3L0/summationdisplay
ℓ=1|βm
ℓ−ˆβm
ℓ|.ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 25
APPENDIX B: PROOF OF THEOREM 4.3
PROOF . Since Lemma D.3 implies
N⋆/parenleftbigg2
DX⊤U/parenrightbigg
≤1√
L/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
DX⊤U/vextenddouble/vextenddouble/vextenddouble/vextenddouble
∞
=1√
Lmax
m∈[M]max
l∈[L]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
DTm/summationdisplay
t=1Um
tXm
t−l/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle,
it sufﬁces, by an union bound argument, to ﬁnd high-probabil ity upper bound of the inner-
product (see (16) for notations)
τm,l··=2/a\}bracketle{tUm,Xm,l/a\}bracketri}ht=2/parenleftiggTm/summationdisplay
t=1Um
tXm
t−l/parenrightigg
. (46)
We now present standard techniques, as in Basu and Michailid is (2015), for example. One
has
τm,l=/vextenddouble/vextenddouble/vextenddoubleU(m)+X(m,l)/vextenddouble/vextenddouble/vextenddouble2
2−/vextenddouble/vextenddouble/vextenddoubleU(m)/vextenddouble/vextenddouble/vextenddouble2
2−/vextenddouble/vextenddouble/vextenddoubleX(m,l)/vextenddouble/vextenddouble/vextenddouble2
2.
Hence, for any η >0, an union bound argument, together with mutual independenc e of the
Gaussian random variable Um
jand the variables Xm
<jfor eachj∈[Tm], implies
P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingleτm,l
D/vextendsingle/vextendsingle/vextendsingle>3
2η/bracketrightbigg
≤P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle(U(m))⊤(U(m))−Var(Um)/vextendsingle/vextendsingle/vextendsingle>D
2η/bracketrightbigg
+P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle(X(m,l))⊤(X(m,l))−Var(Xm)/vextendsingle/vextendsingle/vextendsingle>D
2η/bracketrightbigg
+P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle(U(m)+X(m,l))⊤(U(m)+X(m,l))−Var(Xm+Um)/vextendsingle/vextendsingle/vextendsingle>D
2η/bracketrightbigg
.
Here, Var (Y)denotes the trace of the covariance matrix of the random vari ableY. We now
estimate each summand separately, starting with
pm,l,η··=P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle(U(m))⊤(U(m))−Var(U(m))/vextendsingle/vextendsingle/vextendsingle>D
2η/bracketrightbigg
.
By the running assumptions, the vector U(m)isTm-dimensional mean zero Gaussian having
covariance matrix σ2
mITm; by the inequality in proposition E.1, one has
pm,l,η≤2e−c0Dη
8σ2mmin{1,Dη
8σ2mTm}. (47)
Next, we consider
qm,l,η··=P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle(X(m,l))⊤(X(m,l))−Var(X(m,l))/vextendsingle/vextendsingle/vextendsingle>D
2η/bracketrightbigg
.
Recall that the random vector X(m,l)is mean-zero Gaussian with covariance matrix Γ(m)
mentioned in equation (59) below. One has
Var(X(m,l))=tr(Γ(m))=TmE[(Xm
0)2].
From lemma E.2, one has ||Γ(m)||op≤ǫ−2σ2
m; thus, proposition E.1 yields
qm,l,η≤2e−c0Dη
8σ2mǫ−2min{1,Dη
8σ2mTmǫ−2}. (48)26
Finally, we consider
rm,l,η··=P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingle(U(m)+X(m,l))⊤(U(m)+X(m,l))−Var(Xm+Um)/vextendsingle/vextendsingle/vextendsingle>D
2η/bracketrightbigg
.
Note that U(m)+X(m,l)is a mean zero Gaussian with symmetric covariance matrix ˜Γ(m),
whose(t,s)-entry (for t≥s) is given by
˜Γ(m)
t,s··=E[(Xm
t−l+Um
t)(Xm
s−l+Um
s)]
=E[(Xm
t−lXm
s−l]+E[Xm
s−lUm
t]+E[Xm
t−lUm
s]+E[Um
tUm
s]
=E[(Xm
t−lXm
s−l]+E[Xm
t−lUm
s]+σ2
m1t=s.
Together with lemma E.3, this implies
||˜Γ(m)||op≤ǫ−2σ2
m+ǫ−4σ2
m+σ2
m
=σ2
m(1+ǫ−2+ǫ−4),
and by proposition E.1, we fetch
rm,l,η≤2e−c0Dη
8σ2m(1+ǫ−2+ǫ−4)min{1,Dη
8σ2mTm(1+ǫ−2+ǫ−4)}. (49)
Now suppose that
η≥8C♯σ2
max(1+ǫ−2+ǫ−4)
M;
then, the inequalities in (47), (48), and (49) are simpliﬁed as follows:
(50)pm,l,η≤2e−c0Dη
8σ2m;
qm,l,η≤2e−c0Dη
8σ2mǫ−2,
rm,l,η≤2e−c0Dη
8σ2m(1+ǫ−2+ǫ−4).
Of these, the right hand side is the largest in the bottom-mos t inequality (50). Note that, the
union bound implies
P/bracketleftigg
sup
m,l/vextendsingle/vextendsingle/vextendsingleτm,l
D/vextendsingle/vextendsingle/vextendsingle>3
2η/bracketrightigg
≤/summationdisplay
m,lP/bracketleftbigg/vextendsingle/vextendsingle/vextendsingleτm,l
D/vextendsingle/vextendsingle/vextendsingle>3
2η/bracketrightbigg
≤/summationdisplay
l∈[L]/summationdisplay
m∈[M]P/bracketleftbigg/vextendsingle/vextendsingle/vextendsingleτm,l
D/vextendsingle/vextendsingle/vextendsingle>3
2η/bracketrightbigg
(50)⇒ ≤ 6/summationdisplay
l∈[L]/summationdisplay
m∈[M]e−c0Dη
8σ2m(1+ǫ−2+ǫ−4).
Thus, for any δ>0, in order to have the inequality
P/bracketleftbigg
N⋆/parenleftbigg2
DX⊤U/parenrightbigg
>3η
2√
L/bracketrightbigg
≤δ,
it sufﬁces to have
(51) exp/parenleftbigg
−c0Dη
8σm(1+ǫ−2+ǫ−4)/parenrightbigg
≤δ
MLESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 27
for eachm∈[M]. Equivalently, it sufﬁces to have the following for each m∈[M]:
c0Dη
8σ2m(1+ǫ−2+ǫ−4)≥log/parenleftbiggML
δ/parenrightbigg
equivalently, D≥8σ2
m(1+ǫ−2+ǫ−4)
c0ηlog/parenleftbiggML
δ/parenrightbigg
.
This is equivalent to
D≥8σ2
max(1+ǫ−2+ǫ−4)
c0ηlog/parenleftbiggML
δ/parenrightbigg
. (52)
It then follows from the argument above that
(53) P/bracketleftbigg2
DN⋆(X⊤U)≥3η
2√
L/bracketrightbigg
≤δ
holds, provided
D≥8σ2
max(1+ǫ−2+ǫ−4)
c0ηlog/parenleftbiggML
δ/parenrightbigg
.
APPENDIX C: PROOF OF PROPOSITION 4.6
We ﬁrst observe what happens in the M=1case: this amounts to restricting Xto the top
left block
X1··=
x1
1−1... x1
1−L
·...·
·...·
x1
T1−1... x1
T1−L
.
LEMMA C.1 (Blockwise concentration inequality). For any integer s1>0, write
K(s1)··=B0(s1)∩B1(1). Then
P/bracketleftigg
sup
v1∈K(s1)/vextendsingle/vextendsingle/vextendsinglev⊤
1(X⊤
1X1−T1Γ(1))v1/vextendsingle/vextendsingle/vextendsingle≥T2
1ζ1Λmax(ΣU1)
2m(f)/bracketrightigg
≤2exp/parenleftbigg
−T1min{ζ1,ζ2
1}
2+s1min{logL,log(21eL/s1)}/parenrightbigg
. (54)
PROOF OF LEMMA C.1. Let v1∈RLbe a ﬁxed unit-normed vector. Then, u1··=
X1v1is a random mean-zero Gaussian vector in RT1, with covariance matrix ΣX1v1··=
E/bracketleftbig
X1v1v⊤
1X⊤
1/bracketrightbig. One has
(ΣX1v1)r,s=L/summationdisplay
j,l=1E/bracketleftbig
X1
r−jv1,jv1,lX1
s−l/bracketrightbig
=L/summationdisplay
j,l=1v1,jE/bracketleftbig
X1
r−jX1
s−l/bracketrightbig
v1,l
=v⊤
1Γr,sv1,28
whereΓr,sis the covariance of the vectors X(1)
randX(1)
s. Note that
tr(ΣX1v1)=E/bracketleftig
tr(X1v1v⊤
1X⊤
1)/bracketrightig
=T1E/bracketleftbig
(X1v1)2
11/bracketrightbig
=T1v⊤
1Γ(1)v1,
whereΓ(1)is the autocovariance matrix of {X1
t}. Moreover, one has by Lemma E.2, the
inequality
||ΣX1v1||op≤Λmax(ΣU1)
m(f).
By the inequality in Proposition E.1, one has
P/bracketleftbig/vextendsingle/vextendsingleuT
1u1−tr(ΣX1v1)/vextendsingle/vextendsingle≥4T1ζ1||ΣX1v1||op/bracketrightbig
≤2e−T1
2min{ζ1,ζ2
1}
for anyζ1>0. In particular, this implies that for any ﬁxed v1∈RL, the inequality
P/bracketleftbigg/vextendsingle/vextendsingle/vextendsinglev⊤
1(X⊤
1X1−T1Γ(1))v1/vextendsingle/vextendsingle/vextendsingle≥4T1ζ1Λmax(ΣU1)
m(f)/bracketrightbigg
≤2e−T1
2min{ζ1,ζ2
1}(55)
holds for any ζ1>0. Applying Lemma E.4 with G··=X⊤
1X1−T1Γ(1), we conclude that
P/bracketleftigg
sup
v1∈K(s1)/vextendsingle/vextendsingle/vextendsinglev⊤
1(X⊤
1X1−T1Γ(1))v1/vextendsingle/vextendsingle/vextendsingle≥4T1ζ1Λmax(ΣU1)
m(f)/bracketrightigg
≤2exp/parenleftbigg
−T1min{ζ1,ζ2
1}
2+s1min{logL,log(21eL/s1)}/parenrightbigg
(56)
holds for any integer s1≥1, and any ζ1>0.
Proof of Proposition 4.6.
PROOF . Whens1>0is even, it follows from Lemma E.5 that
P/bracketleftbigg
sup
v1∈RL/vextendsingle/vextendsingle/vextendsinglev1(X⊤
1X1−T1Γ(1))v1/vextendsingle/vextendsingle/vextendsingle≤108T1ζ1Λmax(ΣU1)
m(f)/parenleftbigg
||v1||2
2+2
s1||v1||2
1/parenrightbigg/bracketrightbigg
≥1−2exp/parenleftbigg
−T1min{ζ1,ζ2
1}
2+s1min{logL,log(21eL/s0)}/parenrightbigg
.
An application of Lemma E.2 implies that for
δ··=2e−T1min{ζ1,ζ2
1}
2+s1min{logL,log(21eL/s1)},
the conclusion of the following sequence of inequalities ho lds for all v1∈RLwith probabil-
ity at least 1−δ:
||X1v1||2
2≥T1λmin(Γ(1))||v1||2
2−108T1ζ1Λmax(ΣU1)
m(f)/parenleftbigg
||v1||2
2+2
s1||v1||2
1/parenrightbigg
≥T1Λmin(ΣU1)
M(f)||v1||2
2−108T1ζ1Λmax(ΣU1)
m(f)/parenleftbigg
||v1||2
2+2
s1||v1||2
1/parenrightbigg
=T1/parenleftbiggΛmin(ΣU1)
M(f1)−108ζ1Λmax(ΣU1)
m(f1)/parenrightbigg
||v1||2
2−216T1ζ1Λmax(ΣU1)
s1m(f1)||v1||2
1.ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 29
We specialize to the case of univariate ǫ-stable autoregressive time series, with ΣU1=σ2
1,
and (byǫ-stability)
ǫ2≤m(f1)≤M(f1)≤ǫ−2.
Lettingζ1··=6−3ǫ4, we obtain
P/bracketleftbigg
inf
v1∈RL/parenleftbigg
||X1v1||2
2−T1
2σ2
1ǫ2/parenleftbigg
||v1||2
2−2
s1||v1||2
1/parenrightbigg/parenrightbigg
≥0/bracketrightbigg
≥1−2e−T1
2ζ2
1+s0min{logL,log(21eL/s0)}. (57)
Finally, we consider all the blocks of X⊤X; the deviation inequality above yields
P/bracketleftbigg
∀m∈[M]inf
vm∈RL/parenleftbigg
||Xmvm||2
2−Tmσ2ǫ2
2/parenleftbigg
||vm||2
2−2
sm||vm||2
1/parenrightbigg/parenrightbigg
≥0/bracketrightbigg
≥1−2M/summationdisplay
m=1e−Tmζ2
2+smmin{logL,log(21eL/sm)}. (58)
This proves the proposition.
APPENDIX D: MORE ON DUAL NORMS
SinceN⋆(0)=0 , we will assume, in the following computation of the dual nor m, thatα/\e}atio\slash=
0. This ensures N⋆(α)>0too. Note that N⋆(α)=N⋆(|α|), where|α|=(|α1|,···,|αL|);
this is because Nis invariant under arbitrary sign changes of the coordinate s of its argument.
Thus, for N⋆(α) =α·β, we may assume (without loss of generality) that |α|=αand
|β|=β.
The advantage of the hierarchical group norm N(β)— in the setting of sparse recovery
via regularized least square regression — is that, while it s ets any group of parameters to
zero — because of the hierarchy in the group structure — all va riables in all groups that
appear further down the order of hierarchy are set to zero aut omatically. Thus, these norms
are well-suited for determination of the true lag order in th e context of autoregressive process.
In order to use this group norm in our analysis, we need to coll ect some basic facts on the
norm. Note that the norm resembles ℓ1at the group level, while within each group it is the ℓ2
norm; from this, one might expect that the dual of the norm sho uld “resemble" the ℓ∞-norm
at the group level, and within a group it should be ℓ2. We will prove that this intuition goes
quite well, in the sense that the actual dual norm can be upper -bounded by this mixed ℓ∞,2
norm.
LEMMA D.1 (Dual norm is attained on the boundary).
N⋆(α)= max
N(β)=1/a\}bracketle{tα,β/a\}bracketri}ht.
PROOF OF LEMMA D.1. Note that — by continuity of β/ma√sto→/a\}bracketle{tα,β/a\}bracketri}ht, compactness of the
subsetCβ··={β:N(β)≤1}, and since N⋆(α)/\e}atio\slash= 0 — there is nonzero β0∈Cβsuch
thatN⋆(α) =/a\}bracketle{tα,β0/a\}bracketri}ht. Suppose, if possible, that β0satisﬁesN(β0)<1. In(0,+∞), the30
functionc/ma√sto→ N(cβ0)is strictly increasing (continuous) function; thus, there isc>1such
thatcN(β0)=N(cβ0)≤1; one has
max{/a\}bracketle{tα,β/a\}bracketri}ht:N(β)≤1}≥/a\}bracketle{tα,cβ0/a\}bracketri}ht
=c·/a\}bracketle{tα,β0/a\}bracketri}ht
>/a\}bracketle{tα,β0/a\}bracketri}ht
=N⋆(α),
which is a contradiction.
In order to facilitate our computations of the dual norm in RML, we start with the special
case ofM=1; after Proposition D.2, we will extend this to the general ca se. Now, we deﬁne
N1(α)··=L/summationdisplay
l=1/radicaltp/radicalvertex/radicalvertex/radicalbt(L−l+1)L/summationdisplay
j=lα2
j.
Let us write α≥j=(0,αj,αj+1,···,αL)forj∈[L]. From now on, we assume that
/productdisplay
j∈[L]αj/\e}atio\slash=0.
We have the following proposition.
PROPOSITION D.2 (L∞norm bounds dual norm, M=1case). The following inequality
holds for α∈RL:
N1
⋆(α)≤L−1
2||α||∞.
PROOF OF PROPOSITION D.2. We apply the elementary inequalities
(√
L−l+1||β≥l||2−√
L−l+1|βl|)2≥(L−l+1)||β≥l||2
2−(L−l+1)|βl|2
=(L−l+1)/summationdisplay
j>lβ2
j
=
/radicaligg
(L−l+1)/summationdisplay
j>lβ2
j
2
Cauchy-Schwarz ⇒ ≥
/summationdisplay
j>lβj
2
,
to derive
N1(β)=L/summationdisplay
l=1√
L−l+1||β≥l||2
≥L/summationdisplay
l=1
√
L−l+1|βl|+/summationdisplay
j>l|βj|

=L/summationdisplay
l=1/parenleftig√
L−l+1+l−1/parenrightig
|βl|
≥√
L|βl|.ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 31
SinceN1
⋆(α)≤ ||α||∞N1(1), where1∈RLis the all-one vector, it sufﬁces to show that
N1
⋆(1)≤L−1
2. Since
N1
⋆(1)= max
N1(β)=1/summationdisplay
j∈[L]βj,
and
1=N1(β)
=/summationdisplay
j∈[L]√
L−l+1||β≥j||2
≥√
L/summationdisplay
j∈[L]|βj|,
the claim follows.
With the above proposition dealing with the case M=1, we can formulate in general the
following proposition.
PROPOSITION D.3 (L∞norm bounds dual norm, general case). Supposeα∈RML, and
N(α)=L/summationdisplay
l=1/radicalbig
M(L−l+1)||α(≥l)||2,
where ||α(≥l)||2:=/radicaltp/radicalvertex/radicalvertex/radicalbtL/summationdisplay
j=lM/summationdisplay
m=1α2
(m−1)+j.
ThenN⋆(α)≤L−1
2||α||∞.
PROOF OF PROPOSITION D.3. Again, it sufﬁces to show that N(α)≤L−1
2whenαis
the all-one vector.
Now,N⋆(α)is the optimum value of the problem
maximize/summationdisplay
j∈[ML]βj
subject to N(β)=1.
Writeβ(l)··= (βl,βL+l,···,β(M−1)L+l)⊤andxl··=√
M||β(l)||2forl∈[L]; by Cauchy-
Schwarz, we have
/summationdisplay
j∈[ML]βj≤L/summationdisplay
l=1xl.
Moreover, N(β)=N1(x), wherex··=(x1,···,xL); thus, the lemma follows by D.2.32
APPENDIX E: SOME KNOWN RESULTS USED IN THE PROOFS
The following tail bound on the norm of Gaussian random vecto rs is well-known; see
(Vershynin, 2018, Theorem 6.2.1) for details.
PROPOSITION E.1 (Wright-Hansen inequality). There is an absolute constant c0>0
such that the following statement holds. If q∼N(0,Σ)inRd, whereΣis symmetric positive
deﬁnite, then the following holds for any τ >0:
P/bracketleftbigg1
d/vextendsingle/vextendsingleqTq−tr(Σ)/vextendsingle/vextendsingle≥4τ||Σ||op/bracketrightbigg
≤2e−c0dmin{τ2,τ}.
We need to collect some information about the time series mod el in equation (9). Let
T··=minm∈[M]Tm, and recall that we have written D=T1+···+TM. LetΓ(m,l)be the
covariance matrix of the random vector X(m,l); this is a symmetric matrix, and for any r,s∈
{1−l,...,T m−l}withr≤s, the stationarity of the time series yields
(59) Γ(m,l)
r,s=E[X(m,l)
r−lX(m,l)
s−l]=E[Xm
0Xm
s−r].
Thus, we can write Γ(m)··=Γ(m,l)for the “auto-covariance" matrix.
Now, let further, for any two integers T > L > 0,T,LΓdenote the T1L×T1Lma-
trix, whose (r,s)-th block is the covariance of the vectors (Xr−1,Xr−2,...,X r−L)and
(Xs−1,Xs−2,...,X s−L).
The following lemma gives eigenvalue bounds of these block c ovariance matrices.
LEMMA E.2 (Eigenvalue bound for Toeplitz matrices). Letf(z)denote the reverse char-
acteristic polynomial of a stable univariate AR-process {Xt}. Letm(f) = inf |z|≤1|f(z)|2
andM(f)=sup|z|≤1|f(z)|2. The following inequality holds:
Λmin(Σǫ)
M(f)≤Λmin(T,LΓ)≤Λmax(T,LΓ)≤Λmax(Σǫ)
m(f).
PROOF OF LEMMA E.2. This is immediate from (Basu and Michailidis, 2015, Pro posi-
tion 2.3) if we consider the stable dim-L V AR process Yt··=(Xt−1,...,X t−L).
The following bound appeared as of (Basu and Michailidis, 20 15, Proposition 2.4b).
LEMMA E.3 (Basu-Michailidis). Let∆(m)denote the Tm-dimensional matrix with
∆(m)
t,s··=E[Xm
t−lUm
s]. The following inequality holds:
Λmax(∆(m))≤Λmax(Σǫm)M(fm)
m(f).
The following appeared as (Basu and Michailidis, 2015, Lemm a F.2). Recall that ML(R)
denotes the algebra of all L×Lreal matrices, and SL−1the unit sphere in RL.
LEMMA E.4 (Basu-Michailidis). Suppose that G∈ML(R)is a random symmetric ma-
trix for which the following inequality holds for every u∈SL−1,T1∈N, andη>0:
P/bracketleftbig/vextendsingle/vextendsingleuTGu/vextendsingle/vextendsingle≥Cη/bracketrightbig
≤2e−cT1min{η,η2},
whereC,c>0are parameters independent of ηandu. Then the following inequality holds
for any integer s0>0:
P/bracketleftigg
sup
||u||0≤s0,||u||2≤1/vextendsingle/vextendsingleuTGu/vextendsingle/vextendsingle≥Cη/bracketrightigg
≤2e−cT1min{η,η2}+s0min{logL,log(21eL/s0)}. (60)ESTIMATION OF STABLE PARAMETERS FORMULTIPLE AUTOREGRESSI VE PROCESSES 33
The following result was obtained as (Loh and Wainwright, 20 12, Lemma 12). Recall that,
for any integer s0>0, we denote
K(s0)={v∈RL:||v||2≤1,||v||0≤s0}.
LEMMA E.5 (Loh-Wainwright). Suppose that G∈ML(R)is a symmetric matrix for
which the following holds:
sup
||u||0≤s0,||u||2≤1/vextendsingle/vextendsingleuTGu/vextendsingle/vextendsingle≤δ.
Then
(61) sup
u∈RL/vextendsingle/vextendsingleuTGu/vextendsingle/vextendsingle≤27δ/parenleftbigg
||u||2
2+2
s0||u||2
1/parenrightbigg
.