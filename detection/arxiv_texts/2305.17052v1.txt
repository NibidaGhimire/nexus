A Framework for Incentivized Collaborative Learning
Xinran Wang
Computer Science and Engineering
University of Minnesota-Twin Cities
Minneapolis, MN 55455, USA
wang8740@umn.eduQi Le
Computer Science and Engineering
University of Minnesota-Twin Cities
Minneapolis, MN 55455, USA
le000288@umn.edu
Ahmad Faraz Khan
Computer Science
Virginia Tech
Blacksburg, V A 24060, USA
ahmadfk@vt.eduJie Ding
School of Statistics
University of Minnesota-Twin Cities
Minneapolis, MN 55455, USA
dingj@umn.edu
Ali Anwar
Computer Science and Engineering
University of Minnesota-Twin Cities
Minneapolis, MN 55455, USA
aanwar@umn.edu
Abstract
Collaborations among various entities, such as companies, research labs, AI agents,
and edge devices, have become increasingly crucial for achieving machine learning
tasks that cannot be accomplished by a single entity alone. This is likely due to
factors such as security constraints, privacy concerns, and limitations in computa-
tion resources. As a result, collaborative learning (CL) research has been gaining
momentum. However, a significant challenge in practical applications of CL is
how to effectively incentivize multiple entities to collaborate before any collabora-
tion occurs. In this study, we propose ICL, a general framework for incentivized
collaborative learning, and provide insights into the critical issue of when and
why incentives can improve collaboration performance. Furthermore, we show the
broad applicability of ICL to specific cases in federated learning, assisted learning,
and multi-armed bandit with both theory and experimental results.
1 Introduction
Over the past decade, artificial intelligence (AI) has achieved significant success in engineering and
scientific domains, e.g., robotic control [ 1,2], natural language processing [ 3,4], computer vision
[5,6], and finance [ 7,8]. With this trend, a growing number of entities, e.g., governments, hospitals,
companies, and edge devices, are integrating AI models into their workflows to facilitate data analysis
and enhance decision-making. A market analysis conducted in 2021 revealed that nearly 50% of
companies worldwide are using AI, with an estimated improvement in productivity of 40% [9].
While a variety of standardized machine learning models and computation frameworks are readily
available for entities to implement their AI tasks, the performance of these models is heavily dependent
on the quality and availability of local training data, models, and computation resources [ 10]. For
example, a local bankâ€™s financial modeling performance may be constrained by the small size of its
local population and the limited number of feature variables in the financial sector. However, it is
possible that this bank could improve its modeling quality by integrating additional observations
from other banks (e.g., from other regions) or external feature variables from other industry sectorsarXiv:2305.17052v1  [cs.LG]  26 May 2023(e.g., an e-commerce company that observes the same population). Therefore, there is a strong need
for a modern machine learning framework that allows entities to enhance their model performance
while respecting the proprietary nature of local resources. This has motivated recent research on
collaborative learning approaches, such as Federated Learning (FL) [ 11â€“14] and Assisted Learning
(AL) [15â€“18], which can improve learning performance from distributed data.
1.1 Design goals of incentivized collaborative learning
Machine learning entities, similar to humans, can collaborate to accomplish tasks that benefit each
participant. However, these entities possess local machine learning resources that can be highly
heterogeneous in terms of training procedures, computation cost, sample size, and data quality. A key
challenge in facilitating such collaborations is understanding the motivations and incentives that drive
entities to participate in the first place. For example, a recent study on parallel assisted learning [18,
Section III.D] has demonstrated cases where two entities, Alice and Bob, collaborate to improve
the performance of their distinct tasks simultaneously. However, it may be the case that Alice can
assist Bob more than the other way due to, e.g., data heterogeneity. In such scenarios, an effective
incentive mechanism is crucial for facilitating a â€œbenignâ€ collaboration in which high-quality entities
are suitably motivated to maximize the overall benefit.
The need to deploy collaborative learning systems in the real world has motivated a lot of recent
research in incentivized FL. The existing literature has studied different aspects of incentives in
particular application cases, e.g., using contract theory to set the price for participating in FL, or
evaluating contributions for reward or penalty allocation, which we will review in Subsection 1.3.
However, understanding when and why incentive mechanism design can enhance collaboration
performance is under-explored. This work aims to address this problem by developing an incentivized
collaborative learning framework and general principles that can be applied to specific use cases. Two
examples are given below to show the potential use scenarios that will be revisited in later sections.
Example 1 . Multiple clinics hold data on different patients. They must decide whether to participate
in an FL platform to improve their local prediction performance. Specifically, each participating clinic
will have the chance to be selected as an active participant (if not all clinics due to communication
constraints) to realize an FL-updated model using their local data and computation resources. All
participants will receive the updated model. A clinic has the incentive to participate as long as
the expected model improvement is more valuable than the participation price it must pay (for the
platform, other participants, or both). From the system perspective, the incentive aims to maximize
the model improvement or monetary profit from entitiesâ€™ payments.
Example 2 . Multiple entities collect surveys from the same cohort of customers but in different
modalities, e.g., demographic, activity, and healthcare information. The user data can be collated
by a non-private identifier, such as an encrypted username. They will use assisted learning to
improve predictability by leveraging each otherâ€™s modalities without sharing models. The incentive
mechanism must be designed to enable entities to reach a consensus on which participants to realize
the collaboration gain while the remaining participants will pay others.
This work establishes an incentivized collaborative learning framework to abstract common appli-
cation scenarios. In our framework, a set of learners play three roles as the game proceeds: 1)
candidates , who decide whether to participate in the game based on a pricing plan, 2) participants ,
whose final payment (which can be negative if interpreted as a reward) depends on the pricing plan
and actual outcomes of collaboration, and 3) active participants , who jointly realize a collaboration
gain that is ultimately enjoyed by all participants. The system-level goal is to promote high-quality
collaborations that ultimately maximize an objective, such as maximizing collaboration gain or social
welfare. Examples of the collaboration gain include improved models, predictability, and rewards.
We will use the following design principles of incentives to benefit collaboration:
â€¢Each participant can simultaneously play the roles of a contributor to and a beneficiary of the
collaboration gain, e.g., improvement of models in FL and predictability in AL.
â€¢Each participant will pay to participate in return for a collaboration gain if the improvement over its
local gain outweighs its participation cost.
â€¢The pricing plan, which determines the cost of an entityâ€™s participation, can be positive or non-
positive, tailored to each entity to reward those who contribute positively and charge those who hinder
collaboration or disproportionately benefit from it.
â€¢A system that offers a platform for collaboration may incur a net zero cost, while still engaging
entities to achieve the maximum possible collaborative gains.
2We will demonstrate insights based on this framework for FL, AL, and multi-armed bandit. Without
this unified framework, it would be necessary to design incentive mechanisms for each specific
application case. By employing a unified understanding, we can achieve a modular design that
ensures certain incentive properties. For instance, we will show how incentives can be used to reduce
exploration complexity from a system perspective, ultimately creating win-win situations for all
participants in the collaborative system.
1.2 Contribution of this work
The main contributions of this work are threefold.
â€¢First, we introduce a framework of incentivized collaborative learning (ICL) to capture the nature
of â€œcollaborationâ€ in that eligible entities are incentivized for active participation and collaboration
to benefit the community. We also develop general mechanism design principles in line with the
intuitive aspects discussed earlier.
â€¢Second, we apply the proposed framework and principles to three concrete use scenarios, incen-
tivized FL, AL, and multi-armed bandit (in the Appendix), and quantify the related insights. In
particular, we will demonstrate how the pricing and selection plans can play crucial roles in reducing
the cost of exploration in learning, ultimately creating mutual benefits for all participating entities.
We also conduct experimental studies to corroborate the developed theory and insights.
â€¢Lastly, the unified incentive framework for collaborative learning promotes interoperability among
different collaborative learning settings, allowing researchers and practitioners to adapt the same
architecture across various use cases. These advantages make our approach more appealing than
designing specifically tailored incentive mechanisms for each use case.
Overall, the developed ICL framework helps us understand how to achieve win-win solutions for
entities with unique resources, enabling effective interaction and collaboration for mutual gains.
1.3 Related work
To address the challenges in collaborative learning, the existing studies have focused on various
aspects, such as security [ 19,20], privacy [ 21,22], fairness [ 23,24], personalization [ 25,22], model
heterogeneity [ 26,13], and lack-of-labels [ 14,27]. However, a fundamental question remains: why
would participants want to join collaborative learning in the first place? This has motivated an active
line of research to use incentive schemes to enhance collaboration. We briefly review them below.
Promoter of incentives Who want to design mechanisms to incentivize participants and initiate
a collaboration? From this angle, existing work can be roughly categorized in two classes: server-
centric, meaning that a collaboration is initiated by a server who owns the model and aims to
incentivize edge devices to join model training [ 28â€“31], and participant-centric where the incentives
are designed at the participantsâ€™ interest [32].
Different goals of incentives What is the objective of an incentive mechanism design? Most
existing work on incentivized collaborative learning, in particular FL, have adopted some common
rules for incentive mechanism design, e.g. incentive compatibility andindividual rationality [28,
31]. The eventual objective for incentivized collaboration is often maximizing profit from the
perspective of the incentive mechanism designer, which is either the coordinator (also called â€œserverâ€,
â€œplatformâ€) [ 28] or the participants (also called â€œclientsâ€ in FL) [ 32]. Another commonly studied
objective is maximizing global model performance in FL, which can be commercialized and turned
into profit [ 30,23,33]. Other objectives being studied include budget balance [34,35],computational
efficiency [36, 35], fairness [37], and Pareto efficiency [30].
Components of incentive mechanism design How to implement incentives? The existing work
has focused one of the following machinery: 1) pricing, in which participants bid for collaboration
(using the auction theory) [ 30,35] or a coordinating server determines the price (based on contract
theory) [ 28,38],and 2) payment allocation, based on contribution evaluation [ 39], fair sharing [ 40,24],
rewarding, or local accuracy [41].
We refer to [ 42â€“45] for more literature reviews. Overall, the role of incentives in collaborative
learning has inspired many recent studies on bringing economics and game theoretic concepts to
design better learning platforms. Most existing work in this direction has focused on FL, especially
mobile edge computing scenarios. Nonetheless, the need for collaboration extends beyond FL, as
shown in [ 37] which studied synthetic data generation based on collaborative data sharing, and [ 18]
which developed an AL scheme where an entity being assisted is bound to assist others based on
implicit mechanism design. Moreover, incentivized collaborative learning is under-studied in two
critical aspects. Firstly, how to develop an understanding of the incentive mechanism design from a
3unified perspective, considering the existing work often focuses on specific application scenarios of
collaborative learning? Secondly, when and why do incentives improve collaboration performance?
Prior work has often focused on designing an incentive as a separate problem based an on existing
collaboration scheme, instead of treating incentive as part of the collaborative learning itself. These
gaps motivated us to establish a unified treatment of incentivized collaborative learning. More
discussions on the motivations of the framework are included in the Appendix.
2 Formulation of Incentivized Collaborative Learning (ICL)
2.1 Overview of ICL and intuitions
We will focus on a generic round and provide an overview of the ICL formulation below. As
illustrated in Fig. 1, a collaboration consists of four stages. In Stage 1, the coordinator sets a pricing
CandidatesParticipantsActive Participants
Coordinator
PricingPlanCollaboration
InformationSelection PlanPayment    Stage 1Stage 2Stage 3Stage 4Gain
Figure 1: An overview of the incentivized col-
laborative learning gameplan based on prior knowledge of the candidatesâ€™
potential gains (e.g., from previous rounds), and
each candidate decides whether to be a participant
by committing a payment at the end of this round.
In Stage 2, the coordinator collects participantsâ€™
information (e.g., their estimated gains) and uses
a selection plan to choose the active participants.
In Stage 3, the active participants collaborate to
produce an outcome, which is enjoyed by all par-
ticipants (including non-active ones). In Stage 4,
the coordinator charges according to the pricing
plan, the realized collaboration gain, and individ-
ual gains of active participants. Here, a gain (e.g., decrease in test loss) is assumed to be a function of
the realized outcome (e.g., trained model).
2.2 Detailed explanations of the ICL components
The proposed collaborative learning game includes two parties: candidate entities (or â€œcandidatesâ€)
and a coordinator who interact through a collaboration mechanism. For notational convenience, we
will first introduce a single-round game and extend to multi-round cases in Section 3.
Candidates : Consider Mcandidates indexed by [M]âˆ†={1, . . . , M }. In an ICL game, each candidate
mcanpotentially produce an outcome xmâˆˆ X, such as a model parameter. Any element in Xcan
be mapped to a gain ZâˆˆR, e.g., reduced prediction loss. But such a gain will not necessarily be
realized unless the candidate becomes an active collaborator of the game. At the beginning of a
game, a candidate will receive a pricing plan from the coordinator specifying the cost of participating
in the game and use that to decide whether to become a participant of the game. If a candidate
participates, it has the opportunity to be selected as an active participant . All active participants will
then collaborate to produce an outcome (e.g., model or prediction protocol), which also generates a
collaboration gain. This outcome is distributed among all participants to benefit them. At the end of
the game, all participants must pay according to the pre-specified pricing plan, with the actual price
depending on the realized collaboration gain.
We let IPandIAdenote the set of participants and active participants, respectively (so IAâŠ†IPâŠ†[M]).
Given the above, an entity has a consumer-provider bi-profile in the sense that it can serve as a
consumer who wishes to benefit from and also a provider who contributes to the collaboration.
Coordinator : A coordinator, e.g., company, government agency, or platform, orchestrates the game
by performing the following actions in order: determine a pricing plan of the participation costs based
on initial information collected from candidates, select providers from those candidates that have
chosen to become participants, realize the collaboration gain, and charge the participants according
to the realized collaboration gain. The coordinator can be a virtual entity rather than a physical one.
Collaboration gain : Given active participants represented by IA, we suppose the collaborative gain
is a function of their individual outcomes, denoted by G: (xm, mâˆˆIA)7â†’zIAâˆˆR. This gain will
be enjoyed by all participants and the coordinator, e.g., in the form of an improved model distributed
by the coordinator. With a slight abuse of notation, we use G:xm7â†’zmâˆˆR,mâˆˆ[M], to denote
the gain of an individual outcome.
Pricing plan : Here, the pricing plan is a function from R|IA|+1toR|IP|that maps the collaboration
gain and individual gains of active participants to a cost needed to participate in the game, denoted by
P: (z, zm, mâˆˆIA)7â†’(cj, jâˆˆIP), (1)
4where zdenotes the realized collaboration gain. In practice, we may parameterize Pso that it is
low for active and good-performing entities, medium for non-active entities, and high for active and
laggard/disruptive entities, a point we will demonstrate in the experiments. We assume that the active
participants will share their individual gains, namely zm, mâˆˆIA, so that all other participantsâ€™ cost
can be evaluated. The Pwill provide incentives to each candidate to decide to participate or not. As
such, we denote the set of participants by IP=Incent m(P).
Profit : For each party, the profit will consist of two components: monetary profit from participation
fees and gain-converted profit from provider-generated gains. More specifically, let cmdenote the
final participation cost for entity m. Let the Utility-Income (UI) function z7â†’ U(z)determine the
amount of income uniquely associated with any particular gain Z. We suppose the UI function is the
same for participants and the system. Then, the profit of client misâˆ’cm+U(z)âˆ’U(zm)(where the
last term contrasts with its standalone learning) if it is a participant and zero otherwise, expressed by
PROFIT mâˆ†= 1mâˆˆIPÂ·(âˆ’cm+U(zIA)âˆ’ U(zm)), (2)
where zIAâˆ†=G(xm, mâˆˆIA). We define the system-level profit as the overall income from participa-
tion,P
mâˆˆIPcm, weighted plus the amount converted from collaboration gain, U(zIA), namely
PROFIT sysâˆ†=Î»X
mâˆˆIPcm+U(zIA), (3)
where Î»â‰¥0is a pre-specified control variable that balances the monetary income and collaboration
gain. We can regard the system-level profit as the coordinatorâ€™s profit.
Remark 1 (Coordinatorâ€™s profit) .One may put additional constraints on the coordinatorâ€™s monetary
incomeP
mâˆˆIPcm. A particular case is to restrict thatP
mâˆˆIPcm= 0,which may be interpreted that
the system does not need actual monetary income but rather uses the mechanism for model improve-
ment. This is typical in coordinator-free decentralized learning (to be illustrated in Section 3.2).
Selection plan . The coordinator will select the active participants IAfromIPbased on a set of
available information, denoted by I. We assume that the Iconsists of the coordinatorâ€™s belief of
the distributions of xm(namely the realizable gain) for each client minIP. The selection plan is a
function that maps from IandIPto a set IAâŠ†IP, denoted by
S: (I,IP)7â†’IA. (4)
This can be a randomized map, e.g., when each participant is selected with a certain probability
(Section 2.4.2). In practice, Imay refer to the coordinatorâ€™s estimates of the underlying distribution
ofxm,mâˆˆIP, based on historical performance on the participant side.
Objective of mechanism design . Our objective in designing a collaboration mechanism is to
maximize the system-level profit under constraints tied to candidatesâ€™ individual incentives, which
will be revisited in Section 2.4. The maximization is over the pricing plan Pand selection plan S.
With the earlier discussions, the objective is to solve
Objective: max
P,SE
Î»X
mâˆˆIPcm+U(zIA)
,where (5)
cmis specified by Pin (1) , zIA=G(xm, mâˆˆIA),IA=S(I,IP),s.t.IP=Incent m(P).(6)
We will elaborate on (6) in Section 2.3.
Remark 2 (Interpretation of the objective) .We discuss three interesting cases of the objective. First,
when Î»= 1, the above objective is equivalent to maximizing the overall profit. Second, when Î»= 0,
the above objective is to improve the modeling through a collaboration mechanism. In this case,
the system has no interest in the participation income but only provides a platform to incentivize
the non-active to pay for the gain obtained by the active. Since the system need not pay for any
participants, it is natural to assume the â€œzero-balanceâ€ constraintP
mâˆˆIPcm= 0. Thus, we have
Objective: max
P,SE{U(zIA)}. (7)
Third, as Î»â†’ âˆ , the objective reduces to maximizing the system profit, maxP,SE{P
mâˆˆIPcm}.
Intuitively, the collaboration gain should still be reasonable to attract sufficiently many participants.
Lastly, the following proposition shows that by properly replacing the Î», the systemâ€™s objective can
be interpreted as an alternative objective that combines the systemâ€™s and participantsâ€™ gains.
Proposition 1. LetÎ»â€²âˆ†=Î»âˆ’1
|IP|+1. The Objective (5) where Î»â€²is replaced with Î»is equivalent to
maximizing the average social welfare defined by (PROFIT sys+P
mâˆˆIPPROFIT m)/(|IP|+ 1) .
52.3 Incentives of participation
We study the incentives of collaboration from the candidatesâ€™ perspectives. First, we will elaborate on
(6) here. For each candidate, the incentive to become a participant is the larger profit of receiving the
collaboration gain compared with realizing a gain on its own. Then, candidate mhas the incentive to
participate in the game if and only if
Incent m:E
âˆ’cm+U(zIA)âˆ’ U(zm)	
â‰¥0, (8)
where zIAandcmwere introduced in (6). Here, Edenotes the expectation regarding the random
quantities, including the active participant set and the realized gains.
Remark 3 (Inaccurate candidate) .A candidate may have its own expectation Emin place of Ein (8)
when making its decision. In this case, if the candidate is overly confident about the collaboration
gain â€“ its expected ztends to be larger than the actual, either intentionally or not â€“ it will participate
in the game. The system can have a further screening of it: 1) if this participant is selected as an active
participant, it will likely suffer from a penalty since its realized gain will be seen by the coordinator,
which will implicitly give feedback as an incentive to that candidate; 2) if not selected, it will become
an inactive participant with zero gain, which will contribute to the systemâ€™s profit but not harm the
collaboration. In this way, a candidate will have a limited extent to harm the system.
2.4 Mechanism design for the ICL game
The idea of mechanism design in economic theory is to devise mechanisms to jointly regulate the
decisions of multiple parties in a game to eventually attain a systemâ€™s desired goal (see, e.g., [ 46]). In
our ICL game, the systemâ€™s desired goal is to maximize (5), and the mechanisms to design include P
andS. Section 2.3 discussed the incentives from the candidatesâ€™ view. This subsection studies the
mechanism designs from the systemâ€™s perspective.
2.4.1 Pricing plan: from candidates to participants
From the systemâ€™s view, we can cast the Mcandidates and the coordinator as the parties in a game.
Consider the following strategy choices of each party. Each candidate mhas two choices: whether to
participate or not, represented by bmâˆ†= 1mâˆˆIPâˆˆ {0,1}; the coordinator has a choice of the pricing
and selection plans, denoted by (P,S). Following the notation in (4), for a set of participants that
exclude m, denoted by I(âˆ’m)
P , we let I(âˆ’m)
Aâˆ†=S(I,I(âˆ’m)
P)andIAâˆ†=S(I,I(âˆ’m)
Pâˆª {m}). We have
the following condition under a Nash equilibrium.
Theorem 1 (Equilibrium condition) .The condition for a strategy profile to attain Nash equilibrium is
Incent m:E
âˆ’cm+U(zIA)âˆ’ U(zm)	
â‰¥0,if and only if mâˆˆIP, (9)
Incent sys:E
Î»cm+U(zIA)âˆ’ U(zI(âˆ’m)
A)	
â‰¥0,if and only if mâˆˆIP. (10)
Remark 4 (Pricing as a part of the collaborative learning) .A critical reader may wonder why not
price participants directly based on the realized gains, which we refer to as post-collaboration pricing,
e.g., using the Shapley value [see, e.g., 47,48]. The main distinction is that our studied pricing plan
can not only generate profit or reallocate resources on the system side but also influence collaboration
gains. Specifically, the pricing plan can screen higher-quality candidates to allow the coordinator to
improve model performance in the subsequent collaboration. For instance, consider the case where
the sole purpose is to maximize collaboration gain, namely Î»= 0. In this situation, an entity m
violating the condition in (10) is treated as a laggard, and cmcan be designed accordingly to ensure
this client will not participate, as per violating (9).
2.4.2 Selection plan: from participants to active participants
We introduce a general probabilistic selection plan. Assume the information transmitted from
participant mis a distribution of xm, denoted by Pmfor all mâˆˆIP. Suppose the system expects to
select Ïâˆˆ(0,1]proportion of the participants. Consider a probabilistic selection plan that will select
each client minIPwith probability qmâˆˆ[0,1]. Letq= [qm]mâˆˆIP. We thus have the constraint
qâˆˆQ(Ï,IP)âˆ†=
q:X
mâˆˆIPqm=Ï|IP|
. (11)
Letbmdenote an independent Bernoulli random variable with P(bm= 1) = qm, orbmâˆ¼B(qm).
Then, conditional on the existing participants IP, maximizing any system objective, e.g., (5) and (7),
6will lead to a particular law of client selection represented by q. For example, for the objective (7),
we may solve the following problem.
qâˆ—= arg max
qâˆˆQ(Ï,IP)U(q)âˆ†=E
U(zIA) =U(G(xm, mâˆˆIA))
,
where the expectation is over bmâˆ¼B(qm), xmâˆˆ Pm, andIA={mâˆˆIP:bm= 1}. We will show
specific examples in Section 3. It is worth mentioning that existing works have examined client
sampling from perspectives other than incentives, such as minimizing gradient variance [49].
Remark 5 (Free-rider and adversarial participants) .We will briefly discuss how pricing and selection
plans can jointly address concerns regarding free-riders and adversarial participants. A free-rider
is an entity mwith a low local gain zmbut hopes to participate to enjoy the collaboration gain
realized by other more capable participants with a relatively small participation cost. To that
end, the entity may deliberately inform the system of a poor Pmso that the system, if following
the above-optimized selection plan, will not select it as active. Consequently, the free-riderâ€™s
actual local gain will not be revealed and may not suffer a high participation cost. This case
motivates us to adopt a random selection to a certain extent in selecting the active participants.
More specifically, suppose every participant mâˆˆIPwill have at least a Â¯Ï > 0probability of
being selected to be active. Then, it expects to pay at least Â¯ÏÂ·E{cm}= Â¯ÏÂ· P(z, zi, iâˆˆIA)in
return for an additional model gain of E{U(zIA)âˆ’ U(zm)}, where IAcontains m. Thus, it is not
worth the entity mâ€™s participation should the system design a cost function that meets the following:
Â¯ÏÂ·E{P(z, zm, mâˆˆIA)} â‰¥E{U(zIA)âˆ’U(zm)}for all zmoverly small. For example, the coordinator
may impose a high cost whenever the realized local gain zmrevealed after the collaboration exceeds
a pre-specified threshold. On the other hand, there may be an adversarial participant with a poor local
gain but informs the system of an excellent Pmso that the system will select it to be active. In such
cases, the same argument regarding the choice of the pricing plan applies, so no adversarial entity
would dare to risk paying an excessively high cost after participating in the game.
3 Use cases of collaborative learning
3.1 ICL for federated learning
Federated learning (FL) [ 11â€“13] is a popular distributed learning framework where the main idea
is to learn a joint model using the averaging of locally learned model parameters. Its original
goal is to exploit the resources of massive edge devices (also called â€œclientsâ€) to achieve a global
objective orchestrated by a central coordinator (â€œserverâ€) in a way that the training data do not
need to be transmitted. In line with FL, we suppose that at any particular round, the outcome of
client m,xm, represents a model. The collaboration will generate an outcome in an additive form:
zIAâˆ†=G(xm, mâˆˆIA) =G(P
iâˆˆIAÎ¶ixi/P
iâˆˆIAÎ¶i),where Î¶iâ€™s are the pre-determined unnormalized
positive weights associated with all the candidates, e.g., according to the sample size [ 12] or uncer-
tainty quantification [ 25]. Without loss of generality, we let IPâˆ†= [K], where Kâ‰¤Mis the number
of participants, and Mis the number of candidates.
3.1.1 Large-participation approximation
We assume the selection plan is based on a random sampling of IPwith a given probability, say
Ïâˆˆ(0,1), for each participant to be active. Using the previous notation, we assume bm, formâˆˆ[K],
are IID Bernoulli random variables with P(bm= 1) = Ï. LetU â—¦ G denote the composition of Uand
G, and(U â—¦G)â€²its derivative. Then, we have the following result that will approximate the equilibrium
conditions in Section 2.4 in the presence of a large number of participating clients. Let Â¯Î¶IPandÂ¯xIP
denote all the participantsâ€™ average of Î¶and weighted average of X, namely Â¯Î¶IPâˆ†= (P
iâˆˆIPÎ¶i)/|IP|,
Â¯xIPâˆ†= (P
iâˆˆIPÎ¶ixi)/(P
iâˆˆIPÎ¶i).
Theorem 2. Assume max{|Î¶m|,âˆ¥Î¶mxmâˆ¥âˆ} â‰¤Ï„for all mâˆˆIPandlogd/|IP| â†’0as|IP| â†’ âˆ .
Then, the equilibrium conditions (9) and (10) can be written as
E{cm} â‰¤E
(U â—¦ G )(Â¯xIP+op(1))âˆ’ U â—¦ G (xm)
, (12)
Î»E{cm} â‰¥E
(U â—¦ G )â€²(Â¯xIP+op(1))bm
(1 +op(1))ÏÃ—Î¶m
KÂ¯Î¶IP(Â¯xIPâˆ’xm)
,
where op(1)denotes a generic term whose â„“1-norm converges in probability to zero as the number of
participants |IP|goes to infinity.
7The proof is based on large-sample analysis, e.g., to show that (P
iâˆˆIPbiÎ¶ixi)/(P
jâˆˆIPbjÎ¶j)is
asymptotically close to Â¯xIPfor large |IP|. From the above result and its proof, the systemâ€™s marginal
profit increase attributed to an entity mis approximately
bmÂ·E
Î»cmâˆ’(U â—¦ G )â€²(Â¯xK)Î¶m
KÂ¯Î¶K(Â¯xKâˆ’xm)
. (13)
3.1.2 Iterative mechanism design
To optimize the mechanism in practice, the server cannot evaluate the collaboration gain G(xm, mâˆˆ
IA)due to its complex dependency on the individual outcomes xm. Alternatively, the server may
optimize its mechanism by maximizing the sum of the quantities in (13) as a surrogate of the
collaboration gain. We will elaborate on this idea in an incentivized FL algorithm in Appendix D.
The determination of the pricing plan will involve multiple candidatesâ€™ choices. Since a practical FL
system involves multiple rounds, we suppose each client will use the results from the previous rounds
to approximate the current strategy set and make the next moves.
Remark 6 (Random sampling for the noninformative scenario) .We will show by an example that
if the information is noninformative, random sampling is close to the optimal selection strategy
following the discussions in Section 2.4.2. To illustrate the point, we suppose the information
transmitted from participant mis the mean and variance of xmâˆˆR, denoted by Âµm, Ïƒ2for all
mâˆˆIP, so a large Ïƒ2means less information. The following result shows that the random selection
mechanism is close to the optimal when Ïƒis large.
Proposition 2. Assume the gain is defined by G(x)âˆ†=âˆ’E(xâˆ’Âµ)2, where Âµrepresents the underlying
parameter of interest, and the participantsâ€™ weights Î¶mâ€™s are the same. Assume that Ïƒ2/(|IP| Â·
max mâˆˆIP(Âµmâˆ’Âµ)2)â†’ âˆ as|IP| â†’ âˆ . Then, we have U(q)/U(qâˆ—)â†’p1as|IP| â†’ âˆ .
3.2 ICL for assisted learning
Assisted learning (AL) [ 15,17,18] is a decentralized learning framework that allows organizations to
autonomously improve their learning quality within only a few assistance rounds and without sharing
local models, objectives, or data. Unlike FL schemes, AL is 1) decentralized in that there is no global
model to be shared or synchronized among all the entities in the training and 2) assistance-persistent
in the sense that an entity still needs the output from other entities in the prediction stage. From
the perspective of incentivized collaboration, the above naturally leads to two considerations with
complementary insights into the pricing and selection plans compared with FL in Section 3.1.
â€¢Consideration 1: Autonomous incentive design without a coordinator Since each entity can initiate
and terminate assistance, it is natural to consider a coordinator-free scenario, where entities can
autonomously reach a consensus on collaboration partners based on their pricing plans.
â€¢Consideration 2: Limited information for incentive design In AL, an entity aims to seek assistance
to enhance prediction performance without sharing proprietary local models. Thus, we suppose the
communicated information for collaboration is limited to gains ( z) rather than outcomes ( x).
To put it into perspective, we will study a three-entity setting in Section 3.2.1 to develop insights into
the incentive that allows for a consensus on collaboration in Stage 1 (Fig. 1). In Section 3.2.2, we
will further study a multi-entity setting where multiple less-competitive participants are allowed to
enter Stage 2 to enjoy the collaboration gain, but they will not compete for being active participants.
To numerically study how the incentive affects collaboration and model gains, we will apply our ICL
results to the parallel assisted learning (PAL) framework [ 18] to develop an incentivized PAL. In
this framework, entities solicit assistance from others who observe the same cohort of subjects but
possibly different modalities to benefit their own tasks simultaneously. For example, consider two
entities, say Alice and Bob, with regression tasks. Such an â€œassistanceâ€ is operated in the following
way: In the training stage, Alice calculates the residuals that approximate the under-fitted part of the
data and send them to Bob. Then, Bob will mix them with his task labels to fit a local model and
send the updated residuals back to Alice, who will then fit her local model. The above round repeats
until a stop criterion is triggered, e.g., when the validation loss of any entity no longer decreases. In
the prediction stage, Alice and Bob must jointly decode their respective prediction results based on
their locally trained models from each round. As a result, entities need not share models but share
modality-specific pieces of prediction results. In practice, every entity may decide who to collaborate
with based on the empirical gains in earlier rounds and reach a consensus to perform PAL in each
round. The detailed algorithms and experimental studies are included in Appendix E.
83.2.1 Consensus of competing candidates
In this section, we study three candidate entities, Alice, Bob, and Carol, and suppose each candidate
aims to maximize its profit. We suppose a collaboration (round) can only consist of two entities.
Then, the collaboration will only occur when two out of the three, say Alice and Bob, can maximally
assist each other. From Aliceâ€™s perspective, Carol is less competitive than Bob, and meanwhile, from
Bobâ€™s perspective, Carol is less competitive than Alice.
We will provide the necessary and sufficient conditions for the above setting to reach a consensus.
We suppose each entity has its own payment plan: entity iwill pay a price pi(zâˆ’zi)for any given
collaboration gain zand its local gain (without assistance) zi, for all iâˆˆ[M]. So, if entities iand
jcollaborate, the actual price iwill pay is pi(zâˆ’zi)âˆ’pj(zâˆ’zj). The goal of each entity is
to maximize the expected gain-converted profit minus the participation cost, namely the quantity
in (2). For simplicity, suppose pi(âˆ†z) =ciÂ·âˆ†zandU(z) =uÂ·z. LetÂµi,jâˆ†=E(U(zi,j))denote the
expected income of the collaboration gain formed by entities iandj, andÂµjâ†iâˆ†=E(U(zi,j)âˆ’U(Zj))
the additional gain brought by itoj. With the above setup, we have the following result.
Theorem 3. A consensus on collaboration exists if and only if there are two entities 1and2satisfying
(uâˆ’c1)Âµ1,2+c2Âµ2â†1â‰¥(uâˆ’c1)Âµ1,3+c3Âµ3â†1, (14)
(uâˆ’c2)Âµ2,1+c1Âµ1â†2â‰¥(uâˆ’c2)Âµ2,3+c3Âµ3â†2.
Inequality (14) can be interpreted that the total gain of entity 1received from entity 2, which consists
of the collaboration-generated gain (uâˆ’c1)Âµ1,2and the pricing-based gain c2Âµ2â†1, is no larger than
that from entity 3. To show the effect of pricing, we develop an incentivized PAL algorithm where
each round encourages mutual assistance between a pair of entities. The details are in Appendix E.
3.2.2 Consensus of non-competing candidates
We will further study a multi-entity setting where multiple less-competitive participants are allowed to
enjoy the collaboration gain, but they will not compete for being active participants. The objective is
to develop an incentive to maximize the collaboration gain, namely Objective (7), that will eventually
benefit all the participants. For ease of presentation, we will consider only one active participant
(namely |IA|= 1). In general, we may regard a set of participants as one â€œmegaâ€ participant.
Following the above two considerations of AL, we will first study the following setup. Suppose K
entities decide to participate in a collaboration, where one of them will be selected to realize the
collaboration gain. For example, if participant mis active, it will realize a model gain zâˆ¼ G(xm),
where xmâˆ¼ P mis the potential outcome of participant m. LetPâˆ—
mdenote the distribution of zm
induced by Pmformâˆˆ[K]and suppose they are the shared information among participants (recall
Consideration 1). We note that the randomness of zmmay arise from limited test data, random seed,
or other sources of uncertainty, and Pâˆ—
mmay be empirically approximated in practice. Moreover,
following Consideration 1, the participation costs can have a zero balance, namelyP
mâˆˆ[K]cm= 0.
Following the notation in (1), we consider the following particular pricing plan:
P:z7â†’C(z) 1jÌ¸âˆˆIAâˆ’(Kâˆ’1)C(z) 1jâˆˆIA, jâˆˆIP, (15)
where Cis non-increasing so that the less cost (or equivalently, more reward) is associated with a
larger gain. In other words, each of the Kâˆ’1non-active participants will pay a cost of C(z), which
depends on the realized z, to the active participant. Then, a necessary and sufficient condition to
reach a consensus among the participants is the existence of a participant, say participant 1, such
that when it is active, 1) the collaboration gain is maximized, and 2) every participant sees that its
individual profit is maximized. More specifically, we have the following result.
Theorem 4. Assume Uis a pre-specified nondecreasing function. Consider the pricing plan in (15)
where Ccan be any non-negative and non-decreasing function. Let Âµmâˆ†=EPâˆ—m{U(zm)}denote the
expected gain of participant mwhen it is active, mâˆˆ[K]. The necessary and sufficient condition for
reaching a pricing consensus is the existence of a participant, say participant 1, that satisfies
Âµ1âˆ’ujâ‰¥EPâˆ—
1{C(z1)}+EPâˆ—
j{(Kâˆ’1)C(zj)} (16)
for all jÌ¸= 1. In particular, assume the linearity U(z)âˆ†=uÂ·zandC(z) =cÂ·z. Inequality (16) is
equivalent to câ‰¤minjÌ¸=1uÂ·(Âµ1âˆ’Âµj)/(Âµ1+ (Kâˆ’1)Âµj).
94 Conclusion
As the demand for machine learning tasks continues to grow, collaboration among entities becomes
increasingly important for enhancing their performance. We proposed a framework of incentivized
collaborative learning to study how entities can be properly incentivized to collaborate and create
common benefits. While our work provides a foundation for incentivizing collaborative learning,
there are several limitations worth further investigation. For instance, future studies could examine
the functional forms of pricing plans, use cases to promote model security [ 50] and privacy [ 51â€“53],
and potential trade-offs between collaboration and competition in collaborative learning contexts. The
Appendix contains more discussions on the ICL framework and related work, detailed use cases and
experiments of FL, AL, and collaborative multi-armed bandit, ethics discussion, and all the proofs.
References
[1] M. P. Deisenroth, G. Neumann, and J. Peters, A Survey on Policy Search for Robotics , 2013.
[2]J. Kober and J. Peters, Reinforcement Learning in Robotics: A Survey . Springer, 2014, pp.
9â€“67.
[3]J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao, â€œDeep reinforcement learning
for dialogue generation,â€ in Proc. Conference on Empirical Methods in Natural Language
Processing , 2016, pp. 1192â€“1202.
[4]D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y . Bengio, â€œAn
actor-critic algorithm for sequence prediction,â€ in Proc. International Conference on Learning
Representations , 2017.
[5]F. Liu, S. Li, L. Zhang, C. Zhou, R. Ye, Y . Wang, and J. Lu, â€œ3dcnn-dqn-rnn: A deep rein-
forcement learning framework for semantic parsing of large-scale 3d point clouds,â€ in Proc.
International Conference on Computer Vision (ICCV) , 2017, pp. 5679â€“5688.
[6]G. Brunner, O. Richter, Y . Wang, and R. Wattenhofer, â€œTeaching a machine to read maps with
deep reinforcement learning,â€ in Proc. Association for the Advancement of Artificial Intelligence
(AAAI , 11 2017.
[7]J. Lee, R. Kim, S.-W. Yi, and J. Kang, â€œMaps: Multi-agent reinforcement learning-based portfo-
lio management system.â€ in Proceedings of the Twenty-Ninth International Joint Conference on
Artificial Intelligence, IJCAI-20 , 7 2020, pp. 4520â€“4526.
[8]J. Lussange, I. Lazarevich, S. Bourgeois-Gironde, S. Palminteri, and B. Gutkin, â€œModelling
stock markets by multi-agent reinforcement learning,â€ Computational Economics , vol. 57, 01
2021.
[9]Financesonline, â€œMarket share & data analysis,â€ https://financesonline.com/
machine-learning-statistics/, 2021, accessed: 2021-01-18.
[10] I. J. Goodfellow, Y . Bengio, and A. Courville, Deep Learning . Cambridge, MA, USA: MIT
Press, 2016.
[11] J. Konecny, H. B. McMahan, F. X. Yu, P. RichtÃ¡rik, A. T. Suresh, and D. Bacon, â€œFederated
learning: Strategies for improving communication efficiency,â€ arXiv preprint arXiv:1610.05492 ,
2016.
[12] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, â€œCommunication-efficient
learning of deep networks from decentralized data,â€ in Proc. AISTATS , 2017, pp. 1273â€“1282.
[13] E. Diao, J. Ding, and V . Tarokh, â€œHeteroFL: Computation and communication efficient federated
learning for heterogeneous clients,â€ in International Conference on Learning Representations ,
2021.
[14] â€”â€”, â€œSemiFL: Communication efficient semi-supervised federated learning with unlabeled
clients,â€ in Advances in Neural Information Processing Systems , 2022.
10[15] X. Xian, X. Wang, J. Ding, and R. Ghanadan, â€œAssisted learning: A framework for multi-
organization learning,â€ in Advances in Neural Information Processing Systems , vol. 33, 2020.
[16] C. Chen, J. Zhou, J. Ding, and Y . Zhou, â€œAssisted learning for organizations with limited data,â€
Transactions on Machine Learning Research , 2023.
[17] E. Diao, J. Ding, and V . Tarokh, â€œGAL: Gradient assisted learning for decentralized multi-
organization collaborations,â€ in Advances in Neural Information Processing Systems , 2022.
[18] X. Wang, J. Zhang, M. Hong, Y . Yang, and J. Ding, â€œParallel assisted learning,â€ IEEE Transac-
tions on Signal Processing , vol. 70, pp. 5848â€“5858, 2022.
[19] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, â€œAnalyzing federated learning through
an adversarial lens,â€ in International Conference on Machine Learning . PMLR, 2019, pp.
634â€“643.
[20] E. Bagdasaryan, A. Veit, Y . Hua, D. Estrin, and V . Shmatikov, â€œHow to backdoor federated
learning,â€ in International Conference on Artificial Intelligence and Statistics . PMLR, 2020,
pp. 2938â€“2948.
[21] S. Truex, N. Baracaldo, A. Anwar, T. Steinke, H. Ludwig, R. Zhang, and Y . Zhou, â€œA hybrid
approach to privacy-preserving federated learning,â€ in Proceedings of the 12th ACM workshop
on artificial intelligence and security , 2019, pp. 1â€“11.
[22] Q. Le, E. Diao, X. Wang, A. Anwar, V . Tarokh, and J. Ding, â€œPersonalized federated recom-
mender systems with private and partially federated autoencoders,â€ Asilomar Conference on
Signals, Systems, and Computers , 2022.
[23] H. Yu, Z. Liu, Y . Liu, T. Chen, M. Cong, X. Weng, D. Niyato, and Q. Yang, â€œA sustainable
incentive scheme for federated learning,â€ IEEE Intelligent Systems , vol. 35, no. 4, pp. 58â€“69,
2020.
[24] â€”â€”, â€œA fairness-aware incentive scheme for federated learning,â€ in Proceedings of the
AAAI/ACM Conference on AI, Ethics, and Society , 2020, pp. 393â€“399.
[25] H. Chen, J. Ding, E. Tramel, S. Wu, A. K. Sahu, S. Avestimehr, and T. Zhang, â€œSelf-aware
personalized federated learning,â€ Conference on Neural Information Processing Systems , 2022.
[26] C. He, E. Mushtaq, J. Ding, and S. Avestimehr, â€œFednas: Federated deep learning via neural
architecture search,â€ 2020.
[27] E. Mushtaq, Y . F. Bakman, J. Ding, and S. Avestimehr, â€œFederated alternate training (FAT):
Leveraging unannotated data silos in federated segmentation for medical imaging,â€ 20th Inter-
national Symposium on Biomedical Imaging (ISBI) , 2023.
[28] J. Kang, Z. Xiong, D. Niyato, H. Yu, Y .-C. Liang, and D. I. Kim, â€œIncentive design for efficient
federated learning in mobile networks: A contract theory approach,â€ in 2019 IEEE VTS Asia
Pacific Wireless Communications Symposium (APWCS) . IEEE, 2019, pp. 1â€“5.
[29] S. R. Pandey, N. H. Tran, M. Bennis, Y . K. Tun, Z. Han, and C. S. Hong, â€œIncentivize to build:
A crowdsourcing framework for federated learning,â€ in 2019 IEEE Global Communications
Conference (GLOBECOM) . IEEE, 2019, pp. 1â€“6.
[30] R. Zeng, S. Zhang, J. Wang, and X. Chu, â€œFmore: An incentive scheme of multi-dimensional
auction for federated learning in MEC,â€ in 2020 IEEE 40th International Conference on
Distributed Computing Systems (ICDCS) . IEEE, 2020, pp. 278â€“288.
[31] Y . Zhan, P. Li, Z. Qu, D. Zeng, and S. Guo, â€œA learning-based incentive mechanism for federated
learning,â€ IEEE Internet of Things Journal , vol. 7, no. 7, pp. 6360â€“6368, 2020.
[32] G. Huang, X. Chen, T. Ouyang, Q. Ma, L. Chen, and J. Zhang, â€œCollaboration in participant-
centric federated learning: A game-theoretical perspective,â€ IEEE Transactions on Mobile
Computing , 2022.
11[33] Y . J. Cho, D. Jhunjhunwala, T. Li, V . Smith, and G. Joshi, â€œTo federate or not to federate:
Incentivizing client participation in federated learning,â€ arXiv preprint arXiv:2205.14840 , 2022.
[34] M. Tang and V . W. Wong, â€œAn incentive mechanism for cross-silo federated learning: A public
goods perspective,â€ in IEEE INFOCOM 2021-IEEE Conference on Computer Communications .
IEEE, 2021, pp. 1â€“10.
[35] J. Zhang, Y . Wu, and R. Pan, â€œOnline auction-based incentive mechanism design for horizontal
federated learning with budget constraint,â€ arXiv preprint arXiv:2201.09047 , 2022.
[36] J. Xu, J. Xiang, and D. Yang, â€œIncentive mechanisms for time window dependent tasks in
mobile crowdsensing,â€ IEEE Transactions on Wireless Communications , vol. 14, no. 11, pp.
6353â€“6364, 2015.
[37] S. S. Tay, X. Xu, C. S. Foo, and B. K. H. Low, â€œIncentivizing collaboration in machine learning
via synthetic data rewards,â€ in Proceedings of the AAAI Conference on Artificial Intelligence ,
vol. 36, no. 9, 2022, pp. 9448â€“9456.
[38] D. Ye, X. Huang, Y . Wu, and R. Yu, â€œIncentivizing semi-supervised vehicular federated learning:
A multi-dimensional contract approach with bounded rationality,â€ IEEE Internet of Things
Journal , 2022.
[39] X. Yang, W. Tan, C. Peng, S. Xiang, and K. Niu, â€œFederated learning incentive mechanism
design via enhanced shapley value method,â€ Wireless Communications and Mobile Computing ,
vol. 2022, 2022.
[40] B. Carbunar and M. Tripunitara, â€œFair payments for outsourced computations,â€ in 2010 7th An-
nual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications
and Networks (SECON) . IEEE, 2010, pp. 1â€“9.
[41] J. Han, A. F. Khan, S. Zawad, A. Anwar, N. B. Angel, Y . Zhou, F. Yan, and A. R. Butt, â€œTiff:
Tokenized incentive for federated learning,â€ in 2022 IEEE 15th International Conference on
Cloud Computing (CLOUD) . IEEE, 2022, pp. 407â€“416.
[42] R. Zeng, C. Zeng, X. Wang, B. Li, and X. Chu, â€œA comprehensive survey of incentive mechanism
for federated learning,â€ arXiv preprint arXiv:2106.15406 , 2021.
[43] X. Tu, K. Zhu, N. C. Luong, D. Niyato, Y . Zhang, and J. Li, â€œIncentive mechanisms for federated
learning: From economic and game theoretic perspective,â€ IEEE Transactions on Cognitive
Communications and Networking , 2022.
[44] G. D. NÃ©meth, M. Ã. Lozano, N. Quadrianto, and N. Oliver, â€œA snapshot of the frontiers of
client selection in federated learning,â€ arXiv preprint arXiv:2210.04607 , 2022.
[45] L. Witt, M. Heyer, K. Toyoda, W. Samek, and D. Li, â€œDecentral and incentivized federated
learning frameworks: A systematic literature review,â€ arXiv preprint arXiv:2205.07855 , 2022.
[46] E. S. Maskin, â€œMechanism design: How to implement social goals,â€ American Economic
Review , vol. 98, no. 3, pp. 567â€“76, 2008.
[47] A. E. Roth, The Shapley value: essays in honor of Lloyd S. Shapley . Cambridge University
Press, 1988.
[48] H. Moulin, â€œAn application of the shapley value to fair division with money,â€ Econometrica:
Journal of the Econometric Society , pp. 1331â€“1349, 1992.
[49] W. Chen, S. Horvath, and P. Richtarik, â€œOptimal client sampling for federated learning,â€ arXiv
preprint arXiv:2010.13723 , 2020.
[50] X. Xian, G. Wang, J. Srinivasa, A. Kundu, X. Bi, M. Hong, and J. Ding, â€œUnderstanding
backdoor attacks through the adaptability hypothesis,â€ Proc. International Conference on
Machine Learning , 2023.
[51] X. Wang, Y . Xiang, J. Gao, and J. Ding, â€œInformation laundering for model privacy,â€ Proc.
ICLR , 2021.
12[52] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor, â€œOur data, ourselves: Privacy
via distributed noise generation,â€ in Proc. EUROCRYPT . Springer, 2006, pp. 486â€“503.
[53] J. Ding and B. Ding, â€œInterval privacy: A privacy-preserving framework for data collection,â€
IEEE Trans. Signal Process. , 2022.
[54] E. Diao, J. Ding, and V . Tarokh, â€œHeteroFL: Computation and communication efficient fed-
erated learning for heterogeneous clients,â€ in Prof. International Conference on Learning
Representations , 2021.
[55] X. Wu and H. Yu, â€œMars-fl: Enabling competitors to collaborate in federated learning,â€ IEEE
Transactions on Big Data , 2022.
[56] Y . Shi, H. Yu, and C. Leung, â€œTowards fairness-aware federated learning,â€ IEEE Transactions
on Neural Networks and Learning Systems , 2023.
[57] M. Rabin, â€œIncorporating fairness into game theory and economics,â€ The American economic
review , pp. 1281â€“1302, 1993.
[58] H. Zheng and C. Peng, â€œCollaboration and fairness in opportunistic spectrum access,â€ in IEEE
International Conference on Communications, 2005. ICC 2005. 2005 , vol. 5. IEEE, 2005, pp.
3132â€“3136.
[59] L. Lyu, X. Xu, Q. Wang, and H. Yu, â€œCollaborative fairness in federated learning,â€ Federated
Learning: Privacy and Incentive , pp. 189â€“204, 2020.
[60] C. Wu, Y . Zhu, R. Zhang, Y . Chen, F. Wang, and S. Cui, â€œFedab: Truthful federated learning
with auction-based combinatorial multi-armed bandit,â€ IEEE Internet of Things Journal , 2023.
[61] X. Xian, X. Wang, J. Ding, and R. Ghanadan, â€œAssisted learning: A framework for multi-
organization learning,â€ vol. 33, 2020, pp. 14 580â€“14 591.
[62] E. Diao, J. Ding, and V . Tarokh, â€œGAL: Gradient assisted learning for decentralized multi-
organization collaborations,â€ Advances in Neural Information Processing Systems , vol. 35, pp.
11 854â€“11 868, 2022.
[63] R. H. L. Sim, Y . Zhang, M. C. Chan, and B. K. H. Low, â€œCollaborative machine learning with
incentive-aware model rewards,â€ in International conference on machine learning . PMLR,
2020, pp. 8927â€“8936.
[64] D. Yang, G. Xue, X. Fang, and J. Tang, â€œCrowdsourcing to smartphones: Incentive mechanism
design for mobile phone sensing,â€ in Proceedings of the 18th annual international conference
on Mobile computing and networking , 2012, pp. 173â€“184.
[65] H. Yu, H.-Y . Chen, S. Lee, S. Vishwanath, X. Zheng, and C. Julien, â€œidml: Incentivized
decentralized machine learning,â€ arXiv preprint arXiv:2304.05354 , 2023.
[66] J. Ding, E. Tramel, A. K. Sahu, S. Wu, S. Avestimehr, and T. Zhang, â€œFederated learning
challenges and opportunities: An outlook,â€ in Proc. International Conference on Acoustics,
Speech, and Signal Processing , 2022.
[67] H. Xiao, K. Rasul, and R. V ollgraf, â€œFashion-mnist: a novel image dataset for benchmarking
machine learning algorithms,â€ arXiv preprint arXiv:1708.07747 , 2017.
[68] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng, â€œReading digits in natural
images with unsupervised feature learning,â€ 2011.
[69] L. N. Darlow, E. J. Crowley, A. Antoniou, and A. J. Storkey, â€œCinic-10 is not imagenet or
cifar-10,â€ arXiv preprint arXiv:1810.03505 , 2018.
[70] J. Shi, W. Wan, S. Hu, J. Lu, and L. Y . Zhang, â€œChallenges and approaches for mitigating
byzantine attacks in federated learning,â€ in 2022 IEEE International Conference on Trust,
Security and Privacy in Computing and Communications (TrustCom) . IEEE, 2022, pp.
139â€“146.
13[71] S. Marano, V . Matta, and L. Tong, â€œDistributed detection in the presence of byzantine attacks,â€
IEEE Transactions on Signal Processing , vol. 57, no. 1, pp. 16â€“29, 2008.
[72] M. Fang, X. Cao, J. Jia, and N. Z. Gong, â€œLocal model poisoning attacks to byzantine-robust
federated learning,â€ in Proceedings of the 29th USENIX Conference on Security Symposium ,
2020, pp. 1623â€“1640.
[73] Q. Xia, Z. Tao, Z. Hao, and Q. Li, â€œFaba: an algorithm for fast aggregation against byzantine
attacks in distributed neural networks,â€ in IJCAI , 2019.
[74] N. M. Jebreel, J. Domingo-Ferrer, D. SÃ¡nchez, and A. Blanco-Justicia, â€œDefending against the
label-flipping attack in federated learning,â€ arXiv preprint arXiv:2207.01982 , 2022.
[75] W. Gill, A. Anwar, and M. A. Gulzar, â€œFeddebug: Systematic debugging for federated learning
applications,â€ arXiv preprint arXiv:2301.03553 , 2023.
[76] Q. Li, Y . Diao, Q. Chen, and B. He, â€œFederated learning on non-iid data silos: An experimental
study,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE,
2022, pp. 965â€“978.
[77] G. F. Jenks and F. C. Caspall, â€œError on choroplethic maps: definition, measurement, reduction,â€
Annals of the Association of American Geographers , vol. 61, no. 2, pp. 217â€“244, 1971.
[78] E. Diao, V . Tarokh, and J. Ding, â€œPrivacy-preserving multi-target multi-domain recommender
systems with assisted autoencoders,â€ arXiv preprint arXiv:2110.13340 , 2022.
[79] C. Chen, J. Zhang, J. Ding, and Y . Zhou, â€œAssisted unsupervised domain adaptation,â€ Proc.
International Symposium on Information Theory , 2023.
[80] A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody,
P. Szolovits, L. A. Celi, and R. G. Mark, â€œMimic-iii, a freely accessible critical care database,â€
Sci. Data , vol. 3, p. 160035, 2016.
[81] S. Purushotham, C. Meng, Z. Che, and Y . Liu, â€œBenchmarking deep learning models on large
healthcare datasets,â€ J. Biomed. Inform , vol. 83, pp. 112â€“134, 2018.
[82] H. Harutyunyan, H. Khachatrian, D. C. Kale, G. Ver Steeg, and A. Galstyan, â€œMultitask learning
and benchmarking with clinical time series data,â€ Scientific data , vol. 6, no. 1, pp. 1â€“18, 2019.
14Appendix for â€œA Framework for Incentivized Collaborative Learningâ€
The Appendix contains additional discussions on the ICL framework and related work, ethical
considerations, detailed use cases of FL, AL, and collaborative multi-armed bandit, their experimental
studies, and all the technical proofs.
A Further remarks on the proposed framework
Next, we will discuss more detailed aspects of the proposed framework.
Motivation to have a unified framework . A general incentive framework is important due to the
following reasons:
1. Generalizability: A unified framework captures the essence of incentives in various collaborative
learning settings, enabling its application across different scenarios. This allows researchers and
practitioners to apply the same principles and techniques to multiple use cases, saving time and
resources. For instance, we applied the ICL framework to federated learning, assisted learning, and an
additional case study called incentivized multi-armed bandit, which we introduce in Appendix F. In
these cases, we demonstrated how the crucial components of the pricing plan and selection plan could
play key roles in reducing exploration complexity from the learning perspective, thereby creating
win-win situations for all participants in the collaborative system.
2. Interoperability: A unified incentive framework promotes interoperability among different collabo-
rative learning settings. Researchers and practitioners can build upon the same principles, avoiding
the need to reinvent the wheel for every new use case.
3. Ease of adaptation: A unified framework provides a structured approach to design incentives
under a general collaborative learning framework with guaranteed incentive properties. This makes
adapting the framework for specific use cases easier, such as defining collaboration gain based on
model fairness if that is part of the system-level goal.
In summary, a unified incentive framework for collaborative learning offers generalizability, interop-
erability, and ease of adaptation. These advantages make it a more appealing approach compared to
designing specifically tailored incentive mechanisms for each use case.
Lie about the realized gain . Participants may lie about the realized gain to pay less than they have
to. This is an important aspect to consider in practical collaborative systems. There are several
possible ways to encourage honest behavior in collaborative learning. First, if the realized gain
must be published at the end of the collaboration (e.g., due to third-party verification, or the final
collaboration gain is a function of individual realized gains), we can design an incentive mechanism
to discourage dishonest behavior by rewarding clients who turn out to be honest at the end of the
learning process. This type of expected reward can motivate clients to behave well. For example,
when defining the â€œCollaboration gainâ€ in Section 2.2, we considered a generic scenario that the final
collaboration gain depends on the transparent outcomes of active participants, which indicates that
the system has a way to evaluate the realized gains. Additionally, our incentive framework allows
the active participants to gain from non-active participantsâ€™ paid costs. In that particular case, the
active participantsâ€™ realized outcomes are transparent to the coordinator, so they cannot lie, while the
non-active participants do not realize anything but only enjoy the collaboration gain, so they may
have to pay a flat rate regardless of whether they lie.
There are several ways to verify clientsâ€™ realized gain in FL settings, especially FL with heterogeneous
clients (e.g., [ 54] and the references therein). One way is to estimate the realized local gain from
global/transparent testing data; Another possible way is to estimate the realized gain from local test
data which is uploaded to a verified third party before training. In our work, in the FL setting, the
system knows the submitted local models and can estimate their realized gains.
Heterogeneous contributions to entities . The contribution to one entity might not be the same as the
contribution in another. In a setting where participants have heterogeneous evaluation metrics (e.g.,
due to distributional heterogeneity or personalization need), one can let the utility-income function
depend on specific entities. For example, in Eq. (9) of the main paper, we can add a subscript of â€œmâ€
to the U-function to highlight its possible dependence on entities. Such a change will be notational,
15as it will not affect the key ideas and insights in the proposed work. For instance, the idea of using
incentives to regulate entitiesâ€™ local decisions still applies.
Sub-selling of models . There may be a participant in one round that sub-sell the trained model. This
concern can be addressed in three aspects. First, when the utility-income function is defined (by
domain experts), it can already account for all the values of a trained model, including its potential
subselling. Second, although participants may use a global model to generate synthetic data or apply
knowledge distillation for potential performance improvement, these approaches may require spend
extra costs in data and modeling, which may not be worth it for the client. Third, even if a participant
can leverage an earlier round to better position itself in the next round, we are fine with it as long as it
benefits the system â€“ we allow each client to have varying capability in each round.
Collaboration among competitors . In practice, there is a possible revenue loss due to competitors
getting better. Modeling competing entities in a collaborative learning framework is an interesting fu-
ture problem. A recent work on MarS-FL framework [ 55] introduced Î´-stable market and friendliness
concepts to assess FL viability and market acceptability. In our framework, one possible formulation
of the potential collaborators have conflicting interests is to define the collaboration gain as a function
of not only active participantsâ€™ outcomes but also their level of conflicts.
Explicit coordinator in the incentivized learning . We do not need an explicit coordinator in the
incentivized learning We employ the term â€œcoordinatorâ€ to represent the role of the collaboration
initiator or promoter, which could either be a physical coordinator (e.g., the server in an FL setting) or
one of the participants (e.g., the entity initiating collaboration in an AL setting). Therefore, we do not
really need a physical coordinator in the incentivized learning for all the models. Also, the developed
concepts (e.g., entitiesâ€™ incentives, pricing mechanism, selection mechanism) and the derived insights
do not depend on the coordinator.
Timing for each participant to pay . The general idea of mechanism design is to develop incentives
to regulate entitiesâ€™ local decisions to better achieve a global objective. In our framework, a critical
component of mechanism design is to use the pricing plan before collaborative learning to provide
a suitable incentive for improving collaboration. For transparency, we assume the pricing plan can
inform the participants of what they actually would be paying. The pricing plan (mapping from
outcome to price) is determined before each candidate entity decides participation or not, as shown in
Fig. 1. Additionally, since every entityâ€™s outcome could be random, their decisions are all based on
expected values.
Determination of the value of machine learning results . Assessing the relationship between utility
and income in practical applications can be challenging. This is analogous to defining â€œutilityâ€ in
economic studies, as its precise value depends on the particular context. For instance, the monetary
value of a machine learning model can vary depending on its commercial viability.
B Ethics discussion
A critical reader may raise the fairness concern that the studied incentive design be unfair and
potentially worsen the inequality between high-quality and low-quality entities. We would like to
respectfully point out that this has no conflict with the scope of our work.
Firstly, the literature contains numerous definitions of fairness [ 56]. It is impractical to evaluate
the fairness of a procedure without identifying a specific type of fairness and examining it within
the context of a particular use case. We do not see that the proposed framework conflicts with
some popular types of fairness. One common type of fairness is model fairness, meaning that the
model performs equally well on different subpopulations. In the incentivized collaborative learning
framework, there are multiple ways to incorporate such fairness when defining model performance
(e.g., using weighted accuracy across different subpopulations). Another type of fairness, more
widely studied in game-theoretic settings [ 57,58] and recently in federated learning contexts [ 59], is
collaboration fairness. Rooted in the game theory [ 57], this fairness notion dictates that a clientâ€™s
payoff should be proportional to its contribution to the FL model. Hence, a system that rewards higher-
quality contributors more is inherently â€œfair.â€ This direction of fairness aligns with our framework
since the goal of the incentive here is to encourage high-quality clients to actively contribute to
the collaboration.
16Secondly, even though our incentive mechanism encourages high-quality entities to actively contribute,
it does not mean that low-quality entities will be "unfairly" treated. In fact, our framework advocates
for a win-win situation in which all participants can enjoy the collaborationâ€™s results. Keep in mind
that each entityâ€™s final profit/gain consists of the part attributed to the participation cost/reward and
the part attributed to the collaboration gain. If the latter is higher due to a more reasonable incentive,
everyone can enjoy positive post-collaboration gains. Therefore, a performance-oriented system goal
does not necessarily violate fairness from the perspective of individual participants.
Thirdly, several published works have used the contribution to the model to allocate rewards (e.g., [ 39]
and references therein). Related to that direction of work, we aim to design the mechanism of
allocation before collaborative learning has trained a model, to provide a suitable incentive for
improving the modeling. The collaborative result would then benefit every participant.
In summary, concerning fairness, our work aligns more with contribution fairness (also called
collaboration fairness in FL [ 58]). Our motivation for incentives is to suitably regulate the entitiesâ€™
local decisions (who will autonomously make decisions) to facilitate collaborative performance that
will be enjoyed by even low-quality entities (but willing to pay), thus creating win-win collaborations.
The performance can be measured in various ways depending on practical considerations, such as
reflecting model fairness when defining the quality of collaboration, but this is beyond the scope of
this work.
C More discussions on related works
Table 1: Summary of the most related literature on incentive-based collaboration.
Setting Incentive Key feature
Federated learning FLI [24] fair reward distribution
Federated learning FMore [30] multi-factor utility function
Federated learning CrossSiloFLI[34] social welfare maximization
Federated learning FedAB [60] multi-armed bandit for client selection
Assisted learning AL [61], GAL [62] intertwined interest in prediction
Assisted learning PAL [18] intertwined interest in training and prediction
Collaborative generative modeling CGM [37] synthetic data as rewards
Crowdsourcing CML [63] model as rewards
Crowdsourcing MSensing [64] reverse auction-based mechanism
Decentralized learning iDML [65] blockchain-based mechanism
Our proposed ICL framework builds on the related literature on incentivized collaborative systems,
which often focuses on one or more of three components: pricing, payment allocation, and participant
selection [ 42]. Many incentive mechanisms have been designed for these components, depending on
particular application scenarios. In Table 1, we summarize a few representative works and their main
features. Next, we will relate these existing works to our proposed ICL framework.
In [24], a fairness-aware incentive scheme for FL called FLI was established to maximize collabora-
tion gain while minimizing inequality in payoffs. The payoff allocation in FLI can be regarded as
a particular pricing plan designed to promote both FL model gain and equitable reward allocation.
In [30], the authors proposed an FL incentive scheme called FMore that utilizes a multi-dimensional
auction to encourage participation from high-quality edge nodes. The scheme determines a utility
function based on multiple client-specific factors, which provides a general approach to designing
our pricing plan. In [ 34], the authors formulate a particular social welfare objective problem under
a budget balance constraint to address the challenges of organization heterogeneity in cross-silo
FL. Recently, [ 60] developed an auction-based combinatorial multi-armed bandit algorithm called
FedAB for FL client selection to maximize the serverâ€™s utility. In our framework, this approach can
be particularly helpful when the server lacks trust in the clientsâ€™ shared information. In [ 61,62],
the assisted entity continually relies on othersâ€™ provided information during the prediction stage,
leading to persistent collaboration even after training. The work of [ 18] develops an assisted learning
approach for entities with different learning objectives/tasks, requiring them to combine their tasks
17during the training phase and jointly decode the prediction results during the prediction stage. As
a result, the approach enforces an implicit mechanism design that compels an assisted entity to
assist others. In [ 37], the authors proposed a collaborative learning problem to incentivize data
contribution from entities for training generative models. The synthetic data is treated as rewards in
our framework to solicit contributions from private data sources, different from the commonly used
trained models or monetary rewards. In [ 63], the collaboration system uses the Shapley value and and
information gain to assess the post-collaboration contribution of participants and charge accordingly.
Two particular incentive models for mobile phone sensing systems are developed in [ 64] to encourage
user participation. The platform-centric approach uses a Stackelberg game-based mechanism to
approximate a solution to Nash equilibrium in a generic round. The user-centric approach employs
an auction-based mechanism to determine entity participation. In [ 65], the authors proposed a decen-
tralized and transparent learning system resistant to fraud or manipulation by utilizing blockchain
technology. This approach can be compatible with our ICL framework and can be used for secure
information sharing.
D ICL for federated learning
Federated Learning (FL): For an entity to achieve near-oracle performance as if all the local
resources were centralized, many distributed learning methods have been developed. A popular
direction of research is FL [ 11â€“13,66]. The main concept behind FL involves learning a joint
model by alternating between the following steps in each federated or communication round: 1) a
server sends a model to clients, who subsequently perform multiple local updates, and 2) the server
aggregates models from a subset of clients. This approach leverages the resources of a large number
of edge devices, also known as "clients," to achieve a global objective coordinated by a central server.
In this section, we give a more specific incentivized FL setup and operational algorithm based on the
development in Section 3.1.
D.1 Operational algorithm of incentivized FL
First, let us recall the general Objective (6):
Objective: max
P,SE
Î»X
mâˆˆIPcm+U(zIA)
,where
cmwas introduced in (1) , (17)
zIAâˆ†=G(xm, mâˆˆIA),IAâˆ†=S(I,IP),
s.t.IP=Incent m(P). (18)
The above could be regarded as a collaboration game in any particular round of FL. As FL consists of
multiple rounds, it is natural to use the previous rounds as a basis for each candidate client to decide
whether to participate in the current round. We will use a subscript tto highlight the dependence of
a quantity on the FL round. For example, the outcome xmwill be replaced with xm,tfor round t.
More specifically, we consider the following FL equipped with an incentive setup:
â€¢Let the outcome xm,trepresent the updated model of participant mat round t.
â€¢The â€œgainâ€ function Gmaps a model to the decreased test loss compared with the previous round,
so the larger, the better. We assume the FL server (coordinator) holds a test dataset to compute the
gain.
â€¢The pricing plan PÎ¸: (z, zm, mâˆˆIA)7â†’(cj, jâˆˆIP)can be written in the following form
parameterized by Î¸= [Î¸1, Î¸2]:
cjâˆ†=A1(z;Î¸1) + 1jâˆˆIAA2(zâˆ’zm;Î¸2), (19)
where A1, A2are pre-specified functions. The intuitions is described as follows. On the one hand,
if a participant is non-active, the server will never observe its outcome and local gain. So, it is
reasonable to let all non-active participants pay the same price A1(z;Î¸1)that depends on the realized
collaboration gain. On the other hand, if a participant is active, the server will observe its local gain
and thus leverage that information to correct the price by A2(zâˆ’zm;Î¸2), a function of the improved
gain due to collaboration (which can be negative). With such a design, it is possible to incentivize a
18laggard or adversarial client not to participateâ€“since, otherwise, it will possibly be active and suffer
from a large (penalty) cost.
â€¢To optimize the pricing plan parameterized by Î¸, we need to establish an objective function of Î¸.
Based on the proof of Theorem 2, the marginal gain brought by any client mwould be
E
Î»cmâˆ’(U â—¦ G )â€²(Â¯xK)Î¶m
KÂ¯Î¶K(Â¯xKâˆ’xm)
(20)
given that the client mparticipates in the collaboration. The server would foresee the clientâ€™s strategy
based on Inequality (12): namely, it would participate if
E
U â—¦ G (Â¯xK)âˆ’ U â—¦ G (xm)âˆ’cm	
â‰¥0.
We let bmâˆˆ {0,1}denote the indicator variable of the above condition. Given the above, we propose
to optimize Î¸that maximizes the total marginal gains over those who will participate:
O(Î¸)âˆ†=bmÂ·E
Î»cmâˆ’(U â—¦ G )â€²(Â¯xK)Î¶m
KÂ¯Î¶K(Â¯xKâˆ’xm)
. (21)
However, bmis not differentiable in Î¸. For gradient descent-based optimization of Î¸, we use the
sigmoid function with scaling factor s, denoted by Ïƒs(b) = (1 + eâˆ’b/s)âˆ’1to approximate it. In other
words, we let
bm=Ïƒs
E
U(Â¯zK)âˆ’ U(zm)âˆ’cm	
. (22)
The hyperparameter scontrols the softness of the approximation, which is tuned in the experiment.
Moreover, the collaboration or client-specific gains are not observable at the current round tsince the
outcomes have not been realized. To address that, we will use the performance from the previous
rounds to approximate them. More specifically, we will approximate Â¯zKwith the serverâ€™s model gain
from the last round, denoted by Â¯ztâˆ’1; we approximate zmwith the clientâ€™s model gain from the last
round when it was active, denoted by Â¯zÏ„m,t. Here and afterward, Ï„m,tâ‰¤tdenotes the last time client
mwas active by time t.
D.2 Pseudocode
The pseudocode is summarized in Algorithm 1.
D.3 Experimental details for incentivized FL
In the experimental studies, we considered the following setup.
Data We evaluate the FashionMNIST [ 67], CIFAR10 [ 68], and CINIC10 [ 69] datasets. In Table 2,
we present the key statistics of each dataset.
Table 2: Statistics of datasets in our experiments
Datasets FashionMNIST CIFAR10 CINIC10
Training instances 60,000 50,000 20,000
Test instances 10,000 10,000 10,000
Features 784 1,024 1,024
Classes 10 10 10
Settings Standard FL can be vulnerable to poor-quality or adversarial clients. It is envisioned that a
reasonable incentive mechanism can enhance the robustness of FL training against participation of
these unwanted clients. To demonstrate this intuition, we consider an extreme case where there exist
Byzantine attacks carried out by malicious or faulty clients [ 70]. We then demonstrate the robustness
of the incentivized FL (labeled as â€œICLâ€ in the results) against two types of Byzantine attacks [ 71,72]:
19Algorithm 1 Incentivized Federated Learning
Input: Datasets D1:Mdistributed on Mlocal clients, active rate Ïâˆˆ(0,1], number of communication
rounds T, objective parameter Î», clientsâ€™ unnormalized weights Î¶1:M, test loss L, pricing
planPÎ¸: (z, zm, mâˆˆIA)7â†’(cj, jâˆˆIP), where Î¸= [Î¸1, Î¸2]is the unknown parameter,
cjâˆ†=A1(z;Î¸1) + 1jâˆˆIAA2(zâˆ’zm;Î¸2), andA1,A2are pre-specified functions, serverâ€™s test
dataset Dtest, sigmoid function Ïƒs:v7â†’(1 +eâˆ’v/s)âˆ’1where s >0is a hyperparameter,
learning rate Î· >0for optimizing Î¸. Recall that Ï„m,tâ‰¤tdenotes the last round when the
client mwas active before round t, andUis a given Utility-Income function known to all the
clients and the server.
Output: Serverâ€™s updated model Â¯xt, overall realized profit Î»P
mâˆˆIP,tcâˆ—
m,t+U(Â¯xt),t= 1, . . . , T
Initialization: Î¸0,Â¯x0,Â¯z0âˆ†=G(Â¯x0)
System executes:
foreach communication round t= 1, . . . , T do
Î¸tâˆ†= (Î¸1,t, Î¸2,t)â†ServerStrategy (Â¯zÏ„m,t, zm,Ï„m,t,Â¯xÏ„m,t, xÏ„m,t, Î¶m, mâˆˆ[M], Î¸tâˆ’1)
Distribute Ptâ‰¡ PÎ¸tto all Mclients
bm,tâ†ClientStrategy (Pt, zm,Ï„m,t,Â¯ztâˆ’1), mâˆˆ[M]
IP,tâ† {mâˆˆ[M] :bm,t= 1}
IA,tâ†randomly sample max(âŒŠÏÂ· |IP,t|âŒ‹,1)active clients from IP,t
foreach client mâˆˆIA,tin parallel do
Distribute the serverâ€™s model parameter Â¯xtâˆ’1to local client m
xm,tâ†ClientUpdate (Dm,Â¯xtâˆ’1)// use any standard local update
Send xm,tto the server
end
Receive model parameters from active clients, and calculate Â¯xt =
(P
mâˆˆIA,tÎ¶m)âˆ’1P
mâˆˆIA,tÎ¶mxm,t
Calculate the collaboration gain Â¯zt=G(xm,t, mâˆˆIA)and broadcast to all candidate clients
Calculate the individual gain of each active participant zm,t=G(xm,t)and return it to the
associated client m
Participant mpayscâˆ—
m,tâˆ†=A1(Â¯zt;Î¸1,t) + 1mâˆˆIA,tÂ·A2(Â¯ztâˆ’zm,t;Î¸2,t), for all mâˆˆIP,t
end
ServerStrategy (Â¯zÏ„m,t, zm,Ï„m,t,Â¯xÏ„m,t, xÏ„m,t, Î¶m, mâˆˆ[M], Î¸tâˆ’1):
Define the objective function of Î¸to maximize:
Ot(Î¸) =MX
m=1Ïƒs(Î´m,t)Â·
Î»Â·cm,t(Î¸)âˆ’Î¶mP
jâˆˆIA,Ï„m,tÎ¶jÂ·fâ€²(Â¯xÏ„m,t)Â·(Â¯xÏ„m,tâˆ’xm,Ï„m,t)
,where
cm,t(Î¸)âˆ†=A1(Â¯zÏ„m,t;Î¸1) +ÏÂ·A2(Â¯zÏ„m,tâˆ’zm,Ï„m,t;Î¸2) (23)
Î´m,tâˆ†=U(Â¯ztâˆ’1)âˆ’ U(zm,Ï„m,t)âˆ’cm,t(Î¸), (24)
where fis defined by
f(x)âˆ†=U 
âˆ’L(x, D test)
. (25)
Return Î¸tâ†Î¸tâˆ’1+Î·Â· âˆ‡Î¸Ot(Î¸tâˆ’1)
ClientStrategy (Pt, zm,Ï„m,t,Â¯ztâˆ’1):
Return bm,tâˆ†= 1Î´m,t>0, where Î´m,tis the same as in (24) but with Î¸=Î¸t
random modification, which is a training data-based attack [ 73,70], and label flipping, which is a
parameter-based attack [ 74,75]. Specifically, the random modification is based on generating local
model parameters from a uniform distribution between âˆ’0.25and0.25, and the label flipping uses
cyclic alterations, such as changing â€˜dogâ€™ to â€˜catâ€™, and vice versa. Byzantine client ratios are adjusted
to two levels: {0.2, 0.3}. In the reported results below, Byzantine-0.2 refers to a scenario where 20%
of the total clients are adversarial. We generate 100 clients using IID partitioning of the training part
of each dataset. Among participating clients, the server will select 10% of clients as active clients
20per round. We adopt the model architecture and hyperparameter settings similar to [ 76], which are
summarized in Tables 3 and 4.
Table 3: Network architecture
Data FashionMNIST CIFAR10 CINIC10
CNN Hidden Size [120, 84]
Table 4: Hyperparameters
Global Communication round 100
ClientBatch size 10
Local epoch 5
Optimizer SGD
Learning rate 3.00E-02
Weight decay 5.00E-04
Momentum 9.00E-01
Nesterov âœ“
Scheduler Cosine Annealing
Objective functionLearning rate 1.00E-04
Optimizer SGD
Learning rate 1.00E-02
Weight decay 5.00E-04
Momentum 5.00E-01
Nesterov âœ“
Scheduler Cosine Annealing
Sigmoid s 0.005
Pricing We consider the pricing plan in the form of (19) with
cj=A1(z;Î¸1) + 1jâˆˆIAA2(zâˆ’zm;Î¸2) (26)
=Î¸1Â·z+ 1jâˆˆIAÎ¸1Â·zÂ·(âˆ’1 +Î³Ïƒs(zâˆ’zmâˆ’Î¸2)). (27)
Accordingly, we replace the calculation of cost in (23) with
cm,t(Î¸)âˆ†=Î¸1Â·Â¯zÏ„m,t
1 +ÏÂ·(âˆ’1 +Î³Ïƒs(Â¯zÏ„m,tâˆ’zm,Ï„m,tâˆ’Î¸2))
(28)
In our study, we conduct an ablation test with three different values of the hyperparameter Î³:11,101,
and2001 . These are henceforth referred to as ICL plans 1, 2, and 3 in the following results. A larger
value of Î³implies a more substantial penalty for the underperforming clients. The parameter Î¸2is
initialized in each global communication round using the Jenks natural breaks technique [ 77] and
optimized based on the objective function.
Profit Recall that the profit of a client or the server consists of monetary profit from participation fees
and gain- converted profit from collaboration gains. In FL, we define the collaboration gain Ztas the
negative test loss of the updated server model at round t, so the larger, the better. More specifically,
Â¯zt=G(xm,t, mâˆˆIA) =âˆ’L(Â¯xt, Dtest)
âˆ†=âˆ’X
(input,output)âˆˆDtestâ„“(input,output ; Â¯xt),
where â„“is the cross-entropy loss under the model parameterized by Â¯xt. Likewise, the local gain of a
client mis defined by
zm,t=G(xm,t)âˆ†=âˆ’L(xm,t, Dtest) (29)
âˆ†=âˆ’X
(input,output)âˆˆDtestâ„“(input,output ;xm,t). (30)
21Notably, the test set only needs to be stored and operated by the server, and the clients only need to
access their own historical gains and the serverâ€™s gains. We use Î»= 0.1andU:z7â†’z(the identity
map) in the experiment.
Results We visualize the learning curves on each dataset in Figures 2, 3, and 4. The model
performance is assessed using the top-1 accuracy on the test dataset. We also summarize the best
model performance in Tables 5, 6, and 7, respectively.
The following key points can be drawn from the results. Firstly, in comparison with non-incentivized
Federated Learning (FL) based on FedAvg, our proposed incentivized FL (denoted as â€œICLâ€) algo-
rithm can deliver a higher and more rapidly increasing model performance (representing Collaboration
Gain in our context), as defined in (7). Across all settingsâ€“including two types of adversarial attacks,
two ratios of adversarial clients, and three datasetsâ€“ICL with pricing plan 3 consistently outperforms
FedAvg by a significant margin. On the other hand, ICL with pricing plan 1 underperforms, which is
expected as it only imposes a mild penalty on laggard/adversarial active clients. The results suggest
that it is possible to significantly mitigate the influence of malicious clients by precluding them from
participating in an FL round, given that the pricing penalty is sufficiently large. Furthermore, the
figures also indicate that the random modification attack poses a more significant threat compared to
the label flipping attack, making it particularly difficult for non-incentivized FedAvg to converge.
(a)RM,Byzantine-0.2(b)RM,Byzantine-0.3(c)LP ,Byzantine-0.2(d)LP ,Byzantine-0.3
Figure 2: Learning curves of ICL (incorporating three pricing plans) and FedAvg measured by
Accuracy, assessed for random modification (RM) label flipping (LP) attacks and two malicious
client ratios (0.2 and 0.3), applied to the FashionMNIST dataset.
(a)RM,Byzantine-0.2(b)RM,Byzantine-0.3(c)LP ,Byzantine-0.2(d)LP ,Byzantine-0.3
Figure 3: Learning curves of ICL (incorporating three pricing plans) and FedAvg measured by
Accuracy, assessed for random modification (RM) label flipping (LP) attacks and two malicious
client ratios (0.2 and 0.3), applied to the CIFAR10 dataset.
(a)RM,Byzantine-0.2(b)RM,Byzantine-0.3(c)LP ,Byzantine-0.2(d)LP ,Byzantine-0.3
Figure 4: Learning curves of ICL (incorporating three pricing plans) and FedAvg measured by
Accuracy, assessed for random modification (RM) label flipping (LP) attacks and two malicious
client ratios (0.2 and 0.3), applied to the CINIC10 dataset.
22Table 5: Best model prediction accuracy of ICL (incorporating three pricing plans) and FedAvg
measured by Accuracy, assessed for random modification (RM) label flipping (LP) attacks and two
malicious client ratios (0.2 and 0.3), applied to the FashionMNIST dataset.
MethodRandom modification Label flipping
Byzantine-0.2 Byzantine-0.3 Byzantine-0.2 Byzantine-0.3
FedAvg 87.1(0.1) 86.5(0.1) 89.6(0.0) 89.3(0.0)
ICL pricing plan 1 87.2(0.0) 86.4(0.0) 89.5(0.1) 89.0(0.1)
ICL pricing plan 2 87.0(0.0) 86.4(0.3) 89.3(0.1) 89.4(0.2)
ICL pricing plan 3 89.4(0.0) 89.2(0.1) 89.4(0.1) 89.2(0.1)
Table 6: Best model prediction accuracy of ICL (incorporating three pricing plans) and FedAvg
measured by Accuracy, assessed for random modification (RM) label flipping (LP) attacks and two
malicious client ratios (0.2 and 0.3), applied to the CIFAR10 dataset.
MethodRandom modification Label flipping
Byzantine-0.2 Byzantine-0.3 Byzantine-0.2 Byzantine-0.3
FedAvg 55.7(0.4) 50.1(0.3) 67.3(0.2) 66.9(0.2)
ICL pricing plan 1 54.8(0.2) 42.0(0.9) 67.0(0.4) 66.9(0.3)
ICL pricing plan 2 68.1(0.0) 67.0(0.2) 67.6(0.1) 67.1(0.1)
ICL pricing plan 3 67.1(0.2) 66.4(0.1) 67.9(0.2) 67.4(0.1)
Table 7: Best model prediction accuracy of ICL (incorporating three pricing plans) and FedAvg
measured by Accuracy, assessed for random modification (RM) label flipping (LP) attacks and two
malicious client ratios (0.2 and 0.3), applied to the CINIC10 dataset.
MethodRandom modification Label flipping
Byzantine-0.2 Byzantine-0.3 Byzantine-0.2 Byzantine-0.3
FedAvg 45.9(0.8) 41.0(0.6) 54.3(0.4) 53.8(0.8)
ICL pricing plan 1 44.1(0.0) 38.6(2.1) 54.4(0.3) 53.9(0.1)
ICL pricing plan 2 54.1(0.0) 41.2(7.8) 55.4(0.8) 54.7(0.6)
ICL pricing plan 3 54.6(0.0) 54.7(0.3) 55.1(0.6) 54.2(0.3)
23E ICL for assisted learning
Assisted learning (AL): AL [ 15] is a decentralized learning framework that enables autonomous
organizations to significantly improve their performance by seeking feedback from peers with just a
few assistance rounds, without sharing their private models, objectives, or data. Since then, AL has
been expanded to encompass a broader range of learning tasks, including supervised learning [ 17],
recommendation systems [ 78], and multi-task learning [ 18]. AL has primarily focused on vertically
partitioned data, where entities possess data with distinct feature variables collected from the same
cohort of subjects. Recently, AL has been extended to support organizations with horizontally
partitioned data based on the idea of transmitting model training trajectories, within applications to
reinforcement learning [ 16] (in which the assisting agent has diverse environments) and unsupervised
domain adaptation [79] (where the assisting agent possesses supplementary data domains).
In this section, we give a more specific incentivized AL setup and operational algorithm based on the
development in Section 3.1 and the parallel assisted learning (PAL) [18] framework.
E.1 Operational algorithm
Review of non-incentivized PAL . The main idea is to let each entity seek assistance from others
by iteratively exchanging non-private statistics. Such statistics received by an entity will contain
unknown task information deliberately injected by others so that each entity has an incentive to â€˜do
well.â€™ In the training stage (Stage I), at the first round of assistance, Asends its residual vector to
B. Upon receipt of the query, Bblends the received vector with some statistics sb,aâˆˆRncalculated
from its local task, treats it as learning labels, and fits a model. After that, Bsends the residual back
toA, who will initialize the next round. The above procedure continues until an appropriate stop
criterion is triggered. The training stage initialized by Ais then completed. After that, Bwill initialize
another training stage by swapping the role with A. After the second training stage is completed, both
entities can proceed to the prediction stage.
In the prediction stage (Stage II), upon arrival of a new feature vector, Aqueries the prediction results
from Bâ€™s local models and combines them with its own to form the final prediction. So does B. Each
entity will need to tell how they blend the learned labels to â€˜decodeâ€™ the faithful prediction values.
Our incentivized PAL protocol . Following Equation (14), each entity ineeds to evaluate (uâˆ’
ci)Âµi,j+cjÂµjâ†iat a particular PAL round. We will estimate Âµi,j(respectively Âµjâ†i) with the
collaboration gain from i, j(respectively the additional gain brought by itoj, at the last round
they collaborated. More specifically, Âµi,jis estimated by the reduced prediction loss on a validation
dataset for entity iâ€™s task, comparing the collaborative PAL protocol trained from previous and current
rounds. The Âµjâ†iis estimated by the reduced prediction loss comparing the entity iâ€™s locally trained
model and the collaboratively trained PAL protocol at the current rounds.
The pseudocode is summarized in Algorithm 2. More specifically, the PAL training stage consists
of multiple rounds. In each round, each of the three entities will decide who to collaborate based
on the estimated expected gain. If a pair of entities favors each other, they will form PAL, and the
remaining entity learns on its own; otherwise, every entity will learn on its own. The PAL training of
any entity can be terminated at the discretion of that entity. This provides flexibility to ensure that any
entity can stop appropriately if other participantsâ€™ model/data are no longer helpful or its validation
performance has reached a plateau. Despite when an entity exits the training stage, it will still be
dedicated to working with others in the prediction stage for joint prediction, using its locally learned
models.
E.2 Experimental details for incentivized AL
In the experimental studies, we considered the following setup.
Data We consider a real-world clinical dataset MIMIC [ 80] under PAL setting [ 18]. We conduct
experiments to show our pricing plan can promote mutually beneficial collaboration. We pre-
processed MIMIC3 following [ 81,82] and obtained a dataset of 6916 patients and 18 variables. We
used 35% training, 15% validation, and 50% testing. Suppose there are three divisions/entities that
collected various features from the same cohort of patients and have different regression tasks. In
particular, Entity 1, 2, and 3 aim to predict the heart rate, the systolic blood pressure, and the length
of stay, respectively. We supposed Entity 1 holds the variables of capillary refill rate, diastolic blood
24Algorithm 2 Incentivized Parallel Assisted Learning
Input: Entity i(iâˆˆ[3]) with local task label yiâˆˆR, local feature variables xiâˆˆRdi, concatenated
predictor variables (in hindsight) xâˆ†= [x1, x2, x3]âˆˆRd1+d2+d3, pricing function pi: âˆ†z7â†’
ciÂ·âˆ†z, locally trained model fi,0(without collaboration), blending parameter Ï„ifor PAL,
test loss function Li, and Utility-Income function U:z7â†’uÂ·z
Output: Prediction protocol for each entity irepresented by fi,t(xj, jâˆˆCt),t= 1, . . . , T , where
CtâŠ‚[3]denotes the set of collaborating entities at round t.
Initialization: Entity iâ€™s initial label riâˆ†=yiâˆ’fi,0(xi),iâˆˆ[3]
Training stage:
foreach assistance round t= 1, . . . , T do
foreach entity iâˆˆ[3]in parallel do
For all jÌ¸=i, letÏ„i,j,tdenotes the most recent round when i, jcollaborated by round t(if
any) and calculate:
qi,j,tâˆ†=
(uâˆ’ci)Â·Âµi,j,Ï„i,j,t+cjÂ·Âµjâ†i,Ï„i,j,t ifi, jhave collaborated in at least one round
âˆ otherwise
(31)
Entity ifavors iâˆ—âˆ†= arg maxjÌ¸=iqi,j,tâ€“ if there are more than one maximum, randomly
choose one
ifEntity ihas collaborated with entity jandÂµi,j,Ï„i,j,tâ‰¤0for all jÌ¸=ithen
Entity iwill no longer favor anyone and stop collaborations
end
end
ifthere exists two entities, say AandB, that favor each other then
fA,t,ÂµA,B,ÂµBâ†A,fB,t,ÂµB,A,ÂµAâ†B=runPAL (subject-aligned observations of
(xA, rA),(xB, rB))
rAâ†yAâˆ’fA,t(x),rBâ†yBâˆ’fB,t(x),Ctâ† {A, B}
end
For each remaining entity, say C, train a local model from (xC, rC)to obtain a function
hC,t(xC), and construct a prediction-stage function fC,t(x) =fC,tâˆ’1(x) +hC,t(xC).
end
For each entity, if its validation loss no longer decreases, it will quit the training stage and will be
in the prediction stage
runPAL (Entity Aâ€™s local sample of (xA, rA), entity Bâ€™s local sample of (xB, rB)):
Entities A and B run PAL for one round with (xa, ra)and(xb, rb), which result in two local
models hA,tandhB,t
Entity A obtains a prediction-stage function fA,t(x)âˆ†=fA,tâˆ’1(x) +hA,t(xA) +hB,t(xB)
Entity A trains a local model from (xA, rA)to obtain a function ËœhA,t(xA), and construct a a
prediction-stage function without collaboration at t:ËœfA,t(x) =fA,tâˆ’1(x) +Ëœh(xA).
Entity A evaluates the gains using the empirical expectation over Aâ€™s local test data (denoted by
Etest):
ÂµA,B,tâˆ†=Etest{LA(fA,tâˆ’1(x), yA)âˆ’LA(fA,t(x), yA)}, Âµ Aâ†B,tâˆ†=Etest{LA(ËœfA,t(x), yA)âˆ’LA(fA,t(x), yA)}.
Repeat the above for entity B
Return: A holds fA,t,ÂµA,B,t ,ÂµBâ†A,t, B holds fB,t,ÂµB,A,t ,ÂµAâ†B,t
Prediction stage (a future subject whose modality xf
iis observed by entity iâŠ‚[3]):
Each entity iwill predict with yf
i=P
tâˆˆ[T]:iâˆˆCtfi,t(xj, jâˆˆCt)
//Note : each entity will only operate on its local data and receive assistance (namely hj,t(xj))
from other entities
pressure, fraction inspired oxygen, heart rate, height, mean blood pressure, oxygen saturation, and
respiratory rate; Entity 2 holds the variables of capillary refill rate, diastolic blood pressure, and
fraction inspired oxygen. Entity 3 holds the variables of temperature, weight, pH, glucose, Glasgow
coma scale total, Glasgow coma scale, eye Glasgow coma scale motor, and Glasgow coma scale
25verbal. Each entity will locally used a random forest model with 50 trees and max tree depth 5 in each
round. We ran ten times of the experiments and recorded the standard errors, where the randomness
comes from the local training and data splitting.
To demonstrate how incentives can help assisted learning, we studied the following three settings.
In Setting i(i= 1,2,3), entity iusesci= 0, namely it does not pay, while other two entities use
cj=u= 10 (following the notation in Algorithm 2). It is expected that in Setting i, entity iwill not
receive a good assistance from others as it does not incentivize peer entities.
Results The results are summarized in Fig. 5. The results show that for every entity i, its performance
curves tends to decrease faster, or gain more, if it provide incentives to other peers. We also
summarized the eventual PAL gain of each entity in Table 8. The results show that incentivized PAL
can significantly improve upon an entityâ€™s local learning performance (without collaboration).
0123456789
Round14.515.015.516.016.517.0Test error
Entity 1
Setting 1
Setting 2
Setting 3
0123456789
Round101112131415Test error
Entity 2
Setting 1
Setting 2
Setting 3
0123456789
Round178180182184186188190Test error
Entity 3
Setting 1
Setting 2
Setting 3
Figure 5: Test performance of each entity in incentivized PAL: test error (measured by quadratic loss)
against assistance round in three settings. In Setting i, entity idoes not pay, namely ci= 0following
the notation in Algorithm 2.
Table 8: Test performance of each entity after incentivized PAL in three settings.
Entity 1 Entity 2 Entity 3
Setting 1 Setting 2 Setting 3 Setting 1 Setting 2 Setting 3 Setting 1 Setting 2 Setting 3
Incentivized PAL 16.01 (0.05) 14.78 (0.08) 15.37 (0.09) 11.95 (0.51) 10.97 (0.46) 9.85 (0.06) 180.00 (1.16) 178.97 (1.28) 182.06 (2.14)
Local modeling 16.22 (0.04) 15.07 (0.05) 188.99 (1.37)
26F ICL for multi-armed bandit
F.1 Collaborative multi-armed bandit
As a follow-up to our Introduction section, we present another motivating example.
Example 3 . An investment team is recruiting strategists. Once recruited, a participating strategist will
be selected, following a standard multi-armed bandit selection rule, to realize a reward (e.g., value
of a strategy) that will be enjoyed by all the participants. Each strategist will pay a cost or receive
a reward to participate in the game. Under a suitable incentive, 1) a top-performing strategist may
participate because it will likely be selected and receive collected payment from other participants, 2)
a mediocre strategist may participate to enjoy the reward shared by the top strategist at a relatively
smaller participation cost, and 3) a laggard strategist may not participate because of an overly high
participation cost, which can benefit the collaborative system by reducing the exploration cost in
multi-armed bandit.
In the standard Multi-Armed Bandit (MAB) setting, suppose there are Marms, each producing a
gainziwith mean Âµi. The player will select an arm, denoted by it, at each round and realize a gain,
and its goal is to maximize the expected cumulative reward EPT
t=1zit	
=EPT
t=1Âµit	
at any
stopping time T. Note that the above itâ€™s can be random due to the selection rule. Without loss of
generality, we suppose the arms are ordered in such a way that Âµ1â‰¥ Â·Â·Â· â‰¥ ÂµM.We consider the
following MAB-inspired collaborative setting, which we refer to as collaborative MAB.
Collaborative MAB : Each arm represents a candidate entity that knows its underlying Âµibut is unaware
of othersâ€™. In each round, entities decide whether to participate and if so, they may be selected to
realize the reward. All participants share the realized reward and pay accordingly. Meanwhile, the
coordinator employs a selection scheme to maximize system-level profit.
We will demonstrate some key insights using the collaborative MAB problem. In particular, we will
show that the collaboration requires screening at participation and selection stages, so that strong
candidates will remain to maximize the collaboration gain, medium candidates will pay to enjoy the
gain, and the laggard candidates (with low expected reward) will not participate in the game to avoid
potential large costs.
F.2 Application of ICL to the multi-armed bandit
Specifically, we consider the selection rule that each time, the coordinator will select the entity
with the largest average realized reward from history with probability 1âˆ’Îµand select among the
participants uniformly at random with probability Îµ. We suppose U(z) =z.
Then, the Equilibrium condition in Theorem 1 can be rewritten as
Incent m:E
âˆ’cm+ÂµA	
âˆ’Âµmâ‰¥0,Incent sys:E
Î»cm+ÂµAâˆ’ÂµA(âˆ’m)	
â‰¥0, (32)
where AandA(âˆ’m)denote respectively the selected active entity/arm from all the participants and
that from all the participants excluding m.
Based on the above, we will then focus on the ideal equilibrium scenario where the best-performing
arm, namely arm 1, consistently participates and also has the largest empirical reward in history,
namely Aequals 1with probability 1âˆ’Îµand any remaining arm with probability Îµ/(|IP| âˆ’1). Then,
we have
E(ÂµA) = (1 âˆ’Îµ)Âµ1+ÎµP
iâˆˆIPÂµi
|IP|, (33)
E(ÂµA(âˆ’m)) = (1 âˆ’Îµ)Âµ1+ÎµP
iâˆˆI(âˆ’m)
PÂµi
|I(âˆ’m)
P|,âˆ€mÌ¸= 1, (34)
E(ÂµA(âˆ’m))âˆ’E(ÂµA)â‰ˆÎµÂ¯ÂµIPâˆ’Âµm
|IP|, (35)
where the last equation is an asymptotic approximation for large |IP|, and Â¯ÂµIP=P
iâˆˆIPÂµi
|IP|is the
average of Âµâ€™s within IP. Assume Î» >0for now. Taking the above to (32), we obtain the conditions
27for the pricing plan:
E{cm} â‰¤(1âˆ’Îµ)Âµ1+ÎµE{Â¯ÂµIP} âˆ’Âµm, (36)
E{cm} â‰¥Îµ
Î»|IP|(Â¯ÂµIPâˆ’Âµm). (37)
â€¢Interpretation of (36): Arm mwill participate if its expected cost is no larger than the expected
additional reward. For large T, we often let Îµvanish. In this case, we can further approximate this
condition as
E{cm} â‰¤Âµ1âˆ’Âµm. (38)
â€¢Interpretation of (37): The system will welcome Arm mif its expected payment can overcome its
harm in degrading the collaboration reward (as reflected in the term Â¯ÂµIPâˆ’Âµm. Also, the large Î», the
more tolerance of the system to the â€œlaggardâ€ arms. This is intuitive as a large Î»means the systemâ€™s
profit from participation costs outweighs that from the collaboration gain.
Remark 7 (Incentive eliminates the â€œlaggardâ€ candidates) .In practice, there may be many laggard
or even adversarial arms, meaning that their Âµmâ€™s are much smaller than Â¯ÂµIP, that aim to participate
to enjoy the gain or harm the system with only a small participation cost. To alleviate this concern,
we should design the pricing plan in a way that significantly penalize the laggard arms, so that when
they are selected to be active, albeit with a small probability, the overall expected participation cost is
very high. With such an incentive design, we aim to achieve the following goals:
1) prevent the laggard arms from participating by leveraging their incentives, which can further
improve collaboration the collaboration gain,
2) encourage the medium-performing arms to participate to enjoy the collaboration gain, which can
boost the overall profit, and
3) reward the top-performing arms to participate to make sure that they can be consistently selected
to be active.
Next, we will exemplify the above insights by studying a specific pricing plan.
Example : Consider the pricing plan
cj=b0+b1 1zj<Îº1âˆ’b2 1zj>Îº2 (39)
where b0â‰¥0is the baseline price, b1â‰¥0is the extra price for realizing a reward (if selected) that
is less than a threshold Îº1, and b2â‰¥0is the reward for realizing a reward that is greater than a
threshold Îº2.
We suppose each arm mknows its distribution of reward zmand the mean Âµm. Assume Zm|Âµmâˆ¼
N(Âµm, s2), and let Fdenote the cumulative distribution function (CDF) of N(0, s2). According to
(36), arm mâ€™s condition to participate is
E{cm} â‰¤(1âˆ’Îµ)Âµ1+ÎµE{Â¯ÂµIP} âˆ’Âµm,
which we approximate with
b0+b1P(zm< Îº1)âˆ’b2P(zm> Îº2)
=b0+b1Fs(Îº1âˆ’Âµm)âˆ’b2Fs(Âµmâˆ’Îº2)
â‰¤(1âˆ’Îµ) max
jâˆˆ[M]Ë†Âµj,t+ÎµÂ·Mâˆ’1X
jâˆˆ[M]Ë†Âµj,tâˆ’Âµm, (40)
where Ë†Âµj,tis the empirical average reward realized by arm jup to time t.
To compare with the standard non-incentivized MAB, we will consider Î»= 0, namely the system-
level profit is simply the cumulative realized rewards. We will show that with incentive design , the
MAB can attain a cumulative consistently larger that that of a non-incentivized MAB. To that end, we
discuss how to choose the pricing plan as parameterized by b= [b0, b1, b2]andÎº= [Îº1, Îº2]. They
need to satisfy two conditions:
1)Effective incentive : The arms incentivized to participate must be of high quality (with large
means) so that the exploration cost of MAB can be significantly reduced to benefit the cumulative
rewards.
28In the asymptotic regime where the number of rounds is large and Îµtends to zero, condition (40)
becomes
b0+b1Fs(Îº1âˆ’Âµm)âˆ’b2Fs(Âµmâˆ’Îº2)â‰¤Âµ1âˆ’Âµm. (41)
It can be seen that both the left and right-hand sides are decreasing in Âµm. We will design b,Îºsuch
that
Âµm7â†’Âµ1âˆ’Âµmâˆ’
b0+b1Fs(Îº1âˆ’Âµm)âˆ’b2Fs(Âµmâˆ’Îº2)
, (42)
referred to as the profit-performance function , is strictly increasing in Âµm. Recall that IPis the set of
mthat meets the condition (41). Therefore, IPcan be written in the form of
IP={m:Âµmâ‰¥Âµmâˆ—} (43)
where mâˆ—is such that Âµmâˆ—is the smallest among all Âµâ€™s that guarantees the condition (41), namely
the arms whose underlying Âµis above a threshold.
2)Non-negative balance : The overall participation cost should be non-negative so that the system
does not need to spend extra cost on incentivizing candidate arms.
For this condition to hold, we should have
X
mâˆˆIP{b0+b1Fs(Îº1âˆ’Âµm)âˆ’b2Fs(Âµmâˆ’Îº2)} â‰¥0
where IPis determined by the condition in (43).
We conduct a simulated experiment to demonstrate the insights. We generate M Âµ â€™s from a Gaussian
distribution, generate zâ€™s from Gaussian distributions centered at the corresponding Âµ, runTtime
steps. We choose M= 50, T= 150 , and hyperparameters b= [1,5,10],Îº= [2,4]. We record the
systemâ€™s cumulative profit and the participation activities of each arm over 20 random experiments
and plot their average in Fig. 6. To show the pricing plan is reasonable, we also visualized the
underlying Âµâ€™s from one random experiment and its corresponding profit-performance function as
defined in (42).
290 20 40 60 80 100 120 1405001000150020002500System cummulative balance
0 5 10 15 20 25 30 35 40 450100200300
Arm's round-wise profit
0 20 40 60 80 100 120 1400100200300400System's cummulative realized gains
incentivized
non-incentivized
0 20 40 60 80 100 120 140253035404550
trace of active armFigure 6: Performance of the incentivized MAB: systemâ€™s cumulative profit (from participation
costs), 50 armsâ€™ profit in boxplot, cumulative realized rewards and comparison with nonincentivized
game, and trace of selected active arms (larger index, larger performance), averaged over 20 random
experiments.
0 20 400.51.01.52.02.53.03.5Entities' capability mu
1 2 3
mu2
1
0123
Expected net profit under the incentive
Figure 7: Illustration of a particular incentive ( b= [1,5,10],Îº= [2,4]): an armâ€™s expected net profit
by participating the game against its underlying capability ( Âµ). Arms with better performance expect
to have positive profits and are therefore incentivized to participate.
30G Proofs of theoretical results
G.1 Proof of Proposition 1
Proposition 1 (restatement) LetÎ»â€²âˆ†=Î»âˆ’1
|IP|+1. The Objective (5) where Î»â€²is replaced with Î»is equiva-
lent to maximizing the average social welfare defined by (PROFIT sys+P
mâˆˆIPPROFIT m)/(|IP|+ 1) .
Proof. According to (2) and (3), and the fact that non-participating candidates have a zero profit from
collaboration, we have
PROFIT sys+X
mâˆˆIPPROFIT m
=Î»X
mâˆˆIPcm+U(zIA) +X
mâˆˆIP1mâˆˆIPÂ·(âˆ’cm+U(zIA)âˆ’ U(zm))
=Î»X
mâˆˆIPcm+U(zIA) +X
mâˆˆIPÂ·(âˆ’cm+U(zIA)âˆ’ U(zm))
= (Î»âˆ’1)X
mâˆˆIPcm+ (|IP|+ 1)U(zIA)âˆ’X
mâˆˆIPU(zm)
= (|IP|+ 1)Ã—Î»âˆ’1
|IP|+ 1X
mâˆˆIPcm+U(zIA)âˆ’const
,
where const = (|IP|+1)âˆ’1P
mâˆˆIPU(zm)is a term that does not depend on the incentive mechanism.
Therefore, maximizing the average profit is equivalent to maximizing
Î»âˆ’1
|IP|+ 1X
mâˆˆIPcm+U(zIA), (44)
which concludes the proof.
G.2 Proof of Theorem 1
Theorem 1 (restatement) The condition for a strategy profile (bâˆ—
m, mâˆˆ[M],Pâˆ—,Sâˆ—)to attain a
Nash equilibrium is
Incent m:E
âˆ’cm+U(zIA)âˆ’ U(zm)	
â‰¥0, (45)
Incent sys:E
Î»cm+U(zIA)âˆ’ U(zI(âˆ’m)
A)	
â‰¥0, (46)
if and only if mâˆˆIP.
Proof. From the perspective each entity m, its expected profit if participating the game is E
âˆ’cm+
U(zIA)âˆ’ U(zm)	
. Thus, conditional on other partiesâ€™ decisions, entity mwill participate if and only
if Inequality 45 holds. From the perspective each entity m, from the definition in (3), the additional
expected profit of a candidate mparticipating the game is
E
Î»X
iâˆˆIPci+U(zIA)
âˆ’E
Î»X
iâˆˆIPâˆ’{m}ci+U(zI(âˆ’m)
A)
=E
Î»cm+U(zIA)âˆ’ U(zI(âˆ’m)
A)
.
(47)
Then, the systemâ€™s strategy is to welcome a participant mif and only if its expected marginal gain
brought by this participant is nonnegative, which leads to Condition 46.
G.3 Proof of Theorem 2
Theorem 2 (restatement) LetÂ¯Î¶IPandÂ¯xIPdenote all the participantsâ€™ average of Î¶and weighted
average of X, namely
Â¯Î¶IPâˆ†=P
iâˆˆIPÎ¶i
|IP|,Â¯xIPâˆ†=P
iâˆˆIPÎ¶ixiP
iâˆˆIPÎ¶i. (48)
31Assume that max{|Î¶m|,âˆ¥Î¶mxmâˆ¥âˆ} â‰¤Ï„for all mâˆˆIPandlogd/|IP| â†’0as|IP| â†’ âˆ . Then, the
equilibrium Conditions (9) and (10) can be written as
E{cm} â‰¤E
(U â—¦ G )(Â¯xIP+op(1))âˆ’ U â—¦ G (xm)
, (49)
Î»E{cm} â‰¥E
(U â—¦ G )â€²(Â¯xIP+op(1))bm
(1 +op(1))ÏÎ¶m
KÂ¯Î¶IP(Â¯xIPâˆ’xm)
, (50)
where op(1)denotes a term whose â„“1-norm converges in probability to zero as |IP|goes to infinity.
Proof. By the definition of zIAand Taylor expansion, we have
U(zIA) =U â—¦ GP
iâˆˆIAÎ¶ixiP
iâˆˆIAÎ¶i
=U â—¦ GX
iâˆˆIPBiÎ¶ixiP
jâˆˆIPBjÎ¶j
,
U(zI(âˆ’m)
A) =U â—¦ GX
iâˆˆIPâˆ’{m}BiÎ¶ixiP
jâˆˆIPâˆ’{m}BjÎ¶j
=U(zIA) + (U â—¦ G )â€²(Ï‰K,m)Ã—X
iâˆˆIPâˆ’{m}BiÎ¶ixi1P
jâˆˆIPâˆ’{m}BjÎ¶jâˆ’1P
jâˆˆIPBjÎ¶j
âˆ’bmÎ¶mxmP
jâˆˆIPBjÎ¶j
=U(zIA) + (U â—¦ G )â€²(Ï‰K,m)Ã—bmÎ¶mP
jâˆˆIPBjÎ¶jP
iâˆˆIPâˆ’{m}BiÎ¶ixiP
jâˆˆIPâˆ’{m}BjÎ¶jâˆ’xm
, (51)
where Ï‰K,m is on the line connecting
P
iâˆˆIPBiÎ¶ixiP
jâˆˆIPBjÎ¶jandP
iâˆˆIPâˆ’{m}BiÎ¶ixiP
jâˆˆIPâˆ’{m}BjÎ¶j. (52)
When Kâˆ†=|IP|is large, we will show that both the above terms (and thus Ï‰K,m) will concentrate at
Â¯xIPwith high probability. Let xâ„“denote the â„“-th element of a vector XâˆˆRd. Using the bound of
âˆ¥Î¶ixiâˆ¥âˆ, the fact that bmâ€™s follow independent B(Ï), and Hoeffdingâ€™s inequality, we have for any
small constant v >0,
PX
iâˆˆIPBiÎ¶ixi,â„“âˆ’ÏX
iâˆˆIPÎ¶ixi,â„“â‰¥vKÏ„Â¯Î¶IP
â‰¤2 exp(âˆ’v2Â¯Î¶2
IPK) (53)
for every â„“= 1, . . . , d . It follows from the union bound that
PX
iâˆˆIPBiÎ¶ixiâˆ’ÏX
iâˆˆIPÎ¶ixi
âˆâ‰¤vKÏ„Â¯Î¶IP
â‰¥1âˆ’2dexp(âˆ’v2Â¯Î¶2
IPK). (54)
Likewise, we have
PX
iâˆˆIPBiÎ¶iâˆ’ÏX
iâˆˆIPÎ¶i=X
iâˆˆIPBiÎ¶iâˆ’ÏKÂ¯Î¶IPâ‰¤vKÏ„Â¯Î¶IP
â‰¥1âˆ’2 exp(âˆ’v2Â¯Î¶2
IPK). (55)
Taking Inequalities (54) and (55) into (52), we have
P
iâˆˆIPBiÎ¶ixiP
jâˆˆIPBjÎ¶jâˆ’ÏP
iâˆˆIPÎ¶ixi
ÏP
iâˆˆIPÎ¶i
âˆ
=P
iâˆˆIPBiÎ¶ixiâˆ’ÏP
iâˆˆIPÎ¶ixi
ÏKÂ¯Î¶IP+1P
jâˆˆIPBjÎ¶jâˆ’1
ÏKÂ¯Î¶IPX
iâˆˆIPBiÎ¶ixi
âˆ
â‰¤vKÏ„Â¯Î¶IP
ÏKÂ¯Î¶IP+vKÏ„Â¯Î¶IP
(ÏKÂ¯Î¶IP)(ÏKÂ¯Î¶IPâˆ’vKÏ„Â¯Î¶IP)ÏX
iâˆˆIPÎ¶ixi
âˆ+vKÏ„Â¯Î¶IP
â‰¤vÏ„
Ï+vÏ„
Ï(Ïâˆ’vÏ„)KÂ¯Î¶IP(ÏKÏ„Â¯Î¶IP+vKÏ„Â¯Î¶IP) =vÏ„
Ï+v(Ï+v)Ï„2
Ï(Ïâˆ’vÏ„)(56)
32where the last two lines are due to the triangle inequality. From (54), (55), and (56), and the
assumption that logd/Kâ†’0asKâ†’ âˆ , we can choose v= (Kâˆ’1logd)2/3so that vâ†’0and
v2Â·K/logdâ†’ âˆ asKâ†’ âˆ , which leads to
P
iâˆˆIPBiÎ¶ixiP
jâˆˆIPBjÎ¶j=ÏP
iâˆˆIPÎ¶ixi
ÏP
iâˆˆIPÎ¶i+op(1) = Â¯ xIP+op(1), (57)
where op(1)denotes a term whose â„“1-norm converges in probability to zero. The same result applies
toP
iâˆˆIPâˆ’{m}BiÎ¶ixiP
jâˆˆIPâˆ’{m}BjÎ¶j.
By a similar argument as above, we have
1P
jâˆˆIPBjÎ¶j=1
ÏK(Â¯Î¶IP+op(1)). (58)
Taking (57) and (58) into (51), we conclude the proof.
G.4 Proof of Proposition 2
Proposition 2 (restatement) Suppose the information transmitted from participant mis the mean
and variance of xmâˆˆR, denoted by Âµm, Ïƒ2for all mâˆˆIP. Assume the gain is defined by the
quadratic loss G(x)âˆ†=âˆ’E(Xâˆ’Âµ)2, where Âµrepresents the underlying parameter of interest, and
the participantsâ€™ weights Î¶mâ€™s are the same. Assume that Ïƒ2/(|IP| Â·max mâˆˆIP(Âµmâˆ’Âµ)2)â†’ âˆ as
|IP| â†’ âˆ . Then, we have U(q)/U(qâˆ—)â†’p1as|IP| â†’ âˆ .
Proof. By the definition of G, we have
G(xm, mâˆˆIA) =GP
mâˆˆIPbmxmP
mâˆˆIPbm
=âˆ’EP
mâˆˆIPbmxmP
mâˆˆIPbmâˆ’Âµ2
(59)
=âˆ’(T1+T2)
where
T1âˆ†=EP
mâˆˆIPbmxmP
mâˆˆIPbmâˆ’P
mâˆˆIPbmÂµmP
mâˆˆIPbm2
=E
VarP
mâˆˆIPbmxmP
mâˆˆIPbm|bm, mâˆˆIP
=EP
mâˆˆIPb2
mVar(xm)
 P
mâˆˆIPbm2|bm, mâˆˆIP
=EP
mâˆˆIPbmÏƒ2
 P
mâˆˆIPbm2
=EÏƒ2
P
mâˆˆIPbm
=|IP|âˆ’1Ïƒ2
|IP|âˆ’1P
mâˆˆIPbm(60)
T2âˆ†=EP
mâˆˆIPbmÂµmP
mâˆˆIPbmâˆ’Âµ2
â‰¤E(âˆ†Âµ2). (61)
33By the Markov inequality, we have
PP
mâˆˆIPbm
|IP|âˆ’Ïâ‰¥Îµ
=PP
mâˆˆIP(bmâˆ’qm)
|IP|â‰¥Îµ
â‰¤Îµâˆ’2EP
mâˆˆIP(bmâˆ’qm)
|IP|2
=Îµâˆ’2P
mâˆˆIPqm(1âˆ’qm)
|IP|2
â‰¤Îµâˆ’21
4|IP|â†’0
as|IP| â†’ âˆ . Therefore, based on (60), (61), and the assumption that |IP|âˆ’1Ïƒ2â‰«âˆ†Âµ2,T1will
asymptotically dominate T2and
G(xm, mâˆˆIA)
(Ï|IP|)âˆ’1Ïƒ2â†’p1 (62)
as|IP| â†’ âˆ . Since the denominator in (62) does not depend on the particular qintroduced in (11),
we conclude the proof.
G.5 Proof of Theorem 3
We consider a more general setting. Let pi(âˆ†z)denote the price iis willing to pay for any additional
gain of âˆ†z. The actual gain entity iwill pay in a collaboration, say with j, is denoted by Ciâ†jâˆ†=
pi(zi,jâˆ’zi)âˆ’pj(zj,iâˆ’zj). LetÂµi=E{zi}andÂµi,j=E{Z}i,jfor all i, jâˆˆ[M]. For all i, j, let
Âµiâ†jâˆ†=Âµi,jâˆ’Âµidenote the expected additional gain bought by entity itoj.
Theorem 3 (restatement) Suppose pi(âˆ†z) =ciÂ·âˆ†zfor all iâˆˆ[M]andU(z) =uÂ·z. Then, there
exists a consensus on collaboration if and only if there exist two entities, say i, jâˆˆ[M], such that
E{U(zi,j)âˆ’Ciâ†j} â‰¥0. (63)
In the linear case, the above conditions became
(uâˆ’ci)Âµi,j+cjÂµjâ†iâ‰¥(uâˆ’ci)Âµi,k+ckÂµkâ†i,
(uâˆ’cj)Âµj,i+ciÂµiâ†jâ‰¥(uâˆ’cj)Âµj,k+ckÂµkâ†j,
for all kÌ¸=i, j.
Proof. Based on the setup, the expected profit of an entity iwill be the sum of the profit converted
from PAL gain minus the paid cost. We let PROFIT iâ†jandzi,jrespectively denote the PROFIT iand
the collaboration gain Zgenerated from the PAL formed by entities iandj. Then, we have
PROFIT iâ†jâˆ†=E{U(zi,j)âˆ’Ciâ†j}. (64)
For entity ito favor jover others, we have
PROFIT iâ†jâ‰¥PROFIT iâ†k,âˆ€kÌ¸=i, j, (65)
which, according to (64), implies that
E{U(zi,j)} âˆ’E{Ciâ†j} â‰¥E{U(zi,k)} âˆ’E{Ciâ†k},âˆ€kÌ¸=i, j. (66)
Likewise, for entity jto favor iover others, we have
E{U(zj,i)} âˆ’E{Cjâ†i} â‰¥E{U(zj,k)} âˆ’E{Cjâ†k},âˆ€kÌ¸=i, j. (67)
Finally, for entity ito have a non-negative profit so that it will participate in the collaboration, we
have
E{U(zi,j)} âˆ’E{Ciâ†j} â‰¥0. (68)
These proves the first part of the theorem.
34With the linear assumption of piâ€™s, we have
E{U(zi,j)} âˆ’E{Ciâ†j}=uÂµi,jâˆ’ {ci(Âµi,jâˆ’Âµi)âˆ’cj(Âµi,jâˆ’Âµj)} (69)
= (uâˆ’ci+cj)Âµi,j+ciÂµiâˆ’cjÂµj, (70)
and similarly for other subscripts. Thus, it can be calculated that Inequalities (66) and (67) can reduce
to
(uâˆ’ci)(Âµi,jâˆ’Âµi,k) +cjÂµjâ†iâˆ’ckÂµkâ†iâ‰¥0,
(uâˆ’cj)(Âµj,iâˆ’Âµj,k) +ciÂµiâ†jâˆ’ckÂµkâ†jâ‰¥0,âˆ€kÌ¸=i, j.
Also, Inequality (68) reduces to
uÂµi,jâˆ’ciÂµiâ†j+cjÂµjâ†iâ‰¥0. (71)
These conclude the proof.
G.6 Proof of Theorem 4
Theorem 4 (restatement) Assume Uis a pre-specified nondecreasing function. Consider the pricing
plan in (15) where Ccan be any non-negative and non-decreasing function. Let Âµmâˆ†=EPâˆ—m{U(zm)}
be the expected gain of participant mwhen active, mâˆˆ[K]. The necessary and sufficient condition
for reaching a pricing consensus is the existence of a participant, say participant 1, that satisfies
Âµ1âˆ’ujâ‰¥EPâˆ—
1{C(z1)}+EPâˆ—
j{(Kâˆ’1)C(zj)} (72)
for all jÌ¸= 1. In particular, assume the linearity U(z)âˆ†=uÂ·zandC(z) =cÂ·z. Inequality (72) is
equivalent to
câ‰¤min
jÌ¸=1uÂ·(Âµ1âˆ’Âµj)
Âµ1+ (Kâˆ’1)Âµj.
Proof. The necessary and sufficient conditions for participant 1 to be active with consensus are: 1)
from the systemâ€™s perspective, participant 1 can maximize the expected collaboration gain; 2) from
participant 1 perspective, its expected profit serving as an active participant is better than that of
serving as non-active participant; from other participantsâ€™ perspectives, the expected earn of being
active is no better than being active. Specifically, we have the following conditions:
Perspective of collaboration gain 1 :
EPâˆ—
1{G(z1)} â‰¥EPâˆ—
j{G(zj)},âˆ€jÌ¸= 1 (73)
Perspective of Participant 1 :
EPâˆ—
1{(Kâˆ’1)C(z1) +G(z1)} â‰¥EPâˆ—
j{âˆ’C(zj) +G(zj)},âˆ€jÌ¸= 1 (74)
Perspective of Participant jÌ¸= 1 :
EPâˆ—
1{âˆ’C(z1) +G(z1)} â‰¥EPâˆ—
j{(Kâˆ’1)C(zj) +G(zj)}. (75)
Here, Inequality (73) states that if there exists an active participant with consensus, it must achieve
the maximal collaboration gain. Inequality (75) is equivalent to Inequality (72). By the assumption of
non-negativeness of C, Inequality (75) implies Inequality (73), which further implies Inequality (74).
This proves the first part of the theorem.
In particular, assuming G(z) =gÂ·zandC(z) =cÂ·z, Inequality (75) can be equivalently written as
âˆ’cÂµ1+gÂµ1â‰¥max
jÌ¸=1{(Kâˆ’1)cÂµj+gÂµj}, (76)
which can be equivalently written as
câ‰¤min
jÌ¸=1gÂ·(Âµ1âˆ’Âµj)
Âµ1+ (Kâˆ’1)Âµj. (77)
This concludes the proof.
35