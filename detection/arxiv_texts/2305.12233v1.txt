arXiv:2305.12233v1  [cs.CL]  20 May 2023A Measure of Explanatory Eﬀectiveness∗
Towards a Formal Model of Explanation
Dylan R. Cope†and Peter McBurney
King’s College London
Abstract In most conversations about explanation and AI, the recipi-
ent of the explanation (the explainee ) is suspiciously absent, despite the
problem being ultimately communicative in nature. We pose t he prob-
lem ‘explaining AI systems’ in terms of a two-player coopera tive game in
which each agent seeks to maximise our proposed measure of explanat-
ory eﬀectiveness . This measure serves as a foundation for the automated
assessment of explanations, in terms of the eﬀects that any g iven action
in the game has on the internal state of the explainee.
Keywords: Explanation · XAI · Explainee-centric · Artiﬁcial Intel-
ligence · Algorithmic Information Theory · Dialogues
1 Introduction
The term explanation in artiﬁcial intelligence (AI) is often conﬂated with the
concepts of interpretability andexplainable AI (XAI), but there are important
distinctions to be made. Miller (2019) deﬁnes interpretabi lity and XAI as the
process of building AI systems that humans can understand. I n other words, by
design, the AI’s decision-making process is inherently tra nsparent to a human.
In contrast, explicitly explaining the decision-making to an arbitrary human is
explanation generation . The latter is the subject of this paper. More speciﬁc-
ally, we are working towards developing a formal framework f or the automated
generation and assessment of explanations.
Firstly, some key terminology: an explanation is generated through a dia-
lectical interaction whereby one agent, the explainer , seeks to ‘explain’ some
phenomenon, called the explanandum , to another agent, the explainee . In this
article, we propose a novel measure of explanatory eﬀectiveness that can be used
to motivate artiﬁcial agents to generate good explanations (e.g. in the form of a
reward signal), or to analyse the behaviours of existing com municating agents.
We then deﬁne explanation games as cooperative games where two (or more)
agents seek to maximise the eﬀectiveness measure.
∗Presented at the 1stInternational Workshop on Trusted Automated Decision-
Making (TADM) co-located with ETAPS 2021 . Work done by DC is thanks to the UKRI
Centre for Doctoral Training in Safe and Trusted AI (EPSRC Pr oject EP/S023356/1).
PM wishes to thank Simon Parsons and Elizabeth Sonenberg for discussions on these
topics. DC would like to thank Alex Jackson and Nandi Schoots for helping to under-
stand understanding.
†Correspondence: dylan.cope@kcl.ac.uk2 Dylan R. Cope and Peter McBurney
2 Related Literature
Intepretability and XAI have received an abundance of recen t attention (see
Adadi & Berrada (2018) for a review). This is largely due to tw o factors; regulat-
ory demands (UK Information Commissioner’s Oﬃce 2019) and t he emergence
of highly-performant black-box models, such as deep neural networks, that are
naturally inscrutable. However, the central crux of interp retability techniques is
the need to deﬁne a ﬁxed interpretable domain from which we can derive explan-
ations. This presents two challenges: there are no formal pr ocedures for determ-
ining if a given domain is interpretable; and a domain may be i nterpretable to
some agents, but not others, or only within some contexts. Mo ving away from
interpretability, the problem of explanation generation h as a long history in AI
(Mueller et al. 2019). To some, there is a sense in which gener ating explanations
is the hallmark of intelligence itself (Schank 1984). To oth ers, explanation is
simply about building models – a process which is seen as mere ly instrumental
to intelligent behaviour (Russell & Norvig 2010, Hutter 200 5, Chaitin 2006).
In the philosophy of science the concept of explanation is po sed in terms
of generating descriptions of, or hypotheses regarding, la tent phenomena. This
has led to investigations of formal measures of explanatory power , with an early
example being Popper’s (1959) notion of the ‘degree of corro boration’ . This de-
veloped into a line of philosophers devising subjectivist d eﬁnitions for capturing
aspects of the ‘goodness’ of explanations or hypotheses (Li pton 2003, Glass 2002,
Okasha 2000, Schupbach & Sprenger 2011). However, by the sub jectivity of these
measures they may only assess the degree to which one believe s (or simply likes)
an explanation, which is not necessarily correlated with th e degree to which an
explanation is actually true (or representative of the worl d).
Recently, calls have been made for the need for human-centre d explana-
tion (Kirsch 2017, Abdul et al. 2018). Yet, the framing of exp lanation gener-
ation as a cooperative problem between a human and machine da tes back to
the era of expert systems (Karsenty & Brezillon 1995, Johnso n & Johnson 1993,
Graesser et al. 1996). By articulating explanation as a form al dialogue, a related
direction of investigation is dialogue games (McBurney & Parsons 2002). In par-
ticular, information-seeking (Walton & Krabbe 1995) and education (Sklar & Parsons
2004) dialogues are especially relevant. Sklar & Azhar (201 8) conducted empir-
ical research with a human-machine collaboration task wher e the agents parti-
cipated in a dialogue and explanations were provided to a hum an based of an
argumentation framework (Dung 1995).
3 What is Explanation?
In this work we treat explanatory processes as involving two agents — an ex-
plainer and an explainee — and the result is that the explaine eunderstands
the explanandum better by the end than they did at the start. W e deﬁne ‘an
explanation’ as any sequence of observations made by the exp lainee that leads
to this result. Thus an explanation could be a piece of text or spoken language,
but it could also be a diagram or a piece of interactive media.A Measure of Explanatory Eﬀectiveness 3
With this we shift the problem onto formally deﬁning a measur e of an agent’s
‘understanding’ of some arbitrary phenomenon. We approach the question in
terms of four stances1towards comprehension, understanding as: (1) a sen-
sation (Hume 1751); (2) information compression (Chaitin 2006, Zenil 2019,
Maguire et al. 2016); (3) performance capacity (Turing 1950, Perkins 1993); or
(4)organised information (Lakoﬀ & Johnson 1980, Hofstadter & Sander 2012).
The sensation stance states that comprehension is a conscio us experience —
you understand something if you feel that you apprehend it. T he compression
stance says that understanding is the formulation of concis e and accurate de-
scriptions of phenomena. The performance stance argues tha t having information
is not enough; you must also know how to use the information. T he organised-
information stance tells us that utilisation and compressi on are a byproduct of
something more important; namely that the agent represents information in re-
lation to their own conceptual framework. While each of the s tances has issues
of their own, combined they provide a persuasive account. In other words, if
someone claims they understand something, they can use thei r information to
do things, and their description of the phenomenon is concis e, accurate, and
grounded in other concepts that they understand, then it is h ard to argue that
they do not grasp the phenomenon.
4 Technical Background
4.1 Algorithmic Information Theory
Algorithmic Information Theory (AIT) is a view of information that takes a
fundamentally computational approach (Solomonoﬀ 1964, Ko lmogorov 1968,
Chaitin 1975). Formally, AIT is built on the notion of Kolmogorov complex-
ity, denoted K(x).K(x) is deﬁned as the length of the shortest program, p, on
a Universal Turing Machine (UTM), U, that outputs x.
K(x) = min
p{|p|:U(p) =x},where |p|measures the length of p (1)
The conditional Kolmogorov complexity, K(x|y), is similarly deﬁned by the
length of the shortest program that produces xwhen given input y.
K(x|y) = min
p{|p|:U(yp) =x} (2)
Thus we can deﬁne a measure of mutual information:
I(x;y) =K(y)−K(y|x) (3)
Unless otherwise speciﬁed, when we talk of the mutual inform ation between two
objects we will be referring to an application of Equation 3.
1These stances do not represent arguments defended by anyone in particular, but
rather we are constructing them here as rhetorical tools to h elp decompose the problem.4 Dylan R. Cope and Peter McBurney
4.2 Agents
In its most basic conception, ‘an agent’ is any system that ma kes observations
and takes actions. For any agent Xt∈ Xat time t, we will denote that they make
observations ot
X∈ O Xand take actions at
X∈ A X. Another important factor in
describing agents is their internal state . This phrase can refer to various aspects
of an agent’s cognition, but we are mostly interested in this object insofar as it
stores information. Firstly, we will assume that an agent’s internal state may fall
into a variety of conﬁgurations, i.e. there exists a set of po ssible internal states for
an agent, which we will denote ZX. Secondly, we will talk of information being
‘encoded’ in an agent’s internal state. Given an object o, we will denote X’s
encoding of oas/an}bracketle{to/an}bracketri}htX, where /an}bracketle{to/an}bracketri}htX∈ {p:U(p) =o}for some UTM U. We will
speak of the agent ‘having’ this encoding, or its internal st ate ‘containing’ this
encoding. This is independent to how this is achieved, e.g. t he agent’s internal
state may simply store a list of encodings, or multiple encod ings may overlap in
a distributed storage medium such as a neural network.
4.3 Universal Intelligence Theory
Universal Intelligence Theory (UIT), proposed by Legg & Hutter (2007), es-
tablishes a deﬁnition of machine intelligence based on algo rithmic information
theory and reinforcement learning. In order to meaningfull y compare diﬀerent
performances over a potentially inﬁnite number of time step s, the scope of pos-
sible environments is limited such that the sum of rewards (t he return) is always
less than one. We will refer this as the set of bounded-test environments . With
this, the universal intelligence of an agent πis given by:
Υ(π) =/summationdisplay
µ∈E2−K(µ)Vπ
µ (4)
Where Vπ
µis the return that πachieves in environment µ.
4.4 Universal Artiﬁcial Intelligence
Consider a stochastic environment with dynamics described by a probability dis-
tribution µ(ek|æ<k), where ekis the percept (observation-reward tuple) given
at time k, and æ <kis the action-percept history. In order to perform optim-
ally, the agent in this environment must infer µ. This is known as the problem
of induction. By combining Solomonoﬀ induction (Solomonoﬀ 1964) with von
Neumann-Morgenstern rational decision-making (Morgenst ern & von Neumann
1953), Hutter (2005) deﬁnes AIXI; an agent that chooses the b est possible action
at every time step given perfect inductive inference.
5 Formalising Understanding
5.1 Partitioning the Internal State
In order to devise a measure of understanding, we will start b y deﬁning partitions
of the information in the internal state. These partitions a re constructed withA Measure of Explanatory Eﬀectiveness 5
respect to a given phenomenon p∈ P. There are four: the p-relevant informa-
tion (all information related to p), the p-irrelevant (all information completely
unrelated to p), the p-speciﬁc (the information that only relates to p), and the
p-background information (all information that is not speci ﬁcally related to p).
In the following formal deﬁnitions we are using a particular notation that war-
rants explanation. As we have already established, zXdenotes the internal state
of agent X. We denote p-relevant notation with a comma after the Xfollowed
by∗p,zX,∗p. The star indicates that we are ‘selecting’ allof the information
relevant to p, rather than only the information speciﬁc to p. When the star is
omitted we are referring to to speciﬁc information regardin g whatever follows
the comma, e.g. zX,pis the p-speciﬁc information and zX,¬pis the information
speciﬁc to everything that is not p(the p-irrelevant information).
Deﬁnition 1 ( p-Relevant Information). Given a phenomenon p∈ P and
an agent Xwith internal state zX, the p-relevant information zX,∗p∈ Z Xis
the object where I(zX,∗p;p) =I(zX;p)andI(zX,∗p;zX)is minimised, i.e. there
exists no z′
X,∗psuch that I(z′
X,∗p;p) =I(zX;p)andI(z′
X,∗p;zX)< I(zX,∗p;zX).
Deﬁnition 2 ( p-Irrelevant Information). Given a phenomenon p∈ P and
an agent Xwith internal state zX∈ Z X, the p-irrelevant information zX,¬pis
the object where I(zX,¬p;p) = 0 andI(zX,¬p;zX)is maximised, i.e. there exists
noz′
X,¬psuch that I(z′
X,¬p;p) = 0 andI(z′
X,¬p;zX)> I(zX,¬p;zX).
Deﬁnition 3 ( p-Speciﬁc Information). Given a phenomenon p∈ P and
an agent Xwith internal state zX∈ Z X, the p-speciﬁc information zX,pis
the object where I(zX,p;p)>0, I(zX,p;p′) = 0 ∀p′∈ P, p′/ne}ationslash=pand the mu-
tual information I(zX,p;zX)is maximised, i.e. there exists no z′
X,psuch that
I(z′
X,p;p)>0, I(z′
X,p, p′) = 0 ∀p′∈ P, p′/ne}ationslash=pandI(z′
X,p;zX)> I(zX,p;zX).
Deﬁnition 4 ( p-Background Information). Given a phenomenon p∈ P
and an agent Xwith internal state zX∈ Z Xandp-speciﬁc information zX,p,
thep-background information2zX,∗¬pis the object where I(zX,p;zX,∗¬p) = 0 and
I(zX,∗¬p;zX)is maximised, i.e. there exists no z′
X,∗¬psuch that I(zX,p;z′
X,∗¬p) =
0andI(z′
X,∗¬p;zX)> I(zX,∗¬p;zX).
5.2 Information Compression
With these partitions we can deﬁne how compressed the p-relevant information
is:
Deﬁnition 5 ( p-Compression Factor). Suppose a phenomenon p∈ P and
an agent X. The p-compression factor c:X × P → (0,1]is given as the ratio
of the Kolmogorov complexity of the p-relevant information object to the size of
the agent’s encoding of that information:
c(X, p) =K(zX,∗p)
|/an}bracketle{tzX,∗p/an}bracketri}htX|(5)
2By the notation ∗¬pwe can see that this is ‘everything relevant’ to ‘not p’ .6 Dylan R. Cope and Peter McBurney
5.3 Information Utilisation
Next, we will attempt to formalise the performance stance on understanding, i.e.
we will try to deﬁne X’sinformation utilisation ofp. To do this, we will need to
construct a set of ‘fair tests of p’ for X. We will start by noting: (1) A fair test
forXshould require X’s background information; (2) a test of pshould require
information about p. We will use the formalisation of rational decision-making ,
AIXI, to ‘benchmark’ how information is utilised in a given e nvironment. Unlike
a typical test-taker, AIXI enters into an environment with n o prior knowledge,
and thus we must present any priors to AIXI as a part of its perc ept sequence.
Therefore, to decide whether or not a given task meets the cri teria outlined
above we will construct a ‘meta-task’ for AIXI where relevan t observations are
prepended to the task.
Deﬁnition 6 ( (X, p)-tests). Given a phenomenon p∈ Pand an agent Xwith
internal state zX∈ Z X, we start with the set of bounded-test environments E,
we deﬁne the set of (X, p)-tests, EX,p, as follows:
EX,p=/braceleftBig
µ∈E:VAIXI
(p,b)◦µ=V∗
µ>0, VAIXI
(p)◦µ=VAIXI
(b)◦µ=VAIXI
µ = 0/bracerightBig
(6)
Where bis a shorthand for the p-background information b=zX,∗¬p, and x◦µ
denotes the construction of a new environment µ′such that:
∀xi∈x,∀a<i, µ′((xi,0)|a<i) = 1 (7)
∀k >|x|, µ′(ek|a<k) =µ(ek|aj...k),where j=|x| (8)
It is worth noting why we are using only the p-background information and
not the agent’s entire internal state as required prior know ledge. This is because if
the agent knows anything about pthen AIXI would be able to use the information
encoded in the internal state to pass the test when only given b. We want AIXI
to only get information about pfrom pitself so that we can strictly outline the
criteria above.
Using the set of fair tests for X, we can deﬁne a measure of information
utilisation by measuring the agent’s intelligence across these environ ments. This
is an adaptation of Hutter’s (2005) measure of intelligence (Equation 4).
Deﬁnition 7 ( p-Utilisation). Given an agent Xand phenomenon p, the p-
utilisation Υp:X → [0,1]is deﬁned:
Υp(X) =/summationdisplay
µ∈EX,p2−K(µ)VX
µ (9)
5.4 Information Integration
With the deﬁnitions we have constructed here, we can also int roduce a measure
of how ‘integrated’ the p-relevant information is.A Measure of Explanatory Eﬀectiveness 7
Deﬁnition 8 ( p-Integration). Suppose we have a phenomenon p∈ P, and an
agent X∈ X with p-relevant information zX,∗pandp-speciﬁc information zX,p.
Thep-integration, φ:X × P → [0,1), is deﬁned,
φ(X, p) = tanh/parenleftbigg|/an}bracketle{tzX,∗p/an}bracketri}htX|
|/an}bracketle{tzX,p/an}bracketri}htX|−1/parenrightbigg
(10)
As the p-relevant information will always be larger-than or equal t o the p-
speciﬁc information ( |/an}bracketle{tzX,∗p/an}bracketri}htX| ≥ |/an}bracketle{t zX,p/an}bracketri}htX|), the ratio in this measure will equal
1 when all relevant information is speciﬁc. In this case, the re is no relevant
information that is used for anything else, i.e. the p-relevant information is not
at all integrated into the rest of the internal state (or noth ing else exists to
integrate with). Conversely, the smaller the speciﬁc infor mation gets, the more
the relevant information must be sharing with encodings for other phenomena.
5.5 The Measure of Understanding
Finally we bring these ideas together to deﬁne our measure of understanding.
The resulting measure is bounded by 0 and 1.
Deﬁnition 9 (Understanding). Given an agent X∈ Xwith internal state zX
and phenomenon p∈ P, the measure of X’sunderstanding of phenomenon p,
κ:X × P → [0,1), is deﬁned as:
κ(X, p) =ˆκ(X, p)·c(X, p)·φ(X, p)·Υp(X)·I(zX;p)
K(p)(11)
Where ˆκ(X, p)∈ {0,1}isX’s self-reported understanding of p.
6 Explanation Games
With our measure of understanding, we are ready to deﬁne expl anatory eﬀect-
iveness:
Deﬁnition 10 (Explanatory Eﬀectiveness). The eﬀectiveness of an ex-
planation is the change in explainee’s understanding of the explanandum p∈ P
over the course of the explanatory process. Formally, given an explainer agent
Aand an explainee agent Bthat interact over τtime steps, the explanatory
eﬀectiveness is a function ξ:O∗
B× P → (−1,1)deﬁned as:
ξ(oB, p) =κ(Bτ, p)−κ(B1, p) (12)
Where Btdenotes Bat time tandoBis the sequence of observations that B
made during the interaction.
Deﬁnition 11 (Explanation Game). Suppose an explainer agent A, explainee
agent B, and explanandum p∈ P. An explanation game G= (A, B, p, τ )is
a cooperative ﬁnite sequential game with asymmetric inform ation in which the
participants seek to maximise ξ(oB, p)over the course of τtime steps, where oB
is the sequence of all observations made by B.8 Dylan R. Cope and Peter McBurney
From these deﬁnitions, there are a few observations that we c an make. Firstly,
there is nothing to stop a game from having negative eﬀective ness, i.e. the ex-
plainee understands the phenomenon less after the ‘explana tion’ . As κis bounded
by 0 and 1, ξis bounded by -1 and 1. Secondly, there is no necessary link be tween
eﬀectiveness and the explainee’s beliefs regarding their o wn understanding. It is
possible for the explainee to believe that the explanation w as more eﬀective than
it was (e.g. ˆ κ(X, p) = 1, but I(zX;p) = 0). Thirdly, we can use this notion to
discuss the motivation of the explainer. It makes sense to co nsider an agent as
an explainer, rather than a deceiver, only if they expect the sign of the ξto be
positive. Finally, it is worth noting that this measure chan ges according to time
in which we choose to record it. The explainer may start out st rong and increase
the explainee’s understanding of the explanandum, but then say something that
leads to confusion.
7 Discussion
In this paper we have presented a formal model for assessing t he the ‘explanat-
ory eﬀectiveness’ ξof a dialectical process between two agents. We used this to
deﬁne explanation games in which participants seek to maximise ξ. Along the
way we used AIT and UIT to develop a measure of an agent’s ‘unde rstanding’ of
a given phenomenon p. This involved partitioning the information in the agent’s
mental state into four objects relative to p; the p-relevant, p-irrelevant, p-speciﬁc,
andp-background information. We used these to deﬁne the p-compression factor
(how compressed the agent’s representation of pis),p-integration (what pro-
portion of the representation is only encoding for p), and the p-utilisation. For
the last of these we needed to construct a set of ‘fair tests’, i.e. a set of envir-
onments that would rely on both knowledge of pand the agent’s background
knowledge to solve. We ﬁnd these environments by asking: “Co uld AIXI solve
this environment when given this information?” . However, i t should not be taken
for granted that this is the right question to ask, and thus we should study this
space of environments more precisely to see if it includes un fair tests or leaves
out potential fair tests.
Future work should investigate the trustworthiness of expl anations generated
in our framework, as we have made the implicit assumption tha t if an agent un-
derstands something they can assess whether or not they trus t it. One direction
to look in is the implications of explainees with limited cap acities, i.e. either
time/space complexity constraints, or explainees who are b iased in particular
ways. Additionally, the assumption that explanation games are always cooper-
ative should be challenged, as in many real situations parti cipants may have
conﬂicting or ulterior agendas. For both the cooperative an d non-cooperative
case a useful research project will be to articulate rules fo r the dialogue game
between explainer and explainee (McBurney & Parsons 2002) a nd to develop
strategies for each player, given their goals. Finally, as Kand AIXI are not com-
putable, alternatives for these components (for the purpos es of this framework)
should be devised and studied.Bibliography
Abdul, A., Vermeulen, J., Wang, D., Lim, B. Y. & Kanhanhalli, M. (2018),
Trends and Trajectories for Explainable, Accountable and I ntelligible Systems:
An HCI Research Agenda, in‘Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems’, pp. 1–18.
Adadi, A. & Berrada, M. (2018), ‘Peeking Inside the Black-Bo x: A Survey on
Explainable Artiﬁcial Intelligence (XAI)’, IEEE Access 6, 52138–52160.
Chaitin, G. (2006), ‘The Limits of Reason’, Scientiﬁc American .
Chaitin, G. J. (1975), ‘A Theory of Program Size Formally Ide ntical to Inform-
ation Theory’, Journal of the Association for Computing Machinery 22, 329–
340.
Dung, P. M. (1995), ‘On the acceptability of arguments and it s fundamental role
in nonmonotonic reasoning, logic programming and n-person games’, Artiﬁcial
Intelligence 77(2), 321–357.
Glass, D. H. (2002), Coherence, Explanation, and Bayesian n etworks, in‘Lecture
Notes in Artiﬁcial Intelligence’, Vol. 2464, Springer Verl ag, pp. 177–182.
Graesser, A. C., Baggett, W. & Williams, K. (1996), ‘Questio n-driven Explan-
atory Reasoning’, Applied Cognitive Psychology 10(7), 17–31.
Hofstadter, D. R. & Sander, E. (2012), Surfaces and essences : Analogy as the
fuel and ﬁre of thinking , Basic Books.
Hume, D. (1751), An Enquiry Concerning Human Understanding .
Hutter, M. (2005), Universal Artiﬁcial Intelligence , Texts in Theoretical Com-
puter Science, Springer Berlin Heidelberg, Berlin, Heidel berg.
Johnson, H. & Johnson, P. (1993), ‘Explanation facilities a nd interactive sys-
tems’, International Conference on Intelligent User Interfaces pp. 159–166.
Karsenty, L. & Brezillon, P. J. (1995), ‘Cooperative proble m solving and explan-
ation’, Int. J. Expert Systems with Applications 8(4), 445–462.
Kirsch, A. (2017), Explain to Whom? Putting the User in the Ce nter of Explain-
able AI, in‘Proceedings of the First International Workshop on Compre hens-
ibility and Explanation in AI and ML 2017 co-located with 16t h International
Conference of the Italian Association for Artiﬁcial Intell igence (AI*IA 2017)’ .
Kolmogorov, A. N. (1968), ‘Three approaches to the quantita tive deﬁnition of
information’, International Journal of Computer Mathematics 2(1-4), 157–
168.
Lakoﬀ, G. & Johnson, M. (1980), Metaphors We Live By , University of Chicago
Press.
Legg, S. & Hutter, M. (2007), ‘Universal intelligence: A deﬁ nition of machine
intelligence’, Minds and Machines 17(4), 391–444.
Lipton, P. (2003), Inference to the Best Explanation , 2 edn, Routledge (2003).
Maguire, P., Moser, P. & Maguire, R. (2016), ‘Understanding Consciousness as
Data Compression’, Journal of Cognitive Science 17, 63–94.
McBurney, P. & Parsons, S. (2002), ‘Games That Agents Play: A Formal Frame-
work for Dialogues between Autonomous Agents’, Journal of Logic, Language,
and Information 11, 315–334.10 Dylan R. Cope and Peter McBurney
Miller, T. (2019), ‘Explanation in Artiﬁcial Intelligence : Insights from the Social
Sciences’, Artiﬁcial Intelligence 267, 1–38.
Morgenstern, O. & von Neumann, J. (1953), Theory of games and economic
behavior , Princeton University Press.
Mueller, S. T., Hoﬀman, R. R., Clancey, W., Emrey, A. & Klein, G. (2019),
Explanation in Human-AI Systems: A Literature Meta-Review , Synopsis of
Key Ideas and Publications, and Bibliography for Explainab le AI, Technical
report, DARPA.
Okasha, S. (2000), ‘Van Fraassen’s critique of inference to the best explanation’,
Studies in History and Philosophy of Science Part A 31(4), 691–710.
Perkins, D. (1993), What is Understanding?, in‘Teaching for Understanding’ .
Popper, K. (1959), The Logic of Scientiﬁc Discovery , Routledge Classics (2005),
London and New York.
Russell, S. & Norvig, P. (2010), Artiﬁcial Intelligence: A Modern Approach , 3
edn, Pearson.
Schank, R. C. (1984), The Explanation Game, Technical repor t, Yale University.
Schupbach, J. N. & Sprenger, J. (2011), ‘The Logic of Explana tory Power’,
Philosophy of Science 78(1), 105–127.
Sklar, E. I. & Azhar, M. Q. (2018), Explanation through Argum entation, in
‘Proceedings of the 6th International Conference on Human- Agent Interac-
tion’, Association for Computing Machinery (ACM), New York , NY, USA,
pp. 277–285.
Sklar, E. & Parsons, S. (2004), Towards the application of ar gumentation-based
dialogues for education, in‘Proceedings of the Third International Joint Con-
ference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004. ’,
pp. 1420–1421.
Solomonoﬀ, R. J. (1964), ‘A Formal Theory of Inductive Infer ence, Part 1’,
Information and Control 7, 1–22.
Turing, A. M. (1950), ‘Computing Machinery and Intelligenc e’,Mind
LIX(236), 433–460.
UK Information Commissioner’s Oﬃce (2019), Guide to the GDP R, Technical
report.
Walton, D. & Krabbe, E. C. W. (1995), Commitment in Dialogue: Basic Con-
cepts of Interpersonal Reasoning, in‘SUNY series in Logic and Language’,
State University of New York Press.
Zenil, H. (2019), Compression is Comprehension, and the Unr easonable Eﬀect-
iveness of Digital Computation in the Natural World, inS. Wuppuluri & F. A.
Doria, eds, ‘Unravelling Complexity’, World Scientiﬁc, pp . 201–238.