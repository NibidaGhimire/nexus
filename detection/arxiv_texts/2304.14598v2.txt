1
A Manifold Learning-based CSI Feedback
Framework for FDD Massive MIMO
Yandi Cao, Haifan Yin, Senior Member, IEEE , Ziao Qin, Weidong Li, Weimin Wu and M ´erouane
Debbah, Fellow, IEEE
Abstract —Massive multi-input multi-output (MIMO) in Fre-
quency Division Duplex (FDD) mode suffers from heavy feedback
overhead for Channel State Information (CSI). In this paper, a
novel manifold learning-based CSI feedback framework (MLCF)
is proposed to reduce the feedback and improve the spectral
efficiency for FDD massive MIMO. Manifold learning (ML) is
an effective method for dimensionality reduction. However, most
ML algorithms focus only on data compression, and lack the
corresponding recovery methods. Moreover, the computational
complexity is high when dealing with incremental data. Consid-
ering to utilize the intrinsic manifold structure where the CSI
samples reside, we propose a landmark selection algorithm to
describe the topological skeleton of this manifold. Based on the
learned skeleton, the local patch of the incremental CSI on the
manifold can be easily determined by its nearest landmarks. This
motivates us to propose an incremental CSI compression and re-
construction scheme by keeping the local geometric relationships
with landmarks invariant. We theoretically prove the convergence
of the proposed landmark selection algorithm. Meanwhile, the
upper bound on the error of approximating CSI with landmarks
is derived. Simulation results under an industrial channel model
of 3GPP demonstrate that the proposed MLCF outperforms
existing deep learning based algorithms.
Index Terms —Massive MIMO, FDD, CSI feedback, manifold
learning, representative landmarks, manifold skeleton, local ge-
ometric property.
I. I NTRODUCTION
MASSIVE multiple-input multiple-output (MIMO) is one
of the key enabling technologies for the fifth generation
(5G) wireless communication systems [2] [3]. By deploying
a large number of antennas at the base station (BS), massive
MIMO systems have the potential to provide high spectral
and energy efficiency [4]. These performance gains depend
on accurate and timely channel state information (CSI) at the
transmitter. Since full channel reciprocity is not available in
the frequency division duplex (FDD) system, the downlink CSI
has to be estimated from pilots at the user equipment (UE) and
then fed back to the BS [5] [6]. Unfortunately, the dimension
Y . Cao, H. Yin, Z. Qin, W. Li and W. Wu are with School of Electronic
Information and Communications, Huazhong University of Science and Tech-
nology, 430074 Wuhan, China (e-mail: ydcao@hust.edu.cn, yin@hust.edu.cn,
ziao qin@hust.edu.cn, weidongli@hust.edu.cn, wuwm@hust.edu.cn).
M. Debbah is with KU 6G Research Center, Khalifa University of Sci-
ence and Technology, P O Box 127788, Abu Dhabi, UAE (email: mer-
ouane.debbah@ku.ac.ae) and also with CentraleSupelec, University Paris-
Saclay, 91192 Gif-sur-Yvette, France.
The corresponding author is Weimin Wu.
A part of this work [1] was presented in the IEEE International Conference
on Communications (IEEE ICC 2022).
This work was supported by the National Natural Science Foundation of
China under Grants 62071191, 62071192, and 1214110.of the channel matrix scales with the number of BS antennas,
exacerbating the feedback overhead in massive MIMO systems
[7]. In addition, the amount of feedback is constrained by the
coherence time and coherence bandwidth of the channel, both
of which are limited in a mobility environment with multipath
components. Therefore, one of the most challenging tasks
for FDD massive MIMO is how to reduce the CSI feedback
overhead while keeping the accuracy of the reconstructed CSI
at the BS high.
The urgent demand for limited CSI feedback has motivated
extensive research. A vector quantization codebook [8] [9] is
designed to be shared between the BS and the UE, with the
UE feeding back the index of codeword that best matches the
downlink CSI. The size of the codebook required to maintain
the level of communication quality grows exponentially with
the number of antennas, which is costly in the case of massive
MIMO. In view of the sparsity of massive MIMO channel in
a certain transform domain, the theory of compressed sensing
(CS) is introduced in [10]–[13]. Such approaches rely on
the sparse assumption of the channel, which may not always
be present in practice. Moreover, iterative algorithms are
proposed for CSI recovery, resulting in heavy computational
costs. In particular, in order to exploit the sparsity of the
channel, some works [12] [13] parameterize the azimuth angle
of arrival (AoA) and azimuth angle of departure (AoD), and
design over-complete dictionaries to approximate the steering
vectors. The resolution of AoA and AoD is reflected in the
size of the dictionary, which directly affects the reconstruction
performance.
Due to the powerful feature extraction ability, deep learning
(DL) has recently shown great potential in the field of wire-
less communication, such as hybrid precoding [14], channel
prediction [15] and channel extrapolation [16] [17]. This has
also inspired a number of DL-based algorithms to address the
problem of limited CSI feedback in FDD massive MIMO. An
auto-encoder network named CsiNet [18] has been proposed
for CSI compression and recovery. Specifically, an encoder
compresses the channel matrices into codewords at the UE,
and then a decoder reconstructs the channel matrices from
the received codewords at the BS. Enlightened by the fact
that DL is an appealing way for CSI feedback, a multi-
resolution CRBlock [19] is designed to improve the robustness
under various scenarios and compression ratios. To extract the
temporal correlation of CSI in adjacent time slots, the authors
in [20] design a long short-term memory (LSTM) network
to further improve the reconstruction quality. In [21], a fully-
convolutional neural network called DeepCMC is proposedarXiv:2304.14598v2  [cs.IT]  24 Aug 20242
that jointly considers the CSI compression, quantization, and
entropy coding to enable a trade-off between the CSI recon-
struction quality and feedback overhead. Nevertheless, the DL-
based algorithms typically require very large training data sets.
Other relevant works have also been carried out to avoid the
use of DL. The authors in [22] employ Principal Component
Analysis (PCA), a classical Machine Learning technique, to
compress the CSI into the latent space dimension.
The aforementioned research mainly focused on the charac-
teristics of the channel structure in the angular-delay/frequency
domain [18] [23] or the structure of the channel covariance
matrix [22] [24] [25]. However, the intrinsic manifold structure
where the CSI resides is neglected. It has been verified in [26]
[27] that connecting manifold to codebook design is possible
under the assumption that the optimal beamformer for each
channel resides on the Grassmann manifold. Similarly, we
consider reformulating the limited CSI feedback problem as a
manifold optimization problem by means of manifold learning
(ML) [28]–[30]. Even though the manifold structure is taken
into consideration in both our work and that of [27], there
are fundamental differences in the problems solved and the
algorithms designed. The authors in [27] reduce the beam-
forming codebook design problem to a clustering problem
on the Grassmann manifold where the cluster centroids form
the codebook. In this paper, however, we deal with the CSI
compression and reconstruction problem by maintaining the
local geometric structure of the manifold unchanged before
and after compression.
As a nonlinear dimensionality reduction approach, ML can
map a high-dimensional data set into a low-dimensional space
while preserving the intrinsic manifold structure in the data.
However, two open issues hinder ML from being applied in
practice. Firstly, most ML algorithms, such as Locally Linear
Embedding (LLE) [29] and Local Tangent Space Alignment
(LTSA) [30], work in a “batch” mode, which means that
dimensionality reduction cannot be performed in an incre-
mental way. When a new sample arrives, the original data
set needs to be updated to incorporate the new sample, and
the ML algorithms have to be rerun with the new data set.
This process undoubtedly increases the computational cost.
Secondly, most ML algorithms lack an inverse mapping from
the low-dimensional embedding to the high-dimensional data,
i.e., they focus on data compression, while the corresponding
reconstruction methods are not provided. Some works [30]
[31] attempt to recover the original data by introducing kernel
regression functions to fit the high-dimensional data. However,
the number of required kernel functions scales linearly with
the dimension of high-dimensional data, so determining the
parameters of these functions might be costly when the data
is of large dimensionality.
In order to address the aforementioned issues in traditional
ML algorithms and adopt the idea of ML to limited CSI
feedback, we propose a novel ML-based CSI feedback frame-
work (MLCF) to improve the spectral efficiency (SE) of FDD
massive MIMO systems. It is assumed that the CSI samples lie
on a smooth, low-dimensional manifold embedded in a high-
dimensional space. Without prior knowledge of the intrinsic
manifold, a data set consisting of high-density CSI samplesis constructed to characterize the manifold. It may contain
spatially redundant samples, thus we carefully select repre-
sentative landmarks from the data set to describe the manifold
skeleton. At this point, for the newly sampled CSI, its local
patch on the manifold can be easily identified by its nearest
neighbors in the landmarks rather than the data set. Then the
local geometric relationship between CSI and the landmarks
in the high-dimensional space can be easily determined. This
relationship is expected to be maintained between the low-
dimensional embedding of CSI and the landmarks in low-
dimensional space, so as to calculate the compressed CSI. This
idea also allows for the reconstruction of the original CSI.
The main contributions of this paper are summarized as
follows:
1) To the best of our knowledge, this paper is the first to
employ the idea of ML to the limited CSI feedback prob-
lem. Without having prior knowledge of the manifold
structure, we design landmarks to characterize the topo-
logical skeleton of the manifold where the CSI samples
reside. An alternating iteration optimization algorithm
is proposed to select representative landmarks from the
manifold. Additionally, the closed-form solutions of the
algorithm are provided.
2) Based on the manifold skeleton, the local patch of the
incremental CSI on the manifold can be determined by
its nearest neighbors in landmarks. Inspired by this, we
propose an incremental CSI compression and reconstruc-
tion scheme that alleviates the drawbacks of traditional
ML algorithms, including the high complexity when
handling the incremental data and the absence of inverse
reconstruction.
3) We prove the convergence of the proposed alternating
iterative optimization algorithm, and show that the value
of the objective function decreases monotonically in
each iteration. The theoretical proof and the simulation
results under an industrial channel model of 3GPP are
in agreement.
4) We derive an upper bound on the error of approximating
the CSI with landmarks. We show the main factors that
affect the error are the number and correctness of the
nearest neighbors in the landmarks. If all neighbors lie
in a sufficiently compact region, the approximation error
is quite small.
Simulation results under the industrial channel model of
3GPP demonstrate large gains in terms of the normalized mean
square error (NMSE) of the reconstructed CSI. In particular,
when the compression ratio is 1/4, the proposed MLCF method
brings a gain of at least 7 dB compared with existing DL-based
methods.
Notations: We use boldface to denote vectors and matrices.
(A)Tand(A)Hdenote the transpose and conjugate transpose
of the matrix A, respectively. ℜ{·}andℑ{·}represent the
real and imaginary parts of a matrix, respectively. ∥ · ∥2is
the Euclidean norm of a vector, and ∥ · ∥Fis the Frobenius
norm of a matrix. tr{·}denotes the trace of a square matrix.
e∈Rk×1is a column vector in which all elements are ones.
Ai∗is the i-th row of the matrix A.span(A)denotes the3
column space of A.diag( q1, q2, . . . , q n)represents a diagonal
matrix with q1, q2, . . . , q nat the main diagonal. E{·}denotes
the expectation.
II. S YSTEM MODEL
In this work, we consider a single-cell FDD massive MIMO
system, where the BS is equipped with a uniform linear array
(ULA) with Ntantennas and the UE is equipped with a
single antenna. The antenna elements in ULA are separated
horizontally by half a carrier wavelength. The system operates
in orthogonal frequency division multiplexing (OFDM) mod-
ulation with a ∆fsubcarrier spacing and Nfsubcarriers. For
brevity of exposition, we focus on an arbitrary UE in the cell.
The received signal at the UE is expressed as
ri=hipisi+ni, (1)
where hi∈C1×Nt,pi∈CNt×1,si∈C, and ni∈Cdenote
the downlink channel vector, the precoding vector, the transmit
data symbol, and the additive noise at the i-th subcarrier,
respectively. In the FDD system, the transmitter needs the
knowledge of the accurate and instantaneous downlink CSI
to design pito achieve high SE.
The clustered delay line (CDL) channel model conforming
to the 3GPP TR 38.901 specifications [32] is considered. There
exists Ncscattering clusters in the propagation environment,
each containing Nprays. The downlink channel vector be-
tween the BS and the UE at the certain frequency fiand time
tis modeled as
hfi(t) =NcP
n=1NpP
m=1αn,me−j2πfiτn,mejwn,mta(θn,m),(2)
where αn,m,τn,m,wn,m andθn,m are the channel gain, the
time delay, the Doppler frequency, and the AoD of the m-th
ray in the n-th cluster, respectively. The steering vector a(θ)
is formulated as
a(θ) =h
1ej2πDsinθ
λ0. . . ej2π(Nt−1)Dsinθ
λ0i
∈C1×Nt,
(3)
withDandλ0being the spacing between element antennas
and the central carrier wavelength, respectively.
The channel vectors over Nfsubcarriers are integrated into
a wideband channel matrix H∈CNf×Nt, defined as
H(t) =h
hT
f1(t)hT
f2(t). . .hT
fNf(t)iT
. (4)
The total number of feedback parameters is proportional to
NforNt. Thus, how to compress CSI efficiently becomes a
challenging task for FDD massive MIMO.
Given that ML only supports the calculation of real values,
the complex channel matrix His decomposed into real and
imaginary parts, stacked as below
eH(t) =
ℜ{H(t)}
ℑ{H(t)}
∈R2Nf×Nt. (5)
Note that here we consider perfect channel samples. However,
our method still works well in the case of noisy channel
estimation, as will be shown in Sec. V. Suppose that the real
CSI is sampled from a high-dimensional manifold M. Themanifold topology can be characterized by a data set consisting
of high-density samples on the manifold. However, the large-
sized data set may have spatially redundant samples leading to
high computational complexity. Therefore, we replace the data
set by selecting representative landmarks that not only have
small size but also can characterize the topological skeleton
ofMwell.
In details, by means of ML, the UE compresses the down-
link CSI estimated from the pilot signals as
Y=f(eH,Ddr
H,Ddr
L), (6)
where Y∈Rd×Ntis the low-dimensional embedding of eH,
f(·)denotes the compression function, Ddr
HandDdr
Lare a set
of dictionaries for dimensionality reduction. The two dictionar-
ies consist of selected landmarks on the high-dimensional and
low-dimensional manifolds, respectively. After receiving Y,
the reconstruction operation is performed at the BS to recover
the downlink CSI by
ˆH=g(Y,Drc
H,Drc
L), (7)
where g(·)is the reverse operation of compression, Drc
Hand
Drc
Lare a set of reconstruction dictionaries. Note that these
two sets of dictionaries are learned in advance.
III. P ROPOSED LANDMARK SELECTION METHOD FOR
MANIFOLD LEARNING
In this section, we first describe how to obtain the intrinsic
manifold structure where the CSI resides. Then, a landmark
selection algorithm is proposed to characterize the topological
skeleton of this manifold. Meanwhile, the closed-form solu-
tions and convergence analysis of the algorithm are provided.
A. Manifold structure of CSI
In real-world systems, it is common to deal with numerous
high-dimensional data with low intrinsic degrees of freedom.
Frustratingly, the high-dimensional data suffers from the curse
of dimensionality [28] [33], exacerbating storage requirements
and computational burden. A typical strategy to alleviate this
problem is ML, which aims to map a high-dimensional data set
into a low-dimensional space while maintaining the underlying
structure in the data.
To satisfy the ML assumption, we consider all CSI samples
residing on a d-dimensional manifold Sembedded in a 2Nf-
dimensional space M. Since the true manifold distribution M
is not accessible, we collect a large number of CSI samples
from each UE to form a training set in the hope that it is close
to the original manifold structure. In FDD massive MIMO
systems, a total of Tsdownlink CSI samples are observed to
form the data set
X=h
eH(1) . . .eH(Ts)i
, (8)
which serves to represent the high-dimensional space M. For
simplicity, let xibe the i-th column of X∈R2Nf×N, and
N=NtTsbe the size of X. The process of dimensionality
reduction is defined as
yi=f(xi) +ϵi, (9)4
where f(·) :M → S is the compression function, ϵi∈Rd×1
is the error. Then the corresponding low-dimensional embed-
ding of Xis obtained by Y=f(X)∈Rd×N, which can
characterize the low-dimensional manifold S. Obviously, the
compression ratio is
γ=dN
2NfN=d
2Nf, (10)
where d≪2Nf.
In short, given a high-dimensional data set X, the task of
ML is to obtain its low-dimensional representation Ywhile
finding out the mapping function f. However, data reconstruc-
tion, as the inverse process of dimensionality reduction, is
frequently overlooked, resulting in difficultly recovering the
original data from the low-dimensional embedding. In the
following, we will discuss how to obtain the reconstruction
mapping g(·) :S → M . Once the explicit functions fand
gare known, the newly sampled CSI can be efficiently
compressed and reconstructed.
B. Landmark selection
It should be emphasised that the complexity of most ML
algorithms is strongly influenced by the dimensionality and
quantity of data samples. Typically, the low-dimensional em-
bedding is obtained by performing an eigenvector analysis on
the data similarity matrix, whose size is N×N. As a result,
ML is not suitable for large-scale data sets, which demand
excessive computational and storage resources. Considerable
efforts have been devoted to deal with the case of the large
data set. The idea of perfectly approximating the manifold
skeleton with a collection of landmarks [34] emerges, however
selecting the right landmarks is essential. This also motivates
us to employ landmarks to streamline the data set X, thus al-
leviating the computing burden. The representative landmarks
are selected from the historical CSI data set to characterize
the topological skeleton of M. The main selection principle
is that the landmarks can linearly approximate all samples on
the manifold with minimal error. For the newly sampled CSI,
its local patch on the manifold can be easily determined with
the known landmarks (see Fig. 1 for an illustration).
M 
2
fN
span( ( ))i Jyg
ix
(a)
S
d
iy (b)
Fig. 1. An illustration of the high-dimensional manifold (a) and the corre-
sponding low-dimensional embedding (b). The CSI data xi(the solid circle) is
sampled from the surface in (a). span(Jg(yi))is the tangent space at xi. The
hollow circles are the landmarks selected by our proposed Algorithm 1. The
local geometric relationship between xiand its nearest landmarks remains
unchanged before and after dimensionality reduction.The problem of dimensionality reduction focuses on finding
the mapping ffrom the high-dimensional space to the low-
dimensional space. We may think of first establishing a local
mapping relationship between the previously collected data
setsXandY, and then extending the local mapping to
the global mapping. LLE [29] expects each sample and its
neighbors to lie on or be close to a local patch on the manifold.
In this case, each sample can be approximated by the linear
combination of its nearest neighbors to preserve the local
geometric property. Similar to LLE, we hope each sample on
the manifold Mcan be linearly approximated by its knearest
landmarks to capture the local geometric relationship.
LetDdr
H=
d1d2. . .dM
∈R2Nf×Mbe a high-
dimensional dictionary, composed of landmarks selected from
the high-dimensional space M, to represent the manifold
skeleton. Specifically, di, i= 1,2, . . . , M , is the i-th column
inDdr
H, and M(M≪N)is the size of Ddr
H. Right now, we
have
xi≈MX
j=1wjidj=Ddr
Hwi,
subject to
eTwi= 1 i= 1, . . . , N
wji= 0 if j /∈ Nxi, (11)
where wi=w1iw2i. . . w MiT∈RM×1is a weight
vector, and Nxiis an index set containing the column indices
of the knearest landmarks of xi. The sum of the weights is
enforced to be one such that the local geometry of the manifold
is invariant in scaling, rotating, and shifting the coordinate
system. If djis not in the neighborhood of xi, the weight wji
is set to zero to ensure the locality constraint.
Meanwhile, the above local relationship is expected to be
tenable between yiand the low-dimensional dictionary Ddr
L=f(d1)f(d2). . . f (dM)
∈Rd×M, i.e.,
yi≈MX
j=1wjif(dj) =Ddr
Lwi, (12)
where the weight vector wiand the index set Nyiare the
same as that in Eq. (11). In the following, we will analyze
whether the local linear approximation based on landmarks is
feasible and what affects the approximation error.
Proposition 1 The linear approximation error satisfies
xi−MX
j=1wjidj≤ξ∥Jg(yi)∥FMX
j=1∥bj−yi∥
+ξ1∥Ψ∥FMX
j=1∥bj−yi∥2,(13)
where ξ= max {wji}jis the largest entry in wi,ξ1=q
Nf
2ξ,
Jg(yi)is the Jacobi matrix of gatyi,bj=f(dj)is the low-
dimensional embedding of dj, and
Ψ=h
ΨT
g1(yi)ΨT
g2(yi). . .ΨT
g2Nf(yi)iT
,5
among which Ψgl(yi)is the Hessian matrix of the l-th
component function glofgatyi.
Proof : Please refer to Appendix A. □
According to the above analysis, selecting appropriate land-
marks in a compact region is the key that straightway impacts
the error. However, the neighbor set may contain the incorrect
landmarks due to noise interference or outliers. In this case,
the local patch spanned by the nearest landmarks cannot
sufficiently reflect the local geometry on the manifold. This
prompts us to add an extra term to exclude far away landmarks
as outliers.
In order to minimize the error of linearly approximating the
CSI samples in the data set Xwith landmarks, the objective
function with respect to Ddr
Hand the weight matrix W=w1w2. . .wN
∈RM×Nis formulated as
⟨Ddr
H,W⟩= arg min
Ddr
H,WX−Ddr
HW2
F
+λNX
i=1yi−MX
j=1wjif(dj)2
2+µ∥W∥2,1,
(14)
which has the same constraints as Eq. (11). Moreover, λandµ
are the regularization parameters that tune the approximation
error and the weight error, and ∥·∥2,1is the L2,1norm of
the matrix, which is defined as ∥W∥2,1=PM
i=1∥wi∗∥2.
Adding the extra term ∥W∥2,1is expected to exclude distant
landmarks by penalizing small row weights in W. The weight
∥wi∗∥2is not squared and thus ∥W∥2,1penalizes more for
small weights than ∥W∥2. Since the explicit function f
is not available for a learning task, minimizing the above
objective function becomes impractical. Hence, Lemma 1 [35]
is introduced to discard the unknown function f, thereby
obtaining a tractable optimization problem.
Lemma 1 Letp∈ Upbe an open set on the manifold Mwith
respect to p, such that ∀q∈ Up, the line segment pqremains
inUp. If|∂fm/∂qn| ≤C,1≤m≤d,1≤n≤2Nf, then
for∀q∈ Up, we have
∥f(q)−f(p)∥2≤2NfdC2∥q−p∥2.
This lemma is a generalization of the mean value theorem
[35]. It shows that as qlies in a small neighborhood of p,
there exists an upper bound of ∥f(q)−f(p)∥2. Assuming the
aforementioned conditions hold, the second term in Eq. (14)
meets the following inequality
yi−MX
j=1wjif(dj)2
(a)=MX
j=1wji[f(xi)−f(dj)]2
(b)
≤MMX
j=1w2
ji∥f(xi)−f(dj)∥2(c)
≤τ1MX
j=1w2
ji∥xi−dj∥2,
(15)
where (a) applies the constraint that eTwi=PM
j=1wji= 1,
(b) is derived from the inequality of arithmetic and geometric
means, and in (c) τ1= 2NfMdC2is a constant deduced from
Lemma 1.Therefore, the optimization problem of dimensionality re-
duction can be reformulated as
⟨Ddr
H,W⟩= arg min
Ddr
H,WX−Ddr
HW2
F
+λNX
i=1MX
j=1w2
ji∥xi−dj∥2
2+µ∥W∥2,1,
s.t.
eTwi= 1 i= 1, . . . , N
wji= 0 if j /∈ Nxi.(16)
By solving this optimization problem, we can select the most
representative landmarks to linearly approximate the entire
dataset with minimal error.
Reconstructing the original CSI from the low-dimensional
embedding is an inverse problem of dimensionality reduction.
It can be described as how to discover the reconstruction
function gfrom the training data sets YandX. Likewise,
letDrc
L=b1b2. . .bM
∈Rd×Mbe a reconstruc-
tion dictionary in the low-dimensional space. And the corre-
sponding dictionary in the high-dimensional space is Drc
H=
g(b1)g(b2). . . g (bM)
∈R2Nf×M. In accordance
with the idea of minimizing the local approximation error in
Eq. (14), the optimization problem of reconstruction can be
formulated as
⟨Drc
L,W⟩= arg min
Drc
L,W∥Y−Drc
LW∥2
F
+λNX
i=1xi−MX
j=1wjig(bj)2
2+µ∥W∥2,1,
(17)
which is similar to Eq. (14), except that the parameters xi,dj
and the function fare replaced by yi,bjandg, respectively.
In fact, it has been shown in part of our previous work [1]
that the solution to the reconstruction problem can be inferred
directly from the solution to the dimensionality reduction prob-
lem. Therefore, the detailed derivation of the reconstruction
problem is omitted. The next subsection only provides how to
optimize the problem of dimensionality reduction.
C. Proposed alternating iterative optimization algorithm
Even though the optimization problem Eq. (16) is not jointly
convex to (Ddr
H,W), it is convex to Ddr
HorWwhile the
other is fixed. Therefore, we propose an alternating iteration
optimization algorithm to split the joint optimization problem
into two sub-problems. That is, the dictionary Ddr
His fixed so
that there is only one variable in Eq. (16), making it convenient
to calculate the optimal weight matrix W. In a similar way,
the dictionary is optimized by keeping Wfixed. In addition,
each sub-problem has a closed-form solution. In what follows,
we provide the implementation details of the optimization
algorithm.
To begin with, suppose the weight matrix Whas been
initialized or updated in the previous iteration. We decompose
the minimization problem of Ddr
Hinto sequential minimization
problems [36]. Each column djinDdr
His updated separately to6
simplify the solution procedure. In light of this, the objective
function with respect to djcan be rewritten as
⟨dj⟩= arg min
dj∥E−djwj∗∥2
F+λ(NX
i=1w2
ji∥xi−dj∥2
+NX
i=1MX
m̸=jw2
mi∥xi−dm∥2

+µ∥W∥2,1,
(18)
where E=X−MP
m̸=jdmwm∗, and wj∗∈R1×Nis the j-th
row of W.
Theorem 1 The optimal solution to the problem Eq. (18) is
dj=EwT
j∗+λX(w2
j∗)T
(1 +λ)wj∗wT
j∗, (19)
where w2
j∗represents the square of each entry in wj∗.
Proof : Please refer to Appendix B. □
In the weight update stage, fix the dictionary Ddr
Hafter
it has been initialized or updated. Based on the k-NN cri-
terion, the knearest landmarks of xican be determined
in terms of Euclidean distance and form a neighborhood
matrix Nxi=da1da2. . .dak
∈R2Nf×k. The
corresponding column indices make up an index vector a=a1a2. . . a k
∈R1×k. In fact, the weight vector wi
is sparse and contains only knon-zero entries. Let ˆwi=
ˆw1iˆw2i. . . ˆwkiT∈Rk×1be a compact sub-vector
of non-zero entries in wi, where ˆwjiis equal to w(aj)i,
j= 1, . . . , k . By replacing wiwith ˆwiand dropping Ddr
H,
the optimization problem about ˆwiis simplified to
⟨W⟩= arg min
WNX
i=1
∥xi−Nxiˆwi∥2
2
+λkX
j=1ˆw2
jixi−daj2
2
+µ∥W∥2,1,
(20)
where the weights still satisfy the sum-to-one constraint.
In general, it is challenging to minimize the L2,1-norm.
Fortunately, the derivative of ∥W∥2,1can be easily accessible.
Its equivalent is the derivative of tr(WTGW)[37], where G
is a diagonal matrix with the i-th diagonal element being
gii=1
2∥wi∗∥2. (21)
Theorem 2 The optimal ˆwito the problem Eq. (20) is
ˆwi=
R+λφ(R) +µˆG−1
e
eT
R+λφ(R) +µˆG−1
e, (22)
where R= (xieT−Nxi)T(xieT−Nxi),φ(R)is a matrix
that only preserves the diagonal elements of the matrix Rand
sets the rest to zero, and ˆG= diag( ga1a1, ga2a2, . . . , g akak).
Proof : Please refer to Appendix C. □According to the above closed-form solutions, WandDdr
H
can be optimized alternatively. Once Eq. (16) falls below a
predetermined threshold or the maximum number of iterations
is reached, the iteration process will be terminated. Afterwards,
the low-dimensional dictionary Ddr
Lis obtained by solving
min
Ddr
LNX
i=1yi−MX
j=1wjif(dj)2
2=Y−Ddr
LW2
F,(23)
which is based on the fact that the local geometric relationships
between xiandDdr
HonMas well as those between yiand
Ddr
LonSare characterized by the same set of local weights.
The least squares solution to Eq. (23) is
Ddr
L=YWT(WWT)−1. (24)
Until now, both the high-dimensional dictionary Ddr
Hand the
low-dimensional dictionary Ddr
Lare available for dimensional-
ity reduction, which are pre-stored at the UE side to calculate
the embedding of the incremental CSI. The landmark selection
method is summarized in Algorithm 1. Note that the weight
matrix is an intermediate variable that might be discarded.
Similarly, the group of reconstruction dictionaries, namely Drc
H
andDrc
L, can also be obtained, enabling the BS to recover the
high-dimensional CSI from the low-dimensional embedding.
D. Convergence analysis for the proposed algorithm
In this section, we will analyze the convergence of the
proposed Algorithm 1. Here, Lemma 2 [37] is introduced to
assist the analysis.
Lemma 2 For any non-zero vectors s,v∈Rn, the following
inequality holds
∥s∥2
2
2∥s∥2− ∥s∥2≤∥v∥2
2
2∥s∥2− ∥v∥2.
The convergence of Algorithm 1 is summarized in the fol-
lowing theorem. To simplify the notation, both the superscript
(·)drand the subscript (·)HofDdr
Hare dropped.
Theorem 3 In each iteration, the value of the objective func-
tion Eq. (16) decreases monotonically:
U(Dt,Wt) +µ∥Wt∥2,1≥U(Dt+1,Wt+1) +µ∥Wt+1∥2,1,
where in the t-th iteration, U(Dt,Wt) =∥X−DtWt∥2
F+
λNP
i=1MP
j=1(wT
ji)2xi−dT
j2
2.
Proof : Since the derivative of ∥W∥2,1equals the derivative
oftr(WTGW), it can be easily verified that the solution to
(16) is the solution to the following problem
min
D,WU(D,W) +µtr(WTGW). (25)
Thus in the t-th iteration,

Dt+1= arg min
DU(D,Wt) +µtr(WT
tGtWt)
Wt+1= arg min
WU(Dt+1,W) +µtr(WTGtW).
(26)7
According to the convex optimization theory [38], we have
U(Dt,Wt) +µtr(WT
tGtWt)
≥U(Dt+1,Wt+1) +µtr(WT
t+1GtWt+1). (27)
On the other hand, the right side of the above inequality meets
U(Dt+1,Wt+1) +µtr(WT
t+1GtWt+1)
(a)=U(Dt+1,Wt+1) +µtr(WT
t+1Gt+1Wt+1)
+µMX
i=1 Wt+1
i∗2
2
2WT
i∗
2−1
2Wt+1
i∗
2!
(b)
≥U(Dt+1,Wt+1) +µtr(WT
t+1Gt+1Wt+1)
+µMX
i=11
2Wt+1
i∗
2−1
2WT
i∗
2, (28)
where, according to the definition of the matrix trace, we have
tr(WTGW) =MP
i=1gii∥Wi∗∥2
2in(a), and (b)is derived from
Lemma 2.
Combining Eq. (27) and Eq. (28), we have
U(Dt,Wt) +µtr(WT
tGtWt)
≥U(Dt+1,Wt+1) +µtr(WT
t+1Gt+1Wt+1)
+µMX
i=11
2Wt+1
i∗
2−1
2WT
i∗
2
(c)=U(Dt+1,Wt+1) + 2µtr(WT
t+1Gt+1Wt+1)
−µ
2MX
i=1WT
i∗
2,(29)
where (c)uses the definition of Gin Eq. (21). Further, the
following inequality holds
U(Dt,Wt) + 2µtr(WT
tGtWt)
=U(Dt,Wt) +µtr(WT
tGtWt) +µ
2MX
i=1WT
i∗
2
≥U(Dt+1,Wt+1) + 2µtr(WT
t+1Gt+1Wt+1).(30)
That is to say,
U(Dt,Wt) +µ∥Wt∥2,1≥U(Dt+1,Wt+1) +µ∥Wt+1∥2,1,
(31)
which indicates that the value of the objective function
Eq. (16) will decrease monotonically in each iteration. □
IV. MLCF- BASED CSI F EEDBACK
In previous sections, we have obtained two sets of dictio-
naries, one for compression and the other for reconstruction.
It is worth mentioning that these two sets of dictionaries are
learned at the BS beforehand from the training data sets X
andY. Once both sets are known, the BS will store the group
of reconstruction dictionaries itself and broadcast this group of
compression dictionaries to the UE. Below, we will show how
to compress the newly sampled CSI at the UE and reconstruct
the original CSI at the BS with these dictionaries.Algorithm 1 Landmark selection for dimensionality reduction
Input: The historical CSI data set X, the size of dictionary
M, the number of nearest landmarks k, the intrinsic dimen-
sionaity d,λ,µ.
Output: A group of dictionaries Ddr
HandDdr
L.
1:t= 0, initialize (Ddr
H)tby randomly selecting Mcolumns
fromX, initialize Wt;
2:while Eq. (16) converges do
3: Update Gtbased on Eq. (21);
4: foreach landmark dj,j= 1, . . . , M do
5: Compute the j-th column of (Ddr
H)t+1by Eq. (19);
6: end for
7: foreach sample xi,i= 1, . . . , N do
8: Determine the neighborhood matrix Nxi;
9: Compute the weight vector wt+1
iby Eq. (22);
10: end for
11: t←t+ 1;
12:end while
13:Compute the low-dimensional embedding Yusing a ML
algorithm;
14:Compute the low-dimensional dictionary Ddr
Lby Eq. (24);
15:return Ddr
H,Ddr
L
A. Dimensionality reduction for the incremental CSI
In FDD massive MIMO systems, we hope to reduce the
downlink CSI feedback overhead at the UE while accurately
reconstructing the CSI at the BS. With that in mind, the UE
adopts the group of aforementioned compression dictionaries,
namely Ddr
HandDdr
L, to compute the low-dimensional em-
bedding of the downlink CSI.
Let the incremental CSI, estimated from the downlink pilots
at a new time slot, be denoted by
eHnew=
h1. . .hNt
∈R2Nf×Nt.
Since the landmarks on the manifold Mhave been learned,
the local geometric property of eHnew onMcan be easily
characterized by its nearest neighbors in the landmarks.
In the process of dimensionality reduction, we first search
for the knearest neighbors of hi,i= 1,2, . . . , N t, inDdr
Hand
model the local geometries on Mas a collection of linear
patches. The geometries are expected to remain consistent
in both high-dimensional and low-dimensional spaces. Thus,
the weight and neighborhood relationships can be settled by
optimizing the following objective function
min
wihi−Ddr
Hwi2+λf(hi)−MX
j=1wjif(dj)2
s.t.
eTwi= 1 i= 1, . . . , N t
wji= 0 if j /∈ Nhi,(32)
which is analogous to Eq. (14), except that the dictionary Ddr
H
is already known and µis a special case equal to 0. Referring
to the derivation in Appendix C, the optimal solution to the k
non-zero entries in wiis
ˆwi=(R+λφ(R))−1e
eT(R+λφ(R))−1e, (33)8
where
R= (hieT−Nhi)T(hieT−Nhi).
With iascending from 1 to Nt, we obtain the whole weight
matrix Wdr=w1w2. . .wNt
∈RM×Nt. Since
the weights Wdrcan also characterize the local geometric
property in the low-dimensional space, the low-dimensional
embedding of eHnewcan be calculated by
Ynew=Ddr
LWdr. (34)
Then, the UE feeds the low-dimensional embedding Ynew=y1y2. . .yNt
∈Rd×Ntback to the BS without
additional parameters, which is convenient in practical appli-
cations.
Compressing the incremental CSI with landmarks has the
advantage of low computational complexity. On the one hand,
when the new CSI arrives, only the low-dimensional embed-
ding of eHnewis calculated. There is no need to rerun the entire
ML algorithms with the original data set augmented by the
new sample. On the other hand, compared to the conventional
ML algorithms that operate on the entire data set, it is easier to
obtain the neighborhood and weight relationships between the
CSI and the landmarks. The time complexity for computing a
weight vector is O(2Nfk2) +O(k3), which is dominated by
O(2Nfk2)ask≪2Nf. In additional, computing Ynewhas
a time complexity of O(dMN t). Hence, the total complexity
of dimensionality reduction is O(2NfNtk2) +O(dMN t).
B. Reconstruction for the incremental CSI
After receiving the low-dimensional embedding Ynew, the
BS attempts to reconstruct the CSI ˆHnewas close as possible
to the true value Hnew. Perfectly reconstructing the CSI is
not feasible since some information is lost during the process
of dimensionality reduction. In order to simplify the recon-
struction while guaranteeing the reconstruction quality, we
provide a low-complexity reconstruction scheme based on the
selected landmarks. Inspired by the process of compressing the
incremental CSI, we believe that the reconstruction mapping
may still be established by keeping the local geometry on
the manifold unchanged. For the newly received Ynew, its
local geometric relationship with Drc
Lcan be easily identified.
Under this premise, the reconstructed ˆHnewcan be obtained
by maintaining the same geometric relationship with Drc
Hin
the high-dimensional space.
Firstly, search for the knearest neighbors of yiin the pre-
stored dictionary Drc
L. Based on the idea that keeps the local
geometric property constant, the optimization problem with
respect to the reconstruction weights can be formulated as
min
wi∥yi−Drc
Lwi∥2+λg(yi)−MX
j=1wjig(bj)2
s.t.
eTwi= 1 i= 1, . . . , N t
wji= 0 if j /∈ Nyi,(35)
where wiis the i-th column of the reconstruction weight
matrix Wrc=w1w2. . .wNt
∈RM×Nt. It can be
observed that the objective function Eq. (35) is comparable toEq. (33) except for some parameters. Hence, the knon-zeros
entries of wican likewise be computed by Eq. (33), where R
is reset to (yieT−Nyi)T(yieT−Nyi). Subsequently, based
onWrcand the known dictionary Drc
H, the BS reconstructs
ˆHnewby
ˆHnew=Drc
HWrc. (36)
Utilizing the knowledge of manifold structure makes CSI
reconstruction simple. Moreover, the above process only re-
quires vector and matrix calculations, rather than multiple
iterations. The overall time complexity of reconstruction is
O(Ntk3)+O(dNtk2)+O(2NfMN t). At this point, the tasks
of compressing and reconstructing the incremental CSI have
been accomplished.
V. N UMERICAL RESULTS
In this section, we evaluate the performance of the proposed
MLCF method under the industrial CDL channel model, con-
forming to 3GPP TR 38.901 [32]. With the help of MATLAB,
we carry on the simulation for the 5G new radio (NR) Release
16 and generate the training and testing data sets [39]. Table. I
shows part of the default parameters in the channel model.
For an OFDM system, it is necessary to consider multiple
subcarriers and OFDM symbols. In practice, Nfcan also
denote the number of resource blocks (RBs) or the number
of groups of consecutive RBs. In this paper, RB is adopted as
the basic feedback granularity. Unless particularly specified,
the constant λ,µ, the number of nearest neighbors k, the size
of dictionary M, the size of data set N, and the number of
BS antenna Ntare set to 0.001, 0.001, 70, 400, 4000, and
32, respectively. The k-nearest neighbor ( k-NN) strategy is
adopted to select the neighbors in landmarks.
TABLE I
THE PARAMETER SETTINGS FOR CDL CHANNEL
Parameters Default values
Channel model CDL-A
Number of clusters Nc= 23
Total number of rays 460
Delay spread 300 ns
Time slot 1 ms
Downlink carrier frequency 3.5 GHz
Subcarrier spacing ∆f= 15 kHz
Resource blocks NRB= 48
UE speed 30 km/h
Number of BS antennas Nt= 32/64
To analyze the reconstruction performance, we introduce
two evaluation metrics. The first metric NMSE measures the
difference between the reconstructed CSI and the original CSI,
defined as
NMSE = 10 lg(
E∥eH−ˆH∥2
F
∥eH∥2
F)
. (37)
The other one is cosine similarity, defined as
ρ=E

1
NfNfX
n=1|ˆhnhH
n|
∥ˆhn∥2∥hn∥2

, (38)9
where hnandˆhnare the n-th subcarrier of the true complex
channel and reconstructed channel, respectively.
Three DL-based algorithms (CLNet [40], NR-CsiNet [41],
FISTA-Net [42]) are introduced as benchmarks, all of which
employ a two-step compression operation. Specifically, the
CSI matrix H∈CNf×Ntis transformed by discrete Fourier
transformation (DFT) into the angular-delay domain ¯H, which
exhibits sparsity with only Narows composed of non-zero
values. First, ¯His compressed to Ha∈CNa×Ntby retaining
theNanon-zero rows of ¯Hand removing the remaining rows.
The compression ratio of this step is γ1=Na/Nf. Next,
the benchmarks achieve the second step of compression by
designing an encoder network to compress Hainto a codeword
vector c∈RL×1. The compression ratio is γ2=L/2NaNt.
Meanwhile, a decoder network is deployed at the BS to
recover ˆHafrom the codeword. Finally, the CSI matrix is
recovered by performing inverse DFT. The total compression
ratio is γ=γ1γ2=L/2NfNt. To enable that our proposed
MLCF has the same compression ratio as the benchmarks,
we set γ1= 1/2 andγ2= [1/2 ,1/4,1/8]. Note that
the three network structures described in the related papers
are maintained with the exception of the input and output
dimensions. The training data set, validation data set, and
testing data set contain 100,000, 30,000, and 20,000 samples
respectively. For consistent comparisons, the testing set that
MLCF and the benchmark algorithms employ is the same.
TABLE II
PERFORMANCE (NMSE I NdB, C OSINE SIMILARITY ) ANDCOMPLEXITY
COMPARISONS
γ MethodsPerformance Complexity
NMSE ρTrainable
ParametersFLOPs
1/16CLNet
NR-CsiNet
FISTA-Net
MLCF-16.76
-19.97
-22.88
-23.740.9905
0.9960
0.9981
0.9982593.798K
593.336K
331.672K
81.6K2.332M
3.279M
26.428M
23.706M
1/8CLNet
NR-CsiNet
FISTA-Net
MLCF-19.97
-21.45
-23.16
-25.060.9960
0.9974
0.9985
0.99861.184M
1.183M
626.576K
86.4K2.922M
3.869M
27.018M
24.762M
1/4CLNet
NR-CsiNet
FISTA-Net
MLCF-21.89
-21.49
-23.26
-30.400.9977
0.9974
0.9985
0.99962.364M
2.363M
1.216M
96K4.102M
5.049M
28.198M
26.874M
Table. II summarizes the performance and complexity com-
parisons among the proposed MLCF and the benchmarks.
With respect to the reconstruction performance, the best results
for NMSE and cosine similarity ρare presented in bold
fonts. The parameter kis set to 50. One may observe that
our proposed MLCF outperforms the other algorithms at all
compression ratios. In particular, at a compression ratio of γ
= 1/4, our MLCF brings a gain of at least 7 dB in terms of
the NMSE.
Turning to complexity, the results of the lightest algo-
rithm, i.e., the trainable parameters and floating point of
operations (FLOPs), are shown in bold. From Table. II, it
can be seen that the proposed MLCF has fewer trainable
parameters. Coupled with the likewise small training data
set, there is a relatively low training overhead for MLCF.
However, the proposed MLCF is second only to FISTA-Net inFLOPs. MLCF sacrifices a certain amount of computational
complexity in exchange for better reconstruction performance.
In addition, it is noted that as the compression rate increases,
the computational complexity of each method increases more.
200025003000350040004500500055006000
The size of data set-32-31-30-29-28-27-26-25-24-23-22NMSE [dB] MLCF . = 1/4
MLCF . = 1/4 rand
MLCF . = 1/8
MLCF . = 1/8 rand
MLCF . = 1/16
MLCF . = 1/16 rand
Fig. 2. The normalized mean square error vs. the size of data set X.
Next, we show the impact of the size of data set Xon the
reconstruction performance, as shown in Fig. 2. The values
ofλandkare set to 0.05 and 80, respectively. For a certain
compression ratio, the NMSE gradually decreases and eventu-
ally tends to converge as the size value Nincreases, indicating
that greater reconstruction performance may be obtained with
a larger training size. Moreover, the computational complexity
of Algorithm 1 is proportional to N, which determines the time
required to learn the dictionaries. Based on this phenomenon,
it is possible to make a trade-off between the reconstruction
performance and the training time. In the meanwhile, we show
the case of randomly chosen landmarks, labeled “rand”. The
NMSE is lower when reconstructing the CSI with the proposed
MLCF scheme, demonstrating the effectiveness of our scheme.
50 100 150 200 250 300 350 400
The number of neighbors-34-32-30-28-26-24-22NMSE [dB]MLCF Nt=64 . = 1/4
MLCF Nt=64 . = 1/8
MLCF Nt=64 . = 1/16
MLCF Nt=32 . = 1/4
MLCF Nt=32 . = 1/8
MLCF Nt=32 . = 1/16
Fig. 3. The normalized mean square error vs. the number of neighbors k.10
Fig. 3 depicts how the number of nearest neighbors kaffects
the reconstruction quality NMSE. The size of dictionary M
is set to 600. The value kranges from 50 to 400 with a step
size of 50. It can be found that as kincreases, the NMSE
decreases, indicating that having more neighbors enhances the
reconstruction performance. In fact, there is no systematic
guide for the choice of k. Generally it can be selected based on
Fig. 3 and adjusted according to the actual process. In addition,
the BS antenna configurations of 32 and 64 are presented. For
the same number of neighbors and compression ratio, the more
antennas, the lower the reconstruction quality.
0 80 160 240 320 400
The number of iterations234567891011LossMLCF . = 1/4
MLCF . = 1/8
MLCF . = 1/16
Fig. 4. The loss of the objective function Eq. (16) vs. the number of iterations.
Since the explicit mapping function fis not available, the
problem of dimensionality reduction is solved by optimizing
the objective function Eq. (16), which is the upper bound of
the original objective function Eq. (14). In this simulation,
we verify the convergence of Algorithm 1 proposed to solve
Eq. (16), as shown in Fig. 4. In each iteration, the updated
dictionary and weight matrix are used to calculate the loss
value of Eq. (16). The loss value is affected by the training
sizeN, thus it is divided by Nfor normalization. We discover
that the loss of Eq. (16) ultimately converges after numerous
iterations, which is aligned with our theoretical analysis in
Theorem 3.
To apply the limited feedback algorithms to the realistic
communication scenarios, quantization is considered in the
downlink CSI feedback process. That is, the compressed CSI
is quantized by binary numbers before being transmitted to
the BS. Uniform quantization is performed for the case of γ=
1/16. The performance of NMSE under different quantization
bits and different algorithms is shown in Fig. 5. As can be
observed, when the number of quantization bits is 10, the
NMSE performance of all algorithms except FISTA-Net is not
significantly worse than the unquantized situation. However,
with lower quantization bits, a degradation in performance is
observed, which is expected. The above results demonstrate
that the proposed MLCF may be applicable in practical
communication scenarios.
Fig. 5. The normalized mean square error vs. the algorithms with quantization
and non-quantization.
We evaluate how the noise in the channel estimation affects
the reconstruction quality. The networks of the benchmarks
and the dictionaries of our MLCF are fixed in the simulation;
they are all acquired through training with perfect CSI. The
compression ratio is 1/8 and λis set to 0.05. And the signal-
to-noise ratio (SNR) of channel estimation ranges from 5 dB
to 30 dB. As shown in Fig. 6, it is clear that our proposed
MLCF performs well even under noisy channel estimation
conditions, indicating that it is robust to noise. Furthermore, as
the SNR of channel estimation rises, so does the reconstruction
performance of each method.
5 10 15 20 25 30
SNR of channel estimation [dB]-30-25-20-15-10-505NMSE [dB]CLNet
NR-CsiNet
FISTA-Net
MLCF
Fig. 6. The normalized mean square error vs. SNR of channel estimation.
Finally, Fig. 7 shows the spectral efficiency of FDD massive
MIMO systems with different SNRs. The downlink precoder
is the Eigen Zero-Forcing. The curve labeled “Perfect CSI”
means the BS has perfect downlink CSI, which is the upper-
bound. The compression ratio and quantization bits are 1/1611
0 5 10 15 20 25 30
SNR [dB]051015Spectral Efficiency [bps/Hz]Perfect CSI
CLNet
NR-CsiNet
FISTA-Net
MLCF
Fig. 7. The spectral efficiency vs. SNR.
and 4, respectively. The SE performance of our proposed
MLCF is very close to the ideal case, indicating that the error
between the reconstructed channel and the original channel is
very small.
VI. C ONCLUSION
In this paper, a novel manifold learning-based CSI feedback
framework was proposed to reduce the feedback overhead of
FDD massive MIMO systems. We considered the intrinsic
manifold structure where the CSI samples reside. Without
prior knowledge of the manifold structure, we constructed a
data set consisting of high-density CSI samples to characterize
it. In order to streamline the data set, a landmark selection
algorithm was proposed to select the most representative
landmarks so as to reconstruct all the CSI samples in the data
set with minimum error. At the same time, the convergence
of the algorithm was proved theoretically. Based on the pre-
obtained landmarks, we proposed a low-complexity algorithm
to efficiently compress and reconstruct the incremental CSI by
keeping the local relationship with the landmarks unchanged.
Simulation results showed that our proposed MLCF retained
the advantages of manifold learning to achieve dimensionality
reduction, and had superior reconstruction performance than
existing DL-based algorithms.
APPENDIX A
PROOF OF PROPOSITION 1
Proof : For an arbitrary CSI sample xi=g(yi), the linear
structure of its neighborhood can be characterized by the
tangent space of the manifold Matxi. Assume the manifold is
smooth enough and the dictionaries have been obtained. Using
first-order Taylor expansion of gatxi, a neighbor dj=g(bj)
in the landmarks can be represented as
dj=xi+Jg(yi)(bj−yi) +δ(yi,bj), (39)
where Jg(yi)∈R2Nf×dis the Jacobi matrix of gatyi,
whose column vectors span the tangent space, and δ(yi,bj)is the reminder term beyond the first-order expansion, which
measures the approximation error of xito the tangent space.
In particular, the l-th component of δ(yi,bj)is approximately
equal to δl≈1
2(bj−yi)TΨgl(yi)(bj−yi), where Ψgl(yi)
is the Hessian matrix of the l-th component function glofg
atyi.
Based on the sum-to-one constraintPM
j=1wji= 1, the error
between the sample xiand its local linear approximation is
εi=xi−MX
j=1wjidj=MX
j=1wji(xi−dj). (40)
Substituting Eq. (39) into the above error, we have
εi=MX
j=1wji(Jg(yi)(bi−yj) +δ(yi,bj))
≤Jg(yi)MX
j=1wji(bi−yj)+ξMX
j=1∥δ(yi,bj)∥
≤Jg(yi)MX
j=1wji(bi−yj)
+ξMX
j=12NfX
l=11
2(bi−yj)TΨgl(yi)(bi−yj)
≤Jg(yi)(MX
j=1wjibj−yi)
| {z }
e1+ξ1∥Ψ∥FMX
j=1∥bi−yj∥2
| {z }
e2
≤ξ∥Jg(yi)∥FMX
j=1∥bj−yi∥
| {z }
e1+ξ1∥Ψ∥FMX
j=1∥bj−yi∥2
| {z }
e2,
(41)
where ξ= max {wji}jis the largest entry in wi,ξ1=q
Nf
2ξ,
andΨ=h
ΨT
g1(yi). . .ΨT
g2Nf(yi)iT
∈R2Nfd×d.
Two terms make up the approximation error, as can be seen.
In the first term, since the column vectors of Jg(yi)span the
tangent space of gatyi,Jg(yi)yirepresents the projection of
yion the tangent space. Therefore, e1reflects the projected
distance between yiandP
jwjibjon the tangent space. In
the second term, the Hessian matrix Ψand its upper bound
∥Ψ∥Fare determined by the local curvature of the manifold
atxi. We have no prior knowledge of the manifold structure
in a learning task, hence the constrains on the manifold such
as local curvature are impractical in real implementations.
Taking another shortcut, we consider the influence of neigh-
bors on the approximation error. If the neighbors of xiin
the dictionary Ddr
Hlie in a sufficiently compact region, the
hyperplane spanned by the neighbors is almost the same as
the tangent space. In this case, max{∥bj−yi∥}jwill be tiny.
Meanwhile, the local curvature at xiand∥Ψ∥Fwill be zero
or close to zero. At this point, the approximation error εiis
undoubtedly quite small. Thus, Proposition 1 is proved. □12
APPENDIX B
PROOF OF THEOREM 1
Proof : Eliminating the terms irrelevant to dj, Eq. (18) can
be simplified to
L(dj) =∥E−djWj∗∥2
F+λNX
i=1w2
ji∥xi−dj∥2
= tr
(E−djWj∗)(E−djWj∗)T	
+λNX
i=1w2
ji(xi−dj)T(xi−dj),(42)
where each term employs the Lp-norm regularizer and satisfies
p≥1. Obviously, the objective function Eq. (42) is convex.
The gradient of L(dj)with regard to djis
∂L(dj)
∂dj=−2(E−djWj∗)WT
j∗−2λNX
i=1w2
ji(xi−dj).(43)
By setting ∂L(dj)/∂djto be zero, the optimal solution to dj
is as follows
dj=EWT
j∗+λX(W2
j∗)T
(1 +λ)Wj∗WT
j∗, (44)
where W2
j∗represents the square of each entry in Wj∗, and
X(W2
j∗)T=NP
i=1w2
jixi. Thus, Theorem 1 is proved. □
APPENDIX C
PROOF OF THEOREM 2
Proof : Taking the constraint eTˆwi= 1 into account, the
first term in Eq. (20) can be rewritten as
∥xi−Nxiˆwi∥2=(xieT−Nxi)ˆwi2=ˆwT
iRˆwi,(45)
where R= (xieT−Nxi)T(xieT−Nxi).
The second term can be reformulated as
kX
j=1ˆw2
ji∥xi−daj∥2=ˆwT
iφ(R)ˆwi. (46)
Note that ∥xi−da1∥2, . . . ,∥xi−dak∥2are the same as the
diagonal elements of R. We define a matrix operator φ(·)that
preserves only the diagonal elements of the matrix and sets
the rest to zero. Therefore, for simplicity, we have φ(R) =
diag(∥xi−da1∥2, . . . ,∥xi−dak∥2).
The derivative of the third term ∥w∥2,1equals the derivative
oftr(wTGw), where Ghas been defined in Eq. (21).
The above three items all satisfy the norm greater than or
equal to 1, thus it is clear that Eq. (20) is a convex function.
Introducing the Lagrange multiplier, the optimization problem
can be reformulated as
L(ˆwi, η) =∥xi−Nxiˆwi∥2+λkX
j=1ˆw2
ji∥xi−daj∥2
+µtr(WTGW) +η(eTˆwi−1)
=ˆwT
i 
R+λφ(R)ˆwi+µtr(WTGW)
+η(eTˆwi−1).(47)Setting the partial derivatives of L(ˆwi, η)with regard to ˆwi
andηto be zero, we have


∂L
∂ˆwi= 2 (R+λφ(R))ˆwi+ 2µˆGˆwi+ηe= 0
∂L
∂η=eTˆwi−1 = 0,(48)
where ˆG= diag( ga1a1, ga2a2, . . . , g akak)∈Rk×k. Therefore,
we have the optimal solution to the weight sub-vector as
ˆwi=
R+λφ(R) +µˆG−1
e
eT
R+λφ(R) +µˆG−1
e. (49)
Thus, Theorem 2 is proved. □
REFERENCES
[1] Y . Cao, H. Yin, G. He, and M. Debbah, “Manifold Learning-Based
CSI Feedback in Massive MIMO Systems,” in IEEE International
Conference on Communications (IEEE ICC 2022) , Seoul,South Korea,
May 2022, pp. 225–230.
[2] E. G. Larsson, O. Edfors, F. Tufvesson, and T. L. Marzetta, “Massive
MIMO for next generation wireless systems,” IEEE Commun. Mag. ,
vol. 52, no. 2, pp. 186–195, Feb. 2014.
[3] T. L. Marzetta, “Massive MIMO: an introduction,” Bell Labs Tech. J. ,
vol. 20, pp. 11–22, 2015.
[4] ——, “Noncooperative cellular wireless with unlimited numbers of base
station antennas,” IEEE Trans. Wireless Commun. , vol. 9, no. 11, pp.
3590–3600, Nov. 2010.
[5] D. J. Love, R. W. Heath, V . K. Lau, D. Gesbert, B. D. Rao, and M. An-
drews, “An overview of limited feedback in wireless communication
systems,” IEEE J. Sel. Areas Commun. , vol. 26, no. 8, pp. 1341–1365,
2008.
[6] Z. Qin, H. Yin, Y . Cao, W. Li, and D. Gesbert, “A partial reciprocity-
based channel prediction framework for FDD massive MIMO with high
mobility,” IEEE Trans. Wireless Commun. , vol. 21, no. 11, pp. 9638–
9652, 2022.
[7] P. Liang, J. Fan, W. Shen, Z. Qin, and G. Y . Li, “Deep learning
and compressive sensing-based CSI feedback in FDD massive MIMO
systems,” IEEE Trans. Veh. Technol. , vol. 69, no. 8, pp. 9217–9222,
Aug. 2020.
[8] V . Raghavan, R. W. Heath, and A. M. Sayeed, “Systematic codebook
designs for quantized beamforming in correlated MIMO channels,” IEEE
J. Sel. Areas Commun. , vol. 25, no. 7, pp. 1298–1310, Sep. 2007.
[9] K. Kim, T. Kim, D. J. Love, and I. H. Kim, “Differential feedback in
codebook-based multiuser MIMO systems in slowly varying channels,”
IEEE Trans. Commun. , vol. 60, no. 2, pp. 578–588, 2012.
[10] P.-H. Kuo, H. Kung, and P.-A. Ting, “Compressive sensing based chan-
nel feedback protocols for spatially-correlated massive antenna arrays,”
in2012 IEEE Wireless Communications and Networking Conference
(WCNC) , Apr. 2012, pp. 492–497.
[11] Z. Gao, L. Dai, S. Han, I. Chih-Lin, Z. Wang, and L. Hanzo, “Compres-
sive sensing techniques for next-generation wireless communications,”
IEEE Wireless Commun. , vol. 25, no. 3, pp. 144–153, 2018.
[12] P. N. Alevizos, X. Fu, N. D. Sidiropoulos, Y . Yang, and A. Bletsas,
“Limited feedback channel estimation in massive MIMO with non-
uniform directional dictionaries,” IEEE Trans. Signal Process. , vol. 66,
no. 19, pp. 5127–5141, 2018.
[13] H. Sun, Z. Zhao, X. Fu, and M. Hong, “Limited feedback double
directional massive MIMO channel estimation: From low-rank modeling
to deep learning,” in 2018 IEEE 19th International Workshop on Signal
Processing Advances in Wireless Communications (SPAWC) , 2018, pp.
1–5.
[14] C. Tian, A. Liu, M. B. Khalilsarai, G. Caire, W. Luo, and M.-J. Zhao,
“Randomized channel sparsifying hybrid precoding for FDD massive
MIMO systems,” IEEE Trans. Wireless Commun. , vol. 19, no. 8, pp.
5447–5460, 2020.
[15] Y . Yang, F. Gao, G. Y . Li, and M. Jian, “Deep learning-based downlink
channel prediction for FDD massive MIMO system,” IEEE Commun.
Lett., vol. 23, no. 11, pp. 1994–1998, 2019.13
[16] S. Zhang, Y . Liu, F. Gao, C. Xing, J. An, and O. A. Dobre, “Deep
learning based channel extrapolation for large-scale antenna systems:
Opportunities, challenges and solutions,” IEEE Wireless Commun. ,
vol. 28, no. 6, pp. 160–167, 2021.
[17] B. Lin, F. Gao, S. Zhang, T. Zhou, and A. Alkhateeb, “Deep learning-
based antenna selection and CSI extrapolation in massive MIMO sys-
tems,” IEEE Trans. Wireless Commun. , vol. 20, no. 11, pp. 7669–7681,
2021.
[18] C.-K. Wen, W.-T. Shih, and S. Jin, “Deep learning for massive MIMO
CSI feedback,” IEEE Wireless Commun. Lett. , vol. 7, no. 5, pp. 748–751,
Oct. 2018.
[19] Z. Lu, J. Wang, and J. Song, “Multi-resolution CSI feedback with
deep learning in massive MIMO system,” in ICC 2020-2020 IEEE
International Conference on Communications (ICC) , June 2020, pp. 1–
6.
[20] T. Wang, C.-K. Wen, S. Jin, and G. Y . Li, “Deep learning-based CSI
feedback approach for time-varying massive MIMO channels,” IEEE
Wireless Commun. Lett. , vol. 8, no. 2, pp. 416–419, 2018.
[21] M. B. Mashhadi, Q. Yang, and D. G ¨und¨uz, “Distributed deep convo-
lutional compression for massive MIMO CSI feedback,” IEEE Trans.
Wireless Commun. , vol. 20, no. 4, pp. 2621–2633, 2020.
[22] M. Nerini, V . Rizzello, M. Joham, W. Utschick, and B. Clerckx,
“Machine learning-based CSI feedback with variable length in FDD
massive MIMO,” IEEE Trans. Wireless Commun. , vol. 22, no. 5, pp.
2886–2900, 2022.
[23] W. Li, H. Yin, Z. Qin, Y . Cao, and M. Debbah, “A Multi-Dimensional
Matrix Pencil-Based Channel Prediction Method for Massive MIMO
with Mobility,” IEEE Trans. Wireless Commun. , 2022.
[24] H. Yin, D. Gesbert, M. Filippou, and Y . Liu, “A coordinated approach
to channel estimation in large-scale multiple-antenna systems,” IEEE J.
Sel. Areas Commun. , vol. 31, no. 2, pp. 264–273, 2013.
[25] H. Yin, D. Gesbert, and L. Cottatellucci, “Dealing with interference in
distributed large-scale MIMO systems: A statistical approach,” IEEE J.
Sel. Top. Signal Process. , vol. 8, no. 5, pp. 942–953, 2014.
[26] D. J. Love, R. W. Heath, and T. Strohmer, “Grassmannian beamforming
for multiple-input multiple-output wireless systems,” IEEE Trans. Inf.
Theory , vol. 49, no. 10, pp. 2735–2747, 2003.
[27] K. Bhogi, C. Saha, and H. S. Dhillon, “Learning on a Grassmann
manifold: CSI quantization for massive MIMO systems,” in 2020 54th
Asilomar Conference on Signals, Systems, and Computers . IEEE, 2020,
pp. 179–186.
[28] T. Lin and H. Zha, “Riemannian manifold learning,” IEEE Trans. Pattern
Anal. Mach. Intell. , vol. 30, no. 5, pp. 796–809, May 2008.
[29] S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by
locally linear embedding,” Science , vol. 290, no. 5500, pp. 2323–2326,
2000.
[30] Z. Zhang and H. Zha, “Principal manifolds and nonlinear dimensionality
reduction via tangent space alignment,” SIAM J. Sci. Comput. , vol. 26,
no. 1, pp. 313–338, 2004.
[31] C. Zhang, J. Wang, N. Zhao, and D. Zhang, “Reconstruction and analysis
of multi-pose face images based on nonlinear dimensionality reduction,”
Pattern Recognition , vol. 37, no. 2, pp. 325–336, 2004.
[32] 3GPP, Study on channel model for frequencies from 0.5 to 100 GHz (Re-
lease 16) . Technical Report TR 38.901, available: http://www.3gpp.org,
2019.
[33] L. Van Der Maaten, E. O. Postma, and H. J. van den Herik, “Dimension-
ality reduction: A comparative review,” J. Mach. Learn. Res. , vol. 10,
no. 66-71, p. 13, 2009.
[34] K. Zhang and J. T. Kwok, “Clustered Nystr ¨om method for large
scale manifold learning and dimension reduction,” IEEE Trans. Neural
Networks , vol. 21, no. 10, pp. 1576–1587, 2010.
[35] W. M. Boothby and W. M. Boothby, An introduction to differentiable
manifolds and Riemannian geometry . Gulf Professional Publishing,
2003, vol. 120.
[36] S. K. Sahoo and A. Makur, “Dictionary training for sparse representation
as generalization of k-means clustering,” IEEE Signal Process Lett. ,
vol. 20, no. 6, pp. 587–590, 2013.
[37] F. Nie, H. Huang, X. Cai, and C. Ding, “Efficient and robust feature
selection via joint L2,1-norms minimization,” Advances in neural infor-
mation processing systems , vol. 23, 2010.
[38] S. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization .
Cambridge university press, 2004.
[39] J. Wang, G. Gui, T. Ohtsuki, B. Adebisi, H. Gacanin, and H. Sari,
“Compressive sampled CSI feedback method based on deep learning for
FDD massive MIMO systems,” IEEE Trans. Commun. , vol. 69, no. 9,
pp. 5873–5885, 2021.[40] S. Ji and M. Li, “CLNet: Complex input lightweight neural network
designed for massive MIMO CSI feedback,” IEEE Wireless Commun.
Lett., vol. 10, no. 10, pp. 2318–2322, 2021.
[41] E. Zimaglia, D. G. Riviello, R. Garello, and R. Fantini, “A novel
deep learning approach to CSI feedback reporting for NR 5G cellular
systems,” in 2020 IEEE Microwave Theory and Techniques in Wireless
Communications (MTTW) , vol. 1. IEEE, 2020, pp. 47–52.
[42] J. Guo, L. Wang, F. Li, and J. Xue, “CSI feedback with model-driven
deep learning of massive MIMO systems,” IEEE Commun. Lett. , vol. 26,
no. 3, pp. 547–551, 2021.
Yandi Cao received the B.Sc. degree in Commu-
nication Engineering from the School of Microelec-
tronics and Communication Engineering, Chongqing
University, Chongqing, China, in 2020. She is cur-
rently pursuing the Ph.D. degree with the School
of Electronic Information and Communications from
Huazhong University of Science and Technology,
Wuhan, China. Her research interests include ma-
chine learning, signal processing, and channel feed-
back for massive MIMO systems.
Haifan Yin (Senior Member, IEEE) received the
B.Sc. degree in electrical and electronic engineering
and the M.Sc. degree in electronics and informa-
tion engineering from the Huazhong University of
Science and Technology, Wuhan, China, in 2009
and 2012, respectively, and the Ph.D. degree from
T´el´ecom ParisTech in 2015. From 2009 to 2011,
he was a Research and Development Engineer with
the Wuhan National Laboratory for Optoelectronics,
Wuhan, working on the implementation of TD-LTE
systems. From 2016 to 2017, he was a DSP Engineer
at Sequans Communications (IoT chipmaker), Paris, France. From 2017 to
2019, he was a Senior Research Engineer working on 5G standardization at
Shanghai Huawei Technologies Company Ltd., where he has made substantial
contributions to 5G standards, particularly the 5G codebooks. Since May
2019, he has been a Full Professor with the School of Electronic Information
and Communications, Huazhong University of Science and Technology. His
current research interests include 5G and 6G networks, signal processing, ma-
chine learning, and massive MIMO systems. He was the National Champion
of 2021 High Potential Innovation Prize awarded by the Chinese Academy
of Engineering, a recipient of the China Youth May Fourth Medal (the top
honor for young Chinese), and a recipient of the 2024 Stephen O. Rice Prize.
Ziao Qin received the B.S. degree in information
engineering from Beijing Institute of Technology,
Beijing, China, in 2014. From 2014 to 2017, he
worked in industry in Beijing, China. He received
the Ph.D. degree in information and communications
engineering from Huazhong University of Science
and Technology, Wuhan, China in 2024. His research
interests include channel estimation, signal process-
ing, codebook design and beamforming for massive
MIMO systems.14
Weidong Li received the B.Sc degree in electronic
information science and technology from Nanjing
Agricultural University, Nanjing, China, in 2017,
and the M.Sc degree in electronic engineering from
the Nanjing University of Aeronautics and Astro-
nautics, Nanjing, China, in 2020. He is currently
pursuing the Ph.D. degree with the School of
Electronic Information and Communications from
Huazhong University of Science and Technology,
Wuhan, China. His research interests include chan-
nel estimation, signal processing, and the mobility
of massive MIMO and ELAA.
Weimin Wu received the B.E. degree in computer
software from Xidian University, Xi’an, China, in
1992, the M.E. degree in computer application from
Sichuan University, Chengdu, China, in 1995, and
the Ph.D. degree in communications and infor-
mation systems from the Huazhong University of
Science and Technology, Wuhan, China, in 2007.
He is currently an Associate Professor with the
School of Electronic Information and Communica-
tions, Huazhong University of Science and Technol-
ogy. His current research interests include Internet
streaming, broadband wireless communications, and networking.
M´erouane Debbah (Fellow, IEEE) is Chief Re-
searcher at the Technology Innovation Institute in
Abu Dhabi. He is a Professor at Centralesupelec
and an Adjunct Professor with the Department of
Machine Learning at the Mohamed Bin Zayed Uni-
versity of Artificial Intelligence. He received the
M.Sc. and Ph.D. degrees from the Ecole Normale
Sup´erieure Paris-Saclay, France. He was with Mo-
torola Labs, Saclay, France, from 1999 to 2002, and
also with the Vienna Research Center for Telecom-
munications, Vienna, Austria, until 2003. From 2003
to 2007, he was an Assistant Professor with the Mobile Communications
Department, Institut Eurecom, Sophia Antipolis, France. In 2007, he was
appointed Full Professor at CentraleSupelec, Gif-sur-Yvette, France. From
2007 to 2014, he was the Director of the Alcatel-Lucent Chair on Flexible
Radio. From 2014 to 2021, he was Vice-President of the Huawei France
Research Center. He was jointly the director of the Mathematical and Algo-
rithmic Sciences Lab as well as the director of the Lagrange Mathematical
and Computing Research Center. Since 2021, he is leading the AI & Digital
Science Research centers at the Technology Innovation Institute. He has
managed 8 EU projects and more than 24 national and international projects.
His research interests lie in fundamental mathematics, algorithms, statistics,
information, and communication sciences research. He is an IEEE Fellow,
a WWRF Fellow, a Eurasip Fellow, an AAIA Fellow, an Institut Louis
Bachelier Fellow and a Membre ´em´erite SEE. He was a recipient of the
ERC Grant MORE (Advanced Mathematical Tools for Complex Network
Engineering) from 2012 to 2017. He was a recipient of the Mario Boella
Award in 2005, the IEEE Glavieux Prize Award in 2011, the Qualcomm
Innovation Prize Award in 2012, the 2019 IEEE Radio Communications
Committee Technical Recognition Award and the 2020 SEE Blondel Medal.
He received more than 20 best paper awards, among which the 2007 IEEE
GLOBECOM Best Paper Award, the Wi-Opt 2009 Best Paper Award, the
2010 Newcom++ Best Paper Award, the WUN CogCom Best Paper 2012
and 2013 Award, the 2014 WCNC Best Paper Award, the 2015 ICC Best
Paper Award, the 2015 IEEE Communications Society Leonard G. Abraham
Prize, the 2015 IEEE Communications Society Fred W. Ellersick Prize, the
2016 IEEE Communications Society Best Tutorial Paper Award, the 2016
European Wireless Best Paper Award, the 2017 Eurasip Best Paper Award,
the 2018 IEEE Marconi Prize Paper Award, the 2019 IEEE Communications
Society Young Author Best Paper Award, the 2021 Eurasip Best Paper Award,
the 2021 IEEE Marconi Prize Paper Award, the 2022 IEEE Communications
Society Outstanding Paper Award, the 2022 ICC Best paper Award as well
as the Valuetools 2007, Valuetools 2008, CrownCom 2009, Valuetools 2012,
SAM 2014, and 2017 IEEE Sweden VT-COM-IT Joint Chapter best student
paper awards. He is an Associate Editor-in-Chief of the journal Random
Matrix: Theory and Applications. He was an Associate Area Editor and Senior
Area Editor of the IEEE TRANSACTIONS ON SIGNAL PROCESSING from
2011 to 2013 and from 2013 to 2014, respectively. From 2021 to 2022, he
serves as an IEEE Signal Processing Society Distinguished Industry Speaker.