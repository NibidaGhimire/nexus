O2RNet: Occluder-Occludee Relational Network for Robust Apple Detection in
Clustered Orchard Environments
Pengyu Chua, Zhaojian Lia, Kaixiang Zhanga, Dong Chena, Kyle Lammersa, Renfu Lub
Zhaojian Li (lizhaoj1@msu.edu) is the corresponding author
aDepartment of Mechanical Engineering, Michigan State University, East Lansing, MI 48824, USA
bDepartment of Agriculture (USDA) Agricultural Research Service (ARS), East Lansing, MI 48824, USA
Abstract
Automated apple harvesting has attracted signiÔ¨Åcant research interest in recent years due to its potential to revolu-
tionize the apple industry, addressing the issues of shortage and high costs in labor. One key technology to fully enable
ecient automated harvesting is accurate and robust apple detection, which is challenging due to complex orchard
environments that involve varying lighting conditions and foliage /branch occlusions. Furthermore, clustered apples
are common in the orchard, which brings additional challenges as the clustered apples may be identiÔ¨Åed as one apple.
This will cause issues in localization for subsequent robotic operations. In this paper, we present the development
of a novel deep learning-based apple detection framework, Occluder-Occludee Relational Network (O2RNet), for
robust detection of apples in such clustered environments. This network exploits the occuluder-occludee relationship
modeling head by introducing a feature expansion structure to enable the combination of layered traditional detectors
to split clustered apples and foliage occlusions. More speciÔ¨Åcally, we collect a comprehensive apple orchard image
dataset under di erent lighting conditions (overcast, front lighting, and back lighting) with frequent apple occlusions.
We then develop a novel occlusion-aware network for apple detection, in which a feature expansion structure is in-
corporated into the convolutional neural networks to extract additional features generated by the original network for
occluded apples. Comprehensive evaluations are performed, which show that the developed O2RNet outperforms
state-of-the-art models with a higher accuracy of 94% and a higher F1-score of 0.88 on apple detection.
Keywords: computer vision, apple detection, fruit harvesting, occlusion-aware detection, transfer learning
1. Introduction
Driven by rising costs and growing shortages in har-
vesting labor, robotic apple harvesting has gained in-
creased research attention over the past decade. In
the U.S. alone, fruit harvesting requires more than 10
million worker hours annually, attributing to approxi-
mately 15% of the total apple production cost (Gallardo
and Galinato, 2012). Mechanization and automation
promise next-gen harvesting systems with low operat-
ing cost and high e ciency, as well as the ability to as-
sess individual fruit for quality and maturity at the point
of harvest (Li et al., 2016).
As such, several research groups have been develop-
ing robotic harvesting systems (Kang and Chen, 2019;
Wan and Goudos, 2020; Qingchun et al., 2012; De-
An et al., 2011; Zhang et al., 2021). Despite pro-
gresses, several important challenges in developing a
fully functional robotic harvesting system remain, andno commercially-viable systems are yet available in the
market. One key challenge that is pointed out by the
existing works is e cient and robust fruit detection in
the presence of varying light conditions and fruit /foliage
occlusions. Indeed, the perception system provides the
robot system with information on target fruits, which
are Ô¨Årst and foremost for subsequent planning and con-
trol tasks. In addition, fruit perception techniques have
also been used in other applications of interest, includ-
ing yield estimation and crop health status monitor-
ing (Patel et al., 2011). Perception in unstructured or-
chard environments, however, is a daunting task as a re-
sult of variations in illumination and appearance, noisy
backgrounds, and clustered environments with occlu-
sions (Chu et al., 2021). The goal of this paper is thus
to present a novel deep learning-based detection algo-
rithm to convergently address the aforementioned chal-
lenges. We show that the developed algorithm is able to
1arXiv:2303.04884v1  [cs.CV]  8 Mar 2023achieve state-of-the-art performance. Before describing
the technical details, we review relevant backgrounds
and state-of-the-art approaches to put our algorithm in
better context.
1.1. Image Sensing Techniques
Vision-based perception schemes can be classiÔ¨Åed
into four categories based on the sensor used: monocu-
lar camera scheme, binocular stereovision scheme, laser
active visual scheme, and thermal imaging scheme,
which cover both two-dimension imaging schemes and
three-dimension imaging schemes (Zhao et al., 2016).
SpeciÔ¨Åcally, the monocular scheme uses a single camera
to acquire image data, and it is widely used in fruit har-
vesting due to its low cost and rich information provided
by the RGB images. For instance, Tian et al. (2019a) de-
veloped an improved YOLOv3 (Redmon and Farhadi,
2018) model based on a single camera to detect apples
with an accuracy of 85.0%. In Kang and Chen (2020),
the authors proposed a new LedNet model for apple de-
tection that achieves an accuracy of 85.3%. The main
disadvantage of the monocular scheme is that the color
images are sensitive to Ô¨Çuctuating illumination.
Dierent from the monocular camera schemes, the
binocular stereovision schemes exploit two cameras
separated in a certain distance /angle to obtain two im-
age data on the same scene. The point cloud of fruit
can then be constructed through triangulation on ex-
tracted features (Sun et al., 2011). For instance, Si et al.
(2015) used a stereo camera to detect and localize ma-
ture apples in tree canopies, and achieved an accuracy
of 89.5%. In Xiang et al. (2014), the authors developed
a clustered tomato detection method based on a stereo
camera, and the recognition accuracy was 87.9%. Al-
though the stereovision scheme tends to render better
results, it su ers from high complexity, long computa-
tion time, and uncertainties in stereo matching (Hannan
and Burks, 2004).
On the other hand, the laser active visual schemes ob-
tain three-dimensional features using laser scans, where
laser beam reÔ¨Çections are exploited to generate a 3D
point cloud based on the time-of-Ô¨Çight principle. The
3D point cloud can then be used to reconstruct the
scene. For example, (Tanigaki et al., 2008) utilized in-
frared laser scanning devices to recognize cherry on the
tree. (Zhang et al., 2015) acquired a total of 200 im-
ages for independent ‚ÄòFuji‚Äô apples and developed an ap-
ple recognition method using the near-infrared linear-
array structured light for 3D reconstruction. (Tsoulias
et al., 2020) proposed a point cloud based apple detec-
tion method using a LiDAR laser scanner and reached
a 88.2% overall accuracy on the defoliated tree dataset(Tsoulias et al., 2020). Note the defoliated scene is sig-
niÔ¨Åcantly less challenging than the real orchard condi-
tions during the harvest season. Furthermore, the laser
point cloud is generally sparse and it is challenging to
be used in real-world orchards with dense backgrounds.
The high cost and complexity also limit its practical ap-
plication in agricultural applications.
Finally, the thermal imaging schemes make use of
the distinct thermal characteristics of fruit and leaves
(e.g., the di erent temperature distributions) to obtain
the visualization of infrared radiation (Lu et al., 2014).
In Bulanon et al. (2008), citruses are successfully seg-
mented using a thermal infrared camera according to
the largest temperature di erence in both day and night
conditions. An enhanced approach for fruit detection
(Bulanon et al., 2009) was developed using the combi-
nation of the thermal image and the color image. The
results showed a promising performance under weak
lighting environments. However, in the thermal imaging
scheme, the accuracy of recognition is largely a ected
by the shadow of the tree canopy (Stajnko et al., 2004).
Considering the cost, performance, and real-time
constraints, our work focuses on the monocular cam-
era scheme, the state-of-art of which will be discussed
next.
1.2. Recognition Approaches
Image-based fruit recognition approaches can be
classiÔ¨Åed into feature analysis approaches and deep
learning-based approaches, depending on how features
are obtained. In feature analysis approaches, hand-
crafted features are Ô¨Årst extracted based on the fruit
characteristics, and classiÔ¨Åcation approaches are then
developed to recognize fruit. Slaughter and Harrell
(1987); Sites and Delwiche (1988) developed thresh-
olding methods to classify fruit from other background
objects using smoothing Ô¨Ålters that remove irrelevant
noises. The large segmented regions are then recog-
nized as fruits. This method is capable of segmenting
fruit regions in simple backgrounds but it is susceptible
to varying lighting conditions and complex canopies.
Whittaker et al. (1987); Benady and Miles (1992) pro-
posed a circular Hough Transform approach to obtain
binary edge images and then used a voting matrix to
identify fruits. This approach is sensitive to complex
structured environments and it generally fails in a dense
scene. In Qiu and Shearer (1992); Cardenas-Weber et al.
(1991); Levi et al. (1988); Zhao et al. (2005), they com-
bined the shape and texture of the fruit to obtain a richer
set of feature representations. Then, extracted features
between fruit and leaves are compared and contrasted to
2identify the fruits. However, this method is also sensi-
tive to lighting conditions and occlusions.
On the other hand, deep learning-based approaches
have found great successes in object detection and se-
mantic image segmentation Sa et al. (2016); Bargoti
and Underwood (2017). They can learn feature rep-
resentations automatically without the need of manual
feature engineering. Compared to conventional meth-
ods, Convolutional Neural Networks (CNNs) have been
showing great advantages in the Ô¨Åeld of object detec-
tion in recent years. The CNN makes it possible to
recognize fruits in complex situations due to its deep
extraction of high-dimensional features of objects. R-
CNN and its variants Fast R-CNN and Faster R-CNN
(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015)
have enjoyed particular successes. Their key idea is to
Ô¨Årst obtain regions of interest and then perform clas-
siÔ¨Åcation in the region. The Region proposal network
(RPN) is employed to reduce high computational costs
so that the model can simultaneously predict and clas-
sify object boundaries at each location. The parame-
ters of the two networks are shared, which results in
much faster inference and are thus optimized for real-
time purposes. Faster Region-Based CNN, proposed by
Sa et al. (2016), employed transfer learning using Im-
ageNet, and used both early fusion and late fusion to
integrate RGB and NIR (near infrared) inputs. Modi-
Ô¨Åed Inception-ResNet (MI-ResNet) (Rahnemoonfar and
Sheppard, 2017) used deep simulated learning for yield
estimation. The model was developed to address chal-
lenges including the varying degree of fruit sizes and
overlap, natural lighting, and foliage occlusions. The
overhead for object detection and localization is opti-
mized by utilizing synthetic data for training, and reach-
ing the accuracy of 91% on their fruit dataset. You Only
Look Once (YOLOv3) (Redmon and Farhadi, 2018), a
representative of the one-stage object detector, detects
the fruit on the entire image and classiÔ¨Åes fruit vari-
ety into uncertainty retail conditions without the help of
RPN. SpeciÔ¨Åcally, YOLOv3 uses logistic regression to
predict an objectless score for each bounding box. Due
to the simple optimization pipeline, YOLOv3 enjoys
much faster inference than the aforementioned region-
based methods. E cientDet (Tan et al., 2020), an aug-
mented variant of YOLOv3, exploits a pyramid network
to enable the detection of scaling targets.
However, the aforementioned Deep CNN approaches
do not address the challenge of fruit /foliage occlusions
in real-world orchards. Towards that end, Composi-
tional Convolutional Neural Network (Compositional-
Net) (Kortylewski et al., 2020) was proposed to de-
tect partially occluded objects. The framework ex-ploits a di erentiable fully compositional model that
uses occluder kernels to localize occluders (the occlud-
ing objects). Bilayer Convolutional Network (BCNet)
(Ke et al., 2021), another model to address the occlu-
sion challenge, applies two Graph Convolutional Net-
work (GCN) layers to separately infer the occluding
objects (occluder) and partially occluded instance (oc-
cludee). By sharing parameters between the top and
bottom GCN layers, BCNet decouples the occluder and
occludee on the input image. Superior performance was
reported on occluded scenarios.
1.3. Our Contributions
In this paper, we develop a novel Occluder-Occludee
Relational Network (O2RNet) to enhance apple de-
tection in the presence of occlusions in clustered ap-
ples that are frequently present in real-world orchards.
SpeciÔ¨Åcally, we employ ResNet (He et al., 2016) and
RPN (Ren et al., 2015) to extract features of targets
and utilize occluder-occludee layers to split candidates
into occluder and occludee. Compared to other occlu-
sion models, we only use bounding boxes as labels in-
stead of pixel-level masks that contain more texture and
shape information. In addition, we present a new ap-
ple dataset1collected in two Michigan apple orchards
in multiple harvesting seasons. We evaluate the perfor-
mance against state-of-the-art object detection models
and demonstrate superior performances. The contribu-
tions of this paper are highlighted as follows:
1. The presentation of a comprehensive apple dataset
consisting of 900 images with di erent lighting
conditions and occlusion levels collected in mul-
tiple orchards across multiple harvesting seasons.
2. The development of Occluder-Occludee Relational
Network (O2RNet), a novel occlusion-aware net-
work for enhanced apple detection in the presence
of occlusion due to apple clusters.
3. A comprehensive evaluation and benchmark of 12
state-of-the-art deep learning-based models for ap-
ple detection where we show that the developed
O2RNet outperforms state-of-the-art algorithms.
2. Materials and Methods
2.1. Data Collection and Processing
In this study, apple images were taken in two or-
chards: the commercial orchard in Sparta, Michigan,
1The database is open-sourced at https://github.com/
pengyuchu/MSUAppleDatasetv2.git .
3USA during the 2019 harvest season and the experimen-
tal orchard of Michigan State University in East Lans-
ing, Michigan, USA during the 2021 harvest season.
The apples are mainly ‚ÄòGala‚Äô that are generally red over
a green /yellow background (see Fig. 1). An RGB cam-
era with a resolution of 1280 720 was used to take im-
ages of apples at a distance of 1  2 meters from the tree
trunks, which is the typical range of harvesting robots
(De-An et al., 2011; Zhang et al., 2021, 2022). The im-
ages were collected across multiple days to cover both
cloudy and sunny weather conditions. In a single day,
the data were also collected at di erent times of the day,
including 9am, noon, and 3pm, to cover di erent light-
ing angles: front-lighting, back-lighting, side-lighting,
and scattered lighting. Furthermore, we also captured
clustered apples with di erent occlusion levels includ-
ing both foliage and branches occlusion. When captur-
ing images, the camera was placed parallel to the ground
and directly facing the trees to mimic the harvesting
scenario. Compared to our previous work (Chu et al.,
2021), an additional set of 200 images were added to
extend our dataset to a total of 900 images where a few
sample images are shown in Fig. 1.
We then processed the acquired raw orchard images
into formats that can be used to train and evaluate deep
networks. SpeciÔ¨Åcally, apples in the images were anno-
tated by rectangles using VGG Image Annotator (Dutta
and Zisserman, 2019), and the annotations were then
compiled into the human-readable format. Compared
to polygon and mask annotations, rectangular annota-
tion used here accelerates data preparation, particularly
in dense images like our dataset. The annotated dataset
was then split into training, validation, and test sub-
sets with the apple quantities of 7522, 3001, and 3995
respectively. The processed image database is open-
sourced and can be accessed at https://github.
com/pengyuchu/MSUAppleDatasetv2.git .
2.2. Transfer Learning
We employ transfer learning to enable faster training
and improved performance. Transfer learning is a pop-
ular scheme that starts the model development with a
pre-trained model on a large-scale dataset and then Ô¨Åne-
tunes the model on a customized dataset from the spe-
ciÔ¨Åc domain of interest (Zhuang et al., 2020). For apple
detection in this study, we used ImageNet (Deng et al.,
2009) to pre-train each model and only replaced the last
fully-connected layers in each model. Since there are
objects of apple and alike in ImageNet, the pre-trained
models converge faster in our customized apple dataset
compared to randomized initial parameters.
(a)(d)
(e)
(c)(f)(b)Figure 1: Six sample images from the collected dataset: (a)-(c) apples
on older trees under overcast, back-lighting, and direct lighting condi-
tions, respectively; and (d)-(e) apples on younger trees under overcast,
back-lighting, and direct lighting conditions, respectively.
2.3. Performance Metrics
For model development and evaluation, convention-
ally the apple dataset is randomly partitioned into train-
ing, validation, and test sets for model training and eval-
uation, respectively. To quantitatively evaluate the de-
tection performance, we use performance metrics in-
cluding precision, recall, and F1-score for algorithm
evaluation. All detection outcomes are divided into four
types: true positive ( T P), false positive ( FP), true neg-
ative ( T N), and false negative ( FN), based on the re-
lation between the true class and predicted class. The
precision ( P) and recall ( R) are deÔ¨Åned as follows:
P=T P
T P+FP;R=T P
T P+FN: (1)
The F1-score is then subsequently deÔ¨Åned as:
F1=2PR
P+R: (2)
4To better evaluate the precision between the predic-
tion and the ground truth, we also employ Microsoft
Common Objects in Context (COCO) dataset (Lin et al.,
2014) evaluation metrics. SpeciÔ¨Åcally, after the calcu-
lation of precision and recall, we calculate the average
precision ( AP) and average recall ( AR) based on dif-
ferent Intersection over Union (IoU) between the pre-
diction and the ground truth. For example, APIoU=:50
orAP50denotes that AP is averaged over IoU =
0:50 values, which belongs to PASCAL VOC metric
(RezatoÔ¨Åghi et al., 2019). We also use APIoU=:75or
AP75, which is a stricter metric for model evaluations.
In our study, we use a spectrum of 10 IoU thresholds
ranging 0:50 : 0:05 : 0:95 to average over multiple IoUs
to obtain a comprehensive set of results.
2.4. Data Augmentation
Data augmentation is a method that can be adopted to
increase data diversity for achieving robust training and
enhanced performance of computer vision models. For
example, transformations and rotations are frequently
employed to increase the number of images from a sin-
gle source. It has been shown to be a powerful tool
in agriculture applications (Wu et al., 2020; Su et al.,
2021; Divyanth et al., 2022) as it generates additional
data from existing orchard data. This is especially use-
ful for applications with a limited dataset by detecting
anomalies in images with di erent transformations and
making it possible to generate new training examples
without actually acquiring new data.
SpeciÔ¨Åcally, in the considered application of apple
detection in orchards, the collected dataset can only
cover a limited set of scenarios. Therefore, we ap-
plied several data augmentation techniques (Chlap et al.,
2021) on the collected and processed data to enhance
the data diversity for improving the inference perfor-
mance of our models. SpeciÔ¨Åcally, besides geometric
transformations including scaling, translating, rotating,
reÔ¨Çecting, and shearing, we also applied color space
augmentations such as modifying the brightness and
contrast to Ô¨Åt di erent intensities. In addition, we in-
jected Gaussian noises on the collected images by ran-
domly modifying the pixel intensities based on a Gaus-
sian distribution. Furthermore, we applied Mixup by
randomly selecting two images from the dataset and
blending the intensities of the corresponding voxels of
the two images (Lu et al., 2022). Filtering is another
augmentation approach we applied where we modify
the intensities of each pixel using convolution (Shorten
and Khoshgoftaar, 2019). SpeciÔ¨Åcally, we exploited
sharpening (Shorten and Khoshgoftaar, 2019) to detectand intensify the edges of objects found within the im-
age. We applied these additional augmentation tech-
niques on our dataset and the beneÔ¨Åts of data augmen-
tation will be demonstrated in the experiment section.
3. Methodology
In this section, we Ô¨Årst present the key challenges
of object detection in clustered environments and an
overview of the general object detection framework.
Based on those, we describe the proposed Occluder-
Occludee Relational Network (O2RNet) with explicit
occluder-occludee relation modeling. Finally, we spec-
ify the objective functions for the entire network opti-
mization, followed by details on the training and infer-
ence processes.
3.1. Challenge and Main Idea
(a)(b)(c)(d)
(e)(f)(g)(h)
Figure 2: Eight sample images from the collected dataset show cas-
caded apples in di erent occlusion levels: (a)-(d) apples are in the
normal occlusion and can be identiÔ¨Åed in most models; (e)-(h) apples
are highly cascaded and usually detected as one apple.
For images with heavy occlusions, multiple overlap-
ping objects captured in the same bounding box can
result in confusing object outlines from both front ob-
jects and occlusion boundaries. In apple orchards, the
apple clusters are very common (see Fig. 2 for a few
examples). However, the prediction head design of
Faster R-CNN directly regresses the occludee with a
fully convolutional network, which neglects both the
occluding instances and the overlapping relations be-
tween objects. With this limitation, Faster R-CNNs will
inevitably omit some occludes due to Non-maximum
Suppression (NMS). On the other hand, with a prop-
erly tuned threshold, the RPN can propose many can-
didates after feeding the target features from CNN (see
Fig. 3), but the NMS will suppress the nearby bound-
ing boxes and neglect occludees. Motivated by this ob-
servation, the proposed O2RNet aims at extending the
5existing two-stage object detection methods by adding
an occlusion perception branch parallel to the original
object prediction pipeline. By explicitly modeling the
relationship between occluder and occludee, the inter-
actions between objects within the Region of Interest
(RoI) region can be well incorporated during the bound-
ing box regression stage.
anchor pointfeature mapanchor boxes
Figure 3: Illustration of how RPN works: The RPN selects anchor
points on the feature map and generates anchor boxes for each anchor
point. The anchor boxes are generated based on two parameters ‚Äî
scales and aspect ratios.
3.2. O2RNet WorkÔ¨Çow
In this subsection, we describe our proposed O2RNet.
As illustrated in Fig. 4, the O2RNet follows the two-
stage architecture used in Faster R-CNN (Ren et al.,
2015) and consists of three main parts. First, we use
a Residual Network (ResNet) (He et al., 2016) as the
backbone for feature learning /extraction over the entire
image. SpeciÔ¨Åcally, we instantiate ResNet-101-FPN
(He et al., 2017) as its backbone for feature extraction,
as it outperforms other single ConvNets mainly due to
its capability of maintaining strong semantic features at
various resolution scales. Even though ResNet101 is a
deep network, the residual blocks and dropouts function
help it avoid gradient vanishing and exploding prob-
lems. Second, we employ an RPN (Ren et al., 2015)
to generate object regions, which is a small convolu-
tional network to convert feature maps into scored re-
gion proposals around where the object lies. The gener-
ated proposals with a certain height and width are called
anchors, which are a set of predeÔ¨Åned bounding boxes.
The anchors are designed to capture the scale and aspect
ratio of speciÔ¨Åc object classes and are typically chosen
to be consistent with object sizes in the dataset. RPN
is mainly used for predicting bounding boxes in Faster
R-CNN but it can also provide enough anchors with dif-
ferent scales that will be exploited in our network as
explained in the sequel. Third, we build an occlusion-
aware modeling head with a structure of two classiÔ¨Åca-
tion and regression branches for occluder and occludeefor decoupling overlapping relations and segments the
instance proposals obtained from the RPN. Compared
to the traditional class-agnostic classiÔ¨Åcation, we di-
vide this task into two complementary tasks: occluder
prediction using the original classiÔ¨Åcation head and oc-
cludee modeling with an additional Feature Expansion
Structure (FES), where the occluder predictions provide
rich foreground cues like textures and the FES predicts
the positions of occluding regions to guide occludee ob-
ject regression.
More speciÔ¨Åcally, an input image is Ô¨Årst processed
by the ResNet backbone to extract intermediate convo-
lutional features for downstream processing. The ob-
ject detection head (i.e., RPN) then predicts bounding
box proposals, which are then consumed by the occlu-
sion perception branches into the occluder branch and
the occluee branch. For the occluder branch, we adopt
the object detection head in Faster R-CNN (Ren et al.,
2015) to output positions as well as categories for in-
stance candidates and prepare the cropped RoI features
for the occludee branch. In the occludee branch, the
input consists of both cropped RoI features from the oc-
cluder branch and expanded features from FES, which
is targeted for modeling occluded regions by jointly de-
tecting boundaries. Essentially, the distilled occlusion
features are added to the original input RoI features
and passed to the next module. Finally, the occludee
branch, which has a similar structure to the occluder
branch, predicts the occludee guided by these expanded
features and outputs classes and bounding boxes for
the partially occluded instances. We next describe the
occluder-occludee relational modeling in more details.
3.3. Occluder-Occludee Relationship Modeling
For highly-overlapped apples, in typical Faster-
RCNN-based models, the generated region proposals
corresponding to the partially occluded ones may be
separated into disjoint subregions by the occluder. As
such, we employ the FES to obtain boundary features
from the occludee, where expansion in each direction
extends the potential proposals for the occludee. In
our implementation, we expand tsteps in k(k=8 in
this study) directions from the original RoI proposals,
and the expanded RoI proposals will contain additional
boundary features. The rationale is that irregular oc-
clusion boundaries unrelated to the occludee can cause
confusion to the network, which in turn provides es-
sential cues for decoupling occludees from occluders.
Therefore, we explicitly model occlusion patterns by
detecting bounding boxes of the occluders using the oc-
cluder detection branch, and since the occludee detec-
tion branch jointly predicts bounding boxes for the oc-
6k
Occluder Branch
Occludee Branch...+
bbox
class+
bbox
classBackboneRegion Proposal NetworkFeature ExpansionROI FeatureFeature MapConvFCNConvFCN
bbox
class
Figure 4: Network structure of the proposed Occluder-Occludee Relational Network (O2RNet). It consists of a feature learning backbone, RoI
feature extraction, and object detection heads with occluder and occludee branches. The Feature Expansion Structure (FES) provides expanded
RoI features along with features from the occluder branch to facilitate the detection of occludee.
cludee, the overlap between the two layers can be di-
rectly identiÔ¨Åed as occlusion boundary that can thus be
distinguished from the real object bounding boxes. In
order to reach this goal, the occluder modeling module
is designed as a simple 3 3 convolutional layer fol-
lowed by one FCN layer, the output of which is fed to
the up-sampling layer and one 1 1 convolutional layer
to obtain one channel feature map for occludee branch.
3.4. End-to-end Learning
As we have two separate detection heads in the oc-
cluder and the occludee branches, we deÔ¨Åne two loss
functions in the following way. For the occluder branch,
we adopt the loss function used in Faster R-CNN (Ren
et al., 2015), which deÔ¨Ånes a multi-task loss on each
sampled region of interest as
LOccluder =Lcls+Lbbox; (3)
where LclsandLbboxare, respectively, classiÔ¨Åcation loss
and bounding box loss deÔ¨Åned in Faster R-CNN Ren
et al. (2015).
The Ô¨Ånal loss Lis a weighted sum of the loss from
occluder branch and the loss from occludee branch de-
Ô¨Åned as:
L=1LOccluder +2LOccludee: (4)
Here LOccludee is the occludee branch loss that is the sum
of the kexpanded proposal losses, i.e.,
LOccludee =kX
i=0(Li
cls+Li
bbox): (5)Here1and2are two positive linear weights and
1+2=1, which are tuned to balance the
two loss functions. In our study, 1was tuned to
bef1:0;0:75;0:5;0:25;0gon various trials for cross-
validation.
3.5. Training and Inference
During the training process, we Ô¨Ålter out parts of
the non-occluded RoI proposals to keep occlusion cases
taking up 50% for balanced sampling. SGD with mo-
mentum is employed to train the model with 60 Kiter-
ations where it starts with 1 Kconstant warm-up itera-
tions. The batch size is set to 2 and the initial learning
rate is 0:01 with a weights decay of 0 :95. In our study,
ResNet-101-FPN is used as the backbone and the in-
put images are resized without changing the aspect ra-
tio, i.e., by keeping the shorter side and longer side of
no more than 1200 pixels. For inference, the occludee
branch predicts bounding boxes for the occluded tar-
get object in the high-score box proposals generated by
the RPN, while the occluder branch produces occlusion-
aware features as input for the occludee branch. The one
with the highest score is then chosen as the output.
4. Experiment and Discussions
4.1. Experimental Setup
In this section, we evaluate the e cacy of the pro-
posed O2RNet on the processed data as discussed in
Section 2.1. The network hyper-parameters, includ-
ing the momentum, learning rate, decay factor, training
7Table 1: Performance of O2RNet on the customized apple dataset. The step is from FES, which represents how much features expanded. The
evaluation uses AP, AR, and F1-score at the di erent IoUs.
Model Step AP AP 50 AP75 AR AR 50 AR75 F1-Score
O2RNett=1 0.511 0.945 0.935 0.351 0.938 0.803 0.864
t=2 0.490 0.920 0.900 0.330 0.900 0.770 0.820
t=3 0.490 0.920 0.904 0.328 0.900 0.770 0.820
Table 2: Model parameters numbers between the state-of-the-art
networks and our proposed Occluder-occludee Relational Network
(O2RNet). ‚ÄúM‚Äù stands for a million.
Models Parameters
FCOS 2.0M
YOLOv4 0.6M
Faster R-CNN (ResNet50) 2.0M
Faster R-CNN (ResNet101) 3.6M
EcientDet-b0 0.1M
EcientDet-b1 0.3M
EcientDet-b2 1.2M
EcientDet-b3 1.6M
EcientDet-b4 2.4M
EcientDet-b5 3.6M
CompNet via BBV 0.8M
CompNet via RPN 1.4M
O2RNet (ResNet50) 2.0M
O2RNet (ResNet101) 3.6M
steps, and batch size, are set as 0 :9, 0:001, 0:0005, 934,
and 1, respectively, through cross-validation. The input
image size is 1280 720, which is aligned with the reso-
lution of the camera used in our data collection. To bet-
ter analyze the training process, we set up 80 epochs for
training. We exploit a pre-trained model on the COCO
dataset (Lin et al., 2014), where we train on 2017train
(115kimages) and evaluate results on both 2017val and
2017test-dev to pre-train model parameters. This pre-
trained model generally only takes 50 epochs to con-
verge. By tuning the steps tin FES, di erent results
are obtained and listed in Table 1, which shows that
O2RNet with t=1 leads to the best performance.
4.2. Performance Comparison and Analysis
To accelerate the model training on our customized
dataset, we initialize parameters by transfer learning
from ImageNet (Deng et al., 2009). ImageNet pro-
vides large-scale images in di erent Ô¨Åelds (including
apples) and large-scale ground truth annotation. Dur-
ing the transfer learning process, our model learns spe-
ciÔ¨Åc characteristics with an e ective transfer of featuresfrom ImageNet. Compared to randomized parameters,
the results (see Fig. 5) shows that our model converges
faster as beneÔ¨Åted from the pretraining on a large-scale
database.
0 20 40 60 80
Epoch0.250.500.751.001.251.501.752.00LossTransfer learning
From scratch
Figure 5: Training loss comparison between transfer learning and
training from scratch on our model (O2RNet). The training loss with
transfer learning from ImageNet apparently decreases and converges
faster as compared with training from scratch.
Furthermore, data augmentation is another useful
technique to optimize detection performance without in-
creasing inference complexity. We applied Ô¨Åve augmen-
tation strategies, including geometric transformations
(GTs), color space transformations (CSTs), Gaussian
noise injection, mixup and sharpening data augmenta-
tion, to extend our dataset. The results are summarized
in Table 3. It shows that GTs such as rotation, Ô¨Çipping
and scaling ‚Äì by changing the pixel position of the im-
age and reordering apples in the image ‚Äì improve the ac-
curacy performance by around 1%. Through changing
color illumination and intensity of an image, CSTs also
roughly increases the performance by 1%. Due to the
sparsity of apples on some images, mixup helps enlarge
apple density on the image and enhances the accuracy
by 2%. It turns out that Gausian noise and sharpening
do not help much, as they try to change textures and in-
crease complexities on the dataset, which generate con-
fusing data and is not suitable for our model. Finally, the
8Table 3: Performance of O2RNet on the augmented dataset. The geometric transformations consist of rotation, Ô¨Çipping and scaling. The color
space transformations consist of brightness and contrast shifting. Finally, all of the augmentation methods are integrated to evaluate the O2RNet.
Augmentation AP AP 50 AP75 AR AR 50 AR75 F1-Score
Base 0.51 0.92 0.90 0.35 0.91 0.80 0.84
Geometric transformations (GTs) 0.52 0.93 0.91 0.35 0.91 0.80 0.85
Color space transformations (CSTs) 0.52 0.93 0.91 0.35 0.91 0.81 0.85
Gausian noise 0.48 0.91 0.90 0.34 0.91 0.80 0.83
Mixup 0.52 0.93 0.92 0.35 0.92 0.81 0.85
Sharpening 0.52 0.92 0.90 0.35 0.91 0.80 0.84
GTs+CSTs +Mixup 0.52 0.96 0.94 0.36 0.94 0.83 0.88
All 0.52 0.94 0.92 0.36 0.92 0.83 0.86
Table 4: Performance comparison of our own models and other 12 state-of-the-art deep learning models on the customized apple dataset.
Models AP AP 50 AP75 AR AR 50 AR75 F1-score
FCOS (Ahmad et al., 2021) 0.48 0.89 0.87 0.34 0.87 0.78 0.80
YOLOv4 (Pandey et al., 2021) 0.45 0.87 0.84 0.29 0.84 0.73 0.76
Faster R-CNNResNet50 (Norsworthy et al., 2012) 0.48 0.89 0.87 0.32 0.87 0.78 0.81
ResNet101 (Norsworthy et al., 2012) 0.49 0.94 0.93 0.31 0.84 0.75 0.82
EcientDetEcientDet-b0 (Oerke, 2006) 0.45 0.89 0.85 0.30 0.82 0.71 0.77
EcientDet-b1 (Oerke, 2006) 0.45 0.89 0.86 0.30 0.82 0.72 0.77
EcientDet-b2 (Oerke, 2006) 0.46 0.89 0.87 0.30 0.82 0.73 0.78
EcientDet-b3 (Oerke, 2006) 0.49 0.93 0.91 0.32 0.84 0.75 0.81
EcientDet-b4 (Oerke, 2006) 0.50 0.94 0.92 0.34 0.88 0.78 0.82
EcientDet-b5 (Oerke, 2006) 0.50 0.95 0.93 0.34 0.88 0.78 0.83
CompNetCompNet via BBV (Young et al., 2014) 0.50 0.94 0.92 0.36 0.94 0.80 0.85
CompNet via RPN (Fennimore and Cutulle, 2019) 0.51 0.95 0.94 0.35 0.94 0.80 0.86
O2RNetO2RNet-ResNet50 0.50 0.93 0.91 0.35 0.91 0.80 0.84
O2RNet-ResNet101 0.52 0.96 0.94 0.36 0.94 0.83 0.88
augmentation combination of GTs, CSTs and Mixup of-
fers the best enhancement by increasing the accuracy of
4% on our dataset.
To better evaluate the performance of our model, we
compare our O2RNet with the-state-of-art object detec-
tion methods on our customized apple dataset (see Ta-
ble 2 for a list of benchmark models and their number
of parameters). In particular, FCOS and YOLOv4 are
representatives of one-stage detectors, achieving con-
sistent improvement and demonstrating their e ective-
ness by outperforming the SSD method (Liu et al.,
2016) on several public datasets (Tian et al., 2019b;
Bochkovskiy et al., 2020). We also evaluate Faster
R-CNN and E cientDet since they are state-of-the-art
models with promising performance demonstrated in
fruit harvesting-related works (MekhalÔ¨Å et al., 2021;
Yan et al., 2021). We also compare O2RNet with
the state-of-the-art occlusion-aware network CompNet
(Fennimore and Cutulle, 2019).
We then use the same experimental setup to train each
model and evaluate them on the same apple test dataset.The results are shown in Table 4, which compares the
detection precision and recall over di erent IoUs among
the 14 selected models (including our O2RNet). No-
tably, in addition to FCOS, E cientDet-b5 and Faster
R-CNN achieved decent F1-scores of 0 :83 and 0:82,
respectively. Two occlusion-aware networks, Comp-
Net and our O2RNet clearly outperform all traditional
models with F1-scores of 0 :86 and 0:88, respectively,
and O2RNet clearly shows superior performance over
CompNet. Some representative inference results are
shown in Fig. 6. It can be seen that our O2RNet can ef-
fectively separate clustered apples and thereby improves
the precision and recall and subsequently the F1-score.
5. Conclusion
In this study, we collected a comprehensive apple
dataset under di erent lighting conditions and at vari-
ous occlusion levels from two real orchards. A novel
Occluder-Occludee Relational Network (O2RNet) was
developed to robustly detect clustered apples from the
9FCOSYOLOv4Faster R-CNNEfÔ¨ÅcientDet-b5CompNetO2RNetGround Truth
A.1A.2A.3A.4A.5A.6A
B.1B.2B.3B.4B.5B.6B
C.1C.2C.3C.4C.5C.6C
Figure 6: Results from six models on the various lighting conditions and occlusions
dataset. Our developed O2RNet signiÔ¨Åcantly reduced
false detection and improved the detection rate by em-
bedding relationships between the occluder and the oc-
cludee. State-of-art performance was demonstrated in
comprehensive experiments. We also found that transfer
learning and data augmentation techniques were useful
tools to enhance learning e ciency and model perfor-
mance.
Our future work will include the incorporation of fo-
liage information in the network design to further im-
prove the detection performance since the current work
only focuses on the clustered apples. Furthermore,
branch detection will be developed to provide necessary
contextual information for the robot to maneuver, e.g.,
avoiding collisions with tree branches. Lastly, we will
also investigate whether artiÔ¨Åcial lighting augmentation
can enhance the detection performance.
Acknowledgement
This research was funded by the USDA-ARS inhouse
project 5050-43640-003-000D. The authors would also
like to acknowledge Michigan State University‚Äôs Teach-
ing and Research Center in Holt, Michigan and Schawl-
lier‚Äôs Country Basket in Sparta, Michigan for their sup-
port for collecting the image data in the orchards. Theauthors thank Mr. Lingxuan Hao ans Wanqun Yang for
helping label the orchard images.
References
A. Ahmad, D. Saraswat, V . Aggarwal, A. Etienne, and B. Hancock.
Performance of deep learning models for classifying and detecting
common weeds in corn and soybean production systems. Comput-
ers and Electronics in Agriculture , 184:106081, 2021.
S. Bargoti and J. P. Underwood. Image segmentation for fruit de-
tection and yield estimation in apple orchards. Journal of Field
Robotics , 34(6):1039‚Äì1060, 2017.
M. Benady and G. E. Miles. Locating melons for robotic harvest-
ing using structured light. Paper-American Society of Agricultural
Engineers (USA) , 1992.
A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao. Yolov4: Op-
timal speed and accuracy of object detection. arXiv preprint
arXiv:2004.10934 , 2020.
D. Bulanon, T. Burks, and V . Alchanatis. Study on temporal variation
in citrus canopy using thermal imaging for citrus fruit detection.
Biosystems Engineering , 101(2):161‚Äì171, 2008.
D. Bulanon, T. Burks, and V . Alchanatis. Image fusion of visible and
thermal images for fruit detection. Biosystems engineering , 103
(1):12‚Äì22, 2009.
M. Cardenas-Weber, A. Hetzroni, and G. E. Miles. Machine vision
to locate melons and guide robotic harvesting. Paper-American
Society of Agricultural Engineers (USA) , 1991.
P. Chlap, H. Min, N. Vandenberg, J. Dowling, L. Holloway, and
A. Haworth. A review of medical image data augmentation tech-
niques for deep learning applications. Journal of Medical Imaging
and Radiation Oncology , 65(5):545‚Äì563, 2021.
10P. Chu, Z. Li, K. Lammers, R. Lu, and X. Liu. Deep learning-based
apple detection using a suppression mask r-cnn. Pattern Recogni-
tion Letters , 147:206‚Äì211, 2021.
Z. De-An, L. Jidong, J. Wei, Z. Ying, and C. Yu. Design and control
of an apple harvesting robot. Biosystems engineering , 110(2):112‚Äì
122, 2011.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet:
A large-scale hierarchical image database. In 2009 IEEE confer-
ence on computer vision and pattern recognition , pages 248‚Äì255.
Ieee, 2009.
L. Divyanth, D. Guru, P. Soni, R. Machavaram, M. Nadimi, and
J. Paliwal. Image-to-image translation-based data augmentation
for improving crop /weed classiÔ¨Åcation models for precision agri-
culture applications. Algorithms , 15(11):401, 2022.
A. Dutta and A. Zisserman. The VIA annotation software for
images, audio and video. In Proceedings of the 27th ACM
International Conference on Multimedia , MM ‚Äô19, New York,
NY , USA, 2019. ACM. ISBN 978-1-4503-6889-6 /19/10. doi:
10.1145 /3343031.3350535. URL https://doi.org/10.1145/
3343031.3350535 .
S. A. Fennimore and M. Cutulle. Robotic weeders can improve weed
control options for specialty crops. Pest management science , 75
(7):1767‚Äì1774, 2019.
K. Gallardo and P. Galinato. 2012 cost estimates of establishing, pro-
ducing, and packing red delicious apples in washington. FS099E,
Washington State University Extension Fact Sheet, 2012.
R. Girshick. Fast r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 1440‚Äì1448, 2015.
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierar-
chies for accurate object detection and semantic segmentation. In
Proceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 580‚Äì587, 2014.
M. W. Hannan and T. F. Burks. Current developments in automated
citrus harvesting. In 2004 ASAE annual meeting , page 1. American
Society of Agricultural and Biological Engineers, 2004.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 770‚Äì778, 2016.
K. He, G. Gkioxari, P. Doll ¬¥ar, and R. Girshick. Mask r-cnn. In Pro-
ceedings of the IEEE international conference on computer vision ,
pages 2961‚Äì2969, 2017.
H. Kang and C. Chen. Fruit detection and segmentation for apple
harvesting using visual sensor in orchards. Sensors , 19(20):4599,
2019.
H. Kang and C. Chen. Fast implementation of real-time fruit detection
in apple orchards using deep learning. Computers and Electronics
in Agriculture , 168:105108, 2020.
L. Ke, Y .-W. Tai, and C.-K. Tang. Deep occlusion-aware instance
segmentation with overlapping bilayers. In Proceedings of the
IEEE /CVF conference on computer vision and pattern recognition ,
pages 4019‚Äì4028, 2021.
A. Kortylewski, J. He, Q. Liu, and A. L. Yuille. Compositional con-
volutional neural networks: A deep architecture with innate ro-
bustness to partial occlusion. In Proceedings of the IEEE /CVF
Conference on Computer Vision and Pattern Recognition , pages
8940‚Äì8949, 2020.
P. Levi, A. Falla, and R. Pappalardo. Image controlled robotics applied
to citrus fruit harvesting. In 7th International Conference on Robot
Vision and Sensory Controls, Zurich (Switzerland), 2-4 Feb 1988 .
IFS Publications, 1988.
B. Li, A. Zhou, C. Yang, and S. Zheng. The design and realization
of fruit harvesting robot based on iot. In 2016 International Con-
ference on Computer Engineering, Information Science &Appli-
cation Technology (ICCIA 2016) , pages 158‚Äì161. Atlantis Press,
2016.T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ¬¥ar, and C. L. Zitnick. Microsoft coco: Common objects in
context. In European conference on computer vision , pages 740‚Äì
755. Springer, 2014.
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and
A. C. Berg. Ssd: Single shot multibox detector. In Computer
Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The
Netherlands, October 11‚Äì14, 2016, Proceedings, Part I 14 , pages
21‚Äì37. Springer, 2016.
J. Lu, N. Sang, Y . Hu, and H. Fu. Detecting citrus fruits with highlight
on tree based on fusion of multi-map. Optik , 125(8):1903‚Äì1907,
2014.
Y . Lu, D. Chen, E. Olaniyi, and Y . Huang. Generative adversarial net-
works (gans) for image augmentation in agriculture: A systematic
review. Computers and Electronics in Agriculture , 200:107208,
2022.
M. L. MekhalÔ¨Å, C. Nicol `o, Y . Bazi, M. M. Al Rahhal, N. A. Alsharif,
and E. Al Maghayreh. Contrasting yolov5, transformer, and e -
cientdet detectors for crop circle detection in desert. IEEE Geo-
science and Remote Sensing Letters , 19:1‚Äì5, 2021.
J. K. Norsworthy, S. M. Ward, D. R. Shaw, R. S. Llewellyn, R. L.
Nichols, T. M. Webster, K. W. Bradley, G. Frisvold, S. B. Powles,
N. R. Burgos, et al. Reducing the risks of herbicide resistance:
best management practices and recommendations. Weed science ,
60(SP1):31‚Äì62, 2012.
E.-C. Oerke. Crop losses to pests. The Journal of Agricultural Sci-
ence, 144(1):31‚Äì43, 2006.
P. Pandey, H. N. Dakshinamurthy, and S. N. Young. Autonomy in
detection, actuation, and planning for robotic weeding systems.
Transactions of the ASABE , page 0, 2021.
H. N. Patel, R. Jain, M. V . Joshi, et al. Fruit detection using improved
multiple features based algorithm. International journal of com-
puter applications , 13(2):1‚Äì5, 2011.
F. Qingchun, Z. Wengang, Q. Quan, J. Kai, and G. Rui. Study on
strawberry robotic harvesting system. In 2012 IEEE International
Conference on Computer Science and Automation Engineering
(CSAE) , volume 1, pages 320‚Äì324. IEEE, 2012.
W. Qiu and S. Shearer. Maturity assessment of broccoli using the
discrete fourier transform. Transactions of the ASAE , 35(6):2057‚Äì
2062, 1992.
M. Rahnemoonfar and C. Sheppard. Deep count: fruit counting based
on deep simulated learning. Sensors , 17(4):905, 2017.
J. Redmon and A. Farhadi. Yolov3: An incremental improvement.
arXiv preprint arXiv:1804.02767 , 2018.
S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
time object detection with region proposal networks. Advances in
neural information processing systems , 28, 2015.
H. RezatoÔ¨Åghi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and
S. Savarese. Generalized intersection over union: A metric and a
loss for bounding box regression. In Proceedings of the IEEE /CVF
conference on computer vision and pattern recognition , pages
658‚Äì666, 2019.
I. Sa, Z. Ge, F. Dayoub, B. Upcroft, T. Perez, and C. McCool. Deep-
fruits: A fruit detection system using deep neural networks. Sen-
sors, 16(8):1222, 2016.
C. Shorten and T. M. Khoshgoftaar. A survey on image data augmen-
tation for deep learning. Journal of Big Data , 6(60):1‚Äì48, 2019.
Y . Si, G. Liu, and J. Feng. Location of apples in trees using stereo-
scopic vision. Computers and Electronics in Agriculture , 112:68‚Äì
74, 2015.
P. W. Sites and M. J. Delwiche. Computer vision to locate fruit on a
tree. Transactions of the ASAE , 31(1):257‚Äì0265, 1988.
D. C. Slaughter and R. C. Harrell. Color vision in robotic fruit har-
vesting. Transactions of the ASAE , 30(4):1144‚Äì1148, 1987.
D. Stajnko, M. Lakota, and M. Ho Àácevar. Estimation of number and
11diameter of apple fruits in an orchard during the growing season
by thermal imaging. Computers and Electronics in Agriculture , 42
(1):31‚Äì42, 2004.
D. Su, H. Kong, Y . Qiao, and S. Sukkarieh. Data augmentation for
deep learning based semantic segmentation and crop-weed clas-
siÔ¨Åcation in agricultural robotics. Computers and Electronics in
Agriculture , 190:106418, 2021.
J. Sun, B. Lu, H. Mao, et al. Fruits recognition in complex background
using binocular stereovision. Journal of Jiangsu University-
Natural Science Edition , 32(4):423‚Äì427, 2011.
M. Tan, R. Pang, and Q. V . Le. E cientdet: Scalable and e cient ob-
ject detection. In Proceedings of the IEEE /CVF conference on com-
puter vision and pattern recognition , pages 10781‚Äì10790, 2020.
K. Tanigaki, T. Fujiura, A. Akase, and J. Imagawa. Cherry-harvesting
robot. Computers and electronics in agriculture , 63(1):65‚Äì72,
2008.
Y . Tian, G. Yang, Z. Wang, H. Wang, E. Li, and Z. Liang. Apple
detection during di erent growth stages in orchards using the im-
proved yolo-v3 model. Computers and electronics in agriculture ,
157:417‚Äì426, 2019a.
Z. Tian, C. Shen, H. Chen, and T. He. Fcos: Fully convolutional
one-stage object detection. In Proceedings of the IEEE /CVF inter-
national conference on computer vision , pages 9627‚Äì9636, 2019b.
N. Tsoulias, D. S. Paraforos, G. Xanthopoulos, and M. Zude-Sasse.
Apple shape detection based on geometric and radiometric features
using a lidar laser scanner. Remote Sensing , 12(15):2481, 2020.
S. Wan and S. Goudos. Faster r-cnn for multi-class fruit detection
using a robotic vision system. Computer Networks , 168:107036,
2020.
D. Whittaker, G. Miles, O. Mitchell, and L. Gaultney. Fruit location
in a partially occluded image. Transactions of the ASAE , 30(3):
591‚Äì0596, 1987.
Q. Wu, Y . Chen, and J. Meng. Dcgan-based data augmentation for
tomato leaf disease identiÔ¨Åcation. IEEE Access , 8:98716‚Äì98728,
2020.
R. Xiang, H. Jiang, and Y . Ying. Recognition of clustered tomatoes
based on binocular stereo vision. Computers and Electronics in
Agriculture , 106:75‚Äì90, 2014.
B. Yan, P. Fan, X. Lei, Z. Liu, and F. Yang. A real-time apple targets
detection method for picking robot based on improved yolov5. Re-
mote Sensing , 13(9):1619, 2021.
S. L. Young, G. E. Meyer, and W. E. Woldt. Future directions for auto-
mated weed management in precision agriculture. In Automation:
The future of weed control in cropping systems , pages 249‚Äì259.
Springer, 2014.
B. Zhang, W. Huang, C. Wang, L. Gong, C. Zhao, C. Liu, and
D. Huang. Computer vision recognition of stem and calyx in apples
using near-infrared linear-array structured light and 3d reconstruc-
tion. Biosystems Engineering , 139:25‚Äì34, 2015.
K. Zhang, K. Lammers, P. Chu, Z. Li, and R. Lu. System design and
control of an apple harvesting robot. Mechatronics , 79:102644,
2021. doi: https: //doi.org /10.1016 /j.mechatronics.2021.102644.
K. Zhang, K. Lammers, P. Chu, N. Dickinson, Z. Li, and R. Lu. Algo-
rithm design and integration for a robotic apple harvesting system.
InIEEE /RSJ International Conference on Intelligent Robots and
Systems , pages 9217‚Äì9224, 2022. doi: 10.1109 /IROS47612.2022.
9981417.
J. Zhao, J. Tow, and J. Katupitiya. On-tree fruit recognition using tex-
ture properties and color data. In 2005 IEEE /RSJ International
Conference on Intelligent Robots and Systems , pages 263‚Äì268.
IEEE, 2005.
Y . Zhao, L. Gong, Y . Huang, and C. Liu. A review of key techniques
of vision-based control for harvesting robot. Computers and Elec-
tronics in Agriculture , 127:311‚Äì323, 2016.
F. Zhuang, Z. Qi, K. Duan, D. Xi, Y . Zhu, H. Zhu, H. Xiong, andQ. He. A comprehensive survey on transfer learning. Proceedings
of the IEEE , 109(1):43‚Äì76, 2020.
12