CowStallNumbers: A Small Dataset for Stall Number Detection
of Cow Teats Images
Youshan Zhang
Yeshiva University, NYC, NY
youshan.zhang@yu.edu
Abstract
In this paper, we present a small cow stall number
dataset named CowStallNumbers, which is extracted from
cow teat videos with the goal of advancing cow stall num-
ber detection. This dataset contains 1042 training images
and 261 test images with the stall number ranging from 0
to 60. In addition, we ﬁne-tuned a ResNet34 model and
augmented the dataset with the random crop, center crop,
and random rotation. The experimental result achieves a
92% accuracy in stall number recognition and a 40.1% IoU
score in stall number position prediction.
1. Introduction
In recent years, the dairy industry has expanded rapidly
as the demand for milk and milk products grow. One of the
critical aspects of dairy farming is the management of the
herd’s health and productivity. To maximize milk produc-
tion and maintain the cows’ health, it is necessary to ensure
that the cows have a comfortable environment, appropri-
ate feed, and regular health checks. We previously studied
how to classify different categories of teats [25, 26]. How-
ever, these teat images were taken by phone and manually
cropped. To save time, we set up a camera toward the teat-
end areas [27]; we proposed to extract the key teat frames
from the video images, which leads to another problem that
we cannot recognize the cows’ ID unless we can recognize
the stall number in the videos. Therefore, we need to de-
velop a model to recognize the stall numbers.
In this paper, we ﬁne-tune the ResNet34 model for cow
stall detection. We evaluate our method using a small
dataset of cow images with ground truth annotations for cow
stall locations. Our results show that our method achieves
high accuracy in cow stall detection, and can provide a valu-
able tool for dairy farmers to optimize their operations. By
enabling accurate and efﬁcient cow stall detection, we can
improve the health and productivity of cows and, in turn,
increase the sustainability and proﬁtability of the dairy in-dustry. Our contributions are two-fold:
• We present a small cow stall number dataset named
CowStallNumber with the goal of advancing the ﬁeld
of cow stall number detection.
• We ﬁne-tuned the pre-trained ResNet34 model to get
the position of the stall number and recognize differ-
ent stall numbers. We achieved a 95% accuracy in stall
number recognition and a 40.1% IoU score in box po-
sition prediction.
2. Related Work
The faster Region-based Convolutional Neural Network
(R-CNN) algorithm has been applied for cow tail detec-
tion, achieving higher accuracy and faster detection times
than the R-CNN algorithm [11]. However, one limitation of
these object detection algorithms is that they struggle with
detecting objects in certain environmental conditions, such
as poor lighting or low contrast. This is a concern in cow de-
tection since cows blend in with their surroundings or have
their identifying features obscured by shadows or other vi-
sual obstructions.
To address this issue, thermal imaging cow detection has
been explored [2]. Thermal imaging uses infrared technol-
ogy to detect the heat signature of an object, which allows
for more accurate detection in low-light or low-contrast en-
vironments. In addition, thermal imaging can detect cows
even in situations where they are partially hidden, such as
behind bushes or in tall grass. Despite the advances in cow
detection technology, there are still challenges to be over-
come. For example, detecting cows in large herds can be
difﬁcult, as cows can tightly pack together and partially ob-
struct each other from view. Furthermore, different breeds
of cows may have unique physical characteristics that re-
quire speciﬁc detection algorithms. We will review the re-
lated work of object detection in the following paragraphs.
Girshick et al. proposed the R-CNN model, which ap-
plied a region proposal algorithm to generate candidate ob-
ject regions. Then, it is passed to a convolutional neuralarXiv:2303.10444v2  [cs.CV]  21 Mar 2023Figure 1. Timeline of You Only Look Once (YOLO) variants.
Figure 2. Example of three stall images (44, 13, and 0).
Figure 3. Example contents in the CSV ﬁle. ImageFilename is the cow stall number image. [Box position 1, Box position 2,
Box position 3, Box position 4] means that [x, y, height, width] of the bounding box location. Class name is the stall number class.
Empty space means we cannot detect the stall number in the image.
network (CNN) for classiﬁcation [8]. Later, Girshick pro-
posed a Fast R-CNN model, which is an improved version
of R-CNN that uses a shared feature map for region pro-
posal and classiﬁcation, resulting in faster and more accu-
rate detection [7]. Ren et al. developed a Faster R-CNN
model, which is another extension of R-CNN that intro-
duces a Region Proposal Network (RPN) to generate region
proposals more efﬁciently, making it the ﬁrst real-time ob-
ject detection method [22]. You Only Look Once (YOLO)
is a real-time object detection method that generates predic-
tions directly from the whole image, which is based on a
single CNN that simultaneously predicts the class and posi-
tion of each object. Redmon et al. [18] presented a Single
Shot MultiBox Detector (SSD) model, which generates ob-
ject proposals and predicts their class and location using a
single neural network. SSD is faster compared to Faster
R-CNN and achieves higher accuracy than YOLO. Lin et
al. proposed a RetinaNet [17], which introduces a novel fo-
cal loss function that down-weights easy examples and up-
weights hard examples. Mask R-CNN [9] is an extension
of Faster R-CNN, which adds a branch to predict segmenta-tion masks for each object, and it can segment the detected
objects in addition to their bounding boxes. Cascade R-
CNN [4] model is a variant of Faster R-CNN that utilizes a
cascade of detectors to improve the detection quality. Lin et
al. proposed a Feature Pyramid Network (FPN) model [16]
that generates a pyramid of feature maps that are used for
object detection at different scales and improves the perfor-
mance of object detectors. FPN. DetNet [15] incorporates
deformation-aware convolutional layers that can handle ob-
ject deformations and viewpoint changes. DenseBox [10]
applies dense regression to predict the coordinates of ob-
ject bounding boxes. CoupleNet [28] can predict the posi-
tion and size of objects in a coupled manner, leading to im-
proved performance compared to other real-time detection
methods. Kong et al. proposed a Reverse Connection with
Objectness Prior Networks (RON) model [13] that uses a re-
verse connection to capture context information for object
proposal generation. ReﬁneDet [24] is a multi-stage object
detection method that progressively reﬁnes the location and
size of object proposals. Edge Sampling Decoder (ESD) [5]
incorporates edge sampling into deep neural networks to de-tect edges and boundaries of objects. Neural Architecture
Search with Feature Pyramid Network (NAS-FPN) [6] uses
neural architecture search to design a feature pyramid net-
work that improves object detection performance.
There are also different variants of YOLO methods as
shown in Fig. 1. YOLOv1 [19] was introduced in 2016.
YOLOv2 (YOLO9000) [20] was released in 2017 and used
a Darknet-19 network architecture and incorporated batch
normalization and anchor boxes. It also introduced multi-
scale predictions, enabling the detection of objects at dif-
ferent scales. YOLOv3 [21] was released in 2018 and used
a Darknet-53 network architecture, introduced feature pyra-
mid networks (FPNs), and used residual connections. It also
used anchor boxes with dynamic scaling, leading to bet-
ter detection at different scales. YOLOv4 [3] was released
in 2020, which used a CSPDarknet-53 network architec-
ture, added various improvements such as spatial pyramid
pooling, and used a complex data augmentation pipeline.
YOLOv4 also included efﬁcient training approaches such
as mosaic data augmentation and focal loss. YOLOv5 [12]
was a PyTorch implementation rather than a fork from
the original Darknet, which has a CSP backbone and PA-
NET neck. The major improvement includes auto-learning
bounding box anchors. YOLOv6 [14] was released in 2022
and was a single-stage object detection framework dedi-
cated to industrial applications, with hardware-friendly ef-
ﬁcient design and high performance. YOLOv7 [23] was re-
leased in 2022 and reduced the number of parameters by
75%, requires 36% less computation, and achieves 1.5%
higher AP (average precision) compared with YOLOv4.
YOLOv7-X achieves a 21 FPS faster inference speed than
YOLOv5-X. The YOLOv8 [1] model is fast, accurate, and
easy to use, which can be applied to a wide range of object
detection and image segmentation tasks. It can be trained on
large datasets and run on various hardware platforms, from
CPUs to GPUs.
3. Data collection
The stall number images are retrieved from cow teat
videos, which are recorded to inspect the cow teats’ health
status. More details of video recording settings can be
found in [27]. We ﬁrst applied the unsupervised few-shot
key frame extraction (UFSKEF) model in [27] to extract the
coarse stall number key frames. We then manually checked
these key frames and removed the wrong key frame images.
Fig. 2 shows three example stall numbers, where 0 means
that we cannot detect the stall numbers. Tab. 1 lists the
statistics of our CowStallNumber dataset. Fig. 3 represents
the content of training and test CSV ﬁles.Table 1. Statistics of our CowStallNumber dataset
Training Test Total
Numbers 1042 261 1303
4. Methods
Since stall number detection is an object detection prob-
lem, we evaluate the performance of the stall number box
using the IoU score in Eq.(1) and report the recognition ac-
curacy of stall number using accuracy in Eq.(2).
IoU=jA\Bj
jA[Bj; (1)
whereAis the ground truth stall bounding box, and Bis the
predicted stall bounding box.
Accuracy =1
nnX
i=1(Yi==^Yi)100; (2)
whereYiis ground truth stall number, and ^Yi=f(Xi)is
the predicted stall number ( Xiis the input stall image and
fis our ﬁne-tune ResNet34 model).
We apply L1 loss to optimize the bounding box and
cross-entropy loss to optimize the stall number recognition.
The overall architecture of our model is shown in Fig. 4, and
the objective function is shown in the following equation.
L=1
nnX
i=1fjAi Bij1+Lce(f(Xi);Yi)g; (3)
wherejjis the L1 loss,Lceis the typical cross-entropy
loss andis the balance factor between L1 loss and cross-
entropy loss.
5. Implementation details
During the training of our model, we set batch size
= 64, training iteration I= 300 , and learning rate =
0.06,= 1=1000 with an Adam optimizer on a 48G
RTX A6000 GPU using PyTorch. The input image size
is270480. We also perform the random crop, cen-
ter crop, and random rotation to augment more images.
The dataset is available at https://github.com/
YoushanZhang/Cow_stall_number .
6. Results
Tab. 2 shows the results of our model. We can ﬁnd that
the stall number recognition accuracy is high, while the IoU
score is low. This implies that our model can successfully
recognize the stall numbers. However, the low IoU score
means that the bounding box of the stall number is not cor-
rectly predicted, which can lead to a low value during theFigure 4. The architecture of our stall number detection model. We ﬁne-tune a ResNet34 model and add two output layers (for stall number
box[88 ;268 ;108 ;292] and stall number 50, respectively).
Figure 5. Three example outputs of our model. True box: true bounding box location; Pred box: predicted bounding box location.
training. Another reason is that there are signiﬁcant differ-
ences between different stall images (e.g., light, location,
etc.). Fig. 5 shows three output images of our model. All
stall numbers are correctly predicted, while the stall number
positions differ slightly from the ground truth positions.
Table 2. Stall number detection results (% means 100)
Method IoU (%) Accuracy (%)
ResNet34 40.1 95.0
7. Conclusion
In this paper, we create a small cow stall number dataset
named CowStallNumbers with the goal of advancing cow
stall number detection. We ﬁne-tuned a ResNet34 model
and augmented the datasets with the random crop, cen-
ter crop, and random rotation. The experimental result
achieves a 92% accuracy in stall number recognition and
a 40.1% IoU score in stall number position prediction. For
future work, exploring a better object detection model (e.g.,
YOLOV8) could further improve the performance of stall
number detection.
References
[1] Yolov8 docs. https://docs.ultralytics.com/ .
Accessed: 2023-03-15. 3
[2] A Anagnostopoulos, M Barden, J Tulloch, K Williams,
B Grifﬁths, C Bedford, M Rudd, Androniki Psiﬁdi, G Banos,
and Georgios Oikonomou. A study on the use of ther-
mal imaging as a diagnostic tool for the detection of dig-ital dermatitis in dairy cattle. Journal of dairy science ,
104(9):10194–10202, 2021. 1
[3] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-
Yuan Mark Liao. Yolov4: Optimal speed and accuracy of
object detection. arXiv preprint arXiv:2004.10934 , 2020. 3
[4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-
ing into high quality object detection. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 6154–6162, 2018. 2
[5] Kean Chen, Jianguo Li, Weiyao Lin, John See, Ji Wang,
Lingyu Duan, Zhibo Chen, Changwei He, and Junni Zou.
Towards accurate one-stage object detection with ap-loss. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 5119–5127, 2019. 2
[6] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:
Learning scalable feature pyramid architecture for object de-
tection. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 7036–7045,
2019. 3
[7] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision , pages 1440–1448,
2015. 2
[8] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Region-based convolutional networks for accurate
object detection and segmentation. IEEE transactions on
pattern analysis and machine intelligence , 38(1):142–158,
2015. 2
[9] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2961–2969, 2017. 2[10] Lichao Huang, Yi Yang, Yafeng Deng, and Yinan Yu. Dense-
box: Unifying landmark localization with end to end object
detection. arXiv preprint arXiv:1509.04874 , 2015. 2
[11] Xiaoping Huang, Xinru Li, and Zelin Hu. Cow tail detec-
tion method for body condition score using faster r-cnn. In
2019 IEEE International Conference on Unmanned Systems
and Artiﬁcial Intelligence (ICUSAI) , pages 347–351. IEEE,
2019. 1
[12] Nelson Joseph and Solawetz Jacob. Yolov5 is here: State-
of-the-art object detection at 140 fps. https://blog.
roboflow.com/yolov5-is-here/ . Accessed: 2023-
03-15. 3
[13] Tao Kong, Fuchun Sun, Anbang Yao, Huaping Liu, Ming
Lu, and Yurong Chen. Ron: Reverse connection with object-
ness prior networks for object detection. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 5936–5944, 2017. 2
[14] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei
Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,
Weiqiang Nie, et al. Yolov6: A single-stage object detec-
tion framework for industrial applications. arXiv preprint
arXiv:2209.02976 , 2022. 3
[15] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong
Deng, and Jian Sun. Detnet: A backbone network for object
detection. arXiv preprint arXiv:1804.06215 , 2018. 2
[16] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyra-
mid networks for object detection. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2117–2125, 2017. 2
[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2980–2988, 2017. 2
[18] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In Computer
Vision–ECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11–14, 2016, Proceedings,
Part I 14 , pages 21–37. Springer, 2016. 2
[19] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Uniﬁed, real-time object de-
tection. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 779–788, 2016. 3
[20] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,
stronger. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 7263–7271, 2017. 3
[21] Joseph Redmon and Ali Farhadi. Yolov3: An incremental
improvement. arXiv preprint arXiv:1804.02767 , 2018. 3
[22] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information process-
ing systems , 28, 2015. 2[23] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-
Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets
new state-of-the-art for real-time object detectors. arXiv
preprint arXiv:2207.02696 , 2022. 3
[24] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and
Stan Z Li. Single-shot reﬁnement neural network for ob-
ject detection. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4203–4212,
2018. 2
[25] Youshan Zhang, Parminder S Basran, Ian R Porter, and
Matthias Wieland. Dairy cows teat-end condition classiﬁca-
tion using separable transductive learning. In 61st National
Mastitis Council (NMC) Annual Meeting , 2022. 1
[26] Youshan Zhang, Ian R Porter, Matthias Wieland, and Par-
minder S Basran. Separable conﬁdent transductive learning
for dairy cows teat-end condition classiﬁcation. Animals ,
12(7):886, 2022. 1
[27] Youshan Zhang, Matthias Wieland, and Parminder S Bas-
ran. Unsupervised few shot key frame extraction for cow
teat videos. Data , 7(5):68, 2022. 1, 3
[28] Yousong Zhu, Chaoyang Zhao, Jinqiao Wang, Xu Zhao,
Yi Wu, and Hanqing Lu. Couplenet: Coupling global struc-
ture with local parts for object detection. In Proceedings of
the IEEE international conference on computer vision , pages
4126–4134, 2017. 2