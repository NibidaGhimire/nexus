arXiv:2301.01076v1  [math.ST]  3 Jan 2023LEAST PRODUCT RELATIVE ERROR
ESTIMATION FOR FUNCTIONAL
MULTIPLICATIVE MODEL AND
OPTIMAL SUBSAMPLING
Qian Yan, Hanyu Li∗
Chongqing University
Abstract: In this paper, we study the functional linear multiplicativ e model based
on the least product relative error criterion. Under some re gularization condi-
tions, we establish the consistency and asymptotic normali ty of the estimator.
Further, we investigate the optimal subsampling for this mo del with massive
data. Both the consistency and the asymptotic distribution of the subsampling
estimator are ﬁrst derived. Then, we obtain the optimal subs ampling proba-
bilities based on the A-optimality criterion. Moreover, th e useful alternative
subsampling probabilities without computing the inverse o f the Hessian matrix
are also proposed, which are easier to implement in practise . Finally, numeri-
cal studies and real data analysis are done to evaluate the pe rformance of the
∗Corresponding author: Hanyu Li, College of Mathematics and Statis tics,
Chongqing University, Chongqing, 401331, P.R. China. E-mail: lihy.hy@ gmail.com or
hyli@cqu.edu.cn.proposed approaches.
Key words and phrases: Asymptotic normality, functional multiplicative model,
least product relative error, massive data, optimal subsam pling
1. Introduction
In the era of big data, data can be collected and recorded on a dens e sample
of observations in time and space. These observations are of a fun ctional
nature and typically take the form of curves and images. Functiona l data
analysis has been shown to perform wonderfully well with these data sets.
Functional regression models with scalar response have been exte nsively
studied and the most popular one is the functional linear model.
We consider a scalar-on-function linear multiplicative model
y= exp/parenleftbigg/integraldisplay1
0X(t)β(t)dt/parenrightbigg
ǫ, (1.1)
where the covariate X(t) and slope β(t) are smooth and square integrable
functions deﬁned on [0 ,1],yis the scalar response variable, and ǫis the
random error. Moreover, both yandǫare strictly positive. By taking the
logarithmic transformation, the model (1.1) becomes the regular f unctional
linear model. However, in comparison, the multiplicative model is more
useful andﬂexible tohandle positive responses such asincomes, st ockprices
and survival times.As we know, to estimate the slope, absolute errors are the most po p-
ular choices for designing loss functions, such as the least squares (LS)
and the least absolute deviation (LAD). However, in practical applic ations,
loss functions based on relative errors may be more eﬀective and su itable.
There are two types of relative errors, relative to the target valu eyand
relative to the prediction of y. Chen et al. (2010) summed the two rel-
ative errors and proposed the least absolute relative error (LARE ) crite-
rion for scalar linear multiplicative model. However, the LARE criterion
is non-smooth, which makes calculating it a little complicated. Later, b y
multiplying the two relative errors, Chen et al. (2016) improved it and pre-
sented the least product relative error (LPRE) criterion. The LPR E cri-
terion is inﬁnitely diﬀerentiable and strictly convex, resulting in a sim-
ple and unique estimator. Moreover, they also proved that the LPR E
estimation is more eﬀective than the LARE, LAD, and LS estimations
under some certain conditions. As a result, this criterion has also be en
widely used in other scalar multiplicative models (Chen and Liu (2021);
Chen, Liu and Ma (2022); Ming, Liu and Yang (2022)).
Forfunctionalmultiplicativemodels, tothebestofourknowledge, t here
are only a few works and all of them focus on the LARE criterion. For
example, Zhang, Zhang and Li (2016) extended the LARE criterion to thefunctionalmodelfortheﬁrst time. Theydeveloped thefunctiona lquadratic
multiplicative model and derived the asymptotic properties of the es tima-
tor. Later, Zhang et al. (2019) and Fan, Zhang and Wu (2022) con sidered
the variable selection for partially and locally sparse functional linear mul-
tiplicative models based on the LARE criterion, respectively. It seem s that
there is no study on the LPRE criterion conducted for functional d ata. To
ﬁll the gap, we propose the LPRE criterion for the functional linear mul-
tiplicative model, and derive the consistency and asymptotic normalit y of
the estimator.
Considering that traditional techniques areno longer usable forma ssive
data due to the limitation of computational resources, several re searchers
have devoted to developing eﬃcient or optimal subsampling strateg ies for
statisticalmodelswithmassivedata. Forexample, forlinearmodel, M a, Mahoney and Yu
(2015) studied the biases and variance of the algorithmic leveraging esti-
mator. Ma et al.(2020)furtherprovidedtheasymptoticdistribut ionsofthe
RandNLAsubsampling∗estimators. Forlogisticregression, Wang, Zhu and Ma
(2018) proposed an optimal subsampling method based on some opt imality
criteria (Atkinson, Donev and Tobias (2007)). Subsequently, Wan g (2019)
∗The probabilities of this kind sampling have close relationship with levera ge values,
which are typically used to devise randomized algorithms in numerical lin ear algebra.proposed a more eﬃcient estimation method and Poisson subsampling to
improve the estimation and computation eﬃciency. Later, Yao and W ang
(2019), Yu, Wang and Ai (2020) and Ai et al. (2021b) extended th e opti-
mal subsampling method to softmax regression, quasi-likelihood and gen-
eralized linear models, respectively. Furthermore, considering the eﬀect
of heavy-tailed errors or outliers in responses, some scholars hav e investi-
gated more robust models. For example, Wang and Ma (2021), Ai et al.
(2021a), Fan, Liu and Zhu (2021), and Shao, Song and Zhou (2022 ) em-
ployed the optimal subsampling method in ordinary quantile regressio n,
and Shao and Wang (2021) and Yuan et al. (2022) developed the sub sam-
plingforcompositequantileregression. Veryrecently, Ren, Zhao a nd Wang
(2022) considered the optimal subsampling strategy based on the LARE
criterion in linear multiplicative model. They derived the asymptotic dis-
tribution of the subsampling estimator and proved that LARE outpe rforms
LS and LAD under the optimal subsampling strategy. Wang and Zhan g
(2022) further extended the optimal subsampling to linear multiplica tive
model based on the LPRE criterion.
For functional regression models, now only little work has been done
in the area of subsampling (Liu, You and Cao (2021); He and Yan (202 2);
Yan, Li and Niu (2022)). Speciﬁcally, He and Yan (2022) proposed a func-tional principal subspace sampling probability for functional linear r egres-
sion with scalar response, which eliminates the impact of eigenvalue in-
side the functional principal subspace and properly weights the re siduals.
Liu, You and Cao (2021) and Yan, Li and Niu (2022) extended the op timal
subsampling method to functional generalized linear models and func tional
quantile regression with scalar response, respectively.
Inspired by the above works, we further study the optimal subsa mpling
for functional linear multiplicative model based on the LPRE criterion , and
ﬁrst establish the consistency and asymptotic normality of the sub sampling
estimator. Then, the optimal subsampling probabilities are obtained by
minimizing the asymptotic integrated mean squared error (IMSE) un der
the A-optimality criterion. In addition, a useful alternative minimizat ion
criterion is also proposed to further reduce the computational co st.
The rest of this paper is organized as follows. Section 2 introduces
the functional linear multiplicative model based on the LPRE criterion and
investigates the asymptotic properties of the estimator. In Sect ion 3, we
present the asymptotic properties of the subsampling estimator a nd the
optimal subsampling probabilities. The modiﬁed version of these prob abil-
ities is also considered in this section. Section 4 and Section 5 illustrate our
methodology through numerical simulations and real data, respec tively.2. LPRE estimation
2.1 Estimation
Suppose that {(xi(t),yi),i= 1,2,...,n}are samples from the model (1.1)
with the independent and identical distribution. The functional LPR E
estimator for the model (1.1), says ˆβ(t), is established by
arginf
βn/summationdisplay
i=1

/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleyi−exp/parenleftBig/integraltext1
0xi(t)β(t)dt/parenrightBig
yi/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle×/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleyi−exp/parenleftBig/integraltext1
0xi(t)β(t)dt/parenrightBig
exp/parenleftBig/integraltext1
0xi(t)β(t)dt/parenrightBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle

,
which is equivalent to
arginf
βn/summationdisplay
i=1/braceleftbigg
yiexp/parenleftbigg
−/integraldisplay1
0xi(t)β(t)dt/parenrightbigg
+y−1
iexp/parenleftbigg/integraldisplay1
0xi(t)β(t)dt/parenrightbigg
−2/bracerightbigg
.
We aim to estimate the slope function β(t) via a penalized spline
method. Deﬁne Kequispaced interior knots as 0 = t0< t1< ... <
tK< tK+1= 1. Let B(t) = (B1(t),B2(t),...,B K+p+1(t))Tbe the set
of the normalized B-spline basis functions of degree pon each sub-interval
[tj,tj+1],j= 0,1,...,Kandp−1 times continuously diﬀerentiable on [0 ,1].
The details of the B-spline functions can be found in de Boor (2001). Our
functional LPRE estimator ˆβ(t) ofβ(t) is thus deﬁned as
ˆβ(t) =K+p+1/summationdisplay
j=1ˆθjBj(t) =BT(t)ˆθfull,2.1 Estimation
whereˆθfullminimizes the penalized functional LPRE loss function
L(θ;λ,K)
=n/summationdisplay
i=1/braceleftbigg
yiexp/parenleftbigg
−/integraldisplay1
0xi(t)BT(t)θdt/parenrightbigg
+y−1
iexp/parenleftbigg/integraldisplay1
0xi(t)BT(t)θdt/parenrightbigg
−2/bracerightbigg
+λ
2/integraldisplay1
0/braceleftbigg/parenleftBig
B(q)(t)/parenrightBigT
θ/bracerightbigg2
dt, (2.2)
whereλ >0 is the smoothing parameter, and B(q)(t) is the square in-
tegrated q-th order derivative of all the B-splines functions for some in-
tegerq≤p. For convenience, we let Bi=/integraltext1
0xi(t)B(t)dtandDq=
/integraltext1
0B(q)(t){B(q)(t)}Tdt, the loss function (2.2) thus can be rewritten as
L(θ;λ,K) =n/summationdisplay
i=1/braceleftbig
ωi(θ)+ωi(θ)−1−2/bracerightbig
+λ
2θTDqθ, (2.3)
whereωi(θ) =yiexp/parenleftbig
−BT
iθ/parenrightbig
. Of note, the model (2.3) is inﬁnitely dif-
ferentiable and strictly convex. The Newton-Raphson method will b e used
since there is no general closed-form solution to the functional LP RE esti-
mator. That is, the estimator ˆθfullcan be obtained by iteratively applying
the following formula until ˆθt+1converges.
ˆθt+1=ˆθt−/braceleftBiggn/summationdisplay
i=1/parenleftBig
ωi(ˆθt)+ωi(ˆθt)−1/parenrightBig
BiBT
i+λDq/bracerightBigg−1
×/braceleftBiggn/summationdisplay
i=1/parenleftBig
−ωi(ˆθt)+ωi(ˆθt)−1/parenrightBig
Bi+λDqˆθt/bracerightBigg
.
Note that the computational complexity for calculating ˆθfullis about
O(ζn(K+p+1)2), where ζis the number of iterations until convergence.2.2 Theoretical properties of ˆβ(t)
As we can see, the computational cost is expensive when the full da ta size
nis very large. To deal with this issue, we will propose a subsampling
algorithm to reduce computational cost in Section 3.
2.2 Theoretical properties of ˆβ(t)
We will show the consistency and asymptotic normality of ˆβ(t). For sim-
plicity, the following notations are given ﬁrstly. For the function f(t)
belonging to Banach space, /bar⌈blf/bar⌈blm= (/integraltext1
0|f(t)|mdt)1/mfor 0< m < ∞.
For the matrix A= (aij),/bar⌈blA/bar⌈bl∞= max ij|aij|. In addition, deﬁne H=
E{BBT(ǫ+ǫ−1)}+λ/nDq,G= E{BBT(ǫ−ǫ−1)2}, and
ˆG=1
nn/summationdisplay
i=1/braceleftBig
−ωi(ˆθfull)+ωi(ˆθfull)−1/bracerightBig2
BiBT
i,
ˆH=1
nn/summationdisplay
i=1/braceleftBig
ωi(ˆθfull)+ωi(ˆθfull)−1/bracerightBig
BiBT
i+λ/nDq.(2.4)
Furthermore, we assume the following regularization conditions hold .
(H.1): For the functional covariate X(t), assume that E( /bar⌈blX/bar⌈bl8
8)<∞.
(H.2): Assumetheunknownfunctionalcoeﬃcient β(t)issuﬃcientlysmooth.
That is,β(t) has ad′-th derivative β(d′)(t) such that
|β(d′)(t)−β(d′)(s)|≤C2|t−s|v, t,s∈[0,1],
where the constant C2>0 andv∈[0,1]. In what follows, we set
d=d′+v≥p+1.2.2 Theoretical properties of ˆβ(t)
(H.3): E{(ǫ−ǫ−1)|X}= 0.
(H.4): E{(ǫ+ǫ−1)6|X}<∞.
(H.5): Assume the smoothing parameter λsatisﬁesλ=o(n1/2K1/2−2q)
withq≤p.
(H.6): Assume the number of knots K=o(n1/2) andK/n1/(2d+1)→ ∞as
n→ ∞.
Remark 1. Assumptions (H.1) and (H.2) are quite usual in the functional
setting(seee.g.,Cardot, Ferraty and Sarda(2003);Claeskens, Krivobokova and Opsomer
(2009)). Assumption (H.3) is an identiﬁability condition for the LPRE e s-
timation of β(t). Assumptions (H.3) and (H.4) ensure the consistency and
asymptoticnormality oftheLPREestimator. Assumptions (H.5)and (H.6)
are mainly used to obtain the asymptotic unbiasedness of the LPRE e sti-
mator.
Now, we present the consistency and asymptotic normality of ˆβ(t).
Theorem 1. Under Assumptions (H.1)–(H.6), for t∈[0,1], asn→ ∞,
we have
(1): (Consistency) There exists a LPRE estimator ˆβ(t)such that
/bar⌈blˆβ−β/bar⌈bl2=OP(n−1/2K1/2);(2): (Asymptotic normality)
{B(t)TVfullB(t)}−1/2/radicalbig
n/K(ˆβ(t)−β(t))→N(0,1)
in distribution, where Vfull=K−1H−1GH−1, which is consistently
estimated by K−1ˆH−1ˆGˆH−1deﬁned in (2.4).
3. Optimal subsampling
3.1 Subsampling estimator and its theoretical properties
We ﬁrst introduce a general random subsampling algorithm for the f unc-
tional linear multiplicative model, in which the subsamples are taken at
random with replacement based on some sampling distributions.
1.Sampling. Given a larger K, we generate Bi=/integraltext1
0xi(t)B(t)dtand
the new data is {(Bi,yi),i= 1,2,...,n}. Assign the subsampling
probabilities {πi}n
i=1to all data points and draw a random subsample
of sizer(≪n) with replacement based on {πi}n
i=1from the new data.
Denote the subsample as {(Bi,yi,Ri),i= 1,2,...,n}, whereRide-
notes the total number of times that the i-th data point is selected
from the full data and/summationtextn
i=1Ri=r.
2.Estimation. Givenλ, minimize the following loss function to get the3.1 Subsampling estimator and its theoretical properties
estimate ˜θbased on the subsample,
L∗(θ;λ,K) =1
rn/summationdisplay
i=1Ri
πi/parenleftBig
wi(˜θt)+wi(˜θt)−1−2/parenrightBig
+λ
2θTDqθ.(3.5)
Due to the convexity of L∗(θ;λ,K), the Newton-Raphson method is
adopted until ˜θt+1and˜θtare close enough,
˜θt+1=˜θt−/braceleftBiggn/summationdisplay
i=1Ri
πi/parenleftBig
ωi(˜θt)+ωi(˜θt)−1/parenrightBig
BiBT
i+λDq/bracerightBigg−1
×/braceleftBiggn/summationdisplay
i=1Ri
πi/parenleftBig
−ωi(˜θt)+ωi(˜θt)−1/parenrightBig
Bi+λDq˜θt/bracerightBigg
.(3.6)
Finally, we can get the subsample estimator ˜β(t) =BT(t)˜θ.
Theloss function (3.5) isguaranteed to beunbiased in cases when we use an
inverse probability weighted technique since the subsampling probab ilities
πimay depend on the full data Fn={(xi(t),yi),i= 1,2,...,n,t ∈[0,1]}.
Below we establish the consistency and asymptotic normality of ˜β(t) to-
wardsˆβ(t). An extra condition is needed.
(H.7): Assume that max 1≤i≤nr(nπi)−1=OP(1) andr=o(K2).
Remark 2. Assumption (H.7) is often used in inverse probability weighted
algorithms to restrict the weights such that the loss function is not exces-
sively inﬂated by data points with extremely small subsampling probab ili-
ties (Ai et al. (2021b); Liu, You and Cao (2021); Yan, Li and Niu (20 22)).3.2 Optimal subsampling probabilities
Theorem 2. Under Assumptions (H.1)–(H.7), for t∈[0,1], asr,n→ ∞,
conditionally on Fnin probability, we have
(1): (Consistency) There exists a subsampling estimator ˜β(t)such that
/bar⌈bl˜β−ˆβ/bar⌈bl2=OP|Fn(r−1/2K1/2);
(2): (Asymptotic normality)
/braceleftbig
B(t)TV B(t)/bracerightbig−1/2/radicalbig
r/K(˜β(t)−ˆβ(t))→N(0,1)
in distribution, where
V=1
KˆH−1VπˆH−1,
Vπ=1
n2n/summationdisplay
i=11
πi/braceleftBig
−ωi(ˆθfull)+ωi(ˆθfull)−1/bracerightBig2
BiBT
i.(3.7)
3.2 Optimal subsampling probabilities
To better approximate ˆβ(t), it is important to choose the proper subsam-
pling probabilities. A commonly used criterion is to minimize the asymp-
totic IMSE of ˜β(t). By Theorem 2, we have the asymptotic IMSE of ˜β(t)
as follows
IMSE(˜β(t)−ˆβ(t)) =K
r/integraldisplay1
0BT(t)V B(t)dt. (3.8)
Note that Vdeﬁned in (3.7) is the asymptotic variance-covariance ma-
trix of/radicalbig
r/K(˜θ−ˆθfull) and the integral inequality/integraltext1
0BT(t)V B(t)dt≤3.2 Optimal subsampling probabilities
/integraltext1
0BT(t)V′B(t)dtholds if and only if V≤V′holds in the L¨ owner-
ordering sense. Thus, we focus on minimizing the asymptotic MSE of ˜θ
and choose the subsampling probabilities such that tr( V) is minimized.
This is called the A-optimality criterion in optimal experimental designs ;
see e.g., Atkinson, Donev and Tobias (2007). Using this criterion, we are
able to derive the optimal subsampling probabilities provided in the follo w-
ing theorem.
Theorem3 (A-optimality) .Ifthe subsamplingprobabilities πi,i= 1,2,...,n,
are chosen as
πFAopt
i=|−yiexp(−BT
iˆθfull)+y−1
iexp(BT
iˆθfull)|/bar⌈blˆH−1Bi/bar⌈bl2/summationtextn
i=1|−yiexp(−BT
iˆθfull)+y−1
iexp(BT
iˆθfull)|/bar⌈blˆH−1Bi/bar⌈bl2,(3.9)
then the total asymptotic MSE of/radicalbig
r/K(˜θ−ˆθfull),tr(V), attains its min-
imum, and so does the asymptotic IMSE of ˜β(t).
However, from (2.4), we have that ˆHrequires the chosen of smoothing
parameter λ, andthecalculationof /bar⌈blˆH−1Bi/bar⌈bl2costsO(n(K+p+1)2), which
is expensive. These weaknesses make these optimal subsampling pr obabil-
ities not suitable for practical use. So, it is necessary to ﬁnd altern ative
probabilities without ˆHto reduce the computational complexity.
Note that, as observed in (3.7), only Vπinvolvesπiin the asymptotic
variance-covariance matrix V. Thus, from the L¨ owner-ordering, we can3.2 Optimal subsampling probabilities
only focus on Vπand minimize its trace, which can be interpreted as min-
imizing the asymptotic MSE of/radicalbig
r/KˆH(˜θ−ˆθfull) due to its asymptotic
unbiasedness. This is called the L-optimality criterion in optimal exper i-
mental designs (Atkinson, Donev and Tobias (2007)). Therefore , to reduce
thecomputing time, weconsider themodiﬁedoptimalcriterion: minimiz ing
tr(Vπ).
Theorem4 (L-optimality) .Ifthe subsamplingprobabilities πi,i= 1,2,...,n,
are chosen as
πFLopt
i=|−yiexp(−BT
iˆθfull)+y−1
iexp(BT
iˆθfull)|/bar⌈blBi/bar⌈bl2/summationtextn
i=1|−yiexp(−BT
iˆθfull)+y−1
iexp(BT
iˆθfull)|/bar⌈blBi/bar⌈bl2,(3.10)
thentr(Vπ)attains its minimum.
From (3.10), it is seen that the functional L-optimal subsampling pr ob-
abilitiesπFLopt
irequiresO(n(K+p+1)) ﬂops to compute, which is much
cheaper than computing πFAopt
iasKincreases.
Consider that the subsampling probabilities (3.10) depend on ˆθfull,
which is the full data estimation to be estimated, so an exact probab ility
distribution is not applicable directly. Next, we consider an approxima te
one and propose a two-step algorithm.
1.Step 1: Draw a small subsample of size r0to obtain a pilot esti-
mator˜θpilotby running the general subsampling algorithm with the3.3 Tuning parameters selection
uniform sampling probabilities π0
i= 1/nandλ= 0. Replace ˆθfull
with˜θpilotin (3.10) to derive the approximation of the optimal sub-
sampling probabilities.
2.Step 2: Drawa subsample ofsize rby using theapproximate optimal
probabilities from Step 1. Given λ, obtain the estimate ˘θ(λ) with
the subsample by using (3.6), and the λis determined by minimizing
BIC(λ) discussed below based on the corresponding subsample. Once
the optimal λis determined, we can get the estimator ˘β(t) =BT(t)˘θ.
3.3 Tuning parameters selection
For the degree pand the order of derivation q, we empirically choose B-
splines of degree 3 and a second-order penalty. The number of kno tsKis
not a crucial parameter because smoothing is controlled by the rou ghness
penalty parameter λ(see e.g., Ruppert (2002); Cardot, Ferraty and Sarda
(2003)). For the parameter λ, we choose the BIC criterion to determine it:
BIC(λ) = log(RSS)+log(n)
ndf,
where RSS = 1 /n/summationtextn
i=1{ωi(ˆθfull) +ωi(ˆθfull)−1−2}, and df denotes the
eﬀective degrees of freedom, i.e., the number of non-zero parame ter esti-
mates. However, using full data to select the optimal λis computationally
expensive, we approximate it by BIC under the optimal subsample da ta.4. Simulation studies
In this section, we aim to study the ﬁnite sample performance of the pro-
posed methods by using synthetic data.
4.1 LPRE performance
Inthisexperiment, weshall comparetheperformanceofthefunc tional least
square (FLS), functional least absolute deviation (FLAD) and fun ctional
least product relative error (FLPRE). The FLS and FLAD estimates are
deﬁned as minimizing/summationtextn
i=1[log(yi)−/integraltext1
0xi(t)β(t)dt]2and/summationtextn
i=1|log(yi)−
/integraltext1
0xi(t)β(t)dt|, respectively. The functional covariates in the model (1.1)
are identically and independently generated as: xi(t) =/summationtextaijBj(t),i=
1,2,...,n, whereBj(t) are cubic B-spline basis functions that are sampled
at 100 equally spaced points between 0 and 1. We consider the followin g
two diﬀerent distributions for the basis coeﬃcient A= (aij):
•C1. Multivariate normal distribution N(0,Σ), where Σij= 0.5|i−j|;
•C2. Multivariate tdistributionwith5degreesoffreedom, t5(0,Σ/10).
The slope function β(t) = 7t3+2sin(4πt+0.2) and the random errors, ǫi,
are generated in four cases:
•R1. log(ǫ)∼N(0,1);4.1 LPRE performance
•R2. log(ǫ)∼U(−2,2);
•R3.ǫhasthedistributionwiththedensityfunction f(x) =cexp(−x−
x−1−log(x)+2)I(x >0) andcis a normalization constant;
•R4.ǫ∼U(0.5,b) withbbeing chosen such that E( ǫ) = E(1/ǫ).
In the speciﬁc simulation, we ﬁrst take n= 100,500,1000 for train-
ing, and then n= 300,1500,3000 for testing, and let the number of knots
K= 10. Based on 500 replications, we use the root IMSE to evaluate th e
qualities of estimates and assess the performances of prediction o n test data
by the root predicted square error (RPSE), respectively. They a re deﬁned
as follows:
IMSE =1
500500/summationdisplay
k=1/bracketleftbigg/integraldisplay1
0/braceleftBig
ˆβ(k)(t)−β(t)/bracerightBig2
dt/bracketrightbigg1/2
,
and
RPSE =1
500500/summationdisplay
k=1/bracketleftBigg
1
nn/summationdisplay
i=1/parenleftbigg/integraldisplay1
0xi(t)β(t)dt−/integraldisplay1
0xi(t)ˆβ(k)(t)dt/parenrightbigg2/bracketrightBigg1/2
,
where˜β(k)(t) is the estimator from the k-th run.
The simulation results are presented in Tables 1 and 2, which show
that FLPRE performs considerably better than FLS and LAD in all ca ses
except the one C2-R1. In case C2-R1, FLPRE always outperforms FLAD,
while the gap between LPRE and FLS gradually decreases as the samp le4.2 Subsampling performance
size increases, and LPRE slightly outperforms FLS when the sample s ize
reaches 1000. In addition, the IMSE and RPSE of all estimators dec rease
as the sample size is increasing, which implies that the performance of all
estimators becomes better when the sample size enlarges.
4.2 Subsampling performance
In this experiment, we ﬁrst take n= 105for training, and then m= 1000
for testing to compare the performance of the functional L-opt imal sub-
sampling (FLopt) method with the uniform subsampling (Unif) method .
The simulated data distributions are the same as those in Subsection 4.1,
to which we add a case about the basis coeﬃcient.
•C3. A mixture of two multivariate normal distributions 0 .5N(1,Σ)+
0.5N(−1,Σ).
Inaddition, fromAssumption(H.6), weletthenumberofknots K=⌈n1/4⌉.
For fair comparison, we use the same basis functions and the same
smoothing parameters inall cases asthose forfull data. The root IMSE and
RPSE of the subsampling estimators corresponding to various subs ampling
sizes of 2000,5000,8000,10000,15000 with r0= 1000 are computed, where4.2 Subsampling performance
Table 1: IMSE of each estimator.
Dist Method R1 R2 R3 R4
n= 100/300 C1 FLPRE 1.4958 1.4588 1.2258 1.0766
FLS 1.5023 1.6294 1.3061 1.1707
FLAD 1.6797 2.0704 1.4343 1.2370
C2 FLPRE 2.7053 2.5396 1.9177 1.4743
FLS 2.4989 2.7184 1.9261 1.5298
FLAD 2.8300 3.8908 2.2250 1.7391
n= 500/1500 C1 FLPRE 0.7550 0.7023 0.6086 0.5447
FLS 0.8890 0.9575 0.7952 0.7321
FLAD 1.1018 1.3702 0.9422 0.7867
C2 FLPRE 1.5923 1.4627 1.1942 1.0242
FLS 1.5522 1.6664 1.2809 1.1278
FLAD 1.7679 2.2696 1.4302 1.2366
n= 1000/3000 C1 FLPRE 0.5508 0.4930 0.3879 0.3255
FLS 0.6213 0.6600 0.5286 0.4834
FLAD 0.8532 1.0904 0.6803 0.5346
C2 FLPRE 1.2123 1.0902 0.9203 0.7943
FLS 1.2552 1.3307 1.0612 0.9450
FLAD 1.4475 1.8175 1.2096 1.03334.2 Subsampling performance
Table 2: RPSE of each prediction.
Dist Method R1 R2 R3 R4
n= 100/300 C1 FLPRE 0.2488 0.2416 0.2022 0.1770
FLS 0.2516 0.2722 0.2170 0.1940
FLAD 0.2828 0.3486 0.2394 0.2054
C2 FLPRE 0.1869 0.1749 0.1323 0.1015
FLS 0.1731 0.1882 0.1331 0.1053
FLAD 0.1967 0.2696 0.1536 0.1201
n= 500/1500 C1 FLPRE 0.1240 0.1151 0.0983 0.0872
FLS 0.1455 0.1569 0.1288 0.1181
FLAD 0.1814 0.2276 0.1537 0.1274
C2 FLPRE 0.1078 0.0989 0.0807 0.0686
FLS 0.1055 0.1135 0.0872 0.0761
FLAD 0.1208 0.1559 0.0978 0.0837
n= 1000/3000 C1 FLPRE 0.0902 0.0805 0.0625 0.0514
FLS 0.1009 0.1083 0.0848 0.0767
FLAD 0.1390 0.1792 0.1098 0.0855
C2 FLPRE 0.0819 0.0734 0.0613 0.0525
FLS 0.0847 0.0899 0.0710 0.0630
FLAD 0.0985 0.1243 0.0815 0.06914.2 Subsampling performance
the deﬁnitions of root IMSE and RPSE are as follows
IMSE =1
10001000/summationdisplay
k=1/bracketleftbigg/integraldisplay1
0/braceleftBig
˜β(k)(t)−ˆβ(t)/bracerightBig2
dt/bracketrightbigg1/2
,
RPSE =1
10001000/summationdisplay
k=1/bracketleftBigg
1
mm/summationdisplay
i=1/parenleftbigg/integraldisplay1
0xi(t)˜β(k)(t)dt−/integraldisplay1
0xi(t)ˆβ(t)dt/parenrightbigg2/bracketrightBigg1/2
.
(4.11)
Based on 1000 replications, all results are shown in Figures 1 and 2 by
logarithmic transformation.
From Figure 1, it is clear to see that the FLopt subsampling method
always has smaller IMSE compared with the Unif subsampling method fo r
all cases, which is in agreement with the theoretical results. That is , the
formercanminimize theasymptotic IMSEofthesubsampling estimato r. In
particular, the FLopt method performs much better when the err ors obey
case R1. From Figure 2, same as the case on IMSE, we can see that t he
RPSE of the FLopt subsampling estimator is always better than that of the
Unif subsampling estimator for all cases. Furthermore, Figures 1 a nd 2 also
show that the FLopt method depends on both types of random err ors and
covariates, andtheeﬀectoferrorsisgreaterthanthatofcova riates. Besides,
as expected, the estimation and prediction eﬃciency of the subsam pling
estimators is getting better as the subsample size increases.
To evaluate the computational eﬃciency of the subsampling method s,4.2 Subsampling performance
−0.8−0.6−0.4−0.2
4000 8000 12000
rIMSEMethod
FLopt
UnifC1−R1
−0.5−0.4−0.3−0.2−0.10.00.1
4000 8000 12000
rIMSEMethod
FLopt
UnifC2−R1
−0.8−0.6−0.4−0.2
4000 8000 12000
rIMSEMethod
FLopt
UnifC3−R1
−0.8−0.7−0.6−0.5−0.4−0.3
4000 8000 12000
rIMSEMethod
FLopt
UnifC1−R2
−0.4−0.3−0.2−0.10.0
4000 8000 12000
rIMSEMethod
FLopt
UnifC2−R2
−0.9−0.8−0.7−0.6−0.5
4000 8000 12000
rIMSEMethod
FLopt
UnifC3−R2
−0.9−0.8−0.7−0.6−0.5
4000 8000 12000
rIMSEMethod
FLopt
UnifC1−R3
−0.6−0.5−0.4−0.3−0.2−0.1
4000 8000 12000
rIMSEMethod
FLopt
UnifC2−R3
−0.9−0.8−0.7−0.6−0.5
4000 8000 12000
rIMSEMethod
FLopt
UnifC3−R3
−1.1−1.0−0.9−0.8
4000 8000 12000
rIMSEMethod
FLopt
UnifC1−R4
−0.8−0.7−0.6−0.5−0.4
4000 8000 12000
rIMSEMethod
FLopt
UnifC2−R4
−1.4−1.3−1.2−1.1−1.0
4000 8000 12000
rIMSEMethod
FLopt
UnifC3−R4
Figure 1: IMSE for diﬀerent subsampling sizes rand ﬁxed ﬁrst step sub-
sampling size r0= 1000 with diﬀerent distributions when n= 105.4.2 Subsampling performance
−1.6−1.4−1.2
4000 8000 12000
rRPSEMethod
FLopt
UnifC1−R1
−1.6−1.4−1.2
4000 8000 12000
rRPSEMethod
FLopt
UnifC2−R1
−1.6−1.4−1.2
4000 8000 12000
rRPSEMethod
FLopt
UnifC3−R1
−1.6−1.5−1.4−1.3−1.2−1.1
4000 8000 12000
rRPSEMethod
FLopt
UnifC1−R2
−1.6−1.5−1.4−1.3−1.2−1.1
4000 8000 12000
rRPSEMethod
FLopt
UnifC2−R2
−1.7−1.6−1.5−1.4−1.3
4000 8000 12000
rRPSEMethod
FLopt
UnifC3−R2
−1.7−1.6−1.5−1.4−1.3
4000 8000 12000
rRPSEMethod
FLopt
UnifC1−R3
−1.7−1.6−1.5−1.4−1.3
4000 8000 12000
rRPSEMethod
FLopt
UnifC2−R3
−1.7−1.6−1.5−1.4−1.3
4000 8000 12000
rRPSEMethod
FLopt
UnifC3−R3
−1.9−1.8−1.7−1.6
4000 8000 12000
rRPSEMethod
FLopt
UnifC1−R4
−2.0−1.9−1.8−1.7−1.6
4000 8000 12000
rRPSEMethod
FLopt
UnifC2−R4
−2.2−2.1−2.0−1.9−1.8
4000 8000 12000
rRPSEMethod
FLopt
UnifC3−R4
Figure 2: RPSE for diﬀerent subsampling sizes rand ﬁxed ﬁrst step sub-
sampling size r0= 1000 with diﬀerent distributions when n= 105.4.2 Subsampling performance
Table 3: CPU seconds for diﬀerent subsampling sizes rwithn= 106,
r0= 200,K= 50 and a ﬁxed λ. The times are the mean times calculated
from 100 implementations of each method.
Methodr
1000 2000 3000 4000 5000
FLopt 0.6036 0.6101 0.6230 0.6298 0.6393
Unif 0.0142 0.0250 0.0355 0.0455 0.0554
Full data CPU seconds: 11.8155
we record the computing time of each method used in the case C1-R1 on a
PC with an Intel I5 processor and 8GB memory using R, where the tim e
required to generate the data is not included. We set n= 106,r0= 200
and enlarge the number of knots for spline function to K= 50. Each
subsampling strategy is evaluated 100 times. The results on diﬀeren trwith
a ﬁxedλfor the FLopt and Unif subsampling methods are given in Table
3. It is clear that subsampling can signiﬁcantly improve computationa l
eﬃciency compared withfulldata, andtheFLoptmethodismoreexp ensive
than the Unif method as expected.5. Real data analysis
5.1 Tecator data
Tecator data is available in fda.uscpackage, which has 215 meat samples.
For each sample, the data consists of 100 channel spectrum of ab sorbance
and the contents of fat, water and protein which are measured in p ercent.
The100channel spectrum measured over thewavelength range8 50-1050nm
provides a dense measurement spaced 2nm apart that can be cons idered
functional data. Figure 3 shows 50 randomly selected curves of sp ectrum of
absorbance and the histogram of the content of protein. In this e xperiment,
we shall study protein content by our proposed FLPRE.
234
850 900 950 1000 1050
Wavelengthabsorbance
010203040
10.0 12.5 15.0 17.5 20.0 22.5
Content of ProteinFrequency
Figure 3: Left subﬁgure: A random subset of 50 spectrometric cu rves.
Right subﬁgure: Histogram of the content of protein.5.2 Beijing multi-site air-quality data
Table 4: MAPE and MPPE for Tecator data.
Criterion FLS FLAD FLPRE
MAPE 3.5836 3.7452 3.5420
MPPE 0.0745 0.0739 0.0727
We employ the ﬁrst 160 observations to ﬁt the model, and then apply
the remaining samples to evaluate the prediction eﬃciency by the mea n of
absolute prediction errors (MAPE) and product relative prediction errors
(MPPE). The two mean criteria are measured by:
MAPE =1
55215/summationdisplay
i=161|yi−ˆyi|;
MPPE =1
55215/summationdisplay
i=161(yi−ˆyi)2/(yiˆyi),
where ˆyi= exp(/integraltext1
0xi(t)ˆβ(t)dt). All results are presented in Table 4 which
illustrates that the proposed FLPRE outperforms FLS and FLAD fo r pre-
dicting the protein content.
5.2 Beijing multi-site air-quality data
Thisdatasetisavailablein https://archive-beta.ics.uci.edu/ml/datasets/beijin g+multi+site+air+quality+data ,
and consists of hourly air pollutants data from 12 nationally controlle d air-
qualitymonitoringsites inBeijingfromMarch1, 2013toFebruary28, 2017.
Our primary interest here is to predict the maximum of daily PM 10con-5.2 Beijing multi-site air-quality data
Table 5: MAPE and MPPE for the air-quality data.
Criterion FLS FLAD FLPRE
MAPE 7.9902 8.7580 7.6727
MPPE 0.5031 0.5223 0.4970
centrations ( µg/m3) using the PM 10trajectory (24 hours) of the last day.
We delete all missing values and obtain a sample of 15573 days’ complet e
records. We take the top 80% of the sample as the training set and t he rest
as the test set. The raw observations after the square-root tr ansformation
are ﬁrst transformed into functional data using 15 Fourier basis f unctions.
This transformation can be implemented with the Data2fd function in the
fdapackage, suggested in Sang and Cao (2020). A random subset of 1 00
curves of 24-hourly PM 10concentrations is presented in the left panel of
Figure 4, where the time scale has been transformed to [0 ,1]. The right
panel of Figure 4 depicts the histogram of the maximal values of intr aday
PM10concentrations.
We assess the performances of prediction by the MAPE and MPPE
criteria. Table 5 illustrates that the proposed FLPRE outperforms FLS
and FLAD for predicting the PM 10concentrations.
Further, we calculate the IMSE and RPSE using (4.11) and compare5.2 Beijing multi-site air-quality data
1020
0.0 0.2 0.4 0.6 0.8 1.0
TimeSquare root of PM10
0200400600800
10 20 30
Maximal intraday sqrt(PM10)Frequency
Figure 4: Left subﬁgure: A random subset of 100 curves of 24-ho urly PM 10
concentrations. Right subﬁgure: Histogram of the maximal values of intra-
day PM 10concentrations.
the FLopt method with the Unif method. Figure 5 shows the results f or
diﬀerent subsampling sizes r= 1000,1500,2000,2500,3000 with r0= 1000.
We can ﬁnd that the FLopt method always has smaller IMSE and RPSE
compared with the Unif method. Besides, all IMSE and RPSE gradually
decrease as the subsampling size rincreases, showing the estimation consis-
tency of the subsampling methods and better approximation to the results
based on full data.
Supplementary Materials
All technical proofs are included in the online Supplementary Materia l.REFERENCES
−0.625−0.600−0.575−0.550
1000 1500 2000 2500 3000
rIMSEMethod
FLopt
Unif
−1.40−1.35−1.30−1.25−1.20−1.15
1000 1500 2000 2500 3000
rRPSEMethod
FLopt
Unif
Figure 5: IMSE and RPSE for diﬀerent subsampling sizes rwithr0= 1000
for 1000 repetitions.
Acknowledgements
This work was supported by the National Natural Science Foundat ion of
China (No. 11671060) and the Natural Science Foundation Projec t of CQ
CSTC (No. cstc2019jcyj-msxmX0267).
References
Ai M., Wang F., Yu J. and Zhang H. (2021a). Optimal subsamplin g for large-scale quantile
regression. Journal of Complexity 62, 101512.
Ai M., Yu J., Zhang H. and Wang H. (2021b). Optimal subsamplin g algorithms for big data
regression. Statistica Sinica 31, 749–772.REFERENCES
Atkinson A., Donev A. N. and Tobias R. D. (2007) Optimum Exper imental Designs, with SAS.
Oxford University Press, New York.
Cardot H., Ferraty F. and Sarda P. (2003). Spline estimators for the functional linear model.
Statistica Sinica 13, 571–591.
Chen, K., Guo S., Lin Y. and Ying Z. (2010). Least absolute rel ative error estimation. Journal
of the American Statistical Association 105, 1104–1112.
Chen K., Lin Y., Wang Z. and Ying Z. (2016). Least product rela tive error estimation. Journal
Of Multivariate Analysis 144, 91–98.
Chen Y. and Liu H. (2021). A new relative error estimation for partially linear multiplicative
model.Communications in Statistics-Simulation and Computation , 1–19.
Chen Y., Liu H. and Ma J.(2022). Local least product relative error estimation for single-index
varying-coeﬃcient multiplicative model with positive res ponses.Journal of Computational
and Applied Mathematics 415, 114478.
Claeskens G., Krivobokova T. and Opsomer J. D. (2009). Asymp totic properties of penalized
spline estimators. Biometrika 96, 529–544.
de Boor C. (2001). A Practical Guide to Splines. Springer-Ve rlag, Berlin.
Fan, R., Zhang S. and Wu Y. (2022). Penalized relative error e stimation of functional multi-
plicative regression models with locally sparse propertie s.Journal of the Korean Statistical
Society51, 666–691.REFERENCES
Fan Y., Liu Y. and Zhu L. (2021). Optimal subsampling for line ar quantile regression models.
Canadian Journal of Statistics 49, 1039–1057.
He S., Yan X. (2022). Functional principal subspace samplin g for large scale functional data
analysis. Electronic Journal of Statistics 16,2621–2682.
Liu H., You J. and Cao J. (2021). Functional L-optimality sub sampling for massive data. arXiv
preprint arXiv:2104.03446.
Ma P., Mahoney M. W. and Yu B. (2015). A statistical perspecti ve on algorithmic leveraging.
Journal of Machine Learning Research 16, 861–911.
Ma P., Zhang X., Xing X., Ma J. and Mahoney M. W. (2020). Asympt otic analysis of sam-
pling estimators for randomized numerical linear algebra a lgorithms. In Proceedings of the
Twenty Third International Conference on Artiﬁcial Intell igence and Statistics , 1026–1035.
Ming H., Liu H. and Yang H. (2022). Least product relative err or estimation for identiﬁcation in
multiplicative additive models. Journal of Computational and Applied Mathematics 404,
113886.
Ren M., Zhao S. and Wang M. (2022). Optimal subsampling for le ast absolute relative error
estimators with massive data. Journal of Complexity 74, 101694.
RuppertD.(2002) Selectingthenumberofknotsfor penalize dsplines. Journal of Computational
and Graphical Statistics 11, 735–757.
Sang P. and Cao J. (2020). Functional single-index quantile regression models. Statistics andREFERENCES
Computing 30, 771–781.
Shao L., Song S. and Zhou Y. (2022). Optimal subsampling for l arge-sample quantile regression
with massive data. Canadian Journal of Statistics .https://doi.org/10.1002/cjs.11697
Shao Y. and Wang L. (2021). Optimal subsampling for composit e quantile regression model in
massive data. Statistical Papers 63, 1139–1161.
Wang H. (2019). More eﬃcient estimation for logistic regres sion with optimal subsamples. Jour-
nal of Machine Learning Research 20, 1–59.
Wang H. and Ma Y. (2021). Optimal subsampling for quantile re gression in big data.
Biometrika 108, 99–112.
Wang H., Zhu R. and Ma P. (2018). Optimal subsampling for larg e sample logistic regression.
Journal of the American Statistical Association 113, 829–844.
Wang T. and Zhang H. (2022). Optimal subsampling for multipl icative regression with massive
data.Statistica Neerlandica 76, 418–449.
Yao Y. and Wang H. (2019). Optimal subsampling for softmax re gression. Statistical Papers 60,
585–599.
Yan Q., Li H. and Niu C. (2022). Optimal subsampling for funct ional quantile regression.
Statistical Papers .https://doi.org/10.1007/s00362-022-01367-z .
Yu J., Wang H., Ai M. and Zhang H. (2020). Optimal distributed subsampling for maximum
quasi-likelihood estimators with massive data. Journal of the American Statistical Associ-REFERENCES
ation117, 265–276.
Yuan X., Li Y., Dong X. and Liu T. (2022). Optimal subsampling for composite quantile
regression in big data. Statistical Papers 63, 1649–1676.
Zhang T., Huang Y., Zhang Q., Ma S. and Ahmed S. E. (2019). Pena lized relative error esti-
mation of a partially functional linear multiplicative mod el. In Ahmed S. E., Carvalho F.
and Puntanen S. (Eds.), Matrices, Statistics and Big Data , Springer, Cham.
Zhang T., Zhang Q. and Li N. (2016). Least absolute relative e rror estimation for functional
quadratic multiplicative model. Communications in Statistics-Theory and Methods 45,
5802–5817.
Qian Yan
College of Mathematics and Statistics, Chongqing Universi ty, Chongqing 401331, China.
E-mail: qianyan@cqu.edu.cn
Hanyu Li
College of Mathematics and Statistics, Chongqing Universi ty, Chongqing 401331, China.
E-mail: lihy.hy@gmail.com or hyli@cqu.edu.cn