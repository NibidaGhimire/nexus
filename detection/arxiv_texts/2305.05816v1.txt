1–45
Best-Effort Adaptation
Pranjal Awasthi PRANJALAWASTHI @GOOGLE .COM
Google Research, Mountain View
Corinna Cortes CORINNA @GOOGLE .COM
Google Research, New York
Mehryar Mohri MOHRI @GOOGLE .COM
Google Research and Courant Institute of Mathematical Sciences, New York
Abstract
We study a problem of best-effort adaptation motivated by several applications and consid-
erations, which consists of determining an accurate predictor for a target domain, for which a
moderate amount of labeled samples are available, while leveraging information from another do-
main for which substantially more labeled samples are at one’s disposal. We present a new and
general discrepancy-based theoretical analysis of sample reweighting methods, including bounds
holding uniformly over the weights. We show how these bounds can guide the design of learning
algorithms that we discuss in detail. We further show that our learning guarantees and algorithms
provide improved solutions for standard domain adaptation problems, for which few labeled data
or none are available from the target domain. We ﬁnally report the results of a series of experiments
demonstrating the effectiveness of our best-effort adaptation and domain adaptation algorithms, as
well as comparisons with several baselines. We also discuss how our analysis can beneﬁt the design
of principled solutions for ﬁne-tuning .
Keywords: Domain adaptation, Distribution shift, ML fairness.
1. Introduction
Consider the following adaptation problem that frequently arises in applications. Suppose we have
access to a fair amount of labeled data from a target domain Pand to a signiﬁcantly larger amount
of labeled data from a different domain Q. How can we best exploit both collections of labeled
data to come up with as accurate a predictor as possible for the target domain P? We will refer to
this problem as the best-effort adaptation problem since we seek the best method to leverage the
additional labeled data from Qto come up with a best predictor for P. One would imagine that the
data from Qshould be helpful in improving upon the performance obtained by training only on the
Pdata, if Qis not too different from P. The question is how to measure this difference and account
for it in the learning algorithm. This best-effort problem differs from standard domain adaptation
problems where typically very few or no labeled data from the target is at one’s disposal.
Best-effort adaptation can also be motivated by fairness considerations, such as racial disparities
in automated speech recognition (Koenecke et al., 2020). A signiﬁcant gap has been reported for
the accuracy of speech recognition systems when tested on speakers of vernacular English versus
non-vernacular English speakers. In practice, there is a substantially larger amount of labeled data
available for the non-vernacular domain since it represents a larger population of English speakers.
As a result, it might not be possible, with the training data in hand, to achieve an accuracy for vernac-
ular speech similar to the one achieved for non-vernacular speech. Such a recognition system might
© P. Awasthi, C. Cortes & M. Mohri.arXiv:2305.05816v1  [cs.LG]  10 May 2023AWASTHI CORTES MOHRI
therefore have only one method for equalizing accuracy between these populations: namely, degrad-
ing the system’s performance on the larger population. Alternatively, one could instead formulate
the problem of maximizing the performance of the system on the vernacular speakers, leveraging
allthe data available at hand to ﬁnd the best-effort predictor for vernacular speakers.
Here, we present a detailed study of best-effort adaptation, including a new and general theo-
retical analysis of reweighting methods using the notion of discrepancy, as well as new algorithms
and empirical evaluations. We further show how our analysis can be extended to that of domain
adaptation problems, for which we also design new algorithms and report experimental results.
There is a very broad literature dealing with adaptation solutions for distinct scenarios and we
cannot present a comprehensive survey here. Instead, we brieﬂy discuss here the most closely
related work and give a detailed discussion of previous work in Appendix A. We also refer the
reader to papers such as (Pan and Yang, 2009; Wang and Deng, 2018). Let us add that similar
scenarios to best-effort adaptation have been studied in the past under some different names such
asinductive transfer orsupervised domain adaptation but with the assumption of much smaller
labeled data from the target domain (Garcke and Vanck, 2014; Hedegaard et al., 2021).
The work we present includes a signiﬁcant theoretical component and beneﬁts from prior theo-
retical analyses of domain adaptation. The theoretical analysis of domain adaptation was initiated
by Kifer et al. (2004) and Ben-David et al. (2006) with the introduction of a dA-distance between
distributions. They used this notion to derive VC-dimension learning bounds for the zero-one loss,
which was elaborated on in subsequent works (Blitzer et al., 2008; Ben-David et al., 2010a). Later,
Mansour et al. (2009a) and Cortes and Mohri (2011, 2014) presented a general analysis of single-
source adaptation for arbitrary loss functions, where they introduced the notion of discrepancy , a
divergence measure nicely aligned with domain adaptation. Discrepancy coincides with the dA-
distance in the special case of the zero-one loss. It takes into account the loss function and hy-
pothesis set and, importantly, can be estimated from ﬁnite samples. The authors gave a discrepancy
minimization algorithm based on a reweighting of the losses of sample points. We use their notion
of discrepancy in our new analysis. Cortes et al. (2019b) presented an extension of the discrepancy
minimization algorithm based on the so-called generalized discrepancy , which both incorporates a
hypothesis-dependency and works with a less conservative notion of local discrepancy deﬁned by
a supremum over a subset of the hypothesis set. The notion of local discrepancy has been since
adopted in several recent publications, in the study of active learning or adaptation (de Mathelin
et al., 2021; Zhang et al., 2019c, 2020b) and is also used in part of our analysis.
While our main motivation is best-effort adaptation, in Section 3, we present a general analysis
that holds for all sample reweighting methods . Our theoretical analysis and learning bounds are new
and are based on the notion of discrepancy. They include learning guarantees holding uniformly
with respect to the weights, as well as a lower bound suggesting the importance of the discrepancy
term in our bounds. Our theory guides the design of principled learning algorithms for best-effort
adaptation, BEST and SBEST , that we discuss in detail in Section 4. This includes our estimation of
the discrepancy terms via DC-programming (Appendix B.3).
In Section 5, we further show how our analysis can be extended to the case where few labeled
data or none are available from the target domain, that is the scenario of (unsupervised or weakly
supervised) domain adaptation. Here too, we derive new discrepancy-based learning bounds based
on reweighting, including uniform bounds with respect to the weights (Section 5.1). Interestingly,
here, an additional set of sample weights naturally appears in the analysis, to account for the absence
of labels from the target. Our theoretical analysis leads to the design of a new adaptation algorithms,
2BEST-EFFORT ADAPTATION
BEST -DA(Section 5.2). We further discuss in detail how in this scenario labeled discrepancy terms
can be upper-bounded in terms of unlabeled ones, including unlabeled local discrepancies, and how
some additional amount of labeled data can be beneﬁcial (Section 5.3).
In Section 6, we report the results of experiments with both our best-effort adaptation algorithms
and our domain adaptation algorithms demonstrating their effectiveness, as well as comparisons
with several baselines. This includes a discussion and empirical analysis of how our results ben-
eﬁt the design of principled solutions for ﬁne-tuning and other few-shot algorithms (Section A.2).
We start with the introduction of some preliminary deﬁnitions and concepts related to adaptation
(Section 2).
2. Preliminaries
We denote by Xthe input space and Ythe output space. In the regression setting, Yis assumed to
be a measurable subset of R. We will denote by Ha hypothesis set of functions mapping from Xto
Yand by`∶Y×Y→Ra loss function assumed to take values in [0;1].
We will study problems with a source domain Qand target domain P, where QandPare
distributions over X×Y. We will denote by ̂Qthe empirical distribution associated to a sample S
of sizemdrawn from Qmand similarly by ̂Pthe empirical distribution associated to a sample S′
of sizendrawn from Pn. We will denote by QXandPXthe marginal distributions of QandPon
X. We will denote by L(P;h)the population loss of a hypothesis over Pdeﬁned as: L(P;h)=
E(x;y)∼P[`((x);y)].
Several notions of discrepancy have been shown to be adequate measures between distributions
for adaptation problems (Kifer et al., 2004; Mansour et al., 2009a; Mohri and Mu ˜noz Medina, 2012;
Cortes and Mohri, 2014; Cortes et al., 2019b). We will denote by dis(P;Q)thelabeled discrepancy
ofPandQ, also called Y-discrepancy in (Mohri and Mu ˜noz Medina, 2012; Cortes et al., 2019b) and
deﬁned by:
dis(P;Q)=sup
h∈HE
(x;y)∼P[`(h(x);y)]−E
(x;y)∼Q[`(h(x);y)]: (1)
Note that we are not using absolute values around the difference of expectations, as in the original
discrepancy deﬁnitions in prior work as the one-sided deﬁnition sufﬁces for our analysis. We will
denote the version with absolute values as: Dis(P;Q)=max{dis(P;Q);dis(Q;P)}.
By deﬁnition, computing the labeled discrepancy assumes access to labels from both PandQ.
In contrast, the unlabeled discrepancy , denoted by dis(P;Q), requires no access to such labels
dis(P;Q)=sup
h;h′∈HE
x∼PX/bracketleft.alt1`(h(x);h′(x))/bracketright.alt−E
x∼QX/bracketleft.alt1`(h(x);h′(x))/bracketright.alt: (2)
We will similarly denote by Dis(P;Q)the counterpart of this deﬁnition with absolute values. As
shown by Mansour et al. (2009a), the unlabeled discrepancy can be accurately estimated from ﬁnite
(unlabeled) samples from QXandPXwhenHadmits a favorable Rademacher complexity, for
example a ﬁnite VC-dimension. The unlabeled discrepancy is a divergence measure tailored to
(unsupervised) adaptation that can be upper bounded by the `1-distance. It coincides with the so-
calleddA-distance introduced by Kifer et al. (2004) in the special case of the zero-one loss. We will
also be using the ﬁner notion of local labeled discrepancy for some suitably chosen subsets H1and
H2ofH:
disH1×H2(P;Q)=sup
(h;h′)∈H1×H2E
x∼PX/bracketleft.alt1`(h(x);h′(x))/bracketright.alt−E
x∼QX/bracketleft.alt1`(h(x);h′(x))/bracketright.alt: (3)
3AWASTHI CORTES MOHRI
Local discrepancy (Cortes et al., 2019b) is deﬁned by a supremum over smaller sets and is thus a
more favorable quantity. We further extend all the discrepancy deﬁnitions just presented to the case
where PandQare ﬁnite signed measures over X×Y, using the same expressions as above. We also
abusively extend the deﬁnition of discrepancy to distributions over sample indices. As an example,
given the samples SandS′and a distribution qover their [m+n]indices, we deﬁne the discrepancy
dis(̂P;q)as follows: dis(̂P;q)=suph∈H1
n∑n
i=m+1`(h(xi);yi)−∑m+n
i=1qi`(h(xi);yi).
3. Discrepancy-based generalization bounds
There are many algorithms in adaptation based on various methods for reweighting sample losses
and it is natural to seek a similar solution for best-effort adaptation (see Appendix A). We present a
general theoretical analysis covering all such sample reweighting methods. We give new discrepancy-
based generalization bounds, including learning bounds holding uniformly over the weights.
We assume that the learner has access to a labeled sample S=((x1;y1);:::;(xm;ym))drawn
fromQmand a labeled sample S′=((xm+1;ym+1);:::;(xm+n;ym+n))drawn from Pn. In the
problems we consider, we typically have m/uni226Bn, but our analysis applies is general. For a non-
negative vector qin[0;1][m+n], we denote by qthe total weight on the ﬁrstmpoints: q=∑m
i=1qi
and byRq(`○H)theq-weighted Rademacher complexity:
Rq(`○H)=E
S;S′;/bracketleft.alt4sup
h∈Hm+n
/summation.disp
i=1iqi`(h(xi);yi)/bracketright.alt4; (4)
where the Rademacher variables iare independent random variables distributed uniformly over
−;+. The q-weighted Rademacher complexity is a natural extension of the Rademacher complexity
taking into consideration distinct weights assigned to sample points. It can be upper-bounded as
follows in terms of the (unweighted) Rademacher complexity: Rq(`○H)≤/parallel.alt1q/parallel.alt1∞(m+n)Rm+n(`○
H), with equality for uniform weights (see Lemma 9, Appendix B).
The following is a general learning guarantee expressed in terms of the weights q. Note that we
do not require qto be a distribution over [m+n], that is/parallel.alt1q/parallel.alt11may not equal one.
Theorem 1 Fix a vector qin[0;1][m+n]. Then, for any >0, with probability at least 1−over
the choice of a sample Sof sizemfromQand a sample S′of sizenfromP, the following holds for
allh∈H:
L(P;h)≤m+n
/summation.disp
i=1qi`(h(xi);yi)+dis/parenleft.alt2/bracketleft.alt1(1−/parallel.alt1q/parallel.alt11)+q/bracketright.altP;qQ/parenright.alt2+2Rq(`○H)+/parallel.alt1q/parallel.alt12/roottop
/rootmod/rootmod/rootbotlog1

2:
This bound is tight as a function of the discrepancy term, as shown by the following theorem,
which underscores the importance of that term. The proofs for both theorems are given in Ap-
pendix B.
Theorem 2 Fix a distribution qinm+n. Then, for any >0, there exists h∈Hsuch that, for any
>0, the following lower bound holds with probability at least 1−over the choice of a sample S
of sizemfromQand a sample S′of sizenfromP:
L(P;h)≥m+n
/summation.disp
i=1qi`(h(xi);yi)+qdis(P;Q)−2Rq(`○H)−/parallel.alt1q/parallel.alt12/roottop
/rootmod/rootmod/rootbotlog1

2−:
4BEST-EFFORT ADAPTATION
In particular, for /parallel.alt1q/parallel.alt12;Rq(`○H)∈O(1√m+n), we have:
L(P;h)≥m+n
/summation.disp
i=1qi`(h(xi);yi)+qdis(P;Q)+
/parenleft.alt41√m+n/parenright.alt4:
The bound of Theorem 1 cannot be used to choose qsince it holds for a ﬁxed choice of that
vector. A standard way to derive a uniform bound over qis via covering numbers. That requires
applying the union bound to the centers of an -covering of [0;1][m+n]for the`1distance. But,
the corresponding covering number N1would be in O((1/slash.left)m+n), resulting in an uninformative
bound, even for /parallel.alt1q/parallel.alt12=1/slash.left√m+n, since/radical.alt1
logN1/slash.leftm+nwould be a constant. Instead, we present
an alternative analysis, generalizing Theorem 1 to hold uniformly over qin/braceleft.alt1q∶0</parallel.alt1q−p0/parallel.alt11<1/braceright.alt1,
where p0could be interpreted as a reference (or ideal) reweighting choice. The proof is presented
in Appendix B.
Theorem 3 For any>0, with probability at least 1−over the choice of a sample Sof sizemfrom
Qand a sample S′of sizenfromP, the following holds for all h∈Handq∈/braceleft.alt1q∶0≤/parallel.alt1q−p0/parallel.alt11<1/braceright.alt1:
L(P;h)≤m+n
/summation.disp
i=1qi`(h(xi);yi)+dis/parenleft.alt2/bracketleft.alt1(1−/parallel.alt1q/parallel.alt11)+q/bracketright.altP;qQ/parenright.alt2+dis(q;p0)
+2Rq(`○H)+5/parallel.alt1q−p0/parallel.alt11+/bracketleft.alt1/parallel.alt1q/parallel.alt12+2/parallel.alt1q−p0/parallel.alt11/bracketright.alt/uni23A1/uni23A2/uni23A2/uni23A2/uni23A3/radical.alt2
log log22
1−/parallel.alt1q−p0/parallel.alt11+/radical.alt3
log2

2/uni23A4/uni23A5/uni23A5/uni23A5/uni23A6:
Note that for q=p0, the bound coincides with that of Theorem 1.
Learning bounds insights . Theorems 1 and 3 provide general guarantees for best-effort adapta-
tion. They suggest that for adaptation to succeed via sample reweighting, a favorable balance of
several key terms is important. The ﬁrst term suggests minimizing the q-weighted empirical loss.
However, the bound advises against doing so at the price of assigning non-zero weights only to a
small fraction of the points since that would increase the /parallel.alt1q/parallel.alt12term. In fact, a comparison with the
familiar inverse of square-root of the sample size term appearing in other bounds suggests interpret-
ing/parenleft.alt11/slash.left/parallel.alt1q/parallel.alt12
2/parenright.alt1as the effective sample size . Note also that when qis a distribution, the second term
admits the following simpler form: dis([(1−/parallel.alt1q/parallel.alt11)+q]P;qQ)=dis(qP;qQ)=qdis(P;Q). Thus,
the second term of these bounds suggests allocating less weight to the points drawn from Q, when
the discrepancy dis(P;Q)is large. The weighted discrepancy term dis(q;p0)and the`1-distance
/parallel.alt1q−p0/parallel.alt11in Theorem 3 both press qto be chosen relatively closer to the reference p0. Finally, the
Rademacher complexity term is a familiar measure of the complexity of the hypothesis set, which
here additionally takes into consideration the weights.
In Appendix B.2, we compare the bound of Theorem 1 with some existing discrepany-based
ones and show how they can be recovered as special cases. In particular, we show that the discrepancy-
based bound of Cortes et al. (2019b), which is the basis for the discrepancy minimization algorithm
of Cortes and Mohri (2014), is always an upper bound on a special case (speciﬁc choice of the
weights) of the bound of Theorem 1.
We note that assigning non-uniform weights to the points in Sshould not be viewed as unnatural,
even though the points are sampled from the same distribution. This is because these weights serve
to make the q-weighted empirical loss closer to the empirical loss for the target sample. As an
5AWASTHI CORTES MOHRI
example, importance weighting seeks distinct weights for each point based on the source and target
distributions. Nevertheless, in Appendix B.2, we consider a simple -reweighting method, which
allocates uniform weights to source points. We show that, under some assumptions, even for this
very simple choice of the weights, the learning bound can be more favorable than the one for training
only on target samples.
Theorem 3 suggests choosing h∈Handq∈{q∶0≤/parallel.alt1q−p0/parallel.alt11<1}to minimize the right-hand
side of the inequality and seek the best balance between these key terms. This guides the design of
our learning algorithms. The following corollary provides a slightly simpliﬁed version of Theorem 3
(see Appendix B).
Corollary 4 For any>0, with probability at least 1−over the choice of a sample Sof
sizemfromQand a sample S′of sizenfromP, the following holds for all h∈Handq∈
/braceleft.alt1q∶0≤/parallel.alt1q−p0/parallel.alt11<1/braceright.alt1:
L(P;h)≤m+n
/summation.disp
i=1qi`(h(xi);yi)+qdis(P;Q)+dis(q;p0)+2Rq(`○H)
+6/parallel.alt1q−p0/parallel.alt11+/bracketleft.alt1/parallel.alt1q/parallel.alt12+2/parallel.alt1q−p0/parallel.alt11/bracketright.alt/uni23A1/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A3/radical.alt2
log log22
1−/parallel.alt1q−p0/parallel.alt11+/roottop
/rootmod/rootmod/rootbotlog2

2/uni23A4/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A6:
4. Best-Effort adaptation algorithms
In this section, we describe new learning algorithms for best-effort adaptation directly beneﬁting
from the theoretical analysis of the previous section.
Optimization problem, BEST and SBEST algorithms . The previous section suggests seeking h∈
Handq∈[0;1]m+nto minimize the bound of Theorem 3 or that of Corollary 4. To simplify the
discussion, we will focus on the algorithm derived from Corollary 4. A similar but ﬁner algorithm
consists instead of using directly Theorem 3.
Assume that His a subset of a normed vector space and that the Rademacher complexity term
can be bounded by an upper bound on the norm squared /parallel.alt1h/parallel.alt12. Then, using the shorthand di=
dis(P;Q)1i∈[m], the optimization problem can be written as:
min
h∈H;q∈[0;1]m+nm+n
/summation.disp
i=1qi[`(h(xi);yi)+di]+dis(q;p0)+∞/parallel.alt1q/parallel.alt1∞/parallel.alt1h/parallel.alt12
+1/parallel.alt1q−p0/parallel.alt11+2/parallel.alt1q/parallel.alt12
2;
where1,2and∞are non-negative hyperparameters. A natural choice for p0in our scenario is
the uniform distribution over S′, which is the empirical distribution in the absence of any point from
a different distribution Q. We will refer by BEST to an algorithm seeking to minimize this objective.
We will also consider a simpler version of our algorithm, SBEST , where we upper-bound dis(q;p0)
by/parallel.alt1q−p0/parallel.alt11, in which case this additional term is subsumed by the existing one with 1factor.
When the loss function `is convex with respect to its ﬁrst argument, the objective function is
convex inhand in q. In particular, dis(q;p0)is a convex function of qas a supremum of convex
6BEST-EFFORT ADAPTATION
functions (afﬁne functions in q):dis(q;p0)=suph∈H/braceleft.alt1∑m+n
i=1(qi−p0
i)`(h(xi);yi)/braceright.alt1. But, the ob-
jective function is not jointly convex.
Alternating minimization solution . One method for solving the problem consists of alternating
minimization (or block coordinate descent), that is of minimizing the objective over Hfor a ﬁxed
value of qand next of minimizing with respect to qfor a ﬁxed value of h. In general, this method
does not beneﬁt from convergence guarantees, although there is a growing body of literature proving
guarantees under various assumptions (Grippo and Sciandrone, 2000; Li et al., 2019; Beck, 2015).
DC-programming solution . An alternative solution consists of casting the problem as an instance
of DC-programming (difference of convex) by rewriting the objective as a difference. Note that for
any non-negative and convex function fand any non-decreasing and convex function 	deﬁned
overR+,	○fis convex: for all (x;x′)∈X2and∈[0;1],
(	○f)(x+(1−)x′)≤	(f(x)+(1−)f(x′))
≤(	○f)(x)+(1−)(	○f)(x′);
where the ﬁrst inequality holds by the convexity of fand the non-decreasing property of 	and
the last one by the convexity of 	. In particular, for any non-negative and convex function f,f2
is convex. Thus, we can rewrite the non-jointly convex terms of the objective as the following
DC-decompositions:
qi`(h(xi);yi)=1
2/bracketleft.alt1[qi+u]2−/bracketleft.alt1q2
i+u2/bracketright.alt/bracketright.alt;
/parallel.alt1q/parallel.alt1∞/parallel.alt1h/parallel.alt12=1
2/bracketleft.alt2/bracketleft.alt1/parallel.alt1q/parallel.alt1∞+/parallel.alt1h/parallel.alt12/bracketright.alt2−/bracketleft.alt1/parallel.alt1q/parallel.alt12
∞+/parallel.alt1h/parallel.alt12/bracketright.alt/bracketright.alt2;
whereu=`(h(xi);yi). We can then use the DCA algorithm of Tao and An (1998), (see also Tao
and An (1997)), which in our differentiable case coincides with the CCCP algorithm of Yuille and
Rangarajan (2003), further analyzed by Sriperumbudur et al. (2007). The DCA algorithm guaran-
tees convergence to a critical point. The global optimum can be found by combining DCA with a
branch-and-bound or cutting plane method (Tuy, 1964; Horst and Thoai, 1999; Tao and An, 1997).
Discrepancy estimation . Our algorithm requires estimating the discrepancy terms. We discuss our
DC-programming solution to this problem in detail in Appendix B.3.
As already pointed, our learning bounds are general and can be used for the analysis of various
speciﬁc reweighting methods with bounded weights, including discrepancy minimization (Cortes
and Mohri, 2014), KMM (Huang et al., 2006), KLIEP (Sugiyama et al., 2007b), importance weight-
ing (Cortes et al., 2010), when the weights are bounded, and many others. However, unlike our
algorithms, which simultaneously learn the weights and the hypothesis and directly beneﬁt from
the learning bounds of the previous section, these algorithms typically consist of two stages and do
not exploit the guarantees discussed: in the ﬁrst stage, they determine some weights q, irrespec-
tive of the labeled samples and the empirical loss; in the second stage, they use these weights to
learn a hypothesis minimizing the q-weighted empirical loss. Additionally, some methods admit
other speciﬁc drawbacks. For example, it was shown by Cortes et al. (2010), both theoretically and
empirically, that, in general, importance weighting may not succeed. Note also that the method
relies only on the ratio of the densities and does not take into account, unlike the discrepancy, the
hypothesis set and the loss function.
7AWASTHI CORTES MOHRI
5. Domain adaptation
The analysis of Section 3 can also be used to derive general discrepancy-based guarantees for do-
main adaptation, where the learner has access to few or no labeled points from the target domain.
In this section, we analyze the case where the input points in S′are unlabeled. Our analysis can be
straightforwardly extended to the case where a small fraction of the labels in S′are available. Our
theoretical analysis leads to the design of new algorithms for domain adaptation.
5.1. Domain adaptation generalization bounds
For convenience, in this section, we will use a different notation for the weights on SandS′:
q∈[0;1]mfor the weights on S,q′∈[0;1]nfor the weights on S′. The labels of the points in
S′appear in the ﬁrst term of the bound of Theorem 1, the q-weighted empirical loss. Since they
are not available, we upper-bound the empirical loss in terms of a p-weighted empirical loss and a
discrepancy term:
m
/summation.disp
i=1qi`(h(xi);yi)+n
/summation.disp
i=1q′
i`(h(xm+i);ym+i)≤m
/summation.disp
i=1(qi+pi)`(h(xi);yi)+dis(q′;p); (5)
for any weight vector p∈[0;1]m. This yields immediately the following theorem.
Theorem 5 Fix the vectors qin[0;1][m]andq′∈[0;1]n. Then, for any >0, with probability at
least 1−over the choice of a sample Sof sizemfromQand a sample S′of sizenfromP, the
following holds for all pin[0;1][m]andh∈H:
L(P;h)≤m
/summation.disp
i=1(qi+pi)`(h(xi);yi)+dis(q′;p)+dis/parenleft.alt2/bracketleft.alt11−/parallel.alt1q′/parallel.alt11/bracketright.altP;/parallel.alt1q/parallel.alt11Q/parenright.alt2
+2R(q;q′)(`○H)+/roottop
/rootmod/rootmod/rootbot/parenleft.alt1/parallel.alt1q/parallel.alt12
2+/parallel.alt1q′/parallel.alt12
2/parenright.alt1log1

2:
This learning bound can be extended to hold uniformly over
/braceleft.alt1(q;q′)∈[0;1]m×[0;1]n∶0</parallel.alt1(q;q′)−p0/parallel.alt11<1/braceright.alt1
and all pin[0;1][m], where p0is a reference (or ideal) reweighting choice over the (m+n)points
(see Theorem 10 and Corollary 11 in Appendix C). Note that, here, both pandq′can be chosen
to make the weighted-discrepancy term dis(q′;p)smaller. Several of the comments on Theorem 1
similarly apply here. In particular, it is worth pointing out that the learning bound of Cortes et al.
(2019b) can be recovered for a speciﬁc choice of the weights. This holds even in the special case
where q=0and where q′is a distribution:
L(P;h)≤m
/summation.disp
i=1pi`(h(xi);yi)+dis(q′;p)+2Rq′(`○H)+/parallel.alt1q′/parallel.alt12/roottop
/rootmod/rootmod/rootbotlog1

2:
In that case, choosing q′to be the empirical distribution on S′leads to the bound of Cortes et al.
(2019b) (see also inequality (17), in Appendix B.2). An alternative choice of the weights may lead
8BEST-EFFORT ADAPTATION
to a smaller discrepancy term dis(q′;p)and a better guarantee overall. Our learning algorithm will
seek an optimal choice for the weights.
The discrepancy quantities appearing in the bound of the theorem cannot be estimated in the
absence of labels for S′. Thus, we need to resort to upper-bounds expressed in terms of unlabeled
discrepancies, using only unlabeled data from P. A detailed analysis is presented in Appendix 5.3.
5.2. Domain adaptation BEST -DAalgorithm
The analysis of the previous section suggests seeking h∈H,qandpin[0;1]mandq′in[0;1]n
to minimize the bound of Theorem 10 or that of Corollary 11. As in Section 4, assume that His a
subset of a normed vector space and that the Rademacher complexity term can be bounded in terms
of an upper bound on the norm squared /parallel.alt1h/parallel.alt12. Then, the optimization problem corresponding to
Corollary 11 can be written as follows:
min
h∈H;q;p∈[0;1]m
q′∈[0;1]nm
/summation.disp
i=1(qi+pi)`(h(xi);yi)+/parallel.alt1q/parallel.alt11d+dis(q′;p)+dis((q;q′);p0) (6)
+∞/parallel.alt1(q;q′)/parallel.alt1∞/parallel.alt1h/parallel.alt12+1/parallel.alt1(q;q′)−p0/parallel.alt11+2(/parallel.alt1q/parallel.alt12
2+/parallel.alt1q′/parallel.alt12
2);
where1,2and∞are non-negative hyperparameters and where we used the shorthand d=
dis(P;Q). We are omitting subscripts to simplify the presentation but, as discussed in the previous
section, the unlabeled discrepancies in the optimization problem may be local unlabeled discrep-
ancies, which are ﬁner quantities. As in the best-effort adaptation, a natural choice for p0in the
domain adaptation scenario is the uniform distribution over the input points of S′. In practice,
speciﬁc applications may motivate better choices.
We will refer by BEST -DAto the algorithm seeking to minimize this objective. Our comments
and analysis of the BEST optimization (Section 4) apply similarly here. In particular, the problem
can be similarly cast as a DC-programming problem or a convex optimization problem. The unla-
beled discrepancy term d=dis(P;Q)can be accurately estimated from dis(P;Q). In Appendix C.4,
we show in detail how to compute dis(P;Q)and how to evaluate the sub-gradients of the weighted
discrepancy terms.
Discussion of new BEST -DAalgorithm
Our BEST -DAalgorithm beneﬁts from more favorable guarantees than previous discrepancy-based
algorithms (Mansour et al., 2009a; Cortes and Mohri, 2014; Cortes et al., 2019b) and algorithms
seeking to minimize the learning bound (17), with the unlabeled discrepancy upper bounded by the
label discrepancy. This is because, as already pointed out, BEST -DAis based on a learning guarantee
that admits as a special case (17). Thus, the best choice of the weights and predictor sought by the
algorithm include those corresponding to previous algorithms as a special case.
Moreover, as discussed in Section 3, our upper bounds in terms of local discrepancy are ﬁner
than those used in previous work. In particular, BEST -DAimproves upon the DMalgorithm ( discrep-
ancy minimization ) of Cortes and Mohri (2014), which has been shown empirically by the authors
to outperform other domain adaptation baselines in regression tasks. DMseeks to minimize (17)
via a two-stage method, by ﬁrst seeking weights that minimize the unlabeled weighted-discrepancy
(second term) and subsequently seeking h∈Hto minimize the empirical loss for that ﬁxed choice
9AWASTHI CORTES MOHRI
ofq. This two-stage method may be suboptimal, compared to an algorithm seeking to directly min-
imize the bound to ﬁnd (h;q). The solution qfound to minimize the discrepancy term in the ﬁrst
stage may, for example, assign signiﬁcantly larger weights to some sample points, which could lead
to a poor choice of the predictor in the second stage.
An alternative sophisticated technique based on the so-called generalized discrepancy is advo-
cated by Cortes et al. (2019b). The main beneﬁt of this technique is to allow for the weights to
be chosen as a function of the hypotheses, unlike the two-stage DMsolution of Cortes and Mohri
(2014). Our BEST -DAalgorithm, however, already offers that advantage since the hypothesis hand
the weights q,q′andpare sought simultaneously as a solution of the optimization problem. Note,
however that the choice of the weights in the generalized discrepancy method does not take into
consideration the empirical losses, unlike our algorithm. Furthermore, BEST -DAminimizes a learn-
ing bound admitting as a special case (17), the best learning guarantee presented by the authors
in support of their algorithm. Let us add that authors state that their guarantee for the generalized
discrepancy method is not comparable to that of DMalgorithm.
5.3. Labeled discrepancy upper bounds
The analysis of Section 3 is based on the labeled discrepancy measure dis(P;Q)or its estimate
from ﬁnite samples dis(̂P;̂Q), which assumes access to labeled data from the target distribution P.
In typical domain adaptation problems, however, there is little labeled data or none from the target
domain P. Thus, instead we need to resort to an upper-bound on dis(P;Q)in terms of the unlabeled
discrepancy, which only uses unlabeled data from P.
We will discuss two types of upper bounds, ﬁrst in the special case of the squared loss, next in
the case of an arbitrary -Lipschitz loss. Our analysis beneﬁts from that of previous work (Cortes
and Mohri, 2014; Cortes et al., 2019b) but improves upon that, as discussed later.
Squared loss. Here, we give an upper bound on the labeled discrepancy in the case of the squared
loss. For any hypothesis h0∈H, we denote by H;h0(̂P;̂Q)thesquared-loss label discrepancy of̂P
and̂Q:
H;h0(̂P;̂Q)=sup
h∈H/divides.alt0E
(x;y)∼̂P[h(x)(y−h0(x))]−E
(x;y)∼̂Q[h(x)(y−h0(x))]/divides.alt0: (7)
Lemma 6 Let`be the squared loss. Then, for any hypothesis h0inH, the following upper bound
holds for the labeled discrepancy:
dis(̂P;̂Q)≤disH×(̂P;̂Q)+2H;h0(̂P;̂Q):
The proof is given below in Appendix C.2. The local unlabeled discrepancy disH×(̂P;̂Q)captures
the closeness of the input distributions ̂PXand̂QX. It is a signiﬁcantly more favorable term that
the standard unlabeled discrepancy since it admits only a maximum over h∈Hand not over both h
andh′inH.
For a suitable choice of h0∈H, the termH;h0(̂P;̂Q)captures the closeness of the empiri-
cal output labels on ̂Pand̂Q. Note that for ̂P=̂Q, we haveH;h0(̂P;̂Q)=0for anyh0∈H.
When the covariate-shift assumption holds and the problem is separable, h0can be chosen so that
H;h0(̂P;̂Q)=0. More generally, when h0can be chosen so that /divides.alt0y−h0(x)/divides.alt0is relatively small on
both samples corresponding to ̂Pand̂Qand the hypotheses h∈Hare bounded by some M>0,
10BEST-EFFORT ADAPTATION
thenH;h0(̂P;̂Q)is relatively small. Note that adaptation is in general not possible if the learner
receives vastly different labels on the source domain Qthan those corresponding to the target P.
-Lipschitz loss. Here, we give an upper bound on the labeled discrepancy for any -Lipschitz
loss. For any hypothesis h0∈H, we denote by H;h0(̂P;̂Q)theLipschitz loss labeled discrepancy
deﬁned by
H;h0(̂P;̂Q)=E
(x;y)∼̂P[/divides.alt0y−ho(x)/divides.alt0]+E
(x;y)∼̂Q[/divides.alt0y−ho(x)/divides.alt0]: (8)
Lemma 7 Let`be a loss function that is -Lipschitz with respect to its second argument. Then,
for any hypothesis h0inH, the following upper bound holds for the labeled discrepancy:
dis(̂P;̂Q)≤disH×(̂P;̂Q)+H;h0(̂P;̂Q):
The proof is given below in Appendix C.3.
The Lipschitz loss labeled discrepancy H;h0(̂P;̂Q)is a coarser quantity than H;h0(̂P;̂Q). In
particular, even when ̂P=̂Q,H;h0(̂P;̂Q)is not zero. However, as with H;h0(̂P;̂Q)it captures
the closeness of the output labels on ̂Pand̂Q. Whenh0can be chosen so that the sum of expected
values/divides.alt0y−h0(x)/divides.alt0is relatively small on both samples corresponding to ̂Pand̂Qthen,H;h0(̂P;̂Q)
is relatively small. As already pointed out, adaptation is not possible when the learner received very
different labels on the two domains.
The upper bounds of Lemmas 6 and 7 hold in the stochastic setting and are thus more general
than those derived for the deterministic label setting in previous work (Cortes and Mohri, 2014;
Cortes et al., 2019b). They are also ﬁner bounds expressed in terms of the more favorable local
discrepancy and somewhat more favorable label discrepancy terms deﬁned in terms of expectation
over the empirical distributions as opposed to a supremum.
In both the squared loss and Lipschitz cases, when a relatively small labeled sample S′drawn
i.i.d. from Pis available, we can use it to select h0via
h0=argmin
h0∈HH;h0(̂PS′;̂Q)orh0=argmin
h0∈HH;h0(̂PS′;̂Q):
When no labeled data from the target domain is at our disposal, we cannot choose h0by leveraging
any existing information. We can then assume that minh0∈HH;h0(̂P;̂Q)/uni226A1in the squared loss
case or minh0∈HH;h0(̂P;̂Q)/uni226A1in the Lipschitz case, that is that the source labels are relatively
close to the target ones based on these measures and use the standard unlabeled discrepancy:
dis(̂P;̂Q)≤dis(̂P;̂Q)+2 min
h0∈HH;h0(̂P;̂Q)
dis(̂P;̂Q)≤dis(̂P;̂Q)+min
h0∈HH;h0(̂P;̂Q):
6. Experimental evaluation
We evaluated our algorithms in best-effort adaptation, ﬁne-tuning, and (unsupervised) domain adap-
tation. We performed cross-validation using labeled data from the target to pick the hyperparameters
for our algorithms and the baselines. See Appendix D for details on data and experimental proce-
dures. For all the experiments we use the SBEST algorithm.
11AWASTHI CORTES MOHRI
Table 1: Performance of SBEST , compared to baseline approaches in CIFAR-10.
Fine-tuning Train on P gapBoost SBEST
Last layer (CIFAR-10) 88:61±:43 87:1±:0189:62±:32
Full model (CIFAR-10) 90:18±:31 90:8±:0292:30±:24
Last layer ( Civil ) 63:1±:12 64:7±:1165:8±:12
Full model ( Civil ) 65:8±:01 67:2±:0168:3±:14
6.1. Best-Effort adaptation
Here we have labeled data both from the source and the target. Two natural baselines are to train
solely on P, or solely Q. A third baseline is the -reweighted qas described in Appendix B.2.
200 400 600 800 1000
n0.700.750.800.850.900.951.00Test Accuracy=10%
train on P
train on Q
-reweighting
Alt. Min.
DC
Figure 1: Simulated data.Simulated data. The goal of this experiment was to demonstrate
that SBEST outperforms the simple baselines just mentioned and to
compare the performance of the Alternate Minimization ( SBEST -
AM) and the DC-programming ( SBEST -DC) optimization solutions.
We consider a linear binary classiﬁcation task with the labels for
Pgenerated as sgn (wp⋅x)for a randomly chosen unit vector wp. The
distribution Qadmits two parts. For ∈(0:5;1),(1−)mexamples
are labeled according to sgn (wq⋅x)where/parallel.alt1wp−wq/parallel.alt1≤, while the
remaining examples are set to a ﬁxed vector uand labeled +1. These
mexamples represent the noise in Qand, asincreases, dis(P;Q)
gets larger. For this setting, we evaluated the baselines and SBEST with the logistic loss and linear
hypotheses. See Appendix D for more details and examples.
Figure 1 shows the performance for =10% asnincreases. For small sizes, n, of the target
dataP, both-reweighting and the baseline that trains solely on Qare signiﬁcantly impacted. This
is because these methods cannot distinguish between non-noisy and noisy data points. On the other
hand, both SBEST -AM and SBEST -DC can counter the effect of the noise by generating q-weights
that are predominantly supported on the non-noisy samples. The performance of these algorithms is
fairly independent of the size of nas, for=10%, they can still make an effective use of 90% of the
m=1000 examples. As nincreases,-reweighting and the baseline that trains solely on Preach
the performance of SBEST . We also note that SBEST -AM and SBEST -DC perform equivalently and
in all the following experiments, we use SBEST -AM. For experiments with other values of and
further discussion of this experiment, see Appendix D.
6.2. Fine-tuning tasks
Here, we applied our algorithms to ﬁne-tuning pre-trained models in classiﬁcation. In the pre-
training/ﬁne-tuning paradigm (Raffel et al., 2019), a model is pre-trained on a generalist dataset
(coming from Q). The model is then ﬁne-tuned on a task-speciﬁc dataset (generated from P). Two
predominantly used ﬁne-tuning approaches are last-layer ﬁne-tuning (Subramanian et al., 2018;
Kiros et al., 2015) and full-model ﬁne-tuning (Howard and Ruder, 2018). In the former, the rep-
resentations obtained from the last layer of the pre-trained model are used to train a simple model
(often a linear hypothesis) on the data from P. We chose the simple model to be a multi-class lo-
12BEST-EFFORT ADAPTATION
gistic regression model. In the latter approach, the model is initialized from the pre-trained model
and all the parameters are ﬁne-tuned (often via gradient descent) on P. We explored the additional
advantages of combining data from both PandQduring ﬁne-tuning. There has been recent interest
in carefully combining various tasks/data for the purpose of ﬁne-tuning and avoid the phenomenon
of “negative transfer” (Aribandi et al., 2021). Our proposed theory presents a principled approach.
We used the CIFAR-10 vision dataset (Krizhevsky et al., 2009) and formed a pre-training task
(source) by combining data from classes: f’airplane’, ’automobile’, ’bird’, ’cat’, ’deer’, ’dog’ g. For
this task we use a standard ResNet-18 architecture (He et al., 2016). The ﬁne-tuning task (target)
consists of data from classes: f’frog’, ’horse’, ’ship’, ’truck’ g. In addition, we also used the Civil
Comments dataset. For this we used a BERT-small model (Devlin et al., 2018) for pre-training.
For more detail on the dataset and experimental procedure, see Appendix D. As can be seen from
Table 1, SBEST comfortably outperforms both the standard approach of training just on P, as well
as gapBoost.
7. Conclusion
We presented a comprehensive study of best-effort adaptation (or supervised adaptation), including
a new discrepancy-based theoretical analysis, algorithms beneﬁting from the corresponding learn-
ing guarantees, as well as a series of empirical results demonstrating the performance of these
algorithms in several tasks. We further showed how our analysis can be leveraged to derive learning
guarantees in domain adaptation, as well as new enhanced adaptation algorithms. Our analysis and
algorithms are likely to be useful in the study of other adaptation scenarios and admit a variety of
other applications. In fact, our analysis applies to any sample reweighting method.
Acknowledgments
We thank Jamie Morgenstern for several discussions about this work at Google Research.
13AWASTHI CORTES MOHRI
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal
Gupta. Muppet: Massive multi-task representations with pre-ﬁnetuning, 2021.
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta,
Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and
Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning, 2021.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-
based meta-learning. In Proceedings of ICML , volume 97, pages 424–433. PMLR, 2019.
Amir Beck. On the convergence of alternating minimization for convex programming with applica-
tions to iteratively reweighted least squares and decomposition schemes. SIAM J. Optim. , 25(1):
185–209, 2015.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Proceedings of NIPS , pages 137–144. MIT Press, 2006.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning , 79(1-2):151–175,
2010a.
Shai Ben-David, Tyler Lu, Teresa Luu, and D ´avid P ´al. Impossibility theorems for domain adapta-
tion. Journal of Machine Learning Research - Proceedings Track , 9:129–136, 2010b.
Christopher Berlind and Ruth Urner. Active nearest neighbors in changing environments. In Pro-
ceedings of ICML , volume 37, pages 1870–1879. JMLR.org, 2015.
Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classiﬁcation
tasks to a new unlabeled sample. In NIPS , pages 2178–2186, 2011.
John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classiﬁcation. In Proceedings of ACL , pages 440–
447, 2007.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. Learning
bounds for domain adaptation. In Proceedings of NIPS , pages 129–136, 2008.
Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Un-
supervised pixel-level domain adaptation with generative adversarial networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pages 3722–3731, 2017.
Rita Chattopadhyay, Wei Fan, Ian Davidson, Sethuraman Panchanathan, and Jieping Ye. Joint
transfer and batch-mode active learning. In Proceedings of ICML , volume 28, pages 253–261.
JMLR.org, 2013.
Minmin Chen, Kilian Q Weinberger, and John Blitzer. Co-training for domain adaptation. In Nips,
volume 24, pages 2456–2464. Citeseer, 2011.
Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non-
convex objectives. In Advances in Neural Information Processing Systems , pages 4705–4714,
2017.
14BEST-EFFORT ADAPTATION
Corinna Cortes and Mehryar Mohri. Domain adaptation in regression. In Proceedings of ALT , pages
308–323, 2011.
Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and
algorithm for regression. Theor. Comput. Sci. , 519:103–126, 2014.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
InProceedings of NIPS , pages 442–450. Curran Associates, Inc., 2010.
Corinna Cortes, Spencer Greenberg, and Mehryar Mohri. Relative deviation learning bounds and
generalization with unbounded loss functions. Ann. Math. Artif. Intell. , 85(1):45–70, 2019a.
Corinna Cortes, Mehryar Mohri, and Andr ´es Mu ˜noz Medina. Adaptation based on generalized
discrepancy. J. Mach. Learn. Res. , 20:1:1–1:30, 2019b.
Corinna Cortes, Mehryar Mohri, Ananda Theertha Suresh, and Ningshan Zhang. A discriminative
technique for multiple-source adaptation. In Marina Meila and Tong Zhang, editors, Proceedings
of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
Event , volume 139 of Proceedings of Machine Learning Research , pages 2132–2143. PMLR,
2021.
Nicolas Courty, R ´emi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for do-
main adaptation. IEEE transactions on pattern analysis and machine intelligence , 39(9):1853–
1865, 2016.
Nicolas Courty, R ´emi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for do-
main adaptation. IEEE Trans. Pattern Anal. Mach. Intell. , 39(9):1853–1865, 2017.
Koby Crammer, Michael J. Kearns, and Jennifer Wortman. Learning from multiple sources. Journal
of Machine Learning Research , 9(Aug):1757–1774, 2008.
Hal Daum ´e III. Frustratingly easy domain adaptation. ACL 2007 , page 256, 2007.
Antoine de Mathelin, Mathilde Mougeot, and Nicolas Vayatis. Discrepancy-based active learning
for domain adaptation. CoRR , abs/2103.03757, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Simon S. Du, Jayanth Koushik, Aarti Singh, and Barnab ´as P´oczos. Hypothesis transfer learning
via transformation functions. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neu-
ral Information Processing Systems 30: Annual Conference on Neural Information Processing
Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 574–584, 2017.
Lixin Duan, Ivor W. Tsang, Dong Xu, and Tat-Seng Chua. Domain adaptation from multiple sources
via auxiliary classiﬁers. In ICML , volume 382, pages 289–296, 2009.
Lixin Duan, Dong Xu, and Ivor Wai-Hung Tsang. Domain adaptation from multiple sources: A
domain-dependent regularization approach. IEEE Transactions on Neural Networks and Learn-
ing Systems , 23(3):504–518, 2012.
15AWASTHI CORTES MOHRI
Basura Fernando, Amaury Habrard, Marc Sebban, and Tinne Tuytelaars. Unsupervised visual do-
main adaptation using subspace alignment. In Proceedings of the IEEE international conference
on computer vision , pages 2960–2967, 2013.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th Interna-
tional Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 ,
volume 70 of Proceedings of Machine Learning Research , pages 1126–1135. PMLR, 2017.
Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source domain gen-
eralization. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 87–97, 2016.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research , 17(1):2096–2030, 2016.
Jochen Garcke and Thomas Vanck. Importance weighted inductive transfer learning for regression.
In Toon Calders, Floriana Esposito, Eyke H ¨ullermeier, and Rosa Meo, editors, Proceedings of
ECML , volume 8724 of Lecture Notes in Computer Science , pages 466–481. Springer, 2014.
Pascal Germain, Amaury Habrard, Franc ¸ois Laviolette, and Emilie Morvant. A PAC-bayesian ap-
proach for domain adaptation with specialization to linear classiﬁers. In Proceedings of the 30th
International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013 ,
volume 28 of JMLR Workshop and Conference Proceedings , pages 738–746. JMLR.org, 2013.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generaliza-
tion for object recognition with multi-task autoencoders. In Proceedings of the IEEE international
conference on computer vision , pages 2551–2559, 2015.
Muhammad Ghifary, David Balduzzi, W Bastiaan Kleijn, and Mengjie Zhang. Scatter component
analysis: A uniﬁed framework for domain adaptation and domain generalization. IEEE transac-
tions on pattern analysis and machine intelligence , 39(7):1414–1430, 2016a.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep
reconstruction-classiﬁcation networks for unsupervised domain adaptation. In European con-
ference on computer vision , pages 597–613. Springer, 2016b.
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic ﬂow kernel for unsupervised
domain adaptation. In CVPR , pages 2066–2073, 2012.
Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Discrimina-
tively learning domain-invariant features for unsupervised domain adaptation. In ICML , vol-
ume 28, pages 222–230, 2013a.
Boqing Gong, Kristen Grauman, and Fei Sha. Reshaping visual datasets for domain adaptation. In
NIPS , pages 1286–1294, 2013b.
16BEST-EFFORT ADAPTATION
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Sch¨olkopf. Domain adaptation with conditional transferable components. In Maria-Florina Bal-
can and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on
Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 , volume 48 of JMLR
Workshop and Conference Proceedings , pages 2839–2848. JMLR.org, 2016.
Luigi Grippo and Marco Sciandrone. On the convergence of the block nonlinear gauss-seidel
method under convex constraints. Oper. Res. Lett. , 26(3):127–136, 2000.
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris.
Spottune: Transfer learning through adaptive ﬁne-tuning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR) , June 2019.
Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. In Hanna M.
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ´e-Buc, Emily B. Fox, and Roman
Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada , pages 9867–9877, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
770–778, 2016.
Lukas Hedegaard, Omar Ali Sheikh-Omar, and Alexandros Iosiﬁdis. Supervised domain adapta-
tion: A graph embedding perspective and a rectiﬁed experimental protocol. IEEE Trans. Image
Process. , 30:8619–8631, 2021.
Judy Hoffman, Brian Kulis, Trevor Darrell, and Kate Saenko. Discovering latent domains for mul-
tisource domain adaptation. In ECCV , volume 7573, pages 702–715, 2012.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-source
adaptation. In Proceedings of NeurIPS , pages 8256–8266, 2018.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Multiple-source adaptation theory and algo-
rithms. Annals of Mathematics and Artiﬁcial Intelligence , 89(3-4):237–270, 2021.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Multiple-source adaptation theory and algo-
rithms - addendum. Annals of Mathematics and Artiﬁcial Intelligence , 90(6):569–572, 2022.
R Horst and Nguyen V Thoai. DC programming: overview. Journal of Optimization Theory and
Applications , 103(1):1–43, 1999.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for
NLP. CoRR , abs/1902.00751, 2019. URL http://arxiv.org/abs/1902.00751 .
Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation.
InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 328–339, Melbourne, Australia, July 2018. Association for
17AWASTHI CORTES MOHRI
Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https://aclanthology.
org/P18-1031 .
Jiayuan Huang, Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard
Sch¨olkopf. Correcting sample selection bias by unlabeled data. In NIPS 2006 , volume 19, pages
601–608, 2006.
Xingchang Huang, Yanghui Rao, Haoran Xie, Tak-Lam Wong, and Fu Lee Wang. Cross-domain
sentiment classiﬁcation via topic-related tradaboost. In Thirty-First AAAI Conference on Artiﬁcial
Intelligence , 2017.
I-Hong Jhuo, Dong Liu, DT Lee, and Shih-Fu Chang. Robust visual domain adaptation with low-
rank reconstruction. In 2012 IEEE conference on computer vision and pattern recognition , pages
2168–2175. IEEE, 2012.
Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba. Undo-
ing the damage of dataset bias. In ECCV , volume 7572, pages 158–171, 2012.
Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In Mario A.
Nascimento, M. Tamer ¨Ozsu, Donald Kossmann, Ren ´ee J. Miller, Jos ´e A. Blakeley, and K. Bern-
hard Schiefer, editors, (e)Proceedings of the Thirtieth International Conference on Very Large
Data Bases, VLDB 2004, Toronto, Canada, August 31 - September 3 2004 , pages 180–191. Mor-
gan Kaufmann, 2004.
Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems ,
pages 3294–3302, 2015.
Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor
Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel. Racial disparities in automated speech
recognition. Proc. Natl. Acad. Sci. USA , 117(14):7684–7689, 2020.
Nikola Konstantinov and Christoph Lampert. Robust learning from untrusted sources. In Interna-
tional Conference on Machine Learning , pages 3488–3498, 2019.
Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the beneﬁts of labels in
covariate-shift. In S ´ebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Conference
On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018 , volume 75 of Proceedings
of Machine Learning Research , pages 1882–1886. PMLR, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Toronto University, 2009.
Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. Universal source-free domain
adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition , pages 4544–4553, 2020.
Ilja Kuzborskij and Francesco Orabona. Stability and hypothesis transfer learning. In Proceed-
ings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA,
18BEST-EFFORT ADAPTATION
16-21 June 2013 , volume 28 of JMLR Workshop and Conference Proceedings , pages 942–950.
JMLR.org, 2013.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Isoperimetry and Processes .
Springer, New York, 1991.
Jingjing Li, Ke Lu, Zi Huang, Lei Zhu, and Heng Tao Shen. Transfer independently together: A
generalized framework for domain adaptation. IEEE transactions on cybernetics , 49(6):2144–
2155, 2018.
Qi Li. Literature survey: domain adaptation algorithms for natural language processing. Department
of Computer Science The Graduate Center, The City University of New York , pages 8–10, 2012.
Qiuwei Li, Zhihui Zhu, and Gongguo Tang. Alternating minimizations converge to second-order
optimal solutions. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA , volume 97 of Proceedings of Machine Learning Research , pages 3935–3943.
PMLR, 2019.
Hongfu Liu, Ming Shao, and Yun Fu. Structure-preserved multi-source domain adaptation. In 2016
IEEE 16th International Conference on Data Mining (ICDM) , pages 1059–1064. IEEE, 2016.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features
with deep adaptation networks. In Francis R. Bach and David M. Blei, editors, Proceedings of the
32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 ,
volume 37 of JMLR Workshop and Conference Proceedings , pages 97–105. JMLR.org, 2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation
with residual transfer networks. arXiv preprint arXiv:1602.04433 , 2016.
Nan Lu, Tianyi Zhang, Tongtong Fang, Takeshi Teshima, and Masashi Sugiyama. Rethinking
importance weighting for transfer learning. CoRR , abs/2112.10157, 2021. URL https://
arxiv.org/abs/2112.10157 .
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec,
Canada, June 18-21, 2009 , 2009a.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple
sources. In NIPS , pages 1041–1048, 2009b.
Yishay Mansour, Mehryar Mohri, Jae Ro, Ananda Theertha Suresh, and Ke Wu. A theory of
multiple-source adaptation with limited target labeled data. In Arindam Banerjee and Kenji
Fukumizu, editors, The 24th International Conference on Artiﬁcial Intelligence and Statistics,
AISTATS 2021, April 13-15, 2021, Virtual Event , volume 130 of Proceedings of Machine Learn-
ing Research , pages 2332–2340. PMLR, 2021.
Andreas Maurer. Bounds for linear multi-task learning. J. Mach. Learn. Res. , 7:117–139, 2006.
19AWASTHI CORTES MOHRI
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of multitask
representation learning. J. Mach. Learn. Res. , 17:81:1–81:32, 2016.
Mehryar Mohri and Andres Mu ˜noz Medina. New analysis and algorithm for learning with drift-
ing distributions. In Nader H. Bshouty, Gilles Stoltz, Nicolas Vayatis, and Thomas Zeugmann,
editors, Algorithmic Learning Theory - 23rd International Conference, ALT 2012, Lyon, France,
October 29-31, 2012. Proceedings , volume 7568 of Lecture Notes in Computer Science , pages
124–138. Springer, 2012.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning .
MIT Press, second edition, 2018.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Inter-
national Conference on Machine Learning , pages 4615–4625. PMLR, 2019.
Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial
domain adaptation. In Advances in Neural Information Processing Systems , pages 6670–6680,
2017a.
Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Uniﬁed deep super-
vised domain adaptation and generalization. In Proceedings of the IEEE International Conference
on Computer Vision , pages 5715–5725, 2017b.
Krikamol Muandet, David Balduzzi, and Bernhard Sch ¨olkopf. Domain generalization via invariant
feature representation. In ICML , volume 28, pages 10–18, 2013.
Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. CoRR ,
abs/1803.02999, 2018. URL http://arxiv.org/abs/1803.02999 .
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering , 22(10):1345–1359, 2009.
John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, and Ion Androutsopoulos. Toxi-
city detection: Does context really matter?, 2020.
F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning
Research , 12:2825–2830, 2011.
Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adap-
tation. In AAAI , pages 3934–3941, 2018.
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment match-
ing for multi-source domain adaptation. In Proceedings of the IEEE International Conference on
Computer Vision , pages 1406–1415, 2019.
Anastasia Pentina and Shai Ben-David. Multi-task Kernel Learning based on Probabilistic Lips-
chitzness. In Firdaus Janoos, Mehryar Mohri, and Karthik Sridharan, editors, Algorithmic Learn-
ing Theory, ALT 2018, 7-9 April 2018, Lanzarote, Canary Islands, Spain , volume 83 of Proceed-
ings of Machine Learning Research , pages 682–701. PMLR, 2018.
20BEST-EFFORT ADAPTATION
Anastasia Pentina and Christoph H. Lampert. A PAC-bayesian bound for lifelong learning. In
Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,
China, 21-26 June 2014 , volume 32 of JMLR Workshop and Conference Proceedings , pages
991–999. JMLR.org, 2014.
Anastasia Pentina and Christoph H. Lampert. Lifelong learning with non-i.i.d. tasks. In Corinna
Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Ad-
vances in Neural Information Processing Systems 28: Annual Conference on Neural Information
Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , pages 1540–1548,
2015.
Anastasia Pentina and Christoph H. Lampert. Multi-task learning with labeled and unlabeled tasks.
In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference
on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , volume 70 of
Proceedings of Machine Learning Research , pages 2807–2816. PMLR, 2017.
Anastasia Pentina and Ruth Urner. Lifelong learning with weighted majority votes. In Daniel D.
Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Ad-
vances in Neural Information Processing Systems 29: Annual Conference on Neural Information
Processing Systems 2016, December 5-10, 2016, Barcelona, Spain , pages 3612–3620, 2016.
Micha ¨el Perrot and Amaury Habrard. A theoretical analysis of metric hypothesis transfer learning.
In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference
on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , volume 37 of JMLR Workshop
and Conference Proceedings , pages 1708–1717. JMLR.org, 2015.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers) , pages 2227–2237, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL
https://aclanthology.org/N18-1202 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683 , 2019.
Ievgen Redko and Youn `es Bennani. Non-negative embedding for fully unsupervised domain adap-
tation. Pattern Recognit. Lett. , 77:35–41, 2016.
Ievgen Redko, Amaury Habrard, and Marc Sebban. Theoretical analysis of domain adaptation with
optimal transport. In Michelangelo Ceci, Jaakko Hollm ´en, Ljupco Todorovski, Celine Vens, and
Saso Dzeroski, editors, Machine Learning and Knowledge Discovery in Databases - European
Conference, ECML PKDD 2017, Skopje, Macedonia, September 18-22, 2017, Proceedings, Part
II, volume 10535 of Lecture Notes in Computer Science , pages 737–753. Springer, 2017.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classiﬁer dis-
crepancy for unsupervised domain adaptation. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition , pages 3723–3732, 2018.
21AWASTHI CORTES MOHRI
Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised
domain adaptation via minimax entropy. In Proceedings of the IEEE International Conference
on Computer Vision , pages 8050–8058, 2019.
Ozan Sener, Hyun Oh Song, Ashutosh Saxena, and Silvio Savarese. Learning transferrable repre-
sentations for unsupervised domain adaptation. In Advances in Neural Information Processing
Systems , pages 2110–2118, 2016.
Bharath K. Sriperumbudur, David A. Torres, and Gert R. G. Lanckriet. Sparse eigen methods by
D.C. programming. In ICML , pages 831–838, 2007.
Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. Learning general
purpose distributed sentence representations via large scale multi-task learning. arXiv preprint
arXiv:1804.00079 , 2018.
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M ¨uller. Covariate shift adaptation by
importance weighted cross validation. volume 8, pages 985–1005, 2007a.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul von B ¨unau, and Motoaki Kawanabe.
Direct importance estimation with model selection and its application to covariate shift adapta-
tion. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, Advances in
Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference
on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December
3-6, 2007 , pages 1433–1440. Curran Associates, Inc., 2007b.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European conference on computer vision , pages 443–450. Springer, 2016.
Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 30, 2016.
Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, and Jieping Ye. A two-stage weighting
framework for multi-source domain adaptation. In Advances in neural information processing
systems , pages 505–513, 2011.
Pham Dinh Tao and Le Thi Hoai An. Convex analysis approach to DC programming: theory,
algorithms and applications. Acta Mathematica Vietnamica , 22(1):289–355, 1997.
Pham Dinh Tao and Le Thi Hoai An. A DC optimization algorithm for solving the trust-region
subproblem. SIAM Journal on Optimization , 8(2):476–505, 1998.
Hoang Tuy. Concave programming under linear constraints. Translated Soviet Mathematics , 5:
1437–1440, 1964.
Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across
domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision ,
pages 4068–4076, 2015.
Boyu Wang, Jorge A. Mendez, Mingbo Cai, and Eric Eaton. Transfer learning via minimizing the
performance gap between domains. In Proceedingz of NeurIPS , pages 10644–10654, 2019a.
22BEST-EFFORT ADAPTATION
Chang Wang and Sridhar Mahadevan. Heterogeneous domain adaptation using manifold alignment.
InTwenty-second international joint conference on artiﬁcial intelligence , 2011.
Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S Yu. Visual domain
adaptation with manifold embedded distribution alignment. In Proceedings of the 26th ACM
international conference on Multimedia , pages 402–410, 2018.
Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing , 312:
135–153, 2018.
Tao Wang, Xiaopeng Zhang, Li Yuan, and Jiashi Feng. Few-shot adaptive faster r-cnn. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7173–7182,
2019b.
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V . Le. Finetuned language models are zero-shot learners, 2021.
Junfeng Wen, Russell Greiner, and Dale Schuurmans. Domain aggregation networks for multi-
source domain adaptation. In International Conference on Machine Learning , pages 10214–
10224. PMLR, 2020.
Jun Yang, Rong Yan, and Alexander G. Hauptmann. Cross-domain video concept detection using
adaptive svms. In ACM Multimedia , pages 188–197, 2007.
Liu Yang, Steve Hanneke, and Jaime G. Carbonell. A theory of transfer learning with applications
to active learning. Mach. Learn. , 90(2):161–189, 2013.
Kaichao You, Zhi Kou, Mingsheng Long, and Jianmin Wang. Co-tuning for transfer learning.
Advances in Neural Information Processing Systems , 33, 2020.
Alan L. Yuille and Anand Rangarajan. The concave-convex procedure. Neural Computation , 15(4):
915–936, 2003.
Kun Zhang, Bernhard Sch ¨olkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In Proceedings of the 30th International Conference on Machine
Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013 , volume 28 of JMLR Workshop and
Conference Proceedings , pages 819–827. JMLR.org, 2013.
Tianyi Zhang, Ikko Yamane, Nan Lu, and Masashi Sugiyama. A one-step approach to covariate
shift adaptation. In Proceedings of ACML , volume 129 of Proceedings of Machine Learning
Research , pages 65–80. PMLR, 2020a.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm
for domain adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings
of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine
Learning Research , pages 7404–7413. PMLR, 09–15 Jun 2019a.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm
for domain adaptation. In International Conference on Machine Learning , pages 7404–7413.
PMLR, 2019b.
23AWASTHI CORTES MOHRI
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I. Jordan. Bridging theory and algorithm
for domain adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA , volume 97 of Proceedings of Machine Learning Research , pages 7404–
7413. PMLR, 2019c.
Yuchen Zhang, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. On localized discrepancy
for domain adaptation. CoRR , abs/2008.06242, 2020b.
Han Zhao, Shanghang Zhang, Guanhang Wu, Jos ´e MF Moura, Joao P Costeira, and Geoffrey J Gor-
don. Adversarial multiple source domain adaptation. Advances in neural information processing
systems , 31:8559–8570, 2018.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning , pages
7523–7532. PMLR, 2019.
Lutao Zheng, Guanjun Liu, Chungang Yan, Changjun Jiang, Mengchu Zhou, and Maozhen Li.
Improved tradaboost and its application to transaction fraud detection. IEEE Transactions on
Computational Social Systems , 7(5):1304–1316, 2020.
24BEST-EFFORT ADAPTATION
Contents of Appendix
A Related work 26
A.1 Adaptation and transfer learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.2 Relationship with ﬁne-tuning methods . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B Best-effort adaptation 30
B.1 Theorems and proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.2 Discussion of learning bound of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . 34
B.3 Discrepancy estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
B.4 Pseudocode of alternate minimization procedure . . . . . . . . . . . . . . . . . . . . 36
C Domain adaptation 37
C.1 Theorems and proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
C.2 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
C.3 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
C.4 Sub-Gradients and estimation of unlabeled discrepancy terms . . . . . . . . . . . . . 39
C.4.1 Sub-Gradients of unlabeled weighted discrepancy terms . . . . . . . . . . . 39
C.4.2 Estimation of unlabeled discrepancy terms . . . . . . . . . . . . . . . . . . . 40
D Further details about experimental settings 41
D.1 Best-Effort adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
D.1.1 Simulated data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
D.2 Fine-tuning tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
D.3 Domain adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
D.3.1 Domain adaptation – covariate-shift . . . . . . . . . . . . . . . . . . . . . . . 44
25AWASTHI CORTES MOHRI
Appendix A. Related work
A.1. Adaptation and transfer learning
Discrepancy-based adaptation theory. The work we present includes a signiﬁcant theoretical
component and beneﬁts from prior theoretical analyses of domain adaptation. The theoretical anal-
ysis of domain adaptation was initiated by Kifer et al. (2004) and Ben-David et al. (2006) with the
introduction of a dA-distance between distributions. They used this notion to derive VC-dimension
learning bounds for the zero-one loss, which was elaborated on in follow-up publications like
(Blitzer et al., 2008; Ben-David et al., 2010a). Later, Mansour et al. (2009a) and Cortes and Mohri
(2011, 2014) presented a general analysis of single-source adaptation for arbitrary loss functions,
where they introduced the notion of discrepancy , which they argued is a divergence measure tailored
to domain adaptation. The notion of discrepancy coincides with the dA-distance in the special case
of the zero-one loss. It takes into account the loss function and the hypothesis set and, importantly,
can be estimated from ﬁnite samples. The authors further gave Rademacher complexity learning
bounds in terms of the discrepancy for arbitrary hypothesis sets and loss functions, as well as point-
wise learning bounds for kernel-based hypothesis sets. They also gave a discrepancy minimization
algorithm based on a reweighting of the losses of sample points. We use their notion of discrepancy
in our new analysis. Cortes et al. (2019b) presented an extension of the discrepancy minimiza-
tion algorithm based on the so-called generalized discrepancy , which allows for the weights to be
hypothesis-dependent and which works with a less conservative notion of local discrepancy deﬁned
by a supremum over a subset of the hypothesis set. The notion of local discrepancy has been since
adopted in several recent publications, in the study of active learning or adaptation (de Mathelin
et al., 2021; Zhang et al., 2019c, 2020b) and is also used in part of our analysis. Finally, a PAC-
Bayesian analysis of adaptation has also been given by Germain et al. (2013), using a related notion
of discrepancy. Note also that, as argued in Appendix B.3, for our analysis of best-effort adaptation
and algorithms, we can restrict ourselves to a small ball B(hP;r)around the best hypothesis found
by training on P, withrin the order of 1/slash.left√n. This leads to a more favorable discrepancy term,
which is similar to the super transfer orlocalization beneﬁts mentioned by Hanneke and Kpotufe
(2019). This advantage can be leveraged when there is a sufﬁcient amount of labeled data from
the target distribution, as in the scenario of best-effort adaptation. In standard domain adaptation,
however, it would not be possible to estimate such local discrepancy quantities, which are also used
in the analysis of Zhang et al. (2020b), and thus the corresponding learning bounds or notions would
be not be algorithmically useful.
A theoretical analysis and algorithm for driting distributions are given by Mohri and Mu ˜noz Med-
ina (2012). The assumptions made in the analysis of adaptation were discussed by Ben-David et al.
(2010b) who presented several negative results for the zero-one loss.
Many of the theoretical guarantees for domain adaptation (Ben-David et al., 2006; Ben-David
et al., 2010a; Zhang et al., 2019a) have upper bounds that include the term H=minh∈H{L(P;h)+L(Q;h)},
which, as pointed out by Mansour et al. (2009a), roughly doubles the representation error one incurs
forHand results overall in learning bounds with a factor of 3of the error with the respect to an
ideal target. This can make these bounds vacuous in some natural scenarios. Moreover, the H
terms cannot be estimated from observations. The learning bounds of Mansour et al. (2009a) do
not admit the factor of 3of the error drawback, but they also contain terms depending on the best-
in-class predictors with respect to both distributions that cannot be estimated. In general, they are
not comparable with the bounds of Ben-David et al. (2006). Our learning bounds differ from these
26BEST-EFFORT ADAPTATION
analyses since we compare the target loss of a predictor with an empirical q-weighted empirical loss
on a sample from Qor both QandPand not just with an unweighted loss for a sample drawn from
Q. Furthermore, our learning guarantees are high-probability bounds, while those of these previ-
ous work hold with probability one. The latter can be derived from straightforward applications of
triangle inequality. Crucially, our learning bounds can be leveraged by algorithms, while previous
bounds do not include any non-trivial term that can be optimized.
Multiple-source adaptation theory. Mansour et al. (2021) presented a theory of multiple-
source adaptation with limited target labeled data using the notion of discrepancy. A series of
publications by Mansour et al. (2009a,b), Hoffman et al. (2018, 2021, 2022) and Cortes et al. (2021)
give an extensive theoretical and algorithmic analysis of the problem of multiple-source adaptation
(MSA) scenario where the learner has access to unlabeled samples and a trained predictor for each
source domain, with no access to source labeled data. This approach has been further used in
many applications such as object recognition (Hoffman et al., 2012; Gong et al., 2013a,b). Zhao
et al. (2018) and Wen et al. (2020) considered MSA with only unlabeled target data available and
provided generalization bounds for classiﬁcation and regression.
Other adaptation analyses. There are alternative analyses of the adaptation problem based
on divergences between distributions that do not take into account the speciﬁc loss function or
hypothesis set used. These include methods based on importance weighting (Sugiyama et al., 2007b;
Zhang et al., 2020a; Lu et al., 2021; Sugiyama et al., 2007a). Cortes et al. (2010) gave a theoretical
analysis of importance weighting, including learning bounds based on the analysis of unbounded
loss functions (see also (Cortes et al., 2019a)), showing both theoretically and empirically that
importance weighting can fail in a number of cases, depending on the magnitude of the second-
moment of the weights, including in simple cases of the two domain being Gaussian distributions.
This holds even for perfectly estimated importance weights. The publications in this category also
include those using the Wasserstein distance (Courty et al., 2017; Redko et al., 2017), which in
some sense is closer to the notion of discrepancy but yet does not capture the hypothesis set used.
An alternative distance used is that of Kernel Mean Matching (KMM), which is the difference
between the expectation of the feature vector in the source domain and the target domain (Huang
et al., 2006). Several other publications have also adopted also that distance (Long et al., 2015;
Redko and Bennani, 2016). The KMM algorithm seeks to reweight the source sample to make this
difference as small as possible. This, however, ignores other moments of the distributions, as well
as the loss function and the hypothesis sets. Nevertheless, in some instances, the distance is close to
and somewhat related to discrepancy. The experiments reported by Cortes and Mohri (2014) suggest
that, while in some instances KMM performs well, in some others it does not. This variance might
be due to the fact that the distance does not always capture key aspects related to the loss function
and the hypothesis set. In other experiments reported by Cortes et al. (2019b), the performance of
KMM is sometimes worse than training on the sample Sdrawn from Q(without reweighting). This
problem was already reported for another algorithm, KLIEP, by Sugiyama et al. (2007b). Variants
of boosting designed for transfer also tacitly reweight examples (Huang et al., 2017; Zheng et al.,
2020).
Note that the algorithms suggested for KMM, importance-weighting, KLIEP and other similar
methods can all be viewed as speciﬁc methods for reweighting the sample losses. In that sense,
they are all covered by our general analysis, when the weights are bounded. However, note also that
they are all two-stage algorithms: the weights are ﬁrst chosen to reduce or minimize some distance,
27AWASTHI CORTES MOHRI
irrespective of their effect on the weighted empirical loss, and next the weights are ﬁxed and used
to minimize the empirical weighted loss.
An interesting non-parametric analysis of adaptation is presented in (Kpotufe and Martinet,
2018; Hanneke and Kpotufe, 2019). Hanneke and Kpotufe (2019) do not give an adaptation algo-
rithm, however. A causal view of adaptation is also analyzed in (Zhang et al., 2013; Gong et al.,
2016).
Transfer learning analyses. Other scenarios of transfer learning have been studied by Kuzborskij
and Orabona (2013); Perrot and Habrard (2015); Du et al. (2017) including by leveraging smaller
target labeled data and auxiliary hypotheses (see also (Hanneke and Kpotufe, 2019) already men-
tioned). The problem of active adaptation or transfer learning has been investigated by several
publications Yang et al. (2013); Chattopadhyay et al. (2013); Berlind and Urner (2015). Another
somewhat related problem is that of multi-task learning studied by Maurer (2006); Maurer et al.
(2016); Pentina and Lampert (2017); Pentina and Ben-David (2018). The scenario of life-long
learning is also somewhat related (Pentina and Lampert, 2014, 2015; Pentina and Urner, 2016; Bal-
can et al., 2019).
Other adaptation or transfer learning publications. The space of transfer learning and do-
main adaptation approaches is massive (Chen et al., 2011; Zhang et al., 2019b; Wang and Mahade-
van, 2011; Sener et al., 2016; Hoffman et al., 2012; Ghifary et al., 2016b; Zhao et al., 2019, 2018;
Li et al., 2018; Bousmalis et al., 2017; Sun et al., 2016; Kundu et al., 2020; Sun and Saenko, 2016;
Ghifary et al., 2016a; Long et al., 2016; Courty et al., 2016; Saito et al., 2018; Wang et al., 2018;
Motiian et al., 2017a; Sun and Saenko, 2016) and includes interesting analyses and observations
such as that of Daum ´e III (2007) about a surprisingly good baseline and follow-up by Sun et al.
(2016). We recommend readers to surveys such as Pan and Yang (2009); Wang and Deng (2018);
Li (2012) for a comprehensive overview. We brieﬂy outline the most relevant approaches here.
There is a very large recent literature dealing with experimental studies of domain adaptation
in various tasks. Ganin et al. (2016) proposed to learn features that cannot discriminate between
source and target domains. Tzeng et al. (2015) proposed a CNN architecture to exploit unlabeled
and sparsely labeled target domain data. Motiian et al. (2017b), Motiian et al. (2017a) and Wang
et al. (2019b) proposed to train maximally separated features via adversarial learning. Saito et al.
(2019) proposed to use a minmax entropy method for domain adaptation.
Several algorithms have been proposed for multiple-source adaptation. Khosla et al. (2012);
Blanchard et al. (2011) proposed to combine all the source data and train a single model. Duan et al.
(2009, 2012) used unlabeled target data to obtain a regularizer. Domain adaptation via adversarial
learning was studied by Pei et al. (2018); Zhao et al. (2018). Crammer et al. (2008) considered
learning models for each source domain, using close-by data of other domains. Gong et al. (2012)
ranked multiple source domains by how well they can adapt to a target domain. Other solutions to
multiple-source domain adaptation include, clustering (Liu et al., 2016), learning domain-invariant
features (Gong et al., 2013a), learning intermediate representations (Jhuo et al., 2012), subspace
alignment techniques (Fernando et al., 2013), attributes detection (Gan et al., 2016), using a linear
combination of pre-trained classiﬁers (Yang et al., 2007), using multitask auto-encoders (Ghifary
et al., 2015), causal approaches (Sun et al., 2011), two-state weighting approaches (Sun et al.,
2011), moments alignment techniques (Peng et al., 2019) and domain-invariant component analysis
(Muandet et al., 2013).
When some labeled data from both source and target are available, a variety of practical methods
have been studied. Daum ´e III (2007) performs an empirical comparison amongst a collection of
28BEST-EFFORT ADAPTATION
basic models when some labeled data is available from both source and target: source-only, target-
only, training on all data together, uniformly -weighting the source data and (1−)-weighting
the target data, using the prediction of a model on the source as a feature for training on the target,
linearly interpolating between source-only and target-only models, and a “lifted” approach where
each sample is projected into X3, corresponding to source/target/general information copies of the
feature space, and show empirically that each of these benchmarks performs fairly well, with the
latter outperforming the others most of the time.
Some recent work focuses on adversarial adaptation (Motiian et al., 2017a; Pei et al., 2018;
Ganin et al., 2016). The problem of domain generalization , that is generalization to an arbitrary
target distribution within some set has been studied by (Mohri et al., 2019) and is also related to that
of robust learning (Chen et al., 2017; Konstantinov and Lampert, 2019; Jhuo et al., 2012).
We discuss separately, in the following section, the relationship of our work with ﬁne-tuning
methods.
A.2. Relationship with ﬁne-tuning methods
Here, we discuss the connection of our work with ﬁne-tuning (Howard and Ruder, 2018; Peters
et al., 2018; Houlsby et al., 2019) of pre-trained models. A comprehensive description of ﬁne-tuning
methods is beyond the scope of this work, but see (Guo et al., 2019; You et al., 2020; Aribandi et al.,
2021; Aghajanyan et al., 2021; Wei et al., 2021) for some recent results. A related area is few shot-
learning algorithms and related meta-learning algorithms such as MAML (Finn et al., 2017) include
(Wang et al., 2019b; Motiian et al., 2017a), and Reptile (Nichol et al., 2018).
In general, consider a scenario where there exists good common feature mapping ∶X→Rd
for both the QandP. Letfbe the result of pre-training a neural network on Qdata. The mapping
infcorresponding to some depth of the hidden layers can then be viewed as a good approximation
of. Alternatively, may be the output of a representation learning algorithm.
There are several ﬁne-tuning methods introduced in the literature (Subramanian et al., 2018;
Kiros et al., 2015; Howard and Ruder, 2018; Raffel et al., 2019) that consists of adapting fto domain
P. This may be by using fas an initialization point and applying SGD with sample S′drawn from
P, while ﬁxing the hidden layer parameters to a given depth. It may be by forgetting the weights at
the top layer(s) and retraining them by using S′alone. Or, it may be done by continuing training
with a mixture of S′and a new sample from S. Training on such a mixture avoids ‘catastrophic
forgetting’. In all cases, the problem can be cast as that of learning a hypothesis with feature vector
by using sample SandS′, or sampleS′alone, which is a special case of the scenario we analyzed
in Section 3. The algorithms presented in Section 4 provide a principled solution to this problem by
taking into consideration the discrepancy between QandPand by selecting suitable q-weights to
guarantee a better generalization.
29AWASTHI CORTES MOHRI
Appendix B. Best-effort adaptation
B.1. Theorems and proofs
Below we will work with a generalized notion of discrepancy as deﬁned in (9). Given distributions
P;Qand positive real numbers a;bwe deﬁne the weighted discrepancy as
dis(aP;bQ)=sup
h∈HE
(x;y)∼P[a⋅`(h(x);y)]−E
(x;y)∼Q[b⋅`(h(x);y)]: (9)
Theorem 1 Fix a vector qin[0;1][m+n]. Then, for any >0, with probability at least 1−over
the choice of a sample Sof sizemfromQand a sample S′of sizenfromP, the following holds for
allh∈H:
L(P;h)≤m+n
/summation.disp
i=1qi`(h(xi);yi)+dis/parenleft.alt2/bracketleft.alt1(1−/parallel.alt1q/parallel.alt11)+q/bracketright.altP;qQ/parenright.alt2+2Rq(`○H)+/parallel.alt1q/parallel.alt12/roottop
/rootmod/rootmod/rootbotlog1

2:
Proof LetS=((x1;y1);:::;(xm;ym))be a sample of size mdrawn i.i.d. from Qand similarly
S′=((xm+1;ym+1);:::;(xm+n;ym+n))a sample of size ndrawn i.i.d. from P. LetTdenote
the sample formed by SandS′,T=(S;S′). For any such sample T, let(T)denote (T)=
suph∈HL(qQ+(/parallel.alt1q/parallel.alt11−q)P;h)−L(q;h), withL(q;h)=∑m+n
i=1qi`(h(xi);yi). Changing point xi
to some other point x′
iaffects (T)by at most qi. Thus, by McDiarmid’s inequality, for any >0,
with probability at least 1−, the following holds for all h∈H:
L(qQ+(/parallel.alt1q/parallel.alt11−q)P;h)≤L(q;h)+E[(T)]+/parallel.alt1q/parallel.alt12/roottop
/rootmod/rootmod/rootbotlog1

2: (10)
We now analyze the expectation term:
E[(T)]=E
T/bracketleft.alt4sup
h∈HL(qQ+(/parallel.alt1q/parallel.alt11−q)P;h)−LT(q;h)/bracketright.alt4
=E
T/bracketleft.alt4sup
h∈HE
T′[LT′(q;h)−LT(q;h)]/bracketright.alt4
≤E
T;T′/bracketleft.alt4sup
h∈HLT′(q;h)−LT(q;h)/bracketright.alt4
=E
T;T′/bracketleft.alt4sup
h∈Hm+n
/summation.disp
i=1qi`(h(x′
i);y′
i)−qi`(h(xi);yi)/bracketright.alt4
=E
T;T′;/bracketleft.alt4sup
h∈Hm+n
/summation.disp
i=1i/parenleft.alt1qi`(h(x′
i);y′
i)−qi`(h(xi);yi)/parenright.alt1/bracketright.alt4
≤2E
T;/bracketleft.alt4sup
h∈Hm+n
/summation.disp
i=1iqi`(h(xi);yi)/bracketright.alt4=2Rq(`○H):
We make a remark about the validity of the second equality in the above derivation. Let T′be a
sample that has the same distribution as T. Furthermore we will use (x′
i;y′
i)to denote the ith sample
30BEST-EFFORT ADAPTATION
inT′. Then notice that
E
T′[LT′(q;h)]=m
/summation.disp
i=1qiE[`(h(x′
i);yi)]+m+n
/summation.disp
i=m+1qiE[`(h(x′
i);yi)] (11)
=m
/summation.disp
i=1qiL(Q;h)+m+n
/summation.disp
i=m+1qiL(P;h) (12)
=qL(Q;h)+(/parallel.alt1q/parallel.alt11−q)L(P;h) (13)
=L(qQ+(/parallel.alt1q/parallel.alt11−q)P;h) (14)
Finally, using the upper bound L(P;h)−L(qQ+(/parallel.alt1q/parallel.alt11−q)P;h)≤dis(P;qQ+(/parallel.alt1q/parallel.alt11−q)P)=
dis([(1−/parallel.alt1q/parallel.alt11)+q]P;qQ)completes the proof.
Next, we show that the bound above is tight in terms of the weighted-discrepancy term.
Theorem 2 Fix a distribution qinm+n. Then, for any >0, there exists h∈Hsuch that, for any
>0, the following lower bound holds with probability at least 1−over the choice of a sample S
of sizemfromQand a sample S′of sizenfromP:
L(P;h)≥m+n
/summation.disp
i=1qi`(h(xi);yi)+qdis(P;Q)−2Rq(`○H)−/parallel.alt1q/parallel.alt12/roottop
/rootmod/rootmod/rootbotlog1

2−:
In particular, for /parallel.alt1q/parallel.alt12;Rq(`○H)∈O(1√m+n), we have:
L(P;h)≥m+n
/summation.disp
i=1qi`(h(xi);yi)+qdis(P;Q)+
/parenleft.alt41√m+n/parenright.alt4:
Proof LetL(q;h)denote∑m+n
i=1qi`(h(xi);yi). By deﬁnition of discrepancy as a supremum, for
any>0, there exists h∈Hsuch that L(P;h)−L(Q;h)≥dis(P;Q)−. For thath, we have
L(P;h)−qdis(P;Q)−L(q;h)≥L(P;h)−q(L(P;h)−L(Q;h))−L(q;h)−
=(1−q)L(P;h)+qL(Q;h)−L(q;h)−
=E[L(q;h)]−L(q;h)−:
By McDiarmid’s inequality, with probability at least 1−, we have E[L(q;h)]−L(q;h)≥−2Rq(`○
H)−/parallel.alt1q/parallel.alt12/radical.alt3
log1

2. Thus, we have:
L(P;h)−qdis(P;Q)−L(q;h)≥−2Rq(`○H)−/parallel.alt1q/parallel.alt12/roottop
/rootmod/rootmod/rootbotlog1

2−:
The last inequality follows directly by using the assumptions and Lemma 9.
Theorem 3 For any>0, with probability at least 1−over the choice of a sample Sof sizemfrom
Qand a sample S′of sizenfromP, the following holds for all h∈Handq∈/braceleft.alt1q∶0≤/parallel.alt1q−p0/parallel.alt11<1/braceright.alt1:
L(P;h)≤m+n
/summation.disp
i=1qi`(h(xi);yi)+dis/parenleft.alt2/bracketleft.alt1(1−/parallel.alt1q/parallel.alt11)+q/bracketright.altP;qQ/parenright.alt2+dis(q;p0)
+2Rq(`○H)+5/parallel.alt1q−p0/parallel.alt11+/bracketleft.alt1/parallel.alt1q/parallel.alt12+2/parallel.alt1q−p0/parallel.alt11/bracketright.alt/uni23A1/uni23A2/uni23A2/uni23A2/uni23A3/radical.alt2
log log22
1−/parallel.alt1q−p0/parallel.alt11+/radical.alt3
log2

2/uni23A4/uni23A5/uni23A5/uni23A5/uni23A6:
31AWASTHI CORTES MOHRI
Proof Consider two sequences (k)k≥0and(qk)k≥0. By Theorem 1, for any ﬁxed k≥0, we have:
P/uni23A1/uni23A2/uni23A2/uni23A2/uni23A2/uni23A3L(P;h)>m+n
/summation.disp
i=1qk
i`(h(xi);yi)+dis/parenleft.alt1/bracketleft.alt1(1−/parallel.alt1qk/parallel.alt11)+qk/bracketright.altP;qkQ/parenright.alt1
+2Rqk(`○H)+/parallel.alt1qk/parallel.alt12√
2k/uni23A4/uni23A5/uni23A5/uni23A5/uni23A5/uni23A6≤e−2
k:
Choosek=+/radical.alt1
2 log(k+1). Then, by the union bound, we can write:
P/uni23A1/uni23A2/uni23A2/uni23A2/uni23A2/uni23A3∃k≥1∶L(P;h)>m+n
/summation.disp
i=1qk
i`(h(xi);yi)+dis/parenleft.alt1/bracketleft.alt1(1−/parallel.alt1qk/parallel.alt11)+qk/bracketright.altP;qkQ/parenright.alt1 (15)
+2Rqk(`○H)+/parallel.alt1qk/parallel.alt12√
2k/uni23A4/uni23A5/uni23A5/uni23A5/uni23A5/uni23A6
≤+∞
/summation.disp
k=0e−2
k≤+∞
/summation.disp
k=0e−2−log((k+1)2)=e−2+∞
/summation.disp
k=11
k2=2
6e−2≤2e−2:
We can choose qksuch that /parallel.alt1qk−p0/parallel.alt11=1−1
2k. Then, for any q∈/braceleft.alt1q∶0≤/parallel.alt1q−p0/parallel.alt11<1/braceright.alt1, there
existsk≥0such that /parallel.alt1qk−p0/parallel.alt11≤/parallel.alt1q−p0/parallel.alt11</parallel.alt1qk+1−p0/parallel.alt11and thus such that
/radical.alt1
2 log(k+1)=/radical.alt4
2 log log21
1−/parallel.alt1qk+1−p0/parallel.alt11=/radical.alt4
2 log log22
1−/parallel.alt1qk−p0/parallel.alt11
≤/radical.alt4
2 log log22
1−/parallel.alt1q−p0/parallel.alt11:
Furthermore, for that k, the following inequalities hold:
m+n
/summation.disp
i=1qk
i`(h(xi);yi)≤m+n
/summation.disp
i=1qi`(h(xi);yi)+dis(qk;q)
≤m+n
/summation.disp
i=1qi`(h(xi);yi)+dis(qk;p0)+dis(p0;q)
≤m+n
/summation.disp
i=1qi`(h(xi);yi)+/parallel.alt1qk−p0/parallel.alt11+dis(q;p0)
≤m+n
/summation.disp
i=1qi`(h(xi);yi)+/parallel.alt1q−p0/parallel.alt11+dis(q;p0);
dis/parenleft.alt1/bracketleft.alt1(1−/parallel.alt1qk/parallel.alt11)+qk/bracketright.altP;qkQ/parenright.alt1≤dis([(1−/parallel.alt1q/parallel.alt11)+q]P;qQ)
+/parallel.alt2/bracketleft.alt1(/parallel.alt1q/parallel.alt11−q)−(/parallel.alt1qk/parallel.alt11−qk)/bracketright.altP+/bracketleft.alt1q−qk/bracketright.altQ/parallel.alt21
≤dis([(1−/parallel.alt1q/parallel.alt11)+q]P;qQ)+/parallel.alt1qk−q/parallel.alt11
≤dis([(1−/parallel.alt1q/parallel.alt11)+q]P;qQ)+2/parallel.alt1q−p0/parallel.alt11;
Rqk(`○H)≤Rq(`○H)+/parallel.alt1qk−q/parallel.alt11≤Rq(`○H)+2/parallel.alt1q−p0/parallel.alt11;
and /parallel.alt1qk/parallel.alt12≤/parallel.alt1q/parallel.alt12+/parallel.alt1qk−q/parallel.alt12
≤/parallel.alt1q/parallel.alt12+/parallel.alt1qk−q/parallel.alt11≤/parallel.alt1q/parallel.alt12+2/parallel.alt1q−p0/parallel.alt11:
32BEST-EFFORT ADAPTATION
Plugging in these inequalities in (15) concludes the proof.
Corollary 8 For any>0, with probability at least 1−over the choice of a sample Sof
sizemfromQand a sample S′of sizenfromP, the following holds for all h∈Handq∈
/braceleft.alt1q∶0≤/parallel.alt1q−p0/parallel.alt11<1/braceright.alt1:
L(P;h)≤m+n
/summation.disp
i=1qi`(h(xi);yi)+qdis(P;Q)+dis(q;p0)+2Rq(`○H)
+6/parallel.alt1q−p0/parallel.alt11+/bracketleft.alt1/parallel.alt1q/parallel.alt12+2/parallel.alt1q−p0/parallel.alt11/bracketright.alt/uni23A1/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A3/radical.alt2
log log22
1−/parallel.alt1q−p0/parallel.alt11+/roottop
/rootmod/rootmod/rootbotlog2

2/uni23A4/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A6:
Proof Note that the discrepancy term of the bound of Theorem 3 can be further upper bounded as
follows:
dis([(1−/parallel.alt1q/parallel.alt11)+q]P;qQ)
=sup
h∈H/braceleft.alt4[(1−/parallel.alt1q/parallel.alt11)+q]E
(x;y)∼P[`(h(x);y)]−qE
(x;y)∼Q[`(h(x);y)]/braceright.alt4
≤qdis(P;Q)+/divides.alt11−/parallel.alt1q/parallel.alt11/divides.alt1sup
h∈HE
(x;y)∼P[`(h(x);y)]
≤qdis(P;Q)+/divides.alt11−/parallel.alt1q/parallel.alt11/divides.alt1
=qdis(P;Q)+/divides.alt1/parallel.alt1p0/parallel.alt11−/parallel.alt1q/parallel.alt11/divides.alt1
≤qdis(P;Q)+/parallel.alt1p0−q/parallel.alt11:
Plugging this in the right-hand side in the bound of Theorem 3 completes the proof.
Lemma 9 Fix a distribution qover[m+n]. Then, the following holds for the q-weighted Rademacher
complexity:
Rq(`○H)≤/parallel.alt1q/parallel.alt1∞(m+n)Rm+n(`○H):
Proof Since for any i∈[m+n], the function 'i∶x/uni21A6qixisqi-Lipschitz and thus /parallel.alt1q/parallel.alt1∞-Lipschitz,
the result is an application of Talagrand’s inequality (Ledoux and Talagrand, 1991).
Note that the bound of the lemma is tight: equality holds when qis chosen to be the uniform
distribution. By McDiarmid’s inequality, the q-weighted Rademacher complexity can be estimated
from the empirical quantity
̂Rq;S;S′(`○H)=E
/bracketleft.alt4sup
h∈Hm+n
/summation.disp
i=1iqi`(h(xi);yi)/bracketright.alt4;
modulo a term in O(/parallel.alt1q/parallel.alt12).
33AWASTHI CORTES MOHRI
B.2. Discussion of learning bound of Theorem 1
It is instructive to examine some special cases for the choice of q, which will demonstrate how
our guarantees can recover several previous bounds as a special case. Since our algorithms seek to
choose the best weight (and best hypothesis) based on these bounds, this shows that their search
space includes that of algorithms based on those previous bounds.
qchosen uniformly on S.Forqchosen to be the uniform distribution on S, we have q=1,
/parallel.alt1q/parallel.alt12=1√m, and the bound coincides with the labeled discrepancy-based bound for Pof Cortes
et al. (2019b)[Prop. 5; Eq. (9)]. Indeed, for qchosen to be supported only on S, the theorem gives
aq-discrepancy domain adaptation bound from QtoP, in terms of a q-Rademacher complexity and
/parallel.alt1q/parallel.alt12.
qchosen uniformly on S′.Here q=0,/parallel.alt1q/parallel.alt12=1√n, and the bound coincides with the standard
Rademacher complexity bound for Pfor learning from a labeled sample of size n:
L(P;h)≤1
nm+n
/summation.disp
i=m+1`(h(xi);yi)+2Rn(`○H)+/roottop
/rootmod/rootmod/rootbotlog1

2n: (16)
Here,Rn(`○H)is the standard Rademacher complexity deﬁned as in (4) where the expectation is
overS′andqis the uniform distribution over S′. Thus, for qminimizing the right-hand side of the
bound of the theorem, the learning bound is at least as favorable as one restricted to learning from
the labeled points from P. But the bound also demonstrates that it is possible to do better than just
learning from P. In fact, for Q=P, we have dis(P;Q)=0, and qcan be chosen to be uniform
overT=(S;S′), thus/parallel.alt1q/parallel.alt12=1√m+n. The bound then coincides with the standard Rademacher
complexity bound for a sample of size m+nfor the distribution P. More generally, such a bound
holds for any two distributions PandQwithdis(P;Q)=0.
The learning bound (16) can be straightforwardly upper-bounded by the weighted discrepancy
bound of Cortes et al. (2019b)[Prop. 5; Eq. (10)], for any pwith support S:
L(P;h)≤m
/summation.disp
i=1pi`(h(xi);yi)+dis(̂P;p)+2Rn(`○H)+/bracketleft.alt4log1

2n/bracketright.alt41
2
; (17)
using the inequality
L(̂P;h)≤m
/summation.disp
i=1pi`(h(xi);yi)+dis(̂P;p);
which holds for any p, by deﬁnition of the discrepancy. Thus, there is a speciﬁc choice of the
weights in our bound that makes it a lower bound for that of Cortes et al. (2019b), regardless of
how the weights pare chosen in their bound (the inequality holds uniformly over p). Our algorithm
seeks the best choice of the weights in our bound, for which our bound is thus guaranteed to be a
lower bound for that of Cortes et al. (2019b), regardless of how the weights pare chosen in their
bound.
The weighted-discrepancy minimization algorithm of Cortes and Mohri (2014) is based on a
two-stage minimization of (17) and in that sense is sub-optimal compared to an algorithm seeking
to minimize the bound of Theorem 1.
34BEST-EFFORT ADAPTATION
qchosen uniformly -weighted on S.Letd=dis(P;Q),̂dand̂d=dis(̂Q;̂P). Consider the
following simple, and in general suboptimal, choice of qas a distribution deﬁned by:
q=m
m+nqi=/uni23A7/uni23AA/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23AA/uni23A9q
m=
m+nifi∈[m];
1−q
n=m(1−)+n
(m+n)notherwise;
where=	(1−d)for some non-decreasing function 	with 	(0)=0and	(1)=1. We will
compare the right-hand side of the bound of Theorem 1, which we denote by B, with its right-hand
sideB0forqchosen to be uniform over S′corresponding to supervised learning on just S′:
B0=L(̂P;h)+2Rn(`○H)+/roottop
/rootmod/rootmod/rootbotlog1

2n:
We now show that under some assumptions, we have B−B0≤0. Thus, even for this sub-optimal
choice of q, under those assumptions, the guarantee of the theorem is then strictly more favorable
than the one for training on S′only, uniformly over h∈H.
By deﬁnition of ̂d, we can write:
L(q;h)=qL(̂Q;h)+(1−q)L(̂P;h)≤q̂d+L(̂P;h):
By deﬁnition of the q-Rademacher complexity and the sub-additivity of the supremum, the follow-
ing inequality holds:
Rq(`○H)≤qRm(`○H)+(1−q)Rn(`○H):
By deﬁnition of q, we can write:
/parallel.alt1q/parallel.alt12
2n=n/bracketleft.alt4m/parenleft.alt3q
m/parenright.alt32
+n/parenleft.alt31−q
n/parenright.alt32
/bracketright.alt4=n
mq2+(1−q)2
=1−2q+m+n
mq2
=1−(2−)q≤1−q:
Thus, using the inequality√
1−x≤1−x
2,x≤1, we have:
B−B0≤2q[Rm(`○H)−Rn(`○H)]+q(d+̂d)+/bracketleft.alt2/radical.alt1
1−q−1/bracketright.alt2/bracketleft.alt3log1

2n/bracketright.alt31
2
≤2q[Rm(`○H)−Rn(`○H)]+q(d+̂d)−q/bracketleft.alt3log1

8n/bracketright.alt31
2:
Suppose we are in the regime of relatively small discrepancies and that, given n, both the discrep-
ancy and the empirical discrepancies are upper bounded as follows: max/braceleft.alt1d;d/braceright.alt1</radical.alt2
log 1/slash.left
32n. Assume
also that for m/uni226Bn(which is the setting we are interested in), we have Rm(`○H)−Rn(`○H)≤0.
Then, the ﬁrst term is non-positive and, regardless of the choice of <1, we haveB−B0≤0.
Thus, even for this sub-optimal choice of q, under some assumptions, the guarantee of the theorem
is then strictly more favorable than the one for training on S′only, uniformly over h∈H.
Note that the assumption about the difference of Rademacher complexities is natural. For exam-
ple, for a kernel-based hypothesis set Hwith a normalized kernel such as the Gaussian kernel and
the norm of the weight vectors in the reproducing kernel Hilbert space (RKHS) bounded by , it is
known that the following inequalities hold:1√
2√m≤Rm(H)≤√m(Mohri et al., 2018). Thus, for
m>2n, we have Rm(H)−Rn(H)≤√m−√
2n<0.
35AWASTHI CORTES MOHRI
B.3. Discrepancy estimation
First, note that if the P-drawn labeled sample at our disposal is sufﬁciently large, we can reserve a
sub-sample of size n1to train a relatively accurate model hP. Thus, we can subsequently reduce H
to a ball B(hP;r)of radiusr∼1√n1. This helps us work with a ﬁner (local) discrepancy since the
maximum in the deﬁnition is now taken over a smaller set.
We do not have access to the discrepancy value dis(P;Q), which deﬁnes dis. Instead, we can
use the labeled samples from QandPto estimate it. Our estimate ̂dof the discrepancy is given by
̂d=max
h∈H/braceleft.alt41
nm+n
/summation.disp
i=m+1`(h(xi);yi)−1
mm
/summation.disp
i=1`(h(xi);yi)/braceright.alt4:
Thus, for a convex loss `, the optimization problems for computing ̂dcan be naturally cast as
DC-programming problem, which can be tackled using the DCA algorithm (Tao and An, 1998)
and related methods already discussed for SBEST . For the squared loss, the DCA algorithms is
guaranteed to converge to a global optimum (Tao and An, 1998).
By McDiarmid’s inequality, with high probability, /divides.alt0dis(P;Q)−̂d/divides.alt0can be bounded by O(/radical.alt1m+n
mn).
More reﬁned bounds such as relative deviation bounds or Bernstein-type bounds provide more fa-
vorable guarantee when the discrepancy is relatively small. When His chosen to be a small ball
B(hP;r), our estimate of the discrepancy is further reﬁned.
B.4. Pseudocode of alternate minimization procedure
Input: Samples {(x1;y1);:::(xm+n;ym+n)}, tolerance, distribution p0, max iterations T, hyperparame-
ters∞;1;2, discrepancy estimate ^d.
1. Initialize q0to be the uniform distribution over [m+n].
2. Initialize h0=argminh∈H∑m+n
i=1q0;i`(h(xi);yi)+∞/parallel.alt1q0/parallel.alt1∞/parallel.alt1h/parallel.alt12.
3. Fort=1;:::T ,
• Set curr objval=∑m
i=1qt−1;i/parenleft.alt1`(ht−1(xi);yi)+^d/parenright.alt1+∑m+n
i=m+1qt−1;i`(ht−1(xi);yi)+
∞/parallel.alt1qt−1/parallel.alt1∞/parallel.alt1ht−1/parallel.alt12+1/parallel.alt1qt−1−p0/parallel.alt11+2/parallel.alt1qt−1/parallel.alt12.
• Compute qt=argminq∈m+n∑m
i=1qi/parenleft.alt1`(ht−1(xi);yi)+^d/parenright.alt1+∑m+n
i=m+1qi`(ht−1(xi);yi)+
∞/parallel.alt1q/parallel.alt1∞/parallel.alt1ht−1/parallel.alt12+1/parallel.alt1q−p0/parallel.alt11+2/parallel.alt1q/parallel.alt12.
• Compute ht=argminh∈H∑m
i=1qt;i/parenleft.alt1`(ht−1(xi);yi)+^d/parenright.alt1+∑m+n
i=m+1qt;i`(ht−1(xi);yi)+
∞/parallel.alt1qt/parallel.alt1∞/parallel.alt1h/parallel.alt12.
• Set new objval=∑m
i=1qt;i/parenleft.alt1`(ht(xi);yi)+^d/parenright.alt1+∑m+n
i=m+1qt;i`(ht(xi);yi)+∞/parallel.alt1qt/parallel.alt1∞/parallel.alt1ht/parallel.alt12+
1/parallel.alt1qt−p0/parallel.alt11+2/parallel.alt1qt/parallel.alt12.
• If/divides.alt0curr objval−new objval/divides.alt0≤, returnqt;ht
4. Print: AM did not converge in T iterations . ReturnqT;hT.
Figure 2: Alternate minimization procedure for best effort adaptation.
36BEST-EFFORT ADAPTATION
Appendix C. Domain adaptation
C.1. Theorems and proofs
Let(q;q′)denote the vector in [0;1]m+nformed by appending q′toq. The learning bound of
Theorem 5 can be extended to hold uniformly over all pin[0;1][m]and(q;q′)in
/braceleft.alt1(q;q′)∈[0;1]m×[0;1]n∶0</parallel.alt1(q;q′)−p0/parallel.alt11<1/braceright.alt1;
where p0is a reference (or ideal) reweighting choice over the (m+n)points.
Theorem 10 For any>0, with probability at least 1−over the choice of a sample Sof sizem
fromQand a sample S′of sizenfromP, the following holds for all h∈H,q∈/braceleft.alt1q∶0≤/parallel.alt1(q;q′)−p0/parallel.alt11<1/braceright.alt1
and all p∈[0;1]m:
L(P;h)≤m
/summation.disp
i=1(qi+pi)`(h(xi);yi)+dis(q′;p)
+dis/parenleft.alt2/bracketleft.alt11−/parallel.alt1q′/parallel.alt11/bracketright.altP;/parallel.alt1q/parallel.alt11Q/parenright.alt2
+dis((q;q′);p0)+2R(q;q′)(`○H)+5/parallel.alt1(q;q′)−p0/parallel.alt11
+/bracketleft.alt1/parallel.alt1q/parallel.alt12+2/parallel.alt1(q;q′)−p0/parallel.alt11/bracketright.alt/uni23A1/uni23A2/uni23A2/uni23A2/uni23A3/radical.alt2
log log22
1−/parallel.alt1(q;q′)−p0/parallel.alt11+/radical.alt3
log2

2/uni23A4/uni23A5/uni23A5/uni23A5/uni23A6:
Proof The proof follows immediately by applying inequality (5), which holds for all p∈[0;1]m, to
the bound of Theorem 3.
Corollary 11 For any>0, with probability at least 1−over the choice of a sample S
of sizemfromQand a sample S′of sizenfromP, the following holds for all h∈H,q∈
/braceleft.alt1q∶0≤/parallel.alt1(q;q′)−p0/parallel.alt11<1/braceright.alt1and all p∈[0;1]m:
L(P;h)≤m
/summation.disp
i=1(qi+pi)`(h(xi);yi)+dis(q′;p)
+/parallel.alt1q/parallel.alt11dis(P;Q)
+dis((q;q′);p0)+2R(q;q′)(`○H)+6/parallel.alt1(q;q′)−p0/parallel.alt11
+/bracketleft.alt1/parallel.alt1q/parallel.alt12+2/parallel.alt1(q;q′)−p0/parallel.alt11/bracketright.alt/uni23A1/uni23A2/uni23A2/uni23A2/uni23A3/radical.alt2
log log22
1−/parallel.alt1(q;q′)−p0/parallel.alt11+/radical.alt3
log2

2/uni23A4/uni23A5/uni23A5/uni23A5/uni23A6:
Proof The result follows Theorem 10 and the application of the upper bound used in the proof of
Corollary 1.
C.2. Proof of Lemma 6
Lemma 12 Let`be the squared loss. Then, for any hypothesis h0inH, the following upper bound
holds for the labeled discrepancy:
dis(̂P;̂Q)≤disH×(̂P;̂Q)+2H;h0(̂P;̂Q):
37AWASTHI CORTES MOHRI
Proof For anyh0, using the deﬁnition of the squared loss, the following inequalities hold:
dis(̂P;̂Q)=sup
h∈H/divides.alt4E
(x;y)∼̂P[`(h(x);y)]−E
(x;y)∼̂Q[`(h(x);y)]/divides.alt4
≤sup
h∈H/divides.alt4E
(x;y)∼̂P[`(h(x);h0(x))]−E
(x;y)∼̂Q[`(h(x);h0(x))]/divides.alt4
+sup
h∈H/divides.alt4E
(x;y)∼̂P[`(h(x);y)]−E
(x;y)∼̂P[`(h(x);h0(x))]
+E
(x;y)∼̂Q[`(h(x);h0(x))]−E
(x;y)∼̂Q[`(h(x);y)]/divides.alt4
=disH×(̂P;̂Q)
+2 sup
h∈H/divides.alt4E
(x;y)∼̂P[h(x)(y−h0(x))]−E
(x;y)∼̂Q[h(x)(y−h0(x))]/divides.alt4
(def. of squared loss)
=disH×(̂P;̂Q)+2H;h0(̂P;̂Q): (def. of local discrepancy)
This completes the proof.
C.3. Proof of Lemma 7
Lemma 13 Let`be a loss function that is -Lipschitz with respect to its second argument. Then,
for any hypothesis h0inH, the following upper bound holds for the labeled discrepancy:
dis(̂P;̂Q)≤disH×(̂P;̂Q)+H;h0(̂P;̂Q):
Proof When the loss function `is-Lipschitz with respect to its second argument, we can use the
following upper bound:
dis(̂P;̂Q)=sup
h∈H/divides.alt4E
(x;y)∼̂P[`(h(x);y)]−E
(x;y)∼̂Q[`(h(x);y)]/divides.alt4
≤sup
h∈H/divides.alt4E
(x;y)∼̂P[`(h(x);h0(x))]−E
(x;y)∼̂Q[`(h(x);h0(x))]/divides.alt4
+sup
h∈H/divides.alt4E
(x;y)∼̂P[`(h(x);y)]−E
(x;y)∼̂P[`(h(x);h0(x))]
+E
(x;y)∼̂Q[`(h(x);h0(x))]−E
(x;y)∼̂Q[`(h(x);y)]/divides.alt4
≤disH×(̂P;̂Q)+E
(x;y)∼̂P[/divides.alt0y−ho(x)/divides.alt0]+E
(x;y)∼̂Q[/divides.alt0y−ho(x)/divides.alt0]:
(`assumed-Lipschitz)
This completes the proof.
38BEST-EFFORT ADAPTATION
C.4. Sub-Gradients and estimation of unlabeled discrepancy terms
Here, we ﬁrst describe how to compute the sub-gradients of the unlabeled weighted discrepancy
term dis(q′;p)that appears in the optimization problem for domain adaptation (6), and similarly
dis((q;q′);p0), in the case of the squared loss with linear functions. Next, we show how the
same analysis can be used to compute the empirical discrepancy term dis(̂P;̂Q), which provides an
accurate estimate of d=dis(P;Q).
C.4.1. S UB-GRADIENTS OF UNLABELED WEIGHTED DISCREPANCY TERMS
Let`be the squared loss and let Hbe the family of linear functions deﬁned by H={x/uni21A6w⋅(x)∶/parallel.alt1w/parallel.alt12≤},
where is a feature mapping from XtoRk. We can analyze the unlabeled discrepancy term
dis(q′;p)using an analysis similar to that of Cortes and Mohri (2014). By deﬁnition of the unla-
beled discrepancy, we can write:
dis(q′;p)=sup
h;h′∈H/braceleft.alt4n
/summation.disp
i=1q′
i`(h(xm+i);h′(xm+i))−m
/summation.disp
i=1pi`(h(xi);h′(xi))/braceright.alt4
= sup
/parallel.alt1w/parallel.alt12;/parallel.alt1w′/parallel.alt12≤/braceleft.alt4n
/summation.disp
i=1q′
i/bracketleft.alt1(w−w′)⋅(xm+i)/bracketright.alt2−m
/summation.disp
i=1pi/bracketleft.alt1(w−w′)⋅(xi)/bracketright.alt2/braceright.alt4
=sup
/parallel.alt1u/parallel.alt12≤2/braceleft.alt4n
/summation.disp
i=1q′
i[u⋅(xm+i)]2−m
/summation.disp
i=1pi[u⋅(xi)]2/braceright.alt4
=sup
/parallel.alt1u/parallel.alt12≤2/braceleft.alt4n
/summation.disp
i=1q′
iu/uni22BA(xm+i)(xm+i)/uni22BAu−m
/summation.disp
i=1piu/uni22BA(xi)(xi)/uni22BAu/braceright.alt4
=sup
/parallel.alt1u/parallel.alt12≤2/braceleft.alt4u/uni22BA/bracketleft.alt4n
/summation.disp
i=1q′
i(xm+i)(xm+i)/uni22BA−m
/summation.disp
i=1pi(xi)(xi)/uni22BA/bracketright.alt4u/braceright.alt4
=42sup
/parallel.alt1u/parallel.alt12≤1u/uni22BAM(q′;p)u
=42max/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A90;sup
/parallel.alt1u/parallel.alt12=1u/uni22BAM(q′;p)u/uni23AB/uni23AA/uni23AA/uni23AC/uni23AA/uni23AA/uni23AD
=42max/braceleft.alt10;max/parenleft.alt1M(q′;p)/parenright.alt1/braceright.alt1;
where M(q′;p)=∑n
i=1q′
i(xm+i)(xm+i)/uni22BA−∑m
i=1pi(xi)(xi)/uni22BAand wheremax(M(q′;p))
denotes the maximum eigenvalue of the symmetric matrix M(q′;p). Thus, the unlabeled discrep-
ancy dis(q′;p)can be obtained from the maximum eigenvalue of a symmetric matrix that is an
afﬁne function of q′andp. Sincemaxis a convex function and since composition with an afﬁne
function preserves convexity, max(M(q′;p))is a convex function of q′andp. Since the maximum
of two convex function is convex, max{0;max(M(q′;p))}is also convex.
39AWASTHI CORTES MOHRI
Rewritingmax(M(q′;p))asmax/parallel.alt1u/parallel.alt12=1u/uni22BAM(q′;p)uhelps derive its sub-gradient using the
sub-gradient calculation of the maximum of a set of functions:
∇(q′;p)max/parenleft.alt1M(q′;p)/parenright.alt1=/uni23A1/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A3u/uni22BA(xm+1)(xm+1)/uni22BAu
⋮
u/uni22BA(xm+n)(xm+n)/uni22BAu
−u/uni22BA(x1)(x1)/uni22BAu
⋮
−u/uni22BA(xm)(xm)/uni22BAu/uni23A4/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A6=/uni23A1/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A2/uni23A3((xm+1)⋅u)2
⋮
((xm+n)⋅u)2
−((x1)⋅u)2
⋮
−((xm)⋅u)2/uni23A4/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A5/uni23A6;
where uis the eigenvector corresponding to the maximum eigenvalue of M(q′;p). Alternatively,
we can approximate the maximum eigenvalue via the softmax expression
f(q′;p)=1
log/uni23A1/uni23A2/uni23A2/uni23A2/uni23A2/uni23A3k
/summation.disp
j=1ej(M(q′;p))/uni23A4/uni23A5/uni23A5/uni23A5/uni23A5/uni23A6=1
log/bracketleft.alt2Tr/parenleft.alt2eM(q′;p)/parenright.alt2/bracketright.alt2;
whereeM(q′;p)denotes the matrix exponential of M(q′;p)andj(M(q′;p))thejth eigenvalue
ofM(q′;p). The matrix exponential can be computed in O(k3)time by computing the singular
value decomposition (SVD) of the matrix. We have:
max(M(q′;p))≤f(q′;p)≤max(M(q′;p))+logk
:
Thus, for=logk
,f(q′;p)provides a uniform -approximation of max(M(q′;p)). The gradient
off(q′;p)is given for all j∈[n]andi∈[m]by
∇q′
jf(q′;p)=/uni27E8.alteM(q′;p);(xm+j)(xm+j)/uni22BA/uni27E9.alt1
Tr/parenleft.alt1eM(q′;p)/parenright.alt1=(xm+j)/uni22BAeM(q′;p)(xm+j)
Tr/parenleft.alt1eM(q′;p)/parenright.alt1
∇pif(q′;p)=−/uni27E8.alteM(q′;p);(xi)(xi)/uni22BA/uni27E9.alt1
Tr/parenleft.alt1eM(q′;p)/parenright.alt1=(xi)/uni22BAeM(q′;p)(xi)
Tr/parenleft.alt1eM(q′;p)/parenright.alt1:
The sub-gradient of the unlabeled discrepancy term dis((q;q′);p0)or a smooth approximation can
be derived in a similar, using the same analysis as above.
C.4.2. E STIMATION OF UNLABELED DISCREPANCY TERMS
The unlabeled discrepancy d=dis(P;Q)can be accurately estimated from its empirical version
dis(̂P;̂Q)(Mansour et al., 2009a). In view of the analysis of the previous section, we have
dis(̂P;̂Q)=42max/parenleft.alt1M(̂P;̂Q)/parenright.alt1
=42max/parenleft.alt41
nn
/summation.disp
i=1(xm+i)(xm+i)/uni22BA−1
mm
/summation.disp
i=1(xi)(xi)/uni22BA/parenright.alt4:
Thus, this last expression can be used in place of din the optimization problem for domain adapta-
tion.
40BEST-EFFORT ADAPTATION
Appendix D. Further details about experimental settings
In this section we provide further details on our experimental setup starting with best effort adapta-
tion.
D.1. Best-Effort adaptation
Recall that in this setting we have labeled data from both source and target, however the amount of
labeled data from the source is much larger. We start by describing the baselines that we compare
our algorithms with. For the best-effort adaptation problem two natural baselines are to learn a
hypothesis solely on the target P, or train solely on the source Q. A third baseline that we consider
is the-reweighted qas discussed in Section B.2. Note, =1corresponds to training on all the
available data with a uniform weighting.
D.1.1. S IMULATED DATA
We ﬁrst consider a simulated scenario where nsamples from the target distribution Pare generated
by ﬁrst drawing the feature vector xi.i.d. from a normal distribution with zero mean and spherical
covariance matrix, i.e, N(0;Id×d). Givenx, a binary label y∈−;+is generated as sgn (wp⋅x)for
a randomly chosen unit vector wp∈Rd. For a ﬁxed ∈(0:5;1),m=1;000i.i.d. samples from the
source distribution Qare generated by ﬁrst drawing (1−)mexamples from N(0;Id×d)and labeled
according to sgn (wq⋅x)where/parallel.alt1wp−wq/parallel.alt1≤, for a small value of . Notice that when is small,
the(1−)msamples are highly relevant for learning the target P. The remaining mexamples
fromQare all set to a ﬁxed vector uand are labeled as +1. These examples represent the noise in
Qand asincreases the presence of such examples makes dis(P;Q)larger. In our experiments we
setd=20;=0:01, and vary∈{0:05;0:1;0:15;0:2}.
On the above adaptation problem we evaluate the performance of the previously discussed base-
lines with our proposed SBEST algorithm implemented via the alternate minimization, SBEST -AM,
and the DC-programming algorithms, SBEST -DC, where the loss function considered is the logistic
loss and the hypothesis set is the set of linear models with zero bias. For each value of , the results
are averaged over 50independent runs using the data generation process described above.
Figure 3 shows the performance of the different algorithms for various values of the noise level
and as the number of examples nfrom the target increases. As can be seen from the ﬁgure, both
-reweighting and the baseline that trains solely on Qdegrade signiﬁcantly in performance as 
increases. This is due to the fact the -reweighting procedure cannot distinguish between non-noisy
and noisy data points within the msamples generated from Q.
In Figure 4(Left) we plot the best chosen by the -reweighting procedure as a function of n.
For reference we also plot the amount of mass on the non-noisy points from Q, i.e.,(1−)⋅m/slash.left(m+
n). As can be seen from the ﬁgure, as nincreases the amount of mass selected over the source Q
decreases. Furthermore, as expected this decrease is sharper as the amount of noise level increases.
In particular, -reweighting is not able to effectively use the non-noisy samples from Q.
On the other hand, both SBEST -AM and SBEST -DC are able to counter the effect of the noise
by generating q-weightings that are predominantly supported on the non-noisy samples. In Fig-
ure 4(Right) we plot the amount of probability mass that the alternate minimization and the DC-
programming implementations of SBEST assign to the noisy data points.
As can be seen from the ﬁgure, the total probability mass decreases with nand is also decreasing
with the noise levels. These results also demonstrate that our algorithms that compute a good q-
41AWASTHI CORTES MOHRI
200 400 600 800 1000
n0.700.750.800.850.900.951.00Test Accuracy=5%
train on P
train on Q
-reweighting
Alt. Min.
DC
200 400 600 800 1000
n0.700.750.800.850.900.951.00Test Accuracy=10%
train on P
train on Q
-reweighting
Alt. Min.
DC
200 400 600 800 1000
n0.700.750.800.850.900.951.00Test Accuracy=15%
train on P
train on Q
-reweighting
Alt. Min.
DC
200 400 600 800 1000
n0.700.750.800.850.900.951.00Test Accuracy=20%
train on P
train on Q
-reweighting
Alt. Min.
DC
Figure 3: Comparison of SBEST against the baselines on simulated data in the classiﬁcation setting.
As the noise rate and therefore the discrepancy between PandQincreases the perfor-
mance of the baselines degrades. In contrast, both the alternate minimization and the
DC-programming algorithms effectively ﬁnd a good q-weighting and can adapt to the
target.
weighting can do effective outlier detection since they lead to solutions that assign much smaller
mass to the noisy points.
0 200 400 600 800 1000
n0.20.40.60.81.0best  in -reweighting
ref.=5%
ref.=10%
ref.=15%
ref.=20%
=5%
=10%
=15%
=20%
200 400 600 800 1000
n0.0000.0050.0100.0150.0200.0250.030mass on noisy pointsAM, =5%
AM,=10%
AM,=15%
AM,=20%
DC, =5%
DC,=10%
DC,=15%
DC,=20%
Figure 4: (Left) Best chosen by-reweighting as a function of n. (Right) Total probability mass
assigned by SBEST to the noisy points.
D.2. Fine-tuning tasks
In this section we demonstrate the effectiveness of our proposed algorithms for the purpose of
ﬁne-tuning pre-trained representations. In the standard pre-training/ﬁne-tuning paradigm (Raffel
et al., 2019) a model is ﬁrst pre-trained on a generalist dataset (which is identiﬁed as coming from
distribution Q). Once a good representation is learned, the model is then ﬁne-tuned on a task speciﬁc
42BEST-EFFORT ADAPTATION
dataset (generated from target P). Two of the predominantly used ﬁne-tuning approaches in the
literature are last layer ﬁne-tuning (Subramanian et al., 2018; Kiros et al., 2015) and full model
ﬁne-tuning (Howard and Ruder, 2018). In the former approach the representations obtained from
the last layer of the pre-trained model are used to train a simple model (often a linear hypothesis)
on the data coming from P. In our experiments we ﬁx the choice of the simple model to be a multi-
class logistic regression model. In the latter approach, the model when train on P, is initialized
from the pre-trained model and all the parameters of the model are ﬁne-tuned (via gradient descent)
on the target distribution P. In this section we explore the additional advantages of combining
data from both PandQduring the ﬁne-tuning stage via our proposed algorithms. There has been
recent interest in carefully combining various tasks/data for the purpose of ﬁne-tuning and avoid the
phenomenon of “negative transfer” (Aribandi et al., 2021). Our proposed theoretical results present
a principled approach towards this.
To evaluate the effectiveness of our theory for this purpose, we consider the CIFAR-10 vision
dataset (Krizhevsky et al., 2009). The dataset consists of 50000 training and 10000 testing exam-
ples belonging to 10classes. We form a pre-training task on data from Q, by combining all the
data belonging to classes: f’airplane’, ’automobile’, ’bird’, ’cat’, ’deer’, ’dog’ g. The ﬁne-tuning
task consists of data belonging to classes: f’frog’, ’horse’, ’ship’, ’truck’ g. We consider both the
approaches of last layer ﬁne-tuning and full-model ﬁne-tuning and compare the standard approach
of ﬁne-tuning only using data from Pwith our proposed algorithms. We use 60% of the data from
the source for pre-training, and the remaining 40% is used in ﬁne-tuning.
We split the ﬁne-tuning data from Prandomly into a 70% training set to be used in ﬁne-tuning,
10% for cross validation and and the remaining 20% to be used as a test set. The results are reported
over5such random splits. We perform pre-training on a standard ResNet-18 architecture (He et al.,
2016) by optimizing the cross-entropy loss via the Adam optimizer. As can be seen in Table 1 both
gapBoost and SBEST that combine data from PandQlead to a classiﬁer with better performance
for the downstream task, however, SBEST clearly outperforms gapBoost.
The second dataset we consider is the Civil Comments dataset Pavlopoulos et al. (2020).
This dataset consists of text comments in online forums and the goal is to predict whether a given
comment is toxic or not. Each data point is also labeled with identity terms that describes which
subgroup the text in the comment is related to. We create a subsample of the dataset where the target
consists of examples from the data points where the identity terms is “asian” and the source is the
remaining set of points. This leads to 394;000points from the source and 20;000points from the
target. We create 5random splits of the data by randomly partitioning the target data into 10;000
examples for ﬁnetuning, 2000 for validation and 8000 for testing. We perform pre-training on a
BERT-small model (Devlin et al., 2018) starting from the default checkpoint as obtained from the
standard tensorﬂow implementation of the model.
D.3. Domain adaptation
In this section we evaluate the effectiveness of our proposed BEST -DAobjective for adaptation in
settings where the target has very little to no labeled data. In order to do this we consider multi-
domain sentiment analysis dataset of (Blitzer et al., 2007) that has been used in prior works on
domain adaptation. The dataset consists of text reviews associated with a star rating from 1to5
for various different categories such as BOOKS ,DVD, etc. We speciﬁcally consider four categories
namely BOOKS ,DVD,ELECTRONICS , and KITCHEN . Inspired form the methodology adapted in
43AWASTHI CORTES MOHRI
prior works (Mohri and Mu ˜noz Medina, 2012; Cortes and Mohri, 2014), for each category, we form
a regression task by converting the review text to a 128dimensional vector and ﬁtting a linear regres-
sion model to predict the rating. In order to get the features we ﬁrst combine all the data from the
four tasks and convert the raw text to a TF-IDF representation using scikit-learn’s feature extraction
library (Pedregosa et al., 2011). Following this, we compute the top 5000 most important features
by using scikit-learn’s feature selection library, that in turn uses a chi-squared test to perform feature
selection. Finally, we project the obtained onto a 128dimensional space via performing principal
component analysis.
After feature extraction, for each task we ﬁt a ridge regression model in the 128dimensional
space to predict the ratings. The predictions of the model are then deﬁned as the ground truth
regression labels. Following the above pre-processing we form 12 adaptation problems for each
pair of distinct tasks: (TaskA, TaskB) where TaskA, TaskB are in fBOOKS ,DVD,ELECTRONICS ,
KITCHEN g. In each case we form the source domain ( Q) by taking 500 labeled samples from
TaskA and 200labeled examples from TaskB. The target ( P) is formed by taking 300unlabeled
examples from TaskB. To our knowledge, there exists no principled method for cross-validation in
fully unsupervised domain adaptation. Thus, in our adaptation experiments, we used a small labeled
validation set of size 50to determine the parameters for all the algorithms. This is consistent with
experimental results reported in prior work (e.g., (Cortes and Mohri, 2014)).
We compare our BEST -DAalgorithm with the discrepancy minimization (DM) algorithm of
Cortes and Mohri (2014), and the (GDM) algorithm, (Cortes et al., 2019b), which is a state of the
art adaptation algorithm for regression problems. We also compare with the popular Kernel Mean
Matching (KMM) algorithm, (Huang et al., 2006), for domain adaptation. the results averaged
over10independent source and target splits, where we normalize the mean squared error (MSE) of
BEST -DAto be 1:0and present the relative MSE achieved by the other methods. The results show
that in most adaptation problems, BEST -DAoutperforms (boldface) or ties with (italics) existing
methods.
D.3.1. D OMAIN ADAPTATION –COVARIATE -SHIFT
Here we perform experiments for domain adaptation only under covariate shift and compare the
performance of our proposed BEST -DAobjective with previous state of the art algorithms. We again
consider the multi-domain sentiment analysis dataset (Blitzer et al., 2007) from the previous section
and in particular focus on the books category. We use the same feature representation as before and
deﬁne the ground truth as y=w∗⋅x+2wherew∗is obtained by ﬁtting a ridge regression classiﬁer.
We let the target be the uniform distribution over the entire dataset. We deﬁne the source as follows:
for a ﬁxed value of , we pick a random hyperplane wand consider a mixture distribution with
mixture weight 0:99on the setw⋅x≥and the mixture weight of 0:01on the setw⋅x<. The
performance of BEST -DAas compared to DM and KMM is shown in Table 2. As can be seen our
proposed algorithm either matches or outperforms current algorithms.
Hyperparameters for the algorithms .
For our proposed SBEST and SBEST -DAalgorithms the hyperparameters ∞;1;2were cho-
sen via cross-validation in the union of the sets {1e−3;1e−2;1e−1},{0;1;2;:::; 10}, and
{0;1000;2000;10000;50000;100000}. Thehoptimization step of alternate minimization was
performed using sklearn’s linear regression/logistic regression methods (Pedregosa et al., 2011).
During full layer ﬁne-tuning on ResNet/BERT models we use the Adam optimizer for the hop-
44BEST-EFFORT ADAPTATION
Table 2: MSE achieved by BEST -DAas compared to DM and KMM on the covariate shift task for
various values of .
METHOD =0 =0:2=0:4=0:6=0:8=1:0
TRAIN ON Q0:051±0:001 0:06±0:001 0:06±0:004 0:07±0:006 0:073±0:002 0:073±0:005
KMM 0:05±1e−4 0:05±1e−4 0:05±3e−4 0:06±1e−4 0:06±1e−4 0:07±2e−4
DM 0:02±0:005 0:06±0:003 0:05±0:003 0:05±0:001 0:06±0:005 0:06±0:003
BEST -DA 0:01±0:006 0:02±0:006 0:027±0:005 0:04±0:004 0:04±0:007 0:04±0:004
timization step with the default learning rates used for the CIFAR-10 dataset and the BERT-small
models.
For theqoptimization we used projected gradient descent and the step size was chosen via cross
validation in the range {1e−3;1e−2;1e−1}.
We re-implemented the gapBoost algorithm (Wang et al., 2019a) in Python. Following the pre-
scription by the authors of gapBoost we set the parameter =1/slash.leftnwherenis the size of the target.
We tune parameters S;Tin the range {0:1;0:2;:::; 1}and the number of rounds of boosting in
the range {5;10;15;20}. We also re-implemented baselines DM (Cortes and Mohri, 2014) and
the GDM algorithm (Cortes et al., 2019b). These DM algorithm was implemented via gradient
descent and the second stage of the GDM algorithm was implemented via alternate minimization.
The learning rates in each case searched in the range {1e−3;1e−2;1e−1}and the regularization
parameters were searched in the range {1e−3;1e−2;1e−1;0;10;100}. The radius parameter for
GDM was searched in the range [0:01;1]in steps of 0:01.
To our knowledge, there exists no principled method for cross-validation in fully unsupervised
domain adaptation. Thus, in our unsupervised adaptation experiments, we used a small labeled
validation set of size 50to determine the parameters for all the algorithms. This is consistent with
experimental results reported in prior work (Cortes and Mohri, 2014; Cortes et al., 2019b).
45