Teaching MLOps
in Higher Education through Project-Based Learning
Filippo Lanubile
University of Bari
Bari, Italy
ﬁlippo.lanubile@uniba.itSilverio Mart ´ınez-Fern ´andez
Universitat Polit `ecnica de Catalunya
Barcelona, Spain
silverio.martinez@upc.eduLuigi Quaranta
University of Bari
Bari, Italy
luigi.quaranta@uniba.it
Abstract —Building and maintaining production-grade ML-
enabled components is a complex endeavor that goes beyond
the current approach of academic education, focused on the
optimization of ML model performance in the lab. In this
paper, we present a project-based learning approach to teaching
MLOps, focused on the demonstration and experience with
emerging practices and tools to automatize the construction of
ML-enabled components. We examine the design of a course
based on this approach, including laboratory sessions that cover
the end-to-end ML component life cycle, from model building
to production deployment. Moreover, we report on preliminary
results from the ﬁrst edition of the course. During the present
year, an updated version of the same course is being delivered
in two independent universities; the related learning outcomes
will be evaluated to analyze the effectiveness of project-based
learning for this speciﬁc subject.
Index Terms —machine learning, data science, software engi-
neering for AI, model deployment, reproducibility1
I. I NTRODUCTION
Over the past few years, Machine Learning (ML) techniques
have been massively adopted in the software industry to solve
a wide range of problems in a large and growing number of
application domains [1]. As a direct consequence, ML-related
job positions –e.g., data scientist, ML engineer, data engineer–
are among the most sought after in today’s IT job market [2].
Notwithstanding its high promises, however, the adoption of
ML typically turns out to be costly and challenging, especially
for traditional software companies with little experience in
the ﬁeld. While building well-performing models is a rel-
atively easy task to achieve – especially thanks to modern
ML frameworks, which abstract out the hard parts of the
process – integrating such models into production systems and
maintaining them over time is a complex endeavor. The results
of an extensive survey published by Algorithmia in 2020 show
that 55% of the surveyed companies had not yet managed
to deploy a single model to production, despite considerably
investing in ML [3].
For this reason, several tools and practices are being pro-
posed to support ML teams in translating their lab prototypes
into production-grade ML components. For the most part,
1IEEE copyright notice (© 2023 IEEE): This is an Author’s Accepted
Manuscript consisting of a post-peer-review, pre-copyedit version of a paper to
be published by IEEE in the 2023 IEEE/ACM 45th International Conference
on Software Engineering: Software Engineering Education and Training
(ICSE-SEET). The ﬁnal authenticated version will be available online.these activities are recognized as “MLOps”, an umbrella term
for the development of ML-enabled systems. Rooted in Soft-
ware Engineering and inspired by DevOps [4], MLOps places
emphasis on process automation to achieve the continuous
delivery of intelligent software functionalities; moreover, it
takes into account the challenges that are peculiar to ML
(e.g., reproducibility, explainability, and data privacy), and
tries to address them with targeted improvements of the ML-
component building process.
In the surge of enthusiasm for ML, many universities have
started offering specialized degree courses and learning paths.
Nonetheless, so far, the academic focus has been mainly on
the construction of high-performance ML models, leveraging
state-of-the-art techniques, such as deep learning [5]. To the
best of our knowledge, university courses do not generally
emphasize MLOps, i.e., the practices and tools aimed at sup-
porting the full life-cycle of production-ready ML components.
Besides delaying the acquisition of core competencies that
are increasingly in demand, not providing training on MLOps
represents a missed opportunity to ﬁll the well-known cultural
gap between data-related roles and software engineers.
In this paper, we describe our effort to teach MLOps
practices and tools following a project-based learning method
in the context of two university courses. Speciﬁcally, these
courses are offered by two independent universities located
in different countries. Both are targeted at students that are
already acquainted with ML techniques; a key novelty is that
the courses are based on a project assignment whose end goal
is the deployment of a production-ready ML component. Upon
providing the students with basic training on a curated set of
MLOps technologies, we ask them to leverage the presented
tools to achieve their project end goal. Before and after project
execution, we gauge the students’ expectations and perceived
instructional effectiveness of the adopted technologies.
II. R ELATED WORK
With the proliferation of ML-based systems, notable re-
search groups from large companies have highlighted the
diverse skills and knowledge required for their development,
deployment, and maintenance. For instance, practitioners from
Google were pioneers in bringing attention on the fact that
software engineering practices shall not be dismissed for ML
code [6]. In the other way around, practitioners from MicrosoftarXiv:2302.01048v1  [cs.SE]  2 Feb 2023Research remarked the growing demand of professionals with
a deep knowledge of ML in ML-based systems projects [7].
In this context, it has emerged the need for a multidisciplinary
approach to develop ML-based systems, by involving data
scientists (with a strong ML background), software engineers,
and experts in the application domain. Indeed, this needed
multidisciplinarity was argued in an SE4AI Ask-me-anything
session as an educational problem [8]. While there are many
strong academic programs on either software engineering
or data science/machine learning, their synergies are now
increasingly starting to be key in academic curricula. In this
scenario, we can ﬁnd courses focused on the diverse skills
required to build ML-based systems.
Kaestner and Kang created a course called “Software engi-
neering for AI-enabled systems” for undergraduate to master
and PhD students [5]. The course addresses fundamental SE
skills for ML systems (requirements, design, quality assurance,
operations, team, and processes) and responsible ML engineer-
ing (e.g., safety and ethics). This course is currently entitled
“Machine Learning in Production”. With high-quality theory
contents and an online progressing book [9], it does not apply
project-based learning as a teaching methodology.
At a more abstract level, we can also ﬁnd the need of of-
fering new academic programs, aimed at training AI software
engineers. Such a new role would cover both data science
and software engineering. For instance, Bublin et al. propose
a curriculum path for AI software engineers, which can be
used to update a traditional SE curriculum [10]. In a similar
direction, Heck et al. present a bachelor path for AI engineers
[11]. However, there is a gap of information and courses
for teaching the whole MLOps lifecycle with a project-based
learning method including clear guidelines of which practices
and tools to apply.
III. T HEMLO PSPROJECT -BASED COURSE
We propose a project-based learning approach to teach
MLOps. The main objective of a course following this ap-
proach is to provide the students with theoretical and practical
knowledge on building high-quality, production-grade ML
components, ready to be integrated into ML-enabled systems.
The students will have an opportunity to train in both
hard and soft skills. First, they will form and consolidate
expertise in MLOps; this will happen in the context of a
practical project. Second, they will strengthen collaboration
and team-building skills, as they will work in groups of 3 to 5
people, leveraging agile practices such as tracking their work
using a Kanban-style project board or conducting review and
retrospective meetings at the end of each project milestone.
Third, the students will train their presentation skills, as teams
will need to periodically present the progress of their work to
the rest of the class.
The course is designed for both graduate and undergraduate
students. A prerequisite is that students shall have a back-
ground in ML from previous courses (e.g., computer vision,
speech recognition, etc.). In particular, the attendees need to
already know how to build an ML model from a data source.A. Project organization
In the proposed course, the development of theoretical
concepts goes hand in hand with their practical demonstration
and application. Throughout the course, the students engage
with the execution of their project assignments. Each the-
oretical lecture, introducing one or more MLOps practices,
is followed by a practical demonstration of state-of-the-art
MLOps tools. Soon after these demos, the students have time
to autonomously deepen the study of the topic and apply what
they have learned to their own projects.
B. Project milestones
Overall, the project assignments are organized into 6 mile-
stones with a 2-week duration, reﬂecting the distribution of
the course contents. In Table I, for each milestone, we report
the presented MLOps theory, the related practices, and the set
of software solutions used to exemplify their implementation.
The criteria used by the instructors to select these tools are
three-fold: their availability as open-source software, their pop-
ularity in the MLOps community, and their ease of learning. It
should be noted, however, that such choices are not binding for
the students; indeed, the course attendees are free to explore
available alternatives and pick their preferred option.
1)MILESTONE 1– Project inception: the course starts
with a general introduction to building ML-enabled systems,
including key concepts and the main challenges. Afterward,
the instructors present the project assignment: the students will
need to build a production-ready ML component that solves a
real-world problem; the ML model on which the component
will be based can be either pre-existing (e.g., created by the
students to solve the assignment of a previous course) or newly
developed in a parallel course.
a) ML problem speciﬁcation: as a ﬁrst step, the members
of each team discuss to select a problem that can be effectively
solved with ML. Once the decision is made, they deﬁne the
requirements for the ML component they intend to build; these
are documented by ﬁlling in a model card and a dataset
card. Model cards were introduced in [12] to provide a
structured description of ML models including their intended
use cases, ethical considerations, and limitations; they provide
a template for the reporting of basic model requirements and
characteristics. Together with dataset cards – an application of
the same idea to the speciﬁcation of datasets – these templates
are currently used to specify and organize ML assets within
popular ML hubs like Hugging Face [13].
b) Project coordination and communication: to ﬁnalize
the project inception, the student teams set up a project man-
agement tool to coordinate their work and a communication
channel to exchange information.
2)MILESTONE 2– Model building (ensuring ML pipeline
reproducibility): the focus of the second milestone is ensuring
the reproducibility of the end-to-end model building process.
a) Project structure deﬁnition: as a ﬁrst step, the instruc-
tors underline the importance of consistently organizing ML
projects in a structured way, to improve their readability andmaintainability, facilitating collaboration among team mem-
bers. This can be achieved by sticking to a shared convention –
deﬁned at the team level – when organizing source code, data,
and model ﬁles. To exemplify this practice, the instructors
present the cookiecutter data science template [14], a popular
directory structure used to organize data science projects.
b) Code and data versioning: then, the focus shifts to the
versioning of source code and data. Besides recommending the
adoption of git as a Version Control System, the importance
of sticking to a well-deﬁned collaboration workﬂow, leverag-
ing a code hosting platform like GitHub, is stressed. To this
end, the instructors present the GitHub Flow workﬂow [15],
a popular branch-based convention for organizing the devel-
opment of software projects on GitHub. Also, the intricacies
of versioning and comparing computational notebooks (e.g.,
Jupyter notebooks) are discussed, including approaches to
overcome the current limitations of this process.
ML models result from the application of algorithms to data;
therefore, data is a ﬁrst citizen in ML workﬂows and – for the
sake of model traceability and reproducibility – its versioning
is as important as the versioning of source code. To show
how the data versioning process can be seamlessly integrated
with code versioning, the instructors demonstrate the use of
DVC (“Data Version Control”), a popular open-source solutionthat integrates with git to track the versions of large data
ﬁles. DVC can also be used to memorize the computational
steps that compose an ML pipeline, keeping track of the
dependencies and outputs of each step and – ultimately –
codifying a reproducible ML workﬂow in a human-readable
YAML ﬁle.
c) Experiment tracking: to ensure the reproducibility and
accountability of ML model building pipelines and record
the rationale behind the several decisions involved, another
important aspect to consider is the tracking of ML experi-
ments. Indeed, during fast paced experimentations, it is easy
to lose track of the various trials performed, i.e., of how
variations of the datasets and changes in the experimental
conﬁguration affect the performance of output models. To
exemplify experiment tracking, we use the Tracking module
of MLﬂow [16], a popular open-source platform for the
management of the end-to-end ML lifecycle. MLﬂow features
an intuitive Python API that allows its users to log the
parameters and metrics of each experiment, as well as the
artifacts produced by the ML pipeline (e.g., the pre-processed
dataset used for model training and the trained ML models);
after each experiment execution, the logged information can
be visualized and compared using a web UI.3)MILESTONE 3– Model building (quality assurance):
the third project milestone is focused on assuring the quality
of the model building pipeline and the resulting models.
a) Energy efﬁciency measurement: besides requiring
substantial human effort, training state-of-the-art ML models
is a resource-demanding process; computationally intensive
algorithms are repeatedly executed on large input datasets
to train ML models and keep them updated over time. For
this reason, the energy efﬁciency of model training pipelines
should be taken into due account and considered as an im-
portant quality factor of ML-enabled components [17], [18].
To sensitize the students to this theme and provide them
with a practical solution to assess their training pipelines, we
demonstrate the use of Code Carbon; shipped as a lightweight
Python package, this tool can be employed to estimate the
amount of carbon dioxide ( CO 2) produced by various kinds
of computing resources.
b) Static analysis and testing: the source code developed
during the experimental phase of ML projects constitutes
the foundation of production code. Hence, to facilitate the
translation of experimental artifacts (i.e., of scripts and com-
putational notebooks) into production-grade components, it
is crucial to adopt high quality standards since the start
of ML projects. Towards this aim, practitioners generally
implement well-established software engineering practices like
static program analysis and testing; however, these are not
always easy to adapt and apply to experimental ML code:
for instance, assuring the quality of computational notebooks
is not straightforward. For this reason, along with popular
static analysis tools of the Python ecosystem (e.g., Pylint and
ﬂake8), we present and demonstrate the use of Pynblint [19],
a specialized linter for notebooks in the .ipynb format.
Based on a catalog of empirically validated best practices
[20], Pynblint can be employed to analyze Jupyter notebooks
written in Python and detect potential defects hindering the
reproducibility of computations as well as collaboration among
team members.
Another crucial aspect of QA is testing. As the operation
of ML-enabled components is not only based on code, but
also on data and models, all of these entities need to be
thoroughly tested to ensure the quality of ML-enabled sys-
tems. Accordingly, we demonstrate to the students how Pytest
[21] can be adopted to test code and models while Great
Expectations [22] is employed for data testing. Pytest is a
general-purpose testing framework for Python. It can be used
to test model training pipelines as well as the performance
and behavior of models. As for the latter, we emphasize the
importance to gauge the quality of models not only based
on quantitative metrics (e.g., accuracy, precision, and recall),
but also in terms of the behavior exhibited when models
are applied to speciﬁc categories of input data. Behavioral
model testing is a novel research ﬁeld addressing this need;
in our course, we introduce the students to this practice by
means of examples – implemented with Pytest – of behavioral
test instances drawn from [23]. Finally, to test the quality
of data, we demonstrate the use of Great Expectations; thisspecialized testing framework offers dozens of pre-deﬁned
tests and various utilities, like data proﬁlers. Moreover, it can
be used to document the data sources used in an ML project.
4)MILESTONE 4– Model deployment (API development):
once an ML model has been trained and ﬁne-tuned, it shall
be deployed as an ML component to be easily integrated in a
larger ML system.
a) ML component and system physical architecture de-
sign: we start by examining the most common architectural
styles used to organize the components of ML-enabled sys-
tems: N-tier, microservices, batch, and stream big data archi-
tectures. Students shall understand the impact that deploying
an ML component to the server or client side has on quality
attributes. Upon designing the physical architecture of their
component, the students select a cloud platform to host it.
Beyond the variegated set of commercial options – which
typically offer free tiers that are sufﬁcient to host proof-
of-concept applications – the students can leverage virtual
machines provided by their own university.
b) API development and testing for ML: then, we delve
into the development of APIs for ML services. Speciﬁcally,
we present background information on the design of web-
based software interfaces and characterize the most common
architectural styles (e.g., RPC and REST). Next, to provide the
students with practical guidance on this matter, we demon-
strate the implementation of a RESTful API for a simple
ML model. The featured example is developed using FastAPI
[24], an open-source web framework for Python; FastAPI
allows the development and documentation of production-
grade APIs that comply with OpenAPI (i.e., the industry-
standard speciﬁcation); moreover, the framework offers testing
utilities that can be used with Pytest to verify API endpoints.
5)MILESTONE 5– Model deployment (component deliv-
ery): the next milestone is dedicated to the delivery of ML
components as self-contained software packages.
a) Containerization and orchestration: to date, con-
tainerization is one of the most widespread forms of software
packaging. Using containers to ship ML components has
several advantages, e.g., the portability and reproducibility of
the execution environment, which is fully speciﬁed in code.
Furthermore, thanks to container orchestrators – i.e., programs
designed to automate the end-to-end container life cycle –
container-based systems tend to be robust to failures and easy
to scale. In this regard, we demonstrate the use of Docker
[25], the de-facto standard for software containerization, and
of Docker Compose, a command-line tool for the orchestration
of multi-container Docker applications.
b) CI/CD for ML: process automation is one of the
main prerogatives of MLOps. Thus, to reﬁne the deployment
process, we explain how the execution of the main MLOps
practices presented so far can be fully automatized in the
context of Continuous Integration and Continuous Delivery
(CI/CD) workﬂows (from model training, to quality assurance
and containerization). In particular, to exemplify this practice
we leverage GitHub Actions, a workﬂow automation tool
integrated with GitHub.6)MILESTONE 6– Monitoring: the ﬁnal project milestone
is dedicated to the monitoring of deployed ML components.
a) Resource monitoring: ﬁrst, we stress the importance
of continually monitoring the performance and resource con-
sumption of ML-components seen as standard software units;
the metrics collected as part of this practice enable oper-
ations teams to make timely decisions concerning resource
allocation; moreover, the same information can be leveraged
to automatically scale resources and balance the load within
cloud-based execution environments. The minimum form of
resource monitoring is availability monitoring. In our course,
we exemplify this practice using Better Uptime [26], a com-
mercial availability monitor operated via a user-friendly web
application. Then, we demonstrate how a couple of open-
source solutions can be used, in tandem, to collect and visual-
ize comprehensive sets of resource key performance indicators
(KPIs): Prometheus [27] – an open-source monitoring system
based on an efﬁcient time series database – and Grafana
[28] – an open-source dashboarding software for live data
visualization.
b) Model performance monitoring: no matter how accu-
rate models are, the aspects of reality that they capture are
expected to change over time; this is at the root of well-
known phenomena like data and concept drift, which in turn
entail the degradation of performances in production models.
The early detection of these issues is a crucial aspect of
the maintenance of ML-enabled components. For this reason,
the input data stream of each model should be continuously
monitored to timely detect drifts and trigger model re-training
pipelines. In our course, we demonstrate how the Alibi Detect
library [29] can be used to periodically monitor production
data and determine the emergence of outliers, alterations in
variable distributions, and a range of further anomalies that
might result in worsened model performances.
IV. P ROPOSED FORMAL EVALUATION STRATEGY
In 2022 Fall, the project-based learning course is going to
be offered by two independent institutions, i.e., the University
of Bari (Italy) – hereinafter U NIBA – and the Universitat
Polit `ecnica de Catalunya (Spain) – hereinafter UPC. In partic-
ular, U NIBA will offer the course to graduate students attend-
ing an MSc in Computer Science, while UPC will deliver it
to undergraduate students from a BSc in Data Science. In both
cases, the students will be already well trained on building ML
models, having succeeded in previous courses on the subject.
We use the Goal/Question/Metric (GQM) method [30] as an
aid to shape the evaluation goal.
Analyze the project-based learning approach to teach
MLOps for the purpose of evaluation with respect to
the instructional effectiveness from the point of view
ofinstructors and students in the context of university
courses.
In particular, we will measure instructional effectiveness by
focusing on two learning outcomes [31]:1) Affective outcomes: i.e., how the students perceive the
beneﬁts and, more in general, the experience of project-based
learning.
We will collect qualitative feedback from the students by
administering one or more surveys. For instance, the ques-
tionnaires will gauge whether the students perceived working
in a team at the lab project useful to learn MLOps.
2) Artifact quality: i.e., the quality of the artifacts delivered
by the students at each milestone.
We will assess the quality of the artifacts produced by the
students by looking at how they applied each of the MLOps
practices from Table I. Speciﬁcally, we will consider three
possible outcomes:
poor : the students were not able to apply the recom-
mended MLOps practice;
fair: the students implemented the MLOps practice by
replicating the provided examples with minor changes;
good : the students implemented the MLOps practice in
an extended or innovative way.
V. P RELIMINARY RESULTS
In 2021 Fall, we offered a pilot version of the proposed
course to graduate students at U NIBA . Although there was not
a formal evaluation strategy for the pilot, we had a chance to
appreciate the positive feedback from the students and learn
some lessons from the course retrospective. First, we learned
that we could avoid some tutorials because the contents were
already known (e.g. Jupyter notebooks and git) or felt as
overkill for the task (e.g., Kubernetes). On the other side, we
realized the need to go beyond resource monitoring to also
include model performance monitoring and then teach how to
notice the model drift in production.
By the end of the course, all the student teams were able
to deploy their ML components in a cloud-based production
environment. Most of them found the proposed technologies
attractive and useful to carry out the project assignments; in
some cases, they even went beyond the instructor demonstra-
tions, trying out alternative software solutions (e.g., Tensor-
Board instead of MLﬂow for experiment tracking). Finally,
the presentations conducted at the end of each milestone were
perceived by the students as inspiring opportunities to learn
alternative solutions to the challenges faced.
VI. C ONCLUSION
The topics of the project-based course on MLOps cover
the end-to-end life cycle of ML-enabled software components;
for each MLOps practice presented in theory, an hands-on
demonstration of its practical application is delivered to the
students, who in turn apply the acquired knowledge to build
and deploy their own ML-enabled component. As future work,
we will analyze the effectiveness of the proposed project-based
learning approach by taking into account the student feedback
and the quality of their work.ACKNOWLEDGMENT
This work is partially supported by the project TED2021-
130923B-I00, funded by MCIN/AEI/10.13039/501100011033
and the European Union Next Generation EU/PRTR.
REFERENCES
[1] L. Deng, “Artiﬁcial intelligence in the rising wave of deep learning:
The historical path and future outlook [perspectives],” IEEE Signal
Processing Magazine , vol. 35, no. 1, pp. 180–177, 2018.
[2] M. Meesters, P. Heck, and A. Serebrenik, “What is an ai engineer?
an empirical analysis of job ads in the netherlands,” in International
Conference on AI Engineering: Software Engineering for AI . IEEE
Computer Society, 2022.
[3] Algorithmia, “2020 state of enterprise machine learning,” Tech. Rep.,
2020.
[4] L. E. Lwakatare, I. Crnkovic, and J. Bosch, “Devops for ai – chal-
lenges in development of ai-enabled applications,” in 2020 International
Conference on Software, Telecommunications and Computer Networks
(SoftCOM) , 2020, pp. 1–6.
[5] C. K ¨astner and E. Kang, “Teaching software engineering for al-enabled
systems,” in 2020 IEEE/ACM 42nd International Conference on Soft-
ware Engineering: Software Engineering Education and Training (ICSE-
SEET) . IEEE, 2020, pp. 45–48.
[6] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,
V . Chaudhary, M. Young, J.-F. Crespo, and D. Dennison, “Hidden tech-
nical debt in machine learning systems,” Advances in neural information
processing systems , vol. 28, 2015.
[7] P. Y . Simard, S. Amershi, D. M. Chickering, A. E. Pelton, S. Ghorashi,
C. Meek, G. Ramos, J. Suh, J. Verwey, M. Wang et al. , “Machine
teaching: A new paradigm for building machine learning systems,” arXiv
preprint arXiv:1707.06742 , 2017.
[8] “AMA - Software Engineering for Machine Learning,” 2020. [Online].
Available: https://www.youtube.com/watch?v=bLAyj1c JZ0
[9] C. K ¨astner, Machine Learning in Production ,
2021. [Online]. Available: https://ckaestne.medium.com/
machine-learning-in-production-book-overview-63be62393581
[10] M. Bublin, S. Schefer-Wenzl, and I. Miladinovi ´c, “Educating ai software
engineers: Challenges and opportunities,” in International Conference on
Interactive Collaborative Learning . Springer, 2021, pp. 241–251.
[11] P. Heck and G. Schouten, “Lessons learned from educating ai engi-
neers,” in 2021 IEEE/ACM 1st Workshop on AI Engineering-Software
Engineering for AI (WAIN) . IEEE, 2021, pp. 1–4.
[12] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman,
B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru, “Model Cards
for Model Reporting,” in Proceedings of the Conference on Fairness,
Accountability, and Transparency . Atlanta GA USA: ACM, Jan.
2019, pp. 220–229. [Online]. Available: https://dl.acm.org/doi/10.1145/
3287560.3287596
[13] “Hugging Face.” [Online]. Available: https://huggingface.co/
[14] DrivenData, “Cookiecutter Data Science.” [Online]. Available: https:
//drivendata.github.io/cookiecutter-data-science/
[15] GitHub, “GitHub ﬂow.” [Online]. Available: https://docs.github.com/en/
get-started/quickstart/github-ﬂow
[16] “MLﬂow - A platform for the machine learning lifecycle.” [Online].
Available: https://mlﬂow.org/
[17] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,”
Commun. ACM , vol. 63, no. 12, p. 54–63, nov 2020. [Online].
Available: https://doi.org/10.1145/3381831
[18] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy
considerations for deep learning in nlp,” 2019. [Online]. Available:
https://arxiv.org/abs/1906.02243
[19] L. Quaranta, F. Calefato, and F. Lanubile, “Pynblint: a Static Analyzer
for Python Jupyter Notebooks,” May 2022, arXiv:2205.11934 [cs].
[Online]. Available: http://arxiv.org/abs/2205.11934
[20] ——, “Eliciting best practices for collaboration with computational
notebooks,” Proceedings of the ACM on Human-Computer Interaction ,
vol. 6, no. CSCW1, pp. 1–41, 2022.
[21] “pytest: helps you write better programs — pytest documentation.”
[Online]. Available: https://docs.pytest.org/en/7.1.x/
[22] “Great Expectations.” [Online]. Available: https://greatexpectations.io/
[23] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond Accuracy:
Behavioral Testing of NLP models with CheckList,” 2020.[24] “FastAPI.” [Online]. Available: https://fastapi.tiangolo.com/
[25] “Docker Documentation,” Oct. 2022. [Online]. Available: https:
//docs.docker.com/
[26] “Better Uptime - Free Web Monitoring & Status Page.” [Online].
Available: https://betterstack.com/better-uptime
[27] Prometheus, “Prometheus - Monitoring system & time series database.”
[Online]. Available: https://prometheus.io/
[28] “Grafana: The open observability platform.” [Online]. Available:
https://grafana.com/
[29] “Alibi Detect.” [Online]. Available: https://github.com/SeldonIO/
alibi-detect
[30] V . R. Basili, G. Caldiera, and D. H. Rombach, “The Goal Question
Metric Approach,” Encyclopedia of Software Engineering , vol. I, 1994.
[31] P. Guo, N. Saab, L. S. Post, and W. Admiraal, “A review of
project-based learning in higher education: Student outcomes and
measures,” International Journal of Educational Research , vol. 102,
p. 101586, 2020. [Online]. Available: https://www.sciencedirect.com/
science/article/pii/S0883035519325704