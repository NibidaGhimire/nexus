Bayesian Structure Scores for Probabilistic Circuits
Yang Yang*Gennaro Gala*Robert Peharz
Eindhoven University of Technology
KU LeuvenEindhoven University of Technology Graz University of Technology
Eindhoven University of Technology
Abstract
Probabilistic circuits (PCs) are a prominent
representation of probability distributions with
tractable inference. While parameter learning in
PCs is rigorously studied, structure learning is of-
ten more based on heuristics than on principled
objectives. In this paper, we develop Bayesian
structure scores for deterministic PCs, i.e., the
structure likelihood with parameters marginal-
ized out, which are well known as rigorous objec-
tives for structure learning in probabilistic graph-
ical models. When used within a greedy cutset
algorithm, our scores effectively protect against
overﬁtting and yield a fast and almost hyper-
parameter-free structure learner, distinguishing it
from previous approaches. In experiments, we
achieve good trade-offs between training time
and model ﬁt in terms of log-likelihood. More-
over, the principled nature of Bayesian scores un-
locks PCs for accommodating frameworks such
as structural expectation-maximization.
1 INTRODUCTION
Probabilistic circuits (PCs) (Vergari et al., 2020) have
emerged as a powerful framework for describing tractable
probabilistic models, such as Chow-Liu trees (Chow & Liu,
1968), arithmetic circuits (Darwiche, 2003), sum-product
networks (Poon & Domingos, 2011), probabilistic senten-
tial decision diagrams (Kisa et al., 2014) and cutset net-
works (CNets) (Rahman et al., 2014). PCs can be catego-
rized into several sub-families speciﬁed by structural prop-
erties orconstraints , which go hand in hand with certain
tractable inference routines (Vergari et al., 2020), such as
marginalization ,conditioning ,most-probable explanation ,
expectations , etc.
*These authors contributed equally.
Proceedings of the 26thInternational Conference on Artiﬁcial
Intelligence and Statistics (AISTATS) 2023, Valencia, Spain.
PMLR: V olume 206. Copyright 2023 by the author(s).PCs can either be compiled from other models (Darwiche,
2003) or learned directly from data (Lowd & Domin-
gos, 2008; Poon & Domingos, 2011). When learning
PCs from data, we might—similar as in classical prob-
abilistic graphical models (PGMs) (Koller & Friedman,
2009)—distinguish between parameter learning andstruc-
ture learning . Parameter learning has been rigorously stud-
ied in PCs and mirrors state-of-the-art in PGMs. Speciﬁ-
cally, techniques for parameter learning in PCs are (closed-
form) maximum-likelihood estimation (for the sub-class of
deterministic PCs) (Kisa et al., 2014; Peharz et al., 2014),
expectation-maximization (Peharz et al., 2016), concave-
convex procedures (Zhao, Poupart, & Gordon, 2016),
Bayesian approaches (Zhao, Adel, et al., 2016; Rashwan et
al., 2016; Vergari et al., 2019; Trapp et al., 2019), discrim-
inative methods (Rashwan et al., 2018; Peharz, Vergari, et
al., 2020), continuous embeddings (Shao et al., 2022; Cor-
reia et al., 2022), and latent variable distillation (Liu et al.,
2022).
Structure learning in PCs, however, is generally less prin-
cipled than in PGMs. In the latter, the two main styles
of structure learning are constraint-based , i.e., detect-
ing conditional independencies in the data and translat-
ing them into a corresponding graph structure (Koller &
Friedman, 2009); and score-based , i.e., phrasing struc-
ture learning as a discrete optimization problem, aiming to
ﬁnd a graph that maximizes some structure score. Score-
based approaches have turned out to be more practical, as
constraint-based approaches tend to be sensitive to statisti-
cal noise in conditional independence tests (Koller & Fried-
man, 2009, page 790). In particular, Bayesian structure
scores (Buntine, 1991; Geiger & Heckerman, 1994; Heck-
erman et al., 1995) are some of the most prominent scores,
due to their principled nature and asymptotic consistency
guarantees (Koller & Friedman, 2009). Bayesian scores
are the (log-)likelihood of the candidate structures, with pa-
rameters marginalized out, and have been the target of both
greedy (Cooper & Herskovits, 1992) and exact (De Cam-
pos et al., 2009) structure optimizers.
In PCs, however, structure learners are often based on
heuristics. The most prominent structure learning scheme
is probably LearnSPN (Gens & Domingos, 2013), which,arXiv:2302.12130v1  [cs.LG]  23 Feb 2023Bayesian Structure Scores for Probabilistic Circuits
in a nutshell, uses top-down multi-clustering to lay out
the PC structure. The same approach is taken in ID-SPN
(Rooshenas & Lowd, 2014), which augments LearnSPN
by using expressive distribution models as sub-modules.
Other structure learners are based on singular-value decom-
position (Adel et al., 2015), bottom-up information bottle-
neck learning (Peharz et al., 2013) or are motivated by deci-
sion tree learning (Rahman et al., 2014). None of these ap-
proaches declare an explicit objective for structure learning,
although at test time they are usually evaluated in terms of
log-likelihood. Moreover, they often rely on a large num-
ber of hyper-parameters, which are tedious to set or need to
be tuned on a separate validation set.
There are also score-based approaches to learn PC struc-
tures, e.g. the ones by Lowd and Domingos (2008); Pe-
harz et al. (2014), who use a weighted sum of training log-
likelihood and circuit size as a structure score. The circuit
size directly corresponds to the worst-case inference cost
in PCs, thus these scores have the interesting interpretation
of trading off “model ﬁt” vs. “inference complexity.” How-
ever, a deeper theoretical justiﬁcation of these scores has
yet to be shown. Similarly, the Strudel method (Dang et al.,
2020) also performs a structure search based on “ heuris-
tic scores calculated from data .” Thus, these score-based
methods are—similarly as the previously mentioned struc-
ture learners—of heuristic nature, usually equipped with
various (often unintuitive) hyper-parameters for regulariz-
ing the model, and require a separate validation set.
In this paper, we propose a principled avenue to PC struc-
ture learning and put a particular emphasis on elegance
and simplicity. Speciﬁcally, we derive Bayesian struc-
ture scores fordeterministic PCs,1by equipping parame-
ters with suitable priors and marginalizing them from the
Bayesian model. Thus, similarly as in PGMs, a Bayesian
score is the marginal likelihood of the PC structure under
a particular choice of parameter prior. Intuitively, since
the Bayesian score averages over allpossible choices of
parameters, it effectively protects against overﬁtting, as
overly complex structures will have a signiﬁcant portion of
“bad” parameters that are not well-supported by the data.
In the special case of discrete data and Dirichlet priors,
one yields the well-known Bayes-Dirichlet (BD) score
(Buntine, 1991; Heckerman et al., 1995). We employ the
BD score within greedy cutset learning (Rahman et al.,
2014), a simple and fast structure learning scheme for PCs.
Our method has a single hyper-parameter governing the
Dirichlet prior, the equivalent sample size (ESS), which
we, however, keep ﬁxed to 0:1throughout our experiments,
rendering our method effectively hyper-parameter-free .
Additionally, we consider the Bayesian information crite-
rion (BIC), which is frequently used as an alternative to the
1Extensions to general, non-deterministic PCs are left to future
work, but possible directions are given in the Discussion.BD score, as they are asymptotically equivalent (Schwarz,
1978; Koller & Friedman, 2009). The BIC score has in-
deed been considered for PCs (Di Mauro et al., 2015), but
in a form which undercounts the actual number of parame-
ters. Our corrected version of the BIC score improves the
results by Di Mauro et al. (2015) and is demonstrated to be
a viable and efﬁcient alternative to our full BD score.
Both BD and BIC are de facto hyper-parameter-free and
designed to reﬂect generalization. Therefore, they can use
the full training data, without the need to dedicate part of
it to a validation set. Both scores deliver models com-
petitive with state-of-the-art when learned on 20 common
benchmark density estimation data sets, even though we are
using a simplistic structure learner. We compare our ap-
proach with various other PC structure learners, both sim-
ple ones—which we run, like our methods, with their de-
fault hyper-parameters on the entire training data—and so-
phisticated ones—which use extensive tuning but consume
orders of magnitude more computational resources. Our
structure learners are often located on the Pareto front de-
termined by training time and test log-likelihood, i.e. they
are among the best fast structure learners. This, together
with the fact that we use well-principled structure objec-
tives, make our algorithms highly practical.
Moreover, since a Bayesian score is just the marginal
structure likelihood, we can naturally embed our structure
learner within a structural expectation maximization (EM)
algorithm (Friedman, 1998) for learning mixtures of PCs.
Structural EM, which to the best of our knowledge has not
been applied to PCs before, consistently improves over sin-
gle PCs learned with BD, and delivers models close to or
surpassing state-of-the-art.
The source code to replicate the experiments is avail-
able at https://github.com/yangyang-pro/
bayesian-scores-pc .
2 PROBABILISTIC CIRCUITS
Given a set of random variables X, a probabilistic circuit
(PC) over Xis a computational (directed acyclic) graph
G= (V;E)containing three types of computational nodes,
namely distribution nodes ,weighted sums andproducts .
As we elaborate below, each node N2Vcomputes a (pos-
sibly unnormalized) probability distribution over some sub-
set of X, denoted as the scope ofN. Hence, we associate
with each PC a scope function :V!2X, assigning to
each node N2Vits scope(N)X. Naturally, the scope
of any internal node Nsatisﬁes(N) =S
N02in(N)(N0),
where in(N)is the set of all input nodes to N, thus the
scope function is completely speciﬁed by the scopes of
the leaves (also called input nodes) of G.
Any leaf L2Vof the PC is a probability distribution
over its scope (L), where one typically uses some “sim-Yang Yang, Gennaro Gala, Robert Peharz
ple” parametric distribution like Gaussian, Poisson, cate-
gorical, etc., whose parameters are denoted as L. The
internal nodes of the PC are either sums or products. A
sum node S, equipped with weights wS=fwSNgN2in(S),
computes a convex combination (mixture) of its inputs,
i.e.,S=P
N2in(S)wSNN, whereP
N2in(S)wSN= 1 and
wSN0; a product node Pcomputes the product of its
inputs, i.e., P=Q
N2in(P)N. The parameters f;wg
of the PC are all parameters of the leaf distributions
 =fLgL2Vand all sum weights w=fwSgS2V, where
the expression L2V(respectively S2V) means that we
range over all leaves (respectively sum nodes) in V.
Since all leaves are probability distributions and all sum-
weights are non-negative, it follows that each node in V
computes some (possibly unnormalized) distribution over
its scope. We assume that the PC has a single output node,
having full scope X, which represents the model distribu-
tion. It can be computed for any input sample xusing a
feed-forward pass in the PC.
Besides evaluating the model density, PCs also allow a
wide range of tractable probabilistic inference routines, if
they satisfy certain structural properties (or constraints),
denoted as decomposability ,smoothness ,determinism , and
structured decomposability . Speciﬁcally:
• A PC is decomposable , if for each product node Pit
holds that(N)\(N0) =;, for all N6=N02in(P).
• A PC is smooth , if for each sum node Sit holds that
(N) =(N0), for all N;N02in(S).
• A PC is deterministic , if for each sum node Sand each
sample x, at most one of the inputs of Sis non-zero.
• Furthermore, structured decomposability denotes a
stricter form of decomposability, where the decom-
positions of all products adhere to a common pattern
described by a so-called vtree (Pipatsrisawat & Dar-
wiche, 2008; Kisa et al., 2014; Ahmed et al., 2022).
Not all properties are needed for each and every inference
scenario, and studying the correspondence between struc-
tural properties and tractable inference routines is one of
the intriguing facets of PCs (Darwiche & Marquis, 2002;
Vergari et al., 2020). Typically one assumes at least the
properties decomposability and smoothness, which ensure
that each probability distribution computed by any node
N2Vis already normalized. More importantly, they
enable tractable marginals andconditionals —the two core
routines of probabilistic reasoning (Ghahramani, 2015)—
in time linear in the circuit size (Peharz et al., 2015). In the
remainder of the paper, we assume that any PC is decom-
posable and smooth.
Adding determinism yields tractable inference of most-
probable explanations (MPE), i.e., ﬁnding maximizers ofthe PC distribution, and exact computation of maximum-
likelihood parameters (Kisa et al., 2014; Peharz et al.,
2014). In this paper, we will see that determinism also
allows to compute Bayesian structure scores exactly and
efﬁciently.
Structured decomposability allows taking expectations of
one circuit with respect to another (Khosravi et al., 2019),
computing divergences between PCs (Vergari et al., 2021),
etc.
PCs have evolved into a “lingua franca” of tractable mod-
els, since many of them can be cast into the PC framework.
PCs might be compiled from other models (Darwiche,
2003) or learned directly from data, where we can distin-
guish between parameter learning and structure learning.
As mentioned in the introduction, structure learning in PCs
is usually rather ad-hoc and heuristic. In this paper, we de-
velop Bayesian scores as well-principled objectives for PC
structure learning.
3 BAYESIAN STRUCTURE SCORES FOR
DETERMINISTIC PCS
Assume a training set D=fx(n)gN
n=1ofNsamples drawn
i.i.d. from an unknown probability distribution, and a struc-
tured, parametric density model p(xj;G), that is, a distri-
bution over Xconditional on continuous parameters and
a structure parameter G. The basic idea of the Bayesian
structure score is to (i) equip with some prior distribu-
tionp(jG), and to (ii) derive the marginal likelihood of G
by marginalizing the parameters from the Bayesian model:
B(G):=p(DjG) =Z
p(jG)Y
x2Dp(xj;G) d:(1)
The Bayesian score might be used as a direct target of opti-
mization, i.e. a maximum likelihood approach with respect
toG. Alternatively, it might also be wrapped into a larger
probabilistic framework, e.g. a Bayesian approach with re-
spect to structure (Friedman & Koller, 2003) or structural
expectation maximization (Friedman, 1998).
While eq. (1) is a hard computational problem in general,
there are interesting special cases where it can be com-
puted exactly and efﬁciently. In particular, in the context
of Bayesian networks over discrete data, the well-known
Bayes-Dirichlet (BD) score has been studied by Buntine
(1991); Cooper and Herskovits (1992); Heckerman et al.
(1995). By using Dirichlet priors on the parameters, to-
gether with particular assumptions such as independence
among variable families, parameter modularity among dif-
ferent structures, etc., the BD score can be computed an-
alytically in Bayesian networks. Similarly, for Bayesian
networks with Gaussian parameters and normal-Wishart
priors, the Bayes-Gauss (BG) score has been developed
(Geiger & Heckerman, 1994).Bayesian Structure Scores for Probabilistic Circuits
Let nowG= (V;E)be a candidate PC structure
with parametersf;wg, where  =fLgL2Vandw=
fwSgS2V. The parameters of each leaf Lare equipped with
a priorp(L), where we assume that eq. (1), when restricted
toL, can be computed exactly and efﬁciently. That is, we
can compute
BL=Z
p(L)Y
x2Dp(xjL) dL: (2)
In order to keep the notation uncluttered, we write p(xjL),
which should be understood as an evaluation of the leaf
on the sub-vector of xwhose entries correspond to (L).
Eq. (2) can be computed in closed form for many expo-
nential families with conjugate priors, e.g. the Gaussian-
Wishart and Binomial-Dirichlet families.
Further, let Sbe an arbitrary sum node with weights
wS=fwSNgN2in(S), which we equip with a Dirichlet prior
p(wS) =1
B(S)Y
N2in(S)wSN 1
SN; (3)
parameterized by S=fSNgN2in(S), whereSN>0,
andB(S)is the Beta function, i.e. the normalization con-
stant of the Dirichlet distribution. Assuming a-priori pa-
rameter independence, we get the prior
p(;wjG) =Y
S2Vp(wS)Y
L2Vp(L)
=Y
S2V1
B(S)Y
N2in(S)wSN 1
SNY
L2Vp(L):
(4)
The structureG, leaf parameters , and sum-weights w
completely parametrize the PC distribution p(xj;w;G),2
such that the Bayesian structure score for PCs is given as
BPC(G) =Z Z
p(;wjG)Y
x2Dp(xj;w;G) d dw:
(5)
While computing BPC(G)is intractable for non-
deterministic PCs, due to the high-dimensional integrals
which do not simplify further, we can derive an analytic
solution for deterministic PCs.
Recall that in deterministic PCs, for each possible sample
xand each sum node Sat most one input to Sis non-zero.
In general, whether a PC is deterministic or not might de-
pend on both the structure Gand the parameters f;wgin
a convoluted manner. In this paper, however, we require
that for givenGdeterminism holds for each choice of pa-
rameters, a property which we call structural determinism .
Moreover, we require that the indices of the non-zero sum
2For simplicity, we leave the scope function implicit, i.e. we
assume that each leaf “knows” its scope.inputs remain unchanged when varying parameters, i.e.,
for each sample xand sum node Sit holds that if Shas
a non-zero input under two distinct parameters f0;w0g
andf00;w00g, then the same input must be non-zero for
both parameter choices. We call this property parameter-
consistent determinism , which is often assumed by default
in literature (Choi et al., 2020).
A simple way to enforce structural and parameter-
consistent determinism is to associate with each sum Sa
corresponding random variable XS2X, and let Sbe of
the form
S=X
kpk(XS)pk((S)nXS); (6)
wherepk(XS)are distributions over XS(leaves of the PC)
whose supports3are disjoint and independent of any pa-
rameters, and pk((S)nXS)are arbitrary PC nodes with
scope(S)nXS. This form of determinism is very com-
mon and known under various names such as decision
(Darwiche & Marquis, 2002) or regular selectivity (Peharz
et al., 2014).
The key to computing the Bayes score in deterministic PCs
is the tree decomposition introduced by Zhao, Adel, et al.
(2016). Speciﬁcally, they introduced the notion of induced
tree, a sub-graph ofGobtained by (i) removing all inputs
except one from each sum node in G, and (ii) removing all
nodes which are rendered unreachable from the root. Since
we can decide for each sum node independently which in-
put to remove, the number of induced trees is exponential
in the number of sum nodes. They show that the PC distri-
bution can be written as a “large ﬂat mixture,” the so-called
tree decomposition (cf. also Trapp et al. (2019))
p(xj;w;G) =X
2GY
SN2Ew1[SN2]
SNY
L2Vp(xjL)1[L2];
(7)
where the sum runs over all induced trees contained in G,
the ﬁrst product runs over all sum-input edges, and the sec-
ond product over all leaves. Here, 1[SN2]is the indi-
cator that the edge between SandNis contained in and
1[L2]indicates whether Lis in. Thus, each sum term
in (7) is the product of all sum-weights and leaves appear-
ing in.
It turns out that, for some parameter set of non-zero mea-
sure under prior (4), there is for each x2D exactly one
non-zero term in the exponential sum in eq. (7), corre-
sponding to a particular induced tree x. First, we can as-
sume without loss of generality that there exists at least
one non-zero term—otherwise (5) would evaluate to zero,
which would be the correct score for the considered candi-
date structure. Further, in decomposable and deterministic
PCs, there can be at most one non-zero term, correspond-
ing to a particular induced tree x. This induced tree can
3Thesupport is the set of inputs where the density is non-zero.Yang Yang, Gennaro Gala, Robert Peharz
be found by tracing the PC backwards from root to leaves,
where at sum nodes the single non-zero input is followed
and at product nodes all inputs are followed. If at any sum
node a zero-input was selected, it can be shown that the
corresponding term in eq. (7) evaluates to zero (Peharz et
al., 2014). Consequently, eq. (7) reduces to
p(xj;w;G) =Y
SN2Ew1[SN2x]
SNY
L2Vp(xjL)1[L2x]:
(8)
Substituting (4) and (8) into (5) and re-arranging products
yields
BPC(G)=ZZ Y
S2V1
B(S)Y
N2in(S)wSN 1
SNY
x2Dw1[SN2x]
SN!
 Y
L2Vp(L)Y
x2Dp(xjL)1[L2x]!
d dw:
(9)
Since we assume parameter-consistent determinism, the in-
duced treexremains ﬁxed when ranging over ;w. To-
gether with parameter independence, this allows us to pull
the integrals into the products, yielding
BPC(G) =Y
S2V0
@Z1
B(S)Y
N2in(S)wSN+n[SN] 1
SNdwS1
A
Y
L2V Z
p(L)Y
x2DLp(xjL) dL!
; (10)
wheren[SN]:=P
x2D1[SN2x]counts how often the
edge between SandNappears in an induced tree through-
out the dataset, and DL:=fx2DjL2xgis the sub-
dataset of samples where Lappears in the corresponding
induced tree.
The ﬁrst line of (10) takes essentially the same form as
eq. (23) in Heckerman et al. (1995) and yields the widely
know Bayes-Dirichlet score
Y
S2V0
@ (S)
 (n[S] +S)Y
N2in(S) (n[SN] +SN)
 (SN)1
A;(11)
whereS:=P
N2in(S)SNandn[S]:=P
N2in(S)n[SN].
The factors in the second line of (10) are exactly eq. (2),
which we assumed to be tractable. For instance, if the
leaves are categorical, the solution is again the Bayes-
Dirichlet score; if they are Gaussian, we can use the
Bayesian-Gaussian score (Geiger & Heckerman, 1994).
4 BAYESIAN INFORMATION
CRITERION
As an alternative to the Bayesian score, we also adapt the
Bayesian information criterion (BIC) as a structure scorefor PCs. In general, given a graph structure Gand a training
setD, the BIC score is deﬁned as
BIC(G) = LL(G;D) logjDj
2kGk; (12)
where LL(G;D)is the log-likelihood under the graph struc-
tureGwhen using maximum likelihood parameters (poten-
tially with Laplace smoothing), jDjis the size ofDandkGk
is the number of independent parameters encoded in G. The
second termlogjDj
2kGk is essentially a regularization term
penalizing complex structures. BIC is often used as an al-
ternative to BD and for Bayesian networks its asymptotic
consistency has been shown (Koller & Friedman, 2009).
The BIC score has been used to learn CNets in Di Mauro et
al. (2015). However, the authors deﬁned kGk as the num-
ber of conditioning nodes in the CNet, but did notcount
the parameters of the contained Chow-Liu trees (see next
section for details), which actually undercounts the inde-
pendent parameters in the corresponding PC and weakens
the penalty. In our experiments, we show that the corrected
BIC score reﬂects better generalization performance and
outperforms Di Mauro et al. (2015).
5 SCORE-BASED CUTSET LEARNING
Ideally, we would like to use our structure scores to ﬁnd
aglobal optimum among all deterministic PCs, which is
presumably NP-hard. Thus, in order to use our scores for
structure learning, we need a method to “navigate” through
the space of deterministic PCs. Cutset learning (Rahman
et al., 2014) is an attractive method to this end, since it per-
forms greedy search using large structural changes, making
it fast and effective. In this section, we provide the required
background and adapt cutset learning to our purposes, i.e.,
guide the search by our structure scores. We will for sim-
plicity consider binary data, although cutset learning can
be generalized to continuous data as well.
Chow-Liu Trees. A Chow-Liu tree (CLT) is a Bayesian
network (Koller & Friedman, 2009), denoted as (T;)
whereTis a directed tree (meaning each node has
at most one parent) over random variables Xand
 =fXj(X)gX2Xis a collection of conditional prob-
ability tables (CPTs), where (X)denotes the parent of
XinT. The Chow-Liu Algorithm (Chow & Liu, 1968)
learns the tree structure Tby running a maximum span-
ning tree algorithm (Kruskal, 1956) on the fully-connected
graph over X, weighted with the mutual information be-
tween pairs of variables, which is estimated from data. As
shown by Chow & Liu, this spanning tree yields a maxi-
mizer of the log-likelihood structure score (Koller & Fried-
man, 2009), making the Chow-Liu algorithm an early ex-
ample of a principled structure learning algorithm. The
edges of maximal spanning tree are then directed and 
is estimated using maximum likelihood.Bayesian Structure Scores for Probabilistic Circuits
X1 X4T4
X4 X1T5X4
X1X2T1
X1 X5
X4T2
X5 X4
X1T3
X2
0.2 0.8C4X2
0.5 0.5C3
X5
0.4 0.6C2X30.3 0.7C1
Figure 1: A CNet over binary random variables
X=fX1;X2;X3;X4;X5g. Inner nodesfC1;C2;C3;C4g
are decision nodes whose left (resp. right) branches rep-
resent conditioning on state 0(resp. 1). The leaves of the
CNet are CLTsf(Ti;i)g5
i=1.
Cutset Networks. Cutset networks (CNets) (Rahman et
al., 2014) improve CLTs by embedding them in a hierar-
chical conditioning process. A CNet is a binary decision
tree, whose decision nodes correspond to some variable in
Xand whose leaves are CLTs over the undecided variables,
i.e., the variables not appearing in any decision node on the
unique path from root to leaf. Further, the outgoing edges
of decision nodes are equipped with normalized weights.
For any sample x, the probability assigned by the CNet is
the product of weights on the path from root to the selected
CLT leaf (following decisions according to the values in x),
times the probability the CLT assigns to the undecided vari-
ables. An example CNet is shown in Figure 1.
CNets have been extensively studied in literature since they
deliver simple, effective and fast structure learning algo-
rithms (Rahman et al., 2014; Di Mauro et al., 2015, 2017;
Dang et al., 2020; Di Mauro et al., 2021). CNets can
be converted into smooth, deterministic and decomposable
PCs (Dang et al., 2020; Di Mauro et al., 2021), making
them amenable to our structure scores.
Pseudo code for cutset learning with structure scores is
shown in Algorithm 1, which recursively selects variables
to condition upon. Our strategy is to select a variable,
which, when used in a new decision node with two con-
ditional CLT leaves, would increase our score most. This
selection is done in S ELECT BESTCUT, which uses either
our Bayesian or BIC score of the corresponding PC.4As
testing every unconditioned variable increases runtime lin-
early in the overall number of variables, we restrict the
selection to the set of the most promising candidates
(SELECT BESTCANDIDATES ) according to the information
gain heuristic in Rahman et al. (2014). This heuristic, akin
tobeam search , did not impact performance but yielded
4In our implementation we avoided explicit compilation to
PCs, but performed the corresponding computations directly
within the CNet.Algorithm 1 CUTSET STRUCTURE LEARNING
Require: A training setD=fx(n)gN
n=1over binary RVs X;
number of candidate conditioning nodes .
Ensure: a CNetCrepresenting a deterministic PC.
1:function CUT(X;D;)
2:T LEARN CLT(X;D)
3: ~X SELECT BESTCANDIDATES (X;D;)
4:X;C;Dl;Dr SELECT BESTCUT(~X;D)
5: ifS(T)>S(C)then
6: returnT
7: else
8:C CNET(X;CUT(XnX;Dl;);
CUT(XnX;Dr;))
9: returnC
10: end if
11:end function
12:function SELECT BESTCUT(~X;D)
13: for allX2~Xdo
14:Dl;Dr SPLIT(X;D)
15:Tl LEARN CLT(XnX;Dl)
16:Tr LEARN CLT(XnX;Dr)
17:C CNET(X;Tl;Tr)
18:S(C) SCORE (C)
19: end for
20:X bestXwith the highest S(C)
21: returnX;C;Dl;Dr
22:end function
substantial runtime improvements. Our structure learner
stops as soon as no improvement of the score is achieved.
6 STRUCTURAL
EXPECTATION-MAXIMIZATION
Since the Bayesian structure score is the (marginal) like-
lihood of the structure, it can naturally be incorpo-
rated in larger probabilistic frameworks such as structural
expectation-maximization (Friedman, 1998). Speciﬁcally,
we consider mixtures of CNets of the form
pmix(x) =KX
k=1akp(xj;w;G)
whereak0andP
kak= 1, andKis the number of
components. We initialize the model by clustering the data
set intoKclusters using k-means, and training individ-
ual CNets on each portion. The structural E-step consists
of computing the responsibilities of each component pro-
portional to k/akp(xjG), where we use the posterior
predictive distribution to average over parameters. The re-
sponsibilities are then used in the structural M-step as (i)
weighted average to update akand (ii) as fractional sam-
ples within our cutset learner. It should be noted that also
other (heuristic) structure learners can be employed in such
a scheme, as they presumably also optimize the structure
likelihood in some indirect way.Yang Yang, Gennaro Gala, Robert Peharz
7 EXPERIMENTS
We evaluate our approach on both density estimation and
image completion tasks, where for simplicity we focus on
binary data. We use the full Bayesian score—yielding the
BDeu score (Heckerman et al., 1995) in the context of bi-
nary data, denoted as CNetBD—and the BIC score, de-
noted as CNetBIC. We limit the number of candidate
conditioning nodes in these two approaches to ten.
The following structure learners of PCs from literature are
used as competitors: the original cutset network (CNet)
(Rahman et al., 2014), XCNet (Di Mauro et al., 2017),
dCSN (Di Mauro et al., 2015), Strudel (Dang et al.,
2020), LearnSPN (Gens & Domingos, 2013) and ID-SPN
(Rooshenas & Lowd, 2014). All cutset learners except
for dCSN are developed in Python using the DeeProb-kit
library (Loconte & Gala, 2022). For LearnSPN, we re-
port results from (Gens & Domingos, 2013). For all other
structure learners, we use the open-source implementations
from respective authors.
7.1 Standard Density Estimation Benchmarks
As a ﬁrst experiment, we evaluate all structure learning
approaches as density estimators on twenty commonly-
used benchmark data sets. For CNetBD and CNetBIC, the
equivalent sample size (ESS, uniform Dirichlet parameter
) and the Laplace smoothing factor ( ) are the only hyper-
parameters, respectively. For all data sets, we ﬁx = 0:1
for CNetBD and = 0:01for CNetBIC, rendering our
methods are de facto hyper-parameter-free.
Moreover, the Bayesian nature of our scores should effec-
tively protect against overﬁtting and directly correspond to
generalization. Hence, to demonstrate this beneﬁt, we train
on all available data, without the need of a separate valida-
tion set. We apply the same procedure (ﬁxed hyperparame-
ter, no validation set) to the more simple structure learners
CNet, XCNet and dCSN, and provide the more “sophisti-
cated” structure learners Strudel, LearnSPN, and ID-SPN
with a validation set.
Speciﬁcally, we re-train CNet on all data sets using the
hyper-parameters reported in Rahman et al. (2014). In the
case of XCNet and dCSN, we set = 0:5,= 500 (the
minimum number of samples to decompose) and = 3
(the minimum number of features to decompose). These
hyper-parameter values are expected to perform well on all
data sets and are used as default values in their open-source
implementations. Due to the randomness of XCNet, we use
the average test log-likelihood and report the average time
on ten different runs.
The other structure learners—Strudel, LearnSPN and ID-
SPN—have more complex learning strategies. Their im-
plementations or reported results all rely on cross-tuningon a separate validation set. Therefore, we use the original
training and validation splits for these methods, rather than
the combined data.
Ideally, we wish to learn density estimators that are both
accurate in terms of the test log-likelihood and efﬁcient
in terms of the training time. Therefore, we consider the
trade-off between the learning time and the accuracy as a
multi-objective optimization problem. Figure 2 shows vi-
sualizations of the learning time and test log-likelihoods of
all structure learners for each data set. We plot a Pareto
frontier as a dashed line for each data set. The Pareto-
efﬁcient methods on the line are emphasized with ampli-
ﬁed “aura” markers. From Figure 2, we observe that both
CNetBD and CNetBIC appear to be Pareto-efﬁcient on
most data sets. Compared to other cutset learners with
the same greedy learning strategy, CNetBD and CNetBIC
achieved leading performance among fast methods. In
comparison with more complex learners, CNet and CNet-
BIC still outperform or are on par with Strudel and Learn-
SPN on many data sets.
The detailed results for each method are summarized in
Table 1. Here we also include the mixture model based
learned with structural EM (EM-CNetBD) to compare
with the “complex structure learners.” Since the mixture
model has the number of mixture components as a hyper-
parameter, we used the same split in training and valida-
tion sets as used for the complex learners, and considered
K2f2;3;5;8;10;20g. We see that mixtures learned with
structural EM consistently improved over single models.
Moreover, our mixtures are close to or improve state-of-
the-art PCs. Additional results can be found in Appendix C.
7.2 Binary MNIST
We further evaluate our models on the Binary-MNIST data
set (Larochelle & Murray, 2011). We compare against
Einsum networks (EiNets) (Peharz, Lang, et al., 2020),
which allows large-scale training due to its tensor-based
implementation. CNetBD (50K parameters) achieves a test
log-likelihood of 118:71, and a mixture model based on
CNetBD with 300 components (470K parameters) has a
test log-likelihood of  103:95. In comparison, an Ein-
Net generally has an rather large number of parameters,
e.g. EinNet (1.2M parameters) has a test log-likelihood of
 113:55and a larger EinNet (84M parameters) achieves
 99:82.
Figure 3 shows image generation from CNetBD and
EM models with different numbers of components. We
can clearly observe that the generated images become
more plausible with increasing the number of components.
Moreover, Figure 3 shows in inpainting experiments using
approximate MPE inference (Poon & Domingos, 2011),
where we can see the reconstructions of covered images
from mixture models are plausible.Bayesian Structure Scores for Probabilistic Circuits
100102-6.10-6.05NLTCS
101103-6.12-6.10-6.08-6.05MSNBC
101103-2.40-2.30-2.20KDD
101103-14.00-13.50-13.00-12.50Plants
100102104-44.00-42.00-40.00Audio
100102104-80.00-70.00-60.00-50.00Jester
100102104-65.00-60.00-55.00Netﬂix
100102104-32.00-30.00-28.00-26.00Accidents
100102-13.00-12.00-11.00Retail
101103-28.00-26.00-24.00-22.00Pumsb-star
100102-90.00-85.00-80.00DNA
101103-12.00-11.00Kosarek
101103-12.00-11.00-10.00MSWeb
101103-50.00-45.00-40.00-35.00Book
101103-65.00-60.00-55.00EachMovie
101103-180.00-160.00WebKB
101103-110.00-100.00-90.00-80.00Reuters-52
102104-180.00-160.0020Newsgroup
101103105-325.00-300.00-275.00-250.00BBC
101103105-60.00-40.00-20.00Ad
Learning Time (s)Test Log-likelihood
CNetBD CNetBIC CNet XCNet dCSN Strudel LearnSPN ID-SPN
Figure 2: Test log-likelihoods (in nats) vs. learning times of all structure learners on 20 benchmark data sets. The optimal
learners on the Pareto curve are highlighted with ampliﬁed ”aura” markers.
Data Set CNet XCNet dCSN CNetBD CNetBIC Strudel LearnSPN ID-SPN EM-CNetBD
NLTCS -6.113 -6.059 -6.048 -6.064 -6.043 -6.068 -6.110 -6.030 -6.032
MSNBC -6.057 -6.061 -6.057 -6.052 -6.046 -6.046 -6.113 -6.065 -6.041
KDD -2.185 -2.227 -2.371 -2.240 -2.201 -2.173 -2.182 -2.150 -2.159
Plants -13.187 -13.381 -13.312 -13.713 -13.466 -13.956 -12.977 -12.687 -12.902
Audio -44.571 -43.022 -42.810 -42.210 -41.927 -42.319 -40.503 -39.950 -39.943
Jester -61.670 -56.173 -55.506 -55.454 -55.231 -55.255 -75.989 -52.886 -52.923
Netﬂix -65.572 -59.492 -58.877 -58.820 -58.649 -58.659 -57.328 -56.575 -56.604
Accidents -30.860 -31.720 -29.880 -30.568 -30.497 -29.633 -30.038 -26.876 -29.919
Retail -11.043 -11.595 -13.282 -10.940 -10.939 -10.908 -11.043 -10.913 -10.927
Pumsb-star -24.497 -25.575 -23.900 -24.361 -24.513 -27.935 -24.781 -22.251 -24.009
DNA -90.143 -88.199 -87.621 -87.643 -87.642 -88.167 -82.523 -80.550 -85.437
Kosarek -11.197 -12.165 -12.462 -10.999 -10.991 -10.993 -10.989 -10.600 -10.695
MSWeb -10.060 -11.830 -10.909 -10.012 -10.039 -10.192 -10.252 -9.797 -9.963
Book -37.293 -43.550 -48.116 -37.181 -37.143 -35.841 -35.886 -34.200 -34.854
EachMovie -58.153 -63.420 -61.946 -61.400 -59.365 -60.503 -52.485 -52.588 -53.378
WebKB -161.650 -182.453 -171.743 -161.634 -164.611 -159.989 -158.204 -151.680 -157.053
Reuters-52 -88.386 -106.251 -94.102 -87.766 -89.461 -91.778 -85.067 -83.954 -84.993
20Newsgroup -162.488 -184.297 -181.165 -163.671 -162.227 -160.786 -155.925 -152.37 -155.002
BBC -259.870 -286.869 -273.465 -320.052 -268.861 -260.084 -250.687 -249.12 -258.691
Ad -16.436 -60.120 -15.774 -15.676 -16.161 -16.521 -19.733 -26.85 -14.923
Table 1: Average test log-likelihoods (in nats) for all structure learners. The cutset learners are in the left group and other
structure learners are in the right group. The model with the best performance in each group is highlighted in bold.
(a) Sampling results.
 (b) Inpainting results.
Figure 3: Samples (a) and inpainting results (b) for Binary-MNIST for CNetBD (left), EM-CNetBD with 100 components
(middle) and 300 components (right). In (b), the original test images are shown in the top row; then in alternating rows the
images with missing part and reconstructions are shown.Yang Yang, Gennaro Gala, Robert Peharz
8 CONCLUSION
Structure learning is a challenging task in PCs. While many
techniques have been proposed in recent years, most of
them are based on heuristics rather than on principled learn-
ing objectives. This is in stark contrast to classical PGMs,
where constraint-based and score-based approaches have
been used. Our main motivation in this paper is to establish
such principled approaches also for PC structure learning.
In particular, we derived the Bayesian score known from
the PGM literature to structure learning of deterministic
PCs, and furthermore proposed a correction to the BIC
score for PCs. Both scores were used in a simple structure
learner and were allowed to “speak for themselves:” no ex-
tensive cross-validation and no other defenses against over-
ﬁtting, such as early stopping, were required. The results
demonstrate that such a simple and principled approach can
be effective and efﬁcient. We hope that our results stimu-
late further research in this direction.
Our approach is appealing both from a theoretical and prac-
tical perspective, since our score is derived from probabilis-
tic considerations, while being analytically tractable and
fast in practice. The main caveat is that our score is re-
stricted to deterministic circuits. Future directions, how-
ever, might include approximations of these scores to gen-
eral (non-deterministic) PC, using e.g. sampling (Vergari et
al., 2019; Trapp et al., 2019), variational techniques (Zhao,
Adel, et al., 2016), continuous relaxations (Lang et al.,
2022), or numeric integration (Correia et al., 2022).
Further interesting directions are studying the theoretical
properties of Bayesian scores for PCs. For example, for
PGMs, consistency of both the Bayesian and BIC score
has been shown, meaning that they will identify the correct
structure in the inﬁnite data limit, provided that there ex-
ists a PGM which is a perfect map for the data-generating
distribution (Koller & Friedman, 2009). It is likely that
such properties also hold for PCs, which might explain the
similar performance of the BD and BIC scores in our ex-
periments. However, rigorously establishing these results
is left to future work.
Acknowledgements
We thank the Eindhoven Artiﬁcial Intelligence Systems In-
stitute (EAISI) for its support. This research was supported
by the Graz Center for Machine Learning (GraML). We
thank YooJung Choi, Antonio Vergari, and Martin Trapp
for helpful discussions.
References
Adel, T., Balduzzi, D., & Ghodsi, A. (2015). Learning the
structure of sum-product networks via an SVD-based
algorithm. In UAI (pp. 32–41).Ahmed, K., Teso, S., Chang, K.-W., den Broeck, G. V ., &
Vergari, A. (2022). Semantic probabilistic layers for
neuro-symbolic learning. In A. H. Oh, A. Agarwal,
D. Belgrave, & K. Cho (Eds.), Advances in Neural
Information Processing Systems.
Buntine, W. (1991). Theory reﬁnement on Bayesian net-
works. In Uncertainty Proceedings 1991 (pp. 52–
60).
Choi, Y ., Vergari, A., & Van den Broeck, G. (2020). Prob-
abilistic circuits: A unifying framework for tractable
probabilistic models. UCLA. URL: http://starai. cs.
ucla. edu/papers/ProbCirc20. pdf .
Chow, C., & Liu, C. (1968). Approximating discrete prob-
ability distributions with dependence trees. Transac-
tions on Information Theory ,14(3), 462–467.
Cooper, G. F., & Herskovits, E. (1992). A Bayesian method
for the induction of probabilistic networks from data.
Machine Learning ,9(4), 309–347.
Correia, A., Gala, G., Quaeghebeur, E., de Campos,
C., & Peharz, R. (2022). Continuous mixtures
of tractable probabilistic models. arXiv preprint
arXiv:2209.10584 .
Dang, M., Vergari, A., & Broeck, G. (2020). Strudel:
Learning structured-decomposable probabilistic cir-
cuits. In International Conference on Probabilistic
Graphical Models (pp. 137–148).
Darwiche, A. (2003). A differential approach to inference
in Bayesian networks. Journal of the ACM (JACM) ,
50(3), 280–305.
Darwiche, A., & Marquis, P. (2002). A knowledge com-
pilation map. Journal of Artiﬁcial Intelligence Re-
search ,17, 229–264.
De Campos, C. P., Zeng, Z., & Ji, Q. (2009). Structure
learning of Bayesian networks using constraints. In
International Conference on Machine Learning (pp.
113–120).
Di Mauro, N., Gala, G., Iannotta, M., & Basile, T. M.
(2021). Random probabilistic circuits. In Uncer-
tainty in Artiﬁcial Intelligence (pp. 1682–1691).
Di Mauro, N., Vergari, A., Basile, T., & Esposito, F. (2017).
Fast and accurate density estimation with extremely
randomized cutset networks. In Joint European Con-
ference on Machine Learning and Knowledge Dis-
covery in Databases (pp. 203–219).
Di Mauro, N., Vergari, A., & Esposito, F. (2015). Learn-
ing accurate cutset networks by exploiting decom-
posability. In Congress of the Italian Association for
Artiﬁcial Intelligence (pp. 221–232).
Friedman, N. (1998). The Bayesian structural EM algo-
rithm. In Conference on Uncertainty in Artiﬁcial In-
telligence (pp. 129–138).
Friedman, N., & Koller, D. (2003). Being Bayesian about
network structure. A Bayesian approach to structure
discovery in Bayesian networks. Machine Learning ,
50(1), 95–125.Bayesian Structure Scores for Probabilistic Circuits
Geiger, D., & Heckerman, D. (1994). Learning Gaus-
sian networks. In Uncertainty Proceedings 1994 (pp.
235–243).
Gens, R., & Domingos, P. (2013). Learning the structure of
sum-product networks. In International Conference
on Machine Learning (pp. 873–880).
Ghahramani, Z. (2015). Probabilistic machine learning and
artiﬁcial intelligence. Nature ,521(7553), 452–459.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995).
Learning Bayesian networks: The combination of
knowledge and statistical data. Machine Learning ,
20(3), 197–243.
Khosravi, P., Liang, Y ., Choi, Y ., & Van den Broeck, G.
(2019). What to expect of classiﬁers? Reasoning
about logistic regression with missing features. arXiv
preprint arXiv:1903.01620 .
Kisa, D., Van den Broeck, G., Choi, A., & Darwiche,
A. (2014). Probabilistic sentential decision dia-
grams. In International Conference on the Principles
of Knowledge Representation and Reasoning.
Koller, D., & Friedman, N. (2009). Probabilistic graphical
models: principles and techniques . MIT press.
Kruskal, J. B. (1956). On the shortest spanning subtree of a
graph and the traveling salesman problem. Proceed-
ings of the American Mathematical society ,7(1), 48–
50.
Lang, S., Mundt, M., Ventola, F., Peharz, R., & Kerst-
ing, K. (2022). Elevating perceptual sample quality
in PCs through differentiable sampling. In NeurIPS
2021 workshop on pre-registration in machine learn-
ing(V ol. 181, pp. 1–25).
Larochelle, H., & Murray, I. (2011). The neural autoregres-
sive distribution estimator. In International Confer-
ence on Artiﬁcial Intelligence and Statistics (pp. 29–
37).
Liu, A., Zhang, H., & Broeck, G. V. d. (2022). Scal-
ing up probabilistic circuits by latent variable distil-
lation. arXiv preprint arXiv:2210.04398 .
Loconte, L., & Gala, G. (2022). Deeprob-kit: a python
library for deep probabilistic modelling. arXiv
preprint arXiv:2212.04403 .
Lowd, D., & Domingos, P. (2008). Learning arithmetic
circuits. In Conference on Uncertainty in Artiﬁcial
Intelligence (p. 383-392).
Peharz, R., Geiger, B. C., & Pernkopf, F. (2013). Greedy
part-wise learning of sum-product networks. In
Joint European Conference on Machine Learning
and Knowledge Discovery in Databases (pp. 612–
627).
Peharz, R., Gens, R., & Domingos, P. (2014). Learning
selective sum-product networks. In ICML Workshop
on Learning Tractable Probabilistic Models.
Peharz, R., Gens, R., Pernkopf, F., & Domingos, P. (2016).
On the latent variable interpretation in sum-product
networks. IEEE Transactions on Pattern Analysisand Machine Intelligence ,39(10), 2030–2044.
Peharz, R., Lang, S., Vergari, A., Stelzner, K., Molina, A.,
Trapp, M., . . . Ghahramani, Z. (2020). Einsum net-
works: Fast and scalable learning of tractable proba-
bilistic circuits. In International Conference on Ma-
chine Learning (pp. 7563–7574).
Peharz, R., Tschiatschek, S., Pernkopf, F., & Domingos,
P. (2015). On theoretical properties of sum-product
networks. In International Conference on Artiﬁcial
Intelligence and Statistics (pp. 744–752).
Peharz, R., Vergari, A., Stelzner, K., Molina, A., Shao, X.,
Trapp, M., . . . Ghahramani, Z. (2020). Random
sum-product networks: A simple and effective ap-
proach to probabilistic deep learning. In Conference
on Uncertainty in Artiﬁcial Intelligence (pp. 334–
344).
Pipatsrisawat, K., & Darwiche, A. (2008). New compila-
tion languages based on structured decomposability.
InAAAI Conference on Artiﬁcial Intelligence (V ol. 8,
pp. 517–522).
Poon, H., & Domingos, P. (2011). Sum-product networks:
A new deep architecture. In 2011 IEEE International
Conference on Computer Vision Workshops (ICCV
Workshops) (pp. 689–690).
Rahman, T., Kothalkar, P., & Gogate, V . (2014). Cut-
set networks: A simple, tractable, and scalable ap-
proach for improving the accuracy of chow-liu trees.
InJoint European Conference on Machine Learning
and Knowledge Discovery in Databases (pp. 630–
645).
Rashwan, A., Poupart, P., & Zhitang, C. (2018). Discrimi-
native training of sum-product networks by extended
Baum-Welch. In International Conference on Prob-
abilistic Graphical Models (pp. 356–367).
Rashwan, A., Zhao, H., & Poupart, P. (2016). Online
and distributed Bayesian moment matching for pa-
rameter learning in sum-product networks. In In-
ternational Conference on Artiﬁcial Intelligence and
Statistics (pp. 1469–1477).
Rooshenas, A., & Lowd, D. (2014). Learning sum-
product networks with direct and indirect variable in-
teractions. In International Conference on Machine
Learning (pp. 710–718).
Schwarz, G. (1978). Estimating the dimension of a model.
The Annals of Statistics , 461–464.
Shao, X., Molina, A., Vergari, A., Stelzner, K., Peharz, R.,
Liebig, T., & Kersting, K. (2022). Conditional sum-
product networks: Modular probabilistic circuits via
gate functions. International Journal of Approximate
Reasoning ,140, 298–313.
Trapp, M., Peharz, R., Ge, H., Pernkopf, F., & Ghahra-
mani, Z. (2019). Bayesian learning of sum-product
networks. Advances in Neural Information Process-
ing Systems ,32.
Vergari, A., Choi, Y ., Liu, A., Teso, S., & Van den Broeck,Yang Yang, Gennaro Gala, Robert Peharz
G. (2021). A compositional atlas of tractable circuit
operations for probabilistic inference. Advances in
Neural Information Processing Systems ,34, 13189–
13201.
Vergari, A., Choi, Y ., Peharz, R., & Van den Broeck, G.
(2020). Probabilistic circuits: Representations, in-
ference, learning and applications. In Tutorial at the
The 34th AAAI Conference on Artiﬁcial Intelligence.
Vergari, A., Molina, A., Peharz, R., Ghahramani, Z., Ker-
sting, K., & Valera, I. (2019). Automatic Bayesian
density analysis. In AAAI Conference on Artiﬁcial
Intelligence (V ol. 33, pp. 5207–5215).
Zhao, H., Adel, T., Gordon, G., & Amos, B. (2016).
Collapsed variational inference for sum-product net-
works. In International Conference on Machine
Learning (pp. 1310–1318).
Zhao, H., Poupart, P., & Gordon, G. J. (2016). A uniﬁed
approach for learning the parameters of sum-product
networks. Advances in Neural Information Process-
ing Systems ,29.Bayesian Structure Scores for Probabilistic Circuits
A Learning a Chow-Liu Tree
The algorithm L EARN CLT for learning a Chow-Liu tree is shown in Algorithm 2. Overall, for a dataset D, the algorithm
has time complexity O(jXj2jDj).
Algorithm 2 LEARN CLT
Require: A training setD=fx(n)gN
n=1over RVs X, equivalent sample size (ESS) .
Ensure: a parameterized Chow-Liu tree T
1:function LEARN CLT(D;)
2: MI COMPUTE MUTUAL INFORMATION (X)
3:T0 COMPUTE MAXIMUM SPANNING TREE(MI)
4:T  ASSIGN EDGEDIRECTIONS (T0)
5:T  COMPUTE BAYESIAN POSTERIOR PARAMETERS (D;T;)
6: returnT
7:end function
B Candidate Selection Heuristic
We adapt the information gain heuristic introduced by Rahman et al. (2014) to restrict the conditioning node selection
to a ﬁxed number of candidates. Intuitively, the heuristic wishes to select variables with maximum information gain or
expected entropy reduction by conditioning on the variables. Concretely, the entropy ^H(D)of a data setD=fx(n)gN
n=1
over random variables Xis deﬁned as
^H(D) =1
jXjX
X2XHD(X); (13)
whereHD(X)is the entropy of RV XgivenD, which is given by
HD(X) = X
x2Xp(x) logp(x); (14)
whereXis the set of all possible states of Xwhen assuming Xis discrete. Based on the entropy of the data, the information
gainGain(Xi)after conditioning on an RV Xiis deﬁned as
Gain(Xi) =^H(D) X
xi2X ijDxij
jDj^H(Dxi); (15)
whereDxi=fx(n)2Djx(n)
i=xig. In S ELECT BESTCANDIDATES , we select top RVs with the maximum information
gain to form a candidate set ~X.
C Additional Experimental Results
C.1 Data Sets
The characteristics of the 20 benchmark density estimation data sets are reported in Table 2. The number of variables
ranges from 16to1556 .
C.2 Learning Time
To evaluate the efﬁciency of different structure learners, we report their learning times in Table 3, which are all measured
in total seconds during training. Combining with the test log-likelihoods of various structure learners, we can observe both
CNetBD and CNetBIC achieve a better trade-off between the performance and the training time.
C.3 Circuit Size
Table 4 summarizes the circuit size of models learned by different cutset structure learners. Clearly, CNetBD and CNetBIC
deliver much smaller circuits than other cutset learners on almost all data sets. In addition, compare to other cutset learners,
CNetBD and CNetBIC often achieve leading performance. These further validate our theoretical analysis that both the
BDeu and the BIC score can effectively protect against overﬁtting.Yang Yang, Gennaro Gala, Robert Peharz
Dataset #Features #Train #Valid. #Test
NLTCS 16 16181 2157 3236
MSNBC 17 291326 38843 58265
KDD 64 180092 19907 34955
Plants 69 17412 2321 3482
Audio 100 15000 2000 3000
Jester 100 9000 1000 4116
Netﬂix 100 15000 2000 3000
Accidents 111 12758 1700 2551
Retail 135 22041 2938 4408
Pumsb-star 163 12262 1635 2452
DNA 180 1600 400 1186
Kosarek 190 33375 4450 6675
MSWeb 294 29441 3270 5000
Book 500 8700 1159 1739
EachMovie 500 4525 1002 591
WebKB 839 2803 558 838
Reuters-52 889 6532 1028 1540
20Newsgroup 910 11293 3764 3764
BBC 1058 1670 225 330
Ad 1556 2461 327 491
Table 2: Characteristics of the benchmark density estimation data sets
Data Set CNet XCNet dCSN CNetBD CNetBIC Strudel LearnSPN ID-SPN
NLTCS 0.8 0.1 5.6 0.7 1.1 500 623 48
MSNBC 6.1 1.3 50.2 10.3 13.5 8374 5843 4604
KDD 4.4 6.0 563.4 25.6 35.7 697 9943 5126
Plants 0.8 0.7 35.9 6.8 4.4 5999 2318 986
Audio 2.3 0.4 80.8 4.4 4.9 1476 3977 1920
Jester 2.8 0.2 62.6 3.3 3.3 1239 3946 1020
Netﬂix 4.2 0.3 92.0 4.7 4.1 966 5062 2485
Accidents 0.8 0.4 76.5 6.5 4.2 2284 681 1102
Retail 0.4 1.9 530.6 1.3 1.1 276 671 601
Pumsb-star 1.1 0.9 146.4 14.5 5.5 496 684 1132
DNA 0.4 0.1 26.1 0.8 0.3 382 339 279
Kosarek 1.3 6.3 1658.7 13.1 8.5 1381 1068 1485
MSWeb 0.9 16.1 2474.1 14.8 8.3 188 1404 1279
Book 2.1 12.0 10729.9 26.2 14.7 2742 4137 9355
EachMovie 2.6 10.1 1360.6 79.4 21.4 3318 406 9014
WebKB 3.0 9.8 3800.4 71.2 50.0 3069 2616 6943
Reuters-52 3.6 36.0 8156.3 172.6 68.9 7166 4166 12653
20Newsgroup 9.7 47.5 24660.7 403.3 122.1 8303 44341 25637
BBC 2.5 6.8 4718.8 1175.7 68.1 765 20199 6348
Ad 3.4 193.7 34576.8 246.5 17.3 361 969 2080
Table 3: Learning times (in seconds) of all structure learners. Due to the randomness of XCNet, we use the average learning
time on 10 different runs.
Data Set CNet XCNet dCSN CNetBD CNetBIC
NLTCS 4543 1089.4 1017 205 315
MSNBC 20143 5414.0 7069 1695 1865
KDD 8601 47638.4 47888 2823 1651
Plants 18605 21263.8 8532 3399 3163
Audio 54651 15166.0 12530 1549 3437
Jester 72873 7037.6 6000 1359 2689
Netﬂix 100497 10412.6 8258 1741 2881
Accidents 13763 15273.6 9345 2355 2775
Retail 3667 52036.2 85730 269 269
Pumsb-star 16023 32990.8 12203 4129 3187
DNA 5625 2329.0 1767 359 359
Kosarek 10725 175426.0 166168 1875 1875
MSWeb 5801 455462.6 158034 2333 1753
Book 13887 219122.2 166226 2989 3981
EachMovie 16857 185578.2 18787 8931 6959
WebKB 10035 86246.6 16692 5023 10035
Reuters-52 10635 334000.0 35350 10633 12403
20Newsgroup 30783 402505.8 63209 23531 19939
BBC 6337 43752.4 12646 58849 10551
Ad 6219 1076052.0 77207 9325 3111
Table 4: Circuit sizes (measured in the number of independent parameters) of all structure learners. Due to the randomness
of XCNet, we use the average circuit size on 10 different runs.