Exploring Visual Prompts for Whole Slide Image
ClassiÔ¨Åcation with Multiple Instance Learning
Yi Liny1, Zhongchen Zhaoy2, Zhengjie ZHU1, Lisheng WangB2, Kwang-Ting
Cheng1, and Hao ChenB1
1The Hong Kong University of Science and Technology, Hong Kong, China.
2Shanghai Jiao Tong University, China.
lswang@sjtu.edu.cn ;jhc@cse.ust.hk
Abstract. Multiple instance learning (MIL) has emerged as a popular
method for classifying histopathology whole slide images (WSIs). How-
ever, existing approaches typically rely on pre-trained models from large
natural image datasets, such as ImageNet, to generate instance features,
which can be sub-optimal due to the signiÔ¨Åcant diÔ¨Äerences between nat-
ural images and histopathology images that lead to a domain shift. In
this paper, we present a novel, simple yet eÔ¨Äective method for learn-
ing domain-speciÔ¨Åc knowledge transformation from pre-trained models
to histopathology images. Our approach entails using a prompt compo-
nent to assist the pre-trained model in discerning diÔ¨Äerences between the
pre-trained dataset and the target histopathology dataset, resulting in
improved performance of MIL models. We validate our method on two
publicly available datasets, Camelyon16 and TCGA-NSCLC. Extensive
experimental results demonstrate the signiÔ¨Åcant performance improve-
ment of our method for diÔ¨Äerent MIL models and backbones. Upon pub-
lication of this paper, we will release the source code for our method.
Keywords: Visual prompt ¬∑Multiple instance learning ¬∑Whole slide
image ¬∑Deep learning.
1 Introduction
Whole slide images (WSI) play a vital role in histopathology image analysis
and clinical disease diagnosis [6,23,26]. With the advent of deep-learning-based
techniques, histopathology image analysis has undergone a signiÔ¨Åcant transfor-
mation [9,24]. However, there are still challenges when it comes to classify-
ing WSI. Due to their massive size, WSIs cannot be directly fed into typical
deep-learning models. Therefore, WSIs are often divided into patches for pro-
cessing [11]. Unfortunately, annotating patch-level labels is labor-intensive and
time-consuming, which limits the applicability of conventional supervised learn-
ing methods [16,18]. To address this issue, multiple instance learning (MIL) has
emerged as the dominant technique for WSI analysis. In this approach, each
WSI is considered as a bag containing multiple patches (instances), and a WSI
yEqual contribution; Bcorresponding author.arXiv:2303.13122v1  [cs.CV]  23 Mar 20232 Lin et al.
PredictionMIL
ClassifierFeature
Extractor
New Task Pretrained 
(a) Frozen (b) Fine-tuning (c) Prompt LearningPredictionMIL
ClassifierFeature
Extractor
PredictionMIL
ClassifierFeature
ExtractorPrompt
Fig. 1.An illustration of three adapting schemes for WSI classiÔ¨Åcation with MIL: (a)
freezing the pretrained feature extractor while only training the MIL classiÔ¨Åer; (b)
Ô¨Åne-tuning the feature extractor and training the MIL classiÔ¨Åer; (c) only training the
prompt blocks and the MIL classiÔ¨Åer.
bag is labeled negative only if all patches (instances) of this bag are negative.
Conversely, the bag‚Äôs label is positive if at least one of its instances is positive.
Downsampling and feature extraction are necessary due to the large num-
ber of patches in a WSI. The quality of the extracted patch features greatly
inÔ¨Çuences the performance of the subsequent MIL classiÔ¨Åcation. Most existing
methods [13,17,29,19] extract patch features by a frozen feature extractor pre-
trained on the large natural image datasets, such as ImageNet [7], and then
train the MIL classiÔ¨Åer for the WSI prediction, as shown in Fig. 1 (a). However,
such a MIL training scheme overlooks the domain shift issue between natural
and pathological images. To narrow the domain shift, some researchers [28] pro-
pose to use self-supervised pre-training methods such as SimCLR [5] to train
the feature extractor. However, these self-supervised learning methods do not
take full advantage of the bag labels, resulting in limited performance. Another
naive solution is to use partial patches instead of all patches to Ô¨Åne-tune the
pre-trained feature extractor, as illustrated in Fig. 1 (b). However, Ô¨Åne-tuning
all the parameters of the feature extractor using limited patches without patch-
level labels may impair the beneÔ¨Åts from pre-training on large-scale datasets like
ImageNet, which increases the risk of overÔ¨Åtting in downstream tasks.
Inspired by the breakthrough of prompt learning in natural language process-
ing (NLP), we introduce visual prompts to adapt the pre-trained feature extrac-
tor to pathological images, addressing the aforementioned issue. Our prompt
learning framework for MIL-based WSI classiÔ¨Åcation is shown in Fig. 1 (c).
Limited by memory capacity, we propose a feasible solution that selects repre-
sentative pathological images, instead of all patches, to Ô¨Åne-tune the pre-trainedExploring Visual Prompts for MIL-based WSI ClassiÔ¨Åcation. 3
feature extractor. Based on the selected images, we design a prompt component
added to the feature extractor to learn visual prompts, and freeze the backbone
while only training the prompt component with the lightweight MIL classiÔ¨Åer.
In this way, our method can improve the performance of the ImageNet pre-
trained feature extractor and achieve domain transformation to pathological im-
age data. Our method also makes the entire training process highly eÔ¨Écient and
lightweight. To the best of our knowledge, this is the Ô¨Årst work to explore prompt
learning for WSI classiÔ¨Åcation. In summary, our contributions are three-fold:
We,fortheÔ¨Årsttime,introducevisualpromptsintoWSIclassiÔ¨Åcation,which
enables data domain transformation by learning prompt components.
We propose an intuitive but eÔ¨Äective method for end-to-end prompt train-
ing, which involves representative patch selection to reduce the number of
instances in a WSI bag.
We conduct extensive experiments to validate the eÔ¨Äectiveness of the pro-
posed method on two public datasets, i.e., Camelyon16 and TCGA-NSCLC.
Experimental results demonstrate consistent improvements across diÔ¨Äerent
MIL methods and backbones.
2 Related Work
MIL for WSI classiÔ¨Åcation. MIL-based methods [13,17,29,19] have gained
popularity in WSI classiÔ¨Åcation due to their high eÔ¨Äectiveness. These methods
typically involve using a feature extractor to extract features from image patches
of WSI, followed by an aggregation step to obtain a feature representation at
the WSI level. A lightweight classiÔ¨Åer is then employed to predict the WSI cate-
gory [13]. In WSI classiÔ¨Åcation, an eÔ¨Äective feature extractor that generates rep-
resentative feature is crucial for accurate classiÔ¨Åcation results. However, existing
MIL-based methods for WSI classiÔ¨Åcation mainly adopt a pre-trained feature ex-
tractor without Ô¨Åne-tuning, which results in sub-optimal performance [12]. This
is due to the domain shift and task discrepancies between the pre-training task
(e.g., ImageNet) and the downstream task ( e.g., histopathology) [27]. For this is-
sue, we introduce visual prompts for WSI classiÔ¨Åcation, enabling smooth feature
modulation from the upstream dataset to the downstream WSI classiÔ¨Åcation.
Prompt Learning. Prompt learning has recently emerged as a lightweight
and eÔ¨Écient transfer learning paradigm in NLP and has achieved remarkable
success[14].Thefundamentalideabehindpromptlearningistofreezelarge-scale
NLP models, such as BERT [8] and GPT-3 [3], that have been pre-trained on
vastdatasetsandusetask-speciÔ¨Åcpromptstoadaptthemtodiversedownstream
taskswithoutupdatinganyparameters[1].BuildingontheNLPpromptlearning
paradigm, several studies [20,22,4] have proposed to extend prompt learning to
natural images in computer vision. For example, L√ºddecke et al. [20] used text
and image prompts to adapt the frozen pre-trained CLIP model [25] to new
image segmentation tasks. However, the eÔ¨Äectiveness of prompt learning in the
Ô¨Åeld of histopathology analysis is under-investigated.4 Lin et al.
Attention Score
MILClassifierAll Patches
Patch Features
RepresentativePatches
ResNetGAPMLPReLUMLPSigmoidVisual PromptsN√ó
PromptResNetMILClassifierPrediction
ResNetBlockùëÉ!ResNetBlockùëÉ"
ResNetBlockùëÉ#......Prompt Block
TrainableFrozen
Fig. 2.Overview of our method. (I) MIL classiÔ¨Åer training with all patch features. (II)
Representative patch selection. (III) Prompt learning with representative patches.
3 Method
Fig. 2 illustrates the proposed prompt learning framework, which consists of
three primary steps: (I) MIL classiÔ¨Åer training, (II) representative patch selec-
tion, and (III) prompt Ô¨Åne-tuning. First, an ImageNet pre-trained ResNet [10]
is used to extract patch features, which are then used to train the MIL classiÔ¨Åer
for the attention score of each patch. Second, representative patches in each WSI
bag are chosen based on their attention scores to form a new bag. Third, the
representative patches are used to Ô¨Åne-tune the prompt blocks plugged into the
feature extractor and MIL classiÔ¨Åer in an end-to-end manner. we will elaborate
on each step in the following sections.
3.1 Attention-based MIL ClassiÔ¨Åer with Frozen Feature Extractor
In attention-based MIL for WSI classiÔ¨Åcation, the standard training process Ô¨Årst
uses a frozen feature extractor f()pre-trained on ImageNet to extract all patch
features. Then all patch features in a WSI bag are aggregated to form the WSI
feature using the attention mechanism, which learns an attention score kfor
each patch kthrough the MIL classiÔ¨Åer. The WSI feature Fis obtained by
computing the attention-weighted average of all patch features in a WSI as [13]:
F=KX
k=1kf(xk); (1)Exploring Visual Prompts for MIL-based WSI ClassiÔ¨Åcation. 5
where
k=exp
wT(tanh ( V1f(xk))sigmoid ( V2f(xk)))	
KP
j=1expfwT(tanh ( V1f(xk))sigmoid ( V2f(xk)))g;(2)
where w,V1andV2are learnable parameters in the MIL classiÔ¨Åer, is the
element-wise multiplication, and tanh()and sigmoid()denote the tanh and
sigmoid activation function, respectively.
Finally, the MIL classiÔ¨Åer head h()predicts the label of WSI from the WSI
feature F, represented as:
~y=h(F); (3)
where ~ydenotes the prediction of the WSI label. During training, we minimize
the prediction error using the cross-entropy (CE) loss.
3.2 Representative Patch Selection.
In WSI classiÔ¨Åcation, it‚Äôs common for only a small number of patches within a
WSI to be associated with the disease of interest. For example, in the positive
slides of Camelyon16 [2], on average, less than 10% of the patches in a WSI are
tumor patches. Thus, only a few patches are suÔ¨Écient to represent the entire
WSI bag. Based on this observation, we propose a feasible solution that selects
representative pathological images to Ô¨Åne-tune the pre-trained feature extractor
for WSI classiÔ¨Åcation. In attention-based MIL, the patch with a higher attention
scorekin a WSI bag is more likely to have the same category semantics as the
WSI. Thus, based on the patches‚Äô attention score kcalculated by the MIL clas-
siÔ¨Åer, we select the top- Kpatches with the highest attention scores in each WSI
as a new bag. Here, Kis set to 200, which will be discussed in Section 4.2. The
new bag label is assigned as the original WSI bag label for prompt Ô¨Åne-tuning.
In this way, we reduce vast quantities of patches in each WSI to a small subset,
enabling an end-to-end training for both feature extractor and MIL classiÔ¨Åer.
3.3 Prompt Fine-tuning.
Withtheselectedrepresentativepatches,wedesignapromptlearningframework
to adapt the feature extractor from the natural image domain to the pathological
image domain, while retaining the advantage of pre-training on the large and di-
verse ImageNet dataset. SpeciÔ¨Åcally, we design a prompt block that sequentially
consists of a global average pooling (GAP), a multi-layer perceptron (MLP) with
two layers, and a sigmoid activation, as shown in Fig. 2. Given the intermediate
feature map fifrom thei-th block of the feature extractor, the prompt block is
added in parallel to the basic ResNet block gi()to generate the visual prompt
pi2RD, whereDdenotes the dimension of the prompt vector. Subsequently,
the generated prompts piare channel-wise multiplied with feature maps fi+1in
the next block, represented as:
pi= sigmoid( Wi2ReLU( Wi1GAP( fi))); (4)6 Lin et al.
fi+1=gi(fi)pi; (5)
where ReLU ()denotes rectiÔ¨Åed linear unit, Wi1andWi2are the learnable
parameters to be Ô¨Åne-tuned, and the parameters of gi()remain frozen during
training. During the training process, only the parameters of prompt blocks and
the lightweight MIL classiÔ¨Åer are updated in an end-to-end manner, while the
original pre-trained feature extractor is frozen.
4 Experiments
4.1 Datasets
CAMELYON16. The Camelyon16 [2] dataset consists of 399 H&E stained
slides from breast cancer screening, with two classes: normal and tumor. We
employ the oÔ¨Écial 129 testing set, and the oÔ¨Écial 270 training set is further
divided randomly into training and validation sets, with a ratio of 9: 1. After
preprocessing, a total of 4,610,687 patches with the size of 256256are obtained
at20magniÔ¨Åcation, with an average of 11,556 patches per slide.
TCGA-NSCLC. The Cancer Genome Atlasy(TCGA) non-small cell lung can-
cer (NSCLC) [21] dataset is comprised of two subtypes of lung cancer: Lung
Adenocarcinoma (TCGA-LUAD) and Lung Squamous Cell Carcinoma (TGCA-
LUSC). It contains a total of 1,053 WSIs, with 541 LUAD slides and 512 LUSC
slides. We split the dataset into training, validation, and testing sets at the slide
level, with a distribution ratio of 65:10:25. In our study, we extract 256256
patches at 20magniÔ¨Åcation from each WSI. After preprocessing, a total of
3,252,431 patches are obtained, with an average of 3,089 patches per WSI.
4.2 Implementation Details
In our approach, we adopt the ResNet model as the feature extractor, which is
pre-trained on the ImageNet dataset. We remove the last layer of the ResNet
following [19] and add prompt blocks to the third layer. For ResNet-18 and
ResNet-50, we set the number of prompt blocks as 2 and 6, respectively. For
representative patch selection, we select the top 200 patches for each WSI bag
by default. For the prompt Ô¨Åne-tuning, we freeze the backbone of ResNet and
only train the prompt block and MIL classiÔ¨Åer using the Adam optimizer [15]
with a learning rate of 1e-4 and weight decay of 1e-4 for 100 epochs. All the
experiments are conducted with an NVIDIA GeForce RTX 3090 GPU.
4.3 Comparison Results
In this study, we comprehensively evaluate the eÔ¨Äectiveness of our proposed
framework with various experiment settings, including two datasets: Camey-
lon16[2]andTCGA-NSCLC[21];twobackbonenetworks:ResNet-18andResNet-
50; and two MIL classiÔ¨Åers: a state-of-the-art model DTFD [29] and a common
yhttps://www.cancer.gov/tcgaExploring Visual Prompts for MIL-based WSI ClassiÔ¨Åcation. 7
Table 1. Results (%) of comparative experiments on Camelyon-16 and TCGA-NSCLC
dataset. In the table, the best results are in bold. ‚ÄúRPS‚Äù: representative patch selection,
‚ÄúFT‚Äù: Ô¨Åne-tuning, ‚ÄúPT‚Äù: prompt Ô¨Åne-tuning.
Dataset MethodResNet-18 ResNet-50
AUC F1 Acc AUC F1 Acc
Camelyon16DTFD [29] 87.11 78.05 86.05 90.07 81.63 86.05
DTFD-RPS-FT 81.40 69.23 81.58 89.34 81.32 86.82
DTFD-RPS-PT 90.94 78.72 84.50 91.40 83.52 88.37
ABMIL [13] 84.08 74.07 83.72 85.96 77.11 85.27
ABMIL-RPS-FT 83.98 76.19 82.95 83.78 80.90 86.82
ABMIL-RPS-PT 87.45 76.40 83.72 86.7378.6585.27
TCGA
NSCLCDTFD [29] 92.91 87.2287.12 93.47 89.38 89.01
DTFD-RPS-FT 92.97 85.28 85.22 92.27 86.55 85.99
DTFD-RPS-PT 94.1987.16 87.50 93.80 89.71 89.39
ABMIL [13] 93.56 86.05 86.36 93.34 88.39 88.26
ABMIL-RPS-FT 89.76 83.22 81.06 90.85 85.29 84.85
ABMIL-RPS-PT 93.84 87.46 86.74 94.21 89.55 89.39
model ABMIL [13]. We report the area under curve (AUC), accuracy (Acc), and
F1-score as evaluation metrics for WSI classiÔ¨Åcation task. Table 1 shows the
eight diÔ¨Äerent settings of experiments. The Ô¨Årst row in each setting represents
the baseline approach with the frozen feature extractor. The second row (RPS-
FT) represents using the representative patches to Ô¨Åne-tune both the feature
extractor and MIL classiÔ¨Åer. The third row (RPS-PT) represents our prompt
Ô¨Åne-tuning method.
In Table 1, we can observe that our method achieves a consistent improve-
ment compared to both the baseline approach and the Ô¨Åne-tuning approach.
Notably, when using ResNet-18 as the feature extractor, our method achieves
a higher AUC than the DTFD baseline, with an improvement of 3.83% on
the Camelyon16 dataset and 1.28% on the TCGA-NSCLC dataset. Notably,
our method with prompt blocks signiÔ¨Åcantly outperforms the RPS-FT scheme,
demonstrating the advantage of our visual prompts. Besides, our method shows
particularly superior results on the Camelyon16 dataset compared to the results
onthe TCGA-NSCLCdataset. Thiscanbeattributed tothefact thattheCame-
lyon16 dataset has fewer WSIs and a smaller proportion of tumor regions. This
further conÔ¨Årms the advantage of our approach in handling challenging datasets.
Overall, the experiment results indicate that our method eÔ¨Äectively improves the
performance of attention-based MIL methods on the WSI classiÔ¨Åcation tasks.
4.4 Ablation Study
EÔ¨Äectiveness of the prompt block number. To study the impact of the
prompt component number, we conduct experiments on the Camelyon16 dataset
using DTFD as the MIL classiÔ¨Åer with diÔ¨Äerent numbers of prompt blocks added
to ResNet-50. The results presented in Fig. 3 (a)show that the AUC values8 Lin et al.
0.820.840.860.880.900.920.94
0123456AUC(a) NumberofPromptBlocksDTFD-RPS-PT0.820.840.860.880.900.920.94
0510AUC(b) Number of Representative PatchesDTFD-RPS-FTDTFD-RPS-PT
41664128200
Fig. 3.Results of ablation studies on CAMELYON16 dataset: (a) EÔ¨Äectiveness of the
number of prompt blocks (b) InÔ¨Çuence of the number of representative patches.
with diÔ¨Äerent prompt quantities are consistently improved by approximately 1%
compared to the baseline approach when using more than one prompt block.
These results suggest that our prompt block component is robust and eÔ¨Äectively
improves the performance of the feature extractor.
InÔ¨Çuence of the number of representative patches. We investigate the
eÔ¨Äect of diÔ¨Äerent top- Kvalues in the RPS procedure on the performance of our
methodusingResNet-50andDTFDontheCamelyon16dataset.Fig.3 (b)shows
the AUC values under two diÔ¨Äerent settings: Ô¨Åne-tuning scheme (RPS-FT) and
our prompt Ô¨Åne-tuning scheme (RPS-PT). Our method consistently outperforms
the Ô¨Åne-tuning scheme across all Kvalues using less than 50% GPU resources,
demonstrating the eÔ¨Éciency and eÔ¨Äectiveness of our method.
4.5 Analysis on the EÔ¨Äectiveness of Visual Prompts
During the experiment, we found that Ô¨Åne-tuning produced unsatisfactory per-
formance, which was even inferior to the baselines where the feature extractor
was frozen. This subpar performance can be attributed to the speciÔ¨Åc nature of
the WSI classiÔ¨Åcation task. It requires processing all the patches in a given WSI
during each iteration of model updating, which is computationally expensive.
As a result, only a small portion of the instances are used for training, leading
to a high risk of overÔ¨Åtting. In contrast, our proposed method of using visual
prompts allows for the quick learning of the policy of the current task based
on the previously learned representation, enabling an eÔ¨Écient task and domain
adaption, thereby overcoming the limitations of Ô¨Åne-tuning.
5 Conclusion
In this paper, we propose a novel prompt learning method to learn domain-
speciÔ¨Åc knowledge transformation from ImageNet pre-trained models to patho-
logical images. Our innovation is based on the observation that there is a largeExploring Visual Prompts for MIL-based WSI ClassiÔ¨Åcation. 9
domain shift and task discrepancy between the upstream datasets and patholog-
ical tasks, resulting in sub-optimal feature representation. To relieve this issue,
we introduce a prompt component and representative patches selection strategy
to Ô¨Åne-tune the prompt blocks while freezing the feature extractor backbone. In
this way, the extracted patch features can be adapted for pathological images
and boost the WSI classiÔ¨Åcation with MIL models. Experiments on two public
datasets ( i.e., Cameylon16 and TCGA-NSCLC) with two MIL classiÔ¨Åers ( i.e.,
DTFDandABMIL)demonstratetheeÔ¨ÄectivenessandeÔ¨Éciencyofourmethod.
References
1. Bahng,H., Jahanian, A.,Sankaranarayanan, S., Isola,P.: Exploringvisual prompts
for adapting large-scale models. arXiv preprint arXiv:2203.17274 1(3), 4 (2022)
2. Bejnordi, B.E., Veta, M., Van Diest, P.J., Van Ginneken, B., Karssemeijer, N.,
Litjens, G., Van Der Laak, J.A., Hermsen, M., Manson, Q.F., Balkenhol, M., et al.:
Diagnostic assessment of deep learning algorithms for detection of lymph node
metastases in women with breast cancer. Jama 318(22), 2199‚Äì2210 (2017)
3. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. Advances in neural information processing systems 33, 1877‚Äì1901 (2020)
4. Chen, H., Tao, R., Zhang, H., Wang, Y., Ye, W., Wang, J., Hu, G., Savvides, M.:
Conv-adapter: Exploring parameter eÔ¨Écient transfer learning for convnets. arXiv
preprint arXiv:2208.07463 (2022)
5. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-
trastive learning of visual representations. In: International Conference on Machine
Learning. pp. 1597‚Äì1607. PMLR (2020)
6. Cornish, T.C., Swapp, R.E., Kaplan, K.J.: Whole-slide imaging: routine pathologic
diagnosis. Advances in Anatomic Pathology 19(3), 152‚Äì159 (2012)
7. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. In: 2009 IEEE Conference on Computer Vision and
Pattern Recognition. pp. 248‚Äì255. Ieee (2009)
8. Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:Bert:Pre-trainingofdeepbidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018)
9. Hashimoto, N., Fukushima, D., Koga, R., Takagi, Y., Ko, K., Kohno, K.,
Nakaguro, M., Nakamura, S., Hontani, H., Takeuchi, I.: Multi-scale domain-
adversarial multiple-instance cnn for cancer subtype classiÔ¨Åcation with unanno-
tated histopathological images. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 3852‚Äì3861 (2020)
10. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:
Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 770‚Äì778 (2016)
11. Hou, L., Samaras, D., Kurc, T.M., Gao, Y., Davis, J.E., Saltz, J.H.: Patch-based
convolutional neural network for whole slide tissue image classiÔ¨Åcation. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
pp. 2424‚Äì2433 (2016)
12. Hu, Y., Sirinukunwattana, K., Gaitskell, K., Wood, R., Verrill, C., Rittscher,
J.: Predicting molecular traits from tissue morphology through self-interactive10 Lin et al.
multi-instance learning. In: Medical Image Computing and Computer Assisted
Intervention‚ÄìMICCAI 2022: 25th International Conference, Singapore, September
18‚Äì22, 2022, Proceedings, Part II. pp. 130‚Äì139. Springer (2022)
13. Ilse,M.,Tomczak,J.,Welling,M.:Attention-baseddeepmultipleinstancelearning.
In: International Conference on Machine Learning. pp. 2127‚Äì2136. PMLR (2018)
14. Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.:
Visual prompt tuning. In: Computer Vision‚ÄìECCV 2022: 17th European Confer-
ence,TelAviv,Israel,October23‚Äì27,2022,Proceedings,PartXXXIII.pp.709‚Äì727.
Springer (2022)
15. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
16. Lerousseau, M., Vakalopoulou, M., Classe, M., Adam, J., Battistella, E., Carr√©,
A., Estienne, T., Henry, T., Deutsch, E., Paragios, N.: Weakly supervised mul-
tiple instance learning histopathological tumor segmentation. In: Medical Image
ComputingandComputerAssistedIntervention‚ÄìMICCAI2020:23rdInternational
Conference, Lima, Peru, October 4‚Äì8, 2020, Proceedings, Part V 23. pp. 470‚Äì479.
Springer (2020)
17. Li, B., Li, Y., Eliceiri, K.W.: Dual-stream multiple instance learning network for
whole slide image classiÔ¨Åcation with self-supervised contrastive learning. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion. pp. 14318‚Äì14328 (2021)
18. Li, S., Liu, Y., Sui, X., Chen, C., Tjio, G., Ting, D.S.W., Goh, R.S.M.: Multi-
instance multi-scale cnn for medical image classiÔ¨Åcation. In: Medical Image Com-
puting and Computer Assisted Intervention‚ÄìMICCAI 2019: 22nd International
Conference, Shenzhen, China, October 13‚Äì17, 2019, Proceedings, Part IV 22. pp.
531‚Äì539. Springer (2019)
19. Lu, M.Y., Williamson, D.F., Chen, T.Y., Chen, R.J., Barbieri, M., Mahmood,
F.: Data-eÔ¨Écient and weakly supervised computational pathology on whole-slide
images. Nature Biomedical Engineering 5(6), 555‚Äì570 (2021)
20. L√ºddecke, T., Ecker, A.: Image segmentation using text and image prompts. In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition. pp. 7086‚Äì7096 (2022)
21. Newman, A.M., Liu, C.L., Green, M.R., Gentles, A.J., Feng, W., Xu, Y., Hoang,
C.D., Diehn, M., Alizadeh, A.A.: Robust enumeration of cell subsets from tissue
expression proÔ¨Åles. Nature Methods 12(5), 453‚Äì457 (2015)
22. Nie, X., Ni, B., Chang, J., Meng, G., Huo, C., Zhang, Z., Xiang, S., Tian,
Q., Pan, C.: Pro-tuning: UniÔ¨Åed prompt tuning for vision tasks. arXiv preprint
arXiv:2207.14381 (2022)
23. Pantanowitz, L., Valenstein, P.N., Evans, A.J., Kaplan, K.J., Pfeifer, J.D., Wilbur,
D.C.,Collins,L.C.,Colgan,T.J.:Reviewofthecurrentstateofwholeslideimaging
in pathology. Journal of Pathology Informatics 2(1), 36 (2011)
24. Pinckaers, H., Van Ginneken, B., Litjens, G.: Streaming convolutional neural net-
works for end-to-end learning with multi-megapixel images. IEEE Transactions on
Pattern Analysis and Machine Intelligence 44(3), 1581‚Äì1590 (2020)
25. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. In: International Conference on Machine Learning.
pp. 8748‚Äì8763. PMLR (2021)
26. Srinidhi, C.L., Ciga, O., Martel, A.L.: Deep neural network models for computa-
tional histopathology: A survey. Medical Image Analysis 67, 101813 (2021)Exploring Visual Prompts for MIL-based WSI ClassiÔ¨Åcation. 11
27. Stacke, K., Eilertsen, G., Unger, J., Lundstr√∂m, C.: Measuring domain shift for
deep learning in histopathology. IEEE Journal of Biomedical and Health Infor-
matics 25(2), 325‚Äì336 (2020)
28. Yang,P.,Hong,Z.,Yin,X.,Zhu,C.,Jiang,R.:Self-supervisedvisualrepresentation
learning for histopathological images. In: Medical Image Computing and Computer
Assisted Intervention‚ÄìMICCAI 2021: 24th International Conference, Strasbourg,
France,September27‚ÄìOctober1,2021,Proceedings,PartII24.pp.47‚Äì57.Springer
(2021)
29. Zhang, H., Meng, Y., Zhao, Y., Qiao, Y., Yang, X., Coupland, S.E., Zheng,
Y.: DTFD-MIL: Double-tier feature distillation multiple instance learning for
histopathology whole slide image classiÔ¨Åcation. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 18802‚Äì18812 (2022)