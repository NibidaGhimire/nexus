∆-Patching: A Framework for Rapid Adaptation of Pre-trained Convolutional
Networks without Base Performance Loss
Chaitanya Devaguptapu1*Samarth Sinha2,4K J Joseph6
Vineeth N Balasubramanian3Animesh Garg2,4,5
1Fujitsu Research India2University of Toronto3Indian Institute of Technology, Hyderabad
4Vector Institute5NVIDIA6Adobe Research
Abstract
Models pre-trained on large-scale datasets are often
fine-tuned to support newer tasks and datasets that arrive
over time. This process necessitates storing copies of the
model over time for each task that the pre-trained model
is fine-tuned to. Building on top of recent model patching
work, we propose ∆-Patching for fine-tuning neural net-
work models in an efficient manner, without the need to
store model copies. We propose a simple and lightweight
method called ∆-Networks to achieve this objective. Our
comprehensive experiments across setting and architecture
variants show that ∆-Networks outperform earlier model
patching work while only requiring a fraction of parame-
ters to be trained. We also show that this approach can
be used for other problem settings such as transfer learn-
ing and zero-shot domain adaptation, as well as other tasks
such as detection and segmentation.
1. Introduction
The proliferation of pre-trained models, especially in the
domain of vision tasks, has been remarkable [1]. These
models, trained on extensive datasets, possess features that
are universally beneficial across a wide array of tasks. Over
the years, a significant use of such pre-trained models has
been in a fine-tuning or transfer learning context, where the
model weights are partially or fully modified while maxi-
mizing performance on a target task. While effective, this
strategy tends to specialize the model’s features, often at the
expense of its original capabilities, causing a decline in per-
formance on the original task for which the model was ini-
tially trained. Recently, there have been efforts to “ patch ”
pre-trained models towards improved performance on dif-
*Major part of the work was done when Chaitanya was a Visiting Grad-
uate Researcher at University of Toronto and a Masters student at IIT-
Hyderabad. Corresponding author: email@chaitanya.one
Large scale pre-trained modelAdapt to new set of categories (Task-1)Adapt to new group of customers (Task-2)Transfer LearningContinual LearningPatching
Base ModelTask-1Task-2Task-3Zero drop in performanceNo guarantee on performanceNot applicable
Fully Finetuned  ModelSequentially learned modelΔ-Patching(proposed setting)
Weight Interpolation
No explicit copy of the model is made(just for illustration)Base model is never modifiedΔ-Netfor Task-1Δ-Netfor Task-2Δ-Netfor Task-3Adapt to new set of samples (Task-3)Figure 1. Comparison of ∆-Patching with relevant learning with
limited data paradigms. ∆-Patching ensures there is zero-drop
in performance on the base task and ∆-Networks (proposed ap-
proach to perform ∆-Patching) use significantly less trainable pa-
rameters as compared to existing approaches
ferent target tasks with minimal loss of performance on the
original task [19]. Model patching [19] draws parallels with
software patching in software engineering and aligns with
the growing trend of treating machine learning models as
open-source software [19, 34].
In this work, we propose ∆-Patching a novel framework en-
gineered for the swift adaptation of large-scale pre-trained
models to new tasks without degrading accuracy on exist-
ing tasks. The impetus for ∆-Patching arises from prac-
tical needs observed in various industries. Consider, for
example, an e-commerce platform equipped with a large-
scale apparel classification model. As fashion trends evolve
or seasons change, the model pre-trained on a comprehen-
sive dataset encompassing a multitude of apparel categories,
must adapt to evolving fashion trends and seasonal vari-
ations without losing its proficiency in recognizing exist-
ing categories. In the medical realm, a healthcare system
1arXiv:2303.14772v2  [cs.CV]  21 Sep 2023employing a pre-trained model for medical image diagno-
sis might need to detect a newly identified condition from
MRI scans, without diminishing its diagnostic accuracy for
established conditions like tumors. Contrasting with tradi-
tional Incremental/Continual Learning (IL) methods, which
risk ”catastrophic forgetting” due to continual modification
of the base model, ∆-Patching adopts a different philoso-
phy. It capitalizes on the same large-scale pre-trained model
for all tasks, ensuring the preservation of its intrinsic knowl-
edge. While IL methods grapple with balancing new task
adaptation and old task performance retention, ∆-Patching
bypasses this dilemma. It promises rapid and adaptable in-
tegration of new tasks while safeguarding the original capa-
bilities of the pre-trained model. This positions ∆-Patching
as a compelling strategy, especially when rapid, flexible
adaptation to new tasks, without compromising existing ca-
pabilities, is paramount.
As an embodiment to achieve ∆-Patching, we introduce
∆-Networks. While not entirely novel, ∆-Networks ex-
cel in terms of efficiency and ease of adaptability, mak-
ing it a highly effective solution for ∆-Patching. ∆-
Networks is a hypernetwork [13] with a twist: it incorpo-
rates input-conditioned skip connections for adaptive and
efficient patching, all while not modifying a single weight
of the original model. While sharing the overarching goal
of model adaptability with existing work [19], our approach
distinguishes itself in several key aspects. First, while [19]
focuses on patching open-vocabulary models like CLIP
[33], we focus on general-purpose CNNs, which are ubiq-
uitously used and fine-tuned across various domains. Un-
like PAINT [19] that necessitate dual model copies for in-
terpolation [19], ∆-Networks introduces input-conditioned
skip connections for patching, leaving the original model
weights untouched. The essence of our approach, symbol-
ized by the ∆in∆-patching and ∆-networks, lies in the
minimal input-conditioned skip connections added. This
design facilitates memory-efficient model patching, adapt-
able to varying memory and computational constraints.
Crucially, ∆-Networks is fully differentiable, eliminating
the need for model interpolations or empirically determined
weight coefficients. Figure 1 illustrates the proposed set-
ting and its difference from earlier work. Table 9 presents a
feature-wise comparison of the setting when compared to
similar settings in related work. While ∆-Networks are
primarily engineered for ∆-Patching, they support a wide
range of tasks, like object detection, segmentation and can
also be used to improve zero-shot domain adaption, transfer
learning. The simplicity of our method is its strength. In
fact,∆-Networks achieve new state-of-the-art for transfer
learning by improving the performance of an existing trans-
fer learning method. Our key contributions can be summa-
rized as follows:
• Introduction of ∆-Patching, a new model patching strat-egy that enables rapid adaptation of pre-trained models to
new tasks without sacrificing original performance, im-
plemented through ∆-Networks.
• Development of an end-to-end differentiable method that
allows patching at varying levels based on computational
constraints.
• Extensive evaluation of our methodology across multiple
datasets and tasks, demonstrating superior performance
with fewer trainable parameters, and setting a new state-
of-the-art in transfer learning.
2.∆-Networks for Efficient Model Patching
Consider a model MStrained for a base task TSusing sam-
ples from a dataset DS. Our primary goal is to patchMS
for performing well on a new task TPwith samples from
DPwithout any performance degradation on TS. Impor-
tantly, we operate under the practical constraints that nei-
ther the samples DSnor any samples generated using MS
are accessible during this process. Let the features learned
usingDSfor the base/supporting task TSbe denoted as FS.
FSkthen denotes a layer (or a block of layers) (for e.g.
with convolutional, max-pooling operations and activation
functions in a traditional CNN). FS, the final representation
learned with samples from DScan be viewed as a composi-
tion of features learned at each layer, and can be written as
below for a given input sample xsfrom the dataset DS.
FS=FSk(FSk−1(. . .(FS1(xs)))) (1)
The final model MSis formed by passing the features FS
into a classification layer/network, denoted using ϕSwhich
typically is a fully-connected layer with neurons equal to
the number of classes in DS:
MS=FS◦ϕS (2)
When a new patching task TParrives with a corresponding
dataset DP, the number of classes in the patching task is
generally different from that of the original task. So, adding
a new classification layer, ϕPis most often inevitable. Di-
rectly modifying FSto perform better on TPaffects the per-
formance on TS, which defeats our primary goal. We hence
introduce an architectural modification to MSin such a
way that it helps address the new task while not affecting
the performance on the original task.
As mentioned previously, Fkdenotes a sub-block of lay-
ers inside MS. Most modern-day architectures stack sub-
blocks to build blocks which are again stacked together to
build larger architectures. In a general-purpose CNN archi-
tecture such as ResNet [15], Fkis a residual block with one
skip connection (we refer to this as a sub-block). The output
representation for any Fjwhich represents a sub-block of
layers with one skip connection can be denoted as follows:
Fj=Fj(Fj−1(. . .(F1(x)));Wj) +Fj−1(x;Sw)|{z}
Skip connection(3)
2where Wjdenotes the weights corresponding to the jthsub-
block and the highlighted term in Eqn 3 refers to the repre-
sentation learned by the previous sub-block, mapped using a
weight matrix Sw. In a standard residual network, this high-
lighted component is obtained by setting Sw=I, where I
is the identity matrix. Generalizing the notion of skip con-
nections using Swis a key component of the proposed ∆-
Networks methodology.
While most architectures as well as earlier transfer learn-
ing/patching literature focus on updating Wj, we instead
turn our focus to the above generalization of skip connec-
tions to leverage the features learned on the base/original
task. Firstly, we consider all possible skip connections be-
tween each sub-block in a block as possible routes for a
given input. (Note that we do not introduce skip connec-
tions inside layers of a sub-block to keep the overhead min-
imal.) Figure 2 shows an illustration.
Figure 2. Illustration of a block in ∆-Networks
One could view this as similar to a sparse variant of
DenseNets, however with a difference – each connection
has a learnable weight matrix Swin our case, as described
later in this section. The output of a block in Eqn 3 can
hence be written as:
Fj=Fj(Fj−1(. . .(F1(x)));Wj) +j−1X
i=1Fi(x;Sw)(4)
where Swis learnable and specifies how two sub-blocks in-
teract. Since this modification does not involve any changes
to existing model weights, off-the-shelf pre-trained models
can be easily leveraged. Besides, not making any changes
to the base model ensures that the performance on the base
task is retained without any drop (by simply dropping the
added skip connections). Through this generalization, our
objective can be viewed as developing a mechanism that al-
lows us to control interactions among pre-trained features
at various granularities (for e.g, early vs late layers) so that
it enables effective patching for new tasks.
The∆-module. When learning Sws for skip connections
on top of a pre-trained model for a target task, the same
weights that are optimal for one input might be sub-optimal
for another input, even from the same dataset or class label.
Keeping this in mind, we propose the use of a ∆-modulewhich is a separate, small and lightweight network parame-
terized by weights θthat learns the weights of the additional
skip connections independent of the base model. Thus, for
a given input x, the added skip connections are weighted
in an input-conditioned manner using a ∆-module. Follow-
ing Eqn 4, the output of a sub-block Fjis now changed as
follows when a ∆-module (denoted using ∆below) is in-
troduced:
Λi= ∆( x;θ)
Fj(x) =Fj(Fj−1(. . .(F1(x)));Wj) +j−1X
i=1Fi(x; Λi)
Intuitively, we first pass the input to the ∆-module to ob-
tain weighting coefficients Λ{1,...(j−1)}for each of the new
skip connections in our ∆-network. Since the ∆-module
is a parametrized neural network and therefore fully dif-
ferentiable, it can be trained directly on the new task loss
via gradient descent. Unlike standard skip connections with
Λi=I, adaptive skip connections have input-conditioned
weights on them instead. Since the skip connections are
introduced after training, our model can be directly used
with existing models that are pre-trained for a different task.
(For models that already have skip connections – for exam-
ple, DenseNets, the Λis for those pre-existing skip connec-
tions are defaulted to the identity matrix when addressing
the base task.) An end-to-end summary of our approach is
provided in Algorithm 1.
Algorithm 1: ∆-Networks for Model Patching
Inputs and Initialization:
• Model trained on supporting task MS=FS◦ϕS,FS→ FS1...k
withkblocks;
• Randomly initialized classification head ϕP, and∆-Networks, ∆1...k
• Samples from patching task xdm=1...n∈ DPinnbatches, Loss
function used for the task L
• Add ϕTtoFS; Introduce all possible skip connections ( p) in each
sub-block of FS1...k
Training:
fornum epochs do
forxdm=1...ndo
forj= 1. . . k do
Λ1...p= ∆j(xm
d;θj)
FSj=FSj(xm
d;Wj) +j−1X
i=1FSi(xm
d; Λi)
end
MP= (FS,∆1...k)◦ϕT
Calculate L
Update ∆k,ϕT
end
end
Output: FS◦ϕT→model patched for task TP
Conditioning skip connections using an external fully-
differentiable network helps finetune the model easily on
a new patching task. When new tasks arrive in a sequence,
we introduce a new ∆-module and a classification layer for
each task. These ∆-modules are intended to be lightweight
3(2-layer feedforward networks in our implementation), and
hence can be stored easily for each task arriving over time.
As each task has its own classifier and ∆-Networks, there
is no loss of performance on any of the preceding tasks and
the model can support as many tasks as required.
Adapting to Memory/Compute Requirements. When us-
ing Algorithm 1, the memory and compute requirements
can be controlled using the number of ∆-modules used for a
task under consideration. When there are memory/compute
constraints, one can choose to use a small ∆-module for
learning the weights of skip connections of a few blocks
alone. ∆hence becomes a unit quantum of additional over-
head, lending the name ∆-Networks to this approach. With
more memory, a set of ∆-modules can be used to learn dif-
ferent skip connections on the base model. The proposed al-
gorithm thus allows customizing ∆-Patching in a memory-
sensitive manner. Increasing the number of ∆-modules in-
creases the number of trainable parameters, and hence re-
sults in performance gain in general. We provide some de-
sign choices and thumb rules of connecting ∆-module to
subsets of skip connections in the following section
Implementation and Design Details. Any neural network
model developed by stacking sub-blocks of layers can be
segregated into a set of blocks, denoted as Fjearlier in
this section, with each block containing varying number of
sub-blocks. For example, ResNet architectures are often di-
vided into four blocks. If a block contains ksub-blocks,
we introduce a total k(k+ 1)/2skip connections in the
block, which is controlled by one ∆-module. One can use
additional ∆-modules to weight skip connections in other
blocks. We denote ∆-Networks (n)to denote a ∆-network
withn∆-modules. Considering that we only need to learn
the weights of the skip connections (and not all parameters
of the network), each of our ∆-module consists of a simple
two-layer CNN with ReLU activations, normalization lay-
ers, and a fully-connected layer to determine the weights for
skip connections. The output of a ∆-module is constrained
between [0,1]using a sigmoid activation. Whenever skip
connections are present in the architecture, there is a prob-
lem of increase in variance [52] (we formally show this in
the Appendix). With ∆-Networks, when all possible skip
connections are introduced, the variance increases rapidly
since we add features from all preceding blocks. We ad-
dress this issue using one BN layer in each ∆-module in
our implementation.
3. Experiments and Results
In this section, we aim to provide a holistic evaluation of
∆-Networks, contextualizing its performance against both
established and alternative baselines. We first compare
∆-Networks with other relevant methods adapted for the
patching setting, and then proceed to a detailed empirical
analysis against PAINT [19], the established baseline formodel patching.
Beyond patching, we also explore the applicability of ∆-
Networks to a range of other tasks, such as object detec-
tion and segmentation. Additionally, we examine its perfor-
mance in specialized settings like zero-shot domain adapta-
tion and transfer learning. To delve deeper into the underly-
ing factors contributing to the success of ∆-Networks, we
conclude this section with an ablation analysis.
Experimental Setup: For all the experiments, we lever-
age CNN models pre-trained on ImageNet, sourced from
the official PyTorch [30] and PyTorch Image Models [49]
libraries. To ensure a fair comparison, we employ the offi-
cial code repository of PAINT [19] and apply their method
to all models under consideration in our study. In all ex-
periments, both for ∆-Networks and PAINT, we append
a fully connected (FC) layer to the model architecture for
the new task, consistent with the approach in PAINT. Since
PAINT mandates fine-tuning of the entire set of pre-trained
weights, we adhere to a 50-epoch fine-tuning regimen for
all comparative experiments, utilizing the same data splits
and procedures as described in PAINT, to maintain fairness
in evaluation. For the experiments where we compare with
adapted baselines, we follow the data splits and use the code
provided in their official code repositories [27, 35, 36]. Our
primary performance metric is the harmonic mean of top-
1 accuracies for both pre-trained and fine-tuned tasks, ex-
cept in the case of sequential patching where we use the
mean. Detailed top-1 accuracy scores for individual tasks
are available in the appendix for a comprehensive under-
standing. When deploying ∆-Networks, we introduce one,
two, or four ∆-modules. A single ∆-module is added to
the last block of the base architecture, complete with corre-
sponding skip connections. For configurations with two or
four∆-modules, they are distributed across the last two or
all four blocks of the network, respectively. Each ∆-module
incrementally increases the number of trainable parameters,
offering a lens through which to study the scalability of ∆-
Networks with respect to parameter size and hardware con-
straints.
3.1. Evaluation of ∆-Networks for Patching
Comparison with Adapted Baselines : To situate ∆-
Networks within the broader landscape of model patching,
we adapt existing methods such as Parallel and Series Adap-
tors [35, 36], as well as a recent normalization-based ap-
proach [27], to the patching setting. The harmonic mean of
Top-1 accuracies serves as our performance metric, as men-
tioned earlier. As shown in Table 1, ∆-Networks outper-
forms these adapted methods on 8 out of 9 datasets, thereby
establishing its efficacy.
An additional baseline that warrants consideration in the
context of patching is fine-tuning based solely on a Fully
Connected (FC) layer. This approach serves as a fundamen-
4Flowers Aircraft UCF101 GTSRB DTD SVHN CIFAR-100 OmniGlot Daimler Ped
PAINT 12.73 4.94 21.33 21.36 8.54 62.49 9.27 9.22 18.58
Parallel Adapters [35] 71.46 52.01 73.21 83.20 55.68 81.30 75.52 77.59 83.15
Series Adapters [36] 59.10 46.65 70.67 83.20 39.97 81.34 70.40 77.94 83.11
BatchNormAdapt [27] 71.52 52.52 73.23 83.00 53.20 81.28 75.70 78.07 83.07
∆-Networks (4) 75.16 58.44 75.52 83.21 56.06 81.52 75.12 78.40 83.16
Table 1. ∆-Networks excels in 8/9 datasets. Comparative Analysis vs. Adapted Methods in ∆-Patching context. Harmonic Mean of
Top-1 Accuracies reported.
Architecture Method # Parameters STL10 RESISC45 GTSRB Flowers Cars Aircraft
ResNet-18PAINT 11.2M 55.41 24.38 18.39 12.70 2.01 3.51
∆-Networks (1) 0.4M 79.55 74.48 70.22 75.65 46.78 46.56
∆-Networks (2) 0.6M 79.66 76.08 76.20 75.91 51.29 50.48
∆-Networks (4) 0.6M 79.57 76.42 78.94 75.65 76.23 52.67
ResNet-34PAINT 21.8M 62.64 21.91 19.87 16.16 4.39 2.94
∆-Networks (1) 0.4M 83.41 78.02 70.83 78.31 47.31 45.74
∆-Networks (2) 0.6M 83.59 80.18 78.11 79.04 55.24 51.39
∆-Networks (4) 0.6M 83.60 80.70 82.36 78.82 57.33 54.82
ResNet-50PAINT 23.5M 86.30 48.04 28.11 56.97 12.54 7.33
∆-Networks (1) 7.1M 87.53 84.02 77.79 82.64 59.31 55.81
∆-Networks (2) 8.9M 87.41 84.93 84.09 82.74 65.96 62.14
∆-Networks (4) 9.3M 87.40 85.03 85.67 80.34 67.11 61.48
ConvNeXT-TinyPAINT 27.8M 84.57 34.81 12.36 69.06 6.87 6.18
∆-Networks (1) 4.0M 89.69 86.54 82.47 87.49 73.00 69.59
∆-Networks (2) 5.0M 89.32 86.97 88.01 84.89 75.44 71.74
∆-Networks (4) 5.3M 89.35 87.24 88.96 82.35 76.23 70.81
Table 2. ∆-Networks outperforms PAINT with fewer parame-
ters. Comparison in Single-Task Patching across architectures.
Harmonic Mean of Top-1 accuracies reported.
tal comparison point, given its widespread use and simplic-
ity. We delve into this comparison in greater detail later in
the paper, presenting the results in Table 8.
Evaluating ∆-Networks in various patching scenarios :
In alignment with the experimental setup outlined in PAINT
[19], we evaluate ∆-Networks across three distinct patch-
ing scenarios: (1) Single-Task Patching , where the model
is adapted for one new task; (2) Joint Patching , involving
adaptation for a pre-defined set of tasks; and (3) Sequential
Patching , where the model is adapted for tasks arriving in
sequence.
Single-Task Patching: Table 2 presents the outcomes of
our experiments in a single-task setting. We explore six di-
verse datasets and four architectural variations. Additional
results for other architectural variants can be found in the
Appendix. For rows related to PAINT, we select the model
interpolation coefficient that maximizes the harmonic mean
metric previously discussed. As evident from the table,
∆-Networks consistently outperforms PAINT by substan-
tial margins while utilizing a significantly lower number
of trainable parameters. As we only finetune the fully-
connected layer and the small ∆-module, the number of
trainable parameters are much fewer compared to full fine-
tuning approaches like PAINT. Increasing the number of ∆-
modules by introducing them in the first, second and third
blocks along with the last block increases the performance
even more.
Joint Patching: In this experimental setting, we have ac-
cess to all the datasets for the tasks that need to be learned
simultaneously. In alignment with [19], we concatenate the
training and test sets for all tasks prior to fine-tuning. TheArchitecture MethodAircraft
Flowers
STL10GTSRB
Cars
FlowersSTL10
Cars
GTSRB
ResNet-18 PAINT 22.33 4.35 0.66
∆-Networks (1) 66.91 61.41 66.91
∆-Networks (2) 69.04 64.71 69.45
∆-Networks (4) 71.14 66.64 71.14
ResNet-34 PAINT 13.75 6.75 15.20
∆-Networks (1) 70.31 64.51 70.31
∆-Networks (2) 82.48 67.81 72.93
∆-Networks (4) 75.39 71.32 75.39
ResNet-50 PAINT 55.65 4.70 35.16
∆-Networks (1) 76.56 71.96 76.56
∆-Networks (2) 80.61 76.94 80.02
∆-Networks (4) 81.79 78.75 81.79
ConvNeXt-TINY PAINT 34.72 1.61 6.70
∆-Networks (1) 81.12 77.45 81.02
∆-Networks (2) 82.69 80.91 83.29
∆-Networks (4) 82.91 82.32 84.33
Table 3. Performance Evaluation of ∆-Networks and PAINT [19]
inJoint Patching scenario. Harmonic Mean of Top-1 accuracies
is reported.
Architecture MethodAircraft
Flowers
STL10GTSRB
Cars
FlowersSTL10
Cars
GTSRB
ResNet-18PAINT 20.02 14.28 26.85
∆-Networks (1) 70.52 64.95 67.51
∆-Networks (2) 71.93 69.89 72.37
∆-Networks (4) 72.41 79.03 81.60
ConvNeXT-TinyPAINT 19.95 28.68 27.01
∆-Networks (1) 83.52 64.72 65.74
∆-Networks (2) 82.71 83.45 85.94
∆-Networks (4) 81.06 83.04 86.84
Table 4. Performance Evaluation of ∆-Networks and PAINT [19]
inSequential Patching scenario. Mean of Top-1 accuracies is re-
ported.
newly introduced fully connected layer comprises neurons
equal to the total number of classes across all tasks. ∆-
modules are trained in a unified manner for all tasks. Table
3 displays the results, which demonstrate that our method
consistently outperforms PAINT across all task and archi-
tecture variants.
Sequential Patching: In this experimental configuration,
tasks requiring patching are encountered sequentially. For
PAINT, in accordance with [19], we commence with a pre-
trained model, append a new fully connected (FC) layer,
5and proceed to fine-tune the backbone along with this new
layer for the first task. The optimal interpolation coeffi-
cient is determined based on the harmonic mean of top-1
accuracies for both the pre-trained and newly patched tasks.
These interpolated weights are then employed for subse-
quent tasks, and the best interpolation coefficient is recal-
ibrated using the harmonic mean of accuracies across all
preceding tasks. During evaluation, the task ID guides the
selection of the appropriate FC layer. For ∆-Networks, a
new∆-module is introduced for each incoming task, in tan-
dem with a new FC layer. These ∆-modules are trained
alongside the FC layer for each individual task. Table 4
presents the average accuracy of the final patched model
across all prior tasks. It’s worth noting that we opt for aver-
age accuracy over harmonic mean in this setting due to the
simultaneous evaluation of multiple tasks at the conclusion
of the experiment. Unlike PAINT, which does not preserve
the base model and relies on interpolated models for future
tasks, ∆-Networks is designed to maintain consistent per-
formance on previously learned tasks. A more comprehen-
sive analysis of these findings is available in the Appendix.
3.2.∆-Networks for Other Tasks and Settings
We extend our investigation of ∆-Networks to diverse sce-
narios, including transfer learning and zero-shot domain
adaptation, as well as additional vision tasks like object de-
tection and segmentation. Unlike previous patching efforts,
which did not explore these settings, our aim is to demon-
strate the versatility of ∆-Networks in adapting pre-trained
models across a range of tasks and conditions. While our
focus is not on achieving state-of-the-art performance, we
do illustrate how the incorporation of ∆-modules enhances
the fine-tuning capabilities of existing pre-trained models.
Method FGVC Aircraft Stanford Cars
15% 30% 50% 100% Avg 15% 30% 50% 100% Avg
Vanilla Baseline 41.6 57.8 68.7 80.2 62.1 41.1 65.9 78.4 87.8 68.3
+ LWF 44.1 60.6 68.7 82.4 64.0 44.9 67.0 77.6 87.5 69.3
+ BSS 43.6 59.5 69.6 81.2 63.5 43.3 67.6 79.6 88.0 69.6
+ DELTA 44.4 61.9 71.4 82.7 65.1 45.0 68.4 79.6 88.4 70.4
+ Co-Tuning 45.9 61.2 71.3 82.2 65.2 49.0 70.6 81.9 89.1 72.7
+ StochNorm 44.3 60.6 70.1 81.5 64.1 44.4 68.1 79.1 87.9 69.9
+ Bi-Tuning 47.2 64.3 73.7 84.3 67.4 48.3 72.8 83.3 90.2 73.7
+∆-Networks (4) 46.9 64.8 74.3 85.3 67.8 48.5 73.8 82.5 90.4 73.8
+ Bi-Tuning + ∆-Networks (4) 47.7 65.1 76.5 87.1 69.1 49.2 73.9 84.3 91.1 74.6
Table 5. ∆-Networks outperforms and enhances existing meth-
ods (see Bi-Tuning + ∆-Networks) . Performance of ∆-Networks
vs. established methods on TL benchmark [20] under varied data
availability (e.g., 50% = half training data). ∆-Networks (4) indi-
cates use of four ∆-modules.
Transfer Learning (TL): We study ∆-Networks in a tra-
ditional TL setting and compare our methodology with
six different transfer learning methods, including state-
of-the-art approaches: LWF [24], BSS [5], DELTA [23],
StochNorm [21], Co-Tuning [51], and Bi-Tuning [53]. We
also compare with a vanilla baseline, where the backbone
architecture is fine-tuned end-to-end on the target dataset,and a combination of our method with Bi-Tuning [53].
We use the standard TL benchmark and training proto-
cols provided in [20], comprising FGVC-Aircraft [26] and
Stanford-Cars [22] datasets, with ResNet-50 as the back-
bone, following [21, 51, 53].To test the effectiveness of our
approach in the low-data regime, we train the model at dif-
ferent levels, i.e. on 15%, 30%, 50% and 100% of the
training data, for the above experiments. Table 5 presents
our results. ∆-Networks outperforms existing approaches
on almost all experiments consistently, even with varying
amounts of training data in the target domain. Bi-Tuning
[53] uses contrastive learning and leverages both super-
vised and unsupervised pre-trained representations, which
perhaps makes it stronger compared to other baselines. ∆-
Networks instead uses a simple strategy to outperform all
baselines including Bi-Tuning on most settings. Moreover,
adding ∆-Networks to Bi-Tuning helps achieve a new state-
of-the-art on a standard TL benchmark [20].
ResNet-18 MNIST →USPS USPS →MNIST SVHN →MNIST
Vanilla 49.0 42.81 69.7
+∆-Networks (4) 56.5 47.0 75.3
+ Uniform prior [43] 67.2 56.2 71.3
+ Uniform prior + ∆-Networks (4) 71.3 61.0 77.9
ADDA [46] 88.2 89.0 73.4
+∆-Networks (4) 90.1 90.7 81.1
+ Uniform Prior 91.6 92.7 79.4
+ Uniform Prior + ∆-Networks (4) 92.4 94.1 83.6
Target only 98.1 99.8 99.8
Table 6. ∆-Networks boosts ZSDA performance. ∆-Networks
(4) indicates four ∆-modules used.
Zero-shot Domain Adaptation (ZSDA): In ZSDA, the
models are trained on a given source dataset and evalu-
ated on their ability to transfer to a different target dataset
that has the same classes but is from a different data dis-
tribution. We adopt the experimental framework outlined
in [43] to evaluate ∆-Networks using a ResNet-18 archi-
tecture. Our comparison includes both the Uniform Prior
method [43] and Adversarial Discriminative Domain Adap-
tation (ADDA) [46]. Additionally, we explore the syner-
gistic effects of combining our approach with these existing
methods. The results are presented in Table 6 We continue
to observe that by adding ∆-Networks, we can improve the
network’s performance on the target dataset significantly.
Importantly, the results also show that ∆-Networks can be
a general strategy to use along with existing techniques.
Object Detection and Segmentation: The utility of
pre-trained ImageNet models extends beyond classification
tasks; they serve as foundational backbones for object de-
tection and semantic segmentation. To assess the efficacy
of∆-Networks in these contexts, we integrate it into es-
tablished architectures for object detection and semantic
segmentation. Specifically, we employ the Faster-RCNN
model [37] with ResNet-18 and ResNet-34 backbones for
object detection, and DeepLab-V3 [4] with a ResNet-50
backbone for semantic segmentation. All experiments are
6Task Backbone mAP/IoU
Object
DetectionResNet-18 59.72
+∆-Networks (4) 61.33
ResNet-34 64.40
+∆-Networks (4) 67.80
Semantic
SegementationResNet-50 70.02
+∆-Networks (4) 70.80
Table 7. ∆-Networks enhances performance on complex tasks
like detection and segmentation. Results on Faster-RCNN and
DeepLab-v3 with various backbones. ∆-Networks (4) indicates
four∆-modules used.
conducted on the PASCAL-VOC dataset [7].
In these experiments, we introduce ∆-Networks into the
backbone networks and train only the ∆-modules, keeping
the backbone parameters frozen. The results, summarized
in Table 7, demonstrate that the inclusion of ∆-Networks
leads to performance gains in both object detection and se-
mantic segmentation tasks.
3.3. What makes ∆-Networks work? Analysis and
Ablation Studies
We herein study the importance of the components of ∆-
Networks– in particular, the use of skip connections, both
learnable and input-conditioned, as well as the relevance of
batch normalization layers. We consider four variants of
ResNets, including additional architectures beyond what we
studied so far (for analysis purposes), and train them in mul-
tiple settings on two datasets namely, FGVC-Aircraft [26]
and Stanford-Cars [22].
FC fine-tuning as a baseline: A compelling baseline
for the patching process is the fine-tuning of just the fully-
connected (FC) layer. As elaborated in Section 2, the addi-
tion of a new FC layer is an unavoidable step when patching
CNNs. Given that FC-based fine-tuning does not necessi-
tate any modifications to the backbone architecture, it serves
as an ideal starting point for our ablation studies. This ap-
proach allows us to isolate and understand the impact of
each component introduced in ∆-Networks. By comparing
the performance of ∆-Networks against this baseline, we
can more precisely attribute any performance gains to the
specific elements of our method.
Role of Skip Connections and Input-Conditioning. In
order to study this, we introduce four kinds of skip con-
nections in the standard ResNets. (1) Learnable Skip Con-
nections: We make the already existing skip connections
in residual networks soft by introducing a weight param-
eter, i.e. when adding an input to the output block, in-
stead of using a simple identity mapping, we use a scaled
identity mapping (multiply the input using this learnable
weight parameter). (2) All Skips: We introduce all pos-
sible skip connections inside each ResNet module. (3)
All Skips (Learnable): All Skip connections with learnable
weights i.e., after all possible skip connections are intro-Architecture FGVC Aircraft Stanford Cars
ResNet-18 + FC (R-18) 59.89 67.63
+ Learnable Skip connections 59.26 51.71
R-18 + All Skips (Learnable) 63.70 59.85
+ BN 70.69 77.63
R-18 + All Skips 55.18 61.43
+ BN 71.12 75.43
+ Random 21.33 26.38
+∆-Networks (4) 72.91 77.21
ResNet-34 + FC (R-34) 59.83 68.9
+ Learnable Skip connections 61.48 65.26
R-34 + All Skips (Learnable) 41.52 27.91
+ BN 64.63 79.31
R34 + All Skips 47.34 44.45
+ BN 67.78 66.19
+ Random 9.31 7.16
+∆-Networks (4) 75.19 81.51
ResNeXt-50 + FC (RX-50) 61.06 69.39
+ Learnable Skip connections 58.87 69.23
RX-50 + All Skips (Learnable) 47.53 72.49
+ BN 50.85 77.04
RX-50 + All Skips 52.72 50.14
+ BN 72.82 75.56
+ Random 20.94 21.81
+∆-Networks (4) 78.04 85.25
WideResNet-50 + FC (WRN-50) 55.96 64.89
+ Learnable Skip connections 56.55 64.45
WRN-50 + All Skips (Learnable) 30.92 60.47
+ BN 53.51 76.40
WRN-50 + All Skips 51.28 48.97
+ BN 72.52 75.15
+ Random 14.37 12.05
+∆-Networks (4) 76.18 83.08
Table 8. Quantitative ablation analysis of marginal contribution of
each component in ∆-Networks. ∆-Networks (4) denotes the use
of four ∆-modules
duced, we assign a separate weight to all of them and learn
the weight using standard backpropagation. (4) Skip Con-
nections in ∆-Networks: When a ∆-module is used, we
introduce all possible skip connections and weight those
skip connections in an input-conditioned manner i.e., the
weight of the skip connection is determined by the ∆-
module. In (1) and (3), the skip connections weights are
not input-conditioned; once learned on the training set, they
stay fixed during inference irrespective of the input. Ta-
ble 8 shows the results of this study on patching. Evi-
dently, ∆-Networks outperform other strategies in the re-
sults. Introducing all possible skip connections without
input-conditioning does not help much. While learnable
skip connections seems to help marginally, the improve-
ment is low compared to ∆-Networks-based skip connec-
tions which are input-conditioned.
Role of Batch Normalization (BN). We explicitly study
the role of BN layers since introducing all possible skip con-
7CHARACTERISTICS →
SETTINGS ↓Adapt off-the-shelf
pre-trained modelsMaintain performance
on base taskAdapt to multiple
tasks overtimeEnd-to-end
learning
Meta Learning [16] /reve /reve /
Zero/Few-shot Learning [3, 45] /reve / /reve /
Continual Learning [32, 41] /reve / /
Transfer Learning [54] / /reve /
Model Editing/Debugging [38, 44] / / /reve /
Patching [19] / / /reve
∆-Patching (proposed method) / / / /
Table 9. Comparison of proposed ∆-Patching with other related settings. /: Yes,/reve: No,: Yes in certain cases. Similar to [19], we
focus on adapting off-the-shelf pre-trained models in this work, unlike zero/few-shot learning or continual learning.
nections can result in an increased variance. We introduce
a BN layer in our approach to curtail this. Table 8 presents
related results; as seen in the table, adding BN layers im-
proves performance in general even for other approaches
when skip connections are added, demonstrating their use-
fulness in approaches that modify or adapt skip connections.
4. Related Work
We categorize the related work into two overarching
themes: settings and methods. This organization allows us
to delineate the unique aspects of our proposed setting, ∆-
Patching, and method, ∆-Networks, in relation to existing
paradigms.
Related Settings We provide a comparative analysis of var-
ious problem settings, as summarized in Table 9 and Fig 1,
to contextualize ∆-Patching
Meta-Learning (ML): ML focuses on training a meta-model
that generalizes across tasks [9, 16]. Unlike ∆-Patching,
meta-learning does not aim to preserve primary task perfor-
mance while adapting to new tasks.
Zero-shot/Few-shot learning (ZSL/FSL): These paradigms
aim for generalization with limited labeled data for new
tasks [3, 45]. ∆-Patching diverges by enabling multi-task
fine-tuning without altering the base model.
Continual Learning (CL): CL methods incrementally adapt
to new tasks while mitigating catastrophic forgetting [32,
41]. In contrast, ∆-Patching employs a static, large-scale
pre-trained model for rapid task adaptation without model
modification. The primary objective of ∆-Patching is quick
adaptation of a pre-trained model for a given task without
modifying the original model , while IL methods incremen-
tally modify a model being performant on all tasks intro-
duced to it.
Transfer Learning (TL): TL methods fine-tune pre-trained
models for new tasks [31, 54]. Unlike TL, ∆-Patching
explicitly maintains base task performance, allowing for
multi-task support with a single model instance.
Model Editing/Debugging: These methods modify models
at the sample level to correct or update predictions [6, 28,
38–40, 44]. ∆-Patching, however, operates at the task or
dataset level.
Related Methods. We now shift our focus to methods thatshare similarities or objectives with our proposed approach
∆-Networks.
Model Patching: Recent works have explored patching pre-
trained models for specific objectives [11, 19]. While our
work aligns more with the goals of [19], our approach and
formulation, as detailed in Sections 1 and 2, are very dis-
tinct.
Use of Skip Connections: Skip connections have been a sub-
ject of extensive research since their introduction [15]. Vari-
ants like DenseNets [17] and SparseNets [25] have explored
different configurations of skip connections. Other works
like [47] have introduced weighted skip connections for
specific applications like super-resolution. However, none
of these works leverage skip connections for the purpose of
multi-task learning or patching, which is the primary focus
of this work.
Input-conditioned architectures: Several methods have
been proposed to adapt neural network architectures based
on the input [2, 8, 18, 48, 50] with the primary goal of ef-
ficient inference. SpotTune [12], for instance, fine-tunes
layers in an input-conditioned manner primarily for trans-
fer learning. While ∆-Networks also adapts to each input,
it does so uniquely by focusing on input-adaptive skip con-
nections, setting it apart from methods that adapt at the layer
or block level. To ensure a fair comparison, we compare our
method with the most recent transfer learning works in Ta-
ble 5.
5. Conclusions and Future Work
In this work, we propose a new setting ∆-Patching, to ex-
tend the definition of model patching [19] to general pur-
pose CNNs, and also ensuring the maintenance of perfor-
mance on a base task. We propose ∆-Networks as a sim-
ple and lightweight architectural modification to efficiently
implement our model patching variant in this work. ∆-
Patching significantly outperforms existing patching meth-
ods while only using a fraction of the parameters for train-
ing. We also show that the proposed approach can be
used for other problem settings such as transfer learning
and zero-shot domain adaptation, as well as other vision
tasks such as object detection and segmentation. Future di-
8rections include extensions to large-scale vision-language
models and simultaneous adaptation to multiple tasks while
only training a fraction of parameters.
References
[1] Pre-trained models for Image Classification, 01 2023.
https://huggingface.co/models?pipeline_
tag=image-classification&sort=downloads .
1
[2] Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max
Welling. Batch-shaping for learning conditional channel
gated networks. In International Conference on Learning
Representations , 2020. 8
[3] Nihar Bendre, Hugo Terashima Mar ´ın, and Peyman Najafi-
rad. Learning from few samples: A survey. arXiv preprint
arXiv: Arxiv-2007.15484 , 2020. 8
[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV) , September 2018. 6
[5] Xinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, and
Jianmin Wang. Catastrophic forgetting meets negative trans-
fer: Batch spectral shrinkage for safe transfer learning. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems , volume 32. Curran Associates,
Inc., 2019. 6
[6] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual
knowledge in language models. In Conference on Empirical
Methods in Natural Language Processing (EMNLP) , 2021.
8
[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2007 (VOC2007) Results. http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.
7
[8] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori
Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and
Daan Wierstra. PathNet: Evolution Channels Gradient
Descent in Super Neural Networks. arXiv e-prints , page
arXiv:1701.08734, Jan. 2017. 8
[9] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
InProceedings of the 34th International Conference on Ma-
chine Learning , pages 1126–1135, 2017. 8
[10] Xavier Glorot and Yoshua Bengio. Understanding the diffi-
culty of training deep feedforward neural networks. In JMLR
W&CP: Proceedings of the Thirteenth International Confer-
ence on Artificial Intelligence and Statistics (AISTATS 2010) ,
volume 9, pages 249–256, May 2010. 11
[11] Karan Goel, Albert Gu, Yixuan Li, and Christopher Re.
Model patching: Closing the subgroup performance gap with
data augmentation. In International Conference on Learning
Representations , 2021. 8
[12] Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grau-
man, Tajana Rosing, and Rogerio Feris. Spottune: Trans-
fer learning through adaptive fine-tuning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2019. 8
[13] David Ha, Andrew M. Dai, and Quoc V . Le. Hypernetworks.
InInternational Conference on Learning Representations ,
2017. 2
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification. In 2015 IEEE Interna-
tional Conference on Computer Vision (ICCV) , pages 1026–
1034, 2015. 11
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2016. 2, 8
[16] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and
Amos Storkey. Meta-learning in neural networks: A survey.
IEEE transactions on pattern analysis and machine intelli-
gence , 44(9):5149–5169, 2021. 8
[17] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-
ian Q. Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , July 2017. 8
[18] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-
ian Q. Weinberger. Deep networks with stochastic depth. In
Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, edi-
tors, Computer Vision – ECCV 2016 , pages 646–661, Cham,
2016. Springer International Publishing. 8
[19] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre,
Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali
Farhadi, and Ludwig Schmidt. Patching open-vocabulary
models by interpolating weights. In Advances in Neural In-
formation Processing Systems , 2022. 1, 2, 4, 5, 8
[20] Jiang Junguang, Baixu Chen, Fu Bo, and Mingsheng Long.
Transfer-learning-library. https://github.com/
thuml/Transfer-Learning-Library , 2020. 6
[21] Zhi Kou, Kaichao You, Mingsheng Long, and Jianmin Wang.
Stochastic normalization. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems , volume 33, pages
16304–16314. Curran Associates, Inc., 2020. 6
[22] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
4th International IEEE Workshop on 3D Representation and
Recognition (3dRR-13) , Sydney, Australia, 2013. 6, 7, 15
[23] Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao,
Liping Liu, and Jun Huan. DELTA: DEEP LEARNING
TRANSFER USING FEATURE MAP WITH ATTENTION
FOR CONVOLUTIONAL NETWORKS. In International
Conference on Learning Representations , 2019. 6
[24] Zhizhong Li and Derek Hoiem. Learning without forgetting.
InECCV , 2016. 6
[25] Wenqi Liu and Kun Zeng. SparseNet: A Sparse
DenseNet for Image Classification. arXiv e-prints , page
arXiv:1804.05340, Apr. 2018. 8
[26] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
Fine-grained visual classification of aircraft. Technical re-
port, 2013. 6, 7, 15
9[27] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and
Horst Bischof. The norm must go on: dynamic unsupervised
domain adaptation by normalization. In CVPR , 2022. 4, 5
[28] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn,
and Christopher D Manning. Fast model editing at scale.
InInternational Conference on Learning Representations
(ICLR) , 2021. 8
[29] M-E. Nilsback and A. Zisserman. Automated flower classi-
fication over a large number of classes. In Proceedings of the
Indian Conference on Computer Vision, Graphics and Image
Processing , Dec 2008. 15
[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas K ¨opf, Edward Yang, Zach DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu
Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-
perative style, high-performance deep learning library. arXiv
preprint arXiv: Arxiv-1912.01703 , 2019. 4
[31] Jo Plested and Tom Gedeon. Deep transfer learning for im-
age classification: a survey. arXiv preprint arXiv: Arxiv-
2205.09904 , 2022. 8
[32] Haoxuan Qu, Hossein Rahmani, Li Xu, Bryan Williams, and
Jun Liu. Recent advances of continual learning in com-
puter vision: An overview. arXiv preprint arXiv: Arxiv-
2109.11369 , 2021. 8
[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In Marina Meila
and Tong Zhang, editors, Proceedings of the 38th Interna-
tional Conference on Machine Learning , volume 139 of Pro-
ceedings of Machine Learning Research , pages 8748–8763.
PMLR, 18–24 Jul 2021. 2
[34] Colin Raffel. A call to build models like we build opensource
software, 2021. https://cutt.ly/d8Jbq4J . 1
[35] S-A Rebuffi, H. Bilen, and A. Vedaldi. Learning multiple
visual domains with residual adapters. In NeurIPS , 2017. 4,
5
[36] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Efficient parametrization of multi-domain deep neural net-
works. In CVPR , 2018. 4, 5
[37] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In Advances in Neural Information
Processing Systems (NIPS) , 2015. 6
[38] Marco Tulio Ribeiro and Scott Lundberg. Adaptive testing
and debugging of nlp models. In Association for Computa-
tional Linguistics (ACL) , 2022. 8
[39] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and
Sameer Singh. Beyond accuracy: Behavioral testing of NLP
models with CheckList. In Association for Computational
Linguistics (ACL) , 2020. 8
[40] Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango,
David Bau, Antonio Torralba, and Aleksander Madry. Edit-
ing a classifier by rewriting its prediction rules. In Advancesin Neural Information Processing Systems (NeurIPS) , 2021.
8
[41] Khadija Shaheen, Muhammad Abdullah Hanif, Osman
Hasan, and Muhammad Shafique. Continual learning for
real-world autonomous systems: Algorithms, challenges and
frameworks. Journal of Intelligent & Robotic Systems ,
105(1):1–32, 2022. 8
[42] Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and
Bhiksha Raj. Is normalization indispensable for training
deep neural network? In H. Larochelle, M. Ranzato, R. Had-
sell, M. F. Balcan, and H. Lin, editors, Advances in Neural
Information Processing Systems , volume 33, pages 13434–
13444. Curran Associates, Inc., 2020. 11, 12
[43] Samarth Sinha, Karsten Roth, Anirudh Goyal, Marzyeh
Ghassemi, Zeynep Akata, Hugo Larochelle, and Animesh
Garg. Uniform priors for data-efficient learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops , pages 4017–4028,
June 2022. 6
[44] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,
Sergei Popov, and Artem Babenko. Editable neural net-
works. In International Conference on Learning Represen-
tations (ICLR) , 2020. 8
[45] Yisheng Song, Ting Wang, Subrota K Mondal, and Jy-
oti Prakash Sahoo. A comprehensive survey of few-shot
learning: Evolution, applications, challenges, and opportu-
nities. arXiv preprint arXiv: Arxiv-2205.06743 , 2022. 8
[46] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Dar-
rell. Adversarial discriminative domain adaptation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , July 2017. 6
[47] Jiachen Wang and Yingyun Yang. Sellf-adaptive weighted
skip connections for image super-resolution. In 2020 Inter-
national Conference on Culture-oriented Science Technol-
ogy (ICCST) , pages 192–197, 2020. 8
[48] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and
Joseph E. Gonzalez. Skipnet: Learning dynamic routing
in convolutional networks. In Proceedings of the European
Conference on Computer Vision (ECCV) , September 2018. 8
[49] Ross Wightman. Pytorch image models, 2019. 4
[50] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven
Rennie, Larry S Davis, Kristen Grauman, and Rogerio Feris.
Blockdrop: Dynamic inference paths in residual networks.
InCVPR , 2018. 8
[51] Kaichao You, Zhi Kou, Mingsheng Long, and Jianmin Wang.
Co-tuning for transfer learning. In H. Larochelle, M. Ran-
zato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances
in Neural Information Processing Systems , volume 33, pages
17236–17246. Curran Associates, Inc., 2020. 6
[52] Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Residual
learning without normalization via better initialization. In In-
ternational Conference on Learning Representations , 2019.
4, 11
[53] Jincheng Zhong, Ximei Wang, Zhi Kou, Jianmin Wang, and
Mingsheng Long. Bi-tuning of Pre-trained Representations.
arXiv e-prints , page arXiv:2011.06182, Nov. 2020. 6
[54] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi,
Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A
10comprehensive survey on transfer learning. Proceedings of
the IEEE , PP:1–34, 07 2020. 8
Appendix
In this appendix, we provide additional details which we
could not include in the main paper due to space constraints,
including some quantitative and qualitative results, as well
as corresponding discussion and analysis that provide more
insights into the proposed method.
6. Theoretical Analysis of Variance Issues
As mentioned towards the end of Section 2, with ∆-
Networks, when all possible skip connections are intro-
duced, the variance increases rapidly since we add features
from all preceding blocks. We address this issue using one
BN layer in each ∆-module in our implementation. In this
section, we discuss the reasons for increase in variance in
detail, and prove it formally.
Weight initialization methods like He’s initialization [14]
are quite commonly used in all CNN architectures to main-
tain the variance between input and output of every block
or layer. However, when skip connections are present in an
architecture, such initialization methods [14] result in in-
creased variance as shown in [52]. To understand this prob-
lem better, we look at the output of a given jth block, Fj,
with one skip connection. The output representation is then
given by: Fj=Fj(Fj−1(x)) +Fj−1(x). The variance of
the output representation can hence be written as:
Varh
Fji
= Varh
Fj(Fj−1(x) +Fj−1(x)i
= Varh
Fj(Fj−1(x))i
+ Varh
Fj−1(x))i
+ Covh
Fj(Fj−1(x)),Fj−1(x))i (5)
where the first two terms refer to variance of input and
output blocks respectively, and the third term denotes the
covariance between input and output. While initialization
methods maintain the variance between input and output of
a block [14], when skip connections are present, the overall
variance of the block output is approximately two times the
variance of the input.
Varh
Fji
≈2 Varh
Fj−1(x)i
(6)
To show this formally, we follow two observations: (1)
When Kaiming initialization is used, the output variance
of a residual block (without skip connection) is (approxi-
mately) equal to the input variance i.e., Var(Fj−1)(x)≈
Var(x); and (2) The input and output of a residual block
are weakly correlated (as also suggested in [42]).
(1) Output Variance ≈Input Variance: With Kaiming
initialization, Var(Fj−1(x))≈Var(x)[14]. Consider the
variance at a particular convolutional layer of a neural net-
work, whose output is given by:
yl= W lxl+bl (7)where xis ak2c×1vector that represents k×kpixels in c
input channels, and kis the spatial filter size. With n=k2c
denoting the number of connections of a response, Wis a
d-by-nmatrix, where dis the number of filters and each
row of Wrepresents the weights of a filter. bis a vector of
biases, yis the response at a pixel of the output map, and l
denotes the index of a layer. xl=f(yl−1)where fis the
activation and cl=dl−1. As in [10,14], we assume that ini-
tialized elements in Wlare mutually independent and share
the same distribution, as well as that xlare mutually in-
dependent and share the same distribution. xlandWlare
independent of each other. Then:
Var[yl] =nlVar[wlxl], (8)
where yl,xl, andwlrepresent the random variables of each
element in yl,Wl, and xlrespectively. Since Kaiming ini-
tialization forces wlto have a zero mean, the variance of the
product of independent variables gives us:
Var[yl] =nlVar[wl]E[x2
l]. (9)
Please note that E[x2
l]̸= Var[ xl]unless xlhas zero mean.
For the ReLU activation (which is used in modern-day ar-
chitectures), xl=max(0, yl−1)and thus may not have
zero mean. If we let wl−1also have a symmetric distri-
bution around zero and bl−1= 0, then yl−1has zero mean
and has a symmetric distribution around zero. This leads
toE[x2
l] =1
2Var[yl−1]when fis ReLU. Combining this
with Eqn (9), we obtain:
Var[yl] =1
2nlVar[wl] Var[ yl−1]. (10)
With Llayers put together, we have:
Var[yL] = Var[ y1] LY
l=21
2nlVar[wl]!
. (11)
The above equation is key for Kaiming initialization. A
good initialization method should avoid reducing or magni-
fying the magnitudes of input signals significantly. Hence,
for the product in Eqn 11 to only grow linearly (say by a
scalar factor 1), a sufficient condition is:
1
2nlVar[wl] = 1,∀l. (12)
This leads to a zero-mean Gaussian distribution whose stan-
dard deviation (std) isp
2/nl. This is used in Kaiming ini-
tialization along with initializing b= 0. Because of this
careful initialization, the output variance of a residual block
without skip connection, Var(Fj−1(x))is approximately
equal to the input variance with Kaiming initialization, i.e.
Var(Fj−1(x))≈Var(x) (13)
11(2) The input and output of the residual block are
weakly correlated: Following [42], to evaluate the covari-
ance term Cov(Fj(Fj−1(x)),Fj−1(x)))in Eqn 5, we as-
sume that any two coordinates in xare uncorrelated. With
this assumption, one can show that the covariance term is
about O(1/√
d)small. To this end, we consider a single
layer: F(x) = Wx, where elements of Ware sampled
from a Gaussian as: wij∼ N (0,1/d)to match the in-
put/output variance.
Cov 
[x]i,
Fj(x)
i
= Cov
[x]i,dX
j=1wij[x]j

=dX
j=1wijCov
[x]i,[x]j
=wiiVar ([ x]i)(14)
The above equation holds since we assume any two coordi-
nates in xare uncorrelated. From Eqn 14, Var([ xk+1]i)≈
(2 +wii) Var([ xk]i). Since wij∼ N(0,1/d), with proba-
bility at least 1−exp(−d/4),2 +wii>2−1/√
2. In a
common ResNet, with d≥32, the output variance of the
residual block increases exponentially with very high prob-
ability. If there are multiple layers in the residual block F,
ReLU activations would further decrease (at least not in-
crease) the correlation between xandF(x). So, the corre-
lations between xandF(x)areO(√
d)small. We request
the interested reader to refer to [42] for more details on this
analysis.
The above analysis formally shows the problem of increase
in variance when skip connections are introduced. This
forms the primary motivation for introducing additional
batch-normalization layers in ∆-Networks (as discussed to-
wards the end of Section 2).
7. Ablation Studies for Patching
Owing to space constraints, we could not provide a detailed
analysis of the patching experiments in the main paper. We
present these results in this section.
7.1. Single-Task Patching
Tables 2, 3, 4 in the main paper presents the harmonic mean
of the top-1 accuracies of the final patched model on the
patching task and the Base Task. While harmonic mean as
discussed in the main paper gives equal importance to per-
formance on all the previously learned tasks and the patch-
ing task, Top-1 accuracies on the base and patched tasks
provide a holistic view. Table 10 presents top-1 accuracies
of the patched model on the base task (ImageNet) and the
patching task (dataset for which the model is leveraged for).
In most cases, PAINT with α= 0has the highest harmonicmean, where α= 0 corresponds to the model containing
only the weights pertaining to the base task i.e., ImageNet
pre-trained weights in this setting. As a result, the ImageNet
accuracy is good but the accuracy on the patched task is
very low as evident in Table 10. ∆-Networks perform sig-
nificantly better than PAINT on the downstream task while
also maintaining the performance on the base task.
7.2. Sequential Patching: Understanding the per-
formance drop in PAINT
In Table 4 of the main paper, we presented results for se-
quential patching where we report the mean of Top-1 accu-
racies of the final model on all patching tasks. As PAINT
requires choosing an αvalue empirically, we choose the α
value that gives best performance on all the previous tasks
(mean based aggregation). To provide better insights into
how PAINT performs on all previous tasks with various in-
terpolation coefficient values, we present the top-1 accura-
cies for all values of αin Table 11. As PAINT does not
retain the base model weights or weights learned for each
task, there is significant drop in performance on all previ-
ously learned tasks.
8. Training ∆-Networks from Scratch
All experiments shown so far deal with pre-trained mod-
els. It is also important to understand how ∆-Networks be-
haves when training the model from scratch. To this end,
we train and compare standard models and models with ∆-
Networks without any form of pre-training. Results for this
experiment are shown in Table 12. ∆-Networks shows con-
sistent improvement even when there is no pre-training in-
volved. Since the fine-grained datasets considered are rela-
tively smaller in size, larger models tend to overfit and per-
form poorly on test data. ∆-Networks helps in avoiding this
overfitting problem when training from scratch with limited
data.
To test the effectiveness of ∆-Networks on commonly used
image classification benchmarks, we run experiments on
CIFAR-10, CIFAR-100 and ImageNet datasets. Results are
shown in Table 13. ∆-Networks once again shows im-
provement on these benchmarks with no pre-training (with-
out significant hyperparameter tuning). (For the Imagenet
dataset, we show results only with ResNet-18 architecture
due to the significant time and computation requirements
otherwise.)
9. Visualizations
9.1. Visualizing Learned ΛValues
To see if ∆-Networks uses all skip connections, we visual-
ize the weights of these skip connections in Fig 3. We take
the average value of weights for all test samples in FGVC-
aircraft and Stanford-Dogs datasets. Even without imposing
any specific sparsity schemes such as L1-penalty, some of
12Architecture Method # Parameters RESISC-45 STL-10 GTSRB Aircraft Flowers Cars
ImageNet RESISC-45 ImageNet STL-10 ImageNet GTSRB ImageNet Aircraft ImageNet Flowers ImageNet Cars
ResNet-18PAINT 11.2M 68.87 14.81 68.87 48.35 68.87 10.61 68.87 1.80 68.87 6.99 68.87 1.02
∆-Networks (1) 0.4M 68.87 81.10 68.87 94.14 68.87 71.62 68.87 35.16 68.87 83.90 68.87 35.42
ResNet-34PAINT 21.8M 74.66 12.84 74.66 53.95 74.66 11.46 74.66 1.50 74.66 9.06 74.66 2.26
∆-Networks (1) 0.4M 74.66 81.70 74.66 94.49 74.66 67.38 74.66 32.97 74.66 82.34 74.66 34.62
ResNet-50PAINT 23.5M 80.15 34.30 80.15 93.46 80.15 17.05 80.15 3.84 52.91 61.70 80.15 6.80
∆-Networks (1) 7.1M 80.15 88.29 80.15 96.40 80.15 75.57 80.15 42.81 80.15 85.28 80.15 47.07
ResNet-101PAINT 42.5M 81.87 27.57 74.66 56.79 78.27 10.26 81.87 2.54 64.50 52.90 81.87 5.75
∆-Networks (1) 7.1M 81.87 85.13 81.87 97.01 81.87 73.35 81.87 39.18 81.87 83.72 81.87 43.10
ConvNeXt-TinyPAINT 27.8M 82.47 22.06 86.78 82.47 82.47 6.68 82.47 3.21 76.69 62.81 82.47 3.58
∆-Networks (1) 4.0M 82.47 91.03 82.47 98.29 82.47 82.48 82.47 60.19 82.47 93.15 82.47 65.49
ConvNeXt-SmallPAINT 49.5M 83.30 25.95 83.30 79.71 83.30 14.93 83.30 4.20 76.22 63.23 83.30 4.53
∆-Networks (1) 4.0M 83.30 89.11 83.30 98.53 83.30 80.38 83.30 54.40 83.30 91.59 83.30 60.27
ConvNeXt-BasePAINT 87.6M 84.15 26.52 84.15 75.54 84.15 11.25 84.15 3.39 77.52 80.53 84.15 6.85
∆-Networks (1) 7.1M 84.15 89.43 84.15 98.98 84.15 80.52 84.15 53.23 84.15 90.06 84.15 62.53
ConvNeXt-LargePAINT 196.2M 84.52 22.92 84.52 77.25 84.52 9.89 8.02 61.33 80.99 73.87 84.52 5.77
∆-Networks (1) 15.9M 84.52 90.29 84.52 98.99 84.52 79.40 84.52 52.99 84.52 89.80 84.52 62.60
Table 10. Top-1 accuracies of the final patched model on the base task (ImageNet) and the patching task. Harmonic mean of these values
are presented in Table 2 of main paper. denotes the experiments where the accuracy of our proposed method and the baseline (PAINT)
are same. ∆-Networks significantly outperform PAINT on the patching task, while maintaining the accuracy on the base task.
Method (Base Model)Alpha
(interpolation
coefficient)Aircraft-Flowers-STL GTSRB-Cars-Flowers STL-Cars-GTSRB
PAINT
(ConvNeXT-Tiny)ImageNet Aircraft Flowers STL ImageNet GTSRB Cars Flowers ImageNet STL10 Cars GTSRB
0.0 0.09 1.26 28.59 18.31 0.11 1.91 71.74 13.03 0.09 11.18 72.96 7.63
0.1 0.10 0.99 0.33 10.00 0.10 1.98 71.62 15.53 0.07 11.34 23.06 52.37
0.2 0.10 0.99 0.42 10.00 0.10 1.95 71.41 18.38 0.09 10.60 2.49 89.12
0.3 0.10 0.99 0.42 10.00 0.10 1.91 71.09 21.74 0.11 10.54 1.09 96.05
0.4 0.10 0.99 0.42 10.00 0.09 1.77 70.40 26.26 0.13 10.53 0.80 97.34
0.5 0.10 0.99 0.42 10.00 0.08 1.80 69.95 30.61 0.13 10.74 0.63 97.77
0.6 0.07 0.99 0.65 10.09 0.09 1.84 68.95 35.71 0.12 10.76 0.53 97.85
0.7 0.07 1.02 1.09 12.51 0.09 1.94 67.77 40.32 0.13 10.51 0.53 97.85
0.8 0.08 0.54 1.68 33.81 0.11 1.92 66.57 44.30 0.12 10.36 0.53 97.71
0.9 0.12 1.05 2.26 65.40 0.11 2.01 65.30 47.78 0.12 10.20 0.52 97.43
1.0 0.12 1.17 3.55 74.95 0.12 1.94 63.15 49.52 0.12 10.14 0.51 97.26
ConvNeXT-Tiny
+
∆-Networks (1)- 82.47 60.19 93.15 98.29 82.47 82.48 65.49 93.15 82.47 98.29 65.49 82.48
PAINT
(ResNet-18)0.0 0.07 1.38 13.73 37.08 0.06 5.70 2.54 0.55 0.08 35.25 1.14 6.13
0.1 0.10 0.99 0.47 10.00 0.09 5.69 4.24 0.55 0.10 10.00 0.45 5.46
0.2 0.10 1.00 0.50 10.00 0.09 5.73 7.72 0.55 0.10 10.00 0.53 5.46
0.3 0.10 1.00 0.50 10.00 0.07 5.66 13.98 0.57 0.10 10.00 0.53 5.46
0.4 0.10 1.00 0.50 10.00 0.07 6.32 23.70 0.85 0.10 10.00 0.44 5.46
0.5 0.10 1.00 0.50 10.00 0.08 4.62 32.48 1.72 0.10 10.01 0.44 5.46
0.6 0.11 1.00 0.60 10.03 0.07 3.20 39.25 3.43 0.10 10.00 0.44 2.85
0.7 0.09 1.01 0.82 11.07 0.09 2.15 41.67 8.51 0.10 10.00 0.44 4.99
0.8 0.10 0.79 1.06 25.95 0.09 2.28 38.74 18.26 0.14 12.68 0.55 17.81
0.9 0.12 0.97 1.24 64.39 0.07 2.88 29.39 31.11 0.10 10.46 0.36 94.14
1.0 0.12 1.02 1.73 77.20 0.06 4.55 13.80 38.69 0.10 8.44 0.47 98.38
ResNet-18
+
∆-Networks (1)- 68.87 35.16 83.9 94.14 68.87 71.62 35.42 83.9 68.87 94.14 35.42 71.62
Table 11. Sequential Patching on ConvNeXT-Tiny and ResNet-18: Top-1 accuracies of the final model for each value of interpolation coefficient ( αused by
PAINT) on all previously learned tasks, including the base task. While PAINT has a significant drop in performance on the previous tasks, ∆-Networks
have zero-drop in performance on the previously learned tasks.
the skip connection weights are zeros, which shows how the
∆-Networks learns to combine features with few skip con-
nections, which is better than finetuning entire architectures
for feature reuse.9.2. Visualization of Learned Design Principles
We visualize the learned values of Λ(weights assigned for
the skip connection by the ∆-module) for ResNet-18 on
samples from three of the datasets we considered in our ex-
13Architecture FGVC-Aircraft Stanford-Cars
ResNet-18 53.40 53.47
+∆-Networks (4) 54.12 55.92
ResNet-34 48.59 43.39
+∆-Networks (4) 50.93 54.54
Table 12. Comparison of different architectures when trained
from scratch without any pre-training. ∆-Networks can help
when training models from scratch on fine-grained datasets .
Architecture CIFAR-10 CIFAR-100 ImageNet (Top-5)
ResNet-18 85.21 50.43 85.87
+∆-Networks (4) 85.47 52.17 86.16
ResNet-34 81.25 48.05 -
+∆-Networks (4) 84.76 51.77 -
Table 13. Comparison of different architectures when trained
from scratch without any pre-training on standard image classifi-
cation datasets. ∆-Networks can help improve performance on
common image classification benchmark datasets without pre-
training .
Block-1Block-2Block-3Block-4
Index of the Skip connection 𝜆value (avg) FGVC-Aircraft
Stanford Cars
Figure 3. Despite not imposing a specific sparsity scheme, ∆-Networks
learns to select input-conditioned skip connections in a task-specific
manner and zeros out some skip connections (especially in Blocks
2 and 3) . Qualitative visualization of the weights (average) learned by
the∆-Networks for all test samples of FGVC-Aircraft and Stanford Cars
datasets, we use ResNeXt-50 as the base network and introduce four ∆-
modules in our ∆-Networks, one for each block
periments.These visualizations are shown in Figures 4, 5, 6.
These visualizations provide interesting insights that could
be useful for architecture design in general. In particular,
we note that: (i) In the initial modules, skip connections
from the beginning of the block to the end of the block are
sufficient, skip connections across blocks have a very less
weight; and (ii) In the last module, all skip connections are
required with each of them having a weight of 1. One com-
mon trend seen across all datasets is that weights for most
skip connections in the last module of the network are al-
ways 1 consistently. This can also be observed in Figure3.
141 10.31 101 0.80.91 11
1 0.90.51 010.9 0.70.81 11
Figure 4. Visualization of Λvalues on two samples of FGVC-Aircraft dataset [26] for ResNet-18 network
1 101 10.11 10.91 11
0.9 0.80.11 0.90.20.9 0.90.81 11
Figure 5. Visualization of Λvalues on two samples of Flowers-102 dataset [29] for ResNet-18 network
1 10.31 0.901 0.90.61 11
1 0.90.11 101 10.51 11
Figure 6. Visualization of Λvalues on two samples of Stanford-Cars dataset [22] for ResNet-18 network
15