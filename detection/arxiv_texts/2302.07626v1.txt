Correction of  ‘ J. Laderman, V. Pan, X.-H. Sha, On practical Algorithms for 
Accelerated Matrix Multiplication,  Linear Algebra and its Applications. Vol. 162-164 
(1992) pp. 557-588’  1 
Jerzy S. Respondek  
Silesian University of Technology, Faculty of Automatic Control, Electronics and Computer Science 
Applied Informatics Department, ul. Akademicka 16, 44-100 Gliwice, Poland, jrespondek@polsl.pl 
 
Abstract 
 
In this article we corrected the trilinear formula for triple disjoint matrix multiplication given 
in the article ‘ J. Laderman, V. Pan, X. H. Sha, On practical Algorithms for Accelerated Matrix 
Multiplication, Linear Algebra and its Applications. Vol. 162-164 (1992) pp. 557-588’ , which 
is incorrect for matrix dimensions equal to two or greater. That formula is a base of two 
algorithms, for disjoint and single matrix multiplication. The necessary correction made the 
amount of non-scalar products raise, slightly increasing the algorithm time complexity. We also 
corrected explicit formulas, in the bilinear form, for triple disjoint matrix multiplication. They 
are explicit, thus convenient for practical use of fast matrix multiplication algorithms in 
question. 
Keywords : Fast matrix multiplication, Numerical mathematics, Matrix algebra 
MSC: 65F05, 15A99  
 
1. Introduction 
The article [4], as a base for practical algorithms for triple disjoint and single matrix 
multiplication, gave trilinear identity (1) with no proof. We show that it needs to be corrected 
to find correct values of certain constant parameters. 
To find correct values of those parameters, and simultaneously correct algorithms  
in the bilinear form, we proposed proof of the identity (1). The article [4] proposed  
for the ,f g parameters to use particular values, which caused that a few terms vanished,  
this way improving the efficiency. We proved that in the correct form of trilinear identity (1) 
such optimization is not possible. 
The paper is organized as follows: in section 2 we performed the proof of trilinear identity 
(1) finding correct values of a few parameters, in section 3 we presented the algorithm for triple 
disjoint matrix multiplication in the bilinear form, in section 4 we determined the complexities 
for triple disjoint and single matrix multiplication yielding from identity (1) and in section 5 
we discussed the results. 
  
                                                 
1 This work is supported by the National Research Fund No 02/100/BK_23/0027 J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
2/10 
 2. Trilinear representation of three disjoint matrix products 
Our objective is to calculate three disjoint matrix products ,C AB W UV   and Z XY, 
where [ ],ijA a [ ],jkB b [ ],jkU u [ ],kiV v [ ],kiX x [ ],ijY y and , ,i j k range from
1 to n. The article [4], on pages 565-566 of section 3, proposes the following trilinear identity, 
with the q parameter equal to 1, representing three disjoint matrix products2: 
, ,Trace( ) ( )ij jk ki jk ki ij ki ij jk
i j kABC UVW XYZ a b c u v w x y z         
, ,
0 1 2
' ' '
0 1 2( )( )( )
( , , ) ( , , ) ( , , )ij jk ki jk ki ij ki ij jk
i j ka u x b v y c w z
T i j k T j k i T k i j
T T T       
  
   
 
,
,
,1 1
1 1
1 1
1 1ij ki ij ki ij ki
i j k k k
ij jk ij jk ij jk
i j k k k
jk ij jk ij jk ij
j k i i i
jk ki jk ki
i ia x y v gw cg g
a u y b hw zh h
u a b y gz wg g
u x b vh h                
             
                
        
   
   
 
,
,
,(*)
1 1
1 1jk ki
j k i
ki jk ki jk ki jk
k i j j j
ki ij ki ij ki ij
k i j j jhz c
x u v b gc zg g
x a v y hc wh h







      
                
                
   
    (1)  
for .g h n  The sub-expressions are the following: 
0 1 2( , , ) , ( , , ) , ( , , )ij ki jk jk ij ki ki jk ij T i j k a v z T j k i u y c T k i j x b w   
  
'
0
2 21 1 1 1
ij ki ij ki ij ki
i j k j k j k
ij ij ij ki ki ki
j j j k k kT a x y v g w h ch g h g
q g q ha y w x v ch g             
    
      
      
                                                 
2 We changed the notation from 0 1( ), ( )T i T j  and 2( )T k to ' '
0 1,T T and '
2T, because these expressions  
    do not depend on ,i j and k, respectively. J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
3/10 
 '
1
2 21 1 1 1
ij jk ij jk ij jk
j i k i k i k
ij ij ij jk jk jk
i i i k k kT a u y b h w g zg h g h
q h q ga y w u b zg h                
         
      
'
2
2 21 1 1 1
ki jk ki jk ki jk
k i j i j i j
ki ki ki jk jk jk
i i i j j jT x u v b g c h zh g h g
q g q hx v c u b zh g             
    
      
      
To find the value of q, we will carry out the proof of (1). Let us start from the algebraic 
identity (5) from section 5 of the article [6], page 171: 
( )( )( )
( ) ( ) ( )ij jk ki jk ki ij ki ij jk ij jk ki jk ki ij ki ij jk
ij ij ki ij jk jk jk ij jk ki ki ki jk ki ija b c u v w x y z a u x b v y c w z
a y c w z u b w z c x v z c w         
              

     ij jk ki ij jk ki ij jk ki ij jk ki
ij jk ki ki jk ki ij ij ki ij jk jka b v w u v y z x y b c
a u v c u x y w x a b z      
        (2) 
ij ki jk jk ij ki ki jk ija v z u y c x b w    
  
Let us sum both sides of (2) with respect to ,i j and k. We arrive at formulas (6, 7) of [6]:  
, ,Trace( ) ( )ij jk ki jk ki ij ki ij jk
i j kABC UVW XYZ a b c u v w x y z        
, ,( )( )( )ij jk ki jk ki ij ki ij jk
i j ka u x b v y c w z         (3) 
, ,
,( ) ( )
(A)
( )ij ij ki ij jk jk jk ij jk ki
i j k j k i
ki ki jk ki ij
k i ja y c w z u b w z c
x v z c w       

    
   
 
   
   
   , ,
, ,
, ,(B)ij jk ki ij jk ki ij jk
i j k j k i
ki ij jk ki ij jk ki ki
k i j k i j
jk ki ij ij ki ij jk jk
i j k j k ia b v w u v y z
x y b c a u v c
u x y w x a b z                 
             
    
                   
   
    

, , , , , ,(C)ij ki jk jk ij ki ki jk ij
i j k i j k i j ka v z u y c x b w    
 
 J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
4/10 
 We will show that the trilinear formula (1) reduces to identity (3). For this purpose 
we divide factors in (1) into separate, complementary groups. 
 As the first group of factors let us have a look at a series of sums indicated by (*) of (1), 
the following three sums of triple products: 
, ,
, ,
, ,ij ij ij ki ij ij ij jk
i j k i j k
jk jk jk ij jk jk jk ki
j k i j k i
ki ki ki jk ki ki ki ij
k i j k i ja y gw c a y hw z
u b gz w u b hz c
x v gc z x v hc w           
           
        
      
   
    (4)
  
Considering g h n  we obtain, by the example of the first sum: 
, ,
, ,ij ij ij ki ij ij ij jk
i j k i j k
ij ij ij ij ki jk ij ij ij ki jk
i j k k i j k ka y gw c a y hw z
a y gw hw c z a y nw c z            
                   
     
 
We can rewrite ijnw in the form ij
kw. So: 
, ,( )ij ij ij ki jk ij ij ki ij jk
i j k k i j ka y nw c z a y c w z           
  
Thus this part of proven equality (1) is reduced to the first sum of part (A) in (3). 
The remaining two expressions in (4) analogically represent the second and third sum 
of part (A) in (3). 
 For the second group of terms we again look at from the series (*) in (1) six triple product 
sums of the following form: 
, ,
, ,
,1 1 1 1,
1 1 1 1,
1 1 1 1,ki ki ij jk jk ij
i j k k i j k k
ij ij jk ki ki jk
j k i i j k i i
jk jk ki ij ij
k i j j j jx v gw u b hwg g h h
a y gz x v hzg g h h
u b gc a y hg g h h     
          
     
          
     
     
          
     
    
,ki
k ic  (5) 
To get to know which of the above terms (5) corresponds to the terms in (3) and/or 
is reduced by the correction terms ' ' '
0 1 2, ,T T T and 0 1 2( , , ), ( , , ), ( , , )T i j k T j k i T k i j ,  
  J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
5/10 
      we look at the example of a selected expression from (5): 
,1 1 1 1
ij ij ki ij ij ki
k i j j i j j ka y hc a y h ch h h h           
              
It is now clear that each of the expressions (5) is reduced by its matching part  
in the terms ' '
0 1,T T or '
2T just in the trilinear equality (1). 
 Another group of terms includes expressions from the series (*) in (1) of the form: 
, ,
, ,
,1 1
1 1
1 1ij jk ij jk ij jk jk ij
i j k k j k i i
jk ki jk ki jk ki ki jk
j k i i k i j j
ki jk ki jk ki jk
k i j j ia b hw z a b gz wh g
u v hz c u v gc zh g
x b gc z x b hg h                     
                     
                   
     
   
,
, ,
, ,
,1 1
1 1
1jk ki
j k i
ij ki ij ki ij ki ki ij
i j k k k i j j
jk ij jk ij jk ij ij jk
j k i i i j k k
ki ij ki ij
k i j jz c
a v gw c a v hc wg h
u y gz w u y hw zg h
x y hc wh   
                        
                        
     
   
     
     
 
,1
ki ij ij ki
i j k kx y gw cg              (6) 
By the example of the first expression in series (6) we give a general rule: 
 1 1
1 1
1 1
1 1, ,
, ,1 1
1 1
1 1ij jk ij jk ij jk jk ij
i j k k j k i i
ij jk ij jk ij jk jk i j
i j k k i
ij jk ij ij jk jk ij jk jk i j
k ia b hw z a b gz wh g
a b w z a b z wh g
a b w a b z a b z wh g                      
            
     
         
         
  
 
, ,i j k   
We can see that each of the six sums (6) produces two groups of expressions: 
- Sum of the expressions of the form ij jk ij ij jk jka b w a b z . It is easy to see that they are 
desired terms, which summed over all six expressions (6) forms part (B) of equality (3). 
- Sum of the expressions of the form  1 11 11 1ij jk jk i j k ia b h z g w    which are undesired. 
They need to be reduced by correction terms of trilinear equality (1). Let us find 
a corresponding expression in (1), giving terms of a general formij jk ija b w    
and ij jk jka b z   . J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
6/10 
 We can find a proper expression in the correction term '
1T: 
1 1
1 1
1 1
1 1 , ,1 1 1
1 1ij jk ij jk ij jk i j jk
j i k i k j i k i k
ij jk jk i j
i j k k ia b h w g z a b h w g zg h gh
a b z wh g                      
    
        
   
To sum up, the group of six terms of the form (6) create group (B) of equality (3) 
and the undesired expressions they produce are corrected by the corresponding correction 
terms of ' ' '
0 1 2, ,T T T.  
It is also worth to notice that all the terms in (3) are covered by the so-far presented 
groups of expressions from trilinear identity (1). It means that the remaining terms in (1) 
reduce themselves within that identity.  
 The last group of terms in the series (*) in (1) comprises six sums of triple products,  
of a common form: 
, ,
, ,1 1 1 1,
1 1 1 1,
1 1ki ki ki jk jk jk
i j k k k i j k k k
ij ij ij ki ki ki
j k i i i j k i i i
jk jk jk
j j jx v c u b zg g h h
a y w x v cg g h h
u b zg g        
                
        
                
  
  
         
       
  
, ,1 1,ij ij ij
k i k i j j ja y wh h    
    
         (7)  
Let us analyze the first trilinear product in (7): 
,1 1
ki ki ki
i j k k kx v cg g    
           
  
The inside, summed expression  1 1ki ki kik k kg x g v c    does not depend 
on the j variable. So we can claim: 
,1 1 1 1
ki ki ki ki ki ki
i j k k k i k k kx v c n x v cg g g g                                      (8)
  
Thus the corresponding term in '
0T reduces (8) iff q n. For the remaining sums 
of triple products (7) there exist corresponding five reducing expressions in ' '
0 1,T T and '
2T. 
  J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
7/10 
  The last group of terms are in ' '
0 1,T T and '
2T. These terms balance themselves in the given 
term, in the outside sums. The terms in question are the following: 
'
02 2
'
12 2
'
22 2: ,
: ,
: ,ij ij ij ki ki ki
j j j k k k
ij ij ij jk jk jk
i i i k k k
ki ki ki jk jk jk
i i i j j jg hT a y w x v ch g
h gT a y w u b zg h
g hT x v c u b zh g     
     
      
To sum up, in the trilinear identity in section 3 of the article [4], pages 565-566,  
its part included in the correction terms ' '
0 1,T T and '
2T of a general form: 
1 2 1 2 1 2 1 2 1 2 1 2(1) (2) (3) (4) (5) (6)
2 21 1, andii ii ii ii ii iig hf f f f f fh g        
requires to be corrected to, respectively: 
1 2 1 2 1 2 1 2 1 2 1 2(1) (2) (3) (4) (5) (6)
2 2, andii ii ii ii ii iin g n hf f f f f fh g        
3. Bilinear form of the algorithm 
In this section we show the bilinear form of the algorithm which is ready for practical 
purposes. It is obtained by equating the coefficients of ,ki ijc w and jkz on both sides.  
Like in [4] we assume that 1g and 1h n  as well as we omit the 0 1( , , ), ( , , )T i j k T j k i  and 
2( , , )T k i j  terms. 
At first let us define the non-scalar products. We have to perform 3n of ijkP products: 
( )( )ijk ij jk ki jk ki ijP a u x b v y     
Next come six series with 2n products in each of them, of the form: 
(0) (1)
, ,
(2) (3)
, ,
(4)1 1,1 1
1 1,1 1ij ij ki ij ki ij ij jk ij jk
i j k k i j k k
jk jk ij jk ij jk jk ki jk ki
j k i i j k i i
ki ki jk
jP a x y v P a u y bn n
P u a b y P u x b vn n
P x u                      
                      
 
     
     
(5)
, ,1 1,1 1ki jk ki ki ij ki ij
k i j k i j jv b P x a v yn n                        
 
 
 
 J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
8/10 
 Finally from ' '
0 1,T T and '
2T we have nine groups with n products in each, of the form: 
(0)
(1) (2)1 1
1 1
1,1i ij ki ij ki
j k j k
i ij ij ij i ki ki ki
j j j k k kP a x y vn n
P a y w P x v cn         
    
      
(3)
(4) (5)1 1
1 1
1,1j ij jk ij jk
i k i k
j ij ij ij j jk jk jk
i i i k k kP a u y bn n
P a y w P u b zn          
    
      
(6)
(7) (8)1 1
1 1
1,1k ki jk ki jk
i j i j
k ki ki ki k jk jk jk
i i i j j jP x u v bn n
P x v c P u b zn         
    
      
The desired three matrix products are expressed explicitly by linear combinations  
of the above products: 
 (0) (3) (4) (5) (0) (2) (6) (7)( 1) ( 1)ijk ij jk ki ki i i k k ik
jC AB P P P P n P n P P P P                  
 (2) (5) (0) (1) (0) (1) (3) (4)( 1) ( 1)ijk jk ki ij ij i i j j ji
kW UV P P P P n P P P n P P                  
 (1) (4) (2) (3) (3) (5) (6) (8)( 1) ( 1)ijk ij ki jk jk j j k k kj
iZ XY P P P P n P P P n P P                  
4. Time complexity issues 
Without the 0 1( , , ), ( , , )T i j k T j k i  and 2( , , )T k i j  terms, trilinear identity (1) contains 
3 26 9n n n   products. Taking like in the article [4] 1g and 1h n , or any other non-zero 
values satisfying g h n , does not cause 3n terms to vanish. Applying the 2 2 block 
scheme for n n matrices from section 4 of the article [4] we get rid of the 0 1( , , ), ( , , )T i j k T j k i  
and 2( , , )T k i j  terms and obtain: 
3 2 3 2
disjoint(2 ) 8( 6 9 ) (2 ) 12(2 ) 36(2 )M n n n n n n n        
products necessary to calculate. The obtained exponent expression 
is 2 disjointlog (2 )/3nM n    . For 2 48n we obtain an algorithm with exponent 2.77706  
Applying trilinear identity (1) for single matrix multiplication, in a way from  
section 6 of the article [4], we have to calculate 3 2( )/3 2 3n n n n    products.  
The 2 2 block scheme for n n matrices leads here to J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
9/10 
 3
3 2 2
single8 (2 ) 32(2 ) ( ) 16 24 4(2 ) (2 )3 3 3nM n n n n n n n         
terms. The exponent inequality here is 2 singlelog (2 )nM n     with minimum also for 2 48n 
equal to 2.776706 . 
5. Conclusions and perspectives 
Since the publication of Strassen’s work [7], which gave the first non-commutative 
algorithm for matrix multiplication with time complexity exponent smaller than definitional 3, 
the exponent was improved a series of times currently reaching 2.3728596  in the work [1]. 
The exponents yielding from the algorithms proposed in this article and the article [4] 
are slightly above 2.77, but they are easier to use. 
As an example of the technique used in algorithms giving better exponents we can mention 
the so-called approximate algorithms, devised by Bini, Capovani et al. [2]. The approximate 
algorithms can be transformed into exact ones by the technique presented by Bini [3],  
but it induces an additional logarithmic term in the obtained complexity. Another technique 
is to apply disjoint matrix multiplication for a single matrix multiplication, but to be effective 
it requires a series of recursion steps. The general theorem to determine the complexity 
exponent by this technique, as well as for the combination with approximate methods,  
is given in the article [5]. 
On the opposite, algorithms presented in [4] corrected by this article require neither 
recursion nor approximate techniques to reduce the number of non-scalar multiplication  
to be performed. Thus they have potential to be convenient in practice.  
Acknowledgement 
I would like to thank my university colleagues for stimulating discussions. 
References 
[1] J. Alman, V. Vassilevska-Williams, A Refined Laser Method and Faster Matrix 
Multiplication, Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms, 
pp. 522-539. 
[2] D. Bini, M. Capovani, G. Lotti, F. Romani, O(n^2.7799) Complexity for approximate 
matrix multiplication. Inf. Proc. Lett. 8 (5) (1979) 234-235. 
[3] D. Bini, Relations between exact and approximate bilinear algorithms. Applications. 
Calcolo 17 (1980) 87-97. 
[4] J. Laderman, V. Pan, X.-H. Sha, On practical Algorithms for Accelerated Matrix 
Multiplication, Linear Algebra Appl. 162-164 (1992) 557-588. 
[5] A. Schönhage, Partial and Total Matrix Multiplication, SIAM J. Comput., 10 (3) (1981) 
434-455. 
[6] V. Pan, Strassen’s Algorithm is not Optimal. Trilinear Technique of Aggregating Uniting 
and Canceling for Constructing Fast Algorithms for Matrix Operations, Proceedings of the J. Respondek,                                                   Correction of  ‘J. Laderman, V. Pan, X. H. Sha, On practical Algorithms…' 
10/10 
 19th Annual Symposium on Foundations of Computer Science, Ann Arbor, Michigan, 
USA, 1987, pp. 166-176. 
[7] V. Strassen, Gaussian Elimination is not Optimal. Numer. Math. 13 (4) (1969) 354-356. 