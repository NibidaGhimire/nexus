1
Model-Assisted Learning for Adaptive Cooperative
Perception of Connected Autonomous Vehicles
Kaige Qu, Member, IEEE , Weihua Zhuang, Fellow, IEEE , Qiang Ye, Senior Member, IEEE ,
Wen Wu, Senior Member, IEEE , and Xuemin Shen, Fellow, IEEE
Abstract â€”Cooperative perception (CP) is a key technology
to facilitate consistent and accurate situational awareness for
connected and autonomous vehicles (CA Vs). To tackle the net-
work resource inefficiency issue in traditional broadcast-based
CP, unicast-based CP has been proposed to associate CA V pairs
for cooperative perception via vehicle-to-vehicle transmission. In
this paper, we investigate unicast-based CP among CA V pairs.
With the consideration of dynamic perception workloads and
channel conditions due to vehicle mobility and dynamic radio
resource availability, we propose an adaptive cooperative percep-
tion scheme for CA V pairs in a mixed-traffic autonomous driving
scenario with both CA Vs and human-driven vehicles. We aim to
determine when to switch between cooperative perception and
stand-alone perception for each CA V pair, and allocate commu-
nication and computing resources to cooperative CA V pairs for
maximizing the computing efficiency gain under perception task
delay requirements. A model-assisted multi-agent reinforcement
learning (MARL) solution is developed, which integrates MARL
for an adaptive CA V cooperation decision and an optimization
model for communication and computing resource allocation.
Simulation results demonstrate the effectiveness of the proposed
scheme in achieving high computing efficiency gain, as compared
with benchmark schemes.
Index Terms â€”Connected and autonomous vehicles (CA Vs),
cooperative perception, data fusion, autonomous driving, multi-
agent reinforcement learning, model-assisted learning.
I. I NTRODUCTION
The advances in sensing, artificial intelligence (AI), and
vehicles-to-everything (V2X) communication technologies
have paved the way for autonomous driving, leading to a
potential paradigm shift in future transportation systems to-
wards improved road safety and traffic efficiency [1]â€“[3].
Reliable and real-time environment perception is a key compo-
nent in autonomous driving that facilitates the connected and
autonomous vehicles (CA Vs) to accurately and continuously
perceive the surrounding objects, such as traffic participants,
by using on-board cameras, light detection and ranging (Li-
DAR) sensors, and radar sensors [4], [5]. To enhance the
perception reliability in terms of both coverage and accuracy,
cooperative perception (CP) has been proposed to enable the
This work was supported by the Natural Sciences and Engineering Research
Council (NSERC) of Canada.
Kaige Qu, Weihua Zhuang, and Xuemin (Sherman) Shen are with
the Department of Electrical and Computer Engineering, University of
Waterloo, Waterloo, ON N2L 3G1, Canada (emails: {k2qu, wzhuang,
sshen}@uwaterloo.ca).
Qiang Ye is with University of Calgary, Calgary, AB T2N 1N4, Canada
(email: qiang.ye@ucalgary.ca).
Wen Wu is with Peng Cheng Laboratory, Shenzhen, Guangdong, China,
518055 (email: wuw02@pcl.ac.cn). He contributed to this study while work-
ing as a postdoctoral fellow at the University of Waterloo, Canada.sensory information sharing among CA Vs by leveraging V2X
communication, as a supplement to the stand-alone perception
(SP) by individual CA Vs based on their own viewpoints [6]â€“
[12]. In case of unreliable network connectivity or network
congestion due to limited network resources, CA Vs can switch
back to the default SP mode [13].
According to the type of shared sensory information, there
are three CP levels, including raw level, feature level, and
decision level. In the raw-level CP, complete [7], [14] or partial
raw data [13], [15] are shared among CA Vs, which preserves
the most fine-grained environmental information and leads to
the highest perception performance gain at the cost of huge
communication overhead due to the large data volume. The
decision-level CP integrates lightweight perception results of
individual CA Vs, which is communication-efficient but with
limited perception performance gain [10]. The feature-level
CP, which has gained significant attention in the computer
vision field, can balance between communication overhead
and perception performance gain [16], [17]. Research studies
on feature-level CP have focused on the design of AI-based
fusion schemes, but the underlying communication scheme
is usually simple, e.g., via broadcast-based vehicle-to-vehicle
(V2V) communication. Specifically, each CA V fuses all the
received feature data broadcast from neighboring CA Vs with
its own data and processes the fused data for inference [16],
[17]. Although the feature data are compressed from the
raw data, the data size is still large, e.g., in the scale of
Mbits. Hence, the broadcast-based CP schemes are commu-
nication inefficient especially in dense-traffic scenarios and
even not applicable when the available transmission resources
are limited. Moreover, due to individual computation at each
CA V , the overall computation demand is intensive, which is
roughly proportional to the number of CA Vs and the data
volume processed at each CA V [13]. The communication
and computation in such broadcast-based CP schemes lead to
large network resource consumption for satisfying the stringent
delay requirement of real-time perception tasks.
Recently, there are some studies on the deployment of CP
schemes in a practical network environment by considering
the limited V2X communication bandwidth and on-board
computing resources [7]â€“[10]. As nearby CA Vs collect sensing
data of common objects in a shared environment from diverse
viewpoints, adding sensing data from more CA Vs for data
fusion potentially improves the perception performance with
a diminishing marginal gain, at the cost of almost linearly
increasing network resources. For resource efficiency, unicast-
based CP schemes have been studied in [7]â€“[9], which dealarXiv:2401.10156v1  [cs.NI]  18 Jan 20242
with the association of CA V pairs that perform the cooper-
ative perception via unicast-based V2V communication. Two
CA Vs with complementary or enhancing sensory information
usually provide higher perception performance gain through
cooperation and tend to be associated, due to more even
spatial distribution or higher intensity of fused sensing data.
In comparison with the broadcast-based counterparts, unicast-
based CP schemes significantly improve the network resource
efficiency without a remarkable compromise on the perception
performance through proper association of CA V pairs.
In this work, we investigate unicast-based feature-level CP
among CA Vs. Different from the existing works on CA V
pair association to improve the perception performance gain,
we investigate how to support CP for predetermined CA V
pairs in a complex and dynamic network environment with
high resource efficiency. Specifically, we consider a practical
mixed-traffic autonomous driving scenario where a cluster
of CA Vs and human-driven vehicles (HDVs) traverse a road
segment with intermittent road-side-unit (RSU) coverage due
to the high RSU deployment cost. Each CA V pair works in
either the SP mode by default or the unicast-based feature-
level CP mode by selection. Considering the radio resource
sharing among vehicles, the radio resource availability for
CA Vs dynamically changes over time. Due to vehicle mobility,
the perception workloads and channel conditions for CA Vs are
dynamic in different perception task periods. In such a network
scenario, it is challenging to constantly support all CA V pairs
to work in the CP mode with delay satisfaction.
To accommodate the network dynamics, we propose an
adaptive cooperative perception scheme, which facilitates a
dynamic selection of CA V pairs for cooperative perception.
The selected CA V pairs are referred to as cooperative CAV
pairs , and the non-selected CA V pairs work in the SP mode
by default. For each cooperative CA V pair, we dynamically
allocate a fraction of available radio resources to support the
feature data transmission, and adjust the CPU frequency at the
CA Vs on demand, to balance between the transmission and
computation delays under the network dynamics, while satis-
fying a perception delay requirement. For the joint adaptive
CA V cooperation and resource allocation, there is a trade-off
between a total computing efficiency gain and a total cost
for dynamically switching between the SP and CP modes
for the CA V pairs. Specifically, for a CA V pair, the total
computing demand is significantly reduced in the CP mode,
by performing the data fusion and inference at one CA V and
allowing the computation result sharing within the CA V pair.
However, due to the on-demand CPU frequency allocation, the
CPU frequency in the CP mode can be occasionally higher
than that in the SP mode. As the CA Vs work in the SP
mode by default, we characterize the computing efficiency gain
of a CA V pair as the reduced amount of computing energy
consumption in comparison with that in the SP mode, which
depends on both computing demand and CPU frequency. We
focus on increasing the total computing efficiency gain while
reducing the total switching cost, via proper selection of
cooperative CA V pairs and optimal resource allocation. The
main contributions of this paper are summarized as follows.
â€¢We propose an adaptive cooperative perception schemefor CA V pairs in a moving mixed-traffic vehicle cluster,
which allows each CA V pair to dynamically switch
between the SP and CP modes over different perception
task periods, to adapt to the network dynamics;
â€¢We formulate a joint adaptive CA V cooperation and re-
source allocation problem, which can be decoupled to an
adaptive CA V cooperation subproblem in the long run and
a series of instantaneous resource allocation subproblems
in each perception task period, to maximize the total
computing efficiency gain with minimum switching cost,
while satisfying the perception delay requirement;
â€¢We propose a model-assisted multi-agent reinforcement
learning (MARL) solution, where MARL is used to learn
the adaptive cooperation decisions among CA V pairs, and
a model-based solution is used for resource allocation
given each cooperation decision.
The remainder of this paper is organized as follows. The
system model is presented in Section II, with a performance
analysis included in Section III. The joint adaptive CA V
cooperation and resource allocation problem is formulated in
Section IV, with a model-assisted MARL solution presented
in Section V. Simulation results are provided in Section VI,
and conclusions are drawn in Section VII.
II. S YSTEM MODEL
A. Mixed-Traffic Autonomous Driving Scenario
We consider a vehicle cluster including MHDVs and K
CA V pairs in set K, moving on a multi-lane unidirectional
road under a consistent base station (BS) coverage and an
intermittent RSU coverage. The CA V pairs are predetermined
by using existing CA V pair association algorithms [8], [9]. a
cluster head is selected among the vehicles based on existing
vehicle clustering algorithms, to coordinate the communica-
tion, computing, and sensing in the vehicle cluster [4]. Both
BS and RSU provide the edge computing capability, facilitated
by co-located edge servers. Fig. 1 illustrates three snapshots
for a vehicle cluster moving through an RSUâ€™s coverage area.
Initially, all the vehicles in the cluster have no access to the
RSU but the leading vehicle is about to move into the RSU
coverage. Then, the vehicles gradually move into and later
leave the RSU coverage. Consider a time-slotted system, in
which the time slots are indexed by integer n.
Each CA V initiates a perception task in each time slot to
identify the surrounding objects, whose results are essential
to supporting autonomous driving applications, such as path
planning and maneuver control. Each CA V pair kâˆˆ K consists
of one transmitter CA V and one receiver CA V , and both CA Vs
share a similar view but from different angles. Each CA V pair
works in the SP mode by default and in the CP mode by
selection. Let x(n) ={xk(n),âˆ€kâˆˆ K} be a binary perception
mode selection decision vector for all CA V pairs (also referred
to as cooperation decisions for brevity) at time slot n, with
xk(n) = 1 indicating the CP mode and xk(n) = 0 indicating
the SP mode for CA V pair k. A CA V pair in the CP mode is
referred to as a cooperative CA V pair. Let KC(n)denote a set
of cooperative CA V pairs at time slot n, with KC(n)âŠ‚ K.3
time Moving 
direction 
V2R 
link V2V 
link 
CAV pairs 
 HDVs RSU Edge 
servers Considered Cluster Considered Cluster 
Considered Cluster 
V2B 
link 
BS Snapshot 1
Snapshot 2
Snapshot 3
Fig. 1: A mixed-traffic autonomous driving scenario.
Each HDV occasionally requests an infotainment service,
e.g., mobile virtual reality, which is throughput sensitive and
computation intensive. For energy and delay efficiency, the
computation tasks at an HDV can be offloaded to a more
powerful edge server either at a BS or at an RSU, and the
data transmission is supported by either vehicle-to-BS (V2B)
or vehicle-to-RSU (V2R) communications [18].
B. Perception Task Model
For environment perception, lightweight data pre-processing
algorithms are used to slice the raw sensing data into ob-
ject partitions each containing one object of interest and
background partitions that contain only background infor-
mation [19]. An object tracking algorithm associates the
object partitions with existing objects in a maintained object
tracking list, by comparing the identified and predicted object
locations [20]. Only the new objects and the objects with
reduced tracking accuracy are further processed by a deep
neural network (DNN) for classification [21]. For CA V pair k,
letWk(n),WT
k(n), and WR
k(n)denote the number of objects
that require DNN model processing in the overlapping sensing
No Feature 
extraction Fast 
inference  ?Yes Input 
TI I pEarly exit output Each CAV 
Full 
inference Main 
output Fig. 2: Object classification by using a default DNN model.
Feature 
extraction Feature 
fusion Input 
Transmitter CAV Feature data 
transmission Receiver CAV 
Feature 
extraction Input No Fast 
inference  ?Yes 
TI I pEarly exit output 
Full 
inference Main 
output 
Fig. 3: Object classification by using a feature-fusion DNN model.
range of both CA Vs, in the non-overlapping sensing range of
the transmitter CA V , and in the non-overlapping sensing range
of the receiver CA V , respectively, at time slot n. We refer to the
objects in the overlapping and non-overlapping sensing ranges
as shared and individual objects respectively. Then, Wk(n)is
also referred to as shared workload, and WT
k(n),WR
k(n)are
referred to as individual workloads, for CA V pair k.
At time slot n, the object classification tasks for all the
objects of CA V pair kshould be completed within a time
duration of âˆ†, which is typically smaller than the time slot
length, with the consideration of 1) the raw data pre-processing
and object tracking procedures before the generation of object
classification tasks, and 2) the response time for autonomous
driving applications based on the object classification results.
1) DNN models: To support the object classification at
each CA V pair, we consider a default DNN model deployed
at both the transmitter and receiver CA Vs and a feature-
fusion DNN model partitioned between them, both employing
an early-exit DNN architecture, as illustrated in Fig. 2 and
Fig. 3 respectively [22]â€“[25]. As the objects can be processed
independently, we consider that both DNN models operate
on a per-object basis [19], [26]. Specifically, in the default
DNN model, a feature extraction module, mainly composed of
convolution ( CONV ) layers, first generates compressed feature
data based on an input of object sensing data. Then, the
feature data are further processed by a fast inference module,
composed of both CONV and fully-connected ( FC) layers, to
generate a DNN inference result, referred to as a fast inference
result. Letting Zdenote the total number of object classes,
a DNN inference result is a Z-dimension estimated class4
probability vector, where the classification performance is
measured by confidence level defined as one minus normalized
entropy [22], [23]. A higher confidence level indicates a less
uncertain estimation and implies a higher accuracy [23]. Let
Î·denote the confidence level of a fast inference result. If Î·
reaches a predetermined threshold, Î·T, an object classification
result is obtained at an early exit output. Otherwise, a full
inference module, composed of deeper CONV andFClayers,
is triggered to re-process the feature data and generate another
DNN inference result, referred to as a full inference result, at
a main output. Let Ïâˆˆ(0,1)be the early exit probability for
the default DNN model, representing the probability that an
object classification result is obtained at the early exit output.
For a shared object of a CA V pair, the transmitter and
receiver CA Vs have object sensing data from different view-
points, implying a potential confidence level gain from data
fusion. Such an object can be processed by using the feature-
fusion DNN model. Specifically, both CA Vs process their own
object sensing data and extract features based on a feature
extraction module. Then, the feature data of the transmitter
CA V are transmitted via V2V communication to the receiver
CA V , where the feature data from both CA Vs are fused and
processed by the fast inference and selective full inference
modules. The object classification result is obtained at either
an early exit output with probability ËœÏâˆˆ(0,1), or a main
output with probability 1âˆ’ËœÏ, at the receiver CA V , which is then
sent back to the transmitter CA V . Typically, we have ËœÏ > Ï ,
as more fast inference results can satisfy the confidence level
requirement due to the confidence level gain from data fusion.
For the DNN models, let Î´1,Î´2,Î´3andÎ´4be the computing
demand (in CPU cycles) for feature extraction, feature fusion,
fast inference, and full inference. Typically, we have Î´2â‰ª
min{Î´3, Î´4}, as the feature fusion module can be implemented
by simple operations such as concatenation, maxout, and aver-
age operations [16], [22] or lightweight attention schemes [17].
The average computing demand for processing one object by
the default and feature-fusion DNN models, denoted by Î´and
ËœÎ´respectively, are given by
Î´=Î´1+Î´3+ (1âˆ’Ï)Î´4 (1)
ËœÎ´= 2Î´1+Î´2+Î´3+ (1âˆ’ËœÏ)Î´4. (2)
2) Cooperative Perception Mode: For a CA V pair in the
CP mode, the shared objects are collaboratively processed
by using the feature-fusion DNN model, while the individual
objects are independently processed at each CA V by using the
default DNN model. Consider two CPU cores at each CA V ,
for processing the shared and individual objects separately
using different DNN models. For each CPU core, the dynamic
voltage and frequency scaling (DVFS) technique is used
to allow on-demand CPU frequency scaling, to support the
dynamic perception workloads [7].
3) Stand-Alone Perception Mode: In the default SP mode,
both CA Vs in a CA V pair independently perform the object
classification tasks. Both the shared and individual objects
are processed by using the default DNN model. For ease of
analysis, we assume that the shared and individual objects are
processed at separate CPU cores, as in the CP mode.TABLE I: CPU F REQUENCY CONFIGURATION FOR A CAV P AIR
CPU Core Core 1 Core 2
Object Type Shared Objects Individual Objects
Perception Mode CP SP CP SP
CPU Frequencyfk(n)fD
k(n)fT
k(n) fT
k(n)at Transmitter CAV
CPU Frequencyfk(n)fD
k(n)fR
k(n) fR
k(n)at Receiver CAV
C. Computing Model
For CA V pair k, letÎ´C
k(n),Î´S
k(n),Î´T
k(n), and Î´R
k(n)be
the average total computing demand for processing the shared
objects in the CP mode, the shared objects in the SP mode, the
transmitter CA Vâ€™s individual objects, and the receiver CA Vâ€™s
individual objects, at time slot n, given by
Î´C
k(n) =ËœÎ´Wk(n), Î´S
k(n) = 2 Î´Wk(n) (3)
Î´T
k(n) =Î´WT
k(n), Î´R
k(n) =Î´WR
k(n). (4)
There is a positive total computing demand reduction for CA V
pairkthrough cooperation which increases proportionally
to shared workload Wk(n), given by (2Î´âˆ’ËœÎ´)Wk(n) =
[Î´3+ (1 + Ëœ Ïâˆ’2Ï)Î´4âˆ’Î´2]Wk(n)>0.
For CA V pair k, letfk(n),fT
k(n), and fR
k(n)denote the
CPU frequencies (in Hz or cycle/s) for processing the shared
objects at both CA Vs, the individual objects at transmitter
CA V , and the individual objects at receiver CA V , respectively,
at time slot n. We have fk(n) =fD
k(n)ifxn(k) = 0 , where
fD
k(n)is the CPU frequency for processing the shared objects
at both CA Vs in the default SP mode. Table I summarizes
the relationships among CPU cores, object types, perception
modes, and CPU frequencies for a CA V pair. As the shared and
individual objects can be processed in parallel at the separate
CPU cores, we have fD
k(n) =Î´Wk(n)
âˆ†,fT
k(n) =Î´WT
k(n)
âˆ†,
andfR
k(n) =Î´WR
k(n)
âˆ†to ensure that the default DNN model
processing can be finished within delay bound âˆ†for the shared
objects in the SP mode, and for the individual objects in either
SP or CP mode, without CPU frequency over-provisioning.
All CPU frequencies are upper limited by a maximum CPU
frequency, fM, supported by DVFS, leading to an upper limit,
WM=fMâˆ†
Î´, forWk(n),WT
k(n), and WR
k(n).
The total computing energy consumption by all CPU cores
of CA V pair kat time slot n, denoted by ek(n), is given by
ek(n) =Îº
fk(n)2Î´C
k(n)xk(n) +fD
k(n)2Î´S
k(n) (1âˆ’xk(n))
+fT
k(n)2Î´T
k(n) +fR
k(n)2Î´R
k(n)
(5)
where Îºis the energy efficiency coefficient of a CPU core [23].
From (5), we see that only the portion of computing energy for
processing the shared objects depends on cooperation decision
xk(n). The computing energy is a comprehensive metric that
integrates both CPU frequency and computing demand. We
characterize the computing efficiency gain of CA V pair kat
time slot n, denoted by Gk(n), as the reduced amount of
computing energy in comparison with that in the default SP
mode. We have Gk(n)â‰¡0fork /âˆˆ K C(n). For cooperative5
CA V pair kâˆˆ KC(n), we have
Gk(n) =ÎºfD
k(n)2Î´S
k(n)âˆ’Îºfk(n)2Î´C
k(n)
= 2ÎºÎ´fD
k(n)2Wk(n)âˆ’ÎºËœÎ´fk(n)2Wk(n),âˆ€kâˆˆ KC(n)(6)
as a decreasing function of fk(n)>0. Here, Gk(n)is inde-
pendent of the individual workloads, as only shared objects
are processed differently between the SP and CP modes.
D. Communication Model
Consider a radio resource pool with total bandwidth B
for V2X sidelink communication, which is shared between
the V2R transmission from HDVs and the V2V transmission
for cooperative CA V pairs, and is non-overlapping with that
for V2B communication. The radio resource sharing between
CA Vs and HDVs occur only in the RSU coverage. Consider a
transmission priority for HDVs, as the CA Vs can work in the
SP mode without radio resource usage by default. Orthogonal
frequency division multiplexing (OFDM) based transmission
schemes are employed for the V2X sidelink communication.
LetB(n)be the time-varying available radio spectrum
bandwidth for CA Vs, with the consideration of dynamic
background radio resource usage by HDVs. Let Î²(n) =
{Î²k(n),âˆ€kâˆˆ K} be a radio resource allocation decision vector
for all CA V pairs at time slot n, with Î²k(n)denoting the
fraction of available radio resources allocated to CA V pair k
for supporting the feature data transmission at time slot n. The
average transmission rate for CA V pair kat time slot nis
Rk(n) =Î²k(n)B(n) log2
1 +pkgk(n)
Ïƒ2
,âˆ€kâˆˆ K (7)
where pkis the transmit power of CA V pair k,gk(n)is the
channel power gain between both CA Vs in CA V pair kat
time slot n, and Ïƒ2represents the received noise power. Due to
high vehicle mobility, we consider only the large-scale channel
conditions, specifically the path loss, for the CA V pairs.
E. Delay Model
Under the assumption of max{Wk(n), WT
k(n), WR
k(n)} â‰¤
WM, the CPU frequencies for processing the shared objects in
the SP mode and for processing the individual objects in both
SP and CP modes can be feasibly scaled up/down to ensure
delay satisfaction. Here, we focus on the delay performance of
the shared objects in the CP mode. Let wdenote the feature
data size in the unit of bit. If CA V pair kworks in the CP
mode, the average object classification delay for each shared
object, denoted by dk(n), depends on the feature-fusion DNN
model. The delay is composed of feature extraction delay
Î´1
fk(n), feature data transmission delayw
Rk(n), feature fusion
delayÎ´2
fk(n), and the average inference delay,Î´3+(1âˆ’ËœÏ)Î´4
fk(n). The
delay for sending the classification results is negligible due to
the small data size. For delay satisfaction, dk(n)should not
exceed a per-object delay budget,âˆ†
Wk(n), given by
dk(n) =w
Rk(n)+Ë†Î´
fk(n)â‰¤âˆ†
Wk(n),âˆ€kâˆˆ KC(n)(8)
where Ë†Î´=Î´1+Î´2+Î´3+ (1âˆ’ËœÏ)Î´4is a constant.F . Generalization
For computing efficiency gain and perception accuracy
enhancement, nearby CA Vs can be grouped for cooperative
perception by using a Y-input feature-fusion DNN model,
where Yis a general group size. At a given vehicle density,
the content similarity within a group tends to reduce as Y
increases, due to a lower average percentage of shared objects.
As only the shared objects can be collaboratively processed
by using the feature-fusion DNN model, the reduced content
similarity may gradually compromise the total computing effi-
ciency gain and the average perception accuracy enhancement
asYincreases. Additionally, a larger group size increases the
accuracy of shared objects with a diminishing marginal gain.
However, there is a higher overall communication cost for
supporting the feature data transmission of more transmitter
CA Vs in a larger group. For simplicity, we consider Y= 2
and pair the CA Vs based on existing works [7]â€“[9]. How to
select the best group size, Y, and how to optimally group the
CA Vs given Yremain to be investigated in our future work.
III. 2D P ERFORMANCE REGION ANALYSIS
For problem formulation, we analyze the performance of
an arbitrary cooperative CA V pair, kâˆˆ K C(n), at time slot
n, under different transmission rates and CPU frequencies for
supporting the classification of shared objects. The condition
for non-negative computing efficiency gain via cooperation,
i.e.,Gk(n)â‰¥0, is a CPU frequency requirement, given by
fk(n)â‰¤fP
k(n) =r
2Î´
ËœÎ´fD
k(n) =s
2Î´3
ËœÎ´Wk(n)
âˆ†(9)
where fP
k(n)corresponds to zero computing efficiency gain
and increases proportionally to shared workload Wk(n). We
have fP
k(n)> fD
k(n), asq
2Î´
ËœÎ´>1. Under assumption
Wk(n)â‰¤WM, there are two cases for the relationship among
fD
k(n),fP
k(n)andfM, depending on shared workload Wk(n):
â€¢Low shared workload: ForWk(n)â‰¤q
ËœÎ´
2Î´fMâˆ†
Î´=q
ËœÎ´
2Î´WM, we have fD
k(n)< fP
k(n)â‰¤fM. For a feasible
CPU frequency scale-up from fD
k(n)tofM, computing
efficiency gain Gk(n)transits from positive to negative,
with a zero value at fk(n) =fP
k(n);
â€¢High shared workload: Forq
ËœÎ´
2Î´WM< W k(n)â‰¤WM,
we have fD
k(n)â‰¤fM< fP
k(n). Asfk(n)scales up from
fD
k(n)tofM,Gk(n)decreases but remains positive.
LetRM,RP
k(n), and RD
k(n)be the minimum transmission
rates for delay satisfaction if cooperative CA V pair koper-
ates at CPU frequencies fM,fP
k(n), and fD
k(n)respectively
for processing the shared objects. The three rate-frequency
pairs, [RM, fM],
RP
k(n), fP
k(n)
, and
RD
k(n), fD
k(n)
, all lie
on a curve indicating dk(n) =âˆ†
Wk(n). We obtain RM=
w.
âˆ†
Wk(n)âˆ’Ë†Î´
fM
,RP
k(n) =Wk(n)wÏ•
(Ï•âˆ’1)âˆ†where Ï•=q
2Î´3
Ë†Î´2ËœÎ´>1
is a constant, and RD
k(n) =Wk(n)wÎ´
[(ËœÏâˆ’Ï)Î´4âˆ’Î´2]âˆ†, all increasing
with shared workload Wk(n). For cooperative CA V pair k,
there are multiple 2D performance regions with different delay
performance, CPU frequency scaling range, and computing6
TABLE II: S UMMARY OF DIFFERENT PERFORMANCE REGIONS OF A COOPERATIVE CAV P AIR
Region Shared Workload Delay Performance CPU Frequency Scaling Range Computing Efficiency Gain
R1 Low/High Violation N/A N/A
R2 Low/High Satisfaction Infeasible scale-up: fk(n)> fM N/A
R3 Low Satisfaction Feasible scale-up: fP
k(n)< fk(n)â‰¤fM Negative
R4 Low/High SatisfactionFeasible scale-up:
Non-negative fD
k(n)< fk(n)â‰¤fP
k(n)at a low workload,
fD
k(n)< fk(n)â‰¤fMat a high workload
R5 Low/High Satisfaction Default or scale-down: fk(n)â‰¤fD
k(n) Positive
20 25 30 35 40 45 50 
Transmission rate (Mbps)246810 CPU frequency (GHz) 
(a)Low workload
40 50 60 70 80 
Transmission rate (Mbps)45678910 CPU frequency (GHz) (b)High workload
Fig. 4: Examples of 2D performance regions for cooperative CA V pair
kat a different shared workload, where [RM, fM],
RP
k(n), fP
k(n)
, and
RD
k(n), fD
k(n)
are indicated by blue, pink, and green circles.
efficiency gain, for different combinations of Rk(n)andfk(n),
as summarized in Table II. The number of performance regions
depends on the shared workload, as illustrated in Fig. 4.
From Fig. 4 and Table II, we obtain some useful principles
in the resource allocation for each cooperative CA V pair. First,
only the rate-frequency pairs in Regions R4andR5should
be selected for both delay satisfaction and non-negative com-
puting efficiency gain at a feasible CPU frequency. Second,
a subset of Region R4andRegion R5rate-frequency pairs
that lie on curve dk(n) =âˆ†
Wk(n)are the ideal candidate rate-
frequency pairs without transmission rate over-provisioning,
as indicated by red curves in Fig. 4. Accordingly, the ideal
candidate rate-frequency pairs for each cooperative CA V pair
kâˆˆ KC(n)should satisfy the following constraints,
fk(n)â‰¤min
fP
k(n), fM	
,âˆ€kâˆˆ KC(n) (10)
w
Rk(n)+Ë†Î´
fk(n)=âˆ†
Wk(n),âˆ€kâˆˆ KC(n). (11)
IV. P ROBLEM FORMULATION
Due to the dynamic radio resource availability, the current
available radio resources may be insufficient for supporting
all CA V pairs to work in the CP mode with delay satis-
faction and non-negative computing efficiency gain. Thus,
an adaptive set of cooperative CA V pairs, KC(n)âŠ‚ K
should be determined for each time slot. Due to environmental
changes, the shared workload and the transmitter-receiver
distance vary over time for each cooperative CA V pair. As
the total computing demand reduction through cooperationincreases in proportion to the shared workload, a moderate
shared workload increase potentially brings more computing
efficiency gain. However, due to a reduced per-object delay
budget, a heavier shared workload requires a higher CPU
frequency for delay satisfaction under limited radio resources,
compromising the computing efficiency gain. For a longer
transmitter-receiver distance, the average transmission rate
decreases due to a higher path loss, leading to a higher CPU
frequency requirement for delay satisfaction, which reduces
the computing efficiency gain. Hence, the dynamics in both
shared workloads and transmitter-receiver distances should be
considered in the adaptive selection of cooperative CA V pairs,
to maximize the total computing efficiency gain.
Moreover, the cooperation decisions for each CA V pair
should not change too frequently, as the switching between
the SP and CP modes incur a CPU process switching over-
head between scheduling the default and feature-fusion DNN
models [27]. Let C(n)be the total number of CA V pairs that
change the cooperation status at time slot n, given by
C(n) =X
kâˆˆK|xk(n)âˆ’xk(nâˆ’1)|. (12)
The total switching cost increases proportionally to C(n). To
maximize the total computing efficiency gain while minimiz-
ing the total switching cost in the long run, we study a joint
adaptive CA V cooperation and resource allocation problem,
to adaptively switch between the SP and CP modes and
allocate resources among all CA V pairs for each time slot.
Letf(n) ={fk(n),âˆ€kâˆˆ K} be a CPU frequency allocation
decision vector for processing shared objects at all CA V pairs
at time slot n. Let Ë™x={x(n),âˆ€n},Ë™Î²={Î²(n),âˆ€n},
Ë™f={f(n),âˆ€n}denote the CA V cooperation, radio resource
allocation, and CPU frequency allocation decisions for all time
slots. Then, the joint problem is formulated as
P0: max
Ë™x,Ë™Î²,Ë™fX
n" X
kâˆˆKGk(n)!
âˆ’ËœÏ‰C(n)#
(13)
s.t. (7) ,(10),(11)X
kâˆˆKÎ²k(n)â‰¤1 (14)
0â‰¤Î²k(n)â‰¤xk(n), kâˆˆ K (15)
(1âˆ’xk(n))fD
k(n)â‰¤fk(n)
â‰¤(1âˆ’xk(n))fD
k(n) +xk(n)M, kâˆˆ K (16)7
where ËœÏ‰is a positive weight that controls the trade-off between
gain and cost, and Mis a very large constant. Among the con-
straints, (7) is the expression of transmission rate Rk(n), (10)
and (11) are conditions for the ideal candidate rate-frequency
pairs in the 2D performance regions. Constraints (14) and (15)
ensure that the total fraction of allocated bandwidth for all
CA V pairs does not exceed one, while guaranteeing that no
radio resources are allocated to CA V pairs in the SP mode.
With constraint (16), we have fk(n) =fD
k(n)in the SP mode
and0â‰¤fk(n)â‰¤Min the CP mode for CA V pair k.
Letâˆ—associate the optimal solution to problem P0. Given
Ë™xâˆ—, the resource allocation decisions can be decoupled among
time slots in the objective function and all the constraints.
Hence, given Ë™xâˆ—,(Î²âˆ—(n),fâˆ—(n))must be the resource alloca-
tion solution that maximizes the instantaneous total computing
efficiency gain,P
kâˆˆKGk(n), for time slot n, since the total
switching cost depends only on Ë™x. Therefore, problem P0
can be decoupled to a long-term optimization subproblem for
the adaptive CA V cooperation and a series of instantaneous
optimization subproblems for resource allocation, as follows.
A. Resource Allocation Subproblem
For time slot n, given any CA V cooperation decision x(n), a
cooperative CA V pair set, KC(n), is determined. For CA V pair
kin the SP mode, we have Î²k(n) = 0 ,fk(n) =fD
k(n)based
on (15) and (16), and Gk(n) = 0 . Accordingly, a resource
allocation subproblem for time slot nis formulated as
P1: max
Î²C(n),fC(n)X
kâˆˆKC(n)Gk(n) (17)
s.t. (7) ,(10),(11)X
kâˆˆKC(n)Î²k(n)â‰¤1 (18)
where Î²C(n) = {Î²k(n),âˆ€kâˆˆ K C(n)}andfC(n) =
{fk(n),âˆ€kâˆˆ KC(n)}are the resource allocation decisions for
cooperative CA V pairs in KC(n). If problem P1is feasible,
we have Gâˆ—(n) =P
kâˆˆKC(n)Gâˆ—
k(n)as the maximal total
computing efficiency gain achieved with optimal resource
allocation; otherwise, Gâˆ—(n)is undefined.
B. Adaptive CAV Cooperation Subproblem
We formulate an adaptive CA V cooperation subproblem as
a multi-agent Markov decision process (MMDP), where each
agent corresponds to a CA V pair that makes binary cooperation
decisions based on local observations over time. We consider
cooperative agents in the MMDP, where all CA V pairs collab-
oratively maximize an expected total discounted reward. An
MMDP is represented as (K,S,{Ak}, P, R, {â„¦k}, Î³), where
Kis a set of agents, Sis the state space, Akis the action space
for agent k, withA=Ã—kAkbeing the set of joint actions, Pis
an unknown state transition probability matrix, R:SÃ—A 7â†’ R
is a reward function, â„¦kis a set of observations for agent k,
andÎ³is a discount factor in [0,1). The observation, action,
and reward of agent kare given as follows.
â€¢Observation : The local observation for agent kat time
slotn, denoted by o(n)
k, includes the available radiospectrum bandwidth for CA Vs, B(n), shared workload
Wk(n), transmitter-receiver distance Dk(n), and previous
cooperation status xk(nâˆ’1), given by
o(n)
k={B(n), Wk(n), Dk(n), xk(nâˆ’1)}; (19)
â€¢Action : At time slot n, agent kâ€™s action is the cooper-
ation decision, xk(n). Joint action x(n) ={xk(n),âˆ€k}
determines a cooperative CA V pair set, KC(n);
â€¢Reward : The reward for any agent at time slot n, denoted
byr(n), is given by
r(n)=
Gâˆ—(n)âˆ’ËœÏ‰C(n), ifP1is feasible
P, otherwise(20)
where Gâˆ—(n)is the maximal total computing efficiency
gain associated with optimal resource allocation for
KC(n). If problem P1is infeasible under the selected
joint action, a negative penalty, P, is used as the reward.
As both the objective function and the delay constraint
in problem P1are derived based on the early exit proba-
bilities of DNN models, we aim to learn the statistically
optimal adaptive CA V cooperation actions in the long run
by using the reward function in (20).
V. M ODEL -ASSISTED MULTI -AGENT REINFORCEMENT
LEARNING SOLUTION
We propose a model-assisted learning solution to the joint
problem. First, a model-based optimal solution is derived for
the resource allocation subproblem. Then, a model-assisted
MARL approach that relies on the model-based optimal re-
source allocation solution for reward calculation is proposed,
to solve the MMDP for adaptive CA V cooperation.
A. Optimal Resource Allocation Solution
With (6), maximizingP
kâˆˆKC(n)Gk(n)inP1is equivalent
to minimizingP
kâˆˆKC(n)Wk(n)fk(n)2. By writing Î²k(n)as
a function of Rk(n), we combine constraints (7) and (18) as
X
kâˆˆKC(n)Rk(n)
B(n) log2(1 +pkgk(n)/Ïƒ2)â‰¤1. (21)
From constraint (11), Rk(n) = w
âˆ†
Wk(n)âˆ’Ë†Î´
fk(n)
is a
function of fk(n). Substituting Rk(n)in (21), we transform
(21) into a constraint on decision variables fC(n), given by
h(f) =X
kâˆˆKC(n)ck
bkâˆ’Ë†Î´/fk(n)âˆ’1â‰¤0 (22)
where bk=âˆ†
Wk(n)andck=w
B(n) log2(1+pkgk(n)/Ïƒ2)are
known parameters given network status for time slot n. Here,
h(f)is a monotonically decreasing constraint function of
fC(n), defined in domain {fk(n)>Ë†Î´
bk,âˆ€kâˆˆ K C(n)}
to ensure Rk(n)>0forkâˆˆ K C(n). Let f0
k(n) =
min
fP
k(n), fM	
, which is known to CA V pair k, and let
f0
C(n) =
f0
k(n),âˆ€kâˆˆ KC(n)	
. Then, problem P1is trans-
formed to a CPU frequency allocation problem, given by
P2: min
fC(n)X
kâˆˆKC(n)Wk(n)fk(n)2(23)8
s.t. fC(n)âª¯f0
C(n) (24)
h(f)â‰¤0. (25)
Theorem 1. Problem P2is convex, and strong duality holds
if the problem is feasible under condition h(f0)<0.
The proof of Theorem 1 is given in Appendix. Based
on Theorem 1, Karush-Kuhn-Tucker (KKT) conditions are
necessary and sufficient conditions for optimal solution to
problem P2[28]. The Lagrangian of P2is
L(f,Î», Î½) =X
kâˆˆKC(n)Wk(n)fk(n)2
+X
kâˆˆKC(n)Î»k 
fk(n)âˆ’f0
k(n)
+Î½ h(f)(26)
where Î»={Î»k,âˆ€kâˆˆ K C(n)}andÎ½are dual variables. Its
gradient with respect to primal variable fk(n)is given by
âˆ‚L(f,Î», Î½)
âˆ‚fk(n)=2Wk(n)fk(n)+Î»kâˆ’Î½ckË†Î´

bkfk(n)âˆ’Ë†Î´2.(27)
Letfâˆ—be a primal optimal point and (Î»âˆ—, Î½âˆ—)be a dual
optimal point. Then, the KKT conditions for the optimal
solution to problem P2are given by
fâˆ—
kâ‰¤f0
k,âˆ€kâˆˆ KC (28a)
h(fâˆ—)â‰¤0 (28b)
Î»âˆ—
kâ‰¥0,âˆ€kâˆˆ KC (28c)
Î½âˆ—â‰¥0 (28d)
Î»âˆ—
k 
fâˆ—
kâˆ’f0
k
= 0,âˆ€kâˆˆ KC (28e)
2Wkfâˆ—
k+Î»âˆ—
kâˆ’Î½âˆ— ckË†Î´

bkfâˆ—
kâˆ’Ë†Î´2= 0,âˆ€kâˆˆ KC (28f)
where time slot index nis omitted for brevity. Among all
conditions, (28a) and (28b) are primal feasible conditions,
(28c) and (28d) are dual feasible conditions, (28e) indicates
the complementary slackness, and (28f) ensures that the La-
grangian gradient in (27) vanishes at fâˆ—asfâˆ—minimizes
L(f,Î»âˆ—, Î½âˆ—)[28]. From (28e) and (28f), we obtain
ï£«
ï£¬ï£­Î½âˆ—ckË†Î´

bkfâˆ—
kâˆ’Ë†Î´2âˆ’2Wkfâˆ—
kï£¶
ï£·ï£¸ 
fâˆ—
kâˆ’f0
k
= 0,âˆ€kâˆˆ KC.(29)
For dual variable Î½, letS(fk, Î½) =Î½ckË†Î´
(bkfkâˆ’Ë†Î´)2âˆ’2Wkfk,
which is a monotonically decreasing function in domain
fk>Ë†Î´
bk. Letf1
k(Î½)be the root of S(fk, Î½), which corresponds
to the intersection point of functions Î½ckË†Î´
(bkfkâˆ’Ë†Î´)2and2Wkfk
in domain fk>Ë†Î´
bk. Fig. 5 illustrates the root of function
S(fk, Î½)forÎ½1< Î½2. We see that, for a smaller value of dual
variable Î½, the root of S(fk, Î½), i.e., f1
k(Î½), has a smaller
value. According to (28f), we have Î»âˆ—
k=S(fâˆ—
k, Î½âˆ—), and
condition (29) is rewritten as
S(fâˆ—
k, Î½âˆ—) 
fâˆ—
kâˆ’f0
k
= 0,âˆ€kâˆˆ KC. (30)
Then, the primal optimal point, fâˆ—, that minimizes the objec-
0 1 2 3 4 5 6 7 8 9 100100200300400Fig. 5: An illustration of the root of function S(fk, Î½)forÎ½1< Î½ 2.
tive function in (23) is given by
fâˆ—
k=
f1
k(Î½âˆ—), if f1
k(Î½âˆ—)â‰¤f0
k
f0
k, if f0
k< f1
k(Î½âˆ—)(31)
for any kâˆˆ KC, to guarantee condition (30) and ensure that
Î»âˆ—
k=S(fâˆ—
k, Î½âˆ—)â‰¥0in (28c). Specifically, if f1
k(Î½âˆ—)â‰¤f0
k,
we have Î»âˆ—
k= 0; otherwise, we have Î»âˆ—
k>0.
AsÎ½âˆ—is an unknown optimal dual variable in (31), we will
find the primal optimal point, fâˆ—, by exploring the possible
values of Î½âˆ—. Let Ë†Î½âˆ—be a candidate value of Î½âˆ—, and let Ë†fâˆ—
be the corresponding candidate value of fâˆ—forÎ½âˆ—= Ë†Î½âˆ—
based on (31). For a smaller Ë†Î½âˆ—value, Ë†f1
k(Ë†Î½âˆ—)is smaller,
then Ë†fâˆ—
kis potentially smaller according to (31), leading to a
potentially smaller objective value in (23). However, as h(Ë†fâˆ—)
is a decreasing function of Ë†fâˆ—
k, the primal feasible condition
in (28b) might be violated if the value of Ë†Î½âˆ—is too small.
Therefore, we should find a minimum non-negative value for
Ë†Î½âˆ—that satisfies (28b) and (28d), to obtain Î½âˆ—andfâˆ—.
To solve problem P2, the feasibility is first checked by
calculating h(f0). Ifh(f0)>0, the problem is infeasible;
ifh(f0) = 0 , we can directly obtain the optimal solution as
fâˆ—=f0; ifh(f0)<0, we continue to use a binary-search
method to iteratively find Î½âˆ—in a gradually reduced interval
[Î½L, Î½R]. The detailed algorithm is described as follows.
For initialization, as fâˆ—
kâˆˆË†Î´
bk, f0
ki
,Î½LandÎ½Rare set to sat-
isfySË†Î´
maxkâˆˆKCbk+Ïµ, Î½L
= 0 andS 
minkâˆˆKCf0
k, Î½R
=
0. Here, Ïµis a very small number satisfying 0< Ïµâ‰ª1. The
shared workloads among all cooperative CA V pairs determine
the initial binary-search interval, [Î½L, Î½R]. In each iteration,
Ë†Î½âˆ—is set asÎ½L+Î½R
2, and Ë†fâˆ—is obtained based on (31). For
f=Ë†fâˆ—, let Ë†Gâˆ—=P
kâˆˆKCË†Gâˆ—
kdenote the corresponding total
computing efficiency gain in (17). Constraint (28b) is checked
by calculating h(Ë†fâˆ—). Whether or not the binary search ends
at the current iteration and how to update interval [Î½L, Î½R]for
the next iteration depend on h(Ë†fâˆ—):
â€¢Ifh(Ë†fâˆ—) = 0 , the optimal points, Î½âˆ—andfâˆ—, are obtained
asË†Î½âˆ—and Ë†fâˆ—, and the algorithm is ideally finished. In
practice, we set a stopping criteria, âˆ’10âˆ’4< h(Ë†fâˆ—)<0,
and obtain asymptotically optimal primal and dual vari-
ables when the binary search ends;
â€¢Ifh(Ë†fâˆ—)>0, constraint (28b) is infeasible, and the
candidate value for Î½âˆ—should be increased in the next
iteration, thus we set Î½L= Ë†Î½âˆ—;9
â€¢Ifh(Ë†fâˆ—)<0, the candidate value for Î½âˆ—can be further
reduced in the next iteration to increase h(Ë†fâˆ—)and reduce
the objective value in (23), thus we set Î½R= Ë†Î½âˆ—.
Such a centralized iterative algorithm can be executed at
the cluster head which is responsible for collecting the overall
network dynamics in the vehicle cluster.
B. Model-Assisted Multi-Agent Reinforcement Learning
We use a model-assisted MARL algorithm to solve the
MMDP for adaptive CA V cooperation. To address the in-
herent nonstationary issue due to the partial observability at
each agent, we use a multi-agent deep deterministic policy
gradient (MADDPG) algorithm which adopts a centralized
training distributed execution (CTDE) framework [29], [30].
The model-assisted MADDPG algorithm is presented in Al-
gorithm 1. Each agent trains a critic network and an actor
network based on the global state and joint action in a
centralized training stage, and uses the trained actor network
for decision based on local observation in a distributed exe-
cution stage. To enhance the observability at each agent, we
augment the local observation by two additional elements,
i.e., the average workload and the average transmitter-receiver
distance among all agents, both of which can be provided
by the cluster head [31]. In this manner, a CA V pair has
both local information and some statistical global information
for better-informed decisions during the distributed execution
stage, without acquiring the full global state. Let s(n)
kbe
the augmented local observation for agent kat time slot
n, given by s(n)
k=n
o(n)
k,P
kâˆˆKWk(n)
K,P
kâˆˆKDk(n)
Ko
. Let
s(n)={s(n)
k,âˆ€kâˆˆ K} be the global state at time slot n.
As the MMDP has a discrete action space for each agent,
specifically a binary action space for whether or not to
cooperate, a Gumbel-Softmax estimator is used by each agent
to allow the backpropagation of gradients through the actor
network during training [29]. For agent k, an actor network,
Âµk(sk), parameterized by weights Ï†k, is trained to learn a
continuous action, ak={ak,j, j= 0,1}, based on aug-
mented local observation sk. Here, akis an estimated two-
dimension Gumbel-Softmax distribution over the binary action
space [32]. Let a(n)
k={a(n)
k,j, j= 0,1}be the continuous
action of agent kat time slot n, and let a(n)={a(n)
k,âˆ€k}be
the joint continuous action at time slot n. Agent kobtains the
cooperation decision as the binary action with the maximum
Gumbel-Softmax probability, i.e., xk(n) = arg max j=0,1a(n)
k,j.
In addition to actor network Âµk(sk), agent ktrains a
critic network parameterized by weights Î¸kto approximate
a centralized Q-function, Qk(s,a) =EhPNâˆ’1
n=0Î³nr(n)
ks,ai
,
that takes global state sand joint action aas input to estimate
aQ-value, where Nis the maximum number of learning steps
in an episode. In the training stage of such an actor-critic
framework, although the agents independently take actions
based on the augmented local observations, they evaluate
the actions and refine the policies by taking the actions of
other agents into consideration in Qk(s,a), thus facilitating a
collaborative exploration of the vehicular network environment
to maximize a collective reward. To overcome the divergenceAlgorithm 1: A Model-Assisted MADDPG Algorithm
/*Centralized Training Stage */
1All agents initialize networks with random weights.
2foreach episode do
3 Initialize local observation o(0)
kforkâˆˆ K.
4 forlearning step ndo
5 foragent kdo
6 Send local observation o(n)
kto cluster head.
7 Collect augmented local observation s(n)
k.
8 Decide continuous action a(n)
k=Âµk
s(n)
k
,
derive binary cooperation decision xk(n), and
senda(n)
kto cluster head.
9 Cluster head solves the resource allocation
subproblem, obtains Gâˆ—(n)andr(n), and
broadcasts s(n),a(n), and r(n)to all agents.
10 foragent kdo
11 Add
s(nâˆ’1),a(nâˆ’1), r(nâˆ’1),s(n)
to buffer B
ifnâ‰¥1.
12 Sample mini-batch of experiences from B.
13 Update primary and target critic and actor
networks based on (33), (34), and (32).
/*Distributed Execution Stage */
14foreach episode do
15 forlearning step ndo
16 foragent kdo
17 Send local observation o(n)
kto cluster head.
18 Collect augmented local observation s(n)
k.
19 Decide continuous action a(n)
k=Âµk
s(n)
k
,
derive binary cooperation decision xk(n), and
sendxk(n)to cluster head.
20 Cluster head allocates resources to cooperative CA V
pairs.
update issue, agent kalso has a target critic network, Ë†Qk(s,a),
parameterized by Ë†Î¸k, and a target actor network, Ë†Âµk(sk),
parameterized by Ë†Ï†k, with delayed updates. Agent kinitializes
the primary and target critic and actor networks with random
weights before training ( line 1) and then continually updates
the weights until convergence. MADDPG employs a soft
updating strategy, where agent kupdates weights Ë†Î¸kandË†Ï†k
of the target networks in each learning step ( line13) as
Ë†Î¸k=Î¾Î¸k+ (1âˆ’Î¾)Ë†Î¸kand Ë†Ï†k=Î¾Ï†k+ (1âˆ’Î¾)Ë†Ï†k(32)
withÎ¾being the soft updating rate of the target networks.
The agents interact with the network environment in a
sequence of episodes, each containing a finite number of
learning steps, one learning step for one time slot. An episode
starts when a vehicle cluster is about to move into an RSUâ€™s
coverage area and ends once it leaves the coverage. At the
beginning of each episode, each agent initializes the local
observation ( line3). At the beginning of time slot n, each agent
ksends local observation o(n)
kvia a dedicated control channel
to the cluster head ( line6), which calculates and then returns
the augmented information. Agent kcollects augmented local
observation s(n)
k(line 7), based on which the agent decides
continuous action a(n)
kasÂµk
s(n)
k
by the primary actor10
network, and then discretizes it into a binary cooperation
decision, xk(n)(line 8). All agents send the continuous
actions to the cluster head ( line8). The cluster head allocates
resources to cooperative CA V pairs, determines the maximal
total computing efficiency gain, Gâˆ—(n), using the model-based
resource allocation solution, and calculates reward r(n)in (20)
which is then broadcast to all agents together with global state
s(n)and joint action a(n)(line9). Then, each agent adds a new
transition tuple 
s(nâˆ’1),a(nâˆ’1), r(nâˆ’1),s(n)
to an experience
replay buffer, B, ifnâ‰¥1(line11).
To train the critic and actor networks at each learn-
ing step, each agent samples a mini-batch of Iexperi-
ences from B, among which 
s(i),a(i), r(i),s(i+1)
repre-
sents the i-th experience ( line 12). Agent kupdates the
critic network by minimizing a loss function, Lk(Î¸k) =
1
IPI
i=1h
y(i)
kâˆ’Qk 
s(i),a(i)i2
, where y(i)
k=r(i)+
Î³Ë†Qk
s(i+1),Ë†Âµ1
s(i+1)
1
, . . . , Ë†ÂµK
s(i+1)
K
is a target value
estimated by the target critic and actor networks. Weights Î¸k
are updated via a gradient descent ( line13), given by
Î¸kâ†Î¸kâˆ’Î±Î¸âˆ‡Î¸kLk(Î¸k) (33)
where Î±Î¸is the learning rate for critic networks. The actor
network of agent kaims to maximize a long-term total
expected reward, Jk(Ï†k) =EhPN
n=0Î³nr(n)
ki
, which is the
expected Q-value among all state-action pairs, i.e., Jk(Ï†k) =
Es,aQk(s,a). Thus, weights Ï†kof the actor network are
updated via a gradient ascent ( line13), given by
Ï†kâ†Ï†k+Î±Ï†âˆ‡Ï†kJk(Ï†k) (34)
where Î±Ï†is the learning rate for actor networks, and the
gradient of Jk(Ï†k)is given by
âˆ‡Ï†kJk(Ï†k) =1
IIX
i=1h
âˆ‡Ï†kÂµk
s(i)
k
âˆ‡akQk
s(i), a(i)
1,
. . . , a k, . . . , a(i)
K
ak=Âµk
s(i)
k#
. (35)
After the centralized training stage, each agent uses the
trained actor network for decision in a distributed execution
stage, with the assistance of the cluster head ( lines 14-20).
VI. S IMULATION RESULTS
A. Simulation Setup
We consider a four-lane unidirectional highway, where
Kâˆˆ {2,3,4,5,6}CA V pairs are moving together with 10
HDVs in a vehicle cluster, with an intermittent RSU coverage.
The RSU is 10maway from the highway and provides a
communication radius of 250 m. In the RL task, we consider
a1500 mhighway segment for each episode, during which a
vehicle cluster moves for a distance of 1000 mthrough the
RSU coverage. The vehicle speed is uniformly set in a range
of[23,27]m/s. The time slot length is 500ms. Each episode
spans over an average time duration of 40sand contains
80time slots on average. The delay requirement for object
classification in each time slot is âˆ† = 100 ms.TABLE III: S YSTEM PARAMETERS IN SIMULATION
Parameters Value
Center frequency ( fc) 6GHz
Noise power ( Ïƒ2) âˆ’104 dBm
Transmit power ( pk) 23dBm
Maximum local CPU frequency ( fM) 8GHz
Energy efficiency coefficient ( Îº) 10âˆ’28J/s/Hz3
Feature extraction computing demand ( Î´1)4Ã—106cycles
Feature fusion computing demand ( Î´2) 1000 cycles
Fast inference computing demand ( Î´3) 3.1Ã—105cycles
Full inference computing demand ( Î´4) 7.7Ã—107cycles
Feature data size ( w) 0.29Mbits
Default early exit probability ( Ï) 0.3
Feature-fusion early exit probability ( ËœÏ) 0.6
We use the Simulation of Urban Mobility (SUMO) traffic
simulator to simulate the vehicle trajectories in each episode,
based on which the transmitter-receiver distances of each CA V
pair can be obtained [9], [31], [33]. To obtain the time-varying
channel power gain for each CA V pair, we use the 3GPP
NR-V2X 37.885 highway case for the V2V link path loss
calculation [34]. The path loss in dBfor CA V pair kduring
time slot nis calculated as LdB(n) = 32 .4+20 log10Dk(n)+
20 log10fc, where Dk(n)is the transmitter-receiver distance
in meter, and fcis the center frequency in GHz. For CA V pair
k, the transitions of shared workload, Wk(n), across different
time slots follow a Markov chain with states in {4,5,6,7,8}.
When residing in the RSU coverage, each HDV generates
V2R transmission requests in each time slot according to
aBernoulli (0.5)distribution, with each V2R transmission
request occupying a bandwidth of 0.5MHz. With a total band-
width of B= 10 .5MHz for V2X sidelink communication,
the available radio spectrum bandwidth for V2V transmission
at time slot nisB(n) = Bâˆ’0.5M(n), where M(n)is
the number of HDVs that request V2R transmission at time
slotn. On average, the B(n)value in an episode follows
a decreasing-then-increasing trend when the vehicle cluster
drives through the RSU coverage. Other system parameters
are given in Table III.
We implement both the iterative algorithm for optimal
resource allocation and the MADDPG algorithm for adaptive
CA V cooperation using Python 3.9.2. The learning modules
are implemented using TensorFlow 2.11.0. Each learning
agent has two hidden layers with (64,64)neurons and Relu
activation functions in critic and actor networks. The critic
network has a one-dimension output with no activation func-
tion, and the actor network has a two-dimension output with
Gumbel-SoftMax activation. We set weight ËœÏ‰âˆˆ[0,1]and
penalty P=âˆ’10in reward function (20), and use Î±Î¸= 10âˆ’2
andÎ±Ï†= 10âˆ’3as the critic and actor learning rates, a soft
updating rate of Î¾= 0.01for target network update, and
discount factor Î³= 0.95. For training at each learning step, a
mini-batch of I= 1024 experiences are sampled from buffer
Bwith size 100000 .11
0 500 1000 1500 2000 250000.40.81.21.622.42.8
(a)
0 500 1000 1500 2000 2500-0.6-0.4-0.200.20.4
(b)
Fig. 6: Performance of the optimal resource allocation solution for a different
number of cooperative CA V pairs ( |KC|) atW= 6. (a) Total computing
efficiency gain. (b) Constraint value.
B. Performance Evaluation
We first evaluate the performance of the optimal resource al-
location solution, for a set, KC, of cooperative CA V pairs with
an available bandwidth of 10.5MHz for V2V transmission.
Without loss of generality, all cooperative CA V pairs have an
identical shared workload, W. We examine the impact of |KC|
andWon the total computing efficiency gain.
In the first set of simulations, the performance of the
optimal resource allocation solution is evaluated for |KC| âˆˆ
{2,3,4,5,6}, with W= 6. The transmitter-receiver distances
of all the cooperative CA V pairs are set as 20m. For the
constant workload, the initial binary-search interval, [Î½L, Î½R],
is the same for all the |KC|values. Fig. 6 shows the vari-
ations of total computing efficiency gain Ë†Gâˆ—and constraint
value h(Ë†fâˆ—)during the binary-search of candidate optimal
dual variable Ë†Î½âˆ—, for each |KC|. The asymptotically optimal
total computing efficiency gain and constraint value, Gâˆ—and
h(fâˆ—), obtained at an asymptotically optimal dual variable,
Î½âˆ—, are represented by a red dot for each |KC|value. As
|KC|increases, the radio resources are shared among more
cooperative CA V pairs for feature data transmission, and each
cooperative CA V pair should increase the CPU frequency
to compensate for the lower average transmission rate, to
achieve delay satisfaction. Accordingly, as |KC|increases, the
asymptotically optimal CPU frequency allocation variables,
fâˆ—, are larger, corresponding to a larger Î½âˆ—value. Although
0 2000 4000 6000 8000 1000001234(a)
0 2000 4000 6000 8000 10000-0.6-0.4-0.200.20.4
(b)
Fig. 7: Performance of the optimal resource allocation solution for a different
workload ( W) at|KC|= 5 . (a) Total computing efficiency gain. (b)
Constraint value.
the total reduced amount of computing demand increases
in proportion to |KC|, the total computing efficiency gain
gradually decreases as the CPU frequency further increases,
leading to a first-increasing-then-decreasing trend of Gâˆ—, as
shown in Fig. 6(a). At the asymptotically optimal points,
constraint value h(fâˆ—)approaches zero, as shown in Fig. 6(b).
In the second set of simulations, the performance of the
optimal resource allocation solution is evaluated for |KC|= 5,
with shared workload Wâˆˆ {4,5,6,7,8}, as shown in Fig. 7.
The transmitter-receiver distances of the 5cooperative CA V
pairs are set as [20.4,16.5,11.4,29.7,28.3]m, respectively.
For a heavier workload, there is a right shift for the initial
binary-search interval, [Î½L, Î½R], as both Î½LandÎ½Rhave a
larger value. When Wincreases, the total computing demand
reduction increases proportionally, while the per-object delay
budget,âˆ†
W, decreases inverse proportionally. With a constant
radio spectrum bandwidth, the CPU frequency should be
increased at each CA V pair to satisfy the more stringent delay
requirement as Wincreases. Hence, in Fig. 7(a), we observe
a first-increasing-then-decreasing trend for Gâˆ—, due to a trade-
off between computing demand and CPU frequency. Fig. 7(b)
shows that the constraint value, h(fâˆ—), approaches zero at the
asymptotically optimal points. Fig. 6 and Fig. 7 demonstrate
that, with a limited amount of radio resources, it is necessary
to select the best subset of CA V pairs for cooperation while
taking the shared workload into account, to improve the total12
0 0.2 0.4 0.6 0.8 100.30.60.91.21.51.82.1Computing efficiency gain
(a)
0 0.2 0.4 0.6 0.8 100.30.60.91.21.51.82.1Switching cost
(b)
Fig. 8: An illustration of gain-cost trade-off for different ËœÏ‰values. (a)
Computing efficiency gain. (b) Switching cost.
computing efficiency gain.
Before the performance evaluation of the MADDPG al-
gorithm for adaptive CA V cooperation, we examine the im-
pact of weight ËœÏ‰in reward function (20) on the trade-off
between computing efficiency gain and switching cost. We
setËœÏ‰âˆˆ {0,0.2,0.4,0.6,0.8,1}. For each ËœÏ‰value, a group of
experiments are performed for a different number of CA V pairs
(K). In each experiment, a brute-force search is conducted
among all possible CA V cooperation decisions for a maximum
instantaneous reward in each time slot of 2000 episodes. Fig. 8
shows the slot-average performance in terms of computing
efficiency gain and switching cost for different Kvalues as
ËœÏ‰increases. As a larger ËœÏ‰value puts more emphasis on
minimizing the switching cost, we observe a decreasing trend
for both gain and cost with the increase of ËœÏ‰. We also observe
higher gain and cost for more CA V pairs at a given ËœÏ‰value,
which is to be discussed later. The weight, ËœÏ‰, can be selected
according to the desired gain-cost trade-off. In the following,
ËœÏ‰is set to 0.4, as we observe that the cost is reduced by more
than80% at a gain loss of less than 20% in comparison with
that achieved at ËœÏ‰= 0, forK= 6.
Next, we evaluate the convergence of the MADDPG al-
gorithm, in terms of both reward and training loss, for a
different number of CA V pairs. Fig. 9 shows the convergence
of the average reward per learning step over 15000 episodes,
for a different agent number ( K) from 2to6. In a practical
implementation, when there is a negative penalty in the reward
0 3000 6000 9000 12000 15000
Episode-8-6-4-202Reward with penaltySmoothed
2 Agents
3 Agents
4 Agents5 Agents
6 Agents9000 12000 150001.31.41.5(a)
0 3000 6000 9000 12000 15000
Episode-2-1.5-1-0.500.511.5Reward without penaltySmoothed
2 Agents
3 Agents
4 Agents5 Agents
6 Agents9000 12000 150001.31.41.5
(b)
Fig. 9: Convergence of the reward during the training process. (a) With
penalty. (b) Without penalty.
due to infeasible resource allocation, we can refine the joint
action and let all the CA V pairs work in the default SP mode,
which gives a zero computing efficiency gain and a refined
reward without penalty. The original reward with penalty
and the original action are used during training to guide the
learning agents towards a least penalty after convergence,
while the reward without penalty is the true reward for the
CA V pairs when the action refinement is enabled. Fig. 9
shows the average rewards with and without penalty during
the training process. As Kincreases, both rewards converge
in a slower speed, indicated by a later increase to a converged
value interval. By comparing the smoothed rewards with and
without penalty after convergence, we see that the penalty
has been effectively suppressed, indicated by the limited large
negative glitches in the reward with penalty. It implies that the
learning agents have collaboratively learn the adaptive CA V
cooperation decisions that are feasible for resource allocation
under the network dynamics. Moreover, we observe that more
learning agents tend to improve the reward, as to be discussed.
Fig. 10 shows the per-agent average critic and actor loss
during training for Kfrom 2to6. For the critic network,
the input dimension grows linearly with K. Thus, it is more
difficult to minimize the critic loss if there are more agents,
leading to a higher average critic loss after convergence as
Kincreases, as shown in Fig. 10(a). As the reward tends
to increase for more agents, as shown in Fig. 9, the average
Q-value for the sampled mini-batch of experiences at each13
0 1 2 3 4 5 6 7 8 9 10 11 12
Learning step 10501020304050Average critic loss
Smoothed2 Agents
3 Agents
4 Agents5 Agents
6 Agents
6 7 8 9 10 11 12
1050.511.522.5
(a)
0 1 2 3 4 5 6 7 8 9 10 11 12
Learning step 105-30-25-20-15-10-505101520Average actor lossSmoothed2 Agents
3 Agents
4 Agents5 Agents
6 Agents
9 10 11 12
105-28-27-26
(b)
Fig. 10: Convergence of the critic loss and actor loss during the training
process. (a) Average critic loss. (b) Average actor loss.
learning step increases for more agents. As the actor loss is
the negative average Q-value, there is a lower average actor
loss after convergence as Kincreases, as shown in Fig. 10(b).
To evaluate the effectiveness of the MADDPG algorithm
in improving the computing efficiency gain and reducing the
switching cost, we compare the performance between the
trained MADDPG algorithm and three benchmark algorithms,
for a different number of CA V pairs ( K). The first benchmark
is a random CA V cooperation scheme, in which each CA V pair
switches between SP and CP modes at random in each time
slot. Action refinement is enabled. In the second benchmark,
all CA V pairs always cooperate if there exists a feasible
resource allocation solution among them. Otherwise, action
refinement is triggered to let all CA V pairs work in the default
SP mode. Hence, the solution switches between all CA V pairs
working in the CP mode and all CA V pairs working in the
SP mode, referred to as â€œall CP modeâ€ and â€œall SP modeâ€
respectively. In the third benchmark, we conduct a step-wise
brute-force search among all candidate joint CA V cooperation
decisions in each time slot, for a maximum instantaneous
reward. The time complexity for the brute-force benchmark is
2Ktimes of that for a trained MADDPG algorithm, for solving
2Kresource allocation subproblems given each candidate joint
CA V cooperation decision. By using a trained MADDPG
algorithm, only one resource allocation subproblem is solved
given one learned joint action. For performance comparison,
the25%,50%, and 75% percentiles of the slot-average total
2 3 4 5 6
Number of CAV pairs00.30.60.91.21.51.82.1Computing efficiency gainRandom All MADDPG Brute-force(a)
2 3 4 5 6
Number of CAV pairs00.511.522.53Switching costRandom All MADDPG Brute-force
(b)
2 3 4 5 6
Number of CAV pairs-0.500.511.52RewardRandom All MADDPG Brute-force
(c)
Fig. 11: Performance comparison between the proposed and benchmark
algorithms. (a) Computing efficiency gain. (b) Switching cost. (c) Reward.
computing efficiency gain, slot-average total switching cost,
and slot-average reward are evaluated for each Kvalue using
the four algorithms, as shown in Fig. 11.
Fig. 11(a) demonstrates an increasing trend of the com-
puting efficiency gain as the number of CA V pairs increases
for both the MADDPG algorithm and the brute-force bench-
mark, both of which significantly outperform the other two
benchmarks. Fig. 6(a) demonstrates the first-increasing-then-
decreasing computing efficiency gain when more CA V pairs
cooperate. Accordingly, for the random benchmark in which
the average number of selected cooperative CA V pairs in-
creases linearly with K, we observe such a trend in the
computing efficiency gain as Kincreases, with a turning
point at K= 4. The second benchmark achieves a gradually
degraded computing efficiency gain when Kincreases, until
obtaining a zero gain at K= 6 . For a larger Kvalue,
the chance for infeasible resource allocation among all CA V14
pairs increases, leading to more frequent action refinement
to the â€œall SP modeâ€ with zero gain. Nevertheless, in both
the MADDPG algorithm and the brute-force benchmark, as
there are more candidate CA V pairs in the system, there is
a higher flexibility in selecting the best subset of CA V pairs
for cooperation, by considering their shared workloads and
transmitter-receiver distances. This flexibility contributes to a
further increase in the computing efficiency gain as Kfurther
increases from 4to6. We observe a decreasing speed in the
increase of computing efficiency gain as Kfurther increases,
as the gain from the higher flexibility gradually saturates.
Especially, it is more difficult for the MADDPG algorithm
to find the optimal solution in a distributed fashion as the
agent number, K, increases, leading to a slightly larger sub-
optimality gap from the brute-force benchmark.
From Fig. 11(b), we observe an almost linear increas-
ing switching cost for the random benchmark, while both
the MADDPG algorithm and the brute-force benchmark can
significantly reduce the switching cost. Especially, as the
switching cost has time correlation, the MADDPG algorithm
which can minimize the total switching cost in the long run
achieves a lower average switching cost in comparison with
the step-wise brute-force benchmark which does not take the
future states into account. For the second benchmark, as K
increases, the dominant solution gradually changes from the
â€œall CP modeâ€ to the â€œall SP modeâ€, due to a higher action
refinement probability. Accordingly, the highest switching cost
is obtained at a medium Kvalue. For both the MADDPG
algorithm and the brute-force benchmark, due to a higher
flexibility in cooperative CA V pair selection as Kincreases,
there are more changes in the selection decision, leading to an
increasing switching cost as Kincreases. The reward, which
is a linear combination of the computing efficiency gain and
the switching cost, is shown in Fig. 11 (c) for reference.
VII. C ONCLUSION
In this paper, we develop an adaptive cooperative percep-
tion framework for CA Vs in a moving mixed-traffic vehicle
cluster, while considering the dynamic shared workloads and
channel conditions due to vehicle mobility, dynamic radio
resource availability, and intermittent RSU coverage. A model-
assisted multi-agent reinforcement learning solution is devel-
oped, which integrates learning-based adaptive CA V coopera-
tion decision over time with model-based resource allocation
decision in each time slot. Simulation results demonstrate the
necessity for dynamically activating the cooperative perception
among CA V pairs to improve the total computing efficiency
gain. The effectiveness of the model-assisted MADDPG algo-
rithm is verified, in improving the total computing efficiency
gain under a limited switching cost. In future works, we will
explore the generalization of the learning model for adaptive
CA V cooperation, while considering a varying vehicle cluster
size and a non-stationary network environment.
APPENDIX : PROOF OF THEOREM 1
Lemma 1. Constraint (25) must be active for an optimal
solution to problem P2.Proof. Assume there is an optimal solution to P2which
achieves â€œ <â€ in (25). Then, there must be another feasible so-
lution achieving â€œ =â€ in (25) by decreasing the CPU frequency
for some CA V pairs, thus further decreasing the objective
value in (23) without violating constraint (24), as the objective
function is an increasing function of fC(n)while constraint
function h(f)is a decreasing function of fC(n). Therefore,
the assumption must be false, and constraint (25) must be
active for an optimal solution. Lemma 1 is proved.
For problem P2, the objective function is a second-order
function of decision variable fk(n)âˆˆfC(n)with coefficient
Wk(n)>0, which is convex. Constraint (24) is linear.
For constraint (25), the second-order derivative of constraint
function h(f)with respect to fk(n), is given by
âˆ‚h2(f)
âˆ‚(fk(n))2=2ckbkË†Î´

bkfk(n)âˆ’Ë†Î´3. (A1)
Asfk(n)>Ë†Î´
bk, we have âˆ‚h2(f)/âˆ‚(fk(n))2>0, thus h(f)
is convex. Therefore, P2is convex.
For a convex problem, strong duality holds if Slaterâ€™s
condition (or a weaker form) is satisfied, which requires the
nonlinear inequality constraints to be strictly feasible [28]. For
problem P2, there is only one nonlinear inequality constraint
in (25). Suppose the optimal solution is denoted as fâˆ—
C(n)if
the problem is feasible. Based on Lemma 1, we have h(fâˆ—) =
0. Then, as long as h(f0)<0, there must exist at least one
strictly feasible non-optimal solution fâ‹„
C(n)âª°fâˆ—
C(n)which
gives a larger objective value while satisfying fâ‹„
C(n)âª¯f0
C(n)
andh(fâ‹„)<0. Therefore, strong duality holds for P2if
h(f0)<0. Theorem 1 is proved. Note that, if h(f0) = 0 , we
can directly obtain the optimal solution as fâˆ—
C(n) =f0
C(n); if
h(f0)>0, the problem is infeasible.
REFERENCES
[1] W. Zhuang, Q. Ye, F. Lyu, N. Cheng, and J. Ren, â€œSDN/NFV-
empowered future IoV with enhanced communication, computing, and
caching,â€ Proc. IEEE , vol. 108, no. 2, pp. 274â€“291, 2019.
[2] X. Shen, J. Gao, W. Wu, M. Li, C. Zhou, and W. Zhuang, â€œHolistic
network virtualization and pervasive network intelligence for 6G,â€ IEEE
Commun. Surv. Tutor. , vol. 24, no. 1, pp. 1â€“30, 2021.
[3] Y . Hui, X. Ma, Z. Su, N. Cheng, Z. Yin, T. H. Luan, and Y . Chen,
â€œCollaboration as a service: Digital-twin-enabled collaborative and dis-
tributed autonomous driving,â€ IEEE Internet Things J. , vol. 9, no. 19,
pp. 18 607â€“18 619, 2022.
[4] J. Wang, J. Liu, and N. Kato, â€œNetworking and communications in
autonomous driving: A survey,â€ IEEE Commun. Surv. Tutor. , vol. 21,
no. 2, pp. 1243â€“1274, 2018.
[5] J. Zhang and K. B. Letaief, â€œMobile edge intelligence and computing
for the Internet of vehicles,â€ Proc. IEEE , vol. 108, no. 2, pp. 246â€“261,
2019.
[6] X. Zheng, S. Li, Y . Li, D. Duan, L. Yang, and X. Cheng, â€œConfidence
evaluation for machine learning schemes in vehicular sensor networks,â€
IEEE Trans. Wirel. Commun. , vol. 22, no. 4, pp. 2833â€“2846, 2023.
[7] Y . Jia, R. Mao, Y . Sun, S. Zhou, and Z. Niu, â€œOnline V2X scheduling
for raw-level cooperative perception,â€ in Proc. IEEE ICC , 2022, pp.
309â€“314.
[8] â€”â€”, â€œMass: Mobility-aware sensor scheduling of cooperative per-
ception for connected automated driving,â€ IEEE Trans. Veh. Technol. ,
vol. 72, no. 11, pp. 14 962â€“14 977, 2023.
[9] M. K. Abdel-Aziz, C. Perfecto, S. Samarakoon, M. Bennis, and W. Saad,
â€œVehicular cooperative perception through action branching and feder-
ated reinforcement learning,â€ IEEE Trans. Commun. , vol. 70, no. 2, pp.
891â€“903, 2022.15
[10] Z. Xiao, J. Shu, H. Jiang, G. Min, H. Chen, and Z. Han, â€œPerception
task offloading with collaborative computation for autonomous driving,â€
IEEE J. Sel. Areas Commun. , vol. 41, no. 2, pp. 457â€“473, 2023.
[11] Y . Sun, J. Xu, and S. Cui, â€œUser association and resource allocation
for MEC-enabled IoT networks,â€ IEEE Trans. Wirel. Commun. , vol. 21,
no. 10, pp. 8051â€“8062, 2022.
[12] J. Lin, P. Yang, N. Zhang, F. Lyu, X. Chen, and L. Yu, â€œLow-latency edge
video analytics for on-road perception of autonomous ground vehicles,â€
IEEE Trans. Industr. Inform. , vol. 19, no. 2, pp. 1512â€“1523, 2022.
[13] X. Zhang, A. Zhang, J. Sun, X. Zhu, Y . E. Guo, F. Qian, and Z. M. Mao,
â€œEMP: Edge-assisted multi-vehicle perception,â€ in Proc. 27th Annual
International Conf. Mobile Computing and Networking , 2021, pp. 545â€“
558.
[14] Q. Chen, S. Tang, Q. Yang, and S. Fu, â€œCooper: Cooperative perception
for connected autonomous vehicles based on 3D point clouds,â€ in Proc.
IEEE Int. Conf. Distrib. Comput. Syst. (ICDCS) , 2019, pp. 514â€“524.
[15] H. Qiu, P. Huang, N. Asavisanu, X. Liu, K. Psounis, and R. Govin-
dan, â€œAutocast: Scalable infrastructure-less cooperative perception for
distributed collaborative driving,â€ in Proc. ACM Int. Conf. Mobile Syst.,
Appl. and Services (MobiSys) , 2021, pp. 128â€“141.
[16] Q. Chen, X. Ma, S. Tang, J. Guo, Q. Yang, and S. Fu, â€œF-cooper: Feature
based cooperative perception for autonomous vehicle edge computing
system using 3D point clouds,â€ in Proc. ACM/IEEE Symp. Edge Comput.
(SEC) , 2019, pp. 88â€“100.
[17] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and R. Ur-
tasun, â€œV2VNeT: Vehicle-to-vehicle communication for joint perception
and prediction,â€ in Proc. Eur. Conf. Comput. Vision (ECCV) , 2020, pp.
605â€“621.
[18] W. Wu, N. Chen, C. Zhou, M. Li, X. Shen, W. Zhuang, and X. Li,
â€œDynamic RAN slicing for service-oriented vehicular networks via
constrained learning,â€ IEEE J. Sel. Areas Commun. , vol. 39, no. 7, pp.
2076â€“2089, 2021.
[19] W. Zhang, Z. He, L. Liu, Z. Jia, Y . Liu, M. Gruteser, D. Raychaudhuri,
and Y . Zhang, â€œElf: accelerate high-resolution mobile deep vision with
content-aware parallel offloading,â€ in Proc. ACM MobiCom , 2021, pp.
201â€“214.
[20] H. Wang, Q. Li, H. Sun, Z. Chen, Y . Hao, J. Peng, Z. Yuan, J. Fu, and
Y . Jiang, â€œVabus: Edge-cloud real-time video analytics via background
understanding and subtraction,â€ IEEE J. Select. Areas Commun. , vol. 41,
no. 1, pp. 90â€“106, 2023.
[21] K. Yang, J. Yi, K. Lee, and Y . Lee, â€œFlexPatch: Fast and accurate object
detection for on-device high-resolution live video analytics,â€ in IEEE
Proc. INFOCOM , 2022, pp. 1898â€“1907.
[22] S. Teerapittayanon, B. McDanel, and H.-T. Kung, â€œDistributed deep
neural networks over the cloud, the edge and end devices,â€ in Proc.
IEEE ICDCS , 2017, pp. 328â€“339.
[23] K. Qu, W. Zhuang, W. Wu, M. Li, X. Shen, X. Li, and W. Shi,
â€œStochastic cumulative DNN inference with RL-aided adaptive IoT
device-edge collaboration,â€ IEEE Internet Things J. , vol. 10, no. 20,
pp. 18 000â€“18 015, 2023.
[24] E. Li, L. Zeng, Z. Zhou, and X. Chen, â€œEdge AI: On-demand acceler-
ating deep neural network inference via edge computing,â€ IEEE Trans.
Wirel. Commun. , vol. 19, no. 1, pp. 447â€“457, 2020.
[25] Z. Liu, Q. Lan, and K. Huang, â€œResource allocation for multiuser
edge inference with batching and early exiting,â€ IEEE J. Select. Areas
Commun. , vol. 41, no. 4, pp. 1186â€“1200, 2023.
[26] H. Wang, H. Bao, L. Zeng, K. Luo, and X. Chen, â€œReal-time high-
resolution pedestrian detection in crowded scenes via parallel edge
offloading,â€ in Proc. IEEE ICC , 2023.
[27] K. Qu, W. Zhuang, Q. Ye, X. Shen, X. Li, and J. Rao, â€œDynamic
flow migration for embedded services in SDN/NFV-enabled 5G core
networks,â€ IEEE Trans. Commun. , vol. 68, no. 4, pp. 2394â€“2408, 2020.
[28] S. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization .
Cambridge university press, 2004.
[29] R. Lowe, Y . I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
â€œMulti-agent actor-critic for mixed cooperative-competitive environ-
ments,â€ in Proc. 30th Adv. Neural Inf. Process. Syst. (NeurIPS) , 2017,
pp. 6379â€“6390.
[30] J. Tian, Q. Liu, H. Zhang, and D. Wu, â€œMultiagent deep-reinforcement-
learning-based resource allocation for heterogeneous QoS guarantees for
vehicular networks,â€ IEEE Internet Things J. , vol. 9, no. 3, pp. 1683â€“
1695, 2022.
[31] Q. Ye, W. Shi, K. Qu, H. He, W. Zhuang, and X. Shen, â€œJoint RAN
slicing and computation offloading for autonomous vehicular networks:
A learning-assisted hierarchical approach,â€ IEEE Open J. Veh. Technol. ,
vol. 2, pp. 272â€“288, 2021.[32] E. Jang, S. Gu, and B. Poole, â€œCategorical reparameterization with
gumbel-softmax,â€ Proc. ICLR , 2017.
[33] â€œSimulation of Urban MObility (SUMO) 1.16.0,â€ https://www.eclipse.
org/sumo/, 2023, [Online; accessed 8-January-2024].
[34] 3GPP, â€œStudy on evaluation methodology of new Vehicle-to-Everything
(V2X) use cases for LTE and NR,â€ 3rd Generation Partnership Project
(3GPP), Technical Specification (TS) 37.885, 2019, version 15.3.0.