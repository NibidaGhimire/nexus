arXiv:2301.12865v3  [cs.LG]  1 Sep 2023SMDP-Based Dynamic Batching for Efﬁcient
Inference on GPU-Based Platforms
Yaodan Xu, Jingzhou Sun, Sheng Zhou, Zhisheng Niu
Beijing National Research Center for Information Science a nd Technology
Department of Electronic Engineering, Tsinghua Universit y, Beijing 100084, P.R. China
{xyd21, sunjz18 }@mails.tsinghua.edu.cn, {sheng.zhou, niuzhs }@tsinghua.edu.cn
Abstract —In up-to-date machine learning (ML) applications
on cloud or edge computing platforms, batching is an importa nt
technique for providing efﬁcient and economical services a t scale.
In particular, parallel computing resources on the platfor ms, such
as graphics processing units (GPUs), have higher computati onal
and energy efﬁciency with larger batch sizes. However, larg er
batch sizes may also result in longer response time, and thus it
requires a judicious design. This paper aims to provide a dyn amic
batching policy that strikes a balance between efﬁciency an d
latency. The GPU-based inference service is modeled as a bat ch
service queue with batch-size dependent processing time. T hen,
the design of dynamic batching is a continuous-time average -cost
problem, and is formulated as a semi-Markov decision proces s
(SMDP) with the objective of minimizing the weighted sum of
average response time and average power consumption. The
optimal policy is acquired by solving an associated discret e-
time Markov decision process (MDP) problem with ﬁnite state
approximation and “discretization”. By introducing an abs tract
cost to reﬂect the impact of “tail” states, the space complex ity
and the time complexity of the procedure can decrease by 63.5 %
and 98%, respectively. Our results show that the optimal pol icies
potentially possess a control limit structure. Numerical r esults
also show that SMDP-based batching policies can adapt to dif fer-
ent trafﬁc intensities and outperform other benchmark poli cies.
Furthermore, the proposed solution has notable ﬂexibility in
balancing power consumption and latency.
I. I NTRODUCTION
The last decade has witnessed the rapid growth of machine
learning (ML), and graphics processing units (GPUs) play a
prominent role in accelerating the training and inference o f
neural networks due to their advantage in parallel computin g
[1]. The wide deployment of ML on various devices sparks
a growing demand for ML-as-a-Service (MLaaS) platforms
such as Google Cloud Prediction [2], where trained models
are published in the cloud to provide inference (prediction )
services for massive end-users.
An important factor that affects both cost and performance
of ML inference serving is batch processing, or batching [3],
[4]. For GPUs, batching brings about a signiﬁcant increase
in both computational efﬁciency and energy efﬁciency [5],
i.e., the average processing time (energy consumption) per
task decreases with the batch size. As depicted in Fig. 1,
the GPU-based computing platform can bundle the same type
of inference requests from possibly different users, and se rve
them concurrently using the same ML application, for exampl e
in the scenario of Function as a Service (FaaS) [6].
GPU Server User 1 
User 2 
User 3 Results 
batch processing Inference Requests 
Fig. 1. Batching of the inference requests from different us ers on a GPU-
based platform.
Despite the well-known beneﬁts, there are two main
challenges in batching: 1) Although batching improves the
throughput of GPU, the inference response time is likely to
increase because of waiting to form a batch, as well as longer
processing time of serving more requests at once [4], [7].
2) Static conﬁgured batching does not ﬁt realistic scenario s
[8], where it shows poor responsiveness at low load and
low throughput at high load [9]. To tackle these problems,
a dynamic batching scheme is required, to judiciously adjus t
the batch size to balance the performance and cost.
Recently, several dynamic batching schemes have been
proposed for machine learning inference systems [3], [4], [ 7]–
[11]. Meanwhile, only a small amount of researches have made
progresses in the performance analysis. SERF [10] models
the inference serving as a M/D/c queue, but unfortunately
batching is not explicitly considered in the model. Another
work BATCH [7] models the request arrival as a Poisson
process or Markov-modulated Poisson process with two phase s
(MMPP(2)) [12] and gets the number of requests that arrive be -
fore timeout. However, the cases where requests arrive duri ng
processing time are missing from the analysis. The author of
[13] gives a closed-form queuing analysis of dynamic batchi ng
under the work-conserving policy, which is not optimal sinc e
it eliminates the potential of forming larger batches.
In fact, dynamic batching decision is an optimal control
problem in batch service queues. Although there are abundan t
researches concerning this topic in operational research [ 14]–
[16], they do not cope with our problem because of their
essential assumption that the processing time of a batch is
independent of its batch size. In [17], the author models the
load-balancing problem in multiple batch service queues wi th
size-dependent processing time as a Markov decision process
(MDP) and claims it as an open problem. Actually, optimal
batching in one such batch service queue, as the simplest sub -
problem of [17], still remains unsolved.To the best of our knowledge, our work is the ﬁrst to for-
mulate and optimally solve the dynamic batching of inferenc e
requests as a semi-Markov decision process (SMDP). The
considered objective is a weighted sum of average response
time and average power consumption. To solve the problem,
we provide a procedure composed of ﬁnite state approxima-
tion, model “discretization” and relative value iteration . The
demanding problem of state space explosion is tackled by
a novel introduction of an abstract cost, which reﬂects the
impact of costs in “tail” states. From numerical results, we
observe that the optimal policies potentially possess a con trol
limit structure, which could inspire the simpliﬁcation of r epre-
sentation and computation in future researches. Compariso ns
with other benchmarks demonstrate that: 1) The SMDP-based
policies always achieve the best objective in different set tings
of parameters. 2) When having the same average response
time, the SMDP-based policies never consume more energy
than any other benchmark policy, and vice versa. Moreover,
the proposed solution can adapt to different trafﬁc intensi ties,
and it can also ﬂexibly balance the response time and power
consumption.
II. S YSTEM MODEL
We consider the scenario of a single GPU-based server
in the continuous-time setting. Batching is executed on the
same type of inference requests from possibly different end -
users, and it cannot be interrupted until the current batch i s
processed. The system is modeled as a single service queue
with the arrival of inference requests following a Poisson
process with intensity λ. The requests waiting to be processed
are stored in a buffer which is assumed to be inﬁnitely large.
The total number of requests in the buffer as well as being
processed at time t≥0is denoted by s(t)∈ {0,1,2,...}.
Many researchers have conducted experiments to proﬁle
the computation latency of GPU when batch processing ML
inference tasks [4], [7]–[11], [13], [18], [19]. It has been
examined in [7] that the coefﬁcient of variation (CV) of the
batch processing time for image recognition is near CV=0, i. e.
deterministic . This result is reasonable since the calculations
in ML inference tasks are predeﬁned. Meanwhile, proﬁling
results also exhibit that the inference latency grows linearly
with the batch size [9], [11], [13]. As shown in Fig. 2, linear
regression on the statistics from NIVIDIA [5] also validate s
the conclusion.
Letb∈ B/defines{1,2,...,B max}denote the batch size, where
Bmax is the maximum batch size allowed by the system. We
assume that the batch processing time is a deterministic val ue
τ[b]and is linear with the batch size b, given by
τ[b]=αb+τ0, (1)
for some α >0andτ0≥0. Deﬁne µ[b]=b/τ[b]as the
mean throughput (the number of inference requests processe d
per unit time) for a batch size b. It is easy to verify that the
mean throughput µ[b]is non-decreasing with the batch size
b, and the maximum throughput is acquired at the maximum
batch size Bmax. Letρ=λ/µ[Bmax]denote the ratio of the1 2 4 8 16 32 64 128
Batch Size b0510152025303540Inference Latency [ms]P4 GoogLeNet
V100 GoogLeNet
0.30506b + 1.0524 (R2=0.99989)
0.091051 b + 1.0316 (R2=0.99962)
Fig. 2. Inference latency for batch processing GoogLeNet [2 0] on TESLA P4
and TESLA V100 respectively. The batch size is plotted in log2coordinate.
1 2 4 8 16 32 64 128
Batch Size b00.511.522.533.54Energy Consumption [Joule]P4 GoogLeNet
V100 GoogLeNet
0.019899 b + 0.019603 (R2=0.99997)
0.028348 b + 0.076003 (R2=0.99967)
Fig. 3. Energy consumption for batch processing GoogLeNet [ 20] on TESLA
P4 and TESLA V100 respectively. The batch size is plotted in log2coordinate.
arrival rate over the maximum throughput. We assume that
ρsatisﬁesρ∈[0,1), which is a necessary condition for the
system stability.
The energy consumption of processing a batch of brequests
is denoted by ζ[b], which can be calculated by the product of
the average board power and the batch processing time. Also
based on the statistics from [5], we perform linear regressi on
as shown in Fig. 3 and make the following assumption that
the energy consumption ζ[b]is linear with the batch size b,
which is given by
ζ[b]=βb+ζ0, (2)
for some β >0andζ0≥0. The energy efﬁciency η[b]=
b/ζ[b]is deﬁned as the average number of requests served
with one unit of energy consumption. It is clear that the ener gy
efﬁciency η[b]is non-decreasing with the batch size b.
As a result, given the GPU and the inference model, the
parameters Bmax,α,τ0,β,ζ0can be proﬁled and therefore
determined.
There are two main factors to be considered in the objective:
One is the request response time, or latency, which includes
both waiting and processing time, as the performance metric .
The other is power consumption on the server, as the running
cost metric. Our objective is to minimize the average weight ed
sum of these two metrics.The serving process consists of sequential service rounds.
Deﬁneti(ti≥0, i= 1,2,...)as the start time of the ith
service round and b(ti)∈ B as the batch size in the ith service
round. Let N(t)∈Ndenote the total number of service rounds
until time t≥0. The objective is then expressed as
min limsup
T→∞1
T/braceleftBigg
w11
λ/integraldisplayT
0s(t)dt+w2N(T)/summationdisplay
i=1ζ[b(ti)]/bracerightBigg
,(3)
wherew1≥0andw2≥0are the weights. Note that here the
average request response time is equivalently transformed to
the average queue length by Little’s Law [21].
III. SMDP F ORMULATION
It is natural to formulate (3) as a continuous-time system an d
utilize the embedded Markov chain method, or semi-Markov
decision process (SMDP). The continuous-time SMDP for-
mulation reduces the system state from three elements to one
element, compared to the discrete-time MDP formulation [17 ].
As a result, it can relieve the curse of dimensionality when
applying iteration-based algorithms.
In SMDP, we only consider the states at decision points,
when either the server completes a batch of service, or a re-
quest arrives while the server is idle. The decision points d ivide
the timeline into sequential decision epochs. Let {sm,m=
0,1,...}denote the state process, where the state is the
number of requests in the system, taking values from the stat e
spaceS/defines{0,1,2,...}. At each epoch m, the server takes an
actionamfrom the action space A/defines{0,1,2,...,B max}. The
actionamis the size of batch to be processed, where am= 0
means to keep idle in the mth epoch. Let As⊆ A be the set
of feasible actions for a given state s. Note that the number of
requests to be batched should be no more than the available
requests, which means As/defines{0,1,2,...,min{s,Bmax}}.
The state transition is associated with the action. Let
m(j|s,a)denote the probability that the semi-Markov decision
process occupies state jat the next decision epoch when action
ais chosen at the state s. Letp[b]
kdenote the probability that
krequests arrive during the period of processing a batch of
b∈ B requests. Since the arrival of requests follows a Poisson
process, we have
p[b]
k=e−λτ[b](λτ[b])k
k!, k= 0,1,... (4)
Then the transition probability for ∀j,s∈ S,∀a∈ Asis
expressed as
m(j|s,a) =

p[a]
j−s+aj≥s−a, a∈ As, a/ne}ationslash= 0
1 j=s+1, a= 0
0 otherwise.(5)
Let a random variable γmdenote the sojourn time between
the(m−1)th and the mth epoch. Let δ(·)denote the impulse
function. The probability density function (PDF) of γin state
sunder action ais denoted by fs,a(x) (x≥0), given by
fs,a(x) =/braceleftBigg
δ/parenleftbig
x−τ[a]/parenrightbig
a∈ As,a/ne}ationslash= 0
λe−λxa= 0,∀s∈ S.(6)Deﬁney(s,a) =E[γ|s,a]as the expected sojourn time until
the next decision epoch, given by
y(s,a) =/braceleftBigg
τ[a]a∈ As,a/ne}ationslash= 0
1/λ a = 0,∀s∈ S. (7)
Costs are charged for serving the requests as well as holding
them. The cost of serving a batch of xrequests is denoted by
u(x), and the cost of holding xrequests in the system per unit
time is denoted by v(x). Letc(s,a)denote the expected cost
until the next decision epoch when action a∈ Asis taken in
states. We have c(s,0) =u(0)+v(s)y(s,0), and for a/ne}ationslash= 0,
c(s,a) =u(a)+/integraldisplay∞
0/integraldisplayx
0fs,a(x)∞/summationdisplay
k=0v(s+k)e−λt(λt)k
k!dtdx.
(8)
The cost functions corresponding to the objective in (3) are
u(x) =w2ζ[x](x >0),u(0) = 0 andv(x) =w1
λx. This leads
to a detailed description of c(s,a)as
c(s,a) =u(a)+/integraldisplayτ[a]
0∞/summationdisplay
k=0v(s+k)e−λt(λt)k
k!dt
=w2ζ[a]+/integraldisplayτ[a]
0w1
λ(s+λt)dt
=w2ζ[a]+w1[s
λτ[a]+1
2(τ[a])2], a∈ As,a/ne}ationslash= 0,
c(s,0) =w1s
λ2.
(9)
A decision rule d(m)∈R|A|is a vector of probabilities
assigned to each action at epoch m, and it is deterministic
if one action is taken with probability 1. A policy π=
{d(0),d(1),d(2),...}is a sequence of decision rules, and it
is called stationary ifd(m)=d,∀m∈N. Letntdenote the
number of decisions up to time t. Our goal is to ﬁnd a policy
that minimizes the long term average expected cost gπ(s0),
given that the system occupies state s0∈ S att= 0, which is
min
πgπ(s0) = limsup
T→∞1
TEπ
s/braceleftBigg/integraldisplayT
0v(s(t)) dt+nT−1/summationdisplay
i=0u(ai)/bracerightBigg
.
(10)
SinceSis countable and the model can be easily veriﬁed
to be unichain, according to [22], the optimality equations are
h(s) = min
a∈As/braceleftBigg
c(s,a)−gy(s,a)+/summationdisplay
j∈Sm(j|s,a)h(j)/bracerightBigg
,(11)
for∀s∈ S, whereh(s)denotes the relative value of being
in statesandgdenotes the average cost per unit time. By
Theorem 11.4.4 in [22], the constant and functions (g,h)
satisfying (11) are exactly the optimal average cost per uni t
time and the corresponding relative value functions.
The considered model is an inﬁnite state SMDP with non-
negative, unbounded costs and ﬁnite action sets. Sennott ha s
proved in [23] that an average expected optimal stationary
deterministic policy exists in such a model under several
conditions that ﬁrmly hold in our problem. The details are
omitted here due to the page limits.IV. P ROCEDURE FOR SOLVING INFINITE STATE SMDP
A. Finite State Approximation
The SMDP problem has inﬁnite states in S={0,1,2,...},
and is impractical to be solved by numerical methods. Hence,
we truncate the inﬁnite state space to a ﬁnite state space
S′={0,1,...,s max,So}, which replaces the states larger
thansmax by an “overﬂow” state So. The dimension of the
ﬁnite state space is |S′|=smax+ 2, wheresmax needs to
be at least no less than Bmax. The rationale of the state space
truncation is that the tail probability, deﬁned as the proba bility
of being in the “tail” states ˆS={smax+ 1,smax+ 2,...},
decreases with smax. Whensmax is large enough, the “tail”
states are negligible. Detailed analysis of convergence an d
error bounds of ﬁnite state approximation can be found in
[24].
In the truncated model, the action space A, the sojourn time
distribution fs,a, and the expected sojourn time y(s,a)are the
same as before, while the feasible action space at state Sois
ASo={0,1,...,B max} ≡ A sincesmax≥Bmax. Original
transitions to the “tail” states ˆSare aggregated to So, and we
assume the number of requests at Soissmax. The adapted
transition probability m′(j|s,a)for∀j,s∈ S′,∀a∈ Asis
m′(j|So,a) =

p[a]
j−smax+aj≥smax−a,j/ne}ationslash=So,a/ne}ationslash= 0
1−a/summationtext
i=0p[a]
ij=So,a/ne}ationslash= 0
1 j=So,a= 0
0 otherwise,
m′(j|s,a) (s/ne}ationslash=So) =


p[a]
j−s+a j≥s−a,j/ne}ationslash=So,a/ne}ationslash= 0
1−smax−s+a/summationtext
i=0p[a]
ij=So,a/ne}ationslash= 0
1 j=s+1,s < s max,a= 0
1 j=So,s=smax,a= 0
0 otherwise.(12)
The unbounded holding cost induced by the inﬁnite states in
the primal problem is also erased by the truncation. Therefo re,
we introduce an abstract cost coy(s,a) (co≥0)to the
“overﬂow” state So, working as an estimation of the difference
between the expected holding cost at “tail” states and the
holding cost at smax. The adapted cost c′(s,a)is
c′(s,a) =/braceleftBigg
c(smax,a)+coy(s,a)s=So
c(s,a) s/ne}ationslash=So,s∈ S′,∀a∈ As.
(13)
Sinceρ∈[0,1), the optimal policy must stabilize the system.
The abstract cost can be also interpreted as an overﬂow pun-
ishment, which pushes the optimal policy away from causing
overﬂow. Note that the abstract cost coy(s,a)in (13) is rarely
mentioned in the literature, without which the problem can b e
solved as well, but with a larger satisfactory smaxand higher
computational complexity in iteration algorithms (which w ill
be discussed in section V-C).We establish a criterion to assess the approximation regard -
ing the difference in average cost: Given a stationary deter -
ministic policy as a function π:S′→ A , the corresponding
state transition matrix is Pπ∈R|S′|×|S′|. Suppose that the
Markov chain with Pπhas a unique stationary distribution
µ= (µ0,µ1,...,µ So). Then the average cost per unit time is
gπ=/summationtext
s∈S′µs·c′(s,π(s))/summationtext
s∈S′µs·y(s,π(s)). (14)
Let∆πbe the average cost contributed by Soper unit time:
∆π=µSo·c′(So,π(So))/summationtext
s∈S′µs·y(s,π(s)). (15)
Given a predeﬁned constant δ >0, if∆π< δ, we state that
the approximation is acceptable with tolerance δ. Otherwise,
the approximation is not acceptable with tolerance δand a
largersmax should be selected.
The optimality equations of the ﬁnite state SMDP are
h(s) = min
a∈As/braceleftBigg
c′(s,a)−gy(s,a)+/summationdisplay
j∈S′m′(j|s,a)h(j)/bracerightBigg
,
(16)
for∀s∈ S′. Denote g∗as the optimal average expected cost.
B. Associated Discrete-Time MDP
The ﬁnite state continuous-time SMDP is associated with a
discrete-time MDP through a “discretization” transformation
(See Chapter 11.4 in [22]). The state space S′, the action
spaceAand the feasible action space Asfor anys∈ S′keep
unchanged in the transformed model. The transformed cost
˜c(s,a)and the transformed transition probability ˜m(j|s,a)for
∀j,s∈ S′,∀a∈ Asare
˜c(s,a)/definesc′(s,a)/y(s,a),
˜m(j|s,a)/defines/braceleftBigg
ηm′(j|s,a)/y(s,a) j/ne}ationslash=s
1+η[m′(s|s,a)−1]/y(s,a)j=s,(17)
whereηsatisﬁes
0< η < y(s,a)/(1−m′(s|s,a)), (18)
for alla∈Asands∈S′for which m′(s|s,a)<1.
By (7) and (12), an appropriate ηshould satisfy
0< η <min/braceleftBigg
1
λ,min
a∈A/braceleftbiggτ[a]
1−p[a]
a,τ[a]
a/summationtext
i=0p[a]
i/bracerightbigg/bracerightBigg
. (19)
And from experiments we ﬁnd that the larger the ηis, the
faster the value-based iteration algorithm converges.
For the discrete-time MDP, the optimality equations are
h(s) = min
a∈As/braceleftBigg
˜c(s,a)−g+/summationdisplay
j∈S′˜m(j|s,a)h(j)/bracerightBigg
,∀s∈ S′.
(20)
By Proposition 11.4.5 in [22], if (˜g,˜h)satisfy the discrete-
time optimality equations in (20), then (˜g,η˜h)satisfy (16)
and˜g=g∗is the optimal average cost in the continuous-time
model. And Theorem 11.4.6 in [22] proves the existence of
an average optimal stationary deterministic policy.0.9 0.7 0.5 0.3 0.1 1 2 3 4
01 2 3 … 4
01 2 3 … 4
01 2 3 … 4
0 02 3 … 4
0 02 3 … 40 s s
0.9 0.7 0.5 0.3 0.1 1 2 3 4
01 2 3 … 4
002 3 … 4
002 3 … 4
0 02 3 … 4
0 02 3 … 40
ρ ρ0.9 0.7 0.5 0.3 0.1 1 2 3 40 6 7 8 9 5
002 3 5 4 6 7 8 9 …
00 0 0 5 4 6 7 8 9 …
00 0 0 0 0 0 7 8 9 …
00 0 0 0 0 0 0 8 9 …
00 0 0 0 0 0 7 8 9 …
(a) (b) (c) 0.9 0.7 0.5 0.3 0.1 1 … 31 32 0 34 33 
00 … 0 32 32 32 …
(d) 00 … 0 32 32 32 …
00 … 0 32 32 32 …
00 … 0 32 32 32 …
00 … 0 32 32 32 …
ρ ρ ρ ρ ρ ρs s s s s s
Fig. 4. The optimal policies for the average cost problem (10 ) with energy and latency characteristics of GoogLeNet on TE SLA P4. The maximum batch
size is chosen as Bmax= 32 . The weights are (a) w1= 1,w2= 0; (b)w1= 1,w2= 0.1; (c)w1= 1,w2= 1; (d)w1= 1,w2= 500 .
C. Relative Value Iteration
In this paper, we utilize the value-based iteration algorit hm
to solve the discrete-time MDP. Speciﬁcally, for the averag e-
cost MDP, the standard value iteration is numerically unsta ble,
so we use relative value iteration (RVI) [22] instead.
LetJn(s)be the value function at state s∈ S′in thenth
iteration, where n∈N. At the beginning of the algorithm, a
states∗∈ S′is arbitrarily chosen. The iterative formula of
RVI is
Jn+1(s) = min
a∈As/braceleftBigg
˜c(s,a)−Jn(s∗)+/summationdisplay
j∈S′˜m(j|s,a)Jn(j)/bracerightBigg
.
(21)
Note that in each iteration, the value function of s∗is sub-
tracted from the standard Bellman equation. And the compute d
policy consists of a∗(s)that minimizes the RHS of (21).
In practice, we specify a maximum number of iterations,
which isitermax. Another stopping criterion is that the span of
the difference between iterations is smaller than a predeﬁn ed
constantǫ >0, which is expressed as
max
s∈S′{Jn+1(s)−Jn(s)}−min
s∈S′{Jn+1(s)−Jn(s)}< ǫ. (22)
Suppose the total number of iterations is n. The number
of multiplications per iteration is/summationtext
s∈S′|As||S′| ≈Bmaxs2
max.
Then, the time complexity is O(nBmaxs2
max). The space
complexity is mainly inﬂuenced by ˜c(s,a)and˜m(j|s,a). Re-
ferring to (12), the storage of ˜m(j|s,a)reduces to the storage
ofp[a]
i. Therefore, the space complexity is O(Bmaxsmax).
As aforementioned, the optimal policy of the discrete-time
MDP (in section IV-B) is also optimal in its associated ﬁnite
state SMDP (in section IV-A). Furthermore, it should be
optimal in the original inﬁnite state SMDP (in section III)
as well if the ﬁnite state approximation (in section IV-A) is
accurate enough. Moreover, the computational complexity c an
be decreased if a smaller smaxis used in the approximation.
V. N UMERICAL RESULTS
In this section, we take the GoogLeNet inference on TESLA
P4 as the model, where the batch processing latency is
τ[b]= 0.3051b+ 1.052 (ms) and the energy consumption is
ζ[b]= 19.90b+19.60(mJ). The maximum batch size Bmaxis
taken as 32 and the maximum service rate is µ[Bmax]= 2.96
(requests/ms). The average power consumption is measured
by (mJ/ms), which is Watt (W). We use ρ∈[0,1)to represent
the trafﬁc intensity normalized by µ[Bmax].0 5 10 15 20050100150200250300350400Average Cost per Unit TimeMaximum Batching
Static Batching ( b=16)
Static Batching ( b=8)
Work-Conserving
SMDP Solution
Fig. 5. The comparison of different policies on the average c ost per unit time,
wherew1= 1 andw2varies between 0and20.
A. SMDP Solution
Fig. 4 demonstrates the policies obtained by solving the
SMDP under different ρandw2. Each row in the chart is
a stationary deterministic policy, where each element deno tes
the action taken at the state corresponding to the column. It
can be concluded from the solutions that the system should
not serve until the state exceeds a threshold, which is calle d
acontrol limit . This is exactly the conclusion in batch service
queues with i.i.d. batch processing time [14]. The control
limit structure generally exists in SMDP solutions from the
numerous results we have obtained. From Fig. 4, we ﬁnd that
the control limit increases with w2. Whenw2is as large as
500, the control limits under different trafﬁc intensities are all
Bmax. This is reasonable because the importance of power
consumption grows with w2, and the energy is better saved
with a larger batch size. Another observation is that when
w1= 1 andw2= 0, i.e. the objective is only concerned
with latency, the control limits are close to 1. It means that in
this parameter setting, the increased computational efﬁci ency
rarely compensates the additional latency introduced by a
larger control limit.
B. Performance Comparison
We compare the SMDP solutions with the work-conserving
policy and static batching policies with batch size b=
8,16,32. In the work-conserving policy, the server processes
the maximum feasible batch of current requests and never
waits for a new-coming request unless there are no requests
waiting. In static batching policies, the server always pro cesses
batches in a constant batch size, and waits for new-coming
requests if there are not enough requests. The static batchi ngTABLE I
EVALUATION OF APPROXIMATIONS ACCEPTABLE WITH TOLERANCE δ= 0.001 UNDER DIFFERENT co.
co 10000 1000 100 10 0
minsmax 89 78 70 161 192
Iterations 1847 1635 1483 10000 10000
Space Complexity 2848 2496 2240 5152 6144
Time Complexity 4.68×1083.18×1082.33×1088.29×1091.18×1010
∆π9.36×10−49.78×10−48.36×10−46.14×10−121.30×10−14
gπ66.1384 66 .1383 66 .1377 66 .1374 66 .1374
0 5 10 15 20 25
Average Request Response Time [ms]4045505560Average Power Consumption [W]Maximum Batching
Static Batching ( b=16)
Static Batching ( b=8)Work-Conserving
SMDP Solution
Fig. 6. The tradeoff between request response time and power consumption.
0 5 10 15 20 25 30 35
Average Request Response Time [ms]253035404550Average Energy Efficiency [requests/J]Maximum Batching
Static Batching ( b=16)
Static Batching ( b=8)
Work-Conserving
SMDP Solution
Fig. 7. The tradeoff between request response time and energ y efﬁciency.
withb= 32 is a special case called maximum batching policy
in this setting where Bmax= 32 . The objectives are computed
using (14). It can be seen from Fig. 5 that the SMDP solution
achieves the best among all policies under various paramete r
settings. The work-conserving policy is far from optimal wh en
w2>5and the maximum batching policy works badly when
ρ= 0.1. The other two static batching policies can approach
the optimal policy under certain parameter scales.
By going through different w1,w2, the optimal policies of
the weighted-objective SMDP can balance latency and energy
ﬂexibly, where the pairs are formed into tradeoff curves as
shown in Fig. 6 and Fig. 7. The performance pairs of other
policies are separate points on the ﬁgures since they do not
change with the weights. The latency-energy pairs of the wor k-
conserving policy and maximum batching policy are near the
two end points of SMDP’s tradeoff curve, which agrees with
our observation in Fig. 4 (a) and (d). Several points of stati c
batching policies are close to SMDP’s tradeoff curve, meani ng
that these policies can well approximate the SMDP solutions32 50 100 150 200 250101102
10-1510-1010-5100
Fig. 8. The evolution of gπ(the average cost per unit time) and ∆π(the
average cost contributed by Soper unit time) regarding smax under different
co, wheresmax andcoare the parameters in ﬁnite state approximation.
under certain parameters, which agrees with the observatio n in
Fig. 5. In other cases, the advantage of SMDP-based policies
over static batching can be evidently observed. For example ,
static batching with b= 8 is above (below) the SMDP curve
in Fig. 6 (Fig. 7) when ρ= 0.7, which means that it consumes
more energy (has lower energy efﬁciency) than a SMDP-based
policy that has the same latency. And it does not even stabili ze
the system when ρ≥0.8. Similarly, static batching with b=
16has considerably longer latency when ρ= 0.9(see Fig. 6).
C. Approximation Accuracy and Complexity
We want to evaluate the accuracy and complexity of differ-
ent ﬁnite space approximations. As mentioned in section IV- A,
there are two parameters in the approximation: smax andco,
which control the dimension of the state space and the abstra ct
cost, respectively. Given the approximate model with certa in
smaxandco, a policy πis computed following the procedure in
section IV-B, IV-C, and the average cost per unit time gπcan
be obtained. The accuracy of approximation is evaluated by
∆π, which is the average cost contributed by Soper unit time.
The more accurate the approximation is, the less the So(as
the aggregation of the “tail” states) should contribute, an d thus
the smaller the ∆πis. We conduct experiments in the setting
ofρ= 0.9and[w1,w2] = [1,1]. The stopping parameters in
RVI are chosen as ǫ= 0.01anditermax= 10000 .
In Fig. 8, we exhibit the evolution of gπand∆πwithsmax
from 32 to 250. The gπwithco= 10000,1000,100decreases
and converges around smax= 35,50,70, whilegπwithco=
10,0increases and converges around smax= 170,200. We can
infer that the abstract cost with co= 10000,1000,100(10,0)
overestimates (underestimates) the impact of “tail” state s,
leading to the gπmostly larger (smaller) than the convergence
value. From the orange curves, we see that ∆πdecreases withsmax, and there exists a sharp drop for co= 10,0. It is because
the underestimated impact of “tail” states with co= 10,0leads
to an underestimated value for waiting ( a= 0) in the RHS
of (21). And the computed policy is to always wait, until the
smaxis large enough so that the cost of waiting is comparable
with the cost of serving. The ∆πis near stable when smax
exceeds 200, and it increases with cosince the µandyin (15)
are almost the same for all stable cases. Although the stable
∆πis no more than 10−13, we only need an approximation
acceptable with tolerance δ, and we choose δ= 0.001. We list
the minimum values of smax that satisfy the approximation
requirement in Table I. The ∆πandgπ, the number of RVI
iterations, as well as the space and time complexity that
correspond to the minimum smaxare also recorded. It can be
seen that all the ∆πare less than 0.001, and the differences
betweengπare also less than 0.001. The least required smax
is 70, which appears in the approximation with co= 100 .
Compared to the ordinary ﬁnite space approximation with
co= 0, the minimum smax decreases from 192 to 70, the
space complexity reduces by 63.5%, and the time complexity
reduces by 98% whenco= 100 . Furthermore, approximations
with larger (smaller) coshow an increasing trend in complexity
due to the growing overestimation (underestimation). In pr ac-
tice, we need to choose an adequate coto effectively reduce
the complexity.
VI. C ONCLUSION
In this paper, we studied the problem of dynamic batching
for machine learning inference on GPU-based platforms. The
problem is modeled as an inﬁnite state semi-Markov decision
process (SMDP) with the weighted sum of average response
time and average power consumption as the objective. The
SMDP is solved with ﬁnite state approximation, “discretiza -
tion” transformation and relative value iteration. The com puta-
tional complexity is largely reduced owning to the introduc tion
of an abstract cost. We take the GoogLeNet inference on
TESLA P4 as an example and conduct extensive numerical
experiments in various parameter settings. Numerical resu lts
have validated the superiority of the SMDP solution. Com-
pared to existing dynamic batching schemes, our proposed
solution is theoretically derived, rather than simulation tryouts.
As a result, our scheme can be computed ofﬂine and release the
system from the burden of extra complex modules. The limi-
tation is that our method only considers the average objecti ve,
instead of hard Service-Level Objective (SLO) constraints , for
which the proposed solution can function as a basic guidelin e
and needs to be modiﬁed while running in real time.
ACKNOWLEDGMENT
The paper is sponsored in part by the National Key R &D
Program of China No. 2020YFB1806605, by the National
Natural Science Foundation of China under Grants 62022049
and 62111530197, and by Hitachi.
REFERENCES
[1] K.-S. Oh and K. Jung, “GPU implementation of neural netwo rks,”
Pattern Recognition , vol. 37, no. 6, pp. 1311–1314, 2004.[2] “Google cloud prediction API documentation.”
https://cloud.google.com/prediction/docs/, 2017. (acc essed 19-Oct-
2022).
[3] C. Zhang, M. Yu, W. Wang, and F. Yan, “MArk: Exploiting clo ud
services for Cost-Effective, SLO-Aware machine learning i nference
serving,” in 2019 USENIX Annual Technical Conference (USENIX ATC
19), pp. 1049–1062, 2019.
[4] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonza lez, and
I. Stoica, “Clipper: A Low-Latency online prediction servi ng system,”
in14th USENIX Symposium on Networked Systems Design and Imple -
mentation (NSDI 17) , pp. 613–627, 2017.
[5] “NVIDIA AI inference platform technical overview.”
https://www.nvidia.com/en-us/data-center/resources/ inference-technical-overview/,
2018. (accessed 23-Nov-2019).
[6] P. Castro, V . Ishakian, V . Muthusamy, and A. Slominski, “ The rise of
serverless computing,” Communications of the ACM , vol. 62, no. 12,
pp. 44–54, 2019.
[7] A. Ali, R. Pinciroli, F. Yan, and E. Smirni, “BATCH: Machi ne learn-
ing inference serving on serverless platforms with adaptiv e batching,”
inSC20: International Conference for High Performance Compu ting,
Networking, Storage and Analysis , pp. 1–15, IEEE, 2020.
[8] Y . Choi, Y . Kim, and M. Rhu, “Lazy batching: An SLA-aware b atching
system for cloud machine learning inference,” in 2021 IEEE Interna-
tional Symposium on High-Performance Computer Architectu re (HPCA) ,
pp. 493–506, IEEE, 2021.
[9] W. Cui, Q. Chen, H. Zhao, M. Wei, X. Tang, and M. Guo, “E2bird:
Enhanced elastic batch for improving responsiveness and th roughput
of deep learning services,” IEEE Trans. Parallel Distrib. Syst. , vol. 32,
no. 6, pp. 1307–1321, 2020.
[10] F. Yan, O. Ruwase, Y . He, and E. Smirni, “SERF: Efﬁcient s cheduling
for fast deep neural network serving via judicious parallel ism,” in SC16:
Proceedings of the International Conference for High Perfo rmance
Computing, Networking, Storage and Analysis , pp. 300–311, 2016.
[11] C. Yao, W. Liu, W. Tang, and S. Hu, “EAIS: Energy-aware ad aptive
scheduling for CNN inference on high-performance GPUs,” Future
Generation Computer Systems , vol. 130, pp. 253–268, 2022.
[12] X. Lu, J. Yin, H. Chen, and X. Zhao, “An approach for burst y and
self-similar workload generation,” in International Conference on Web
Information Systems Engineering , pp. 347–360, Springer, 2013.
[13] Y . Inoue, “Queueing analysis of GPU-based inference se rvers with
dynamic batching: A closed-form characterization,” Performance Eval-
uation , vol. 147, p. 102183, 2021.
[14] R. K. Deb and R. F. Serfozo, “Optimal control of batch ser vice queues,”
Advances in Applied Probability , vol. 5, no. 2, pp. 340–361, 1973.
[15] K. P. Papadaki and W. B. Powell, “Exploiting structure i n adaptive dy-
namic programming algorithms for a stochastic batch servic e problem,”
European Journal of Operational Research , vol. 142, no. 1, pp. 108–127,
2002.
[16] J. W. Fowler and L. M¨ onch, “A survey of scheduling with p arallel
batch (p-batch) processing,” European Journal of Operational Research ,
vol. 298, pp. 1–24, Apr. 2022.
[17] Y . Inoue, “A load-balancing problem for distributed bu lk-service
queues with size-dependent batch processing times,” Queueing Systems ,
vol. 100, no. 3, pp. 449–451, 2022.
[18] W. Shi, S. Zhou, Z. Niu, M. Jiang, and L. Geng, “Multi-use r co-
inference with batch processing capable edge server,” IEEE Trans.
Wireless Commun. , pp. 1–1, 2022.
[19] J. Hanhirova, T. K¨ am¨ ar¨ ainen, S. Sepp¨ al¨ a, M. Siekk inen, V . Hirvisalo, and
A. Yl¨ a-J¨ a¨ aski, “Latency and throughput characterizati on of convolutional
neural networks for mobile computer vision,” in Proceedings of the 9th
ACM Multimedia Systems Conference , pp. 204–215, 2018.
[20] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Angue lov, D. Erhan,
V . Vanhoucke, and A. Rabinovich, “Going deeper with convolu tions,”
inProceedings of the IEEE conference on computer vision and pa ttern
recognition , pp. 1–9, 2015.
[21] J. D. Little, “A proof for the queuing formula: L = λw,”Operations
Research , vol. 9, no. 3, pp. 383–387, 1961.
[22] M. L. Puterman, Markov decision processes: discrete stochastic dynamic
programming . John Wiley & Sons, 2014.
[23] L. I. Sennott, “Average cost semi-Markov decision proc esses and the
control of queueing systems,” Probability in the Engineering and
Informational Sciences , vol. 3, no. 2, pp. 247–272, 1989.[24] L. Thomas and D. Stengos, “Finite state approximation a lgorithms for
average cost denumerable state Markov decision processes, ”Operations-
Research-Spektrum , vol. 7, no. 1, pp. 27–37, 1985.