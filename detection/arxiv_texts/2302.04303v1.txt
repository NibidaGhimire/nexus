Proceedings of Machine Learning Research 193:1{15, 2022 Machine Learning for Health (ML4H) 2022
Adapting Pre-trained Vision Transformers from 2D to 3D
through Weight Ination Improves Medical Image
Segmentation
Yuhui Zhang yuhuiz@stanford.edu
Shih-Cheng Huang mschuang@stanford.edu
Zhengping Zhou zpzhou@stanford.edu
Matthew P. Lungren mlungren@stanford.edu
Serena Yeung syyeung@stanford.edu
Stanford University, Stanford, CA 94305, USA
Abstract
Given the prevalence of 3D medical
imaging technologies such as MRI and
CT that are widely used in diagnos-
ing and treating diverse diseases, 3D
segmentation is one of the fundamen-
tal tasks of medical image analysis. Re-
cently, Transformer-based models have
started to achieve state-of-the-art per-
formances across many vision tasks,
through pre-training on large-scale nat-
ural image benchmark datasets. While
works on medical image analysis have
also begun to explore Transformer-
based models, there is currently no opti-
mal strategy to eectively leverage pre-
trained Transformers, primarily due to
the dierence in dimensionality between
2D natural images and 3D medical im-
ages. Existing solutions either split 3D
images into 2D slices and predict each
slice independently, thereby losing cru-
cial depth-wise information, or mod-
ify the Transformer architecture to sup-
port 3D inputs without leveraging pre-
trained weights. In this work, we use
a simple yet eective weight ination
strategy to adapt pre-trained Trans-
formers from 2D to 3D, retaining the
benet of both transfer learning and
depth information. We further investi-
gate the eectiveness of transfer from
dierent pre-training sources and ob-jectives. Our approach achieves state-
of-the-art performances across a broad
range of 3D medical image datasets, and
can become a standard strategy eas-
ily utilized by all work on Transformer-
based models for 3D medical images, to
maximize performance.1
Keywords: Medical Image Segmenta-
tion, Transfer Learning, CT, MRI.
1. Introduction
The increased utilization of medical imag-
ing technologies in recent years is causing a
dramatic increase in radiologists' daily work-
load (Smith-Bindman et al., 2008; Hendee
et al., 2010), leading to longer delays in diag-
nosis and higher misdiagnosis rates (Alonso-
Mart nez et al., 2010; Hendriksen et al.,
2017). Computer vision techniques such as
3D segmentation have the potential to alle-
viate the burden for radiologists by automat-
ing or assisting current image interpretation
workow, ultimately improving clinical care
and patient outcome (Esteva et al., 2021;
Huang et al., 2020).
Recently, the computer vision commu-
nity has witnessed a paradigm shift from
using CNNs to self-attention based Trans-
1. Codes are available at https://github.com/
yuhui-zh15/TransSeg .
Â©2022 Y. Zhang, S.-C. Huang, Z. Zhou, M.P. Lungren & S. Yeung.arXiv:2302.04303v1  [cs.CV]  8 Feb 2023Zhang Huang Zhou Lungren Yeung
Embedding Layer 2DVision Transformer
Aggregated3D SegmentationWindow InputCenter Slice Segmentations
EmbeddingLayerEncoderLayer
3D Medical Image3DVision TransformerEmbeddingLayerEncoderLayerUPerNetDecoder
Weight InflationWeightInflationforEmbeddingLayer(PyTorchcode):WeightShape:(Co,Ci,P,P)â†’(Co,Ci,P,P,K)â€¢Average Inflation: Wâ€™=W.unsqueeze(-1).repeat(1, 1, 1, 1, K)/Kâ€¢Centering Inflation:Wâ€™=torch.zeros((Co,Ci,P,P,K))Wâ€™[..., K // 2]=WNatural ImagesNatural VideosCTImagesDHWWHdWHWHD
Pre-train
Figure 1: Approach overview. Large-scale pre-trained Transformers are used as the encoder
in the segmentation model for transfer learning, in which weights are adapted
using the ination strategy to support 3D inputs. Each 3D image is split into
windows, which contain a small number of neighbor slices. Each window is fed
into the segmentation model and the segmentation of the center slice is predicted.
All the predicted slices are aggregated to form the nal 3D prediction.
former (Vaswani et al., 2017) architectures.
By pre-training at scale using supervised or
self-supervised learning, many vision Trans-
former models have achieved state-of-the-
art performances when ne-tuned for vari-
ous vision tasks, including image classica-
tion, object detection, and semantic segmen-
tation (Caron et al., 2021; Liu et al., 2021b;
Bao et al., 2021; Carion et al., 2020; Doso-
vitskiy et al., 2020).
However, while many recent works on
medical image analysis have started to ex-
plore Transformer-based models, it is still
unclear what is the optimal strategy to eec-
tively leverage Transformers for 3D medical
image segmentation, primarily due to the dif-
ference in dimensionality between 2D natural
images and 3D medical images. A straight-
forward approach adopted by most existing
works is to split 3D images into 2D slices
along the depth axis and independently seg-
ment each of them (Chen et al., 2021; Liu
et al., 2021a; Huang et al., 2021). How-
ever, this solution compromises the depth
information, which is crucial for identifyingsegmentation boundaries. On the contrary,
another line of research modies the Trans-
former architecture to support 3D inputs,
but the modication makes the pre-trained
weights not directly applicable (Xie et al.,
2021; Hatamizadeh et al., 2021).
In this work, we investigate strategies and
best practices for adapting Transformers for
3D medical images. We rst analyze the
importance of transfer learning and depth
information , and nd that both are criti-
cal for segmentation performance, especially
transfer learning. The model initialized from
pre-trained weights outperforms the same
randomly initialized model by 11.18% on a
multi-organ segmentation dataset, and the
randomly initialized 3D model outperforms
the randomly initialized 2D model by 6.07%.
To retain these two advantages, we use a sim-
ple yet eective weight ination strategy to
adapt pre-trained Transformers from 2D to
3D, which has been a standard approach in
video understanding to transfer from mod-
els pre-trained on images since it was rst
proposed in I3D (Carreira and Zisserman,
2Adapting Pre-trained Vision Transformers from 2D to 3D Improves Segmentation
2017). After careful ablations of dierent in-
ation and transfer settings, our best strat-
egy achieves 1.95% improvements over base-
lines with only a 0.75% increase in computa-
tional cost, and outperforms all the state-of-
the-art methods.
We further evaluate our method on 11 ad-
ditional datasets to understand the general-
izability of our best practice. Experiments
show that our approach consistently bene-
ts from weight ination and achieves many
state-of-the-art performances.
In summary, the major contribution of
our work to the medical image segmenta-
tion community is that we show how a sim-
ple yet eective weight ination strategy,
which is not currently used by state-of-the-
art medical image segmentation methods,
can lead to substantial improvements in per-
formance. Moreover, we also performed sys-
tematic ablations, which further provide in-
sights into the best strategies to adapt pre-
trained Transformers from 2D to 3D through
weight ination. Our method can become a
standard strategy utilized by all future works
on Transformer-based models for 3D medical
images to maximize performance.
Our best practice is summarized in Fig-
ure 1 and below:
â€¢Find a vision Transformer model pre-
trained on natural images via a combi-
nation of self-supervised and supervised
learning.
â€¢Adapt pre-trained weights in the em-
bedding layer using the centering ina-
tion strategy by transferring weights to
the center-most slice and initializing all
other weights to zero.
â€¢Parse 3D input into windows of a few
neighbor slices, and segment only the
center slice. Aggregate 3D segmentation
with predictions from all windows.2. Related Works
Vision Transformers. Originally pro-
posed for machine translation (Vaswani
et al., 2017), Transformer based architec-
tures have started to ourish in computer vi-
sion in recent years. ViT (Dosovitskiy et al.,
2020) split images into patches and create
patch embeddings as inputs to the Trans-
former model, which lays the foundation
to use Transformer architectures for visual
recognition. However, ViT requires more
training data to surpass convolutional neu-
ral networks. More recent works explored
dierent self-supervised pre-training strate-
gies to reduce the requirement of large-scale
labeled datasets. Caron et al. (Caron et al.,
2021) proposed DINO, a self-distillation con-
trastive framework. Drawing inspiration
from BERT in the NLP eld, Bao et al. (Bao
et al., 2021) proposed BEiT, which is pre-
trained to predict masked image patches
given context. These approaches achieve
state-of-the-art performances in image clas-
sication and semantic segmentation.
Several works have explored leveraging
Transformers for video inputs by utilizing
weights from pre-trained image models (Car-
reira and Zisserman, 2017). ViViT (Arnab
et al., 2021) and Video SwinTransformer (Liu
et al., 2021b) explored ination and center-
ing strategies to initialize weights of video
Transformers from pre-trained image Trans-
formers. In this work, we systematically in-
vestigate the benet of transfer learning from
these Transformer models under signicant
domain shifts, and compare the eectiveness
of transferring from dierent sources and pre-
training objectives.
Transformers for 3D Medical Im-
age Segmentation. Following successful
adaptations of Transformers in vision, some
works in medical image segmentation also
achieved state-of-the-art results by utilizing
Transformers. Earlier work such as Tran-
3Zhang Huang Zhou Lungren Yeung
sUNet (Chen et al., 2021) or CoTr (Xie et al.,
2021) still relied on convolutional layers for
the Transformer model. More recent meth-
ods, such as SwinUNet (Cao et al., 2021)
and UNETR (Xie et al., 2021), use a purely
Transformer-based encoder. To improve the
information ow of the segmentation frame-
work, dierent mechanisms such as context
bridge (Huang et al., 2021) or interleaved lay-
ers (Gao et al., 2021) are designed.
Based on their model input dimension,
these prior works can be grouped into two
categories: 3D approaches that develop 3D
model architectures directly predicting 3D
segmentation (Xie et al., 2021; Hatamizadeh
et al., 2021); and 2D approaches that split
3D images into 2D slices and stack 2D pre-
dictions to form the 3D segmentation (Chen
et al., 2021; Cao et al., 2021; Huang et al.,
2021).
3D segmentation benets from having
more depth-wise contextual information,
which is especially important for medical im-
ages where awareness of anatomical struc-
tures is crucial for identifying segmentation
boundaries. However, due to the limited
availability of pre-trained 3D Transformers,
all prior 3D approaches randomly initialized
their models and used no transfer learning.
On the contrary, 2D approaches can easily
utilize Transformer weights pre-trained on
natural images, but lack the spatial context
awareness of 3D models. In this work, we
attempt to address the shortcomings of ex-
isting approaches by combining the benet
of transfer learning of 2D approaches and
depth-wise information of 3D approaches.
3. Method
In this section, we rst introduce semantic
segmentation (Sec. 3.1) and vision Trans-
former (Sec. 3.2), and then present strategies
for adapting Transformers pre-trained on 2D
image to 3D medical image inputs (Sec. 3.3).3.1. Semantic Segmentation
Semantic segmentation is the task of assign-
ing a class label for each pixel (voxel for
3D) of an input image. Most prior works
useencoder-decoder frameworks for semantic
segmentation. The encoder progressively ex-
tracts higher-level features that capture the
input image's global information. Then the
decoder utilizes these features and progres-
sively reconstructs ne-grained pixel/voxel-
level predictions. Therefore, the quality of
the features extracted by the encoder di-
rectly impacts the segmentation quality.
CNNs have been the dominant choices of
the encoder in semantic segmentation mod-
els for years (Long et al., 2015; Ronneberger
et al., 2015). Recently, several studies have
reported improvements in performance for
both natural and medical image segmenta-
tion by using Transformers as encoders (Liu
et al., 2021a; Bao et al., 2021; Zheng et al.,
2021; Chen et al., 2021; Cao et al., 2021). In
this work, we compare dierent pre-trained
Transformers as encoders and study the most
eective way to perform transfer learning for
3D medical images.
3.2. Vision Transformer
Transformer was rst introduced in (Vaswani
et al., 2017) and adapted to visual inputs
in (Dosovitskiy et al., 2020). Unlike CNNs,
Transformer uses the self-attention mecha-
nism to aggregate information from the input
image, which can be viewed as more expres-
sive convolutions and can capture long-range
dependencies (Cordonnier et al., 2019). Em-
pirical results show that vision Transformers
achieve state-of-the-art performances across
many vision tasks when pre-trained on large-
scale natural image datasets (Dosovitskiy
et al., 2020; Liu et al., 2021a; Caron et al.,
2021; Bao et al., 2021).
Patch Partitioning and Embedding
Layer. Vision Transformers rst use a
4Adapting Pre-trained Vision Transformers from 2D to 3D Improves Segmentation
patch partitioning and embedding layer to
convert a visual input into patch embed-
dings to reduce input dimensionality. Specif-
ically, for a 2D image with HWpix-
els, the image is partitioned into multiple
smaller patches of PPpixels ( P2f16;32g
in practice). Similarly, 3D images of size
HWDcan be partitioned into patches
ofPPPvoxels. A linear layer is then
applied to each patch's attened pixel/voxel
values to generate input embeddings to the
Transformer encoder. By using patch par-
titioning, the number of inputs T=H
PW
P
(T=H
PW
PD
Pfor 3D) is much smaller than
the number of pixels/voxels, which makes the
computation tractable for subsequent Trans-
former encoder layers given the O(T2) com-
putational cost. Notably, the input patch
partitioning and embedding layer is equiva-
lent to strided convolution, where the kernel
size and stride size are both equal to PP
(PPPfor 3D). This is important as we
introduce weight ination strategies to adapt
Transformers from 2D to 3D in Sec 3.3.
Transformer Encoder. These patch em-
beddings H02RTDare then provided
as a sequence input into a Transformer en-
coder that generates contextualized repre-
sentations for each patch HL2RTD. The
encoder is constructed with a stack of L
multi-head self-attention layers, which facil-
itates the aggregation of information from
all the other input patches. After informa-
tion exchange from every layer in a sequen-
tial manner Hl= Layerl(Hl 1)2RTD,
the output vectors of the last layer HLare
used as contextualized representations for
the input patches. We refer to (Rush, 2018;
Vaswani et al., 2017) for a more detailed de-
scription of the Transformer model.
Variations of Vision Transformers.
While many vision Transformers' variations
exist, they all have similar architectures as
described above. In this work, we comparefour variations: DINO (Caron et al., 2021),
BEIT (Bao et al., 2021), SwinT (Liu et al.,
2021a), and VideoSwinT (Liu et al., 2021b).
DINO and BEiT have the same architectures
as the original vision Transformer (Dosovit-
skiy et al., 2020) and only dier in the self-
supervised pre-training objectives. SwinT
proposes shifted window attention and hier-
archical feature size shrinking to introduce
inductive bias of locality. VideoSwinT is an
adaptation of SwinT for video inputs. As
each variation has many size congurations,
we use all the base models with around 85M
parameters for fair comparisons.
3.3. Adapting Transformers from 2D
to 3D through Weight Ination
Many variations of Transformers that have
been pre-trained on existing large-scale
datasets are publicly available for transfer
learning (Caron et al., 2021; Bao et al., 2021;
Liu et al., 2021a,b). However, direct utiliza-
tion of these Transformers is non-trivial, due
to the dierence in the dimensionality be-
tween 2D natural images and 3D medical im-
ages. Most existing solutions either split 3D
images into 2D slices and predict each slice
independently, thereby losing crucial depth-
wise information (Chen et al., 2021; Cao
et al., 2021). Other solutions modify the
Transformer architecture to support 3D in-
puts, but the modication makes pre-trained
weights not directly applicable (Xie et al.,
2021; Hatamizadeh et al., 2021).
We combine both advantages inspired by
recent works in video understanding. Videos
are collections of 2D image frames organized
along the temporal axis, similar to medical
images where 2D slices are organized along
the depth axis. I3D (Carreira and Zisserman,
2017) proposed to transfer CNNs pre-trained
on 2D images to 3D video inputs by inating
the convolutional weights along the tempo-
ral axis. As the input patch partitioning and
5Zhang Huang Zhou Lungren Yeung
Table 1: Details of the twelve publicly available 3D medical image segmentation datasets we
used, including BCV, ACDC, and 10 MSD datasets . These datasets cover major
anatomical structures of the body and dierent modalities. We use the BCV
dataset to develop our methods and compare them with existing methods, and
use the remaining eleven datasets to test the generalizability of our best practices.
Data Modality Raw Reso. Input Reso. Clip Range Label Split
BCV CT 512 512126 5125125 [-175, 250] 13 18/12/0
ACDC CT 220 24710 5125125 [-175, 250] 3 70/10/20
Brain MRI 240 240155 2402405 N/A 3 387/72/25
Heart MRI 320 320114 3203205 N/A 1 16/3/1
Liver CT 512 512447 5125125 [-175, 250] 2 104/20/7
Hippocampus MRI 34 4936 2402405 N/A 2 208/39/13
Prostate MRI 320 32019 3203205 N/A 2 25/5/2
Lung CT 512 512282 5125125 [-175, 250] 1 50/9/4
Pancreas CT 512 51294 5125125 [-175, 250] 2 224/42/15
Vessel CT 512 51269 5125125 [-175, 250] 2 242/45/16
Spleen CT 512 51288 5125125 [-175, 250] 1 32/6/3
Colon CT 512 512110 5125125 [-175, 250] 1 100/19/7
embedding layer in Transformer is equivalent
to CNN, we adopt the weight ination strat-
egy to initialize the 3D Transformers.
We compare two ination strategies: av-
erage ination and centering ination . For
average ination, we copy the weights of the
input layer Ktimes in the depth axis and
divide them by K. This setting assumes in-
put slices are similar within a certain range
of depths, and the model treats all the input
slices equally at the beginning. For centering
ination, we transfer pre-trained weights for
the center-most slice and initialize all other
weights to zero. For this setting, the model
uses only information from the center slice
at the beginning and progressively learns to
contextualize information from all the neigh-
bor slices. Both strategies keep the mean and
variance of the input to Transformer encoder,
allowing more eective transfer learning.
Another dierence is in the input channel.
Transformers pre-trained on colored natural
images have three channels. Similarly, We re-
duce the channel to one by modifying the in-put layer weight, where we take the sum over
the input channel axis. We further use av-
erage ination in the input channel for med-
ical images with more than one input chan-
nel such as MRIs. These modications still
keep the mean and variance of the input un-
changed for the Transformer encoder. After
adapting the weight in the input patch par-
tition and embedding layer, the weights of
Transformer encoder layers can be directly
initialized from 2D pre-trained Transformers
since input shapes are aligned.
4. Results
4.1. Dataset
We use 12 publicly available 3D medi-
cal image datasets in this study: Beyond
the Cranial Vault (BCV) multi-organ seg-
mentation dataset (BCV), Automated Car-
diac Diagnosis Challenge (ACDC) (ACD),
and 10 datasets from Medical Segmenta-
tion Decathlon (MSD) (Antonelli et al.,
2021). These datasets cover major anatom-
6Adapting Pre-trained Vision Transformers from 2D to 3D Improves Segmentation
Table 2: Segmentation results on the BCV dataset. (a) Our approach achieves state-of-the-
art performances by combining the advantages of transfer learning (T) and depth
information (D). DSC is the Dice coecient averaged on the 8 major organs. (b)
Transfer eectiveness from models pre-trained with dierent sources and objec-
tives. NI, NV, and MI are natural images, natural videos, and medical images. SL
and SSL are supervised learning and self-supervised learning.
(a)Main Result
Method T D DSC
TransUNet (2021) 4 6 84.36
SwinUNet #(2021) 4 6 81.12
UNETR (2021) 6 4 80.07
Ours w/o T&D 6 6 74.00
Ours w/o T 6 4 74.70
Ours w/o D 4 6 85.18
Ours 4 4 87.13(b)Transfer Source and Objective
Encoder Source Objective DSC
BEiT-Random - - 74.70
CT-DINO MI (RSNACT) SSL 82.56
BEiT-SSL (2021) NI (IN22K) SSL 84.41
DINO (2021) NI (IN1K) SSL 84.60
SwinT (2021a) NI (IN22K) SL 85.57
VideoSwinT (2021b) NV (K600) SL 85.94
BEiT (2021) NI (IN22K) SSL+SL 87.13
ical structures of the body and dierent
modalities such as CT and MRI. We use the
same BCV training and validation set as ex-
isting works (Chen et al., 2021; Cao et al.,
2021) to develop our method and fairly com-
pare it with other methods. We use the re-
maining 11 datasets to test the generaliz-
ability of our best practice. Details of the
datasets such as labels, modality, resolution,
data split are available in Table 1.
4.2. Main Result
Most existing works such as Tran-
sUNet (Chen et al., 2021) and Swin-
UNet (Cao et al., 2021) that use Transform-
ers for 3D medical image segmentation are
slice-based approaches, which split a 3D im-
age into 2D slices and predict segmentation
independently on each slice. This approach
enables the direct utilization of pre-trained
weights for transfer learning, but none of
these works study the eect of transfer
learning by comparing their methods with
randomly initialized encoders. We nd that
transfer learning signicantly boosts thesegmentation performance even under sig-
nicant domain shifts . Using the pre-trained
encoder leads to 11.18% improvements in
DSC as compared to the same randomly
initialized encoder (Table 2(a))2.
Recent works such as CoTr (Xie et al.,
2021) or UNETR (Hatamizadeh et al., 2021)
modify the Transformer architecture to sup-
port 3D inputs to leverage depth informa-
tion. However, they all use randomly ini-
tialized 3D Transformers as encoders, which
sacrices the clear benet of transfer learn-
ing as discussed above. Due to the dierent
amount of training data and number of la-
bels reported in their papers3, we reproduced
UNETR using its ocially released codes for
a fair comparison. From Table 2, we can see
UNETR underperforms TransUNet or Swi-
2. SwinUNet#indicates an input resolution of 384,
while other models use an input resolution of 512.
TransUNet and SwinUNet results are from their
original papers, while other results are from our
experiments.
3. CoTr use 21 training data from the BCV dataset
and reported DSC based on 11 organs, while UN-
ETR uses 28 + external training data and the
result is based on an ensemble of four models.
7Zhang Huang Zhou Lungren Yeung
Table 3: Fine-grained segmentation results of dierent organs on the BCV dataset. We re-
port performances on Aorta, Gallbladder, Kidney (Left), Kidney (Right), Liver,
Pancreas, Spleen, and Stomach, respectively. The most signicant improvements
of our method are Gallbladder, Pancreas, and Stomach segmentation, which can
be interpreted as the larger the variation between neighbor slices, the more im-
provement from incorporating depth information through weight ination.
Method Aor Gal KidL KidR Liv Pan Spl Sto
TransUNett (2021) 90.68 71.99 86.04 83.71 95.54 73.96 88.80 84.20
SwinUNet #(2021) 87.07 70.53 84.64 82.87 94.72 63.73 90.14 75.29
UNETR (2021) 86.30 63.79 84.94 83.93 95.94 58.01 88.95 78.76
Ours w/o T&D 82.71 61.34 80.73 76.25 94.37 46.52 82.62 67.46
Ours w/o T 81.26 63.78 80.90 75.41 94.82 53.67 82.15 65.64
Ours w/o D 90.22 67.51 93.84 91.40 96.27 66.62 94.22 81.33
Ours 90.29 75.20 93.70 91.40 96.17 71.82 93.81 84.62
# Voxels 441936 118445 887310 879180 9762041 504539 1386739 2641170
# Slices 827 131 362 350 631 316 332 421
Slice Variation 90.33 69.84 82.80 82.57 88.93 75.91 83.37 84.94
nUNet, but it achieves 6.07% improvements
in DSC compared to the randomly initialized
slice-based model, demonstrating the impor-
tance of depth information .
To combine the advantages of transfer
learning and depth information , we use
the weight ination strategy introduced in
Sec. 3.3 to initialize 3D Transformers. We
observe consistent improvements with this
initialization over random, but we nd that
the weight ination depth should not be
too large (Sec. 4.4). Our best model uses
a window-based approach. Specically, we
split 3D images into small windows along the
depth axis, where each window consists of a
few (e.g., 5) neighbor slices. We aggregate
all the window predictions to form the nal
prediction. By incorporating shallow depth
information, our method achieves 1.95% im-
provements in DSC compared to slice-based
methods . More importantly, such improve-
ment is at only 0.75% increased computa-
tional cost (213.3 GFLOPS before adapta-
tion vs. 215.0 after).We also evaluate our method stability by
running our model and baselines on the BCV
dataset three times with three random seeds
(1234, 5678, 910). Our model (i.e., Ours
in Table 2(a)) achieves 86.65 0.35 (87.13,
86.52, 86.29 for three runs) DSC, while the
model without weight ination (i.e., Ours
w/o D in Table 2(a)) only achieves 85.14 
0.20 (85.18, 84.88, 85.36 for three runs) DSC.
The results indicate a 98% level of signi-
cance using the paired t-test.
To better evaluate the improvements of
our method, we extend Table 2 and show the
DSC results for each of the 8 organs in Ta-
ble 3. We nd that the most signicant im-
provements from weight ination are Gall-
bladder, Pancreas, and Stomach segmenta-
tion. We compute dierent statistics of each
organ to understand the reason, including
the number of voxels, the number of occur-
ring slices, and the variation between slices
(measured by the average DSC between la-
bels of neighbor slices). We nd that the
larger the variation between neighbor slices,
8Adapting Pre-trained Vision Transformers from 2D to 3D Improves Segmentation
Input Label Pred Pred w/o D Pred w/o T&D
Figure 2: Transfer learning reduces noise in local predictions, and depth information further
improves prediction consistency across slices. We show ve consecutive input
images, ground-truth labels, predictions from Ours ,Ours w/o D ,Ours w/o T&D .
the more improvements from incorporating
depth information through weight ination .
This can be interpreted as inter-slice predic-
tion consistencies are signicantly improved
with depth information based on the qualita-
tive result shown in Figure 2.
4.3. Pre-training Source and
Objective
Since pre-training sources may signicantly
impact the transfer performance, we com-pare the transfer eectiveness of various vi-
sion Transformers pre-trained with dier-
ent sources (natural images, medical images,
natural videos) and objectives (supervised
learning, self-supervised learning, combina-
tion of both) (Table 2(b)). To fairly compare
models, we carefully select same-size mod-
els with similar computations. SwinT (Liu
et al., 2021a) is pre-trained on natural im-
age dataset ImageNet-22K (Deng et al.,
2009) via supervised learning. DINO (Caron
9Zhang Huang Zhou Lungren Yeung
Table 4: Best practice of weight ination on the BCV dataset. Initializing the model with
the centering ination strategy and only predicting the center slice's segmentation
from a small number of neighbor slices leads to the best performance.
(a)Prediction Target
Prediction All Center
DSC 73.93 76.33(b)Weight Initialization
Initialization Random Average Centering
DSC 72.49 76.33 84.30
(c)Input Stride
Input Stride 1 2 3
DSC 84.74 84.30 77.68(d)Input Slices
Input Slices 1 5 11 21
DSC 83.34 84.74 83.83 77.52
et al., 2021) is pre-trained on ImageNet-
1K via self-supervised contrastive learning.
BEiT-SSL (Bao et al., 2021) is pre-trained
on ImageNet-22K via self-supervised masked
image modeling, and BEiT is further ne-
tuned on the same dataset via supervised
learning. VideoSwinT (Liu et al., 2021b)
is pre-trained on natural video dataset
Kinetics-600 (Carreira et al., 2018). We
also pre-trained a CT-DINO on the medi-
cal image dataset RSNA-CT (Colak et al.,
2021) using the same model and objective as
DINO.
Interestingly, we nd that models pre-
trained on medical images performs worse
than models pre-trained on natural images
if both models are pre-trained using self-
supervised learning . This might be at-
tributed to the high inter-class similarity for
medical images due to the nature of hu-
man anatomy, causing limitations in learn-
ing meaningful representations using self-
supervised learning. Furthermore, medical
image datasets are much smaller than natu-
ral image datasets. Designing eective self-
supervised learning methods in medical im-
ages will be meaningful future work. More-
over, we nd that additional supervision
from image labels or video labels further im-
proved the transfer performance , as models
pre-trained via supervised learning generallyachieved better performances than models
pre-trained via self-supervised learning.
Since our goal is to explore the best
strategy to leverage existing large-scale pre-
trained 2D vision Transformers given their
strong adaptability to various tasks and pub-
lic availability, we did not perform addi-
tional pre-training. Thoroughly exploring
pre-training on dierent sources with dier-
ent objectives and data amounts is a mean-
ingful future direction but requires extensive
computational resources and costs.
4.4. Best Practice of Weight Ination
We conduct ablations on dierent ination
settings, including prediction target, weight
initialization, stride, and the number of in-
put slices (Table 4)4. We nd that initial-
izing the model with the centering ination
strategy and only predicting the center slice's
segmentation leads to the best performance .
This setting is similar to residual connec-
tion (He et al., 2016), where the model grad-
ually learns to utilize additional depth in-
formation, and it should not be worse than
the model without depth information. More-
over, we nd that including a few neighbor
slices is enough , because neighbor slices are
more similar to the center slice than non-
4. Here DSC is averaged on all the 14 labels instead
of 8 major organs in Table 2.
10Adapting Pre-trained Vision Transformers from 2D to 3D Improves Segmentation
Table 5: Generalizability of best practice. To verify the generalization of our best practice
that adapts Transformers from 2D to 3D, we compare our adapted Transformers
with original Transformers on 11 additional datasets and observe improvements
over 10 datasets. DSC excluding background on the test set is reported. We also
include other baseline performances for reference, but numbers are not fully com-
parable due to no standardized data split available for these datasets (+ indicates
same data split ratio but not same split; * indicates dierent data split ratios).
Method ACDC Bra Hea Liv Hip Pro Lun Pan Ves Spl Col
SwinUNet+(2021) 90.00 - - - - - - - - - -
UNETR+(2021) - 71.1 - - - - - - - 96.4 -
nnUNet(2021) 91.79 74.11 93.28 79.71 88.91 75.37 72.11 67.45 68.37 96.38 45.53
Ours w/o D 92.62 79.30 91.22 87.24 86.24 74.29 62.01 58.60 67.95 95.12 46.28
Ours 92.69 80.13 90.78 87.67 86.83 75.20 70.20 59.94 70.42 95.56 51.70
neighbor slices are and provide more infor-
mation for the model to predict accurate seg-
mentation. Including too many slices may
lead to overtting. This nding also applies
to VideoSwinT, where we observe 81.10 and
81.83 DSC for 9 and 5 slices, respectively.
4.5. Generalization of Best Practice
To verify the generalizability of our best
practice | adapting pre-trained vision
Transformers from 2D to 3D using the strate-
gies in Table 4, we train and evaluate our
method on 11 additional 3D medical image
datasets of dierent anatomical regions and
imaging modalities. From Table 5, we ob-
serve that weight-inated vision Transform-
ers (Ours) achieve consistently better perfor-
mances on 10 of 11 datasets compared to the
original Transformers (Ours w/o D) , which
clearly demonstrates the eectiveness of our
proposed method . Also, these results are ob-
tained with just a single set of hyperparame-
ters tuned on the BCV dataset, showing the
robustness of our method .
Since our model uses all the hyperparam-
eters tuned on the BCV dataset, continu-
ing tuning hyperparameters on each dataset
should lead to further improvements. For
example, since the hyperparameters of ourmethod are tuned on the CT dataset, the op-
timal hyperparameters may be very dierent
for MRI datasets, thus MRI datasets such as
Bra, Hea, Hip, and Pro show smaller gains
than other CT datasets. However, since the
main purpose of our paper is to show the im-
portance of weight ination, not to achieve
dierent state-of-the-arts, we leave the rela-
tion between data characteristics and opti-
mal hyperparameters to future work.
5. Conclusion
In this work, we investigated adapting pre-
trained Transformers for 3D medical image
segmentation via simple yet eective weight
ination strategies. Our approach achieved
consistent improvements on 12 datasets with
only 0.75% increased computational cost,
which can become a standard strategy eas-
ily utilized by all work on Transformer-based
models for 3D medical images, to maximize
performance.
Acknowledgments
We thank all the reviewers for their construc-
tive feedback. This work is partially sup-
ported by the Stanford Center for Articial
Intelligence in Medicine & Imaging (AIMI).
11Zhang Huang Zhou Lungren Yeung
References
Automated cardiac diagnosis chal-
lenge (acdc). https://www.creatis.
insa-lyon.fr/Challenge/acdc/ .
Beyond the cranial vault (bcv)
multi-organ segmentation. https:
//www.synapse.org/#!Synapse:
syn3193805/wiki/217789 .
Jos e Luis Alonso-Mart nez, FJ Anniccherico
S anchez, and MA Urbieta Echezarreta.
Delay and misdiagnosis in sub-massive and
non-massive acute pulmonary embolism.
European Journal of Internal Medicine ,
2010.
Michela Antonelli, Annika Reinke, Spyri-
don Bakas, Keyvan Farahani, Bennett A
Landman, Geert Litjens, Bjoern Menze,
Olaf Ronneberger, Ronald M Summers,
Bram van Ginneken, et al. The medical
segmentation decathlon. arXiv preprint
arXiv:2106.05735 , 2021.
Anurag Arnab, Mostafa Dehghani, Georg
Heigold, Chen Sun, Mario Lu ci c, and
Cordelia Schmid. Vivit: A video
vision transformer. arXiv preprint
arXiv:2103.15691 , 2021.
Hangbo Bao, Li Dong, and Furu Wei. Beit:
Bert pre-training of image transformers.
arXiv preprint arXiv:2106.08254 , 2021.
Hu Cao, Yueyue Wang, Joy Chen, Dong-
sheng Jiang, Xiaopeng Zhang, Qi Tian,
and Manning Wang. Swin-unet: Unet-
like pure transformer for medical im-
age segmentation. arXiv preprint
arXiv:2105.05537 , 2021.
Nicolas Carion, Francisco Massa, Gabriel
Synnaeve, Nicolas Usunier, Alexander Kir-
illov, and Sergey Zagoruyko. End-to-end
object detection with transformers. In
ECCV , 2020.Mathilde Caron, Hugo Touvron, Ishan
Misra, Herv e J egou, Julien Mairal, Piotr
Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision
transformers. In ICCV , 2021.
Joao Carreira and Andrew Zisserman. Quo
vadis, action recognition? a new model
and the kinetics dataset. In CVPR , 2017.
Joao Carreira, Eric Noland, Andras Banki-
Horvath, Chloe Hillier, and Andrew Zis-
serman. A short note about kinetics-600.
arXiv preprint arXiv:1808.01340 , 2018.
Jieneng Chen, Yongyi Lu, Qihang Yu, Xi-
angde Luo, Ehsan Adeli, Yan Wang,
Le Lu, Alan L Yuille, and Yuyin Zhou.
Transunet: Transformers make strong en-
coders for medical image segmentation.
arXiv preprint arXiv:2102.04306 , 2021.
Errol Colak, Felipe C Kitamura, Stephen B
Hobbs, Carol C Wu, Matthew P Lun-
gren, Luciano M Prevedello, Jayashree
Kalpathy-Cramer, Robyn L Ball, George
Shih, Anouk Stein, et al. The rsna pul-
monary embolism ct dataset. Radiology:
Articial Intelligence , 2021.
Jean-Baptiste Cordonnier, Andreas Loukas,
and Martin Jaggi. On the relationship be-
tween self-attention and convolutional lay-
ers. In ICLR , 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia
Li, Kai Li, and Li Fei-Fei. Imagenet: A
large-scale hierarchical image database. In
CVPR , 2009.
Alexey Dosovitskiy, Lucas Beyer, Alexan-
der Kolesnikov, Dirk Weissenborn, Xiao-
hua Zhai, Thomas Unterthiner, Mostafa
Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for im-
age recognition at scale. In ICLR , 2020.
12Adapting Pre-trained Vision Transformers from 2D to 3D Improves Segmentation
Andre Esteva, Katherine Chou, Serena Ye-
ung, Nikhil Naik, Ali Madani, Ali Mot-
taghi, Yun Liu, Eric Topol, Je Dean, and
Richard Socher. Deep learning-enabled
medical computer vision. NPJ digital
medicine , 2021.
Yunhe Gao, Mu Zhou, and Dimitris N
Metaxas. Utnet: a hybrid transformer ar-
chitecture for medical image segmentation.
InMICCAI , 2021.
Ali Hatamizadeh, Dong Yang, Holger Roth,
and Daguang Xu. Unetr: Transformers
for 3d medical image segmentation. arXiv
preprint arXiv:2103.10504 , 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren,
and Jian Sun. Identity mappings in deep
residual networks. In ECCV , 2016.
William R Hendee, Gary J Becker, James P
Borgstede, Jennifer Bosma, William J
Casarella, Beth A Erickson, C Douglas
Maynard, James H Thrall, and Paul E
Wallner. Addressing overutilization in
medical imaging. Radiology , 2010.
Janneke MT Hendriksen, Marleen Koster-
van Ree, Marcus J Morgenstern, Ruud
Oudega, Roger EG Schutgens, Karel GM
Moons, and Geert-Jan Geersing. Clinical
characteristics associated with diagnostic
delay of pulmonary embolism in primary
care: a retrospective observational study.
BMJ Open , 2017.
Shih-Cheng Huang, Anuj Pareek, Saeed
Seyyedi, Imon Banerjee, and Matthew P
Lungren. Fusion of medical imaging and
electronic health records using deep learn-
ing: a systematic review and implemen-
tation guidelines. NPJ digital medicine ,
2020.
Xiaohong Huang, Zhifang Deng, Dandan Li,
and Xueguang Yuan. Missformer: An ef-
fective medical image segmentation trans-former. arXiv preprint arXiv:2109.07162 ,
2021.
Fabian Isensee, Paul F Jaeger, Simon AA
Kohl, Jens Petersen, and Klaus H Maier-
Hein. nnu-net: a self-conguring method
for deep learning-based biomedical image
segmentation. Nature methods , 2021.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yix-
uan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo. Swin transformer: Hierar-
chical vision transformer using shifted win-
dows. arXiv preprint arXiv:2103.14030 ,
2021a.
Ze Liu, Jia Ning, Yue Cao, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Han Hu.
Video swin transformer. arXiv preprint
arXiv:2106.13230 , 2021b.
Jonathan Long, Evan Shelhamer, and Trevor
Darrell. Fully convolutional networks for
semantic segmentation. In CVPR , 2015.
Olaf Ronneberger, Philipp Fischer, and
Thomas Brox. U-net: Convolutional net-
works for biomedical image segmentation.
InMICCAI , 2015.
Alexander M Rush. The annotated trans-
former. In ACL NLP-OSS Workshop ,
2018.
Rebecca Smith-Bindman, Diana L
Miglioretti, and Eric B Larson. Ris-
ing use of diagnostic medical imaging in
a large integrated health system. Health
aairs , 2008.
Ashish Vaswani, Noam Shazeer, Niki Par-
mar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez,  Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In
NeurIPS , 2017.
Yutong Xie, Jianpeng Zhang, Chunhua Shen,
and Yong Xia. Cotr: Eciently bridg-
13Zhang Huang Zhou Lungren Yeung
ing cnn and transformer for 3d medi-
cal image segmentation. arXiv preprint
arXiv:2103.03024 , 2021.
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao,
Xiatian Zhu, Zekun Luo, Yabiao Wang,
Yanwei Fu, Jianfeng Feng, Tao Xiang,
Philip HS Torr, et al. Rethinking semantic
segmentation from a sequence-to-sequence
perspective with transformers. In CVPR ,
2021.
14Adapting Pre-trained Vision Transformers from 2D to 3D Improves Segmentation
Table 6: Details of the experimental settings . We do not perform heavy hyperparameter
tuning to ensure the generalizability of our best practices. The ensemble is not
used, which can further improve performances. *For Brain, Vessel, and Pancreas,
we train each model with 250,000 steps given their larger training set. We provide
all the codes, including data pre-processing, data loading, model training, and
evaluation at https://github.com/yuhui-zh15/TransSeg
Hyperparam Value Hyperparam Value
Batch Size 16 Patch Size 16 165
Loss Function DiceFocal Optimizer AdamW
Learning Rate 3e-5 Weight Decay 5e-2
Scheduler Slanted Triangular Warm-up Steps 20
Step 25,000* Hyperparam Tuning No
GPUs 8 Titan RTX Time 8 Hours
Ensemble No multi-model, multi-view, multi-scale ensemble
Train Data Random Zoom ([0 :5;2]), Random Crop (if Zoom >1),
Normalize Intensity ([  175;250] (CT) or [0 ;MAX] (MRI)![ 1;1]),
Random Flip ( p= 0:1), Random Rotation ( p= 0:1),
Random Shift Intensity ([  0:1;+0:1] with p= 0:5), Pad (if Zoom <1)
Inference Data Normalize Intensity (same as training)
15