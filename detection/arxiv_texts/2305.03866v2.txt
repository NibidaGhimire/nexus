   
 
   
 Spiking neural networks  with Hebbian  plasticity  
for unsupervised  representation  learning  
Naresh Ravichandran1, Anders Lansner1,2, Pawel Herman1,3 * 
1Â­ Division of Computational Science and Technology,  School of Electrical Engineering and 
Computer Science, KTH Royal Institute of Technology,  Sweden   
2Â­ Department of Mathematics  Â­ Stockholm University , Sweden  
3- Digital Futures, KTH Royal Institute of Technology , Sweden  
Abstract . We introduce a n ovel spiking neural network  model  for learning  
distributed internal representations  from data  in an unsupervised procedure . We 
achieved  this by transforming the non-spiking feedforward Bayesian 
Confidence Propagation Neural Network (BCPNN)  model , employing  an 
online  correlation -based  Hebbian -Bayesian learning and rewiring mechanism , 
shown previously to perform representation learning , into a spiking neural 
network with  Poisson statistics and  low firing rate comparable  to in vivo  
cortical pyramidal neurons . We evaluated  the representations  learned by our 
spiking model  using a linear  classifier  and show performance  close to the non-
spiking BCPNN , and competitive with other Hebbian -based spiking networks 
when trained on  MNIST and F -MNIST machine learning  benchmarks.  
1 Introduction  
The success of deep learning (DL) in solving various real -world pattern recognition 
benchmarks  has shown the importance of building large -scale artificial  neural networks  
(ANNs)  with the ability  to learn  distributed  intern al representations from real-world  
data. One of the emerging concerns however is the energy footprint of heavy 
computations involved in training large ANN architectures . In response to this 
challenge  there has been growing interest in neuromorphic approach es that build on 
more biologically plausible spiking neural networks (SNNs ). This new generation of 
neural network models holds  a promise for energy -efficient neuromorphic hardware  
that can handle  real-time data streams efficiently  with sparse and asynchronous event -
based communication [1]. It is therefore  imperative  that, in parallel to DL  development , 
we develop SNNs that can learn representations  from real -world data. Building such 
SNN models  has been  typically addressed either  by converting a  traditional  non-spiking 
ANN  trained with gradient descent learning  into a SNN , or by modifying backprop -
based  gradient descent algorithms  to accommodate spiking  neurons  [1,2]. Since the 
 
* This work is funded by the Swedish e -Science Research Centre (SeRC), VetenskapsrÃ¥det (Swedish 
Research Council) Grants No. 2018 -05360 and 201 8-07079 (Indo -Swedish joint network grant 2018). 
Simulations were performed at the PDC Center for High Performance Computing, KTH Royal Institute of 
Technology and at Vega at the Institute of Information Science (Slovenia). We would like to thank Jing 
Gong , Artem  Zhmurov, and Lilit Axner (ENCCS) and Martin Rehn (KTH) for collaborating and developing 
the code with GPU acceleration.     
 
   
 current approaches do not fully leverage the biological nature  of the learning principles 
in SNNs, there is a motivated concern that full potential of SNNs and their 
neuromorphic implementations  may not be harnessed.  
 
Our philosophy  for SNN design  is steeped into  the biological brainâ€™s inspirations  and 
hence  we aim  to develop a  biologically  constrained SNN  model that performs  
unsupervised representation learning based on Hebbian learning  principles.  For this , 
we derive our model from an abstract (non -spiking) brain -like BCPNN architecture, 
previously  shown to  perform representation learning by solely using Hebbian  learning  
(synaptic plasticity ) and Hebbian rewiring  (structural plasticity)  mechanism s [5]. 
Crucially, we employ  on-line Hebbian learning  directly  in the spiking domain. To this 
end, we interpret spikes as stochastic independent samples from a Poisson distribution , 
where  the underlying firing rates are  computed  as probabilities  from the BCPNN 
model.  This is motivated from the observations that in vivo  cortical pyramidal  neurons 
show reliable firing rate whereas  the timing of spikes is highly irregular and  the 
corresponding inter -spike intervals  closely resembles a Poisson distribution [3,4].  Our 
main contribution is to show that the BCPNN  model can be converted to a SNN  
preserving the biological  details with minimal compromise on performance.  The 
spiking statistics in our model reach a maximum firing rate of around 50 spikes/s , 
matching the sparse firing  of in vivo  cortical pyramidal neurons.  We evaluat ed the 
internal representation of the model by means of  a linear classifier  and compared it  with 
the corresponding non-spiking  model  as well as other SNNs with Hebbian learning .  
2 Model description  
We summarize key details of the model r elevant to the spiking version  and refer to 
previous work s on the feedforward non-spiking BCPNN model for full details [ 5].  
 
Modular network design : Our spiking BCPNN model consists  of one spiking input 
layer and one spiking hidden layer. The layer archit ecture is derived  from the columnar 
organization of the neocortex . Each  layer in our network  model  is composed o f many 
identical hypercolumns modules , each of which  in turn comprises  many  neuron units 
(referred to as minicolumn s) sharing the same receptive field . 
 
Localized learning : The learning mechanism is local, online, and correla tion-based 
Hebbian -Bayesian synaptic plasticity where  each synapse accumulates  short and long -
term traces of pre -, post -, and joint activities . From the  pre- and post-synaptic spikes  at 
time ğ‘¡, ğ‘†ğ‘–,ğ‘†ğ‘—âˆˆ{0,1}, we compute ğ‘-traces , ğ‘ğ‘– and ğ‘ğ‘—, as a form of short -term filtering 
(ğœğ‘§ ~ few milliseconds ) providing a coincidence detection window between pre - and 
post-synaptic spikes for subsequent  LTP/LTD induc tion (Eq. 1). The ğ‘-traces are 
further transformed  into ğ‘ƒ-traces, ğ‘ƒğ‘–,ğ‘ƒğ‘—, and ğ‘ƒğ‘–ğ‘—, with long  time-constants (ğœğ‘ ~ seconds 
to hours)  reflecting LTP/LTD synaptic processes  (Eq. 2). The ğ‘ƒ-traces are finally 
transformed to bias and weight parameter  of the synapse  corresponding to terms in    
 
   
 ANNs  (Eq. 3). All the spike and trace variables are time dependent  (time index is 
dropped for the notation brevity) . 
ğœğ‘§ğ‘‘ğ‘ğ‘–
ğ‘‘ğ‘¡=ğœğ‘§
âˆ†ğ‘¡ğ‘†ğ‘–âˆ’ğ‘ğ‘–, ğœğ‘§ğ‘‘ğ‘ğ‘—
ğ‘‘ğ‘¡=ğœğ‘§
âˆ†ğ‘¡ğ‘†ğ‘—âˆ’ğ‘ğ‘—, (1) 
ğœğ‘ ğ‘‘ğ‘ƒğ‘–
ğ‘‘ğ‘¡=ğ‘ğ‘–âˆ’ğ‘ƒğ‘–, ğœğ‘ ğ‘‘ğ‘ƒğ‘–ğ‘—
ğ‘‘ğ‘¡=ğ‘ğ‘– ğ‘ğ‘—âˆ’ğ‘ƒğ‘–ğ‘—, ğœğ‘ ğ‘‘ğ‘ƒğ‘—
ğ‘‘ğ‘¡=ğ‘ğ‘—âˆ’ğ‘ƒğ‘—, (2) 
ğ‘ğ‘—=log ğ‘ƒğ‘—, ğ‘¤ğ‘–ğ‘—=log ğ‘ƒğ‘–ğ‘—
ğ‘ƒğ‘– ğ‘ƒğ‘—, (3) 
Localized rewiring : The synaptic rewiring mechanism adaptively finds efficient 
sparse connectivity between the layers, mimicking structural plasticity in the brain [ 5].  
This mechanism uses the ğ‘ƒ-traces locally available at each synapse to maximize  a 
â€œusageâ€  score and  updat es the sparse binary connectivity matrix ğ‘ğ‘–ğ‘— accordingly . 
 
Neuron al activation : The total input ğ¼ğ‘— for neuron ğ‘— is updated  to be weighted sum of 
incoming  spikes  with the time -constant ğœğ‘§ (acting here as the depolarization time 
constant ) (Eq. 4).  The activation  of the neuron, ğœ‹ğ‘—, is computed as a softmax function 
of the  input  ğ¼ğ‘— (Eq. 5) , which  induces a soft -winner -takes -all competition between the 
minicolumn units within each hypercolumn module.  The output of the softmax function 
reflects the posterior belief probability of the minicolumn unit according to the BCPNN 
formalism  [5]. In the non -spiking  (rate-based) BCPNN model, this activation ğœ‹ğ‘— acts as 
the firing rate and can be directly communicated  as the neuronal signal . For SNNs, w e 
independently sample binary values from this ğœ‹ğ‘— activation probability  scaled by the 
maximum firing rate ğ‘“ğ‘šğ‘ğ‘¥ for each time step (Eq. 6 ). Note that when ğ‘“ğ‘šğ‘ğ‘¥ = 1000  
spikes/s  (and âˆ†ğ‘¡ = 1ms) , the spike generation process from Eq. 6 is simply a stochastic 
sampling of the underlying firing rate probability and  setting  ğ‘“ğ‘šğ‘ğ‘¥ <1/âˆ†ğ‘¡ linearly 
scales the activation probability  to the maximum firing rate.  Also, i n both learning (Eq. 
1) and synaptic integration  (Eq. 4) steps, we scaled the binary spiking signal by  ğœğ‘§âˆ†ğ‘¡â„ 
as this renders the  filtered spike statistics of model to be equivalent to  the rate model . 
ğœğ‘§ğ‘‘ğ¼ğ‘—
ğ‘‘ğ‘¡=ğ‘ğ‘—+ğœğ‘§
âˆ†ğ‘¡âˆ‘ ğ‘†ğ‘– ğ‘¤ğ‘–ğ‘— ğ‘ğ‘–ğ‘—ğ‘ğ‘–
ğ‘–=0âˆ’ğ¼ğ‘—, (4) 
 
ğœ‹ğ‘—= exp (ğ¼ğ‘—)
âˆ‘ exp (ğ¼ğ‘˜)ğ‘€â„
ğ‘˜=1, (5) 
 
ğ‘†ğ‘— ~ ğ‘ƒ(spike  between  t and t+âˆ†ğ‘¡)=ğœ‹ğ‘— ğ‘“ğ‘šğ‘ğ‘¥ âˆ†ğ‘¡ (6) 
3 Experiments  
3.1 Comparison of classification performance  
To benchmark our spiking BCPNN model  on the MNIST (hand -written digit images) 
and F -MNIST (fashion apparel images)  datasets, we first trained it in a purely    
 
   
 unsupervised manner (representation learning) and then used a linear classifier  (cross 
entropy loss, SGD with Adam optimizer, 100 training epochs) to predict class labels ( ğ‘› 
= 3 randomized runs , all parameters are listed in Table 1 ). Table 2  shows that the 
classification accuracy of our model is competitive with the non-spiking  BCPNN as 
well as other SNNs with Hebbian -like plasticity (STDP  and its variants ). 
 
Type  Parameter  Value  Description  
Synaptic  ğœğ‘§ 20 ms  Short -term filtering time constant  
ğœğ‘ 5 s Long -term learning time constant  
ğ‘ğ‘ğ‘œğ‘›ğ‘› 10 %  Sparse connectivity between layers  
Neuronal  ğ»ğ‘–,ğ‘€ğ‘– 784, 2  N:o input layer hypercolumns & minicolumns  
ğ»â„,ğ‘€â„ 100, 100  N:o hidden layer hypercolumns & minicolumns  
ğ‘“ğ‘šğ‘ğ‘¥ 50 spikes/s  Maximum firing rate  
Training 
protocol  âˆ†ğ‘¡ 1 ms  Simulation time step  
ğ‘‡ğ‘ğ‘ğ‘¡ 200 ms  Time period for each pattern  
ğ‘‡ğ‘”ğ‘ğ‘ 100 ms  Time period of gap between patterns  
ğ‘ğ‘’ğ‘ğ‘œğ‘ â„ 10 N:o of training epochs  
ğ‘ğ‘ğ‘ğ‘¡ 60000  N:o of training patterns  
Table 1: Network parameters.  
Model  Activity  Plasticity  MNIST  F-MNIST  
BCPNN  (this work)  spiking  BCPNN  97.7 Â± 0.09  83.8 Â± 0.12  
BCPNN  rate BCPNN  98.6 Â± 0.08  89.9 Â± 0.09  
Diehl & Cook , 2015  [6] spiking  STDP  95.0 -- 
Kheradpisheh et al. , 2018  [7] spiking  STDP  98.4 -- 
Mozafari et al. , 2019  [8] spiking  STDP -R 97.2 -- 
Hao et al. , 2019  [9] spiking  sym-STDP  96.7 85.3 
Dong et al. , 2023  [10] spiking  STB-STDP  97.9 87.0 
Table 2: Linear classification accuracy  (%). 
3.2 Spiking BCPNN  with sparse firing  learn s distributed representations  
In Fig. 1 A we  plotted  the neuronal  support, i.e., input , ğ¼ğ‘—, superimposed with the spiking 
output, ğ‘†ğ‘—, for 30 randomly selected neurons within a single  hypercolumn module  after 
training the network on MNIST data  (for visualization, we offset each neuronâ€™s input 
by 50mV  vertically , scaled the m to be in the range -80mV to -55mV and added  a spike  
event  of 40 mV ) and observe d sparse spiking with occasional bursts .  In Fig. 1B we 
plotted the firing rate of each neuron  in a single  randomly chosen hypercolumn module  
by convolving the spike train with a Gaussian kernel (ğœ = 50ms ). We see th at most 
neurons  have low firing rates (~2 spikes/s), with a very few (typically one) neurons  
showing high level of activity (~ 50 spikes/s ) within  the duration of a stimulus pattern 
presentation (gray vertical bars)  due to the local competition within the hypercolumn . 
We plotted the receptive fields  for three hypercolumns  and the filters learned by six 
minicolumns  each (randomly chosen ) in Fig. 1C . They provide a good qualitative match 
to the previous ly publish ed results  of the non -spiking BCPNN model [ 5].    
 
   
  
Figure  1 A. Neuronal support recorded  after training  for a time period  of 6s  across 30 randomly selected 
neuron s shows sparse spiking activity.  B. Firing rate computed  from all ( ğ‘€â„=100) neurons within  one 
hypercolumn . For the duration of pattern  presentations  (gray vertical bars, ğ‘‡ğ‘ğ‘ğ‘¡ = 200ms), mostly a single  
neuron show s a high firing rate while the rest are at a baseline firing rate. C. Local r eceptive fields  are formed 
from randomly initialized connections through the rewiring mechanism , and individual minicolumns learn 
filters within the ir receptive field resembling orientation edge detectors . 
3.3  Filtering enables  spiking model s to approximate the non-spiking  model  
We studied the effects of short -term 
filtering  (ğ‘-traces ) in terms of 
classification performance  (Fig. 2) . We 
ran our experiments  by training on  a 
reduced  version of MNIST dataset with 
1000 train ing and 1000 test patterns  while 
varying  ğœğ‘§ and ğ‘“ğ‘šğ‘ğ‘¥ (all other parameters 
same as  in Table 1) . For biologically 
realistic  values of  ğ‘“ğ‘šğ‘ğ‘¥, like 50 spikes/s, 
performance with  ğœğ‘§â‰¤ 10ms  is very low 
(ğœğ‘§ = 1ms is effectively  no filtering ). This is because pre- and post - synaptic spikes are 
expected to coincide  within this time-window  for learning to occur , whereas spikes are 
generated sparsely and irregularly from  a Poisson distribution . However, for ğœğ‘§ ~50ms , 
the performance closely approximates the non-spiking  model since  this time window is 
sufficient to expect pre - and post -synaptic spikes  to coincide and be associated. For 
ğ‘“ğ‘šğ‘ğ‘¥ > 500Hz (non -biological  case), accuracy i s high for ğœğ‘§ over a wider range since 
the spikes are dense samples of the underlying neuronal activation  and short -term 
filtering is not necessarily helpful. All m odels  irrespective of  ğ‘“ğ‘šğ‘ğ‘¥ drop sharply  in 
performance after ğœğ‘§> 100ms , very likely because the temporal window provided  is 
too long compared to the presentation time of each pattern ( ğ‘‡ğ‘ğ‘ğ‘¡+ğ‘‡ğ‘”ğ‘ğ‘ = 300ms) and 
the learning wrongly associates  spikes of a pattern  with spikes  from previous patterns . 
    
  
re iringh perco u n 
 inico u n
Figure  2: Effect of short -term filtering on 
classification performance .    
 
   
 4 Conclusion  
We have demonstrated  that our spiking BCPNN model can learn internal  
representations , preserving  the learning and rewiring mechanisms  introduced in the 
non-spiking BCPNN model , offering  competitive classification performance . Our 
Poisson spike generation mechanism  is simpler than integrate -and-fire models,  but it 
still recapitulates  in vivo  irregular cortical pyramidal spik ing patterns  with realistic 
firing rates. We suggest th at it is the  Hebbian plasticity  mechanism that provides a 
robust learning algorithm  tolerating the highly irregular  sparse spikin g. This is in stark 
contrast to backprop -based algorithms  where it is not straightforward to accommodate 
spiking neurons . We found that short -term filtering  (ğ‘-traces ) was crucial for this 
process.  The time  constants we found to work best ( ğœğ‘§ ~50ms) roughly match the 
dendritic  depolarization time constant  (paralleling  the integration step in Eq. 4), and the 
NMDA -dependent Ca2+ influx  required for synaptic plasticity ( learning step in Eq. 1).  
 
Our scaling experiments  (not shown)  suggested that the  network scales well  in terms of 
performance although the running time is 100x slower compared to the non-spiking  
model since  the timestep n eeds to be around 1ms (simulations took ~10 hours on 
custom CUDA code running on A100 GPU s). More efficient software  and custom 
hardware implementation  can make large -scale SNN simulations  more efficient . 
Another direction of interest is in developing a more complex network architecture that 
combines  recurrent attractors implementing associative memory with hierarchical  
representation learning  (unsupervised) networks . 
References  
[1] Roy, Kaushik, Akhilesh Jaiswal, and Priyadarshini Panda. "Towards spike -based machine intelligence 
with neuromorphic computing." Nature  575.7784 (2019): 607 -617. 
[2] Tavanaei, Amirhossein, et al. "Deep learning in spiking neural networks." Neural networks  111 
(2019): 47 -63. 
[3] Shadlen, Michael N., and William T. Newsome. "The variable discharge of cortical neurons: 
implications for connectivity, computation, and information coding." Journal of neuroscience  18.10 
(1998): 3870 -3896.  
[4] Churchland, Mark M., et al. "Stim ulus onset quenches neural variability: a widespread cortical 
phenomenon." Nature neuroscience  13.3 (2010): 369 -378. 
[5] Ravichandran, N. B., Lansner, A., & Herman, P. (2020, July). Learning representations in Bayesian 
Confidence Propagation neural networks. I n 2020 International Joint Conference on Neural Networks 
(IJCNN)  (pp. 1 -7). IEEE . 
[6] Diehl, Peter U., and Matthew Cook. "Unsupervised learning of digit recognition using spike -timing -
dependent plasticity." Frontiers in computational neuroscience  9 (2015): 99.  
[7] Kheradpisheh, Saeed Reza, et al. "STDP -based spiking deep convolutional neural networks for object 
recognition." Neural Networks  99 (2018): 56 -67. 
[8] Mozafari, Milad, et al. "Bio -inspired digit recognition using reward -modulated spike -timing -
dependent plasti city in deep convolutional networks." Pattern recognition  94 (2019): 87 -95. 
[9] Hao, Yunzhe, et al. "A biologically plausible supervised learning method for spiking neural networks 
using the symmetric STDP rule." Neural Networks  121 (2020): 387 -395. 
[10] Dong, Yiting, et al. "An unsupervised spiking neural network inspired by biologically plausible 
learning rules and connections." arXiv preprint arXiv:2207.02727  (2022).  