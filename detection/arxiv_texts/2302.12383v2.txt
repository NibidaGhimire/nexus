arXiv:2302.12383v2  [cs.LG]  28 Feb 2023Generalization Analysis for Contrastive Representation
Learning
Yunwen Lei1Tianbao Yang2Yiming Ying3∗Ding-Xuan Zhou4
1Department of Mathematics, Hong Kong Baptist University
2Department of Computer Science and Engineering, Texas A&M Unive rsity
3Department of Mathematics and Statistics, State University
of New York at Albany
4School of Mathematics and Statistics, University of Sydney
yunwen@hkbu.edu.hk tianbao-yang@tamu.edu yying@albany .edu
dingxuan.zhou@sydney.edu.au
March 1, 2023
Abstract
Recently, contrastive learning has found impressive succe ss in advancing the state of the art in
solvingvarious machinelearning tasks. However, theexist inggeneralization analysis is verylimited
or even not meaningful. In particular, the existing general ization error bounds depend linearly on
the number kof negative examples while it was widely shown in practice th at choosing a large k
is necessary to guarantee good generalization of contrasti ve learning in downstream tasks. In this
paper, we establish novel generalization bounds for contra stive learning which do not depend on k,
up to logarithmic terms. Our analysis uses structural resul ts on empirical covering numbers and
Rademacher complexities to exploit the Lipschitz continui ty of loss functions. For self-bounding
Lipschitz loss functions, we further improve our results by developing optimistic bounds which
imply fast rates in a low noise condition. We apply our result s to learning with both linear
representation and nonlinear representation by deep neura l networks, for both of which we derive
Rademacher complexity bounds to get improved generalizati on bounds.
1 Introduction
The performance of machine learning (ML) models often depends lar gely on the representation of
data, which motivates a resurgence of contrastive representat ion learning (CRL) to learn a representa-
tion function f:X /ma√sto→Rdfrom unsupervised data [11, 20, 23]. The basic idea is to pull togethe r similar
pairs (x,x+) and push apart disimilar pairs ( x,x−) in an embedding space, which can be formulated
as minimizing the following objective [11, 32]
Ex,x+,{x−
i}k
i=1log/parenleftig
1+k/summationdisplay
i=1exp/parenleftig
−f(x)⊤/parenleftbig
f(x+)−f(x−
i)/parenrightbig/parenrightig/parenrightig
,
wherekis the number of negative examples. The hope is that the learned rep resentation f(x) would
capture the latent structure and be beneﬁcial to other downstr eam learning tasks [3, 40]. CRL has
achieved impressive empirical performance in advancing the state- of-the-art performance in various
domains such as computer vision [10, 10, 11, 20] and natural langu age processing [8, 15, 33].
∗Corresponding author
1Assumption Arora et al.’19 Ours
O/parenleftig
k√
d√n/parenrightig
/tildewideO/parenleftig√
d√n/parenrightig
low noise O/parenleftig
k√
d√n/parenrightig
/tildewideO/parenleftig
d
n/parenrightig
Table 1: Comparison between our generalization bounds and those in Arora et al. [3] for the logistic
loss. Heredis the number of learned features. The notation /tildewideOignores log factors.
The empirical success of CRL motivates a natural question on theo retically understanding how the
learned representation adapts to the downstream tasks, i.e.,
How would the generalization behavior of downstream ML mode ls beneﬁt from the representation
function built from positive and negative pairs? Especiall y, how would the number of negative examples
aﬀect the learning performance?
Arora et al. [3] provided an attempt to answer the above questions by developing a theoretical
framework to study CRL. They ﬁrst gave generalization bounds fo r a learned representation function
in terms of Rademacher complexities. Then, they showed that this g eneralization behavior measured
by an unsupervised loss guarantees the generalization behavior of a linear classiﬁer in the downstream
classiﬁcation task. However, the generalization bounds there enj oy a linear dependency on k, which
would not be eﬀective if kis large. Moreover, this is not consistent with many studies which sho w a
largenumber ofnegativeexamples[11, 21, 23, 38] is necessaryfor goodgeneralizationperformance. For
example, the work [20] used 65536 negative examples in unsupervise d visual representation learning,
for which the existing analysis requires n≥(65536)2dtraining examples to get non-vacuous bounds [3].
Therefore, the existing analysis does not fully answer the question on the super performance of CRL
to downstream tasks as already shown in many applications. Yuan et al. [45] has demonstrated the
beneﬁts using all negative data for each anchor data for CRL and p roposed an eﬃcient algorithms for
optimizing global contrastive loss. Therefore, the existing analysis does not fully answer the question
on the super performance of CRL to downstream tasks as already shown in many applications.
In this paper, we aim to further deepen our understanding of CRL b y fully exploiting the Lipschitz
continuity of loss functions. Our contributions are listed as follows.
1. We develop generalization error bounds for CRL. We consider thr ee types of loss functions: ℓ2-
Lipschitz loss, ℓ∞-Lipschitz loss and self-bounding Lipschitz loss. For ℓ2-Lipschitz loss, we develop a
generalization bound with a square-root dependency on kby two applications of vector-contraction
lemmas on Rademacher complexities, which improves the existing boun d by a factor of√
k[3]. Forℓ∞-
Lipschitz loss, we develop generalization bounds which does not depe nd onk, up to some logarithmic
terms, by approximating the arguments of loss functions via expan ding the original dataset by a factor
ofkto fully exploit the Lipschitz continuity. For self-bounding Lipschitz lo ss, we develop optimistic
bounds involving the training errors, which can imply fast rates unde r a low noise setting. All of our
generalization bounds involve Rademacher complexities of feature c lasses, which preserve the coupling
among diﬀerent features.
2. We then apply our general result to two unsupervised represen tation learning problems: learn-
ing with linear features and learning with nonlinear features via deep n eural networks (DNNs). For
learning with linear features, we consider two regularization scheme s, i.e.,p-norm regularizer and
Schatten-norm regularizer. For learning with nonlinear features, we develop Rademacher complexity
and generalization bounds with a square-root dependency on the d epth of DNNs. To this aim, we
adapt the technique in Golowich et al. [16] by using a diﬀerent moment g eneralization function to
capture the coupling among diﬀerent features.
3. Finally, we apply our results on representationlearning to the gen eralizationanalysis of downstream
classiﬁcation problems, which outperforms the existing results by a factor ofk(ignoring a log factor).
The remaining parts of the paper are organized as follows. Section 2 reviews the related work, and
Section 3 provides the problem formulation. We give generalization bo unds for CRL in Section 4 for
three types of loss functions, which are then applied to learning with both linear and nonlinear features
in Section 5. Conclusions are given in Section 6.
22 Related Work
The most related work is the generalization analysis of CRL in Arora et al. [3], where the authors
developed generalization bounds for unsupervised errors in terms of Rademacher complexity of repre-
sentation function classes. Based on this, they further studied t he performance of linear classiﬁers on
the learned features. In particular, they considered the mean cla ssiﬁer where the weight for a class
label is the mean of the representation of corresponding inputs. A major result in Arora et al. [3] is to
show that the classiﬁcation errors of the mean classiﬁer can be bou nded by the unsupervised errors of
learned representation functions. This shows that the downstre am classiﬁcation task can beneﬁt from
a learned representation function with a low unsupervised error.
The above work motivates several interesting theoretical study of CRL. Nozawa et al. [31] studied
CRLinaPAC-Bayesiansetting, whichaimstolearnaposteriordistribu tionofrepresentationfunctions.
Nozawa et al. [31] derived PAC-Bayesian bounds for the posterior d istribution and applied it to get
PAC-Bayesian bounds for the mean-classiﬁer, which relaxes the i.i.d. assumption. Negative examples
in the framework [3] are typically taken to be randomly sampled datap oints, which may actually have
the same label of the point of interest. This introduces a bias in the o bjective function of CRL, which
leads to performance drops in practice. Motivated by this, Chuang et al. [12] introduced a debiased
CRL algorithm by building an approximation of unbiased error in CRL, an d developed generalization
guarantees for the downstream classiﬁcation.
Several researchers studied CRL from other perspectives. Lee et al. [24] proposed to learn a rep-
resentation function fto minimize E(X1,X2)[/ba∇dblX2−f(X1)/ba∇dbl2
2], whereX1,X2are unlabeled input and
pretext target. Under an approximate conditional independency assumption, the authors showed that
a linear function based on the learned representation approximate s the ground true predictor on down-
stream problems. In a generative modeling setup, Tosh et al. [41] pr oposed to learn representation
functions by a landmark embedding procedure, which can reveal th e underlying topic posterior in-
formation. Tosh et al. [40] studied CRL in a multi-view setting with two v iews available for each
datum. Under an assumption on the redundancy between the two v iews, the authors showed that
low-dimensional representation can achieve near optimal downstr eam performance with linear mod-
els. HaoChen et al. [19] studied self-supervised learning from the pe rspective of spectral clustering
based on a population augmentation graph, and proposed a spectr al contrastive loss. They further
developed generalization bounds for both representation learning and the downstream classiﬁcation.
This result is improved in a recent work by developing a guarantee to in corporate the representation
function class [35]. There are also recent work on theoretical analy sis of representation learning via
gradient-descent dynamics [24, 39], mutual information [42], alignme nt of representations [43], and
causality [29]. CRL is related to metric learning, for which generalizatio n bounds have been studied
in the literature [9].
3 Problem Formulation
LetXdenote the space of all possible datapoints. In CRL, we are given se veralsimilardata in the
form of pairs ( x,x+) drawn from a distribution Dsimandnegative datax−
1,x−
2,...,x−
kdrawn from
a distribution Dnegunrelated to x. Our aim is to learn a feature map f:X /ma√sto→Rdfrom a class of
representation functions F=/braceleftbig
f:/ba∇dblf(·)/ba∇dbl2≤R/bracerightbig
for someR >0, where /ba∇dbl ·/ba∇dbl2denotes the Euclidean
norm. Here d∈Ndenotes the number of features.
We follow the framework in Arora et al. [3] to deﬁne the distribution Dsimand the distribution
Dneg. LetCdenote the set of all latent classes and for each class c∈ Cwe assume there is a probability
distribution DcoverX, which quantiﬁes the relevance of xto the class c. We assume there is a
probability distribution ρdeﬁned over C. Then we deﬁne Dsim(x,x+) andDneg(x−) as follows
Dsim(x,x+) =Ec∼ρ/bracketleftbig
Dc(x)Dc(x+)/bracketrightbig
,
Dneg(x−) =Ec∼ρ/bracketleftbig
Dc(x−)/bracketrightbig
.
Intuitively, Dsim(x,x+) measures the probability of xandx+being drawn from the same class c∼ρ,
whileDneg(x−) measures the probability of drawing an un-relevant x−. Let (xj,x+
j)∼ Dsimand
(x−
j1,...,x−
jk)∼ Dneg,j∈[n] :={1,...,n}, wherekdenotes the number of negative examples. We
3collect these training examples into a dataset
S=/braceleftig
(x1,x+
1,x−
11,...,x−
1k),(x2,x+
2,x−
21,...,x−
2k),...,(xn,x+
n,x−
n1,...,x−
nk)/bracerightig
.(3.1)
Given a representation function f, we can measure its performance by building a classiﬁer based on
this representation and computing the accuracy of the classiﬁer. To this aim, we deﬁne a ( K+1)-way
supervised task Tconsisting of distinct classes {c1,...,c K+1} ⊆ C. The examples for this supervised
task are drawn by the following process:
We ﬁrst draw a label c∈ T={c1,...,c K+1}from a distribution DToverT, after which we draw
an example xfromDc. This deﬁnes the following distribution over labeled pairs ( x,c):DT(x,c) =
Dc(x)DT(c).Since there is a label for each example, we can build a multi-class classiﬁ erg:X /ma√sto→RK+1
forT, wheregc(x) measures the “likelihood” of assigning the class label cto the example x. The loss
ofgon a point ( x,y)∈ X × T can be measured by ℓs/parenleftbig/braceleftbig
g(x)y−g(x)y′}y′/ne}ationslash=y/parenrightbig
, whereℓs:RK/ma√sto→R+.
We quantify the performance of a classiﬁer gon the task Tby the supervised loss. By minimizing
the supervised loss, we want to build a classiﬁer whose component as sociated to the correct label is
largest.
Deﬁnition 3.1 (Supervised loss) .Letg:X /ma√sto→RK+1be a multi-class classiﬁer. The supervised loss
ofgis deﬁned as
Lsup(T,g) :=E(x,c)∼DT/bracketleftig
ℓs/parenleftbig/braceleftbig
g(x)c−g(x)c′/bracerightbig
c′/ne}ationslash=c/parenrightbig/bracketrightig
.
For CRL, we often consider gas a linear classiﬁer based on the learned representation f, i.e.,
g(x) =Wf(x), whereW∈R(K+1)×d. Then the performance of the representation function f(x) can
be quantiﬁed by the accuracy of the best linear classiﬁer on the rep resentation f(x):
Lsup(T,f) = min
W∈R(K+1)×dLsup(T,Wf).
To ﬁnd a good representation fbased on unsupervised dataset S, we need to introduce the concept
of unsupervised loss functions. Let ℓ:Rk/ma√sto→R+be a loss function for which popular choices include
the hinge loss
ℓ(v) = max/braceleftbig
0,1+max
i∈[k]{−vi}/bracerightbig
(3.2)
and the logistic loss
ℓ(v) = log/parenleftbig
1+/summationdisplay
i∈[k]exp(−vi)/parenrightbig
. (3.3)
Letf(x)⊤denote the transpose of f(x).
Deﬁnition 3.2 (Unsupervised error) .The population unsupervised error is deﬁned as
Lun(f) :=E/bracketleftbig
ℓ/parenleftbig/braceleftbig
f(x)⊤(f(x+)−f(x−
i))/bracerightbigk
i=1/parenrightbig/bracketrightbig
.
The empirical unsupervised error with Sis deﬁned as
ˆLun(f) :=1
nn/summationdisplay
j=1ℓ/parenleftbig/braceleftbig
f(xj)⊤(f(x+
j)−f(x−
ji))/bracerightbigk
i=1/parenrightbig
.
A natural algorithm is to ﬁnd among Fthe function with the minimal empirical unsupervised loss,
i.e.,ˆf:= argmin f∈FˆLun(f). This function can then be used for the downstream supervised le arning
task, e.g., to ﬁnd a linear classiﬁer g(x) =Wf(x) indexed by W∈R(K+1)×d.
4 Generalization Error Bounds
In this paper, we are interested in the performance of ˆfon testing, i.e., how the empirical behavior
ofˆfonSwould generalizewell to testing examples. Speciﬁcally, we will control Lun(ˆf)−ˆLun(ˆf). Since
ˆfdepends on the dataset S, we need to controlthe uniform deviation between population unsu pervised
error and empirical unsupervised error over the function class F, which depends on the complexity of
F. In this paper, we will use Rademacher complexity to quantify the co mplexity of F[6].
4Deﬁnition 4.1 (Rademacher Complexity) .Let/tildewideFbe a class of real-valued functions over a space Z
and/tildewideS={zi}n
i=1⊆ Z. Theempirical Rademacher complexity of /tildewideFwith respect to (w.r.t.) /tildewideSis deﬁned
asR/tildewideS(/tildewideF) =Eǫ/bracketleftbig
supf∈/tildewideF1
n/summationtext
i∈[n]ǫif(zi)/bracketrightbig
,whereǫ= (ǫi)i∈[n]∼ {±1}nare independent Rademacher
variables. We deﬁne the worst-case Rademacher complexity as RZ,n(/tildewideF) = sup /tildewideS⊆Z:|/tildewideS|=nR/tildewideS(/tildewideF),where
|/tildewideS|is the cardinality of /tildewideS.
For anyf∈ F, we introduce gf:Xk+2/ma√sto→Ras follows
gf(x,x+,x−
1,...,x−
k) =ℓ/parenleftbig/braceleftbig
f(x)⊤/parenleftbig
f(x+)−f(x−
i)/parenrightbig/bracerightbigk
i=1/parenrightbig
.
It is then clear that
Lun(f)−ˆLun(f) =Ex,x+,x−
1,...,x−
k/bracketleftbig
gf(x,x+,x−
1,...,x−
k)/bracketrightbig
−1
n/summationdisplay
j∈[n]gf(xj,x+
j,x−
j1,...,x−
jk).
Results in learning theory show that we can bound Lun(ˆf)−ˆLun(ˆf) byRS(G) [6], where
G=/braceleftbig
(x,x+,x−
1,...,x−
k)/ma√sto→gf(x,x+,x−
1,...,x−
k) :f∈ F/bracerightbig
.
Note functions in Ginvolve the nonlinear function ℓ:Rk/ma√sto→R+, which introduces diﬃculties in the
complexity analysis. Our key idea is to use the Lipschitz continuity of ℓto reduce the complexity of
Gto the complexity of another function class without ℓ. Since the arguments in ℓare vectors, we
can have diﬀerent deﬁnition of Lipschitz continuity w.r.t. diﬀerent no rms [14, 25, 26, 37]. For any
a= (a1,...,a k)∈Rkandp≥1, we deﬁne the ℓp-norm as /ba∇dbla/ba∇dblp=/parenleftbig/summationtextn
i=1|ai|p/parenrightbig1
p.
Deﬁnition 4.2 (Lipschitz continuity) .We sayℓ:Rk/ma√sto→R+isG-Lipschitz w.r.t. the ℓp-norm iﬀ
|ℓ(a)−ℓ(a′)| ≤G/ba∇dbla−a′/ba∇dblp,∀a,a′∈Rk.
In this paper, we are particularly interested in the Lipschitz continu ity w.r.t. either the ℓ2-norm or
theℓ∞-norm. According to Proposition F.1, the loss functions deﬁned in Eq . (3.2) and Eq. (3.3) are
1-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl∞, and 1-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl2[26].
Note each component of the arguments in ℓare of the form f(x)⊤(f(x+)−f(x−)). This motivates
the deﬁnition of the following function class
H=/braceleftig
hf(x,x+,x−) =f(x)⊤/parenleftbig
f(x+)−f(x−)/parenrightbig
:f∈ F/bracerightig
.
As we will see in the analysis, the complexity of Gis closely related to that of H. Therefore, we ﬁrst
show how to control the complexity of H. In the following lemma, we provide Rademacher complexity
bounds of Hw.r.t. a general dataset S′of cardinality n. We will use a vector-contraction lemma to
proveit [28]. The basic idea is to notice the Lipschitz continuity ofthe map (x,x+,x−)/ma√sto→x⊤(x+−x−)
w.r.t./ba∇dbl·/ba∇dbl2onX3. The proof is given in Section A.
Lemma 4.3. Letn∈NandS′={(xj,x+
j,x−
j) :j∈[n]}. Assume /ba∇dblf(x)/ba∇dbl2≤Rfor anyf∈ Fand
x∈S′. Then
RS′(H)≤√
12R
nEǫ∼{±1}n×{±1}d×{±1}3/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
t∈[d]/parenleftig
ǫj,t,1ft(xj)+ǫj,t,2ft(x+
j)+ǫj,t,3ft(x−
j)/parenrightig/bracketrightig
,
whereft(x)is thet-th component of f(x)∈Rd.
Remark 4.4.We compare Lemma 4.3 with the following Rademacher complexity bound in HaoChen
et al. [19]
Eǫ∼{±}n/bracketleftig
sup
f∈F/summationdisplay
j∈[n]ǫjf(xj)⊤f(x+
j)/bracketrightig
≤dmax
t∈[d]Eǫ∼{±}n/bracketleftig
sup
ft∈Ft/summationdisplay
j∈[n]ǫjft(xj)ft(x+
j)/bracketrightig
,(4.1)
5whereFt=/braceleftbig
x/ma√sto→ft(x) :f∈ F/bracerightbig
.As a comparison, our analysis in Lemma 4.3 can imply the following
bound
Eǫ∼{±}n/bracketleftig
sup
f∈F/summationdisplay
j∈[n]ǫjf(xj)⊤f(x+
j)/bracketrightig
≤2REǫ∼{±}2nd/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(xj)+ǫj,t,2ft(x+
j)/parenrightbig/bracketrightig
.
Eq. (4.1) decouples the relationship among diﬀerent features since the maximization over t∈[d] is
outside of the expectation operator. As a comparison, our result preserves this coupling since the
summation over t∈[d] is inside the supermum over f∈ F. This preservation of coupling has an eﬀect
on the bound. Indeed, it is expected that
Eǫ∼{±}2nd/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(xj)+ǫj,t,2ft(x+
j)/parenrightbig/bracketrightig
=
O/parenleftig√
dEǫ∼{±}2n/bracketleftig
sup
ft∈Ft/summationdisplay
j∈[n]/parenleftbig
ǫj,1ft(xj)+ǫj,2ft(x+
j)/parenrightbig/bracketrightig/parenrightig
.
In this case, our result implies a bound with a better dependency on das compared to Eq. (4.1) (the
factor ofdin Eq. (4.1) is replaced by√
dhere). We can plug our bound into the analysis in HaoChen
et al. [19] to improve their results.
Remark 4.5.Lemma 4.3 requires an assumption /ba∇dblf(x)/ba∇dbl2≤R. This assumption can be achieved
by adding a projection operator as f(x) =PR(˜f(x)) for˜f∈ F, wherePRdenotes the projection
operator onto the Euclidean ball with radius Raround the zero point. According to the inequality
/ba∇dblPR(˜f(x))− PR(˜f′(x))/ba∇dbl2≤ /ba∇dbl˜f(x)−˜f′(x)/ba∇dbl2, the arguments in the proof indeed show the following
inequality with H=/braceleftbig
h˜f(x,x+,x−) =PR(˜f(x))⊤/parenleftbig
PR(˜f(x+))−PR(˜f(x−))/parenrightbig
:˜f∈ F/bracerightbig
:
RS′(H)≤√
12R
nEǫ∼{±1}3nd/bracketleftig
sup
˜f∈F/summationdisplay
j∈[n]/summationdisplay
t∈[d]/parenleftig
ǫj,t,1˜ft(xj)+ǫj,t,2˜ft(x+
j)+ǫj,t,3˜ft(x−
j)/parenrightig/bracketrightig
.
That is, we can add a projection operator over Fto remove the assumption /ba∇dblf(x)/ba∇dbl2≤R.
Loss Arora et al.’19 Ours
1-ℓ2-Lipschitz√
kB
nA
n
1-ℓ∞-Lipschitz√
kB
nC
n√
k∗
S.B. 1-Lipschitz n−1+n−2k−1C2+(n−1
2+n−1k−1
2C)ˆL1
2un(f)∗
Table 2: Comparison between our generalization bounds and those in Arora et al. [3]. The notation∗
means we ignore log factors. S.B. means self-bounding. The notatio nsA,BandCare deﬁned in Eq.
(4.2), (4.4) and (4.5), which are typically of the same order. Then, o ur results improve the bounds in
Arora et al. [3] by a factor of√
kforℓ2-Lipschitz loss, and by a factor of kforℓ∞-Lipscthiz loss. For
self-bounding loss, we get optimistic bounds.
4.1ℓ2Lipschitz Loss
We ﬁrst consider ℓ2Lipschitz loss. The following theorem to be proved in Section A gives
Rademacher complexity and generalization error bounds for unsup ervised loss function classes. We
always assume ℓ/parenleftbig/braceleftbig
f(x)⊤(f(x+)−f(x−
i))/bracerightbigk
i=1/parenrightbig
≤Bfor anyf∈ Fin this paper.
6Theorem 4.6 (Generalization bound: ℓ2-Lipschitz loss) .Assume /ba∇dblf(x)/ba∇dbl2≤Rfor anyf∈ Fand
x∈ X. LetSbe deﬁned as in Eq. (3.1). Ifℓ:Rk/ma√sto→R+isG2-Lipschitz w.r.t. the ℓ2-norm, then
RS(G)≤√
24RG2A
n,where
A=E{ǫ}∼{±1}3nkdE/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
i∈[k]/summationdisplay
t∈[d]/parenleftig
ǫj,i,t,1ft(xj)+ǫj,i,t,2ft(x+
j)+ǫj,i,t,3ft(x−
ji)/parenrightig/bracketrightig
.(4.2)
Furthermore, for any δ∈(0,1), with probability at least 1−δthe following inequality holds for any
f∈ F
Lun(f)−ˆLun(f)≤4√
6RG2A
n+3B/radicalbigg
log(2/δ)
2n.
Remark 4.7.Under the same Lipschitz continuity w.r.t. /ba∇dbl·/ba∇dbl2, the following bound was established in
Arora et al. [3]
Lun(f) =ˆLun(f)+O/parenleftigG2R√
kB
n+B/radicalbigg
log(1/δ)
n/parenrightig
, (4.3)
where
B=Eǫ∼{±1}n×{±1}d×{±1}k+2/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
t∈[d]/parenleftig
ǫj,t,k+1ft(xj)+ǫj,t,k+2ft(x+
j)+/summationdisplay
i∈[k]ǫj,t,kft(x−
ji)/parenrightig/bracketrightig
.(4.4)
NoteAandBare of the same order. Therefore, it is clear that our bound in Theo rem 4.6 improves
the bound in Arora et al. [3] by a factor of√
k.
4.2ℓ∞Lipschitz Loss
We now turn to the analysis for the setting with ℓ∞Lipschitz continuity assumption, which is more
challenging. The following theorem controls the Rademacher complex ity ofGw.r.t. the dataset Sin
terms of the worst-case Rademacher complexity of Hdeﬁned on the set SH, where
SH=/braceleftig
(x1,x+
1,x−
11),(x1,x+
1,x−
12),...,(x1,x+
1,x−
1k)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
induced by the ﬁrst example,(x2,x+
2,x−
21),(x2,x+
2,x−
22),...,(x2,x+
2,x−
2k)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
induced by the second example,...,
(xn,x+
n,x−
n1),(xn,x+
n,x−
n2),...,(xn,x+
n,x−
nk)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
induced by the last example/bracerightig
.
As compared to G, the function class Hremoves the loss function ℓand is easier to handle. Our
basic idea is to exploit the Lipschitz continuity of ℓw.r.t./ba∇dbl· /ba∇dbl∞: to approximate the function class
{ℓ(v1(y),...,v k(y))}, it suﬃces to approximate each component vj(y),j∈[k]. This explains why we
expand the set Sof cardinality nto the setSHof cardinality nk. The proof is given in Section B.
Theorem 4.8 (Complexity bound: ℓ∞-Lipschitz loss) .Assume /ba∇dblf(x)/ba∇dbl2≤Rfor anyf∈ Fand
x∈ X. LetSbe deﬁned as in Eq. (3.1). Ifℓ:Rk/ma√sto→R+isG-Lipschitz w.r.t. the ℓ∞-norm, then
RS(G)≤24G(R2+1)n−1
2+48G√
kRSH,nk(H)×/parenleftbigg
1+log(4R2n3
2k)/ceilingleftig
log2R2√n
12/ceilingrightig/parenrightbigg
,
where
RSH,nk(H) = max/braceleftbig
(˜xj,˜x+
j,˜x−
j)/bracerightbig
j∈[nk]⊆SHEǫ∼{±1}nk/bracketleftig
sup
h∈H1
nk/summationdisplay
j∈[nk]ǫjf(˜xj)⊤(f(˜x+
j)−f(˜x−
j))/bracketrightig
.
Note inRSH,nk(H) we restrict the domain of functions in HtoSH, and allow an element in SHto
be chosen several times in the above maximization.
WecanuseLemma4.3tocontrol RSH,nk(H)inTheorem4.8, andderivethefollowinggeneralization
error bound. The proof is given in Section B.
7Theorem 4.9 (Generalization bound: ℓ∞-Lipschitz loss) .Letℓ:Rk/ma√sto→R+beG-Lipschitz continuous
w.r.t./ba∇dbl·/ba∇dbl∞. Assume /ba∇dblf(x)/ba∇dbl2≤R,δ∈(0,1). Then with probability at least 1−δoverSfor allf∈ F
we have
Lun(f)≤ˆLun(f)+3B/radicalbigg
log(2/δ)
2n+48G(R2+1)n−1
2+96√
12GR
n√
k/parenleftbigg
1+log(4R2n3
2k)/ceilingleftig
log2R2√n
12/ceilingrightig/parenrightbigg
C,
where
C= max
{(˜xj,˜x+
j,˜x−
j)}nk
j=1⊆SHEǫ∼{±1}nk×{±1}d×{±1}3/bracketleftig
sup
f∈F/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(˜xj)+ǫj,t,2ft(˜x+
j)+ǫj,t,3ft(˜x−
j)/parenrightbig/bracketrightig
.
(4.5)
Remark 4.10.We now compare our bound with Eq. (4.3) developed in Arora et al. [3]. I t is reasonable
to assume CandBare of the same order.1Then, our bound becomes
Lun(f) =ˆLun(f)+O/parenleftigGRBlog2(nRk)
n√
k+B/radicalbigg
log(1/δ)
n/parenrightig
. (4.6)
We know if ℓisG2-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl2, it is also√
kG2-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl∞.
Therefore, in the extreme case we have G=√
kG2. Even in this extreme case, our bound is of the
orderLun(f) =ˆLun(f) +O/parenleftig
G2RBlog2(nRk)
n+B/radicalig
log(1/δ)
n/parenrightig
, which improves Eq. (4.3) by a factor of
√
kup to a logarithmic factor. For popular loss functions deﬁned in Eq. ( 3.2) and Eq. (3.3), we have
G=G2= 1 and in this case, our bound in Eq. (4.6) improves Eq. (4.3) by a fact or ofkif we ignore
a logarithmic factor.
4.3 Self-bounding Lipschitz Loss
Finally, we consider a self-bounding Lipschitz condition where the Lips chitz constant depends on
the loss function values. This deﬁnition was given in Reeve & Kaban [34].
Deﬁnition 4.11 (Self-bounding Lipschitz Continuity) .A loss function ℓ:Rk/ma√sto→R+is said to be
Gs-self-bounding Lipschitz continuous w.r.t. ℓ∞norm if for any a,a′∈Rk
/vextendsingle/vextendsingleℓ(a)−ℓ(a′)/vextendsingle/vextendsingle≤Gsmax/braceleftbig
ℓ(a),ℓ(a′)/bracerightbig1
2/ba∇dbla−a′/ba∇dbl∞.
It was shown that the logistic loss given in Eq. (3.3) satisﬁes the self- bounding Lipschtiz continuity
withGs= 2 [34]. In the following theorem, we give generalization bounds for lea rning with self-
bounding Lipschitz loss functions. The basic idea is to replace the Lips chitz constant Gin Theorem
4.9 with empirical errors by using the self-bounding property. We us e/tildewideOto hide logarithmic factors.
The proof is given in Section C.
Theorem 4.12 (Generalization bound: self-bounding Lipschitz loss) .Letℓ:Rk/ma√sto→R+beGs-self-
bounding Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl∞. Assume /ba∇dblf(x)/ba∇dbl2≤R,δ∈(0,1). Then with probability at
least1−δoverSwe have the following inequality uniformly for all f∈ F
Lun(f) =ˆLun(f)+/tildewideO/parenleftig
(B+G2
sR4)n−1+G2
sR2n−2k−1C2/parenrightig
+/tildewideO/parenleftig
(√
B+GsR2)n−1
2+GsRn−1k−1
2C/parenrightig
ˆL1
2un(f).
1Indeed, under a typical behavior of Rademacher complexity a sEǫ∼{±}nsupa∈A⊂Rn/bracketleftbigǫiai/bracketrightbig=O(√n) [6], we have
C=O(√
nkd) andB=O(√
nkd).
8Remark 4.13.Theorem 4.12 gives optimistic generalization bounds in the sense that the upper bounds
depend on empirical errors [36]. Therefore, the generalization bou nds forLun(f)−ˆLun(f) would
beneﬁt from low training errors. In particular, if ˆL1
2un(f) = 0, Theorem 4.12 implies generalization
bounds
Lun(f) =ˆLun(f)+/tildewideO/parenleftig
Bn−1+G2
sR4n−1+G2
sR2n−2k−1C2/parenrightig
.
Typically, we have C=O(√
nk) and in this case Lun(f) =ˆLun(f)+/tildewideO/parenleftbig
Bn−1+G2
sR4n−1/parenrightbig
. In other
words, we get fast-decaying error bounds in an interpolating sett ing.
5 Applications
To apply Theorem 4.6 and Theorem 4.9, we need to control the term AorC, which is related to the
Rademacher complexity of a function class. In this section, we will sh ow how to control Cfor features
of the form x/ma√sto→Uv(x), whereU∈Rd×d′is a matrix and v:X /ma√sto→Rd′. Herevmaps the original data
x∈ Xto an intermediate feature in Rd′, which is used for all the ﬁnal features. If vis the identity
map, then we get linear features. If vis a neural network, then we get nonlinear features. For a norm
/ba∇dbl·/ba∇dblon a matrix, we denote by /ba∇dbl·/ba∇dbl∗its dual norm. The following lemmas to be proved in Section D
give general results on Rademacher complexities. Lemma 5.1 gives up per bounds, while Lemma 5.2
gives lower bounds. It is immediate to extend our analysis to control A. For brevity we ignore such a
discussion.
Lemma 5.1 (Upper bound) .Letd,d′∈N. LetVbe a class of functions from XtoRd′. Let
F={f(x) =Uv(x) :U∈ U,v∈ V}, whereU=/braceleftbig
U= (u1,...,ud)⊤∈Rd×d′:/ba∇dblU⊤/ba∇dbl ≤Λ/bracerightbig
and
f(x) =Uv(x) = (u1,...,ud)⊤v(x).Then
Eǫ∼{±1}ndsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫj,tft(xj)≤ΛEǫ∼{±1}ndsup
v∈V/vextenddouble/vextenddouble/parenleftbig/summationdisplay
j∈[n]ǫ1,jv(xj),...,/summationdisplay
j∈[n]ǫd,jv(xj)/parenrightbig/vextenddouble/vextenddouble
∗.
Lemma 5.2 (Lower bound) .IfFis symmetric in the sense that f∈ Fimplies−f∈ F, then we have
C≥/radicalbigg
nk
2sup
f∈Fmax
(˜x,˜x+,˜x−)∈SH/parenleftig
/ba∇dblf(˜x)/ba∇dbl2
2+/ba∇dblf(˜x+)/ba∇dbl2
2+/ba∇dblf(˜x−)/ba∇dbl2
2/parenrightig1
2.
5.1 Linear Features
We ﬁrst apply Lemma 5.1 to derive Rademacher complexity bounds for learning with linear fea-
tures. For any p≥1 and a matrix W= (w1,...,wd′)∈Rd×d′, theℓ2,pnorm ofWis deﬁned as
/ba∇dblW/ba∇dbl2,p=/parenleftbig/summationtext
i∈[d′]/ba∇dblwi/ba∇dblp
2/parenrightbig1
p. Ifp= 2, this becomes the Frobenius norm /ba∇dblW/ba∇dblF. For anyp≥1, the
Schatten-pnorm of a matrix W∈Rd×d′is deﬁned as the ℓp-norm of the vector of singular values
(σ1(W),...,σ min{d,d′}(W))⊤(the singular values are assumed to be sorted in non-increasing ord er),
i.e.,/ba∇dblW/ba∇dblSp:=/ba∇dblσ(W)/ba∇dblp. Letp∗be the number satisfying 1 /p+1/p∗= 1. The following proposition
to be proved in Section D.1 gives complexity bounds for learning with line ar features.
Proposition 5.3 (Linearrepresentation) .Consider the feature map deﬁned in Lemma 5.1 with v(x) =
x.
(a) If/ba∇dbl·/ba∇dbl=/ba∇dbl·/ba∇dbl2,p, then
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤min
q≥p/braceleftig
Λd1/q∗max(/radicalbig
q∗−1,1)/bracerightig/parenleftig/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightig1
2.
9(b) If/ba∇dbl·/ba∇dbl=/ba∇dbl·/ba∇dblSpwithp≤2, then
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤Λ2−1
4min
q∈[p,2]/radicalbigg
q∗π
emax/braceleftig/vextenddouble/vextenddouble/vextenddouble/parenleftig
d/summationdisplay
j∈[n]xjx⊤
j/parenrightig1
2/vextenddouble/vextenddouble/vextenddouble
Sq∗,d1/q∗/parenleftig/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightig1/2/bracerightig
.
We now plug the above Rademacher complexity bounds into Theorem 4 .9 to give generalization
errorbounds for learning with unsupervised loss. Let Bx= max{/ba∇dblxj/ba∇dbl2,/ba∇dblx+
j/ba∇dbl2,/ba∇dblx−
jt/ba∇dbl2:j∈[n],t∈[k]}.
Note/parenleftbig/summationtext
j∈[nk]/ba∇dbl˜xj/ba∇dbl2
2/parenrightbig1
2≤√
nkBxfor˜xjin the deﬁnition of C, from which and Proposition 5.3 we
get the following bound for the case v(x) =x(the deﬁnition of Cinvolvesnkexamples, while in
Proposition 5.3 we consider nexamples):
C=O/parenleftig√
nkBxmin
q≥p/braceleftbig
Λd1/q∗max(/radicalbig
q∗−1,1)/bracerightbig/parenrightig
.
The following corollary then follows from Theorem 4.9.
Corollary 5.4. Consider the feature map in Proposition 5.3 with /ba∇dbl·/ba∇dbl=/ba∇dbl·/ba∇dbl2,p. Letℓbe the logistic
loss andδ∈(0,1). Then with probability at least 1−δthe following inequality holds for all f∈ F
Lun(f)−ˆLun(f) =Blog1
2(1/δ)√n+/tildewideO/parenleftigGRBxminq≥p/braceleftbig
Λd1/q∗max(√q∗−1,1)/bracerightbig
√n/parenrightig
.
It is also possible to give generalization bounds for learning with ℓ2-Lipschitz loss functions, and
optimistic generalization bounds for learning with self-bounding Lipsc hitz loss functions. We omit the
discussion for brevity.
5.2 Nonlinear Features
We now consider Rademacher complexity for learning with nonlinear fe atures by DNNs. The
following lemma to be proved in Section D.2 gives Rademacher complexity bounds for learning with
features by DNNs. We say an activation σ:R/ma√sto→Ris positive-homogeneous if σ(ax) =aσ(x) for
a≥0, contractive if |σ(x)−σ(x′)| ≤ |x−x′|. The ReLU activation function σ(x) = max{x,0}is both
positive-homogeneous and contractive.
Proposition 5.5 (Nonlinear representation) .Consider the feature map deﬁned in Lemma 5.1 with
/ba∇dbl·/ba∇dbl=/ba∇dbl·/ba∇dblFand
V=/braceleftig
x/ma√sto→v(x) =σ/parenleftbig
VLσ/parenleftbig
VL−1···σ(V1x)/parenrightbig/parenrightbig
:/ba∇dblVl/ba∇dblF≤Bl,∀l∈[L]/bracerightig
,
whereσis positive-homogeneous, contractive and σ(0) = 0, andLis the number of layers. Then
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤√
dΛBLBL−1···B1/parenleftbigg
16L/parenleftbig/summationdisplay
1≤i<j≤n(x⊤
ixj)2/parenrightbig1
2+/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightbigg1
2
.
Remark 5.6.Ifd= 1, the following bound was established in Golowich et al. [16]
Eǫsup
f∈F/summationdisplay
j∈[n]ǫjft(xj) =O/parenleftig√
L/parenleftig/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightig1
2/productdisplay
l∈[L]Bl/parenrightig
.
Proposition 5.5 extends this bound to the general case d∈N. In particular, if d= 1, our result
matches the result in Golowich et al. [16] within a constant factor. W e need to introduce diﬀerent tech-
niques to handle the diﬃculty in considering the coupling among diﬀeren t features u⊤
tv(x),t∈[d],
which is reﬂected by the regularizer on Uas/ba∇dblU/ba∇dblF≤Λ. Ignoring this coupling would imply a
10bound with a crude dependency on d. For example, we consider the moment generation function
of supf∈F/parenleftbig/summationtext
t∈[d]/summationtext
j∈[n]ǫt,jft(xj)/parenrightbig2, and then reduce it to the moment generation function of a
Rademacher chaos variable/summationtext
1≤i<j≤nǫiǫjx⊤
ixjby repeated applications of contraction inequalities of
Rademacher complexities. A direct application of the analysis in Golowic h et al. [16] would imply
boundsEǫsupf∈F/summationtext
j∈[n]ǫjft(xj) =O/parenleftbig
d√
L/parenleftbig/summationtext
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightbig1
2/producttext
l∈[L]Bl/parenrightbig
. As a comparison, our analy-
sis implies a bound with a square-root dependency on d. We will give more details on the comparison
of technical analysis in Remark D.6.
Note/parenleftbig/summationtext
1≤i<j≤n(x⊤
ixj)2/parenrightbig1
2=O/parenleftbig/summationtext
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightbig
, from which and Proposition 5.5 we get for non-
linear features that C=O/parenleftbig√
dLΛ(nk)1
2Bx/producttext
l∈[L]Bl/parenrightbig
.The following proposition then follows directly
from Theorem 4.9.
Corollary 5.7. Consider the feature map in Proposition 5.5. Let ℓbe the logistic loss and δ∈(0,1).
With probability at least 1−δthe following inequality holds for all f∈ F
Lun(f)−ˆLun(f) =/tildewideO/parenleftigGR√
dLΛBx/producttext
l∈[L]Bl+Blog1
21
δ√n/parenrightig
.
5.3 Generalization for Downstream Classiﬁcation
In this subsection, we apply the above generalization bounds on uns upervised learning to derive
generalization guarantees for a downstream supervised learning t ask. Similar ideas can be dated
back to metric/similarity learning, where one shows that similarity-ba sed learning guarantees a good
generalizationoftheresultantclassiﬁcation[4,5,18]. FollowingAror aetal.[3], weconsideraparticular
mean classiﬁer with rows being the means of the representation of each class, i.e., x/ma√sto→Wµf(x) with
thec-th row of Wbeing the mean µcof representations of inputs with label c:µc:=Ex∼Dc[f(x)].
Consider the average supervised loss
Lµ
sup(f) :=E{ci}K+1
i=1∼ρK+1/bracketleftbig
Lsup({ci}K+1
i=1,Wµf)|ci/ne}ationslash=cj/bracketrightbig
,
where we take the expectation over T={ci}K+1
i=1. The following lemma shows that the generalization
performance of the mean classiﬁer based on a representation fcan be guaranteed in terms of the
generalization performance of the representation in unsupervise d learning.
Lemma 5.8 (Arora et al. 3) .There exists a function ρ:CK+1/ma√sto→R+such that the following inequality
holds for any f∈ F:ET ∼D/bracketleftbig
ρ(T)Lµ
sup(f)/bracketrightbig
≤Lun(f).
We refer the interested readers to Arora et al. [3] for the expres sion ofρ(T), which is independent
ofn. The following corollariesare immediate applications of Lemma 5.8 and ou r generalization bounds
for unsupervised learning. We omit the proof for brevity.
Corollary 5.9 (Linear representation) .Consider the feature map in Proposition 5.3 with /ba∇dbl·/ba∇dbl=/ba∇dbl·/ba∇dbl2,p.
Letℓs,ℓbe the logistic loss and δ∈(0,1). Then with probability at least 1−δthe following inequality
holds
ET ∼D/bracketleftig
ρ(T)Lµ
sup(ˆf)/bracketrightig
=ˆLun(ˆf)+/tildewideO/parenleftigBlog1
2(1/δ)√n+GRBxminq≥p/braceleftbig
Λd1/q∗max(√q∗−1,1)/bracerightbig
√n/parenrightig
.
Remark 5.10.Ifp≤(logd)/(logd−1), we setq= (logd)/(logd−1), and get d1/q∗max(√q∗−1,1) =
O(log1
2d) (q∗= logd). In this case, we get a bound with a logarithmic dependency on the n umber of
features. It is possible to extend our discussion to more general n orms/ba∇dbl·/ba∇dbl=/ba∇dbl·/ba∇dblp,q,p,q≥1 [22].
Corollary 5.11 (Nonlinear representation) .Consider the feature map in Proposition 5.5. Let ℓs,ℓbe
the logistic loss and δ∈(0,1). With probability at least 1−δwe have
ET ∼D/bracketleftig
ρ(T)Lµ
sup(ˆf)/bracketrightig
=ˆLun(ˆf)+/tildewideO/parenleftigGR√
dLΛBx/producttext
l∈[L]Bl√n+Blog1/2(1/δ)√n/parenrightig
.
11Remark 5.12.If we combine our Rademacher complexity bounds in Section 5 and Eq. (4.3) developed
in Arora et al. [3], we would get generalization bounds for supervised c lassiﬁcation with a linear
dependency on k. If we combine our complexity bounds and Theorem 4.6, we would get g eneralization
bounds for supervised classiﬁcation with a square-root dependen cy onk. These discussions use the
Lipschitz continuity of ℓw.r.t/ba∇dbl · /ba∇dbl2. As a comparison, the use of Lipschitz continuity w.r.t. /ba∇dbl · /ba∇dbl∞
allows us to derive generalization bounds with a logarithmic dependenc y onkin Corollary 5.9 and
Corollary 5.11. Furthermore, we can improve the bounds /tildewideO(1/√n) in these corollaries to /tildewideO(1/n) in
an interpolation setting by applying Theorem 4.12.
6 Conclusion
Motivatedbytheexistinggeneralizationboundswithacrudedepend encyonthenumberofnegative
examples, we present a systematic analysis on the generalization be havior of CRL. We consider three
types of loss functions. Our results improve the existing bounds by a factor of√
kforℓ2Lipschitz
loss, and by a factor of kforℓ∞Lipschitz loss (up to a log factor). We get optimistic bounds for self-
bounding Lipschitz loss, which imply fast rates under low noise conditio ns. We justify the eﬀectiveness
of our results with applications to both linear and nonlinear features .
OuranalysisbasedonRademachercomplexitiesimpliesalgorithm-indep endentbounds. Itwouldbe
interestingto developalgorithm-dependentbounds to understan dthe interactionbetweenoptimization
and generalization. For ℓ∞loss, our bound still enjoys a logarithmic dependency on k. It would be
interesting to study whether this logarithmic dependency can be re moved in further study.
References
[1] Alon, N., Ben-David, S., Cesa-Bianchi, N., and Haussler, D. Scale-s ensitive dimensions, uniform
convergence, and learnability. Journal of the ACM , 44(4):615–631, 1997.
[2] Anthony, M. and Bartlett, P. L. Neural network learning: Theoretical foundations , volume 9.
1999.
[3] Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saun shi, N. A theoretical analysis
of contrastive unsupervised representation learning. In International Conference on Machine
Learning , pp. 9904–9923, 2019.
[4] Balcan, M.-F. and Blum, A. On a theory of learning with similarity func tions. In international
Conference on Machine Learning , pp. 73–80, 2006.
[5] Balcan, M.-F., Blum, A., and Srebro, N. Improved guarantees for learning via similarity functions.
InConference on Learning Theory , pp. 287–298, 2008.
[6] Bartlett,P.andMendelson, S. Rademacherandgaussiancomple xities: Riskboundsandstructural
results.Journal of Machine Learning Research , 3:463–482, 2002.
[7] Boucheron, S., Lugosi, G., and Massart, P. Concentration Inequalities: A Nonasymptotic Theory
of Independence . Oxford university press, 2013.
[8] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,
Shyam, P., Sastry, G., Askell, A., et al. Language models are few-sho t learners. Advances in
neural information processing systems , 33:1877–1901, 2020.
[9] Cao, Q., Guo, Z.-C., and Ying, Y. Generalization bounds for metric a nd similarity learning.
Machine Learning , 102(1):115–132, 2016.
[10] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin , A. Unsupervised learning
of visual features by contrasting cluster assignments. Advances in Neural Information Processing
Systems, 33:9912–9924, 2020.
12[11] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framew ork for contrastive learn-
ing of visual representations. In International Conference on Machine Learning , pp. 1597–1607.
PMLR, 2020.
[12] Chuang, C.-Y., Robinson, J., Lin, Y.-C., Torralba, A., and Jegelka, S. Debiased contrastive
learning. In Advances in Neural Information Processing Systems , volume 33, pp. 8765–8775,
2020.
[13] De la Pena, V. and Gin´ e, E. Decoupling: from dependence to independence . Springer Science &
Business Media, 2012.
[14] Foster, D. J. and Rakhlin, A. ℓ∞vector contraction for rademacher complexity. arXiv preprint
arXiv:1911.06468 , 2019.
[15] Gao, T., Yao, X., and Chen, D. Simcse: Simple contrastive learning of sentence embeddings. In
Proceedings of the 2021 Conference on Empirical Methods in N atural Language Processing , pp.
6894–6910, 2021.
[16] Golowich,N., Rakhlin, A., andShamir, O. Size-independent samplec omplexityofneuralnetworks.
InConference On Learning Theory , pp. 297–299. PMLR, 2018.
[17] Guermeur, Y. Lp-norm sauer–shelah lemma for margin multi-cat egory classiﬁers. Journal of
Computer and System Sciences , 89:450–473, 2017.
[18] Guo, Z.-C. and Ying, Y. Guaranteed classiﬁcation via regularized similarity learning. Neural
Computation , 26(3):497–522, 2014.
[19] HaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. Provable guarante es for self-supervised deep
learning with spectral contrastive loss. Advances in Neural Information Processing Systems , 34:
5000–5011, 2021.
[20] He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contras t for unsupervised visual
representation learning. In Computer Vision and Pattern Recognition , pp. 9729–9738, 2020.
[21] H´ enaﬀ, O. J., Srinivas, A., De Fauw, J., Razavi, A., Doersch, C., E slami, S. A., and Van Den Oord,
A. Data-eﬃcientimagerecognitionwithcontrastivepredictivecodin g. InInternational Conference
on Machine Learning , pp. 4182–4192, 2020.
[22] Kakade, S. M., Shalev-Shwartz, S., and Tewari, A. Regularizatio n techniques for learning with
matrices. Journal of Machine Learning Research , 13:1865–1890, 2012.
[23] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., M aschinot, A., Liu, C., and
Krishnan,D. Supervisedcontrastivelearning. Advances in Neural Information Processing Systems ,
33:18661–18673, 2020.
[24] Lee, J. D., Lei, Q., Saunshi, N., and Zhuo, J. Predicting what you a lready know helps: Provable
self-supervised learning. Advances in Neural Information Processing Systems , 34:309–323, 2021.
[25] Lei, Y., Dogan, U., Binder, A., and Kloft, M. Multi-class svms: From tighter data-dependent
generalizationbounds to novelalgorithms. In Advances in Neural Information Processing Systems ,
pp. 2026–2034, 2015.
[26] Lei, Y., Dogan, ¨U., Zhou, D.-X., and Kloft, M. Data-dependent generalization bound s for multi-
class classiﬁcation. IEEE Transactions on Information Theory , 65(5):2995–3021, 2019.
[27] Lust-Piquard, F. and Pisier, G. Non commutative khintchine and paley inequalities. Arkiv f¨ or
Matematik , 29(1):241–260, 1991.
[28] Maurer, A. A vector-contraction inequality for rademacher c omplexities. In International Con-
ference on Algorithmic Learning Theory , pp. 3–17, 2016.
[29] Mitrovic, J., McWilliams, B., Walker, J., Buesing, L., and Blundell, C. Re presentation learning
via invariant causal mechanisms. arXiv preprint arXiv:2010.07922 , 2020.
[30] Mohri, M., Rostamizadeh, A., and Talwalkar, A. Foundations of Machine Learning . MIT press,
2012.
[31] Nozawa, K., Germain, P., and Guedj, B. PAC-bayesian contrast ive unsupervised representation
13learning. In Uncertainty in Artiﬁcial Intelligence , pp. 21–30. PMLR, 2020.
[32] Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748 , 2018.
[33] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S ., Sastry, G., Askell, A.,
Mishkin, P.,Clark,J., etal. Learningtransferablevisualmodelsfrom naturallanguagesupervision.
InInternational Conference on Machine Learning , pp. 8748–8763. PMLR, 2021.
[34] Reeve,H.andKaban,A. Optimisticboundsformulti-outputlear ning. InInternational Conference
on Machine Learning , pp. 8030–8040. PMLR, 2020.
[35] Saunshi, N., Ash, J., Goel, S., Misra, D., Zhang, C., Arora, S., Kaka de, S., and Krishnamurthy,
A. Understanding contrastive learning requires incorporating indu ctive biases. In International
Conference on Machine Learning , volume 162, pp. 19250–19286, 2022.
[36] Srebro, N., Sridharan, K., and Tewari, A. Smoothness, low noise and fast rates. In Advances in
Neural Information Processing Systems , pp. 2199–2207, 2010.
[37] Tewari, A. and Chaudhuri, S. Generalization errorbounds for le arning to rank: Does the length of
document lists matter? In International Conference on Machine Learning , pp. 315–323. PMLR,
2015.
[38] Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. InEuropean Conference on
Computer Vision , pp. 776–794. Springer, 2020.
[39] Tian, Y., Yu, L., Chen, X., and Ganguli, S. Understanding self-sup ervised learning with dual
deep networks. arXiv preprint arXiv:2010.00578 , 2020.
[40] Tosh, C., Krishnamurthy, A., and Hsu, D. Contrastive learning, multi-view redundancy, and
linear models. In Algorithmic Learning Theory , pp. 1179–1206. PMLR, 2021.
[41] Tosh, C., Krishnamurthy, A., and Hsu, D. Contrastive estimatio n reveals topic posterior informa-
tion to linear models. Journal of Machine Learning Research , 22:281–1, 2021.
[42] Tsai, Y.-H. H., Wu, Y., Salakhutdinov, R., and Morency, L.-P. Demy stifying self-supervised
learning: An information-theoretical framework. arXiv preprint arXiv:2006.05576 , 2, 2020.
[43] Wang, T. and Isola, P. Understanding contrastive represent ation learning through alignment and
uniformity on the hypersphere. In International Conference on Machine Learning , pp. 9929–9939.
PMLR, 2020.
[44] Ying, Y. andCampbell, C. Rademacherchaoscomplexitiesforlear ningthe kernelproblem. Neural
computation , 22(11):2858–2886, 2010.
[45] Yuan, Z., Wu, Y., Qiu, Z., Du, X., Zhang, L., Zhou, D., and Yang, T. P rovable stochastic opti-
mization for global contrastive learning: Small batch does not harm performance. In Proceedings
of International Conference of Machine Learning , 2022.
[46] Zhou, D.-X. The covering number in learning theory. Journal of Complexity , 18(3):739–767, 2002.
A Proof of Theorem 4.6
To prove Theorem 4.6, we ﬁrst prove Lemma 4.3 by the following vecto r-contraction lemma on
Rademacher complexities.
Lemma A.1 (Maurer 28) .LetS={zj}n
j=1∈ Zn. LetF′be a class of functions f′:Z /ma√sto→Rdand
h:Rd/ma√sto→RbeG-Lipschitz w.r.t. ℓ2-norm. Then
Eǫ∼{±1}n/bracketleftig
sup
f′∈F′/summationdisplay
j∈[n]ǫj(h◦f′)(zj)/bracketrightig
≤√
2GEǫ∼{±1}nd/bracketleftig
sup
f′∈F′/summationdisplay
j∈[n]/summationdisplay
t∈[d]ǫj,tf′
t(zj)/bracketrightig
.
In Section E, we will provide an extension of the above lemma.
14Proof of Lemma 4.3. Letf′:X3/ma√sto→R3dbe deﬁned as
f′(x,x+,x−) =/parenleftbig
f(x),f(x+),f(x−)/parenrightbig
∈R3d
andh:R3d/ma√sto→Rbe deﬁned as
h(y,y+,y−) =y⊤(y+−y−),y,y+,y−∈Rd.
Then it is clear that
f(x)⊤(f(x+)−f(x−)) =h◦f′(x,x+,x−).
Furthermore, for any y1,y+
1,y−
1,y2,y+
2,y−
2with Euclidean norm less than or equal to R, we have
h(y1,y+
1,y−
1)−h(y2,y+
2,y−
2) =y⊤
1(y+
1−y−
1)−y⊤
2(y+
2−y−
2)
=y⊤
1(y+
1−y−
1)−y⊤
1(y+
2−y−
2)+y⊤
1(y+
2−y−
2)−y⊤
2(y+
2−y−
2)
=y⊤
1(y+
1−y−
1−y+
2+y−
2)+(y1−y2)⊤(y+
2−y−
2).
It then follows from the elementary inequality ( a+b)2≤(1+p)a2+(1+1/p)b2that
/vextendsingle/vextendsingleh(y1,y+
1,y−
1)−h(y2,y+
2,y−
2)/vextendsingle/vextendsingle2≤
2(1+p)/ba∇dbly1/ba∇dbl2/ba∇dbly+
1−y+
2/ba∇dbl2+2(1+p)/ba∇dbly1/ba∇dbl2/ba∇dbly−
1−y−
2/ba∇dbl2
2+(1+1/p)/ba∇dbly+
2−y−
2/ba∇dbl2
2/ba∇dbly1−y2/ba∇dbl2
2.
We can choose p= 2 and get
/vextendsingle/vextendsingleh(y1,y+
1,y−
1)−h(y2,y+
2,y−
2)/vextendsingle/vextendsingle2≤6R2/parenleftbig
/ba∇dbly+
1−y+
2/ba∇dbl2+/ba∇dbly−
1−y−
2/ba∇dbl2
2+/ba∇dbly1−y2/ba∇dbl2
2/parenrightbig
= 6R2/ba∇dbl(y1,y+
1,y−
1)−(y2,y+
2,y−
2)/ba∇dbl2
2.
This shows that his√
6R-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl2. We can apply Lemma A.1 to derive
Eǫ∼{±1}n/bracketleftig
sup
f′∈F′/summationdisplay
j∈[n]ǫj(h◦f′)(zj)/bracketrightig
≤√
12REǫ∼{±1}n×{±1}d×{±1}3/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
t∈[d]/parenleftig
ǫj,t,1ft(xj)+ǫj,t,2ft(x+
j)+ǫj,t,3ft(x−
j)/parenrightig/bracketrightig
.
The proof is completed.
The following standard lemma gives generalization error bounds in ter ms of Rademacher complex-
ities.
Lemma A.2 (Mohri et al. 30) .Let/tildewideGbe a function class and /tildewideS={z1,...,zn}. If for any g∈/tildewideGwe
haveg(z)∈[0,B], then for any δ∈(0,1)the following inequality holds with probability (w.r.t. /tildewideS) at
least1−δ
E[g(z)]≤1
nn/summationdisplay
i=1g(zi)+2R/tildewideS(/tildewideG)+3B/radicalbigg
log(2/δ)
2n,∀g∈/tildewideG.
Proof of Theorem 4.6. According to the G2-Lipschitz continuity of ℓw.r.t.ℓ2-norm and Lemma A.1,
we have
Eǫ∼{±1}n/bracketleftig
sup
f∈F/summationdisplay
j∈[n]ǫjℓ/parenleftbig/braceleftbig
f(xj)⊤(f(x+
j)−f(x−
ji))/bracerightbig
i∈[k]/parenrightbig/bracketrightig
≤√
2G2E{ǫ}∼{±1}nkE/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
i∈[k]ǫj,if(xj)⊤(f(x⊤
j)−f(x−
ji))/bracketrightig
.
15According to Lemma 4.3, we further get
E{ǫ}∼{±1}nkE/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
i∈[k]ǫj,if(xj)⊤(f(x⊤
j)−f(x−
ji))/bracketrightig
≤
√
12RE{ǫ}∼{±1}3nkdE/bracketleftig
sup
f∈F/summationdisplay
j∈[n]/summationdisplay
i∈[k]/summationdisplay
t∈[d]/parenleftig
ǫj,i,t,1ft(xj)+ǫj,i,t,2ft(x+
j)+ǫj,i,t,3ft(x−
ji)/parenrightig/bracketrightig
.
We can combine the above two inequalities to get the Rademacher com plexity bounds.
We now turn to the generalization bounds. Applying Lemma A.2, with pr obability at least 1 −δ
the following inequality holds with probability at least 1 −δ
Lun(f)≤ˆLun(f)+2RS(G)+3B/radicalbigg
log(2/δ)
2n,∀f∈ F.
The stated bound on generalizationerrorsthen followsby pluggingt he Rademachercomplexity bounds
into the above bound. The proof is completed.
B Proof of Theorem 4.9
We ﬁrst introduce several complexity measures such as covering n umbers and fat-shattering dimen-
sion [1, 2, 46].
Deﬁnition B.1 (Covering number) .Let/tildewideS:={z1,...,zn} ∈ Zn. Let/tildewideFbe a class of real-valued
functions deﬁned over a space Z. For anyǫ >0 andp≥1, the empirical ℓp-norm covering number
Np(ǫ,/tildewideF,/tildewideS) with respect to /tildewideSis deﬁned asthe smallest number mofacollectionofvectors v1,...,vm∈
{(f(z1),...,f(zn)) :f∈/tildewideF}such that
sup
f∈/tildewideFmin
j∈[m]/parenleftig1
n/summationdisplay
i∈[n]|f(zi)−vj
i|p/parenrightig1
p≤ǫ,
wherevj
iis thei-th component of the vector vj. In this case, we call {v1,...,vm}an (ǫ,ℓp)-cover of
/tildewideFwith respect to /tildewideS.
Deﬁnition B.2 (Fat-Shattering Dimension) .Let/tildewideFbe a class of real-valued functions deﬁned over a
space/tildewideZ. We deﬁne the fat-shattering dimension fat ǫ(/tildewideF) at scaleǫ>0 as the largest D∈Nsuch that
there existDpointsz1,...,zD∈˜Zand witnesses s1,...,s D∈Rsatisfying: for any δ1,...,δ D∼ {±1}
there exists f∈/tildewideFwith
δi(f(zi)−si)≥ǫ/2,∀i∈[D].
To prove Theorem 4.8, we need to introduce the following lemma on Rad emacher complexity, fat-
shattering dimension and covering numbers. Part (a) shows that t he covering number can be bounded
by fat-shattering dimension (see, e.g., Theorem 12.8 in Anthony & Ba rtlett [2]). Part (b) shows that
the fat-shattering dimension can be controlled by the worst-case Rademacher complexity, which was
developed in Srebro et al. [36]. Part (c) is a discretization of the chain integral to control Rademacher
complexity by covering numbers [36], which can be found in Guermeur [ 17]. Letebe the base of the
natural logarithms.
Lemma B.3. Let/tildewideS:={z1,...,zn} ⊆˜Z. Let/tildewideFbe a class of real-valued functions deﬁned over a
space/tildewideZ.
(a) If functions in /tildewideFtake values in [−B,B], then for any ǫ>0withfatǫ(/tildewideF)<nwe have
logN∞(ǫ,/tildewideF,/tildewideS)≤1+fat ǫ/4(/tildewideF)/parenleftig
log24eBn
ǫfatǫ/4(/tildewideF)/parenrightig/parenleftig
log16B2n
ǫ2/parenrightig
.
16(b) For any ǫ>R/tildewideZ,n(/tildewideF), we have fatǫ(/tildewideF)<4n
ǫ2R2
/tildewideZ,n(/tildewideF).
(c) Let(ǫj)∞
j=0be a monotone sequence decreasing to 0and any (a1,...,a n)∈Rn. If
ǫ0≥/radicaltp/radicalvertex/radicalvertex/radicalbtn−1sup
f∈/tildewideFn/summationdisplay
i=1/parenleftbig
f(zi)−ai/parenrightbig2,
then for any non-negative integer Nwe have
R/tildewideS(/tildewideF)≤2N/summationdisplay
j=1/parenleftbig
ǫj+ǫj−1)/radicaligg
logN∞(ǫj,/tildewideF,/tildewideS)
n+ǫN. (B.1)
According to Part (a) of Lemma B.3, the following inequality holds for a nyǫ∈(0,2B] (the case
fatǫ/4(/tildewideF) = 0 istrivialsince in this casewehave N∞(ǫ,/tildewideF,/tildewideS) = 1, and otherwisewehavefat ǫ/4(/tildewideF)≥1)
logN∞(ǫ,/tildewideF,/tildewideS)≤1+fat ǫ/4(/tildewideF)log2
28eB2|/tildewideS|
ǫ2. (B.2)
We follow the arguments in Lei et al. [26] to prove Theorem 4.8.
Proof of Theorem 4.8. Weﬁrstrelatetheempirical ℓ∞-coveringnumberof Fw.r.t.S={(xj,x+
j,x−
j1,x−
j2,...,x−
jk) :
j∈[n]}to the empirical ℓ∞-covering number of Hw.r.t.SH. Let
/braceleftig
rm=/parenleftbig
rm
1,1,rm
1,2,...,rm
1,k,...,rm
n,1,rm
n,2,...,rm
n,k/parenrightbig
:m∈[N]/bracerightig
be an (ǫ/G,ℓ∞)-cover of Hw.r.t.SH. Recall that
hf(x,x+,x−) =f(x)⊤/parenleftbig
f(x+)−f(x−)/parenrightbig
. (B.3)
Then, by the deﬁnition of ℓ∞-cover we know for any f∈ Fwe can ﬁnd m∈[N] such that
max
j∈[n]max
i∈[k]/vextendsingle/vextendsinglehf(xj,x+
j,x−
ji)−rm
j,i/vextendsingle/vextendsingle≤ǫ/G.
By the Lipschitz continuity of ℓ, we then get
max
j∈[n]/vextendsingle/vextendsingleℓ/parenleftbig/braceleftbig
f(xj)⊤/parenleftbig
f(x+
j)−f(x−
ji)/parenrightbig/bracerightbigk
i=1/parenrightbig
−ℓ/parenleftbig
{rm
j,i}k
i=1/parenrightbig/vextendsingle/vextendsingle
≤G/vextenddouble/vextenddouble/parenleftbig
f(xj)⊤/parenleftbig
f(x+
j)−f(x−
ji)/parenrightbig/parenrightbigk
i=1−/parenleftbig
rm
j,i/parenrightbigk
i=1/vextenddouble/vextenddouble
∞=G/vextenddouble/vextenddouble/parenleftbig
hf(xj,x+
j,x−
ji)/parenrightbigk
i=1−/parenleftbig
rm
j,i/parenrightbigk
i=1/vextenddouble/vextenddouble
∞
≤Gǫ/G=ǫ.
This shows that/braceleftbig/parenleftbig
ℓ/parenleftbig
{rm
1,i}k
i=1/parenrightbig
,ℓ/parenleftbig
{rm
2,i}k
i=1/parenrightbig
,...,ℓ/parenleftbig
{rm
n,i}k
i=1/parenrightbig/parenrightbig
:m∈[N]/bracerightbig
is an (ǫ,ℓ∞)-cover of Gw.r.t.
Sand therefore
N∞(ǫ,G,S)≤ N∞(ǫ/G,H,SH). (B.4)
Since we consider empirical covering number of Fw.r.t.S, we can assume functions in Hare deﬁned
overSH. For simplicity, we denote Rnk(H) :=RSH,nk(H). We now control N∞(ǫ/G,H,SH) by
Rademacher complexities of H. For anyǫ>2Rnk(H), it follows from Part (b) of Lemma B.3 that
fatǫ(H)≤4nk
ǫ2R2
SH,nk(H)≤nk. (B.5)
17Note for any f∈ F, we havef(x)⊤(f(x+)−f(x−))∈[−2R2,2R2]. It then follows from Eq. (B.2)
and Eq. (B.5) that (replace Bby 2R2)
logN∞(ǫ,H,SH)≤1+fat ǫ/4(H)log2(32eR4nk/ǫ2)
≤1+64nkR2
nk(H)
ǫ2log2(32eR4nk/ǫ2), ǫ∈(0,4R2].
We can combine the above inequality and Eq. (B.4) to derive the followin g inequality for any
2GRnk(H)≤ǫ≤4GR2
logN∞(ǫ,G,S)≤1+64nkG2R2
nk(H)
ǫ2log2(32eR4G2nk/ǫ2). (B.6)
LetǫN= 24Gmax/braceleftbig√
kRnk(H),n−1
2/bracerightbig
,
ǫj= 2N−jǫN, j= 0,...,N−1,
where
N=/ceilingleftbigg
log22GR2
24Gmax/braceleftig√
kRnk(H),n−1
2/bracerightig/ceilingrightbigg
.
It is clear from the deﬁnition that
ǫ0≥2GR2≥ǫ0/2.
The Lipschitz continuity of ℓimplies
ℓ(({hf(x,x+,x−
i)}i∈[k]))−ℓ((0,0,...,0))≤G/ba∇dblhf(x,x+,x−)/ba∇dbl∞≤2R2G.
According to the above inequality and Part (c) of Lemma B.3, we know (noteǫN≥2GRnk(H) and
therefore Eq. (B.6) holds for ǫ=ǫj,j= 1,...,N)
RS(G)≤2N/summationdisplay
j=1(ǫj+ǫj−1)/radicalbigg
logN∞(ǫj,G,S)
n+ǫN
≤2n−1
2N/summationdisplay
j=1(ǫj+ǫj−1)+16G√
nkRnk(H)√nN/summationdisplay
j=1(ǫj+ǫj−1)log(32eR4G2nk/ǫ2
j)
ǫj+ǫN
≤6ǫ0n−1
2+ǫN+48G√
nkRnk(H)√nN/summationdisplay
j=1log(32eR4G2nk/ǫ2
j)
≤24GR2n−1
2+ǫN+48GN√
kRnk(H)log(32eR4G2nk)+48G√
kRnk(H)N/summationdisplay
j=1log(1/ǫ2
j).
According to the deﬁnition of ǫk, we know
N/summationdisplay
j=1log(1/ǫ2
j) =N/summationdisplay
j=1log(22j/ǫ2
0) =N/summationdisplay
j=1log(1/ǫ2
0)+log4 ·N/summationdisplay
j=1j=Nlog(1/ǫ2
0)+N(N+1)log4
2
=N/parenleftig
log1/ǫ2
0+(N+1)log2/parenrightig
=Nlog2N+1/ǫ2
0=Nlog/parenleftig1
ǫN2
ǫ0/parenrightig
≤Nlog/parenleftig1
ǫN2
2GR2/parenrightig
≤Nlog/parenleftig√n
24G1
GR2/parenrightig
=Nlog√n
24G2R2.
18We can combine the above two inequalities together to get
RS(G)≤24GR2n−1
2+ǫN+48NG√
kRnk(H)/parenleftig
log(32eR4G2nk)+log√n
24G2R2/parenrightig
≤24GR2n−1
2+ǫN+48G√
kRnk(H)/parenleftig
log(32eR4G2nk)+log√n
24G2R2/parenrightig/ceilingleftig
log22GR2√n
24G/ceilingrightig
≤24GR2n−1
2+ǫN+48G√
kRnk(H)/parenleftig
log(4R2n3
2k)/parenrightig/ceilingleftig
log2R2√n
12/ceilingrightig
,
where we have used the deﬁnition of Nand 32e/24≤4. The proof is completed.
Proof of Theorem 4.9. Applying Lemma A.2, with probability at least 1 −δthe following inequality
holds with probability at least 1 −δ
Lun(f)≤ˆLun(f)+2RS(G)+3B/radicalbigg
log(2/δ)
2n,∀f∈ F.
According to Theorem 4.8 and Lemma 4.3, we know
RS(G)≤48G(R2+1)n−1
2+48G√
kRnk(H)/parenleftbigg
1+log(R2n3
2k)/ceilingleftig
log2R2√n
12/ceilingrightig/parenrightbigg
≤48G(R2+1)n−1
2+48√
12GR√
k
nk/parenleftbigg
1+log(R2n3
2k)/ceilingleftig
log2R2√n
12/ceilingrightig/parenrightbigg
C.
We can combine the above two inequalities together and derive the st ated bound. The proof is com-
pleted.
C Proof of Theorem 4.12
To prove Theorem 4.12, we introduce the following lemma on generaliza tion error bounds in terms
of local Rademacher complexities [34].
Lemma C.1 (Reeve & Kaban 34) .Consider a function class Gof functions mapping Zto[0,b]. For
any/tildewideS={zi:i∈[n]}andg∈ G, letˆE/tildewideS[g] =1
n/summationtext
i∈[n]g(zi). Assume for any /tildewideS∈ Znandr >0, we
have
R/tildewideS/parenleftbig
{g∈ G:ˆE/tildewideS[g]≤r}/parenrightbig
≤φn(r),
whereφn:R+/ma√sto→R+is non-decreasing and φn(r)/√ris non-increasing. Let ˆrnbe the largest solution
of the equation φn(r) =r. For anyδ∈(0,1), with probability at least 1−δthe following inequality
holds uniformly for all g∈ G
Ez[g(z)]≤ˆE/tildewideS[g]+90(ˆrn+r0)+4/radicalig
ˆE/tildewideS[g](ˆrn+r0),
wherer0=b/parenleftbig
log(1/δ)+6loglog n/parenrightbig
/n.
Proof of Theorem 4.12. For anyr>0, we deﬁne Fras a subset of Fwith the empirical error less than
or equal to r
Fr=/braceleftig
f∈ F:1
n/summationdisplay
j∈[n]gf(xj,x+
j,x−
j1,...,x−
jk)≤r/bracerightig
.
Let/braceleftig
rm=/parenleftbig
rm
1,1,rm
1,2,...,rm
1,k,...,rm
n,1,rm
n,2,...,rm
n,k/parenrightbig
:m∈[N]/bracerightig
19be an (ǫ/(√
2rGs),ℓ∞)-cover of Hr:=/braceleftbig
hf∈ H:f∈ Fr/bracerightbig
w.r.t.SH. Then, by the deﬁnition of
ℓ∞-cover we know for any f∈ Frwe can ﬁnd m∈[N] such that
max
j∈[n]max
i∈[k]/vextendsingle/vextendsinglehf(xj,x+
j,x−
ji)−rm
j,i/vextendsingle/vextendsingle≤ǫ/(√
2rGs).
According to the self-bounding Lipschitz continuity of ℓ, we know
1
n/summationdisplay
j∈[n]/vextendsingle/vextendsingleℓ/parenleftbig/braceleftbig
f(xj)⊤/parenleftbig
f(x+
j)−f(x−
ji)/parenrightbig/bracerightbigk
i=1/parenrightbig
−ℓ/parenleftbig
{rm
j,i}k
i=1/parenrightbig/vextendsingle/vextendsingle2
≤G2
s
n/summationdisplay
j∈[n]max/braceleftig
ℓ/parenleftbig/braceleftbig
f(xj)⊤/parenleftbig
f(x+
j)−f(x−
ji)/parenrightbig/bracerightbigk
i=1/parenrightbig
,ℓ/parenleftbig
{rm
j,i}k
i=1/parenrightbig/bracerightig/vextenddouble/vextenddouble/parenleftbig
f(xj)⊤/parenleftbig
f(x+
j)−f(x−
ji)/parenrightbig/parenrightbigk
i=1−/parenleftbig
rm
j,i/parenrightbigk
i=1/vextenddouble/vextenddouble2
∞
≤G2
s
n/summationdisplay
j∈[n]/parenleftbig
ℓ/parenleftbig/braceleftbig
f(xj)⊤/parenleftbig
f(x+
j)−f(x−
ji)/parenrightbig/bracerightbigk
i=1/parenrightbig
+ℓ/parenleftbig
{rm
j,i}k
i=1/parenrightbig/parenrightbig/vextenddouble/vextenddouble/parenleftbig
hf(xj,x+
j,x−
ji)/parenrightbigk
i=1−/parenleftbig
rm
j,i/parenrightbigk
i=1/vextenddouble/vextenddouble2
∞
≤2G2
srǫ2/(2rG2
s) =ǫ2,
where we have used the following inequalities due to the deﬁnition of Fr
1
n/summationdisplay
j∈[n]ℓ/parenleftbig/braceleftbig
f(xj)⊤/parenleftbig
f(x+
j)−f(x−
ji)/parenrightbig/bracerightbigk
i=1/parenrightbig
≤r,1
n/summationdisplay
j∈[n]ℓ/parenleftbig
{rm
j,i}k
i=1/parenrightbig
≤r.
Therefore, we have
N2(ǫ,Gr,S)≤ N∞(ǫ/(√
2rGs),Hr,SH),
whereGr={gf∈ G:f∈ Fr}. Analyzing analogously to the proof of Theorem 4.8, we get (replacin g
Gthere by√
2rGs)
RS(Gr)≤24√
2rGs(R2+1)n−1
2+48√
2rGs√
kRSH,nk(H)/parenleftbigg
1+log(4R2n3
2k)/ceilingleftig
log2R2√n
12/ceilingrightig/parenrightbigg
:=ψn(r).
Let ˆrnbe the point satisfying ˆ rn=ψn(ˆrn):
ˆrn= 24/radicalbig
2ˆrnGs(R2+1)n−1
2+48/radicalbig
2ˆrnGs√
kRSH,nk(H)/parenleftbigg
1+log(4R2n3
2k)/ceilingleftig
log2R2√n
12/ceilingrightig/parenrightbigg
,
from which we get
ˆrn=/tildewideO/parenleftig
G2
sR4n−1+G2
skR2
SH,nk(H)/parenrightig
.
We can apply Lemma C.1 to get the following inequality with probability at le ast 1−δuniformly for
allf∈ F
Lun(f) =ˆLun(f)+/tildewideO/parenleftig
Bn−1+G2
sR4n−1+G2
skR2
SH,nk(H)/parenrightig
+/tildewideO/parenleftig√
Bn−1
2+GsR2n−1
2+Gs√
kRSH,nk(H)/parenrightig
ˆL1
2un(f).
We can apply Lemma 4.3 to control RSH,nk(H) and derive the following bound
Lun(f) =ˆLun(f)+/tildewideO/parenleftig
Bn−1+G2
sR4n−1+G2
sR2n−2k−1C2/parenrightig
+/tildewideO/parenleftig√
Bn−1
2+GsR2n−1
2+GsRn−1k−1
2C/parenrightig
ˆL1
2un(f).
The proof is completed.
20D Proof on Rademacher Complexities
We ﬁrst proveRademacher complexity bounds for feature spaces in Lemma 5.1, and then give lower
bounds. Finally, we will apply it to prove Proposition 5.3 and Proposition 5.5.
Proof of Lemma 5.1. LetU= (u1,...,ud)⊤andVS= (v(x1),...,v(xn)). Then it is clear
UVS=
u⊤
1v(x1)···u⊤
1v(xn)
.........
u⊤
dv(x1)···u⊤
dv(xn)
.
Let
Mǫ=
ǫ1,1···ǫ1,n
.........
ǫd,1···ǫd,n
∈Rd×n.
Then we have
/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,ju⊤
tv(xj) =/angbracketleftbig
Mǫ,UVS/an}b∇acket∇i}ht= trace(M⊤
ǫUVS) = trace(UVSM⊤
ǫ)
=/angbracketleftbig
U⊤,VSM⊤
ǫ/angbracketrightbig
≤ /ba∇dblU⊤/ba∇dbl/ba∇dblVSM⊤
ǫ/ba∇dbl∗,
where trace denotes the trace of a matrix. Therefore, we have
sup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj) = sup
U∈U,v∈V/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,ju⊤
tv(xj)
≤Λsup
v∈V/ba∇dblVSM⊤
ǫ/ba∇dbl∗= Λsup
v∈V/vextenddouble/vextenddouble/parenleftbig/summationdisplay
j∈[n]ǫ1,jv(xj),...,/summationdisplay
j∈[n]ǫd,jv(xj)/parenrightbig/vextenddouble/vextenddouble
∗.
The proof is completed.
Proof of Lemma 5.2. Note
/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(˜xj)+ǫj,t,2ft(˜x+
j)+ǫj,t,3ft(˜x−
j)/parenrightbig/vextendsingle/vextendsingle/vextendsingle=
max/braceleftig/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(˜xj)+ǫj,t,2ft(˜x+
j)+ǫj,t,3ft(˜x−
j)/parenrightbig
,−/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(˜xj)+ǫj,t,2ft(˜x+
j)+ǫj,t,3ft(˜x−
j)/parenrightbig/bracerightig
.
According to the symmetry of Fwe know
C= max
{(˜xj,˜x+
j,˜x−
j)}nk
j=1⊆SHEǫ∼{±1}nk×{±1}d×{±1}3/bracketleftig
sup
f∈F/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(˜xj)+ǫj,t,2ft(˜x+
j)+ǫj,t,3ft(˜x−
j)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/bracketrightig
≥sup
f∈Fmax
{(˜xj,˜x+
j,˜x−
j)}nk
j=1⊆SHEǫ∼{±1}nk×{±1}d×{±1}3/bracketleftig/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(˜xj)+ǫj,t,2ft(˜x+
j)+ǫj,t,3ft(˜x−
j)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/bracketrightig
,
where we have used the Jensen’s inequality in the last step.
Since we take maximization over {(˜xj,˜x+
j,˜x−
j)}nk
j=1⊆SH, we can choose ( ˜xj,˜x+
j,˜x−
j) = (˜x,˜x+,˜x−)
21for any ( ˜x,˜x+,˜x−)∈SH. Then we get
C≥sup
f∈Fmax
(˜x,˜x+,˜x−)∈SHEǫ∼{±1}nk×{±1}d×{±1}3/bracketleftig/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
ǫj,t,1ft(˜x)+ǫj,t,2ft(˜x+)+ǫj,t,3ft(˜x−)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/bracketrightig
≥2−1
2sup
f∈Fmax
(˜x,˜x+,˜x−)∈SH/parenleftig/summationdisplay
j∈[nk]/summationdisplay
t∈[d]/parenleftbig
f2
t(˜x)+f2
t(˜x+)+f2
t(˜x−)/parenrightbig/parenrightig1
2
= 2−1
2sup
f∈Fmax
(˜x,˜x+,˜x−)∈SH/parenleftig/summationdisplay
j∈[nk]/parenleftbig
/ba∇dblf(˜x)/ba∇dbl2
2+/ba∇dblf(˜x+)/ba∇dbl2
2+/ba∇dblf(˜x−)/ba∇dbl2
2/parenrightbig/parenrightig1
2
=√
2−1nksup
f∈Fmax
(˜x,˜x+,˜x−)∈SH/parenleftig
/ba∇dblf(˜x)/ba∇dbl2
2+/ba∇dblf(˜x+)/ba∇dbl2
2+/ba∇dblf(˜x−)/ba∇dbl2
2/parenrightig1
2,
where we have used the following Khitchine-Kahane inequality [13]
Eǫ/vextendsingle/vextendsinglen/summationdisplay
i=1ǫiti/vextendsingle/vextendsingle≥2−1
2/bracketleftbign/summationdisplay
i=1|ti|2/bracketrightbig1
2,∀t1,...,tn∈R, (D.1)
The proof is completed.
Remark D.1.The analysis in the proof implies a lower bound for R/tildewideS(/tildewideF) for a symmetric /tildewideFand
/tildewideS={z1,...,zn}
R/tildewideS(/tildewideF)≥1√
2nsup
f∈/tildewideF/vextenddouble/vextenddouble/parenleftbig
f(z1),...,f(zn)/parenrightbig/vextenddouble/vextenddouble
2.
Indeed, by the symmetry of F, the Jensen inequality and Eq. (D.1), we have
R/tildewideS(/tildewideF) =Eǫ/bracketleftbig
sup
f∈/tildewideF1
n/summationdisplay
i∈[n]ǫif(zi)/bracketrightbig
=Eǫ/bracketleftbig
sup
f∈/tildewideF1
n/vextendsingle/vextendsingle/summationdisplay
i∈[n]ǫif(zi)/vextendsingle/vextendsingle/bracketrightbig
≥1
nsup
f∈/tildewideFEǫ/bracketleftbig/vextendsingle/vextendsingle/summationdisplay
i∈[n]ǫif(zi)/vextendsingle/vextendsingle/bracketrightbig
≥1√
2nsup
f∈/tildewideF/parenleftig/summationdisplay
i∈[n]f2(zi)/parenrightig1
2.
D.1 Proof of Proposition 5.3
The following Khintchine-Kahane inequality [13, 27] is very useful for us to estimate Rademacher
complexities.
Lemma D.2. Letǫ1,...,ǫ nbe a sequence of independent Rademacher variables.
(a) Letv1,...,vn∈ H, whereHis a Hilbert space with /ba∇dbl·/ba∇dblbeing the associated norm. Then, for any
p≥1there holds
/bracketleftbig
Eǫ/ba∇dbln/summationdisplay
i=1ǫivi/ba∇dblp/bracketrightbig1
p≤max(/radicalbig
p−1,1)/bracketleftbign/summationdisplay
i=1/ba∇dblvi/ba∇dbl2/bracketrightbig1
2. (D.2)
(b) LetX1,...,X nbe a set of matrices of the same dimension. For all q≥2,
/parenleftig
Eǫ/vextenddouble/vextenddoublen/summationdisplay
i=1ǫiXi/vextenddouble/vextenddoubleq
Sq/parenrightig1
q≤2−1
4/radicalbiggqπ
emax/braceleftig/vextenddouble/vextenddouble/parenleftbign/summationdisplay
i=1X⊤
iXi/parenrightbig1
2/vextenddouble/vextenddouble
Sq,/vextenddouble/vextenddouble/parenleftbign/summationdisplay
i=1XiX⊤
i/parenrightbig1
2/vextenddouble/vextenddouble
Sq/bracerightig
.(D.3)
Proof of Proposition F.1. Letq≥p. It is clear q∗≤p∗. The dual norm of /ba∇dbl·/ba∇dbl2,pis/ba∇dbl·/ba∇dbl2,p∗. Therefore,
according to Lemma 5.1 and /ba∇dbl·/ba∇dblp∗≤ /ba∇dbl·/ba∇dbl q∗we know
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤ΛEǫ/parenleftig/summationdisplay
t∈[d]/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫt,jxj/vextenddouble/vextenddoublep∗
2/parenrightig1/p∗
≤ΛEǫ/parenleftig/summationdisplay
t∈[d]/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫt,jxj/vextenddouble/vextenddoubleq∗
2/parenrightig1/q∗
≤Λ/parenleftig
Eǫ/bracketleftig/summationdisplay
t∈[d]/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫt,jxj/vextenddouble/vextenddoubleq∗
2/bracketrightig/parenrightig1/q∗
= Λ/parenleftig
dEǫ/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddoubleq∗
2/parenrightig1/q∗
,
22where we have used Jense’s inequality and the concavity of x/ma√sto→x1/q∗. By Lemma D.2, we know
Eǫ/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddoubleq∗
2≤max(/radicalbig
q∗−1,1)q∗/parenleftig/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightigq∗
2.
It then follows that
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤Λd1/q∗max(/radicalbig
q∗−1,1)/parenleftig/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightig1
2.
Note the above inequality holds for any q≥p. This proves Part (a).
We now prove Part (b). Since the dual norm of /ba∇dbl·/ba∇dblSpis/ba∇dbl·/ba∇dblSp∗, by Lemma 5.1 we know
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤ΛEǫ/vextenddouble/vextenddouble/parenleftbig/summationdisplay
j∈[n]ǫ1,jxj,...,/summationdisplay
j∈[n]ǫd,jxj/parenrightbig/vextenddouble/vextenddouble
Sp∗.
For anyt∈[d] andj∈[n], deﬁne
/tildewideXt,j=/parenleftig
0···0xj0···0/parenrightig
,
i.e., thet-th column of /tildewideXt,j=xj, and other columns are zero vectors. This implies that
/parenleftbig/summationdisplay
j∈[n]ǫ1,jxj,...,/summationdisplay
j∈[n]ǫd,jxj/parenrightbig
=/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,j/tildewideXt,j.
It is clear that /tildewideXt,j/tildewideX⊤
t,j=xjx⊤
jand
/tildewideX⊤
t,j/tildewideXt,j=
0··· ··· 0
......···0
... 0x⊤
jxj0
0··· ··· ....
=x⊤
jxjdiag(0,...,0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
t−1,1,0...,0),
where diag( a1,...,a n) denotes the diagonal matrix with elements a1,...,a n. Therefore, we have
/summationdisplay
t∈[d]/summationdisplay
j∈[n]/tildewideXt,j/tildewideX⊤
t,j=d/summationdisplay
j∈[n]xjx⊤
j
and/summationdisplay
t∈[d]/summationdisplay
j∈[n]/tildewideX⊤
t,j/tildewideXt,j=/parenleftbig/summationdisplay
j∈[n]x⊤
jxj/parenrightbig
Id×d,
whereId×ddenotes the identity matrix in Rd×d. Therefore, we can apply Lemma D.2 to show that
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤Λ/parenleftig
Eǫ/vextenddouble/vextenddouble/parenleftbig/summationdisplay
j∈[n]ǫ1,jxj,...,/summationdisplay
j∈[n]ǫd,jxj/parenrightbig/vextenddouble/vextenddoubleq∗
S∗q/parenrightig1/q∗
≤Λ2−1
4/radicalbigg
q∗π
emax/braceleftig/vextenddouble/vextenddouble/vextenddouble/parenleftig
d/summationdisplay
j∈[n]xjx⊤
j/parenrightig1
2/vextenddouble/vextenddouble/vextenddouble
Sq∗,d1/q∗/parenleftbig/summationdisplay
j∈[d]/ba∇dblxj/ba∇dbl2
2/parenrightbig1
2/bracerightig
.
The proof is completed.
23D.2 Proof of Proposition 5.5
For convenience we introduce the following sequence of function sp aces
Vk=/braceleftig
x/ma√sto→σk/parenleftbig
Vkσ/parenleftbig
Vk−1···σ(V1x)/parenrightbig/parenrightbig
:/ba∇dblVj/ba∇dblF≤Bj/bracerightig
, k∈[L].
To prove Proposition 5.5, we need to introduce several lemmas. The following lemma shows how the
supremum over a matrix can be transferred to a supremum over a v ector. It is an extension of Lemma
1 in Golowich et al. [16] from d= 1 tod∈N, and can be proved exactly by the arguments in Golowich
et al. [16].
Lemma D.3. Letσ:R/ma√sto→Rbe a1-Lipschitz continuous, positive-homogeneous activation function
which is applied elementwise. Then for any vector-valued fu nction class /tildewideF
sup
˜f∈/tildewideF,V∈Rh×h′:/bardblV/bardblF≤B/summationdisplay
t∈[d]/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫt,jσ(V˜f(xj))/vextenddouble/vextenddouble/vextenddouble2
2≤B2sup
˜f∈/tildewideF,˜v∈Rh′:/bardbl˜v/bardbl2≤1sup
/bardbl˜v/bardbl2≤1/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2
.
Proof.Letv⊤
1,...,v⊤
hberowsofmatrix V, i.e.,V⊤=/parenleftbig
v1,...,vh/parenrightbig
. Thenbythepositive-homogeneous
property of activation function we have
/summationdisplay
t∈[d]/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫt,jσ(V˜f(xi))/vextenddouble/vextenddouble/vextenddouble2
2=/summationdisplay
t∈[d]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
/summationtext
j∈[n]ǫt,jσ(v⊤
1˜f(xj))
...
/summationtext
j∈[n]ǫt,jσ(v⊤
h˜f(xj))
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
2=/summationdisplay
t∈[d]/summationdisplay
r∈[h]
/summationdisplay
j∈[n]ǫt,jσ(v⊤
r˜f(xj))
2
=/summationdisplay
r∈[h]/ba∇dblvr/ba∇dbl2
2/summationdisplay
t∈[d]/parenleftbigg/summationdisplay
j∈[n]ǫt,jσ/parenleftigv⊤
r
/ba∇dblvr/ba∇dbl2˜f(xj)/parenrightig/parenrightbigg2
≤/parenleftig/summationdisplay
r∈[h]/ba∇dblvr/ba∇dbl2
2/parenrightig
max
r∈[h]/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ/parenleftigv⊤
r
/ba∇dblvr/ba∇dbl2˜f(xj)/parenrightig/vextendsingle/vextendsingle/vextendsingle2
≤B2sup
/bardbl˜v/bardbl2≤1/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2
.
The proof is completed.
The following lemma gives a general contraction lemma for Rademache r complexities. It allows
us to remove a nonlinear function ψ, which is very useful for us to handle the activation function in
DNNs.
Lemma D.4 (Contraction Lemma, Thm 11.6 in Boucheron et al. [7]) .Let˜τ:R+/ma√sto→R+be convex
and nondecreasing. Suppose ψ:R/ma√sto→Ris contractive in the sense |ψ(t)−ψ(˜t)| ≤ |t−˜t|andψ(0) = 0.
Then the following inequality holds for any /tildewideF
Eǫ˜τ/parenleftbigg
sup
f∈/tildewideFn/summationdisplay
i=1ǫiψ/parenleftbig
f(xi)/parenrightbig/parenrightbigg
≤Eǫ˜τ/parenleftbigg
sup
f∈/tildewideFn/summationdisplay
i=1ǫif(xi)/parenrightbigg
.
The following lemma gives bounds of moment generation functions for a random variable Z=/summationtext
1≤i<j≤nǫiǫjaij, which is called a Rademacher chaos variable [13, 44].
Lemma D.5 (page167inDe laPena&Gin´ e[13]) .Letǫi,i∈[n]be independent Rademacher variables.
Letai,j∈R,i,j∈[n]. Then for Z=/summationtext
1≤i<j≤nǫiǫjaijwe have
Eǫexp/parenleftig
|Z|/(4es)/parenrightig
≤2,wheres2:=/summationdisplay
1≤i<j≤na2
i,j.
24Proof of Proposition 5.5. The dual norm of /ba∇dbl · /ba∇dblFis/ba∇dbl · /ba∇dblF. Therefore, according to Lemma 5.1 we
know
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤ΛEǫsup
v∈V/parenleftig/summationdisplay
t∈[d]/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫt,jv(xj)/vextenddouble/vextenddouble2
2/parenrightig1/2
= ΛEǫ/parenleftbigg
sup
˜f∈VL−1,V:/bardblV/bardblF≤BL/summationdisplay
t∈[d]/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫt,jσ(V˜f(xj))/vextenddouble/vextenddouble/vextenddouble2
2/parenrightbigg1
2
≤ΛBLEǫ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg1
2
,
where we have used Lemma D.3 in the second inequality. Let λ≥0 andτ(x) = exp(λx2). It is clear
thatτis convex and increasing in the interval [0 ,∞). It then follows from the Jensen’s inequality that
exp/parenleftbigg
λ/parenleftig
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)/parenrightig2/parenrightbigg
≤exp/parenleftbigg
λ/parenleftbigg
ΛBLEǫ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg1
2/parenrightbigg2/parenrightbigg
≤Eǫexp/parenleftbigg
λ/parenleftbigg
ΛBL/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg1
2/parenrightbigg2/parenrightbigg
=Eǫexp/parenleftbigg
λΛ2B2
Lsup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg
≤Eǫexp/parenleftbigg
λΛ2B2
L/summationdisplay
t∈[d]sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg
=Eǫ/productdisplay
t∈[d]exp/parenleftbigg
λΛ2B2
Lsup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg
=/productdisplay
t∈[d]Eǫtexp/parenleftbigg
λΛ2B2
Lsup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg
=Eǫ∼{±1}nexp/parenleftbigg
dλΛ2B2
Lsup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫjσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg
,
where we have used the independency between ǫt= (ǫt,j)j∈[n],t∈[d]. Let ˜τ:R+/ma√sto→R+be deﬁned as
˜τ(x) = exp(dλΛ2B2
Lx2). Then we have
exp/parenleftbigg
λ/parenleftig
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)/parenrightig2/parenrightbigg
≤Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫjσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
≤Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
j∈[n]ǫjσ(˜v⊤˜f(xj))/parenrightbigg
+Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1−/summationdisplay
j∈[n]ǫjσ(˜v⊤˜f(xj))/parenrightbigg
= 2Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
j∈[n]ǫjσ(˜v⊤˜f(xj))/parenrightbigg
≤2Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
j∈[n]ǫj˜v⊤˜f(xj)/parenrightbigg
=Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1˜v⊤/summationdisplay
j∈[n]ǫj˜f(xj)/parenrightbigg
=Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫj˜f(xj)/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
.(D.4)
where we have used Lemma D.4 and the contraction property of σin the last inequality.
25According to Lemma D.3, we know
Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫj˜f(xj)/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
=Eǫ∼{±1}n˜τ/parenleftbigg
sup
/bardblVL−1/bardblF≤BL−1,˜f∈VL−2/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjσ/parenleftbig
VL−1˜f(xj)/parenrightbig/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
≤Eǫ∼{±1}n˜τ/parenleftbigg
BL−1sup
/bardbl˜v/bardbl2≤1,˜f∈VL−2/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫjσ/parenleftbig
˜v⊤˜f(xj)/parenrightbig/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
.
It then follows that
Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫj˜f(xj)/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
≤Eǫ∼{±1}n˜τ/parenleftbigg
BL−1sup
/bardbl˜v/bardbl2≤1,˜f∈VL−2/summationdisplay
j∈[n]ǫjσ/parenleftbig
˜v⊤˜f(xj)/parenrightbig/parenrightbigg
+Eǫ∼{±1}n˜τ/parenleftbigg
BL−1sup
/bardbl˜v/bardbl2≤1,˜f∈VL−2−/summationdisplay
j∈[n]ǫjσ/parenleftbig
˜v⊤˜f(xj)/parenrightbig/parenrightbigg
= 2Eǫ∼{±1}n˜τ/parenleftbigg
BL−1sup
/bardbl˜v/bardbl2≤1,˜f∈VL−2/summationdisplay
j∈[n]ǫjσ/parenleftbig˜v⊤˜f(xj)/parenrightbig/parenrightbigg
≤2Eǫ∼{±1}n˜τ/parenleftbigg
BL−1sup
/bardbl˜v/bardbl2≤1,˜f∈VL−2/summationdisplay
j∈[n]ǫj˜v⊤˜f(xj)/parenrightbigg
= 2Eǫ∼{±1}n˜τ/parenleftbigg
BL−1sup
/bardbl˜v/bardbl2≤1,˜f∈VL−2˜v⊤/summationdisplay
j∈[n]ǫj˜f(xj)/parenrightbigg
≤2Eǫ∼{±1}n˜τ/parenleftbigg
BL−1sup
˜f∈VL−2/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫj˜f(xj)/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
.
We can apply the above inequality recursively and derive
Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫj˜f(xj)/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
≤2L−1Eǫ∼{±1}n˜τ/parenleftbigg
BL−1···B1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg
.
Furthermore, by Eq. (D.4) we know
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj) =τ−1τ/parenleftbigg
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)/parenrightbigg
≤τ−1/parenleftbigg
Eǫ∼{±1}n˜τ/parenleftbigg
sup
˜f∈VL−1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫj˜f(xj)/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg/parenrightbigg
≤τ−1/parenleftbigg
2L−1Eǫ∼{±1}n˜τ/parenleftbigg
BL−1···B1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddouble/vextenddouble
2/parenrightbigg/parenrightbigg
=τ−1/parenleftbigg
2L−1Eǫ∼{±1}nexp/parenleftbigg
dλΛ2B2
LB2
L−1···B2
1/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddouble/vextenddouble2
2/parenrightbigg/parenrightbigg
,
where the last identity follows from the deﬁnition of ˜ τ. Letλ0=dλΛ2B2
LB2
L−1···B2
1. Then
Eǫ∼{±1}nexp/parenleftig
λ0/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddouble/vextenddouble2
2/parenrightig
=Eǫ∼{±1}nexp/parenleftig
λ0/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2+2λ0/summationdisplay
1≤i<j≤nǫiǫjx⊤
ixj/parenrightig
≤exp/parenleftig
λ0/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightig
Eǫ∼{±1}nexp/parenleftig
2λ0/summationdisplay
1≤i<j≤nǫiǫjx⊤
ixj/parenrightig
.
We chooseλ=1
8esdΛ2B2
LB2
L−1···B2
1, wheres=/parenleftbig/summationtext
1≤i<j≤n(x⊤
ixj)2/parenrightbig1
2.Then it is clear λ0=1
8es. We can
apply Lemma D.5 to derive that
Eǫ∼{±1}nexp/parenleftig
2λ0/summationdisplay
1≤i<j≤nǫiǫjx⊤
ixj/parenrightig
≤2
and therefore
Eǫ∼{±1}nexp/parenleftig
λ0/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddouble/vextenddouble2
2/parenrightig
≤2exp/parenleftig
λ0/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightig
.
26We knowτ−1(x) =/radicalbig
λ−1logx. It then follows that
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤/parenleftbigg
λ−1(L−1)log2+λ−1logEǫ∼{±1}nexp/parenleftig
λ0/vextenddouble/vextenddouble/vextenddouble/summationdisplay
j∈[n]ǫjxj/vextenddouble/vextenddouble/vextenddouble2
2/parenrightig/parenrightbigg1
2
≤/parenleftbigg
λ−1(L−1)log2+λ−1log/parenleftig
2exp/parenleftig
λ0/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightig/parenrightig/parenrightbigg1
2
=/parenleftbigg
λ−1Llog2+λ−1λ0/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightbigg1
2
=/parenleftbigg
8esdΛ2B2
LB2
L−1···B2
1Llog2+dΛ2B2
LB2
L−1···B2
1/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightbigg1
2
=√
dΛBLBL−1···B1/parenleftbigg
8esLlog2+/summationdisplay
j∈[n]/ba∇dblxj/ba∇dbl2
2/parenrightbigg1
2
.
The proof is completed by noting 8 e(log2)≤16.
Remark D.6.Our proof of Proposition 5.5 is motivated by the arguments in Golowich et al. [16], which
studies Rademacher complexity bounds for DNNs with d= 1. Our analysis requires to introduce
techniques to handle the diﬃculty of considering dfeatures simultaneously. Indeed, we control the
Rademacher complexity for learning with dfeatures by
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤Eǫ/parenleftbigg
sup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg1
2
.
Ifd= 1, this becomes
Eǫsup
f∈F/summationdisplay
t∈[d]/summationdisplay
j∈[n]ǫt,jft(xj)≤Eǫsup
˜f∈VL−1,˜v:/bardbl˜v/bardbl2≤1/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫjσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle,
and the arguments in Golowich et al. [16] apply. There are two diﬃcultie s in applying the arguments
in Golowich et al. [16] to handle general d∈N. First, the term/summationtext
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationtext
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsinglecannot
be written as a Rademacher complexity due to the summation over t∈[d]. Second, there is a square
function of the term/vextendsingle/vextendsingle/vextendsingle/summationtext
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle. To handle this diﬃculty, we introduce the function
τ(x) = exp(λx2) instead of the function τ(x) = exp(λx) in Golowich et al. [16]. To this aim, we need
to handle the moment generation function of a Rademacher chaos v ariable/summationtext
1≤i<j≤jǫiǫj(x⊤
ixj)2,
which is not a sub-Gaussian variable. As a comparison, the analysis in G olowich et al. [16] considers
the moment generation function for a sub-Gaussian variable. One c an also use the following inequality
/parenleftbigg/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle2/parenrightbigg1
2
≤/summationdisplay
t∈[d]/vextendsingle/vextendsingle/vextendsingle/summationdisplay
j∈[n]ǫt,jσ(˜v⊤˜f(xj))/vextendsingle/vextendsingle/vextendsingle,
the latter of which can then be further controlled by the argument s in Golowich et al. [16]. This,
however, incurs a bound with a linear dependency on d. As a comparison, our analysis gives a bound
with a square-root dependency on d.
27E A General Vector-contraction Inequality for Rademacher
Complexities
In this section, we provide a general vector-contraction inequalit y for Rademacher complexities,
which recovers Lemma A.1 with τ(a) =a. The lemma is motivated from Lemma D.4 by considering
a general convex and nondecreasing τ.
Theorem E.1. LetFbe a class of bounded functions f:Z /ma√sto→Rdwhich contains the zero function.
Letτ:R+→R+be a continuous, non-decreasing and convex function. Assum e˜g1,...,˜gn:Rd→R
areG-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl2and satisfy ˜gi(0) = 0. Then
Eǫ∼{±1}nτ/parenleftig
sup
f∈Fn/summationdisplay
i=1ǫi˜gi(f(xi))/parenrightig
≤Eǫ∼{±1}ndτ/parenleftig
G√
2sup
f∈Fn/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/parenrightig
.(E.1)
The following lemma is due to [28]. We provide here the proof for complet eness.
Lemma E.2. LetFbe a class of functions f:Z /ma√sto→Rdandgbe any functional deﬁned on F. Assume
that˜g1,...,˜gn:Rd→RareG-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl2. Then,
Eǫ∼{±1}nsup
f∈F/bracketleftbig
g(f)+n/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
≤Eǫ∼{±1}ndsup
f∈F/bracketleftig
g(f)+G√
2n/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightig
.(E.2)
Proof.We prove this result by induction. According to the symmetry betwe enfand˜f, we derive
Eǫnsup
f∈F/bracketleftbig
g(f)+n/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
=1
2sup
f,˜f∈F/bracketleftig
g(f)+g(˜f)+n−1/summationdisplay
i=1ǫi˜gi(f(xi))+n−1/summationdisplay
i=1ǫi˜gi(˜f(xi))+ ˜gn(f(xn))−˜gn(˜f(xn))/bracketrightig
=1
2sup
f,˜f∈F/bracketleftig
g(f)+g(˜f)+n−1/summationdisplay
i=1ǫi˜gi(f(xi))+n−1/summationdisplay
i=1ǫi˜gi(˜f(xi))+/vextendsingle/vextendsingle˜gn(f(xn))−˜gn(˜f(xn))/vextendsingle/vextendsingle/bracketrightig
,(E.3)
According to the Lipschitz property and Eq. (D.1), we derive
/vextendsingle/vextendsingle˜gn(f(xn))−˜gn(˜f(xn))/vextendsingle/vextendsingle≤G/vextenddouble/vextenddoublef(xn)−˜f(xn)/vextenddouble/vextenddouble
2≤G√
2Eǫn,1,...,ǫn,j/vextendsingle/vextendsingled/summationdisplay
j=1ǫn,j/bracketleftbig
fj(xn)−˜fj(xn)/bracketrightbig/vextendsingle/vextendsingle.
Plugging the above inequality back into (E.3) and using the Jensen’s ine quality, we get
Eǫnsup
f∈F/bracketleftbig
g(f)+n/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
≤1
2Eǫn,1,...,ǫn,jsup
f,˜f∈F/bracketleftig
g(f)+g(˜f)+n−1/summationdisplay
i=1ǫi˜gi(f(xi))+n−1/summationdisplay
i=1ǫi˜gi(˜f(xi))+G√
2/vextendsingle/vextendsingled/summationdisplay
j=1ǫn,j/bracketleftbig
fj(xn)−˜fj(xn)/bracketrightbig/vextendsingle/vextendsingle/bracketrightig
=1
2Eǫn,1,...,ǫn,jsup
f,˜f∈F/bracketleftig
g(f)+g(˜f)+n−1/summationdisplay
i=1ǫi˜gi(f(xi))+n−1/summationdisplay
i=1ǫi˜gi(˜f(xi))+G√
2d/summationdisplay
j=1ǫn,j/bracketleftbig
fj(xn)−˜fj(xn)/bracketrightbig/bracketrightig
=Eǫn,1,...,ǫn,jsup
f∈F/bracketleftig
g(f)+n−1/summationdisplay
i=1ǫi˜gi(f(xi))+G√
2d/summationdisplay
j=1ǫn,jfj(xn)/bracketrightig
,
where we have used the symmetry in the second step.
The stated result can be derived by continuing the above deduction with expectation over ǫn−1,
ǫn−2and so on.
28To prove Theorem E.1, we introduce the following lemmas on the appro ximation of a continuous,
non-decreasing and convex function. Let a+= max{a,0}.
Lemma E.3. Letf: [a,b]→R+be a continuous, non-decreasing and convex function and m≥2.
Leta=x1<···<xm=b. Then the function ˜g: [a,b]→Rdeﬁned by
˜g(x) =f(xk)+f(xk+1)−f(xk)
xk+1−xk(x−xk),ifx∈[xk,xk+1]
belongs to the set
H[a,b]:=/braceleftig
c0+m/summationdisplay
i=1ci(x−ti)+:ci≥0,i∈[n],ti∈R,m∈N,x∈[a,b]/bracerightig
. (E.4)
Proof.Deﬁne
¯f(x) =f(x1)+m−1/summationdisplay
i=1f(xi+1)−f(xi)
xi+1−xi/bracketleftbig
(x−xi)+−(x−xi+1)+/bracketrightbig
.
We ﬁrst show that ¯f(x) = ˜g(x) for allx∈[a,b]. Suppose that x∈[xk,xk+1). Then, it is clear that
¯f(x) =f(x1)+k−1/summationdisplay
i=1f(xi+1)−f(xi)
xi+1−xi/bracketleftbig
(x−xi)+−(x−xi+1)+/bracketrightbig
+f(xk+1)−f(xk)
xk+1−xk/bracketleftbig
(x−xk)+−(x−xk+1)+/bracketrightbig
+m−1/summationdisplay
i=k+1f(xi+1)−f(xi)
xi+1−xi/bracketleftbig
(x−xi)+−(x−xi+1)+/bracketrightbig
=f(x1)+k−1/summationdisplay
i=1f(xi+1)−f(xi)
xi+1−xi/bracketleftbig
(x−xi)−(x−xi+1)/bracketrightbig
+f(xk+1)−f(xk)
xk+1−xk/bracketleftbig
(x−xk)−0/bracketrightbig
=f(xk)+f(xk+1)−f(xk)
xk+1−xk(x−xk) = ˜g(x).
We now show that ¯f(x) belongs to the set H[a,b]. Indeed, it follows from ( x−xm)+= 0 for all
x≤xm=bthat
¯f(x) =f(x1)+m−1/summationdisplay
i=1f(xi+1)−f(xi)
xi+1−xi(x−xi)+−m/summationdisplay
i=2f(xi)−f(xi−1)
xi−xi−1(x−xi)+
=f(x1)+f(x2)−f(x1)
x2−x1(x−x1)++m−1/summationdisplay
i=2/bracketleftigf(xi+1)−f(xi)
xi+1−xi−f(xi)−f(xi−1)
xi−xi−1/bracketrightig
(x−xi)+.
Therefore, ¯f(x)canbewrittenas ¯f(x) =c0+/summationtextm−1
i=1ci(x−ti)+withti=xi,c0=f(x1),c1=f(x2)−f(x1)
x2−x1
andci=f(xi+1)−f(xi)
xi+1−xi−f(xi)−f(xi−1)
xi−xi−1,i= 2,...,m−1. The terms c1,...,c m−1are all non-negative
sincefis non-decreasing and convex. The proof is completed.
Lemma E.4. Iff: [a,b]→R+is continuous, non-decreasing and convex, then fbelongs to the closure
ofH[a,b]deﬁned in Eq. (E.4).
Proof.Letm∈N. We can ﬁnd a=x(m)
1<x(m)
2<···<x(m)
n+1=bsuch that
f(x(m)
k)−f(x(m)
k−1)≤f(b)−f(a)
n.
Introduce
f(m)(x) :=f(x(m)
k)+f(x(m)
k+1)−f(x(m)
k)
x(m)
k+1−x(m)
k(x−x(m)
k) ifx∈[x(m)
k,x(m)
k+1].
29For anyx∈[x(m)
k,x(m)
k+1], it follows from the convexity of fthat
|f(m)(x)−f(x)|=/vextendsingle/vextendsingle/vextendsinglef(x(m)
k)−f(x)+/parenleftbig
f(x(m)
k+1)−f(x(m)
k)/parenrightbig
(x−x(m)
k)
x(m)
k+1−x(m)
k/vextendsingle/vextendsingle/vextendsingle
=f(x(m)
k)−f(x)+/parenleftbig
f(x(m)
k+1)−f(x(m)
k)/parenrightbig
(x−x(m)
k)
x(m)
k+1−x(m)
k
≤/parenleftbig
f(x(m)
k+1)−f(x(m)
k)/parenrightbig
(x−x(m)
k)
x(m)
k+1−x(m)
k≤f(b)−f(a)
n,
from which we knowlim n→∞|f(m)(x)−f(x)|= 0 for allx∈[a,b]. Lemma E.3showsthat f(m)∈H[a,b]
for allm∈N. Therefore, fbelongs to the closure of H[a,b]. The proof is completed.
Proof of Theorem E.1. According to the boundedness assumption of f∈ Fand the fact 0∈ F, there
existB >0 such that
0≤min/braceleftig
sup
f∈Fn/summationdisplay
i=1ǫi˜gi(f(xi)),G√
2sup
f∈Fn/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracerightig
≤max/braceleftig
sup
f∈Fn/summationdisplay
i=1ǫi˜gi(f(xi)),G√
2sup
f∈Fn/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracerightig
≤B
for allǫ∈ {±1}n. Lett∈Rbe an arbitrary number. Deﬁne gt:F /ma√sto→Rbygt(f) = 0 for any f/ne}ationslash=0
andgt(0) =t. It is clear that
Eǫ∼{±1}nsup
f∈F/bracketleftbig
gt(f)+n/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
=Eǫ∼{±1}nmax/braceleftig
sup
f∈F:f/ne}ationslash=0/bracketleftbign/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
,t/bracerightig
and
Eǫ∼{±1}ndsup
f∈F/bracketleftbig
gt(f)+G√
2n/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightbig
=Eǫ∼{±1}ndmax/braceleftig
G√
2 sup
f∈F:f/ne}ationslash=0/bracketleftbign/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightbig
,t/bracerightig
.
Plugging the above identities into (E.2) with g=gtgives
Eǫ∼{±1}nmax/braceleftig
sup
f∈F:f/ne}ationslash=0/bracketleftbign/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
,t/bracerightig
≤Eǫ∼{±1}ndmax/braceleftig
G√
2 sup
f∈F:f/ne}ationslash=0/bracketleftbign/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightbig
,t/bracerightig
.
Ift≥0, the above inequality is equivalent to
Eǫ∼{±1}nmax/braceleftig
sup
f∈F/bracketleftbign/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
,t/bracerightig
≤Eǫ∼{±1}ndmax/braceleftig
G√
2sup
f∈F/bracketleftbign/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightbig
,t/bracerightig
(E.5)
by noting ˜gi(0) = 0 for all i∈Nn. Ift<0, it follows from (E.2) with g(f) = 0 that
Eǫ∼{±1}nmax/braceleftig
sup
f∈F/bracketleftbign/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
,t/bracerightig
=Eǫ∼{±1}nsup
f∈F/bracketleftbign/summationdisplay
i=1ǫi˜gi(f(xi))/bracketrightbig
≤Eǫ∼{±1}ndsup
f∈F/bracketleftig
G√
2n/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightig
=Eǫ∼{±1}ndmax/braceleftig
G√
2sup
f∈F/bracketleftbign/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightbig
,t/bracerightig
,
30where we have used ˜ gi(0) = 0 for all i∈Nnin the ﬁrst identity. That is, (E.5) holds for all t∈R.
Subtracting tfrom both sides of Eq. (E.5) gives
Eǫ∼{±1}n/parenleftig
sup
f∈Fn/summationdisplay
i=1ǫi˜gi(f(xi))−t/parenrightig
+≤Eǫ∼{±1}nd/parenleftig
G√
2sup
f∈F/bracketleftbign/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/bracketrightbig
−t/parenrightig
+,∀t∈R,(E.6)
from which we know
Eǫ∼{±1}n˜τ/parenleftig
sup
f∈Fn/summationdisplay
i=1ǫi˜gi(f(xi))/parenrightig
≤Eǫ∼{±1}nd˜τ/parenleftig
G√
2sup
f∈Fn/summationdisplay
i=1d/summationdisplay
j=1ǫi,jfj(xi)/parenrightig
,∀˜τ∈H[0,B].
According to Lemma E.4, we know τ: [0,B]→R+belongs to the closure of H[0,B]. Therefore, Eq.
(E.1) holds. The proof is completed.
F Lipschitz Continuity of Loss Functions
The following proposition is known in the literature [26]. We prove it for completeness.
Proposition F.1. (a) Letℓbe deﬁned as Eq. (3.2). Thenℓis1-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl∞
and1-Lipschitz continuous w.r.t. /ba∇dbl·/ba∇dbl2.
(b) Letℓbe deﬁned as Eq. (3.3). Thenℓis1-Lipschitz continuous w.r.t. /ba∇dbl · /ba∇dbl∞and1-Lipschitz
continuous w.r.t. /ba∇dbl·/ba∇dbl2.
Proof.We ﬁrst prove Part (a). For any vandv′, we have
|ℓ(v)−ℓ(v′)|=/vextendsingle/vextendsinglemax/braceleftbig
0,1+max
i∈[k]{−vi}/bracerightbig
−max/braceleftbig
0,1+max
i∈[k]{−v′
i}/bracerightbig/vextendsingle/vextendsingle
≤ |max
i∈[k]{−vi}−max
i∈[k]{−v′
i}| ≤max
i∈[k]|vi−v′
i|=/ba∇dblv−v′/ba∇dbl∞,
where we have used the elementary inequality
|max
i∈[k]ai−max
i∈[k]bi| ≤max
i∈[k]|ai−bi|.
This proves Part (a).
We now prove Part (b). It is clear that
∂ℓ(v)
∂vi=−exp(−vi)
1+/summationtext
i∈[k]exp(−vi).
Therefore, the ℓ1norm of the gradient can be bounded as follows
/ba∇dbl∇ℓ(v)/ba∇dbl1≤1
1+/summationtext
i∈[k]exp(−vi)/summationdisplay
i∈[k]exp(−vi)≤1.
This proves Part (b). The proof is completed.
31