Generative Emotional AI for Speech Emotion Recognition:
The Case for Synthetic Emotional Speech Augmentation
Abdullah Shahida, Siddique Latifb,<and Junaid Qadirc
aInformation Technology University (ITU),Punjab, Pakistan
bUniversity of Southern Queensland, Australia
cQatar University, Doha, Qatar
ARTICLE INFO
Keywords :
Tacotron,WaveRNN,speechsynthesis,
text-to-speech, emotional speech syn-
thesis, speech emotion recognitionABSTRACT
Despiteadvancesindeeplearning,currentstate-of-the-artspeechemotionrecognition(SER)systems
stillhavepoorperformanceduetoalackofspeechemotiondatasets. Thispaperproposesaugmenting
SERsystemswithsyntheticemotionalspeechgeneratedbyanend-to-endtext-to-speech(TTS)system
basedonanextendedTacotronarchitecture. TheproposedTTSsystemincludesencodersforspeaker
andemotionembeddings,asequence-to-sequencetextgeneratorforcreatingMel-spectrograms,anda
WaveRNNtogenerateaudiofromtheMel-spectrograms. Extensiveexperimentsshowthatthequality
of the generated emotional speech can signiï¬cantly improve SER performance on multiple datasets,
as demonstrated by a higher mean opinion score (MOS) compared to the baseline. The generated
samples were also eï¬€ective at augmenting SER performance.
1. Introduction
Speech emotion recognition (SER) is a rapidly growing
ï¬eldwithmanyapplicationsinï¬eldssuchashealthcare,cus-
tomer service, media, education, and forensics. While deep
learning (DL) has shown promise in developing SER sys-
tems, their performance is still limited by the scarcity of
emotion datasets [20, 24]. Existing SER corpora are small
since the process of creating emotional data is costly and
time-consuming, as multiple annotators have to manually
listentoandannotatethematerial[26,31]. Toincreasedata
size,somestudieshaveusedmultiplecorpora,butthenum-
berofstandardbenchmarkdatasetsisalsolimited,hindering
progress in SER systems [19].
Researchershavelongbeeninterestedincreatingnatural-
sounding TTS systems. TTS technology has come a long
way from early TTS systems that often used pre-recorded
waveforms pieced together based on input text [11]. Such
systems were prone to boundary artefact issues and statis-
tical techniques were later developed to generate smoothed
audiofeaturesforthevocodertosynthesisespeech[37,44].
Morerecently,end-to-endneuralnetwork-basedapproaches
havebeenproposedthatcansynthesisemorenatural-sounding
humanspeech[3,34]. Currentstate-of-the-artTTSsystems
are trained using DL algorithms in an end-to-end fashion,
withpopularmodelsincludingTacotron[41],Deepvoice[3],
Fastspeech [33, 34], Fastpitch [18], to name a few.
Unlike traditional systems, end-to-end TTS models can
learn to generate a spectrogram directly from text without
any complex pre-processing. These models, however, are
currentlyonlyabletosynthesisenaturalspeech. Usinggen-
erative DL techniques such as generative adversarial net-
works (GANs) [8] for emotional speech synthesis is also
challenging, as it requires a large amount of time-aligned
data of a single speaker speaking the same content in dif-
<Corresponding author
ORCID(s):ferent emotions and complex equations to guide the model
in converting emotions using audio features. Some studies
haveachievedpromisingresultsinsingle-speakeremotional
speech synthesis using TTS models [17], but the quality of
syntheticspeechinaugmentingSERhasnotbeenevaluated.
Inthispaper,weproposeamethodforaugmentingSER
systemsusinganemotionaltext-to-speech(TTS)systemand
maketwomaincontributions. Firstly,wedevelopan end-to-
end multi-speaker emotional TTS system that does not re-
quire any alignment of audio ï¬les for emotion conversion
or complex pre-processing of input data. Inspired by the
success of end-to-end TTS models, we adopt a similar ar-
chitecture to Tacotron. We propose to use a condition en-
coder to control the speakersâ€™ voices and emotions in the
output speech. We generate speaker voice feature vectors
usingtheencodernetwork. Thesefeaturevectorsaremodu-
lated with one of the encoded emotional feature representa-
tions. Thesemodulatedfeaturevectorsareusedtocondition
theTacotrontosynthesisespeechindiï¬€erentspeakervoices
andemotions. Subjectiveevaluationtasksshowthatourpro-
posed model improves controllability and successfully syn-
thesisesemotionalspeech. Secondly,weusethesynthesised
emotional speech to augment an SER system and conduct
multiple experiments to evaluate the generated data quan-
titatively. Results show that the synthesised data can help
improveSERperformanceinbothwithin-corpusandcross-
corpus settings.
The rest of the paper is organised as follows. In Section
2, we brieï¬‚y introduce the related work to change diï¬€erent
features of audio. The modelâ€™s architecture, loss functions,
and ï¬‚ow of our architecture are described in Section 3. The
details of the dataset and experimental condition in which
we trained our model and hyper-parameters are provided in
Section 4. We report our results in Section 5. Finally, this
paper is concluded in Section 6.
A Shahid et al.: Preprint submitted to Elsevier Page 1 of 9arXiv:2301.03751v1  [cs.SD]  10 Jan 2023Multi-Speaker Emotional Speech Synthesis
2. Previous Work
Inthissection,wereviewtheliteraturethathasemerged
around(1)theuseofTacotronforTTS,andfor(2)emotional
speech synthesis, and (3) the process of augmenting SER.
2.1. Tacotron Based TTS Systems
ManyrecentstudieshavefocusedonmodifyingtheTacotron
model in order to better control the output of TTS systems.
For instance, [13] presented a Tacotron-based model that
synthesisesmulti-speakerspeechbyconditioningtheTacotron
onthespeakerâ€™svoiceembedding,whichwasgeneratedfrom
aspeakerveriï¬cationmodel[40]. [42]introducedaTacotron
variant that can change the speaking style, by learning dif-
ferent styles and saving them as vectors or tokens. These
tokens are obtained by clustering similar accents and repre-
senting each cluster with an average. During synthesis, the
Tacotron is conditioned on one of these tokens to produce
speech with a speciï¬c style. [35] presented a multi-speaker
Tacotron that can change accents (e.g., American, Indian,
British). Their model uses two encoder networks with the
Tacotron and requires two audio samples (one for the ac-
cent and one for the speakerâ€™s voice) as input to generate
the desired output. [36] proposed a Tacotron model that is
trainedwithencodedoutputaudiofromavariationalautoen-
coder as input. This not only improves the multi-speaker
performanceofTacotronbutalsoallowsforcontroloverthe
energy of the generated audio through the mean-variance
property of the variational autoencoder. [43] developed a
Tacotron model that can learn more complex vocalisations
by using the self-attention mechanism in Tacotron to learn
complex dependencies related to pitch in diï¬€erent accents.
They claim that their model outperforms traditional end-to-
endapproachesforlanguageswithmorepitch-dependentac-
cents, such as Japanese. Our proposed model also gener-
atesspeechinamulti-speakersettingandincludesadditional
control over the emotions in the output.
2.2. Tacotron Based Emotional TTS Systems
Severalpreviousworkshaveattemptedtogenerateemo-
tional speech using TTS systems. For example, [38] devel-
opedanemotioncontrolmethodforaTTSsystembasedon
the GST-Tacotron network [35], and demonstrated its eï¬€ec-
tivenessinsynthesisingemotionalspeechinasingle-speaker
settinginKorean. [27]alsoevaluatedaTacotron-basedemo-
tionalspeechsynthesizerinKorean,andfoundimprovements
in the quality of the generated speech for a single speaker.
Other studies, such as [15, 17], have also proposed meth-
odsforcontrollingemotionalspeechsynthesis,buttheseap-
proachesonlysynthesiseemotionalspeechinasinglespeakerâ€™s
voice. In contrast, our proposed method achieves control
over emotional speech synthesis for multi-speaker TTS and
we also evaluate the quality of the synthesised data to aug-
ment the SER system.
2.3. Augmenting Techniques for SER
Speed perturbation [16] is a popular data augmentation
technique that has been widely studied in diï¬€erent contexts[2,23]. Ithasbeenfoundtoimprovespeechemotionrecog-
nition (SER) performance by creating copies of input data
withdiï¬€erentspeedeï¬€ects. Mixup[45]isanotherdataaug-
mentation technique that generates augmented samples as a
linear combination of original samples from the input data.
Severalstudieshavedemonstratedtheeï¬€ectivenessofmixup
inSER,includingLatifetal.[25],whousedthetechniqueto
augmentanSERsystemandachieveimprovedperformance
androbustness. ArecentmethodcalledSpecAugment[30],
originally proposed for automatic speech recognition, has
alsobeenappliedtoSER[4]. Inthisstudy,theauthorsaug-
mented the SER system with duplicate samples by a factor
of two and found that SpecAugment improved model per-
formance. Other studies [2, 21, 23] have also achieved im-
proved performance by using input perturbation-based data
augmentation techniques to increase the training data.
Further research is required to explore data-driven ap-
proachestoincreasethetrainingdataforSER.Inthispaper,
weproposetoexploreTTSbaseddataaugmentationmethod
where we explored diï¬€erent variations in the training data
bychangingthespeakerandgendervoicesindiï¬€erentemo-
tions.
3. Proposed Framework
WeproposetogeneratesyntheticspeechusingaTacotron-
based emotional TTS system. We use synthetic speech data
toaugmentthespeechemotionclassiï¬er. Thedetailsofboth
emotional TTS and classiï¬er are presented next.
3.1. Emotional Speech Synthesis
OurmodelconsistsofanencoderwhichconditionsTacotron
(asdepictedinFigure1)toalterthespeakerâ€™svoiceandemo-
tion in the output. Tacotron generates a Mel-spectrogram
fromagiventextandembeddingvector,whileaWave-RNN-
based vocoder is used to generate an audio signal from the
Mel-spectrogram
3.1.1. Condition Encoder
We propose using a condition encoder to create an em-
bedding that represents both speaker identity and emotion.
To do this, we use a speaker identiï¬cation model presented
in[40],whichcreatesaï¬xed-dimensionalembedding,known
asad-vector[10,39],usingasequenceofMel-spectrograms
computedfromaspeechsignalofarbitrarylength. Wetrain
thismodelusinganend-to-endspeakerveriï¬cationlossthat
maximisesthecosinesimilaritybetweenutterancesfromthe
samespeakerwhileminimisingthecosinesimilaritybetween
utterances from diï¬€erent speakers. We ï¬ne-tune this net-
workonanemotionalcorpustocreateanemotionalembed-
ding as well. Thus, the condition encoder is optimised to
maximise the cosine similarity between embeddings of the
same speaker with diï¬€erent emotions and to minimise the
similarity between diï¬€erent emotions and diï¬€erent speak-
ers. Inthisway,themodellearnstogenerateafeaturevector
thatcontainsbothemotionandspeakeridentityinformation.
Thespeakerâ€™svoiceaudioandemotionaudioareembedded
A Shahid et al.: Preprint submitted to Elsevier Page 2 of 9Multi-Speaker Emotional Speech Synthesis
Figure 1: Architectural ï¬‚ow diagram. The reference speakerâ€™s voice is ï¬rst encoded and then modulated to desired emotion as
described in the model schema. The output is then passed to the Tacotron decoder with the text embedding to synthesise the
Mel-spectrogram.
using the condition encoder and combined to generate a ï¬-
nalembedding,whichisusedtoconditionthesynthesizerto
outputspeechwiththeselectedemotionandspeakerâ€™svoice.
For each unique emotion of every speaker in dataset, a
centroidckiscalculatedbytakingtheaverageofembedding
for each unique emotion of every unique speaker. Loss for
an embedding eiwhen the embedding and the centroid ck
have the same speaker and emotion is calculated as:
L.ei;ck/ = *1 Â.cos.ei;ck// (1)
Wheneihavediï¬€erentemotionordiï¬€erentspeakerforcen-
troidckthen loss is calculated as:-
L.ei;ck/ =.cos.ei;ck// (2)
LG.S/ =Ã‰
i;kL.ei;ck/ (3)
Equation1maximisesthecosinesimilaritybetweenem-
beddingsforthesamespeakervoiceandsameemotion. Equa-
tion 2 represents the cosine similarity between embedding
andcentroidwhentheyhavediï¬€erentspeakervoicesordif-
ferentemotionsorboth. Equation3representstheï¬nalloss
overeveryembedding,whichiscalculatedasthesumofthe
loss for every embedding with every centroid.
The condition encoder consists of three LSTM layers
with 768 cells each, and a ï¬nal 256-length fully connected
layer. TheinputtothemodelistheMel-spectrogramgener-
ated from a speech utterance of a reference speakerâ€™s audio
sample, and its output is an embedding vector of size 256
which represents the speakerâ€™s identity. After training the
model, we use it to extract speaker and emotional informa-
tion from a given audio. To separate the emotion from thespeakerâ€™s voice, we generate vectors that only contain emo-
tionalinformationbyusingthetrainedconditionencoderto
generate embedding vectors for both the neutral and emo-
tional voices of the same speaker. The neutral embedding
vectoristhensubtractedfromtheemotionalonesusingEqua-
tion (4), resulting in a vector that only contains emotional
information. This vector can be used at inference time to
control the emotion of the synthesised audio.
embem= .emben*embneu/ (4)
Whereembenrepresents the embedding with emotion and
voice information generated from the emotional voice of a
speaker;embneuisgeneratedfromneutralaudioofthesame
speaker,and embemrepresentstheembeddingthatonlycon-
tainsemotionalinformation. Duringinference,referenceau-
dio embedding (voice in which we want our output sample
to be synthesised) and emotional embedding are added to
generate a ï¬nal embedding vector.
embï¬nal=embref+embem (5)
Finally, the modulated embedding vector and text are fed
to Tacotron, which generates the Mel-spectrograms. These
Mel-spectrograms are converted to the time domain using a
vocoder, resulting in an audio signal.
3.1.2. Synthesizer architecture
The synthesizer is a variation of Tacotron [41], which
is a sequence-to-sequence model that generates output one
frameatatimebasedontheinput. Inaddition,wecondition
this synthesizer on an embedding vector generated by the
conditionencoder,whichcontainsinformationaboutthede-
siredoutputemotionandthespeakerâ€™svoice. Thecondition
embedding is concatenated with the text embedding of the
synthesizerandthenpassedthroughadecodertosynthesise
A Shahid et al.: Preprint submitted to Elsevier Page 3 of 9Multi-Speaker Emotional Speech Synthesis
theoutputMel-spectrogram. Thesynthesizerwastrainedon
80-channel Mel-spectrograms with a window size of 50 ms
andahopsizeof12.5ms. Thesynthesizerencodestheinput
charactersintoahiddenrepresentationusingthreeconvolu-
tion layers, which learn longer-term context like an n-gram.
The output of these convolution layers is passed to a single
bi-directionalLSTMlayerwith256units,whichlearnstime
dependencies from these n-gram-like features. The LSTM
layer returns an encoded vector that fully represents the in-
puttextsequence. Thisvectorisconcatenatedwithavector
of emotional and speaker embeddings from the encoder.
Itisworthnotingthatatthispoint,theencoderhasbeen
trained and its weights are not updated. The combined text,
speaker,andemotionembeddingispassedtothedecoderto
generate a Mel-spectrogram. The decoder architecture in-
cludes a location-sensitive attention mechanism that trans-
forms the input embedding into a ï¬xed-length vector. The
output frame from the previous step is passed through two
fullyconnectedlayersandconcatenatedwiththeembedding
vector to ensure that sequences are generated without any
timeartefacts. ThisvectoristhenpassedthroughtwoLSTM
layers,andalineartransformationisappliedtogeneratethe
next frame of the Mel-spectrogram. The output from this
LSTMisalsoprojecteddowntoasinglescalar,whichserves
asastoptokenandindicateswhentostopgeneratingfurther
frames. Once the Mel-spectrogram has been generated, it
is passed through a 5-layer convolution network called the
PostNet to improve overall reconstruction.
3.1.3. Vocoder
Traditionally, the Griï¬ƒn-Lim algorithm [9] was used to
generate time-domain audio from a spectrogram, but it was
slow and the output speech lacked naturalness. To address
this, we use a vocoder based on the WaveRNN architecture
[14],whichisafasterandmorepowerfulrecurrentnetwork
for sequential modelling of high-ï¬delity audio. It employs
residual convolutions and GRU layers to generate a time-
domainaudiosignalframebyframefromaMel-spectrogram.
3.2. Emotion Classiï¬er
Toevaluatethesynthesisedemotions,wetrainedadeep
neural network (DNN) for SER. We implemented a convo-
lutionalneuralnetwork(CNN)-basedclassiï¬erthatconsists
of a convolutional layer, a batch normalisation layer, and a
denselayerbeforethesoftmaxlayer. Mel-frequencycepstral
coeï¬ƒcients (MFCCs) are used as the input to the classiï¬er.
TheCNNlayerslearnhigh-levelfeaturesfromtheinputfea-
tures, which are then transformed by the dense layer into a
more discriminative space for better emotion classiï¬cation
after passing through the normalisation layer.
4. Experimental Protocol
This section describes the details of the dataset, input
feature, and model training.4.1. Datasets
We used the Librispeech dataset [29] to train our TTS
model. Itconsistsof1000hoursofspeechdatafromvarious
speakers, sampled at 16 kHz. For the emotion embeddings,
we used the Emotional Voices Database (EVD) [1] and the
TorontoEmotionalSpeechSet(TESS)[7],whichcontainsix
diï¬€erent speakers reading diï¬€erent sentences with diï¬€erent
emotions. We conducted multiple experiments to evaluate
the performance of our model. For emotion classiï¬cation
experiments, we used the Ryerson Audio-Visual Database
ofEmotionalSpeechandSong(RAVDESS)[28]andTESS.
Forcross-corpusemotionclassiï¬cation,weusedtheCREMA-
D[6],SAVEE[12],EmoDB[5],andsynthesisedaudio. The
details of these datasets are presented in Table 1. We used
one speaker from Librispeech, as well as all the speakers
from EVD and TESS with two samples that were not in-
cluded in the training set, to determine the mean opinion
score. Foremotionclassiï¬cationexperiments,weusespeaker-
independentemotionclassiï¬cation. Werandomlyselect70%
of CREMA-D for training, 10% for validation, and 20% for
testing. The full corpora including RAVDESS and EmoDB
wereusedasthetestsetintheemotionclassiï¬cationexperi-
ments,andtheSAVEEdatasetwasusedasthetestsetinthe
cross-corpus emotion classiï¬cation experiments.
4.2. Input Features
Tacotrontakestextstringsasinput,whicharesequences
of characters. Each character is encoded into a one-hot en-
coded vector and embedded in a continuous vector. The
otherinputtoTacotronisaconditionembeddingvectorthat
contains speaker and emotion information. This vector is
obtained from an encoder, which takes speaker audio as in-
put and converts it into Mel-frequency cepstral coeï¬ƒcients
(MFCCs). TheseMFCCshave40logï¬lterbanks,80frames,
andnooverlappingwindow. Togeneratet-distributedstochas-
ticneighbourembedding(t-SNE)plotsofsynthesisedaudio,
weencodedoursynthesisedaudiousingthemodelpresented
in [13]. The input to this model is also MFCCs with 40 log
ï¬lter banks, 80 frames, and no overlapping window, result-
ing in an 80x40-dimensional feature vector. This model is
alsousedinevaluatingtheequalerrorrate(EER)inspeaker
veriï¬cation. Inemotionclassiï¬cationandcross-corpusemo-
tion classiï¬cation, we use MFCCs with 40 log ï¬lter banks
andahopsizeof64milliseconds. TheMFCCarrayistrans-
posed and the arithmetic mean is calculated across its hori-
zontal axis as in a previous work [32].
4.3. Speech Synthesis Models Training
First, the encoder is trained on the Librispeech dataset
to learn to generate a speaker embedding that is distinct for
each speaker. It takes a Mel-spectrogram as input and out-
puts an embedding vector of size 256. From these embed-
dingvectors,asimilaritymatrixisconstructedsuchthateach
column contains an embedding vector for a unique speaker,
andcosinesimilarityismaximisedinallcellsofthecolumns
and minimised in all cells of the rows. Cosine similarity is
maximised along the columns because they contain audio
embeddings for the same person, whereas it is minimised
A Shahid et al.: Preprint submitted to Elsevier Page 4 of 9Multi-Speaker Emotional Speech Synthesis
Table 1
Description of all the considered datasets.
NameNumber of
SpeakersNumber of
Utterances
CREMA-D 91 7,442
EmoDB 10 535
EVD 5 7,590
Librispeech 2484 281,241
REVDESS 24 7,356
SAVEE 4 480
TESS 2 2,800
along the rows because they contain audio embeddings for
diï¬€erent people. In this way, the embeddings of the same
people are similar and those of diï¬€erent people are diï¬€er-
ent.
After training the encoder on the Librispeech data, it is
ï¬ne-tuned on the EVD and TESS datasets to generate dis-
tinct embedding vectors for diï¬€erent emotions. This time,
a similarity matrix is constructed such that a column con-
tains embedding vectors generated for a single emotion for
the same speaker, and other emotions are placed in other
columns. Thisisdoneforallspeakers,andthencosinesim-
ilarity is maximised along a column and minimised across
columns. This is done to increase the distance between dif-
ferent emotions of the same person, so cosine similarity is
minimised by adding it across columns rather than within
the same columns. We used a batch size of 30 and a learn-
ing rate of 10*4.
Duringtraining,thesynthesizermodelisï¬rsttrainedon
the Librispeech data so that it can learn to generate audio
of diï¬€erent speakers from a diverse range of text. This is
becausetheEVDandTESSdatasetscombinedonlyhavesix
speakers. Once the synthesizer is trained enough that it can
generate audio resembling the reference speaker, we ï¬ne-
tune it to generate diï¬€erent emotional Mel-spectrograms by
trainingitontheEVDandTESSdatasets. Weusealearning
rateof 103thatexponentiallydecaysto 10*5,andabatchsize
of30fortrainingthesynthesizer. TheAdamoptimiserwith
1= 0:9,2= 0:999,and= 10*6isusedastheoptimiser.
The teacher forcing ratio is set to 1 (meaning the original
previous sequence is shown to the model for prediction of
the next sequence). The mean squared error is minimised
for the predicted Mel-spectrogram.
5. Results
In this section, we evaluate the performance of our pro-
posed model in terms of the similarity of the synthesized
speakers and the granularity of synthesized emotions.
5.1. Evaluating Synthetic Speech Quality
Toevaluatethequalityofsyntheticspeech,weconducted
multiple experiments. The details of these experiments are
presented below.Table 2
Speaker veriï¬cation EERs of diï¬€erent synthesizers.
# of samples EER
Emotion + voice conversion TTS 100 0.16
Baseline Emotion conversion TTS 100 0.24
Voice conversion TTS 100 0.10
Real audios 100 0.04
Table 3
Mean Opinion Score (MOS) with 95% conï¬dence interval.
Emotion Angry Happy SadNeutral Overall
Recorded 4.6 4.50 4.504.60 4.55
Baseline 2.80 3.10 2.704.20 3.20
Proposed 3.60 3.70 3.804.10 3.80
5.1.1. Speaker Veriï¬cation
We evaluated the speaker similarity of synthesised au-
dios with real speech using speaker veriï¬cation and mea-
sured the equal error rate (EER) following [13]. The EER
isusedtomeasuretheperformanceofaspeakerveriï¬cation
systembycomparingthefalserejectrate(FRR)andfalseac-
ceptrate(FAR)atdiï¬€erentsensitivitylevels. TheEERisthe
pointatwhichtheFRRandFARareequal. Tocalculatethe
EER,weused100audiosamples,40ofwhichweresynthe-
sised. We enrolled only synthesised speakers in the system
and calculated the EER. We achieved an EER of 0.10% by
performingvoiceconversionusingamulti-speakerTacotron
model[13]. Wealsogeneratedemotionalaudiosamplesus-
ingabasemodel,andthespeakerveriï¬cationmodelgavean
EER of 0.24% on these synthesised audios. In contrast, we
achieved an EER of 0.16% when using the proposed model
for both emotion and voice conversion. The EER on real
samples using the approach in [13] was 0.04%. We have
compared the EER of these models in Table 2.
5.1.2. Listening Experiments
We performed mean opinion score (MOS) evaluations
tomeasurethequalityofsynthesisedspeech. Weaskedsub-
jectswithpost-graduateexposuretogiveascoreafterlisten-
ing to the audio based on the following standard: 1 = Bad;
2 = Poor; 3 = Fair; 4 = Good; and 5 = Excellent. The re-
sults, shown in Table 3, indicate that the proposed model
can synthesise high-quality emotional speech compared to
the baseline model. The proposed model signiï¬cantly im-
proves the MOS score for emotions including angry, sad,
and happy compared to the baseline. However, it achieves
slightly lower MOS scores for natural speech compared to
the baseline. This may be because the baseline model is
speciï¬cally designed to generate natural speech and there-
fore performs better for neutral speech. Nevertheless, our
proposedmodelperformswellforallemotions. Readerscan
listen to samples of the generated speech at this URL1.
1https://emotaco.github.io/Emotional_Tacotron/
A Shahid et al.: Preprint submitted to Elsevier Page 5 of 9Multi-Speaker Emotional Speech Synthesis
Figure 2: Comparison of target and synthesized Mel-spectrograms for various emotions in Male and Female audios.
5.1.3. Speaker and Emotion Visualisation
During this experiment, we did not use teacher forcing
and generated audio as described in the inference part. The
synthesised Mel-spectrograms for diï¬€erent emotions by the
baseline and proposed models were plotted in Figure 2, and
theresultswerecomparedwiththetargetMel-spectrograms.
Incontrasttothebaseline,ourproposedmodeldidnotsmooth
the generated Mel-spectrograms that help produce a better
quality of emotional speech using WaveRNN vocoder.
Forthepurposeofevaluation,wepresentthet-SNEplot,
which was generated by embedding vectors generated from
synthesisedoutputsamplesusingaspeakerveriï¬cationmodel
astheencoder. Notethatthespeakerencoderwasnottrained
with the synthesizer, so it is not optimised for synthesizer
output. We generated t-SNE plots for emotional audio syn-
thesisedusingthemodelfromthebasepapersandcompared
the results with the proposed model. These t-SNE plots for
synthesisedspeechinbothmaleandfemalevoicesareshown
inFigure3and4,respectively. Theseplotsdemonstratethat
our model is able to synthesise distinct emotions compared
tothebasemodel. Itcanbeobservedthatdiï¬€erentemotions
areseparatedandsimilaremotionsareclusteredtogether,in-
dicating similarity between emotions.
Sincetheangryemotionhasmoreexpressioncompared
to the sad and happy emotions, which are tone variations,
theclusterofangryemotionsisfartherfromthehappyemo-
tions. Wealsovisualisethet-SNEplotofmultiplespeakers
in neutral speech using our proposed model in Figure 5. It
shows distinct clusters for diï¬€erent speakers indicating that
the model is able to learn the multiple speaker embeddings
eï¬€ectively.
.
5.2. Augmenting Speech Emotion Recognition
(SER)
Inthissection,weusedthesyntheticspeechtoaugment
theSERsystem. Weperformedourevaluationsusingcorpus
Figure 3: Comparison of t-SNE plots of male audio for various
emotions using baseline and our proposed model shows that
our model demonstrates better emotion performance.
andcross-corpussettings. Resultsfortheseexperimentsare
presented next.
5.2.1. Within Corpus Evaluations
We used the RAVDESS and TESS datasets for evalua-
tions. We combined both datasets and then randomly split
the data into a ratio of 70:10:20 for train, validation, and
test sets, respectively. We trained the model for 45 epochs.
Wecomparedtheresultsforspeakerrecognitiononrealand
synthesised speech in Figure 6. We achieved an accuracy
of 80% for synthesised speech, while the accuracy for the
real speech test set was 92.4%. This demonstrates that our
model can synthesise the emotional characteristics of out-
put speech. We also augmented the classiï¬er with synthetic
dataandperformedtrainingusingbothrealandsynthesised
speech data. We achieved an accuracy of 94.6%, which is
better compared to the classiï¬er trained on real data alone.
This experiment shows that our model can also be used to
A Shahid et al.: Preprint submitted to Elsevier Page 6 of 9Multi-Speaker Emotional Speech Synthesis
Figure 4: Comparison of t-SNE plots of female audio for var-
ious emotions using baseline and our proposed model shows
that our model demonstrates better emotion performance.
Figure 5: The t-SNE plot for speaker voice of synthesised re-
sults shows that individual speakersâ€™ voices are distinctly clus-
tered together.
generateadditionalaudiodatawhichcanbeusedtoaugment
speaker recognition systems to improve their performance.
Figure 6: Bar plot which shows that our synthesized audioâ€™s
emotion and real audio emotions are almost similarly classiï¬ed
by the classiï¬cation model.
We have also plotted confusion matrices in Figure 7 foremotion classiï¬cation on real audio, synthetic audio, and a
combination of real and synthetic data in the training set.
Theconfusionmatrixshowsthatthemodelaugmentedwith
syntheticdataisabletobetterclassifyspeechemotions. The
accuracy of other emotions has also been improved, but the
most signiï¬cant improvement can be seen in the classiï¬ca-
tion of happy emotions.
5.2.2. Cross-Corpus Corpus Evaluations
We also evaluated the eï¬€ect of augmenting with syn-
thetic data by performing cross-corpus emotion classiï¬ca-
tion. Todothis,weimplementedaclassiï¬erconsistingofan
LSTMlayer,threedenselayers,andasoftmaxlayerforemo-
tionclassiï¬cation. Wealsousedtwodropoutlayersbetween
dense layers to learn more generalised representations. We
selected the architecture of the model based on previous re-
searchï¬ndings[22,23]. Wetrainedtheclassiï¬eronMFCC
featuresextractedfromtheinputaudio. Themodelwastrained
with a sparse categorical cross-entropy loss and Adam op-
timiser for 100 epochs. The model was trained using the
CREMA-D dataset and the CREMA-D dataset augmented
with synthetic data and was evaluated on the CREMA-D,
SAVEE,andEMODBdatasets. Theresults,showninFigure
8, demonstrate that adding synthesised data increases accu-
racy not only on the SAVEE and EMODB datasets without
ï¬ne-tuning the model but also on the CREMA-D test set as
well.
5.2.3. Changing Gender and Speaker Distributions
In this experiment, we compare the results of data aug-
mentation with new speaker voices that are not present in
the given corpus. For instance, the SAVEE corpus has four
malespeakers,andsyntheticdatacanbecreatedeitherinthe
voices of these four male speakers or in the voices of addi-
tionalmaleandfemalespeakerstobringdiversitytothedata
and augment speech emotion classiï¬cation. We present the
resultsinTable4. Wecomparedtheresultswiththebaseline
model, which was trained without any augmentation, and
also with the application of speed perturbation to the train-
ing data. We followed [19] and created two copies of aug-
mentedsamplesusingthespeedperturbationdataaugmenta-
tiontechnique. Wefoundthataugmentingthedatawithdif-
ferent speaker voices helps improve performance compared
tothebaselineandthewidelyuseddataaugmentationtech-
nique of speed perturbation.
6. Conclusions
Thispaperproposestoutiliseanemotionaltext-to-speech
(TTS)systemtoaugmentaspeechemotionrecognition(SER)
system. We present a Tacotron-based multi-speaker emo-
tional TTS system for synthetic speech generation in dif-
ferent speaker voices and use it for data augmentation in
speech emotion recognition to improve performance. The
results showed that the proposed TTS system can generate
high-quality emotionally discriminative samples. When we
augmenttheSERsystemwiththeseaugmentedsamples,we
ï¬nd that using synthetic data in diï¬€erent emotional voices
A Shahid et al.: Preprint submitted to Elsevier Page 7 of 9Multi-Speaker Emotional Speech Synthesis
Figure 7: Confusion matrix for the test set of real, synthetic, and combined synthetic and real audio. The addition of synthetic
data improves emotion classiï¬cation.
Table 4
Results using diï¬€erent distributions of synthetic data for speakers and gender
DatasetAccuracy (%)
BaselineSpeed perturbation
augmentationMale spakers
synthetic dataFemale speakers
synthetic dataBoth female and
male synthetic data
SAVEE 65.4 66.8 68.2 69.4 72.3
CREMA-D 68.3 70.1 72.7 72.9 74.3
Figure 8: Test results in cross-corpus setting, which shows
improvements when the model is augmented with synthetic
data.
canhelpimproveperformancecomparedtothewidelyused
speechdataaugmentationtechniqueinSER.Ourfuturework
will focus on investigating the learning of a uniï¬ed embed-
dingforcontrollingstyleandemotionsforallpeople,regard-
less of age, background, and gender.
References
[1] Adigwe, A., Tits, N., Haddad, K.E., Ostadabbas, S., Dutoit, T.,
2018. The emotional voices database: Towards controlling the
emotion dimension in voice generation systems. arXiv preprint
arXiv:1806.09514 .[2] Aldeneh,Z.,Provost,E.M.,2017. Usingregionalsaliencyforspeech
emotion recognition, in: 2017 IEEE international conference on
acoustics, speech and signal processing (ICASSP), IEEE. pp. 2741â€“
2745.
[3] Arik, S.O., Chrzanowski, M., Coates, A., Diamos, G., Gibian-
sky, A., Kang, Y., Li, X., Miller, J., Ng, A., Raiman, J., et al.,
2017. Deep voice: Real-time neural text-to-speech. arXiv preprint
arXiv:1702.07825 .
[4] Baird, A., Amiriparian, S., Milling, M., Schuller, B.W., 2021. Emo-
tionrecognitioninpublicspeakingscenariosutilisinganlstm-rnnap-
proach with attention, in: 2021 IEEE Spoken Language Technology
Workshop (SLT), IEEE. pp. 397â€“402.
[5] Burkhardt,F.,Paeschke,A.,Rolfes,M.,Sendlmeier,W.F.,Weiss,B.,
2005. A database of german emotional speech, in: Ninth European
Conference on Speech Communication and Technology.
[6] Cao, H., Cooper, D.G., Keutmann, M.K., Gur, R.C., Nenkova, A.,
Verma, R., 2014. Crema-d: Crowd-sourced emotional multimodal
actorsdataset. IEEEtransactionsonaï¬€ectivecomputing5,377â€“390.
[7] Dupuis, K., Pichora-Fuller, M.K., 2010. Toronto emotional speech
set (tess)-younger talker_happy .
[8] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley,
D.,Ozair,S.,Courville,A.,Bengio,Y.,2014. Generativeadversarial
nets. Advances in neural information processing systems 27.
[9] Griï¬ƒn, D., Lim, J., 1984. Signal estimation from modiï¬ed short-
timefouriertransform. IEEETransactionsonAcoustics,Speech,and
Signal Processing 32, 236â€“243.
[10] Heigold, G., Moreno, I., Bengio, S., Shazeer, N., 2016. End-to-end
text-dependentspeakerveriï¬cation,in: 2016IEEEInternationalCon-
ferenceonAcoustics,SpeechandSignalProcessing(ICASSP),IEEE.
pp. 5115â€“5119.
[11] Hunt, A.J., Black, A.W., 1996. Unit selection in a concatenative
speechsynthesissystemusingalargespeechdatabase,in: 1996IEEE
International Conference on Acoustics, Speech, and Signal Process-
ing Conference Proceedings, IEEE. pp. 373â€“376.
[12] Jackson, P., Haq, S., 2014. Surrey audio-visual expressed emotion
(savee) database. University of Surrey: Guildford, UK .
A Shahid et al.: Preprint submitted to Elsevier Page 8 of 9Multi-Speaker Emotional Speech Synthesis
[13] Jia, Y., Zhang, Y., Weiss, R., Wang, Q., Shen, J., Ren, F., Nguyen,
P., Pang, R., Moreno, I.L., Wu, Y., et al., 2018. Transfer learning
fromspeakerveriï¬cationtomultispeakertext-to-speechsynthesis,in:
Advances in neural information processing systems, pp. 4480â€“4490.
[14] Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande,
N., Lockhart, E., Stimberg, F., Oord, A.v.d., Dieleman, S.,
Kavukcuoglu, K., 2018. Eï¬ƒcient neural audio synthesis. arXiv
preprint arXiv:1802.08435 .
[15] Kim, T.H., Cho, S., Choi, S., Park, S., Lee, S.Y., 2020. Emo-
tional voice conversion using multitask learning with text-to-speech,
in: ICASSP2020-2020IEEEInternationalConferenceonAcoustics,
Speech and Signal Processing (ICASSP), IEEE. pp. 7774â€“7778.
[16] Ko,T.,Peddinti,V.,Povey,D.,Khudanpur,S.,2015. Audioaugmen-
tationforspeechrecognition,in: SixteenthAnnualConferenceofthe
International Speech Communication Association.
[17] Kwon, O., Jang, I., Ahn, C., Kang, H.G., 2019. An eï¬€ective style
token weight control technique for end-to-end emotional speech syn-
thesis. IEEE Signal Processing Letters 26, 1383â€“1387.
[18] Åa/uni0144cucki,A.,2021. Fastpitch: Paralleltext-to-speechwithpitchpre-
diction, in: ICASSP 2021-2021 IEEE International Conference on
Acoustics,SpeechandSignalProcessing(ICASSP),IEEE.pp.6588â€“
6592.
[19] Latif, S., 2020. Deep representation learning for improving speech
emotion recognition. Doctoral Consortium, Interspeech 2020.
[20] Latif, S., CuayÃ¡huitl, H., Pervez, F., Shamshad, F., Ali, H.S., Cam-
bria, E., 2022a. A survey on deep reinforcement learning for audio-
based applications. Artiï¬cial Intelligence Review , 1â€“48.
[21] Latif, S., Khalifa, S., Rana, R., Jurdak, R., 2020a. Federated
learning for speech emotion recognition applications, in: 2020 19th
ACM/IEEE International Conference on Information Processing in
Sensor Networks (IPSN), IEEE. pp. 341â€“342.
[22] Latif,S.,Qadir,J.,Bilal,M.,2019a.Unsupervisedadversarialdomain
adaptationforcross-lingualspeechemotionrecognition,in: 20198th
international conference on aï¬€ective computing and intelligent inter-
action (ACII), IEEE. pp. 732â€“737.
[23] Latif, S., Rana, R., Khalifa, S., Jurdak, R., Epps, J., 2019b. Di-
rect Modelling of Speech Emotion from Raw Speech, in: Proc. In-
terspeech 2019, pp. 3920â€“3924. URL: http://dx.doi.org/10.21437/
Interspeech.2019-3252 , doi: 10.21437/Interspeech.2019-3252 .
[24] Latif, S., Rana, R., Khalifa, S., Jurdak, R., Qadir, J., Schuller, B.W.,
2021. Survey of deep representation learning for speech emotion
recognition. IEEE Transactions on Aï¬€ective Computing .
[25] Latif, S., Rana, R., Khalifa, S., Jurdak, R., Schuller, B.W., 2020b.
Deep architecture enhancing robustness to noise, adversarial attacks,
and cross-corpus setting for speech emotion recognition. Proc. Inter-
speech 2020 , 2327â€“2331.
[26] Latif, S., Rana, R., Khalifa, S., Jurdak, R., Schuller, B.W., 2022b.
Multitask learning from augmented auxiliary data for improving
speech emotion recognition. IEEE Transactions on Aï¬€ective Com-
puting .
[27] Lee, Y., Rabiee, A., Lee, S.Y., 2017. Emotional end-to-end neural
speech synthesizer. arXiv preprint arXiv:1711.05447 .
[28] Livingstone, S.R., Russo, F.A., 2018. The ryerson audio-visual
database of emotional speech and song (ravdess): A dynamic, multi-
modal set of facial and vocal expressions in north american english.
PloS one 13, e0196391.
[29] Panayotov, V., Chen, G., Povey, D., Khudanpur, S., 2015. Lib-
rispeech: anasrcorpusbasedonpublicdomainaudiobooks,in: 2015
IEEE international conference on acoustics, speech and signal pro-
cessing (ICASSP), IEEE. pp. 5206â€“5210.
[30] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D.,
Le, Q.V., 2019. Specaugment: A simple data augmentation method
for automatic speech recognition. Proc. Interspeech 2019 , 2613â€“
2617.
[31] Parthasarathy, S., Busso, C., 2020. Semi-supervised speech emotion
recognitionwithladdernetworks. IEEE/ACMtransactionsonaudio,
speech, and language processing 28, 2697â€“2709.
[32] de Pinto, M.G., Polignano, M., Lops, P., Semeraro, G., 2020. Emo-tions understanding model from spoken language using deep neu-
ral networks and mel-frequency cepstral coeï¬ƒcients, in: 2020 IEEE
Conference on Evolving and Adaptive Intelligent Systems (EAIS),
IEEE. pp. 1â€“5.
[33] Ren,Y.,Hu,C.,Tan,X.,Qin,T.,Zhao,S.,Zhao,Z.,Liu,T.Y.,2020.
Fastspeech2: Fastandhigh-qualityend-to-endtexttospeech,in: In-
ternational Conference on Learning Representations.
[34] Ren,Y.,Ruan,Y.,Tan,X.,Qin,T.,Zhao,S.,Zhao,Z.,Liu,T.Y.,2019.
Fastspeech: Fast,robustandcontrollabletexttospeech,in: Advances
in Neural Information Processing Systems, pp. 3171â€“3180.
[35] Skerry-Ryan, R., Battenberg, E., Xiao, Y., Wang, Y., Stanton, D.,
Shor,J.,Weiss,R.J.,Clark,R.,Saurous,R.A.,2018. Towardsend-to-
end prosody transfer for expressive speech synthesis with Tacotron.
arXiv preprint arXiv:1803.09047 .
[36] Sun,G.,Zhang,Y.,Weiss,R.J.,Cao,Y.,Zen,H.,Wu,Y.,2020.Fully-
hierarchical ï¬ne-grained prosody modeling for interpretable speech
synthesis, in: ICASSP 2020-2020 IEEE International Conference on
Acoustics,SpeechandSignalProcessing(ICASSP),IEEE.pp.6264â€“
6268.
[37] Tokuda, K., Yoshimura, T., Masuko, T., Kobayashi, T., Kitamura,
T., 2000. Speech parameter generation algorithms for HMM-
based speech synthesis, in: 2000 IEEE International Conference on
Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.
00CH37100), IEEE. pp. 1315â€“1318.
[38] Um, S.Y., Oh, S., Byun, K., Jang, I., Ahn, C., Kang, H.G., 2020.
Emotional speech synthesis with rich and granularized control, in:
ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), IEEE. pp. 7254â€“7258.
[39] Variani, E., Lei, X., McDermott, E., Moreno, I.L., Gonzalez-
Dominguez, J., 2014. Deep neural networks for small footprint text-
dependent speaker veriï¬cation, in: 2014 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP), IEEE.
pp. 4052â€“4056.
[40] Wan, L., Wang, Q., Papir, A., Moreno, I.L., 2018. Generalized end-
to-endlossforspeakerveriï¬cation,in: 2018IEEEInternationalCon-
ferenceonAcoustics,SpeechandSignalProcessing(ICASSP),IEEE.
pp. 4879â€“4883.
[41] Wang, Y., Skerry-Ryan, R., Stanton, D., Wu, Y., Weiss, R.J.,
Jaitly, N., Yang, Z., Xiao, Y., Chen, Z., Bengio, S., et al., 2017.
Tacotron: Towards end-to-end speech synthesis. arXiv preprint
arXiv:1703.10135 .
[42] Wang, Y., Stanton, D., Zhang, Y., Skerry-Ryan, R., Battenberg, E.,
Shor, J., Xiao, Y., Ren, F., Jia, Y., Saurous, R.A., 2018. Style to-
kens: Unsupervisedstylemodeling,controlandtransferinend-to-end
speech synthesis. arXiv preprint arXiv:1803.09017 .
[43] Yasuda, Y., Wang, X., Takaki, S., Yamagishi, J., 2019. Investiga-
tionofenhancedTacotrontext-to-speechsynthesissystemswithself-
attention for pitch accent language, in: ICASSP 2019-2019 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE. pp. 6905â€“6909.
[44] Zen,H.,Tokuda,K.,Black,A.W.,2009. Statisticalparametricspeech
synthesis. speech communication 51, 1039â€“1064.
[45] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D., 2018. mixup:
Beyondempiricalriskminimization,in: InternationalConferenceon
Learning Representations.
A Shahid et al.: Preprint submitted to Elsevier Page 9 of 9