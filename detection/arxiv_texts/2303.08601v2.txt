GCRE-GPT: A Generative Model for Comparative Relation Extraction
Yequan Wang1, Hengran Zhang2,3, Aixin Sun4, Xuying Meng2
1Beijing Academy of Artificial Intelligence, Beijing, China
2Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
3University of Chinese Academy of Sciences, Beijing, China
4School of Computer Science and Engineering, Nanyang Technological University, Singapore
tshwangyequan@gmail.com, axsun@ntu.edu.sg, {zhanghengran22z,mengxuying}@ict.ac.cn
Abstract
Given comparative text, comparative relation
extraction aims to extract two targets ( e.g., two
cameras) in comparison and the aspect they
are compared for ( e.g., image quality). The
extracted comparative relations form the basis
of further opinion analysis. Existing solutions
formulate this task as a sequence labeling task,
to extract targets and aspects. However, they
cannot directly extract comparative relation(s)
from text. In this paper, we show that compara-
tive relations can be directly extracted with high
accuracy, by generative model. Based on GPT-
2, we propose a Generation-based Comparative
Relation Extractor (GCRE-GPT). Experiment
results show that GCRE-GPT achieves state-of-
the-art accuracy on two datasets.
1 Introduction
Comparison is a common linguistic expression.
Kessler and Kuhn (2013) reports that 10% of texts
contain at least one comparison. A typical compar-
ative relation contains two targets and one aspect,
e.g., “D80 vs. D70 in terms of weight” where the
two targets are two cameras and the aspect to com-
pare is weight. An extracted comparative relation
is the basis for opinion mining in further analysis.
Hence, it is crucial to accurately extract compara-
tive relations, to support product/service compari-
son to better inform consumers and enterprises.
Given a piece of text ( e.g., a sentence), denoted
byS= [w1, w2, . . . , w n], the task of comparative
relation extraction is to predict the comparative re-
lation(s) expressed in the given text. Each relation
is a 3-tuple: (t1, t2, a), where t1andt2are targets
in comparison, and adenotes the corresponding
aspect for which the two targets are compared. In
some studies, the two targets are further classified
into subject and object (Liu et al., 2021; Arora
et al., 2017; Kessler and Kuhn, 2013; Jindal and
Liu, 2006; Wang et al., 2017). In our problem def-
inition, we do not distinguish such roles between
the two targets.Existing solutions for comparative relation ex-
traction use rules or sequence labeling models. Jin-
dal and Liu (2006) extract pre-defined comparative
tuples using rules based on opinion words. Kessler
and Kuhn (2013) recognize comparative predicate
by semantic role labeling method. Furthermore,
CRF (Wang et al., 2017) and LSTM (Arora et al.,
2017) have also been used to extract the compara-
tive elements. Liu et al. (2021) use sequence label-
ing method to obtain elements, then use Cartesian
product to compute all relations. Here, multiple
relations in a single sentence may be combined
by Cartesian product. Nevertheless, multiple rela-
tions in long texts ( e.g., a paragraph) may not be
logically combined. Although sequence labeling
models perform well at sentence level, they can-
not directly extract relation tuples. Instead, they
extract targets and aspects and combine them into
relation tuples. The permutation and combination
of elements bring in a lot of complexity. More im-
portantly, many reviews in real life contain more
than one relation. This calls for a fundamentally
different approach to extract comparative relations.
Pre-trained language models, e.g., GPT (Rad-
ford et al., 2019) and BERT (Devlin et al., 2019),
have enabled generative models to achieve promis-
ing results on many tasks. Examples include ma-
chine translation (Zhang et al., 2021; Tu et al.,
2016), question answer (Duan et al., 2017), and
generation-based dialog system (Tu et al., 2022). In
this paper, we make the first attempt to extract com-
parative relations with generative model. Specifi-
cally, we propose Generation-based Comparative
Relation Extractor based on GPT -2, namely
GCRE-GPT, to extract comparative relations from
text. We represent a comparative relation tuple in a
piece of text, and train our model to generate such
texts, based on an input sentence. Experiments
show that GCRE-GPT achieves state-of-the-art per-
formance. Further, GCRE-GPT performs best on
texts containing multiple relations.arXiv:2303.08601v2  [cs.CL]  27 Jun 20242 Related Work
Most existing studies cast the task of comparative
relation extraction as a task of comparative element
extraction. Relations are then built on top of the
extracted elements. Accordingly, many solutions
are based on sequence labeling models (Jindal and
Liu, 2006; Kessler and Kuhn, 2013; Wang et al.,
2017; Arora et al., 2017; Liu et al., 2021). These
models are fundamentally different from ours, as
we adopt the generative approach with pre-trained
language models.
As a well known pre-training language model,
BERT (Devlin et al., 2019) has contributed to sig-
nificant performance increase on various NLP tasks.
Based on BERT, Liu et al. (2021) propose a mul-
tiple stage model to analyze comparative opinion.
The comparative elements are firstly extracted, fol-
lowed by building comparative relations. We in-
stead adopt a generative approach, built on top of
GPT-2. GPT-2 (Radford et al., 2019) adopts generic
transformer structure (Zhang et al., 2020) and en-
codes text from left to right. The auto-regressive
and unidirectional structure make it a strong per-
former on generation tasks.
3 GCRE-GPT
The architecture of the proposed GCRE-GPT is de-
picted in Figure 1. Rooted in GPT-2, GCRE-GPT
has two main modules: (i) encoder with prompt
words, and (ii) comparative aware decoder. Next,
we detail the design of GCRE-GPT and its opti-
mization objective.
3.1 Encoder with Prompt Words
To better extract comparative relations, we design
a simple yet effective prompt words injection layer.
Given an input sentence, S= [w1, w2,···, wn],
we use prompt words injection layer to obtain a
new input:
S′= [w1, w2,···, wn, tp], (1)
where tpdenotes the injected span of prompt words.
Then we represent this input by using Encoder:
r=Encoder ([w1, w2,···, wn, tp]), (2)
where rrefers to the encoded representation of S′.
3.2 Comparative Aware Decoder
The comparative aware decoder is built on top of
the decoder in GPT-2. The main add-on to the
Sentence
Encoder
Encoder with Prompt W ords[ CLS]  D80 vs.D70 in battery
Comparative Aware DecoderSelf AttentionAdd & NormFeed ForwardAdd & NormD80 vs. D70 in battery [ EOS]
Filter Layer
Prompt Word 
tp
S
Figure 1: The architecture of GCRE-GPT.
original decoder is a filter layer. Next, we detail
the format of the generated text, then explain the
filter layer.
Format of the generated text. To better utilize
the capability of text generation of pre-trained lan-
guage models, we formulate our task of compar-
ative relation extraction as a text generation task.
Specifically, a target comparative relation (t1, t2, a)
to be extracted from input text, becomes a target
piece of text “ t1vs.t2ina” to be generated by
the model. Then, we use the decoder to generate
text word by word, starting with a [CLS] token (see
Figure 1):
Pi=p(ri|rj<i, S′), (3)
where Pidenotes the probability distribution of
the predicted i-th word, computed based on the
input and the earlier generated sequence. The cor-
responding word wiis obtained by argmax (Pi).
The filter layer. A typical generative model gener-
ates words from its entire vocabulary. Hence, the
model will output words that are not in the original
input text. In our problem setting, these words are
not part of the expected results. To guarantee that
all elements of the generated relations are from the
original text, we design a filter layer, to filter all
relations generated:
Sg= [···,reli,···], (4)
where relidenotes the i-th generated relation and
all elements of it ( i.e.,two targets t1t2, and aspect
a) must be from the original text S. Otherwise, the
generated relation is discarded.
3.3 Training Objective
The objective of the GCRE-GPT is to predict the
transferred comparative relation, from the inputTable 1: Statistics of the two dataests CameraReview
and CompSent-08. The train/dev/test split is 7:1:2 for
both datasets.
Instance CameraReview CompSent-08
Sentence 1279Train:895
628Train:440
Dev:128 Dev:63
Test:256 Test:125
Relation 1780Train:1218
751Train:522
Dev:172 Dev:74
Test:390 Test:155
text. We adopt objective L(θ)by simply using the
auto-regressive cross-entropy:
L(θ) =−NcX
i=1log(p(ri|rj<i, S′)), (5)
where θdenotes the parameters of the model. Nc
represents the total length of the transferred com-
parative relations.
As discussed earlier, a sentence may contain mul-
tiple comparative relations, making the generation
problem complicated. In this paper, we use a sim-
ple way to guide the proposed model. For simple
comparative relation, the order of targets t1andt2
follows their relative positions in the original text.
If there are multiple relations, we prioritize t1then
t2, ift1is consistent in multiple relations.
4 Experiments
4.1 Dataset
CameraReview. This is a manually annotated
dataset, containing camera reviews (Kessler and
Kuhn, 2014).1CameraReview is currently the
largest dataset of comparative sentences, contain-
ing1,279comparative sentences and 1,780rela-
tions. The comparative relation is in the format
of (subject, object, opinion words). In this dataset,
74.4% sentences each contains one relation, 17.4%
sentences contain two relations each, and remain-
ing sentences contain more than two relations each.
CompSent-08. This dataset is constructed
by Ganapathibhotla and Liu (2008), which con-
tains both comparative and non-comparative sen-
tences.2We select the comparative sentences from
this dataset in our experiments. In this dataset,
85.7% sentences each contains one relation, 11.6%
1https://wiltrud.hwro.de/research/
data/reviewcompandcontanarisons.html
2https://www.cs.uic.edu/~liub/FBS/
sentiment-analysis.html#datasetsTable 2: Precision, recall and F1on both datasets. All
baseline methods are our own implementation.
CameraReview
Model Precision Recall F1
CRF 0.151 0.215 0.178
BERT 0.289 0.414 0.341
BERT-CRF 0.279 0.454 0.345
GPT-2 0.375 0.284 0.323
GCRE-GPT 0.419 0.329 0.368
CompSent-08
Model Precision Recall F1
CRF 0.000 0.000 0.000
BERT 0.097 0.097 0.097
BERT-CRF 0.118 0.103 0.110
GPT-2 0.275 0.213 0.240
GCRE-GPT 0.267 0.232 0.248
contain two relations and the remaining contain
more than two.
For both datasets, we randomly split the in-
stances with the ratio of 7:1:2 for training, vali-
dation/development, and testing. Table 1 reports
dataset statistics.
4.2 Compared Methods
We compare GCRE-GPT with the following base-
lines, CRF (Wang et al., 2017), BERT (Devlin et al.,
2019), BERT-CRF. All these models are pipeline
models: the models extract comparative elements
as a sequence labeling task, and then build compar-
ative relations by using Cartesian product.
We also compare GCRE-GPT with GPT-2, the
base model of GCRE-GPT. Note that, we do not
compare our model with traditional rule-based so-
lutions as re-producing such models are time con-
suming. Due to the small dataset size, we do not
train LSTM-based models as well, because LSTM
in general requires a lot of training data.
4.3 Overall Performance
We use Precision, Recall and F1of extracted re-
lation to evaluate the accuracy of all models. It
is worth noting that the two targets in our defined
comparative relation are unordered. An extracted
comparative relation is marked as correct, when
both targets and the aspect are all correct, otherwise
is marked as wrong. Table 2 reports the results of
GCRE-GPT and 4 baselines, on both datasets.
On both datasets, our proposed GCRE-GPT
achieves the best F1. On CameraReview, BERT-
based models outperform CRF model, revealing
the powerful ability of pre-trained language models,Table 3: Precision, recall and F1on CameraReview, by
number of comparative relations Nin each sentence.
N Model Precision Recall F1
1 CRF 0.165 0.238 0.195
BERT 0.236 0.354 0.283
BERT-CRF 0.229 0.392 0.289
GPT-2 0.322 0.320 0.321
GCRE-GPT 0.386 0.392 0.389
2 CRF 0.119 0.138 0.128
BERT 0.348 0.340 0.344
BERT-CRF 0.198 0.277 0.231
GPT-2 0.422 0.287 0.342
GCRE-GPT 0.388 0.277 0.323
>2 CRF 0.152 0.245 0.187
BERT 0.341 0.588 0.432
BERT-CRF 0.428 0.725 0.538
GPT-2 0.537 0.216 0.308
GCRE-GPT 0.600 0.265 0.367
much as expected. Because BERT-based models
first extract comparative elements, then build rela-
tions by combining the extracted elements, these
models produce many relations, leading to higher
recall but lower precision, compared to generative
models GPT-2 and our model. On CompSent-08,
all pipeline models produce low accuracy, com-
pared to generative models. A key reason is the
small dataset size. Sequence labeling methods
typically need many instances for training, and
CompSent-08 does not provide sufficient training
data.
4.4 Number of Comparative Relations
Table 3 reports a detailed look at the results on
CameraReview dataset, a break down of results on
sentences containing 1, 2, or more relations.
GCRE-GPT achieves the best scores by all mea-
sures, on sentences with a single relation. However,
our model does not perform well on sentences con-
taining 2 or more relations. Recall that, in Camer-
aReview dataset, majority (or 74.4%) sentences
contain only one relation. Hence the model is
learned to focus on one relation extraction only,
for a given sentence.
To more properly evaluate the model capabil-
ity of handling multiple relations in the input text,
we augment the model training by concatenating a
pair of sentences in training set, to form a longer
text with multiple relations. Then we evaluate the
model on sentences with two or more relations in
CameraReview. The results are reported in Table 4.
GCRE-GPT outperforms all baselines after train-
ing with augmented data with multiple relations inTable 4: Precision, recall and F1on texts with two or
more comparative relations on CameraReview dataset.
Model Precision Recall F1
BERT 0.118 0.455 0.188
BERT-CRF 0.115 0.444 0.183
GPT-2 0.335 0.237 0.277
GCRE-GPT 0.402 0.290 0.337
Table 5: Precision, recall and F1of models with differ-
ent prompt words on CameraReview dataset.
Prompt Words Pre. Rec. F1
“Let me see:” 0.387 0.141 0.206
“[SEP]” 0.359 0.292 0.322
“generate relations:” 0.367 0.289 0.323
“My name:” 0.360 0.297 0.326
“relations:” 0.393 0.316 0.350
“comparative relations:” 0.410 0.308 0.352
“comparative relations tuple:” 0.419 0.329 0.368
input text.
4.5 Impact of Prompt Words
Existing studies on prompt-based models reveal
that PLM models are heavily affected by the chosen
prompt. In our experiments, we also study the
impact of different prompt words on our model.
Table 5 reports the results.
We list seven popular prompt words in Ta-
ble 5, including “generate relation:”, “Let me see:”,
“My name:”, “relations:”, “comparative relation:”,
“[SEP]” and “comparative relation tuple:”. Among
them, prompt “[SEP]” cannot guide the model to
generate a comparison relationship linguistically.
“Let me see:” and “My name:” are less relevant
to the task, leading to poor results as expected.
Prompt words containing “comparative” lead to
better results than others. The prompt words “com-
parative relation tuple:” achieves the best perfor-
mance on all measures.
5 Conclusion
In this paper, we study comparative relations extrac-
tion and propose GCRE-GPT model. The key idea
of GCRE-GPT is to use generative method to ex-
tract complex multiple comparative relations from
input text. Experiment results show that our pro-
posed model achieves state-of-the-art performance
against strong baselines. More importantly, GCRE-
GPT is able to handle texts containing multiple
comparative relations, after trained with augmented
data.Limitations
In this paper, we make the very first attempt to
perform comparative relation extraction by gener-
ative way. There are two main limitations: data
and model. The size of the datasets used in ex-
periments are small, so we cannot design a model
with more parameters to perform more in-depth
research. Nevertheless, these two are the only two
datasets publicly available to our best knowledge.
Another limitation is the design of the model. This
work only considers discrete prompt words and ex-
plores a small collection of prompt words to get a
local optimum.
For the comparative relation extraction task, an
important problem is the lack of comparative ele-
ments. Our work and existing studies both ignore
this problem. We consider it is an important di-
mension to explore in future, because sometimes
a lacked comparative element may refer to all re-
maining elements. For example, “My iPhone 13 is
the best”. The missing target here refers to all the
remaining phones, and the missing aspect refers to
all aspects in comparison.
References
Jatin Arora, Sumit Agrawal, Pawan Goyal, and Sayan
Pathak. 2017. Extracting entities of interest from
comparative product reviews. In Proceedings of
the2017 ACM onConference onInformation and
Knowledge Management, CIKM 2017, Singapore,
November 06-10,2017, pages 1975–1978. ACM.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings ofthe2019 Conference
oftheNorth American Chapter oftheAssociation
forComputational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis,
MN, USA, June 2-7, 2019, V olume 1(Long and
Short Papers) , pages 4171–4186. Association for
Computational Linguistics.
Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou.
2017. Question generation for question answer-
ing. In Proceedings ofthe2017 Conference on
Empirical Methods inNatural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September
9-11, 2017 , pages 866–874. Association for Compu-
tational Linguistics.
Murthy Ganapathibhotla and Bing Liu. 2008. Min-
ing opinions in comparative sentences. In
COLING 2008, 22nd International Conference
onComputational Linguistics, Proceedings ofthe
Conference, 18-22 August 2008, Manchester, UK,
pages 241–248.Nitin Jindal and Bing Liu. 2006. Mining comparative
sentences and relations. In Proceedings, The
Twenty-First National Conference onArtificial
Intelligence and the Eighteenth Innovative
Applications ofArtificial Intelligence Conference,
July 16-20, 2006, Boston, Massachusetts, USA ,
pages 1331–1336. AAAI Press.
Wiltrud Kessler and Jonas Kuhn. 2013. Detection of
product comparisons - how far does an out-of-the-
box semantic role labeling system take you? In
Proceedings ofthe2013 Conference onEmpirical
Methods inNatural Language Processing, EMNLP
2013, 18-21 October 2013, Grand Hyatt Seattle,
Seattle, Washington, USA, Ameeting ofSIGDAT,
aSpecial Interest Group oftheACL , pages 1892–
1897. ACL.
Wiltrud Kessler and Jonas Kuhn. 2014. A corpus of
comparisons in product reviews. In Proceedings
oftheNinth International Conference onLanguage
Resources andEvaluation, LREC 2014, Reykjavik,
Iceland, May 26-31, 2014 , pages 2242–2248. Euro-
pean Language Resources Association (ELRA).
Ziheng Liu, Rui Xia, and Jianfei Yu. 2021. Comparative
opinion quintuple extraction from product reviews.
InProceedings ofthe2021 Conference onEmpirical
Methods inNatural Language Processing, EMNLP
2021, Virtual Event /Punta Cana, Dominican
Republic, 7-11 November, 2021 , pages 3955–3965.
Association for Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Quan Tu, Yanran Li, Jianwei Cui, Bin Wang, Ji-
Rong Wen, and Rui Yan. 2022. MISC: A
mixed strategy-aware model integrating COMET
for emotional support conversation. In Proceedings
ofthe60th Annual Meeting oftheAssociation
forComputational Linguistics (V olume 1:Long
Papers), ACL 2022, Dublin, Ireland, May 22-27,
2022 , pages 308–319. Association for Computational
Linguistics.
Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua
Liu, and Hang Li. 2016. Modeling coverage for
neural machine translation. In Proceedings of
the54th Annual Meeting oftheAssociation for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, V olume 1:Long Papers . The
Association for Computer Linguistics.
Wei Wang, Guodong Xin, Bailing Wang, Junheng
Huang, and Yang Liu. 2017. Sentiment information
extraction of comparative sentences based on CRF
model. Comput. Sci.Inf.Syst., 14(3):823–837.
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei
Zhai, Jingfang Xu, and Yang Liu. 2021. Neural
machine translation with explicit phrase alignment.
IEEE ACM Trans. Audio Speech Lang. Process. ,
29:1001–1010.Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun
Chen, Chris Brockett, Xiang Gao, Jianfeng Gao,
Jingjing Liu, and Bill Dolan. 2020. DIALOGPT
: Large-scale generative pre-training for conver-
sational response generation. In Proceedings of
the58th Annual Meeting oftheAssociation for
Computational Linguistics: System Demonstrations,
ACL 2020, Online, July 5-10, 2020 , pages 270–278.
Association for Computational Linguistics.