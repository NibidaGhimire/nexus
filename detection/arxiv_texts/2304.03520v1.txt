ON THE SUITABILITY OF REPRESENTATIONS FOR QUALITY
DIVERSITY OPTIMIZATION OF SHAPES
A P REPRINT
Ludovico Scarton
Department of Computer Science
Bonn-Rhein-Sieg University of Applied Sciences
Sankt Augustin, 53757, Germany
ludovico.scarton@smail.inf.h-brs.de
Alexander Hagg
Institute of Technology, Resource and Energy-efﬁcient Engineering (TREE)
Bonn-Rhein-Sieg University of Applied Sciences
Sankt Augustin, 53757, Germany
alexander.hagg@h-brs.de
April 6, 2023
ABSTRACT
The representation, or encoding, utilized in evolutionary algorithms has a substantial effect
on their performance. Examination of the suitability of widely used representations for
quality diversity optimization (QD) in robotic domains has yielded inconsistent results
regarding the most appropriate encoding method. Given the domain-dependent nature of
QD, additional evidence from other domains is necessary. This study compares the impact
of several representations, including direct encoding, a dictionary-based representation,
parametric encoding, compositional pattern producing networks, and cellular automata, on
the generation of voxelized meshes in an architecture setting. The results reveal that some
indirect encodings outperform direct encodings and can generate more diverse solution
sets, especially when considering full phenotypic diversity. The paper introduces a multi-
encoding QD approach that incorporates all evaluated representations in the same archive.
Species of encodings compete on the basis of phenotypic features, leading to an approach
that demonstrates similar performance to the best single-encoding QD approach. This is
noteworthy, as it does not always require the contribution of the best-performing single
encoding.
Keywords encoding, representation, quality diversity, compositional pattern producing networks, cellular
automata, parametric
1 Introduction
QD optimization is an evolutionary paradigm based on the separation of the search space, the genome, and
the niching space, consisting of features that describe particular aspects of the solution, the phenotype. The
algorithms in this paradigm create diverse, high-performing solution sets. In doing so, QD can provide
insights into the underlying structure of the problem, offer many design variants to the user, and increase
the robustness of a solution in dynamic and uncertain environments. It is crucial that the solution encoding,
also called representation, allows the creation of solution sets that are as high-quality and diverse as possible.
QD optimization usually targets engineering solutions of high-dimensional nature, because they encompass
extended phenotypes (Hagg, 2021b) like time-dependent robot walking gait strategies (Cully et al., 2015) or
This is the ﬁnal version and has been accepted for publication at the GECCO conferencearXiv:2304.03520v1  [cs.NE]  7 Apr 2023On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
Cellular Automata Compositional Pattern
Producing NetworksDictionary-based Direct Parametric
air inﬂow
Figure 1: Architectural massing design examples generated by QD using ﬁve representations.
the shape of the air ﬂow around buildings (Hagg et al., 2020). Past research has shown that the encoding of
solutions has a major impact on the performance of evolutionary algorithms (EAs) but usually focuses on
the time to convergence of EAs on one hand, and the quality of solutions on the other, as was done in the
time-quality framework presented in (Rothlauf, 2006). One important ﬁnd was that the convergence time of
EAs tends to grow super-linearly with dimensionality of the encoding (Chen et al., 2015). But in contrast
to other EAs, QD algorithms put a strong focus on the diversity of solutions. This article aims to provide
insights into the impact different encodings have not only on the quality, but also on the diversity of the
solution sets generated with QD.
A comparison between various encodings that produce neural controllers for robots (Tarapore et al., 2016)
gave evidence that generative encodings might actually limit QD, and that in their case, direct encodings
more easily ﬁlled the archive of solutions and produced a solution set that was generally of higher ﬁtness
than when producing it through indirect encoding. They provided evidence to the conclusion that locality,
when small mutations produce small changes in the phenotype, which is the case with direct encodings,
might be more important for QD than for more classical EAs, However, when evolving robot arm shapes,
other work found that indirect encodings allow for further exploration of the design space and improved
ﬁtness (Collins et al., 2019). It is still an open question how encodings behave in QD optimization problems
like shape domains. Does QD beneﬁt more from smooth mutation with direct encodings, or from faster
exploration with indirect encodings?
A typical shape domain is architectural design, where architects push for freedom of creativity within the
boundaries of optimal design. During the massing design phase in architecture, many design requirements
have not yet been fully deﬁned and are discovered (Maher, 2000). While this phase allows the largest design
freedom, the decisions made here constrain the creative freedom and maximum design quality in later
design phases. The massing design phase is therefore an excellent problem to be solved with QD, to inform
urban planners and architects early on in the design process. An important quality metric of a construction
project is its climate impact, for example cold air ﬂow throughput, heat absorption during the day and heat
radiation at night, or wind nuisance. Classically, such climate impacts are considered in later stages of a
design project, especially through ﬂow analyses of a concrete design. This is a place where QD can be used:
early on in the design process.
Due to the high dimensionality of building designs, it can be useful when the encoding maps low-dimensional
genomes onto these high-dimensional phenotypes. By restricting the dimensionality of the genome, indirect
encodings can reduce the convergence time of the search process, so it is expected that indirect encodings
allow ﬁnding high-quality solutions faster. Simultaneously this means that not all phenotypes might be
reached, which might lead to those encodings to produce lower diversity or even lower ﬁtness. A good
encoding uses a small number of genetic dimensions to reach many high-performing solutions. This
alignment between genetic space and the set of useful phenotypes is domain-dependent, which provides
the necessity of this article. This article focuses on encodings that are able to generate voxelized three-
dimensional meshes, a common representation of massing design in architecture. The impacts of a range
of encodings is analyzed with respect to the resulting set diversity and quality. Evidence is given to help
answering the following research questions:
1.Do direct encodings provide higher QD performance than indirect encodings, as was concluded by
Tarapore et al. (2016) (Tarapore et al., 2016)?
2.Does low genetic dimensionality result in higher QD performance, as is suggested by Chen et al.
(2015) (Chen et al., 2015)?
3. What encoding is most suitable for the domain at hand?
4. Do different encodings cover distinct regions in phenotypic space?
5. Does a combination of encodings in QD improve the diversity of the solution set?
2On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
c.voxel mesh b. intermediate phenotype a. genome
decodecold air inﬂow3
2
12
10000000
00 0000
00 0 00
00 0 00
00 0000
0000000
00000000
view from direction of inﬂowd. ﬁtness function
+
Figure 2: An encoding deﬁnes how a genome (a) is decoded into the phenotype (b, c). Illustrated example
contains less grid cells than the actual phenotype used in this article. The ﬁtness of a solution is calculated
by the relative number of built cells in the direction of the cold air inﬂow (d).
To the best of our knowledge, this is the ﬁrst study to systematically compare the set diversity and other
characteristics of widely used encodings for voxelized three-dimensional mesh representations for QD.
We believe that our ﬁndings will be of interest to research on QD in general, and practitioners working in
the area of the built environment, or shape design, speciﬁcally. This work represents an initial study for a
particular domain.
2 Methods
Let us deﬁne the terminology around representations commonly used in EAs, which are inspired by biological
evolution. Representations in EA are described by the genome-phenotype decoding and phenotype-ﬁtness
mapping (Rothlauf, 2006). A phenotype p2P is the expressed solution for a speciﬁc problem domain. Xis
deﬁned as the genetic search space of the EA. For continuous optimization problems, the genome x2X
usually consists of a tuple of real-valued numbers. A phenotype pis encoded into a genome xusing an
encodingE:P!X . The term encoding can also interchangeably be used instead of representation. To
retrievepfrom a genome, a decoding process D:X!P is used.
Depending on what line of thought is followed, the full phenotype in building mass design might either
be the morphology of the design or the in-situ design causing for example a particular air ﬂow around
the building, wind design or speciﬁc heat release at night. In common QD problem domains, the genome-
phenotype (and phenotype-ﬁtness) mapping does not sufﬁce to describe the representation, as we are
interested not only in the ﬁtness metric derived by the behavior in an environment, but by other aspects
of the behavior as well. QD is often used to evolve robot controller behavior strategies or, in our case, a
three-dimensional voxel mesh that acts on a ﬂow. To this end, the deﬁnition of a representation in QD
is expanded to include mapping the phenotype to the extended phenotype (Dawkins, 1982), which also
includes a solution’s “behavior” in an environment (Hagg, 2021a).
The phenotype in this work is deﬁned as a 1114grid of 33meter cells (Figure 2b) which contain height
values. The extended phenotype is a mesh based on the grid where each cell contains a mass of either 0, 3, 6
or 9 meters high (Figure 2c), situated in a ﬂow. The problem domain is inspired by a real world construction
project with urban planning constraints. The dimensionality of the phenotype is 154. The computationally
expensive computational ﬂuid dynamics (CFD) that would be used in city planning and architectural design
optimization is replaced using a very simple surrogate for the experiments in this work: the relative built up
area facing the main direction of cold air inﬂow (Figure 2d). Minimizing this area is a proxy for minimal
air ﬂow impact, good enough to produce realistic and explainable results, while keeping computational
demand low. The most straightforward representation of massing design voxel meshes would be an array of
height values. However, a plethora of more complex representations exist. We give an overview of those
that are commonly used in shape optimization problems similar the three-dimensional voxel domain used
in this article.
In the rest of this section, the QD algorithm and a number of common representations are described, including
their mutation operators. Finally, a multi-encoding QD algorithm is introduced that explores the solution
space using different representations simultaneously.
3On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
2.1 MAP-Elites
One of the ﬁrst QD algorithms, multidimensional archive of phenotypic elites (MAP-Elites) (Cully et al., 2015),
uses a multidimensional grid-like archive as a phenotypic niching space (see Algorithm1). Its dimensions are
deﬁned by phenotypic features, which in our case are the total built area and the number of separate buildings .
After initializing a population of random genomes X, their phenotypes Pare derived through the decoding
function. Based on these phenotypes, the quality (ﬁtness) and features of the individuals can be derived. The
individual genomes are placed in the archive A. New individuals are only accepted if they are better than
the individual that already occupies the niche, or if the niche is empty. The archive serves as a population
from which random parents are drawn. The parents Xpare perturbed using a mutation operator to produce
childrenXc. The childrenXcthen replace those individuals in the archive that belong to the same niche, if
the old niche elites perform worse or the niche is empty. This procedure is repeated for a certain maximum
number of generations, or a stopping criterion is reached.
Algorithm 1 MAP-Elites algorithm
X;A Initialize () .Initialize genomes and archive
procedure MAP -ELITES (X;A)
P D (X) .Decode genomes into phenotypes
f;p Fitness (P) .Get features and performance
A Replace (A;X;f;p) .Replace niches
whilegens<maxGens do
Xp Random (A) .Select random parents
Xc Perturb (Xp) .Perturb parents
Pc D(Xc)
f;p Fitness (Pc)
A Replace (A;Xc;f;p)
end while
end procedure
2.2 Direct Encodings
The simplest encoding type performs a one-to-one mapping of the genome to the phenotype (Figure 3). The
decoding function that translates genomes into phenotypes is deﬁned as follows:
D:p=x;x2X;p2P (1)
This encoding has been the ﬁrst to be widely adapted in EA (Rechenberg, 1973; Schwefel and Schwefel,
1977). For the voxel mesh problem, each height value of the voxel grid would be represented by a number
(usually real-valued but it can be binary as well) and be given its own locus in the genome. The resulting
high dimensionality of the genome makes the search process cumbersome and slow and is not expected to
easily produce high-quality solution sets. Technically, we could expect this encoding to be able to ﬁnd a very
diverse set of solutions, as all potential phenotypes can be reached. However, this is only reasonable as long
as all parts of the search space are reachable and do not have to cross through invalid regions. The search
process could also take an insurmountable amount of time in this high-dimensional space.
x0
xn
genomex0
xnphenotype
Figure 3: Direct encoding: the height values of the cells are directly encoded into the genome.
The height of every grid cell is directly encoded. The dimensionality of this encoding therefore equals the
dimensionality of the phenotype, d= 154 . Solutions are mutated with a small probability p. A mutation
increases or decreases a gene’s value by 1, only allowing values from the minimum to maximum height.
4On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
2.3 Dictionary-based
In order to reduce the dimensionality of the search space, a dictionary of building blocks can be used
to generate solutions (Figure 4). This approach is often used in ﬂoorplan layout design optimization
problems (Modrak et al., 2021). The decoding function is deﬁned as:
D:p=DICT (x);x2N (2)
DICT is a dictionary of building blocks from which a vector of elements pis selected based on an integer-
valued genome x. This method was used for city layout design optimization (Xu et al., 2019) where the
phenotype consists of cells whose state is determined by an element from the dictionary. The dimensionality
of the search space is determined by the size of the dictionary elements.
genomex0
xnx0
xn0 1
7 9
33 35
phenotype
Figure 4: Dictionary encoding: grid cells are grouped and for every group all buildings blocks are precalculated.
The genome encodes which cell group gets assigned which building block.
The dictionary contains building blocks of size nbym, with all possible states of voxel ﬁllings that do
not contain ﬂoating voxels. It can therefore create exactly the same shapes as the direct encoding. The
mutation operator can be used to determine which state transitions are allowed. It is deﬁned such that, with
a small probability p, a random state transition of the gene is performed according to the allowed rules in the
dictionary. A mutation will only change the building block such that the cells in the old and new states have
a Manhattan distance of 5, to provide a more conservative mutation operator.
2.4 Parametric Encodings
Parametric encodings are common in engineering design, as they are easy to understand and allow a high
degree of control. Spline encodings are an example where key points of splines can be moved, which allows
parameters to only move local shape features (Sobieczky, 1999). Other parametric encodings include sets of
coordinates that determine the locations or sizes of a set of cubes (Kaushik and Janssen, 2013). As a trade-off
to the high level of control, these encodings are often quite restricting in terms of the diversity of solutions
that can be produced. To counter this, more generalized parametric encodings have been developed, such as
the additive or subtractive parametric encoding (Wang et al., 2019), by allowing the user to predetermine
global features of the design, such as the number of buildings (Wang et al., 2020). The dimensionality of the
search space of such encodings is generally lower than a direct encoding, but the diversity of the resulting
solution set might suffer.
genomex0y0w0l0h0
xnynwnlnhnh0
x0y0
w0l0
phenotype
Figure 5: Parametric encoding: the genome encodes the position and length sizes of a ﬁxed number of
rectangles.
Many parameterizations are possible but for the problem domain in this article, the following deﬁnition is
used. The decoding process Dgeneratesnrectangles, deﬁned by four parameters: the xandyposition of
the rectangle, having length land width w(Figure 5). The mutation operator, with a small probability p,
adds an integer value according to the values of a rounded up normal distribution with standard deviation
. The rectangles are not allowed to be placed outside of the buildable area. It is ensured that wandlare
non-negative.
5On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
2.5 Compositional Pattern Producing Networks
Compositional pattern producing networks (CPPNs) were introduced as a way to directly insert geometrical
abstractions into developmental encodings (Stanley, 2006). The representation was used on voxel optimiza-
tion problems (Auerbach and Bongard, 2010; Barthet et al., 2022). CPPNs are compositions of a variety of
mathematical functions that produce a phenotype by simple cell-wise query of a substrate. A substrate
assigns coordinates to the phenotype, which can be a grid or locations of motors in a robotical system. The
encoding has been used to create two- and three-dimensional pixel/voxel grids, for example to generate
three-dimensional objects (Clune and Lipson, 2011). The decoding is deﬁned as follows:
D:p(x;y) =G(x;y);whereGa neural graph (3)
The neural graph Greturns a value for each pixel or voxel in the phenotype (Figure 6). CPPNs are usually
evolved using neuroevolution of augmented topologies (NEAT), a neuroevolution method that evolves the
structure of graph objects (Stanley and Miikkulainen, 2002), although any neuroevolution algorithm can be
used.
w0
genomewny
xw0
yx
h
phenotype iterate over all cellswn
Figure 6: CPPN encoding: The weights and activation functions of a ﬁxed neural network are encoded by the
genome.
In the case of a grid-wise phenotype, the ( x,y) coordinates of the grid cells serve as a query input and the
height of the building block is determined by the CPPN. NEAT grows neural graphs and is therefore not
appropriate for the analysis in this article, due to the variable dimensionality of the search space. Instead of
using NEAT, the dimensionality of the search space is controlled by using a ﬁxed architecture for the graph,
containing one or two hidden layers and a preconﬁgured number of hidden neurons. The genome encodes
the weights of this ﬁxed neural network graph consisting of llayers andnneurons that have an activation
function randomly appointed from Gaussian, tanh, sigmoid, sine, cosine, constant zero, constant one and a
step function. With a small probability p, the mutation operator randomly selects a new activation function,
or adds a small value to the weight, drawn from a normal distribution with standard deviation .
2.6 Cellular Automata
Developmental encodings like cellular automata (CA), introduced by Von Neumann (Neumann, 1966),
consist of a rule set and are developed over a number of iterations. This time factor determines how long
the state an object can develop, based on neighborhood rules. In CA, the cells in homogeneous lattice grids
can take on a given ﬁnite number of states, which change according to simple rules in relation to a cell
neighborhood. Commonly used is the Moore neighborhood (Moore, 1962), which consists of the state of
a cell and all its eight neighbors in the previous time step. The extended version allows taking neighbors
that are more than one Chebyshev distance away from the cell Ranjan Nayak et al. (2013). The decoding
process consists of iterating over all cells in the phenotype for titerations, where trepresents the length of
the development process (Figure 7). In each iteration, for each cell, the value of the cell in the next iteration is
calculated as follows, depending on the old values of it and its neighbors:
D:p(x;y)t+1= j+1
j 1i+1
i 1(wi;jp(i;j)t) (4)
In basic CA, the weights wi;jare often binary rules that check whether a neighbor is either on or off,
and determines the new value in a cell accordingly. The most well-known variant is Conway’s Game of
Life (Gardner, 1970) that includes rules about how many neighbor cells are alive or dead. Continuous-valued
weights have been used in neural CA approaches (Nichele et al., 2017; Mordvintsev et al., 2020).
Emerging patterns are difﬁcult to predict, as they evolve over time and are determined by local interactions.
Two main lines of research exist: employing CA either in goal-directed ways or in an open-ended explorative
manner (Herr and Ford, 2016). Predicting the effect of transition rule sets are hard, but this might be beneﬁcial
to the exploratory character of algorithms in QD.
6On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
y
xh0h1h2
h3h4h5
h6h7h8
iterate 1) over all cells 2) for s steps
phenotypew0
genomewnyx
ssum over weighted neighbors 
Figure 7: CA encoding: the weights of the inﬂuence of neighboring cells in a Moore neighborhood are encoded
in the genome. The phenotype is developed in a ﬁxed number of iterations.
This work uses a simple neural cellular automata setup. First, a seed at position (x;y), encoded in the
genome, is set to 1. In order to fully express the phenotype, a cell’s height values are determined based on
an iterative developmental procedure as follows. An extended Moore neighborhood of size nn;n3,
contains weights that are used in Equation 4 to calculate a cell’s value in the next time step. This equation is
applied to all cells for a number of time steps t. The mutation operator, with a small probability pmoves
the seed position’s xandycoordinates by 1. The weights of the Moore neighborhood mask are changed
according to a normal distribution.
2.7 Other Encodings
Other encodings have been applied to similar problem spaces, but are out of the scope of this work. The most
prominently used other encoding are procedural grammar-based, which can use chains or trees of commands
from computer-aided design (CAD)-programs to create objects (Wonka et al., 2003). Graph-based encodings
can also be used to plan a design based on a predeﬁned set of CAD building blocks (Keshavarzi and
Rahmani-Asl, 2021). This article does not consider these software-driven encodings due to their proprietary
nature and licensing models but could be considered in a more extensive analysis in the future.
Deep learning has created major breakthroughs in generative design. Data-driven generative models such as
variational autoencoder (VAE) (Kingma and Welling, 2014) can be used to learn an encoding from data (Gaier
et al., 2020). Although these data-driven generative models can move knowledge from later into earlier
design phases, they depend on the availability of a large amount of training data, which is usually not
publicly available. Especially smaller architectural ﬁrms, communities and projects can therefore not rely
on such techniques and can become more dependent on privatized models. However, QD can be used
as a precomputation method to generate diverse training data sets. The generative model then produces
high quality solutions (Gaier et al., 2020) that can also adhere to design space constraints (Bentley et al.,
2022). Precomputation can solve the problem of large computation time usually seen in evolutionary design
systems (Janssen et al., 2022).
Because of the aforementioned reasons, a large interest remains in non-data-driven and openly available
representation techniques. It is therefore of interest to compare which of these techniques can generate the
most diverse and high-quality solution set.
102103104
Generations0.250.300.350.40Avg. Fitness
102103104
Generations0.250.500.751.00Coverage
102103104
Generations0255075100 QD-Score
102103104
Generations1022102310241025Phenotypic Diversity CACPPNdictionarydirect
parametric
Figure 8: Evolution of all representations. Included is our approach that combines multiple encodings
in MAP-Elites. Experiments are replicated 10 times. Shown are the median (dashed line) and 25%/75%
percentiles.
7On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
2.8 Multi-encoding QD
We also evaluate whether making use of mixed representations in QD advances the diversity of the archive.
Multi-encoding QD contains differently encoded species that compete on a phenotypic level. Although
classical speciation is built around the fact that individuals can only breed with members of their own species
through crossover, we refrain from using the crossover mechanism entirely in this preliminary analysis. Due
to the fact that MAP-Elites does not use a crossover operation, using multiple encodings in MAP-Elites is
straightforward. Each encoding comes with its own mutation operator and solutions are only compared
based on their phenotypic features and ﬁtness. The initial population contains equally many individuals
from all encodings.
3 Evaluation
To answer the research questions that were posed in Section 1, the direct and indirect representations
described in Sections 2.2–2.6 are compared. In order to do this, MAP-Elites is run ten times for every
representation for 50,000 generations from an initial population of 100 individuals, creating ten children
in each generation. The surrogate ﬁtness function that was described in Section 2 is used to determine the
quality of solutions. We measure how well the archive is ﬁlled by using the coverage metric, the percentage of
bins that are ﬁlled. The QD-score (Pugh et al., 2015) sums up the ﬁtness values over all cells in the archive, and
has become a standard metric to measure the performance of QD algorithms that use a ﬁxed grid. The score,
however, depends on the feature space that is used in the problem domain and does not capture the full
diversity of the solutions. To measure phenotypic diversity , the sum of all pair-wise distances of phenotypes in
the archive is calculated. By using the L0:1-norm to measure the distance between individuals, the diversity
metric is made more robust for high-dimensional spaces (Wang et al., 2016).
50 100 150
Dimensionali ty0.300.350.40 Avg. Fitness
50 100 150
Dimensionali ty0.70.80.9Coverage
50 100 150
Dimensionali ty6080100 QD-Score
50 100 150
Dimensionali ty10231024Pheno typicDiversity
CACPPNdictionarydirect
parametric
Figure 9: Relationships between four metrics and the encodings’ dimensionality. Different shapes denote
different parameterizations for each encoding. Please refer to Section E in the Appendix to retrieve the
values of the parameters.
Furthermore, the representations’ dimensionality is measured by counting the number of degrees of freedom
the representation offers. We also evaluate, whether representations ﬁnd similar solutions or distinct regions
in the solution space. Finally, representations are combined, as described in Section 2.8, to determine what
effect this has on QD’s performance. Two-sampled t-tests are used to determine the signiﬁcance between
ﬁtness, coverage, QD-score and phenotypic diversity of the solution sets.
3.1 Hyperparameters
The various hyperparameters of the encodings are determined by parameter sweeps using a grid search. For
each encoding, MAP-Elites is run ten times for every representation for 25,000 generations with an initial
population of 100, creating ten children in each generation. The average ﬁtness and phenotypic diversity
were ordered in fronts according to Pareto-dominance. Then, starting from the ﬁrst Pareto front, the best
four hyperparameter conﬁgurations were selected. The hyperparameter values can be taken from the code
repository.
3.2 Quality and Diversity of Representations
The evolution for each representation is shown in Figure 8. The quality of the representations at initialization
is very different. During the ﬁrst 20,000 generations, the ﬁtness of the parametric encoding outperforms all
other representations. After this point in the runs the CPPN overtakes the parametric encoding to ﬁnd the
highest quality solutions. The performance of the direct and dictionary-based encodings is similar, which is
expected, as they can express the same shapes. They do not reach the same ﬁtness levels as the parametric
8On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
direct dictionary parametric CPPN CA combined
CACPPNdictionarydirect
parametric
Figure 10: Visualization of phenotypic space of all encodings using t-SNE. Encodings are color coded by
ﬁtness and combined on the right to show what parts of the phenotype space they reach.
and CPPN. The CA’s ﬁtness is the lowest of all encodings. The archive coverage starts out in essentially
the opposite order: the CA immediately outperforms the other encodings, although the CPPN ends up at
the same level. The direct encoding only outperforms the parametric encoding in terms of coverage and
underperforms compared to other indirect encodings.
The QD-score shows a more complete story, as both the coverage and ﬁtness are taken into account. The
CPPN-encoding ﬁnds the highest-performing solutions over the largest part of the archive, although it
takes close to a 1,000 generations before it overtakes the other encodings. The CA-encoding underperforms
in quality but this is partially compensated by a large archive coverage. The direct and dictionary-based
encodings trail behind the CPPN but perform better than the CA and parametric encodings. The phenotypic
diversity is shown on the right of the ﬁgure. The CA and CPPN encodings are clearly more phenotypically
diverse. The diversity between the parametric, direct and dictionary-based encoding are similar. Encoding
examples are shown in Figure 1.
Two-sample t-test were performed to determine the signiﬁcance of results. Please refer to the supplementary
material to ﬁnd all pair-wise signiﬁcance tests. Most comparisons between the encodings are statistically
signiﬁcant with p<0:05.
3.3 Dimensionality of Representations
When plotting ﬁtness, coverage, and QD-score in relation to the encodings’ dimensionality, a pattern
emerges in Figure 9. The best-performing encodings tend to have a lower dimensionality, both in ﬁtness
as well as in coverage. The phenotypic diversity of the CPPN and CA encodings is much higher, both
outperforming the other encodings, which perform similarly. Some encodings are more sensitive to the
hyperparameterization than others. An example of this is the direct encoding’s performance, which is most
sensitive to the hyperparameters. The dictionary-based encoding is able to reach a coverage similar to that
of the CPPN and CA, depending on the hyperparameters used. However, the encoding does not seem to be
equally sensitive as the direct encoding, as the four points are not as far apart. The indirect encodings seem
to be mostly insensitive to the hyperparameterization, which makes them easier to use.
The ﬁtness of the solution sets does not seem to depend much on the dimensionality. However, both the
coverage of the archive, the QD-score, and the phenotypic diversity are higher for most lower-dimensional
encodings.
3.4 Different Regions in Phenotypic Space
Encodings might be able to reach different parts of the phenotypic space. Figure 10 shows all solutions
from all encodings in a two-dimensional similarity space that was calculated using t-distributed stochastic
neighborhood embedding (t-SNE) (Maaten and Hinton, 2008). The embedding was calculated based on all
solutions from all 10 replicates of all encodings. Some of the indirect encodings reach different parts of the
phenotypic space, although with mixed results. Although the CA encoding ﬁnds very different solutions in
the left and right of the space, they also account for mostly low-performing solutions. What stands out is
that most encodings might indeed ﬁnd noticeably different, often high-performing solutions.
3.5 Multi-encoding QD
Because the encodings found quite different solutions, an open question is, whether it makes sense to
use multiple encodings in QD. The performance metrics of the single-encoding QD runs are compared to
multi-encoding QD in Table 1. Using multiple encodings, QD is able to reach a QD-score similar to that of
the best single-encoding runs with CPPNs. The phenotypic diversity is not as high though, and certainly not
as high as the CA runs, although the latter ﬁnds mostly lower-performance solutions.
The question is, whether multi-encoding QD relies solely on CPPN to reach this result. To answer this, we
analyze the proportion of encodings in the population. Figure 11 shows an example archive, one where the
9On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
Avg. Fitness Coverage QD-Score Phenotypic Diversity
Direct Encoding 0:4110:004 0:8590:042 90:4303:749 6:65310221:0441022
Dictionary-based Encoding 0:4090:005 0:8990:038 93:9802:971 6:86410222:3651022
Parametric Encoding 0:4160:001 0:6940:092 73:9119:650 7:18210226:0321022
Compositional Pattern Producing Networks 0:4200:00:9770:0104:9370:075 6:46510232:6031023
Cellular Automata 0:3070:001 0:9770:0 76:8320:244 4:32210244:2431023
Multi-encoding 0:4140:002 0:9770:0 103:5040:496 1:28110230:6251023
Table 1: Performance of single- and multi-encoding QD. Reported are the mean and over 10 experiments.
CPPN has died out. The dictionary-based, parametric and CA encodings each take up a contiguous part of
the archive.
CACPPNdictionarydirect
parametric
# buildingstotal built area
Figure 11: Example distribution of encodings in an archive of multi-encoding QD.
We investigate the proportion of the encodings in the population over all ten runs (Figure 12). MAP-Elites was
initialized with the same number of individuals for each encoding, 20 per encoding. However, individuals
can be assigned to the same niche, leading to the removal of the weaker individual. Through intra- or
inter-species competition, the proportion of each encoding after initialization is not the same. As can be
seen from the ﬁgure, the largest portion of the archive is ﬁlled with solutions from the direct encoding in
approximately the ﬁrst 100 generations. After that it gets overtaken by the dictionary-based, parametric and
CA encodings. Interestingly enough, when we combine all encodings in a single QD archive, the CPPN,
which performs best between all encodings, gets mostly removed from the archive after 100 generations.
This is probably due to the underperformance in early evolution. Since we do not reinject encodings into the
archive, as soon as one encoding dies out, it cannot return.
102104
Generations0204060Proportio ninArchive [%]
CACPPNdictionarydirect
parametric
Figure 12: Proportion of encodings in multi-encoding QD. Shown are the median (dashed line) and 25%/75%
percentiles.
There are however two runs where CPPN survives after 50,000 generations (see Table 2). In one of them, it
takes over half of the archive. This gives us some evidence, that we need to either protect some species that
might have more difﬁculty in the beginning of the search, or develop other suitable methods from classical
EA to deal with the problem.
10On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
# CA CPPN Direct Parametric Dictionary
1 19,2% 0% 0% 31.2% 49.6%
2 13.2% 4.4% 2.8% 22.8% 56.8%
3 12.8% 0% 0% 32.8% 54.4%
4 16.4% 0% 0% 40.4% 43.2%
5 15.6% 0% 0% 30.8% 53.6%
6 14.8% 0% 0% 38.0% 47.2%
7 0% 52.4% 0% 19.2% 28.4%
8 13.6% 0% 0.4% 40.8% 45.2%
9 13.6% 0% 1.6% 30.8% 54.0%
10 15.2% 0% 0% 34.4% 50.4%
Table 2: Proportion of encodings in multi-encoding archive. Each line is a separate run (#).
3.6 Discussion
In Section 3.2 we observed that in our experiments indirect encodings can but do not always outperform
direct encodings in terms of quality, but are better at ﬁlling the archive, having a higher coverage. Parametric
encodings ﬁnd high-quality solutions for only a small part of the archive early on in QD but the QD-score is
lower than all other encodings. In the speciﬁc domain we deﬁned, we observed that CPPN outperforms
all other encodings, followed by the dictionary-based and direct encodings. In Section 3.3 we observed
that low genetic dimensionality tends to produce higher coverage. The visualization in Section 3.4 shows
that different encodings can cover distinct regions in phenotype space. In Section 3.5 we observed that
multi-encoding QD outperforms all other single encodings, except that it performs similar to CPPN, although
it barely uses that encoding.
4 Conclusion
Contrary to the evidence shown in (Tarapore et al., 2016) we observed that indirect encodings can outperform
direct encodings in QD. This naturally depends heavily on whether the encoding can reach appropriate
regions in phenotypic space. Lower encoding dimensionality usually allows signiﬁcantly higher archive
coverage but has only a small impact on the solutions’ ﬁtness. We observed that, although CPPN encodings
outperform others both in quality and diversity, by using multiple encodings in MAP-Elites we can get
similar results, even if CPPNs barely survive against the other encodings. Multi-encoded QD is a novel
approach to EA as it allows multiple qualitatively different species of encodings to compete based on their
phenotypic (or behavioral) features.
Limitations Although using these simple surrogates instead of CFD diminishes the realism for practical
usage of produced solutions, they still allow comparing encodings while reducing the computational effort
that went into this analysis.
Another limitation is that it is possible that the mutation operators themselves grant an advantage to one
encoding over another, either in terms of their ability to escape local optima through larger movements in
the search space, or conversely, through an ability to make more subtle movements.
The multi-encoding approach has a weakness in its initialization due to the structured archive in MAP-Elites.
Solutions that could evolve later on in the optimization process can be pushed out by early-evolvable
encodings during initialization or in the early optimization process. Speciation has been around for a long
time in EAs but QD might introduce a new perspective.
Future Work A selection of common representations was compared in this work, leaving out some of the
common CAD-driven encodings, which should be included in future work. A more rigorous and complete
analysis that includes multiple categories of domains, from robotics control to shape optimization, should
shed more light on the question of whether indirect or direct encodings are more appropriate in QD. A
more indepth analysis of mutation, crossover and selection operators should be taken into consideration.
The observations in this work beg for a more theoretical analysis of representations with respect to their
redundancy, scaling and locality (Rothlauf, 2006), especially how these effects might explain and improve
the behavior of multi-encoding QD. For example, because some encodings like CPPN evolve more slowly in
the beginning, protection mechanisms or different mutation operators could be introduced. Finally, during
initialization, an unstructured archive could be used to make sure solutions are not overwritten immediately.
This work showed that one should be mindful about the encoding that is used when applying QD
11On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
optimization to a problem domain. A multi-encoding approach might take more advantage of the strengths
of multiple encodings, while reducing their weaknesses. Due to the qualitative difference in encodings, we
can interpret them as truly different species.
Acknowledgements
The authors thank Rudolf Berrendorf and Javed Razzaq for their continuous development and support of
the BRSU’s computing cluster.
Funding
The computer hardware was supported by the Federal Ministry for Education and Research and by the
Ministry for Innovation, Science, Research, and Technology of the state of Northrhine-Westfalia (research
grant 13FH156IN6).
References
Auerbach, J. E. and Bongard, J. C. (2010). Evolving cppns to grow three-dimensional physical structures. In
Proceedings of the 12th annual conference on Genetic and evolutionary computation , pages 627–634.
Barthet, M., Liapis, A., and Yannakakis, G. N. (2022). Open-ended evolution for minecraft building generation.
IEEE Transactions on Games .
Bentley, P . J., Lim, S. L., Gaier, A., and Tran, L. (2022). Evolving through the looking glass: Learning improved
search spaces with variational autoencoders. In International Conference on Parallel Problem Solving from
Nature , pages 371–384. Springer.
Chen, S., Montgomery, J., and Bolufé-Röhler, A. (2015). Measuring the curse of dimensionality and its effects
on particle swarm optimization and differential evolution. Applied Intelligence , 42:514–526.
Clune, J. and Lipson, H. (2011). Evolving 3d objects with a generative encoding inspired by developmental
biology. ACM SIGEVOlution , 5(4):2–12.
Collins, J., Cottier, B., and Howard, D. (2019). Comparing direct and indirect representations for environment-
speciﬁc robot component design. In 2019 IEEE Congress on Evolutionary Computation (CEC) , pages 2705–2712.
IEEE.
Cully, A., Clune, J., Tarapore, D., and Mouret, J.-B. (2015). Robots that can adapt like animals. Nature ,
521(7553).
Dawkins, R. (1982). The Extended Phenotype . Oxford University Press Oxford.
Gaier, A., Asteroth, A., and Mouret, J.-B. (2020). Discovering representations for black-box optimization. In
Proceedings of the 2020 Genetic and Evolutionary Computation Conference , pages 103–111.
Gardner, M. (1970). The fantastic combinations of john conway’s new solitaire game of life. Sc. Am. ,
223:20–123.
Hagg, A. (2021a). Discovering the preference hypervolume: an interactive model for real world computational
co-creativity . PhD thesis, Leiden University.
Hagg, A. (2021b). Phenotypic niching using quality diversity algorithms. In Metaheuristics for Finding Multiple
Solutions , pages 287–315. Springer.
Hagg, A., Wilde, D., Asteroth, A., and Bäck, T. (2020). Designing air ﬂow with surrogate-assisted phenotypic
niching. In International Conference on Parallel Problem Solving from Nature , pages 140–153. Springer.
Herr, C. M. and Ford, R. C. (2016). Cellular automata in architectural design: From generic systems to speciﬁc
design tools. Automation in Construction , 72:39–45.
Janssen, P ., Bui, T. D. P ., and Wang, L. (2022). Möbius evolver: Competitive exploration of urban massing
strategies. In Artiﬁcial Intelligence in Urban Planning and Design , pages 293–321. Elsevier.
Kaushik, V . and Janssen, P . (2013). An evolutionary design process–adaptive-iterative explorations in
computational embryogenesis.
Keshavarzi, M. and Rahmani-Asl, M. (2021). Genﬂoor: Interactive generative space layout system via
encoded tree graphs. Frontiers of Architectural Research , 10(4):771–786.
Kingma, D. P . and Welling, M. (2014). Auto-encoding variational bayes. stat, 1050:1.
12On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
Maaten, L. v. d. and Hinton, G. (2008). Visualizing data using t-sne. Journal of Machine Learning Research ,
9(Nov).
Maher, M. L. (2000). A model of co-evolutionary design. Engineering with computers , 16(3):195–208.
Modrak, V ., Pandian, R. S., and Semanco, P . (2021). Calibration of ga parameters for layout design optimiza-
tion problems using design of experiments. Applied Sciences , 11(15):6940.
Moore, E. F. (1962). Machine models of self-reproduction. In Proceedings of symposia in applied mathematics ,
volume 14, pages 17–33. American Mathematical Society New York.
Mordvintsev, A., Randazzo, E., Niklasson, E., and Levin, M. (2020). Growing neural cellular automata. Distill ,
5(2):e23.
Neumann, J. v. (1966). Theory of self-reproducing automata. Mathematics of Computation , 21:745.
Nichele, S., Ose, M. B., Risi, S., and Tufte, G. (2017). Ca-neat: evolved compositional pattern produc-
ing networks for cellular automata morphogenesis and replication. IEEE Transactions on Cognitive and
Developmental Systems , 10(3):687–700.
Pugh, J. K., Soros, L. B., Szerlip, P . A., and Stanley, K. O. (2015). Confronting the challenge of quality diversity.
InProceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation , pages 967–974.
Ranjan Nayak, D., Kumar Sahu, S., and Mohammed, J. (2013). A cellular automata based optimal edge
detection technique using twenty-ﬁve neighborhood model. International Journal of Computer Applications ,
84(10):27–33.
Rechenberg, I. (1973). Evolutionsstrategie: Optimierung technischer systeme nach prinzipien der biologis-
chen evolution, frommann–holzboog. stuttgart Germany .
Rothlauf, F. (2006). Representations for genetic and evolutionary algorithms . Springer.
Schwefel, H.-P . and Schwefel, H.-P . (1977). Evolutionsstrategien für die numerische optimierung . Springer.
Sobieczky, H. (1999). Parametric airfoils and wings. Recent development of aerodynamic design methodologies:
inverse design and optimization , pages 71–87.
Stanley, K. O. (2006). Exploiting regularity without development. In AAAI Fall Symposium: Developmental
Systems , page 49.
Stanley, K. O. and Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies.
Evolutionary computation , 10(2):99–127.
Tarapore, D., Clune, J., Cully, A., and Mouret, J.-B. (2016). How do different encodings inﬂuence the
performance of the map-elites algorithm? In Proceedings of the Genetic and Evolutionary Computation
Conference 2016 , pages 173–180.
Wang, H., Jin, Y., and Yao, X. (2016). Diversity assessment in many-objective optimization. IEEE transactions
on cybernetics , 47(6):1510–1522.
Wang, L., Chen, K. W., Janssen, P ., and Ji, G. (2020). Enabling optimisation-based exploration for building
massing design-a coding-free evolutionary building massing design toolkit in rhino-grasshopper.
Wang, L., Janssen, P ., Chen, K. W., Tong, Z., and Ji, G. (2019). Subtractive building massing for performance-
based architectural design exploration: a case study of daylighting optimization. Sustainability , 11(24):6965.
Wonka, P ., Wimmer, M., Sillion, F., and Ribarsky, W. (2003). Instant architecture. ACM Transactions on Graphics
(TOG) , 22(3):669–677.
Xu, X., Yin, C., Wang, W., Xu, N., Hong, T., and Li, Q. (2019). Revealing urban morphology and outdoor
comfort through genetic algorithm-driven urban block design in dry and hot regions of china. Sustainability ,
11(13):3683.
13On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
Appendix
Signiﬁcance Tests Fitness
cppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄedcppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄed011101111111111111110
100011111111111111111
100011111111111111111
100011111111111111111
011101111111111111111
111110101101111111100
111111010010111111111
111110101101111111101
111111010010111111111
111111010010111111111
111110101101111111101
111111010010111111111
111111111111001111111
111111111111001111111
111111111111110011111
111111111111110011111
111111111111111101111
111111111111111110111
111111111111111111011
111110101101111111101
011110111111111111110Fitness
Figure 13: Two-sampled t-test results comparing pair-wise ﬁtness values of encodings/hyperparameters, for
p<0:05.
Signiﬁcance Tests Coverage
14On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
cppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄedcppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄed000011111111000011110
000011111111000011110
000011111111000011110
000011111111000011110
111101001111111111011
111110011111111110111
111100011111111110111
111101101111111111011
111111110011111111111
111111110011111111111
111111111101111111111
111111111110111111111
000011111111000011110
000011111111000011110
000011111111000011110
000011111111000011110
111111111111111101101
111110011111111110111
111101101111111111011
111111111111111101101
000011111111000011110Coverage
Figure 14: Two-sampled t-test results comparing pair-wise coverage values of encodings/hyperparameters,
forp<0:05.
15On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
Signiﬁcance Tests QD-Score
cppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄedcppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄed011111111111111111110
100011111111111111111
100011111111111111111
100011111111111111111
111101001111000010111
111110111111111101111
111101001111111111111
111101001111000011111
111111110011111111111
111111110011111111111
111111111101111111111
111111111110111111111
111101101111001111111
111101101111001111111
111101101111110011111
111101101111110011111
111110111111111101111
111101111111111110111
111111111111111111011
111111111111111111101
011111111111111111110QD-Score
Figure 15: Two-sampled t-test results comparing pair-wise QD-score of encodings/hyperparameters, for
p<0:05.
Signiﬁcance Tests Phenotypic Diversity
16On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
cppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄedcppn layers: 1 neurons: 2 p: 0.25 s: 10
cppn layers: 1 neurons: 5 p: 0.2 s: 9
cppn layers: 2 neurons: 2 p: 0.3 s: 11
cppn layers: 2 neurons: 5 p: 0.25 s: 11
parametric p: 0.05 s: 1
parametric p: 0.05 s: 3
parametric p: 0.1 s: 3
parametric p: 0.1 s: 2
dictionary 1 by 8 p: 0.05
dictionary 1 by 6 p: 0.05
dictionary 1 by 3 p: 0.05
dictionary 1 by 4 p: 0.05
ca 3 by 3 s: 0.5 iter: 8
ca 3 by 3 s: 0.5 iter: 12
ca 5 by 5 s: 0.5 iter: 12
ca 7 by 7 s: 0.5 iter: 12
direct p: 0.1
direct p: 0.2
direct p: 0.3
direct p: 0.05
multi-encoding shuﬄed000011111111111111111
000011111111111111111
000011111111111111111
000011111111111111111
111100000000111100000
111100000000111100001
111100000000111100000
111100000000111100101
111100000000111110101
111100000000111110101
111100000000111100101
111100000000111100101
111111111111001111111
111111111111001111111
111111111111110111111
111111111111111011111
111100001100111101101
111100000000111110101
111100011111111111010
111100000000111100101
111101011111111111010Sum of Diversity
Figure 16: Two-sampled t-test results comparing pair-wise phenotypic diversity values of encod-
ings/hyperparameters, for p<0:05.
Explanation of Symbols
17On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
ca 3x3 σ:0.5 iter:12
ca 3x3 σ:0.5 iter:8
ca 5x5 σ:0.5 iter:12
ca 7x7 σ:0.5 iter:12cppn l:1 n:2 p:0.25 σ:10
cppn l:1 b:5 p:0.2  σ:9
cppn l:2 n:2 p:0.3 σ:11
cppn l:2 n:5 p:0.25 σ:11dictionary 1x3 p:0.05
dictionary 1x4 p:0.05
dictionary 1x6 p:0.05
dictionary 1x8 p:0.05direct p:0.05
direct p:0.1
direct p:0.2
direct p:0.3parametric p:0.05 σ:1
parametric p:0.05 σ:3
parametric p:0.1 σ:2
parametric p:0.1 σ:3
Figure 17: Explanation of different parameterizations for each encoding.
18On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
More Encoding Examples
Figure 18: Randomly selected archive with 25 representative solutions using direct encoding.
Figure 19: Randomly selected archive with 25 representative solutions using dictionary encoding.
19On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
Figure 20: Randomly selected archive with 25 representative solutions using parametric encoding.
Figure 21: Randomly selected archive with 25 representative solutions using CPPN encoding.
20On the Suitability of Representations for Quality Diversity Optimization of Shapes A P REPRINT
Figure 22: Randomly selected archive with 25 representative solutions using CA encoding.
21