WA VELET-INSPIRED MULTISCALE GRAPH CONVOLUTIONAL RECURRENT NETWORK
FOR TRAFFIC FORECASTING
Qipeng Qian1,2, Tanwi Mallick2
1Department of Applied Mathematics, University of Arizona, Tucson, AZ, USA
2Mathematics and Computer Science Division, Argonne National Laboratory, IL, USA
ABSTRACT
Traffic forecasting is the foundation for intelligent transporta-
tion systems. Spatiotemporal graph neural networks have
demonstrated state-of-the-art performance in traffic fore-
casting. However, these methods do not explicitly model
some of the natural characteristics in traffic data, such as
the multiscale structure that encompasses spatial and tem-
poral variations at different levels of granularity or scale.
To that end, we propose a Wavelet-Inspired Graph Convo-
lutional Recurrent Network (WavGCRN) which combines
multiscale analysis (MSA)-based method with Deep Learn-
ing (DL)-based method. In WavGCRN, the traffic data is
decomposed into time-frequency components with Discrete
Wavelet Transformation (DWT), constructing a multi-stream
input structure; then Graph Convolutional Recurrent net-
works (GCRNs) are employed as encoders for each stream,
extracting spatiotemporal features in different scales; and fi-
nally the learnable Inversed DWT and GCRN are combined
as the decoder, fusing the information from all streams for
traffic metrics reconstruction and prediction. Furthermore,
road-network-informed graphs and data-driven graph learn-
ing are combined to accurately capture spatial correlation.
The proposed method can offer well-defined interpretabil-
ity, powerful learning capability, and competitive forecasting
performance on real-world traffic data sets.
Index Terms —Spatiotemporal correlation, wavelet-
inspired network, graph learning, traffic forecasting
1. INTRODUCTION
The intelligent transportation system (ITS) has garnered sig-
nificant attention due to ever-escalating traffic pressures. In
this context, traffic forecasting plays a pivotal role in compre-
hending and crafting an efficient transportation system while
mitigating traffic congestion.
Traditional traffic forecasting methods have their origins
in time series analysis, e.g., vector auto-regression, support
vector regression, auto-regressive integrated moving average
(ARIMA), hidden Markov model. Their model assumptions
are frequently at odds with real-world traffic data, and com-
plex domain knowledge is needed, such as queuing theory andbehavior simulation in various traffic scenarios [1]. In recent
years, Deep Learning (DL)-based methods have emerged as
a mainstream trend in traffic forecasting. They autonomously
unearth the underlying information of traffic data, bypassing
the need for crafting a traffic metrics model tailor-made for
each road segment involved.
The spatiotemporal correlation in traffic data is the basis
of modeling and prediction, therefore how to effectively ex-
ploit spatiotemporal information in learning procedures is a
crucial problem that largely determines the predictive perfor-
mance. Graphs are effective tools to represent spatial correla-
tions in distributed observed data, so Graph Neural Networks
(GNNs) have been used to analyze signals or features residing
on graph [2]. On the other hand, to capture the temporal cor-
relations of traffic data, Recurrent Neural Networks (RNNs)
like Long Short-Term Memory (LSTM) [3], Gated Recurrent
Unit (GRU) [4], and Transformer [5] have gained wide recog-
nition. Integrating GNNs with RNNs leads to a number of
state-of-the-art traffic forecasting methods. DCRNN [6] is
the first work to combine GNN and RNN. STGCN [7] ap-
plies CNN instead of RNN to improve the model efficiency at
the expense of some performance. DGCRN [8] considers the
dynamic change of the underlying graph. Hierarchical GCN
[9] exploits the hierarchical patterns of traffic data.
However, these DL-based methods can not explicitly in-
vestigate some natural characteristics in traffic data, such as
multiscale structure, necessitating high-quality data and art-
ful strategies to capture these features. On the other hand,
Multiscale Analysis (MSA) models can represent the hidden
frequency information through rigors mathematical and phys-
ical perspectives [10], [11], [12]. Thus explicitly decompos-
ing traffic data into different frequency domains can decou-
ple the complex relationship and facilitate learning and in-
terpretation. However, effectively combining MSA with DL
into a consistent method is still a challenging problem in traf-
fic forecasting research, especially coupling multiscale spa-
tiotemporal feature learning and traffic graph learning, de-
signing MSA-based DL architecture, and understanding pre-
diction methods and results [13].
In this paper, we proposed a Wavelet-Inspired Graph Con-
volutional Recurrent Network (WavGCRN), which is a multi-
stream encoder-decoder architecture. Discrete wavelet trans-arXiv:2401.06040v3  [cs.LG]  4 Mar 2024form (DWT) and learnable inverse DWT (LIDWT) are com-
bined with graph convolutional recurrent network (GCRN) to
make up the backbone modules. This network is MSA-based
and DL-based, aligning the wavelet transform process with
the deep network architecture. Meanwhile, distinct graphs
are learned for each stream, making the graphs better reflect
the multiscale correlations. The main contributions include:
a) DWT is applied to construct a multi-stream encoder.
DWT-based signal decomposition can decouple traffic infor-
mation, augmenting the power of data-driven DL and helping
to uncover the implicit relationship buried in the data.
b) LIDWT is used as a fusion decoder. This architecture
not only aggregates learned features from different encoder
streams but simulates the transformation and inverse transfor-
mation processes of wavelet as well, which will benefit inter-
pretation and implementation.
c) An optimization-based graph learning method is com-
bined with a prior road-network-informed graph to take full
advantage of physical-informed and data-driven graph gener-
ation methods.
2. METHODOLOGY
The traffic forecasting problem discussed in this paper is
specifically defined as predicting future traffic metrics given
previously observed traffic metrics from Ncorrelated sen-
sors located on the different positions in a road network. We
represent the sensor relationship as a weighted directed graph
G= (V, E, A )with|V|=N. Denote the traffic metrics
observed on Gas a graph signal X∈RN×d, where dis
the feature dimension for each node. Let Xtrepresent the
graph signal observed at time t. The traffic forecasting task is
learning a function hthat maps the historical data of length L
to the future prediction of length T:
h(Xt−L+1, ..., Xt;G) = (Xt+1, ..., Xt+T) (1)
WavGCRN is shown in Figure 1. The traffic metrics
signal of each sensor is decomposed by DWT and each de-
composed component constitutes a stream of encoder. In
each stream, the spatiotemporal feature of each sensor is ex-
tracted by a GCRN-based encoder. The encoded features of
all streams are fused by LIDWT and then input to a GCRN-
based decoder to restore and predict the traffic metrics signals
of all sensors. The graph is updated iteratively based on the
learned node features in the current training iteration.
2.1. DWT-based Multi-stream Input
MSA is a good choice to decouple the spatiotemporal struc-
ture by either spatial-multiscale or temporal-multiscale de-
composition. In general, the low-frequency signal compo-
nents are able to capture slow-varying character while thehigh-frequency ones embody fine-grained changes. Separat-
ing these different scale components, addressing them respec-
tively, and fusing them interactively in a network model, will
enhance the model interpretability and reduce learning load.
DWT is a well-known tool for time series analyze [10]
due to its multiscale time-frequency property. For raw input
time series X∈RN×L, if Haar wavelet is used, the DWT
coefficients at level ωis calculated as
a(ω)
i,:(k) =1
2[b(ω−1)
i,:(2k) +b(ω−1)
i,:(2k+ 1)],
b(ω)
i,:(k) =1
2[b(ω−1)
i,:(2k)−b(ω−1)
i,:(2k+ 1)].(2)
where a(ω)is approximate coefficients and b(0)=X,b(ω)is
detailed coefficients. The output of Ω-level DWT is
{a(ω)}Ω
ω=1∪ {b(Ω)}.
2.2. Graph Convolutional Recurrent Network (GCRN)
GCRN is built by GCN with GRU [6], [8], which is used for
multi-stream encoder to process each signal component indi-
vidually, and for fusion decoder to restore and predict traffic
metrics.
The message propagation rule of GCN is derived from
graph spectral convolution [14], which works as a parame-
terized filter gθin the frequency domain. Chebyshev spec-
tral CNN [15] uses Chebyshev polynomials to approximate
gθwith predefined order kand with order 1we get GCN. We
useK-hop graph convolution defined as
H(k)=αHin+β˜AH(k−1),
Hout=KX
k=0H(k)W(k), H(0)=Hin,
˜A=D−1A.(3)
where Ais the adjacent matrix of current learned graph, and
Dis its degree matrix. Hinis the input hidden state, Houtis
the output of K-hop graph convolution. W(k)∈Rdin×dout
are learnable parameters, dinanddoutare the input and output
dimensions. αandβare weight parameters. Abbreviate the
above equations as
Hout= Θ∗G(Hin, A).
Replacing matrix multiplication in GRU with ∗G, we get
rt=σ(Θr∗G(a(ω)
:,t|H(ω)
t−1, A(ω))),
ut=σ(Θu∗G(a(ω)
:,t|H(ω)
t−1, A(ω))),
Ct= tan(Θ C∗G(a(ω)
:,t|rt⊙H(ω)
t−1, A(ω))),
H(ω)
t=ut⊙Ct+ (1−ut)⊙H(ω)
t−1,(4)
where |is the concatenation operation.Fig. 1 : The architecture of WavGCRN with DWT-based multi-stream encoder and LIDWT-based multi-stream fused decoder.
For each stream of the encoder, the number of layers
formed by GCRN is determined by the input signal length.
The number of layers formed by GCRN in decoder is deter-
mined by the time horizon of the specific prediction task.
2.3. Learnable Inverse DWT (LIDWT)
Under neural network framework, signal transformation can
be parameterized/learnable [16], [12]. To ensure consistency
with the DWT process, we incorporate a LIDWT to fuse the
outputs from the multi-stream encoder. The proposed LIDWT
strikes a balance between preserving the physical significance
of the wavelet-transform-based representation while benefit-
ing from the expressive power and flexibility of deep learning.
LIDWT aggregates the outputs of all encoder streams
({H(ω)}Ω+1
ω=1) by
b(ω−1)
i =D(ω)
L(k)a(ω)
k+D(ω)
H(k)b(ω)
k, i= 2k;
b(ω−1)
i =A(ω)
L(k)a(ω)
k+A(ω)
H(k)b(ω)
k, i= 2k+ 1;
b(Ω)
k=H(Ω+1)
k,:, a(ω)
k=H(ω)
k,:, ω = 1, ...,Ω.(5)
{DL, DH, AL, AH}are the learnable parameters that serve
as reconstruction coefficients in IDWT.
2.4. Loss function and Train Strategy
We use Mean Absolute Error (MAE) as the loss function,
Lpre=1
TTX
i=1|ˆXt+i−Xt+i|, (6)
where Xis the ground truth and ˆXis prediction result.
Adaptive moment estimation is used as the basic training
algorithm. Furthermore, in order to alleviate performance
degradation caused by discrepancy between the input distri-
butions of training and testing, we adopt scheduled sampling
method [6], and to reduce the time and memory costs of
GCRN, we use curriculum learning strategy [8].2.5. Graph Learning
In most of GCN based traffic metrics forecasting methods,
either the graph is directly derived from the reachable shortest
distances between sensor locations in road network or learned
from traffic data during training procedure. In this paper we
propose a new graph learning method that combines these two
kinds of methods together, which simultaneously utilize both
physics-informed and data-informed information.
A=γAdata+ (1−γ)Aroad. (7)
Adata is the learned graph and Adistis prior graph directly
derived from road network. γconsiders the balance between
two graphs. The iterative updating scheme is used to balance
the current learned graph and the last learned graph, enabling
the graph learning smooth.
Most of graph learning methods employed in previous
work like [17] and [8] are based on the similarity between
node features. However, node feature similarity-based graph
cannot explicitly represent causal relationship and manifold
structure. As a consequence, we turn to optimization-based
graph learning algorithm, named NOTEARS in [18].
NOTEARS estimates the structure of a directed acyclic
graph (DAG) Aby
minimize Score (A;X) =1
2B∥X−XA∥2
F+λ∥A∥1,
subject to tr (eA⊗A)−N= 0.
(8)
Score function evaluates whether the graph structure effec-
tively reflects the node causal relationships and the constraint
holds iff there is no cycle in the graph A. We use L-BFGS
[19] algorithm to solve this optimization problem.
Since the spatial correlations in different frequency-
domains are also different, e.g., the graph underlying low-
frequency signal component might be quite sparser than the
graph of high-frequency component. The different graphs are
learned separately for each input stream.Table 1 : Comparison on METR-LA dataset
ModelHorizon 3 Horizon 6 Horizon 12
MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
DCRNN 2.77 5.38 7.30% 3.15 6.45 8.80% 3.60 7.59 10.50%
STGCN 2.88 5.74 7.62% 3.47 7.24 9.57% 4.59 9.40 12.70%
Graph WaveNet 2.69 5.15 6.90% 3.07 6.22 8.37% 3.53 7.37 10.01%
AGCRN 2.87 5.58 7.70% 3.23 6.58 9.00% 3.62 7.51 10.38%
MTGNN 2.69 5.18 6.86% 3.05 6.17 8.19% 3.49 7.23 9.87%
WavGCRN 2.67 5.13 6.76% 3.05 6.22 8.15% 3.50 7.38 9.81%
Table 2 : Comaprison on PEMS-BAY dataset
ModelHorizon 3 Horizon 6 Horizon 12
MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE
DCRNN 1.38 2.95 2.90% 1.74 3.97 3.90% 2.07 4.74 4.90%
STGCN 1.36 2.96 2.90% 1.81 4.27 4.17% 2.49 5.69 5.79%
Graph WaveNet 1.30 2.74 2.73% 1.63 3.70 3.67% 1.95 4.52 4.63%
AGCRN 1.37 2.87 2.94% 1.69 3.85 3.87% 1.96 4.54 4.64%
MTGNN 1.32 2.79 2.77% 1.65 3.74 3.69% 1.94 4.49 4.53%
WavGCRN 1.30 2.73 2.73% 1.64 3.75 3.68% 1.94 4.58 4.62%
3. EXPERIMENT
Two popular real-world datasets are used in our experiments.
METR-LA was gathered from a network of 207loop detec-
tors strategically positioned throughout the Los Angeles high-
way system, the time spans from 03/01/2012 to 06/30/2012
[20]. PEMS-BAY was acquired from 325sensors spanning
the extensive Bay Area landscape, and covers a period from
01/01/2017 to 05/31/2017 [6]. Both of them provide aver-
age traffic flow speeds in 5 minutes in miles per hour (mph).
All experiments are carried out using a system based on the
NVIDIA DGX A100 platform. Our codebase can be found at
https://github.com/qqian99/WavGCRN .
WavGCRN is compared with several state-of-the-art traf-
fic prediction approaches, including DCRNN [6], STGCN
[7], Graph WaveNet [21], AGCRN [22], and MTGNN [17].
Three popular criterions Mean Absolute Error (MAE), Root
Mean Squared Error (RMSE), and Mean Absolute Percent-
age Error (MAPE) are used for evaluating forecasting perfor-
mance. The experiments are set by 15 minutes (horizon 3), 30
minutes (horizon 6), and 1 hour (horizon 12) ahead forecast-
ing based on 1-hour historical input data from the META-LA
and the PEMS-BAY datasets.
Results are outlined in Tables 1 and 2 respectively. Apart
from WavGCRN, the results of all other methods are directlycopied from their papers and [8]. The best and second best re-
sults are highlighted. It can be seen that WavGCRN exhibits
superior performance as a whole compared with other mod-
els. Especially, our approach demonstrates a prominent ad-
vantage in short-range horizon prediction. Notice that traffic
condition in METR-LA is more complex than in PEMS-BAY ,
as Los Angeles is renowned for its intricate traffic dynamics.
Thus the forecasting on METR-LA is also more difficult than
on PEMS-BAY , and the superiority of our method on METR-
LA is more significant.
4. CONCLUSION
In this paper, a novel MSA-based and DL-based hybrid
method WavGCRN, is proposed for traffic forecasting, which
is a DWT-inspired encoder-decoder network architecture
with GCRN and graph learning. Compared with the cur-
rent DL-based traffic forecasting methods, our model is able
to better explore the multiscale spatiotemporal relationship
buried among the traffic metrics data and has more concrete
physical interpretability. The proposed method can be used
for other application scenarios in which time series signals
are observed on associated sensors, such as Internet of Things
and social networks.Acknowledgments
This research was supported by the National Science Founda-
tion through the MSGI Internship Program and the U.S. De-
partment of Energy, Office of Science, Advanced Scientific
Computing Research, through the SciDAC-RAPIDS2 insti-
tute under Contract DE-AC02-06CH11357. Additionally, it
utilized resources from the Argonne Leadership Computing
Facility under contract DE-AC02-06CH11357.
References
[1] E. Cascetta, Transportation systems engineering: the-
ory and methods , vol. 49, Springer Science & Business
Media, 2013.
[2] T. N. Kipf and M. Welling, “Semi-supervised classifi-
cation with graph convolutional networks,” in Proc. Int.
Conf. Learn. Representations , 2017.
[3] S. Hochreiter and J. Schmidhuber, “Long short-term
memory,” Neural Comput. , vol. 9, no. 8, pp. 1735–1780,
1997.
[4] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Em-
pirical evaluation of gated recurrent neural networks on
sequence modeling,” arXiv preprint arXiv:1412.3555 ,
2014.
[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,
“Attention is all you need,” in Proc. Adv. Neural Inf.
Process. Syst. , 2017, vol. 30, p. 5998–6008.
[6] Y . Li, R. Yu, C. Shahabi, and Y . Liu, “Diffusion con-
volutional recurrent neural network: Data-driven traffic
forecasting,” arXiv preprint arXiv:1707.01926 , 2017.
[7] B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph
convolutional networks: A deep learning framework for
traffic forecasting,” in Proc. 27th Int. Joint Conf. Artif.
Intell. , 2018, p. 3634–3640.
[8] F. Li, J. Feng, H. Yan, G. Jin, F. Yang, F. Sun, D. Jin, and
Y . Li, “Dynamic graph convolutional recurrent network
for traffic prediction: Benchmark and solution,” ACM
Trans. Knowl. Discov. Data , vol. 17, no. 1, pp. 1–21,
2023.
[9] K. Guo, Y . Hu, Y . Sun, S. Qian, J. Gao, and B. Yin,
“Hierarchical graph convolution network for traffic fore-
casting,” in Proc. 35th AAAI Conf. Artif. Intell. , 2021,
vol. 35, pp. 151–159.
[10] T. Kriechbaumer, A. Angus, D. Parsons, and M. R.
Casado, “An improved wavelet–arima approach for
forecasting metal prices,” Resour. Policy , vol. 39, pp.
32–41, 2014.[11] J. Wang, Z. Wang, J. Li, and J. Wu, “Multilevel
wavelet decomposition network for interpretable time
series analysis,” in Proc. 24th ACM SIGKDD Int. Conf.
Knowl. Discov. Data Min. , 2018, pp. 2437–2446.
[12] D. Chen, L. Chen, Y . Zhang, B. Wen, and C. Yang, “A
multiscale interactive recurrent network for time-series
forecasting,” IEEE Trans. Cybern. , vol. 52, no. 9, pp.
8793–8803, 2021.
[13] A. J. Huang and S. Agarwal, “Physics-informed deep
learning for traffic state estimation: Illustrations with
lwr and ctm models,” IEEE Open J. Intell. Transp. Syst. ,
vol. 3, pp. 503–518, 2022.
[14] F. RK. Chung, Spectral graph theory , vol. 92, American
Mathematical Soc., 1997.
[15] M. Defferrard, X. Bresson, and P. Vandergheynst, “Con-
volutional neural networks on graphs with fast localized
spectral filtering,” in Proc. Adv. Neural Inf. Process.
Syst., 2016, pp. 3844–3852.
[16] R.-G. Cirstea, C. Guo, B. Yang, T. Kieu, X. Dong,
and S. Pan, “Triformer: Triangular, variable-specific
attentions for long sequence multivariate time series
forecasting–full version,” in Proc. 31th Int. Joint Conf.
Artif. Intell. , 2022.
[17] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and
C. Zhang, “Connecting the dots: Multivariate time se-
ries forecasting with graph neural networks,” in Proc.
26th ACM SIGKDD Int. Conf. Knowl. Discov. Data
Min., 2020, pp. 753–763.
[18] X. Zheng, B. Aragam, P. K. Ravikumar, and E. P. Xing,
“Dags with no tears: Continuous optimization for struc-
ture learning,” in Proc. Int. Conf. Neural Inf. Process.
Syst., 2018, pp. 9472–9483.
[19] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, “A limited
memory algorithm for bound constrained optimization,”
SIAM J. Sci. Comput. , vol. 16, no. 5, pp. 1190–1208,
1995.
[20] H. V . Jagadish, J. Gehrke, A. Labrinidis, Y . Papakon-
stantinou, J. M. Patel, R. Ramakrishnan, and C. Shahabi,
“Big data and its technical challenges,” Commun. ACM ,
vol. 57, no. 7, pp. 86–94, 2014.
[21] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, “Graph
wavenet for deep spatial-temporal graph modeling,” in
Proc. 28th Int. Joint Conf. Artif. Intell. , 2019, pp. 1–7.
[22] L. Bai, L. Yao, C. Li, X. Wang, and C. Wang, “Adaptive
graph convolutional recurrent network for traffic fore-
casting,” in Proc. Adv. Neural Inf. Process. Syst. , 2020,
vol. 33, pp. 17804–17815.