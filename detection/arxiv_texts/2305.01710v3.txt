FROM STARS TO INSIGHTS : EXPLORATION AND
IMPLEMENTATION OF UNIFIED SENTIMENT ANALYSIS WITH
DISTANT SUPERVISION
A P REPRINT
Wenchang Li
Peking University
Beijing, China
wli@stu.pku.edu.cn
John P. Lalor,
 Yixing Chen,
 Vamsi K. Kanuri
University of Notre Dame
Notre Dame, Indiana, USA
john.lalor@nd.edu ,ychen43@nd.edu ,vkanuri@nd.edu
ABSTRACT
Sentiment analysis is integral to understanding the voice of the customer and informing businesses’
strategic decisions. Conventional sentiment analysis involves three separate tasks: aspect-category
detection (ACD), aspect-category sentiment analysis (ACSA), and rating prediction (RP). However,
independently tackling these tasks can overlook their interdependencies and often requires expensive,
fine-grained annotations. This paper introduces Unified Sentiment Analysis (Uni-SA), a novel
learning paradigm that unifies ACD, ACSA, and RP into a coherent framework. To achieve this, we
propose the Distantly Supervised Pyramid Network (DSPN), which employs a pyramid structure
to capture sentiment at word, aspect, and document levels in a hierarchical manner. Evaluations on
multi-aspect review datasets in English and Chinese show that DSPN, using only star rating labels for
supervision, demonstrates significant efficiency advantages while performing comparably well to a
variety of benchmark models. Additionally, DSPN’s pyramid structure enables the interpretability
of its outputs. Our findings validate DSPN’s effectiveness and efficiency, establishing a robust,
resource-efficient, unified framework for sentiment analysis.
Keywords Sentiment analysis, pyramid structure, distant supervision
1 Introduction
As markets grow more competitive, businesses are increasingly turning to customer feedback for richer insights that
can help them address customer pain points, enhance customer satisfaction, and drive growth. Online reviews have
become a vital resource for gathering these insights because they provide unfiltered feedback from consumers with
direct experience of a company’s products and services (Zhang et al ., 2023; Kang and Zhou, 2019), and capture a wide
spectrum of customer experiences, including those involving product/service features, service quality, price, and brand
reputation.
Consequently, researchers have prescribed several frameworks to analyze online reviews. A bulk of those primarily
focus on determining the overall sentiment within a review (Liu and Zhang, 2012). However, by concentrating only on
an aggregate sentiment score, such analyses can overlook more detailed feedback within reviews. This limitation may
obscure actionable insights that could drive targeted improvements in customer service or product design.
Consider, for example, the review shown in Figure 1: “ The food is great, but the waitress was not friendly at all .”
Traditional sentiment analysis might interpret this review as portraying a neutral sentiment overall. In contrast, a more
granular approach reveals that, while the customer enjoyed the food, they were dissatisfied with the service. This type
of fine-grained analysis allows businesses to capture richer and more nuanced sentiments—such as a positive response
to food and a negative response to service—that can lead to targeted improvements, such as enhancing customer service
while maintaining food quality.arXiv:2305.01710v3  [cs.CL]  26 Nov 2024Unified Sentiment Analysis with Distant Supervision A P REPRINT
Figure 1: An overview of Unified Sentiment Analysis (Uni-SA). While ACD, ACSA, and RP can be performed
individually, leveraging the implicit pyramid structure of reviews enables efficient completion of all three tasks using
only RP labels.
To obtain such nuanced insights from an online review, an analyst would need to perform three key tasks: identify
key aspects within a review (i.e., Aspect-Category Detection, or ACD), classify the sentiment associated with each
aspect (i.e., Aspect-Category Sentiment Analysis, or ACSA), and predict the overall sentiment or rating of the review
(i.e., Rating Prediction, or RP). For example, a model would use ACD to identify aspects like Food andService in the
aforementioned example, and apply ACSA to assess their sentiment (e.g., Food: Positive ,Service: Negative ), and use
RP to predict an overall rating (e.g., two stars).
Despite its benefits, fine-grained sentiment analysis presents significant implementation challenges. Prior studies
that combine ACD, ACSA, and RP often treat these tasks independently,1requiring large, annotated datasets that are
costly and time-intensive to create. Producing aspect-based labels is particularly labor-intensive, as each review must
be manually tagged for sentiments tied to specific attributes. Additionally, prior research seldom incorporates the
hierarchical structure in sentiment data, which ranges from word-level expressions (e.g., “delicious”) to aspect-level
(e.g., “food”) and overall sentiment (e.g., “five stars”). Not accounting for such hierarchical structure within unstructured
text data can result in efficiency loss and poor predictive accuracy (e.g., Chen and Qian, 2020, for ACD and ACSA).
To address these challenges, we propose an integrated approach to sentiment analysis, referred to as Unified Sentiment
Analysis (Uni-SA). Uni-SA is grounded in the idea that a review’s overall rating reflects the cumulative sentiments
associated with various aspects and that aspect-level sentiments are informed by word-level expressions within the
review (Bu et al ., 2021). To accomplish Uni-SA, we propose a Distantly Supervised Pyramid Network (DSPN). Notably,
DSPN can unify the ACD, ACSA, and RP tasks by using only review-level star ratings for training. DSPN employs
a pyramid structure to capture sentiment at each level—word, aspect, and review. Specifically, the model begins by
identifying word-level sentiments, aggregates them to determine aspect-level sentiments, and then synthesizes these
to predict the review’s overall rating. For instance, in our prior example, recall that the reviewer stated that the food
was good, but the service quality was unacceptable. Considering the two aspects (i.e., food and service), the reviewer
gave the restaurant a two-star rating (Figure 1). DSPN holistically captures these nuances by acknowledging that the
aspect-level preferences drive the overall review of two stars (out of a possible five). By relying solely on readily
available review star ratings for training (Li et al ., 2020b), DSPN eliminates the need for costly, manually annotated
aspect-level labels, making it an efficient and cost-effective tool for comprehensive sentiment analysis. Our empirical
results show that this efficient approach can improve performance for the RP task, indicating that the pyramid structure
can both make predictions for unlabeled tasks while maintaining performance on the main labeled task.
Overall, in this paper, we make three contributions. First, we introduce Uni-SA as a unified task of three important
but separate sentiment analysis tasks, specifically ACD, ACSA, and RP. We formally give definitions and notations
of Uni-SA and its constituent parts (See Section 3) that future research can follow. Second, we propose the Distantly
1In some cases, ACD and ACSA are combined (e.g., Schmitt et al ., 2018; Liu et al ., 2021). However, RP is rarely integrated into
aspect-level analyses (Chebolu et al., 2023).
2Unified Sentiment Analysis with Distant Supervision A P REPRINT
Supervised Pyramid Network (DSPN) as a novel model for the Uni-SA task. DSPN captures the pyramid sentiment
structure effectively and shows significant efficiency gains with only RP labels as training signal. Third, we validate
DSPN through experimental results on Chinese and English multi-aspect datasets and demonstrate its effectiveness and
efficiency.2
2 Related Work
Comprehensive literature reviews suggest that sentiment analysis includes three key tasks: ACD, ACSA, and RP (Liu
and Zhang, 2012; Schouten and Frasincar, 2015). Table 1 lists the most relevant studies, which serve as benchmarks in
our experiments later. Next, we briefly review these studies in the context of our research.
ReferenceSupported TasksSupervision Model Architecture
ACD ACSA RP
He et al. (2017) ! Unsupervised AutoEncoder
Xue and Li (2018) ! Supervised Gated Convolutional Networks
Sun et al. (2019) ! Supervised BERT
Schmitt et al. (2018) ! ! Supervised LSTM, CNN
Li et al. (2020a) ! ! Supervised BERT
Liu et al. (2021) ! ! Supervised BERT, BART
This paper ! ! !Unsupervised (Module 1)
Distantly Supervised (Module 2)BERT
Table 1: Comparison with representative approaches on supported tasks, supervision, and model architecture.
2.1 Aspect-Category Detection
ACD methods can be categorized as rule-based, supervised, or unsupervised. Rule-based approaches (e.g., Hai et al .,
2011; Schouten et al ., 2014) rely on manually defined rules and domain knowledge. Supervised methods (e.g., Toh and
Su, 2016; Xue et al ., 2017) require reviews annotated with predefined aspect categories. Unsupervised models (e.g.,
Titov and McDonald, 2008; Brody and Elhadad, 2010; Zhao et al ., 2010) extract aspects by identifying word co-
occurrence patterns. The ABAE model (He et al ., 2017), which employs an autoencoder-style network, is a foundational
unsupervised approach and forms the basis of our Module 1. Recently, Tulkens and van Cranenburgh (2020) introduced
a model that combines POS tagging and word embeddings with a contrastive attention mechanism, outperforming more
complex methods. Extending prior work, we propose a novel aspect-attention mechanism that integrates ACD outputs
into the ACSA task.
2.2 Aspect-Category Sentiment Analysis
Most ACSA methods in the literature are supervised (Schouten and Frasincar, 2015; Li et al ., 2020a; Liu et al ., 2021)
and require costly, time-intensive aspect-level data annotation. Unsupervised LDA-based ACSA models (e.g., Zhao
et al., 2010; Xu et al ., 2012; García-Pablos et al ., 2018) often depend on external resources such as part-of-speech
tagging and sentiment word lexicons but can suffer from a topic-resemblance problem (Huang et al ., 2020). To address
this issue, Huang et al .(2020) proposed a weakly-supervised approach that learns a joint aspect-sentiment topic
embedding. However, this method is limited to documents with a single annotated aspect, effectively reducing the
task to RP. Recently, Kamila et al .(2022) introduced AX-MABSA, an extremely weakly supervised ACSA model
that achieves strong performance without any labeled data. The current paper advances prior research by proposing a
distantly supervised pyramid network that efficiently performs the ACSA task using only star rating labels.
2.3 Rating Prediction
RP, or review-level sentiment analysis, is used to extract user opinions from large review datasets or predict ratings in
recommender systems. Typically modeled as a multi-class classification task, RP has been well-studied (e.g., Ganu et al .,
2009; Li et al ., 2011; Liu and Zhang, 2012; Chen et al ., 2018). Recent advances focus on improving interpretability
and prediction accuracy. For example, the CEER framework (Zhou et al ., 2024) enhances explanation generation in
2Our code is available at https://github.com/nd-ball/DSPN .
3Unified Sentiment Analysis with Distant Supervision A P REPRINT
recommender systems by utilizing macro concepts from user-item reviews. Sejwal and Abulaish (2022) introduced
RecTE, which uses topic embeddings to predict ratings and address cold-start issues. These approaches show the
benefits of incorporating textual data into RP models to boost performance and user trust. However, the pyramid
sentiment structure has not yet been leveraged in RP models to assist in aspect-level sentiment learning, which we
explore in this paper.
2.4 Multi-Task Sentiment Analysis
There has been work on jointly learning ACSA and RP (Bu et al ., 2021), using RP information for ACSA (Yin et al .,
2017; Li et al ., 2018; He et al ., 2018), and using ACSA information for RP (Cheng et al ., 2018; Wu et al ., 2019). Prior
approaches on document-level multi-aspect sentiment classification predict user ratings for different product or service
aspects (Yin et al ., 2017; Li et al ., 2018), often incorporating user information and star ratings to boost performance,
though at the cost of efficiency. Other works (Bu et al ., 2021; Fei et al ., 2022) have jointly learned ACD and ACSA;
these methods require costly aspect-level data annotation, hindering efficiency. For instance, Schmitt et al .(2018)
proposed joint models for ACD and ACSA in an end-to-end manner. However, to our knowledge, this is among the first
studies that simultaneously models all three tasks with a single task source, RP labels, for supervision.
3 Unified Sentiment Analysis
Before describing our model, we first define our notation and present the unifying framework of Uni-SA. We borrow
notation from the prior work where possible and introduce new notation as needed for consistency across tasks (Pontiki
et al., 2016). For clarity and consistency, we provide a comprehensive description of the notation we use in this article
(Table 2). Our corpus is a collection of reviews R={R1, R2, . . . , R |R|}. Each review Riconsists of a sequence of
word tokens (hereafter “words”): Ri={t(1)
i, t(2)
i, . . . , t(n)
i}.
3.1 Constituent Components
Aspect-Category Detection In the ACD task, there are Npredefined aspect categories (hereafter “aspects”): A=
{A1, A2, . . . , A N}. The set of aspects present in Riis defined as: ARi={A(1)
Ri, A(2)
Ri, . . . , A(K)
Ri}, where K≤N. To
train unsupervised ACD models, the required training data is simply R.
Aspect-Category Sentiment Analysis For a given review Riand one of its aspects A(k)
Ri, the goal of ACSA is to
predict the polarity of the aspect: ˆyA(k)
Ri. Aspect polarity is typically binary ( positive ornegative ) or categorical (with a
third option of neutral ). Supervised ACSA models require review-aspect-polarity triples: {Ri,(A(k)
Ri, yA(k)
Ri)K
k=1}|R|
i=1.
In the case of multi-aspect ACSA, multiple aspects are present in each review, and therefore, ACSA requires K× |R|
labels for each review, a factor of Klarger than in RP.
Rating Prediction Given a review Ri, RP aims to predict the star rating ˆyRi. Supervised RP models requires
review-sentiment tuples: {(Ri, yRi)}|R|
i=1.
3.2 Uni-SA Training
Typically, ACD, ACSA, and RP are considered standalone tasks. With Uni-SA, we propose a unified approach, where
with training data of only RP labels, a model can output present aspects (ACD), the sentiment of those aspects (ACSA),
and an overall document-level sentiment score (RP). This approach uses training labels from a single task (RP) to
efficiently learn multiple distinct sentiment analysis tasks. More specifically, for a model M, the training data required
is the same as the RP task: {(Ri, yRi)}|R|
i=1. At run-time, the model provides three outputs for a new review Ri: (1) The
predicted aspects present in the review ( ˆARi), (2) the sentiment polarity of each identified aspect ( ˆyA(k)
Ri∀AR(k)
i∈ˆARi),
and (3) the overall sentiment prediction for the review ( ˆyRi).
4 Distantly Supervised Pyramid Network
In this section, we describe our novel implementation for Uni-SA, DSPN. The overall model architecture is illustrated
in Figure 2.
4Unified Sentiment Analysis with Distant Supervision A P REPRINT
Variable Description Dimension
dw Embedding dimension R768
R Reviews in our dataset -
Ri i-th review consisted of a sequence of word tokens Rn×dw
n Number of word tokens in Ri R100
t(j)
i j-th word in Ri Rdw
A Predefined aspect categories RN
ARi The set of aspects present in Ri RK(K≤N)
A(k)
Rik-th aspect in ARi Rdw
yA(k)
RiSentiment polarity of A(k)
RiR3
yRi Star rating of Ri R3
M Model -
ˆARi Prediction of ARi -
ˆyA(k)
RiPrediction of yA(k)
RiR3
ˆyRi Prediction of yRi R3
Xi Input sequence Rn×dw
zi Sentence embedding of Xi(pooler_output of BERT) Rdw
T Aspect embedding matrix RN×dw
Tk Embedding of k-th aspect Rdw
ri Reconstructed sentence embedding Rdw
pi Weight vectors of Kaspect embeddings (aspect importance) RN
L(θACD)Loss function of ACD task (Module 1) -
λACD Weight of regularization term -
U(θ) Regularization term -
nj Each negative sample Rdw
h(j)
i hidden state of j-th word (last_hidden_state of BERT) Rdw
w(j)
i Sentiment prediction vector of j-th word Rdw×3
d(j)
kDistance between j-th word and k-th aspect -
a(j)
kAttention weight of j-th word towards k-th aspect -
L(θRP) Loss function of RP task (Module 2) -
λ Weight of L(θACD) -
L(θ) Overall loss function -
Table 2: Description of variables in our formulation.
4.1 Module 1: Aspect-Category Detection
For the ACD task, we utilize an autoencoder-style network (He et al ., 2017). For a review Ri, the input sequence Xiis
constructed as {[CLS ], t(1)
i, t(2)
i, . . . , t(n)
i,[SEP ]}. We use BERT (Devlin et al ., 2019) to generate embeddings zifor
each sample.
To generate aspect embeddings, we first set the aspect and keyword map dictionary for each aspect.3Then, for each
aspect, we use BERT to encode the sentence composed of key words related to the aspect and obtain its output as the
initial embedding of the aspect. In this way, we initialize the aspect embedding matrix T. Lastly, Module 1 performs
sentence reconstruction at the aspect level through a linear layer:
3There are Npredefined aspects in ACD task, and many prior works have identified the representative words for each one of
them (Bu et al ., 2021; Wang et al ., 2010). For example, “staff,” “customer,” and “friendly” can be the representative words for
“Service” aspect. Based on this, we proposed first constructing a sentence that contains top representative words, then using the
embedding of this sentence as the initial embedding for the aspect.
5Unified Sentiment Analysis with Distant Supervision A P REPRINT
Figure 2: Overall architecture of DSPN. Aspect embedding matrix Tis used to calculate the distance between words
and aspects, which is regarded as the word-level attention weights for each aspect. Aspect importance piis learned by
Module 1 and is used as the attention weights of aspects.
zi=BERT (Xi) (1)
pi=softmax (W1·zi+b1) (2)
ri=T⊤·pi (3)
where riis the reconstructed sentence embedding and piis the aspect importance vector.
The loss function for the unsupervised Module 1 is defined as a hinge loss to maximize the inner product between
the input sentence embedding and its reconstruction while minimizing the inner product between the input sentence
embedding and randomly sampled negative examples:
L(θACD) =X
Ri∈RmX
j=1ϕRi,j+λACDU(θ) (4)
ϕRi,j= max(0 ,1−rizi+rinj) (5)
where njrepresents each negative sample, and U(θ)represents the regularization term to encourage unique aspect
embeddings (He et al ., 2017). The aspect embedding matrix Tand aspect importance vector piare inputs for attention
calculation in DSPN’s pyramid network (Module 2).
6Unified Sentiment Analysis with Distant Supervision A P REPRINT
4.2 Module 2: Pyramid Sentiment Analysis
Module 2 is based on the intuition that the sentiment of a review is an aggregation of the sentiments of the aspects
contained in the review (Bu et al ., 2021). In addition, the sentiment of an aspect is an aggregation of the sentiments of
the words indicating that aspect, forming a three-layer structure. We propose using a pyramid network to capture this
structure, and we can use easy-to-obtain RP ratings as training labels. In the Word Sentiment Prediction Layer, we use
the hidden vector of each word output by BERT to obtain word representations, where h(j)
iis the representation of the
j-th word. We use two fully connected layers to produce a word-level sentiment prediction vector:
w(j)
i=W3·ReLU (W2·h(j)
i+b2) +b3 (6)
Then, we can calculate the similarity of words and aspects using the word representations and the aspect embedding
matrix Toutput by Module 1. This similarity will be treated as the attention weights of words for the aspect. When
predicting aspect-level sentiment, for the k-th aspect, the sentiment ˆyA(k)
Riis computed as:
d(j)
k=T⊤
k·h(j)
i (7)
a(j)
k=exp(d(j)
k)
Pn
m=1exp(d(m)
k)(8)
ˆyA(k)
Ri=softmax (nX
j=1w(j)
ia(j)
k) (9)
Finally, the review-level sentiment ˆyRiis computed by:
ˆyRi=softmax (ˆyARi·pi) (10)
Here, piis the aspect importance vector output by Module 1 (§4.1), which is regarded as the attention weights of
aspects in a review. ˆyARiis the matrix concatenation of aspect-level sentiments in the review.
4.3 Loss
For the RP task, as each prediction is a 3-class classification problem, the loss function is defined by the categorical
cross-entropy between the true label and the model output:
L(θRP) =−X
iyRi·log(ˆyRi) (11)
We jointly train DSPN for RP and ACD by minimizing the combined loss function:
L(θ) =λL(θACD) +L(θRP) (12)
where λis the weight of ACD loss. Although no direct supervision is required for ACSA, due to the construction of
DSPN, the model inherently learns aspect sentiment predictions.
4.4 Inference
Having been trained, for any new text input (e.g., restaurant or hotel review) Ri, DSPN generates three outputs: a
distribution over aspects ( pi), estimated sentiment for each aspect ( ˆyA(k)
Ri), and an overall sentiment prediction ( ˆyRi).
5 Experiments
5.1 Datasets
Although DSPN can learn ACD, ACSA, and RP with only RP labels, to benchmark DSPN across all three Uni-SA
tasks, we need datasets with ACD, ACSA, and RP labels. To validate DSPN as an efficient and effective model for
7Unified Sentiment Analysis with Distant Supervision A P REPRINT
unified sentiment analysis, we experiment with two datasets: one in English (TripDMS) and one in Chinese (ASAP).
Details of these datasets are provided in Table 3.
TripDMS is an English-language hotel review dataset from Tripadvisor.com (Wang et al ., 2010; Yin et al ., 2017), with
RP labels on a 5-star scale and ACSA labels ( positive ,negative ,neutral ) for seven aspects: Value, Room, Location,
Cleanliness, Check-in, Service, Business. ASAP is a Chinese-language restaurant review dataset from a leading
e-commerce platform in China (Bu et al ., 2021). It includes RP labels on a 5-star scale and ACSA labels ( positive ,
negative ,neutral ) for aspects identified in the review text (Pontiki et al ., 2016). For ACSA, sentiment is aggregated
at the entity level for five aspects: Food, Price, Location, Service, Ambience, determined by majority vote. To our
knowledge, these are the only two publicly available datasets with both RP and ACSA labels for evaluating performance.
Dataset Language MA MAS Split ReviewsOverall Sentiment Aspect Sentiments
Pos. Neu. Neg. Pos. Neu. Neg. NaN
TripDMS English 100% 100%Train 23,515 8,998 5,055 9,462 64,984 34,200 43,391 22,030
Val 2,939 1,161 613 1,165 8,174 4,245 5,349 2,805
Test 2,939 1,079 647 1,213 8,002 4,355 5,437 2,779
ASAP Chinese 95.97% 63.85%Train 36,850 29,132 5,241 2,477 77,507 27,329 17,299 62,115
Val 4,940 3,839 784 317 10,367 3,772 2,373 8,188
Test 4,940 3,885 717 338 10,144 3,729 2,403 8,424
Table 3: Statistics of the datasets. MA is the percentage of multi-aspect instances in the dataset and MAS is the
percentage of multi-aspect multi-sentiment instances.
5.2 Evaluation
Recall that DSPN aims to achieve higher efficiency and accuracy in performing unified sentiment analysis through
distant supervision. Therefore, to test its performance, we compare DSPN’s performance to that of existing ACD,
ACSA, and RP models.
Specifically, for ACD, we compare DSPN with the unsupervised ABAE (He et al ., 2017). To ensure a fair comparison
with DSPN, we replaced ABAE’s encoder with BERT and updated the aspect embedding matrix Tinitialization. We
call this ABAE-BERT and report its performance. Following previous work (Ruder et al ., 2016; Ghadery et al ., 2019),
we use thresholding to assign aspects based on probability, selecting the threshold that yields the best performance
(1e−4). ACD is evaluated using F1 score.
For ACSA, we benchmark DSPN against several strong supervised models, incorporating non-BERT models
(GCAE (Xue and Li, 2018), End2end-LSTM/CNN (Schmitt et al ., 2018), and AC-MIMLLN (Li et al ., 2020a))
as well as BERT-based models (AC-MIMLLN-BERT (Li et al ., 2020a) and ACSA-Generation (Liu et al ., 2021)). ACSA
is evaluated using accuracy.
For RP, a text classification task, we compare DSPN with BERT fine-tuning strategies (Sun et al ., 2019): BERT-Feat,
BERT-FiT, and BERT-ITPT-FiT. We convert the 5-star RP ratings into three classes (Negative, Neutral, Positive) and
evaluate using accuracy.
6 Results
6.1 Overall Performance
To compare DSPN with existing models, we evaluate two pipeline approaches: a high-performance pipeline using
the best-performing model for each task and a high-efficiency pipeline using the most efficient model in terms of
parameters. Our benchmarking results are shown in Tables 4 and 5. The efficiency metric is the sum of the separate
models’ parameters, while the performance metric reflects the individual performance of each model on the three tasks.4
Notably, DSPN is the only model capable of performing all three tasks with only RP supervision . DSPN outperforms all
benchmark models for RP, demonstrating the benefit of incorporating the pyramid structure on the task of RP. In ACD,
DSPN achieves better F1 scores than ABAE and ABAE-BERT on TripDMS. Our results also show that ABAE-BERT
outperforms ABAE, highlighting the benefit of incorporating pretrained language models for higher-quality aspect
detection.
4Results for all benchmarking models are presented in Appendix A for completeness.
8Unified Sentiment Analysis with Distant Supervision A P REPRINT
Efficiency Performance
Parameters Labels Training Time ACD ACSA RP
(MM) (Thousands) (Minutes) (F1) (Acc) (Acc)
TripDMSABAE-BERT (ACD) 91.2 0 40 92.3
AC-MIMML-BERT (ACSA) 105 142.6 55 64.3
BERT-ITPT-FiT (RP) 82.7 23.5 102 72.4
Pipeline 278.9 166.1 197 92.3 64.3 72.4
DSPN (Ours) 102.9 23.5 95 92.7 53.2 72.5
Delta -63.1 -85.9 -51.8 0.4 -17.3 0.1
ASAPABAE-BERT (ACD) 97.5 0 42 80.1
AC-MIMML-BERT (ACSA) 107.2 122.1 55 77.2
BERT-ITPT-FiT (RP) 91 36.9 110 80.3
Pipeline 295.7 159 207 80.1 77.2 80.3
DSPN (Ours) 111 36.9 88 79.4 65.4 81.3
Delta -62.5 -76.8 -57.5 -0.9 -15.3 1.3
Table 4: Comparison between DSPN and a high-performance pipeline approach to unified sentiment analysis. For
efficiency comparisons, negative deltas indicate improvements over the pipeline; for performance, positive deltas
indicate improvements over the pipeline.
Efficiency Performance
Parameters Labels Training Time ACD ACSA RP
(MM) (Thousands) (Minutes) (F1) (Acc) (Acc)
TripDMSABAE (ACD) 3.1 0 15 91.2
GCAE (ACSA) 4.2 142.6 5 55.1
BERT-Feat (RP) 80.2 23.5 35 71.4
Pipeline 87.5 166.1 55 91.2 55.1 71.4
DSPN (Ours) 102.9 23.5 95 92.7 53.2 72.5
Delta 17.6 -85.9 72.7 1.64 -3.5 1.5
ASAPABAE (ACD) 3.1 0 15 79.4
GCAE (ACSA) 4.4 122.1 6 70.3
BERT-Feat (RP) 80.8 36.9 42 79.2
Pipeline 88.3 159 63 79.4 70.3 79.2
DSPN (Ours) 111 36.9 88 79.4 65.4 81.3
Delta 25.7 -76.8 39.7 0.0 -7.0 2.7
Table 5: Comparison between DSPN and a high-efficiency pipeline approach to unified sentiment analysis. For
efficiency comparisons, negative deltas indicate improvements over the pipeline; for performance, positive deltas
indicate improvements over the pipeline.
DSPN’s performance on ACSA is lower than the supervised benchmarks. This is to be expected as DSPN’s only
supervision is RP labels. From an efficiency point of view, ACSA models require 142,575 labels on TripDMS to
learn one task (ACSA), while DSPN only requires 23,515 labels (84% fewer) to learn three tasks. Therefore, by using
DSPN, we see a tradeoff of an 84% reduction in labels for a prediction performance that is only 17% lower than the
best-performing supervised model for ACSA. Similarly, for ASAP, we observe a label-performance tradeoff where for a
70% reduction in labels, DSPN performance is only 15% lower than the best-performing supervised model for ACSA.
What’s more, DSPN outperforms the fully-supervised End2end-CNN baseline model.
Our single-task benchmarks set the “upper bound” of performance for the task when given a fully labeled dataset. They
can be considered target values for future Uni-SA models. However, if only RP labels exist for a given dataset, then
DSPN is the only method for learning all three tasks.
Considering that DSPN does not use any aspect-level labels, the fact that the effectiveness of DSPN is comparable to
supervised models on the ACSA task is empirical validation of the Uni-SA in general and the DSPN architecture in
particular.
9Unified Sentiment Analysis with Distant Supervision A P REPRINT
To better visualize the comparison between DSPN and the two pipelines, we plotted each relevant metric in Figure 3.
From a managerial perspective, models are often incorporated into broader pipelines (Lalor et al ., 2024). Therefore,
considering a holistic view and analyzing the metrics together is important.
Figure 3: Comparing DSPN to the two pipelines. In the top row, higher values are better. In the bottom row, lower
values are better.
6.2 Quality Analysis
6.2.1 Case Study
In order to visualize and analyze DSPN’s performance, we first take two reviews from TripDMS as examples (Figure
4a). For each example, the trained DSPN model takes the review text as input and first outputs word-level sentiment
predictions. Then, DSPN (i) identifies aspect keywords via a word attention calculation, (ii) obtains the aspect
importance, (iii) calculates aspect-level sentiment through the sentiments of the key words, and lastly (iv) combines
aspect sentiment with aspect importance to predict the final review-level sentiment (“Overall” in Figure 4).
For case 1 in Figure 4a, DSPN correctly labels the review as positive and also correctly identifies and labels the Service ,
Value ,Room , and Cleanliness aspects with no aspect-level annotations. For case 2, DSPN gives correct predictions on
word-, aspect-, and review-level sentiments.
6.2.2 Error Analysis
To exemplify errors in DSPN, we then examine two examples of error cases from TripDMS in Figure 4b. We find
that extreme star rating labels sometimes influence DSPN. For example, for case 1 in Figure 4b, DSPN gives correct
word-level sentiments but tends to give positive predictions at the aspect level due to the overall 5-star rating. Similarly,
for case 2, DSPN gives negative predictions on all three levels due to a 1-star rating. This is to be expected as
DSPN’s only supervision is star rating labels. Addressing this disconnect is a promising area for future Uni-SA model
development.
10Unified Sentiment Analysis with Distant Supervision A P REPRINT
(a)
(b)
Figure 4: Case studies of correct predictions (4a) and incorrect predictions (4b). True RP and ACSA labels are outside
of the pyramid; DSPN’s predictions are within the pyramid. For space, we show a portion of the review.
7 Additional Analyses
7.1 Budget Constraint Experiment
For a more direct comparison between DSPN and the supervised ACSA models, we designed a budget-constraining
experiment. Specifically, we randomly selected ACSA labels for TripDMS and ASAP so that the supervised models
have the same training label size as DSPN.
Model TripDMS ASAP
End2end-LSTM 54.2 65.1
End2end-CNN 53.6 64.9
GCAE 54.0 70.1
AC-MIMLLN 61.4 75.8
AC-MIMLLN-BERT 63.9 76.6
ACSA-Generation 60.2 75.8
DSPN (Ours) 53.2 65.4
Table 6: ACSA results when all models are trained with the same amount of data.
In this setting, DSPN’s performance is closer to the supervised models’ performance (Table 6). In particular, DSPN
outperforms both End2end-LTSM and End2end-CNN on ASAP. Overall, the supervised models still outperform DSPN,
but this is to be expected given that the labels used for training are ACSA labels. Recall that DSPN still performs RP
11Unified Sentiment Analysis with Distant Supervision A P REPRINT
and ACD on top of these ACSA results, while the benchmark models only learn ACSA. DSPN is trained to perform
RP but is also able to perform ACSA in a way that is comparable to these supervised models under the same budget
constraint.
7.2 DSPN without RP Labels
We have shown DSPN’s effectiveness using two datasets that include both review-level star rating labels (for RP) and
aspect-level sentiment annotations (for ACSA). However, a large number of current ACSA datasets do not contain
rating data (RP), such as Rest-14 (Pontiki et al ., 2014), Rest-15 (Pontiki et al ., 2015), Rest-16 (Pontiki et al ., 2016) and
MAMS (Jiang et al., 2019).
Model Supervision Rest-14 Rest-15 Rest-16 MAMS
ACSA-Generation Supervised 78.43 71.91 73.76 70.30
JASen Weakly supervised 26.62 19.44 23.23 14.74
AX-MABSA Weakly supervised 49.68 42.74 36.47 29.74
DSPN (Ours)Unsupervised (Module 1)
Distantly Supervised (Module 2)30.01 18.23 24.01 12.79
Table 7: ACSA results on datasets with no RP labels. Benchmark results are from (Kamila et al., 2022).
To enable DSPN to run on such datasets, we use the aggregate value of aspect ratings as the training labels instead of
the star rating labels given by users. Moreover, we can also evaluate our distantly supervised model against existing
weakly supervised ACSA models. For example, JASen (Huang et al ., 2020) uses a small number of keywords for each
aspect-polarity pair as supervision. Table 7 shows that DSPN performs comparably to the JASen. This result indicates
that RP is not simply an average over ACSA labels and that the RP labels used by DSPN provide a strong signal.
7.3 Dimensional Bias Issue
DSPN uses star rating labels for training. However, the user rating may not be consistent with the overall sentiment
of the review text, thus generating the noise of distant labels. This is because the user may not have written all the
aspects in the review, or the user’s sentiment is heavily dominated by a certain aspect (Ge and Li, 2015). Therefore, we
conduct a simple additional experiment. In the experiment, we utilize several unsupervised sentiment analysis tools
(V ADER (Hutto and Gilbert, 2014), TextBlob (Loria, 2018), and Zero-shot text classification (Yin et al ., 2019)) to
directly generate sentiment labels, which will replace the star rating labels given by users for training.
Model Label Source Performance
DSPN Star ratings 53.2
UPN TextBlob 50.2
UPN V ADER 51.1
UPN Zero-shot 53.3
Table 8: DSPN results compared to a fully unsupervised pyramid network (UPN).
We name the version of DSPN as UPN (U for unsupervised), and we report the ACSA results of DSPN and UPN on
TripDMS. Results in Table 8 are mixed, demonstrating that while dimensional bias exists due to the inherent limitations
of star rating labels, its impact on DSPN’s overall performance is minimal.
8 Conclusion and Future Work
In this paper, we introduce unified sentiment analysis (Uni-SA) to integrate three key tasks within sentiment analysis.
To model Uni-SA, we propose a Distantly Supervised Pyramid Network (DSPN) that shows an efficiency advantage by
only using star rating labels for training. Experiments conducted on two multi-aspect datasets demonstrate the strong
performance of DSPN on RP and ACD – as well as the effectiveness – with only RP labels as supervision. DSPN’s
performance demonstrates the validity of considering sentiment analysis holistically, and this empirical evidence shows
that it is possible to use a signal from a single task (RP) to learn three distinct tasks effectively and efficiently. We hope
this paper spurs more research on novel Uni-SA approaches as well as the design of novel tasks that leverage one label
source for efficient learning for multiple tasks.
12Unified Sentiment Analysis with Distant Supervision A P REPRINT
We offer several notable substantive implications for practitioners. First, our framework does not require extensive
and costly manual annotations and can help managers efficiently generate actionable, granular insights from customer
feedback. This is particularly valuable for organizations aiming to enhance customer experiences without significant
investments in manual data labeling and organizations that do not have a sizable budget for market research. Second,
our framework informs the application of multi-task learning to integrate related tasks into a unified framework, which
can be applied to other domains where data interdependencies exist. For example, an AI tool assisting with hiring could
leverage labeled final outcomes (e.g., hired or not) in a pyramid structure to better model aspects such as cover letter
appropriateness, relevant keyword extraction from CVs, or appropriate candidate recognition for advertised positions.
In these applications, a pyramid structure would need to be augmented with earlier labels. Still, the signal from the top
of the pyramid (i.e., hired or not) could guide labeling decisions, mirroring the efficiency and interpretability achieved
in sentiment analysis while addressing domain-specific challenges.
This paper also has several limitations that shed light on promising avenues for future research. One limitation relates
to data availability. There are a number of datasets for ACSA and datasets for RP separately in the literature. However,
datasets rarely support unified sentiment analysis. Therefore, we were restricted to TripDMS and ASAP as the only
two datasets available for our main evaluation. We encourage future work on creating more datasets with both ACSA
and RP labels to drive further research in Uni-SA model development and evaluation. Next, DSPN currently does not
leverage the capabilities of Large Language Models (LLMs). A potent future direction could include the integration
of Large Language Models (LLMs) with DSPN’s pyramid structure. Although LLMs are highly advanced, they face
persistent challenges such as limited interpretability. Future research could investigate how combining LLMs with
DSPN can leverage their strengths while mitigating these limitations, ultimately advancing sentiment analysis tasks.
Acknowledgements
The authors would like to thank Lei Wang, Qiangming Yan, and Shuang Zheng for their helpful comments on earlier
versions of this work.
References
Samuel Brody and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews. In Human
Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics . 804–812.
Jiahao Bu, Lei Ren, Shuang Zheng, Yang Yang, Jingang Wang, Fuzheng Zhang, and Wei Wu. 2021. ASAP: A
chinese review dataset towards aspect category sentiment analysis and rating prediction. In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies . 2069–2079.
Siva Uday Sampreeth Chebolu, Franck Dernoncourt, Nedim Lipka, and Thamar Solorio. 2023. A review of datasets for
aspect-based sentiment analysis. In Proceedings of the 13th International Joint Conference on Natural Language
Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics
(Volume 1: Long Papers) . 611–628.
Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma. 2018. Neural attentional rating regression with review-level
explanations. In Proceedings of the 2018 World Wide Web Conference . 1583–1592.
Zhuang Chen and Tieyun Qian. 2020. Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment
Analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , Dan Jurafsky,
Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 3685–3694.
https://doi.org/10.18653/v1/2020.acl-main.340
Zhiyong Cheng, Ying Ding, Lei Zhu, and Mohan Kankanhalli. 2018. Aspect-aware latent factor model: Rating
prediction with ratings and reviews. In Proceedings of the 2018 World Wide Web Conference . 639–648.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) .
4171–4186.
Hao Fei, Jingye Li, Yafeng Ren, Meishan Zhang, and Donghong Ji. 2022. Making decision like human: Joint aspect
category sentiment analysis and rating prediction with fine-to-coarse reasoning. In Proceedings of the ACM Web
Conference 2022 . 3042–3051.
13Unified Sentiment Analysis with Distant Supervision A P REPRINT
Gayatree Ganu, Noémie Elhadad, and Amélie Marian. 2009. Beyond the stars: Improving rating predictions using
review text content. In International Workshop on the Web and Databases .
Aitor García-Pablos, Montse Cuadros, and German Rigau. 2018. W2VLDA: Almost unsupervised system for aspect
based sentiment analysis. Expert Systems with Applications 91 (2018), 127–137.
Yong Ge and Jingjing Li. 2015. Measure and mitigate the dimensional bias in online reviews and ratings. In International
Conference on Information System .
Erfan Ghadery, Sajad Movahedi, Masoud Jalili Sabet, Heshaam Faili, and Azadeh Shakery. 2019. Licd: A language-
independent approach for aspect category detection. In European Conference on Information Retrieval . 575–589.
Zhen Hai, Kuiyu Chang, and Jung-jae Kim. 2011. Implicit feature identification via co-occurrence association rule
mining. In International Conference on Intelligent Text Processing and Computational Linguistics . 393–404.
Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2017. An unsupervised neural attention model for
aspect extraction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) . 388–397.
Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2018. Exploiting document knowledge for aspect-level
sentiment classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers) . 579–585.
Jiaxin Huang, Yu Meng, Fang Guo, Heng Ji, and Jiawei Han. 2020. Weakly-supervised aspect-based sentiment analysis
via joint aspect-sentiment topic embedding. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) . 6989–6999.
Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media
text. In Proceedings of the International AAAI Conference on Web and Social Media , V ol. 8. 216–225.
Qingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and Min Yang. 2019. A challenge dataset and effective models for
aspect-based sentiment analysis. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . 6280–
6285.
Sabyasachi Kamila, Walid Magdy, Sourav Dutta, and MingXue Wang. 2022. AX-MABSA: A framework for extremely
weakly supervised multi-label aspect based sentiment analysis. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing . 6136–6147.
Yin Kang and Lina Zhou. 2019. Helpfulness assessment of online reviews: The role of semantic hierarchy of product
features. ACM Transactions on Management Information Systems (TMIS) 10, 3 (2019), 1–18.
John P Lalor, Ahmed Abbasi, Kezia Oketch, Yi Yang, and Nicole Forsgren. 2024. Should fairness be a metric or a
model? a model-based framework for assessing bias in machine learning pipelines. ACM Transactions on Information
Systems 42, 4 (2024), 1–41.
Fangtao Li, Nathan Nan Liu, Hongwei Jin, Kai Zhao, Qiang Yang, and Xiaoyan Zhu. 2011. Incorporating reviewer and
product information for review rating prediction. In 32nd International Joint Conference on Artificial Intelligence .
Junjie Li, Haitong Yang, and Chengqing Zong. 2018. Document-level multi-aspect sentiment classification by jointly
modeling users, aspects, and overall ratings. In Proceedings of the 27th International Conference on Computational
Linguistics . 925–936.
Qiudan Li, Daniel Dajun Zeng, David Jingjun Xu, Ruoran Liu, and Riheng Yao. 2020b. Understanding and predicting
users’ rating behavior: A cognitive perspective. INFORMS Journal on Computing 32, 4 (2020), 996–1011.
Yuncong Li, Cunxiang Yin, Sheng-hua Zhong, and Xu Pan. 2020a. Multi-instance multi-label learning networks
for aspect-category sentiment analysis. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) . 3550–3560.
Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis. In Mining Text Data . Springer,
415–463.
Jian Liu, Zhiyang Teng, Leyang Cui, Hanmeng Liu, and Yue Zhang. 2021. Solving aspect category sentiment analysis as
a text generation task. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing .
4406–4416.
Steven Loria. 2018. Textblob Documentation. Release 0.15 2 (2018).
Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad Al-Smadi,
Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, et al .2016. Semeval-2016 task 5: Aspect based
sentiment analysis. In International Workshop on Semantic Evaluation . 19–30.
14Unified Sentiment Analysis with Distant Supervision A P REPRINT
Maria Pontiki, Dimitrios Galanis, Harris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-
2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic
Evaluation (SemEval-2015) . 486–495.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar.
2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014) . 27–35.
Sebastian Ruder, Parsa Ghaffari, and John G Breslin. 2016. INSIGHT-1 at semeval-2016 task 5: Deep Learning
for multilingual aspect-based sentiment analysis. In Proceedings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016) . 330–336.
Martin Schmitt, Simon Steinheber, Konrad Schreiber, and Benjamin Roth. 2018. Joint aspect and polarity classification
for aspect-based sentiment analysis with end-to-end neural networks. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing . 1109–1114.
Kim Schouten and Flavius Frasincar. 2015. Survey on aspect-level sentiment analysis. IEEE Transactions on Knowledge
and Data Engineering 28, 3 (2015), 813–830.
Kim Schouten, Flavius Frasincar, and Franciska De Jong. 2014. Commit-p1wp3: A co-occurrence based approach
to aspect-level sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation
(SemEval-2014) . 203–207.
Vineet K. Sejwal and Muhammad Abulaish. 2022. A hybrid recommendation technique using topic embedding for
rating prediction and to handle cold-start problem. Expert Systems with Applications 209 (2022), 118307.
Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune bert for text classification. In Chinese
Computational Linguistics: 18th China National Conference . 194–206.
Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the
17th International Conference on World Wide Web . 111–120.
Zhiqiang Toh and Jian Su. 2016. Nlangp at semeval-2016 task 5: Improving aspect based sentiment analysis using
neural network features. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016) .
282–288.
Stéphan Tulkens and Andreas van Cranenburgh. 2020. Embarrassingly simple unsupervised aspect extraction. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . 3182–3187.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: a rating
regression approach. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining . 783–792.
Chuhan Wu, Fangzhao Wu, Junxin Liu, Yongfeng Huang, and Xing Xie. 2019. Arp: Aspect-aware neural review rating
prediction. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management .
2169–2172.
Xueke Xu, Songbo Tan, Yue Liu, Xueqi Cheng, and Zheng Lin. 2012. Towards jointly extracting aspects and
aspect-specific sentiment knowledge. In Proceedings of the 21st ACM International Conference on Information and
Knowledge Management . 1895–1899.
Wei Xue and Tao Li. 2018. Aspect based sentiment analysis with gated convolutional networks. In Proceedings of the
56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . 2514–2523.
Wei Xue, Wubai Zhou, Tao Li, and Qing Wang. 2017. MTNA: A neural multi-task model for aspect category
classification and aspect term extraction on restaurant reviews. In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Volume 2: Short Papers) . 151–156.
Wenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Benchmarking zero-shot text classification: Datasets, evaluation and
entailment approach. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th Interna-
tional Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019 . Association for Computational
Linguistics, 3914–3923.
Yichun Yin, Yangqiu Song, and Ming Zhang. 2017. Document-level multi-aspect sentiment classification as machine
comprehension. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing .
2044–2054.
Zelin Zhang, Kejia Yang, Jonathan Z Zhang, and Robert W Palmatier. 2023. Uncovering synergy and dysergy in
consumer reviews: A machine learning approach. Management Science 69, 4 (2023), 2339–2360.
15Unified Sentiment Analysis with Distant Supervision A P REPRINT
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a
maxent-lda hybrid. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing .
56–65.
Huachi Zhou, Shuang Zhou, Hao Chen, Ninghao Liu, Fan Yang, and Xiao Huang. 2024. Enhancing explainable rating
prediction through annotated macro concepts. In Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) . 11736–11748.
A Additional Benchmarking
Tables 9, 11, and 10 present the comprehensive results of our benchmarking. We selected our pipeline models from
these benchmarks based on predictive performance and efficiency.
TripDMS ASAP
Model Accuracy Params Train Time Accuracy Params Train Time
DSPN 70.5 5.28M 12min 78.5 6.1M 13min
DSPN-BERT 72.5 102.92M 95min 81.3 111M 88min
BERT-Feat 71.4 80.15M 35min 79.2 80.8M 42min
BERT-FiT 72.2 81M 37min 81 81.25M 30min
BERT-ITPT-FiT 72.4 82.7M 102min 80.3 91M 110min
Table 9: Comprehensive RP Results
TripDMS ASAP
Model Accuracy Params Train Time Accuracy Params Train Time
DSPN 51.4 5.28M 12min 64.4 6.1M 13min
DSPN-BERT 53.2 102.92M 95min 65.4 111M 88min
End2end-LSTM 57.4 5.3M 8min 66.1 6.22M 8min
End2end-CNN 57.9 5.12M 7min 65.2 5.32M 7min
GCAE 55.1 4.23M 5min 70.3 4.4M 6min
AC-MIMLLN 62.1 31M 50min 76 31.2M 50min
AC-MIMLLN-BERT 64.3 105M 55min 77.2 107,2M 55min
ACSA-Generation 64.1 142M 208min 76.1 145.18M 210min
Table 10: Comprehensive ACSA Results
TripDMS ASAP
Model F1 Params Train Time F1 Params Train Time
DSPN 92.7 5.28M 12min 78.6 6.1M 13min
DSPN-BERT 92.7 102.92M 95min 79.4 111M 88min
ABAE 91.2 3.1M 15min 79.4 3.1M 15min
ABAE-BERT 92.3 91.2M 40min 80.1 97.5M 42min
Table 11: Comprehensive ACD Results
16