Proceedings of Machine Learning Research vol XX:1‚Äì12, 2023
Policy Evaluation in Distributional LQR
Zifan Wang1ZIFANW @KTH.SE
Yulong Gao2YULONG .GAO@CS.OX.AC.UK
Siyi Wang3SIYI.WANG @TUM .DE
Michael M. Zavlanos4MICHAEL .ZAVLANOS @DUKE .EDU
Alessandro Abate2ALESSANDRO .ABATE @CS.OX.AC.UK
Karl H. Johansson1KALLEJ @KTH.SE
1Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden
2Department of Computer Science, University of Oxford, UK
3Chair of Information-oriented Control, Technical University of Munich, Germany
4Department of Mechanical Engineering and Materials Science, Duke University, USA
Abstract
Distributional reinforcement learning (DRL) enhances the understanding of the effects of the ran-
domness in the environment by letting agents learn the distribution of a random return, rather than
its expected value as in standard RL. At the same time, a main challenge in DRL is that policy
evaluation in DRL typically relies on the representation of the return distribution, which needs to
be carefully designed. In this paper, we address this challenge for a special class of DRL problems
that rely on discounted linear quadratic regulator (LQR) for control, advocating for a new distribu-
tional approach to LQR, which we call distributional LQR . SpeciÔ¨Åcally, we provide a closed-form
expression of the distribution of the random return which, remarkably, is applicable to all exoge-
nous disturbances on the dynamics, as long as they are independent and identically distributed
(i.i.d.). While the proposed exact return distribution consists of inÔ¨Ånitely many random variables,
we show that this distribution can be approximated by a Ô¨Ånite number of random variables, and the
associated approximation error can be analytically bounded under mild assumptions. Using the ap-
proximate return distribution, we propose a zeroth-order policy gradient algorithm for risk-averse
LQR using the Conditional Value at Risk (CVaR) as a measure of risk. Numerical experiments are
provided to illustrate our theoretical results.
Keywords: Distributional LQR, distributional RL, policy evaluation, risk-averse control
1. Introduction
In reinforcement learning, the value of implementing a policy at a given state is captured by a value
function, which models the expected sum of returns following this prescribed policy. Recently,
Bellemare et al. (2017) proposed the notion of distributional reinforcement learning (DRL), which
learns the return distribution of a policy from a given state, instead of only its expected return.
Compared to the scalar expected value function, the return distribution is inÔ¨Ånite-dimensional and
contains far more information. It is, therefore, not surprising that a few DRL algorithms, including
C51 (Bellemare et al., 2017), D4PG (Barth-Maron et al., 2018), QR-DQN (Dabney et al., 2018b)
and SDPG (Singh et al., 2022), dramatically improve the empirical performance in practical appli-
cations over their non-distributional counterpart.
¬© 2023 Z. Wang, Y . Gao, S. Wang, M.M. Zavlanos, A. Abate & K.H. Johansson.arXiv:2303.13657v1  [math.OC]  23 Mar 2023POLICY EVALUATION IN DISTRIBUTIONAL LQR
In DRL, the practical effectiveness of algorithms builds on the theory by Bellemare et al. (2017),
where the distributional Bellman operator is shown to be a contraction in the (maximum form of) the
Wasserstein metric between probability distributions. However, it is usually difÔ¨Åcult to characterise
the exact return distribution in DRL with Ô¨Ånite data. Approximations of the return distribution
are thus necessary to make it computable in practice. To address this challenge, Bellemare et al.
(2017) propose a categorical method that partitions the return distribution into a Ô¨Ånite number of
uniformly spaced atoms in a Ô¨Åxed region. One drawback of this method is that it relies on prior
knowledge of the range of the returned values. To address this limitation, a quantile function method
(Dabney et al., 2018b) and a sample-based method (Singh et al., 2022) have been recently proposed.
However, these works cannot provide an analytical expression for the approximation error, and
computational cost needs to be decided manually to guarantee approximation accuracy.
In this paper, we characterise the return distribution of the random cost for the classical dis-
counted linear quadratic regulator (LQR) problem, which we term distributional LQR . To the best
of our knowledge, the return distribution in LQR has not been explored in the literature. Our con-
tributions are summarised as follows:
1. We provide an analytical expression of the random return for distributional LQR problems and
prove that this return function is a Ô¨Åxed-point solution of the distributional Bellman equation.
SpeciÔ¨Åcally, we show that the proposed analytical expression consists of inÔ¨Ånitely many ran-
dom variables and holds for arbitrary i.i.d. exogenous disturbances, e.g., non-Gaussian noise
or noise with non-zero mean.
2. We develop an approximation of the distribution of the random return using a Ô¨Ånite number
of random variables. Under mild assumptions, we theoretically show that the sup of the
difference between the exact and approximated return distributions deceases linearly with the
numbers of random variables: this is also validated by numerical experiments.
3. The proposed analytical return distribution provides a theoretical foundation for distributional
LQR, allowing for general optimality criteria for policy improvement. In this work, we em-
ploy the return distribution to analyse risk-averse LQR problems using the Conditional Value
at Risk (CVaR) as the risk measure. Since the gradient of CVaR is generally difÔ¨Åcult to
compute analytically, we propose a risk-averse policy gradient algorithm that relies on the
zeroth-order optimisation to seek an optimal risk-averse policy. Numerical experiments are
provided to showcase this application.
Related Work: Most closely related to the problem considered in this paper is work on reinforce-
ment learning for LQR, which focuses on learning the expected return through interaction with the
environment; see, e.g., Dean et al. (2020); Tu and Recht (2018); Fazel et al. (2018); Malik et al.
(2019); Li et al. (2021); Yaghmaie et al. (2022); Zheng et al. (2021). For example, Fazel et al.
(2018) propose a model-free policy gradient algorithm for LQR and showed its global convergence
with Ô¨Ånite polynomial computational and sample complexity. Moreover, Zheng et al. (2021) study
model-based reinforcement learning for the Linear Quadratic Gaussian problems, in which a model
is Ô¨Årst learnt from data and then used to design the policy. However, all these works rely on the
expected return instead of the return distribution, hence these methods cannot be applied here.
Since the return distribution captures the intrinsic randomness of the long-term cost, it provides
a natural framework to consider more general optimality criteria, e.g., optimal risk-averse poli-
cies. There exist recent works on risk-averse policy design for DRL, including Singh et al. (2020);
2POLICY EVALUATION IN DISTRIBUTIONAL LQR
Dabney et al. (2018a); Tang et al. (2019). For example, the work in Dabney et al. (2018a) use
the quantile function to approximate the return distribution, which is then applied to design risk-
sensitive policies for Atari games. On the other hand, Singh et al. (2020) show that risk-averse DRL
achieves robustness against system disturbances in continuous control tasks. All these works focus
on empirical improvements in speciÔ¨Åc tasks, however, without theoretical analysis. Related to this
paper is also work on risk-sensitive LQR, which has been studied in Van Parys et al. (2015); Tsiamis
et al. (2021); Kim and Yang (2021); Chapman and Lessard (2021); Kishida and Cetinkaya (2022).
Similarly, these methods however do not analyse the return distribution.
2. Problem Statement
Consider a discrete-time linear dynamical system:
xt+1=Axt+But+vt; (1)
wherext2Rn,ut2Rp,vt2Rnare the system state, control input, and the exogenous distur-
bance, respectively. We assume that the exogenous disturbances vtwith bounded moments, t2N,
are i.i.d. sampled from a distribution Dof arbitrary form.
2.1. Classical LQR
The canonical LQR problem aims to Ô¨Ånd a control policy :Rn!Rpto minimise the objective
J(u) =E"1X
t=0t(xT
tQxt+uT
tRut)#
; (2)
whereQ;R are positive-deÔ¨Ånite constant matrices and 2(0;1)is a discount parameter. Given a
control policy , letV(x) =EP1
t=0k(xT
tQxt+uT
tRut)
denote the expected return from an
initial statex0=xwithut=(xt). For the static linear policy (xt) =Kxt, the value function
V(x)satisÔ¨Åes the Bellman equation
V(x) =xT(Q+KTRK)x+ E
X0=(A+BK)x+v0[V(X0)]; (3)
where the capital letter X0denotes a random variable over which we take the expectation.
When the exogenous disturbances vtare normally distributed with zero mean, the value function
is known to take the quadratic form V(x) =xTPx+q, whereP > 0is the solution of the
Lyapunov equation P=Q+KTRK+AT
KPAKandqis a scalar related to the variance of vt. In
particular, the optimal control feedback gain is obtained as K= (R+BTPB) 1PAandPis
the solution to the classic Riccati equation P=ATPA 2ATPB(R+BTPB) 1BTPA+Q.
2.2. Distributional LQR
Motivated by the advantages of DRL in better understanding the effects of the randomness in the
environment and in considering more general optimality criteria, in this paper we propose a distri-
butional approach to the LQR problem. Unlike classical reinforcement learning, which relies on
expected returns, DRL (Bellemare et al., 2023) relies on the distribution of random returns. The
return distribution characterises the probability distribution of different returns generated by a given
3POLICY EVALUATION IN DISTRIBUTIONAL LQR
policy and, as such, it contains much richer information on the performance of a given policy com-
pared to the expected return. In the context of LQR, we denote by G(x)the random return using
the static control strategy ut=(xt)from the initial state x0=x, which is deÔ¨Åned as
G(x) =1X
t=0t(xT
tQxt+uT
tRut); ut=(xt);x0=x: (4)
It is straightforward to see that the expectation of G(x)is equivalent to the value function V(x).
The standard Bellman equation in (3) decomposes the long-term expected return into an immediate
stage cost plus the expected return of future actions starting at the next step. Similarly, we can deÔ¨Åne
the distributional Bellman equation for the random return as
G(x)D=xTQx+(x)TR(x) +G(X0); X0=Ax+B(x) +v0: (5)
Here we use the notationD=to denote that two random variables Z1;Z2are equal in distribution,
i.e.,Z1D=Z2. Note thatX0denotes a random variable, as in (3). Compared to the expected return in
LQR, which is a scalar, here the return distribution is inÔ¨Ånite-dimensional and can have a complex
form. It is challenging to estimate an inÔ¨Ånite-dimensional function exactly with Ô¨Ånite data and thus
an approximation of the return distribution is necessary in practice.
In this paper, we Ô¨Årst analytically characterise the random return for the LQR problem. Then
we show how to approximate the distribution of the random return using Ô¨Ånite random variables,
so such that the approximated distribution is computationally tractable and the approximation error
is bounded. The proposed distributional LQR framework allows us to consider more general opti-
mality criteria, which we demonstrate by using the proposed return distribution to develop a policy
gradient algorithm for risk-averse LQR.
3. Main Results
3.1. Exact Form of the Return Distribution
In this section, we precisely characterise the distribution of the random return that satisÔ¨Åes the
distributional Bellman equation (5). Given a static linear policy (xt) =Kxt, we denote by GK(x)
the random return G(x)under the policy (xt)from the initial state x0=x, which is deÔ¨Åned as
GK(x) =1X
t=0txT
t(Q+KTRK)xt; x 0=x:
The random return GK(x)satisÔ¨Åes the following distributional Bellman equation
GK(x)D=xTQKx+GK(X0); X0=AKx+v0; (6)
whereAK:=A+BK andQK:=Q+KTRK. In the following theorem, we provide an explicit
expression of the random return GK(x).
Theorem 1 Suppose that the feedback gain Kis stabilizing, i.e., AK=A+BK is stable. Let
GK(x) =xTPx+1X
k=0k+1wT
kPwk+ 21X
k=0k+1wT
kPAk+1
Kx+ 21X
k=1k+1wT
kPk 1X
=0Ak 
Kw;
(7)
4POLICY EVALUATION IN DISTRIBUTIONAL LQR
wherePis obtained from the algebraic Riccati equation P=Q+KTRK+AT
KPAK, and
the random variables wkD are independent from each other for all k2N. Then, the random
variableGK(x)deÔ¨Åned in (7)is a Ô¨Åxed point solution to the distributional Bellman equation (6).
Proof Recall thatX0=AKx+v0, wherev0is a random variable sampled from the distribution D
and is independent from wk,k2N, in (7). Substituting (7) into the right hand side of the equation
(6), we have that
xT(Q+KTRK)x+GK(X0)
=xTQKx+X0TPX0+1X
t=0t+2wT
tPwt+ 21X
t=0t+2wT
tPAt+1
KX0
+ 21X
t=1t+2wT
tPAKt 1X
i=0At 1 i
Kwi
=xTQKx+(AKx+v0)TP(AKx+v0) +21X
t=0twT
tPwt+ 221X
t=1twT
tPt 1X
i=0At i
Kwi
+ 221X
t=0twT
tPAt+1
K(AKx+v0)
=xT(QK+AT
KPAK)x+vT
0Pv0+21X
t=0twT
tPwt
|{z}
:=T1+ 2vT
0PAKx+ 221X
t=0twT
tPAt+2
Kx
| {z }
:=T2
+ 221X
t=1twT
tPt 1X
i=0At i
Kwi+ 221X
t=0twT
tPAt+1
Kv0
| {z }
:=T3:
DeÔ¨Åne0:=v0,t=wt 1,t= 1;2;:::. From the deÔ¨Ånition of the term T1, we have that
T1=vT
0Pv0+21X
t=0twT
tPwtk=t+1=T
0P0+1X
k=1kT
kPk=1X
k=0kT
kPk:
For the term T2, we have that
T2= 2vT
0PAKx+ 221X
t=0twT
tPAt+2
Kx= 2T
0PAKx+ 221X
t=0tT
t+1PAt+2
Kx
k=t+1= 2T
0PAKx+ 21X
k=1kT
kPAk+1
Kx= 21X
k=0kT
kPAk+1
Kx:
Using similar techniques for the term T3, we obtain that T3= 2P1
k=1kT
kPAKPk 1
i=0Ak 1 i
Ki:
Due to the fact that P=Q+KTRK+AT
KPAK, we have
xTQKx+GK(X0) =xTPx+T1+T2+T3
=xTPx+1X
k=0kT
kPk+ 21X
k=0kxTPAk+1
Kk+ 21X
k=1kT
kPAKk 1X
i=0Ak 1 i
Ki;(8)
5POLICY EVALUATION IN DISTRIBUTIONAL LQR
which is in the same form as in (7). Since fkg1
k=0andfwkg1
k=0are i.i.d., we have that the two
random variables (7) and (8) have the same distribution, i.e., GK(x)D=xTQKx+GK(X0):
3.2. Approximation of the Return Distribution with Finite Parameters
In this section, we show how to approximate the random return deÔ¨Åned in (7) using a Ô¨Ånite number
of random variables. Considering only the Ô¨Årst Nterms in the summations in the expression in (7)
and disregarding the terms for klarger thanNyields the following:
GK
N(x) =xTPx+N 1X
k=0k+1wT
kPwk+ 2N 1X
k=0k+1wT
kPAk+1
Kx+ 2N 1X
k=1k+1wT
kPk 1X
=0Ak 
Kw:
(9)
LetFK
xandFK
x;Ndenote the cumulative distribution function (CDF) of GK(x)andGK
N(x), respec-
tively. The following theorem provides an upper bound on the difference between FK
xandFK
x;N,
and shows that the sequence fGK
N(x)gN2Nconverges pointwise in distribution to GK(x),8x2Rn.
Theorem 2 Assume that the probability density functions of wkexist and are bounded, and satisfy
E[wT
kwk]2
0,E[kwkk2]0, for8k2N. Suppose that the feedback gain Kis stabilizing such
thatkAKk2=K<1. Then, the sup difference between the CDFs FK
xandFK
x;Nis bounded by
sup
zjFK
x(z) FK
x;N(z)jCN; (10)
whereCis a constant that depends on the matrices A;B;Q;R;K , the initial state value x, and the
parameters;K;0;0.
Proof DeÔ¨ÅneYN:=GK(x) GK
N(x), we have
sup
zjFK
x(z) FK
x;N(z)j= sup
zjP(GK
N(x)z) P(GK(x)z)j
= sup
zjP(GK
N(x)z) P(GK
N(x) +YNz)j
= sup
zP(GK
N(x)z)Z1
 1P(YN=t)dt Z1
 1P(GK
N(x)z t)P(YN=t)dt
= sup
zZ1
 1P(YN=t) 
FK
x;N(z) FK
x;N(z t)
dt: (11)
Since the random variables wtare i.i.d for all t>0and the probability density function of wtexists,
the function FK
x;Nis continuous and differentiable. Applying the mean value theorem, when t>0
there exists a point z02[z t;z]such thatFK
x;N(z) FK
x;N(z t) =fK
x;N(z0)t, wherefK
x;Nis the
probability density function of GK
N(x). Since the probability density function of wtis bounded, it
further follows that fK
x;Nis bounded. Then, we have that jFK
x;N(z) FK
x;N(z t)j=jfK
x;N(z0)tj
L0jtj, whereL0is an upper bound of the probability function fK
x;N. Following a similar argument,
we can show that this inequality holds when t0. Substituting this inequality into (11), we obtain
sup
zjFK
x(z) FK
x;N(z)jsup
zZ1
 1P(YN=t)L0jtjdt=L0EjYNj: (12)
6POLICY EVALUATION IN DISTRIBUTIONAL LQR
From the deÔ¨Ånition of YN, we obtain that
YN=1X
k=Nk+1wT
kPwk+ 21X
k=Nk+1wT
kPAk+1
Kx+ 21X
k=Nk+1wT
kPk 1X
=0Ak 
Kw
t=k N=N1X
t=0t+1wT
t+NPwt+N+ 21X
t=0t+1wT
t+NPAt+N+1
Kx
+ 21X
t=0t+1wT
t+NPt+N 1X
=0At+N 
Kw
:
Taking the expectation of the absolute value of YN, we have
EjYNjN1X
t=0t+1EjwT
t+NPwt+Nj+ 21X
t=0t+1EjwT
t+NPAt+N+1
Kxj
+ 21X
t=0t+1EjwT
t+NPt+N 1X
=0At+N 
Kwj
:
We handle the terms in the above inequality one by one. For the Ô¨Årst term, we have that
1X
t=0t+1EjwT
t+NPwt+Nj1X
t=0t+1Ejmax(P)wT
t+Nwt+Njmax(P)2
0
1 : (13)
For the second term, we have that
21X
t=0t+1EjwT
t+NPAt+N+1
Kxj21X
t=0t+1kPk2At+N+1
K
2kxk2
21X
t=0t+1kPk2t+N 1
Kkxk22kPk2jxjN 1
K
1 K2kPk2jxj
1 K; (14)
where the second inequality is due to the fact thatAt+N+1
K
2(kAKk2)t+N+1t+N+1
Kand
the last inequality follows from the fact that N1. For the third term, we have that
21X
t=0t+1EjwT
t+NPt+N 1X
=0At+N 
Kwj21X
t=0t+1E"
wT
t+N
2kPk2t+N 1X
=0At+N 
Kw
2#
2kPk21X
t=0t+1E"t+N 1X
=0At+N 
Kw
2#
2kPk21X
t=0t+1E"t+N 1X
=0At+N 
K
2kwk2#
22kPk21X
t=0t+1t+N 1X
=0t+N 
K22kPk21X
t=0t+1K
1 K22kPk2K
(1 )(1 K);
(15)
where the second inequality is due to the fact that wandwt+Nare independent and the second to
last inequality follows from the fact thatPt+N 1
=0t+N 
K=Pt+N
=1
KK
1 K. Combining (13),
7POLICY EVALUATION IN DISTRIBUTIONAL LQR
(14) and (15), we have that
sup
zjFK
x(z) FK
x;N(z)jL0EjYNj
L0N
max(P)2
0
1 + 2kPk2jxj
1 K+ 22kPk2K
(1 )(1 K)
:=CN:
The proof is complete and also yields the expression of the constant C.
Remark 3 The bound on the distribution approximation in (10) relies on the conditions of Theo-
rem 2, which ensure that the PDF of GK
Nis continuous and bounded. Note that these conditions
are not particularly strict, and indeed hold for many noise distributions commonly used in linear
dynamical systems, including Gaussian and uniform. Future work will investigate relaxations of
these conditions.
3.3. Numerical Experiments on Quality of the Approximation of the Return Distribution
In the following experiment, we consider a scalar model with matrices A=B= 1. Similarly, the
weighting matrices in the LQR cost are chosen as Q=R= 1. The exogenous disturbances are
standard normal distributions with zero mean.
Even for this scalar system, it is impossible to simplify the expression of the exact return distri-
bution, which still depends on an inÔ¨Ånite number of random variables. Thus, as a baseline for the
return distribution, we generate an empirical distribution that approximates the true distribution of
the random return. More speciÔ¨Åcally, we use the Monte Carlo (MC) method to obtain 10000 samples
of the random return and use the sample frequency over evenly-divided regions as an approximation
of the probability density function. According to the law of large numbers, the empirical distribution
approaches the real one as the number of trials increases. Note that, although the MC method pro-
vides an alternative way to approximate the return distribution, it relies on using sufÔ¨Åciently many
samples that can be time-consuming, and its (statistical) approximation error is generally difÔ¨Åcult
to analyse. Thus, the MC method is not applicable for practical policy evaluation of distributional
LQR, and in this experiment, it is used only to verify our approximate return distribution. In com-
parison, the approximate return distribution using Ô¨Ånite number of random variables in this paper
is analytical for policy evaluation and the corresponding approximation error can be bounded: as
such, it is further usable for policy optimisation, as shown in Section 4. We denote here by fNthe
distribution of the approximated random return GK
N(x0)obtained considering Nrandom variables.
We Ô¨Åx the feedback gain as K= 0:4684 and select different values of andx0. The results
are shown in Fig. 1. SpeciÔ¨Åcally, Fig. 1 (a) and (c) show that when is small, the return distribution
can be well approximated using only few random variables ( N= 3works well). However, when 
approaches 1, more random variables are needed for an accurate approximation: we employ N= 15
andN= 20 random variables in the case of = 0:8and= 0:85, respectively, as shown in Fig. 1
(b) and (d). Moreover, the value of the initial state x0has an inÔ¨Çuence on the shape of the return
distribution, which can be clearly observed from the scalar case. When x0is large, the random
variablewT
kPAk+1
Kx0dominates and, therefore, its distribution is close to a Gaussian distribution,
as shown in Fig. 1 (c) and (d). If instead x0is small, then the random variable wT
kPwkplays a
leading role, so the overall distribution is close to the chi-square one, as shown in Fig. 1 (a) and (b).
In conclusion, when Nis large, the approximate distribution is closer to the distribution obtained
from the MC method, and thus to the true distribution.
8POLICY EVALUATION IN DISTRIBUTIONAL LQR
-10 0 10 20
Values00.20.4Probability DensityMC
f3
f15
(a)= 0:6,x0= 1.
-10 0 10 20
Values00.10.20.3Probability DensityMC
f3
f15(b)= 0:8,x0= 1.
-20 0 20 40
Values00.020.040.060.08Probability DensityMC
f3
f15(c)= 0:6,x0= 8.
-20 0 20 40
Values00.020.040.06Probability DensityMC
f3
f20(d)= 0:85,x0= 8.
Figure 1: Return distribution and its approximation with Ô¨Ånite number of random variables for
differentandx0. MC denotes the distribution returned by the Monte Carlo method and
fNdenotes the distribution of the approximated random return GK
N(x0).
Algorithm 1 Risk-Averse Policy Gradient
Require: initial values K0,x, step size, smoothing parameter , and dimension n
1:forepisodet = 1;:::;T do
2: Sample ^Kt=Kt+Ut, whereUtis drawn at random over matrices whose norm is ;
3: Compute the distribution of the random variable G^Kt
N;
4: Compute ^CN(^Kt);
5:Kt+1=Kt gt, wheregt=n
2
^C(^Kt) ^C(^Kt 1)
Ut.
6:end for
4. Application to Risk-Averse LQR
In this section, we consider a risk-averse LQR problem and leverage the closed-form expression
of the random return GK(x)to obtain an optimal policy. Since the distribution of the random
returnGK(x)consists of an inÔ¨Ånite number of random variables, it is computationally unwieldy.
Instead, we employ the approximate random return GK
N(x)proposed in Section 3.2. As a risk
measure for the problem at hand, we select the well-known Conditional Value at Risk (CVaR)
(Rockafellar et al., 2000). We then construct an approximate risk-averse objective function, as
^CN(K) := CVaR 
GK
N(x)
. For a random variable Zwith the CDF Fand a risk level 2(0;1],
theCVaR value is deÔ¨Åned as CVaR[Z] =EF[ZjZ >Z], whereZis the 1 quantile of the
distribution of the random variable Z. Given this objective function, the goal is to Ô¨Ånd the optimal
risk-averse controller, that is, to select the feedback gain Kthat minimises ^CN(K).
4.1. Risk-Averse Policy Gradient Algorithm
In what follows, we propose a policy gradient method to solve this problem. We assume that the
matricesA;B;Q;R are known. The Ô¨Årst-order gradient descent step is hard to compute as it hinges
on the gradient of the CVaR function. Therefore, we rely on zeroth-order optimisation to derive the
policy gradient, as detailed in Algorithm 1.
SpeciÔ¨Åcally, at each episode t, we sample an approximate feedback gain ^Kt=Kt+Ut, where
Utis drawn uniformly at random from the set of matrices with norm . Given ^Kt, we compute the
approximate distribution of the random return G^Kt
N(x)in (9) and the value of ^CN(^Kt). Then, we
can perform the feedback gain update as Kt+1=Kt gt, wheregt=n
2
^C(^Kt) ^C(^Kt 1)
Ui.
9POLICY EVALUATION IN DISTRIBUTIONAL LQR
0 1000 2000
Iterations-0.5-0.4-0.3-0.2KAlgorithm 1
K*
(a) TheKvalues when
= 1.
0 1000 2000
Iterations3.844.24.44.6Algorithm 1(b) The CVaR values
when= 1.
0 1000 2000
Iterations-0.6-0.4-0.2KAlgorithm 1(c) TheKvalues when
= 0:4.
0 1000 2000
Iterations678Algorithm 1(d) The CVaR values
when= 0:4.
Figure 2: Risk-averse control using Algorithm 1. The solid lines are averages over 20 runs.
Here, the zeroth-order residual feedback technique proposed in Zhang et al. (2022) is used to reduce
the variance. The theoretical analysis of this algorithm is left as our future work.
4.2. Numerical Experiments
Next, we consider a risk-averse LQR problem and experimentally illustrate the performance of
Algorithm 1. We illustrate our approach for the same scalar system with the same cost function
as in Section 3.3. The other parameters are selected as = 0:6,= 0:1,= 0:0004 ,N= 10 ,
respectively. The initial controller is set as K0= 0:2, which is a stable one.
We Ô¨Årst set = 1: in this case, the risk-averse control problem is reduced to a risk-neutral
control problem. Therefore, we can use traditional LQR techniques to compute the optimal feed-
back gainK= 0:4684 . We run the proposed risk-averse policy gradient Algorithm 1 and the
simulation results are presented in Fig. 2 (a) and (b). SpeciÔ¨Åcally, in Fig. 2 (a), the controller K
returned by Algorithm 1 converges to K, which veriÔ¨Åes our proposed method for the risk-neutral
case. Fig. 2 (b) illustrates the values of CVaR achieved by Algorithm 1. Additionally, we select
= 0:4to Ô¨Ånd the optimal risk-averse controller. The simulation results are presented in Fig. 2
(c) and (d). We see that Kconverges to 0:55, which leads to a smaller A+BK compared to
K= 0:4684 .
5. Conclusions
We have proposed a new distributional approach to the classic discounted LQR problem. SpeciÔ¨Å-
cally, we Ô¨Årst provided an analytic expression for the exact random return that depends on inÔ¨Ånitely
many random variables. Since the computation of this expression is difÔ¨Åcult in practice, we also
proposed an approximate expression for the distribution of the random return that only depends on
a Ô¨Ånite number of random variables, and have further characterised the error between these two
distributions. Finally, we utilised the proposed random return to obtain an optimal controller for a
risk-averse LQR problem using the CVaR as a measure of risk. To the best of our knowledge, this
is a Ô¨Årst framework for distributional LQR: it inherits the advantages of DRL methods compared
to standard RL methods that rely on the expected return to evaluate the effect of a given policy,
but it also provides an analytic expression for the return distribution, an area where current DRL
methods signiÔ¨Åcantly lack. Future research includes analyzing the theoretical convergence of risk-
averse policy gradient algorithms and exploring a model-free setup where the system matrices are
unknown.
10POLICY EVALUATION IN DISTRIBUTIONAL LQR
Acknowledgments
This work is supported in part by the Knut and Alice Wallenberg Foundation, the Swedish Strategic
Research Foundation, the Swedish Research Council, AFOSR under award #FA9550-19-1-0169,
and NSF under award CNS-1932011.
References
Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. arXiv preprint arXiv:1804.08617 , 2018.
Marc G Bellemare, Will Dabney, and R ¬¥emi Munos. A distributional perspective on reinforcement
learning. In Proceedings of International Conference on Machine Learning , pages 449‚Äì458.
PMLR, 2017.
Marc G. Bellemare, Will Dabney, and Mark Rowland. Distributional Reinforcement Learning . MIT
Press, 2023. http://www.distributional-rl.org .
Margaret P Chapman and Laurent Lessard. Toward a scalable upper bound for a CVaR-LQ problem.
IEEE Control Systems Letters , 6:920‚Äì925, 2021.
Will Dabney, Georg Ostrovski, David Silver, and R ¬¥emi Munos. Implicit quantile networks for
distributional reinforcement learning. In Proceedings of International Conference on Machine
Learning , pages 1096‚Äì1105. PMLR, 2018a.
Will Dabney, Mark Rowland, Marc Bellemare, and R ¬¥emi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of AAAI Conference on ArtiÔ¨Åcial Intelligence ,
volume 32, 2018b.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample com-
plexity of the linear quadratic regulator. Foundations of Computational Mathematics , 20(4):
633‚Äì679, 2020.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gra-
dient methods for the linear quadratic regulator. In Proceedings of International Conference on
Machine Learning , pages 1467‚Äì1476. PMLR, 2018.
Kihyun Kim and Insoon Yang. Distributional robustness in minimax linear quadratic control with
Wasserstein distance. arXiv preprint arXiv:2102.12715 , 2021.
Masako Kishida and Ahmet Cetinkaya. Risk-aware linear quadratic control using conditional value-
at-risk. IEEE Transactions on Automatic Control , 2022.
Yingying Li, Yujie Tang, Runyu Zhang, and Na Li. Distributed reinforcement learning for decentral-
ized linear quadratic control: A derivative-free policy optimization approach. IEEE Transactions
on Automatic Control , 2021.
11POLICY EVALUATION IN DISTRIBUTIONAL LQR
Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter Bartlett, and Martin Wain-
wright. Derivative-free methods for policy optimization: guarantees for linear quadratic systems.
InProceedings of 22nd International Conference on ArtiÔ¨Åcial Intelligence and Statistics , pages
2916‚Äì2925. PMLR, 2019.
R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal
of Risk , 2:21‚Äì42, 2000.
Rahul Singh, Qinsheng Zhang, and Yongxin Chen. Improving robustness via risk averse distribu-
tional reinforcement learning. In Proceedings of Learning for Dynamics and Control Conference ,
pages 958‚Äì968. PMLR, 2020.
Rahul Singh, Keuntaek Lee, and Yongxin Chen. Sample-based distributional policy gradient. In
Proceedings of Learning for Dynamics and Control Conference , pages 676‚Äì688. PMLR, 2022.
Yichuan Charlie Tang, Jian Zhang, and Ruslan Salakhutdinov. Worst case policy gradients. arXiv
preprint arXiv:1911.03618 , 2019.
Anastasios Tsiamis, Dionysios S Kalogerias, Alejandro Ribeiro, and George J Pappas. Linear
quadratic control with risk constraints. arXiv preprint arXiv:2112.07564 , 2021.
Stephen Tu and Benjamin Recht. Least-squares temporal difference learning for the linear quadratic
regulator. In Proceedings of International Conference on Machine Learning , pages 5005‚Äì5014.
PMLR, 2018.
Bart PG Van Parys, Daniel Kuhn, Paul J Goulart, and Manfred Morari. Distributionally robust
control of constrained stochastic systems. IEEE Transactions on Automatic Control , 61(2):430‚Äì
442, 2015.
Farnaz Adib Yaghmaie, Fredrik Gustafsson, and Lennart Ljung. Linear quadratic control using
model-free reinforcement learning. IEEE Transactions on Automatic Control , 2022.
Yan Zhang, Yi Zhou, Kaiyi Ji, and Michael M Zavlanos. A new one-point residual-feedback oracle
for black-box learning and control. Automatica , 136:110006, 2022.
Yang Zheng, Luca Furieri, Maryam Kamgarpour, and Na Li. Sample complexity of linear quadratic
Gaussian (LQG) control for output feedback systems. In Proceedings of Learning for Dynamics
and Control Conference , pages 559‚Äì570. PMLR, 2021.
12