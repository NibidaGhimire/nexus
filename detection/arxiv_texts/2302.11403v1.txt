arXiv:2302.11403v1  [cond-mat.dis-nn]  6 Feb 2023Brain-like features of MemComputing machines
Massimiliano Di Ventra, Fellow, IEEE
Department of Physics
University of California, San Diego
La Jolla, CA 92093, USA
diventra@physics.ucsd.edu
Abstract —MemComputing is a new model of computation that
exploits the non-equilibrium property—we call “memory”—o f
any physical system to respond to external perturbations by
keeping track of how it has reacted at previous times. Its dig ital,
scalable version maps a ﬁnite string of symbols into a ﬁnite s tring
of symbols. In this paper, I will discuss some analogies of th e
operation of MemComputing machines—in general, and digita l
in particular—with a few physical properties of the biologi cal
brain. These analogies could be a source of inspiration to im prove
on the design of these machines. In turn, they could suggest n ew
directions of study in (computational) neuroscience.
Index Terms —MemComputing, brain, neurons, instantons
I. I NTRODUCTION
Although the complete understanding of how the biological
brain works is still far from complete, some of its features h ave
been experimentally determined. These include, for instan ce,
the main components and operation of a single neuron, the
establishment of short- and long-term memories [1], the lon g-
range correlations in the dynamics of clusters of neurons [2 ],
etc. Neuro-inspired computation, when implemented in soli d-
state hardware [3] or emulated in artiﬁcial neural networks [4],
aims at reproducing and/or exploiting some of these feature s
for some tasks such as learning, association, pattern recog ni-
tion, prediction, and so on.
Recently, a new model of computation has been introduced,
named MemComputing [5]–[7], which takes advantage of a
physical property shared by all physical systems: memory
(another name for “time non-locality”). This non-equilibrium
property means that when the state of a physical system is
perturbed, the perturbation affects the system’s state at a later
time [7], [8]. Of course, in some cases the memory decays so
fast that it is not experimentally detectable, or it is too sm all
to be technologically useful. However, there are situation s
in which time non-locality is strong enough that it can be
exploited for computing and that is the main idea behind
MemComputing [7].
In this paper I will discuss some analogies MemComputing
machines (in general, and digital in particular) share with the
operation of the biological brain. Some of these analogies
areemergent phenomena of the dynamics of these machines.
Therefore, they could shed further light on the operation of
the biological brain itself.
This work was supported by the National Science Foundation u nder Grant
ECCS-2229880.II. U NIVERSAL MEMCOMPUTING MACHINE
Let us start from the general deﬁnition of the universal
MemComputing machine (UMM) [6].1A UMM is deﬁned
as the eight-tuple [6]
UMM= (M,∆,P,S,Σ,p0,s0,F). (1)
Here,Mindicates the set of possible states of a single
memprocessor (the fundamental unit of a MemComputing
machine). The set Pcontains the arrays of pointers, pα, that
select the memprocessors called by the transition function
δα. The set of indexes is indicated with S.Σis the set of
initial states written by the input device on the computatio nal
memory. p0∈ P is the initial array of pointers, s0is the initial
indexα, andF⊆Mis the set of ﬁnal states.
The set of transition functions ,∆, has elements
δα:Mmα\F×P →Mm′
α×P2×S, (2)
with a number mα<∞of input memprocessors (read by the
transition function δα), andm′
α<∞output memprocessors
written by the same transition function.
I note ﬁrst that it was shown in [6] that the mathematical
deﬁnition of the UMM encompasses also the description of
artiﬁcial neural networks (ANNs). In other words, ANNs can
be viewed as a special case of MemComputing machines.
However, Eqs. (1) and (2) allow us to go beyond this real-
ization and show additional similarities with the operatio n of
the biological brain. These are as follows.
A. Massively parallel architecture with combined informat ion
processing and storage
Any transition function δαof a UMM simultaneously acts
on a set of memprocessors at once. This was named “intrinsic
parallelism” in [6] and it is fundamentally different from
the “standard parallelism” of our modern computers (or even
parallel Turing machines). Instead, this feature seems to b elong
to the biological brain, or at the very least, its representa tion
as an artiﬁcial neural network [9].
In addition, by construction, memprocessors and their net-
work ( computational memory ) can process and store informa-
tion simultaneously. Although it is still not fully clear ho w the
brain performs these two tasks, compelling evidence points to
the collection of neurons and synapses in the brain as the mai n
agents able to concomitantly carry out these functions [1].
1The term “universal” means that this machine is Turing-comp lete [6]. © 2023 IEEEB. Asynchronous computation
Asynchronous computation means that all or a large chunk
of processing units of a machine compute and exchange
information simultaneously, without the need to wait for a
predetermined period of time, such as the global clock perio d
in our modern computers. Asynchronous computation is a
main feature of the biological brain, and models of artiﬁcia l
neural networks. It also follows from the general deﬁnition
of MemComputing machines; cf. Eqs. (1) and (2). These
machines do not require a global clock, and the different mem -
processors compute and exchange information simultaneous ly.
C. Information overhead
From the deﬁnition of a UMM as a collection of mem-
processors it is clear that the topology of such a network
needs to be speciﬁed and represents a fundamental aspect
of these machines. This property has been called information
overhead [6] and it is a type of “data compression” which
is embedded in the machine at the outset of the computa-
tion, and does not vary during dynamics. Different types of
information overheads (topologies) can be assumed ranging
from polynomial to exponential [6]. It is indeed this proper ty
that allows a UMM with exponential information overhead to
solve NP-complete problems with polynomial resources [6],
[7]. (Note that this statement does notimply that NP=P, since
MemComputing machines are notTuring machines.)
A similar concept has been discussed in the context of the
brain [9]. In fact, the brain appears to have a high level of
speciﬁcity, in both the types of neurons and their network
architecture (the connectome), even at a mesoscopic level. This
indicates that the brain physical topology is not completel y
random. Rather, it shows some degree of specialization (in-
formation overhead) with important functional properties [1].
D. Functional polymorphism
The set∆of transition functions (2) of a UMM may contain
more than a single element. This means that a UMM can,
in principle, compute different functions without modifying
the topology of the machine network, by simply applying the
appropriate input signals. This feature was named functional
polymorphism [6], and it is not available to our modern
computers (or Turing machines). A practical realization of
this concept was reported in [10]. The biological brain does
have this property to a certain degree. In fact, the brain can
perform a wide variety of tasks without changing substantia lly
the physical topology of its network, by simply responding t o
external stimuli [9].
E. Analog vs. digital computation
MemComputing machines can be deﬁned as both analog
and digital (or mixed) according to the structure of the set
Mof possible states of a single memprocessor. If that set is
ﬁnite then the machine is digital. If it is a continuum or inﬁn ite
discrete set of states then the machine operates in the analo g
regime. Finally, if it is some direct sum of the previous twotypes of sets, then the machine operates in a mixed digital-
analog regime. Of course, only the digital MemComputing
machines (DMMs) are easily scalable. The brain, instead,
seems to operate mainly in the analog regime [1]. However,
it is interesting to note that also DMMs showcase features
similar to the elementary building blocks of the brain (the
neurons) and their collective behavior. In the next Section , I
will expand on this analogy.
III. D IGITAL MEMCOMPUTING MACHINES (DMM S)
As already mentioned, if the set Mof possible states of
a single memprocessor is ﬁnite, then the machine is digital.
The next question is then whether such a machine can be
realized in hardware. In order to answer this question, a new
set of gates, called self-organizing gates (SOGs) have been
introduced [11]. These are terminal agnostic gates able to
always satisfy their logical proposition irrespective of w hether
the incoming signals are from the traditional input or the
traditional output. The key for their realization is the cou pling
of the variables of the problem—DMMs are designed to
solve—with memory degrees of freedom.
A. Short- and long-term memories
To make this discussion more concrete, I report here the
dynamical equations representing a DMM designed to solve
for the ground state of a spin glass model Hamiltonian:
H=−/summationdisplay
i>jJijsisj, si∈ {−1,1}, (3)
where the interaction strength Jij(between the spin variables
siplaced on a the sites of a lattice) is random, and may involve
only nearest neighbor spins or any type of interaction betwe en
spins, e.g., long range.
To design a DMM that solves such a problem, the spins
are ﬁrst linearized (namely they acquire a continuous value
between−1and+1), and then they are coupled to two types
of memory degrees of freedom (short- and long-term memory)
so that the phase space of the spin plus memory dynamics has
only saddle points and equilibria representing the solutio n of
the problem [12]. The full set of equations is then:
˙si=α/summationdisplay
jJijsj−2β/summationdisplay
jxs
ijsi,
˙xs
ij=γCij−xl
ij, xs
ij∈[0,1]→short-term memory ,
˙xl
ij=δxs
ij−ζ, xl
ij∈[1,L]→long-term memory ,(4)
whereCij=1
2(Jijsisj+ 1)∈[0,1], is a clause function,
andα,β,γ,δ,ζ are time-scale parameters, ﬁxed for all system
sizes, and Lis an arbitrary but ﬁnite upper limit for the long-
term memory (see [12] for the choice of these parameters
and for a thorough explanation of how these equation have
been derived). Note that Eqs. (4) can be compactly written as
˙x(t) =F(x(t)), withxthe collection of all variables, and F
the ﬂow vector ﬁeld (the right hand side of Eqs. (4)).
The important point to make is that the short-term memory
contains information on the recent history of the systemdynamics, while the long-term memory contains information
on the entire history. The existence and coupling of these tw o
types of memories is an important ingredient to realize in
practice DMMs for the solution of combinatorial optimizati on
problems [7].
It is interesting to note that the human brain showcases also
two types of memories: long-term and short-term memory [1].
The short-term memory (which is believed to be mainly
located in the prefrontal cortex) is assumed to be a “working
memory”, namely it allows us to accomplish certain tasks tha t
may be forgotten within a relatively short time, without muc h
detriment. Instead, the long-term memory (which is located in
the hippocampus, and from there, it is supposedly distribut ed
to the cerebral cortex) is responsible for the storing of eve nts
far in the past. It is presumably created by the reinforcemen t
of short memories. This is similar to how the DMMs (e.g.,
practically realized in Eqs. (4)) operate: the long-term me mory
is “reinforced” (via physical coupling) by the short-term o ne.
B. Instantons, action potentials, critical points and node s of
Ranvier
DMMs ﬁnd the solution of a given problem by traversing
speciﬁc trajectories in phase space, known as instantons [1 4],
[15]. Instantons in DMMs are families of trajectories (a
manifold) connecting a critical point (a point, x, where the
ﬂow vector ﬁeld, F, is zero) in phase space with another more
stable critical point [7]. Instantons are sudden and relati vely
short bursts (avalanches) of the variables around the groun d
potential of the system.
Once the ﬁrst instanton is initiated, it propagates the exci ta-
tion to the next instanton, and so forth until the system reac hes
an equilibrium (if it exists). The critical points in betwee n
two successive instantons act as some sort of “regenerative ”
centers (repeaters) of the signal that propagates in the pha se
space, since at a critical point the system spends enough tim e
to “decide” on the next instanton (trajectory) to take.
The mechanism I just described is very much reminiscent
of how action potentials (electrical polarization signals ) prop-
agate in myelinated axons. Action potentials, like instant ons,
quickly “rise and fall” around the resting potential state o f the
axon membrane. However, in myelinated axons they propagate
in a saltatory fashion from a node of Ranvier to the next [1].
The nodes of Ranvier are myelin-sheath gaps along the axon
where exchange of ions between the axon membrane and the
environment can occur, so that the next action potential can
be generated and travel along the myelinated part of the axon .
This way the action potential can “jump” from one node of
Ranvier to the next, allowing for a faster conduction of the
signal. The critical points in the phase space of DMMs are the n
the equivalent of the nodes of Ranvier in myelinated axons.
C. Long-range order
An interesting physical property of the animal brain is the
observed scale-free behavior in the ﬁring of neurons, even
in the absence of external stimuli [2]. This is an emergentproperty of the collection of neurons, and it has been demon-
strated in several experiments, although there is still muc h
debate regarding its origin [2].
For instance, experiments have shown that cortical neurons ,
when deposited on a grid of electrodes, ﬁre collectively, an d
the size, S, of the neuronal avalanches (how many neurons
ﬁre together) follows a power-law distribution, S−τ, withτ
close to3/2.
This is similar to the critical Borel distribution of the siz e of
the variable avalanches (instantons) found in DMMs solving
combinatorial optimization problems [13]. In fact, by mean s
of a mean-ﬁeld theory it was shown that the distribution of th e
size of the avalanches in DMMS is also a power-law S−3/2,
which is conﬁrmed by numerical experiments. Together with
the analogy between instantons and action potentials, this
emergent property of both DMMs and the brain makes the
former an interesting test bed to explore phenomena that cou ld
have implications on the latter.
D. Robustness against noise vs. fault tolerance
Since DMMs employ objects of topological nature to com-
pute (instantons) they are robust against small perturbati ons
and noise whose strength is not enough to affect the topologi -
cal structure of the phase space. However, if the architectu re of
the network of memprocessors is changed (by, e.g., changing
even a single SOG in the circuit), the DMM would not
solve the original combinatorial optimization problem it w as
designed for. Rather, it would attempt to solve this “new”
problem. In other words, while DMMs are robust against noise
and small perturbations, they are not fault tolerant.
The brain instead seems to have both properties. In fact, it i s
known that neurons in the brain both die off and are generated
continuously [1]. It is then obvious that the architecture
(physical topology) of the network of neurons is not ﬁxed in
time. Despite these changes (provided they are not substant ial),
the brain continues to function as expected, namely it is bot h
robust against noise/perturbations (the ﬁring of single ne urons
in the brain still occurs) as well as topological changes: it is
fault tolerant to a high degree.
This fault tolerance could be due to the presence of time
non-locality (memory) in synapses and the fact that the brai n,
unlike a DMM, is not attempting to solve a speciﬁc combi-
natorial optimization problem whose Boolean (or algebraic )
expression is well deﬁned. Time non-locality is a feature th at
allows “re-routing” of information “on the ﬂy”, despite fai lure
of single units, as it was demonstrated in networks of resist ive
memories [16]. That particular network was designed for the
solution of the shortest-path problem. Memory (time non-
locality) would still promote self organization of the netw ork
into the shortest possible path or paths, in the presence of
defects in the network (created by eliminating some resisti ve
memories).
All these results seem to suggest that the more specialized
the physical architecture of a network is (as for DMMs
designed for speciﬁc combinatorial optimization problems ),
the less robust it is to topological changes. Indeed, our bra inscan tackle a wide variety of tasks, but are not particularly g ood
at solving, e.g., combinatorial optimization problems.
This fact may also be related to recent research on autism
spectrum disorder (ASD). For instance, some experimental
studies, employing neuro-imaging techniques, have shown
structural differences in several brain regions in people w ith
ASD compared with individuals without ASD [17]. These dif-
ferences may be the reason children with ASD may sometimes
show some mathematical skills outperforming non-autistic
peers, while struggling in some other tasks. It may very well
be that the brain structure of people with ASD is topological ly
more constrained than that of the general population.
IV. C ONCLUSIONS
In summary, I have brieﬂy outlined the brain-like features
of MemComputing machines. Of course, these similarities do
not imply that MemComputing machines are brain-like. They
simply indicate that some of their dynamical properties are
also observed in the operation of the biological brain.
Of particular note is that some of these properties are
emerging phenonema , namely they emerge from the collective
dynamics of the units making up these machines. For instance ,
long-range order in DMMs—arguably the most important
property for the solution of hard combinatorial optimizati on
problems—is a feature that originates from the time non-
local response of their memprocessors [7]. In fact, DMMs
enter this long-range ordered state “naturally” without tu ning
any parameter during dynamics. This realization may help
understand the supposed critical dynamics of the brain, whi ch
is still not fully understood. Work along this direction cou ld
then be beneﬁcial in the ﬁeld of (computational) neuroscien ce.
REFERENCES
[1] L.R. Squire, F.E. Bloom, N.C. Spitzer, S. du Lac, A. Ghosh , and D.
Berg, Fundamental Neuroscience: 3rd Edition . Academic Press, 2008.
[2] M.A. Mu˜ noz, “Colloquium: Criticality and dynamical sc aling in living
systems,” Reviews of Modern Physics, vol. 90, 2018, pp. 0310 01.
[3] C. Meade, Analog VLSI and Neural Systems . Addison-Wesley, 1989.
[4] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning , MIT Press,
Cambridge, MA, 2016.
[5] M. Di Ventra and Y . V . Pershin, “The parallel approach,” N ature Physics,
vol. 9, 2013, pp. 200.
[6] F.L. Traversa and M. Di Ventra “Universal MemComputing M achines,”
IEEE Transactions on Neural Networks and Learning Systems, vol. 26,
2015, pp. 2702.
[7] M. Di Ventra, MemComputing: Fundamentals and Applications , Oxford
University Press, Oxford, UK, 2022.
[8] M. Di Ventra and Y .V . Pershin, Memristors and Memelements: Mathe-
matics, Physics, and Fiction , Springer, 2023.
[9] T. Kohonen, T. Self-organization and Associative Memory (3rd edn) ,
Springer-Verlag, New York (1989).
[10] F.L. Traversa, F. Bonani, Y .V . Pershin, and M. Di Ventra , “Dynamic com-
puting random access memory,” Nanotechnology, 25,2014, pp . 285201.
[11] F. L. Traversa and M. Di Ventra, “Polynomial-time solut ion of prime
factorization and NP-complete problems with digital MemCo mputing
machines,” Chaos: An Interdisciplinary Journal of Nonline ar Science,
vol. 27, 2017, pp. 023107.
[12] Y .R. Pei and M. Di Ventra “Non-equilibrium criticality and efﬁcient
exploration of glassy landscapes with memory dynamics,” Ph ysica A:
Statistical Mechanics and its Applications vol. 591, 2022, pp. 126727.
[13] S. R. B. Bearden, F. Sheldon, and M. Di Ventra, “Critical branching
processes in digital MemComputing machines,” Europhysics Letters,
vol. 127, 2019, pp. 30005.[14] M. Di Ventra, F. L. Traversa, and I. V . Ovchinnikov, “Top ological ﬁeld
theory and computing with instantons,” Annalen der Physik ( Berlin),
vol. 529, 2017, pp. 1700123.
[15] M. Di Ventra and I. V . Ovchinnikov, “Digital MemComputi ng: from
logic to dynamics to topology,” Annals of Physics, vol. 409, 2019,
167935.
[16] Y .V . Pershin and M. Di Ventra, M. “Self-organization an d solution of
shortest-path optimization problems with memristive netw orks,” Physi-
cal Review E, vol. 88, 2013, pp. 013305.
[17] T. Iuculano, M. Rosenberg-Lee, K. Supekar, C.J. Lynch, A. Khouzam,
J. Phillips, L.Q. Uddin, V . and Menon, “Brain organization u nderlying
superior mathematical abilities in children with autism,” Biological
Phsychiatry, vol. 75, 2014, pp. 223.