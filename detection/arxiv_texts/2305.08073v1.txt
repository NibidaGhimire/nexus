HIPERFORMER : HIERARCHICALLY
PERMUTATION -EQUIVARIANT TRANSFORMER FOR TIME
SERIES FORECASTING
A P REPRINT
Ryo Umagami
The University of Tokyo
umagami@mi.t.u-tokyo.ac.jpYu Ono
The University of Tokyo
ono@mi.t.u-tokyo.ac.jpYusuke Mukuta
The University of Tokyo / RIKEN AIP
mukuta@mi.t.u-tokyo.ac.jp
Tatsuya Harada
The University of Tokyo / RIKEN AIP
harada@mi.t.u-tokyo.ac.jp
ABSTRACT
It is imperative to discern the relationships between multiple time series for accurate forecasting. In
particular, for stock prices, components are often divided into groups with the same characteristics,
and a model that extracts relationships consistent with this group structure should be effective. Thus,
we propose the concept of hierarchical permutation-equivariance, focusing on index swapping of
components within and among groups, to design a model that considers this group structure. When
the prediction model has hierarchical permutation-equivariance, the prediction is consistent with
the group relationships of the components. Therefore, we propose a hierarchically permutation-
equivariant model that considers both the relationship among components in the same group and
the relationship among groups. The experiments conducted on real-world data demonstrate that the
proposed method outperforms existing state-of-the-art methods.
Keywords Permutation-Equivariance Time Series Forecasting Multi-Agent Trajectory Prediction Hierarchical
Forecasting
1 Introduction
Time series forecasting facilitates decision-making in several facets of life. If the number of units of a product sold can
be predicted, proÔ¨Åts can be maximized by placing orders without excess or shortage. In practice, there are numerous
time series, some of which are strongly inÔ¨Çuenced by each other. Previous studies have demonstrated strong correlations
and dependencies among stocks belonging to the same sector in the U.S. stock market [Sukcharoen and Leatham, 2016].
If time series are inÔ¨Çuencing each other, it is reasonable to input the set of series into the model simultaneously and
make forecasts considering their relationships. Then the model must be equipped to accommodate the entry and exit of
the series. The entry and exit of a series correspond to the launch and discontinuation of new products in retail and
initial public offerings and bankruptcies in stock prices. However, a model that acquires dependencies among speciÔ¨Åc
series based on the input order of the series cannot solve this problem. This is because the model lacks a mechanism for
acquiring the relationship between the new index and the existing series. But in fact, most existing methods that leverage
the relationships among series cannot handle the entry and exit of series, because they Ô¨Åx the number of series and the
input order, and use that order to train a model. Permutation-equivariant models are used to address this issue [Zaheer
et al., 2017]. A permutation-equivariant mechanism takes a set of series as input and outputs a corresponding series,
that is rearranged according to the permutation of the input. Details are given in 2.3. In summary, when predicting a
set of time series, we must have a permutation-equivariant model that can acquire relationships among the series and
handle variable-size series sets.
Real-world time series groups are often divided into classes that group together elements with the same characteristics,
such as teams in sports or industries in businesses. It is expected that this class structure could be effective in timearXiv:2305.08073v1  [cs.LG]  14 May 2023HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
series forecasting by learning the dependencies among series. We propose the concept of hierarchical permutation-
equivariance as a guideline for building prediction models considering such class relationships. This concept focuses on
the permutation of series considering the class relationships. Furthermore, we propose a hierarchically permutation-
equivariant transformer (HiPerformer) that models intra-class, inter-class and time effects with self-attention and is
hierarchically permutation-equivariant. HiPerformer outperforms existing models in experiments using various real
time series.
Our contributions are as follows:
‚Ä¢ We deÔ¨Åne hierarchical permutation and hierarchical permutation-equivariance.
‚Ä¢ We proposed two types of models that can accommodate the entry and exit of a series.
1.HiPerformer: A model which is hierarchically permutation-equivariant and can capture the hierarchical
dependencies among series. This model requires class information for each series.
2.HiPerformer-w/o-class: A model which is permutation-equivariant and capture the dependencies among
series. This model does not require class information for each series.
‚Ä¢We proposed a distribution estimator that outputs joint distributions with time-varying covariance to handle
uncertain real time series.
‚Ä¢The proposed model outperforms state-of-the-art methods in experiments on artiÔ¨Åcial and real datasets where
the series have a hierarchical dependency structure. This shows that utilizing the hierarchical dependency
structure is effective for forecasting time series that are classiÔ¨Åed.
2 Related Works
2.1 Time Series Forecasting
Conventional linear time series prediction models, such as the autoregressive model (AR) are slightly expressive and
require manual input of domain knowledge such as trends and seasonality. To address these issues, various deep
neural network models have been proposed, such as recurrent neural network (RNN) [Cho et al., 2014]. RNNs, though
theoretically can consider all prior inputs, suffer from the challenges of gradient vanishing and explosion, impeding their
use for long-term information reÔ¨Çection. Long short-term memory (LSTM) and gated recurrent units (GRUs) alleviate
these problems but still cannot retrieve information in the distant past [Khandelwal et al., 2018]. Furthermore, since
past inference results are used for prediction during inference, inference errors can accumulate in long-term predictions.
The recently proposed Transformer [Vaswani et al., 2017] allows access to past information regardless of distance
through the attention mechanism, allowing for longer-term dependencies to be obtained. Numerous improvements to
Transformer have been published for computational savings and seasonality considerations [Li et al., 2019, Zhou et al.,
2021, Wu et al., 2021, Zhou et al., 2022, Liu et al., 2021].
2.2 Capturing Relationships Among Series
The vector autoregressive model (V AR) is the traditional model for examining series relationships, and DeepV AR is
an extension of this model to incorporate deep learning with RNNs. DeepV AR is a probabilistic forecasting method
that can determine the parameters of the distribution that target multivariate time series follow. AST [Wu et al., 2020]
combining Transformers and generative adversarial networks (GANs) [Goodfellow et al., 2020] can output predictions
for each series that are viable for simultaneous achievement. Another approach is using GNNs [Scarselli et al., 2008],
mapping series to nodes and relationships between series to edges. AGCRN [Bai et al., 2020] is a method that combines
RNNs and graph convolutional networks [Defferrard et al., 2016, Kipf and Welling, 2016].
Multi-agent trajectory prediction is predicting the movements of multiple agents that inÔ¨Çuence each other, and it is
effective to consider the relationships among the agents. Multi-agent trajectory prediction has various applications, such
as automated driving [Zhao et al., 2021, Salzmann et al., 2020, Mangalam et al., 2020, Leon and Gavrilescu, 2021],
robot planning [Kretzschmar et al., 2014, Schmerling et al., 2018], sports analysis [Felsen et al., 2017], and is being
actively researched. GRIN [Li et al., 2021] has been successful in obtaining advanced interactions between agents using
GATs [Velickovic et al., 2017].
Hierarchical forecasting focuses on hierarchical relationships among series. In this task, a series in the upper hierarchy
is an aggregate of series in the lower hierarchy, and forecasts are made for the series in the bottom hierarchy and the
series in the upper aggregation hierarchy. In this setting, data on the number of students in a school is the aggregate of
the student totals in all grades, and the student count in each grade is the sum of the student numbers in all classes.
HierE2E [Rangapuram et al., 2021] and DPMN [Olivares et al., 2021] indicate that hierarchical consistency regarding
sums may lead to higher accuracy than forecasting each series separately.
2HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Despite their successes, none of the previous methods consider hierarchical dependencies among series, leading to an
information loss.
2.3 Permutation-Equivariance / Invariance
Permutation-equivariance and permutation-invariance were deÔ¨Åned in [Zaheer et al., 2017]. A permutation-equivariant
mechanism fis; for a tensor X= [x1;:xN]Tand any substitution matrix P2RNN,f(PX) =Pf(X)holds.
On the other hand, a permutation-invariant mechanism fis one for which f(PX) =f(X)holds.
In [Lee et al., 2019], set attention block (SAB) and induced set attention block (ISAB) were proposed which is
permutation-equivariant and take self-attention among a set of series. Self-attention can acquire relationships among
series and is used in Transformer introduced also in 2.1. The computational complexity of SAB is on a square order
to the number of series, while ISAB exhibits comparable performance with a linear complexity. Also, pooling by
multi-head attention (PMA) was proposed which is permutation-invariant and compress a series set in the series
dimension using multi-head attention. Notably, SAB, ISAB, and PMA accept an arbitrary size set of series as input,
are permutation-equivariant or permutation-invariant for series dimension, and can extract features considering the
relationships among series. These properties align with the objectives of this study and form the basis of the proposed
model.
3 Methods
3.1 Problem DeÔ¨Ånition
Suppose that we have a collection of Srelated multivariate time series fXi;T Tin+1:TgS
i=1, whereXi;T Tin+1:T2
RTinDindenotesDinexplanatory variables of time series iduring time T Tin+ 1 :T. Furthermore, we
have class information of each series c2NS, when ci=cj, it means time series iand time series jbelong to
the same class. We will predict the time-varying probability distribution that fYi;T+1:T+ToutgS
i=1follows, where
Yi;T+1:T+Tout2RToutDoutdenotesDoutobjective variables of time series iduring time T+ 1 :T+Tout. Series
inÔ¨Çuence each other hierarchically, and the acquisition of hierarchical dependencies is effective in forecasting.
3.2 Hierarchical Permutation-Equivariance
‚ÄúHierarchical permutation-equivariance‚Äù is a paradigm extension of permutation-equivariance described in 2.3. Pre-
liminarily, we introduce ‚Äúhierarchical permutation‚Äù. A hierarchical permutation is one that satisÔ¨Åes (1) or (2): (1)
a permutation of the series in the same class, and (2) a permutation of the class itself. SpeciÔ¨Åcally, hierarchical
permutations are permutations except for permutations among series in different classes. For example, consider the
case where series sets fA1;A2;B1;B2;B3g, divided into classes A and B, are given in this order. Here, the permutation
in (1) is one likefA2;A1;B3;B1;B2gand the permutation in (2) is like fB1;B2;B3;A1;A2g. Combining (1) and (2),
fB3;B1;B2;A2;A1gis also a hierarchical permutation. 1 visualizes what is a hierarchical-permutation and what is not.
We then deÔ¨Åne ‚Äúhierarchically permutation-equivariant‚Äù as being equivariant for hierarchical permutation of series.
Predictions of a hierarchically permutation-equivariant model are: independent of the input order of classes and series
in the same class, and consistent with the class structure. Therefore, we propose a prediction model that is hierarchically
permutation-equivariant and can capture the hierarchical dependencies among series.
3.3 Model Overview
We can obtain the distribution that Yfollows by passing the input tensor Xthrough the Feature Extractor and
Distribution Estimator proposed in this study. These proposed modules have the following properties.
‚Ä¢ Capture hierarchical dependencies among series.
‚Ä¢Hierarchically permutation-equivariant and capable of handling the entry and exit of series accepting series set
of arbitrary size.
‚Ä¢ Outputs time-varying and diverse joint distributions.
3.4 Feature Extractor
The feature extractor is a stack of 3D self-attention layers shown in 2. Before passing to it, we classify the input tensor
Xusing the class information c. IfSseries are divided into Cclasses and the number of series in the largest class is
3HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
can swap
cannot swap can swapclass
e.g. sector
series
e.g. company
Figure 1: Hierarchical permutation
Algorithm 1 3D self-attention
1:Input:M2RCScTinDmodel
2:
3:/* Self-Attention Block for ScDimension
CurriculateMSc2RCScTinDmodel */
4:forc= 1toCdo
5: fort= 1toTindo
6:MSc
c;:;t;: SelfAttForSc (Mc;:;t;:)
7: end for
8:end for
9:
10:/* Self-Attention Block for CDimension
CurriculateMC2RCScTinDmodel */
11:forc= 1toCdo
12:MP
c;:;:;: Pool(Mc;:;:;:)2RTinDmodel
13:end for
14:fort= 1toTindo
15:MP
:;t;: SelfAttForC (MP
:;t;:)
16:end for
17:fors= 1toScdo
18:MC
:;s;:;: MP
19:end for
20:
21:/* Self-Attention Block for TDimension
CurriculateMT2RCScTinDmodel */
22:MT MSc+MC
23:forc= 1toCdo
24: fors= 1toScdo
25:Mc;s; :;: SelfAttForT (Mc;s; :;:)
26: end for
27:end for
28:
29:Output:MT2RCScTinDmodel
Sc,Xcan be formatted into a tensor of RCScTinDin. Here,Xi;jrepresents the observation of the series jin class
i. Note thatXi;j, wherejexceeds the number of series belonging to the class i, will not be referenced because it is
information on a series that does not exist. After this, cis never used and never input to the model as a feature.
Then,Xis embedded into the in-model tensor M2RCScTinDmodel , and then it is input into the 3D self-attention
layer.
3.4.1 3D Self-Attention Layer
Algorithm. A 3D self-attention layer consists of blocks that take self-attention for intra-class ( Sc), inter-class ( C), and
time (T) dimensions. The blocks for ScandCare parallel, followed by the Tblock. The algorithm is shown in 1,
4HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Positional
Encoding
Input
EmbeddingMasked
Multi-Head
AttentionFeed
Forward
Add & NormRepeatAdd & Norm
Add & Norm
Feed
Forward
InputsN√ó
Masked
Multi-Head
AttentionAdd & NormAdd & Norm
Multi-Head
AttentionAdd & NormFeed
ForwardAdd & Norm Feed
ForwardFeed
Forward
PoolingPoolingOutput
Features
Self-Attention block
for  dimensionSelf-Attention block
for  dimensionSelf-Attention block
for  dimension
Figure 2: Feature extractor architecture
and line 3-8, 10-19, 21-27 corresponds Sc,C, andTdimension block respectively. SelfAttForSc, SelfAttForC, and
SelfAttForT are the self-attention functions for Sc,C, andTdimensions respectively. When taking self-attention for
Scdimension, we Ô¨Åx the class index and time, and then take self-attention among series in the class. When taking
self-attention for Cdimension, Ô¨Årst we compress class information (line 8-10), and then take self-attention among
classes with Ô¨Åxing time (line 12-14). Finally, we repeat the self-attention output for Scdimension (line 15-17). When
taking self-attention for Tdimension, we Ô¨Åx the class and series index, and then take self-attention among series in the
class.
Properties. We can obtain hierarchical dependencies among the series by combining self-attention for ScandC
dimensions, and Ô¨Ånally, by taking self-attention for T, we can capture complex time evolution considering the entire
input. Furthermore, the entire 3D self-attention layer is hierarchically permutation-equivariant and accepts any size set
of series as input. An explanation for this is provided below.
5HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Self-Attention Block forScDimension. This block is permutation-equivariant for Cdimension and accepts any number
of classes as input since the same process is applied to all the classes. As Scdimension is only subjected to self-attention,
it is permutation-equivariant and can accept any number of series from the same class as input.
Self-Attention Block forCDimension. This block is permutation-equivariant for Scand accepts any number of series
in a class as input since it only performs permutation-invariant compression and repeats for the Scdimension. Also, for
theCdimension, since it only considers self-attention, it is permutation-equivariant and accepts any number of classes
as input.
Self-Attention Block forTDimension. This block is hierarchically permutation-equivariant and accepts any number
of classes and series in a class as input because the same process is applied to all classes and series in a class for this
calculation.
3.4.2 Post Process and Summary
The shape of tensor Mdoes not change in the 3D self-attention layer. After fusing CandScdimensions of M, we
obtain the time-varying features for each series Z= (Z1;:::;ZS)2RSToutDmodel by removing the padded portions
and pooling for Tdimension.Zis a feature that captures both the heterarchical dependencies among series and the
temporal evolution of each series through 3D self-attention. Note that by removing the self-attention block for C
dimension from the 3D self-attention layer and considering all series in the same class, it becomes another proposed
model HiPerformer-w/o-class that does not use class information. This model has a mere permutation-equivariance (not
hierarchical) and can acquire relationships among series, and has the same properties as the full model.
3.5 Distribution Estimator
A distribution estimator is a mechanism that takes a feature Zobtained with 3.4 as input and predicts a time-varying
probability distribution that the target variable Yfollows. In particular, this study assumes that Y:;t;d follows a
multidimensional normal distribution N(:;t;d;:;:;t;d), and output its mean 2RSToutDoutand covariance
matrix 2RSSToutDout. The algorithm for calculating ;is described below.
3.5.1 Calculation of 
is calculated with a linear layer of f:RDmodel!RDout, as in the following formula.
= (f(Z1);:::;f(ZS))T2RSToutDout(1)
3.5.2 Calculation of 
Since the covariance matrix is a positive (semi)deÔ¨Ånite, the output :;:;t;d here must also be positive deÔ¨Ånite. We have
Doutlinear layers each of fr:RDmodel!RDr; fl:RDmodel!RDl; f:RDmodel!RD. Let thed-th ones be
frd,fld, andfdrespectively, then i;j;t;d is calculated as in the following equation.
i;j;t;d = (ri;j;t;d +li;j;t;d )i;j;t;d
whereri;j;t;d =kRBF(frd(Zi;t; :);frd(Zj;t; :))
li;j;t;d =kLIN(fld(Zi;t; :);fld(Zj;t; :))
i;j;t;d =kLIN(fd(Zi;t; :);fd(Zj;t; :))(2)
Here,kRBF(x;y) = exp(kx yk2); kLIN(x;y) =xTy, representing the RBF and linear kernels respectively.
:;:;t;d is positive deÔ¨Ånite because it is a Gram matrix of kernels combining positive deÔ¨Ånite kernels by sum and
product. The RBF kernel can extract nonlinear relationships, but its value range is limited to (0;1]. Hence, it is
combined with a linear kernel to improve its expressive power.
3.6 Training Strategy
We optimize the model to maximize the likelihoodP
tP
dN(Y:;t;dj:;t;d;:;:;t;d); that is, we can minimize the
following negative log likelihood.
L=X
tX
d logN(Y:;t;dj:;t;d;:;:;t;d) (3)
6HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Note that the negative log-likelihood of a multivariate normal distribution,  logN(ujv;W)is calculated as follows.
 logN(ujv;W) (4)
=1
2
Dlog(2) + logjWj+ (u v)TW 1(u v)
(5)
4 Experiments
In this section, we validate the performance of the proposed method on two different tasks described in 2.2, multi-agent
trajectory prediction, and hierarchical forecasting. For each task, Ô¨Årst, we introduce datasets, evaluation metrics, and
methods. Thereafter, we conduct evaluations.
4.1 Multi-Agent Trajectory Prediction
4.1.1 Datasets
We use one artiÔ¨Åcial dataset and one real-world dataset, where the series interact with each other.
Charged Dataset [Kipf et al., 2018] is an artiÔ¨Åcial dataset that records the motion of charged particles controlled by
simple physical laws. Each scene contains Ô¨Åve particles, each with a positive or negative charge. Particles with the
same charge repel each other, and particles with different charges attract each other. The task is to predict the trajectory
for the next 20 periods based on the trajectory and velocity for 80 periods. Subsequently, 50K, 10K, and 10K scenes
of data are used for training, validation, and testing, respectively. In the proposed model, each particle is classiÔ¨Åed
according to whether its charge is positive or negative.
NBA Dataset [Yue et al., 2014] is a dataset of National Basketball Association (NBA) games for the 2012‚Äì2013 season.
Each scene has 10 players and 1 ball and the task is to predict the trajectory of the following 10 periods based on the
historical trajectory and velocity for 40 periods. Here, 80000, 48299, and 13464 scenes of data are used for training,
validation, and testing, respectively. The proposed model categorizes each series into three classes: ball, and the team to
which it belongs.
4.1.2 Evaluation Metrics
To evaluate the accuracy of probabilistic forecasts, we evaluate the likelihood of the forecast distribution and the results
of point forecasts.
Average displacement error (ADE) and Ô¨Ånal displacement error (FDE) : ADE is the root mean squared error
between the ground truth and predicted trajectories. FDE measures the root mean squared error between the ground
truth Ô¨Ånal destination and the predicted Ô¨Ånal destination. For stochastic models, we report the minimum (and mean for
one method) displacement error of all predicted trajectories/destinations. The lower values are preferred.
Negative log likelihood (NLL) : It measures how plausible the ground truth is for the predicted distribution. For
stochastic models, we report the mean NLL of all trajectories. We only report this metric on models that are trained
with NLL or evidence lower bound (ELBO) [Sohn et al., 2015]. The lower values are preferred.
4.1.3 Methods
We adopt the following four methods „ÄÄas baselines that utilize interactions among agents.
Fuzzy query attention (FQA) [Kamra et al., 2020] is a method that combines RNN with a module that acquires
induced bias from the relative movements, intentions, and interactions of sequences.
Neural relational inference (NRI) [Kipf et al., 2018] represents interactions among agents as a potential graph from
past trajectories and generates future trajectories as variational auto-encoders (V AEs) [Kingma and Welling, 2013].
Social-GAN [Gupta et al., 2018] is a traditional GAN-based method that effectively handles multimodality.
GRIN [Li et al., 2021] extracts latent representations that separate the intentions of each agent and the relationships
between agents and generates trajectories considering high-level interactions between agents utilizing GATs [Velickovic
et al., 2017]. To the best of our knowledge, this is the state-of-the-art method in this dataset.
Next we explain about our proposed model HiPerformer.
HiPerformer. We compare aforesaid baselines with HiPerformer-prob, the full proposed model; HiPerformer-det, which
is HiPerformer-prob without the covariance estimation described in 3.5.2 and only predicts trajectories; HiPerformer-
7HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Table 1: Comparison of baselines and the proposed method. The 4is added because DeepV AR+ and HierE2E estimate
hierarchical dependency structures only with respect to sums, and HiPerformer is hierarchically permutation-equivariant,
although full permutation-equivariance is not guaranteed.
Method FQA NRI Social-GAN DeepV AR DeepV AR+ HierE2EHiPerformer
-w/o-class (Ours)HiPerformer
(Ours)
Probabilistic X X X X X X X
Inter-Series Hierarchical Dependencies     4 4  X
Inter-Series Dependencies X X X X X X X X
Entry and Exit of Series   X   X X
Hierarchically-Permutation-Equivariant        X
Permutation-Equivariant   X   X4
No Class Information Needed X X X X   X
Table 2: Performance comparison in multi-agent trajectory prediction. Non-generative methods have yafter their
names.„ÄÄFor GRIN and the proposed methods, we report the mean and standard deviation over Ô¨Åve runs. The best
results are in bold .
Dataset Charged NBA
Model ADE FDE NLL ADE FDE NLL
FQAy 0.82 1.76 - 2.42 4.81 -
NRI-min (0.63) (1.30) - (2.10) (4.56) 586.9
Social-GAN-min (0.66) (1.25) - (1.88) (3.64) -
GRIN-min (0.530.01) (1.100.02)237.8630.51(1.730.03) (3.670.08)509.273.23GRIN-mean 0.690.06 1.450.12 1.980.03 4.260.08
HiPerformer-w/o-class
-det (Ours)y0.510.01 1.140.02 - 1.650.00 3.640.00 -
HiPerformer-det (Ours) y 0.580.00 1.220.02 - 1.640.01 3.600.02 -
HiPerformer-w/o-class
-prob (Ours)y0.440.00 1.050.00 -11.701.57 1.640.01 3.690.02 20.960.20
HiPerformer-prob (Ours) y 0.520.01 1.180.01 2.93.48 1.650.00 3.680.01 21.140.26
AttT-dety 0.630.01 1.370.02 - 1.830.00 4.060.00 -
AttT-proby 0.610.01 1.370.02 13.851.03 1.920.00 4.250.00 26.580.14
w/o-class-prob, which is HiPerformer-prob without the self-attention block for Cdimension in 3.4.1; and HiPerformer-
w/o-class-det, which is HiPerformer-w/o-class-prob without the covariance estimation. While HiPerformer-prob and
HiPerformer-w/o-class-prob are trained with NLL, HiPerformer-det, and HiPerformer-w/o-class-det are trained with
distance errors, such as mean absolute error (MAE) and mean square error (MSE). In this task, we use MAE. Also note
that we never use class information for HiPerformer-w/o-class-prob and HiPerformer-w/o-class-det, as described in
3.4.2. We use SAB described in 2.3 when taking self-attention for Sc;Cdimension.
A comparison of baselines with the proposed method is shown in 1.
Implementation Details. We implement our model using PyTorch [Paszke et al., 2019]. We stack two 3D self-attention
layer,Dmodel is 64, the number of heads for multi-head attention is 4, the batch size is 8 for Charged dataset and 64 for
NBA dataset. We train our model using Adam optimizer [Kingma and Ba, 2014] with learning rate 110 3. The
results of baseline methods are from [Li et al., 2021] except for GRIN. We train GRIN with parameters used in [Li
et al., 2021].
4.1.4 Quantitative Evaluation
Baseline Comparison. Results for the baseline and proposed methods are shown in 2. The methods with ‚Äúmin‚Äù at the
end of the model name are those that generate predictions probabilistically, and we report the best ADE and FDE among
them. For NLL, the average is reported. We generate 100 predictions for both datasets. While this metric is common for
evaluating generative methods [Gupta et al., 2018, Mohamed et al., 2020, Li et al., 2021], we cannot simply compare it
with the results of nongenerative methods such as FQA and proposed methods; it would be better to compare using
mean or median instead of the minimum value, and we bracket the min-model metrics in 2. Therefore, for GRIN, the
best-performing existing method, we report the average of the metrics for the generated predictions as ‚ÄúGRIN-mean.‚Äù
In the Charged dataset, HiPerformer-w/o-class-prob has the best performance for all metrics and all the proposed
methods outperform the GRIN-mean results. Especially for NLL, HiPerformer-w/o-class-prob signiÔ¨Åcantly exceeds
baselines. In this experiment, HiPerformer-w/o-class, which does not use class information, outperformed HiPerformer,
which attempts to acquire hierarchical dependencies among series using the class structure. This could be because the
8HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Table 3: Comparison of HiPerformer-w/o-class-prob and HiPerformer-prob performance in NBA dataset when tested
with 0-4 players removed from team A and B, respectively. In order from the table above, ADE, FDE, and NLL are
represented. Training was performed on all players‚Äô data, and players were randomly removed during the test phase. The
left side of each square is the result of HiPerformer-w/o-class-prob and the right side is the result of HiPerformer-prob.
We report the mean and standard deviation over Ô¨Åve runs. The better results are in bold .
AADE B
0 1 2 3 4
0 (1.640.01 - 1.650.00) 1.690.01 - 1.690.01 1.730.01 - 1.730.01 1.780.01 - 1.780.01 1.850.01 - 1.840.01
1 1.670.01 - 1.680.01 1.710.01 - 1.720.01 1.760.01 - 1.770.01 1.830.01 - 1.830.01 1.920.01 - 1.910.01
2 1.690.01 - 1.710.01 1.740.01 - 1.760.01 1.810.01 - 1.820.01 1.890.01 - 1.890.01 2.010.01 - 2.010.01
3 1.730.01 - 1.740.01 1.790.01 - 1.800.01 1.870.01 - 1.880.01 1.980.01 - 1.990.01 2.150.01 - 2.160.01
4 1.760.01 - 1.790.01 1.840.01 - 1.870.01 1.950.01 - 1.970.01 2.110.01 - 2.130.01 2.380.01 - 2.400.01
AFDE B
0 1 2 3 4
0 (3.690.02 - 3.680.01) 3.780.02 - 3.760.01 3.870.02 - 3.840.01 3.980.02 - 3.940.01 4.130.02 - 4.070.01
1 3.730.02 - 3.720.01 3.820.02 - 3.800.01 3.930.02 - 3.900.01 4.060.02 - 4.020.01 4.250.02 - 4.190.01
2 3.760.02 - 3.760.01 3.860.02 - 3.850.01 3.990.02 - 3.970.01 4.160.02 - 4.130.01 4.410.02 - 4.350.01
3 3.810.02 - 3.810.01 3.930.02 - 3.930.01 4.090.02 - 4.080.01 4.310.02 - 4.290.01 4.650.02 - 4.610.01
4 3.860.02 - 3.870.01 4.010.02 - 4.020.01 4.220.02 - 4.220.02 4.530.02 - 4.520.02 5.060.02 - 5.020.02
ANLL B
0 1 2 3 4
0 (20.960.20 - 21.140.26) 21.430.29 - 21.400.23 21.890.32 - 21.790.24 22.450.35 - 22.260.24 22.970.39 - 22.670.24
1 21.390.30 - 21.420.24 21.740.31 - 21.690.23 22.290.35 - 22.160.24 22.990.40 - 22.750.25 23.690.45 - 23.310.25
2 21.680.33 - 21.680.22 22.100.34 - 22.010.22 22.780.39 - 22.600.23 23.680.45 - 23.360.24 24.650.52 - 24.150.23
3 22.040.37 - 22.010.22 22.570.39 - 22.440.21 23.450.45 - 23.190.22 24.660.54 - 24.220.23 26.120.65 - 25.440.22
4 22.510.41 - 22.430.20 23.210.44 - 23.000.19 24.390.53 - 24.020.19 26.130.65 - 25.510.21 28.570.84 - 27.560.17
number of particles is small (5) and the relationships are simpler than those in the real-world dataset. Thus, HiPerformer-
w/o-class could obtain richer information from other class particles using the raw data, rather than compressing each
class information as in the 8-9 lines of 1.
For NBA, the proposed method also outperformed baselines in all indices. HiPerformer-det was best for ADE and FDE.
HiPerformer-w/o-class-prob and HiPerformer-prob performed similarly, both signiÔ¨Åcantly outperforming baselines for
NLL.
Ablation Study. We conduct experiments to conÔ¨Årm the effectiveness of self-attention for Sc;Cdimension to acquire
inter-series dependencies. The results for AttT, which removes the self-attention block for Sc;Cdimension from the
full proposed model and takes self-attention for Tdimension only, are shown at the bottom of the 2. AttT-prob estimates
the mean and covariance, while AttT-det predicts only the mean. Consistently, HiPerformer and HiPerformer-w/o-class
outperforms AttT, indicating that self-attention for Sc;Cdimension is effective in obtaining dependencies among series.
Remove Series. We investigated the performance of the proposed method on changes in the number of series using the
NBA dataset. We created a new test dataset set by randomly removing some players from the original test dataset, and
evaluated the trained models obtained in Baseline Comparison . A comparison of HiPerformer-w/o-class-prob and
HiPerformer-prob is shown in the 3. For ADE, the difference between the two is small. On the other hand, for FDE and
NLL, HiPerformer-prob is more robust against removing series, and tends to perform better than HiPerformer-w/o-
class-prob as more players are removed. In particular, the difference between the two is large for NLL. From this, we
can say that when the class imbalance is large or the number of series is small, the performance degradation can be
mitigated by using the relationship among classes.
4.1.5 Qualitative Evaluation
The trajectory predictions in the NBA dataset are shown in 3. Looking at the attention scores among classes, we can see
that a wide variety of information can be retrieved, with the top row showing more attention to Team B and the bottom
row showing more attention to Team A. Next, take a look at the attention score of player A5 in the upper row. Here, A5
is the player with the ball, and since the enemy B4 is approaching, he is probably trying to pass the ball to a player
on his side. Now, A5 is paying attention to A1, A2, and A4, and this may be because there are few enemy players
around A1 and A2, and A4 is close to the goal. The proposed method is able to capture the complex relationships
among real-world time series as described above.
9HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Figure 3: Trajectory prediction on NBA dataset. The left two Ô¨Ågures are ground truth and predicted trajectories.
Green represents a ball, red represents a player from Team A, and blue represents a player from Team B. The light line
represents the 40-period trajectory of the input and the dark line represents the 10-period trajectory of the ground truth
or prediction. The third Ô¨Ågure from the left represents the atttention score among the classes, and the fourth and Ô¨Åfth
Ô¨Ågures represent the atttention score among the players of each team. Higher attention scores are in darker color. The
upper and lower rows show the attention scores of different heads on the same input data.
Table 4: RMSE and NLL at each aggregation level. We report the mean and standard deviation for proposed method
and best of baselines over 5 runs. The best results are in bold .
Metrics RMSE NLL
Dataset LevelDeepV AR
(not coherent)DeepV AR+ HierE2EHiPerformer
(Ours)DeepV AR
(not coherent)DeepV AR+ HierE2EHiPerformer
(Ours)
Labour1 561.759.2 629.0117.0 656.3151.5 4.582.05 13.111.19 12.143.12 14.643.54 4.330.41
2 104.513.5 112.915.3 112.518.3 2.640.89 7.440.95 7.791.79 7.851.23 3.880.55
3 55.86.5 57.07.7 57.48.9 1.920.63 6.490.81 6.251.19 6.520.89 3.580.37
4 31.43.5 31.93.4 32.624.55 1.570.45 4.790.36 4.810.56 5.140.49 4.280.32
TrafÔ¨Åc1 41.908.09 22.408.44 22.5915.71 3.102.26 5.190.15 4.650.18 4.770.36 3.960.84
2 24.798.16 14.661.98 15.135.70 2.211.59 4.650.31 4.200.08 4.190.25 3.500.67
3 17.353.98 10.703.89 9.822.29 1.561.12 4.280.41 3.920.22 3.710.18 3.430.77
4 1.460.08 1.440.05 1.470.03 0.290.12 1.650.14 1.570.07 1.440.12 3.991.35
Wiki1 110886717 117217688 41252483 204.73175.54 11.050.44 11.500.36 9.880.35 8.841.48
2 3107941 41881218 3855340 158.6891.94 9.570.20 10.030.24 12.661.34 8.130.45
3 277360 3082335 2851187 142.5836.72 9.690.17 9.280.12 12.850.92 8.120.43
4 2737178 2943264 2769151 145.9037.89 9.450.46 9.210.12 15.561.68 8.110.43
5 93122 97865 93226 96.6022.26 8.020.20 8.170.43 9.610.84 8.260.24
4.2 Hierarchical Forecasting
The data used in 4.1 have three levels from bottom to top, namely, bottom, class, and root levels, and we make
predictions and evaluations only for the bottom-level or agent. In contrast, at this time, we will conduct experiments on
data with four or more levels, where the feature values of the upper-level series are the sum of those of the lower-level
series. Furthermore, we perform prediction and evaluation for the bottom-level series and the aggregate series at each
level.
4.2.1 Datasets
For all datasets, given data for Tperiods, to make predictions for periods, we use 1 :T 2data in training, perform
validation at T 2+ 1 :T , and test at T + 1 :T. This is a setting following existing studies using this
dataset [Rangapuram et al., 2021, Olivares et al., 2021], and we also adopt the same forecasting period .
Labour [Australian Bureau of Statistics, 2021] is monthly Australian employment data from Feb. 1978 to Dec. 2020.
We construct a 57-series hierarchy using categories such as region, gender, and employment status, as in [Rangapuram
10HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
et al., 2021]. In this data, Tis 514 andis 8. It has four levels, and we classify the bottom-level series at the second
level from the top in the proposed method.
TrafÔ¨Åc [Cuturi, 2011, Dua and Graff, 2017] records the occupancy of 963 car lanes in the San Francisco Bay Area
freeways from January 2008 to March 2009. We obtain daily observations for one year and generate a 207-series
hierarchy using the same aggregation method as in previous hierarchical forecasting research [Ben Taieb and Koo, 2019,
Rangapuram et al., 2021]. In these data, Tis 366 andis 1. It has four levels, and we classify the bottom-level series at
the second level from the top in the proposed method.
Wiki1consists of daily views of 145K Wikipedia articles from July 2015 to December 2016. We follow previous
studies [Ben Taieb and Koo, 2019, Rangapuram et al., 2021, Olivares et al., 2021] to select 150 bottom series and
generate a 199-series hierarchy. In these data, Tis 366 andis 1. It has Ô¨Åve levels, and we classify the bottom-level
series at the fourth level from the top in the proposed method.
4.2.2 Evaluation Metrics
As with 4.1.2, to evaluate the accuracy of probabilistic forecasts, we evaluate the likelihood of the forecast distribution
with NLL and the results of point forecasts with RMSE. For both evaluation metrics, we calculate averages for each
aggregation level.
4.2.3 Methods
We adopt as baselines the following three recent methods that have been successful in this task.
DeepV AR [Salinas et al., 2019] extends the V AM to deep learning with RNN, a probabilistic forecasting method that
can determine the parameters of the distribution that a multivariate time series follows. Hierarchical consistency is not
guaranteed.
DeepV AR+ [Rangapuram et al., 2021] adds hierarchical consistency to DeepV AR‚Äôs prediction with respect to sums.
HierE2E [Rangapuram et al., 2021] Ô¨Årst estimates the distribution followed by the target set of series and then projects
the predictions sampled from it to be hierarchically consistent with respect to the sums. To the best of our knowledge,
this is the state-of-the-art method for this task.
Next we explain about our proposed model HiPerformer.
HiPerformer. We use full proposed model HiPerformer, which we call HiPerformer-prob in 4.1. We Ô¨Årst select one of
the aggregation levels as the class level and classify the bottom-level series based on it. Subsequently, we obtain the
feature values of each bottom-level series by feature extractor in 3.4. Here, the feature values of the aggregate series are
the sum of the feature values of the bottom-level series in the sub-tree with itself as the root. Currently, we have the
feature values of all the series including the aggregate series and predict the distribution using the distribution estimator
in 3.5. We use ISAB described in 2.3 to reduce computational cost when taking self-attention for Sc;Cdimension, and
the dimension of inducing points [Lee et al., 2019] is set to 20.
A comparison of baselines with the proposed method is shown in 1.
Implementation Details. We use the same parameters in 4.2.3 for our model, except for batch size. We set the batch
size 8 for labour, 4 for trafÔ¨Åc, and 4 for wiki. We train baseline models with parameters used in [Rangapuram et al.,
2021].
4.2.4 Evaluation
The results are shown in the 4. The proposed model outperforms the baseline on all datasets and metrics except for
the bottom-level NLL of TrafÔ¨Åc and Wiki. Particularly, the improvement in performance on RMSE is remarkable,
indicating that the proposed method captures the hierarchical consistency for sums well. The NLL is often markedly
improved compared to the baseline, demonstrating the proposed method‚Äôs capacity to generate diverse joint distributions
while preserving the hierarchical dependency structure, even for multilevel data.
5 Conclusion
We proposed two prediction models HiPerformer and HiPerformer-w/o-class that can be applied to a set of time
series in which new entries and exits of series can occur. The Ô¨Årst model is HiPerformer that can capture hierarchical
1https://www.kaggle.com/c/web-trafÔ¨Åc-time-series-forecasting/data
11HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
dependencies among series. To achieve this property, we deÔ¨Åned the new concepts of hierarchical permutation and
hierarchical permutation-equivariance and showed that HiPerformer is hierarchically permutation-equivariant. The
second model is HiPerformer-w/o-class that is permutation-equivariant and captures the relationships among series
without using class information. Both models can output various joint distributions, including time-varying covariances.
These proposed models achieve state-of-the-art performance on synthetic and real-world datasets. From this, we
indicate that using its hierarchical dependency structure is effective in forecasting classiÔ¨Åed time series sets.
Acknowledgements
This work was partially supported by JST AIP Acceleration Research JPMJCR20U3, Moonshot R&D Grant Number
JPMJPS2011, CREST Grant Number JPMJCR2015, JSPS KAKENHI Grant Number JP19H01115 and Basic Research
Grant (Super AI) of Institute for AI and Beyond of the University of Tokyo.
References
Kunlapath Sukcharoen and David J Leatham. Dependence and extreme correlation among us industry sectors. Studies
inEconomics andFinance, 33(1):26‚Äì49, 2016.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola.
Deep sets. In Advances inNeural Information Processing Systems, volume 30, pages 3391‚Äì3401, 2017.
Kyunghyun Cho, Bart van Merrienboer, √áaglar G√ºl√ßehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learn-
ing phrase representations using RNN encoder-decoder for statistical machine translation. CoRR , abs/1406.1078,
2014. URL http://arxiv.org/abs/1406.1078 .
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural language models
use context. arXiv preprint arXiv:1805.04623, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances inNeural Information Processing Systems , volume 30, pages
5998‚Äì6008, 2017.
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing
the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances inneural
information processing systems, 32, 2019.
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond
efÔ¨Åcient transformer for long sequence time-series forecasting. In Proceedings oftheAAAI Conference onArtiÔ¨Åcial
Intelligence, volume 35, pages 11106‚Äì11115, 2021.
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-
correlation for long-term series forecasting. Advances inNeural Information Processing Systems , 34:22419‚Äì22430,
2021.
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced
decomposed transformer for long-term series forecasting. arXiv preprint arXiv:2201.12740, 2022.
Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-
complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on
Learning Representations, 2021.
Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei, and Junzhou Huang. Adversarial sparse transformer for
time series forecasting. Advances inneural information processing systems, 33:17105‚Äì17115, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Communications oftheACM, 63(11):139‚Äì144, 2020.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural
network model. IEEE transactions onneural networks, 20(1):61‚Äì80, 2008.
Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent network for trafÔ¨Åc
forecasting. Advances inneural information processing systems, 33:17804‚Äì17815, 2020.
Micha√´l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast
localized spectral Ô¨Åltering. Advances inneural information processing systems, 29, 2016.
Thomas N Kipf and Max Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks. arXiv preprint
arXiv:1609.02907, 2016.
12HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai,
Cordelia Schmid, et al. Tnt: Target-driven trajectory prediction. In Conference onRobot Learning, pages 895‚Äì904.
PMLR, 2021.
Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory
forecasting with heterogeneous data. In European Conference onComputer Vision , pages 683‚Äì700. Springer, 2020.
Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. From Goals, Waypoints & Paths To Long Term
Human Trajectory Forecasting, 2020.
Florin Leon and Marius Gavrilescu. A review of tracking and trajectory prediction methods for autonomous driving.
Mathematics, 9(6):660, 2021.
Henrik Kretzschmar, Markus Kuderer, and Wolfram Burgard. Learning to predict trajectories of cooperatively navigating
agents. In 2014 IEEE international conference onrobotics andautomation (ICRA), pages 4015‚Äì4020. IEEE, 2014.
Edward Schmerling, Karen Leung, Wolf V ollprecht, and Marco Pavone. Multimodal probabilistic model-based planning
for human-robot interaction. In 2018 IEEE International Conference onRobotics andAutomation (ICRA) , pages
3399‚Äì3406. IEEE, 2018.
Panna Felsen, Pulkit Agrawal, and Jitendra Malik. What will happen next? forecasting player moves in sports videos.
InProceedings oftheIEEE international conference oncomputer vision, pages 3342‚Äì3351, 2017.
Longyuan Li, Jian Yao, Li Wenliang, Tong He, Tianjun Xiao, Junchi Yan, David Wipf, and Zheng Zhang. Grin:
Generative relation and intention network for multi-agent trajectory prediction. Advances inNeural Information
Processing Systems, 34:27107‚Äì27118, 2021.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph
attention networks. stat, 1050:20, 2017.
Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, and Tim
Januschowski. End-to-end learning of coherent probabilistic forecasts for hierarchical time series. In International
Conference onMachine Learning, pages 8832‚Äì8843. PMLR, 2021.
Kin G Olivares, O Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, and Lee Dicker. Probabilistic hierarchical
forecasting with deep poisson mixtures. arXiv preprint arXiv:2110.13179, 2021.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A
framework for attention-based permutation-invariant neural networks. In Proceedings ofthe36th International
Conference onMachine Learning, volume 97, pages 3744‚Äì3753, 2019.
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for
interacting systems. In International Conference onMachine Learning, pages 2688‚Äì2697. PMLR, 2018.
Yisong Yue, Patrick Lucey, Peter Carr, Alina Bialkowski, and Iain Matthews. Learning Ô¨Åne-grained spatial models for
dynamic sports play prediction. In 2014 IEEE international conference ondata mining , pages 670‚Äì679. IEEE, 2014.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional
generative models. Advances inneural information processing systems, 28, 2015.
Nitin Kamra, Hao Zhu, Dweep Kumarbhai Trivedi, Ming Zhang, and Yan Liu. Multi-agent trajectory prediction with
fuzzy query attention. Advances inNeural Information Processing Systems, 33:22530‚Äì22541, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable
trajectories with generative adversarial networks. In Proceedings oftheIEEE conference oncomputer vision and
pattern recognition, pages 2255‚Äì2264, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
Advances inneural information processing systems, 32, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014.
Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and Christian Claudel. Social-stgcnn: A social spatio-temporal
graph convolutional neural network for human trajectory prediction. In Proceedings oftheIEEE/CVF Conference
onComputer Vision andPattern Recognition, pages 14424‚Äì14432, 2020.
2020 Australian Bureau of Statistics. Australian bureau of statistics. labour force, australia, dec
2020., 2021. URL https://www.abs.gov.au/statistics/labour/employment-and-unemployment/
labour-force-australia/dec-2020 . Accessed on 01.18.2023.
13HiPerformer: Hierarchically Permutation-Equivariant Transformer A P REPRINT
Marco Cuturi. Fast global alignment kernels. In Proceedings ofthe28th international conference onmachine learning
(ICML-11), pages 929‚Äì936, 2011.
D. Dua and Graff. Uci machine learning repository, 2017. URL http://archive.ics.uci.edu/ml . Accessed on
01.18.2023.
Souhaib Ben Taieb and Bonsoo Koo. Regularized regression for hierarchical forecasting without unbiasedness
conditions. In Proceedings ofthe25th ACM SIGKDD International Conference onKnowledge Discovery &Data
Mining, pages 1337‚Äì1347, 2019.
David Salinas, Michael Bohlke-Schneider, Laurent Callot, Roberto Medico, and Jan Gasthaus. High-dimensional
multivariate forecasting with low-rank gaussian copula processes. Advances inneural information processing
systems, 32, 2019.
14