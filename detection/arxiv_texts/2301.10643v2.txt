Automatic Locally Robust Estimation with Generated
Regressors∗
Juan Carlos Escanciano
Universidad Carlos III de Madrid
Telmo J. P´ erez-Izquierdo
BCAM - Basque Center for Applied Mathematics
November 8, 2023
Abstract
Many economic and causal parameters of interest depend on generated regressors. Exam-
ples include structural parameters in models with endogenous variables estimated by control
functions and in models with sample selection, treatment effect estimation with propensity
score matching, and marginal treatment effects. Inference with generated regressors is com-
plicated by the very complex expression for influence functions and asymptotic variances. To
address this problem, we propose Automatic Locally Robust/debiased GMM estimators in a
general setting with generated regressors. Importantly, we allow for the generated regressors
to be generated from machine learners, such as Random Forest, Neural Nets, Boosting, and
many others. We use our results to construct novel Doubly Robust and Locally Robust es-
timators for the Counterfactual Average Structural Function and Average Partial Effects in
models with endogeneity and sample selection, respectively. We provide sufficient conditions
for the asymptotic normality of our debiased GMM estimators and investigate their finite
sample performance through Monte Carlo simulations.
Keywords: Local robustness, orthogonal moments, double robustness, semiparametric es-
timation, bias, GMM. JEL Classification: C13; C14; C21; D24
∗Research supported by MICIN/AEI/10.13039/501100011033, grant CEX2021-001181-M, Comunidad de Madrid,
grants EPUC3M11 (V PRICIT) and H2019/HUM-5891, and grant PID2021-127794NB-I00 (MCI/AEI/FEDER, UE).
1arXiv:2301.10643v2  [econ.EM]  7 Nov 20231 Introduction
Many economic and causal parameters of interest depend on generated regressors. Leading exam-
ples include the Counterfactual Average Structural Function (CASF) in models with endogenous
variables estimated by control functions (see, e.g., Blundell and Powell, 2004; Stock, 1989, 1991), Av-
erage Partial Effects (APE) in sample selection models (see, e.g., Das et al., 2003), Propensity Score
Matching (PSM) (see, e.g., Heckman et al., 1998), and Marginal Treatment Effects (MTE) using Lo-
cal Instrumental Variables (see, e.g., Heckman and Vytlacil, 2005). There are currently no general
econometric methods for inference on these parameters allowing for generated regressors obtained
by machine learning. The goal of this paper is to propose Automatic Locally Robust/Debiased
estimators of and inference on structural parameters in such models. The estimators and inferences
that we propose are automatic in the sense that influence functions and asymptotic variances are
estimated automatically from data and identifying moments. There is no need to find their analytic
shape.
We extend Chernozhukov et al. (2022a)’s results to build debiased moment conditions in the
presence of nonparametric/semiparametric generated regressors. By applying the chain rule, the
debiasing correction term can be decomposed into one accounting for the first step, which is used
to generate a regressor, and the term accounting for the second step, where the outcome variable is
regressed onto the generated variable (among other covariates). Chernozhukov et al. (2022a) con-
struct debiased moment conditions which account for this second step (with no generated regressor).
Our paper provides the additional correction term that accounts for (i) the plug-in of generated
regressors in the moment condition and (ii) the effect of generated regressor on the estimation of
the second step.
Each of the two correction terms depends on additional nuisance parameters. Under a lineariza-
tion assumption (as in, e.g., Ichimura and Newey, 2022; Newey, 1994), we show how the additional
nuisance parameters in the correction term can be estimated separately without knowing their spe-
cific analytic shape. This process is called automatic estimation (see Chernozhukov et al., 2022b).
Automatic estimation is particularly well motivated in the case of generated regressors, where the
nuisance parameters in the correction term take complex shapes (see, for instance, Escanciano et al.,
2014; Hahn and Ridder, 2013; Mammen et al., 2016).
As a leading application of our methods, we propose novel Automatic Locally Robust estimators
for the CASF parameter of Blundell and Powell (2004). This parameter is a linear functional of a
second step function satisfying orthogonality conditions involving generated regressors (the control
function) from a first step. We show that it is relatively straightforward to construct Automatic
Doubly Robust estimators that are robust to functional form assumptions for the second step. For
instance, a practical approach could be to fit a partially linear specification for the second step, like
in Robinson (1988), but with a non-parametric function of the generated regressors. Our results
cover this case, in which the second step is semiparametric.
2The Doubly Robust estimators are, however, not Locally Robust to the generated regressors
in general. To construct fully Locally Robust estimators we use numerical derivatives to account
for the presence of generated regressors. Fortunately, our automatic approach is amenable to any
machine learning method for which out of sample predictions are available. Another approach
could be to specify a model for the second step for which analytical derivatives are available, e.g.,
a partially linear model. We note that the Doubly Robust moment conditions are robust to this
model being misspecified when the adjustment term is consistently estimated (see Remark 2.2).
We provide mild sufficient conditions for the asymptotic normality of the debiased GMM esti-
mator. We illustrate the results with the CASF example. The finite sample performance of the
proposed debiased estimator for the CASF is evaluated through Monte Carlo simulations. We
use Lasso with different dictionaries (linear, quadratic, and one including interactions) to fit the
first and second step parameters, as well as the nuisance parameters in the first and second step
correction terms. Our Monte Carlo results confirm that the plug-in estimator is substantially bi-
ased. Correcting the moment condition for the second step estimation reduces de bias, but a first
step correction must be also added to remove it totally throughout all the different specifications
considered.
The paper builds on two different literatures. The first literature is the classical literature on
semiparametric estimators with generated regressors, see Ahn and Powell (1993); Heckman et al.
(1998); Ichimura and Lee (1991); Imbens and Newey (2009); Newey et al. (1999); Rothe (2009),
among many others. The asymptotic properties of several estimators within this class is given, for
example, by Escanciano et al. (2014), Hahn and Ridder (2013, 2019) and Mammen et al. (2012,
2016). With respect to these papers, we allow the second step to be semiparametric or parametric
(on top of fully non-parametric). Furthermore, we contribute to this literature by providing auto-
matic debiased GMM estimators, which allow for machine learning generated regressors and reduce
regularization and model selection biases. These biases are shown to be important in practice and
are significantly reduced by our methods in the CASF example. Our results are thus motivated by
the many applications where machine learning methods are used to generate regressors, including
high dimensional propensity score estimates for matching and treatment effects, imputed missing
covariates, and sentiment in text analysis, among many others (see Fong and Tyler, 2021, for a
partial review of applications).
The second literature we build on is the more recent literature on Locally Robust/Debiased
estimators, see, e.g., Chernozhukov et al. (2018, 2022a). With the only exception of Sasaki and Ura
(2021), this literature has not considered models with generated regressors. Our results complement
the analysis of the Policy Relevant Treatment Effect (PRTE) in Sasaki and Ura (2021) by providing
a general setting for problems with generated regressors and automatic estimation of adjustment
terms. Relative to the Automatic Locally Robust literature (e.g. Chernozhukov et al., 2022b) we
innovate by considering a nonlinear setting with an implicit functional (the generated regressor
3as a conditioning argument) for which an analytic derivative is not available for general machine
learners. We also exploit novel partial or separate robustness results for identifying and estimating
individual Riesz representers.
The proposed debiased estimators and inferences are flexible, modern, robustified, and cross-
fitted versions of commonly employed estimators in the applied econometrics literature, see, e.g.,
Heckman (1979), Rivers and Vuong (1988), and Wooldridge (2015). More broadly, our methods pro-
vide new robust estimators for models with generated regressors for which inference was unavailable,
such as that for the CASF in Blundell and Powell (2004).
We illustrate the simplicity of our estimators with the CASF parameter for a partially linear
second step ˆh(Xi,ˆVi) =ˆβ′Xi+ ˆκ(ˆVi), a control variable ˆVi=Di−ˆg(Zi) for the endogenous regressor
Di, and first step ˆ g(Zi). Both steps could be obtained, for example, from Lasso fits. Then, the
plug-in (PI), Doubly Robust (DR), and fully Locally Robust (LR) estimators are given, respectively,
by
ˆθPI=ˆβ′¯X∗,
ˆθDR=ˆθPI+1
nLX
ℓ=1X
i∈Iℓˆα2ℓ(Xi,ˆViℓ)·(Yi−ˆhℓ(Xi,ˆViℓ)),
ˆθLR=ˆθDR+1
nLX
ℓ=1X
i∈Iℓˆα1ℓ(Zi)·(Di−ˆgℓ(Zi)),
where ¯X∗is a counterfactual average for X, ˆgℓandˆhℓare cross-fitted estimates for the first and
second step parameters (not using observations in the set Iℓ),ˆViℓ=Di−ˆgℓ(Zi), and ˆ α2ℓ(Xi,ˆViℓ) and
ˆα1ℓ(Zi) are automatic estimators that we propose in (4.7) and (4.10), respectively. We also consider
nonparametric versions of these estimators that are robust to misspecification of the partially linear
model. These estimators are prototypes of a general class of automatic debiased GMM estimators
with generated regressors that we propose in this paper.
The rest of the paper is organized as follows. Section 2 introduces the setting and the exam-
ples. Section 2.1 finds the influence function of parameters identified by moments with generated
regressors. Section 3 gives the general construction of Automatic Locally Robust moments with
generated regressors. In Section 4, we provide the details for Debiased/Locally Robust GMM es-
timation. A summary of the estimation algorithm is given in Section 4.2. The asymptotic theory
for the proposed estimator is developed in Section 5. Monte Carlo simulations are presented in
Section 6. Section 7 concludes. Appendix A provides the details for the APE estimation in sample
selection models, while Appendix B gathers the proofs of the main results.
42 Setting
We observe data W= (Y, D, Z ) with cumulative distribution function (cdf) F0. For simplicity of
exposition, we consider that YandDare one-dimensional. In our setting, there is a first step
linking Dwith Z. The first step results in a one-dimensional generated regressor
V≡φ(D, Z, g 0),
where φis a known function of observed variables ( D, Z) and an unknown function g0∈∆1, for ∆ 1
a linear and closed subspace of L2(Z). Henceforth, for a generic random variable U, we denote by
L2(U) the Hilbert space of square-integrable functions of U. The unknown function g0solves the
orthogonal moments
E[δ1(Z)(D−g0(Z))] = 0 for all δ1∈∆1. (2.1)
This setting covers semiparametric and non-parametric first steps g0. For example, when ∆ 1=
L2(Z), we have g0(Z) =E[D|Z]. For general parametric first steps see Remark 3.3.
The second step links Ywith a component of ( D, Z), denoted by X, and the generated regressor
V, through the moment restrictions
E[δ2(X, V)(Y−h0(X, V))] = 0 for all δ2∈∆2(g0), (2.2)
where ∆ 2(g0) is a linear and closed subspace of L2(X, V) (note ∆ 2depends on g0because V
depends on g0). For instance, Hahn and Ridder (2013) and Mammen et al. (2016) consider cases
where the second step h0is a non-parametric regression of Yon (X, V), i.e., ∆ 2(g0) =L2(X, V).
Semiparametric examples of ∆ 2(g0) are given below.
Let Θ ⊆Rdenote the parameter space where the structural parameter of interest lies. Consider
the moment function m:Rdim(W)×L2(Z)×L2(X, V)×Θ→R. The parameter of interest θ0is
identified in a third step by a GMM moment condition
E[m(W, g 0, h0, θ0)] = 0 .
Here we assume that θ0is identified by these moments, i.e. that θ0is the unique solution to
E[m(W, g 0, h0, θ)] = 0 over θ∈Θ.
Our result allows for an arbitrary number of parameters θ∈Rdim(θ)and moment conditions
m:Rdim(W)×L2(Z)×L2(X, V)×Θ→Rdim(m), with dim( m)≥dim(θ)≥1. To ease the exposition,
most results are derived for the one-dimensional case (the extension to multiple parameters is
straightforward). Our results can be also extended to (i) allow for multidimensional DandYand
(ii) first and second steps satisfying different orthogonality conditions, as in Section 3 of Ichimura
and Newey (2022).
The following problem serves as a running example to illustrate the result of this paper:
5Example 1(Control Function Approach) We observe W= (Y, D, Z ) satisfying the model
Y=H(X, U), for an unknown function H. The main feature of this model is that D, a component
ofX, may be an endogenous regressor. We assume that the endogenous regressor satisfies D=
g0(Z) +V, with UandVbeing unobserved correlated error terms. The function g0could be
identified by a conditional mean restriction, as in equation (2.1). We assume a Control Function
approach: U|D, Z∼U|X, V∼U|V, where ∼denotes equally distributed. Thus, the corresponding
φis
V≡φ(X, Z, g 0)≡D−g0(Z).
As in Blundell and Powell (2003), the Control Function assumption implies
E[Y|X=x, V=v] =E[H(X, U)|X=x, V=v] =E[H(x, U)|X=x, V=v]
=E[H(x, U)|V=v]≡h0(x, v).
This defines the second step, which satisfies (2.2) with ∆ 2(g0) =L2(X, V).
The Control Function assumption allows us to identify the Average Structural Function (ASF)
at a point x∈Rdim(X):
ASF 0(x)≡E[H(x, U)] =E[E[H(x, U)|V]] =E[h0(x, V)].
Some well-known conditions on the support of the random vectors are needed for the above equation
to hold (see Blundell and Powell, 2004; Imbens and Newey, 2009).
In this setup, a parameter of interest is the Counterfactual Average Structural Function (CASF)
given by
θ0=Z
ASF 0(x∗)dF∗(x∗),
for a counterfactual distribution F∗. When F∗is implied by a certain policy, the CASF may be
used to measure the effect of the policy (see Blundell and Powell, 2004; Stock, 1989, 1991). By
Fubini’s Theorem, the CASF can be written as a function of ( g0, h0):
θ0=Z
E[h0(x∗, D−g0(Z))]dF∗(x∗) =EZ
h0(x∗, D−g0(Z))dF∗(x∗)
.
Hence, the moment function that identifies the CASF is:
m(w, g, h, θ ) =Z
h(x∗, d−g(z))dF∗(x∗)−θ. (2.3)
We will propose below novel Doubly Robust and fully Locally Robust estimators for the CASF
under nonparametric and semiparametric specifications of h0. ■
A remarkable feature of the above problem is that, to find the value of the moment condition for
a certain point w= (y, d, z ), one needs to know the whole shape of the second-step regression h0.
6Thus, it does not fall in Hahn and Ridder (2013, 2019)’s setup, where the moment condition evalu-
ated at wsolely depends on h0through its value at ( x, φ(d, z, g 0)). Indeed, our setting generalizes
theirs in three ways: (i) we consider a larger class of moment conditions (covering, for instance,
the CASF), (ii) we allow for semiparametric first and second steps, and (iii) we allow for a wider
range of generated regressors φ(D, Z, g 0), as the authors consider generated regressors with shape
φ(D, Z, g 0) =g0(Z). We show how their setup accommodates to this paper’s framework:
Example 2(Hahn and Ridder (2013)’ Setup) This example discusses the non-parametric
setup in Hahn and Ridder (2013, Th. 5). The authors focus on the case where there is a function
η:Rdim(W)+1→Rsuch that
m(w, g, h, θ ) =η(w, h(x, g(z)))−θ.
That is, in Hahn and Ridder (2013)’s setup, ( g, h) enters the moment condition by the values that
the “link” function η, with domain in an Euclidean space, takes at ( w, h(x, g(z))). Note that they
fixφ(d, z, g ) =g(z) and that their Theorem 5 covers the fully non-parametric case: ∆ 1=L2(Z)
and ∆ 2(g0) =L2(X, V) (other results in Hahn and Ridder, 2013, cover parametric first steps). ■
Example 1(continuing from p. 6) A practical semiparametric specification for h0when X
isp-dimensional and pis high is a partially linear model, where
h0(x, v) =x′β0+κ0(v),
with β0andκ0unknown finite and infinite-dimensional parameters, respectively. In this speci-
fication, Xcontains an intercept, and hence, we can assume that κ0belongs to the subspace of
zero mean functions in L2(V), denoted as L0
2(V). This setting corresponds to the semiparametric
orthogonality conditions (2.2) where
∆2(g0) ={δ(x, v) =x′β+κ(v):β∈Rp, κ∈L0
2(V)} ⊆L2(X, V).
This specification generalizes the classical linear structural control function approach to a non-
Gaussian semiparametric setting. In this partly linear model, the CASF is given by θ0=β′
0E∗[X],
where E∗[X] denotes the mean of Xunder the counterfactual distribution F∗.
A further generalization yields
h0(x, v) =x′β0+κ0(v) +dκ1(v),
where κ1is an additional unknown infinite-dimensional parameter, corresponding, for example, to a
semiparametric Correlated Random Coefficient specification where the coefficient of the endogenous
variable Dis random; see Wooldridge (2015) for a description of parametric versions of this model.
This version generalizes Garen (1984) to a semiparametric non-Gaussian setting. ■
7Remark 2.1(Profiling) Our results can be extended to allow for profiling as in Mammen et al.
(2016). That is, we may consider that the second step nuisance parameter depends on θ:h0(x, v, θ )
is the solution in hofE[δ2(X, V)(Y−h(X, V, θ ))] = 0 for all δ2∈∆2(g0, θ).
For instance, if h0is the conditional expectation given some transformation of ( D, Z), denoted
T(D, Z, g, θ ) as in Mammen et al. (2016), then one would take ∆ 2(g, θ)≡L2(T). In models with an
index restriction, T(D, Z, g, θ ) = (θ′X, φ(D, Z, g )), where Xis a subvector of ( D, Z) (see Escanciano
et al., 2016, for example applications).
■
2.1 Orthogonal Moment Functions with Generated Regressors
We follow Chernozhukov et al. (2022a, henceforth, CEINR) for the construction of Locally Robust-
Debiased-Orthogonal moment functions. To that end, we study the effect of the first and second
step estimation separately. This will allow us to construct separate automatic estimators of the
nuisance parameters in first and second step Influence Functions (IF).
We begin by introducing some additional concepts and notation. Let Fdenote a possible cdf for
a data observation W. We denote by g(F) the probability limit of an estimator ˆ gof the first step
when the true distribution of WisF, i.e., under general misspecification (see Newey, 1994). That
is,Fis unrestricted except for regularity conditions such as existence of g(F) or the expectation of
certain functions of the data. For example, if ˆ g(z) is a nonparametric estimator of E[D|Z=z] then
g(F)(z) =EF[D|Z=z] is the conditional expectation function when Fis the true distribution of
W, denoted by EF,which is well defined under the regularity condition that EF[|D|] is finite. We
assume that g(F) is identified as the solution in gto
EF[δ1(Z)(D−g(Z))] = 0 for all δ1∈∆1.
Hence, we have that g(F0) =g0,consistent with g0being the probability limit of ˆ gwhen F0is the
cdf of W.
To study the effect of the second step, suppose that Wis distributed according to F. However,
the first step parameter is independently fixed to g. Let h(F, g) be the solution in h∈∆2(g) to
EF[δ2(X, V(g)){Y−h(X, V(g))}] = 0 for all δ2∈∆2(g), (2.4)
where V(g)≡φ(D, Z, g ) and ∆ 2(g) is a linear and closed subspace of L2(X, V(g)). The solu-
tion of the above equation is a function of ( x, v):h(F, g)(x, v). We have that h(F0, g0) = h0.
Thus, henceforth, a subindex of 0 in hmeans the conditioning variable is V≡V(g0), for example
h0(x, φ(d, z, g )) =E[Y|X=x, V =φ(d, z, g )] in the nonparametric case. We may think of the
mapping h(F, g) as the probability limit of an estimator of h0under the following conditions: (i)
the true distribution of WisFand (ii) the estimator is built with the first step parameter fixed to
8g. A feasible estimator ˆhofh0will, however, rely on the estimator ˆ g. Therefore, we assume that
the probability limit of ˆhunder general misspecification is h(F, g(F)).
To introduce orthogonal moments, let Hbe some alternative distribution that is unrestricted
except for regularity conditions, and Fτ≡(1−τ)F0+τHforτ∈[0,1].We assume that His chosen
so that g(Fτ) and h(Fτ, g(Fτ)) exist for τsmall enough, and possibly other regularity conditions are
satisfied. The IF that corrects for both first and second step estimation , as introduced in CEINR,
is the function ϕ(w, g, h, α, θ ) such that
d
dτE[m(W, g(Fτ), h(Fτ, g(Fτ)), θ)] =Z
ϕ(w, g 0, h0, α0, θ)dH(w),
E[ϕ(W, g 0, h0, α0, θ)] = 0 ,andE[ϕ(W, g 0, h0, α0, θ)2]<∞,(2.5)
for all Hand all θ.Here αis an unknown function, additional to ( g, h), on which only the IF depends.
The “true parameter” α0is the αsuch that equation (2.5) is satisfied. Throughout the paper, d/dτ
is the derivative from the right (i.e. for non-negative values of τ) atτ= 0.Equation (2.5) is the
Gateaux derivative characterization of the IF of the functional ¯ m(g(F), h(F, g(F)), θ), with
¯m(g, h, θ )≡E[m(W, g, h, θ )].
Orthogonal moment functions can be constructed by adding this IF to the original identifying
moment functions to obtain
ψ(w, g, h, α, θ )≡m(w, g, h, θ ) +ϕ(w, g, h, α, θ ). (2.6)
This vector of moment functions has two key orthogonality properties. First, we have that varying
(g, h) away from ( g0, h0) has no effect, locally, on E[ψ(W, g, h, α 0, θ)]. The second property is that
varying αwill have no effect, globally, on E[ψ(W, g 0, h0, α, θ)]. These properties are shown in great
generality in CEINR.
The IF in equation (2.5) measures the effect that the first step (estimation of g0) and the second
step (estimation of h0) will have on the moment condition. By the chain rule, we have that these
two effects can be studied separately:
d
dτ¯m(g(Fτ), h(Fτ, g(Fτ)), θ) =d
dτ¯m(g(Fτ), h(F0, g(Fτ)), θ)
+d
dτ¯m(g0, h(Fτ, g0), θ).(2.7)
In the above display, the first derivative in the right hand side (RHS) accounts for the first
step. As in Hahn and Ridder (2013), the first step affects the moment condition in two ways (see
Figure 1). We have a direct impact on ¯m, which includes the effect of evaluating hon the generated
regressor. We also have an indirect effect on the moment that comes from gaffecting estimation of
h0in the second step (through conditioning). This is present in the term h(F0, g(Fτ)). Both effects
9(direct and indirect) are considered in (2.7). The second derivative in (2.7) accounts for the effect
of the second step. This effect is independent of the first step and, as such, considers that g0is
known. This is captured by h(Fτ, g0).
τ Fτ g(Fτ) h(Fτ, g(Fτ))
¯m(g(Fτ), h(Fτ, g(Fτ)), θ)(E)
(2S)(D)
Figure 1The effect of a deviation Fτon the moment condition. (2S) represents the second step effect.
(D) represents the direct effect of the first step. The path (E)-(2S) represents the indirect estimation effect
of the first step.
We may then find an IF corresponding to each step: ϕ1(w, g, α 1, θ) and ϕ2(w, h, α 2, θ), respec-
tively. The IFs satisfy, for all θandH:
d
dτ¯m(g(Fτ), h(F0, g(Fτ)), θ) =Z
ϕ1(w, g 0, α01, θ)dH(w) and (2.8)
d
dτ¯m(g0, h(Fτ, g0), θ) =Z
ϕ2(w, h 0, α02, θ)dH(w), (2.9)
on top of the zero mean and square integrability conditions (see equation (2.5)). We therefore
have that the IF accounting for both the first and second step is ϕ(w, g, h, α, θ ) =ϕ1(w, g, α 1, θ) +
ϕ2(w, h, α 2, θ), with α= (α1, α2) . The true values are denoted by α0= (α01, α02).
We now provide separate orthogonality conditions that will serve as a basis for the auto-
matic estimation of the nuisance parameters α01andα02. Define the following moment condi-
tions: ψ1(w, g, α 1, θ)≡m(w, g, h (F0, g), θ) +ϕ1(w, g, α 1, θ) for the first step, and ψ2(w, h, α 2, θ)≡
m(w, g 0, h, θ) +ϕ2(w, h, α 2, θ) for the second step. Applying separately Theorem 1 in CEINR to ψ1
andψ2one gets
d
dτE[ψ1(W, g(Fτ), α1(Fτ), θ)] = 0 andd
dτE[ψ2(W, h(Fτ, g0), α2(Fτ), θ)] = 0 .
Since ∆ 1and ∆ 2(g0) are linear, the above equations mean that, for all θ∈Θ,
d
dτE[ψ1(W, g 0+τδ1, α01, θ)] = 0 for all δ1∈∆1and
d
dτE[ψ2(W, h 0+τδ2, α02, θ)] = 0 for all δ2∈∆2(g0).(2.10)
10This result comes from applying Theorem 3 in CEINR separately to each step. Here δ1represents a
possible direction of deviation of g(F) from g0. In turn, δ2represents a possible deviation of h(F, g 0)
from h0. The parameter τis the size of a deviation. The innovation with respect to CEINR is that
we can compute the IF ϕby separately studying ψ1andψ2, corresponding to the first and second
steps, respectively. This means we can separately identify α01andα02from (2.10). To the best of
our knowledge, this separate identification result argument is new to this paper.
Remark 2.2(Doubly Robust and Locally Robust) A Doubly Robust estimator (with
respect to (w.r.t.) the second step h0is based on the moment E[ψ2(W, h 0, α02, θ)] = 0 when
E[ψ2(W, h, α 02, θ)] is affine in h. To provide intuition, consider a simpler setting where the parameter
of interest is a moment linear in the second step, such that
θ0=E[m(W, h 0)] =E[α02(X, V)h0(X, V)].
CEINR shows that a LR moment for θ0, not accounting for the first step, is
ψ2(w, h, α 2, θ) =θ−m(w, h)−α2(x, v)(y−h(x, v)).
This is a Doubly Robust moment w.r.t. hin the sense that
E[ψ2(W, h, α 2, θ)] =θ−θ0+θ0−E[α02(X, V)h(X, V)]−E[α2(X, V){h0(X, V)−h(X, V)}]
=θ−θ0+E[α02(X)h0(X, V)]−E[α02(X, V)h(X, V)]
+E[α2(X, V){h(X;V)−h0(X, V)}]
=θ−θ0+E[{α2(X, V)−α02(X, V)}{h(X, V)−h0(X, V)}].
From this fundamental equation in CEINR, the doubly robust property follows. We only need α2
orhto be correctly specified to identify the parameter of interest. The estimator based on ψ2does
not account for the estimation of the generated regressors, and as such, is not fully locally robust.
For the latter, the estimator needs to be based on the ψgiven in (2.6) above. ■
3 Automatic estimation of the nuisance parameters
The debiased moments require a consistent estimator ˆ αof the nuisance parameters α0≡(α01, α02).
When the form of α0is known, one can plug-in nonparametric estimators of the unknown com-
ponents of α0to form ˆ α. In the generated regressors setup, however, the nuisance parameters
(especially α01) have a complex analytical shape (see the result in equation (B.6) in the Appendix,
the examples in Section 3.1, and Hahn and Ridder, 2013). Therefore, the plug-in estimator for ˆ α
may be cumbersome to compute in practice.
We propose an alternative approach which uses the orthogonality of ψ1andψ2with respect to
gandh, respectively, to construct estimators of ( α01, α02). This approach does not require knowing
11the form of α0. It is “automatic” in only requiring the orthogonal moment functions and data for
the construction of ˆ α. Moreover, an automatic estimator can be constructed separately for each
step. For more details, we refer to Section 3.2.
This section shows that, under some assumptions, the correction term takes to form:
ϕ(w, g 0, h0, α0, θ) =α01(z)·[d−g0(z)]| {z }
=ϕ1(w,g0,α01,θ)+α02(x, φ(d, z, g 0))·[y−h0(x, φ(d, z, g 0))]| {z }
=ϕ2(w,h0,α02,θ). (3.1)
That is, each correction term is built by multiplying the nuisance parameter by each step’s prediction
error. An important ingredient for the automatic construction is a consistent estimator of the
linearization of the moment condition with respect to each parameter ( gfor the first step and hfor
the second). Section 3.1 provides the formal development.
3.1 First and Second Step Linearization
We start with the linearization of the second step effect. This result is well established in the
literature (see, e.g., Newey, 1994, Equation 4.1) and will follow immediately if ¯ m(g0, h, θ) is linear
inh.
Before introducing the result, we note that throughout this section (i) τ7→hτdenotes a dif-
ferentiable path, i.e., 0 7→h0anddhτ/dτexists (equivalently for gτ) and (ii) His regular in the
sense that, for Fτ≡(1−τ)F0+τH,g(Fτ) is a differentiable path in L2(Z), and h(Fτ, g0) and
h(F0, g(Fτ)) are differentiable paths in L2(X, V).
3.1.1 Second Step Linearization
We assume that ¯ mcan be linearized with respect to the second step parameter:
Assumption 3.1 There exists a function D2(w, h) such that
d¯m(g0, hτ, θ)
dτ=dE[D2(W, h τ)]
dτ
for every θ∈Θ. Moreover, h7→E[D2(W, h)] is linear and continuous in L2(X, V).
The same assumption has been considered in Newey (1994). We can then get the shape of the
second step IF:
Pr oposition 3.1 Under Assumption 3.1:
(Lin) We can linearize the effect of the second step estimation:
d
dτ¯m(g0, h(Fτ, g0), θ) =d
dτE[D2(W, h(Fτ, g0))].
12(IF) There exists an α02∈∆2(g0)such that the function
ϕ2(w, h 0, α02, θ) =α02(x, φ(d, z, g 0))· {y−h0(x, φ(d, z, g 0))},
satisfies equation (2.8) and is thus the Second Step IF.
The shape of the second step nuisance parameter α02is given in equation (B.1) in the Appendix.
We note that, since ¯ mis linearized at ( g0, h0, θ),D2(and also α02) may also depend on ( g0, h0, θ).
This is omitted for notational simplicity but will become relevant to construct feasible automatic
estimators (see Section 4). We now verify the linearization of ¯ m(g0, h, θ) in some examples:
Example 1(continuing from p. 6) Assumption 3.1 is easy to check for the CASF. Since
m(w, g 0, h, θ) is already linear in h, we have that
D2(w, h) =Z
h(x∗, φ(d, z, g 0))dF∗(x∗).
In this case, we can compute the analytic shape of the correction term nuisance parameter α02.
To find it, we follow P´ erez-Izquierdo (2022) and assume the existence of densities f∗,fv
0and,fxv
0for
F∗,Fv
0andFxv
0, respectively. Here Fv
0andFxv
0denote the distribution under F0ofVand ( X, V),
respectively. We then have that
E[D2(W, h)] =Z
h(x∗, v)f∗(x∗)fv
0(v)dx∗dv=Zf∗(x∗)fv
0(v)
fxv
0(x∗, v)h(x∗, v)fxv
0(x∗, v)dx∗dv
=E[r2(X, V)h(X, V)],
with r2(x, v)≡f∗(x)fv
0(v)/fxv
0(x, v). Thus, a sufficient condition for Assumption 3.1 is that r2is
square-integrable. In the nonparametric case, i.e. ∆ 2(g0) =L2(X, V), it follows that α02=r2. Note
that, even if we have found the nuisance parameter α02, it has a rather complex shape. It depends
on the density of the generated regressor Vand on the joint density of ( X, V). For semiparametric
specifications of the second step, such as the partially linear model, the nuisance parameter α02is the
orthogonal projection of r2onto ∆ 2(g0). These objects are generally hard to estimate and may cause
the plug-in estimator for α02to behave poorly. We advocate automatic estimation (Section 3.2) as
a potential solution to this issue. ■
Example 2(continuing from p. 7) We linearize the moment condition in h. To do it, we as-
sume that η(w, s) is differentiable w.r.t. s. In that case, as long as we can interchange differentiation
and integration:
d
dτ¯m(g0, hτ, θ) =Ed
dτη(W, h τ(X, g 0(Z)))
=E∂η
∂s(W, h 0(X, g 0(Z)))d
dτhτ(X, g 0(Z))
=d
dτE∂η
∂s(W, h 0(X, g 0(Z)))hτ(X, g 0(Z))
,
13so that
D2(w, h) =∂η(w, h 0(x, g0(z)))/∂s·h(x, g0(z)).
Letr2denote the conditional expectation of ∂η(W, h 0(X, g 0(Z)))/∂sgiven ( X, V). In the fully
non-parametric case, the second step nuisance parameter is α02=r2. More generally, α02is the
orthogonal projection of r2onto ∆ 2(g0). ■
3.1.2 First Step Linearization
We now move to linearize the first step effect. Note that if the chain rule can be applied:
d
dτ¯m(g(Fτ), h(F0, g(Fτ)), θ) =d
dτ¯m(g(Fτ), h0, θ)
+d
dτ¯m(g0, h(F0, g(Fτ)), θ).(3.2)
The first derivative in the RHS can be easily analyzed if we linearize ¯ m(g, h0, θ) ing:
Assumption 3.2 There exists a function D11(w, g) such that
d¯m(gτ, h0, θ)
dτ=dE[D11(W, g τ)]
dτ
for every θ∈Θ. Moreover, g7→E[D11(W, g)] is linear and continuous in L2(Z).
To study d¯m(g0, h(F0, g(Fτ)), θ)/dτwe generalize the key Lemma 1 in Hahn and Ridder (2013)
to allow for semiparametric second steps as in equation (2.2):
Lemma 3.1 Assume that the chain rule can be applied along the path τ7→gτ. Then, for every
δ2∈L2(X, V)satisfying that there exists an ε >0such that δ2∈ ∩ τ<ε∆2(gτ):
d
dτE[δ2(X, V)·h(F0, gτ)(X, V)] =−d
dτE[δ2(X, V)·h0(X, V(gτ))]
+d
dτE[δ2(X, V(gτ))·(Y−h0(X, V))].
The condition that the function δ2belongs to every set ∆( g) for gclose to g0is related to
“regularity” of ∆ 2(g). If the functions in the sets ∆ 2(g) have the same shape, one would expect
that many δ2’s satisfy the condition in the above lemma. The condition allows to take derivatives
in equation (2.4) along the path ( F, g) = (F0, gτ).
To linearize the first-step, we ask the second step nuisance parameter α02to satisfy the condition
in Lemma 3.1. We should also impose some additional assumptions on the paths τ7→h(F0, gτ).
This allows us to express d¯m(g0, h(F0, g(Fτ)), θ)/dτas an inner product.
Assumption 3.3 For every path τ7→gτthere exits an ε >0 such that
a.α02∈ ∩ τ<ε∆2(gτ), and
14b.h(F0, gτ)∈∆2(g0) for all τ < ε .
As we have emphasized, this assumption is related to “regularity” in the shape of the functions
in ∆ 2(g). In both the non-parametric case ∆ 2(g) =L2(X, V(g)) and the partly linear case ∆ 2(g) =
{β′x+κ(v):β∈Rp, κ∈L2(V(g))}the assumption translates into square-integrability conditions
(see Remark 3.2 below). What Assumption 3.3 rules out is to specify a partly linear model for some
g’s and a non-parametric regression for others.
Once we can apply Lemma 3.1, the remaining step is to linearize the terms h0(X, φ(D, Z, g (Fτ)))
andα02(X, φ(D, Z, g (Fτ))). To achieve this, we require h0,α0, and φto be differentiable in the
appropriate sense:
Assumption 3.4 h0(x, v) and α02(x, v) are a.s. differentiable w.r.t. v. Moreover, the function
φ(d, z, g ), understood as a mapping g7→φ(d, z, g ) from L2(Z) toL2(D, Z), is Hadamard differen-
tiable at g0, with derivative Dφ.
The Hadamard derivative of φis a linear and continuous map Dφ:L2(Z)→L2(D, Z) such that
d
dτφ(d, z, g τ) =d
dτDφgτ.
In many applications, either φ(d, z, g ) =g(z) (first step prediction) or φ(d, z, g ) =d−g(z) (first
step residual). In those cases, Dφg=gorDφg=−g, respectively.
The next theorem gives the shape of the first step IF:
Theorem 3.1 Under Assumptions 3.1-3.4:
(Lin) The function
D1(w, g)≡D11(w, g) +∂
∂v[α02(x, v)(y−h0(x, v))]·Dφg, (3.3)
where the derivative is evaluated at v=φ(d, z, g 0), satisfies
d
dτ¯m(g(Fτ), h(F0, g(Fτ)), θ) =d
dτE[D1(W, g(Fτ))].
(IF) There exists an α01∈∆1such that the function
ϕ1(w, g 0, α01, θ) =α01(z)· {d−g0(z)},
satisfies equation (2.9) and is thus the First Step IF.
The shape of the first step nuisance parameter α01is given in equation (B.6) in the Appendix. It
generally is a rather complex function. Indeed, the linearization with respect to the first step effect
is also complex (see its definition in equation (3.3)). The first term is standard and corresponds to
the linearization of the direct effect of g. It is given by D11, the linearization of d¯m(gτ, h0, θ)/τ. The
15second term corresponds to the indirect effect. Consistent estimation of the second term generally
requires estimators for (i) g0, (ii) h0, (iii) ∂h0/∂v, (iv) α02, and (v) ∂α02/∂v. Section 4 provides the
details on how to estimate E[D1(W, g)]. We also note that α01is generally different from zero, and
hence, not accounting for the first step effect in the asymptotic variance leads to invalid inferences.
Remark 3.1(Influence function under an index restriction) We can expand the
derivative in equation (3.3) to get
D1(w, g) =D11(w, g) +∂α02
∂v(x, v)(y−h0(x, v))−∂h0
∂v(x, v)α02(x, v)
·Dφg
This expression is simplified if we impose the index restriction E[Y|D, Z] =E[Y|X, V], or more
generally, if ( Y−h0(X, V)) is orthogonal to ∂α02(X, V)/∂v·Dφg(Fτ)(D, Z). Indeed, since Dφgis
a function of ( D, Z), by the Law of Iterated Expectations:
d
dτ¯m(g(Fτ), h(F0, g(Fτ)), θ) =d
dτE
D11(W, g(Fτ))
+∂α02
∂v(X, V)(y−h0(X, V))−∂h0
∂v(X, V)α02(X, V)
Dφg(Fτ)
=d
dτE
D11(W, g(Fτ))−∂h0
∂v(X, V)α02(X, V)Dφg(Fτ)
.
That is, the term depending on the derivative of α02is no longer present. This result was highlighted
in Remark 1 in Hahn and Ridder (2013), see also Hahn et al. (2023). ■
Remark 3.2(About Assumption 3.3) Assumption 3.3 was missing in the fundamental Lemma
1 of Hahn and Ridder (2013). When the second step is non-parametric or partly linear, As-
sumption 3.3 reduces to square-integrability conditions. For the non-parametric case (∆ 2(g) =
L2(X, V(g))), the assumption requires that α02∈L2(X, V(gτ)) and h(F0, gτ)∈L2(X, V) for small
τ. That is, for all 0 ≤τ < ε ,
Z
α02(x, φ(d, z, g τ))2dF0(w)<∞andZ
h(F0, gτ)(x, φ(d, z, g 0))2dF0(w)<∞.
Assumption 3.3 also leads to similar requirements in the partly linear case, where ∆ 2(g) =
{β′x+κ(v):β∈Rp, κ∈L2(V(g))}. Note that, since α02∈∆2(g0), we have that α02(x, v) =
β′
0x+κ0(v) for β0∈Rpandκ0∈L2(V). Then α02∈∆2(gτ)⇐⇒ κ0∈L2(V(gτ)). In turn, since
h(F0, gτ)∈∆2(gτ), we have that h(F0, gτ)(x, v) =β′
τx+κτ(v) for βτ∈Rpandκτ∈L2(V(gτ)).
Therefore, Assumption 3.3.b imposes κτ∈L2(V).
The next proposition gives primitive sufficient conditions for Assumption 3.3.a:
Pr oposition 3.2 Under Assumption 3.4, if ∂α02(x, v)/∂vis bounded, then Assumption 3.3.a is
satisfied in the non-parametric and partly linear cases.
16We can give sufficient conditions for Assumption 3.3.b in terms of smoothness of the distribution
of the data as gτapproaches g0. Let Fxv
τandFv
τbe the distributions of ( X, V(gτ)) and V(gτ),
respectively. Note that Fxv
0andFv
0denote the distributions of ( X, V) and V, respectively. Then:
Pr oposition 3.3 Assumption 3.3.b is satisfied in the non-parametric case under the following
conditions:
1.E[Y4]<∞,
2. there exists an ε >0such that, for τ < ε ,Fxv
τandFxv
0are equivalent measures (absolutely
continuous between each other), and
3.E[ντ(X, V)]<∞, being ντthe Radon-Nikodym density of Fxv
0w.r.t. Fxv
τ.
Moreover, Assumption 3.3.b is satisfied in the partly linear case if Condition 1 is replaced by:
Condition 1∗.E[YrXs]<∞, for every s, r∈Nsatisfying s+r= 4, and Conditions 2-3 hold with
Fv
τreplacing Fxv
τ.
■
Remark 3.3(General Parametric First Steps) Letgbe a general finite dimensional
parameter, with g0denoting the true value, and let ˆ gbe an estimator for g0satisfying
√n(ˆg−g0)) =1√nnX
i=1ψ(Di, Zi) +oP(1).
Then, all our previous results apply with the adjustment term
ϕ1(w, g 0, α01, θ) =α01·ψ(d, z),
where α01=∂E[D1(W, g 0)]/∂g. ■
We conclude the section by finding D1for several examples:
Example 1(continuing from p. 6) The Control Function setup used in this paper satisfies
Assumption 3.3. In the nonparametric case, h0(X, φ(D, Z, g 0)) =E[Y|D, Z] and the simplification
of Remark 3.1 applies.
Moreover, the Control Function approach we follow here uses the residual of the first step to
control for potential endogeneity. Thus, φ(d, z, g ) =d−g(z) and its linearization is Dφg=−g.
Provided that h0is differentiable w.r.t. v(Assumption 3.4), this allows us to linearize, w.r.t. g, the
17moment condition defining the CASF. We have that:
d
dτ¯m(gτ, h0, θ) =d
dτEZ
h0(x∗, φ(D, Z, g τ))dF∗(x∗)−θ
=EZd
dτh0(x∗, φ(D, Z, g τ))dF∗(x∗)
=EZ∂h0
∂v(x∗, φ(D, Z, g 0))d
dτφ(D, Z, g τ)dF∗(x∗)
=d
dτE
−Z∂h0
∂v(x∗, φ(D, Z, g 0))dF∗(x∗)gτ(Z)
.
This means that the linearization of the moment condition w.r.t. gis, with some abuse of notation,
D11(w, g) =D11(d, z)g(z), with
D11(d, z)≡ −Z∂h0
∂v(x∗, d−g0(z))dF∗(x∗).
We can now plug in the expression for D11into equation (3.3), where the linearization of the
first step effect is defined. Recall that Dφg=−g. Then, for the CASF, equation (3.3) becomes
D1(w, g)≡
D11(d, z) +∂h0
∂v(x, v)α02(x, v)
g(z).
As discussed above, the linearization depends on h0andα02, and the derivative of h0w.r.t. v. It
also depends on g0, asv≡d−g0(z). Section 3.2 discusses how to construct an automatic estimator
for the first step nuisance parameter α02. Finding an estimator of the derivative of h0will depend
on the estimator at hand. In Section 4 we propose a numerical derivative approach that works for
a variety of second step estimators, such as Random Forest. ■
Example 2(continuing from p. 13) Theorem 3.1 generalizes Theorem 5 in Hahn and Ridder
(2013) to allow for (i) arbitrary functionals ¯ m(g, h, θ ) that are Hadamard differentiable w.r.t. g
andh, (ii) semiparametric first and second steps, and (iii) generated regressors given by arbitrary
Hadamard differentiable functions φ.
We show how the expression for D1simplifies to that in Hahn and Ridder (2013, Th. 5). We
start by linearizing ¯ m(g, h0, θ) w.r.t. g. Note that Hahn and Ridder (2013), in the non-parametric
case, fix φ(d, z, g ) =g(z). Then, Dφg=g. On top of ηbeing differentiable w.r.t. s, we require h0
to be differentiable w.r.t. v(Assumption 3.4). Then:
d
dτ¯m(gτ, h0, θ) =E∂η
∂s(W, h 0(X, g 0(Z)))d
dτh0(X, g τ(Z))
=E∂η
∂s(W, h 0(X, g 0(Z)))∂h0
∂v(X, g 0(Z))d
dτgτ(Z)
=d
dτE∂η
∂s(W, h 0(X, g 0(Z)))∂h0
∂v(X, g 0(Z))gτ(Z)
,
18and therefore D11(w, g) =∂η(w, h 0(x, g0(z)))/∂s·∂h0(x, g0(z))/∂v·g(z).
Recall from the previous discussion that the Second Step nuisance parameter satisfies:
α02(x, v) =E∂η
∂s(W, h 0(X, g 0(Z)))X=x, g0(Z) =v
.
So, if we denote ξ(w)≡∂η(w, h 0(x, g0(z)))/∂s, equation (3.3) becomes:
D1(w, g)≡
(y−h0(x, v))·∂α02
∂v(x, v) + (ξ(w)−α02(x, v))·∂h0
∂v(x, v)
g(z),
where v≡g0(z). This is the result in Hahn and Ridder (2013, Th. 5).
Moreover, note that α02(x, v) =E[ξ(W)|X=x, V=v]. Then, if ξis only a function of ( x, v),
the second term in the above equation is zero. This is the case in Theorem 2 in Hahn and Ridder
(2013). There, η:R→R, and therefore, ξ(w) =∂η(h0(x, v))/∂sis a function of ( x, v). ■
3.2 Building the automatic estimators
We can obtain automatic estimators for α0from the linearization results in the previous section.
We start with the procedure to automatically estimate α02, the nuisance parameter of the Second
Step IF. We want to stress, nevertheless, that the procedure is quite general. Indeed, we will also
apply it, mutatis mutandis , to the estimation of the nuisance parameter in the first step.
The starting point is to expand the second equation in (2.10). For δ2∈∆2(g0),
d
dτ¯m(g0, h0+τδ2, θ) +d
dτE[ϕ2(W, h 0+τδ2, α02, θ)] = 0 . (3.4)
We will now combine the above equation with the linearization result in Proposition 3.1. By
continuity and linearity of D2, we have that
d
dτ¯m(g0, h0+τδ2, θ) =d
dτE[D2(W, h 0+τδ2)] =E[D2(W, δ 2)],for any δ2∈∆2(g0). (3.5)
Moreover, Proposition 3.1 gives us that ϕ2=α02(y−h0). Thus, for any δ2∈∆2(g0),
d
dτE[ϕ2(W, h 0+τδ2, α02, θ)] =−E[δ2(D, Z)α02(X, V)]. (3.6)
From equations (3.4)-(3.6), for each δ2∈∆2(g0),
E[D2(W, δ 2)]−E[δ2(D, Z)α02(X, V)] = 0 ,for each δ2∈∆2(g0). (3.7)
We now assume that there is a dictionary ( bj)∞
j=1whose closed linear span is ∆ 2(g0). That
is, any function in ∆ 2(g0) can be approximated, in the L2sense, by a linear combination of bj’s.
Then, there exists a sequence of real numbers ( ρ0j)∞
j=1such that α02=P∞
j=1ρ0jbj. Thus, α02can
19be approximated by b′
Jρ0J, where bJ= (b1, ..., b J)′andρ0J= (ρ01, ..., ρ 0J)′.1We can now plug in
b′
Jρ0Jinto equation (3.7) for δ2=bj,j= 1, ..., J . This gives the following Jmoment conditions:
E[bJ(X, V)bJ(X, V)′]ρ0J=E[D2(W,bJ)],
where D2(w,bJ)≡(D2(w, b 1), ..., D 2(w, b J))′.
The above moment conditions can be used to construct an OLS-like estimator of ρ0J. Note,
however, that in high dimensional settings E[bJ(X, V)bJ(X, V)′] may be near singular. Therefore,
we rather focus on a regularized estimator. The above Jmoment conditions are the first order
conditions of the minimization problem:
min
ρJ∈RJ{−2E[D2(W,bJ)′]ρJ+ρ′
JE[bJ(X, V)bJ(X, V)′]ρJ}.
We can regularize the problem by adding a penalty to the above objective function. Let ∥ρJ∥q≡
(PJ
j=1|ρj|q)1/qforq≥1. For a tunning parameter λ≥0, we can estimate ρ0Jby minimizing:
min
ρJ∈RJ
−2E[D2(W,bJ)′]ρJ+ρ′
JE[bJ(X, V)bJ(X, V)′]ρJ+λ∥ρJ∥q
q	
. (3.8)
Forq= 1, the above is the Lasso objective function, while q= 2 corresponds to Ridge Regression.
Additionally, we could consider elastic net type penalties, where λ(ξ∥ρJ∥2
2+ (1−ξ)∥ρJ∥1), for
ξ∈[0,1], is added to the objective function.
We propose now an automatic estimator of α01, the nuisance parameter of the First Step IF.
The procedure is parallel to that proposed above. By Theorem 3.1, we can linearize ¯ m(g, h(F0, g), θ)
byD1(w, g) (see equation (3.3)). Again, we assume that there is a dictionary ( ck)∞
k=1that spans
∆1. Thus, α01=P∞
k=1β0kckfor a sequence of real numbers ( β0k)∞
k=1. We can therefore construct
Kmoment conditions
E[cK(Z)cK(Z)′]β0K=E[D1(W,cK)],
where cK= (c1, ..., c K)′,β0K= (β01, ..., β 0K)′, and D1(w,cK)≡(D1(w, c 1), ..., D 1(w, c K))′. We use
these conditions as a basis to construct the objective function to estimate β0K:
min
βK∈RK
−2E[D1(W,cK)′]βK+β′
KE[cK(Z)cK(Z)′]βK+λ∥βK∥q
q	
, (3.9)
where the tuning parameter λmay be different from that of the second step.
From the above discussion, we conclude that automatic estimation of the first and second step
nuisance parameters reduces to finding a consistent estimator of E[D2(W,bJ)] and E[D1(W,cK)].
We note that, in general, both D2andD1depend on ( g0, h0, θ). In the sample moment conditions,
these are replaced by cross-fit estimators (Section 4.1).
Furthermore, D1may additionally depend on ∂h0/∂v, the nuisance parameter of the Second
Step α02and its derivative ∂α02/∂v(see equation (3.3)). Estimation of ∂h0/∂vis discussed in
1For a d1×d2matrix A,A′denotes its transpose. In this respect, vectors are considered d1×1 matrices.
20Section 4.1. Here, we sketch a possible approach to estimate the derivative of α02. Recall that the
Second Step nuisance parameter can be approximated by b′
Jρ0J. We may assume that the atoms
bj(x, v) are differentiable w.r.t. v. We can then replace the nuisance parameter by its approximation
b′
Jρ0Jand its derivative by ( ∂bJ/∂v)′ρ0Jin equation (3.3). We illustrate the results of this section
with the Partially Linear model for the CASF.
Example 1(continuing from p. 7) We illustrate how some simplifications for estimating the
linearizations may occur in semiparametric settings with the partially linear model and the CASF.
The zero mean restriction of the nonparametric component in the partial linear specification implies
that
E[D2(W, δ 2)] =β′E∗[X].
forδ2(x, v) =β′x+κ(v). Therefore, if we select a dictionary bJ= (b1, ..., b J)′such that the first
pcomponents ( b1, ..., b p) =Xand ( bp+1, ..., b J) are functions of v, then, the linear approximations
necessary for the automatic estimation of α02are known in this example, with
E[D2(W, X )] =E∗[X]
E[D2(W, b j)] = 0 ,forj=p+ 1, . . . , J.
The Second Step nuisance parameter α02can be approximated by b′
Jρ0J, with ρ0Jsolving (3.8).
Likewise, the expression for the linearization w.r.t. the first step simplifies to
E[D1(W, c k)] =E[(α02(X, V)−1) ˙κ0(V)ck(Z)],
where ˙ κ0(v) =∂h0/∂vcan be approximated by ( ∂bJ/∂v)′η0Jwhen h0is approximated by b′
Jη0J.
■
4 Estimation
In this section, we build debiased sample moment conditions for GMM estimation of θ. Debiased
sample moments are based in the orthogonal moment function ψin equation (2.6). The IF ϕthat
corrects for both the first and second step estimation is ϕ=ϕ1+ϕ2, the sum of the First and
Second Step IFs. Its shape is given in equation (3.1) (see also Proposition 3.1 and Theorem 3.1).
The full estimation algorithm is summarized in Figure 2 (Section 4.2).
We propose to construct the sample moment conditions using cross-fitting, as in Chernozhukov
et al. (2018). That is, we split the sample so that ψ(Wi, g, h, α, θ ) is averaged over observations i
that are not used to estimate ( g, h, α, θ ). Cross-fitting (i) eliminates the “own observation bias”,
helping remainders to converge faster to zero, and (ii) eliminates the need for Donsker conditions
for the estimators of ( g, h, α ), which is important for first and second step ML estimators (see
Chernozhukov et al., 2018).
21We partition the sample ( Wi)n
i=1intoLgroups Iℓ, for ℓ= 1, ..., L . For each group, we have
estimators ˆ gℓ,ˆhℓand ˆαℓ= (ˆα1ℓ,ˆα2ℓ) that use observations that are not in Iℓ. We construct automatic
estimators of α0satisfying this property in Section 4.1. Moreover, for each group, we consider that
there is an initial estimator of θ0, namely ˜θℓ, which does not use the observations in Iℓ. CEINR
propose to chose L= 5 for medium size datasets and L= 10 for small datasets.
Following CEINR, debiased sample moment functions are
ˆψ(θ)≡1
nLX
ℓ=1X
i∈Iℓˆψiℓ(θ), (4.1)
with,
ˆψiℓ(θ)≡m(Wi,ˆgℓ,ˆhℓ, θ) + ˆα1ℓ(Zi)·(Di−ˆgℓ(Zi)) + ˆα2ℓ(Xi,ˆViℓ)·(Yi−ˆhℓ(Xi,ˆViℓ)), (4.2)
forˆViℓ≡φ(Di, Zi,ˆgℓ). Note that the original moment condition is evaluated at θ. On the other
hand, the initial estimators ˜θℓare used to construct the correction term ϕ(i.e., to estimate α01and
α02).
In the general case where there is more than one moment condition, the correction term for each
component of mis constructed following equation (4.2). This means that a different correction term
must be estimated for each component of m(see Section 4.1 for the details about how to proceed
with automatic estimation of each term). We use these debiased moment functions to construct the
debiased GMM estimator:
ˆθ= argmin
θ∈Θˆψ(θ)′ˆΥˆψ(θ), (4.3)
where ˆΥ is a positive semi-definite weighting matrix of dimension dim( m)×dim(m). Under some
conditions (see Section 5), the above estimator will be asymptotically normal with the usual GMM
asymptotic variance. Indeed, as in Chernozhukov et al. (2022a), there is no need to account for
estimation of ( g0, h0) and ( α01, α02) because of orthogonality of ψ.
To introduce the asymptotic variance, let
M≡E∂m
∂θ(W, g 0, h0, θ0)
and
Ψ≡E[ψ(W, g 0, h0, α0, θ0)ψ(W, g 0, h0, α0, θ0)′],
where ∂m/∂θ is the dim( m)×dim(θ)-dimensional Jacobian matrix. If ˆΥP− →Υ, the asymptotic
variance of√n(ˆθ−θ0) is Ξ ≡(M′ΥM)−1M′Υ′ΨΥM(M′ΥM)−1. A consistent estimator of the
asymptotic variance can be build by replacing the terms in Ξ by their sample analogs:
ˆM≡1
nLX
ℓ=1X
i∈Iℓ∂m
∂θ(Wi,ˆgℓ,ˆhℓ,˜θℓ) and (4.4)
ˆΨ≡1
nLX
ℓ=1X
i∈Iℓˆψiℓ(˜θℓ)ˆψiℓ(˜θℓ)′. (4.5)
22As usual in GMM, a choice of ˆΥ that minimizes the asymptotic variance of ˆθisˆΥ = ˆΨ−1. With
that choice of a weighting matrix, the asymptotic variance can be estimated by ( ˆM′ˆΨ−1ˆM)−1.
We illustrate the theory with the construction of debiased GMM estimator for the CASF:
Example 1(continuing from p. 6) Note that ϕ1=α01(d−g0) and ϕ2=α02(y−h0) (see
Theorem 3.1 and Proposition 3.1, respectively). Thus, obtaining ˆϕis straightforward once we have
cross-fitted estimators for the nuisance parameters (see Section 4.1 for the construction of ˆ α1ℓand
ˆα2ℓ).
Recall that the moment function defining the CASF is
m(w, g, h, θ ) =Z
h(x∗, φ(d, z, g ))dF∗(x∗)−θ.
We take as given that the econometrician has computed cross-fitted estimators for the first and
second steps: ˆ gℓandˆhℓ. Since the counterfactual distribution F∗is fixed by the econometrician, we
propose a numerical integration approach to obtain the debiased sample moments.
We consider that the econometrician can sample from F∗. Let ( X∗
s)S
s=1be a sample of size S
from F∗. For and observation i∈Iℓ, letˆViℓ≡Di−ˆgℓ(Zi). We approximate the value of the moment
function m(Wi,ˆgℓ,ˆhℓ, θ) by
1
SSX
s=1ˆhℓ(X∗
s,ˆViℓ)−θ.
Note that Smay be arbitrarily large (increasing the computational cost), so that the above term
is close to m(Wi,ˆgℓ,ˆhℓ, θ).
Following equations (4.1) and (4.3), the debiased estimator for the CASF is
ˆθ=1
nSLX
ℓ=1X
i∈IℓSX
s=1ˆhℓ(X∗
s,ˆViℓ) +1
nLX
ℓ=1X
i∈Iℓϕ(Wi,ˆgℓ,ˆhℓ,ˆαℓ,˜θℓ). (4.6)
The next section develops an automatic estimator for the correction term ϕ. ■
4.1 Automatic estimation with cross-fitting
Debiased sample moment functions require estimators of the nuisance parameters (ˆ α1ℓ,ˆα2ℓ) for each
group Iℓ. These estimators must use only observations not in Iℓ. This section is devoted to the
construction of automatic estimators satisfying this property. Through the section, we consider
that the econometrician has at her disposal first and second step estimators, ˆ gℓℓ′andˆhℓℓ′, and an
initial estimator, ˜θℓℓ′, that use only observations not in Iℓ∪Iℓ′; and estimators (ˆ gℓℓ′ℓ′′,ˆhℓℓ′ℓ′′,˜θℓℓ′ℓ′′)
that use only observations not in Iℓ∪Iℓ′∪Iℓ′′.
The key to automatic estimation of the Second Step nuisance parameter is to find a consistent es-
timator of the linearization of the moment condition. In this section, we will write D2(w, h|g0, h0, θ)
23to make explicit that the linearization may depend on ( h0, g0, θ) (see Examples 1 and 2). For the
linearization of the effect of first step estimation, we will write D1(w, g|g0, h0, α02, θ), to emphasize
that it may also depend on the Second Step nuisance parameter. D1generally depends also on the
derivatives ∂h0/∂vand∂α02/∂v. We do not make this explicit, but we will address the issue in this
section.
We start with the automatic estimator for the Second Step nuisance parameter. For each ℓ, we
provide a sample version of the objective function in (3.8) that uses only observations not in Iℓ.
Recall that we have a dictionary ( bj)∞
j=1that spans ∆ 2(g0). We estimate E[D2(W,bJ)] by
ˆD2ℓ≡1
n−nℓX
ℓ′̸=ℓX
i∈Iℓ′D2(Wi,bJ|ˆgℓℓ′,ˆhℓℓ′,˜θℓℓ′),
where nℓis the number of observations in Iℓ. In turn, E[bJ(X, φ(D, Z, g 0))bJ(X, φ(D, Z, g 0))′] is
estimated by
ˆBℓ≡1
n−nℓX
ℓ′̸=ℓX
i∈Iℓ′bJ(Xi, φ(Di, Zi,ˆgℓℓ′))bJ(Xi, φ(Di, Zi,ˆgℓℓ′))′.
With this, we can build an automatic estimator of the Second Step nuisance parameter that
only uses observations not in Iℓ. It is given by ˆ α2ℓ=b′
JˆρJℓ, where
ˆρJℓ= argmin
ρJ∈RJn
−2ˆD′
2ℓρJ+ρ′
JˆBℓρJ+λ∥ρJ∥q
qo
. (4.7)
The tuning parameter λcan be chosen by cross-validation.
Example 1(continuing from p. 6) We provide the ingredients to conduct an automatic
estimator of α02for the CASF. Recall that the moment condition for the CASF was already linear
inhand hence
D2(w, b j|g0, h0, θ) =Z
bj(x∗, φ(d, z, g 0))dF∗(x∗),
for each atom bjin the dictionary.
We follow the same strategy as before and approximate D2by numerical integration. Let ( X∗
s)S
s=1
be a sample drawn from F∗. To construct the objective function to estimate ˆρJℓ, for an observation
i∈Iℓ′, we set
D2(Wi, bj|ˆgℓℓ′,ˆhℓℓ′,˜θℓℓ′) =1
SSX
s=1bj(X∗
s, Di−ˆgℓℓ′(Zi)).
for each j= 1, . . . , J . ■
We now discuss the automatic estimation of the First Step nuisance parameter. Again, for each
ℓ, the goal is to build a sample version of the objective function in (3.9) that uses only observations
not in Iℓ. The construction is almost similar to the one above. We will focus on the main differences.
24For a dictionary ( cj)∞
j=1that spans ∆ 1, we can estimate E[cK(Z)cK(Z)′] by
ˆCℓ≡1
n−nℓX
ℓ′̸=ℓX
i∈Iℓ′cK(Zi)cK(Zi)′,
andE[D1(W,cK)] by
ˆD1ℓ≡1
n−nℓX
ℓ′̸=ℓX
i∈Iℓ′D1(Wi,cK|ˆgℓℓ′,ˆhℓℓ′,ˆα2ℓℓ′,˜θℓℓ′). (4.8)
The first difference is that D1depends on α02on top of ( g0, h0, θ). We therefore need to plug-in
an estimator ˆ α2ℓℓ′that only uses observations not in Iℓ∪Iℓ′. This estimator can be constructed
using the methodology above. For instance, to construct ˆ α2ℓℓ′=b′
JˆρJℓℓ′, it is simple to write the
optimization problem that ˆρJℓℓ′solves. Indeed, we can define
ˆD2ℓℓ′≡1
n−nℓ−nℓ′X
ℓ′′/∈{ℓ,ℓ′}X
i∈Iℓ′′D2(Wi,bJ|ˆgℓℓ′ℓ′′,ˆhℓℓ′ℓ′′,˜θℓℓ′ℓ′′) and
ˆBℓℓ′≡1
n−nℓ−nℓ′X
ℓ′′/∈{ℓ,ℓ′}X
i∈Iℓ′′bJ(Xi, φ(Di, Zi,ˆgℓℓ′ℓ′′))bJ(Xi, φ(Di, Zi,ˆgℓℓ′ℓ′′))′.
Thus, ˆρJℓℓ′is given by the optimization problem in (4.7) with ˆD2ℓℓ′and ˆBℓℓ′replacing ˆD2ℓand ˆBℓ,
respectively.
The most important difference is that D1generally depends also on the derivatives ∂h0/∂vand
∂α02/∂v. In Section 3.2, we have presented a parsimonious approach to estimate the derivative of
α02. It is straightforward to construct an estimator ∂ˆα2ℓℓ′/∂vof the derivative of α02that uses only
observations not in Iℓ∪Iℓ′. Since we have already estimated ˆ α2ℓℓ=b′
JˆρJℓℓ′, if each bjis differentiable
w.r.t. v, we have that ∂ˆα2ℓℓ′/∂v≡(∂bJ/∂v)′ˆρJℓℓ′.
Estimation of ∂h0/∂vmay be more tricky. It will depend on the shape of the estimator ˆhℓℓ.
Note that, since h0∈∆2(g0), we may use the dictionary ( bj)∞
j=1to approximate the parameter. In
this case, ˆhℓℓwill be a Lasso or Ridge Regression estimator and we can estimate the derivative of h0
as we have estimated the derivative of α02. Moreover, if estimating h0is a low dimensional problem,
e.g., as in the partially linear model, we can often take ˆhℓℓas a Kernel or a Local Linear Regression
estimator. Then, the derivatives of h0can be estimated by finding the analytical expression of the
derivatives of the kernel function.
For a general ML estimator ˆhℓℓ′(e.g., Random Forest), we propose a numerical derivative ap-
proach to estimate ∂h0/∂v. Let tnbe a tuning parameter depending on the sample size with tn↓0.
We propose to estimate ∂h0(x, v)/∂vby
∂ˆhℓℓ′
∂v(x, v)≡ˆhℓℓ′(x, v+tn)−ˆhℓℓ′(x, v)
tn. (4.9)
Note that, usually, we need to compute the derivative evaluated at ( Xi, φ(Di, Zi,ˆgℓℓ′)).
25We have now seen all the difficulties in estimating D1ℓin equation (4.8). With these solved,
we can proceed to construct and automatic estimator of the First Step nuisance parameter. The
estimator is given by ˆ α1ℓ=c′
KˆβKℓ, where
ˆβKℓ= argmin
βK∈RKn
−2ˆD′
1ℓβK+β′
KˆCℓβK+λ∥βK∥q
qo
. (4.10)
We illustrate this procedure by constructing an automatic estimator of the First Step nuisance
parameter for the CASF:
Example 1(continuing from p. 6) From the previous discussion, we have that:
D1(w, g) =
D11(d, z) +∂h0
∂v(x, v)α02(x, v)
g(z),with
D11(d, z)≡ −Z∂h0
∂v(x∗, d−g0(z))dF∗(x∗).
We approximate D11by numerical integration. Let ( X∗
s)S
s=1be a sample from F∗. To estimate
D1ℓ, we approximate D11(Di, Zi), with i∈Iℓ′, by
−1
SSX
s=1∂ˆhℓℓ′
∂v(X∗
s, Di−ˆgℓℓ′(Zi)),
where, ∂ˆhℓℓ′(x, v)/∂v= (∂bJ/∂v)′ˆηℓℓ′. The parameters ˆηℓℓ′are Lasso cross-fitted slope estimates
for the second step h0. These estimates can incorporate semiparametric restrictions such as those
of a partly linear model by a suitable choice of the dictionary.
To estimate D1ℓ, it remains to show how to estimate the second term in the brackets, for an
observation i∈Iℓ′. Being Viℓℓ′≡Di−ˆgℓℓ′(Zi), we can estimate the second term by
bJ(Xi,ˆViℓℓ′)′ˆρJℓℓ′·∂ˆhℓℓ′
∂v(Xi,ˆViℓℓ′).
Therefore, to estimate D1ℓaccording to equation (4.8), we have that, for i∈Iℓ′,
D1(Wi, ck|ˆgℓℓ′,ˆhℓℓ′,ˆα2ℓℓ′,˜θℓℓ′) =ck(Zi)·(
−1
SSX
s=1∂ˆhℓℓ′
∂v(X∗
s, Viℓℓ′)
+bJ(Xi,ˆViℓℓ′)′ˆρJℓℓ′·∂ˆhℓℓ′
∂v(Xi,ˆViℓℓ′))
,
for each k= 1, ..., K . This can then be used to construct the objective function to estimate ˆβKℓ.
■
264.2 Estimation Algorithm
Here we provide an illustration of our estimation algorithm. The inputs to the algorithm are cross-
fitted estimators of g0andh0. An initial estimator of θ0must also be supplied. We note that
one must provide a total of Lestimators (ˆ gℓ,ˆhℓ,˜θℓ) only using observations not in Iℓ,L(L−1)/2
estimators (ˆ gℓℓ′,ˆhℓℓ′,˜θℓℓ′) only using observations not in Iℓ∪Iℓ′, and L(L−1)(L−2)/6 estimators
(ˆgℓℓ′ℓ′′,ˆhℓℓ′ℓ′′,˜θℓℓ′ℓ′′) only using observations not in Iℓ∪Iℓ′∪Iℓ′′.
Figure 2 provides a diagram showing how to compute the moment condition ˆψiℓ(θ) for an
observation i∈Iℓ. The debiased moment condition is given in equation (4.2). To this equation, the
diagram below adds the discussion in the above section, i.e., how to construct automatic estimators
of the nuisance parameters ( α01andα02) in the correction term. The arrows in the diagram indicate
how to estimate each term.
Once the debiased moment condition ˆψiℓis built, Automatic Debiased GMM estimation is
conducted with the objective function in equation (4.3).
5 Asymptotic theory
This section gives general conditions for asymptotic normality of the automatic debiased GMM
and conditions for consistent estimation of its asymptotic variance. The conditions are based on
the mean-square consistency, small interaction of estimation biases, and locally robust conditions
discussed in CEINR. Furthermore, estimation rates for the nuisance parameters ( α01, α02) require
(i) that the dictionaries approximate well the nuisance parameters and (ii) being able to estimate
the linear approximations of ¯ m(g, h, θ ) given by E[D1(W, g)] and E[D2(W, h)] at a certain rate (see
Chernozhukov et al., 2022b).
In the presence of generated regressors, the theory needs to account for the fact that the estimator
of the correction term (and probably that of the moment condition) evaluates the estimators ˆhℓand
ˆα2ℓin the generated regressor ˆViℓ≡φ(Di, Zi,ˆgℓ) (c.f., equation (4.2)). We modify the expansion of
ˆψiℓ(θ0)−ψ(Wi, g0, h0, α0, θ0) given by CEINR to deal with this fact. Also, we rely on smoothness
conditions on the dictionaries and φto ensure that evaluating at the generated regressor is not
problematic.
We begin with assumptions on the dictionaries. The first assumption formally states that the
dictionaries ( bn)∞
j=1and ( ck)∞
k=1span ∆ 2(g0) and ∆ 1, respectively.2
Assumption 5.1
a.For every j,bj∈∆2(g0). Also, ∀δ2∈∆2(g0) and for every ε >0, there exist JandρJsuch
that∥δ2−b′
JρJ∥2< ε.
2In this section, for a measurable function f,∥f∥2≡p
E[f(W)2] denotes its L2-norm. Also, for a d1×d2matrix
A= (Ai,j)d1,d2
i=1,j=1,∥A∥∞≡maxi,j|Aij|.
27ˆψiℓ(θ)≡m(Wi,ˆgℓ,ˆhℓ, θ)+ˆα1ℓ(Zi)·(Di−ˆgℓ(Zi)))+ ˆα2ℓ(Xi,ˆViℓ)·(Yi−ˆhℓ(Xi,ˆViℓ))=b′
JˆρJℓ +λ∥ρJ∥q
q} ρ′
JˆBℓρJ+ −2ˆD′
2ℓρJargminρJ∈RJ{Average on ℓ′̸=ℓ,s∈Iℓ′, of
D2(Ws,bJ|ˆgℓℓ′,ˆhℓℓ′,˜θℓℓ′)Average on ℓ′̸=ℓ,s∈Iℓ′, of
bJ(Xs, φ(Ds, Zs,ˆgℓℓ′))bJ(Xs, φ(Ds, Zs,ˆgℓℓ′))′
=c′
KˆβKℓargminβK∈RK{−2ˆD′
1ℓβK+β′
KˆCℓβK+λ∥βK∥q
q}
Average on ℓ′̸=ℓ,s∈Iℓ′, of
D1(Ws,cK|ˆgℓℓ′,ˆhℓℓ′,ˆα2ℓℓ′,˜θℓℓ′)Average on ℓ′̸=ℓ,s∈Iℓ′, of
cK(Zs)cK(Zs)′·DφcK (y−ˆhℓℓ′(Xs, v))] ˆα2ℓℓ′(Xs, v)∂
∂v[ D11(Ws,cK)+Evaluated at
v=φ(Ds, Zs,ˆgℓℓ′)
=b′
JˆρJℓℓ′ argminρJ∈RJ{−2ˆD′
2ℓℓ′ρJ+ρ′
JˆBℓℓρJ+λ∥ρJ∥q
q}
Average on ℓ′′/∈ {ℓ, ℓ′},ι∈Iℓ′′, of
D2(Wι,bJ|ˆgℓℓ′ℓ′′,ˆhℓℓ′ℓ′′,˜θℓℓ′ℓ′′)Average on ℓ′′/∈ {ℓ, ℓ′},s∈Iℓ′′, of
bJ(Xι, φ(Dι, Zι,ˆgℓℓ′ℓ′′))bJ(Xι, φ(Dι, Zι,ˆgℓℓ′ℓ′′))′
Figure 2Illustration of the algorithm to estimate the moment condition ˆψiℓfor an observation i∈Iℓ.
28b.For every k,ck∈∆1. Also, ∀δ1∈∆1and for every ε >0, there exist KandβKsuch that
∥δ1−c′
KβK∥2< ε.
We also assume bounded dictionaries (see, for instance, Newey, 1997):
Assumption 5.2 supj∈N|bj(X, V)|<∞and supk∈N|ck(Z)|<∞almost surely.
The assumption translates into consistency of ˆBℓand ˆCℓ. Also, on top of the following assump-
tion, it will guarantee that the correction term nuisance parameters are bounded:
Assumption 5.3 For the real-valued sequences ( ρ0j)∞
j=1and ( β0k)∞
k=1such that α02(x, v) =P∞
j=1ρ0jbj(x, v) and α01(z) =P∞
k=1β0kck(z):
a.P∞
j=1|ρ0j|<∞andP∞
k=1|β0k|<∞.
b.For a C >0, the atoms bjandckcorresponding to the largest C√nvalues of ρ0jandβ0kare
included in bJandcK.
This assumption keeps the L1-norm of the coefficient of the Lasso penalized regression under
control. The result is relevant to estimate the asymptotic variance (see Chernozhukov et al., 2022b)
and to evaluate the estimators at the generated regressor. We also note that the absolute summa-
bility of the coefficients imposes a sparsity condition on the relevant terms to approximate α01and
α02(see Chernozhukov et al., 2022b, p. 985).
We require the following estimation rates:
Assumption 5.4 There is 1 /3< r < 1/2 such that
a.∥ˆgℓ−g0∥2=Op(n−r) and∥ˆhℓ−h0∥2=Op(n−r).
b.∥ˆD1ℓ−E[D1(W,cK)]∥∞=Op(n−r) and∥ˆD2ℓ−E[D2(W,bJ)]∥∞=Op(n−r).
This assumption imposes standard rate conditions on the estimators of the nuisance parameters
and on the linearization of the moment condition (for Lasso rates, see Bickel et al., 2009; Bunea
et al., 2007; Zhang and Huang, 2008, and references therein). Under some regularity conditions on
the linearizations (see Chernozhukov et al., 2022b, Ass. 12) and a condition allowing evaluation at
the generated regressor (see Assumption 5.9 below), Assumption 5.4.b can be derived from the rate
conditions on the estimators of the nuisance parameters.
We also ask for the following rates for the Lasso penalty and the number of terms in the
dictionaries:
Assumption 5.5
a.The Lasso penalty term λ=λ(n) for estimation of ( α01, α02) satisfies: n−r=o(λ) and
λ=o(nc−r) for every c >0.
29b.The number of terms in the dictionaries satisfy J, K =O(nκ) for a constant κ >0.
This assumption asks for the Lasso penalty to go to zero slightly slower than n−r. For instance,
a rate of log( n)/nris allowed. Moreover, it requires polynomial rates in the growth of the number
of terms in the dictionaries.
The above are general conditions imposed on the dictionaries and the tuning parameters for
the Lasso penalized regression. The specific problem at hand only appears in two instances. First,
Assumption 5.1 requires that the dictionaries approximate well the correction term nuisance param-
eters (living in ∆ 1and ∆ 2(g0), respectively). Second, Assumption 5.4 requires (i) mean-square rates
for the estimators of g0andh0and (ii) to be able to estimate the linearizations at the same rate. As
discussed before, these conditions provide rates of estimators of the nuisance parameters in correc-
tion terms α01andα02(see Chernozhukov et al., 2022b). For instance, the convergence rate of ˆ α1ℓ
will be fast enough to guarantee that the interaction term satisfies ∥ˆα1ℓ−α01∥2·∥ˆgℓ−g0∥2=op(n−1/2)
(c.f. Assumption 2 in CEINR).
We now provide assumptions on the moment condition. The first is a mean-square consistency
condition similar to Assumption 1 in CEINR:
Assumption 5.6
a.E[m(W, g 0, h0, θ0)2]<∞.
b.R
[m(w,ˆgℓ,ˆhℓ, θ0)−m(w, g 0, h0, θ0)]2dF0(w)P− →0.
c.R
[m(w,ˆgℓ,ˆhℓ,˜θℓ)−m(w,ˆgℓ,ˆhℓ, θ0)]2dF0(w)P− →0.
d.E[(Y−h0(X, V))2|X, V] andE[(D−g0(Z))2|Z] are bounded almost surely.
Assumption 5.6.a is necessary for regular estimation of θ0. Assumptions 5.6.b and 5.6.c are mean-
square consistency conditions for the moment condition. Boundedness of the conditional variances
(Assumption 5.6.d) easily translates into mean-square consistency conditions for the correction
term ϕ. We repeat here that boundedness of the correction term nuisance parameters α01andα02
is implied by Assumptions 5.2 and 5.3.a.
We require the linear approximation of ¯ m(g, h, θ 0) to be good enough (in a neighborhood of
(g0, h0)):
Assumption 5.7 There is a ε >0 and a C > 0 such that, if ∥g−g0∥2< εand∥h−h0∥2< ε,
then
|E[m(W, g, h, θ 0)−m(W, g 0, h0, θ0)−D1(W, g−g0)−D2(W, h−h0)]|
≤C 
∥g−g0∥2
2+∥h−h0∥2
2
.
This Assumption translates into the Locally Robust property in Assumption 3.iii in CEINR. It
asks that, once we have removed the first-order effect of estimating ( g0, h0), the remainder term
must be at most quadratic.
30The GMM procedure requires consistent estimation of the Jacobian of the moment condition.
Being the following assumption specific to the GMM procedure, it is stated for the case with an
arbitrary number of parameters and moment conditions.
Assumption 5.8 There exists a neighborhood Nofθ0such that, for small ∥g−g0∥2and∥h−h0∥2:
a.m(W, g, h, θ ) is almost surely differentiable in N.
b.There exists a C >0 and a function d(W, g, h ), with E[d(W, g, h )]< C, such that for θ∈ N
∂m
∂θ(W, g, h, θ )−∂m
∂θ(W, g, h, θ 0)
∞≤d(W, g, h )∥θ−θ0∥1/C
∞almost surely .
Moreover, we assume that:
c.M, the expectation of the Jacobian, exists.
d.It holds that Z∂m
∂θ(w,ˆgℓ,ˆhℓ, θ0)−∂m
∂θ(w, g 0, h0, θ0)
∞dF0(w)P− →0.
We conclude the set of assumptions with smoothness conditions on the dictionary ( bj)∞
j=1and
the second step regression h0. The following assumption allows us to deal with ˆhℓand ˆα2ℓbeing
evaluated at the generated regressor.
Assumption 5.9
a.h0also satisfies Assumption 5.3.
b.supj∈N|∂bj(X, V)/∂v|<∞almost surely.
c.There exists a ε >0 such that, for all g∈∆1and∥g−g0∥< ε,bj∈∆2(g) for every j∈N.
In Theorem 5.1, we give asymptotic normality of the automatic debiased GMM estimator when
ˆhℓis a Lasso penalized regression of YontobJ(X, V) (see Section 4). For other estimators of the
second step nuisance parameters, Assumption 5.9.a may be replaced by ∥˜hℓ−h0∥2=Op(∥ˆhℓ−h0∥2),
where ˜hℓ(w)≡ˆhℓ(x, φ(d, z,ˆgℓ)). Assumption 5.9.b allows us to bound the effect of departures from
evaluation at the “true” generated regressor V≡φ(D, Z, g 0). Assumption 5.9.c simply requires
that the dictionary is valid for small deviations in the generated regressor.
Assumptions 5.3, 5.4, 5.6, and 5.7 are stated for a single moment condition. In the presence
of more than one condition, they must be understood to hold componentwise. The same happens
with Assumptions 3.1, 3.2, and 3.4 in Section 3.1. Assumption 5.8, since it refers to a GMM-specific
situation, is already formulated in the general case. The remaining assumptions do not depend on
the dimension of the moment condition (they depend, on the other hand, on the dimension of Y
andD).
31Recall that Ξ ≡(M′ΥM)−1M′Υ′ΨΥM(M′ΥM)−1gives the asymptotic variance of the au-
tomatic debiased GMM estimator. Define ˆΞ≡(ˆM′ˆΥˆM)−1ˆM′ˆΥ′ˆΨˆΥˆM(ˆM′ˆΥˆM)−1as the plug-in
estimator of the asymptotic variance, where ˆMandˆΨ are given in equations (4.4) and (4.5), re-
spectively. The following theorem ensures asymptotic normality of√n(ˆθ−θ0):
Theorem 5.1 Consider that Assumptions 3.1-3.4 and 5.1-5.9 are satisfied, ˆΥP− →Υ,M′ΥMis
non-singular, and ˆhℓare Lasso estimators of h0. Then, the automatic debiased GMM estimator in
equation (4.3) satisfies√n(ˆθ−θ0)D− →N(0,Ξ).
Moreover, the plug-in estimator for the asymptotic variance is consistent: ˆΞP− →Ξ.
We conclude the section by giving sufficient conditions to apply the above theorem to the CASF:
Example 1(continuing from p. 6) Here we verify assumptions 5.6, 5.7, and 5.8. These are
the assumptions that depend on the identifying moment condition for the parameter of interest. In
the case of the CASF, the moment condition is
m(w, g, h, θ ) =Z
h(x∗, d−g0(z))dF∗(x∗)−θ.
We also provide sufficient conditions for Assumptions 3.1 and 3.2 required for linearizing the
moment condition. Recall that the assumption require
E[D11(w, g)] =E
−Z∂h0
∂v(x∗, V)dF∗(x∗)·g(Z)
and
E[D2(w, h)] =EZ
h(x∗, V)dF∗(x∗)
=Ef∗(X)fv
0(V)
fxv
0(X, V)·h(X, V)
to be L2-continuous. To achieve this, we require some regularity on the distribution of ( X, V) and
on the dependence of the outcome Yon (X, V).
Assumption 5.10
a.α02(X, V) is bounded almost surely and E[Y2]<∞.
b.For almost every x,h0andα02are twice continuously differentiable with respect to v. More-
over, ∂h/∂v (X, V),∂α02/∂v(X, V),∂2h/∂v2(X, V), and ∂2α02/∂v2(X, V) are bounded almost
surely.
Assumption 5.10.a is related to regular identification of the CASF (i.e., the asumption guar-
antees that the necessary condition for regular identification in P´ erez-Izquierdo, 2022, is satisfied).
Assumption 5.10.b implies continuity of the linearization with respect to g. We can then show that
the assumptions for Theorem 5.1 hold for the moment condition defining the CASF.
32Pr oposition 5.1 Suppose that Assumptions 5.3, 5.4.a, and 5.9. Then Assumption 5.10 guar-
antees that Assumptions 3.1, 3.2, 5.6.a-5.6.c, 5.7, and 5.8 are satisfied for the moment condition
identifying the CASF.
■
6 Monte Carlo simulation
This section describes the Monte Carlo simulation to evaluate the finite sample properties of the
CASF estimator proposed in this paper. Before presenting the results, we briefly describe the Data
Generating Process (DGP) and the implemented estimators.
6.1 Description
The DGP is
(Z, U, V )∼N
0,
Id60 0
0 1 1 /2
0 1 /2 1

,
where Id 6denotes the 6 ×6 Identity Matrix. Therefore, Zis a 6-dimensional random vector. The
correlation between UandVis 1/2. Note that the fact that Z⊥UandZ⊥Vguarantees that
the Control Function Assumption is satisfied. Here X= (D, Z 1, ..., Z 5).
Both DandYare generated by the following linear models:
Y=5X
k=1Zk+ 2D+Uand
D=6X
k=1Zk+V.
SoZ6is excluded from the structural equation (i.e., it does not directly affect Y) and may be used
as an instrument.
We estimate the CASF for the following counterfactual distribution F∗
X: (i) the distribution
of (Z1, . . . , Z 5) remains unchanged and (ii) Dis normal with mean 1 (instead of 0) and the same
variance as in the DGP. Therefore, the true parameter is θ0= 2, the first step is g0(z) =P6
k=1zk,
and the second step is h0(x, v) =P5
k=1zk+ 2d+v/2.
We note here that, even if the model considered is linear, the second-step correction nuisance
parameter is highly non-linear. Letting s≡P5
k=1zk, the Riesz representer is
r2(z1, . . . , z 5, d, v) =C·exp
−1
4−s
2+d
2+s2
4+v2
2+d2
4−sd
2+sv−dv
,
33for a constant C. The function α02is the orthogonal projection of r2onto ∆ 2(g0).
We display results for three different estimators of the CASF:
•The naive plug-in estimator: ˆθPI≡n−1Pn
i=1m(Wi,ˆg,ˆh), where mis given in equation (2.3).
•A cross-fitted Doubly Robust debiased estimator that only corrects for the effect of pluging-
inˆh:ˆθDRis as in equation (4.6) but with ϕ(Wi,ˆgℓ,ˆhℓ,ˆαℓ,˜θℓ) replaced by ˆ α2ℓ(Xi,ˆViℓ)·(Yi−
ˆhℓ(Xi,ˆViℓ)). That is, the correction term for the first step is omitted.
•The cross-fitted fully Locally Robust debiased estimator: ˆθLRas in equation (4.6). That is,
the estimator is based on the fully debiased moment condition in equation (4.2).
Numerical integration, with a sample of size S= 107, is used to compute the integrals w.r.t. F∗
X.
The estimators for the nuisance parameters g0,h0,α01, and α02are Lasso with three dictionaries:
one that includes linear terms, another including linear and quadratic terms, and a last one including
linear, quadratic, and interaction terms. The number of splits for cross-fitting is L= 5 for every
sample size.
To perform inference with each estimator, we present results that parallel common practice.
The fully debiased estimator uses the correct asymptotic variance, the one accounting for first and
second step estimation. This is given by equation (4.5). The estimator ˆθDRonly accounts for the
second step when computing the asymptotic variance (as it does for estimation). Its asymptotic
variance can be constructed by replacing ϕ(Wi,ˆgℓ,ˆhℓ,ˆαℓ,˜θℓ) by ˆ α2ℓ(Xi,ˆViℓ)·(Yi−ˆhℓ(Xi,ˆViℓ)) in
the second step IF. To emphasize that plug-in estimation leads to an asymptotic bias problem,
confidence intervals for the plug-in estimator are built with correct asymptotic variance (the one in
equation (4.5)).
6.2 Results
The next tables report results for a Monte Carlo simulation with B= 1098 replications. Each table
gives results for a different dictionary: linear, quadratic or the one which also includes interaction
terms.
Tables 1 and 2 present results for the linear and quadratic dictionaries, respectively. Correcting
for the second step already reduces a large amount of the bias of the plug-in estimator. Adding
the first-step correction further decreases bias. As shown in the tables, however, the estimator
accounting only for the second step fails to keep coverage at the nominal 95% level as the sample
size increases.
The tables highlight that the plug-in estimator suffers from severe asymptotic bias issues: cov-
erage decreases rapidly, even if the confidence interval is constructed with correct standard errors.
Indeed, Figure 3 shows that, for a sample size of n= 10000, the distribution of plug-in estimators
has almost zero mass near the true parameter θ0= 2.
34Mean Absolute Bias Standard Error Coverage (95%)
n PI DR LR PI DR LR PI DR LR
100 0.2285 0.1463 0.1482 0.1649 0.1812 0.1733 0.6388 0.8681 0.8626
500 0.1425 0.0594 0.0516 0.0685 0.0692 0.0645 0.3876 0.8954 0.9208
1000 0.1236 0.0435 0.0376 0.049 0.0488 0.0462 0.2266 0.8744 0.9272
5000 0.0852 0.0234 0.0169 0.0227 0.0212 0.0202 0.0227 0.7925 0.9290
10000 0.0746 0.0181 0.0123 0.017 0.0148 0.0142 0.0018 0.7489 0.9163
T able 1CASF results for the dictionary including linear terms.
Figure 3Distribution of the CASF estimators using a quadratic dictionary for a sample size of n=
10000.
Mean Absolute Bias Standard Error Coverage (95%)
n PI DR LR PI DR LR PI DR LR
100 0.2764 0.1898 0.1957 0.1766 0.2003 0.2 0.5532 0.7534 0.7561
500 0.1462 0.0603 0.0527 0.0692 0.0709 0.0663 0.3794 0.8963 0.9327
1000 0.1214 0.0446 0.0374 0.0491 0.0488 0.0465 0.2329 0.8717 0.929
5000 0.079 0.0259 0.0161 0.0236 0.0212 0.0202 0.0437 0.737 0.9354
10000 0.0694 0.0209 0.0115 0.0172 0.0148 0.0143 0.0036 0.6533 0.9327
T able 2CASF results for the dictionary including linear and quadratic terms.
Table 3 displays results for the dictionary that also includes interaction terms. The results are
striking, as the Doubly Robust estimator that only accounts for the second step performs well. It is
able to keep coverage at nominal levels, outperforming the fully debiased estimator. Nevertheless,
the decrease in coverage is small: 1-2% for intermediate sample sizes ( n= 500 and 1000) and
4-5% for large samples ( n= 5000 and 10000). We believe that this fact rests on the dictionaries
performing well to estimate the second step correction, but notably worst to estimate the more
complex fist step correction. This result suggest that the complexity of the dictionary must be
increased faster when accounting for the first step.
35Mean Absolute Bias Standard Error Coverage (95%)
n PI DR LR PI DR LR PI DR LR
100 0.3447 0.3583 0.3857 0.179 0.3606 0.4054 0.9016 0.7969 0.806
500 0.1707 0.0997 0.1085 0.0693 0.1213 0.1359 0.7925 0.9481 0.9227
1000 0.1367 0.0635 0.0674 0.0488 0.0776 0.0849 0.5883 0.9372 0.9262
5000 0.0846 0.0259 0.0283 0.0229 0.0312 0.0349 0.1383 0.9372 0.8926
10000 0.0729 0.0173 0.0205 0.017 0.0207 0.0228 0.0337 0.9399 0.8926
T able 3CASF results for the dictionary including linear, quadratic, and interaction terms.
7 Conclusion
We propose Automatic Locally Robust estimators for structural parameters in the presence of
generated regressors. We show that the debiasing correction term can be decomposed into terms
accounting for first step and second step estimation. Each of the first and second step IF depends
on an additional nuisance parameter, which can be automatically estimated (i.e., estimated without
finding their analytic shape).
We apply our results to construct Automatic Locally Robust estimators for the CASF under the
control function assumption, see also Appendix A for the Average Partial Effects example in sample
selection models. The analytic shape of the nuisance parameters in these two cases is particularly
complex, as the moment conditions depend on the whole shape of the second step parameter (not
only its pointwise value). Therefore, automatic estimation is particularly suited for these problems.
We have shown that commonly used plug-in methods lead to highly biased inferences for the CASF.
Doubly Robust and fully Locally Robust estimators correct the bias and deliver much more accurate
inference in a complex setting with generated regressors.
36Appendix A Partial effects in sample selection models
Example 3We observe W= (Y, D, Z ) following the model Y=Y∗D≡H(X, ε)D, where X
is a component of Z, and we do not observe Y∗when D= 0. This is a very general setting
for sample selection models. We do not know much about the selection, so this is given by D=
1 [g0(Z)−U≥0], where Uis uniformly distributed in [0 ,1] . The unobserved errors εandU,
though independent of Z, are correlated with each other (selection on unobservables). In this
example, V=g0(Z) =E[D|Z]. Then, it can be shown that
E[Y|Z] =E[H(X, ε)1 [g0(Z)−U≥0]|Z]
=h0(X, V).
This setting provides a nonparametric extension of the classical model of Heckman (1979), where
H(X, ε) =X′β0+ε, g0(Z) =Z′γ0, and the joint distribution of ( ε, U) is bivariate Gaussian.
As a parameter of interest consider the Average Partial Effects (APE) given, for simplicity of
presentation for a one-dimensional continuous regressor, by
θ0=E∂h0
∂x(X, V)
.
The moment function identifying the APE is
m(w, g, h, θ ) =∂
∂sh(s, g(z))
s=x−θ.
This parameter is covered by Proposition 5 in Hahn and Ridder (2019). However, the authors do
not consider Locally Robust estimation. Here we propose a novel Locally Robust estimator for the
APE which (i) is Doubly Robust to the second step and (ii) allows for ML first and second step
estimators.
Let∂h(x, v)/∂xdenote the derivative of h(x, v) w.r.t. its first argument at ( x, v). Let ∂2h(x, v)/∂x∂v
denote the derivative w.r.t. both arguments at ( x, v). For the APE, we have that the moment func-
tion is linear in h. Thus:
D2(w, h|g0, h0, θ) =∂h
∂x(x, g0(z)),
where we have already make explicit the dependence of D2on (g0, h0, θ). We can also linearize the
moment condition in gto obtain:
D1(w, g|g0, h0, α0, θ) =
D11(d, z) +∂
∂v[α02(x, v)(y−h0(x, v))]
g(z),with
D11(d, z)≡∂2h0
∂x∂v(x, g0(z)).
The debias estimator for the APE is
ˆθ=1
nLX
ℓ=1X
i∈Iℓ∂ˆhℓ
∂x(Xi,ˆgℓ(Zi)) +ˆϕ,
37where, for the estimator ˆhℓ, we can estimate its derivative w.r.t. xby
ˆhℓ
∂x(x, v)≡ˆhℓ(x+sn, v)−ˆhℓ(x, v)
sn,
for a tuning parameter sn. Alternatively, we can take advantage of a differentiable dictionary
(bj(x, v))∞
j=1, as described below.
To construct ˆϕ, we need to estimate α01andα02. We propose automatic estimators for these
nuisance parameters. We assume that ∂h0/∂xand∂h0/∂vare differentiable, so we can interchange
the order of differentiation.
Consider a dictionary ( bj)∞
j=1that is differentiable w.r.t. both xandv. To Estimate ˆD2ℓ, we
can compute D2(Wi, bj|ˆgℓℓ′,ˆhℓℓ′,˜θℓℓ′) for an observation i∈Iℓ′by
∂bj
∂x(Xi,ˆgℓℓ′(Zi)).
This derivative can be found analytically for each atom. We can use this to obtain an automatic
estimator of α02.
To construct ˆD1ℓ, we need to estimate D1(Wi, ck|ˆgℓℓ′,ˆhℓℓ′,ˆα2ℓℓ′,˜θℓℓ′) for an observation i∈Iℓ′
and an arbitrary atom ckin a dictionary. The first term, D11(Di, Zi), can be estimated by
∂2bJ
∂x∂v(Xi,ˆgℓℓ′(Zi))′
ˆηJ,
in case that h0(x, v) is approximated by bJ(x, v)′ˆηJ. An estimator of the second term, ∂[α02(x, v)·
(y−h0(x, v)]/∂v, is
∂bJ
∂v(Xi,ˆgℓℓ′(Zi))′ˆρJ·[Yi−bJ(Xi,ˆgℓℓ′(Zi))′ˆηJℓℓ′]−bJ(Xi,ˆgℓℓ′(Zi))′ˆρJℓℓ′·∂bJ
∂v(Xi,ˆgℓℓ′(Zi))′ˆηJ
■
38Appendix B Proofs of the results
Pr oof of Pr oposition 3.1: For the (differentiable) path τ7→h(Fτ, g0), Assumption 3.1 implies
d
dτ¯m(g0, h(Fτ, g0), θ) =d
dτE[D2(W, h(Fτ, g0)].
This gives the linearization (LIN) .
To find the shape of the IF, note that E[D2(W, h)] is a linear and continuous functional in
L2(X, V), a Hilbert space of square-integrable functions. Thus, by the Riesz Representation Theo-
rem, there exists a r2such that E[D2(W, h)] =E[r2(X, V)h(X, V)], with V≡φ(D, Z, g 0). Therefore:
d
dτ¯m(g0, h(Fτ, g0), θ) =d
dτE[r2(X, V)h(Fτ, g0)(X, V)],
where h(F, g)(x, v) denotes h(F, g) evaluated at ( x, v). This is Assumption 1 in Ichimura and Newey
(2022). Since Assumption 2 in that paper is satisfied in our setup, Proposition 1 in Ichimura and
Newey (2022) gives: ϕ2(w, h 0, α02, θ) =α02(d, z){y−h0(x, φ(d, z, g 0))}. The parameter α02is the
L2-projection of r2onto ∆ 2(g0):
α02= argmin
α∈∆2(g0)E[(r2(X, φ(D, Z, g 0))−α(D, Z))2]. (B.1)
This gives Point (IF). ■
Pr oof of Lemma 3.1: We proceed as in Hahn and Ridder (2013, Lma. 1). Let τ7→gτbe a
differentiable path. For any function δ2∈∆2(gτ), we have that
E[δ2(X, V(gτ))· {Y−h(F0, gτ)(X, V(gτ))}] = 0.
This is the orthogonality condition that defines h(F0, gτ) (it is equation (2.4) for ( F, g) = (F0, gτ)).
Ifδ2∈∆2(gτ) when τ < ε , we can take derivatives in the above equation. Thus, applying the chain
rule, we get
d
dτE[δ2(X, V)·h(F0, gτ)(X, V)] =−d
dτE[δ2(X, V)·h0(X, V(gτ))]
+d
dτE[δ2(X, V(gτ))·(Y−h0(X, V))].
■
Pr oof of Theorem 3.1: We compute d¯m(g(Fτ), h0, θ)/dτandd¯m(g0, h(F0, g(Fτ)), θ)/dτsepa-
rately and then add them according to equation (3.2). By Assumptions 3.1 and 3.2, using the Riesz
Representation Theorem, we have that for the differentiable paths τ7→g(Fτ) and τ7→h(F0, g(Fτ)):
d
dτ¯m(g(Fτ), h0, θ) =d
dτE[D11(W, g(Fτ))] =d
dτE[r1(Z)g(Fτ)(Z)] (B.2)
39and, being V≡φ(D, Z, g 0),
d
dτ¯m(g0, h(F0, g(Fτ)), θ) =d
dτE[D2(W, h(F0, g(Fτ)))] =d
dτE[r2(X, V)h(F0, g(Fτ))(X, V)].
In these equations, g(F)(z) means g(F) evaluated at z, and h(F, g)(x, v) means h(F, g) evaluated
at (x, v). By Assumption 3.3.b, h(F0, g(Fτ))∈∆2(g0). This means that h(F0, g(Fτ)) is orthogonal
tor2−α02(since α02is the L2-projection of r2onto ∆ 2(g0)). Then, we can write:
d
dτ¯m(g0, h(F0, g(Fτ)), θ) =d
dτE[α02(X, V)h(F0, g(Fτ))(X, V)]. (B.3)
By Assumption 3.3.a we can apply Lemma 3.1 to the RHS of equation (B.3) to get:
d
dτ¯m(g0, h(F0, g(Fτ)), θ) =d
dτE[α02(X, V)h(F0, g(Fτ))(X, V)]
=−d
dτE[α02(X, V)h0(X, φ(D, Z, g (Fτ)))]
+d
dτE[α02(X, φ(D, Z, g (Fτ)))·(Y−h0(X, V))].(B.4)
Under Assumption 3.4, the term in the second row can be linearized in g(Fτ) as
d
dτE[α02(X, V)h0(X, φ(D, Z, g (Fτ)))] = Ed
dτ{α02(X, V)h0(X, φ(D, Z, g (Fτ)))}
=E
α02(X, V)∂h0
∂v(X, V)d
dτφ(D, Z, g (Fτ))
=E
α02(X, V)∂h0
∂v(X, V)d
dτDφg(Fτ)(D, Z)
=d
dτE
α02(X, V)∂h0
∂v(X, V)Dφg(Fτ)(D, Z)
,
where Dφg(d, z) denotes Dφgevaluated at ( d, z). We have assumed that derivatives and expectations
can be interchanged (we may impose some regularity conditions on Hsuch that this is possible).
We can equivalently linearize the term in the third row of equation (B.4) to get
d
dτE[α02(X, φ(D, Z, g (Fτ)))·(Y−h0(X, V))] =d
dτE
(Y−h0(X, V))∂α02
∂v(X, V)Dφg(Fτ)(D, Z)
.
Pluging in these results back in equation (B.4):
d
dτ¯m(g0, h(F0, g(Fτ)), θ) =d
dτE
−α02(X, V)∂h0
∂v(X, V)
+(Y−h0(X, V))∂α02
∂v(X, V)
Dφg(Fτ)(D, Z)
=E∂
∂v{α02(X, v)·(Y−h0(X, v))}
v=VDφg(Fτ)(D, Z)
.(B.5)
40Since Dφis linear in g, the function inside the expectation in the RHS is linear in g. We now use
equation (3.2) to combine the results in equations (B.2) and (B.5). This gives:
d
dτ¯m(g(Fτ), h(F0, g(Fτ)), θ) =d
dτE
D11(W, g(Fτ))
+∂
∂v{α02(X, v)·(Y−h0(X, v))}
v=VDφg(Fτ)(D, Z)
,
which gives the linearization result of the Theorem (LIN) .
To find the shape of the IF, note that the adjoint D∗
φofDφis defined by the equation
E[δ(D, Z)Dφg(D, Z)] =E[D∗
φδ(Z)g(Z)]. Therefore, by the Law of Iterated Expectations in equa-
tion (B.5), noting that V≡φ(D, Z, g 0) is a function of ( D, Z):
d
dτ¯m(g0, h(F0, g(Fτ)), θ) =d
dτE
E∂
∂v{α02(X, v)·(Y−h0(X, v))}
v=VDφg(Fτ)(D, Z)D, Z
=E[ν(D, Z)Dφg(Fτ)(D, Z)] =E[D∗
φν(Z)g(Fτ)(Z)],
with
ν(d, z)≡∂
∂v{α02(x, v)·(E[Y|D=d, Z=z]−h0(x, v))}
v=φ(d,z,g 0).
Again, we can use equation (3.2) to combine this last result with that in equation (B.2):
d
dτ¯m(g(Fτ), h(F0, g(Fτ)), θ) =d
dτE[{r1(Z) +D∗
φν(Z)}g(Fτ)(Z)].
This is Assumption 1 in Ichimura and Newey (2022). Since Assumption 2 in that paper is satisfied in
our setup, Proposition 1 in Ichimura and Newey (2022) gives the shape of the IF: ϕ1(w, g 0, α01, θ) =
α01(z)· {d−g0(z)}. The parameter α01is the L2-projection:
α01= argmin
α∈∆1E[(˜ν(Z)−α(Z))2], (B.6)
where ˜ ν=r1+D∗
φν. ■
Pr oof of Pr oposition 3.2: Start with the non-parametric case. Let v(g)≡φ(d, z, g ) By the
triangle inequality (applied to the L2(W)-norm):
Z
α02(x, v(gτ))2dF0(w)1/2
≤Z
α02(x, v(g0))2dF0(w)1/2
+Z
[α02(x, v(gτ))−α02(x, v(g0))]2dF0(w)1/2
The fist quantity in the RHS is finite since α02∈L2(X, V). For quantity in the second row, by the
MVT and Hadamard differentiability of φ:
Z
(α02(x, v(gτ))−α02(x, v))2dF0(w)1/2
≤C∥φ(gτ)−φ(g0)∥
=C∥φ(gτ)−φ(g0)−Dφ(gτ−g0) +Dφ(g−g0)∥ ≤C(∥Dφ∥ · ∥gτ−g0∥+o(∥gτ−g0∥)),
41where Cis the bound of ∂α02/∂vand, in some abuse of notation, ∥·∥denotes the norm in the
corresponding space.3Letεbe such that o(∥gτ−g0∥)≤ ∥gτ−g0∥. For τ < ε :
Z
(α02(x, v(gτ))−α02(x, v))2dF0(w)1/2
≤C(∥Dφ∥+ 1)∥gτ−g0∥,
which is finite since τ7→gτis a differentiable path in ∆ 1.
For the partly linear case, where α02(x, v) =β′
0x+κ0(v), simply note that
∂α02
∂v(x, v) =∂κ0
∂v(x, v).
Thus, ∂κ0/∂vis bounded and we can proceed as above to show thatR
κ0(v(gτ))2dF0(w) is finite. ■
Pr oof of Pr oposition 3.3: We start with the non-parametric case. Throughout the proof, we
callhτ≡h(F0, gτ). We note that hτ∈∆2(g0) =L2(X, V) if and only ifR
hτ(x, v)2dF0
xv(x, v)<∞.
Under the conditions in the statement of the proposition, by Cauchy-Schwarz’ inequality:
Z
hτ(x, v)2dF0
xv(x, v) =Z
hτ(x, v)2ντ(x, v)dFτ
xv(x, v)≤Z
hτ(x, v)4dFτ
xv(x, v)+Z
ντ(x, v)2dFτ
xv(x, v).
Furthermore, since hτ(X, V(gτ)) =E[Y|X, V(gτ)], by conditional Jensen’s inequality and the Law
of Iterated Expectations
Z
hτ(x, v)4dFτ
xv(x, v) =E[E[Y|X, V(gτ)]4]≤E[E[Y4|X, V(gτ)]] =E[Y4]<∞.
Also, since the Radon-Nikodym density of Fτ
xvw.r.t. F0
xvisντ(x, v)−1(Shao, 2003, Prop. 1.7):
Z
ντ(x, v)2dFτ
xv(x, v) =Z
ντ(x, v)dF0
xv(x, v) =E[ντ(X, V)]<∞.
For the partly linear case we need to show thatR
κτ(v)2dF0
v(v)<∞. We can proceed as above
to get: Z
κτ(v)2dF0
v(v)≤Z
κτ(v)4dFτ
v(v) +E[ντ(V)],
where the second quantity in the RHS is finite by assumption. In the partly linear model we have
thatE[Y|V(gτ)] =β′
τE[X|V(gτ)] +κτ(V(gτ)). Therefore,
Z
κτ(v)4dFτ
v(v) =E[κτ(V(gτ))4] =E[E[Y−β′
τX|V(gτ)]4]≤E[(Y−β′
τX)4].
This is finite if the expectation of the fourth-order cross-products between YandXis finite. ■
3Namely, ∥g−g0∥is the L2(Z)-norm of g−g0and∥Dφ∥is the strong norm of Dφin the space of continuous
linear functionals from L2(Z) toL2(D, Z).
42The asymptotic normality and consistent estimation of the asymptotic variance result in Theo-
rem 5.1 relies on the following lemma:
Lemma B.1 Consider Assumptions 3.4, 5.3, 5.4.a, and 5.9. Let r, ξ > 0. Then, for ˜hℓ(w)≡
ˆhℓ(x, φ(d, z,ˆgℓ))and˜α2ℓ(w)≡ˆα2ℓ(x, φ(d, z,ˆgℓ)):
∥ˆhℓ−h0∥2=Op(n−r)⇒ ∥˜hℓ−h0∥2=Op(n−r),and
∥ˆα2ℓ−α02∥2=op(nξn−r/2)⇒ ∥˜α2ℓ−α02∥2=op(nξn−r/2).
Pr oof of Lemma B.1: Recall that ˆhℓ(x, v)≡bJ(x, v)′ˆηJand ˆα2ℓ(x, v)≡bJ(x, v)′ˆρJ. We show
that
∥˜hℓ−ˆhℓ∥2=Op(n−r).
The conclusion for ˜ α2ℓfollows the same reasoning.
Let˜bj(w)≡bj(w, φ(d, z,ˆgℓ)). By the triangle inequality ∥ˆhℓ−ˆhℓ∥2≤PJ
j=1|ˆηj|∥˜bj−bj∥2.
Moreover, by the Mean Value Theorem and Assumption 5.9.b,
∥˜bj−bj∥2
2=Z
(bj(x, φ(d, z,ˆgℓ))−bj(x, v))2dF0(w)≤κ2∥φ(·,·,ˆgℓ)−φ(·,·, g0)∥2
2.
Then, supj≤J∥˜bj−bj∥2≤κ∥φ(·,·,ˆgℓ)−φ(·,·, g0)∥2. Also, since φis Hadamard differentiable
(Assumption 3.4): ∥φ(·,·,ˆgℓ)−φ(·,·, g0)−Dφ(ˆgℓ−g0)∥2=op(∥ˆgℓ−g0∥) and ∥Dφ(ˆgℓ−g0)∥2≤
C∥ˆgℓ−g0∥2(see Yamamuro, 1974, Result 1.2.6). Thus, by the triangle inequality:
sup
j≤J∥˜bj−bj∥2≤κC∥ˆgℓ−g0∥2+op(∥ˆgℓ−g0∥2).
Now, Assumption 5.3 allows us to apply Lemma A9 in Chernozhukov et al. (2022b) to getPJ
j=1|ˆηj|=Op(1). Therefore, if Assumption 5.4.a holds,
∥ˆhℓ−ˆhℓ∥2≤JX
j=1|ˆηj|∥˜bj−bj∥2≤ JX
j=1|ˆηj|!
(κC∥ˆgℓ−g0∥2+op(∥ˆgℓ−g0∥2))
=Op(1)·[Op(n−r) +op(n−r)] =Op(n−r).(B.7)
The conclusion for ∥˜hℓ−h0∥2follows directly from equation (B.7) and the triangle inequality.
The conclusion for ∥˜α2ℓ−α02∥2follows equally, taking into account that Op(n−r) =op(nξn−r/2) for
ξ >0. ■
Pr oof of Theorem 5.1: We start with asymptotic normality of√n(ˆθ−θ0). The proof follows
standard GMM techniques and Theorem 9 in Chernozhukov et al. (2022b). A relevant deviation
from the previous results is that the estimators are evaluated at the generated regressor. Lemma B.1
allows to deal with that situation.
43The cornerstone of the result is Lemma 8 in Chernozhukov et al. (2022a), CEINR in what
follows, which states that:
√nˆψ(θ0) =1√nnX
i=1ψ(Wi, g0, h0, α0, θ0) +op(1) (B.8)
under some conditions. We will apply Lemma 8 to a modified expansion of the difference be-
tween ˆψ(θ0) and n−1/2Pn
i=1ψ(Wi, g0, h0, α0, θ0) that allows to deal with estimators evaluated at the
generated regressor.
Consider first that equation (B.8) holds for each component of ˆψ. Consistency of ˆθfollows
under standard conditions that guarantee uniform convergence of ˆψ(θ)′ˆΥˆψ(θ) in Θ (c.f. Wooldridge,
2010, Th. 14.1). These conditions will follow from Assumption 5.8 if Θ is compact. Moreover, by
Assumptions 5.4.a and 5.8.a we can apply the Mean Value Theorem to get
√n
ˆψ(ˆθ)−ˆψ(θ0)
=√n∂ˆψ
∂θ(¯θ)·(ˆθ−θ0)
for¯θa point between θ0andˆθ(that is ¯θP− →θ0). Then, if equation (B.8) holds:
√n∂ˆψ
∂θ(¯θ)·(ˆθ−θ0) =1√nnX
i=1ψ(Wi, g0, h0, α0, θ0) +√nˆψ(ˆθ) +op(1). (B.9)
Now, note that
∂ˆψ
∂θ(θ) =∂
∂θ 
1
nLX
ℓ=1X
i∈Iℓh
m(Wi,ˆgℓ,ˆhℓ, θ) +ϕ(Wi,ˆgℓ,ˆhℓ,ˆαℓ,˜θℓ)i!
=1
nLX
ℓ=1X
i∈Iℓ∂m
∂θ(Wi,ˆgℓ,ˆhℓ, θ)
Then, since Assumptions 5.4.a and 5.8, on top of ˆθP− →θ0, guarantee that we can apply Lemma E2
in CEINR, we have that ∂ˆψ(ˆθ)/∂θP− →M, so it is bounded in probability. Therefore, since ˆΥ is also
Op(1), equation (B.9) implies
∂ˆψ
∂θ(ˆθ)′ˆΥ∂ˆψ
∂θ(¯θ)·√n(ˆθ−θ0) =∂ˆψ
∂θ(ˆθ)′ˆΥ·1√nnX
i=1ψ(Wi, g0, h0, α0, θ0)
+√n∂ˆψ
∂θ(ˆθ)′ˆΥˆψ(ˆθ) +op(1).
Thus, since ( ∂ˆψ(ˆθ)/∂θ)′ˆΥˆψ(ˆθ) = 0 is the first-order condition for the minimization problem in
equation (4.3), ∂ˆψ(¯θ)/∂θP− →M(by Lemma E2 in CEINR), and M′ΥMis non-sigular:
√n(ˆθ−θ0) = (M′ΥM)−11√nnX
i=1ψ(Wi, g0, h0, α0, θ0) +op(1).
44Then, the asymptotic normality result follows from n−1/2Pn
i=1ψ(Wi, g0, h0, α0, θ0)D− →N(0,Ψ).
It remains to verify the assumptions for Lemma 8 in CEINR, so that equation (B.8) holds
for each component of ˆψ. First, to handle estimators evaluated at the generated regressor, we
provide a modified expansion of the difference between ˆψ(θ0) and n−1/2Pn
i=1ψ(Wi, g0, h0, α0, θ0).
Let¯ϕ(w,¯v, g, h, α )≡α1(z)·(d−g(z)) + α2(x,¯v)·(y−h(x, φ(d, z, g ))), which makes explicity
that α2is evaluated at ( x,¯v). Then, being ˆψiℓ(θ) given by equation (4.2), we have that ˆψiℓ(θ0)−
ψ(Wi, g0, h0, α0, θ0) =ˆR1iℓ+ˆR2iℓ+ˆR3iℓ+ˆ∆iℓ, where
ˆR1iℓ≡m(Wi,ˆgℓ,ˆhℓ, θ0)−m(Wi, g0, h0, θ0),
ˆR2iℓ≡¯ϕ(Wi, φ(Di, Zi, g0),ˆgℓ,ˆhℓ, α0)−ϕ(Wi, g0, h0, α0, θ0),
ˆR3iℓ≡¯ϕ(Wi, φ(Di, Zi,ˆgℓ), g0, h0,ˆαℓ)−ϕ(Wi, g0, h0, α0, θ0),
ˆ∆iℓ≡ϕ(Wi,ˆgℓ,ˆhℓ,ˆαℓ,˜θℓ)−¯ϕ(Wi, φ(Di, Zi, g0),ˆgℓ,ˆhℓ, α0),and
−¯ϕ(Wi, φ(Di, Zi,ˆgℓ), g0, h0,ˆαℓ) +ϕ(Wi, g0, h0, α0, θ0).(B.10)
We will apply Lemma 8 in CEINR to this expansion.
Following Chernozhukov et al. (2022b), we begin by providing rates for estimation of α01and
α02. Assumption 5.2 allows us to apply Lemma A10 in Chernozhukov et al. (2022b) to get ∥ˆBℓ−
E[bJ(X, V)bJ(X, V)′]∥∞=Op(p
log(J)/n) and∥ˆCℓ−E[cK(Z)cK(Z)′]∥∞=Op(p
log(K)/n). The
fact that Assumption 5.5.b imposes a polynomial rate on JandKthen implies that Op(p
log(J)/n) =
Op(n−r) and Op(p
log(K)/n) =Op(n−r), since r <1/2 (Assumption 5.4). This, on top of Assump-
tions 5.1, 5.3, 5.4.b, and 5.5, means that we can apply Theorem 2 in Chernozhukov et al. (2022b)
to get:
∥ˆα1ℓ−α01∥2=op(nξn−r/2) and∥ˆα2ℓ−α02∥2=op(nξn−r/2)
for any ξ >0. We choose a ξsatisfying 0 < ξ < (3r−1)/2< r/ 2, which is possible since, by
Assumption 5.4, r∈(1/3,1/2). This guarantees that
∥ˆα1ℓ−α01∥2=op(1) and ∥ˆα2ℓ−α02∥2=op(1); and (B.11)
√n∥ˆα1ℓ−α01∥2∥ˆg−g0∥2=op(1) and√n∥ˆα2ℓ−α02∥2∥ˆhℓ−h0∥=op(1), (B.12)
where the last line follows from Assumption 5.4.a.
We now check Assumption 1 in CEINR. Assumption 1.i is identical to Assumption 5.6.a and
5.6.b. To show the remaining points, define ˜hℓ(w)≡ˆhℓ(x, φ(d, z,ˆgℓ)) and ˜ α2ℓ(w)≡ˆα2ℓ(x, φ(d, z,ˆgℓ)).
Note also that by Assumptions 5.2 and 5.3.a:
|α01(Z)| ≤∞X
k=1|βk||ck(Z)| ≤sup
k∈N|ck(Z)| ·∞X
k=1|βk| ≡κ1<∞and
|α02(X, V)| ≤∞X
j=1|ρj||bj(X, V)| ≤sup
j∈N|bj(X, V)| ·∞X
j=1|ρj| ≡κ2<∞.
45Thus, by the triangle inequality, being v≡φ(d, z, g 0):
Zh
¯ϕ(w, φ(d, z, g 0),ˆgℓ,ˆhℓ, α0)−ϕ(w, g 0, h0, α0, θ0)i2
dF0(w)≤Z
α01(z)2[ˆgℓ(z)−g0(z)]2dF0(w)
+Z
α02(x, v)2h
˜hℓ(w)−h0(x, v)i2
dF0(w)
≤κ2
1∥ˆgℓ−g0∥2
2+κ2
2∥˜hℓ−h0∥2
2.
Assumption 1.ii in CEINR follows from the above display, Assumption 5.4.a, and Lemma B.1.
Also, calling κ3, κ4<∞to the bounds given by Assumption 5.6.d:
Z¯ϕ(w, φ(d, z,ˆgℓ), g0, h0,ˆαℓ)−ϕ(w, g0, h0, α0, θ0)2dF0(w)≤Z
[d−g0(z)]2[ˆα1ℓ(z)−α01(z)]2dF0(w)
+Z
[y−h0(x, v)]2[˜α2ℓ(w)−α02(x, v)]2dF0(w)
≤κ2
3∥ˆα1ℓ−α01∥2
2+κ2
4∥˜α2ℓ−α02∥2
2,
Thus, Assumption 1.iii in CEINR follows from the above display, equation (B.11), and Lemma B.1.
We now move to check Assumption 2 in CEINR. In particular, we show that 2.iii holds. We have that
ˆ∆iℓ= [ˆα1ℓ(Zi)−α01(Zi)]·[ˆgℓ(Zi)−g0(Zi)] + [˜α2ℓ(Wi)−α02(Xi, Vi)]·[˜hℓ(Wi)−h0(Xi, Vi)].
Thus, as in Chernozhukov et al. (2022b, proof of Th. 9), an application of the Cauchy-Schwarz, conditional
Markov, and triangle inequalities leads to:
1√nX
i∈Iℓˆ∆iℓ=Op(√n∥ˆα1ℓ−α01∥2∥ˆgℓ−g0∥2) +Op(√n∥˜α2ℓ−α01∥2∥˜hℓ−h0∥2).
Thus, by equation (B.12) and Lemma B.1, Assumption 2.iii in CEINR is satisfied.
To see that Assumption 3.iii in CEINR holds, note that by Assumptions 3.1-3.4: E[D1(W, g))] =
E[α01(Z)g(Z)] and E[D2(W, h(·, φ(·,·, g)))] = E[α02(X, V)h(X, φ(D, Z, g ))]. The last equality also uses
Assumption 5.9.c. Moreover, D−g0(Z) and Y−h0(X, V) are orthogonal to ∆ 1and ∆ 2(g0), respectively.
Then,
E[¯ϕ(W, φ(D, Z, g 0), g, h, α 0)] =E[α01(Z)(g0(Z)−g(Z))] +E[α02(X, V)(h0(X, V)−h(X, φ(D, Z, g )))]
=−E[D1(W, g−g0)]−E[D2(W, h(·, φ(·,·, g))−h0)].
Thus, by Assumption 5.7, for ∥g−g0∥2< εand∥h−h0∥2< ε:
E[m(W, g, h, α 0, θ0) +¯ϕ(W, φ(D, Z, g 0), g, h, α 0)]
=|E[m(W, g, h, θ 0)−m(W, g 0, h0, θ0)−D1(W, g−g0)−D2(W, h(·, φ(·,·, g))−h0)]|
≤C 
∥g−g0∥2
2+∥h(·, φ(·,·, g))−h0∥2
2
.
The above display, on top of Assumption 5.4.a and Lemma B.1, gives Assumption 3.iii in CEINR for the
functional ( g, h)7→E[m(W, g, h, α 0, θ0) +¯ϕ(W, φ(D, Z, g 0), g, h, α 0)].
46To conclude, we verify that Lemma 8 in CEINR can be applied to our modified expansion. Being Ic
ℓ
all observations not in Iℓ, note that
E[ˆR1iℓ+ˆR2iℓ|Ic
ℓ] =E[m(W,ˆgℓ,ˆhℓ, α0, θ0) +¯ϕ(W, φ(D, Z, g 0),ˆgℓ,ˆhℓ, α0)|Ic
ℓ] and
E[ˆR3iℓ|Ic
ℓ] = 0.
The last equation follows from orthogonality of D−g0(Z) and Y−h0(X, V) to ∆ 1and ∆ 2(g0), respectively,
and Assumption 5.9.c. This means that the strategy of Lemma 8 can be applied to our expansion (for
more details, we refer to the proof of the lemma in Chernozhukov et al., 2022a).
We conclude the proof of Theorem 5.1 by providing consistency of ˆΞ. Call ψi≡ψ(Wi, g0, h0, α0, θ0)
and¯Ψ≡n−1Pn
i=1ψiψ′
i. We have that
∥ˆΨ−¯Ψ∥∞≤LX
ℓ=11
nX
i∈Iℓ
∥ˆψiℓ−ψi∥2
∞+ 2∥ˆψiℓ−ψi∥∞∥ψi∥∞
We now expand ˆψiℓ(˜θℓ)−ψ(Wi, g0, h0, α0, θ0) =ˆR1iℓ+ˆR2iℓ+ˆR3iℓ+ˆR4iℓ+ˆ∆iℓ, with
ˆR4iℓ≡m(Wi,ˆgℓ,ˆhℓ,˜θℓ)−m(Wi,ˆgℓ,ˆhℓ, θ0)
and the remaining terms are given in equation (B.10). Then
1
nX
i∈Iℓ∥ˆψiℓ−ψi∥2
∞≤C1
nX
i∈Iℓ
∥ˆR1iℓ∥2
∞+∥ˆR2iℓ∥2
∞+∥ˆR3iℓ∥2
∞+∥ˆR4iℓ∥2
∞+∥ˆ∆iℓ∥2
∞
by the triangle inequality. The constant Ccomes from the presence of the interation terms: for instance,
we have that 2 ∥ˆR1iℓ∥∞∥ˆR2iℓ∥∞≤2 max{∥ˆR1iℓ∥∞,∥ˆR2iℓ∥∞}.
We apply Assumptions 5.6.b and 5.6.c to each component of ˆR1iℓand ˆR4iℓ, respectively. This yields
E[∥ˆR1iℓ∥2
∞|Ic
ℓ]P− →0 and E[∥ˆR4iℓ∥2
∞|Ic
ℓ]P− →0. Moreover, by the argument we have followed to show that
Assumption 1.ii and 1.ii in CEINR are satisfied: E[∥ˆR2iℓ∥2
∞|Ic
ℓ]P− →0 and E[∥ˆR3iℓ∥2
∞|Ic
ℓ]P− →0. Also, by the
Cauchy-Schwarz inequality, equation (B.12) and Lemma B.1 (applied to each component):
E[∥ˆ∆iℓ∥2
∞|Ic
ℓ]≤3
∥ˆα1ℓ−α01∥2∥ˆgℓ−g0∥2+∥˜α2ℓ−α02∥2∥˜hℓ−h0∥2
=op(1).
Thus, collecting the above results:
E
1
nX
i∈Iℓ∥ˆψiℓ−ψi∥2
∞Ic
ℓ
≤CEh
∥ˆR1iℓ∥2
∞+∥ˆR2iℓ∥2
∞+∥ˆR3iℓ∥2
∞+∥ˆR4iℓ∥2
∞+∥ˆ∆iℓ∥2
∞Ic
ℓi
=op(1).
An application of the conditional Markov inequality gives then n−1P
i∈Iℓ∥ˆψiℓ−ψi∥2
∞=op(1). Also, by
Assumptions 5.2, 5.3.a, 5.6.a, and 5.6.d: E[ψiψ′
i]<∞. So, by the Law of Large Numbers, ¯ΨP− →E[ψiψ′
i].
Therefore, by Cauchy-Schwarz:
∥ˆΨ−¯Ψ∥∞≤LX
ℓ=1
1
nX
i∈Iℓ∥ˆψiℓ−ψi∥2
∞+ 2s
1
nX
i∈Iℓ∥ˆψiℓ−ψi∥2∞s
1
nX
i∈Iℓ∥ψi∥2∞

=op(1) + op(1)·Op(1) = op(1).
This leads to ˆΨ = ¯Ψ +op(1)P− →E[ψiψ′
i]. ■
47Pr oof of Pr oposition 5.1: We start by formally checking Assumptions 3.1 and 3.2, i.e., that
E[D11(W, g)] and E[D2(W, h)] are continuous. This is guaranteed by Assumption 5.10. Being C1
the bound of ∂h/∂v andC2the bound of α02,
|E[D11(W, g)]|=E
−Z∂h0
∂v(x∗, V)dF∗(x∗)·g(Z)≤CE[|g(Z)|]≤C1∥g∥2,
|E[D2(W, h)]| ≤ ∥α02∥2∥h∥2≤C2∥h∥2,
where we have used Jensen’s and Cauchy-Schwarz’ inequalities.
We continue with Assumption 5.6. If the CASF is well defined, Assumption 5.6.a is equivalent
to
E"Z
h0(x∗, V)dF∗(x∗2#
<∞.
By Jensen’s inequality:
E"Z
h0(x∗, V)dF∗(x∗2#
≤Z
h0(x∗, v)2f∗(x∗)fv
0(v)dx∗dv=E[α02(X, V)h0(X, V)2].
This is finite by Assumption 5.10.a: E[α02(X, V)h0(X, V)]≤C2E[Y2]<∞.
To check Assumption 5.6.b, again by Jensen’s inequality:
Z
[m(w,ˆgℓ,ˆhℓ, θ0)−m(w, g 0, h0, θ0)]2dF0(w) =ZZ
[ˆhℓ(x∗, φ(d, z,ˆgℓ))−h0(x, v)]dF∗(x∗)2
dF0(w)
≤Z
ˆhℓ(x∗, φ(d, z,ˆgℓ))−h0(x, v)2
dF∗(x∗)dF0(w).
Under Assumption 5.9.c:
Z
ˆhℓ(x∗, φ(d, z,ˆgℓ))−h0(x, v)2
dF∗(x∗)dF0(w)
=Z
α02(x, v)
ˆhℓ(x∗, φ(d, z,ˆgℓ))−h0(x, v)2
dF0(x, v)
≤C2∥˜hℓ−h0∥2
2,
where ˜hℓ(w)≡ˆhℓ(x, φ(d, z,ˆgℓ)). This converges to zero by Lemma B.1.
Regarding Assumption 5.6.c, since m(w,ˆgℓ,ˆhℓ,˜θℓ)−m(w,ˆgℓ,ˆhℓ, θ0) =˜θℓ−θ0, it suffices to have
a consistent estimator of the CASF.
We now check Assumption 5.7. The idea is to show that ¯ m(g, h, θ ) is twice continuously differ-
entiable along the paths g(F) and h(F, g(F)). Let ( g, h) be an arbitrary point in the neighborhood
of (g0, h0). Set V(g)≡D−g(Z) and recall that V≡D−g0(Z). As in the computations in the
main text, the first derivatives at ( g, h) are (using Assumption 5.9.c):
Dg¯m(g, h, θ ) =−Z∂h
∂v(x∗, V(g))dF∗(x∗) +∂h
∂v(X, V(g))α02(X, V(g)),and
Dh¯m(g, h, θ ) =α02(X, V(g)).
48Note that both derivatives map ( g, h) onto L2(D, Z). Then, for instance, the second derivative w.r.t.
gat point ( g0, h0) (denoted Dgg¯m(g0, h0, θ)) maps g∈∆1to the space of linear maps between ∆ 1
andL2(D, Z). To characterize it, take b∈L2(D, Z). By the characterization of the derivative in
Yamamuro (1974, p. 9), for a path τ7→gτwith ∂gτ/∂τ= ˜g:
E[b·Dgg¯m(g0, h0, θ)(˜g)] =∂
∂τE[b·Dg¯m(gτ, h0, θ)] =E
b·∂
∂τDg¯m(gτ, h0, θ)
Since this holds for all b, we have that Dgg¯m(g0, h0, θ)(˜g) =∂Dg¯m(gτ, h0, θ)/∂τ. In the case of the
first derivative, this gives:
Dgg¯m, θ(g0, h0) (˜g) =Z∂2h0
∂v2(x∗, V)dFx∗(x∗)−∂2h0
∂v2(X, V)α02(X, V)
−∂h0
∂v(X, V)∂α02
∂v(X, V)
˜g(Z).
Thus, by Assumption 5.10, ∥Dgg¯m(g0, h0, θ) (˜g)∥2≤√
C∥˜g∥2, where Cis the corresponding sum of
the bounds of the derivatives of h0andα02.
Note that, since Dh¯m(g, h, θ ) does not depend on h(i.e., ¯ mis linear in h),Dhh¯m(g0, h0, θ) = 0.
The cross derivative is:
Dgh¯m(g0, h0, θ)(˜g) =∂Dh¯m(gτ, h0, θ)
∂τ=−∂α02
∂v(X, V),˜g(Z)
which is also bounded (i.e., continuous) under Assumption 5.10. Therefore, by Proposition 3 in
Luenberger (1997, Sec. 7.3), Assumption 5.7 is satisfied (see also Th. 3 in Chernozhukov et al.,
2022a).
Regarding Assumption 5.8, this is trivial in case of the moment condition identifying the CASF.
Simply note that
∂m
∂θ(W, g, h, θ ) =−1.
■
49References
Ahn, H. and Powell, J. L. (1993). Semiparametric estimation of censored selection models with a
nonparametric selection mechanism. Journal of Econometrics , 58(1-2):3–29.
Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of Lasso and Dantzig
selector. The Annals of Statistics , 37(4):1705 – 1732.
Blundell, R. and Powell, J. L. (2003). Endogeneity in nonparametric and semiparametric regression
models. Econometric society monographs , 36:312–357.
Blundell, R. W. and Powell, J. L. (2004). Endogeneity in semiparametric binary response models.
The Review of Economic Studies , 71(3):655–679.
Bunea, F., Tsybakov, A., and Wegkamp, M. (2007). Sparsity oracle inequalities for the Lasso.
Electronic Journal of Statistics , 1(none):169 – 194.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins,
J. (2018). Double/debiased machine learning for treatment and structural parameters. The
Econometrics Journal , 21(1):C1–C68.
Chernozhukov, V., Escanciano, J. C., Ichimura, H., Newey, W. K., and Robins, J. M. (2022a).
Locally robust semiparametric estimation. Econometrica , 90(4):1501–1535.
Chernozhukov, V., Newey, W. K., and Singh, R. (2022b). Automatic debiased machine learning of
causal and structural effects. Econometrica , 90(3):967–1027.
Das, M., Newey, W. K., and Vella, F. (2003). Nonparametric estimation of sample selection models.
The Review of Economic Studies , 70(1):33–58.
Escanciano, J. C., Jacho-Ch´ avez, D., and Lewbel, A. (2016). Identification and estimation of
semiparametric two-step models. Quantitative Economics , 7(2):561–589.
Escanciano, J. C., Jacho-Ch´ avez, D. T., and Lewbel, A. (2014). Uniform convergence of weighted
sums of non and semiparametric residuals for estimation and testing. Journal of Econometrics ,
178:426–443.
Fong, C. and Tyler, M. (2021). Asymptotic variance of semiparametric estimators with generated
regressors. Political Analysis , 29(4):467–484.
Garen, J. (1984). The returns to schooling: A selectivity bias approach with a continuous choice
variable. Econometrica: Journal of the Econometric Society , pages 1199–1218.
50Hahn, J., Liao, Z., Ridder, G., and Shi, R. (2023). The influence function of semiparametric two-step
estimators with estimated control variables. Economics Letters , 231:111–277.
Hahn, J. and Ridder, G. (2013). Machine learning predictions as regression covariates. Economet-
rica, 81(1):315–340.
Hahn, J. and Ridder, G. (2019). Three-stage semi-parametric inference: Control variables and
differentiability. Journal of econometrics , 211(1):262–293.
Heckman, J. J. (1979). Sample selection bias as a specification error. Econometrica: Journal of the
econometric society , pages 153–161.
Heckman, J. J., Ichimura, H., and Todd, P. (1998). Matching as an econometric evaluation estima-
tor.The review of economic studies , 65(2):261–294.
Heckman, J. J. and Vytlacil, E. (2005). Structural equations, treatment effects, and econometric
policy evaluation 1. Econometrica , 73(3):669–738.
Ichimura, H. and Lee, L.-F. (1991). Semiparametric least squares estimation of multiple index
models: single equation estimation. In Nonparametric and semiparametric methods in economet-
rics and statistics: Proceedings of the Fifth International Symposium in Economic Theory and
Econometrics. Cambridge , pages 3–49.
Ichimura, H. and Newey, W. K. (2022). The influence function of semiparametric estimators.
Quantitative Economics , 13(1):29–61.
Imbens, G. W. and Newey, W. K. (2009). Identification and estimation of triangular simultaneous
equations models without additivity. Econometrica , 77(5):1481–1512.
Luenberger, D. G. (1997). Optimization by vector space methods . John Wiley & Sons.
Mammen, E., Rothe, C., and Schienle, M. (2012). Nonparametric regression with nonparametrically
generated covariates. The Annals of Statistics , 40(2):1132–1170.
Mammen, E., Rothe, C., and Schienle, M. (2016). Semiparametric estimation with generated
covariates. Econometric Theory , 32(5):1140–1177.
Newey, W. K. (1994). The asymptotic variance of semiparametric estimators. Econometrica ,
62(6):1349–1382.
Newey, W. K. (1997). Convergence rates and asymptotic normality for series estimators. Journal
of econometrics , 79(1):147–168.
51Newey, W. K., Powell, J. L., and Vella, F. (1999). Nonparametric estimation of triangular simulta-
neous equations models. Econometrica , 67(3):565–603.
P´ erez-Izquierdo, T. J. (2022). The determinants of counterfactual identification in the binary choice
model with endogenous regressors. Unpublished manuscript.
Rivers, D. and Vuong, Q. H. (1988). Limited information estimators and exogeneity tests for
simultaneous probit models. Journal of econometrics , 39(3):347–366.
Robinson, P. M. (1988). Root-n-consistent semiparametric regression. Econometrica: Journal of
the Econometric Society , pages 931–954.
Rothe, C. (2009). Semiparametric estimation of binary response models with endogenous regressors.
Journal of Econometrics , 153(1):51–64.
Sasaki, Y. and Ura, T. (2021). Estimation and inference for policy relevant treatment effects.
Journal of Econometrics .
Shao, J. (2003). Mathematical statistics . Springer Science & Business Media.
Stock, J. H. (1989). Nonparametric policy analysis. Journal of the American Statistical Association ,
84(406):567–575.
Stock, J. H. (1991). Nonparametric policy analysis: an application to estimating hazardous waste
cleanup benefits. Nonparametric and Semiparametric Methods in Econometrics and Statistics ,
pages 77–98.
Wooldridge, J. M. (2010). Econometric analysis of cross section and panel data . The MIT Press.
Wooldridge, J. M. (2015). Control function methods in applied econometrics. Journal of Human
Resources , 50(2):420–445.
Yamamuro, S. (1974). Differential calculus in topological linear spaces , volume 374. Springer.
Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the Lasso selection in high-dimensional
linear regression. The Annals of Statistics , 36(4):1567 – 1594.
52