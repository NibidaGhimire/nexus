Automatic Locally Robust Estimation with Generated
Regressorsâˆ—
Juan Carlos Escanciano
Universidad Carlos III de Madrid
Telmo J. PÂ´ erez-Izquierdo
BCAM - Basque Center for Applied Mathematics
November 8, 2023
Abstract
Many economic and causal parameters of interest depend on generated regressors. Exam-
ples include structural parameters in models with endogenous variables estimated by control
functions and in models with sample selection, treatment effect estimation with propensity
score matching, and marginal treatment effects. Inference with generated regressors is com-
plicated by the very complex expression for influence functions and asymptotic variances. To
address this problem, we propose Automatic Locally Robust/debiased GMM estimators in a
general setting with generated regressors. Importantly, we allow for the generated regressors
to be generated from machine learners, such as Random Forest, Neural Nets, Boosting, and
many others. We use our results to construct novel Doubly Robust and Locally Robust es-
timators for the Counterfactual Average Structural Function and Average Partial Effects in
models with endogeneity and sample selection, respectively. We provide sufficient conditions
for the asymptotic normality of our debiased GMM estimators and investigate their finite
sample performance through Monte Carlo simulations.
Keywords: Local robustness, orthogonal moments, double robustness, semiparametric es-
timation, bias, GMM. JEL Classification: C13; C14; C21; D24
âˆ—Research supported by MICIN/AEI/10.13039/501100011033, grant CEX2021-001181-M, Comunidad de Madrid,
grants EPUC3M11 (V PRICIT) and H2019/HUM-5891, and grant PID2021-127794NB-I00 (MCI/AEI/FEDER, UE).
1arXiv:2301.10643v2  [econ.EM]  7 Nov 20231 Introduction
Many economic and causal parameters of interest depend on generated regressors. Leading exam-
ples include the Counterfactual Average Structural Function (CASF) in models with endogenous
variables estimated by control functions (see, e.g., Blundell and Powell, 2004; Stock, 1989, 1991), Av-
erage Partial Effects (APE) in sample selection models (see, e.g., Das et al., 2003), Propensity Score
Matching (PSM) (see, e.g., Heckman et al., 1998), and Marginal Treatment Effects (MTE) using Lo-
cal Instrumental Variables (see, e.g., Heckman and Vytlacil, 2005). There are currently no general
econometric methods for inference on these parameters allowing for generated regressors obtained
by machine learning. The goal of this paper is to propose Automatic Locally Robust/Debiased
estimators of and inference on structural parameters in such models. The estimators and inferences
that we propose are automatic in the sense that influence functions and asymptotic variances are
estimated automatically from data and identifying moments. There is no need to find their analytic
shape.
We extend Chernozhukov et al. (2022a)â€™s results to build debiased moment conditions in the
presence of nonparametric/semiparametric generated regressors. By applying the chain rule, the
debiasing correction term can be decomposed into one accounting for the first step, which is used
to generate a regressor, and the term accounting for the second step, where the outcome variable is
regressed onto the generated variable (among other covariates). Chernozhukov et al. (2022a) con-
struct debiased moment conditions which account for this second step (with no generated regressor).
Our paper provides the additional correction term that accounts for (i) the plug-in of generated
regressors in the moment condition and (ii) the effect of generated regressor on the estimation of
the second step.
Each of the two correction terms depends on additional nuisance parameters. Under a lineariza-
tion assumption (as in, e.g., Ichimura and Newey, 2022; Newey, 1994), we show how the additional
nuisance parameters in the correction term can be estimated separately without knowing their spe-
cific analytic shape. This process is called automatic estimation (see Chernozhukov et al., 2022b).
Automatic estimation is particularly well motivated in the case of generated regressors, where the
nuisance parameters in the correction term take complex shapes (see, for instance, Escanciano et al.,
2014; Hahn and Ridder, 2013; Mammen et al., 2016).
As a leading application of our methods, we propose novel Automatic Locally Robust estimators
for the CASF parameter of Blundell and Powell (2004). This parameter is a linear functional of a
second step function satisfying orthogonality conditions involving generated regressors (the control
function) from a first step. We show that it is relatively straightforward to construct Automatic
Doubly Robust estimators that are robust to functional form assumptions for the second step. For
instance, a practical approach could be to fit a partially linear specification for the second step, like
in Robinson (1988), but with a non-parametric function of the generated regressors. Our results
cover this case, in which the second step is semiparametric.
2The Doubly Robust estimators are, however, not Locally Robust to the generated regressors
in general. To construct fully Locally Robust estimators we use numerical derivatives to account
for the presence of generated regressors. Fortunately, our automatic approach is amenable to any
machine learning method for which out of sample predictions are available. Another approach
could be to specify a model for the second step for which analytical derivatives are available, e.g.,
a partially linear model. We note that the Doubly Robust moment conditions are robust to this
model being misspecified when the adjustment term is consistently estimated (see Remark 2.2).
We provide mild sufficient conditions for the asymptotic normality of the debiased GMM esti-
mator. We illustrate the results with the CASF example. The finite sample performance of the
proposed debiased estimator for the CASF is evaluated through Monte Carlo simulations. We
use Lasso with different dictionaries (linear, quadratic, and one including interactions) to fit the
first and second step parameters, as well as the nuisance parameters in the first and second step
correction terms. Our Monte Carlo results confirm that the plug-in estimator is substantially bi-
ased. Correcting the moment condition for the second step estimation reduces de bias, but a first
step correction must be also added to remove it totally throughout all the different specifications
considered.
The paper builds on two different literatures. The first literature is the classical literature on
semiparametric estimators with generated regressors, see Ahn and Powell (1993); Heckman et al.
(1998); Ichimura and Lee (1991); Imbens and Newey (2009); Newey et al. (1999); Rothe (2009),
among many others. The asymptotic properties of several estimators within this class is given, for
example, by Escanciano et al. (2014), Hahn and Ridder (2013, 2019) and Mammen et al. (2012,
2016). With respect to these papers, we allow the second step to be semiparametric or parametric
(on top of fully non-parametric). Furthermore, we contribute to this literature by providing auto-
matic debiased GMM estimators, which allow for machine learning generated regressors and reduce
regularization and model selection biases. These biases are shown to be important in practice and
are significantly reduced by our methods in the CASF example. Our results are thus motivated by
the many applications where machine learning methods are used to generate regressors, including
high dimensional propensity score estimates for matching and treatment effects, imputed missing
covariates, and sentiment in text analysis, among many others (see Fong and Tyler, 2021, for a
partial review of applications).
The second literature we build on is the more recent literature on Locally Robust/Debiased
estimators, see, e.g., Chernozhukov et al. (2018, 2022a). With the only exception of Sasaki and Ura
(2021), this literature has not considered models with generated regressors. Our results complement
the analysis of the Policy Relevant Treatment Effect (PRTE) in Sasaki and Ura (2021) by providing
a general setting for problems with generated regressors and automatic estimation of adjustment
terms. Relative to the Automatic Locally Robust literature (e.g. Chernozhukov et al., 2022b) we
innovate by considering a nonlinear setting with an implicit functional (the generated regressor
3as a conditioning argument) for which an analytic derivative is not available for general machine
learners. We also exploit novel partial or separate robustness results for identifying and estimating
individual Riesz representers.
The proposed debiased estimators and inferences are flexible, modern, robustified, and cross-
fitted versions of commonly employed estimators in the applied econometrics literature, see, e.g.,
Heckman (1979), Rivers and Vuong (1988), and Wooldridge (2015). More broadly, our methods pro-
vide new robust estimators for models with generated regressors for which inference was unavailable,
such as that for the CASF in Blundell and Powell (2004).
We illustrate the simplicity of our estimators with the CASF parameter for a partially linear
second step Ë†h(Xi,Ë†Vi) =Ë†Î²â€²Xi+ Ë†Îº(Ë†Vi), a control variable Ë†Vi=Diâˆ’Ë†g(Zi) for the endogenous regressor
Di, and first step Ë† g(Zi). Both steps could be obtained, for example, from Lasso fits. Then, the
plug-in (PI), Doubly Robust (DR), and fully Locally Robust (LR) estimators are given, respectively,
by
Ë†Î¸PI=Ë†Î²â€²Â¯Xâˆ—,
Ë†Î¸DR=Ë†Î¸PI+1
nLX
â„“=1X
iâˆˆIâ„“Ë†Î±2â„“(Xi,Ë†Viâ„“)Â·(Yiâˆ’Ë†hâ„“(Xi,Ë†Viâ„“)),
Ë†Î¸LR=Ë†Î¸DR+1
nLX
â„“=1X
iâˆˆIâ„“Ë†Î±1â„“(Zi)Â·(Diâˆ’Ë†gâ„“(Zi)),
where Â¯Xâˆ—is a counterfactual average for X, Ë†gâ„“andË†hâ„“are cross-fitted estimates for the first and
second step parameters (not using observations in the set Iâ„“),Ë†Viâ„“=Diâˆ’Ë†gâ„“(Zi), and Ë† Î±2â„“(Xi,Ë†Viâ„“) and
Ë†Î±1â„“(Zi) are automatic estimators that we propose in (4.7) and (4.10), respectively. We also consider
nonparametric versions of these estimators that are robust to misspecification of the partially linear
model. These estimators are prototypes of a general class of automatic debiased GMM estimators
with generated regressors that we propose in this paper.
The rest of the paper is organized as follows. Section 2 introduces the setting and the exam-
ples. Section 2.1 finds the influence function of parameters identified by moments with generated
regressors. Section 3 gives the general construction of Automatic Locally Robust moments with
generated regressors. In Section 4, we provide the details for Debiased/Locally Robust GMM es-
timation. A summary of the estimation algorithm is given in Section 4.2. The asymptotic theory
for the proposed estimator is developed in Section 5. Monte Carlo simulations are presented in
Section 6. Section 7 concludes. Appendix A provides the details for the APE estimation in sample
selection models, while Appendix B gathers the proofs of the main results.
42 Setting
We observe data W= (Y, D, Z ) with cumulative distribution function (cdf) F0. For simplicity of
exposition, we consider that YandDare one-dimensional. In our setting, there is a first step
linking Dwith Z. The first step results in a one-dimensional generated regressor
Vâ‰¡Ï†(D, Z, g 0),
where Ï†is a known function of observed variables ( D, Z) and an unknown function g0âˆˆâˆ†1, for âˆ† 1
a linear and closed subspace of L2(Z). Henceforth, for a generic random variable U, we denote by
L2(U) the Hilbert space of square-integrable functions of U. The unknown function g0solves the
orthogonal moments
E[Î´1(Z)(Dâˆ’g0(Z))] = 0 for all Î´1âˆˆâˆ†1. (2.1)
This setting covers semiparametric and non-parametric first steps g0. For example, when âˆ† 1=
L2(Z), we have g0(Z) =E[D|Z]. For general parametric first steps see Remark 3.3.
The second step links Ywith a component of ( D, Z), denoted by X, and the generated regressor
V, through the moment restrictions
E[Î´2(X, V)(Yâˆ’h0(X, V))] = 0 for all Î´2âˆˆâˆ†2(g0), (2.2)
where âˆ† 2(g0) is a linear and closed subspace of L2(X, V) (note âˆ† 2depends on g0because V
depends on g0). For instance, Hahn and Ridder (2013) and Mammen et al. (2016) consider cases
where the second step h0is a non-parametric regression of Yon (X, V), i.e., âˆ† 2(g0) =L2(X, V).
Semiparametric examples of âˆ† 2(g0) are given below.
Let Î˜ âŠ†Rdenote the parameter space where the structural parameter of interest lies. Consider
the moment function m:Rdim(W)Ã—L2(Z)Ã—L2(X, V)Ã—Î˜â†’R. The parameter of interest Î¸0is
identified in a third step by a GMM moment condition
E[m(W, g 0, h0, Î¸0)] = 0 .
Here we assume that Î¸0is identified by these moments, i.e. that Î¸0is the unique solution to
E[m(W, g 0, h0, Î¸)] = 0 over Î¸âˆˆÎ˜.
Our result allows for an arbitrary number of parameters Î¸âˆˆRdim(Î¸)and moment conditions
m:Rdim(W)Ã—L2(Z)Ã—L2(X, V)Ã—Î˜â†’Rdim(m), with dim( m)â‰¥dim(Î¸)â‰¥1. To ease the exposition,
most results are derived for the one-dimensional case (the extension to multiple parameters is
straightforward). Our results can be also extended to (i) allow for multidimensional DandYand
(ii) first and second steps satisfying different orthogonality conditions, as in Section 3 of Ichimura
and Newey (2022).
The following problem serves as a running example to illustrate the result of this paper:
5Example 1(Control Function Approach) We observe W= (Y, D, Z ) satisfying the model
Y=H(X, U), for an unknown function H. The main feature of this model is that D, a component
ofX, may be an endogenous regressor. We assume that the endogenous regressor satisfies D=
g0(Z) +V, with UandVbeing unobserved correlated error terms. The function g0could be
identified by a conditional mean restriction, as in equation (2.1). We assume a Control Function
approach: U|D, Zâˆ¼U|X, Vâˆ¼U|V, where âˆ¼denotes equally distributed. Thus, the corresponding
Ï†is
Vâ‰¡Ï†(X, Z, g 0)â‰¡Dâˆ’g0(Z).
As in Blundell and Powell (2003), the Control Function assumption implies
E[Y|X=x, V=v] =E[H(X, U)|X=x, V=v] =E[H(x, U)|X=x, V=v]
=E[H(x, U)|V=v]â‰¡h0(x, v).
This defines the second step, which satisfies (2.2) with âˆ† 2(g0) =L2(X, V).
The Control Function assumption allows us to identify the Average Structural Function (ASF)
at a point xâˆˆRdim(X):
ASF 0(x)â‰¡E[H(x, U)] =E[E[H(x, U)|V]] =E[h0(x, V)].
Some well-known conditions on the support of the random vectors are needed for the above equation
to hold (see Blundell and Powell, 2004; Imbens and Newey, 2009).
In this setup, a parameter of interest is the Counterfactual Average Structural Function (CASF)
given by
Î¸0=Z
ASF 0(xâˆ—)dFâˆ—(xâˆ—),
for a counterfactual distribution Fâˆ—. When Fâˆ—is implied by a certain policy, the CASF may be
used to measure the effect of the policy (see Blundell and Powell, 2004; Stock, 1989, 1991). By
Fubiniâ€™s Theorem, the CASF can be written as a function of ( g0, h0):
Î¸0=Z
E[h0(xâˆ—, Dâˆ’g0(Z))]dFâˆ—(xâˆ—) =EZ
h0(xâˆ—, Dâˆ’g0(Z))dFâˆ—(xâˆ—)
.
Hence, the moment function that identifies the CASF is:
m(w, g, h, Î¸ ) =Z
h(xâˆ—, dâˆ’g(z))dFâˆ—(xâˆ—)âˆ’Î¸. (2.3)
We will propose below novel Doubly Robust and fully Locally Robust estimators for the CASF
under nonparametric and semiparametric specifications of h0. â– 
A remarkable feature of the above problem is that, to find the value of the moment condition for
a certain point w= (y, d, z ), one needs to know the whole shape of the second-step regression h0.
6Thus, it does not fall in Hahn and Ridder (2013, 2019)â€™s setup, where the moment condition evalu-
ated at wsolely depends on h0through its value at ( x, Ï†(d, z, g 0)). Indeed, our setting generalizes
theirs in three ways: (i) we consider a larger class of moment conditions (covering, for instance,
the CASF), (ii) we allow for semiparametric first and second steps, and (iii) we allow for a wider
range of generated regressors Ï†(D, Z, g 0), as the authors consider generated regressors with shape
Ï†(D, Z, g 0) =g0(Z). We show how their setup accommodates to this paperâ€™s framework:
Example 2(Hahn and Ridder (2013)â€™ Setup) This example discusses the non-parametric
setup in Hahn and Ridder (2013, Th. 5). The authors focus on the case where there is a function
Î·:Rdim(W)+1â†’Rsuch that
m(w, g, h, Î¸ ) =Î·(w, h(x, g(z)))âˆ’Î¸.
That is, in Hahn and Ridder (2013)â€™s setup, ( g, h) enters the moment condition by the values that
the â€œlinkâ€ function Î·, with domain in an Euclidean space, takes at ( w, h(x, g(z))). Note that they
fixÏ†(d, z, g ) =g(z) and that their Theorem 5 covers the fully non-parametric case: âˆ† 1=L2(Z)
and âˆ† 2(g0) =L2(X, V) (other results in Hahn and Ridder, 2013, cover parametric first steps). â– 
Example 1(continuing from p. 6) A practical semiparametric specification for h0when X
isp-dimensional and pis high is a partially linear model, where
h0(x, v) =xâ€²Î²0+Îº0(v),
with Î²0andÎº0unknown finite and infinite-dimensional parameters, respectively. In this speci-
fication, Xcontains an intercept, and hence, we can assume that Îº0belongs to the subspace of
zero mean functions in L2(V), denoted as L0
2(V). This setting corresponds to the semiparametric
orthogonality conditions (2.2) where
âˆ†2(g0) ={Î´(x, v) =xâ€²Î²+Îº(v):Î²âˆˆRp, ÎºâˆˆL0
2(V)} âŠ†L2(X, V).
This specification generalizes the classical linear structural control function approach to a non-
Gaussian semiparametric setting. In this partly linear model, the CASF is given by Î¸0=Î²â€²
0Eâˆ—[X],
where Eâˆ—[X] denotes the mean of Xunder the counterfactual distribution Fâˆ—.
A further generalization yields
h0(x, v) =xâ€²Î²0+Îº0(v) +dÎº1(v),
where Îº1is an additional unknown infinite-dimensional parameter, corresponding, for example, to a
semiparametric Correlated Random Coefficient specification where the coefficient of the endogenous
variable Dis random; see Wooldridge (2015) for a description of parametric versions of this model.
This version generalizes Garen (1984) to a semiparametric non-Gaussian setting. â– 
7Remark 2.1(Profiling) Our results can be extended to allow for profiling as in Mammen et al.
(2016). That is, we may consider that the second step nuisance parameter depends on Î¸:h0(x, v, Î¸ )
is the solution in hofE[Î´2(X, V)(Yâˆ’h(X, V, Î¸ ))] = 0 for all Î´2âˆˆâˆ†2(g0, Î¸).
For instance, if h0is the conditional expectation given some transformation of ( D, Z), denoted
T(D, Z, g, Î¸ ) as in Mammen et al. (2016), then one would take âˆ† 2(g, Î¸)â‰¡L2(T). In models with an
index restriction, T(D, Z, g, Î¸ ) = (Î¸â€²X, Ï†(D, Z, g )), where Xis a subvector of ( D, Z) (see Escanciano
et al., 2016, for example applications).
â– 
2.1 Orthogonal Moment Functions with Generated Regressors
We follow Chernozhukov et al. (2022a, henceforth, CEINR) for the construction of Locally Robust-
Debiased-Orthogonal moment functions. To that end, we study the effect of the first and second
step estimation separately. This will allow us to construct separate automatic estimators of the
nuisance parameters in first and second step Influence Functions (IF).
We begin by introducing some additional concepts and notation. Let Fdenote a possible cdf for
a data observation W. We denote by g(F) the probability limit of an estimator Ë† gof the first step
when the true distribution of WisF, i.e., under general misspecification (see Newey, 1994). That
is,Fis unrestricted except for regularity conditions such as existence of g(F) or the expectation of
certain functions of the data. For example, if Ë† g(z) is a nonparametric estimator of E[D|Z=z] then
g(F)(z) =EF[D|Z=z] is the conditional expectation function when Fis the true distribution of
W, denoted by EF,which is well defined under the regularity condition that EF[|D|] is finite. We
assume that g(F) is identified as the solution in gto
EF[Î´1(Z)(Dâˆ’g(Z))] = 0 for all Î´1âˆˆâˆ†1.
Hence, we have that g(F0) =g0,consistent with g0being the probability limit of Ë† gwhen F0is the
cdf of W.
To study the effect of the second step, suppose that Wis distributed according to F. However,
the first step parameter is independently fixed to g. Let h(F, g) be the solution in hâˆˆâˆ†2(g) to
EF[Î´2(X, V(g)){Yâˆ’h(X, V(g))}] = 0 for all Î´2âˆˆâˆ†2(g), (2.4)
where V(g)â‰¡Ï†(D, Z, g ) and âˆ† 2(g) is a linear and closed subspace of L2(X, V(g)). The solu-
tion of the above equation is a function of ( x, v):h(F, g)(x, v). We have that h(F0, g0) = h0.
Thus, henceforth, a subindex of 0 in hmeans the conditioning variable is Vâ‰¡V(g0), for example
h0(x, Ï†(d, z, g )) =E[Y|X=x, V =Ï†(d, z, g )] in the nonparametric case. We may think of the
mapping h(F, g) as the probability limit of an estimator of h0under the following conditions: (i)
the true distribution of WisFand (ii) the estimator is built with the first step parameter fixed to
8g. A feasible estimator Ë†hofh0will, however, rely on the estimator Ë† g. Therefore, we assume that
the probability limit of Ë†hunder general misspecification is h(F, g(F)).
To introduce orthogonal moments, let Hbe some alternative distribution that is unrestricted
except for regularity conditions, and FÏ„â‰¡(1âˆ’Ï„)F0+Ï„HforÏ„âˆˆ[0,1].We assume that His chosen
so that g(FÏ„) and h(FÏ„, g(FÏ„)) exist for Ï„small enough, and possibly other regularity conditions are
satisfied. The IF that corrects for both first and second step estimation , as introduced in CEINR,
is the function Ï•(w, g, h, Î±, Î¸ ) such that
d
dÏ„E[m(W, g(FÏ„), h(FÏ„, g(FÏ„)), Î¸)] =Z
Ï•(w, g 0, h0, Î±0, Î¸)dH(w),
E[Ï•(W, g 0, h0, Î±0, Î¸)] = 0 ,andE[Ï•(W, g 0, h0, Î±0, Î¸)2]<âˆ,(2.5)
for all Hand all Î¸.Here Î±is an unknown function, additional to ( g, h), on which only the IF depends.
The â€œtrue parameterâ€ Î±0is the Î±such that equation (2.5) is satisfied. Throughout the paper, d/dÏ„
is the derivative from the right (i.e. for non-negative values of Ï„) atÏ„= 0.Equation (2.5) is the
Gateaux derivative characterization of the IF of the functional Â¯ m(g(F), h(F, g(F)), Î¸), with
Â¯m(g, h, Î¸ )â‰¡E[m(W, g, h, Î¸ )].
Orthogonal moment functions can be constructed by adding this IF to the original identifying
moment functions to obtain
Ïˆ(w, g, h, Î±, Î¸ )â‰¡m(w, g, h, Î¸ ) +Ï•(w, g, h, Î±, Î¸ ). (2.6)
This vector of moment functions has two key orthogonality properties. First, we have that varying
(g, h) away from ( g0, h0) has no effect, locally, on E[Ïˆ(W, g, h, Î± 0, Î¸)]. The second property is that
varying Î±will have no effect, globally, on E[Ïˆ(W, g 0, h0, Î±, Î¸)]. These properties are shown in great
generality in CEINR.
The IF in equation (2.5) measures the effect that the first step (estimation of g0) and the second
step (estimation of h0) will have on the moment condition. By the chain rule, we have that these
two effects can be studied separately:
d
dÏ„Â¯m(g(FÏ„), h(FÏ„, g(FÏ„)), Î¸) =d
dÏ„Â¯m(g(FÏ„), h(F0, g(FÏ„)), Î¸)
+d
dÏ„Â¯m(g0, h(FÏ„, g0), Î¸).(2.7)
In the above display, the first derivative in the right hand side (RHS) accounts for the first
step. As in Hahn and Ridder (2013), the first step affects the moment condition in two ways (see
Figure 1). We have a direct impact on Â¯m, which includes the effect of evaluating hon the generated
regressor. We also have an indirect effect on the moment that comes from gaffecting estimation of
h0in the second step (through conditioning). This is present in the term h(F0, g(FÏ„)). Both effects
9(direct and indirect) are considered in (2.7). The second derivative in (2.7) accounts for the effect
of the second step. This effect is independent of the first step and, as such, considers that g0is
known. This is captured by h(FÏ„, g0).
Ï„ FÏ„ g(FÏ„) h(FÏ„, g(FÏ„))
Â¯m(g(FÏ„), h(FÏ„, g(FÏ„)), Î¸)(E)
(2S)(D)
Figure 1The effect of a deviation FÏ„on the moment condition. (2S) represents the second step effect.
(D) represents the direct effect of the first step. The path (E)-(2S) represents the indirect estimation effect
of the first step.
We may then find an IF corresponding to each step: Ï•1(w, g, Î± 1, Î¸) and Ï•2(w, h, Î± 2, Î¸), respec-
tively. The IFs satisfy, for all Î¸andH:
d
dÏ„Â¯m(g(FÏ„), h(F0, g(FÏ„)), Î¸) =Z
Ï•1(w, g 0, Î±01, Î¸)dH(w) and (2.8)
d
dÏ„Â¯m(g0, h(FÏ„, g0), Î¸) =Z
Ï•2(w, h 0, Î±02, Î¸)dH(w), (2.9)
on top of the zero mean and square integrability conditions (see equation (2.5)). We therefore
have that the IF accounting for both the first and second step is Ï•(w, g, h, Î±, Î¸ ) =Ï•1(w, g, Î± 1, Î¸) +
Ï•2(w, h, Î± 2, Î¸), with Î±= (Î±1, Î±2) . The true values are denoted by Î±0= (Î±01, Î±02).
We now provide separate orthogonality conditions that will serve as a basis for the auto-
matic estimation of the nuisance parameters Î±01andÎ±02. Define the following moment condi-
tions: Ïˆ1(w, g, Î± 1, Î¸)â‰¡m(w, g, h (F0, g), Î¸) +Ï•1(w, g, Î± 1, Î¸) for the first step, and Ïˆ2(w, h, Î± 2, Î¸)â‰¡
m(w, g 0, h, Î¸) +Ï•2(w, h, Î± 2, Î¸) for the second step. Applying separately Theorem 1 in CEINR to Ïˆ1
andÏˆ2one gets
d
dÏ„E[Ïˆ1(W, g(FÏ„), Î±1(FÏ„), Î¸)] = 0 andd
dÏ„E[Ïˆ2(W, h(FÏ„, g0), Î±2(FÏ„), Î¸)] = 0 .
Since âˆ† 1and âˆ† 2(g0) are linear, the above equations mean that, for all Î¸âˆˆÎ˜,
d
dÏ„E[Ïˆ1(W, g 0+Ï„Î´1, Î±01, Î¸)] = 0 for all Î´1âˆˆâˆ†1and
d
dÏ„E[Ïˆ2(W, h 0+Ï„Î´2, Î±02, Î¸)] = 0 for all Î´2âˆˆâˆ†2(g0).(2.10)
10This result comes from applying Theorem 3 in CEINR separately to each step. Here Î´1represents a
possible direction of deviation of g(F) from g0. In turn, Î´2represents a possible deviation of h(F, g 0)
from h0. The parameter Ï„is the size of a deviation. The innovation with respect to CEINR is that
we can compute the IF Ï•by separately studying Ïˆ1andÏˆ2, corresponding to the first and second
steps, respectively. This means we can separately identify Î±01andÎ±02from (2.10). To the best of
our knowledge, this separate identification result argument is new to this paper.
Remark 2.2(Doubly Robust and Locally Robust) A Doubly Robust estimator (with
respect to (w.r.t.) the second step h0is based on the moment E[Ïˆ2(W, h 0, Î±02, Î¸)] = 0 when
E[Ïˆ2(W, h, Î± 02, Î¸)] is affine in h. To provide intuition, consider a simpler setting where the parameter
of interest is a moment linear in the second step, such that
Î¸0=E[m(W, h 0)] =E[Î±02(X, V)h0(X, V)].
CEINR shows that a LR moment for Î¸0, not accounting for the first step, is
Ïˆ2(w, h, Î± 2, Î¸) =Î¸âˆ’m(w, h)âˆ’Î±2(x, v)(yâˆ’h(x, v)).
This is a Doubly Robust moment w.r.t. hin the sense that
E[Ïˆ2(W, h, Î± 2, Î¸)] =Î¸âˆ’Î¸0+Î¸0âˆ’E[Î±02(X, V)h(X, V)]âˆ’E[Î±2(X, V){h0(X, V)âˆ’h(X, V)}]
=Î¸âˆ’Î¸0+E[Î±02(X)h0(X, V)]âˆ’E[Î±02(X, V)h(X, V)]
+E[Î±2(X, V){h(X;V)âˆ’h0(X, V)}]
=Î¸âˆ’Î¸0+E[{Î±2(X, V)âˆ’Î±02(X, V)}{h(X, V)âˆ’h0(X, V)}].
From this fundamental equation in CEINR, the doubly robust property follows. We only need Î±2
orhto be correctly specified to identify the parameter of interest. The estimator based on Ïˆ2does
not account for the estimation of the generated regressors, and as such, is not fully locally robust.
For the latter, the estimator needs to be based on the Ïˆgiven in (2.6) above. â– 
3 Automatic estimation of the nuisance parameters
The debiased moments require a consistent estimator Ë† Î±of the nuisance parameters Î±0â‰¡(Î±01, Î±02).
When the form of Î±0is known, one can plug-in nonparametric estimators of the unknown com-
ponents of Î±0to form Ë† Î±. In the generated regressors setup, however, the nuisance parameters
(especially Î±01) have a complex analytical shape (see the result in equation (B.6) in the Appendix,
the examples in Section 3.1, and Hahn and Ridder, 2013). Therefore, the plug-in estimator for Ë† Î±
may be cumbersome to compute in practice.
We propose an alternative approach which uses the orthogonality of Ïˆ1andÏˆ2with respect to
gandh, respectively, to construct estimators of ( Î±01, Î±02). This approach does not require knowing
11the form of Î±0. It is â€œautomaticâ€ in only requiring the orthogonal moment functions and data for
the construction of Ë† Î±. Moreover, an automatic estimator can be constructed separately for each
step. For more details, we refer to Section 3.2.
This section shows that, under some assumptions, the correction term takes to form:
Ï•(w, g 0, h0, Î±0, Î¸) =Î±01(z)Â·[dâˆ’g0(z)]| {z }
=Ï•1(w,g0,Î±01,Î¸)+Î±02(x, Ï†(d, z, g 0))Â·[yâˆ’h0(x, Ï†(d, z, g 0))]| {z }
=Ï•2(w,h0,Î±02,Î¸). (3.1)
That is, each correction term is built by multiplying the nuisance parameter by each stepâ€™s prediction
error. An important ingredient for the automatic construction is a consistent estimator of the
linearization of the moment condition with respect to each parameter ( gfor the first step and hfor
the second). Section 3.1 provides the formal development.
3.1 First and Second Step Linearization
We start with the linearization of the second step effect. This result is well established in the
literature (see, e.g., Newey, 1994, Equation 4.1) and will follow immediately if Â¯ m(g0, h, Î¸) is linear
inh.
Before introducing the result, we note that throughout this section (i) Ï„7â†’hÏ„denotes a dif-
ferentiable path, i.e., 0 7â†’h0anddhÏ„/dÏ„exists (equivalently for gÏ„) and (ii) His regular in the
sense that, for FÏ„â‰¡(1âˆ’Ï„)F0+Ï„H,g(FÏ„) is a differentiable path in L2(Z), and h(FÏ„, g0) and
h(F0, g(FÏ„)) are differentiable paths in L2(X, V).
3.1.1 Second Step Linearization
We assume that Â¯ mcan be linearized with respect to the second step parameter:
Assumption 3.1 There exists a function D2(w, h) such that
dÂ¯m(g0, hÏ„, Î¸)
dÏ„=dE[D2(W, h Ï„)]
dÏ„
for every Î¸âˆˆÎ˜. Moreover, h7â†’E[D2(W, h)] is linear and continuous in L2(X, V).
The same assumption has been considered in Newey (1994). We can then get the shape of the
second step IF:
Pr oposition 3.1 Under Assumption 3.1:
(Lin) We can linearize the effect of the second step estimation:
d
dÏ„Â¯m(g0, h(FÏ„, g0), Î¸) =d
dÏ„E[D2(W, h(FÏ„, g0))].
12(IF) There exists an Î±02âˆˆâˆ†2(g0)such that the function
Ï•2(w, h 0, Î±02, Î¸) =Î±02(x, Ï†(d, z, g 0))Â· {yâˆ’h0(x, Ï†(d, z, g 0))},
satisfies equation (2.8) and is thus the Second Step IF.
The shape of the second step nuisance parameter Î±02is given in equation (B.1) in the Appendix.
We note that, since Â¯ mis linearized at ( g0, h0, Î¸),D2(and also Î±02) may also depend on ( g0, h0, Î¸).
This is omitted for notational simplicity but will become relevant to construct feasible automatic
estimators (see Section 4). We now verify the linearization of Â¯ m(g0, h, Î¸) in some examples:
Example 1(continuing from p. 6) Assumption 3.1 is easy to check for the CASF. Since
m(w, g 0, h, Î¸) is already linear in h, we have that
D2(w, h) =Z
h(xâˆ—, Ï†(d, z, g 0))dFâˆ—(xâˆ—).
In this case, we can compute the analytic shape of the correction term nuisance parameter Î±02.
To find it, we follow PÂ´ erez-Izquierdo (2022) and assume the existence of densities fâˆ—,fv
0and,fxv
0for
Fâˆ—,Fv
0andFxv
0, respectively. Here Fv
0andFxv
0denote the distribution under F0ofVand ( X, V),
respectively. We then have that
E[D2(W, h)] =Z
h(xâˆ—, v)fâˆ—(xâˆ—)fv
0(v)dxâˆ—dv=Zfâˆ—(xâˆ—)fv
0(v)
fxv
0(xâˆ—, v)h(xâˆ—, v)fxv
0(xâˆ—, v)dxâˆ—dv
=E[r2(X, V)h(X, V)],
with r2(x, v)â‰¡fâˆ—(x)fv
0(v)/fxv
0(x, v). Thus, a sufficient condition for Assumption 3.1 is that r2is
square-integrable. In the nonparametric case, i.e. âˆ† 2(g0) =L2(X, V), it follows that Î±02=r2. Note
that, even if we have found the nuisance parameter Î±02, it has a rather complex shape. It depends
on the density of the generated regressor Vand on the joint density of ( X, V). For semiparametric
specifications of the second step, such as the partially linear model, the nuisance parameter Î±02is the
orthogonal projection of r2onto âˆ† 2(g0). These objects are generally hard to estimate and may cause
the plug-in estimator for Î±02to behave poorly. We advocate automatic estimation (Section 3.2) as
a potential solution to this issue. â– 
Example 2(continuing from p. 7) We linearize the moment condition in h. To do it, we as-
sume that Î·(w, s) is differentiable w.r.t. s. In that case, as long as we can interchange differentiation
and integration:
d
dÏ„Â¯m(g0, hÏ„, Î¸) =Ed
dÏ„Î·(W, h Ï„(X, g 0(Z)))
=Eâˆ‚Î·
âˆ‚s(W, h 0(X, g 0(Z)))d
dÏ„hÏ„(X, g 0(Z))
=d
dÏ„Eâˆ‚Î·
âˆ‚s(W, h 0(X, g 0(Z)))hÏ„(X, g 0(Z))
,
13so that
D2(w, h) =âˆ‚Î·(w, h 0(x, g0(z)))/âˆ‚sÂ·h(x, g0(z)).
Letr2denote the conditional expectation of âˆ‚Î·(W, h 0(X, g 0(Z)))/âˆ‚sgiven ( X, V). In the fully
non-parametric case, the second step nuisance parameter is Î±02=r2. More generally, Î±02is the
orthogonal projection of r2onto âˆ† 2(g0). â– 
3.1.2 First Step Linearization
We now move to linearize the first step effect. Note that if the chain rule can be applied:
d
dÏ„Â¯m(g(FÏ„), h(F0, g(FÏ„)), Î¸) =d
dÏ„Â¯m(g(FÏ„), h0, Î¸)
+d
dÏ„Â¯m(g0, h(F0, g(FÏ„)), Î¸).(3.2)
The first derivative in the RHS can be easily analyzed if we linearize Â¯ m(g, h0, Î¸) ing:
Assumption 3.2 There exists a function D11(w, g) such that
dÂ¯m(gÏ„, h0, Î¸)
dÏ„=dE[D11(W, g Ï„)]
dÏ„
for every Î¸âˆˆÎ˜. Moreover, g7â†’E[D11(W, g)] is linear and continuous in L2(Z).
To study dÂ¯m(g0, h(F0, g(FÏ„)), Î¸)/dÏ„we generalize the key Lemma 1 in Hahn and Ridder (2013)
to allow for semiparametric second steps as in equation (2.2):
Lemma 3.1 Assume that the chain rule can be applied along the path Ï„7â†’gÏ„. Then, for every
Î´2âˆˆL2(X, V)satisfying that there exists an Îµ >0such that Î´2âˆˆ âˆ© Ï„<Îµâˆ†2(gÏ„):
d
dÏ„E[Î´2(X, V)Â·h(F0, gÏ„)(X, V)] =âˆ’d
dÏ„E[Î´2(X, V)Â·h0(X, V(gÏ„))]
+d
dÏ„E[Î´2(X, V(gÏ„))Â·(Yâˆ’h0(X, V))].
The condition that the function Î´2belongs to every set âˆ†( g) for gclose to g0is related to
â€œregularityâ€ of âˆ† 2(g). If the functions in the sets âˆ† 2(g) have the same shape, one would expect
that many Î´2â€™s satisfy the condition in the above lemma. The condition allows to take derivatives
in equation (2.4) along the path ( F, g) = (F0, gÏ„).
To linearize the first-step, we ask the second step nuisance parameter Î±02to satisfy the condition
in Lemma 3.1. We should also impose some additional assumptions on the paths Ï„7â†’h(F0, gÏ„).
This allows us to express dÂ¯m(g0, h(F0, g(FÏ„)), Î¸)/dÏ„as an inner product.
Assumption 3.3 For every path Ï„7â†’gÏ„there exits an Îµ >0 such that
a.Î±02âˆˆ âˆ© Ï„<Îµâˆ†2(gÏ„), and
14b.h(F0, gÏ„)âˆˆâˆ†2(g0) for all Ï„ < Îµ .
As we have emphasized, this assumption is related to â€œregularityâ€ in the shape of the functions
in âˆ† 2(g). In both the non-parametric case âˆ† 2(g) =L2(X, V(g)) and the partly linear case âˆ† 2(g) =
{Î²â€²x+Îº(v):Î²âˆˆRp, ÎºâˆˆL2(V(g))}the assumption translates into square-integrability conditions
(see Remark 3.2 below). What Assumption 3.3 rules out is to specify a partly linear model for some
gâ€™s and a non-parametric regression for others.
Once we can apply Lemma 3.1, the remaining step is to linearize the terms h0(X, Ï†(D, Z, g (FÏ„)))
andÎ±02(X, Ï†(D, Z, g (FÏ„))). To achieve this, we require h0,Î±0, and Ï†to be differentiable in the
appropriate sense:
Assumption 3.4 h0(x, v) and Î±02(x, v) are a.s. differentiable w.r.t. v. Moreover, the function
Ï†(d, z, g ), understood as a mapping g7â†’Ï†(d, z, g ) from L2(Z) toL2(D, Z), is Hadamard differen-
tiable at g0, with derivative DÏ†.
The Hadamard derivative of Ï†is a linear and continuous map DÏ†:L2(Z)â†’L2(D, Z) such that
d
dÏ„Ï†(d, z, g Ï„) =d
dÏ„DÏ†gÏ„.
In many applications, either Ï†(d, z, g ) =g(z) (first step prediction) or Ï†(d, z, g ) =dâˆ’g(z) (first
step residual). In those cases, DÏ†g=gorDÏ†g=âˆ’g, respectively.
The next theorem gives the shape of the first step IF:
Theorem 3.1 Under Assumptions 3.1-3.4:
(Lin) The function
D1(w, g)â‰¡D11(w, g) +âˆ‚
âˆ‚v[Î±02(x, v)(yâˆ’h0(x, v))]Â·DÏ†g, (3.3)
where the derivative is evaluated at v=Ï†(d, z, g 0), satisfies
d
dÏ„Â¯m(g(FÏ„), h(F0, g(FÏ„)), Î¸) =d
dÏ„E[D1(W, g(FÏ„))].
(IF) There exists an Î±01âˆˆâˆ†1such that the function
Ï•1(w, g 0, Î±01, Î¸) =Î±01(z)Â· {dâˆ’g0(z)},
satisfies equation (2.9) and is thus the First Step IF.
The shape of the first step nuisance parameter Î±01is given in equation (B.6) in the Appendix. It
generally is a rather complex function. Indeed, the linearization with respect to the first step effect
is also complex (see its definition in equation (3.3)). The first term is standard and corresponds to
the linearization of the direct effect of g. It is given by D11, the linearization of dÂ¯m(gÏ„, h0, Î¸)/Ï„. The
15second term corresponds to the indirect effect. Consistent estimation of the second term generally
requires estimators for (i) g0, (ii) h0, (iii) âˆ‚h0/âˆ‚v, (iv) Î±02, and (v) âˆ‚Î±02/âˆ‚v. Section 4 provides the
details on how to estimate E[D1(W, g)]. We also note that Î±01is generally different from zero, and
hence, not accounting for the first step effect in the asymptotic variance leads to invalid inferences.
Remark 3.1(Influence function under an index restriction) We can expand the
derivative in equation (3.3) to get
D1(w, g) =D11(w, g) +âˆ‚Î±02
âˆ‚v(x, v)(yâˆ’h0(x, v))âˆ’âˆ‚h0
âˆ‚v(x, v)Î±02(x, v)
Â·DÏ†g
This expression is simplified if we impose the index restriction E[Y|D, Z] =E[Y|X, V], or more
generally, if ( Yâˆ’h0(X, V)) is orthogonal to âˆ‚Î±02(X, V)/âˆ‚vÂ·DÏ†g(FÏ„)(D, Z). Indeed, since DÏ†gis
a function of ( D, Z), by the Law of Iterated Expectations:
d
dÏ„Â¯m(g(FÏ„), h(F0, g(FÏ„)), Î¸) =d
dÏ„E
D11(W, g(FÏ„))
+âˆ‚Î±02
âˆ‚v(X, V)(yâˆ’h0(X, V))âˆ’âˆ‚h0
âˆ‚v(X, V)Î±02(X, V)
DÏ†g(FÏ„)
=d
dÏ„E
D11(W, g(FÏ„))âˆ’âˆ‚h0
âˆ‚v(X, V)Î±02(X, V)DÏ†g(FÏ„)
.
That is, the term depending on the derivative of Î±02is no longer present. This result was highlighted
in Remark 1 in Hahn and Ridder (2013), see also Hahn et al. (2023). â– 
Remark 3.2(About Assumption 3.3) Assumption 3.3 was missing in the fundamental Lemma
1 of Hahn and Ridder (2013). When the second step is non-parametric or partly linear, As-
sumption 3.3 reduces to square-integrability conditions. For the non-parametric case (âˆ† 2(g) =
L2(X, V(g))), the assumption requires that Î±02âˆˆL2(X, V(gÏ„)) and h(F0, gÏ„)âˆˆL2(X, V) for small
Ï„. That is, for all 0 â‰¤Ï„ < Îµ ,
Z
Î±02(x, Ï†(d, z, g Ï„))2dF0(w)<âˆandZ
h(F0, gÏ„)(x, Ï†(d, z, g 0))2dF0(w)<âˆ.
Assumption 3.3 also leads to similar requirements in the partly linear case, where âˆ† 2(g) =
{Î²â€²x+Îº(v):Î²âˆˆRp, ÎºâˆˆL2(V(g))}. Note that, since Î±02âˆˆâˆ†2(g0), we have that Î±02(x, v) =
Î²â€²
0x+Îº0(v) for Î²0âˆˆRpandÎº0âˆˆL2(V). Then Î±02âˆˆâˆ†2(gÏ„)â‡â‡’ Îº0âˆˆL2(V(gÏ„)). In turn, since
h(F0, gÏ„)âˆˆâˆ†2(gÏ„), we have that h(F0, gÏ„)(x, v) =Î²â€²
Ï„x+ÎºÏ„(v) for Î²Ï„âˆˆRpandÎºÏ„âˆˆL2(V(gÏ„)).
Therefore, Assumption 3.3.b imposes ÎºÏ„âˆˆL2(V).
The next proposition gives primitive sufficient conditions for Assumption 3.3.a:
Pr oposition 3.2 Under Assumption 3.4, if âˆ‚Î±02(x, v)/âˆ‚vis bounded, then Assumption 3.3.a is
satisfied in the non-parametric and partly linear cases.
16We can give sufficient conditions for Assumption 3.3.b in terms of smoothness of the distribution
of the data as gÏ„approaches g0. Let Fxv
Ï„andFv
Ï„be the distributions of ( X, V(gÏ„)) and V(gÏ„),
respectively. Note that Fxv
0andFv
0denote the distributions of ( X, V) and V, respectively. Then:
Pr oposition 3.3 Assumption 3.3.b is satisfied in the non-parametric case under the following
conditions:
1.E[Y4]<âˆ,
2. there exists an Îµ >0such that, for Ï„ < Îµ ,Fxv
Ï„andFxv
0are equivalent measures (absolutely
continuous between each other), and
3.E[Î½Ï„(X, V)]<âˆ, being Î½Ï„the Radon-Nikodym density of Fxv
0w.r.t. Fxv
Ï„.
Moreover, Assumption 3.3.b is satisfied in the partly linear case if Condition 1 is replaced by:
Condition 1âˆ—.E[YrXs]<âˆ, for every s, râˆˆNsatisfying s+r= 4, and Conditions 2-3 hold with
Fv
Ï„replacing Fxv
Ï„.
â– 
Remark 3.3(General Parametric First Steps) Letgbe a general finite dimensional
parameter, with g0denoting the true value, and let Ë† gbe an estimator for g0satisfying
âˆšn(Ë†gâˆ’g0)) =1âˆšnnX
i=1Ïˆ(Di, Zi) +oP(1).
Then, all our previous results apply with the adjustment term
Ï•1(w, g 0, Î±01, Î¸) =Î±01Â·Ïˆ(d, z),
where Î±01=âˆ‚E[D1(W, g 0)]/âˆ‚g. â– 
We conclude the section by finding D1for several examples:
Example 1(continuing from p. 6) The Control Function setup used in this paper satisfies
Assumption 3.3. In the nonparametric case, h0(X, Ï†(D, Z, g 0)) =E[Y|D, Z] and the simplification
of Remark 3.1 applies.
Moreover, the Control Function approach we follow here uses the residual of the first step to
control for potential endogeneity. Thus, Ï†(d, z, g ) =dâˆ’g(z) and its linearization is DÏ†g=âˆ’g.
Provided that h0is differentiable w.r.t. v(Assumption 3.4), this allows us to linearize, w.r.t. g, the
17moment condition defining the CASF. We have that:
d
dÏ„Â¯m(gÏ„, h0, Î¸) =d
dÏ„EZ
h0(xâˆ—, Ï†(D, Z, g Ï„))dFâˆ—(xâˆ—)âˆ’Î¸
=EZd
dÏ„h0(xâˆ—, Ï†(D, Z, g Ï„))dFâˆ—(xâˆ—)
=EZâˆ‚h0
âˆ‚v(xâˆ—, Ï†(D, Z, g 0))d
dÏ„Ï†(D, Z, g Ï„)dFâˆ—(xâˆ—)
=d
dÏ„E
âˆ’Zâˆ‚h0
âˆ‚v(xâˆ—, Ï†(D, Z, g 0))dFâˆ—(xâˆ—)gÏ„(Z)
.
This means that the linearization of the moment condition w.r.t. gis, with some abuse of notation,
D11(w, g) =D11(d, z)g(z), with
D11(d, z)â‰¡ âˆ’Zâˆ‚h0
âˆ‚v(xâˆ—, dâˆ’g0(z))dFâˆ—(xâˆ—).
We can now plug in the expression for D11into equation (3.3), where the linearization of the
first step effect is defined. Recall that DÏ†g=âˆ’g. Then, for the CASF, equation (3.3) becomes
D1(w, g)â‰¡
D11(d, z) +âˆ‚h0
âˆ‚v(x, v)Î±02(x, v)
g(z).
As discussed above, the linearization depends on h0andÎ±02, and the derivative of h0w.r.t. v. It
also depends on g0, asvâ‰¡dâˆ’g0(z). Section 3.2 discusses how to construct an automatic estimator
for the first step nuisance parameter Î±02. Finding an estimator of the derivative of h0will depend
on the estimator at hand. In Section 4 we propose a numerical derivative approach that works for
a variety of second step estimators, such as Random Forest. â– 
Example 2(continuing from p. 13) Theorem 3.1 generalizes Theorem 5 in Hahn and Ridder
(2013) to allow for (i) arbitrary functionals Â¯ m(g, h, Î¸ ) that are Hadamard differentiable w.r.t. g
andh, (ii) semiparametric first and second steps, and (iii) generated regressors given by arbitrary
Hadamard differentiable functions Ï†.
We show how the expression for D1simplifies to that in Hahn and Ridder (2013, Th. 5). We
start by linearizing Â¯ m(g, h0, Î¸) w.r.t. g. Note that Hahn and Ridder (2013), in the non-parametric
case, fix Ï†(d, z, g ) =g(z). Then, DÏ†g=g. On top of Î·being differentiable w.r.t. s, we require h0
to be differentiable w.r.t. v(Assumption 3.4). Then:
d
dÏ„Â¯m(gÏ„, h0, Î¸) =Eâˆ‚Î·
âˆ‚s(W, h 0(X, g 0(Z)))d
dÏ„h0(X, g Ï„(Z))
=Eâˆ‚Î·
âˆ‚s(W, h 0(X, g 0(Z)))âˆ‚h0
âˆ‚v(X, g 0(Z))d
dÏ„gÏ„(Z)
=d
dÏ„Eâˆ‚Î·
âˆ‚s(W, h 0(X, g 0(Z)))âˆ‚h0
âˆ‚v(X, g 0(Z))gÏ„(Z)
,
18and therefore D11(w, g) =âˆ‚Î·(w, h 0(x, g0(z)))/âˆ‚sÂ·âˆ‚h0(x, g0(z))/âˆ‚vÂ·g(z).
Recall from the previous discussion that the Second Step nuisance parameter satisfies:
Î±02(x, v) =Eâˆ‚Î·
âˆ‚s(W, h 0(X, g 0(Z)))X=x, g0(Z) =v
.
So, if we denote Î¾(w)â‰¡âˆ‚Î·(w, h 0(x, g0(z)))/âˆ‚s, equation (3.3) becomes:
D1(w, g)â‰¡
(yâˆ’h0(x, v))Â·âˆ‚Î±02
âˆ‚v(x, v) + (Î¾(w)âˆ’Î±02(x, v))Â·âˆ‚h0
âˆ‚v(x, v)
g(z),
where vâ‰¡g0(z). This is the result in Hahn and Ridder (2013, Th. 5).
Moreover, note that Î±02(x, v) =E[Î¾(W)|X=x, V=v]. Then, if Î¾is only a function of ( x, v),
the second term in the above equation is zero. This is the case in Theorem 2 in Hahn and Ridder
(2013). There, Î·:Râ†’R, and therefore, Î¾(w) =âˆ‚Î·(h0(x, v))/âˆ‚sis a function of ( x, v). â– 
3.2 Building the automatic estimators
We can obtain automatic estimators for Î±0from the linearization results in the previous section.
We start with the procedure to automatically estimate Î±02, the nuisance parameter of the Second
Step IF. We want to stress, nevertheless, that the procedure is quite general. Indeed, we will also
apply it, mutatis mutandis , to the estimation of the nuisance parameter in the first step.
The starting point is to expand the second equation in (2.10). For Î´2âˆˆâˆ†2(g0),
d
dÏ„Â¯m(g0, h0+Ï„Î´2, Î¸) +d
dÏ„E[Ï•2(W, h 0+Ï„Î´2, Î±02, Î¸)] = 0 . (3.4)
We will now combine the above equation with the linearization result in Proposition 3.1. By
continuity and linearity of D2, we have that
d
dÏ„Â¯m(g0, h0+Ï„Î´2, Î¸) =d
dÏ„E[D2(W, h 0+Ï„Î´2)] =E[D2(W, Î´ 2)],for any Î´2âˆˆâˆ†2(g0). (3.5)
Moreover, Proposition 3.1 gives us that Ï•2=Î±02(yâˆ’h0). Thus, for any Î´2âˆˆâˆ†2(g0),
d
dÏ„E[Ï•2(W, h 0+Ï„Î´2, Î±02, Î¸)] =âˆ’E[Î´2(D, Z)Î±02(X, V)]. (3.6)
From equations (3.4)-(3.6), for each Î´2âˆˆâˆ†2(g0),
E[D2(W, Î´ 2)]âˆ’E[Î´2(D, Z)Î±02(X, V)] = 0 ,for each Î´2âˆˆâˆ†2(g0). (3.7)
We now assume that there is a dictionary ( bj)âˆ
j=1whose closed linear span is âˆ† 2(g0). That
is, any function in âˆ† 2(g0) can be approximated, in the L2sense, by a linear combination of bjâ€™s.
Then, there exists a sequence of real numbers ( Ï0j)âˆ
j=1such that Î±02=Pâˆ
j=1Ï0jbj. Thus, Î±02can
19be approximated by bâ€²
JÏ0J, where bJ= (b1, ..., b J)â€²andÏ0J= (Ï01, ..., Ï 0J)â€².1We can now plug in
bâ€²
JÏ0Jinto equation (3.7) for Î´2=bj,j= 1, ..., J . This gives the following Jmoment conditions:
E[bJ(X, V)bJ(X, V)â€²]Ï0J=E[D2(W,bJ)],
where D2(w,bJ)â‰¡(D2(w, b 1), ..., D 2(w, b J))â€².
The above moment conditions can be used to construct an OLS-like estimator of Ï0J. Note,
however, that in high dimensional settings E[bJ(X, V)bJ(X, V)â€²] may be near singular. Therefore,
we rather focus on a regularized estimator. The above Jmoment conditions are the first order
conditions of the minimization problem:
min
ÏJâˆˆRJ{âˆ’2E[D2(W,bJ)â€²]ÏJ+Ïâ€²
JE[bJ(X, V)bJ(X, V)â€²]ÏJ}.
We can regularize the problem by adding a penalty to the above objective function. Let âˆ¥ÏJâˆ¥qâ‰¡
(PJ
j=1|Ïj|q)1/qforqâ‰¥1. For a tunning parameter Î»â‰¥0, we can estimate Ï0Jby minimizing:
min
ÏJâˆˆRJ
âˆ’2E[D2(W,bJ)â€²]ÏJ+Ïâ€²
JE[bJ(X, V)bJ(X, V)â€²]ÏJ+Î»âˆ¥ÏJâˆ¥q
q	
. (3.8)
Forq= 1, the above is the Lasso objective function, while q= 2 corresponds to Ridge Regression.
Additionally, we could consider elastic net type penalties, where Î»(Î¾âˆ¥ÏJâˆ¥2
2+ (1âˆ’Î¾)âˆ¥ÏJâˆ¥1), for
Î¾âˆˆ[0,1], is added to the objective function.
We propose now an automatic estimator of Î±01, the nuisance parameter of the First Step IF.
The procedure is parallel to that proposed above. By Theorem 3.1, we can linearize Â¯ m(g, h(F0, g), Î¸)
byD1(w, g) (see equation (3.3)). Again, we assume that there is a dictionary ( ck)âˆ
k=1that spans
âˆ†1. Thus, Î±01=Pâˆ
k=1Î²0kckfor a sequence of real numbers ( Î²0k)âˆ
k=1. We can therefore construct
Kmoment conditions
E[cK(Z)cK(Z)â€²]Î²0K=E[D1(W,cK)],
where cK= (c1, ..., c K)â€²,Î²0K= (Î²01, ..., Î² 0K)â€², and D1(w,cK)â‰¡(D1(w, c 1), ..., D 1(w, c K))â€². We use
these conditions as a basis to construct the objective function to estimate Î²0K:
min
Î²KâˆˆRK
âˆ’2E[D1(W,cK)â€²]Î²K+Î²â€²
KE[cK(Z)cK(Z)â€²]Î²K+Î»âˆ¥Î²Kâˆ¥q
q	
, (3.9)
where the tuning parameter Î»may be different from that of the second step.
From the above discussion, we conclude that automatic estimation of the first and second step
nuisance parameters reduces to finding a consistent estimator of E[D2(W,bJ)] and E[D1(W,cK)].
We note that, in general, both D2andD1depend on ( g0, h0, Î¸). In the sample moment conditions,
these are replaced by cross-fit estimators (Section 4.1).
Furthermore, D1may additionally depend on âˆ‚h0/âˆ‚v, the nuisance parameter of the Second
Step Î±02and its derivative âˆ‚Î±02/âˆ‚v(see equation (3.3)). Estimation of âˆ‚h0/âˆ‚vis discussed in
1For a d1Ã—d2matrix A,Aâ€²denotes its transpose. In this respect, vectors are considered d1Ã—1 matrices.
20Section 4.1. Here, we sketch a possible approach to estimate the derivative of Î±02. Recall that the
Second Step nuisance parameter can be approximated by bâ€²
JÏ0J. We may assume that the atoms
bj(x, v) are differentiable w.r.t. v. We can then replace the nuisance parameter by its approximation
bâ€²
JÏ0Jand its derivative by ( âˆ‚bJ/âˆ‚v)â€²Ï0Jin equation (3.3). We illustrate the results of this section
with the Partially Linear model for the CASF.
Example 1(continuing from p. 7) We illustrate how some simplifications for estimating the
linearizations may occur in semiparametric settings with the partially linear model and the CASF.
The zero mean restriction of the nonparametric component in the partial linear specification implies
that
E[D2(W, Î´ 2)] =Î²â€²Eâˆ—[X].
forÎ´2(x, v) =Î²â€²x+Îº(v). Therefore, if we select a dictionary bJ= (b1, ..., b J)â€²such that the first
pcomponents ( b1, ..., b p) =Xand ( bp+1, ..., b J) are functions of v, then, the linear approximations
necessary for the automatic estimation of Î±02are known in this example, with
E[D2(W, X )] =Eâˆ—[X]
E[D2(W, b j)] = 0 ,forj=p+ 1, . . . , J.
The Second Step nuisance parameter Î±02can be approximated by bâ€²
JÏ0J, with Ï0Jsolving (3.8).
Likewise, the expression for the linearization w.r.t. the first step simplifies to
E[D1(W, c k)] =E[(Î±02(X, V)âˆ’1) Ë™Îº0(V)ck(Z)],
where Ë™ Îº0(v) =âˆ‚h0/âˆ‚vcan be approximated by ( âˆ‚bJ/âˆ‚v)â€²Î·0Jwhen h0is approximated by bâ€²
JÎ·0J.
â– 
4 Estimation
In this section, we build debiased sample moment conditions for GMM estimation of Î¸. Debiased
sample moments are based in the orthogonal moment function Ïˆin equation (2.6). The IF Ï•that
corrects for both the first and second step estimation is Ï•=Ï•1+Ï•2, the sum of the First and
Second Step IFs. Its shape is given in equation (3.1) (see also Proposition 3.1 and Theorem 3.1).
The full estimation algorithm is summarized in Figure 2 (Section 4.2).
We propose to construct the sample moment conditions using cross-fitting, as in Chernozhukov
et al. (2018). That is, we split the sample so that Ïˆ(Wi, g, h, Î±, Î¸ ) is averaged over observations i
that are not used to estimate ( g, h, Î±, Î¸ ). Cross-fitting (i) eliminates the â€œown observation biasâ€,
helping remainders to converge faster to zero, and (ii) eliminates the need for Donsker conditions
for the estimators of ( g, h, Î± ), which is important for first and second step ML estimators (see
Chernozhukov et al., 2018).
21We partition the sample ( Wi)n
i=1intoLgroups Iâ„“, for â„“= 1, ..., L . For each group, we have
estimators Ë† gâ„“,Ë†hâ„“and Ë†Î±â„“= (Ë†Î±1â„“,Ë†Î±2â„“) that use observations that are not in Iâ„“. We construct automatic
estimators of Î±0satisfying this property in Section 4.1. Moreover, for each group, we consider that
there is an initial estimator of Î¸0, namely ËœÎ¸â„“, which does not use the observations in Iâ„“. CEINR
propose to chose L= 5 for medium size datasets and L= 10 for small datasets.
Following CEINR, debiased sample moment functions are
Ë†Ïˆ(Î¸)â‰¡1
nLX
â„“=1X
iâˆˆIâ„“Ë†Ïˆiâ„“(Î¸), (4.1)
with,
Ë†Ïˆiâ„“(Î¸)â‰¡m(Wi,Ë†gâ„“,Ë†hâ„“, Î¸) + Ë†Î±1â„“(Zi)Â·(Diâˆ’Ë†gâ„“(Zi)) + Ë†Î±2â„“(Xi,Ë†Viâ„“)Â·(Yiâˆ’Ë†hâ„“(Xi,Ë†Viâ„“)), (4.2)
forË†Viâ„“â‰¡Ï†(Di, Zi,Ë†gâ„“). Note that the original moment condition is evaluated at Î¸. On the other
hand, the initial estimators ËœÎ¸â„“are used to construct the correction term Ï•(i.e., to estimate Î±01and
Î±02).
In the general case where there is more than one moment condition, the correction term for each
component of mis constructed following equation (4.2). This means that a different correction term
must be estimated for each component of m(see Section 4.1 for the details about how to proceed
with automatic estimation of each term). We use these debiased moment functions to construct the
debiased GMM estimator:
Ë†Î¸= argmin
Î¸âˆˆÎ˜Ë†Ïˆ(Î¸)â€²Ë†Î¥Ë†Ïˆ(Î¸), (4.3)
where Ë†Î¥ is a positive semi-definite weighting matrix of dimension dim( m)Ã—dim(m). Under some
conditions (see Section 5), the above estimator will be asymptotically normal with the usual GMM
asymptotic variance. Indeed, as in Chernozhukov et al. (2022a), there is no need to account for
estimation of ( g0, h0) and ( Î±01, Î±02) because of orthogonality of Ïˆ.
To introduce the asymptotic variance, let
Mâ‰¡Eâˆ‚m
âˆ‚Î¸(W, g 0, h0, Î¸0)
and
Î¨â‰¡E[Ïˆ(W, g 0, h0, Î±0, Î¸0)Ïˆ(W, g 0, h0, Î±0, Î¸0)â€²],
where âˆ‚m/âˆ‚Î¸ is the dim( m)Ã—dim(Î¸)-dimensional Jacobian matrix. If Ë†Î¥Pâˆ’ â†’Î¥, the asymptotic
variance ofâˆšn(Ë†Î¸âˆ’Î¸0) is Î â‰¡(Mâ€²Î¥M)âˆ’1Mâ€²Î¥â€²Î¨Î¥M(Mâ€²Î¥M)âˆ’1. A consistent estimator of the
asymptotic variance can be build by replacing the terms in Î by their sample analogs:
Ë†Mâ‰¡1
nLX
â„“=1X
iâˆˆIâ„“âˆ‚m
âˆ‚Î¸(Wi,Ë†gâ„“,Ë†hâ„“,ËœÎ¸â„“) and (4.4)
Ë†Î¨â‰¡1
nLX
â„“=1X
iâˆˆIâ„“Ë†Ïˆiâ„“(ËœÎ¸â„“)Ë†Ïˆiâ„“(ËœÎ¸â„“)â€². (4.5)
22As usual in GMM, a choice of Ë†Î¥ that minimizes the asymptotic variance of Ë†Î¸isË†Î¥ = Ë†Î¨âˆ’1. With
that choice of a weighting matrix, the asymptotic variance can be estimated by ( Ë†Mâ€²Ë†Î¨âˆ’1Ë†M)âˆ’1.
We illustrate the theory with the construction of debiased GMM estimator for the CASF:
Example 1(continuing from p. 6) Note that Ï•1=Î±01(dâˆ’g0) and Ï•2=Î±02(yâˆ’h0) (see
Theorem 3.1 and Proposition 3.1, respectively). Thus, obtaining Ë†Ï•is straightforward once we have
cross-fitted estimators for the nuisance parameters (see Section 4.1 for the construction of Ë† Î±1â„“and
Ë†Î±2â„“).
Recall that the moment function defining the CASF is
m(w, g, h, Î¸ ) =Z
h(xâˆ—, Ï†(d, z, g ))dFâˆ—(xâˆ—)âˆ’Î¸.
We take as given that the econometrician has computed cross-fitted estimators for the first and
second steps: Ë† gâ„“andË†hâ„“. Since the counterfactual distribution Fâˆ—is fixed by the econometrician, we
propose a numerical integration approach to obtain the debiased sample moments.
We consider that the econometrician can sample from Fâˆ—. Let ( Xâˆ—
s)S
s=1be a sample of size S
from Fâˆ—. For and observation iâˆˆIâ„“, letË†Viâ„“â‰¡Diâˆ’Ë†gâ„“(Zi). We approximate the value of the moment
function m(Wi,Ë†gâ„“,Ë†hâ„“, Î¸) by
1
SSX
s=1Ë†hâ„“(Xâˆ—
s,Ë†Viâ„“)âˆ’Î¸.
Note that Smay be arbitrarily large (increasing the computational cost), so that the above term
is close to m(Wi,Ë†gâ„“,Ë†hâ„“, Î¸).
Following equations (4.1) and (4.3), the debiased estimator for the CASF is
Ë†Î¸=1
nSLX
â„“=1X
iâˆˆIâ„“SX
s=1Ë†hâ„“(Xâˆ—
s,Ë†Viâ„“) +1
nLX
â„“=1X
iâˆˆIâ„“Ï•(Wi,Ë†gâ„“,Ë†hâ„“,Ë†Î±â„“,ËœÎ¸â„“). (4.6)
The next section develops an automatic estimator for the correction term Ï•. â– 
4.1 Automatic estimation with cross-fitting
Debiased sample moment functions require estimators of the nuisance parameters (Ë† Î±1â„“,Ë†Î±2â„“) for each
group Iâ„“. These estimators must use only observations not in Iâ„“. This section is devoted to the
construction of automatic estimators satisfying this property. Through the section, we consider
that the econometrician has at her disposal first and second step estimators, Ë† gâ„“â„“â€²andË†hâ„“â„“â€², and an
initial estimator, ËœÎ¸â„“â„“â€², that use only observations not in Iâ„“âˆªIâ„“â€²; and estimators (Ë† gâ„“â„“â€²â„“â€²â€²,Ë†hâ„“â„“â€²â„“â€²â€²,ËœÎ¸â„“â„“â€²â„“â€²â€²)
that use only observations not in Iâ„“âˆªIâ„“â€²âˆªIâ„“â€²â€².
The key to automatic estimation of the Second Step nuisance parameter is to find a consistent es-
timator of the linearization of the moment condition. In this section, we will write D2(w, h|g0, h0, Î¸)
23to make explicit that the linearization may depend on ( h0, g0, Î¸) (see Examples 1 and 2). For the
linearization of the effect of first step estimation, we will write D1(w, g|g0, h0, Î±02, Î¸), to emphasize
that it may also depend on the Second Step nuisance parameter. D1generally depends also on the
derivatives âˆ‚h0/âˆ‚vandâˆ‚Î±02/âˆ‚v. We do not make this explicit, but we will address the issue in this
section.
We start with the automatic estimator for the Second Step nuisance parameter. For each â„“, we
provide a sample version of the objective function in (3.8) that uses only observations not in Iâ„“.
Recall that we have a dictionary ( bj)âˆ
j=1that spans âˆ† 2(g0). We estimate E[D2(W,bJ)] by
Ë†D2â„“â‰¡1
nâˆ’nâ„“X
â„“â€²Ì¸=â„“X
iâˆˆIâ„“â€²D2(Wi,bJ|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,ËœÎ¸â„“â„“â€²),
where nâ„“is the number of observations in Iâ„“. In turn, E[bJ(X, Ï†(D, Z, g 0))bJ(X, Ï†(D, Z, g 0))â€²] is
estimated by
Ë†Bâ„“â‰¡1
nâˆ’nâ„“X
â„“â€²Ì¸=â„“X
iâˆˆIâ„“â€²bJ(Xi, Ï†(Di, Zi,Ë†gâ„“â„“â€²))bJ(Xi, Ï†(Di, Zi,Ë†gâ„“â„“â€²))â€².
With this, we can build an automatic estimator of the Second Step nuisance parameter that
only uses observations not in Iâ„“. It is given by Ë† Î±2â„“=bâ€²
JË†ÏJâ„“, where
Ë†ÏJâ„“= argmin
ÏJâˆˆRJn
âˆ’2Ë†Dâ€²
2â„“ÏJ+Ïâ€²
JË†Bâ„“ÏJ+Î»âˆ¥ÏJâˆ¥q
qo
. (4.7)
The tuning parameter Î»can be chosen by cross-validation.
Example 1(continuing from p. 6) We provide the ingredients to conduct an automatic
estimator of Î±02for the CASF. Recall that the moment condition for the CASF was already linear
inhand hence
D2(w, b j|g0, h0, Î¸) =Z
bj(xâˆ—, Ï†(d, z, g 0))dFâˆ—(xâˆ—),
for each atom bjin the dictionary.
We follow the same strategy as before and approximate D2by numerical integration. Let ( Xâˆ—
s)S
s=1
be a sample drawn from Fâˆ—. To construct the objective function to estimate Ë†ÏJâ„“, for an observation
iâˆˆIâ„“â€², we set
D2(Wi, bj|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,ËœÎ¸â„“â„“â€²) =1
SSX
s=1bj(Xâˆ—
s, Diâˆ’Ë†gâ„“â„“â€²(Zi)).
for each j= 1, . . . , J . â– 
We now discuss the automatic estimation of the First Step nuisance parameter. Again, for each
â„“, the goal is to build a sample version of the objective function in (3.9) that uses only observations
not in Iâ„“. The construction is almost similar to the one above. We will focus on the main differences.
24For a dictionary ( cj)âˆ
j=1that spans âˆ† 1, we can estimate E[cK(Z)cK(Z)â€²] by
Ë†Câ„“â‰¡1
nâˆ’nâ„“X
â„“â€²Ì¸=â„“X
iâˆˆIâ„“â€²cK(Zi)cK(Zi)â€²,
andE[D1(W,cK)] by
Ë†D1â„“â‰¡1
nâˆ’nâ„“X
â„“â€²Ì¸=â„“X
iâˆˆIâ„“â€²D1(Wi,cK|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,Ë†Î±2â„“â„“â€²,ËœÎ¸â„“â„“â€²). (4.8)
The first difference is that D1depends on Î±02on top of ( g0, h0, Î¸). We therefore need to plug-in
an estimator Ë† Î±2â„“â„“â€²that only uses observations not in Iâ„“âˆªIâ„“â€². This estimator can be constructed
using the methodology above. For instance, to construct Ë† Î±2â„“â„“â€²=bâ€²
JË†ÏJâ„“â„“â€², it is simple to write the
optimization problem that Ë†ÏJâ„“â„“â€²solves. Indeed, we can define
Ë†D2â„“â„“â€²â‰¡1
nâˆ’nâ„“âˆ’nâ„“â€²X
â„“â€²â€²/âˆˆ{â„“,â„“â€²}X
iâˆˆIâ„“â€²â€²D2(Wi,bJ|Ë†gâ„“â„“â€²â„“â€²â€²,Ë†hâ„“â„“â€²â„“â€²â€²,ËœÎ¸â„“â„“â€²â„“â€²â€²) and
Ë†Bâ„“â„“â€²â‰¡1
nâˆ’nâ„“âˆ’nâ„“â€²X
â„“â€²â€²/âˆˆ{â„“,â„“â€²}X
iâˆˆIâ„“â€²â€²bJ(Xi, Ï†(Di, Zi,Ë†gâ„“â„“â€²â„“â€²â€²))bJ(Xi, Ï†(Di, Zi,Ë†gâ„“â„“â€²â„“â€²â€²))â€².
Thus, Ë†ÏJâ„“â„“â€²is given by the optimization problem in (4.7) with Ë†D2â„“â„“â€²and Ë†Bâ„“â„“â€²replacing Ë†D2â„“and Ë†Bâ„“,
respectively.
The most important difference is that D1generally depends also on the derivatives âˆ‚h0/âˆ‚vand
âˆ‚Î±02/âˆ‚v. In Section 3.2, we have presented a parsimonious approach to estimate the derivative of
Î±02. It is straightforward to construct an estimator âˆ‚Ë†Î±2â„“â„“â€²/âˆ‚vof the derivative of Î±02that uses only
observations not in Iâ„“âˆªIâ„“â€². Since we have already estimated Ë† Î±2â„“â„“=bâ€²
JË†ÏJâ„“â„“â€², if each bjis differentiable
w.r.t. v, we have that âˆ‚Ë†Î±2â„“â„“â€²/âˆ‚vâ‰¡(âˆ‚bJ/âˆ‚v)â€²Ë†ÏJâ„“â„“â€².
Estimation of âˆ‚h0/âˆ‚vmay be more tricky. It will depend on the shape of the estimator Ë†hâ„“â„“.
Note that, since h0âˆˆâˆ†2(g0), we may use the dictionary ( bj)âˆ
j=1to approximate the parameter. In
this case, Ë†hâ„“â„“will be a Lasso or Ridge Regression estimator and we can estimate the derivative of h0
as we have estimated the derivative of Î±02. Moreover, if estimating h0is a low dimensional problem,
e.g., as in the partially linear model, we can often take Ë†hâ„“â„“as a Kernel or a Local Linear Regression
estimator. Then, the derivatives of h0can be estimated by finding the analytical expression of the
derivatives of the kernel function.
For a general ML estimator Ë†hâ„“â„“â€²(e.g., Random Forest), we propose a numerical derivative ap-
proach to estimate âˆ‚h0/âˆ‚v. Let tnbe a tuning parameter depending on the sample size with tnâ†“0.
We propose to estimate âˆ‚h0(x, v)/âˆ‚vby
âˆ‚Ë†hâ„“â„“â€²
âˆ‚v(x, v)â‰¡Ë†hâ„“â„“â€²(x, v+tn)âˆ’Ë†hâ„“â„“â€²(x, v)
tn. (4.9)
Note that, usually, we need to compute the derivative evaluated at ( Xi, Ï†(Di, Zi,Ë†gâ„“â„“â€²)).
25We have now seen all the difficulties in estimating D1â„“in equation (4.8). With these solved,
we can proceed to construct and automatic estimator of the First Step nuisance parameter. The
estimator is given by Ë† Î±1â„“=câ€²
KË†Î²Kâ„“, where
Ë†Î²Kâ„“= argmin
Î²KâˆˆRKn
âˆ’2Ë†Dâ€²
1â„“Î²K+Î²â€²
KË†Câ„“Î²K+Î»âˆ¥Î²Kâˆ¥q
qo
. (4.10)
We illustrate this procedure by constructing an automatic estimator of the First Step nuisance
parameter for the CASF:
Example 1(continuing from p. 6) From the previous discussion, we have that:
D1(w, g) =
D11(d, z) +âˆ‚h0
âˆ‚v(x, v)Î±02(x, v)
g(z),with
D11(d, z)â‰¡ âˆ’Zâˆ‚h0
âˆ‚v(xâˆ—, dâˆ’g0(z))dFâˆ—(xâˆ—).
We approximate D11by numerical integration. Let ( Xâˆ—
s)S
s=1be a sample from Fâˆ—. To estimate
D1â„“, we approximate D11(Di, Zi), with iâˆˆIâ„“â€², by
âˆ’1
SSX
s=1âˆ‚Ë†hâ„“â„“â€²
âˆ‚v(Xâˆ—
s, Diâˆ’Ë†gâ„“â„“â€²(Zi)),
where, âˆ‚Ë†hâ„“â„“â€²(x, v)/âˆ‚v= (âˆ‚bJ/âˆ‚v)â€²Ë†Î·â„“â„“â€². The parameters Ë†Î·â„“â„“â€²are Lasso cross-fitted slope estimates
for the second step h0. These estimates can incorporate semiparametric restrictions such as those
of a partly linear model by a suitable choice of the dictionary.
To estimate D1â„“, it remains to show how to estimate the second term in the brackets, for an
observation iâˆˆIâ„“â€². Being Viâ„“â„“â€²â‰¡Diâˆ’Ë†gâ„“â„“â€²(Zi), we can estimate the second term by
bJ(Xi,Ë†Viâ„“â„“â€²)â€²Ë†ÏJâ„“â„“â€²Â·âˆ‚Ë†hâ„“â„“â€²
âˆ‚v(Xi,Ë†Viâ„“â„“â€²).
Therefore, to estimate D1â„“according to equation (4.8), we have that, for iâˆˆIâ„“â€²,
D1(Wi, ck|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,Ë†Î±2â„“â„“â€²,ËœÎ¸â„“â„“â€²) =ck(Zi)Â·(
âˆ’1
SSX
s=1âˆ‚Ë†hâ„“â„“â€²
âˆ‚v(Xâˆ—
s, Viâ„“â„“â€²)
+bJ(Xi,Ë†Viâ„“â„“â€²)â€²Ë†ÏJâ„“â„“â€²Â·âˆ‚Ë†hâ„“â„“â€²
âˆ‚v(Xi,Ë†Viâ„“â„“â€²))
,
for each k= 1, ..., K . This can then be used to construct the objective function to estimate Ë†Î²Kâ„“.
â– 
264.2 Estimation Algorithm
Here we provide an illustration of our estimation algorithm. The inputs to the algorithm are cross-
fitted estimators of g0andh0. An initial estimator of Î¸0must also be supplied. We note that
one must provide a total of Lestimators (Ë† gâ„“,Ë†hâ„“,ËœÎ¸â„“) only using observations not in Iâ„“,L(Lâˆ’1)/2
estimators (Ë† gâ„“â„“â€²,Ë†hâ„“â„“â€²,ËœÎ¸â„“â„“â€²) only using observations not in Iâ„“âˆªIâ„“â€², and L(Lâˆ’1)(Lâˆ’2)/6 estimators
(Ë†gâ„“â„“â€²â„“â€²â€²,Ë†hâ„“â„“â€²â„“â€²â€²,ËœÎ¸â„“â„“â€²â„“â€²â€²) only using observations not in Iâ„“âˆªIâ„“â€²âˆªIâ„“â€²â€².
Figure 2 provides a diagram showing how to compute the moment condition Ë†Ïˆiâ„“(Î¸) for an
observation iâˆˆIâ„“. The debiased moment condition is given in equation (4.2). To this equation, the
diagram below adds the discussion in the above section, i.e., how to construct automatic estimators
of the nuisance parameters ( Î±01andÎ±02) in the correction term. The arrows in the diagram indicate
how to estimate each term.
Once the debiased moment condition Ë†Ïˆiâ„“is built, Automatic Debiased GMM estimation is
conducted with the objective function in equation (4.3).
5 Asymptotic theory
This section gives general conditions for asymptotic normality of the automatic debiased GMM
and conditions for consistent estimation of its asymptotic variance. The conditions are based on
the mean-square consistency, small interaction of estimation biases, and locally robust conditions
discussed in CEINR. Furthermore, estimation rates for the nuisance parameters ( Î±01, Î±02) require
(i) that the dictionaries approximate well the nuisance parameters and (ii) being able to estimate
the linear approximations of Â¯ m(g, h, Î¸ ) given by E[D1(W, g)] and E[D2(W, h)] at a certain rate (see
Chernozhukov et al., 2022b).
In the presence of generated regressors, the theory needs to account for the fact that the estimator
of the correction term (and probably that of the moment condition) evaluates the estimators Ë†hâ„“and
Ë†Î±2â„“in the generated regressor Ë†Viâ„“â‰¡Ï†(Di, Zi,Ë†gâ„“) (c.f., equation (4.2)). We modify the expansion of
Ë†Ïˆiâ„“(Î¸0)âˆ’Ïˆ(Wi, g0, h0, Î±0, Î¸0) given by CEINR to deal with this fact. Also, we rely on smoothness
conditions on the dictionaries and Ï†to ensure that evaluating at the generated regressor is not
problematic.
We begin with assumptions on the dictionaries. The first assumption formally states that the
dictionaries ( bn)âˆ
j=1and ( ck)âˆ
k=1span âˆ† 2(g0) and âˆ† 1, respectively.2
Assumption 5.1
a.For every j,bjâˆˆâˆ†2(g0). Also, âˆ€Î´2âˆˆâˆ†2(g0) and for every Îµ >0, there exist JandÏJsuch
thatâˆ¥Î´2âˆ’bâ€²
JÏJâˆ¥2< Îµ.
2In this section, for a measurable function f,âˆ¥fâˆ¥2â‰¡p
E[f(W)2] denotes its L2-norm. Also, for a d1Ã—d2matrix
A= (Ai,j)d1,d2
i=1,j=1,âˆ¥Aâˆ¥âˆâ‰¡maxi,j|Aij|.
27Ë†Ïˆiâ„“(Î¸)â‰¡m(Wi,Ë†gâ„“,Ë†hâ„“, Î¸)+Ë†Î±1â„“(Zi)Â·(Diâˆ’Ë†gâ„“(Zi)))+ Ë†Î±2â„“(Xi,Ë†Viâ„“)Â·(Yiâˆ’Ë†hâ„“(Xi,Ë†Viâ„“))=bâ€²
JË†ÏJâ„“ +Î»âˆ¥ÏJâˆ¥q
q} Ïâ€²
JË†Bâ„“ÏJ+ âˆ’2Ë†Dâ€²
2â„“ÏJargminÏJâˆˆRJ{Average on â„“â€²Ì¸=â„“,sâˆˆIâ„“â€², of
D2(Ws,bJ|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,ËœÎ¸â„“â„“â€²)Average on â„“â€²Ì¸=â„“,sâˆˆIâ„“â€², of
bJ(Xs, Ï†(Ds, Zs,Ë†gâ„“â„“â€²))bJ(Xs, Ï†(Ds, Zs,Ë†gâ„“â„“â€²))â€²
=câ€²
KË†Î²Kâ„“argminÎ²KâˆˆRK{âˆ’2Ë†Dâ€²
1â„“Î²K+Î²â€²
KË†Câ„“Î²K+Î»âˆ¥Î²Kâˆ¥q
q}
Average on â„“â€²Ì¸=â„“,sâˆˆIâ„“â€², of
D1(Ws,cK|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,Ë†Î±2â„“â„“â€²,ËœÎ¸â„“â„“â€²)Average on â„“â€²Ì¸=â„“,sâˆˆIâ„“â€², of
cK(Zs)cK(Zs)â€²Â·DÏ†cK (yâˆ’Ë†hâ„“â„“â€²(Xs, v))] Ë†Î±2â„“â„“â€²(Xs, v)âˆ‚
âˆ‚v[ D11(Ws,cK)+Evaluated at
v=Ï†(Ds, Zs,Ë†gâ„“â„“â€²)
=bâ€²
JË†ÏJâ„“â„“â€² argminÏJâˆˆRJ{âˆ’2Ë†Dâ€²
2â„“â„“â€²ÏJ+Ïâ€²
JË†Bâ„“â„“ÏJ+Î»âˆ¥ÏJâˆ¥q
q}
Average on â„“â€²â€²/âˆˆ {â„“, â„“â€²},Î¹âˆˆIâ„“â€²â€², of
D2(WÎ¹,bJ|Ë†gâ„“â„“â€²â„“â€²â€²,Ë†hâ„“â„“â€²â„“â€²â€²,ËœÎ¸â„“â„“â€²â„“â€²â€²)Average on â„“â€²â€²/âˆˆ {â„“, â„“â€²},sâˆˆIâ„“â€²â€², of
bJ(XÎ¹, Ï†(DÎ¹, ZÎ¹,Ë†gâ„“â„“â€²â„“â€²â€²))bJ(XÎ¹, Ï†(DÎ¹, ZÎ¹,Ë†gâ„“â„“â€²â„“â€²â€²))â€²
Figure 2Illustration of the algorithm to estimate the moment condition Ë†Ïˆiâ„“for an observation iâˆˆIâ„“.
28b.For every k,ckâˆˆâˆ†1. Also, âˆ€Î´1âˆˆâˆ†1and for every Îµ >0, there exist KandÎ²Ksuch that
âˆ¥Î´1âˆ’câ€²
KÎ²Kâˆ¥2< Îµ.
We also assume bounded dictionaries (see, for instance, Newey, 1997):
Assumption 5.2 supjâˆˆN|bj(X, V)|<âˆand supkâˆˆN|ck(Z)|<âˆalmost surely.
The assumption translates into consistency of Ë†Bâ„“and Ë†Câ„“. Also, on top of the following assump-
tion, it will guarantee that the correction term nuisance parameters are bounded:
Assumption 5.3 For the real-valued sequences ( Ï0j)âˆ
j=1and ( Î²0k)âˆ
k=1such that Î±02(x, v) =Pâˆ
j=1Ï0jbj(x, v) and Î±01(z) =Pâˆ
k=1Î²0kck(z):
a.Pâˆ
j=1|Ï0j|<âˆandPâˆ
k=1|Î²0k|<âˆ.
b.For a C >0, the atoms bjandckcorresponding to the largest Câˆšnvalues of Ï0jandÎ²0kare
included in bJandcK.
This assumption keeps the L1-norm of the coefficient of the Lasso penalized regression under
control. The result is relevant to estimate the asymptotic variance (see Chernozhukov et al., 2022b)
and to evaluate the estimators at the generated regressor. We also note that the absolute summa-
bility of the coefficients imposes a sparsity condition on the relevant terms to approximate Î±01and
Î±02(see Chernozhukov et al., 2022b, p. 985).
We require the following estimation rates:
Assumption 5.4 There is 1 /3< r < 1/2 such that
a.âˆ¥Ë†gâ„“âˆ’g0âˆ¥2=Op(nâˆ’r) andâˆ¥Ë†hâ„“âˆ’h0âˆ¥2=Op(nâˆ’r).
b.âˆ¥Ë†D1â„“âˆ’E[D1(W,cK)]âˆ¥âˆ=Op(nâˆ’r) andâˆ¥Ë†D2â„“âˆ’E[D2(W,bJ)]âˆ¥âˆ=Op(nâˆ’r).
This assumption imposes standard rate conditions on the estimators of the nuisance parameters
and on the linearization of the moment condition (for Lasso rates, see Bickel et al., 2009; Bunea
et al., 2007; Zhang and Huang, 2008, and references therein). Under some regularity conditions on
the linearizations (see Chernozhukov et al., 2022b, Ass. 12) and a condition allowing evaluation at
the generated regressor (see Assumption 5.9 below), Assumption 5.4.b can be derived from the rate
conditions on the estimators of the nuisance parameters.
We also ask for the following rates for the Lasso penalty and the number of terms in the
dictionaries:
Assumption 5.5
a.The Lasso penalty term Î»=Î»(n) for estimation of ( Î±01, Î±02) satisfies: nâˆ’r=o(Î») and
Î»=o(ncâˆ’r) for every c >0.
29b.The number of terms in the dictionaries satisfy J, K =O(nÎº) for a constant Îº >0.
This assumption asks for the Lasso penalty to go to zero slightly slower than nâˆ’r. For instance,
a rate of log( n)/nris allowed. Moreover, it requires polynomial rates in the growth of the number
of terms in the dictionaries.
The above are general conditions imposed on the dictionaries and the tuning parameters for
the Lasso penalized regression. The specific problem at hand only appears in two instances. First,
Assumption 5.1 requires that the dictionaries approximate well the correction term nuisance param-
eters (living in âˆ† 1and âˆ† 2(g0), respectively). Second, Assumption 5.4 requires (i) mean-square rates
for the estimators of g0andh0and (ii) to be able to estimate the linearizations at the same rate. As
discussed before, these conditions provide rates of estimators of the nuisance parameters in correc-
tion terms Î±01andÎ±02(see Chernozhukov et al., 2022b). For instance, the convergence rate of Ë† Î±1â„“
will be fast enough to guarantee that the interaction term satisfies âˆ¥Ë†Î±1â„“âˆ’Î±01âˆ¥2Â·âˆ¥Ë†gâ„“âˆ’g0âˆ¥2=op(nâˆ’1/2)
(c.f. Assumption 2 in CEINR).
We now provide assumptions on the moment condition. The first is a mean-square consistency
condition similar to Assumption 1 in CEINR:
Assumption 5.6
a.E[m(W, g 0, h0, Î¸0)2]<âˆ.
b.R
[m(w,Ë†gâ„“,Ë†hâ„“, Î¸0)âˆ’m(w, g 0, h0, Î¸0)]2dF0(w)Pâˆ’ â†’0.
c.R
[m(w,Ë†gâ„“,Ë†hâ„“,ËœÎ¸â„“)âˆ’m(w,Ë†gâ„“,Ë†hâ„“, Î¸0)]2dF0(w)Pâˆ’ â†’0.
d.E[(Yâˆ’h0(X, V))2|X, V] andE[(Dâˆ’g0(Z))2|Z] are bounded almost surely.
Assumption 5.6.a is necessary for regular estimation of Î¸0. Assumptions 5.6.b and 5.6.c are mean-
square consistency conditions for the moment condition. Boundedness of the conditional variances
(Assumption 5.6.d) easily translates into mean-square consistency conditions for the correction
term Ï•. We repeat here that boundedness of the correction term nuisance parameters Î±01andÎ±02
is implied by Assumptions 5.2 and 5.3.a.
We require the linear approximation of Â¯ m(g, h, Î¸ 0) to be good enough (in a neighborhood of
(g0, h0)):
Assumption 5.7 There is a Îµ >0 and a C > 0 such that, if âˆ¥gâˆ’g0âˆ¥2< Îµandâˆ¥hâˆ’h0âˆ¥2< Îµ,
then
|E[m(W, g, h, Î¸ 0)âˆ’m(W, g 0, h0, Î¸0)âˆ’D1(W, gâˆ’g0)âˆ’D2(W, hâˆ’h0)]|
â‰¤C 
âˆ¥gâˆ’g0âˆ¥2
2+âˆ¥hâˆ’h0âˆ¥2
2
.
This Assumption translates into the Locally Robust property in Assumption 3.iii in CEINR. It
asks that, once we have removed the first-order effect of estimating ( g0, h0), the remainder term
must be at most quadratic.
30The GMM procedure requires consistent estimation of the Jacobian of the moment condition.
Being the following assumption specific to the GMM procedure, it is stated for the case with an
arbitrary number of parameters and moment conditions.
Assumption 5.8 There exists a neighborhood NofÎ¸0such that, for small âˆ¥gâˆ’g0âˆ¥2andâˆ¥hâˆ’h0âˆ¥2:
a.m(W, g, h, Î¸ ) is almost surely differentiable in N.
b.There exists a C >0 and a function d(W, g, h ), with E[d(W, g, h )]< C, such that for Î¸âˆˆ N
âˆ‚m
âˆ‚Î¸(W, g, h, Î¸ )âˆ’âˆ‚m
âˆ‚Î¸(W, g, h, Î¸ 0)
âˆâ‰¤d(W, g, h )âˆ¥Î¸âˆ’Î¸0âˆ¥1/C
âˆalmost surely .
Moreover, we assume that:
c.M, the expectation of the Jacobian, exists.
d.It holds that Zâˆ‚m
âˆ‚Î¸(w,Ë†gâ„“,Ë†hâ„“, Î¸0)âˆ’âˆ‚m
âˆ‚Î¸(w, g 0, h0, Î¸0)
âˆdF0(w)Pâˆ’ â†’0.
We conclude the set of assumptions with smoothness conditions on the dictionary ( bj)âˆ
j=1and
the second step regression h0. The following assumption allows us to deal with Ë†hâ„“and Ë†Î±2â„“being
evaluated at the generated regressor.
Assumption 5.9
a.h0also satisfies Assumption 5.3.
b.supjâˆˆN|âˆ‚bj(X, V)/âˆ‚v|<âˆalmost surely.
c.There exists a Îµ >0 such that, for all gâˆˆâˆ†1andâˆ¥gâˆ’g0âˆ¥< Îµ,bjâˆˆâˆ†2(g) for every jâˆˆN.
In Theorem 5.1, we give asymptotic normality of the automatic debiased GMM estimator when
Ë†hâ„“is a Lasso penalized regression of YontobJ(X, V) (see Section 4). For other estimators of the
second step nuisance parameters, Assumption 5.9.a may be replaced by âˆ¥Ëœhâ„“âˆ’h0âˆ¥2=Op(âˆ¥Ë†hâ„“âˆ’h0âˆ¥2),
where Ëœhâ„“(w)â‰¡Ë†hâ„“(x, Ï†(d, z,Ë†gâ„“)). Assumption 5.9.b allows us to bound the effect of departures from
evaluation at the â€œtrueâ€ generated regressor Vâ‰¡Ï†(D, Z, g 0). Assumption 5.9.c simply requires
that the dictionary is valid for small deviations in the generated regressor.
Assumptions 5.3, 5.4, 5.6, and 5.7 are stated for a single moment condition. In the presence
of more than one condition, they must be understood to hold componentwise. The same happens
with Assumptions 3.1, 3.2, and 3.4 in Section 3.1. Assumption 5.8, since it refers to a GMM-specific
situation, is already formulated in the general case. The remaining assumptions do not depend on
the dimension of the moment condition (they depend, on the other hand, on the dimension of Y
andD).
31Recall that Î â‰¡(Mâ€²Î¥M)âˆ’1Mâ€²Î¥â€²Î¨Î¥M(Mâ€²Î¥M)âˆ’1gives the asymptotic variance of the au-
tomatic debiased GMM estimator. Define Ë†Îâ‰¡(Ë†Mâ€²Ë†Î¥Ë†M)âˆ’1Ë†Mâ€²Ë†Î¥â€²Ë†Î¨Ë†Î¥Ë†M(Ë†Mâ€²Ë†Î¥Ë†M)âˆ’1as the plug-in
estimator of the asymptotic variance, where Ë†MandË†Î¨ are given in equations (4.4) and (4.5), re-
spectively. The following theorem ensures asymptotic normality ofâˆšn(Ë†Î¸âˆ’Î¸0):
Theorem 5.1 Consider that Assumptions 3.1-3.4 and 5.1-5.9 are satisfied, Ë†Î¥Pâˆ’ â†’Î¥,Mâ€²Î¥Mis
non-singular, and Ë†hâ„“are Lasso estimators of h0. Then, the automatic debiased GMM estimator in
equation (4.3) satisfiesâˆšn(Ë†Î¸âˆ’Î¸0)Dâˆ’ â†’N(0,Î).
Moreover, the plug-in estimator for the asymptotic variance is consistent: Ë†ÎPâˆ’ â†’Î.
We conclude the section by giving sufficient conditions to apply the above theorem to the CASF:
Example 1(continuing from p. 6) Here we verify assumptions 5.6, 5.7, and 5.8. These are
the assumptions that depend on the identifying moment condition for the parameter of interest. In
the case of the CASF, the moment condition is
m(w, g, h, Î¸ ) =Z
h(xâˆ—, dâˆ’g0(z))dFâˆ—(xâˆ—)âˆ’Î¸.
We also provide sufficient conditions for Assumptions 3.1 and 3.2 required for linearizing the
moment condition. Recall that the assumption require
E[D11(w, g)] =E
âˆ’Zâˆ‚h0
âˆ‚v(xâˆ—, V)dFâˆ—(xâˆ—)Â·g(Z)
and
E[D2(w, h)] =EZ
h(xâˆ—, V)dFâˆ—(xâˆ—)
=Efâˆ—(X)fv
0(V)
fxv
0(X, V)Â·h(X, V)
to be L2-continuous. To achieve this, we require some regularity on the distribution of ( X, V) and
on the dependence of the outcome Yon (X, V).
Assumption 5.10
a.Î±02(X, V) is bounded almost surely and E[Y2]<âˆ.
b.For almost every x,h0andÎ±02are twice continuously differentiable with respect to v. More-
over, âˆ‚h/âˆ‚v (X, V),âˆ‚Î±02/âˆ‚v(X, V),âˆ‚2h/âˆ‚v2(X, V), and âˆ‚2Î±02/âˆ‚v2(X, V) are bounded almost
surely.
Assumption 5.10.a is related to regular identification of the CASF (i.e., the asumption guar-
antees that the necessary condition for regular identification in PÂ´ erez-Izquierdo, 2022, is satisfied).
Assumption 5.10.b implies continuity of the linearization with respect to g. We can then show that
the assumptions for Theorem 5.1 hold for the moment condition defining the CASF.
32Pr oposition 5.1 Suppose that Assumptions 5.3, 5.4.a, and 5.9. Then Assumption 5.10 guar-
antees that Assumptions 3.1, 3.2, 5.6.a-5.6.c, 5.7, and 5.8 are satisfied for the moment condition
identifying the CASF.
â– 
6 Monte Carlo simulation
This section describes the Monte Carlo simulation to evaluate the finite sample properties of the
CASF estimator proposed in this paper. Before presenting the results, we briefly describe the Data
Generating Process (DGP) and the implemented estimators.
6.1 Description
The DGP is
(Z, U, V )âˆ¼Nï£«
ï£¬ï£­0,ï£®
ï£¯ï£°Id60 0
0 1 1 /2
0 1 /2 1ï£¹
ï£ºï£»ï£¶
ï£·ï£¸,
where Id 6denotes the 6 Ã—6 Identity Matrix. Therefore, Zis a 6-dimensional random vector. The
correlation between UandVis 1/2. Note that the fact that ZâŠ¥UandZâŠ¥Vguarantees that
the Control Function Assumption is satisfied. Here X= (D, Z 1, ..., Z 5).
Both DandYare generated by the following linear models:
Y=5X
k=1Zk+ 2D+Uand
D=6X
k=1Zk+V.
SoZ6is excluded from the structural equation (i.e., it does not directly affect Y) and may be used
as an instrument.
We estimate the CASF for the following counterfactual distribution Fâˆ—
X: (i) the distribution
of (Z1, . . . , Z 5) remains unchanged and (ii) Dis normal with mean 1 (instead of 0) and the same
variance as in the DGP. Therefore, the true parameter is Î¸0= 2, the first step is g0(z) =P6
k=1zk,
and the second step is h0(x, v) =P5
k=1zk+ 2d+v/2.
We note here that, even if the model considered is linear, the second-step correction nuisance
parameter is highly non-linear. Letting sâ‰¡P5
k=1zk, the Riesz representer is
r2(z1, . . . , z 5, d, v) =CÂ·exp
âˆ’1
4âˆ’s
2+d
2+s2
4+v2
2+d2
4âˆ’sd
2+svâˆ’dv
,
33for a constant C. The function Î±02is the orthogonal projection of r2onto âˆ† 2(g0).
We display results for three different estimators of the CASF:
â€¢The naive plug-in estimator: Ë†Î¸PIâ‰¡nâˆ’1Pn
i=1m(Wi,Ë†g,Ë†h), where mis given in equation (2.3).
â€¢A cross-fitted Doubly Robust debiased estimator that only corrects for the effect of pluging-
inË†h:Ë†Î¸DRis as in equation (4.6) but with Ï•(Wi,Ë†gâ„“,Ë†hâ„“,Ë†Î±â„“,ËœÎ¸â„“) replaced by Ë† Î±2â„“(Xi,Ë†Viâ„“)Â·(Yiâˆ’
Ë†hâ„“(Xi,Ë†Viâ„“)). That is, the correction term for the first step is omitted.
â€¢The cross-fitted fully Locally Robust debiased estimator: Ë†Î¸LRas in equation (4.6). That is,
the estimator is based on the fully debiased moment condition in equation (4.2).
Numerical integration, with a sample of size S= 107, is used to compute the integrals w.r.t. Fâˆ—
X.
The estimators for the nuisance parameters g0,h0,Î±01, and Î±02are Lasso with three dictionaries:
one that includes linear terms, another including linear and quadratic terms, and a last one including
linear, quadratic, and interaction terms. The number of splits for cross-fitting is L= 5 for every
sample size.
To perform inference with each estimator, we present results that parallel common practice.
The fully debiased estimator uses the correct asymptotic variance, the one accounting for first and
second step estimation. This is given by equation (4.5). The estimator Ë†Î¸DRonly accounts for the
second step when computing the asymptotic variance (as it does for estimation). Its asymptotic
variance can be constructed by replacing Ï•(Wi,Ë†gâ„“,Ë†hâ„“,Ë†Î±â„“,ËœÎ¸â„“) by Ë† Î±2â„“(Xi,Ë†Viâ„“)Â·(Yiâˆ’Ë†hâ„“(Xi,Ë†Viâ„“)) in
the second step IF. To emphasize that plug-in estimation leads to an asymptotic bias problem,
confidence intervals for the plug-in estimator are built with correct asymptotic variance (the one in
equation (4.5)).
6.2 Results
The next tables report results for a Monte Carlo simulation with B= 1098 replications. Each table
gives results for a different dictionary: linear, quadratic or the one which also includes interaction
terms.
Tables 1 and 2 present results for the linear and quadratic dictionaries, respectively. Correcting
for the second step already reduces a large amount of the bias of the plug-in estimator. Adding
the first-step correction further decreases bias. As shown in the tables, however, the estimator
accounting only for the second step fails to keep coverage at the nominal 95% level as the sample
size increases.
The tables highlight that the plug-in estimator suffers from severe asymptotic bias issues: cov-
erage decreases rapidly, even if the confidence interval is constructed with correct standard errors.
Indeed, Figure 3 shows that, for a sample size of n= 10000, the distribution of plug-in estimators
has almost zero mass near the true parameter Î¸0= 2.
34Mean Absolute Bias Standard Error Coverage (95%)
n PI DR LR PI DR LR PI DR LR
100 0.2285 0.1463 0.1482 0.1649 0.1812 0.1733 0.6388 0.8681 0.8626
500 0.1425 0.0594 0.0516 0.0685 0.0692 0.0645 0.3876 0.8954 0.9208
1000 0.1236 0.0435 0.0376 0.049 0.0488 0.0462 0.2266 0.8744 0.9272
5000 0.0852 0.0234 0.0169 0.0227 0.0212 0.0202 0.0227 0.7925 0.9290
10000 0.0746 0.0181 0.0123 0.017 0.0148 0.0142 0.0018 0.7489 0.9163
T able 1CASF results for the dictionary including linear terms.
Figure 3Distribution of the CASF estimators using a quadratic dictionary for a sample size of n=
10000.
Mean Absolute Bias Standard Error Coverage (95%)
n PI DR LR PI DR LR PI DR LR
100 0.2764 0.1898 0.1957 0.1766 0.2003 0.2 0.5532 0.7534 0.7561
500 0.1462 0.0603 0.0527 0.0692 0.0709 0.0663 0.3794 0.8963 0.9327
1000 0.1214 0.0446 0.0374 0.0491 0.0488 0.0465 0.2329 0.8717 0.929
5000 0.079 0.0259 0.0161 0.0236 0.0212 0.0202 0.0437 0.737 0.9354
10000 0.0694 0.0209 0.0115 0.0172 0.0148 0.0143 0.0036 0.6533 0.9327
T able 2CASF results for the dictionary including linear and quadratic terms.
Table 3 displays results for the dictionary that also includes interaction terms. The results are
striking, as the Doubly Robust estimator that only accounts for the second step performs well. It is
able to keep coverage at nominal levels, outperforming the fully debiased estimator. Nevertheless,
the decrease in coverage is small: 1-2% for intermediate sample sizes ( n= 500 and 1000) and
4-5% for large samples ( n= 5000 and 10000). We believe that this fact rests on the dictionaries
performing well to estimate the second step correction, but notably worst to estimate the more
complex fist step correction. This result suggest that the complexity of the dictionary must be
increased faster when accounting for the first step.
35Mean Absolute Bias Standard Error Coverage (95%)
n PI DR LR PI DR LR PI DR LR
100 0.3447 0.3583 0.3857 0.179 0.3606 0.4054 0.9016 0.7969 0.806
500 0.1707 0.0997 0.1085 0.0693 0.1213 0.1359 0.7925 0.9481 0.9227
1000 0.1367 0.0635 0.0674 0.0488 0.0776 0.0849 0.5883 0.9372 0.9262
5000 0.0846 0.0259 0.0283 0.0229 0.0312 0.0349 0.1383 0.9372 0.8926
10000 0.0729 0.0173 0.0205 0.017 0.0207 0.0228 0.0337 0.9399 0.8926
T able 3CASF results for the dictionary including linear, quadratic, and interaction terms.
7 Conclusion
We propose Automatic Locally Robust estimators for structural parameters in the presence of
generated regressors. We show that the debiasing correction term can be decomposed into terms
accounting for first step and second step estimation. Each of the first and second step IF depends
on an additional nuisance parameter, which can be automatically estimated (i.e., estimated without
finding their analytic shape).
We apply our results to construct Automatic Locally Robust estimators for the CASF under the
control function assumption, see also Appendix A for the Average Partial Effects example in sample
selection models. The analytic shape of the nuisance parameters in these two cases is particularly
complex, as the moment conditions depend on the whole shape of the second step parameter (not
only its pointwise value). Therefore, automatic estimation is particularly suited for these problems.
We have shown that commonly used plug-in methods lead to highly biased inferences for the CASF.
Doubly Robust and fully Locally Robust estimators correct the bias and deliver much more accurate
inference in a complex setting with generated regressors.
36Appendix A Partial effects in sample selection models
Example 3We observe W= (Y, D, Z ) following the model Y=Yâˆ—Dâ‰¡H(X, Îµ)D, where X
is a component of Z, and we do not observe Yâˆ—when D= 0. This is a very general setting
for sample selection models. We do not know much about the selection, so this is given by D=
1 [g0(Z)âˆ’Uâ‰¥0], where Uis uniformly distributed in [0 ,1] . The unobserved errors ÎµandU,
though independent of Z, are correlated with each other (selection on unobservables). In this
example, V=g0(Z) =E[D|Z]. Then, it can be shown that
E[Y|Z] =E[H(X, Îµ)1 [g0(Z)âˆ’Uâ‰¥0]|Z]
=h0(X, V).
This setting provides a nonparametric extension of the classical model of Heckman (1979), where
H(X, Îµ) =Xâ€²Î²0+Îµ, g0(Z) =Zâ€²Î³0, and the joint distribution of ( Îµ, U) is bivariate Gaussian.
As a parameter of interest consider the Average Partial Effects (APE) given, for simplicity of
presentation for a one-dimensional continuous regressor, by
Î¸0=Eâˆ‚h0
âˆ‚x(X, V)
.
The moment function identifying the APE is
m(w, g, h, Î¸ ) =âˆ‚
âˆ‚sh(s, g(z))
s=xâˆ’Î¸.
This parameter is covered by Proposition 5 in Hahn and Ridder (2019). However, the authors do
not consider Locally Robust estimation. Here we propose a novel Locally Robust estimator for the
APE which (i) is Doubly Robust to the second step and (ii) allows for ML first and second step
estimators.
Letâˆ‚h(x, v)/âˆ‚xdenote the derivative of h(x, v) w.r.t. its first argument at ( x, v). Let âˆ‚2h(x, v)/âˆ‚xâˆ‚v
denote the derivative w.r.t. both arguments at ( x, v). For the APE, we have that the moment func-
tion is linear in h. Thus:
D2(w, h|g0, h0, Î¸) =âˆ‚h
âˆ‚x(x, g0(z)),
where we have already make explicit the dependence of D2on (g0, h0, Î¸). We can also linearize the
moment condition in gto obtain:
D1(w, g|g0, h0, Î±0, Î¸) =
D11(d, z) +âˆ‚
âˆ‚v[Î±02(x, v)(yâˆ’h0(x, v))]
g(z),with
D11(d, z)â‰¡âˆ‚2h0
âˆ‚xâˆ‚v(x, g0(z)).
The debias estimator for the APE is
Ë†Î¸=1
nLX
â„“=1X
iâˆˆIâ„“âˆ‚Ë†hâ„“
âˆ‚x(Xi,Ë†gâ„“(Zi)) +Ë†Ï•,
37where, for the estimator Ë†hâ„“, we can estimate its derivative w.r.t. xby
Ë†hâ„“
âˆ‚x(x, v)â‰¡Ë†hâ„“(x+sn, v)âˆ’Ë†hâ„“(x, v)
sn,
for a tuning parameter sn. Alternatively, we can take advantage of a differentiable dictionary
(bj(x, v))âˆ
j=1, as described below.
To construct Ë†Ï•, we need to estimate Î±01andÎ±02. We propose automatic estimators for these
nuisance parameters. We assume that âˆ‚h0/âˆ‚xandâˆ‚h0/âˆ‚vare differentiable, so we can interchange
the order of differentiation.
Consider a dictionary ( bj)âˆ
j=1that is differentiable w.r.t. both xandv. To Estimate Ë†D2â„“, we
can compute D2(Wi, bj|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,ËœÎ¸â„“â„“â€²) for an observation iâˆˆIâ„“â€²by
âˆ‚bj
âˆ‚x(Xi,Ë†gâ„“â„“â€²(Zi)).
This derivative can be found analytically for each atom. We can use this to obtain an automatic
estimator of Î±02.
To construct Ë†D1â„“, we need to estimate D1(Wi, ck|Ë†gâ„“â„“â€²,Ë†hâ„“â„“â€²,Ë†Î±2â„“â„“â€²,ËœÎ¸â„“â„“â€²) for an observation iâˆˆIâ„“â€²
and an arbitrary atom ckin a dictionary. The first term, D11(Di, Zi), can be estimated by
âˆ‚2bJ
âˆ‚xâˆ‚v(Xi,Ë†gâ„“â„“â€²(Zi))â€²
Ë†Î·J,
in case that h0(x, v) is approximated by bJ(x, v)â€²Ë†Î·J. An estimator of the second term, âˆ‚[Î±02(x, v)Â·
(yâˆ’h0(x, v)]/âˆ‚v, is
âˆ‚bJ
âˆ‚v(Xi,Ë†gâ„“â„“â€²(Zi))â€²Ë†ÏJÂ·[Yiâˆ’bJ(Xi,Ë†gâ„“â„“â€²(Zi))â€²Ë†Î·Jâ„“â„“â€²]âˆ’bJ(Xi,Ë†gâ„“â„“â€²(Zi))â€²Ë†ÏJâ„“â„“â€²Â·âˆ‚bJ
âˆ‚v(Xi,Ë†gâ„“â„“â€²(Zi))â€²Ë†Î·J
â– 
38Appendix B Proofs of the results
Pr oof of Pr oposition 3.1: For the (differentiable) path Ï„7â†’h(FÏ„, g0), Assumption 3.1 implies
d
dÏ„Â¯m(g0, h(FÏ„, g0), Î¸) =d
dÏ„E[D2(W, h(FÏ„, g0)].
This gives the linearization (LIN) .
To find the shape of the IF, note that E[D2(W, h)] is a linear and continuous functional in
L2(X, V), a Hilbert space of square-integrable functions. Thus, by the Riesz Representation Theo-
rem, there exists a r2such that E[D2(W, h)] =E[r2(X, V)h(X, V)], with Vâ‰¡Ï†(D, Z, g 0). Therefore:
d
dÏ„Â¯m(g0, h(FÏ„, g0), Î¸) =d
dÏ„E[r2(X, V)h(FÏ„, g0)(X, V)],
where h(F, g)(x, v) denotes h(F, g) evaluated at ( x, v). This is Assumption 1 in Ichimura and Newey
(2022). Since Assumption 2 in that paper is satisfied in our setup, Proposition 1 in Ichimura and
Newey (2022) gives: Ï•2(w, h 0, Î±02, Î¸) =Î±02(d, z){yâˆ’h0(x, Ï†(d, z, g 0))}. The parameter Î±02is the
L2-projection of r2onto âˆ† 2(g0):
Î±02= argmin
Î±âˆˆâˆ†2(g0)E[(r2(X, Ï†(D, Z, g 0))âˆ’Î±(D, Z))2]. (B.1)
This gives Point (IF). â– 
Pr oof of Lemma 3.1: We proceed as in Hahn and Ridder (2013, Lma. 1). Let Ï„7â†’gÏ„be a
differentiable path. For any function Î´2âˆˆâˆ†2(gÏ„), we have that
E[Î´2(X, V(gÏ„))Â· {Yâˆ’h(F0, gÏ„)(X, V(gÏ„))}] = 0.
This is the orthogonality condition that defines h(F0, gÏ„) (it is equation (2.4) for ( F, g) = (F0, gÏ„)).
IfÎ´2âˆˆâˆ†2(gÏ„) when Ï„ < Îµ , we can take derivatives in the above equation. Thus, applying the chain
rule, we get
d
dÏ„E[Î´2(X, V)Â·h(F0, gÏ„)(X, V)] =âˆ’d
dÏ„E[Î´2(X, V)Â·h0(X, V(gÏ„))]
+d
dÏ„E[Î´2(X, V(gÏ„))Â·(Yâˆ’h0(X, V))].
â– 
Pr oof of Theorem 3.1: We compute dÂ¯m(g(FÏ„), h0, Î¸)/dÏ„anddÂ¯m(g0, h(F0, g(FÏ„)), Î¸)/dÏ„sepa-
rately and then add them according to equation (3.2). By Assumptions 3.1 and 3.2, using the Riesz
Representation Theorem, we have that for the differentiable paths Ï„7â†’g(FÏ„) and Ï„7â†’h(F0, g(FÏ„)):
d
dÏ„Â¯m(g(FÏ„), h0, Î¸) =d
dÏ„E[D11(W, g(FÏ„))] =d
dÏ„E[r1(Z)g(FÏ„)(Z)] (B.2)
39and, being Vâ‰¡Ï†(D, Z, g 0),
d
dÏ„Â¯m(g0, h(F0, g(FÏ„)), Î¸) =d
dÏ„E[D2(W, h(F0, g(FÏ„)))] =d
dÏ„E[r2(X, V)h(F0, g(FÏ„))(X, V)].
In these equations, g(F)(z) means g(F) evaluated at z, and h(F, g)(x, v) means h(F, g) evaluated
at (x, v). By Assumption 3.3.b, h(F0, g(FÏ„))âˆˆâˆ†2(g0). This means that h(F0, g(FÏ„)) is orthogonal
tor2âˆ’Î±02(since Î±02is the L2-projection of r2onto âˆ† 2(g0)). Then, we can write:
d
dÏ„Â¯m(g0, h(F0, g(FÏ„)), Î¸) =d
dÏ„E[Î±02(X, V)h(F0, g(FÏ„))(X, V)]. (B.3)
By Assumption 3.3.a we can apply Lemma 3.1 to the RHS of equation (B.3) to get:
d
dÏ„Â¯m(g0, h(F0, g(FÏ„)), Î¸) =d
dÏ„E[Î±02(X, V)h(F0, g(FÏ„))(X, V)]
=âˆ’d
dÏ„E[Î±02(X, V)h0(X, Ï†(D, Z, g (FÏ„)))]
+d
dÏ„E[Î±02(X, Ï†(D, Z, g (FÏ„)))Â·(Yâˆ’h0(X, V))].(B.4)
Under Assumption 3.4, the term in the second row can be linearized in g(FÏ„) as
d
dÏ„E[Î±02(X, V)h0(X, Ï†(D, Z, g (FÏ„)))] = Ed
dÏ„{Î±02(X, V)h0(X, Ï†(D, Z, g (FÏ„)))}
=E
Î±02(X, V)âˆ‚h0
âˆ‚v(X, V)d
dÏ„Ï†(D, Z, g (FÏ„))
=E
Î±02(X, V)âˆ‚h0
âˆ‚v(X, V)d
dÏ„DÏ†g(FÏ„)(D, Z)
=d
dÏ„E
Î±02(X, V)âˆ‚h0
âˆ‚v(X, V)DÏ†g(FÏ„)(D, Z)
,
where DÏ†g(d, z) denotes DÏ†gevaluated at ( d, z). We have assumed that derivatives and expectations
can be interchanged (we may impose some regularity conditions on Hsuch that this is possible).
We can equivalently linearize the term in the third row of equation (B.4) to get
d
dÏ„E[Î±02(X, Ï†(D, Z, g (FÏ„)))Â·(Yâˆ’h0(X, V))] =d
dÏ„E
(Yâˆ’h0(X, V))âˆ‚Î±02
âˆ‚v(X, V)DÏ†g(FÏ„)(D, Z)
.
Pluging in these results back in equation (B.4):
d
dÏ„Â¯m(g0, h(F0, g(FÏ„)), Î¸) =d
dÏ„E
âˆ’Î±02(X, V)âˆ‚h0
âˆ‚v(X, V)
+(Yâˆ’h0(X, V))âˆ‚Î±02
âˆ‚v(X, V)
DÏ†g(FÏ„)(D, Z)
=Eâˆ‚
âˆ‚v{Î±02(X, v)Â·(Yâˆ’h0(X, v))}
v=VDÏ†g(FÏ„)(D, Z)
.(B.5)
40Since DÏ†is linear in g, the function inside the expectation in the RHS is linear in g. We now use
equation (3.2) to combine the results in equations (B.2) and (B.5). This gives:
d
dÏ„Â¯m(g(FÏ„), h(F0, g(FÏ„)), Î¸) =d
dÏ„E
D11(W, g(FÏ„))
+âˆ‚
âˆ‚v{Î±02(X, v)Â·(Yâˆ’h0(X, v))}
v=VDÏ†g(FÏ„)(D, Z)
,
which gives the linearization result of the Theorem (LIN) .
To find the shape of the IF, note that the adjoint Dâˆ—
Ï†ofDÏ†is defined by the equation
E[Î´(D, Z)DÏ†g(D, Z)] =E[Dâˆ—
Ï†Î´(Z)g(Z)]. Therefore, by the Law of Iterated Expectations in equa-
tion (B.5), noting that Vâ‰¡Ï†(D, Z, g 0) is a function of ( D, Z):
d
dÏ„Â¯m(g0, h(F0, g(FÏ„)), Î¸) =d
dÏ„E
Eâˆ‚
âˆ‚v{Î±02(X, v)Â·(Yâˆ’h0(X, v))}
v=VDÏ†g(FÏ„)(D, Z)D, Z
=E[Î½(D, Z)DÏ†g(FÏ„)(D, Z)] =E[Dâˆ—
Ï†Î½(Z)g(FÏ„)(Z)],
with
Î½(d, z)â‰¡âˆ‚
âˆ‚v{Î±02(x, v)Â·(E[Y|D=d, Z=z]âˆ’h0(x, v))}
v=Ï†(d,z,g 0).
Again, we can use equation (3.2) to combine this last result with that in equation (B.2):
d
dÏ„Â¯m(g(FÏ„), h(F0, g(FÏ„)), Î¸) =d
dÏ„E[{r1(Z) +Dâˆ—
Ï†Î½(Z)}g(FÏ„)(Z)].
This is Assumption 1 in Ichimura and Newey (2022). Since Assumption 2 in that paper is satisfied in
our setup, Proposition 1 in Ichimura and Newey (2022) gives the shape of the IF: Ï•1(w, g 0, Î±01, Î¸) =
Î±01(z)Â· {dâˆ’g0(z)}. The parameter Î±01is the L2-projection:
Î±01= argmin
Î±âˆˆâˆ†1E[(ËœÎ½(Z)âˆ’Î±(Z))2], (B.6)
where Ëœ Î½=r1+Dâˆ—
Ï†Î½. â– 
Pr oof of Pr oposition 3.2: Start with the non-parametric case. Let v(g)â‰¡Ï†(d, z, g ) By the
triangle inequality (applied to the L2(W)-norm):
Z
Î±02(x, v(gÏ„))2dF0(w)1/2
â‰¤Z
Î±02(x, v(g0))2dF0(w)1/2
+Z
[Î±02(x, v(gÏ„))âˆ’Î±02(x, v(g0))]2dF0(w)1/2
The fist quantity in the RHS is finite since Î±02âˆˆL2(X, V). For quantity in the second row, by the
MVT and Hadamard differentiability of Ï†:
Z
(Î±02(x, v(gÏ„))âˆ’Î±02(x, v))2dF0(w)1/2
â‰¤Câˆ¥Ï†(gÏ„)âˆ’Ï†(g0)âˆ¥
=Câˆ¥Ï†(gÏ„)âˆ’Ï†(g0)âˆ’DÏ†(gÏ„âˆ’g0) +DÏ†(gâˆ’g0)âˆ¥ â‰¤C(âˆ¥DÏ†âˆ¥ Â· âˆ¥gÏ„âˆ’g0âˆ¥+o(âˆ¥gÏ„âˆ’g0âˆ¥)),
41where Cis the bound of âˆ‚Î±02/âˆ‚vand, in some abuse of notation, âˆ¥Â·âˆ¥denotes the norm in the
corresponding space.3LetÎµbe such that o(âˆ¥gÏ„âˆ’g0âˆ¥)â‰¤ âˆ¥gÏ„âˆ’g0âˆ¥. For Ï„ < Îµ :
Z
(Î±02(x, v(gÏ„))âˆ’Î±02(x, v))2dF0(w)1/2
â‰¤C(âˆ¥DÏ†âˆ¥+ 1)âˆ¥gÏ„âˆ’g0âˆ¥,
which is finite since Ï„7â†’gÏ„is a differentiable path in âˆ† 1.
For the partly linear case, where Î±02(x, v) =Î²â€²
0x+Îº0(v), simply note that
âˆ‚Î±02
âˆ‚v(x, v) =âˆ‚Îº0
âˆ‚v(x, v).
Thus, âˆ‚Îº0/âˆ‚vis bounded and we can proceed as above to show thatR
Îº0(v(gÏ„))2dF0(w) is finite. â– 
Pr oof of Pr oposition 3.3: We start with the non-parametric case. Throughout the proof, we
callhÏ„â‰¡h(F0, gÏ„). We note that hÏ„âˆˆâˆ†2(g0) =L2(X, V) if and only ifR
hÏ„(x, v)2dF0
xv(x, v)<âˆ.
Under the conditions in the statement of the proposition, by Cauchy-Schwarzâ€™ inequality:
Z
hÏ„(x, v)2dF0
xv(x, v) =Z
hÏ„(x, v)2Î½Ï„(x, v)dFÏ„
xv(x, v)â‰¤Z
hÏ„(x, v)4dFÏ„
xv(x, v)+Z
Î½Ï„(x, v)2dFÏ„
xv(x, v).
Furthermore, since hÏ„(X, V(gÏ„)) =E[Y|X, V(gÏ„)], by conditional Jensenâ€™s inequality and the Law
of Iterated Expectations
Z
hÏ„(x, v)4dFÏ„
xv(x, v) =E[E[Y|X, V(gÏ„)]4]â‰¤E[E[Y4|X, V(gÏ„)]] =E[Y4]<âˆ.
Also, since the Radon-Nikodym density of FÏ„
xvw.r.t. F0
xvisÎ½Ï„(x, v)âˆ’1(Shao, 2003, Prop. 1.7):
Z
Î½Ï„(x, v)2dFÏ„
xv(x, v) =Z
Î½Ï„(x, v)dF0
xv(x, v) =E[Î½Ï„(X, V)]<âˆ.
For the partly linear case we need to show thatR
ÎºÏ„(v)2dF0
v(v)<âˆ. We can proceed as above
to get: Z
ÎºÏ„(v)2dF0
v(v)â‰¤Z
ÎºÏ„(v)4dFÏ„
v(v) +E[Î½Ï„(V)],
where the second quantity in the RHS is finite by assumption. In the partly linear model we have
thatE[Y|V(gÏ„)] =Î²â€²
Ï„E[X|V(gÏ„)] +ÎºÏ„(V(gÏ„)). Therefore,
Z
ÎºÏ„(v)4dFÏ„
v(v) =E[ÎºÏ„(V(gÏ„))4] =E[E[Yâˆ’Î²â€²
Ï„X|V(gÏ„)]4]â‰¤E[(Yâˆ’Î²â€²
Ï„X)4].
This is finite if the expectation of the fourth-order cross-products between YandXis finite. â– 
3Namely, âˆ¥gâˆ’g0âˆ¥is the L2(Z)-norm of gâˆ’g0andâˆ¥DÏ†âˆ¥is the strong norm of DÏ†in the space of continuous
linear functionals from L2(Z) toL2(D, Z).
42The asymptotic normality and consistent estimation of the asymptotic variance result in Theo-
rem 5.1 relies on the following lemma:
Lemma B.1 Consider Assumptions 3.4, 5.3, 5.4.a, and 5.9. Let r, Î¾ > 0. Then, for Ëœhâ„“(w)â‰¡
Ë†hâ„“(x, Ï†(d, z,Ë†gâ„“))andËœÎ±2â„“(w)â‰¡Ë†Î±2â„“(x, Ï†(d, z,Ë†gâ„“)):
âˆ¥Ë†hâ„“âˆ’h0âˆ¥2=Op(nâˆ’r)â‡’ âˆ¥Ëœhâ„“âˆ’h0âˆ¥2=Op(nâˆ’r),and
âˆ¥Ë†Î±2â„“âˆ’Î±02âˆ¥2=op(nÎ¾nâˆ’r/2)â‡’ âˆ¥ËœÎ±2â„“âˆ’Î±02âˆ¥2=op(nÎ¾nâˆ’r/2).
Pr oof of Lemma B.1: Recall that Ë†hâ„“(x, v)â‰¡bJ(x, v)â€²Ë†Î·Jand Ë†Î±2â„“(x, v)â‰¡bJ(x, v)â€²Ë†ÏJ. We show
that
âˆ¥Ëœhâ„“âˆ’Ë†hâ„“âˆ¥2=Op(nâˆ’r).
The conclusion for Ëœ Î±2â„“follows the same reasoning.
LetËœbj(w)â‰¡bj(w, Ï†(d, z,Ë†gâ„“)). By the triangle inequality âˆ¥Ë†hâ„“âˆ’Ë†hâ„“âˆ¥2â‰¤PJ
j=1|Ë†Î·j|âˆ¥Ëœbjâˆ’bjâˆ¥2.
Moreover, by the Mean Value Theorem and Assumption 5.9.b,
âˆ¥Ëœbjâˆ’bjâˆ¥2
2=Z
(bj(x, Ï†(d, z,Ë†gâ„“))âˆ’bj(x, v))2dF0(w)â‰¤Îº2âˆ¥Ï†(Â·,Â·,Ë†gâ„“)âˆ’Ï†(Â·,Â·, g0)âˆ¥2
2.
Then, supjâ‰¤Jâˆ¥Ëœbjâˆ’bjâˆ¥2â‰¤Îºâˆ¥Ï†(Â·,Â·,Ë†gâ„“)âˆ’Ï†(Â·,Â·, g0)âˆ¥2. Also, since Ï†is Hadamard differentiable
(Assumption 3.4): âˆ¥Ï†(Â·,Â·,Ë†gâ„“)âˆ’Ï†(Â·,Â·, g0)âˆ’DÏ†(Ë†gâ„“âˆ’g0)âˆ¥2=op(âˆ¥Ë†gâ„“âˆ’g0âˆ¥) and âˆ¥DÏ†(Ë†gâ„“âˆ’g0)âˆ¥2â‰¤
Câˆ¥Ë†gâ„“âˆ’g0âˆ¥2(see Yamamuro, 1974, Result 1.2.6). Thus, by the triangle inequality:
sup
jâ‰¤Jâˆ¥Ëœbjâˆ’bjâˆ¥2â‰¤ÎºCâˆ¥Ë†gâ„“âˆ’g0âˆ¥2+op(âˆ¥Ë†gâ„“âˆ’g0âˆ¥2).
Now, Assumption 5.3 allows us to apply Lemma A9 in Chernozhukov et al. (2022b) to getPJ
j=1|Ë†Î·j|=Op(1). Therefore, if Assumption 5.4.a holds,
âˆ¥Ë†hâ„“âˆ’Ë†hâ„“âˆ¥2â‰¤JX
j=1|Ë†Î·j|âˆ¥Ëœbjâˆ’bjâˆ¥2â‰¤ JX
j=1|Ë†Î·j|!
(ÎºCâˆ¥Ë†gâ„“âˆ’g0âˆ¥2+op(âˆ¥Ë†gâ„“âˆ’g0âˆ¥2))
=Op(1)Â·[Op(nâˆ’r) +op(nâˆ’r)] =Op(nâˆ’r).(B.7)
The conclusion for âˆ¥Ëœhâ„“âˆ’h0âˆ¥2follows directly from equation (B.7) and the triangle inequality.
The conclusion for âˆ¥ËœÎ±2â„“âˆ’Î±02âˆ¥2follows equally, taking into account that Op(nâˆ’r) =op(nÎ¾nâˆ’r/2) for
Î¾ >0. â– 
Pr oof of Theorem 5.1: We start with asymptotic normality ofâˆšn(Ë†Î¸âˆ’Î¸0). The proof follows
standard GMM techniques and Theorem 9 in Chernozhukov et al. (2022b). A relevant deviation
from the previous results is that the estimators are evaluated at the generated regressor. Lemma B.1
allows to deal with that situation.
43The cornerstone of the result is Lemma 8 in Chernozhukov et al. (2022a), CEINR in what
follows, which states that:
âˆšnË†Ïˆ(Î¸0) =1âˆšnnX
i=1Ïˆ(Wi, g0, h0, Î±0, Î¸0) +op(1) (B.8)
under some conditions. We will apply Lemma 8 to a modified expansion of the difference be-
tween Ë†Ïˆ(Î¸0) and nâˆ’1/2Pn
i=1Ïˆ(Wi, g0, h0, Î±0, Î¸0) that allows to deal with estimators evaluated at the
generated regressor.
Consider first that equation (B.8) holds for each component of Ë†Ïˆ. Consistency of Ë†Î¸follows
under standard conditions that guarantee uniform convergence of Ë†Ïˆ(Î¸)â€²Ë†Î¥Ë†Ïˆ(Î¸) in Î˜ (c.f. Wooldridge,
2010, Th. 14.1). These conditions will follow from Assumption 5.8 if Î˜ is compact. Moreover, by
Assumptions 5.4.a and 5.8.a we can apply the Mean Value Theorem to get
âˆšn
Ë†Ïˆ(Ë†Î¸)âˆ’Ë†Ïˆ(Î¸0)
=âˆšnâˆ‚Ë†Ïˆ
âˆ‚Î¸(Â¯Î¸)Â·(Ë†Î¸âˆ’Î¸0)
forÂ¯Î¸a point between Î¸0andË†Î¸(that is Â¯Î¸Pâˆ’ â†’Î¸0). Then, if equation (B.8) holds:
âˆšnâˆ‚Ë†Ïˆ
âˆ‚Î¸(Â¯Î¸)Â·(Ë†Î¸âˆ’Î¸0) =1âˆšnnX
i=1Ïˆ(Wi, g0, h0, Î±0, Î¸0) +âˆšnË†Ïˆ(Ë†Î¸) +op(1). (B.9)
Now, note that
âˆ‚Ë†Ïˆ
âˆ‚Î¸(Î¸) =âˆ‚
âˆ‚Î¸ 
1
nLX
â„“=1X
iâˆˆIâ„“h
m(Wi,Ë†gâ„“,Ë†hâ„“, Î¸) +Ï•(Wi,Ë†gâ„“,Ë†hâ„“,Ë†Î±â„“,ËœÎ¸â„“)i!
=1
nLX
â„“=1X
iâˆˆIâ„“âˆ‚m
âˆ‚Î¸(Wi,Ë†gâ„“,Ë†hâ„“, Î¸)
Then, since Assumptions 5.4.a and 5.8, on top of Ë†Î¸Pâˆ’ â†’Î¸0, guarantee that we can apply Lemma E2
in CEINR, we have that âˆ‚Ë†Ïˆ(Ë†Î¸)/âˆ‚Î¸Pâˆ’ â†’M, so it is bounded in probability. Therefore, since Ë†Î¥ is also
Op(1), equation (B.9) implies
âˆ‚Ë†Ïˆ
âˆ‚Î¸(Ë†Î¸)â€²Ë†Î¥âˆ‚Ë†Ïˆ
âˆ‚Î¸(Â¯Î¸)Â·âˆšn(Ë†Î¸âˆ’Î¸0) =âˆ‚Ë†Ïˆ
âˆ‚Î¸(Ë†Î¸)â€²Ë†Î¥Â·1âˆšnnX
i=1Ïˆ(Wi, g0, h0, Î±0, Î¸0)
+âˆšnâˆ‚Ë†Ïˆ
âˆ‚Î¸(Ë†Î¸)â€²Ë†Î¥Ë†Ïˆ(Ë†Î¸) +op(1).
Thus, since ( âˆ‚Ë†Ïˆ(Ë†Î¸)/âˆ‚Î¸)â€²Ë†Î¥Ë†Ïˆ(Ë†Î¸) = 0 is the first-order condition for the minimization problem in
equation (4.3), âˆ‚Ë†Ïˆ(Â¯Î¸)/âˆ‚Î¸Pâˆ’ â†’M(by Lemma E2 in CEINR), and Mâ€²Î¥Mis non-sigular:
âˆšn(Ë†Î¸âˆ’Î¸0) = (Mâ€²Î¥M)âˆ’11âˆšnnX
i=1Ïˆ(Wi, g0, h0, Î±0, Î¸0) +op(1).
44Then, the asymptotic normality result follows from nâˆ’1/2Pn
i=1Ïˆ(Wi, g0, h0, Î±0, Î¸0)Dâˆ’ â†’N(0,Î¨).
It remains to verify the assumptions for Lemma 8 in CEINR, so that equation (B.8) holds
for each component of Ë†Ïˆ. First, to handle estimators evaluated at the generated regressor, we
provide a modified expansion of the difference between Ë†Ïˆ(Î¸0) and nâˆ’1/2Pn
i=1Ïˆ(Wi, g0, h0, Î±0, Î¸0).
LetÂ¯Ï•(w,Â¯v, g, h, Î± )â‰¡Î±1(z)Â·(dâˆ’g(z)) + Î±2(x,Â¯v)Â·(yâˆ’h(x, Ï†(d, z, g ))), which makes explicity
that Î±2is evaluated at ( x,Â¯v). Then, being Ë†Ïˆiâ„“(Î¸) given by equation (4.2), we have that Ë†Ïˆiâ„“(Î¸0)âˆ’
Ïˆ(Wi, g0, h0, Î±0, Î¸0) =Ë†R1iâ„“+Ë†R2iâ„“+Ë†R3iâ„“+Ë†âˆ†iâ„“, where
Ë†R1iâ„“â‰¡m(Wi,Ë†gâ„“,Ë†hâ„“, Î¸0)âˆ’m(Wi, g0, h0, Î¸0),
Ë†R2iâ„“â‰¡Â¯Ï•(Wi, Ï†(Di, Zi, g0),Ë†gâ„“,Ë†hâ„“, Î±0)âˆ’Ï•(Wi, g0, h0, Î±0, Î¸0),
Ë†R3iâ„“â‰¡Â¯Ï•(Wi, Ï†(Di, Zi,Ë†gâ„“), g0, h0,Ë†Î±â„“)âˆ’Ï•(Wi, g0, h0, Î±0, Î¸0),
Ë†âˆ†iâ„“â‰¡Ï•(Wi,Ë†gâ„“,Ë†hâ„“,Ë†Î±â„“,ËœÎ¸â„“)âˆ’Â¯Ï•(Wi, Ï†(Di, Zi, g0),Ë†gâ„“,Ë†hâ„“, Î±0),and
âˆ’Â¯Ï•(Wi, Ï†(Di, Zi,Ë†gâ„“), g0, h0,Ë†Î±â„“) +Ï•(Wi, g0, h0, Î±0, Î¸0).(B.10)
We will apply Lemma 8 in CEINR to this expansion.
Following Chernozhukov et al. (2022b), we begin by providing rates for estimation of Î±01and
Î±02. Assumption 5.2 allows us to apply Lemma A10 in Chernozhukov et al. (2022b) to get âˆ¥Ë†Bâ„“âˆ’
E[bJ(X, V)bJ(X, V)â€²]âˆ¥âˆ=Op(p
log(J)/n) andâˆ¥Ë†Câ„“âˆ’E[cK(Z)cK(Z)â€²]âˆ¥âˆ=Op(p
log(K)/n). The
fact that Assumption 5.5.b imposes a polynomial rate on JandKthen implies that Op(p
log(J)/n) =
Op(nâˆ’r) and Op(p
log(K)/n) =Op(nâˆ’r), since r <1/2 (Assumption 5.4). This, on top of Assump-
tions 5.1, 5.3, 5.4.b, and 5.5, means that we can apply Theorem 2 in Chernozhukov et al. (2022b)
to get:
âˆ¥Ë†Î±1â„“âˆ’Î±01âˆ¥2=op(nÎ¾nâˆ’r/2) andâˆ¥Ë†Î±2â„“âˆ’Î±02âˆ¥2=op(nÎ¾nâˆ’r/2)
for any Î¾ >0. We choose a Î¾satisfying 0 < Î¾ < (3râˆ’1)/2< r/ 2, which is possible since, by
Assumption 5.4, râˆˆ(1/3,1/2). This guarantees that
âˆ¥Ë†Î±1â„“âˆ’Î±01âˆ¥2=op(1) and âˆ¥Ë†Î±2â„“âˆ’Î±02âˆ¥2=op(1); and (B.11)
âˆšnâˆ¥Ë†Î±1â„“âˆ’Î±01âˆ¥2âˆ¥Ë†gâˆ’g0âˆ¥2=op(1) andâˆšnâˆ¥Ë†Î±2â„“âˆ’Î±02âˆ¥2âˆ¥Ë†hâ„“âˆ’h0âˆ¥=op(1), (B.12)
where the last line follows from Assumption 5.4.a.
We now check Assumption 1 in CEINR. Assumption 1.i is identical to Assumption 5.6.a and
5.6.b. To show the remaining points, define Ëœhâ„“(w)â‰¡Ë†hâ„“(x, Ï†(d, z,Ë†gâ„“)) and Ëœ Î±2â„“(w)â‰¡Ë†Î±2â„“(x, Ï†(d, z,Ë†gâ„“)).
Note also that by Assumptions 5.2 and 5.3.a:
|Î±01(Z)| â‰¤âˆX
k=1|Î²k||ck(Z)| â‰¤sup
kâˆˆN|ck(Z)| Â·âˆX
k=1|Î²k| â‰¡Îº1<âˆand
|Î±02(X, V)| â‰¤âˆX
j=1|Ïj||bj(X, V)| â‰¤sup
jâˆˆN|bj(X, V)| Â·âˆX
j=1|Ïj| â‰¡Îº2<âˆ.
45Thus, by the triangle inequality, being vâ‰¡Ï†(d, z, g 0):
Zh
Â¯Ï•(w, Ï†(d, z, g 0),Ë†gâ„“,Ë†hâ„“, Î±0)âˆ’Ï•(w, g 0, h0, Î±0, Î¸0)i2
dF0(w)â‰¤Z
Î±01(z)2[Ë†gâ„“(z)âˆ’g0(z)]2dF0(w)
+Z
Î±02(x, v)2h
Ëœhâ„“(w)âˆ’h0(x, v)i2
dF0(w)
â‰¤Îº2
1âˆ¥Ë†gâ„“âˆ’g0âˆ¥2
2+Îº2
2âˆ¥Ëœhâ„“âˆ’h0âˆ¥2
2.
Assumption 1.ii in CEINR follows from the above display, Assumption 5.4.a, and Lemma B.1.
Also, calling Îº3, Îº4<âˆto the bounds given by Assumption 5.6.d:
ZÂ¯Ï•(w, Ï†(d, z,Ë†gâ„“), g0, h0,Ë†Î±â„“)âˆ’Ï•(w, g0, h0, Î±0, Î¸0)2dF0(w)â‰¤Z
[dâˆ’g0(z)]2[Ë†Î±1â„“(z)âˆ’Î±01(z)]2dF0(w)
+Z
[yâˆ’h0(x, v)]2[ËœÎ±2â„“(w)âˆ’Î±02(x, v)]2dF0(w)
â‰¤Îº2
3âˆ¥Ë†Î±1â„“âˆ’Î±01âˆ¥2
2+Îº2
4âˆ¥ËœÎ±2â„“âˆ’Î±02âˆ¥2
2,
Thus, Assumption 1.iii in CEINR follows from the above display, equation (B.11), and Lemma B.1.
We now move to check Assumption 2 in CEINR. In particular, we show that 2.iii holds. We have that
Ë†âˆ†iâ„“= [Ë†Î±1â„“(Zi)âˆ’Î±01(Zi)]Â·[Ë†gâ„“(Zi)âˆ’g0(Zi)] + [ËœÎ±2â„“(Wi)âˆ’Î±02(Xi, Vi)]Â·[Ëœhâ„“(Wi)âˆ’h0(Xi, Vi)].
Thus, as in Chernozhukov et al. (2022b, proof of Th. 9), an application of the Cauchy-Schwarz, conditional
Markov, and triangle inequalities leads to:
1âˆšnX
iâˆˆIâ„“Ë†âˆ†iâ„“=Op(âˆšnâˆ¥Ë†Î±1â„“âˆ’Î±01âˆ¥2âˆ¥Ë†gâ„“âˆ’g0âˆ¥2) +Op(âˆšnâˆ¥ËœÎ±2â„“âˆ’Î±01âˆ¥2âˆ¥Ëœhâ„“âˆ’h0âˆ¥2).
Thus, by equation (B.12) and Lemma B.1, Assumption 2.iii in CEINR is satisfied.
To see that Assumption 3.iii in CEINR holds, note that by Assumptions 3.1-3.4: E[D1(W, g))] =
E[Î±01(Z)g(Z)] and E[D2(W, h(Â·, Ï†(Â·,Â·, g)))] = E[Î±02(X, V)h(X, Ï†(D, Z, g ))]. The last equality also uses
Assumption 5.9.c. Moreover, Dâˆ’g0(Z) and Yâˆ’h0(X, V) are orthogonal to âˆ† 1and âˆ† 2(g0), respectively.
Then,
E[Â¯Ï•(W, Ï†(D, Z, g 0), g, h, Î± 0)] =E[Î±01(Z)(g0(Z)âˆ’g(Z))] +E[Î±02(X, V)(h0(X, V)âˆ’h(X, Ï†(D, Z, g )))]
=âˆ’E[D1(W, gâˆ’g0)]âˆ’E[D2(W, h(Â·, Ï†(Â·,Â·, g))âˆ’h0)].
Thus, by Assumption 5.7, for âˆ¥gâˆ’g0âˆ¥2< Îµandâˆ¥hâˆ’h0âˆ¥2< Îµ:
E[m(W, g, h, Î± 0, Î¸0) +Â¯Ï•(W, Ï†(D, Z, g 0), g, h, Î± 0)]
=|E[m(W, g, h, Î¸ 0)âˆ’m(W, g 0, h0, Î¸0)âˆ’D1(W, gâˆ’g0)âˆ’D2(W, h(Â·, Ï†(Â·,Â·, g))âˆ’h0)]|
â‰¤C 
âˆ¥gâˆ’g0âˆ¥2
2+âˆ¥h(Â·, Ï†(Â·,Â·, g))âˆ’h0âˆ¥2
2
.
The above display, on top of Assumption 5.4.a and Lemma B.1, gives Assumption 3.iii in CEINR for the
functional ( g, h)7â†’E[m(W, g, h, Î± 0, Î¸0) +Â¯Ï•(W, Ï†(D, Z, g 0), g, h, Î± 0)].
46To conclude, we verify that Lemma 8 in CEINR can be applied to our modified expansion. Being Ic
â„“
all observations not in Iâ„“, note that
E[Ë†R1iâ„“+Ë†R2iâ„“|Ic
â„“] =E[m(W,Ë†gâ„“,Ë†hâ„“, Î±0, Î¸0) +Â¯Ï•(W, Ï†(D, Z, g 0),Ë†gâ„“,Ë†hâ„“, Î±0)|Ic
â„“] and
E[Ë†R3iâ„“|Ic
â„“] = 0.
The last equation follows from orthogonality of Dâˆ’g0(Z) and Yâˆ’h0(X, V) to âˆ† 1and âˆ† 2(g0), respectively,
and Assumption 5.9.c. This means that the strategy of Lemma 8 can be applied to our expansion (for
more details, we refer to the proof of the lemma in Chernozhukov et al., 2022a).
We conclude the proof of Theorem 5.1 by providing consistency of Ë†Î. Call Ïˆiâ‰¡Ïˆ(Wi, g0, h0, Î±0, Î¸0)
andÂ¯Î¨â‰¡nâˆ’1Pn
i=1ÏˆiÏˆâ€²
i. We have that
âˆ¥Ë†Î¨âˆ’Â¯Î¨âˆ¥âˆâ‰¤LX
â„“=11
nX
iâˆˆIâ„“
âˆ¥Ë†Ïˆiâ„“âˆ’Ïˆiâˆ¥2
âˆ+ 2âˆ¥Ë†Ïˆiâ„“âˆ’Ïˆiâˆ¥âˆâˆ¥Ïˆiâˆ¥âˆ
We now expand Ë†Ïˆiâ„“(ËœÎ¸â„“)âˆ’Ïˆ(Wi, g0, h0, Î±0, Î¸0) =Ë†R1iâ„“+Ë†R2iâ„“+Ë†R3iâ„“+Ë†R4iâ„“+Ë†âˆ†iâ„“, with
Ë†R4iâ„“â‰¡m(Wi,Ë†gâ„“,Ë†hâ„“,ËœÎ¸â„“)âˆ’m(Wi,Ë†gâ„“,Ë†hâ„“, Î¸0)
and the remaining terms are given in equation (B.10). Then
1
nX
iâˆˆIâ„“âˆ¥Ë†Ïˆiâ„“âˆ’Ïˆiâˆ¥2
âˆâ‰¤C1
nX
iâˆˆIâ„“
âˆ¥Ë†R1iâ„“âˆ¥2
âˆ+âˆ¥Ë†R2iâ„“âˆ¥2
âˆ+âˆ¥Ë†R3iâ„“âˆ¥2
âˆ+âˆ¥Ë†R4iâ„“âˆ¥2
âˆ+âˆ¥Ë†âˆ†iâ„“âˆ¥2
âˆ
by the triangle inequality. The constant Ccomes from the presence of the interation terms: for instance,
we have that 2 âˆ¥Ë†R1iâ„“âˆ¥âˆâˆ¥Ë†R2iâ„“âˆ¥âˆâ‰¤2 max{âˆ¥Ë†R1iâ„“âˆ¥âˆ,âˆ¥Ë†R2iâ„“âˆ¥âˆ}.
We apply Assumptions 5.6.b and 5.6.c to each component of Ë†R1iâ„“and Ë†R4iâ„“, respectively. This yields
E[âˆ¥Ë†R1iâ„“âˆ¥2
âˆ|Ic
â„“]Pâˆ’ â†’0 and E[âˆ¥Ë†R4iâ„“âˆ¥2
âˆ|Ic
â„“]Pâˆ’ â†’0. Moreover, by the argument we have followed to show that
Assumption 1.ii and 1.ii in CEINR are satisfied: E[âˆ¥Ë†R2iâ„“âˆ¥2
âˆ|Ic
â„“]Pâˆ’ â†’0 and E[âˆ¥Ë†R3iâ„“âˆ¥2
âˆ|Ic
â„“]Pâˆ’ â†’0. Also, by the
Cauchy-Schwarz inequality, equation (B.12) and Lemma B.1 (applied to each component):
E[âˆ¥Ë†âˆ†iâ„“âˆ¥2
âˆ|Ic
â„“]â‰¤3
âˆ¥Ë†Î±1â„“âˆ’Î±01âˆ¥2âˆ¥Ë†gâ„“âˆ’g0âˆ¥2+âˆ¥ËœÎ±2â„“âˆ’Î±02âˆ¥2âˆ¥Ëœhâ„“âˆ’h0âˆ¥2
=op(1).
Thus, collecting the above results:
Eï£®
ï£°1
nX
iâˆˆIâ„“âˆ¥Ë†Ïˆiâ„“âˆ’Ïˆiâˆ¥2
âˆIc
â„“ï£¹
ï£»â‰¤CEh
âˆ¥Ë†R1iâ„“âˆ¥2
âˆ+âˆ¥Ë†R2iâ„“âˆ¥2
âˆ+âˆ¥Ë†R3iâ„“âˆ¥2
âˆ+âˆ¥Ë†R4iâ„“âˆ¥2
âˆ+âˆ¥Ë†âˆ†iâ„“âˆ¥2
âˆIc
â„“i
=op(1).
An application of the conditional Markov inequality gives then nâˆ’1P
iâˆˆIâ„“âˆ¥Ë†Ïˆiâ„“âˆ’Ïˆiâˆ¥2
âˆ=op(1). Also, by
Assumptions 5.2, 5.3.a, 5.6.a, and 5.6.d: E[ÏˆiÏˆâ€²
i]<âˆ. So, by the Law of Large Numbers, Â¯Î¨Pâˆ’ â†’E[ÏˆiÏˆâ€²
i].
Therefore, by Cauchy-Schwarz:
âˆ¥Ë†Î¨âˆ’Â¯Î¨âˆ¥âˆâ‰¤LX
â„“=1ï£®
ï£°1
nX
iâˆˆIâ„“âˆ¥Ë†Ïˆiâ„“âˆ’Ïˆiâˆ¥2
âˆ+ 2s
1
nX
iâˆˆIâ„“âˆ¥Ë†Ïˆiâ„“âˆ’Ïˆiâˆ¥2âˆs
1
nX
iâˆˆIâ„“âˆ¥Ïˆiâˆ¥2âˆï£¹
ï£»
=op(1) + op(1)Â·Op(1) = op(1).
This leads to Ë†Î¨ = Â¯Î¨ +op(1)Pâˆ’ â†’E[ÏˆiÏˆâ€²
i]. â– 
47Pr oof of Pr oposition 5.1: We start by formally checking Assumptions 3.1 and 3.2, i.e., that
E[D11(W, g)] and E[D2(W, h)] are continuous. This is guaranteed by Assumption 5.10. Being C1
the bound of âˆ‚h/âˆ‚v andC2the bound of Î±02,
|E[D11(W, g)]|=E
âˆ’Zâˆ‚h0
âˆ‚v(xâˆ—, V)dFâˆ—(xâˆ—)Â·g(Z)â‰¤CE[|g(Z)|]â‰¤C1âˆ¥gâˆ¥2,
|E[D2(W, h)]| â‰¤ âˆ¥Î±02âˆ¥2âˆ¥hâˆ¥2â‰¤C2âˆ¥hâˆ¥2,
where we have used Jensenâ€™s and Cauchy-Schwarzâ€™ inequalities.
We continue with Assumption 5.6. If the CASF is well defined, Assumption 5.6.a is equivalent
to
E"Z
h0(xâˆ—, V)dFâˆ—(xâˆ—2#
<âˆ.
By Jensenâ€™s inequality:
E"Z
h0(xâˆ—, V)dFâˆ—(xâˆ—2#
â‰¤Z
h0(xâˆ—, v)2fâˆ—(xâˆ—)fv
0(v)dxâˆ—dv=E[Î±02(X, V)h0(X, V)2].
This is finite by Assumption 5.10.a: E[Î±02(X, V)h0(X, V)]â‰¤C2E[Y2]<âˆ.
To check Assumption 5.6.b, again by Jensenâ€™s inequality:
Z
[m(w,Ë†gâ„“,Ë†hâ„“, Î¸0)âˆ’m(w, g 0, h0, Î¸0)]2dF0(w) =ZZ
[Ë†hâ„“(xâˆ—, Ï†(d, z,Ë†gâ„“))âˆ’h0(x, v)]dFâˆ—(xâˆ—)2
dF0(w)
â‰¤Z
Ë†hâ„“(xâˆ—, Ï†(d, z,Ë†gâ„“))âˆ’h0(x, v)2
dFâˆ—(xâˆ—)dF0(w).
Under Assumption 5.9.c:
Z
Ë†hâ„“(xâˆ—, Ï†(d, z,Ë†gâ„“))âˆ’h0(x, v)2
dFâˆ—(xâˆ—)dF0(w)
=Z
Î±02(x, v)
Ë†hâ„“(xâˆ—, Ï†(d, z,Ë†gâ„“))âˆ’h0(x, v)2
dF0(x, v)
â‰¤C2âˆ¥Ëœhâ„“âˆ’h0âˆ¥2
2,
where Ëœhâ„“(w)â‰¡Ë†hâ„“(x, Ï†(d, z,Ë†gâ„“)). This converges to zero by Lemma B.1.
Regarding Assumption 5.6.c, since m(w,Ë†gâ„“,Ë†hâ„“,ËœÎ¸â„“)âˆ’m(w,Ë†gâ„“,Ë†hâ„“, Î¸0) =ËœÎ¸â„“âˆ’Î¸0, it suffices to have
a consistent estimator of the CASF.
We now check Assumption 5.7. The idea is to show that Â¯ m(g, h, Î¸ ) is twice continuously differ-
entiable along the paths g(F) and h(F, g(F)). Let ( g, h) be an arbitrary point in the neighborhood
of (g0, h0). Set V(g)â‰¡Dâˆ’g(Z) and recall that Vâ‰¡Dâˆ’g0(Z). As in the computations in the
main text, the first derivatives at ( g, h) are (using Assumption 5.9.c):
DgÂ¯m(g, h, Î¸ ) =âˆ’Zâˆ‚h
âˆ‚v(xâˆ—, V(g))dFâˆ—(xâˆ—) +âˆ‚h
âˆ‚v(X, V(g))Î±02(X, V(g)),and
DhÂ¯m(g, h, Î¸ ) =Î±02(X, V(g)).
48Note that both derivatives map ( g, h) onto L2(D, Z). Then, for instance, the second derivative w.r.t.
gat point ( g0, h0) (denoted DggÂ¯m(g0, h0, Î¸)) maps gâˆˆâˆ†1to the space of linear maps between âˆ† 1
andL2(D, Z). To characterize it, take bâˆˆL2(D, Z). By the characterization of the derivative in
Yamamuro (1974, p. 9), for a path Ï„7â†’gÏ„with âˆ‚gÏ„/âˆ‚Ï„= Ëœg:
E[bÂ·DggÂ¯m(g0, h0, Î¸)(Ëœg)] =âˆ‚
âˆ‚Ï„E[bÂ·DgÂ¯m(gÏ„, h0, Î¸)] =E
bÂ·âˆ‚
âˆ‚Ï„DgÂ¯m(gÏ„, h0, Î¸)
Since this holds for all b, we have that DggÂ¯m(g0, h0, Î¸)(Ëœg) =âˆ‚DgÂ¯m(gÏ„, h0, Î¸)/âˆ‚Ï„. In the case of the
first derivative, this gives:
DggÂ¯m, Î¸(g0, h0) (Ëœg) =Zâˆ‚2h0
âˆ‚v2(xâˆ—, V)dFxâˆ—(xâˆ—)âˆ’âˆ‚2h0
âˆ‚v2(X, V)Î±02(X, V)
âˆ’âˆ‚h0
âˆ‚v(X, V)âˆ‚Î±02
âˆ‚v(X, V)
Ëœg(Z).
Thus, by Assumption 5.10, âˆ¥DggÂ¯m(g0, h0, Î¸) (Ëœg)âˆ¥2â‰¤âˆš
Câˆ¥Ëœgâˆ¥2, where Cis the corresponding sum of
the bounds of the derivatives of h0andÎ±02.
Note that, since DhÂ¯m(g, h, Î¸ ) does not depend on h(i.e., Â¯ mis linear in h),DhhÂ¯m(g0, h0, Î¸) = 0.
The cross derivative is:
DghÂ¯m(g0, h0, Î¸)(Ëœg) =âˆ‚DhÂ¯m(gÏ„, h0, Î¸)
âˆ‚Ï„=âˆ’âˆ‚Î±02
âˆ‚v(X, V),Ëœg(Z)
which is also bounded (i.e., continuous) under Assumption 5.10. Therefore, by Proposition 3 in
Luenberger (1997, Sec. 7.3), Assumption 5.7 is satisfied (see also Th. 3 in Chernozhukov et al.,
2022a).
Regarding Assumption 5.8, this is trivial in case of the moment condition identifying the CASF.
Simply note that
âˆ‚m
âˆ‚Î¸(W, g, h, Î¸ ) =âˆ’1.
â– 
49References
Ahn, H. and Powell, J. L. (1993). Semiparametric estimation of censored selection models with a
nonparametric selection mechanism. Journal of Econometrics , 58(1-2):3â€“29.
Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of Lasso and Dantzig
selector. The Annals of Statistics , 37(4):1705 â€“ 1732.
Blundell, R. and Powell, J. L. (2003). Endogeneity in nonparametric and semiparametric regression
models. Econometric society monographs , 36:312â€“357.
Blundell, R. W. and Powell, J. L. (2004). Endogeneity in semiparametric binary response models.
The Review of Economic Studies , 71(3):655â€“679.
Bunea, F., Tsybakov, A., and Wegkamp, M. (2007). Sparsity oracle inequalities for the Lasso.
Electronic Journal of Statistics , 1(none):169 â€“ 194.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins,
J. (2018). Double/debiased machine learning for treatment and structural parameters. The
Econometrics Journal , 21(1):C1â€“C68.
Chernozhukov, V., Escanciano, J. C., Ichimura, H., Newey, W. K., and Robins, J. M. (2022a).
Locally robust semiparametric estimation. Econometrica , 90(4):1501â€“1535.
Chernozhukov, V., Newey, W. K., and Singh, R. (2022b). Automatic debiased machine learning of
causal and structural effects. Econometrica , 90(3):967â€“1027.
Das, M., Newey, W. K., and Vella, F. (2003). Nonparametric estimation of sample selection models.
The Review of Economic Studies , 70(1):33â€“58.
Escanciano, J. C., Jacho-ChÂ´ avez, D., and Lewbel, A. (2016). Identification and estimation of
semiparametric two-step models. Quantitative Economics , 7(2):561â€“589.
Escanciano, J. C., Jacho-ChÂ´ avez, D. T., and Lewbel, A. (2014). Uniform convergence of weighted
sums of non and semiparametric residuals for estimation and testing. Journal of Econometrics ,
178:426â€“443.
Fong, C. and Tyler, M. (2021). Asymptotic variance of semiparametric estimators with generated
regressors. Political Analysis , 29(4):467â€“484.
Garen, J. (1984). The returns to schooling: A selectivity bias approach with a continuous choice
variable. Econometrica: Journal of the Econometric Society , pages 1199â€“1218.
50Hahn, J., Liao, Z., Ridder, G., and Shi, R. (2023). The influence function of semiparametric two-step
estimators with estimated control variables. Economics Letters , 231:111â€“277.
Hahn, J. and Ridder, G. (2013). Machine learning predictions as regression covariates. Economet-
rica, 81(1):315â€“340.
Hahn, J. and Ridder, G. (2019). Three-stage semi-parametric inference: Control variables and
differentiability. Journal of econometrics , 211(1):262â€“293.
Heckman, J. J. (1979). Sample selection bias as a specification error. Econometrica: Journal of the
econometric society , pages 153â€“161.
Heckman, J. J., Ichimura, H., and Todd, P. (1998). Matching as an econometric evaluation estima-
tor.The review of economic studies , 65(2):261â€“294.
Heckman, J. J. and Vytlacil, E. (2005). Structural equations, treatment effects, and econometric
policy evaluation 1. Econometrica , 73(3):669â€“738.
Ichimura, H. and Lee, L.-F. (1991). Semiparametric least squares estimation of multiple index
models: single equation estimation. In Nonparametric and semiparametric methods in economet-
rics and statistics: Proceedings of the Fifth International Symposium in Economic Theory and
Econometrics. Cambridge , pages 3â€“49.
Ichimura, H. and Newey, W. K. (2022). The influence function of semiparametric estimators.
Quantitative Economics , 13(1):29â€“61.
Imbens, G. W. and Newey, W. K. (2009). Identification and estimation of triangular simultaneous
equations models without additivity. Econometrica , 77(5):1481â€“1512.
Luenberger, D. G. (1997). Optimization by vector space methods . John Wiley & Sons.
Mammen, E., Rothe, C., and Schienle, M. (2012). Nonparametric regression with nonparametrically
generated covariates. The Annals of Statistics , 40(2):1132â€“1170.
Mammen, E., Rothe, C., and Schienle, M. (2016). Semiparametric estimation with generated
covariates. Econometric Theory , 32(5):1140â€“1177.
Newey, W. K. (1994). The asymptotic variance of semiparametric estimators. Econometrica ,
62(6):1349â€“1382.
Newey, W. K. (1997). Convergence rates and asymptotic normality for series estimators. Journal
of econometrics , 79(1):147â€“168.
51Newey, W. K., Powell, J. L., and Vella, F. (1999). Nonparametric estimation of triangular simulta-
neous equations models. Econometrica , 67(3):565â€“603.
PÂ´ erez-Izquierdo, T. J. (2022). The determinants of counterfactual identification in the binary choice
model with endogenous regressors. Unpublished manuscript.
Rivers, D. and Vuong, Q. H. (1988). Limited information estimators and exogeneity tests for
simultaneous probit models. Journal of econometrics , 39(3):347â€“366.
Robinson, P. M. (1988). Root-n-consistent semiparametric regression. Econometrica: Journal of
the Econometric Society , pages 931â€“954.
Rothe, C. (2009). Semiparametric estimation of binary response models with endogenous regressors.
Journal of Econometrics , 153(1):51â€“64.
Sasaki, Y. and Ura, T. (2021). Estimation and inference for policy relevant treatment effects.
Journal of Econometrics .
Shao, J. (2003). Mathematical statistics . Springer Science & Business Media.
Stock, J. H. (1989). Nonparametric policy analysis. Journal of the American Statistical Association ,
84(406):567â€“575.
Stock, J. H. (1991). Nonparametric policy analysis: an application to estimating hazardous waste
cleanup benefits. Nonparametric and Semiparametric Methods in Econometrics and Statistics ,
pages 77â€“98.
Wooldridge, J. M. (2010). Econometric analysis of cross section and panel data . The MIT Press.
Wooldridge, J. M. (2015). Control function methods in applied econometrics. Journal of Human
Resources , 50(2):420â€“445.
Yamamuro, S. (1974). Differential calculus in topological linear spaces , volume 374. Springer.
Zhang, C.-H. and Huang, J. (2008). The sparsity and bias of the Lasso selection in high-dimensional
linear regression. The Annals of Statistics , 36(4):1567 â€“ 1594.
52