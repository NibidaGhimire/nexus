New multivariate Gini’s indices
Marco Capaldo1∗Jorge Navarro2†
1Dipartimento di Matematica, Universit` a degli Studi di Salerno
Via Giovanni Paolo II, 132, I-84084 Fisciano (SA), Italy
2Departamento de Estad´ ıstica e Investigaci´ on Operativa, Facultad de Matem´ atica
Universidad de Murcia, Murcia 30100, Spain
Email: mcapaldo@unisa.it, jorgenav@um.es
December 14, 2023
Abstract
The Gini’s mean difference was defined as the expected absolute difference between a random variable
and its independent copy. The corresponding normalized version, namely Gini’s index, denotes two
times the area between the egalitarian line and the Lorenz curve. Both are dispersion indices because
they quantify how far a random variable and its independent copy are. Aiming to measure dispersion in
the multivariate case, we define and study new Gini’s indices. For the bivariate case we provide several
results and we point out that they are “dependence-dispersion” indices. Covariance representations
are exhibited, with an interpretation also in terms of conditional distributions. Further results, bounds
and illustrative examples are discussed too. Multivariate extensions are defined, aiming to apply both
indices in more general settings. Then, we define efficiency Gini’s indices for any semi-coherent system
and we discuss about their interpretation. Empirical versions are considered in order as well to apply
multivariate Gini’s indices to data.
Mathematics Subject Classification : 62H05, 91B82, 60E15.
Keywords: Gini’s index, Gini’s mean difference, Dispersion measure, Copula, Bound, Coherent
system.
1 Introduction
Information measures provide more specific details about random variables than those given by mean
values, variances, and others moments. In this sense, they allow to broaden the knowledge about
phenomena shaped by random variables. The Gini’s index and the Gini’s mean difference are two
popular dispersion measures. The Gini’s index is a well known measure of income inequality in a
population related with the Lorenz curve. For its history and alternative formulations we refer the
reader to Arnold and Sarabia [3] and Ceriani and Verme [7].
Given a nonnegative random variable X, with cumulative distribution function (c.d.f.) F(t) =
Pr(X≤t) and survival function (s.f.) ¯F= 1−F, the Gini’s mean difference of Xwas defined as
GMD (X):=E|X−X′|= 2Z+∞
0F(t)¯F(t)dt, (1.1)
where X′is an independent copy of X(see, for instance, Arnold and Sarabia [3] and Yitzhaki [35]).
This information measure quantifies the variability between two independent and identically dis-
tributed (i.i.d.) random variables by using the absolute mean difference. In other words, the Gini’s
∗ORCID: 0000-0002-0255-8935
†ORCID: 0000-0003-2822-915X
1arXiv:2401.01980v2  [math.PR]  5 Jan 2024mean difference measures how far they are respect to the egalitarian line, and in this sense it can be
seen as dispersion index. A normalized version of Eq. (1.1), namely Gini’s index of X, was defined as
G(X):=GMD (X)
2E(X)∈[0,1] (1.2)
where 0 <E(X)<∞. It represents twice the area between the egalitarian line and the Lorenz curve
(see, for example, Gastwirth [14] and Arnold and Sarabia [3], p. 52). We point out that the Gini’s
indices given in Eqs. (1.1) and (1.2) share many properties with the variance, since
GMD (X) = 4 Cov(X, F(X)), G(X) =2
E(X)Cov(X, F(X)). (1.3)
They can be informative about the properties of distributions that depart from normality, see Yitzhaki
[35]. For this and other reasons, Eq. (1.2) plays an important role in economics and social sciences,
when the normal distribution does not provide a good approximation to data (see Yitzhaki [35] and
references therein).
Some generalizations and extensions of the Gini’s index have been considered in the literature,
see, for instance, Koshevoy and Mosler [16], Sections 5.3.2 and 7.4.1 in Arnold and Sarabia [3] and
references therein. Moreover, different versions of the Gini’s indices introduced above can be found in
the context of concordance measures. For example, if ( X, Y) has copula C, with absolutely continuous
marginal distributions, then the population version of Gini’s measure of association for XandYis
given by
γC= 4Z1
0C(u,1−u)du−Z1
0(u−C(u, u))du
,
see Section 5.1.4 of Nelsen [23] and Fuchs and Tschimpke [13]. Recently, the copula-distorted Gini’s
mean difference of Xhas been defined in Capaldo et al. [5], aiming to generalize Eq. (1.1) for the
cases in which the random variables involved are possibly dependent and where one s.f. (or c.d.f.)
is distorted from the other, making use of a distortion function in a parametric family. Since the
absolute mean difference can be view as a sort of distance between the random variables involved,
related minimizing problems of distance can be found in Capaldo et al. [5] and Ortega-Jim´ enez et
al. [24]. Moreover, see Ortega-Jim´ enez et al. [25] for stochastic comparisons between absolute mean
differences.
In this paper for a random vector ( X, Y) we propose and study a bivariate version of the Gini’s
mean difference in Eq. (1.1), defined as GMD (X, Y) = E|X−Y|. In this case the distribution of
Yis not necessarily equal to the distribution of Xand they can be dependent. We also define a
bivariate version of the Gini’s index in Eq. (1.2). We extend both Gini’s indices to the multivariate
case. Moreover we focus on the meaning of the new measures that represent “dependence-dispersion”
indices. Along this line, for fixed marginal distributions, we prove that the indices decrease when the
positive dependence increases. It is also related with the Schur-concavity of the underline copula.
Additionally, for fixed dependence we prove that the indices decrease as the marginal dispersion
decreases. We also show how they can be informative about the reliability of a system and, in this
sense, we define their efficiency versions for any semi-coherent system. Empirical versions are also
considered aiming to provide applications for the new Gini’s indices to data. We show some illustrative
examples where we find interpretations in terms of areas.
The paper is organized as follows. In Section 2 we recall useful notions regarding stochastic orders,
copula functions with related properties, and some probabilistic inequalities. Section 3 is devoted to
the bivariate Gini’s mean difference and bivariate Gini’s index. After definitions we obtain covariance
representations for the new measures. Moreover, we show basic properties and provide several results,
by extending them to the multivariate case. Section 4 contains bounds and inequalities for both
bivariate Gini’s indices. In Section 5 we define the efficiency Gini’s mean difference for any semi-
coherent system. We also define an efficiency version for the bivariate Gini’s index and we discuss
about its interpretation. Examples and simulations are given in Section 6, by providing also empirical
versions for the new Gini’s indices.
22 Background and notations
In this section we fix the notation by recalling preliminary notions such as usual stochastic and
dispersive orders, copula functions and useful probabilistic inequalities. Throughout the paper the
terms increasing and decreasing are used as “non-decreasing” and “non-increasing”, respectively. In
this paper we consider only nonnegative random variables, namely random lifetime . However we must
note that some results also holds without this assumption.
For a random lifetime Xwith c.d.f. F, the quantile function is denoted as F−1(u) = sup {x:F(x)≤
u}, for u∈[0,1]. In addition, in the absolutely continuous case, the probability density function
(p.d.f.) is defined as f(x) =F′(x) at points xsuch that Fis derivable. We assume that 0 < µ =
E(X) =R+∞
0¯F(t)dt <∞, where ¯F= 1−F.
We say that Xis smaller than Yin the usual stochastic order , denoted by X≤stY, if and only
if¯FX(t)≤¯FY(t), for all t≥0. If there is equality in law, then we write X=stY. We recall that X
is said to be smaller than Yin the dispersive order , denoted as X≤dY, if and only if
F−1
X(v)−F−1
X(u)≥F−1
Y(v)−F−1
Y(u) whenever 0 < u≤v <1.
In addition, if XandYare absolutely continuous, then
X≤dYif and only if fX(F−1
X(u))≥fY(F−1
Y(u))∀u∈(0,1).
While the usual stochastic order is used to compare two random variables in size, the dispersive
order compares their dispersion. Moreover, if X≤stYthen E(X)≤E(Y), while for X≤dYone
hasVar(X)≤Var(Y). For more details about stochastic orders we refer the reader to Shaked and
Shanthikumar [31].
From Sklar’s theorem a multivariate distribution can be written in terms of the marginal distri-
butions, making use of a suitable function, namely copula function . Thus, given a random vector
X= (X1, . . . , X n) with joint c.d.f.
FX(x1, . . . , x n) =Pr(X1≤x1, . . . , X n≤xn)
and marginal c.d.f.’s FX1, . . . , F Xn, it follows
FX(x1, . . . , x n) =C(FX1(x1), . . . , F Xn(xn)), (2.1)
forx1, . . . , x n∈R, where Cis called copula function of X. If the marginal distributions are all
continuous then the copula is unique. Similarly, referring to the joint s.f.
¯FX(x1, . . . , x n) =Pr(X1> x1, . . . , X n> xn)
and marginal s.f.’s ¯FX1, . . . , ¯FXn, it follows
¯FX(x1, . . . , x n) =bC(¯FX1(x1), . . . , ¯FXn(xn)), (2.2)
forx1, . . . , x n∈R, where bCis called survival copula function ofX. Moreover, the functions
δ(u) =C(u, . . . , u ),bδ(u) =bC(u, . . . , u ), u ∈[0,1],
represent the diagonal sections of CandbC, respectively.
We recall that X= (X1, . . . , X n) is said to be exchangeable (exc.) if
(X1, . . . , X n) =st(Xσ(1), . . . , X σ(n))
for all permutations σof order n. From exchangeability it follows that X1, . . . , X nare identically
distributed (i.d.).
For simplicity in the bivariate case we now recall some properties regarding copulas. First, for all
u, v∈[0,1] the relation between CandbCis given by
bC(u, v) =u+v−1 +C(1−u,1−v). (2.3)
3Moreover, for every copula Cand for all u, v∈[0,1] one has
W(u, v)≤C(u, v)≤M{u, v}, (2.4)
where W(u, v) = max {u+v−1,0}andM(u, v) = min {u, v}are two copulas called Fr´ echet-Hoeffding
(FH) bounds (lower and upper, respectively). Another important copula that we will encounter is
the independence copula, usually denoted as Π( u, v) =uv, for all u, v∈[0,1]. Moreover, we will also
consider the Farlie-Gumbel-Morgenstern (FGM) family of copulas, defined for all u, v∈[0,1] as
Cθ(u, v) =uv(1 +θ(1−u)(1−v)), θ ∈[−1,1]. (2.5)
Clearly, from Eq. (2.5) one has C0(u, v) = Π( u, v). A family of copulas that includes M,Π and Wis
called comprehensive . An example is the Clayton family of copulas, defined for all u, v∈[0,1] as
Cθ(u, v) =
max( u−θ+v−θ−1,0)−1
θ, θ ∈[−1,0)∪(0,+∞), (2.6)
where C−1=W,C0= Π and C+∞=M. Another example of comprehensive family of copulas is
represented by the Frank family of copulas, defined for all u, v∈[0,1] as
Cθ(u, v) =−1
θln
1−(1−e−θu)(1−e−θv)
e−θ−1
, θ ∈(−∞,0)∪(0,+∞), (2.7)
where C−∞=W,C0= Π and C+∞=M.
For our aims, we recall that a copula C1is smaller than another copula C2under the concordance
order , and write C1≺C2, ifC1(u, v)≤C2(u, v) for all u, v∈[0,1]. Moreover, a random vector ( X, Y)
with a copula Cis said to be positively quadrant dependent (PQD), if and only if Π ≺C. Conversely it
has the negatively quadrant dependent (NQD) property if and only if C≺Π. Throughout the paper,
we will denote with ∂1C(respectively ∂2C) the partial derivative with respect to the first (second)
argument of C. For these and other basic properties of copulas we refer the reader to Durante and
Sempi [12] and Nelsen [23].
In addition, we now recall some properties for bivariate c.d.f.’s or s.f.’s, that clearly are related
with copulas and survival copulas from Eqs. (2.1) and (2.2). Let ( X, Y) be a random vector with joint
s.f.¯H. We say that ¯His Schur-concave (Schur-convex) if
¯H(x, y)≤¯H(˜x,˜y) (≥), (2.8)
for all x+y= ˜x+˜yand min( x, y)≤min(˜x,˜y), see Durante and Sempi [12], p. 278. We remark that the
Fr´ echet-Hoeffding lower bound in Eq. (2.4) represents the only Schur-convex copula (see, e.g., Nelsen
[23], p. 104). This properties can be relaxed as follows: ¯His weakly Schur-concave (Schur-convex) if
¯H(x, y)≤¯H(z, z) (≥), (2.9)
for all x, yandz= (x+y)/2, see Durante and Papini [11]. Moreover, ¯His Schur-constant if
¯H(x, y) =¯G(x+y), (2.10)
for all x, y≥0, where ¯Gis a univariate s.f., see Caramellino and Spizzichino [6]. We remark that in
the case of Schur-constant XandYare exc. See also Pellerey and Navarro [26] for Schur-constant
joint s.f. related to monotonicity properties of dependent variables given their sum.
We conclude this section by recalling well known probabilistic inequalities, useful for our aims.
Given a random variable Zand a convex function ψ, the Jensen’s inequality states that
ψ(E(Z))≤E(ψ(Z)). (2.11)
Moreover, if Zis a random lifetime and a >0, the Markov’s inequality states that
Pr(Z≥a)≤E(Z)
a. (2.12)
43 Bivariate Gini’s indices
We now define a bivariate version of the Gini’s mean difference introduced in Eq. (1.1), and we study
its basic properties. The formal definition can be stated as follows.
Definition 3.1 Let(X, Y)be a random vector. The bivariate Gini’s mean difference of (X, Y)is
defined as
GMD (X, Y) =E|X−Y|. (3.1)
For our aims, we denote U= max {X, Y}andL= min {X, Y}. By using |x−y|= max {x, y} −
min{x, y}in Eq. (3.1), it follows
GMD (X, Y) =E(U)−E(L). (3.2)
Therefore, Eq. (3.2) can be seen as the expected distance between the lifetimes of a parallel system
and a series system, both having two components distributed as XandY, respectively. Note that X
andYcan be dependent. Moreover, since |x−y|=x+y−2 min{x, y}, from Eq. (3.1) it also follows
GMD (X, Y) =E(X) +E(Y)−2E(L), (3.3)
and, by using Eqs. (3.2) and (3.3), one has the well known property E(X) +E(Y) =E(U) +E(L).
Suppose now that ( X, Y) has survival copula bC, with marginal s.f.’s ¯FX,¯FY, respectively. Then,
for all t≥0 one has
¯FL(t) =Pr(X > t, Y > t ) =bC(¯FX(t),¯FY(t))
and therefore Eq. (3.3) can be rewritten as
GMD (X, Y) =Z+∞
0n
¯FX(t) +¯FY(t)−2bC(¯FX(t),¯FY(t))o
dt. (3.4)
Clearly, if XandYare i.i.d. Eq. (3.4) is the univariate Gini’s mean difference in Eq. (1.1). If
¯FY(t) =hα(¯FX(t)) for all t, where hαis a distortion function in a parametric family with real value
αand, by considering also a parametric family of survival copulas for ( X, Y), Eq. (3.4) reduces to
the copula-distorted Gini’s mean difference defined in Capaldo et al. [5]. One can use the c.d.f.’s
configuration instead of the s.f.’s one, since from Eqs. (2.3) and (3.4) it follows
GMD (X, Y) =Z+∞
0{FX(t) +FY(t)−2C(FX(t), FY(t))}dt, (3.5)
where Cdenotes the copula function of ( X, Y), and FX,FYthe marginal c.d.f.’s.
As for the univariate case in Eq. (1.2), we now define the bivariate Gini’s index as a ratio related
to Eq. (3.1).
Definition 3.2 Let(X, Y)be a random vector. The bivariate Gini’s index of (X, Y)is defined as
G(X, Y) =GMD (X, Y)
E(X) +E(Y). (3.6)
Clearly, if XandYare i.i.d., Eq. (3.6) is the univariate Gini’s index in Eq. (1.2). We remark that
Eq. (3.6) extends the copula-distorted Gini’s index defined in [5]. Recalling Eq. (3.2), one has
G(X, Y) =E(U)−E(L)
E(U) +E(L). (3.7)
Hence G(X, Y)∈[0,1], with G(X, Y) = 0 if and only if E(U) =E(L). In addition, G(X, Y) = 1 when
E(L) = 0.
As for the univariate case (see Eq. (1.3)), we now explore the possibility to express the bivariate
Gini’s indices in terms of a covariance between the random lifetimes involved and their suitable
5transformations based on distributions. For this reason, given a random vector ( X, Y) we define the
following functions
γ1(t):=Pr(Y > t|X=t), γ 2(t):=Pr(X > t|Y=t), t≥0. (3.8)
IfbCdenotes the survival copula of ( X, Y), then it is easy to get (see, for instance, Navarro and Sordo
[20] or Nelsen [23], p. 217)
γ1(t) =∂1bC ¯FX(t),¯FY(t)
, γ 2(t) =∂2bC ¯FX(t),¯FY(t)
, t≥0. (3.9)
In the next result we first obtain a covariance representation of E(L).
Proposition 3.1 Let(X, Y)be a random vector with absolutely continuous distribution. Then
E(L) =Cov(X, γ 1(X)) + Cov(Y, γ2(Y)) + E(X)E(γ1(X)) + E(Y)E(γ2(Y)). (3.10)
Proof. Since ¯FL(t) =bC ¯FX(t),¯FY(t)
, the p.d.f. of Lis
fL(t) =fX(t)∂1bC ¯FX(t),¯FY(t)
+fY(t)∂2bC ¯FX(t),¯FY(t)
, t ≥0,
where fXandfYare the marginal p.d.f.’s. Therefore, recalling Eq. (3.9), one has
E(L) =E(Xγ1(X)) + E(Y γ2(Y)).
Finally, Eq. (3.10) follows from the definition of covariance. □
IfXandYare absolutely continuous, then from Eq. (3.8) one has E(γ1(X)) = Pr(Y > X ) and
E(γ2(Y)) = 1 −E(γ1(X)). Therefore Eq. (3.10) can also be written as
E(L) =Cov(X, γ 1(X)) + Cov(Y, γ2(Y)) + ( E(X)−E(Y))Pr(Y > X ) +E(Y). (3.11)
Along the same line, in the next result we obtain a covariance representation for the bivariate Gini’s
mean difference. Recalling Eq. (3.6), dividing by E(X) +E(Y) one can obtain a covariance represen-
tation for the bivariate Gini’s index.
Theorem 3.1 Let(X, Y)be a random vector with absolutely continuous distribution. Then
GMD (X, Y) = 2 [ E(X)−E(Y)]1
2−Pr(Y > X )
−2Cov(X, γ 1(X))−2Cov(Y, γ2(Y)).
Proof. The thesis immediately follows from Eqs. (3.3) and (3.11). □
Corollary 3.1 Under the same assumptions of Theorem 3.1:
(i)ifE(X) =E(Y)or if Pr(Y > X ) = 1 /2, then
GMD (X, Y) =−2Cov(X, γ 1(X))−2Cov(Y, γ2(Y));
(ii)ifXandYare exc., then
GMD (X, Y) =−4Cov(X, γ 1(X)). (3.12)
From the left-hand-side of Eq. (3.8), we denote with F2|1(t|t) =Pr(Y≤t|X=t) = 1−γ1(t), for
allt≥0, the probability that the second lifetime is less or equal to tconditioned by the fact that the
first lifetime is equal to t. IfXandYare exc., from Eqs. (3.6) and (3.12), it follows
G(X, Y) =2
E(X)Cov(X, F 2|1(X|X)) =−2
E(X)Cov(X, γ 1(X)), (3.13)
that extends the covariance representation of the univariate Gini’s index in Eq. (1.3). Under the
exchangeability assumption, Eq. (3.13) represents another way to compute the bivariate Gini’s index
making use of conditional distributions instead of copula representations.
6As an example, given a random vector ( X, Y), let us consider the exponential conditional distri-
butions with joint p.d.f. given by
f(x, y) = exp {−(c11+c12x+c21y+c22xy)}, x > 0, y > 0,
where c12>0,c21>0,c22≥0 (the case c22= 0 corresponds to the independent case) and
c11= ln"
1
c12c21Z+∞
0e−u
1 +c22u
c12c21−1
du#
. (3.14)
Therefore one has
Pr(Y > y |X=x) = exp {−(c21+c22x)y}, x > 0, y > 0,
(see Arnold et al. [1] and [2]). For instance, if c12=c21=c22= 1, from Eq. (3.14) one has
c11=−0.516932. Hence, recalling Eq. (3.8), it follows
γ1(t) = exp {−(t+t2)}, t > 0.
Under these assumptions, XandYare exc. and one numerically has E(X) = 0 .676875, E(γ1(X)) = 0 .5
andE(Xγ1(X)) = 0 .135428. Therefore, from the definition of covariance and Eq. (3.13), one obtains
G(X, Y) = 0 .599843.
In the next result we show some basic properties satisfied by the bivariate Gini’s mean difference.
Theorem 3.2 The bivariate Gini’s mean difference of (X, Y)satisfies the following properties:
(i) (law invariance) if (X, Y) =st(˜X,˜Y), then GMD (X, Y) =GMD (˜X,˜Y);
(ii) (translation invariance) GMD (X+λ, Y+λ) =GMD (X, Y), for all λ∈R;
(iii) (homogeneity) GMD (λX, λY ) =|λ|GMD (X, Y), for all λ∈R;
(iv) (non-negativity) GMD (X, Y)≥0for all XandY. In addition if XandYare i.d. and C(u, v) =
min{u, v}, for all u, v∈[0,1], then GMD (X, Y) = 0 . Conversely, if GMD (X, Y) = 0 , then
FX=FYand if FXis continuous then C(u, v) = min {u, v}, for all u, v∈[0,1].
Proof. The property (i) is trivial. The properties (ii) and (iii) immediately follows from Eq. (3.1).
GMD (X, Y)≥0 also holds trivially from Eq. (3.1). If XandYare i.d. and C(u, v) = min {u, v}, for
allu, v∈[0,1], then GMD (X, Y) = 0 from Eq. (3.5). Conversely, if GMD (X, Y) = 0, from Eq. (3.2)
one has E(U) =E(L) and since L≤stU, from Theorem 1.A.8. in [31], it follows ¯FL=¯FU. Therefore,
since L≤stX≤stUandL≤stY≤stUthen FX=FY. Under these assumptions and if FXis
continuous, from Eq. (3.5) one has δ(u) =ufor all u∈[0,1]. Therefore for all u, v∈[0,1]
C(u, v)≥C(min{u, v},min{u, v}) =δ(min{u, v}) = min {u, v},
and, by recalling Min Eq. (2.4), finally the thesis holds. □
Similarly, from Eq. (3.6), it is easy to see that the bivariate Gini’s index satisfies the following prop-
erties:
(i) (law invariance) if ( X, Y) =st(˜X,˜Y), then G(X, Y) =G(˜X,˜Y);
(ii) (translation) G(X+λ, Y+λ) =E(X)+E(Y)
E(X)+E(Y)+2λG(X, Y), for all λ∈R;
(iii) (homogeneity invariance) G(λX, λY ) =G(X, Y), for all λ∈R+;
(iv) (non-negativity) G(X, Y)≥0 for all XandY. In addition G(X, Y) = 0 under the same
conditions given in (iv) of Theorem 3.2 for GMD (X, Y).
Aiming to compare different bivariate Gini’s mean difference, below we provide some results under
suitable assumptions on the marginal distributions and copula functions involved in Eqs. (3.4) and
(3.5). First we show a dependence ordering for the bivariate Gini’s mean difference and bivariate Gini’s
index through concordance order between two copula functions when we maintain the marginals.
7Proposition 3.2 Let(X, Y)and(˜X,˜Y)be two random vectors with copulas C1andC2, respectively.
Suppose that X=st˜XandY=st˜Y, with c.d.f.’s FXandFY, respectively. If C1≺C2, then
GMD (X, Y)≥GMD (˜X,˜Y), G(X, Y)≥G(˜X,˜Y).
Proof. Under these assumptions, recalling Eq. (3.5), one has
GMD (X, Y)−GMD (˜X,˜Y) = 2Z+∞
0{C2(FX(t), FY(t))−C1(FX(t), FY(t))}dt. (3.15)
Hence, recalling also Eq. (3.6), the thesis follows from C1≺C2. □
Therefore if we fix the marginals and the dependence increases in the concordance order, then
GMD (X, Y) and G(X, Y) decrease, that is, these dispersions indices decrease when the positive de-
pendence increases. When all the marginals are i.d. we have the following result.
Proposition 3.3 Let(X, Y)and(˜X,˜Y)be two random vectors with diagonal sections δ1andδ2,
respectively. Suppose that all the marginals are i.d. with common c.d.f. F. Ifδ1≤δ2, then
GMD (X, Y)≥GMD (˜X,˜Y), G(X, Y)≥G(˜X,˜Y),
with equality reached when δ1=δ2.
Proof. The thesis immediately follows from Eq. (3.15) when X=stY. □
Making use of Corollary 8 in Ortega-Jim´ enez et al. [25] and Proposition 3.3, we can go further and
get the following result.
Theorem 3.3 Let(X,˜X)and(Y,˜Y)be two random vectors with diagonal sections δ1andδ2, respec-
tively. Suppose that X=st˜XandY=st˜Y. IfY≤dXandδ1≤δ2, then
GMD (X,˜X)≥GMD (Y,˜Y).
Proof. Consider ( Y1,˜Y1) with diagonal section δ1and suppose that Y=stY1=st˜Y1. The thesis
follows since from Corollary 8 in Ortega-Jim´ enez et al. [25] and Proposition 3.3 one has, respectively,
GMD (X,˜X)≥GMD (Y1,˜Y1)≥GMD (Y,˜Y).
□
In the following result we compare the bivariate Gini’s mean difference in the cases of homogeneous
and heterogeneous marginal distributions.
Proposition 3.4 Let(X, Y)be a random vector with survival copula bCand strictly increasing di-
agonal section bδ. Let (Z,˜Z)be another random vector with the same survival copula and common
marginal s.f. ¯G. If
bδ−1(bC(¯FX(t),¯FY(t)))≤¯G(t)≤¯FX(t) +¯FY(t)
2(3.16)
holds for all t, then GMD (X, Y)≥GMD (Z,˜Z).
Proof. From Eqs. (3.4) and (3.16), as bC(¯FX(t),¯FY(t)))≤bδ(¯G(t)), one has
GMD (X, Y) = 2Z∞
0¯FX(t) +¯FY(t)
2−bC(¯FX(t),¯FY(t))
dt
≥2Z∞
0
¯G(t)−bδ(¯G(t))
dt
=GMD (Z,˜Z)
and the proof is complete. □
8Note that both ¯G1(t) :=bδ−1(bC(¯FX(t),¯FX(t))) and ¯G2(t) := ( ¯FX(t) +¯FY(t))/2 are s.f.’s obtained
as distortions of ¯FXand¯FY. We also remark that the bivariate Gini’s mean difference decreases when
the dependence does not change and the marginal distributions are homogeneous, that is Zand˜Zare
more similar than the original XandY. For example, when bC= Π, then we can apply Proposition 3.4
if¯Gis between the geometric and the arithmetic means of ¯FXand ¯FY, since the condition expressed
by Eq. (3.16) becomes
¯G1(t) =q
¯FX(t)¯FY(t)≤¯G(t)≤¯G2(t) =¯FX(t) +¯FY(t)
2.
Note that we can also apply Proposition 3.4 to both ¯G1and ¯G2. A similar result can be stated for
the c.d.f. configuration from Eq. (3.5), as follows.
Proposition 3.5 Let(X, Y)be a random vector with copula Cand strictly increasing diagonal section
δ. Let (Z,˜Z)be another random vector with the same copula and common marginal c.d.f. G. If
δ−1(C(FX(t), FY(t)))≤G(t)≤FX(t) +FY(t)
2
holds for all t, then GMD (X, Y)≥GMD (Z,˜Z).
We can also obtain a result in order to compare heterogeneous cases. It can be stated as follows.
A similar result holds for the c.d.f. configuration given in Eq. (3.5).
Proposition 3.6 Let(X, Y)be a random vector with survival copula bC. Let (˜X,˜Y)be another
random vector with the same survival copula. If E(X) +E(Y)≥E(˜X) +E(˜Y)and
bC(¯FX(t),¯FY(t))≤bC(¯F˜X(t),¯F˜Y(t))
holds for all t, then GMD (X, Y)≥GMD (˜X,˜Y).
We remark that, under the assumptions of Proposition 3.6, when E(X) +E(Y) =E(˜X) +E(˜Y) one
also has G(X, Y)≥G(˜X,˜Y). Moreover, recalling Eq. (2.8), if the survival copula is Schur-concave we
can go further and get the following result.
Proposition 3.7 Let(X, Y)be a random vector with survival copula bC. Let (˜X,˜Y)be another
random vector with the same survival copula. If bCis Schur-concave, ¯FX+¯FY=¯F˜X+¯F˜Yand
min( ¯FX,¯FY)≤min( ¯F˜X,¯F˜Y), then
GMD (X, Y)≥GMD (˜X,˜Y), G(X, Y)≥G(˜X,˜Y).
Proof. Under the stated assumptions, if bCis Schur-concave, then
bC(¯FX(t),¯FY(t))≤bC(¯F˜X(t),¯F˜Y(t)).
Therefore
GMD (X, Y) = 2Z∞
0¯FX(t) +¯FY(t)
2−bC(¯FX(t),¯FY(t))
dt
≥2Z∞
0¯F˜X(t) +¯F˜Y(t)
2−bC(¯F˜X(t),¯F˜Y(t))
dt
=GMD (˜X,˜Y)
and the proof is complete. The result for the bivariate Gini’s index holds since E(X) +E(Y) =
E(˜X) +E(˜Y). □
We remark that, for bCSchur-concave, if ¯FX+¯FY=¯F˜X+¯F˜YandX≤st˜X≤st˜Y≤stYhold,
then
GMD (X, Y)≥GMD (˜X,˜Y), G(X, Y)≥G(˜X,˜Y).
9Thus, for a fixed copula, if one increases the dispersions between the random lifetimes involved in the
vector, then the bivariate Gini’s indices increase.
The result included in Proposition 3.7 is related with Parrondo’s paradox in reliability stated in
Di Crescenzo [8] and in Navarro and Spizzichino [21]. In this sense we know that many copulas are
Schur-concave (for example all the Archimedean copulas). Hence the preceding result holds for many
dependence models. If we just compare with the homogeneous case, we can relax the condition of the
copula to the weakly Schur-concavity defined in Eq. (2.9).
Proposition 3.8 Let(X, Y)be a random vector with survival copula bC. Let (Z,˜Z)be another random
vector with the same survival copula and common marginal s.f. ¯G. IfbCis weakly Schur-concave
(respectively Schur-convex) and ¯G= (¯FX+¯FY)/2, then
GMD (X, Y)≥GMD (Z,˜Z) (≤), G(X, Y)≥G(Z,˜Z) (≤).
Proof. Under the stated assumptions, if bCis weakly Schur-concave, then
bC(¯FX(t),¯FY(t))≤bC(¯G(t),¯G(t)).
Therefore
GMD (X, Y) = 2Z∞
0¯FX(t) +¯FY(t)
2−bC(¯FX(t),¯FY(t))
dt
≥2Z∞
0
¯G(t)−bC(¯G(t),¯G(t))
dt
=GMD (Z,˜Z)
and the thesis follows. The proof for weakly Schur-convex copulas is similar. The result for the
bivariate Gini’s index holds since E(X) +E(Y) = 2 E(Z). □
Recalling Eq. (2.10), in the following we consider the Schur-constant assumption for bC.
Proposition 3.9 LetbCbe the survival copula of (X, Y). IfbCis Schur-constant, then
GMD (X, Y) =E(X), G(X, Y) =1
2. (3.17)
Proof. IfbCis Schur-constant, then |X−Y|=stX, as shown in Theorem 4 of Nelsen [22]. Therefore,
the thesis follows recalling Eqs. (3.1) and (3.6). □
Clearly, from the left-hand-side of Eq. (3.17), for ( X, Y)∼bC1and ( ˜X,˜Y)∼bC2with Schur-
constant copulas, if X≤st˜X, then GMD (X, Y)≤GMD (˜X,˜Y).
We conclude this section by extending the bivariate Gini’s indices to the multivariate case. Given
ann-dimensional random vector ( X1, . . . , X n), we use the following notation
X1:n= min {X1, . . . , X n} and Xn:n= max {X1, . . . , X n}.
Definition 3.3 LetX= (X1, . . . , X n)be an n-dimensional random vector. The multivariate Gini’s
mean difference of Xis defined as
GMD (X) =GMD (X1:n, Xn:n). (3.18)
Moreover, by denoting with R:=Xn:n−X1:n≥0, the range of the random vector X, one has
GMD (X) =E(Xn:n)−E(X1:n) =E(R). Clearly GMD (X) = 0 if and only if X1≡ ··· ≡ Xn. In this
sense the multivariate Gini’s mean difference can be used to measure the dispersion of the random
vector X.
According to Eq. (3.6), one can define a multivariate ratio based on Eq. (3.18), namely multivariate
Gini’s index ofX, as follows
G(X):=GMD (X)
E(X1:n) +E(Xn:n)∈[0,1].
10IfX1, . . . , X nare i.d., with common s.f. ¯F, having copula Cand survival copula bC, Eq. (3.18)
becomes
GMD (X) =Z+∞
0n
1−δ(F(t))−ˆδ ¯F(t)o
dt, (3.19)
where δandˆδare the diagonal sections of CandbC, respectively. Moreover, if X1, . . . , X nare i.i.d.,
having common s.f. ¯F, then Eq. (3.19) reduces to
GMD (X) =Z+∞
0
1−(F(t))n− ¯F(t)n	
dt. (3.20)
Remark 3.1 IfXi∼ U(0,1), for i∈ {1, . . . , n }, and they are independent, then Eq. (3.20) becomes
GMD (X) =Z1
0{1−tn−(1−t)n}dt=n−1
n+ 1.
Hence, GMD (X)→1asn→+∞. For instance, when X1, . . . , X ndescribe the lifetimes of n
components in a system, then GMD (X)goes to 1when the number of the components increases sig-
nificantly. In addition, since in this case E(Xn:n) =n/(n+ 1) andE(X1:n) = 1 /(n+ 1), it follows
that GMD (X) =G(X). As examples, for n= 2,3,4,5one has G(X) =1
3,1
2,3
5,2
3, respectively.
Remark 3.2 IfXi∼Exp(1) , for i∈ {1, . . . , n }, and they are independent, by setting t=e−x, Eq.
(3.20) becomes
GMD (X) =Z1
0{1−(1−t)n−tn}
tdt=H(n)−1
n,
where H(n) =Pn
k=11
kis the truncation of the harmonic series. One has GMD (X)→+∞as
n→+∞. In addition, it follows
G(X) =nH(n)−1
nH(n) + 1,
that goes to 1asnincreases. As examples, for n= 2,3,4,5one has G(X) =1
2,9
13,11
14,125
149, respectively.
4 Bounds and inequalities
This section is devoted to show bounds and inequalities for the bivariate Gini’s indices defined in
Section 3. Bounds for the univariate Gini’s indices given in Eqs. (1.1) and (1.2) can be found, for
instance, in Yin et al. [34]. In the next result we use the Jensen’s inequality in Eq. (2.11) aiming to
provide lower bounds for Eqs. (3.1) and (3.6).
Proposition 4.1 Let(X, Y)be a random vector. Then
GMD (X, Y)≥ |E(X)−E(Y)|, G(X, Y)≥|E(X)−E(Y)|
E(X) +E(Y). (4.1)
Proof. The thesis immediately follows by recalling Eqs. (3.1) and (3.6) and making use of Eq. (2.11)
with ψequal to the absolute value function and Z=X−Y. □
Clearly, if E(X)≤E(Y), then from Eq. (4.1)
GMD (X, Y)≥E(Y)−E(X), G(X, Y)≥E(Y)−E(X)
E(Y) +E(X). (4.2)
Remark 4.1 LetXandYbe exponentially distributed having s.f.’s ¯FX(t) =e−λ1tand¯FY(t) =e−λ2t,
fort≥0, respectively. If λ1> λ 2>0, from Eq. (4.2) one has
GMD (X, Y)≥λ1−λ2
λ1λ2, G(X, Y)≥λ1−λ2
λ1+λ2.
Making use of the Markov’s inequality in Eq. (2.12), below we interpret GMD (X, Y) as a bound
for the probability that Yassumes values in a neighborhood of X.
11Proposition 4.2 Let(X, Y)be a random vector. For a >0, one has
Pr(X−a < Y < X +a)≥1−GMD (X, Y)
a. (4.3)
Proof. The thesis follows making use of Eq. (2.12) with Z=|X−Y|, and by recalling Eq. (3.1). □
In addition, one can use the Markov’s inequality in Eq. (2.12) in order to compare the bivariate
Gini’s mean difference, for i.d. marginals, with the univariate one, as shown in the next result.
Theorem 4.1 Let(X, Y)be a random vector having i.d. marginals with mean µ. One has
PrX
µ−2G(X)<Y
µ<X
µ+ 2G(X)
≥1−G(X, Y)
G(X).
Proof. Since X=stY, recalling Eqs. (1.1), (1.2), (3.1) and (3.6), one has
GMD (X, Y)
GMD (X)=G(X, Y)
G(X).
Therefore, the thesis follows from Eq. (4.3) for a=GMD (X) and dividing by µin the inequalities
stated in the left-hand-side. □
In the next result, making use of the FH bounds in Eq. (2.4) and Proposition 3.2, we obtain
attainable bounds for the bivariate Gini’s mean difference. The bounds for the bivariate Gini’s index
can be obtained dividing by E(X) +E(Y).
Proposition 4.3 Let(X, Y)be a random vector with marginal s.f.’s ¯FXand ¯FY. Then
Z+∞
0|¯FX(t)−¯FY(t)|dt≤GMD (X, Y)≤Z+∞
0
1− |¯FX(t) +¯FY(t)−1|	
dt, (4.4)
with bounds reached when the copula of (X, Y)coincides with the FH bounds in Eq. (2.4).
Proof. By recalling Eq. (3.4), from the FH upper bound one has
GMD (X, Y)≥Z+∞
0¯FX(t) +¯FY(t)−2 min{¯FX(t),¯FY(t)}	
dt
=Z+∞
0|¯FX(t)−¯FY(t)|dt,
where the last equality follows from |x−y|=x+y−2 min{x, y}. Similarly, from the FH lower bound
we set
GMD (X, Y)≤Z+∞
0¯FX(t) +¯FY(t)−2 max{¯FX(t) +¯FY(t)−1,0}	
dt
=Z+∞
0
1− |¯FX(t) +¯FY(t)−1|	
dt,
where the last equality follows from 2 max {x, y}=|x−y|+x+y. This conclude the proof. □
We remark that, if X≤stY, then the lower bound in Eq. (4.4) coincides with the one given in
Eq. (4.2), attained when C=M.
Remark 4.2 Suppose that X∼ U(0, a)andY∼ U(0, b), with 0< a < b . For any copula C, from
Eq. (4.4) with some calculations it follows
b2−a2
2(a+b)≤GMD (X, Y)≤a2+b2
2(a+b),b2−a2
(a+b)2≤G(X, Y)≤a2+b2
(a+b)2.
Suppose now that X∼Exp(1) andY∼ U(0,1). Then from Eq. (4.4) with some calculations one
obtains
0.5≤GMD (X, Y)≤0.955937 , 0.333333 ≤G(X, Y)≤0.637291 .
12In the next result we provide an upper bound for the bivariate Gini’s mean difference when
X=stY, with particular attention to the cases in which the median of Xcoincides with its mean
value.
Proposition 4.4 Let(X, Y)be a random vector with i.d. marginals having c.d.f. F, mean µand
median m. Then
0≤GMD (X, Y)≤2 (µ−m) + 4Zm
0F(t)dt. (4.5)
In particular, if µ=m, then GMD (X, Y)≤4Rm
0F(t)dt.
Proof. Under these assumptions, from Eq. (4.4), with few calculations one has
GMD (X, Y)≤Z+∞
01− |2¯F(t)−1|dt
= 2Zm
0F(t)dt+ 2Z+∞
m¯F(t)dt
= 2 (µ−m) + 4Zm
0F(t)dt,
where the last equality is obtained by adding and subtracting 2Rm
0¯F(t)dt. □
Remark 4.3 IfXandYare uniformly distributed over [0, b], with b >0, from Eq. (4.5), recalling
thatµ=m= 1/2, one has
GMD (X, Y)≤b
2, G(X, Y)≤1
2.
IfXandYare exponentially distributed with mean µ >0, from Eq. (4.5) it follows
GMD (X, Y)≤2µln(2), G(X, Y)≤ln(2).
Remark 4.4 Making use of the Jensen’s inequality in Eq. (2.11), it is easy to see that the median
and the mean satisfy the following well known relation
|µ−m| ≤σ,
where σis the standard deviation. Analogously, for an absolutely continuous random lifetime X, from
Markov’s inequality in Eq. (2.12) we get
Pr(X≥m) =1
2≤µ
m
and therefore µ≥m/2. Moreover, from Eq. (4.5) one can obtain another lower bound for the mean
as
µ≥m−2Zm
0F(t)dt.
In the next result we give bounds for the bivariate Gini’s mean difference when XandYare
stochastically ordered, by using the bivariate Gini’s mean differences of their respectively homogeneous
cases. The bounds for the bivariate Gini’s index follow dividing by E(X) +E(Y).
Proposition 4.5 Let(X, Y),(X,˜X)and(Y,˜Y)be three random vectors with the same survival copula
bC, where X=st˜XandY=st˜Y, respectively. If X≤stY, then
GMD (Y,˜Y)−2 (E(Y)−E(X))≤GMD (X, Y)≤GMD (X,˜X) + 2 ( E(Y)−E(X)). (4.6)
Proof. We only show how to obtain the right-hand-side of Eq. (4.6), since the left-hand-side similarly
follows. Under the stated assumptions, since ¯FX≤st¯FYone has E(X)≤E(Y) and E(min{X,˜X})≤
E(min{X, Y}), and therefore
GMD (X, Y)≤2E(Y)−2E(min{X,˜X}).
The proof follows making use of Eq. (3.3) in E(min{X,˜X}). □
In the independent case, the Gini’s indices in the bounds given in Eq. (4.6) can be replaced with
the respectively univariate indices defined in Eq. (1.1).
135 Efficiency Gini’s indices in systems
In this section we define particular versions of the bivariate Gini’s indices, for the cases in which the
random lifetimes XandYin Eqs. (3.1) and (3.6) are suitable systems. First, we recall that a (binary)
system with (binary) components of order nis a Boolean structure function (map) Φ : {0,1}n→ {0,1},
where Φ ( x1, . . . , x n)∈ {0,1}represents the system’s state that is completely determined by the
components’ states represented by x1, . . . , x n∈ {0,1}. Asemi-coherent system of order nis a system
Φ that is increasing and such that Φ (0 , . . . , 0) = 0 and Φ (1 , . . . , 1) = 1. In addition, we say that Φ
is acoherent system of order nif it is increasing and strictly increasing in each variable in at least a
point. We remark that this function Φ can be extended to real numbers and then the random lifetime
Tof the system can be written as T= Φ(X1, . . . , X n), where X1, . . . , X nare the random lifetimes of
the components. For more details about systems see Navarro [17].
We now are ready to define efficiency versions of the new Gini’s indices. The purpose is to measure
how good is a given system T.
Definition 5.1 Let(X1, . . . , X n)be a random vector and let T= Φ( X1, . . . , X k)be the lifetime of
any semi-coherent system, for k≤n. Consider X1:n= min {X1, . . . , X n}. The efficiency Gini’s mean
difference of order nofTis defined as
GMD n(T) =GMD (X1:n, T). (5.1)
Clearly, from Eq. (3.1), one has GMD n(T) =E(T)−E(X1:n). In addition, GMD n(X1:n) = 0 and,
recalling Eq. (3.18), GMD n(Xn:n) =GMD (X).
From Eq. (3.6), one can obtain a ratio based on Eq. (5.1) as GMD n(T)/(E(T)+E(X1:n)). However,
in order to give more information about the efficiency of a system, we prefer to define the efficiency
Gini’s index as
Gn(T):=GMD n(T)
GMD n(Xn:n), (5.2)
where Gn(T) = 0 if and only if T≡X1:n, and Gn(T) = 1 if and only if T≡Xn:n. Therefore, since Eq.
(5.1) represents the distance between the expected first component failure and the expected failure of
T, one can measure the efficiency of any semi-coherent system by using Eq. (5.2). Hence, the efficiency
of a system in terms of its duration increases as Gn(T) increases. For predictions of Tfrom X1:nsee
Navarro et al. [18].
Making use of Eq. (4.3) we provide below a useful interpretation of the efficiency Gini’s index.
Proposition 5.1 Under the assumptions of Definition 5.1, for c >0one has
Pr(T < X 1:n+cGMD n(Xn:n))≥1−Gn(T)
c. (5.3)
Proof. The thesis immediately follows from Eq. (4.3) when a=cGMD n(Xn:n). □
Hence, Eq. (5.3) leads to consider the efficiency Gini’s index as a “control index” for the probability
that the system fails after the first failure in an instant given by the difference between the expectations
of the first and the last component failures.
One could define a dual version of Eq. (5.1) for any semi-coherent system T, referring to Xn:n
instead of X1:n, asGMD (T, X n:n). In this case the corresponding Gini’s index does not describe the
same efficiency pointed out in Eq. (5.2). Indeed, GMD (T, X n:n) measures the distance between the
last component failure and the failure of the system, and therefore it represents a sort of “inefficiency”.
We now provide a signature representation of the efficiency Gini’s mean difference defined in Eq.
(5.1). Samaniego [29] (see also Samaniego [30]) introduced the first signature representation for the
reliability of a coherent system. The interpretation is intuitively justified by the fact that a coherent
system Tis going to fail with a component failure, and therefore the reliability of Tcan viewed as a
weighted sum of the ordered statistics (ordered component failures) Xi:nfori∈ {1, . . . , n }. Hence, if
Thas i.i.d. component lifetimes X1, . . . , X nwith continuous s.f. ¯F, then
¯FT(t) =nX
i=1si¯Fi:n(t), (5.4)
14for all t, where s1, . . . , s nare nonnegative coefficients such thatPn
i=1si= 1 and that do not depend
on¯F, and where ¯Fi:nis the reliability function of Xi:n, fori∈ {1, . . . , n }. The vector s= (s1, . . . , s n)
is called the signature of the system, where si=Pr(T=Xi:n) for i∈ {1, . . . , n }. Recalling that, for
allt, the s.f. of Xi:nis
¯Fi:n(t) =i−1X
j=0n
j
[F(t)]j¯F(t)n−j,
(see, for instance, Proposition 2.2 in [17], p. 30), from Eq. (5.4), by interchanging the order of sum-
mations, it follows
¯FT(t) =nX
i=1Sn−i+1n
i
[F(t)]n−i¯F(t)i, (5.5)
where Sj=Pn
i=jsiforj∈ {1, . . . , n }, is the probability that the system works when exactly n−j+1
components work.
In Capaldo et al. [4] the authors defined the cumulative information generating function of Xas
GX(α, β):=Z+∞
0[F(t)]α¯F(t)βdt, (5.6)
for all ( α, β)∈R2such that the right-hand-side of Eq. (5.6) is finite. Recalling Eq. (1.1), it easy is
to see that GMD (X) = 2 GX(1,1). See [4] also for different generalizations of the univariate Gini’s
mean difference in Eq. (1.1). Eq. (5.6) provides a unifying mathematical tool suitable to deal with
cumulative entropies based on the s.f. and c.d.f., introduced and studied in Rao et al. [28] and Di
Crescenzo and Longobardi [10], respectively. In addition, it is able to recover both generalized and
fractional cumulative entropies, see Di Crescenzo et al. [9], Kayal [15], Psarrakos and Navarro [27],
Toomaj and Di Crescenzo [32] and Xiong et al. [33] as references. Marginal versions of Eq. (5.6) have
been also discussed in [4]. In particular, we recall the cumulative residual information generating
measure of X, defined as
KX(β):=GX(0, β), (5.7)
for all β∈Rsuch that KX(β) is finite.
In the next result we provide an alternative expression of Eq. (5.1) for k=nin terms of Eq. (5.6),
from the signature representation discussed above.
Theorem 5.1 LetT= Φ(X1, . . . , X n)be any coherent system having i.i.d. components. Then
GMD n(T) =n−1X
i=1Sn−i+1n
i
GX(n−i, i).
Proof. Making use of Eq. (5.5) for the calculation of E(T) in Eq. (5.1), the thesis immediately follows
from Eq. (5.6). □
The Samaniego’s representation does not necessarily hold in the general case. Another way to compute
the system reliability from a signature representation is given in Navarro et al. [19], namely minimal
signature representation, making use of the concept of minimal path set representation (for further
details see Section 2.3 in [17]). Therefore, if Tis the lifetime of a coherent (or semi-coherent) system
with exc. component lifetimes X1, . . . , X n, then
¯FT(t) =nX
i=1ai¯F1:i(t), (5.8)
for all t, where a1, . . . , a nare some integer coefficients such that a1+···+an= 1, and ¯F1:i(t) =
Pr(min{X1, . . . , X i}> t) for i∈ {1, . . . , n }. The vector a= (a1, . . . , a n) is called the minimal signa-
tureofT. Similarly to Theorem 5.1, in the next result we provide an alternative expression of Eq.
(5.1) in terms of Eqs. (5.7) and (5.8).
15Theorem 5.2 LetT= Φ(X1, . . . , X n)be any coherent system having i.i.d. components. Then
GMD n(T) =n−1X
i=1aiKX(i) + (an−1)KX(n).
Proof. By using Eq. (5.8) in the calculation of E(T) in Eq. (5.1), the thesis immediately follows from
Eq. (5.7). □
The Samaniego’s representation can also be extended to semi-coherent systems, making use of the
structural signature s(n)of order n. A result similar to Theorem 5.1 holds for s(n). Analogously,
a(n)= (a1, . . . , a k,0, . . . , 0) is called the minimal signature of order n (see [17], pp. 48-49). Along the
same line of Theorem 5.2, below we provide an alternative expression of Eq. (5.1) in terms of Eq.
(5.7), by using the minimal signature of order n.
Theorem 5.3 LetT= Φ( X1, . . . , X k)be any semi-coherent system having i.i.d. components, con-
tained in a random vector (X1, . . . , X n). Therefore
GMD n(T) =kX
i=1aiKX(i)−KX(n). (5.9)
Consider now an exc. random vector ( X1, . . . , X n) having marginals with common s.f. ¯F. Then for
any semi-coherent system T= Φ(X1, . . . , X k), for k≤n, one has
GMD n(T) =kX
i=1aiZ+∞
0bδi(¯F(t))dt−Z+∞
0bδn(¯F(t))dt, (5.10)
where bδi(u) =bC(u, . . . , u, 1, . . . , 1), for urepeated i-times, is the diagonal section of the copula of
(X1, . . . , X i), for i∈ {1, . . . , n }andu∈[0,1]. Clearly, in the i.i.d. case Eq. (5.10) reduces to Eq.
(5.9).
In Table 1, making use of Eq. (5.9), we compute the efficiency Gini’s index for all the coherent
systems with 1-4 i.i.d. components, uniformly or exponentially distributed. Table 2 shows the efficiency
Gini’s index for all coherent systems with 1-4 i.d. components, uniformly or exponentially distributed,
computed using Eq. (5.10) under the following 4-dimensional FGM copula
bC(u1, u2, u3, u4) =u1u2u3u4(1 +θ(1−u1)(1−u2)(1−u3)(1−u4)), u 1, u2, u3, u4∈[0,1],
(5.11)
with diagonal section
bδ4(u) =u4 
1 +θ(1−u)4
, u ∈[0,1],
forθ= 1,−1, where bδi(u) =ui, fori= 1,2,3.
Finally, we remark that Eq. (5.10) can also be extended to the not exc. case, making use of minimal
path set representation for semi-coherent systems (see [17], p. 37).
6 Examples and simulations
In this section we show examples of the bivariate Gini’s indices defined in Section 3. Interpretations
in terms of areas also arises for the bivariate Gini’s index in Eq. (3.6), as for the univariate one given
in Eq. (1.2). We also define empirical Gini’s indices to approximate them from data, by providing
simulations. We also compute the efficiency Gini’s index for some systems.
In the first example we consider bivariate Gini’s indices of ( X, Y) with i.d. marginals, according
to the uniform distribution, under different copulas. Similarly to the univariate case, the bivariate
Gini’s index turns out to be two times the area between the egalitarian line and the diagonal section
of (X, Y).
16Table 1: Efficiency Gini’s index for all coherent systems with 1-4 i.i.d. components, uniformly or
exponentially distributed, making use of the minimal signature representation of order 4. The values
have been calculated from Eq. (5.9), by rounding to the third decimal place.
i Ti a(4)Gn(T) Gn(T)
Xj∼ U(0,1) Xj∼Exp(1)
1 X1:1=X1 (1,0,0,0) 0 .500 0 .409
2 X1:2= min( X1, X2) (2-series) (0 ,1,0,0) 0 .222 0 .136
3 X2:2= max( X1, X2) (2-parallel) (2 ,−1,0,0) 0 .778 0 .682
4 X1:3= min( X1, X2, X3) (3-series) (0 ,0,1,0) 0 .083 0 .045
5 min( X1,max( X2, X3)) (0 ,2,−1,0) 0 .361 0 .227
6 X2:3(2-out-of-3) (0 ,3,−2,0) 0 .500 0 .318
7 max( X1,min(X2, X3)) (1 ,1,−1,0) 0 .639 0 .500
8 X3:3= max( X1, X2, X3) (3-parallel) (3 ,−3,1,0) 0 .917 0 .864
9 X1:4= min( X1, X2, X3, X4) (4-series) (0 ,0,0,1) 0 0
10 max(min( X1, X2, X3),min(X2, X3, X4)) (0 ,0,2,−1) 0 .167 0 .091
11 min( X2:3, X4) (0 ,0,3,−2) 0 .250 0 .136
12 min( X1,max( X2, X3),max( X3, X4)) (0 ,1,1,−1) 0 .306 0 .182
13 min( X1,max( X2, X3, X4)) (0 ,3,−3,1) 0 .417 0 .273
14 X2:4(3-out-of-4) (0 ,0,4,−3) 0 .333 0 .182
15 max(min( X1, X2),min(X1, X3, X4),min(X2, X3, X4)) (0 ,1,2,−2) 0 .389 0 .227
16 max(min( X1, X2),min(X3, X4)) (0 ,2,0,−1) 0 .444 0 .273
17 max(min( X1, X2),min(X1, X3),min(X2, X3, X4)) (0 ,2,0,−1) 0 .444 0 .273
18 max(min( X1, X2),min(X2, X3),min(X3, X4)) (0 ,3,−2,0) 0 .500 0 .318
19 max(min( X1,max( X2, X3, X4)),min(X2, X3, X4)) (0 ,3,−2,0) 0 .500 0 .318
20 min(max( X1, X2),max( X1, X3),max( X2, X3, X4)) (0 ,4,−4,1) 0 .556 0 .364
21 min(max( X1, X2),max( X3, X4)) (0 ,4,−4,1) 0 .556 0 .364
22 min(max( X1, X2),max( X1, X3, X4),max( X2, X3, X4)) (0 ,5,−6,2) 0 .611 0 .409
23 X3:4(2-out-of-4) (0 ,6,−8,3) 0 .667 0 .455
24 max( X1,min(X2, X3, X4)) (1 ,0,1,−1) 0 .583 0 .455
25 max( X1,min(X2, X3),min(X3, X4)) (1 ,2,−3,1) 0 .694 0 .545
26 max( X2:3, X4) (1 ,3,−5,2) 0 .750 0 .591
27 min(max( X1, X2, X3),max( X2, X3, X4)) (2 ,0,−2,1) 0 .833 0 .727
28 X4:4= max( X1, X2, X3, X4) (4-parallel) (4 ,−6,4,−1) 1 1
17Table 2: Efficiency Gini’s index for all coherent systems considered in Table 1, in this case with 1-4 i.d.
components, uniformly or exponentially distributed, having FGM copula in Eq. (5.11), for θ= 1,−1
and making use of the minimal signature representation of order 4. The values have been calculated
from Eq. (5.10), by rounding to the third decimal place.
i a(4)Gn(T) Gn(T) Gn(T) Gn(T)
Xj∼ U(0,1), θ= 1 Xj∼ U(0,1), θ=−1Xj∼Exp(1) , θ= 1 Xj∼Exp(1) , θ=−1
1 (1 ,0,0,0) 0 .500 0 .500 0 .409 0 .409
2 (0 ,1,0,0) 0 .221 0 .224 0 .135 0 .138
3 (2 ,−1,0,0) 0 .779 0 .776 0 .683 0 .681
4 (0 ,0,1,0) 0 .081 0 .086 0 .044 0 .047
5 (0 ,2,−1,0) 0 .360 0 .362 0 .226 0 .228
6 (0 ,3,−2,0) 0 .500 0 .500 0 .317 0 .319
7 (1 ,1,−1,0) 0 .640 0 .638 0 .500 0 .500
8 (3 ,−3,1,0) 0 .919 0 .914 0 .865 0 .862
9 (0 ,0,0,1) 0 0 0 0
10 (0 ,0,2,−1) 0 .162 0 .171 0 .087 0 .094
11 (0 ,0,3,−2) 0 .243 0 .257 0 .131 0 .142
12 (0 ,1,1,−1) 0 .302 0 .309 0 .179 0 .185
13 (0 ,3,−3,1) 0 .419 0 .414 0 .274 0 .272
14 (0 ,0,4,−3) 0 .324 0 .342 0 .175 0 .189
15 (0 ,1,2,−2) 0 .383 0 .395 0 .222 0 .232
16 (0 ,2,0,−1) 0 .441 0 .447 0 .270 0 .276
17 (0 ,2,0,−1) 0 .441 0 .447 0 .270 0 .276
18 (0 ,3,−2,0) 0 .500 0 .500 0 .317 0 .319
19 (0 ,3,−2,0) 0 .500 0 .500 0 .317 0 .319
20 (0 ,4,−4,1) 0 .559 0 .553 0 .365 0 .362
21 (0 ,4,−4,1) 0 .559 0 .553 0 .365 0 .362
22 (0 ,5,−6,2) 0 .617 0 .605 0 .413 0 .406
23 (0 ,6,−8,3) 0 .676 0 .658 0 .460 0 .449
24 (1 ,0,1,−1) 0 .581 0 .586 0 .452 0 .457
25 (1 ,2,−3,1) 0 .698 0 .691 0 .548 0 .543
26 (1 ,3,−5,2) 0 .757 0 .743 0 .595 0 .587
27 (2 ,0,−2,1) 0 .838 0 .829 0 .730 0 .724
28 (4 ,−6,4,−1) 1 1 1 1
18Example 6.1 Let(X, Y)be a random vector with diagonal section δ. IfXandYare uniformly
distributed over [0, b], with b >0, from Eq. (3.5) it follows that
GMD (X, Y) = 2 bZ1
0{t−δ(t)}dt=b
1−2Z1
0δ(t)dt
and, from Eq. (3.6), one has
G(X, Y) = 2Z1
0{t−δ(t)}dt= 1−2Z1
0δ(t)dt, (6.1)
that does not depend on b. The first expression in Eq. (6.1) is similar to the representation for the
univariate Gini’s index as two times the area between the Lorenz curve and the line y=x(see Fig.
5.2 in [3]). For example, referring to Eq. (6.1), in the left-hand-side of Fig. 1 we plot the area (dark
grey) in the i.i.d. case, i.e., for δ(t) =t2fort∈[0,1](black line). We obtain an area of 1/6and
soG(X, Y) = 1 /3, that clearly represents also the value of the univariate Gini’s index of the uniform
distribution (since XandYare independent). We also add the two extreme cases obtained from the
FH bounds in Eq. (2.4). For M(red line) we get the lower bound for the index G(X, Y) = 0 , since
δ(t) =t. For W(blue line) we obtain an area of 1/4(dark and light grey) and therefore G(X, Y) = 1 /2
is the upper bound for the Gini’s index of any bidimensional copula with uniform marginals (as shown
in Remark 4.3). Moreover, referring to Eq. (6.1), in the right-hand-side of Fig. 1 one can see the
areas for the diagonal section of the FGM family of copulas defined in Eq. (2.5) for θ= 0(black, i.i.d.
case), 1,0.5(orange) and −1,−0.5(green). We provide the areas for θ= 1,−1 (2/15,3/15)that lead
to the extreme Gini’s indices G(X, Y) = 4 /15 = 0 .2666667 andG(X, Y) = 2 /5 = 0 .4for this family
of copulas. Note that the case θ= 0gives the bound G(X, Y) = 1 /3for positive (θ >0)and negative
dependence cases (θ <0). In this family the Gini’s index is decreasing with θ, as expected.
Similarly, referring to Eq. (6.1), in the left-hand-side of Fig. 2 we plot δfor the Clayton copulas
defined in Eq. (2.6) with θ= 1,2,5,10,20(orange) and θ=−0.2,−0.4,−0.6,−0.8(green). The
independent case is obtained when θ→0(black) while Mis obtained when θ→ ∞ (red) and Wfor
θ=−1(blue). We provide the areas for θ= 1 (0 .1137056) andθ=−0.8 (0.2340531) that lead to
G(X, Y) = 0 .2274112 andG(X, Y) = 0 .4681062 , respectively. The upper bound is 0.5(blue line) and
1/3is also a bound for positive (θ >0)and negative (θ <0)dependence cases. Moreover, referring
to Eq. (6.1), in the right-hand-side of Fig. 2 we plot δfor the Frank copulas defined in Eq. (2.7) with
θ= 1,2,5,10,20(orange) and θ=−1,−2,−5,−10,−20(green). The independent case is obtained
when θ→0(black) while Mis obtained when θ→+∞(red) and Wforθ→ −∞ (blue). We
provide the areas for θ= 1 (0 .1498039) andθ=−1 (0.1827476) that lead to G(X, Y) = 0 .2996078
andG(X, Y) = 0 .3654952 , respectively.
In the second example we consider bivariate Gini’s indices of ( X, Y) with i.d. marginals, according
to the exponential distribution, under different copulas. In this model, the interpretation in terms of
area is different from the univariate case. Indeed, the bivariate Gini’s index turns out to be the area
between the unitary line and the ratio based on the diagonal section of ( X, Y) and the egalitarian
line.
Example 6.2 Let(X, Y)be a random vector with survival diagonal section bδ. If XandYare
exponentially distributed with mean µ, from Eq. (3.1) it follows that
GMD (X, Y) = 2 µZ1
0(
1−ˆδ(t)
t)
dt
and, from Eq. (3.6), one has
G(X, Y) = 1−Z1
0ˆδ(t)
tdt. (6.2)
Eq. (6.2) just depends on the diagonal section bδ(u) =bC(u, u), for all u∈[0,1], and the bivariate
Gini’s index can also be represented as the area between 1andˆδ(u)/u. It is easy to see that in the
190.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
udelta(u)1/6
1/4
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
udelta(u)2/15
/
1/5Fig. 1: Plots and areas referring to Eq. (6.1) for the diagonal section δin the independent case (black),
M(red) and W(blue) and FGM copulas for θ= 1,0.5 (orange) and −1,−0.5 (green).
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
udelta(u)0.1137056
0.2340531
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
udelta(u)0.1498039
0.1827476
Fig. 2: Plots and areas referring to Eq. (6.1) for the diagonal section δof Clayton copulas (left-
hand-side) for θ= 1,2,5,10,20 (orange) and θ=−0.2,−0.4,−0.6,−0.8 (green), while Frank copulas
(right-hand-side) for θ= 1,2,5,10,20 (orange) and θ=−1,−2,−5,−10,−20 (green).
PQD (NQD) case one has bδ(u)≥u2(≤)andG(X, Y)≤1/2 (≥). For example, in the left-hand-side
of Fig. 3 we plot the area (dark grey) in the i.i.d. case, i.e., ˆδ(t) =t2fort∈[0,1](black line), and
we also add the two extreme cases obtained from the FH bounds in Eq. (2.4). For the i.i.d. case we
obtain an area of 1/2and therefore G(X, Y) = 1 /2, that is also the value of the univariate Gini’s
index of the exponential distribution (since XandYare independent). For M(red line) we get the
lower bound for the index G(X, Y) = 0 , while for W(blue line) we obtain an area of ln 2(dark and
light grey) and therefore G(X, Y) = ln 2 = 0 .6931472 is the upper bound for the Gini’s index of any
bidimensional copula with exponential marginal distributions (as shown in Remark 4.3). Moreover, in
the right-hand-side of Fig. 3 one can see the areas for the FGM family of copulas defined in Eq. (2.5)
20withθ= 0(black, i.i.d. case), 1,0.5(orange) and −1,−0.5(green). The respective Gini’s indices are
G(X, Y) = 0 .4166667 andG(X, Y) = 0 .5833333 .
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
udelta(u)/u0.5
0.6931472
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0
udelta(u)/u0.4166667
0.5833333
Fig. 3: Plots and areas referring to Eq. (6.2) for ˆδ(u)/uin the exponential independent case (black),
M(red) and W(blue) and FGM copulas for θ= 1,0.5 (orange) and −1,−0.5 (green).
By recalling Eqs. (3.2) and (3.7) we now define empirical versions of the bivariate Gini’s indices
introduced in Eqs. (3.1) and (3.6), respectively.
Definition 6.1 Let(X, Y)be a random vector. Let (Xi, Yi)be i.i.d. from (X, Y), for i∈ {1, . . . , n }.
Consider Li= min( Xi, Yi)andUi= max( Xi, Yi). The empirical bivariate Gini’s mean difference and
empirical bivariate Gini’s index of (X, Y)are defined respectively as
[GMD (X, Y) =¯U−¯L, bG(X, Y) =¯U−¯L
¯U+¯L,
where ¯L=1
nPn
i=1Liand¯U=1
nPn
i=1Ui.
Note that Zi=|Xi−Yi|, fori∈ {1, . . . , n }, are i.i.d. random variables from Z=|X−Y|and that
[GMD (X, Y) =1
nnX
i=1Zi=¯Z, bG(X, Y) =Pn
i=1ZiPn
i=1(Xi+Yi)=¯Z
¯X+¯Y
and therefore we can apply here all the classic convergence theorems for the sample means. Let us
see two examples.
Example 6.3 To get a simulated sample we consider the Clayton copula in Eq. (2.6) for θ= 1given
by
C(u, v) =uv
u+v−uv, u, v ∈[0,1].
IfX=stY∼ U(0,1), then the conditional distribution function of Y|X=uforu∈(0,1)is
C2|1(v|u) =∂1C(u, v) =v2
(u+v−uv)2, v ∈[0,1].
In order to simulate Y|X=ufrom a given value u, we obtain the inverse function of C2|1as
C−1
2|1(z|u) =u√z
1−(1−u)√z, z ∈[0,1]. (6.3)
21To get a sample with 100data we simulate Xi∼ U(0,1)fori∈ {1, . . . , 100}and then we obtain
Yifrom Eq. (6.3) with u=Xiandz=Zi∼ U(0,1)fori∈ {1, . . . , 100}. Therefore we get the
estimation for the bivariate Gini’s index bG(X, Y) = 0 .2357458 given in Definition 6.1. The exact
value is G(X, Y) = 2 ·0.1137056 = 0 .2274112 (see Fig. 2, left). The data obtained for (Li, Ui)can
be seen in the left-hand-side of Fig. 4. To get a sample from standard exponential distributions if
˜X=st˜Y∼Exp(1) with this survival copula we just apply the inverse transform ¯F−1(z) =−ln(z)to
the above uniform data obtaining the estimation bG(X, Y) = 0 .5·0.6366323 = 0 .3183161 . The exact
value is G(X, Y) = 0 .3068528 . The data obtained for (˜Li,˜Ui)can be seen in the right-hand-side of
Fig. 4.
0.0 0.2 0.4 0.6 0.80.00.20.40.60.81.0
MINMAX
0 1 2 3 4012345
MINMAX
Fig. 4: Simulated ordered data obtained from the Clayton copula in Example 6.3 with standard
uniform (left-hand-side) or exponential (right-hand-side) distributions.
Example 6.4 We now consider the Frank copula in Eq. (2.7) for θ=−1given by
C(u, v) = ln
1 +(eu−1)(ev−1)
e−1
, u, v ∈[0,1].
IfX=stY∼ U(0,1), then the conditional distribution function of Y|X=uforu∈(0,1)is
C2|1(v|u) =eu(ev−1)
e−eu+ev(eu−1), v ∈[0,1]
and thus the inverse function of C2|1is
C−1
2|1(z|u) = ln(eu−e)z−eu
(eu−1)z−eu
, z ∈[0,1]. (6.4)
As before, from a sample with 100data we get the estimation for the bivariate Gini’s index bG(X, Y) =
0.377477 given in Definition 6.1 by using Eq. (6.4). The exact value is G(X, Y) = 2 ·0.1827476 =
0.3654952 (see Fig. 2, right). The data obtained for (Li, Ui)can be seen in the left-hand-side of Fig.
5. To get a sample from standard exponential distributions if ˜X=st˜Y∼Exp(1) with this survival
copula we just apply the inverse transform ¯F−1(z) =−ln(z)to the above uniform data obtaining the
estimation bG(X, Y) = 0 .5·1.069694 = 0 .534847 . The exact value is G(X, Y) = 0 .539814 . The data
obtained for (˜Li,˜Ui)can be seen in the right-hand-side of Fig. 5.
In the last example we compute the empirical efficiency Gini’s index of two coherent systems.
22Example 6.5 Consider the system T16given in Table 1, with i.i.d. components having standard
exponential distributions. From a sample with 100data we get the estimation of the efficiency Gini’s
indexbG4(T) = 0 .250. The exact value is G4(T) = 0 .273as shown in Table 1. The data obtained for
(X1:4, T)can be seen in the left-hand-side of Fig. 6. Similarly, if one consider the system T22given
in Table 1, with i.i.d. components having standard exponential distribution, then bG4(T) = 0 .416. The
exact value is G4(T) = 0 .409as shown in Table 1. The data obtained for (X1:4, T)can be seen in the
right-hand-side of Fig. 6.
0.0 0.2 0.4 0.6 0.80.20.40.60.81.0
MINMAX
0.0 0.5 1.0 1.5 2.001234
MINMAX
Fig. 5: Simulated ordered data obtained from the Frank copula in Example 6.4 with standard uniform
(left-hand-side) or exponential (right-hand-side) distributions.
0.0 0.2 0.4 0.6 0.80.5 1.0 1.5 2.0
MINT
0.0 0.2 0.4 0.6 0.80.00.51.01.52.02.53.0
MINT
Fig. 6: Simulated ordered data obtained of ( X1:4, T), for the systems T16(left-hand-side) and T22
(right-hand-side) introduced in Table 1, with i.i.d. components having standard exponential distribu-
tions.
23Declaration of Competing Interest
The authors declare that they have no known competing financial interests or personal relationships
that could have appeared to influence the work reported in this paper.
Acknowledgements
M.C. is member of the group GNCS of INdAM (Istituto Nazionale di Alta Matematica). This
work is partially supported by MUR-PRIN 2022, project 2022XZSAFN “Anomalous Phenomena
on Regular and Irregular Domains: Approximating Complexity for the Applied Sciences”, and MUR-
PRIN 2022 PNRR, project P2022XSF5H “Stochastic Models in Biomathematics and Applications”.
M.C. expresses his warmest thanks to the Departamento de Estad´ ıstica e Investigaci´ on Operativa
of Universidad de Murcia for the hospitality during a three-month visit carried out in 2023. J.N.
thanks the support of Ministerio de Ciencia e Innovaci´ on of Spain under grants PID2019-103971GB-
I00/AEI/10.13039/501100011033 and MCIN/AEI/10.13039/501100011033 and the project TED2021-
129813A-I00 with the support of the European Union “NextGenerationEU”/PRTR.
References
[1] Arnold, B.C., Castillo, E., Sarabia, J.M., 1999. Conditional Specification of Statistical Models.
New York, NY: Springer New York.
[2] Arnold, B.C., Castillo, E., Sarabia, J.M., 2012. Conditionally Specified Distributions. Springer
Science & Business Media.
[3] Arnold, B.C., Sarabia, J.M., 2018. Majorization and the Lorenz Order with Applications in
Applied Mathematics and Economics. Statistics for Social and Behavioral Sciences. Springer.
[4] Capaldo, M., Di Crescenzo, A., Meoli, A., 2023. Cumulative information generating function and
generalized Gini functions. Metrika: http://dx.doi.org/10.1007/s00184-023-00931-3.
[5] Capaldo, M., Di Crescenzo, A., Pellerey, F., 2024. Generalized Gini’s mean difference through
distortions and copulas, and related minimizing problems. Statistics & Probability Letters, 206,
109981.
[6] Caramellino, L., Spizzichino, F., 1994. Dependence and aging properties of lifetimes with Schur-
constant survival functions. Probability in the Engineering and Informational Sciences 8(1), 103-
111.
[7] Ceriani, L., Verme, P., 2012. The origins of the Gini index: extracts from Variabilit` a e Mutabilit` a
(1912) by Corrado Gini. The Journal of Economic Inequality 10(3), 421-443.
[8] Di Crescenzo, A., 2007. A Parrondo paradox in reliability theory. The Mathematical Scientist
32(1), 17–22.
[9] Di Crescenzo, A., Kayal, S., Meoli, A., 2021. Fractional generalized cumulative entropy and its
dynamic version. Communications in Nonlinear Science and Numerical Simulation 102, 105899.
[10] Di Crescenzo, A., Longobardi, M., 2009. On cumulative entropies. Journal of Statistical Planning
and Inference 139(12), 4072-4087.
[11] Durante, F., Papini, P.L., 2007. A weakening of Schur-concavity for copulas. Fuzzy Sets and
Systems 158(12), 1378-1383.
[12] Durante, F., Sempi, C., 2016. Principles of Copula Theory. CRC/Chapman & Hall, London.
[13] Fuchs, S., Tschimpke, M., 2024. A novel positive dependence property and its impact on a popular
class of concordance measures. Journal of Multivariate Analysis 200, 105259.
[14] Gastwirth, J.L., 1972. The estimation of the Lorenz curve and Gini index. The review of eco-
nomics and statistics, 306-316.
24[15] Kayal, S., 2016. On generalized cumulative entropies. Probability in the Engineering and Infor-
mational Sciences 30(4), 640-662.
[16] Koshevoy, G.A., Mosler, K., 1997. Multivariate gini indices. Journal of Multivariate Analysis
60(2), 252-276.
[17] Navarro, J., 2022. Introduction to System Reliability Theory. Springer.
[18] Navarro, J., Arriaza, A., Su´ arez-Llorens, A., 2023. Predicting failure times of coherent systems.
To appear in Naval Research and Logistics.
[19] Navarro, J., Ruiz, J.M., Sandoval, C.J., 2007. Properties of coherent systems with dependent
components. Communications in Statistics Theory and Methods 36(1), 175–191.
[20] Navarro, J., Sordo, M.A., 2018. Stochastic comparisons and bounds for conditional distributions
by using copula properties. Dependence Modeling 6(1), 156-177.
[21] Navarro, J., Spizzichino, F., 2010. Comparisons of series and parallel systems with components
sharing the same copula. Applied Stochastic Models in Business and Industry 26(6), 775-791.
[22] Nelsen, R.B., 2005. Some properties of Schur-constant survival models and their copulas. Brazil-
ian Journal of Probability and Statistics 19(2), 179-190.
[23] Nelsen, R.B., 2006. An Introduction to Copulas. Springer Science & Business Media.
[24] Ortega-Jim´ enez, P., Pellerey, F., Sordo, M.A., Su´ arez-Llorens, A., 2022. A minimizing problem
of distances between random variables with proportional reversed hazard rate functions. In:
Building Bridges Between Soft and Statistical Methodologies for Data Science. Springer, Cham,
pp. 311-318.
[25] Ortega-Jim´ enez, P., Sordo, M.A., Su´ arez-Llorens, A., 2021. Stochastic comparisons of some dis-
tances between random variables. Mathematics 9(9), 981.
[26] Pellerey, F., Navarro, J., 2022. Stochastic monotonicity of dependent variables given their sum.
Test31(2), 543-561.
[27] Psarrakos, G., Navarro, J., 2013. Generalized cumulative residual entropy and record values.
Metrika 76(5), 623-640.
[28] Rao, M., Chen, Y., Vemuri, B.C., Wang, F., 2004. Cumulative residual entropy, a new measure
of information. IEEE Transactions on Information Theory 50(6), 1220-1228.
[29] Samaniego, F.J., 1985. On closure of the IFR class under formation of coherent systems. IEEE
Transactions on Reliability R-34 (1), 69–72.
[30] Samaniego, F.J., 2007. System Signatures and Their Applications in Engineering Reliability.
International Series in Operations Research & Management Science 110, Springer, New York.
[31] Shaked, M., Shanthikumar, J.G., 2007. Stochastic Orders. Springer Series in Statistics. Springer,
New York.
[32] Toomaj, A., Di Crescenzo, A., 2020. Generalized entropies, variance and applications. Entropy
22(6), 709.
[33] Xiong, H., Shang, P., Zhang, Y., 2019. Fractional cumulative residual entropy. Communications
in Nonlinear Science and Numerical Simulation 78, 104879.
[34] Yin, X., Balakrishnan, N., Yin, C., 2023. Bounds for Gini’s mean difference based on first four
moments, with some applications. Statistical Papers 64, 2081-2100.
[35] Yitzhaki, S., 2003. Gini’s mean difference: a superior measure of variability for non-normal
distributions. Metron 61(2), 285-316.
25