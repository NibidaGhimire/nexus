arXiv:2301.08039v1  [math.PR]  19 Jan 2023Kinetic Langevin MCMC Sampling Without
Gradient Lipschitz Continuity - the Strongly
Convex Case
Tim Johnston1, Iosif Lytras1, and Sotirios Sabanis1, 2, 3
1School of Mathematics, The University of Edinburgh, Edinbu rgh, UK.
2The Alan Turing Institute, London, UK.
3National Technical University of Athens, Athens, Greece.
January 20, 2023
Abstract
In this article we consider sampling from log concave distri butions in Hamiltonian
setting, without assuming that the objective gradient is gl obally Lipschitz. We propose
two algorithms based on monotone polygonal (tamed) Euler sc hemes, to sample from
a target measure, and provide non-asymptotic 2-Wasserstei n distance bounds between
the law of the process of each algorithm and the target measur e. Finally, we apply
these results to bound the excess risk optimization error of the associated optimization
problem.
1 Introduction
We are interested in non-asymptotic estimates for samples t aken from distributions of the
form
µβ(dθ)∝exp(−βu(θ))dθ, (1)
whereu:Rd→R+andβ >0.This problem has many applications in optimization
and particularly, in machine learning as it is known that for large values of the temperature
parameterβ >0, the target measure µβconcentrates around the global minimizers of
the function u([16]). An efﬁcient way of sampling from the target measure is thr ough
simulating the associated (overdamped) Langevin SDE
dLt=−∇u(Lt)dt+/radicalbigg2
βdBt, t≥0,
L0=θ0,(2)
exploiting the fact that this SDE admits the target measure a s its unique invariant measure.
Inspired by this observation Langevin-based algorithms ha ve become a popular choice for
sampling from distributions in high dimensional spaces.
1The non-asymptotic convergence rates of algorithms based o n (2) have been studied
extensively in recent literature. For the setting of determ inistic gradients some recent key
references are [ 11], [7], [4], [23], whilst for stochastic gradient algorithms recent import ant
contributions in the convex case are [ 3], [9], [2] and in the non-convex setting [ 21], [6] [5].
More recently work has been done assuming only local conditi ons in [ 25] and [ 19].
An alternative to methods based on overdamped Langevin SDE i s the class of algo-
rithms based on the underdamped Langevin SDE, which has also gained signiﬁcant atten-
tion in recent years. Let γ >0. The underdamped Langevin SDE is given by
d˜Vt=−γ˜Vtdt−∇u/parenleftBig
˜θt/parenrightBig
dt+/radicalbigg2γ
βdBt,
d˜θt=˜Vtdt, t>0.(3)
with initial conditions˜V0=V0,
˜θ0=θ0,
where{Bt}t≥0is Brownian motion adapted to its natural ﬁltration F:={Ft}t≥0and/braceleftBig/parenleftBig
˜θt,˜Vt/parenrightBig/bracerightBig
t≥0are called position and momentum process respectively. Sim ilarly to the
overdamped Langevin SDE, this diffusion can be used as both a n MCMC sampler and non-
convex optimizer, since under appropriate conditions, the Markov process/braceleftBig/parenleftBig
˜θt,˜Vt/parenrightBig/bracerightBig
t≥0
has a unique invariant measure given by
πβ(dθ,dv)∝exp/parenleftbigg
−β/parenleftbigg1
2|v|2+u(θ)/parenrightbigg/parenrightbigg
dθdv. (4)
Consequently, the marginal distribution of ( 4) inθis precisely the target measure de-
ﬁned in ( 1). This means that sampling from ( 4) in the extended space and then keeping the
samples in the θspace deﬁnes a valid sampler for the density given by ( 1).
The underdamped Langevin MCMC has shown improved convergen ce rates in the
case of convex and Lipschitz potential u, e.g see [ 8], [10]. In the non-convex setting im-
portant work has been done in [ 1], [13], [5]. The current article extends these results by
introducing a novel way to sample from distributions associ ated with objective functions
with non-global Lipschitz gradients, utilising a decompos ition proposed in [ 17]. To our
knowledge this relaxation of the global gradient Lipschitz assumption is a novelty in un-
derdamped (HMC) sampling, and can be seen as an analogous of t he algorithms in [ 4] and
[19] which are examples of taming sampling algorithms in the ove rdamped case.
Notation . For an integer d≥1, the Borel sigma-algebra of Rdis denoted by B/parenleftbig
Rd/parenrightbig
.
We write the dot product on Rdas∝an}⌊∇a⌋ketle{t·,·∝an}⌊∇a⌋ket∇i}ht, whilst| · |denotes the associated vector norm
and|| · || denotes the Euclidean matrix norm. Moreover we denote by Cmthe space
ofm-times continuously differentiable functions, and by C1,1the space of continuously
differentiable functions with Lipschitz gradient. The set of probability measures deﬁned
on the measurable space/parenleftbig
Rd,B/parenleftbig
Rd/parenrightbig/parenrightbig
is denoted by P/parenleftbig
Rd/parenrightbig
. For anRd-valued random
variable,L(X)andE[X]are used to denote its law and its expectation respectively. Note
that we also write E[X]asEX. Forµ,ν∈ P/parenleftbig
Rd/parenrightbig
, letC(µ,ν)denote the set of probability
measures ΓonB/parenleftbig
R2d/parenrightbig
so that its marginals are µ,ν. Finally, for µ,ν∈ P/parenleftbig
Rd/parenrightbig
, the
2Wasserstein distance of order p≥1, is deﬁned as
Wp(µ,ν) := inf
Γ∈C(µ,ν)/parenleftbigg/integraldisplay
Rd/integraldisplay
Rd|θ−θ′|pΓ(dθ,dθ′)/parenrightbigg1/p
,
and for an open Ω⊂Rdthe Sobolev space H1(Ω)as
H1(Ω) :=/braceleftbig
u∈L2(Ω) :∃g1,g2,...,g N∈L2(Ω),such that
/integraldisplay
Ωu∂φ
∂xi=−/integraldisplay
Ωgiφ, φ∈C∞
c(Ω),i: 1≤i≤N/bracerightbigg
,
whereC∞
c(Ω)denotes the space of inﬁnitely times differentiable and com pactly supported
functions.
2 Overview of main results
2.1 Assumptions and marginal distributions of interest
Letu∈C2(Rd)be a non-negative objective function let h:=∇u. Then we assume the
following:
Assumption 2.1. (strong convexity). There exists an m>0such that
∝an}⌊∇a⌋ketle{th(x)−h(y),x−y∝an}⌊∇a⌋ket∇i}ht ≥2m|x−y|2x,y∈Rd.
Assumption 2.2. (Local Lipschitz continuity). There exists l>0andL>0such that
|h(x)−h(y)| ≤L(1+|x|+|y|)l|x−y|x,y∈Rd.
Assumption 2.3. There holds
E|θ0|2k+E|V0|2k<∞ ∀k∈N.
Remark 2.4. The moment requirement in Assumption 2.3can be further relaxed. This
condition is chosen for notational convenience to improve t he readability of the article.
We denote by x∗the minimizer of u. Let us now deﬁne the marginal distributions of
πβwhich is given in ( 4), and plays a pivotal role in our analysis.
Deﬁnition 2.5. Letµβ(x) :=e−βu(x)/integraltext
Rde−βu(x)dxandkβ(y) :=e−β|y|2
2
/integraltext
Rde−β|y|2
2dy.
Thenπβis the product measure with density µβ⊗kβ.
2.2 Statement of main results
Let{(¯θλ
n,¯Vλ
n)}n≥0,{(¯xλ
n,¯Yλ
n)}n≥0be the processes generated by the tKLMC1 and tKLMC2
algorithms respectively as given in ( 29) and ( 35) respectively. Let us state the main theo-
rems regarding their W2convergence to the invariant measure πβ.
3Theorem 2.6. Letq >0,ǫ >0and Assumptions 2.1-2.3hold. LetKbe the Lipschitz
constant of the gradient of the ǫ-Moreau-Yosida approximation of u. Then there exists
˙C >0such that, for γ≥max{/radicalBig
K+m
β,K,32,48(2m+1)2
m}, andλ<γ−1,
W2(L(¯θλ
n,¯Vλ
n),πβ)≤˙C/parenleftBig√
λγ3
2+γ−q+√
2e−λm
βγnW2(L(θ0,V0),πβ)/parenrightBig
+ǫ,
where˙Chas(2l+2)2qdependence ond
β.
Remark 2.7. Settingǫ=λ1
5and allowing the worst-case scenario that K=1
ǫthen,
settingγ=O(1
ǫ)andq= 5yields
W2(L(¯θλ
n,¯Vλ
n),πβ)≤4˙C/parenleftbigg
λ1
5+√
2e−λ6
5m
βnW2(L(θ0,V0),πβ)/parenrightbigg
∀n∈N.
Theorem 2.8. Letǫ >0,q >0and Assumptions 2.1-2.3hold. LetKbe the Lipschitz
constant of the gradient of the ǫ-Moreau-Yosida approximation of u. There exists Csuch
that forγ≥/radicalBig
2K
βandλ≤ O(γ−5),
W2/parenleftbig
L(¯xλ
n,¯Yλ
n),πβ/parenrightbig
≤C(λγ2K+γǫ+γ−q+2)+√
2e−m
γβλnW2(L(θ0,V0),πβ),
whereChas(2l+2)2qdependence ond
β.
Remark 2.9. The worst case bounds, when K=2
ǫcan be derived by setting γ=/radicalBig
2
β/radicalBig
1
ǫ,
W2/parenleftbig
L(¯xλ
n,¯Yλ
n),πβ/parenrightbig
≤2C(λγ4+γ−1+γ−q
2+1)+√
2e−m
γβλnW2(L(θ0,V0),πβ)
so ifλ=O(γ−5)
W2/parenleftbig
L(¯xλ
n,¯Yλ
n),πβ/parenrightbig
≤¯Cλ1
5+√
2e−m
γβλnW2(L(θ0,V0),πβ)
The two algorithms are two alternative tamed Euler discreti zations of the underdamped
Langevin dynamics. They can be seen as the tamed analogous (i n the deterministic gradi-
ent case) of the SGHMC1 and SGHMC2 algorithms in [ 14]. From Theorem 1 and Theo-
rem 2 one can see that the convergence rates are fairly compar able although the KLCM2
algortithm has superior λ- dependence on the ﬁrst term, which leads to improved resutl s
in the Lipschitz case. In addition, we show that for some R0depending on uwhich is
given in the proof, the process ¯θλ
ngenerated by the tKLMC1 algorithm (similar results can
be obtained for the process ¯xλ
nbuilt by the tKLMC2 algortihm), can be used for the op-
timization of the excess risk of the associated optimizatio n problem, when one estimates
the minimizer of uforβ→ ∞ .
Theorem 2.10. Let Assumptions 2.1-2.3hold. andR0=/radicalBig
2u(0)
m+/radicalBig
(u(0)+1)d
β+1.Then,
Eu/parenleftbig¯θλ
n1B(0,R0)(¯θλ
n)/parenrightbig
−u(x∗)≤C′W2(L(¯θλ
n,¯Vλ
n),πβ)+2u(0)W2(L(¯θλ
n,¯Vλ
n),πβ)+2d
mβ
whereC′is a constant with polynomial dependence on d.
Remark 2.11. Although we are in a strongly convex setting, the fact that th e gradient is
not assumed to be globally Lipschitz provides additional co mplexity. To the best of the
authors’ knowledge, there is little in the literature for th is class of problems, even for
gradient descent algorithms which make no use of Langevin dy namics.
42.3 Overview of contributions and comparison with existing litera-
ture
In this article we study the problem of sampling from strongl y log-concave distributions
with superlinearly-growing gradients. To our knowledge, t he current literature for sam-
pling via kinetic Langevin (underdamped) algorithms assum es only global Lipschitz con-
tinuity of the objective gradient, which is in many cases res trictive. The main contribution
of this article, therefore, is the weakening of the gradient -Lipschitz assumption, replacing
it with Assumption 2.2which is considerably weaker. This means our results can be a p-
plied to the case of superlinearly growing gradients, for in stance for neural networks as in
[19].
A natural comparison can be made with [ 10], which also considers sampling from
strongly convex distributions with kinetic Langevin algor ithms in the Lipschitz setting.
The converge rate of our algorithm is worse, which is unsurpr ising since there are no
known contraction rates in the non-gradient Lipschitz sett ing.
To address this problem we ﬁrst sample from the approximatin g Moreau-Yosida mea-
sure (since the Moreau Yosida gradient is Lipschitz). In the general case one has that the
gradient of the Moreau-Yosida regularization hMY,ǫ is1
ǫ-Lispchitz (although in practice
could be considerably lower). Since the proof of the converg ence results relies upon the
contraction results of [ 10], this imposes a technical restriction, connecting the par ameter
γto the worst case Lipschitz constant,1
ǫ. Another explanation for the suboptimality of
the convergence rates, is the fact that instead of sampling w ith an algorithm that involves
the Moreau-Yosida gradient (since we ﬁrst sample from the Mo reau-Yosida measure) we
chose a tamed scheme of the original gradient h. The reason for this is that an algorithm
involving the Moreau-Yosida gradient, would impose many pr actical difﬁculties in appli-
cations as it would require a solution of an optimization pro blem in each step. The "taming
error” (i.e second term in Theorem 1 and third term in Theorem 3) can be seen as the price
we pay for this adjustment. Despite all these technical rest rictions, the fact we were able to
establish convergence in the non-Lipschitz setting shows t he advantage of a more sophis-
ticated taming scheme like the one used in the article, makin g a ﬁrst promising step into
the direction of sampling from disrtibution with superline ar log-gradients in the kinetic
Langevin setting.
In the case where hisM-Lipschitz one may choose γ≥/radicalBig
M+m
βindependently of
ǫ, so that setting ǫ=√
λyields a convergence rate of O(√
λ+γ−q)for tKLMC1 and
O(λ+γ−q+2)for tKLMC2 where the term γ−qis only a result of the ’taming’ (and
therefore is not needed if the gradient is growing linearly) .
An interesting comparison is the one with the analogous resu lt in the overdamped case in
[4]. One signiﬁcant advantage of our tamed scheme compared to t he one used in [ 4] is that
it doesn’t require Assumption H2 to prove the necessary mome nt bounds. Furthermore,
although the achieved convergence rates are worse than thos e in [ 4] with respect to the
stepsize, the dependence of the dimension of our constants i s polynomial, whilst in [ 4] it
is exponential (possibly due to overestimations as stated i n Remark 16).
Finally , our algorithms could perform well for optimizatio n problems as an alternative to
gradient descent by picking βlarge enough. Since there are few works in the non-gradient
Lipschitz setting this could possibly be useful in applicat ions. For example, compared with
the work in [ 24], our complexity bounds have better dependence on the growt h parameter
lin2.2, which theoretically makes it more suitable for higher orde r problems.
53 First remarks and premiliminary statements
We start our analysis by stating some preliminary results de rived from a assumptions 2.1-
2.2.
Remark 3.1. Sincehis the gradient of a strongly convex function u≥0one obtains that
u(0)≥u(x)+∝an}⌊∇a⌋ketle{th(x),0−x∝an}⌊∇a⌋ket∇i}ht+m|x|2.
Then it can be easily deduced that
∝an}⌊∇a⌋ketle{th(x),x∝an}⌊∇a⌋ket∇i}ht ≥m
2|x|2−u(0), x∈Rd. (5)
Remark 3.2. Settingx=x∗in(5)yields
m
2|x∗|2−u(0)≤0,
which implies that
x∗∈¯B(0,/radicalbigg
2u(0)
m).
As a consequence of these results, we obtain useful bounds on the moments of the
invariant measure.
Lemma 3.3 ( [21], Lemma 3.2) .Let Assumptions 2.1-2.3hold. LetLtbe given by the
overdamped Langevin SDE in (2)with initial condition L0. Then
E|Lt|2≤E|L0|2e−mt+2u(0)+d/β
m(1−e−mt), (6)
foraandbas above.
Proof. Using Ito’s formula on Yt=|Lt|2one deduces
dYt=−2∝an}⌊∇a⌋ketle{tLt,h(Lt)∝an}⌊∇a⌋ket∇i}htdt+2d
β+/radicalbigg8
β∝an}⌊∇a⌋ketle{tLt,dBt∝an}⌊∇a⌋ket∇i}ht.
Applying Ito’s formula on e2mt|Lt|2
d(emtYt) =−2emt∝an}⌊∇a⌋ketle{tLt,h(Lt)∝an}⌊∇a⌋ket∇i}htdt+memtYtdt+2d
βemtdt+/radicalbigg8
βemt∝an}⌊∇a⌋ketle{tLt,dBt∝an}⌊∇a⌋ket∇i}ht
so after rearranging
Yt=e−mtY0−2/integraldisplayt
0e−m(t−s)∝an}⌊∇a⌋ketle{tLs,h(Ls)∝an}⌊∇a⌋ket∇i}htds+m/integraldisplayt
0em(s−t)Ysds+2d
mβ/parenleftbig
1−e−mt/parenrightbig
+Mt,
(7)
whereMtis anFtmartingale (as one can show by applying standard stopping ti me tech-
niques initially to show the ﬁniteness of relative moments w ithout the use of integrated
factors). Using the dissipativity conditon on the second te rm yields
−2/integraldisplayt
0e−m(t−s)∝an}⌊∇a⌋ketle{tLs,h(Ls)∝an}⌊∇a⌋ket∇i}htds≤2/integraldisplayt
0em(s−t)(u(0)−m
2Ys)ds
≤2u(0)
m/parenleftbig
1−e−mt/parenrightbig
−m/integraldisplayt
0e−m(t−s)Ysds.
6Inserting this into ( 7) leads to
Yt=e−mtY0+2u(0)
m/parenleftbig
1−e−mt/parenrightbig
+2d
mβ/parenleftbig
1−e−mt/parenrightbig
+Mt.
Taking expectations and using the martingale property comp letes the proof.
Lemma 3.4. Let Assumptions 2.1-2.3hold. IfYis a random variable such that L(Y) =
µβthen,
E|Y|2≤2
m(u(0)+d
β).
As a result,
E|Y| ≤/radicalBigg
2
m(u(0)+d
β). (8)
Proof. Since the solution of the Langevin SDE ( 2) converges in W2distance toµβand
supt≥0E|¯Lt|2<∞due to ( 6), this also implies the convergence of second moments.
Therefore, if Yis random variable such that L(W) =µβby Lemma 3.3there holds that
E|Y|2= lim
t→∞E|¯Lt|2≤2
m(u(0)+d
β).
Theorem 3.5. [[15], Theorem 1.1] Let XfollowN(m0,Σ)with density φ, and letYhave
densityφfwherefis a log-concave function. Then for any convex map gthere holds:
E[g(Y−EY)]≤E[g(X−EX)].
Proposition 3.6. Letp≥2and letYbe a random variable such that L(Y) =µβ.Then
E|Y|p≤2p−1((d
βm)p
2(1+p/d)p
2−1+/parenleftbigg2
m(u(0)+d
β)/parenrightbiggp/2
:=Cµβ,p.
Proof. Sincee−βu(x)=e−β(u−m
2|x|2)e−βm
2|x|2and since the function u−m
2|x|2is con-
vex (due to strong convexity of u) the assumptions of Theorem 3.5are valid for Xwhich
has distribution N(0,1
βmId), and forYwith density µβ. Since the function g:x→ |x|p
is convex applying the result of Theorem 3.5leads to
E|Y−EY|p≤E|X|p= (d
βm)p
2Γ((d+p)/2)
Γ(d/2)(d/2)p
2≤(d
βm)p
2(1+p/d)p
2−1. (9)
Combining ( 9) and the result of Lemma 3.4yields
E|Y|p≤2p−1(E|Y−EY|p+(E|Y|)p)≤2p−1((d
βm)p
2(1+p/d)p
2−1+/parenleftbigg2
m(u(0)+d
β)/parenrightbiggp/2
,
(10)
which completes the proof.
74 A brief introduction to Moreau-Yosida regularization
It is well-known that in order to construct a Langevin-type a lgorithm to efﬁciently sample
from a target measure, one requires a contraction property. However, to the best of the au-
thors’ knowledge, Lipschitz continuity of the objective fu nction gradient (drift coefﬁcient
in the case of the underdamped Langevin SDE) is a requirement in the literature in order
to obtain contraction rates in the underdamped case. The pur pose of this section, therefore,
is to construct a C1,1approximation of uthat inherits the convexity properties of u, and
for which the associated target measure is close to πβ.
Let us present some facts about the Moreau-Yosida regulariz ation which are central to
the subsequent analysis. For the interested reader, key ref erences are [ 22], [12], [18]. For
an arbitrary lower semicontinuous function g:Rd→(−∞,∞]andǫ >0theǫ-Moreau
Yosida regularization gǫ:Rd→(−∞,∞]is given by
gǫ(x) := inf
y∈Rd{g(y)+1
2ǫ|x−y|2}.
The ﬁrst thing to note is that in the case that gis convex
argmingǫ= argming,
mingǫ= ming.(11)
Furthermore, if one deﬁnes the convex conjugate of gas
g∗(v) := sup
x∈dom(g){∝an}⌊∇a⌋ketle{tv,x∝an}⌊∇a⌋ket∇i}ht−g(x)}.
then it can be shown that
gǫ=/parenleftBig
g∗+ǫ
2|·|2/parenrightBig∗
. (12)
The approximation gǫinherits the convexity property of gand is continuously differen-
tiable with1
ǫLipschitz gradient, i.e
|∇gǫ(x)−∇gǫ(y)| ≤1
ǫ|x−y|, x,y∈Rd,
and∇gǫis given by
∇gǫ(x) =1
ǫ(x−proxǫ
g(x)), (13)
where
proxǫ
g(x) := argmin
y∈Rd/braceleftbigg
g(y)+1
2ǫ|x−y|2/bracerightbigg
,
is a continuous mapping. In addition, for every x∈Rd, the function ǫ∝ma√sto→gǫ(x)is
decreasing, and one has the bound gǫ≤gand the convergence result
lim
ǫ→0gǫ(x) =g(x), x∈Rd.
Finallygǫinherits the differentiability and convexity properties o fg, and ifg∈C1then
one has the relation
∇gǫ(x) =∇g(proxǫ
g(x)). (14)
In particular, the following result regarding the second or der differentiability of gǫis im-
portant for our analysis.
8Theorem 4.1. [[20], Theorem 3.12] Let gbe a proper lower semicontinuous convex func-
tion. Then if g∈ C2one has that gǫ∈ C2.
Lemma 4.2. Letgbe a proper lower semicontinuous mstrongly convex function. Then,
ifǫ<1
m,gǫis anm
2strongly convex function.
Proof. Proposition 12.60 in [ 22] implies that
∇(gǫ)∗(x) =∇g∗(x)+ǫx. (15)
Furthermore, since gism- strongly convex we have that ∇g∗is1
m-Lipschitz. Hence, by
the assumption ǫ<1
mit follows from ( 15) that∇(gǫ)∗is2
m-Lipschtiz. Then utilising the
same argument from [ 22], one obtains that gǫism
2strongly convex.
4.1 Finding the right approximation for the objective gradi ent
Let us now introduce the function uMY,ǫ:Rd→R+given as the Moreau-Yosida regular-
ization ofu, that is
uMY,ǫ(x) := inf{u(y)+1
2ǫ|x−y|2}, (16)
and denote by hMY,ǫ=∇uMY,ǫ its gradient. Let λ >0be the stepsize of the proposed
algorithm, and let ǫ=ǫ(λ)be a function of λ. By Theorem 4.1the function uMY,ǫ is twice
continuously differentiable, and by Lemma 4.2the function uMY,ǫ is strongly convex with
parametermifǫ<1
m. Moreover, hMY,ǫ is1
ǫLipschitz continuous, and as a result
mId≤Hess(uMY,ǫ)≤1
ǫId. (17)
Sincex∗is the unique minimizer of uand due to ( 11),
0≤u(x∗) =uMY,ǫ(x∗)≤uMY,ǫ(x), (18)
for everyx∈Rd. Another key property is the inequality
uMY,ǫ(x)≤u(x), (19)
which immediately implies that
Zǫ
β:=/integraldisplay
Rde−βuMY,ǫ(x)dx≥/integraldisplay
Rde−βu(x)dx:=Zβ. (20)
Finally, one observes that
∝an}⌊∇a⌋ketle{thMY,ǫ(x),x∝an}⌊∇a⌋ket∇i}ht ≥m
2|x|2−uMY,ǫ(0)≥m
2|x|2−u(0). (21)
Deﬁnition 4.3. Let0<ǫ<1
m. We deﬁne
µǫ
β(x) :=e−βuMY,ǫ(x)
/integraltext
Rde−βuMY,ǫ(x)dx,
and
πǫ
β(x,y) :=e−βuMY,ǫ(x)−β
2|y|2
/integraltext
Rde−βuMY,ǫ(x)−β
2|y|2dxdy.
9Having stated the key properties of uMY,ǫ , let us consider the properties of the respec-
tive target measure with respect to the W2distance.
Lemma 4.4. Letǫ>0. Then one obtains
|∇uMY,ǫ(x)| ≤ |∇u(x)|,
for everyx∈Rd.
Proof. Recall that by ( 14) and ( 13) one hash(proxǫ
u(x)) =1
ǫ(x−proxǫ
u(x)), where we
have deﬁned h:=∇u.Whenx= proxǫ
u(x)the result holds trivially. So let us assume
x∝ne}ationslash= proxǫ
u(x). First of all by the strong convexity there holds
∝an}⌊∇a⌋ketle{th(x)−h(proxǫ
u(x)),x−proxǫ
u(x)∝an}⌊∇a⌋ket∇i}ht ≥m|x−proxǫ
u(x)|2,
which implies that
|h(x)||x−proxǫ
u(x)| ≥ ∝an}⌊∇a⌋ketle{th(x),x−proxǫ
u(x)∝an}⌊∇a⌋ket∇i}ht
≥m|x−proxǫ
u(x)|2+∝an}⌊∇a⌋ketle{th(proxǫ
u(x)),x−proxǫ
u(x)∝an}⌊∇a⌋ket∇i}ht
= (1
ǫ+m)|x−proxǫ
u(x)|2
≥1
ǫ|x−proxǫ
u(x)|2.
This means that since x∝ne}ationslash= proxǫ
u(x)there holds, due to ( 14),
|hMY,ǫ(x)|=1
ǫ|x−proxǫ
u(x)| ≤ |h(x)|,
which completes the proof.
Lemma 4.5. Let0<ǫ<1
m. Then,∀x∈Rd,
|h(x)−hMY,ǫ(x)| ≤22l+2L(1+|x|+√
R)2l+2ǫ
whereR=/radicalBig
2u(0)
m).
Proof. Using ( 14), Assumption 2.2and Lemma 4.4one obtains,
|h(x)−hMY,ǫ(x)| ≤ |h(x)−h(proxǫ
u(x)|
≤L(1+|x|+|proxǫ
u(x)|)l|x−proxǫ
u(x)|
≤L(1+|x|+|proxǫ
u(x)|)l|hMY,ǫ(x)|ǫ
≤L(1+|x|+|proxǫ
u(x)|)l|h(x)|ǫ
≤L(1+|x|+|proxǫ
u(x)|)l(1+|x|+|x∗|)l|x−x∗|ǫ
≤L(1+|x|+|proxǫ
u(x)|)l(1+|x|+|x∗|)l+1ǫ.(22)
Since the proximal operator is 1-Lipschitz and proxǫ
u(x∗) =x∗, we see that
|proxǫ
u(x)| ≤ |x|∗+|proxǫ
u(x)−proxǫ
u(x∗)| ≤ |x∗|+|x−x∗| ≤ |x|+2√
R, (23)
at which point inserting ( 23) into ( 22) yields the result.
10Corollary 4.6. There exists a constant ¯c>0such that
W2(πβ,πǫ
β)≤¯cǫ,
for everyǫ>0.
Proof. Letxtbe the overdamped Langevin SDE given as :
dxt=−h(xt)dt+/radicalbig
2β−1dBt,
and letytbe given as
dyt=−hMY,ǫ(yt)dt+/radicalbig
2β−1dBt
for initial conditions X0,Y0satisfying L(X0) =L(Y0) =µβ. As a result, since xt−yt
has vanishing diffusion, one can take the time derivative de ﬁned a.s ,i.e
d
dt|xt−yt|2=−2∝an}⌊∇a⌋ketle{txt−yt,h(xt)−hMY,ǫ(yt)∝an}⌊∇a⌋ket∇i}ht
=−2∝an}⌊∇a⌋ketle{txt−yt,h(xt)−hMY,ǫ(xt)∝an}⌊∇a⌋ket∇i}ht−2∝an}⌊∇a⌋ketle{txt−yt,hMY,ǫ(xt)−hMY,ǫ(yt)∝an}⌊∇a⌋ket∇i}ht
≤m
2|xt−yt|2+2
m|h(xt)−hMY,ǫ(xt)|2−m|xt−yt|2
≤ −m
2|xt−yt|2+2
m|h(xt)−hMY,ǫ(xt)|2.
where the last step was obtained by Young’s inequality and th e strong convexity of uMY,ǫ .
Sinceµβis the invariant measure for ( 22), we know that xthas law equal to µβfor every
t≥0. So by ( 22),
E|h(xt)−hMY,ǫ(xt)|2≤ǫ224l+4L2E(1+|xt|+√
R)4l+4≤Cl,L,Rǫ2,
whereCl,L,R is derived by Proposition 3.6. As a result,
Ed
dt|xt−yt|2≤ −m
2E|xt−yt|2+cǫ2,
and since the right hand side is ﬁnite one can exchange deriva tive and expectation to obtain
d
dtE|xt−yt|2≤ −m
2E|xt−yt|2+cǫ2.
Settingf(t) :=E|xt−yt|2, by a simple calculations one obtains
(em
2tf(t)−2c
mǫ2em
2t)′≤0,
which implies by the fundamental theorem of calculus that
em
2tf(t)−2c
mǫ2em
2t≤f(0)−2c
mǫ2=−2c
mǫ2,
and thus one concludes that
E|xt−yt|2≤2c
mǫ2, t≥0. (24)
11Then asxtfollows the law of the invariant measure, one has the bound
W2(yt,µβ)≤/radicalbig
E|xt−yt|2≤/radicalbigg
2c
mǫ,
and as a result,
W2(µβ,µǫ
β)≤W2(yt,µǫ
β)+/radicalbigg
2c
mǫ.
Sinceytconverges in W2to its invariant measure, letting t→ ∞ yields the result. Noticing
thatW2(πβ,πǫ
β)≤W2(µǫ
β,µβ)and setting ¯c=/radicalBig
2c
mthen completes the proof.
5 New Euler-Krylov Polygonal (Tamed) Scheme
Our goal is to construct a stable and efﬁcient sampling algor ithm with which to obtain ap-
proximate samples from πǫ
β(and therefore essentially from πβ), given that the log-gradient
of the density is of the class described by Assumption 2.2. A natural step would be to use a
discretised version of ( 3) with the Moreau Yosida gradient hMY,ǫ in place of ∇u. However,
such an algorithm would require additional computation in o rder to calculate the Moreau
Yosida gradient at each iteration, dramatically increasin g its computational complexity.
Instead we use a new Euler-Krylov polygonal scheme with drif t coefﬁcient htam,γ
depending on γ. This new function has linear growth and additionally satis ﬁes a dissi-
pativity condition that is crucial for proving uniform mome nt estimates for the algorithm
inλ. In this section we prove these growth and dissipativity pro perties and additionally
demonstrate its convergence to the original gradient.
Deﬁnition 5.1. Let
f(x) =h(x)−m
2x
We deﬁneftam,γ in the following way:
ftam,γ(x) =f(x) if|f(x)| ≤√γ
ftam,γ(x) =2f(x)
1+γ−1
2|f(x)|if|f(x)|>√γ.
Then,
htam,γ=ftam,γ+m
2x.
Lemma 5.2. There holds,
∝an}⌊∇a⌋ketle{thtam,γ(x),x∝an}⌊∇a⌋ket∇i}ht ≥m
2|x|2−u(0)
Proof. The proof begins by writing
∝an}⌊∇a⌋ketle{thtam,γ(x),x∝an}⌊∇a⌋ket∇i}ht=∝an}⌊∇a⌋ketle{tftam,γ(x),x∝an}⌊∇a⌋ket∇i}ht+m
2|x|2.
We split the proof in two parts depending on the sign of ∝an}⌊∇a⌋ketle{tf(x),x∝an}⌊∇a⌋ket∇i}ht.
If∝an}⌊∇a⌋ketle{tf(x),x∝an}⌊∇a⌋ket∇i}ht ≥0then, it is easy to see that ∝an}⌊∇a⌋ketle{tftam,γ(x),x∝an}⌊∇a⌋ket∇i}ht ≥0so
∝an}⌊∇a⌋ketle{thtam,γ(x),x∝an}⌊∇a⌋ket∇i}ht ≥m
2|x|2. (25)
12Alternatively if ∝an}⌊∇a⌋ketle{tf(x),x∝an}⌊∇a⌋ket∇i}ht<0then, noticing that
|∝an}⌊∇a⌋ketle{tftam,γ(x),x∝an}⌊∇a⌋ket∇i}ht| ≤ |∝an}⌊∇a⌋ketle{tf(x),x∝an}⌊∇a⌋ket∇i}ht|,
there follows that
∝an}⌊∇a⌋ketle{tftam,γ(x),x∝an}⌊∇a⌋ket∇i}ht ≥ −|∝an}⌊∇a⌋ketle{tf(x),x∝an}⌊∇a⌋ket∇i}ht|=∝an}⌊∇a⌋ketle{tf(x),x∝an}⌊∇a⌋ket∇i}ht.
This leads to
∝an}⌊∇a⌋ketle{thtam,γ(x),x∝an}⌊∇a⌋ket∇i}ht ≥ ∝an}⌊∇a⌋ketle{tf(x),x∝an}⌊∇a⌋ket∇i}ht+m
2|x|2
=∝an}⌊∇a⌋ketle{th(x),x∝an}⌊∇a⌋ket∇i}ht −m
2|x|2+m
2|x|2
=∝an}⌊∇a⌋ketle{th(x),x∝an}⌊∇a⌋ket∇i}ht
≥m
2|x|2−u(0).(26)
Combining ( 25) and ( 26) yields the result.
Lemma 5.3. There holds
|htam,γ(x)| ≤2√γ+m
2|x|
Proof. If|f(x)| ≤√γthen,
|htam,γ(x)|=|f(x)+m
2x| ≤√γ+m
2|x|. (27)
On the other hand, if |f(x)|>√γthen,
|htam,γ(x)| ≤ |ftam,γ(x)|+m
2|x| ≤2√γ+m
2|x| (28)
Combining ( 27) and ( 28) yields the result.
Lemma 5.4. Letp >0and a random variable Xwith ﬁnite (4p+ 4)(l+ 1) moments.
Then,
E|htam,γ(X)−h(X)|2≤cγ−p
wherec= 22p+2(L+m
2)2p+2/radicalbig
E|X|(4p+4)(l+1)+22p+2|h(0)|2p+2
Proof. By using the deﬁnition of htam,γ
|htam,γ(X)−h(X)|2
=|ftam,γ(X)−f(X)|2
=|f(X)|2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleγ−1
2|f(X)|−1
1+γ−1
2|f(X)|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1{|f(X)|≥√γ}
≤ |f(X)|21{|f(X)|≥√γ}
Taking expectations and using Cauchy Swartz inequality one obtains,
E|htam,γ(X)−h(X)|2≤/radicalbig
E|f(X)|4/radicalBig
P(|f(X)| ≥√γ)
13and then, an application of Markov’s inequality yields
E|htam,γ(X)−h(X)|2≤/radicalbig
E|f(X)|4/radicalbig
E|f(X)|4pγ−p=/radicalbig
E|f(X)|4p+4γ−p.
Noticing that
|f(x)| ≤ |h(x)|+m
2|x| ≤ |h(0)|+L(1+|x|)l+1+m
2|x|
yields the result.
6 Sampling with tKLMC1 Algorithm
We are now ready to construct the tKLMC1 algorithm by using th e gradient term htam,γ
as the drift coefﬁcient. The new algorithm has the initial co ndition(¯θλ
0,¯V0λ) = (θ0,V0)
and is a type of Euler-scheme given by the recursion
¯Vλ
n+1=¯Vλ
n−λ/bracketleftbig
γ¯Vλ
n+htam,γ/parenleftbig¯θλ
n/parenrightbig/bracketrightbig
+/radicalBig
2γλ
βξn+1,
¯θλ
n+1=¯θλ
n+λ¯Vλ
n, n≥0,(29)
whereλ >0is the step-size and {ξn}n≥1ared-dimensional independent standard Gaus-
sian random variables. Moreover, it is assumed that θ0,V0andξnare independent.
Let us now introduce additional auxiliary processes that pl ay an important role in our
analysis. Consider the scaled process (ζλ,n
t,Zλ,n
t) := (˜θλt,˜Vλt)where(˜θt,˜Vt)are given
in (3). Additionally, deﬁne
dRλ
t=−λ/parenleftbig
γRλ
t+hMY,ǫ/parenleftbig
rλ
t/parenrightbig/parenrightbig
dt+/radicalbig
2γλβ−1dBλ
t,
drλ
t=λRλ
tdt,(30)
whereBλ
t:=1√
λBλt. The new Brownian motion is adapted to its natural ﬁltration Fλ:=
{Fλ
t}t≥0which is independent of σ(θ0,V0). In addition, we deﬁne the continuous-time
interpolation of the algorithm as
dVλ
t=−λ/parenleftBig
γVλ
⌊t⌋+htam,γ(θλ
⌊t⌋)/parenrightBig
dt+/radicalbig
2γλβ−1dBλ
t,
dθλ
t=λVλ
⌊t⌋dt, t≥0,(31)
with initial condition Vλ
0=V0andθλ
0=θ0. Note that L(θλ
⌊t⌋,Vλ
⌊t⌋) =L(¯θλ
⌊t⌋,¯Vλ
⌊t⌋).
Finally, we deﬁne the underdamped Langevin process/parenleftBig
/hatwideζs,u,v,λ
t,/hatwideZs,u,v,λ
t/parenrightBig
fors≤t
d/hatwideZs,u,v,λ
t=−λ/parenleftBig
γ/hatwideZs,u,v,λ
t+hMY,ǫ/parenleftBig
/hatwideζs,u,v,λ
t/parenrightBig/parenrightBig
dt+/radicalbig
2γλβ−1dBλ
t,
d/hatwideζs,u,v,λ
t=λ/hatwideZs,u,v,λ
tdt,
with initial conditions /hatwideθs,u,v,λ
s=uand/hatwideVs,u,v,λ
s=v.
14Deﬁnition 6.1. Fixn∈Nand letT:=⌊1/λ⌋. Then we deﬁne
ζλ,n
t:=/hatwideζnT,¯θλ
nT,¯Vλ
nT,λ
t ,andZλ,n
t:=/hatwideZnT,¯θλ
nT,¯Vλ
nT,λ
t ,
such that the process/parenleftBig
ζλ,n
t,Zλ,n
t/parenrightBig
t≥nTis an underdamped Langevin process started at
timenTwith initial conditions/parenleftbig
θλ
nT,Vλ
nT/parenrightbig
.
Throughout our analysis we assume that
γmin,1= max{/radicalBigg
K+m
β,K,32,48(2m+1)2
m}andλmax,1=γ−1
min,1 (32)
6.1 Moment bounds (tKLMC1)
In order to proceed with the convergence properties of the al gorithm ( 29) we ﬁrst need
some high moment estimates. Proofs are postponed to the Appe ndix.
Lemma 6.2. Forλ<γ−1<m
48(2m+1)2one has
sup
nE|¯θλ
n|2≤¯C2,
and
sup
nE|¯Vλ
n|2≤¯B2,
where the constants ¯C2¯B2are independent of γ, have dependence on the dimension at
mostO(d
β)and are given explicitly in the proof.
Proof. Postponed to the Appendix
Lemma 6.3. Forλ<γ−1<m
48(2m+1)2there holds
sup
nE|¯θλ
n|2q≤¯C2q,
where¯C2q=O((d
β)q)and is independent of γ.
Proof. Postponed to the Appendix.
We conclude this section by presenting the one-step errors f or the continuous interpo-
lation of the algorithm.
Lemma 6.4. For everyt≥0one has the bound
E|θλ
t−θλ
⌊t⌋|2≤λ¯B2,
and
E|Vλ
⌊t⌋−Vλ
t|2≤λγC1,v,
whereC1,v= max{˜C2,16d
β}where¯C2and¯B2are given explicitly in the proof of Lemma
6.2.
15Proof. Postponed to the appendix.
Lemma 6.5 ([14] Lemma 16 (i) ) .LetU∈ C1(Rd)be a function satisfying
1.U≥0
2.∇Uis Lipschitz
3. There exist a,b>0such that
∝an}⌊∇a⌋ketle{tx,∇U(x)∝an}⌊∇a⌋ket∇i}ht ≥a|x|2−b, x∈Rd.
Furthermore, consider the following underdamped Langevin SDE{(Xt,Yt)}t≥0
dYt=−γYtdt−λ∇U(Xt)dt+/radicalBigg
2λγ
βdBt,
dXt=λYtdt, t> 0.
with initial condition (X0,V0)such that
E|X0|2+E|Y0|2<∞.
Then one has the bounds,
sup
t≥0E|Xt|2<∞,
and
sup
t≥0E|Yt|2<∞.
Corollary 6.6. Let Assumptions 2.1,2.3hold. Then for 0<λ<λ max one has
sup
nsup
t≥nTE|ζλ,n
t|2<∞,
and
sup
nsup
t≥nTE|Zλ,n
t|2<∞.
Proof. One notices that due to ( 18), (21), (17) the function uMY,ǫ satisﬁes the assumptions
of Lemma 6.5, the process (ζλ
⌊t⌋,Zλ,n
t)is the solution to an underdamped Langevin SDE
with initial condition (θλ
nT,Vλ
nT), and so by Lemma 6.2there holds that
E|θλ
nT|2+E|Vλ
nT|2≤sup
nE|¯θλ
n|2+sup
nE|¯Vλ
n|2<∞.
Applying the result of Lemma 6.5concludes the proof.
166.2 Convergence of the tKLMC1 algorithm
Before we proceed with convergence results, we present the f undamental contraction prop-
erty that is needed for the derivation of the main results.
Theorem 6.7 ( cf[10], Theorem 1) .Letǫ >0be such that uMY,ǫ∈ C2andmId≤
Hess(uMY,ǫ)≤KId. Letγ≥/radicalBig
m+K
β. Then, ifRrλ
tandRrλ′
tare two solutions of the
Langevin SDE (30)with initial condition Rr0= (R0,r0)andRr′
0= (R′
0,r′
0)respectively,
one has the bound
W2(L(Rrλ
t),L(Rrλ′
t))≤√
2exp(−λm
βγt)W2(L(Rr0),L(Rr′
0)).
The following corollary follows immediately from the previ ous result.
Corollary 6.8. Let2.1-2.3hold. Let (rλ
t,Rλ
t)be the underdamped Langevin SDE as in
(30)with initial condition (θ0,V0). There holds
W2(L(Rλ
t,rλ
t),πǫ
β)≤√
2exp(−λm
βγt)W2(L(θ0,V0),πǫ
β).
In order to prove the main convergence result, one uses the fo llowing splitting:
Wn((L(¯Vt,¯θt),πβ)≤W2(L(¯Vt,¯θt),L(Zλ,n
t,ζλ,n
t))+W2(L(Zλ,n
t,ζλ,n
t),L(Rλ
t,rλ
t))
+W2(L(Rλ
t,rλ
t),πǫ
β)+W2(πǫ
β,πβ). (33)
In order to bound W2(L(¯Vt,¯θt),L(Zλ,n
t,ζλ,n
t))the following Lemma is required, which
demonstrates the good approximating properties of the tame d scheme. From now on we
assume that γ−1≤min{ǫ,1
32,m
48(2m+1)2}andλ<γ−1.
Lemma 6.9. Let Assumptions 2.1-2.3hold andq >0. Then, by Lemma 5.4wherep=
2q+1and Lemma 6.3, there follows
E|htam,γ(θλ
⌊s⌋)−h(θλ
⌊s⌋)|2≤CA
16γ−(2q+1),
whereCA:=O(2(2(l+2)+2q(2l+2))¯C2(l+2)+2q(2l+2)).
Proposition 6.10. Let Assumptions 2.1-2.3hold. Then, for every n∈Nand fornT≤
t≤(n+1)Tone has
W2/parenleftBig
L(Vλ
t,θλ
t),L(Zλ,n
t,ζλ,n
t)/parenrightBig
≤C/parenleftBigg
√
λ√γ+/radicalBigg
Sλ,γ
γ/parenrightBigg
,
whereC:=/radicalbig
6(1+C1,v)whereC1,vis given in Lemma 6.4andSλ,γis given by (56).
Theorem 6.11. Let Assumptions 2.1-2.3hold and let (rλ
t,Rλ
t)be the solution to the un-
derdamped Langevin SDE (30)with initial condition (θ0,V0). Then for every n∈N,
nT≤t≤(n+1)T
W2(L(ζλ,n
t,Zλ,n
t),L(rλ
t,Rλ
t))≤C′/parenleftBig√
λγ3
2+√γ/radicalbig
Sλ,γ/parenrightBig
whereC′is given explicitly in the proof and Sλ,γis given by (56).
177 Sampling with the tKLMC2 algorithm
In this section we propose a tamed version of KLMC2, an algori thm which was ﬁrst de-
veloped in [ 8] and analysed in depth in [ 10] and [ 14] under the assumption of Lipschitz
continuity for the gradient. The creation of this algorithm is motivated by the fact that ( 3),
after applying Itô’s formula to the product eγtVtand by following standard calculations,
can be rewritten as
¯Vt=e−γtV0−/integraldisplayt
0e−γ(t−s)∇u(¯θs)ds+/radicalbigg2γ
β/integraldisplayt
0e−γ(t−s)dBs
¯θt=θ0+/integraldisplayt
0¯Vsds.(34)
Since the class of functions satisfying Assumption 2.2allows for gradients with superlinear
growth, we shall tame the gradient part of the drift cofﬁcien t using the tamed technique
developed in Section 5. Our strategy to prove the moment bounds and convegence rate s is
similar to the one used for the proof of the respective result s for the tKLMC1 algorithm in
Section 6.
The new iterative scheme of tKLMC2 is given by
¯Yλ
n+1=ψ0(λ)¯Yλ
n−ψ1(λ)htam,γ(¯xλ
n)+/radicalbig
2γβ−1Ξn+1
¯xλ
n+1= ¯xλ
n+ψ1(λ)¯Yλ
n−ψ2(λ)htam,γ(¯xλ
n)+/radicalbig
2γβ−1Ξ′
n+1(35)
whereψ0(t) =e−γtandψi+1=/integraltextt
0ψi(s)dswhere/parenleftbig
Ξk+1,Ξ′
k+1/parenrightbig
is a2d-dimensional
centered Gaussian vector satisfying the following conditi ons:
• -/parenleftbig
Ξj,Ξ′
j/parenrightbig′s are iid and independent of the initial condition (V0,θ0),
• for any ﬁxed j, the random vectors/parenleftBig
(Ξj)1,/parenleftbig
Ξ′
j/parenrightbig
1/parenrightBig
,/parenleftBig
(Ξj)2,/parenleftbig
Ξ′
j/parenrightbig
2/parenrightBig
,...,/parenleftBig
(Ξj)d,/parenleftbig
Ξ′
j/parenrightbig
d/parenrightBig
are iid with the covariance matrix
C=/integraldisplayλ
0[ψ0(t),ψ1(t)]⊤[ψ0(t),ψ1(t)]dt.
At this point and in view of ( 34), one claims that the continuous time interpolation of ( 35)
is given by
˜Yt=e−γ(t−nλ)˜Ynλ−/integraldisplayt
nλe−γ(t−s)htam,γ(˜xnλ)ds+/radicalbig
2γβ−1/integraldisplayt
nλe−γ(t−s)dWs
˜xt= ˜xnλ+/integraldisplayt
nλ˜Ysds,(36)
which naturally leads to the following Lemma.
Lemma 7.1. Letλ>0,(¯Yλ
n,¯xλ
n)be given by (35)and(˜Ynλ,˜xnλ)be given by (36). Then,
L(¯Yλ
n,¯xλ
n) =L(˜Ynλ,˜xnλ),∀n∈N.
The details of the proof can be found in the proof section.
18Deﬁnition 7.2. For everyt∈[nλ,(n+ 1)λ]we deﬁne the auxiliary process such that
(Qnλ,pnλ) = (˜Ynλ,˜xnλ)
dQλ,n
t=−γQλ,n
t−hMY,ǫ(pλ,n
t)+/radicalbig
2γβ−1dWt
dpλ,n
t=Qλ,n
t(37)
or alternatively
Qλ,n
t=e−γ(t−j)λ)Qλ,n
nλ−/integraldisplayt
nλe−γ(t−s)hMY,ǫ(pλ,n
s)ds+/radicalbig
2γβ−1/integraldisplayt
nλe−γ(t−s)dWs
pλ,n
t=pλ,n
nλ+/integraldisplayt
nλQλ,n
sds.
whereWtis the same Brownian motion as in (36). Throughout our analysis we assume
that
γmin=/radicalBigg
2K
βandλmax=1
C′6γ−5. (38)
whereC′
6is given in the proof section.
7.1 Moment bounds(tKLMC2)
Lemma 7.3. Under Assumptions 2.1-2.3,λ<λmax,2,γ >γmin,2there holds,
E|¯xλ
n|2≤Cx
and
E|¯Yλ
n|2≤Cyγ2.
whereCx,Cyare given in the proof and depend on the d,β(their dependence is at most
O(d
β)) andE|V0|2+E|θ0|2and are also independent of γ.
Lemma 7.4. Under Assumptions 2.1-2.3,λ<λmax,2,γ >γmin,2there holds
E|¯xλ
n|2q≤/radicalbig
C4q.
7.2 Convergence of the KLMC2 algorithm
Lemma 7.5. Letn∈N. Under Assumptions 2.1-2.3,λ < λ max,2,γ > γ min,2for any
t∈[nλ,(n+1)λ],n∈Nthere holds
W2/parenleftBig
L(pλ,n
t,Qλ,n
t),L(¯xλ
t,¯Yλ
t)/parenrightBig
≤2Rλ,γ
whereRλ,γis given by (90).
Lemma 7.6. Let(Xt,Yt)the kinetic Langevin SDE with initial condition (θ0,V0).Let
n∈N. Under Assumptions 2.1-2.3,λ < λmax,2,γ > γmin,2for anyt∈[nλ,(n+1)λ],
there holds,
W2(L(pλ,n
t,Qλ,n
t),L(Xt,Yt))≤C′γ
λ/radicalbig
Rλ,γ
whereRλ,γis given by (90).
198 Solving the optimization problem
The main goal of this section is to build a process xnassociated with our algorithm that
can be used to minimize the excess risk optimization problem u(xn)−u(x∗).
To this end, let R0=/radicalBig
2u(0)
m+/radicalBig
(u(0)+1)d
β+1,E=¯B(0,R0)andE′=¯B(0,R0+1),
and letXn,Xbe random variables satisfying L(Xn) =L(¯θλ
n),L(X) =µβand
/radicalbig
E|Xn−X|2=W2(L(¯θλ
n),µβ).
Then we deﬁne xn:=¯θλ
n1E(¯θλ
n).
Lemma 8.1. Let Assumptions 2.1-2.3hold. Then,
Eu(xn)−Eu(X1E(Xn))≤L(1+2R0)l+1W2(L(¯θλ
n),µβ).
Lemma 8.2. Under Assumptions 2.1-2.3there holds
Eu(X1E(Xn))−u(x∗)≤C′′((E|X|2l+4)1/2+1)W2(¯θλ
n,µβ)+CE|X−x∗|2+2u(0)W2
2(¯θλ
n,µβ),
whereE|X|2l+4can be controlled using Proposition 3.4.
Lemma 8.3. Under Assumptions 2.1,2.2one has
E|X−x∗|2≤2d
mβ.
Corollary 8.4. Under Assumptions 2.1,2.2there holds
Eu/parenleftbig¯θλ
n1E(¯θλ
n)/parenrightbig
−u(x∗)≤C′W2(L(¯θλ
n,¯Vλ
n),πβ)+2u(0)W2(L(¯θλ
n,¯Vλ
n),πβ)+2d
mβ,
where the Wasserstein distance can be bounded by Theorem 2.6.
20A Proofs of section 6
For the rest of the section
Mn=γ2
4|¯θλ
n+γ−1¯Vλ
n|2+1
4|¯Vλ
n|2−rγ2
4|¯θλ
n|2.
Proof of Lemma 6.2.Let
∆n=¯θλ
n+γ−1¯Vλ
n−λ
γhtam,γ(¯θλ
n),
and
En=¯Vλ
n−λγ¯Vλ
n−λhtam,γ(¯θλ
n).
First observe that
|¯θλ
n+1+γ−1¯Vλ
n+1|2=|∆n|2+2/radicalBigg
2λ
γβ∝an}⌊∇a⌋ketle{t∆n,ξn+1∝an}⌊∇a⌋ket∇i}ht+2λ
γβ|ξn+1|2.
Furthermore, there holds
|∆n|2=|¯θλ
n+γ−1¯Vλ
n|2−2λγ−1∝an}⌊∇a⌋ketle{t¯θλ
n+γ−1¯Vλ
n,htam,γ(¯θλ
n)∝an}⌊∇a⌋ket∇i}ht+λ2γ−2|htam,γ(¯θλ
n)|2
=|¯θλ
n+γ−1¯Vλ
n|2−2λγ−2∝an}⌊∇a⌋ketle{t¯Vλ
n,htam,γ(¯θλ
n)∝an}⌊∇a⌋ket∇i}ht
−2λγ−1∝an}⌊∇a⌋ketle{t¯θλ
n,htam,γ(¯θλ
n)∝an}⌊∇a⌋ket∇i}ht+λ2γ−2|htam,γ(¯θλ
n)|2
≤ |¯θλ
n+γ−1¯Vλ
n|2+2λγ−2|¯Vλ
n||htam,γ(¯θλ
n)|
−2λγ−1∝an}⌊∇a⌋ketle{t¯θλ
n,htam,γ(¯θλ
n)∝an}⌊∇a⌋ket∇i}ht+λ2γ−2|htam,γ(¯θλ
n)|2
≤ |¯θλ
n+γ−1¯Vλ
n|2+λγ−22
m|¯Vλ
n|2+λγ−2m
2|htam,γ(¯θλ
n)|2−2λγ−1A|¯θλ
n|2
+2λγ−1B+λ2γ−2|htam,γ(¯θλ
n)|2
where in the last step we used the dissipativity property of htam,γ given in Lemma 5.2.
This leads to
γ2
4/parenleftbig
|¯θn+1+γ−1Vn+1|2−|¯θλ
n+γ−1¯Vλ
n|2/parenrightbig
≤λ8
m|¯Vλ
n|2+λm
8|htam,γ(¯θλ
n)|2−λm
2γ|¯θλ
n|2
+λγB
2+λ21
4|htam,γ(¯θλ
n)|2
+γ2/radicalBigg
λ
2γβ∝an}⌊∇a⌋ketle{t∆n,ξn+1∝an}⌊∇a⌋ket∇i}ht+λγ
2β|ξn+1|2.
(39)
1
4(|¯Vλ
n+1|2−|¯Vλ
n|2)≤ −λγ
2|¯Vλ
n|2+λ2γ2
4|¯Vλ
n|2+λ
2|∝an}⌊∇a⌋ketle{t√γ¯Vλ
n,/radicalbig
γ−1htam,γ(¯θλ
n)∝an}⌊∇a⌋ket∇i}ht|+λ2
4|htam,γ(¯θλ
n)|2
+/radicalBigg
λγ
2β∝an}⌊∇a⌋ketle{tEn,ξn+1∝an}⌊∇a⌋ket∇i}ht+λγ
2β|ξn+1|2
≤ −λγ
2|¯Vλ
n|2+λ2γ2
4|¯Vλ
n|2+1
4λγ|¯Vλ
n|2+1
4λγ−1|htam,γ(¯θλ
n)|2
+λ2
4|htam,γ(¯θλ
n)|2+/radicalBigg
λγ
2β∝an}⌊∇a⌋ketle{tEn,ξn+1∝an}⌊∇a⌋ket∇i}ht+λγ
2β|ξn+1|2.
(40)
21Finally,
−γ2r
4(|¯θλ
n+1|2−|¯θλ
n|2) =−λγ2r
2∝an}⌊∇a⌋ketle{t¯θλ
n,¯Vλ
n∝an}⌊∇a⌋ket∇i}ht−λ2γ2r
4|¯Vλ
n|2.
Observing that
−γ
2∝an}⌊∇a⌋ketle{t¯θλ
n,¯Vλ
n∝an}⌊∇a⌋ket∇i}ht ≤ −Mn+γ2
4|¯θλ
n|2+1
2|¯Vλ
n|2,
yields
−γ2r
4(|¯θλ
n+1|2−|¯θλ
n|2)≤ −λrγMn+λrγ3|¯θλ
n|2+λrγ1
2|¯Vλ
n|2. (41)
Adding ( 39), (40) and ( 41) yields
Mn+1≤(1−λrγ)Mn+λ/parenleftbigg
λγ21
4+8
m+1
4+r
2γ−γ
2/parenrightbigg
|¯Vλ
n|2
+λ/parenleftBig
−mγ
4+rγ3/parenrightBig
|¯θλ
n|2
+λ/parenleftbiggλ
2+m
8+λ
2/parenrightbigg
|htam,γ(¯θλ
n)|2
+γ2/radicalBigg
λ
2γβ∝an}⌊∇a⌋ketle{t∆n,ξn+1∝an}⌊∇a⌋ket∇i}ht+/radicalBigg
λγ
2β∝an}⌊∇a⌋ketle{tEn,ξn+1∝an}⌊∇a⌋ket∇i}ht+3λγ
β|ξn+1|2+λγB
2
≤(1−λrγ)Mn+λ/parenleftbigg
λγ21
4+8
m+1
4+r
2γ−γ
2/parenrightbigg
|¯Vλ
n|2
+λ/parenleftbigg
−mγ
4+rγ3+2λm2
4+m3
8/parenrightbigg
|¯θλ
n|2
+γ2/radicalBigg
λ
2γβ∝an}⌊∇a⌋ketle{t∆n,ξn+1∝an}⌊∇a⌋ket∇i}ht+/radicalBigg
λγ
2β∝an}⌊∇a⌋ketle{tEn,ξn+1∝an}⌊∇a⌋ket∇i}ht+3λγ
β|ξn+1|2+λγB
2+4λ2γ+m
2λγ
≤(1−λγr)Mn+Kn.
(42)
where
Kn=γ2/radicalBigg
λ
2γβ∝an}⌊∇a⌋ketle{t∆n,ξn+1∝an}⌊∇a⌋ket∇i}ht+/radicalBigg
λγ
2β∝an}⌊∇a⌋ketle{tEn,ξn+1∝an}⌊∇a⌋ket∇i}ht+3λγ
β|ξn+1|2+λγB
2+4λ2γ+m
2λγ
(43)
Settingr=m
8γ−2forγ >8
m+1
4+m
8γ−1+λγ2andγ >8
m(2λm2
4+m3
16)one obtains
E[Mn+1|Mn]≤(1−λm
8γ−1)Mn+λγ(3
βd+4λ+u(0)
2+m
2) (44)
As a result,
EMn+1≤(1−λγ−1m
8)nEM(0)+γ28
m/parenleftbigg3
βd+4+u(0)
2+m
2/parenrightbigg
.
Since
Mn≥max/braceleftbigg1
8(1−2r)γ2|¯θλ
n|2,1
4(1−2r)|¯Vλ
n|2/bracerightbigg
, (45)
22and
EM(0) =O(γ2),
then,
sup
nE|¯θλ
n|2≤¯C2, (46)
where¯C2= 4E|θ0|2+ 6E|V0|2+64
m/parenleftBig
3
βd+4+u(0)
2+m
2/parenrightBig
.We conclude the proof by
ﬁnding a better bound for the moments of ¯Vλ
n. There holds
E|¯Vλ
n+1|2≤(1−λγ)E|¯Vλ
n|2−2(1−λγ)λE∝an}⌊∇a⌋ketle{t¯Vλ
n,htam,γ(¯θλ
n)∝an}⌊∇a⌋ket∇i}ht+λ2E|htam,γ(¯θλ
n)|2+2λγ
βd
≤(1−λγ)E|¯Vλ
n|2+1
2λγE|¯Vλ
n|2+2λγ−1E|htam,γ(¯θλ
n)|2+λ2E|htam,γ(¯θλ
n)|2+2λγ
βd
≤(1−λγ
2)E|¯Vλ
n|2+4λm2
4¯C2+8+λ2(m2+4γ¯C2)+2λγ
βd.
where the last step is derived from the growth of htam,γ and ( 46). Afterniterations one
obtains,
E|¯Vλ
n+1|2≤(1−λγ
2)nE|V0|2+2m2¯C2+16+4m2+8¯C2+4
βd. (47)
so one concludes that
sup
nE|¯Vλ
n|2≤¯B2
where¯B2=E|V0|2+2m2¯C2+16+4m2+8¯C2+4
βd.
Proof of Lemma 6.3.Let
fn=γ2/radicalBigg
λ
2γβ∝an}⌊∇a⌋ketle{t∆n,ξn+1∝an}⌊∇a⌋ket∇i}ht+/radicalBigg
λγ
2β∝an}⌊∇a⌋ketle{tEn,ξn+1∝an}⌊∇a⌋ket∇i}ht,
and setgn:=Kn−fnwhereKnis given by ( 43) ands= 1−λm
8γ. We also deﬁne
En(·) =E[·|Mn]. Taking conditional expectations in ( 42) yields
En(M2q
n+1)≤ |sMn|2q+2q|sMn|2(q−1)En(sMnKn)+2q/summationdisplay
i=2Ci
2qEn[|s¯Vλ
n|2q−i|Kn|i]
≤ |sMn|2q+2q|sMn|2q−1Egn+2q−2/summationdisplay
i=0/parenleftbigg
2q
i+2/parenrightbigg
En/bracketleftbig
|sMn|2q−2−i|Kn|l|Kn|2/bracketrightbig
=|sMn|2q+2q|sMn|2q−1λγC2+(2q
2)2q−2/summationdisplay
i=0(2q−2
i)Ci
2q−2En/bracketleftbig
|sMn|2q−2−i|Kn|l+2/bracketrightbig
≤ |sMn|2q+2q|sMn|2q−1λγC2+q(2q−1)En/bracketleftbig
(|sMn|+|Kn|)2q−2|Kn|2/bracketrightbig
≤s|Mn|2q+2q|Mn|2q−1λγC2
+q(2q−1)22q−3|Mn|2q−2En|Kn|2+q(2q−1)22q−3En|Kn|2q.
(48)
23We proceed by bounding the moments of Kn. Firstly we calculate that
|fn|2≤2γ3β−1λ|∆n|2|ξn+1|2+λγβ−1|En|2|ξn+1|2
≤2γ3β−1λ|ξn+1|2|¯θλ
n+γ−1¯Vλ
n−λγ−1htam,γ(¯θλ
n)|2)
+λγβ−1|ξn+1|2|(1−λγ)¯Vλ
n−λhtam,γ(¯θλ
n)|2
≤6γ3β−1λ|ξn+1|2/parenleftbigg
|¯θλ
n|2+γ−2|¯Vλ
n|2+1
2λ2γ−1(m2|¯θλ
n|2+4γ)/parenrightbigg
+λγβ−1|ξn+1|2/parenleftbigg
(1−λγ)2|¯Vλ
n|2+1
2λ2(m2|¯θλ
n|2+4γ)/parenrightbigg
≤C3β−1γ(Mn+1)λ|ξn+1|2,
ince
|Kn|2≤2|fn|2+2|gn|2,
then,
En|Kn|2≤2C3dβ−1λγMn+λγC4,
whereC4= 6(C′
0+d
β)and
En|Kn|2q≤22q−1En|fn|2q+22q−1En|gn|2q
≤22q−1Cq
3β−qλqγq|Mn|qE|ξn+1|2q+λ2qγ2qC5E|ξn+1|4q
≤λγC6(|Mn|q+1),
forC6=Cq
322q(d
β)q. Substituting the moments of Kninto ( 48) one obtains
En|Mn+1|2q≤(1−λ
γm
32)|Mn|2q+/parenleftbigg
2qλγ(C2+C3d)|Mn|2q−1−λ
γm
32|Mn|2q/parenrightbigg
+/parenleftbigg
q(2q−1)22q−2λγC4|Mn|2q−2−λ
γm
32|Mn|2q/parenrightbigg
+/parenleftbigg
q(2q−1)22q−3λγC6|Mn|q−λ
γm
3|Mn|2q/parenrightbigg
+λγC6q(2q−1)22q−3.
As a result there exists Nindependent of γgiven by
N=/parenleftbigg32
m/parenrightbigg2q−1
(2q(C2+C3d))2q+((32
m)q−1q(2q−1)22q−2C4)q+(32
m)q(q(2q−1)22q−3C6)q+1
+C6q(2q−1)22q−3,
(49)
such that
En|Mn+1|2q≤(1−λm
32γ−1)|Mn|2q+Nλγ4q−1,
which implies
sup
nE|Mn|2q≤E|M0|2q+32N
mγ4q.
Applying ( 45) one concludes that
E|¯θλ
n|4q≤82q1
γ4q(1
1−2r)2qE|Mn|2q≤¯C4q,
24so that
E|¯θλ
n|2q≤/radicalBig
¯C4q.
Proof of Lemma 6.4.Since for every t≥0
Vλ
t=Vλ
⌊t⌋−λγ/integraldisplayt
⌊t⌋Vλ
⌊s⌋ds−λ/integraldisplayt
⌊t⌋htam,γ(θλ
⌊s⌋)ds+/radicalbig
2γλβ−1(Bλ
t−Bλ
⌊t⌋).
one has that
E|Vλ
t−V⌊t⌋|2≤8λ2γ2E|/integraldisplayt
⌊t⌋Vλ
⌊s⌋ds|2+8λ2E|/integraldisplayt
⌊t⌋htam,γ(θλ
⌊s⌋)ds|2+16λγ
βE|B⌊t⌋−Bt|2
≤8λ2γ2/integraldisplayt
⌊t⌋E|Vλ
⌊s⌋|2ds+8λ2/integraldisplayt
⌊t⌋E/vextendsingle/vextendsinglehtam,γ(θλ
⌊s⌋)/vextendsingle/vextendsingle2ds+16λγ
βd
≤8λ2γ2/integraldisplayt
⌊t⌋E|Vλ
⌊s⌋|2ds+8λ2(A/integraldisplayt
⌊t⌋E|θλ
⌊s⌋|2ds+4γ)+16λγd
β.
(50)
Then, noting that the law of the continuous interpolation ag rees with the algorithm at grid
points, the moment bounds of the previous section yield
E|Vλ
⌊s⌋|2≤¯B2,
and
E|θλ
⌊s⌋|2≤¯C2,
so (50) yields
E|Vλ
t−Vλ
⌊t⌋|2≤C1,vλγ.
In addition, it easy to see that
θλ
t=θλ
⌊t⌋+λ/integraldisplayt
⌊t⌋Vλ
⌊s⌋ds,
so that
E|θλ
t−θλ
⌊t⌋|2≤λ¯B2.
25B Proofs of convergence results
Proof of Theorem 6.10.Firstly one calculates
d
dsE|Vλ
t−Zλ,n
t|2=−2λγE∝an}⌊∇a⌋ketle{tVλ
t−Zλ,n
t,Vλ
⌊t⌋−Zλ,n
t∝an}⌊∇a⌋ket∇i}ht
−2λE∝an}⌊∇a⌋ketle{tVλ
t−Zλ,n
t,htam,γ(θλ
⌊t⌋)−hMY,ǫ(ζλ,n
t)∝an}⌊∇a⌋ket∇i}ht
=−2λγE∝an}⌊∇a⌋ketle{tVλ
t−Zλ,n
t,Vλ
⌊t⌋−Vλ
t∝an}⌊∇a⌋ket∇i}ht−2λγE|Vλ
t−Zλ,n
t|2
−2λE∝an}⌊∇a⌋ketle{tVλ
t−Zλ,n
t,htam,γ(θλ
⌊t⌋)−hMY,ǫ(ζλ,n
t)∝an}⌊∇a⌋ket∇i}ht
≤λγ
2E|Vλ
t−Zλ,n
t|2+2λγE|Vλ
⌊t⌋−Vλ
t|2−2λγE|Vλ
t−Zλ,n
t|2
+λγ
2E|Vλ
t−Zλ,n
t|2+2λγ−1E|htam,γ(θλ
⌊t⌋)−hMY,ǫ(ζλ,n
t)|2
≤ −λγE|Vλ
t−Zλ,n
t|2+2λγE|Vλ
⌊t⌋−Vλ
t|2
+2λγ−1E|htam,γ(θλ
⌊t⌋)−hMY,ǫ(ζλ,n
t)|2
≤2λγ−1E|htam,γ(θλ
⌊t⌋)−hMY,ǫ(ζλ,n
t)|2+2λγ2C1,v−λγE|Vλ
t−Zλ,n
t|2
≤λ(16γ−1E|htam,γ(θλ
⌊t⌋)−h(θλ
⌊t⌋)|2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
At
+16γ−1E|h(θλ
⌊t⌋)−hMY,ǫ(θλ
⌊t⌋)|2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B′
t
+16γ−1E|hMY,ǫ(θλ
⌊t⌋)−hMY,ǫ(θλ
t)|2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Ct
+16γ−1E|hMY,ǫ(θλ
t)−hMY,ǫ(ζλ,n
t)|2
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Dt
+2λγ2C1,v)−λγE|Vλ
t−Zλ,n
t|2
≤λ/parenleftbig
At+B′
t+Ct+Dt+2λγ2C1,v/parenrightbig
−λγE|Vλ
t−Zλ,n
t|2.
(51)
Then using Lemma 6.9it is clear that
At=CAγ−(2q+1). (52)
In addition, since hMY,ǫ isK≤1
ǫ-Lipschitz and by Lemma 6.4it holds that
Ct≤γ−1K2λ¯B2, (53)
and
Dt≤16λγ−1K2E|θλ
t−ζλ,n
t|2≤1
4λγE|θλ
t−ζλ,n
t|2. (54)
Additionally, by Lemma 4.5one has that
|h(x)−hMY,ǫ(x)|2≤24l+4L2(1+2|x|+√
R)4l+4ǫ2,
and as a result,
B′
t≤CB′γ−1ǫ2, (55)
26whereCB′= 28l+8L2/parenleftBig
(1+√
R)4l+4+¯C4l+4/parenrightBig
. Now let
Sλ,γ=CAγ−(2q+1)+λγ−1K2¯B2+CB′γ−1ǫ2+C1,vλγ2, (56)
so that bringing ( 52), (55), (53), (54) together yields
d
dtE|Vλ
t−Zλ,n
t|2≤ −λγE|Vλ
t−Zλ,n
t|2+λSλ,γ+1
4λγE|θλ
t−ζλ,n
t|2. (57)
Furthermore, by Lemma 6.6
sup
t≥nTE|ζλ,n
t|2<∞,
and so by the bound
E|θλ
t−ζλ,n
t|2≤4E|ζλ,n
t|2+4E|θλ
⌊t⌋|2+4E|θλ
t−θλ
⌊t⌋|2,
one has,
sup
nT≤t≤(n+1)TE|θλ
t−ζλ,n
t|2<∞.
Then one can deﬁne J:= supnT≤t≤(n+1)TE|θλ
t−ζλ,n
t|2so that ( 57) becomes
d
dtE|Vλ
t−Zλ,n
t|2≤ −λγE|Vλ
t−Zλ,n
t|2+λSλ,γ+λγJ
4,
which implies/parenleftbigg
eλγtE|Vλ
t−Zλ,n
t|2−eλγt(Sλ,γ
γ+J
4/parenrightbigg′
≤0,
so by the fundamental theorem of calculus
eλγtE|Vλ
t−Zλ,n
t|2−eλγt(Sλ,γ
γ+J
4)≤ −eλγnT(Sλ,γ
γ+J
4),
which leads to
E|Vλ
t−Zλ,n
t|2≤Sλ,γ
γ+J
4t∈[nT,(n+1)T]. (58)
On the other hand, one notices that for every t∈[nT,(n+1)T]
E|θλ
t−ζλ,n
t|2≤λ2E/parenleftbigg/integraldisplayt
nT|Vλ
⌊s⌋−Zλ,n
s|ds/parenrightbigg2
≤λ/integraldisplayt
nTE|Vλ
⌊s⌋−Zλ,n
s|2ds
≤2λ/integraldisplayt
nTE|Vλ
s−Zλ,n
s|2ds+2λ/integraldisplayt
nTE|Vλ
s−Vλ
⌊s⌋|2ds
≤2λ/integraldisplayt
nTE|Vλ
s−Zλ,n
s|2ds+2C1,vλγ
≤2Sλ,γ
γ+J
2+2C1,vλγ.(59)
so that taking supremum in the inequality yields
J≤2Sλ,γ
γ+J
2+2C1,vλγ,
27which implies that
E|θλ
t−ζλ,n
t|2≤J≤4Sλ,γ
γ+4C1,vλγ t∈[nT,(n+1)T], (60)
so that substituting this back into ( 58) yields
E|Vλ
t−Zλ,n
t|2≤5
4Sλ,γ
γ+C1,vλ. (61)
Bringing ( 60) and ( 61) together, one concludes that
W2
2/parenleftBig
L(Vλ
t,θλ
t),L(Zλ,n
t,ζλ,n
t)/parenrightBig
≤6(C1,v+1)/parenleftbiggSλ,γ
γ+λγ/parenrightbigg
.
Proof of Theorem 6.11.Using the results obtained in Theorem 6.7and6.10 one calcu-
lates for every t∈[nT,(n+1)T]that
W2(L(ζλ,n
t,Zλ,n
t),L(rλ
t,Rλ
t))≤n/summationdisplay
k=1W2(L(ζλ,k
t,Zλ,k
t),L(ζλ,k−1
t,Zλ,k−1
t)
=n/summationdisplay
k=1W2(L(ˆζλ,kT,θλ
kT,VkT
t ,ˆZλ,kT,θλ
kT,VkT
t ),L(ˆζλ,kT,ζλ,k−1
kT,Zλ,k−1
kT,ˆZλ,kT,ζλ,k−1
kT,Zλ,k−1
kT))
≤n/summationdisplay
k=1√
2e−m
βγλ(t−kT)W2(L(θλ
kT,Vλ
kT),L(ζλ,k−1
kT,Zλ,k−1
kT)
≤√
2Cn/summationdisplay
k=1e−m
βγ(n−k)/parenleftBigg
√
λ√γ+/radicalBigg
Sλ,γ
γ/parenrightBigg
≤√
2C/parenleftBigg
√
λ√γ+/radicalBigg
Sλ,γ
γ/parenrightBigg
1
1−e−m
βγ
≤√
2C/parenleftBigg
√
λ√γ+/radicalBigg
Sλ,γ
γ/parenrightBigg
β
mγem
βγ
≤C′/parenleftBig√
λγ3
2+√γ/radicalbig
Sλ,γ/parenrightBig
,
Proof of Theorem 2.6.By the triangle inequality of Wasserstein distance and by Co rol-
laries 4.6,6.8, Lemma 6.9and Theorem 6.11, for everyt∈[nT,(n+ 1)T]there holds
28that
W2(L(Vλ
t,θλ
t),πβ)≤W2(L(Vλ
t,θλ
t),L(Zλ,n
t,Zλ,n
t)+W2(L(Zλ,n
t,Zλ,n
t),L(Rλ
t,rλ
t)
+W2(L(Rλ
t,rλ
t),πǫ
β)+W2(πǫ
β,πβ)
≤C/parenleftBigg/radicalBigg
Sλ,γ
γ+√
λ√γ/parenrightBigg
+C′/parenleftBig√
λγ3
2+√γ/radicalbig
Sλ,γ/parenrightBig
+√
2e−m
βγλnW2(L(θ0,V0),πǫ
β)+W2(πβ,πǫ
β)
≤C/parenleftBigg/radicalBigg
Sλ,γ
γ+√
λ√γ/parenrightBigg
+C′/parenleftBig√
λγ3
2+√γ/radicalbig
Sλ,γ/parenrightBig
+√
2e−m
βγλtW2(L(θ0,V0),πβ)+W2(πβ,πǫ
β)
≤6max{C,C′,√
2+/radicalbig
2c/m}/parenleftBig√
λγ3
2+√γ/radicalbig
Sλ,γ/parenrightBig
+√
2e−m
βγλtW2(L(θ0,V0),πβ)+ǫ
≤˙C/parenleftBig√
λγ3
2+γ−q/parenrightBig
+√
2e−m
βγλtW2(L(θ0,V0),πβ)+ǫ,
at which point the result follows from the fact that L/parenleftBig
¯Vλ
⌊t⌋,¯θλ
⌊t⌋/parenrightBig
=L/parenleftBig
Vλ
⌊t⌋,θλ
⌊t⌋/parenrightBig
.
C Proofs of Section 7
Proof of Lemma 7.1.For simplicity we prove that L(¯Yλ
1,¯xλ
1) =L(˜Yλ,˜Xλ).
Expanding ˜Yλone notices that
˜Yλ=e−λγ˜Y0−e−λγhtam,γ(˜x0)/integraldisplayλ
0eγsds+/radicalbig
2γβ−1e−λγ/integraldisplayλ
0eγsdWs
=ψ0(λ)˜Y0−e−λγhtam,γ(˜x0)1
γ(eλγ−1)+/radicalbig
2γβ−1e−λγ/integraldisplayλ
0eγsdWs
=ψ0(λ)˜Y0−htam,γ(˜x0)/integraldisplayλ
0ψ0(s)ds+/radicalbig
2γβ−1e−λγ/integraldisplayλ
0eγsdWs
=ψ0(λ)˜Y0−ψ1(λ)htam,γ(˜x0)+/radicalbig
2γβ−1e−λγ/integraldisplayλ
0eγsdWs(62)
29One notices that e−λγ/integraltextλ
0eγsdWs follows a zero mean Gaussian distribution and
E(e−λγ/integraldisplayλ
0eγsdWs)(e−λγ/integraldisplayλ
0eγsdWs)T=e−2λγ/integraldisplayλ
0e2γsdsId
=1
2γe−2λγ(e2λγ−1)Id
=1
2γ(1−e−2λγ)Id
= (/integraldisplayλ
0e−2γsds)Id
= (/integraldisplayλ
0ψ2
0(s)ds)Id
For the second process,
˜xλ= ˜x0+/integraldisplayλ
0ψ0(s)ds˜Y0−htam,γ(˜x0)/integraldisplayλ
0/integraldisplays
0e−γ(s−u)duds+/radicalbig
2γβ−1/integraldisplayλ
0e−γt/integraldisplayt
0eγsdWsdt
= ˜x0+ψ1(λ)˜Y0−htam,γ(˜x0)1
γ/integraldisplayλ
0e−γs(eγs−1)ds+/radicalbig
2γβ−1/integraldisplayλ
0e−γt/integraldisplayt
0eγsdWsdt
= ˜x0+ψ1(λ)˜Y0−htam,γ(˜x0)/integraldisplayλ
0ψ1(s)ds+/radicalbig
2γβ−1/integraldisplayλ
0e−γt/integraldisplayt
0eγsdWsdt
= ˜x0+ψ1(λ)˜Y0−ψ2(λ)htam,γ(˜x0)+/radicalbig
2γβ−1/integraldisplayλ
0e−γtMtdt
(63)
whereMt:=/integraltextt
0eγsdWs. Since
d(e−γtMt) =−γe−γtMtdt+e−γtdMt=−γe−γtMtdt+dWt
or equivalently
e−γtMt=−γ/integraldisplayλ
0e−γsMsds+Wt
settingt=λand using the fact that Mλ=/integraltextλ
0eγsdWs, one deduces that
/integraldisplayλ
0e−γtMtdt=1
γ/integraldisplayλ
0(1−eγ(t−λ))dWt (64)
which is a zero-mean Gaussian distribution with
E(/integraldisplayλ
0e−γtMtdt)(/integraldisplayλ
0e−γtMtdt)T=1
γ2/integraldisplayλ
0(1−e−γt)2Id= (/integraldisplayλ
0ψ2
1(t)dt)Id.
Finally,
E/parenleftBigg
e−λγ/integraldisplayλ
0eγsdWs/parenrightBigg/parenleftbigg1
γ/integraldisplay
0(1−eγ(s−λ))dWs/parenrightbiggT
=1
γ/parenleftBigg/integraldisplayλ
0eγ(s−λ)(1−eγ(s−λ))ds/parenrightBigg
Id
=/integraldisplayλ
0ψ0(s)ψ1(s)ds Id
30We have essentially proved that the terms/integraltextλ
0e−γtMtdtappearing in ( 63) ande−λγ/integraltextλ
0eγsdWs
in (62) follow Gaussian distributions with the required cross-co variance matrix.
The result immediately follows.
Proof of Lemma 7.3.The proof begins by writing
¯Yn+1= (1−λγ)¯Yλ
n−λhtam,γ(¯xλ
n)+En+/radicalbig
2γβ−1Ξn+1
¯xn+1= ¯xλ
n+λ¯Yλ
n+en+/radicalbig
2γβ−1Ξ′
n+1(65)
where
En= (ψ0(λ)−1+λγ)¯Yλ
n+(λ−ψ1(λ))htam,γ(¯xλ
n)
en= (ψ1(λ)−λ)¯Yλ
n−ψ2(λ)htam,γ(¯xλ
n).(66)
By elementary calculations using the deﬁnition of ψione obtains
max{|ψ0−1+λγ|,|λ−ψ1(λ)|,|ψ2(λ)|} ≤(1+γ2)λ2. (67)
Additionally, using the deﬁnition of the covariance matrix of the2d−Gaussian random
variable and using the inequality/summationtextk
i=0xk/k!≤exfork= 1,2,x≤0, there holds
C11(λ) :=E|Ξn+1|2≤λd
C22(λ) :=E|Ξ′
n+1|2≤d
3λ3
C12(λ) :=E∝an}⌊∇a⌋ketle{tΞn+1,Ξ′
n+1∝an}⌊∇a⌋ket∇i}ht ≤d
2λ2.(68)
Using the expression in ( 65) we shall use the same construction as in the proof of 6.2, and
we shall use ( 67) and ( 68) to bound the remaining terms.
γ2
4E|¯xn+1+γ−1¯Yn+1|2=γ2
4E|¯xλ
n+γ−1¯Yλ
n−γ−1λhtam,γ(¯xλ
n)|2+δ2(n)(69)
where
δ2(n) =γ
2β/parenleftbig
C11(λ)/2+γ2C22(λ)+γC12(λ)/parenrightbig
+1
4E|γen+En|2
+1
2E∝an}⌊∇a⌋ketle{tγ¯xλ
n+¯Yλ
n−λhtam,γ(¯xλ
n),γen+En∝an}⌊∇a⌋ket∇i}ht
≤C1λγ+γ2E|en|2+E|En|2+λ2γ4E|γ¯xλ
n+¯Yλ
n−λhtam,γ(¯xλ
n)|2
+γ2
2λ2γ4E(|en|2+|En|2)
≤C1λγ+C2λ2γ6E|¯xλ
n|2+C3λ2γ4E|¯Yλ
n|2+C4λ2E|htam,γ(¯xλ
n)|2(70)
where the last steps where obtained by elementary inequalit ies and the use of ( 67). Fur-
thermore1
4E|¯Yn+1|2=1
4E|(1−λγ¯Yn−λhtam,γ(¯xλ
n)|2+δ3(n) (71)
where
δ3(n) =1
4E|En|2+1
2E∝an}⌊∇a⌋ketle{t(1−λγ¯Yλ
n−λhtam,γ(¯xλ
n),En∝an}⌊∇a⌋ket∇i}ht+γ
2βC11(λ)
≤C1λ2γ4(E|¯Yλ
n|2+λ2|htam,γ(¯xλ
n)|2)+C′
2λγdβ−1.(72)
31Finally,
−γ2r
4E|¯xn+1|2≤ −1
4γ2rE|¯xλ
n+λ¯Yλ
n|2+δ4(n) (73)
where
δ4(n) =−1
2γ2rE∝an}⌊∇a⌋ketle{t¯xλ
n+λ¯Yλ
n,en∝an}⌊∇a⌋ket∇i}ht+C22(λ)
≤C′
4λ2γ4E|¯xλ
n+λ¯Yλ
n|2+C′
41
λ2E|en|2+d
3λ3
≤C′
5λ2γ4/parenleftbig
E|htam,γ(¯xλ
n)|2+E|¯Yλ
n|2+E|¯xλ
n|2/parenrightbig
.(74)
As a result, one notices that
4/summationdisplay
i=2δi(n)≤C′
6λ2γ6(E|¯xλ
n|2+E|¯Yλ
n|2)+C′
7λγ. (75)
Adapting the proof of 6.2, and deﬁning
Mn=γ2
4|¯xλ
n+γ−1¯Yn|2+1
4|¯Yn|2−rγ2
4|¯xλ
n|2(76)
as before, there holds
E[Mn+1|Mn]≤(1−λrγ)Mn+λ/parenleftbigg
λγ21
4+1
2m+1
4+r
2γ−γ
2/parenrightbigg
E|¯Yλ
n|2
+λ/parenleftbigg
−mγ
4+rγ3+1
2λm2+m3
16/parenrightbigg
E|¯xλ
n|2+4/summationdisplay
i=2δi(n)+C0λγ
≤(1−λrγ)Mn+λ/parenleftbigg
λγ21
4+1
2m+1
4+r
2γ−γ
2+C′
6λγ6/parenrightbigg
E|¯Yλ
n|2
+λ/parenleftbigg
−mγ
4+rγ3+1
2λm2+m3
16+C′
6λγ6/parenrightbigg
E|¯xλ
n|2+C′
8λγ
Ifλ<γ−51
4C′
6the terms that multiply E|¯xλ
n|2andE|¯Yλ
n|2are negative so
EMn+1≤(1−λrγ)EMn+C′
8λγ
which yields the result.
Proof of Lemma 7.4.Applying once again the method used in the proof of Lemma 6.3
one sees that
Mn+1≤(1−rλγ)Mn+C′
KK′
n (77)
whereC′
Kis an absolute constant and K′
n=K′
1,n+K′
2,n+K′
3,n+C′
8λγis given by
K′
1,n=/radicalbig
2γβ−1∝an}⌊∇a⌋ketle{tγ¯xλ
n+¯Yλ
n−λhtam,γ(¯xλ
n),Ξn+1+Ξ′
n+1∝an}⌊∇a⌋ket∇i}ht
+8γ
β/parenleftbig
|Ξn+1|2+|Ξ′
n+1|2/parenrightbig
+/radicalbig
2γβ−1∝an}⌊∇a⌋ketle{tEn+γen,Ξn+1+Ξ′
n+1∝an}⌊∇a⌋ket∇i}ht
≤C/radicalbig
2γβ−1/radicalbig
Mn(|Ξn+1|+|Ξ′
n+1|)
+8γ
β/parenleftbig
|Ξn+1|2+|Ξ′
n+1|2/parenrightbig
+/radicalbig
2γβ−1λ2(1+γ2)(/radicalbig
Mn+1)|Ξn+1+Ξ′
n+1|,(78)
32K′
2,n=/radicalbig
2γβ−1∝an}⌊∇a⌋ketle{t(1−λγ)¯Yλ
n−λhtam,γ(¯xλ
n),Ξn+1∝an}⌊∇a⌋ket∇i}ht+2γβ−1|Ξn+1|2+/radicalbig
2γβ−1∝an}⌊∇a⌋ketle{tEn,ξn+1∝an}⌊∇a⌋ket∇i}ht
≤C‘2/parenleftbigg/radicalbig
γβ−1(/radicalbig
Mn+1)|Ξn+1|+2γβ−1|Ξn+1|2+λ2(1+γ2)2γ
β/radicalbig
Mn|Ξn+1|/parenrightbigg
(79)
and
K′
3=−1
2γ2r∝an}⌊∇a⌋ketle{t¯xλ
n+λ¯Yλ
n,/radicalbig
2γβ−1Ξ′
n+1∝an}⌊∇a⌋ket∇i}ht ≤C‘3/radicalbig
2γβ−1/radicalbig
Mn|Ξ′
n+1|. (80)
Combining the expressions for K′
1,K′
2,K′
3yields
Kn≤CK/radicalbig
2β−1√γ/radicalbig
Mn(|Ξn+1|+|Ξ′
n+1|)+γ
β(|Ξn+1|2+|Ξ′
n+1|2)+λγ (81)
so using that λ2γ2≤λγand the expressions for the expectations of the squares of Ξ,Ξ′
one obtains the crucial estimates
EnK′
n=CK,1λγ (82)
EnK2
n≤CK,2λγ(Mn+1) (83)
and
EnK2p
n≤CK,pλpγp(Mp
n+1)
whereCK,phas at most (d
β)pdependence on the dimension.
Using the same techniques as in the proof of the higher moment s of KLMC1 one
obtains
En|Mn+1|2p≤s|Mn|2p+2q|Mn|2p−1EnKn
+p(2p−1)22p−3|Mn|2p−2En|Kn|2+p(2p−1)22p−3En|Kn|2p
≤2CK,1sM2
npqλγM2q−1
n+λγp(2p−1)22p−3M(2p−1)
n+(CK,1+CK,p)λγ
Following again the same steps as in that proof one deduces th at there exists N′such that
EnM2p
n+1≤(1−λγ−1m/32)M2p
n+λN′γ4q−1(84)
which leads to
sup
nE|Mn|2q≤E|M0|2q+32N′
mγ4q.
Since
E|¯xλ
n|4q≤82q1
γ4q(1
1−2r)2qE|Mn|2q≤¯C4q,
one concludes that
E|¯xλ
n|2q≤/radicalBig
¯C4q.
Proof of Lemma 7.5.One begins by calculating via Jensen’s inequality
E|˜xt−pλ,n
t|2=E|/integraldisplayt
nλ˜Ys−Qλ,n
sds|2≤(t−nλ)E/integraldisplayt
nλ|˜Ys−Qλ,n
s|2ds≤λ/integraldisplayt
nλE|˜Ys−Qλ,n
s|2ds.
(85)
33E|˜Yt−Qλ,n
t|2=E/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplayt
nλe−γ(t−s)(hMY,ǫ(pλ,n
s)−htam,γ(˜xnλ))ds/vextendsingle/vextendsingle/vextendsingle/vextendsingle2
≤(t−nλ)/integraldisplayt
nλe−2γ(t−s)E|hMY,ǫ(pλ,n
s)−htam,γ(˜xnλ)|2ds
≤λ/integraldisplayt
nλE|hMY,ǫ(pλ,n
s)−htam,γ(˜xnλ)|2ds
≤8λ/integraldisplayt
nλE|hMY,ǫ(pλ,n
s)−hMY,ǫ(˜xnλ)|2ds
+8λ/integraldisplayt
nλE|hMY,ǫ(˜xnλ)−h(˜xnλ)|2ds
+8λ/integraldisplayt
nλE|h(˜xnλ)−htam,γ(˜xnλ)|2ds.
For the ﬁrst term, a further analysis yields
8λ/integraldisplayt
nλE|hMY,ǫ(pλ,n
s)−hMY,ǫ(˜xnλ)|2ds≤8λK2/integraldisplayt
nλE|pλ,n
s−˜xnλ|2ds
≤16λK2/integraldisplayt
nλE|˜xs−˜xnλ|2ds+16λK2/integraldisplayt
nλE|˜xs−pλ,n
s|2
≤16λK2/integraldisplayt
nλE|/integraldisplays
nλ˜Yudu|2ds+16λK2/integraldisplayt
nλE|˜xs−pλ,n
s|2ds
≤16K2λ4sup
s∈[nλ,(n+1)λE|˜Ys|2+16λK2/integraldisplayt
nλE|˜xs−pλ,n
s|2ds.(86)
For the second term, there holds
8λ/integraldisplayt
nλE|hMY,ǫ(˜xnλ)−h(˜xnλ)|2ds≤Cpλ2ǫ2(87)
For the third term,
8λ/integraldisplayt
nλE|h(˜xnλ)−htam,γ(˜xnλ)|2ds≤C3λ2γ−2q+1. (88)
Inserting ( 86) (87) (88) into ( 85) yields
sup
nλ≤s≤tE|˜xs−pλ,n
s|2≤λ2Rλ,γ+16λ2K2/integraldisplayt
nλ/integraldisplays
nλsup
nλ≤u≤zE|˜xu−pλ,n
u|2dzds
≤λ2Rλ,γ+16λ3K2/integraldisplayt
nλsup
nλ≤u≤sE|˜xs−pλ,n
s|2ds
An application of Grownwall’s inequality yields
E|˜xt−pλ,n
t|2≤λ2Rλγe16λ3K2(t−nλ). (89)
Inserting this into ( 86) and combining all together yields
E|˜Yt−Qλ,n
t|2≤(1+16K2λ4)Rλ,γ≤2Rλ,γ
34whereRλ,γis given by
Rλ,γ=λ416K2sup
[nλ≤s≤(n+1)λ]E|˜Ys|2+C3λ2γ−2q+1+Cpλ2ǫ2. (90)
Proof of Lemma 7.6.Similarly as in the proof of 2.6, one calculates via Lemmas 7.5,7.6
W2(L(pλ,n,Qλ,n),L(Xt,Yt))≤n/summationdisplay
k=1W2(L(pλ,k,Qλ,k),L(pλ,k−1,Qλ,k−1))
≤n/summationdisplay
k=1W2/parenleftBig
L(ˆpkλ,(˜xkλ,˜Ykλ),ˆQkλ,(˜xkλ,˜Ykλ)),L(ˆpkλ,(pλ,k−1
kλ,Qλ,k−1
kλ),ˆQkλ,(pλ,k−1
kλ,Qλ,k−1
kλ))/parenrightBig
≤n/summationdisplay
k=1√
2e−m
βγ(t−kλ)W2(L(˜xkλ,˜Qkλ),L(pλ,k−1
kλ,Qλ,k−1
kλ))
≤n/summationdisplay
k=1√
2e−m
βγλ(n−k)W2(L(˜xkλ,˜Qkλ),L(pλ,k−1
kλ,Qλ,k−1
kλ))
≤√
2C/radicalbig
Rλ,γn/summationdisplay
k=1√
2e−m
βγλ(n−k)
≤√
2C/radicalbig
Rλ,γ1
1−e−m
βγλ
≤√
2C/radicalbig
Rλ,γβγ
mλem
βγλ
≤C′γ
λ/radicalbig
Rλ,γ.
D Proofs of section 8
Proof of Lemma 8.2.By strong convexity there holds that for every x∈E
u(x)−u(y)≤ ∝an}⌊∇a⌋ketle{th(x),x−y∝an}⌊∇a⌋ket∇i}ht ≤ |h(x)||x−y| ≤L(1+2R0)l+1|x−y|. (91)
Furthermore, since Eu(xn) =Eu(Xn1E(Xn)), settingx:=Xn1E(Xn)andy:=
X1E(Xn)in (91) yields
Eu(xn)−u(X1E(Xn)≤EL(1+2R0)l+1|Xn1E(Xn)−X1E(Xn)| ≤L(1+2R0)l+1E|Xn−X|,
which by Cauchy-Swartz inequality implies
Eu(xn)−u(X1E(Xn)≤L(1+2R0)l+1/radicalbig
E|Xn−X|2=L(1+2R0)l+1W2(L(¯θλ
n,µβ).
35Proof of Lemma 8.2.We begin by calculating
E[u(X1E(Xn))]−u(x∗)≤E[u(X)1E(Xn)]−u(x∗)
+u(0)P(Xn∈Ec)
=E[u(X)1E(Xn)1E′(X)]+E[u(X)1E(Xn)1E′c(X)]−u(x∗)
+u(0)P(Xn∈Ec)
≤E[u(X)1E′(X)]−u(x∗)+/radicalbig
Eu2(X)/radicalbig
P({Xn∈E}∩{X∈E′c})
+u(0)P(Xn∈Ec).
(92)
In addition, one notices that uisM0=L(1+2(R0+1))lsmooth onE′so
E[u(X)1E′(X)]−u(x∗)≤/integraldisplay
E′u(x)−u(x∗)dµβ(x)≤M0
2/integraldisplay
E′|x−x∗|2dµβ(x)≤M0
2E|X−x∗|2.
(93)
Furthermore, since uis convex
u(x)−u(0)≤ ∝an}⌊∇a⌋ketle{th(x)−h(x∗),x∝an}⌊∇a⌋ket∇i}ht
≤L(1+|x|+|x∗|)|x−x∗||x| ≤L(1+|x|+2u(0)
m)l+2
≤2l+2L(1+2u(0)
m)l+2+2l+2L|x|l+2,
and in addition
Eu2(X)≤2u2(0)+22l+8L2(1+2u(0)
m)2l+4+22l+8E|X|2l+4. (94)
Then since
P({Xn∈E}∩{X∈E′c})≤P(|Xn−X|>1)≤E|Xn−X|2,
one deduces that
((E|X|2l+4)1/2+C′)/radicalbig
P({Xn∈E}∩{X∈E′c})≤C′′(||X||l+2
2l+4+1)W2(¯θλ
n,µβ).
(95)
Finally one has the bound
P(Xn∈Ec)≤P(|Xn−x∗|>1)≤E|Xn−x∗|2≤2E|X−x∗|2+W2(¯θλ
n,µβ),
(96)
so that inserting ( 93),(95),(96) into ( 92) completes the proof.
Proof of Lemma 8.3.Consider the Langevin SDE given as
d¯Lt=−h(¯Lt)dt+/radicalbigg2
βdBt,
36with initial condition L0=x∗. By Ito’s formula, one obtains
d|¯Lt−x∗|2=−2∝an}⌊∇a⌋ketle{th(¯Lt),¯Lt−x∗∝an}⌊∇a⌋ket∇i}htdt+2d
βdt+dGt
=−2∝an}⌊∇a⌋ketle{th(¯Lt)−h(x∗),¯Lt−x∗∝an}⌊∇a⌋ket∇i}htdt+2d
βdt+dGt
≤ −m|¯Lt−x∗|2dt+2d
βdt+dGt.
wheredGtis a martingale. Taking expectations, and since by ( 6) one has that supt≥0E|¯Lt|2<
∞, we are able to interchange the differentiation with expect ation so that
d
dtE|¯Lt−x∗| ≤ −mE|¯Lt−x∗|2dt+2d
β,
Then applying Grownwall’s inequality yields
E|¯Lt−x∗|2≤2d
βm, t>0.
Due to the fact that ¯Ltconverges in the W2distance to the invariant measure, one obtains
L1convergence and the bound E|¯Lt|2→E|X|2, so since there also holds
|E|¯Lt−x∗|2−E|X−x∗|2|=/vextendsingle/vextendsingleE|¯Lt|2−E|X|2/vextendsingle/vextendsingle+2|x|∗E|¯Lt−X|,
there follows that
E|X−x∗|2= lim
t→∞E|¯Lt−x∗|2≤2d
mβ.
Proof of Corollary 8.4.Combining Lemmas 8.1,8.2,8.3and using the fact that
W2(L(¯θλ
n),µβ)≤W2(L(¯θλ
n,¯Vλ
n),πβ),
the result follows immediately.
37E Table of constants
Table 1: Basic constants and dependency on key parameters
Constant Key parameters
d
βm E|M0|p
˜Cp (d
β)qm−q/radicalbig
E|M0|2p/γ2q
˜B2d
β(2m+1)2E|V0|2
Cµβ,p(d
β)p
2m−p/2-
Table 2: Derived constants and their dependency to basic con stants
Constant Key parameters
β˜Cp˜B2Cµβ,p
C1,v -˜C2 - -
CA -˜C2(l+2)+2q(2l+2) -
c - - -/radicalbigCµβ,4l+44l+4
C -/radicalBig
˜C2(l+2)+2q(2l+2)/radicalbig˜B2-
˙C β/radicalBig
˜C2(l+2)+2q(2l+2)/radicalbig˜B2/radicalbigCµβ,4l+44l+4
38References
[1] Ö. D. Akyildiz and S. Sabanis. Nonasymptotic analysis of stochastic gradient hamil-
tonian monte carlo under local conditions for nonconvex opt imization. arXiv preprint
arXiv:2002.05465 , 2020.
[2] M. Barkhagen, N. H. Chau, É. Moulines, M. Rásonyi, S. Saba nis, and Y . Zhang. On
stochastic gradient langevin dynamics with dependent data streams in the logconcave
case. Bernoulli , 27(1):1–33, 2021.
[3] N. Brosse, A. Durmus, and E. Moulines. The promises and pi tfalls of stochastic
gradient Langevin dynamics. In Advances in Neural Information Processing Systems ,
pages 8268–8278, 2018.
[4] N. Brosse, A. Durmus, É. Moulines, and S. Sabanis. The tam ed unadjusted Langevin
algorithm. Stochastic Processes and their Applications , 129(10):3638–3663, 2019.
[5] H. N. Chau and M. Rasonyi. Stochastic Gradient Hamiltoni an Monte Carlo for Non-
Convex Learning. arXiv preprint arXiv:1903.10328 , 2019.
[6] N. H. Chau, É. Moulines, M. Rásonyi, S. Sabanis, and Y . Zha ng. On stochastic
gradient langevin dynamics with dependent data streams: Th e fully nonconvex case.
SIAM Journal on Mathematics of Data Science , 3(3):959–986, 2021.
[7] X. Cheng, N. S. Chatterji, Y . Abbasi-Yadkori, P. L. Bartl ett, and M. I. Jordan. Sharp
convergence rates for langevin dynamics in the nonconvex se tting. arXiv preprint
arXiv:1805.01648 , 2018.
[8] X. Cheng, N. S. Chatterji, P. L. Bartlett, and M. I. Jordan . Underdamped Langevin
MCMC: A non-asymptotic analysis. In Conference On Learning Theory , pages 300–
323, 2018.
[9] A. S. Dalalyan and A. Karagulyan. User-friendly guarant ees for the Langevin Monte
Carlo with inaccurate gradient. Stochastic Processes and their Applications , 2019.
[10] A. S. Dalalyan and L. Riou-Durand. On sampling from a log -concave density using
kinetic langevin diffusions. Bernoulli , 26(3):1956–1988, 2020.
[11] A. Durmus, E. Moulines, et al. Nonasymptotic convergen ce analysis for the unad-
justed Langevin algorithm. The Annals of Applied Probability , 27(3):1551–1587,
2017.
[12] A. Durmus, E. Moulines, and M. Pereyra. Efﬁcient bayesi an computation by prox-
imal markov chain monte carlo: when langevin meets moreau. SIAM Journal on
Imaging Sciences , 11(1):473–506, 2018.
[13] X. Gao, M. Gurbuzbalaban, and L. Zhu. Breaking reversib ility accelerates langevin
dynamics for global non-convex optimization. arXiv preprint arXiv:1812.07725 ,
2018.
[14] X. Gao, M. Gürbüzbalaban, and L. Zhu. Global convergenc e of stochastic gradient
hamiltonian monte carlo for nonconvex stochastic optimiza tion: Nonasymptotic per-
formance bounds and momentum-based acceleration. Operations Research , 2021.
39[15] G. Hargé. A convex/log-concave correlation inequalit y for gaussian measure and an
application to abstract wiener spaces. Probability theory and related ﬁelds , 130(3):
415–440, 2004.
[16] C.-R. Hwang. Laplace’s method revisited: weak converg ence of probability mea-
sures. The Annals of Probability , 8(6):1177–1182, 1980.
[17] T. Johnston and S. Sabanis. A strongly monotonic polygo nal euler scheme. arXiv
preprint arXiv:2112.15596 , 2021.
[18] C. Lemaréchal and C. Sagastizábal. Practical aspects o f the moreau–yosida regu-
larization: Theoretical preliminaries. SIAM journal on optimization , 7(2):367–385,
1997.
[19] A. Lovas, I. Lytras, M. Rásonyi, and S. Sabanis. Taming n eural networks with tusla:
Non-convex learning via adaptive stochastic gradient lang evin algorithms. To appear,
SIAM Journal on Mathematics of Data Science, arXiv:2006.14 514, 2020.
[20] C. Planiden and X. Wang. Proximal mappings and moreau en velopes of single-
variable convex piecewise cubic functions and multivariab le gauge functions. arXiv
preprint arXiv:1909.05795 , 2019.
[21] M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex l earning via Stochastic
Gradient Langevin Dynamics: a nonasymptotic analysis. In Conference on Learning
Theory , pages 1674–1703, 2017.
[22] R. T. Rockafellar and R. J.-B. Wets. Variational analysis , volume 317. Springer
Science & Business Media, 2009.
[23] S. Sabanis and Y . Zhang. Higher order Langevin Monte Car lo algorithm. Electronic
Journal of Statistics , 13(2):3805–3850, 2019.
[24] J. Zhang and M. Hong. First-order algorithms without li pschitz gradient: A sequen-
tial local optimization approach. arXiv preprint arXiv:2010.03194 , 2020.
[25] Y . Zhang, Ö. D. Akyildiz, T. Damoulas, and S. Sabanis. No nasymptotic estimates
for Stochastic Gradient Langevin Dynamics under local cond itions in nonconvex op-
timization. Applied Mathematics & Optimization , 87(25), 2023.
40