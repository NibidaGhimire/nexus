Graph Neural Networks for temporal graphs: State of the art, open
challenges, and opportunities
Antonio Longa1,2,*, Veronica Lachi3,*, Gabriele Santin1,*, Monica Bianchini3, Bruno Lepri1,
Pietro Li` o4, Franco Scarselli3, and Andrea Passerini2
1Fondazione Bruno Kessler, Trento, Italy
2University of Trento, Trento, Italy
3University of Siena, Siena, Italy
4University of Cambridge, Cambridge, United Kingdom
*Equal contribution
July 11, 2023
Abstract
Graph Neural Networks (GNNs) have become the
leading paradigm for learning on (static) graph-
structured data. However, many real-world systems
are dynamic in nature, since the graph and node/edge
attributes change over time. In recent years, GNN-
based models for temporal graphs have emerged as a
promising area of research to extend the capabilities
of GNNs. In this work, we provide the first com-
prehensive overview of the current state-of-the-art of
temporal GNN, introducing a rigorous formalization
of learning settings and tasks and a novel taxonomy
categorizing existing approaches in terms of how the
temporal aspect is represented and processed. We
conclude the survey with a discussion of the most
relevant open challenges for the field, from both re-
search and application perspectives.
1 Introduction
The ability to process temporal graphs is becom-
ing increasingly important in a variety of fields such
as recommendation systems [20, 88], social network
analysis [15, 19], transportation systems [36, 101],face-to-face interactions [48], human mobility [52, 24],
epidemic modeling and contact tracing [8, 74], and
many others. Traditional graph-based models are
not well suited for analyzing temporal graphs as
they assume a fixed structure and are unable to cap-
ture its temporal evolution. Therefore, in the last
few years, several models capable to directly encode
temporal graphs have been developed, such as ma-
trix factorization-based approaches [1] and temporal
motif-based methods [47].
Recently, also GNNs have been successfully ap-
plied to temporal graphs. Indeed, their success in
various static graph tasks, including node classifi-
cation [30, 81, 39, 22, 25, 56, 87] and link predic-
tion [102, 7, 103], has not only established them
as the leading paradigm in static graph process-
ing, but has also indicated the importance of ex-
ploring their potential in other graph domains, such
as temporal graphs. With approaches ranging from
attention-based methods [92] to Variational Graph-
Autoencoders (VGAEs) [29], Temporal Graph Neu-
ral Networks (TGNNs) have achieved state-of-the-art
results on tasks such as temporal link prediction [70],
node classification [66] and edge classification [83].
Despite the potential of GNN-based models for tem-
1arXiv:2302.01018v4  [cs.LG]  8 Jul 2023poral graph processing and the variety of different
approaches that emerged, a systematization of the
literature is still missing. Existing surveys either
discuss general techniques for learning over tempo-
ral graphs, only briefly mentioning temporal exten-
sions of GNNs [37, 3, 96, 91], or focus on specific top-
ics, like temporal link prediction [63, 73] or temporal
graph generation [28], or present an overview of GNN
models designed for different types of graphs without
providing in-depth coverage of temporal GNNs [77].
This work aims to fill this gap by providing a system-
atization of existing GNN-based methods for tempo-
ral graphs and a formalization of the tasks being ad-
dressed.
Our main contributions are the following:
•We propose a coherent formalization of the dif-
ferent learning settings and of the tasks that can
be performed on temporal graphs, unifying exist-
ing formalism and informal definitions that are
scattered in the literature, and highlighting sub-
stantial gaps in what is currently being tackled;
•We organize existing TGNN works into a com-
prehensive taxonomy that groups methods ac-
cording to the way in which time is represented
and the mechanism with which it is taken into
account;
•We highlight the limitations of current TGNN
methods, discuss open challenges that deserve
further investigation and present critical real-
world applications where TGNNs could provide
substantial gains.
2 Temporal Graphs
We provide a formal definition of the different types
of graphs analyzed in this work and we structure dif-
ferent existing notions in a common framework.
Definition 1 (Static Graph - SG) AStatic Graph
is a tuple G= (V, E, XV, XE), where Vis the
set of nodes, E⊆V×Vis the set of edges, and
XV, XEaredV-dimensional node features and dE-
dimensional edge features.Node and edge features may be empty. In the fol-
lowing, we assume that all graphs are directed, i.e.,
(u, v)∈Edoes not imply that ( v, u)∈E. Moreover,
given v∈V, the set
N[v] :={u∈V: (u, v)∈V},
denotes the neighborhood of vinG.
Extending [63], we define Temporal Graphs as fol-
lows.
Definition 2 (Temporal Graph - TG) ATempo-
ral Graph is a tuple GT= (V, E, V T, ET), where V
andEare, respectively, the set of all possible nodes
and edges appearing in a graph at any time, while
VT:={(v, xv, ts, te) :v∈V, xv∈RdV, ts≤te},
ET:={(e, xe, ts, te) :e∈E, xe∈RdE, ts≤te},
are the temporal nodes and edges, with time-
dependent features and initial and final timestamps.
A set of temporal graphs is denoted as GT.
Observe that we implicitly assume that the ex-
istence of a temporal edge in ETrequires the si-
multaneous existence of the corresponding temporal
nodes in VT. Moreover, the definition implies that
node and edge features are constant inside each in-
terval [ ts, te], but may otherwise change over time.
Since the same node or edge may be listed multi-
ple times, with different timestamps, we denote as
¯ts(v) = min {ts: (v, xv, ts, te)∈VT}and¯te(v) =
max{te: (v, xv, ts, te)∈VT}the time of first and last
appearance of a node, and similarly for ¯ts(e),¯te(e),
e∈E. Moreover, we set Ts(GT) := min {¯ts(v) :
v∈V},Te(GT) := max {¯te(v) :v∈V}as the ini-
tial and final timestamps in a TG GT. For two TGs
Gi
T:= (Vi, Ei, Vi
T, Ei
T),i= 1,2, we write G1
T⊆VG2
T
to indicate the topological inclusion V1⊆V2, while
no relation between the corresponding timestamps is
required.
Given v∈V, the set
Nt[v] :={u∈V:∃(e, xe, ts, te)∈ET
with e= (u, v), ts≤t}
is the temporal neighborhood of vat time t.
2General TGs have no restriction on their times-
tamps, which can take any value (for simplicity, we
just assume that they are non-negative). However, in
some applications, it makes sense to force these val-
ues to be multiples of a fixed time-step. This leads to
the notion of Discrete Time Temporal Graphs, which
are defined as follows.
Definition 3 (Discrete Time Temporal Graph
- DTTG) Let∆t >0be a fixed time-step and let
t1< t2<···< tnbe timestamps with tk+1=tk+
∆t. ADiscrete Time Temporal Graph GDTis a TG
where for each (v, xv, ts, te)∈VTor(e, xe, ts, te)∈
ET, the timestamps ts, teare taken from the set of
fixed timestamps (i.e., ts, te∈ {t1, t2, . . . , t n}, with
ts< te).
2.1 Representation of temporal
graphs
In the existing literature, dynamic graphs are of-
ten divided into DTTG (as in Definition 3) and
continuous-time temporal graphs (CTTG) (or time
sequence graphs), which are defined e.g. in [37, 3,
50, 28].
However, we find that this separation does not cap-
ture well the central difference between various graph
characterizations, which is rather based on the fact
that the data are represented as a stream of static
graphs, or as a stream of single node and edge ad-
dition and deletion events. We thus formalize the
following two categories for the description of time-
varying graphs, based on snapshots or on events.
These different representations lead to different al-
gorithmic approaches and become particularly useful
when organizing the methods in a taxonomy.
The snapshot-based strategy focuses on the tem-
poral evolution of the whole graph. Snapshot-based
Temporal Graphs can be defined as follows.
Definition 4 (Snapshot-based Temporal Graph
- STG) Lett1< t2<···< tnbe the ordered set of
all timestamps ts, teoccurring in a TG GT. Set
Vi:={(v, xv) : (v, xv, ts, te)∈VT, ts≤ti≤te},
Ei:={(e, xe) : (e, xe, ts, te)∈ET, ts≤ti≤te},and define the snapshots Gi:= (Vi, Ei),i= 1, . . . , n .
Then a Snapshot-based Temporal Graph representa-
tion of GTis the sequence
GS
T:={(Gi, ti) :i= 1, . . . , n }
of time-stamped static graphs.
This representation is mostly used to describe
DTTGs, where the snapshots represent the TG cap-
tured at periodic intervals (e.g., hours, days, etc.).
The event-based strategy is instead more appro-
priate when the focus is on the temporal evolution of
individual nodes or edges. This leads to the following
definition.
Definition 5 (Event-based Temporal Graph -
ETG) LetGTbe a TG, and let εdenote one of the
following events :
•Node insertion ε+
V:= (v, t): the node vis added
toGTat time t, i.e., there exists (v, xv, ts, te)∈
VTwith ts=t.
•Node deletion ε−
V:= (v, t): the node vis re-
moved from GTat time t, i.e., there exists
(v, xv, ts, te)∈VTwith te=t.
•Edge insertion ε+
E:= (e, t): the edge eis added
toGTat time t, i.e., there exists (e, xe, ts, te)∈
ETwith ts=t.
•Edge deletion ε−
E:= (e, t): the edge eis re-
moved from GTat time t, i.e., there exists
(e, xe, ts, te)∈ETwith te=t.
AnEvent-based Temporal Graph representation of
TG is a sequence of events
GE
T:={ε:ε∈ {ε+
V, ε−
V, ε+
E, ε−
E}}.
Here it is implicitly assumed that node and edge
events are consistent (e.g., a node deletion event im-
plies the existence of an edge deletion event for each
incident edge). In the case of an ETG, the TG
structure can be recovered by coupling an insertion
and deletion event for each temporal edge and node.
ETGs are better suited than STGs to represent TGs
with arbitrary timestamps.
3We will use the general notion of TG, which com-
prises both STG and ETG, in formalizing learning
tasks in the next section. On the other hand, we will
revert to the STG and ETG notions when introducing
the taxonomy of TGNN methods in Section 6, since
TGNNs use one or the other representation strategy
in their algorithmic approaches.
3 Basic notions on Graph Neu-
ral Networks
GNNs are a class of neural network architectures
specifically designed to process and analyze graph-
structured data; they learn a function hv=
GNN(v, G;θ), with v∈Vandθbeing a set of train-
able parameters. GNNs rely on the so called message
passing mechanism, which implement a local com-
putational scheme to process graphs. Formally, the
information related to a node vis stored into a fea-
ture vector hvthat is iteratively updated by aggre-
gating the features of its neighboring nodes. After k
iterations, the vector hk
vcontains both the structural
information and the node content of the k–hop neigh-
borhood of v. Given a sufficient number of iterations,
the node feature vectors can be used to classify the
nodes or the entire graph. Specifically, the output of
thek-th layer of a message passing GNN is:
hk
v=COMBINE(k)(hk−1
v,AGGREGATE(k)(xk−1
u, u∈ N[v]))
where AGGREGATE(k)is a function that aggregates the
node features from the neighborhood N[v] at the
(k−1)-th iteration, and COMBINE(k)is a function that
combines the features of the node vwith those of
its neighbors. The aggregation step often involves
employing permutation invariant operations such as
mean, max-pooling, and sum. These operations en-
sure that the final aggregated representation is insen-
sitive to the ordering of the nodes. Instead, typical
choices for the COMBINE function are concatenation
and summation. In node level tasks, a READOUT func-
tion is used to produce an output for each node, based
on its features at the final layer K:
ov=READOUT (hK
v)whereas, in graph level tasks, a READOUT function
produces the final output given the feature vectors
from the last layer K:
o=READOUT (hK
v, v∈V}).
In our work we explore models that specifically
tackle temporal graphs employing this kind of mes-
sage passing scheme.
4 Other approaches to model
temporal graphs
Traditionally, machine learning models for graphs
have been mostly designed for static graphs [89, 104].
However, many applications involve temporal graphs
[42, 14, 76]. This introduces important challenges
for learning and inference since nodes, attributes,
and edges change over time. Many different rep-
resentation learning techniques for temporal graphs
have been recently proposed. A popular class of ap-
proaches for learning an embedding function for tem-
poral graphs is the class of random walk methods.
For example, in [84] temporal random walks are ex-
ploited to efficently and automatically sample tem-
poral network motifs, i.e, connected subgraphs with
links that appear within a restricted time range. Sim-
ilarly, in [44], a time-reinforced random walk is pro-
posed to effectively sample the structural and tempo-
ral contexts over graph evolution. With DyANE [71],
instead, the temporal graphs are transformed into a
static graph representation called a supra-adjacency
representation. In this approach, the nodes are de-
fined as (node, time) pairs from the original tempo-
ral graph. This static graph representation retains
the temporal paths of the original network, crucial
for comprehending and constraining the underlying
dynamical processes. Afterwards, standard embed-
ding techniques for static graphs, utilizing random
walks, are employed. Temporal graph representation
learning has leveraged the use of temporal point pro-
cesses as well. Temporal point processes are stochas-
tic processes employed for modeling sequential asyn-
chronous discrete events occurring in continuous time
[43]. DyRep [79] is capable of learning a set of
4functions that can effectively generate evolving, low-
dimensional node embeddings. By using the obtained
node embeddings, a temporal point process is em-
ployed to estimate the likelihood of an edge between
nodes at a certain timestamp. In [80], instead, the
occurrence of an edge in a temporal graph is mod-
eled as a multivariate point process, where the in-
tensity function is influenced by the score assigned
to that edge, which is computed using the learned
entity embeddings. The entity embeddings, which
evolve over time, are acquired through a recurrent
architecture. Also non-negative matrix factorization
(NMF) has been employed for the purpose of link
prediction in temporal graphs. In [1], novel iterative
rules for NMF are proposed to construct the matrix
factors that capture crucial features of the temporal
graph, enhancing the accuracy of the link prediction
process.
Lastly, the majority of recently proposed meth-
ods employ deep learning techniques. For example,
DynGem [26] is a dynamical autoencoder for growing
graphs that construct the embedding of a snapshot
based on the embedding of the previous snapshot.
TRRN [94], instead, uses multi-head self-attention
to process a set of memories, enabling efficient in-
formation flow from past observations to current la-
tent representations through shortcut paths. It in-
corporates policy networks with differentiable binary
routers to estimate the activation probability of each
memory and dynamically update them at the most
relevant time steps. In [93], a spatio-temporal atten-
tive recurrent network model, called STAR, is pro-
posed for interpretable temporal node classification.
In [59] a node-level regression task is achieved by
training embeddings to maximize the mutual infor-
mation between patches of the graph, at any given
time step, and between features of the central nodes
of patches, in the future. Finally, TSNet [105] is a
comprehensive framework for node classification in
temporal graphs, consisting of two key steps. Firstly,
the graph snapshots undergo a sparsification process
using edge sampling, guided by a learned distribution
derived from the supervised classification task. This
step effectively reduces the density of the snapshots.
Subsequently, the sparsified snapshots are aggregated
and processed through a convolutional network to ex-tract meaningful features for node classification.
In our survey, we aim to explore and analyze meth-
ods that leverage and adapt the GNN framework to
temporal graphs. By delving into this specific subset
of techniques, we seek to gain a deeper understand-
ing of their applicability, effectiveness, and potential
in capturing the temporal dynamics of complex graph
structures.
5 Learning tasks on temporal
graphs
Thanks to their learning capabilities, TGNNs are ex-
tremely flexible and can be adapted to a wide range
of tasks on TGs. Some of these tasks are straight-
forward temporal extensions of their static counter-
parts. However, the temporal dimension has some
non-trivial consequences in the definition of learn-
ing settings and tasks, some of which are often only
loosely formalized in the literature. We start by for-
malizing the notions of transductive and inductive
learning for TGNNs, and then describe the different
tasks that can be addressed.
5.1 Learning settings
The machine learning literature distinguishes be-
tween inductive learning, in which a model is learned
on training data and later applied to unseen test in-
stances, and transductive learning, in which the input
data of both training and test instances are assumed
to be available, and learning is equivalent to leverag-
ing the training inputs and labels to infer the labels of
test instances given their inputs. This distinction be-
comes extremely relevant for graph-structured data,
where the topological structure gives rise to a nat-
ural connection between nodes, and thus to a way
to propagate the information in a transductive fash-
ion. Roughly speaking, transductive learning is used
in the graph learning literature when the node to be
predicted and its neighborhood are known at training
time — and is typical of node classification tasks —,
while inductive learning indicates that this informa-
tion is not available — and is most often associated
to graph classification tasks.
5Figure 1: Learning settings. Schematic representation of the learning settings on TGs formalized in
Section 5.1. The temporal graphs are represented as sequences of snapshots, with training (red) and inference
(green) nodes connected by edges (solid lines), and where a dotted line connects instances of the same node
(with possibly different features and/or labels) in successive snapshots. The four categories are obtained
from the different combinations of a temporal and a topological dimension. The temporal dimension
distinguishes the future setting, where the training nodes are all observed before the inference nodes (first
row), from the past setting where inference is performed also on nodes appearing before the observation of
the last training node (second row). The topological dimension comprises a transductive setting, where
each inference node is observed (unlabelled) also during training (left column), and an inductive setting,
where inference is performed on nodes that are unknown at training time (right column).
However, when talking about GNNs with their rep-
resentation learning capabilities, this distinction is
not so sharp. For example, a GNN trained for node
classification in transductive mode could still be ap-
plied to an unseen graph, thus effectively performing
inductive learning. The temporal dimension makes
this classification even more elusive, since the graph
structure is changing over time and nodes are nat-
urally appearing and disappearing. Defining node
membership in a temporal graph is thus a challenging
task in itself.
Below, we provide a formal definition of trans-
ductive and inductive learning for TGNNs which is
purely topological, i.e., linked to knowing or not the
instance to be predicted at the training time, and we
complete it with a temporal dimension, which dis-
tinguishes between past and future prediction tasks.
A schematic representation of these settings is visu-
alized in Figure 1. We recall (see Section 2) that
Te(GT) is the final timestamp in a TG GT.Definition 6 (Learning settings) Assume that a
model is trained on a set of n≥1temporal graphs
GT:={Gi
T:= (Vi, Ei, XV
i, XE
i), i= 1, . . . , n }.
Moreover, let
Tall
e:= max
i=1,...,nTe(Gi
T), Vall:=∪n
i=1Vi, Eall:=∪n
i=1Ei,
be the final timestamp and the set of all nodes and
edges in the training set. Then, we have the following
settings:
•Transductive learning : inference can only be per-
formed on v∈Vall,e∈Eall, orGT⊆VGi
Twith
Gi
T∈ GT.
•Inductive learning : inference can be performed
also on v /∈Vall,e /∈Eall, orGT̸⊆VGi
T, for all
i= 1, . . . , n .
•Past prediction : inference is performed for t≤
Tall
e.
6•Future prediction : inference is performed for t >
Tall
e.
We remark that all combinations of topological and
temporal settings are meaningful, except for the case
of inductive graph-based tasks. Indeed, the measure
of time used in TGs is relative to each single graph.
Moving to an unobserved graph would thus make the
distinction between past and future pointless. More-
over, let us observe that, in all other cases, the two
temporal settings are defined based on the final time
of the entire training set, and not of the specific in-
stances (nodes or edges), since their embedding may
change also as an effect of the change of their neigh-
bors in the training set.
We will use this categorization to describe super-
vised and unsupervised learning tasks in Section 5.2-
5.3, and to present existing models in Section 6.
5.2 Supervised learning tasks
Supervised learning tasks are based on a dataset
where each object is annotated with its label (or
class), from a finite set of possible choices C:=
{C1, C2, . . . , C k}.
5.2.1 Classification
Definition 7 (Temporal Node Classification)
Given a TG GT= (V, E, V T, ET), the node classi-
fication task consists in learning the function
fNC:V×R+→ C
which maps each node to a class C∈ C, at a time
t∈R+.
This is one of the most common tasks in the TGNN
literature. For instance, [61, 92, 83, 106, 66] focus
on a future-transductive (FT) setting, i.e., predict-
ing the label of a node in future timestamps. TGAT
[92] performs future-inductive (FI) learning, i.e., it
predicts the label of an unseen node in the future.
Finally, DGNN [51] is the only method that has been
tested on a past-inductive (PI) setting, i.e., predicting
labels of past nodes that are unavailable (or masked)
during training, while no approach has been appliedto the past-transductive (PT) one. A significant ap-
plication may be in epidemic surveillance, where con-
tact tracing is used to produce a TG of past human
interactions, and sample testing reveals the labels (in-
fection status) of a set of individuals. Identifying the
past infection status of the untested nodes is a PT
task.
Definition 8 (Temporal Edge Classifica-
tion) Given a TG GT= (V, E, V T, ET), the temporal
edge classification task consists in learning a function
fEC:E×R+→ C
which assigns each edge to a class at a given time
t∈R+.
Temporal edge classification has been less explored
in the literature. Existing methods have focused on
FT learning [61, 83], while FI, PI and PT have not
been tackled so far. An example of PT learning con-
sists in predicting the unknown past relationship be-
tween two acquaintances in a social network given
their subsequent behavior. For FI, one may predict
if a future transaction between new users is a fraud
or not.
In the next definition we use the set of real and
positive intervals I+:={[ts, te]⊂R+}.
Definition 9 (Temporal Graph Classifica-
tion) LetGTbe a domain of TGs. The graph classi-
fication task requires to learn a function
fGC:GT×I+→ C
that maps a temporal graph, restricted to a time in-
terval [ts, te]∈I+, into a class.
The definition includes the classification of a single
snapshot (i.e., ts=te). As mentioned above, in the
inductive setting the distinction between past and fu-
ture predictions is pointless. In the transductive set-
ting, instead, a graph GT∈ GTmay be classified in
a past mode if [ Ts(GT), Te(GT)]⊆[ts, te], or in the
future mode, otherwise.
The only existing method addressing the classifi-
cation of temporal graphs is found in [55], where
the discrimination between STGs characterized by
7different dissemination processes is formalized as a
PT classification task. The temporal graph classifi-
cation task can have numerous relevant applications.
For instance, an example of inductive temporal graph
classification is predicting mental disorders from the
analysis of the brain connectome [32]. On the other
hand, detecting critical stages during disease progres-
sion from gene expression profiles [23] can be framed
as a past transductive graph classification task.
5.2.2 Regression
The tasks introduced for classification can all be
turned into corresponding regression tasks, simply by
replacing the categorical target Cwith the set R. We
omit the formal definitions for the sake of brevity.
Static GNNs have already shown outstanding results
in this setting, e.g., in weather forecasting [38] and
earthquake location and estimation [53]. However,
limited research has been conducted on the applica-
tion of TGNNs to regression tasks. Notable excep-
tions are the use of TGNNs in two FT regression
tasks, the traffic prediction [10] and the prediction
of the incidence of chicken pox cases in neighboring
countries [55].
5.2.3 Link prediction
Link prediction requires the model to predict the rela-
tion between two given nodes, and can be formulated
by taking as input any possible pair of nodes. Thus,
we consider the setting to be transductive when both
node instances are known at training time, and in-
ductive otherwise. Instead, [63] adopt a different ap-
proach and identify Level-1 (the set of nodes is fixed)
andLevel-2 (nodes may be added and removed over
time) temporal link prediction tasks.
Definition 10 (Temporal Link Prediction) Let
GT= (V, E, V T, ET)be a TG. The temporal link
prediction task consists in learning a function
fLP:V×V×R+→[0,1]
which predicts the probability that, at a certain time,
there exists an edge between two given nodes.The domain of the function fLPis the set of all
feasible pairs of nodes, since it is possible to pre-
dict the probability of future interactions between
nodes that have been connected in the past or not,
as well as the probability of missing edges in a past
time. Most TGNN approaches for temporal link
prediction focus on future predictions, forecasting
the existence of an edge in a future timestamp be-
tween existing nodes (FT is the most common set-
ting) [61, 70, 29, 100, 92, 50, 83, 51, 66, 106], or un-
seen nodes (FI) [29, 92, 66]. The only model that in-
vestigates past temporal link prediction is [50], which
devises a PI setting by masking some nodes and pre-
dicting the existence of a past edge between them.
Note that predicting past temporal links can be ex-
tremely useful for predicting, e.g., missing interac-
tions in contact tracing for epidemiological studies.
Definition 11 (Event Time Prediction) Let
GT= (V, E, V T, ET)be a TG. The aim of the event
time prediction task is to learn a function
fEP:V×V→R+
that predicts the time of the first appearance of an
edge.
None of the existing methods address this task.
Potential FT applications of event time prediction
include predicting when a customer will pay an in-
voice to its supplier, or how long it takes to connect
two similar users in a social network.
5.3 Unsupervised learning tasks
In this section, we formalize unsupervised learning
tasks on temporal graphs, an area that has received
little to no attention in the TGNN literature so far.
5.3.1 Clustering
Temporal graphs can be clustered at the node or
graph level, with edge-level clustering being a minor
variation of the node-level one. Some relevant appli-
cations can be defined in terms of temporal cluster-
ing.
8Definition 12 (Temporal Node Cluster-
ing) Given a TG GT= (V, E, V T, ET), the tempo-
ral node clustering task consists in learning a time-
dependent cluster assignment map
fNCl:V×R+→ P(V),
where P(V) :={p1, p2, . . . , p k}is a partition of the
node set V, i.e., pi⊂VT,pi∩pj=∅, ifi̸=j,
∪N
i=1pi=VT.
While node clustering in SGs is a very common task,
its temporal counterpart has not been explored yet
for TGNNs, despite its potential relevance in appli-
cation domains like epidemic modelling, e.g., identi-
fying groups of exposed individuals, in both inductive
and transductive settings [69, 33, 41, 12, 13], trend
detection in customer profiling, mostly in transduc-
tive settings [45, 67], or disease clustering, mostly in
future transductive settings [35, 54, 85, 60].
Definition 13 (Temporal Graph Cluster-
ing) Given a set of temporal graphs GT, the temporal
graph clustering task consists in learning a cluster-
assignment function
fGCl:GT×I+→ P(GT),
where P(GT) :={p1, . . . , p k}is a partition of the set
of temporal graphs in the given time interval.
Relevant examples of tasks of inductive temporal
graph clustering are grouping social interaction net-
works (e.g., hospitals, workplaces, schools) according
to their interaction patterns [48], or grouping diseases
in terms of similarity between their spreading pro-
cesses [17, 58].
5.3.2 Low-dimensional embedding (LDE)
LDEs are especially useful in the temporal setting,
e.g., to visually inspect temporal dynamics of indi-
vidual nodes or entire graphs, and identify relevant
trends and patterns. No GNN-based models have
been applied to these tasks, neither at the node nor
at the graph level. We formally define the tasks of
temporal node and graph LDE as follows.Definition 14 (Low-dimensional temporal node
embedding) Given a TG GT= (V, E, V T, ET), the
low-dimensional temporal node embedding task con-
sists in learning a map
fNEm :V×R+→Rd
to map a node, at a given time, into a low dimensional
space.
Definition 15 (Low-dimensional temporal
graph embedding) Given a domain of TGs GT,
the low-dimensional temporal graph embedding task
aims to learn a map
fGEm :GT×I+→Rd,
which represents each graph as a low dimensional vec-
tor in a given time interval.
6 A taxonomy of TGNNs
This section describes the taxonomy with which we
categorize existing TGNN approaches (see Figure 2).
All these methods learn a time-dependent embed-
dinghv(t) = TGNN(v, GT;θ) of each node v∈VT
of a TG GT, where again θrepresents a set of train-
able parameters. Following the representation strate-
gies outlined in Section 2.1, the first level groups
methods into Snapshot-based andEvent-based . The
second level of the taxonomy further divides these
two macro-categories based on the techniques used
to manage the temporal dependencies. The leaves of
the taxonomy in Figure 2 correspond to the individ-
ual models, with a colored symbol indicating their
main underlying technology.
In the following we will denote as REC(v1, . . . , v k) a
network that can process streams of tensors v1, . . . , v k
and predict the next one in the sequence. This is
usually a Recurrent Neural Network (from which we
set the name REC), but other mechanisms such as
temporal attention can be used.
6.1 Snapshot-based models
Snapshot-based models are specifically tailored for
STGs (see Def. 4) and thus, consistently with the
9Figure 2: The proposed TGNN taxonomy and an analysis of the surveyed methods. The top
panel shows the new categories introduced in this work with the corresponding model instances (Section 6),
where the colored bullets additionally indicate the main technology that they employ. The bottom table
maps these methods to the task (Section 5) to which they have been applied in the respective original
paper, with an additional indication of their use in the future (F), past (P), inductive (I), or transductive
(T) settings (Section 5.1). Notice that no method has been applied yet to clustering and visualization, for
neither graphs nor nodes. Moreover, only four out of ten models have been tested in the past mode (three
in PT, one in PI).
definition, they are equipped with a suitable method
to process the entire graph at each point in time, and
with a mechanism that learns the temporal depen-
dencies across time-steps. Based on the mechanism
used, we can further distinguish between Model Evo-
lution andEmbedding Evolution methods.
6.1.1 Model Evolution methods
We call Model Evolution the evolution of the param-
eters of a static GNN model over time. This mecha-
nism is appropriate for modelling STG, as the evolu-
tion of the model is performed at the snapshot level.
More formally, these methods learn an embedding
hv(ti) =GNN(v, Gi;θ(ti)), where θ(ti) =REC(θ(ti−j) :
1≤j≤imax) is a parameter-evolution network, and
imaxis the memory length.
To the best of our knowledge, the only ex-
isting method belonging to this category isEvolveGCN [61]. This model utilizes a Recurrent
Neural Network (RNN) to update the Graph Con-
volutional Network (GCN) [39] parameters at each
time-step, allowing for model adaptation that is not
constrained by the presence or absence of nodes.
The method can effectively handle new nodes with-
out prior historical information. A key advantage
of this approach is that the GCN parameters are no
longer trained directly, but rather they are computed
from the trained RNN, resulting in a more manage-
able model size that does not increase with the num-
ber of time-steps. The paper presents two versions
of this method: EvolveGCN-O uses a Long Short-
Term Memory (LSTM) to simply evolve the weights
in time, while EvolveGCN-H represents the weights
as hidden states of a Gated Recurrent Unit (GRU),
whose input is the previous node embedding.
106.1.2 Embedding Evolution methods
Rather than evolving the parameters of a static GNN
model, Embedding Evolution methods focus on evolv-
ing the embeddings produced by a static model.
This means to learn a node embedding hv(ti) =
REC(hv(ti−j), i= 1, . . . , t max) as the evolution of pre-
vious embeddings, where hv(ti−j) =GNN(v, Gi−j;θ)
are GNN embeddings for the SG Gi−j.
There are several different TGNN models that fall
under this category. These networks differ from one
another in the techniques used for processing both
the structural information and the temporal dynam-
ics of the STGs. DySAT [70] introduces a general-
ization of Graph Attention Network (GAT) [81] for
STGs. First, it uses a self-attention mechanism to
generate static node embeddings at each timestamp.
Then, it uses a second self-attention block to pro-
cess past temporal embeddings for a node to gener-
ate its novel embedding. Decoupling graph evolution
into two modular blocks allows for efficient computa-
tions of temporal node representations. The struc-
tural and temporal self-attention layers, combined
and stacked, enable flexibility and scalability. The
VGRNN model [29] uses VGAE [40] on each snap-
shot, where the latent representations are conditioned
on a state variable modelled by Semi-Implicit Vari-
ational Inference (SIVI) [98] to handle the variation
of the graph over time. The learned latent repre-
sentation is then evolved through an LSTM condi-
tioned on the previous time’s latent representation,
allowing the model to predict the future evolution
of the graph. ROLAND [100] is a general frame-
work for extending state-of-the-art GNN techniques
to STGs. The key insight is that node embeddings
at different GNN layers can be viewed as hierarchical
node states. To generalize a static GNN for dynamic
settings, hierarchical node states are updated based
on newly observed nodes and edges through a Gated
Recurrent Unit (GRU) update module [9]. The pa-
per presents two versions of the model: ROLAND-
MLP, which uses a 2-layer MLP to update node em-
beddings, and ROLAND- moving average , which up-
dates the node embeddings through the moving aver-
age among previous node embeddings. Finally, reser-
voir computing techniques have also been proposed.DynGESN [55] presents a method where each node
embedding is updated by a recurrent mechanism us-
ing its temporal neighborhood and previous embed-
ding, with fixed and randomly initialized recurrent
weights. SSGNN [10] follows a similar approach but
introduces trainable parameters in the decoder and
combines randomized components in the encoder:
initially, the encoder creates representations of the
time series data observed at each node, by utilizing
a reservoir that captures dynamics at various time
scales; these representations are then further pro-
cessed to incorporate spatial dynamics dictated by
the graph structure.
6.2 Event-based models
Models belonging to the Event-based macro category
are designed to process ETGs (see Def. 5). These
models are able to process streams of events by in-
corporating techniques that update the representa-
tion of a node whenever an event involving that node
occurs, and they are an extension of message pass-
ing to TGs, since they combine and aggregate node
representations over temporal neighborhoods.
The models that lie in this macro category can be
further classified in Temporal Embedding andTempo-
ral Neighborhood methods, based on the technology
used to learn the time dependencies. In particular,
theTemporal Embedding models use recurrent or self-
attention mechanisms to model sequential informa-
tion from streams of events, while also incorporating
a time encoding. This allows for temporal signals to
be modeled by the interaction between time embed-
ding, node features and the topology of the graph.
Temporal Neighborhood models, instead, use a mod-
ule that stores functions of events involving a specific
node at a given time. These values are then aggre-
gated and used to update the node representation as
time progresses.
6.2.1 Temporal Embedding methods
Temporal embedding methods model TGs by com-
bining time embedding, node features, and graph
topology. These models use an explicit functional
time encoding, i.e., a vector embedding gtof time
11based on Random Fourier Features (RFF) [64], which
is translation-invariant (i.e., it depends only on the
elapsed and not the absolute time).
They extend the message passing architecture to
temporal neighborhoods, where the time is encoded
bygt, i.e.,
hv(t) =COMBINE ((hv(t),g0),
AGGREGATE ({(hu(t′),gt−t′), u∈ NT[v]}))
where t′is the time of the connection event between u
andv. Thus, gt−t′encodes the time elapsed between
the current time tand the time of connection between
uandv.
TGAT [92], for example, introduces a graph-
temporal attention mechanism which works on the
embeddings of the temporal neighbors of a node,
where the positional encoding is replaced by a tempo-
ral encoding based on RFFs. In addition, [92] imple-
ment a version of TGAT with all temporal attention
weights set to an equal value (Const-TGAT). On the
other hand, NAT [50] collects the temporal neigh-
bors of each node into dictionaries, and then it learns
the node representation with a recurrent mechanism,
using the historical neighborhood of the current node
and a RFF based time embedding. Note that [50]
propose a dedicated data structure to support paral-
lel access and update of the dictionary on GPUs.
6.2.2 Temporal Neighborhood methods
TheTemporal Neighborhood class includes all TGNN
models that make use of a special mailbox module
to update node embeddings based on events. When
an event εoccurs, a function is evaluated on the de-
tails of the event to compute a mail or a message
mε. For example, when a new edge appears between
two nodes, a message is produced, taking into ac-
count the time of occurrence of the event, the node
features, and the features of the new edge. The node
representation is then updated at each time by aggre-
gating all the generated messages. In more details,
these methods extend message passing by learning an
embedding
hv(t) =COMBINE (hv(t),
AGGREGATE (
mε, ε=ε±
E= (u, t′) with u∈ NT[v]	
)),where ε±
E, with u∈ NT[v], is the addition or deletion
of a temporal neighbor of v.
Several existing TGNN methods belong to this cat-
egory. APAN [83] introduces the concept of asyn-
chronous algorithm, which decouples graph query
and model inference. An attention-based encoder
maps the content of the mailbox to a latent repre-
sentation of each node, which is decoded by an MLP
adapted to the downstream task. After each node
update following an event, mails containing the cur-
rent node embedding are sent to the mailboxes of its
neighbors using a propagator. DGNN [51] combines
aninteract module — which generates an encoding of
each event based on the current embedding of the in-
teracting nodes and its history of past interactions —
and a propagate module — which transmits the up-
dated encoding to each neighbors of the interacting
nodes. The aggregation of the current node encoding
with those of its temporal neighbors uses a modified
LSTM, which permits to work on non-constant time-
steps, and implements a discount factor to down-
weight the importance of remote interactions. TGN
[66] provides a generic framework for representation
learning in ETGs, and it makes an effort to integrate
the concepts put forward in earlier techniques. This
inductive framework is made up of separate and inter-
changeable modules. Each node the model has seen
so far is characterized by a memory vector, which is
a compressed representation of all its past interac-
tions. Given a new event, a mailbox module com-
putes a mail for every node involved. Mails will then
be used to update the memory vector. To overcome
the so-called staleness problem [37], an embedding
module computes, at each timestamp, the node em-
beddings using their neighborhood and their memory
states. Finally, TGL [106] is a general framework
for training TGNNs on graphs with billions of nodes
and edges by using a distributed training approach.
In TGL, a mailbox module is used to store a limited
number of the most recent interactions, called mails.
When a new event occurs, the node memory of the
relevant nodes is updated using the cached messages
in the mailbox. The mailbox is then updated after
the node embeddings are calculated. This process
is also used during inference to ensure consistency in
the node memory, even though updating the memory
12is not required during this phase.
6.3 Category comparison
The categories of models identified in our taxonomy
exhibit various strengths, weaknesses, or suitability
for specific scenarios. First and foremost, the com-
parison between the macro categories of Snapshot-
based and Event-based methods is straightforward,
hinging on the choice of temporal graph representa-
tion, namely STGs or ETGs. Within each macro
category, sub-categories exhibit their own set of ad-
vantages and disadvantages. For instance, in the
Model Evolution category, learning the evolution of
GNN parameters becomes complex when the GNN
has a large number of parameters. On the other
hand, the Embedding Evolution category has a limi-
tation in that temporal learning exclusively relies on
recurrent mechanisms, which may not fully guaran-
tee the preservation of temporal correlations among
substructures. Despite this drawback, the approach
offers simplicity and intuitiveness, allowing for the
exploration and evaluation of various recurrent mech-
anisms.
In Temporal Embedding methods, defining the
time encoding function is not trivial because it should
capture different aspects of the graph, such as the
temporal periodicity of interactions, recurrent inter-
actions over time, and more. One advantage of this
category, though, is that ad hoc time encoding func-
tions can be defined, depending on the application
domain.
Finally, for the Temporal Neighborhood category,
constructing the mailbox can be complex. For exam-
ple, dense graphs have large mailboxes, which neces-
sitate managing scalability issues. A specific mech-
anism to decide which nodes to include in the mail-
box needs to be carefully designed, and this mecha-
nism may also be domain-dependent. Similarly to the
Temporal Embedding category, a benefit of the Tem-
poral Neighborhood models is their ability to define
domain-specific mailbox mechanisms.
In summary, each category of models for tempo-
ral graph learning has its own set of advantages and
disadvantages. Choosing a category depends on the
representation of the input graph, the complexity oflearning the temporal dynamics, the need for domain-
specific encoding or mailbox mechanisms, and scala-
bility considerations.
7 Open challenges
Building on existing libraries of GNN methods, two
major TGNN libraries have been developed, namely
PyTorch Geometric Temporal (PyGT) [68], based on
PyTorch Geometric1, and DynaGraph [27], based on
Deep Graph Library2. While these are substantial
contributions to the development and practical ap-
plication of TGNN models, several open challenges
still need to be faced to fully exploit the potential of
this technology. We discuss the ones we believe are
the most relevant in the following.
Evaluation The evaluation of GNN models has
been greatly enhanced by the Open Graph
Benchmark (OGB) [34], which provides a stan-
dardized evaluation protocol and a collection of
graph datasets enabling a fair and consistent
comparison between GNN models. An equally
well-founded standardized benchmark for eval-
uating TGNNs does not currently exist. As a
result, each model has been tested on its own
selection of datasets, making it challenging to
compare and rank different TGNNs on a fair ba-
sis. For instance, [106] introduced two real-world
datasets with 0.2 billion and 1.3 billion temporal
edges which allow to evaluate the scalability of
TGNNs to large scale real-world scenarios, but
they only tested the TGL model [106]. The va-
riety and the complexity of learning settings and
tasks described in Section 5 makes a standardiza-
tion of tasks, datasets and processing pipelines
especially crucial to allow a fair assessment of
the different approaches and foster innovation in
the field.
Another crucial aspect of evaluating GNN mod-
els is explainability, which is the ability to in-
terpret and understand their decision process.
While explainability has been largely explored
1https://pytorch-geometric.readthedocs.io
2https://docs.dgl.ai/
13for standard GNNs [49, 99, 46, 2], only few works
focused on explaining TGNNs [90, 82, 31].
Expressiveness Driven by the popularity of (static)
GNNs, the study of their expressive power
has received a lot of attention in the last few
years [72]. For instance, appropriate formula-
tions of message-passing GNNs have been shown
to be as powerful as the Weisfeiler-Lehman
isomorphism test (WL test) in distinguishing
graphs or nodes [95], and higher-order gener-
alizations of message-passing GNNs have been
proposed that can match the expressivity of the
k-WL test [57]. Moreover, it has been proven
that GNNs are a sort of universal approximators
on graphs modulo the node equivalence induced
by the WL test [16]. Finally, also the expressive
power of GNNs equipped with pooling operators
have been studied in [5].
Conversely, the expressive power of TGNNs is
still far from being fully explored, and the de-
sign of new WL tests, suitable for TGNNs, is a
crucial step towards this aim. This is a challeng-
ing task since the definition of a node neighbor-
hood in temporal graphs is not as trivial as for
static graphs, due to the appearing/disappearing
of nodes and edges. In [4], a new version of the
WL test for temporal graphs has been proposed,
applicable only to DTTGs. Instead, [75] pro-
posed a novel WL test for ETGs, and the TGN
model [66] has been proved to be as powerful as
this test. Finally, [4] proved a universal approx-
imation theorem, but the result just holds for
a specific TGNN model for STGs, composed of
standard GNNs stacked with an RNN.
To the best of our knowledge, these are the
only results achieved so far on the expressive
power of TGNNs. A complete theory of the WL
test for the different TG representations, such
as universal approximation theorems for event-
based models, is still lacking. Moreover, no ef-
forts have been made to incorporate higher-order
graph structures to enhance the expressiveness
of TGNNs. This task is particularly demand-
ing, since it requires not only the definition of
the temporal counterpart of the k-WL test butalso some techniques to scale to large datasets.
Indeed, a drawback of considering higher-order
structures is that of high memory consumption,
which can only get worse in the case of TGs,
as they usually have a greater number of nodes
than static graphs.
Learnability Training standard GNNs over large
and complex graph data is highly non-trivial, of-
ten resulting in problems such as over-smoothing
and over-squashing. A theoretical explanation
for this difficulty has been given using algebraic
topology and Sheaf theory [6, 78]. More intu-
itively, we yet do not know how to reproduce the
breakthrough obtained in training very deep ar-
chitectures over vector data when training deep
GNNs. Such a difficulty is even more challeng-
ing with TGNNs, because the typical long-term
dependency of TGs poses additional problems to
those due to over-smoothing and over-squashing.
Modern static GNN models face the problems
arising from the complexity of the data using
techniques such as dropout, virtual nodes, neigh-
bor sampling, but a general solution is far from
being reached. The extension of the above men-
tioned techniques to TGNNs, and the corre-
sponding theoretical studies, are open challenges
and we are aware of only one work towards this
goal [97]. On the other hand, the goal of propos-
ing general very deep TGNNs is even more chal-
lenging due to the difficulty in designing the
graph dynamics in a hierarchical fashion.
Real-world applications The analysis of the tasks
in Section 5 revealed several opportunities for
the use of TGNNs far beyond their current scope
of application. We would like to outline here
some promising directions of application.
A challenging and potentially disruptive direc-
tion for the application of TGNNs is the learn-
ing of dynamical systems through the combi-
nation of machine learning and physical knowl-
edge [86]. Physic Informed Neural Networks
(PINNs) [65] are already revolutionizing the field
of scientific computing [11], and static GNNs
have been employed in this framework with great
14success [62, 21]. Adapting TGNNs to this field
may enable to carry over these results to the
treatment of time-dependent problems. Climate
science [18] is a particularly attractive field of
application, both for its critical impact in our
societies and for the promising results achieved
by GNNs in climate modelling tasks [38]. We
believe that TGNNs may rise to be a prominent
technology in this field, thanks to their unique
capability to capture spatio-temporal correla-
tions at multiple scales. Epidemics studies are
another topic of enormous everyday impact that
may be explored through the lens of TGNNs,
since a proper modelling of the spreading dy-
namics needs to be tightly coupled to the un-
derlying TG structure [17]. Both fields requires
a better development of TGNNs for regression
problems, a task that is still underdeveloped (see
Section 5).
8 Conclusion
GNN based models for temporal graphs have become
a promising research area. However, we believe that
the potential of GNNs in this field has only been par-
tially explored. In this work, we propose a system-
atic formalization of tasks and learning settings for
TGNNs, which was lacking in the literature, and a
comprehensive taxonomy categorizing existing meth-
ods and highlighting unaddressed tasks. Building on
this systematization of the current state-of-the-art,
we discuss open challenges that need to be addressed
to unleash the full potential of TGNNs. We con-
clude by stressing the fact that the issues open to
date are very challenging, since they presuppose con-
sidering both the temporal and relational dimension
of data, suggesting that forthcoming new computa-
tional models must go beyond the GNN framework
to provide substantially better solutions.
15Acknowledgments
This research was partially supported by TAILOR,
a project funded by EU Horizon 2020 research and
innovation programme under GA No 952215.
References
[1] N.M. Ahmed, L. Chen, Y. Wang, B. Li, Y. Li,
and W. Li. DeepEye: Link prediction in dy-
namic networks based on non-negative matrix
factorization. Big Data Mining and Analytics ,
2018.
[2] S. Azzolin, A. Longa, P. Barbiero, P. Li` o, and
A. Passerini. Global explainability of GNNs via
logic combination of learned concepts. arXiv
preprint arXiv:2210.07147 , 2022.
[3] C.D.T. Barros, M.R.F. Mendon¸ ca, A. Vieira,
and A. Ziviani. A survey on embedding dy-
namic graphs. ACM CSUR , 2021.
[4] S. Beddar-Wiesing, G.A. D’Inverno,
C. Graziani, V. Lachi, A. Moallemy-Oureh,
F. Scarselli, and J.M. Thomas. Weisfeiler–
Lehman goes dynamic: An analysis of the
expressive power of graph neural networks for
attributed and dynamic graphs. arXiv preprint
arXiv:2210.03990 , 2022.
[5] F.M. Bianchi and V. Lachi. The expressive
power of pooling in graph neural networks.
arXiv preprint arXiv:2304.01575 , 2023.
[6] C. Bodnar, F. Di Giovanni, B.P. Chamberlain,
P. Li` o, and M. Bronstein. Neural sheaf diffu-
sion: A topological perspective on heterophily
and oversmoothing in GNNs. In ICLR , 2022.
[7] Lei Cai and Shuiwang Ji. A multi-scale ap-
proach for graph link prediction. In Proceed-
ings of the AAAI conference on artificial intel-
ligence , volume 34, pages 3308–3315, 2020.
[8] G. Cencetti, G. Santin, A. Longa, E. Pi-
gani, A. Barrat, C. Cattuto, S. Lehmann,
M. Salathe, and B. Lepri. Digital proximitytracing on empirical contact networks for pan-
demic control. Nature Communications , 2021.
[9] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio.
Empirical evaluation of gated recurrent neural
networks on sequence modeling. arXiv preprint
arXiv:1412.3555 , 2014.
[10] A. Cini, I. Marisca, F.M. Bianchi, and
C. Alippi. Scalable spatiotemporal graph neu-
ral networks. arXiv preprint arXiv:2209.06520 ,
2022.
[11] S. Cuomo, V.S. Di Cola, F. Giampaolo,
G. Rozza, M. Raissi, and F Piccialli. Scientific
machine learning through physics–informed
neural networks: where we are and what’s next.
Journal of Scientific Computing , 2022.
[12] Alexandre Darbon, Davide Colombi, Eugenio
Valdano, Lara Savini, Armando Giovannini,
and Vittoria Colizza. Disease persistence on
temporal contact networks accounting for het-
erogeneous infectious periods. Royal Society
open science , 6(1):181404, 2019.
[13] Alexandre Darbon, Eugenio Valdano, Chiara
Poletto, Armando Giovannini, Lara Savini,
Luca Candeloro, and Vittoria Colizza.
Network-based assessment of the vulnerabil-
ity of italian regions to bovine brucellosis.
Preventive veterinary medicine , 158:25–34,
2018.
[14] Shib Sankar Dasgupta, Swayambhu Nath Ray,
and Partha Talukdar. Hyte: Hyperplane-based
temporally aware knowledge graph embedding.
InProceedings of the 2018 conference on em-
pirical methods in natural language processing ,
pages 2001–2011, 2018.
[15] S. Deng, H. Rangwala, and Y. Ning. Learn-
ing dynamic context graphs for predicting so-
cial events. In ACM SIGKDD , 2019.
[16] G.A. D’Inverno, M. Bianchini, M.L. Sampoli,
and F. Scarselli. A new perspective on the
approximation capability of GNNs. arXiv
preprint arXiv:2106.08992 , 2021.
16[17] J. Enright and R.K. Rowland. Epidemics on
dynamic networks. Epidemics , 2018.
[18] J. H. Faghmous and V. Kumar. A Big Data
Guide to Understanding Climate Change: The
Case for Theory-Guided Data Science. Big
Data, 2(3), 2014.
[19] W. Fan, Y. Ma, Q. Li, Y. He, E. Zhao, J. Tang,
and D. Yin. Graph neural networks for social
recommendation. WWW ’19, New York, NY,
USA, 2019. Association for Computing Machin-
ery.
[20] C. Gao, X. Wang, X. He, and Y. Li. Graph neu-
ral networks for recommender system. WSDM
’22, page 1623–1625, New York, NY, USA,
2022. Association for Computing Machinery.
[21] H. Gao, M.J. Zahr, and J. Wang. Physics-
informed graph neural Galerkin networks: A
unified framework for solving PDE-governed
forward and inverse problems. Computer Meth-
ods in Applied Mechanics and Engineering ,
2022.
[22] Hongyang Gao, Zhengyang Wang, and Shui-
wang Ji. Large-scale learnable graph con-
volutional networks. In Proceedings of the
24th ACM SIGKDD international conference
on knowledge discovery & data mining , pages
1416–1424, 2018.
[23] R. Gao, J. Yan, P. Li, and L. Chen. Detect-
ing the critical states during disease develop-
ment based on temporal network flow entropy.
Briefings in Bioinformatics , 2022.
[24] S. Gao. Spatio-temporal analytics for exploring
human mobility patterns and urban dynamics
in the mobile age. Spatial Cognition & Compu-
tation , 15(2):86–114, 2015.
[25] Johannes Gasteiger, Aleksandar Bojchevski,
and Stephan G¨ unnemann. Predict then propa-
gate: Graph neural networks meet personalized
pagerank. arXiv preprint arXiv:1810.05997 ,
2018.[26] Palash Goyal, Nitin Kamra, Xinran He,
and Yan Liu. Dyngem: Deep embedding
method for dynamic graphs. arXiv preprint
arXiv:1805.11273 , 2018.
[27] M. Guan, A.P. Iyer, and T. Kim. DynaGraph:
dynamic graph neural networks at scale. In
ACM SIGMOD22 GRADES-NDA , 2022.
[28] S. Gupta and S. Bedathur. A survey
on temporal graph representation learning
and generative modeling. arXiv preprint
arXiv:2208.12126 , 2022.
[29] E. Hajiramezanali, A. Hasanzadeh,
K. Narayanan, N. Duffield, M. Zhou, and
X. Qian. Variational graph recurrent neural
networks. NeurIPS , 32, 2019.
[30] W. Hamilton, Z. Ying, and J. Leskovec. In-
ductive representation learning on large graphs.
NeurIPS , 30, 2017.
[31] W. He, M. N. Vu, Z. Jiang, and M. T. Thai. An
explainer for temporal graph neural networks.
InGLOBECOM - IEEE Global Communica-
tions Conference 2022 , pages 6384–6389. IEEE,
2022.
[32] M.P. Van Den Heuvel, R.C.W. Mandl, C.J.
Stam., R.S. Kahn, P. Hulshoff, and E. Hilleke.
Aberrant frontal and temporal complex net-
work structure in schizophrenia: A graph theo-
retical analysis. Journal of Neuroscience , 2010.
[33] Pietro Hiram Guzzi, Francesco Petrizzelli, and
Tommaso Mazza. Disease spreading modeling
and analysis: A survey. Briefings in Bioinfor-
matics , 23(4):bbac230, 2022.
[34] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren,
B. Liu, M. Catasta, and J. Leskovec. Open
Graph Benchmark: Datasets for machine learn-
ing on graphs. NeurIPS , 33:22118–22133, 2020.
[35] GM Jacquez, JR Meliker, RR Rommel, and
PE Goovaerts. Exposure reconstruction using
space-time information technology. Encyclope-
dia of Environmental Health , pages 793–804,
2019.
17[36] Weiwei Jiang and Jiayun Luo. Graph neural
network for traffic forecasting: A survey. Expert
Systems with Applications , 207:117921, 2022.
[37] S.M. Kazemi, R. Goel, K. Jain, I. Kobyzev,
A. Sethi, P. Forsyth, and P. Poupart. Represen-
tation learning for dynamic graphs: A survey.
Journal of Machine Learning Research , 2020.
[38] R. Keisler. Forecasting global weather
with graph neural networks. arXiv preprint
arXiv:2202.07575 , 2022.
[39] T.N. Kipf and M. Welling. Semi-supervised
classification with graph convolutional net-
works. In ICLR 2016 .
[40] T.N. Kipf and M. Welling. Variational
graph auto-encoders. arXiv preprint
arXiv:1611.07308 , 2016.
[41] Andreas Koher, Hartmut HK Lentz, James P
Gleeson, and Philipp H¨ ovel. Contact-based
model for epidemic spreading on temporal net-
works. Physical Review X , 9(3):031017, 2019.
[42] Srijan Kumar, Xikun Zhang, and Jure
Leskovec. Learning dynamic embeddings
from temporal interactions. arXiv preprint
arXiv:1812.02289 , 2018.
[43] PAW Lewis. Multivariate point processes.
InProceedings of the Berkeley Symposium on
Mathematical Statistics and Probability , vol-
ume 1, page 401. University of California Press,
1972.
[44] Z. Liu, D. Zhou, Y. Zhu, J. Gu, and J. He.
Towards fine-grained temporal network repre-
sentation via time-reinforced random walk. In
AAAI , volume 34, pages 4973–4980, 2020.
[45] Karmela Ljubiˇ ci´ c, Andro Mer´ cep, and Zvonko
Kostanjˇ car. Analysis of complex customer net-
works: A real-world banking example. In
2022 45th Jubilee International Convention on
Information, Communication and Electronic
Technology (MIPRO) , pages 321–326. IEEE,
2022.[46] A. Longa, S. Azzolin, G. Santin, G. Cencetti,
P. Li` o, B. Lepri, and A. Passerini. Ex-
plaining the explainers in graph neural net-
works: a comparative study. arXiv preprint
arXiv:2210.15304 , 2022.
[47] A. Longa, G. Cencetti, S. Lehmann,
A. Passerini, and B. Lepri. Neighbourhood
matching creates realistic surrogate temporal
networks. arXiv preprint arXiv:2205.08820 ,
2022.
[48] A. Longa, G. Cencetti, B. Lepri, and
A. Passerini. An efficient procedure for min-
ing egocentric temporal motifs. Data Mining
and Knowledge Discovery , 2022.
[49] D. Luo, W. Cheng, D. Xu, W. Yu, B. Zong,
H. Chen, and X. Zhang. Parameterized ex-
plainer for graph neural network. Advances
in Neural Information Processing Systems ,
33:19620–19631, 2020.
[50] Y. Luo and P. Li. Neighborhood-aware scal-
able temporal network representation learning.
arXiv preprint arXiv:2209.01084 , 2022.
[51] Y. Ma, Z. Guo, Z. Ren, J. Tang, and D. Yin.
Streaming graph neural networks. In ACM SI-
GIR, 2020.
[52] G. Mauro, M. Luca, A. Longa, B. Lepri, and
L. Pappalardo. Generating mobility networks
with generative adversarial networks. EPJ
Data Science , 11(1):58, 2022.
[53] I.W. McBrearty and G.C. Beroza. Earthquake
location and magnitude estimation with graph
neural networks. In IEEE ICIP , 2022.
[54] Jaymie R Meliker, Geoffrey M Jacquez, Pierre
Goovaerts, Glenn Copeland, and May Yassine.
Spatial cluster analysis of early stage breast
cancer: a method for public health practice
using cancer registry data. Cancer Causes &
Control , 20:1061–1069, 2009.
[55] A. Micheli and D. Tortorella. Discrete-time dy-
namic graph echo state networks. Neurocom-
puting , 496:85–95, 2022.
18[56] Federico Monti, Davide Boscaini, Jonathan
Masci, Emanuele Rodola, Jan Svoboda, and
Michael M Bronstein. Geometric deep learning
on graphs and manifolds using mixture model
cnns. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages
5115–5124, 2017.
[57] C. Morris, M. Ritzert, M. Fey, W.L. Hamilton,
J.E. Lenssen, G. Rattan, and M. Grohe. Weis-
feiler and Leman go neural: Higher-order graph
neural networks. In AAAI , 2019.
[58] Audun Myers, David Mu˜ noz, Firas A Kha-
sawneh, and Elizabeth Munch. Temporal net-
work analysis using zigzag persistence. EPJ
Data Science , 12(1):6, 2023.
[59] F. L. Opolka, A. Solomon, C. Cangea,
P. Veliˇ ckovi´ c, P. Li` o, and R.D. Hjelm. Spatio-
temporal deep graph infomax. arXiv preprint
arXiv:1904.06316 , 2019.
[60] Al Ozonoff, Thomas Webster, Veronica Vieira,
Janice Weinberg, David Ozonoff, and Ann As-
chengrau. Cluster detection methods applied to
the upper cape cod cancer data. Environmental
Health , 4:1–9, 2005.
[61] A. Pareja, G. Domeniconi, J. Chen, T. Ma,
T. Suzumura, H. Kanezashi, T. Kaler,
T. Schardl, and C. Leiserson. EvolveGCN:
Evolving graph convolutional networks for dy-
namic graphs. In AAAI , 2020.
[62] T. Pfaff, M. Fortunato, A. Sanchez-Gonzalez,
and P. Battaglia. Learning mesh-based simula-
tion with graph networks. In ICLR , 2021.
[63] M. Qin and D Yeung. Temporal link predic-
tion: A unified framework, taxonomy, and re-
view. arXiv preprint arXiv:2210.08765 , 2022.
[64] A. Rahimi and B. Recht. Random features for
large-scale kernel machines. In NeurIPS , 2008.
[65] M. Raissi, P. Perdikaris, and G.E. Karniadakis.
Physics informed deep learning (part i): Data-
driven solutions of nonlinear partial differentialequations. arXiv preprint arXiv:1711.10561 ,
2017.
[66] E. Rossi, B. Chamberlain, F. Frasca, D. Ey-
nard, F. Monti, and M. Bronstein. Temporal
graph networks for deep learning on dynamic
graphs. arXiv preprint arXiv:2006.10637 , 2020.
[67] Asma Rosyidah, Isti Surjandari, et al. Explor-
ing customer data using spatio-temporal anal-
ysis: Case study of fixed broadband provider.
International Journal of Applied Science and
Engineering , 16(2):133–147, 2019.
[68] B. Rozemberczki, P. Scherer, Y. He,
G. Panagopoulos, A. Riedel, M. Astefanoaei,
O. Kiss, F. Beres, G. L´ opez, N. Collignon, et al.
Pytorch Geometric Temporal: Spatiotemporal
signal processing with neural machine learning
models. In ACM CIKM , 2021.
[69] Xiaolei Ru, Jack Murdoch Moore, Xin-Ya
Zhang, Yeting Zeng, and Gang Yan. Infer-
ring patient zero on temporal networks via
graph neural networks. In Proceedings of
the AAAI Conference on Artificial Intelligence ,
volume 37, pages 9632–9640, 2023.
[70] A. Sankar, Y. Wu, L. Gou, W. Zhang, and
H. Yang. Dysat: Deep neural representation
learning on dynamic graphs via self-attention
networks. In WSDM , 2020.
[71] Koya Sato, Mizuki Oka, Alain Barrat, and Ciro
Cattuto. Dyane: dynamics-aware node em-
bedding for temporal networks. arXiv preprint
arXiv:1909.05976 , 2019.
[72] R. Sato. A survey on the expressive power
of graph neural networks. arXiv preprint
arXiv:2003.04078 , 2020.
[73] J. Skarding, B. Gabrys, and K. Musial. Foun-
dations and modeling of dynamic networks us-
ing dynamic graph neural networks: A survey.
IEEE Access , 9:79143–79168, 2021.
19[74] M. K.P. So, A. Tiwari, A. M.Y. Chu, J. T.Y.
Tsang, and J. N.L. Chan. Visualizing covid-
19 pandemic risk through network connected-
ness. International Journal of Infectious Dis-
eases , 96:558–561, Jul 2020.
[75] A.H. Souza, D. Mesquita, S. Kaski, and
V. Garg. Provably expressive temporal graph
networks. NeurIPS , 2022.
[76] Aynaz Taheri, Kevin Gimpel, and Tanya
Berger-Wolf. Learning to represent the evo-
lution of dynamic graphs with recurrent mod-
els. In Companion proceedings of the 2019 world
wide web conference , pages 301–307, 2019.
[77] Josephine M Thomas, Alice Moallemy-Oureh,
Silvia Beddar-Wiesing, and Clara Holzh¨ uter.
Graph neural networks designed for differ-
ent graph types: A survey. arXiv preprint
arXiv:2204.03080 , 2022.
[78] J. Topping, F. Di Giovanni, B.P. Chamberlain,
X. Dong, and M. Bronstein. Understanding
over-squashing and bottlenecks on graphs via
curvature. arXiv preprint arXiv:2111.14522 ,
2021.
[79] R. Trivedi, M. Farajtabar, P. Biswal, and
H. Zha. Dyrep: Learning representations over
dynamic graphs. In International Conference
on Learning Representations , 2019.
[80] Rakshit Trivedi, Hanjun Dai, Yichen Wang,
and Le Song. Know-evolve: Deep temporal rea-
soning for dynamic knowledge graphs. In inter-
national conference on machine learning , pages
3462–3471. PMLR, 2017.
[81] P. Veliˇ ckovi´ c, G. Cucurull, A. Casanova,
A. Romero, P. Li` o, and Y. Bengio.
Graph attention networks. arXiv preprint
arXiv:1710.10903 , 2017.
[82] M. N. Vu and M. T. Thai. On the limit of ex-
plaining black-box temporal graph neural net-
works. arXiv preprint arXiv:2212.00952 , 2022.[83] X. Wang, D. Lyu, M. Li, Y. Xia, Q. Yang,
X. Wang, X. Wang, P. Cui, Y. Yang, B. Sun,
et al. APAN: Asynchronous propagation at-
tention network for real-time temporal graph
embedding. In SIGMOD , 2021.
[84] Y. Wang, Y. Chang, Y. Liu, J. Leskovec, and
P. Li. Inductive representation learning in tem-
poral networks via causal anonymous walks.
arXiv preprint arXiv:2101.05974 , 2021.
[85] David C Wheeler. A comparison of spatial
clustering and cluster detection techniques for
childhood leukemia incidence in ohio, 1996–
2003. International journal of health geograph-
ics, 6(1):1–16, 2007.
[86] J. Willard, X. Jia, S. Xu, M. Steinbach, and
V. Kumar. Integrating scientific knowledge
with machine learning for engineering and envi-
ronmental systems. ACM Computing Surveys ,
55(4), 2022.
[87] Felix Wu, Amauri Souza, Tianyi Zhang,
Christopher Fifty, Tao Yu, and Kilian Wein-
berger. Simplifying graph convolutional net-
works. In International conference on machine
learning , pages 6861–6871. PMLR, 2019.
[88] S. Wu, F. Sun, W. Zhang, X. Xie, and B. Cui.
Graph neural networks in recommender sys-
tems: a survey. ACM CSUR , 2022.
[89] Feng Xia, Ke Sun, Shuo Yu, Abdul Aziz, Liang-
tian Wan, Shirui Pan, and Huan Liu. Graph
learning: A survey. IEEE Transactions on Ar-
tificial Intelligence , 2(2):109–127, 2021.
[90] W. Xia, M. Lai, C. Shan, Y. Zhang, X. Dai,
X. Li, and D. Li. Explaining temporal graph
models through an explorer-navigator frame-
work. In The Eleventh International Confer-
ence on Learning Representations .
[91] Y. Xie, C. Li, B. Yu, C. Zhang, and Z. Tang. A
survey on dynamic network embedding. arXiv
preprint arXiv:2006.08093 , 2020.
20[92] D. Xu, C. Ruan, E. Korpeoglu, S. Ku-
mar, and K. Achan. Inductive representation
learning on temporal graphs. arXiv preprint
arXiv:2002.07962 , 2020.
[93] Dongkuan Xu, Wei Cheng, Dongsheng Luo,
Xiao Liu, and Xiang Zhang. Spatio-temporal
attentive rnn for node classification in temporal
attributed graphs. In IJCAI , pages 3947–3953,
2019.
[94] Dongkuan Xu, Junjie Liang, Wei Cheng,
Hua Wei, Haifeng Chen, and Xiang Zhang.
Transformer-style relational reasoning with dy-
namic memory updating for temporal network
modeling. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , volume 35, pages
4546–4554, 2021.
[95] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How
powerful are graph neural networks? arXiv
preprint arXiv:1810.00826 , 2018.
[96] G. Xue, M. Zhong, J. Li, J. Chen, C. Zhai, and
R. Kong. Dynamic network embedding survey.
Neurocomputing , 2022.
[97] M. Yang, Z. Meng, and I. King. Featurenorm:
L2 feature normalization for dynamic graph
embedding. In ICDM , 2020.
[98] M. Yin and M. Zhou. Semi-implicit variational
inference. In ICML , 2018.
[99] Z. Ying, D. Bourgeois, J. You, M. Zitnik, and
J. Leskovec. Gnnexplainer: Generating expla-
nations for graph neural networks. Advances
in Neural Information Processing Systems , 32,
2019.
[100] J. You, T. Du, and J. Leskovec. ROLAND:
graph learning framework for dynamic graphs.
InACM SIGKDD , 2022.
[101] B. Yu, H. Yin, and Z. Zhu. Spatio-temporal
graph convolutional networks: A deep learn-
ing framework for traffic forecasting. arXiv
preprint arXiv:1709.04875 , 2017.[102] M. Zhang and Y. Chen. Link prediction based
on graph neural networks. NeurIPS , 2018.
[103] Muhan Zhang and Yixin Chen. Weisfeiler-
lehman neural machine for link prediction. In
Proceedings of the 23rd ACM SIGKDD inter-
national conference on knowledge discovery and
data mining , pages 575–583, 2017.
[104] Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep
learning on graphs: A survey. IEEE Trans-
actions on Knowledge and Data Engineering ,
34(1):249–270, 2020.
[105] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin
Song, Jingchao Ni, Wenchao Yu, Haifeng
Chen, and Wei Wang. Node classification in
temporal graphs through stochastic sparsifica-
tion and temporal structural convolution. In
Machine Learning and Knowledge Discovery
in Databases: European Conference, ECML
PKDD 2020, Ghent, Belgium, September 14–
18, 2020, Proceedings, Part III , pages 330–346.
Springer, 2021.
[106] H. Zhou, D. Zheng, I. Nisa, V. Ioanni-
dis, X. Song, and G. Karypis. TGL: A
general framework for temporal GNN train-
ing on billion-scale graphs. arXiv preprint
arXiv:2203.14883 , 2022.
21