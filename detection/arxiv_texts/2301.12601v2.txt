Regret Bounds for Markov Decision Processes with Recursive
Optimized Certainty Equivalents
Wenhao Xu1, Xuefeng Gao2, Xuedong He3
June 9, 2023
Abstract
The optimized certainty equivalent (OCE) is a family of risk measures that cover impor-
tant examples such as entropic risk, conditional value-at-risk and mean-variance models. In
this paper, we propose a new episodic risk-sensitive reinforcement learning formulation based
on tabular Markov decision processes with recursive OCEs. We design an efficient learning
algorithm for this problem based on value iteration and upper confidence bound. We derive
an upper bound on the regret of the proposed algorithm, and also establish a minimax lower
bound. Our bounds show that the regret rate achieved by our proposed algorithm has optimal
dependence on the number of episodes and the number of actions.
1 Introduction
Reinforcement learning (RL) studies the problem of sequential decision making in an unknown
environment by carefully balancing between exploration and exploitation (Sutton and Barto 2018).
In the classical setting, it describes how an agent takes actions to maximize expected cumulative re-
wards in an environment typically modeled by a Markov decision process (MDP, Puterman (2014)).
However, optimizing the expected cumulative rewards alone is often not sufficient in many practi-
cal applications such as finance, healthcare and robotics. Hence, it may be necessary to take into
account of the risk preferences of the agent in the dynamic decision process. Indeed, a rich body of
literature has studied risk-sensitive (and safe) RL, incorporating risk measures such as the entropic
risk measure and conditional value-at-risk (CVaR) in the decision criterion, see, e.g., Shen et al.
(2014), Garcıa and Fern´ andez (2015), Tamar et al. (2016), Chow et al. (2017), Prashanth L and Fu
(2018), Fei et al. (2020) and the references therein.
In this paper we study risk-sensitive RL for tabular MDPs with unknown transition probabilities
in the finite-horizon, episodic setting, where an agent interacts with the MDP in episodes of a fixed
length with finite state and action spaces. To incorporate risk sensitivity, we consider a broad and
important class of risk measures known as Optimized Certainty Equivalent (OCE, (Ben-Tal and
Teboulle 1986, 2007)). The OCE is a (nonlinear) risk function which assigns a random variable Xto
a real value, and it depends on a concave utility function, see Equation (1) for the definition. With
an appropriate choice of the utility function , OCE covers important examples of risk measures,
1Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong
Kong, China. Email: whxu@se.cuhk.edu.hk.
2Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong
Kong, China. Email: xfgao@se.cuhk.edu.hk.
3Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong
Kong, China. Email: xdhe@se.cuhk.edu.hk.
1arXiv:2301.12601v2  [cs.LG]  8 Jun 2023including the entropic risk, CVaR and mean-variance models, as special cases, so it is popular
in financial applications, such as portfolio optimization, and in the machine learning literature.
See Section 2.1 for details. Using this unified framework, we aim to develop efficient learning
algorithms for risk-sensitive RL with OCEs and provide worst-case regret bounds, where the regret
measures the sub-optimality of the learning algorithm compared to an optimal policy should the
model parameters be completely known.
We formulate a new risk-sensitive episodic RL problem with recursive OCEs. The conventional
objective in risk-sensitive MDPs (when the model is known) is to optimize a static risk mea-
sure/functional applied to the (possibly discounted) cumulative rewards over the decision horizon
(Howard and Matheson 1972, Marcus et al. 1997). Except for the entropic risk measure, this ap-
proach typically suffers from the time-inconsistency issue, which prevents one from directly applying
the dynamic programming principle (Artzner et al. 2007). In addition, the optimal policies can be
non-Markovian, and are often difficult to compute (Mannor and Tsitsiklis 2011, Du et al. 2022).
In view of the time-inconsistency issue and the computational difficulty, we consider an alternative
approach, which is to consider MDPs with recursive risk measures (Ruszczy´ nski 2010, Shen et al.
2013, B¨ auerle and Glauner 2022). In this approach, instead of optimizing a static risk measure
of the cumulative rewards, one optimizes the value function defined by a recursive application of
a risk measure at each period , which essentially replaces the expectation operator in the standard
value iteration by the risk measure (OCEs in our setting). This approach is also partly motivated
by recursive utilities in the economics literature (Kreps and Porteus 1978, Epstein and Zin 1989).
Indeed, our recursive OCE model is a special case of the so-called dynamic mixture-averse prefer-
ences, which have an axiomatic foundation (Sarver 2018) and is a special class of recursive utilities.
The recursive structure in our OCE model implies time consistency and dynamic programming,
which leads to a Bellman equation in a known environment; see for instance B¨ auerle and Glauner
(2022). Our formulation of episodic RL with recursive OCEs is built on this Bellman equation.
Due to the generality of OCE, our RL formulation unifies and generalizes several existing episodic
RL formulations in the literature, including standard risk-neutral RL (see, e.g., Azar et al. (2017)),
RL with entropic risk in Fei et al. (2020), and RL with iterated CVaR in Du et al. (2022). See
Section 2.2 for details.
A special case of OCE is the entropic risk measure, which is obtained by setting the utility
function in OCE to be an exponential function. In this special case, the recursive OCE model is
equivalent to applying the entropic risk measure to the cumulative reward over the entire decision
horizon. In general, the recursive OCE model and the model of applying OCE to the cumulative
reward directly are different and can be applied in different problems to account for different
attitudes toward risk. The former tends to lead to more conservative actions than the latter does,
because in the former the agent is concerned about risk in every step and in every state; see Du et al.
(2022) for a detailed discussion and a concrete example in this regard in the context of recursive
CVaR.
In this paper, we develop a model-based algorithm for risk-sensitive RL with recursive OCEs.
Our algorithm is a variant of the UCBVI (Upper Confidence Bound Value Iteration) algorithm in
Azar et al. (2017) for risk-neural RL. The main novelty in our algorithm design is that the bonus
term used to encourage exploration depends on the utility function in the specific OCE that one
considers. Theoretically, we prove regret bounds for our algorithm in learning MDPs with a wide
family of recursive risk measures including the mean-variance criterion, by considering different
2utility functions in OCEs. Such bounds are new to the literature, to the best of our knowledge.
The regret analysis of algorithms for risk-sensitive RL is difficult mainly due to the nonlinearity
of the objective (Fei et al. 2020). Although the structure of our regret analysis of the proposed
algorithm follows the optimism principle in provably efficient risk-neutral RL (see, e.g., Azar et al.
(2017), Agarwal et al. (2021)), we develop two new ingredients to overcome the difficulty in our
risk-sensitive setting: (a) concentration bounds for the OCE of the next-state value function under
the estimated transition distributions, and (b) a change-of-measure technique to bound the OCE
of the estimated value function under the true transition distribution with an affine functional (see
Equation (11)). Our concentration bounds for OCEs of value functions are different from recent
results in LA and Bhat (2022) which rely on the Lipschitz continuity of the utility function. Our
technique (b) is inspired by the regret analysis in (Du et al. 2022) for iterated CVaR, but it is
much more general and thus is applicable to OCEs. Conceptually, the main insight is to use the
fact that the OCE is a concave risk functional (Ben-Tal and Teboulle 2007, Theorem 2.1). Its
(algebraic) subgradient is a linear functional Ruszczy´ nski and Shapiro (2006) which turns out to be
in the form of an expectation with respect to a new probability distribution that is related to the
true transition distribution via change-of-measure. This linearization method is crucial in carrying
out the recursions (in the time parameter) in our regret analysis. Due to change-of-measure, the
corresponding Radon-Nikodym derivative naturally appears in our analysis and we need to carefully
bound it.
In addition to the regret upper bound, we also establish a minimax lower bound. It shows
that the regret rate achieved by our proposed algorithm has optimal dependence on the number
of episodes Kand the number of actions A, up to logarithmic factors. The proof of our lower
bound proof is built on the hard MDP instances constructed in Domingues et al. (2021) for tabular
risk-neutral RL. The main novelty in our analysis lies in modifying such hard instances to adapt
to the OCEs and bounding value functions which are defined recursively via OCEs that involve an
optimization problem.
1.1 Related Work
Despite rich literature in risk-sensitive RL, there are fairly limited number of studies on regret
minimization in risk-sensitive MDPs. We provide a concise review below, and leave the detailed
comparisons of existing regret bounds (for entropic risk and CVaR only) with our bounds to Sec-
tion 4.1.
To the best of our knowledge, the first regret bound for risk-sensitive tabular MDP is due to Fei
et al. (2020), who study episodic RL with the goal of maximizing the entropic risk of the cumulative
rewards. By the pleasant properties of exponential functions in entropic risk, their RL formulation
is in fact equivalent to our general (iterative) formulation when the OCE is entropic risk.
The results in Fei et al. (2020) have been improved in Fei et al. (2021) for tabular MDPs
with entropic risk, where they design two model-free algorithms with improved regret bounds. In
addition, these algorithms have been extended to the function approximation setting in Fei et al.
(2021) and to non-stationary MDPs with variation budgets in Ding et al. (2022). Liang and Luo
(2022) also consider RL with the entropic risk, and they use tools from distributional RL (Bellemare
et al. 2017). They propose algorithms with regret upper bounds matching the results in Fei et al.
3(2021).
Du et al. (2022) propose Iterated CVaR RL, which is an episodic risk-sensitive RL formulation
with the objective of maximizing the tail of the reward-to-go at each step. Their RL formulation is a
special case of ours. Du et al. (2022) study both regret miminization and best policy identification,
and provide matching upper and lower bounds with respect to the number of episodes.
All the aforementioned studies focus on one single risk measure (entropic risk or CVaR only) for
regret analysis in risk-sensitive MDPs. Their algorithms and analysis typically rely on the properties
of the special risk measure they consider. Bastani et al. (2022) study episodic RL with a class of
risk-sensitive objectives known as spectral risk measures , which includes CVaR as an example (but
not the entropic risk and mean-variance criterion). They develop an upper-confidence-bound style
algorithm and obtain a regret upper bound for their algorithm. Although spectral risk measures
cover CVaR as an example, their work is different from Du et al. (2022) and ours in that their
objective is to optimize the (static) spectral risk of the cumulative rewards, rather than the value
function obtained from iterative application of the risk measure at each time step. See Appendix
A in Du et al. (2022) and Section 3 in Bastani et al. (2022) for further discussions.
Paper Organization. The rest of the paper is organized as follows: we present the problem
formulation in Section 2 and describe the algorithm in Section 3. We state and discuss the main
results in Section 4. We provide the proof sketch of our regret upper bound in Section 5, and
conclude in Section 6. Due to space constraints, proofs and experiments are given in the Appendix.
2 Problem Formulation
In this section, we introduce the optimized certainty equivalent (OCE), and formulate the risk-
sensitive reinforcement learning problem with recursive OCE.
2.1 The Optimized Certainty Equivalent
We introduce OCE, following Ben-Tal and Teboulle (2007). Let u:R→[−∞,∞) be a nonde-
creasing, closed, concave utility function with effective domain dom u={x∈R|u(t)>−∞} ̸ =∅.
Suppose usatisfies u(0) = 0 and 1 ∈∂u(0), where ∂u(·) denotes the subdifferential of u. We
denote this class of normalized utility functions by U0. The optimized certainty equivalent (OCE)
is defined by
OCEu(X) = sup
λ∈R{λ+E[u(X−λ)]}, (1)
where Xis a bounded random variable (so that OCEu(X) is finite). The interpretation for OCE
in (1) is as follows: a decision maker can consume part of the future uncertain income of Xdollars
at present, and this is denoted by λ. The present value of Xthen becomes λ+E[u(X−λ)], and
the OCE represents the optimal allocation of Xbetween present and future consumption.
OCE captures the risk attitude of a decision maker via the utility function u. With different
choices of the utility functions, OCE covers important examples of popular risk measures, including
the entropic risk measure, CVaR and mean-variance models, as special cases. See Table 1. Due to
4Table 1: Popular OCEs and corresponding utility functions. For CVaR, q(α) = min {x|FX(x)≥α}
where FXis the cumulative distribution function of Xand [−t]+= max {−t,0}.
Name OCEu(X) Utility function u
Mean E[X] u(t) =t
Entropic risk1
βlogE
eβX
uβ(t) =1
βeβt−1
β
CVaR E[X|X≤q(α)]uα(t) =−1
α[−t]+
Mean-Variance E[X]−c·Var(X)uc(t) = (t−ct2)1{t≤1
2c}+1
4c1{t >1
2c}
its tractability and flexibility, OCE has been applied in many areas including finance and machine
learning; see, e.g., Ben-Tal and Teboulle (2007), Lee et al. (2020), LA and Bhat (2022).
2.2 Episodic Risk-Sensitive MDPs with Recursive OCE
Consider a finite-horizon, tabular, non-stationary Markov decision process (MDP), M(S,A, H,P, r),
where Sis the set of states with |S|=S,Ais the set of actions with |A|=A,His the number of
steps in each episode, Pis the transition matrix so that Ph(·|s, a) gives the distribution over states if
action ais taken for state sat step h∈[H], where [ H] ={1,2,···, H}, and rh:S×A → [0,1] is the
deterministic reward function at step h. We define sH+1as the terminate state, which represents
the end of an episode. A policy πis a collection of Hfunctions Π:={πh:S → A} h∈[H].
The reinforcement learning agent repeatedly interacts with the MDP M:=M(S,A, H,P, r)
over Kepisodes. For simplicity (as in many prior studies (Azar et al. 2017, Du et al. 2022))
we assume that the reward function ( rh(s, a))s∈S,a∈Ais known, but the transition probabilities
(Ph(·|s, a))s∈S,a∈Aare unknown. In each episode k= 1,2,···, K, an arbitrary fixed initial state
sk
1=s1∈ S is picked.4An algorithm algo initializes and implements a policy π1for the first
episode, and executes policy πkthroughout episode kbased on the observed past data (states,
actions and rewards) up to the end of episode k−1,k= 2,···, K.
To capture the (dynamic) risk in the decision making process of the agent, we propose a
novel RL formulation with recursive OCEs based on the studies of MDPs with recurisve mea-
sures (Ruszczy´ nski 2010, B¨ auerle and Glauner 2022). Specifically, we use Vπ
h:S →Rto denote
the value function at step hunder policy πand we use Qπ
h:S × A → Rto denote the state-action
value function at step h. They are recursively defined as follows: for all h∈[H],s∈ Sanda∈ A,
Qπ
h(s, a) =rh(s, a) +OCEu
s′∼Ph(·|s,a)(Vπ
h+1(s′)), (2)
Vπ
h(s) =Qπ
h(s, πh(s)), Vπ
H+1(s) = 0 , (3)
where
OCEu
s′∼Ph(·|s,a)(g(s′)) = sup
λ∈R{λ+Es′∼Ph(·|s,a)[u(g(s′)−λ)]},(4)
with g:S →Rbeing a real-valued function.
4The results of the paper can also be extended to the case where the initial states are drawn from a fixed distribution
overS.
5Note that in (2)–(3), the risk measure OCE is applied to the next-state value at each period.
Due to the generality of OCEs, the recursions in (2)–(3) cover and unify several existing frameworks:
(a) when u(t) =t, the OCE becomes the mean, and (2)–(3) become the standard Bellman equation
for the policy πin risk-neutral RL; (b) when uis an exponential function given in Table 1, OCE
becomes the entropic risk, and (2)–(3) recover the Bellman equation for the policy πin risk-sensitive
RL with entropic risk (see Equation (3) in Fei et al. (2021)); (c) when uis a piecewise linear function
and the OCE becomes the CVaR, (2)–(3) reduce to the recursion of value functions in risk-sensitive
RL with iterated CVaR (see Equation (1) in Du et al. (2022)). We also remark that when the OCE
is a coherent risk measure (e.g. CVaR), it has a dual or robust representation, and the recursion
(2)–(3) can be interpreted as the Bellman equation of a distributionally robust MDP, see Section
6 of B¨ auerle and Glauner (2022) for detailed discussions.
Because S,A, Hare finite, by Theorem 4.8 in B¨ auerle and Glauner (2022), there exists an
optimal Markov policy π∗which gives the optimal value function V∗
h(s) = max π∈ΠVπ
h(s) for all
s∈ Sandh∈[H]. The optimal Bellman equation is given by
Q∗
h(s, a) =rh(s, a) +OCEu
s′∼Ph(·|s,a)(V∗
h+1(s′)), (5)
V∗
h(s) = max
a∈AQ∗
h(s, a), V∗
H+1(s) = 0 . (6)
The expected (total) regret for algorithm algo overKepisodes of interaction with the MDP Mis
then defined as
Regret (M,algo, K) =E"KX
k=1(V∗
1(sk
1)−Vπk
1(sk
1))#
, (7)
where the term V∗
1(sk
1)−Vπk
1(sk
1) measures the performance loss when the agent executes (subopti-
mal) policy πkin episode k. Our goal is to propose an efficient learning algorithm with a provable
worst-case regret upper bound that scales sublinearly in K, as well as to establish a minimax lower
bound.
3 The OCE-VI Algorithm
In this section, we propose a model-based algorithm, denoted by OCE-VI, for risk-sensitive RL
with recursive OCE.
Before presenting the algorithm, we first introduce some notations. A state-action-state triplet
(s, a, s′) means that the process is in state s, takes an action aand then moves to state s′. Similarly,
a state-action pair ( s, a) means that the process is in state s and takes an action a. At the beginning
of the k-th episode, we set the observed cumulated visit counts to ( s, a, s′) at step hup to the end
of episode k−1 asNk
h(s, a, s′) for s, s′∈ Sanda∈ A, and the cumulated visit counts to ( s, a)
at step hup to the end of episode k−1 asNk
h(s, a) for s∈ Sanda∈ A. When 2 ≤k≤K, for
6s∈ S, a∈ A, s′∈ S, the formulas for Nk
h(s, a, s′) and Nk
h(s, a) are given by
Nk
h(s, a, s′) =k−1X
i=11{(si
h, ai
h, si
h+1) = (s, a, s′)},
Nk
h(s, a) =k−1X
i=11{(si
h, ai
h) = (s, a)}.
When k= 1, we set Nk
h(s, a, s′) =Nk
h(s, a) = 0 for s∈ S, a∈ A, s′∈ S. Then the empirical
transition probabilities are given by
ˆPk
h(s′|s, a) =Nk
h(s, a, s′)
max{1, Nk
h(s, a)}.
In particular, if ( s, a) has not been sampled before episode k,ˆPk
h(s′|s, a) = 0 for all s′.
Similar to UCBVI in Azar et al. (2017) for risk-neutral RL, the OCE-VI algorithm achieves
exploration by awarding some bonus for exploring some state-action pairs during the learning
process. We consider the bonus
bk
h(s, a) =|u(−H+h)|s
2 log SAHK
δ
max{1, Nk
h(s, a)}, (8)
where ( s, a)∈ S × A , and δ∈(0,1) is an input parameter in our algorithm. Importantly, the
bonus here depends on the utility function uin the OCE (1), which is natural given that we study
risk-sensitive RL with recursive OCE. The details of the OCE-VI algorithm are summarized in
Algorithm 1.
Remark 1. The dependence of the bonus on the utility function usheds some light on how the
degree of risk aversion affects the degree of exploration. Ben-Tal and Teboulle (2007) show that
an agent with OCE preferences is weakly risk averse (i.e., any random payoff is less preferred by
the agent to its mean) if and only if the utility function is dominated by the identify function (i.e.,
u(x)≤x, x∈R). Now, consider two agents with recursive OCE preferences represented by utility
functions u1andu2, respectively. If u1is dominated by u2(i.e., u1(x)≤u2(x), x∈R), then
|u1(−H+h)| ≥ |u2(−H+h)|because −H+h≤0andui(x)≤0, x≤0, i= 1,2. Consequently, the
exploration bonus for agent 1 is larger than for agent 2. Therefore, if we interpret the dominance
ofu2overu1as a higher degree of risk aversion of agent 1 than that of agent 2, as suggested by the
characterization of weak risk aversion (Ben-Tal and Teboulle 2007), then in our algorithm for a
more risk averse agent we need to have a larger bonus to encourage her to explore. We also remark
that our bonus (8)is based on Chernoff-Hoeffding’s concentration inequalities and it scales linearly
with|u(−H+h)|. It might be possible to design tighter bonuses that may depend on the utility
function in a nonlinear manner. This is an open problem for risk-sensitive RL with recursive OCE
and we leave it for future work.
Remark 2. The OCE-VI algorithm is computationally tractable. In each episode, the computa-
tional cost of the algorithm is similar to solving a known MDP with value iteration, except that one
needs to to compute the quantity OCEu
s′∼ˆPh(·|s,a)(ˆVh+1(s′))when updating the Q function. For cer-
tain special utility functions such as those in Table 1, this quantity can be explicitly computed because
7Algorithm 1 The OCE-VI Algorithm
Input: Parameters δ,S,A, H, K, r and an utility function u∈U0
Initialize ˆVh(s)←0,Nh(s, a, s′)←0 and Nh(s, a)←0 for all ( s, a, h )∈ S × A × [H+ 1].
forepisode k= 1,···, Kdo
forsteph=H, H−1,···,1do
for(s, a)∈ S × A do
ifNh(s, a)≥1then
Update bh(s, a) by (8) according to the utility function u
ˆQh(s, a)←min{rh(s, a) +OCEu
s′∼ˆPh(·|s,a)(ˆVh+1(s′)) +bh(s, a), H−h+ 1}
ˆVh(s)←max a′∈AˆQh(s, a′)
else
ˆQh(s, a)←H−h+ 1
end if
end for
end for
For all h∈[H], take πk
h(sh)←argmax a′∈AˆQh(sh, a′)
Play the episode kwith policy πk, update Nh(s, a), Nh(s, a, s′) and ˆPh(s′|s, a) for all h∈[H]
end for
the state space is finite. In general, computing this OCE is equivalent to solving the optimization
problem supλ∈R{λ+Es′∼ˆPh(·|s,a)[u(ˆVh+1(s′)−λ)]}. This is a one-dimensional concave optimization
problem because the utility function uis concave and ˆPh(·|s, a)is a probability distribution. Because
the state space is finite, we can exchange the expectation and the derivative/subgradient with respec-
tive to λin the first order optimality condition of the above optimization problem. Thus, when the
utility function is differentiable, this concave optimization problem can be solved efficiently using
the gradient descent or Newton’s method. When the utility function is nondifferentiable, it can be
solved with efficient proximal gradient methods; see, e.g., Parikh et al. (2014).
4 Main Results
In this section, we present our main results. Our first main result is an upper bound on the expected
regret of the proposed OCE-VI algorithm.
Theorem 1. The expected regret of the OCE-VI algorithm satisfies
Regret (M,OCE-VI , K)≤˜O
HX
h=1|u(−H+h)|Svuuth−1Y
i=1u′
−(−H+i)AK
,
where ˜O(·)ignores the logarithmic factors in S, A, H andKandu′
−(·)is the left derivative of u.
The regret upper bound depends on the utility function uin the OCE (1) via the term |u(−H+
h)|, which comes from the bonus (8), and the termh−1Q
i=1u′
−(−H+i), which comes from bounding
8the Radon-Nikodym derivative arising from the linearization of the OCE as a concave functional in
our regret analysis (see Equation (14)). We provide a sketch of the proof of Theorem 1 in Section 5,
and give the full details in Appendix B.
We next present our second main result, which provides a minimax regret lower bound for RL
with recursive OCE. We first state the following assumption.
Assumption 1. The number of states and actions satisfy S≥6, A≥2, and there exists an integer
dsuch that S= 3+Ad−1
A−1. In addition, the horizon Hsatisfies H≥c2d, where c2>2is a constant.
Assumption 1 is adapted from Assumption 1 in Domingues et al. (2021), who provide a minimax
lower bound in the risk-neutral episodic RL setting. This assumption is imposed to simplify the
analysis, more precisely the construction of hard MDP instances, and it can be relaxed following
the discussion in Appendix D of Domingues et al. (2021).
Theorem 2. Under Assumption 1, for any algorithm algo, there exists an MDP Mwhose tran-
sition probabilities depend on hsuch that
Regret (M,algo, K)≥1
18√c1c2·
u
1−2
c2
H−λ∗
−u(−λ∗)√
SAHK
for all K≥c1HSA
2c2, where the constants c1≥4, c2>2andλ∗satisfies
1∈
1−2
c1
∂u
1−2
c2
H−λ∗
+2
c1∂u(−λ∗).
Note that when u(t) =t,OCE becomes expectation, and our regret lower bound in Theorem 2
is Ω(H√
SAHK ), by choosing for instance c1= 4 and c2= 3. This recovers the (tight) regret lower
bound in Domingues et al. (2021) in learning risk-neutral tabular MDP. For a general utility function
uin OCE, the choices of constants c1≥4, c2>2 should be based on the specific utility function
to generate tighter lower bounds. For illustrations, we provide some examples in Section 4.1.
The proof of Theorem 2 is based on extending the proof of Theorem 9 in Domingues et al. (2021)
to our risk-sensitive setting. There are essential difficulties in this extension. These include how
to construct hard MDP instances that adapt to the OCE, and how to bound the value functions
defined recursively via OCE that involves an optimization problem. Due to space limitations, we
provide the proof details in Appendix C.
Remark 3. For the simplicity of presentation, we focus on OCE in (1), which exhibits the risk
aversion property with OCEu(X)≤E[X], due to the concavity of the utility function; see Propo-
sition 2.2 in Ben-Tal and Teboulle (2007). Our main results in the paper hold in the risk-seeking
setting as well, where OCEu(X)is defined by infλ∈R{λ+E[u(X−λ)]}with a convex utility func-
tionu. In this case, we need to use a bonus bk
h(s, a) =|u(−H+h)|r
2Slog(SAHK
δ)
max{1,Nk
h(s,a)}in the OCE-VI
algorithm. Compared with (8), this bonus has an extra term√
S, which arises from a technical step
in the proof for the risk-seeking case (see inequality (2) of Lemma 5). The regret bounds still hold
in this setting.
94.1 Examples and Comparisons to Related Work
We consider several specific utility functions and the resulting OCEs to illustrate our regret bounds
in Theorems 1 and 2.
4.1.1 Mean-variance Model
When the utility function is uc(t) = ( t−ct2)1{t≤1
2c}+1
4c1{t >1
2c}, the corresponding OCE
is the celebrated mean-variance model Markowitz (1952), where c >0 is a given risk parameter
representing the degree of risk aversion. To the best of our knowledge, the following results are the
first regret bounds for risk-sensitive MDPs with the recursive mean-variance model.
•Upper bound. Our regret upper bound in Theorem 1 is ˜O
(1 + 2 cH)H−1
2(H2+cH3)S√
AK
.
•Lower bound. We can choose c1= 8, c2= 4, and then λ∗=
1−2
c1
1−2
c2
H= 3H/8.
The regret lower bound in Theorem 2 becomes Ω
(H+1
4cH2)√
SAHK
.
4.1.2 (Iterated) CVaR
When the utility function is uα(t) =−1
α[−t]+, α > 0, the corresponding OCE is CVaR, where α >0
is the risk level of CVaR. Our RL formulation in Section 2.2 reduces to the one in Du et al. (2022),
and our OCE-VI algorithm becomes their ICVaR algorithm with a smaller exploration bonus.
•Upper bound. Our regret upper bound in Theorem 1 becomes ˜O
(1√α)H−1−H(1√α−1)
(1−√α)2 S√
AK
.
When 0 < α≤3−√
5
2, this upper bound can be further bounded by ˜O
1√
αH+1−H√α
S√
AK
.
When3−√
5
2< α < 1, the regret bound can be further bounded by ˜O
H2S√
AK√
αH+1
. Du
et al. (2022) design the ICVaR algorithm and can obtain a worst-case regret upper bound of
˜O
H2S√
AK√
αH+1
.5Our result improves the result of (Du et al. 2022) by a factor of H2when
0< α≤3−√
5
2. This is due to a smaller exploration bonus used in our algorithm compared
with theirs.
•Lower bound. We can choose c1=2
αandc2= 4 in Theorem 2, and let λ∗=
1−3
c2
H.
Then, our regret lower bound becomes Ω
Hq
SAHK
α
and it is problem-independent. This
is in contrast with Du et al. (2022), who derive a regret lower bound that depends on some
problem-dependent quantity, specifically, the minimum probability of visiting an available
state under any feasible policy.
5Du et al. (2022) consider stationary MDPs, and we modify their regret bounds to adapt to our non-stationary
setting.
104.1.3 Entropic Risk
When the utility function is uβ(t) =1
βeβt−1
β, β < 0, the corresponding OCE is entropic risk,
where β <0 is a given risk parameter representing the degree of risk aversion. In this case, our
RL formulation in Section 2.2 is equivalent to the one in Fei et al. (2020). Note, however, that our
OCE-VI algorithm is model-based and is different from the model-free algorithms proposed in Fei
et al. (2020).
•Upper bound. Our regret upper bound in Theorem 1 for the OCE-VI algorithm becomes
˜O
exp(−βH2
4)exp(−βH)−1
−βS√
AK
. This bound has a factor that is exponential in |β|H2,
which is similar as the bounds in Fei et al. (2020). Recently, Fei et al. (2021) propose the
RSVI2 and RSQ2 algorithms, and they manage to remove this factor. Their algorithms are
based on the nice properties of the exponential utility, in particular, the so-called exponential
Bellman equation which takes the exponential on both sides of the Bellman equation in Fei
et al. (2020). However, such techniques can not be applied to our general setting, because
general utility functions do not possess the same nice properties as the exponential function.
Even though our upper bound is worse than the one in Fei et al. (2021), we show numerically
that our algorithm can outperform their algorithms on randomly generated MDP instances.
See Appendix D for experimental details.
•Lower Bound. We can choose c1=2
e−β−1·exp
−β
1−2
c2
H
andc2= 6 in Theorem 2, and
the corresponding λ∗=1
βlog
1−2
c1
exp
β
1−2
c2
H
+2
c1
. Then our regret lower
bound becomes Ω
exp(−1
3βH)−1
−β√
SAHK
. By contrast, Fei et al. (2020) derive a regret
lower bound of Ωexp(1
2|β|H)−1
|β|√
K
, which does not depend on SorA(due to the simple
structure of the hard instances they construct). Liang and Luo (2022) derive a regret lower
bound Ωexp(1
6|β|H)−1
|β|√
SAHK
in the risk-seeking setting when β >0, but they mention
that it is unclear whether a similar bound holds in the risk-averse setting when β <0; see
page 30 of their paper.
4.2 Discussions on tightness of our regret bounds
Theorems 1 and 2 imply that the OCE-VI algorithm achieves a regret rate with the optimal
dependence on the number of episodes Kand the number of actions A, up to logarithmic factors.
While the bounds on Kare the most important as they imply the convergence rates of learning
algorithms, it remains an important open question whether one can improve the dependence of
these bounds on HandSto narrow down the gap between the upper and lower bounds in the
risk-sensitive RL setting. We elaborate further on this issue below.
From Theorems 1 and 2, we can see that the gap between our upper and lower bounds in terms
ofSis√
S, where Sis the number of states. The extra√
Sin our regret upper bound arises from
a step in our proof where we apply an L1concentration bound for the S-dimensional empirical
transition probability vector, see Equation (10) in Section 5. This extra√
Sfactor can be removed
11in RL for risk-neutral MDPs by directly maintaining confidence intervals on the optimal value
function; see, e.g., Azar et al. (2017), Zanette and Brunskill (2019). However, it is not clear how to
adapt this technique to our risk-sensitive setting, i.e., remove√
Sin (10). This is primarily because
the estimated value functions ˆVk
hin our algorithm are not only random, but they also involve OCE
which is nonlinear and defined by an optimization problem (so the optimizer is also random).
There is an exponential gap in terms of Hbetween our upper and lower bounds. This gap is
due to the linearization of OCE in the recursive procedure of the regret analysis of our algorithm.
Indeed, if u(t) =t, the corresponding regret upper bound is ˜O(H2S√
AK), which does not have the
exponential term of H. In the risk-neutral setting, one can improve the dependence of the upper
bound on Hby considering Bernstein-style exploration bonus which is built from the empirical
variance of the estimated value function ˆVk
hat the next state, see, e.g., Azar et al. (2017). However,
it is still an open problem how to use Bernstein bonus to improve the regret bound in risk-sensitive
RL (Fei et al. 2021, Du et al. 2022). In our RL setting with recursive OCEs, it is possible to
design a Bernstein-type bonus, but it may not lead to improved regret bounds, at least within our
current analysis framework. We provide some informal discussions below including the challenges
in improving bounds.
First, to ensure optimism with Bernstein-type (or variance-related) bonuses, we need analogous
results to Lemmas 4 and 5 in the appendix, which provide concentration bounds for OCEs of next-
state value functions. Using Bernstein inequality instead of Hoeffding inequality, the confidence
bound in Lemma 4 becomes
s
2 Var s′∼Ph(·|s,a) 
u(V∗
h+1(s′)−λ∗
h+1)
log SAHK
δ
Nk
h(s, a)
+ lower order term .
This bound allows us to design a Bernstein-type bonus bk
h(s, a) in the form of
2vuutVars′∼ˆPk
h(·|s,a)
u(ˆVk
h+1(s′)−ˆλk
h+1)
log SAHK
δ
Nk
h(s, a)| {z }
main term
+ lower order term .
Compared with the Bernstein bonus in the risk-neutral RL setting (see e.g. Azar et al. (2017)),
ˆVk
h+1(s′) inside the variance operator is replaced by u(ˆVk
h+1(s′)−ˆλk
h+1) in our risk-sensitive RL
setting. We use this approach because the OCE involves an optimization problem and we need to
‘linearize’ it (i.e., remove the sup in the definition of OCE) and work with the utility uapplied to
the value function first in order to derive concentration bounds for OCEs. With this new bonus,
we might be able to get the same regret bound as the one presented in the current paper.
However, it is difficult to get improved bounds as we explain below. In the risk-neutral setting,
Azar et al. (2017) use an iterative-Bellman-type-Law of Total Variance so that the sum of the
variances of V∗
h+1over Hsteps is bounded by the variance of the sum of H-step rewards; see
Equation (26) in Azar et al. (2017) and Lemma C.5 in Jin et al. (2018) for a proof of this result.
This is a key technical result in obtaining improved bounds in H. However, this result does not hold
12in our setting for two reasons: first, our value function is not the expected sum of H-step rewards;
second, while the value V∗
h+1satisfies a Bellman recursion, the quantity u(V∗
h+1(s′)−λ∗
h+1) (that
appears in the variance operator) does not. Therefore, we may still have to use the crude bound for
the variance term in the Bernstein-type bonus by using a maximum bound for u(V∗
h+1(s′)−λ∗
h+1).
This leads to the same bound as in our current paper and we do not obtain improvements in the
regret with respect to H.
5 Proof Sketch of Theorem 1
The structure of the proof of Theorem 1 follows the optimism principle in provably efficient risk-
neutral RL (see, e.g., (Agarwal et al. 2021, Chapter 7)), however, we provide two new ingredients
in our analysis: (a) concentration bounds for the OCE of the next-state value function under
estimated transitions (see (9) and (10)), and (b) a change-of-measure technique to bound the OCE
of the estimated value function (under the true transition) with an affine function (see (11)), and
bound the the Radon-Nikodym derivative (see (14)). For notational simplicity, we use Phto denote
Ph(sk
h+1|sk
h, ak
h) when there is no ambiguity.
Step 1: Optimism. We can first show optimism, i.e., the event ˆVk
h≥V∗
hfor all h, kholds with
a high probability, where ˆVk
his the estimated value function in our algorithm in episode k. This
step relies on a concentration bound of the OCE of the optimal value function under the estimated
transitions ˆPh: with probability 1 −δ(where δ∈(0,1)),
OCEu
Ph(V∗
h+1)−OCEu
ˆPk
h(V∗
h+1)≤bk
h. (9)
This bound can be proved by using the representation of the OCE in (1), together with similar
martingale arguments used in the risk-neutral RL setting (Agarwal et al. 2021, Lemma 7.3). By
optimism, the regret in (7) is upper bounded by E[PK
k=1(ˆVk
1−Vπk
1)].
Step 2: Bounding ˆVk
h−Vπk
h,∀k, h.By definition,
ˆVk
h−Vπk
h≤bk
h+OCEu
ˆPk
h(ˆVk
h+1)−OCEu
Ph(Vπk
h+1)
=bk
h+h
OCEu
ˆPk
h(ˆVk
h+1)−OCEu
Ph(ˆVk
h+1)i
+h
OCEu
Ph(ˆVk
h+1)−OCEu
Ph(Vπk
h+1)i
.
Step 2.1: The second term in the above equation can be bounded by using a concentration result
for the OCE of the estimated value function ˆVk
h+1: with probability 1 −δ,
OCEu
ˆPk
h(ˆVk
h+1)−OCEu
Ph(ˆVk
h+1)≤√
S·bk
h. (10)
The extra√
Sfactor, compared with (9), is because both ˆVk
h+1and ˆPk
hare random and we use L1
concentration bounds for ||ˆPk
h−Ph||1as in Jaksch et al. (2010).
Step 2.2: The third term OCEu
Ph(ˆVk
h+1)−OCEu
Ph(Vπk
h+1) is more difficult to bound. Because the
OCE is a concave nonlinear functional, we expect that
OCEu
Ph(ˆVk
h+1)−OCEu
Ph(Vπk
h+1)≤ℓ(ˆVk
h+1−Vπk
h+1),
13where ℓ(·) is a linear function of random variables and it is a subgradient of the OCE. We actually
show that ℓcan be represented in the form of an expectation:
OCEu
Ph(ˆVk
h+1)−OCEu
Ph(Vπk
h+1)≤EBh(ˆVk
h+1−Vπk
h+1), (11)
where the expectation EBh[·] is taken with respect to a new probability distribution Bhthat is
linked to the true transition distribution Phby change-of-measure. Specifically, using the first
order optimality condition for OCEu
Ph(Vπk
h+1) as a concave optimization problem, we have 1 ∈
Es′∼Ph[∂u(Vπk
h+1(s′)−λk
h+1)] where λk
h+1is an optimal solution. We can find a subgradient Λk
h+1(s′)∈
∂u(Vπk
h+1(s′)−λk
h+1) that satisfies EPh[Λk
h+1] = 1, and define the new distribution Bhby
Bh(s′|s, a) =Ph(s′|s, a)Λk
h+1(s′),∀s′∈ S.
Here, Λk
h+1is the Radon-Nikodym derivative.
By combining Steps 2.1 and 2.2, we obtain
ˆVk
h−Vπk
h≤2√
S·bk
h+EBh[ˆVk
h+1−Vπk
h+1]. (12)
Step 3: Bounding the regret. Applying (12) recursively over hand using (8), we have that
with probability 1 −2δ,
KX
k=1
ˆVk
1−Vπk
1
≤HX
h=1KX
k=1EwB
hk
2√
2|u(−H+h)|s
Slog SAHK
δ
Nk
h
, (13)
where wB
hkis the probability of πkvisiting ( sk
h, ak
h) at step hstarting from sk
1under probability
measures Bi(·|sk
i, ak
i), i= 1,···, h−1. Specifically,
EwB
hk[·] :=(
1 h= 1,
EB1
EB2
···EBh−1[·]
h≥2.
The main difficulty in bounding E[PK
k=1(ˆVk
1−Vπk
1)] is that wB
hkis built upon the probability
measure Bhfor any k∈[K], h∈[H] while we have to take expectation under probability measure
Phoutside the summation over k∈[K]. To address this issue, we first note that EwB
hk
1√
Nk
h
=
E"
Λk
2···Λk
h1√
Nk
hsk
1, ak
1#
, which implies EPK
k=1EwB
hk
1√
Nk
h
=PK
k=1E
Λk
2···Λk
h1√
Nk
h
.Using
Cauchy–Schwarz inequality, it can be upper bounded byrPK
k=1E[Λk
2···Λk
h]2·PK
k=1Eh
1
Nk
hi
. It
is well-known thatPK
k=11
Nk
h≤SAlog(3K) (Azar et al. 2017). One can show that E[Λk
2···Λk
h] = 1
and 0 ≤Λk
i+1≤u′
−(−H+i). Then we have
KX
k=1E[Λk
2···Λk
h]2≤h−1Y
i=1u′
−(−H+i)K. (14)
Summing over h, choosing δ=1
2KHand applying a standard argument (see, e.g., Chapter 7.3 of
Agarwal et al. (2021) ), we obtain the bound on the expected regret in Theorem 1.
146 Conclusion and Future Work
In this paper we have proposed a risk-sensitive RL formulation based on episodic finite MDPs
with recursive OCEs. We develop a learning algorithm, OCE-VI, and establish a worst-case regret
upper bound. We also prove a regret lower bound, showing that the regret rate achieved by our
proposed algorithm actually has the optimal dependence on the numbers of episodes and actions.
Because OCEs encompass a wide family of risk measures, our paper generates new regret bounds
for episodic risk-sensitive RL problems with those risk measures.
Regret minimization for risk-sensitive MDPs is still largely unexplored. For future work, one im-
portant direction is to improve regret bounds in the number of states and the horizon length. Other
interesting directions include, to name a few, studying large or continuous state/action spaces, con-
sidering risk measures other than OCEs, and obtaining problem-dependent regret bounds.
15References
Agarwal, A., N. Jiang, S. M. Kakade, and W. Sun (2021). Reinforcement learning: Theory and algorithms.
https: // rltheorybook. github. io/ rltheorybook_ AJKS. pdf .
Artzner, P., F. Delbaen, J.-M. Eber, D. Heath, and H. Ku (2007). Coherent multiperiod risk adjusted values
and bellman’s principle. Annals of Operations Research 152 (1), 5–22.
Azar, M. G., I. Osband, and R. Munos (2017). Minimax regret bounds for reinforcement learning. In
International Conference on Machine Learning , pp. 263–272. PMLR.
Bastani, O., Y. J. Ma, E. Shen, and W. Xu (2022). Regret bounds for risk-sensitive reinforcement learning.
arXiv preprint arXiv:2210.05650 .
B¨ auerle, N. and A. Glauner (2022). Markov decision processes with recursive risk measures. European
Journal of Operational Research 296 (3), 953–966.
Bellemare, M. G., W. Dabney, and R. Munos (2017). A distributional perspective on reinforcement learning.
InInternational Conference on Machine Learning , pp. 449–458. PMLR.
Ben-Tal, A. and M. Teboulle (1986). Expected utility, penalty functions, and duality in stochastic nonlinear
programming. Management Science 32 (11), 1445–1466.
Ben-Tal, A. and M. Teboulle (2007). An old-new concept of convex risk measures: the optimized certainty
equivalent. Mathematical Finance 17 (3), 449–476.
Chow, Y., M. Ghavamzadeh, L. Janson, and M. Pavone (2017). Risk-constrained reinforcement learning
with percentile risk criteria. The Journal of Machine Learning Research 18 (1), 6070–6120.
Dann, C. (2019). Strategic Exploration in Reinforcement Learning-New Algorithms and Learning Guarantees .
Ph. D. thesis, Carnegie Mellon University.
Ding, Y., M. Jin, and J. Lavaei (2022). Non-stationary risk-sensitive reinforcement learning: Near-optimal
dynamic regret, adaptive detection, and separation design. arXiv preprint arXiv:2211.10815 .
Domingues, O. D., P. M´ enard, E. Kaufmann, and M. Valko (2021). Episodic reinforcement learning in finite
mdps: Minimax lower bounds revisited. In Algorithmic Learning Theory , pp. 578–598. PMLR.
Du, Y., S. Wang, and L. Huang (2022). Risk-sensitive reinforcement learning: Iterated cvar and the worst
path. arXiv preprint arXiv:2206.02678 .
Epstein, L. G. and S. E. Zin (1989). Substitution, risk aversion, and the temporal behavior of consumption
and asset returns: a theoretical framework. Econometrica 57 , 937–969.
Fei, Y., Z. Yang, Y. Chen, and Z. Wang (2021). Exponential bellman equation and improved regret bounds for
risk-sensitive reinforcement learning. Advances in Neural Information Processing Systems 34 , 20436–
20446.
Fei, Y., Z. Yang, Y. Chen, Z. Wang, and Q. Xie (2020). Risk-sensitive reinforcement learning: Near-optimal
risk-sample tradeoff in regret. Advances in Neural Information Processing Systems 33 , 22384–22395.
Fei, Y., Z. Yang, and Z. Wang (2021). Risk-sensitive reinforcement learning with function approximation:
A debiasing approach. In International Conference on Machine Learning , pp. 3198–3207. PMLR.
Garcıa, J. and F. Fern´ andez (2015). A comprehensive survey on safe reinforcement learning. Journal of
Machine Learning Research 16 (1), 1437–1480.
Garivier, A., P. M´ enard, and G. Stoltz (2019). Explore first, exploit next: The true shape of regret in bandit
problems. Mathematics of Operations Research 44 (2), 377–399.
Howard, R. A. and J. E. Matheson (1972). Risk-sensitive markov decision processes. Management sci-
ence 18 (7), 356–369.
Jaksch, T., R. Ortner, and P. Auer (2010). Near-optimal regret bounds for reinforcement learning. Journal
of Machine Learning Research 11 (51), 1563–1600.
Jin, C., Z. Allen-Zhu, S. Bubeck, and M. I. Jordan (2018). Is q-learning provably efficient? Advances in
neural information processing systems 31 .
16Kreps, D. M. and E. L. Porteus (1978). Temporal resolution of uncertainty and dynamic choice theory.
Econometrica: journal of the Econometric Society , 185–200.
LA, P. and S. P. Bhat (2022). A wasserstein distance approach for concentration of empirical risk estimates.
Journal of Machine Learning Research 23 , 1–61.
Lee, J., S. Park, and J. Shin (2020). Learning bounds for risk-sensitive learning. Advances in Neural
Information Processing Systems 33 , 13867–13879.
Liang, H. and Z.-Q. Luo (2022). Bridging distributional and risk-sensitive reinforcement learning with
provable regret bounds. arXiv preprint arXiv:2210.14051 .
Mannor, S. and J. N. Tsitsiklis (2011). Mean-variance optimization in markov decision processes. In Pro-
ceedings of the 28th International Conference on International Conference on Machine Learning , pp.
177–184.
Marcus, S. I., E. Fern´ andez-Gaucherand, D. Hern´ andez-Hernandez, S. Coraluppi, and P. Fard (1997). Risk
sensitive markov decision processes. In Systems and control in the twenty-first century , pp. 263–279.
Springer.
Markowitz, H. (1952). Portfolio selection. The Journal of Finance 7 (1), 77–91.
Parikh, N., S. Boyd, et al. (2014). Proximal algorithms. Foundations and trends ®in Optimization 1 (3),
127–239.
Prashanth L, A. and M. Fu (2018). Risk-sensitive reinforcement learning. arXiv e-prints , arXiv:1810.09126.
Puterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming . John Wiley
& Sons.
Ruszczy´ nski, A. (2010). Risk-averse dynamic programming for markov decision processes. Mathematical
programming 125 (2), 235–261.
Ruszczy´ nski, A. and A. Shapiro (2006). Optimization of convex risk functions. Mathematics of operations
research 31 (3), 433–452.
Sarver, T. (2018). Dynamic mixture-averse preferences. Econometrica 86 (4), 1347–1382.
Shen, Y., W. Stannat, and K. Obermayer (2013). Risk-sensitive markov control processes. SIAM Journal
on Control and Optimization 51 (5), 3652–3672.
Shen, Y., M. J. Tobia, T. Sommer, and K. Obermayer (2014). Risk-sensitive reinforcement learning. Neural
computation 26 (7), 1298–1328.
Sutton, R. S. and A. G. Barto (2018). Reinforcement learning: An introduction . MIT press.
Tamar, A., D. Di Castro, and S. Mannor (2016). Learning the variance of the reward-to-go. The Journal of
Machine Learning Research 17 (1), 361–396.
Zanette, A. and E. Brunskill (2019). Tighter problem-dependent regret bounds in reinforcement learning
without domain knowledge using value function bounds. In International Conference on Machine
Learning , pp. 7304–7312. PMLR.
17A Preliminary Lemmas
In this section, we present some preliminary lemmas that will be used in the proofs of Theorems 1
and 2.
Lemma 1 is (Ben-Tal and Teboulle 2007, Theorem 2.1) and it summarizes some fundamental
properties of OCE.
Lemma 1. (Main Properties of OCE) For any utility function u∈U0, and any bounded random
variable Xthe following properties hold:
(a)Shift Additivity. OCEu(X+c) =OCEu(X) +c,∀c∈R.
(b)Consistency. OCEu(c) =c, for any constant c∈R.
(c)Monotonicity. Let Ybe any random variable such that X(w)≤Y(w),∀w∈Ω. Then,
OCEu(X)≤OCEu(Y).
(d)Concavity. For any random variables X1, X2and any µ∈(0,1), we have
OCEu(µX1+ (1−µ)X2)≥µOCEu(X1) + (1 −µ)OCEu(X2).
The following lemma provides preliminary bounds for the value functions (see (2) and (3)) and
the regret of learning algorithms.
Lemma 2. For any s∈ S, a∈ A, h∈[H], π∈Πandu∈U0, we have Qπ
h(s, a)∈[0, H−h+1]and
Vπ
h(s)∈[0, H−h+ 1]. Consequently, for each K≥1, we have 0≤Regret (M,algo, K)≤KH for
anyalgo.
Proof. Recall that Qπ
h(s, a) =rh(s, a) +OCEu
s′∼Ph(·|s,a)(Vπ
h+1(s′)) and Vπ
H+1(s) = 0 ,∀s∈ S. Then,
we can calculate that
Qπ
H(s, a) =rH(s, a) +OCEu
s′∼PH(·|s,a)(Vπ
H+1(s′))(1)=rH(s, a)∈[0,1],
Vπ
H(s) = max
a∈ArH(s, a)∈[0,1],
where equality (1) is due to property (b) in Lemma 1. Hence, we have
Qπ
H−1(s, a) =rH−1(s, a) +OCEu
s′∼PH−1(·|s,a)(Vπ
H(s′))(1)
≤rH−1(s, a) +OCEu
s′∼PH−1(·|s,a)(1)∈[1,2],
Qπ
H−1(s, a) =rH−1(s, a) +OCEu
s′∼PH−1(·|s,a)(Vπ
H(s′))(2)
≥rH−1(s, a) +OCEu
s′∼PH−1(·|s,a)(0)∈[0,1],
Vπ
H−1(s) = max
a∈AQπ
H−1(s, a)∈[0,2],
where inequalities (1) and (2) hold due to properties (b) and (c) in Lemma 1. Carrying out this
procedure repeatedly until step h, we can get
Qπ
h(s, a)∈[0, H−h+ 1],and Vπ
h(s)∈[0, H−h+ 1].
Using the definition (7), we then immediately obtain that 0 ≤Regret (M,algo, K)≤KH for any
algo.
18With Lemma 2, we can obtain the following result, which shows that the optimization problem
inOCEu
s′∼Ph(·|s,a)(Vπ
h+1(s′)) has an optimal solution in the support of the random variable Vπ
h+1(s′).
Lemma 3. For any probability measure Ph(·|s, a), any s∈ S, a∈ A, h∈[H], suppose Vπ
h+1(s′)∈
[0, H−h]fors′∼Ph(·|s, a). Then, we have
OCEu
s′∼Ph(·|s,a)(Vπ
h+1(s′)) = max
λ∈[0,H−h]{λ+Es′∼Ph(·|s,a)[u(Vπ
h+1(s′)−λ)]}. (15)
Proof. Note that Vπ
h+1(s′)∈[0, H−h] for s′∼Ph(·|s, a) by Lemma 2. By the concavity and
continuity of u, we deduce that the function G(λ) :=λ+Es′∼Ph(·|s,a)[u(Vπ
h+1(s′)−λ)] is concave
and continuous, and moreover, G(λ)≤Es′∼Ph(·|s,a)[Vπ
h+1(s′)]<∞for all λ∈Rdue to the fact
that u(x)≤xfor all x. In addition, ∂G(λ) = 1−Es′∼Ph(·|s,a)[∂u(Vπ
h+1(s′)−λ)] due to the finite
state space S, and thus, G(λ) will be nonincreasing when λ≥H−hdue to the fact that η≥1
for all η∈∂u(x), x≤0. It follows that the set of optimal solutions to the problem supλ∈RG(λ)
is nonempty. Hence, we can apply Proposition 2.1 in Ben-Tal and Teboulle (2007) and obtain the
desired result.
B Proof of Theorem 1
We present a series of lemmas in Section B.1, and prove Theorem 1 in Section B.2. The relation of
different lemmas is given below.
Lemma 4 Lemma 5 Lemma 6
Lemma 7 Lemma 8
Lemma 9 Lemma 10
Lemma 11 Lemma 12Theorem 1
B.1 Preparations for the Proof of Theorem 1
In this subsection, we state and prove a few lemmas needed for the proof of theorem 1. Recall that
the bonus in the OCE-VI algorithm is bk
h(s, a) =|u(−H+h)|r
2 log(SAHK
δ)
max{1,Nk
h(s,a)}for any ( s, a, h, k )∈
S × A × [H]×[K].
Lemma 4 provides a bound for the difference between Es′∼Ph(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
and its
estimation for all h∈[H], where
λ∗
h+1∈arg max
λ∈[0,H−h]{λ+Es′∼Ph(·|s,a)[u(V∗
h+1(s′)−λ)]}.
19Note that both V∗
h+1andλ∗
h+1are deterministic quantities. To facilitate the presentation, we let
Hk
h= ((S × A )H−1× S)k−1×(S × A )h−1× S (16)
be the set of possible histories up to step hin episode k. Then, one sample of the history up to
stephin episode kis
Hk
h= (s1
1, a1
1, s1
2, a1
2,···, s1
H,···, sk
1, ak
1,···, sk
h−1, ak
h−1, sk
h)∈Hk
h.
Lemma 4. For any δ∈(0,1), we have
P 
Es′∼Ph(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
−Es′∼ˆPk
h(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
≤u(H−h−λ∗
h+1)−u(−λ∗
h+1)s
2 log(SAHK
δ)
max{1, Nk
h(s, a)},
V∗
h+1:S → [0, H−h], λ∗
h+1∈[0, H−h],∀(s, a, h, k )∈ S × A × [H]×[K]!
≥1−δ.(17)
Proof. We adapt the proof of Lemma 7.3 in Agarwal et al. (2021) who consider the risk neural
episodic RL setting. For each fixed ( s, a, h, k )∈ S × A × [H]×[K], we have to consider two cases.
Firstly, according to section 3, when Nk
h(s, a) = 0, we have ˆPk
h(s′|s, a) = 0 for all s′∈ S.
According to Lemma 2 and Lemma 3, V∗
h+1(si
h+1)∈[0, H−h] and λ∗
h+1∈[0, H−h]. Thus, we have
u(V∗
h+1(si
h+1)−λ∗
h+1)∈[u(−λ∗
h+1), u(H−h−λ∗
h+1)], where u(−λ∗
h+1)≤0 and u(H−h−λ∗
h+1)≥0.
Then, we have
Es′∼Ph(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
−Es′∼ˆPk
h(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
(1)=Es′∼Ph(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
(2)
≤ |u(H−h−λ∗
h+1)−u(−λ∗
h+1)|
(3)
≤u(H−h−λ∗
h+1)−u(−λ∗
h+1)s
2 log(SAHK
δ)
max{1, Nk
h(s, a)},
where equality (1) holds because ˆPk
h(s′|s, a) = 0 for all s′∈ S, inequality (2) holds because
u(V∗
h+1(si
h+1)−λ∗
h+1)≤u(H−h−λ∗
H+1)≤ |u(H−h−λ∗
h+1)−u(−λ∗
h+1)|,
and inequality (3) holds because Nk
h(s, a) = 0 and log(SAHK
δ)>1. Therefore,
P 
Es′∼Ph(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
−Es′∼ˆPk
h(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
≤u(H−h−λ∗
h+1)−u(−λ∗
h+1)s
2 log(SAHK
δ)
max{1, Nk
h(s, a)},
V∗
h+1:S → [0, H−h], λ∗
h+1∈[0, H−h]!
= 1≥1−δ
SAHK.
20Secondly, when Nk
h(s, a)≥1, by the definition of ˆPk
h, we have
Es′∼ˆPk
h(·|s,a)
u(V∗
h+1(s′)−λ∗
h+1)
=1
Nk
h(s, a)k−1X
i=11{(si
h,ai
h)=(s,a)}u(V∗
h+1(si
h+1)−λ∗
h+1).
Remark that when Nk
h(s, a)≥1, we have k≥2. We define for i= 1, . . . , k −1,
Xi=Eh
1{(si
h,ai
h)=(s,a)}u(V∗
h+1(si
h+1)−λ∗
h+1)Hi
hi
−1{(si
h,ai
h)=(s,a)}u(V∗
h+1(si
h+1)−λ∗
h+1).
By the same argument as in the previous case, we conclude that
u(V∗
h+1(si
h+1)−λ∗
h+1)∈[u(−λ∗
h+1), u(H−h−λ∗
h+1)].
Thus, we have
u(−λ∗
h+1)−u(H−h−λ∗
h+1)≤Xi≤u(H−h−λ∗
h+1)−u(−λ∗
h+1).
In addition, it is evident that E[Xi|Hi
h] = 0, which implies that ( Xi) is a martingale difference
sequence. Then, by Azuma-Hoeffding’s inequality for martingales, with a probability of at least
1−δ
SAHK, we have
k−1X
i=1Xi=Nk
h(s, a)Es′∼Ph(·|s,a)[u(V∗
h+1(s′)−λ∗
h+1)]−k−1X
i=11{(si
h,ai
h)=(s,a)}u(V∗
h+1(si
h+1)−λ∗
h+1)
≤ |u(H−h−λ∗
h+1)−u(−λ∗
h+1)|r
2Nk
h(s, a) log(SAHK
δ)
Divided by Nk
h(s, a) on both sides of the above inequality, combining the above two cases and using
a union bound over all ( s, a, h, k )∈ S × A × [H]×[K], we obtain (17).
By Lemma 4, we can derive the following concentration bound for the OCE applied to the
optimal value function at the next state (under the estimated transition distribution).
Lemma 5. For any δ∈(0,1), we have that
P 
OCEu
s′∼Ph(·|s,a)(V∗
h+1(s′))−OCEu
s′∼ˆPk
h(·|s,a)(V∗
h+1(s′))≤ |u(−H+h)|s
2 log SAHK
δ
max{1, Nk
h(s, a)},
V∗
h+1:S → [0, H−h],∀(s, a, h, k )∈ S × A × [H]×[K]!
≥1−δ.
(18)
Proof. According to Lemma 4, with a probability of at least 1 −δ, for any k∈[K], s∈ S, a∈
21A, h∈[H], we have
OCEu
s′∼Ph(·|s,a)(V∗
h+1(s′))−OCEu
s′∼ˆPk
h(·|s,a)(V∗
h+1(s′))
(1)= max
λ∈[0,H−h]{λ+Es′∼Ph(·|s,a)[u(V∗
h+1(s′)−λ)]} − max
λ∈[0,H−h]{λ+Es′∼ˆPk
h(·|s,a)[u(V∗
h+1(s′)−λ)]}
(2)
≤λ∗
h+1+Es′∼Ph(·|s,a)[u(V∗
h+1(s′)−λ∗
h+1)]−λ∗
h+1−Es′∼ˆPk
h(·|s,a)[u(V∗
h+1(s′)−λ∗
h+1)]
(3)
≤u(H−h−λ∗
h+1)−u(−λ∗
h+1)s
2 log(SAHK
δ)
max{1, Nk
h(s, a)}
≤ sup
λ∈[0,H−h]|u(H−h−λ)−u(−λ)|s
2 log SAHK
δ
max{1, Nk
h(s, a)},
where equality (1) follows from Lemma 3, inequality (2) holds because λ∗
h+1is the optimal solution
to max λ∈[0,H−h]{λ+Es′∼Ph(·|s,a)[u(V∗
h+1(s′)−λ)]}and inequality (3) follows from Lemma 4. One
can check that u(H−h−λ)−u(−λ) is a nondecreasing function of λ∈[0, H−h]. To see this,
note that the subdifferential of u(H−h−λ)−u(−λ) is∂u(−λ)−∂u(H−h−λ) and for any
z∈∂u(−λ)−∂u(H−h−λ), we have z≥0, because the utility function uis concave. This implies
that the function u(H−h−λ)−u(−λ) is nondecreasing. In addition, this function is non-negative
because uis nondcreasing and thus u(H−h)−u(0)≥0 for h∈[H]. Thus, we can deduce that
supλ∈[0,H−h]|u(H−h−λ)−u(−λ)| ≤u(0)−u(−H+h) =|u(−H+h)|since u(0) = 0. The proof
is then complete.
Lemma 5 immediately implies the following result. To facilitate the presentation, we define the
following event from Lemma 5:
G1=(
OCEu
s′∼Ph(·|s,a)(V∗
h+1(s′))−OCEu
s′∼ˆPk
h(·|s,a)(V∗
h+1(s′))≤ |u(−H+h)|s
2 log SAHK
δ
max{1, Nk
h(sk
h, ak
h)},
V∗
h+1:S → [0, H−h],∀(s, a, h, k )∈ S × A × [H]×[K])
. (19)
Lemma 6 (Optimism) .Conditional on the event G1, we have ˆVk
h(s)≥V∗
h(s)for any k∈[K], s∈
S, h∈[H].
Proof. We prove the result by induction. Set ˆVk
H+1(s) =V∗
H+1(s) = 0 ,∀s∈ S. Conditional on the
occurrence of the event G1, assume ˆVk
h+1(s′)≥V∗
h+1(s′),∀s′∈ S. Then, under event G1, for step h,
we have
bk
h(s, a) +rh(s, a) +OCEϕ
s′∼ˆPk
h(·|s,a)(ˆVk
h+1(s′))−rh(s, a)−OCEϕ
s′∼Ph(·|s,a)(V∗
h+1(s′))
(1)
≥bk
h(s, a) +OCEϕ
s′∼ˆPk
h(·|s,a)(V∗
h+1(s′))−OCEϕ
s′∼Ph(·|s,a)(V∗
h+1(s′))
(2)
≥bk
h(s, a)− |u(−H+h)|s
2 log SAHK
δ
max{1, Nk
h(s, a)}
= 0, (20)
22where inequality (1) follows from the assumption ˆVk
h+1(s′)≥V∗
h+1(s′),∀s′∈ Sand property (c) in
Lemma 1, and inequality (2) holds due to Lemma 5. Recall that
ˆQk
h(s, a) = min {bk
h(s, a) +rh(s, a) +OCEϕ
s′∼ˆPk
h(·|s,a)(ˆVk
h+1(s′)), H−h+ 1},
Q∗
h(s, a) =rh(s, a) +OCEϕ
s′∼Ph(·|s,a)(V∗
h+1(s′)).
By (20) and Lemma 2, we can immediately obtain
ˆQk
h(s, a)−Q∗
h(s, a)≥0.
Because ˆVk
h(s) = max a′∈AˆQk
h(s, a′), we have ˆVk
h(s)≥V∗
h(s). The result then follows by induction.
We next state a concentration bound (Lemma 8) for the OCE of the estimated next-state value
function ˆVk
h+1under the estimated transition distribution ˆPk
h. This is different from Lemma 5 in that
ˆVk
h+1is a random quantity depending on the data while the optimal value function is deterministic.
The proof of Lemma 8 relies on the following well-known result on the L1concentration bound for
the empirical transition probabilities (see, e.g., Lemma 17 in Jaksch et al. (2010)):
Lemma 7. For any δ∈(0,1), we have
P ˆPk
h(·|s, a)−Ph(·|s, a)
1≤s
2Slog SAHK
δ
max{1, Nk
h(s, a)},∀(s, a, h, k )∈ S × A × [H]×[K]!
≥1−δ.
Lemma 8. For any δ∈(0,1), we have
P OCEu
s′∼Ph(·|s,a)(ˆVk
h+1(s′))−OCEu
s′∼ˆPk
h(·|s,a)(ˆVk
h+1(s′))≤ |u(−H+h)|s
2Slog SAHK
δ
max{1, Nk
h(s, a)},
∀(s, a, h, k )∈ S × A × [H]×[K]!
≥1−δ.
(21)
Proof. With probability at least 1 −δ, we have that for any k∈[K], s∈ S, a∈ A, h∈[H],
OCEu
s′∼Ph(·|s,a)(ˆVk
h+1(s′))−OCEu
s′∼ˆPk
h(·|s,a)(ˆVk
h+1(s′))
=max
λ∈[0,H−h]{λ+Es′∼Ph(·|s,a)[u(ˆVk
h+1(s′)−λ)]} − max
λ∈[0,H−h]{λ+Es′∼ˆPk
h(·|s,a)u(ˆVk
h+1(s′)−λ)]}
≤max
λ∈[0,H−h]Es′∼Ph(·|s,a)[u(ˆVk
h+1(s′)−λ)]−Es′∼ˆPk
h(·|s,a)[u(ˆVk
h+1(s′)−λ)
= max
λ∈[0,H−h]X
s′∈S
ˆPk
h(s′|s, a)−Ph(s′|s, a)
·u(ˆVk
h+1(s′)−λ)
(1)
≤max
λ∈[0,H−h]ˆPk
h(·|s, a)−Ph(·|s, a)
1·u(ˆVk
h+1(·)−λ)
∞
(2)
≤s
2Slog SAHK
δ
max{1, Nk
h(s, a)}·max
λ∈[0,H−h]u(ˆVk
h+1(·)−λ)
∞,
23where inequality (1) follows from H¨ older’s inequality and inequality (2) follows from Lemma 7.
Because ˆVk
h+1(s′)∈[0, H−h] for any s′by the design of the OCE-VI algorithm and because
λ∈[0, H−h], we can immediately obtain that
max
λ∈[0,H−h]u(ˆVk
h+1(·)−λ)
∞≤ |u(−H+h)|,
where we use the fact that uis nondecreasing and concave. The proof is then completed.
In the next lemma, we will bound the following difference
OCEu
s′∼Ph(·|s,a)
ˆVk
h+1(s′)
−OCEu
s′∼Ph(·|s,a)
Vπk
h+1(s′)
(22)
for any ( s, a, h, k )∈ S ×A× [H]×[K], which is the key step in the recursion of the regret analysis.
We first introduce some notations. Pick any λk
h+1∈[0, H−h] such that
λk
h+1∈arg maxλ∈[0,H−h]{λ+Es′∼Ph(·|s,a)[u(Vπk
h+1(s′)−λ)]}. (23)
By the first order optimality condition of the above optimization problem and the fact that the
state space Sis finite, we have
1∈Es′∼Ph(·|s,a)[∂u(Vπk
h+1(s′)−λk
h+1)]. (24)
Thus, we can find Λk
h+1(s′)∈∂u(Vπk
h+1(s′)−λk
h+1), s′∈ Ssuch that Es′∼Ph(·|s,a)
Λk
h+1(s′)
= 1. In
addition, because the utility function uis nondecreasing, we have Λk
h+1(s′)≥0,∀s′∈ S. Now we can
define the following new probability measure Bh(·|s, a): for any ( s, a, s′, h, k)∈ S×A×S× [H]×[K],
define
Bh(s′|s, a) :=Ph(s′|s, a)Λk
h+1(s′), (25)
whereP
s′∈SBh(s′|s, a) = 1 because Es′∼Ph(·|s,a)
Λk
h+1(s′)
= 1. Now we can state the following
important result.
Lemma 9. For any (h, k)∈[H]×[K]and functions ˆVk
h+1, Vπk
h+1, V∗
h+1:S → [0, H−h], we have
OCEu
sk
h+1∼Ph(·|sk
h,ak
h)
ˆVk
h+1(sk
h+1)
−OCEu
sk
h+1∼Ph(·|sk
h,ak
h)
Vπk
h+1(sk
h+1)
≤Esk
h+1∼Bh(·|sk
h,ak
h)h
ˆVk
h+1(sk
h+1)−Vπk
h+1(sk
h+1)i
,(26)
where Bh(·|sk
h, ak
h)is the new probability measure given in (25).
Proof. Pick any µk
h+1∈[0, H−h] such that
µk
h+1∈arg maxλ∈[0,H−h]{λ+Esk
h+1∼Ph(·|sk
h,ak
h)[u(ˆVk
h+1(sk
h+1)−λ)]},
24and recall λk
h+1∈[0, H−h] given in (23). We can then compute
OCEu
sk
h+1∼Ph(·|sk
h,ak
h)
ˆVk
h+1(sk
h+1)
−OCEu
sk
h+1∼Ph(·|sk
h,ak
h)
Vπk
h+1(sk
h+1)
(1)= max
λ∈[0,H−h]n
λ+Esk
h+1∼Ph(·|sk
h,ak
h)[u(ˆVk
h+1(sk
h+1)−λ)]o
−max
λ∈[0,H−h]n
λ+Esk
h+1∼Ph(·|sk
h,ak
h)[u(Vπk
h+1(sk
h+1)−λ)]o
=µk
h+1+Esk
h+1∼Ph(·|sk
h,ak
h)[u(ˆVk
h+1(sk
h+1)−µk
h+1)]−λk
h+1−Esk
h+1∼Ph(·|sk
h,ak
h)[u(Vπk
h+1(sk
h+1)−λk
h+1)]
(2)
≤µk
h+1−λk
h+1+Esk
h+1∼Ph(·|sk
h,ak
h)h
Λk
h+1(sk
h+1)·(ˆVk
h+1(sk
h+1)−Vπk
h+1(sk
h+1)−(µk
h+1−λk
h+1))i
=
1−Esk
h+1∼Ph(·|sk
h,ak
h)h
Λk
h+1(sk
h+1)i
(µk
h+1−λk
h+1)
+Esk
h+1∼Ph(·|sk
h,ak
h)h
Λk
h+1(sk
h+1)·(ˆVk
h+1(sk
h+1)−Vπk
h+1(sk
h+1))i
(3)=Esk
h+1∼Bh(·|sk
h,ak
h)[ˆVk
h+1(sk
h+1)−Vπk
h+1(sk
h+1)],
where equality (1) holds due to Lemma 3, inequality (2) holds due to the fact that u(y)≤u(x) +
z(y−x) for any x, y∈[−H+h, H−h], z∈∂u(x) when u(x) is a concave function, and equality
(3) follows from (25) and the fact that Esk
h+1∼Ph(·|sk
h,ak
h)[Λk
h+1(sk
h+1)] = 1. The proof is therefore
completed.
In the next lemma, we bound the term ˆVk
1(sk
1)−Vπk
1(sk
1) by using a recursive procedure. Lemma
10 below is an extension of the so-called simulation lemma in the risk-neutral RL (see, e.g, Agarwal
et al. (2021)) to our risk-sensitive RL setting. The key to overcome the difficulty in the recursion
setting due to the nonlinearity of the OCE is Lemma 9. To facilitate the presentation, we first
introduce the following notations.
For any k∈[K], h∈[H], let whk(sk
h, ak
h) be the state-action distribution induced by πkat time
stephstarting from sk
1, i.e., the probability of πkvisiting ( sk
h, ak
h) at time step hstarting from sk
1.
Mathematically, the formula of whk(sk
h, ak
h) is given by
whk(sk
h, ak
h) =

1, h = 1,
P1(sk
2|sk
1, ak
1), h = 2,
X
sk
2∈S···X
sk
h−1∈SP1(sk
2|sk
1, ak
1)···Ph−1(sk
h|sk
h−1, ak
h−1), h ≥3.(27)
Similarly, let wB
hk(sk
h, ak
h) be the probability of πkvisiting ( sk
h, ak
h) at step hstarting from sk
1under
probability measures Bi(·|sk
i, ak
i), i= 1,···, h. The explicit formula of wB
hk(sk
h, ak
h) is given by
wB
hk(sk
h, ak
h) =

1, h = 1,
B1(sk
2|sk
1, ak
1), h = 2,
X
sk
2∈S···X
sk
h−1∈SB1(sk
2|sk
1, ak
1)···Bh−1(sk
h|sk
h−1, ak
h−1), h ≥3.(28)
25Equivalently, by (25) we have
wB
hk(sk
h, ak
h) =

1, h = 1,
P1(sk
2|sk
1, ak
1)Λk
2(sk
2), h = 2,
X
sk
2∈S···X
sk
h−1∈SP1(sk
2|sk
1, ak
1)···Ph−1(sk
h|sk
h−1, ak
h−1)Λk
2(sk
2)···Λk
h(sk
h), h ≥3.
(29)
Finally, given ( sk
1, ak
1), we define
E(sk
h,ak
h)∼wB
hk[·] :=

1, h = 1,
Esk
2∼B1(·|sk
1,ak
1)h
Esk
3∼B2(·|sk
2,ak
2)h
···Esk
h∼Bh−1(·|sk
h−1,ak
h−1)[·]ii
, h ≥2.(30)
Lemma 10. For each episode k∈[K], we have
ˆVk
1(sk
1)−Vπk
1(sk
1) (31)
≤HX
h=1E(sk
h,ak
h)∼wB
hkh
bk
h(sk
h, ak
h) (32)
+OCEu
sk
h+1∼ˆPk
h(·|sk
h,ak
h)(ˆVk
h+1(sk
h+1))−OCEu
sk
h+1∼Ph(·|sk
h,ak
h)(ˆVk
h+1(sk
h+1))i
.
Proof. For any k∈[K], let ak
h= arg maxa∈AˆQk
h(sk
h, a), h∈[H]. Then, we can compute
ˆVk
1(sk
1)−Vπk
1(sk
1)
(1)
≤ˆQk
1(sk
1, ak
1)−Qπk
1(sk
1, ak
1)
≤bk
1(sk
1, ak
1) +OCEu
sk
2∼ˆPk
1(·|sk
1,ak
1)(ˆVk
2(sk
2))−OCEu
sk
2∼P1(·|sk
1,ak
1)(Vπk
2(sk
2))
=bk
1(sk
1, ak
1) +OCEu
sk
2∼ˆPk
1(·|sk
1,ak
1)(ˆVk
2(sk
2))−OCEu
sk
2∼P1(·|sk
1,ak
1)(ˆVk
2(sk
2))
+OCEu
sk
2∼P1(·|sk
1,ak
1)(ˆVk
2(sk
2))−OCEu
sk
2∼P1(·|sk
1,ak
1)(Vπk
2(sk
2))
(2)
≤bk
1(sk
1, ak
1) +OCEu
sk
2∼ˆPk
1(·|sk
1,ak
1)(ˆVk
2(sk
2))−OCEu
sk
2∼P1(·|sk
1,ak
1)(ˆVk
2(sk
2))
+Esk
2∼B1(·|sk
1,ak
1)h
ˆVk
2(sk
2)−Vπk
2(sk
2)i
≤bk
1(sk
1, ak
1) +OCEu
sk
2∼ˆPk
1(·|sk
1,ak
1)(ˆVk
2(sk
2))−OCEu
sk
2∼P1(·|sk
1,ak
1)(ˆVk
2(sk
2))
+Esk
2∼B1(·|sk
1,ak
1)h
bk
2(sk
2, ak
2) +OCEu
sk
3∼ˆPk
2(·|sk
2,ak
2)(ˆVk
3(sk
3))−OCEu
sk
3∼P2(·|sk
2,ak
2)(ˆVk
3(sk
3))
+Esk
3∼B2(·|sk
2,ak
2)h
ˆVk
3(sk
3)−Vπk
3(sk
3)ii
,
where inequality (1) holds because ˆVk
1(sk
1) = max a∈AˆQk
1(sk
1, a) = ˆQk
1(sk
1, ak
1) and inequality (2)
holds due to Lemma 9. Applying the above procedure recursively and using the fact that ˆVk
H+1(s) =
V∗
H+1(s) = 0 for any s∈ S, we immediately obtain (31).
26From Lemma 10, it is clear that we need to bound the sum of bonuses in order to bound the
regret. We present such a bound in Lemma 12. To this end, we first state Lemma 11, which is
adapted from a well-known result heavily used in the risk-neutral setting (see page 24-25 of Azar
et al. (2017) or page 21 of Jin et al. (2018)). Lemma 12 is a nontrivial extension of Lemma 11 due
to the new probability measure wB
hkinvovled in the expectation.
Lemma 11. Consider arbitrary Ksequences of trajectories τk={sk
h, ak
h}H
h=1fork= 1,···, K, we
have
KX
k=11
max{1, Nk
h(sk
h, ak
h)}≤SAlog(3K).
Lemma 12. We have
E
KX
k=1HX
h=1E(sk
h,ak
h)∼wB
hk
|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}


≤HX
h=1|u(−H+h)|vuuth−1Y
i=1u′
−(−H+i)SAK log(3K), (33)
where E(sk
h,ak
h)∼wB
hk[·]given in (30)is taken over (sk
h, ak
h)conditional on (sk
1, ak
1)andu′
−(·)is the left
derivative of u.
Proof. By (29) and (30), we have
E(sk
h,ak
h)∼wB
hk
|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}

=X
sk
2∈S···X
sk
h∈SP1(sk
2|sk
1, ak
1)···Ph−1(sk
h|sk
h−1, ak
h−1)Λk
2(sk
2)···Λk
h(sk
h)|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}.(34)
This implies
E
E(sk
h,ak
h)∼wB
hk
|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}


=E
E
Λk
2(sk
2)···Λk
h(sk
h)|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}sk
1, ak
1


=E
Λk
2(sk
2)···Λk
h(sk
h)|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}
.
27Then, we have
E
KX
k=1HX
h=1E(sk
h,ak
h)∼wB
hk
|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}


=HX
h=1KX
k=1E
E(sk
h,ak
h)∼wB
hk
|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}


=HX
h=1|u(−H+h)|KX
k=1E
Λk
2(sk
2)···Λk
h(sk
h)1q
max{1, Nk
h(sk
h, ak
h)}

(1)
≤HX
h=1|u(−H+h)|KX
k=1q
E
Λk
2(sk
2)···Λk
h(sk
h)2·s
E1
max{1, Nk
h(sk
h, ak
h)}
(2)
≤HX
h=1|u(−H+h)|vuutKX
k=1E
Λk
2(sk
2)···Λk
h(sk
h)2·vuutKX
k=1E1
max{1, Nk
h(sk
h, ak
h)}
where the inequalities (1) and (2) follow from Cauchy–Schwarz inequality.
Recall that Λk
h+1(s′)∈∂u(Vπk
h+1(s′)−λk
h+1), s′∈ Sand it satisfies Es′∼Ph(·|s,a)
Λk
h+1(s′)
= 1,
where λk
h+1is defined in (23). By Lemma 2 and Lemma 3 and the concavity of the utility function
u, we have 0 ≤Λk
h+1≤u′
−(−H+h). Because
E(sk
h,ak
h)∼wB
hk[1] =X
sk
2∈S···X
sk
h∈SP1(sk
2|sk
1, ak
1)···Ph−1(sk
h|sk
h−1, ak
h−1)Λk
2(sk
2)···Λk
h(sk
h) = 1 ,
taking the expectation on both sides, we have E
Λk
2(sk
2)···Λk
h(sk
h)
= 1. Then, we have
KX
k=1Eh
Λk
2(sk
2)···Λk
h(sk
h)i2
≤KX
k=1Eh
Λk
2(sk
2)···Λk
h(sk
h)i
·h−1Y
i=1u′
−(−H+i)
=K·h−1Y
i=1u′
−(−H+i).
Combining this inequality and Lemma 11, we have
E
KX
k=1HX
h=1E(sk
h,ak
h)∼wB
hk
|u(−H+h)|q
max{1, Nk
h(sk
h, ak
h)}


≤HX
h=1|u(−H+h)|vuuth−1Y
i=1u′
−(−H+i)SAK log(3K),
which completes the proof.
28B.2 Proof of Theorem 1
Now we are ready to prove Theorem 1. Recall G1in (19) and we define
G2=(OCEu
s′∼Ph(·|s,a)(ˆVk
h+1(s′))−OCEu
s′∼ˆPk
h(·|s,a)(ˆVk
h+1(s′))
≤ |u(−H+h)|s
2Slog SAHK
δ
max{1, Nk
h(sk
h, ak
h)},∀(s, a, h, k )∈ S × A × [H]×[K])
.
We also define G=G1∩G2. From Lemmas 5 and 8, we know that P(G1)≥1−δandP(G2)≥1−δ,
which implies that P(G)≥1−2δ.
Proof of Theorem 1. For any k∈[K], let ak
h= arg maxa∈AˆQk
h(sk
1, a), h∈[H]. Then, when the
eventGholds, we can compute
V∗
1(sk
1)−Vπk
1(sk
1)
(1)
≤ˆVk
1(sk
1)−Vπk
1(sk
1)
(2)
≤HX
h=1E(sk
h,ak
h)∼wB
hkh
bk
h(sk
h, ak
h)
+OCEu
sk
h+1∼ˆPk
h(·|sk
h,ak
h)(ˆVk
h+1(sk
h+1))−OCEu
sk
h+1∼Ph(·|sk
h,ak
h)(ˆVk
h+1(sk
h+1))i
(3)
≤HX
h=1E(sk
h,ak
h)∼wB
hk
2√
2|u(−H+h)|s
Slog SAHK
δ
max{1, Nk
h(sk
h, ak
h)}
, (35)
where inequality (1) follows from Lemma 6, inequality (2) holds due to Lemma 10, inequality (3)
holds due to Lemma 8 and the fact that bk
h(sk
h, ak
h)≤ |u(−H+h)|r
2Slog(SAHK
δ)
max{1,Nk
h(sk
h,ak
h)}.
We can write the expected regret as follows:
Regret (M,OCE-VI , K)
=E"KX
k=1
V∗
1(sk
1)−Vπk
1(sk
1)#
=E"
1G·KX
k=1
V∗
1(sk
1)−Vπk
1(sk
1)#
+E"
1Gc·KX
k=1
V∗
1(sk
1)−Vπk
1(sk
1)#
(4)
≤E"
1G·KX
k=1
V∗
1(sk
1)−Vπk
1(sk
1)#
+ 2δKH,
where inequality (4) holds because P(Gc)≤2δand 0 ≤Vπk
1(sk
1)≤V∗
1(sk
1)≤Hby Lemma 2. Using
29(35) and Lemma 12, we deduce that
E"
1G·KX
k=1
V∗
1(sk
1)−Vπk
1(sk
1)#
≤E
KX
k=1HX
h=1E(sk
h,ak
h)∼wB
hk
2√
2|u(−H+h)|s
Slog SAHK
δ
max{1, Nk
h(sk
h, ak
h)}


≤2√
2HX
h=1|u(−H+h)|Svuuth−1Y
i=1u′
−(−H+i)AKlog(3K) logSAHK
δ
.
Then, we have
Regret (M,OCE-VI , K)
≤2√
2HX
h=1|u(−H+h)|Svuuth−1Y
i=1u′
−(−H+i)AKlog(3K) logSAHK
δ
+ 2δKH
≤2√
2HX
h=1|u(−H+h)|Svuuth−1Y
i=1u′
−(−H+i)AKlog(3K) log (2 SAH2K2) + 1,
where the last inequality follows by choosing δ=1
2KH. The proof is then completed.
C Proof of Theorem 2
Proof. We adapt the proof of Theorem 9 in Domingues et al. (2021) to our risk-sensitive setting.
The proof of Theorem 2 is long, so we divide it into a few steps.
•Step 1: Constructing the hard MDP instances.
We first construct hard MDP instances, which are almost the same as the ones in Domingues
et al. (2021) except one small yet important difference: the transition probabilities in (36).
Based on assumption 1, we can construct a full A-ary tree of depth d−1 with root ˜ sroot,
which has S−3 states. In this rooted tree, each node has exactly Achildren and the total
number of nodes is given byPd−1
i=0Ai=S−3. We add three special states to the tree: a
“waiting” state ˜ swwhere the agent starts and can choose action ˜ awto stay up to a stage
¯H < H −d, a “good” state ˜ sgwhere the agent obtains rewards, and a “bad” state ˜ sbthat
gives no reward. Note that ¯His a parameter to be chosen later. Both ˜ sgand ˜sbare absorbing
states. For any state in the tree, the transitions are deterministic, the a-th action in a node
leads to the a-th child of that node. The agent stays or leaves ˜ swwith probability
Ph(˜sw|˜sw, a) := 1{a= ˜aw, h≤¯H}, Ph(˜sroot|˜sw, a) := 1 −Ph(˜sw|˜sw, a).
Then, the agent transverses the tree until she arrives at the leaves. Let Lbe the number of
leaves, and the set of the leaves is L={˜s1,···,˜sL}. For any ˜ si∈ L, any action will lead to
30a transition to either ˜ sgor ˜sbwith the transition probability
Ph(˜sg|˜si, a) =p+ ∆ (h∗,s∗,a∗)(h,˜si, a), Ph(˜sb|˜si, a) = 1−p−∆(h∗,s∗,a∗)(h,˜si, a), (36)
where the parameter pand the function ∆ will be specified later. In Domingues et al. (2021),
pis set to be 0 .5 in the risk-neutral setting, whereas we will tune pin our risk-sensitive setting
to obtain a tighter regret lower bound.
The reward function is defined as
rh(s, a) := 1{s= ˜sg, h≥¯H+d+ 1},∀a∈ A.
For each
(h∗, s∗, a∗)∈ {1 +d,···,¯H+d} × L × A =:Z,
we define an MDP M(h∗,s∗,a∗), where ∆ (h∗,s∗,a∗)(h,˜si, a) = 1{h=h∗,˜si=s∗, a=a∗}ϵand
ϵis a parameter to be chosen later. Denote by P(h∗,s∗,a∗)andE(h∗,s∗,a∗)the probability
measure and expectation, respectively, in the MDP M(h∗,s∗,a∗). Let M0be the MDP with
∆0(h,˜si, a) = 0 for all ( h,˜si, a)∈[H]×L×A . Denote by P0andE0the probability measure
and expectation, respectively, in the MDP M0.
•Step 2: Computing the Expected Regret of an Algorithm in M(h∗,s∗,a∗).
We now compute Regret (M(h∗,s∗,a∗),algo, K) for a learning algorithm algo, which executes
policy πkin episode k∈[K]. By (7), we need to compute the optimal value function, V∗
1(sk
1),
and the value function under policy πk,Vπk
1(sk
1), for k∈[K]. Unlike Domingues et al.
(2021), these quantities can not be computed explicitly in general in our risk-sensitive setting
because the OCE is defined by an optimization problem. Hence, in the following, we will
bound V∗
1(sk
1)−Vπk
1(sk
1) in order to lower bound the regret.
We first compute the value function Vπk
1(sk
1) under policy πk. For notational simplicity, we
denote πk
h(sk
h) asak
hfor all h∈[H], k∈[K]. Under policy πk, we use ˆHto denote the number
of time steps in which the agent stays in the “waiting” state, which is no larger than ¯H.
Because there is no reward collected before step ˆH+d, we can obtain
Vπk
1(sk
1) =Vπk
ˆH+d(sk
ˆH+d). (37)
Next, we compute Vπk
ˆH+d(sk
ˆH+d).To this end, we first show
Vπk
¯H+d+1(sk¯H+d+1) = (H−¯H−d)×1{sk¯H+d+1= ˜sg}. (38)
We prove it by recursion. It is clear that
Vπk
H(sk
H) =rH(sk
H, ak
H) +OCEsk
H+1∼PH(·|sk
H,ak
H)(Vπk
H+1(sk
H+1))(1)= 1{sk
H= ˜sg}
(2)= 1{sk¯H+d+1= ˜sg},
31where equality (1) holds because Vπk
H+1(sk
H+1) = 0, and equality (2) follows from the fact that
the agent is in the absorbing states when h≥¯H+d+ 1. Then, we can compute
Vπk
H−1(sk
H−1) =rH−1(sk
H−1, ak
H−1) +OCEsk
H∼PH−1(·|sk
H−1,ak
H−1)(Vπk
H(sk
H))
(3)= 2×1{sk¯H+d+1= ˜sg},
where equality (3) holds because the random variable 1 {sk¯H+d+1= ˜sg}is known at step H−1,
and we use property (b) in Lemma 1. Repeating this procedure until time step h=¯H+d+1,
we obtain (38).
Given (38), we next compute the value function under policy πkat time ˆH+d+1. Note that at
time step ˆH+d, the agent is at the leaf of the rooted tree, where ˆH≤¯H.Hence, the probability
that the agent is at good state ˜ sgat step h=ˆH+d+1 is the same as that at step h=¯H+d+1.
In addition, the reward function is given by rh(s, a) = 1{s= ˜sg, h≥¯H+d+ 1},∀a∈ A.
Hence, we obtain
Vπk
ˆH+d+1(sk
ˆH+d+1) =Vπk
¯H+d+1(sk¯H+d+1) = (H−¯H−d)×1{sk¯H+d+1= ˜sg}
= (H−¯H−d)×1{sk
ˆH+d+1= ˜sg}.
It then follows that
Vπk
1(sk
1) =Vπk
ˆH+d(sk
ˆH+d)
=rˆH+d(sk
ˆH+d, ak
ˆH+d) +OCEsk
ˆH+d+1∼PˆH+d(·|sk
ˆH+d,ak
ˆH+d)(Vπk
ˆH+d+1(sk
ˆH+d+1))
=OCEsk
ˆH+d+1∼PˆH+d(·|sk
ˆH+d,ak
ˆH+d)
(H−¯H−d)×1{sk
ˆH+d+1= ˜sg}
= sup
λ∈[0,H−¯H−d]{λ+P(h∗,s∗,a∗)(sk
ˆH+d+1= ˜sg)u(H−¯H−d−λ)
+ (1−P(h∗,s∗,a∗)(sk
ˆH+d+1= ˜sg))u(−λ)}
= sup
λ∈[0,H−¯H−d]{λ+P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg)u(H−¯H−d−λ)
+ (1−P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg))u(−λ)}.(39)
Similar to Equation (7) in Domingues et al. (2021), we can derive
P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg)
=¯H+dX
h=1+dpP(h∗,s∗,a∗)(sk
h∈ L) + 1{h=h∗}P(h∗,s∗,a∗)(sk
h=s∗, ak
h=a∗)ϵ
=p+ϵ·P(h∗,s∗,a∗)(sk
h∗=s∗, ak
h∗=a∗).(40)
Together with (39), we obtain an expression of the value function Vπk
1(sk
1).
We next compute the optimal value function V∗
1(sk
1). Based on (39), one can easily show
that the optimal policy is to let P(h∗,s∗,a∗)(sk
h∗=s∗, ak
h∗=a∗) = 1. Specifically, in the MDP
32M(h∗,s∗,a∗), the optimal policy is to traverse the tree at step h∗−d, so that the agent visits
the leaf state s∗at time step h∗and takes the action a∗at this leaf state. Thus, the optimal
value function is given by
V∗
1(sk
1) = sup
λ∈[0,H−¯H−d]{λ+ (p+ϵ)u(H−¯H−d−λ) + (1 −p−ϵ)u(−λ)}.
Now, we can compute that for each episode k∈[K],
V∗
1(sk
1)−Vπk
1(sk
1)
= sup
λ∈[0,H−¯H−d]{λ+ (p+ϵ)u(H−¯H−d−λ) + (1 −p−ϵ)u(−λ)}
− sup
λ∈[0,H−¯H−d]{λ+P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg)u(H−¯H−d−λ)
+ (1−P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg))u(−λ)}
(1)
≥ρ+ (p+ϵ)u(H−¯H−d−ρ) + (1 −p−ϵ)u(−ρ)
−ρ−P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg)u(H−¯H−d−ρ)−(1−P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg))u(−ρ)
(2)=ϵ[u(H−¯H−d−ρ)−u(−ρ)]×[1−P(h∗,s∗,a∗)(sk
h∗=s∗, ak
h∗=a∗)],
where inequality (1) holds by setting
ρ∈arg max
λ∈[0,H−¯H−d]{λ+P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg)u(H−¯H−d−λ)
+ (1−P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg))u(−λ)}, (41)
and equality (2) holds by applying (40).
Therefore, the regret of a learning algorithm algo inM(h∗,s∗,a∗)can be lower bounded as
follow:
Regret (M(h∗,s∗,a∗),algo, K)
=KX
k=1E(h∗,s∗,a∗)[V∗
1(sk
1)−Vπk
1(sk
1)]
≥ϵ[u(H−¯H−d−ρ)−u(−ρ)]KX
k=1
1−P(h∗,s∗,a∗)(sk
h∗=s∗, ak
h∗=a∗)
=ϵ[u(H−¯H−d−ρ)−u(−ρ)]
K−E(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
, (42)
where
NK
(h∗,s∗,a∗)=KX
k=11{sh∗=s∗, ah∗=a∗}. (43)
33•Step 3: Bounding Maximum Regret over all possible M(h∗,s∗,a∗).
We can deduce from (42) that the maximum regret of an algorithm algo over all possible
M(h∗,s∗,a∗)is lower bounded by
max
(h∗,s∗,a∗)∈ZRegret (M(h∗,s∗,a∗),algo, K)
≥1
¯HLAX
(h∗,s∗,a∗)∈ZRegret (M(h∗,s∗,a∗),algo, K)
≥K[u(H−¯H−d−ρ)−u(−ρ)]ϵ
1−1
K¯HLAX
(h∗,s∗,a∗)E(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
.(44)
So to lower bound the regret, we have to upper boundP
(h∗,s∗,a∗)∈ZE(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
.
•Step 4: BoundingP
(h∗,s∗,a∗)∈ZE(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
.
For this step, we use similar arguments to those used in Domingues et al. (2021); see page
13 therein. Fix ( h∗, s∗, a∗)∈[H]× S × A . Because1
KNK
(h∗,s∗,a∗)∈[0,1], one can obtain from
Lemma 1 of Garivier et al. (2019) that
kl1
KE0h
NK
(h∗,s∗,a∗)i
,1
KE(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
≤KL(P0, P(h∗,s∗,a∗)),
where KL denotes the Kullback-Leibler divergence between two probability measures and
kl(p, q) denotes the KL divergence between two Bernoulli distributions with success proba-
bilities pandqrespectively; see Definition 4 in Domingues et al. (2021). It then follows from
Pinsker’s inequality, ( p−q)2≤1
2kl(p, q), that
1
KE(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
≤1
KE0h
NK
(h∗,s∗,a∗)i
+r
1
2KL(P0, P(h∗,s∗,a∗)).
Because M0andM(h∗,s∗,a∗)differ at stage h∗when ( sh∗, ah∗) = ( s∗, a∗), by Lemma 5 of
Domingues et al. (2021) and Lemma 13 in Appendix C.1, we can prove that
KL 
P0, P(h∗,s∗,a∗)
=E0h
NK
(h∗,s∗,a∗)i
kl(p, p+ϵ)≤E0h
NK
(h∗,s∗,a∗)ic1ϵ2
p,
where c1≥2 is a certain positive constant, p∈[0,1−1
c1] and ϵsatisfies
ϵ∈
0,(1−2p) +q
1−4p
c1
2
. (45)
Thus,
1
KE(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
≤1
KE0h
NK
(h∗,s∗,a∗)i
+rc1
2pϵr
E0h
NK
(h∗,s∗,a∗)i
.
34According to the definition of NK
(h∗,s∗,a∗)in (43), we know thatP
(h∗,s∗,a∗)∈ZNK
(h∗,s∗,a∗)≤K.
Then, by Cauchy-Schwarz inequality, we have
1
KX
(h∗,s∗,a∗)∈ZE(h∗,s∗,a∗)h
NK
(h∗,s∗,a∗)i
≤1 +rc1
2pϵp
¯HLAK. (46)
•Step 5: Optimizing ϵand Choosing ¯Hand p.
By combining (44) with (46), we have
max
(h∗,s∗,a∗)∈ZRegret (M(h∗,s∗,a∗),algo, K)
≥K[u(H−¯H−d−ρ)−u(−ρ)]ϵ 
1−1
¯HLA−rc1
2pϵ√¯HLAK
¯HLA!
, (47)
where the right-hand side of the inequality is a quadratic function of ϵ. Maximizing this
function by taking
ϵ=rp
2c1
1−1
¯HLAr¯HLA
K, (48)
we derive
max
(h∗,s∗,a∗)∈ZRegret (M(h∗,s∗,a∗),algo, K)
≥1
2√
2rp
c1[u(H−¯H−d−ρ)−u(−ρ)]p
¯HLAK (1−1
¯HLA)2. (49)
According to Assumption 1, we have A≥2,S≥6, and thus L= (1−1
A)(S−3) +1
A≥S
4.
Then, we can deduce from (49) that
max
(h∗,s∗,a∗)∈ZRegret (M(h∗,s∗,a∗),algo, K)
≥1
2√
2·rp
c1[u(H−¯H−d−ρ)−u(−ρ)]r
¯H·S
4AK·4
9
=1
9√
2·rp
c1[u(H−¯H−d−ρ)−u(−ρ)]p
SA¯HK. (50)
The bound in (50) is not explicit in the sense that the quantity ρdefined in (41) depends
on the unknown probability P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg), which equals p+ϵ·P(h∗,s∗,a∗)(sk
h∗=
s∗, ak
h∗=a∗). We next lower bound the term on the right-hand-side of (50) in order to derive
the explicit bound given in Theorem 2.
Because ρsatisfies (41), by the first-order optimality condition, we have
1∈P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg)∂u(H−¯H−d−ρ) + (1 −P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg))∂u(−ρ).
(51)
35According to Assumption 1, we have H≥c2dwith c2>2. We choose
¯H=H
c2. (52)
Then, by the monotonicity of the subgradients of the concave function u, we obtain from (51)
that
1≤P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg)∂u
1−2
c2
H−ρ
+ (1−P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg))∂u(−ρ),
(53)
where the inequality means every element in the set on the right-hand-side is greater than
one. Recall that P(h∗,s∗,a∗)(sk¯H+d+1= ˜sg) =p+ϵ·P(h∗,s∗,a∗)(sk
h∗=s∗, ak
h∗=a∗). Then, we
can deduce from (51) that
1≤p·∂u
1−2
c2
H−ρ
+ (1−p)·∂u(−ρ), (54)
where we use the fact that ∂uis monotone so that all elements in the set ∂u((1−2
c2)H−ρ)−
∂u(−ρ) are all non-negative. Now consider the function p·∂u((1−2
c2)H−λ)+(1−p)·∂u(−λ)
forλ∈[0, ρ]. When λ= 0, it is clear that p·∂u((1−2
c2)H) + (1 −p)·∂u(0) contains an
element that is smaller than one, because 1 ∈∂u(0) and the elements in ∂u((1−2
c2)H) are
smaller than one. Together with (54) and the continuity of the subdifferential mapping, we
then deduce that there exists some λ∗∈[0, ρ] such that
1∈p·∂u
1−2
c2
H−λ∗
+ (1−p)·∂u(−λ∗). (55)
Now, we are ready to lower bound the right-hand-side of (50). Note that u(H−¯H−d−λ)−
u(−λ) is nondecreasing in λ∈[0, H−¯H−d]. Using (52) and the assumption H≥2c2d, we
then have
u(H−¯H−d−ρ)−u(−ρ)≥u
1−2
c2
H−λ∗
−u(−λ∗). (56)
For fixed c1≥4, we can choose
p= 1−2
c1≥1
2. (57)
It follows from (50) and (52) that
max
(h∗,s∗,a∗)∈ZRegret (M(h∗,s∗,a∗),algo, K)
≥1
9√
2·rp
c1[u(H−¯H−d−ρ)−u(−ρ)]p
SA¯HK
≥1
18√2c1c2·
u
1−2
c2
H−λ∗
−u(−λ∗)√
SAHK. (58)
36Finally, we need to make ϵin (48) satisfy the constraint (45). It is easy to check that
ϵ≤q
HSA
2c1c2K. Moreover, we have(1−2p)+q
1−4p
c1
2≥1
c1. Hence, we can choose K≥c1HSA
2c2to
make ϵin (48) feasible. The proof is therefore completed.
C.1 An Auxiliary Lemma and Its Proof
Recall that for any p, q∈(0,1) with p+q= 1, kl(p, q) denotes the KL divergence between two
Bernoulli distributions with success probabilities pandqrespectively, i.e.,
kl(p, q) =plogp
q
+qlog1−p
1−q
.
Lemma 13. Fix any constant c1≥2. Ifp∈[0,1−1
c1]andϵ∈"
0,(1−2p)+q
1−4p
c1
2#
, then we have
kl(p, p+ϵ)≤c1ϵ2
p.
Proof. Using the inequality log(1 + x)≤xfor any x >−1, we have
kl(p, p+ϵ) =plogp
p+ϵ
+ (1−p) log1−p
1−p−ϵ
≤pp
p+ϵ−1
+ (1−p)1−p
1−p−ϵ−1
=ϵ2
(p+ϵ)(1−p−ϵ)
(1)
≤c1ϵ2
p,
where inequality (1) holds if we have
p
c1≤p(1−p) + (1 −2p)ϵ−ϵ2.
One can easily verify that the above inequality holds if p∈[0,1−1
c1] and ϵ∈"
0,(1−2p)+q
1−4p
c1
2#
.
The proof is then completed.
D Numerical Experiments
In this section, we conduct numerical experiments to illustrate the performance of the OCE-VI
algorithm on randomly generated MDPs.
37We adopt the methods in (Dann 2019, Section 4.7) to randomly generate MDPs with state
spaceS={1,···, S}, action space A={1,···, A}and episode length H. For each h= 1,2, . . . , H ,
the transition probabilities Ph(·|s, a) are generated independently from the Dirichlet distribution
Dir(0.1,···,0.1). Reward functions rh(s, a) are set to 0 with probability 85% and generated
independently from the uniform distribution U[0,1] with probability 15%. In comparing the per-
formance of different learning algorithms, we assume that the reward functions are known, but the
transition probabilities are unknown.
In our experiments we consider two different OCEs6: entropic risk and mean-variance models.
For entropic risk, we compare the performance of our OCE-VI algorithm with the RSVI2 and RSQ2
algorithm in Fei et al. (2021). For mean-variance models, because there is no existing benchmark
algorithm in the episodic RL setting with recursive mean-variance criterion, we compare our OCE-
VI algorithm with the UCBVI-CH (with Chernoff-Hoeffding bonus) and UCBVI-BF (with Bernstein
bonus) algorithms in Azar et al. (2017) designed for the risk-neutral episodic RL. While the original
UCBVI algorithms in Azar et al. (2017) are developed for MDPs with stationary transitions, we
adapt them to our non-stationary MDP setting with time-dependent transition probabilities.
We consider two sets of parameters. The first one is ( H, S, A ) = (3 ,6,3), and we use the risk-
aversion parameter β=−0.6 for the entropic risk and c=1
6for the mean-variance models. We
setK= 106andδ=1
2KHfor all algorithms. The second one is ( H, S, A ) = (6 ,20,3), and we use
β=−0.6 for the entropic risk and c=1
12for the mean-variance models. Because the size of the
MDP becomes larger and learning can be more difficult in the second setting, we consider K= 107
to show the sublinear regret (in K) of algorithms.
Figures 1 and 2 illustrate the performance comparisons of the OCE-VI algorithm with other
algorithms, where we plot the average regret of each algorithm as a function of the number of
episodes K. We compute the expected regret of each algorithm by averaging over 30 independent
runs, but we do not plot the confidence intervals since the confidence intervals estimated from the
30 samples are very narrow compared with the magnitude of the regret and are almost invisible
in the figures. We can observe from Figures 1 and 2 that for episodic RL with recursive entropic
risk, our algorithm can outperform the RSVI2 algorithm in Fei et al. (2021) on randomly generated
MDPs in the same risk-sensitive RL setting. For episodic RL with recursive mean-variance models,
we find that our algorithm performs better than UCBVI algorithms in Azar et al. (2017), though
this is not surprising given that UCBVI is designed for the risk-netural RL setting.
6For CVaR, our OCE-VI algorithm is essentially the ICVaR algorithm Du et al. (2022), so we do not compare
their performances in the experiments.
38(a)
 (b)
Figure 1: Performance comparison of OCE-VI algorithm with other algorithms on a randomly
generated MDP with ( H, S, A ) = (3 ,6,3). Figure 1a is for episodic RL with recursive entropic risk
and Figure 1b is for the mean-variance models.
(a)
 (b)
Figure 2: Performance comparison of OCE-VI algorithm with other algorithms on a randomly
generated MDP with ( H, S, A ) = (6 ,20,3). Figure 2a is for episodic RL with recursive entropic
risk and Figure 2b is for the mean-variance models.
39