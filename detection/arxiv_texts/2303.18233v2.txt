arXiv:2303.18233v2  [math.ST]  4 Apr 2023INFERENCE ON EIGENVECTORS OF NON-SYMMETRIC MATRICES
JEROME R. SIMONS
Abstract. Please click here for the latest version. This paper argues t hat the symmetrisabil-
ity condition in Tyler (1981) is not necessary to establish a symptotic inference procedures for
eigenvectors. We establish distribution theory for a Wald a ndt-test for full-vector and indi-
vidual coeﬃcient hypotheses, respectively. Our test stati stics originate from eigenprojections
of non-symmetric matrices. Representing projections as a m apping from the underlying matrix
to its spectral data, we ﬁnd derivatives through analytic pe rturbation theory. These results
demonstrate how the analytic perturbation theory of Sun (19 91) is a useful tool in multivariate
statistics and are of independent interest. As an applicati on, we construct conﬁdence sets for
Bonacich centralities estimated from adjacency matrices i nduced by directed graphs.
1.Introduction
Finding eigenvectors of matrices is a fundamental task in ma ny disciplines. It is common for
eigenvectors to appear as stochastic objects. We consider h ypothesis testing for eigenvectors
and the construction of conﬁdence regions of a deterministi c matrix of which we have a random
estimate. Thereby the popular matrix denoising model appli es in our setting, with the exception
that we focus on ﬁxed-size matrices.
We assume that we observe a sample of nmatricesM1,...,M n, such that the columns of
an(Mn−M)converge weakly to a multivariate normal distribution cent ered at the columns
ofM∈Rp×pfor an increasing sequence an. We are interested in testing the associated null
hypothesis of υbelonging to an eigenspace of Mindexed by a collection of roots Laccording to
H0:υ∈eigLM
against the one-sided alternative H1:υ /∈eigLM. Associating eigenvectors with a set of roots,
we assume that eigenvalues split according to
L:=LI∪LJ.
A case distinction arises from the rank of υ:
Case1.rkυ=r≤ |LI|,i.e. the number of linearly independent columns in υis less than or
equal to the dimension of the invariant subspace spanned by t he vectors associated
with roots in LI. To accommodate this case, deﬁne the null hypothesis
(1.1) H0:υ∈eigLIM
against the alternative H1:υ /∈eigLIM, where eigLIMrefers to the eigenspace
associated with roots in LI.
Case2.rkυ=r >|LI|, i.e. the number of linearly independent columns in υexceeds the
number of roots in LI. We deﬁne the null hypothesis
H∗
0:eigLIM∈spυ,
Date: 27 March ‘23.
1which can be tested by constructing the p×qmatrixυ⊥such thatυT
⊥υ= 0. Conse-
quently, we write
(1.2) H∗
0:υ⊥∈eigLJ/negationslash=IM.
Case 1 is intuitive because the columns of υspan a lower-dimensional subspace than eigLMwhile
Case 2 is eﬀectively an overdetermination of the eigenspace of interest so that the estimated
eigenspace eigLIM, under the null hypothesis, is wholly contained in the candi date space spυ.
Tyler (1981) implements Case 2 by inverting the hypothesis ( 1.1) so that Case 2 becomes (1.2)
and thereby reduces to Case 1. The orthocomplement, υ⊥, is always easy to compute from υ
using, e.g. an LU factorization or the Gram-Schmidt algorit hm. Moreover, for the beneﬁt of the
reader wanting to appreciate the details of the perturbatio n theory, the notation of H∗
0implies
simpler notation. We shall therefore write our arguments in the form of Case 2, where inference
onυ⊥belonging to eigLJimplies inference on υwithout loss of generality.
The main limitation of the existing approaches is that we hav e to assume that Mis such that
ΓMis symmetric for a positive deﬁnite symmetric matrix Γ. We illustrate this issue with
Example 1. The matrix M1is symmetric in the metric of Γfor
Γ =/bracketleftBigg
2 1
1 6/bracketrightBigg
M1=/bracketleftBigg
1 3
1 1/bracketrightBigg
whereasM2is not symmetric in the metric of Γ
M2=/bracketleftBigg
1−3
−1 1/bracketrightBigg
.
Any2×2matrixΓwould have to be negative deﬁnite for M1Γto be symmetric. Therefore,
inferential procedures for non-symmetric matrices apply t oM1but not toM2. In the following,
we shall establish that no such requirement is necessary for inference on eigenspaces of non-
symmetric matrices.
1.1.Related literature on eigenspace inference. Statistical analysis of eigenvectors and
eigenvalues ﬁnds use in diverse applications such as quasi- cointegration analysis, the study of
centrality in social networks (Bonacich 1987), or in radio si gnal processing (Penna, Garello, and
Spirito 2009).
The study of inference and estimation of eigenvectors of mor e general, dense square random
matricesM∈Rp×pwithpﬁxed began with the seminal study of Davis (1977) in the non-n ormal
case. James (1977) posed the problem of eigenvector inferen ce for general eigenvectors without
associating them with a particular set of eigenvalues. Tyle r (1981) considers null hypotheses of
eigenvectors associated with particular eigenvalues. His study served as the inspiration for our
paper and requires quasi-symmetry of M, which we relax.
The ﬁrst study to examine spectra of symmetric random matric es is the seminal paper by
Anderson (1963), which provided joint density functions fo r eigenvectors and eigenvalues of
covariance matrices and a testing procedure for equality of eigenvalues. Subsequently, James
(1964) oﬀered a general method to obtain the distribution of statistics derived from random
matrices. Since then, the ﬁeld of eigenspace statistics bas ed on random matrices has progressed
along two strands.
The ﬁrst branch of the literature considers the spectra of Wi shart matrices. To see how they
arise, consider a matrix A∈Rp×nwhose columns Aiare independent and normal according to
2Ai∼Np(0,Ω)whereΩ∈Rp×pis a positive-deﬁnite covariance matrix. The variate M:=AAT
will then have a central, p-variate Wishart distribution with ndegrees of freedom, usually denoted
byM∼Wp(Ω,n). A representative study with a further overview of the devel opment of density
functions of such matrices is given in Zanella, Chiani, and W in (2009).1Further reﬁnements
include expanding or restricting the spectral structure su ch as the study by Takemura and
Sheena (2005) who derive the asymptotic distribution of eig envalues and eigenvectors of Wishart
matrices in the case of inﬁnitely dispersed population eige nvalues.
A second strand of inference on eigenspaces are the spectra o f symmetric square matrices
M∈Rp×pwithp→ ∞ . The often sparse nature of this type of matrix implies that o ne
can truncate spectral data or singular value decomposition to estimate low-rank representations
of such matrices. This literature began with the semi-circl e law of Wigner (1958) for the joint
eigenvalue distribution and Tracy and Widom (1994) for the d istribution of the largest eigenvalue.
For applied researchers, Chen, Cheng, and Fan (2020) develo p a method to conduct inference on
low-rank representations of random matrices that obtain fr om deterministic symmetric matrices
that are (potentially asymmetrically) perturbed. As a furt her reﬁnement of this case, Cheng,
Wei, and Chen (2021) consider matrices with ﬁnely spaced eig envalues while Iain M Johnstone,
Onatski, et al. (2020) develop a testing framework for high- dimensional matrices with a single,
dominant eigenvalue, or spike. That latter phenomenon arises for example in auto-covaria nce
matrices of embedded time series if the process possesses a r oot of unity. In the class of low-rank
matrices with p→ ∞, Cheng, Wei, and Chen (2021) oﬀer an estimation and inferenc e procedure
for eigenvectors of such random matrices. Bura and Pfeiﬀer (2 008) add theory for inference on
singular vectors.
A subspecialty of the aforementioned literature studies es timation and inference procedures
for eigenvectors and principal component vectors of covari ance matrices, as they arise in in factor
models. If the column dimension of Andiverges as n→ ∞, central limit theorems imply that a
suitably normalized Mn:=AnAT
nconverges to a central Wishart matrix as above. This result
nests the limiting distributions of a large class of covaria nce matrix estimators. Tyler (1983)
provides a procedure to compute asymptotic conﬁdence sets f or principal components which
obtain from eigenspaces of covariance matrices. Iain M. Joh nstone (2001) and Lee and Schnelli
(2016) contribute results on the largest eigenvalue statis tics of principal component estimators
and covariance matrices. Koltchinskii, Löﬄer, Nickl, et al . (2020) consider a representative
functional extension and focus on eﬃciently estimating lin ear functionals of principal components,
which have symmetric underlying covariance matrices.
A related literature studies the singular value decomposit ion of rectangular random matrices
M∈Rp×dwherep,d <∞andpanddare ﬁxed. Cai et al. (2019) oﬀer a method to estimate
the column space via a singular value decomposition and Girk o (1999) provides conditions for
and derive a convergence law for associated singular values .
2.Assumptions
We work with the matrix M∈Rp×pwith associated estimators ˆMn. To simplify the subse-
quent exposition and to more easily deﬁne test statistics, w e vectorise the matrix Mfor most of
the arguments. The following deﬁnes our framework:
1Owing to their use as representations of physical systems, s tudies in eigenstatistics of Wishart matrices tend to
prioritize those with complex entries. For the social scien ces, this literature is therefore less useful because resul ts
derived using complex matrices do not, in general, apply to r eal matrices.
3Assumption 2. LetˆMnbe an estimator of MandˆΩbe an estimator of Ω. Then,
(1)√nvec/parenleftBig
ˆMn−M/parenrightBig
/squigglerightN(0,Ω)for a full rank covariance matrix Ω∈Rp2×p2. The
columns of√n/parenleftBig
ˆMn−M/parenrightBig
/squigglerightN(0,ΩM)for a full rank Ω∈Rp×pso thatΩ =Ip⊗ΩM.
(2)ˆMnp→M.
(3) For a covariance matrix estimator, ˆΩp→Ω.
(4) The set of roots of interest, LI, is closed under conjugation, i.e. for any z∈ LI, the
complex conjugate ¯z∈ LI.
(5) Estimators of right and left eigenvectors ˆRnandˆLnconverge in probability to their pop-
ulation analogues.
(6) The eigenvalues of Mcorrespond to unit blocks in the Jordan normal form of M.
(7)LI∩LJ=∅.
Assumption 2.5 may be relaxed with very little additional wo rk. We do not pursue this
extension to focus on the perturbation theory. Assumption 2 .7 is fairly innocuous and simply
means that we can discriminate between eigenvalues of inter est and remaining ones. Assumption
2.4 simpliﬁes our arguments but is also not strictly necessa ry.
3.Eigenprojections as functionals of underlying matrix
Before introducing our test statistics, let us study eigenpr ojections as our key ingredient for
a test statistic. Deﬁne the eigenprojection over the subspa ce spanned by eigenvectors associated
with roots in LI, for right and left eigenvectors associated with root λ,rλandlλ, via
PI:=/summationdisplay
λ∈LIλrλl⊺
λ.
To see eigenprojections in practice, we consider
Example 3. Let
(3.1) M=/bracketleftBigg
0.8 0.5
0 0.4/bracketrightBigg
.
Then, right eigenvectors are r⊺
1= [1 0]andr⊺
2= [−0.781 0.625]and eigenvalues are λ1= 0.8and
λ2= 0.4. Its left eigenvectors are l⊺
1= [1 1.25]andl⊺
2= [0 1.6]. The associated eigenprojections
(3.2) Pλi=ril⊺
i
fori∈ {1,2}
Pλ1=/bracketleftBigg
1 1.25
0 00/bracketrightBigg
Pλ2=/bracketleftBigg
0−1.25
0 1/bracketrightBigg
We see that Pλiare rank-1 projection matrices for which PλiPλj=δijand ifυbelongs to an
eigenspace associated with root λi, we haveυPλi=PλiυandPλ1+Pλ2=I. Finally, under the
maintained null hypothesis,
Pλiυ=υ,
from which we can build a test statistic.
For the normalisation of test statistics, we shall need a gen eralized inverse for a matrix A.
which is a matrix A+such thatAA+A=A. For a matrix of rank s, let the eigenprojection be
4as in Example 3. Then, a generalized inverse
(3.3) A+:=s/summationdisplay
i=1λ−1
iPλi.
Respecting the division of eigenvalues
M=RIΛILT
I+RJΛJLT
J
R=/bracketleftBig
RIRI/bracketrightBig
L=/bracketleftBig
LILJ/bracketrightBig
Λ =/bracketleftBigg
ΛI0
0 ΛJ/bracketrightBigg
, (3.4)
we introduce the skew projection
(3.5) PI:=RILT
I.
If the roots in the set LIare closed under conjugation, under Assumption 2.4, then we may take
RIandLIto be real without loss of generality. We proceed with testin gH0:υ∈eigLIvia
requiringυT
⊥PI= 0.
Invariant subspace functionals. Our study proceeds by considering the map ψfromMto its
invariant subspaces, which we specialise to eigenspaces. Bu ilding on the perturbation theory in
Stewart and Sun (1991), we compute ﬁrst and second order deri vatives ofψ. This approach has
the advantage that its conclusions apply to any invariant su bspace map ψ. For a member of an
invariant subspace X1∈Rp×r, the deﬁning equality holds
(3.6) MX1=X1A1
for some matrix A1∈Rr×r. Thereby, we nest the Schur decomposition for A1being a Schur
form ofM. Let the ring of matrices satisfying Assumption 2 be Q. Then, deﬁne the map
ψ:Q/ma√sto→Rr×qby
(3.7) ψ(M):=υ⊺
⊥PI(M).
Introduce the following notation for convenience
(3.8) ˆ∆n,IJ=L⊺
J/parenleftBig
ˆMn−M/parenrightBig
RI,
and the operator S:Rr×q→Rr×q
(3.9) S(Q) =QΛI−ΛJQ,
whereQ∈Rr×q. Then, the ﬁrst-order Fréchet derivative of ψis
(3.10) ˙ψ(M) =RIS−1/parenleftBig
ˆ∆JI,n/parenrightBig
L⊺
Jυ.
Before concluding this section with our main result, we deﬁne the Jacobian matrix of a vec-
torized, matrix-valued function F(X) :Rp×p→Rr×qvia ﬁrst diﬀerentials as Jsatisfying
dF=JdvecX.
We conclude with
Lemma 4. Let the map ψ(M)be as in (3.7). Then, the Jacobian matrix with respect to the
eigenspace spanned by the columns of LJisBsuch that dvecψ=BdvecRIand
(3.11) B=/parenleftBig
LI⊗υT
⊥RJ/parenrightBig/braceleftBig/parenleftBig
ΛT
I⊗IJ/parenrightBig
−(II⊗ΛJ)/bracerightBig−1/parenleftBig
Iq⊗LT
J/parenrightBig
.
54.Asymptotic distribution of estimators
We deﬁne the estimator of the eigenprojection υ⊺
⊥ˆPI/parenleftBig
ˆM/parenrightBig
:=υ⊺
⊥ˆRIˆLI. Then, we obtain
Proposition 5. Suppose Assumption 2 holds. Then,
√nvecυ⊺
⊥ˆPI/parenleftBig
ˆM/parenrightBig
/squigglerightN(0,BΩB).
Proposition 5 forms the basis for the Wald statistic and is ou r prototype statistic. In some
cases, it is convenient to have a unique orthocomplement est imator with a normalization of the
test matrices υandυ⊥that eﬀects interpretable coeﬃcients.
Eigenvectors are only unique up to a normalisation. We there fore introduce the normalisation
for a coeﬃcient matrix D∈Rq×r. The normalization of υis such that υT
⊥Nυ⊥=IpforN=/bracketleftbigIr0
0 0/bracketrightbig
∈Rp×p. The test matrices become
(4.1) υ:=/bracketleftBigg
D
Ir/bracketrightBigg
,
and
(4.2) υ⊥:=/bracketleftBigg
Ir
−D/bracketrightBigg
Unfortunately, uniqueness comes at the steep cost of making the second order moment potentially
unbounded as Anderson (2010) discusses. Letting ΩMbe as in Assumption 2.1, we have the
alternative normalization υT
k,⊥ΩMυk,⊥=Ipfor every column kinυ.2
We deﬁne the set estimator
(4.3) /hatwideυ⊥,n:=/braceleftBig
υ⊥∈Rp:υT
⊥ˆPi,n= 0/bracerightBig
,
whereˆPI,n:=ˆRI,nˆLT
I,nuses sample analogues of RIandLI. For a closed-form expression of ˆDn
under the normalization (4.2), partition RIconformably for RI,1∈Rr×qandRI,2∈Rq×q.
(4.4)/bracketleftBigg
RI,1
RI,2/bracketrightBigg
:=RI.
Consequently, deﬁne
(4.5) ˆDn=/parenleftBig
ˆRI,2,n/parenrightBig−1ˆRT
I,1,n.
and observe that (4.5) deﬁnes ˆDnuniquely. Then, we obtain
Theorem 6. LetˆDnbe as in (4.5)and suppose Assumption 2 holds. Then,
n1/2vec/braceleftBig
ˆDT
n−DT/bracerightBig
/squigglerightN/parenleftBig
0,BAΩMBT
A/parenrightBig
for
BA:=/parenleftBig
R−1
I,2⊗υT
⊥RJ/parenrightBig
{/parenleftBig
ΛT
I⊗IJ/parenrightBig
−(II⊗ΛJ)}−1(RT
I⊗LT
J).
Finally, for covariance matrix estimators we record
Lemma 7. Suppose Assumption 2 holds, then, whenever ˆBp→B,ˆBˆΩˆBTp→BΩMB.
2For an approach that avoids normalizations see Silin and Fan (2020) in the context of symmetric matrices.
65.tests
We deﬁne a Wald test statistic for H∗
0:υ⊺
⊥PI= 0. Based on Proposition 5 we deﬁne a Wald
test statistic for ˆΩ+
Wdenoting a generalised inverse of ˆΩW,
(5.1) ˆWn(υ⊥):=vec/parenleftBig
υ⊺
⊥ˆPI/parenrightBigTˆΩ+
Wvec/parenleftBig
υ⊺
⊥ˆPI/parenrightBig
and an associated covariance matrix estimator ˆΩW=ˆBWˆΩˆBWfor
(5.2) ˆBW:=/parenleftBig
ˆLI⊗υT
⊥ˆRJ/parenrightBig/braceleftBig/parenleftBig
ˆΛT
I⊗IJ/parenrightBig
−/parenleftBig
II⊗ˆΛJ/parenrightBig/bracerightBig−1/parenleftBig
ˆRT
I⊗ˆLT
J/parenrightBig
.
Theorem 8. Suppose Assumption 2 holds and let. Then,
ˆWn(υ⊥)/squigglerightχ2
qm. (5.3)
The test proposed in Theorem 8 is for a full-vector hypothesi s on the eigenspace. To be able
to conduct subset inference, we wish to consider a null hypot hesis about individual coeﬃcients.
LetˆR−1
I,2be the empirical analogue of R−1
I,2deﬁned in (4.4) and deﬁne analogously to (5.2),
ˆBD:=/parenleftBig
ˆR−1
I,2⊗ˆυT
⊥ˆRJ/parenrightBig/braceleftBig/parenleftBig
ΛT
I⊗IJ/parenrightBig
−(II⊗ΛJ)/bracerightBig−1/parenleftBig
ˆRT
I⊗ˆLT
J/parenrightBig
and deﬁne a t-test statistic for inference on the ijth coeﬃcient of Dvia
(5.4) tn(a0):=dij−d0
ˆσij,
whereˆσ2
ij:=/parenleftBig
ˆΩ−1
D/parenrightBig
ijandˆΩD:=ˆBDΩˆBT
D.
Theorem 9. Suppose Assumption 2 holds, then
ti,n(d0)/squigglerightN(0,1).
6.Application
InterpretAas representing a graph with vertices indexed by iandjand entries representing
trade ﬂows qij>0if ﬁrmiis a net seller to ﬁrm jandqij<0otherwise. Then, the matrix A
captures the directed graph of the trade network. The import ance scoresi,
(6.1) si:=1
λ/summationdisplay
j∈Nisj,
measures the relative importance of ﬁrm iin the network. The sum runs over all jthat are
direct neighbours of ﬁrm i, denoted by Niandλis a constant. The measure siis known as the
Katz or eigenvector centrality, and was ﬁrst introduced by Bo nacich (1987). For the full vector
of scoress, we obtain the relationship
(6.2) As=λs,
which deﬁnes an eigenvector problem. If the matrix Ais estimated with a degree of uncertainty
or measurement error, we will be able to test hypotheses of th e form
H0:sis a vector of importance scores.
which we construct via positing that s∈Rp×1is an eigenvector of the adjacency matrix A
belonging to the largest eigenvalue λagainst the alternative that some entries do not contain
7importance scores. To derive an asymptotically valid conﬁd ence set of importance scores, we use
Theorem 8 for an inversion of the Wald test. Find the orthocom plements⊥∈Rp×(p−1)such
thatsT
⊥s= 0and deﬁne a level αconﬁdence set as
C:=/braceleftBig
ssuch that ˆWn(s⊥)≤χ2
1−α/bracerightBig
.
Ifλ∈Cand|λ|is maximal across LAthen we take the two eigenvectors corresponding to λand
its complex conjugate, and set wi:=|si|withwireplacingsieverywhere.
An extension left for a future development of this section is the simulation of this method to
study the performance numerically.
7.Conclusion
This paper has extended the theory of Tyler (1981) to cover no n-symmetric matrices. In
addition to testing full vector hypotheses, we also provide a t-test on individual coeﬃcients of
the vector and derive the asymptotic distribution of the est imator for normalised eigenvectors.
We caution the reader that normalised estimates may suﬀer fr om a potentially unbounded second
moment. However, we believe that the beneﬁts of being able to normalise coeﬃcients outweigh
its drawbacks.
We have also shown how to apply the analytic perturbation the ory of Sun (1991) to the problem
of inference on eigenvectors and hope that our exposition ca n aid researchers in similar settings.
Naturally, the perturbation theory of Kato (1995) delivers the same results. For a pedagogic
introduction to the subject, we refer the reader to Greenbau m, Li, and Overton (2019).
The leading application of our results is to graph Laplacian s or adjacency matrices that di-
rected graphs induce. A future version of this paper will oﬀe r further statistical development of
this application.
8.Proofs
8.1.Expansions of eigenprojections. We remark that the perturbation theory employed in
this paper does not stipulate a normalisation for eigenvect ors, which is innocuous; derivatives of
normalised maps deliver the same results (Duﬀy and Simons 20 20).
In addition to the map ψ, deﬁne the following notation:
(8.1)/bracketleftBiggˆ∆n,I
ˆ∆n,SLˆ∆n,J/bracketrightBigg
:=
LT
I/parenleftBig
ˆMn−M/parenrightBig
RI
LT
J/parenleftBig
ˆMn−M/parenrightBig
RILT
J/parenleftBig
ˆMn−M/parenrightBig
RJ
.
We collect ﬁrst and second-order derivatives in
Lemma 10. Suppose Assumption 2 holds. Then the ﬁrst and second order Fr échet derivatives
ofψ(M)atMare
˙ψ/parenleftBig
ˆMn−M/parenrightBig
=υT
⊥RJS−1/parenleftBig
ˆ∆n,SL/parenrightBig
LT
I (8.2)
¨ψ/parenleftBig
ˆMn−M/parenrightBig
=2υT
⊥RJS−1/parenleftBig/parenleftBig
ˆ∆n,JS−1/parenleftBig
ˆ∆n,IJ/parenrightBig
−S−1/parenleftBig
ˆ∆n,IJ/parenrightBig
ˆ∆n,I/parenrightBig/parenrightBig
LT
I. (8.3)
Record the following result for the expansion of ψand denote by r(.)the remainder term of
the asymptotic expansion.
Lemma 11. (1) We have the expansion
√nυT
⊥/parenleftBig
ˆPn,I−PI/parenrightBig
=√nυT
⊥ˆRn,JS−1(∆n,IJ)ˆLT
n,I+√nr/parenleftBig
ˆMn−M/parenrightBig
8(2) The remainder term obeys r/parenleftBig
ˆMn−M/parenrightBig
=O/parenleftbig
n−1/parenrightbig
.
8.2.Asymptotic distribution of eigenprojection and Wald test s tatistic. Further, deﬁne
BT
IJ:=RI⊗LJ. Then, we have
Lemma 12. Let∆n,IJbe as in Lemma 10. Then, for υ⊥∈Rp×(p−m),
(1)√nvec∆n,IJ/squigglerightN/parenleftbig
0,BIJΩBT
IJ/parenrightbig
.
(2)√nvec/braceleftBig
υT
⊥ˆRn,JS−1(∆n,IJ)ˆLT
n,I/bracerightBig
/squigglerightN(0,ΩW)
(3)rkΩW=qm.
Proposition 5 follows from Lemma 10, Lemma 11, and Lemma 12. L emma 7 follows from the
continuous mapping theorem.
To prove Theorem 8, recall that
υT
⊥ˆPn,I=υT
⊥/parenleftBig
ˆPn,I−PI/parenrightBig
becauseυT
⊥PI= 0under the maintained hypothesis. By Lemma 111, we obtain the e xpansion
υT
⊥/parenleftBig
ˆPn,I−PI/parenrightBig
=√nυT
⊥Rn,JS−1/parenleftBig
ˆ∆n,IJ/parenrightBig
LT
n,I+√nr/parenleftBig
ˆMn−M/parenrightBig
.
The ﬁrst term in the preceding display converges according t o Lemma 12, while the second
converges in probability to zero by Lemma 112.
For convergence rates, we record:
Corollary 13. Letˆ∆n,iwithi∈ {I,IJ,J}be as in (8.1). Then,
/bracketleftBiggˆ∆n,I
ˆ∆n,IJˆ∆n,J/bracketrightBigg
=/bracketleftBigg
Op/parenleftbig
n−1/2/parenrightbig
Op/parenleftbig
n−1/2/parenrightbig
Op/parenleftbig
n−1/2/parenrightbig/bracketrightBigg
.
8.3.Asymptotic distributions of test statistics. To prove Theorem 6, apply to√n/parenleftBig
ˆDT
n−DT/parenrightBig
the expression given in the following
Lemma 14. Let the estimator ˆDn=ˆRn,2,I/parenleftBig
ˆRn,1,I/parenrightBig−1
. Then, the centered estimator
(8.4)/parenleftBig
ˆDn−D/parenrightBigT
=υT
⊥/parenleftBig
ˆRIˆR−1
2,I−RIR−1
2,I/parenrightBig
.
The following result is a corollary to Lemma 11.
Corollary 15. (1) We have the expansion
√n/parenleftBig
ˆDT
n−DT/parenrightBig
=√nυT
⊥ˆRn,JS−1(∆n,IJ)R−1
n,2,I+√nr/parenleftBig
ˆMn−M/parenrightBig
(2) The remainder term obeys r/parenleftBig
ˆMn−M/parenrightBig
=O/parenleftbig
n−1/parenrightbig
.
Then, we obtain the desired result by applying Lemma 12 to the expansion obtained in Corol-
lary 15 by substituting R−1
n,2,IforLT
n,I.
In the following, we show two results that should be compared to their analogues in Lemma 10
and Lemma 11. Moreover, deﬁne in analogy to (3.7) the map ψD:Q/ma√sto→Rr×qwithψD(M):=
υT
⊥RI(M)R−1
2,I. Then, we have
Corollary 16. The Fréchet derivative of ψD,
˙ψD/parenleftBig
ˆMn−M/parenrightBig
=υT
⊥Rn,JS−1/parenleftBig
ˆ∆n,SL/parenrightBig
LT
n,IR−1
n,2,I.
Theorem 6 follows from Lemma 14, Corollary 16, and Corollary 15.
98.4.Proofs for perturbation expansions.
Proof of Lemma 10. We wish to construct the ﬁrst and second derivatives of the ma p
ψ(M(t)) =υT
⊥RI(t)LT
I(t) =υT
⊥PI(t),
where we have made the dependence on texplicit and M(0) =M. Then based on Sun (1991,
Remark 4.2), the Fréchet and Gateaux derivatives coincide a nd we can write
˙ψM/parenleftBig
ˆMn−M/parenrightBig
=υT
⊥˙PI(t)|t=0,
which by the product rule implies
υT
⊥˙P(0) =υT
⊥˙RI(0)LT
I+υT
⊥RI˙LT
I(0) =υT
⊥˙RI(0)LT
I
becauseυT
⊥RI(0) =υT
⊥RI= 0. /square
Proof of Corollary 13. The result follows from Assumption 2(1) and the deﬁnition in (8.1)./square
Proof of Lemma 12. (1) By Lemma 10,
vecˆ∆n,IJ=/parenleftBig
RT
I⊗LT
J/parenrightBig
vec/parenleftBig
ˆMn−M/parenrightBig
.
The result follows by the continuous mapping theorem and Ass umption 2.
(2) By the preceding argument, and the deﬁnition of S−1in Lemma 10, we see that
υT
⊥Rn,JS−1/parenleftBig
ˆ∆n,IJ/parenrightBig
LT
n,I
is a linear and continuous transformation of ˆ∆n,IJ. Vectorizing S−1/parenleftBig
ˆ∆n,IJ/parenrightBig
and taking
the expectation of the outer product provides the result.
(3) We construct an adjacent matrix and shall argue by contin uity to establish the rank. In
particular, if we can show that Ωis of rankqm, we can argue by continuity to establish
the result. For ΛI≈Iq, the matrix ΩWis close to Σ
Σ:=BSΩMBT
S
BS:=/parenleftBig
LIRT
I⊗υT
⊥RJ(IJ−ΛJ)−1LT
J/parenrightBig
We begin by showing that rk υT
⊥RJ(IJ−ΛJ)−1LT
J=m.By Thm. 2.4 in Gohberg, Lan-
caster, and Rodman (1982) we know that
M−1(µ) =RI(II−ΛI)−1LT
I+RJ(IJ−ΛJ)−1LT
J
withµ /∈ L. Now, we wish to restrict ourselves to the subspace R⊥
I:= spR⊥
Iand hence
consider
υT
⊥M−1(µ) =υT
⊥RJ(IJµ−ΛJ)−1LT
J.
Postmultiply by M(µ)to obtain
υT
⊥=υT
⊥RJ(IJµ−ΛJ)−1LT
JM(µ),
forµ /∈ L.By continuity of Masµ→1we can extend the allowable µ. Now, since
rk/parenleftBig
υT
⊥RJ(IJ−ΛJ)−1LT
JM(1)/parenrightBig
≤min/braceleftBig
rkυT
⊥RJ(IJµ−ΛJ)−1LT
J,rkM(1)/bracerightBig
10and rkM(1)≥mand as a result of the preceding displayed equation we arrive at
rk/parenleftBig
υT
⊥RJ(IJ−ΛJ)−1LT
JM(1)/parenrightBig
=m.
We have rk υT
⊥RJ(IJµ−ΛJ)−1LT
J≥m. Finally, we observe that
rkυT
⊥RJ(IJµ−ΛJ)−1LT
J≤m
because it is m×p−qandm≤p−q. Now, under Assumption 2, ΩMhas full rank and
the result follows.
/square
Proof of Lemma 11.
(1) We Taylor-expand
√nυT
⊥ˆPn,I=√nψ/parenleftBig
ˆMn/parenrightBig
=(2)√n/parenleftBig
ψ/parenleftBig
ˆMn/parenrightBig
−ψ(M)/parenrightBig
=√n˙ψ/parenleftBig
ˆMn−M/parenrightBig
+√nr/parenleftBig
ˆMn−M/parenrightBig
=(3)√nυT
⊥Rn,JS−1/parenleftBig
ˆ∆n,IJ/parenrightBig
LT
n,I+√nr/parenleftBig
ˆMn−M/parenrightBig
where=(2)follows from application of the null hypothesis and =(3)follows from Lemma
10.
(2) We obtain a remainder term
r/parenleftBig
ˆM−M/parenrightBig
:=ψ/parenleftBig
ˆM/parenrightBig
−ψ(M)−˙ψ/parenleftBig
ˆM−M/parenrightBig
.
To ensure that the ﬁrst order term dominates the remainder, w e need to verify that
standardisation by√ndoes not cause dominant second order terms. By Assumption 2,/vextenddouble/vextenddouble/vextenddoubleˆM−M/vextenddouble/vextenddouble/vextenddouble=Op/parenleftbig
n−1/2/parenrightbig
. By Taylor’s theorem, r/parenleftBig
ˆM−M/parenrightBig
=O/parenleftbig
n−1/2/parenrightbig
. For simplicity,
we can treat/parenleftBig
ˆMn−M/parenrightBig
as a deterministic sequence and then apply Vaart (2000, Lemm a
2.12, Ch. 2) to obtain the relevant statistical result. Firs t, recognize that S−1(anM) =
anS−1(M)for any scalar sequence anand admissible argument M. Then, rewrite ras
the sum of the next higher order term and another unspeciﬁed r emainder term of known
order via application of Lemma 10:
r/parenleftBig
ˆMn−M/parenrightBig
= 2υT
⊥RJS−1/parenleftBig/parenleftBig
ˆ∆n,JV−Vˆ∆n,I/parenrightBig/parenrightBig
LT
I+r2/parenleftBig
ˆMn−M/parenrightBig
,
wherer2collects higher order terms. By Corollary 13 and Lemma 10, we o btain the
tighter bound n1/2r/parenleftBig
ˆMn−M/parenrightBig
=O/parenleftbig
n−1/2/parenrightbig
, which implies that normalization by√nis
innocuous and does not aﬀect convergence.
/square
Proof of Lemma 14. Add and subtract υT
⊥to obtain ([ˆDT
nIr]±[DTIr])Rn,I= 0. Therefore,
/parenleftBig
ˆDT
n−DT/parenrightBig
ˆRn,2,I=/bracketleftBig
ˆDT
nIr/bracketrightBig/parenleftBig
ˆRn,I−RIR−1
2,IˆRn,2,I/parenrightBig
,
whence the result follows. /square
Proof of Corollary 16. In Lemma 10, replace LT
IbyR−1
2,Iand the result follows. /square
11Proof of Corollary 15. Apply Corollary 16 to the expression in Lemma 14 and the resul t follows.
/square
8.5.Perturbation of invariant subspaces. To prove Lemma 10, we ﬁnd an expansion of
basis vectors of invariant subspaces of a matrix in response to a small perturbation of that
matrix. To aid with the general exposition of the idea behind invariant subspaces and associated
perturbations, we adopt the notation of Sun (1991). Similar results to those considered here are
available in Kato (1995, Ch. 2).
LetX1∈Rp×qwithrkX1=q,XT
1X1=IqandAX1=X1A1for some matrix A1∈Rq×q.
ThenspX1is an invariant subspace of A∈Rp×pif and only if there exists a non-singular matrix
X= [X1X2]∈Rp×pwithXT
2X2=Ip−qsuch that
X−1AX=/bracketleftBigg
A11A12
0A22/bracketrightBigg
, A11∈Rq×q.
We now introduce the perturbation matrix of A,
C=/bracketleftBigg
C11C12
C21C22/bracketrightBigg
.
The blockC21measures the departure from Bbeing upper-triangular. Letting t∈Cwe perturb
Aand obtain A(t):=A+tC.For the purposes of this study, we are interested in the span o f
the vectors that are orthogonal to the columns of X1.Recall the operator (3.9),
S(Q) =QA11−A22Q
andQ∈R(p−q)×q.Then, ifA11andA22do not share any eigenvalues, Theorem 2.1 in Sun (1991)
applies. Introducing the parameter t∈Cto help us keep track of the order of the expansion.
We state Sun (1991, Thm. 1):
Theorem 17. There exists a unique q-dimensional invariant subspace spX1(t)ofA(t) (t∈C)
such that spX1(0) = spX1and the basis vectors (columns of X1) may be deﬁned to be analytic
functions of tin some neighbourhood of the origin of C. Further, the analytic matrix-valued
functionX1(t)has the second order perturbation expansion
X1(t) =X1 (8.5)
+X2S−1(C21)t
+X2S−1/parenleftbig
C22S−1(C21)−S−1(C21)C11−S−1(C21)A12S−1(C21)/parenrightbig
t2
+O/parenleftbig
t3/parenrightbig
.
We now interpret Theorem 17 to make it amenable for our statis tical analysis. Expression
(8.5) shows how small perturbations in the matrix Aaﬀect the column space of X1.which
correspond to the ﬁrst and second-order derivatives. The eq uivalents of AandX1areΛandRI
respectively. Furthermore, A11:= ΛIandA22:= ΛJ, so that by Assumption 2.7, A11andA22
have no eigenvalues in common.
References
Anderson, Theodore W. (1963). “Asymptotic theory for princ ipal component analysis”. In: The
Annals of Mathematical Statistics 34.1, pp. 122–148.
12Anderson, Theodore W. (2010). “The LIML estimator has ﬁnite moments!” In: Journal of Econo-
metrics 157.2, pp. 359–361. issn: 0304-4076. doi:10.1016/j.jeconom.2010.02.004 .url:
http://www.sciencedirect.com/science/article/pii/S0 304407610000801 .
Bonacich, Phillip (1987). “Power and centrality: A family of measures”. In: American Journal of
Sociology 92.5, pp. 1170–1182.
Bura, E and R Pfeiﬀer (2008). “On the distribution of the left s ingular vectors of a random
matrix and its applications”. In: Statistics & Probability Letters 78.15, pp. 2275–2280.
Cai, Changxiao et al. (2019). “Subspace Estimation from Unb alanced and Incomplete Data Ma-
trices: Statistical Guarantees”. In: arXiv preprint arXiv:1910.04267 .
Chen, Yuxin, Chen Cheng, and Jianqing Fan (2020). Asymmetry Helps: Eigenvalue and Eigenvec-
tor Analyses of Asymmetrically Perturbed Low-Rank Matrice s. arXiv:1811.12804 [math.ST] .
Cheng, Chen, Yuting Wei, and Yuxin Chen (2021). Tackling small eigen-gaps: Fine-grained eigen-
vector estimation and inference under heteroscedastic noi se. arXiv:2001.04620 [math.ST] .
Davis, AW (1977). “Asymptotic Theory for Principal Compone nt Analysis: Non-normal Case”.
In:Australian Journal of Statistics 19.3, pp. 206–212.
Duﬀy, James A. and Jerome R. Simons (2020). The Cointegrated VAR without Unit Roots:
Representation Theory and Asymptotics . arXiv:2002.08092 [econ.EM] .
Girko, V. L. (1999). “Strong Law for the singular values and e igenvectors of random matrices
III. Inequalities for the spectral radius of large random ma trices”. In: Random Operators and
Stochastic Equations 7.2, pp. 177–200. doi:https://doi.org/10.1515/rose.1999.7.2.177 .
url:https://www.degruyter.com/view/journals/rose/7/2/ar ticle-p177.xml .
Gohberg, I, P. Lancaster, and L. Rodman (1982). Matrix Polynomials . Society for Industrial and
Applied Mathematics. url:341000/082996b96e33bfa23b3ed8decb058625 .
Greenbaum, Anne, Ren-cang Li, and Michael L. Overton (Mar. 2 , 2019). “First-order Perturba-
tion Theory for Eigenvalues and Eigenvectors”. Version 1. I n: arXiv: http://arxiv.org/abs/1903.00785v2 [math.NA, cs.NA, 47A 55, 65F15] .
url:http://arxiv.org/abs/1903.00785v2 .
James, Alan T. (1964). “Distributions of Matrix Variates an d Latent Roots Derived from Normal
Samples”. In: The Annals of Mathematical Statistics , pp. 475–501.
– (1977). “Tests for a prescribed subspace of principal comp onents”. In: Multivariate Analysis
IV, Ed. PR Krishnaiah, North-Holland: Amsterdam , pp. 73–77.
Johnstone, Iain M, Alexei Onatski, et al. (2020). “Testing i n high-dimensional spiked models”.
In:Annals of Statistics 48.3, pp. 1231–1254.
Johnstone, Iain M. (2001). “On the Distribution of the Large st Eigenvalue in Principal Compo-
nents Analysis”. In: The Annals of Statistics 29.2, pp. 295–327. issn: 00905364. url:http://www.jstor.org/stable/2674106 .
Kato, Tosio (1995). Perturbation Theory for Linear Operators . 2nd. Springer-Verlag Berlin and
Heidelberg.
Koltchinskii, Vladimir, Matthias Löﬄer, Richard Nickl, et al. (2020). “Eﬃcient estimation of
linear functionals of principal components”. In: The Annals of Statistics 48.1, pp. 464–490.
Lee, Ji Oon and Kevin Schnelli (Dec. 2016). “Tracy–Widom dis tribution for the largest eigenvalue
of real sample covariance matrices with general population ”. In: Ann. Appl. Probab. 26.6,
pp. 3786–3839. doi:10.1214/16-AAP1193 .url:https://doi.org/10.1214/16-AAP1193 .
Penna, F., R. Garello, and M. A. Spirito (2009). “Cooperativ e spectrum sensing based on the
limiting eigenvalue ratio distribution in wishart matrice s”. In: IEEE Communications Letters
13.7, pp. 507–509.
13Silin, Igor and Jianqing Fan (2020). “Hypothesis testing fo r eigenspaces of covariance matrix”.
In:arXiv preprint arXiv:2002.09810 .
Stewart, G. W. and Ji-guang Sun (1991). Matrix perturbation theory . Vol. 33. Academic Press,
p. 74.isbn: 9781466507289. doi:10.1016/0378-4754(91)90038-5 .
Sun, Ji-guang (1991). “Perturbation expansions for invari ant subspaces”. In: Linear Algebra And
Its Applications .
Takemura, Akimichi and Yo Sheena (2005). “Distribution of e igenvalues and eigenvectors of
Wishart matrix when the population eigenvalues are inﬁnite ly dispersed and its application to
minimax estimation of covariance matrix”. In: Journal of Multivariate Analysis 94.2, pp. 271–
299.
Tracy, Craig A. and Harold Widom (1994). “Level-spacing dis tributions and the Airy kernel”. In:
Comm. Math. Phys. 159.1, pp. 151–174. url:https://projecteuclid.org:443/euclid.cmp/1104254495 .
Tyler, David E. (1981). “Asymptotic Inference for Eigenvec tors”. In: The Annals of Statistics 9,
pp. 725–736. issn: 0090-5364. doi:10.1214/aos/1176345514 .
– (1983). “A Class of Asymptotic Tests for Principal Compone nt Vectors”. In: The Annals of Sta-
tistics 11.4, pp. 1243–1250. doi:10.1214/aos/1176346337 .url:http://dx.doi.org/10.1214/aos/1176346337 .
Vaart, Aad W. van der (2000). Asymptotic Statistics . Cambridge Series in Statistical and Prob-
abilistic Mathematics. Cambridge University Press.
Wigner, Eugene P (1958). “On the distribution of the roots of certain symmetric matrices”. In:
Annals of Mathematics , pp. 325–327.
Zanella, Alberto, Marco Chiani, and Moe Z Win (2009). “On the marginal distribution of the
eigenvalues of Wishart matrices”. In: IEEE Transactions on Communications 57.4, pp. 1050–
1060.
14