RMES: Real-Time Micro-Expression Spotting
Using Phase From Riesz Pyramid
Yini Fangy,Didan Deng,Liang Wu,Frederic Jumellexy, and Bertram Shi
Hong Kong University of Science and Technology
yYdentity Organization
xBright Nation Limited
fyfangba, ddeng, liang.wu g@connect.ust.hk, f.jumelle@brightnationlimited.com, eebert@ust.hk
Abstract —Micro-expressions (MEs) are involuntary and subtle
facial expressions that are thought to reveal feelings people
are trying to hide. ME spotting detects the temporal intervals
containing MEs in videos. Detecting such quick and subtle
motions from long videos is difﬁcult. Recent works leverage
detailed facial motion representations, such as the optical ﬂow,
and deep learning models, leading to high computational com-
plexity. To reduce computational complexity and achieve real-
time operation, we propose RMES, a real-time ME spotting
framework. We represent motion using phase computed by Riesz
Pyramid, and feed this motion representation into a three-
stream shallow CNN, which predicts the likelihood of each
frame belonging to an ME. In comparison to optical ﬂow, phase
provides more localized motion estimates, which are essential for
ME spotting, resulting in higher performance. Using phase also
reduces the required computation of the ME spotting pipeline
by 77.8%. Despite its relative simplicity and low computational
complexity, our framework achieves state-of-the-art performance
on two public datasets: CAS(ME)2and SAMM Long Videos.
Index Terms —Facial expression recognition, emotion detection,
image pyramid, CNN
I. I NTRODUCTION
Macro-expressions are the commonly encountered facial
expressions like happiness, sadness, surprise or anger. They
typically last from 1/2 to 4 seconds and are often generated
consciously or voluntarily. Micro-expressions (ME) are much
shorter in time (typically 1/25 to 1/2 second) and smaller in
magnitude [1]. They are thought to be largely involuntary,
making them difﬁcult to hide, even when people seek to
suppress them. They are essential non-verbal communication
clues, providing insight into the genuine emotional state of
individuals. They have been widely to various ﬁelds such as
public safety, recruitment, medicine, and neuropsychology [2].
ME spotting is an important ﬁrst step in automated facial
expression recognition (AFER) [3]. An ME has three distinct
stages: onset, apex, and offset. The onset occurs when the
facial muscles start to contract. The apex is the facial action’s
point of peak intensity. The offset marks the moment when
muscles return to their neutral state. ME spotting aims to detect
onset and offset of all MEs in a video sequence.
Because MEs are composed of quick and subtle motions
with low spatial amplitude, the best-performing ME spotting
This work was sponsored in part by the Hong Kong Innovation and
Technology Fund (grant ITS/210/20), Bright Nation Limited and the Hong
Kong Research Grants Council (grant 16214821).algorithms usually extract image motion as optical ﬂow at the
front end, followed by deep neural network models performing
detection at the back end. However, the high computational
complexity of optical ﬂow makes them difﬁcult to run in
real time, especially in resource-constrained environments,
such as remote monitoring for public safety. Thus, researchers
seek to reduce computational complexity while maintaining
high accuracy. Liong et al. [4] proposed to reduce back end
complexity with a three-stream shallow Convolutional neural
network (CNN), while using optical ﬂow as the front end.
Other work has focused on the front end, using phase as an
alternative to the optical ﬂow. Phase is often used as a pre-
processing step for more complex image motion algorithms,
such as motion magniﬁcation and optical ﬂow estimation. It
provides local motion estimates, which more complex optical
ﬂow algorithms reﬁne by integrating information across space,
e.g. using smoothness constraints, to address issues like the
aperture problem.
In this paper, we propose the RMES ( Real-time Micro-
Expression Spotting) framework, which reduces computational
complexity at both ends, combining phase features extracted
by a Riesz Pyramid in the front end with a lightweight three-
stream shallow CNN in the back end. Our approach uses a
shallow CNN structure similar to that of Liong et al. [4],
but with phase instead of optical ﬂow to represent motion
information.
To the best of our knowledge, this paper describes the ﬁrst
comparison of phase versus optical ﬂow for a ME-related task.
With the changes to the network mentioned above and detailed
below, using phase instead of optical ﬂow improves the F1
score by 26.94%, as we demonstrate in Sec. VII. Although
optical ﬂow reduces noise and solves the aperture problem,
it also removes the small variations in local motion that are
critical to detecting MEs. However, to use phase effectively, an
additional facial alignment step is critical. It is also important
to choose the scale from the Riesz pyramid correctly, as there
is a trade-off between motion sensitivity and motion range, as
we describe in Sec. IV.
As an added beneﬁt, phase reduces inference time by 77.8%
compared to optical ﬂow. Despite its simplicity and low
computational complexity, our method achieves the state-of-
the-art F1 scores on both the CAS(ME)2and SAMM Long
Video datasets.arXiv:2305.05523v1  [cs.CV]  9 May 2023II. R ELATED WORK
Image motion information is critical for ME spotting, re-
quiring integration of information over both time and space.
Some approaches extract spatial features ﬁrst, then integrate
them over time. For example, Yang et al. [5] present a
deep learning framework based on facial action units (AU)
detection in each frame, followed by the combination of AU
information across time. Other approaches integrate across
both dimensions simultaneously. For example, Yap et al. [6]
propose an end-to-end 3D-CNN framework that learns features
directly from appearance.
Most recent works in ME spotting use explicit representa-
tions of image motion, such as optical ﬂow. This is then fed
into a classiﬁer, such as a deep CNN model [7]–[10]. This
combination achieves the state-of-the-art results in expression
spotting. However, it is time-consuming, as optical ﬂow ex-
traction and deep CNNs are both computationally complex.
Liong et al. [4] addressed this partially with a shallow three-
stream CNN model to predict the likelihood score for both
macro- and micro-expressions from optical ﬂow. This reduced
the time complexity of the neural network, but not the time
complexity of optical ﬂow.
Optical ﬂow is the most common representation for motion
information. A notable exception is Duque et al.’s work [3]
[11], which we discuss in more detail below. Although optical
ﬂow is intuitively appealing and widely used, we argue that it
is not the best motion representation for ME spotting. Optical
ﬂow algorithms usually integrate information over large spatial
regions, under the assumption that the ﬂow is smooth, i.e.,
values at neighbouring pixels are close to each other. This is
a good assumption for rigid objects, and helps to address the
aperture problem, which arises because local measurements
can only estimate displacements orthogonal to the dominant
texture orientation. However, it is a poor assumption for the
face, which is non-rigid, and for MEs, which are spatially
localized.
Given the small image displacements in MEs, some work
has used motion magniﬁcation as a preprocessing step to
amplify them. For example, Kumar et al. [12] ﬁrst ampliﬁed
motion using Eulerian Motion Magniﬁcation (EMM), then
computed optical ﬂow and input it to a graph attention
network. However, this approach is wasteful, as motion in-
formation is extracted twice: ﬁrst by the EMM algorithm to
magnify the motion and second by the optical ﬂow algorithm
to measure it.
We argue that it is more efﬁcient to use the motion in-
formation extracted during motion magniﬁcation directly. The
most effective algorithms for motion magniﬁcation represent
motion information using phase, which we describe in more
detail in the next section. For example, Wadhwa et al.’s real-
time phase-based motion magniﬁcation algorithm computes
phase using the Riesz Pyramid [13]. Our proposed RMES
algorithm uses the same representation. While phase is noisier
than optical ﬂow due to limited spatial integration, it provides
more localized information, which can capture the non-rigid
Fig. 1. Two representations of a 2-level Riesz Pyramid: Cartesian coordinates
(i.e., monogenic signal) and spherical coordinates.
motion of the face during MEs.
Our approach differs from previous work using phase for
facial emotion analysis. Deng et al.’s MiMaMo net for macro-
expression emotion recognition used phase computed by a
Complex Steerable Pyramid in the frequency domain [14].
We use the Riesz Pyramid, which is computed entirely in the
spatial domain, making it easier to avoid artifacts due to phase
wrap-around. It is also approximately four times faster. Duque
et al. used the phase variance, a simple hand-crafted feature
computed from Riesz Pyramid phase for ME spotting [3] and
classiﬁcation [11]. They spatially averaged the phase over
three pre-deﬁned facial regions (the two eyes and the mouth) to
estimate the net motion in each region, then looked for peaks
in the squared difference between the instantaneous phase and
the mean phase over the entire video. This approach ignores
spatially localized motion, e.g. in different parts of the eye
or mouth, that is critical to ME spotting. It also ignores tem-
porally localized motion, looking only at absolute changes in
position versus the mean. In contrast, our approach preserves
spatially and temporally localized motion information about
motion. Rather than averaging, we maintain phase estimates at
each pixel. Rather than measuring instantaneous versus global
mean phase, we look at inter-frame phase differences.
III. P HASE DIFFERENCES FROM THE RIESZ PYRAMID
Fig. 1 illustrates a two-level Riesz Pyramid constructed from
an input face image.
We ﬁrst build a Laplacian pyramid for each image in
the input video. Images at coarser levels are obtained from
previous levels by Gaussian blurring and downsampling. We
then calculate differences between the images at subsequent
levels (subbands).
Next, we take the approximate Riesz transform of each
subband in the Laplacian pyramid. The Riesz transform is a
steerable Hilbert transformer. We compute a quadrature pair
of ﬁlters that are 90 degrees out of phase with respect to each
other along the dominant orientation at every pixel [13]. The
ﬁlters have transfer functions:  i!x
k~ !kand i!y
k~ !k.
Applying this ﬁlter pair to the image at frame m,Im, we
obtain a pair of ﬁlter responses, ( R1m;R2m). The input Iand
the ﬁlter responses form a triple called the monogenic signal
(Im;R1m;R2m), which we combine into a quaternion rm,Fig. 2. The Real-time Micro-Expression Spotting (RMES) framework.
which can also be expressed in terms of the local amplitude
Am, local orientation mand local phase m:
rm=Im+iR1m+jR2m (1)
=Amcosm+iAmsinmcosm
+jAmsinmsinm (2)
The solution of Eq. 2 is not unique, since both
(Am;m;m) and (Am; m;m+) map to the same
monogenic signal. Therefore, we use the quaternionic phase
(mcosm;msinm), which is invariant to the ambiguity
between (m;m) and ( m;m+).
Following [15] and [16], we compute a sequence of quater-
nionic phase differences between adjacent frames from the
sequence of unit quaternions ^ rm=rm=krmkaccording to
log(^ rm^ r 1
m 1)imcosm+jmsinm (3)
where the phase difference m=m m 1is a measure of
the inter-frame motion in the direction of the local orientation
m, which is measured with respect to the horizontal image
axis. The approximation holds if the local orientation is
roughly constant in time.
IV. RMES F RAMEWORK
Fig. 2 shows the structure of our proposed RMES frame-
work. Inspired by [4], the structure consists of three stages:
preprocessing, shallow CNN, and postprocessing.
In the preprocessing step, we apply face alignment and the
Riesz Pyramid to obtain a sequence of quaternionic phase
differences as described in the previous section.
We then accumulate these phase differences over Kframes
to measure the motion between images spaced Kframes apart.
We chooseKto be half the average length of the MEs in the
dataset. If the apex typically occurs about halfway between
the onset and offset, then Kis the average number of frames
from onset to apex and from apex to offset. Intuitively, themagnitude of facial movement should be largest for these
intervals.
In the shallow CNN stage, we crop signals from Regions of
Interest (RoI) relevant to MEs and input them into the shallow
CNN, which outputs a score representing the likelihood the
frame belongs to an ME. This results in a sequence of (T K)
scores, where Tis the video length.
In postprocessing, we smooth the (T K)output scores and
detect peaks above the predeﬁned threshold as apex frames,
fangN
n=1, whereNis the number of detected peaks. The ﬁnal
spotted intervals are f[an K;a n+K]gN
n=1.
We describe these steps in more detail below.
Face alignment . Face alignment is an essential preprocess-
ing step to compensate for head motion. Prior approaches
accounted for head motion by subtracting the average motion
across the image or a region, such as the noise. While this
can compensate for translation, it cannot compensate for other
effects, such as tilt and rotation. Given the sensitivity of phase,
our experimental results suggest that accurate face alignment
is critical to ensuring that the Riesz Pyramid extracts motion
relevant to ME. We do face alignment using the OpenFace
Toolkit [17], which detects 68 facial landmarks and uses them
to align the face by linear warping followed by cropping of the
face region. We resize cropped and aligned images to 224224
pixel resolution.
Riesz Pyramid . We temporally ﬁlter the sequence of quater-
nionic phase differences in Eq. 3 to isolate motions of interest
and remove noise. Since we seek to detect subtle motions,
we use a non-causal ﬁnite impulse response (FIR) ﬁlter with
no group delay. Unlike EMM, which focuses on amplifying
periodic motions, ME motions are non-periodic. Rather than
using the temporal bandpass ﬁlter in EMM, which can result
in large oscillations near the onset and offset of the ME due to
the Gibbs phenomenon, we use a lowpass ﬁlter, which ﬁlters
out high frequency noise.
We accumulate the ﬁltered phase difference sequence to
measure the motion between frames spaced Kframes apart.
If, with a slight abuse of notation, we use imcosm+
jmsinmto denote the ﬁltered sequence, the accumulated
sequence is given by imcos  m+jmsin  mwhere
mcos  m=K 1X
k=0m kcosm k (4)
msin  m=K 1X
k=0m ksinm k (5)
We also compute the phase difference magnitude
jmj=q
( msin  m)2+ ( mcos  m)2:(6)
The accumulation of the quaternionic phase differences in
Eqs. 4 and 5 avoids phase overlap that might be encountered
due to the relatively large displacements over Kframes, but
still assumes that the phase difference between successive
frames is always in the interval ( ;][16]. This assumption
is often valid for ME spotting, since small motions resultin small phase differences. However, the size of the phase
shift also depends upon the dominant spatial frequency, which
varies with the level of the Laplacian pyramid. For the same
motion, larger spatial frequencies (ﬁner scales) result in the
larger phase shifts. This increases sensitivity, but also increases
the chance of phase wrap-around. Thus, the choice of pyramid
level is a trade-off between sensitivity and motion range.
RoI cropping . We further crop the images based on the
Regions of Interest (RoIs) where MEs might occur according
to Facial Action Coding System (FACS), i.e., the eyebrows
and the mouth as illustrated in Fig. 3 [18]. We then resample
each region to 1530pixels and stack them, resulting in
an input feature map with 3030pixels and three channels
(mcos  m,msin  mandjmj). We normalize the
feature maps using the Z score.
Shallow CNN . The shallow CNN has three streams: one
for each channel. Each stream consists of a CNN layer (ﬁlter
size33and stride size 11), followed by max pooling
layer (kernel size 66and stride size 66). Following [4],
the number of ﬁlters varies according to stream depending
upon the importance of the features (3, 5, and 8 ﬁlters,
respectively). For example, mcos  mandmsin  m
measure horizontal and vertical displacements, respectively.
Since MEs typically involve more vertical than horizontal
movements, we allocate more ﬁlters to the  sin  mstream.
We concatenate and ﬂatten the feature maps, resulting in a
5516 = 400 dimensional feature vector. This is fed
into two fully connected layers, the ﬁrst one (400, 400) to
summarize the features, and the second one (400, 1) to output
the ﬁnal score s, which indicates the likelihood that the frame
is part of an ME interval.
We assign binary ground truth scores Sito each frame
iaccording to the Intersection Over Union (IoU) between
the interval between two frames used to compute the phase
difference at frame iand the nearest ME interval in the ground
truth, i.e.,Si= 1 if IoU0:5;elseSi= 0.
We train the model with a Mean Square Error (MSE) loss
function:L=1
NPN
i=1(si Si)2;whereNis the total
number of image pairs in the dataset.
Postprocessing . For each video, we smooth the scores by
averaging across the 2K+ 1frames:
^si=1
2K+ 1i+KX
j=i Ksj;fori=K+ 1;:::;T K; (7)
Then, we perform peak detection to ﬁnd local maxima ex-
ceeding a predeﬁned threshold Hand such that the horizontal
distance between adjacent peaks is no less than k. We set
H= ^smean+h(^smax ^smean); (8)
where ^smean and^smaxare the mean and maximum scores over
the entire video. h2[0;1]is a hyperparameter controlling the
threshold. For each detected peak, we obtain a spotted interval
[P K;P +K], wherePis the peak location.TABLE I
SUMMARY OF DATASET PROPERTIES
#Vid. #Sub. #Samp. FPS Resolution
CAS(ME)298 22 57 30 640480
SAMMLV 147 32 159 200 20401088
V. M ETHODOLOGY
Datasets . We evaluated our model on two public datasets:
CAS(ME)2[19] and SAMM Long Videos [20]–[22]. Table I
summarizes the properties of these datasets.
Metrics . We compare model performance using the F1
score proposed in [23]. Let Xibe the ground truth number
of micro-expressions in video i,Yibe the number of spotted
intervals, and aibe the number of true positives. A spotted
interval is considered to be a true positive if its IoU with a
ground truth interval exceeds 0.5. The F1 score is deﬁned as
F1=2PR
P+R
where the precision Pand recallRare given by
R=PV
i=1aiPV
i=1XiandP=PV
i=1aiPV
i=1Yi
Implementation We implemented our framework using
PyTorch 1.11.0, and trained on an NVIDIA A40-16Q GPU.
We apply Leave-one-subject-out (LOSO) cross-validation in
order to ensure all samples are evaluated. We used the third
level of the Riesz Pyramid. The cutoff frequency of the
lowpass ﬁlter was 10Hz, corresponding to a time constant of
100ms. The value of Kwas 6 and 47 for CAS(ME)2and
SAMM Long Videos, respectively, half the average length of
MEs in the two datasets (200ms and 235ms). The value of
hused for peak detection was 0.7. For our experiments with
optical ﬂow, we used the Python library OpenCV 4.5.2 running
on the same device as phase calculation.
VI. E XPERIMENTAL RESULTS
Table II compares the F1 scores of our RMES framework
with other SOTA systems. Duque et al. [3] perform peak
detection on phase variance computed from the Riesz Pyramid.
Yang et al. [5] and Yap et al. [6] use end-to-end deep learning
frameworks with appearance-based representation. The others
[4][7]-[10] cascade an optical ﬂow front-end with a CNN-
based back-end.
Our RMES framework outperforms all other SOTA systems
listed. To see the effect of phase versus optical ﬂow, we
compare our results with those of Liong et al. [4], who use an
optical ﬂow front end followed by a shallow CNN similar to
that used in our model. Our model with phase improves the F1
score by 26.94% and 9.67% for CAS(ME)2and SAMM Long
Videos. This supports our claim that phase features are better
representations of facial motions for ME spotting than optical
ﬂow. The F1 scores listed do not tend to increase over time
because most systems, except [10], focused on optimizing anTABLE II
F1SCORES OF BENCHMARKS AND OUR MODEL
Methods CAS(ME)2SAMM LV
Duque et al., 2018 [3] 0.0806 0.0711
Liong et al., 2021 [4] 0.1173 0.1520
Wang et al., 2021 [7] 0.0360 0.0880
Yu et al., 2021 [8] 0.0420 0.1310
Yang et al., 2021 [5] 0.0153 0.1155
Liong et al., 2022 [9] 0.0808 0.0878
Yap et al., 2022 [6] 0.0714 0.0466
Liong et al., 2023 [10] 0.1214 0.0949
Ours 0.1489 0.1667
TABLE III
TIME COMPLEXITY COMPARISON (UNIT :SECOND )
Methods Preprocessing Inference Overall
Liong et al., 2021 [4] 0.12 0.0020 0.122
Ours 0.0266 0.0019 0.0285
overall F1 score evaluating both macro- and micro-expression
spotting, rather than micro-expression spotting alone.
Table III compares the time complexity of Liong et al. [4]
and RMES. The inference time of these two methods only
takes 1.6% and 6.7% of the overall time. Thus, focusing on
improving the preprocessing step is vital to improving speed,
as it takes most of the time. Even including the extra face
alignment step, which accounts for the bulk (0.018 seconds
or 68%) of the preprocessing time, the preprocessing time of
RMES is only 22% of [4], corresponding to more than four
times speedup.
Table IV lists the number of parameters and FLOPs for
all models where that information was provided. Our model
requires the fewest FLOPs. Compared to Liong et al. [4], our
model also reduces the computational complexity at the CNN
back end, because we use smaller receptive ﬁelds ( 33vs.
55) and a smaller input dimension ( 3030vs.4242).
Fig. 3 compares the phase differences and optical ﬂow
for an aligned image sequence pair where the subject raises
the right corner of her mouth. Due to the spatial integration
in optical ﬂow, its vector ﬁeld is smoother and cleaner, but
ignores the local details that phase preserves. For example, the
non-rigid deformation of the mouth is reﬂected in the varying
directions of the phase difference vectors, but the optical ﬂow
vectors all point in the same direction (upwards). These subtle
changes in the face are vital for ME spotting and should not
be smoothed out. This example supports our intuition that
phase provides more robust and richer feature representation
for micro-expression.
VII. A BLATION STUDIES
We conducted ablation studies to show the effect of using
face alignment, taking input from different levels of the Reisz
pyramid, the choice of temporal ﬁlter in the pyramid, and
cropping RoIs.
Face alignment . Face alignment (FA) reduces the effect
of global translations, rotations and scaling, which are lessTABLE IV
MODEL SIZE AND LATENCY COMPARISON
Methods # Param # FLOPs
Liong et al., 2021 [4] 315k, 2.1M
Yu et al., 2021 [8] 23M 810M
Liong et al., 2022 [9] 67k 1.4M
Liong et al., 2023 [10] 161k 2.7M
Ours 161k 0.6M
Fig. 3. Comparison of phase differences and the optical ﬂow. Red rectangles
show the RoI areas. (Left) An aligned image sequence pair. (Middle) Quiver
plot of phase differences overlaid upon a red-green anaglyph of the image
pair. (Right) Similar plot for optical ﬂow.
relevant in ME spotting. Table V compares the effect of
using/not using FA for both datasets. Using FA signiﬁcantly
reduces false positive detections, improving precision and
therefore the F1 score by 21.7% and 42.7% on CAS(ME)2
and SAMMLV , respectively. Without face alignment, global
motion can introduce large phase differences, which the model
classiﬁes mistakenly as a ME.
Riesz Pyramid Level . Different levels in the Riesz Pyramid
correspond to different frequency ranges or scales. Earlier
levels correspond to ﬁner scales and higher resolution. They
contain more detailed information, but cannot represent large
displacements, tend to be noisier and are more prone to phase
wrap-around. The left plot in Fig. 4 shows the F1 score
when using phase from different levels as input. The F1 score
initially increases as we move up the pyramid, as displacement
range increases and the noise decreases, but degrades after the
third level due to the low resolution ( 1313) at level 4.
Temporal Filter . The middle plot in Fig. 4 compares the
effect of ﬁltering the unwrapped phase differences with a
lowpass ﬁlter with 10Hz cutoff frequency and a band-pass
TABLE V
THE EFFECT OF FACE ALIGNMENT (FA).
(TP = T RUE POSITIVE , FP = F ALSE POSITIVE , FN = F ALSE NEGATIVE )
FACAS(ME)2
TP FP FN Recall Precision F1
True 14 117 43 0.2456 0.1069 0.1489
False 14 158 43 0.2456 0.0814 0.1223
FASAMM Long Videos
TP FP FN Recall Precision F1
True 30 171 129 0.1887 0.1493 0.1667
False 31 341 128 0.195 0.0833 0.1168Fig. 4. Ablation studies on the effect of different pyramid levels, temporal
ﬁlters, and use of RoIs.
ﬁlter between 2Hz and 10Hz. Eliminating lower frequencies
degrades performance.
Region of Interest . We compare the F1 score when using
input only from eye and mouth RoIs and when using input
from the full image. The right plot in Fig. 4 shows that
restricting input to RoIs results in better F1 scores. We
designed the RoI based on Facial Action Coding System,
which is used to describe MEs. This focuses the network on
information relevant to MEs and avoids irrelevant information,
such as from the jawline and areas below it.
VIII. C ONCLUSION
In this paper, we addressed the high time complexity of
ME spotting algorithms by proposing the Real-time Micro-
Expression Spotting (RMES) framework, which uses phase
extracted from Riesz Pyramid followed by a three-stream shal-
low CNN. In our evaluations on CAS(ME)2and SAMM Long
Videos, RMES achieves the state-of-the-art performance with
signiﬁcantly lower computational complexity. Our comparison
of phase differences and optical ﬂow suggests that phase
differences are better suited for ME spotting, as it localized
motion details. Moving forward, possible extensions of this
work include replacing the peak detection postprocessing step
with frame by frame labels, which will enable the framework
to detect ME intervals with varying length. We also plan to
explore approaches other than Gaussian smoothing for denois-
ing, such as an attention mechanism that could dynamically
assign small weights to noisy regions.
REFERENCES
[1] Wen-Jing Yan, Qi Wu, Jing Liang, Yu-Hsin Chen, and Xiaolan Fu,
“How fast are the leaked facial expressions: The duration of micro-
expressions,” Journal of Nonverbal Behavior , vol. 37, no. 4, pp. 217–
230, 2013.
[2] Xiaobai Li, Xiaopeng Hong, Antti Moilanen, Xiaohua Huang, Tomas
Pﬁster, Guoying Zhao, and Matti Pietik ¨ainen, “Towards reading hid-
den emotions: A comparative study of spontaneous micro-expression
spotting and recognition methods,” IEEE Transactions on Affective
Computing , vol. 9, no. 4, pp. 563–577, 2017.
[3] Carlos Arango Duque, Olivier Alata, Remi Emonet, Anne-Claire
Legrand, and Hubert Konik, “Micro-expression spotting using the Riesz
pyramid,” in 2018 IEEE Winter Conference on Applications of Computer
Vision (WACV) . IEEE, 2018, pp. 66–74.
[4] Gen-Bing Liong, John See, and Lai-Kuan Wong, “Shallow optical ﬂow
three-stream CNN for macro-and micro-expression spotting from long
videos,” in 2021 IEEE International Conference on Image Processing
(ICIP) . IEEE, 2021, pp. 2643–2647.
[5] Bo Yang, Jianming Wu, Zhiguang Zhou, Megumi Komiya, Koki Kishi-
moto, Jianfeng Xu, Keisuke Nonaka, Toshiharu Horiuchi, Satoshi Ko-
morita, Gen Hattori, et al., “Facial action unit-based deep learning
framework for spotting macro-and micro-expressions in long video
sequences,” in Proceedings of the 29th ACM International Conference
on Multimedia , 2021, pp. 4794–4798.[6] Chuin Hong Yap, Moi Hoon Yap, Adrian Davison, Connah Kendrick,
Jingting Li, Su-Jing Wang, and Ryan Cunningham, “3D-CNN for facial
micro-and macro-expression spotting on long video sequences using
temporal oriented reference frame,” in Proceedings of the 30th ACM
International Conference on Multimedia , 2022, pp. 7016–7020.
[7] Su-Jing Wang, Ying He, Jingting Li, and Xiaolan Fu, “MESNet: A
convolutional neural network for spotting multi-scale micro-expression
intervals in long videos,” IEEE Transactions on Image Processing , vol.
30, pp. 3956–3969, 2021.
[8] Wang-Wang Yu, Jingwen Jiang, and Yong-Jie Li, “LSSNet: A two-
stream convolutional neural network for spotting macro-and micro-
expression in long videos,” in Proceedings of the 29th ACM Inter-
national Conference on Multimedia , 2021, pp. 4745–4749.
[9] Gen Bing Liong, Sze-Teng Liong, John See, and Chee-Seng Chan,
“MTSN: A multi-temporal stream network for spotting facial macro-
and micro-expression with hard and soft pseudo-labels,” in Proceedings
of the 2nd Workshop on Facial Micro-Expression: Advanced Techniques
for Multi-Modal Facial Expression Analysis , 2022, pp. 3–10.
[10] Gen-Bing Liong, John See, and Chee-Seng Chan, “Spot-then-Recognize:
A micro-expression analysis network for seamless evaluation of long
videos,” Signal Processing: Image Communication , vol. 110, pp.
116875, 2023.
[11] Carlos Arango Duque, Olivier Alata, R ´emi Emonet, Hubert Konik,
and Anne-Claire Legrand, “Mean oriented Riesz features for micro
expression classiﬁcation,” Pattern Recognition Letters , vol. 135, pp.
382–389, 2020.
[12] Ankith Jain Rakesh Kumar and Bir Bhanu, “Three stream graph
attention network using dynamic patch selection for the classiﬁcation
of micro-expressions,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022, pp. 2476–2485.
[13] Neal Wadhwa, Michael Rubinstein, Fr ´edo Durand, and William T
Freeman, “Riesz pyramids for fast phase-based video magniﬁcation,”
in2014 IEEE International Conference on Computational Photography
(ICCP) . IEEE, 2014, pp. 1–10.
[14] Didan Deng, Zhaokang Chen, Yuqian Zhou, and Bertram Shi, “MI-
MAMO Net: Integrating micro-and macro-motion for video emotion
recognition,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , 2020, vol. 34, pp. 2621–2628.
[15] Jehee Lee and Sung Yong Shin, “General construction of time-domain
ﬁlters for orientation data,” IEEE Transactions on Visualization and
Computer Graphics , vol. 8, no. 2, pp. 119–128, 2002.
[16] Neal Wadhwa, Michael Rubinstein, Fr ´edo Durand, and William T.
Freeman, “Quaternionic representation of the Riesz pyramid for video
magniﬁcation,” Tech. Rep. 2014-009, MIT CSAIL, 2014.
[17] Tadas Baltrusaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe
Morency, “Openface 2.0: Facial behavior analysis toolkit,” in 2018
13th IEEE International Conference on Automatic Face & Gesture
Recognition (FG 2018) . IEEE, 2018, pp. 59–66.
[18] Li-Wei Zhang, Jingting Li, Su-Jing Wang, Xian-Hua Duan, Wen-Jing
Yan, Hai-Yong Xie, and Shu-Cheng Huang, “Spatio-temporal fusion for
macro-and micro-expression spotting in long video sequences,” in 2020
15th IEEE International Conference on Automatic Face and Gesture
Recognition (FG 2020) . IEEE, 2020, pp. 734–741.
[19] Fangbing Qu, Su-Jing Wang, Wen-Jing Yan, and Xiaolan Fu, “CAS
(ME) 2: A database of spontaneous macro-expressions and micro-
expressions,” in International Conference on Human-Computer Inter-
action . Springer, 2016, pp. 48–59.
[20] Adrian K Davison, Cliff Lansley, Nicholas Costen, Kevin Tan, and
Moi Hoon Yap, “Samm: A spontaneous micro-facial movement dataset,”
IEEE transactions on affective computing , vol. 9, no. 1, pp. 116–129,
2016.
[21] Adrian K Davison, Walied Merghani, and Moi Hoon Yap, “Objective
classes for micro-facial expression recognition,” Journal of imaging ,
vol. 4, no. 10, pp. 119, 2018.
[22] Chuin Hong Yap, Connah Kendrick, and Moi Hoon Yap, “SAMM
long videos: A spontaneous facial micro-and macro-expressions dataset,”
in2020 15th IEEE International Conference on Automatic Face and
Gesture Recognition (FG 2020) . IEEE, 2020, pp. 771–776.
[23] Jingting Li, Catherine Soladie, Renaud S ´eguier, Su-Jing Wang, and
Moi Hoon Yap, “Spotting micro-expressions on long videos sequences,”
in2019 14th IEEE International Conference on Automatic Face &
Gesture Recognition (FG 2019) . IEEE, 2019, pp. 1–5.