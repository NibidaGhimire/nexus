This draft was prepared using the LaTeX style file belonging to the Journal of Fluid Mechanics 1
Learn to Flap: Foil Non-parametric Path Planning
via Deep Reinforcement Learning
Z.P. Wang1,2,*, R.J. Lin3,4,*, Z.Y. Zhao3,4, P.M. Guo1,+, N. Yang3,4,+and D.X. Fan1â€ 
1School of Engineering, Westlake University, Hangzhou 310024, Zhejiang, China.
2School of Mechanical and Material Engineering, Queenâ€™s University, Kingston, Ontario K7L 3N6,
Canada.
3University of Chinese Academy of Sciences, Beijing 100049, China
4Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China
(Received xx; revised xx; accepted xx)
To optimize flapping foil performance, the application of deep reinforcement learning (DRL)
on controlling foil non-parametric motion is conducted in the present study. Traditional control
techniques and simplified motions cannot fully model nonlinear, unsteady and high-dimensional
foil-vortex interactions. A DRL-training framework based on Proximal Policy Optimization
and Transformer architecture is proposed. The policy is initialized from the sinusoidal expert
display. We first demonstrate the effectiveness of the proposed DRL-training framework which
can optimize foil motion while enhancing foil generated thrust. By adjusting reward setting and
action threshold, the DRL-optimized foil trajectories can gain further enhancement compared to
sinusoidal motion. Via flow analysis of wake morphology and instantaneous pressure distribu-
tions, it is found that the DRL-optimized foil can adaptively adjust the phases between motion
and shedding vortices to improve hydrodynamic performance. Our results give a hint for solving
complex fluid manipulation problems through DRL method.
Key words: flow control, reinforcement learning, flapping foil
1. Introduction
Designing novel bio-inspired flapping propulsors has been of great interest to the scientific
community, as â€œabout 109years of animal evolution ... have inevitably produced rather refined
means of generating fast movement at low energy costâ€ (Lighthill 1969). For simplification,
researchers mainly use the model of sinusoidally flapping foils whose trajectories can be easily
parameterized (Wu et al. 2020). For example, studies show that the flapping efficiency is strongly
associated with the Strouhal number ğ‘†ğ‘Ÿ, manifested in its narrow range for the optimal propulsive
strategy of aquatic animals across different sizes (Qi et al. 2022). However, recent observations
reveal animals travel in non-sinusoidal motions, such as the Burst-and-Coast pattern of fish (Li
et al. 2021), to manipulate the flow and achieve higher efficiency (Chin & Lentink 2016) and
better maneuverability (Triantafyllou et al. 2016).
As the flapping motion becomes more complex, its trajectory cannot be easily parameterized.
Preliminary studies (Teng et al. 2016; Liu et al. 2020; Ashraf et al. 2021) sampled several
non-sinusoidal flapping trajectories and showed a hydrodynamic enhancement that cannot be
* These authors contribute equally.
â€ Email address for correspondence: fandixia@westlake.edu.cnarXiv:2305.12687v2  [physics.flu-dyn]  25 May 20232 A. N. Other, H.-C. Smith and J. Q. Public
ignored. In fact, even a tiny change in the instantaneous angle of attack may result in significant
force alternation, attributed to the complex foil-vortex interactions (Izraelevitz & Triantafyllou
2014). Therefore, the underlying flow mechanism of foil non-parametric flapping has yet to
be fully explored and remains unclear. As a typical nonlinear, unsteady, and high-dimensional
flow control problem, optimizing such non-parametric flapping trajectories poses a considerable
challenge for traditional control techniques that may result in inefficient and intractable solutions.
Deep Reinforcement Learning (DRL) recently has gained significant attention in fluid mechan-
ics for its astonishing achievement in solving complex problems of games (Silver et al. 2017),
robotics (Won et al. 2020) and other industrial tasks (Degrave et al. 2022). Several successes
include reducing bluff body drag (Rabault et al. 2019; Fan et al. 2020) and enhancing airfoil
lift (Wang et al. 2022), by learning statistically mean control actions to induce a switch to a
favourable wake pattern. However, as mentioned above, optimizing foil flapping motions requires
learning a coherently cyclical motion, meticulously manipulating the strength and timing of the
shedding vortices, and their interaction with the moving foil (Muhammad et al. 2022), which a
simple transfer of the readily available DRL algorithm may not work.
The present study proposes a DRL training framework based on Proximal Policy Optimization
(PPO) algorithm and Transformer architecture. In addition, the policy is initialized from the
expert display instead of randomization. We compare with other DRL frameworks to demonstrate
whether the proposed agent can learn to flap. By carefully adjusting the reward optimization
weights and motion thresholds, we illustrate considerable improvements in flapping hydrodynamic
performance with learned non-parametric flapping trajectories. At last, by flow visualization, we
shed light on why the agent may flap better compared to the statistically equivalent sinusoidal
motion.
2. Materials and Methods
2.1. Physical model
We numerically study a 2-dimensional NACA0016 foil flapping in the uniform inflow at
ğ‘…ğ‘’=ğ‘ˆâˆğ‘
ğœˆ= 1173, where ğ‘ˆâˆis the uniform free stream velocity, ğ‘is the foil chord length
andğœˆis the fluid kinematic viscosity. The trajectory of the flapping foil is first prescribed as a
sinusoidal motion combined with both heave â„ğ‘ (ğ‘¡)and pitchğœƒğ‘ (ğ‘¡)aroundğ‘/4, and is later learned
intelligently by DRL agents. The prescribed sinusoidal motion can be parameterized as follows,
â„ğ‘ (ğ‘¡)=â„0sin(2ğœ‹ğ‘“ğ‘¡), ğœƒğ‘ (ğ‘¡)=ğœƒ0sin(2ğœ‹ğ‘“ğ‘¡+ğœ™), (2.1)
where the frequency ğ‘“is set to be the same for both pitch and heave with amplitudes of ğœƒ0
andâ„0, respectively. And ğœ™denotes the phase difference between the two motions. Therefore,
the non-dimensional parameters of Strouhal number ğ‘†ğ‘Ÿand scaled amplitude factor ğ´ğ·can be
defined as follows,
ğ‘†ğ‘Ÿ=ğ‘“ğ·
ğ‘ˆâˆ, ğ´ğ·=2ğ´
ğ·, (2.2)
whereğ·is the foil thickness, and ğ´is the foil peak-to-peak trailing edge (TE) amplitude. And we
quantify the flapping performance via the mean thrust coefficient Â¯ğ¶ğ‘‡and efficiency coefficient ğœ‚
as follows,
Â¯ğ¶ğ‘‡=1
Tâˆ«T
0ğ¶ğ‘‡ğ‘‘ğ‘¡, ğœ‚=Â¯ğ¶ğ‘‡âˆ«T
0ğ¶Pğ‘‘ğ‘¡=Â¯ğ¶ğ‘‡âˆ«T
01
ğ‘ˆâˆ(ğ¶ğ¿Â¤â„+ğ¶ğ‘€Â¤ğœƒ)ğ‘‘ğ‘¡, (2.3)
whereğ¶ğ‘‡=2ğ¹ğ‘¥
ğœŒğ‘ˆ2âˆğ‘,ğ¶ğ¿=2ğ¹ğ‘¦
ğœŒğ‘ˆ2âˆğ‘andğ¶ğ‘€=2ğ‘€
ğœŒğ‘ˆ2âˆğ‘2are the instantaneous thrust, lift and moment
coefficients. ğ¹ğ‘¥andğ¹ğ‘¦are the fluid forces opposite and perpendicular to the inflow direction.Guidelines for authors 3
ğ‘ğ‘âˆ—Ì‚â„’
CFD Calculation
X
YOnline Learning
ï¿½ğ‘‰ğ‘‰ğœƒğœƒ
ï¿½ğ‘‰ğ‘‰y
V probsParallel Environment
XML -RPC
Ì‡â„ğ‘ ğ‘ Ì‡ğœƒğœƒğ‘ ğ‘ X
YExpert Data
Pretrained 
Policy NetImitation Learning
ğ‘Ÿğ‘Ÿ:ğ›½ğ›½ğ¶ğ¶ğ¶ğ¶ğ‘‡ğ‘‡âˆ’ğ›½ğ›½ğ‘ƒğ‘ƒğ¶ğ¶ğ’«ğ’«PPO 
AlgorithmTransformer 
Implemented 
Policy Net
Transformer 
Implemented 
Critic NetAgent
ğ‘œğ‘œ:(ğœƒğœƒ,ğ‘¦ğ‘¦,ğ¶ğ¶ğ‘ƒğ‘ƒğ‘–ğ‘–,ï¿½ğ‘‰ğ‘‰ğœƒğœƒ,ï¿½ğ‘‰ğ‘‰y)
A
ğœƒğœƒğ‘¦ğ‘¦
Figure 1. Sketch of RL framework and data flow. The arrows indicate the control sequence: the policy
net is first pre-trained by the expert data (selected sinusoidal motion). The agent inquires about the states
via observation ğ‘œfrom ten parallel environments, then sends actions ğ‘adjusted by Ë†L. We implement
XML-RPC protocol to enable cross-platform communication between the environment (CFD solver) and
RL agent (python).
ğ‘€is the fluid moment around the pitching point. Â¤â„andÂ¤ğœƒare the heaving and angular velocity
respectively. ğœŒis fluid density, and Tis the flapping period.
For the DRL learning cases, the instantaneous heaving and pitching velocities Ë†ğ‘‰ğ‘¦and Ë†ğ‘‰ğœƒare
used, and hence the instantaneous heaving position and pitching angle are cumulative values from
the start of training till the current time, which can be described by:
ğ‘¦ğ‘™(ğ‘¡)=ğ‘¡âˆ‘ï¸
ğ‘–=0Ë†ğ‘‰ğ‘–
ğ‘¦Â·ğ›¿ğ‘¡, ğœƒğ‘™(ğ‘¡)=ğ‘¡âˆ‘ï¸
ğ‘–=0Ë†ğ‘‰ğ‘–
ğœƒÂ·ğ›¿ğ‘¡, (2.4)
whereğ‘–denotes the current step for simulation. The multiplier ğ›¿ğ‘¡is one time step in simulation.
2.2. Numerical Method
The CFD solver based on the boundary data immersion method (BDIM)(Weymouth & Yue
2011) was chosen in the current work for its capability to simulate complex geometries undergoing
rapid motion with large amplitudes (Schlanderer et al. 2017). The solver has been extensively
validated with various experimental data (Maertens & Weymouth 2015). In detail, the simulation
is set the resolution of 32 grids per foil chord and a domain size of 16 ğ‘Ã—12ğ‘. The calculation
time step is set to adapt dynamically to the complexity of the calculation. The calculation speed
and efficient data communication make current BDIM solver a potential DRL environment.
It is noted that both the mesh density, domain size, Reynolds number and resolution could
easily be increased in the future study, but are kept low here as it allows for fast training which
is the primary aim to demonstrate our proof-of-concept of the DRL learning process for flapping
foil.
2.3. Reinforcement Learning Framework and Algorithm
Key Challenges. Integrating DRL into foil non-parametric path planning entails overcoming
domain-specific obstacles, such as partial observation limitations, vast exploration spaces,
and multi-objective preferences. To address these challenges, we employ a Proximal Policy
Optimization (PPO)-based algorithm (Schulman et al. 2017) combined with a transformer
architecture (Vaswani et al. 2017). Since limited environmental data (e.g., force, pressure on4 A. N. Other, H.-C. Smith and J. Q. Public
the foil) can be obtained in real-world situations, we formulate this issue as a Partially Observable
Markov Decision Process (POMDP), effectively addressed through our Transformer architecture
implementation. In the following, we emphasize the primary elements of our DRL framework.
More detailed descriptions of PPO, POMDP and Transformer will be given in Appendix A.
PPO algorithm. PPO has several advantages, such as facilitating support for large-scale
parallel training, essential for long-episodic and high-dimensional tasks (Berner et al. 2019). In
addition, PPO is robust as an efficient approximation of the trust region optimization approach,
which promises reliable policy improvement in noisy environments. Furthermore, the use of
Generalized Advantage Estimation (GAE) assists in long-term credit assignment, improving the
algorithmâ€™s performance and data efficiency.
Transformer. In order to effectively model the time-dependent behaviour of foil flapping,
we employ the Transformer architecture, which has been demonstrated to excel in capturing
long-term interactions and supporting high training throughput (Brown et al. 2020; Esslinger
et al. 2022). Central to the Transformer is its attention mechanism, which uncovers intricate
inter-dependencies within input sequences through a scaled dot-product function.
Pretraining on Expert Demonstration. The foil actionâ€™s cause-and-effect relationship is non-
instantaneous and the complete motion pattern consists of thousands of steps. This vast policy
space enables the potential discovery of superior foil motion control strategies. However, the
exponentially-expanding exploration space as a function of the simulation length poses challenges
to the learning performance. To address the substantial exploration space, we use a pre-trained
model, imitated from sinusoidal expert policies, as an initial starting point in the high-value
subspace of the overall policy space.
Diverse Reward Functions. Foil motion optimization objectives are efficiency coefficient and
thrust coefficient thus designing diverse reward functions to boost the diversity of motion patterns
is essential. The reward function applied in the present work is
ğ‘Ÿğ‘¡=ğ›½ğ¶clip(ğ¶ğ‘¡
ğ‘‡,âˆ’ğ¶ğ‘š
ğ‘‡,ğ¶ğ‘š
ğ‘‡)âˆ’ğ›½Pclip(ğ¶ğ‘¡
P,âˆ’ğ¶ğ‘š
P,ğ¶ğ‘š
P) (2.5)
to balance the maximization of thrust and energy consumption of foil motion. The clip parameters
ğ¶ğ‘š
ğ‘‡,ğ¶ğ‘š
Palleviate extreme value of ğ¶ğ‘‡andğ¶P, and linear weight ğ›½ğ¶,ğ›½ğ‘ƒbalance the importance
between thrust and energy consumption. The tuple (ğ›½ğ¶,ğ›½P,ğ¶ğ‘š
ğ‘‡,ğ¶ğ‘š
P)describes a specific reward
function, which is initialized as (0.1,1
3000,10,3000)respectively.
Training Pipeline. To enhance data interaction throughput, we employ ten parallel simulations,
serving as the environment to interact with the transformer agent to minimize training time.
Communication sequences and data flow are shown in figure 1. Agents are initialized from a fully
developed simulation in which the foil remains stationary and void of DRL interference, ensuring
a stable vortex shedding state at the onset of training. The pretrained Transformer receives 24
observation signals, normalized to [0,1], and outputs normalized rotational and vertical velocity,
Ë†ğ‘‰ğœƒand Ë†ğ‘‰ğ‘¦. These output actions are then scaled by Ë†Lfor amplitude adjustment. Initialized at
[0.5,0.5],Ë†Lremains constant throughout training, and its effects will be discussed in Section
3.2. The interactions and updates will be repeated at every time step between agents and the
environment.
3. Results and discussion
We divide this section into three parts, each designed to examine a particular aspect of the DRL-
based control flapping foil performance: 1) the effectiveness of the proposed DRL framework, 2)
the enhanced performance of the DRL optimized trajectory compared to sinusoidal motions, and
3) physical insight on the benefit of DRL optimization.Guidelines for authors 5
5c 3c 1.5c
A
B
C
A
B
C
Figure 2. (Top) Reward over 200 episodes of different combinations of RL algorithms and NN structures.
The solid line and shadow represent the mean and variance of three repeated training results. The inset ğ¶ğ‘‡
andğœ‚plots are selected for TPPT agent. Note that the agent of MLP+PPO 1 (50) uses the current observation
(50 history data collection) as the state. (Bottom) The mean wake velocity profiles, instantaneous vorticity,
the time traces of actions (Ë†ğ‘‰ğ‘¦,Ë†ğ‘‰ğœƒ)and forces(ğ¶ğ‘‡,ğ¶ğ¿)(from left to right) for three cases denoted as red
dots in the top figure: (A) the 14ğ‘¡â„episode, Â¯ğ¶ğ‘‡=1.93 andğœ‚=0.12; (B) the 104ğ‘¡â„episode, Â¯ğ¶ğ‘‡=4.0 and
ğœ‚=0.13; (C) the 182ğ‘¡â„episode, Â¯ğ¶ğ‘‡=3.52 andğœ‚=0.16.
3.1. Whether it can flap: the DRL Training process of different agents
To demonstrate the effectiveness and efficiency of the proposed learning framework, we
compare in figure 2 the reward (set (ğ›½ğ¶,ğ›½P,ğ¶ğ‘š
ğ‘‡,ğ¶ğ‘š
P)=(0.1,1/3000,10,3000)in (2.5)) of
each episode for selected different combinations of RL algorithms and NN structures, including
RNN+SAC, Multilayer Perceptron(MLP)+PPO, and Transformer+PPO (TP).
Figure 2 (top) shows the two TP agents (green and yellow) manage to learn the nature of
oscillatory flapping motion while reaching the highest reward, compared with other selected
algorithms. In addition, the TP with pretraining (TPPT) agent reaches convergence faster and
with a smaller variance (shadow) among repeated training processes, compared with the agent of
TP using random initialization. Furthermore, we inset the ğ¶ğ‘‡andğœ‚figure for the TPPT agent,
showing an increase and convergence of the hydrodynamic performance.
We select three cases (red dots in figure 2) to describe the evolution of the TPPT agent and plot
their mean wake velocity profiles, instantaneous vorticity, the time history of actions and forces6 A. N. Other, H.-C. Smith and J. Q. Public
â…¢
â…¡
â… 
ğ¶ğ‘‡= 4.40,ğœ‚= 0.184
ğ¶ğ‘‡= 25.7,ğœ‚= 0.08
ğ¶ğ‘‡= 14.3,ğœ‚= 0.14
Figure 3. Scatters of Â¯ğ¶ğ‘‡andğœ‚for sinusoidal (blue) and TPPT agent (red) optimized motions. Note that the
big red dots have different rewards tuples and the small red dots are acquired by adjusting Ë†Lafter the training
is finished. The inset figures plot instantaneous vorticity, the Ë†ğ‘‰ğ‘¦(red) and Ë†ğ‘‰ğœƒ(blue) time trace of the selected
three cases. In scenario I, Â¯ğ¶ğ‘‡=4.40,ğœ‚=0.184, the reward tuple is (0.1,1
3000,10,3000)and Ë†L=[0.5,0.5].
In scenario II, Â¯ğ¶ğ‘‡=14.3,ğœ‚=0.14, reward tuple is (0.1,1
3000,10,3000)and Ë†L=[0.58,0.58]. In scenario
III,Â¯ğ¶ğ‘‡=25.7,ğœ‚=0.08, the reward tuple is (0.1,1
3000,30,3000)and the Ë†L=[0.5,0.5].
in figure2 (bottom). It first can be seen from case A that the action curve resembles a sinusoidal
motion owing to the expert demonstration from the pretraining. Then the action curve in case B
becomes non-parametric, though still oscillatory, yet evolves into a period with sudden changes,
spikes, and plateaus. In the meantime, comparing the mean velocity profiles in the wake between
cases A and B, a stronger jet develops, indicating the hydrodynamic performance of a larger ğ¶ğ‘‡
learned by the TPPT agent. Though the reward does not change much as training continues, we
observe that action in case C becomes smoother, accompanied by a less spiked force profile. Note
thatğœ‚increases by 23% from case B to C.
3.2. How well it flaps: the hydrodynamic outperformance through reward shaping
After showing feasibility and efficiency of the TPPT agent, to demonstrate its superior on foil
trajectory optimization, we compare in figure 3 the hydrodynamic performances of the different
TPPT agents optimized trajectories with sinusoidal motion Brute-force (B.F.) Â¯ğ¶ğ‘‡,ğœ‚search results
some of which work as expert policy and initialization of the TPPT agent.
Figure 3 shows the Pareto front formed by B.F. results (small blue dots) where parameters
search space are ğ‘†ğ‘Ÿâˆˆ[0.1,0.4],â„0âˆˆ[0.1,0.6],ğœƒ0âˆˆ[5â—¦,70â—¦],ğœ™âˆˆ[0â—¦,180â—¦]. In addition,
two TPPT agents (big red dots) and four evolved TPPT agents (small red dots) among different
training obviously exceed the Pareto front under different reward tuples and Ë†Lvalues setting.
We select three scenarios to clearly show the enhancement of Â¯ğ¶ğ‘‡andğœ‚introduced by trajectory
change and plot their instantaneous vorticity and the time history of actions. It can be seen that
all scenariosâ€™ actions are non-sinusoidal, sharing the same features of case C in 3.1. In scenario I,
Â¯ğ¶ğ‘‡,ğœ‚improves compared to the case C in 3.1, but action history is different due to total training
episode being prolonged. Compared to scenario I, when increase Ë†Lvalue, the Â¯ğ¶ğ‘‡of scenario II get
extremely enhanced and amplitude of Ë†ğ‘‰ğœƒincrease with spike appearance. Compared to scenarioGuidelines for authors 7
D1
(c)
D2
D2
D3
D3
D4
D4
D1
(a)
(d)
D3
(+) (-)
(b)
S1
S2S3
S4
S4
S3
S2
S1(e) (f)
ğ¶ğ‘ƒ= 10
(+) (-)
ğ¶ğ‘ƒ= 10
Figure 4. Instantaneous vorticity, pressure distributions and time trace of action and force over two vortex
shedding periods for TPPT scenario I (left)(see Movie 1) and its statistically equivalent sinusoidal motion
(right)(see Movie 2). Four foil posture moments are selected to show complete pressure distribution change.
For better visualization, the foil shown here is NACA 0020, and the ğ¶ğ‘is scaled to 1/10 from its original
value.
I, when increasing the ğ¶ğ‘š
ğ‘‡to emphasize Â¯ğ¶ğ‘‡optimization, the Â¯ğ¶ğ‘‡of scenario III get further
extremely enhanced and itâ€™s action frequency and amplitude increase with small oscillation. Note
that thoughğœ‚decreases from scenario I to scenario III, the overall hydrodynamic performances
still outperform the B.F. results.
3.3. Why it flaps better: the physical insight of DRL optimization strategy
To shed light on the underlying mechanism behind the flapping performance improvement
from the TPPT agent, we analyze and plot in figure 4 the vorticity ( ğ‘,ğ‘), pressure distributions
around foil(ğ‘,ğ‘’), time history of actions and forces ( ğ‘‘,ğ‘“) of scenario I (left, from figure 3) and
its statistically equivalent (same ğ‘†ğ‘Ÿ,â„0,ğœƒ0andğœ™) sinusoidal motion counterpart (right).
In figure4 (ğ‘,ğ‘), stronger separated vortices are observed in the whole area. A closer look at the
stronger leading-edge vortices reveals that the TPPT-controlled motion generates higher negative
pressure alongside forward-facing surfaces, contributing to thrust (Batchelor & Batchelor 1967).
The effects of strengthened separated wake are represented quantitatively by pressure distributions
around the foil, compared in figure4 ( ğ‘,ğ‘’). Based on the orientation of the foil surface, the pressure
acting perpendicularly to the surface can contribute to thrust or drag forces (Lucas et al. 2020). The
values of positive and negative pressure around the foil trailing edge are significantly increased
in the D3 moment when compared with S3, which functions to increase ğ¶ğ‘‡directly. Note
that the D3 moment is the moment with the maximum acceleration. Thus, we hypothesize that
under the objectives of maximizing Â¯ğ¶ğ‘‡and minimizing ğœ‚, the TPPT agent senses the separated
vortices around the foil body via pressure and takes advantage of them by actively controlling the
kinematics and thus improves the hydrodynamic performance. This agrees with the hypothesis
(MÂ¨ uller et al. 1997) that fish can adjust their kinematics to control near-body flow and improve
swimming performance.
4. Conclusion
In the present work, we aim to answer whether the DRL agent can learn a reasonable strategy
for complex unsteady fluid control problems such as foil flapping, how well it performs compared
to the sinusoidal motion, and if so, why the agent can learn better.
By carefully devising the training framework (TPPT in our case), the agent can outperform the
brute-force search of the sinusoidal motion by taking advantage of the vortex-foil interaction and8 A. N. Other, H.-C. Smith and J. Q. Public
learning coherent non-parametric trajectories. In addition, by adding expert data as initialization,
the agent can reach convergence rapidly with the highest reward value.
Furthermore, with a close look into the wake morphology and instantaneous pressure around
the foil, the agent can adaptively adjust the statistically similar sinusoidal motion, generating
stronger vortices and alternating phases between the motions of the foil and shedding vortices,
thus leading to an improvement in hydrodynamic performance.
It is noted that in the current work, we select the simulation environment of low mesh density
and the Reynolds number for the proof-of-concept demonstration of DRL with unsteady flapping
foil flow control. We believe that our result, for the first time, shows the potential of DRL in
complex and time-variant flow control, providing a feasible method to reproduce animal-similar
flapping motion and solve other complex flow manipulation tasks.
Appendix A
POMDP describes the process of an agent at time ğ‘¡and in stateğ‘ ğ‘¡receiving observation ğ‘œğ‘¡
with a belief ğ‘over the state space, and then taking action ğ‘based on policy ğœ‹(ğ‘|ğ‘œ,ğ‘)with
feedback reward ğ‘Ÿğ‘¡. Specifically, the POMDP is defined by a tuple (S,Z,A,ğ‘‚,P,ğ‘…,ğ›¾), where
S,Z,Aare finite sets of state ğ‘ , observation ğ‘œ, and action ğ‘. The transition and observation
functionsP:SÃ—Aâ†’Î”(S)andğ‘‚:SÃ—Aâ†’Î”(Z) describe the probability of the next state
ğ‘ ğ‘¡+1and observation ğ‘œğ‘¡+1in a given state ğ‘ ğ‘¡after taking a given action ğ‘ğ‘¡. In addition, the reward
functionğ‘…:SÃ—Aâ†’ Rdefines the reward received by the agent, and ğ›¾âˆˆ[0,1]is the discount
factor. Therefore, the goal of the agent to find a policy ğœ‹that maximizes the expected discounted
sum of rewards over time, subject to the uncertainty of the environment, as follows:
max
ğœ‹Eğ‘ 0,ğ‘0,ğ‘ 1,ğ‘1,..."âˆâˆ‘ï¸
ğ‘¡=0ğ›¾ğ‘¡ğ‘…(ğ‘ ğ‘¡,ğ‘ğ‘¡)#
, (4.1)
where in the current problem, ğ‘œğ‘¡is a 24-dimensional array, containing the instantaneous heave
and pitch positions and velocities, and pressure measured from 20 sparse sensors around the foil.
ğ‘ğ‘¡is a 2-dimensional array of the prescribed pitch and heave velocities determined by the agent.
PPO algorithm follows the actor-critic framework in reinforcement learning. The actor
ğœ‹(ğ‘|ğ‘œ,ğ‘), parameterized as ğœƒ, interacts with the environment, while the critic ğ‘‰(ğ‘ ), parameterized
asğœ™, predicts the onward cumulative reward. For Actor, PPO maximizes a clip objective to penalize
changes to the policy that move ğ‘Ÿğ‘¡(ğœƒ)far away from the old policy:
ğ¿ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ(ğœƒ)=Ë†Eğ‘¡h
min
ğ‘Ÿğ‘¡(ğœƒ)Ë†ğ´ğ‘¡,clip(ğ‘Ÿğ‘¡(ğœƒ),1âˆ’ğœ–,1+ğœ–)Ë†ğ´ğ‘¡i
, (4.2)
whereğ‘Ÿğ‘¡(ğœƒ)=ğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡)
ğœ‹ğœƒold(ğ‘ğ‘¡|ğ‘ ğ‘¡)denote the probability ratio, ğœ–is a hyper-parameter to constrain the
change of policy, and ğ´ğ‘¡is the advantage to reduce policy gradient variance (Sutton & Barto
2018). For the critic, PPO minimizes the temporal difference loss as:
ğ¿ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘(ğœ™)=ğ‘Ÿğ‘¡+ğ›¾ğ‘‰(ğ‘ ğ‘¡+1;ğœ™)âˆ’ğ‘‰(ğ‘ ğ‘¡;ğœ™). (4.3)
Therefore, the learning objective for PPO is defined as: ğ¿(ğœƒ,ğœ™)=ğ¿ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘(ğœ™)+ğ¿ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ(ğœƒ).
Transformer. The core of the Transformer is the attention mechanism, defined as:
Attention(Q,K,V)=softmax QKğ‘‡
âˆšğ‘‘ğ‘˜V, (4.4)
where Q,K,Vare vectors of queries, keys, and values, respectively, which are learned during
training, and ğ‘‘ğ‘˜is the dimension of QandK. In self-attentions, Q,K,Vshare the same set of
parameters. The attention mechanism allows for the estimation of ğ‘ƒ(y|x)orğ‘ƒ(ğ‘¦ğ‘›|ğ‘¥1,...,ğ‘¥ğ‘›)Guidelines for authors 9
without the need for recursive processes, as in RNNs, which results in higher computational
efficiency and long-term interaction modelling ability. Our customized Transformer architecture
uses the history sequence as the belief about the state, which comprises two primary components: a
two-layer encoder and a linear layer as the decoder. Each encoder features a self-attention structure
and a feed-forward neural network (FNN) layer. The self-attention structure incorporates two
attention heads, a hidden state dimension of 32, and a query dimension of 128. Our experiments
are conducted on a server with 2 Nvidia A100 GPU and AMD EPYC 7742 CPU.
Declaration of Interests
The authors report no conflict of interest.
REFERENCES
Ashraf, I., Wassenbergh, S. V. & Verma, S. 2021 Burst-and-coast swimming is not always energetically
beneficial in fish (hemigrammus bleheri). Bioinspir. Biomim. 16(1), 016002.
Batchelor, Cx K & Batchelor, George Keith 1967 An Introduction to Fluid Dynamics . Cambridge
university press.
Berner, Christopher, Brockman, Greg, Chan, Brooke, Cheung, Vicki, Dkebiak, Przemyslaw,
Dennison, Christy, Farhi, David, Fischer, Quirin, Hashme, Shariq, Hesse, Chris & others
2019 Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680 p. 1.
Brown, Tom, Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared D, Dhariwal,
Prafulla, Neelakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Amanda & others
2020 Language models are few-shot learners. Advances in neural information processing systems 33,
1877â€“1901.
Chin, Diana D & Lentink, David 2016 Flapping wing aerodynamics: from insects to vertebrates. Journal
of Experimental Biology 219(7), 920â€“932.
Degrave, Jonas, Felici, Federico, Buchli, Jonas, Neunert, Michael, Tracey, Brendan, Carpanese,
Francesco, Ewalds, Timo, Hafner, Roland, Abdolmaleki, Abbas, de Las Casas, Diego &
others 2022 Magnetic control of tokamak plasmas through deep reinforcement learning. Nature
602(7897), 414â€“419.
Esslinger, Kevin, Platt, Robert & Amato, Christopher 2022 Deep transformer q-networks for partially
observable reinforcement learning. arXiv preprint arXiv:2206.01078 .
Fan, Dixia, Yang, Liu, Wang, Zhicheng, Triantafyllou, Michael S & Karniadakis, George Em
2020 Reinforcement learning for bluff body active flow control in experiments and simulations.
Proceedings of the National Academy of Sciences 117(42), 26091â€“26098.
Izraelevitz, Jacob S & Triantafyllou, Michael S 2014 Adding in-line motion and model-based
optimization offers exceptional force control authority in flapping foils. Journal of Fluid Mechanics
742, 5â€“34.
Li, Gen, Ashraf, Intesaaf, Franc Â¸ois, Bill, Kolomenskiy, Dmitry, Lechenault, Fr Â´edÂ´eric, Godoy-
Diana, Ramiro & Thiria, Benjamin 2021 Burst-and-coast swimmers optimize gait by adapting
unique intrinsic cycle. Communications biology 4(1), 40.
Lighthill, James 1969 Hydromechanics of aquatic animal propulsion. Annu. Rev. Fluid Mech. 1(1),
413â€“446.
Liu, K., Huang, H. B. & Lu, X.-Y. 2020 Hydrodynamic benefits of intermittent locomotion of a self-propelled
flapping plate. Phys. Rev. E 102, 053106.
Lucas, Kelsey N, Lauder, George V & Tytell, Eric D 2020 Airfoil-like mechanics generate thrust on
the anterior body of swimming fishes. Proceedings of the National Academy of Sciences 117(19),
10585â€“10592.
Maertens, Audrey P & Weymouth, Gabriel D 2015 Accurate cartesian-grid simulations of near-body
flows at intermediate reynolds numbers. Computer Methods in Applied Mechanics and Engineering
283, 106â€“129.
Muhammad, Zaka, Alam, Md Mahbub & Noack, Bernd R 2022 Efficient thrust enhancement by modified
pitching motion. Journal of Fluid Mechanics 933, A13.
MÂ¨uller, UK, Van Den Heuvel, BLE, Stamhuis, EJ & Videler, JJ 1997 Fish foot prints: morphology and10 A. N. Other, H.-C. Smith and J. Q. Public
energetics of the wake behind a continuously swimming mullet (chelon labrosus risso). The Journal
of Experimental Biology 200(22), 2893â€“2906.
Qi, Jixiang, Chen, Zihao, Jiang, Peng, Hu, Wenxia, Wang, Yonghuan, Zhao, Zeang, Cao, Xiaofei,
Zhang, Shushan, Tao, Ran, Li, Ying & others 2022 Recent progress in active mechanical
metamaterials and construction principles. Advanced Science 9(1), 2102662.
Rabault, Jean, Kuchta, Miroslav, Jensen, Atle, R Â´eglade, Ulysse & Cerardi, Nicolas 2019 Artificial
neural networks trained through deep reinforcement learning discover control strategies for active
flow control. Journal of Fluid Mechanics 865, 281â€“302.
Schlanderer, Stefan C, Weymouth, Gabriel D & Sandberg, Richard D 2017 The boundary
data immersion method for compressible flows with application to aeroacoustics. Journal of
Computational Physics 333, 440â€“461.
Schulman, John, Wolski, Filip, Dhariwal, Prafulla, Radford, Alec & Klimov, Oleg 2017 Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 .
Silver, David, Schrittwieser, Julian, Simonyan, Karen, Antonoglou, Ioannis, Huang, Aja, Guez,
Arthur, Hubert, Thomas, Baker, Lucas, Lai, Matthew, Bolton, Adrian & others 2017
Mastering the game of go without human knowledge. Nature 550(7676), 354â€“359.
Sutton, Richard S & Barto, Andrew G 2018 Reinforcement learning: An introduction . MIT press.
Teng, Lubao, Deng, Jian, Pan, Dingyi & Shao, Xueming 2016 Effects of non-sinusoidal pitching motion
on energy extraction performance of a semi-active flapping foil. Renewable Energy 85, 810â€“818.
Triantafyllou, Michael S, Weymouth, Gabriel D & Miao, Jianmin 2016 Biomimetic survival
hydrodynamics and flow sensing. Annual Review of Fluid Mechanics 48, 1â€“24.
Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N,
Kaiser, Lukasz & Polosukhin, Illia 2017 Attention is all you need. Advances in Neural Information
Processing Systems 30.
Wang, Yi-Zhe, Mei, Yu-Fei, Aubry, Nadine, Chen, Zhihua, Wu, Peng & Wu, Wei-Tao 2022 Deep
reinforcement learning based synthetic jet control on disturbed flow over airfoil. Physics of Fluids
34(3), 033606.
Weymouth, Gabriel D & Yue, Dick KP 2011 Boundary data immersion method for cartesian-grid
simulations of fluid-body interaction problems. Journal of Computational Physics 230(16), 6233â€“
6247.
Won, Dong-Ok, M Â¨uller, Klaus-Robert & Lee, Seong-Whan 2020 An adaptive deep reinforcement
learning framework enables curling robots with human-like performance in real-world conditions.
Science Robotics 5(46), eabb9764.
Wu, Xia, Zhang, Xiantao, Tian, Xinliang, Li, Xin & Lu, Wenyue 2020 A review on fluid dynamics of
flapping foils. Ocean Engineering 195, 106712.