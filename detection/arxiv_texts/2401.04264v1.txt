arXiv:2401.04264v1  [cs.GT]  8 Jan 2024General Performance Evaluation for Competitive Resource A llocation Games via
Unseen Payoff Estimation
N’yoma Diamond1,2, Fabricio Murai1
1Worcester Polytechnic Institute
2University of Cambridge
Abstract
Many high-stakes decision-making problems, such as those
found within cybersecurity and economics, can be modeled
as competitive resource allocation games. In these games,
multiple players must allocate limited resources to overco me
their opponent(s), while minimizing any induced individua l
losses. However, existing means of assessing the performan ce
of resource allocation algorithms are highly disparate and
problem-dependent. As a result, evaluating such algorithm s
is unreliable or impossible in many contexts and applicatio ns,
especially when considering differing levels of feedback. To
resolve this problem, we propose a generalized deﬁnition of
payoff which uses an arbitrary user-provided function. Thi s
uniﬁes performance evaluation under all contexts and lev-
els of feedback. Using this deﬁnition, we develop metrics
for evaluating player performance, and estimators to appro xi-
mate them under uncertainty (i.e., bandit or semi-bandit fe ed-
back). These metrics and their respective estimators provi de
a problem-agnostic means to contextualize and evaluate alg o-
rithm performance. To validate the accuracy of our estimato r,
we explore the Colonel Blotto ( CB) game as an example. To
this end, we propose a graph-pruning approach to efﬁciently
identify feasible opponent decisions, which are used in com -
puting our estimation metrics. Using various resource allo ca-
tion algorithms and game parameters, a suite of CBgames are
simulated and used to compute and evaluate the quality of our
estimates. These simulations empirically show our approac h
to be highly accurate at estimating the metrics associated w ith
the unseen outcomes of an opponent’s latent behavior.
1 Introduction
The necessity for strategic resource allocation pervades
many critical real-world domains, such as cybersecurity,
economics, and epidemiology. We oftentimes have limited
resources for our goals, and thus need to allocate them care-
fully to reach an optimal outcome. For example, the failure
or success of a cyberattack may depend on the allocations
of offensive or defensive resources across a variety of digi -
tal systems. If the defending entity fails to allocate enoug h
resources to defend a vulnerable system, the attacking enti ty
may succeed in an attack on that system. Yet, overcompen-
sating by allocating excess resources may result in deﬁcien -
cies elsewhere. In practice, accurately identifying optim al
ways to efﬁciently allocate resources is a challenging task ,
especially when we lack information about the adversary’s
allocations—i.e., under bandit or semi-bandit feedback.Competitive resource allocation games emerge as a natu-
ral way to model these problems. One such highly explored
game is the Colonel Blotto ( CB) game. In theCBgame, two
players compete by allocating limited resources to a number
of battleﬁelds with the goal of overpowering their opponent
on as many of them as possible. Under this interpretation, th e
received payoff directly relates to the player’s allocatio ns for
the round, henceforth referred to as their “ decision ”. Sub-
stantial work has been done in the past to create algorithms
that approach approximately optimal strategies for the CB
game and other analogous resource allocation games when
given only minimal information to learn from. However,
in order to develop and evaluate these algorithms, varying
and inconsistent assumptions are made to narrow the prob-
lem space. This makes them highly scenario-speciﬁc, and
thus incomparable and potentially inapplicable to many rea l-
world problems. In particular, we failed to identify any ex-
tant literature which can be applied to the context of mutu-
ally adaptive adversaries; the broader context such that bo th
agents may be actively attempting to actively hinder their
opponent in some way. This circumstance is applicable to
many real-world scenarios (such as the cybersecurity exam-
ple mentioned earlier), and can be used to generalize the
evaluation of resource allocation algorithms more broadly .
To remedy this gap, we develop a general deﬁnition of
payoff for competitive resource allocation games with the
goal of unifying performance evaluation independent of
context and feedback. Furthermore, we propose a suite of
practical evaluation metrics based on our general payoff de f-
inition, and practical means for approximating them under
uncertainty. Focusing on the CBgame, we propose a graph-
pruning approach for identifying feasible opponent deci-
sions, which can be used to efﬁciently compute our esti-
mated payoff metrics. Finally, to improve the efﬁciency and
accuracy of our approach, we prove a number of bounds
for feasible opponent allocations under semi-bandit feed-
back. This provides an approach through which future re-
search and development may assess the performance of re-
source allocation algorithms under theoretical contexts a nd
active real-world use. To empirically validate the effecti ve-
ness of our approach, we perform simulations of CBgames
and evaluate them utilizing our proposed metrics and algo-
rithm. Our experiments show that our approach is highly ef-
fective at estimating the true payoff metrics associated wi ththe actual opponent behavior, even under uncertainty.
Outline. In § 2 we discuss the related work on competi-
tive resource allocation games and performance evaluation .
In § 3 we propose our deﬁnition of generalized payoff. In
§§ 4 and 5, we propose, respectively, our evaluation metrics
utilizing general payoff and their corresponding estimato rs.
In § 6 we present the CBgame used as a case study for our
metrics. In § 7 we propose a novel technique for narrowing
down the set of feasible opponent decisions in the CBgame.
In § 8 we identify bounds on feasible opponent allocations
under semi-bandit feedback. In § 9 we utilize the proposed
techniques to simulate and evaluate the quality of our es-
timation metrics with respect to their true counterparts. I n
§ 10 we summarize the conclusions of our work.
2 Related Work
Many researchers explore the problem of strategic resource
allocation through the lens of combinatorial bandits. This is
a variation on the highly-explored multi-armed bandit prob -
lem, such that the agent may simultaneously pull multiple
arms within some budget, as opposed to only a single arm.
This problem introduces the distinction between “bandit”
and “semi-bandit” feedback (Audibert, Bubeck, and Lugosi
2014). Under bandit feedback, the agent receives a single
aggregate payoff as feedback, which makes it difﬁcult to de-
termine the individual impact of each component of a deci-
sion. Conversely, under semi-bandit feedback scenario, th e
agent receives distinct feedback about the payoffs associ-
ated with each component of a decision. This provides more
information than bandit feedback, while still having uncer -
tainty about the true behavior of how rewards are provided.
Under combinatorial bandits, Zuo and Joe-Wong (2021)
propose two online algorithms using combinatorial decisio n
spaces for the discrete and continuous resource cases. They
consider a general reward function, making their algorithm s
applicable to many contexts. However, their algorithms de-
pend on the usage of an unspeciﬁed oracle function which
may not be practical or possible to implement efﬁciently,
and thus compare against. Additionally, Koc´ ak et al. (2014 )
propose a version of the existing EXP3(Auer et al. 2012) al-
gorithm leveraging observability graphs to consider the po -
tential value of unexplored decisions.
Vu, Loiseau, and Silva (2019); Vu et al. (2020) reframe
the combinatorial interpretation as a path-planning prob-
lem with inspiration from the observability graphs from
Koc´ ak et al. (2014). To do so, they expand on the exist-
ingCOMBAND (Cesa-Bianchi and Lugosi 2012) and EXP3
(Auer et al. 2012) algorithms. Vu (2020) presents a com-
prehensive survey and analysis of the existing literature r e-
garding combinatorial bandits and the CBgame to propose a
suite of resource allocation algorithms utilizing varying lev-
els of feedback. To the best of our knowledge, the works of
Vu, Loiseau, and Silva (2019); Vu et al. (2020); Vu (2020)
are the only present literature explicitly studying genera l al-
gorithms for mutually adversarial resource allocation gam es.
In this work we provide metrics and techniques which may
be used to reliably evaluate and compare algorithms such as
these.3 General Payoff
The payoff received by a player pin the set of players Pis
predicated on their chosen decision πand their opponent’s
decisionφ. Thus, we denote the set of all feasible decisions
(e.g., valid allocations in a round of the CBgame) avail-
able to player pand their opponent as ΠpandΦp, respec-
tively. Using this notation, we describe general payoff to b e
a function Lp: Πp×Φp/mapsto→Qthat returns the scalar payoff
awarded to player pif they play decision π∈Πpand their
opponent plays decision φ∈Φp. Thus for a given round t,
playerp’s payoff for that round is calculated as
Lt
p=Lp(πt,φt), (1)
whereπt∈Πpdenotes the decision played by player pin
roundtandφt∈Φpdenotes the decision played by their
opponent. Further, by ﬁxing φtbut allowing πto vary, we
introduce the following shorthand:
Lt
p(π):=Lp(π,φt), (2)
which represents the payoff that player pwould receive for
playing a speciﬁc decision πin roundt.
Note that a generalized interpretation of regret can be pro-
duced trivially by taking the difference of the general pay-
off between any two possible games. That is, for a compar-
ison of two arbitrary player decisions against one arbitrar y
opponent decision, generalized regret may be calculated as
Lp(π,φ)−Lp(π′,φ); asLt
p(π)−Lt
p(π′)given a ﬁxed oppo-
nent decision for round t; or asLt
p(π)−Lt
pwhen also given
a ﬁxed player decision for round t.
4 Payoff Metrics
Using our generalized deﬁnition of payoff, we propose two
useful metrics: Max Payoff and Expected Payoff. Max Pay-
offis deﬁned as the maximum possible (i.e., optimal) payoff
that can be received by player pagainst a particular decision
by opponent p′. It can be formally expressed as the function
L∗
p(φ):= max
π∈ΠpLp(π,φ). (3)
For a ﬁxed opponent decision φt, we denote it as:
L∗t
p:= max
π∈ΠpLt
p(π). (4)
It is important to note that L∗t
prepresents the maximum pos-
sible payoff that player pcan achieve for a given round t.
Thus this metric is highly valuable towards computing re-
gret and any associated metrics.
Let the player’s decision in a given round tbe a random
variableπ∼Dt(Πp), whereDt(Πp)is the probability dis-
tribution over the player’s decisions in Πpin roundt.Dtis
parameterized by tbecause the distribution may change de-
pending on the game’s history (such as when the players are
adaptive adversaries). Using this distribution, we deﬁne t he
Expected Payoff as player p’s expectation of payoff over
Dt(Πp). Formally, it is given by
/hatwideLp(φ):=E
π∼Dt(Πp)[Lp(π,φ)]. (5)Over a large number of plays against the same opponent
decision, the player’s average payoff will approach the Ex-
pected Payoff. Therefore this metric can be used as a refer-
ence point to identify whether an algorithm is behaving as
intended or is outperforming the expected (average) perfor -
mance of another algorithm.Yet again, we can leverage our
previously described shorthand by ﬁxing the opponent deci-
sionφt:
/hatwideLt
p:=E
π∼Dt(Πp)/bracketleftbig
Lt
p(π)/bracketrightbig
. (6)
We note that the necessary analysis to identify Dt(Πp)is
outside the scope of this paper. Therefore, for the purpose o f
experimentation, we addres this using the Uniform Decision
Assumption described in § 5.
5 Estimating Metrics Under Uncertainty
The key challenge in computing the proposed metrics is that
we often do not observe φ. Thus, to approximate them under
uncertainty (i.e., bandit or semi-bandit feedback), we pro -
pose three associated estimation metrics: Observable Max
Payoff, Supremum Payoff, and Observable Expected Payoff.
These metrics are agnostic to whether the player receives
bandit or semi-bandit feedback. This is done by identifying
and using a set of feasible opponent decisions that would re-
sult in the observed game. That is, given a player p, a round
t, a decision πt∈Πp, and some round-speciﬁc feedback
(such as an observed payoff Lt
por a vector of payoffs asso-
ciated with a decision), the player computes a set of feasibl e
opponent decisions Φt
p. Given a ﬁxed player decision πt, any
and all decisions φ∈Φt
pmust produce the same feedback
as that observed for round t. Assuming the user has sufﬁ-
cient knowledge of the nature of the game, it should always
be possible to identify feasible opponent decisions in some
capacity. A speciﬁc approach using the CBgame as an ex-
ample is discussed in §§ 7 and 8.
We propose two approximations of Max Payoff, each with
distinct purposes: Observable Max Payoff and Supremum
Payoff. We deﬁne Observable Max Payoff as the expecta-
tion of Max Payoff over Dt(Φt
p):
L∗t
p≈E
φ∼Dt(Φtp)/bracketleftbigg
max
π∈ΠpLp(π,φ)/bracketrightbigg
. (7)
Observable Max Payoff may be used as a direct approxima-
tion of the true value of Max Payoff. This is because over a
large number of rounds the running average of Max Payoff
for a particular opponent decision should approach the Ob-
servable Max Payoff, since the latter is the expectation of t he
former (assumingDt(Φt
p)does not change signiﬁcantly).
We also specify the Supremum Payoff as the minimum
possible Max Payoff over Φt
p:
sup(Lt
p):= min
φ∈Φtp/bracketleftbigg
max
π∈ΠpLp(π,φ)/bracketrightbigg
. (8)
Supremum Payoff represents the minimal upper bound on
possible payoff. In other words, it the Supremum Payoff is
the best possible payoff that player pcan receive if their
opponent played their best possible decision. This is theworst-case scenario for p, where the opponent’s decision
minimizes p’s best possible payoff. Additionally, Supremum
Payoff represents the maximum payoff the player can guar-
antee is achievable in a particular round, given the infor-
mation available to them. Hence, it may be valuable to use
Supremum Payoff as an objective function for player p. Fur-
thermore, Supremum Payoff is guaranteed to be equivalent
to or underestimate the true Max Payoff, making it a pes-
simistic metric. However, in situations where the opponent
has substantially more resources than the player, the value of
Supremum Payoff may approach the observed payoff Lt
p, as
the opponent will have many more possible decisions which
favor them. That is, it is likely that there exists an opponen t
decision that makes it impossible for the player to improve
their payoff. One of the notable beneﬁts of Supremum Pay-
off over Observable Max Payoff is that it does not rely on
knowing the nature of Dt(Φt
p).
Similar to Observable Max Payoff, in order to approxi-
mate Expected Payoff, we compute its expectation over Φt
p.
We refer to this estimation metric as Observable Expected
Payoff . This is equivalent to the expectation of payoff with
respect to bothDt(Πp)andDt(Φt
p):
/hatwideLt
p≈ E
φ∼Dt(Φtp),π∼Dt(Πp)[Lp(π,φ)]. (9)
The Uniform Decision Assumption
In practice, it is often functionally impossible to identif y the
true nature ofDt(Φp)without knowing the opponent’s algo-
rithm and its parameters. Hence, to compute our estimation
metrics, we introduce the Uniform Decision Assumption
(UDA ). Under the UDA, we assume that the true distribution
of opponent decisions Dt(Φp)is approximately equivalent
to the uniform distribution. Clearly, adversarial algorit hms
do not produce decisions uniformly. However, we believe
the UDA to be an adequate means to enable meaningful ap-
proximation of opponent behavior when lacking a method
of identifyingDt(Φp). We implicitly validate the effects of
this assumption in our experimental analysis of our estima-
tion metrics, as any signiﬁcant error resultant from using
the UDA should cause the quality of any estimates depen-
dent on it to be highly inaccurate. Speciﬁcally, the UDA is
used when computing Observable Max Payoff and Observ-
able Expected Payoff due to requiring knowledge of Dt(Φp)
(andDt(Πp)in the case of Observable Expected Payoff).
6 Formulation of the CBGame Example
To explore the usage and efﬁcacy of our metrics and esti-
mates, we consider the example of the CBgame. An instance
of theCBgame is deﬁned as a two-player repeated constant-
sum game of (potentially unknown) length T∈N1such
that the set of players is denoted by P={A,B}. Player
p∈Pand their opponent p′are allotted some ﬁxed number
of resources Np,Np′∈N1. LetK∈N1be the number of
battleﬁelds, such that each battleﬁeld can be represented b y
an integer i∈I= [1..K]. For each round t∈[1..T],
the resource allocation by player pto a battleﬁeld i∈Iis
denotedπt
i, while their opponent’s allocation is denoted φt
i.
The sum of allocations by any player pin a given round tareﬁxed, such that/summationtextK
i=1πt
i=Np. We specify that all alloca-
tions are discrete, such that πt
i,φt
i∈N0. For each round all
of the players’ resources are renewed and a static one-shot
CBgame is played in which each player produces a decision
(i.e., vector of allocations) denoted πtfor player pandφt
forp′, such that
πt:=/a\}⌊ra⌋ketle{tπt
1,πt
2,...,πt
K/a\}⌊ra⌋ketri}ht, (10)
φt:=/a\}⌊ra⌋ketle{tφt
1,φt
2,...,φt
K/a\}⌊ra⌋ketri}ht. (11)
To enforce the constant-sum nature of the game, we en-
force a bias when both players allocate the same number of
resources to a battleﬁeld (i.e., a draw). Without loss of gen er-
ality, we specify the variable δp∈{0,1}indicating whether
playerpwins (1) or loses ( 0) draws. Thus for a given battle-
ﬁeldiand round t, ifπt
i+δp> φt
ithen player pwins the
battleﬁeld. Otherwise, their opponent p′wins the battleﬁeld.
Thus, the payoff function used with the CBgame is
Lp(π,φ):=/summationdisplay
i∈I[πi+δp> φi]. (12)
Given a player p, for each battleﬁeld iin roundt, we de-
note the associated payoff as ℓt,i
psuch that ℓt,i
p= 1ifpwon
the battleﬁeld, or ℓt,i
p= 0if they lost. The vector of payoffs
Lt
pawarded to a player pin a given round is denoted as
Lt
p:=/a\}⌊ra⌋ketle{tℓt,1
p,ℓt,2
p,...,ℓt,K
p/a\}⌊ra⌋ketri}ht, (13)
and the total scalar payoff received by player pat roundtis
Lt
p=Lp(πt,φt) =K/summationdisplay
i=1ℓt,i. (14)
Note that computing Max Payoff for the CBgame is trivial
as explained in § S11.1
Under bandit feedback, player ponly receives their total
payoffLt
p, while under semi-bandit feedback, they receive
the vector payoffLt
p. For the purposes of our exploration in
this paper, we choose to focus on semi-bandit feedback to
provide greater ability to narrow down the set of feasible
opponent decisions via bounding the possible allocations,
making our estimates more accurate and easier to compute.
This formulation is a slight modiﬁcation of that used by
Roberson (2006) and Schwartz, Loiseau, and Sastry (2014).
Our alterations are as follows: Firstly, we do not assume
that the player that loses draws must have the same or fewer
number of resources compared to their opponent. Secondly,
we allow the player to provided a vector containing the re-
spective payoffs for each battleﬁeld, instead of the aggreg ate
score (i.e., semi-bandit instead of bandit feedback).
7 Modeling Feasible CBGames
We adapt the path-planning approach from Vu, Loiseau, and
Silva (2019) to model feasible opponent decisions within th e
CBgame. This model utilizes a graph-based interpretation
of theCBto efﬁciently explore varying decisions and their
associated outcomes. Notably, this approach accommodates
1See supplementary materials.Battleﬁeld i= 1
Battleﬁeld i= 2
Battleﬁeld i= 3K= 3Np= 4
s
(1,0)(1,1) (1,2) (1,3)(1,4)
(2,0)(2,1) (2,2) (2,3)(2,4)
d
Figure 1: Decision graph G3,4for a game with K= 3 bat-
tleﬁelds given Np= 4 resources. Blue path represents the
decision/a\}⌊ra⌋ketle{t1,0,3/a\}⌊ra⌋ketri}ht; red path represents the decision /a\}⌊ra⌋ketle{t4,0,0/a\}⌊ra⌋ketri}ht.
varying levels of feedback received by the player (i.e., ban dit
or semi-bandit). Using this model, we develop an efﬁcient
technique for generating the set of decisions available to a
player or their opponent in each round of the CBgame.
TheCBDecision Graph
For aCBgame with Kbattleﬁelds indexed by I= [1..K],
we focus on a given player pwith resources Np. Vu, Loiseau,
and Silva (2019) have shown that there exists a DAG GK,Np,
which we henceforth refer to as a “decision graph,” such that
the set of all decisions Πpplayable by pmaps one-to-one
against the set of all paths through GK,Np(see Fig. 1).
LetVbe the set of all vertices in the graph. Each ver-
tex has two coordinates iandnrepresenting its position,
such that vi,n∈V is a vertex located at coordinate (i,n).
Values of iandnare discrete, such that i∈{0}∪Iand
n∈[0..Np]. Thusirepresents a particular battleﬁeld in the
set of battleﬁelds I, plus an additional i= 0 position, and
nrepresents the cumulative amount of resources allocated
on the path up to and including a given vertex. Vertices are
present at every point in space [1..K−1]×[0..Np], in
addition to the vertices s:=v0,0andd:=vK,Np. That is,
V:={vi,n|i∈I\{K}∧n∈[0..Np]}∪{s,d}.(15)
Every vertex vi,nsuch thati >0(i.e., where vi,n/\e}atio\slash=s) has
inward edges from all vertices vi−1,n′wheren′∈[0..n].
For any directed edge connecting a vertex vi−1,n′to vertex
vi,n, denoted vi−1,n′→vi,n, we assign a weight of n−n′.
In doing so, the weight observed when moving along any
edge represents the player’s allocation πi(i.e.,πi=n−n′)
to battleﬁeld i. Thus the set of all edges Eis
E:={vi−1,n′→vi,n|vi−1,n′,vi,n∈V∧n′≤n}.(16)
We can use the decision graph GK,Npto identify all fea-
sible decisions π∈Πpby applying depth-ﬁrst search to
enumerate all paths starting at vertex sand ending at vertex
d. The weights of the edges observed, in order of traversal,
represent the decision associated with each path explored.
Note that the complexity of this computation will be propor-
tional to the number of paths through the graph, which is
O(2min(K−1,Np))(Vu, Loiseau, and Silva 2019).Pruning for Feasible Opponent Decisions
Under bandit or semi-bandit feedback, the player does not
receive sufﬁcient information to identify their opponent’ s
true decision. As such, we propose that CBdecision graphs
can be used to efﬁciently compute the set of feasible deci-
sions that an opponent may have made based on the infor-
mation received by the player in a given round. While the
problem is still non-polynomial in nature, we may signiﬁ-
cantly reduce its running time by using this approach.
GivenKbattleﬁelds, player p, roundt, player decision
πt, and opponent resources Np′, we can compute a decision
graphGt
K,Np′representing all decisions by the opponent
that would produce the same feedback received by player
pin roundt(e.g.,Lt
porLt
p). That is, every path from stod
through this feasible decision graph represents a decision φ
which, when played against πt, results in the same feedback
observed by the player. This produces the set Φt
pdeﬁned in
§ 5. For example, under bandit feedback we may identify
thatΦt
p={φ∈Φp|Lp(πt,φ) =Lt
p}.
The player can compute bounds on the feasible opponent
decisions that would produce the observed information us-
ing any available information. By computing bounds on the
feasible allocations for a given battleﬁeld, edges can be re -
moved from the opponent’s feasible decision graph that rep-
resent impossible allocations. A visual example of the prun -
ing process for a full round is shown in Fig. 2. Focusing on
a speciﬁc battleﬁeld, we may follow the example displayed
in Fig. 2(a). In this example, a player that loses draws (i.e. ,
δp= 0) allocated 3 resources to battleﬁeld 2 and won. Thus
the opponent must have allocated 2 or fewer resources to that
battleﬁeld. Thus, any edges entering a vertex representing
battleﬁeld 2 with a weight greater than 2 are invalid and can
be pruned (indicated in blue in the middle layer of Fig. 2(a)) .
In practice, the received information and how it can be used
depends on the context. To address this, we propose a gen-
eral technique that only requires bounds on the feasible op-
ponent allocations, regardless of the feedback type. There -
fore, the user only needs to implement a way compute the
bounds on the opponent’s feasible allocations.Recall that, for any edge vi−1,n′→vi,n∈E, the oppo-
nent allocation φito battleﬁeld iisφi=n−n′. We denote
the lower and upper bounds on the feasible values of φiby
φiandφi, respectively. That is, the feasible allocations by a
player to battleﬁeld iare bounded such that φi∈[φi..φi].
Thus any edge vi−1,n′→vi,nis invalid if φi=n−n′< φi
orφi=n−n′>φi. This is shown in Fig. 2(a).
Pruning edges using these bounds may create dead-ends
(i.e., vertices with outdegree 0). Trivially, no paths to ve rtex
dexist which pass through a dead-end vertex. Therefore, we
can prune all dead-end vertices (except for d) and any edges
directed at them from the feasible decision graph. This may
create new dead-end vertices. Therefore, pruning can be per -
formed iteratively from i=K−1toi= 0, eliminating all
dead-end vertices. This process is displayed in Fig. 2(b). A f-
terwards, dcan be reached from any remaining vertex in the
graph, as seen in Fig. 2(c). Thus all attempted paths start-
ing atsrepresent a valid decision. Algorithm 2 presents an
efﬁcient procedure for performing dead-end pruning.1Al-
though not necessary, we can prune vertices with indegree 0
(excluding s) to make every vertex reachable from s. How-
ever, this does not have any meaningful impact beyond po-
tentially reducing the necessary memory space required to
represent and store the graph.
8 Bounding Opponent CBAllocations
To enhance the precision and computational efﬁciency of
our payoff estimates for the CBgame, we consider the ex-
ample of semi-bandit feedback to establish bounds on an op-
ponent’s feasible decisions. The computation of these limi ts
is conducted on a per-round basis and is not dependent on
previous rounds. For this reason, in this section we omit the
round index tfrom the notation introduced in § 6. Proofs for
all proposed theorems and lemmas are presented in § S15.1
For a given round, the player receives a vector of scalar
payoffs for each battleﬁeld (semi-bandit feedback). The
player aims to estimate the feasible bounds φiandφion
their opponent’s true decision φ. We denote any battleﬁeld
(a) Allocation bound pruning
s
(1,0)(1,1) (1,2) (1,3)(1,4)
(2,0)(2,1) (2,2) (2,3)(2,4)
d(b) Dead-end pruning
s
(1,0)(1,1) (1,2) (1,3)(1,4)
(2,0)(2,1) (2,2) (2,3)(2,4)
d(c) Pruned feasible decision graph
s
(1,0)(1,1) (1,2)
(2,1) (2,2)
d
Figure 2: Pruning opponent decision graph Gt
3,4(Fig. 1) given πt=/a\}⌊ra⌋ketle{t1,3,2/a\}⌊ra⌋ketri}ht,Lt
p=/a\}⌊ra⌋ketle{t0,1,0/a\}⌊ra⌋ketri}ht, andδp= 0. Opponent allocation
bounds are φt=/a\}⌊ra⌋ketle{t1,0,2/a\}⌊ra⌋ketri}htandφt=/a\}⌊ra⌋ketle{t4,2,3/a\}⌊ra⌋ketri}ht. (a) Red and blue edges exceed feasible allocation lower and upper bounds,
respectively. (b) Red vertices are dead-ends for i= 1, while blue vertices are dead-ends for i= 2 after pruning for i= 1. (c)
The ﬁnal pruned graph of feasible opponent decisions. Thus t here are only 3 feasible decisions: /a\}⌊ra⌋ketle{t1,0,3/a\}⌊ra⌋ketri}ht,/a\}⌊ra⌋ketle{t1,1,2/a\}⌊ra⌋ketri}ht, and/a\}⌊ra⌋ketle{t2,0,2/a\}⌊ra⌋ketri}ht.i∈Ithat player ploses to be in the set Λ, and any bat-
tleﬁeld they win the be in the set Ω. As described in § 7,
the true opponent allocation φimust be bounded such that
φi∈[φi..φi].
Note that there are several valid values for φiandφi.
Therefore, our goal is to minimize the size of the enclosed
range by identifying the tightest feasible bounds. This is
achieved by maximizing φiand minimizing φi. Table 1 sum-
marizes the tightest generally applicable bounds derived i n
this section and their associated conditions.
Bound Value Condition
φiπi+δp−1 i∈Ω
Np′−/summationtext
λ∈Λ\{i}[πλ+δp]i∈Λ
φi0 i∈Ω
πi+δp i∈Λ
Table 1: Bounds on feasible opponent allocation φigiven
i∈Iimplemented in experimentation.
Using our earlier deﬁnitions, the following propositions
follow trivially:
Proposition 1. Np=/summationtext
λ∈Λπλ+/summationtext
ω∈Ωπωfor any
playerp.
Proposition 2. φλ=πλ+δpis a valid lower bound for any
battleﬁeld λ∈Λ.
Proposition 3. φω= 0is a valid lower bound for any bat-
tleﬁeldω∈Ω.
Proposition 4. φλ=Np′is a valid upper bound for any
battleﬁeld λ∈Λ.
Proposition 5. φω=πω+δp−1is a valid upper bound
for any battleﬁeld ω∈Ω.
These lead to the following lemma regarding the upper
bound on any opponent allocation φi:
Lemma 1. φi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp]is a valid upper
bound for any battleﬁeld i∈I.
We want to determine when this bound is tighter than our
existing bounds from Propositions 4 and 5. This being the
case, we want to identify on what, if any, conditions it is
tighter. To do so, we propose the following lemmas, which
are used later in Theorem 1:
Lemma 2. Giveni∈Λ, thenφi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp]
(Lemma 1) is always an equivalent or tighter valid upper
bound compared to φi=Np′(Proposition 4).
Lemma 3. Giveni∈Ω, thenφi=Np′−/summationtext
λ∈Λ[πλ+
δp](Lemma 1) is a tighter valid upper bound compared to
φi=πi+δp−1(Proposition 5) if and only if Np′+1</summationtext
λ∈Λ∪{i}[πλ+δp].
These lemmas can be combined to yield a tight upper
boundφi:Theorem 1. φi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp]is a tighter
upper bound compared to the bounds in Propositions 4 and 5
given a battleﬁeld i∈Iif and only if i∈ΛorNp′+1</summationtext
λ∈Λ∪{i}[πλ+δp].
Using a similar approach to Lemma 1 and Theorem 1, we
identify the following lemma regarding the lower bound on
any opponent allocation φi:
Lemma 4. φi=πi·1Λ(i) + (|Λ|−1−1Λ(i))×/parenleftbig/summationtext
λ∈Λ[πλ+δp]−Np′/parenrightbig
−/summationtext
ω∈Ω\{i}[πω+δp−1]is a valid
lower bound given any battleﬁeld i∈I.
1S(i)denotes the indicator function of whether ibelongs
to setS. To identify when this bound is tighter than our ex-
isting bounds from Propositions 2 and 3, we propose the fol-
lowing lemmas, which are used later in Theorem 2:
Lemma 5. Given a battleﬁeld i∈Λ, the bound φi=Np′−
Np+πi−δp+(K−1)(1−δp)is valid and a tighter lower
bound compared to the bound in Proposition 2 if and only if
Λ ={i}andNp+2δp< Np′+(K−1)(1−δp).
Lemma 6. Given a battleﬁeld i∈Ω, the bound φi=Np′−
Np+πi+(K−1)(1−δp)is a valid and tighter lower bound
compared to the bound in Proposition 3 if and only if Λ =∅
andNp< Np′+πi+(K−1)(1−δp).
These lemmas can be combined to identify a tight lower
boundφi:
Theorem 2. φi=Np′−Np+πi−δp·1Λ(i) + (K−
1)(1−δp)is a tighter lower bound compared to the bounds
in Propositions 2 and 3 given a battleﬁeld i∈Iif and only if
Λ∈{∅,{i}}andNp+2δp·1Λ(i)< Np′+πi(1−1Λ(i))+
(K−1)(1−δp).
We note that the special condition identiﬁed in Lemma 3
and used in Theorem 1, and the bound identiﬁed in Theo-
rem 2 were not implemented in our experimentation (§ 9)
and are omitted from Table 1. This was done for the sake of
simplicity and computational efﬁciency.
9 Empirical Analysis of Estimate Quality
We conduct a proof-of-concept evaluation of our estimation
metrics using simulated semi-banditCBgames. In our es-
timate calculations we use the graph-pruning approach pro-
posed in § 7 to narrow down the number of feasible oppo-
nent decisions. Leveraging the bounds identiﬁed in § 8, we
greatly improve the efﬁciency of computing our estimates
and, in theory, their accuracy by ignoring infeasible deci-
sions.
To perform our simulations, we consider a uniformly ran-
dom decision-maker and three online resource allocation al -
gorithms:MARA (Dagan and Koby 2018), CUCB-DRA (Zuo
and Joe-Wong 2021), and EDGE (Vu, Loiseau, and Silva
2019). These algorithms are used purely for the purpose of
generating allocations with varying behaviors. For the pur -
poses of this paper, we exclusively evaluate the accuracy of
our estimators and do not consider the comparative perfor-
mance of these allocation algorithms .
In our experimentation, we denote players A and B such
thatδA= 0 andδB= 1 . For each game we simulateK N ANB|ΠA| |ΠB|
3 10 10 66 66
3 15 10 136 66
3 15 15 136 136
5 15 15 3876 3876
5 20 15 10626 3876
5 20 20 10626 10626
Table 2: Simulated CBgame conﬁgurations.
T= 1000 sequential rounds. The set of simulated game con-
ﬁgurations is described in Table 2. We ensure that NA≥NB
to prevent player B from having a signiﬁcant advantage due
to both winning draws and having an excess of resources.
Every algorithm competes against every other algorithm for
each game conﬁguration, resulting in 16 matchups per con-
ﬁguration, making a total of 96 simulated games. Evaluating
our estimates for both players gives us a total of 192 data
points for each metric.
We use small values of Kdue to the complexity of com-
puting our metrics and their estimates increasing exponen-
tially with respect to the number of battleﬁelds (as describ ed
in § 7). No repetitions were performed as the large number
of rounds ( T= 1000 ) effectively serves the same purpose
in this context.
Our algorithm implementation details are as follows: For
MARA we set the required input parameter c= 2.5to match
the settings used by Dagan and Koby (2018). However,
to convert from continuous allocations to discrete alloca-
tions, we used the procedure described in Algorithm 3.1For
CUCB-DRA , we implement a naive oracle which selects a
decision by directly computing the mean payoff of the algo-
rithm’s current estimates for a large sample of possible dec i-
sions and selecting the decision with the maximum believed
mean payoff. For EDGE we used set the parameter γ= 0.25
(i.e., a25% chance of exploring each round) and the explo-
ration distribution µto be uniform, i.e., µ=U(Πp).
To analyze the quality of our metrics, we analyze the error
of our estimated metrics compared to their true counterpart s
(e.g., Observable Max Payoff versus Max Payoff). To eval-
uate the magnitude of our error, we utilize normalized root
mean square error (NRMSE). That is, the RMSE divided by
the of the mean true value:
NRMSE:=1
mean(y)/radicalbigg/summationtext
t(y′
t−yt)2
N. (17)
To evaluate the deviation of our errors, we utilize relative
residual standard deviation (RRSD). That is, the standard
deviation of residuals divided by the mean of the true value:
RRSD:=1
mean(y)/radicalbigg/summationtext
t((y′
t−yt)−mean(y′−y))2
N,
(18)
whereytis the true metric (i.e., Max Payoff or Expected
Payoff) for round tandy′
tis the estimated metric (i.e., Ob-
servable Max Payoff, Supremum Payoff, or Expected Pay-
off).We normalize by the mean true metric to compensate for
artiﬁcial increases in the magnitude of error due to a larger
number of battleﬁelds. This is because the number of possi-
ble decisions grows exponentially with the number of bat-
tleﬁelds, leading to a larger range of possible values and
payoffs, but also greater magnitude of error and deviation
in our estimates. The exact results of these simulations are
provided in Tables S3 to S14.1
Observable Max Payoff
Across all matchups and game conﬁgurations, RMSE and
RRSD of Observable Max Payoff were effectively zero. The
maximum RMSE and RRSD observed throughout all simu-
lations were 0.01 for both measures. Notably, these values
were only observed for games with K= 5 battleﬁelds (Ta-
bles S9b and S12b). This corroborates our hypothesis that
error should scale with the number of battleﬁelds. These re-
sults demonstrate that Observable Max Payoff is highly ef-
fective at estimating the true value of Max Payoff.
Supremum Payoff
For all but two game conﬁgurations, every matchup pro-
duced RMSE and RRSD of effectively zero for Supremum
Payoff. However, player A observed notable errors in the
game with conﬁguration K= 5,NA= 15 , andNB= 15
(Table S9c), while player B observed similar behavior with
conﬁguration K= 5 ,NA= 20 , andNB= 15 (Ta-
ble S12c). This behavior can be explained due by the in-
herently pessimistic nature of Supremum Payoff discussed
in § 5. Speciﬁcally, it becomes increasingly likely that the re
exists opponent decisions that would induce relatively sma ll
Max Payoff values as the number of feasible opponent deci-
sions increases (as it does exponentially with respect to K,
as discussed in § 7). Even so, the interpretation of Supremum
Payoff as the worst-case Max Payoff and the fact that the ob-
served errors are relatively small continues to highlight t he
usefulness of Supremum Payoff.
Observable Expected Payoff
Across nearly all matchups and game conﬁgurations, the ob-
served NRMSE and RRSD of Observable Expected Payoff
compared to true Expected Payoff are very low. NRMSE
was below 0.20 in 186 out of the 192 data points, and
163 were below 0.15. Notably, all of the data points with
NRMSE greater than 0.20 occurred in matchups with the
CUCB-DRA algorithm. The maximum observed NRMSE
was 0.259. The fact that CUCB-DRA is highly non-uniform
in the decision space suggests that the larger errors are
mostly due to the UDA. Even so, the associated error is rela-
tively small, suggesting the UDA’s impact to be minor. Addi-
tionally, in 183 out of 192 data points RRSD was below 0.15,
and 156 were below 0.10. The maximum observed RRSD
was 0.170. This indicates that Observable Expected Payoff
is a good proxy for the true value of Expected Payoff.
10 Conclusion
In this paper, we developed a general deﬁnition for payoff in
the context of mutually adversarial games, as well as a num-
ber of useful performance evaluation metrics and means forapproximating them which utilize general payoff. Under the
context of theCBgame, we proposed an efﬁcient approach
for identifying feasible opponent decisions from observed
feedback to improve the accuracy of our payoff metric esti-
mates. To that end, the existence and usability of a number
of bounds on opponent actions based on semi-bandit feed-
back for theCBgame are proven for use with the decision
graph pruning process.
References
Audibert, J.-Y .; Bubeck, S.; and Lugosi, G. 2014. Regret in
Online Combinatorial Optimization. Mathematics of Oper-
ations Research , 39(1): 31–45.
Auer, P.; Cesa-Bianchi, N.; Freund, Y .; and Schapire, R. E.
2012. The Nonstochastic Multiarmed Bandit Problem.
SIAM Journal on Computing .
Cesa-Bianchi, N.; and Lugosi, G. 2012. Combinatorial Ban-
dits. Journal of Computer and System Sciences , 78(5):
1404–1422.
Dagan, Y .; and Koby, C. 2018. A Better Resource Allocation
Algorithm with Semi-Bandit Feedback. In Proceedings of
Algorithmic Learning Theory , 268–320. PMLR.
Koc´ ak, T.; Neu, G.; Valko, M.; and Munos, R. 2014. Efﬁ-
cient Learning by Implicit Exploration in Bandit Problems
with Side Observations. In Advances in Neural Information
Processing Systems , volume 27. Curran Associates, Inc.
Roberson, B. 2006. The Colonel Blotto Game. Economic
Theory , 29(1): 1–24.
Schwartz, G.; Loiseau, P.; and Sastry, S. S. 2014. The Het-
erogeneous Colonel Blotto Game. In 2014 7th International
Conference on NETwork Games, COntrol and OPtimization
(NetGCoop) , 232–238.
Vu, D. Q. 2020. Models and Solutions of Strategic Resource
Allocation Problems: Approximate Equilibrium and Online
Learning in Blotto Games . Ph.D. thesis, Sorbonne Univer-
sites, UPMC University of Paris 6.
Vu, D. Q.; Loiseau, P.; and Silva, A. 2019. Combinatorial
Bandits for Sequential Learning in Colonel Blotto Games.
In2019 IEEE 58th Conference on Decision and Control
(CDC) , 867–872.
Vu, D. Q.; Loiseau, P.; Silva, A.; and Tran-Thanh, L. 2020.
Path Planning Problems with Side Observations—When
Colonels Play Hide-and-Seek. Proceedings of the AAAI
Conference on Artiﬁcial Intelligence , 34(02): 2252–2259.
Zuo, J.; and Joe-Wong, C. 2021. Combinatorial Multi-armed
Bandits for Resource Allocation. In 2021 55th Annual Con-
ference on Information Sciences and Systems (CISS) , 1–4.Supplemental Materials: General Performance Evaluation f or Competitive Resource Allocation Games via
Unseen Payoff Estimation
S11CBMax Payoff Algorithm
Calculating the Max Payoff for a speciﬁc decision φis trivial for theCBgame, as described in Algorithm 1. The optimal
strategy is the greedy algorithm: Starting from the battleﬁ eld in which the opponent allocated the least resources, all ocate the
minimum number of resources required to win; repeat this pro cess until the player does not have enough resources to overc ome
the opponent. Note that if the allocations are already sorte d, this process can be done in linear time O(K). However, if sorting
is required, the process becomes limited by the time complex ity of the sorting algorithm (typically O(KlogK)).
Algorithm 1: Max Payoff computation
Input: Opponent decision φ, player resources N, playerp
Output: The maximum achievable payoff
1:L←0
2:forφi∈φin ascending order of φido
3: ifN > φ i−δpthen
4:L←L+1
5:N←N−φi−δp+1
6: else
7: returnL
8: end if
9:end for
10:returnL
S12 Dead-End Pruning Algorithm
ForKbattleﬁelds and Nresources available to the player associated with the graph , Algorithm 2 can be applied with complexity
O(K·N)provided that removing all edges entering a particular vert ex from the set of edges can be done in constant time. If
not, then the algorithm has worst-case complexity O(E), which is equivalent to O(K·N2).
Algorithm 2: Decision graph dead-end pruning
Input: Decision graph G, battleﬁelds K, resources N
Output: Decision graph with dead-ends removed
1:V←VERTICES (G)
2:E←EDGES(G)
3:fori←K−1to0do
4: forn←Nto0do
5: ifdeg+(vi,n) = 0 then
6:V←V\ vi,n
7:E←E\INEDGES(vi,n)
8: end if
9: end for
10:end for
11:returnBUILDGRAPH(V,E)
S13MARA Discretization
First, Algorithm 3 normalizes and scales the continuous all ocation vector produced by MARA with respect to the available
resources N(theNORMALIZE function). Next, the ﬂoor function is applied to each elemen t of the scaled allocation vector
to get the integer portion of the allocations. The non-integ er remainder is then normalized. If the allocated resources do not
sum up to N, the normalized remainders are treated as probabilities fr om which to sample allocations without replacement
and increment by 1 until all resources have been used. This tr eats the magnitude of the remainder as the proportional desi re to
allocate more resources to a particular battleﬁeld. The pro babilistic allocation of remaining resources is based on th e assumption
that, on average, the ﬁnal discrete allocations will conver ge to the value of the continuous allocation times N.Algorithm 3: Discretize continuous allocations
Input: Continuous allocations vector X, resources N
Output: The discretized allocation vector
1:FunctionDISCRETIZE (X,N ) :
2:X←N·NORMALIZE (X)
3:X′←⌊X⌋
4:P←NORMALIZE (Xmod 1)
5: if/summationtextX′> N then
6: SampleN−/summationtextX′indicesIfromX′with probability distribution Pwithout replacement
7: fori∈I:
8: X′
i←X′
i+1
9: end for
10: end if
11: returnX′
12:endDISCRETIZE
13:FunctionNORMALIZE (Y) :
14: returnY//summationtextY
15:endNORMALIZE
S14 Computation Hardware/Software
Utilized compute nodes ran Ubuntu 20.04 LTS with AMD EPYC 754 3 and Intel Xeon Gold 6248 CPUs, and 64 GB of random
access memory.
S15 Proofs
Proposition 1. Np=/summationtext
λ∈Λπλ+/summationtext
ω∈Ωπωfor any player p.
Proposition 2. φλ=πλ+δpis a valid lower bound for any battleﬁeld λ∈Λ.
Proposition 3. φω= 0is a valid lower bound for any battleﬁeld ω∈Ω.
Proposition 4. φλ=Np′is a valid upper bound for any battleﬁeld λ∈Λ.
Proposition 5. φω=πω+δp−1is a valid upper bound for any battleﬁeld ω∈Ω.
Proposition 6. ΩandΛare a partition of I(i.e.,Ω∪Λ =IandΩ∩Λ =∅).
This leads to the following proofs regarding the upper bound on any opponent allocation φi:
Lemma 1. φi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp]is a valid upper bound for any battleﬁeld i∈I.
Proof. Consider that we can pull out φifrom/summationtext
λ∈Λπλand/summationtext
ω∈Ωφωin Proposition 1, as φimust be present in either ΩorΛ,
but not both (Proposition 6). This gives us Np′=φi+/summationtext
λ∈Λ\{i}φλ+/summationtext
ω∈Ω\{i}φω. If we move φito stand alone on the LHS,
we getφi=Np′−/summationtext
λ∈Λ\{i}φλ−/summationtext
ω∈Ω\{i}φω. Notice that we may ﬁnd a valid upper bound on φiby ﬁnding a minimal
upper bound (i.e., supremum) on this equation, which we can d o by using our existing lower bounds from Propositions 2 and 3 .
That is,
φi= sup
Np′−/summationdisplay
λ∈Λ\{i}φλ−/summationdisplay
ω∈Ω\{i}φω

=Np′−/summationdisplay
λ∈Λ\{i}φλ−/summationdisplay
ω∈Ω\{i}φω
=Np′−/summationdisplay
λ∈Λ\{i}[πλ+δp]−/summationdisplay
ω∈Ω\{i}0
=Np′−/summationdisplay
λ∈Λ\{i}[πλ+δp].(19)
Thusφi≤φi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp].We can also verify this via contradiction: Consider the inve rse case where φi> Np′−/summationtext
λ∈Λ\{i}[πλ+δp]. This can be
adjusted as follows:
φi> Np′−/summationdisplay
λ∈Λ\{i}[πλ+δp]
⇐⇒φi+/summationdisplay
λ∈Λ\{i}[πλ+δp]> Np′.(20)
Recall that πλ+δp≤φλfor alli∈Λ. Thus,
φi+/summationdisplay
λ∈Λ\{i}φi≥φi+/summationdisplay
λ∈Λ\{i}[πλ+δp]> Np′
=⇒φi+/summationdisplay
λ∈Λ\{i}φi> Np′
⇐⇒φi+/summationdisplay
λ∈Λ\{i}φi>/summationdisplay
λ∈Λφλ+/summationdisplay
ω∈Ωφω (by Proposition 1 )
⇐⇒φi+/summationdisplay
λ∈Λ\{i}φi> φi+/summationdisplay
λ∈Λ\{i}φλ+/summationdisplay
ω∈Ω\{i}φω
⇐⇒ 0>/summationdisplay
ω∈Ω\{i}φω.(21)
However, φi≥0for alli∈Iby deﬁnition, therefore 0≤/summationtext
ω∈Ω\{i}φω. Thus0/\e}atio\slash>/summationtext
ω∈Ω\{i}φω. This is a contradiction,
provingφi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp]is a valid upper bound.
Note that we want to identify when this bound is tighter than o ur existing bounds from Propositions 3 and 4. This being the
case, we attempt to identify on what, if any, conditions it is tighter. We start by focusing on the case where i∈Λ, which will
later be used in Theorem 2:
Lemma 2. Giveni∈Λ, thenφi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp](Lemma 1) is always an equivalent or tighter valid upper boun d
compared to φi=Np′(Proposition 4).
Proof. We wish to show that Np′−/summationtext
λ∈Λ\{i}[πλ+δp]≤Np′. This simpliﬁes to 0≤/summationtext
λ∈Λ\{i}[πλ+δp]. Recall that
δp∈{0,1}andπi≥0by deﬁnition. Therefore we can always ensure that 0≤/summationtext
λ∈Λ\{i}[πλ+δp].
We now focus on the case where i∈Ω, which will also later be used in Theorem 2:
Lemma 3. Giveni∈Ω, thenφi=Np′−/summationtext
λ∈Λ[πλ+δp](Lemma 1) is a tighter valid upper bound compared to φi=πi+δp−1
(Proposition 5) if and only if Np′+1</summationtext
λ∈Λ∪{i}[πλ+δp].
Proof. Notice that i∈Ω⇐⇒i/\e}atio\slash∈Λ(Proposition 6), therefore φi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp] =Np′−/summationtext
λ∈Λ[πλ+δp]
(Lemma 1). In order to validate that this is a tighter valid up per bound compared to φi=πi+δp−1(Proposition 5), we need
the former to be less than the latter. That is,
Np′−/summationdisplay
λ∈Λ[πλ+δp]< πi+δp−1
⇐⇒ Np′+1< πi+δp+/summationdisplay
λ∈Λ[πλ+δp]
⇐⇒ Np′+1</summationdisplay
λ∈Λ∪{i}[πλ+δp].(22)
Notably, we can easily reconcile the upper bounds from Lemma s 2 and 3:
Theorem 1. φi=Np′−/summationtext
λ∈Λ\{i}[πλ+δp]is a tighter upper bound compared to the bounds in Propositio ns 4 and 5 given a
battleﬁeld i∈Iif and only if i∈ΛorNp′+1</summationtext
λ∈Λ∪{i}[πλ+δp].Proof. It follows trivially from combining Lemmas 2 and 3.
Notice that we may use a similar approach to Lemma 1 and Theore m 2 to ﬁnd potential lower bounds as well:
Lemma 4. φi=πi·1Λ(i)+(|Λ|−1−1Λ(i))×/parenleftbig/summationtext
λ∈Λ[πλ+δp]−Np′/parenrightbig
−/summationtext
ω∈Ω\{i}[πω+δp−1]is a valid lower bound
given any battleﬁeld i∈I.
Proof. Consider that we can pull out φifrom/summationtext
λ∈Λπλand/summationtext
ω∈Ωφωin Proposition 1, as φimust be present in either Ωor
Λ, but not both (Proposition 6). This gives us Np′=φi+/summationtext
λ∈Λ\{i}φλ+/summationtext
ω∈Ω\{i}φω. If we move φito stand alone on
the LHS, we get φi=Np′−/summationtext
λ∈Λ\{i}φλ−/summationtext
ω∈Ω\{i}φω. Notice that we may ﬁnd a valid lower bound on φiby ﬁnding
a maximal lower bound (i.e., inﬁmum) this equation, which we can do by using our existing upper bounds from Lemma 1
and Proposition 5. That is,
φi= inf
Np′−/summationdisplay
λ∈Λ\{i}φλ−/summationdisplay
ω∈Ω\{i}φω

=Np′−/summationdisplay
λ∈Λ\{i}φλ−/summationdisplay
ω∈Ω\{i}φω
=Np′−/summationdisplay
j∈Λ\{i}
Np′−/summationdisplay
λ∈Λ\{j}[πλ+δp]
−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=Np′−Np′(|Λ|−1Λ(i))+/summationdisplay
j∈Λ\{i}/summationdisplay
λ∈Λ\{j}[πλ+δp]−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=−Np′(|Λ|−1−1Λ(i))+(|Λ|−1−1Λ(i))/summationdisplay
λ∈Λ[πλ+δp]+πi·1Λ(i)−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=πi·1Λ(i)+(|Λ|−1−1Λ(i))/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω\{i}[πω+δp−1].(23)
Thusφi=πi·1Λ(i) +(|Λ|−1−1Λ(i))/parenleftbig/summationtext
λ∈Λ[πλ+δp]−Np′/parenrightbig
−/summationtext
ω∈Ω\{i}[πω+δp−1]is a valid lower bound for
anyi∈I.
The following lemma will be used in Lemmas 5 and 6:
Lemma 7. πω+δp−1≥0for any battleﬁeld ω∈Ω.
Proof by contradiction. Supposeπω+δp−1<0. This gives two cases:
Case 1: Letδp= 1(i.e.,pwins draws), thus πω<0. However, πi≥0for alli∈Iby deﬁnition. This is a contradiction
Case 2: Letδp= 0 (i.e.,ploses draws), thus πω<1. Because πi∈N0for alli∈Iby deﬁnition, that means πω= 0.
However, if pwon battleﬁeld w, thenπω+δ > φω, orφω< πω= 0, thusφω<0. However, φi∈N0for alli∈Iby
deﬁnition. This is a contradiction.
Because both cases fail, πω+δp−1/\e}atio\slash<0, therefore πω+δp−1≥0for any battleﬁeld ω∈Ω.
Given the bound identiﬁed in Lemma 1, it is unintuitive wheth er this bound is ever tighter than our existing bounds from
Propositions 2 and 3. This being the case, we attempt to ident ify on what, if any, conditions it is tighter. We start by focu sing
on the case where i∈Λ, which will later be used in Theorem 1:
Lemma 5. Given a battleﬁeld i∈Λ, the bound φi=Np′−Np+πi−δp+(K−1)(1−δp)is valid and a tighter lower bound
compared to the bound in Proposition 2 if and only if Λ ={i}andNp+2δp< Np′+(K−1)(1−δp).Proof. Supposei∈Λ, we want to compare the bound identiﬁed in Lemma 4 against φi=πi+δp(Proposition 2). Thus we
wish to verify
i∈Λ∧πi+δp< πi·1Λ(i)+(|Λ|−1−1Λ(i))/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=⇒ πi+δp< Np′+πi+(|Λ|−2)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω[πω+δp−1]
⇐⇒ δp< Np′+(|Λ|−2)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω[πω+δp−1].(24)
Notice that Lemma 7 implies that −/summationtext
ω∈Ω[πω+δp−1]≤0. Additionally, consider that we can expand Np′based on
Proposition 1:
δp< Np′+(|Λ|−2)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω[πω+δp−1]
=⇒δp< Np′+(|Λ|−2)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
(by Lemma 7 )
⇐⇒δp<(|Λ|−2)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−/summationdisplay
λ∈Λφλ−/summationdisplay
ω∈Ωφω/parenrightBigg
⇐⇒δp<(|Λ|−2)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp−φλ]−/summationdisplay
ω∈Ωφω/parenrightBigg
.(25)
Notice that πi+δp≤φifor alli∈Λby deﬁnition, therefore/summationtext
λ∈Λ[πλ+δp−φλ]≤0. Additionally, φi≥0for alli∈I
by deﬁnition, therefore −/summationtext
ω∈Ωφω≤0. Thus/summationtext
λ∈Λ[πλ+δp−φλ]−/summationtext
ω∈Ωφλ≤0. Therefore because δp∈{0,1}, the
inequality can only hold if |Λ|−2<0, which, given i∈Λis only possible when |Λ|= 1(i.e.,Λ ={i}).
In the circumstance where Λ ={i}, we can simplify Lemma 4:
Λ ={i}∧φi=πi·1Λ(i)+(|Λ|−1−1Λ(i))/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=⇒ φi=πi−(πi+δp−Np′)−/summationdisplay
ω∈Ω[πω+δp−1]
⇐⇒ =Np′−δp+|Ω|(1−δp)−/summationdisplay
ω∈Ωπω
⇐⇒ =Np′−δp+|Ω|(1−δp)−(Np−πi)
⇐⇒ =Np′−Np+πi−δp+(K−1)(1−δp) (Ω = I\{i}).(26)
We can also simplify the inequality for verifying this bound ’s usefulness compared against φi=πi+δp(Proposition 2):
πi+δp< Np′−Np+πi−δp+(K−1)(1−δp)
⇐⇒Np+2δp< Np′+(K−1)(1−δp)(27)
Therefore given i∈Λ, the bound φi=Np′−Np+πi−δp+ (K−1)(1−δp)is useful if and only if Λ ={i}and
Np+2δp< Np′+(K−1)(1−δp).
We now focus on the case where i∈Ω, which will later be used in Theorem 1:
Lemma 6. Given a battleﬁeld i∈Ω, the bound φi=Np′−Np+πi+(K−1)(1−δp)is a valid and tighter lower bound
compared to the bound in Proposition 3 if and only if Λ =∅andNp< Np′+πi+(K−1)(1−δp).
Proof. Supposei∈Ω, we want to compare the bound identiﬁed in Lemma 4 against φi= 0(Proposition 3). Thus we wish toverify
i∈Ω∧0< πi·1Λ(i)+(|Λ|−1−1Λ(i))/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=⇒ 0<(|Λ|−1)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=⇒ 0<(|Λ|−1)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
(by Lemma 7 ).(28)
Notice that we can expand Np′based on Proposition 1:
0<(|Λ|−1)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
⇐⇒0<(|Λ|−1)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−/summationdisplay
λ∈Λφλ−/summationdisplay
ω∈Ωφi/parenrightBigg
⇐⇒0<(|Λ|−1)/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp−φλ]−/summationdisplay
ω∈Ωφi/parenrightBigg
.(29)
Recall that πi+δp≤φifor alli∈Λby deﬁnition, therefore/summationtext
λ∈Λ[πλ+δp−φλ]≤0. Additionally, φi≥0for alli∈I
by deﬁnition, therefore −/summationtext
ω∈Ωφi≤0. Therefore the inequality can only hold if |Λ|−1<0, which is only possible when
|Λ|= 0, in which case Λ =∅.
In the circumstance where i∈ΩandΛ =∅, we can simplify Lemma 4:
i∈Ω∧Λ =∅
∧φi=πi·1Λ(i)+(|Λ|−1−1Λ(i))/parenleftBigg/summationdisplay
λ∈Λ[πλ+δp]−Np′/parenrightBigg
−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
=⇒φi=Np′−/summationdisplay
ω∈Ω\{i}[πω+δp−1]
⇐⇒ =Np′+(|Ω|−1)(1−δp)−/summationdisplay
ω∈Ω\{i}πω
⇐⇒ =Np′+(|Ω|−1)(1−δp)−/parenleftBigg/summationdisplay
ω∈Ωπω−πi/parenrightBigg
⇐⇒ =Np′+(|Ω|−1)(1−δp)−Np+πi
⇐⇒ =Np′−Np+πi+(K−1)(1−δp) (Ω = I).(30)
We can also simplify the inequality for verifying this bound ’s usefulness compared against φi= 0(Proposition 3):
0< Np′−Np+πi+(K−1)(1−δp)
⇐⇒Np< Np′+πi+(K−1)(1−δp)(31)
Therefore given i∈Ω, the bound φi=Np′−Np+πi+ (K−1)(1−δp)is useful if and only if Λ =∅andNp<
Np′+πi+(K−1)(1−δp).
Notably, we can easily reconcile the lower bounds from Lemma s 5 and 6:
Theorem 2. φi=Np′−Np+πi−δp·1Λ(i)+(K−1)(1−δp)is a tighter lower bound compared to the bounds in Propositio ns 2
and 3 given a battleﬁeld i∈Iif and only if Λ∈{∅,{i}}andNp+2δp·1Λ(i)< Np′+πi(1−1Λ(i))+(K−1)(1−δp).
Proof. It follows trivially from combining Lemmas 5 and 6.S16 Empirical Results
The tables in this section focus on the error with respect to e ach payoff estimation metric. The sub-tables show the follo wing
metrics computer over the course of the game: (a) The Normali zed Root Mean-Squared Error (NRMSE) ±the Relative Residual
Standard Deviation (RRSD) between Observable Expected Pay off and True Expected Payoff; (b) the NRMSE ±RRSD between
Observable Max Payoff and True Max Payoff over the course of t he game; (c) the NMRSE ±RRSD between Supremum Payoff
and True Payoff. The values presented are observed by the pla yer designated by the rows against the player designated by t he
columns.(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.027±.019.114±.091.085±.085.088±.088
CUCB-DRA .163±.078.096±.093.085±.085.086±.086
EDGE.127±.068.107±.094.081±.081.080±.080
Random .119±.076.107±.092.082±.082.082±.082
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S3: Empirical results focusing on player A (rows) ver-
sus player B (columns) for games with T= 1000 ,K= 3,
NA= 10 , andNB= 10 .
(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.016±.016.074±.069.058±.058.059±.059
CUCB-DRA .133±.057.088±.079.063±.063.061±.061
EDGE.130±.056.089±.073.067±.067.064±.064
Random .123±.060.083±.073.063±.063.066±.066
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S4: Empirical results focusing on player B (rows) ver-
sus player A (columns) for games with T= 1000 ,K= 3,
NA= 10 , andNB= 10 .(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.014±.011.038±.033.033±.033.030±.030
CUCB-DRA .076±.028.040±.038.032±.032.032±.032
EDGE.072±.029.040±.037.030±.030.031±.031
Random .072±.031.037±.037.030±.030.030±.030
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S5: Empirical results focusing on player A (rows) ver-
sus player B (columns) for games with T= 1000 ,K= 3,
NA= 15 , andNB= 10 .
(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.139±.055.177±.170.156±.156.155±.155
CUCB-DRA .112±.106.180±.163.158±.158.152±.152
EDGE.104±.103.187±.146.158±.158.156±.156
Random .101±.100.184±.148.149±.149.155±.155
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S6: Empirical results focusing on player B (rows) ver-
sus player A (columns) for games with T= 1000 ,K= 3,
NA= 15 , andNB= 10 .(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.029±.014.112±.103.083±.083.082±.082
CUCB-DRA .175±.082.102±.097.081±.081.080±.080
EDGE.110±.061.113±.093.079±.079.077±.077
Random .121±.064.102±.088.076±.076.079±.079
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S7: Empirical results focusing on player A (rows) ver-
sus player B (columns) for games with T= 1000 ,K= 3,
NA= 15 , andNB= 15 .
(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.015±.010.084±.080.064±.064.066±.066
CUCB-DRA .135±.067.096±.087.062±.062.065±.065
EDGE.128±.060.093±.081.070±.070.066±.066
Random .119±.059.093±.082.066±.066.067±.067
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S8: Empirical results focusing on player B (rows) ver-
sus player A (columns) for games with T= 1000 ,K= 3,
NA= 15 , andNB= 15 .(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.047±.029.211±.108.100±.100.102±.102
CUCB-DRA .255±.102.090±.088.096±.096.095±.096
EDGE.112±.058.132±.120.087±.087.089±.089
Random .124±.058.151±.117.088±.088.091±.091
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.008±.008.000±.000.000±.000.000±.000
CUCB-DRA .008±.008.008±.008.000±.000.000±.000
EDGE.008±.008.000±.000.000±.000.011±.011
Random .008±.008.000±.000.000±.000.008±.008
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.186±.124.182±.125.204±.118.209±.114
CUCB-DRA .174±.125.213±.112.210±.114.209±.115
EDGE.082±.078.119±.105.135±.114.139±.115
Random .076±.072.109±.098.127±.110.127±.109
Table S9: Empirical results focusing on player A (rows) ver-
sus player B (columns) for games with T= 1000 ,K= 5,
NA= 15 , andNB= 15 .
(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.041±.018.112±.082.076±.076.072±.072
CUCB-DRA .039±.039.163±.084.075±.075.078±.078
EDGE.115±.052.168±.080.079±.079.077±.077
Random .111±.051.169±.082.076±.076.081±.081
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S10: Empirical results focusing on player B (rows)
versus player A (columns) for games with T= 1000 ,K=
5,NA= 15 , andNB= 15 .(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.028±.016.139±.084.059±.059.055±.055
CUCB-DRA .198±.073.060±.058.060±.060.061±.061
EDGE.131±.056.095±.086.056±.056.057±.057
Random .126±.050.114±.081.054±.054.059±.059
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S11: Empirical results focusing on player A (rows)
versus player B (columns) for games with T= 1000 ,K=
5,NA= 20 , andNB= 15 .
(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.073±.043.136±.106.116±.115.118±.118
CUCB-DRA .058±.058.196±.091.113±.113.113±.113
EDGE.134±.069.203±.093.117±.117.119±.119
Random .135±.069.205±.092.114±.114.121±.121
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.008±.008.000±.000.000±.000.000±.000
CUCB-DRA .008±.008.000±.000.000±.000.000±.000
EDGE.008±.008.000±.000.000±.000.000±.000
Random .008±.008.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.221±.104.016±.016.138±.115.130±.111
CUCB-DRA .092±.086.031±.030.111±.099.129±.111
EDGE.033±.032.032±.031.103±.094.105±.095
Random .037±.037.032±.031.100±.092.110±.099
Table S12: Empirical results focusing on player B (rows)
versus player A (columns) for games with T= 1000 ,K=
5,NA= 20 , andNB= 15 .(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.084±.020.226±.112.100±.100.097±.097
CUCB-DRA .259±.105.093±.088.097±.097.096±.096
EDGE.120±.054.146±.127.091±.091.093±.093
Random .116±.054.175±.103.089±.089.091±.091
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S13: Empirical results focusing on player A (rows)
versus player B (columns) for games with T= 1000 ,K=
5,NA= 20 , andNB= 20 .
(a) Observable Expected Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.034±.017.119±.086.080±.080.075±.075
CUCB-DRA .039±.037.167±.087.079±.079.078±.078
EDGE.122±.054.175±.083.080±.080.082±.082
Random .115±.054.177±.084.080±.080.082±.082
(b) Observable Max Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
(c) Supremum Payoff Normalized Error
MARA CUCB-DRA E DGE Random
MARA.000±.000.000±.000.000±.000.000±.000
CUCB-DRA .000±.000.000±.000.000±.000.000±.000
EDGE.000±.000.000±.000.000±.000.000±.000
Random .000±.000.000±.000.000±.000.000±.000
Table S14: Empirical results focusing on player B (rows)
versus player A (columns) for games with T= 1000 ,K=
5,NA= 20 , andNB= 20 .