IMPROVING THE OUT-OF-DISTRIBUTION GENERALIZATION CAPABILITY OF
LANGUAGE MODELS: COUNTERFACTUALLY-AUGMENTED DATA IS NOT ENOUGH
Caoyun Fan1yWenqing Chen2yJidong Tian1Yitian Li1Hao He1Yaohui Jin1
1MoE Key Lab of ArtiÔ¨Åcial Intelligence, AI Institute, Shanghai Jiao Tong University, China
2School of Software Engineering, Sun Yat-sen University, China
ABSTRACT
Counterfactually-Augmented Data (CAD) has the poten-
tial to improve language models‚Äô Out-Of-Distribution (OOD)
generalization capability, as CAD induces language models
to exploit causal features and exclude spurious correlations.
However, the empirical results of OOD generalization on CAD
are not as efÔ¨Åcient as expected. In this paper, we attribute the
inefÔ¨Åciency to Myopia Phenomenon caused by CAD: lan-
guage models only focus on causal features that are edited
in the augmentation and exclude other non-edited causal fea-
tures. As a result, the potential of CAD is not fully exploited.
Based on the structural properties of CAD, we design two
additional constraints to help language models extract more
complete causal features contained in CAD, thus improving
the OOD generalization capability. We evaluate our method
on two tasks: Sentiment Analysis and Natural Language Infer-
ence, and the experimental results demonstrate that our method
could unlock CAD‚Äôs potential and improve language models‚Äô
OOD generalization capability.
Index Terms ‚ÄîCounterfactually-Augmented Data, Out-
Of-Distribution Generalization, Language Models
1. INTRODUCTION
Despite the remarkable performance of language models
in Natural Language Processing (NLP) [ 1,2], the Out-Of-
Distribution (OOD) generalization capability of language
models is often disappointing [ 3,4]. Many studies [ 5,6] have
pointed out that such limited generalization capability partly
arises from the language models‚Äô exploitation of spurious cor-
relations [ 7,8,9,10] in the dataset. SpeciÔ¨Åcally, the language
models tend to exploit dataset-speciÔ¨Åc correlation bias [ 5,11]
rather than the intrinsic properties of tasks to make predictions
during the training process, while the spurious correlations
can not be generalized to OOD data.
To solve the problem of spurious correlations, a recent
promising direction is Counterfactually-Augmented Data
(CAD) [ 12,13,14]: minimal editing of sentence to Ô¨Çip the
corresponding label Y, where the edited part is considered
*Corresponding author.
‚Ä†These authors contributed equally.
Original:Nolan is an excellentfilm director. !"#$%&%'()Edited:Nolan is a terriblefilm director. !*(+,&%'()(a) An example of counterfactual sentence pairs. We expect the lan-
guage model to exploit the causal features (in gray) and exclude the
possible spurious correlations (e.g., Nolan in the sentence).
‚Ñé!‚Ñé"Œ¶#"$	ùëëùëéùë°ùëé#"$ùëëùëéùë°ùëé!%Œ¶
(b) Original Dataset
‚Ñé!‚Ñé"Œ¶#$%	 (c) CAD
Fig. 1 . The motivation of CAD. Counterfactual augmentation
of texts (Fig. 1(a)) changes the data distribution of the dataset
(from Fig. 1(b) to Fig. 1(c)), which helps the model to
exploit causal features hcand exclude correlated features hr.
to be the intrinsic properties of the task and have a causal
effect on the label (Fig. 1(a)). Unlike the Independent Identi-
cally Distribution (IID) principle of most data augmentation
methods, CAD aims to change the data distribution of the
dataset so that the language models can alleviate reliance on
dataset-speciÔ¨Åc bias and exclude spurious correlations.
Under the ideal conditions assumed by [ 6]1, CAD keeps
correlated features hrin the counterfactual sentence pairs
constant while the causal features hcchange. Therefore, the
classiÔ¨Åer can make predictions based on causal features and
then exclude the interference of spurious correlations as:
(hc;hr) =Y
(h
c;hr) =Y(1)
whereh
candYare the causal features and the label of the
edited sentence, respectively. Intuitively, the classiÔ¨Åer no
longer focuses on hrbecause different labels correspond to
1Under the ideal conditions, each sentence consists of causal features hc
whose joint distribution with labels is invariant, and correlated features hr
whose joint distribution can vary.arXiv:2302.09345v1  [cs.CL]  18 Feb 2023Premise: three children are watching a movie.Hypothesis: Childrenarewatching a film.(!"#$%&'!"#)Editing4: Sixchildrenarewatching a film.(()"#*$+%(#%)")Editing1: Brothersarewatching a film.("!,#*$&)Editing2: Childrenare watchingBatman. ("!,#*$&)Editing3: Childrenarewatching TV.(()"#*$+%(#%)")Fig. 2 . An example of multiple counterfactual augmentation results in Natural Language Inference. Editing different causal
components in the Hypothesis can all serve the purpose of Ô¨Çipping the corresponding label.
the samehr, as shown in Fig. 1(b) & 1(c). However, some
experiments [ 15,16] have demonstrated that CAD is not efÔ¨Å-
cient in improving the generalization capability of language
models, especially in more complex tasks. This is not in line
with our expectations for CAD.
In this work, we attribute the inefÔ¨Åciency of CAD in gener-
alization to the CAD-imposed myopia phenomenon: language
models focus only on causal features edited by counterfactual
augmentation, which means correlated features along with
other non-edited causal features are excluded. However, all
causal features are beneÔ¨Åcial for OOD generalization [ 17].
ToExtract more complete Causal Features and unlock the
potential of CAD for language models, we design the ECF
algorithm: introducing additional constraints in the training
process based on the structural properties of CAD. SpeciÔ¨Åcally,
we extract invariant causal features over both distributions of
CAD and the original dataset by the Invariant Risk Minimiza-
tion [ 18] method (dataset level) and constrain the correlated
feature similarity of counterfactual sentence pairs (sentence
level). Through extensive experiments across multiple lan-
guage models and NLP tasks, we conclude that the proposed
ECF algorithm could help language models to extract more
complete causal features, and then improve the OOD general-
ization capability in multiple NLP tasks.
2. MYOPIA PHENOMENON IN CAD
As mentioned before, the essence of CAD is to change the
data distribution through data augmentation, thereby reduc-
ing the dataset-speciÔ¨Åc bias implied in the data distribution.
Intuitively, by comparing the differences in counterfactual sen-
tence pairs, language models could capture the features that
have a causal effect on the labels. However, the results of coun-
terfactual augmentation are diverse for a particular sentence,
as illustrated in Fig. 2. SpeciÔ¨Åcally, the causal components
and the perturbation types [ 6] (e.g., negation, quantiÔ¨Åer, lex-
ical, delete) that can Ô¨Çip labels are diverse, so the different
counterfactual sentence can be obtained by making a speciÔ¨Åc
perturbation for a particular causal component, while the other
causal components remain unchanged.
Therefore, compared to Eq. 1, a more reasonable assump-
tion is that only part of hcin the counterfactual sentence pairswould change with the counterfactual augmentation as:
(he;hu;hr) =Y
(h
e;hu;hr) =Y(2)
wherehcis distinguished into edited features hethat change
with augmentation and non-edited features huthat do not
change. This assumption is empirically convincing because of
the analysis and experiments in [ 6,12]. Similar to the analysis
of Eq. 1, Eq. 2 gives us an important insight: language
models trained on original data and CAD focus on different
features in the sentence. On the one hand, CAD eliminates the
interference of correlated features; on the other hand, language
models inevitably ignore non-edited causal features. In this
paper, we refer to this as Myopia Phenomenon .
3. PROPOSED METHOD
To solve the myopia phenomenon and extract more complete
causal features, we propose two insights on the structural
properties of CAD at the dataset level and at the sentence level,
and design additional constraints based on these insights, to
further exploit the generalization potential of CAD.
3.1. Dataset-Level Constraint
Insight: the data distribution of the original dataset can alle-
viate the Myopia Phenomenon of CAD.
Due to the change in data distribution, the features that
language models focus on are different: models with CAD
only focus on edited causal features he(Myopia Phenomenon),
while models with the original dataset confuse hcandhr
(but no Myopia Phenomenon). Different data distributions
lead to different problems, which indicates that the original
data distribution carries information that is missing in CAD.
Therefore, there are potential complementary effects of the
original dataset and CAD on causal feature extraction.
Inspired by [ 18], we adopt the Invariant Risk Minimization
(IRM) method to extract more complete causal features in
CAD. The role of IRM is to estimate invariant causal features
from multiple training environments. As mentioned before,
counterfactual augmentation does not follow the IID principle,
which allows us to consider the original dataset and CAD asDataset Method Original CAD SST-2 Amazon Yelp Mean
SeedLSTM 74.3 64.0 64.5 61.7 62.3 62.8
BERT 85.2 85.5 82.8 88.0 85.5 85.4
LSTM 81.0 86.7 65.3 74.0 72.7 70.7
CAD h LSTM+ECF 80.7 84.4 71.4 74.0 77.4 74.3
[12] BERT 88.9 92.3 86.9 90.3 89.8 89.0
BERT+ECF 84.7 89.2 84.9 92.9 92.4 90.1
LSTM 56.1 66.7 61.5 57.6 57.9 59.0
CAD a LSTM+ECF 57.9 67.5 62.4 59.0 58.5 60.0
[14] BERT 55.1 72.1 75.9 84.7 83.1 81.2
BERT+ECF 80.4 63.8 78.9 88.1 86.2 84.4
LSTM 67.8 75.4 59.8 63.3 62.6 61.9
CAD a LSTM+ECF 76.1 79.7 61.7 66.4 63.8 64.0
[13] BERT 87.1 88.0 83.2 88.4 88.9 86.8
BERT+ECF 85.7 77.8 83.6 90.3 89.5 87.8
Table 1 . Accuracy of different language models and datasets in
SA. The best performance is bold .CAD handCAD arepresent
manually annotated CAD and automatically generated CAD,
respectively.
two different training environments Etr=feori;eCADg, and
then adopt the IRM method to fuse the advantages of both
environments. SpeciÔ¨Åcally, to induce the language model M
to learn invariant causal features across environments, the
additional constraint LIRM is designed as:
LIRM =X
e2Etrkr!j!=1:0Re(!M)k2(3)
whereRe()is the risk [ 18] under environment e, and!= 1:0
as a scalar is a Ô¨Åxed ‚Äòdummy‚Äô classiÔ¨Åer. The essence of LIRM
is a gradient norm penalty that measures the optimality of
the ‚Äòdummy‚Äô classiÔ¨Åer in each environment, in order to Ô¨Ånd
invariant causal features that match all environments.
3.2. Sentence-Level Constraint
Insight: the correlated features hrof counterfactual sentence
pairs are not guaranteed to be aligned.
In our assumptions, the correlated features hrof counter-
factual sentence pairs are similar, because the augmentation
operation only affects part of hc. However, this property is not
guaranteed for language models trained directly on CAD, and
this potential dissimilarity gives language models the conve-
nience to utilize hr. Therefore, it is reasonable to design an
explicit constraint on hrfor counterfactual sentence pairs.
However,hrandhcin CAD are hard to decouple in lan-
guage models, so a sensible proxy for hris needed. Noting
thathrhas little effect on the prediction in CAD, based on
this property, we creatively construct the proxy of hrusing the
mechanism of feature classiÔ¨Åer. Most feature classiÔ¨Åers are
fully-connected layers, where each row of the weight matrix
can be interpreted as a label vector hY[19], and the label
probability can be obtained by the dot product of the sentence
vectorhand each label vector hYas:
p(yk) =exp(hk
Yh)PN
i=1exp(hi
Yh)(4)Dataset Method Original CAD MNLI-m MNLI-mm Mean
SeedLSTM 41.8 33.9 35.9 35.0 35.5
BERT 71.5 53.8 53.6 55.1 54.3
Roberta 83.8 67.2 67.4 68.4 67.9
LSTM 39.8 39.0 34.4 35.0 34.7
LSTM+ECF 44.2 37.6 36.2 36.2 36.2
CAD h BERT 79.2 71.2 62.6 64.3 63.5
[12] BERT+ECF 77.0 72.0 64.0 65.5 64.7
Roberta 80.2 75.4 70.5 71.5 71.0
Roberta+ECF 82.5 76.7 72.6 72.8 72.7
Table 2 . Accuracy of different language models and datasets
in NLI. The best performance is bold .
In this way, hcan be decomposed along hY, where the
parallel component hkYdetermines the prediction and the
orthogonal component h?Yhas no effect on the prediction.
The commonality between h?Yandhrmakesh?Yan ideal
proxy forhr. SpeciÔ¨Åcally, for a counterfactual sentence feature
pair(h;h), we designLOCD to penalize their Orthogonal
Component Distance as:
LOCD =kh?Y h
?Yk2(5)
This is a positive feedback process, so even if initially the
classiÔ¨Åer has large estimation errors, it will gradually become
accurate with the help of the prediction loss and LOCD .
3.3. Training Process
Compared to the original prediction loss LP, the proposed
ECF algorithm also combines dataset-level constraint LIRM
and sentence-level constraint LOCD as:
L=LP+LIRM +LOCD (6)
where,are the weighting coefÔ¨Åcients to balance the lan-
guage model‚Äôs In-Distribution predictive power and additional
constraints introduced for OOD Generalization.
4. EXPERIMENTS
4.1. Datasets
We conducted experiments on two tasks: Sentiment Analysis
(SA) and Natural Language Inference (NLI).
Sentiment Analysis The seed dataset in SA was IMDb
[20] dataset. [ 12] collected a subset of IMDb as a seed dataset,
and manually annotated the corresponding counterfactual sen-
tences to construct CAD h, while [ 14,13] utilized WordNet
[21] to automatically generate counterfactual sentences and
constructed CAD a. We evaluated each language model‚Äôs OOD
generalization capability on three OOD datasets: SST-2 [ 22],
Amazon review [23], Yelp review [24].
Natural Language Inference [12] constructed CAD hby
manually editing seed dataset from SNLI [ 25] dataset. Because
the NLI task is more complex, there is little research related
to the automatic generation of counterfactual sentences. WeSST-2 Amazon Yelp Mean
T ask60.062.565.067.570.072.575.077.580.0AccuracyBackbone
\ OCD
\ IRM
ECFFig. 3 . Ablation analysis of two constraints on Sentiment
Analysis.ndenotes the removing operation.
treated MNLI (split into matched and mismatched parts) [ 26]
as our OOD dataset for evaluation.
4.2. Implementation Details
We implemented LSTM [ 27] and pre-trained models BERT [ 1],
Roberta [ 2] as our backbones, and selected the best checkpoint
on the training set for testing. For LSTM, The word embedding
dimension was set to 300, the batch size was set to 32, and
the learning rate of the Adam optimizer to 1e-3. We set =
1.6 and= 0.1. We trained each model for 100 epochs in
SA/NLI task. For pre-trained models, we used the Hugging
Face implementation to Ô¨Ånetune the pre-trained models. The
batch size was set to 8/5 for SA/NLI tasks respectively, and
the learning rate of Adam optimizer to 1e-5. We set = 0.1
and= 0.1. We trained each model for 10 epochs.
4.3. Main Results
Results on SA The results are presented in Table 1, where
the ECF algorithm beat all the compared backbones in terms
of the average accuracy of OOD datasets. SpeciÔ¨Åcally, CAD h
was more effective for language models‚Äô generalization, while
the ECF algorithm improved the average accuracy of LSTM
and BERT on OOD datasets by 3.6% and 1.1%, respectively.
The language models trained on CAD awere relatively weaker
in generalization, and the ECF algorithm also helped LSTM
and BERT improve their average accuracy by 1.0%/2.1% and
3.2%/1.0% on two CAD a, respectively.
Results on NLI The results are presented in Table 2. The
ECF algorithm improved the average accuracy of LSTM on
OOD datasets by 1.5%. The ECF algorithm also helped pre-
trained models, improving the OOD generalization accuracy
by 1.2% on BERT and by 1.7% on Roberta.
4.4. Ablation Study
We investigated the independent impact of each constraint in
our ECF algorithm. We chose BERT as the backbone, and
the results are reported in Fig. 3. When we removed LIRM
andLOCD , the performance decreased signiÔ¨Åcantly. This
LSTM BERT
Method60657075808590AccuracyOrigin
CAD
ECF(a) SA
LSTM BERT Roberta
Method30354045505560657075AccuracyOrigin
CAD
ECF (b) NLI
Fig. 4 . Data efÔ¨Åciency analysis of CAD.
illustrated that the language models trained directly on CAD
did not fully exploit the potential of CAD, and two additional
constraints we proposed further unlocked CAD‚Äôs potential.
4.5. Data EfÔ¨Åciency
Counterfactual augmentation expanded the size of the seed
dataset, which also contributed to OOD generalization. To
demonstrate the validity of CAD and our ECF algorithm for
language models, we compared the generalization capability
of multiple language models trained with the same amount
of unaugmented data, as shown in Fig. 4. The experimental
results illustrated that CAD cannot always outperform the
same amount of unaugmented data, while our ECF algorithm
could steadily improve the generalization capability.
5. RELATED WORK
CAD is an emerging technique in NLP Ô¨Åeld since [ 12], which
aims to help language models extract causal features by chang-
ing data distribution. Some studies [ 15,16] pointed out CAD
inefÔ¨Åciency in terms of empirical experimental results, and
[6] attempted to provide a theoretical explanation for this in-
efÔ¨Åciency. Previous approaches to improving CAD efÔ¨Åciency
fall into two categories: (1) improving the generation quality
[13,14] of counterfactual texts. (2) debiasing for speciÔ¨Åc bias
[17] (e.g., gender, race) in CAD. To the best of my knowledge,
our paper is the Ô¨Årst attempt to improve the efÔ¨Åciency of CAD
by designing additional constraints, which neither change the
dataset nor require additional information, and is the most
general application scenario.
6. CONCLUSION
In this paper, we attributed the inefÔ¨Åciency of CAD to Myopia
Phenomenon caused by counterfactual augmentation opera-
tions, and designed dataset-level and sentence-level constraints
based on the structural properties of CAD to help language
models to extract more complete causal features and then un-
lock the potential of CAD.7. REFERENCES
[1]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova, ‚ÄúBert: Pre-training of deep bidi-
rectional transformers for language understanding,‚Äù in
NAACL , 2019.
[2]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov, ‚ÄúRoberta: A robustly
optimized bert pretraining approach,‚Äù ArXiv , 2019.
[3]Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang,
and Tao Qin, ‚ÄúGeneralizing to unseen domains: A survey
on domain generalization,‚Äù in IJCAI , 2021.
[4]Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang,
Renzhe Xu, Han Yu, and Peng Cui, ‚ÄúTowards out-of-
distribution generalization: A survey,‚Äù ArXiv , 2021.
[5]Damien Teney, Ehsan Abbasnejad, and Anton van den
Hengel, ‚ÄúLearning what makes a difference from coun-
terfactual examples and gradient supervision,‚Äù ECCV ,
2020.
[6]Nitish Joshi and He He, ‚ÄúAn investigation of the
(in)effectiveness of counterfactually augmented data,‚Äù
ACL, 2022.
[7]Tom Michael Mitchell, ‚ÄúThe need for biases in learning
generalizations,‚Äù Department of Computer Science, Lab-
oratory for Computer Science Research, Rutgers Univ. ,
2007.
[8]Antonio Torralba and Alexei A. Efros, ‚ÄúUnbiased look
at dataset bias,‚Äù CVPR , 2011.
[9]R. Thomas McCoy, Ellie Pavlick, and Tal Linzen, ‚ÄúRight
for the wrong reasons: Diagnosing syntactic heuristics
in natural language inference,‚Äù in ACL, 2019.
[10] Zhao Wang and Aron Culotta, ‚ÄúIdentifying spurious
correlations for robust text classiÔ¨Åcation,‚Äù in Findings of
EMNLP , 2020.
[11] Diana F. Spears and Marie desJardins, ‚ÄúEvaluation and
selection of biases in machine learning,‚Äù Machine Learn-
ing, 2004.
[12] Divyansh Kaushik, Eduard H. Hovy, and Zachary Chase
Lipton, ‚ÄúLearning the difference that makes a difference
with counterfactually-augmented data,‚Äù ICLR , 2020.
[13] Zhao Wang and Aron Culotta, ‚ÄúRobustness to spuri-
ous correlations in text classiÔ¨Åcation via automatically
generated counterfactuals,‚Äù in AAAI , 2021.[14] Linyi Yang, Jiazheng Li, P‚Äôadraig Cunningham, Yue
Zhang, Barry Smyth, and Ruihai Dong, ‚ÄúExploring the
efÔ¨Åcacy of automatically generated counterfactuals for
sentiment analysis,‚Äù ACL, 2021.
[15] William Huang, Haokun Liu, and Samuel R. Bowman,
‚ÄúCounterfactually-augmented snli training data does not
yield better generalization than unaugmented data,‚Äù in
INSIGHTS , 2020.
[16] Daniel Khashabi, Tushar Khot, and Ashish Sabharwal,
‚ÄúMore bang for your buck: Natural perturbation for robust
question answering,‚Äù in EMNLP , 2020.
[17] Ananth Balashankar, Xuezhi Wang, Ben Packer, Nithum
Thain, Ed H. Chi, and Alex Beutel, ‚ÄúCan we improve
model robustness through secondary attribute counterfac-
tuals?,‚Äù in EMNLP , 2021.
[18] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski,
‚ÄúThe risks of invariant risk minimization,‚Äù ICLR , 2021.
[19] Cunxiao Du, Zhaozheng Chen, Fuli Feng, Lei Zhu, Tian
Gan, and Liqiang Nie, ‚ÄúExplicit interaction model to-
wards text classiÔ¨Åcation,‚Äù in AAAI , 2019.
[20] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, A. Ng, and Christopher Potts, ‚ÄúLearning word
vectors for sentiment analysis,‚Äù in ACL, 2011.
[21] Christiane D. Fellbaum, ‚ÄúWordnet : an electronic lexical
database,‚Äù Language , 2000.
[22] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, A. Ng, and Christopher Potts,
‚ÄúRecursive deep models for semantic compositionality
over a sentiment treebank,‚Äù in EMNLP , 2013.
[23] Jianmo Ni, Jiacheng Li, and Julian McAuley, ‚ÄúJustifying
recommendations using distantly-labeled reviews and
Ô¨Åne-grained aspects,‚Äù in EMNLP , 2019.
[24] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun,
‚ÄúCharacter-level convolutional networks for text classiÔ¨Å-
cation,‚Äù NIPS , 2015.
[25] Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning, ‚ÄúA large annotated corpus
for learning natural language inference,‚Äù in EMNLP ,
2015.
[26] Adina Williams, Nikita Nangia, and Samuel R. Bow-
man, ‚ÄúA broad-coverage challenge corpus for sentence
understanding through inference,‚Äù in NAACL , 2018.
[27] Sepp Hochreiter and J ¬®urgen Schmidhuber, ‚ÄúLong short-
term memory,‚Äù Neural Computation , 1997.