On the Convergence of No-Regret Learning Dynamics in
Time-Varying Games
Ioannis Anagnostides1, Ioannis Panageas2, Gabriele Farina3, and Tuomas Sandholm4
1,3,4Carnegie Mellon University
2University of California Irvine
4Strategy Robot, Inc.
4Optimized Markets, Inc.
4Strategic Machine, Inc.
{ianagnos,gfarina,sandholm }@cs.cmu.edu , and ipanagea@ics.uci.edu
October 19, 2023
Abstract
Most of the literature on learning in games has focused on the restrictive setting where the
underlying repeated game does not change over time. Much less is known about the convergence
of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize
the convergence of optimistic gradient descent ( OGD)in time-varying games. Our framework
yields sharp convergence bounds for the equilibrium gap of OGDin zero-sum games parame-
terized on natural variation measures of the sequence of games, subsuming known results for
static games. Furthermore, we establish improved second-order variation bounds under strong
convexity-concavity, as long as each game is repeated multiple times. Our results also apply to
time-varying general-sum multi-player games via a bilinear formulation of correlated equilibria,
which has novel implications for meta-learning and for obtaining refined variation-dependent
regret bounds, addressing questions left open in prior papers. Finally, we leverage our framework
to also provide new insights on dynamic regret guarantees in static games.arXiv:2301.11241v3  [cs.LG]  18 Oct 20231 Introduction
Most of the classical results in the literature on learning in games—exemplified by, among others, the
work of Hart and Mas-Colell [HM00], Foster and Vohra [FV97], and Freund and Schapire [FS99]—rest
on the assumption that the underlying repeated game remains invariant. Yet, in many paradigmatic
learning environments that is unrealistic [Duv+22; Zha+22; Car+19; MS21]. One such class is
settings where the underlying game is actually changing, such as routing problems on the inter-
net [MO11], online advertising auctions [LST16], and dynamic mechanism design [Pap+22; DMZ21].
Another such class consists of settings in which many similar games need to be solved [Har+23].
For example, one may want to solve variations of a game for the purpose of sensitivity analysis with
respect to the modeling assumptions used to construct the game model.
Despite the considerable interest in such dynamic multiagent environments, much less is known
about the convergence of no-regret learning algorithms in time-varying games. No-regret dynamics
are natural learning algorithms that have desirable convergence properties in static settings. Also,
the state-of-the-art algorithms for finding minimax equilibria in two-player zero-sum games are based
on advanced forms of no-regret dynamics [FKS21; BS19a]. Indeed, all the superhuman milestones
in poker have used them in the equilibrium-finding module of their architectures [Bow+15; BS18;
BS19b].
In this paper, we seek to fill this knowledge gap by understanding properties of no-regret dy-
namics in time-varying games. In particular, we primarily investigate the convergence of optimistic
gradient descent ( OGD)[Chi+12; RS13] in time-varying games. Unlike traditional no-regret learning
algorithms, such as (online) gradient descent, OGDhas been recently shown to exhibit last-iterate
convergence in static (two-player) zero-sum games [Das+18; GPD20; COZ22; GTG22]. For the more
challenging scenario where the underlying game can vary in every round, a fundamental question
arises: Under what conditions on the sequence of games does OGDapproximate (with high probability)
the sequence of Nash equilibria?
1.1 Our results
In this paper, we develop a framework that enables us to characterize the convergence of OGD, and
generalizations thereof, in a number of fundamental time-varying multiagent settings.
Bilinear saddle-point problems First, building on the work of Zhang et al. [Zha+22], we
identify natural variation measures on the sequence of games whose sublinear growth guarantees
that almost all iterates of OGDunder a constant learning rate are approximate Nash equilibria in
time-varying (two-player) zero-sum games (Corollary 3.4). More precisely, in Theorem 3.3 we derive
a sharp non-asymptotic characterization of the equilibrium gap of OGDas a function of the minimal
first-order variation of approximate Nash equilibria and the second-order variation of the payoff
matrices. We stress that our variation measure that depends on the deviation of approximate Nash
equilibria of the games can be arbitrarily smaller than the one based on (even the least varying)
sequence of exact Nash equilibria (Proposition 3.2), thereby significantly sharpening the measure
considered by Zhang et al. [Zha+22]. It is also a compelling property, in light of the multiplicity
of Nash equilibria, that the variation of the Nash equilibria is measured in terms of the most
favorable— i.e., one that minimizes the variation—such sequence.
From a technical standpoint, our analysis revolves around a connection between the convergence of
OGDin time-varying games and dynamic regret . In particular, the first key observation is that dynamic
1regret cannot be too negative under any sequence of approximate Nash equilibria (Property 3.1); this
was first observed by Zhang et al. [Zha+22] under a sequence of exact Nash equilibria. We also general-
ize this property by connecting it to the admission of a minimax theorem (Property A.4), as well as the
so-called MVI property in the more general context of time-varying variational inequalities (VIs); this
will enable us to extend our scope to certain multi-player games such as polymatrix zero-sum [Cai+16]
(Theorem A.16). By combining Property 3.1 with a dynamic RVU bound [Zha+22], we obtain a
variation-dependent bound for the second-order path length of OGDunder a constant learning rate
in time-varying zero-sum games. In turn, this leads to our first main result, Theorem 3.3, discussed
above. In the special case of static games, our theorem reduces to a tight T−1/2rate. We also instan-
tiate our results to the special setting of meta-learning , where each game is repeated multiple times.
Strongly convex-concave games Moreover, for strongly convex-concave time-varying games,
we obtain a refined second-order variation bound on the sequence of Nash equilibria, as long as each
game is repeated multiple times (Theorem 3.5); this is inspired by an improved second-order bound
for dynamic regret under analogous conditions due to Zhang et al. [Zha+17]. As a byproduct of
our techniques, we point out that anyno-regret learners are approaching a Nash equilibrium under
strong convexity-concavity (Proposition 3.6; cf. [Wan+22]). Those results apply even in non-strongly
convex-concave settings by suitably trading off the magnitude of a regularizer that makes the game
strongly convex-concave. This offers significant gains in the meta-learning setting as well.
General-sum multi-player games Next, we extend our results to time-varying general-sum
multi-player games by expressing correlated equilibria via a bilinear saddle-point problem (BSPP);
while this formulation goes back to the early work of Hart and Schmeidler [HS89], it is rarely em-
ployed in the literature on learning in games. By leveraging a structural property of the underlying
BSPP, we manage to obtain convergence bounds parameterized on the variation of the correlated
equilibria (Theorem 3.9). To illustrate the power of our framework, we immediately recover natural
and algorithm-independent similarity measures for the meta-learning setting (Proposition A.14) even
in general games (Corollary A.25), thereby addressing an open question of Harris et al. [Har+23].
Our techniques also imply new per-player regret bounds in zero-sum and general-sum games (Corol-
laries A.12 and A.26), the latter addressing a question left open by Zhang et al. [Zha+22], albeit
under a different learning paradigm (Section 3.3 contains a discussion on this point). We further
parameterize the convergence of (vanilla) gradient descent in time-varying potential games in terms
of the deviation of the potential functions (Theorem 3.7).
Finally, building on our techniques in time-varying games, we investigate the best dynamic-regret
guarantees possible in static games (see also the work of Cai and Zheng [CZ23]). We first note that in-
stances of optimistic mirror descent guarantee O(√
T)dynamic per-player regret (Proposition 3.10),
matching the known rate of (online) gradient descent but for the significantly weaker notion of external
regret. We further point out that O(logT)dynamic regret is attainable, but in a stronger two-point
feedback model. In stark contrast, even obtaining sublinear dynamic regret for each player is pre-
cluded in general-sum games (Proposition 3.12). This motivates studying a relaxation of dynamic re-
gret that constrains the number of switches in the comparator, for which we derive accelerates rates in
general games (Theorem 3.13) by leveraging the techniques of Syrgkanis et al. [Syr+15] in conjunction
with the dynamic RVU bound. Interesting, this relaxation of dynamic regret gives rise to a natural
refinement of coarse correlated equilibria, recently investigated by Crippa, Gur, and Light [CGL23].
21.2 Further related work
Even in static (two-player) zero-sum games, the pointwise convergence of no-regret learning algo-
rithms is a tenuous affair. Indeed, traditional learning dynamics within the no-regret framework, such
as (online) mirror descent, may even diverge away from the equilibrium; e.g., see [SAF02; MPP18;
Vla+20; GVM21]. Notwithstanding the lack of pointwise convergence, the empirical frequency of
no-regret learners is well-known to approach the set of Nash equilibria in zero-sum games [FS99], and
the set of coarse correlated equilibria in general-sum games [HM00; FV97]—a standard relaxation of
the Nash equilibrium [MV78; Aum74]. Unfortunately, those classical results are of little use beyond
static games, thereby offering a crucial impetus for investigating iterate-convergence in games with
a time-varying component—a ubiquitous theme in many practical scenarios of interest [DMZ21;
Pap+22; Ven21; Gar17; Van10; RG22; PKB22; YH15; RJW21; BSV23]. One concrete and quite
broad example worth mentioning here revolves around the steering problem [Zha+23], where a
mediator dynamically modifies the utilities of the game via nonnegative and vanishing payments so
as to guide no-regret learners to desirable equilibria. In particular, when the payment function varies
over time, the steering problem can be naturally phrased as a time-varying zero-sum game [Zha+23].
In this context, there has been a considerable effort endeavoring to extend the scope of traditional
game-theoretic results to the time-varying setting, approached from a variety of different standpoints
in prior work [LST16; Zha+22; Car+19; MO11; MS21; Duv+22; Fie+21; TRP23; Fen+23; YZZ23].
In particular, our techniques in Section 3.1 are strongly related to the ones developed by Zhang
et al. [Zha+22], but our primary focus is different: Zhang et al. [Zha+22] were mainly interested in
obtaining variation-dependent regret bounds, while our results revolve around iterate-convergence
ofOGD(under a constant learning rate) to Nash equilibria. Minimizing regret and approaching
Nash equilibria are two inherently distinct problems, although connections have emerged [Ana+22b;
Zha+22], and are further cultivated in this paper. Moreover, in an independent work Feng et al.
[Fen+23] established a surprising separation between the behavior of optimistic GDA and the
extra-gradient method in time-varying unconstrained bilinear games; it is open whether such a
discrepancy persists in the constrained setting as well. We also point out the concurrent work
of Yan, Zhao, and Zhou [YZZ23] that focuses on improved guarantees for strongly monotone games;
the results we obtain in Section 3.2 are complementary to theirs.
Another closely related direction is on meta-learning in games [Har+23; Syc+23; Dua+23],
where each game can be repeated for multiple iterations. Such considerations are motivated in part
by a number of use-cases in which many “similar” games—or multiple game variations—ought to be
solved [BS16], such as Poker with different stack-sizes. While the meta-learning problem is a special
case of our general setting, our results are strong enough to have new implications for meta-learning
in games, even though the algorithms considered herein are not tailored to operate in that setting.
2 Preliminaries
Notation We let N∶={1,2, . . . ,}be the set of natural numbers. For a number p∈N, we let
[[p]]∶={1, . . . , p}. For a vector w∈Rd, we use ∥w∥2to represent its Euclidean norm; we also
overload that notation so that ∥⋅∥2denotes the spectral norm when the argument is a matrix.
For a two-player zero-sum game, we denote by X⊆RdxandY⊆Rdythe strategy sets of the two
players—namely, Player xand Player y, respectively—where dx, dy∈Nrepresent the corresponding
dimensions. It is assumed that XandYare nonempty convex and compact sets. For example, in the
special case where X∶=∆dxandY∶=∆dy—each set corresponds to a probability simplex—the game is
3said to be in normal form . Further, we denote by DXtheℓ2-diameter of X, and by ∥X∥2the maximum
ℓ2-norm attained by a point in X. We will always assume that the strategy sets remain invariant,
while the payoff matrix can change in each round. For notational convenience, we will denote by
z∶=(x,y)the concatenation of xandy, and by Z∶=X×Ythe Cartesian product of XandY. In
general n−player games, we instead use subscripts indexed by i∈[[n]]to specify quantities related
to a player. Superscripts are typically reserved to identify the time index. Finally, to simplify the
exposition, we often use the O(⋅)notation to suppress time-independent parameters of the problem.
Dynamic regret We operate in the usual online learning setting under full feedback. Namely,
at every time t∈Nthe learner decides on a strategy x(t)∈X, and then subsequently observes a
utility function x↦⟨x,u(t)
x⟩, foru(t)
x∈Rdx. A strong performance benchmark in this online setting
isdynamic regret , defined for a time horizon T∈Nas follows:
DReg(T)
x(s(T)
x)∶=T
∑
t=1⟨x(t,⋆)−x(t),u(t)
x⟩, (1)
where s(T)
x∶=(x(1,⋆), . . . ,x(T,⋆))∈XTabove is the sequence of comparators ; by setting x(1,⋆)=
x(2,⋆)=⋅⋅⋅=x(T,⋆)in(1)we recover the standard notion of (external) regret (denoted simply by
Reg(T)
x), which is commonly used to establish convergence of the time-average strategies in static
two-player zero-sum games [FS99]. On the other hand, the more general notion of dynamic regret,
introduced in (1), has been extensively used in more dynamic environments; e.g., [Zha+20; Zha+17;
Jad+15; Ces+12; HS09; Kal+21; Mok+16]. We also define DReg(T)
x∶=maxs(T)
x∈XTDReg(T)
x(s(T)
x).
While ensuring o(T)dynamic regret is clearly hopeless in a truly adversarial environment, Section 3.4
reveals that non-trivial guarantees are possible when learning in zero-sum games (see also [CZ23]).
Optimistic gradient descent Optimistic gradient descent ( OGD)[Chi+12; RS13] is a no-regret
algorithm defined with the following update rule:
x(t)∶=ΠX(ˆx(t)+ηm(t)
x),
ˆx(t+1)∶=ΠX(ˆx(t)+ηu(t)
x).(OGD)
Here, η>0 is the learning rate ;ˆx(1)∶=arg minˆx∈X∥ˆx∥2
2represents the initialization of OGD;m(t)
x∈Rdx
is the prediction vector at time t, and it is set as m(t)
x∶=u(t−1)
x when t≥2, and m(1)
x∶=0dx; and
finally, Π X(⋅)represents the Euclidean projection to the set X, which is well-defined, and can be
further computed efficiently for structured sets, such as the probability simplex. For our purposes,
we will posit access to a projection oracle for the set X, in which case the update rule (OGD)is
indeed efficiently implementable.
In a multi-player n-player game, each player i∈[[n]]is associated with a utility function
ui∶⨉n
i=1Xi→R. We recall the following central definition [Nas50].
Definition 2.1 (Approximate Nash equilibrium) .A joint strategy profile (x⋆
1, . . . ,x⋆
n)∈⨉n
i=1Xiis
anϵ-approximate Nash equilibrium (NE), for an ϵ≥0, if for any player i∈[[n]]and any possible
deviation x′
i∈Xiit holds that ui(x⋆
1, . . . ,x⋆
i, . . . ,x⋆
n)≥ui(x⋆
1, . . . ,x′
i, . . . ,x⋆
n)−ϵ.
43 Convergence in time-varying games
In this section, we formalize our results regarding convergence in time-varying games. We organize
this section as follows: First, in Section 3.1, we study the convergence of OGDin time-varying bilinear
saddle-point problems (BSPPs) and beyond, culminating in the non-asymptotic characterization
of Theorem 3.3; Section 3.2 formalizes our improvements under strong convexity-concavity; we then
extend our scope to time-varying multi-player general-sum and potential games in Section 3.3; and
finally, Section 3.4 is concerned with dynamic regret guarantees in static games.
3.1 Bilinear saddle-point problems
We first study an online learning setting wherein two players interact in a sequence of time-
varying BSPPs [Zha+22]. We assume that in every repetition t∈[[T]]the players select a pair
of strategies (x(t),y(t))∈X×Y. Then, Player xreceives the utility u(t)
x∶=−A(t)y(t)∈Rdx, where
A(t)∈Rdx×dyrepresents the payoff matrix at the t-th repetition; similarly, Player yreceives the
utility u(t)
y∶=(A(t))⊺x(t)∈Rdy. We also extend our scope to a more general class of time-varying
problems, as we formalize in Section 3.1.1. The proofs of this subsection are in Appendix A.1.
We commence by pointing out a crucial property: by selecting a sequence of approximate Nash
equilibria as the comparators, the sum of the players’ dynamic regrets cannot be too negative :
Property 3.1. Suppose that Z∋z(t,⋆)=(x(t,⋆),y(t,⋆))is an ϵ(t)-approximate Nash equilibrium of
thet-th game. Then, for s(T)
x=(x(t,⋆))1≤t≤Tands(T)
y=(y(t,⋆))1≤t≤T,
DReg(T)
x(s(T)
x)+DReg(T)
y(s(T)
y)≥−2T
∑
t=1ϵ(t).
A special case of this property was previously noted by Zhang et al. [Zha+22] by considering a
sequence of exact Nash equilibria; the approximate version stated above leads to a significant improve-
ment in the convergence bounds, as we shall see in the sequel. In fact, as we note in Property A.4,
Property 3.1 applies even in certain (time-varying) nonconvex-nonconcave min-max optimization
problems, and it is a consequence of the minimax theorem . Property 3.1 also holds for time-varying
variational inequalities (VIs) that satisfy the so-called MVI property , as we elaborate further in
Section 3.1.1.
Now, building on the work of Zhang et al. [Zha+22], let us introduce some natural measures
of the games’ variation. The first-order variation of the Nash equilibria was defined by Zhang
et al. [Zha+22] for T≥2 asV(T)
NE∶=infz(t,⋆)∈Z(t,⋆),∀t∈[[T]]∑T−1
t=1∥z(t+1,⋆)−z(t,⋆)∥2, where Z(t,⋆)is the
(nonempty) set of Nash equilibria of the t-th game. We recall that there can be a multiplicity
of Nash equilibria [van91]; as such, a compelling feature of the above variation measure is that
it depends on the most favorable sequence of Nash equilibria—one that minimizes the first-order
variation.
It is important, however, to come to terms with the well-known fact that Nash equilibria can
change abruptly even under a “small” perturbation in the payoff matrix (see Example A.5), which
is an important caveat of the variation V(T)
NE. To address this, and in accordance with our more
general Property 3.1, we consider a more favorable variation measure, defined as
V(T)
ϵ−NE∶=inf{T−1
∑
t=1∥z(t+1,⋆)−z(t,⋆)∥2+CT
∑
t=1ϵ(t)}, (2)
5for a sufficiently large parameter C>0 (see (28)); the infimum above is subject to ϵ(t)∈R≥0and
z(t,⋆)∈Z(t,⋆)
ϵ(t)for all t∈[[T]], where Z(t,⋆)
ϵ(t)is the set of ϵ(t)-approximate NE. It is evident that
V(T)
ϵ−NE≤V(T)
NEsince one can take ϵ(1)=⋅⋅⋅=ϵ(T)=0; in fact, V(T)
ϵ−NEcan be arbitrarily smaller:
Proposition 3.2. For any T≥4, there is a sequence of Tgames such that V(T)
NE≥T
2whileV(T)
ϵ−NE≤δ,
for any δ>0.
This shows that V(T)
ϵ−NEis a more compelling variation measure compared to V(T)
NE. It is worth
noting here that Zhang et al. [Zha+22] also considered the variation measure W(T)
A∶=∑T
t=1∥A(t)−¯A∥2,
where ¯Ais the average payoff matrix and ∥⋅∥2denotes the spectral norm.1It is in fact not hard to
construct instances such that V(T)
ϵ−NE≪min{V(T)
NE,W(T)
A}(Proposition A.6).
Moreover, we also recall another quantity that captures the variation of the payoff matrices:
V(T)
A∶=∑T−1
t=1∥A(t+1)−A(t)∥2
2. Unlike V(T)
NE, the variation measure V(T)
Adepends on the second-order
variation (of the payoff matrices), which could translate to a lower-order impact compared to V(T)
NE
(see, e.g., Corollary A.9). We stress that while our convergence bounds will be parameterized
based on V(T)
ϵ−NEandV(T)
A, the underlying algorithm—namely OGD—will remain oblivious to those
variation measures. We only make the mild assumption that players know in advance the value of
L∶=max 1≤t≤T∥A(t)∥2, so that they can tune the learning rate ηappropriately.
We are now ready to state the main result of this subsection. Below, we use the notation
EqGap(t)(z(t))to represent the Nash equilibrium gap of (x(t),y(t))=z(t)∈Zat the t-th
game. More precisely, EqGap(t)(z(t))∶=max{BR(t)
x(x(t)),BR(t)
y(y(t))}, where BR(t)
x(x(t))∶=
maxx∈X{⟨x,u(t)
x⟩}−⟨x(t),u(t)
x⟩is the best response gap of Player x(and analogously for Player y).
Theorem 3.3 (Detailed version in Theorem A.11) .Suppose that both players employ OGDwith
learning rate η=1
4Lin a sequence of time-varying BSPPs, where L∶=max 1≤t≤T∥A(t)∥2. Then,
∑T
t=1(EqGap(t)(z(t)))2=O(1+V(T)
ϵ−NE+V(T)
A), where (z(t))1≤t≤Tis the sequence of joint strategy
profiles produced by OGD.
The proof of this theorem is based on upper bounding the second-order path length ofOGD,
∑T
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2), as a function of the variation measures V(T)
ϵ−NEandV(T)
A. This is
shown via a dynamic RVU bound [Zha+22] (Lemmas A.1 and A.2) in conjunction with Property 3.1.
It is worth noting that when the deviation of the payoff matrices is controlled by the deviation
of the players’ strategies, in the sense that ∑T−1
t=1∥A(t+1)−A(t)∥2
2≤W2∑T−1
t=1∥z(t+1)−z(t)∥2
2for some
parameter W∈R>0, the variation measure V(T)
Ain Theorem 3.3 can be entirely eliminated; see Corol-
lary A.9. The same in fact applies under an improved prediction mechanism (see Remark A.13);
while that prediction is not implementable in the online learning setting, it can be used, for example,
when the sequence of games is known in advance (as is the case in certain applications).
We next state some immediate consequences of Theorem 3.3. (Item 2 below follows from Theo-
rem 3.3 by Jensen’s inequality.)
Corollary 3.4. In the setting of Theorem 3.3,
1. If at least a δ-fraction of the iterates have ϵ>0NE gap, ϵ2δ≤O(1
T(1+V(T)
ϵ−NE+V(T)
A)).
1Any equivalent norm in the definition of W(T)
Asuffices for the purpose of Proposition A.6.
62. The average NE gap is bounded as O(√
1
T(1+V(T)
ϵ−NE+V(T)
A)).
In particular, in terms of asymptotic implications, if limT→+∞V(T)
ϵ−NE
T,limT→+∞V(T)
A
T=0, then (i)
for any ϵ>0 the fraction of iterates of OGDwith at least an ϵNash equilibrium gap converges to 0;
and (ii) the average Nash equilibrium gap of the iterates of OGDconverges to 0.
In the special case where V(T)
ϵ−NE,V(T)
A=O(1), Theorem 3.3 recovers the T−1/2iterate-convergence
rate of OGDin static bilinear saddle-point problems.
Meta-learning Our results also have immediate applications in the meta-learning setting. More
precisely, meta-learning in games is a special case of time-varying games which consists of a sequence
ofH∈Nseparate games, each of which is repeated for m∈Nconsecutive rounds, so that T∶=m×H.
The central goal in meta-learning is to obtain convergence bounds parameterized by the similarity
of the games; identifying suitable similarity metrics is a central question in that line of work.
In this context, we highlight that Theorem 3.3 shown above readily provides a meta-learning
guarantee parameterized by the following notion of similarity between the Nash equilibria of the
games: infz(h,⋆)∈Z(h,⋆),∀h∈[[H]]∑H−1
h=1∥z(h+1,⋆)−z(h,⋆)∥2, where Z(h,⋆)is the set of Nash equilibria of
theh-th game in the meta-learning sequence,2as well as the similarity of the payoff matrices—
corresponding to the term V(T)
A. In fact, under a suitable prediction—the one used by Harris et al.
[Har+23]—the dependence on V(T)
Acan be entirely removed; see Proposition A.14 for our formal
result. A compelling aspect of our meta-learning guarantee is that the considered algorithm is
oblivious to the boundaries of the meta-learning. It is also worth noting that our similarity metric
can be arbitrarily smaller than the one considered by Harris et al. [Har+23] (Proposition A.15).
We further provide some novel results on meta-learning in general-sum games in Section 3.3.
3.1.1 Beyond bilinear saddle-point problems
As we alluded to earlier, our approach can be generalized beyond time-varying bilinear saddle-point
problems to a more general class of time-varying variational inequality (VIs) as follows. Let F(t)∶
Z→Zbe the (single-valued) operator of the VI problem at time t∈N.F(t)is said to satisfy the MVI
property [Mer+19; FP03] if there exists a point z(t,⋆)∈Zsuch that ⟨z−z(t,⋆), F(t)(z)⟩≥0 for any z∈
Z. For example, in the special case of a bilinear saddle-point problem we have that F∶z∶=(x,y)↦
(Ay,−A⊺x), and the MVI property is satisfied by virtue of Von Neumann’s minimax theorem. It is
direct to see that Property 3.1 applies to any time-varying sequence of VIs with respect to (z(t,⋆))1≤t≤T
as long as every operator in the sequence (F(1), . . . , F(T))satisfies the MVI property. (Even more
broadly, it would suffice if almost all operators in the sequence—in that their fraction converges
to 1 as T→+∞—satisfy the MVI property.) This observation enables extending Theorem 3.3 to a
more general class of problems. As a concrete example, we provide a generalization of Theorem 3.3
(Theorem A.16) to polymatrix zero-sum games [Cai+16] in Appendix A.1.7. By contrast, in problems
where the MVI property fails it appears that a much different approach is called for.
We finally point out that our framework could have certain implications for solving (static)
general VIs, as we discuss in Appendix A.1.8.
2In accordance with Theorem 3.3, this similarity metric can be refined using a sequence of approximate NE.
73.2 Strongly convex-concave games
In this subsection, we show that under additional structure we can significantly improve the variation
measures established in Theorem 3.3. More precisely, we first assume that each objective function
f(x,y)isµ-strongly convex with respect to xandµ-strongly concave with respect to y. Our second
assumption is that each game is played for multiple rounds m∈N, instead of only a single round;
this is akin to the meta-learning setting. The key insight is that as long as mis large enough,
m=Ω(1/µ), those two assumptions suffice to obtain a second-order variation bound in terms of the
sequence of Nash equilibria, S(H)
NE∶=∑H−1
h=1∥z(h+1,⋆)−z(h,⋆)∥2
2, where z(h,⋆)is a Nash equilibrium of
theh-th game. This significantly refines the result of Theorem 3.3, and is inspired by the improved
dynamic regret bounds obtained by Zhang et al. [Zha+17]. Below we sketch the key ideas of the
improvement; full proofs are deferred to Appendix A.2.
In this setting, it is assumed that Player xobtains the utility u(t)
x∶=−∇xf(t)(x(t),y(t))at every
time t∈[[T]], while its regret will be denoted by Reg(T)
L,y; similar notation applies for Player y. The
first observation is that, focusing on a single (static) game, under strong convexity-concavity the
sum of the players’ regrets are strongly nonnegative (Lemma A.18):
Reg(m)
L,x(x⋆)+Reg(m)
L,y(y⋆)≥µ
2m
∑
t=1∥z(t)−z⋆∥2
2, (3)
for any Nash equilibrium z⋆∈Zof the game. In turn, this can be cast in terms of dynamic regret over
the sequence of the hgames (Lemma A.19). Next, combining those dynamic-regret lower bounds
with a suitable RVU-type property leads to a refined second-order path length bound as long as m=
Ω(1/µ), which in turn leads to our main result below. Before we present its statement, let us introduce
the following measure of variation of the gradients: V(H)
∇f∶=∑H−1
h=1maxz∈Z∥F(h+1)(z)−F(h)(z)∥2
2,
where let F∶z∶=(x,y)↦(∇xf(x,y),−∇yf(x,y)). This variation measure is analogous to V(T)
A
we introduced earlier for time-varying BSPPs.
Theorem 3.5 (Detailed version in Theorem A.21) .Letf(h)∶X×Ybe a µ-strongly convex-
concave and L-smooth function, for all h∈[[H]]. Suppose further that both players employ OGD
with learning rate η=min{1
8L,1
2µ}forT∈Nrepetitions, where T=m×Handm≥2
ηµ. Then,
∑T
t=1(EqGap(t)(z(t)))2=O(1+S(H)
NE+V(H)
∇f).
Our techniques also imply the improved regret bounds Reg(T)
L,x,Reg(T)
L,y=O(√
1+S(H)
NE+V(H)
∇f),
under suitable tuning of the learning rate (see Corollary A.22).
There is another immediate but important implication of (3):anyno-regret algorithm in a
(static) strongly convex-concave setting ought to be approaching the Nash equilibrium;3in contrast,
this property is spectacularly false in (general) monotone settings [MPP18].
Proposition 3.6. Letf∶X×Y→Rbe a µ-strongly convex-concave function. If players incur
regrets such that Reg(T)
L,x+Reg(T)
L,y≤CT1−ω, for some parameters C>0andω∈(0,1], then for any
ϵ>0andT>(2C
µϵ2)1/ω
there is a pair of strategies z(t)∈Zsuch that ∥z(t)−z⋆∥2≤ϵ, where z⋆∈Z
is a Nash equilibrium.
3Such observations were independently documented by Wang et al. [Wan+22].
8The insights of this subsection are also of interest in general monotone settings by incorpo-
rating a strongly convex regularizer; tuning its magnitude allows us to trade off between a better
approximation and the benefits of strong convexity-concavity revealed in this subsection.
3.3 General-sum multi-player games
Next, we turn our attention to general-sum multi-player games. For simplicity, in this subsection
we posit that the game is represented in normal form, so that each player i∈[[n]]has a finite set of
available actions Ai, andXi∶=∆(Ai). The proofs of this subsection are included in Appendix A.3.
Potential games First, we study the convergence of (online) gradient descent ( GD) in time-varying
potential games (see Definition A.23 for the formal description); we recall that unlike two-player
zero-sum games, gradient descent is known to approach Nash equilibria in potential games. In our
time-varying setup, it is assumed that each round t∈[[T]]corresponds to a different potential game
described with a potential function Φ(t). We further let d∶(Φ,Φ′)↦maxz∈⨉n
i=1Xi(Φ(z)−Φ′(z)),
so that V(T)
Φ∶=∑T−1
t=1d(Φ(t),Φ(t+1)); we call attention to the fact that d(⋅,⋅)is not symmetric.
Analogously to Theorem 3.3, we use EqGap(t)(z(t))∈R≥0to represent the NE gap of the joint
strategy profile z(t)∶=(x(t)
1, . . . ,x(t)
n)at the t-th game.
Theorem 3.7. Suppose that each player employs (online) GDwith a sufficiently small learning rate
in a sequence of time-varying potential games. Then, ∑T
t=1(EqGap(t)(z(t)))2=O(Φmax+V(T)
Φ),
where Φmaxis such that ∣Φ(t)(⋅)∣≤Φmax.
We refer to Appendix B for some illustrative experiments related to Theorem 3.7.
General games Unfortunately, unlike the settings considered thus far, computing Nash equilibria
in general games is computationally hard even under a crude approximation ϵ=Θ(1)[DGP08;
CDT09; Del+22]. Instead, learning algorithms are known to converge—in a time-average sense—to
relaxations of the Nash equilibrium, known as (coarse) correlated equilibria . For our purposes, we
will employ a bilinear formulation of correlated equilibria, which dates back to the seminal work
of Hart and Schmeidler [HS89] (see also [Von21, Chapter 12] for an excellent exposition). This will
allow us to translate the results of Section 3.1 to general multi-player games.
Specifically, correlated equilibria4can be phrased via a game between the nplayers and a media-
tor, an additional agent. At a high level, the mediator is endeavoring to identify a correlated strategy
µ∈Ξ∶=∆(⨉n
i=1Ai)for which no player has an incentive to deviate from the recommendation. In
contrast, the players are trying to optimally deviate so as to maximize their own utility. More
precisely, there exist matrices A1, . . . ,An, with each matrix Aidepending solely on the utility of
Player i, for which the bilinear saddle-point problem can be expressed as
min
µ∈Ξmax
(¯x1,...,¯xn)∈⨉n
i=1¯Xin
∑
i=1µ⊺Ai¯xi, (4)
where ¯Xiabove is a suitable set of strategies; we elaborate more on this formulation in Ap-
pendix A.3. This zero-sum game has the property that there exists a strategy µ⋆∈Ξ such that
max¯xi∈¯Xi(µ⋆)⊺Ai¯xi≤0, for any player i∈[[n]], which is precisely a correlated equilibrium.
4There is also a bilinear formulation tailored to coarse correlated equilibria, but we will focus solely on the stronger
variant (CE) for concreteness.
9Before we proceed, it is important to note that the learning paradigm considered here deviates
from the traditional one in that orchestrating the protocol requires an additional learning agent,
resulting in a less decentralized protocol. Yet, the dynamics induced by solving (4)via online
algorithms remain uncoupled [HM00], in the sense that each player obtains feedback—corresponding
to the deviation benefit—that depends solely on its own utility.
Now in the time-varying setting, the matrices A1, . . . ,Anthat capture the players’ utilities can
change in each repetition. Crucially, we show that the structure of the induced bilinear problem (4)
is such that there is a sequence of correlated equilibria that guarantee nonnegative dynamic regret;
this refines Property 3.1 in that only one player’s strategies suffice to guarantee nonnegativity, even
if the strategy of the other player remains invariant. Below, we denote by DReg(T)
µthe dynamic
regret of Player min in (4), and by Reg(T)
ithe regret of each player i∈[[n]]up to time T∈N, so
that the regret of Player max in (4) can be expressed as ∑n
i=1Reg(T)
i.
Property 3.8. Suppose that Ξ∋µ(t,⋆)is a correlated equilibrium of the game at any time t∈[[T]].
Then, DReg(T)
µ(µ(1,⋆), . . . ,µ(T,⋆))+∑n
i=1Reg(T)
i≥0.
As a result, this enables us to apply Theorem 3.3 parameterized on (i) the variation of the CE
V(T)
CE∶=infµ(t,⋆)∈Ξ(t,⋆),∀t∈[[T]]∑T−1
t=1∥µ(t+1,⋆)−µ(t,⋆)∥2, where Ξ(t,⋆)denotes the set of CE of the t-th
game, and (ii) the variation in the players’ utilities V(T)
A∶=∑n
i=1∑T−1
t=1∥A(t+1)
i−A(t)
i∥2
2; below, we
denote by CeGap(t)(µ(t))the CE gap of µ(t)∈Ξ at the t-th game.
Theorem 3.9. Suppose that each player employs OGDin a sequence of time-varying BSPPs (4)
with a sufficiently small learning rate. Then, ∑T
t=1(CeGap(t)(µ(t)))2=O(1+V(T)
CE+V(T)
A).
There are further interesting implications of our framework that are worth highlighting. First,
we obtain meta-learning guarantees for general games that depend on the (algorithm-independent)
similarity of the correlated equilibria (Corollary A.25); that was left as an open question by Harris
et al. [Har+23], where instead algorithm-dependent similarity metrics were derived. Further, by
applying Corollary A.12, we derive natural variation-dependent per-player regret bounds in general
games (Corollary A.26); this addresses a question left by Zhang et al. [Zha+22], albeit under a
different learning paradigm. We suspect that obtaining such results—parameterized on the variation
of the CE—are not possible without the presence of the additional agent, as in (4).
3.4 Dynamic regret bounds in static games
Finally, in this subsection we switch gears by investigating dynamic regret guarantees when learning
in static games. The proofs of this subsection are included in Appendix A.4.
First, we point out that while traditional no-regret learning algorithms guarantee O(√
T)external
regret, instances of OMD—a generalization of OGD; see (5)in Appendix A—in fact guarantee O(√
T)
dynamic regret in two-player zero-sum games, which is a much stronger performance measure:
Proposition 3.10. Suppose that both players in a (static) two-player zero-sum game employ OMD
with a smooth regularizer. Then, DReg(T)
x,DReg(T)
y=O(√
T).
In proof, the dynamic regret of each player under OMDwith a smooth regularizer can be
bounded by the first-order path length of that player’s strategies, which in turn can be bounded
10byO(√
T)given that the second-order path length is O(1)(Theorem A.7). In fact, Theo-
rem A.7 readily extends Proposition 3.10 to time-varying zero-sum games as well, implying that
DReg(T)
x,DReg(T)
y=O(√
T(1+V(T)
ϵ−NE+V(T)
A)).
A question that arises from Proposition 3.10 is whether the O(√
T)guarantee for dynamic
regret of OMDcan be improved in the online learning setting. Below, we point out a significant
improvement to O(logT), but under a stronger two-point feedback model;5namely, we posit that
in every round each player can select an additional auxiliary strategy, and each player then gets to
additionally observe the utility corresponding to the auxiliary strategies. Notably, this is akin to
how the extra-gradient method works [Hsi+19] (also cf. [RS13, Section 4.2] for multi-point feedback
models in the bandit setting).
Observation 3.11. Under two-point feedback, there exist learning algorithms that guarantee
DReg(T)
x,DReg(T)
y=O(logT)in two-player zero-sum games.
In particular, it suffices for each player to employ OMD, but with the twist that the first strategy
in each round is the time-average ofOMD; the auxiliary strategy is the standard output of OMD. Then,
the dynamic regret of each player will grow as O(∑T
t=11
t)=O(logT)since the duality gap of the
average strategies is decreasing with a rate of T−1[RS13]. It is an interesting question whether the
bound of Observation 3.11 can be improved to O(1)[CZ23]; we conjecture that there is a lower
bound of Ω (logT).
General-sum games In stark contrast, no (computationally efficient) sublinear dynamic regret
guarantees are possible in general-sum games:
Proposition 3.12. Any polynomial-time algorithm incurs ∑n
i=1DReg(T)
i≥CTfor any polynomial
T∈N, even if n=2andC>0is an absolute constant, unless ETH for PPAD is false [Rub16].
Indeed, this follows immediately since computing a Nash equilibrium to O(1)precision in
two-player games requires superpoylnomial time [Rub16]. As such, Proposition 3.12 applies beyond
the online learning setting. This motivates considering a relaxation of dynamic regret, wherein the
sequence of comparators is subject to the constraint ∑T−1
t=1 1{x(t+1,⋆)
i≠x(t,⋆)
i}≤K−1, for some param-
eterK∈N[CGL23]; this will be referred to as K-DReg(T)
i. Naturally, external regret coincides with
K-DReg(T)
iunder K=1. In this context, we employ Lemma A.1 to bound K-DReg(T)under OGD:
Theorem 3.13 (Precise version in Theorem A.28) .Suppose that all nplayers employ OGDwith a
suitable learning rate in an L-smooth game. Then, for any K∈N,
1.∑n
i=1K-DReg(T)
i=O(K√nLD2
Z);
2.K-DReg(T)
i=O(K3/4T1/4n1/4L1/2D3/2
Xi), for any player i∈[[n]].
One question that arises here is whether the per-player bound of O(K3/4T1/4)(Item 2) can
be improved to ˜O(K), where ˜O(⋅)hides logarithmic factors. The main challenge is that, even
forK=1, all known methods that obtain ˜O(1)[DFG21; PSS22; Ana+22a; Far+22a] rely on
5The same asymptotic bound was independently established by Cai and Zheng [CZ23] but in the standard feedback
model.
11non-smooth regularizers that violate the preconditions of Lemma A.2—our dynamic RVU bound
beyond (squared) Euclidean regularization; yet, we point out that it is possible under a slightly
stronger feedback model (Remark A.29). We finally highlight that the game-theoretic significance
of the solution concept that arises as the limit point of no-regret learners when K-DReg=o(T)was
recently investigated by Crippa, Gur, and Light [CGL23].
4 Conclusions and future work
In this paper, we developed a framework for characterizing iterate-convergence of no-regret learning
algorithms—primarily optimistic gradient descent ( OGD)—in time-varying games. There are many
promising avenues for future research. Besides closing the obvious gaps we highlighted in Section 3.4,
it is important to characterize the behavior of no-regret learning algorithms in other fundamental
time-varying multiagent settings, such as Stackelberg (security) games [Bal+15]. Moreover, our
results operate in the full-feedback model where each player receives feedback on all possible ac-
tions. Extending the scope of our framework to capture partial-feedback models as well is another
interesting direction for future work.
Acknowledgements
We are grateful to anonymous referees for providing valuable and constructive feedback. We also
thank Vince Conitzer and Caspar Oesterheld for many helpful comments. This material is based on
work supported by the National Science Foundation under grants IIS-1901403 and CCF-1733556
and by the ARO under award W911NF2210266.
References
[Ana+22a] Ioannis Anagnostides, Gabriele Farina, Christian Kroer, Chung-Wei Lee, Haipeng Luo,
and Tuomas Sandholm. “Uncoupled Learning Dynamics with O(logT)Swap Regret in
Multiplayer Games”. In: Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS) . 2022.
[Ana+22b] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. “On
Last-Iterate Convergence Beyond Zero-Sum Games”. In: International Conference
on Machine Learning (ICML) . Vol. 162. Proceedings of Machine Learning Research.
PMLR, 2022, pp. 536–581.
[Aum74] Robert Aumann. “Subjectivity and Correlation in Randomized Strategies”. In: Journal
of Mathematical Economics 1 (1974), pp. 67–96.
[Azi+20] Wa¨ ıss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. “A
Tight and Unified Analysis of Gradient-Based Methods for a Whole Spectrum of
Differentiable Games”. In: The 23rd International Conference on Artificial Intelligence
and Statistics, AISTATS . Vol. 108. Proceedings of Machine Learning Research. PMLR,
2020, pp. 2863–2873.
12[Bal+15] Maria-Florina Balcan, Avrim Blum, Nika Haghtalab, and Ariel D. Procaccia. “Commit-
ment Without Regrets: Online Learning in Stackelberg Security Games”. In: Proceedings
of the ACM Conference on Economics and Computation (EC) . ACM, 2015, pp. 61–78.
[BMW21] Heinz H. Bauschke, Walaa M. Moursi, and Xianfu Wang. “Generalized monotone
operators and their averaged resolvents”. In: Math. Program. 189.1 (2021), pp. 55–74.
[Bow+15] Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. “Heads-up
Limit Hold’em Poker is Solved”. In: Science 347.6218 (Jan. 2015).
[BS16] Noam Brown and Tuomas Sandholm. “Strategy-Based Warm Starting for Regret
Minimization in Games”. In: AAAI Conference on Artificial Intelligence (AAAI) . 2016.
[BS18] Noam Brown and Tuomas Sandholm. “Superhuman AI for heads-up no-limit poker:
Libratus beats top professionals”. In: Science 359.6374 (2018), pp. 418–424.
[BS19a] Noam Brown and Tuomas Sandholm. “Solving imperfect-information games via dis-
counted regret minimization”. In: AAAI Conference on Artificial Intelligence (AAAI) .
2019.
[BS19b] Noam Brown and Tuomas Sandholm. “Superhuman AI for multiplayer poker”. In:
Science 365.6456 (2019), pp. 885–890.
[BSV23] Lucas Baudin, Marco Scarsini, and Xavier Venel. “Strategic Behavior and No-Regret
Learning in Queueing Systems”. In: CoRR abs/2302.03614 (2023).
[Cai+16] Yang Cai, Ozan Candogan, Constantinos Daskalakis, and Christos H. Papadimitriou.
“Zero-Sum Polymatrix Games: A Generalization of Minmax”. In: Math. Oper. Res. 41.2
(2016), pp. 648–655.
[Car+19] Adrian Rivera Cardoso, Jacob D. Abernethy, He Wang, and Huan Xu. “Competing
Against Nash Equilibria in Adversarially Changing Zero-Sum Games”. In: International
Conference on Machine Learning (ICML) . Vol. 97. Proceedings of Machine Learning
Research. PMLR, 2019, pp. 921–930.
[CDT09] Xi Chen, Xiaotie Deng, and Shang-Hua Teng. “Settling the Complexity of Computing
Two-Player Nash Equilibria”. In: Journal of the ACM (2009).
[Ces+12] Nicol` o Cesa-Bianchi, Pierre Gaillard, G´ abor Lugosi, and Gilles Stoltz. “Mirror Descent
Meets Fixed Share (and feels no regret)”. In: Proceedings of the Annual Conference on
Neural Information Processing Systems (NeurIPS) . 2012, pp. 989–997.
[CGL23] Ludovico Crippa, Yonatan Gur, and Bar Light. “Equilibria in Repeated Games under
No-Regret with Dynamic Benchmarks”. In: CoRR abs/2212.03152 (2023).
[Chi+12] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
Jin, and Shenghuo Zhu. “Online optimization with gradual variations”. In: Conference
on Learning Theory . 2012, pp. 6–1.
[COZ22] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. “Finite-Time Last-Iterate Conver-
gence for Learning in Multi-Player Games”. In: Proceedings of the Annual Conference
on Neural Information Processing Systems (NeurIPS) . 2022.
[CP04] Patrick L. Combettes and Teemu Pennanen. “Proximal Methods for Cohypomonotone
Operators”. In: SIAM J. Control. Optim. (2004), pp. 731–742.
13[CZ22] Yang Cai and Weiqiang Zheng. “Accelerated Single-Call Methods for Constrained
Min-Max Optimization”. In: CoRR abs/2210.03096 (2022).
[CZ23] Yang Cai and Weiqiang Zheng. “Doubly Optimal No-Regret Learning in Monotone
Games”. In: International Conference on Machine Learning (ICML) . Vol. 202. Pro-
ceedings of Machine Learning Research. PMLR, 2023, pp. 3507–3524.
[Das+18] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. “Training
GANs with Optimism”. In: 6th International Conference on Learning Representations,
ICLR . OpenReview.net, 2018.
[Das22] Constantinos Daskalakis. Non-Concave Games: A Challenge for Game Theory’s Next
100 Years . 2022.
[DDJ21] Jelena Diakonikolas, Constantinos Daskalakis, and Michael I. Jordan. “Efficient Meth-
ods for Structured Nonconvex-Nonconcave Min-Max Optimization”. In: The 24th
International Conference on Artificial Intelligence and Statistics, AISTATS . Vol. 130.
Proceedings of Machine Learning Research. PMLR, 2021, pp. 2746–2754.
[DDK11] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. “Near-optimal no-
regret algorithms for zero-sum games”. In: Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA) . 2011.
[Del+22] Argyrios Deligkas, John Fearnley, Alexandros Hollender, and Themistoklis Melissourgos.
“Pure-Circuit: Strong Inapproximability for PPAD”. In: Proceedings of the Annual
Symposium on Foundations of Computer Science (FOCS) . IEEE, 2022, pp. 159–170.
[DFG20] Constantinos Daskalakis, Dylan J. Foster, and Noah Golowich. “Independent Policy
Gradient Methods for Competitive Reinforcement Learning”. In: Proceedings of the
Annual Conference on Neural Information Processing Systems (NeurIPS) . 2020.
[DFG21] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. “Near-Optimal No-
Regret Learning in General Games”. In: Proceedings of the Annual Conference on
Neural Information Processing Systems (NeurIPS) . 2021, pp. 27604–27616.
[DGP08] Constantinos Daskalakis, Paul Goldberg, and Christos Papadimitriou. “The Complexity
of Computing a Nash Equilibrium”. In: SIAM Journal on Computing (2008).
[Dia20] Jelena Diakonikolas. “Halpern Iteration for Near-Optimal and Parameter-Free Mono-
tone Inclusion and Strong Solutions to Variational Inequalities”. In: Conference on
Learning Theory (COLT) . Ed. by Jacob D. Abernethy and Shivani Agarwal. Vol. 125.
Proceedings of Machine Learning Research. PMLR, 2020, pp. 1428–1451.
[DL15] Cong D. Dang and Guanghui Lan. “On the convergence properties of non-Euclidean
extragradient methods for variational inequalities with generalized monotone operators”.
In:Comput. Optim. Appl. (2015), pp. 277–310.
[DMZ21] Yuan Deng, Vahab Mirrokni, and Song Zuo. “Non-Clairvoyant Dynamic Mechanism
Design with Budget Constraints and Beyond”. In: Proceedings of the 22nd ACM
Conference on Economics and Computation . Proceedings of the ACM Conference on
Economics and Computation (EC). Association for Computing Machinery, 2021.
[Dua+23] Zhijian Duan, Wenhan Huang, Dinghuai Zhang, Yali Du, Jun Wang, Yaodong Yang,
and Xiaotie Deng. “Is Nash Equilibrium Approximator Learnable?” In: Autonomous
Agents and Multi-Agent Systems . ACM, 2023, pp. 233–241.
14[Duv+22] Benoit Duvocelle, Panayotis Mertikopoulos, Mathias Staudigl, and Dries Vermeulen.
“Multiagent online learning in time-varying games”. In: Mathematics of Operations
Research (2022).
[Far+22a] Gabriele Farina, Ioannis Anagnostides, Haipeng Luo, Chung-Wei Lee, Christian Kroer,
and Tuomas Sandholm. “Near-Optimal No-Regret Learning for General Convex Games”.
In:Proceedings of the Annual Conference on Neural Information Processing Systems
(NeurIPS) . 2022.
[Far+22b] Gabriele Farina, Christian Kroer, Chung-Wei Lee, and Haipeng Luo. “Clairvoyant
Regret Minimization: Equivalence with Nemirovski’s Conceptual Prox Method and
Extension to General Convex Games”. In: CoRR abs/2208.14891 (2022).
[Fen+23] Yi Feng, Hu Fu, Qun Hu, Ping Li, Ioannis Panageas, Bo Peng, and Xiao Wang. “On the
Last-iterate Convergence in Time-varying Zero-sum Games: Extra Gradient Succeeds
where Optimism Fails”. In: CoRR abs/2310.02604 (2023).
[Fie+21] Tanner Fiez, Ryann Sim, Stratis Skoulakis, Georgios Piliouras, and Lillian J. Ratliff.
“Online Learning in Periodic Zero-Sum Games”. In: Proceedings of the Annual Confer-
ence on Neural Information Processing Systems (NeurIPS) . 2021, pp. 10313–10325.
[FKS21] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. “Faster Game Solving
via Predictive Blackwell Approachability: Connecting Regret Matching and Mirror
Descent”. In: Proceedings of the AAAI Conference on Artificial Intelligence . 2021.
[FP03] Francisco Facchinei and Jong-Shi Pang. Finite-dimensional variational inequalities and
complementarity problems . Springer, 2003.
[FS99] Yoav Freund and Robert Schapire. “Adaptive game playing using multiplicative
weights”. In: Games and Economic Behavior 29 (1999), pp. 79–103.
[FV97] Dean Foster and Rakesh Vohra. “Calibrated Learning and Correlated Equilibrium”.
In:Games and Economic Behavior 21 (1997), pp. 40–55.
[Gar17] Daniel F. Garrett. “Dynamic mechanism design: Dynamic arrivals and changing values”.
In:Games and Economic Behavior 104 (2017), pp. 595–612.
[GPD20] Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. “Tight last-iterate
convergence rates for no-regret learning in multi-player games”. In: Proceedings of the
Annual Conference on Neural Information Processing Systems (NeurIPS) . 2020.
[GTG22] Eduard Gorbunov, Adrien B. Taylor, and Gauthier Gidel. “Last-Iterate Convergence
of Optimistic Gradient Method for Monotone Variational Inequalities”. In: Proceedings
of the Annual Conference on Neural Information Processing Systems (NeurIPS) . 2022.
[GVM21] Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mer-
tikopoulos. “Survival of the strictest: Stable and unstable equilibria under regularized
learning with partial information”. In: Conference on Learning Theory (COLT) . Vol. 134.
Proceedings of Machine Learning Research. PMLR, 2021, pp. 2147–2148.
[Har+23] Keegan Harris, Ioannis Anagnostides, Gabriele Farina, Mikhail Khodak, Steven Wu,
and Tuomas Sandholm. “Meta-Learning in Games”. In: The Eleventh International
Conference on Learning Representations, ICLR . OpenReview.net, 2023.
15[HM00] Sergiu Hart and Andreu Mas-Colell. “A Simple Adaptive Procedure Leading to Corre-
lated Equilibrium”. In: Econometrica 68 (2000), pp. 1127–1150.
[HS09] Elad Hazan and C. Seshadhri. “Efficient Learning Algorithms for Changing Environ-
ments”. In: International Conference on Machine Learning (ICML) . Association for
Computing Machinery, 2009, pp. 393–400.
[HS89] Sergiu Hart and David Schmeidler. “Existence of Correlated Equilibria”. In: Mathe-
matics of Operations Research 14.1 (1989), pp. 18–25.
[Hsi+19] Yu-Guan Hsieh, Franck Iutzeler, J´ erome Malick, and Panayotis Mertikopoulos. “On
the convergence of single-call stochastic extra-gradient methods”. In: Proceedings of
the Annual Conference on Neural Information Processing Systems (NeurIPS) . 2019,
pp. 6936–6946.
[Jad+15] Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan.
“Online Optimization : Competing with Dynamic Comparators”. In: Proceedings of
the Eighteenth International Conference on Artificial Intelligence and Statistics . 2015,
pp. 398–406.
[Kal+21] Deepak S. Kalhan, Amrit Singh Bedi, Alec Koppel, Ketan Rajawat, Hamed Hassani,
Abhishek K. Gupta, and Adrish Banerjee. “Dynamic Online Learning via Frank-Wolfe
Algorithm”. In: IEEE Trans. Signal Process. 69 (2021), pp. 932–947.
[LST16] Thodoris Lykouris, Vasilis Syrgkanis, and ´Eva Tardos. “Learning and Efficiency in
Games with Dynamic Population”. In: Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA) . SIAM, 2016, pp. 120–129.
[Mer+19] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay
Chandrasekhar, and Georgios Piliouras. “Optimistic mirror descent in saddle-point
problems: Going the extra (gradient) mile”. In: 7th International Conference on
Learning Representations, ICLR . OpenReview.net, 2019.
[MO11] Ishai Menache and Asuman E. Ozdaglar. Network Games: Theory, Models, and Dynam-
ics. Synthesis Lectures on Communication Networks. Morgan & Claypool Publishers,
2011.
[Mok+16] Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. “Online
optimization in dynamic environments: Improved regret rates for strongly convex
problems”. In: 55th IEEE Conference on Decision and Control, CDC 2016 . IEEE,
2016, pp. 7195–7201.
[MPP18] Panayotis Mertikopoulos, Christos H. Papadimitriou, and Georgios Piliouras. “Cycles
in Adversarial Regularized Learning”. In: Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA) . SIAM, 2018, pp. 2703–2717.
[MRS20] Eric Mazumdar, Lillian J. Ratliff, and S. Shankar Sastry. “On Gradient-Based Learning
in Continuous Games”. In: SIAM J. Math. Data Sci. (2020), pp. 103–131.
[MS21] Panayotis Mertikopoulos and Mathias Staudigl. “Equilibrium Tracking and Convergence
in Dynamic Games”. In: 2021 60th IEEE Conference on Decision and Control (CDC) .
IEEE, 2021, pp. 930–935.
16[MV21] Oren Mangoubi and Nisheeth K. Vishnoi. “Greedy adversarial equilibrium: an efficient
alternative to nonconvex-nonconcave min-max optimization”. In: Proceedings of the
Annual Symposium on Theory of Computing (STOC) . ACM, 2021, pp. 896–909.
[MV78] H. Moulin and J.-P. Vial. “Strategically zero-sum games: The class of games whose
completely mixed equilibria cannot be improved upon”. In: International Journal of
Game Theory 7.3-4 (1978), pp. 201–221.
[Nas50] John Nash. “Equilibrium points in N-person games”. In: Proceedings of the National
Academy of Sciences 36 (1950), pp. 48–49.
[Nem04] Arkadi Nemirovski. “Prox-method with rate of convergence O(1/t) for variational
inequalities with Lipschitz continuous monotone operators and smooth convex-concave
saddle point problems”. In: SIAM Journal on Optimization 15.1 (2004).
[Nou+19] Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D. Lee, and Meisam Raza-
viyayn. “Solving a Class of Non-Convex Min-Max Games Using Iterative First Order
Methods”. In: Proceedings of the Annual Conference on Neural Information Processing
Systems (NeurIPS) . 2019, pp. 14905–14916.
[Pap+22] Christos H. Papadimitriou, George Pierrakos, Alexandros Psomas, and Aviad Rubin-
stein. “On the complexity of dynamic mechanism design”. In: Games and Economic
Behavior 134 (2022), pp. 399–427.
[PKB22] Jorge I. Poveda, Miroslav Krstic, and Tamer Basar. “Fixed-Time Seeking and Tracking
of Time-Varying Nash Equilibria in Noncooperative Games”. In: American Control
Conference, ACC 2022, Atlanta, GA, USA, June 8-10, 2022 . IEEE, 2022, pp. 794–799.
[PSS22] Georgios Piliouras, Ryann Sim, and Stratis Skoulakis. “Beyond Time-Average Con-
vergence: Near-Optimal Uncoupled Online Learning via Clairvoyant Multiplicative
Weights Update”. In: Proceedings of the Annual Conference on Neural Information
Processing Systems (NeurIPS) . 2022.
[RG22] Aitazaz Ali Raja and Sergio Grammatico. “Payoff Distribution in Robust Coalitional
Games on Time-Varying Networks”. In: IEEE Trans. Control. Netw. Syst. 9.1 (2022),
pp. 511–520.
[RJW21] Jad Rahme, Samy Jelassi, and S. Matthew Weinberg. “Auction Learning as a Two-
Player Game”. In: 9th International Conference on Learning Representations, ICLR .
2021.
[RS13] Alexander Rakhlin and Karthik Sridharan. “Optimization, learning, and games with
predictable sequences”. In: Advances in Neural Information Processing Systems . 2013,
pp. 3066–3074.
[Rub16] Aviad Rubinstein. “Settling the Complexity of Computing Approximate Two-Player
Nash Equilibria”. In: Proceedings of the Annual Symposium on Foundations of Computer
Science (FOCS) . IEEE Computer Society, 2016, pp. 258–265.
[SAF02] Yuzuru Sato, Eizo Akiyama, and J. Doyne Farmer. “Chaos in learning a simple
two-person game”. In: Proceedings of the National Academy of Sciences 99.7 (2002),
pp. 4748–4751.
[Sha12] Shai Shalev-Shwartz. “Online Learning and Online Convex Optimization”. In: Founda-
tions and Trends in Machine Learning 4 (2012).
17[Son+20] Chaobing Song, Zhengyuan Zhou, Yichao Zhou, Yong Jiang, and Yi Ma. “Optimistic
Dual Extrapolation for Coherent Non-monotone Variational Inequalities”. In: Proceed-
ings of the Annual Conference on Neural Information Processing Systems (NeurIPS) .
2020.
[Syc+23] David Sychrovsky, Michal Sustr, Elnaz Davoodi, Marc Lanctot, and Martin Schmid.
“Learning not to Regret”. In: CoRR abs/2303.01074 (2023).
[Syr+15] Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. “Fast conver-
gence of regularized learning in games”. In: Advances in Neural Information Processing
Systems . 2015, pp. 2989–2997.
[TRP23] Feras Al Taha, Kiran Rokade, and Francesca Parise. “Gradient Dynamics in Linear
Quadratic Network Games with Time-Varying Connectivity and Population Fluctua-
tion”. In: CoRR abs/2309.07871 (2023).
[Van10] Ngo Van Long. A survey of dynamic games in economics . Vol. 1. World Scientific,
2010.
[van91] Eric van Damme. Stability and perfection of Nash equilibria . Vol. 339. Springer, 1991.
[Ven21] Xavier Venel. “Regularity of dynamic opinion games”. In: Games and Economic
Behavior 126 (2021), pp. 305–334.
[Vla+20] Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, Thanasis Lianeas, Panay-
otis Mertikopoulos, and Georgios Piliouras. “No-Regret Learning and Mixed Nash
Equilibria: They Do Not Mix”. In: Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS) . 2020.
[Von21] Bernhard Von Stengel. Game theory basics . Cambridge University Press, 2021.
[Wan+22] Zifan Wang, Yi Shen, Michael Zavlanos, and Karl Henrik Johansson. No-Regret Learning
in Strongly Monotone Games Converges to a Nash Equilibrium . 2022.
[YH15] Maojiao Ye and Guoqiang Hu. “Distributed Seeking of Time-Varying Nash Equilibrium
for Non-Cooperative Games”. In: IEEE Trans. Autom. Control. 60.11 (2015), pp. 3000–
3005.
[YKH20] Junchi Yang, Negar Kiyavash, and Niao He. “Global Convergence and Variance-Reduced
Optimization for a Class of Nonconvex-Nonconcave Minimax Problems”. In: CoRR
abs/2002.09621 (2020).
[YM23] Yuepeng Yang and Cong Ma. “ O(T−1)Convergence of Optimistic-Follow-the-Regularized-
Leader in Two-Player Zero-Sum Markov Games”. In: The Eleventh International
Conference on Learning Representations, ICLR . OpenReview.net, 2023.
[YR21] Taeho Yoon and Ernest K. Ryu. “Accelerated Algorithms for Smooth Convex-Concave
Minimax Problems with O(1/k2)Rate on Squared Gradient Norm”. In: International
Conference on Machine Learning (ICML) . Vol. 139. Proceedings of Machine Learning
Research. PMLR, 2021, pp. 12098–12109.
[YZZ23] Yu-Hu Yan, Peng Zhao, and Zhi-Hua Zhou. “Fast Rates in Time-Varying Strongly
Monotone Games”. In: International Conference on Machine Learning (ICML) . Vol. 202.
Proceedings of Machine Learning Research. PMLR, 2023, pp. 39138–39164.
18[Zha+17] Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. “Improved Dy-
namic Regret for Non-degenerate Functions”. In: Proceedings of the Annual Conference
on Neural Information Processing Systems (NeurIPS) . 2017, pp. 732–741.
[Zha+20] Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. “Dynamic Regret of
Convex and Smooth Functions”. In: Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS) . 2020.
[Zha+22] Mengxiao Zhang, Peng Zhao, Haipeng Luo, and Zhi-Hua Zhou. “No-Regret Learning
in Time-Varying Zero-Sum Games”. In: International Conference on Machine Learning
(ICML) . Vol. 162. Proceedings of Machine Learning Research. PMLR, 2022, pp. 26772–
26808.
[Zha+23] Brian Hu Zhang, Gabriele Farina, Ioannis Anagnostides, Federico Cacciamani, Stephen
Marcus McAleer, Andreas Alexander Haupt, Andrea Celli, Nicola Gatti, Vincent
Conitzer, and Tuomas Sandholm. “Steering No-Regret Learners to Optimal Equilibria”.
In:CoRR abs/2306.05221 (2023).
[Zin03] Martin Zinkevich. “Online Convex Programming and Generalized Infinitesimal Gradient
Ascent”. In: International Conference on Machine Learning (ICML) . 2003, pp. 928–936.
19A Omitted proofs
In this section, we provide the proofs from the main body (Section 3).
A.1 Proofs from Section 3.1
First, we start with the proofs of the dynamic RVU bounds (Lemmas A.1 and A.2 below). Before
we proceed, it will be useful to express the update rule ( OGD) in the following equivalent form:
x(t)∶=arg max
x∈X{Ψ(t)
x(x)∶=⟨x,m(t)
x⟩−1
ηBϕx(x∥ˆx(t))},
ˆx(t+1)∶=arg max
ˆx∈X{ˆΨ(t)
x(ˆx)∶=⟨ˆx,u(t)
x⟩−1
ηBϕx(ˆx∥ˆx(t))}.(5)
Here, Bϕx(⋅∥⋅)denotes the Bregman divergence induced by the (squared) Euclidean regularizer
ϕx∶x↦1
2∥x∥2
2; namely, Bϕx(x∥x′)∶=ϕ(x)−ϕ(x′)−⟨∇ϕ(x′),x−x′⟩=1
2∥x−x′∥2
2, forx,x′∈X.
The update rule (5)for general Bregman divergences will be referred to as optimistic mirror descent
(OMD).
A.1.1 Dynamic RVU bounds
The first key ingredient that we need for the proof of Theorem 3.3 is the property of regret bounded
by variation in utilities (RVU) , in the sense of Syrgkanis et al. [Syr+15], but with respect to dynamic
regret; such a bound is established below, and is analogous to that obtained by Zhang et al. [Zha+22].
Lemma A.1 (RVU bound for dynamic regret) .Consider any sequence of utilities (u(1)
x, . . . ,u(T)
x)
up to time T∈N. The dynamic regret (1)ofOGDwith respect to any sequence of comparators
(x(1,⋆), . . . ,x(T,⋆))∈XTcan be bounded by
D2
X
2η+DX
ηT−1
∑
t=1∥x(t+1,⋆)−x(t,⋆)∥2+ηT
∑
t=1∥u(t)
x−m(t)
x∥2
2
−1
2ηT
∑
t=1(∥x(t)−ˆx(t)∥2
2+∥x(t)−ˆx(t+1)∥2
2). (6)
In the special case of external regret— x(1,⋆)=x(2,⋆)=⋅⋅⋅=x(T,⋆)—(6)recovers the bound for
OGDof Syrgkanis et al. [Syr+15]. The key takeaway from Lemma A.1 is that the overhead of dynamic
regret in (6) grows with the first-order variation of the sequence of comparators.
Proof of Lemma A.1. First, by (1/η)-strong convexity of the function Ψ(t)
x(defined in (5)) for any
time t∈[[T]], we have that
⟨x(t),m(t)
x⟩−1
2η∥x(t)−ˆx(t)∥2
2−⟨ˆx(t+1),m(t)
x⟩+1
2η∥ˆx(t+1)−ˆx(t)∥2
2≥1
2η∥x(t)−ˆx(t+1)∥2
2,(7)
where we used [Sha12, Lemma 2.8, p. 135]. Similarly, by (1/η)-strong convexity of the function
ˆΨ(t)
x(defined in (5)) for any time t∈[[T]], we have that for any comparator x(t,⋆)∈X,
⟨ˆx(t+1),u(t)
x⟩−1
2η∥ˆx(t+1)−ˆx(t)∥2
2−⟨x(t,⋆),u(t)
x⟩+1
2η∥x(t,⋆)−ˆx(t)∥2
2
≥1
2η∥ˆx(t+1)−x(t,⋆)∥2
2. (8)
20Thus, adding (7) and (8),
⟨x(t,⋆)−ˆx(t+1),u(t)
x⟩+⟨ˆx(t+1)−x(t),m(t)
x⟩≤1
2η(∥ˆx(t)−x(t,⋆)∥2
2−∥ˆx(t+1)−x(t,⋆)∥2
2)
−1
2η(∥x(t)−ˆx(t)∥2
2+∥x(t)−ˆx(t+1)∥2
2). (9)
We further see that
⟨x(t,⋆)−x(t),u(t)
x⟩=⟨x(t)−ˆx(t+1),m(t)
x−u(t)
x⟩+⟨x(t,⋆)−ˆx(t+1),u(t)
x⟩
+⟨ˆx(t+1)−x(t),m(t)
x⟩. (10)
Now the first term on the right-hand side can be upper bounded using the fact that, by (7)and(8),
⟨x(t)−ˆx(t+1),m(t)
x−u(t)
x⟩≥1
η∥ˆx(t+1)−x(t)∥2
2/Leftr⫯g⊸tl⫯ne⇒∥ˆx(t+1)−x(t)∥2≤η∥m(t)
x−u(t)
x∥2,
by Cauchy-Schwarz, in turn implying that ⟨x(t)−ˆx(t+1),m(t)
x−u(t)
x⟩≤η∥m(t)
x−u(t)
x∥2
2. Thus, the
proof follows by combining this bound with (9) and (10), along with the fact that
T
∑
t=1(∥ˆx(t)−x(t,⋆)∥2
2−∥ˆx(t+1)−x(t,⋆)∥2
2)≤∥ˆx(1)−x(1,⋆)∥2
2
+T−1
∑
t=1(∥ˆx(t+1)−x(t+1,⋆)∥2
2−∥ˆx(t+1)−x(t,⋆)∥2
2)
≤D2
X+2DXT−1
∑
t=1∥x(t+1,⋆)−x(t,⋆)∥2,
where the last bound follows since
∥ˆx(t+1)−x(t+1,⋆)∥2
2−∥ˆx(t+1)−x(t,⋆)∥2
2≤2DX∣∥ˆx(t+1)−x(t+1,⋆)∥2−∥ˆx(t+1)−x(t,⋆)∥2∣
≤2DX∥x(t+1,⋆)−x(t,⋆)∥2,
where we recall that DXdenotes the ℓ2-diameter of X.
As an aside, we remark that assuming that m(t)
x∶=0and∥u(t)
x∥2≤1 for any t∈[[T]], Lemma A.1
implies that dynamic regret can be upper bounded by O(√
(1+∑T−1
t=1∥x(t+1,⋆)−x(t,⋆)∥2)T), for
any(bounded)—potentially adversarially selected—sequence of utilities (u(1)
x, . . . ,u(T)
x), for η∶=√
D2
X
2T+DX∑T−1
t=1∥x(t+1,⋆)−x(t,⋆)∥2
T, which is a well-known result in online optimization [Zin03]; while
that requires setting the learning rate based on the first-order variation of the (optimal) comparators,
there are standard techniques that would allow bypassing that assumption.
Next, we provide an extension of Lemma A.1 to the more general OMDalgorithm under a broad
class of regularizers.
Lemma A.2 (Extension of Lemma A.1 beyond Euclidean regularization) .Consider a 1-strongly
convex continuously differentiable regularizer ϕwith respect to a norm ∥⋅∥such that (i) ∥∇ϕ(x)∥∗≤G
21for any x, and (ii) Bϕx(x∥x′)≤L∥x−x′∥for any x,x′. Then, for any sequence of utilities
(u(1)
x, . . . ,u(T)
x)up to time T∈Nthe dynamic regret (1)ofOMDwith respect to any sequence of
comparators (x(1,⋆), . . . ,x(T,⋆))∈XTcan be bounded as
Bϕx(x(1,⋆)∥ˆx(1))
η+L+2G
ηT−1
∑
t=1∥x(t+1,⋆)−x(t,⋆)∥+ηT
∑
t=1∥u(t)
x−m(t)
x∥2
∗
−1
2ηT
∑
t=1(∥x(t)−ˆx(t)∥2+∥x(t)−ˆx(t+1)∥2).
The proof is analogous to that of Lemma A.1, and relies on the well-known three-point identity
for the Bregman divergence:
Bϕx(x∥x′)=Bϕx(x∥x′′)+Bϕx(x′′∥x′)−⟨x−x′′,∇ϕ(x′)−∇ϕ(x′′)⟩. (11)
In particular, along with the assumptions of Lemma A.2 imposed on the regularizer ϕx,(11)implies
that the term ∑T−1
t=1(Bϕx(x(t+1,⋆)∥ˆx(t+1))−Bϕx(x(t,⋆)∥ˆx(t+1)))is equal to
T−1
∑
t=1(Bϕx(x(t+1,⋆)∥x(t,⋆))−⟨x(t+1,⋆)−x(t,⋆),∇ϕ(ˆx(t+1))−∇ϕ(x(t,⋆))⟩)
≤(L+2G)T−1
∑
t=1∥x(t+1,⋆)−x(t,⋆)∥,
sinceBϕx(x(t+1,⋆)∥x(t,⋆))≤L∥x(t+1,⋆)−x(t,⋆)∥(by assumption) and
⟨x(t+1,⋆)−x(t,⋆),∇ϕ(ˆx(t+1))−∇ϕ(x(t,⋆))⟩
≤∥x(t+1,⋆)−x(t,⋆)∥∥∇ϕ(ˆx(t+1))−∇ϕ(x(t,⋆))∥∗ (12)
≤(∥∇ϕ(ˆx(t+1))∥∗+∥∇ϕ(x(t,⋆))∥∗)∥x(t+1,⋆)−x(t,⋆)∥ (13)
≤2G∥x(t+1,⋆)−x(t,⋆)∥, (14)
where (12)follows from the Cauchy-Schwarz inequality; (13)uses the triangle inequality for the
dual norm ∥⋅∥∗; and (14) follows from the assumption of Lemma A.2 that ∥∇ϕ(⋅)∥∗≤G. The rest
of the proof of Lemma A.2 is analogous to Lemma A.1, and it is therefore omitted. One important
question here is whether Lemma A.2 can be extended under a broader class of regularizers; as we
explain in Section 3.4, this is related to improving Theorem 3.13.
A.1.2 Nonnegativity of dynamic regret
We next proceed with the proof of Property 3.1. For completeness, we first prove the following
special case [Zha+22]; the proof of Property 3.1 is then analogous.
Property A.3 (Special case of Property 3.1) .Suppose that Z∋z(t,⋆)=(x(t,⋆),y(t,⋆)))is a
Nash equilibrium of the t-th game, for any time t∈[[T]]. Then, for s(T)
x=(x(t,⋆))1≤t≤Tand
s(T)
y=(y(t,⋆))1≤t≤T,
DReg(T)
x(s(T)
x)+DReg(T)
y(s(T)
y)≥0.
22Proof. Letv(t)∶=⟨x(t,⋆),A(t)y(t,⋆)⟩be the value of the t-th game, for some t∈[[T]]. Then, we
have that v(t)=⟨x(t,⋆),A(t)y(t,⋆)⟩≤⟨x,A(t)y(t,⋆)⟩for any x∈X, since x(t,⋆)is a best response to
y(t,⋆); similarly, v(t)=⟨x(t,⋆),A(t)y(t,⋆)⟩≥⟨x(t,⋆),A(t)y⟩for any y∈Y. Hence, ⟨x(t),A(t)y(t,⋆)⟩−
⟨x(t,⋆),A(t)y(t)⟩≥0, or equivalently, ⟨x(t,⋆),u(t)
x⟩+⟨y(t,⋆),u(t)
y⟩≥0. But given that the game is
zero-sum, it holds that ⟨x(t),u(t)
x⟩+⟨y(t),u(t)
y⟩=0, so the last inequality can be in turn cast as
⟨x(t,⋆),u(t)
x⟩−⟨x(t),u(t)
x⟩+⟨y(t,⋆),u(t)
y⟩−⟨y(t),u(t)
y⟩≥0,
for any t∈[[T]]. As a result, summing over all t∈[[T]]we have shown that
DReg(T)
x(x(1,⋆), . . . ,x(T,⋆))+DReg(T)
y(y(1,⋆), . . . ,y(T,⋆))
=T
∑
t=1⟨x(t,⋆),u(t)
x⟩−⟨x(t),u(t)
x⟩+⟨y(t,⋆),u(t)
y⟩−⟨y(t),u(t)
y⟩≥0.
Property 3.1. Suppose that Z∋z(t,⋆)=(x(t,⋆),y(t,⋆))is an ϵ(t)-approximate Nash equilibrium of
thet-th game. Then, for s(T)
x=(x(t,⋆))1≤t≤Tands(T)
y=(y(t,⋆))1≤t≤T,
DReg(T)
x(s(T)
x)+DReg(T)
y(s(T)
y)≥−2T
∑
t=1ϵ(t).
Proof. Given that (x(t,⋆),y(t,⋆))∈Zis an ϵ(t)-approximate Nash equilibrium of the t-th game, it
follows that ⟨x(t,⋆),A(t)y(t,⋆)⟩≤⟨x(t),A(t)y(t,⋆)⟩+ϵ(t)
xand⟨x(t,⋆),A(t)y(t,⋆)⟩≥⟨x(t,⋆),A(t)y(t)⟩−ϵ(t)
y,
for some ϵ(t)
x, ϵ(t)
y≤ϵ(t). Thus, we have that ⟨x(t),A(t)y(t,⋆)⟩≥⟨x(t,⋆),A(t)y(t)⟩−ϵ(t)
x−ϵ(t)
y, or
equivalently, ⟨x(t,⋆),u(t)
x⟩+⟨y(t,⋆),u(t)
y⟩≥−ϵ(t)
x−ϵ(t)
y≥−2ϵ(t). As a result,
⟨x(t,⋆),u(t)
x⟩−⟨x(t),u(t)
x⟩+⟨y(t,⋆),u(t)
y⟩−⟨y(t),u(t)
y⟩≥−2ϵ(t), (15)
for any t∈[[T]], and the statement follows by summing (15) over all t∈[[T]].
In fact, as we show below (in Property A.4), Property A.3 is a more general consequence of the
minimax theorem. In particular, for a nonlinear online learning problem, we define dynamic regret
with respect to a sequence of comparators (x(1,⋆), . . . ,x(T,⋆))∈XTas follows:
DReg(T)
x(x(1,⋆), . . . ,x(T,⋆))∶=T
∑
t=1(u(t)
x(x(t,⋆))−u(t)
x(x(t))), (16)
where u(1)
x, . . . , u(T)
x∶x↦Rare the continuous utility functions observed by the learner, which
could be in general nonconcave, and (x(t))1≤t≤Tis the sequence of strategies produced by the
learner; (16)generalizes the notion of dynamic regret (1)in online linear optimization, that is, when
u(t)
x∶x↦⟨x,u(t)
x⟩, where u(t)
x∈Rdxfor any time t∈[[T]].
Property A.4. Suppose that f(t)∶X×Y→Ris a continuous function such that for any t∈[[T]],
min
x∈Xmax
y∈Yf(t)(x,y)=max
y∈Ymin
x∈Xf(t)(x,y).
Let also x(t,⋆)∈arg minx∈Xmaxy∈Yf(t)(x,y)andy(t,⋆)∈arg maxy∈Yminx∈Xf(t)(x,y), for any
t∈[[T]]. Then, for s(T)
x=(x(t,⋆))1≤t≤Tands(T)
y=(y(t,⋆))1≤t≤T,
DReg(T)
x(s(T)
x)+DReg(T)
y(s(T)
y)≥0.
23Proof. By definition of dynamic regret (16), it suffices to show that f(t)(x(t),y(t,⋆))≥f(t)(x(t,⋆),y(t)),
for any time t∈[[T]]. Indeed,
f(t)(x(t),y(t,⋆))≥min
x∈Xf(t)(x,y(t,⋆)) (17)
=max
y∈Ymin
x∈Xf(t)(x,y) (18)
=min
x∈Xmax
y∈Yf(t)(x,y) (19)
=max
y∈Yf(t)(x(t,⋆),y) (20)
≥f(t)(x(t,⋆),y(t)), (21)
where (17)and(21)are obvious; (18)and(20)follow from the definition of y(t,⋆)∈Yandx(t,⋆)∈X,
respectively; and (19) holds by assumption. This concludes the proof.
A.1.3 Variation of the Nash equilibria
In our next example, we point out the standard observation that an arbitrarily small change in the
entries of the payoff matrix can lead to a substantial deviation in the Nash equilibrium.
Example A.5.Consider a 2 ×2 (two-player) zero-sum game, where X∶=∆2,Y∶=∆2, described by
the payoff matrix
A∶=[2δ0
0δ], (22)
for some δ>0. Then, it is easy to see that the unique Nash equilibrium of this game is such that
x⋆,y⋆∶=(1
3,2
3)∈∆2. Suppose now that the original payoff matrix (22)is perturbed to a new matrix
A′∶=[δ0
0 2δ]. (23)
The new (unique) Nash equilibrium now reads x⋆,y⋆∶=(2
3,1
3)∈∆2. We conclude that an arbitrarily
small deviation in the entries of the payoff matrix can lead to a non-trivial change in the Nash
equilibrium.
Next, we leverage the simple observation of the example above to establish Proposition 3.2, the
statement of which is recalled below.
Proposition 3.2. For any T≥4, there is a sequence of Tgames such that V(T)
NE≥T
2whileV(T)
ϵ−NE≤δ,
for any δ>0.
Proof. We consider a sequence of T≥4 games such that X,Y∶=∆2and
A(t)=⎧⎪⎪⎨⎪⎪⎩A iftmod 2=1,
A′iftmod 2=0,
where A,A′are the payoff matrices defined in (22) and(23), and are parameterized by δ>0
(Example A.5). Then, the exact Nash equilibria read
x(t,⋆),y(t,⋆)=⎧⎪⎪⎨⎪⎪⎩(1
3,2
3)iftmod 2=1,
(2
3,1
3)iftmod 2=0.
24As a result, it follows that V(T)
NE∶=∑T−1
t=1∥z(t+1,⋆)−z(t,⋆)∥2=2
3(T−1)≥T
2, forT≥4. In contrast, it
is clear that V(T)
ϵ−NE≤CδT, which follows by simply considering the sequence of strategies wherein
both players are always selecting actions uniformly at random; we recall that C>0 here is the value
that parameterizes V(T)
ϵ−NE. Thus, taking δ∶=δ′
CT, for some arbitrarily small δ′>0, concludes the
proof.
In the above sequence of games, the variation measure W(T)
Ais in fact also arbitrarily small as
δ→0; this reflects the fact that, although the exact Nash equilibria change abruptly, the sequence
of payoff matrices exhibits small variation. Nevertheless, we next point out a modified sequence of
games where Vϵ−NEremains small even though both W(T)
AandV(T)
NEcan be unbounded.
Proposition A.6. For any T≥4, there is a sequence of games such that V(T)
NE,W(T)
A=Ω(T)while
V(T)
ϵ−NE≤δ, for any δ>0.
Proof. We consider a sequence of T≥4 games such that X,Y∶=∆2and
A(t)=⎧⎪⎪⎨⎪⎪⎩A+12×2iftmod 2=1,
A′iftmod 2=0,
where A,A′are the payoff matrices defined in (22)and(23), and 12×2denotes the all-ones 2 ×2
matrix. Given that x⊺(A+12×2)y=x⊺Ay+1, for any x,y∈∆2, it follows that the set of ϵ-
approximate Nash equilibria of each game coincides with the set of ϵ-approximate Nash equilibria
of the corresponding game in Proposition 3.2. Thus, by the same argument as in Proposition 3.2
earlier, we have that V(T)
NE≥T
2andV(T)
ϵ−NE≤CδT. It is also clear that W(T)
A=Ω(T), concluding the
proof.
In words, the phenomenon identified above occurs because W(T)
Ais affected by perturbations in
the payoff matrices that do not strategically alter the game—namely, the addition of the matrix
12×2.
A.1.4 Proof of Theorem 3.3
Next, we proceed with the proof of one of our main results, namely Theorem 3.3. The key ingredient
is Theorem A.7 below, which bounds the second-order path length of OGDin terms of the considered
variation measures.
Theorem A.7. Suppose that both players employ OGDwith learning rate η≤1
4Lin a time-varying
bilinear saddle-point problem, where L∶=max 1≤t≤T∥A(t)∥2. Then, for any time horizon T∈N,
T
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2)≤2D2
Z+4η2L2∥Z∥2
2+4DZV(T)
ϵ−NE+8η2∥Z∥2
2V(T)
A.
Proof of Theorem A.7. First, for any t≥2 we have that ∥u(t)
x−m(t)
x∥2
2is equal to
∥A(t)y(t)−A(t−1)y(t−1)∥2
2≤2∥A(t)(y(t)−y(t−1))∥2
2+2∥(A(t)−A(t−1))y(t−1)∥2
2 (24)
≤2∥A(t)∥2
2∥y(t)−y(t−1)∥2
2+2∥A(t)−A(t−1)∥2
2∥y(t−1)∥2
2 (25)
≤2L2∥y(t)−y(t−1)∥2
2+2∥Y∥2
2∥A(t)−A(t−1)∥2
2, (26)
25where (24)uses the triangle inequality for the norm ∥⋅∥2along with the inequality 2 ab≤a2+b2for
anya, b∈R;(25)follows from the definition of the operator norm; and (26)uses the assumption
that∥A(t)∥2≤Land∥y∥2≤∥Y∥2for any y∈Y. A similar derivaiton shows that for t≥2,
∥u(t)
y−m(t)
y∥2
2≤2L2∥x(t)−x(t−1)∥2
2+2∥X∥2
2∥A(t)−A(t−1)∥2
2. (27)
Further, for t=1 we have that ∥u(1)
x−m(1)
x∥2=∥u(1)
x∥2=∥−A(1)y(1)∥2≤L∥Y∥2, and∥u(1)
y−m(1)
y∥2=
∥u(1)
y∥2=∥(A(1))⊺x(1)∥2≤L∥X∥2. Next, we will use the following simple corollary, which follows
similarly to Lemma A.1.
Corollary A.8. For any sequence s(T)
z∶=(z(t,⋆))1≤t≤T, the dynamic regret DReg(T)
z(s(T)
z)∶=
DReg(T)
x(s(T)
x)+DReg(T)
y(s(T)
y)can be bounded by
D2
Z
2η+DZ
ηT−1
∑
t=1∥z(t+1,⋆)−z(t,⋆)∥2+ηT
∑
t=1∥u(t)
z−m(t)
z∥2
2
−1
2ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2),
where m(t)
z∶=(m(t)
x,m(t)
y)andu(t)
z∶=(u(t)
x,u(t)
y)for any t∈[[T]].
As a result, combining (27) and (26) with Corollary A.8 applied for the dynamic regret
of both players with respect to the sequence of comparators ((x(t,⋆),y(t,⋆)))1≤t≤Tyields that
DReg(T)
x(x(1,⋆), . . . ,x(T,⋆))+DReg(T)
y(y(1,⋆), . . . ,y(T,⋆))is upper bounded by
D2
Z
2η+ηL2∥Z∥2
2+DZ
ηT−1
∑
t=1∥z(t+1,⋆)−z(t,⋆)∥2+2η∥Z∥2
2V(T)
A
−1
4ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2),
where we used the fact that
2ηL2T
∑
t=2∥z(t)−z(t−1)∥2
2−1
4ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2)
≤(2ηL2−1
8η)T
∑
t=2∥z(t)−z(t−1)∥2
2≤0,
for any leaning rate η≤1
4L. Finally, using the fact that
DReg(T)
x(x(1,⋆), . . . ,x(T,⋆))+DReg(T)
y(y(1,⋆), . . . ,y(T,⋆))≥−2T
∑
t=1ϵ(t)
for a suitable sequence of ϵ(t)-approximate Nash equilibria (Property 3.1)—one that attains the
variation measure V(T)
ϵ−NE—yields that
0≤D2
Z
2η+ηL2∥Z∥2
2+DZ
ηV(T)
ϵ−NE+2η∥Z∥2
2V(T)
A−1
4ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2),
26where in the above derivation it suffices if the parameter CofV(T)
ϵ−NEis such that
2≤DZ
ηC⇐⇒ C=C(η)≥2η
DZ. (28)
Thus, rearranging the last displayed inequality concludes the proof.
Next, we refine this theorem in time-varying games in which the deviation of the payoff matrices
is bounded by the deviation of the players’ strategies, in the following formal sense.
Corollary A.9. Suppose that both players employ OGDwith learning rate η≤min{1
4L,1
8W∥Z∥}in a
time-varying bilinear saddle-point problem, where L∶=max 1≤t≤T∥A(t)∥2andV(T)
A≤W2∑T−1
t=1∥z(t+1)−
z(t)∥2
2, for some parameter W∈R>0. Then, for any time horizon T∈N,
T
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥ˆz(t)−ˆz(t+1)∥2
2)≤4D2
Z+8η2L2∥Z∥2
2+8DZV(T)
NE.
Proof. Following the proof of Theorem A.7, we have that for any η≤1
4L,
0≤D2
Z
2η+ηL2∥Z∥2
2+DZ
ηV(T)
NE+2η∥Z∥2
2V(T)
A−1
4ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2).
Further, for η≤1
8W∥Z∥2,
2η∥Z∥2
2V(T)
A−1
8ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2)
≤(2η∥Z∥2
2W2−1
16η)T−1
∑
t=1∥z(t+1)−z(t)∥2
2≤0.
Thus, we have shown that
0≤D2
Z
2η+ηL2∥Z∥2
2+DZ
ηV(T)
NE−1
8ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2),
and rearranging concludes the proof.
Thus, in such time-varying games it is the first-order variation term, V(T)
NE, that will drive our
convergence bounds.
Now before proving Theorem 3.3, we state the connection between the equilibrium gap and
the deviation of the players’ strategies (∥z(t)−ˆz(t)∥2+∥z(t)−ˆz(t+1)∥2). In particular, the following
claim can be extracted by [Ana+22b, Claim A.14]. (We caution that we use a slightly different
indexing for the secondary sequence (ˆx(t)
i)in the definition of OMD(5) compared to [Ana+22b].)
Claim A.10. Suppose that the sequences (x(t))1≤t≤Tand(ˆx(t))1≤t≤T+1are produced by OMDunder
aG-smooth regularizer 1-strongly convex with respect to a norm ∥⋅∥. Then, for any time t∈[[T]]
and any x∈X,
⟨x(t),u(t)
x⟩≥⟨x,u(t)
x⟩−GDX
η∥ˆx(t+1)−ˆx(t)∥−∥u(t)
x∥∗∥x(t)−ˆx(t+1)∥.
27We are now ready to prove Theorem 3.3, the precise version of which is stated below.
Theorem A.11 (Detailed version of Theorem 3.3) .Suppose that both players employ OGDwith
learning rate η=1
4Lin a time-varying bilinear saddle-point problem, where L∶=max 1≤t≤T∥A(t)∥2.
Then,
T
∑
t=1(EqGap(t)(z(t)))2
≤2L2(4DZ+∥Z∥2)2(2D2
Z+4η2L2∥Z∥2
2+4DZV(T)
ϵ−NE+8η2∥Z∥2
2V(T)
A),
where(z(t))1≤t≤Tis the sequence of joint strategy profiles produced by OGD.
Proof. Let us fix t∈[[T]]. For convenience, we denote by BR(t)
x(x(t))∶=maxx∈X{⟨x,u(t)
x⟩}−
⟨x(t),u(t)
x⟩, the best response gap of Player’s xstrategy x(t)∈X, and similarly for BR(t)
y(y(t)). By
definition, it holds that EqGap(t)(z(t))∶=max{BR(t)
x(x(t)),BR(t)
y(y(t))}. By Claim A.10, we have
that
BR(t)
x(x(t))≤DX
η∥ˆx(t+1)−ˆx(t)∥2+∥u(t)
x∥2∥x(t)−ˆx(t+1)∥2 (29)
≤4LDX∥ˆx(t+1)−ˆx(t)∥2+L∥Y∥2∥x(t)−ˆx(t+1)∥2 (30)
≤L(4DZ+∥Z∥2)(∥x(t)−ˆx(t)∥2+∥x(t)−ˆx(t+1)∥2), (31)
where (29)follows from Claim A.10 for G=1 (since the squared Euclidean regularizer ϕx∶x↦1
2∥x∥2
2)
is trivially 1-smooth; (30)uses the fact that η∶=1
4Land∥u(t)
x∥2=∥−A(t)y(t)∥2≤L∥Y∥; and (31)
follows from the triangle inequality. A similar derivation shows that
BR(t)
y(y(t))≤L(4DZ+∥Z∥2)(∥y(t)−ˆy(t)∥2+∥y(t)−ˆy(t+1)∥2). (32)
Thus,
T
∑
t=1(EqGap(t)(z(t)))2
=T
∑
t=1(max{BR(t)
x(x(t)),BR(t)
y(y(t))})2
≤T
∑
t=1((BR(t)
x(x(t)))2
+(BR(t)
y(y(t)))2
)
≤2L2(4DZ+∥Z∥2)2T
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2), (33)
where the last bound uses (31)and(32). Combining (33)with Theorem A.7 concludes the proof.
A.1.5 Variation-dependent regret bounds
Here we state for completeness an implication of Theorem 3.3 for deriving variation-dependent
regret bounds in time-varying bilinear saddle-point problems; cf. [Zha+22].
Corollary A.12. In the setup of Theorem A.7, it holds that
Reg(T)
x≤D2
X
η+8ηL2D2
Z+ηL2∥Y∥2
2+16η3L4∥Z∥2
2+16ηL2DZV(T)
NE
+(2η∥Y∥2
2+32η3L2∥Z∥2
2)V(T)
A,
28and
Reg(T)
y≤D2
Y
η+8ηL2D2
Z+ηL2∥X∥2
2+16η3L4∥Z∥2
2+16ηL2DZV(T)
NE
+(2η∥X∥2
2+32η3L2∥Z∥2
2)V(T)
A.
Proof. First, applying Lemma A.1 under x(1,⋆)=⋅⋅⋅=x(T,⋆), we have
Reg(T)
x≤D2
X
η+ηL2∥Y∥2
2+2ηL2T
∑
t=2∥y(t)−y(t−1)∥2
2+2η∥Y∥2
2T
∑
t=2∥A(t)−A(t−1)∥2
2, (34)
and similarly,
Reg(T)
y≤D2
Y
η+ηL2∥X∥2
2+2ηL2T
∑
t=2∥x(t)−x(t−1)∥2
2+2η∥X∥2
2T
∑
t=2∥A(t)−A(t−1)∥2
2. (35)
Now, by Theorem A.7 we have
T
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2)≤2D2
Z+4η2L2∥Z∥2
2+4DZV(T)
NE+8η2∥Z∥2
2V(T)
A. (36)
Further,
T
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2)≥T
∑
t=1(∥x(t)−ˆx(t)∥2
2+∥x(t)−ˆx(t+1)∥2
2)
≥1
2T
∑
t=2∥x(t)−x(t−1)∥2
2.
Combining this bound with (36)and(35)gives the claimed regret bound on Reg(T)
y, and a similar
derivation also gives the claimed bound on Reg(T)
x.
Hence, selecting optimally the learning rate gives an O(√
1+V(T)
NE+V(T)
A)bound on the indi-
vidual regret of each player; while that optimal value depends on the variation measures, which
are not known to the learners, there are techniques that would allow bypassing this [Zha+22].
Corollary A.12 can also be readily parameterized in terms of the improved variation measure V(T)
ϵ−NE.
A.1.6 Meta-learning
We next provide the implication of Theorem 3.3 in the meta-learning setting. We first make a
remark regarding the effect of the prediction of OGDto Theorem 3.3, and how that relates to an
assumption present in earlier work on this problem [Har+23].
Remark A.13 (Improved predictions) .Throughout Section 3.1, we have considered the standard
prediction m(t)
x∶=u(t−1)
x=−A(t−1)y(t−1)fort≥2, and similarly for Player y. It is easy to see that
using the predictions
m(t)
x∶=−A(t)y(t−1)andm(t)
y∶=(A(t))⊺x(t−1)(37)
29fort≥1 (where z(0)∶=ˆz(1)) entirely removes the dependency on V(T)
Aon all our convergence
bounds. While such a prediction cannot be implemented in the standard online learning model,
there are settings in which we might, for example, know the sequence of matrices in advance; the
meta-learning setting offers such examples, and indeed, Harris et al. [Har+23] use the improved
prediction of (37).
Proposition A.14 (Meta-learning) .Suppose that both players employ OGDwith learning rate η=1
4L,
where L∶=max 1≤h≤H∥A(h)∥2, and the prediction of (37) in a meta-learning bilinear saddle-point
problem with H∈Ngames, each repeated for m∈Nconsecutive iterations. Then, for an average
game,⎡⎢⎢⎢⎢⎢P
Hϵ2+P′V(H)
NE
Hϵ2⎤⎥⎥⎥⎥⎥+1 (38)
iterations suffice to reach an ϵ-approximate Nash equilibrium, where P∶=4L2(4DZ+∥Z∥2)2D2
Z,
P′∶=8L2(4DZ+∥Z∥2)2DZ, and
V(H)
NE∶= inf
z(h,⋆)∈Z(h,⋆),∀h∈[[H]]H−1
∑
h=1∥z(h+1,⋆)−z(h,⋆)∥2.
The proof is a direct application of Theorem A.11, where we remark that the term depending on
V(T)
Aand the term 4 η2L2∥Z∥2
2from Theorem A.11 are eliminated because of the improved prediction
of Remark A.13. More precisely, if we let m(h)∈Nbe the number of iterations required so that the
equilibrium gap is at most ϵat the h-th game, then Theorem A.11 implies that
P+P′V(H)
NE≥T
∑
t=1(EqGap(t)(z(t)))2
>ϵ2H
∑
h=1(m(h)−1).
Solving with respect to1
H∑H
h=1m(h)thus yields Proposition A.14.
The first term in the iteration complexity bound (38)vanishes in the meta-learning regime—as
the number of games increases H≫1—while the second term is proportional toV(H)
NE
H, a natural
similarity measure; (38)always recovers the m−1/2rate, but offers significant gains if the games as
similar, in the sense thatV(H)
NE
H≪1. It is worth noting that, unlike the similarity measure derived
by Harris et al. [Har+23],V(H)
NE
Hdepends on the order of the games. As a result, a natural problem
that arises in settings where the sequence of games is known in advance is to optimize the order of
the games so as to minimize V(H)
NE; the key challenge of course is to minimize V(H)
NEwithout actually
solving each game since that without defeat the purpose. We further remark that Proposition A.14
can be readily extended even if each game in the meta-learning sequence is not repeated for the
same number of iterations.
Another natural question in this context is to compare the similarity metric of Proposition A.14
compared to that obtained by Harris et al. [Har+23]—namely minz∈Z∑H
h=1∥z(h,⋆)−z∥2
2, where we
can assume here that z(h,⋆)is the unique Nash equilibrium of the h-th game. Below, we observe
thatV(H)
NEcan be arbitrarily smaller.
Proposition A.15. There exists a sequence of Hgames such that V(H)
NE=O(1)while
min
z∈ZH
∑
h=1∥z(h,⋆)−z∥2
2≥Ω(H)
30.
Proof. We let X,Y=∆3. We consider a sequence of Hgames so that x(h,⋆)=y(h,⋆)for all h∈[[H]],
and the sequence of points (x(h,⋆))1≤h≤Hforms a regular H-gon within the relative interior of ∆3
with a constant radius. We note that a unique fully-mixed equilibrium ((α, β, γ),(α, β, γ))∈∆3×∆3
can be obtained by considering the diagonal payoff matrix
⎡⎢⎢⎢⎢⎢⎣1/α0 0
0 1/β0
0 0 1 /γ⎤⎥⎥⎥⎥⎥⎦.
It is easy to see that the above sequence of games establishes the claim.
A.1.7 Polymatrix zero-sum games
As we claimed earlier in Section 3.1.1, Theorem 3.3 can be extended to more general time-varying VIs
as long as the MVI property holds. For concreteness, here we focus on a multi-player generalization
of two-player zero-sum games, namely polymatrix zero-sum games [Cai+16]. More precisely, here it
is assumed that there is an underlying n-node graph so that each player is uniquely associated with
a node in the graph. The utility of each player i∈[[n]]can be expressed as ∑i′∈Nix⊺
iAi,i′xi′, where
Nidenotes the set of neighbors of Player iandAi,i′∈Rdi×di′. It is also assumed that A⊺
i,i′=−Ai′,i,
for any edge {i, i′}. Ifz∶=(x1, . . . ,xn)∈Z, the corresponding operator is defined as
F(z)=−⎛
⎝∑
i∈N1A1,ixi, . . . ,∑
i∈NnAn,ixi⎞
⎠.
It is easy to see that Fismonotone , in that ⟨z−z′, F(z)−F(z′)⟩≥0, for any z,z′∈Z. Now,
for any ϵ-approximate Nash equilibrium z⋆it holds, by definition, that ⟨z−z⋆, F(z⋆)⟩≥−nϵ, for
anyz∈Z. Thus, by monotonicity, ⟨z−z⋆, F(z)⟩≥⟨z−z⋆, F(z⋆)⟩≥−nϵ, for any z∈Z. In other
words, approximate Nash equilibria approximately satisfy the MVI property. This suffices to directly
extend Property 3.1 to time-varying polymatrix zero-sum games. The rest of the argument of
Theorem 3.3 is analogous, leading to the following result. Below, we define V(T)
ϵ−NEas in (2), for a
suitable parameter C>0, and V(T)
A∶=∑T−1
t=1∑n
i=1∑i′∈Ni′∥A(t+1)
i,i′−A(t)
i,i′∥2
2.
Theorem A.16. Suppose that each player employs OGDwith a sufficiently small learning rate in a
sequence of time-varying polymatrix zero-sum games. Then,
T
∑
t=1(EqGap(t)(z(t)))2
=O(1+V(T)
ϵ−NE+V(T)
A),
where(z(t))1≤t≤Tis the sequence of joint strategies produced by OGD.
A.1.8 General variational inequalities
Although our main focus in this paper is on the convergence of learning algorithms in time-varying
games, our techniques could also be of interest for solving (static) general variational inequality
(VI) problems.
31In particular, let F∶Z→Zbe a single-valued operator. Solving general VIs is well-known
to be computationally intractable, and so instead focus has been on identifying broad subclasses
that elude those intractability barriers (see the references below). Our framework in Section 3.1
motivates introducing the following measure of complexity for a VI problem:
C(F)∶= inf
z(1,⋆),...,z(T,⋆)∈ZT−1
∑
t=1∥z(t+1,⋆)−z(t,⋆)∥2, (39)
subject to
DReg(T)
z(z(1,⋆), . . . ,z(T,⋆))≥0⇐⇒T
∑
t=1⟨z(t)−z(t,⋆), F(z(t))⟩≥0. (40)
In words, (39)expresses the infimum first-order variation that a sequence of comparators must have
in order to guarantee nonnegative dynamic regret (40); it is evident that (40)always admits a feasible
sequence, namely s(T)
z∶=(z(t))1≤t≤T. We note that, in accordance to our results in Section 3.1, one
can also consider an approximate version of the complexity measure (39), which could behave much
more favorably (recall Proposition 3.2).
Now in a (static) bilinear saddle-point problem, it holds that C(F)=0 given that there exists a
static comparator that guarantees nonnegativity of the dynamic regret. More broadly, our techniques
imply O(poly(1/ϵ))iteration-complexity bounds for any VI problem such that C(F)≤CT1−ω, for a
time-independent parameter C>0 and ω∈(0,1]:
Proposition A.17. Consider a variational inequality problem described with the operator F∶Z→Z
such that FisL-Lipschitz continuous, in the sense that ∥F(z)−F(z′)∥2≤L∥z−z′∥2, and C(F)≤
CT1−ωforC>0andω∈(0,1]. Then, OGDwith learning rate η=1
4Lreaches an ϵ-strong solution
z⋆∈ZinO(ϵ−2/ω)iterations; that is, ⟨z−z⋆, F(z⋆)⟩≥−ϵfor any z∈Z.
This result should be viewed as part of an ongoing effort to characterize the class of variational
inequalities (VIs) that are amenable to efficient algorithms; see [DDJ21; CZ22; Azi+20; BMW21;
CP04; DL15; MV21; MRS20; Nou+19; Son+20; YKH20; Das22], and the many references therein.
It is worth comparing (39)with another natural complexity measure, namely infz⋆∈Z∑T
t=1⟨z(t)−
z⋆, F(z(t))⟩; the latter measures how negative (external) regret can be, and has already proven
useful in certain settings that go bilinear saddle-point problems [YM23], although unlike (39)it
does not appear to be of much use in characterizing time-varying bilinear saddle-point problems. In
this context, O(poly(1/ϵ))iteration-complexity bounds can also be established whenever
•infz⋆∈Z∑T
t=1⟨z(t)−z⋆, F(z(t))⟩≥−CT1−ωfor a time-invariant C>0, or
•infz⋆∈Z∑T
t=1⟨z(t)−z⋆, F(z(t))⟩≥−C∑T−1
t=1∥z(t+1)−z(t)∥2
2, for a sufficiently small C>0.
Following [YM23], identifying VIs that satisfy those relaxed conditions but not the MVI property
is an interesting direction. In particular, it is important to understand if those relaxations can shed
led more light into the convergence properties of OGDin Shapley’s two-player zero-sum stochastic
games .
A.2 Proofs from Section 3.2
In this subsection, we provide the proofs from Section 3.2, leading to our main result in Theorem 3.5.
32Let us first introduce some additional notation. We let f(t)∶X×Y→Rbe a continuously
differentiable function for any t∈[[T]].6We recall that in Section 3.2 it is assumed that the objective
function changes only after m∈N(consecutive) repetitions, which is akin to the meta-learning
setting. Analogously to our setup for bilinear saddle-point problems (Section 3.1), it is assumed that
Player xis endeavoring to minimizing the objective function, while Player yis trying to maximize
it. We will denote by Reg(T)
L,x(x⋆)∶=∑T
t=1⟨x(t)−x⋆,−u(t)
x⟩andReg(T)
L,y(y⋆)∶=∑T
t=1⟨y⋆−y(t),u(t)
y⟩,
where u(t)
x∶=−∇xf(x(t),y(t))andu(t)
y∶=∇yf(x(t),y(t))for any t∈[[T]]; similar notation is used
for DReg(T)
L,x,DReg(T)
L,y.
Furthermore, we let s(T)
z=((x(t,⋆),y(t,⋆)))1≤t≤T, so that x(t,⋆)=x(h,⋆)andy(t,⋆)=y(h,⋆)for any
t∈[[T]]such that ⌊(t−1)/m⌋=h∈[[H]]. The first important step in our analysis is that, following
the proof of Lemma A.1,
DReg(T)
L,x(s(T)
x)≤1
2ηH
∑
h=1∥ˆx(h,1)−x(h,⋆)∥2
2−∥ˆx(h,m+1)−x(h,⋆)∥2
2+ηT
∑
t=1∥u(t)
x−m(t)
x∥2
2
−1
2ηT
∑
t=1(∥x(t)−ˆx(t)∥2
2+∥x(t)−ˆx(t+1)∥2
2), (41)
where ˆx(h,k)∶=ˆx((h−1)×m)+k)for any (h, k)∈[[H]]×[[m]],ˆx(h,m+1)∶=ˆx(h+1,1)forh∈[[H−1]], and
ˆx(H,m+1)∶=ˆx(T+1). Similarly,
DReg(T)
L,y(s(T)
y)≤1
2ηH
∑
h=1∥ˆy(h,1)−y(h,⋆)∥2
2−∥ˆy(h,m+1)−y(h,⋆)∥2
2+ηT
∑
t=1∥u(t)
y−m(t)
y∥2
2
−1
2ηT
∑
t=1(∥y(t)−ˆy(t)∥2
2+∥y(t)−ˆy(t+1)∥2
2). (42)
Next, we will use the following key observation, which lower bounds the sum of the players’
(external) regrets under strong convexity-concavity.
Lemma A.18. Suppose that f∶X×Y→Ris aµ-strongly convex-concave function with respect to
∥⋅∥2. Then, for any Nash equilibrium z⋆=(x⋆,y⋆)∈Z,
Reg(m)
L,x(x⋆)+Reg(m)
L,y(y⋆)≥µ
2m
∑
t=1∥z(t)−z⋆∥2
2.
Proof. First, by µ-strong convexity of f(x,⋅), we have that for any time t∈[[m]],
⟨x(t)−x⋆,∇xf(x(t),y(t))⟩≥f(x(t),y(t))−f(x⋆,y(t))+µ
2∥x(t)−x⋆∥2
2. (43)
Similarly, by µ-strong concavity of f(⋅,y), we have that for any time t∈[[m]],
⟨y⋆−y(t),∇yf(x(t),y(t))⟩≥f(x(t),y⋆)−f(x(t),y(t))+µ
2∥y(t)−y⋆∥2
2. (44)
Further, for any Nash equilibrium (x⋆,y⋆)∈Zit holds that f(x(t),y⋆)≥f(x(t),y(t))≥f(x⋆,y(t)).
Combining this fact with (43) and (44) and summing over all t∈[[m]]gives the statement.
6In case the interior (X×Y)○is empty, here and throughout this paper we tacitly posit differentiability on a closed
and convex neighborhood ˜X×˜Ysuch that X⊆˜X○andY⊆˜Y○. This assumption suffices for our arguments; see, for
example, the treatment of Daskalakis, Foster, and Golowich [DFG20].
33In turn, this readily implies the following lower bound for the dynamic regret.
Lemma A.19. Suppose that f(h)∶X×Y→Ris aµ-strongly convex-concave function with respect
to∥⋅∥2, for any h∈[[H]]. Consider a sequence s(T)
z=((x(t,⋆),y(t,⋆)))1≤t≤T, so that x(t,⋆)=x(h,⋆)
andy(t,⋆)=y(h,⋆)for any t∈[[T]]such that ⌊(t−1)/m⌋=h∈[[H]]. If(x(h,⋆),y(h,⋆))∈Zis a Nash
equilibrium of f(h),
DReg(T)
L,x(s(T)
x)+DReg(T)
L,y(s(T)
y)≥µ
2H
∑
h=1m
∑
k=1∥z(h,k)−z(h,⋆)∥2
2,
where z(h,k)∶=z((h−1)×m)+k)for any (h, k)∈[[H]]×[[m]].
We next combine this with the following monotonicity property of OGD: Ifz⋆is a Nash equilibrium,
∥ˆz(t)−z⋆∥2is a decreasing function in t[Har+23, Proposition C.10]. This leads to the following
refinement of Lemma A.19.
Lemma A.20. Under the assumptions of Lemma A.19, if additionally η≤1
2µ,
DReg(T)
L,x(s(T)
x)+DReg(T)
L,y(s(T)
y)+1
4ηT
∑
t=1∥z(t)−ˆz(t+1)∥2
2≥µm
4H
∑
h=1∥ˆz(h,m+1)−z(h,⋆)∥2
2.
Proof. By Lemma A.19,
DReg(T)
L,x(s(T)
x)+DReg(T)
L,y(s(T)
y)+1
4ηT
∑
t=1∥z(t)−ˆz(t+1)∥2
2
≥µ
2H
∑
h=1m
∑
k=1∥z(h,k)−z(h,⋆)∥2
2+1
4ηT
∑
t=1∥z(t)−ˆz(t+1)∥2
2
≥µ
4H
∑
h=1m
∑
k=1∥ˆz(h,k+1)−z(h,⋆)∥2
2 (45)
≥µm
4H
∑
h=1∥ˆz(h,m+1)−z(h,⋆)∥2
2, (46)
where (45)uses that1
4η≥µ
2along with Young’s inequality and triangle inequality, and (46)follows
from [Har+23, Proposition C.10].
Armed with this important lemma, we are ready to establish our main result (Theorem 3.5), the
detailed version of which is given below. We first recall that the function f∶X×Y→Ris said to be
L-smooth if ∥F(z)−F(z′)∥2≤L∥z−z′∥2, where F(z)∶=(∇xf(x,y),−∇yf(x,y)).
Theorem A.21 (Detailed version of Theorem 3.5) .Letf(h)∶X×Ybe aµ-strongly convex-concave
andL-smooth function, for h∈[[H]]. Suppose that both players employ OGDwith learning rate
η=min{1
8L,1
2µ}forTrepetitions, where T=m×Handm≥2
ηµ. Then,
T
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2)≤4D2
Z+8η2∥F(z(1))∥2
2+8S(H)
NE+16η2V(H)
∇f.
Thus,∑T
t=1(EqGap(t)(z(t)))2=O(1+S(H)
NE+V(H)
∇f).
34Proof. Combining Lemma A.20 with (41) and (42),
0≤1
2ηH
∑
h=1(∥ˆz(h,1)−z(h,⋆)∥2
2−2+ηµm
2∥ˆz(h,m+1)−z(h,⋆)∥2
2)
+ηT
∑
t=1∥u(t)
z−m(t)
z∥2
2−1
4ηT
∑
t=1(∥z(t)−ˆz(t)∥2
2+∥z(t)−ˆz(t+1)∥2
2),
for a sequence of Nash equilibria (z(h,⋆))1≤h≤H, where we used the notation u(t)
z∶=(u(t)
x,u(t)
y)and
m(t)
z∶=(m(t)
x,m(t)
y). Now we bound the first term of the right-hand side above as
1
2ηH
∑
h=1(∥ˆz(h,1)−z(h,⋆)∥2
2−2∥ˆz(h,m+1)−z(h,⋆)∥2
2)≤
1
2η∥ˆz(1,1)−z(1,⋆)∥2
2+1
2ηH−1
∑
h=1(∥ˆz(h+1,1)−z(h+1,⋆)∥2
2−2∥ˆz(h+1,1)−z(h,⋆)∥2
2),
where we used the fact that m≥2
ηµandˆz(h,m+1)=ˆz(h+1,1), forh∈[[H−1]]. Hence, continuing from
above,
1
2ηH
∑
h=1(∥ˆz(h,1)−z(h,⋆)∥2
2−2∥ˆz(h,m+1)−z(h,⋆)∥2
2)≤1
2η∥ˆz(1,1)−z(1,⋆)∥2
2
+1
ηH−1
∑
h=1∥z(h+1,⋆)−z(h,⋆)∥2
2,
since∥ˆz(h+1,1)−z(h+1,⋆)∥2
2≤2∥ˆz(h+1,1)−z(h,⋆)∥2
2+2∥z(h,⋆)−z(h+1,⋆)∥2
2, by the triangle inequality
and Young’s inequality. Moreover, for t≥2,
∥u(t)
z−u(t−1)
z∥2
2=∥F(t)(z(t))−F(t−1)(z(t−1))∥2
2≤2L2∥z(t)−z(t−1)∥2
2
+2∥F(t)(z(t−1))−F(t−1)(z(t−1))∥2
2,
byL-smoothness. As a result,
T−1
∑
t=1∥u(t+1)
z−u(t)
z∥2
2≤2L2T−1
∑
t=1∥z(t)−z(t−1)∥2
2+2V(H)
∇f,
and the claimed bound on the second-order path length follows. Finally, the second claim of the
theorem follows from Claim A.10 using convexity-concavity, analogously to Theorem 3.3.
We conclude this subsection by pointing out an improved variation-dependent regret bound,
which follows directly from Theorem A.21 ( cf. Corollary A.12).
Corollary A.22. In the setup of Theorem A.21, the maximum of the two players’ regrets,
max{Reg(T)
L,x,Reg(T)
L,y}, can be upper bounded by
D2
Z
η+16ηL2D2
Z+(32η3L2+η)∥F(z(1))∥2
2+32ηL2S(H)
NE+(64η3L2+2η)V(H)
∇f.
Thus, setting the learning rate optimally implies that Reg(T)
L,x,Reg(T)
L,y=O(√
1+S(H)
NE+V(H)
∇f).
35A.3 Proofs from Section 3.3
In this subsection, we provide the proofs from Section 3.3.
A.3.1 Potential games
We first characterize the behavior of (online) gradient descent ( GD) in time-varying potential games;
we recall that GDis equivalent to OGDunder the prediction m(t)
x=0for all t. Below we give the
formal definition of a potential game.
Definition A.23 (Potential game) .Ann-player game admits a potential if there exists a function
Φ∶⨉n
i=1Xi→Rsuch that for any player i∈[[n]], any joint strategy profile x−i∈⨉i′≠iXi′, and any
pair of strategies xi,x′
i∈Xi,
Φ(xi,x−i)−Φ(x′
i,x−i)=ui(xi,x−i)−ui(x′
i,x−i).
The key ingredient in the proof of Theorem 3.7 is the following bound on the second-order path
length of the dynamics.
Proposition A.24. Suppose that each player employs GDwith a sufficiently small learning rate η>0
and initialization (x(1)
1, . . . ,x(1)
n)∈⨉n
i=1Xiin a sequence of time-varying potential games. Then,
1
2ηT
∑
t=1n
∑
i=1∥x(t+1)
i−x(t)
i∥2
2≤T
∑
t=1(Φ(t)(x(t+1)
1, . . . ,x(t+1)
n)−Φ(t)(x(t)
1, . . . ,x(t)
n)). (47)
This bound can be derived from [Ana+22b, Theorem 4.3]. Now, we note that if Φ(1)=Φ(2)=
⋅⋅⋅=Φ(T), the right-hand side of (47)telescops, thereby implying that the second-order path-length
is bounded. More generally, the right-hand side of (47) can be upper bounded by
2Φmax+T−1
∑
t=1(Φ(t)(x(t+1)
1, . . . ,x(t+1)
n)−Φ(t+1)(x(t+1)
1, . . . ,x(t+1)
n))≤2Φmax+V(T)
Φ, (48)
where Φ maxis an upper bound on ∣Φ(t)(⋅)∣for any t∈[[T]], andV(T)
Φis the variation measure of
the potential functions we introduced in Section 3.3. Namely, V(T)
Φ∶=∑T−1
t=1d(Φ(t),Φ(t+1)), where
d∶(Φ,Φ′)↦maxz∈⨉n
i=1Xi(Φ(z)−Φ′(z)). Furthermore, similarly to Claim A.10, we know that the
Nash equilibrium gap in the t-th potential game can be bounded in terms of ∑n
i=1∥x(t+1)
i−x(t)
i∥2.
As a result, combining this property with Proposition A.24 and (48)establishes Theorem 3.7, the
statement of which is recalled below.
Theorem 3.7. Suppose that each player employs (online) GDwith a sufficiently small learning rate
in a sequence of time-varying potential games. Then, ∑T
t=1(EqGap(t)(z(t)))2=O(Φmax+V(T)
Φ),
where Φmaxis such that ∣Φ(t)(⋅)∣≤Φmax.
A.3.2 General-sum games
We next turn out attention to general-sum multi-player games in normal form using the well-
known bilinear formulation of correlated equilibria (Section 3.3). More precisely, this zero-sum
game is played between a mediator , who selects strategies from the set of correlated distributions
36Ξ∶=∆(⨉n
i=1Ai), and the nplayers. Per the usual description of correlated equilibria, a mediator
recommends to each player i∈[[n]]an action in Ai, and then that player selects an action that
could be different from the one recommended by the mediator. As a result, each pure strategy
of Player icorresponds to a different mapping from AitoAi. Player iwants to maximize the
deviation benefit, while the mediator wants to minimize it. We let ¯Xibe the set of (mixed) strategies
of Player i. The identity mapping di∶Ai∋ai↦aiwill be referred to as the direct (or obedient)
strategy, as it prescribes following the recommendation of the mediator. As a result, this game
can be naturally cast as the bilinear saddle-point problem in the form of (4). By the existence
of correlated equilibria [HS89], it follows that there exists a mediator strategy µ⋆∈Ξ such that
¯xi⊺A⊺
iµ⋆≤0, for any player i∈[[n]]and ¯xi∈¯Xi; in words, there is no benefit for any player to
deviate from the recommendation of the mediator.
Now, to establish Property 3.8, let us first define the regret of any player i∈[[n]]as
Reg(T)
i(¯x⋆
i)∶=T
∑
t=1⟨¯x⋆
i−¯x(t)
i,(A(t)
i)⊺µ(t)⟩,
where ¯x⋆
i∈¯Xi, so that∑n
i=1Reg(T)
iis easily seen to be equal to the regret of Player max in (4).
Further, the dynamic regret of the mediator—Player min in (4)—can be expressed as
DReg(T)
µ(µ(1,⋆), . . . ,µ(T,⋆))∶=T
∑
t=1⟨µ(t)−µ(t,⋆),n
∑
i=1A(t)
i¯x(t)
i⟩.
The key idea in the proof of Property 3.8 is that the time-invariant direct strategy (d1, . . . ,dn)
suffices to guarantee Property 3.1 as long as the mediator is selecting a sequence of CE.
Property 3.8. Suppose that Ξ∋µ(t,⋆)is a correlated equilibrium of the game at any time t∈[[T]].
Then, DReg(T)
µ(µ(1,⋆), . . . ,µ(T,⋆))+∑n
i=1Reg(T)
i≥0.
Proof. We have that
DReg(T)
µ+n
∑
i=1Reg(T)
i(¯x⋆
i)=n
∑
i=1T
∑
t=1⟨¯x⋆
i,(A(t)
i)⊺µ(t)⟩−T
∑
t=1⟨µ(t,⋆),n
∑
i=1A(t)
i¯x(t)
i⟩.
Now for any correlated equilibrium µ(t,⋆)of the t-th game, we have that ⟨µ(t,⋆),A(t)
i¯x(t)
i⟩≤0 for any
player i∈[[n]],¯xi∈¯Xi, and time t∈[[T]], which in turn implies that −∑T
t=1⟨µ(t,⋆),∑n
i=1A(t)
i¯x(t)
i⟩≥0.
Moreover,∑n
i=1max¯x⋆
i∈¯Xi∑T
t=1⟨¯x⋆
i,(A(t)
i)⊺µ(t)⟩≥∑n
i=1∑T
t=1⟨di,(A(t)
i)⊺µ(t)⟩=0, by definition of the
direct strategy di∈¯Xi. This concludes the proof.
Property 3.8 in conjunction with the argument of Theorem 3.3 readily imply Theorem 3.9. We
should note here that the variation measure V(T)
A∶=∑n
i=1∑T−1
t=1∥A(t+1)
i−A(t)
i∥2
2is also immediately
bounded in terms of the second-order variation of the sum of the players’ utility tensors. It is
also worth noting that the description of the bilinear formulation (4)grows exponentially with
the number of players; this is unavoidable in explicitly represented (normal-form) games, but it is
worth investigating whether more compact representations exist in succinct classes of games, such
as multi-player polymatrix.
Next, we provide the main implication of Theorem 3.9 in the meta-learning setting, which is
similar to the meta-learning guarantee of Proposition A.14 we established earlier in two-player
zero-sum games. Below, we denote by Ξ(h,⋆)the set of correlated equilibria of the h-th game in the
meta-learning sequence.
37Corollary A.25 (Meta-learning in general games) .Suppose that each player employs OGDin(4)
with a suitable learning rate η>0and the prediction of (37) in a meta-learning general-sum problem
withH∈Ngames, each repeated for m∈Nconsecutive iterations. Then, for an average game,
O⎛
⎝1
ϵ2H+V(H)
CE
ϵ2H⎞
⎠(49)
iterations suffice so that the mediator reaches an ϵ-approximate correlated equilibrium, where
V(H)
CE∶= inf
µ(h,⋆)∈Ξ(h,⋆)∥µ(h+1,⋆)−µ(h,⋆)∥2.
In particular, in the meta-learning regime, H≫1, the iteration-complexity bound (49) is
dominated by the (algorithm-independent) similarity metric of the correlated equilibriaV(H)
CE
H.
Corollary A.25 establishes significant gains whenV(H)
CE
H≪1.
Finally, we conclude this subsection by providing a variation-dependent regret bound in general-
sum multi-player games. To do so, we combine Corollary A.12 with Theorem 3.9, leading to the
following guarantee.
Corollary A.26 (Regret in general-sum games) .In the setup of Theorem 3.9,
Reg(T)
µ,Reg(T)
i=O(1
η+η(1+V(T)
CE+V(T)
A)),
for any player i∈[[n]].
In particular, if one selects optimally the learning rate, Corollary A.26 implies that the individual
regret of each player is bounded by O(√
1+V(T)
CE+V(T)
A). We note again that there are techniques
that would allow (nearly) recovering such regret guarantees without having to know the variation
measures in advance [Zha+22].
A.4 Proofs from Section 3.4
Finally, in this subsection we present the proofs omitted from Section 3.4. We begin with Proposi-
tion 3.10, the statement of which is recalled below. We first recall that a regularizer ϕx, 1-strongly
convex with respect to a norm ∥⋅∥, is said to be G-smooth if ∥∇ϕx(x)−∇ϕx(x′)∥∗≤G∥x−x′∥, for
allx,x′.
Proposition 3.10. Suppose that both players in a (static) two-player zero-sum game employ OMD
with a smooth regularizer. Then, DReg(T)
x,DReg(T)
y=O(√
T).
Proof. First, using Claim A.10, it follows that the dynamic regret DReg(T)
xof Player xup to time
Tcan be bounded as
T
∑
t=1(max
x(t,⋆)∈X{⟨x(t,⋆),u(t)
x⟩}−⟨x(t),u(t)
x⟩)
≤T
∑
t=1((GDX
η+∥u(t)
x∥∗)∥x(t)−ˆx(t+1)∥+GDX
η∥x(t)−ˆx(t)∥), (50)
38where G>0 is the smoothness parameter of the regularizer, and η>0 is the learning rate. We further
know that∑T
t=1(∥x(t)−ˆx(t)∥2+∥x(t)−ˆx(t+1)∥2)=O(1)for any instance of OMDin a two-player zero-
sum game [Ana+22b], which in turn implies that ∑T
t=1(∥x(t)−ˆx(t)∥+∥x(t)−ˆx(t+1)∥)=O(√
T)
by Cauchy-Schwarz. Thus, combining with (50)we have shown that DReg(T)
x=O(√
T). Similar
reasoning yields that DReg(T)
y=O(√
T), concluding the proof.
Let us next make a certain refinement of Observation 3.11. Staying on (static) bilinear saddle-
point problems, we consider the unconstrained setting where X=RdxandY=Rdy, and we focus on
the following update rule for τ∈N.
x(τ+1)=x(τ)−2ηAy(τ)+ηAy(τ−1),
y(τ+1)=y(τ)+2ηA⊺x(τ)−ηA⊺x(τ−1),(51)
where x(0),x(−1)∈Xandy(0),y(−1)∈Y. This is an instance of optimistic follow the regularized
leader ( OFTRL )[Syr+15], although our discussion here also covers the corresponding OMDvariant.
Then, summing (51) for τ=1,2, . . . , t we have
t
∑
τ=1x(τ+1)=t
∑
τ=1x(τ)−2ηA(t
∑
τ=1y(τ))+ηA(t
∑
τ=1y(τ−1)),
t
∑
τ=1y(τ+1)=t
∑
τ=1y(τ)+2ηA⊺(t
∑
τ=1x(τ))−ηA⊺(t
∑
τ=1x(τ−1)),
in turn implying that
¯x(t+1)=1
t+1¯x(1)+ηA1
t+1y(0)+t
t+1¯x(t)−2ηt
t+1A¯y(t)+ηt−1
t+1A¯y(t−1),
¯y(t+1)=1
t+1¯y(1)−ηA⊺1
t+1x(0)+t
t+1¯y(t)+2ηt
t+1A⊺¯x(t)+ηt−1
t+1A⊺¯x(t−1).(52)
Above, we define ¯x(t)∶=1
t∑t
τ=1x(τ)and¯y(t)∶=1
t∑t
τ=1y(τ). That is, the update rule (52)describes
the evolution of the time average of (51). As a result, this implies that (52)converges in a last-iterate
sense with a T−1rate, simply because OFTRL incurs O(1)regret [Syr+15]. This is conceptually
interesting as it suggests a simple way to analyze the convergence of the last iterate solely through a
regret-based analysis. In fact, (52)is a variant of the so-called Halpern’s iteration [Dia20]—a classical
technique in optimization—that incorporates optimism; a similar update rule was recently shown
to exhibit a new form of acceleration by Yoon and Ryu [YR21], and has engendered considerable
interest ever since. Returning to Observation 3.11, one can use (52)to guarantee O(logT)dynamic
regret in the standard feedback model, although it is not clear how to extend this argument in the
constrained setting ( cf.[CZ23]).
General-sum games We next extend our scope beyond bilinear saddle-point problems. We first
observe that obtaining sublinear dynamic regret is precluded in general-sum games. In particular, we
note that the computational-hardness result below (Proposition 3.12) holds beyond the online learning
setting. It should be stressed that, at least in normal-form games, without imposing computational
or memory restrictions there are trivial online algorithms that guarantee even O(1)dynamic regret
by first exploring the payoff tensors and then computing a Nash equilibrium [DDK11]. We suspect
that under the memory limitations imposed by Daskalakis, Deckelbaum, and Kim [DDK11] there
could be unconditional information-theoretic lower bounds, but that is left for future work.
39Proposition 3.12. Any polynomial-time algorithm incurs ∑n
i=1DReg(T)
i≥CTfor any polynomial
T∈N, even if n=2andC>0is an absolute constant, unless ETH for PPAD is false [Rub16].
Proof. We will make use of the fact that computing a Nash equilibrium in two-player (normal-form)
games to a sufficiently small accuracy ϵ=O(1)requires superpolynomial time, unless the exponential-
time hypothesis for PPAD fails [Rub16]. Indeed, suppose that there exist polynomial-time algorithms
that always guarantee that ∑n
i=1DReg(T)
i≤ϵTfor some polynomial T∈N, where n∶=2. Then, this
implies that there exists a time t∈[[T]]such that
max
x(t,⋆)
1∈X1⟨x(t,⋆)
1,u(t)
1⟩−⟨x(t)
1,u(t)
1⟩+max
x(t,⋆)
2∈X2⟨x(t,⋆)
2,u(t)
2⟩−⟨x(t)
2,u(t)
2⟩≤ϵ,
which in turn implies that (x(t)
1,x(t)
2)is an ϵ-approximate Nash equilibrium. Further, such a time
t∈[[T]]can be identified in polynomial time since T≤poly(∣A1∣,∣A2∣). This concludes the proof.
We also note the following computational hardness result, which rests on the more standard
complexity assumption that PPAD is not contained in P. As in Proposition 3.12, below we tacitly
assume that the underlying algorithm is deterministic.
Proposition A.27. Unless PPAD⊆P, no polynomial-time algorithm guarantees ∑n
i=1DReg(T)
i≤
poly(∣A1∣,∣A2∣)T1−ωfor all T∈N, where ω∈(0,1]is an absolute constant, even if n=2.
The proof follows directly from the PPAD -hardness result of Chen, Deng, and Teng [CDT09].
Of course, if we do not operate in the online learning model, it is computationally trivial to come
up with algorithms that guarantee that one of the two players will have 0 dynamic regret by simply
best responding to the strategy of the other player.
Finally, we provide the proof of Theorem 3.13, the detailed version of which is provided below.
Theorem A.28 (Detailed version of Theorem 3.13) .Consider an n-player game such that
∥∇xiui(z)−∇xiui(z′)∥2≤L∥z−z′∥2, where z,z′∈⨉n
i=1Xi, for any player i∈[[n]]. Then, if
all players employ OGDwith learning rate η>0it holds that
1.∑n
i=1K-DReg(T)
i=O(K√nLD2
Z)ifη=Θ(1
L√n);
2.K-DReg(T)
i=O(K3/4T1/4n1/4L1/2D3/2
Xi), for any i∈[[n]], ifη=Θ(K1/4D1/2
Xi
n1/4L1/2T1/4).
Proof. First, applying Lemma A.1 subject to the constraint that ∑T−1
t=1 1{x(t+1,⋆)
i≠x(t,⋆)
i}≤K−1
gives that for any Player i∈[[n]],
K-DReg(T)
i≤D2
Xi
2η(2K−1)+η∥u(1)
i∥2
2+ηT−1
∑
t=1∥u(t+1)
i−u(t)
i∥2
2
−1
4ηT−1
∑
t=1∥x(t+1)
i−x(t)
i∥2
2. (53)
Further, by L-smoothness we have that
∥u(t+1)
i−u(t)
i∥2
2=∥∇xiui(z(t+1))−∇xiui(z(t))∥2
2≤L2n
∑
i=1∥x(t+1)
i−x(t)
i∥2
2,
40for any t∈[[T−1]], where (x(t)
1, . . . ,x(t)
n)=z(t)∈⨉n
i=1Xiis the joint strategy profile at time
t. Thus, summing (53) over all i∈[[n]]and taking η≤1
2L√nimplies that ∑n
i=1K-DReg(T)
i≤
2K−1
2η∑n
i=1D2
Xi+η∑n
i=1∥u(1)
i∥2
2, yielding the first part of the statement since D2
Z=∑n
i=1D2
Xi, where
we recall the notation Z∶=⨉n
i=1Xi. The second part follows directly from (53)using the stability
property of OGD:∥x(t+1)
i−x(t)
i∥2=O(η), for any time t∈[[T−1]].
Remark A.29.It is not hard to show that Item 2 above can be improved to OK,T(K)using
clairvoyant mirror descent ( CMD)[PSS22], but under a stronger feedback model. In particular, using
the (squared) Euclidean regularizer it is direct to show (see [Far+22b; Nem04]) that the dynamic
regret of CMDis bounded solely by the first-order variation of the sequence of comparators, which
suffices for this claim. It is open whether similar results can be obtained in the standard feedback
model.
B Experimental examples
Finally, although the focus of this paper is theoretical, in this section we provide some illustrative
experimental examples. In particular, Appendix B.1 contains experiments on time-varying potential
games, while Appendix B.2 focuses on time-varying (two-player) zero-sum games. For simplicity, we
will be assuming that each game is represented in normal form.
B.1 Time-varying potential games
Here we consider time-varying 2-player identical-interest games. We point out that such games
are potential games (recall Definition A.23), and as such, they are indeed amenable to our theory
in Section 3.3.
In our first experiment, we first sampled two matrices A,P∈Rdx×dy, where dx=dy=1000.
Then, we defined each payoff matrix as A(t)∶=A(t−1)+Pt−αfort≥1, where A(0)∶=A. Here, α>0
is a parameter that controls the variation of the payoff matrices. In this time-varying setup, we
let each player employ (online) GDwith learning rate η∶=0.1. The results obtained under different
random initializations of matrices AandPare illustrated in Figure 1.
Next, we operate in the same time-varying setup but each player is now employing multiplicative
weights update ( MWU), instead of gradient descent, with η∶=0.1. As shown in Figure 2, while the
cumulative equilibrium gap is much larger compared to using GD(Figure 1), the dynamics still
appear to be approaching equilibria, although our theory does not cover MWU. We suspect that
theoretical results such as Theorem 3.7 should hold for MWUas well, but that has been left for future
work.
In our third experiment for identical-interest games, we again first sampled two matrices
A,P∈Rdx×dy, where dx=dy=1000. Then, we defined A(t)∶=A(t−1)+ϵPfort≥1, where A(0)∶=A.
Here, ϵ>0 is the parameter intended to capture the variation of the payoff matrices. The results
obtained under different random initializations of AandPare illustrated in Figure 3. As an aside, it
is worth pointing out that this particular setting can be thought of as a game in which the variation
in the payoff matrices is controlled by another learning agent. In particular, our theoretical results
could be helpful for characterizing the convergence properties of two-timescale learning algorithms,
in which the deviation of the game is controlled by a player constrained to be updating its strategies
with a much smaller learning rate.
410 50 100 150 200024/summationtextt
τ=1(EG(τ))2
0 50 100 150 2000.00.20.40.6EG(t)α= 0.5
α= 0.2
α= 0.1
0 50 100 150 2000246max(Reg(t)
x, Reg(t)
y)
0 50 100 150 2000123/summationtextt
τ=1(EG(τ))2
0 50 100 150 2000.00.20.40.6EG(t)α= 0.5
α= 0.2
α= 0.1
0 50 100 150 2000246max(Reg(t)
x, Reg(t)
y)
0 50 100 150 20001234/summationtextt
τ=1(EG(τ))2
0 50 100 150 2000.00.20.40.60.8EG(t)α= 0.5
α= 0.2
α= 0.1
0 50 100 150 2000246max(Reg(t)
x, Reg(t)
y)
0 50 100 150 200
Iteration (t)024/summationtextt
τ=1(EG(τ))2
0 50 100 150 200
Iteration (t)0.00.20.40.60.8EG(t)α= 0.5
α= 0.2
α= 0.1
0 50 100 150 200
Iteration (t)0246max(Reg(t)
x, Reg(t)
y)Figure 1: The equilibrium gap and the players’ regrets in 2-player time-varying identical-interest
games when both players are employing (online) GDwith learning rate η∶=0.1 for T∶=200 iterations.
Each row corresponds to a different random initialization of the matrices A,P∈Rdx×dy, which in
turn induces a different time-varying game. Further, each figure contains trajectories corresponding
to three different values of α∈{0.1,0.2,0.5}, but under the same initialization of AandP. As
expected, smaller values of αgenerally increase the equilibrium gap since the variation of the games
is more significant. Nevertheless, for all games we observe that the players are gradually approaching
equilibria.
B.2 Time-varying zero-sum games
We next conduct experiments on time-varying bilinear saddle-point problems when players are
employing OGD. Such problems were studied extensively earlier in Section 3.1 from a theoretical
standpoint.
First, we sampled two matrices A,P∈Rdx×dy, where dx=dy=10; here we consider lower-
dimensional payoff matrices compared to the experiments in Appendix B.1 for convenience in the
graphical illustrations. Then, we defined each payoff matrix as A(t)∶=A(t−1)+Pt−αfort≥1, where
A(1)∶=A. The results obtained under different random initializations are illustrated in Figure 4.
420 50 100 150 2000100200/summationtextt
τ=1(EG(τ))2
0 50 100 150 200024EG(t)α= 0.5
α= 0.2
α= 0.1
0 20 40 60 80 1000246max(Reg(t)
x, Reg(t)
y)
0 50 100 150 2000100200300/summationtextt
τ=1(EG(τ))2
0 50 100 150 200024EG(t)α= 0.5
α= 0.2
α= 0.1
0 20 40 60 80 1000246max(Reg(t)
x, Reg(t)
y)
0 50 100 150 2000100200/summationtextt
τ=1(EG(τ))2
0 50 100 150 200024EG(t)α= 0.5
α= 0.2
α= 0.1
0 20 40 60 80 1000246max(Reg(t)
x, Reg(t)
y)
0 50 100 150 200
Iterations0100200300/summationtextt
τ=1(EG(τ))2
0 50 100 150 200
Iterations024EG(t)α= 0.5
α= 0.2
α= 0.1
0 20 40 60 80 100
Iterations0246max(Reg(t)
x, Reg(t)
y)Figure 2: The equilibrium gap and the players’ regrets in 2-player time-varying identical-interest
games when both players are employing (online) GDwith learning rate η∶=0.1 for T∶=200 iterations.
Each row corresponds to a different random initialization of the matrices A,P∈Rdx×dy, which in
turn induces a different time-varying game. Further, each figure contains trajectories corresponding
to three different values of α∈{0.1,0.2,0.5}, but under the same initialization of AandP. The
MWUdynamics still appear to be approaching equilibria, although the cumulative gap is much larger
compared to GD(Figure 1).
430 100 200 300 400 5000.00.51.0/summationtextt
τ=1(EG(τ))2
0 100 200 300 400 5000.00.10.2EG(t)/epsilon1= 0.001
/epsilon1= 0.01
/epsilon1= 0.1
0 50 100 150 2000246max(Reg(t)
x, Reg(t)
y)
0 100 200 300 400 5000.00.51.0/summationtextt
τ=1(EG(τ))2
0 100 200 300 400 5000.00.10.20.3EG(t)/epsilon1= 0.001
/epsilon1= 0.01
/epsilon1= 0.1
0 50 100 150 2000246max(Reg(t)
x, Reg(t)
y)
0 100 200 300 400 500012/summationtextt
τ=1(EG(τ))2
0 100 200 300 400 5000.00.10.20.3EG(t)/epsilon1= 0.001
/epsilon1= 0.01
/epsilon1= 0.1
0 50 100 150 2000246max(Reg(t)
x, Reg(t)
y)
0 100 200 300 400 500
Iterations0123/summationtextt
τ=1(EG(τ))2
0 100 200 300 400 500
Iterations0.00.10.2EG(t)/epsilon1= 0.001
/epsilon1= 0.01
/epsilon1= 0.1
0 50 100 150 200
Iterations0246max(Reg(t)
x, Reg(t)
y)Figure 3: The equilibrium gap and the players’ regrets in 2-player time-varying identical-interest
games when both players are employing (online) GDwith learning rate η∶=0.1 for T∶=500 iterations.
Each row corresponds to a different random initialization of the matrices A,P∈Rdx×dy, which in
turn induces a different time-varying game. Further, each figure contains trajectories from three
different values of ϵ∈{0.1,0.01,0.001}, but under the same initialization of AandP. As expected,
larger values of ϵgenerally increase the equilibrium gap since the variation of the games is more
significant. Yet, even for the larger value ϵ=0.1, the dynamics are still appear to be approaching
Nash equilibria.
440 200 400 600 800 10000100002000030000/summationtextt
τ=1(EG(τ))2
0 200 400 600 800 10000.00.20.40.60.8EG(t)α= 2
α= 1
α= 0.7
0 200 400 600 800 100005101520max(Reg(t)
x, Reg(t)
y)
0 200 400 600 800 1000025005000750010000/summationtextt
τ=1(EG(τ))2
0 200 400 600 800 10000.00.20.40.60.8EG(t)α= 2
α= 1
α= 0.7
0 200 400 600 800 1000010203040max(Reg(t)
x, Reg(t)
y)
0 200 400 600 800 100002000040000/summationtextt
τ=1(EG(τ))2
0 200 400 600 800 10000.00.20.40.60.8EG(t)α= 2
α= 1
α= 0.7
0 200 400 600 800 10000102030max(Reg(t)
x, Reg(t)
y)
0 200 400 600 800 1000
Iteration (t)025005000750010000/summationtextt
τ=1(EG(τ))2
0 200 400 600 800 1000
Iteration (t)0.00.20.40.6EG(t)α= 2
α= 1
α= 0.7
0 200 400 600 800 1000
Iteration (t)01020max(Reg(t)
x, Reg(t)
y)Figure 4: The equilibrium gap and the players’ regrets in 2-player time-varying zero-sum games
when both players are employing OGDwith learning rate η∶=0.01 and T∶=1000 iterations. Each row
corresponds to a different random initialization of the matrices A,P∈Rdx×dy, which in turn induces
a different time-varying game. Further, each figure contains trajectories from three different values
ofα∈{0.7,1,2}, but under the same initialization of AandP. The OGDdynamics appear to be
approaching equilibria, albeit with a much slower rate compared to the ones observed earlier for
potential games (Figure 1).
45