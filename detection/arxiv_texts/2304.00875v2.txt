arXiv:2304.00875v2  [eess.SP]  11 Nov 2023Optimal Semantic-aware Sampling and Transmission in
Energy Harvesting Systems Through the AoII
Abolfazl Zakeri∗, Mohammad Moltafet†, and Marian Codreanu‡
∗Centre for Wireless Communications – Radio Technologies, U niversity of Oulu, Finland, email: abolfazl.zakeri@oulu. ﬁ
†Department of Electrical and Computer Engineering, Univer sity of California Santa Cruz, email: mmoltafe@ucsc.edu
‡Department of Science and Technology, Link¨ oping Universi ty, Sweden, email: marian.codreanu@liu.se
Abstract —We study a real-time tracking problem in an energy
harvesting status update system with a Markov source and an
imperfect channel, considering both sampling and transmis sion
costs. The problem’s primary challenge stems from the non-
observability of the source due to the sampling cost. By usin g
the age of incorrect information (AoII) as a semantic-aware per-
formance metric, our main goal is to ﬁnd an optimal policy tha t
minimizes the time average AoII subject to an energy-causal ity
constraint. To this end, a stochastic optimization problem is
formulated and solved by modeling it as a partially observab le
Markov decision process (POMDP). More speciﬁcally, to solv e the
main problem, we use the notion of a belief state and cast the
problem as a belief MDP problem. Then, for the perfect channe l
setup, we effectively truncate the corresponding belief sp ace
and solve the MDP problem using the relative value iteration
method. For the general setup, a deep reinforcement learnin g
policy is proposed. The simulation results show the efﬁcacy of
the derived policies in comparison to an AoI-optimal policy and
an opportunistic baseline policy.
Index Terms —Real-time tracking, age of incorrect information,
semantic communication, partially observable Markov deci sion
process.
I. I NTRODUCTION
The age of information (AoI) has been introduced to quan-
tify the information freshness in status update systems [1] .
Since then, there has been signiﬁcant research on the AoI
in different areas, e.g., queuing systems [2], and scheduli ng
and sampling problems [3], [4]. Besides the AoI, various
related metrics have been also proposed, e.g., the value of
information [5] and the age of incorrect information (AoII)
[6], which also accounts for the value of information. The Ao II
essentially amalgamates time penalty with accuracy/disto rtion
penalty to quantize the discrepancy between the informatio n
source and its estimation on the monitor side, capable of
capturing the semantics/meanings of data transfer [7]–[9] , the
provisioning of the right piece of information to the right p oint
of computation (or actuation) at the right point in time [8].
The ultimate goal in status update systems is the real-time
tracking of a real-world stochastic process on the side of a
remote monitor [10]. Recently, the real-time tracking prob lem
has been studied in a handful of papers, e.g., the papers [11] ,
[12] which used distortion-based metrics, the work in [13]
This research has been ﬁnancially supported by the Academy o f Finland
(grant 323698), and the 6G Flagship program (grant 346208). The work of M.
Codreanu has also been ﬁnancially supported in part by the Sw edish Research
Council (grant 2022-03664). We would like to gratefully ack nowledge the
contributions and insights of Markus Leinonen to this paper .which used goal-oriented metrics, and the works [14]–[18]
which used the AoII metric. In this paper, we study the real-
time tracking problem with the AoII metric.
The work [14] provided the AoI-, the real-time error-, and
the AoII- optimal policies for the remote tracking problem o f
a symmetric binary Markov source in a source-monitor-paire d
system. Their results show that the sample-at-change polic y,
which simultaneously samples and transmits whenever there
is a difference (or a change) between the source state and its
estimation, optimizes both the real-time error and the AoII .
The authors of [15] studied the remote tracking problem of
a Markov source in a multi-source setup, where the decision-
maker resides on the monitor side. They developed a heuris-
tic scheduling policy that minimizes the mean AoII using
the partially observable Markov decision process (POMDP)
framework and the idea of the Whittle index policy. They then
optimized the AoII under an unknown Markov source in [16].
However, in most of the works on the AoII, the AoII
optimization relies on fully observable source, e.g., [6], [9],
[10], [14], [17]–[19]. Having a fully observable source req uires
continuous sampling and processing of the source signal.
However, in practice, this could be challenging due to high
sampling costs, or even impossible due to, e.g., insufﬁcien t
energy to make sampling at each time, as is often the case in
energy harvesting systems.
To the extent of our knowledge, only [15] (and its sub-
sequent extension [16]) optimized the AoII under a partiall y
observable source where the partial observability comes fr om
the controller’s location, whereas in this paper it is due to
the sampling cost. Furthermore, in [15], the sampling of the
source state and its immediate transmission are upon reques t
at any given time. In contrast, in this paper, both sampling
and transmission operations are subjected to the stochasti c
availability of energy and are associated with distinct cos ts.
Additionally, different to [15], in our system, there is a
possibility to retransmit an old sample.
The goal of this paper is to ﬁnd an AoII-optimal joint
sampling and transmission policy in an energy-harvesting r eal-
time tracking system under both sampling and transmission
costs, where the sampling cost renders the source unobserv-
able. We consider a discrete-time status update system with an
imperfect channel consisting of a source, a sampler, a buffe r-
aided transmitter, and a monitor, depicted in Fig. 1. Operat ion
of each sampling and transmission consumes some units ofenergy, imposing an energy causality constraint.
We formulate a stochastic optimization problem aiming at
minimizing the average AoII subject to the energy causality
constraint. The problem is modeled as a POMDP that is
subsequently turned into a belief MDP problem. Solving the
belief MDP is challenging owing to its continuous belief sta te
space. Despite the challenge, for a perfect channel case, we
effectively truncate the belief space and ﬁnd an optimal pol icy
via the relative value iteration (RVI) algorithm. Moreover , for
the general imperfect channel case, a deep Q-network (DQN)
policy is proposed. Simulation results are conducted to sho w
the effectiveness of the derived policies compared to an AoI -
optimal policy and an opportunistic baseline policy.
The rest of this paper is organized as follows. The system
model and problem formulation are presented in Section II.
The solution method is provided in Section III. Finally, the
numerical results and conclusions are presented, respecti vely,
in Section IV and Section V.
II. S YSTEM MODEL AND PROBLEM FORMULATION
A. System Model
We consider a real-time tracking system consisting of an
information source, a sampler, a transmitter, and a monitor ,
as shown in Fig. 1. The system is powered by an energy-
harvesting module equipped with a ﬁnite-capacity battery. The
monitor is interested in the real-time tracking of the sourc e. To
this end, the controller, located at the transmitter side, s hould
appropriately decide when to sample and when to transmit
the taken sample. Most importantly, the controller does not
observe the source unless a sample is taken; the controller
observes the battery level, the information in the transmit ter’s
buffer, and the transmission results (i.e., ACK/NACK feedb ack
from the monitor).
We assume a discrete-time system with unit time slots
t∈{0,1,2,...}. The source is modeled via a two-state (bi-
nary) symmetric discrete-time Markov process X(t)∈{0,1},
with the self-transition probability p. Without loss of gener-
ality, we assume p >0.5for the sake of presentation clarity.
Note that the results are identical for p <0.5provided that
the monitor employs an appropriate state estimation strate gy
speciﬁed below. The binary source is a commonly used
model (e.g., [6], [10], [14], [18]) and it provides fundamen tal
insights into the sampling and transmission optimization i n the
system.1The process X(t)isnot observable at slottunless
a sample is taken. Thus, sampling at slot trevealsX(t).
When requested by the controller, the sampling of the source
takes place at the beginning of the slot, right after the stat e
transition (it is assumed that the source and the system cloc ks
are synchronized). The last taken sample is always stored in
the transmitter’s buffer. We denote the last sampled state o f
the source in the buffer at slot tby˜X(t).
Estimation strategy: The monitor needs to have real-time
estimate of the source. We denote the source estimate at slot
tbyˆX(t). We assume that the monitor employs a maximum
1An extension to a multi-state Markov source is deferred for f uture work.
Fig. 1: System model.
likelihood estimation, which for the source with p >0.5is the
last received sample, and for p <0.5is alternating the last
received sample at each slot [10].
Communication channel: We assume an imperfect channel
between the transmitter and the monitor. Each transmission
takes one slot and it is successfully received with probabil ity
q, referred to as the reception success probability. The un-
successfully received samples can be retransmitted, and th ey
experience the same reception success probability. We assu me
that perfect (i.e., instantaneous and error-free) feedbac k is
available for each transmission.
Sampling and Transmission Costs: We assume that each
sampling consumes csunits of energy (i.e., sampling cost),
and each transmission consumes ctunits of energy (i.e.,
transmission cost). The costs are assumed to be constant.
Decision/optimization variables: In each slot, the controller
decides the sampling and the transmission decisions. Let
α(t)∈{0,1}denote the transmission decision at slot t, where
α(t) = 1 means transmitting a sample; otherwise, α(t) = 0 .
Letβ(t)∈{0,1}denote the sampling decision at slot t, where
β(t) = 1 means sampling (and observing the source’s current
state); otherwise, β(t) = 0 . We assume that in the case of the
concurrent sampling and transmission, i.e., β(t) =α(t) = 1 ,
the transmitted sample is the current (updated) source stat e,
i.e.,˜X(t) =X(t).
Energy harvesting model: The energy supplier of the system
harvests energy and stores it in a ﬁnite-capacity battery of
Eunits of energy. Similarly to, e.g., [20], [21], we assume
that the energy arrivals u(t)follows a Bernoulli process with
parameter µ, i.e.,Pr{u(t) = 1}=µ. The battery level at slot
t, denoted by e(t)∈{0,...,E}, evolves as
e(t+1) = min/braceleftbig
e(t)+u(t+1)−/parenleftbig
β(t)cs+α(t)ct/parenrightbig
,E/bracerightbig
.
(1)
The Age of Incorrect Information: We adopt the AoII used
in [15] (and further studied in [16]). The AoII here is the
time elapsed since the last time when the source state was the
same as the current estimate at the monitor, ˆX(t). Formally,
letV(t)/definesmax{t′≤t:X(t′) =ˆX(t)}. The AoII at slot t,
denoted by δ(t), is deﬁned by
δ(t) = (t−V(t)). (2)B. Problem Formulation
Given the above deﬁnitions, our goal is to solve the follow-
ing stochastic optimization problem:
minimize limsup
T→∞1
TT/summationdisplay
t=1E{δ(t)} (3a)
subject to e(t)−β(t)cs−α(t)ct≥0,∀t, (3b)
with variables{α(t),β(t)}t=1,2,..., where the constraint (3b)
is the energy causality constraint. Furthermore, E{·}is the
expectation notation which is taken with respect to the sys-
tem’s randomness (due to the source, the energy arrivals, an d
the wireless channel) and the (possibly randomized) decisi on
variables α(t)andβ(t)made in reaction to the available
observations at the controller.
III. A NOPTIMAL POLICY
Here we present an optimal policy for problem (3). Note that
AoII is a function of the source X(t)which is not observable
due to the sampling cost. Thus, we ﬁrst model problem (3) as
a POMDP and subsequently cast it into an MDP problem.
The POMDP is described by the following elements:
•State: Letρ(t)be a binary indicator indicating whether the
last sample at the buffer ˜X(t)equals to the estimate ˆX(t).
Speciﬁcally, ρ(t)is deﬁned as
ρ(t)/defines/braceleftbigg
0,if˜X(t) =ˆX(t),
1,if˜X(t)/ne}ationslash=ˆX(t).(4)
We deﬁne the state at slot tbys(t) = (e(t),δ(t),ρ(t)). The
state space is denoted by S.
•Observation: The observation at slot t, denoted by o(t),
iso(t) = (e(t),ρ(t)).
•Action: There are totally four possible actions at each slot.
However, by taking into account the goal of problem (3) (i.e. ,
minimizing the average AoII): i) re-transmitting an old sam ple
when˜X(t) =ˆX(t), or ii) transmitting a fresh sample when
X(t) =ˆX(t)are both only wasting energy without reducing
the distortion; thus, the decision to simultaneously sampl e
and transmit, i.e., α(t) =β(t) = 1 , can be encoded to the
action of taking a sample and transmitting that sample only
ifX(t)/ne}ationslash=ˆX(t), and ii) the decision to take a new sample
without simultaneously transmitting it (i.e., β(t) = 1 and
α(t) = 0 ) can be eliminated without losing the optimality;
this is because, in the case where the sample will not be
transmitted at a later time, it simply consumes energy witho ut
enhancing performance, and in the case where the sample
is to be transmitted later, taking a fresh sample just before
transmission instead always has a higher probability to rec tify
the estimate and hence improve performance. Thus, the actio n
space of the POMDP, shown by A, has three elements which
are speciﬁed in the following. The action at slot tis deﬁned by
a(t)∈{0,1,2}, wherea(t) = 0 indicates that the sampler and
transmitter stay idle, a(t) = 1 indicates that the transmitter
re-transmits the sample in the buffer (the action could be
a(t) = 1 if there is enough energy for at least one transmission
and˜X(t)/ne}ationslash=ˆX(t)), anda(t) = 2 indicates that the samplertakes a new sample and the transmitter transmits that sample
whenX(t)/ne}ationslash=ˆX(t)(the action could be a(t) = 2 if there is
enough energy for at least one sampling and one transmission ).
Actions are determined by a policy, denoted by π, which is a
(possibly randomized) mapping from StoA.
•State Transition Probabilities: The transition probabilities
from current state s= (e,δ,ρ)to next state s′= (e′,δ′,ρ′)
under a given action ais denoted by Pr{s′|s,a}.To fa-
cilitate a compact description of Pr{s′|s,a}, we employ
the shorthand notations ¯q/defines1−q,¯µ/defines1−µ,¯p/defines1−p,
andc/definescs+ct. Since for a given action and state, the
evolution of ρ, the AoII, and the energy arrival process
are independent, the transition probabilities can be writt en
asPr{s′|s,a}= Pr{ρ′/vextendsingle/vextendsingleρ, δ, a}Pr{δ′|δ, a}Pr{e′|e,δ, a},
where
Pr{ρ′/vextendsingle/vextendsingleρ, δ, a}=

1,ifa= 0, ρ′=ρ,
q,ifa= 1, ρ′= 0,
¯q,ifa= 1, ρ′=ρ,
1,ifa= 2, δ= 0, ρ′= 0,
q,ifa= 2, δ/ne}ationslash= 0, ρ′= 0,
¯q,ifa= 2, δ/ne}ationslash= 0, ρ′= 1,
0,otherwise .(5)
Pr{δ′/vextendsingle/vextendsingleδ, a= 0}=

p,ifδ= 0, δ′= 0,
¯p,ifδ= 0, δ′= 1,
¯p,ifδ/ne}ationslash= 0, δ′= 0,
p,ifδ/ne}ationslash= 0, δ′=δ+1,
0,otherwise .(6)
Pr{δ′/vextendsingle/vextendsingleδ, a= 1}=

q¯p,ifδ= 0, δ′= 0,
qp, ifδ= 0, δ′= 1,
¯qp, ifδ= 0, δ′= 0,
¯q¯p,ifδ= 0, δ′= 1,
qp, ifδ/ne}ationslash= 0, δ′= 0,
q¯p,ifδ/ne}ationslash= 0, δ′=δ+1,
¯q¯p,ifδ/ne}ationslash= 0, δ′= 0,
¯qp, ifδ/ne}ationslash= 0, δ′=δ+1,
0,otherwise .(7)
Pr{δ′/vextendsingle/vextendsingleδ, a= 2}=

p, ifδ= 0, δ′= 0,
¯p, ifδ= 0, δ′= 1,
qp, ifδ/ne}ationslash= 0, δ′= 0,
q¯p,ifδ/ne}ationslash= 0, δ′= 1,
¯q¯p,ifδ/ne}ationslash= 0, δ′= 0,
¯qp, ifδ/ne}ationslash= 0, δ′=δ+1,
0,otherwise ,(8)
Pr{e′/vextendsingle/vextendsinglee,δ, a}=

µ, ifa= 0, e′= min{e+1,E},
¯µ, ifa= 0, e′=e,
µ, ifa= 1, e′=e+1−ct,
¯µ, ifa= 1, e′=e−ct,
µ, ifa= 2, e′=e+1−c, δ/ne}ationslash= 0,
¯µ, ifa= 2, e′=e−c, δ/ne}ationslash= 0,
µ, ifa= 2, e′=e+1−cs, δ= 0,
¯µ, ifa= 2, e′=e−cs, δ= 0,
0,otherwise .(9)•Observation function: The observation function is
Pr{o(t)|s(t),a(t−1)}, which is a deterministic function,
i.e.,Pr{o(t)/vextendsingle/vextendsingles(t),a(t−1)}=1{o(t)=(e(t),δ(t),ρ(t))}.
•Cost function: The immediate cost function at slot tis
deﬁned by C(s(t)) =δ(t).
Belief MDP Formulation: To have optimal decision-making,
we need to deﬁne state-like quantities that preserve the Mar kov
property and summarize all the necessary information calle d
sufﬁcient information states. Widely used sufﬁcient state s, as
in this paper, are belief states [22, Ch. 7].
LetIC(t)denote the complete information state at slot
tconsisting of [22, Ch. 7]: i) the initial probability dis-
tribution over states, ii) all past and current observa-
tions, i.e., (o(0),...,o(t)), and iii) all past actions, i.e.,
(a(0),...,a(t−1)). We deﬁne a belief bi(t)by
bi(t)/definesPr/braceleftbig
δ(t) =i/vextendsingle/vextendsingleIC(t)/bracerightbig
, i= 0,1,..., (10)
The belief is updated as a function of current belief
{bi(t)}i=0,1,..., the observation o(t+ 1) , and current action
a(t). The following proposition gives the belief update.
Proposition 1. Given belief{bi(t)}i=0,1,..., observation
o(t+1) , and action a(t), the belief update function is given
by the following equations:
Ifa(t) = 0 , ora(t) = 1,ρ(t+1) = 1 :
bi(t+1) =

b0(t)p+(1−b0(t))(1−p), i= 0,
(1−p)b0(t), i = 1,
pbi−1(t), i = 2,3,...,
(11)
ifa(t) = 1,ρ(t+1) = 0 :
bi(t+1) =

b0(t)(1−p)+(1−b0(t))p, i= 0,
b0(t)p, i = 1,
(1−p)bi−1(t), i = 2,3,...,
(12)
ifa(t) = 2,ρ(t+1) = 1 :
bi(t+1) =/braceleftbigg1−p, i= 0,
bi−1(t)p, i= 1,2,...,(13)
and ifa(t) = 2,ρ(t+1) = 0 :
bi(t+1) =

p, i = 0,
1−p, i= 1,
0, i= 2,3,....(14)
Having the belief deﬁned, we formulate a belief MDP by
deﬁning its state as
z(t)/defines(e(t),{bi(t)}i=0,1,...,ρ(t)), (15)
and its immediate cost function as the expected AoII given by
C(z(t)) =/summationtext
i=0,1,...bi(t)i.
LetZdenote the state space of the belief MDP, then, the
goal is to ﬁnd the optimal policy π∗:Z→A that is a solutionto the following MDP problem:
π∗(z(0)) =
argmin
π∈Π/braceleftBigg
limsup
T→∞1
TT/summationdisplay
t=1E{C(z(t))/vextendsingle/vextendsinglez(0)}/bracerightBigg
, (16)
where the expectation is with respect to the policy and the
system randomness, and Πis the set of all admissible policies.
The state space of the belief MDP problem (16) is an inﬁnite
set, thus, ﬁnding an optimal policy is extremely challengin g
(see, e.g., [22, Sec. 7.3]); Actually, the problem is PSPACE -
hard even for a ﬁnite horizon [22, Sec. 7.3]. Nonetheless, we
will provide an optimal policy via the RVI algorithm for the
case where the channel is perfect, i.e., q= 1, and propose an
online learning-based algorithm for the general case.
1) An Optimal Policy Under The Perfect Channel: It can be
observable that, under the perfect channel, the re-transmi ssion
actiona(t) = 1 is unnecessary (as always ˜X(t) =ˆX(t)) so
actions are essentially the idle action a(t) = 0 and the sample
and transmission action a(t) = 2 .
Thus, the belief update follows (11) or (14) depending on
the actions taken. Next, we will characterize and effective ly
truncate the belief space using the AoI at the transmitter θ(t),
which allows us to ﬁnd an optimal policy.
The following proposition shows one-to-one mapping be-
tweenθ(t)and belief{bi(t)}i=0,1,....
Proposition 2. Supposeθ(t) =n,n= 1,2,.... Then, for the
perfect channel (i.e., q= 1), the belief at slot tis given by
bi(t) =

g(n), i = 0,
g(n−i)(1−p)p(i−1), i= 1,...,n,
0, i =n+1,...,(17)
where the function g(n)is given by
g(n)/definesPr{δ(t) = 0/vextendsingle/vextendsingleθ(t) =n}= 0.5(1+(2p−1)n),(18)
andg(0)/defines1.
One can observe from (17) that for sufﬁciently large values
of the AoI θ(t), denoted by ¯N, the belief corresponding to
θ(t) =¯N, converges to the following:
bi(t) =

0.5, i = 0,
g(¯N−i)(1−p)p(i−1), i= 1,...,¯N,
0, i =¯N+1,....(19)
Thus, we can effectively truncate the belief space by boundi ng
the AoI with ¯N.
We have shown that both the cost function and belief state
of problem (16) can be written only as a function of the AoI
θ(t), which is bounded by ¯N. Thus, for the perfect channel,
problem (16) can be expressed as a ﬁnite-state MDP problem
with the following elements:
•State: The state at slot tiss(t) = (e(t),θ(t)),whereθ(t)∈
{1,2,...,¯N}. The state space is denoted by S, which is a
ﬁnite set.
•Action: The actions are a(t) = 0 anda(t) = 2 .
•State Transition Probabilities: The transition probabilitiesfrom current state s= (e,θ)to next state s′= (e′,θ′)under a
given action ais deﬁned by Pr{s′|s,a},which can be written
asPr{θ′|θ, a}Pr{e′|e,θ,a}, where
Pr{θ′|θ, a}=

1,ifa/ne}ationslash= 2, θ′= min(θ+1,¯N),
1,ifa= 2, θ′= 1,
0,otherwise ,
(20)
Pr{e′|e,θ, a}=

µ, ifa= 0, e′= min{e+1,E},
¯µ, ifa= 0, e′=e,
µ(1−g(θ)),ifa= 1, e′=e+1−c,
¯µ(1−g(θ)),ifa= 1, e′=e−c,
µg(θ), ifa= 1, e′=e+1−cs,
¯µg(θ), ifa= 1, e′=e−cs,
0, otherwise ,(21)
whereg(θ) = 0.5(1+(2p−1)θ).
•Cost Function: The immediate cost function at slot tis the
expected AoII given by
C(s(t)) =/summationtextθ(t)
i=0bi(t)i, (22)
wherebi(t)is given by (17).
Having the MDP speciﬁed above, we apply the RVI algo-
rithm to ﬁnd an optimal policy for problem (16) under the
perfect channel. The RVI algorithm transforms the Bellman’ s
optimality equation into the following iterative process f or
each state s∈S:
a∗←argmin a/braceleftBig
C(s)+/summationtext
s′∈SPr(s′|s,a)V(s′)/bracerightBig
,
V(s)←/braceleftBig
C(s)+/summationtext
s′∈SPr(s′|s,a∗)V(s′)/bracerightBig
−V(sref),
wheresref∈S is an arbitrarily chosen reference state. Once
the iterative process above converges, the algorithm provi des
an optimal policy π∗and the optimal value of the average
AoII, which equals to V(sref).
2) A Deep Q-Network (DQN) Policy to Solve (16):Here
the aim is to solve the MDP problem (16). However, the
main difﬁculty lies in the fact that the state space of the
problem is an inﬁnite set. Thus, methods, e.g., RVI and linea r
programming [23], which are applicable for problems with a
ﬁnite state space, cannot be utilized. Nonetheless, problem (16)
is an MDP problem and can be solved via online reinforcement
learning algorithms. We adopt a DQN [24] to solve problem
(16). A reader can refer to, e.g., [24, Alg. 1], for more detai ls
of DQN. Implementation details are given in the next section .
IV. N UMERICAL RESULTS
Here, we provide simulation results to assess the perfor-
mance of the derived policies. For the performance compari-
son, we also consider an AoI-optimal policy and a “baseline
policy” which determines actions according to the followin g
rule: Ife(t)≥ct+cs, thena(t) = 2 , i.e., the sampling and
transmission action, else a(t) = 0 , i.e., the idle action. The
sampling cost csand transmission cost ctare ﬁxed to 1, and
the value of ¯Nis set to30, unless speciﬁed otherwise. Further-
more, for the DQN policy, we consider a fully-connected deep0.2 0.4 0.6 0.80.20.40.60.8
(a) The average AoII vs. the self-transition prob-
ability for µ= 0.5
0.2 0.4 0.6 0.80.511.5
(b) The average AoII vs. the energy arrival rate
forp= 0.7
Fig. 2: The average AoII performance of the different polici es, where
E= 5 andq= 1
neural network consisting of an input layer ( |z(t)|=N+3
neurons), 2hidden layers consisting of 64and32neurons
with ReLU activation function, and an output layer ( |A|= 3
neurons); moreover, the number of steps per episode is 400, the
discount factor is 0.99, the mini-batch size is 64, the learning-
rate is0.0001 , and the optimizer is RMSProp .
The average AoII performance of different policies is shown
as a function of the self-transition probability of the sour cep
in Fig. 2(a) and the energy arrival rate µin Fig. 2(b). Each
policy is ﬁrst optimized for the corresponding metric, and
then its average AoII performance is calculated empiricall y.
First, the ﬁgure shows that the AoII-optimal policy exhibit s
a signiﬁcant improvement in performance compared to both
the baseline policy and the AoI-optimal policy. This highli ghts
the signiﬁcance of considering the semantics of sampling an d
transmissions when optimizing the real-time tracking of a
remote source, which is typically the primary goal in most
status update systems. Furthermore, it is observable that w hen
the source undergoes rapid or gradual changes, its trackabi lity
increases owing to the predictability of the source state.
Besides, as expected, the performance for p <0.5is identical
to that of p >0.5.
Figure 3(a) demonstrates the average AoII as a function of
the channel reliability q, where we use a DQN for the AoII op-
timization problem. (Reiterate that for the perfect channe l, we
obtained an AoII-optimal policy.) The ﬁgure shows that when
the channel reliability is higher, the DQN policy demonstra tes
a better performance since at a low reliable channel setup,ﬁnding optimal times of sampling and transmission become
more critical.
Finally, Figure 3(b) shows the average AoII with respect to
the sampling cost. It reveals that the DQN policy coincides
with the AoII-optimal policy. However, there exists a consi d-
erable performance gap between the AoI-optimal policy and
the AoII-optimal policy when the sampling cost is small.
0.3 0.4 0.5 0.6 0.7 0.80.60.811.2
(a) The average AoII vs. the channel reliability q
forµ= 0.5
1 2 3 4 50.60.811.21.41.6
(b) The average AoII vs. the sampling cost for
µ= 0.7andq= 1
Fig. 3: The average AoII performance of the different polici es, where
E= 5 andp= 0.7V. C ONCLUSIONS
We provided an AoII-optimal policy for real-time tracking
in an energy harvesting system under sampling and transmis-
sion costs, where the sampling cost renders the source unob-
servable. To do so, we ﬁrst formulated a stochastic optimiza -
tion problem aimed at minimizing the average AoII subject to
the energy-causality constraint. We proposed a POMDP and
its belief MDP formulation to tackle the partial observabil ity
of the source, and we managed to effectively truncate the
corresponding belief-state space and ﬁnd an optimal policy
when the channel is perfect. Moreover, for the general im-
perfect channel setup, a DQN policy is proposed. Simulation
experiments showed that the derived policies outperform th e
AoI-optimal policy and an opportunistic baseline policy al most
in all circumstances. Additionally, they showed the source
dynamic has a signiﬁcant impact on the performance.
REFERENCES
[1] S. Kaul, R. Yates, and M. Gruteser, “Real-time status: Ho w often should
one update?,” in Proc. IEEE Int. Conf. on Computer Commun. , pp. 2731–
2735, Orlando, FL, USA, Mar. 2012.
[2] M. Costa, M. Codreanu, and A. Ephremides, “On the age of in formation
in status update systems with packet management,” IEEE Trans. Inf.
Theory , vol. 62, no. 4, pp. 1897–1910, Apr. 2016.[3] A. Zakeri, M. Moltafet, M. Leinonen, and M. Codreanu, “Mi nimizing
the AoI in resource-constrained multi-source relaying sys tems: Dynamic
and learning-based scheduling,” IEEE Trans. Wireless Commun. , pp. 1–
1, Early Access, 2023.
[4] I. Kadota, A. Sinha, E. Uysal-Biyikoglu, R. Singh, and E. Modiano,
“Scheduling policies for minimizing age of information in b roadcast
wireless networks,” IEEE/ACM Trans. Netw. , vol. 26, no. 6, pp. 2637–
2650, Dec. 2018.
[5] A. Kosta, N. Pappas, A. Ephremides, and V . Angelakis, “Ag e and value
of information: Non-linear age case,” in Proc. IEEE Inter. Symp. on Inf.
Theory (ISIT) , pp. 326–330, Aachen, Germany, Jun. 2017.
[6] A. Maatouk, S. Kriouile, M. Assaad, and A. Ephremides, “T he age of
incorrect information: A new performance metric for status updates,”
IEEE/ACM Transactions on Networking , vol. 28, no. 5, pp. 2215–2228,
2020.
[7] D. G¨ und¨ uz et al ., “Beyond transmitting bits: Context, semantics, and
task-oriented communications,” IEEE J. Sel. Areas Commun. , vol. 41,
no. 1, pp. 5–41, Jan. 2023.
[8] E. Uysal et al ., “Semantic communications in networked systems: A
data signiﬁcance perspective,” IEEE Netw. , vol. 36, no. 4, pp. 233–240,
Jul. 2022.
[9] A. Maatouk, M. Assaad, and A. Ephremides, “The age of inco rrect in-
formation: An enabler of semantics-empowered communicati on,” IEEE
Trans. Wireless Commun. , vol. 22, no. 4, pp. 2621–2635, Apr. 2023.
[10] C. Kam, S. Kompella, G. D. Nguyen, J. E. Wieselthier, and
A. Ephremides, “Towards an effective age of information: Re mote
estimation of a Markov source,” in Proc. IEEE INFOCOM Workshop ,
pp. 367–372, Honolulu, HI, USA, Apr. 2018.
[11] Y . Sun, Y . Polyanskiy, and E. Uysal, “Sampling of the wie ner process
for remote estimation over a channel with random delay,” IEEE Trans.
Inf. Theory , vol. 66, no. 2, pp. 1118–1135, Feb. 2020.
[12] J. Yun, C. Joo, and A. Eryilmaz, “Optimal real-time moni toring of an
information source under communication costs,” in Proc. IEEE Conf. on
Decis. and Contr. (CDC) , pp. 4767–4772, Miami, FL, USA, Dec. 2018.
[13] N. Pappas and M. Kountouris, “Goal-oriented communica tion for real-
time tracking in autonomous systems,” in Proc. IEEE Inter. Conf. on
Auto. Syst. (ICAS) , pp. 1–5, Montreal, QC, Canada, Aug. 2021.
[14] C. Kam, S. Kompella, and A. Ephremides, “Age of incorrec t information
for remote estimation of a binary Markov source,” in Proc. IEEE
INFOCOM Workshop , pp. 1–6, Toronto, ON, Canada, Jul. 2020.
[15] S. Kriouile and M. Assaad, “Minimizing the age of incorr ect information
for real-time tracking of Markov remote sources,” in Proc. IEEE Inter.
Symp. on Inf. Theory (ISIT) , pp. 2978–2983, Melbourne, Australia, Jul.
2021.
[16] S. Kriouile and M. Assaad, “Minimizing the age of incorr ect information
for unknown Markovian source,” arXiv preprint arXiv:2210.09681 , Oct.
2022.
[17] A. Nayak, A. E. Kalør, F. Chiariotti, and P. Popovski, “A decentralized
policy for minimization of age of incorrect information in s lotted
ALOHA systems,” arXiv preprint arXiv:2301.10987 , Jan. 2023.
[18] Y . Chen and A. Ephremides, “Minimizing age of incorrect information
over a channel with random delay,” arXiv preprint arXiv:2301.06150 ,
Feb. 2023.
[19] K. Bountrogiannis, A. Ephremides, P. Tsakalides, and G . Tzagkarakis,
“Age of incorrect information with hybrid ARQ under a resour ce
constraint for N-ary symmetric Markov sources,” arXiv preprint
arXiv:2303.18128 , Mar. 2023.
[20] P. Raﬁee and O. Ozel, “Active status update packet drop c ontrol in an
energy harvesting node,” in Proc. IEEE Works. on Sign. Proc. Adv. in
Wirel. Comms. , pp. 1–5, Atlanta, GA, USA, May 2020.
[21] Z. Chen, N. Pappas, E. Bj¨ ornson, and E. G. Larsson, “Age of information
in a multiple access channel with heterogeneous trafﬁc and a n energy
harvesting node,” in Proc. IEEE INFOCOM Workshop , pp. 662–667,
Paris, France, May 2019.
[22] O. Sigaud and O. Buffet, Markov decision processes in artiﬁcial intel-
ligence . John Wiley & Sons, 2013.
[23] A. Zakeri, M. Moltafet, M. Leinonen, and M. Codreanu, “ Q uery-Age-
Optimal scheduling under sampling and transmission constr aints,” IEEE
Commun. Lett. , vol. 27, no. 4, pp. 1205–1209, Apr. 2023.
[24] V . Mnih et al. , “Human-level control through deep reinforcement learn-
ing,” Nature , vol. 518, no. 7540, pp. 529–533, Feb., 2015.