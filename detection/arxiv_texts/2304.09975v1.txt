Solving the Kidney-Exchange Problem via Graph
Neural Networks with No Supervision
Pedro Foletto Pimenta
Institute of Informatics
UFRGS Federal University
Porto Alegre, RS, Brazil
pfpimenta@inf.ufrgs.brPedro H. C. Avelar
King’s College London
London, UK
pedro henrique.da costa avelar@kcl.ac.uk
A*STAR
SingaporeLuis C. Lamb
Institute of Informatics
UFRGS Federal University
Porto Alegre, RS, Brazil
lamb@inf.ufrgs.br
Abstract —This paper introduces a new learning-based ap-
proach for approximately solving the Kidney-Exchange Problem
(KEP), an NP-hard problem on graphs. The problem consists of,
given a pool of kidney donors and patients waiting for kidney
donations, optimally selecting a set of donations to optimize the
quantity and quality of transplants performed while respecting
a set of constraints about the arrangement of these donations.
The proposed technique consists of two main steps: the ﬁrst is
a Graph Neural Network (GNN) trained without supervision;
the second is a deterministic non-learned search heuristic that
uses the output of the GNN to ﬁnd a valid solution. To allow for
comparisons, we also implemented and tested an exact solution
method using integer programming, two greedy search heuristics
without the machine learning module, and the GNN alone without
a heuristic. We analyze and compare the methods and conclude
that the learning-based two-stage approach is the best solution
quality, outputting approximate solutions on average 1.1 times
more valuable than the ones from the deterministic heuristic
alone.
Index Terms —Kidney Exchange Problem; Graph Neural Net-
works; Optimization, Machine Learning, Deep Learning, Graph
Theory.
I. I NTRODUCTION
This study addresses machine learning approaches for the
approximate solving of the Kidney Exchange Problem (KEP),
an NP-Hard problem on graphs [1], [2]. This problem consists
of, given a pool of kidney donors and patients waiting for
kidney donations, optimally selecting a set of donations to
optimize the quantity and quality of transplants performed
while still respecting a set of constraints about the arrangement
of these donations.
Thus, this work’s main objective is to answer the following
question: Can the Kidney Exchange problem be better
approximately solved with the help of machine learning?
If positive, we want to evaluate the feasibility of utilizing such
an approach in terms of the quality of the solutions it provides.
Further, we are also interested in assessing how viable would
such a method be in terms of computational time. Additionally,
one hopes that, by answering these questions, we may also
better understand the limitations of the employed machine
learning methods for this problem and the potential future
research directions for solving the KEP and other optimization
problems in graphs.A. On Kidney Exchanges
Kidney disease affects millions of people worldwide, and
the two available treatment options for end-stage kidney
disease are dialysis and kidney transplantation [3]. Transplan-
tation is the preferred treatment for the most severe forms of
kidney disease [1] because it is cheaper and offers a better
quality of life and better life expectancy [4]. The source of
the kidney can be either a cadaver or a live donor, as the
human body has two kidneys, and often only one sufﬁces.
The compatibility of a transplant between a donor and a
recipient is determined by a number of different factors, such
as the blood-group compatibility, tissue-type compatibility, the
ages and general health of the donor and the recipient, the
size of the donor’s kidney, and many others [3]. The lower
the compatibility between a donor and a patient, the lower the
chance of kidney transplant success between them.
In the last decades, there have started to be paired kidney
exchanges , which are cycles involving donor-patient pairs such
that each donor cannot give a kidney to their intended recipient
because of some incompatibility. However, each patient can
receive a kidney from a donor from another pair [1]. These
cycles were ﬁrst performed with only two donor-patient pair
nodes, but later longer cycles of kidney exchanges were per-
formed. Another possibility of an exchange scheme is to create
exchange chains that begin with a donation of an altruistic
or cadaveric kidney donor, followed by chained donations of
patient-donor pairs, and then ﬁnish with a donation either to
a patient with or without an associated donor. In this study,
the donation chains are also referred to as paths , a term often
used for describing sequences of connected nodes in graph
problems. To ﬁnd the best possible allocation, i.e., the optimal
solution to the problem, considering a set of donors, patients,
and patient-donor pairs, a mix of both cycles and chains can
be selected as long as the cycles and paths do not intersect
with each other.
These cycles and chains could have unlimited size. In real
life, however, there is a practical limit to the size of the paired
kidney exchange cycles and chains: The kidney donation surg-
eries in a chain often must be done simultaneously to ensure
every patient receives a kidney before her associated donorarXiv:2304.09975v1  [cs.LG]  19 Apr 2023donates her kidney. However, organizing many simultaneous
surgeries is logistically complex and sometimes impractical
or even impossible. Even if they do not have to be done
simultaneously, it is generally required at least that every
patient-donor pair receive a kidney before they give a kidney.
Furthermore, numerous other logistical difﬁculties arise when
dealing with longer cycles and chains, which makes it highly
desirable or sometimes even necessary that these donation
cycles and chains have limited size. The longest kidney
transplant chain successfully performed had a size of 35 and
happened between 6 January and 17 June 2015 in the USA
[5], although, in most situations, the maximum reasonable size
is considerably smaller.
B. The Kidney Exchange Problem
The Kidney Exchange Problem (KEP) was ﬁrst mathemat-
ically formalized by Roth et al. in [1], then slightly updated
in various ways in subsequent works. A summary of the
variations found in the literature and models and techniques
currently employed to solve them can be found at [6].
In the formalization used in the study, each instance of the
KEP is represented by a directed weighted graph G=fV; Eg.
Each patient, donor, and patient-donor pair is mapped to a
graph node; they will be referred to as patient (P) nodes, non-
directed (or altruistic) donor (NDD) nodes, and patient-donor
pair (PDP) nodes. The set of nodes Vis thus accordingly
partitioned into sets P,NDD andPDP . The graph’s edges
represent donation compatibility: an edge from node A to node
B represents that a kidney donation in this direction is possible;
the edge weight encodes the donor’s compatibility with the
recipient.
Solving a KEP instance means optimally selecting a set
of cycles and chains to optimize the transplants performed.
This optimally includes maximizing the quantity and quality
of the transplants, which is encoded in the edge weights. The
solution must also respect a set of constraints. Each node
may participate at most in one transplant as a donor and
another as a receiver. Also, PDP nodes can only donate a
kidney if they receive one, although they can receive it without
donating. Nodes of type P can only receive donations, and
NDD nodes can only donate. The problem is then described
as follows: ” Given a list of kidney needing patients, kidney
donors, and patient-donor pairs, and a compatibility index
between each possible donor and receiver, what is the best
possible selection of donations that can be performed so that
the total quantity of transplants, weighted by the compatibility
indexes, is maximized, while still respecting a given size limit
to the kidney exchange cycles and chains in the solution? ”
Our KEP formalization is represented in the set of equa-
tions below, inspired on the so-called Recursive Algorithm
formulation described by [2]. We use a binary variable yefor
each edge e2E, that indicates if the edge is part of the
solution or not, as well as auxiliary variables ﬂow in fi
vand
ﬂow out fo
vfor each node v2V, which represents the node’s
number of incoming and outcoming edges contained in the
solution, and are deﬁned at Equations 9 and 10. Crepresentsthe set of existing cycles and paths in graph, where C2C
is a collection of edges, i.e. CE.Ckis a subset ofC(i.e.
CkC) containing the cycles and paths that use kor fewer
edges.
maxX
e2Eweye (1)
s:t:X
e2Nin(v)ye=fi
vv2V (2)
X
e2Nin(v)ye=fo
vv2V (3)
fo
vfi
v1v2PDP (4)
fo
v1v2NDD (5)
fi
v1v2P (6)X
e2CyejCj 1C2CnC k (7)
ye2f0;1ge2E (8)
fi
v=X
e2Nin(v)yev2V (9)
fo
v=X
e2Nout(v)yev2V (10)
Nin(v) =fe8e2E; e= (v0; v)g
Nout(v) =fe8e2E; e= (v; v0)g
The constraints ensure the result is a valid solution for KEP:
the ﬁrst two (Eq. 2 and Eq. 3) are necessary for the use of the
ﬂow in and ﬂow out variables, the third one (Eq. 4) controls
the ﬂow in and ﬂow out of the PDP nodes, the fourth and ﬁfth
ones (Eq. 5 and Eq. 6) do the same but for NDD nodes and
P nodes, respectively, the sixth one (Eq. 7) prohibits cycles or
paths with length longer than a given limit k, and the seventh
one (Eq. 8) deﬁnes the domain of the yvariable. The objective
(deﬁned at Expression 1) is to maximize the number of edges
in the solution y, weighted by the associated edge weights w,
while still respecting the KEP constraints (Equations 4, 5, 6,
7, and 8).
It has been proven that this problem is NP-Hard [7], al-
though it can become polynomial-time solvable if some of the
constraints are relaxed, such as limiting the exchange cycles
and chains length to 2, or removing the length restriction
entirely.
II. M ACHINE LEARNING METHODS FOR OPTIMIZATION
PROBLEMS IN GRAPHS
In the last few years, many machine learning-based ap-
proaches that effectively solve several different optimization
problems in graphs have been proposed, although none of
them designed for solving KEP. Graph optimization problems
already solved with the help of machine learning include
the Set Covering Problem [8], Graph Colouring [9], [10],Minimum Vertex Cover [11], [12], Maximum Cut [13], Graph
Partitioning [14], Maximum Independent Set [15], Maximum
Common Subgraph [16], and the Travelling Salesperson Prob-
lem (also called the travelling salesman problem or TSP), one
of the most famous NP-hard problems, often used to represent
the class, and some variants of it [17]–[22].
In 2015, the authors of [20] presented a new type of neural
network called Pointer Networks, designed to learn how to
reorder the elements of an input sequence; they validated
the method by using it to solve 3 problems, including the
TSP. In 2016, researchers presented in [23] a framework for
combinatorial optimization problems using neural networks
and reinforcement learning; the work focused on the TSP,
which is a graph problem, but the approach was designed to
work with any combinatorial optimization problem. In 2017,
the authors of [13] proposed the utilization of a combination
of graph representation learning and reinforcement learning
to solve graph optimization problems; they showed that their
proposed approach effectively learns to solve at least three of
those problems: Minimum Vertex Cover, Maximum Cut, and
TSP. In 2018, the decision variant of the TSP, called Decision
Traveling Salesman Problem (DTSP), which is to decide if a
given TSP instance admits a Hamiltonian route with a cost no
greater than a given threshold C, and is also NP-Hard, was
solved in [19] with a GNN.
In 2019, the authors of [17] tried to solve the TSP using a
two stage technique that is very similar to the one presented in
this study (described at section IV-C2 and illustrated at Figure
1): ﬁrstly, a GNN processes the input graph and create scores
for each edge of the graph; then, a non-learned search heuris-
tic, which in this case was beam search, uses these scores to
construct a solution. There have been other approaches that use
similar techniques: in [24] the authors review graph learning
methods for solving CO problems, with a focus on two-stage
techniques, where the ﬁrst is based on graph representation
learning, which embeds the input graph into low-dimension
vectors, and the second uses the embeddings learned in the
ﬁrst stage; [25] surveys the use of GNNs as a model of neural-
symbolic computing and their applications, which includes
combinatorial optimization problems; [18] uniﬁes and reﬁnes
several of such two stage techniques for neural combinatorial
optimization, and test it on the TSP.
III. D ATASET
Deep learning methods such as GNNs require lots of
data; usually, many thousands of instances, or even millions,
depending on the problem and the size of the model, are
necessary for the models to converge to a decent behaviour.
With insufﬁcient data, the model is very prone to overﬁtting,
or sometimes may not even learn anything at all. Medical
data, however, is very scarse, and rarely available at this
quantity. This happens for two main reasons. Firstly, a major
problem with healthcare data is its sensitivity: as a lot of it is
conﬁdential information about the patients, it is usually highly
protected and as a rule cannot be used without special consent,
be it from the patients or at least from the health institutethat owns the data. Furthermore, there are a limited number
of medical cases of each given situation registered; although
it would be useful to have a KEP dataset with millions of
instances, this situation has not happened that many times,
and not necessarily all of them have been registered digitally.
Due to the scarcity of the data, and considering that to
train a machine learning model it usually takes at least tens of
thousands of examples, the datasets were generated artiﬁcially.
Three separate datasets were generated: the train dataset ,
with 10 thousand instances for the training of the model; the
validation dataset with 100 instances, used for validation step
during the training; and the test dataset , with 10 thousand
instances, used to evaluate the performance of each one of the
employed methods. This number of instances for the validation
dataset was chosen because it was sufﬁciently big so that still
kept roughly the same properties as the train and test datasets,
but small enough that the validation does not slow down the
training too much.
To generate each instance, ﬁrst 300 separated nodes are
created, and then 5500 edges are added sequentially linking
random nodes, while still guaranteeing that no two edges
connect the same two nodes in the same direction. The weight
value of each edge is sampled from an uniform distribution
of values between 0 and 1. To choose the number of nodes
and edges of the KEP generated instances, the instances used
for benchmarking in [2] were used as reference. Considering
the 25 instances presented in the Table S3, which they call
”difﬁcult” real-data instances, the average number of nodes on
a KEP instance is 265.84, and the average number of edges
is 5695.92. Thus, the values for the number of nodes and
number of edges chosen were 300 and 5500, respectively. The
proportion of the types of nodes was also chosen to be similar
to the instances: roughly 90% of the nodes are PDP nodes,
5% are NDD nodes, and 5% are P nodes.
To keep track of the instances, an unique ID was assigned
to each. For this, the Weisfeiler Lehman graph hash [26] was
used, which guarantees that non-isomorphic graphs will get
different hashes; it also considers node and edge features in its
computation, thus differentiating even between graphs which
have the same structure but different edge weight values.
IV. M ETHODS
We classiﬁed the methods for solving the KEP in 3 cate-
gories: integer programming methods, non-learnable heuristic
methods, and learnable heuristic methods. All of them use the
same input information, which is a KEP instance, and return
the solution in the same format, which is a binary label for
each edge, indicating if it is in the solution or not. All of them
are evaluated on the test dataset, with the exception of the
integer programming method, as later explained in Section V,
but only the learned heuristics use the training dataset, during
their training phase.
A. Integer Programming
To obtain the analytical solution, i.e. the optimal solution,
the formulation presented in Subsection I-B was implementedusing the PyCSP3 Python library [27]. There are other integer
programming formulations for KEP, including two presented
in [2], as well as others in [28], [7] and [29]. This formulation
was chosen because it is the most straightforward one.
B. Non-Learnable Heuristics
To evaluate the implemented methods that use machine
learning, we decided to compare them to non-learnable heuris-
tics, i.e. heuristic methods that do not use learning techniques.
This section aims to describe these non-learnable heuristic
methods. To the best of our knowledge, however, there are
no canonical heuristics for the KEP. For this reason, we
implemented two search heuristics, which are described below.
1) Greedy Paths: This algorithm greedily selects paths that
start on NDD nodes and goes through PDP or P nodes one by
one until there is no more nodes to be selected, or until a P
node is reached. It starts by selecting the edge with the highest
weight considering only the subset of edges that have an NDD
node as source. Then, considering only the edges that come
from the previous node, it follows by selecting always the next
edge with the highest weight, until there are no more available
edges left that would continue the path. After a path has been
added to the solution, the edges connected to nodes of this
path are masked, and Greedy Paths repeats the process until
no more NDD nodes with valid outgoing edges are available.
This algorithm is described at Algorithm 1.
Algorithm 1 Greedy-Paths
procedure GREEDY -PATHS (G= (N; E ); k)
paths []
whilejGP(G; k)j>0do
path GP(G; k)
paths paths[path]
G (Nnfn8n2pathsg; Enfe8e2
E; src (e) =n_tgt(e) =ng)
return paths
procedure GP(G= (N; E ); k).Gets one Greedy Path
ENDD fe8e2E; src (e)2NDDg
ifjENDDj= 0 then
return []
ec arg maxe2ENDDgwe
path [src(ec)]
whilejOE(tgt(ec))j>0^jpathj<(k+ 1) do
path path[tgt(ec)]
ec arg maxe2OE(tgt(ec))nS
n2pathIE(n)we
return path
procedure OE(G= (N; E ); n) .Outgoing Edges
returnfe8e2E; src (e) =ng
procedure IE(G= (N; E ); n) .Incoming Edges
returnfe8e2E; tgt (e) =ng
2) Greedy Cycles: This algorithm greedly selects cycles of
PDP nodes. It starts by selecting the edge with the highest
weight considering only the subset of edges that have a PDP
node as source and a PDP node as destination. Then, PDP
nodes are greedly added to the solution in the same way asdone by the Greedy Paths method until the cycle ends, or
until it arrives at a node already added in the cycle, in which
case the cycle is closed and the nodes before the current node
are removed from the cycle. After a cycle is added to the
solution, the Greedy Cycles algorithm applies a mask on the
edges connected to nodes of this cycle, and then repeats the
process until no more PDP nodes with valid outcoming edges
are available. This algorithm is described at Algorithm 2.
Algorithm 2 Greedy-Cycles
procedure GREEDY -CYCLES (G= (N; E ); k)
cycles []
whilejGC(G; k)j>0do
cycle GP(G; k)
cycles cycles[cycle]
G (Nnfn8n2cyclesg; Enfe8e2
E; src (e) =n_tgt(e) =ng)
return cycles
procedure GP(G= (N; E ); k).Gets one Greedy Cycle
EPDP fe8e2E; src (e)2PDP; dst (e)2PDPg
ifjEPDPj= 0 then
return []
ec arg maxe2EPDPgwe
cycle [src(ec)]
while tgt(ec)=2cycle do
ifjOE(tgt(ec))j0_jcyclejkthen
return [].Unable to close cycle (dead end)
cycle cycle[tgt(ec)]
ec arg maxe2OE(tgt(ec))nS
n2cycleIE(n)we
return cycle
procedure OE(G= (N; E ); n) .Outgoing Edges
returnfe8e2E; src (e) =ng
procedure IE(G= (N; E ); n) .Incoming Edges
returnfe8e2E; tgt (e) =ng
C. Learnable Heuristics
As KEP instances are graphs, using GNNs to extract more
detailed and abstract information can help in constructing an
approximate solution. Therefore, GNN models were chosen as
the main learning module for the machine learning methods.
GNN Architecture: The architecture of the GNN used in this
work is the following: ﬁrstly, there is a message passing phase,
where the original node features are passed through a PNA
layer [30], and then through two consecutive GATv2 layers
[31]; after each message passing layer, a ReLU activation
function is applied, followed by a dropout regularization;
next, the node features are passed through a fully connected
feed forward neural network, followed by another ReLU
activation function; then, the edge features are constructed
by concatenating the original input edge features with the
node features of the origin and destination nodes associated
to each edge; these edge features are then passed through a
fully connected feed forward neural network, which outputs a
score for each edge; at this point, a skip connection adds the
original edge weights to the edge scores; ﬁnally, a node-wisesoftmax operation, which is described below, is applied so as
to normalize these scores in relation to the scores of other
edges that share the same source node.
In order to make information ﬂow not only in the orig-
inal direction of the original edges, each message passing
layer is accompanied by an associated layer, which we call
counter edge layer , that is exactly similar, but with different
learned weights, and with the difference that the information
is propagated in the opposite direction, i.e. ﬂowing from the
destination node to the origin node. Each time message layers
are executed, the output of the original and of the counter edge
layers is concatenated before being passed to the next layers.
KEP Unsupervised Loss: This loss function was designed
to capture, without the need for the exact solution as a label
or any other supervision, the essence of what we are trying
to maximize: the sum of weights of edges that are in the
predicted solution. It is deﬁned as the log of the sum of
weights of all edges of the input instance over the sum of
weights of edges that are in the predicted solution, weighted
by the scores predicted by the GNN. This loss function is
presented in Formula 11, where wrepresents the vector of
edge weights, pred represents the vector of predicted classes
(which indicates if each edge is contained in the solution or
not), and srepresents the vector of scores attributed by the
GNN for each edge.
KEP Loss (w; pred; s ) = logP
e2EweP
e2Ewepred ese(11)
Loss Constraint Regularization: In order to integrate infor-
mation about the KEP ﬂow constraints (Eq. 4, Eq. 5 and Eq.
6) into the learning of the model, a loss regularization function
was developed. It aims to model the restriction that each node
must have at maximum one single outcoming edge and one
incoming edge that are part of the solution. It is deﬁned as
the log of the division between the total quantity of edges in
the solution and the number of unique nodes that appear in
the solution as a origin/destination node. This makes it so that
the regularization term value is proportional to the number of
invalid edges in the solution, i.e. the total quantity in the graph
of extra edges for each source/destination node. This function
can then be added to the unsupervised loss (described in the
subsection above) by summing their output values, weighted
by coefﬁcients, which become new hyper-parameters of the
training.
Node-wise Softmax: The node-wise softmax operation is
the application of an independent softmax operation for each
group of edges that share the same source node, as one can see
in Equation 12, where se(ni;nj)represents the edge score of the
edge connecting node ito node j. In this way, for each node
we will have a probability for each outgoing edge; in KEP,
these values may represent a probability distribution for the
donation options of the donor for each source node. Although
in this work the operation was used grouping edges by source
node, with a simple change of a parameter it can group edges
by groups of common destination node as well.node-wise-softmax( se(ni;nj)) =e se(ni;nj)
P
nk2N(ni)e se(ni;nk)
(12)
To the best of our knowledge, this is the ﬁrst time this
operation is proposed in the literature. It is potentially useful
for any edge classiﬁcation task on graphs, specially when the
problem involves constraints in which only one edge may
be chosen per node, be it destination or origin node. These
constraints are very common in optimization problems in
graphs; this is the case for KEP, for example: each donor or
patient-donor pair may donate at most one kidney, and each
patient or patient-donor pair may receive at most one kidney.
1) Unconstrained GNN Model: This method, referred to
from now on as Unsupervised GNN , consists of a GNN model
which receives a KEP instance and outputs, for each edge
of the input instance, a score and a binary prediction, which
indicates if the edge is part of the predicted solution or not.
The binary prediction is made independently for each edge,
and consists of a simple decision threshold. This GNN model
is trained using the unsupervised loss described at subsection
IV-C with the loss regularization term described at subsection
IV-C. Although there is no guarantee that the solutions given
by this method will be valid, the loss regularization term
is used with the goal of inducing it to respect the problem
constraints.
2) Two Stage Method: Inspired by the approach used in
[17] and [18], this method follows a two steps structure: ﬁrstly,
the learnable step, which is a GNN, takes the KEP graph
instance as an input and outputs a score for each edge; then,
the non-learnable step, which is one of the heuristic methods
described above in the IV-B section is executed, but using the
scores given by the GNN instead of the edge weights. This
process is illustrated in the diagram on Figure 1. The intuition
behind this idea is that the GNN model will learn to encode
in the edge score contextual information that will change the
decisions of the search heuristic so as to maximize the total
score of the ﬁnal output solution.
This GNN model is trained without supervision using loss
described at subsection IV-C. There is no need to use the loss
regularization term described at subsection IV-C because the
second step of the method ensures that the output will be a
valid solution.
Two versions of the two stage method were implemented for
the experiments. Both use the GNN described in subsection
IV-C for the ﬁrst stage, but their second stage consist of
different search heuristics. One uses the Greedy Paths search
heuristic described at subsection IV-B1 and is referred to later
on as GNN+GreedyPaths . The other uses the Greedy Cycles
search heuristic described at subsection IV-B2 and is referred
to later on as GNN+GreedyCycles .
V. E XPERIMENTS
The main goal of the experiments was to assess the machine
learning methods, and compare them to the deterministicFig. 1. Diagram representing an overview of the two stage method . The GNN
takes the input KEP instance and computes a score for each edge of the graph;
then, a greedy heuristic such as GreedyCycles orGreedyPaths uses these edge
scores instead of the original edge weights to build an approximate solution,
which is a binary label for each edge ( edge labels ), indicating if the edge is
part of the approximate solution predicted or not. Source: Author.
heuristics and to the exact solution, both in terms of the quality
of the solutions as well as of their operational performance,
i.e. the time it takes for a solution to be calculated. The integer
programming method, however, could not be measured in the
same dataset because it took to long to run the solver.
The objective of the Kidney Exchange Problem is to maxi-
mize the number of donations weighted by their compatibility
index, i.e. their associated edge weights in the graph. There-
fore, this was the main metric used to quantify the quality
of each solution and to compare the different methods, and
is also referred to as score in this study. As the operational
performance of the methods was also to be compared, the
total time that each method took to run per instance was also
measured.
Ideally, the heuristics could be compared by using the
optimality gap, which is deﬁned as the distance between
the heuristic solution score and the optimal solution score.
However, as the randomly generated KEP instances of 300
nodes have shown to be intractable while using the PyCSP3
solver, i.e. impossible to solve optimally in a reasonable time,
the optimality gap could not be measured.
For the trainable heuristics, the total training time was also
measured and compared. To monitor and guide the training
process, the evolution of the loss function value over the
training time was also collected. Additionally, the mean score
of the current model in the validation dataset was measured
at each validation phase.
For a fair comparison between methods, all the measure-
ments were made in the same test dataset, which is described
in section III, with the exception of the exact solution method,
as explained in Section V below.
A. Solver Execution Time Analysis
To measure how much time it takes to solve a KEP instance
in relation to the input size, the following experiment was
designed: ﬁrst, we randomly generate 100 instances of each
graph size (i.e. number of nodes), starting from 5 nodes and
up to 300 nodes; then, each one is solved optimally using the
integer programming method described at section IV-A. The
elapsed time of each solver execution is collected for later
analysis.B. Training of the ML Models
The training of the ML models was separated in epochs. In
each epoch, we iterate through the 10 thousand instances of
the train dataset, predicting, calculating the loss, and updating
the GNN weights according to it. At every 500 instances, a
validation phase is run, where the model is evaluated on the
validation dataset and a checkpoint is saved. The chosen batch
size was 1, which means that the predictions were done on one
instance at a time, because it empirically seemed to be the best
for the learning process. There were two machine learning
models that were trained in this study: the unconstrained
unsupervised GNN and the two stage method, which are
described at Section IV-C. For each training process, it was
measured how the loss value evolved over time in the training
and in the validation datasets.
C. Evaluation of KEP Solving Methods
In order to do a fair comparison between the different
heuristic methods, all of them were evaluated by predicting
the solutions of all 10 thousand instances of the test dataset.
We did not set the cycles and paths size limit (parameter kin
constraint represented by Eq. 7), i.e. the cycles and paths could
have any length, as long as they respect the KEP constraints.
We intend to assess the performance of these methods with
limited cycles and paths length soon. For each prediction, it
was measured the solution score, the number of edges in the
solution, and the relation between the solution score (which
is the sum of the edge weights of the solution edges) and
the total sum of edge weights of the graph. In addition, the
validity of the predicted solution was evaluated; if invalid, we
measure the number of invalid edges in the solution, i.e. the
number of edges that disrespect the restrictions. Furthermore,
we measured the time it took for each method to solve
each instance using a CPU. Then, it was measured, for each
model in relation to the whole test dataset, the mean, standard
deviation, and distribution for the scores and the prediction
elapsed times.
VI. R ESULTS
A. Solver Time Measurements
Figure 2 shows a box plot of the time that the solver took
to optimally solve KEP instances in relation to the instance
size. For that, graphs of sizes 5 to 15 (i.e. number of nodes)
were used; initially, graphs with up to 300 nodes were going
to be included in the analysis, but as solving graphs with 16
nodes or more would take several days to compute, they were
excluded. To solve a hundred instances with 15 nodes, for
instance, it took 101.2 hours in total.
As we can see, the experiment results show a pattern of
exponential growth of computational time in relation to the
input size. The mean time it took when the graph node number
was beneath 10 was always below 1.5 seconds. For instances
with 15 nodes, the mean time measured was 3569.33 seconds,
i.e. roughly one hour.Fig. 2. Boxplot of the time it takes to run the solver on KEP instances of
sizes 5 to 15 (i.e. number of nodes).
Fig. 3. Evolution of the training and validation loss for GNN+GreedyPaths
method.
B. Training of the ML Models
Figures 3, 4, and 5 show the evolution of the loss value on
the training and validation datasets for the two stage meth-
ods described at Subsection IV-C2, GNN+GreedyPaths and
GNN+GreedyCycles , and for the Unsupervised GNN method
described at Subsection IV-C1, respectively.
Figures 6, 7, and 8 show the evolution of the the mean
score value for GNN+GreedyPaths ,GNN+GreedyCycles , and
Unsupervised GNN , respectively. Figure 9 shows the evolution
of the standard deviation of the scores predicted on the
validation dataset.
C. Methods’ Performances
Figure 10 shows a box plot of the approximate solution
scores (i.e. the sum of the weights of the edges contained in
the approximate solution) achieved by each method in the test
Fig. 4. Evolution of the training and validation loss for GNN+GreedyCycles
method.
Fig. 5. Evolution of the training and validation loss for the Unsupervised
GNN method.
dataset, with the exception of Unsupervised GNN andinteger
programming . As all the solutions found by the Unsupervised
GNN method were invalid, there was no reason to evaluate
their quality. The integer programming method is also absent
from the plot, as it was not possible to run it in instances with
300 nodes.
As we can see, the GreedyPaths heuristic method found
much better solutions than GreedyCycles. The 2 stage method
variations, GNN+GreedyCycles andGNN+ GreedyPaths , ob-
tained very different performances. Unfortunately, the GNN
module in the GNN+GreedyCycles method did not manage to
learn to output edge scores that help the heuristic; on the con-
trary, the quality of its approximate solutions are considerably
worse than those from GreedyCycles . The GNN+GreedyPaths
method, however, effectively learned to output better solutionsFig. 6. Evolution of the score measured in the validation dataset for the
GNN+GreedyPaths method.
Fig. 7. Evolution of the score measured in the validation dataset for the
GNN+GreedyCycles method.
than the non-learnt heuristic achieving a mean score of 228.40
on the test dataset, while GreedyPaths obtained 203.79; this
shows an improvement of 12% of the mean solution score
with the use of the GNN. We can also see that, although
the best scores achieved by each of them are very similar,
the score distribution is very different: while GreedyPaths
outputs approximate solutions with very large range of scores,
GNN+GreedyPaths ’s approximate solutions have much more
consistent scores, obtaining a decent performance throughout
all instances of the dataset.
D. Methods’ Computational Time
The box plot in Figure 11 shows the comparison between
the time it took for each method to solve the KEP instances
of the test dataset. As we can see, all of them took less then
a second. Although the difference is not large, the two basic
Fig. 8. Evolution of the score measured in the validation dataset for the
Unsupervised GNN method.
Fig. 9. Evolution of the standard deviation of score measured in the validation
dataset for the GNN+GreedyPaths method.
search heuristics took less time than the GNN based methods,
andGNN+GreedyPaths took, on average, the most time.
VII. A NALYSIS OF EXPERIMENTAL RESULTS
A. Solver Time Analysis
Because the Kidney Exchange problem is NP Hard, the
time it takes to optimally solve each instance is expected to
grow exponentially as the instance size grows. Regardless,
considering that in [2] real life KEP instances could be
optimally solved in a reasonable time, it was expected that
we would also be able to optimally solve the ones used in
this study, since they have been constructed to have similar
sizes to the ones used in the article. However, as described in
Subsection VI-A, instances of size as small as 15 already took
in average 1 hour to solve. Considering that we would wantFig. 10. Box plot comparing the approximate solution scores obtained when
each of the evaluated methods was used in the test dataset. The evaluated
methods were two non-learnt heuristics, GreedyCycles andGreedyPaths , and
their 2 stage method versions, GNN+GreedyCycles andGNN+GreedyPaths .
Fig. 11. Box plot comparing the time to compute a solution on each
of the 10 thousand KEP instances of the test dataset, each one with 300
nodes. The evaluated methods were two non-learnt heuristics, GreedyCycles
and GreedyPaths , their 2 stage method versions, GNN+GreedyCycles and
GNN+GreedyPaths , and UnsupervisedGNN , which is a GNN trained and used
without an heuristic.
to optimally solve all 10 thousand instances of the test set in
order to fairly compare to the other methods and to measure
their optimality gap, this process would take an unreasonable
time, estimated to be around 10 thousand hours, i.e. roughly
1.14 years. This estimate is only if the KEP instances on the
test dataset had 15 nodes; for instances with 300 nodes, it
would surely take an unreasonably enormous amount of time.
The number of nodes of the input instance is not, however,
the sole factor that determines the time it takes to optimally
solve it; in a set of instances with the same number of nodes,
some are ”harder” than others, i.e. take more time to solve.
As the instance size increases, so does the variability of thetime to solve it: the minimum and maximum times measured
for instances with 15 nodes were 5.17 seconds and 29779.01
seconds (i.e. roughly 8 hours); for comparison, the minimum
and maximum times for instances with 5 nodes were 0.8 and
1.3 seconds. Hence, it is possible that, while some real life
instances are solvable in a reasonable time, a percentage of
them would take too long to solve, thus becoming intractable.
There can be several reasons why the authors of [2] could
compute the optimal solution of their KEP instances in much
less time. First of all, they probably used a solver tool that is
much more efﬁcient than PyCSP3. In addition, the computer
used may be much more powerful than the one used in this
study. Also, it is important to remember that the NP-Hardness
of KEP guarantees that the computational time it takes to solve
the worst case scenario grows exponentially, but in practice
real life instances may often have speciﬁc properties which
may cause them to be either harder or easier to solve. Hence,
another plausible reason is that their instances may be much
easier to solve. Furthermore, the authors used a constrain
relaxation technique that speeds up signiﬁcantly the solving
process. These set of reasons alone may not explain totally
why they were able to optimally solve their KEP instances
much faster than we did on our data; this may be further
investigated in future work.
B. Training of the GNN model
As we can see in Figure 3, the training of the
GNN+GreedyPaths method was successful, seen as it managed
to optimize the GNN by minimizing the loss function. The
training of the GNN+GreedyCycles and Unsupervised GNN
methods, however, were unsuccessful, as shown by the loss
curves of Figures 4 and 5, which do not decrease over time.
Although at ﬁrst glance at Figure 8 the Unsupervised
GNN model seems to achieve great scores, unfortunately all
its output solutions were invalid, i.e. they did not comply
to the KEP constraints. It is clear that the loss constraint
regularization (described at IV-C) added to the loss function
was unsuccessful in helping the model learn to comply to
the KEP constraints. This highlights the necessity of having a
methods that guarantee with total certainty that all its output
solutions are valid. However, even though there were many
manual trials with different hyperparameter combinations, it
is still possible that a variation of this technique could work
with a different setting, i.e. another GNN architecture, other
hyperparameters, and so on.
Because of the skip connection that sums the original edge
weights to the predicted edge scores at the end of the GNN, the
predicted solutions start off very similar to the ones made by
GreedyPaths . Then, the changing of the GNN weights disrupts
these scores, which increases the loss, but goes on to improve
them, eventually arriving at a performance that is better than
GreedyPaths . After some point (around epoch 6 in Figure 6),
the learning converges to a solution, and after a while the
performance starts to slowly worsen. The ﬁnal model was
chosen from the checkpoint with the highest score measured
on the validation dataset, which was in epoch 6, step 3500. Aswe can see on Figure 9, at this point the model’s scores on the
validation also presented the lowest standard deviation, which
indicates that the model’s predictions were more consistent,
maintaining a decent performance throughout all instances.
C. Methods’ Performances
Ideally, we would want to evaluate and compare each
method by measuring their optimality gap for each instance,
i.e. how far the approximate solution is from the optimal one.
However, as we do not have access to the optimal solution,
this was not possible. We can nevertheless estimate it roughly
by examining an upper bound: each instance has 300 nodes,
and each node may donate and receive at most one kidney;
thus, the solution with the most edges would be contain cycles
that together comprehend all nodes. As each edge weight is
a value between 0 and 1, the maximum score possible is
equal to 300, when all edges in the cycle have a weight of
1. Hence, an upper bound for the score is the number of
nodes, which in this case is 300. This is obviously extremely
unrealistic, as it assumes that all nodes are PDPs, that there
are a set of cycles that links all of them, and that the solution
edge weights are equal to 1 (as the edge weights are values
sampled from a uniform distribution between 0 and 1, their
average value is 0.5). Considering the score upper bound of
300 as a very conservative estimate for the mean optimal
solution value, we can estimate that the absolute and relative
optimality gap for the GreedyPaths method would be 96.28
and 32%; for GreedyCycles , these values would be 286.29
and 95.4%; for GNN+GreedyPaths , 71.59 and 23.8%; for
GNN+GreedyCycles , 299.15 and 99.7%. Hence, the improve-
ment on the optimality gap of the GNN+GreedyPaths method
in relation to GreedyPaths would be at least 34.4% (96.28 to
71.59), which is already a very substantial improvement.
It is clear that the two methods that searched for paths
performed much better than the ones that searched for cycles.
There are many possible explanations for this observed be-
haviour. Maybe the best cycles-only solution in a KEP instance
is usually much worse than the best paths-only solution.
This, however, can only be veriﬁed by making comparisons
to the cycles-only and paths-only exact solutions, which are
unavailable. Also, the GreedyCycles method is probably less
efﬁcient because it discards the path it is constructing if it does
not end up closing a cycle. It also shortens the constructed
cycle if it closes before the node it had begun on, which
may also lead to worse performance overall. The GreedyPaths
method does not have these issues, as it always keeps the edges
it adds to each path it constructs.
We can also observe that the while the GNN module in
theGNN+GreedyPaths improved the performance in relation
to the basic non-learnable heuristic, in GNN+GreedyCycles
it only worsened it. It is possible that its GNN module in
GNN+GreedyCycles could not learn the needed context to
know if a given edge would lead to a longer and higher-valued
cycle because it is too complex, and does not depend that
much on the 3-neighborhood context, which is the limit of
information gather in each node with the GNN architectureused, as it only has 3 message passing GNN layers. Another
possible explanation is that it is way harder for a model to learn
to compute edge scores that help the choices of GreedyCycles
because it is inherently more complex than GreedyPaths , i.e. it
is not just a sequence of simple decisions, as it also has to keep
track of the rest of the nodes of the cycle being constructed,
check if it closed a cycle, and remove from the solution in
construction the edges added before the node where the cycle
was closed. Put simply, the more complex the second step
heuristic is, the harder it is for a machine learning model to
learn to help it.
As explained at Section VI-C, GNN+GreedyPaths approx-
imate solutions have much more consistent scores, which
suggests that it probably handle much better ”hard” instances.
A plausible interpretation is that the GNN module helps the
subsequent greedy heuristic to avoid choosing edges that are
only locally good, but lead to worse paths overall. It is able to
do this because it considers information of the neighborhood
context.
D. Methods’ Computational Time
The GNN computational complexity is linear, as it performs
a ﬁxed amount of computations per graph node plus another
ﬁxed amount of computations per edge. As for the heuristic
methods GreedyPaths andGreedyCycles , their computational
complexity is also linear, as the worst case scenario one
edge for each node will be added, one by one, into the
solution; hence, it always performs a quantity of computational
operations linearly proportional to the number of nodes of the
input instance, at worst.
The results from Subsection VI-D show that every method
tested in this work took very little time to execute, with
the exception of the integer programming method, which
took so much time that applying it to 300 nodes instances
became intractable. The GreedyPaths method took a bit more
time than GreedyCycles probably because it found better
solutions overall, and consequently took more computing steps
to construct each solution. The same effect may also explain
the prediction time difference between GNN+GreedyPaths and
GNN+GreedyCycles . The UnsupervisedGNN method took a
bit less time to execute than the two step methods, which was
expected because it runs the same computations, but without
the second step, which is the basic search heuristic.
VIII. C ONCLUSION AND FUTURE WORK
In this work, several heuristic methods with and without ma-
chine learning for approximately solving the Kidney Exchange
Problem were proposed and investigated. They were tested on
an artiﬁcial dataset and compared between each other and with
an implementation of an exact solution method. Additionally,
it was made an experiment for measuring the time it took for
the exact solution method to solve an instance in relation to the
instance size. The results of the evaluations and experiments
were then analysed and discussed.A. Answers to the Research Questions
As seen from the results in subsection VII-C, the main ques-
tion presented at Section I was answered: yes, the Kidney-
Exchange problem can be better approximately solved with
the help of machine learning .
As for the feasibility of the ML methods, the
GNN+GreedyPaths method surpassed all other evaluated
heuristics in terms of the quality of the solutions it provides;
among all evaluated methods, it remains the one that best
approximately solves the dataset instances in a reasonable
time. The other ML methods evaluated in the work
(UnsupervisedGNN and GNN+GreedyCycles ), however, did
not achieve good results.
Regarding the viability of such methods in terms of com-
putational time, the GNN module adds an almost insigniﬁ-
cant overhead when compared to the non-learnable heuristics.
When compared to the solver running the exact solution, it is
several orders of magnitude faster for instances with at least
15 nodes. The complexity of the two stage methods is linear,
turning instances that were previously intractable due to their
size into easily approximately solvable in a reasonable time.
As for the limitations of the employed machine learning
methods for this problem, it is clear that they are not simple
to use, as they need to be properly trained, which is not
easy to do. Although using the two stage method potentially
improves considerably the performance in relation to the basic
heuristics, as happened with the GNN+GreedyPaths method,
it also introduces several new hyperparameters which need to
be adequately set in order for the method to work. Applying
supervised learning turned out to be unfeasible because of
the need for the exact solution to be used as edge labels.
Regarding the UnsupervisedGNN method, our results suggest
that imposing the constraints through adding terms in a loss
function is actually really hard; hence, the method never learns
to output valid solutions, rendering it useless.
B. Main Contributions
In the following list, a summary of each of the main
contributions that this work provided is presented.
Learnable Heuristics for KEP: Although the two stage ap-
proach was already used by past work [17], [18], [24], this was
the ﬁrst work to adapt it and apply it to KEP. Two variations
of the approach were implemented: GNN+GreedyPaths and
GNN+GreedyCycles , the ﬁrst one having achieved satisfactory
performance.
Non-Learnable Heuristics for KEP: Two new determin-
istic heuristic methods for approximately solving the Kidney
Exchange Problem were introduced: GreedyPaths andGreedy-
Cycles . They were also evaluated in the test dataset, giving
insight on their effectiveness.
Node-wise Softmax: A variation of the softmax activation
function designed to be applied to edge scores in graph
problems was created and implemented. It showed to be useful
for the GNN, empirically improving the its performance. The
author plans to contribute to the PyTorch library with theimplementation of this technique, thus making it available and
easily usable by its future users.
KEP Unsupervised Loss: A novel loss function (described
at IV-C) designed for KEP was introduced. It optimizes the
weighted sum of the edges in the predicted solution without
the need for supervision. It was validated in the training of
theGNN+GreedyPaths method, as seen in Figure 3 in Section
VI-B, where it lead the GNN to learn to effectively help the
heuristic method construct better approximate solutions.
C. Future Directions
Using real data: Use data collected by countries’ or
hospital’s healthcare system to evaluate how the presented
methods would perform in real life situations. This would also
allow us to compare our proposed methods with others that
were already evaluated in the same data.
Using better artiﬁcial data: There more sophisticated
methods for generating artiﬁcial KEP instances, such as the
ones presented at [32] and [3]. They are still far from suf-
ﬁciently similar to real data so as to substitute evaluating it.
However, it would still probably give an evaluation of KEP
solving methods that is closer to that of real life situations.
Furthermore, training the model with these instances could
also potentially lead to better results. Another possibility of
generating better artiﬁcial data would be to use a graph
generator model that learns to create instances similar to the
real data; the Graph Variational Auto-Encoder presented at
[33] and the MolGAN presented at [34] are good examples of
candidate methods to be adapted for that goal.
Training the models with supervised learning: Another
direction is to develop a method that learns with supervision,
evaluate it, and compare it to the other methods. For that, we
would ﬁrst have to obtain the optimal solutions, which would
then be used as labels for the supervised training. This could
be done either by improving a lot the integer programming
method’s speed and/or by training on much smaller instances
(i.e. instances with less than 15 nodes). Another promissing
variation of this idea is to use the N best solutions, and create
soft labels where each edge would have a value between 0
and 1 that would indicate how often it appears in the best
solutions, weighted by the quality of these solutions.
Running a GNN before every step of the greedy heuris-
tic:Instead of running the GNN once and then passing the
edge scores to the greedy heuristic method, new node embed-
dings could be generated before each step of the heuristic.
Although signiﬁcantly costlier in terms of inference, as the
GNN is executed many times per instance, this has shown
good results for other graph route optimization problems [22].
This approach is called autoregressive decoding and explored
for solving the TSP by [18].
ACKNOWLEDGMENTS
This work is supported in part by the Brazilian Research
Council CNPq and the CAPES Foundation - Finance Code
001.REFERENCES
[1] A. Roth, T. S ¨onmez, and U. Unver, “Kidney exchange,” The Quarterly
Journal of Economics , vol. 119, no. 2, pp. 457–488, 2004.
[2] R. Anderson, I. Ashlagi, D. Gamarnik, and A. E. Roth, “Finding
long chains in kidney exchange using the traveling salesman problem,”
Proceedings of the National Academy of Sciences , vol. 112, no. 3,
pp. 663–668, 2015.
[3] M. Delorme, S. Garc ´ıa, J. Gondzio, J. Kalcsics, D. Manlove, W. Petters-
son, and J. Trimble, “Improved instance generation for kidney exchange
programmes,” Computers and Operations Research , vol. 141, p. 105707,
2022.
[4] D. A. Axelrod, M. A. Schnitzler, H. Xiao, W. Irish, E. Tuttle-Newhall,
S.-H. Chang, B. L. Kasiske, T. Alhamad, and K. L. Lentine, “An eco-
nomic assessment of contemporary kidney transplant practice,” Ameri-
can Journal of Transplantation , vol. 18, no. 5, pp. 1168–1176, 2018.
[5] “Longest kidney transplant chain - guinness world record organization
distinguishes the national kidney registry for world’s longest kidney
transplant chain.” https://transplantsurgery.ucsf.edu/news–events/ucsf-
news/88223/UCSF-Part-of-Longest-Kidney-Transplant-Chain—
Guinness-World-Record-Organization-Distinguishes-the-National-
Kidney-Registry-for-World%E2%80%99s-Longest-Kidney-Transplant-
Chain, dec 2020. Accessed: 2023-03-01.
[6] P. Bir ´o, J. van de Klundert, D. Manlove, W. Pettersson, T. Andersson,
L. Burnapp, P. Chromy, P. Delgado, P. Dworczak, B. Haase, A. Hemke,
R. Johnson, X. Klimentova, D. Kuypers, A. Nanni Costa, B. Smeulders,
F. Spieksma, M. O. Valent ´ın, and A. Viana, “Modelling and optimisa-
tion in european kidney exchange programmes,” European Journal of
Operational Research , vol. 291, no. 2, pp. 447–456, 2021.
[7] D. J. Abraham, A. Blum, and T. Sandholm, “Clearing algorithms for
barter exchange markets: Enabling nationwide kidney exchanges,” in
Proceedings of the 8th ACM Conference on Electronic Commerce , EC
’07, (New York, NY , USA), p. 295–304, Association for Computing
Machinery, 2007.
[8] Y . Yang and J. Rajgopal, “Learning combined set covering and traveling
salesman problem,” arXiv preprint arXiv:2007.03203 , 2020.
[9] H. Lemos, M. O. R. Prates, P. H. C. Avelar, and L. C. Lamb, “Graph
colouring meets deep learning: Effective graph neural network models
for combinatorial problems,” in 31st IEEE International Conference
on Tools with Artiﬁcial Intelligence, ICTAI 2019, Portland, OR, USA,
November 4-6, 2019 , pp. 879–885, IEEE, 2019.
[10] H. L. d. Santos, “Solving the decision version of the graph coloring
problem: a neural-symbolic approach using graph neural networks,”
Master’s thesis, UFRGS Federal University, Porto Alegre, Brazil, 2020.
[11] R. Sato, M. Yamada, and H. Kashima, “Approximation ratios of graph
neural networks for combinatorial problems,” in Advances in Neural
Information Processing Systems 32 (H. M. Wallach, H. Larochelle,
A. Beygelzimer, F. d’Alch ´e-Buc, E. B. Fox, and R. Garnett, eds.),
pp. 4083–4092, 2019.
[12] K. Abe, Z. Xu, I. Sato, and M. Sugiyama, “Solving np-hard problems
on graphs with extended alphago zero,” arXiv , 2019.
[13] E. B. Khalil, H. Dai, Y . Zhang, B. Dilkina, and L. Song, “Learning
combinatorial optimization algorithms over graphs,” in Advances in
Neural Information Processing Systems 30 (I. Guyon, U. von Luxburg,
S. Bengio, H. M. Wallach, R. Fergus, S. V . N. Vishwanathan, and
R. Garnett, eds.), pp. 6348–6358, 2017.
[14] A. Nazi, W. Hang, A. Goldie, S. Ravi, and A. Mirhoseini, “Gap:
Generalizable approximate graph partitioning framework,” arXiv , 2019.
[15] Z. Li, Q. Chen, and V . Koltun, “Combinatorial optimization with
graph convolutional networks and guided tree search,” in Advances in
Neural Information Processing Systems 31 (S. Bengio, H. M. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds.),
pp. 537–546, 2018.
[16] Y . Bai, D. Xu, Y . Sun, and W. Wang, “GLSearch: Maximum common
subgraph detection via learning to search,” in Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event (M. Meila and T. Zhang, eds.), vol. 139 of
Proceedings of Machine Learning Research , pp. 588–598, PMLR, 2021.
[17] C. K. Joshi, T. Laurent, and X. Bresson, “An efﬁcient graph convolu-
tional network technique for the travelling salesman problem,” arXiv ,
2019.
[18] C. K. Joshi, Q. Cappart, L.-M. Rousseau, and T. Laurent, “Learning tsp
requires rethinking generalization,” in 27th International Conference onPrinciples and Practice of Constraint Programming (CP 2021) , Schloss
Dagstuhl-Leibniz-Zentrum f ¨ur Informatik, 2021.
[19] M. O. R. Prates, P. H. C. Avelar, H. Lemos, L. C. Lamb, and M. Y .
Vardi, “Learning to solve np-complete problems: A graph neural network
for decision TSP,” in The Thirty-Third AAAI Conference on Artiﬁcial
Intelligence, AAAI 2019 , pp. 4731–4738, AAAI Press, 2019.
[20] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” Advances
in neural information processing systems , vol. 28, 2015.
[21] Y . Wu, W. Song, Z. Cao, J. Zhang, and A. Lim, “Learning improvement
heuristics for solving routing problems,” IEEE transactions on neural
networks and learning systems , vol. 33, no. 9, pp. 5057–5069, 2021.
[22] W. Kool, H. van Hoof, and M. Welling, “Attention, learn to solve
routing problems!,” in 7th International Conference on Learning Rep-
resentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 ,
OpenReview.net, 2019.
[23] I. Bello, H. Pham, Q. V . Le, M. Norouzi, and S. Bengio, “Neural combi-
natorial optimization with reinforcement learning,” in 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Workshop Track Proceedings , OpenReview.net, 2017.
[24] Y . Peng, B. Choi, and J. Xu, “Graph learning for combinatorial opti-
mization: A survey of state-of-the-art,” Data Science and Engineering ,
vol. 6, pp. 119–141, Jun 2021.
[25] L. C. Lamb, A. S. d’Avila Garcez, M. Gori, M. O. R. Prates, P. H. C.
Avelar, and M. Y . Vardi, “Graph neural networks meet neural-symbolic
computing: A survey and perspective,” in Proceedings of the Twenty-
Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI
2020 (C. Bessiere, ed.), pp. 4877–4884, ijcai.org, 2020.
[26] N. Shervashidze, P. Schweitzer, E. J. van Leeuwen, K. Mehlhorn,
and K. M. Borgwardt, “Weisfeiler-lehman graph kernels,” Journal of
Machine Learning Research , vol. 12, no. 77, pp. 2539–2561, 2011.
[27] C. Lecoutre and N. Szczepanski, “PYCSP3: modeling combinatorial
constrained problems in python,” CoRR , vol. abs/2009.00326, 2020.
[28] A. E. Roth, T. S ¨onmez, and M. U. ¨Unver, “Efﬁcient kidney exchange:
Coincidence of wants in markets with compatibility-based preferences,”
American Economic Review , vol. 97, pp. 828–851, June 2007.
[29] M. Constantino, X. Klimentova, A. Viana, and A. Rais, “New insights
on integer-programming models for the kidney exchange problem,”
European Journal of Operational Research , vol. 231, no. 1, pp. 57–68,
2013.
[30] G. Corso, L. Cavalleri, D. Beaini, P. Li `o, and P. Veli ˇckovi ´c, “Principal
neighbourhood aggregation for graph nets,” 2020.
[31] S. Brody, U. Alon, and E. Yahav, “How attentive are graph attention
networks?,” 2021.
[32] S. L. Saidman, A. E. Roth, T. S ¨onmez, M. U. ¨Unver, and F. L.
Delmonico, “Increasing the opportunity of live kidney donation by
matching for two- and three-way exchanges,” Transplantation , vol. 81,
no. 5, 2006.
[33] M. Simonovsky and N. Komodakis, “Graphvae: Towards generation
of small graphs using variational autoencoders,” in Artiﬁcial Neural
Networks and Machine Learning - ICANN 2018 , vol. 11139 of Lecture
Notes in Computer Science , pp. 412–422, Springer, 2018.
[34] N. De Cao and T. Kipf, “Molgan: An implicit generative model for
small molecular graphs,” arXiv preprint arXiv:1805.11973 , 2018.