TQ-Net: Mixed Contrastive Representation Learning For Heterogeneous Test
Questions
He Zhu1;3, Xihua Li2, Xuemin Zhao2, Yunbo Cao2, Shan Yu1;3
1Brainnetome Center, National Laboratory of Pattern Recognition (NLPR), CASIA
2Tencent Inc. China
3School of Future Technology, University of Chinese Academy of Sciences (UCAS)
he.zhu@nlpr.ia.ac.cn, lixihua9@126.com, xueminzhao@tencent.com
yunbocao@tencent.com, shan.yu@nlpr.ia.ac.cn
Abstract
Recently, more and more people study online for the conve-
nience of access to massive learning materials (e.g. test ques-
tions/notes), thus accurately understanding learning materi-
als became a crucial issue, which is essential for many ed-
ucational applications. Previous studies focus on using lan-
guage models to represent the question data. However, test
questions (TQ) are usually heterogeneous and multi-modal,
e.g., some of them may only contain text, while others half
contain images with information beyond their literal descrip-
tion. In this context, both supervised and unsupervised meth-
ods are difï¬cult to learn a fused representation of questions.
Meanwhile, this problem cannot be solved by conventional
methods such as image caption, as the images may con-
tain information complementary rather than duplicate to the
text. In this paper, we ï¬rst improve previous text-only repre-
sentation with a two-stage unsupervised instance level con-
trastive based pre-training method ( MCL :Mixture Unsuper-
vised Contrastive Learning). Then, TQ-Net was proposed
to fuse the content of images to the representation of het-
erogeneous data. Finally, supervised contrastive learning was
conducted on relevance prediction-related downstream tasks,
which help the model to effectively learn the representation of
questions. We conducted extensive experiments on question-
based tasks on large-scale, real-world datasets, which demon-
strated the effectiveness of TQ-Net and improve the precision
of downstream applications (e.g. similar questions "2.02%
and knowledge point prediction "7.20%). Our code will be
available, and we will open-source a subset of our data to
promote the development of relative studies.
Introduction
Nowadays, more and more users prefer to learn courses or
take tests from the online learning system. Owning a great
number of question materials, these platforms are expected
to design an automatic model to provide personalized test
and practice lists to effectively improve the weak knowledge
of different users, such as recommending similar or related
knowledge-point exercises to users, which they are poorly
learned.
The key ability of this automatic model would be distin-
guishing various question data, or we can say, extracting the
better representation of question data. In previous works,
Copyright Â© 2023, Association for the Advancement of Artiï¬cial
Intelligence (www.aaai.org). All rights reserved.
In square ABCD, ğ·ğ¸=ğ¸ğ¶, and ğµğ¸meets diagonal ğ´ğ¶at F.  ğ‘†!"#$=45. What is ğ‘†!%&$? ABCDEF(A)100     (B)108     (C)120     (D)135     (E)144What is correct front view in the figure below?FrontABCDWhich point lies on the parabola y = 2ğ‘¥'+2ğ‘¥âˆ’12âˆ’8cos	ğœ‹?(A) (2, 0)     (B)(2, 10)     (C)(2, 20)     (D)(2, 8)Figure 1: Examples of heterogeneous questions. The hetero-
geneous question samples have various data structure, con-
tains formula, text and images. Moreover, the information
of text, formula and image may have different importance in
various questions.
language models (LM) (Yu et al. 2014; Huang et al. 2017;
Pardos and Dadu 2017; Anuyah, Azpiazu, and Pera 2019)
were the main solutions: models trained with the supervised
questions text data. However, language models cannot sat-
isfy all subjects, like geometry questions for example. In
other words, images contain information that are also im-
portant to lots of situations.
But the representation of questions cannot be solved by
conventional methods such as image caption, as the im-
ages may contain information complementary to the text.
It should explore a balance between language model (LM)
and multi-modal methods. How to design an effective rep-
resentation learning method of the heterogeneous questions
became a hard and valuable problem.
Moreover, supervised methods pay expensively for the
detailed labels marked by the different professionals. Re-
cent unsupervised pre-training studies of LM or multi-modal
(Devlin et al. 2018; Sun et al. 2019b; Li et al. 2020; Su et al.
2019; Xie et al. 2018; Sun et al. 2019a; Tan and Bansal 2019;
Zhou et al. 2020) show that self-supervised-learning (SSL)arXiv:2303.08039v1  [cs.CL]  9 Mar 2023What is correct [MASK] view in the figure below?What is correct frontview in the figure below?
Step 1. Mask Language Model Trainğ’•ğŸ~ğ‘»ğ’•ğŸ~ğ‘»What is correct [MASK] viewfront [MASK] in the figure below?TQ-Net  QueryEncoder
Step 2. Contrastive Learningğ’‡ğ’’ğ’‡ğ’Œ%{ğ’‡ğ’ŒğŸ&,ğ’‡ğ’ŒğŸ&,...ğ’‡ğ’Œğ’&}+attractrepulse
Step 3. Supervised Contrastive Learningğ’‡ğ’’ğ’‡ğ’Œ%{ğ’‡ğ’ŒğŸ&,ğ’‡ğ’ŒğŸ&,...ğ’‡ğ’Œğ’&}+attractrepulse
What is correct left view in the figure below?What is correct front view in the figure below?FrontABDC
FrontABFrontCDWhat is correct front view in the figure below?FrontABDCLeftABCMixed Unsupervised Contrastive LearningTQ-Net KeyEncoderSupervised Contrastive Leaning
BERTTQ-Net (Text Stream)BERTResNet50Multi-head AttentionImagesTextğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¡ğ‘’(ğ‘£',ğ‘£(,â€¦ğ‘£),ğ‘¡',ğ‘¡(,â€¦ğ‘¡*)
Q_seedQ_similarTQ-NetFigure 2: Training pipeline and TQ-Net model architecture. TQ-Net model simply consist of ResNet(visual encoder), BERT(text
encoder) and multi-head attention(fusion module), visual tokens features come from multiple images will be concatenated with
text token features and fusd by the attention module, at last output average pooling result as the representation of the input. Our
framework has three steps: Step 1: The text encoder (BERT) will be self-supervised pre-trained by the mask language model
task; Step 2: The multi-modal encoder will be pre-trained by CL with two separate data augmentation operators sampled from
the same family of augmentations ( t1Tandt2T) and applied to each question instance to obtain two correlated views.
Step 3: The multi-modal encoder will be ï¬ne-tuned by choosing the positive and negative pairs based on the human-labelled
similar questions to do the supervised contrastive learning.
was potential to be close to or even better than the repre-
sentation learning capability of supervised methods without
expensive cost.
From view of that, we design the TQ-Net to deal with the
heterogeneous input and propose a contrastive learning (CL)
pre-training(Chen et al. 2020a; He et al. 2020a; Grill et al.
2020; Chen and He 2020; Wu et al. 2020; Radford et al.
2021a; Zbontar et al. 2021) method to solve the dilemma of
representation learning.
On the basis of all the above, we proposed two strategies
to achieve a better unsupervised pre-training for questions:
(1) Separately pre-training the image/text stream encoders
and ï¬ne-tuning together. (2)Uniformly pre-training the two-
stream encoder. Experiments show that the latter was more
beneï¬cial to the representation learning of questions.
Our contribution could be concluded as follows:
â€¢ We proposed the MCL (Mixture Unsupervised
Contrastive Learning) mechanism to improve the
text representation of questions. MCL is a two-stage
unsupervised pre-training method which means that the
model is ï¬rst pre-trained in MLM (Masked Language
Model) and then pre-trained in instance level CL (Con-
trastive Learning) framework (vs. modal level). The
performance of MCL on both frozen and ï¬ne-tuning
experiments are much better than the masked LM
method.
â€¢ We proposed a new model named TQ-Net and instancelevel CL conducted to fuse the content of images to the
representation of heterogeneous data. Compared to pre-
vious question-based model (text only ones and multi-
modal ones), our proposal improves precision of down-
stream applications signiï¬cantly.
â€¢ We proposed a supervised contrastive learning frame-
work, named SCL , which help the model to effectively
learn the representation of questions on relevance-related
downstream task than traditional methods.
Related Work
Question Representation Learning (Yu et al. 2014)
deï¬ned the representation and similarity measure of text
questions based on a word and knowledge tree strategy.
(Pardos and Dadu 2017) used the skip-grams and bag-of-
words to formulate the representation. (Liu et al. 2018)
designed the MERL: a model consists of CNN and LSTM
module, and fused the embeddings of various modals,
learning the representation by the supervised training. (Yin
et al. 2019) proposed the QuesNet: a Holed-Language-
Model to extract the representation of questions data. This
method needed three steps: (1) Embedding pre-training by
auto-encoder for recovering the modal data to learn the
embedding of different modal inputs. (2) Model pre-training
for ï¬lling up every word with both the left and right side
context of it. (3) Fine-tuning the whole network on the
downstream tasks. QuesNet was complex to deploy andthe performance would be limited because the embedding
extractor and the model were separately trained.
Unsupervised Pre-training of Multi-modal Content
VideoBERT (Sun et al. 2019b) built the dictionary for
videos, and proposed using visual and text tokens as the
input of the model. This method borrowed the idea of BERT
(Devlin et al. 2018) to do the pre-training task: (1) Masked
token prediction (2) Text-Video matching prediction. (Li
et al. 2020; Su et al. 2019; Zhou et al. 2020) applied images
as the visual input, these works introduced a detector or
improved the pre-training tasks to capture the semantic
information for better fusion. Moreover, CLIP (Radford
et al. 2021b) aims to maximize the similarity between
different modal inputs, but it is hard to take advantage of
the difference in various modals to build a cooperative
representation learning of heterogeneous data.
Contrastive learning. The CL compares two augmented
views of the same input to capture semantic-invariant fea-
tures, such as object-related characteristics. Some studies
(Chen et al. 2020b; He et al. 2020b; Chen et al. 2020c,d;
Chen, Xie, and He 2021) attract the positive (similar) pairs
and repulse negative (different) pairs to learn the repre-
sentations. Other studies (Chen and He 2020; Caron et al.
2021) also provide a self-distillation framework, which only
matches positive samples to learn the representations.
Method
Network Architecture
We ï¬rst introduce the novel two-forward model TQ-Net as
shown in Figure 2 and 4 to effectively extract the represen-
tation of the heterogeneous test data. Our proposal uses the
text encoder and image encoder to respectively process the
words and images input.
Note that, here we use the pre-norm structure transformer
to build self attention module as shown in Figure 4 (b). And
the shortcut was introduced to the forward pass, which used
to fuse the original modal information, it can also be inter-
preted as fusion of joint (low-level) and coordinated (high-
level) information without lose of details.
Mixed Unsupervised Contrastive Learning
Our unsupervised pretraining was shown in Figure 2 (step
1) as the ï¬rst pretext task to push the text encoder to a suit-
able hidden space rather than directly do the multi-modal
contrastive learning, as latter would easily cause the pattern
collapse.
After that, we can apply the multi-modal contrastive
learning as shown in Figure 2 (step 2): The Query/Key en-
coder have same architecture and initialization. Each net-
work consist of three modules: visual encoder (ResNet-50),
text encoder (BERT) and fusion module (multi-head at-
tention). In this paper, we use the random masked words
and clipped windows as the augmentation of questions as
shown in Figure 2 (b). The augment technologies of im-
ages are same as the (Chen et al. 2020d). Each input af-
ter augmentation consist of visual list [fI1g;fI2g; ::fIbg]and text list(words and formula) [T1; T2; :::Tb]. We will
use the shufï¬‚e batch-normalization (BN) (He et al. 2020a),
which collects the inputs from different GPUs to do the
BN to avoid the device information leakage, to get key
network features k+
1; k+
2; ::; k+
b. At the same time we have
theq1; q2; ::; qbpositive features from the query network
andfk 
1g;fk 
2g; ::;fk 
bgnegative features set from nega-
tive queues. Query network will be updated by the training
loss (Eq.1).
lq;k+;fk g= logeqk+=
eqk+=+P
fk geqk =(1)
Key network parameters pkeywould be driven by a mo-
mentum update with the query encoder parameters pquery
follows Eq.2.
pkey=mpkey+ (1 m)pquery (2)
where m2[0;1)is a momentum coefï¬cient (here we use
m=0.999).
Note that, here we use the two-stage unsupervised con-
trastive learning mechanism to further enhance the represen-
tation ability of TQ-Net. In the ï¬rst stage, MLM (Masked
Language Model) was used to pre-train the text encoder
(BERT in our model). In the second stage, SEQ (separately)
andUNI (uniformly) pre-training were conducted. For SEQ,
text modal and image modal were separately pre-trained in
an unsupervised contrastive learning manner. For UNI, text
modal and image modal were pre-trained in an unsupervised
contrastive learning manner to view the question as an in-
stance where augmentation applied to both text and image
simultaneously (instance-level contrastive learning). Also, a
comprehensive comparison of SEQ and UNI methods was
shown in experiments.
Supervised Contrastive Learning
Though we have proposed an unsupervised learning method
for heterogeneous question data, because the data struc-
ture could be greatly divergent, our model would sometimes
over-ï¬tting the noise. So based on our unsupervised pre-
training method, we propose a supervised contrastive learn-
ing (SCL) ï¬ne-tuning method to build the knowledge anchor
for models as shown in Figure 2 (step 3). The positive pairs
in SCL come from human-labelled similar questions rather
than data augmentation.
In educational applications, the similar question classiï¬-
cation is an important task to evaluate the performance of ed-
ucational model. And here we could adapt this binary clas-
siï¬cation dataset to do our supervised contrastive learning.
lscl= logeqseedqsim=
eqseedqsim=+P
qo2fqothersgeqqo=(3)
Let the seed questionâ€™s feature be qseed, and its sim-
ilar questionâ€™s feature is qsim, others questionsâ€™ features
areqothers . The supervised contrastive learning is based
on Eq.3, here the seed/similar question positive pair comes
from human-labelled similar questions, and we use otherf1iÂ·f1jf2iÂ·f2jÂ·Â·Â·Â·Â·fniÂ·fnj01Â·Â·Â·1PredictionLabelsf1q,f2q,Â·Â·Â·Â·Â·,fnq
f1k,f2k,Â·Â·Â·Â·Â·,fnkf1qÂ·f1k , Â·Â·Â·,f1qÂ·fnm f2qÂ·f1k, Â·Â·Â·,f2qÂ·fnm Â·Â·Â·Â·Â·fnqÂ·f1k,, Â·Â·Â·,fnqÂ·fnm  1        1 â€¦               1PredictionLabelsf1Â·fa , Â·Â·Â·,f1Â·fz f2Â·fa, Â·Â·Â·,f2Â·fz Â·Â·Â·Â·Â·fnÂ·fa,, Â·Â·Â·,fnÂ·fz0, ..â€¦., 11, â€¦..., 0â€¦..1, â€¦..., 1PredictionLabels
(a) Pair Classification(b) Contrastive Learning(c) Our Supervised Contrastive Learningf1k,f2k,Â·Â·Â·Â·Â·,fnkf1q,f2q,Â·Â·Â·Â·Â·,fnqf1,f2,Â·Â·Â·Â·Â·,fn
fa,fb,Â·Â·Â·Â·Â·,fzFigure 3: Different training paradigms based on the similar question dataset. Pair classiï¬cation and contrastive learning needs
pairwise inputting(query and key should be similar/positive), but our supervised contrastive learning could receive arbitrary
instance inputting. Even a question do not have similar question, it can be input as dissimilar with other questions to learn the
discrimination.
questions in the dataset as the negative pairs of the seed.
Note that, the label matrix no longer be diagonal, the label
matrix was calculated by the ground truth: if questions pair
is similar then label is 1, else the label is 0. Some questions
could have multiple similar questions, so itâ€™s necessary to
check the ground truth label matrix rather than applying aug-
mentation as unsupervised contrastive learning used shown
in Figure 3. Another situation is even a question do not have
similar questions, it can be input as dissimilar with other
questions to learn the discrimination.
Experiments
The experiments of this paper mainly consist of two parts:
(1) Similar test question, (2) Knowledge Point Prediction
(Classiï¬cation). In ï¬rst part of experiments, we mainly ex-
plored the effective framework of TQ-Net to learning an
effective representation of questions on both unsupervised
and supervised ways. In second part, we evaluated TQ-Net
compared to text-only methods and previous SOTA hetero-
geneous methods to illustrate the effectiveness of our pro-
posal.
Dataset
The pre-training dataset we used came from an online ed-
ucation system and was collected from middle school math
tests and exams as shown in Table 1. About 30% of ques-
tions contain images. The maximum number of images for
one question is 10. We only use the text and images of ques-
tions and options without label to do the unsupervised rep-
resentation learning.
Implementation details
All our experiments use 8 1080-Ti GPUs. In the pre-
training experiment, we follow the same settings of hyper-
parameters with ofï¬cial MoCo v2 code1. In the similar
1https://github.com/facebookresearch/mocoQuestions
Choice Blank Filling Calculation
86753 42892 49225
Questions with images Images
55635 91456
Table 1: Statistics of the questions dataset.
test question classiï¬cation task, we use SGD optimizer with
lr=3e-4, momentum=0.9, and weight decay=5e-4. The train-
ing batch size was set to 32. In the knowledge classiï¬cation
task, we use the optimizer setting the same as similar test
question, the training batch size was set to 64. Our augmen-
tation for images are same as the visual contrastive learning
methods (He et al. 2020a; Chen et al. 2020c), such as resize,
crop, colour jitter, ï¬‚ip and blur.
Similar test question
The similar test question is a relevance prediction task,
which needs to predict whether two given questions are sim-
ilar or not. In this task, training dataset contains 27637 ques-
tion pairs, and 4526 pairs for evaluation. The labels were
marked by multiple professionals to indicate whether the
two questions in a pair are similar or not based on knowledge
points. Speciï¬cally, each question in the train/test dataset
will have at least 5 similar questions, and there is no overlap
between the train and test datasets. In default, two random
questions are not similar with a high probability. In our ex-
perimental setting, the question type is less considered, we
hope that the model captures the knowledge/problem solv-
ing method/background description from the discrimination
of similarity.
We use precision at top-k as the metrics of performance.
Speciï¬cally, we ï¬rst build the similar mapping for each
question according to the ground truth label, and each ques-tion has at least 5 similar questions on our designed test-
ing datasets. Then based on the prediction result, we build
the predicted mapping. For each question, we would pick
the top-5 largest probability candidate from all the positive
question predictions based on the cosine similarity of em-
beddings. Compared with the ground truth mapping, we cal-
culated the precision at top-5 to illustrate the performance,
as shown in Eq.4. In this paper, we choose the P@5 (re-
sponding to that each question will have at least 5 simi-
lar questions) as the main evaluation metrics of similar test
question for the balance of contingency and generalization.
P@k=1
NNX
i=0card(Si gtk\Si pred k)
card(Si gtk)(4)
Where, Si gtk/Si pred kmeans the set of i-th sample top-
k ground truth or predicted question index, card means the
number of elements in the set.
Similar test question is an essential question-based tasks
for many educational applications. The performance of this
task completely depends on the capability of the model to
extract the representation of various questions. Itâ€™s a dif-
ï¬cult problem because of the heterogeneous, multi-modal
data. Most solutions were only based on the text stream, and
few studies as the pioneer to explore the heterogeneous solu-
tions e.g. QuesNet (Yin et al. 2019). In this section, we ï¬rst
explored to improve the representation learning of only text
stream questions method, then introduced the image content
to our framework.
Method Pre. Fin. P@5
BERTbase - Pair 0.8164
BERT O MLM Pair 0.8319
BERT E MLM Pair 0.8329
BERT E CL Pair 0.8406
BERT E MCL Pair 0.8454
BERTbase - SCL 0.8242
BERT O MLM SCL 0.8348
BERT E MLM SCL 0.8386
BERT E CL SCL 0.8396
BERT E MCL SCL 0.8473
Table 2: Experiments of only text stream with different set-
tings. The subscript â€baseâ€/â€Oâ€/â€Eâ€ respectively represent
BERT pretrained settings are no pretrain/open-source/our
educational dataset pretrained. â€Pre.â€ means pre-trained
method. MLM means the mask language model pretrain, CL
means contrastive learning, and MCL means our proposed
mixed contrastive learning. â€Fin.â€ means ï¬ne-tune method.
Pair/SCL method responding to Figure 3. The best P@5 re-
sults are in bold .
Text Only Input We started from the original text-only
pair training method to illustrate that the contrastive learn-
ing method was more beneï¬cial to learning the representa-
tion. We only use the text stream of our TQ-Net, and ig-
nore the images stream. We respectively used the masked
Image EncoderrQText Encoder(BERT)Text TokenizerInput Q ImagesTextn imagesm words(a) Joint representation
Input Q ImagesTextn imagesm wordsImage EncoderText Encodertext features
image features(n, dim)(m, dim)â€¦â€¦(m+n, dim)+positionâ€¦â€¦Self AttentionPoolingrQMulti-head Attention
âœ–N=3FFNLayer NormLayer Norm
(b) Coordinated representation
Figure 4: Different fusion methods of heterogeneous ques-
tions. Note that QuesNet (a) use the joint representation fu-
sion, and TQ-Net (b) use coordinated representation fusion.
language model (MLM), contrastive learning (CL) to do the
pre-training, and also our proposed two-stage pre-training
method MCL, which means the model ï¬rst pre-training in
MLM and then pre-training in CL framework (two-stage
pre-training). To show the advantage of pre-training, we pro-
vided the performance of random initial and the ofï¬cial pre-
training BERT by (Wolf et al. 2020).
According to the result in Table 2, compared to the MLM
method, CL and MCL showed that contrastive learning was
more helpful for representation learning. Note that, the pre-
training task of CL was also a process to distinguish sim-
ilarities. Thus, we want to ï¬gure out whether this method
would help supervised representation learning. We design
the SCL training method. Figure 3 shows the difference be-
tween SCL and traditional pair training on downstream sim-
ilar question classiï¬cation task. As the result shown in Table
2, itâ€™s reasonable to believe that SCL beneï¬ts the supervised
representation learning and downstream tasks compared to
the previous method.
Multi-modal Input Next, we would introduce the image
content to improve the question representation learning. The
main strategy of previous multi-modal fusion methods like
(Sun et al. 2019b; Li et al. 2020; Radford et al. 2021a; Su
et al. 2019; Li et al. 2020) could be concluded into two types:
joint or coordinated representation fusion as shown in Fig-
ure 4. We ï¬rst tested which one of these methods was more
suitable for the question data representation learning.
The result as Table 3 shows that coordinated fusion may
be suitable for question data. Thus, we use TQ-Net as a co-
ordinated fusion method to process these data. We suppose
these supplementary-each-other multi-modal data is hard to
do the representation learning compared to image caption
tasks. The reason may come from that the text is not the la-Fusion Fine-tune P@5
Joint Pair 0.7969
Coordinated Pair 0.8329
Table 3: The performance of different multi-modal fusion
strategies. The best P@5 results are in bold .
Pre-train Fine-tune P@5
CLIP Pair 0.7995
QuesNet Pair 0.8100
TQ-NetPair 0.8329
SCL 0.8531
Table 4: Experiments of multi-modal input with different
settings. The best P@5 results are in bold .
bel or caption of the images, whatâ€™s more, the relationship
between the image and text modal tends to be more inde-
pendent.
In view of that, we proposed two strategies to achieve
a better unsupervised pre-training for heterogeneous ques-
tion data to improve the downstream performance. (1) Sep-
arately pre-training the image/text encoders of TQ-Net. (2)
Uniformly pre-training the TQ-Net as an instance encoder.
To verify our argument, we test the contrastive way like
CLIP, and the result as shown in Table 4 is much worse
(lower 6%) than our instance contrastive learning. Our pro-
posal greatly improves the capability of processing multi-
modal test question data compared to the previous hetero-
geneous solution. Moreover, both SEP and UNI have better
P@5 performance in pair and SCL mechanisms than text-
only methods (Table 2). UNI was more beneï¬cial to the un-
supervised pre-training process than the frozen experiment.
In the ablation study section, we would ï¬gure out where the
performance improvement came from.
Knowledge Point Prediction
We also invite the knowledge point prediction task to evalu-
ate our model knowledge anchor. We have 118919 questions
on the training dataset, and 29565 on the testing dataset.
Each question has knowledge point labels labelled by pro-
fessional teachers. The total number of knowledge points is
391. So the knowledge point prediction task could be con-
sidered as a classiï¬cation task, and we pick the F1 score as
the metric of evaluation. We froze all the parameters of TQ-
Net and introduce a linear classiï¬er to do classiï¬cation. For
comparing, we provided the QuesNet and different initial
experiments of BERT, both without freezing parameters.
According to the results Table 5, our TQ-Net shown great
power to solve the unsupervised representation learning of
heterogeneous test questions data.
Ablation Study
To ï¬gure out where the improvement of the multi-modal per-
formance came from, we respectively evaluated the model
on different pair structure datasets in similar test questionKP (class num =391)
Method F1
QuesNet 0.557
BERTbase 0.382
BERT
O 0.530
BERT
E 0.552
TQ Net
MCL 0.602
Table 5: Knowledge classiï¬cation experiments. means the
encoder parameters in experiments were frozen during the
ï¬ne-tuning process. The best F1 results are in bold .
Dataset Text-Text Text-Image Image-Image
Method P@5
Text-only 0.8688 0.7149 0.7882
Multi-modal 0.8866 0.7194 0.7960
Table 6: Experiments on datasets consist of different pair
structures. The best P@5 results are in bold .
experiments: (1) Text-Text: two questions of pair do not con-
tain images. (2) Text-Image: one question have images and
the other one is text-only. (3) Image-Image: two questions
of pairs both have images.
According to Table 6, we found that the text input domi-
nates the information of questions. Even without visual con-
tent, the model has the capability to represent the questions.
However, the most improvement of TQ-Net performance
came from the text-only question pairs. It shows that our
multi-modal contrastive learning help to learn better repre-
sentations of questions. Thus, we proposed that the visual
content was necessary, which greatly beneï¬ts representation
learning.
Conclusion
In this paper, we improved the pre-training method of the
previous text-only question representation with a two-stage
contrastive learning mechanism. We also explored the fusion
method of the image content, where a model named TQ-
Net and instance-level contrastive learning was proposed
to process the heterogeneous data. Moreover, we provided
unsupervised and supervised training methods, which help
the model to effectively learn the multi-modal representa-
tion of question data. We conducted extensive experiments
to demonstrate the effectiveness of TQ-Net and improve the
precision of downstream applications (especially relevance-
related tasks). We will continue to explore possible improve-
ments for the following in the future: (1) method for better
fusion of the image content. (2) more effective contrastive
learning framework for multi-modal or heterogeneous data.
We hope this work builds a solid basis for future multi-
modal and heterogeneous representation research and helps
boost more educational applications.References
Anuyah, O.; Azpiazu, I. M.; and Pera, M. S. 2019. Using
structured knowledge and traditional word embeddings to
generate concept representations in the educational domain.
InCompanion Proceedings of The 2019 World Wide Web
Conference , 274â€“282.
Caron, M.; Touvron, H.; Misra, I.; J Â´egou, H.; Mairal, J.; Bo-
janowski, P.; and Joulin, A. 2021. Emerging properties in
self-supervised vision transformers. In ICCV , 9650â€“9660.
Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020a.
A simple framework for contrastive learning of visual repre-
sentations. In International conference on machine learning ,
1597â€“1607. PMLR.
Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. E.
2020b. A Simple Framework for Contrastive Learning of
Visual Representations. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event , volume 119 of Proceedings of Ma-
chine Learning Research , 1597â€“1607.
Chen, T.; Kornblith, S.; Swersky, K.; Norouzi, M.; and Hin-
ton, G. 2020c. Big self-supervised models are strong semi-
supervised learners. arXiv preprint arXiv:2006.10029 .
Chen, X.; Fan, H.; Girshick, R. B.; and He, K. 2020d.
Improved Baselines with Momentum Contrastive Learning.
CoRR , abs/2003.04297.
Chen, X.; and He, K. 2020. Exploring Simple Siamese Rep-
resentation Learning. arXiv preprint arXiv:2011.10566 .
Chen, X.; Xie, S.; and He, K. 2021. An Empirical Study
of Training Self-Supervised Vision Transformers. In 2021
IEEE/CVF International Conference on Computer Vision,
ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 ,
9620â€“9629.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 .
Grill, J.-B.; Strub, F.; Altch Â´e, F.; Tallec, C.; Richemond,
P. H.; Buchatskaya, E.; Doersch, C.; Pires, B. A.; Guo, Z. D.;
Azar, M. G.; et al. 2020. Bootstrap your own latent: A
new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733 .
He, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020a.
Momentum contrast for unsupervised visual representation
learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 9729â€“9738.
He, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. B. 2020b.
Momentum Contrast for Unsupervised Visual Representa-
tion Learning. In CVPR 2020, Seattle, WA, USA, June 13-19,
2020 , 9726â€“9735.
Huang, Z.; Liu, Q.; Chen, E.; Zhao, H.; Gao, M.; Wei, S.;
Su, Y .; and Hu, G. 2017. Question Difï¬culty Prediction for
READING Problems in Standard Tests. In Proceedings of
the AAAI Conference on Artiï¬cial Intelligence .
Li, G.; Duan, N.; Fang, Y .; Gong, M.; and Jiang, D. 2020.
Unicoder-vl: A universal encoder for vision and language
by cross-modal pre-training. In Proceedings of the AAAI
Conference on Artiï¬cial Intelligence , 11336â€“11344.Liu, Q.; Huang, Z.; Huang, Z.; Liu, C.; Chen, E.; Su, Y .; and
Hu, G. 2018. Finding similar exercises in online education
systems. In Proceedings of the 24th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data Mining ,
1821â€“1830.
Pardos, Z. A.; and Dadu, A. 2017. Imputing KCs with rep-
resentations of problem content and context. In Proceedings
of the 25th Conference on User Modeling, Adaptation and
Personalization , 148â€“155.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021a. Learning transferable visual models from natu-
ral language supervision. arXiv preprint arXiv:2103.00020 .
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021b. Learning transferable visual models from nat-
ural language supervision. In International Conference on
Machine Learning , 8748â€“8763. PMLR.
Su, W.; Zhu, X.; Cao, Y .; Li, B.; Lu, L.; Wei, F.; and Dai, J.
2019. Vl-bert: Pre-training of generic visual-linguistic rep-
resentations. arXiv preprint arXiv:1908.08530 .
Sun, C.; Baradel, F.; Murphy, K.; and Schmid, C. 2019a.
Learning video representations using contrastive bidirec-
tional transformer. arXiv preprint arXiv:1906.05743 .
Sun, C.; Myers, A.; V ondrick, C.; Murphy, K.; and Schmid,
C. 2019b. Videobert: A joint model for video and language
representation learning. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , 7464â€“7473.
Tan, H.; and Bansal, M. 2019. Lxmert: Learning cross-
modality encoder representations from transformers. arXiv
preprint arXiv:1908.07490 .
Wolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-
son, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu, J.;
Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.; and
Rush, A. M. 2020. Transformers: State-of-the-Art Natural
Language Processing. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Process-
ing: System Demonstrations , 38â€“45. Online: Association for
Computational Linguistics.
Wu, Z.; Wang, S.; Gu, J.; Khabsa, M.; Sun, F.; and Ma, H.
2020. CLEAR: Contrastive Learning for Sentence Repre-
sentation. arXiv preprint arXiv:2012.15466 .
Xie, S.; Sun, C.; Huang, J.; Tu, Z.; and Murphy, K. 2018.
Rethinking spatiotemporal feature learning: Speed-accuracy
trade-offs in video classiï¬cation. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV) , 305â€“321.
Yin, Y .; Liu, Q.; Huang, Z.; Chen, E.; Tong, W.; Wang, S.;
and Su, Y . 2019. Quesnet: A uniï¬ed representation for het-
erogeneous test questions. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discov-
ery & Data Mining , 1328â€“1336.
Yu, J.; Li, D.; Hou, J.; Liu, Y .; and Yang, Z. 2014. Similarity
measure of test questions based on ontology and vsm. The
Open Automation and Control Systems Journal , 6(1).Zbontar, J.; Jing, L.; Misra, I.; LeCun, Y .; and Deny, S. 2021.
Barlow Twins: Self-Supervised Learning via Redundancy
Reduction. arXiv preprint arXiv:2103.03230 .
Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; and Gao,
J. 2020. Uniï¬ed vision-language pre-training for image cap-
tioning and vqa. In Proceedings of the AAAI Conference on
Artiï¬cial Intelligence , 13041â€“13049.