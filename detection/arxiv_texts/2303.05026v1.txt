SSL2: Self-Supervised Learning meets Semi-Supervised Learning:
Multiple Sclerosis Segmentation in 7T-MRI from large-scale 3T-MRI
Jiacheng Wanga, Hao Lib, Han Liua, Dewei Hub, Daiwei Lua, Keejin Yoonc, Kelsey Barterc,
Francesca Bagnatoc, and Ipek Oguza
aDept. of Computer Science, Vanderbilt University, Nashville, TN, US
bDept. of Electrical and Computer Engineering, Vanderbilt University, Nashville, TN, US
cDept. of Neurology, Vanderbilt University Medical Center, Nashville, TN, US
ABSTRACT
Automated segmentation of multiple sclerosis (MS) lesions from MRI scans is important to quantify disease progression.
In recent years, convolutional neural networks (CNNs) have shown top performance for this task when a large amount of
labeled data is available. However, the accuracy of CNNs suffers when dealing with few and/or sparsely labeled datasets. A
potential solution is to leverage the information available in large public datasets in conjunction with a target dataset which
only has limited labeled data. In this paper, we propose a training framework, SSL2(self-supervised-semi-supervised),
for multi-modality MS lesion segmentation with limited supervision. We adopt self-supervised learning to leverage the
knowledge from large public 3T datasets to tackle the limitations of a small 7T target dataset. To leverage the information
from unlabeled 7T data, we also evaluate state-of-the-art semi-supervised methods for other limited annotation settings,
such as small labeled training size and sparse annotations. We use the shifted-window (Swin) transformer1as our backbone
network. The effectiveness of self-supervised and semi-supervised training strategies is evaluated in our in-house 7T MRI
dataset. The results indicate that each strategy improves lesion segmentation for both limited training data size and for
sparse labeling scenarios. The combined overall framework further improves the performance substantially compared to
either of its components alone. Our proposed framework thus provides a promising solution for future data/label-hungry
7T MS studies.
Keywords: Self-supervised, Semi-supervised, Multiple Sclerosis, 7T MRI, Limited Supervision, Transformers
1. INTRODUCTION
Multiple Sclerosis (MS) is a prevalent inﬂammatory disease of the central nervous system that poses signiﬁcant diagnostic
and monitoring challenges. A commonly utilized tool for addressing these challenges is magnetic resonance imaging
(MRI), which enables the visualization and quantiﬁcation of focal MS lesions. Segmentation of these lesions is a crucial
step for clinical evaluation. However, manual segmentation is time-consuming and labor-intensive, mainly due to large
variations in location, shape, and intensity among lesions.
7 Tesla (7T) MRI scans provide more detailed anatomical data with higher spatial resolution and allow more precise
lesion quantiﬁcation. While many fully-supervised lesion segmentation algorithms2–6exist, these are often impractical for
use with 7T data, given the limited availability of publicly available annotated MS lesion datasets at 7T and the high cost
of annotating in-house 7T MRI datasets.
In order to overcome the limitations posed by the scarcity of annotated data and the labor-intensive nature of man-
ual annotation, recent research has focused on exploring self-supervised and semi-supervised learning techniques. Self-
supervised learning7aims to extract robust high-dimensional features directly from the MRI data using augmentations and
proxy tasks. Augmentations involve transforming the data and using these transformations as supervision to learn features.
Proxy tasks, such as predicting the rotation angle of an image, can then be used to train the model to learn useful features.
On the other hand, semi-supervised learning leverages the abundance of unlabeled data to improve model performance.8
This is achieved through two main approaches: consistency learning9and self-training.10Consistency learning maximizes
the stability of predictions of an unlabeled image and its noise-perturbed counterparts. Self-training generates pseudo labels
from unlabeled data and retrains the model using these weak supervision signals. By utilizing both labeled and unlabeled
data, semi-supervised learning offers a promising solution to the challenge of limited annotated data.
In recent years, researchers have proposed the use of transformers in the medical imaging domain to capture long-term
dependencies.11, 12To further improve the efﬁciency of these approaches, the Shifted windows (Swin)-Transformer1, 13
utilizes a non-overlapping shifted window schema to signiﬁcantly reduce the computational complexity for several visionarXiv:2303.05026v1  [cs.CV]  9 Mar 2023tasks. The utilization of a Transformer-based architecture has the potential to improve the performance of MS lesion
segmentation tasks by effectively capturing the long-range dependencies present in the data.
In this paper, we propose a novel approach to address the challenges of limited data and time-consuming annotations
in MS lesion segmentation using MRI scans. Our approach utilizes self-supervised and semi-supervised learning methods
to leverage the large number of publicly available 3T MRI scans to obtain a robust pre-trained model, using a Swin-
Transformer as our backbone. We ﬁne-tune this pre-trained model for our 7T in-house MR data on the downstream lesion
segmentation task. Our hypothesis is that our pre-trained model from 3T images can boost the performance of MS lesion
segmentation on a sparsely labeled 7T dataset. We consider two sparse labeling scenarios: a small number of fully labeled
volumes, or volumes with only a small number of labeled slices. The contributions of our proposed self-supervised-semi-
supervised learning (SSL2) framework are:
• We demonstrate a signiﬁcant increase in MS lesion segmentation performance compared to traditional supervised
training (Dice score improvement from 0:6971 to0:8186 ) on the in-house 7T MRI dataset.
• Our proposed model generates robust segmentation results, even when using very few training samples or sparsely
annotated datasets, and outperforms other methods in these limited supervision settings.
• We effectively distill knowledge from large public 3T MRI datasets to facilitate 7T MRI studies, and make the
pre-trained model weights publicly available for further research *.
2. MATERIALS AND METHODS
In this section, we describe our proposed framework for addressing the challenge of limited annotation in 7T MRI scan
dataset segmentation. We begin by introducing the datasets used in our framework in Section 2.1. Next, in Section 2.2, we
provide an overview of the entire framework. We then delve into the key components of our framework in Sections 2.3 and
2.4, where we discuss the semi-supervised segmentation and self-supervised training, respectively. Finally, we describe
our implementation details in Section 2.5.
2.1 Datasets
2.1.1 In-house 7T MRI dataset
We obtained an in-house dataset of 7T MRI scans at the Vanderbilt University Medical Center (VUMC) for 37 MS patients
at various disease stages, all at their ﬁrst visit. Table 1 presents the demographic and clinical information. Each subject’s
data includes MP2RAGE (T1-weighted) and FLAIR scans.
Table 1. Demographic and clinical information for the VUMC 7T MS dataset.
Number of Subjects 37
Female 17 ( 45:9%)
Male 20 ( 54:1%)
Black/African American 3 ( 8:1%)
White/Caucasian 34 ( 91:9%)
RRMS (relapsing-remitting MS) 29 ( 78:4%)
CIS (clinically isolated syndrome) 4 ( 10:8%)
RIS (radiologically isolated syndrome) 2 ( 5:4%)
Others 2 ( 5:4%)
Age (meanSTD) 37:469:07
EDSS (expanded disability status scale, mean STD) 1:421:56
Labeled 7T in-house dataset. Expert manual annotation of MS lesions on T1-weighted images was performed by a
single trained annotator (KY) for a subset of 14 subjects. The automated lesion segmentation on these same 14 subjects
was then performed using our existing model, ModDrop++.6ModDrop++ is based on our previous Tiramisu-based model4
which currently holds the top position on the ISBI 2015 challenge14leaderboard ( https://smart-stats-tools.
org/lesion-challenge ). Discrepancies between the manual and automated lesion masks were reviewed and recon-
ciled by a second trained radiologist (KB) to generate the ﬁnal lesion masks for the 14 subjects.
Unlabeled 7T in-house dataset. The remaining 23 subjects are used as our unlabeled dataset.
*https://github.com/MedICL-VU/SSL-squared-7T3T MRI (Public)Pre-trainingSwin Transformer Encoder
Rotation PredictionInpainting ReconstructionContrastive LearningSelf-supervised Loss
Pre-trained Swin Transformer7T MRI (Target)Fine-tuningLabeledUnlabeled
SegmentationGround Truth (Expert Annotation)Cross Pseudo SupervisionMean TeacherMLP Prediction HeadVAE Reconstruction HeadProjection Head for Contrastive LearningRandom Augmentation (Rotations; Cutouts; Patch Swaps; Histogram Shifts)Figure 1. Overall framework. Top. Self-supervised pre-training. 3 heads are attached to the encoder for the 3 proxy tasks. Bottom.
Semi-supervised ﬁne-tuning. Pre-trained weights (purple) are used to initialize the encoder. Mean-teacher and CPS are compared.
2.1.2 Public unlabeled 3T MRI datasets
We employ several publicly available datasets without their annotations for pre-training, including:
• Longitudinal MS Lesion Segmentation Challenge (ISBI 2015) dataset,2with 21 training and 61 testing scans;
• UMCL (University Medical Center Ljubljana) Multi-rater Consensus dataset,15with 30 scans;
• MICCAI MSSeg 2016 Challenge16dataset, with 15 scans;
• MICCAI Brain Tumor Segmentation (BraTS) 2021 Challenge17dataset with 1251 scans.
For consistency, only T1-weighted and FLAIR images are utilized from each of these datasets. Rigid registration is
performed to align each FLAIR image to its corresponding T1w image; all T1w images are rigidly aligned to a single
subject from our 7T dataset.
2.1.3 Preprocessing
We apply standard preprocessing techniques to all images (3T and 7T), including model-based skull stripping with HD-
BET,18N4 bias correction,19and histogram normalization to the range [0;1]. The resulting images have a resolution of
0:90:90:9mm3and a ﬁeld of view of 256256200voxels.
2.1.4 Data representation
For both the 3T and 7T MRI datasets, we crop the image to the bounding box of the skullstripped brain to reduce image
size. To further address the memory constraints commonly encountered when training 3D networks, we randomly extract
two sub-volumes of 969696voxels from each input image. The T1w and FLAIR images are concatenated as input to
our network.2.2 System Overview
Figure 1 shows the overall workﬂow of our system.
We use the 7T MRI dataset (labeled and unlabeled) in a semi-supervised manner to determine the best semi-supervision
strategy for the MS lesion segmentation task. We compare 6 different semi-supervised learning schemes in a 7-fold
cross-validation setting. This experiment identiﬁes the best and worst performing semi-supervision strategies, which are
incorporated into our SSL2framework.
SSL2begins by using the unlabeled 3T MRI datasets to pre-train a Swin transformer model in a self-supervised manner
using 3 proxy heads that correspond to 3 proxy tasks. For this purpose, we use a modiﬁcation of the original 2D Swin-
transformer block13to be suitable for 3D volumes as our backbone network, similar to Swin-UNETR.13Next, the proxy
heads are discarded and only the pre-trained Swin transformer weights are preserved. The 7T MRI dataset (labeled and
unlabeled) and the previously determined best-performing semi-supervision strategy are incorporated to obtain the ﬁnal
segmentation results. We compare this model to the baseline model which uses the worst-performing semi-supervision
strategy.
2.3 Semi-supervised Segmentation in 7T MRI dataset
To choose the semi-supervision strategy for our framework, we compare the performance of six different semi-supervised
learning schemes in a 7-fold cross-validation setting. These methods include Mean Teacher,9Entropy Minimization,20
Deep Adversarial Networks,21Uncertainty Aware Mean Teacher,22FixMatch,23, 24and Cross Pseudo Supervision.10
Each semi-supervised model is trained with a combination of supervised loss Lsupand unsupervised loss Lunsup such
thatLsemi =Lsup+semiLunsup . The supervised loss is computed using Dice Loss and pixel-wise Cross Entropy (CE)
Loss, while the semi-supervised loss varies among the models. During each training iteration, equal number of labeled and
unlabeled samples are used to compute LsupandLunsup , respectively.
Our experiments utilize the 14 subjects from our in-house 7T labeled dataset. We utilize 12 subjects for training and
hold out 2 subjects for validation in each fold. Additionally, we make use of the 23 subjects in our unlabeled in-house
7T dataset for the semi-supervised models. We use a sliding-window inference method as implemented in MONAI. The
window size is chosen as 969696voxels with overlap of 242424voxels. We use the Dice score to evaluate the
performance of the compared methods.
The results are presented in Table 2. We observe that while the performance of these methods is highly comparable,
the Mean Teacher9performs the worst and Cross Pseudo Supervision (CPS)10performs the best. We choose these two
methods as the baseline and top performer, respectively, for the rest of our experiments. These two methods are described
in more detail below.
Baseline: Mean Teacher.9The Mean Teacher method utilizes two identical networks, denoted as student model f()
and teacher model f(^), as shown in the left panel of Figure 2. The core idea of this method is to use the input with
perturbed Gaussian noise as another input to compute the consistency loss deﬁned as Lunsup =jjP2 P1jj2. The network
parametersf()are updated using Lsemi=Lsup+semiLunsup wheresemi is the trade-off weight. The weights for the
teacher model f()are updated using the exponential moving average (EMA) to temporally ensemble the versions of the
student models f()from previous iterations. This strategy enforces stable predictions without the help of annotations.
Top Performer: Cross Pseudo Supervision10(CPS). The CPS combines the ideas of self-training using pseudo
labels and cross-probability consistency. It utilizes two networks, denoted f(1)andf(2), with different dropouts, as
Table 2. Segmentation performance (Dice score) for 6 different semi-supervised models in our 7-fold cross-validation experiment (Sec.
2.3). The fully supervised model does not have access to any unlabeled samples and thus has the lowest performance. Bold indicates the
best performance in each fold. Among the semi-supervised methods, Mean Teacher is the worst performer and Cross Pseudo Supervision
is the top performer for the MS lesion segmentation task.
Methods Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Fold 6 Fold 7 Avg Std
Fully Supervised60.6982 0.6844 0.6867 0.6989 0.6744 0.6904 0.6964 0.6899 0.0089
Mean Teacher90.7107 0.7048 0.6991 0.7055 0.6990 0.7088 0.7048 0.7047 0.0044
Entropy Minimization200.7238 0.7318 0.7312 0.7422 0.7320 0.7391 0.7257 0.7323 0.0066
Deep Adversarial Networks50.7283 0.7455 0.7294 0.7521 0.7252 0.7263 0.7241 0.7330 0.0111
Uncertainty Aware Mean Teacher90.7347 0.7246 0.7279 0.7351 0.7263 0.7352 0.7321 0.7308 0.0045
FixMatch230.7732 0.7753 0.7614 0.7897 0.7726 0.7656 0.7797 0.7739 0.0092
Cross Pseudo Supervision100.7808 0.7894 0.7764 0.7834 0.7901 0.7803 0.7844 0.7835 0.0050Figure 2. Illustration of the semi-supervised methods schema. The left panel shows the Mean Teacher model. The right panel shows the
CPS model. ’!’ means forward operations, ’ 99K’ means loss supervision, and ’ ==’ means stop-gradient.
shown in the right panel of Figure 2. The two networks produce the probability outputs P1andP2, respectively. We
apply argmax to these probability outputs to obtain one-hot labels, L1andL2. These labels are then used to supervise
the training of the other branch, i.e., L1supervisesP2, andL2supervisesP1. The semi-supervised loss is calculated
usingLunsup 1=CE(L2;P1)andLunsup 2=CE(L1;P2). The network parameters f(1)andf(2)are updated using
Lsemi 1=Lsup1+semiLunsup 1andLsemi 2=Lsup2+semiLunsup 2respectively. We note that Lsup1andLsup2are
computed by passing labeled images into the networks f(1)andf(2), respectively.
2.4 Self-supervised training model in public 3T MRI datasets
In this section, we describe our self-supervised augmentation scheme for leveraging unlabeled 3T MRI datasets. We
introduce random augmentations to our input images and use the pairs of real and augmented images for 3 proxy tasks to
allow self-supervision, inspired by Tang et al.25To accomplish this, we concatenate three different task-speciﬁc heads after
the encoder and compute three self-supervised losses: rotation prediction, inpainting reconstruction, and contrastive loss.
The total self-supervised loss for the pre-training is deﬁned as Lpretraining =1Lrot+2Linpaint +3Lcontrast where
iare hyper-parameters. We choose 1=2=3= 1in our experiments.
2.4.1 Augmentation methods
Table 3 lists the three types of augmentation we apply and the associated parameters for each.
1.Random rotations. We follow the method of Chen et al.26and apply a combination of random rotations of r2
f0;90;180;270g. The caser= 180is illustrated in Figure 3 between panels A!B.
2.Random cutouts and patch swaps. We choose random rectangular patches and replace it with either a different
random patch in the brain or with random noise. Recall that the data is represented as sub-volumes of 969696
voxels in our models. The patch sizes are constrained to [0.05, 0.25] of the sub-volume size along each dimension,
and a total volume less than 30% of the sub-volume size. This augmentation is illustrated in Figure 3 between panels
A!CandB!D.
3.Random histogram shifts. We use the random histogram shift as implemented in MONAI ( https://monai.
io). This is illustrated in Figure 3 between panels C!EandD!F. Note that we do not have a proxy task that
leverages this augmentation; instead, the goal of this augmentation is to reduce the domain gap between 3T and 7T
datasets by increasing the diversity of the training dataset.Figure 3. Augmentation for a subject from the UMCL dataset A.The cropped sub-volume of 969696.A!B:Rotation of
r= 180.A!C;B!D:Random cutout and patch swaps. C!E;D!F:Random histogram shifts.
2.4.2 Proxy Task 1: Rotation Prediction
The rotation prediction task is designed to ensure that the encoder learns to extract robust features that are invariant to
rotation. We train the encoder to predict the rotation angle categories r2f0;90;180;270gin the rotated sub-volume.
A single multilayer perceptron (MLP) head is attached to the encoder to predict the softmax rotation angle possibilities ^Pr
given the ground truth Prand cross-entropy loss is used for updating the parameters.
Lrot= 0;90;180;270X
rPrlog(^Pr)
2.4.3 Proxy Task 2: Inpainting Reconstruction
The inpainting reconstruction task is designed to improve the encoder’s generalization ability and semantic understanding.
We train the encoder to reconstruct the missing information from the random cutout and patch swaps. For the inpainting
reconstruction, we apply a Variational Autoencoder (V AE)27head containing 63D convolution blocks with instance nor-
malization. We add a leaky-ReLU activation on the downsampling path for each scale and an MLP layer on the upsampling
path. Given the cutout patch Ipred and the original image Iorg, we use the L1 Loss to train the reconstruction network.
Linpaint =jjIpred Iorgjj1
2.4.4 Proxy Task 3: Contrastive Representation Learning
The contrastive representation learning task is designed to encourage the encoder to learn representations that are robust to
different data augmentations and reduce the domain gap between different datasets. We create a minibatch of 2Nsamples
by applying two random augmentations (combination of rotate/crop/histogram shift) to each of Nsubjects, such that the
minibatch contains two views of each subject. Then, we randomly select two images from the minibatch and train the
Table 3. Data augmentation during our training process. pindicates the probability of applying an augmentation.
Transform Parameters p
Random Rotate r2f0;90;180;270g 0:25;0:25;0:25;0:25
Random Crop and Patch Swap v=0.3,bmax = 0.25,bmin= 0.05 1
Random Histogram Shift # control points = 1;3 0 :5;0:5encoder to predict whether they are from the same subject. Speciﬁcally, a linear MLP projection head is applied to map the
latent features from the encoder into higher dimensions v. We use cosine-similarity28to maximize the agreement among
positive pairs (same subject i, different augmentations) vi;1;vi;2and minimize the negative pairs (different subjects ivs.
k). Thus the loss function is deﬁned as
Lcontrast = logexp(sim(vi;1;vi;2)=t)PN
k=1;k6=iP2
j=1exp(sim(vi;1;vk;j)=t);
wheretis the temperature and sim()denotes cosine similarity.
2.5 Implementation Details
Given the input sub-volumes of size 969696, we use the Swin-Transformer window and patch size of 222,
which leads to 484848patches. We use 4 down-sampling blocks and set the number of features to 12, resulting in a
latent feature size of (333)2412. We set the number of transformer heads as [3;6;12;24]. For the pre-training
stage, the contrastive head uses a latent feature vector vof size 512. The V AE reconstruction head uses a kernel size of
3with 4 up-sampling stages. Our framework is implemented in PyTorch and MONAI on a single NVIDIA 2080Ti. The
pre-training set is split into 80% for training and 20% for evaluation. We use the inpainting reconstruction L1 loss for our
stopping criteria. For all semi-supervised strategies, we adopt semi = 1 and used a stochastic gradient descent (SGD)
with a learning rate of lr= 1e 4.
3. RESULTS
3.1 Labeled training data size experiment
We ﬁrst examine the performance of our model in the scenario of limited labeled data availability. This is a fairly common
scenario in practice: for example, often, a new MRI protocol may need to be evaluated on a preliminary basis after the ﬁrst
few scans are acquired, before the entire dataset is collected. In this scenario, very few annotated images are available to
train, but we can generally assume that unlabeled images from previous similar studies likely exist.
We split our labeled 7T in-house dataset (n=14) into 7 folds, with 12:2 train:test split in each fold. We evaluate three
limited labeled dataset settings, using 3, 5 or 10 labeled images as training data. The remaining scans (12-3=9, 12-5=7,
and 12-10=2, respectively) are treated as unlabeled data along with the unlabeled 7T in-house dataset (n=23).
We present the 7-fold average Dice similarity coefﬁcients in Table 4. We observe that cross pseudo supervision (CPS)
is again the best semi-supervised method and that it provides a dramatic performance increase (0.7915 vs. 0.6872 Dice)
compared to the fully supervised model trained from scratch, i.e., on 7T data only. Even in the extreme scenario of only
3 training samples, our proposed framework can achieve comparable result to the fully supervised model with 10 training
samples (0.6565 vs. 0.6872). We further observe that the 3T pre-training is beneﬁcial to the 7T studies: the CPS with
pre-training performs consistently better than CPS alone (e.g., 0.7541 vs. 0.7915 for 10 samples). These results are also
presented in graphical format in Figure 4.
Table 4. Segmentation performance (Dice score) under varying amounts of labeled training data availability (Sec. 3.1). Bold text indicates
the best performer and underlined text indicates the second best performer in each column.
MethodsLabeled training data size experiment
3 labeled samples 5 labeled samples 10 labeled samples
Fully supervised training on 7T only 0.52110.0227 0.62350.0107 0.68720.0358
Fully supervised training + Self-Supervised pre-train on 3T 0.53290.0196 0.61380.0400 0.69950.0154
Mean Teacher on 7T only 0.57340.0210 0.61730.0371 0.71090.0249
Cross Pseudo Supervision (CPS) on 7T only 0.62230.0321 0.68850.0206 0.75410.0126
Mean Teacher + Self-Supervised pre-train on 3T 0.63580.0203 0.67040.0184 0.72450.0193
CPS + Self-Supervised pre-train on 3T (SSL2) 0.65650.0156 0.73450.0307 0.79150.02303 Labeled Samples 5 Labeled Samples10 Labeled Samples0.500.550.600.650.700.750.800.85Dice Similarity ScoreImpact of Labeled Training Samples on Model Performance
Fully supervised training on 7T only
Fully supervised training with Self-Supervised pre-train on 3T
Mean T eacher on 7T only
Cross Pseudo Supervision (CPS) on 7T only
Mean T eacher + Self-Supervised pre-train on 3T
CPS + Self-Supervised pre-train on 3TFigure 4. A box plot representation of the segmentation performance (Dice score) under varying amounts of labeled training data
availability (Sec. 3.1). The brightest box is our proposed framework and has the best performance; the darkest box is the baseline model
of fully supervised training from scratch.
3.2 Sparse labeling experiment
We next evaluate our model in a sparse labeling scenario. It can be challenging to carefully and conﬁdently label all the
images in an MRI scan due to constraints on time and effort. A potential solution can be to only label a subset of the 2D
slices in a given 3D MRI volume. This can also be useful when some 2D slices are difﬁcult to annotate due to poor image
quality or artifacts. To examine the use of such sparse annotations, we vary the percentage of annotated slices in a given
training image. The remaining slices and data from the unlabeled set are pooled together and used as the new unlabeled
cohort for the experiment. For example, in the 20% setting, we randomly select 40 slices ( 2000:20) as labeled data and
use the remaining 160 as unlabeled data.
To allow a direct comparison to the experiments in Sec. 3.1, we randomly select 2 subjects for our test set and report the
Dice score in Table 5. Similar to the training data size experiment, we note that CPS is the best performer. We observe that
the self-supervised method contributes more than the semi-supervised method in this setting. We note that our proposed
framework can outperform fully supervised training from scratch with 100% of the annotated data when using only 50%
of annotations (0.7823 vs. 0.6971).
Finally, we present qualitative results in Figure 5. Our proposed framework (panel E) yields the closest results to the
manual ground truth annotations. The results of the 7T fully supervised MRI showed comparable performance in some
areas, but had many false negatives (especially in the top zoom panel), which can be problematic for clinical diagnosis. The
self-supervised learning approach improved the detection in some of these regions, but our proposed method combining
semi-supervised and self-supervised learning resulted in the best performance.
Table 5. Segmentation performance (Dice score) under varying amounts of sparsely labeled training data (Sec. 3.2). Bold text indicates
the best performer and underlined text indicates the second best performer in each column.
MethodsSparse labeling experiment
10 % 20 % 50 % 100 %
Fully supervised training on 7T only 0.5174 0.5732 0.6542 0.6971
Fully supervised training + Self-Supervised pre-train on 3T 0.6233 0.6422 0.6855 0.7121
Mean Teacher on 7T only 0.5673 0.5884 0.6627 0.7047
Cross Pseudo Supervision (CPS) on 7T only 0.6107 0.6342 0.7108 0.7835
Mean Teacher + Self-Supervised pre-train on 3T 0.6411 0.6589 0.7232 0.7624
CPS + Self-Supervised pre-train on 3T (SSL2) 0.6523 0.6785 0.7823 0.8186Ground Truth Fully Supervised Fully Supervised + Mean Teacher + CPS +
7T only Self-Supervised Self-Supervised Self-Supervised
pre-train on 3T pre-train on 3T pre-train on 3T
Figure 5. Qualitative segmentation results in our 7T dataset with 100% labeling and 12 labeled training samples. Segmentations are
overlaid on the FLAIR image. Top and bottom rows show zoomed-in views of the yellow ROIs. Green denotes the true positives, red
denotes false negatives, and yellow denotes false positives.
4. CONCLUSION
In this paper, we proposed a novel approach for achieving robust MS lesion segmentation in 7T brain MRI data by utilizing
self-supervised training to embed information from publicly available 3T brain MRI data, in combination with semi-
supervised techniques to leverage limited labeled 7T data. Our experimental results demonstrate the effectiveness of this
approach, achieving higher accuracy with either a small number of training samples or sparsely annotated images.
We make the pre-trained weights of our proposed approach publicly available to beneﬁt future 7T brain MRI studies.
In future work, we aim to investigate the generalizability of our proposed pre-trained encoder to other 7T MRI studies as
well as its potential for use in downstream tasks beyond lesion segmentation.
ACKNOWLEDGMENTS
This work was supported, in part, by NIH grant R01-NS094456 and National Multiple Sclerosis Society grant PP-1905-
34001. Francesca Bagnato receives research support from Biogen Idec, the National Multiple Sclerosis Society (RG-1901-
33190) and the National Institutes of Health (1R01NS109114-01). Francesca Bagnato did not receive ﬁnancial support for
the research, authorship and publication of this article.
REFERENCES
[1] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B., “Swin Transformer: Hierarchical vi-
sion Transforme using shifted windows,” in [ Proceedings of the IEEE/CVF International Conference on Computer
Vision ], 10012–10022 (2021).
[2] Roy, S., Butman, J. A., Reich, D. S., Calabresi, P. A., and Pham, D. L., “Multiple Sclerosis lesion segmentation from
brain MRI via fully convolutional neural networks,” arXiv preprint arXiv:1803.09172 (2018).
[3] Danelakis, A., Theoharis, T., and Verganelakis, D. A., “Survey of automated Multiple Sclerosis lesion segmentation
techniques on Magnetic Resonance Imaging,” Computerized Medical Imaging and Graphics 70, 83–100 (2018).
[4] Zhang, H., Valcarcel, A. M., Bakshi, R., Chu, R., Bagnato, F., Shinohara, R. T., Hett, K., and Oguz, I., “Multiple
Sclerosis lesion segmentation with Tiramisu and 2.5 d stacked slices,” in [ International Conference on Medical Image
Computing and Computer-Assisted Intervention ], 338–346, Springer (2019).[5] Zhang, H., Li, H., and Oguz, I., “Segmentation of new ms lesions with tiramisu and 2.5 d stacked slices,” MSSEG-2
challenge proceedings: Multiple sclerosis new lesions segmentation challenge using a data management and pro-
cessing infrastructure , 61 (2021).
[6] Liu, H., Fan, Y ., Li, H., Wang, J., Hu, D., Cui, C., Lee, H. H., and Oguz, I., “ModDrop++: A dynamic ﬁlter network
with intra-subject co-training for Multiple Sclerosis lesion segmentation with missing modalities,” arXiv preprint
arXiv:2203.04959 (2022).
[7] Taleb, A., Loetzsch, W., Danz, N., Severin, J., Gaertner, T., Bergner, B., and Lippert, C., “3D self-supervised methods
for medical imaging,” Advances in Neural Information Processing Systems 33, 18158–18172 (2020).
[8] Cheplygina, V ., de Bruijne, M., and Pluim, J. P., “Not-so-supervised: a survey of semi-supervised, multi-instance,
and transfer learning in medical image analysis,” Medical image analysis 54, 280–296 (2019).
[9] Tarvainen, A. and Valpola, H., “Mean teachers are better role models: Weight-averaged consistency targets improve
semi-supervised deep learning results,” Advances in neural information processing systems 30(2017).
[10] Chen, X., Yuan, Y ., Zeng, G., and Wang, J., “Semi-supervised semantic segmentation with cross pseudo supervision,”
in [Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ], 2613–2622 (2021).
[11] Hatamizadeh, A., Tang, Y ., Nath, V ., Yang, D., Myronenko, A., Landman, B., Roth, H. R., and Xu, D., “Unetr: Trans-
formers for 3D medical image segmentation,” in [ Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision ], 574–584 (2022).
[12] Li, H., Hu, D., Liu, H., Wang, J., and Oguz, I., “Cats: Complementary CNN and Transformer encoders for segmen-
tation,” in [ 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI) ], 1–5, IEEE (2022).
[13] Hatamizadeh, A., Nath, V ., Tang, Y ., Yang, D., Roth, H. R., and Xu, D., “Swin Unetr: Swin Transformers for semantic
segmentation of brain tumors in MRI images,” in [ International MICCAI Brainlesion Workshop ], 272–284, Springer
(2022).
[14] Carass, A., Roy, S., Jog, A., Cuzzocreo, J. L., Magrath, E., Gherman, A., Button, J., Nguyen, J., Prados, F., Sudre,
C. H., et al., “Longitudinal Multiple Sclerosis lesion segmentation: resource and challenge,” NeuroImage 148, 77–102
(2017).
[15] Lesjak, ˇZ., Galimzianova, A., Koren, A., Lukin, M., Pernu ˇs, F., Likar, B., and ˇSpiclin, ˇZ., “A novel public MR
image dataset of Multiple Sclerosis patients with lesion segmentations based on multi-rater consensus,” Neuroinfor-
matics 16(1), 51–63 (2018).
[16] Commowick, O., Cervenansky, F., and Ameli, R., “Msseg challenge proceedings: Multiple Sclerosis lesions segmen-
tation challenge using a data management and processing infrastructure,” in [ Miccai ], (2016).
[17] Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., Burren, Y ., Porz, N., Slotboom,
J., Wiest, R., et al., “The multimodal brain tumor image segmentation benchmark (BRATS),” IEEE transactions on
medical imaging 34(10), 1993–2024 (2014).
[18] Isensee, F., Schell, M., Pﬂueger, I., Brugnara, G., Bonekamp, D., Neuberger, U., Wick, A., Schlemmer, H.-P., Hei-
land, S., Wick, W., et al., “Automated brain extraction of multisequence MRI using artiﬁcial neural networks,” Human
brain mapping 40(17), 4952–4964 (2019).
[19] Tustison, N. J., Avants, B. B., Cook, P. A., Zheng, Y ., Egan, A., Yushkevich, P. A., and Gee, J. C., “N4ITK: improved
n3 bias correction,” IEEE transactions on medical imaging 29(6), 1310–1320 (2010).
[20] Vu, T.-H., Jain, H., Bucher, M., Cord, M., and P ´erez, P., “Advent: Adversarial entropy minimization for domain
adaptation in semantic segmentation,” in [ Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition ], 2517–2526 (2019).
[21] Zhang, Y ., Yang, L., Chen, J., Fredericksen, M., Hughes, D. P., and Chen, D. Z., “Deep adversarial networks for
biomedical image segmentation utilizing unannotated images,” in [ International conference on medical image com-
puting and computer-assisted intervention ], 408–416, Springer (2017).
[22] Yu, L., Wang, S., Li, X., Fu, C.-W., and Heng, P.-A., “Uncertainty-aware self-ensembling model for semi-supervised
3D left atrium segmentation,” in [ International Conference on Medical Image Computing and Computer-Assisted
Intervention ], 605–613, Springer (2019).
[23] Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L.,
“Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence,” Advances in neural information
processing systems 33, 596–608 (2020).
[24] Luo, X., Wang, G., Liao, W., Chen, J., Song, T., Chen, Y ., Zhang, S., Metaxas, D. N., and Zhang, S., “Semi-
supervised medical image segmentation via uncertainty rectiﬁed pyramid consistency,” Medical Image Analysis 80,
102517 (2022).[25] Tang, Y ., Yang, D., Li, W., Roth, H. R., Landman, B., Xu, D., Nath, V ., and Hatamizadeh, A., “Self-supervised
pre-training of Swin Transformers for 3D medical image analysis,” in [ Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition ], 20730–20740 (2022).
[26] Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., and Rueckert, D., “Self-supervised learning for medical
image analysis using image context restoration,” Medical image analysis 58, 101539 (2019).
[27] An, J. and Cho, S., “Variational autoencoder based anomaly detection using reconstruction probability,” Special
Lecture on IE 2(1), 1–18 (2015).
[28] Chen, X., Fan, H., Girshick, R., and He, K., “Improved baselines with momentum contrastive learning,” arXiv
preprint arXiv:2003.04297 (2020).