arXiv:2302.12358v2  [math.PR]  9 Jan 2025Extending Wormald’s Diﬀerential Equation Method to
One-sided Bounds
Patrick Bennett∗Calum MacRury†
Abstract
In this note, we formulate a “one-sided” version of Wormald’ s diﬀerential equation
method. Inthestandard “two-sided” method, oneis given afa mily of randomvariables
which evolve over time and which satisfy some conditions inc luding a tight estimate
of the expected change in each variable over one time step. Th ese estimates for the
expected one-step changes suggest that the variables ought to be close to the solution
of a certain system of diﬀerential equations, and the standar d method concludes that
this is indeed the case. We give a result for the case where ins tead of a tight estimate
for each variable’s expected one-step change, we have only a n upper bound. Our proof
is very simple, and is ﬂexible enough that if we instead assum e tight estimates on the
variables, then we recover the conclusion of the standard di ﬀerential equation method.
1 Introduction
In the most basic setup of Wormald’s diﬀerential equation method, o ne is given a sequence
of random variables ( Y(i))∞
i=0derived from some random process which evolves step by
step. The random variables ( Y(i))∞
i=0all implicitly depend on some n∈N, and the goal is
understand their typical behaviour as n→ ∞.
Our running example is based on the Erd˝ os–R´ enyi random graph process (Gi)m
i=0on
vertex set [ n] :={1,...,n}whereGi= ([n],Ei) andm,n∈N. HereG0= ([n],E0) is
the empty graph, and Gi+1is constructed from Giby drawing an edge ei+1from/parenleftbig[n]
2/parenrightbig
\Ei
uniformly at random (u.a.r.), and setting Ei+1:=Ei∪ {ei+1}. Suppose that we wish to
understand the size of the matching produced by the greedy algor ithm as it executes on
(Gi)m
i=0. More speciﬁcally, when ei+1arrives, the greedy algorithm adds ei+1to the current
matching if the endpoints of ei+1were not previously matched. We will let m=cn, i.e.
we will add a linear number of random edges. Observe that if Y(i) is the number of edges
∗Department of Mathematics, Western Michigan University ( patrick.bennett@wmich.edu ). Supported
in part by Simons Foundation Grant #426894.
†Graduate Schoolof Business, ColumbiaUniversity, New York, USA, cm4379@columbia.edu . The major-
ity of the author’s work done on this paper was while they were aﬃliate d with the Department of Computer
Science at the University of Toronto.
1ofGimatched by the algorithm, then Y(i) is a function of e1,...,e i(formally, Y(i) isHi-
measurable where Hiis the sigma-algebra generated from e1,...,e i). Then for i < m,
E[∆Y(i)| Hi] =/parenleftbign−2Y(i)
2/parenrightbig
/parenleftbign
2/parenrightbig
−i=/parenleftbigg
1−2Y(i)
n/parenrightbigg2
+O/parenleftbigg1
n/parenrightbigg
, (1)
where ∆Y(i) :=Y(i+1)−Y(i), and the asymptotics are as n→ ∞(which will be the case
for the remainder of this note). By scaling ( Y(i))m
i=0byn, we can interpret the left-hand side
of (1) as the “derivative” of Y(i)/nevaluated at i/n. This suggests the following diﬀerential
equation:
y′(t) = (1−2y(t))2, y(0) = 0 (2)
withinitialcondition y(0) = 0. Wormald’sdiﬀerential equationmethodallowsustoconclude
thatwith high probability (i.e. with probability tending to 1 as n→ ∞, henceforth
abbreviated w.h.p.),
Y(m) = (1+ o(1))y(m/n), (3)
wherey(t) :=t/(1+2t) is the unique solution to (2).
Returning to the general setup of the diﬀerential equation metho d, suppose we are given
a sequence of random variables ( Y(i))∞
i=0which implicitly depend on n∈N. Assume that
the one-step changes are bounded, i.e., there exists a constant β≥0 such that |∆Y(i)| ≤β
for each i≥0. Moreover, suppose each Y(i) is determined by some sigma-algebra Hi, and
itsexpected one-step changes are described by some Lipshitz function f=f(t,y). That is,
for eachi≥0,
E[∆Y(i)| Hi] =f(i/n,Y(i)/n)+o(1). (4)
IfY(0) = (1 + o(1))/tildewideynfor some constant /tildewidey, andm=m(n) is not too large, then the
diﬀerential equation method allows us to conclude that w.h.p. Y(m)/n= (1+o(1))y(m/n)
forywhich satisﬁes the diﬀerential equation suggested by (4), i.e.
y′(t) =f(t,y(t))
with initial condition y(0) =/tildewidey. In this note, we consider the case when we have an inequality
in place of (4). We are motivated by applications to online algorithms in w hich one wishes to
upperboundtheperformanceof anyonlinealgorithm,opposedtojusta particular algorithm.
(See Section 1.1 for an example pertaining to online matching in ( Gi)m
i=0as well as some
discussion of further applications). We are also motivated by the ex istence of deterministic
results of which we wanted to prove a random analogue. For example , consider the following
classical result due to Petrovitch [10]:
Theorem 1. Supposef:R2→Ris Lipschitz continuous, and y=y(t)satisﬁes
y′(t) =f(t,y(t)), y(c) =y0.
Supposez=z(t)is diﬀerentiable and satisﬁes
z′(t)≤f(t,z(t)), z(c) =z0≤y0.
Thenz(t)≤y(t)for allt≥c.
2With the above result in mind (as well as the standard diﬀerential equ ation method), it’s
natural to wonder what can be said about a sequence of random va riables (Zi)∞
i=1satisfying
E[∆Zi| Hi]≤f(i/n,Z i/n)
instead of the equality version (4). More precisely, if ( Yi)∞
i=1satisﬁes (4) and Z0< Y0then
should it not follow that we likely have Zi≤Yi(perhaps modulo some relatively small error
term) for all i≥0?
We brieﬂy point out that if fis nonincreasing in its second variable, then the problem
described in the previous paragraph is much easier. Indeed, whene ver the random variable
satisﬁesZi−Yi≤0, it is also a supermartingale. More precisely, when Zi≤Yiwe have that
E[∆(Zi−Yi)| Hi]≤f(i/n,Z i/n)−f(i/n,Yi/n)≤0
by the monotonicity assumption. In this case, assuming the initial ga p|Z0−Y0|is large
enough, standard martingale techniques can be used to bound the probability that the
supermartingale Zi−Yibecomes positive. However, we would like to handle applications
where we do not have this monotonicity assumption. For instance, in our running example,
f(t,z) = (1−2z)2isnotincreasing in z.
Of course, the diﬀerential equation method in general deals with sy stems of random
variables (and the associated systems of diﬀerential equations). So what can be said about
systems of deterministic functions whose derivatives satisfy inequ alities instead? It turns out
that to generalize Theorem 1 to a system, we need the functions to becooperative . We
say the functions fj:=Ra+1→R,1≤j≤aare cooperative (respectively, competitive )
if eachfjis nondecreasing (respectively, nonincreasing) in all of its a+1 inputs except for
possibly the ﬁrst input and the ( j+1)thone. In other words, fj(t,y1,...ya) is nondecreasing
in all variables except possibly tandyj. Note that some sources refer to a system with the
cooperative property as being quasimonotonic . Observe that in the one-dimensional case
a= 1, every function is cooperative/cooperate. The following theor em is folklore (see [12]
for some relevant background, and Section 3 for a proof):
Theorem 2. Suppose fj:Ra+1→R,1≤j≤aare Lipschitz continuous and cooperative,
andyjsatisﬁes
y′
j(t) =fj(t,y1(t),...,y a(t)),1≤j≤a, t≥c.
Supposezj,1≤j≤aare diﬀerentiable and satisfy zj(c)≤yj(c)and
z′
j(t)≤fj(t,z1(t),...,z a(t)),1≤j≤a, t≥c.
Thenzj(t)≤yj(t)for all1≤j≤a,t≥c.
Cooperativity is necessary in the sense that if we do not have it, the n one can choose
initial conditions for the functions yj,zjto make the conclusion of Theorem 2 fail. Indeed,
suppose we do not have cooperativity, i.e. there exist j,j′withj′/\⌉}atio\slash=j+1 and some points
p,p′∈Ra+1that agree everywhere except for their j′thcoordinate, where we have pj′> p′
j′,
andfj(p)< fj(p′). Consider the following initial conditions:
/parenleftBig
c,y1(c),...,y a(c)/parenrightBig
=p,/parenleftBig
c,z1(c),...,z a(c)/parenrightBig
=p′.
3Then we have that zj(c) =yj(c) =pj+1=p′
j+1. Furthermore, z′
j(c) could be as large as
fj(p′)> fj(p) =y′
j(c) in which case clearly zj(t)> yj(t) for some t > c.
Our main theorem in this paper, Theorem 3, is essentially the random a nalogue of The-
orem 2. Before providing its formal statement, we expand upon wh y it is useful for proving
impossibility/hardness results for online algorithms. The reader can safely skip Section 1.1
if they would ﬁrst like to instead read Theorem 3.
1.1 Motivating Applications
Theexampleconsideredinthissectioniscloselyrelatedtothe1 /2-impossibility(orhardness)
result for an online stochastic matching problem considered by the s econd author, Ma and
Grammel in Theorem 5of [9]. In fact, Theorem 3 is used explicitly topro ve this result, andit
is also used in a similar way in Theorem 1 .4 of [8]. Our theorem can also be used to simplify
the proofs of the1
2(1+e−2)-impossibility result of Fu, Tang, Wu, Wu and Zhang (Theorem
2 in [6]), and the 1 −ln(2−1/e)-impossibility result of Fata, Ma and Simchi-Levi (Lemma
5 in [3]). All of the aforementioned papers prove impossibility results f or various online
stochastic optimization problems – more speciﬁcally, hardness resu lts foronline contention
resolution schemes [4] orprophet inequalities against an “ex-ante relaxation” [7]. We think
that Theorem 3 will ﬁnd further applications as a technical tool in th is area.
Let us now return to the deﬁnition of the Erd˝ os–R´ enyi random g raph process ( Gi)m
i=0
as discussed in Section 1, where we again assume that m=cnfor some constant c >0.
Recall that (3) says that if Y(m) is the size of the matching constructed by the greedy
matching algorithm when executed on ( Gi)m
i=0, then w.h.p. Y(m)/n= (1+o(1))y(c) where
y(c) =c/(1 + 2c). In fact, (3) can be made to hold with probability 1 −o(1/n2), and so
E[Y(m)]/n= (1+o(1))c/(1+2c) after taking expectations.
The greedy matching algorithm is an example of an online (matching) algorithm
on (Gi)m
i=0. An online algorithm begins with the empty matching on G0, and its goal is
to build a matching of Gm. While it knows the distribution of ( Gi)m
i=0upfront, it learns
theinstantiations of the edges sequentially and must execute online. Formally, in each s tep
i≥1, itispresented eiandthenmakes anirrevocabledecisionastowhether ornottoinclud e
eiin its current matching, based upon e1,...,e i−1and its previous matching decisions. Its
output is the matching Mm, and its goal is to maximize E[|Mm|]. Here the expectation is
over (Gi)m
i=1and any randomized decisions made by the algorithm.
Suppose that we wish to prove that the greedy algorithm is asympto tically optimal.
That is, for anyonline algorithm, if Mmis the matching it outputs on Gm, thenE[|Mm|]≤
(1+o(1))E[Y(m)]. In order to prove this directly, one must compare the performa nce of any
online algorithm to the greedy algorithm. This is inconvenient to argue , as there exist rare
instantiations of ( Gi)m
i=0in which being greedy is clearly sub-optimal.
We instead upper bound the performance of any online algorithm by ( 1+o(1))y(c)n. Let
(Mi)m
i=0be the sequence of matchings constructed by an arbitrary online algorithm while
executing on ( Gi)m
i=0. For simplicity, assume that the algorithm is deterministic so that Mi
isHi-measurable. In this case, we can replace (1) with inequality. I.e., if Z(i) :=|Mi|, then
fori < m,
E[∆Z(i)| Hi]≤/parenleftbigg
1−2Z(i)
n/parenrightbigg2
+O/parenleftbigg1
n/parenrightbigg
. (5)
4Recall now the intuition behind the diﬀerential equation method. If w e scale (Z(i))n
i=0by
n, then we can interpret the left-hand side of (5) as the “derivative ” ofZ(i)/nevaluated at
i/n. This suggests the following diﬀerential inequality:
z′≤(1−2z)2,
with inital condition z(0) = 0. By applying Theorem 3 to ( Z(i))m
i=0, we get that
Z(m)/n≤(1+o(1))y(c)
with probability 1 −o(n−2). As a result, E[Z(m)]≤(1+o(1))y(c)n, and so we can conclude
that greedy is asymptotically optimal.
2 Main Theorem
For any sequence ( Z(i))∞
i=0of random variables and i≥0, we will use the notation ∆ Z(i) :=
Z(i+ 1)−Z(i). Note that given a ﬁltration (Hi)∞
i=0(i.e., a sequence of increasing σ-
algebras), we say that ( Zj(i))∞
i=0isadapted to (Hi)∞
i=0, provided ZiisHi-measurable for
eachi≥0. Finally, we say that a stopping time Iisadapted to (Hi)∞
i=0, provided the event
{I=i}isHi-measurable for each i≥0.
Givena∈N, suppose that D ⊆Ra+1is a bounded domain, and for 1 ≤j≤a, let
fj:D →R. We assume that the following hold for each j:
a)fjisL-Lipschitz on D, i.e.
|fj(t,y1,...,y a)−fj(t′,y′
1,...,y′
a)| ≤L·max{|t−t′|,|y1−y′
1|,...,|ya−y′
a|}
b)|fj| ≤BonD, and
c) the (fj)a
j=1are cooperative.
Given (0,˜y1,...,˜ya)∈ D, assume that y1(t),...,y a(t) is the (unique) solution to the system:
y′
j(t) =fj(t,y1(t),...,y a(t)), y j(0) = ˜yj (6)
for allt∈[0,σ], whereσis any positive value.
Theorem 3. Suppose that for each 1≤j≤awe have a sequence of random variables
(Zj(i))∞
i=0which is adapted to some ﬁltration (Hi)∞
i=0. Letn∈N, andβ,b,λ,δ > 0be any
parameters such that λ≥max/braceleftbig
β+B,L+BL+δn
3L/bracerightbig
. Given an arbitrary stopping time I≥0
adapted to (Hi)∞
i=0, suppose that the following properties hold for each 0≤i <min{I,σn}:
1. The ‘Boundedness Hypothesis’: maxj|∆Zj(i)| ≤β, andmaxjE[(∆Zj(i))2| Hi]≤b
2. The ‘Trend Hypothesis’: (i/n,Z 1(i)/n,...Z a(i)/n)∈ Dand
E[∆Zj(i)| Hi]≤fj(i/n,Z 1(i)/n,...Z a(i)/n)+δ
for each 1≤j≤a.
53. The ‘Initial Condition’: Zj(0)≤yj(0)n+λfor all1≤j≤a.
Then, with probability at least 1−2aexp/parenleftBig
−λ2
2(bσn+2βλ)/parenrightBig
,
Zj(i)≤nyj(i/n)+3λe2Li/n
for all1≤j≤aand0≤i≤min{I,σn}.
Remark 4 (Simpliﬁed Parameters) .By taking b=β2andI=⌈σn⌉, we can recover
a simpler version of the theorem which is suﬃcient for many application s, including the
motivating example of Section 1.1.
Remark 5 (Stopping Time Selection) .Let 0≤γ≤1 be an additional parameter to
Theorem 3. The stopping time Iis most commonly applied in the following way. Suppose
that (Ei)∞
i=0is a sequence of events adapted to ( Hi)∞
i=0, and for each 0 ≤i < σn, Conditions
1. and 2. are onlyveriﬁed when Eiholds. Moreover, assume that P[∩σn−1
i=0Ei] = 1−γ. By
deﬁningIto be the smallest i≥0 such that Eidoesnotoccur, Theorem 3 implies that with
probability at least 1 −2aexp/parenleftBig
−λ2
2(bσn+2βλ)/parenrightBig
−γ,
Zj(i)≤nyj(i/n)+3λe2Li/n
for all 1≤j≤aand 0≤i≤σn.
Remark 6 (Competitive Functions) .Theorem 3 yields upper bounds for families of ran-
dom variables. There is a symmetric theorem for lower bounds, wher e all the appropriate
inequalities are reversed and the functions fjare competitive instead of cooperative.
We conclude the section with a corollary of Theorem 3 which provides a useful extension
of the theorem. Roughly speaking, the extension says that when v erifying Conditions 1. and
2. at time 0 ≤i≤min{σn,I}, it does not hurt to assume that (3) holds.
Corollary 7 (of Theorem 3) .Suppose that in the terminology of Theorem 3, Conditions 1.
and 2. are only veriﬁed for each 0≤i≤min{I,σn}which satisﬁes Zj′(i)≤nyj′(i/n) +
3λe2Li/nfor all1≤j′≤a. In this case, the conclusion of Theorem 3 still holds. I.e., with
probability at least 1−2aexp/parenleftBig
−λ2
2(bσn+2βλ)/parenrightBig
,
Zj(i)≤nyj(i/n)+3λe2Li/n
for all1≤j≤aand0≤i≤min{I,σn}.
Proof of Corollary 7. LetI∗be the ﬁrst i≥0 such that
Zj′(i)> nyj′(i/n)+3λe2Li/n
for some 1 ≤j′≤a. Clearly, I∗is a stopping time adapted to ( Hi)∞
i=0. Moreover, by the
assumptions of the corollary, Conditions 1. and 2. hold for each 0 ≤i≤min{I∗,I,σn}and
1≤j≤a. Thus, Theorem 3 implies that with probability at least 1 −2aexp/parenleftBig
−λ2
2(bσn+2βλ)/parenrightBig
,
Zj(i)≤nyj(i/n)+3λe2Li/n
for all 1≤j≤aand 0≤i≤min{I,I∗,σn}. Since the preceding event holds if and only if
I∗>min{I,σn}, the corollary is proven.
63 Proving Theorem 3
Before proceeding to the proof of Theorem 3, we provide some intu ition for our approach by
presenting a proof of the deterministic setting (i.e., Theorem 2). Th e notation and structure
of the proof is intentionally chosen so as to relate to the analogous a pproach taken in the
proof of Theorem 3. Moreover, the main claim we prove can be viewed as anapproximate
version of Theorem 2, in which the upper bounds on zj(0) andz′
jonly hold up to an additive
constant δ >0.
Proof of Theorem 2. Let us assume that c= 0 is the boundary of the domain, and Lis a
Lipschitz constant for the cooperative functions ( fj)a
j=1. We shall prove the following: Given
an arbitrary δ >0, if
z′
j(t)≤f(t,zj(t))+δ, z j(0)≤yj(0)+δ (7)
for all 1≤j≤aandt≥0, then
zj(t)≤yj(t)+δeLt(8)
for each 1 ≤j≤aandt≥0. Since (7) holds eachδ >0 under the assumptions of
Theorem 2, so must (8). This will imply that zj(t)≤yj(t) for each 1 ≤j≤aandt≥0,
thus completing the proof.
In order to prove that (7) implies (8), deﬁne
g(t) := 2δeLt, s j(t) :=zj(t)−(yj(t)+g(t)), I j(t) := [yj(t),yj(t)+g(t)).
It suﬃces to show that max 1≤j≤asj(t)≤0 for all t≥0. Observe ﬁrst that sj(0) =zj(0)−
yj(0)−g(0)≤ −δ <0 for all 1 ≤j≤a. Suppose for the sake of contradiction that there
exists some 1 ≤j′≤asuch that sj′(t)>0 for some t >0. In this case, there must be some
valuet1withsj′(t1) = 0 and max 1≤j≤asj(t)<0 for allt < t1. Furthermore, there must be
somet0< t1such that sj′(t)∈[−g(t),0) for all t0≤t < t1. Thus, for t0≤t < t1we have
that
−g(t)≤zj′(t)−[yj′(t)+g(t)]<0 =⇒yj′(t)≤zj′(t)< yj′(t)+g(t),(9)
and so
fj′/parenleftBig
t,z1(t),...za(t)/parenrightBig
≤fj′/parenleftBig
t,y1(t)+g(t),...,z j′(t),...,y a(t)+g(t)/parenrightBig
≤fj′/parenleftBig
t,y1(t),...,y a(t)/parenrightBig
+Lg(t) (10)
where the ﬁrst line is by cooperativity of the functions fjand the second line is by the
Lipschitzness of fj′applied to (9). As such, for all t∈[t0,t1),
s′
j′(t) =z′
j′(t)−y′
j′(t)−g′(t)
=fj′/parenleftBig
t,z1(t),...,z a(t)/parenrightBig
−fj′/parenleftBig
t,y1(t),...,y a(t)/parenrightBig
−g′(t)
≤Lg(t)−g′(t) = 0
where the last line uses (10). But now we have a contradiction: sj′(t0)∈[−g(t0),0) so it is
negative, s′
j′(t)≤0 on [t0,t1), and yet sj′(t1) = 0.
7Our proof of Theorem 3 is based partly onthe critical interval method . Similar ideas were
used by used by Telcs, Wormald and Zhou [11] as well as Bohman, Friez e and Lubetzky [2]
(whose terminology we use here). For a gentle discussion of the met hod see the paper
of the ﬁrst author and Dudek [1]. Roughly speaking, the critical inte rval method allows
us to assume we have good estimates of key variables during the ver y steps that we are
most concerned with those variables. Historically this method has be en used with more
standard applications of the diﬀerential equation method in order t o exploit self-correcting
random variables, i.e. a variable with the property that when it stray s signiﬁcantly from its
trajectory, its expected one-step change makes it likely to move b ack toward its trajectory.
For such a random variable, knowing that it lies in an interval strictly a bove (or below) the
trajectory gives us a more favorable estimate for its expected on e-step change. In our setting
we use the method for a similar but diﬀerent reason. In particular sin ce we can only hope
for one-sided bounds, we may as well ignore our random variables wh en they are far away
from their bounds (in any case, we do not have or need good estimat es for their expected
one-step changes etc. during the steps when all variables are far from their bounds).
We give an analogy. A rough proof sketch for Theorem 2 is as follows: in order to have
zj(t)> yj(t) for some tthere must be some time interval during which zj≈yjand during
that interval zjmust increase signiﬁcantly faster than yj, which contradicts what we know
about their derivatives. An analogous proof sketch for Theorem 3 is as follows: in order for
Zj(i) to violate its upper bound, it must ﬁrst enter a critical interval wh ich we will deﬁne to
be near the upper bound, and then Zjmust increase signiﬁcantly (more than we expect it
to) over the subsequent steps, which while possible, is unlikely. Our p robability bound will
follow from Freedman’s inequality, which we will now state.
Theorem 8 (Freedman’s Inequality [5]) .Suppose that (Hi)∞
i=0is an increasing sequence of
σ-algebras (i.e., Hi−1⊆ Hifor alli≥1.) Moreover, let (Mi)∞
i=0be a sequence of random
variables adapted to (Hi)∞
i=0(i.e., each MiisHi-measurable). Recall that ∆Mi:=Mi+1−Mi
andVar(∆Mi| Hi) :=E[∆M2
i| Hi]−E[∆Mi| Hi]2fori≥0. Fixm∈Nandβ,b≥0.
Assume that for each 0≤i < m,E[∆Mi| Hi] = 0,|∆Mi| ≤β, andVar(∆Mi| Hi)≤b.
Then, for any 0< ε <1,
P(∃0≤i≤m:|Mi−M0| ≥ε)≤2exp/parenleftbigg
−ε2
2(bm+βε)/parenrightbigg
.
Moreover, if Iis an arbitrary stopping time adapted to (Hi)∞
i=0(i.e.,{I=i}isHi-measurable
for each i≥0), and the above conditions regarding (Mi)∞
i=0are only veriﬁed for all 0≤i <
min{I,m}, then
P(∃0≤i≤min{I,m}:|Mi−M0| ≥ε)≤2exp/parenleftbigg
−ε2
2(bm+βε)/parenrightbigg
.
Proof of Theorem 3. Fix 0≤i≤σn, and set m:=σn,t=ti=i/n, andg(t) := 3λe2Ltfor
8convenience. Deﬁne
Sj(i) : =Zj(i)−(nyj(t)+g(t)), Xj(i) :=i−1/summationdisplay
k=0E[∆Sj(k)| Hk],
Mj(i) : =Sj(0)+i−1/summationdisplay
k=0(∆Sj(k)−E[∆Sj(k)| Hk]),
so thatSj(i) =Xj(i) +Mj(i), (Mj(i))m
i=0is a martingale and Xj(i) isHi−1-measurable
(i.e. (Xj(i) +Mj(i))m
i=0is the Doob decomposition of ( Sj(i))m
i=0). Note that we can view
Sj(i)/nas the random analogue of sj(t) =mj(t) +xj(t) from the proof of Theorem 2. In
the previous deterministic setting, the decomposition sj(t) =mj(t) +xj(t) is redundant,
asmj(t) =sj(0), and so xj(t) andsj(t) diﬀer by a constant. In contrast, Mj(i) isSj(0),
plus/summationtexti−1
k=0(∆Sj(k)−E[∆Sj(k)| Hk]), the latter of which we can view as introducing some
random noise . We handle this random noise by showing that Mj(i) is typically concentrated
aboutSj(0) due to Freedman’s inequality (see Theorem 8). We refer to this a s theproba-
bilisticpart of the proof. Assuming that this concentration holds, we can upper bound Zj(i)
bynyj(t) +g(t) via an argument which proceeds analogously to the proof of Theor em 2.
This is the deterministic part of the proof.
Beginning with the probabilistic part of the proof, we restrict our at tention to 0 ≤i <
min{I,m}. Observe ﬁrst that
∆Mj(i) = ∆Sj(i)−E[∆Sj(i)| Hi]
= ∆[Zj(i)−(nyj(t)+g(t))]−E[∆[Zj(i)−(nyj(t)+g(t))]| Hi]
= ∆Zj(i)−E[∆Zj(i)| Hi],
and so by Condition 1. ,
|∆Mj(i)| ≤ |∆Zj(i)|+|E[∆Zj(i)| Hi]| ≤2β.
Also,Var[∆Mj(i)| Hi] =E[(∆Zj(i)−E[∆Zj(i)| Hi])2| Hi]. Thus,
Var[∆Mj(i)| Hi] =E[∆Zj(i)2| Hi]−E[∆Zj(i)| Hi]2
≤E[∆Zj(i)2| Hi]
≤b by Condition 1.
We can therefore apply Theorem 8 to get that
P(∃0≤j≤a,0≤i≤min{I,m}:|Mj(i)−Mj(0)| ≥λ)≤2aexp/parenleftbigg
−λ2
2(bm+2βλ)/parenrightbigg
.(11)
Suppose the above event does nothappen i.e., for all 0 ≤j≤a,0≤i≤min{m,I}we
have that |Mj(i)−Mj(0)|< λ. We will show that we also have Zj(i)≤nyj(t)+g(t) for all
0≤i≤min{m,I}and 1≤j≤a(equivalently, max jSj(i)≤0 for all 0 ≤i≤min{m,I}).
This implication is the deterministic part of the proof. By combining it wit h the probability
bound of (11), this will complete the proof of Theorem 3.
9Suppose for the sake of contradiction that i′is the minimal integer such that 0 ≤i′≤
min{m,I}andZj(i′)> nyj(ti′)+g(ti′) for some j. Deﬁne the critical interval
Ij(i) := [nyj(t),nyj(t)+g(t)].
First observe that since g(0) := 3λ > λ, Condition 3. implies that i′>0 (and so i′−1≥0.)
We claim that Zj(i′−1)∈Ij(i′−1). Indeed, note that by the minimality of i′we have
thatZj(i′−1)≤nyj(ti′−1)+g(ti′−1). On the other hand, |y′
j|=|fj| ≤Band so each yjis
B-Lipschitz. Thus, since λ≥β+B(by assumption),
Zj(i′−1)≥Zj(i′)−β > ny j(ti′)+g(ti′)−β
≥nyj(ti′−1)+3λ−β−B
≥nyj(ti′−1).
As a result, Zj(i′−1)∈Ij(i′−1). Now let i′′≤i′−1 be the minimal integer with the
property that for all i′′≤i≤i′−1, we have that Zj(i)∈Ij(i). ThenZj(i′′−1)/∈Ij(i′′−1)
and by the minimality of i′we must have that Zj(i′′−1)< nyj(ti′′−1). By once again using
the fact that yjisB-Lipschitz,
Zj(i′′)≤Zj(i′′−1)+β < ny j(ti′′−1)+β≤nyj(ti′′)+β+B. (12)
Now, since Zj(i′)> nyj(ti′)+g(ti′), we can apply (12) to get that
Sj(i′)−Sj(i′′) = (Zj(i′)−nyj(ti′)−g(ti′))−(Zj(i′′)−nyj(ti′′)−g(ti′′))
> g(ti′′)−β−B
≥3λ−β−B. (13)
Intuitively, (13) says that Sj(i) increases signiﬁcantly between steps i′′andi′. We will now
argue that E[∆Sj(i)| Hi] is nonpositive between steps i′′andi′. Informally, by scaling by
nand interpreting E[∆Sj(i)| Hi] as the “derivative” of Sj(i)/nevaluated at i/n, this will
allow us to derive a contradiction in an analogous way as in the ﬁnal par t of the proof of
Theorem 2.
Observe ﬁrst that for each i′′≤i≤i′−1, we have that
E[∆Sj(i)| Hi] =E[∆Zj(i)| Hi]−∆nyj(t)−∆g(t)
≤fj(t,Z1(i)/n,...Z a(i)/n)+δ
−fj(t,y1(t),...,y a(t))+(L+BL)n−1−n−1g′(t) (14)
where the ﬁrst line is by deﬁnition and line (14) will now be justiﬁed. The ﬁrst two terms
follow since by Condition 2, ( t,Z1(i)/n,...Z a(i)/n)∈ D, and
E[∆Zj(i)| Hi]≤fj(t,Z1(i)/n,...Z a(i)/n)+δ.
10For the third and fourth terms of (14), note that
∆nyj(t) =n[yj(ti+1)−yj(ti)] =n/integraldisplayti+1
tiy′
j(τ)dτ
=n/integraldisplayti+1
tifj(τ,y1(τ),...,y a(τ))dτ
≥n/integraldisplayti+1
tifj(t,y1(ti),...,y a(ti))−(L+BL)n−1dτ(15)
=fj(t,y1(ti),...,y a(ti))−(L+BL)n−1,
where (15) follows since for all τ∈[ti,ti+1] we have |yj(τ)−yj(ti)| ≤B|τ−ti| ≤Bn−1(since
theyjareB-Lipschitz), and now since the fjareL-Lipschitz, we have
|fj(τ,y1(τ),...,y a(τ))−fj(t,y1(ti),...,y a(ti))| ≤L·max{|τ−ti|,Bn−1} ≤L(1+B)n−1.
For the last term of (14), we have that
∆g(t) = 3λ/parenleftbig
e2Lti+1−e2Lti/parenrightbig
= 3λe2Lti/parenleftbig
e2L/n−1/parenrightbig
≥3λe2Lti/parenleftbigg2L
n/parenrightbigg
=n−1g′(t).
Observe now that by cooperativity, fj(t,Z1(i)/n,...Z a(i)/n) is upper bounded by
fj/parenleftbigg
t,ny1(t)+g(t)
n,...,nyj−1(t)+g(t)
n,Zj(i)
n,nyj+1(t)+g(t)
n,...,nya(t)+g(t)
n/parenrightbigg
.(16)
Now, since Zj(i)∈Ij(i), we can apply the Lipschitzness of fjto (16) to get that
fj(t,Z1(i)/n,...Z a(i)/n)≤fj(t,y1(t),...,y a(t))+Lg(t)/n. (17)
As such, applied to (14),
E[∆Sj(i)| Hi]≤fj(t,Z1(i)/n,...Z a(i)/n)+δ−fj(t,y1(t),...,y a(t))+(L+BL)n−1−n−1g′(t)
≤Ln−1g(t)+δ−n−1g′(t)+(L+BL)n−1
=Ln−1g(t)+δ−n−12Lg(t)+(L+BL)n−1
≤ −[Lg(t)−(L+BL+δn)]n−1
≤ −[3Lλ−(L+BL+δn)]n−1(18)
≤0, (19)
where the ﬁnal line follows since λ≥L+BL+δn
3L.
Therefore, for i′′≤i≤i′−1 we have that
0≥E[∆Sj(i)| Hi] =E[∆Xj(i)| Hi]+E[∆Mj(i)| Hi] = ∆Xj(i)
since (Mj(i))m
i=0is a martingale and ∆ Xj(i) isHi-measurable. In particular,
Xj(i′)≤Xj(i′′). (20)
11At this point, we use the event we assumed regarding ( Mj(i))m
i=0(directly following (11)) to
get that
Mj(i′)−Mj(i′′)≤ |Mj(i′)−Mj(0)|+|Mj(i′′)−Mj(0)| ≤2λ. (21)
But now we can derive our ﬁnal contradiction (explanation follows):
3λ−β−B < S j(i′)−Sj(i′′)
=Xj(i′)−Xj(i′′)+Mj(i′)−Mj(i′′)
≤2λ.
Indeed the ﬁrst line is from (13), the second is by the Doob decompo sition, and the last line
follows from (20) and (21). But the last line is a contradiction since we choseλ≥β+B.
Remark 9. We can relax several of the Conditions a)-c) on the fjand Conditions 1. and
2. of Theorem 2 and the conclusion still holds (in particular, the proo f we gave still goes
through). We will list some possible relaxations below.
•Condition a): We only use this condition on (15) and (17), and in both s ituations
we are applying it at points ( t,z1,...,z a) which are very close to the trajectory. In
particular, instead of assuming that fjisL-Lipschitz on all of D, it suﬃces to have
thatfjisL-Lipschitz on the set of points
D∗:=/braceleftBig
(t,z1,...,z a)∈Ra+1: 0≤t≤σ,yj′(t)≤zj′≤yj′(t)+g(t) for 1≤j′≤a/bracerightBig
⊆ D.
•Condition b): We only use this condition to conclude that the system ( 6) has a unique
solution and that those solutions yjareB-Lipschitz. So, instead of Condition b) it
suﬃces to just check directly that (6) has a unique solution that is B-Lipschitz.
•Condition c): We only use the cooperativity of the functions to get lin e (16). In this
situation, for our point ( t,z1,...,z a) somezjis in its critical interval (and all the zi
are below their upper bounds). In particular, instead of assuming t he full Condition
c), for (16) it suﬃces to have that fj(t,z1,...za) is upper bounded by
fj/parenleftbigg
t,ny1(t)+g(t)
n,...,nyj−1(t)+g(t)
n,zj,nyj+1(t)+g(t)
n,...,nya(t)+g(t)
n/parenrightbigg
whenever zj′≤yj′(t)+g(t)/nfor allj′andzj≥yj(t).
•Condition 2. : We only use this condition to get (14). In this situation w e have
that one of the Zjis in its critical interval (and all the Ziare below their upper
bounds). Instead of Condition 2. it suﬃces to have the following: fo r each 1 ≤
j≤a. IfZj′(i)≤nyj′(i/n) +g(i/n) for all 1 ≤j′≤aandZj≥nyj(i/n), then
(i/n,Z 1(i)/n,...Z a(i)/n)∈ Dand
E[∆Zj(i)| Hi]≤fj(i/n,Z 1(i)/n,...Z a(i)/n)+δ.
124 Recovering a Version of Wormald’s Theorem
In this section we recover the standard (two-sided) diﬀerential e quation method of Wormald
[14]. ThestatementresemblestherecentversiongivenbyWarnke[1 3]inthesensethatitdoes
not use any asymptotic notation and instead gives explicit bounds fo r error estimates and
failure probabilities. Like Warnke’s proof, ours has a probabilistic part and adeterministic
part. Our probabilistic part is much the same as Warnke’s in that we apply a d eviation
inequality (though we use Freedman’s theorem rather than the Azu ma-Hoeﬀding inequality)
to the martingale part of a Doob decomposition. That being said, the deterministic part of
our argument is quite diﬀerent than the deterministic part of Warnk e’s argument. In fact,
we were not able to adapt Warnke’s argument to the one-sided sett ing.
Givena∈N, suppose that D ⊆Ra+1is a bounded domain, and for 1 ≤j≤a, let
fj:D →R. We assume that the following hold for each j:
1.fjisL-Lipschitz, and
2.|fj| ≤BonD.
Given (0,˜y1,...,˜ya)∈ D, assume that y1(t),...,y a(t) is the (unique) solution to the system
y′
j(t) =fj(t,y1(t),...,y a(t)), y j(0) = ˜yj.
for allt∈[0,σ] whereσis a positive value. Note that unlike in Theorem 3, we make a
further assumption involving σbelow in Theorem 10.
Theorem 10. Suppose for each 1≤j≤awe have a sequence of random variables (Yj(i))∞
i=0
which is adapted to the ﬁltration (Hi)∞
i=0. Letn∈N, andβ,b,λ,δ > 0be any parameters such
thatλ≥L+BL+δn
3L. Moreover, assume that σ >0is any value such that (t,y1(t),...,y a(t))
hasℓ∞-distance at least 3λe2Lt/nfrom the boundary of Dfor allt∈[0,σ). Given an arbitrary
stopping time I≥0adapted to (Hi)∞
i=0, suppose that the following properties hold for each
0≤i <min{I,σn}:
1. The ‘Boundedness Hypothesis’: maxj|∆Yj(i)| ≤β, andmaxjE[(∆Yj(i))2| Hi]≤b.
2. The ‘Trend Hypothesis’: If (i/n,Y1(i)/n,...Y a(i)/n)∈ D, then
/vextendsingle/vextendsingle/vextendsingleE[∆Yj(i)| Hi]−fj(i/n,Y1(i)/n,...Y a(i)/n)/vextendsingle/vextendsingle/vextendsingle≤δ
for each 1≤j≤a.
3. The ‘Initial Condition’: |Yj(0)−yj(0)n| ≤λfor all1≤j≤a.
Then, with probability at least 1−2aexp/parenleftBig
−λ2
2(bσn+2βλ)/parenrightBig
,
|Yj(i)−nyj(i/n)| ≤3λe2Li/n
for all1≤j≤aand0≤i≤min{I,σn}.
13We conclude the section with an extension of Theorem 10 analogous t o Corollary 7 of
Theorem 3. We omit the proof, as it follows almost identically to the pro of of Corollary 7.
Corollary 11 (of Theorem 10) .Suppose that in the terminology of Theorem 10, Conditions
1. and 2. are only veriﬁed for each 0≤i≤min{I,σn}which satisﬁes |Yj′(i)−nyj′(i/n)| ≤
3λe2Li/nfor all1≤j′≤a. In this case, the conclusion of Theorem 10 still holds. I.e. , with
probability at least 1−2aexp/parenleftBig
−λ2
2(bσn+2βλ)/parenrightBig
,
|Yj(i)−nyj(i/n)| ≤3λe2Li/n
for all1≤j≤aand0≤i≤min{I,σn}.
Proof of Theorem 10. Fix 0≤i≤σn, and again set m:=σn,t=ti=i/n, andg(t) :=
3λe2Ltfor convenience. Deﬁne
S±
j(i) :=Yj(i)−(nyj(t)±g(t)),
X±
j(i) :=i−1/summationdisplay
k=0E[∆S±
j(k)| Hk],
M±
j(i) :=S±
j(0)+i−1/summationdisplay
k=0/parenleftbig
∆S±
j(k)−E[∆S±
j(k)| Hk]/parenrightbig
so that (X±
j(i)+M±
j(i))m
i=0is the Doob decomposition of ( S±
j(i))m
i=0. Note that
∆S±
j(k)−E[∆S±
j(k)| Hk] = ∆Yj(k)−E[∆Yj(k)| Hk],
and soM+
j(i) is almost the same as M−
j(i). More precisely, we have
M±
j(i) =Mj(i)∓g(0)
where
Mj(i) :=Yj(0)−nyj(0)+i−1/summationdisplay
k=0(∆Yj(k)−E[∆Yj(k)| Hk])
(which is also a martingale). As in the proof of Theorem 3, we have |∆Mj(i)| ≤2βand
Var[∆Mj(i)| Hi]≤b. Now, by Theorem 8 we have that
P/parenleftBig
∃0≤j≤a,0≤i≤msuch that |Mj(i)−Mj(0)| ≥λ/parenrightBig
≤2aexp/parenleftbigg
−λ2
2(bm+2βλ)/parenrightbigg
.
(22)
Suppose that the event above does not happen, so for all 0 ≤j≤a,0≤i≤mwe have that
|Mj(i)−Mj(0)|< λ. We will show that we also have |Yj(i)−nyj(t)| ≤g(t) for all 0 ≤i≤m.
Note that yjisB-Lipschitz as before. Deﬁne the critical interval
Ij(i) := [nyj(t)−g(t),nyj(t)+g(t)].
Suppose for the sake of contradiction that i′is minimal with 0 ≤i′≤mandYj(i′)/∈Ij(i′)
for some j. We will consider the case where Yj(i′)> nyj(t)+g(t) (the case where Yj(i′)<
14nyj(t)−g(t) ishandledsimilarly withsomeinequalities reversed). Inotherwords, S+
j(i′)>0.
First observe that since g(0) := 3λ, Condition 3. implies S+
j(0)≤ −2λ. In particular, i′>0
and
S+
j(i′)−S+
j(0)>2λ. (23)
For 0≤i < i′, we have (explanation follows)
E[∆S+
j(i)| Hi] =E[∆Yj(i)| Hi]−∆nyj(t)−∆g(t)
≤fj(t,Y1(i)/n,...Y a(i)/n)+δ−fj(t,y1(t),...,y a(t))+(L+BL)n−1−n−1g′(t)
≤Ln−1g(t)+δ+(L+BL)n−1−n−1g′(t)
≤ −[3Lλ−(L+BL+δn)]n−1
≤0.
Indeed, the ﬁrst line is by deﬁnition and the second line follows just like (14), with the
minor caveat that we now must show that ( t,Y1(i)/n,...,Y a(i)/n) is within the domain
Dto apply Condition 2 (recall that in Theorem 3 this is simply assumed). O bserve that
by the deﬁnition of σ, (t,y1(t),...,y a(t)) is inDandat leastℓ∞-distance g(t)/nfrom the
boundary of D. On the other hand, since Yj′(i)∈Ij′(i) for all 1 ≤j′≤a, we know that
|Yj′(i)/n−yj′(t)| ≤g(t)/nfor all 1≤j′≤a. Thus, ( t,Y1(i)/n,...Y a(i)/n)∈ D, and so
E[∆Yj(i)| Hi]≤fj(t,Y1(i)/n,...,Y a(i)/n)+δ
by Condition 2. The remaining justiﬁcation is much the same as before . Since fjis
L-Lipschitz and |Yj′(i)−nyj′(t)| ≤g(t) for all j′, we have fj(t,Y1(i)/n,...Y a(i)/n)≤
fj(t,y1(t),...,y a(t))+Ln−1g(t) and the third line follows. The fourth and ﬁfth lines follow
just as (18) and (19). Therefore, for 0 ≤i < i′we have that
0≥E[∆S+
j(i)| Hi] =E[∆X+
j(i)| Hi]+E[∆M+
j(i)| Hi] = ∆X+
j(i)
since (M+
j(i))∞
i=0is a martingale and ∆ X+
j(i) isHi-measurable. In particular
X+
j(i′)≤X+
j(0). (24)
But now we can derive our ﬁnal contradiction (explanation follows):
2λ < S+
j(i′)−S+
j(0)
=X+
j(i′)−X+
j(0)+Mj(i′)−Mj(0)
≤λ.
Indeed, the ﬁrst line is from (23), the second line is by the Doob deco mposition, and the last
follows from (24) and our assumption that the event described on lin e (22) does not happen.
Of course 2 λ < λis a contradiction and we are done.
References
[1] Patrick Bennett and Andrzej Dudek. A gentle introduction to th e diﬀerential equation
method and dynamic concentration. Discrete Math. , 345(12):Paper No. 113071, 17,
2022.
15[2] Tom Bohman, Alan Frieze, and Eyal Lubetzky. Random triangle re moval.Adv. Math. ,
280:379–438, 2015.
[3] Elaheh Fata, Will Ma, and David Simchi-Levi. Multi-stage and multi-cu stomer assort-
ment optimization with inventory constraints. CoRR, abs/1908.09808, 2019.
[4] Moran Feldman, Ola Svensson, and Rico Zenklusen. Online content ion resolution
schemes with applications to bayesian selection problems. SIAM Journal on Computing ,
50(2):255–300, 2021.
[5] David A. Freedman. On Tail Probabilities for Martingales. The Annals of Probability ,
3(1):100 – 118, 1975.
[6] Hu Fu, Zhihao Gavin Tang, Hongxun Wu, Jinzhao Wu, and Qianfan Zh ang. Random
Order Vertex Arrival Contention Resolution Schemes for Matching , with Applications.
In Nikhil Bansal, Emanuela Merelli, and James Worrell, editors, 48th International
Colloquium on Automata, Languages, and Programming (ICALP 2021), volume 198 of
Leibniz International Proceedings in Informatics (LIPIcs ), pages 68:1–68:20, Dagstuhl,
Germany, 2021. Schloss Dagstuhl – Leibniz-Zentrum f¨ ur Inform atik.
[7] Euiwoong Lee and Sahil Singla. Optimal Online Contention Resolution Schemes via
Ex-Ante Prophet Inequalities. In Yossi Azar, Hannah Bast, and G rzegorz Herman,
editors,26th Annual European Symposium on Algorithms (ESA 2018) , volume 112 of
Leibniz International Proceedings in Informatics (LIPIcs ), pages 57:1–57:14, Dagstuhl,
Germany, 2018. Schloss Dagstuhl–Leibniz-Zentrum fuer Informa tik.
[8] Calum MacRury and Will Ma. Random-order contention resolution v ia continuous
induction: Tightness for bipartite matching under vertex arrivals. In Bojan Mohar, Igor
Shinkar, and Ryan O’Donnell, editors, Proceedings of the 56th Annual ACM Symposium
on Theory of Computing, STOC 2024, Vancouver, BC, Canada, Ju ne 24-28, 2024 ,
pages 1629–1640. ACM, 2024.
[9] Calum MacRury, Will Ma, and Nathaniel Grammel. On (random-orde r) online con-
tention resolution schemes for the matching polytope of (bipartite ) graphs. Operations
Research 0(0) , 2024.
[10] M. Michel Petrovitch. Sur une mani` ere d’´ etendre le th´ eor ` eme de la moyenne aux
´ equations diﬀ´ erentielles du premier ordre. Mathematische Annalen , 54:417–436, 1901.
[11] Andr´ as Telcs, Nicholas Wormald, and Sanming Zhou. Hamiltonicity of random graphs
produced by 2-processes. Random Structures Algorithms , 31(4):450–481, 2007.
[12] Wolfgang Walter. Diﬀerential inequalities and maximum principles: t heory, new meth-
ods and applications. In Proceedings of the Second World Congress of Nonlinear Ana-
lysts, Part 8 (Athens, 1996) , volume 30, pages 4695–4711, 1997.
[13] Lutz Warnke. On wormald’s diﬀerential equation method. Accepted to Combinatorics,
Probability, and Computing , abs/1905.08928, 2019.
16[14] Nicholas C. Wormald. Diﬀerential Equations for Random Process es and Random
Graphs. The Annals of Applied Probability , 5(4):1217 – 1235, 1995.
17