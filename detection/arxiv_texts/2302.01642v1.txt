Cluster-CAM: Cluster-Weighted Visual Interpretation of CNNs‚Äô
Decision in Image ClassiÔ¨Åcation
Zhenpeng Fenga, Hongbing Jia,<, Milo≈° Dakovi/uni0107b, Xiyang Cuia, Mingzhe Zhuaand
Ljubi≈°a Stankovi/uni0107b
aSchool of Electronic Engineering, Xidian University, Xi‚Äôan, China
bFaculty of Electrical Engineering, University of Montenegro, Podgorica, Montenegro
ARTICLE INFO
Keywords :
keywords-1explainable artiÔ¨Åcial intel-
ligence
keywords-2 class activation mapping
keywords-3 clustering algorithm
keywords-4 image classiÔ¨ÅcationABSTRACT
Despite the tremendous success of convolutional neural networks (CNNs) in computer vision, the
mechanism of CNNs still lacks clear interpretation. Currently, class activation mapping (CAM), a
famous visualization technique to interpret CNN‚Äôs decision, has drawn increasing attention. Gradient-
basedCAMsareeÔ¨ÉcientwhiletheperformanceisheavilyaÔ¨Äectedbygradientvanishingandexploding.
In contrast, gradient-free CAMs can avoid computing gradients to produce more understandable
results. However, existing gradient-free CAMs are quite time-consuming because hundreds of forward
interferenceperimagearerequired. Inthispaper,weproposedCluster-CAM,aneÔ¨ÄectiveandeÔ¨Écient
gradient-freeCNNinterpretationalgorithm. Cluster-CAMcansigniÔ¨Åcantlyreducethetimesofforward
propagationbysplittingthefeaturemapsintoclustersinanunsupervisedmanner. Furthermore,we
propose an artful strategy to forge a cognition-base map and cognition-scissors from clustered feature
maps. The Ô¨Ånal salience heatmap will be computed by merging the above cognition maps. Qualitative
resultsconspicuouslyshowthatCluster-CAMcanproduceheatmapswherethehighlightedregions
match the human‚Äôs cognition more precisely than existing CAMs. The quantitative evaluation further
demonstrates the superiority of Cluster-CAM in both eÔ¨Äectiveness and eÔ¨Éciency.
1. Introduction
Convolutional neural networks (CNNs) have provided
a basis for numerous remarkable achievements in various
computer vision tasks like, for example, image classiÔ¨Åcation
[9,8,27,13], object detection [ 19,43,1,12], and seman-
tic segmentation [ 14,11,3]. Despite CNNs‚Äô extraordinary
performance, they still lack a clear interpretation of the in-
ner mechanism [ 10,42,21]. This lack of transparency can
indeedbeadisqualifyingfactorinsomepeculiarscenarios
wheremistakesininterpretationcanjeopardizehumanlife
and health, like in medical image processing or autonomous
vehicles [ 20,38,34,32]. Therefore, it is highly desirable
toÔ¨ÅndawaytounderstandandexplainwhatexactlyCNNs
have learned during the training process [26, 31, 16].
Recently, Class Activation Mapping (CAM), a visual in-
terpretation technique, has drawn increasing attention [ 30,
33]. CAMaimsathighlightingsalience regionsofaninput
imageforCNN‚Äôsdecisionusingalinearlyweightedcombina-
tionoffeaturemaps. VanillaCAMdirectlyutilizestheweight
of each feature map after global average pooling (GAP) cor-
respondingtothetargetclass,soitisonlyavailableforCNNs
with GAP [ 40]. To further extend CAM to more complex
CNN structures, numerous modiÔ¨Åed CAMs are proposed
and they can be broadly categorized as: 1) gradient-based
CAMs,and2)gradient-freeCAMs. Gradient-basedCAMs
(e.g. Grad-CAM[ 23],Grad-CAM++[ 2],SmoothCAM++
[17],XGrad-CAM[ 7]etc.) deÔ¨Ånetheweightsofeachfeature
map using the average of partial gradient of the predicted
scorewithrespecttothefeaturemaps. Gradient-basedCAMs
<Corresponding author: Hongbing Ji
hbji@xidian.edu.cn ( Hongbing Ji)
ORCID(s):areusuallycomputedeÔ¨Éciently. However,theirweightslack
reasonableexplanationandareeasilyimpactedbygradient
exploding or vanishing. To address this limitation, some
gradient-freeCAMsareproposed. TheydeÔ¨Åneanintuitive
impact of each feature map on the predicted score instead of
using the gradient. Examples of gradient-free CAMs are the
Ablation CAM [ 18] and the Score-CAM [ 35]. Gradient-free
CAMscanprovideamoreexplainableweightdeÔ¨Ånitionthan
gradient-basedCAMsinmostcases. Fig.1showsthesalience
heatmaps produced by several aforementioned CAMs.
Althoughgradient-freeCAMsdeÔ¨Ånetheweightsmore
reasonably, they are usually very time-consuming since hun-
dreds of forward propagations per image are required. To
improvetheeÔ¨Éciencyofgradient-freeCAMs,Q.Zhanget
al. proposedGroup-CAMwherefeaturemapsaresplitinto
several groups [ 37]. In this case, only several forward propa-
gationsareneededincomputingtheweights. Nonetheless,
thefeaturemapsaresplitwithoutanyregulationintheGroup-
CAM. Actually, various feature maps have learned diÔ¨Äerent
semantic concepts relevant/irrelevant to the object. There-
fore, the feature maps should be split into groups/clusters.
Those with similar semantics should be assigned to the same
group. Z.Fengetal. proposedSC-SMCAMusingspectral
clusteringtoaccomplish thisgoal,particularlyfor synthetic
apertureradar(SAR)images[ 5]. However,nofurtheranal-
ysisormodiÔ¨ÅcationofweightsismentionedintheSC-SM
CAM [6, 5].
Inthispaper,weproposeaCluster-CAM,aneÔ¨Äectiveand
eÔ¨Écient gradient-free CAM, based on unsupervised cluster-
ing. In Cluster-CAM, an unsupervised clustering technique,
K-means/spectralclusteringisadoptedtosplitfeaturemaps
intoseveralclusters. Subsequently,weprovideanartfulstrat-
Z. Feng et al.: Preprint submitted to Elsevier Page 1 of 10arXiv:2302.01642v1  [cs.CV]  3 Feb 2023egytomergethosefeaturemapsintoacognition-basemap
and a cognition-scissors map which will be combined as the
Ô¨Ånal salience heatmap. The highlights of this paper are as
follows:
‚Ä¢We propose a Cluster-CAM, as the Ô¨Årst attempt to
provide a cluster-weighted CAM framework via unsu-
pervised clustering based on optical images.
‚Ä¢We provide a novel and artful weight-forming strat-
egy to merge the cognition-base map and cognition-
scissors map. These two maps greatly match the hu-
man‚Äôs cognition and intuition, thus the weights are
completely reasonable and understandable.
‚Ä¢Cluster-CAMiseÔ¨ÄectiveandeÔ¨Écient,whichoutper-
forms existing gradient-free CAMs in performance in
most cases with signiÔ¨Åcantly lower computing costs.
Therestofthispaperisorganizedasfollows. Section2
introduces the basic knowledge of various CAMs. Section 3
elaborates on how to generate salience heatmaps by Cluster-
CAM. In Section 4, various experiments are implemented to
demonstratethevalidityofCluster-CAMandfurtheranalyze
the experimental results from various aspects. Section 5
concludes this paper.
2. Related Work
As discussed in Section 1, the key issue in interpreting
CNN‚Äôsdecisionistoexplainwhattheneuralnetworklearned
toÔ¨Ånishareasonableinference[ 42,10]. Tovisualizewhat
CNN focuses on the input image, numerous interpretation
algorithmsareproposed[ 36,24,40,39],amongwhichClass
Activation Mapping draws the most increasing attention due
to its simplicity and good performance.
Vanilla CAM: B. Zhou et al. Ô¨Årstly proposed the vanilla
CAMtoproduceasalienceheatmapbyalinearlyweighted
combinationoffeaturemaps, Fn,withelements Fn.i;j/; n=
1;2;:::;N, at the target convolutional layer before classiÔ¨Åca-
tion [40]
Mc.i;j/ =√â
nc
nFn.i;j/ (1)
Sc=√â
n!c
n√â
i;jFn.i;j/ =√â
nc
n√â
i;jFn.i;j/;(2)
whereMc.i;j/is a heat-map and Scdenotes the predicted
score for the target class c. Thus,care deÔ¨Åned by the
weights,!c,ofeachfeaturemapcorrespondingto c-thunit
in the classiÔ¨Åcation layer. Therefore, CAM is only available
with CNNs with global-average pooling following the last
convolutional layer. To extend CAM to all CNNs, many
modiÔ¨Åed CAMs are further proposed by manipulating the
deÔ¨Ånition of weights, which are generally categorized as: 1)
gradient-based CAMs; 2) gradient-free CAMs.
2.1. Gradient-based Class Activation Mapping
Gradient-basedCAM: Selvarajuetal. proposedGrad-CAM
to visualize any classiÔ¨Åcation CNN architectures by weight-
ing the feature maps in a certain convolutional layer withthe gradients of the predicted score, scwith respect to the
elements ofFn[23], as
c;Grad
n=√â
i;j)sc
)Fn.i;j/; (3)
where diÔ¨Äerent from (2), scis a sparse vector whose ele-
ments are zeros except the c-th element, which is equal to
Sc. However, the highlighted regions generated by Grad-
CAMareusuallymuchsmallerthantheobject. Toprovide
aprecisehighlightedregion,somefurthermodiÔ¨ÅedCAMs
are proposed, like Grad-CAM++[ 2]. A. Chattopadhyay et
al proposed Grad-CAM++ which can produce more pre-
cise highlighted locality. Grad-CAM++ assumes diÔ¨Äerent
elements in thegradientmatrix should have diÔ¨Äerent contri-
butions to features maps, thus an extra factor is introduced
to realize this assumption using higher order partial gradient,
as:
c;Grad ++
n=)2sc
).Fn.i;j//2
2)2sc
).Fn.i;j//2+¬≥
a;bFn.a;b/)3sc
.Fn.i;j//3√â
i;j)sc
)Fn.i;j/:
(4)
However, the gradient,)sc
)Fn, is usually heavily noised or som-
times even all-zero. It is probably because 1) CNN is trained
tolearnageneralizedcapabilitytoclassifyageneralconcept
rather than a speciÔ¨Åc object. 2) some unreasonable phe-
nomena emerged in CNN‚Äôs training, like gradient vanishing
andgradientexploding. D.Omeizaetal. proposedSmooth
Grad-CAM++ to further suppress the noise. The weights of
SmoothGrad-CAM++aredeÔ¨Åned usingthe averageof the
gradients as:
c;SmoothGrad ++
n= (5)
1
m¬≥M
m=1Dn
1
21
m¬≥M
m=1Dn
2+¬≥
a;bFn.a;b/1
m¬≥M
m=1Dn
3.1
mM√â
m=1Dn
1/
whereDn
1=¬≥
i;j)sc
)Fn.i;j/,Dn
2=¬≥
i;j)2sc
).Fn.i;j//2, andDn
3=
¬≥
i;j)3sc
).Fn.i;j//3whentheinputisaddedwithrandomnoisefor
Mtimes (Mis a constant integer). This smoothing strategy
is intuitive but still rough for some complex CNN structures.
Tofurtherenhancetheinterpretabilityofweights,R.Fuet
al. proposed XGrad-CAM by introducing two completely
explainable axioms to form the weight:
c;XGrad
n=√â
i;jFn.i;j/¬≥
i;jFn.i;j/)sc
)Fn.i;j/: (6)
Note neither Smooth Grad-CAM++ nor XGrad CAM can
guaranteecompletelyavoidingtheaboveunreasonablephe-
nomena in gradient computing.
2.2. Gradient-free Class Activation Mapping
Gradient-free CAM: To completely solve the problems re-
sultingfromgradientcomputing,somegradient-freeCAMs
Z. Feng et al.: Preprint submitted to Elsevier Page 2 of 10Input image
 Grad
 Grad++
 Ablation
 Score
 Ours
Figure 1: The heatmaps are produced by diÔ¨Äerent CAMs. The Ô¨Årst column is input images
(indigo Ô¨Ånch, eagle, rooster, and ostrich from top to bottom). The second to Ô¨Åfth columns
are heatmaps produced by Grad-CAM, Grad-CAM++, Ablation-CAM, Score-CAM, and
Cluster-CAM.
are proposed, i.e., Ablation-CAM [ 18] and Score-CAM [ 35],
to form the weights using the impact of each feature map on
the predicted score instead of using gradient. In Ablation-
CAM, the weights are deÔ¨Åned as:
c;Ablation
n=Sc*Sc;n
Sc; (7)
whereSc;ndenotes the predicted score for class cwhenn-th
feature map is set to zero. In this case, a large weight will be
assignedtothecurrentfeaturemapiftheremovalofitcan
lead to a dramatic drop in the predicted score ( Sc*Sc;nis a
large value) and vice versa. The authors argue that Ablation-
CAM is immune to both saturation which marks a Ô¨Ålter as
important althoughit isnot important,and explosion which
marksaÔ¨ÅlterthathasaverysmallinÔ¨Çuenceashavinghigh
importance. DiÔ¨ÄerentfromAblation-CAM,Score-CAMcon-
sidersmeasuringtheimpactofthefeaturemapbyintroducing
the input image, X, as
c;Score
n=Sc.X√ΩHn/ *Sc.Xb/ (8)
Hn=s.Up.Fn//; (9)
where√Ωdenotes the element-wise multiplication, Xbis a
baseline image which can be set the input image itself, Up./
denotestheoperationthatupsamples Fnintotheinputsize
ands./isanormalizationfunctionthatmapseachelement
in the input matrix into [ 0,1].X√ΩHncan be deemed as
Ô¨Ålteringwhichonlypasseselementsin XmaskedbyHn,thusa large weight will be assigned if most target-discriminative
are preserved by the current feature map, i.e. f.X√ΩHn/is
higherthanSc.Xb/andviceversa. Currently,gradient-free
CAMshavedrawnmoreattentionthangradient-basedCAMs
due to their superior performance and explainable deÔ¨Ånition
of weights. However, gradient-free CAMs are much more
time-consuming than gradient-based CAMs because hun-
dreds or even thousands of forward interference are required
whilethosegradient-basedCAMsonlyrequireoneforward
interference.
3. Methodology
Inthissection,wewillÔ¨Årstintroducesomebasicconcepts
on graph-based spectral clustering and K-means. Then we
present the detailed procedures of Cluster-CAM.
3.1. Spectral Clustering and K-means
Spectral clustering is a widely-used unsupervised cluster-
ingalgorithmbasedongraphsignalprocessing[ 28,29,15,
22]. SpeciÔ¨Åcally,theprocesseddata(featuremaps, Fn)are
regarded as vertices in a graph topology. Then the elements,
S.i;j/, of the similarity matrix, S, can be deÔ¨Åned:
S.i;j/ = similarity. Fi;Fj/; (10)
where similarity./refers to a function that measures the
similarity between two vertices (feature maps). If we use the
structural similarity index (SSIM), then it ranges from 0(no
Z. Feng et al.: Preprint submitted to Elsevier Page 3 of 10Classifier
Convolution layers
input image
feature maps clustered feature mapsclusteringrepresentative feature maps
upsamplin g
masked features
cognition -base and cognition -scissors
saliency heatmap
‚Ä¶
0.019 0.491 0.001 0.006 0.308element -wise 
product
cognition couplingFigure 2: The Ô¨Çowchart of Cluster-CAM.
similarity) to 1(identical feature maps). The elements of the
weighted adjacency matrix, A, can be deÔ¨Åned as:
<
A.i;j/ = exp.*.1 * S.i;j//_/;ifS.i;j/>;
A.i;j/ = 0; else,
(11)
whereis a threshold to keep the direct edge in the cor-
responding graph for two neighboring vertices and is a
parameter. Note that, by deÔ¨Ånition of similarity, this adja-
cencymatrixisasymmetricmatrixresultinginanundirected
graph, that is, A.i;j/ =A.j;i/.
ThesimilaritycanbedeÔ¨ÅnedusingthediÔ¨Äerencebetween
two vertices (feature maps), d.i;j/ =√∞√∞Fi*Fi√∞√∞. Then the
weighted adjacency matrix is deÔ¨Åned by
<
A.i;j/ = exp.*d2.i;j/_2/;ifS.i;j/>;
A.i;j/ = 0; else,(12)
whereandhave the same role as in (11).
In order to produce the vectors for spectral clustering,
nowwecontinueandcomputethegraphLaplacianmatrix,
L, as
L=D*A (13)
whereD.i;i/ =¬≥
jA.i;j/are the elements of the degree
matrixDwhich is diagonal.
In practice, the graph Laplacian matrix usually can be
normalized, as
LN=D*1
2LD*1
2=I*D*1
2AD*1
2: (14)
The clustering results obtained using these two matrices are
very similar.The eigendecomposition of the graph Laplacian
L=UTU: (15)
Resultsineigenvectors u1,u2,5,uNthatarethecolumnsof
matrixU. The smoothness index of these vectors is equal to
thecorrespondingeigenvalue i. Inclusteringthedatainto
twoclustersonlytheeigenvector u2isused(Fiedlervector)
since the vector u1is omitted as its elements are constant.
If we want to get a few clusters ( Qclusters) then we can
useKthesmoothesteigenvectors, u2,u35,uK+1,written
in the matrix form as
B=u2u35uK+1=b
f
f
fdu12u135u1.K+1/
u22u235u2.K+1/
4 4 7 4
uN2uN35uN.K+1/c
g
g
ge;
(16)
whereNfeatures with Kdimension are considered.
The clusters are determined based on the K-dimensional
spectralsimilarityvectors, q1= [u12; u13;¬ß;u1.K+1/],q2=
[u22; u23;¬ß;u2.K+1/],¬ß,qN= [uN2; uN3;¬ß;uN.K+1/],
deÔ¨Åned for vertices (features F1,F2,¬ß,FN).
Inthisway,thedimensionofthemeasuringdistanceis
signiÔ¨Åcantly reduced from the original Ndimensional space
ind.i;j/ =√∞√∞Fi*Fi√∞√∞toaverylow K-dimensionalspaces
of spectral vectors qn.
Finally, the clustering result (the data grouped into Q
clusters) can be reÔ¨Åned using K-means and the Euclidean
distanced.i;j/ =√∞√∞Fi*Fi√∞√∞.
Note that the traditional K-means algorithm can be used
withaninitialrandomclusteringoffeaturemapsinto Qclus-
ters, with a slower convergence due to random initialization.
In this case, all the feature maps are grouped into Qinitial
Z. Feng et al.: Preprint submitted to Elsevier Page 4 of 10clusters, Qq,q= 1;2;¬ß;Q. Means of the feature maps
are calculated for each cluster, Mq= mean.Fn;n√ãQq/.
Thedistanceofeachfeaturemapischeckedwithrespectto
eachofthemean Mq. Thefeaturemapisreassignedtothe
cluster whose mean is the closest to the considered feature
map. After all feature maps are considered, the means are
recalculated for the new clusters. The procedure is repeated
until no feature map changes its cluster.
3.2. Cluster-CAM
Now we are ready to introduce spectral clustering and K-
means in Cluster-CAM. Here the feature maps, F, represent
the vertices in (10). Take Euclidean distance as similarity
measurement, (10) can be expressed as:
S.i;j/ = exp ^* √∞√∞Fi*Fj√∞√∞`; (17)
whereashorterdistancemeansahighersimilarity. Bysub-
stituting(17)into(11),(13),(14),and(16),wecansplit N
feature maps into Qclusters, Qq,q= 1;2;¬ß;Q,Q ~ N.
Thenwecanobtainthe Qrepresentativefeaturemaps, ¬ÉF=
[¬ÉF1;¬ÉF2;¬ß;¬ÉFQ], by calculating the mean of feature maps in
each cluster, as
¬ÉFq= mean^Fn; n√ãQq`; q= 1;5;Q: (18)
Next we obtain the Hadamard product of ¬ÉFandX(¬ÉF
will be upsampled to the same size of X). This processing
can be deemed as Ô¨Åltering that mainly passes those elements
corresponding to large values in ¬ÉF. The predicted score of
each masked image is computed as:
y=[y1;y2;¬ß;yQ]T
=[Sc;1.¬ÉF1√ΩX/;¬ß;Sc;Q.¬ÉFQ√ΩX/]: (19)
Inthiscase,wecanobtainthecognition-basemapandcog-
nition scissors as
¬ÉFbase=Fqmax; qmax= arg
qmax.y/ (20)
¬ÉFscissors =Fqmin; qmin= arg
qmin.y/: (21)
Next, we can semantically couple the cognition-base map
and cognition-scissors to form the salience heatmap, as:
HCluster=¬ÉFbase* .1 */¬ÉFscissors; (22)
where√ã [0;1]is a balance factor to adjust the importance
of cognition-base map and cognition-scissors.
4. Experiments
In this section, we will present and analyze the perfor-
mance of Cluster-CAM from various perspectives. Firstly
wewillbrieÔ¨Çydescribethedatasetusedinourexperiments.
ThenweverifythesuperiorityofCluster-CAMtootherex-
isting CAMs.4.1. Experimental Setup
Dataset: In the following experiments, CNNs are trained
on a prevalent benchmark, i.e., ILSVRC [ 4]. In ILSVRC,
there are around 1.2 million images with 1000categories for
training, and 50thousand images with 1000categories for
validation.
Network Structure: In this paper, several classic CNNs,
AlexNet[ 9]andVGG-16[ 25],areusedasclassiÔ¨Åcationmod-
els. Alex-Net is proposed by A. Krizhevsky et al., which
consists of 5convolutional units (a stack of convolutional
layers,ReLU,andmaxpooling)and 3fully-connectedlayers.
VGG-16 is a very deep CNN with 13convolutional layers
and3fully-connected layers. VGG-16 has approximately
134M trainable parameters regardless of the output layer.
4.2. Performance of Discriminative Localization
Fig. 1 shows the salience heatmaps of diÔ¨Äerent input im-
ages(indigoÔ¨Ånch,eagle,rooster,andostrich)byGrad-CAM,
Grad-CAM++, Ablation-CAM, Score-CAM, and Cluster
CAM.Visually,incomparisontoexistingCAMs,thehigh-
lighted region produced by Cluster-CAM mostly matches
human‚Äôs intuitive understanding of the discriminative part
ofthespeciÔ¨Åcobject. TaketheindigoÔ¨Ånchasanexample,
Grad-CAM only highlights the head of the bird, whereas
Grad-CAM++ and Ablation-CAM highlight the complete
Ô¨Ånch but the branch (object-irrelevant information) is also
included. Score-CAM and Cluster-CAM highlight the Ô¨Ånch
bodywithoutthebranch. Butobviously,theregionproduced
by Cluster-CAM matches the proÔ¨Åle of the Ô¨Ånch more pre-
cisely than Score-CAM.
4.3. CNN‚Äôs Cognitive Explanation
Cognition Analysis of Multi-objects Images: Images of
multiple objects are optimal samples to verify the rationality
of the cognition-base map and semantic-scissors in (20) and
(21). AswediscussedinSection3,areasonablecognition-
base map should incorporate the object-relevant informa-
tion as much as possible, while the corresponding cognition-
scissors should include such information as less as better.
Therefore,itisnecessarytocheckwhetherthecognition-base
mapandcognition-scissorscaninterchangeamulti-objects
imageifthe targetclassischanged toanotherobject. Fig.3
shows the cognition-base map and cognition-scissors of two
muti-objects images as well as the corresponding masked
images. There are two types of dogs in the Ô¨Årst image, i.e.
elkhound (the big gray dog on the left) and spaniel (the tiny
browndogontheright). Ifthetargetclassiselkhound,the
third and the fourth clustered feature map are cognition-base
mapandcognition-scissors,respectively(markedbygreen
and red squares). It matches human‚Äôs cognition because
cognition-base map incorporates both objects and cognition-
scissors only selects the spaniel, thus the highlighted region
willonlybeconcentratedontheelkhound,asshowninthe
Ô¨Årst row in the top-left subÔ¨Ågure in Fig. 3. When the tar-
get class is changed to the spaniel, the cognition-base map
and cognition-scissors are also interchanged, as shown in the
third row in the top-right subÔ¨Ågure in Fig. 3. The same
phenomenon also emerges in indigo Ô¨Ånch and goldÔ¨Ånch,
Z. Feng et al.: Preprint submitted to Elsevier Page 5 of 10elkhound
 Grad
 Grad++
 Ablation
 Score
 Ours
spaniel
 Grad
 Grad++
 Ablation
 Score
 Ours
0.140
 0.007
 0.261
 0.000
 0.051
 0.054
0.000
 0.000
 0.000
 0.140
 0.000
 0.000
indigo finch
 Grad
 Grad++
 Ablation
 Score
 Ours
goldfinch
 Grad
 Grad++
 Ablation
 Score
 Ours
/uni00000013/uni00000011/uni00000013/uni00000014/uni0000001c
 /uni00000013/uni00000011/uni00000017/uni0000001c/uni00000014
 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014
 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000019
 /uni00000013/uni00000011/uni00000016/uni00000013/uni0000001b
0.963
 0.004
 0.316
 0.823
 0.073Figure 3: The analysis of feature maps for images of multiple objects. The heatmaps
produced by diÔ¨Äerent CAMs for the target label elkhound (Ô¨Årst row in the top-left subÔ¨Ågure).
The clustered feature maps ( the second row in the top-left subÔ¨Ågure) and corresponding
masked images (the third row in the top-left subÔ¨Ågure). Note that the cognition-base map
and cognition-scissors are marked by green and red squares, respectively. The predicted
score for the current class is provided above each masked image. The results with the
target label spaniel are organized in the same structure (top-right subÔ¨Ågure). The indigo
Ô¨Ånch and goldÔ¨Ånch are shown in the bottom-left and bottom-right subÔ¨Ågures, respectively.
as shown in the bottom subÔ¨Ågures in Fig. 3. In this case,
Cluster-CAM provides solid evidence that CNN‚Äôs recogni-
tion mechanism is similar to human cognition in multiple
objects classiÔ¨Åcation.
CognitionAnalysisofFine-grainedImages: Tofurtherun-
derstand howCNNutilizes the learned informationto make
decisions,wecanuseCAMtointerpretCNNsinÔ¨Åne-grained
image classiÔ¨Åcation. Fine-grained classiÔ¨Åcation aims to dis-
tinguish subordinate categories within entry-level categories.
Examples include recognizing species of birds such as north-
erncardinalorindigobunting;monkeyssuchasguenonor
langur. Fine-grained classiÔ¨Åcation often requires much more
detailedinformationcomparedwithgenericobjectrecogni-
tion, like the texture of the skin, the thickness of the fur, etc,
soCAMsonÔ¨Åne-grainedimagescantellwhethertheinfor-
mationisreasonablylearnedbyCNNforclassiÔ¨Åcation. Fig.4
showstheheatmapsgeneratedbyseveralmentionedCAMs
giventheinputimageofaguenonintheÔ¨Årstrow. Interest-
ingly, they focus on completely diÔ¨Äerent parts of the guenon.
Grad-CAMandGrad-CAM++highlighttheguenon‚Äôseyes
and cheek, respectively. Ablation-CAM and Score-CAM
both highlight the guenon‚Äôs face, whereas Cluster-CAM onlyhighlights the guenon‚Äôs forehead. Intuitively, Ablation-CAM
and Score-CAM seem the most reasonable but the cognition-
basemapandcognition-scissorsclearlyshowthatthefore-
headisthemostdiscriminativepartbutthefaceisnegative
forguenon‚ÄôsclassiÔ¨Åcation. Itwillbeunderstoodifwefurther
study the diÔ¨Äerence in species between guenon and langur.
Guenon (widely distributed in Africa) is characterized by
blond hair on the forehead and a busty white lip, whereas,
langur (distributed in Asia) is characterized by a completely
black face. We mark their characteristics by green and red
circlesinthethirdrowinFig.4. Itisthereasonwhythethird
featuremap(face)isdeemedascognition-scissors,i.e.,the
black face is an interference factor for guenon‚Äôs categoriz-
ing. Thisexampleperfectlydemonstratestherationalityof
Cluster-CAM, particularly the cognition-scissors.
4.4. Ablation Study
Analysisof DiÔ¨ÄerentLayers: MostCNNsareconstructed
by a cascade of convolutional blocks (a block consists of
convolutional layers, nonlinear activation, pooling operation,
etc.). Fig. 5 shows the salience heatmaps of diÔ¨Äerent con-
volutional blocks in VGG-16. The results basically match
Z. Feng et al.: Preprint submitted to Elsevier Page 6 of 10original
 Grad
 Grad++
 Ablation
 Score
 Ours
/uni00000013/uni00000011/uni0000001a/uni00000016/uni00000019
 /uni00000013/uni00000011/uni0000001b/uni00000016/uni00000019
 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000019
 /uni00000013/uni00000011/uni0000001c/uni00000014/uni00000013
 /uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000014
guenon: golden hair and white mouth langur: black faceFigure 4: The cognition-base map (green square) and cognition-
scissors (red square) in merged feature maps (top). The images
are masked by corresponding feature masks as well as the pre-
dicted score (middle). Nine images of guenon and nine images
of langur (bottom). Note that the discriminative characteristics
of guenon (golden hair and white mouth) and langur (black
face) are labeled with green and red circles, respectively.
/uni0000002f/uni00000020/uni00000017
 /uni0000002f/uni00000020/uni0000001c
 /uni0000002f/uni00000020/uni00000014/uni00000019
 /uni0000002f/uni00000020/uni00000015/uni00000016
 /uni0000002f/uni00000020/uni00000016/uni00000013
Figure 5: The heatmaps produced by Cluster-CAM with diÔ¨Äer-
ent numbers of clusters for VGG-16. Lrefers to the indices of
layers in VGG-16.
human‚Äôsintuitionthattheshallowlayersmainlycapturesome
detailedinformation(e.g.,textureandedge),whereasdeep
layers concentrate on those parts with clearer semantics.
NumberofClustersandCNNStructures: Thenumberof
clusters usually plays a critical role in clustering algorithms.
Here we vary the cluster number from 2to8and present the
correspondingsalienceheatmapsforAlexNetandVGG-16
/uni00000034/uni00000020/uni00000015
 /uni00000034/uni00000020/uni00000016
 /uni00000034/uni00000020/uni00000017
 /uni00000034/uni00000020/uni00000018
 /uni00000034/uni00000020/uni00000019
 /uni00000034/uni00000020/uni0000001a
 /uni00000034/uni00000020/uni0000001b
/uni00000034/uni00000020/uni00000015
 /uni00000034/uni00000020/uni00000016
 /uni00000034/uni00000020/uni00000017
 /uni00000034/uni00000020/uni00000018
 /uni00000034/uni00000020/uni00000019
 /uni00000034/uni00000020/uni0000001a
 /uni00000034/uni00000020/uni0000001b
Figure 6: The heatmaps produced by Cluster-CAM (K-means)
with diÔ¨Äerent numbers of clusters for AlexNet (top) and VGG-
16(bottom).
in Fig. 6. It shows the salience heatmaps are sensitive to
the number of clusters and are diÔ¨Äerent with CNN models.
For AlexNet, the highlighted region in heatmaps could be
semantic chaos if the feature maps are split into too many
or too few clusters. It is probably because a large number
ofclustersmayintroducetoomanydetailedpatternsofthe
object and a small number of clusters may directly include
backgroundinformation. Therefore,thenumberofclusters
should be selected as a median value. Note it is only an
empirical conclusion and exclusion exists that the optimal
value is 2for the fourth row (beer) in Fig.6. It is probably
becausetheobjectissimpleandinregularshape,thusonly
twoclustersareenoughtorepresentallnecessaryinformation.
ForVGG-16,thehighlightedregionismoreconcentratedona
speciÔ¨ÅcpartoftheobjectthanAlexNet. Itisprobablybecause
more detailed discriminative information could be captured
in VGG-16 which has much deeper layers than AlexNet.
Clustering Method: In Section 3, we introduced two clus-
teringalgorithms,i.e.,K-meansandspectralclustering. Here
we take each feature map as a vertex in the graph and use
distancetoconstructthesimilaritymatrix,adjacentmatrix,
degreematrix, andLaplacianmatrixusing(17), (10), (11),
and (14). Fig. 7 shows the salience heatmaps produced by
spectral clustering with diÔ¨Äerent clusters and diÔ¨Äerent eigen-
vectors. It can be observed that the heatmaps are highly
related to the number of eigenvectors rather than the clus-
ters. Notethattheoptimalnumberofeigenvectorsishighly
relatedtothe imageitself,thuscareful manipulationofthis
parameterisrequiredfordiÔ¨Äerentobjectstoobtainthebest
heatmap. Therefore we will only use K-means to compute
the qualitative evaluation metrics in the next section.
Z. Feng et al.: Preprint submitted to Elsevier Page 7 of 10/uni00000034/uni00000020/uni00000015
 /uni00000034/uni00000020/uni00000016
 /uni00000034/uni00000020/uni00000017
 /uni00000034/uni00000020/uni00000018
 /uni00000034/uni00000020/uni00000019
 /uni00000034/uni00000020/uni0000001a
 /uni00000034/uni00000020/uni0000001b
/uni00000015
 /uni00000016
 /uni00000017
 /uni00000018
 /uni00000019
 /uni0000001a
Figure 7: The heatmaps produced by Cluster-CAM with dif-
ferent numbers of clusters by spectral clustering (top) and
heatmaps produced by Cluster-CAM with diÔ¨Äerent numbers of
eigenvectors (bottom).
4.5. Qualitative Evaluation
4.5.1. Performance Evaluation
TofurtherevaluatetheinterpretingperformanceofCluster-
CAM quantitatively, two widely-used evaluation metrics are
adopted in this paper, i.e., conÔ¨Ådence drop and increase
number [ 36,41]. First of all, let‚Äôs think about what kind
of heatmap can be regarded as a good interpretation of CNN.
AnaturalandintuitiveideaistomeasurehowmuchtheconÔ¨Å-
dence(predictedscore)ofthetargetclasswilldropwhenthe
originalimageispartlyoccludedaccordingtotheheatmap.
SpeciÔ¨Åcally, for each image, a corresponding explanation
mapLcis generated by element-wise multiplication of the
heatmaps and the current image as in (9) and (8).
ConÔ¨Ådence drop : This metric compares the average drop
ofthemodel‚ÄôsconÔ¨Ådenceforaparticularclassinanimage
after occlusion as:
confidence _drop =Sc.X/ *Sc.X√ΩHn/
Sc.X/;(23)
Forexample,assumethatCNNpredictsanobjectindigoÔ¨Ånch
inanimageXwithconÔ¨Ådence 0:8. Whenweinputtheexpla-
nationmap,X√ΩH,ofthisimage,theCNN‚ÄôsconÔ¨Ådencein
the class indigo Ô¨Ånch falls to 0:6. Then the conÔ¨Ådence_drop
would be 25~. It means that the most discriminative part
(75~)isincludedinthehighlightedregion. ConÔ¨Ådencedrop
is expected to be lower for a better CAM and is usually aver-
aged over many images.
Increasenumber measureshowmanytimestheCNN‚Äôspre-
dictionscorefor cincreasedwhenthemaskedimageisinput.
SpeciÔ¨Åcally, it happens sometimes that the object is com-
pletelyincludedandinterferencepartsareoccluded(e.g.,the
object-irrelevant parts and background) in the highlighted
region. Inthiscase,therewillbeanincreaseintheCNN‚ÄôsTable 1
Performance Evaluation Metrics.
Method ConÔ¨Ådence drop ¬öIncrease number ~
Grad-CAM 17:94 19 :15
Grad-CAM++ 18:44 19 :75
Ablation-CAM 12:38 24 :67
Score-CAM 12:21 25 :48
Cluster-CAM 11:60 26 :10
Table 2
EÔ¨Éciency Evaluation Metrics. Here Q= 6in Cluster-CAM.
Method Computing time ¬önumber of FP ¬ö
Grad-CAM 0:078 1
Grad-CAM++ 0:141 1
Ablation-CAM 2:206 256
Score-CAM 4:647 256
Cluster-CAM 0:382 6
predicted score for the class (i.e., conÔ¨Ådence drop <0). This
value is computed as a percentage through the whole dataset.
Table 1 shows two evaluation metrics of the entire val-
idation set in ILSVRC dataset ( ¬ömeans the lower value is
better and ~means the higher value is better). These two
metrics clearly demonstrate the superiority of Cluster-CAM
tootherexistingCAMs. ThemetricsarecomputedinPytorch
1.8.0+cudnn11.1, NVIDIA RTX-3070.
4.5.2. EÔ¨Éciency Evaluation
Here we present two eÔ¨Éciency metrics, i.e., the average
computing time and the number of forward propagation (FP)
per image in Table 2. It is clear that Cluster-CAM greatly
reduces the number of FP compared with Ablation-CAM
andScore-CAM.Naturally,asigniÔ¨ÅcantimprovementineÔ¨É-
ciency emerges from Cluster-CAM, i.e., Cluster-CAM is 5:7
timesfasterthanAblation-CAMand 12:1timesfasterthan
Score-CAM. Therefore, Cluster-CAM can obtain better visu-
alization and interpretation performance than gradient-based
and gradient-free CAMs with eÔ¨Éciency closer to gradient-
based CAMs.
5. Conclusion
In this paper, we proposed Cluster-CAM, an eÔ¨Äective
andeÔ¨ÉcientCNNinterpretationtechniquebasedonunsuper-
visedclusteringalgorithms. Cluster-CAMistheÔ¨Årstattempt
to comprehensively analyze how to split feature maps into
diÔ¨Äerent groups and provide an artful strategy to remove the
object-irrelevant elements by deÔ¨Åning cognition-scissors. In
Cluster-CAM,onlyseveraltimesofforwardpropagationis
required per image while it is usually more than hundreds
forothergradient-freeCAMs. Qualitativeandquantitative
experimentalresultsveriÔ¨ÅedCluster-CAMcanobtaineven
betterperformancethangradient-freeCAMswithmuchlower
computing time.
Z. Feng et al.: Preprint submitted to Elsevier Page 8 of 10Data Availability Statements
ILSVRC dataset can be downloaded from the website
https://www.image-net.org/challenges/LSVRC/.
Acknowledgments
ThisworkisfundedbytheNationalNaturalScienceFoun-
dationofChina(GrantNo. 62276204,61871301,62071349),
Project2021ZDZX-GY-0001,scienceandtechnologyproject
of Xianyang city.
References
[1]Cao, J., Pang, Y., Han, J., Li, X., 2019. Hierarchical shot detector, in:
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision, pp. 9705‚Äì9714.
[2]Chattopadhay,A.,Sarkar,A.,Howlader,P.,Balasubramanian,V.N.,
2018. Grad-CAM++: Generalizedgradient-basedvisualexplanations
for deep convolutional networks, in: In Proceedings of 2018 IEEE
Winter Conference on Applications of Computer Vision (WACV),
IEEE. pp. 839‚Äì847.
[3]Chen, H., Jin, Y., Jin, G., Zhu, C., Chen, E., 2022. Semisupervised
semantic segmentation by improving prediction conÔ¨Ådence. IEEE
Transactions on Neural Networks and Learning Systems 33, 4991‚Äì
5003. doi: 10.1109/TNNLS.2021.3066850 .
[4]Deng,J., Dong,W.,Socher,R.,Li, L.J.,Li,K., Fei-Fei, L.,2009. Ima-
genet: Alarge-scalehierarchicalimagedatabase,in: Inproceedingsof
2009 IEEE Conference onComputer Vision and Pattern Recognition
(CVPR), pp. 248‚Äì255. doi: 10.1109/CVPR.2009.5206848 .
[5]Feng, Z., Ji, H., Stankovi/uni0107, L., Fan, J., Zhu, M., 2021a. SC-SM
CAM:AneÔ¨ÉcientvisualinterpretationofCNNforSARimagestarget
recognition. Remote Sensing 13, 4139.
[6]Feng, Z., Zhu, M., Stankovi/uni0107, L., Ji, H., 2021b. Self-matching CAM:
A novel accurate visual explanation of CNNs for SAR image interpre-
tation. Remote Sensing 13, 1772.
[7]Fu,R.,Hu,Q.,Dong,X.,Guo,Y.,Gao,Y.,Li,B.,2020. Axiom-based
Grad-CAM: Towards accurate visualization and explanation of CNNs,
in: In Proceedings of the 2020 British Machine Vision Conference
(BMVC 2020).
[8]He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
for image recognition, in: In Proceedings of 2016 IEEE conference
onComputerVisionandPatternRecognition(CVPR),pp.770‚Äì778.
doi:10.1109/CVPR.2016.90 .
[9]Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classiÔ¨Åca-
tion withdeep convolutional neuralnetworks,in: Pereira, F., Burges,
C.,Bottou,L.,Weinberger,K.(Eds.),AdvancesinNeuralInformation
Processing Systems, Curran Associates, Inc.
[10]Lapuschkin,S.,W√§ldchen,S.,Binder,A.,Montavon,G.,Samek,W.,
M√ºller, K.R., 2019. Unmasking clever hans predictors and assessing
what machines really learn. Nature communications 10, 1‚Äì8.
[11]Liang, X., Hu, Z., Zhang, H., Lin, L., Xing, E.P., 2018. Symbolic
graph reasoning meets convolutions. Advances in Neural Information
Processing Systems 31.
[12]Liu, J., Zhang, F., Zhou, Z., Wang, J., 2023. Bfmnet: Bilateral fea-
ture fusion network with multi-scale context aggregation for real-time
semantic segmentation. Neurocomputing 521, 27‚Äì40. doi: https:
//doi.org/10.1016/j.neucom.2022.11.084 .
[13]Liu,K.,Meng,R.,Li,L.,Mao,J.,Chen,H.,2022a. Sisl-net: Saliency-
guided self-supervised learning network for image classiÔ¨Åcation. Neu-
rocomputing 510, 193‚Äì202. doi: https://doi.org/10.1016/j.neucom.
2022.09.029 .
[14]Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.,
2022b. A convnet for the 2020s, in: In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp.
11976‚Äì11986.[15]Ma, X., Zhang, S., Pena-Pena, K., Arce, G.R., 2021. Fast spectral
clusteringmethodbasedongraphsimilaritymatrixcompletion. Signal
Processing 189, 108301. doi: https://doi.org/10.1016/j.sigpro.2021.
108301.
[16]Macpherson, T., Churchland, A., Sejnowski, T., DiCarlo, J., Kamitani,
Y., Takahashi, H., Hikida, T., 2021. Natural and artiÔ¨Åcial intelligence:
A brief introduction to the interplay between ai and neuroscience
research. Neural Networks 144, 603‚Äì613. doi: https://doi.org/10.
1016/j.neunet.2021.09.018 .
[17]Omeiza, D., Speakman, S., Cintas, C., Weldermariam, K., 2019.
Smooth grad-cam++: An enhanced inference level visualization tech-
niquefordeepconvolutionalneuralnetworkmodels. arXivpreprint
arXiv:1908.01224 .
[18]Ramaswamy,H.G.,etal.,2020. Ablation-cam: Visualexplanationsfor
deepconvolutionalnetworkviagradient-freelocalization,in: InPro-
ceedingsoftheIEEEWinterConferenceonApplicationsofComputer
Vision (WACV), pp. 983‚Äì991.
[19]Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only
lookonce: UniÔ¨Åed,real-timeobjectdetection,in: InProceedingsof
2016 IEEE Conference onComputer Vision and Pattern Recognition
(CVPR), pp. 779‚Äì788. doi: 10.1109/CVPR.2016.91 .
[20]Ren, J., Li, M., Liu, Z., Zhang, Q., 2021. Interpreting and disentan-
gling feature components of various complexity from DNNs, in: In
proceedingsofInternationalConferenceonMachineLearning,PMLR.
pp. 8971‚Äì8981.
[21]Saleem, R., Yuan, B., Kurugollu, F., Anjum, A., Liu, L., 2022. Ex-
plaining deep neural networks: A survey on the global interpretation
methods. Neurocomputing 513, 165‚Äì180. doi: https://doi.org/10.
1016/j.neucom.2022.09.129 .
[22]Scalzo, B., Stankovi/uni0107, L., Dakovi/uni0107, M., Constantinides, A.G., Mandic,
D.P., 2023. A class of doubly stochastic shift operators for random
graph signals and their boundedness. Neural Networks 158, 83‚Äì88.
doi:https://doi.org/10.1016/j.neunet.2022.10.035 .
[23]Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D.,
Batra, D., 2017. Grad-CAM: Visual explanations from deep networks
viagradient-basedlocalization,in: InProceedingsofthe2017IEEE
international conference on computer vision, pp. 618‚Äì626.
[24]Simonyan,K.,Vedaldi,A.,Zisserman,A.,2013. Deepinsideconvolu-
tional networks: Visualising image classiÔ¨Åcation models and saliency
maps. arXiv preprint arXiv:1312.6034 .
[25]Simonyan,K.,Zisserman.,A.,2015.Verydeepconvolutionalnetworks
for large-scale image recognition, in: 3rd International Conference on
Learning Representations (ICLR 2015), pp. 1‚Äì14.
[26]Spinelli, I., Scardapane, S., Uncini, A., 2022. A meta-learning ap-
proach for training explainable graph neural networks. IEEE Trans-
actionsonNeuralNetworksandLearningSystems,1‚Äì9doi: 10.1109/
TNNLS.2022.3171398 .
[27]Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.,
2021. Bottleneck transformers forvisual recognition,in: In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
recognition (CVPR), pp. 16519‚Äì16529.
[28]Stankovic,L.,Dakovic,M.,Sejdic,E.,2017. Vertex-frequencyanal-
ysis: A way to localize graph spectral components [lecture notes].
IEEESignalProcessingMagazine34,176‚Äì182. doi: 10.1109/MSP.2017.
2696572.
[29]Stankovic,L.,Mandic,D.P.,Dakovic,M.,Kisil,I.,Sejdic,E.,Constan-
tinides,A.G.,2019. Understandingthebasisofgraphsignalprocessing
via an intuitive example-driven approach. IEEE Signal Processing
Magazine 36, 133‚Äì145. doi: 10.1109/MSP.2019.2929832 .
[30] Sun, S., Song, B., Cai, X., Du, X., Guizani, M., 2022. CAMA: Class
activationmappingdisruptiveattackfordeepneuralnetworks. Neu-
rocomputing500,989‚Äì1002. doi: https://doi.org/10.1016/j.neucom.
2022.05.065 .
[31]Tan, R., Gao, L., Khan, N., Guan, L., 2022. Interpretable artiÔ¨Åcial
intelligencethroughlocalityguidedneuralnetworks. NeuralNetworks
155, 58‚Äì73. doi: https://doi.org/10.1016/j.neunet.2022.08.009 .
[32]Townsend, J., Chaton, T., Monteiro, J.M., 2020. Extracting relational
explanations from deep neural networks: A survey from a neural-
Z. Feng et al.: Preprint submitted to Elsevier Page 9 of 10symbolic perspective. IEEE Transactions on Neural Networks and
Learning Systems 31, 3456‚Äì3470. doi: 10.1109/TNNLS.2019.2944672 .
[33]Tu, Z., Zhou, A., Gan, C., Jiang, B., Hussain, A., Luo, B., 2021.
A novel domain activation mapping-guided network (DA-GNT) for
visual tracking. Neurocomputing 449, 443‚Äì454. doi: https://doi.org/
10.1016/j.neucom.2021.03.056 .
[34]Vlahek, D., Mongus, D., 2021. An eÔ¨Écient iterative approach to
explainable feature learning. IEEE Transactions on Neural Networks
and Learning Systems , 1‚Äì13doi: 10.1109/TNNLS.2021.3107049 .
[35]Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel,
P., Hu, X., 2020. Score-CAM: Score-weighted visual explanations
for convolutional neural networks, in: In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)
workshops, pp. 24‚Äì25.
[36]Zeiler, M.D., Fergus, R., 2014. Visualizing and understanding con-
volutional networks, in: European conference on computer vision,
Springer. pp. 818‚Äì833.
[37]Zhang, Q., Rao, L., Yang, Y., 2021. Group-CAM: Group score-
weighted visual explanations for deep convolutional networks. arXiv
preprint arXiv:2103.13859 .
[38]Zhao,Z.,Xie,X.,Wang,C.,Liu,W.,Shi,G.,Du,J.,2019. Visualizing
andunderstandingoflearnedcompressivesensingwithresidualnet-
work. Neurocomputing359,185‚Äì198. doi: https://doi.org/10.1016/
j.neucom.2019.05.043 .
[39]Zhou, B., Bau, D., Oliva, A., Torralba, A., 2019. Interpreting deep
visualrepresentationsvianetworkdissection. IEEETransactionson
Pattern Analysis and Machine Intelligence 41, 2131‚Äì2145. doi: 10.
1109/TPAMI.2018.2858759 .
[40]Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., 2016.
Learning deep features for discriminative localization, in: In Proceed-
ings of the 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2921‚Äì2929.
[41]Zhou,K.,Kainz,B.,2018. EÔ¨Écientimageevidenceanalysisofcnn
classiÔ¨Åcation results. arXiv preprint arXiv:1801.01693 .
[42]Zhu, M., Feng, Z., Stankovi/uni0107, L., Ding, L., Fan, J., Zhou, X., 2022.
Aprobe-featureforspeciÔ¨ÅcemitteridentiÔ¨Åcationusingaxiom-based
grad-cam. Signal Processing , 108685.
[43]Zhu,Y.,Zhao,C.,Wang,J.,Zhao,X.,Wu,Y.,Lu,H.,2017. Couplenet:
Coupling global structure with local parts for object detection, in:
Proceedings of the IEEE international conference on computer vision,
pp. 4126‚Äì4134.
Zhenpeng Feng was born in Xianyang, Shaanxi,
Chinain1996. HereceivedaB.E.degreeinSchool
of Electronic Engineering, Xidian University in
2019. HeiscurrentlyaPh.D.studentinexplainable
artiÔ¨Åcial intelligence at School of Electronic Engi-
neering, Xidian University. He is also a visiting
studentintheUniversityofMontenegro,working
withProf. Ljubi≈°aStankovi/uni0107‚Äôsresearchteam. His
researchinterestsincludeinterpretingdeepneural
networks and signal processing.
Hongbing Ji received a B.S. degree in radar en-
gineering, an M.S. degree in circuit, signals, and
systems, and the Ph.D. degree in signal and infor-
mationprocessingfromXidianUniversity,Xi‚Äôan,
China, in 1983, 1989, and1999, respectively. He is
currentlyafullprofessoratXidianUniversityand
a senior member of IEEE. His research interests
includepatternrecognition,radarsignalprocessing,
and multi-sensor information fusion.
Milo≈° Dakovi/uni0107 was born in 1970 in Nik≈°i/uni0107, Mon-
tenegro. He received a B.S. in 1996, an M.S. in
2001, and a Ph.D. in 2005, all in electrical engi-
neering from the University of Montenegro. He
is a full professor at the University of Montene-
gro. His research interests are in signal process-
ing, time-frequency signal analysis, compressive
sensing,radarsignalprocessing,andgraphsignal
processing.
Xiyang Cui was born in Handan, Hebei, China
in 1997. He received the B.E. degree and M.E.
degreeinElectronicInformationEngineeringand
ElectricalCircuitSystemfromSchoolofElectronic
Engineering,XidianUniversityin2019and2021,
respectively. He is currently an investigator of an
electronic company and collaborates with Zhen-
pengFengandProf. Ljubi≈°aStankovi/uni0107inscientiÔ¨Åc
research. His research interests include electrical
circuit design and image processing.
Mingzhe Zhu was born in China in 1982. He re-
ceived a B.S. degree in signal and information pro-
cessing,aPh.D.degreeinpatternrecognitionand
intelligentsystemfromXidianUniversityin2004
and 2010, respectively. He is currently an asso-
ciateprofessoratSchoolofElectronicEngineering,
Xidian University. His research interests include
non-stationary signal processing, time-frequency
analysis, and target recognition.
Ljubi≈°aStankovi/uni0107 was born in Montenegro, 1960.
HewasattheRuhrUniversityBochum,1997-1999,
supportedbytheAvHFoundation. Stankovicwas
theRectoroftheUniversityofMontenegro2003-
2008, the Ambassador of Montenegro to the UK,
2011-2015, and a visiting academic to the Imperial
College London, 2012-2013. He published almost
200 journal papers. He is a member of the Na-
tionalAcademyofScienceandArts(CANU)and
theAcademiaEuropaea. Stankovi/uni0107wontheBestpa-
perawardfromtheEURASIPin2017andtheIEEE
SPM Best Column Award for 2020. Stankovi/uni0107 is
aprofessorattheUniversityofMontenegroanda
Fellow of the IEEE.
Z. Feng et al.: Preprint submitted to Elsevier Page 10 of 10