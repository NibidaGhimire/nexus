Cluster-CAM: Cluster-Weighted Visual Interpretation of CNNs’
Decision in Image Classiﬁcation
Zhenpeng Fenga, Hongbing Jia,<, Miloš Dakovi/uni0107b, Xiyang Cuia, Mingzhe Zhuaand
Ljubiša Stankovi/uni0107b
aSchool of Electronic Engineering, Xidian University, Xi’an, China
bFaculty of Electrical Engineering, University of Montenegro, Podgorica, Montenegro
ARTICLE INFO
Keywords :
keywords-1explainable artiﬁcial intel-
ligence
keywords-2 class activation mapping
keywords-3 clustering algorithm
keywords-4 image classiﬁcationABSTRACT
Despite the tremendous success of convolutional neural networks (CNNs) in computer vision, the
mechanism of CNNs still lacks clear interpretation. Currently, class activation mapping (CAM), a
famous visualization technique to interpret CNN’s decision, has drawn increasing attention. Gradient-
basedCAMsareeﬃcientwhiletheperformanceisheavilyaﬀectedbygradientvanishingandexploding.
In contrast, gradient-free CAMs can avoid computing gradients to produce more understandable
results. However, existing gradient-free CAMs are quite time-consuming because hundreds of forward
interferenceperimagearerequired. Inthispaper,weproposedCluster-CAM,aneﬀectiveandeﬃcient
gradient-freeCNNinterpretationalgorithm. Cluster-CAMcansigniﬁcantlyreducethetimesofforward
propagationbysplittingthefeaturemapsintoclustersinanunsupervisedmanner. Furthermore,we
propose an artful strategy to forge a cognition-base map and cognition-scissors from clustered feature
maps. The ﬁnal salience heatmap will be computed by merging the above cognition maps. Qualitative
resultsconspicuouslyshowthatCluster-CAMcanproduceheatmapswherethehighlightedregions
match the human’s cognition more precisely than existing CAMs. The quantitative evaluation further
demonstrates the superiority of Cluster-CAM in both eﬀectiveness and eﬃciency.
1. Introduction
Convolutional neural networks (CNNs) have provided
a basis for numerous remarkable achievements in various
computer vision tasks like, for example, image classiﬁcation
[9,8,27,13], object detection [ 19,43,1,12], and seman-
tic segmentation [ 14,11,3]. Despite CNNs’ extraordinary
performance, they still lack a clear interpretation of the in-
ner mechanism [ 10,42,21]. This lack of transparency can
indeedbeadisqualifyingfactorinsomepeculiarscenarios
wheremistakesininterpretationcanjeopardizehumanlife
and health, like in medical image processing or autonomous
vehicles [ 20,38,34,32]. Therefore, it is highly desirable
toﬁndawaytounderstandandexplainwhatexactlyCNNs
have learned during the training process [26, 31, 16].
Recently, Class Activation Mapping (CAM), a visual in-
terpretation technique, has drawn increasing attention [ 30,
33]. CAMaimsathighlightingsalience regionsofaninput
imageforCNN’sdecisionusingalinearlyweightedcombina-
tionoffeaturemaps. VanillaCAMdirectlyutilizestheweight
of each feature map after global average pooling (GAP) cor-
respondingtothetargetclass,soitisonlyavailableforCNNs
with GAP [ 40]. To further extend CAM to more complex
CNN structures, numerous modiﬁed CAMs are proposed
and they can be broadly categorized as: 1) gradient-based
CAMs,and2)gradient-freeCAMs. Gradient-basedCAMs
(e.g. Grad-CAM[ 23],Grad-CAM++[ 2],SmoothCAM++
[17],XGrad-CAM[ 7]etc.) deﬁnetheweightsofeachfeature
map using the average of partial gradient of the predicted
scorewithrespecttothefeaturemaps. Gradient-basedCAMs
<Corresponding author: Hongbing Ji
hbji@xidian.edu.cn ( Hongbing Ji)
ORCID(s):areusuallycomputedeﬃciently. However,theirweightslack
reasonableexplanationandareeasilyimpactedbygradient
exploding or vanishing. To address this limitation, some
gradient-freeCAMsareproposed. Theydeﬁneanintuitive
impact of each feature map on the predicted score instead of
using the gradient. Examples of gradient-free CAMs are the
Ablation CAM [ 18] and the Score-CAM [ 35]. Gradient-free
CAMscanprovideamoreexplainableweightdeﬁnitionthan
gradient-basedCAMsinmostcases. Fig.1showsthesalience
heatmaps produced by several aforementioned CAMs.
Althoughgradient-freeCAMsdeﬁnetheweightsmore
reasonably, they are usually very time-consuming since hun-
dreds of forward propagations per image are required. To
improvetheeﬃciencyofgradient-freeCAMs,Q.Zhanget
al. proposedGroup-CAMwherefeaturemapsaresplitinto
several groups [ 37]. In this case, only several forward propa-
gationsareneededincomputingtheweights. Nonetheless,
thefeaturemapsaresplitwithoutanyregulationintheGroup-
CAM. Actually, various feature maps have learned diﬀerent
semantic concepts relevant/irrelevant to the object. There-
fore, the feature maps should be split into groups/clusters.
Those with similar semantics should be assigned to the same
group. Z.Fengetal. proposedSC-SMCAMusingspectral
clusteringtoaccomplish thisgoal,particularlyfor synthetic
apertureradar(SAR)images[ 5]. However,nofurtheranal-
ysisormodiﬁcationofweightsismentionedintheSC-SM
CAM [6, 5].
Inthispaper,weproposeaCluster-CAM,aneﬀectiveand
eﬃcient gradient-free CAM, based on unsupervised cluster-
ing. In Cluster-CAM, an unsupervised clustering technique,
K-means/spectralclusteringisadoptedtosplitfeaturemaps
intoseveralclusters. Subsequently,weprovideanartfulstrat-
Z. Feng et al.: Preprint submitted to Elsevier Page 1 of 10arXiv:2302.01642v1  [cs.CV]  3 Feb 2023egytomergethosefeaturemapsintoacognition-basemap
and a cognition-scissors map which will be combined as the
ﬁnal salience heatmap. The highlights of this paper are as
follows:
•We propose a Cluster-CAM, as the ﬁrst attempt to
provide a cluster-weighted CAM framework via unsu-
pervised clustering based on optical images.
•We provide a novel and artful weight-forming strat-
egy to merge the cognition-base map and cognition-
scissors map. These two maps greatly match the hu-
man’s cognition and intuition, thus the weights are
completely reasonable and understandable.
•Cluster-CAMiseﬀectiveandeﬃcient,whichoutper-
forms existing gradient-free CAMs in performance in
most cases with signiﬁcantly lower computing costs.
Therestofthispaperisorganizedasfollows. Section2
introduces the basic knowledge of various CAMs. Section 3
elaborates on how to generate salience heatmaps by Cluster-
CAM. In Section 4, various experiments are implemented to
demonstratethevalidityofCluster-CAMandfurtheranalyze
the experimental results from various aspects. Section 5
concludes this paper.
2. Related Work
As discussed in Section 1, the key issue in interpreting
CNN’sdecisionistoexplainwhattheneuralnetworklearned
toﬁnishareasonableinference[ 42,10]. Tovisualizewhat
CNN focuses on the input image, numerous interpretation
algorithmsareproposed[ 36,24,40,39],amongwhichClass
Activation Mapping draws the most increasing attention due
to its simplicity and good performance.
Vanilla CAM: B. Zhou et al. ﬁrstly proposed the vanilla
CAMtoproduceasalienceheatmapbyalinearlyweighted
combinationoffeaturemaps, Fn,withelements Fn.i;j/; n=
1;2;:::;N, at the target convolutional layer before classiﬁca-
tion [40]
Mc.i;j/ =É
nc
nFn.i;j/ (1)
Sc=É
n!c
nÉ
i;jFn.i;j/ =É
nc
nÉ
i;jFn.i;j/;(2)
whereMc.i;j/is a heat-map and Scdenotes the predicted
score for the target class c. Thus,care deﬁned by the
weights,!c,ofeachfeaturemapcorrespondingto c-thunit
in the classiﬁcation layer. Therefore, CAM is only available
with CNNs with global-average pooling following the last
convolutional layer. To extend CAM to all CNNs, many
modiﬁed CAMs are further proposed by manipulating the
deﬁnition of weights, which are generally categorized as: 1)
gradient-based CAMs; 2) gradient-free CAMs.
2.1. Gradient-based Class Activation Mapping
Gradient-basedCAM: Selvarajuetal. proposedGrad-CAM
to visualize any classiﬁcation CNN architectures by weight-
ing the feature maps in a certain convolutional layer withthe gradients of the predicted score, scwith respect to the
elements ofFn[23], as
c;Grad
n=É
i;j)sc
)Fn.i;j/; (3)
where diﬀerent from (2), scis a sparse vector whose ele-
ments are zeros except the c-th element, which is equal to
Sc. However, the highlighted regions generated by Grad-
CAMareusuallymuchsmallerthantheobject. Toprovide
aprecisehighlightedregion,somefurthermodiﬁedCAMs
are proposed, like Grad-CAM++[ 2]. A. Chattopadhyay et
al proposed Grad-CAM++ which can produce more pre-
cise highlighted locality. Grad-CAM++ assumes diﬀerent
elements in thegradientmatrix should have diﬀerent contri-
butions to features maps, thus an extra factor is introduced
to realize this assumption using higher order partial gradient,
as:
c;Grad ++
n=)2sc
).Fn.i;j//2
2)2sc
).Fn.i;j//2+³
a;bFn.a;b/)3sc
.Fn.i;j//3É
i;j)sc
)Fn.i;j/:
(4)
However, the gradient,)sc
)Fn, is usually heavily noised or som-
times even all-zero. It is probably because 1) CNN is trained
tolearnageneralizedcapabilitytoclassifyageneralconcept
rather than a speciﬁc object. 2) some unreasonable phe-
nomena emerged in CNN’s training, like gradient vanishing
andgradientexploding. D.Omeizaetal. proposedSmooth
Grad-CAM++ to further suppress the noise. The weights of
SmoothGrad-CAM++aredeﬁned usingthe averageof the
gradients as:
c;SmoothGrad ++
n= (5)
1
m³M
m=1Dn
1
21
m³M
m=1Dn
2+³
a;bFn.a;b/1
m³M
m=1Dn
3.1
mMÉ
m=1Dn
1/
whereDn
1=³
i;j)sc
)Fn.i;j/,Dn
2=³
i;j)2sc
).Fn.i;j//2, andDn
3=
³
i;j)3sc
).Fn.i;j//3whentheinputisaddedwithrandomnoisefor
Mtimes (Mis a constant integer). This smoothing strategy
is intuitive but still rough for some complex CNN structures.
Tofurtherenhancetheinterpretabilityofweights,R.Fuet
al. proposed XGrad-CAM by introducing two completely
explainable axioms to form the weight:
c;XGrad
n=É
i;jFn.i;j/³
i;jFn.i;j/)sc
)Fn.i;j/: (6)
Note neither Smooth Grad-CAM++ nor XGrad CAM can
guaranteecompletelyavoidingtheaboveunreasonablephe-
nomena in gradient computing.
2.2. Gradient-free Class Activation Mapping
Gradient-free CAM: To completely solve the problems re-
sultingfromgradientcomputing,somegradient-freeCAMs
Z. Feng et al.: Preprint submitted to Elsevier Page 2 of 10Input image
 Grad
 Grad++
 Ablation
 Score
 Ours
Figure 1: The heatmaps are produced by diﬀerent CAMs. The ﬁrst column is input images
(indigo ﬁnch, eagle, rooster, and ostrich from top to bottom). The second to ﬁfth columns
are heatmaps produced by Grad-CAM, Grad-CAM++, Ablation-CAM, Score-CAM, and
Cluster-CAM.
are proposed, i.e., Ablation-CAM [ 18] and Score-CAM [ 35],
to form the weights using the impact of each feature map on
the predicted score instead of using gradient. In Ablation-
CAM, the weights are deﬁned as:
c;Ablation
n=Sc*Sc;n
Sc; (7)
whereSc;ndenotes the predicted score for class cwhenn-th
feature map is set to zero. In this case, a large weight will be
assignedtothecurrentfeaturemapiftheremovalofitcan
lead to a dramatic drop in the predicted score ( Sc*Sc;nis a
large value) and vice versa. The authors argue that Ablation-
CAM is immune to both saturation which marks a ﬁlter as
important althoughit isnot important,and explosion which
marksaﬁlterthathasaverysmallinﬂuenceashavinghigh
importance. DiﬀerentfromAblation-CAM,Score-CAMcon-
sidersmeasuringtheimpactofthefeaturemapbyintroducing
the input image, X, as
c;Score
n=Sc.XýHn/ *Sc.Xb/ (8)
Hn=s.Up.Fn//; (9)
whereýdenotes the element-wise multiplication, Xbis a
baseline image which can be set the input image itself, Up./
denotestheoperationthatupsamples Fnintotheinputsize
ands./isanormalizationfunctionthatmapseachelement
in the input matrix into [ 0,1].XýHncan be deemed as
ﬁlteringwhichonlypasseselementsin XmaskedbyHn,thusa large weight will be assigned if most target-discriminative
are preserved by the current feature map, i.e. f.XýHn/is
higherthanSc.Xb/andviceversa. Currently,gradient-free
CAMshavedrawnmoreattentionthangradient-basedCAMs
due to their superior performance and explainable deﬁnition
of weights. However, gradient-free CAMs are much more
time-consuming than gradient-based CAMs because hun-
dreds or even thousands of forward interference are required
whilethosegradient-basedCAMsonlyrequireoneforward
interference.
3. Methodology
Inthissection,wewillﬁrstintroducesomebasicconcepts
on graph-based spectral clustering and K-means. Then we
present the detailed procedures of Cluster-CAM.
3.1. Spectral Clustering and K-means
Spectral clustering is a widely-used unsupervised cluster-
ingalgorithmbasedongraphsignalprocessing[ 28,29,15,
22]. Speciﬁcally,theprocesseddata(featuremaps, Fn)are
regarded as vertices in a graph topology. Then the elements,
S.i;j/, of the similarity matrix, S, can be deﬁned:
S.i;j/ = similarity. Fi;Fj/; (10)
where similarity./refers to a function that measures the
similarity between two vertices (feature maps). If we use the
structural similarity index (SSIM), then it ranges from 0(no
Z. Feng et al.: Preprint submitted to Elsevier Page 3 of 10Classifier
Convolution layers
input image
feature maps clustered feature mapsclusteringrepresentative feature maps
upsamplin g
masked features
cognition -base and cognition -scissors
saliency heatmap
…
0.019 0.491 0.001 0.006 0.308element -wise 
product
cognition couplingFigure 2: The ﬂowchart of Cluster-CAM.
similarity) to 1(identical feature maps). The elements of the
weighted adjacency matrix, A, can be deﬁned as:
<
A.i;j/ = exp.*.1 * S.i;j//_/;ifS.i;j/>;
A.i;j/ = 0; else,
(11)
whereis a threshold to keep the direct edge in the cor-
responding graph for two neighboring vertices and is a
parameter. Note that, by deﬁnition of similarity, this adja-
cencymatrixisasymmetricmatrixresultinginanundirected
graph, that is, A.i;j/ =A.j;i/.
Thesimilaritycanbedeﬁnedusingthediﬀerencebetween
two vertices (feature maps), d.i;j/ =ððFi*Fiðð. Then the
weighted adjacency matrix is deﬁned by
<
A.i;j/ = exp.*d2.i;j/_2/;ifS.i;j/>;
A.i;j/ = 0; else,(12)
whereandhave the same role as in (11).
In order to produce the vectors for spectral clustering,
nowwecontinueandcomputethegraphLaplacianmatrix,
L, as
L=D*A (13)
whereD.i;i/ =³
jA.i;j/are the elements of the degree
matrixDwhich is diagonal.
In practice, the graph Laplacian matrix usually can be
normalized, as
LN=D*1
2LD*1
2=I*D*1
2AD*1
2: (14)
The clustering results obtained using these two matrices are
very similar.The eigendecomposition of the graph Laplacian
L=UTU: (15)
Resultsineigenvectors u1,u2,5,uNthatarethecolumnsof
matrixU. The smoothness index of these vectors is equal to
thecorrespondingeigenvalue i. Inclusteringthedatainto
twoclustersonlytheeigenvector u2isused(Fiedlervector)
since the vector u1is omitted as its elements are constant.
If we want to get a few clusters ( Qclusters) then we can
useKthesmoothesteigenvectors, u2,u35,uK+1,written
in the matrix form as
B=u2u35uK+1=b
f
f
fdu12u135u1.K+1/
u22u235u2.K+1/
4 4 7 4
uN2uN35uN.K+1/c
g
g
ge;
(16)
whereNfeatures with Kdimension are considered.
The clusters are determined based on the K-dimensional
spectralsimilarityvectors, q1= [u12; u13;§;u1.K+1/],q2=
[u22; u23;§;u2.K+1/],§,qN= [uN2; uN3;§;uN.K+1/],
deﬁned for vertices (features F1,F2,§,FN).
Inthisway,thedimensionofthemeasuringdistanceis
signiﬁcantly reduced from the original Ndimensional space
ind.i;j/ =ððFi*Fiððtoaverylow K-dimensionalspaces
of spectral vectors qn.
Finally, the clustering result (the data grouped into Q
clusters) can be reﬁned using K-means and the Euclidean
distanced.i;j/ =ððFi*Fiðð.
Note that the traditional K-means algorithm can be used
withaninitialrandomclusteringoffeaturemapsinto Qclus-
ters, with a slower convergence due to random initialization.
In this case, all the feature maps are grouped into Qinitial
Z. Feng et al.: Preprint submitted to Elsevier Page 4 of 10clusters, Qq,q= 1;2;§;Q. Means of the feature maps
are calculated for each cluster, Mq= mean.Fn;nËQq/.
Thedistanceofeachfeaturemapischeckedwithrespectto
eachofthemean Mq. Thefeaturemapisreassignedtothe
cluster whose mean is the closest to the considered feature
map. After all feature maps are considered, the means are
recalculated for the new clusters. The procedure is repeated
until no feature map changes its cluster.
3.2. Cluster-CAM
Now we are ready to introduce spectral clustering and K-
means in Cluster-CAM. Here the feature maps, F, represent
the vertices in (10). Take Euclidean distance as similarity
measurement, (10) can be expressed as:
S.i;j/ = exp ^* ððFi*Fjðð`; (17)
whereashorterdistancemeansahighersimilarity. Bysub-
stituting(17)into(11),(13),(14),and(16),wecansplit N
feature maps into Qclusters, Qq,q= 1;2;§;Q,Q ~ N.
Thenwecanobtainthe Qrepresentativefeaturemaps, F=
[F1;F2;§;FQ], by calculating the mean of feature maps in
each cluster, as
Fq= mean^Fn; nËQq`; q= 1;5;Q: (18)
Next we obtain the Hadamard product of FandX(F
will be upsampled to the same size of X). This processing
can be deemed as ﬁltering that mainly passes those elements
corresponding to large values in F. The predicted score of
each masked image is computed as:
y=[y1;y2;§;yQ]T
=[Sc;1.F1ýX/;§;Sc;Q.FQýX/]: (19)
Inthiscase,wecanobtainthecognition-basemapandcog-
nition scissors as
Fbase=Fqmax; qmax= arg
qmax.y/ (20)
Fscissors =Fqmin; qmin= arg
qmin.y/: (21)
Next, we can semantically couple the cognition-base map
and cognition-scissors to form the salience heatmap, as:
HCluster=Fbase* .1 */Fscissors; (22)
whereË [0;1]is a balance factor to adjust the importance
of cognition-base map and cognition-scissors.
4. Experiments
In this section, we will present and analyze the perfor-
mance of Cluster-CAM from various perspectives. Firstly
wewillbrieﬂydescribethedatasetusedinourexperiments.
ThenweverifythesuperiorityofCluster-CAMtootherex-
isting CAMs.4.1. Experimental Setup
Dataset: In the following experiments, CNNs are trained
on a prevalent benchmark, i.e., ILSVRC [ 4]. In ILSVRC,
there are around 1.2 million images with 1000categories for
training, and 50thousand images with 1000categories for
validation.
Network Structure: In this paper, several classic CNNs,
AlexNet[ 9]andVGG-16[ 25],areusedasclassiﬁcationmod-
els. Alex-Net is proposed by A. Krizhevsky et al., which
consists of 5convolutional units (a stack of convolutional
layers,ReLU,andmaxpooling)and 3fully-connectedlayers.
VGG-16 is a very deep CNN with 13convolutional layers
and3fully-connected layers. VGG-16 has approximately
134M trainable parameters regardless of the output layer.
4.2. Performance of Discriminative Localization
Fig. 1 shows the salience heatmaps of diﬀerent input im-
ages(indigoﬁnch,eagle,rooster,andostrich)byGrad-CAM,
Grad-CAM++, Ablation-CAM, Score-CAM, and Cluster
CAM.Visually,incomparisontoexistingCAMs,thehigh-
lighted region produced by Cluster-CAM mostly matches
human’s intuitive understanding of the discriminative part
ofthespeciﬁcobject. Taketheindigoﬁnchasanexample,
Grad-CAM only highlights the head of the bird, whereas
Grad-CAM++ and Ablation-CAM highlight the complete
ﬁnch but the branch (object-irrelevant information) is also
included. Score-CAM and Cluster-CAM highlight the ﬁnch
bodywithoutthebranch. Butobviously,theregionproduced
by Cluster-CAM matches the proﬁle of the ﬁnch more pre-
cisely than Score-CAM.
4.3. CNN’s Cognitive Explanation
Cognition Analysis of Multi-objects Images: Images of
multiple objects are optimal samples to verify the rationality
of the cognition-base map and semantic-scissors in (20) and
(21). AswediscussedinSection3,areasonablecognition-
base map should incorporate the object-relevant informa-
tion as much as possible, while the corresponding cognition-
scissors should include such information as less as better.
Therefore,itisnecessarytocheckwhetherthecognition-base
mapandcognition-scissorscaninterchangeamulti-objects
imageifthe targetclassischanged toanotherobject. Fig.3
shows the cognition-base map and cognition-scissors of two
muti-objects images as well as the corresponding masked
images. There are two types of dogs in the ﬁrst image, i.e.
elkhound (the big gray dog on the left) and spaniel (the tiny
browndogontheright). Ifthetargetclassiselkhound,the
third and the fourth clustered feature map are cognition-base
mapandcognition-scissors,respectively(markedbygreen
and red squares). It matches human’s cognition because
cognition-base map incorporates both objects and cognition-
scissors only selects the spaniel, thus the highlighted region
willonlybeconcentratedontheelkhound,asshowninthe
ﬁrst row in the top-left subﬁgure in Fig. 3. When the tar-
get class is changed to the spaniel, the cognition-base map
and cognition-scissors are also interchanged, as shown in the
third row in the top-right subﬁgure in Fig. 3. The same
phenomenon also emerges in indigo ﬁnch and goldﬁnch,
Z. Feng et al.: Preprint submitted to Elsevier Page 5 of 10elkhound
 Grad
 Grad++
 Ablation
 Score
 Ours
spaniel
 Grad
 Grad++
 Ablation
 Score
 Ours
0.140
 0.007
 0.261
 0.000
 0.051
 0.054
0.000
 0.000
 0.000
 0.140
 0.000
 0.000
indigo finch
 Grad
 Grad++
 Ablation
 Score
 Ours
goldfinch
 Grad
 Grad++
 Ablation
 Score
 Ours
/uni00000013/uni00000011/uni00000013/uni00000014/uni0000001c
 /uni00000013/uni00000011/uni00000017/uni0000001c/uni00000014
 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014
 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000019
 /uni00000013/uni00000011/uni00000016/uni00000013/uni0000001b
0.963
 0.004
 0.316
 0.823
 0.073Figure 3: The analysis of feature maps for images of multiple objects. The heatmaps
produced by diﬀerent CAMs for the target label elkhound (ﬁrst row in the top-left subﬁgure).
The clustered feature maps ( the second row in the top-left subﬁgure) and corresponding
masked images (the third row in the top-left subﬁgure). Note that the cognition-base map
and cognition-scissors are marked by green and red squares, respectively. The predicted
score for the current class is provided above each masked image. The results with the
target label spaniel are organized in the same structure (top-right subﬁgure). The indigo
ﬁnch and goldﬁnch are shown in the bottom-left and bottom-right subﬁgures, respectively.
as shown in the bottom subﬁgures in Fig. 3. In this case,
Cluster-CAM provides solid evidence that CNN’s recogni-
tion mechanism is similar to human cognition in multiple
objects classiﬁcation.
CognitionAnalysisofFine-grainedImages: Tofurtherun-
derstand howCNNutilizes the learned informationto make
decisions,wecanuseCAMtointerpretCNNsinﬁne-grained
image classiﬁcation. Fine-grained classiﬁcation aims to dis-
tinguish subordinate categories within entry-level categories.
Examples include recognizing species of birds such as north-
erncardinalorindigobunting;monkeyssuchasguenonor
langur. Fine-grained classiﬁcation often requires much more
detailedinformationcomparedwithgenericobjectrecogni-
tion, like the texture of the skin, the thickness of the fur, etc,
soCAMsonﬁne-grainedimagescantellwhethertheinfor-
mationisreasonablylearnedbyCNNforclassiﬁcation. Fig.4
showstheheatmapsgeneratedbyseveralmentionedCAMs
giventheinputimageofaguenonintheﬁrstrow. Interest-
ingly, they focus on completely diﬀerent parts of the guenon.
Grad-CAMandGrad-CAM++highlighttheguenon’seyes
and cheek, respectively. Ablation-CAM and Score-CAM
both highlight the guenon’s face, whereas Cluster-CAM onlyhighlights the guenon’s forehead. Intuitively, Ablation-CAM
and Score-CAM seem the most reasonable but the cognition-
basemapandcognition-scissorsclearlyshowthatthefore-
headisthemostdiscriminativepartbutthefaceisnegative
forguenon’sclassiﬁcation. Itwillbeunderstoodifwefurther
study the diﬀerence in species between guenon and langur.
Guenon (widely distributed in Africa) is characterized by
blond hair on the forehead and a busty white lip, whereas,
langur (distributed in Asia) is characterized by a completely
black face. We mark their characteristics by green and red
circlesinthethirdrowinFig.4. Itisthereasonwhythethird
featuremap(face)isdeemedascognition-scissors,i.e.,the
black face is an interference factor for guenon’s categoriz-
ing. Thisexampleperfectlydemonstratestherationalityof
Cluster-CAM, particularly the cognition-scissors.
4.4. Ablation Study
Analysisof DiﬀerentLayers: MostCNNsareconstructed
by a cascade of convolutional blocks (a block consists of
convolutional layers, nonlinear activation, pooling operation,
etc.). Fig. 5 shows the salience heatmaps of diﬀerent con-
volutional blocks in VGG-16. The results basically match
Z. Feng et al.: Preprint submitted to Elsevier Page 6 of 10original
 Grad
 Grad++
 Ablation
 Score
 Ours
/uni00000013/uni00000011/uni0000001a/uni00000016/uni00000019
 /uni00000013/uni00000011/uni0000001b/uni00000016/uni00000019
 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000019
 /uni00000013/uni00000011/uni0000001c/uni00000014/uni00000013
 /uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000014
guenon: golden hair and white mouth langur: black faceFigure 4: The cognition-base map (green square) and cognition-
scissors (red square) in merged feature maps (top). The images
are masked by corresponding feature masks as well as the pre-
dicted score (middle). Nine images of guenon and nine images
of langur (bottom). Note that the discriminative characteristics
of guenon (golden hair and white mouth) and langur (black
face) are labeled with green and red circles, respectively.
/uni0000002f/uni00000020/uni00000017
 /uni0000002f/uni00000020/uni0000001c
 /uni0000002f/uni00000020/uni00000014/uni00000019
 /uni0000002f/uni00000020/uni00000015/uni00000016
 /uni0000002f/uni00000020/uni00000016/uni00000013
Figure 5: The heatmaps produced by Cluster-CAM with diﬀer-
ent numbers of clusters for VGG-16. Lrefers to the indices of
layers in VGG-16.
human’sintuitionthattheshallowlayersmainlycapturesome
detailedinformation(e.g.,textureandedge),whereasdeep
layers concentrate on those parts with clearer semantics.
NumberofClustersandCNNStructures: Thenumberof
clusters usually plays a critical role in clustering algorithms.
Here we vary the cluster number from 2to8and present the
correspondingsalienceheatmapsforAlexNetandVGG-16
/uni00000034/uni00000020/uni00000015
 /uni00000034/uni00000020/uni00000016
 /uni00000034/uni00000020/uni00000017
 /uni00000034/uni00000020/uni00000018
 /uni00000034/uni00000020/uni00000019
 /uni00000034/uni00000020/uni0000001a
 /uni00000034/uni00000020/uni0000001b
/uni00000034/uni00000020/uni00000015
 /uni00000034/uni00000020/uni00000016
 /uni00000034/uni00000020/uni00000017
 /uni00000034/uni00000020/uni00000018
 /uni00000034/uni00000020/uni00000019
 /uni00000034/uni00000020/uni0000001a
 /uni00000034/uni00000020/uni0000001b
Figure 6: The heatmaps produced by Cluster-CAM (K-means)
with diﬀerent numbers of clusters for AlexNet (top) and VGG-
16(bottom).
in Fig. 6. It shows the salience heatmaps are sensitive to
the number of clusters and are diﬀerent with CNN models.
For AlexNet, the highlighted region in heatmaps could be
semantic chaos if the feature maps are split into too many
or too few clusters. It is probably because a large number
ofclustersmayintroducetoomanydetailedpatternsofthe
object and a small number of clusters may directly include
backgroundinformation. Therefore,thenumberofclusters
should be selected as a median value. Note it is only an
empirical conclusion and exclusion exists that the optimal
value is 2for the fourth row (beer) in Fig.6. It is probably
becausetheobjectissimpleandinregularshape,thusonly
twoclustersareenoughtorepresentallnecessaryinformation.
ForVGG-16,thehighlightedregionismoreconcentratedona
speciﬁcpartoftheobjectthanAlexNet. Itisprobablybecause
more detailed discriminative information could be captured
in VGG-16 which has much deeper layers than AlexNet.
Clustering Method: In Section 3, we introduced two clus-
teringalgorithms,i.e.,K-meansandspectralclustering. Here
we take each feature map as a vertex in the graph and use
distancetoconstructthesimilaritymatrix,adjacentmatrix,
degreematrix, andLaplacianmatrixusing(17), (10), (11),
and (14). Fig. 7 shows the salience heatmaps produced by
spectral clustering with diﬀerent clusters and diﬀerent eigen-
vectors. It can be observed that the heatmaps are highly
related to the number of eigenvectors rather than the clus-
ters. Notethattheoptimalnumberofeigenvectorsishighly
relatedtothe imageitself,thuscareful manipulationofthis
parameterisrequiredfordiﬀerentobjectstoobtainthebest
heatmap. Therefore we will only use K-means to compute
the qualitative evaluation metrics in the next section.
Z. Feng et al.: Preprint submitted to Elsevier Page 7 of 10/uni00000034/uni00000020/uni00000015
 /uni00000034/uni00000020/uni00000016
 /uni00000034/uni00000020/uni00000017
 /uni00000034/uni00000020/uni00000018
 /uni00000034/uni00000020/uni00000019
 /uni00000034/uni00000020/uni0000001a
 /uni00000034/uni00000020/uni0000001b
/uni00000015
 /uni00000016
 /uni00000017
 /uni00000018
 /uni00000019
 /uni0000001a
Figure 7: The heatmaps produced by Cluster-CAM with dif-
ferent numbers of clusters by spectral clustering (top) and
heatmaps produced by Cluster-CAM with diﬀerent numbers of
eigenvectors (bottom).
4.5. Qualitative Evaluation
4.5.1. Performance Evaluation
TofurtherevaluatetheinterpretingperformanceofCluster-
CAM quantitatively, two widely-used evaluation metrics are
adopted in this paper, i.e., conﬁdence drop and increase
number [ 36,41]. First of all, let’s think about what kind
of heatmap can be regarded as a good interpretation of CNN.
Anaturalandintuitiveideaistomeasurehowmuchtheconﬁ-
dence(predictedscore)ofthetargetclasswilldropwhenthe
originalimageispartlyoccludedaccordingtotheheatmap.
Speciﬁcally, for each image, a corresponding explanation
mapLcis generated by element-wise multiplication of the
heatmaps and the current image as in (9) and (8).
Conﬁdence drop : This metric compares the average drop
ofthemodel’sconﬁdenceforaparticularclassinanimage
after occlusion as:
confidence _drop =Sc.X/ *Sc.XýHn/
Sc.X/;(23)
Forexample,assumethatCNNpredictsanobjectindigoﬁnch
inanimageXwithconﬁdence 0:8. Whenweinputtheexpla-
nationmap,XýH,ofthisimage,theCNN’sconﬁdencein
the class indigo ﬁnch falls to 0:6. Then the conﬁdence_drop
would be 25~. It means that the most discriminative part
(75~)isincludedinthehighlightedregion. Conﬁdencedrop
is expected to be lower for a better CAM and is usually aver-
aged over many images.
Increasenumber measureshowmanytimestheCNN’spre-
dictionscorefor cincreasedwhenthemaskedimageisinput.
Speciﬁcally, it happens sometimes that the object is com-
pletelyincludedandinterferencepartsareoccluded(e.g.,the
object-irrelevant parts and background) in the highlighted
region. Inthiscase,therewillbeanincreaseintheCNN’sTable 1
Performance Evaluation Metrics.
Method Conﬁdence drop Increase number ~
Grad-CAM 17:94 19 :15
Grad-CAM++ 18:44 19 :75
Ablation-CAM 12:38 24 :67
Score-CAM 12:21 25 :48
Cluster-CAM 11:60 26 :10
Table 2
Eﬃciency Evaluation Metrics. Here Q= 6in Cluster-CAM.
Method Computing time number of FP 
Grad-CAM 0:078 1
Grad-CAM++ 0:141 1
Ablation-CAM 2:206 256
Score-CAM 4:647 256
Cluster-CAM 0:382 6
predicted score for the class (i.e., conﬁdence drop <0). This
value is computed as a percentage through the whole dataset.
Table 1 shows two evaluation metrics of the entire val-
idation set in ILSVRC dataset ( means the lower value is
better and ~means the higher value is better). These two
metrics clearly demonstrate the superiority of Cluster-CAM
tootherexistingCAMs. ThemetricsarecomputedinPytorch
1.8.0+cudnn11.1, NVIDIA RTX-3070.
4.5.2. Eﬃciency Evaluation
Here we present two eﬃciency metrics, i.e., the average
computing time and the number of forward propagation (FP)
per image in Table 2. It is clear that Cluster-CAM greatly
reduces the number of FP compared with Ablation-CAM
andScore-CAM.Naturally,asigniﬁcantimprovementineﬃ-
ciency emerges from Cluster-CAM, i.e., Cluster-CAM is 5:7
timesfasterthanAblation-CAMand 12:1timesfasterthan
Score-CAM. Therefore, Cluster-CAM can obtain better visu-
alization and interpretation performance than gradient-based
and gradient-free CAMs with eﬃciency closer to gradient-
based CAMs.
5. Conclusion
In this paper, we proposed Cluster-CAM, an eﬀective
andeﬃcientCNNinterpretationtechniquebasedonunsuper-
visedclusteringalgorithms. Cluster-CAMistheﬁrstattempt
to comprehensively analyze how to split feature maps into
diﬀerent groups and provide an artful strategy to remove the
object-irrelevant elements by deﬁning cognition-scissors. In
Cluster-CAM,onlyseveraltimesofforwardpropagationis
required per image while it is usually more than hundreds
forothergradient-freeCAMs. Qualitativeandquantitative
experimentalresultsveriﬁedCluster-CAMcanobtaineven
betterperformancethangradient-freeCAMswithmuchlower
computing time.
Z. Feng et al.: Preprint submitted to Elsevier Page 8 of 10Data Availability Statements
ILSVRC dataset can be downloaded from the website
https://www.image-net.org/challenges/LSVRC/.
Acknowledgments
ThisworkisfundedbytheNationalNaturalScienceFoun-
dationofChina(GrantNo. 62276204,61871301,62071349),
Project2021ZDZX-GY-0001,scienceandtechnologyproject
of Xianyang city.
References
[1]Cao, J., Pang, Y., Han, J., Li, X., 2019. Hierarchical shot detector, in:
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision, pp. 9705–9714.
[2]Chattopadhay,A.,Sarkar,A.,Howlader,P.,Balasubramanian,V.N.,
2018. Grad-CAM++: Generalizedgradient-basedvisualexplanations
for deep convolutional networks, in: In Proceedings of 2018 IEEE
Winter Conference on Applications of Computer Vision (WACV),
IEEE. pp. 839–847.
[3]Chen, H., Jin, Y., Jin, G., Zhu, C., Chen, E., 2022. Semisupervised
semantic segmentation by improving prediction conﬁdence. IEEE
Transactions on Neural Networks and Learning Systems 33, 4991–
5003. doi: 10.1109/TNNLS.2021.3066850 .
[4]Deng,J., Dong,W.,Socher,R.,Li, L.J.,Li,K., Fei-Fei, L.,2009. Ima-
genet: Alarge-scalehierarchicalimagedatabase,in: Inproceedingsof
2009 IEEE Conference onComputer Vision and Pattern Recognition
(CVPR), pp. 248–255. doi: 10.1109/CVPR.2009.5206848 .
[5]Feng, Z., Ji, H., Stankovi/uni0107, L., Fan, J., Zhu, M., 2021a. SC-SM
CAM:AneﬃcientvisualinterpretationofCNNforSARimagestarget
recognition. Remote Sensing 13, 4139.
[6]Feng, Z., Zhu, M., Stankovi/uni0107, L., Ji, H., 2021b. Self-matching CAM:
A novel accurate visual explanation of CNNs for SAR image interpre-
tation. Remote Sensing 13, 1772.
[7]Fu,R.,Hu,Q.,Dong,X.,Guo,Y.,Gao,Y.,Li,B.,2020. Axiom-based
Grad-CAM: Towards accurate visualization and explanation of CNNs,
in: In Proceedings of the 2020 British Machine Vision Conference
(BMVC 2020).
[8]He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning
for image recognition, in: In Proceedings of 2016 IEEE conference
onComputerVisionandPatternRecognition(CVPR),pp.770–778.
doi:10.1109/CVPR.2016.90 .
[9]Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classiﬁca-
tion withdeep convolutional neuralnetworks,in: Pereira, F., Burges,
C.,Bottou,L.,Weinberger,K.(Eds.),AdvancesinNeuralInformation
Processing Systems, Curran Associates, Inc.
[10]Lapuschkin,S.,Wäldchen,S.,Binder,A.,Montavon,G.,Samek,W.,
Müller, K.R., 2019. Unmasking clever hans predictors and assessing
what machines really learn. Nature communications 10, 1–8.
[11]Liang, X., Hu, Z., Zhang, H., Lin, L., Xing, E.P., 2018. Symbolic
graph reasoning meets convolutions. Advances in Neural Information
Processing Systems 31.
[12]Liu, J., Zhang, F., Zhou, Z., Wang, J., 2023. Bfmnet: Bilateral fea-
ture fusion network with multi-scale context aggregation for real-time
semantic segmentation. Neurocomputing 521, 27–40. doi: https:
//doi.org/10.1016/j.neucom.2022.11.084 .
[13]Liu,K.,Meng,R.,Li,L.,Mao,J.,Chen,H.,2022a. Sisl-net: Saliency-
guided self-supervised learning network for image classiﬁcation. Neu-
rocomputing 510, 193–202. doi: https://doi.org/10.1016/j.neucom.
2022.09.029 .
[14]Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.,
2022b. A convnet for the 2020s, in: In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp.
11976–11986.[15]Ma, X., Zhang, S., Pena-Pena, K., Arce, G.R., 2021. Fast spectral
clusteringmethodbasedongraphsimilaritymatrixcompletion. Signal
Processing 189, 108301. doi: https://doi.org/10.1016/j.sigpro.2021.
108301.
[16]Macpherson, T., Churchland, A., Sejnowski, T., DiCarlo, J., Kamitani,
Y., Takahashi, H., Hikida, T., 2021. Natural and artiﬁcial intelligence:
A brief introduction to the interplay between ai and neuroscience
research. Neural Networks 144, 603–613. doi: https://doi.org/10.
1016/j.neunet.2021.09.018 .
[17]Omeiza, D., Speakman, S., Cintas, C., Weldermariam, K., 2019.
Smooth grad-cam++: An enhanced inference level visualization tech-
niquefordeepconvolutionalneuralnetworkmodels. arXivpreprint
arXiv:1908.01224 .
[18]Ramaswamy,H.G.,etal.,2020. Ablation-cam: Visualexplanationsfor
deepconvolutionalnetworkviagradient-freelocalization,in: InPro-
ceedingsoftheIEEEWinterConferenceonApplicationsofComputer
Vision (WACV), pp. 983–991.
[19]Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only
lookonce: Uniﬁed,real-timeobjectdetection,in: InProceedingsof
2016 IEEE Conference onComputer Vision and Pattern Recognition
(CVPR), pp. 779–788. doi: 10.1109/CVPR.2016.91 .
[20]Ren, J., Li, M., Liu, Z., Zhang, Q., 2021. Interpreting and disentan-
gling feature components of various complexity from DNNs, in: In
proceedingsofInternationalConferenceonMachineLearning,PMLR.
pp. 8971–8981.
[21]Saleem, R., Yuan, B., Kurugollu, F., Anjum, A., Liu, L., 2022. Ex-
plaining deep neural networks: A survey on the global interpretation
methods. Neurocomputing 513, 165–180. doi: https://doi.org/10.
1016/j.neucom.2022.09.129 .
[22]Scalzo, B., Stankovi/uni0107, L., Dakovi/uni0107, M., Constantinides, A.G., Mandic,
D.P., 2023. A class of doubly stochastic shift operators for random
graph signals and their boundedness. Neural Networks 158, 83–88.
doi:https://doi.org/10.1016/j.neunet.2022.10.035 .
[23]Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D.,
Batra, D., 2017. Grad-CAM: Visual explanations from deep networks
viagradient-basedlocalization,in: InProceedingsofthe2017IEEE
international conference on computer vision, pp. 618–626.
[24]Simonyan,K.,Vedaldi,A.,Zisserman,A.,2013. Deepinsideconvolu-
tional networks: Visualising image classiﬁcation models and saliency
maps. arXiv preprint arXiv:1312.6034 .
[25]Simonyan,K.,Zisserman.,A.,2015.Verydeepconvolutionalnetworks
for large-scale image recognition, in: 3rd International Conference on
Learning Representations (ICLR 2015), pp. 1–14.
[26]Spinelli, I., Scardapane, S., Uncini, A., 2022. A meta-learning ap-
proach for training explainable graph neural networks. IEEE Trans-
actionsonNeuralNetworksandLearningSystems,1–9doi: 10.1109/
TNNLS.2022.3171398 .
[27]Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.,
2021. Bottleneck transformers forvisual recognition,in: In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
recognition (CVPR), pp. 16519–16529.
[28]Stankovic,L.,Dakovic,M.,Sejdic,E.,2017. Vertex-frequencyanal-
ysis: A way to localize graph spectral components [lecture notes].
IEEESignalProcessingMagazine34,176–182. doi: 10.1109/MSP.2017.
2696572.
[29]Stankovic,L.,Mandic,D.P.,Dakovic,M.,Kisil,I.,Sejdic,E.,Constan-
tinides,A.G.,2019. Understandingthebasisofgraphsignalprocessing
via an intuitive example-driven approach. IEEE Signal Processing
Magazine 36, 133–145. doi: 10.1109/MSP.2019.2929832 .
[30] Sun, S., Song, B., Cai, X., Du, X., Guizani, M., 2022. CAMA: Class
activationmappingdisruptiveattackfordeepneuralnetworks. Neu-
rocomputing500,989–1002. doi: https://doi.org/10.1016/j.neucom.
2022.05.065 .
[31]Tan, R., Gao, L., Khan, N., Guan, L., 2022. Interpretable artiﬁcial
intelligencethroughlocalityguidedneuralnetworks. NeuralNetworks
155, 58–73. doi: https://doi.org/10.1016/j.neunet.2022.08.009 .
[32]Townsend, J., Chaton, T., Monteiro, J.M., 2020. Extracting relational
explanations from deep neural networks: A survey from a neural-
Z. Feng et al.: Preprint submitted to Elsevier Page 9 of 10symbolic perspective. IEEE Transactions on Neural Networks and
Learning Systems 31, 3456–3470. doi: 10.1109/TNNLS.2019.2944672 .
[33]Tu, Z., Zhou, A., Gan, C., Jiang, B., Hussain, A., Luo, B., 2021.
A novel domain activation mapping-guided network (DA-GNT) for
visual tracking. Neurocomputing 449, 443–454. doi: https://doi.org/
10.1016/j.neucom.2021.03.056 .
[34]Vlahek, D., Mongus, D., 2021. An eﬃcient iterative approach to
explainable feature learning. IEEE Transactions on Neural Networks
and Learning Systems , 1–13doi: 10.1109/TNNLS.2021.3107049 .
[35]Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel,
P., Hu, X., 2020. Score-CAM: Score-weighted visual explanations
for convolutional neural networks, in: In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)
workshops, pp. 24–25.
[36]Zeiler, M.D., Fergus, R., 2014. Visualizing and understanding con-
volutional networks, in: European conference on computer vision,
Springer. pp. 818–833.
[37]Zhang, Q., Rao, L., Yang, Y., 2021. Group-CAM: Group score-
weighted visual explanations for deep convolutional networks. arXiv
preprint arXiv:2103.13859 .
[38]Zhao,Z.,Xie,X.,Wang,C.,Liu,W.,Shi,G.,Du,J.,2019. Visualizing
andunderstandingoflearnedcompressivesensingwithresidualnet-
work. Neurocomputing359,185–198. doi: https://doi.org/10.1016/
j.neucom.2019.05.043 .
[39]Zhou, B., Bau, D., Oliva, A., Torralba, A., 2019. Interpreting deep
visualrepresentationsvianetworkdissection. IEEETransactionson
Pattern Analysis and Machine Intelligence 41, 2131–2145. doi: 10.
1109/TPAMI.2018.2858759 .
[40]Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A., 2016.
Learning deep features for discriminative localization, in: In Proceed-
ings of the 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2921–2929.
[41]Zhou,K.,Kainz,B.,2018. Eﬃcientimageevidenceanalysisofcnn
classiﬁcation results. arXiv preprint arXiv:1801.01693 .
[42]Zhu, M., Feng, Z., Stankovi/uni0107, L., Ding, L., Fan, J., Zhou, X., 2022.
Aprobe-featureforspeciﬁcemitteridentiﬁcationusingaxiom-based
grad-cam. Signal Processing , 108685.
[43]Zhu,Y.,Zhao,C.,Wang,J.,Zhao,X.,Wu,Y.,Lu,H.,2017. Couplenet:
Coupling global structure with local parts for object detection, in:
Proceedings of the IEEE international conference on computer vision,
pp. 4126–4134.
Zhenpeng Feng was born in Xianyang, Shaanxi,
Chinain1996. HereceivedaB.E.degreeinSchool
of Electronic Engineering, Xidian University in
2019. HeiscurrentlyaPh.D.studentinexplainable
artiﬁcial intelligence at School of Electronic Engi-
neering, Xidian University. He is also a visiting
studentintheUniversityofMontenegro,working
withProf. LjubišaStankovi/uni0107’sresearchteam. His
researchinterestsincludeinterpretingdeepneural
networks and signal processing.
Hongbing Ji received a B.S. degree in radar en-
gineering, an M.S. degree in circuit, signals, and
systems, and the Ph.D. degree in signal and infor-
mationprocessingfromXidianUniversity,Xi’an,
China, in 1983, 1989, and1999, respectively. He is
currentlyafullprofessoratXidianUniversityand
a senior member of IEEE. His research interests
includepatternrecognition,radarsignalprocessing,
and multi-sensor information fusion.
Miloš Dakovi/uni0107 was born in 1970 in Nikši/uni0107, Mon-
tenegro. He received a B.S. in 1996, an M.S. in
2001, and a Ph.D. in 2005, all in electrical engi-
neering from the University of Montenegro. He
is a full professor at the University of Montene-
gro. His research interests are in signal process-
ing, time-frequency signal analysis, compressive
sensing,radarsignalprocessing,andgraphsignal
processing.
Xiyang Cui was born in Handan, Hebei, China
in 1997. He received the B.E. degree and M.E.
degreeinElectronicInformationEngineeringand
ElectricalCircuitSystemfromSchoolofElectronic
Engineering,XidianUniversityin2019and2021,
respectively. He is currently an investigator of an
electronic company and collaborates with Zhen-
pengFengandProf. LjubišaStankovi/uni0107inscientiﬁc
research. His research interests include electrical
circuit design and image processing.
Mingzhe Zhu was born in China in 1982. He re-
ceived a B.S. degree in signal and information pro-
cessing,aPh.D.degreeinpatternrecognitionand
intelligentsystemfromXidianUniversityin2004
and 2010, respectively. He is currently an asso-
ciateprofessoratSchoolofElectronicEngineering,
Xidian University. His research interests include
non-stationary signal processing, time-frequency
analysis, and target recognition.
LjubišaStankovi/uni0107 was born in Montenegro, 1960.
HewasattheRuhrUniversityBochum,1997-1999,
supportedbytheAvHFoundation. Stankovicwas
theRectoroftheUniversityofMontenegro2003-
2008, the Ambassador of Montenegro to the UK,
2011-2015, and a visiting academic to the Imperial
College London, 2012-2013. He published almost
200 journal papers. He is a member of the Na-
tionalAcademyofScienceandArts(CANU)and
theAcademiaEuropaea. Stankovi/uni0107wontheBestpa-
perawardfromtheEURASIPin2017andtheIEEE
SPM Best Column Award for 2020. Stankovi/uni0107 is
aprofessorattheUniversityofMontenegroanda
Fellow of the IEEE.
Z. Feng et al.: Preprint submitted to Elsevier Page 10 of 10