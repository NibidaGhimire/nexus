Incremental Few-Shot Object Detection via Simple Fine-Tuning
Approach
Tae-Min Choi1and Jong-Hwan Kim1,Fellow, IEEE
Abstract â€” In this paper, we explore incremental few-shot ob-
ject detection (iFSD), which incrementally learns novel classes
using only a few examples without revisiting base classes. Previ-
ous iFSD works achieved the desired results by applying meta-
learning. However, meta-learning approaches show insufï¬cient
performance that is difï¬cult to apply to practical problems. In
this light, we propose a simple ï¬ne-tuning-based approach, the
Incremental Two-stage Fine-tuning Approach (iTFA) for iFSD,
which contains three steps: 1) base training using abundant base
classes with the class-agnostic box regressor, 2) separation of the
RoI feature extractor and classiï¬er into the base and novel class
branches for preserving base knowledge, and 3) ï¬ne-tuning the
novel branch using only a few novel class examples. We evaluate
our iTFA on the real-world datasets PASCAL VOC, COCO, and
LVIS. iTFA achieves competitive performance in COCO and
shows a 30% higher AP accuracy than meta-learning methods
in the LVIS dataset. Experimental results show the effectiveness
and applicability of our proposed method1.
I. INTRODUCTION
Deep learning using convolutional neural networks has
recently made signiï¬cant progress in computer vision [1],
[2], [3], [4], [5]. These works, however, are usually de-
veloped in a supervised learning scheme that relies on
abundant labeled datasets. Large-scale dataset-based training
requires enormous human resources for data collection and
annotation. It is also challenging to adapt models to novel
classes with scarce data. As a result, few-shot learning has
become a key area for many computer vision researchers.
In few-shot learning, it is difï¬cult to train a model directly
from a few novel data; therefore, we train general knowledge
to the model with abundant base classes and transfer it to
the novel classes. Yet, most of the work is focused on image
classiï¬cation tasks. Few-shot object detection (FSOD) is a
more challenging task because it needs to train object recog-
nition and localization using a small training set. Several
methods [6], [7], [8], [9], [10], [11], [12], [13], [14] have
been presented to overcome FSOD; meta-learning and ï¬ne-
tuning based methods have been the main approaches to
FSOD. Meta-learning studies have introduced a learning-to-
learn scheme. They learn general knowledge by episodically
training the model with base class data and transferring it to
Corresponding author
1Tae-Min Choi and Jong-Hwan Kim are with the School of Electrical
Engineering, Korea Advanced Institute of Science and Technology, Daejeon
34141, Korea (e-mail: tmchoi@rit.kaist.ac.kr; johkim@rit.kaist.ac.kr).
This work was supported by the Institute of Information & commu-
nications Technology Planning & Evaluation (IITP) grant funded by the
Korea government (MSIT) (No. 2020-0-00440, Development of Artiï¬cial
Intelligence Technology that Continuously Improves Itself as the Situation
Changes in the Real World)
1Code is available at https://github.com/TMIU/iTFAthe novel classes. These works can adapt quickly without ex-
tra training but have shown low performance that is difï¬cult
to apply in real-world applications. Alternatively, ï¬ne-tuning
methods were developed based on the two-stage ï¬ne-tuning
approach (TFA) [6] and outperformed meta-learning works.
Many FSOD methods usually only focus on the perfor-
mance of the novel classes. Therefore, when adding the novel
classes, they can not preserve the knowledge of the base
classes. This leads to catastrophic forgetting. To overcome
this issue, TFA ï¬ne-tunes the last layers of the object detector
with a balanced dataset, including both the base and novel
classes, to prevent forgetting the base classes. In reality, novel
classes can be detected on the go due to the complex real-
world environment. This creates problematic settings that
cannot be revisited in the base classes. The FSOD methods
degrade in these incremental learning settings, wherein they
tend to forget base information when trained on a few novel
data. In contrast, humans can learn a new concept incremen-
tally using only small data and preserve prior knowledge
without accessing the base data. Motivated by this gap
between humans and machines, incremental few-shot object
detection (iFSD) was proposed by [15]. iFSD uses the same
dataset as FSOD with abundant base classes and a few novel
classes. Also, the training strategy of iFSD is the same as
FSOD; the model is ï¬rst trained with base classes, and then
the detector is incrementally learned using the novel classes.
However, unlike FSOD, iFSD focuses on the performance of
both the base and novel classes and transfers base knowledge
to the novel classes without access to base class data.
In this paper, we propose a simple ï¬ne-tuning approach
for iFSD, called iTFA. Inspired by TFA, we employ a two-
stage training strategy based on Faster R-CNN [3]. We ï¬rst
train the Faster R-CNN with abundant base classes and then
ï¬ne-tune the RoI feature extractor and classiï¬er using a few
novel classes. We focus on ï¬ne-tuning layer decisions and
model design by analyzing the structure of Faster R-CNN
and exploring the compatibility of the classiï¬er optimized
for iFSD.
TFA [6] prevented catastrophic forgetting due to a bal-
anced ï¬ne-tuning training set, including the base classes.
However, as mentioned above, we only access a few novel
classes after training with the base classes in iFSD. There-
fore, the two-stage training scheme must be modiï¬ed to be
suitable for iFSD. We train the object detector with a class-
agnostic regressor using abundant base class data. The RoI
feature extractor and classiï¬er are separated and connected
parallel to the base and novel branches. We also explore
the effect of ï¬ne-tuning layer decision of the RoI featurearXiv:2302.09779v1  [cs.CV]  20 Feb 2023extractor and examine the compatibility of the classiï¬er.
With these modiï¬cations, iTFA outperforms the baselines
on the novel class accuracy and overcomes catastrophic
forgetting. In this paper, iTFA is evaluated on the PASCAL
VOC [16], COCO [17], and LVIS [18] datasets for the iFSD
task. Experiments show the effectiveness and competitive
performance of our iTFA in all datasets.
To summarize, The contribution of this paper is three-
fold: (1) We propose the Incremental Two-stage Fine-tuning
Approach (iTFA), which is a simple ï¬ne-tuning strategy
optimized for iFSD; (2) Concretely, we leverage a class-
agnostic regressor trained on large-scale data. We also sep-
arate the base and novel branches to prevent catastrophic
forgetting and empirically explore compatibility between the
ï¬ne-tuning layer and classiï¬er for the accurate performance
on novel classes; (3) Extensive experiments on the iFSD
benchmark demonstrate higher performance on novel classes
and prevent degradation of the base class accuracy.
II. RELATED WORK
A. Few-shot object detection
FSOD aims to detect novel classes with a few labeled
novel class samples and abundant base class samples. Many
early works on FSOD apply meta-learning [13], [19], [12],
[11], which uses the base samples to train a meta-learner
to class agnostic knowledge so that they can be adapted to
novel classes without extra training. Recently, [6] introduced
TFA, which outperforms meta-learning-based methods. They
presented a simple training strategy that ï¬ne-tunes only the
last layers of the object detector. Following this approach,
various ï¬ne-tuning-based methods [9], [8], [14], [7] have
achieved state-of-the-art performance. Inspired by this work-
ï¬‚ow, we modify the TFA as a ï¬ne-tuning-based method for
iFSD benchmarks.
B. Incremental few-shot object detection
iFSD was ï¬rst proposed in [15]. Unlike FSOD, iFSD does
not access the base class images when learning novel classes.
Due to this assumption, catastrophic forgetting occurs; the
model adds novel classes, then performance degradation
occurs to the base classes. Like the FSOD workï¬‚ow, prior
works [15], [20] have developed meta-learning-based meth-
ods to solve these problems. ONCE [15] presented a class
code generator to generate classiï¬cation heads for novel
classes. Sylph [20] developed a code predictor head and
a code process module to solve iFSD without additional
training and forgetting the base classes. Meta-learning-based
methods can be directly adapted to novel classes; however,
they have low accuracy. Better performance is needed for its
application in a practical scenario. Fine-tuning-based works
[21], [22] have been presented to achieve more accurate
performance. They leverage knowledge distillation loss to
overcome catastrophic forgetting. However, their network
structure and training methods do not preserve the base class
performance. To achieve higher accuracy in both the base
and novel classes, we propose iTFA. This simple ï¬ne-tuningapproach is suitable for iFSD by modifying the structure and
training strategy of TFA.
III. METHOD
A. Formulation
We follow the iFSD settings introduced in [6], [20].
In iFSD, there are two sets of dataset with base classes,
Cbase, and novel classes, Cnovel . Base classes have abundant
annotated training data, and novel classes have few training
data (usually 110 per category). For an object detection
dataset, D=f(x; y); x2X; y2Yg, where xis the
input image and y=f(ci; bi); i= 1; : : : ; Ngdenotes
the categories cand bounding box coordinates bof the
Nobject instances in the image x. The goal of iFSD is
to incrementally train the object detector to recognize both
novel and base classes without revisiting base classes. As
shown in Fig. 1, we split the training stage into the base
model training stage andï¬ne-tuning stage . We ï¬rst pre-train
the object detector with abundant base classes in the base
model training stage. In this stage, we use Dbase containing
only base classes c2Cbase with sufï¬cient labeled images.
After the ï¬rst stage, we transfer prior knowledge to classify
the novel classes that can use only Kshots training data
while maintaining the performance of base classes. In the
ï¬ne-tuning stage, we use Dnovel containing novel classes
c2Cnovel withKexamples per class. In the test stage, our
work aims to train a model that performs well on both base
and novel classes, so we use Dtestthat contains all classes
c2(Cbase[Cnovel ) at the test stage.
B. Incremental two-stage ï¬ne-tuning approach
In this section, we introduce our iTFA method for iFSD.
As shown on the right side of Fig. 1(a), TFA uses base
classes in the ï¬ne-tuning stage to prevent forgetting the prior
knowledge. However, in iFSD, the object detector is ï¬ne-
tuned only using novel classes without access to the base
classes. TFA also freezes all structures before the box clas-
siï¬er, which disturbs the feature alignment of novel classes in
the iFSD setting. To overcome these limitations, we propose
iTFA, a simple ï¬ne-tuning approach optimized for iFSD. The
training strategy of our iTFA has three steps: 1) changing the
class-speciï¬c regressor to the class-agnostic regressor in the
base training stage, 2) separating the RoI feature extractor
(composed of two fc layers) and the classiï¬er of Faster R-
CNN [3] into the novel and base class branches, and 3)
analyzing the effect of the decision of ï¬ne-tuning layers
and compatibility of the classiï¬er, then ï¬ne-tuning the novel
branch.
1) Base model training: In the base training stage, we
train the feature extractor, the classiï¬er, and the box regressor
using the base classes dataset Dbase (left side of Fig. 1). We
use the same training strategy from Faster R-CNN [3]. The
loss term for training is as follows:
L=Lrpn+Lcls+Lloc; (1)(a)
(b)BackboneRPN
RoI
Featureğ¹ğ¹ğ¶ğ¶1ğ¹ğ¹ğ¶ğ¶2Box Regressor
Box Classifier
Base Classes
(Large -scale)
BackboneRPN
RoI
Featureğ¹ğ¹ğ¶ğ¶1ğ¹ğ¹ğ¶ğ¶2Box Classifier
Box RegressorBox Regressor
Box ClassifierFrozen
Novel Classes
(Few)
Base Classes
(Few)Stage 1: Base model training Stage 2: Fine -tuning
Class- specificBase
Novel
BackboneRPN
RoI
Featureğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘1ğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘2Box Regressor
Box Classifier
Base Classes
(Large -scale)
BackboneRPN
RoI
Featureğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘2ğµğµBox Regressor
Box Classifier
ğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘2ğ‘ğ‘Box ClassifierFrozen
ğ‘“ğ‘“(ï¿½;ğ‘Šğ‘Šğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘“ğ‘“(ï¿½;ğ‘Šğ‘Šğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğµğµ)
ğ¹ğ¹(ï¿½;ğ‘Šğ‘Šğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)ğ‘”ğ‘”(ï¿½;ğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘ğµğµ)
ğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘1ğµğµ
ğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘1ğ‘ğ‘
Novel Classes
(Few)Stage 1: Base model trainingStage 2: Fine -tuning
Class- agnostic
ğ‘”ğ‘”(ï¿½;ğ‘Šğ‘Šğ‘“ğ‘“ğ‘ğ‘ğ‘ğ‘)Novel branchBase branchFig. 1. Method overview. (a) the illustration of TFA, (b) the overall framework of iTFA. Both frameworks consist of two stages: base model training and
ï¬ne-tuning stages. The whole object detector is trained with large-scale base classes in the base model training stage. In the ï¬ne-tuning stage, each method
ï¬xes the components indicated by gray boxes and ï¬ne-tunes other structures. TFA uses both base and novel classes in the ï¬ne-tuning stage. However, iTFA
only uses novel classes, which causes catastrophic forgetting. To overcome catastrophic forgetting, iTFA divides the RoI feature extractor and classiï¬er
into two branches and ï¬ne-tunes only the novel branch. Also, the class-agnostic regressor is used for easier adaptation to novel classes.
where Lrpnis leveraged to determine whether the output
of Region Proposal Network (RPN) is foreground or back-
ground and to adjust the anchor position, Lclsis the cross-
entropy loss of the classiï¬er, and Llocis the L1loss for the
box regressor.
2) Class-agnostic box regressor: Previous iFSD methods
[15], [6], [22], [21] applied a class-speciï¬c box regressor
to the object detector. A class-speciï¬c box regressor can
generate a bounding box ï¬tted to each class. However, it
is challenging to train a class-speciï¬c regressor in few-shot
learning due to the few instances of novel classes. It is
difï¬cult to obtain various bounding box information with
a small number of instances because even the same class
has different bounding box characteristics depending on the
style and composition of the image. Therefore, we change the
class-speciï¬c regressor to the class-agnostic box regressor in
the base training stage and use it to predict the bounding
boxes of both base and novel classes. The class-agnostic
regressor can be leveraged for the novel classes because it has
been pre-trained on the abundant dataset. When in the ï¬ne-
tuning stage, the regressor is frozen, so it does not overï¬t to
novel classes and keeps the knowledge of the base classes.
Also, by freezing the class-agnostic box regressor, we can
focus on classiï¬cation rather than localization and prevent
catastrophic forgetting of the base classes.
3) Fine-tuning: In the ï¬ne-tuning stage, the novel classes
are trained incrementally with Dnovel (Kshots per class)
without access to Dbase (right side of Fig. 1(b)). Only the
Lclsin (1) is used due to freezing the backbone, RPN, and
class-agnostic regressor. In this stage, our goal is to maintainthe performance of base classes and adapt to the novel classes
with a few images. In the following, we specify our ï¬ne-
tuning strategy to achieve these goals.
4) Separate novel branch: TFA separates feature repre-
sentation learning and box classiï¬er learning into two stages.
This means feature representations from the base classes
could be transferred to novel classes without an extra weights
update. The base model trained with sufï¬cient base class data
has a well-aligned feature space. Moreover, the classiï¬er,
which is ï¬ne-tuned with a few shot data including both Cbase
andCnovel , classiï¬es novel classes without forgetting the
decision boundary of the base classes. However, in iFSD,
we cannot use Cbase during ï¬ne-tuning. If the base classiï¬er
is ï¬ne-tuned without base class data, it will cause overï¬tting
on the novel classes and lead to catastrophic forgetting of
the base classes. Therefore, we ï¬rst separate the base and
novel classiï¬ers and then freeze the base classiï¬er and ï¬ne-
tune only the novel classiï¬er. Separating the novel and base
classiï¬ers allows us to focus on feature alignment for novel
classes only. For this purpose, as shown in Fig. 1(b), we
add the RoI feature extractor ( WN
fc1andWN
fc2) for the novel
classes in parallel, and it is trained to align the novel class
features. We only separate the RoI feature extractor because
the RoI features generated from the backbone and RPN have
sufï¬cient information about the objects in the image due to
being trained with abundant base classes.
We design the ï¬ne-tuning strategy to align the novelfeature space as follows:
min
WN
fc2;WN
clsLcls(yi;[pB
i; pN
i]); (2)
with pB
i=f(g(zi;WB
fc);WB
cls);
pN
i=f(g(zi;WN
fc);WN
cls); zi=F(xi;Wback);
where f(;WB
cls)andf(;WN
cls)are the base and novel
classiï¬ers, respectively. g(;WB
fc)andg(;WN
fc)are
the RoI feature extractors for the base and novel classes,
respectively. WN
fc2is the last fc layer of the novel RoI
feature extractor. Wback is the backbone network, and zi
means representation vector created from the backbone.
pB
iis (jCbasej+1)-way including the background as base
classes prediction, and pN
iisjCnovelj-way as novel classes
prediction. Then, Lclsis calculated by concatenating the
two predictions. WN
clsis randomly initialized and pre-trained
weights Wfc1andWfc2from the base training stage are
loaded into WN
fc1andWN
fc2, respectively. Other networks
are frozen except WN
clsandWN
fc2to preserve their prior
knowledge. In particular, as shown in Fig. 1(b), we ï¬ne-
tune only WN
fc2in the ï¬ne-tuning stage because, empirically,
ï¬ne-tuning WN
fc1is found to disturb the disentangling of
novel features. The effect of ï¬ne-tuning layers is evaluated
in Section IV-B.
5) Compatibility of classiï¬er: Since the feature spaces of
the base and novel classes are separated, we consider the
compatibility of each RoI feature and classiï¬er. In few-shot
learning, there is a bias in which classiï¬ers predict base
classes with higher scores, which makes it more challenging
to classify novel classes. Therefore, prior few-shot learning
works use cosine classiï¬ers [23], [24], [25] with normalized
features and various margin losses [26], [27] to align intra-
class and inter-class distributions of the novel classes. Cosine
similarity-based classiï¬ers are effective for FSOD since
they focus on disentangling only the feature space of the
novel class. However, in iFSD, we need to align the novel
class features without disturbing the feature space of the
base classes. In our method, making enough space for the
novel classes on the hypersphere is difï¬cult due to freezing
the base class feature space during the ï¬ne-tuning stage.
The inter-class separability is violated because there is no
realignment on the base class feature space. Thus, we test
the compatibility of the classiï¬er and empirically ï¬nd that a
linear classiï¬er helps disentangle the novel class feature and
improve the accuracy of novel classes.
IV. EXPERIMENTS
A. Implementation details
1) Datasets and metric: We evaluated iTFA on PASCAL
VOC 2012 [16], COCO [17], and LVIS v1 [18] datasets. The
evaluation metric and the data split strategy were followed
from [6], [15], [20] for a fair comparison. For COCO, 60
categories disjoint from PASCAL VOC are considered base
classes; the other 20 classes are considered novel classes.
For LVIS v1, 1203 categories are composed of 337 rare, 461
common, and 405 frequent classes. The 337 rare categorieswere treated as novel classes, and the 866 common and
frequent categories as base classes. For the cross-dataset
evaluation from COCO to PASCAL VOC, we used the
same base and novel class set (60 and 20 categories from
COCO) for the base model training and ï¬ne-tuning stage,
respectively. We then evaluated the object detector on the
PASCAL VOC 2012 test set. We evaluated our work on 1,
5, and 10 shots and used AP50 and the standard COCO-
style metric, which calculates the average precision (AP) at
multiple intersection-over-union (IoU) thresholds from 0.5 to
0.95. For stable results, ten tests were made on each random
seed, and the mean average precision was reported for all
experiments. For COCO, base, novel, and harmonic mean
AP (mAP) were reported as bAP, nAP, and hAP, respectively.
For LVIS, APr, APc, and APf are the mAP of rare, common,
and frequent categories, respectively. Also, APs, APm, and
APl are the mAP of small, medium, and large objects. For
cross-dataset evaluation, AP50 was used for comparison to
the other methods.
2) Training setting: Our method was implemented using
MMDetection [28]. We used Faster R-CNN [3] with Feature
Pyramid Network [29] for all experiments. The ResNet-
50/101 [30] backbones were used for COCO, while ResNet-
50 and ResNet-101 were used for LVIS and Pascal VOC for
the fair comparison, respectively. SGD with a momentum
of 0.9, a weight decay of 1e 4, and a batch size of 8 was
used on 4 NVIDIA RTX 2080Ti GPUs, with two images
per GPU. We set the learning rate to 1e 2for both the base
training and the ï¬ne-tuning stages. For COCO and PASCAL
VOC, we trained the base model for 12 epochs and ï¬ne-
tuned the object detector for 3000 steps. The learning rate
decreased ten-fold at epochs 8 and 11 in the base training and
at steps 1500 and 2000 in the ï¬ne-tuning stage. For LVIS,
we trained the base model for 24 epochs, and the learning
rate decreased at epochs 16 and 22. We ï¬ne-tuned the model
for 10000 iterations, and the learning rate decreased at 7000
and 9000 iterations.
B. Ablation study
We evaluated the following ablations on COCO to analyze
how each component of iTFA affects the novel class perfor-
mance. All training settings were maintained as mentioned in
Section IV-A. The performance was evaluated by changing
three components: the ï¬ne-tuning layer selection, regressor,
and classiï¬er. The ï¬ne-tuning layer decides which layers of
the RoI feature extractor to ï¬ne-tune. None means that all
layers are frozen, and only the classiï¬er is updated. fc1and
fc2indicate the ï¬ne-tuning WN
fc1andWN
fc2, respectively.
In addition, we tested the effects of class-speciï¬c and class-
agnostic regressors. The effect of the classiï¬er was tested
using the linear and cosine classiï¬er. The scale factor of the
cosine classiï¬er was set to 20.
As Table I shows, ï¬ne-tuning only WN
fc2with a linear
classiï¬er and class-agnostic regressor outperforms other set-
tings in all shots. This demonstrates that ï¬ne-tuning WN
fc2
with freezing WN
fc1helps disentangle the novel class features.
Also, a cosine classiï¬er with normalized features disturbsTABLE I
ABLATION STUDY OF FINE -TUNING LAYER DECISION ,CLASSIFIER ,AND REGRESSOR OF I TFA. BOLD AND UNDERLINE INDICATE THE BEST AND THE
SECOND BEST PERFORMANCE ,RESPECTIVELY .
Fine-tuning layer Regressor Classiï¬er Shots
None fc 1fc2speciï¬c agnostic Linear Cosine 1 5 10
X X X 3.4 7.0 8.3
X X X 4.3 9.9 11.8
X X X X 3.6 9.0 10.9
X X X 3.6 7.6 9.2
X X X 3.3 8.9 11.0
X X X X 2.6 8.4 10.5
X X X 2.8 7.5 9.7
X X X 3.0 8.2 10.2
TABLE II
PERFORMANCE OF NOVEL AND BASE CLASSES ON THE COCO DATASET .
Shots Method BackboneNovel Base Harmonic
nAP nAP50 bAP bAP50 hAP hAP50
1ONCE [15]
ResNet-500.7 - 17.9 - 1.4 -
Sylph [20] 1.1 - 37.6 - 2.1 -
Ours 3.8 6.5 35.7 55.9 6.8 11.7
TFA [6]
ResNet-1011.9 3.8 31.9 51.8 3.6 7.1
iMTFA [31] 3.2 5.9 27.8 40.1 5.8 10.3
LEAST [21] 4.2 - 29.5 - 7.4 -
Ours 4.3 7.2 37.4 57.4 7.7 12.8
5ONCE [15]
ResNet-501.0 - 17.9 - 1.9 -
Sylph [20] 1.5 - 42.4 - 2.9 -
Ours 8.3 15.7 35.5 56.0 13.5 24.5
TFA [6]
ResNet-1017.0 13.3 32.3 50.5 11.5 21.1
iMTFA [31] 6.1 11.2 24.1 33.7 9.7 16.8
LEAST [21] 9.3 - 31.3 - 14.3 -
Ours 9.9 17.8 37.2 57.3 15.6 27.2
10ONCE [15]
ResNet-501.2 - 17.9 - 2.3 -
Sylph [20] 1.7 - 42.8 - 3.3 -
Ours 10.2 19.3 35.5 56.0 15.9 28.7
TFA [6]
ResNet-1019.1 17.1 32.4 50.6 14.2 25.6
iMTFA [31] 7.0 12.7 23.4 32.4 10.7 18.3
DBF [22] 9.1 - 28.5 - 13.8 -
LEAST [21] 12.8 - 31.3 - 18.2 -
Ours 11.8 21.3 37.2 57.3 17.9 31.1
the novel feature alignment. Compared to rows 2, 5, and
the last two rows of Table I, Faster R-CNN with a class-
agnostic regressor outperforms a class-speciï¬c regressor in
all shots. This justiï¬es that the class-agnostic regressor can
be leveraged to the novel classes and is more suitable for
iFSD tasks.
C. Comparison with prior works
1) Baselines: We considered six baselines in our experi-
ments. TFA [6] is a ï¬ne-tuning-based approach for FSOD.
ONCE [15] and Sylph [20] are the meta-learning-based
approaches for iFSD; they use a one-stage object detector
due to its superior generalization performance. iMTFA [31]
is designed for incremental few-shot semantic segmentation
based on Mask R-CNN [4]; it creates novel classiï¬er weights
through weight imprinting. DBF [22], and LEAST [21] useFaster R-CNN as ï¬ne-tuning-based iFSD methods. DBF
presents a progressive model updating rule to preserve the
long-term memory of old classes. LEAST stores a few exem-
plars from the base training set to preserve prior knowledge
when ï¬ne-tuning the detector.
2) Results on COCO: As shown in Table II, our iTFA
achieves competitive performance in all settings. In partic-
ular, iTFA outperforms TFA by 20-40% on 5 and 10-shots
and even doubles the nAP on a 1-shot setting. This demon-
strates that iTFA is a more suitable ï¬ne-tuning approach
for iFSD tasks. In addition, our method surpasses meta-
learning methods by a large margin in all environments.
This corroborates that our training strategy can be applied
to practical iFSD tasks. Our method also shows consistent
base class performance in all shots, demonstrating that catas-
trophic forgetting is prevented by disentangling the featurespace of the base and novel classes by separating the RoI
feature extractor and classiï¬er. Sylph shows the highest bAP
in all settings. This performance difference is due to the type
of object detector. Sylph used the FCOS [32] as the base
detector, which outperforms Faster R-CNN. In this paper,
our objective is to propose a ï¬ne-tuning strategy for the
iFSD task, so we used Faster R-CNN, the most popular
two-stage object detector, as the base detector. In addition,
although we did not use the base class exemplars in the
ï¬ne-tuning stage, we achieved more accurate performance in
the 1-shot and 5-shot settings and competitive performance
compared to LEAST in the 10-shot setting. LEAST shows
a higher nAP at the 10-shot. LEAST stores kbase classes
exemplars (in k-shot) and uses them for ï¬ne-tuning to keep
the intra-class variance of base classes. It helps distinguish
novel and base classes in the ï¬ne-tuning stage, so LEAST
performs better in a 10-shot setting that can store many base
exemplars. However, saving the base class exemplars is not
a fair comparison because it violates the iFSD assumption.
Despite using a basic detector and only novel class data
for ï¬ne-tuning, iTFA shows almost the best performance for
several environments. These results demonstrate the strength
of our iTFA in preserving knowledge and accurate adaptation
with a few samples.
3) Results on LVIS v1: Our iTFA was also evaluated on
the LVIS dataset. LVIS is a natural long-tail distribution
dataset with a large number of categories. Since rare classes
of LVIS have 1 to 10 instances, each class is sampled
uniformly with 10 instances to balance the novel set. Table
III shows our results compared to recent works on LVIS.
We tested iTFA on the iFSD setting that treated rare classes
(K < 10) to a novel set. Our method signiï¬cantly outper-
forms meta-learning methods with about +4 and +12 AP
gains on rare classes. Although iTFA uses only novel classes
in the ï¬ne-tuning stage, it showed competitive performance
in all categories against TFA and the joint-train model.
This demonstrates the effectiveness of iTFA on the long-tail
distribution dataset.
TABLE III
PERFORMANCE OF RARE ,COMMON ,AND FREQUENT CLASSES ON THE
LVIS V1DATASET .
Method AP APs APm APl APr APc APf
Joint-train [18] 23.5 18.2 30.9 35.4 10.5 22.2 30.6
TFA [6] 24.4 19.9 29.5 38.2 16.9 24.3 27.7
ONCE [15] 12.9 - - - 6.3 11.2 17.7
Sylph [20] 20.7 - - - 13.9 19.0 25.5
Ours 24.0 17.5 31.2 35.9 18.1 21.0 29.9
4) Results on cross-domain from COCO to PASCAL VOC:
We evaluated iTFA in the cross-dataset setting from COCO
to PASCAL VOC. Since PASCAL VOC has no base classes,
we only report the novel class performance. Table IV shows
the same relative results evaluated on COCO. Our method
performs well against the prior methods in all the settings,
indicating the strength of our method.
5) Qualitative results: Fig. 2 illustrates the qualitative
results of our method on the COCO dataset. The top tworows show success results, and the bottom row shows failure
results. The success cases show that base (blue boxes) and
novel classes (red boxes) are correctly detected in one image.
In the bottom row, there are some misclassiï¬ed results for
failure cases due to similar characteristics and appearance.
From left to right: the boat is misclassiï¬ed as an airplane,
the birds are confused for sheep and cow, and the train is
detected well, but the bus is detected even though there is
no bus.
TABLE IV
PERFORMANCE OF THE CROSS -DOMAIN EXPERIMENT FROM COCO TO
PASCAL VOC.
ShotsMethod
Ours DBF [22] LEAST [21] LEAST-NE [21] ONCE [15]
1 11.3 - 6.8 7.2 -
5 26.0 12.5 16.7 16.5 2.4
10 30.4 23.7 18.8 18.5 2.6
Fig. 2. Qualitative results of iTFA on COCO with K=10. The top two
rows are success cases, and the bottom row images are failure cases. The
blue and red boxes indicate the base and novel classes, respectively (zoom
view better).
V. CONCLUSIONS
In this paper, we proposed iTFA for iFSD. To solve iFSD,
we addressed a simple ï¬ne-tuning strategy by modifying
TFA. We divided the RoI feature extractor and classiï¬er
into the base and novel branches and then ï¬ne-tuned only
the novel branch. We also adopted a class-agnostic regressor
and analyzed the compatibility of the classiï¬er. Our method
outperformed the prior meta-learning methods by a large
margin in widely used datasets. Although our method ï¬ne-
tuned the model without exemplars of the base classes,
it showed competitive results with other ï¬ne-tuning-based
methods. We will adapt our ï¬ne-tuning strategy to various
object detector structures for further improvement.REFERENCES
[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classiï¬ca-
tion with deep convolutional neural networks,â€ Advances in neural
information processing systems , vol. 25, 2012.
[2] K. Simonyan and A. Zisserman, â€œVery deep convolutional networks
for large-scale image recognition,â€ arXiv preprint arXiv:1409.1556 ,
2014.
[3] S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster r-cnn: Towards real-
time object detection with region proposal networks,â€ Advances in
neural information processing systems , vol. 28, 2015.
[4] K. He, G. Gkioxari, P. Doll Â´ar, and R. Girshick, â€œMask r-cnn,â€ in
Proceedings of the IEEE international conference on computer vision ,
2017, pp. 2961â€“2969.
[5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, â€œGenerative adversarial nets,â€
Advances in neural information processing systems , vol. 27, 2014.
[6] X. Wang, T. Huang, J. Gonzalez, T. Darrell, and F. Yu, â€œFrustratingly
simple few-shot object detection,â€ in International Conference on
Machine Learning . PMLR, 2020, pp. 9919â€“9928.
[7] C. Xing, N. Rostamzadeh, B. Oreshkin, and P. O. O Pinheiro, â€œAdap-
tive cross-modal few-shot learning,â€ Advances in Neural Information
Processing Systems , vol. 32, 2019.
[8] J. Wu, S. Liu, D. Huang, and Y . Wang, â€œMulti-scale positive sample
reï¬nement for few-shot object detection,â€ in European conference on
computer vision . Springer, 2020, pp. 456â€“472.
[9] B. Sun, B. Li, S. Cai, Y . Yuan, and C. Zhang, â€œFsce: Few-shot object
detection via contrastive proposal encoding,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2021, pp. 7352â€“7362.
[10] J. Kim, I. Yoon, G.-M. Park, and J.-H. Kim, â€œNon-probabilistic cosine
similarity loss for few-shot image classiï¬cation.â€ in BMVC , 2020.
[11] L. Karlinsky, J. Shtok, S. Harary, E. Schwartz, A. Aides, R. Feris,
R. Giryes, and A. M. Bronstein, â€œRepmet: Representative-based metric
learning for classiï¬cation and few-shot object detection,â€ in Proceed-
ings of the IEEE/CVF conference on computer vision and pattern
recognition , 2019, pp. 5197â€“5206.
[12] Y . Xiao and R. Marlet, â€œFew-shot object detection and viewpoint esti-
mation for objects in the wild,â€ in European conference on computer
vision . Springer, 2020, pp. 192â€“210.
[13] B. Kang, Z. Liu, X. Wang, F. Yu, J. Feng, and T. Darrell, â€œFew-
shot object detection via feature reweighting,â€ in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2019, pp.
8420â€“8429.
[14] Y . Cao, J. Wang, Y . Jin, T. Wu, K. Chen, Z. Liu, and D. Lin, â€œFew-
shot object detection via association and discrimination,â€ Advances in
Neural Information Processing Systems , vol. 34, pp. 16 570â€“16 581,
2021.
[15] J.-M. Perez-Rua, X. Zhu, T. M. Hospedales, and T. Xiang, â€œIncre-
mental few-shot object detection,â€ in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2020, pp.
13 846â€“13 855.
[16] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, â€œThe pascal visual object classes (voc) challenge,â€ International
journal of computer vision , vol. 88, no. 2, pp. 303â€“338, 2010.
[17] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll Â´ar, and C. L. Zitnick, â€œMicrosoft coco: Common objects in
context,â€ in European conference on computer vision . Springer, 2014,
pp. 740â€“755.
[18] A. Gupta, P. Dollar, and R. Girshick, â€œLvis: A dataset for large
vocabulary instance segmentation,â€ in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , 2019, pp.
5356â€“5364.
[19] X. Yan, Z. Chen, A. Xu, X. Wang, X. Liang, and L. Lin, â€œMeta r-
cnn: Towards general solver for instance-level low-shot learning,â€ in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2019, pp. 9577â€“9586.
[20] L. Yin, J. M. Perez-Rua, and K. J. Liang, â€œSylph: A hypernetwork
framework for incremental few-shot object detection,â€ in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition , 2022, pp. 9035â€“9045.
[21] P. Li, Y . Li, H. Cui, and D. Wang, â€œClass-incremental few-shot object
detection,â€ arXiv preprint arXiv:2105.07637 , 2021.
[22] Y . Li, H. Zhu, J. Ma, C. S. Teo, C. Xiang, P. Vadakkepat, and T. H.
Lee, â€œTowards generalized and incremental few-shot object detection,â€
arXiv preprint arXiv:2109.11336 , 2021.[23] S. Gidaris and N. Komodakis, â€œDynamic few-shot visual learning with-
out forgetting,â€ in Proceedings of the IEEE conference on computer
vision and pattern recognition , 2018, pp. 4367â€“4375.
[24] H. Qi, M. Brown, and D. G. Lowe, â€œLow-shot learning with imprinted
weights,â€ in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2018, pp. 5822â€“5830.
[25] W.-Y . Chen, Y .-C. Liu, Z. Kira, Y .-C. F. Wang, and J.-B.
Huang, â€œA closer look at few-shot classiï¬cation,â€ arXiv preprint
arXiv:1904.04232 , 2019.
[26] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, â€œArcface: Additive
angular margin loss for deep face recognition,â€ in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition ,
2019, pp. 4690â€“4699.
[27] H. Wang, Y . Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and
W. Liu, â€œCosface: Large margin cosine loss for deep face recognition,â€
inProceedings of the IEEE conference on computer vision and pattern
recognition , 2018, pp. 5265â€“5274.
[28] K. Chen, J. Wang, J. Pang, Y . Cao, Y . Xiong, X. Li, S. Sun, W. Feng,
Z. Liu, J. Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li,
X. Lu, R. Zhu, Y . Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C.
Loy, and D. Lin, â€œMMDetection: Open mmlab detection toolbox and
benchmark,â€ arXiv preprint arXiv:1906.07155 , 2019.
[29] T.-Y . Lin, P. Doll Â´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,
â€œFeature pyramid networks for object detection,â€ in Proceedings of the
IEEE conference on computer vision and pattern recognition , 2017,
pp. 2117â€“2125.
[30] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image
recognition,â€ in Proceedings of the IEEE conference on computer
vision and pattern recognition , 2016, pp. 770â€“778.
[31] D. A. Ganea, B. Boom, and R. Poppe, â€œIncremental few-shot instance
segmentation,â€ in Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , 2021, pp. 1185â€“1194.
[32] Z. Tian, C. Shen, H. Chen, and T. He, â€œFcos: Fully convolutional one-
stage object detection,â€ in Proceedings of the IEEE/CVF international
conference on computer vision , 2019, pp. 9627â€“9636.