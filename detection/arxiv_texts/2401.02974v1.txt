EFFICACY OF UTILIZING LARGE LANGUAGE MODELS TO
DETECT PUBLIC THREAT POSTED ONLINE
Taeksoo Kwon
Algorix Convergence Research Office
New York, New York
henryk@research.algorix.ioConnor Kim
Centennial High School
Corona, California
connorkusa@gmail.com
ABSTRACT
This paper examines the efficacy of utilizing large language models (LLMs) to detect public threats
posted online. Amid rising concerns over the spread of threatening rhetoric and advance notices
of violence, automated content analysis techniques may aid in early identification and moderation.
Custom data collection tools were developed to amass post titles from a popular Korean online
community, comprising 500 non-threat examples and 20 threats. Various LLMs (GPT-3.5, GPT-4,
PaLM) were prompted to classify individual posts as either “threat” or “safe.” Statistical analysis
found all models demonstrated strong accuracy, passing chi-square goodness of fit tests for both threat
and non-threat identification. GPT-4 performed best overall with 97.9% non-threat and 100% threat
accuracy. Affordability analysis also showed PaLM API pricing as highly cost-efficient. The findings
indicate LLMs can effectively augment human content moderation at scale to help mitigate emerging
online risks. However, biases, transparency, and ethical oversight remain vital considerations before
real-world implementation.
Keywords Large Language Model ·Content Moderation ·Public Threat
1 Introduction
The evolution of online communities has propelled the amalgamation of diverse perspectives and ideologies, fostering an
environment where information flows unimpeded across geographical boundaries [ 1]. However, the cloak of anonymity
veils the identities of individuals, allowing for the unrestrained expression of thoughts and actions. This anonymity,
while liberating in some aspects, has unfurled a tapestry of challenges, manifesting prominently in the form of group
polarization [ 2]. The phenomenon of group polarization, accentuated in the digital realm, intensifies ideological
extremities within communities, creating echo chambers that reinforce existing beliefs and amplify radical perspectives.
Moreover, the ease of disseminating harmful content in these spaces, fueled by the shield of anonymity, presents a
critical societal challenge [3].
Incidents, like the distressing sequence of copycat crimes following the Sillim subway station tragedy, underscore the
alarming consequences of unchecked propagation of harmful ideas and actions within online platforms [ 4]. The initial
act of violence echoed into a haunting symphony of fear as a slew of imitative offenses gripped the nation, reverberating
not only in physical spaces but also pervading the digital realm [ 5]. Due to its widespread popularity, online platforms
like DC Inside transformed into channels for unfiltered hate speech and threats [ 6]. This surge intensified the difficulty
of content moderation, amplifying the constraints posed by human limitations as outlined in prevailing online content
governance models. Concurrently, it escalated societal tensions, manifesting through a barrage of virtual and physical
threats directed at random individuals [ 7]. Traditional content moderation methods, predominantly reliant on human
intervention, grapple with the daunting task of sifting through voluminous user-generated content. Platforms, bound by
community guidelines, strive to uphold standards by curtailing the dissemination of objectionable material. However,
the sheer scale and diversity of content pose significant hurdles, stretching the capabilities of human moderators to their
limits [8].arXiv:2401.02974v1  [cs.CL]  29 Dec 2023Utilizing LLM to Detect Public Threat
In light of these challenges, the exploration of artificial intelligence, particularly large language models (LLM), emerges
as a beacon of hope. These AI-driven mechanisms possess the potential to augment existing content moderation
practices by leveraging sophisticated algorithms capable of swiftly analyzing, categorizing, and flagging problematic
content. The ability of AI models to process vast amounts of data at unparalleled speeds offers a glimmer of optimism
in addressing the inadequacies of human-centric moderation strategies.
2 Literature Review
This literature review delves into the evolving landscape of LLM applications within Natural Language Processing
(NLP). Recent research emphasizes the efficacy of autoregressive models, notably GPT, LLaMa, and Alpaca. The
advent of GPT precipitated a surge in interest surrounding LLMs in generative AI, leading to varied developments in
the field. Notably, the open-sourcing of LLaMa facilitated the customization and refinement of LLMs by developers [ 9].
Integrating LLMs into real-world applications necessitates consideration of diverse components, such as plugins,
frameworks, and agents. These elements enable the incorporation of external data and functionalities, language
assistance, and seamless integration interfaces, culminating in comprehensive implementations. Applications in sectors
like healthcare and science, such as DNA-oriented LLMs, exemplify the diverse applications of LLMs [10].
2.1 Recent Trends in Online Threats in America
The Pew Research Center conducted research in 2021, revealing that 41 percent of Americans have personally
experienced some form of online harassment, ranging from common to severe comments [ 11]. While the trend of
online threats stayed consistent since 2017, it is quite evident that it has intensified over the years. These threats can
be identified from least to greatest in terms of offense, offensive name-calling, purposeful embarrassment, stalking,
sustained harassment, sexual harassment, and physical threats.
2.2 Trends in South Korea: Tragedy at Sillim Station
Regarding the research, South Korea has experienced an abnormal growth in online threats, some even correlating to real
life. The Sillim Station stabbings played a major role in the expansion of threats in Korea, contributing approximately
430 murder posts in that month alone after the incident [ 12]. Seohyeon Station, Gwanaksan Park, Hapjeong Station,
Gwangmyeong Station, and other places, Sillim being the most notable, were all locations that contained certain threats
according to the posts in Korea by their communities. Because of the escalation of the situation, Korea reinforced
its security by enforcing police to track the people who have posted threats and take information from citizens about
possible crimes that will be committed [13].
Overall, the impersonation trend enabled the diffusion and normalization of more threatening rhetoric relating to the
Sillim Station case through covert means online. This highlighted the rise in popularity of the emergence of security in
online communities and helped provide alternative solutions such as Terrorless which was created to map out locations
where possible crimes may be committed through user reports and online posts [14].
2.3 The Rise of Large Language Models: What are They?
LLM (Large Language Model), is an advanced deep learning model for natural language processing that is trained
on vast amounts of text data. Some notable LLMs include BERT, GPT-3, GPT-4, and XLNet developed by teams at
Google, OpenAI, Anthropic, and others. The goal of an LLM is to understand and generate human language at a high
level through massive computing power and datasets.
These language models are now being utilized across many industries and fields to augment human capabilities. For
example, GitHub has created their own LLM, CoPilot X to help assist and support developers in programming and fixing
errors [ 15]. In customer support, chatbots powered by LLMs can answer common queries automatically around the
clock [ 16]. LLMs are also used for content creation, science, and research by analyzing papers and data. Furthermore,
they assist professionals in legal, medical, and other domains by reviewing documents, conducting research, and
answering questions to accelerate their work.
2.4 Popularity and Affordability of Large Language Models
OpenAI’s gpt-3.5-turbo-1106 and gpt-4, alongside PaLM API for Google Bard, stand as prominent publicly available
LLMs [ 17]. When considering the cost aspect, gpt-3.5-turbo-1106 is priced at $0.001 per 1K tokens for input and
$0.002 per 1K tokens for output, while gpt-4 costs $0.03 per 1K tokens for input and $0.06 per 1K tokens for output
2Utilizing LLM to Detect Public Threat
[18]. On the other hand, PaLM 2 for Chat (chat-bison) operates at $0.00025 per 1K characters for input and $0.0005 per
1K characters for output [19].
These varying pricing structures make PaLM 2 for Chat notably more affordable in comparison to both gpt-3.5-turbo-
1106 and gpt-4, providing an attractive option for certain applications due to its lower cost per token/character. Despite
this, the popularity of these models is influenced by factors beyond just affordability, including performance, capabilities,
and accessibility, contributing to their diverse usage across different contexts within the LLM landscape [20].
2.5 Cases of Using LLM to Moderate Content
OpenAI researchers have begun applying generative models to the application of online content moderation. As
described in a blog post, their method utilizes GPT-4 to assist in developing and continually refining platform-specific
content policies on issues like hate speech, abuse, and threats in a highly automated and scalable manner [ 21]. Through
an iterative process of policy drafting, example curation, and model feedback, they aim to speed up the traditionally
lengthy process of policy evolution from months to just hours.
A similar case was led by a team of researchers who investigated LLMs and Content Moderation and found that LLMs
can be effective in rule-based content moderation and toxic detection [ 22]. The researchers tested LLMs on rule-based
community moderation and toxic content detection and found that LLMs can be effective for rule-based moderation
and outperform existing toxicity classifiers. However, they also found that the increase in model size only provides a
marginal benefit for toxicity detection. The researchers acknowledge that their results may not extend to other types
of moderation and that the cost of LLMs is currently high. Resulting that while LLMs show promise, more research
is needed to balance performance with cost. The research also includes a case study on the subreddit r/worldnews,
highlighting the errors made by the LLM in moderation decisions. Overall, their research provides a tempered but
optimistic view of using LLMs in content moderation and suggests avenues for future research.
With continued experimentation integrating techniques like distillation and active learning, LLMs may help alleviate
some of the mental burden on human moderators while also enabling faster responses to emerging online risks. However,
limitations around unwanted biases potentially introduced during pretraining also underscore the need for careful
oversight and model validation as these systems grow in real-world impact.
2.6 Limitations of Human Content Moderation
While this study focuses on evaluating the technical capabilities of large language models for content moderation, it’s
worth noting some of the inherent limitations of relying solely on human moderators. At massive scales, human content
moderation can be an inefficient, costly, and imperfect process [23].
Human content moderation at large scales faces significant challenges that make it an inefficient and imperfect solution
on its own. Moderators are susceptible to factors like fatigue, stress, and burnout from constant exposure to harmful or
disturbing content [ 24]. This can negatively impact judgment and consistency over time. Individual biases, cultural
differences, and personal interpretations of guidelines also introduce elements of unpredictability and subjectivity. As
online communities continue growing exponentially, it becomes impossible for human reviewers alone to keep up with
demanded capacities [25].
A research conducted by Petter Törnberg explains the accuracy and reliability of ChatGPT, an LLM, compared to
human classifiers. The research compares the performance of ChatGPT with crowd workers on MTurk and expert
classifiers. It is found that ChatGPT outperforms individual human classifiers [ ?]
Automated techniques using AI models have the potential to supplement human moderation by helping address some of
these limitations at scale. Overall, the limitations indicate a need for supplemental automated techniques.
2.7 Ethical Considerations
Employing AI models for content analysis raises important ethical questions around privacy, accountability, and
unintended consequences. As these systems continue expanding in impact and autonomy, key considerations include
transparency around how decisions are made, access to appeal channels, and validation of model fairness [26].
By training AI systems on data generated by humans, there is a potential risk of replicating and even amplifying the
possible bias problems. Different groups could potentially be treated unfairly or censored at disproportionate rates
based on characteristics like gender, race, political views, etc [ 27]. Content moderation systems must be developed and
evaluated using intersectional approaches to proactively identify and mitigate against such biases. Without adequate
3Utilizing LLM to Detect Public Threat
measures, biases in automated censorship could end up promulgating wider societal inequalities and exclusion online
[28].
Relying too heavily on AI systems for content moderation could potentially lead to many human moderators losing
their jobs. This could have negative economic and social impacts if the transition is not thoughtfully managed. The
role of humans also helps ensure appropriate oversight and governance over these important decision processes. By
delegating broad moderation powers to automated systems, it effectively extends its authority over information flows
and discussions online. There are risks of over-zealous or biased censorship if models are not carefully designed,
overseen, and calibrated. It’s also difficult to understand and challenge specific AI decisions, undermining principles of
transparency. Overall governance structures would need careful consideration to avoid such unintended consequences
[26].
2.8 Future Directions
The exploration of artificial intelligence, particularly large language models (LLMs), for online content moderation
presents a promising avenue for the future. As technology continues to advance, several key directions emerge for the
integration and enhancement of LLMs in the realm of content moderation.
3 Methodology
This study examines the effectiveness of large language models in detecting public threats within an online community.
To curate a diverse dataset inclusive of genuine threat posts, strict adherence to the community’s scraping policies,
as outlined in robots.txt, was prioritized. A custom HTTP tool was crafted to systematically gather and analyze data
via OpenAI’s official GPT models accessed through their public API. Additionally, the same HTTP tool was utilized
to send requests to the PaLM API, which operates Google Bard, enhancing the breadth of model analysis [ 29]. This
method ensures ethical data collection while exploring the capacity of AI models to identify threats within varied online
forums.
3.1 Research Design
The research design in this study is centered around experimentally exploring the feasibility of employing large language
models (LLMs) for content moderation within online communities. The primary objective is to assess the capacity of
these models, including GPT variants and others, to identify potentially threatening content among a diverse range
of posts. However, it’s important to clarify that this investigation doesn’t specifically delve into the affordability or
cost-effectiveness associated with using LLMs for content moderation purposes. The focus remains on evaluating the
effectiveness of these models and their potential utility in mitigating threats, rather than on economic considerations
related to their implementation.
3.2 Data Collection and Analysis
The data collection phase involved extracting 500 post titles from the renowned online platform "DC Inside"1, specifically
from the "실시간베스트갤러리" (Real-time Best Gallery). A specialized scraping tool was meticulously used to
exclude any posts containing public threat content from this dataset [ 30]. Additionally, an auxiliary dataset of 20 post
titles sourced from verified threat post content was compiled. To simulate real-world scenarios, 40 non-threatening
post titles were randomly selected for each attempt alongside 10 threatening titles, and their sequence was deliberately
shuffled. This composite dataset underwent analysis using various large language models, including OpenAI’s gpt-3.5-
turbo-1106 and gpt-4, as well as PaLM API’s chat-bison.
1https://www.dcinside.com/
4Utilizing LLM to Detect Public Threat
• 10분뒤천호동디스코팡팡에서폭탄테러,ㅋㅂㄹ한다
•여의도에서칼부림한다
•수요일날드론으로신촌역인근공중서펜타닐살포예정
•다음주화요일이수역에서10명칼로찔러죽일거다.
•오늘자약간맛이간삼수생칼부림예고...
•인천공항폭탄테러8시예정
•오늘오후7시에강남역에서100명죽일예정
•오늘밤11시에이대역에서칼부림예정입니다
•내가구라안치고일요일날칼들고개좆슼갈들전부찔러죽일거임.
•다음주수요일5시에강남역에서경찰관10명칼로쳐죽일거다
• . . .
Figure 1: Example Public Threat Post Titles Collected [13]
Subsequent statistical analysis, specifically the chi-square goodness of fit test at a general significance level of 0.05, was
conducted to determine the suitability of employing these LLMs, considering their performance in content moderation
[31]. This comprehensive evaluation aimed to assess the models’ proficiency in distinguishing between threatening
and non-threatening content within the dynamic landscape of an online community, taking into account the nuanced
capabilities of various LLMs.
3.3 Prompt Engineering
The experimental design encompassed the repetition of this meticulous process for enhanced reliability and robustness
[32]. Iterating this procedure 25 times reinforced the rigor of the assessment. Through each iteration, the 40 safe and
10 threatening post titles were individually funneled through the designated APIs. This iterative approach not only
amplified the sample size but also provided a comprehensive overview of the models’ consistency and reliability in
accurately classifying diverse content. Consequently, this methodological strategy bolstered the statistical significance
of the findings, affording a more nuanced understanding of the models’ performance and their ability to effectively
discern threat-related content within an online community setting. The base prompt used for this purpose is as follows:
Beda is a content moderator working for a Korean online community website. Beda is given a post
title in Korean to review. Beda judges the post as either safe or unsafe based on whether it contains
content posing a public threat. Beda defines a public threat as advance notices that perpetrators upload
before committing terror in public places. Beda must be considerate to keep the public safe. Beda can
only reply in either ’threat’ or ’safe’, based on whether the text is a public threat or not. Beda does
not translate the text into other languages. Beda should only reply in one word. Beda does not add
any description about the message. You are Beda.
3.4 Evaluation Criteria
The assessment criteria for content moderation employing large language models primarily revolves around accuracy.
The evaluation methodology relies on the chi-square test, utilizing the resulting test statistic to establish decision criteria
[31]. Should the computed statistic exceed the critical value, predetermined by a significance level of 0.05, rejecting the
null hypothesis indicates a significant deviation of the data from the anticipated distribution. This deviation suggests a
lack of significant accuracy in the model’s ability to identify threats within the content being evaluated, supporting the
hypothesis that the model might not possess substantial capabilities for threat detection.
Conversely, the inability to reject the null hypothesis indicates that the data conforms to the specified distribution. This
conformity signifies that the model demonstrates notable accuracy in detecting threats within the dataset. This statistical
framework provides a decisive metric for assessing the model’s performance in discerning threatening content within
the curated dataset.
3.5 Ethical Considerations
The methodology adhered meticulously to ethical considerations throughout the data collection process. Compliance
with ethical guidelines was paramount, adhering strictly to the directives outlined in the robots.txt file and adhering to
5Utilizing LLM to Detect Public Threat
DC Inside’s Terms of Service (TOS) [33][34]. These measures ensured respect for the online community’s guidelines
and policies, preserving the integrity of data acquisition while safeguarding the platform’s regulations. Furthermore, an
anonymous approach was adopted during post-collection to uphold user privacy and confidentiality. This anonymity
shielded individual identities and protected user information, underscoring a commitment to ethical data-handling
practices throughout the research process.
4 Results
The experimentation was structured around a series of 30 distinct trials aimed at rigorously assessing the performance
and consistency of the developed methodology. Each trial offered unique insights into the efficacy of the approach in
detecting threats within online post titles. Notably, all code utilized throughout the experimentation process has been
meticulously documented and made available on GitHub [ 30][29]. This comprehensive repository houses the tools,
scripts, and methodologies employed, ensuring transparency, reproducibility, and accessibility for further scrutiny and
replication of the study’s procedures.
4.1 Data Collected
Model Name Non-threats Judged Correctly Threats Judged Correctly
gpt-3.5-turbo-1106 32.6 9.92
gpt-4 39.16 10
chat-bison (PaLM2) 38.6 10
Table 1: Raw Data Collected from Each Model
4.2 Performance Analysis
gpt-3.5-turbo-1106 gpt-4 chat-bison020406080100
81.597.9 96.5
Model NameNon-threats Judged Correctly (%)
Figure 2: Non-threat Accuracy Comparison
6Utilizing LLM to Detect Public Threat
gpt-3.5-turbo-1106 gpt-4 chat-bison02040608010099.2 100 100
Model NameThreats Judged Correctly (%)
Figure 3: Threat Accuracy Comparison
4.3 Chi-Square Tests
Chi-square goodness of fit tests were conducted to evaluate the performance of each model in detecting public threats
[31]. The null hypothesis ( H0) assumed that there would be no difference between the observed and expected values,
indicating that the model is capable of accurately detecting public threats. On the other hand, the alternative hypothesis
(Ha) posited that a significant difference would exist between the observed and expected values, suggesting that the
model is incapable of effectively detecting public threats. The chi-square tests were employed to assess whether the
observed results deviated significantly from the expected results, providing insights into the model’s performance in
identifying potential public threats.
χ2=X (Oi−Ei)2
Ei(1)
4.3.1 gpt-3.5-turbo-1106
The calculated chi-square value ( χ2= 1.370) with 1 degree of freedom did not exceed the critical value of the chi-square
distribution at a significance level of 0.05 ( χ2distribution = 3.84). Therefore, the test does not provide sufficient
evidence to reject the null hypothesis of a good fit between the observed and expected values.
χ2=(32.6−40)2
40+(9.92−10)2
10≈1.370 (2)
Non-threat Correct Threat Correct
Observed 32.6 9.92
Expected 40 10
Table 2: Chi-Square Test for gpt-3.5-turbo-1106
4.3.2 gpt-4
The calculated chi-square value ( χ2= 0.018) with 1 degree of freedom did not exceed the critical value of the chi-square
distribution at a significance level of 0.05 ( χ2distribution = 3.84). Consequently, the test does not provide enough
evidence to reject the null hypothesis of a good fit between the observed and expected values.
7Utilizing LLM to Detect Public Threat
χ2=(39.16−40)2
40+(10−10)2
10≈0.018 (3)
Non-threat Correct Threat Correct
Observed 39.16 10
Expected 40 10
Table 3: Chi-Square Test for gpt-4
4.3.3 chat-bison
The calculated chi-square value ( χ2= 0.049) with 1 degree of freedom did not exceed the critical value of the chi-square
distribution at a significance level of 0.05 ( χ2distribution = 3.84). Hence, the evidence from the test is insufficient to
reject the null hypothesis of a good fit between the observed and expected values.
χ2=(38.6−40)2
40+(10−10)2
10≈0.049 (4)
Non-threat Correct Threat Correct
Observed 38.6 10
Expected 40 10
Table 4: Chi-Square Test for chat-bison
4.4 Discussion
Across the tested models, a consistent trend emerged regarding their performance in threat detection. All exhibited
commendable accuracy levels, particularly in identifying potential threats within content. Notably, GPT-4 demonstrated
the highest accuracy in both non-threat (97.9%) and threat (96.5%) identification, showcasing its proficiency in this
domain.
Conversely, GPT-3.5 displayed comparatively lower accuracy scores for both non-threat (81.5%) and threat (99.2%)
identification tasks. This observation highlights certain limitations within the GPT-3.5 model concerning its effectiveness
in content moderation, signifying areas for potential improvement.
Among the factors influencing model selection, affordability emerged as a crucial consideration. The cost efficiency
analysis indicated that the PaLM API’s Chat-Bison presented an economically favorable option [ 19]. However, while
it showcased promise, challenges arose during the retrieval of responses in a specific format from Chat-Bison. Its
tendency to occasionally provide nuanced responses, rather than a clear ’threat’ or ’safe’ classification as per the prompt,
necessitated manual judgment for threat assessment based on the content of the response 4. Enhancements in this
aspect could be achieved by tailoring prompts specifically for each model, taking into account their distinct design
characteristics [35].
It’s imperative to note that the dataset used in this research was collected in Korean, emphasizing the potential for
different outcomes when employing these models with English content 1. This distinction highlights the importance
of language and context in training and utilizing AI models for threat detection, suggesting that results might vary
significantly when applied to distinct linguistic and cultural environments [36].
8Utilizing LLM to Detect Public Threat
Figure 4: Screenshot of Manually Interpreting PaLM API’s Result
5 Conclusions
The comprehensive evaluation of various models for content moderation has provided invaluable insights into their
effectiveness and applicability. Across the board, all models successfully passed the rigorous chi-square test at a
significance level of 0.05, affirming their statistical viability for real-world content moderation applications. This
outcome underscores their potential practical utility.
Ultimately, this evaluation illuminates the strengths and considerations surrounding various models for content modera-
tion. It underscores the significance of a balanced assessment encompassing accuracy, affordability, and interpretability
when selecting models for real-world content moderation scenarios. Moreover, this research serves as a foundation,
illustrating the potential pathways for leveraging Large Language Models in future content moderation strategies. As
advancements continue in model development and implementation strategies, these findings pave the way for more
efficient and adaptable content moderation solutions, offering insights into harnessing the capabilities of LLMs for
effective moderation in diverse online environments.
6 Acknowledgements
We acknowledge the indispensable contributions of the online community, DC Inside, for their assistance in collecting
safe post titles data, and the editors of Namuwiki for their efforts in gathering threat posts, ultimately benefiting our
study and the wider community.
Furthermore, special thanks to Dr. Hojin Moon, a luminary in the field of Statistics and a revered professor at California
State University Long Beach for his invaluable support and meticulous review of our statistical analysis method.
References
[1]Samer Faraj, Sirkka L. Jarvenpaa, and Ann Majchrzak. Knowledge collaboration in online communities. Organi-
zation Science , 22(5):1224–1239, Feb 2011.
[2]Kimberly M. Christopherson. The positive and negative implications of anonymity in internet social interactions:
“on the internet, nobody knows you’re a dog”. Computers in Human Behavior , 23(6):3038–3056, Nov 2007.
[3]H.L. Armstrong and P.J. Forde. Internet anonymity practices in computer crime. Information Management &
Computer Security , 11(5):209–215, Dec 2003.
[4]Ray Surette. Measuring copycat crime. Crime, Media, Culture: An International Journal , 12(1):37–64, Sep 2015.
[5] SUNG-EUN LEE. Police release photo, identity of sillim station stabber, Jul 2023.
[6]오늘의베스트.오늘의베스트, Dec 2023.
[7] Hae-rin Lee. Fears grow over copycat crimes following series of stabbing rampages, Aug 2023.
[8] Andrew Arsht and Daniel Etcovitch. The human cost of online content moderation, Mar 2018.
[9] Richard MacManus. Why developers are flocking to llama, meta’s open source llm, May 2023.
[10] Aminu Abdullahi. What are large language models?, Sep 2023.
[11] Emily A. V ogels. The state of online harassment. Pew Research Center , Jan 2021.
9Utilizing LLM to Detect Public Threat
[12] Lee Jung-joo. 2 in 5 murder threat suspects were teens: police, Aug 2023.
[13] 2023 년대한민국다발적흉기난동사태/사건목록-나무위키.
[14] Da-hyun Jung. “terrorless” website grows popular by tracking random attacks, Aug 2023.
[15] Thomas Dohmke. Github copilot x: The ai-powered developer experience, Mar 2023.
[16] Yasmin Altmann. What is a chatbot? | chatbots simply explained, 2023.
[17] Krithi Pushpanathan, Zhi Wei Lim, Samantha Min Er Yew, David Ziyou Chen, Hazel Anne Hui’En Lin, Jocelyn Hui
Lin Goh, Wendy Meihua Wong, Xiaofei Wang, Marcus Chun Jin Tan, Victor Teck Chang Koh, and Yih-Chung
Tham. Popular large language model chatbots’ accuracy, comprehensiveness, and self-awareness in answering
ocular symptom queries. iScience , 26(11):108163, Oct 2023.
[18] OpenAI. Pricing.
[19] Google Cloud. Pricing for generative ai on vertex ai.
[20] Eray Eliaçık. Which free llms are best suited for you? here are the differences. Dataconomy , Nov 2023.
[21] Lilian Weng, Vik Goel, and Andrea Vallone. Using gpt-4 for content moderation.
[22] Deepak Kumar, Yousef Abuhashem, and Zakir Durumeric. Watch Your Language: Large Language Models and
Content Moderation . Sep 2023.
[23] Unitary. Ai vs humans in content moderation: Scale, risk, cost, bias, and context | unitary blog, Feb 2023.
[24] Heather Merrick. Can ai replace one of the most traumatic jobs on the internet?, Oct 2023.
[25] Janselle Miguel. Ai vs. human content moderation, Aug 2022.
[26] Christina Pazzanese. Ethical concerns mount as ai takes bigger decision-making role, Oct 2020.
[27] George Krasadakis. The ethical concerns associated with the general adoption of ai, Nov 2023.
[28] AIContentfy team. The role of ai in content moderation and censorship, Mar 2023.
[29] Taeksoo Kwon and Connor Kim. Threat Post Detection Tool.
[30] Taeksoo Kwon and Connor Kim. DC Inside Website Scraper.
[31] Todd Michael Franke, Timothy Ho, and Christina A. Christie. The chi-square test: Often used and more often
misinterpreted. American Journal of Evaluation , 33(3):448–458, Nov 2011.
[32] Aleksandar J. Spasi ´c and Dragan S. Jankovi ´c. Using chatgpt standard prompt engineering techniques in lesson
preparation: Role, instructions and seed-word prompts. In 2023 58th International Scientific Conference on
Information, Communication and Energy Systems and Technologies (ICEST) , pages 47–50, 2023.
[33] Gold Zachary and Latonero Mark. Robots welcome? ethical and legal considerations for web crawling and
scraping. Washington Journal of Law, Technology & Arts , 13(3), 2018.
[34] DC Inside. 디시인사이드이용약관, Feb 2013.
[35] Oliver Kleinig, Congjie Gao, Joshua G Kovoor, Aashray K Gupta, Stephen Bacchi, and Weng Chan. How to use
large language models in ophthalmology: from prompt engineering to protecting confidentiality. Eye, Oct 2023.
[36] Phillip Rust, Jonas Pfeiffer, Ivan Vuli, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the
monolingual performance of multilingual language models. Association for Computational Linguistics , 1, Jan
2021.
10