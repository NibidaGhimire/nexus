A Comparative Study on Generative Models for High Resolution
Solar Observation Imaging
Mehdi Cherti1Alexander Czernik1;2Stefan Kesselheim1;2
Frederic Eﬀenberger3Jenia Jitsev1
Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)1
Helmholtz AI2
Ruhr-Universität Bochum3
Abstract
Solar activity is one of the main drivers of variability in our solar system and the key source of
space weather phenomena that aﬀect Earth and near Earth space. The extensive record of high
resolution extreme ultraviolet (EUV) observations from the Solar Dynamics Observatory (SDO)
oﬀers an unprecedented, very large dataset of solar images. In this work, we make use of this
comprehensive dataset to investigate capabilities of current state-of-the-art generative models to
accurately capture the data distribution behind the observed solar activity states. Starting from
StyleGAN-based methods, we uncover severe deﬁcits of this model family in handling ﬁne-scale
details of solar images when training on high resolution samples, contrary to training on natural
face images. When switching to the diﬀusion based generative model family, we observe strong
improvements of ﬁne-scale detail generation. For the GAN family, we are able to achieve similar
improvements in ﬁne-scale generation when turning to ProjectedGANs, which uses multi-scale
discriminators with a pre-trained frozen feature extractor. We conduct ablation studies to
clarify mechanisms responsible for proper ﬁne-scale handling. Using distributed training on
supercomputers, we are able to train generative models for up to 1024x1024 resolution that
produce high quality samples indistinguishable to human experts, as suggested by the evaluation
we conduct. We make all code, models and workﬂows used in this study publicly available at
https://github.com/SLAMPAI/generative-models-for-highres-solar-images .
1 Introduction
Generative models for high resolution images have seen a rapid progress in the last years, enabling
for instance generation of highly photo-realistic, diverse natural image samples after training on
large-scale data [ 40,11,36]. Consequently, diﬀerent domains that operate on image-like signals were
seeking to apply the powerful data-driven model class to study various domain-speciﬁc problems.
Here, we pose the question whether in the domain of solar physics that oﬀers high volume, high
quality data on the state and dynamics of the sun recorded during long term observation missions,
generative models can learn the underlying data distribution with a degree of realism suﬃcient for
scientiﬁc requirements.
For this study, we take the large-scale dataset provided by the Solar Dynamic Observatory
[33], operated by NASA since 2010. Motivated by the success on high resolution natural image
generation, we ﬁrst focus on generative adversarial networks, GANs [ 14], that show impressive
results, producing for instance face and other images that are hard to distinguish from originals by
1arXiv:2304.07169v1  [cs.CV]  14 Apr 2023human observers. Surprisingly, we discover that state-of-the-art GANs have troubles when it comes
to faithful reproduction of important ﬁne scale details of solar images even after careful tuning of the
learning procedure, which is contrary to high quality ﬁne scale generation for such natural images as
faces.
We then turn to another type of generative models - diﬀusion models. Using standard ablated
diﬀusion models, ADM [ 11], we are able to generate high resolution solar image samples that show
suﬃcient quality of ﬁne scale details. In order to match this capability with GANs, we experiment
with ProjectedGAN [ 39], a model type that introduces additional mechanisms to deal with multi-scale
nature of images, mixing features and employing multiple discriminators across diﬀerent scales.
We observe that this GAN type can handle ﬁne scale of solar images properly, producing samples
comparable to diﬀusion models. We conduct ablation studies to conﬁrm the importance of multi-scale
mechanisms employed in ProjectedGAN for successfull ﬁne-scale handling.
To further assess the quality of generated samples, we conduct a small study with human
observers, with results suggesting that it is impossible even for the experts from the solar imaging
community to tell the generated from real sample images. We discuss the implications of our study
and conclude that after training on a large volume of high quality scientiﬁc data, generative models
are capable of producing realistic high resolution solar images with a level of detail that makes
generated samples indistinguishable from real observations for human experts in the ﬁeld.
We open-source the code for training, evaluating and the workﬂows around the dataset pre-
processing. We hope that providing this to the community will result in follow-up research using high
resolution - up to 4096x4096 - SDO images to further study generative modelling on an impacting
scientiﬁc dataset that has high demands with regard to accurate generation of important details
across diﬀerent scales.
2 Related Work
Generative adversarial networks. Since the suggestion of GANs in 2014 [ 14], this approach has
led to the generation of synthetic images of ever increasing quality in diﬀerent domains. GANs seek
to ﬁnd a deep neural network G(z;G)parametrized by weights Gthat maps random numbers zto
generated images xg, the generator. The antagonist, the discriminator D, parametrized by D, acts
as a critic that is trained to distinguish the generated images from real images xrby estimating a
probability the image is real. To aim for replicating the real data density, the generator should move
along those directions in G-space which make the correct estimation for discriminator more diﬃcult.
A key ingredient is thus a formulation of loss Lthat generates a suitable gradient signal for both
components. Formally, such loss to be minimized can be derived from a minimax problem:
min
Gmax
DE
xpr(x)[Lr(D(x;D))] +
E
zpz(z)[Lg(D(G(z;G) ;D))]:
The exact functional form of the loss, just as the family of neural networks of both generator and
discriminator are under active research, and many extensions of this scheme e.g. with regularizing
terms have been proposed. While very successful, the minimax structure of the training objective
makes the training notoriously unstable, leading to problems like mode collapse and non-convergence.
Recently, GANs from the StyleGAN-family have been especially successful at generating images
[23], where a key structure of the generator network is inspired by style transfer applications: before
2feeding the random numbers z into a standard generator, a multilayer perceptron preprocesses z and
feeds its output into diﬀerent layers of the generator. In StyleGAN2, the simple feedforward design
of both the generator and the discriminator architecture are changed to skip networks and residual
networks respectively [ 24]. To enable data-eﬃcient training and to overcome inherent instabilities
leading to mode collapse, approaches like DiﬀAug [ 49] and StyleGAN2-ADA[ 21] transform images
with diﬀerentiable augmentations before feeding them into the discriminator. This can lead to
impressive results even under low data condition [ 21]. StyleGAN3 eliminates texture sticking
artifacts by employing signal processing techniques like Fourier feature mapping into the generator’s
architecture [ 22]. Signiﬁcant architectural changes to the discrimininator side were introduced with
ProjectedGAN [ 39]. ProjectedGAN introduces three main novelties there: (a) a frozen pretrained
feature network is used to extract features from both generated and real images, (b) these features
are embedded into a higher-dimensional space and mixed across scales, and (c) then fed into multiple
trainable discriminators. ProjectedGAN reduces computational eﬀort by introducing a frozen
pre-trained feature network into which generated and real images are fed. Additionally, outputs
obtained from diﬀerent layers of the feature network are mixed before each output is processed
by an independent discriminator [ 39]. StyleGAN-XL [ 40] scales this approach up by introducing
progressive growing and leveraging labeled data through classiﬁer guidance and pre-trained class
embedding. With a StyleGAN3 generator increased in width, the authors generate images with
impressive quality in a dataset with very diverse image content such as ImageNet [10].
Diﬀusion models. Diﬀusion models [ 41,43,18] are a class of generative models that have
shown impressive results in generative modelling, rivaling or surpassing GANs in terms of quality,
mode coverage [ 46], and diversity of the samples [ 11] and are the basis of recent breakthroughs in the
text-to-image task in works such as DALL-E 2 [ 34] and Imagen [ 37]. Compared to GANs, they do
not suﬀer from training instabilities and like other likelihood-based models, they provide better mode
coverage [ 46]. The main downside of diﬀusion models is that they are much slower at generating
samples (inference time) than GANs as they require multiple (denoising) steps, which can vary from
hundreds to thousands of forward passes of the model. Following the original works [ 41,43,18],
several works have been proposed to make sample generation faster by using a more eﬃcient sampling
procedure [ 42,29] or by estimating the denoising function with a multi-modal distribution [ 47] or by
using distillation [ 38,30]. In another line of work, latent diﬀusion models [ 36] (LDM) proposed to
apply a denoising diﬀusion model on a pre-trained VQGAN [12] latent space instead of raw pixels,
focusing on high level details and leaving modelling ﬁne-scale details to VQGAN. LDMs have been
shown to require much less denoising steps than vanilla diﬀusion models, and are the basis of the
recentStable Diﬀusion that was used for text-to-image tasks. These works have been however so far
applied mostly to natural images rather than specialized domains such as solar data, and rarely in
setups where the quality of ﬁne-scale details is critical.
Applications in solar physics. Due to the large data set sizes in solar physics, numerous
applications of machine learning and artiﬁcial neural network methods have been employed in recent
years. Examples are solar ﬂare prediction [ 5], coronal hole detection [ 35], coronal mass ejection
(CME) prediction [ 6], solar energetic particle event prediction [ 25], 3D reconstruction of coronal
loops of the solar magnetic ﬁeld [ 8], and sunquake detection [ 31]. The interactive HelioML book [ 7]
gives a more extended overview of machine learning work in heliophysics.
Since especially solar image data from the SDO mission requires extensive preprocessing to have
a uniform dataset available for machine learning, eﬀorts have started to make such datasets available
to the broader community, see, e.g., [ 13], where a large, uniﬁed and multispectral dataset of the SDO
3mission was compiled. However, the resolution of the images in this dataset is limited, therefore, we
follow our own data preparation routine, as described in the following section, to enable learning
with high resolution.
Studies that directly use high resolution solar images or even attempt unconditional generative
modelling are still rare. Eﬀorts focus on the conditional generation of e.g. AIA coronal data
from HMI magnetogram data [ 32] or magnetograms for the solar far side where no corresponding
instruments are available [ 26,44,19]. There are also attempts to desaturate overexposed images
[48], reconstructions of the solar radiation ﬁeld [ 4], and approaches to directly predict ﬂaring activity
from full solar disk images [ 20]. However, unconditional noise-to-image synthesis appears to be
still understudied in the ﬁeld of solar physics. We think such a fundamental approach can serve as
a testbed for model performance, where lessons learned can be transferred to conditional setups.
Furthermore, generative models can help with data augmentation in situations where data availability
(e.g. for certain wavelength ranges) is still limited.
Image Quality Metrics. In practice, the Fréchet Inception Distance [17] is the most frequently
applied image quality metric. It employs the features of the pool3layer (dimension 2048) of the
pre-trained Inception-v3 neural network. These features are calculated both for real images and
generated images and the Fréchet distance between both distributions is calculated assuming each
feature distribution is a multivariate Gaussian. Periodic calculation of the FID for monitoring
the training progress is implemented and has been useful for this study as well. However, FID
has limitations, (a) it is using the Inception-v3 [ 45] network which was pre-trained on natural
images (ImageNet), where the data distribution is largely dissimilar to solar data. In addition, the
pre-trained feature extractors used in ProjectedGAN are also trained on ImageNet which might give
a superﬁcial advantage to ProjectedGAN models’ (b) it is well known that FID is biased and the
bias can depend on the sample size but also the generative model [ 3,9], which can make comparison
between models hard (c) standard FID setup resize images to 299x299, thus it is not ideally suitable
to measure quality of ﬁne-scale details.
To overcome (a), we control FID reported performance using a random Inception-v3 network
(rFID) and RN50-based CLIP pre-trained network, following [ 39]. To overcome (b), we consider
the unbiased KID metric [ 3] as an alternative to FID. To overcome (c), we consider a patch-based
version of FID, where FID is computed on patches at diﬀerent scales.
Next to quantitative image quality metrics such as FID, we found that visual inspection can
guide the investigation as well. Key features are also accessible to the non-experts (a) the roundness
of the solar disc and (b) closed loop-like structures in the solar atmosphere, so called coronal loops.
See Fig. 1 for a visual account of these features. While round discs could be obtained in many cases,
especially the existence of coronal loops in generated data turned out to be the key indicator of
good ﬁne scale image quality. The presence or absence of these structures in generated data that
can be found in most images with high solar activity is the qualitative characteristics that we found
to leave the smallest ambiguity.
In Sec. 6, we report on feedback from human subjects not connected to our study on the results
we have perceived as being best. The reader is kindly referred to the supplemental material in
Appendix A for more detailed impressions of the generated images.
43 Dataset
We base our study on a subset of Extreme UltraViolet (EUV) images from the SDO AIA instrument
[28]. SDO takes data from Earth orbit since February 2010, oﬀering an unprecedented, very large
dataset of solar images in diﬀerent optical and EUV spectral bands, capturing solar atmospheric
structures in high resolution and with excellent coverage and temporal cadence of one image every
12 seconds. It provides high resolution solar observations (up to 4096x4096 pixels). This accounts
for about 1TB of data per day for almost one full solar cycle. The mission also carries a Helioseismic
and Magnetic Imager (HMI) instrument that produces spatially resolved doppler and magnetogram
images with the same spatial resolution and similar temporal cadence as AIA. Each EUV spectral
range explores diﬀerent heights in the solar chromosphere and corona, hence is capable of detecting
diﬀerent structures of interest that contain complementary information about the state of the solar
atmosphere.
Data from SDO is available in diﬀerent formats and at diﬀerent processing levels. We base our
dataset on Level 1 data that can be obtained, for instance, through the SunPy1Federated Internet
Data Obtainer (FIDO) API. The data is downloaded in the Flexible Image Transport System (FITS)
format standard, containing the image pixel intensities together with a large set of metadata.
We process the image data with routines provided by the AIA team in the python package
AIApy2[2], which includes scaling the images to a common plate scale and correcting for instrument
degradation eﬀects. Our pipeline is similar to the procedure described in [13].
For our generative training, we use a dataset of 40K images spread evenly across the SDO mission
duration to cover all solar activity levels. We ﬁlter non-valid images3. The raw data has pixel
intensities ranging from 0 to 16383 where most of the mass is concentrated between 0 and 500;
there are as well negative pixel values which are considered as instrument measurement errors. We
preprocess the data by clipping the pixel values to 1 as a minimum value, then applying a (natural)
log transform followed by normalization to [0;1]by dividing by the maximum value. We train our
models on AIA 193Å subsampled images of size 1024x1024. For visual inspection, real and generated
images are colorized with the colormap sdoaia193 from the SunPy package to align with the most
common visualization of the data.
4 GAN models experiments
4.1 Setup and training procedure
Here we describe setup of series of experiments conducted using diﬀerent GAN models in our attempt
to generate high quality solar image samples, following previous works that report high quality
generation for various natural images.
In a series of experiments, we applied the published implementation of StyleGAN2-ADA, Style-
GAN3, ProjectedGAN and StyleGAN-XL to our prepared dataset. All implementations are based
on the StyleGAN2-ADA implementation by NVIDIA4. We adjusted the respective implementations
for the usage on the supercomputer, and faithfully reproduced the computational setup following
1https://sunpy.org/
2https://aiapy.readthedocs.io/
3The raw FITS images have a metadata "Quality" ﬁeld, we only keep valid ones with Quality=0, see https:
//stanford.io/3R78mVV
4https://github.com/NVlabs/stylegan2-ada-pytorch
5Figure 1: Sample of a real EUV solar image in 193Å with indications to the most typical prominent
features commonly observed, such as coronal holes, active regions and closed coronal loops with
many ﬁne scale structures.
6Figure 2: Image quality comparison between (from left to right) a real image, an image generated with
ProjectedGAN, with StyleGAN2 and StyleGAN3, and a diﬀusion model (ADM). Below each image,
a zoomed in version of the region indicated with the white box is displayed. Only ProjectedGAN
and ADM can reproduce coronal loops, the reentrant structure in the solar atmosphere. StyleGAN2
and StyleGAN3 cannot reproduce these features.
previous work. This involves introducing a multi-node launching procedure, as an 8-GPU-setup
as chosen in the previous work requires the usage of two compute nodes. The implementation
periodically performs an FID evaluation of generated images. In agreement with visual inspection,
we report the generated images with the lowest FID as the best images in every experiment.
Motivated by the failures of the initial experiments, we suspected the relative dynamics of
generator and discriminator learning process to be the key reason. Therefore, we investigated
diﬀerent hyperparameter setups with separately varied learning rates and number of optimization
steps for generator and discriminator for ProjectedGAN. Especially, a reduced generator learning rate
would, in many cases improve the result. However, we also observed a strong ﬂuctuation in the FID
measured after training. Due to the computational eﬀort, we repeated only our best ProjectedGAN
conﬁguration (Suppl. Tab. 2) ﬁve times (Suppl. Fig. 6).
4.2 Results
Our main results obtained with ProjectedGAN, StyleGAN2-ADA and StyleGAN3 are shown in
Fig. 2 along with real images. The mean FID measured in ﬁve ProjectedGAN runs is evaluated
to 4.2 with a standard deviation of 2:0. In the panel, we show a generated sample for a run
with the best obtained FID of 2.2. Visually, the quality of the diﬀerent runs is hard to distinguish.
StyleGAN2-ADA and StyleGAN3 create images of which the FID is evaluated to 15.0 and 11.0
respectively. While ProjectedGAN’s suns are round, StyleGAN2’s suns deviate from circular shape.
StyleGAN3’s suns are better, yet deformation is visible. The quality of ﬁne scale details is very
diﬀerent. For StyleGAN2 the visual image quality appears coarse, while StyleGAN3 shows intricate
7details but with very visible artifacts. Both models do not produce visual structures resembling
coronal loops. ProjectedGAN’s visual features are very ﬁne and clearly also show coronal loops, and
even for the expert, it is diﬃcult to separate real from fake images. None of the training processes
shows signs of mode collapse.
In order to understand this behaviour better, we have performed a series of ablations which
systematically investigate which innovations of ProjectedGAN contribute to this breakthrough. This
ablation study is summarized in Table 1 and selected results are shown in Fig. 3. ProjectedGAN
comes with three main innovations that all aﬀect the structure of the discriminator. On the one
hand, a frozen pre-trained network is used to extract features that are subsequently passed on. To
assess the necessity of pre-training we replaced the pre-trained weights with randomly initialized
weights. To our surprise, this led the solar shape intact and generated reasonable coarse features, but
led to signiﬁcant quality reduction on the ﬁne scale, consistent with an FID of 17.8. Furthermore,
we investigated if a change in the feature network depth aﬀects the image quality. While the
ProjectedGAN authors observe a slight deterioration of image quality, in our case replacing the
EﬃcientNet-Lite0 by respectively deeper variants Lite1, Lite2 and Lite3 does not aﬀect the image
quality (see Suppl. Fig. 4).
Run FID #SR CL
ProjectedGAN (Baseline) 4.2 2.0 ++ ++
Random feature network 17.8 ++ 0
EﬃcientNet-Lite1 7.4 ++ ++
EﬃcientNet-Lite2 4.1 ++ ++
EﬃcientNet-Lite3 3.8 ++ ++
No Cross Scale Mixing 9.6 + +
No Cross Scale/Channel Mixing 11.0 + +
Discriminator 1 10.7 0 +
Discriminator 1,2 7.4 + +
Discriminator 1,2,3 6.5 + ++
Augmentations oﬀ 4.2 ++ ++
Trainable Projections 7.2 ++ ++
Table 1: ProjectedGAN ablations. The columns SR and and CL indicate results from qualitative
visual inspection regarding sun roundness (SR) and presence of coronal loops (CL). A zero indicates
unsatisfactory performance, two plus symbols indicate that deviations from roundness is very diﬃcult
to see or that coronal loops are clearly visible. A single plus sign indicates that artefacts a still
clearly visible.
Furthermore, ProjectedGAN employs a pair of techniques called Cross-Channel and Cross-Scale
Mixing (CCM and CSM). CCM embeds the feature channels of each convolutional output into a
higher-dimensional space. CSM mixes these feature representations from coarser into ﬁner scales by
interpolating and summation. We observe that especially CSM leads to a considerable improvement
of image quality. If either projection method is omitted, the solar roundness is not as good, and the
quality of ﬁne-scale details deteriorates. Coronal loops remain present but exhibit clear artifacts.
Additionally, ProjectedGAN employs multiple discriminators, i.e. the mixed features are digested
by multiple networks with independent discriminator losses of equal functional form. This structure
of multiple discriminators ensures that features on all scales can be employed independently. We
8Figure 3: Qualitative Overview of the ProjectedGAN ablation studies. The original images are
displayed on the left and diﬀerent ablation studies beneath. Each column displays representative
example of (top) the solar disc and (bottom) a coronal loop region.
investigate how using only one, only two and only three discriminators aﬀect the result. Our results
clearly indicate an improvement of the image quality by each discriminator. Interestingly, by using
only D1, the discriminator that processes only ﬁne-scale features directly, and coarse features only
through multiple levels of cross-scale mixing, produces suns with appealing ﬁne-scale scale features
but clear deviations from spherical shape.
Ultimately, we investigated if disabling the applied augmentation has inﬂuence on the obtained
results and if training the parameters of the projections inﬂuences the result. The ﬁrst can be ruled
out, and in the second case we observe a slight decrease in image quality, with FID dropping to 7.3.
5 Diﬀusion models experiments
5.1 Setup and training procedure
WefollowADM[ 11], andusetheirUNetarchitecturebasedonconvolutionalresidualblocksandglobal
attention at diﬀerent resolutions and adapt OpenAI’s implementation of ADMs5. We experiment
with diﬀerent denoising steps (250, 500, 1000, 2000, 4000) and train the models for a maximum of
100K training steps with a learning rate of 0:0001and a batch size of 64. For sample generation, we
use 250 steps to make generation faster.
5.2 Results
For diﬀusion models, the best FID we obtain is 15:3(see Fig. 2) with 1000 training denoising steps
and 250 sampling steps. Interestingly, even if the FID is much higher than the best ProjectedGAN
results (FID= 2:2) obtained in Sec. 4.2, we observe that the best diﬀusion model is able to generate
a correct spherical shape, and high quality ﬁne-scale details, e.g., we observe that it can successfully
reproduce coronal loops (see also Suppl. Fig. 5). To make sure ProjectedGANs do not have an
5https://github.com/openai/guided-diffusion
9advantage in FID due to the fact that it is using ImageNet pre-trained features on the discriminator,
we follow [ 39] and also compute rFID and CLIP-RN50. We ﬁnd that the order of diﬀerent models
(ProjectedGAN ablations in Sec. 4.2 together with diﬀusion models) is consistent. We ﬁnd a
Spearman rank correlation of = 0:75between FID and rFID, and = 0:94between FID and
CLIP-RN50, also ProjectedGANs have consistently smaller FID compared to diﬀusion models (see
also Suppl. Tab. 4, 6, 7).
6 Assessing quality by human experts study
We conducted a small exploratory study with a sample of 20 human subjects with diﬀerent proﬁciency
in solar physics research. On a scale from 1 to 5 they rated their expertise with an average of 2.6
points and a standard deviation of 2.42. We showed them a sample of 5 real and 5 fake samples
generated from both the best GAN results and the diﬀusion models. The aim was to test if humans
can distinguish real from fake samples signiﬁcantly better than random guessing.
The average score of the entire group of 20 subjects was 4.55 with a standard deviation of 1.39.
The histogram of the number of correct responses is shown in Fig. 4. The result is consistent with
the hypothesis of random guessing, with an aggregate two-sided p-value of 0.66. The data indicates
a small correlation between the expertise rating and the number of correct responses. Overall, our
sample size is too small to allow for ﬁrm conclusions but we view this as indication that humans,
even with expertise in solar EUV images, struggle to identify fake images reliably.
2 4 6 8
Correct Responses012345Number of Subjects
1 2 3 4 5
Expertise Rating0123456Number of Subjects
2 3 4 5 6 7
Correct Responses12345Expertise Rating
Figure 4: Histograms of the number of correct responses (out of 10 questions) from the human expert
study (left) and their expertise self assessment (on a scale 1-5, middle). The correlation between
both is shown in the right panel with a correlation coeﬃcient of 0.46.
7 Discussion
Generation of ﬁne-scale details. Going through intensive experiments with various GAN
approaches, we ﬁnally obtain with ProjectedGAN solar images that have comparable good quality to
images produced in the experiments with the diﬀusion model. We clearly see that no single technique
introduced by ProjectedGAN can be made solely responsible for the observed improvement over
less successful GAN models with basic StyleGAN architecture. To achieve good quality, following
components turn out to be necessary from the conducted ablation study: the pre-trained feature
10network, feature mixing across scales and independent discriminators for all scales. This clearly
underlines the importance of the employed discriminator architecture.
We observe that architectures like StyleGANv2 and StyleGANv3, that do not possess such
explicit mechanisms to deal with multi-scale nature of the image signal built into discriminator, fail
to generate necessary ﬁne-scale structures in the solar images. Remarkably, natural images like faces
do not pose such a diﬃculty for the models with basic StyleGAN architectures that struggle on
solar image data (Suppl. Fig. 7). This hints on certain properties of the dataset or single image
signal statistics that make ﬁne-scale detail learning harder for GAN models without discriminator
mechanisms for explicit multi-scale treatment. Moreover, as methods known to stabilize training
and avoid mode collapse by introducing diﬀerentiable augmentation like DiﬀAug or StyleGAN-Ada
alone did not manage to ﬁx the ﬁne-scale generation in basic StyleGANs, we assume that training
instabilities and mode collapse are not the main driving force behind ﬁne-scale corruption here. In
addition to multi-scale mechanisms, the importance of having a ﬁxed, pre-trained feature extractor
is evident, as either starting with ﬁxed random weights or unfreezing the feature extractor destroys
good ﬁne-scale generation despite multi-scale mechanisms still in place.
Contrary to the eﬀorts necessary to get ﬁne-scale generation working well in GANs, standard
diﬀusion model (ADM) operating in pixel space and employing U-Net works without extensive
tuning. This is in line with the already observed beneﬁts of diﬀusion models over GANs, and hints
on advantage of multi-step denoising generation methods for proper ﬁne-scale handling, as opposed
to single-step one pass generation employed in GANs.
Comparingdiﬀerentgenerativemodels. Whencomparingthequalityofsolarimagesamples
generated by diﬀerent models, we notice various degrees of degradation either on ﬁne- or coarse-scale
level. For instance, for ﬁne-scales, we see particular salient solar image features, like coronal loops
outside, or in the active regions within solar disk or on the solar limb, either clearly expressed, or
corrupted or entirely gone, depending on model quality. On the coarse-scale level, easily detectable
is the preservation of the ideally spherical shape of the solar disk or its distortion.
As we measure FID, the scores obtained for the GAN models seem on the one hand to be
well ordered according to the observed quality of ﬁne and coarse scales of the solar images in
correspondence with the trained model quality. The lowest FID scores observed correspond clear
coronal loop structures at the solar limb and on the solar disk at ﬁne scale as well as to the spherical
shape of the sun disk at coarse scale. ProjectedGANs achieve the lowest FIDs. Ablations that
corrupt either ﬁne or coarse scale result in higher FIDs, where we see distortion or loss of coronal
loop structures and distortion of the solar disk shape. Poor models with highest FID scores like
StyleGANv2 family do not posses any proper ﬁne scale structure at all, keeping though a proper
coarse round shape of the solar disk.
On the other hand, we observe that despite being the best among assessed image quality on ﬁne
and coarse scale, diﬀusion models obtain higher FID than generated samples obtained by GANs
which have poorer quality upon visual inspection. This inconsistency cannot be explained solely
by the fact that we use a feature extractor pre-trained on ImageNet in ProjectedGANs, which
would give an advantage in terms of FID, since Inceptionv3 is also trained on ImageNet. We ﬁnd
consistently better performance for ProjectedGANs when using other metrics (rFID, CLIP-RN50)
and similar model ranking in general (Suppl. Tab. 4, 6, 7). After checking the pixel distribution,
we observe that the histogram of pixel intensities of diﬀusion (ADM) samples do not match the
histogram of real data (see Suppl. Fig. 8): the real data has heavier tails at the left side (small pixel
values) and diﬀusion samples have heavier tails on the right side (large pixel values), while the one
11obtained with ProjectedGAN matches the real data much better. This distribution mismatch may
be the reason for higher FID scores and the inconsistency observed. Again, this calls for caution
when judging generated sample image quality - FID alone cannot give a comprehensive answer, and
other domain-speciﬁc scores or visual inspection by experts might be necessary, as it is this case in
our study for solar images.
When conducting the human experts study, we were then taking those sample images that showed
high quality on both ﬁne and coarse scale - generated by ProjectedGAN and ADM. Our study with
human experts conﬁrms the quality of generated samples - as human were not able to distinguish
reliably real from generated solar images.
Limitations of this study While we took various GAN models for the comparative study and
covered a broad range of possible architectures, we did not have enough computational resources to
conduct a dense systematic hyperparameter search for tuning the training procedure. We therefore
cannot exclude the possibility that further extensive tuning would enable basic StyleGAN models
to get ﬁne-scale details of suﬃcient quality on solar images. However, it is clear that models
like ProjectedGAN or ADM did not require such tuning to work well, providing evidence for the
beneﬁts of explicit multi-scale mechanisms in GANs and for multi-step diﬀusion generation. Further,
diﬀusion based training as used in ADM leads to slow inference compared to GANs. Approaches
like latent diﬀusion [ 36] training in latent instead of pixel space substantially decrease inference
time. Pre-training on large-scale generic image datasets may further have increased the capability of
feature extractors like used in ProjectedGAN. Finally, we train here only on static solar images of a
speciﬁc wavelength without taking into account temporal or multi-spectral information that does
exist in the SDO dataset.
8 Conclusion and Outlook
Encouraged by the previous works showing capability to generate high quality high resolution
natural images using architectures like StyleGAN, we started our study with initial expectation
to generate similar high quality high resolution samples from the SDO dataset containing solar
images using same methods. However, we observe that basic StyleGAN architectures and their
extensions like StyleGANv2, StyleGAN-ADA, DiﬀAug and StyleGANv3 are not capable of generating
solar images of suﬃcient quality, failing at crucial ﬁne-scale details - which is not observed in this
way for the scenario of natural image generation. This calls for caution when applying generative
approaches highly successful on natural image data to scientiﬁc image signals. By executing extensive
experiments, we ﬁnd that ProjectedGAN with its pre-trained feature extractor, cross scale mixing
and multi-scale discriminators provides solar image samples with high quality on both ﬁne and
coarse scale. Diﬀusion-based ADM can also achieve comparable high sample quality without tuning
eﬀort. Samples created by both methods are found to be indistinguishable from real data in a human
expert evaluation experiment.
As observed in this study, the scientiﬁc image dataset scenario may diﬀer from standard re-
quirements for natural image generation. To accelerate further progress in direction of generative
modeling for high resolution scientiﬁc data, we open-source the outcomes of this work. For solar image
modelling, exploring latent space of trained models, and using further information like temporal,
multi-spectral and textual meta data available from SDO are future directions leading to powerful,
physics-aware generative models for solar state interpretation and prediction.
129 Acknowledgements
We would like to thank Ruggero Vasile for his contributions during the initial phases of the project
with StyleGAN2 models, including data gathering, data pre-processing and preparation, as well as
Yuri Shprits for support in the begining of the project. We would like to also thank Katja Schwarz
for all her insights and experiment suggestions on our ProjectedGAN ablations.
This work is supported by the Helmholtz Association Initiative and Networking Fund under the
Helmholtz AI platform grant and the HAICORE@JSC partition. The authors gratefully acknowledge
the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by
providing computing time through the John von Neumann Institute for Computing (NIC) on the
GCS Supercomputer JUWELS at the Jülich Supercomputing Centre (JSC). Partial support of DFG
(SFB 1491) is acknowledged.
References
[1]Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regular-
ization for self-supervised learning. arXiv preprint arXiv:2105.04906 , 2021. 22
[2]Will Barnes, Mark Cheung, Monica Bobra, Paul Boerner, Georgios Chintzoglou, Drew Leonard,
Stuart Mumford, Nicholas Padmanabhan, Albert Shih, Nina Shirman, David Stansby, and Paul
Wright. aiapy: A Python Package for Analyzing Solar EUV Image Data from AIA. The Journal
of Open Source Software , 5(55):2801, Nov. 2020. 5
[3]Mikołaj Bińkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying
mmd gans. arXiv preprint arXiv:1801.01401 , 2018. 4
[4]Kyriaki-Margarita Bintsi, Robert Jarolim, Benoit Tremblay, Miraﬂor Santos, Anna Jungbluth,
James Paul Mason, Sairam Sundaresan, Angelos Vourlidas, Cooper Downs, Ronald M. Caplan,
and Andrés Muñoz Jaramillo. SuNeRF: Validation of a 3D Global Reconstruction of the Solar
Corona Using Simulated EUV Images. arXiv e-prints , page arXiv:2211.14879, Nov. 2022. 4
[5]M. G. Bobra and S. Couvidat. Solar Flare Prediction Using SDO/HMI Vector Magnetic Field
Data with a Machine-learning Algorithm. apj, 798(2):135, Jan. 2015. 3
[6]M. G. Bobra and S. Ilonidis. Predicting Coronal Mass Ejections Using Machine Learning
Methods. apj, 821(2):127, Apr. 2016. 3
[7]Monica G. Bobra and James P. Mason. Machine Learning, Statistics, and Data Mining for
Heliophysics . ., 2020. 3
[8]Iulia Chifu and Ricardo Gafeira. 3d solar coronal loop reconstructions with machine learning.
The Astrophysical Journal Letters , 910(1):L10, 2021. 3
[9]Min Jin Chong and David Forsyth. Eﬀectively unbiased ﬁd and inception score and where
to ﬁnd them. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pages 6070–6079, 2020. 4
[10]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009. 3
[11]Prafulla Dhariwal and Alexander Nichol. Diﬀusion models beat gans on image synthesis.
Advances in Neural Information Processing Systems , 34:8780–8794, 2021. 1, 2, 3, 9, 21
13[12]Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution
image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pages 12873–12883, 2021. 3
[13]Richard Galvez, David F Fouhey, Meng Jin, Alexandre Szenicer, Andrés Muñoz-Jaramillo,
Mark CM Cheung, Paul J Wright, Monica G Bobra, Yang Liu, James Mason, et al. A machine-
learning data set prepared from the nasa solar dynamics observatory mission. The Astrophysical
Journal Supplement Series , 242(1):7, 2019. 3, 5
[14]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural
Information Processing Systems , 27, 2014. 1, 2
[15]Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering
interpretable gan controls. Advances in Neural Information Processing Systems , 33:9841–9850,
2020. 19, 25
[16]Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 16000–16009, 2022. 22
[17]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems , 30, 2017. 4
[18]Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diﬀusion probabilistic models. Advances
in Neural Information Processing Systems , 33:6840–6851, 2020. 3
[19]Hyun-Jin Jeong, Yong-Jae Moon, Eunsu Park, Harim Lee, and Ji-Hye Baek. Improved AI-
generated Solar Farside Magnetograms by STEREO and SDO Data Sets and Their Release.
apjs, 262(2):50, Oct. 2022. 4
[20]Eric Jonas, Monica Bobra, Vaishaal Shankar, J. Todd Hoeksema, and Benjamin Recht. Flare
Prediction Using Photospheric and Coronal Image Data. solphys, 293(3):48, Mar. 2018. 4
[21]Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
Training generative adversarial networks with limited data. Advances in Neural Information
Processing Systems , 33:12104–12114, 2020. 3, 20
[22]Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen,
and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Information
Processing Systems , 34:852–863, 2021. 3
[23]Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 4401–4410, 2019. 2
[24]Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages 8110–8119, 2020. 3
[25]Spiridon Kasapis, Lulu Zhao, Yang Chen, Xiantong Wang, Monica Bobra, and Tamas Gombosi.
Interpretable Machine Learning to Forecast SEP Events for Solar Cycle 23. Space Weather ,
20(2):e2021SW002842, Feb. 2022. 3
14[26]TaeyoungKim, EunsuPark, HarimLee, Yong-JaeMoon, Sung-HoBae, DayeLim, SoojeongJang,
Lokwon Kim, Il-Hyun Cho, Myungjin Choi, and Kyung-Suk Cho. Solar farside magnetograms
from deep learning analysis of STEREO/EUVI data. Nature Astronomy , 3:397–400, Mar. 2019.
4
[27]P. Langley. Crafting papers on machine learning. In Pat Langley, editor, Proceedings of the
17th International Conference on Machine Learning (ICML 2000) , pages 1207–1216, Stanford,
CA, 2000. Morgan Kaufmann.
[28]James R. Lemen, Alan M. Title, David J. Akin, Paul F. Boerner, Catherine Chou, Jerry F. Drake,
Dexter W. Duncan, Christopher G. Edwards, Frank M. Friedlaender, Gary F. Heyman, Neal E.
Hurlburt, Noah L. Katz, Gary D. Kushner, Michael Levay, Russell W. Lindgren, Dnyanesh P.
Mathur, Edward L. McFeaters, Sarah Mitchell, Roger A. Rehse, Carolus J. Schrijver, Larry A.
Springer, Robert A. Stern, Theodore D. Tarbell, Jean-Pierre Wuelser, C. Jacob Wolfson, Carl
Yanari, Jay A. Bookbinder, Peter N. Cheimets, David Caldwell, Edward E. Deluca, Richard
Gates, Leon Golub, Sang Park, William A. Podgorski, Rock I. Bush, Philip H. Scherrer, Mark A.
Gummin, Peter Smith, Gary Auker, Paul Jerram, Peter Pool, Regina Souﬂi, David L. Windt,
Sarah Beardsley, Matthew Clapp, James Lang, and Nicholas Waltham. The Atmospheric
Imaging Assembly (AIA) on the Solar Dynamics Observatory (SDO). solphys, 275(1-2):17–40,
Jan. 2012. 5
[29]Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diﬀusion models
on manifolds. arXiv preprint arXiv:2202.09778 , 2022. 3
[30]Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
On distillation of guided diﬀusion models. arXiv preprint arXiv:2210.03142 , 2022. 3
[31]Vanessa Mercea, Alin Razvan Paraschiv, Daniela Adriana Lacatus, Anca Marginean, and Diana
Besliu-Ionescu. A machine learning enhanced approach for automated sunquake detection in
acoustic emission maps. Solar Physics , 298(1):4, 2023. 3
[32]Eunsu Park, Yong-Jae Moon, Jin-Yi Lee, Rok-Soon Kim, Harim Lee, Daye Lim, Gyungin Shin,
and Taeyoung Kim. Generation of Solar UV and EUV Images from SDO/HMI Magnetograms
by Deep Learning. apjl, 884(1):L23, Oct. 2019. 4
[33]W Dean Pesnell, B.J. Thompson, and PC Chamberlin. The solar dynamics observatory (SDO) .
Springer, 2012. 1
[34]Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical
text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022. 3
[35]Martin A. Reiss, Stefan J. Hofmeister, Ruben De Visscher, Manuela Temmer, Astrid M. Veronig,
Véronique Delouille, Benjamin Mampaey, and Helmut Ahammer. Improvements on coronal
hole detection in SDO/AIA images using supervised classiﬁcation. Journal of Space Weather
and Space Climate , 5:A23, July 2015. 3
[36]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-resolution image synthesis with latent diﬀusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 10684–10695, 2022. 1, 3, 12
[37]Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed
Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.
Photorealistic text-to-image diﬀusion models with deep language understanding. arXiv preprint
arXiv:2205.11487 , 2022. 3
15[38]Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diﬀusion models.
arXiv preprint arXiv:2202.00512 , 2022. 3
[39]Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected gans converge faster.
Advances in Neural Information Processing Systems , 34:17480–17492, 2021. 2, 3, 4, 10
[40]Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse
datasets. In ACM SIGGRAPH 2022 conference proceedings , pages 1–10, 2022. 1, 3
[41]Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In International Conference on Machine
Learning , pages 2256–2265. PMLR, 2015. 3
[42]Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diﬀusion implicit models. arXiv
preprint arXiv:2010.02502 , 2020. 3
[43]Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
distribution. Advances in neural information processing systems , 32, 2019. 3
[44]Wenqing Sun, Long Xu, Suli Ma, Yihua Yan, Tie Liu, and Weiqiang Zhang. A Dynamic
Deep-learning Model for Generating a Magnetogram Sequence from an SDO/AIA EUV Image
Sequence. apjs, 262(2):45, Oct. 2022. 4
[45]ChristianSzegedy, VincentVanhoucke, SergeyIoﬀe, JonShlens, andZbigniewWojna. Rethinking
theinceptionarchitectureforcomputervision. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2818–2826, 2016. 4
[46]Lucas Theis, Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models.arXiv preprint arXiv:1511.01844 , 2015. 3
[47]Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma
with denoising diﬀusion gans. arXiv preprint arXiv:2112.07804 , 2021. 3
[48]Xuexin Yu, Long Xu, and Yihua Yan. Image Desaturation for SDO/AIA Using Deep Learning.
solphys, 296(3):56, Mar. 2021. 4
[49]Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Diﬀerentiable augmentation for
data-eﬃcient gan training. Advances in Neural Information Processing Systems , 33:7559–7570,
2020. 3, 20
16Supplementary
A Additional details on solar image sample quality comparison
Here we provide an overview over a wider spectrum of generated images with the best models of both
classes. Fig. 5 shows ﬁve images generated by ProjectedGAN and by the diﬀusion model (ADM) for
diﬀerent solar activity levels. The insets zoom into regions with coronal loop structures. For both
models, the subjective impression is very good. The solar shape is indistinguishable from circular
and coronal loops are clearly visible. However, the FIDs obtained for both models are signiﬁcantly
diﬀerent, being 2.2 for ProjectedGAN and 15.2 for the diﬀusion model, showing the FID alone
cannot properly reﬂect image quality.
Figure 5: Comparison of ProjectedGAN results and Diﬀusion model results. Despite the FID being
much lower for the ProjectedGAN model (FID 2.2) than the Diﬀusion model (FID 15.2), both coarse
and ﬁne scale agree when evaluated by a human. Both models generated sun of which the shape can
visually not be distinguished from a circle. Also ﬁne-scale details are of comparable quality.
17B Additional details on GAN experiments
B.1 Hyperparameters
In this section, we report the hyperparameters employed for ProjectedGAN in the conﬁguration with
which we obtained optimal results. The reported parameter names are inspired by the parameters of
the ProjectedGAN training code6, however for readability, abbreviations have been expanded.
Group Parameter Value
Generator type StyleGAN2
z_dim 64
w_dim 128
num_mapping_layers 2
G-Optimizer type Adam
betas 0,0.99
learning_rate 0.0005
D-Optimizer type Adam
betas 0,0.99
learning_rate 0.002
Training batch_size 32
num_gpus 8
Projection diffaug True
type 2 (CSM+CCM)
out_channels 64
Discriminator num_discs 4
Table 2: ProjectedGAN hyperparameters of the baseline run. The evaluation of multiple runs with
this hyperparameter set can be found in Fig. 6.
B.2 FID variation
With our studies, we observe that diﬀerent GAN training runs with identical parameters can lead
to very diﬀerent behaviour even with identical hyperparameters as given in Tab. 2. Fig. 6 shows
the FID in the course of ﬁve identical ProjectedGAN runs evaluated every 6400 iterations with
a batch size of 32. The ﬁve runs exhibit signiﬁcantly diﬀerent behaviour. In two runs the FID
decreases continuously and subsequently ﬂuctuates between 2 and 3. In three runs, the FID reaches
a pronounced minimum and starts increasing again. The resulting subjective image quality of all
runs is comparable.
B.3 Comparison to natural image generation
Here we show results of standard StyleGANv2 training on natural face image generation. Contrary
to the severe issues StyleGANv2 has when generating ﬁne-scale details for solar images that we
observe in this study (Fig. 3), such issues are absent on natural face images, as demonstrated in
Fig. 7. Fine-scale details like hair or eye structures are faithfully reproduced. This exempliﬁes that
6https://github.com/autonomousvision/projected-gan
18Figure 6: FID in the course of training of ProjectedGAN. The panel shows ﬁve runs with identical
parameters. Horizonal lines at 3,5, and 10 to guide the eye.
diﬀerences in the solar images and natural face images dataset and single image statistics used for
training are crucial for the properties of the resulting models, either being able or not able to handle
ﬁne-scale details depending on dataset nature
B.4 Latent space control
We conducted an preliminary study on unsupervised extraction of meaningful directions in latent
space of the best performing GAN model, ProjectedGAN, following the GANSpace [ 15] method.
In Fig 10, we provide a visualization of the ﬁrst and second PCA components (based on the W
space) with highest eigenvalue, where each row is an independent sample, and columns correspond
to variation of the coordinate of the component. It can be observed that the ﬁrst two principal
components span a space where both intensity of sun activity and number of corona holes can be
traversed, hinting that latent space already contains a well-shaped useful structure that contains
physically meaningful representations of the sun’s state.
C Additional details on diﬀusion model experiments
C.1 Pixel value distribution and FID
In our experiments, we measure strikingly distinct FID values for the samples generated by the
ProjectedGAN and diﬀusion-based ADM. This does not correspond to the ﬁne and coarse scale
details quality as inspected visually - both models are good in capturing such important details as
coronal loops inside and outside the sun disk as well as spherical disk shape. To understand the
reasons behind FID diﬀerences, we were further analyzing the pixel intensity distribution, comparing
distributions underlying real solar images and the generated samples from ProjectedGAN and ADM
(Fig. 8).
We observe that the pixel distribution (with values ranging from 0 to 255) generated by diﬀusion
(ADM) deviates from the real one: the real data has heaver left tail (cutoﬀ 150) while ADM has a
heavier right tail, resulting in mean pixel value of 113for real data and 127for ADM. Opposed to
19Figure7: InourbaselineexperimentswithFFHQ(humanfaces)usingStyleGANv2withdiﬀerentiable
data augmentation [ 49,21], contrary our experiments with solar data (see Fig.2) the model is able to
generate ﬁne-scale details, e.g., wrinkles, in the head hair, and in facial hair. In solar data, despite
optimizing hyper-parameters such as learning rate (for discriminator and generator), trying diﬀerent
augmentations, training on more epochs, we could not ﬁnd a setup where generation of ﬁne-scale
details is acceptable.
that, ProjectedGAN matches not only the pixel mean ( 113), but also the left and right tails. Thus,
the diﬀerence in FID is well reﬂected in diﬀerent degree of matching real data distribution between
ProjectedGAN and ADM. Here, the conclusion is in line with previous observations stating that FID
alone cannot serve as reliable measure of generated samples quality - as samples of similar quality
may have strongly diﬀerent FID scores, clearly evident in our observation.
200 50 100 150 200 250
Pixel distribution0.0000.0020.0040.0060.0080.0100.0120.0140.016
ProjectedGAN
Real data
DiffusionFigure 8: Pixel intensities of diﬀusion model samples and real data do not match together, real data
has heavier tails in the left side and diﬀusion samples have heavier tails in the right side (cutoﬀ of
150), while ProjectedGAN and real data match. This could explain the diﬀerence in FID we observe
between the samples of the two models.
C.2 Hyperparameters
Following [ 11], we use 128 base channels, 2 residual blocks per resolution, attention layers in resolution
16 and 8 with 4 heads, and a linear noise schedule. For training, we use a learning rate of 0:0001, a
batch size of 64, use an EMA rate of 0:9999and train for 100K steps. For evaluation, we select the
model checkpoint with the best FID.
We trained diﬀusion models with diﬀerent training/sampling timesteps and with/without regu-
larization. In Fig. 9a, we show the best FID obtained with number of training timesteps with models
trained up to 100K iterations. In 9b, we show the best FID obtained when varying the sampling
timesteps for a ﬁxed model trained with 1000 denoising timesteps. After observing initially that
FID starts to increase after reaching 60K iterations, we attempted to regularize the model or reduce
capacity (using 1 residual block per resolution, attention layers with 2 heads, and 32 base channels),
results are shown in Tab. 3.
Overall, the best combination we found is 1000 training timesteps, 250 sampling timesteps with
random horizontal ﬂipping. We use the best model in our comparisons with ProjectedGAN models.
Model FID
Random horizontal ﬂip + Reduced model capacity 43.3
No regularization 28.1
Dropout=0.3 25.4
Random Horizontal ﬂip 15.3
Table 3: Eﬀect of regularization and model size on diﬀusion model performance
21(a)
 (b)
Figure 9: Eﬀect of training and sampling denoising timesteps (9a) on diﬀusion model performance
(FID). In 9a, we show the best obtained FID with models trained with diﬀerent training denoising
timesteps. In 9a, we show the best FID obtained when varying the sampling timesteps with DDPM
for a ﬁxed model trained with 1000 denoising timesteps.
D Additional details on evaluation
We evaluate the models using FID, rFID, KID, CLIP-RN50 based FID, precision, and recall. We
randomly sample 50K images from each model. We provide detailed evaluation metrics in Tab. 4
and Tab. 6. In Tab. 7, we measure the aggreement between the diﬀerent metrics using Spearman
rank correlation.
First, we note that from a model selection perspective, the best model according to each metric
will lead to a diﬀerent choice, which makes it hard to automatize model selection, hence human
assessment is still very helpful, especially in domain speciﬁc datasets like solar data. We note however
that our best ProjGAN model (baseline) achieves systematically either ﬁrst or second rank under all
the metrics we consider in Tab .4 and Tab. 6. On the other hand, our best diﬀusion model have a
poor performance in all metrics compared to our ProjectedGAN-based models, despite being the
best among assessed image quality on ﬁne and coarse scale.
Wefurtherinvestigatedwhetherpre-trainingdomainspeciﬁcmodelsandusingthemforevaluation
can agree better with human assessment of solar data image generation. We pre-trained a masked-
autoencoder [ 16] (MAE) and VicReg [ 1] on solar data, and used them to compute Fréchet Distance
(FD) between real and generated samples of the diﬀerent models, and also compare them with
MAE and VicReg models pre-trained on ImageNet. For MAE pre-training on solar data, we used a
Vit-B/16 model with 75% masking ratio and we pre-trained the model for 1600 epochs. For VicReg
on solar data, we pre-trained a ResNet50 for 1000 epochs. For MAE and VicReg pre-trained on
ImageNet data, we used openly available checkpoints for B/167and ResNet50 models8respectively.
In Tab. 5, We observe that with an MAE pre-trained on solar data, our best diﬀusion model
is ranked second best (and better than ProjGAN baseline), while it is ranked poorly using both
7https://github.com/facebookresearch/mae#fine-tuning-with-pre-trained-checkpoints
8https://github.com/facebookresearch/vicreg#pretrained-models-on-pytorch-hub
22Model FID #rFID (103)#KID(103)#CLIP-FID (103)#Prec."Rec:"
ProjectedGAN (Baseline) 2.37 10.79 0.74 12.10 0.60 0.84
EﬃcientNet-Lite3 3.80 13.08 1.61 20.42 0.54 0.71
EﬃcientNet-Lite2 4.07 13.17 0.99 13.43 0.58 0.75
Augmentations oﬀ 4.19 17.34 1.62 10.81 0.66 0.51
Discriminator 1,2,3 6.45 19.25 2.50 20.48 0.65 0.29
Trainable Projections 7.22 15.12 3.88 31.89 0.48 0.57
EﬃcientNet-Lite1 7.42 18.14 4.31 24.91 0.63 0.47
Discriminator 1,2, 7.43 24.99 3.15 26.35 0.54 0.33
No Cross Scale Mixing 9.60 28.75 2.89 22.91 0.60 0.15
Discriminator 1 10.69 22.40 6.15 37.54 0.47 0.39
No Cross Scale/Channel Mixing 10.99 32.29 4.74 27.56 0.62 0.16
Diﬀusion (ADM) 15.27 140.63 15.59 111.25 0.43 0.63
Random feature network 17.72 9.01 16.44 267.15 0.15 0.57
Unfreeze feature network 171.56 3366.57 199.23 2440.33 0.01 0.00
Randomly initialized feature network (unfrozen) 252.43 5119.63 299.56 3111.04 0.00 0.00
Unfreeze feature network+Trainable Projections 328.04 7221.13 405.74 4784.95 0.00 0.00
Table 4: Detailed performance metrics of the models.Best model on each metric is highlighted in
bold.
original FID and with MAE pre-trained on ImageNet. With VicReg, we observe a diﬀerent outcome,
there the best diﬀusion model is ranked poorly both with VicReg pre-trained on ImageNet and on
solar data. We also observe that the ProjGAN with a randomly initialized feature network is ranked
ﬁrst when using MAE or VicReg pre-trained on solar data, although it has poorer sample quality
than the best ProjGAN and diﬀusion models (human assessment).
As a conclusion, model ranking can be signiﬁcantly impacted by the pre-training data used
to compute Frechet distances (in our case, ImageNet vs solar data). This ﬁnding emphasizes the
need for further investigation into the impact of pre-training data (used to train models used for
evaluation) on model selection of existing generative models.
Model FID MAE-IN-FD MAE-SOL-FD VIC-IN-FD VIC-SOL-FD
ProjectedGAN (Baseline) 2.37 8.44 29.49 3.29 4.99
EﬃcientNet-Lite3 3.80 12.44 29.87 4.88 5.80
EﬃcientNet-Lite2 4.07 11.14 29.88 4.45 5.52
Augmentations oﬀ 4.19 9.55 29.07 5.03 5.59
Discriminator 1,2,3 6.45 14.10 29.77 7.84 6.68
Trainable Projections 7.22 16.09 30.72 8.42 6.20
EﬃcientNet-Lite1 7.42 18.53 30.11 8.33 6.53
Discriminator 1,2 7.43 20.29 31.50 8.40 7.08
No Cross Scale Mixing 9.60 18.79 33.27 10.21 8.46
Discriminator 1 10.69 27.06 31.71 10.40 7.76
No Cross Scale/Channel Mixing 10.99 21.73 32.09 10.04 8.90
Diﬀusion 15.27 53.80 28.92 19.55 12.10
Random feature network 17.72 27.16 28.67 23.40 4.59
Unfreeze feature network 171.56 1141.76 66.04 365.75 494.73
Random feature network (unfrozen) 252.43 1727.87 78.49 565.27 1068.64
Unfreeze feature network + Train projs 328.04 4162.40 148.02 555.81 140.88
Table 5: Additional performance metrics of the models based on pre-trained MAE and VicReg (noted
VIC) models to compare between Fréchet Distances based on ImageNet (noted IN) and on solar data
(noted SOL). Best model on each metric is highlighted in bold, while second best is underlined .
23Model FID FID-p64 FID-p128 FID-p256
ProjectedGAN (Baseline) 2.37 1.01 0.84 0.97
EﬃcientNet-Lite3 3.80 0.96 0.99 1.24
EﬃcientNet-Lite2 4.07 1.05 0.90 0.96
Augmentations oﬀ 4.19 3.68 3.55 1.62
Discriminator 1,2,3 6.45 1.07 1.18 1.79
Trainable Projections 7.22 2.37 2.15 2.60
EﬃcientNet-Lite1 7.42 1.28 1.59 2.08
Discriminator 1,2, 7.43 1.37 1.73 2.26
No Cross Scale Mixing 9.60 1.18 1.51 2.55
Discriminator 1 10.69 2.54 2.86 4.00
No Cross Scale/Channel Mixing 10.99 1.65 1.53 2.85
Diﬀusion (ADM) 15.27 8.99 7.92 11.83
Random feature network 17.72 21.80 36.15 55.84
Unfreeze feature network 171.56 134.92 128.03 162.78
Random feature network (unfrozen) 252.43 191.38 175.69 187.80
Unfreeze feature network+Trainable Projections 328.04 414.97 441.43 506.61
Table 6: Results on FID and Patch-based FID, where FID is computed on patches, e.g., FID-p64 is
computed by using patches of size 64x64 extracted randomly from images.
FID rFID KID CLIP-RN50 Prec. Rec. MAE-IN-FD MAE-SOL-FD VIC-IN-FD VIC-SOL-FD
FID 1.00 0.75 0.97 0.94 -0.74 -0.71 0.98 0.53 0.98 0.78
rFID 0.75 1.00 0.68 0.64 -0.44 -0.83 0.75 0.73 0.71 0.98
KID 0.97 0.68 1.00 0.97-0.76 -0.64 0.96 0.48 0.96 0.72
CLIP-RN50 0.94 0.64 0.97 1.00 -0.86 -0.58 0.96 0.51 0.95 0.69
Prec. -0.74 -0.44 -0.76 -0.86 1.00 0.31 -0.81 -0.41 -0.79 -0.48
Rec. -0.71 -0.83 -0.64 -0.58 0.31 1.00 -0.66 -0.81 -0.69 -0.83
MAE-IN-FD 0.98 0.75 0.96 0.96 -0.81 -0.66 1.00 0.54 0.96 0.79
MAE-SOL-FD 0.53 0.73 0.48 0.51 -0.41 -0.81 0.54 1.00 0.53 0.72
VIC-IN-FD 0.98 0.71 0.96 0.95 -0.79 -0.69 0.96 0.53 1.00 0.76
VIC-SOL-FD 0.78 0.98 0.72 0.69 -0.48 -0.83 0.79 0.72 0.76 1.00
Table 7: We measure agreement between performance metrics using Spearman rank correlation ( ).
For each metric (rows), we highlight in boldthe metric with highest correlation.
24(a) PCA Component 1
(b) PCA Component 2
Figure 10: Unsupervised latent space control based on the Wspace of our best ProjectedGAN model
using the GANSpace[ 15] method. We visualize the the ﬁrst (a) and second (b) PCA components
with highest eigenvalue. Each row is an independent sample and columns correspond to images
obtained by varying of the coordinate of the component. We observe that the ﬁrst two principal
components span a space where both intensity of sun activity and number of corona holes can be
traversed.
25