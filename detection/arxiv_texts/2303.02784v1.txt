Censored Quantile Regression with Many Controls
Seoyun Hong∗
March 5, 2023
Abstract
This paper develops estimation and inference methods for censored quantile regression mod-
elswithhigh-dimensionalcontrols. Themethodsarebasedontheapplicationofdouble/debiased
machine learning (DML) framework to the censored quantile regression estimator of Buchinsky
andHahn(1998). Iprovidevalidinferenceforlow-dimensionalparametersofinterestinthepres-
ence of high-dimensional nuisance parameters when implementing machine learning estimators.
The proposed estimator is shown to be consistent and asymptotically normal. The performance
of the estimator with high-dimensional controls is illustrated with numerical simulation and an
empirical application that examines the eﬀect of 401(k) eligibility on savings.
Keywords: Double/debiased machine learning, Neyman orthogonality, post-selection infer-
ence, high-dimensional data, causal inference, censored quantile regression
1 Introduction
Censoring is a common problem in empirical analysis. For example, income survey data are often
right-censored due to top-coding for high-income workers and left-censored due to the minimum
wage or the wage being naturally bounded by zero. Quantile regression is particularly eﬀective in
analyzing censored outcomes due to the equivariance property of the quantile function to monotonic
transformations, such as censoring. Using this insight, censored quantile regression estimators
stemming from the pioneer work of Powell (1986) are widely used in empirical research as they
require weaker distributional assumptions on the error term compared to the traditional Tobit
model. On the other hand, there is a growing literature on the use of high-dimensional data
for causal inference in econometrics. When relying on the selection-on-observables assumption
for causal inference, researchers are motivated to include a set of control variables that may be
correlated with the main variable of interest and the outcome. Control variables can often be
high-dimensional, because of the large data sets such as text or scanner data, or from technical
controls created through transformations of raw controls, including interactions, powers, and b-
splines. When the number of control variables is large relative to the sample size, traditional
econometric methods might not perform well.
I develop estimation and inferential results to a censored quantile regression model that allows
for high-dimensional control variables. While Buchinsky and Hahn (1998) is one of the most widely
used censored quantile regression estimators with ﬁxed censoring, it is not applicable when the
dimensionofcontrolvariables piseithercomparabletothesamplesize Norevenlarger. Acommon
∗Department of Economics, Boston University, seoyun@bu.edu
1arXiv:2303.02784v1  [econ.EM]  5 Mar 2023approach in this setting is to employ regularization, such as by using the Lasso quantile regression
estimator in Belloni and Chernozhukov (2011). However, this may not be appropriate for a causal
inference setting with observational data. While the researcher includes a large set of control
variablestomitigateomittedvariablebias,Lassoselectsvariablesbasedontheircorrelationwiththe
outcome, without considering their potential confounding eﬀect. Therefore, econometricians have
revisited the classic semiparametric inference problem of making inference on a low-dimensional
parameterinthepresenceofhigh-dimensionalnuisanceparameterswhenadoptingmachinelearning
methods to traditional econometric estimators.
The objective of the proposed estimator is to oﬀer a valid inference procedure for the eﬀect
of treatment on the conditional quantile (i.e., quantile treatment eﬀect) in the presence of high-
dimensional nuisance parameters, such as coeﬃcients on many controls. First, to estimate the
nuisance parameters, I implement machine learning methods which enable estimation with high-
dimensional controls by performing regularization. However, naively plugging in machine learning
estimates could cause a severe bias in the target parameter due to the regularization bias and
overﬁtting. Therefore, I employ two tools introduced in Chernozhukov et al. (2018): (1) Neyman-
orthogonal score function and (2) cross-ﬁtting algorithm. The Neyman-orthogonal score ensures
that the moment condition is locally insensitive to the estimates of nuisance parameters. This
implies that the moment condition is less sensitive to regularization mistakes made by the machine
learning estimators. The use of the cross-ﬁtting algorithm avoids imposing strong restrictions on
the growth of entropy and model complexity. I show that the new estimator is√
N-consistent and
asyptotically normal under suitable regularity conditions.
IdemonstratetheimplementationoftheestimatorbyMonte-Carlosimulationsandanempirical
application to estimate the quantile treatment eﬀect of 401(k) eligibility on total ﬁnancial assets.
Simulation results show that the suggested estimator outperforms both the estimator that naively
plugs in machine learning estimates for nuisance parameters and the estimator that uses high-
dimensional controls without model selection. Speciﬁcally, the conﬁdence intervals based on the
developed estimator yield coverage probabilities close to the nominal level across diﬀerent classes of
data generating processes, which illustrates the uniform property. In the empirical application, the
developed estimator produces consistent results when using sets of controls with diﬀerent dimen-
sions. This adds credibility to previous research that has used intuitively chosen low-dimensional
control variables to control for confounding eﬀects. Overall, the proposed estimator provides a
powerful tool for analyzing data with censored outcomes and high-dimensional controls.
1.1 Literature review
There is a large body of literature on censored quantile regression estimators with ﬁxed censoring.
The idea of censored quantile regression was ﬁrst introduced by Powell (1986), but its implemen-
tation faced computational diﬃculties due to a non-convex optimization problem. One approach
in the literature aimed to address this issue by developing the algorithm to implement Powell’s es-
timator; see Fitzenberger (1997), Fitzenberger and Winker (2007), and Koenker (2008). The other
2approach proposed alternative estimation methods based on the result that Powell’s estimator is
asymptotically equivalent to running a standard quantile regression on a subsample for which the
conditional quantile is not aﬀected by censoring. A crucial step in implementing this method is to
estimate the unknown censoring probabilities and select the subsample. To estimate the censoring
probability, researchers have used non-parametric estimation (Buchinsky and Hahn (1998), Khan
and Powell (2001)), a probit model (Chernozhukov and Hong (2002)), and a combination of both
(Tang et al. (2012)). Chernozhukov et al. (2015a) built upon Chernozhukov and Hong (2002) and
incorporated endogeneous regressors. Lastly, Chen (2018) leveraged the continuity of quantile re-
gression coeﬃcients to select the subsample by implementing sequential estimation over a grid of
quantiles. The paper uses the quantile regression coeﬃcients of the upper quantile to identify the
sub-sample of the lower quantile when the outcome is left-censored.
This work builds on recent advancements in econometrics models for high-dimensional data and
post-selection inference. As noted in the introduction, the literature addresses the inferential prob-
lem for low-dimensional target parameters in the presence of high-dimensional nuisance parameters.
Belloni et al. (2012) presented a linear instrumental variable model in which the nuisance function
was the optimal instrument estimated using Lasso. Belloni et al. (2014) introduced a partially
linear model where the nonlinear part of the model was treated as a nuisance function. Belloni
et al. (2017) developed an estimator for a series of treatment eﬀects, including average treatment
eﬀect and quantile treatment eﬀect, in the presence of many control variables. Chernozhukov et al.
(2015b), Chernozhukov et al. (2018), and Chernozhukov et al. (2020) provided a general framework
of constructing an estimator with a valid post-selection inference when using machine learning
estimators. There have also been works on the quantile regression with many controls, such as
Belloni and Chernozhukov (2011) and Belloni et al. (2019), as well as instrument variable quantile
regression by Chernozhukov et al. (2017b) and Chen et al. (2021).
Lastly, I want to discuss existing high-dimensional censored quantile regression models with
random censoring. According to Koenker (2008), random censoring requires that censoring point is
independent of the outcome conditional on covariates. For example, in a study of the eﬀect of med-
ical treatment on the time until cancer recurrence, the outcome may be censored if a patient drops
out or has not experienced recurrence by the end of the study. The assumption of random censoring
is that the censoring point, such as dropout time, is independent of the outcome, given covariates
such as patient’s health conditions. The Zheng et al. (2018) developed a high-dimensional censored
quantile regression model using stochastic integral-based sequential estimation and penalization,
building on the works of Portnoy (2003) and Peng and Huang (2008). Fei et al. (2021) employed a
splitting and fusing scheme to conduct inferences for all variables in the model, not just the target
parameter. He et al. (2022) pointed out that the computational algorithms of Zheng et al. (2018)
and Fei et al. (2021) can be highly ineﬃcient when applied to high-dimensional data, and proposed
a smoothed estimation equation approach to the sequential estimation procedure.
This paper makes two main contributions compared to the previous literature. First, to the
best of my knowledge, this is the ﬁrst work to provide post-selection inference for censored quantile
3regression model with ﬁxed censoring. Aforementioned high-dimensional censored quantile regres-
sion estimators with random censoring can be applied to outcomes with ﬁxed censoring, but they
require more complicated estimation procedures that are not needed when the censoring is ﬁxed. I
argue that the proposed estimator is more practical in economic contexts as economists encounter
ﬁxed censoring more often. Additionally, focusing on post-selection inference for target parameters
is more appropriate for causal inference as it accounts for confounding eﬀects in contrast to naive
implementation of machine learning estimators. Secondly, I derive an orthogonal score function
for estimating quantile treatment eﬀects with censored outcomes, which has advantages even in
low-dimensional setting. In comparison to nonparametric estimation of the selection probability,
the estimator avoids the curse of dimensionality. In comparison to parametric estimation of the
selection probability, the orthogonal score function allows for model misspeciﬁcation of the selection
probability.
1.2 Plan of the paper
The rest of the paper is structured as follows. Section 2 reviews the censored quantile regression
estimator of Buchinsky and Hahn (1998) and highlights its limitations when using machine learning
estimators for estimating nuisance parameters. Section 3 introduces the DMLCQR estimator, its
estimation algorithm, and its asymptotic properties. Section 4 presents the results of a Monte Carlo
simulation, and Section 5 presents the results of an empirical application. Section 6 concludes the
paper, with the proofs of the main results provided in the Appendix.
Notation. I work with triangular array data {ωi,n;i= 1,...,n ;n= 1,2,3,...}where for each
n,{ωi,n;i= 1,...,n}is deﬁned on the probability space (Ω,S,Pn). Eachωi,n= (y/prime
i,n,d/prime
i,n,z/prime
i,n)/primeis a
vectorwhichareindependentacross ibutnotnecessarilyidenticallydistributed(i.n.i.d.). Therefore,
while all parameters that characterize the distribution of {ωi,n;i= 1,...,n}are implicitly indexed
byPnandn, I omit this dependence from the notation to maintain simplicity. I use the following
notation:En[f] =n−1/summationtextn
i=1f(ωi)and ¯En[f] =E[En[f]] =1
n/summationtextn
i=1E[f(ωi)]. Thel2-norm is
denoted by/bardbl·/bardbl; thel0-norm/bardbl·/bardbl0denotes the number of nonzero components of a vector; and the
l∞-norm/bardbl·/bardbl∞denotes the maximal absolute value in the components of a vector.
2 Censored quantile regression estimator
2.1 Censored quantile regression
For a quantile index τ∈(0,1), consider a partially linear censored quantile regression model
y∗
i=diθτ+gτ(zi) +/epsilon1i, Qτ(/epsilon1i|di,zi) = 0 (1)
wherey∗
iis the latent outcome variable, diis the target variable of interest (e.g. a policy or
treatment variable), and the variable zirepresents the counfounding factors which aﬀect the equa-
tion through an unknown function gτ. The observed outcome yiis censored from below such
4thatyi= max(y∗
i,ci). Note that the censoring point can be diﬀerent for each observation and it
could be unknown, as the estimator does not require the knowledge of the censoring point. The
p-dimensional control variables xi=X(zi)are used to approximate the function gτ(zi)which takes
the form
gτ(zi) =x/prime
iβτ+rτi
whererτiis an approximation error. The main parameter of interest θτis the quantile treatment
eﬀect on the latent outcome, while βτandrτare nuisance parameters.
I introduce the censored quantile regression estimator of Buchinsky and Hahn (1998). Let
wi= (di,zi),ti=I(y∗
i> ci)andπ(wi) =P[ti= 1|wi]. In (1),diθτ+gτ(zi)is theτth conditional
quantile of y∗
igivenwi. Therefore, the conditional probability that y∗
i< diθτ+gτ(zi)givenwi,
ti= 1, andπ(wi)>1−τis
P(y∗
i<diθτ+x/prime
iβτ+rτi|wi,ti= 1,π(wi)>1−τ) =π(wi)−(1−τ)
π(wi)≡hτ(wi).
Then, the population parameter (θτ,βτ)is given by
(θτ,βτ) = arg min
θ,βE/bracketleftbigtiI(hτi>0)ρhτi(yi−diθ−x/prime
iβ−rτi)/bracketrightbig= arg min
θ,βE[g(wi,θ,β,π )](2)
whereρτ(u) = (τ−I(u≤0))u. Sinceπ(wi)and accordingly hτ(wi)are unknown, Buchinsky
and Hahn (1998) use nonparametric estimates ˆπ(wi)andˆhτ(wi)from the ﬁrst step, so the sample
estimator is
(ˆθτ,ˆβτ) = arg min
(θ,β)1
NN/summationdisplay
i=1tiI(ˆhτi>0)ρˆhτi(yi−diθ−x/prime
iβ). (3)
Note that the estimator in (3) employs a weighted and rotated quantile regression.
2.2 High-dimensional setting
Consider a setting with high-dimensional xi, where the dimension pcould be either comparable
to the sample size Nor larger than the sample size (p/greatermuchN). The goal is to make inference
for the parameter of interest θτin the presence of high-dimensional nuisance parameters (π,βτ).
Conventional methods employed in the estimator, such as nonparametric estimation and quantile
regression, are not applicable in this setting ( p/greatermuchN), so machine learning methods with regular-
ization must be employed. However, when the machine learning estimates (ˆπ,ˆβτ)are used, the
estimator ˆθτis not necessarily√
N-consistent. This is because while using the estimator (ˆπ,ˆβτ)
to estimate θτcontributes with a bias of the order/parenleftBig
/bardblˆπ−π/bardbl,/vextenddouble/vextenddouble/vextenddoubleˆβτ−βτ/vextenddouble/vextenddouble/vextenddouble/parenrightBig
in principle, machine
learning estimators usually converge slower than√
N. Speciﬁcally, the score function of (2) for θτ
5has non-zero pathwise (Gateaux) derivatives with respect to the nuisance parameter (π,βτ):
∂πE[s(wi,θ,β,π )] [π−π0] =E/bracketleftbigg
tiI(hτi>0)1−τ
π2di(πi−πi0)/bracketrightbigg
/negationslash= 0
∂βE[s(wi,θ,β,π )] [β−β0] =E/bracketleftbigtiI(hτi>0)fidix/prime
i(β−β0)/bracketrightbig/negationslash= 0
where the pathwise derivative is deﬁned in Section 3 and s(wi,θ,β,π ) =∂g(wi,θ,β,π )
∂θ=tiI(hτi>
0)(hτi−I(yi−diθ−x/prime
iβ≤0))di. This implies that the ﬁrst-order bias of nuisance parameter
estimates would aﬀect the target parameter, which are regularization and overﬁtting bias from
using machine learning estimators. Therefore, additional measures are necessary for valid inference.
3 The DMLCQR estimator
3.1 The Neyman-orthogonal score
I refer to the proposed estimator as DMLCQR estimator. The Neyman orthogonality condition is
an important concept in understanding the estimator, so I introduce the deﬁnition in my context
following the deﬁnition in Chernozhukov et al. (2018). Let (θ0,η0)be the true value of the ﬁnite
dimensional parameter of interest θ∈Θ⊂Rdθand the inﬁnite-dimensional nuisance parameter
η∈T, whereTis a convex subset of some normed vector space. I assume that the moment
conditionsE[ψ(Wi,θ0,η0)] = 0hold. The pathwise (Gateaux) derivative map Dr:˜T →Rdθfor
˜T={η−η0|η∈T}is deﬁned as
Dr[η−η0]≡∂r{Ep[ψ(W,θ 0,η0+r(η−η0))]}, η∈T (4)
for allr∈[0,1). For convenience, denote
∂ηEp[ψ(W,θ 0,η0)][η−η0]≡Dr[η−η0], η∈T (5)
which is the pathwise derivative (4) at r= 0. Additionally, let TN⊂Tbe a nuisance realization set
such that estimators of η0take values in this set with high probability. The Neyman orthogonality
condition requires that the derivative in (5) vanishes for all η∈TN.
Deﬁnition 1. The score function ψ(Wi,θ,η)obeys the Neyman orthogonality condition at (θ0,η0)
with respect to the nuisance parameter realization set TN⊂TifEp[ψ(Wi,θ0,η0)] = 0and the
pathwise derivative map Dr[η−η0]exists for all r∈[0,1)andη∈TN, and vanishes at r= 0, that
is
∂ηEp[ψ(W,θ 0,η0)][η−η0] = 0∀η∈TN.
I construct a score function that satisﬁes the Neyman orthogonality condition in Deﬁnition 1.
Letfi=f/epsilon1i(0|wi)denote the conditional density at 0 of the error term /epsilon1iin (1). The construction
6of the orthogonal score function is based on the linear projection of dionxi, both weighted by√fi
on the subsample that satisﬁes ti= 1andhτi>0
/radicalbig
fidi=/radicalbig
fix/prime
iµτ+ui,¯E[tiI(hτi>0)/radicalbig
fixiui] = 0 (6)
whereµτ∈arg minµE/bracketleftbigtiI(hτi>0)fi(di−x/prime
iµ)2/bracketrightbig. The orthogonal score function i s
ψ(wi,θ,η) =tiI(hτi>0) (hτi−I(yi−diθτ−x/prime
iβτ−rτi≤0)) (di−x/prime
iµτ) + (ti−πi)I(hτi>0)(1−τ)
πi(di−x/prime
iµτ)
(7)
=I(hτi>0)/parenleftbigg
ti{hτi−I(yi−diθτ−x/prime
iβτ−rτi≤0)}+ (ti−πi)(1−τ)
πi/parenrightbigg
(di−x/prime
iµτ)
whereητ= (π,βτ,µτ)isthenuisanceparameter. Thisleadstoamomentcondition E[ψ(wi,θ,ητ0)] =
0to estimate θτand satisﬁes the orthogonality condition deﬁned above:
∂πE[ψ(wi,θ,η)] [π−π0] = 0
∂βτE[ψ(wi,θ,η)] [βτ−βτ0] = 0
∂µτE[ψ(wi,θ,η)] [µτ−µτ0] = 0.
Lemma 1. The new score function in (6) obeys the Neyman orthogonality condition.
The proof of the lemma can be found in the appendix.
3.2 Estimation algorithm
Deﬁne the Lasso estimator as
ˆθ∈arg min
θEn[M(yi,wi,θ)] +λ
n/bardblΓθ/bardbl1
whereλis a penalty level and Γis a diagonal matrix of penalty loadings. The Post-Lasso estimator
is then deﬁned as
˜θ∈arg min
θEn[M(yi,wi,θ)] : supp(θ)⊆˜T
where the set ˜Tcontains supp( ˆθ)and may also include additional variables considered as important.
I will set ˜T= supp( ˆθ)unless otherwise noted.
I describe three Lasso regressions to estimate nuisance parameters (ˆπ,ˆβτ,ˆµτ). The ﬁrst is Logit
Lasso regression
ˆα= arg min
αEn/bracketleftbig−tiln Λ(w/prime
iα)−(1−ti) ln[1−Λ(w/prime
iα)]/bracketrightbig+λ1
n/bardblΓ1α/bardbl1
where Λ(u) =exp(u)
1 + exp(u). Construct the estimator of πandhτbyˆπ(wi) = Λ(w/prime
iˆα)andˆhτi=
7ˆπi−(1−τ)
ˆπi. The second is Lasso (weighted and rotated) quantile regression
/parenleftBigˆθτ,ˆβτ/parenrightBig
= arg min
θ,βEn/bracketleftBig
tiI/parenleftBigˆhτi>0/parenrightBig
ρˆhτi/parenleftbigyi−diθ−x/prime
iβ/parenrightbig/bracketrightBig
+λ2
n/bardblΓ2(θ,β)/bardbl1 (8)
and the last is Lasso regression with estimated weights
ˆµτ= arg min
µEn/bracketleftBig
tiI(ˆhτi>0)ˆfi(di−µxi)2/bracketrightBig
+λ3
n/bardblΓ3µ/bardbl1.
The Post-Lasso estimates (˜π,˜βτ,˜µτ)could be equivalently used. In the comments below, I discuss
the recommended choices for for λ= (λ1,λ2,λ3)andΓ = (Γ 1,Γ2,Γ3)and the estimation of the
conditional density function fi. I combine the new score (7) with the cross-ﬁtting algorithm of
Chernozhukov et al. (2018) to propose DMLCQR estimator. The estimator of interest ˜θcan be
constructed either using step 3-1 or 3-2.
Algorithm
1. Take aK-fold random partition (Ik)K
k=1of observation indices [N] ={1,...,N}. For simplic-
ity, assumethatthesizeofeachfold Ikissamewith n=N/K. Foreachk∈[K] ={1,...,K},
deﬁne the auxiliary sample Ic
k= [N]\Ik.
2. For each k∈[K], construct an ML estimator ˆητk= ˆητ((Wi)i∈Ic
k)ofητ0using the auxiliary
sample.
(a) Compute ˜πkfrom Logit Post-Lasso regression of tondandxand calculate ˜hτki=
˜πki−(1−τ)
˜πki.
(b) Compute (ˆθτk,ˆβτk)from Lasso (weighted and rotated) quantile regression of yondand
x. Compute the Post-Lasso estimates (˜θτk,˜βτk).
(c) Estimate the conditional density ˆfk.
(d) Compute ˜µτkfrom the Post-Lasso estimator of/radicalBig
ˆfkdon/radicalBig
ˆfkxusing the subsample
whereti= 1andI(˜hτki>0). Then ˆητk=/parenleftBig
˜πk,˜βτk,˜µτk/parenrightBig
.
3-1. Construct the estimator ˇθτkas
ˇθτk∈arg min
θLn,k(θ) ={En,k[ψi(θ,ˆητk)]}2
En,k[ψi(θ,ˆητk)2]
where ˆψi(θ,ˆητk) =I/parenleftBig˜hτki>0/parenrightBig/parenleftbigg
ti/braceleftBig˜hτki−I(yi−diθ−x/prime
i˜βτk≤0)/bracerightBig
+ (ti−˜πki)(1−τ)
˜πki/parenrightbigg
(di−
x/prime
i˜µτk)andEn,kis the empirical expectation over the kth fold of the data, En,k[ψ(w)] =
8n−1/summationtext
i∈Ikψ(wi). Aggregate the estimators:
˜θ0=1
KK/summationdisplay
k=1ˇθ0,k.
3-2. Construct the estimator ˜θ0as
˜θ0∈arg min
θ1
KK/summationdisplay
k=1Ln,k(θ).
In Step 2-(b), the Post-Lasso estimator is used as there is a possibility that the Lasso estimator will
not select the variable of interest das a relevant control variable. For consistency, the suggested
algorithm uses Post-Lasso estimator for estimating other nuisance parameters (πk,µk), but they
can be estimated using the Lasso estimator. The estimator using Step 3-1 is referred to as DML1
and Step 3-2 as DML2. The diﬀerence between the two is the aggregation method among the
estimates in each subsample. Although the theoretical properties of both methods are the same,
Chernozhukov et al. (2018) note that DML2 might perform better because the pooled objective
function in Step 3-2 is more stable than the separate objective function in Step 3-1. For the
simulation and empirical application results beyond, I use DML2 estimator.
Comment 1. Penalty parameters for Lasso logit and least squares
I follow Belloni et al. (2017) for setting the penalty level and estimating the penalty loadings, i.e.,
λ1=c√nΦ−1/parenleftbigg
1−γ
2(p+ 1)n/parenrightbigg
, λ3=c√n2Φ−1/parenleftbigg
1−γ
2p/parenrightbigg
and penalty loading matrices (ˆΓ1,ˆΓ3)can be estimated using the iterative algorithm in the paper.
Comment 2. Penalty parameters for Lasso quantile regression
IadaptthepenaltyparametersinBelloniandChernozhukov(2011)forweightedandrotatedquatile
regression. The penalty parameter used is
λ2
n=c×/braceleftBig
(1−α)-quantile of/vextenddouble/vextenddouble/vextenddoubleΓ−1En/bracketleftBig
tiI/parenleftBigˆhτi>0/parenrightBig/parenleftBigˆhτi−I(Ui≤ˆhτi)/parenrightBig
xi/bracketrightBig/vextenddouble/vextenddouble/vextenddouble
∞/bracerightBig
whereU1,···,Unare i.i.d uniform (0,1)random variables and Γ2is a diagonal matrix with Γ2
2,11=
En[d2
i]andΓ2
2,jj=En[x2
ij].
Comment 3. Estimation of conditional density function
In Step 2-(c), an estimate of the conditional density function fiis required. I follow the estimation
method in Belloni et al. (2019), in which they use the observation that1
fi=∂Q(τ|di,zi)
∂τ, where
9Q(·|di,zi)denotes the conditional quantile function of the latent outcome y∗
i. Let ˆQ(τ|di,zi)denote
anestimateoftheconditional τ-quantilefunction Q(τ|di,zi)basedonLassoorPost-Lassoestimator
of (8) and let h=hn→0denote a bandwidth parameter. Then an estimator of fican be
constructed as
ˆfi=2h
ˆQ(τ+h|zi,di)−ˆQ(τ−h|zi,di).
3.3 Asymptotic properties
3.3.1 Regularity conditions
Ipresent suﬃcientregularityconditions forthevalidityofthemain estimationandinferenceresults.
The asymptotic results in the paper builds upon Belloni et al. (2019), so the regularity conditions
provided include those in the paper. Speciﬁcally, Condiion AS, M, D are as in Belloni et al. (2019)
and Condition N presents new regularity conditions not in the paper. Let c,C, andqbe ﬁxed
constants with c>0,C≥1, andq≥4, and letln↑∞,δn↓0, and ∆n↓0be sequences of positive
constants. I assume that the following condition holds for the data-generating process P=Pnfor
eachn.
Condition AS (1) Let{(yi,di,xi=X(zi))}be independent random variables satisfying (1)
and (6) with/bardblµ0τ/bardbl+/bardblβτ/bardbl+|θτ|≤C. (2) There exists s≥1and vectors βτandθτsuch that
x/prime
iθ0τ=x/prime
iθτ+rθτi,/bardblθτ/bardbl0≤s,¯E[r2
θτi]≤Cs/n,/bardblθ0τ−θτ/bardbl1≤s/radicalbig
log(pn)/n, and/bardblβτ/bardbl0≤s,¯E[r2
τi]≤
Cs/n. (3) The conditional distribution function of /epsilon1iis absolutely continuous with continuously
diﬀerentiable density f/epsilon1i|di,zi(·|di,zi)such that 0< f≤fi≤suptf/epsilon1i|di,zi(t|di,zi)≤¯f≤Cand
suptf/prime
/epsilon1i|di,zi(t|di,zi)≤¯f/prime≤C.
Condition M (1) We have ¯E[{(di,x/prime
i)ξ}2]≥c/bardblξ/bardbl2and ¯E[{(di,x/prime
i)ξ}4]≤C/bardblξ/bardbl4for allξ∈Rp+1,
c≤min 1≤j≤p¯E/bracketleftBig
|fixijvi−E[fixijvi]|2/bracketrightBig1/2≤max 1≤j≤p¯E/bracketleftBig
|fixijvi|3/bracketrightBig1/3≤C. (2) The approxima-
tion error satisﬁes/vextendsingle/vextendsingle/vextendsingle¯E[fivirτi]/vextendsingle/vextendsingle/vextendsingle≤δnn−1/2and ¯E/bracketleftBig
(x/prime
iξ)2r2
τi/bracketrightBig
≤C/bardblξ/bardbl2¯E/bracketleftbigr2
τi/bracketrightbigfor allξ∈Rp. (3)
Suppose that Kq=E/bracketleftbigmax 1≤i≤n/bardbl(di,vi,x/prime
i)/prime/bardblq
∞/bracketrightbig1/qis ﬁnite and satisﬁes (K2
qs2+s3) log3(pn)≤nδn
andK4
qslog(pn) log3n≤δnn.
Condition D (1) Foru∈U, assume that Qu(y∗
i|zi,di) =diαu+x/prime
iβu+rui,fui=fyi|di,zi(diαu+
x/prime
iβu+rui|zi,di)≥cwhere ¯E[r2
ui]≤δnn−1/2and|rui≤δnh|for alliand the vector βusatisﬁes
/bardblβu/bardbl0≤s. (2) For ˜sθτ=s+nslog(n∨p)
h2λ2+/parenleftBig
nh¯k
λ/parenrightBig2
, supposeh¯k/radicalbig
˜sθτlog(pn)≤δn,h−2K2
qslog(pn)≤
δnn,λK2
q√s≤δnn,h−2s˜sθτlog(pn)≤δnn,λ/radicalbig
s˜sθτlog(pn)≤δnn, andK2
q˜sθτlog2(pn) log3(n)≤
δnn.
Condition N (1)πis bounded away from zero, that is π > c. (2) Letιi=πi−(1−τ).
The conditional distribution function of ιiis absolutely continuous with continuously diﬀerentiable
densityfιi|di,zi(·|di,zi)such thatfιi≤¯fι≤C.
103.3.2 Main results
The following result states that DMLCQR estimator converges to the true parameter at a√
N-rate
and is approximately normally distributed.
Theorem 1. Suppose that condiitons AS, M, D ,N hold. Then, the estimator satisﬁes
¯σ−1
N√
N(˜θ−θ)d→N(0,1)
where ¯σ2
N=/parenleftBig¯EN[tiI(hi>0)fidivi]/parenrightBig−1¯EN/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig/parenleftBig¯EN[tiI(hi>0)fidivi]/parenrightBig−1.
The following result establishes that the variance estimator is consistent.
Theorem 2. Suppose that condiitons AS, M, D ,N hold. The variance estimator ˆσ2
Nis consistent
where
ˆσ2
N=/parenleftBigg
1
KK/summationdisplay
k=1En,k[tiI(ˆhk,i>0)ˆfk,idiˆvk,i]/parenrightBigg−11
KK/summationdisplay
k=1En,k/bracketleftBig
ψ2(wi,˜θ,ˆηk)/bracketrightBig/parenleftBigg
1
KK/summationdisplay
k=1En,k[tiI(ˆhk,i>0)ˆfk,idiˆvk,i]/parenrightBigg−1
.
Theorem 1 and 2 can be used for construction of conﬁdence regions, which are uniformly valid
over a large class of data-generating processes. I formalize the uniform properties as below.
Corollary 1. LetPnbe the collection of all distributions of {(yi,di,z/prime
i)}n
i=1for which Conditions
AS, M, D, N are satisﬁed for given n≥1. Then the conﬁdence interval
CI=/parenleftbigg
˜θ±Φ−1(1−ξ/2)/radicalBig
ˆσ2
N/N/parenrightbigg
obeys
lim
N→∞sup
P∈Pn|P(θ0∈CI)−(1−ξ)|= 0.
4 Monte Carlo simulation
I conduct a simulation study to evaluate the ﬁnite sample performance of the proposed estimators
and conﬁdence regions. I focus on the case of τ= 0.5andτ= 0.75under the following data-
generating process:
y∗
i=diθτ+x/prime
i(cyνy) +εi (9)
di=x/prime
i(cdνd) +vi (10)
whereyi= max(y∗
i,c)andcis the 0.3th sample quantile of y∗
iin each replication sample. θτ= 1
is the parameter of interst, and xi= (1,zi)consists of an intercept and covariate zi∼N(0,Σ).
The regressors are correlated in that Σij=ρ|i−j|andρ= 0.5. The error terms will follow standard
normal distribution, that is εi∼N(0,1)andvi∼N(0,1).
11For the parameters, I consider the case where
νy= (1,1/2,1/3,1/4,1/5,0,0,0,0,0,1,1/2,1/3,1/4,1/5,0,0,..., 0)
νd= (1,1/2,1/3,1/4,1/5,1/6,1/7,1/8,1/9,1/10,0,0,..., 0).
Note that a subset of controls have non-zero coeﬃcients indicating that only a few controls are
relevant to the outcome. The coeﬃcients exhibit a declining pattern which makes l1-based model
selectors more likely to make model selection mistakes on variables with smaller coeﬃcients. The
coeﬃcients (cy,cd)are used to control R2in equation (9) and (10), which I refer to respectively
asR2
yandR2
d. Changing the values of (cy,cd)results in varying levels of signal strength, which
either makes it easier or harder for l1-based model selectors to detect the controls with nonzero
coeﬃcients. I peform 500 Monte Carlo repetitions for each design scenario.
In the ﬁrst exercise, I focus on the setting with R2
y=R2
d= 0.75by setting the appropriate
(cy,cd)values. I compare the performance of four diﬀerent estimators:
1. Naive post-selection estimator (Naive PS): Estimator of θτbased on post-Lasso estimator
of Buchinsky and Hahn (1998). Uses model selection but do not use sample splitting and
orthogonal moment.
2. CQR estimator (HDCQR): Estimator of θτbased on Buchinsky and Hahn (1998) using all p
variables.
3. DML-CQR estimator (DMLCQR): The proposed estimator. Uses model selection, sample
splitting, and orthogonal moment.
4. Oracle estimator (Oracle): Estimator of θτbased on Buchinsky and Hahn (1998) using only
relevant controls among pvariables. Assumes the knowledge of the true identity of controls.
In Table 1 and Figure 1, I provide results for (n,p) = (500,300)and employ 2-fold and 4-fold
cross-ﬁtting for the DMLCQR estimator. As predicted theoretically, the Naive PS and HDCQR
estimators show biases, while the DMLCQR and Oracle estimators exhibit low biases and their
distributions are centered at the true value. This implies that when incorporating high-dimensional
controls in censored quantile regression models, a naive implementation of model selection leads to
regularization and overﬁtting biases in the target parameter. Also, using the original estimator with
high-dimensional controls induces bias and results in a loss of power to learn about the targeted
eﬀect. Therefore, if we use either of these estimators to conduct hypothesis testing, our inferencial
results may be misleading. The simulation results are consistent across diﬀerent quantiles.
12Table 1: Summary of simulation results
Naive PS HDCQR DMLCQR
(2-fold)DMLCQR
(4-fold)Oracle
τ= 0.5RMSE 0.227 0.175 0.131 0.130 0.068
SD 0.094 0.160 0.130 0.130 0.067
Bias 0.206 -0.070 0.016 0.013 -0.010
MAE 0.207 0.140 0.100 0.095 0.053
τ= 0.75RMSE 0.192 0.175 0.106 0.93 0.073
SD 0.086 0.161 0.101 0.091 0.073
Bias 0.172 -0.069 0.032 0.020 -0.006
MAE 0.173 0.140 0.081 0.073 0.058
Note: The table presents root-mean-squared-error (RMSE), standard deviation (SD), mean bias (Bias),
and mean absolute error (MAE) from the simulation results. Estimators include naive post-selection
estimator (Naive PS), Buchinsky and Hahn (1998) with high-dimensional controls (HDCQR), the
proposed estimator (DMLCQR) using 2-fold and 4-fold cross-ﬁtting, and oracle estimator (Oracle) which
applies Buchinsky and Hahn (1998) only with relevant controls.
Figure 1: Histograms of simulation results
Naive PSDensity
0.0 0.5 1.0 1.5 2.00 1 2 3 4HDCQRDensity
0.0 0.5 1.0 1.5 2.00.0 0.5 1.0 1.5 2.0 2.5
DMLCQRDensity
0.0 0.5 1.0 1.5 2.00.00.51.01.52.02.53.0OracleDensity
0.0 0.5 1.0 1.5 2.00 2 4 6
13I now examine conﬁdence intervals of diﬀerent designs by varying
(R2
d,R2
y)∈{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}2,
which gives us 81 diﬀerent designs. Figure 2 shows the rejection frequencies of conﬁdence intervals
with a nominal signiﬁcance level of 5% for the Naive PS estimator and DMLCQR estimator with
2-fold cross-ﬁtting. Ideally, the rejection rate should be the nomial level of 5% regardless of the
underlying data generating process. The conﬁdence regions for the Naive PS estimator deviate
greatly from the ideal level, while those for the DMLCQR estimator are ﬂat with a rejection
rate close to the nominal level. Therefore, the simulation results demonstrate that the DMLCQR
estimator has satisfactory ﬁnite sample performance compared to conventional alternatives.
Figure 2: Rejection probabilities of conﬁdence regions with nominal coverage of 95%
Naive Post Selection
0.20.40.60.8
0.20.40.60.80.00.20.40.60.81.0
Rd2Ry2RP
DML CQR
0.20.40.60.8
0.20.40.60.80.00.20.40.60.81.0
Rd2Ry2RP
5 Empirical application
In this section, I apply the DML-CQR estimator to estimate the quantile treatment eﬀect of 401(k)
eligibility on total ﬁnancial assets. Since working for a ﬁrm that oﬀers a 401(k) plan is non-
random, Poterba et al. (1994) and Poterba et al. (1995) developed a strategy that takes 401(k)
eligibility as exogenous, after conditioning on control variables including income. They argued that
when 401(k) plans were ﬁrst introduced, people did not base their employment decisions on the
availability of 401(k) plans, but instead focused on factors like income and other job characteristics.
To validate the selection on observables assumption, a ﬂexible set of controls that are correlated
with both 401(k) eligibility and the outcome should be included. This led Belloni et al. (2017) and
14Chernozhukov et al. (2017a) to use high-dimensional controls and post-selection inference methods
to study the eﬀect of 401(k) eligibility on household’s net ﬁnancial assets. They revisited previous
empirical ﬁndings that assumed that the confounding eﬀects could be adequately controlled for by
a small number of ex ante chosen controls.
I conduct a similar study to Belloni et al. (2017) and Chernozhukov et al. (2017a), but using
total ﬁnancial asset as the outcome. In the data, 12.4 percent of households reported zero total
ﬁnancial assets, which is naturally bounded by zero, in contrast to the net ﬁnancial assets, which
can be negative. Although this is a corner solution outcome and not from bottom coding, we need
to use an econometric model with a censored outcome to accurately incorporate the data structure.
The treatment of interest is 401(k) eligibility, not participation, so the parameter of interest is the
intent-to-treat eﬀect. Poterba et al. (1995) conducted a median regression of 401(k) eligibility on
total ﬁnancial assets, but they did not consider that the outcome was censored and focused soley
on the median quantile.
I use the same data as Chernozhukov and Hansen (2004). The data set consists of 9915
household-level observations obtained from the 1991 SIPP. Two households were removed due to
reporting negative income, leaving a sample size of N= 9913. The outcome variable Yis total
ﬁnancial assets, which is deﬁned as the sum of IRA balances, 401(k) balances, and other ﬁnancial
assets. The treatment variable Dis a binary indicator of eligibility to enroll in a 401(k) plan. The
vector of raw covariates Xincludes income, age, family size, years of education, marriage status
indicator, a two-earner status indicator, a deﬁned beneﬁt pension status indicator, an IRA partici-
pation indicator, and home ownership indicator. Further information about the data can be found
in Chernozhukov and Hansen (2004).
I consider three diﬀerent sets of controls f(X), modifying the speciﬁcation outlined in Belloni
et al. (2017). The ﬁrst speciﬁcation includes indicators of marriage status, two-earner status,
deﬁned beneﬁt pension status, IRA participation, and home ownership status, as well as second-
order polynomials in family size and education, a third-order polynomial in age, and a quadratic
spline in income with six break points. The second speciﬁcation adds interactions between all
non-income variables and interactions between non-income variables and each term in the income
spline to the ﬁrst speciﬁcation. The third speciﬁcation augments interactions between interaction
of non-incomevariables and the income terms. These three speciﬁcations are referred to as the
low-p,high-p, andvery high-p speciﬁcations, respectively. The dimensions of controls for the ﬁrst,
second, and third speciﬁcations are 20, 182, and 710. Additionally, I use 20, 157, and 498 variables
for methods that do not use variable selection by removing perfectly collinear terms.
Figure 3 presents the quantile treatment eﬀect estimates at quantiles τ= 0.15,0.2,..., 0.9and
95% conﬁdence intervals. In the low-pspeciﬁcation, I apply conventional quantile regression and
censored quantile regression estimator of Buchinsky and Hahn (1998) to compare whether the esti-
mates change according to taking censored outcome into account in the model. The results indicate
that using censored quantile regression model increases the QTE at lower quantiles closer to the
censoring point. This makes sense as conditional quantiles at lower quantiles are more likely to
15be aﬀected by censoring. Therefore, the result recommends the use of censored outcome models
rather than ignoring the latent data structure. In the high-pspeciﬁcation, I compare the results
using DMLCQR estimator and Buchinsky and Hahn (1998), where the latter could be understood
as HDCQR estimator in the simulation. It shows that incorporating high-dimensional controls in
Buchinsky and Hahn (1998) gives similar results to using quantile regression in the low-pspeciﬁca-
tion, while DMLCQR estimates are consistent with estimates in the low-pspeciﬁcation accounting
for censoring. This implies that implementing the conventional censored quantile regression estima-
tor with high-dimensional control could lead to somewhat erratic estimates, close to using quantile
regression with low-dimensional data. Lastly, the application of DMLCQR in the very high-p spec-
iﬁcation has similar results as in the high-pspeciﬁcation. As the results using DMLCQR with
high-pandvery high-p are very close to censored quantile regression estimates in the low-p,my
result complements previous works that were based on the assumption that a small number of ex
ante chosen controls are enough to control the confounding eﬀect.
Figure 3: QTE estimates of the eﬀect of 401(k) eligibility on total ﬁnancial assets
0.2 0.4 0.6 0.80.00 0.05 0.10 0.15 0.20 0.25Low−p
QuantileQuantile treatment effectQuantile regression
Buchinsky and Hahn (1998)
0.2 0.4 0.6 0.80.00 0.05 0.10 0.15 0.20 0.25High−p
QuantileQuantile treatment effectDMLCQR
Buchinsky and Hahn (1998)
0.2 0.4 0.6 0.80.00 0.05 0.10 0.15 0.20 0.25Very−high−p
QuantileQuantile treatment effectDMLCQR
6 Conclusion
Censored quantile regression has been popularly used in the causal inference literature. Some
examples of its applications include the eﬀect of tax incentives on charitable contributions (Fack
16and Landais (2010)), the eﬀect of pollution on productivity (Graﬀ Zivin and Neidell (2012)), and
the price elasticity of medical expenditure (Kowalski (2016)). Empirical researchers frequently face
the challenge of selecting important control variables from a rich set of potential controls, which
could be alleviated by employing machine learning methods with regularization to estimate high-
dimensional nuisance parameters. I propose the DMLCQR estimator, which improves upon the
censored quantile regression estimator of Buchinsky and Hahn (1998), and enables researchers to
use high-dimensional control variables while obtaining valid inference for the target parameter.
This additional beneﬁt makes censored quantile regression more practical for empirical researchers
to analyze wider types of data sets and employ a broader set of machine learning methods for
estimation.
17Appendix
Notation
Iworkwithtriangulararraydata {ωi,n;i= 1,...,n ;n= 1,2,3,...}whereforeach n,{ωi,n;i= 1,...,n}
is deﬁned on the probability space (Ω,S,Pn). Eachωi,n= (y/prime
i,n,d/prime
i,n,z/prime
i,n)/primeis a vector which
are independent across ibut not necessarily identically distributed (i.n.i.d.). Therefore, while
all parameters that characterize the distribution of {ωi,n;i= 1,...,n}are implicitly indexed by
Pnandn, I omit this dependence from the notation to maintain simplicity. I use the follow-
ing notation: En[f] =n−1/summationtextn
i=1f(ωi),¯En[f] =E[En[f]] =1
n/summationtextn
i=1E[f(ωi)], and Gn(f(ωi)) =
1√n/summationtextn
i=1(f(ωi)−E[f(ωi)]). Thel2-norm is denoted by /bardbl·/bardbl; thel0-norm/bardbl·/bardbl0denotes the number
of nonzero components of a vector; and the l∞-norm/bardbl·/bardbl∞denotes the maximal absolute value in
the components of a vector. For a sequence (ti)n
i=1, we denote/bardblti/bardbl2,n=/radicalBig
En/bracketleftbigt2
i/bracketrightbig. We use the
notationa.bto denotea≤cbfor some constant c>0that does not depend on n; anda.Pbto
denotea=Op(b).
A Proof of the theoretical results
A.1 Proof of Lemma 1
Moment condition: From (2), the moment conditions for θτandβτare
E/bracketleftbigg∂g(wi,θ,β,π )
∂θ/bracketrightbigg
=E/bracketleftbigtiI(hτi>0)(hτi−I(yi−diθτ−x/prime
iβτ≤0))di/bracketrightbig= 0 (11)
E/bracketleftbigg∂g(wi,θ,β,π )
∂β/bracketrightbigg
=E/bracketleftbigtiI(hτi>0)(hτi−I(yi−diθτ−x/prime
iβτ≤0))xi/bracketrightbig= 0.(12)
Therefore,
E[ψ(wi,θ0,η0)] =E/bracketleftbigg
I(hτi>0)/parenleftbigg
ti/braceleftbighτi−I(yi−diθτ−x/prime
iβτ−rτi≤0)/bracerightbig+ (ti−πi)(1−τ)
πi/parenrightbigg
(di−x/prime
iµτ)/bracketrightbigg
=E/bracketleftbigI(hτi>0)ti/braceleftbighτi−I(yi−diθτ−x/prime
iβτ−rτi≤0)/bracerightbigdi/bracketrightbig
−E/bracketleftbigI(hτi>0)ti/braceleftbighτi−I(yi−diθτ−x/prime
iβτ−rτi≤0)/bracerightbigx/prime
iµτ/bracketrightbig
+E/bracketleftbigg
(ti−πi)I(hτi>0)(1−τ)
πi(di−x/prime
iµτ)/bracketrightbigg
=E/bracketleftbigg
(ti−πi)I(hτi>0)(1−τ)
πi(di−x/prime
iµτ)/bracketrightbigg
(*(6),(7))
=E/bracketleftbigg
E[ti−πi|wi]I(hτi>0)(1−τ)
πi(di−x/prime
iµτ)/bracketrightbigg
(*Law of iterated expectations )
= 0
and the moment condition E[ψ(wi,θ0,η0)] = 0holds.
18Neyman orthogonality: The pathwise derivative of (4) in the direction η−η0= (π−π0,β−
βτ,µ−µτ)is
∂πE[ψ(wi,θ0,η0)] [π−π0]
=E/bracketleftbigg/braceleftbigg
tiI(hi>0)1−τ
π2
i(di−x/prime
iµτ)−tiI(hi>0)1−τ
π2
i(di−x/prime
iµτ)/bracerightbigg
(πi−π0i)/bracketrightbigg
= 0
∂βE[ψ(wi,θ0,η0)] [β−βτ0]
=∂rE/bracketleftbigg
I(hτi>0)/parenleftbigg
ti/braceleftbig
hτi−P(yi−diθτ−x/prime
iβτ−rτi≤x/prime
i(˜β−βτ)|wi)/bracerightbig
+ (ti−πi)(1−τ)
πi/parenrightbigg
(di−x/prime
iµτ)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle
r=0
=−E/bracketleftbig
I(hτi>0)tif(x/prime
i(˜β−βτ)|wi)(di−x/prime
iµτ)x/prime
i(β−βτ)/bracketrightbig/vextendsingle/vextendsingle/vextendsingle
r=0
=−E[tiI(hi>0)fi(di−x/prime
iµτ)x/prime
i(β−βτ)] = 0 ( *Deﬁnition of µτ)
= 0
∂µE[ψ(wi,θ0,η0)] [µ−µτ0]
=−E[tiI(hi>0) (hi−I{yi≤diθτ+x/prime
iβτ+rτi})x/prime
i(µ−µτ)]−E/bracketleftbigg
(ti−πi)I(hi>0)(1−τ)
πix/prime
i(µ−µτ)/bracketrightbigg
=−E[tiI(hi>0) (hi−Fy{diθτ+x/prime
iβτ+rτi|wi,ti= 1,πi>1−τ})x/prime
i˜µ]
−E/bracketleftbigg
E[ti−πi|wi]·I(hi>0)(1−τ)
πix/prime
i˜µ/bracketrightbigg
= 0.
A.2 Assumption IQR
I suppressτnotation in the parameters for the sake of simplicity.
A.2.1 Assumption
Fork∈[K], I assume that
Ln(ˇθk)≤min
θ∈ΘτLn(θ) +δnn−1.
I suppressknotation in the parameters. We deﬁne
Γ(˜θ,˜η) =¯E[ψθ,η(yi,di,xi)]|θ=˜θ,h=˜η
Γη(˜θ,˜η)[ˆη−η0] = lim
t→0Γ(˜θ,˜η+t[ˆη−η0])−Γ(˜θ,˜η)
t
where Γηis the directional derivative with respect to [ˆη−η0]at(˜θ,˜η).
We denote the nuisance parameters as η= (g,v,π )wherevi=di−x/prime
iµ. We deﬁne the score
with (θ,η)as
ψθ,η(wi) =I(hi>0)/parenleftbigg
ti{hi−I(yi−diθ−gi≤0)}+ (ti−πi)(1−τ)
πi/parenrightbigg
vi. (13)
For some sequences δn→0and∆n→0, we letFdenote a set of functions such that each element
19˜η= (˜g,˜v,˜π)∈Fsatisﬁes
¯E/bracketleftBig
(˜gi−gi)2/bracketrightBig
.δnn−1/2(14)
/vextendsingle/vextendsingle/vextendsingle¯E[fivi{˜gi−gi}]/vextendsingle/vextendsingle/vextendsingle≤δnn−1/2(15)
¯E[(˜vi−vi)2]≤δn (16)
¯E[|˜gi−gi||˜vi−vi|]≤δnn−1/2(17)
¯E/bracketleftBig
|vi|(˜gi−gi)2/bracketrightBig
≤δnn−1/2(18)
¯E/bracketleftBig
|˜vi−vi|(˜gi−gi)2/bracketrightBig
≤δnn−1/2(19)
and with probability 1−∆nwe have
sup
|θ−θ0|≤δ2n,˜η∈F/vextendsingle/vextendsingle/vextendsingle(En−¯E)[ψθ,˜η−ψθ,η0]/vextendsingle/vextendsingle/vextendsingle≤δnn−1/2. (20)
We assume the estimated functions ˆη= (ˆg,ˆv,ˆπ)satisfy the following condition.
Condition IQR. Let{(yi,di,zi)}be random variables independent across isatisfying (1) and
(5). Suppose there are positive constants 0<c≤C <∞such that:
(1)fyi|di,zi(y|di,zi)≤¯f,f/prime
yi|di,zi(y|di,zi)≤¯f/prime;¯E[v4
i] +¯E[d4
i]≤C;c≤|¯E[tiI(hi>0)fidivi]|
(2)/braceleftBig
θ:|θ−θ0|≤n−1/2/δn/bracerightBig
⊂Θτwhere Θτis a (possibly random) compact interval.
(3) with probability at least 1−∆nthe estimated functions ˆη= (ˆg,ˆv,ˆπ)∈Fand
|ˇθ−θ0|≤δnandEn[ψˇθ,ˆη(yi,di,zi)]≤δnn−1/2.
(4) with probability at least 1−∆nthe estimated functions ˆη= (ˆg,ˆv,ˆπ)satisfy
/bardblˆvi−vi/bardbl2,n≤δn (21)
/bardblˆπi−πi/bardbl2,n≤δn (22)
/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/vextenddouble/vextenddouble/vextenddouble
2,n≤δn (23)
/vextenddouble/vextenddouble/vextenddoubleI/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsingledi(θ−ˇθ) +gi−ˆgi/vextendsingle/vextendsingle/vextendsingle/parenrightBig/vextenddouble/vextenddouble/vextenddouble
2,n≤δ2
n. (24)
A.2.2 Proof
Condition IQR (1) requires conditions on the probability density function that are assumed
in Condition AS (3). ¯E[v4
i] +¯E[d4
i]≤Cis implied by Condition M (1) using ξ1= (1,0/prime)/prime
andξ2= (1,−µ/prime
τ)/primesince ¯E[d4
i] = ¯E[{(di,x/prime
i)ξ1}4]≤Cand ¯E[v4
i]≤¯E[{(di,x/prime
i)ξ2}4]≤C(1 +
/bardblµτ/bardbl)4and/bardblµτ/bardbl≤Cas assumed in Condition AS (1). c≤|¯E[tiI(hi>0)fidivi]|is implied by
¯E[tiI(hi>0)fidivi] =¯E[tiI(hi>0)fiv2
i]≥f.
ConditionIQR(3) : Iﬁrstverify ˆη= (ˆg,ˆv,ˆπ)∈F. Bellonietal.(2019)show ¯E/bracketleftbigg/braceleftBig
x/prime
i(˜β−β)/bracerightBig2/bracketrightbigg
.P
20/radicalBigg
slog(pn)
nand ¯E[{x/prime
i(˜µ−µ)}2].1
h2slog(pn)
n+h2¯k+λ2
n2swherehis the bandwidth for estimating
ˆfi. I verify equation (9) to (14) under conditions.
¯E/bracketleftBig
(˜gi−gi)2/bracketrightBig
≤¯E/bracketleftbigg/braceleftBig
x/prime
i(˜β−β)/bracerightBig2/bracketrightbigg
+¯E/bracketleftBig
r2
i/bracketrightBig
.slog(pn)
n.δnn−1/2(*Condition AS (2) )
/vextendsingle/vextendsingle/vextendsingle¯E[tiI(hi>0)fivi(˜gi−gi)]/vextendsingle/vextendsingle/vextendsingle≤/vextendsingle/vextendsingle/vextendsingle¯E[tiI(hi>0)fivix/prime
i(˜β−β)]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle¯E[tiI(hi>0)fiviri]/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle¯E[tiI(hi>0)/radicalbig
fiuix/prime
i(˜β−β)]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle¯E[tiI(hi>0)fiviri]/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle¯E[tiI(hi>0)fiviri]/vextendsingle/vextendsingle/vextendsingle(*¯E[tiI(hτi>0)/radicalbig
fixiui] = 0)
.δnn−1/2(*Condition M (2) )
¯E[(˜vi−vi)2] =¯E[/braceleftbigx/prime
i(˜µ−µ)/bracerightbig2].1
h2slog(pn)
n+h2¯k+λ2
n2s.δn
underh−2slog(pn)≤δnn,h≤δn, andλ2s≤δnn2.
¯E[|˜gi−gi||˜vi−vi|] =¯E[|˜gi−gi|/vextendsingle/vextendsinglex/prime
i(˜µ−µ)/vextendsingle/vextendsingle]
≤¯E[(˜gi−gi)2]1/2¯E[/braceleftbigx/prime
i(˜µ−µ)/bracerightbig2]1/2
.{slog(pn)/n}1/2

1
h/radicalBigg
slog(pn)
n+h¯k+λ√s
n

.δnn−1/2
underh−2s2log2(pn)≤δ2
nnandh¯k/radicalbig
slog(pn)≤δn.
¯E/bracketleftBig
|vi|(˜gi−gi)2/bracketrightBig
=¯E/bracketleftbigg
|vi|/braceleftBig
x/prime
i(˜β−β)/bracerightBig2/bracketrightbigg
+¯E/bracketleftBig
|vi|r2
i/bracketrightBig
≤/parenleftbigg
¯E/bracketleftbigg
v2
i/braceleftBig
x/prime
i(˜β−β)/bracerightBig2/bracketrightbigg
¯E/bracketleftbigg/braceleftBig
x/prime
i(˜β−β)/bracerightBig2/bracketrightbigg/parenrightbigg1/2
+/parenleftBig¯E/bracketleftBig
v2
ir2
i/bracketrightBig¯E/bracketleftBig
r2
i/bracketrightBig/parenrightBig1/2
.{slog(pn)/n}1/2/braceleftbigg
¯E/bracketleftbigg
v2
i/braceleftBig
x/prime
i(˜β−β)/bracerightBig2/bracketrightbigg
+¯E/bracketleftBig
v2
ir2
i/bracketrightBig/bracerightbigg1/2
.slog(pn)/n.δnn−1/2
since/bardbl(1,−µ)/bardbl≤C,¯E[{(di,x/prime
i)ξ}4]≤C/bardblξ/bardbl4, and ¯E/bracketleftBig
(x/prime
iξ)2r2
τi/bracketrightBig
≤C/bardblξ/bardbl2¯E/bracketleftbigr2
τi/bracketrightbig. Lastly,
21¯E/bracketleftBig
|˜vi−vi|(˜gi−gi)2/bracketrightBig
=¯E/bracketleftBig/vextendsingle/vextendsinglex/prime
i(˜µ−µ)/vextendsingle/vextendsingle(˜gi−gi)2/bracketrightBig
=¯E/bracketleftBig/vextendsingle/vextendsinglex/prime
i(˜µ−µ)/vextendsingle/vextendsingle(˜gi−gi)2/bracketrightBig
.slog(pn)/n.δnn−1/2.
Next,Iverifyequation(20). Let F1,η=/braceleftbigψθ,η:|θ−θ0|≤δ2
n/bracerightbigandF2=/braceleftbigψθ,˜η−ψθ,η0:|θ−θ0|≤δ2
n/bracerightbig.
Note thatF2⊂F 1,˜η−F 1,η. SinceF1,ηis the produce of a VC class of dimension 1 with the ran-
dom variable v, its entropy nuber satisﬁes logN/parenleftBig
/epsilon1/bardblF1,η/bardblQ,2,F1,η,/bardbl·/bardblQ,2/parenrightBig
≤vlog(a//epsilon1)whereF1,η
is a measurable envelope for F1,ηthat satisﬁes/bardblF1,η/bardblP,q≤c1. Therefore,/bardblF2/bardblP,q=/bardblF1,η/bardblP,q+
/bardblF1,˜η/bardblP,q≤2c1by the triangle inequality and logN/parenleftBig
/epsilon1/bardblF2/bardblQ,2,F2,/bardbl·/bardblQ,2/parenrightBig
≤2vlog(2a//epsilon1)by the
proof of Theorem 3 in Andrews (1994). To bound supf∈F2|Gn(f)|, I apply Theorem 5.1 of Cher-
nozhukov et al. (2014) conditional on (wi)i∈Icso that ˜ηcan be treated as ﬁxed. By the proof of
Theorem 2 in Section A.5, note that for f2∈F 2,
/bardblf2/bardbl2
P,2
=¯E/bracketleftBig
{ψ(wi,θ,˜η)−ψ(wi,θ,η 0)}2/bracketrightBig
.¯E/bracketleftBig
(ˆvi−vi)2/bracketrightBig
+¯E/bracketleftBig
(ˆπi−πi)2/bracketrightBig
+¯E/bracketleftbigg/parenleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/parenrightBig2/bracketrightbigg
+¯E/bracketleftbigg
v2
iI/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsingledi(θ−˜θ) +gi−˜gi/vextendsingle/vextendsingle/vextendsingle/parenrightBig2/bracketrightbigg
.1
h2slog(pn)
n+h2¯k+λ2
n2s.
Applying the theorem with with σ=C

1
h/radicalBigg
slog(pn)
n+h¯k+λ
n√s

,a=pn,v=s,/bardblM/bardblP,2≤C
yields
sup
|θ−θ0|≤δ2n,˜η∈F/vextendsingle/vextendsingle/vextendsingle(En−¯E)[ψθ,˜η−ψθ,η0]/vextendsingle/vextendsingle/vextendsingle.1√n


1
h/radicalBigg
slog(pn)
n+h¯k+λ
n√s
/radicalBig
slog(pn) +1√nsClog(pn)


.δnn−1/2.
Condition IQR (4) : I verify equations (16) to (19).
/bardblˆvi−vi/bardbl2,n.P¯E/bracketleftBig
(ˆvi−vi)2/bracketrightBig1/2.δ1/2
n
/bardblˆπi−πi/bardbl2,n.P¯E/bracketleftBig
(ˆπi−πi)2/bracketrightBig1/2./radicalBigg
sln(pn)
n=δn
22/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/vextenddouble/vextenddouble/vextenddouble
2,n=/bardblI(ˆπi>1−τ)−I(πi>1−τ)/bardbl2,n
=/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1{I(ˆπi>1−τ)−I(πi>1−τ)}2
Note that
|I(ˆπi>1−τ)−I(πi>1−τ)|=|I(πi−εi>1−τ)−I(πi>1−τ)|(*εi= ˆπi−πi)
=|I(ιi>εi)−I(ιi>0)|(*ιi=πi−(1−τ))
≤I(|ιi|<|εi|).
Therefore,
/bardblI(ˆπi>1−τ)−I(πi>1−τ)/bardbl2
2,n≤1
nn/summationdisplay
i=1I(|ιi|<|ˆπi−πi|)
.P¯E[I(|ιi|<|ˆπi−πi|)]
≤¯fι1
nn/summationdisplay
i=1|ˆπi−πi|./radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1(ˆπi−πi)2.P/radicalBigg
sln(pn)
n=δn.
/vextenddouble/vextenddouble/vextenddoubleI/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsingledi(θ−ˇθ) +gi−˜gi/vextendsingle/vextendsingle/vextendsingle/parenrightBig/vextenddouble/vextenddouble/vextenddouble2
2,n=En/bracketleftBig
I/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsingledi(θ−˜θ)/vextendsingle/vextendsingle/vextendsingle/parenrightBig/bracketrightBig
+En/bracketleftBig
I/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsinglex/prime
i(˜β−β)/vextendsingle/vextendsingle/vextendsingle/parenrightBig/bracketrightBig
+En[I(|/epsilon1i|≤|ri|)]
.P¯E/bracketleftBig
I/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsingledi(θ−˜θ)/vextendsingle/vextendsingle/vextendsingle/parenrightBig/bracketrightBig
+¯E/bracketleftBig
I/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsinglex/prime
i(˜β−β)/vextendsingle/vextendsingle/vextendsingle/parenrightBig/bracketrightBig
+¯E[|/epsilon1i|≤|ri|]
≤¯f/braceleftBigg
1
nn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingledi(θ−˜θ)/vextendsingle/vextendsingle/vextendsingle+1
nn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsinglex/prime
i(˜β−β)/vextendsingle/vextendsingle/vextendsingle+1
nn/summationdisplay
i=1|ri|/bracerightBigg
≤¯f

/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingledi(θ−˜θ)/vextendsingle/vextendsingle/vextendsingle2+/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsinglex/prime
i(˜β−β)/vextendsingle/vextendsingle/vextendsingle2+/radicaltp/radicalvertex/radicalvertex/radicalbt1
nn/summationdisplay
i=1|ri|2


.P¯f/braceleftBigg/vextendsingle/vextendsingle/vextendsingleθ−˜θ/vextendsingle/vextendsingle/vextendsingle/radicalBig
¯E[d2
i] +/radicalBigg
¯E/bracketleftbigg/vextendsingle/vextendsingle/vextendsinglex/prime
i(˜β−β)/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
+/radicalbigg
¯E/bracketleftBig
|ri|2/bracketrightBig/bracerightBigg
.¯f/radicalBigg
sln(pn)
n=δn
A.3 Proof of Theorem 1 (DML 1 case)
A.3.1 Lemma
Lemma 2. Under the condition IQR, we have for all k∈[K],
¯σ−1
n√n(ˇθk−θ) =¯En/bracketleftBig
ψ2(wi,θ0,η0)/bracketrightBig−1/21√n/summationdisplay
i∈Ikψ(wi,θ0,η0)d→N(0,1)
23where ¯σ2
n=/parenleftBig¯En[tiI(hi>0)fidivi]/parenrightBig−1¯En/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig/parenleftBig¯En[tiI(hi>0)fidivi]/parenrightBig−1.
Proof.In the proof, I use (ˇθ,ˆh)instead of (ˇθk,ˆhk).
Step 1. Normality result
We have the following identity
En[ψˇθ,ˆη] =En[ψθ0,η0] +En[ψˇθ,ˆη−ψθ0,η0]
=En[ψθ0,η0] +¯E[ψˇθ,ˆη]
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(1)+n−1/2Gn(ψˇθ,ˆη−ψˇθ,η0)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(2)+n−1/2Gn(ψˇθ,η0−ψθ0,η0)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(3).
By the second relation in IQR (3), the left hand side satisﬁes/vextendsingle/vextendsingle/vextendsingleEn[ψˇθ,ˆη]/vextendsingle/vextendsingle/vextendsingle.δnn−1/2with probability
at least 1−∆n. By step 2, (1) =−¯E[tiI(hi>0)fidivi] (ˆθ−θ0) +O(δnn−1/2+δn/vextendsingle/vextendsingle/vextendsingleˆθ−θ0/vextendsingle/vextendsingle/vextendsingle). Since
ˆη= (ˆg,ˆv,ˆπ)∈Fwith probability at least 1−∆nby IQR (3), equation (15) holds. By condition
IQR (3) we have with probability at least 1−∆nthat|ˇθ−θ0|≤δn. Therefore, we can apply
equation (15) and we have |(2)|.δnn−1/2. By condition IQR (3) we have with probability at least
1−∆nthat|ˇθ−θ0|≤δn. Observe that
(ψθ,η0−ψθ0,η0)(yi,di,zi) =tiI(hi>0) (I(yi−diθ0−gi≤0)−I(yi−diθ−gi≤0))vi
=tiI(hi>0) (I(/epsilon1i≤0)−I(/epsilon1i≤di(θ−θ0)))vi
so that|(ψθ,η0−ψθ0,η0)(yi,di,zi)|≤I{|/epsilon1i|≤δn|di|}|vi|whenever|θ−θ0|≤δn. Since the class of
functions
{(y,d,z )/mapsto→(ψθ,η0−ψθ0,η0)(y,d,z ) :|θ−θ0|≤δn}is a VC subgraph class, we have
sup
|θ−θ0|≤δn|Gn(ψθ,η0−ψθ0,η0)|./parenleftBig¯E/bracketleftBig
I{|/epsilon1i|≤δn|di|}v2
i/bracketrightBig/parenrightBig1/2.δ1/2
n.
This implies that |(3)|.δ1/2
nn−1/2with probability 1−o(1).
Combining these relations we have
¯E[tiI(hi>0)fidivi] (ˆθ−θ0) =En[ψθ0,h0] +Op(δnn−1/2) +Op(δn)/vextendsingle/vextendsingle/vextendsingleˆθ−θ0/vextendsingle/vextendsingle/vextendsingle
√n(ˆθ−θ0)/braceleftBig¯E[tiI(hi>0)fidivi]−Op(δn)/bracerightBig
=√nEn[ψθ0,h0] +Op(δn).
Note that by the Lyapunov CLT
√nEn[ψθ0,h0]
¯E[ψ2
θ0,h0]1/2d→N(0,1)
24¯σ−1
n√n(θ−θ0) =√nEn[ψθ0,h0]
¯E[ψ2
θ0,h0]1/2d→N(0,1)
where ¯σ2
n=¯E[tiI(hi>0)fidivi]−1¯E[ψ2
θ0,h0]¯E[tiI(hi>0)fidivi]−1by¯E[tiI(hi>0)fidivi]≥cin
IQR (1).
Step 2.(1) =−¯E[tiI(hi>0)fidivi] (θ−θ0) +O(δn|θ−θ0|C+δnn−1/2)
Step 2-1. Bounding Γ(θ,ˆη)for|θ−θ0|≤δn
For any (ﬁxed function) ˆη∈F, we have
¯E[ψθ,ˆη] =¯E[ψθ,η0] +¯E[ψθ,ˆη]−¯E[ψθ,η0]
=¯E[ψθ,η0] +/braceleftBig¯E[ψθ,ˆη]−¯E[ψθ,η0]−Γη(θ,η0)(ˆη−η0)/bracerightBig
+ Γη(θ,η0)(ˆη−η0)
=¯E[ψθ,η0]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(a)+/braceleftBig¯E[ψθ,ˆη]−¯E[ψθ,η0]−Γη(θ,η0)(ˆη−η0)/bracerightBig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(b)+ Γη(θ0,η0)[ˆη−η0]/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(c)
+{Γη(θ,η0)[ˆη−η0]−Γη(θ0,η0)[ˆη−η0]}/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(d).
(a): By Taylor expansion, there is some ˜θ∈[θ0,θ]such that
¯E[ψθ,η0] = Γ(θ0,η0) + Γθ(˜θ,η0)(θ−θ0) ={Γθ(θ0,η0) +ηn}(θ−θ0)
where|ηn|≤|θ−θ0|C.δnCby step 2-2.
(b) = Γηη(θ,˜η)[ˆη−η0,ˆη−η0].δnn−1/2by step 2-4.
(c).δnn−1/2and(d).|θ−θ0|δnby step 2-3.
Combining the arguments, we have
¯E[ψθ,ˆη] ={Γθ(θ0,η0) +O(δnC)}(θ−θ0) +O(δnn−1/2) +O(δnn−1/2) +O(|θ−θ0|δn)
= Γθ(θ0,h0)(θ−θ0) +O(δn|θ−θ0|C+δnn−1/2)
=−¯E[tiI(hi>0)fidivi] (θ−θ0) +O(δn|θ−θ0|C+δnn−1/2).
Step 2-2. Relations for Γθin(a)
Γθ(θ,˜η) =−¯E/bracketleftBig
tiI/parenleftBig˜hi>0/parenrightBig
f/epsilon1i|di,zi(di(θ−θ0) + ˜gi−gi)di˜vi/bracketrightBig
Γθ(θ0,η0) =−¯E[tiI(hi>0)fidivi].
25Moreover, Γθalso satisﬁes
|Γθ(θ,η0)−Γθ(θ0,η0)|=/vextendsingle/vextendsingle/vextendsingle¯E/bracketleftBig
tiI(hi>0)f/epsilon1i|di,zi(di(θ−θ0))divi/bracketrightBig
−¯E[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsingle¯E/bracketleftBig
tiI(hi>0)/braceleftBig
f/epsilon1i|di,zi(di(θ−θ0))−fi/bracerightBig
divi/bracketrightBig/vextendsingle/vextendsingle/vextendsingle
≤|θ−θ0|¯f/prime¯E/bracketleftBig
d2
ivi/bracketrightBig
≤|θ−θ0|¯f/prime/braceleftBig¯E/bracketleftBig
d4
i/bracketrightBig¯E/bracketleftBig
v2
i/bracketrightBig/bracerightBig1/2≤C|θ−θ0|
since ¯E/bracketleftbigd4
i/bracketrightbig∨¯E/bracketleftbigv2
i/bracketrightbig≤Cand ¯f/prime≤Cfrom IQR (1).
Step 2-3. Relations for Γηin(c),(d)
Γη(θ,˜η)[ˆη−η0] =−¯E[tiI/parenleftBig˜hi>0/parenrightBig
f/epsilon1i|di,zi(di(θ−θ0) + ˜gi−gi) ˜vi(ˆgi−gi)]
+¯E/bracketleftbigg/braceleftbigg
tiI/parenleftBig˜hi>0/parenrightBig/parenleftBig˜hi−I(yi−diθ−˜gi≤0)/parenrightBig
+ (ti−˜πi)I/parenleftBig˜hi>0/parenrightBig(1−τ)
˜πi/bracerightbigg
(ˆvi−vi)/bracketrightbigg
Note that when Γηis evaluated at (θ0,η0), we have with probability 1−∆n,
|Γη(θ0,η0)[ˆη−η0]|=/vextendsingle/vextendsingle/vextendsingle−¯E[tiI(hi>0)fivi(ˆgi−gi)]/vextendsingle/vextendsingle/vextendsingle≤δnn−1/2
by equation (10).
The expression for Γηalso leads to the following bound
|Γη(θ,h0)[ˆη−η0]−Γη(θ0,η0)[ˆη−η0]|
≤/vextendsingle/vextendsingle/vextendsingle¯E[tiI(hi>0)/braceleftBig
fi−f/epsilon1i|di,zi(di(θ−θ0))/bracerightBig
vi(ˆgi−gi)]/vextendsingle/vextendsingle/vextendsingle+
+/vextendsingle/vextendsingle/vextendsingle¯E[{tiI(hi>0) (Fi(0)−Fi(di(θ−θ0)))}(ˆvi−vi)]/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingle¯E[/braceleftBig
fi−f/epsilon1i|di,zi(di(θ−θ0))/bracerightBig
vi(ˆgi−gi)]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingle¯E[(Fi(0)−Fi(di(θ−θ0))) (ˆvi−vi)]/vextendsingle/vextendsingle/vextendsingle
≤¯E/bracketleftBig
|θ−θ0|¯f/prime|divi||ˆgi−gi|/bracketrightBig
+¯E/bracketleftBig¯f|di(θ−θ0)||ˆvi−vi|/bracketrightBig
≤¯f/prime|θ−θ0|/braceleftBig¯E/bracketleftBig
d2
iv2
i/bracketrightBig¯E/bracketleftBig
|ˆgi−gi|2/bracketrightBig/bracerightBig1/2+¯f|θ−θ0|/braceleftBig¯E/bracketleftBig
d2
i/bracketrightBig¯E/bracketleftBig
(ˆv−vi)2/bracketrightBig/bracerightBig1/2
.P|θ−θ0|δn
by equation (9), (11), and ¯E/bracketleftbigd2
iv2
i/bracketrightbig,¯E/bracketleftbigd2
i/bracketrightbigbeing bounded.
Step 2-4. Second derivative (d)
Provided ˆη∈F,
Γηη(θ,˜η)[ˆη−η0,ˆη−η0] =−¯E/bracketleftBig
tiI/parenleftBig˜hi>0/parenrightBig
f/prime
/epsilon1i|di,zi(di(θ−θ0) + ˜gi−gi) ˜vi(ˆgi−gi)2/bracketrightBig
−2¯E/bracketleftBig
tiI/parenleftBig˜hi>0/parenrightBig
f/epsilon1i|di,zi(di(θ−θ0) + ˜gi−gi) (ˆgi−gi)(ˆvi−vi)/bracketrightBig
26/vextendsingle/vextendsingle/vextendsingle∂η∂η/prime¯E[ψ(θ0,˜η)][ˆη−η0,ˆη−η0]/vextendsingle/vextendsingle/vextendsingle≤¯f/prime¯E/bracketleftBig
|˜vi|(ˆgi−gi)2/bracketrightBig
+ 2¯f¯E[|ˆvi−vi|·|ˆgi−gi|]
≤¯f/prime¯E/bracketleftBig
|vi|(ˆgi−gi)2/bracketrightBig
+¯f/prime¯E/bracketleftBig
|ˆvi−vi|(ˆgi−gi)2/bracketrightBig
+ 2¯f¯E[|ˆvi−vi|·|ˆgi−gi|]
since|˜vi|≤|vi|+|ˆvi−vi|. By equation (12), (13), (14),/vextendsingle/vextendsingle/vextendsingle∂η∂η/prime¯E[ψ(θ0,˜η)][ˆη−η0,ˆη−η0]/vextendsingle/vextendsingle/vextendsingle.δnn−1/2.
A.3.2 Main result
Claim: Let Lemma 2 hold. Then,
¯σ−1
N√
N(˜θ−θ)d→N(0,1)
where ¯σ2
N=/parenleftBig¯EN[tiI(hi>0)fidivi]/parenrightBig−1¯EN/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig/parenleftBig¯EN[tiI(hi>0)fidivi]/parenrightBig−1.
Proof.First, notethat ¯σ−1
N¯σn→1and ¯EN/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig1/2¯En/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig−1/2→1asN→∞.
Then by the Lyapunov CLT we have
¯σ−1
N√
N(˜θ−θ) = ¯σ−1
N√
N/parenleftBigg
1
KK/summationdisplay
k=1ˇθk−θ/parenrightBigg
= ¯σ−1
N√
N1
KK/summationdisplay
k=1/parenleftBigg
¯σn¯En/bracketleftbig
ψ2(wi,θ0,η0)/bracketrightbig−1/21
n/summationdisplay
i∈Ikψ(θ0,η0)/parenrightBigg
= (¯σ−1
N¯σn)(¯EN/bracketleftbig
ψ2(wi,θ0,η0)/bracketrightbig1/2¯En/bracketleftbig
ψ2(wi,θ0,η0)/bracketrightbig−1/2)·¯EN/bracketleftbig
ψ2(wi,θ0,η0)/bracketrightbig−1/21√
NK/summationdisplay
k=1/summationdisplay
i∈Ikψ(wi,θ0,η0)
= (¯σ−1
N¯σn)(¯EN/bracketleftbig
ψ2(wi,θ0,η0)/bracketrightbig1/2¯En/bracketleftbig
ψ2(wi,θ0,η0)/bracketrightbig−1/2)·/parenleftBigg
¯EN/bracketleftbig
ψ2(wi,θ0,η0)/bracketrightbig−1/21√
NN/summationdisplay
i=1ψ(wi,θ0,η0)/parenrightBigg
d→N(0,1).
27A.4 Proof of Theorem 1 (DML 2 case)
We have the following identity
1
KK/summationdisplay
k=1En[ψ˜θ,ˆηk] =1
KK/summationdisplay
k=1En[ψθ0,η0] +1
KK/summationdisplay
k=1En[ψ˜θ,ˆηk−ψθ0,η0]
=1
KK/summationdisplay
k=1En[ψθ0,η0] +1
KK/summationdisplay
k=1¯E[ψ˜θ,ˆηk] +1
KK/summationdisplay
k=1n−1/2Gn(ψ˜θ,ˆηk−ψ˜θ,η0)
+1
KK/summationdisplay
k=1n−1/2Gn(ψ˜θ,η0−ψθ0,η0)
=EN[ψθ0,η0] +1
KK/summationdisplay
k=1¯E[ψ˜θ,ˆηk] +1
KK/summationdisplay
k=1n−1/2Gn(ψ˜θ,ˆηk−ψ˜θ,η0) +N−1/2GN(ψ˜θ,η0−ψθ0,η0).
By deﬁnition,/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
K/summationtextK
k=1En[ψ˜θ,ˆηk]/vextendsingle/vextendsingle/vextendsingle/vextendsingle.δNN−1/2. By the proof of Lemma 2,
1
KK/summationdisplay
k=1n−1/2Gn(ψ˜θ,ˆηk−ψ˜θ,η0).1
KK/summationdisplay
k=1δnn−1/2=√
KδnN−1/2=δNN−1/2
1
KK/summationdisplay
k=1¯E[ψ˜θ,ˆηk] =−¯EN[tiI(hi>0)fidivi] (ˆθ−θ0) +Op(δNN−1/2) +Op(δN/vextendsingle/vextendsingle/vextendsingleˆθ−θ0/vextendsingle/vextendsingle/vextendsingle)
/vextendsingle/vextendsingle/vextendsingleN−1/2GN(ψ˜θ,η0−ψθ0,η0)/vextendsingle/vextendsingle/vextendsingle.δ1/2
NN−1/2.
Therefore,
¯EN[tiI(hi>0)fidivi] (ˆθ−θ0) =EN[ψθ0,η0] +Op(δNN−1/2) +Op(δN)/vextendsingle/vextendsingle/vextendsingleˆθ−θ0/vextendsingle/vextendsingle/vextendsingle
and by the Lyapunov CLT√
NEN[ψθ0,h0]
¯E[ψ2
θ0,h0]1/2d→N(0,1)
¯σ−1
N√
N(θ−θ0) =√
NEN[ψθ0,h0]
¯E[ψ2
θ0,h0]1/2d→N(0,1).
A.5 Proof of Theorem 2
A.5.1 Subsample result
Claim 1.
1
n/summationdisplay
i∈IktiI(ˆhk,i>0)ˆfk,idiˆvk,i−1
n/summationdisplay
i∈IkE[tiI(hi>0)fidivi]→0
28Proof.In the proof, I use En,ˆη,/bardbl·/bardbl2,ninstead ofEn,k,ˆηk,/bardbl·/bardbl2,n,k.
/vextendsingle/vextendsingle/vextendsingleEn[tiI(ˆhi>0)ˆfidiˆvi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingleEn[tiI(ˆhi>0)ˆfidiˆvi]−En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleEn[tiI(hi>0)fidivi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingleEn[tiI(ˆhi>0)ˆfidiˆvi]−En[tiI/parenleftBigˆhi>0/parenrightBig
fidivi]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleEn[tiI/parenleftBigˆhi>0/parenrightBig
fidivi]−En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingleEn[tiI(hi>0)fidivi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingleEn[di/parenleftBigˆfiˆvi−fivi/parenrightBig
]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleEn[/braceleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/bracerightBig
fidivi]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleEn[tiI(hi>0)fidivi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingleEn[(ˆfi−fi)diˆvi]/vextendsingle/vextendsingle/vextendsingle+|En[fidi(ˆvi−vi)]|+/vextendsingle/vextendsingle/vextendsingleEn[/braceleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/bracerightBig
fidivi]/vextendsingle/vextendsingle/vextendsingle
+/vextendsingle/vextendsingle/vextendsingleEn[tiI(hi>0)fidivi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingleEn[(ˆfi−fi)di(ˆvi−vi)]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleEn[(ˆfi−fi)divi]/vextendsingle/vextendsingle/vextendsingle+|En[fidi(ˆvi−vi)]|
+/vextendsingle/vextendsingle/vextendsingleEn[/braceleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/bracerightBig
fidivi]/vextendsingle/vextendsingle/vextendsingle+/vextendsingle/vextendsingle/vextendsingleEn[tiI(hi>0)fidivi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
.P/vextenddouble/vextenddouble/vextenddouble(ˆfi−fi)di/vextenddouble/vextenddouble/vextenddouble
2,n/bardblˆvi−vi/bardbl2,n+/vextenddouble/vextenddouble/vextenddoubleˆfi−fi/vextenddouble/vextenddouble/vextenddouble
2,n/bardbldivi/bardbl2,n+/bardblfidi/bardbl2,n/bardblˆvk,i−vi/bardbl2,n
+/bardblfidivi/bardbl2,n/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/vextenddouble/vextenddouble/vextenddouble
2,n+/vextendsingle/vextendsingle/vextendsingleEn[tiI(hi>0)fidivi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle
Sincefi,ˆfi≤C,¯E[d4
i]≤C,¯E[v4
i]≤Cby Condition IQR (1), /bardblˆvi−vi/bardbl2,n.Pδn,/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhk,i>0/parenrightBig
−I(hi>0)/vextenddouble/vextenddouble/vextenddouble
2,n.Pδn, and Law of large numbers,
/vextendsingle/vextendsingle/vextendsingleEn[tiI(ˆhi>0)ˆfidiˆvi]−¯En[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle.Pδn.
Claim 2.
1
n/summationdisplay
i∈Ikψ2(wi,˜θ,ˆηk)−1
n/summationdisplay
i∈IkE/bracketleftBig
ψ2(wi,θ0,η0)/bracketrightBig
→0
Proof.In the proof, I use En,ˆη,/bardbl·/bardbl2,ninstead ofEn,k,ˆηk,/bardbl·/bardbl2,n,k. SinceEn/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig−
¯E/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig→Pn0byLawoflargenumbers,itsuﬃcestoshow En/bracketleftBig
ψ2(wi,˜θ,ˆη)/bracketrightBig
−En/bracketleftbigψ2(wi,θ0,η0)/bracketrightbig→Pn
0. Since
En/bracketleftBig
ψ2(wi,˜θ,ˆηk)/bracketrightBig
−En/bracketleftBig
ψ2(wi,θ0,η0)/bracketrightBig
=
/parenleftBigg/radicalbigg
En/bracketleftBig
ψ2(wi,˜θ,ˆηk)/bracketrightBig
−/radicalBig
En[ψ2(wi,θ0,η0)]/parenrightBigg
·/parenleftBigg/radicalbigg
En/bracketleftBig
ψ2(wi,˜θ,ˆηk)/bracketrightBig
+/radicalBig
En[ψ2(wi,θ0,η0)]/parenrightBigg
29it suﬃces to show that/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddoubleψ(wi,˜θ,ˆη)/vextenddouble/vextenddouble/vextenddouble
2,n−/bardblψ(wi,θ0,η0)/bardbl2,n/vextendsingle/vextendsingle/vextendsingle/vextendsingle→Pn0.
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextenddouble/vextenddouble/vextenddoubleψ(wi,˜θ,ˆη)/vextenddouble/vextenddouble/vextenddouble
2,n−/bardblψ(wi,θ0,η0)/bardbl2,n/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/vextenddouble/vextenddouble/vextenddoubleψ(wi,˜θ,ˆη)−ψ(wi,θ0,η0)/vextenddouble/vextenddouble/vextenddouble
2,n
≤/vextenddouble/vextenddouble/vextenddoubletiI/parenleftBigˆhi>0/parenrightBig/parenleftBigˆhi−I(yi−di˜θ−ˆgi≤0)/parenrightBig
ˆvi−tiI(hi>0) (hi−I(yi−diθ0−gi≤0))vi/vextenddouble/vextenddouble/vextenddouble
2,n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
A
+/vextenddouble/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
(ti−ˆπi)(1−τ)
ˆπiˆvi−I(hi>0) (ti−πi)(1−τ)
πivi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
B
A≤/vextenddouble/vextenddouble/vextenddoubletiI/parenleftBigˆhi>0/parenrightBig/parenleftBigˆhi−I(yi−di˜θ−ˆgi≤0)/parenrightBig
(ˆvi−vi)/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddouble/braceleftBig
tiI/parenleftBigˆhi>0/parenrightBig/parenleftBigˆhi−I(yi−di˜θ−ˆgi≤0)/parenrightBig
−tiI(hi>0) (hi−I(yi−diθ−gi≤0))/bracerightBig
vi/vextenddouble/vextenddouble/vextenddouble
2,n
≤/vextenddouble/vextenddouble/vextenddoubletiI/parenleftBigˆhi>0/parenrightBig/parenleftBigˆhi−I(yi−di˜θ−ˆgi≤0)/parenrightBig
(ˆvi−vi)/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddouble/braceleftBig
tiI/parenleftBigˆhi>0/parenrightBig/parenleftBigˆhi−I(yi−di˜θ−ˆgi≤0)/parenrightBig
−tiI/parenleftBigˆhi>0/parenrightBig
(hi−I(yi−diθ−gi≤0))/bracerightBig
vi/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddouble/braceleftBig
tiI/parenleftBigˆhi>0/parenrightBig
(hi−I(yi−diθ−gi≤0))−tiI(hi>0) (hi−I(yi−diθ−gi≤0))/bracerightBig
vi/vextenddouble/vextenddouble/vextenddouble
2,n
≤/vextenddouble/vextenddouble/vextenddoubletiI/parenleftBigˆhi>0/parenrightBig/parenleftBigˆhi−I(yi−di˜θ−ˆgi≤0)/parenrightBig
(ˆvi−vi)/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddouble/braceleftBig
tiI/parenleftBigˆhi>0/parenrightBig/parenleftBigˆhi−hi−I(yi−di˜θ−ˆgi≤0) +I(yi−diθ−gi≤0)/parenrightBig/bracerightBig
vi/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddouble/braceleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/bracerightBig
ti(hi−I(yi−diθ−gi≤0))vi/vextenddouble/vextenddouble/vextenddouble
2,n
≤/vextenddouble/vextenddouble/vextenddouble/parenleftBigˆhi−I(yi−di˜θ−ˆgi≤0)/parenrightBig
(ˆvi−vi)/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddoublevi/parenleftBigˆhi−hi/parenrightBig/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddoublevi/parenleftBig
I(yi−di˜θ−ˆgi≤0)−I(yi−diθ−gi≤0)/parenrightBig/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddouble(hi−I(yi−diθ−gi≤0))vi/braceleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/bracerightBig/vextenddouble/vextenddouble/vextenddouble
2,n
/vextenddouble/vextenddouble/vextenddouble/parenleftBigˆhk,i−I(yi−di˜θ−ˆgk,i≤0)/parenrightBig
(ˆvk,i−vi)/vextenddouble/vextenddouble/vextenddouble
2,n.P/bardblˆvk,i−vi/bardbl2,n.Pδn
/vextenddouble/vextenddouble/vextenddoublevi/parenleftBigˆhk,i−hi/parenrightBig/vextenddouble/vextenddouble/vextenddouble
2,n.P/bardblˆπk,i−πi/bardbl2,n.Pδn
/vextenddouble/vextenddouble/vextenddoublevi/parenleftBig
I(yi−di˜θ−ˆgk,i≤0)−I(yi−diθ−gi≤0)/parenrightBig/vextenddouble/vextenddouble/vextenddouble
2,n≤/vextenddouble/vextenddouble/vextenddoubleviI/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsingledi(θ−˜θ) +gi−˜gi/vextendsingle/vextendsingle/vextendsingle/parenrightBig/vextenddouble/vextenddouble/vextenddouble
2,n
≤/braceleftbigg/vextenddouble/vextenddouble/vextenddoublev2
i/vextenddouble/vextenddouble/vextenddouble
2,n/vextenddouble/vextenddouble/vextenddoubleI/parenleftBig
|/epsilon1i|≤/vextendsingle/vextendsingle/vextendsingledi(θ−˜θ) +gi−˜gi/vextendsingle/vextendsingle/vextendsingle/parenrightBig/vextenddouble/vextenddouble/vextenddouble
2,n/bracerightbigg1/2
.Pδn
30/vextenddouble/vextenddouble/vextenddouble(hi−I(yi−diθ−gi≤0))vi/braceleftBig
I/parenleftBigˆhk,i>0/parenrightBig
−I(hi>0)/bracerightBig/vextenddouble/vextenddouble/vextenddouble
2,n.P/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhk,i>0/parenrightBig
−I(hi>0)/vextenddouble/vextenddouble/vextenddouble
2,n,k.Pδn
Therefore,A.Pδn.
B=/vextenddouble/vextenddouble/vextenddouble/vextenddouble(1−τ)ti·I/parenleftBigˆhi>0/parenrightBigˆvi
ˆπi−(1−τ)·I/parenleftBigˆhi>0/parenrightBig
ˆvi−(1−τ)ti·I(hi>0)vi
πi+ (1−τ)·I(hi>0)vi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n
./vextenddouble/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBigˆvi
ˆπi−I(hi>0)vi
πi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n+/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
ˆvi−I(hi>0)vi/vextenddouble/vextenddouble/vextenddouble
2,n
≤/vextenddouble/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBigˆvi
ˆπi−I/parenleftBigˆhi>0/parenrightBigvi
ˆπi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n+/vextenddouble/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBigvi
ˆπi−I/parenleftBigˆhi>0/parenrightBigvi
πi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n+/vextenddouble/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBigvi
πi−I(hi>0)vi
πi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
ˆvi−I/parenleftBigˆhi>0/parenrightBig
vi/vextenddouble/vextenddouble/vextenddouble
2,n+/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
vi−I(hi>0)vi/vextenddouble/vextenddouble/vextenddouble
2,n
=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBigˆvi−vi
ˆπi/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n+/vextenddouble/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
vi1
ˆπiπi(ˆπi−πi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n+/vextenddouble/vextenddouble/vextenddouble/vextenddoublevi
πi/braceleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/bracerightBig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
2,n
+/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhi>0/parenrightBig
(ˆvi−vi)/vextenddouble/vextenddouble/vextenddouble
2,n+/vextenddouble/vextenddouble/vextenddoublevi/braceleftBig
I/parenleftBigˆhi>0/parenrightBig
−I(hi>0)/bracerightBig/vextenddouble/vextenddouble/vextenddouble
2,n
Since ˆπiandπiareboundedawayfromzero, ¯E[v2
i]≤C,/bardblˆvi−vi/bardbl2,n.Pδn,/vextenddouble/vextenddouble/vextenddoubleI/parenleftBigˆhk,i>0/parenrightBig
−I(hi>0)/vextenddouble/vextenddouble/vextenddouble
2,n.P
δn,/bardblˆπi−πi/bardbl2,n.Pδn,B.Pδn.
A.5.2 Main result
Under Claim 1 and Claim 2, the variance estimator ˆσ2
Nis consistent where
ˆσ2
N=/parenleftBigg
1
KK/summationdisplay
k=1En,k[tiI(ˆhk,i>0)ˆfk,idiˆvk,i]/parenrightBigg−11
KK/summationdisplay
k=1En,k/bracketleftBig
ψ2(wi,˜θ,ˆηk)/bracketrightBig/parenleftBigg
1
KK/summationdisplay
k=1En,k[tiI(ˆhk,i>0)ˆfk,idiˆvk,i]/parenrightBigg−1
.
Proof.First, by Claim 1, for all k∈[K]
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
i∈IktiI(ˆhk,i>0)ˆfk,idiˆvk,i−1
n/summationdisplay
i∈IkE[tiI(hi>0)fidivi]/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle→0.
Then,
1
NN/summationdisplay
i=1E[tiI(hi>0)fidivi]−1
KK/summationdisplay
k=1En,k[tiI(ˆhk,i>0)ˆfk,idiˆvk,i]
=1
KK/summationdisplay
k=1

1
n/summationdisplay
i∈IkE[tiI(hi>0)fidivi]−1
n/summationdisplay
i∈IktiI(ˆhk,i>0)ˆfk,idiˆvk,i


→0.
31Next, by Claim 2, for all k∈[K]
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
i∈Ikψ2(wi,˜θ,ˆηk)−1
n/summationdisplay
i∈IkE/bracketleftBig
ψ2(wi,θ0,η0)/bracketrightBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle→0.
Then,
1
NN/summationdisplay
i=1E/bracketleftBig
ψ2(wi,θ0,η0)/bracketrightBig
−1
KK/summationdisplay
k=1En,k[ψ2(wi,˜θ,ˆη0,k)] =1
KK/summationdisplay
k=1

1
n/summationdisplay
i∈IkE/bracketleftBig
ψ2(wi,θ0,η0)/bracketrightBig
−1
n/summationdisplay
i∈Ikψ2(wi,˜θ,ˆηk)


→0.
32References
Andrews, D. W. (1994): “Empiricalprocessmethodsineconometrics,” Handbook of econometrics ,
4, 2247–2294.
Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012): “Sparse models and
methods for optimal instruments with an application to eminent domain,” Econometrica , 80,
2369–2429.
Belloni, A. and V. Chernozhukov (2011): “l1-penalized quantile regression in high-
dimensional sparse models,” The Annals of Statistics , 39, 82–130.
Belloni, A., V. Chernozhukov, I. Fernández-Val, and C. Hansen (2017): “Program
evaluation and causal inference with high-dimensional data,” Econometrica , 85, 233–298.
Belloni, A., V. Chernozhukov, and C. Hansen (2014): “Inference on treatment eﬀects after
selection among high-dimensional controls,” The Review of Economic Studies , 81, 608–650.
Belloni, A., V. Chernozhukov, and K. Kato (2019): “Valid post-selection inference in high-
dimensional approximately sparse quantile regression models,” Journal of the American Statis-
tical Association , 114, 749–758.
Buchinsky, M. and J. Hahn (1998): “An alternative estimator for the censored quantile regres-
sion model,” Econometrica , 653–671.
Chen, J.-e., C.-H. Huang, and J.-J. Tien (2021): “Debiased/Double Machine Learning for
Instrumental Variable Quantile Regressions,” Econometrics , 9, 15.
Chen, S. (2018): “Sequentialestimationofcensoredquantileregressionmodels,” Journal of Econo-
metrics, 207, 30–52.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and W. Newey
(2017a): “Double/debiased/neyman machine learning of treatment eﬀects,” American Economic
Review, 107, 261–265.
Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and
J. Robins (2018): “Double/debiased machine learning for treatment and structural parameters,”
The Econometrics Journal , 21, C1–C68.
Chernozhukov, V., D. Chetverikov, and K. Kato (2014): “Gaussian approximation of
suprema of empirical processes,” The Annals of Statistics , 42, 1564–1597.
Chernozhukov, V., J. C. Escanciano, H. Ichimura, W. K. Newey, and J. M. Robins
(2020): “Locally robust semiparametric estimation,” arXiv preprint arXiv:1608.00033 .
33Chernozhukov, V., I. Fernández-Val, and A. E. Kowalski (2015a): “Quantile regression
with censoring and endogeneity,” Journal of Econometrics , 186, 201–221.
Chernozhukov, V. and C. Hansen (2004): “The eﬀects of 401 (k) participation on the wealth
distribution: an instrumental quantile regression analysis,” Review of Economics and statistics ,
86, 735–751.
Chernozhukov, V., C. Hansen, and M. Spindler (2015b): “Valid post-selection and post-
regularization inference: An elementary, general approach,” Annu. Rev. Econ. , 7, 649–688.
Chernozhukov, V., C. Hansen, and K. Wuthrich (2017b): “Instrumental variable quantile
regression,” in Handbook of Quantile Regression , Chapman and Hall/CRC, 119–143.
Chernozhukov, V. and H. Hong (2002): “Three-step censored quantile regression and extra-
marital aﬀairs,” Journal of the American statistical Association , 97, 872–882.
Fack, G. and C. Landais (2010): “Are tax incentives for charitable giving eﬃcient? Evidence
from France,” American Economic Journal: Economic Policy , 2, 117–41.
Fei, Z., Q. Zheng, H. G. Hong, and Y. Li (2021): “Inference for High-Dimensional Censored
Quantile Regression,” Journal of the American Statistical Association , 1–15.
Fitzenberger, B. (1997): “15 a guide to censored quantile regressions,” Handbook of statistics ,
15, 405–437.
Fitzenberger, B. and P. Winker (2007): “Improving the computation of censored quantile
regressions,” Computational Statistics & Data Analysis , 52, 88–108.
Graff Zivin, J. and M. Neidell (2012): “The impact of pollution on worker productivity,”
American Economic Review , 102, 3652–73.
He, X., X. Pan, K. M. Tan, and W.-X. Zhou (2022): “Scalable estimation and inference for
censored quantile regression process,” The Annals of Statistics , 50, 2899–2924.
Khan, S. and J. L. Powell (2001): “Two-step estimation of semiparametric censored regression
models,” Journal of Econometrics , 103, 73–110.
Koenker, R. (2008): “Censored quantile regression redux,” Journal of Statistical Software , 27,
1–25.
Kowalski, A. (2016): “Censored quantile instrumental variable estimates of the price elasticity
of expenditure on medical care,” Journal of Business & Economic Statistics , 34, 107–117.
Peng, L. and Y. Huang (2008): “Survival analysis with quantile regression models,” Journal of
the American Statistical Association , 103, 637–649.
34Portnoy, S. (2003): “Censored regression quantiles,” Journal of the American Statistical Associ-
ation, 98, 1001–1012.
Poterba, J. M., S. F. Venti, and D. A. Wise (1994): “401 (k) plans and tax-deferred saving,”
inStudies in the Economics of Aging , University of Chicago Press, 105–142.
——— (1995): “Do 401 (k) contributions crowd out other personal saving?” Journal of Public
Economics , 58, 1–32.
Powell, J. L. (1986): “Censored regression quantiles,” Journal of econometrics , 32, 143–155.
Tang, Y., H. J. Wang, X. He, and Z. Zhu (2012): “An informative subset-based estimator for
censored quantile regression,” Test, 21, 635–655.
Zheng, Q., L. Peng, and X. He (2018): “High dimensional censored quantile regression,” Annals
of statistics , 46, 308.
35