A Framework for Active Haptic Guidance Using Robotic Haptic Proxies
Niall L. Williamsy1, Nicholas Rewkowskiy2, Jiasheng Li1, and Ming C. Lin1
https://gamma.umd.edu/active haptic guidance/
Abstract ‚Äî Haptic feedback is an important component of
creating an immersive mixed reality experience. Traditionally,
haptic forces are rendered in response to the user‚Äôs interactions
with the virtual environment. In this work, we explore the
idea of rendering haptic forces in a proactive manner, with
the explicit intention to inÔ¨Çuence the user‚Äôs behavior through
compelling haptic forces. To this end, we present a framework
foractive haptic guidance in mixed reality, using one or more
robotic haptic proxies to inÔ¨Çuence user behavior and deliver
a safer and more immersive virtual experience. We provide
details on common challenges that need to be overcome when
implementing active haptic guidance, and discuss example
applications that show how active haptic guidance can be used
to inÔ¨Çuence the user‚Äôs behavior. Finally, we apply active haptic
guidance to a virtual reality navigation problem, and conduct a
user study that demonstrates how active haptic guidance creates
a safer and more immersive experience for users.
I. INTRODUCTION
In mixed reality (MR), the user is at least partially im-
mersed in a 3D, computer-generated environment. Included
within the mixed reality spectrum are augmented reality
and virtual reality (VR). A major factor that makes MR a
unique medium is that it is interactive‚Äîthe user is able to
interact with the virtual environment (VE) through position-
tracking sensors that update the VE according to the user‚Äôs
movements in the physical environment (PE). For example,
when the user moves their head in the real world, the position
of the camera in the virtual world moves as well. Interactions
like these help to make users feel like they are really in the
VE that they see through the head-mounted display (HMD).
One key component to increasing the user‚Äôs sense of presence
in a VE is to improve the perceptual stimuli matching [9],
wherein the user is provided with perceptual information that
matches their actions (e.g. the viewing perspective updates as
the user‚Äôs head moves). In this work, we focus on the sense
of touch provided by kinesthetic haptic feedback and how we
can use robots to provide more realistic haptic sensations to
improve the sense of immersion and safety in MR.
Robotic technology has in fact been used to provide haptic
feedback in MR to improve the sense of virtual touch and
virtual manipulation [11]. For example, MR can enhance
robotics via telepresence, wherein humans can remotely
operate robot to high precision using immersive controls
afforded by VR.
*This work is partially supported by National Science Foundation and
Lin‚Äôs Barry Mersky and Capital One E-Nnovate Endowed Professorship.
1Authors are with the Department of Computer Science, University of
Maryland, College Park. fniallw, jsli, lin g@umd.edu
2Email: fnick.rewkowski g@gmail.com
yEqual contribution
Fig. 1. An image of a user in the physical environment (left) and virtual
environment (right) in our implementation of active haptic guidance. The
user is tethered to a robot in the physical environment and to a virtual dog
companion in the virtual environment. The robot provides haptic feedback to
the user according to the virtual companion‚Äôs movements, which improves
the user‚Äôs sense of presence in the virtual world and encourages the user to
avoid the boundaries of the virtual reality system‚Äôs tracked space.
In this paper, we introduce the possibility of using robots
to enhance the virtual experience through haptic feedback.
SpeciÔ¨Åcally, we use robots to guide the user as they navigate
through a VE, and reconÔ¨Ågure and virtually expand the
PE to align with the VE; we achieve this through manual
haptic feedback that directs the user‚Äôs locomotion behavior
in the VE, thereby making the virtual experience more
immersive and safer. To this end, we introduce the concept
ofactive haptic guidance , which describes the problem of
reconÔ¨Åguring one or multiple robots in the PE in real time
such that they provide haptic feedback to guide the user
and inÔ¨Çuence their actions and motion in the VE, with the
ultimate goal of improving the user‚Äôs safety or level of
immersion in MR. Unlike traditional haptic guidance [8]
that requires users to follow the haptic forces with the goal
of teaching them speciÔ¨Åc skills, our framework maintains
the user‚Äôs freedom to explore and interact with the VE
however they wish, with the goal of enhancing their virtual
experience. One major challenge with robots for active haptic
feedback in MR is that the physical robots and their virtual
counterparts must be co-located relative to the user , in
order to provide the correct haptic feedback that aligns with
the virtual counterpart. This problem can be exacerbated
when the environments/interactions are dynamic (i.e. the
physical and virtual haptic proxy must move synchronously)
or when there is a decoupling between the user‚Äôs physical and
virtual locations (as is common with some VR interactionarXiv:2301.05311v2  [cs.RO]  27 Feb 2023techniques like redirected walking [28]).
Main contributions: We introduce the concept of ac-
tive haptic guidance for improved virtual locomotion , and
conduct a user study to show an example of how active
haptic guidance can be used to improve a user‚Äôs safety and
feelings of immersion in a virtual experience. Our framework
is general, so it can be applied to use cases other than
locomotion. Our main contributions include:
A formal description of the active haptic guidance
problem and details on common challenges that are
faced when implementing active haptic guidance. Active
haptic guidance involves using robots to proactively
provide realistic haptic feedback to users in mixed
reality, with the goal of inÔ¨Çuencing users‚Äô behaviors to
improve safety and/or sense of presence in the VE.
An prototype realization and user study showing the
beneÔ¨Åts of active haptic guidance. In our study, par-
ticipants completed a virtual navigation task using real
walking, either with or without active haptic guidance.
Our results show that active haptic guidance can signif-
icantly improve the virtual experience by reducing the
number of ‚Äúbreaks in presence‚Äù and keeping them a safe
distance away from physical objects for longer.
II. BACKGROUND AND RELATED WORK
Haptic feedback can be utilized in any mixed reality
setting, but in this work we mainly discuss applications of
haptics to VR settings, since our implementation was done
in VR. In VR, the user wears a HMD through which they
view a 3D, computer-generated VE [16]. The user‚Äôs position
in the PE is tracked, so that whenever the user updates their
position in the PE, the position of the virtual camera updates
accordingly to provide an accurate viewing perspective of
the VE. VR is an interactive experience, meaning that the
user does not passively observe the virtual content, but
instead the environment changes in response to the user‚Äôs
actions and movements. When the virtual experience feels
sufÔ¨Åciently real, the user experiences a sense of presence,
which describes the subjective feeling of really being in the
environment [35]. Factors that contribute to a user‚Äôs feelings
of presence and immersion in a VE include the HMD refresh
rate [4], the environment realism and visual quality [40],
and perceptual stimuli matching [9], [38] (the process of
providing users with perceptual information that matches
their actions in the VE). In this paper, we focus on providing
haptic stimuli for perceptual stimulus matching to improve
the user‚Äôs experience in VR.
Haptic feedback can be provided in a passive or an
active manner. With passive haptics, objects are placed in
the PE such that they align with the locations of objects
in the VE, resulting in haptic feedback when the user
tries to touch objects in the VE [12]. Conversely, active
haptics involves a haptic proxy that dynamically alters its
conÔ¨Åguration in real time to provide the appropriate haptic
force feedback, depending on the user‚Äôs interactions with
the VE. It is common to use robotic systems to render
haptic forces. For example, Zhang et al. [42] used a roboticarm to provide haptic feedback during object assembly by
aligning the arm‚Äôs end effector with the handheld proxy.
Siu et al. [32] used an array of actuated pins to match
the contours of virtual objects. Similarly, other researchers
have used tangible, re-conÔ¨Ågurable electronic peripherals to
represent virtual objects [43], [21]. To recreate the feelings of
grasping virtual objects, Kovacs et al. [19] used a wrist-worn
device to provide on-demand haptic feedback when users
grip virtual objects, while Sinclair et al. [31] used a force-
resisting, handheld controller to render haptic forces for
rigid and compliant objects. Suzuki et al. [37] used mobile
robots to rearrange physical furniture to align with virtual
furniture as the user moved through a virtual world. Robotic
systems have also been used to help visually-impaired users
understand the virtual content [20], [22], [33], [44]. Finally,
mobile robots have been used to simulate different kinds of
virtual terrain that the user walks over [14], [13].
The majority of prior work on active haptics for mixed
reality requires the user to initiate interactions with the VE
before the haptic forces are rendered. That is, the haptic
forces are triggered by the user‚Äôs interactions with the VE,
so it is the user‚Äôs actions that dictate when haptic forces
are rendered. In this work, we make the distinction of using
active haptics speciÔ¨Åcally to direct the user and inÔ¨Çuence
their behavior in the VE (in addition to providing a more
immersive experience, as all haptics aims to do). We deÔ¨Åne
this use of haptics as active haptic guidance , since it is the
haptic forces that direct the user‚Äôs behaviors, rather than
the other way around. We note that there already exists
research on ‚Äúhaptic guidance,‚Äù which Feygin et al. use to
refer to haptic feedback that is used to help people learn
motor skills [8]. The distinction between our work on active
haptic guidance and Feygin et al.‚Äôs work is that we use haptic
feedback to discreetly inÔ¨Çuence the user‚Äôs behavior in an
effort to enhance their feelings of presence and level of safety
in a mixed reality experience, while Feygin et al. use haptics
to teach people motor skills. Furthermore, haptic guidance
by Feygin et al. gives users little autonomy to control their
movements during the guidance, whereas our active haptic
guidance framework does not take away the user‚Äôs autonomy
to interact with the VE however they wish, which is a crucial
component of MR experiences [16]. Finally, haptic guidance
[8] is designed for real-world use-case, while active haptic
guidance is designed speciÔ¨Åcally for MR experiences.
III. PROBLEM DESCRIPTION
Here we describe the active haptic guidance problem, as
well as constraints that need to be satisÔ¨Åed to effectively
implement active haptic guidance.
A. DeÔ¨Ånitions
In virtual reality, the user is located in a PE and VE at
the same time. Each environment consists of objects (either
physical objects or virtual objects represented by textured
meshes) and agents (the users and robots). Note that it is
common to refer to virtual humans and animals as agents,
but in this work we will consider all components of the VEas generic objects for simplicity, and we use ‚Äúagents‚Äù to refer
only to humans and robots in the PE.
LetO=fo1;o2;:::;o igbe a set of polygonal objects,
where each object ois a mesh with vertices in R3. Let
U=fu1;u2;:::;u jgbe the set of users in an environment.
Here,urepresents the user‚Äôs state in an environment, and
usually describes their position and orientation in said envi-
ronment. For example, we can deÔ¨Åne u=fp;g, where
p2R2represents their position in the 2D plane and
2[0;2)represents their orientation in the environment.
LetR=fr1;r2;:::;r kgbe the set of robots in an
environment, and let A=fU[Rgbe the set of all agents.
Each setO;U;R; andAmay be empty.
We deÔ¨Åne an environment Eas a set of obstacles and
agents; that is, E=fO;Ag. To differentiate between the PE
and VE, we denote the PE as EP=fOP;APgand the VE
asEV=fOV;AVg. For each user in VR, they will have a
representation in both the PE and VE, so jUPj=jUVj=n,
wherenis the number of users. Since we only consider
agents to be users and robots in this work, jAVj=n(i.e.,
the only agents in the VE are the users). In the VE, there are
some objects that the user is likely to interact with, which
will improve their sense of presence in the environment. We
deÔ¨Åne this set of ‚Äúobjects of interest‚Äù OOVas the set of
virtual objects for which we render haptic forces when the
user interacts with them.
With these deÔ¨Ånitions of the PE and VE, we can now de-
scribe the two main conditions that need to be met to provide
active haptic guidance to users in MR. First, the robots in the
physical environment need to provide the appropriate haptic
feedback to inÔ¨Çuence the user‚Äôs conÔ¨Åguration. Second, we
need to ensure that the robots that provide haptic feedback
are co-located with the virtual objects of interest with which
the haptic forces are associated.
B. InÔ¨Çuential Haptics Constraint
The Ô¨Årst condition that must be met is that the rendered
haptic forces should inÔ¨Çuence the user‚Äôs behavior such
that they update their physical and virtual conÔ¨Ågurations .
We dub this the inÔ¨Çuential haptics (IH) constraint . For
simplicity, we formalize this constraint using one user, one
robot, and one virtual object of interest, but this constraint
applies to any group of agents and virtual objects for which
we render haptic forces.
Given the user‚Äôs physical and virtual conÔ¨Ågurations uP
anduV, a virtual object of interest o, and a robot rthat
provides haptic feedback for o, we wish to render a haptic
forceFthat compels the user to update uPanduVto
some goal conÔ¨Ågurations u
Pandu
V. Thus, fulÔ¨Ålling the
IH constraint requires completing the following steps:
1) Compute the goal conÔ¨Ågurations u
Pandu
V.
2) Detect or initiate an interaction IbetweenoanduV.
3) Update the conÔ¨Åguration of rto render a haptic force
F(I;uV;uP;u
P;u
V;r)that minimizes an objective
functionf(uV;uP;u
P;u
V).
In practice, computing F(I;uV;uP;u
P;u
V;r)depends
heavily on the mechanics of rand the objective functionf(uV;uP;u
P;u
V). The objective function is usually a dis-
tance function that measures the error between uPandu
P, as
well asuVandu
V(e.g. Euclidean distance), and it depends
on the user‚Äôs conÔ¨Åguration space. By rendering F, the user
hopefully updates their conÔ¨Åguration in order to move closer
tou
Pandu
V.
Computingu
Pandu
Vis a matter of determining how we
want the user to behave, and will vary depending on the MR
application context. In MR, two main reasons to inÔ¨Çuence
the user‚Äôs behavior are to ensure their safety and to deliver a
more immersive experience. In MR systems, the user tries to
navigate through the PE and the VE at the same time, but the
PE is partially or fully occluded. Thus, in order to prevent the
user from bumping into physical objects that they cannot see,
locomotion interfaces for MR usually display a notiÔ¨Åcation
that prompts them to reposition themself to a safer position
away from nearby objects. By using haptics to warn users
(either overtly or subtly), we can decrease the likelihood that
the user collides with unseen physical obstacles or exits the
designated tracking area. In addition to ensuring user safety,
inÔ¨Çuencing the user‚Äôs behavior can be useful for improving
the user‚Äôs sense of presence in the VE. In MR, providing
perceptual stimuli that align with the content rendered on the
visual display enhances the user‚Äôs feeling that they are really
in the VE that they are seeing. To this end, haptic feedback
can signiÔ¨Åcantly improve the user‚Äôs sense of presence in the
VE [12]. In the case of active haptic guidance, the haptic
feedback can be used as an additional narrative element that
encourages users to explore a particular area or interact with
particular objects in the VE (e.g. pairing visual distractors
[25] with haptic feedback to direct the user‚Äôs attention).
C. Relative Co-location Constraint
The second main constraint that should be met when
using active haptic guidance is that the physical robots
that render the haptic forces and their associated virtual
objects should be co-located relative to the user . That is,
the position of the robot and the virtual object should be the
same relative to uPanduV. This is done to ensure that the
user perceives a congruent VE that is augmented by haptic
forces, rather than perceiving a VE along with misaligned
haptic forces, which may break their sense of presence. We
call this the relative co-location (RC) constraint .
GivenuP,uV,o, andrwhich provides haptic feedback for
o, we wish to update rsuch that we minimize the error in the
relative positions between uVandoanduPandr. FulÔ¨Ålling
the RC constraint requires completing the following steps:
1) Compute the conÔ¨Ågurations of oandrrelative touV
anduP, respectively. Usually, these are just positions
poandprofoandrrelative to the user in the
respective environment.
2) Compute a goal conÔ¨Åguration rfor the haptic proxy
that minimizes an objective function f(po;pr).
3) Update the conÔ¨Åguration of rto move it towards r.
In practice, updating the robot‚Äôs conÔ¨Åguration in step #3 is
a motion planning problem where we aim to Ô¨Ånd a paththrough the conÔ¨Åguration space that brings rclose tor,
and it depends on the mechanics of the haptic proxy.
Since MR is an interactive technology, poandprare
constantly changing as the user explores and interacts with
the VE. Thus, evaluating and fulÔ¨Ålling the RC constraint
must be done regularly to ensure that any perceptual stimuli
mismatch is minimized. Failure to adequately meet this
constraint can degrade the user experience, since it increases
the likelihood that the user notices a discrepancy between the
visual and haptic stimuli [15], [24]. Furthermore, knowing
how much error between their relative positions the user will
tolerate is a subjective measure [3], [18], so it is usually
not the case that the robot must reach rexactly. Note
that this relative co-location constraint is not unique to the
active haptic guidance problem (unlike subsection III-B);
other work on active haptics for virtual reality also has to
deal with the problem of ensuring the co-location of robotic
agents and their virtual counterparts [42], [39], [2], [23].
IV. PROTOTYPE REALIZATION EXAMPLES
In this section, we provide details on our prototype im-
plementation of an application of active haptic guidance. In
particular, we implement an active haptic-driven locomotion
application to provide a safer and more immersive virtual
navigation experience for users. We discuss other potential
use-cases for active haptic guidance in the supplementary
materials posted on our project page.
A. Natural Walking in Virtual Reality
In VR, it is common for the PE to be much smaller than
the VE. To enable users to explore large VEs, many different
locomotion interfaces such as teleportation, joystick naviga-
tion, and walking-in-place have been developed [7]. Ideally,
users explore the VE using natural, everyday walking since
it improves their sense of presence [38] and performance
in tasks [10], [26], [30]. One technique that enables natural
walking in VR is redirected walking (RDW) [28].
RDW works by slowly rotating the VE around the user‚Äôs
virtual camera while they walk, which causes them to
unconsciously adjust their physical trajectory to counteract
the VE rotations and remain on their intended path in the
VE. It works because the human perceptual system tends to
believe the user‚Äôs visual stimuli over other stimuli (proprio-
ceptive, vestibular, etc.) when they conÔ¨Çict, as is the case in
RDW [27]. Using RDW, we can steer the user along paths in
the PE that direct them away from objects and edges of the
tracked space, resulting in a safer and more immersive virtual
experience. To help mask the VE rotations, researchers make
use of distractors which grab the user‚Äôs attention to decrease
the likelihood that they attend to the rotations of the VE [5],
[25], [41]. In our prototype implementation, we use a virtual
dog as a distractor in conjunction with a RDW algorithm
known as steer-to-center, which rotates the VE such that the
user is steered towards the center of the PE at all times [27].
B. Virtual Experience and Equipment
For our implementation, a user u1completed a navi-
gation task in a virtual city and had a virtual dog as acompanion (only a single user participated at a time, so
jUPj=jUVj= 1 ). Additionally, u1held a position-
tracked leash that was tethered to a differential wheeled robot
r1. The PE was an empty rectangular room with four walls
(represented by the boundaries of the VR tracking space).
Thus,EP=fOP; APg, whereAP=fu1; r1g. The
virtual dog served as a distractor and was the only object
of interest in EV(jOj= 1), meaning that the robot only
rendered haptic forces associated with the virtual dog.
Our application was implemented using one HTC VIVE
Cosmos VR HMD with two VIVE trackers, and one robot
car (ELEGOO UNO Robot Car kit). We attached one VIVE
tracker to the robot to track its location and orientation data,
and the other was attached to the leash handle to calculate
the distance between u1andr1. The robot was equipped
with an HC-06 Bluetooth LE adapter, which connected to
the PC to transmit robot movement commands. The Unreal
4.22 game engine was used to render the VE.
C. Virtual Companion and Robot Behavior
Here we describe the behavior of the virtual dog com-
panion and how the robot matches the virtual companion‚Äôs
movements and provides haptic feedback.
1) Virtual Dog Companion Behavior: The virtual dog has
two main behavior states: following anddistracting . When
the user walks around and is not at risk of leaving the
tracking space, the dog is in follow mode. In this mode,
the dog walks slightly ahead of the user as they walk, and
remains in one spot when the user stands still.
When the user reaches a boundary of the tracked space, the
VR system initiates what is called a reset , wherein the user
reorients themself such that they face towards the inside of
the tracking space in the PE. To ensure that their orientation
in the VE is not altered, the VR system applies redirection
that effectively cancels out their physical rotation in the
virtual space. When a reset is initiated, the virtual dog enters
distract mode. In distract mode, we compute a goal position
in the VE for the dog to move towards. The idea behind
distract mode is that the user is likely to pay attention to the
virtual dog as it runs to a goal position, which allows the
system to apply stronger redirection (away from the obstacles
in the PE) without interfering with the user‚Äôs experience [25].
During a reset, the goal position is selected by Ô¨Årst
computing the vector from the user towards the center of the
physical space. The goal position is then determined to be
either the endpoint of this vector in the VE, or a virtual object
near the vector‚Äôs endpoint that was labeled as a potential
goal position during development. Potential goal positions
are virtual objects that a dog would be likely to interact with,
such as a Ô¨Åre hydrant or a lamp post. If the vector intersects
with a virtual object (e.g. a virtual building) and there are no
potential goal objects nearby, the goal position is simply the
point furthest along the vector that does not intersect with
any objects. See Figure 2 for a visualization of this process.
2) Robot Haptic Proxy Behavior: The physical robot‚Äôs
main purpose is to provide haptic feedback to make the user‚Äôs
virtual experience feel more immersive and to encourage theFig. 2. Our method of automatically choosing a suitable virtual goal position for the virtual companion. When the user gets close to a boundary of
the physical space, they need to be reoriented away from the boundary before they continue walking. In order to pick a goal destination for the virtual
companion and robotic haptic proxy, we cast a ray from the physical user to the center of the tracked space and then superimpose this vector onto the
user‚Äôs virtual position. If the endpoint of this vector is near a pre-deÔ¨Åned potential goal position, that is chosen as the current goal position. Otherwise, we
choose the furthest point along the vector that does not intersect with any objects in the virtual environment.
user to walk away from nearby objects or tracking space
boundaries in the PE. In both follow anddistract mode, the
physical robot needs to update its position such that it is
aligned with the position of the virtual dog, relative to the
user in either environment. Checking if a position update is
necessary is easily achieved by computing the vector from
the virtual user to the virtual dog and comparing it to the
vector from the user‚Äôs HMD and the robot.
To compute the trajectory that the robot will follow, we
compute a circular arc path based on the robot‚Äôs position,
forward direction, and destination position (determined by
the relative position of the virtual dog and user). The ideal
path for a differential drive robot is a circular arc since it only
requires one set of wheel velocities [6]. The wheel velocities
are computed with the ratio2rd
2r d, whereris the arc radius
anddis the distance between the robot wheels. Note that we
do not use typical PID-based drift correction due to possible
unexpected complications that may arise from the tethering
to the user [1], [29], [34].
D. Maintaining Active Haptic Guidance Constraints
This section describes how our active haptic-drive loco-
motion application satisÔ¨Åes the IH and RC constraints.
1) Directing Users With Haptic Feedback: Since the
virtual object of interest is a dog, the user is attached to the
robot by an elastic tether that resembles a leash. When the
robot moves away from the user in the PE, it simulates the
sensation of a dog tugging on its leash, thereby improving the
realism of the virtual experience. Additionally, this tugging
encourages the user to follow the robot rather than ‚ÄúÔ¨Åght‚Äù it,
allowing us to further inÔ¨Çuence the user‚Äôs movement patterns
in the PE and VE. By triggering the robot to move away from
the user and towards the center of the PE when they get too
close to the tracking space boundaries, the tugging force on
the leash encourages the user to turn and walk towards the
robot and away from the tracked space boundaries.
2) Maintaining Co-location: Normally, maintaining rela-
tive co-location between a haptic proxy and a virtual object
is a matter of updating the position of the haptic proxy when
the virtual object‚Äôs position changes. In our implementation,
we update the position of the robot to match the movements
of the virtual dog. However, our implementation requiresadditional work to maintain co-location due to a problem
that we call the haptic proxy distortion (HPD) problem.
Fig. 3. A visualization of the haptic proxy distortion problem. Left: Initially,
the virtual user and virtual companion have a particular relative position.
Middle: After rotating the virtual environment around the virtual user, the
relative position of the companion changes since the companion is rotated
along with the rest of the environment. Right: In the physical space, the
haptic proxy has not been updated, so its position coincides with the virtual
companion‚Äôs relative position before rotation (opaque robot and vector). The
new relative position of the virtual companion, which the haptic proxy needs
to match, is shown as the translucent dog and dashed-line vector.
In our implementation, we build on the RDW locomotion
interface to enable natural walking in VR. RDW works
by rotating the entire VE around the virtual camera that
represents the user‚Äôs viewpoint in VR. Consequently, the
virtual dog companion may change its position relative to
the virtual user without the dog actually moving to a new
destination in the VE (see Figure 3). As we apply redirection,
the relative position of the virtual dog changes constantly
while the relative position of the physical robot does not.
To resolve this discrepancy in relative position, we update
the robot‚Äôs goal destination in the PE on every frame to
minimize the difference in relative position. The user will
perceive this as the haptic proxy ‚Äúsliding‚Äù across the Ô¨Çoor
around them, which might result in unsmooth motion that
may detract from the user experience. In practice, this did not
seem to be a major problem for users, but we acknowledge
that there may be better solutions to the HPD problem, and
leave that for future work. This HPD problem adds onto the
errors in relative co-location between the haptic proxy and
the virtual companion, which makes it harder to satisfy the
RC constraint.V. EXPERIMENTS & RESULTS
A. Experiment Design and Procedure
To evaluate the effectiveness of our implementation of
active haptic-driven locomotion prototype, we conducted a
user study where participants completed a navigation task.
The goal of our user study was to evaluate how effective the
haptic guidance was at improving users‚Äô sense of presence
in the VE and keeping users away from the boundaries of
the VR system‚Äôs tracked space. We used a between partic-
ipants design, where one group of participants completed
a navigation task with active haptic guidance enabled, and
the other group completed the same task without any haptic
guidance. We chose a between-subjects design to avoid
potential learning effects in the navigation task and to make
the experiment duration more manageable. A total of 20
participants (13 male, 5 female, 2 participants did not report)
completed our experiment (age = 24:59,= 2:37). All
participants were able to walk unassisted when not using VR.
During the experiment, participants were placed in a
virtual city environment with several streets and blocks (see
Figure 1 and our video). Participants started the task at
one intersection in the city, and their task was to reach
a green question mark in the environment that indicated
their destination, which was one block away from the their
starting position. During the experiment, we recorded how
many times users reached the bounds of the PE and the time
taken to complete the task. Once participants Ô¨Ånished the
task, they completed a questionnaire with questions on a
7-point Likert scale that measured their sense of presence
in the VE (7 = high presence, 1 = low presence). Finally,
the experiment was ended with open-ended questions where
participants could provide additional comments. The study
lasted about 15 minutes in total.
B. Results and Discussion
The metrics we used to measure the effectiveness of our
active haptic-driven locomotion interface were the number of
breaks in presence (BiPs), the completion rate and time taken
to complete the task, and participants‚Äô subjective feelings of
presence in the VE. A BiP is incurred when the user reaches
the boundaries of the tracking space and they are forced to
reorient away from the boundary before continuing to walk.
We conducted Mann‚ÄìWhitney U tests to compare the dif-
ferences in performance metrics between the two conditions
(see Table I). The presence of our active haptic guidance
companion resulted in signiÔ¨Åcantly fewer BiPs, lower com-
pletion times and higher completion rates, and slightly higher
(and above-average) presence levels. Meanwhile, participants
who completed the navigation task without any haptic guid-
ance incurred a large number of BiPs, did not Ô¨Ånish the task
in time, and reported below-average levels of presence. Note
that although RDW is an established method for locomotion
in VR, its usefulness is limited if the PE is too small, since
there will not be enough space for the user‚Äôs physical and
virtual paths to signiÔ¨Åcantly deviate such that they can avoid
physical obstacles [36]. These results support the notion thatTABLE I
PERFORMANCE RESULTS FROM OUR USER STUDY .
BiPs Time (s) Presence Completed
Haptics      Total #
With 0.90 0.74 195.20 22.25 4.63 1.77 10
Without 18.90 5.17 309.40 65.14 3.57 1.64 1
SigniÔ¨Åcance
p-valuep<: 0001p= 0:0008p<: 0001p<: 0001
StatisticU= 0:0U= 10:0U= 4010:5 5:0
active haptic guidance can be used to help keep users safe
and feel more immersed in mixed reality experiences.
The results in Table I mainly evaluate the IH constraint
and do not provide much insight into the RC constraint. In
practice, it can be difÔ¨Åcult to accurately measure how well
the RC constraint is met, since it depends on the user‚Äôs
subjective tolerance of the mismatch between haptic and
visual stimuli as well as the particular trajectory the user
travels on. The bluetooth adapter that received movement
commands from the computer had a baud rate of 9600, so it
was capable of updating its position at the frame rate of our
VR application (90 frames per second). During the informal
questioning after the experiment, some participants reported
that they found the dog to be well-behaved and they enjoyed
the haptic feedback it provided, which suggests that the RC
constraint was adequately met.
VI. CONCLUSIONS & FUTURE WORK
In this work, we presented the active haptic guidance
problem for MR, which describes the use of one or more
robots to provide haptic feedback to users in order to create
a richer virtual experience for them, while also inÔ¨Çuencing
their behavior to improve users‚Äô safety and immersion in the
VE. We implemented active haptic guidance in a VR loco-
motion application that enabled the user to explore a large
VE while located in a much smaller PE. By combining active
haptic guidance and RDW, we increased the effective area of
the PE while also decreasing the likelihood that the user exits
the VR system‚Äôs tracked area. The concept of active haptic
guidance is general and can be applied MR applications other
than locomotion; we discuss other potential use cases for
active haptic guidance in the supplementary materials.
One limitation of our work is the haptic proxy distortion
problem, in which the haptic proxy and the associated virtual
object can become misaligned due to mismatches between
the user‚Äôs physical and virtual conÔ¨Ågurations. Solving this
problem requires continuously updating the position of the
haptic proxy, and our proposed solution in this work is likely
not the most optimal solution. Additionally, our system uses
an estimation of drift to update the haptic proxy position,
instead of a more accurate method like PID-based drift
correction. Future work in this area should study the use
of more realistic companion behaviors, and should explore
how active haptic guidance can be applied to other immersive
experiences like social MR settings with other users.REFERENCES
[1] K.-E. ÀöAarz ¬¥en, ‚ÄúA simple event-based pid controller,‚Äù IFAC Proceedings
Volumes , vol. 32, no. 2, pp. 8687‚Äì8692, 1999.
[2] B. Araujo, R. Jota, V . Perumal, J. X. Yao, K. Singh, and D. Wigdor,
‚ÄúSnake charmer: Physically enabling virtual objects,‚Äù in Proceedings
of the TEI‚Äô16: Tenth International Conference on Tangible, Embedded,
and Embodied Interaction , 2016, pp. 218‚Äì226.
[3] M. Azmandian, M. Hancock, H. Benko, E. Ofek, and A. D. Wilson,
‚ÄúHaptic retargeting: Dynamic repurposing of passive haptics for en-
hanced virtual reality experiences,‚Äù in Proceedings of the 2016 chi
conference on human factors in computing systems , 2016, pp. 1968‚Äì
1979.
[4] W. BarÔ¨Åeld and C. Hendrix, ‚ÄúThe effect of update rate on the sense of
presence within virtual environments,‚Äù Virtual Reality , vol. 1, no. 1,
pp. 3‚Äì15, 1995.
[5] H. Chen and H. Fuchs, ‚ÄúSupporting free walking in a large virtual
environment: imperceptible redirected walking with an immersive
distractor,‚Äù in Proceedings of the Computer Graphics International
Conference , 2017, pp. 1‚Äì6.
[6] H. Chitsaz, S. M. LaValle, D. J. Balkcom, and M. T. Mason, ‚ÄúMin-
imum wheel-rotation paths for differential-drive mobile robots,‚Äù The
International Journal of Robotics Research , vol. 28, no. 1, pp. 66‚Äì80,
2009.
[7] M. Di Luca, H. SeiÔ¨Å, S. Egan, and M. Gonzalez-Franco, ‚ÄúLocomotion
vault: the extra mile in analyzing vr locomotion techniques,‚Äù in
Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems , 2021, pp. 1‚Äì10.
[8] D. Feygin, M. Keehner, and R. Tendick, ‚ÄúHaptic guidance: Experi-
mental evaluation of a haptic training method for a perceptual motor
skill,‚Äù in Proceedings 10th Symposium on Haptic Interfaces for Virtual
Environment and Teleoperator Systems. HAPTICS 2002 . IEEE, 2002,
pp. 40‚Äì47.
[9] C. Hendrix and W. BarÔ¨Åeld, ‚ÄúPresence within virtual environments as
a function of visual display parameters,‚Äù Presence: Teleoperators &
Virtual Environments , vol. 5, no. 3, pp. 274‚Äì289, 1996.
[10] E. Hodgson, E. Bachmann, and D. Waller, ‚ÄúRedirected walking
to explore virtual environments: Assessing the potential for spatial
interference,‚Äù ACM Transactions on Applied Perception (TAP) , vol. 8,
no. 4, pp. 1‚Äì22, 2008.
[11] W. Hoenig, C. Milanes, L. Scaria, T. Phan, M. Bolas, and N. Ayanian,
‚ÄúMixed reality for robotics,‚Äù in 2015 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS) . IEEE, 2015, pp.
5382‚Äì5387.
[12] B. E. Insko, Passive haptics signiÔ¨Åcantly enhances virtual environ-
ments . The University of North Carolina at Chapel Hill, 2001.
[13] H. Iwata, H. Yano, H. Fukushima, and H. Noma, ‚ÄúCirculaÔ¨Çoor: A
locomotion interface using circulation of movable tiles,‚Äù in IEEE
Virtual Reality 2005 . IEEE Computer Society, 2005, pp. 223‚Äì230.
[14] H. Iwata, H. Yano, and F. Nakaizumi, ‚ÄúGait master: A versatile
locomotion interface for uneven virtual terrain,‚Äù in Proceedings IEEE
Virtual Reality 2001 . IEEE, 2001, pp. 131‚Äì137.
[15] G. Jansson and M. ¬®Ostr¬®om, ‚ÄúThe effects of co-location of visual and
haptic space on judgments of form,‚Äù in EuroHaptics . Citeseer, 2004,
pp. 516‚Äì519.
[16] J. Jerald, The VR book: Human-centered design for virtual reality .
Morgan & Claypool, 2015.
[17] R. S. Kennedy, N. E. Lane, K. S. Berbaum, and M. G. Lilienthal, ‚ÄúSim-
ulator sickness questionnaire: An enhanced method for quantifying
simulator sickness,‚Äù The international journal of aviation psychology ,
vol. 3, no. 3, pp. 203‚Äì220, 1993.
[18] L. Kohli, ‚ÄúRedirected touching,‚Äù Ph.D. dissertation, The University of
North Carolina at Chapel Hill, 2013.
[19] R. Kovacs, E. Ofek, M. Gonzalez Franco, A. F. Siu, S. Marwecki,
C. Holz, and M. Sinclair, ‚ÄúHaptic pivot: On-demand handhelds in vr,‚Äù
inProceedings of the 33rd Annual ACM Symposium on User Interface
Software and Technology , 2020, pp. 1046‚Äì1059.
[20] A. Kunz, K. Miesenberger, L. Zeng, and G. Weber, ‚ÄúVirtual navigation
environment for blind and low vision people,‚Äù in International Con-
ference on Computers Helping People with Special Needs . Springer,
2018, pp. 114‚Äì122.
[21] J. Li, Z. Yan, E. H. Jarjue, A. Shetty, and H. Peng, ‚ÄúTangiblegrid:
Tangible web layout design for blind users,‚Äù in Proceedings of the 35th
Annual ACM Symposium on User Interface Software and Technology ,
2022, pp. 1‚Äì12.[22] J. Li, Z. Yan, A. Shah, J. Lazar, and H. Peng, ‚ÄúToucha11y: Making
inaccessible public touchscreens accessible,‚Äù in Proceedings of the
2023 CHI Conference on Human Factors in Computing Systems , 2023,
pp. 1‚Äì11.
[23] V . Mercado, M. Marchal, and A. L ¬¥ecuyer, ‚ÄúEntropia: towards inÔ¨Ånite
surface haptic displays in virtual reality using encountered-type rotat-
ing props,‚Äù IEEE transactions on visualization and computer graphics ,
vol. 27, no. 3, pp. 2237‚Äì2243, 2019.
[24] R. Ocampo and M. Tavakoli, ‚ÄúVisual-haptic colocation in robotic
rehabilitation exercises using a 2d augmented-reality display,‚Äù in 2019
International Symposium on Medical Robotics (ISMR) . IEEE, 2019,
pp. 1‚Äì7.
[25] T. C. Peck, H. Fuchs, and M. C. Whitton, ‚ÄúEvaluation of reorientation
techniques and distractors for walking in large virtual environments,‚Äù
IEEE transactions on visualization and computer graphics , vol. 15,
no. 3, pp. 383‚Äì394, 2009.
[26] ‚Äî‚Äî, ‚ÄúAn evaluation of navigational ability comparing redirected free
exploration with distractors to walking-in-place and joystick locomotio
interfaces,‚Äù in 2011 IEEE Virtual Reality Conference . IEEE, 2011,
pp. 55‚Äì62.
[27] S. Razzaque, Redirected walking . The University of North Carolina
at Chapel Hill, 2005.
[28] S. Razzaque, Z. Kohn, and M. C. Whitton, ‚ÄúRedirected walking,‚Äù in
Proceedings of EUROGRAPHICS , vol. 9. Citeseer, 2001, pp. 105‚Äì
106.
[29] D. E. Rivera, M. Morari, and S. Skogestad, ‚ÄúInternal model control:
Pid controller design,‚Äù Industrial & engineering chemistry process
design and development , vol. 25, no. 1, pp. 252‚Äì265, 1986.
[30] R. A. Ruddle and S. Lessels, ‚ÄúThe beneÔ¨Åts of using a walking interface
to navigate virtual environments,‚Äù ACM Transactions on Computer-
Human Interaction (TOCHI) , vol. 16, no. 1, pp. 1‚Äì18, 2009.
[31] M. Sinclair, E. Ofek, M. Gonzalez-Franco, and C. Holz, ‚ÄúCapstan-
crunch: A haptic vr controller with user-supplied force feedback,‚Äù in
Proceedings of the 32nd annual ACM symposium on user interface
software and technology , 2019, pp. 815‚Äì829.
[32] A. F. Siu, E. J. Gonzalez, S. Yuan, J. B. Ginsberg, and S. Follmer,
‚ÄúShapeshift: 2d spatial manipulation and self-actuation of tabletop
shape displays for tangible and haptic interaction,‚Äù in Proceedings of
the 2018 CHI Conference on Human Factors in Computing Systems ,
2018, pp. 1‚Äì13.
[33] A. F. Siu, M. Sinclair, R. Kovacs, E. Ofek, C. Holz, and E. Cutrell,
‚ÄúVirtual reality without vision: A haptic and auditory white cane to
navigate complex virtual worlds,‚Äù in Proceedings of the 2020 CHI
conference on human factors in computing systems , 2020, pp. 1‚Äì13.
[34] S. Skogestad, ‚ÄúSimple analytic rules for model reduction and pid
controller tuning,‚Äù Journal of process control , vol. 13, no. 4, pp. 291‚Äì
309, 2003.
[35] M. Slater and S. Wilbur, ‚ÄúA framework for immersive virtual envi-
ronments (Ô¨Åve): Speculations on the role of presence in virtual en-
vironments,‚Äù Presence: Teleoperators & Virtual Environments , vol. 6,
no. 6, pp. 603‚Äì616, 1997.
[36] E. A. Suma, Z. Lipps, S. Finkelstein, D. M. Krum, and M. Bolas,
‚ÄúImpossible spaces: Maximizing natural walking in virtual envi-
ronments with self-overlapping architecture,‚Äù IEEE Transactions on
Visualization and Computer Graphics , vol. 18, no. 4, pp. 555‚Äì564,
2012.
[37] R. Suzuki, H. Hedayati, C. Zheng, J. L. Bohn, D. SzaÔ¨År, E. Y .-L. Do,
M. D. Gross, and D. Leithinger, ‚ÄúRoomshift: Room-scale dynamic
haptics for vr with furniture-moving swarm robots,‚Äù in Proceedings
of the 2020 CHI conference on human factors in computing systems ,
2020, pp. 1‚Äì11.
[38] M. Usoh, K. Arthur, M. C. Whitton, R. Bastos, A. Steed, M. Slater,
and F. P. Brooks Jr, ‚ÄúWalking >walking-in-place >Ô¨Çying, in virtual
environments,‚Äù in Proceedings of the 26th annual conference on
Computer graphics and interactive techniques . ACM Press/Addison-
Wesley Publishing Co., 1999, pp. 359‚Äì364.
[39] E. V onach, C. Gatterer, and H. Kaufmann, ‚ÄúVrrobot: Robot actuated
props in an inÔ¨Ånite virtual environment,‚Äù in 2017 IEEE Virtual Reality
(VR) . IEEE, 2017, pp. 74‚Äì83.
[40] R. B. Welch, T. T. Blackmon, A. Liu, B. A. Mellers, and L. W.
Stark, ‚ÄúThe effects of pictorial realism, delay of visual feedback, and
observer interactivity on the subjective sense of presence,‚Äù Presence:
Teleoperators & Virtual Environments , vol. 5, no. 3, pp. 263‚Äì273,
1996.[41] N. L. Williams and T. C. Peck, ‚ÄúEstimation of Rotation Gain Thresh-
olds Considering FOV, Gender, and Distractors,‚Äù IEEE Transactions
on Visualization and Computer Graphics , vol. 25, no. 11, pp. 3158‚Äì
3168, 2019.
[42] L. Zhang, Y . Liu, H. Bai, Q. Zou, Z. Chang, W. He, S. Wang, and
M. Billinghurst, ‚ÄúRobot-enabled tangible virtual assembly with coor-
dinated midair object placement,‚Äù Robotics and Computer-Integrated
Manufacturing , vol. 79, p. 102434, 2023.
[43] Y . Zhao, L. H. Kim, Y . Wang, M. Le Goc, and S. Follmer, ‚ÄúRobotic
assembly of haptic proxy objects for tangible interaction and virtual
reality,‚Äù in Proceedings of the 2017 ACM International Conference on
Interactive Surfaces and Spaces , 2017, pp. 82‚Äì91.
[44] Y . Zhao, C. L. Bennett, H. Benko, E. Cutrell, C. Holz, M. R.
Morris, and M. Sinclair, ‚ÄúEnabling people with visual impairments
to navigate virtual reality with a haptic and auditory cane simulation,‚Äù
inProceedings of the 2018 CHI conference on human factors in
computing systems , 2018, pp. 1‚Äì14.APPENDIX
A. Additional Experiment Design and Procedure Details
For participants who completed the task without haptic
guidance, they were instructed to stop walking and turn
around in place until an indicator of the boundary‚Äôs proximity
disappeared, at which point they were free to continue walk-
ing towards the goal. During this reorientation procedure,
redirection was applied to ensure that their virtual heading
direction was the same before and after the reorientation (to
avoid interfering with their navigation process). The naviga-
tion task had a time limit of 5 minutes and 30 seconds, after
which the experiment ended regardless of if the participant
reached the goal destination. Participants were unaware of
this time limit so that they did not rush to complete the task.
Before participants put on the HMD, we debriefed them
on the experiment procedures and had them complete a pre-
study Simulator Sickness Questionnaire (SSQ) [17]. Next,
the user put on the HMD and completed the task in the
VE. The VE was a city environment with several streets and
blocks, and was populated with common objects such as bus
stops, stores, park squares, and virtual humans that roamed
around the environment (see Figure 1 and our video). To
mask any potentially distracting noises from the robot as it
moves, participants wore headphones and background music
was played for the duration of the experiment task. Once
participants Ô¨Ånished the task, they completed another SSQ
survey in addition to the sense of presence questionnaire and
answered some open-ended questions. The study design was
approved by our university‚Äôs Institutional Review Board.
B. Additional Applications of Active Haptic Guidance
Here we discuss other potential applications of active
haptic guidance for immersive applications:
Wood Carving Application: In wood carving, the grain
of the wood will impact the direction in which the artist
carves the wood. That is, sometimes the artist will carve
‚Äúwith the grain‚Äù and sometimes will carve ‚Äúagainst the
grain.‚Äù Using active haptics, one could accurately render
the different resistance forces that arise from carving
with or against the grain of a virtual wooden block,
which will in turn inÔ¨Çuence the way in which the user
carves their virtual wooden sculpture. In addition to
providing a more realistic experience, this could be
used to guide the user to create a more appealing Ô¨Ånal
sculpture (e.g. by altering the direction of the grain to
subtly change their hand movements, which will change
the shape of the Ô¨Ånal carved surface).
Immersive Cooperative Application: A major appeals
of mixed reality experiences is the ability to connect
with other users in shared virtual experiences. Important
to these shared experiences is the ability to touch the
other person, which can provide a greater sense of
companionship and connection between users. Haptic
forces can be used to encourage users to interact with
or follow other users who are also present in their virtualexperience, which may improve the users‚Äô sense of
presence in the experience due to the enhanced realism.
Virtual Cooking Training Application: Given a seated
VR experience where the user is practicing their cook-
ing skills in a virtual environment, a mobile, tabletop
robot can provide haptic feedback that represents feed-
back provided by cooking utensils. For example, when
spreading brownie batter in a baking pan, the user will
feel haptic forces when the virtual spreading utensil gets
too close to the edges of the virtual baking pan. These
forces could be rendered using a mobile robot with a
Ô¨Çat surface that serves as a wall that the user‚Äôs physical
hand will bump into.