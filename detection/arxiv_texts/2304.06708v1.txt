Verbs in Action: Improving verb understanding in video-language models
Liliane Momeni1Mathilde Caron2Arsha Nagrani2Andrew Zisserman1Cordelia Schmid2
1Visual Geometry Group, University of Oxford, UK
2Google Research
two brown horses eating grassLLMOriginal caption
VideoAdd prompttwo brown horses running on the grasstwo brown horses ﬁghting on the grasstwo brown horses lying on the grasseating grassVerb phraseHard verb negative captionstwo brown horses sleeping on the grass
two brown horses playing on the grass
eating grasscleaning camerastandingsquatting
Verb phrase losstwo brown horses eating grass two brown horses lying on the grass a woman standing in a post oﬃceperson squatting at the gyma man cleaning his camera Hard negative loss
Generated hard negative caption
Negative captions in the batchNegative verb phrases in the batch
Batch
Figure 1. Verb-Focused Contrastive (VFC) learning : (Left): Given a video and its corresponding caption, we leverage a Large Language
Model (LLM) to output (1) hard negative captions, where only the verb has been changed while keeping the remaining context, and
(2) verb phrases which succinctly describe the action in the video. (Right): To encourage better verb reasoning, we subsequently enforce
(1) a calibrated hard negative loss, using our generated hard negative captions and other captions in the batch, and (2) a ﬁne-grained, verb
phrase loss. We show that VFC improves verb understanding of video-language models compared to the standard contrastive loss.
Abstract
Understanding verbs is crucial to modelling how peo-
ple and objects interact with each other and the environ-
ment through space and time. Recently, state-of-the-art
video-language models based on CLIP have been shown
to have limited verb understanding and to rely extensively
on nouns, restricting their performance in real-world video
applications that require action and temporal understand-
ing. In this work, we improve verb understanding for CLIP-
based video-language models by proposing a new Verb-
Focused Contrastive (VFC) framework. This consists of two
main components: (1) leveraging pretrained large language
models (LLMs) to create hard negatives for cross-modal
contrastive learning, together with a calibration strategy to
balance the occurrence of concepts in positive and nega-
tive pairs; and (2) enforcing a ﬁne-grained, verb phrase
alignment loss. Our method achieves state-of-the-art results
for zero-shot performance on three downstream tasks that
focus on verb understanding: video-text matching, video
question-answering and video classiﬁcation. To the best
of our knowledge, this is the ﬁrst work which proposes a
method to alleviate the verb understanding problem, and
does not simply highlight it.1. Introduction
Large-scale visual-language models (VLMs) such as
CLIP [57] have shown strong performance on multiple
video-language tasks such as text-to-video retrieval [43],
video question-answering, and open-set action recogni-
tion [41]. These models perform surprisingly well on these
tasks in a zero-shot setting, despite being trained only on
image-language pairs (with no access to temporal data),
even outperforming strong video-speciﬁc models [5, 86].
A recently highlighted and well-documented problem
with such models, however, is their strong noun orob-
jectbias, as evidenced by their lower performance in dis-
tinguishing between verbs in natural language descrip-
tions [30, 52, 92]. This was ﬁrst studied in images alone
by the SVO-Probes benchmark [30], which shows that im-
age-language models struggle to distinguish between dif-
ferent verbs, and often rely on the nouns instead. This
problem persists with video -language models that inherit
these VLMs, even after they are ﬁne-tuned on video-text
datasets [84, 61]. For example, Park et al. [52] similarly
propose evaluation sets with hard verb negatives, and show
that CLIP-based models, even when ﬁne-tuned on video
datasets, have difﬁculties discriminating verbs in a multi-
choice setting where the context remains unchanged. Yuk-
sekgonul et al. [92] further highlight limitations of vision-arXiv:2304.06708v1  [cs.CV]  13 Apr 2023language models at understanding attribute, relationship,
and order information. This deﬁciency in verb understand-
ing limits the model’s applicability for real-world tasks.
Verbs encapsulate how people and objects interact with each
other, and the environment, via actions in space and time.
We believe that there are two probable causes for this
deﬁciency, even after ﬁne-tuning on video-text data: (i) ex-
isting visual-text datasets have a strong bias towards single-
frame concepts such as objects andbackgrounds as well as
static actions [66, 9, 36]. Models are hence less incentivized
to understand dynamics and temporal actions [66], biasing
them towards noun understanding; and (ii) the limitations
of the cross-modal contrastive pretraining objective used by
most current vision-language models [92]. In contrastive
learning, the model is trained to distinguish correct video-
caption pairs from incorrect ones. Since it is unlikely that
existing datasets contain many examples with captions of
similar context but different verbs, the task can be solved
by taking little verb information into account. This relates
to shortcut learning in deep neural networks [26].
In an attempt to mitigate this problem, we propose a
novel training framework for tackling the task of verb un-
derstanding in vision-language models. Our framework,
called Verb-Focused Contrastive pretraining (VFC), con-
sists of two novel technical modiﬁcations to the contrastive
learning framework. We ﬁrst introduce a method to au-
tomatically generate negative sentences for training where
only the verb has changed, keeping the context the same.
This is done using LLMs [58, 22], in an automatic and scal-
able manner. Note that we generate hard negative captions,
unlike works that simply mine hard negatives from an exist-
ing paired dataset [56], or change the order of words [92].
For example, given the caption ‘ two brown horses eating
grass ’, we generate the negative caption ‘ two brown horses
running on the grass ’ (see Fig. 1). While this improves per-
formance on some downstream tasks, we ﬁnd that introduc-
ing concepts simply in negative examples can also lead to
an imbalance in the contrastive objective, favouring certain
concepts in the feature space. To solve this, we propose a
simple but effective calibration strategy to balance the oc-
currence of verbs in both positive and negative captions.
Secondly, inspired by recent works on grounding con-
cepts in vision-language learning [34, 10], we also intro-
duce a verb phrase loss that explicitly isolates the verb from
a caption for more focused training. For example, we ex-
tract the verb phrase ‘ eating grass ’ from the caption ‘ two
brown horses eating grass ’ (see Fig. 1). We ﬁnd that this
helps particularly for zero-shot performance on downstream
tasks that do not use long sentences in their evaluation [27].
Verb phrases are also extracted from sentences using LLMs.
We then train a CLIP-based model [43] on a video-
language dataset with this novel training framework. We
show that a single model trained in this way transfers wellto diverse downstream tasks that focus particularly on verb
understanding, including three video benchmarks (multiple
choice video-text matching on MSR-VTT [84], video ques-
tion answering on Next-QA [81], action recognition on Ki-
netics [11]) and one image benchmark (SVO-probes [30]),
achieving state-of-the-art performance compared to previ-
ous works in zero-shot settings (and often with ﬁne-tuning
as well); while maintaining performance on noun-focused
settings. On Kinetics, we also introduce a verb split of the
data which speciﬁcally highlights classes that are challeng-
ing to distinguish without ﬁne-grained verb understanding
(‘brushing hair ’ vs ‘ curling hair ’) and show that our model
particularly improves performance on this split.
2. Related works
LLMs for video-text tasks. LLMs have been used for vari-
ous vision applications, for example to initialise vision-text
models [65, 12, 44]. Recent works further use frozen LLMs
via prompting for tackling vision-language tasks [3, 69, 94,
88, 75, 90, 23]. LLMs have also been used in creative
ways to obtain better supervision for training for various
tasks [87, 93, 40, 96, 63]. For example, [87] use LLMs
to generate question-answer pairs from transcribed video
narrations, while [93] use LLMs to rephrase questions into
sentences. [40] use LLMs to match noisy speech transcrip-
tions to step descriptions of procedural activities. [50] train
BERT [16] to predict action labels from transcribed speech
segments and use this to scale up training data for action
classiﬁcation. [96] use pretrained LLMs conditioned on
video to create automatic narrations. Recent works [96, 63]
also show the beneﬁts of using LLMs to paraphrase cap-
tions for data augmentation for video-language pretraining.
[38] use LLMs to generate negative captions by manipulat-
ing event structures. Our work differs to [38] in that we
focus speciﬁcally on verb negatives, and videos instead of
images. Most closely related to our work, [52] construct a
test set for verb understanding by leveraging T5 [58] and
highlight the poor performance of current video-language
models. Our work is substantially different: (i) we auto-
matically construct hard negative captions for training (not
testing), (ii) we compare the use of different LLMs, (iii) we
show that training with such negative captions can improve
verb understanding on various verb-focused benchmarks.
Hard negatives for contrastive pretraining. Hard nega-
tives have been used to improve performance in metric rep-
resentation learning and contrastive learning [33, 29, 79].
Recent works mine hard negatives from an existing paired
dataset [56, 83, 89]. In comparison, in our work, we gen-
erate hard negative captions and propose a careful calibra-
tion mechanism for training effectively with such unpaired
data. We also verify here the beneﬁt of the HardNeg-NCE
loss [56] when training with generated hard negative cap-
tions. [92] construct hard negative captions by shufﬂingwords from the original caption to improve order and com-
positionality understanding. Our work differs by (i) focus-
ing speciﬁcally on verb reasoning, as opposed to object-
attribute relationships, (ii) using LLMs to construct hard
verb text negatives as opposed to perturbing the word or-
der, (iii) focusing on video -language models.
Learning from parts-of-speech in video. Recent works
use parts-of-speech (PoS) tags for video understanding [62,
78, 24, 27, 85]. [78] learn multi-label verb-only represen-
tations, while other works focus on learning adverb repre-
sentations [20, 21]. [2] use verb-noun pairs for unsuper-
vised learning with instructional videos, while [24] lever-
age such pairs to generate data augmentations in the feature
space. Other works exploit PoS for ﬁne-grained or hier-
archical alignment between video and text [95, 14]. [77]
learn a separate multi-modal embedding space for each PoS
tag and then combine these embeddings for ﬁne-grained ac-
tion retrieval. [14] construct a hierarchical semantic graph
and use graph reasoning for local-global alignments. Most
closely related to our work, [89] use a PoS based token
contrastive loss. Our work differs in that: (i) we apply a
verb phrase contrastive loss, as opposed to separate verb and
noun losses; (ii) we extract verb phrases using a LLM and
show this performs better than PoS tagging with NLTK [8]
(Tab. 5); (iii) we evaluate our methods on verb-focused
downstream tasks. Similarly to [27], we ﬁnd that train-
ing with verb phrase supervision helps for zero-shot per-
formance on tasks with shorter sentences.
Temporal understanding in videos. A long term goal in
computer vision is temporal understanding in videos [11,
28, 17, 64, 80, 97, 67]. However, current training and
test datasets have a strong visual bias towards objects and
backgrounds as well as static actions [66, 31], with some
works [9, 36] demonstrating strong results with a single
frame. Despite these challenges, many recent works in
video-only self-supervised learning propose pretext tasks
for improving temporal modelling [35, 1, 53, 76, 55, 7, 72,
91, 18, 46, 39, 71, 6, 59, 15]. Unlike these works that use
only video, [68, 10] focus on ﬁne-grained temporal video-
text alignment via localization of text sub-tokens. [4] also
leverage before/after relations in captions to create artiﬁcal
training samples for video-text. Differently to these works
(which create augmented video negatives or positives), we
approach the problem of improving verb understanding in
video-language models from the language side, by leverg-
ing the strong generalization capabilities of LLMs.
3. Method
Our goal is to adapt large-scale vision-language pre-
trained models (such as CLIP) to understand verbs . We
aim to do this without requiring such models to be retrained
from scratch, but by simply ﬁne-tuning them on a video-
language dataset. However, given the pitfalls with using thestandard video-text contrastive setup [57] on existing video-
language datasets, we propose a new framework which we
callVerb-Focused Contrastive pretraining (VFC). It con-
sists of two components, both using the power of LLMs:
(i) a novel calibrated hard negative training method where
we train with synthetic verb-focused hard negative captions,
and (ii) an additional verb phrase loss where videos are con-
trasted against isolated verb phrases as opposed to the entire
caption. Note that a ‘verb phrase’ can be a single verb or
verb-noun pair depending on the caption (see Fig. 1).
3.1. Preliminaries
Large Language Models (LLMs) are generative text mod-
els with impressive capacities, in particular for few-shot
or prompt-based learning [22]. In our work, we design
prompts to instruct a LLM to (i) create verb-focused hard
negative captions and (ii) isolate verb phrases from the cap-
tions of a dataset. LLMs allow scalability and generali-
sation, and as we show in the ablations (see Tab. 2 and
Tab. 5), are preferable to manual or rule based methods
(eg. NLTK [8]). In particular, we use PaLM [22], a state-of-
art autoregressive model, throughout this paper. However,
our framework is agnostic to this choice and other LLMs
can be used instead (see Tab. 2).
Video-language contrastive pretraining works by learn-
ing to distinguish between aligned and non-aligned video-
text pairs. Given a dataset of Npairsf(Vi;Ti)gi2Nwith
videoViand caption text Ti, we extract normalised feature
representations viandtiby using a video encoder fand text
encoderg: we havevi=f(Vi)andti=g(Ti). We use the
InfoNCE loss [70] to make aligned (‘positive’) pairs close
in feature space and all other pairwise combinations in the
batch further apart [57]. We optimize for video-to-text Lv2t
and text-to-video Lt2valignments:
Lt2v
i= t>
ivi=+ logBX
j=1exp(t>
ivj=) (1)
whereBis the batch size and a temperature parameter
controlling the sharpness of the distribution. Lv2tis ob-
tained by inverting vandtin Eq. 1.
Adapting image-text models to videos. We lever-
age CLIP [57] for video-language tasks following the
CLIP4CLIP ‘seqTrans’ protocol [43]. Both single-modal
encoders (video fand textg) are initialized with CLIP
weights, with four additional temporal frame aggregation
transformer blocks stacked on top of the image encoder
(see [43] for more details). Our approach is agnostic
to model architecture and so any state-of-the-art video-
language architecture could be potentially used.
3.2. Verb-Focused Contrastive Pretraining (VFC)
We describe both our calibrated hard negative training
(Sec. 3.2.1) and the proposed verb phrase loss (Sec. 3.2.2).it’s a video of a bald monk sitting at a temple looking at his laptop it’s a video of a bald monk lying at a temple looking at his laptop   it’s a video of a bald monk standing at a temple looking at his laptop   it’s a video of a bald monk dancing around a temple holding his laptop   it’s a video of a bald monk jumping up at a temple closing his laptop it’s a video of a bald monk running in a temple searching for his laptop  
man is arguing with another man in the dark man is kissing another man in the dark man is talking to another man in the daylight man is kicking another man in the light man is hugging another man in the darkman is punching another man in the dark
a person draws a dragona person carves a dragon a person paints a dragon a person doodles a dragon a person sculpts a dragon a person destroys a dragona girl skateboarding in a public placea girl dancing in a public place a girl running in a public place a girl singing in a public place a girl sitting on her skateboard in a public place a girl falling oﬀ her skateboard in a public place
Figure 2. Qualitative examples of hard negatives generated by PaLM. We show a single frame per video and the corresponding caption
in bold, with the verb highlighted in green. We see that PaLM can effectively generate hard negatives where the verb has changed (changes
in red). When there are several verbs in the caption (see top left), PaLM may replace one or all verbs. As a failure case (bottom right),
we show an example where PaLM can change more than just the verb, which could make it an easier negative (replacing ‘punching’ by
‘talking’ but also ‘dark’ by ‘daylight’).
3.2.1 Calibrated Hard Negative training
In regular contrastive learning, given a video-caption pair,
other captions in the batch are simply pushed further in the
feature space. Since it is unlikely that existing datasets con-
tain many examples with captions of similar context but dif-
ferent verbs , the task can be solved by paying little atten-
tion to verbs. Instead, our goal is to encourage the video-
language model to focus on verb reasoning. We do so by
tasking a LLM to generate hard negative captions where
only the verb(s) in the captions change. Second, we train
with these additional negative captions. We ﬁnd that naive
training with additional data leads to imbalances affecting
the resulting video-text feature space. We propose a simple
but effective calibration mechanism to solve this.
Generating verb-focused hard negatives with PaLM.
Given a caption Ti, we task PaLM to replace the verbs with
other verbs that convey a different action, but still form a
linguistically and semantically viable sentence (which may
not be guaranteed with random verb replacements – see
qualitative examples in Sec. B.4 of the appendix). For ex-
ample, in the caption ‘ a man washes his face ’, the verb
‘washes ’ should not be replaced with ‘ jumps ’ or ‘ plays ’.
The generated caption is then a negative match for the cor-
responding video Vi(albeit a hard negative, as the nouns
and context remain the same). We experiment with different
handcrafted prompts, and ﬁnd our best performing prompt
to be the following: ‘In this task, you are given an input sen-
tence. Your job is to tell me 10 output sentences with a dif-
ferent meaning by only changing the action verbs’ . We also
add four input-output pair examples to the prompt, which
increases the quality of PaLM’s predictions (see Sec. A.3.2
of the appendix). We use one PaLM forward pass per cap-
tionTito generate ten verb-focused hard negatives for that
caption (qualitative examples of the generated captions can
be seen in Fig. 2). During training, we randomly sample
Nhardgenerated captions for each pair (Vi;Ti)in the mini-batch, which we denote 
Thard
ik
k2[1;Nhard]. Importantly, note
that aThard
ikis a new generated text caption, or an unpaired
data sample, meaning that it does not come with a corre-
sponding matching (‘positive’) video.
Calibration. Interestingly, we observe that naively adding
in negative captions into training with a contrastive loss
leads to harmful feature space distortions, as some concepts
are only seen in negative captions but never in positives.
This is observed by careful analysis of downstream perfor-
mance (see study in Tab. 3 and Tab. 4). We hence next de-
scribe a calibration mechanism to avoid such distortions:
we ﬁrst denote the vocabulary of all verb phrases in the orig-
inal and generated captions as 
. For each verb phrase !(or
‘concept’) in 
, we useS!to represent the number of times
it appears in the captions of the original dataset and G!for
the number of times it appears in the PaLM-generated cap-
tions. We then derive equations for R!(see Tab. 1), which
we deﬁne as the ratio of the number of times a verb phrase !
is used as a negative versus as a positive during training, for
different choices of the video-to-text contrastive loss (note
Lt2vis unchanged).
Contrastive training with paired data (Baseline). We
ﬁrst note that the ratioR!is independent of the verb phrase
!in regular contrastive learning (paired data only). It sim-
ply depends on the batch size B, asSwis cancelled from
both the numerator and denominator. This means that the
number of times a concept is used as a positive versus neg-
ative sample is the same regardless of the considered verb
phrase. This naturally balances training, and is a great prop-
erty of the contrastive framework.
Adding generated unpaired negative captions (HN).
However, when training with unpaired captions, this ratio
is proportional to G!=S!and therefore becomes depen-
dent on the considered verb phrase !. This can have sig-
niﬁcant consequences for the video-text feature representa-
tions. The model can learn to either ignore or always predictName Video-to-text alignment loss R!
Baseline  v>
iti=+ logPB
j=1exp(v>
itj=)(B 1)S!
S!? ?!
HN v>
iti=+ logPB
j=1exp(v>
itj=) +PB
j=1PNhard
k=1exp(v>
ithard
jk=)
(B 1)S!+BG!
S!/BG!
S!
Calibrated HN  v>
iti=+ logPB
j=1exp(v>
itj=) +PNhard
k=1exp(v>
ithard
ik=)
(B 1)S!+G!
S!/G!
S!withG!tS!
Table 1. Different choices for video-to-text alignment when training with additional hard negatives (HN). R!is the ratio of the number of
times a given verb phrase !is used as a negative versus the number of times it is used as a positive. We note that for the regular contrastive
loss ( Baseline ),R!only depends on the batch size B, however when training with generated hard negatives ( HN), it depends on the verb
phrase!. We minimise this effect using our proposed Calibrated HN loss, which we denote as LCHN
i. See details in Section 3.2.1.
some concepts based on the average concept occurrences in
positive or negative pairs during training.
Hard negatives with calibration (Calibrated HN). In or-
der to make R!as!-agnostic as possible, we introduce an
ensemble of two techniques which we refer to as ‘calibra-
tion’. First, we ignore the hard negative captions from the
other elements of the batch (see row 3 in Tab. 1), which
allows us to mitigate the inﬂuence of G!=S!by not am-
plifying it by the batch size B(equal to 256). Second, we
ﬁlter the generated PaLM captions to have G!tS!. In
practice, we discard some generations so that the number of
times a verb phrase appears in the set of kept generations is
equal to the number of times it is originally present in the
dataset. We denote our video-to-text loss (text-to-video is
unchanged) as LCHN
ifor calibrated hard negative training.
Video mining. An alternative to avoid imbalances due to
the addition of negative captions would be to avoid training
with unpaired data at all, by mining a matching video Vhard
ik
for each generated caption Thard
ik. We attempt this via CLIP-
based text-to-video retrieval in a large video database but
found that ﬁnding a video matching a detailed, long caption
is challenging, as such a precise video may not exist in a
given corpus (see Sec. A.3.1 in the appendix for examples).
3.2.2 The verb phrase loss
In order to further encourage our model to focus on verbs,
we introduce a contrastive ‘verb phrase’ loss. We use PaLM
to extract the verb phrase Tverb
iin a caption Tiwith the fol-
lowing prompt: ‘ In this task, you are given an input sen-
tence. Your job is to output the action verb phrases. ’ While
multiple parts-of-speech (PoS) tagging tools exist, we use a
LLM for the following reasons: (i) we would like to isolate
verb phrases, which may correspond to single verbs or verb-
noun pairs depending on the caption , (ii) LLMs deal better
with ambiguous cases (see qualitative examples in Sec. B.5
of the appendix). We show the beneﬁts experimentally via
an ablation in Tab. 5. During training, we minimize the fol-
lowing loss:
Lverb-phrase
i = v>
itverb
i=+ logBX
j=1exp(v>
itverb
j=)
where the negative verb phrase representations tverb
jsim-
ply come from other captions in the batch. Note that wedo not require the calibration mechanism described in Sec-
tion 3.2.1 since all verb phrases Tverb
ihave a positive video
matchVi(i.e. the video aligned with Ti).
Overall, our verb-focused contrastive (VFC) pretraining op-
timizes the sum of three objectives:
LVFC=1
BBX
i=1
1Lt2v
i+2LCHN
i+3Lverb-phrase
i
with parameters 1,2and3weighting the contribution
of the different terms. We learn the parameters of fandg
via back-propagation.
3.3. Implementation details
Spoken Moments in Time (SMiT) pretraining dataset.
The SMiT [48] training set consists of 481K pairs of 3 sec-
onds video clips with corresponding captions. It is a sub-
set of Moments in Time (MiT) [47]. Our work falls under
the umbrella of transfer learning: we pretrain on SMiT and
then use the resulting features to solve different downstream
tasks in a zero-shot or ﬁne-tuned manner. Pretraining is ei-
ther done as in regular contrastive learning (‘baseline’) or
with our VFC framework. We ﬁnd that the baseline already
performs competitively on our benchmarks, despite the rel-
atively small size of SMiT compared to other datasets such
as HowTo100M [45], due to the quality and diversity of the
manually annotated captions. We encourage the community
to consider SMiT as a powerful pretraining dataset.
PaLM. We use PaLM-540B [22] with beam size 4, output
sequence length 512, and temperature of 0.7. The negative
captions are generated in an autogressive way and are there-
fore of arbitrary length. We post-process them by removing
text after any newline character and by ﬁltering out candi-
dates which contain the same verbs as the original caption.
Training details. Most hyper-parameters follow
CLIP4CLIP [43]. We initialise our model with CLIP
ViT/B-32 and train with VFC for 100 epochs with a batch
size of 256, base learning rate of 1e-7, weight decay of 1e-
2, temperature of 5e-3 and weights 1= 2,2=3= 1
which we empirically ﬁnd to work well in our experiments.
Indeed, this balances the video-to-text and text-to-video
loss terms. We also normalise each loss term by its value
obtained from a random uniform prediction in order to
have all loss terms in the same range (loss always equal to
1 for a random uniform prediction). We sample 32 framesMethod Hard negatives Verb H K-400
Baseline ? 69.9 55.6
w/o LLM
Random verb 73.6 (+3.7) 55.0 (-0.6)
Antonym verb 72.4 (+2.5) 55.4 (-0.2)
w/ LLM
T5 [58] 75.1 (+5.2) 55.8 (+0.2)
Ours PaLM [22] 78.0 (+8.1) 55.8 (+0.2)
Table 2. Hard negatives generation. We explore both LLM
based and non LLM-based methods to obtain hard negative cap-
tions. Although PaLM LLM captions achieve the best perfor-
mance, other LLMs (T5) achieve good results too. All methods
are evaluated with calibration.
per video at 25fps, with a 2 frame stride. See Sec. C in the
appendix for further implementation details and extensive
evaluation protocols.
4. Experiments
We curate a suite of benchmarks from existing works to
evaluate verb understanding which we present in Sec. 4.1.
Then we ablate various components of our VFC framework
in Sec. 4.2. Finally, we demonstrate improved performance
on our diverse set of downstream tasks in Sec. 4.3, and com-
pare to the state of the art.
4.1. Verb-Focused Benchmarks
MSR-VTT multiple choice (MC) is a benchmark of 10K
videos of length 10–30 secs. We evaluate on the standard 3k
split and on Verb Hfrom [52]. In this setting, the task is to
associate each video to the right caption among ﬁve choices.
While the four wrong captions are randomly chosen from
other videos in the standard 3k split, one of them is replaced
by a hard verb negative in Verb H[52].
Video question answering on NEXT-QA The train (resp.
val) split contains 3870 (resp. 570) videos with 32K (resp.
5k) questions. There are three types of questions: causal
(C), temporal (T) and descriptive (D). We consider the stan-
dard setting as well as ATP hard [9], a subset automatically
constructed with questions that are non-trivially solved with
a single frame. ATP hard is designed to be a better bench-
mark for the model’s true causal and temporal understand-
ing which we believe is strongly related to verb reasoning.
Kinetics-400 is a video classiﬁcation dataset with 400 hu-
man action classes. We report top-1, top-5 and their average
classiﬁcation accuracy. We follow [57] to evaluate classiﬁ-
cation in an open-set, zero-shot manner. This benchmark al-
lows to assess transfer ability to action classiﬁcation, which
requires strong verb understanding (given actions are usu-
ally described with verb phrases).
SVO-probes dataset is a benchmark speciﬁcally designed
to measure progress in verb understanding of image-textMethod R! # HN Verb H K-400
Baseline ? ?! 0 69.9 55.6
w/o calibration /BG!
S!8.7M 80.5 (+10.6) 54.5 (-1.1)
w/ calibration/G!
S!,G!tS!0.9M 78.0 (+ 8.1) 55.8 (+0.2)
Table 3. Importance of the calibration mechanism when train-
ing with hard negative captions. The model trained without cal-
ibration suffers from a drop of performance on Kinetics.
w/o calibration w/ calibration
R!/
3712785327
braiding hairbrushing haircurling hairdying hairfixing hairbraiding hairbrushing haircurling hairdying hairfixing hair3814115251002133310501213646231190102030405060
11111
braiding hairbrushing haircurling hairdying hairfixing hairbraiding hairbrushing haircurling hairdying hairfixing hair47252744131747553311445661573120102030405060
braiding hair
brushing hair
curling hair
dying hair
ﬁxing hair
Table 4. Confusion matrix for the hair classes on Kinetics.
Without proper calibration, the verb phrase ‘brushing hair’ be-
comes highly attractive in the video-text feature space. This de-
teriorates the performance on all the ‘hair’ related classes. Our
calibration mechanism alleviates this issue by making the ratio R!
independent of verb phrases (see details in Sec. 3.2.1). More ex-
amples are shown in Sec. B.3 of the appendix.
models [30]. It contains image–caption pairs with 421 dif-
ferent verbs. We simply replicate the image multiple times
as input to our video model. We report Average Precision
(AP) on the entire dataset as well as the verb-focused set-
ting (details about our evaluation protocol are provided in
Sec. C.4 of the appendix).
4.2. Ablation Study
In this section, we analyze our different design choices.
We report results when transferring the models on two
of our benchmarks: MSR-VTT multi-choice verb split
(‘Verb H’) and Kinetics-400 video classiﬁcation (‘K-400’).
We chose these two benchmarks as they have very differ-
ent properties: the ﬁrst involves captions, while the second
involves action labels. We note that Nhard= 1for all abla-
tions unless otherwise speciﬁed.
Hard negative captions generation. In Tab. 2, we ablate
the technique used to obtain additional negative captions:
we compare two LLMs (T5 [58] and PaLM [22]) and two
non LLM-based methods: (i) ‘random verb’: we replace
verbs by random verbs from the UPenn XTag1verb cor-
pus and (ii) ‘antonym verb’: we replace verbs with their
antonyms, using the NLTK [8] package. We see in Tab. 2
1https://www.cis.upenn.edu/ ˜xtag/PaLM captions: Verb HK-400
? 69.9 55.6
Positive 69.3 55.4
Negative 78.0 55.8Verb isolation: Verb HK-400
? 69.9 55.6
MiT labels 69.9 57.0
NLTK [8] 70.1 56.4
PaLM [22] 70.3 57.6
Table 5. (left): Generating negative versus positive captions
with PaLM. (right): Verb phrase isolation methods.
that ‘random verb’ and ‘antonym verb’ already give mod-
erate performance gains on Verb Hcompared to the base-
line. However, using LLM-based generations improves the
results by a large margin compared to the non LLM-based
methods. This is likely due to the fact that (i) random or
antonym replacements often create non semantically or lin-
guistically plausible negative captions; (ii) some verbs do
not have antonyms in NLTK (see qualitative examples in
Sec. B.4 of the appendix). Finally, we see in Tab. 2 that
T5 generations work very well in our framework too, which
demonstrates that our framework is LLM-agnostic and can
be extended to other LLMs. We observe that the best perfor-
mance is achieved using PaLM, with a substantial gain over
the baseline on MSR multi-choice (+8.1%) and a moderate
gain on Kinetics (+0.2%).
Hard negative captions: the importance of calibration.
We demonstrate the effect of the calibration mechanism de-
scribed in Section 3.2.1 for training with unpaired captions.
Tab. 3 shows the performance of hard negative training with
(‘w/’) versus without (‘w/o’) calibration. First, we observe
that the performance boost on MSR-VTT compared to the
baseline is slightly stronger without calibration than with
calibration. We believe this is because calibrating the PaLM
generations reduces their number. However, we see that
training with hard negatives without calibration deteriorates
a lot the performance on Kinetics (  2:0%compared to the
baseline). We hypothesize that this is due to some verb
phrases being seen only as repulsive in the video-text fea-
ture space, while others are seen equally as attractive and
repulsive. We illustrate this in Tab. 4 by showing the confu-
sion matrix for a subset of the Kinetics classes, along with
the ratioR!(deﬁned in Sec. 3.2.1) for each verb phrase.
Intuitively,R!measures the ‘attraction’ (if low) and ‘repul-
sion’ (if high) of a verb phrase !. The confusion matrix in
Tab. 4 shows that the verb phrase ‘brushing hair’ becomes
an attraction point in the absence of calibration. Indeed, the
number of times the verb phrase ‘brushing hair’ is repul-
sive versus attractive is low ( Rbrushing hair t12) compared
to the other concepts such as for example ‘curling hair’
(Rcurling hair t78): we have Rbrushing hair<< R curling hair .
Hence, predictions for ‘brushing hair’ become dominant.
This actually improves the performance for that class but
deteriorates the performance on all the other classes related
to ‘hair’. We see in Tab. 4 that our calibration mechanism
alleviates this effect by making the ratio R!independent ofMethod Hard negatives Verb phrase Verb H K-400
Baseline 69.9 55.6
X 78.0 (+8.1) 55.8 (+0.2)
X 70.3 (+0.4) 57.6 (+2.0)
VFC (Ours) X X 76.3 (+6.4) 58.5 (+2.9)
Table 6. Combining hard negative and verb phrase loss
achieves 9.2% relative improvement on MSR-VTT MC (accuracy)
and 5.2% relative improvement on Kinetics (top-1) compared to
the baseline.
!as in regular contrastive learning. Calibration allows us to
improve performance over the baselines on both tasks with
a single model.
Generating positive versus negative captions. In Tab. 5
(left), we investigate the impact of generating positive cap-
tions instead of negatives with PaLM. In this case, positives
correspond to sentences where the verb in the original cap-
tion is changed to a synonym verb, but the remaining con-
text is unchanged: PaLM therefore acts as a data augmenta-
tion generator for text (similar to [96, 63]). Details about the
positive caption generation implementation are in Sec. C.5
of the appendix. We observe that using positive captions has
a negative impact on the performance in our benchmarks,
possibly because with positive captions the model becomes
more invariant to different verbs.
Verb phrase loss. In Tab. 5 (right), we explore two alter-
natives for verb phrase extraction used in the verb phrase
loss: (i) using human-annotated action labels for clips from
the Moments in Time (MiT) dataset (these are available as
SMiT data inherits from MiT [47]) and (ii) using a rule-
based method (NLTK [8]) to isolate verbs. We observe in
Tab. 5 that using PaLM to extract verb phrases from the
caption outperforms both, probably because it extracts more
ﬁne-grained action information. Qualitative analysis of the
verb phrases is shown in Sec. B.5 of the appendix.
Combining calibrated hard negatives and verb phrase
loss. We show in Tab. 6 the complementarity between our
two contributions: the calibrated hard negative training and
the verb phrase loss. The former greatly improves perfor-
mance on tasks requiring complex language understanding
such as MC Verb H. On the other hand, the verb phrase loss
improves transfer to video classiﬁcation by focusing partic-
ularly on the action label in the sentence. We see in Tab. 6
that combining both approaches during training results in
asingle model with excellent performance on both MSR-
VTT MC and Kinetics zero-shot transfer. Indeed, compared
to the baseline, VFC pretraining achieves 9.2% relative im-
provement on MSR-VTT MC and 5.2% relative improve-
ment on Kinetics.
Number of hard negative captions. In Tab. 7, we experi-
ment with increasing the maximum number of hard negative
captionsNhardsampled per video in the batch. We ﬁnd that
setting this to 5 increases the performance on Verb HwhileMethod NhardVerb H K-400
VFC (Ours) 1 76.3 58.5
VFC (Ours) 3 77.8 58.5
VFC (Ours) 5 78.3 58.5
Table 7. Maximum number of hard negative captions. We ob-
serve that increasing the maximum number of hard negative cap-
tions sampled per video increases the performance on Verb H. We
useNhard= 5in the remaining of the paper.
Method Contrastive loss Verb H K-400
Baseline NCE 69.9 55.6
Baseline HardNeg-NCE 72.0 56.4
VFC (Ours) NCE 78.3 58.5
VFC (Ours) HardNeg-NCE 80.5 58.8
Table 8. Complementarity with other hard negative min-
ing methods. We observe that using the HardNeg-NCE loss,
instead of standard NCE, gives the highest performance. We
use HardNeg-NCE from now on. We note that for VFC we use
Nhard= 5.
maintaining the performance on Kinetics. We use this set-
ting going forward. We note that we do not try larger values
as our maximum number of hard negatives per video after
calibration is 5.
Complementarity with other hard negative mining
methods. We investigate whether our VFC framework
is complementary to existing approaches for hard nega-
tives with the contrastive learning framework. Speciﬁcally,
we reimplement the hard negative noise contrastive multi-
modal alignment loss from [56, 60], which is denoted as
HardNeg-NCE. With this objective, difﬁcult negative pairs
(with higher similarity) are emphasised, and easier pairs are
ignored. We use = 1 and= 0:1in the equations
from [56]. We note that we only adapt Lt2v
iandLCHN
iwith
HardNeg-NCE. Adapting Lverb-phrase
i does not bring further
improvements, so we omit this for simplicity. We observe in
Tab. 8 that VFC is complementary to existing hard negative
frameworks: using HardNeg-NCE instead of the standard
NCE loss achieves the highest performance. We observe
a large boost on Verb H[52], a benchmark that speciﬁcally
involves hard negatives. We therefore adopt HardNeg-NCE
in the remaining of this paper.
4.3. Comparisons to the State of the Art
We compare our VFC features to the state of the art on a
diverse set of tasks requiring verb understanding. Note that
we use the same model across different tasks, which is non-
trivial in itself as the tasks cover a wide range of domains
and evaluation protocols.
MSR-VTT MC results. We see in Tab. 9 that our verb-
focused pretraining transfers well to the MSR-VTT multi-
choice task, especially on the hard verb split (curated to as-
sess exactly the task we are trying to solve). We even out-Model # params. 3k val. Verb H[52]
ZERO-SHOT
VideoCLIP [83] – 73.9 -
CLIP [57] 151M 91.1 64.1
InternVideo [74] t460M 93.4 -
VFC (Ours) 164M 95.1 80.5
FINE-TUNED
ClipBERT [37] – 88.2 -
MMT [25] – 92.4 71.3
VideoCLIP [83] – 92.1 -
CLIP-straight [54] 151M 94.1 65.1
MMT [25] (CLIP features) – 95.0 71.4
C4CL-mP [52] 151M 96.2 73.7
VFC (Ours) 164M 96.2 85.2
Table 9. Multi-choice MSR-VTT. We report accuracy on the
3k val and on the verb-focused Verb H[52] splits. While VFC
improves the performance on both splits in a zero-shot setting, the
gap with previous works is especially important on Verb H[52],
a split measuring verb understanding. When available, we add
model parameter counts.
ATPhard [9]
Model all D T C all T C
ZERO-SHOT
CLIP [57] 43.9 57.0 38.1 43.6 23.0 21.8 23.8
VFC (Ours) 51.5 64.1 45.4 51.6 31.4 30.0 32.2
FINE-TUNED
HGAz[32] 49.7 59.3 50.7 46.3 44.1 45.3 43.3
ATP [9] 49.2 58.9 46.7 48.3 20.8 22.6 19.6
Temp[ATP] [9] 51.5 65.0 49.3 48.6 37.6 36.5 38.4
TAATPy[82] 54.3 66.8 50.2 53.1 - - -
VGT [82] 55.0 64.1 55.1 52.3 - - -
VFC (Ours) 58.6 72.8 53.3 57.6 39.3 38.3 39.9
Table 10. NEXT-QA video question answering. We report accu-
racy. We consider either ‘all’ questions or only causal (‘C’), tem-
poral (‘T’) or descriptive (‘D’) questions. We also use ATP hard
split [9]. VFC improves performance for both zero-shot and ﬁne-
tuning.yTemp[ATP]+ATP.zUses additional motion features.
perform concurrent InternVideo [74] while using a signiﬁ-
cantly smaller setting both in terms of architecture (Intern-
Video uses 2.8more parameters and 12.4 more ﬂops)
and pretraining dataset size (they use 24 more data). We
also note that our method does not degrade performance on
other standard object-based tasks, such as text-to-video re-
trieval on MSR-VTT (results compared to the state of the
art are shown in Sec. A.2 of the appendix).
NEXT-QA results. We show in Tab. 10 that our verb-
focused pretraining gives a signiﬁcant boost in both the
standard and ATP hard setting introduced by [9]. To the
best of our knowledge, we are the ﬁrst work to report zero-
shot results for NEXT-QA and our zero-shot numbers im-
prove upon some previously published ﬁne-tuning numbers.Model # param. top-1 top-5 average
VAL-SET
CLIP [57] 151M 48.9 75.8 62.4
ActionCLIP [73] t164M 56.4 - -
VFC (Ours) 164M 59.4 85.3 72.4
TEST-SET
Flamingo-3B [3] 3B 45.2 66.8 56.0
Flamingo-80B [3] 80B 49.1 71.5 60.3
Flamingo-9B [3] 9B 49.7 71.5 60.6
CLIP [57] 151M 47.9 75.1 61.5
VFC (Ours) 164M 58.8 84.5 71.7
Table 11. Zero-shot transfer to Kinetics-400. We report top-1
accuracy, top-5 accuracy, and their average on the validation and
test set, as well as the parameter counts of the different models.
Model top-1 top-5
ZERO-SHOT
CLIP [57] 59.7 83.9
VFC (Ours) 70.2 92.5
FINE-TUNED
ER-ZSAR [13] 42.1 73.1
X-CLIP [51] 65.2 86.1
X-Florence [51] 68.8 88.4
Table 12. Zero-shot transfer to Kinetics-600. We report av-
erage top-1 and top-5 accuracies over three random 160-class
splits, covering classes not in Kinetics-400 but within Kinetics-
600. While [13, 51] ﬁne-tune on Kinetics-400, we surpass their
performance in zero-shot.
Finally, although HGA [32] performs worse than ours on
the standard setting, it achieves a high accuracy of 44.1 on
ATPhard. Their high performance on ATP hard can be ex-
plained by the use of additional motion features, aiding in
answering hard dynamics questions, as noted by [9]. The
addition of extra motion features on the video side can be
complementary to our verb-focused pretraining approach.
Zero-shot Kinetics-400 results. In Tab. 11 we see that our
verb-focused features transfer very well to Kinetics video
classiﬁcation benchmark in a zero-shot setting, achiev-
ing state-of-the-art results. We achieve better results than
Flamingo models [3] while using a signiﬁcantly smaller
model: relative improvement of 20% over Flamingo-80B
model while using 489 less parameters.
Zero-shot Kinetics-600 results. We evaluate our model on
Kinetics-600 in Tab. 12 and follow the protocol in [51, 13].
Speciﬁcally, the subset of categories which are outside
Kinetics-400, but within Kinetics-600 are used for evalu-
ation. The evaluation is then run on a random sample of
160 categories from this subset. The ﬁnal performance is
averaged over three iterations. We observe that by evaluat-
ing our model in a zero-shot setting, we surpass the perfor-
mance of works [13, 51] which ﬁne-tune on Kinetics-400.Method all Kinetics-verb
Baseline 55.6 52.1
VFC (Ours) 58.8 (+3.2) 57.1 (+5.0)
Table 13. Zero-shot Kinetics-verb. We report accuracy perfor-
mance on our newly proposed Kinetics-verb split (from test split).
Model AP AP verb
CLIP [57] 48.3 52.3
No-MRM-MMT [30] y 51.5 53.1
Baseline (Ours) 60.2 61.9
VFC (Ours) 61.8 64.6
Table 14. Verb understanding on SVO-probes [30]. We re-
port Average Precision (AP) on the entire dataset and on the verb-
specialized setting. yScores provided by authors and used to cal-
culate AP.
Kinetics-verb. To further analyse the VFC framework’s ef-
fect on action classiﬁcation, we introduce the Kinetics-verb
split. We isolate classes from the Kinetics-400 dataset that
share a common noun with another class, but have a differ-
ent verb (and therefore action). For example, distinguising
between ‘braiding hair’, ‘brushing hair’ and ‘curling hair’
requires the model to focus on verb understanding as pre-
dictions cannot be inferred from the simple presence of hair
in the frame. We use this rule to create a subset of 97 classes
from the Kinetics-400 test set (see Sec. C.7 in the appendix)
called ‘Kinetics-verb’. We show in Tab. 13 that our VFC
improves substantially over the baseline (+5%) on this split.
Assessing verb understanding on SVO-probes. In
Tab. 14, we see that our VFC framework improves the per-
formance on SVO-probes compared to the baseline (partic-
ularly in the verb setting), and outperforms prior work [30]
with 21.7% relative improvement in the verb setting.
5. Conclusion
Video-language models based on CLIP have been shown
to have limited verb understanding, relying extensively on
nouns. We attempt to alleviate this problem with two tech-
nical contributions on the contrastive learning framework:
ﬁrst, we leverage LLMs to automatically generate hard
negative captions focused on verbs; second, we introduce a
verb phrase alignment loss. We validate our verb-focused
pretraining by showing improved performance on a suite
of benchmarks, chosen in particular to assess verb under-
standing. Our framework is general and could be employed
for other video-language tasks, and further readily scales
with the rapid progress in language modelling.
Acknowledgements. We would like to thank Ahmet Iscen,
Anurag Arnab, Paul Hongsuck Seo, Antoine Yang, Shya-
mal Buch, Alex Salcianu for their precious help and discus-
sions. We also thank Sagar Vaze for his invaluable support.References
[1] Unaiza Ahsan, Rishi Madhok, and Irfan Essa. Video jigsaw:
Unsupervised learning of spatiotemporal context for video
action recognition. In WACV , 2019. 3
[2] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,
Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. Unsu-
pervised learning from narrated instruction videos. In CVPR ,
2016. 3
[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning.
InNeurips , 2022. 2, 9, 21
[4] Piyush Bagad, Makarand Tapaswi, and Cees G. M. Snoek.
Test of time: Instilling video-language models with a sense
of time. arXiv preprint arXiv:2301.02074 , 2023. 3
[5] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In ICCV , 2021. 1
[6] Nadine Behrmann, Mohsen Fayyaz, Juergen Gall, and
Mehdi Noroozi. Long short view feature decomposition via
contrastive video representation learning. In ICCV , 2021. 3
[7] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri,
William T. Freeman, Michael Rubinstein, Michal Irani, and
Tali Dekel. Speednet: Learning the speediness in videos. In
CVPR , 2020. 3
[8] Steven Bird, Ewan Klein, and Edward Loper. Natural lan-
guage processing with Python: analyzing text with the natu-
ral language toolkit . ” O’Reilly Media, Inc.”, 2009. 3, 6, 7,
19, 20, 21, 26
[9] Shyamal Buch, Cristobal Eyzaguirre, Adrien Gaidon, Jia-
jun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the
“Video” in Video-Language Understanding. In CVPR , 2022.
2, 3, 6, 8, 9, 15, 19, 21
[10] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue
Wang, and Yuexian Zou. Locvtp: Video-text pre-training
for temporal localization. In ECCV , 2022. 2, 3
[11] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In CVPR ,
2017. 2, 3, 26
[12] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed El-
hoseiny. Visualgpt: Data-efﬁcient adaptation of pretrained
language models for image captioning. In CVPR , 2022. 2
[13] Shizhe Chen and Dong Huang. Elaborative rehearsal for
zero-shot action recognition. In ICCV , 2021. 9
[14] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained
video-text retrieval with hierarchical graph reasoning. In
CVPR , 2020. 3
[15] Ishan Dave, Rohit Gupta, Mamshad Nayeem Rizve, and
Mubarak Shah. Tclr: Temporal contrastive learning for video
representation. Computer Vision and Image Understanding ,
2022. 3[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In ACL, 2019. 2, 21,
26
[17] Ali Diba, Mohsen Fayyaz, Vivek Sharma, Manohar Paluri,
Jurgen Gall, Rainer Stiefelhagen, and Luc Van Gool. Large
scale holistic video understanding. In ECCV , 2019. 3
[18] Michael Dorkenwald, Fanyi Xiao, Biagio Brattoli, Joseph
Tighe, and Davide Modolo. Scvrl: Shufﬂed contrastive video
representation learning. In CVPRW , 2022. 3
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021. 22
[20] Hazel Doughty, Ivan Laptev, Walterio Mayol-Cuevas, and
Dima Damen. Action modiﬁers: Learning from adverbs in
instructional videos. In CVPR , 2019. 3
[21] Hazel Doughty and Cees G. M. Snoek. How Do You Do It?
Fine-Grained Action Understanding with Pseudo-Adverbs.
InCVPR , 2022. 3
[22] Aakanksha Chowdhery et al. Palm: Scaling language model-
ing with pathways. arXiv preprint arXiv:2204.02311 , 2022.
2, 3, 5, 6, 7, 26
[23] Zhenfang Chen et al. See, think, conﬁrm: Inter-
active prompting between vision and language models
for knowledge-based visual reasoning. arXiv preprint
arXiv:2301.05226 , 2023. 2
[24] Alex Falcon, Giuseppe Serra, and Oswald Lanz. A feature-
space multimodal data augmentation technique for text-
video retrieval. In ACM , 2022. 3
[25] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia
Schmid. Multi-modal Transformer for Video Retrieval. In
ECCV , 2020. 8, 15, 21
[26] Robert Geirhos, J ¨orn-Henrik Jacobsen, Claudio Michaelis,
Richard Zemel, Wieland Brendel, Matthias Bethge, and Fe-
lix A. Wichmann. Shortcut learning in deep neural networks.
Nature Machine Intelligence , 2020. 2
[27] Deepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan,
Heng Wang, and Dhruv Kumar Mahajan. Large-scale
weakly-supervised pre-training for video action recognition.
InCVPR , 2019. 2, 3
[28] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
Valentin Haenel, Ingo Fr ¨und, Peter N. Yianilos, Moritz
Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax,
and Roland Memisevic. The “something something” video
database for learning and evaluating visual common sense.
InICCV , 2017. 3
[29] Ben Harwood, Vijay Kumar B.G., Gustavo Carneiro, Ian
Reid, and Tom Drummond. Smart mining for deep metric
learning. In ICCV , 2017. 2
[30] Lisa Anne Hendricks and Aida Nematzadeh. Probing image-
language transformers for verb understanding. In ACL, 2021.
1, 2, 6, 9, 21, 23[31] De-An Huang, Vignesh Ramanathan, Dhruv Mahajan,
Lorenzo Torresani, Manohar Paluri, Li Fei-Fei, and
Juan Carlos Niebles. What makes a video a video: Ana-
lyzing temporal information in video understanding models
and datasets. In CVPR , 2018. 3
[32] Pin Jiang and Yahong Han. Reasoning with heterogeneous
graph alignment for video question answering. In AAAI ,
2020. 8, 9, 21
[33] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion,
Philippe Weinzaepfel, and Diane Larlus. Hard negative mix-
ing for contrastive learning. In Neurips , 2020. 2
[34] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan
Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr–
modulated detection for end-to-end multi-modal understand-
ing. arXiv preprint arXiv:2104.12763 , 2021. 2
[35] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-
supervised video representation learning with space-time cu-
bic puzzles. In AAAI , 2019. 3
[36] Jie Lei, Tamara L. Berg, and Mohit Bansal. Revealing single
frame bias for video-and-language learning. arXiv preprint
arXiv:2206.03428 , 2022. 2, 3
[37] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg,
Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for
video-and-language learningvia sparse sampling. In CVPR ,
2021. 8, 15, 21
[38] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou,
Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and
Shih-Fu Chang. Clip-event: Connecting text and images
with event structures. In CVPR , 2022. 2
[39] Hanwen Liang, Niamul Quader, Zhixiang Chi, Lizhe Chen,
Peng Dai, Juwei Lu, and Yang Wang. Self-supervised spa-
tiotemporal representation learning by exploiting video con-
tinuity. In AAAI , 2021. 3
[40] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus
Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning
to recognize procedural activities with distant supervision. In
CVPR , 2022. 2
[41] Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de
Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng
Li. Frozen clip models are efﬁcient video learners. In ECCV ,
2022. 1
[42] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. In Neurips , 2019. 21
[43] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
Nan Duan, and Tianrui Li. CLIP4Clip: An empirical study
of clip for end to end video clip retrieval. arXiv preprint
arXiv:2104.08860 , 2021. 1, 2, 3, 5, 20, 21, 22
[44] Ziyang Luo, Yadong Xi, Rongsheng Zhang, and Jing Ma. A
frustratingly simple approach for end-to-end image caption-
ing. arXiv preprint arXiv:2201.12723 , 2022. 2
[45] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In ICCV , 2019. 5
[46] Ishan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuf-
ﬂe and learn: Unsupervised learning using temporal order
veriﬁcation. In ECCV , 2016. 3[47] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ra-
makrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown,
Quanfu Fan, Dan Gutfruend, Carl V ondrick, et al. Moments
in time dataset: one million videos for event understanding.
TPAMI , 2019. 5, 7, 19
[48] Mathew Monfort, SouYoung Jin, Alexander Liu, David Har-
wath, Rogerio Feris, James Glass, and Aude Oliva. Spoken
moments: Learning joint audio-visual representations from
video descriptions. In CVPR , 2021. 5
[49] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja
Hauth, Manen Santiago, Sun Chen, and Cordelia Schmid.
Learning audio video modalities from image captions. In
ECCV , 2022. 16
[50] Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar,
Cordelia Schmid, and Andrew Zisserman. Speech2action:
Cross-modal supervision for action recognition. In CVPR ,
2020. 2
[51] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang,
Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin
Ling. Expanding language-image pretrained models for gen-
eral video recognition. In ECCV , 2022. 9
[52] Jae Sung Park, Sheng Shen, Ali Farhadi, Trevor Darrell,
Yejin Choi, and Anna Rohrbach. Exposing the limits of
video-text models through contrast sets. In ACL, 2022. 1,
2, 6, 8, 15, 16, 17, 18, 21, 23, 26
[53] Lyndsey C. Pickup, Zheng Pan, Donglai Wei, YiChang Shih,
Changshui Zhang, Andrew Zisserman, Bernhard Scholkopf,
and William T. Freeman. Seeing the arrow of time. In CVPR ,
2014. 3
[54] Jes ´us Andr ´es Portillo-Quintero, Jos ´e Carlos Ortiz-Bayliss,
and Hugo Terashima-Mar ´ın. A straightforward frame-
work for video retrieval using CLIP. arXiv preprint
arXiv:2102.12443 , 2021. 8, 15
[55] Will Price and Dima Damen. Retro-actions: Learning ‘close’
by time-reversing ‘open’ videos. In ICCVW , 2019. 3
[56] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian,
Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen,
Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distil-
lation, and hard negatives for vision-language pre-training.
arXiv preprint arXiv:2301.02280 , 2023. 2, 8
[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. arXiv preprint
arXiv:2103.00020 , 2021. 1, 3, 6, 8, 9, 15, 21, 22, 23
[58] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with a
uniﬁed text-to-text transformer. Journal of Machine Learn-
ing Research , 2020. 2, 6, 26
[59] Adria Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu
Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski,
Viorica Patraaucean, Florent Altch ´e, Michal Valko, Jean-
Bastien Grill, Aaron Oord, and Andrew Zisserman. Broaden
your views for self-supervised video learning. arXiv preprint
arXiv:2021.00129 , 2021. 3[60] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Ste-
fanie Jegelka. Contrastive learning with hard negative sam-
ples. In ICLR , 2021. 8
[61] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket
Tandon, Chris Pal, Hugo Larochelle, Aaron Courville, and
Bernt Schiele. Movie description. IJCV , 2017. 1
[62] Arka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia,
and Aniruddha Kembhavi. Visual semantic role labeling for
video understanding. In CVPR , 2021. 3
[63] Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang,
and Tatsunori Hashimoto. Is a caption worth a thousand im-
ages? a controlled study for representation learning. arXiv
preprint arXiv:2207.07635 , 2022. 2, 7
[64] Konrad Schindler and Luc van Gool. Action snippets: How
many frames does human action recognition require? In
CVPR , 2008. 3
[65] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and
Cordelia Schmid. End-to-end generative pretraining for mul-
timodal video captioning. In CVPR , 2022. 2
[66] Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj
Goswami, Matt Feiszli, and Lorenzo Torresani. Only time
can tell: Discovering temporal data for temporal modeling.
InWACV , 2021. 2, 3
[67] Gunnar Sigurdsson, Olga Russakovsky, and Abhinav Gupta.
What actions are needed for understanding human actions in
videos? In ICCV , 2017. 3
[68] Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan
Yang, and Jianlong Fu. Long-form video-language pre-
training with multimodal temporal contrastive learning. In
Neurips , 2022. 3
[69] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. In Neurips , 2021. 2
[70] A ¨aron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 3
[71] Jue Wang, Gedas Bertasius, Du Tran, and Lorenzo Torresani.
Long-short temporal contrastive learning of video transform-
ers. In CVPR , 2022. 3
[72] Jiangliu Wang, Jianbo Jiao, and Yunhui Liu. Self-supervised
video representation learning by pace prediction. In ECCV ,
2020. 3
[73] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip:
A new paradigm for video action recognition. arXiv preprint
arXiv:2109.08472 , 2021. 9, 21
[74] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali
Wang, Limin Wang, and Yu Qiao. Internvideo: General
video foundation models via generative and discriminative
learning. arXiv preprint arXiv:2212.03191 , 2022. 8, 15, 21
[75] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou,
Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chen-
guang Zhu, Derek Hoiem, et al. Language models with im-
age descriptors are strong few-shot video-language learners.
arXiv preprint arXiv:2205.10747 , 2022. 2[76] Donglai Wei, Joseph J Lim, Andrew Zisserman, and
William T Freeman. Learning and using the arrow of time.
InCVPR , 2018. 3
[77] Michael Wray, G. Csurka, Diane Larlus, and Dima Damen.
Fine-grained action retrieval through multiple parts-of-
speech embeddings. In ICCV , 2019. 3
[78] Michael Wray and Dima Damen. Learning visual actions
using multiple verb-only labels. In BMVC , 2019. 3
[79] Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and
Philipp Kr ¨ahenb ¨uhl. Sampling matters in deep embedding
learning. In ICCV , 2017. 2
[80] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaim-
ing He, Philipp Kr ¨ahenb ¨uhl, and Ross Girshick. Long-Term
Feature Banks for Detailed Video Understanding. In CVPR ,
2019. 3
[81] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
Next-qa: Next phase of question-answering to explaining
temporal actions. In CVPR , 2021. 2
[82] Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan.
Video graph transformer for video question answering. In
ECCV , 2022. 8, 21
[83] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and
Christoph Feichtenhofer. VideoCLIP: Contrastive pre-
training for zero-shot video-text understanding. In EMNLP ,
2021. 2, 8, 15, 21
[84] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
CVPR , 2016. 1, 2
[85] Ran Xu, Caiming Xiong, Wei Chen, and Jason J. Corso.
Jointly modeling deep video and compositional text to bridge
vision and language in a uniﬁed framework. In AAAI , 2015.
3
[86] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi
Zhang, Chen Sun, and Cordelia Schmid. Multiview trans-
formers for video recognition. In CVPR , 2022. 1
[87] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Just ask: Learning to answer questions
from millions of narrated videos. In ICCV , 2021. 2
[88] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Zero-shot video question answering via
frozen bidirectional language models. In Neurips , 2022. 2
[89] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao. Taco:
Token-aware cascade contrastive learning for video-text
alignment. In ICCV , 2021. 2, 3
[90] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yu-
mao Lu, Zicheng Liu, and Lijuan Wang. An empirical study
of gpt-3 for few-shot knowledge-based vqa. In AAAI , 2022.
2
[91] Yuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, and Qixi-
ang Ye. Video playback rate perception for self-supervised
spatio-temporal representation learning. In CVPR , 2020. 3
[92] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri,
Dan Jurafsky, and James Zou. When and why vision-
language models behave like bags-of-words, and what to do
about it? In ICLR , 2023. 1, 2[93] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-
peng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack
Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Multi-
modal neural script knowledge through vision and language
and sound. In CVPR , 2022. 2
[94] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-
manski, Adrian Wong, Stefan Welker, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny
Lee, Vincent Vanhoucke, and Pete Florence. Socratic mod-
els: Composing zero-shot multimodal reasoning with lan-
guage. arXiv preprint arXiv:2204.00598 , 2022. 2
[95] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and
hierarchical modeling of video and text. In ECCV , 2018. 3
[96] Yue Zhao, Ishan Misra, Philipp Kr ¨ahenb ¨uhl, and Rohit Gird-
har. Learning video representations from large language
models. arXiv preprint arXiv:2212.04501 , 2022. 2, 7
[97] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-
ralba. Temporal relational reasoning in videos. In ECCV ,
2018. 3Appendix
Table of Contents
A . Quantitative results 15
A.1 . Standard vs. Verb-Focus Contrastive
(VFC) learning for all benchmarks . 15
A.2 . MSR-VTT retrieval . . . . . . . . . . 15
A.3 . Additional ablations . . . . . . . . . . 15
B . Qualitative results 17
B.1. MSR-VTT . . . . . . . . . . . . . . 17
B.2. NEXT-QA . . . . . . . . . . . . . . 19
B.3. Kinetics-verb: Further analysis of cal-
ibration . . . . . . . . . . . . . . . 19
B.4. PaLM vs. rule-based methods for hard
negative generation . . . . . . . . . 19
B.5. PaLM vs. rule-based methods for verb
phrase extraction . . . . . . . . . . . 19
C . Baselines & Implementation details 20
C.1. Baselines . . . . . . . . . . . . . . . 20
C.2. CLIP4CLIP Architecture . . . . . . . 22
C.3. Fine-tuning details . . . . . . . . . . 22
C.4. Evaluation protocols . . . . . . . . . 23
C.5. PaLM prompting . . . . . . . . . . . 23
C.6. T5 generations . . . . . . . . . . . . 26
C.7. Kinetics-verb . . . . . . . . . . . . . 26MSR-VTT K-400 NEXT-QA SVO
3k val. Verb Hall verb all ATP hard all verb
Method loss MC MC top-1 top-1 MC MC AP AP
ZERO-SHOT
Baseline NCE 94.9 69.9 55.6 52.1 48.6 28.9 60.2 61.9
VFC NCE 94.9 78.3 58.5 56.7 51.0 31.3 61.5 63.9
VFC HardNeg-NCE 95.1 80.5 58.8 57.1 51.5 31.4 61.8 64.6
FINED -TUNED
Baseline NCE 96.8 73.8 - - 57.3 37.8 - -
VFC NCE 96.2 84.8 - -58.4 38.3 - -
VFC HardNeg-NCE 96.2 85.2 - -58.6 39.3 - -
Table A.1. Standard vs. Verb-Focus Contrastive learn-
ing for all benchmarks. We report MSR-VTT random (3k
val.) and Verb H[52] multiple-choice accuracies, Kinetics-400
and Kinetics-verb top-1 accuracies, NEXT-QA and ATP hard [9]
multiple-choice accuracies, and SVO-probes entire dataset and
verb-focused Average Precision. We observe that our VFC learn-
ing performs better than standard contrastive learning (Baseline)
for all verb-focused benchmarks on both zero-shot and ﬁne-tuned
settings, while maintaining performance on more noun-focused
benchmarks, such as MSR-VTT random MC. We observe that
using the HardNeg-NCE loss, instead of standard NCE, further
improves performance for all benchmarks on both zero-shot and
ﬁne-tuned settings.
This appendix to the main paper provides additional
quantitative (Sec. A) and qualitative results (Sec. B), and
further details on baselines and implementation (Sec. C).
A. Quantitative results
In this section, we present results comparing stan-
dard versus Verb-Focused Constrastive (VFC) learning for
all benchmarks (Sec. A.1), comparison to state-of-the-art
methods for MSR-VTT retrieval (Sec. A.2), and additional
ablations (Sec. A.3).
A.1. Standard vs. Verb-Focus Contrastive (VFC)
learning for all benchmarks
We see in Tab. A.1 that our VFC learning performs better
than standard contrastive learning (Baseline) for all verb-
focused benchmarks on both zero-shot and ﬁne-tuned set-
tings while maintaining performance on more noun-focused
benchmarks, such as MSR-VTT random MC. We observe
that using the HardNeg-NCE loss, instead of standard NCE,
further improves performance for all benchmarks on both
zero-shot and ﬁne-tuned settings.
A.2. MSR-VTT retrieval
We see in Tab. A.2 that while our verb-focused pre-
training drastically improves performance on verb-focused
benchmarks – such as Verb Hsplit [52] MSR-VTT (see
main paper Tab. 9) – it maintains performance on noun-
focused benchmarks such as MSR-VTT retrieval T2V (1k
split) in a zero-shot setting. We perform comparably to In-
ternVideo [74] in a zero-shot setting, while using a signiﬁ-1K val.
Model # params. T!V R@1
ZERO-SHOT
VideoCLIP [83] – 10.4
CLIP [57] 151M 30.6
InternVideo [74]z t460M 40.7
VFC (Ours) 164M 40.3
FINED -TUNED
ClipBERT [37] – 22.0
MMT [25] – 26.6
VideoCLIP [83] – 30.9
CLIP-straight [54] 151M 31.2
MMT (CLIP features) [25] – 34.0
C4CL-mP [52] 151M 43.1
CLIP2Video [52] – 45.6
InternVideo [74]z t460M 55.2
VFC (Ours) 164M 44.5
Table A.2. Results on MSR-VTT retrieval. We report T2V
retrieval on the 1k split. While our VFC framework drastically
improves performance on verb-focused benchmarks, including
Verb Hsplit [52] (see main paper Tab. 9), it maintains perfor-
mance on noun-focused benchmarks such as the retrieval 1k split
in the zero-shot setting. In the ﬁne-tuned setting, InternVideo sur-
passes VFC’s performance. zInternVideo is concurrent unpub-
lished work with a larger model (2.8× more parameters and 12.4×
more ﬂops), and has a larger pretraining dataset size (they use 24×
more data).
cantly smaller setting both in terms of architecture (Intern-
Video uses 2.8× more parameters and 12.4× more ﬂops)
and pretraining dataset size (they use 24× more data). In
a ﬁne-tuned setting, InternVideo surpasses VFC’s perfor-
mance. This is expected given our model parameters and
ﬂops are signiﬁcantly smaller – see number of parameters
in Tab. A.2.
A.3. Additional ablations
Here, we present ablation results for video mining
(Sec. A.3.1), PaLM prompting (Sec. A.3.2), the verb phrase
loss (Sec. A.3.3), ﬁne-tuning strategy (Sec. A.3.4) and cal-
ibration (Sec. A.3.5). We note that all ablations are per-
formed with the standard NCE loss (not HardNeg-NCE).
A.3.1 Video mining
An alternative to our proposed calibration strategy to avoid
imbalances due to the addition of negative captions would
be to avoid training with unpaired data at all, by mining a
matching video Vhard
ikfor each generated caption Thard
ik. We
attempt this via CLIP-based text-to-video retrieval in a large
video database. We next explain our pipeline in more detail.
Firstly, we generate hard negative verb captions with
PaLM as explained in Sec. 3.2 of the main paper. For
each hard negative caption, we then perform text-to-imageSMiT videoVideoCC mined video
Original caption a video showing a brown cat sitting Hard negative generated caption a video showing a brown cat standing 
SMiT video
Original caption a woman in a pink bikini lays down  as another woman massages her legs
Hard negative generated caption a woman in a pink bikini stands up  as another woman massages her legs
SMiT videoVideoCC mined video
Original caption a baby standing wearing red striped PJsHard negative generated caption a baby sleeping wearing red striped pjsSMiT video
Original caption there’s someone racing on a  motorcycle with cones next to themHard negative generated caption there’s someone dancing on a  motorcycle with cones next to them
VideoCC mined videoVideoCC mined video
Figure A.1. Video mining: We show examples of mined matching videos for generated hard negative captions. For ease of visualisation,
we show a single frame per video. In some cases, as the top left corner, the mined video from VideoCC closely matches the hard negative
caption. However, often, the new video-text pairs are noisy. For example, in the top right corner, the mined video contains a ‘red striped
shirt’ but no ‘baby sleeping’. In the bottom left example, there is a ‘woman in a pink bikini standing up’ but no ‘woman massaging her
legs’. Finally, in the bottom right example, although the video contains a ‘motorcycle’, the person is not ‘dancing’ and there are no ‘cones
next to them’.
Method # pairs Verb H K-400 SMiT
Baseline 481K 69.9 55.6 78.3
HN 481K 78.0 (+8.1) 55.8 (+0.2) 78.6 (+0.3)
HN+VM 1.22M 78.7 (+8.8) 51.8 (-3.8) 75.0 (-3.3)
Table A.3. Video Mining. We report multi-choice accuracy on
Verb H[52], Kinetics-400 top-1 accuracy and V2T R@1 on Spo-
ken Moment in Time (validation set of our pretraining SMiT data).
We observe that although our Video Mining (VM) approach im-
proves performance on Verb H, it causes a drop in performance on
Kinetics and SMiT, which highlights that the additional video-text
pairs are noisy. For experiments including hard negatives, we note
that one hard negative is sampled for each video here.
retrieval to ﬁnd a matching video in the VideoCC [49]
database. Speciﬁcally, we calculate the cosine similarity be-
tween the hard negative caption CLIP text embedding and
the average of the video frames’ CLIP image embedding,
for all videos in the database. We then keep the video with
closest similarity to the hard negative caption to form a new
video-text pair. Finally, we apply a similarity threshold to
keep only the best matching video-text pairs and add these
to our training set. In practice, we experiment with different
thresholds and ﬁnd a value of 0.28 to work best, adding a to-
tal of 738K new video-text pairs to training (SMiT training
set size is 481K). Note that we also experiment with text-
to-text retrieval: in this case, we calculate the similarity be-
tween each hard negative caption and all VideoCC captions,
and subsequently use the video corresponding to the closestVideoCC caption to form a new pair. However, we ﬁnd this
performs worse.
We observe in Tab. A.3 that our additional video-text
pairs are noisy. In fact, although this approach improves
performance on Verb H, it causes a large drop in perfor-
mance on Kinetics and SMiT (validation set of our pre-
training data). Finding a video matching a speciﬁc, detailed
and long caption is challenging (see qualitative examples in
Fig. A.1). A video matching the caption may not exist in
the VideoCC corpus and even if it did, for this method to be
successful, the mined video must match the generated cap-
tion on the verbs (and CLIP is biased towards images and
objects only, which is exactly the problem we are trying to
solve).
A.3.2 Giving input-output example pairs to PaLM
To generate hard verb negative captions with PaLM, we also
add four input-output pair examples to the prompt (see full
prompt in Sec. C.5) to increase the quality of the generated
hard negatives. We observe in Tab. A.4 that the input-output
pairs improve the performance on Verb Hand Kinetics-400.
A.3.3 Verb phrase loss
We see in Tab. A.5 that using only the video-to-text com-
ponent of the verb phrase loss allows us to maintain per-
formance on noun-focused benchmarks such as MSR-VTTinput-output pairs Verb H K-400
77.5 54.6
X 78.0 55.8
Table A.4. Inclusion of input-output pairs in PaLM prompt.
We report multi-choice accuracy on Verb H[52] and Kinetics-400
top-1 accuracy. We observe that including input-output pairs in the
PaLM prompt for generating hard negative captions increases the
performance on both benchmarks. We note that one hard negative
is sampled for each video here.
MSR-VTT K-400
Verb phrase loss 1k val. Verb H all
Method T!V V!T T!V R@1 MC acc Top-1
Baseline 40.8 69.9 55.6
VFC (Ours) X X 38.8 (-2.0) 77.0 (+7.1) 58.8 (+3.2)
VFC (Ours) X 40.1 (-0.7) 76.3 (+6.4) 58.5 (+2.9)
Table A.5. Verb phrase loss. We report MSR-VTT T2V retrieval
on the 1k split, multi-choice accuracy on Verb H[52] and Kinetics-
400 top-1 accuracy. We observe that using only the video-to-text
component of the verb phrase loss allows us to maintain perfor-
mance on noun-focused benchmarks such as MSR-VTT retrieval,
while also giving a performance boost on Verb Hand K-400. For
experiments including hard negatives, we note that one hard nega-
tive is sampled for each video here.
Method ^text^image Verb H K-400
Baseline 69.9 55.6
VFC (Ours) 76.3 58.5
VFC (Ours) X 72.0 (-4.3) 54.8 (-3.7)
VFC (Ours) X 75.1 (-1.2) 55.1 (-3.4)
Table A.6. Fine-tuning image and text towers. We report multi-
choice accuracy on Verb H[52] and Kinetics-400 top-1 accuracy.
^corresponds to freezing the image or text tower. We observe
that ﬁne-tuning both image and text towers works best. For exper-
iments including hard negatives, we note that one hard negative is
sampled for each video here.
retrieval, while also giving a performance boost on verb fo-
cused benchmarks Verb Hand K-400.
A.3.4 Fine-tuning image and text towers
We experiment with different ﬁne-tuning strategies: (i) ﬁne-
tuning both image and text towers, (ii) freezing the image
CLIP backbone only (here, the sequence Transformer se-
qTrans and text tower are trained – see Sec. C.2 for more
details on the CLIP4CLIP architecture), (iii) freezing the
text tower only (here, seqTrans and image CLIP backbone
are trained), (iv) freezing both image and text towers (here,
only seqTrans is trained). We see in Tab. A.6 that ﬁne-
tuning both image and text towers works best. We do not
include setting (iv) as it performs very poorly.Method reducing BeffectG!tS!Verb H K-400
Baseline 69.9 55.6
HN 80.5 54.5
HN X 79.4 55.4
HN X 78.7 55.2
HN X X 78.0 55.8
Table A.7. Calibration strategy. We report multi-choice accu-
racy on Verb H[52] and Kinetics-400 top-1 accuracy. We observe
that by combining both calibration steps, we avoid a drop in per-
formance on Kinetics-400, while maintaining a large performance
improvement on Verb H. For experiments including hard nega-
tives, we note that one hard negative is sampled for each video
here.
A.3.5 Calibration
As explained in Sec. 3.2 of the main paper, our calibration
strategy is composed of two steps: (1) ignoring hard nega-
tive captions from the other elements of the batch (denoted
as ‘reducing Beffect’, where Bis the batch size); (2) ﬁlter-
ing the generated PaLM captions to have equal number of
concept occurences in positive and negative pairs (denoted
asG!tS!). We show the effect of each of these steps
in Tab. A.7. We observe that by combining both steps, we
avoid a drop in performance on Kinetics-400, while main-
taining a large performance improvement on Verb H.
B. Qualitative results
In this section, we present qualitative results on MSR-
VTT (Sec. B.1) and NEXT-QA (Sec. B.2), further analysis
of calibration on Kinetics-verb (Sec. B.3), and comparisons
of the use of PaLM versus rule-based methods for hard neg-
ative (Sec. B.4) and verb phrase (Sec. B.5) generations.
B.1. MSR-VTT
We show qualitative examples from the Verb H[52] mul-
tiple choice evaluation in Figure A.2. For each video sam-
ple, we show the 5 captions ranked in order of decreas-
ing similarity for both our baseline and VFC models. We
observe that the baseline model often mistakes the hard
negative as matching the video. This effect is reduced
when training with hard negatives, as proposed in our VFC
method, enabling the correct caption to be retrieved from
the 5 options. In some rare cases, as shown on the last row,
the baseline model is correct but training with hard neg-
atives causes the hard negative to have highest similarity
with the video. For example, the model incorrectly ranks
‘a silent clip of a woman smiling at people ’ higher than ‘ a
silent clip of a woman screaming at people ’.a deer is running across a  road in a video gameBaseline (Ours) 1. a deer is rolling across a road in a video game 2. a deer is running across a road in a video game 3. kids are reacting to viral videos 4. a group of people celebrating some kind of festival 5. models are walking down a short runway VFC (Ours) 1. a deer is running across a road in a video game 2. a deer is rolling across a road in a video game 3. kids are reacting to viral videos 4. a group of people celebrating some kind of festival 5. models are walking down a short runwayBaseline (Ours) 1. a cartoon girl dances about in the rain  2. a cartoon girl sings about rain  3. a bird white color is dancing 4. a judge talks to a young performer on the stage 5. man using spary on the underside of a car  VFC (Ours) 1. a cartoon girl sings about rain  2. a cartoon girl dances about in the rain  3. a bird white color is dancing 4. a judge talks to a young performer on the stage 5. man using spary on the underside of a car a cartoon girl sings about rain
two wrestlers are ﬁghting  in the ringBaseline (Ours) 1. two wrestlers are hugging each other in the ring 2. two wrestlers are ﬁghting in the ring 3. two men are on a hill 4. four young girls are sitting and laughing 5. a person is quickly dicing up the onions VFC (Ours) 1. two wrestlers are ﬁghting in the ring 2. two wrestlers are hugging each other in the ring 3. two men are on a hill 4. four young girls are sitting and laughing 5. a person is quickly dicing up the onionsBaseline (Ours) 1. a basketball player misses a layup 2. a basketball player shoots a layup 3. this is a video advertisement about a fruit juice 4. a woman is talking 5. diﬀerent couples are shown at a table VFC (Ours) 1. a basketball player shoots a layup 2. a basketball player misses a layup 3. this is a video advertisement about a fruit juice 4. a woman is talking 5. diﬀerent couples are shown at a tablea basketball player shoots a layup
a woman is singing  while staring at a man Baseline (Ours) 1. a woman is crying while staring at a man 2. a woman is singing while staring at a man 3. a beautiful video presentation of chef jon favreau 4. a little girl plays with a small play set 5. fanfare on an indoor soccer ﬁeld is being shown VFC (Ours) 1. a woman is singing while staring at a man 2. a woman is crying while staring at a man 3. a beautiful video presentation of chef jon favreau 4. a little girl plays with a small play set 5. fanfare on an indoor soccer ﬁeld is being shownBaseline (Ours) 1. a man is sketching a vehicle 2. a man is building a vehicle 3. there is a commentary about the arriving of vips  4. a photoshop tutorial featuring a photo of a woman 5. there is a woman is singing a new song VFC (Ours) 1. a man is building a vehicle 2. a man is sketching a vehicle 3. there is a commentary about the arriving of vips  4. a photoshop tutorial featuring a photo of a woman 5. there is a woman is singing a new songa man is building a vehicle
a man is giving a political  speechBaseline (Ours) 1. a man is listening to a political speech 2. a man is giving a political speech 3. an interesting scene of wrestling 4. a lot of people walking around a small space 5. a sloth is climbing in a tree VFC (Ours) 1. a man is giving a political speech 2. a man is listening to a political speech 3. an interesting scene of wrestling 4. a lot of people walking around a small space 5. a sloth is climbing in a treeBaseline (Ours) 1. a person sleeping on the edge of a building  2. a person standing on the edge of a building 3. she routed on hilary 4. this is a video from the voice kids 5. a man performs a ping pong trick shot VFC (Ours) 1. a person standing on the edge of a building 2. a person sleeping on the edge of a building  3. she routed on hilary 4. this is a video from the voice kids 5. a man performs a ping pong trick shota person standing on the  edge of a building
chef is cooking food hereBaseline (Ours) 1. chef is serving food here 2. chef is cooking food here 3. a old man wears specs talking to media 4. an animation talking about economists 5. someone is showing how to solve a rubik cube VFC (Ours) 1. chef is cooking food here 2. chef is serving food here 3. a old man wears specs talking to media 4. someone is showing how to solve a rubik cube 5. an animation talking about economistsBaseline (Ours) 1. a video of a guy crashing a lamborghini 2. a video of a guy driving a lamborghini 3. a man says he is going to go to samokov to buy some stuﬀ 4. korean guy singing 5. people gathered in a place and release balloons in the air VFC (Ours) 1. a video of a guy driving a lamborghini 2. a video of a guy crashing a lamborghini 3. a man says he is going to go to samokov to buy some stuﬀ 4. korean guy singing 5. people gathered in a place and release balloons in the aira video of a guy driving a lamborghini
a silent clip of a woman screaming at people Baseline (Ours) 1. a silent clip of a woman screaming at people 2. a silent clip of a woman smiling at people 3. texts are being sent back and forth 4. a man speaks about a teaching curriculum 5. the boy is trying to ﬁx the problem VFC (Ours) 1. a silent clip of a woman smiling at people 2. a silent clip of a woman screaming at people 3. texts are being sent back and forth 4. a man speaks about a teaching curriculum 5. the boy is trying to ﬁx the problemBaseline (Ours) 1. a group of people yelling at the camera 2. a group of people dancing at the camera 3. this is a video of chinnese guy rapping 4. a man has his arms crossed 5. a child is preparing to bake something VFC (Ours) 1. a group of people dancing at the camera 2. a group of people yelling at the camera 3. this is a video of chinnese guy rapping 4. a man has his arms crossed 5. a child is preparing to bake somethinga group of people yelling at the camera
Figure A.2. MSR-VTT verb-focused benchmark: We show qualitative examples from the Verb H[52] multiple choice evaluation. For
ease of visualisation, we only show a single frame per video. For each video sample, we show the 5 captions ranked in order of decreasing
similarity for both our baseline and VFC models. We observe that the baseline model often mistakes the hard negative as matching the
video; for example in the top left example, the caption ‘a deer is rolling across a road in a video game’ is ranked higher than the correct
answer ‘a deer is running across a road in a video game’. When training with hard negatives as in our VFC model, the model performance
improves, retrieving the correct caption from the 5 options. On the bottom row, we show two failure cases where training with hard
negatives causes the model to make a mistake; choosing the hard negative (‘a group of people dancing at the camera’, ‘a silent clip of a
woman smiling at people’) as the correct caption instead of (‘a group of people yelling at the camera’, ‘a silent clip of a woman screaming
at people’).Question: how did the men keep their  belongings with them as they cycle?Baseline (Ours) 1. bicycle 2. carry a pouch each 3. follows the animal 4. bag 5. wore socks VFC (Ours) 1. carry a pouch each 2. bicycle 3. follows the animal 4. bag 5. wore socksBaseline (Ours) 1. bite 2. run away 3. push back 4. shout 5. snarl VFC (Ours) 1. push back 2. bite 3. run away 4. shout 5. snarlQuestion: how did the animals respond  when being pushed by each other  during the ﬁght?
Baseline (Ours) 1. looking at the baby 2. bite it 3. wag tail 4. walk towards the cameraman 5. carry dog up VFC (Ours) 1. bite it 2. looking at the baby 3. wag tail 4. walk towards the cameraman 5. carry dog upBaseline (Ours) 1. put into his mouth 2. cabinet 3. eat calmly 4. hide away 5. resumes writing VFC (Ours) 1. cabinet 2. put into his mouth 3. hide away 4. eat calmly 5. resumes writingQuestion: what did the dog do  after bringing the bear across  to the front at the start? 
Question: where was the box of ﬂour kept  after enough was added into the bowl?
Question: what did the lady in brown  skirt do after the second time  she shaked her body?Baseline (Ours) 1. walk back to the lady 2. clap 3. catch baby 4. look at orange cabinet 5. put hands together VFC (Ours) 1. clap 2. walk back to the lady 3. look at orange cabinet 4. catch baby 5. put hands together
Question: why is the person looking  at his laptop the whole time?Baseline (Ours) 1. editing the music 2. for music score reference 3. watch video 4. distracted 5. show the picture from camera VFC (Ours) 1. for music score reference 2. editing the music 3. watch video 4. distracted 5. show the picture from camera
Figure A.3. NEXT-QA: We show qualitative examples from the ATP hard [9] multiple choice evaluation. For ease of visualisation, we only
show a single frame per video. For each video sample, we show the 5 answers ranked in order of decreasing similarity for both our baseline
and VFC models in a zero-shot setting. We observe that our VFC model improves performance, retrieving the correct answer from the 5
options more often.
B.2. NEXT-QA
In Figure A.3, we show qualitative examples from the
ATPhard [9] multiple choice evaluation. For each video
sample, we show the 5 answers ranked in order of decreas-
ing similarity for both our baseline and VFC models in a
zero-shot setting. We observe that our VFC model improves
performance, retrieving the correct answer from the 5 op-
tions more often.
B.3. Kinetics-verb: Further analysis of calibration
In Tab. A.8, we show further examples of confusion ma-
trices comparing training performance with versus without
calibration on Kinetics-verb classes as in Tab. 4 of the main
paper. Once again, we observe that calibration reduces the
effect of ‘attraction’ points, which distort the feature space,
by makingR!the same for all verb phrase concepts.
B.4. PaLM vs. rule-based methods for hard negative
generation
In Fig. A.4, we compare hard negative caption genera-
tion using PaLM to T5 and rule-based methods such as re-placing detected verbs by random verbs or antonym verbs.
We observe that LLM based methods result in linguistically
and semantically viable sentences (which may not be guar-
anteed with random and antonym verb replacements). We
also note that LLM based methods can change more than
just the verb: (i) T5 and PaLM can replace the verb by a
verb-noun pair and, (ii) PaLM can replace pronouns and de-
terminers anywhere in the sentence (as opposed to T5 which
can only replace the verb, see more details in Sec. C.6),
making the negative caption more linguistically correct.
B.5. PaLM vs. rule-based methods for verb phrase
extraction
In Fig. A.5, we compare verb phrase extraction using
PaLM to: (i) using action labels for clips from the Mo-
ments in Time (MiT) dataset (these are available as SMiT
data inherits from MiT [47]) and (ii) using a rule-based
method (NLTK [8]) to isolate verbs. We observe that us-
ing PaLM outperforms both: (i) MiT action labels can be
general and conceal ﬁne-grained action information in the
video which can improve verb understanding, (ii) NLTKhas difﬁculties extracting all verbs in a sentence, and can
often mistake them for nouns; NLTK also cannot extract a
verb phrase when a verb is not present in the sentence (e.g.
for the caption ‘this is an aerial shot of a very nice water-
fall’, NLTK extracts no verb phrase while PaLM extracts
‘water ﬂowing’); ﬁnally, our NLTK approach does not ex-
tract verb-noun pairs (e.g. for the caption ‘this is a video
of two women who are doing gymnastics’, NLTK extracts
‘doing’ while PaLM extracts ‘doing gymnastics’) – this can
be crucial for understanding the action in the video. We
note that although NLTK could be used to extract verbs and
nouns independently through PoS tagging, correctly assign-
ing nouns to the matching verb is not always robust for long,
complex sentences as in SMiT. Indeed, the average length
of a sentence in SMiT is 18 words.
w/o calibration w/ calibration
R!/
11
mopping floorcleaning floormopping floorcleaning floor46121236
010203040717
mopping floorcleaning floormopping floorcleaning floor1731715
010203040
11
mopping floorcleaning floormopping floorcleaning floor46121236
010203040717
mopping floorcleaning floormopping floorcleaning floor1731715
010203040
mopping ﬂoor
cleaning ﬂoor
R!/
11
dunking basketballshooting basketballdunking basketballshooting basketball2520739
010203040417
dunking basketballshooting basketballdunking basketballshooting basketball425298
010203040
11
dunking basketballshooting basketballdunking basketballshooting basketball2520739
010203040417
dunking basketballshooting basketballdunking basketballshooting basketball425298
010203040
dunking basketball
shooting basketball
R!/
1446
doing nailscutting nailsdoing nailscutting nails42686
01020304011
doing nailscutting nailsdoing nailscutting nails4311519
010203040
1446
doing nailscutting nailsdoing nailscutting nails42686
01020304011
doing nailscutting nailsdoing nailscutting nails4311519
010203040
doing nails
cutting nails
Table A.8. Confusion matrix for Kinetics-verb classes. With-
out proper calibration, the verb phrases ‘mopping ﬂoor’, ‘dunking
basketball’, ‘doing nails’ become highly attractive in the video-
text feature space. Our calibration mechanism alleviates this issue
by making the ratio R!independent of verb phrases (see details in
Sec. 3.2 of the main paper).
C. Baselines & Implementation details
In this section, we present detailed descriptions of base-
lines (Sec. C.1), the CLIP4CLIP [43] architecture used in
all our experiments (Sec. C.2), ﬁne-tuning (Sec. C.3) and
evaluation protocols (Sec. C.4), the PaLM prompting pro-
Original caption: a woman squats with  an empty bar that has a couple of  rubber bands attached to it on the ﬂoor 
Hard negative generated captions:  Random verb: a woman gardens with  an empty bar that has a couple of rubber bands attached to it on the ﬂoor Antonym verb: [no antonym] T5: a woman walks with  an empty bar that has a couple of rubber bands attached to it on the ﬂoor PaLM: a woman runs to  an empty bar that has a couple of rubber bands attached to it on the ﬂoorOriginal caption: people are walking  around the mall that is somewhat crowded 
Original caption: a man is sitting  on his bike on his cell phone
Original caption: video of a man  texting on his phoneHard negative generated captions:  Random verb: video of a man  airlifting on his phone Antonym verb: [no antonym] T5: video of a man surﬁng the web  on his phone PaLM: video of a man taking a selﬁe  on his phone
Hard negative generated captions:  Random verb: people are encouraging  around the mall that is somewhat crowded Antonym verb: people are riding around  the mall that is somewhat crowded T5: people are sitting around the mall  that is somewhat crowded PaLM: people are driving around the mall  that is somewhat crowded
Hard negative generated captions:  Random verb: a man is wrestling on his  bike on his cell phone Antonym verb: a man is standing on his bike on his cell phone T5: a man is standing with a woman  on his bike on his cell phone PaLM: a man is riding his bike  on his cell phone
Figure A.4. PaLM vs. rule-based methods for hard negative
generation: We compare hard verb negative caption generation
using PaLM to T5 and rule-based methods such as replacing de-
tected verbs by random verbs or antonym verbs. We observe that
randomly changing the verb often results in sentences which are
linguistically and semantically incorrect, and that antonym verbs
are often not present in NLTK [8]. On the other hand, LLM based
methods such as T5 and PaLM result in meaningful sentences. We
note that LLM based methods can change more than just the verb:
in the last row, replacing ‘texting’ by ‘surﬁng the web’ with T5
and ‘taking a selﬁe’ with PaLM. In some cases, this can make it an
easier negative: for example, in the third row, replacing ‘sitting’
by ‘sitting with a woman’ with T5.
cedure (Sec. C.5), the T5 hard negative generation pro-
cess (Sec. C.6), and the Kinetics-verb split we propose
(Sec. C.7).
C.1. Baselines
We describe in more detail baselines presented in the
main paper for MSR-VTT, NEXT-QA, Kinetics-400 and
SVO-Probes.this is an aerial shot of a very nice waterfall
person is stir-frying vegetables in a wok
MiT labels: ‘dripping’ NLTK: ‘’ PaLM: ‘water ﬂowing’
MiT labels: ‘stirring’ NLTK: ‘’ PaLM: ‘stir-frying vegetables’
two ladies having a conversationthis is a video of two women doing gymnastics
MiT labels: ‘picking’ NLTK: ‘having’ PaLM: ‘having conversation’MiT labels: ‘blocking’ NLTK: ‘doing’ PaLM: ‘doing gymnastics’
several people sit cross-legged on  the ﬂoor and enjoy a game of cardsthis is a cooking clip the person pours oil and  adds mushrooms and other vegetables into a pan
MiT labels: ‘gambling’ NLTK: ‘sit’, ‘enjoy’ PaLM: ‘playing cards’, ‘sitting’, ‘enjoying’MiT labels: ‘stirring’ NLTK: ‘pours’, ‘adds’ PaLM: ‘pouring oil’, ‘adding vegetables’
video of a young child taking a bath outsidea woman is doing a lunge exercise  while also rotating her arms
MiT labels: ‘playing’ NLTK: ‘taking’ PaLM: ‘taking a bath’MiT labels: ‘stretching’ NLTK: ‘rotating’ PaLM: ‘rotating arms’, ‘lunging’
Figure A.5. PaLM vs. rule-based methods for verb phrase ex-
traction: We compare verb phrase extraction using PaLM to:
(i) using action labels for clips from the Moments in Time (MiT)
dataset and (ii) using a rule-based method such as NLTK [8] to
isolate verbs. In the top row, we show examples where NLTK out-
puts no label as a verb is not present in the sentence (ﬁrst row, left)
or is not detected (ﬁrst row, right). In the second row, we show
examples where extracting verbs with NLTK (e.g. ‘doing’, ‘hav-
ing’) does not convey crucial information for understanding the
action in the video. In the last two rows, we show examples where
the MiT labels conceal valuable ﬁne-grained action information
in the video, whereas PaLM can recover this from the caption:
(third row, left) the video is labelled as ‘gambling’, PaLM extracts
‘playing cards’; (third row, right) the video is labelled as ‘stirring’,
PaLM extracts ‘pouring oil’ and ‘adding vegetables’; (last row,
left) the video is labelled as ‘playing’, PaLM extracts ‘taking a
bath’; (last row, right) the video is labelled as ‘streching’, PaLM
extracts ‘rotating arms’ and ‘lunging’. Overall, our PaLM method
of extracting verbs from captions performs best qualitatively and
quantitatively (as shown in Tab. 5 (right) of the main paper).MSR-VTT. We show the performance of VideoCLIP [83],
CLIP [57] and InternVideo [74] zero-shot. VideoCLIP
trains a transformer for video and text by contrasting tem-
porally overlapping positive video-text pairs with hard neg-
atives from nearest neighbor retrieval. More details for
the CLIP baseline can be found in [57]. InternVideo ex-
plores jointly using masked video modelling and video-
language contrastive learning as pretraining objectives. In
the ﬁne-tuned setting, we compare to ClipBERT [37],
MMT [25], VideoCLIP [83], C4CL-mP [52]. ClipBERT
focuses on sparse training to reduce video processing over-
head and applying image-text pretraining for video-text
tasks. MMT uses a multi-modal transformer to encode
video and BERT [16] for text. C4CL-mP corresponds to
the CLIP4CLIP [43] reimplementation by Park et al. [52]
with just mean pooling (without any Transformer Encoder
for temporal modelling of frames).
NEXT-QA. We show the performance of CLIP zero-shot.
More details for this baseline can be found in [57]. In
the ﬁne-tuned setting, we compare to HGA [32]: a deep
heterogenous graph network which aligns inter- and intra-
modality information (appearance, motion and text) to rea-
son and answer the question. We also compare to ATP,
Temp[ATP] and Temp[ATP]+ATP from [9]. ATP consists
of a Transformer which learns to select a single (frozen)
CLIP frame embedding from a video, given the sequence
of video frame embeddings and question embedding, for
the task of video question answering. For training, they
use a cross entropy loss over the answer set. Temp[ATP]
is an extension of ATP, where the video is ﬁrst partitioned
intokclips, and a single frame embedding is selected
using ATP from each clip. These kframe embeddings
are then aggregated to a video-level representation using a
Transformer, before being passed to the downstream task.
Temp[ATP]+ATP corresponds to an ensemble of both ATP
and Temp[ATP]. Finally we compare to VGT [82], which
consists of a video graph transformer that explicitly encodes
objects, relations and dynamics. VGT also uses disentan-
gled video and text Transformers to better measure rele-
vance between video and text.
Kinetics-400. We show the performance of Flamingo [3],
ActionCLIP [73] and CLIP [57] zero-shot. Flamingo is a
visual-language model, which leverages pretrained vision
and language models and bridges them effectively by using
gated cross-attention and dense layers. ActionCLIP [73]
reformulates action recognition into a video-text matching
problem within a multimodal contrastive learning frame-
work. More details for CLIP can be found in [57].
SVO Probes. We show the performance of No-
MRM–MMT (the best performing model in [30]) and
CLIP [57] zero-shot. No-MRM-MMT corresponds to a
multi-modal transformer (similar to the ViLBERT [42] ar-
chitecture) with a masked language modeling loss (MLM),. . .ViT Encoder (ViT). . .Linear Projection of Flattened Patches01234**Extra learnable  [class] embeddingPatch + Position  embeddingFrame representation
The young girl    in    the middle of    the   road012345678Text Encoder (Transformer)Token + Position  embeddingTransformer Encoder. . .0134Frame + Position  embedding. . .Mean PoolingSimilarity
TimeText representationFigure A.6. CLIP4CLIP Architecture: Figure adapted from [43]. The model consists of a video encoder, text encoder and similarity
calculator. Each frame is passed through ViT to obtain a frame representation at the output of the [class] token. The Tframe representa-
tions are then passed through a Transformer for sequence modelling and averaged with a mean pooling operation to obtain a video-level
representation. The video representation is then compared to the text representation through the cosine similarity.
an image-text matching loss (ITM) that classiﬁes if an
image-sentence pair are matching, but no masked region
modeling loss (MRM). More details for the CLIP baseline
can be found in [57].
C.2. CLIP4CLIP Architecture
Here, we describe the CLIP4CLIP network architec-
ture [43], illustrated in Figure A.6, used in all our exper-
iments. This architecture consists of three components: a
video encoder, a text encoder, and a similarity calculator.
We describe each component in detail next. We note that all
three components are ﬁne-tuned in all our experiments.
Video encoder . The pretrained CLIP (ViT-B/32) [57] im-
age encoder is used to obtain frame representations. Specif-
ically, video frames are ﬁrst sampled from the video and
reshaped into a sequence of ﬂattened 2D patches. These
patches are then linearly projected to 1D tokens before be-
ing inputted to ViT [19], a 12-layer Transformer. The output
from the [class] token is used as the video frame represen-
tation: given Tinput frames, we obtain Tframe represen-
tations. In practice, we select 32 frames (with initial resolu-
tion256256, of which augmented crops of size 224224
are taken) in a video at 25fps, at a stride of 2 frames for
training.
Text encoder . The CLIP pretrained text encoder is used to
embed the caption. It corresponds to a 12-layer Transformer
model; further details can be found in [57].Similarity calculator . The goal is to learn a function to
calculate the similarity between video-text pairs inputted to
the model in such a way that video-text pairs which match
have a high similarity, and otherwise have a low similar-
ity. Therefore, we ultimately want to compare a text and
video-clip representation. The ViT encoder outputs a repre-
sentation for each of the sequence of frames without any
temporal modelling. We therefore ﬁrst pass these frame
embeddings (along with temporal positional embeddings)
through a 4-layer Transformer encoder. We then apply a
mean-pooling operation to the new frame embeddings to
obtain a video-level representation. Finally, we calculate
the cosine similarity between the video and text representa-
tions.
Following the protocol in [43], the positional embed-
dings in the similarity calculator are initialised by repeat-
ing the position embedding from CLIP’s text encoder. The
Transformer encoder is initialised by the corresponding lay-
ers’ weight of the pretrained CLIP image encoder. The rest
is randomly initialised.
C.3. Fine-tuning details
MSR-VTT . When ﬁne-tuning on MSR-VTT, we use the 9K
and 7K training split for the retrieval and multi-choice set-
tings respectively. For the 9K split, we train for 100 epochs
with a base learning rate of 1e-7, a weight decay of 1e-2
and temperature of 5e-3. For the 7K split, we train for 100epochs with a base learning rate of 1e-7, a weight decay of
1e-2 and temperature of 5e-3. For both settings, we train
with the hard negative contrastive loss and discard the verb
phrase loss. Indeed, we use PaLM to generate hard nega-
tive captions for MSR-VTT, since it is a video-text retrieval
dataset, similarly to SMiT. We sample 32 frames per video
at 25 fps with a stride of 14.
NEXT-QA . For ﬁne-tuning on NEXT-QA, we concatenate
the question and answer pairs before passing them through
the CLIP4CLIP text tower. We continue using the hard-
negative cross-modal contrastive loss during ﬁne-tuning,
treating the four incorrect question-answer pairs as hard
negatives. We discard the verb phrase loss. We train for
100 epochs with a base learning rate of 1e-6, a weight de-
cay of 5e-2 and temperature of 1e-3. We maintain a batch
size of 256. We sample 32 frames per video at 25 fps with
a stride of 24.
C.4. Evaluation protocols
MSR-VTT . For the standard setting, we evaluate text-to-
video retrieval (R@1) on the 1K validation split and 3K
Random MC. In the former, the model must associate the
text to the correct video, among 1000 videos. For the lat-
ter, the model must associate the video to the right cap-
tion, among 5 captions, where the 4 negative captions are
randomly chosen from other videos. For our verb-focused
setting, we use the Verb Hmultiple choice (MC) validation
split from [52]. Verb HMC covers a subset of the videos in
the 3K Random MC split, with 2,554 video-text instances,
but the task is harder. In the Verb HMC setting, one of ran-
dom negative captions is replaced by a hard verb negative ,
where the correct sentence’s verb has been modiﬁed manu-
ally in such a way that the new sentence is inconsistent with
the video. We mark the model prediction as correct if the
ground truth sentence among the 5 captions has the highest
similarity score with the video. We sample 32 frames per
video at 25 fps with a stride of 14.
Kinetics-400 . We follow [57] to evaluate classiﬁcation in
a zero-shot setting: we feed in all class labels (without any
prompt) to the text tower and mark the prediction as correct
if the correct label has the highest similarity with the video.
For the ‘Kinetics-verb’ split, we restrict the evaluation to 97
classes which we manually identify as requiring verb under-
standing (see Sec. C.7). We note that we still feed in all 400
class labels for measuring classiﬁcation on Kinetics-verb.
We sample 32 frames per video at 25 fps with a stride of 14.
NEXT-QA . We concatenate the question and answer pairs
before passing them through the CLIP4CLIP text tower. We
mark the model prediction as correct if the correct question-
answer pair among the 5 options has the highest similarity
score with the video. We sample 32 frames per video at
25 fps with a stride of 24.
SVO probes . This is an image-text benchmark [30], specif-ically designed to measure progress in verb understanding.
We evaluate our baseline and VFC framework on a subset of
12,936 images from the original 14,102 images since some
images are no longer accessible (the corresponding urls are
corrupted). In [30], the authors calculate the accuracy of
positive and negative image-text pairs: they pass image-text
pairs through their model and label an image–sentence pair
as negative if the classiﬁer output is <0:5and positive oth-
erwise. Our model conﬁdences are calibrated differently,
therefore we instead report Average Precision (AP). To eval-
uate on this dataset, we simply replicate the image 32 times
as input to our video model.
C.5. PaLM prompting
PaLM hard negative generation. We include below
our full prompt template for automatic generation of hard
negatives. We insert the caption for which we want to
generate hard verb negatives at finput caption g.
In this task, you are given an input sentence. Your job is
to tell me 10 output sentences with a different meaning by
only changing the action verbs.
Input: A man walks up to a woman holding an umbrella in
a garden.
Outputs:
1) A man jumps up to a woman throwing an umbrella in a
garden.
2) A man runs up to a woman opening an umbrella in a
garden.
3) A man walks away from a woman buying an umbrella in
a garden.
4) A man throws up on a woman carrying an umbrella in a
garden.
5) A man punches a woman swinging an umbrella in a
garden.
6) A man sits with a woman wrapping up her umbrella in a
garden.
7) A man talks to a woman closing an umbrella in a garden.
8) A man ﬂirts with a woman playing with an umbrella in a
garden.
9) A man skips to a woman leaning on her umbrella in a
garden.
10) A man sprints to a man losing her umbrella in a garden.
Input: Surfers ride the waves in an ocean. Outputs:
1) Surfers get hit by the waves in an ocean.
2) Surfers swimming in the waves in an ocean.
3) Surfers meditating by the waves in an ocean.
4) Surfers drowning in the waves in an ocean.
5) Surfers asking for help in the waves in an ocean.
6) Surfers teaming up in the waves in an ocean.
7) Surfers snorkeling in the waves in the ocean.
8) Surfers taking photos by the waves in the ocean.
9) Surfers getting ready to go into the waves in the ocean.10) Surfers stretching by the waves in the ocean.
Input: A dentist holds the replica of a human mouth he
shows how important ﬂossing your teeth is.
Outputs:
1) A dentist cleans the replica of a human mouth he
presents how unimportant ﬂossing your teeth is.
2) A dentist breaks the replica of a human mouth he
screams how important ﬂossing your teeth is.
3) A dentist ﬁxes the replica of a human mouth he says how
important ﬂossing your teeth is.
4) A dentist buys the replica of a human mouth he explains
how important brushing your teeth is.
5) A dentist plays with the replica of a human mouth he
remembers about how important washing your teeth is.
6) A dentist tidies the replica of a human mouth he rambles
on about how important breaking your teeth is.
7) A dentist rotates the replica of a human mouth he
presents how important fracturing your teeth is.
8) A dentist places on his legs the replica of a human mouth
he shows how important ﬂossing your teeth is.
9) A dentist searches for the replica of a human mouth he
shows how important grinding your teeth is.
10) A dentist picks up the replica of a human mouth he
presents how important whitening your teeth is.
Input: Looks like a band playing on the stage and perhaps
Community Center and people gathered around watching.
Outputs:
1) Looks like a band ﬁghting on the stage and perhaps
Community Center and people gathered around crying.
2) Looks like a band dancing on the stage and perhaps
Community Center and people gathered around smiling.
3) Looks like a band singing on the stage and perhaps
Community Center and people gathered around ﬁlming.
4) Looks like a band bowing on the stage and perhaps
Community Center and people gathered around clapping.
5) Looks like a band making a speech on the stage and
perhaps Community Center and people gathered around
listening.
6) Looks like a band laughing on the stage and perhaps
Community Center and people gathered around cheering.
7) Looks like a band working on the stage and perhaps
Community Center and people gathered around standing.
8) Looks like a band holding hands on the stage and
perhaps Community Center and people gathered around
praying.
9) Looks like a band jumping on the stage and perhaps
Community Center and people gathered around encourag-
ing.
10) Looks like a band yelling on the stage and perhaps
Community Center and people gathered around watching.
Input: finput caption g
Outputs:PaLM verb phrase extraction. We use PaLM to extract
verb phrases from the original caption, where a verb
phrase can correspond to a single verb or a verb-noun pair
depending on the caption. We use PaLM-540B with output
sequence length 256, beam size of 4, and temperature of
0.2. We post-process the outputs by removing text after
any newline character. We include our full prompt template
for automatic extraction of verb phrases below. We insert
the caption for which we want to extract a verb phrase at
finput caption g.
In this task, you are given an input sentence. Your job is to
output the action verb phrases.
Input: the young girl in the middle of the road she is
dancing.
Output: [‘dancing’]
Input: a city area can be seen that has people in the
walkways of runways.
Output: []
Input: this is a video of a birthday and she has a green
colored dress and they are cutting a cake there’s a clown
on the side and the parents seem to be clap.
Output: [‘cutting cake’, ‘clapping’]
Input: one woman is talking to the camera about being safe
he has a shirt with pal pal on it in the greenery behind her.
Output: [‘talking to camera’]
Input: a bicycle with a specialized back wheel slides along
a wet paper.
Output: [‘sliding’]
Input: a person clicking an object that is connected to a
speaker.
Output: [‘clicking’]
Input: it’s a video of a football game and one of the blue
team is throwing the football really far into the endzone.
Output: [‘throwing football’]
Input: this is a video of someone ﬁling their nails.
Output: [‘ﬁling nails’]
Input: airplane with the words British Airways can be seen
over top.
Output: []
Input: man sitting standing at the front of the room is
giving speech and asking an audience if they’ve ever heard
of a speciﬁc song.
Output: [‘standing’, ‘giving speech’, ‘asking’]
Input: it shows a video of a man talking on the phone yeah
glasses and has a black phone.
Output: [‘talking on phone’]
Input: hitchhiker is on the side of the road by a truck stop
pulling a sign that says North.
Output: [‘pulling a sign’]
Input: this is a video of a man on a ladder the man is
cutting down a tree branch the man is wearing red.
Output: [‘cutting tree’]Input: on an indoor gym on a hard Brown meth there’s a
man young man with a barbell with lots of heavy weights
on each side and he has it over his head stiff arm straight
arm going to be and then he drops it on the ﬂoor while he
does so you can hear the clanking of the weight that they
smack against each other.
Output: [‘dropping’]
Input: he is using a large chainsaw to cut inside of a tree
branch.
Output: [‘cutting tree’]
Input: I meant stacking up his cups for cup stacking
concentration for a party.
Output: [‘stacking cups’]
Input: a large ﬁeld shown with garbage and water ﬂowing
through it.
Output: [‘water ﬂowing’]
Input: a washing machine washes the clothes.
Output: [‘washing clothes’]
Input: finput caption g
Output:
PaLM positive generation. We use PaLM to generate
positive sentences where the verb in the original cap-
tion is changed to a synonym verb, but the remaining
context is unchanged. We use PaLM-540B with output
sequence length 512, beam size of 1, and temperature
of 0.7. We post-process the outputs by removing text
after any newline character and by ﬁltering out candidates
which contain the same verbs as the original caption. We
include our full prompt template for automatic genera-
tion of positives below. We insert the caption for which
we want to generate a positive sentence at finput caption g.
In this task, you are given an input sentence. Your job is to
tell me 10 output sentences with the same meaning by only
changing the action verbs.
Input: A man walks up to a woman holding an umbrella in
a garden.
Outputs:
1) A man strolls up to a woman holding an umbrella in a
garden.
2) A man marches up to a woman holding an umbrella in a
garden.
3) A man strides up to a woman holding an umbrella in a
garden.
4) A man wanders up to on a woman carrying an umbrella
in a garden.
5) A man tramps up to a woman holding an umbrella in a
garden.
6) A man steps up to with a woman holding an umbrella in
a garden.
7) A man wanders up to a woman holding an umbrella in a
garden.8) A man treads up to a woman holding an umbrella in a
garden.
9) A man truges up to a woman holding an umbrella in a
garden.
10) A man treaks to a woman holding her umbrella in a
garden.
Input: A dentist holds the replica of a human mouth he
shows how important ﬂossing your teeth is.
Outputs:
1) A dentist grasps the replica of a human mouth he shows
how important ﬂossing your teeth is.
2) A dentist carries the replica of a human mouth he shows
how important ﬂossing your teeth is.
3) A dentist clutches the replica of a human mouth he shows
how important ﬂossing your teeth is.
4) A dentist grips the replica of a human mouth he shows
how important ﬂossing your teeth is.
5) A dentist holds the replica of a human mouth he explains
how important ﬂossing your teeth is.
6) A dentist holds the replica of a human mouth he presents
how important ﬂossing your teeth is.
7) A dentist holds the replica of a human mouth he demon-
strates how important ﬂossing your teeth is.
8) A dentist holds the replica of a human mouth he commu-
nicates how important ﬂossing your teeth is.
9) A dentist holds the replica of a human mouth he displays
how important ﬂossing your teeth is.
10) A dentist holds the replica of a human mouth he
highlights how important ﬂossing your teeth is.
Input: This is a video of somebody touching wood.
Outputs:
1) This is a video of somebody tapping wood.
2) This is a video of somebody stroking wood.
3) This is a video of somebody pressing wood.
4) This is a video of somebody handling wood.
5) This is a video of somebody patting wood.
6) This is a video of somebody brushing wood.
7) This is a video of somebody grazing wood.
8) This is a video of somebody poking wood.
9) This is a video of somebody caressing wood.
10) This is a video of somebody gripping wood.
Input: This is a video of a group of adults outside dancing.
Outputs:
1) This is a video of a group of adults outside whirling.
2) This is a video of a group of adults outside twirling.
3) This is a video of a group of adults outside swaying.
4) This is a video of a group of adults outside partying.
5) This is a video of a group of adults outside getting down.
6) This is a video of a group of adults outside spinning.
7) This is a video of a group of adults outside bouncing.
8) This is a video of a group of adults outside bopping.
9) This is a video of a group of adults outside waltzing.
10) This is a video of a group of adults outside prancing.Input: finput caption g
Outputs:
C.6. T5 generations
As well as using PaLM to generate hard verb nega-
tive captions, we experiment with using a bidirectional
language model, T5-Base [58]: a 220 million parameter
encoder-decoder Transformer. It is pretrained on the Colos-
sal Clean Crawled Corpus (C4) [22] on a multi-task mix-
ture of unsupervised and supervised tasks, with all tasks be-
ing converted into a text-to-text format. T5 is trained with
a Masked Language Modelling (MLM) loss, similarly to
BERT [16], with minor differences. MLM involves mask-
ing certain tokens in an input sequence before passing them
to the model, and tasking the model with predicting the
masked spans.
As T5 has been trained with a span-mask denoising ob-
jective, we use it at inference time in cloze form (ﬁll in the
blanks) to replace words in captions by targeted masking.
Speciﬁcally, our method consists of the following steps:
(1)Verb Identiﬁcation: we start by identifying verbs in
text captions, leveraging PoS tagging with NLTK [8].
(2)T5 prediction: We then replace the verb tokens with
a[MASK] token, and feed the masked sentence to T5.
We keep the Top- Kphrases predicted by the model (with
K= 50 ). Unlike [52], we do not ﬁne-tune T5 for verb
modelling speciﬁcally, but rather use it in a zero-shot set-
ting, which we ﬁnd is sufﬁcient to generate plausible nega-
tives.
(3)Negatives Filtering: TheKcandidate sentences are
then ﬁltered to remove sentences which contain the same
verbs as the original caption.
C.7. Kinetics-verb
In order to assess our method’s true verb understanding
in the downstream task of action classiﬁcation, we intro-
duce ‘Kinetics-verb’: a subset of 97 classes from Kinetics-
400 [11] where we isolate classes that share a common noun
with another class, but have a different verb (and therefore
action). We include the set of 97 classes below:
[hair: braiding hair, brushing hair, curling hair, dying hair,
ﬁxing hair, washing hair, getting a hair cut; nails : do-
ing nails, cutting nails; legs: waxing legs, massaging legs,
shaving legs, stretching leg, swinging legs; hands : wash-
ing hands, shaking hands, arm: stretching arm, exercising
arm, arm wrestling; watermelon : cutting watermelon, eat-
ing watermelon; ﬂoor : mopping ﬂoor, cleaning ﬂoor, sand-
ing ﬂoor, sweeping ﬂoor; baby : baby waking up, carrying
baby, crawling baby; back : waxing back, bending back,
massaging back; feet: massaging feet, washing feet; dog:
walking the dog, grooming dog, training dog; cake : eat-
ing cake, making a cake; guitar : strumming guitar, playingguitar, tapping guitar; cards : shufﬂing cards, playing cards;
present : wrapping present, opening present; egg: cooking
egg, egg hunting, scrambling eggs; shoes : shining shoes,
cleaning shoes; pool: cleaning pool, jumping into pool;
snow : biking through snow, shoveling snow; rope : skip-
ping rope, climbing a rope; ﬁsh: catching ﬁsh, feeding ﬁsh;
eyebrows : ﬁlling eyebrows, waxing eyebrows; computer :
using computer, assembling computer; tree: climbing tree,
planting trees, trimming trees; car: driving car, pushing
car; golf: golf chipping, golf driving, golf putting; beer :
drinking beer, tasting beer; horse : grooming horse, riding
or walking with horse; paper : folding paper, ripping pa-
per, shredding paper; ﬁre: extinguishing ﬁre, juggling ﬁre;
head : shaking head, shaving head; water : surﬁng water,
water skiing, water sliding; ice: ice climbing, ice ﬁshing, ice
skating; basketball : dunking basketball, dribbling basket-
ball, playing basketball, shooting basketball; ﬁnger : drum-
ming ﬁngers, ﬁnger snapping; baseball : catching or throw-
ing baseball, hitting baseball; soccer ball : juggling soccer
ball, kicking soccer ball]