LOCALISATION OF REGULARISED AND MULTIVIEW SUPPORT
VECTOR MACHINE LEARNING
AURELIAN GHEONDEA AND CANKAT T ˙ILK˙I
Abstract. We prove some representer theorems for a localised version of a semisupervised,
manifold regularised and multiview support vector machine learning problem introduced by
H.Q. Minh, L. Bazzani, and V. Murino, Journal of Machine Learning Research ,17(2016) 1–
72, that involves operator valued positive semidefinite kernels and their reproducing kernel
Hilbert spaces. The results concern general cases when convex or nonconvex loss functions
and finite or infinite dimensional underlying Hilbert spaces are considered. We show that the
general framework allows infinite dimensional Hilbert spaces and nonconvex loss functions for
some special cases, in particular in case the loss functions are Gˆ ateaux differentiable. Detailed
calculations are provided for the exponential least squares loss functions that lead to systems of
partially nonlinear equations for which a particular different types of Newton’s approximation
methods based on the interior point method can be used. Some numerical experiments are
performed on a toy model that illustrate the tractability of the methods that we propose.
1.Introduction
Representer theorems are of a special interest in machine learning due to the fact that they
reduce the problem of finding a minimiser for the learning map to the vector space spanned by
the kernel functions, or operators, at the labeled and unlabeled input data. For classical versions
of representer theorem, we recommend the monographs [61] and [66]. There is a large literature
on generalised representer theorems but in this article we refer to the unifying framework in
vector valued reproducing kernel Hilbert spaces for semisupervised, manifold regularised and
multiview machine learning, as investigated by [51] and the vast literature cited there.
The article [51] has remarkable contributions to the domain of representer theorems in sup-
port vector machine learning, firstly by unifying many variants of these theorems referring to
semisupervised, regularised, manifold regularised, multiview machine learning and then by con-
sidering underlying Hilbert spaces that are infinite dimensional. Recently, infinite dimensional
Hilbert spaces in learning with kernels have been of interest, e.g. see [40]. However, although
the general representer theorem, Theorem 2 in [51], is stated for infinite dimensional spaces,
this turns out to be problematic, as we will see in Remark 2.14. Also, there is an interest for
applications to learning problems in which loss functions may not be convex, cf. [71], or even
indefinite, cf. [39].
In this article we are concerned with questions triggered by the investigations in [51] and [71],
such as: to which extent can one allow the underlying input spaces be infinite dimensional and
to which extent nonconvex loss functions can be used in the learning process. In this respect,
we propose a localisation of the minimisation of the semisupervised regularised, manifold reg-
ularised, and multiview machine learning problem studied in [51], in the sense that the output
spaces and the loss functions may be different for each labeled and unlabeled input point. For
this approach we use a generalised version of vector valued reproducing kernel Hilbert spaces
2010 Mathematics Subject Classification. Primary 68T05; Secondary 46E22, 46G05.
Key words and phrases. operator valued reproducing kernel Hilbert spaces, regularised and multiview learn-
ing, support vector machine learning, loss functions, representer theorem.
1arXiv:2304.05655v4  [math.FA]  29 Nov 20242 A. GHEONDEA AND C. T ˙ILK˙I
with bundles of spaces and operators. We think that the localised framework offers more flexi-
bility to the learning problem and it is quite natural, especially when semisupervised multiview
learning is considered, that the output spaces and the loss functions depend locally on the input
points.
There are a few reasons that motivate the localised versions that we consider in this article.
Firstly, for some of the labeled input points, in the multivariable case, some of the components
of the labels (properties) may be missing or some additional components of the labels may
be necessary in order to allow reliable information. This means that the underlying vector
spaces of labels may have different dimensions and hence, making the vector spaces of the
labels depend on the input points solves this obstruction. Secondly, when the input set X
shows a certain homogeneity, the localised version may not be needed but, when this set is
more heterogeneous, the localised version brings the necessary flexibility. To be more precise,
let us imagine the following scenario. The input set Xis a finite union of sets Xi,i= 1, . . . , N ,
where each Xishows homogeneity. By homogeneity we mean that, for each i, the properties
(labels) associated to x∈Xiare of the same type, in particular they live in the same Hilbert
space Yi. However, for different i̸=j, where i, j∈ {1, . . . , N }, the properties (labels) of the
points x∈Xiwhen compared to the properties of the points x∈Xjmay be different, meaning
that the Hilbert spaces of labels YiandYjshould be different. For example, if the set Xis
the collection of all the cells of a body, we can see Xas the union of all its organs Xi, for
i= 1, . . . , N , and it is clear that due to special functions that different organs have in the body,
the properties (labels) of cells in different organs are generally of different type. Moreover,
the learning function, which is calculated in terms of loss functions, may require different loss
functions for different points that have different types of labels. For example, in the scenario
of the body viewed as the union of its organs, comparison of properties of different cells should
be performed differently for cells belonging to different organs. This justifies the dependence
of the loss functions of the input points as well. As a further research project, the localised
version of the machine learning problem might be used for investigating the process of spreading
a malignant tumor (metastasis) in different organs of a body, provided that real data will be
available and appropriate mathematical models will be obtained.
Following [51], the direction of multiview learning we consider in this work is coregularisation,
see e.g. [12], [64], [59], and [67]. In this approach, different hypothesis spaces are used to
construct target functions based on different views of the input data, such as different features
or modalities, and a data dependent regularisation term is used to enforce consistency of output
values from different views of the same input example. The resulting target functions, each
corresponding to one view, are then naturally combined together in a certain fashion to give
the final solution.
The direction of semisupervised learning we follow here is manifold regularisation, cf. [7],
[13], and [50], which attempts to learn the geometry of the input space by exploiting the given
unlabeled data. The latter two papers are recent generalisations of the original scalar version of
manifold regularisation of [7] to the vector valued setting. In [13], a vector valued version of the
graph Laplacian Lis used while in [50] Lis a general symmetric, positive operator, including
the graph Laplacian. The vector valued setting allows one to capture possible dependencies
between output variables by the use of, for example, an output graph Laplacian. For a com-
prehensive discussion on semisupervised learning and a thorough comparison with supervised
and unsupervised learning, see the collection [19].
Because reproducing kernel Hilbert spaces make an essential ingredient in these representer
type theorems, some historical considerations are in order. The classical article of [2] providesLOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 3
an abstract formalisation of scalar reproducing kernel Hilbert spaces of many previous investi-
gations and applications related to spaces of analytic functions, partial differential equations,
and harmonic analysis, as of [47], [8], [52], [11], and [27]. From the point of view of prob-
ability theory, [36] investigated linearisations, or feature spaces, associated to scalar positive
semidefinite kernels. An equivalent formulation of reproducing kernel Hilbert spaces by Hilbert
spaces continuously embeded in a quasicomplete locally convex space was investigated by [63]
while a more group theoretical approach was performed in [37] and [38]. Then, scalar valued
reproducing kernel Hilbert spaces found many applications in machine learning, see [66] and
[61] for a comprehensive list of literature in this direction.
Motivated by problems in operator theory and operator algebras, operator valued positive
semidefinite kernels and either their linearisations (Kolmogorov decompositions) or their repro-
ducing kernel Hilbert spaces have been considered by [68], [56], [54], [24], and [20], to cite a few.
Investigations of operator valued Hermitian kernels that yield Krein spaces have been performed
by [21], [22]. More general operator valued positive semidefinite kernels that yield reproducing
kernel VH-spaces have been considered by [26], [3], [4], and [5]. To make things more precise,
recall that a vector space Sendowed with a map S ∋s7→s∗∈ Swhich is conjugate linear and
involutive, in the sense that ( s∗)∗=sfor all s∈ S, is called a ∗-vector space . If, in addition, a
convex cone S+onSis specified such that for any s∈ S+we have s=s∗, we call San ordered
∗-space and, when a complete locally convex topology on Sis specified and which is related
in a certain fashion with the convex cone S+, one calls Sanadmissible space . The concept of
admissible space is a generalisation of the concept of C∗-algebra which is a mainstream domain
in functional analysis, partly due to its strong connections with quantum theories, e.g. see [23]
and [10]. Admissible spaces Sare then used to consider gramians [ ·,·]:X × S → S , for some
vector space X, which are vector valued inner products and that induce a certain topology on
X. If this topology is complete then we call Xa VH-space (Vector Hilbert space). The concept
of VH-space is a generalisation of the concept of Hilbert modules over C∗-algebras, e.g. see [41],
but it appeared independently and related to problems in probability theory, see [42], [43], and
[44]. In case we have a positive semidefinite kernel K:X×X→ S, for some admissible space
S, one can define a reproducing kernel VH-space as a generalisation of the classical reproducing
kernel Hilbert space. For details and many examples see, for example, [4].
During the last twenty years, operator valued positive semidefinite kernels and their repro-
ducing kernel Hilbert spaces and feature spaces (linearisations, or Kolmogorov decompositions)
became of interest in the theory of machine learning, see [48], [18], [17], [50], [35], but the
investigations have been somehow started from scratch, apparently unaware of the previous
works. More recently, in a sequence of articles, [29], [30], [31], [32], and [33], it is shown that,
using the C∗-algebra-valued gramians (vector valued inner products), one can learn function-
and operator-valued maps, one can design positive definite kernels for structured data using the
noncommutative product, one can use the norm of the∗-algebra to alleviate the dependency
of generalisation error bound on the output dimension using the generalisation of kernel mean
embedding by means of C∗-algebras, one can analyse positive operator valued measures and
spectral measures, one can continuously combine multiple models and use the tools for func-
tions which can be applied to ensemble, multitask, and meta-learning (the noncommutative
product structures in C∗-algebras induce interactions among models), and one can construct
group equivariant neural networks using the products in group C∗-algebras.
We briefly describe the contents of this article. In order to introduce the localised version
of the semisupervised, regularised, manifold regularised, and multiview learning problem, in
Subsection 2.1 we firstly consider operator valued positive semidefinite kernels for which the4 A. GHEONDEA AND C. T ˙ILK˙I
entries are localised by a bundle of Hilbert spaces. For these kernels, we show how their repro-
ducing kernel Hilbert spaces are constructed, their relation to the linearisations (Kolmogorov
decompositions, feature spaces) and basic properties. Although we have been inspired by the
approach in [21], we provide in the appendices proofs for all the statements we make since
in this form they cannot be found elsewhere. In [56], reproducing kernel Hilbert spaces are
considered for kernels over bundles of locally convex spaces and this might be a very interesting
research problem in case applications to machine learning theory might be found.
Then, in Subsection 2.2 we present the localised version of the semisupervised, regularised,
manifold regularised, and multiview machine learning problem inspired by [51]. Under rather
general assumptions, we prove in Subsection 2.1 the representer theorem for general loss func-
tions but assuming the input spaces at all input points be finite dimensional. The finite dimen-
sionality assumption can be relaxed to the condition that the span of the input kernel operators
at the input points is closed, as Proposition 2.10 shows. In this section, most of the underlying
Hilbert spaces may be complex and only in some special cases we have to impose the condition
that they are real.
Further on, in Subsection 3.2 we consider real underlying Hilbert spaces and loss functions
that are Gˆ ateaux differentiable and then, in Theorem 3.7 we show that, under this additional
assumption, a general representer theorem can be obtained for infinite dimensional input spaces.
In Subsection 3.3 we work out the details for the loss functions defined by the least squares which
leads to a linear problem in terms of the unkown coefficients, as in [51], while in Subsection 3.4
we work out the details for the loss functions defined by the exponential least squares which
lead to a mixed problem, linear and nonlinear. Finally, in Subsection 3.5 we tackle algorithms
to obtain approximations of the solutions for the latter machine learning problem by damped
Newton approximation methods, as presented in [53], [15], [16], [70]. Since in the framework
of systems of nonlinear equations solutions are generally hot topics in current research, much
work remains to be done. Some numerical experiments are performed on a toy model for the
latter case and the algorithm is tested. We show the robustness of the method on this toy
model.
Acknowledgements : The authors thank Naci Saldi and Sorin Costiner for very helpful
discussions on available algorithms to solve nonlinear systems of equations and optimisation
problems.
2.A General Semisupervised Regularised and Multiview Machine Learning
Problem
2.1.Operator Valued Kernels. The aim of this subsection is to introduce kernels that take
values in bundles of bounded linear operators on different Hilbert spaces and which will make
the theoretical foundations for the localised machine learning problem. Actually, we prove that
passing from nonlocalised to localised machine learning problem has the advantage of being
very flexible and allowing a very large nonhomogeneity of the input data while not bringing
new obstructions. In particular, this shows that, with minimal changes, similar results and
algorithms can be obtained for localised machine learning problems.
LetXbe a nonempty set and H={Hx}x∈Xa bundle of Hilbert spaces over the field F, that
is either RorC, with base X, that is, Hxis a Hilbert space over F, for any x∈X. In order
to avoid confusion, let us note that the Hilbert spaces Hxare actually tagged Hilbert spaces,
meaning that for x̸=ythe spaces HxandHyare disjoint. An H-operator valued kernel K
is a mapping defined on X×Xsuch that K(y, x)∈ B(Hx,Hy) for all x, y∈X. Here and
throughout this article, if HandKare two Hilbert spaces over the same filed F, we denote byLOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 5
B(H,K) the Banach space of all linear and bounded operators T:H → K , endowed with the
operator norm. We denote by K(X;H) the collection of all H-operator valued kernels on X
and it is clear that K(X;H) is a vector space over F.
Given K∈K(X;H), the adjoint kernel K∗is defined by K∗(x, y) =K(y, x)∗for all x, y∈X.
Clearly K∗∈K(X;H). The kernel Kis called Hermitian orsymmetric ifK=K∗. IfF=C
then any kernel Kis a linear combination of two Hermitian kernels, more precisely, letting
ℜ(K) := ( K+K∗)/2,ℑ(K) := ( K−K∗)/2i, (2.1)
we have
K=ℜ(K) + iℑ(K). (2.2)
It is easy to see that K7→K∗is an involution, that is, it is conjugate linear and involutive.
In this way, K(X;H) is a∗-vector space, that is, K(X;H) is a vector space endowed with an
involution, e.g. see [3].
LetF(X;H) be the vector space over Fof all cross-sections f:X→S
x∈XHx, that is,
f(x)∈ H xfor all x∈X. Addition and multiplication with scalars in F(X;H) are defined
pointwise. Equivalently, F(X;H) can be naturally identified with the vector spaceQ
x∈XHxand
then any cross-section f∈Q
x∈XHxcan be written f={fx}x∈X. For each x∈Xandh∈ H x
we consider the cross-section bh∈ F(X;H), defined by
bh(y) =(
h, ify=x,
0Hy,otherwise.(2.3)
In particular, since Hconsists of tagged Hilbert spaces this means that if either h∈ H xor
l∈ H yare not null and x̸=ythenbh̸=bl. However, if h= 0Hxandl= 0Hywith x̸=y, then
bh=bl= 0F(X;H). Also, for any f∈ F(X;H) we have
f=X
x∈Xbfx, (2.4)
where fx:=f(x) for all x∈X. Clearly, for each y∈Xthe sumP
x∈Xbfx(y) has at most one
nonnull term and hence the sum in (2.4) converges pointwise.
LetF0(X;H) be the vector subspace consisting of all f∈ F(X;H) with finite support.
Clearly, any cross-section of type bh, for some h∈ H x, belongs to F0(X;H) and, for any
f∈ F 0(X;H) there exists uniquely distinct elements x1, . . . , x n∈Xandhi∈ H xi,i= 1, . . . , n ,
such that
f=nX
i=1bhi.
An inner product ⟨·,·⟩0:F0(X;H)× F 0(X;H)→Fcan be defined by
⟨f, g⟩0=X
x∈X⟨f(x), g(x)⟩Hx, f, g ∈ F 0(X;H). (2.5)
In addition, let us observe that the sum in (2.5) makes sense in the more general case when
f, g∈ F(X;H) and at least one of forghas finite support, the other can be arbitrary.
Associated to the kernel K∈K(X;H) there is a sesquilinear form ⟨·,·⟩K:F0(X;H)×
F0(X;H)→Fdefined by
⟨f, g⟩K=X
x,y∈X⟨K(y, x)f(x), g(y)⟩Hy, f, g ∈ F 0(X;H), (2.6)6 A. GHEONDEA AND C. T ˙ILK˙I
that is, ⟨·,·⟩Kis linear in the first variable and conjugate linear in the second variable. Also,
the sesquilinear form ⟨·,·⟩Kis Hermitian, that is, ⟨f, g⟩K=⟨g, f⟩Kfor all f, g∈K(X;H), if
and only if the kernel Kis Hermitian.
Aconvolution operator CK:F0(X;H)→ F (X;H) can be defined by
(CKf)(y) =X
x∈XK(y, x)f(x), f∈ F 0(X;H), y∈X. (2.7)
Clearly CKis a linear operator and, with notation as in (2.5) and (2.6) we have
⟨CKf, g⟩0=⟨f, g⟩K, f, g ∈ F 0(X;H). (2.8)
By definition, the kernel Kispositive semidefinite if the sesquilinear form ⟨·,·⟩Kis nonnega-
tive, that is, if ⟨f, f⟩K≥0 for all f∈ F 0(X;H), equivalently, if for all n∈N, allx1, . . . , x n∈X,
and all h1∈ H x1, . . . , h n∈ H xn, we have
nX
i,j=1⟨K(xj, xi)hi, hj⟩Hxj≥0. (2.9)
An equivalent way of expressing (2.9) is to say that the operator block matrix [ K(xj, xi)]n
i,j=1,
when viewed as a bounded linear operator acting in the orthogonal direct Hilbert sum Hx1⊕
··· ⊕ H xn, is a positive semidefinite operator. On the other hand, the kernel Kis positive
semidefinite if and only if the convolution operator, as defined in (2.7), is positive semidefinite
when viewed as an operator on the inner product space ( F0(X;H);⟨·,·⟩0), more precisely,
⟨CKf, f⟩0≥0, f∈ F 0(X;H). (2.10)
It is easy to see that, if F=Cthen, if the kernel Kis positive semidefinite then it is Hermitian.
IfF=Rthen this is not true and hence, for this case we confine to those positive semidefinite
kernels that are Hermitian, more precisely, in this case, in addition to the property (2.9), by a
positive semidefinite kernel we implicitly understand that it is Hermitian as well. The collection
of all positive semidefinite H-operator valued kernels on Xis denoted by K+(X;H) and it is
easy to see that K+(X;H) is a strict convex cone of the ∗-vector space K(X;H).
Given an arbitrary bundle of Hilbert spaces H={Hx}x∈Xand an H-operator valued Kernel
k, aHilbert space linearisation , or a Kolmogorov decomposition , or a feature pair ofKis, by
definition, a pair ( K;V) subject to the following conditions.
(kd1) Kis a Hilbert space over F.
(kd2) V={V(x)}x∈Xis an operator bundle such that V(x)∈ B(Hx,K) for all x∈X.
(kd3) K(x, y) =V(x)∗V(y) for all x, y∈X.
The linearisation ( K;V) is called minimal if
(kd4) Kis the closed span of {V(x)Hx|x∈X}.
The following theorem is a general version of some classical results, e.g. [36], [54], [24]. This
is also a special case of [21]. We include its proof in the Appendix A.
Theorem 2.1. Given an arbitrary bundle of Hilbert spaces H={Hx}x∈Xand an H-operator
valued kernel K, the following assertions are equivalent.
(a)Kis positive semidefinite.
(b)Khas a Hilbert space linearisation.LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 7
In addition, if Kis positive semidefinite then a minimal Hilbert space linearisation (K;V)
exists and it is unique, modulo unitary equivalence, that is, for any other minimal Hilbert space
linearisation (K′;V′)ofKthere exists a unitary operator U:K′→ K such that V(x) =UV′(x),
for all x∈X.
Due to the uniqueness part in the previous theorem, for any K∈K+(X;H), we denote by
(KK;VK) the minimal Hilbert space linearisation of K, as constructed during the proof of the
implication (a) ⇒(b).
LetXbe a nonempty set and H={Hx}x∈Xa bundle of Hilbert spaces over F. Given an H-
operator valued kernel K, areproducing kernel Hilbert space associated to Kis, by definition,
a Hilbert space R ⊆ F (X;H) subject to the following conditions.
(rk1)Ris a subspace of F(X;H), with all induced algebraic operations.
(rk2) For all x∈Xandh∈ H x, the cross-section Kxh:=K(·, x)hbelongs to R.
(rk3) For all f∈ R we have ⟨f(x), h⟩Hx=⟨f, K xh⟩R, for all x∈Xand all h∈ H x.
Consequently, the following minimality condition holds as well:
(rk4) The span of {Kxh|x∈X, h∈ H x}is dense in R.
Also, it is worth mentioning that by (rk2), for each x∈X, we actually have a bounded linear
operator Kx:Hx→ R defined by Kxh:=K(·, x)h, for all h∈ H x. This operator is bounded,
as proven in (4.1). The following result is a generalisation of Moore-Aronszajn Theorem, [52],
[2], [48], [18], [17], [50], [35]. Also, it is a special case of [21] and it is proven in the Appendix B.
In Appendix C we present a more direct construction of the reproducing kernel Hilbert space
induced by an operator valued positive semidefinite kernel.
Theorem 2.2. Given an arbitrary bundle of Hilbert spaces H={Hx}x∈Xand an H-operator
valued kernel K, the following assertions are equivalent.
(a)Kis positive semidefinite.
(b) There exists a reproducing kernel Hilbert space Rhaving Kits reproducing kernel.
In addition, the reproducing kernel Hilbert space Ris uniquely determined by its reproducing
kernel K.
Remark 2.3. There is a natural bijective transformation between the unitary equivalency class
of minimal linearisations ( K;V) ofKand the reproducing kernel Hilbert space R(K). The
transformation from a minimal linearisations ( K;V) to the reproducing kernel Hilbert space
R(K) is described during the proof of the implication (a) ⇒(b) of Theorem 2.2, see Appendix
B. In the following we describe the inverse of this transformation.
Let (R;⟨·,·⟩R) be a reproducing kernel Hilbert space with reproducing kernel K. We define
the operator bundle V={V(x)}x∈Xby
V(x)h=Kxh, x ∈X, h∈ H x, (2.11)
and remark that V(x):Hx→ R for all x∈X. By means of the reproducing property (rk3) of
the kernel K, we have
⟨V(x)h, V(x)h⟩R=⟨Kxh, K xh⟩R=⟨K(x, x)h, h⟩Hx≤ ∥K(x, x)∥∥h∥2
Hx, x∈X, h∈ H x,
hence V(x)∈ B(Hx,R). Also, using once more the reproducing property (rk3) of K, it follows
that, for all x, y∈X,h∈ H x, and g∈ H y, we have
⟨V(y)∗V(x)h, g⟩Hy=⟨V(x)h, V(y)g⟩R=⟨Kxh, K yg⟩R=⟨K(y, x)h, g⟩Hy.8 A. GHEONDEA AND C. T ˙ILK˙I
Therefore, K(y, x) =V(y)∗V(x) for all x, y∈Xand hence, ( R;V) is a linearistion of K. In
addition, using the minimality property (rk3), it is easy to see that the linearisation ( R;V) is
minimal as well.
One of the most important property of a reproducing kernel Hilbert space consists in the
fact that, as a function space, its topology makes continuous all evaluation operators, see the
proof in the Appendix D.
Theorem 2.4. With notation as before, let Hbe a Hilbert space in the vector space F(X;H).
The following assertions are equivalent.
(a)His a reproducing kernel space of H-valued maps on X.
(b) For any x∈Xthe linear operator H ∋f7→f(x)∈ H xis bounded.
In connection to the previous theorem it is worth mentioning that, for a reproducing kernel
Hilbert space HK⊆ F(X;H) and for arbitrary x∈X, the evaluation operator HK∋f7→
f(x)∈ H xcoincides with K∗
x:HK→ H x, where Kx:Hx→ H Kis the bounded operator, see
the axiom (rk2), defined by Kxh:=K(·, x)h, for all h∈ H x.
2.2.Localisation of Semisupervised, Regularised, and Multiview Learning. LetX
be a nonempty set and W={Wx}x∈Xbe a bundle of Hilbert spaces on X. In this section, it
is not important whether the Hilbert spaces are complex or real, hence all Hilbert spaces are
considered to be over the field F, that is either CorR. There is a difference between the complex
and the real case consisting in the fact that in the latter case, for positive semidefiniteness we
assume also the symmetry, or Hermitian, property, while in the complex case, the symmetry
property is a consequence of the positive semidefiniteness. If Kis a positive semidefinite W-
operator valued kernel, we let HKbe its reproducing kernel Hilbert space, as in the previous
subsection. Also, let Y={Yx}x∈Xbe a bundle of Hilbert spaces.
Forl, u∈N, consider input distinct points x1, . . . , x l+u∈X. Here x1, . . . , x lare the labeled
input points while xl+1, . . . , x l+uare the unlabeled input points. More precisely, there are given
y1, . . . , y loutput points, such that yj∈ Y xjfor all j= 1, . . . , l . Then, for the general data let
x:= (xj)l+u
j=1,y:= (yj)l
j=1,z:= 
(xj)l+u
j=l+1,(yj)l
j=1
.
The input points x1, . . . , x l+uare randomly selected with respect to an unknown probability
and then, depending on the concrete problem, the labels y1, . . . , y lare produced in a certain
way.
LetWl+udenote the Hilbert space
Wl+u=l+uM
j=1Wxj. (2.12)
Forf∈ H Klet
f:= (f(x1), . . . , f (xl+u))∈Wl+u. (2.13)
Also, there is given a (Hermitian, if F=R) positive semidefinite operator M∈ B(Wl+u)
represented as an operator block ( l+u)×(l+u)-matrix M= [Mj,k], with Mj,k∈ B(Wxk,Wxj)
for all j, k= 1, . . . , l +u. Let V={Vx}x∈Xbe a bundle of maps, loss functions, where
Vx:Yx× Y x→Ris a function, for all x∈X. Also, C={Cx}x∈Xis a bundle of bounded
linear operators, where Cx:Wx→ Y xfor all x∈X. The general minimisation problem is
fz,γ= argminf∈HK1
llX
j=1Vxj(yj, Cxjf(xj)) +γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u, (2.14)LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 9
where γ= (γA, γI) and γA>0 and γI≥0 are the regularisation parameters.
The optimisation problem (2.14) is a localised version of the general vector valued reproduc-
ing kernel Hilbert space for semisupervised, regularised, manifold regularised and multiview
learning as in [51]. It is also useful to introduce the map to be minimised
I(f) :=1
llX
j=1Vxj(yj, Cxjf(xj)) +γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u
and, since f(x) =K∗
xffor all f∈ H Kand all x∈X, it equals
=1
llX
j=1Vxj(yj, CxjK∗
xjf) +γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u, (2.15)
In the following we explain the terms in the minimising map (2.15) and their significance from
the point of view of machine learning. Firstly, why labeled and unlabeled data? Traditionally,
in machine learning there are three fundamental approaches: supervised learning, unsupervised
learning, and reinforcement learning, but the last one is out of our concern. We firstly recall
the meaning and limitations of the first two approaches. Supervised learning means that the
training of the machine learning model is using exclusively labeled dataset. The input points
and the labels are selected according to a probability that is usually unknown or the input
points are selected according to an unknown probability and then the labels are produced in
a certain fashion. Basically, this means that a label is a description showing a model, what
it is expected to predict. But supervised learning has some limitations since this process is:
slow, because it requires human experts to either manually label training examples one by one
or carefully supervise the procedure, and costly , because, in order to obtain reliable results, a
model should be trained on the large volumes of labeled data to provide accurate predictions.
Unsupervised learning is that approach when a model tries to find hidden patterns, differences,
and similarities in unlabeled data by itself, without human supervision. Most of the time, in this
approach, data points are grouped into clusters based on similarities. But, while unsupervised
learning is a cheaper way to perform training tasks, it has other limitations: it has a limited
area of applications , mostly for clustering purposes, and provides less accurate results .
Semisupervised learning combines supervised learning and unsupervised learning techniques
to solve some important issues: we train an initial model on a few labeled samples and then
iteratively apply it to a greater number of unlabeled data. Unlike unsupervised learning, semisu-
pervised learning works for a larger variety of problems: classification, regression, clustering,
and association. Unlike supervised learning, the method uses small amounts of labeled data
but large amounts of unlabeled data, with the advantage that it reduces the costs on human
work and the data preparation time, while the accuracy of results is not altered. Of course,
some other issues show up: the unlabeled points should show certain consistency and for this
some regularisation techniques are needed. A comprehensive discussion on this subject can be
found in the collection [19].
Secondly, the reproducing kernel Hilbert space HKis associated to a vector valued positive
semidefinite kernel for several reasons, but mainly because this is related to the multiview
learning, cf. [25], [48], [64], [59], [50], [45], [35], [51], [29], [30], [31], [32]. In this article we
consider localised versions of these operator valued reproducing kernel Hilbert spaces that
offers flexibility for a larger class of learning problems, as explained in the Introduction, and
does not bring additional obstructions, as proven in Subsection 2.1.10 A. GHEONDEA AND C. T ˙ILK˙I
Further on, the first term in (2.15) controls the distance, estimated by local loss (or cost)
functions at the labeled input points with respect to the labels. More precisely, for each label
point xj,j= 1, . . . , l the label yj∈ Y xjis compared, through the cost function Vxj, with
f(xj)∈ W xjby a combination operator Cxj:Wxj→ Y xj, because the Hilbert spaces Yxjmay
be different from Wxj.
Example 2.5. Following [51], for an input point x∈Xconsider the label Hilbert space
Yand let W=Ym, the orthogonal direct sum of mcopies of Y. With this notation, the
kernel Khas values in B(W). A multiview f(x) is then an m-tuple ( f1(x), . . . , fm(x))T, with
each fi(x)∈ Y and let the combination operator C= [C1, . . . , C m]:W=Ym→ Y , that is,
Cf(x) =C1f1(x) +···+Cmfm(x)∈ Y.
In this article, the loss functions are also localised and one strong reason for this is that,
depending on different purposes that this semisupervised learning is used for, there is a very
large pool of choices for loss functions.
Example 2.6. We list in the following a few loss functions of interest in machine learning, see
[71] and [39] for a more comprehensive list and applications.
(1)Least Squares. The least squares loss function is
V(y, z) = (y−z)2, y, z ∈R.
It is convex, nonnegative, and differentiable.
(2)Sigmoid. The sigmoid loss function is
V(y, z) =1
1 + exp( z−y), y, z ∈R.
It is nonnegative, differentiable, and nonconvex.
(3)Hinge. The hinge loss function is
V(y, z) = max {0,1−yz}, y, z ∈R.
It is nonnegative, continuous, convex, but not differentiable.
(4)Exponential Least Squares. The exponential least squares function is
V(y, z) = 1−exp(−(y−z)2), y, z ∈R.
It is nonnegative, upper bounded by 1, differentiable, and nonconvex.
(5)Leaky Hockey Stick. The leaky hockey stick loss function is
V(y, z) =(
−log(zy), yz > 1,
1−yz, yz ≤1.
It is upper and lower unbounded, convex, and differentiable.
The second term in (2.15) is the usual regularisation penalty term, following the Tikhonov
regularisation method, cf. [69]. This is used in order to avoid large target functions fand over-
fitting, that is, optimising functions that match very accurately the labeled data but perform
badly for other data. Because of this, the regularisation parameter γAis always positive. In
the literature, sometimes the second term is replaced by φ(∥f∥HK), where φ:R+→R+is an
increasing function so, in our case φ(t) =γAt2.
The third term in (2.15) combines vector valued manifold regularisation, cf. [50], with mul-
tiview regularisation, cf. [59] and [67]. The parameter γImay be taken 1, without loss ofLOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 11
generality, since it can be absorbed in M. Following [51], the operator multiview regularisation
term γI⟨f, Mf⟩Wl+uis decomposed as
γI⟨f, Mf⟩Wl+u=γB⟨f, MBf⟩Wl+u+γW⟨f, MWf⟩Wl+u, (2.16)
where MB, MW∈ B(Wl+u) are selfadjoint positive operators and γB, γW≥0. As before, the
regularising parameters γBandγWmay be taken 1, without loss of generality, because they can
be absorbed in MBandMW, respectively. The first term in (2.16) is the localised between-view
regularisation while the latter term in (2.16) is the localised within-view regularisation . In the
next example we show by some concrete situations the constructions of the operators MBand
MWand their significance.
Example 2.7. This example follows closely the example of between-view regularisation as in
[51]. With notation as in Example 2.5, let Mm=mIm−1m1T
m, where 1m= (1,1, . . . , 1)T.
More precisely, Mmis the m×mmatrix with all entries equal to m−1 throughout its diagonal
and−1 elsewhere. Then, for each a= (a1, . . . , a m)T∈Rm, we have
aTMma=mX
j,k=1, j<k(aj−ak)2.
Then, for each y= (y1, . . . , y m)T∈ Ym=Wwe have
yT(Mm⊗IY)y=X
j,k=1, j<k∥yj−yk∥2
Y.
So, letting MB=Iu+l⊗(Mm⊗IY), for each f= (f(x1), . . . , f (xu+l)∈ Ym(u+l)=Wu+l, with
f(xi)∈ Ym=W, we have
⟨f, MBf⟩Wul=u+lX
i=1⟨f(xi),(Mm⊗IY)f(xi)⟩W=u+lX
i=1X
j,k=1, j<k∥fj(xi)−fk(xi)∥2
Y.
This term is a control on the consistency between different components fi’s which represent
the outputs on different views.
Example 2.8. This example follows essentially [51] for a within-view manifold regularisation
via multiview graph Laplacians in support vector machine learning, cf. [67]. For manifold
regularisation, a data adjaceny graph is defined in such a way that the entries measure the
similarity or closeness of pairs of inputs. Given an undirected graph G= (V, E), where the
vertices are V={1, . . . , n }and edges are simply pairs ( j, k), assume that for each edge ( j, k)∈E
there is a weight wj,k, and to each edge ( j, k)̸∈Ewe let wj,k= 0, in such a way that the weight
matrix W= [wj,k] is Hermitian and nonnegative (positive semidefinite).
For example, when each vertex jis associated to a vector hj∈Rd, we can use the Gaussian
weights
wj,k= exp( −∥hj−hk∥2/2σ2). (2.17)
In order to simplify the complexity of calculations, cf. [67], for most of the edges ( j, k) we take
wj,k= 0 and only for neighbouring ( j, k), that is, ∥hj−hk∥2≤ϵ, for some ϵ, we define the
weights by (2.17).
Further on, letting vj,j=Pn
k=1wj,kandvj,k= 0 if j̸=k, we make the diagonal matrix
V= [vj,k]. We work under the assumptions that vj,j>0 for all j= 1, . . . , n . Then the graph
Laplacian matrix is L:=V−W, which is positive semidefinite. Sometimes it is useful to work
with the normalised graph Laplacian eL:=V−1/2LV−1/2.12 A. GHEONDEA AND C. T ˙ILK˙I
But, because the learning is from multiviews, this should be performed for each view and
then aggregated in a consistent fashion. From now on we use the same notations and settings
as in Example 2.5 and Example 2.7. Assume that, to each view i, 1≤i≤m, we consider the
undirected graph Gi= (Vi, Ei) where Vi={1, . . . , u +l}, letWi= [wi
j,k] be the corresponding
weight matrix that is Hermitian and nonnegative, and let Li= [li
j,k] be the corresponding graph
Laplacian. Then, for each vector a∈Ru+lwe have
aTLia=l+uX
j,k=1, j<kwi
j,k(aj−ak)2.
Now we aggregate the graph Laplacians into the multiview graph Laplacian as a block matrix
L= [Lj,k], where for each j, k= 1, . . . , u +lwe define
Lj,k= diag( l1
j,k, . . . , lm
j,k).
This implies that for each vector a= (a1, . . . , a u+l), with aj= (a1
j, . . . , am
j)∈Rmfor each
j= 1, . . . , u +l, we have
aTLa=mX
i=1l+uX
j,k=1, j<kwi
j,k(ai
j−ai
k)2.
Finally, letting MW:=L⊗IY, we have
⟨f, MWf⟩Wu+l=mX
i=1l+uX
j,k=1, j<kwi
j,k∥fi(xj)−fi(xk)∥2
Y,
for all f={fi(xj)|i= 1, . . . , m, j = 1, . . . , u +l} ∈ Wu+l=Ym(u+l). Each term in the leftmost
sum is a manifold regularisation for the view iand hence the double sum is the aggregated
manifold regularisation for all views. In this fashion, consistency is enforced for each view.
2.3.A Representer Theorem. We continue to use the notation as in the previous subsection.
Generally speaking, a representer theorem has the goal to prove that the optimal solution to
the problem (2.14) should belong to the space
HK,x=nl+uX
i=1Kxiwi|wi∈ W xio
. (2.18)
LetHK,xdenote its closure in HKand let PHK,xdenote the orthogonal projection of HKonto
HK,x. Let the sampling operator Sx:HK→Wl+ube defined by
Sxf= (K∗
xif)l+u
i=1= (f(xi))l+u
i=1=f, f∈ H K, (2.19)
where x= (x1, . . . , x l+u) and we have taken into account that f(x) =K∗
xffor all f∈ H Kand
allx∈X. Let also EC,x:HK→Yl, where
Yl:=lM
j=1Yxj, (2.20)
be defined by
EC,xf= 
Cx1K∗
x1f, . . . , C xlK∗
xlf
= (Cx1f(x1), . . . , C xlf(xl)), f∈ H K. (2.21)
The main technical fact used in this section is a lemma whose proof is inspired by the proof
of Theorem 2 in [51].LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 13
Lemma 2.9. With notation and assumptions an in Subsection 2.2 and as before, for any
f∈ H Kthe following inequality holds.
I(PHK,xf)≤ I(f).
Proof. We have the decomposition
HK=HK,x⊕ H⊥
K,x. (2.22)
Letf∈ H⊥
K,xbe fixed. Then, for any b∈Yl, since C∗
xibi∈ W xi, for all i= 1, . . . , l +u, and
hence
lX
i=1KxiC∗
xibi∈ H K,x,
we have
⟨EC,xf,b⟩Yl=
f, E∗
C,xb
HK=lX
i=1
f, K xiC∗
xibi
HK=⟨f,lX
i=1KxiC∗
xibi⟩HK= 0.
Consequently,
EC,xf= (Cx1K∗
x1f, . . . , C xlK∗
xlf) = 0 . (2.23)
Similarly, by the reproducing property, letting w= (w1, . . . , w l+u) be an arbitrary vector in
Wl+u, we have
⟨Sxf,w⟩Wl+u=l+uX
i=1⟨f(xi),w⟩Wl+u=l+uX
i=1⟨f, K xiwi⟩HK=*
f,l+uX
i=1Kxiwi+
HK= 0,
hence
f=Sxf= (f(x1), . . . , f (xl+u)) = 0 . (2.24)
For arbitrary f∈ H K, in view of the decomposition (2.22), we have the unique decomposition
f=f0+f1with f0∈HK,xandf1∈ H⊥
K,x, that is, f0=PHK,xf. Then,
∥f0+f1∥2
HK=∥f0∥2
HK+∥f1∥2
HK,
and consequently,
I(f) =I(f0+f1) =1
llX
i=1Vxi(yi, CxiK∗
xif0+CxiK∗
xif1) +γA∥f0∥2
HK+γA∥f1∥2
HK
+γI⟨Sxf0, MS xf0⟩Wl+u+γI⟨Sxf0, MS xf1⟩Wl+u
+γI⟨Sxf1, MS xf0⟩Wl+u+γI⟨Sxf1, MS xf1⟩Wl+u. (2.25)
By (2.23) we then see that
Vxi(yi, CxiK∗
xif0+CxiK∗
xif1) =Vxi(yi, CxiK∗
xif0),
and by (2.24) we see that
⟨Sxf0, MS xf1⟩Wl+u=⟨Sxf0,0⟩Wl+u= 0.
So,
⟨Sxf0, MS xf1⟩Wl+u=⟨Sxf1, MS xf0⟩Wl+u=⟨Sxf1, MS xf1⟩Wl+u= 0 (2.26)14 A. GHEONDEA AND C. T ˙ILK˙I
and hence, by (2.25), we have
I(f) =I(f0+f1) =1
llX
i=1Vxi(yi, CxiK∗
xif0+CxiK∗
xif1) +γA∥f0∥2
HK+γA∥f1∥2
HK
+γI⟨Sxf0, MS xf0⟩Wl+u+γI⟨Sxf0, MS xf1⟩Wl+u+
+γI⟨Sxf1, MS xf0⟩Wl+u+γI⟨Sxf1, MS xf1⟩Wl+u
and then, by (2.23) and (2.26) we get that
I(f) =1
llX
i=1Vxi(yi, CxiK∗
xif0) +γA∥f0∥2
HK+γA∥f1∥2
HK+γI⟨Sxf0, MS xf0⟩Wl+u
≥1
llX
i=1Vxi(yi, CxiK∗
xif0) +γA∥f0∥2
HK+γI⟨Sxf0, MS xf0⟩Wl+u=I(f0), (2.27)
and the proof is finished. □
In order to get a conclusion in the spirit of the representer theorem, extra assumptions are
needed.
Proposition 2.10. Assume that the subspace HK,x, see (2.18) , is closed. This happens, for ex-
ample, if all Hilbert spaces Wx1, . . . ,Wxl+uhave finite dimensions. If the minimisation problem
(2.14) has a solution fz,γthen there exist a1, . . . , a l+u, with aj∈ W xjfor all j= 1, . . . , l +u,
such that
fz,γ=l+uX
j=1Kxjaj.
Proof. SinceHK,xis closed, we have the decomposition
HK=HK,x⊕ H⊥
K,x. (2.28)
Iffis a solution to the minimisation problem (2.14), in view of Lemma 2.9, it follows that
f∈ H K,z, and the conclusion follows. □
The main theorem of this section is a representer theorem under certain general and natural
assumptions.
Theorem 2.11. Assume that the loss functions Vxj(yj,·)are bounded from below and contin-
uous, for all j= 1, . . . , l , and that all Hilbert spaces Wx1, . . . ,Wxl+uhave finite dimensions.
Then the minimisation problem (2.14) has a solution fz,γand, for any such a solution, there
exist a1, . . . , a l+u, with aj∈ W xjfor all j= 1, . . . , l +u, such that
fz,γ=l+uX
j=1Kxjaj.
Proof. We first observe that, since all loss functions Vxj(yj,·),j= 1, . . . , l +u, are lower bounded
andMis positive semidefinite, from (2.15) it follows that I(f) is lower bounded and hence
its infimum exists as a real number. Since all the Hilbert spaces Wx1, . . . ,Wxl+uhave finite
dimensions it follows that the subspace HK,xhas finite dimension and hence it is closed. Then,
from Lemma 2.9 it follows that
−∞<inf
f∈HKI(f) = inf
f∈HK,xI(f).LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 15
So, it remains only to show that the latter infimum is attained.
Indeed, since γA>0 and the loss functions Vxj(yj,·) are bounded from below for all j=
1, . . . , l , it follows that
lim
∥f∥HK→∞I(f) = +∞. (2.29)
Since all loss functions Vxj(yj,·) are continuous, for j= 1, . . . , l , and the evaluation functionals
onHKare continuous as well, see Theorem 2.4, it follows that Iis continuous on HK. Let
f0∈ H K,xbe arbitrary but fixed. From (2.29), for ϵ >0 there exists δ >0 such that
I(f)≥ I(f0) +ϵfor all f∈ H K,xwith∥f−f0∥HK> δ. (2.30)
We consider now the continuous function Irestricted to the closed ball in HK,x
BHK,x
δ(f0) ={f∈ H K,x| ∥f−f0∥HK≤δ},
which is compact, since the vector space HK,xis finite dimensional. This implies that the
infimum of IonBHK,x
δ(f0) is attained. In view of (2.30) it follows that
inf
f∈HK,xI(f) = inf {I(f)|f∈BHK,x
δ(f0)},
and the proof is finished. □
In view of Proposition 2.10, if the loss functions Vxi(yi,·) are convex for all i= 1,2, . . . , l ,
then the assumption on finite dimensionality of the spaces Wxifori= 1, . . . , l +u, can be
slightly weaker. We first record a result that is known but for which we include a proof, for
the reader’s convenience. To this end, we recall some basic definitions. If Vis a vector space,
a subset A⊆ V isconvex if for any x, y∈Aandλ∈(0,1) we have (1 −λ)x+λy∈A. A
function f: Dom( f)→Risconvex if Dom( f) is a convex set in Vand
f(λx+ (1−λ)y)≤λf(x) + (1 −λ)f(y)
for each x, y∈Dom( f) and λ∈(0,1). In addition, fisstrictly convex if
f(λx+ (1−λ)y)< λf (x) + (1 −λ)f(y)
for each x, y∈Dom( f) such that x̸=yandλ∈(0,1).
Lemma 2.12. (a)Given a vector space Vthat is endowed with a seminorm ∥·∥, the square of
the seminorm ∥·∥2:V →Ris a convex function on V.
(b)If, in addition, ∥·∥is a norm associated to an inner product on the real vector space V,
then∥·∥2becomes strictly convex.
Proof. (a) Let f0, f1∈ V, α∈(0,1). Then by triangle inequality we have
∥αf0+ (1−α)f1∥ ≤ ∥ αf0∥+∥(1−α)f1∥=α∥f0∥+ (1−α)∥f1∥,
hence, by squaring both sides we get
∥αf0+ (1−α)f1∥2≤α2∥f0∥2+ (1−α)2∥f1∥2+ 2α(1−α)∥f0∥∥f1∥.16 A. GHEONDEA AND C. T ˙ILK˙I
Further on, if we add and subtract −α∥f0∥2−(1−α)∥f1∥2from the right hand side, we get
∥αf0+ (1−α)f1∥2≤α2∥f0∥2+ (1−α)2∥f1∥2+ 2α(1−α)∥f0∥∥f1∥
−α∥f0∥2−(1−α)∥f1∥2+α∥f0∥2+ (1−α)∥f1∥2
= (α2−α)∥f0∥2+ ((1−α)2−(1−α))∥f1∥2
+ 2α(1−α)∥f0∥∥f1∥+α∥f0∥2+ (1−α)∥f1∥2
=−α(1−α)(∥f0∥ − ∥ f1∥)2+α∥f0∥2+ (1−α)∥f1∥2
≤α∥f0∥2+ (1−α)∥f1∥2. (2.31)
This shows that ∥·∥2is convex.
(b) We assume now that ∥·∥is a norm associated to an inner product ⟨·,·⟩on a real vector
spaceV, that f0̸=f1,α∈(0,1), and that
∥αf0+ (1−α)f1∥2=α∥f0∥2+ (1−α)∥f1∥2, (2.32)
hence, by the last step in (2.31), it follows that ∥f0∥=∥f1∥. Then, by (2.32) and since
∥f0∥=∥f1∥, we get
∥f0∥2=∥αf0+ (1−α)f1∥2=⟨αf0+ (1−α)f1, αf 0+ (1−α)f1⟩
=α2∥f0∥2+ 2α(1−α)⟨f0, f1⟩+ (1−α)2∥f1∥2.
Taking into account that ∥f0∥=∥f1∥and that α(1−α)̸= 0, from here it follows that
⟨f0, f1⟩=∥f0∥2=∥f1∥2, (2.33)
hence we have equality in the Schwarz inequality and, consequently, f0=tf1for some t∈R.
Since∥f0∥=∥f1∥it follows that t=±1. But t= 1 is not possible since f0̸=f1, while f0=−f1
is not possible because, by (2.33), this would imply f0= 0 = f1. □
Theorem 2.13. Assume that all the underlying vector spaces are real, that the subspace HK,x
is closed, and that the loss functions Vxi(yi,·)are convex for all i= 1,2, . . . , l . Then, the
minimisation problem (2.14) has a unique solution fz,γand there exist a1, . . . , a l+u, with aj∈
Wxjfor all j= 1, . . . , l +u, such that
fz,γ=l+uX
j=1Kxjaj.
Proof. Consider the function Vl:Yl×Yl→Rdefined by
Vl(y,y′) :=lX
j=1Vxj(yj, y′
j),y= (y1, . . . , y l),y′= (y′
1, . . . , y′
l), (2.34)
and observe that, for each fixed y∈Y, the function Vl(y,·) is convex on Y, since all maps Vxj
are convex in the second argument, j= 1, . . . , l . Consequently, in the definition of Iat (2.15),
the first term is a convex function. Since the second term is a norm, it is a strictly convex
function, while the third term is a seminorm, hence a convex function as well, by Lemma 2.12.
Thus, Iis a strictly convex function and hence the minimisation problem (2.14) has a unique
solution. Then the conclusion follows from Proposition 2.10. □
Remark 2.14. Theorem 2.13 contains Theorem 2 in [51] in the case when the subspace HK,x,
see (2.18), is closed. This happens, for example, if the Hilbert space Win that theorem is
finite dimensional. In [51] the authors claim that the result is true even in the case when WLOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 17
is an infinite dimensional space, which is not substantiated by the proof they provide. More
precisely, the gap in that proof is that the subspace HK,xmight not be finite dimensional and
hence it might not be closed, which implies that, we have the decomposition (2.22) and not
the decomposition (2.28). Consequently, the only conclusion that can be drawn is that the
minimiser fz,γbelongs to the closure of HK,x, and hence can only be approximated in the norm
ofHKby sums of typePl+u
j=1Kxjaj, but it may never equal such a sum.
3.Differentiable Loss Functions
3.1.Preliminary Results on Differentiable Optimisation. Throughout this section, we
assume that all vector spaces are real. The definitions and proofs of facts recalled in this
subsection are from [57]. If Xis a normed space, the directional derivative of a function
f: Dom( f)(⊆ X)→Rat an interior point x∈Dom( f) in the direction h∈ X is given by
f′(x:h) = lim
t→0f(x+th)−f(x)
t,
provided that the limit exists. A function f: Dom( f)(⊆ X )→RisGˆ ateaux differentiable
at an interior point x∈Dom( f) iffhas directional derivatives for all directions at xand
φx(h) :=f′(x:h) is linear and continuous in h. In this case, we denote the Gˆ ateaux derivative
∇xf∈ B(X,R) =X∗by the gradient notation
(∇xf)h:=φx(h), h∈ X.
In general, if XandYare Banach real spaces and U⊆ X is open and F:X → Y , then F
has directional derivative for all directions at point x∈Uif
lim
τ→0F(x+τh)−F(x)
τ
exists for any h∈ X. In this case we define the map ∇xF:X → Y as
(∇xF)h:= lim
τ→0F(x+τh)−F(x)
τ, h∈ X.
In the following we recall some basic facts.
Theorem 3.1 (Chain Rule for Directional Derivative) .Assume that X,Y,Zare Banach spaces,
F:X → Y ,G:Y → Z and there exists U⊆ X andV⊆ Y open such that F(U)⊆ V,G
has directional derivatives for all directions at y∈ V, and Fhas directional derivatives for all
directions at x∈U. If∇xFand∇yGare continuous in x∈Uandy∈V, respectively, then,
for any h∈ X, we have
∇x(G◦F)(h) =∇F(x)G(∇xF(h)).
LetXbe a Banach real space. For any fixed x∗∈ X∗:=B(X;R) and any y∈ X we denote
⟨x∗, y⟩:=x∗(y)∈R. (3.1)
Remark 3.2. IfHis a real Hilbert space, by Riesz-Fr´ echet Representation Theorem we have
⟨y, x∗⟩=⟨y, fx∗⟩Hfor a unique fx∗∈ H. For simplicity we denote fx∗asx∗.
Letf: Dom( f)(⊆ X)→Rbe convex. A point x∗∈ X∗is asubgradient offatxif
f(y)≤f(x) +⟨x∗, y−x⟩
holds for all yin a neighbourhood of x. The set of all subgradients of fatxis the subdifferential
offatxand is denoted by ∂f(x). If ∂f(x)̸=∅, we say fissubdifferentiable atx. The
domain of subdifferential is denoted as Dom( ∂f) ={x∈X|∂f(x)̸=∅}. By definition
Dom( ∂f)⊆Dom( f).18 A. GHEONDEA AND C. T ˙ILK˙I
Theorem 3.3 (Fermat’s Rule) .Letf:X →Rbe convex. Then ˆxis a global minimiser of f
if and only if 0∈∂f(ˆx).
LetAbe an open subset of XandT:A→ Y . Then given a point x∈A,TisFr´ echet
differentiable atxif there exists a bounded linear operator Lx:X → Y such that
lim
h→0∥T(x+h)−T(x)−Lxh∥Y
∥h∥X= 0.
In this case, due to the uniqueness of Lxwe define the linear bounded operator D xT:X → Y
as D xT:=Lxand call it the Fr´ echet derivative ofTatx.
Theorem 3.4. Let(H,⟨., .⟩H)be a Hilbert real space. Then, given ψ∈ X andF, G:X → H
maps that are Fr´ echet differentiable at ψ, we have
Dψ⟨F·, G·⟩H(h) =⟨(DψF)(h), G(ψ)⟩H+⟨F(ψ),(DψG)(h)⟩H
for any h∈ H.
Remark 3.5. In Theorem 3.4 if we further assume that F, G are bounded linear operators,
then we get
Dψ⟨F·, G·⟩H(h) =⟨Fh, Gψ ⟩H+⟨Fψ, Gh ⟩H
for any h∈ H.
Theorem 3.6. LetX,Ybe Banach spaces. Consider a nonempty open set U⊂ X and a map
F:U→ Y . IfFis Fr´ echet differentiable at x∈Uthen it is Gˆ ateaux differentiable at xand
∇xF= D xF.
3.2.The Representer Theorem for Locally Differentiable Loss Functions. The nota-
tion in this subsection is the same as in Section 2, only that all vector spaces are over the real
fieldR. In this subsection, we show that, if we add the assumption that for any i= 1, . . . , l ,
Vxi(yi,·) :Yx→Ris Gˆ ateaux differentiable, then we can allow Wxi, for all i= 1, . . . , l , to
be infinite dimensional and get the same conclusion as in Proposition 2.10. The proof of this
theorem is inspired to a certain extent by the proof of Theorem 3 in [51], that was proven for
the special case of the least squares loss function.
As in the proof of Lemma 2.9, let the sampling operator Sx:HK→Wl+ube defined as
in (2.19) where x= (x1, . . . , x l+u). Let also EC,x:HK→Ylbe defined as in (2.21) and the
Hilbert space Ylbe defined as in (2.20). Define the function Vl:Yl×Yl→Ras in (2.34)
and then denote Vl
y:Yl→Ras
Vl
y(y′) :=Vl(y,y′),y′= (y′
1, . . . , y′
l)∈Yl, (3.2)
where y= (y1, y2, . . . , y l)∈Yl.
In the next theorem, which is the main result of this section, please note that all Hilbert
spaces Wx1, . . . ,Wxl+uare allowed to be infinite dimensional, without any restriction except
that they are real.
Theorem 3.7. Assume that for any i= 1, . . . , l, the loss function Vxi(yi,·)is Gˆ ateaux differen-
tiable. If the minimisation problem (2.14) has a solution fz,γ∈ H Kthen there exist a1, . . . , a l+u,
withaj∈ W xjfor all j= 1, . . . , l +u, such that
fz,γ=l+uX
j=1Kxjaj, (3.3)LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 19
where the vectors ai∈ W xi,i= 1, . . . , l +u, satisfy the following system
2γAlai+ 2γIll+uX
j,k=1Mi,jK(xi, xj)aj=−C∗
xi(∇EC,xfz,γVl
y)i, ifi= 1, . . . , l, (3.4)
γAai+γIl+uX
j,k=1Mi,jK(xi, xj)aj= 0, ifi=l+ 1, . . . , l +u. (3.5)
Here, for arbitrary y′∈Yl, we abuse some notation and denote ∇y′Vl
yboth as the origi-
nal functional (Gˆ ateaux derivative) and the vector that represents this functional following the
notation as in (3.1).
Proof. We can rewrite the map Ito be minimised, see (2.15), as
I(f) =1
lVl
y(EC,xf) +γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u, f∈ H K. (3.6)
Assuming that the minimisation problem (2.14) has a solution fz,γ, which is an interior point in
the domain of Iand that Vl
yis Gˆ ateaux differentiable, by Theorem 3 .1, the Gˆ ateaux derivative
evaluated at f∈ H Kis
∇f(Vl
y◦EC,x)(h) =∇EC,xfVl
y(∇fEC,x(h)), h∈ H K.
Then, by using the identification as in (3.1) and Theorem 3.6, for all h∈ H Kwe get
∇EC,xfVl
y(∇fEC,x(h)) =
∇EC,xfVl
y,DfEC,xh
=
∇EC,xfVl
y, EC,xh
Yl
=
E∗
C,x∇EC,xfVl
y, h
HK. (3.7)
Since∥f∥2
HK=⟨f, f⟩HK, by Theorem 3.4, the map Hk7→ ∥f∥2
HKis Fr´ echet differentiable and
hence, by Theorem 3.6, it is Gˆ ateaux differentiable. Thus, by Theorem 3.4 and since HKis
real we have
γA∇f∥·∥2
HK(h) = 2 γA⟨f, h⟩HK, h∈ H K. (3.8)
Again by using Theorem 3.4 we get
γI∇f⟨Sx·, MS x·⟩Wl+u(h) =γI⟨Sxf, MS xh⟩Wl+u+γI⟨Sxh, MS xf⟩Wl+u
= 2γI⟨S∗
xMSxf, h⟩HK, h∈ H K. (3.9)
Summing up (3.7), (3.8) and (3.9) we get
∇fI(h) =l−1
E∗
C,x∇EC,xfVl
y, h
HK+ 2γA⟨f, h⟩HK+ 2γI⟨S∗
xMSxf, h⟩HK
=
l−1E∗
C,x∇EC,xfVl
y+ 2γAf+ 2γIS∗
xMSxf, h
HK, h∈ H K. (3.10)
Keeping Fermat’s Theorem 3.3 in mind, since that minimiser is an interior point of the domain,
we should have the minimiser fz,γ∈ H Ksuch that ∇fz,γI(·) = 0. By (3.10), this means
∇fz,γI(h) =
l−1E∗
C,x∇EC,xfz,γVl
y+ 2γAfz,γ+ 2γIS∗
xMSxfz,γ, h
HK= 0 (3.11)
for any h∈ H K. Thus, we get
1
lE∗
C,x∇EC,xfz,γVl
y+ 2γAfz,γ+ 2γIS∗
xMSxfz,γ= 0
and hence
fz,γ=−1
2γAlE∗
C,x∇EC,xfz,γVl
y−γI
γAS∗
xMSxfz,γ.20 A. GHEONDEA AND C. T ˙ILK˙I
Then by using the facts that E∗
C,x= [Kx1C∗
x1Kx2C∗
x2. . . K xlC∗
xl], see (2.21), and that, by
definition, Sx= [K∗
x1K∗
x2. . . K∗
xl+u]T, see (2.19), we get
fz,γ=−1
2γAllX
i=1KxiC∗
xi(∇EC,xfz,γVl
y)i−γI
γAl+uX
i=1Kxi(MSxfz,γ)i.
Thus, we can represent fz,γas in (3.3), where
ai=

−1
2γAlC∗
xi(∇EC,xfz,γVl
y)i−γI
γA(MSxfz,γ)i,ifi= 1, . . . , l
−γI
γA(MSxfz,γ)i, otherwise.(3.12)
Since
(MSxfz,γ)i=l+uX
k=1Mi,kl+uX
j=1K(xk, xj)aj, i= 1, . . . , l +u,
it follows that
ai=

−1
2γAlC∗
xi(∇EC,xfz,γVl
y)i−γI
γAl+uP
j,k=1Mi,kK(xk, xj)aj,ifi= 1, . . . , l,
−γI
γAl+uP
j,k=1Mi,kK(xk, xj)aj, otherwise,
which is equivalent to the system of equations (3.4) and (3.5). □
Remark 3.8. The system of equations (3.4) and (3.5) can be reformulated by means of op-
erators; to be compared with Theorem 4 in [51] that was obtained for the special case of the
least squares function. Let K[x] denote the ( l+u)×(l+u) operator valued matrix whose ( i, j)
entry is K(xi, xj), and let v= (v1, . . . , v l+u) be the vector with entries
vi=(
−C∗
xi(∇EC,xfz,γVl
y)iifi= 1, . . . , l
0, otherwise,
where C∗
x:Yx→ W xfor any x∈X. Then, with notation and assumptions as in Theorem 3.7
and letting a= (a1, . . . , a l+u), a simple algebraic calculation, that we leave to the reader, shows
that the system of equations (3.4) and (3.5) coincides with the operator equation
(2lγIMK[x] + 2lγAI)a=v, (3.13)
where both aandvare considered as column vectors. However, at this level of generality, from
here we cannot get the unknown vector abecause it appears in the vector vas well. From this
point of view, the equation (3.13) is more an implicit form.
Corollary 3.9. With notation and assumptions as in Theorem 3.7, assume that the loss func-
tions Vxiare convex for all i= 1,2, . . . , l . Then the minimisation problem (2.14) has a unique
solution fz,γand there exist a1, . . . , a l+u, with aj∈ W xjfor all j= 1, . . . , l +u, such that
fz,γ=l+uX
j=1Kxjaj,
where the vectors a1, . . . , a l+usatisfy the system of equations (3.4) and(3.5).
Proof. The argument is the same as in Theorem 2.13 and then use Theorem 3.7. □LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 21
Theorem 3.7 and its Corollary 3.9 have a theoretical significance and a less practical impor-
tance because, in the system (3.4) and (3.5), the unknowns a1, . . . , a l+uappear also on the right
hand side and hence it is more an implicit form of expressing them and not an explicit one.
Because of that, for specified loss functions V, one should work further on these expressions in
order to obtain explicit or, at least computable, solutions. In the next two subsections, we work
out the details and show how Theorem 3.7 and its Corollary 3.9 can be improved for special
loss functions, the least squares functions, similar to the results in [51], and the exponential
least squares functions, and compare the formulae.
3.3.The Least Squares Loss Function. If all the loss functions are the least squares, see
Example 2.6.(1), the minimisation function (2.15) becomes
I(f) :=1
llX
j=1∥yj−Cxjf(xj)∥2
Yxj+γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u
=1
llX
j=1∥yj−CxjK∗
xjf∥2
Yxj+γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u, (3.14)
Since, as functions of f∈ H K, all the terms are convex and the middle term is strictly convex,
see Lemma 2.12, the minimisation function I(·) is strictly convex and hence the minimisation
problem has unique solution. Also, I(·) is Fr´ echet differentiable, hence Corollary 3.9 is appli-
cable. According to Fermat’s Theorem, this unique solution fshould vanish the gradient. But,
for each h∈ H K, we have
∇fI(f)h=2
l⟨E∗
C,xEC,xf, h⟩HK−2
l⟨E∗
C,xy, h⟩HK+ 2γA⟨f, h⟩HK+ 2γI⟨SxMSxf, h⟩HK,
hence,
E∗
C,xEC,xf+lγAf+lγIS∗
xMSxf−E∗
C,xy= 0. (3.15)
Since γA>0 this is equivalent with
f=1
lγAE∗
C,xy−1
lγAE∗
C,xEC,xf−γI
γAS∗
xMSxf,
explicitly,
f=lX
i=1Kxi1
lγAC∗
xiyi
+lX
i=1Kxi
−1
lγAC∗
xiCxif(xi)
+l+uX
i=1Kxi
−γI
γAl+uX
k=1Mi,kf(xk)
.(3.16)
In this special case, the representation (3.16) improves Theorem 3.7 by obtaining the repre-
sentation of the optimal solution as f=Pl+u
i=1Kxiai, where,
ai=1
lγAC∗
xiyi−1
lγAC∗
xiCxif(xi)−γI
γAl+uX
k=1Mi,kf(xk),for all i= 1, . . . , l, (3.17)
and,
ai=−γI
γAl+uX
k=1Mi,kf(xk),for all i=l+ 1, . . . , l +u. (3.18)
Then, since for all k= 1, . . . , l +u, we have
f(xk) =l+uX
j=1Kxj(xk)aj=l+uX
j=1K(xk, xj)aj,22 A. GHEONDEA AND C. T ˙ILK˙I
and, consequently, from (3.17) and (3.18), we get
ai+l+uX
j=1h1
lγAC∗
xiCxiK(xi, xj) +γI
γAl+uX
k=1Mi,kK(xk, xj)i
aj=1
lγAC∗
xiyi, i= 1, . . . , l, (3.19)
ai+l+uX
j=1hγI
γAl+uX
k=1Mi,kK(xk, xj)i
aj= 0, i=l+ 1, . . . , l +u. (3.20)
Equations (3.19) and (3.20) make a system of linear equations which can be treated very
efficiently by computational techniques, similarly as in [51] and the literature cited there. This
substantiates our claim that the localised versions offer more flexibility in modelling learning
problems without bringing additional obstructions.
3.4.The Exponential Least Squares Loss Function. If all the loss functions are the
exponential least squares, see Example 2.6.(4), the minimisation function (2.15) becomes
I(f) := 1 −1
llX
j=1exp(−∥yj−Cxjf(xj)∥2
Yxj) +γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u
= 1−1
llX
j=1exp(−∥yj−CxjK∗
xjf∥2
Yxj) +γA∥f∥2
HK+γI⟨f, Mf⟩Wl+u. (3.21)
In this case, since the second term is not, in general, convex, we cannot conclude that the
function Iis convex. However, I(f)≥0 for all f∈ H Kand lim ∥f∥→∞I(f) = + ∞, hence
the minimisation problem has at least one solution f, but this solution might not be unique.
Anyhow, because any solution fis an interior point in HK, Fermat’s Rule is applicable, hence
∇fI= 0. Taking advantage of the calculations performed in the previous subsection, see (3.15),
by calculating the gradient of Iand then, by Fermat’s Rule, the optimal function fshould
satisfy the following equation
exp(−∥yj−Cxjf(xj)∥2
Yxj) 
E∗
C,xEC,xf−E∗
C,xy
+lγAf+lγIS∗
xMSxf= 0. (3.22)
Since γA>0 this is equivalent with
f=exp(−∥yj−Cxjf(xj)∥2
Yxj)
lγA 
E∗
C,xy−E∗
C,xEC,xf
−γI
γAS∗
xMSxf,
explicitly,
f=lX
i=1Kxiexp(−∥yj−Cxjf(xj)∥2
Yxj)
lγA 
C∗
xiyi−C∗
xiCxif(xi)
+l+uX
i=1Kxi
−γI
γAl+uX
k=1Mi,kf(xk)
. (3.23)
In this special case, (3.23) improves Theorem 3.7 by obtaining the representation of the
optimal solution as f=Pl+u
i=1Kxiai, where,
ai=exp(−∥yj−Cxjf(xj)∥2
Yxj)
lγA 
C∗
xiyi−C∗
xiCxif(xi)
,for all i= 1, . . . , l, (3.24)LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 23
and,
ai=−γI
γAl+uX
k=1Mi,kf(xk),for all i=l+ 1, . . . , l +u. (3.25)
Then, since for all j= 1, . . . , l +u, we have
f(xj) =l+uX
k=1Kxk(xj)aj=l+uX
k=1K(xj, xk)ak,
and, consequently, from (3.24) and (3.25), we get
ai+lX
j=1hexp(−∥yj−Cxjl+uP
k=1K(xj, xk)ak∥2
Yxj)
lγAC∗
xiCxiK(xi, xj) +γI
γAl+uX
k=1Mi,kK(xk, xj)i
aj
=1
lγAC∗
xiyi, i= 1, . . . , l, (3.26)
ai+γI
γAl+uX
j=1l+uX
k=1Mi,kK(xk, xj)aj= 0, i=l+ 1, . . . , l +u. (3.27)
Equations (3.26) and (3.27) make a system of equations with respect to the unkowns aifor
i= 1, . . . , l +u, which consists of nonlinear equations for the unknowns aicorresponding to the
labeled input points and of linear equations for the unknowns aicorresponding to the unlabeled
input points.
3.5.Available Numerical Methods for the Exponential Least Square Loss Function.
In this subsection we tackle the question of deriving algorithms to solve the system of equations
defined by (3.26) and (3.27). We use the same notations and assumptions as in the previous
subsection. In addition, we assume that all Hilbert spaces Wxihave finite dimensions. To be
more precise, Wxiis identified with Rdi, fori= 1, . . . , l +u. We first define the vector
a= (a1, a2, . . . , a l+u)∈RN, (3.28)
where aj∈ W xj=Rdj, for each j= 1, . . . , l +u, and
N=l+uX
i=1dim(Wxi) =l+uX
i=1di.
Then we consider the function H:RN→RNwith
H(a) = (H1(a), H 2(a), . . . , H l+u(a)),a∈RN,
defined by
Hi(a) =ai+1
lγAlX
j=1exp 
−∥yj−Cxjl+uX
k=1K(xj, xk)ak∥2
Yxj
C∗
xiCxiK(xi, xj)aj
+γI
γAl+uX
j=1l+uX
k=1Mi,kK(xk, xj)aj−1
lγAC∗
xiyi, i= 1, . . . , l, (3.29)
Hi(a) =ai+γI
γAl+uX
j=1l+uX
k=1Mi,kK(xk, xj)aj, i=l+ 1, . . . , l +u. (3.30)24 A. GHEONDEA AND C. T ˙ILK˙I
In view of the system of equations defined by (3.26) and (3.27) with the unknown vector solution
a∈RNas in (3.28), we search for solutions of the equation H(a) = 0. From the numerical
analysis point of view, this problem can be approached by nonlinear optimisation techniques,
more precisely, we search for an algorithm that yields a sequence ( an)n≥0of vectors in RNwith
the property that for each ϵ >0 there exists an integer n≥0 such that ∥H(an)∥< ϵ. One of
the classical approaches for this kind of nonlinear problems is in the class of Newton’s damped
approximation methods, see [34] for an overview and advances in second-order approximation
methods and machine learning.
There are different methods and algorithms for dealing with constrained nonlinear systems
of differentiable functions. For example, the potential reduction Newton’s method in [53] can
be used in order to provide an algorithm to approximate solutions a∈RNof the equation
H(a) = 0 under the assumption that the Jacobi matrix ∇uHis nonsingular for all u∈RN.
Other methods based on the interior point methods are available, for example see [15], [16],
and [70]. The latter algorithm is implemented in the fmincon function in MATLAB, that we
have used in our example.
Because the optimisation problem is nonlinear, multiple solutions may show up and this
makes the choice of the initialisation vector a0very important: for different choices of the
initialisation vector different pools of solutions of the equations H(a) = 0 might be found. Let
us observe that, on the one hand, the components Hifori=l+ 1, . . . , l +u, corresponding to
unlabeled points xi, are linear and homogeneous and hence that 0 is a solution. On the other
hand, the components Hifori= 1, . . . , l , corresponding to labeled points xi, are nonlinear and
nonhomogeneous and hence that 0 is not a solution. From here, we can see that, on the one
hand, as the learning problem is semisupervised, lis significantly less than uand hence taking
the initialisation vector a0= 0 might be a good choice for the beginning. On the other hand,
in order to find better minimisers, or even the global minimiser, some multigrid methods or
stochastic methods for the choice of the initialisation vector a0might be involved, e.g. see [28].
In our example, we have used the Latin Hypercube Sampling (LHS), see [46], to solve this issue.
Similarly to multigrid methods, the LHS divides the domain into small cubes. Without loss of
generality, assume that the domain is [0 ,1]da cube with dimension d. Then for a fixed n, split
the cube into ndsubcubes by splitting each interval [0 ,1] into intervals with 1 /nlength, that
is,
[0,1] =n−1[
i=0i
n,i+ 1
n
.
Then we pick points such that any rectangle Ri,jdefined below contains only one point
Ri,j= [0,1]j−1×i
n,i+ 1
n
×[0,1]d−j, i, j = 1, . . . , n.
This sampling strategy enjoys asymptotic bounds in expectation [65] with few points. Hence
it is a good candidate for our purposes.
Because there is no guarantee for uniqueness of solution, the algorithms for approximation of
the solutions of the equation H(a) = 0 should be complemented by further steps in which the
obtained solutions should be tested whether they provide minimisers, of the learning function
(3.45), or not and to which extent from the class of all minimisers one can get a global minimiser.
But the most challenging problem refers to the assumption that the Jacobi matrix ∇uHis
nonsingular for all u∈RN. In order to tackle this question we explicitly calculate this Jacobi
matrix. To this end, we first observe that for each i, j= 1, . . . , l +u, letting aj= (a(1)
j, . . . , a(dj)
j),LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 25
we have a partial Jacobi matrix
∂Hi
∂aj=∂Hi
∂a(1)
j, . . . ,∂Hi
∂a(dj)
j
,
that is, a function matrix of dimension di×dj= dim( Wxi)×dim(Wxj). Let i= 1, . . . , l and
letIdidenote the identity operator on Wxiidentified with Rdi. Then, from (3.29), on the one
hand we get
∂Hi
∂ai=Idi+1
lγAexp 
−∥yi−Cxjl+uX
k=1K(xj, xk)ak∥2
Yxj
C∗
xiCxiK(xi, xi)
+1
lγAl+uX
j=12⟨yj−Cxjl+uX
k=1K(xj, xk)ak, CxjK(xj, xi)ai⟩Yxi×
×exp 
−∥yj−Cxjl+uX
k=1K(xj, xk)ak∥2
Yxj
C∗
xiCxiK(xi, xj)
+γI
γAl+uX
k=1Mi,kK(xk, xi), (3.31)
and, on the other hand, for each j= 1, . . . , l +u,j̸=i, we get
∂Hi
∂aj=1
lγAexp 
−∥yj−Cxjl+uX
k=1K(xj, xk)ak∥2
Yxj
C∗
xiCxiK(xi, xj)
+1
lγAl+uX
m=12⟨ym−Cxml+uX
k=1K(xm, xk)ak, CxmK(xm, xj)aj⟩Yxm×
×exp 
−∥ym−Cxml+uX
k=1K(xm, xk)ak∥2
Yxm
C∗
xiCxiK(xi, xm)
+γI
γAl+uX
k=1Mi,kK(xk, xj). (3.32)
Let now i=l+ 1, . . . , l +u. Then, from (3.30), on the one hand we get
∂Hi
∂ai=Idi+γI
γAl+uX
k=1Mi,kK(xk, xi), (3.33)
and, on the other hand, for j= 1, . . . , l +u,j̸=i, we get
∂Hi
∂aj=γI
γAl+uX
k=1Mi,kK(xk, xj). (3.34)
The partial Jacobi matrices obtained in (3.31) through (3.34) make a complete description
of the Jacobi matrix of the Fr´ echet derivative ∇aH. From these formulae, a few observations
follow. A first observation is that we can write
∇aH=IRN+1
γAR, (3.35)
where Ris an N×Nmatrix that can be calculated explicitly from (3.31) through (3.34). Then,
one can use different extra assumptions on the kernels and data points in order to assure that26 A. GHEONDEA AND C. T ˙ILK˙I
∇aHis a nonsingular matrix. For example, one can take into account the fact that the labeled
points x1, . . . , x lare selected in a supervised manner while the unlabeled points xl+1, . . . , x l+u
can be changed in a convenient manner that assures the Jacobi matrix ∇aHbe nonsingular.
Another observation is that in a semisupervised learning problem the number lof labeled points
is significantly less than the number uof unlabeled points and hence the degree of nonlinearity
of the system of equations defined by (3.26) and (3.27) is rather low, which can be used to
search for reliable approximations by systems of linear equations.
Some of the constrained optimisation problem described before depends heavily on the as-
sumption that the Jacobi matrix ∇aHis nonsingular for all a∈RN, a situation that may not
be easy to get. In general, we can find a nonempty open set Ω in RNon which the Jacobi
matrix ∇aHis nonsingular, for example we can use (3.35) to show that Ω contains the set of
thosea∈RNwith the property that ∥R∥< γ A. In view of (3.31) through (3.34), in the non-
linear terms of these partial Jacobi matrices the exponentials with negative exponents tame the
growth of ∥R∥when the vector ais far from the solution and, consequently, by manipulating the
regularisation coefficient γAwe can get very large sets Ω. Then, one can use other techniques to
prevent the approximation sequence to get too close to the boundary of Ω as in the constrained
version of the algorithm proposed in [53]. More recent investigations refer to either adapting the
algorithm in case the Jacobi matrix ∇uHis singular and pseudo-inverses replace the inverses,
see e.g. the analyis of Kaczmarz type algorithms for ill-posed linear problems in [58], or using
stochastic Bregman-Kaczmarz methods, see [28]. These allow us to modify correspondingly
the algorithm that we presented here to the general case. We leave the details for a further
research project on real data sets. As a practical approach, the simplest is to use perturbation
theory, more precisely, for those values of aat which the Fr´ echet derivative is not invertible, a
small perturbation of achanges the point to one for which the Fr´ echet derivative is invertible
and then the stability of the problem to small perturbations guarantees the convergence of the
iteration process.
3.6.A Toy Model. In this subsection we provide a toy model for the localised version of the
regularised machine learning problem in case the loss function is the exponential least square
function as in Subsection 3.4 and test an algorithm following the discussion of the numerical
methods as in Subsection 3.5. To this end, let X=X1∪X2, where
X1:={(α1, α2)|0.25≤α1≤1,0.25≤α2≤1}, (3.36)
X2:={(α1, α2)| −1≤α1≤ −0.25,0.25≤α2≤1}. (3.37)
In the following we use the notation as in Subsection 2.2. We consider x1∈X1andx2∈X2
randomly selected and let the labels y1∈ Y x1=Randy2∈ Y x2=R2be randomly selected.
Also, let x3, x4∈X1\{x1},x3̸=x4, and x5, x6∈X2\{x2},x5̸=x6, randomly selected as well,
be unlabeled points. In particular, l= 2 and u= 4. Let Yx3=Yx4=RandYx5=Yx6=R2.
Then we take Wxj=Yxjfor all j= 1, . . . , 6, in particular the machine learning problem is
single viewed. With respect to the notation (2.12) we have
Wl+u=Yx1⊕ Y x2⊕ Y x3⊕ Y x4⊕ Y x5⊕ Y x6
=R⊕R2⊕R⊕R⊕R2⊕R2=R9,
hence dim( Wl+u) = 9.
We let the regularisation coefficients γAandγIunspecified and we will test different choices
later. Since the problem is single-view we let Cxi=IYxifor all i= 1, . . . , 6 and MB= 0, seeLOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 27
Example 2.7. For the within-view operator, see Example 2.8, we proceed as follows. Let
wj,k= exp 
−∥xj−xk∥2
2σ2
, j, k = 1, . . . , 6, (3.38)
and then, consider the 6 ×6 matrices W= [wj,k]6
j,k=1andV= diag( v1,1, . . . , v 6,6), where
vj,j=6X
k=1wj,k, j = 1, . . . , 6, (3.39)
and the Laplace matrix L=V−W= [lj,k]6
j,k=1. Taking into account that the labels of points
inX1have dimension 1 and the labels of points in X2have dimension 2 the matrix MW=M
looks like this, see Example 2.8.
M=
l1,1l1,20l1,3l1,4l1,50l1,60
l2,1l2,20l2,3l2,4l2,50l2,60
0 0 l2,20 0 0 0 0 0
l3,1l3,20l3,3l3,4l3,50l3,60
l4,1l4,20l4,3l4,4l4,50l4,60
l5,1l5,20l5,3l5,4l5,50l5,60
0 0 0 0 0 0 l5,50 0
l6,1l6,20l6,3l6,4l6,50l6,60
0 0 0 0 0 0 0 0 l6,6
. (3.40)
With notation as in Section 2.2 we have M= [Mj,k]6
j,k=1with
M1,1=l1,1, M 1,2=l1,20
, M 1,3=l1,3, M 1,4=l1,4, M 1,5=l1,50
, M 1,6=l1,60
,
M2,1=
l2,1
0
, M 2,2=
l2,20
0l2,2
, M 2,3=
l2,3
0
, M 2,4=
l2,4
0
, M 2,5=
l2,50
0 0
, M 2,6=
l2,60
0 0
,
M3,1=l3,1, M 3,2=l3,20
, M 3,3=l3,3, M 3,4=l3,4, M 3,5=l3,50
, M 3,6=l3,60
,
M4,1=l4,1, M 4,2=l4,20
, M 4,3=l4,3, M 4,4=l4,4, M 4,5=l4,50
, M 4,6=l4,60
,
M5,1=
l5,1
0
, M 5,2=
l5,20
0 0
, M 5,3=
l5,3
0
, M 5,4=
l5,4
0
, M 5,5=
l5,50
0l5,5
, M 5,6=
l5,60
0 0
,
M6,1=
l6,1
0
, M 6,2=
l6,20
0 0
, M 6,3=
l6,3
0
, M 6,4=
l6,4
0
, M 6,5=
l6,50
0 0
, M 6,6=
l6,60
0l6,6
.
We consider the kernel K:X×X→S
z,ζ∈XB(Wζ,Wz) defined as follows.
K(z, ζ) := exp 
−∥z−ζ∥2
σ2
, z, ζ ∈X1, (3.41)
K(z, ζ) :=
exp 
−∥z−ζ∥2
σ2
0
0 exp( −α∥z−ζ∥)
, z, ζ ∈X2, (3.42)
K(z, ζ) :=h
exp 
−∥z−ζ∥2
σ2
0i
, z∈X1, ζ∈X2, (3.43)
K(z, ζ) :=
exp 
−∥z−ζ∥2
σ2
0
, z∈X2, ζ∈X1. (3.44)
Here the coefficients σandαremain unspecified for the moment.
With these data we have the system of equations (3.26) and (3.27), where a1,a3, and a4are
scalars and a2,a5, and a6are 2-vectors. So, speaking in terms of scalars, we have a system of
nine equations with nine unknowns. Three of these equations are nonlinear and the rest of six
equations are linear.28 A. GHEONDEA AND C. T ˙ILK˙I
In order to find a bounded set on which we can guarantee the existence of the global solution
of the minimisation problem (2.14), we follow the idea of the proof of Theorem 2.11. With
notation as in that theorem, we search for the minimiser
f=6X
j=1Kxjaj (3.45)
and we let f0= 0 and hence the corresponding vector a= 0. Then, by (3.21) we have
I(f0) = 1−1
2 
exp(−y2
1) + exp( −∥y2∥2)
. (3.46)
We search for δ > 0 such that I(f)≥ I(f0), with fas in (3.45), for any vector a=
(a1, a2, a3, a4, a5, a6) of dimension 9 with the property that a̸∈[−δ, δ]9. By the proof of
Theorem 2.11, the minimisation problem argmin ∥H(a)∥has the solution in the cube [ −δ, δ]9.
Taking into account that
I(f)≥γA∥f∥HK≥γA⟨Kxa,a⟩ ≥γAλKx∥a∥2
2≥γAλKx∥a∥2
∞, (3.47)
where,
Kx= [K(xi, xj)]6
i,j=1, (3.48)
is a 9×9 matrix and λKx>0 is the least eigenvalue of the positive matrix Kx. From (3.46)
and (3.47), letting
δ=1p
γAλKxr
1−1
2 
exp(−y2
1) + exp( −∥y2∥2)
, (3.49)
it follows that in order to find the global minimiser for the problem (2.14), it is sufficient to
search for the solutions aof the minimisation problem argmin ∥H(a)∥in the cube [ −δ, δ]9.
There are two conditions to be verified, in order for δto be consistent. Firstly, we work
under the assumption that λKx>0 which can be numerically checked, hence δ <∞. Secondly,
note that if δ= 0 this means that y1= 0 and y2= 0, hence f= 0 is the solution for the global
minimiser and hence, in this case, the problem is trivial. So, we work under the hypothesis
thatδ >0.
Since the optimisation problem is sensitive to the choice of initial conditions, we explored
various possibilities. Specifically, we found that generating a Latin Hypercube Sampling (LHS),
see [46] and the previous section, within the cube [ −δ, δ]9provides a reliable estimate of the
global solution while maintaining a reasonable runtime. Algorithm 1 below summarises the
numerical implementation.
Remark 3.10. In this algorithm, our admissibility criterion at line 7 is 3-fold. We check
whether a0results in both a smaller loss I(fa0), and a smaller gradient norm ∥H(a0)∥compared
toa. Additionally, we verify that the first-order optimality condition is sufficiently small to
ensure that a0corresponds to a local extremum. Among all local optima we select the global
one by a careful investigation and using the Latin Hypercube Sampling implemented in the
algorithm and the code run on MATLAB. An essential part of the algorithm is the use of
the function fmincon of MATLAB that uses the method of interior points for constrained
optimisation, see [15], [16], and [70].LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 29
Algorithm 1 Numerical Implementation
Require: x i∈X1∪X2⊂R2,σ, α > 0,γI, γA>0.
1:D← {x1,x2, . . . ,x6}
2:Construct Min (3.40) and Kin (3.48) using data D
3:Compute δin (3.49)
4:Compute the LHS Lon the cube [ −δ, δ]9
5:foreach element a0∈Ldo
6: Solve the minimisation problem (3.29) and (3.30) on the cube [ −δ, δ]9
with initial condition a0using the function fmincon .
7:if a 0is admissible in the sense of Remark 3.10 then
8: a←a0
9:end if
10:end for
11:return a
Example 3.11. In this example we used γA= 0.25,γI= 10, σ= 0.1 and α= 10, randomly
generated the data xandyand returned the result a.
x=
0.5377 0 .6342 0 .3273 0 .3472 0 .6724 0 .8174
0.3978 −0.4584 0 .3923 0 .4305 −0.7962 −0.3601
y= (1.2108,1.6636,4.3843)
a= (0.8433,1.7226,1.5475,0.4395,0.3944,0.1926,−0.0055,1.4116,−0.1589) (3.50)
The implicit optimality tolerance is ϵ= 10−6. Each time the code produced the mesh of the
solution fbestgiven by (3.45) for the corresponding coefficients given by the coordinates of the
solution a. Below are the meshes of the optimal solution as in (3.50), the first one corresponds
to the set X1where the function fbestis scalar valued while the second one corresponds to the
setX2where the function fbesthas 2 dimensional vector values and there are two meshes, one
for each component. The circles are the labeled points.
30 A. GHEONDEA AND C. T ˙ILK˙I
We observe that the choice of the parameters avoids overfitting. Actually, by varying the
coefficients γA,γI, and the others, one can obtain different degrees of overfitting. The code
runs efficiently on a Mac laptop, for this example it takes only less than a minute, but for other
combinations of coefficients it may be five or more minutes.
We also plot the values of the learning function I(fbest) for each choice of the initial point
0 10 20 30 40 505.77378355.7737845.77378455.7737855.7737855
and the values of H(a), in order to empirically show that the optimal solution ais a good
approximation of the solution of the equation H(a) = 0, for each choice of the initial point,LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 31
0 10 20 30 40 500.40.60.811.21.41.61.822.210-6
In the last figure we empirically check that the solution fbest, corresponding to the optimal
aas in (3.45), is a good approximation of the minimiser of the learning function I, given at
(3.21), for example by meshing it when we keep the last seven entries fixed as the last seven
entries of the optimal aand let each of the first two entries of avary in the interval [ −δ, δ].
The point represents the value of the learning function at fbest.
Although the picture shows a convex surface this is misleading since it represents only a
2-dimensional section of the general 9-dimensional surface generated by the learning function
I(f), when fis parametrised in terms of the 9-dimensional vector aas in (3.45). This behaviour
of the learning map, as well as other traits, may vary for different choices of the coefficients γA,
γI,σ, and α, as we performed test for different combinations.
In this example, one can observe that in the region that is trusted to contain the global
minimum we actually have uniqueness of the solutions, both for the equation H(a) = 0 and
the minimiser of the learning map I. But, due to the nonlinearity of the system H(a) = 0
and the nonconvexity of the learning map I, this does not mean that there may be no other
solutions when the search is performed in a larger region, that is, local minima. This shows
the importance of Theorem 2.11 that provides a bounded region where the solutions that are
of interest live.32 A. GHEONDEA AND C. T ˙ILK˙I
Appendix A.Proof of Theorem 2.1
In the following we use a formalisation of the quotient completion to a Hilbert space of an F-
vector space Vwith respect to a given nonnegative sesquilinear form V ×V ∋ (u, v)7→q(u, v)∈
F, as follows. A pair ( H; Π) is called a Hilbert space induced by (V;q) if:
(ihs1) His a Hilbert space.
(ihs2) Π: V → H is a linear operator with dense range.
(ihs3) q(u, v) =⟨Πu,Πv⟩H, for all u, v∈ V.
Such an induced Hilbert space always exists and is unique, up to a unitary operator. More
precisely, we will use the following construction. Consider the vector subspace of Vdefined by
Nq:={u∈ V | q(u, u) = 0}={u,∈ V | q(u, v) = 0 for all v∈ V} , (1.1)
where the equality holds due to the Schwarz Inequality for q, and then consider the quotient
vector space V/Nq. Letting
eq(u+Nq, v+Nq) :=q(u, v), u, v ∈ V, (1.2)
we have a pre-Hilbert space ( V/Nq;eq) that can be completed to a Hilbert space ( Hq;⟨·,·⟩H).
Letting Π q:V → H qbe defined by
Πqu:=u+Nq∈ V/Nq⊆ H q, u∈ V, (1.3)
it is easy to see that ( Hq,Πq) is a Hilbert space induced by ( V;q).
(a)⇒(b). Assuming that the H-operator valued kernel Kis positive semidefinite, we con-
sider the vector space F0(X;H) of vector cross-sections with finite support and the Hermitian
sesquilinear form ⟨·,·⟩Kdefined as in (2.6). We consider
NK={f∈ F 0(X;H)| ⟨f, f⟩K= 0} (1.4)
={f∈ F 0(X;H)| ⟨f, g⟩K= 0 for all g∈F0(X;H)},
then consider the induced Hilbert space ( HK; ΠK) associated to ( F0(X;H);⟨·,·⟩K), and let
K:=HK. For each x∈XletV(x):Hx→ K be the operator defined by
V(x)h= Π K(bh) =h+N ∈ K , (1.5)
with notation as in (2.3). Since
⟨V(x)h, V(x)h⟩K=⟨K(x, x)h, h⟩Hx≤ ∥K(x, x)∥∥h∥2
Hx, h∈ H x, x∈X, (1.6)
it follows that V(x) is bounded for all x∈X. Note that, in this way, Kis the closed span of
{V(x)Hx|x∈X}. On the other hand,
⟨K(y, x)h, g⟩Hy=⟨h+N, g+N⟩K=⟨V(x)h, V(y)g⟩K, h∈ H x, g∈ H y, x, y∈X,(1.7)
hence, K(y, x) =V(y)∗V(x) for all x, y∈X. We thus proved that ( K;V) is a minimal Hilbert
space linearisation of the H-kernel K.LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 33
In the following we prove that ( K;V) is unique, modulo unitary equivalence. To see this, let
(K′;V′) be another minimal linearisation of K. Then, for arbitrary f∈ F 0(X;H) we have
⟨X
x∈XV(x)fx,X
y∈XV(y)fy⟩0=X
x,y∈X⟨V(y)∗V(x)fx, fy⟩0
=X
x,y∈X⟨K(y, x)fx, fy⟩0
=⟨f, f⟩K
=⟨X
x∈XV′(x)fx,X
y∈XV′(y)fy⟩0,
hence, defining U(P
x∈XV′(x)fx) =P
x∈XV(x)fx, for arbitrary f∈ F 0(X;H), it follows that
Uis isometric and, taking into account of the minimality conditions, it follows that Ucan be
uniquely extended to a unitary operator U:K′→ K, such that UV′(x) =V(x) for all x∈X.
(b)⇒(a). Assuming that ( K;V) is a Hilbert space linearisation of K, we have
X
x,y∈X⟨K(y, x)fx, fy⟩Hy=X
x,y∈X⟨V(y)∗V(x)fx, fy⟩Hy
=∥X
x∈XV(x)fx∥2
K, f∈ F 0(X;H),
hence Kis positive semidefinite.
Appendix B.Proof of Theorem 2.2
(a)⇒(b). If Kis positive definite then, by Theorem 2.1, there exists a minimal linearisation
(K;V) ofK. Define R={V(·)∗f|f∈ K} , that is, Rconsists of all functions X∋x7→
V(x)∗f∈ H x, with f∈ K, in particular, V(·)∗fcan be viewed as an H-vector bundle, that is,
V(·)∗f∈ F(X;H) for all f∈ K. Thus, we can view Ras a linear subspace of F(X;H), with
all its algebraic operations.
We now show that the mapping
K ∋f7→Uf=V(·)∗f∈ R (2.1)
is bijective. By definition, this mapping is surjective, hence it remains to prove that it is
injective. To see this, let f, g∈ Kbe such that V(·)∗f=V(·)∗g. Then for arbitrary x∈Xand
h∈ H xwe have ⟨V(x)∗f, h⟩Hx=⟨V(x)∗g, h⟩Hx, equivalently, ⟨f−g, V(x(h⟩K= 0. Taking into
account the minimality of the linearisation, it follows that f=g. Thus, Uis bijective.
It is obvious that the bijective mapping Uas in (2.1) is linear. On Rwe introduce an inner
product ⟨·,·⟩Rdefined by
⟨Uf, Uf ⟩R=⟨V(·)∗f, V(·)∗g⟩K, f, g ∈ K, (2.2)
in other words, Uis now an isometric isomorphism between the Hilbert space Kand the inner
product space R, hence ( R;⟨·,·⟩R) is a Hilbert space as well.
We now show that ( R;⟨·,·⟩R) is a reproducing kernel Hilbert space with reproducing kernel
K. Indeed, since for all x, y∈Xand all h∈ H xwe have Kx(y)h=K(y, x)h=V(y)∗V(x)h,
it follows that Kx∈ R for all x∈X. On the other hand, for arbitrary f∈ R,x∈X, and
h∈ H x, we have
⟨f, K xh⟩R=⟨V(·)∗g, K xh⟩R=⟨V(·)g, V(·)∗V(x)h⟩R
=⟨g, V(x)h⟩K=⟨V(x)∗g, h⟩Hx,34 A. GHEONDEA AND C. T ˙ILK˙I
where g∈ H is the unique vector such that V(x)∗g=f. Thus, we proved that Kis the
reproducing kernel of R.
(b)⇒(a). Let ( R;⟨·,·⟩R) be a reproducing kernel Hilbert space with reproducing kernel
K. Using the reproducing property (rk3), for arbitrary n∈N,x1, . . . , x n∈X, and h1∈
Hx1, . . . , h n∈ H xn, we have
nX
i,j=1⟨K(xj, xi)hi, hj⟩Hxj=nX
i,j=1⟨Kxihi, Kxjhj⟩R=∥nX
i=1Kxihi∥2
R≥0,
hence Kis positive semidefinite.
Due to the uniqueness property of the reproducing kernel Hilbert space associated to a
positive semidefinite H-operator valued kernel K, it is natural to denote this reproducing
kernel Hilbert space by R(K).
Appendix C.A Direct Construction of R(K).
Given an arbitrary bundle of Hilbert spaces H={Hx}x∈Xand an H-operator valued kernel
K, we described the reproducing kernel Hilbert space R(K) through a minimal linearisation
ofK, as in the proof of the implication (a) ⇒(b) of Theorem 2.2, while a minimal Kolmogorov
decomposition of Kwas obtained as in the proof of the implication (a) ⇒(b) of Theorem 2.1.
One of the unpleasant trait of the mentioned construction of the Kolmogorov decomposition,
a GNS type construction in fact, is that, at a certain step, it makes a factorisation and hence,
the obtained Hilbert space consists of equivalence classes of vector cross-sections. On the other
hand, the reproducing kernel Hilbert space H(K) consists solely of vector cross-sections and,
as noted before, it is a Kolmogorov decomposition as well, hence it would be desirable to have
a direct construction of it, independent of the Kolmogorov decomposition. Such a direct, but
longer, construction, that yields simultaneously the reproducing kernel Hilbert space R(K) and
a minimal Kolmogorov decomposition of K, is more illuminating from certain points of view,
and we describe it in the following.
LetR0be the range of the convolution operator Kdefined at (2.7), more precisely, with the
definition of the convolution operator CKas in (2.7),
R0={f∈ F(X;H)|f=CKgfor some g∈ F 0(X;H)} (3.1)
={f∈ F(X;H)|fy=X
x∈XK(y, x)gxfor some g∈ F 0(X;H),ally∈X}.
A pairing ⟨·,·⟩R0can be defined on R0by
⟨e, f⟩R0=⟨g, h⟩K=⟨CKg, h⟩0=X
y∈X⟨e(y), h(y)⟩Hy=X
x,y∈X⟨K(y, x)g(x), h(y)⟩Hy, (3.2)
where f=CKhande=CKgfor some g, h∈ F 0(X;H). We observe that, with the previous
notation,
⟨e, f⟩R0=X
y∈X⟨e(y), h(y)⟩Hy=X
x,y∈X⟨K(y, x)g(x), h(y)⟩Hy (3.3)
=X
x,y∈X⟨g(x), K(x, y)h(y)⟩Hx=X
x∈X⟨g(x), f(x)⟩Hx,
which shows that the definition in (3.2) is correct, that is, it does not depend on gandh
such that e=CKgandf=CKh. In the following we prove that the pairing ⟨·,·⟩R0is an
inner product. It is easy to verify the linearity in the first argument, conjugate symmetry,LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 35
and nonnegativity. Hence, the Schwarz inequality holds as well. In order to verify its positive
definiteness, let f∈ R 0be such that ⟨f, f⟩R0= 0. By the Schwarz inequality, it follows that
⟨f, f′⟩R0= 0 for all f′∈ R 0. For arbitrary x∈Xandh∈ H xconsider the cross-section
bh∈ F 0(X;H) defined as in (2.3). Letting f′=CKbh∈ R 0, we thus have
0 =⟨f, f′⟩R0=⟨f, C Kbh⟩0=X
x∈X⟨fy,(bh)y⟩Hy=⟨fx, h⟩Hx,
hence, since x∈Xandh∈ H xare arbitrary, it follows that f= 0. Thus, ( R0;⟨·,·⟩R0) is an
inner product space contained in F(X;H).
For any x∈Xandh∈ H x, we consider the vector cross-section bh∈ F 0(X;H) defined at
(2.3) and note that
(CKbh)(y) =X
z∈XK(y, z)(bh)(z) =K(y, x)h=Kx(y)h, y ∈X, (3.4)
that is, CKbh=Kxh, which shows that Kxh∈ R 0. On the other hand, for any f∈ R 0, hence
f=CKgfor some g∈ F 0(X;H), we have
f(y) =X
x∈XK(y, x)g(x) =X
x∈XKx(y)g(x), y∈X, (3.5)
hence R0= Lin{Kxy|x∈X, h∈ H x}. In addition,
⟨f, K xh⟩R0=⟨f, C Kbh⟩R0=X
y∈X⟨f(y),(bh)(y)⟩Hy=⟨f(x), h⟩Hx.
Thus, the inner product space ( R0;⟨·,·⟩R0) has all properties (rk1)–(rk3), as well as a modified
version of the minimality property (rk4), except the fact that it is a Hilbert space.
By the standard procedure, let ( R;⟨·,·⟩R) be an abstract completion of the inner product
space ( R0;⟨·,·⟩R) to a Hilbert space. In order to finish this construction, all we have to
prove is that we can always choose R ⊆ F (X;H), in other words, this Hilbert space abstract
completion can always be realised inside F(X;H). Once this done, after a moment of thought
and taking into account that ( R0;⟨·,·⟩R) essentially has all properties (rk1)–(rk4), we can see
that (R;⟨·,·⟩R) is the reproducing kernel Hilbert space with reproducing kernel K.
Now, in order to prove that the Hilbert space abstract completion of ( R0;⟨·,·⟩R0) can be
realised within F(X;H), we can take at least two paths. One way is to use the existence part
of the reproducing kernel Hilbert space associated to K, a consequence of Theorem 2.2. A
second, more direct way, is to show that any Cauchy sequence, with respect to ∥ · ∥R0, with
elements in R0, converges pointwise on X to a vector cross-section in F(X;H) and that this
vector cross-section can be taken as the strong limit of the sequence as well.
Appendix D.Proof of Theorem 2.4
(a)⇒(b). Let x∈Xbe fixed, but arbitrary. It was already observed in Subsection 2.1
that, if HKis the reproducing kernel Hilbert space in F(X;H) with kernel K, then by the
reproducing property, we have
⟨f(x), h⟩Hx=⟨f, K xh⟩HK, f∈ H K, h∈ H x,
where Kx:Hx→ H Kis the linear operator defined by Kxh:=K(·, x)h, see the axiom (rk2).
Since, by axiom (rk2), Kxh∈ H Kfor all h∈ H x, the operator Kxis correctly defined. It is a
bounded operator because
∥Kxh∥2
HK=⟨Kxh, K xh⟩HK=⟨(Kxh)(x), h⟩Hx=⟨K(x, x)h⟩Hx≤ ∥K(x, x)∥∥h∥Hx,(4.1)36 A. GHEONDEA AND C. T ˙ILK˙I
where we have used the reproducing property (rk3).
Finally, again by the reproducing property (rk3), for any f∈ H Kand any h∈ H xwe have
⟨f(x), h⟩Hx=⟨f, K xh⟩HK=⟨K∗
xf, h⟩Hx,
hence the evaluation operator HK∋f7→f(x)∈ H xcoincides with K∗
xand hence it is bounded.
(b)⇒(a). For arbitrary x∈X, let Ev x:H → H xbe the evaluation operator Ev xf:=f(x),
for all f∈ H. By assumption, Ev xis a bounded operator for all x∈X. We consider the
H-valued kernel
K(y, x) = Ev yEv∗
x, x, y ∈X.
From Theorem 2.1 it follows that Kis a positive semidefinite H-valued kernel and hence, by
Theorem 2.2, there exists and it is unique, the reproducing kernel Hilbert space HKwith kernel
K. In the following we show that His the reproducing kernel Hilbert space with kernel K.
The axiom (rk1) holds, by assumption. For the axiom (rk2), let us observe that, for all x∈X
andh∈ H x, we have
(Kxh)(y) =K(y, x)h= Ev yEv∗
xh= (Ev∗
xh)(y), y∈X,
hence Kxh= Ev∗
xh∈ H K. This proves that the axiom (rk2) holds and, in addition, that
K∗
x= Ev x, x∈X.
Finally, for the axiom (rk3), let f∈ H,x∈X, and h∈ H xbe arbitrary. Then,
⟨f(x), h⟩Hx=⟨Evxf, h⟩Hx=⟨f,Ev∗
xh⟩HK=⟨f, K xh⟩HK.
This shows that the axiom (rk3) holds as well.
Finally, by the uniqueness of the reproducing kernel Hilbert space associated to K, it follows
thatH=HK.
References
[1]N. Aronszajn , La th´ eorie g´ en´ erale des noyaux reproduisants et ses applications, Premi´ ere Partie, Proc.
Cambridge Philos. Soc. 39(1944) p. 133.
[2]N. Aronszajn , Theory of reproducing kernels, Trans. Amer. Math. Soc. 68(1950), 337-404.
[3]S. Ay and A. Gheondea , Representations of ∗-semigroups associated to invariant kernels with values
adjointable operators, Linear Algebra and its Applications 486(2015), 361–388.
[4]S. Ay and A. Gheondea , Representations of ∗-semigroups associated to invariant kernels with values
continuously adjointable operators, Integral Equations Operator Theory 87(2017), no. 2, 263–307.
[5]S. Ay and A. Gheondea , Invariant weakly positive semidefinite kernels with values in topologically
ordered ∗-spaces, Studia Mathematica 248(2019), 255–294.
[6]M.B. Bekka, P. de la Harpe , Irreducibility of unitary group representations and reproducing kernel
Hilbert spaces, Expositiones Mathematicae ,21(2003), 115–149.
[7]M. Belkin, P. Niyogi, and V. Sindhwani , Manifold regularization: A geometric framework for learning
from labeled and unlabeled examples, Journal of Machine Learning Research ,7(2006), 2399–2434.
[8]S. Bergman ,¨Uber die Entwicklung der harmonischen Funktionen der Ebene und des Raumes nach Or-
thogonalfunktionen, Math. Ann. 86(1922), 238–271.
[9]A. Berlinet and C. Thomas-Agnan ,Reproducing Kernel Hilbert Spaces in Probabillity and Statistics ,
Springer Science+Business Media, LLC, New York, 2004.
[10]B. Blackadar ,Operator Algebras. Theory of C∗-Algebras and von Neumann Algebras , Springer, Berlin
2006.
[11]S. Bochner , Hilbert distances and positive definite functions, Ann. of Math. ,42(1941), 647–656.
[12]U. Brefeld, T. T ¨artner, T. Scheffer, and S. Wrobel , Efficient co-regularised least square regres-
sion, in Proceedings of the International Conference on Machine Learning (ICML) , 2006.
[13]C. Brouard, F. D’Alche-Buc, and M. Szafranski , Semi-supervised learning penalized output kernel
regression for link prediction, in Proceedings of the International Conference on Machine Learning (ICML) ,
2011.LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 37
[14]S. Bucak, R. Jin, and A.K. Jain , Multiple kernel learning for visual object recognition: A review, IEEE
Transactions on Pattern Analysis and Machine Intelligence ,36:7(2014), 1354–1369.
[15]R.H. Byrd, M.E. Hribar, and J. Nocedal , An Interior Point Algorithm for Large-Scale Nonlinear
Programming, SIAM Journal on Optimization ,9(1999), 877–900.
[16]R.H. Byrd and J.C. Gilbert and J. Nocedal , A Trust Region Method Based on Interior Point
Techniques for Nonlinear Programming, Mathematical Programming ,89(1) (2000), 149–185.
[17]A. Caponnetto, M. Pontil, C. Micchelli, and Y. Ying , Universal multi-task kernels, Journal of
Machine Learning Research ,9(2008), 1615–1646.
[18]C. Carmelli, E. De Vito, and A. Toigo , Vector-valued reproducing kernel Hilbert spaces of integrable
functions and Mercer theorem, Analysis and Applications ,4(2006), 377–408.
[19]O. Chapelle, B. Sch ¨olkopf, and A. Zien (Eds.) ,Semi-Supervised Learning , MIT Press, Cambridge,
MA, 2006.
[20]T. Constantinescu ,Schur Parameters, Factorization and Dilation Problems , Birkh¨ auser Verlag, Basel
1996.
[21]T. Constantinescu, A. Gheondea , Representations of Hermitian kernels by means of Krein spaces,
Publ. RIMS. Kyoto Univ. ,33(1997), 917–951.
[22]T. Constantinescu, A. Gheondea , Representations of Hermitian kernels by means of Krein spaces. II.
Invariant kernels. Commun. Math. Phys. 216(2001), 409–430.
[23]K.R. Davidson ,C∗-Algebras by Example , American Mathematical Society, Providence RI, 1996.
[24]D.J. Evans and J.T. Lewis ,Dilations of Irreversible Evolutions in Algebraic Quantum Theory , Com-
munications of the Dublin Institute for Advanced Studies, Series A (Theoretical Physics), Vol. 24, Dublin
1977.
[25]T. Evgeniou, M. Pontil, and C.A. Micchelli , Learning multiple tasks with kernel methods, Journal
of Machine Learning Research ,6(2005), 615–637.
[26]A. Gheondea , Dilations of some VH-spaces operator valued invariant kernels, Integral Equations and
Operator Theory 74(2012), no. 4, 451–479.
[27]R. Godement , Les fonctions de type positif et la th´ eorie des groupes, Trans. Amer. Math. Soc. 63(1948),
1–84.
[28]R. Gower and D.A. Lorenz and M. Winkler , A Bregman-Kaczmarz method for nonlinear systems
of equations, Computational Optimization and Applications ,87(2024), 1059–1098.
[29]Y. Hashimoto, I. Ishikawa, M. Ikeda, F. Komura, T. Katsura, and T., Y. Kawahara , Repro-
ducing kernel Hilbert C∗-module and kernel mean embeddings, Journal of Machine Learning Research ,
22(2021), 1—56.
[30]Y. Hashimoto, Z. Wang, and T.C. Matsui ,∗-Algebra net: A new approach generalizing neural net-
work parameters to C∗-algebra, in Proceedings of the 39th International Conference on Machine Learning
(ICML) , 2022
[31]Y. Hashimoto and M. Ikeda and H. Kadri , Learning in rkhm: a C∗-algebraic twist for kernel machines,
inProceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS) ,
2023.
[32]Y. Hashimoto and M. Ikeda and H. Kadri , Deep learning with kernels through RKHM and the
Perron-Frobenius operator, in Proceedings of the 37th Conference on Neural Information Processing Sys-
tems (NeurIPS) , 2023.
[33]Y. Hashimoto, M. Ikeda, H. Kadri ,C∗-Algebraic Machine Learning: Moving in a New Direction,
arXiv:2402.02637 [cs.LG].
[34]S. Hazely, D. Kamzolov, D. Pasechnyuk, A. Gasnikov, P. Richtarik, and M. Takac , A damped
Newton method achieves global O(1/k2) and local quadratic convergence rate, in 36th Conference on Neural
Information Processing Systems (NeurIPS 2022) , 2022.
[35]H. Kadri, E. Duflos, Ph. Preux, S. Canu, N. Rakotomamonjy, and N. Audifren , Operator-
valued kernels for learning from functional response data, Journal of Machine Learning Research ,17(2016),
1–54.
[36]A.N. Kolmogorov , Stationary sequences in Hilbert space [Russian], Bull. Math. Univ. Moscow ,2(1941),
1–40.
[37]M.G. Krein , Hermitian-positive kernels on homogenous spaces. I, Ukrain Math. Zurnal/ Amer. Math.
Soc. Translations (2) ,1:4/34 (1949/1963), 64–98/109–164.
[38]M.G. Krein , Hermitian-positive kernels on homogenous spaces. II, Ukrain Math. Zurnal/ Amer. Math.
Soc. Translation (2) ,2/34 (1950/1963), 10–59/109–164.38 A. GHEONDEA AND C. T ˙ILK˙I
[39]O.R. Kwon and H. Zou , Leaky hockey stick loss: the first negatively divergent margin-based loss function
for classification, Journal of Machine Learning Research ,24(2023), 1–40.
[40]A. Lambert ,Learning Function-Valued Functions in Reproducing Kernel Hilbert Spaces with Integral Loss:
Application to Infinite Task Learning , PhD Thesis, Institut Polytechnique de Paris, 2021.
[41]E.C. Lance ,Hilbert C∗-Modules: A Toolkit for Operator Algebraists , Cambridge University Press, Cam-
bridge 1995.
[42]R.M. Loynes , On generalized positive-definite functions, Proc. London Math. Soc. III. Ser. ,15(1965),
373–384.
[43]R.M. Loynes , Linear operators in VH-spaces, Trans. Amer. Math. Soc. ,116(1965), 167–180.
[44]R.M. Loynes , Some problems arising from spectral analysis, in Symposium on Probability Methods in
Analysis (Loutraki, 1966) , pp. 197–207, Springer Verlag, Berlin 1967.
[45]Y. Luo, D. Tao, C. Xu, C. Xu, H. Liu, and Y. Wen , Multiview vector-valued manifold regular-
ization for multilabel image classification, IEEE Transactions on Neural Networks and Learning Systems ,
24(5) (2013), 709–722.
[46]M. D. McKay and R. J. Beckman and W. J. Conover , A comparison of three methods for selecting
values of input variables in the analysis of output from a computer code, Technometrics ,21(1979), 239–245.
[47]J. Mercer , Functions of positive and negative type and their connection with the theory of integral
equations, Philos. Trans. Roy. Soc. London Ser. A 209(1909) 415–446.
[48]C. A. Micchelli and M. Pontil , On learning vector-valued functions, Neural Computation ,17(2005),
177–204.
[49]E.H. Moore ,General Analysis , Memoirs of the American Philosophical Society, Part I, 1935, Part II,
1939.
[50]H.Q. Minh and V. Sindwhani , Vector-valued manifold regularization, in Proceedings of the 28th Inter-
national Conference on Machine Learning , 2011.
[51]H.Q. Minh, L. Bazzani, and V. Murino , A unifying framework in vector-valued reproducing kernel
Hilbert spaces for manifold regularization and co-regularized multi-view learning, Journal of Machine
Learning Research ,17(2016) 1–72.
[52]E.H. Moore , General analysis, Memoirs of the American Philosophical Society , Part I, 1935, Part II,
1939.
[53]R.D.C. Monteiro and J.-S. Pang , A potential reduction Newton method for constrained equations,
SIAM J. Optim. ,9(1999), 729–754.
[54]K.R. Parthasarathy and K. Schmidt ,Positive-definite Kernels, Continuous Tensor Products and
Central Limit Theorems of Probability Theory , Lecture Notes in Mathematics, vol. 272, Springer Verlag,
Berlin 1972.
[55]V.I. Paulsen and M. Raghupathi ,An Introduction to the Theory of Reproducing Kernel Hilbert Spaces ,
Cambridge University Press, Cambridge 2016.
[56]G. Pedrick ,Theory of Reproducing Kernels for Hilbert Spaces of Vector Valued Functions , PhD Thesis,
University of Kansas, 1957.
[57]J. Peypouquet ,Convex Optimization in Normed Spaces , Springer, Berlin 2014.
[58]C. Popa , Convergence rates for Kaczmarz type algorithms, Numerical Algorithms ,79(2018), 1–17.
[59]D. Rosenberg, V. Sindhwani, P. Bartlett, and P. Niyogi , A kernel for semi-supervised learning
with multi-view point cloud regularization, IEEE Sig. Proc. Mag. ,26(5) (2009), 145–150.
[60]S. Saitoh and Y. Sawano ,Theory of Reproducing Kernels and Applications , Springer Verlag, Berlin
2016.
[61]B. Sch ¨olkopf and A. Smola ,Learning with Kernels: Support Vector Machines, Regularization, Opti-
mization, and Beyond , The MIT Press, Cambridge, 2002.
[62]J. Sch ¨onberg , Metric spaces and positive definite functions, Trans. Amer. Math. Soc. 44(1938), 522–536.
[63]L. Schwartz , Sous espace Hilbertiens d’espaces vectoriel topologiques et noyaux associ´ es (noyaux repro-
duisants), J. Analyse Math. ,13(1964), 115–256.
[64]V. Sindhwani and D.S. Rosenberg , An RKHS for multi-view learning and manifold co-regularization,
inProceedings of the 25thInternational Conference on Machine Learning, Helsinki, Finland, 2008.
[65]M. Stein , Large sample properties of simulations using latin hypercube sampling, Technometrics ,
29(1987), 143–151.
[66]I. Steinwart, A. Christmann ,Support Vector Machines , Springer Verlag, Berlin 2008.
[67]S. Sun , Multi-view Laplacian support vector machines, in Proceedings of the International Conference on
Advanced Data Mining and Applications (ADMA) , pp. 209–211, 2011.LOCALISATION OF REGULARISED AND MULTIVIEW LEARNING 39
[68]B. Sz.-Nagy , Prolongement des transformations de l’espace de Hilbert qui sortent de cet espace, in
Appendice au livre “Le¸ cons d’analyse fonctionnelle” par F. Riesz et B. Sz.-Nagy, pp. 439-573 Akademiai
Kiado, Budapest, 1955.
[69]A.N. Tikhonov , Solution of ill-posed formulated problems and the regularization method [Russian], Dok-
lady Akademii Nauk SSSR ,151(1963), 501–504.
[70]R.A. Waltz and J. L. Morales and J. Nocedal and and D. Orban , An interior algorithm for nonlin-
ear optimization that combines line search and trust region steps, Mathematical Programming ,107(2006),
391–408.
[71]L. Zhao, M. Mammadov, and J. Yearwood , From convex to nonconvex: a loss function analysis for
binary classification, 2010 IEEE International Conference on Data Mining Workshops , 1281–1288.
Institutul de Matematic ˘a al Academiei Rom ˆane, 21 Calea Grivit ¸ei 010702 Bucures ¸ti, Rom ˆania
andDepartment of Mathematics, Bilkent University, 06800 Bilkent, Ankara, Turkey
Email address :A.Gheondea@imar.ro and aurelian@fen.bilkent.edu.tr
Department of Mathematics and Division of Computational Modeling and Data Analytics,
Virginia Polytechnic Institute and State University, Blacksburg Virginia, 24061 U.S.A.
Email address :cankat@vt.edu