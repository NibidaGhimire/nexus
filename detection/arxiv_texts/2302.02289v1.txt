Selecting the Best Optimizers for Deep Learning based Medical
Image Segmentation
Aliasghar Mortazia, Vedat Cicekb, Elif Kelesc, Ulas Bagcic
aV olastra Therapeutics, NYC, NY , USA.
bDepartment of Cardiology, Health Sciences University, Istanbul, Turkey.
cMachine & Hybrid Intelligence Lab, Department of Radiology, Northwestern University, Chicago, IL, USA.
Abstract.
Purpose: The goal of this work is to identify the best optimizers for deep learning in the context of cardiac image
segmentation and to provide guidance on how to design segmentation networks with effective optimization strategies.
Approach: Most successful deep learning networks are trained using two types of stochastic gradient descent (SGD)
algorithms: adaptive learning and accelerated schemes. Adaptive learning helps with fast convergence by starting with
a larger learning rate (LR) and gradually decreasing it. Momentum optimizers are particularly effective at quickly
optimizing neural networks within the accelerated schemes category. By revealing the potential interplay between
these two types of algorithms (LR and momentum optimizers or momentum rate (MR) in short), in this article, we
explore the two variants of SGD algorithms in a single setting. We suggest using cyclic learning as the base optimizer
and integrating optimal values of learning rate and momentum rate. The new optimization function proposed in this
work is based on the Nesterov accelerated gradient optimizer, which is more efï¬cient computationally and has better
generalization capabilities compared to other adaptive optimizers.
Results: We investigated the relationship of LR and MR under an important problem of medical image segmentation
of cardiac structures from MRI and CT scans. We conducted experiments using the cardiac imaging dataset from
the ACDC challenge of MICCAI 2017, and four different architectures shown to be successful for cardiac image
segmentation problems. Our comprehensive evaluations demonstrated that the proposed optimizer achieved better
results (over a 2% improvement in the dice metric) than other optimizers in deep learning literature with similar or
lower computational cost in both single and multi-object segmentation settings.
Conclusions: We hypothesized that combination of accelerated and adaptive optimization methods can have a drastic
effect in medical image segmentation performances. To this end, we proposed a new cyclic optimization method
(CLMR ) to address the efï¬ciency and accuracy problems in deep learning based medical image segmentation. The
proposed strategy yielded better generalization in comparison to adaptive optimizers.
Keywords: Deep Learning Optimization, Segmentation, Cyclic learning, adaptive optimization, accelerated optimiza-
tion.
*Ulas Bagci, ulasbagci@gmail.com
1 Introduction
Optimization algorithms are used in the training phase of deep learning, where the model is pre-
sented with a batch of data, the gradients are calculated, and the weights and biases are updated
using an optimization algorithm. Once the model has been trained, it can then be used for inference
on new data.
1arXiv:2302.02289v1  [eess.IV]  5 Feb 2023Stochastic gradient descent (SGD) algorithms are the main optimization techniques used to
train deep neural networks. These algorithms can be divided into two categories: adaptive learning
rate methods (e.g., Adam and AdaGrad) and accelerated schemes (e.g., Nesterov momentum).
Both the learning rate (LR) and momentum rate (MR) are important factors in the optimization
process. LR, in particular, is a key adjustable parameter that has been extensively studied and
modiï¬ed over the years. The momentum term was introduced to the optimization equation by
Rumelhart and Williams in 1986 to allow for larger changes in the network weights without causing
oscillation.1
There have been controversial results in the literature about the characteristics of available
optimization methods. Therefore, there is a need for exploring which optimization method should
be chosen for particular tasks. Most neural network optimizers have been evaluated and tested on
classiï¬cation tasks, which have much lower output dimensions compared to segmentation tasks,
which have much higher output dimensions. Hence, these differences between classiï¬cation and
segmentation problems imply a different investigation and method for optimization. In this paper,
we develop a new optimization method by exploring LR and MR optimizers for medical image
segmentation problems for the ï¬rst time in the literature. Our proposed optimizer is simple and
promising because it ï¬xes the problems with traditional optimizers and demonstrates how a simple
new formulation can solve surprisingly these problems.
Non-adaptive vs. adaptive optimizers: SGD is the dominant optimization algorithm in deep
learning, which is simple and performs well across many applications. However, it has the disad-
vantage of scaling the gradient uniformly in all directions (for each parameter of network). Another
challenge in SGD is to choose an appropriate value for LR. Since LR is a ï¬xed value in SGD based
approaches, it is critical to set it up appropriately since it can directly affect both the convergence
2speed and prediction accuracy of neural networks. There have been several studies trying to solve
this problem by adaptively changing the LR during training which are mostly known as â€adap-
tive optimizersâ€. Based on the history of changes in gradients during network optimization, LR
is adapted in each iteration. Examples of such methods consist of ADAM ,2ADAGrad,3and RM-
SProp.4In general, adaptive optimizers make training faster, which has led to their wide use in
deep learning applications.
The development of momentum in neural network optimizers has followed a similar trajectory
as the learning rate. Momentum optimizers5introduced to speed up convergence by considering
the changes from last iteration with a multiplier, which called momentum , in updating parameters
in current iteration. Selecting an appropriate value for the momentum rate (MR) was initially
difï¬cult, but this issue was addressed with the introduction of adaptive optimizers like ADAM,
which can adaptively adjust both the MR and LR. These adaptive optimizers have become very
popular in the ï¬eld because they quickly converge on training data.
Although they are widely used, adaptive optimizers may converge to different local minima
compared to classical SGD approaches, which can lead to worse generalization and out-of-sample
performance. This has been demonstrated by a growing number of recent studies.6â€“8To improve
the generalization ability of neural networks, researchers have returned to using original SGD ap-
proaches but with new strategies for improving convergence speed. For example, the YellowFin
optimizer demonstrated that manually tuning the learning rate and momentum rate can lead to
better results than using the ADAM optimizer.8Although it was a proof-of-concept study that pro-
vided evidence for the counterintuitive idea that non-adaptive methods can be effective. However,
in practical applications, manually tuning these rates is challenging and time-consuming.
In another attempt, a cyclic learning rate (CLR) was introduced in7to change the LR according
3to a cycle (i.e triangle or Gaussian), proposing a practical solution to hand-tuning requirements.
The CLRâ€™s only disadvantage was that a ï¬xed MR could limit the search states of LR and MR and
cause them to fail until ï¬nd an optimal solution. Our work will go beyond this constraint.
Summary of our contribution: By motivated from,7herein we introduce a new version of
CLR called â€Cyclic Learning/Momentum Rateâ€ (CLMR). This new optimizer alternates the val-
ues of the learning rate and momentum rate during training, which has two beneï¬ts compared to
adaptive optimizers. First, it is more efï¬cient computationally. Second, it has better generalization
performance. Furthermore, CLMR leads to better results than conventional approaches such as
SGD and CLR. Lastly, we investigate the effect of changing the frequency of cyclic function in
training and generalization and suggest the optimum frequency values. We investigate several op-
timizers commonly used in medical image segmentation problems, and compare their performance
as well as generalization ability in single and multi-object segmentation settings by using cardiac
MR images (Cine-MRI).
The rest of the paper is organized as follows. In Section 2, we introduce the background infor-
mation for neural network optimizers, their notations and their use in medical image segmentation.
In section 3, we give the details of the proposed method and network architectures on which seg-
mentation experiments have been conducted. Experimental results are summarized in Section 4.
Section 5 concludes the paper with discussions and future work.
2 Background
Optimizing a deep neural network, which is a high-dimensional system with millions of parame-
ters, is one of the most challenging aspects of making these systems more practical. Designing and
implementing the best optimizer for deep network training has received a lot of attention in recent
4decades. These studies mainly address two major issues: (1) Making the network training as fast
as possible (fast convergence), (2) increasing the generalizability of networks. SGD optimizers
have been the most popular optimizer in deep networks due to their low computational cost, fast
convergence. There have been major modiï¬cations to original SGD optimizer during last decade
to make them for efï¬cient for training deep nets. The following are some of the key optimization
studies related to our efforts.
2.1 Optimizers with ï¬xed LR/MR
SGD and Mini-batch gradient descent were ï¬rst optimizers used for training neural networks.
The updating rule for these optimizers include only the value of last iteration as shown in Eq. 1.
Choosing appropriate value for a LR is challenging in these optimizers since if LR is very small
then convergence is very slow; and if LR is set high, the optimizer will oscillate around global
minima instead of converging:
i=i 1 riJ(i); (1)
whereis network parameters, is LR, Jis cost function to be minimized (function of ,
X(input), and Y(labels)). The equation 1 can be considered as an updating rule for SGD and
mini-batch gradient descent by choosing XandYas whole samples, a single sample, or a batch
of samples in a dataset.
The Momentum optimizer was designed to accelerate the optimization process by taking into
account the values from previous iterations, weighted by a factor known as â€momentum,â€ as men-
5tioned in.5The updating for this optimizer is deï¬ned as:
i=i 1 riJ(i) (i 1 i 2); (2)
wheredenotes the momentum rate (MR). In Momentum optimizer, the past iterations donâ€™t play
any role in cost function and cost function is only calculated for the current iteration only. Also,
similar to LR, choosing a proper value for MR is challenging and it has a correlation with LR too.
Nesterov accelerated gradient9(NAG) was then introduced to address the limitation of momen-
tum optimizers as well as to accelerate the convergence by including information from previous
iterations in calculating the gradient of the cost function as shown in the following equation:
i=i 1 riJ(i (i 1 i 2)) (i 1 i 2): (3)
Compared to optimizers with ï¬xed LR/MR, the NAG optimizer generally shows improved perfor-
mance in both convergence speed and generalizability.
2.2 Optimizers with adaptive LR and MR
A signiï¬cant disadvantage of optimizers with a ï¬xed LR/MR is that they cannot incorporate in-
formation from the gradients of past iterations in adjusting the learning and momentum rates. For
instance, they cannot increase the learning rate for dimensions with a small slope to improve con-
vergence, or reduce the learning rate for dimensions with a steep slope to avoid oscillation around
the minimum point. Adagrad3is one of ï¬st adaptive LR optimizers used in deep networks adapting
the learning rate for each parameter in the network by dividing the gradient of each parameter by
6its sum of the squares of gradient, as follows:
i=i 1 1pGi+riJ(i); (4)
whereGiis a diagonal (square) matrix and each diagonal element equal to the sum of the square
of gradient of its corresponding parameters:
Gi=IX
i=1(riJ(i))2; (5)
where Iis the current iteration.
One of the drawbacks of AdaGrad is gradient vanishing due to accumulation of all past square
gradient in denominator of Equation 6 during the training. This leads the gradients to converge
to zero after several epochs in training. However, AdaDelta ,RMSProp , and ADAM optimizers
solved this problem by considering a sum of the past samples within a pre-deï¬ned window. ADAM
optimizerâ€™s updating rule uses past squared gradient (as scale) and also like momentum, it keeps
an exponentially decaying average of past gradients. Hence, these adaptive optimizers have advan-
tages over the AdaGrad by adaptively changing both RL and ML as well as resolving the gradient
vanishing issue:
i=i 1 i1rJ(i 2) (1 1)rJ(i 1)p2+riJ(i): (6)
Adaptive learning methods are costly because they are required to calculate and keep all the past
gradients and their squares to update the next parameters. Also, the adaptive learning optimizer
may converge into different minima in comparison with ï¬xed learning rate optimizers.6â€“8
7Alternatively, Cyclic learning rate (CLR) was proposed to change the learning rate during train-
ing, which needed no additional computational cost. CLR is a method for training neural networks
that involves periodically changing the learning rate during training. As mentioned earlier, the
learning rate is typically adjusted according to a predetermined schedule, such as increasing the
learning rate from a low value to a high value and then decreasing it back to the low value over a
set number of training iterations. The learning rate is then reset and the process is repeated. This
can help the optimization process by allowing the model to make larger updates at the beginning of
training and smaller updates as training progresses, potentially leading to faster convergence and
better model performance.7Later in Figure 3a, we show how we use CLR in our methodology.
2.3 Cardiac Image Segmentation
Cardiovascular diseases (CVDs) are the leading cause of death worldwide according to the World
Health Organization (WHO). CVDs lead to millions of deaths annually and are expected to cause
over 23.6 million deaths in 2030.10Cine-MR imaging can provide valuable information about
cardiac diseases due to its excellent soft tissue contrast. For example, ejection fraction (EF), an
important metric measuring how much blood the left ventricle pumps out with each contraction,
can be measured with Cine-MRI. To this end, radiologists often manually measure the volume of
the heart at the end of the systole (ES) and the end of the diastole (ED) to measure EF. This is a
time-consuming process with known inter-, and intra-observer variations. Due to its signiï¬cance
in functional assessment of heart, there have been numerous machine learning based automated
algorithms developed in the literature for measuring EF. In this study, we make our efforts in this
application due to its importance in the clinic.
There is a considerable amount of research dedicated to the problem of cardiac segmentation
8from MR or CT images. Since Xu et al. found a correlation between motion characteristics and tis-
sue properties, they developed a combined motion feature learning architecture for distinguishing
myocardial infarction.11In our another attempt, CardiacNet in12proposed a multi-view CNN to
segment the left atrium and proximal pulmonary veins from MR images following by an adaptive
fusion. The shape prior information from deep networks were used to guide segmentation network
to delineate cardiac substructures from MR images.13, 14As previously stated, the literature and
methodologies for cardiac segmentation are extensive. Readers are invited to consult references15
and16for more comprehensive information.
3 Methods
We approach the optimization problem from the perspective of a signiï¬cant medical image analysis
application: segmentation. Segmentation is rarely studied from an optimization perspective in
comparison to classiï¬cation.
Over the past few years, there has been a dramatic increase in the use of CNN in computer
vision and medical imaging applications, more recently combined with Transformers.17â€“21The
successful CNN-based segmentation approaches can be divided into three broad categories. The
ï¬rst category is named encoder-decoder architecture. One of the most famous works in this cate-
gory has been done by Badrinarayanan and et al.,22called SegNet , and it is designed for semantic
segmentation. Another category of architecture is called ResNet23and it is proposed by He and et
al. for image recognition but later, the U-Net24was proposed from the similar extending recogni-
tion into segmentation with a U-shaped network consisting of skip connections between encoder
and decoder. The last category of the architecture is based on the DenseNet ,25instead of having a
residual connection, the vectors are concatenated to each other to maximize the information ï¬‚ow
9through the network. Hence, the information loss during backpropagation can be minimized by
considering these connections. DenseNet itself is proposed for image classiï¬cation, but by com-
bining concepts of the ResNet andDenseNet , a new architecture was introduced in26in a U-Net
shape to do segmentation. There are many more architectures based on the U-Net style with adap-
tation from the CNN and Transformers literature. In our study, we conducted experiments in three
different (mostly used) segmentation architectures to demonstrate the effect of the connections,
as explained in the following subsection. One may increase the number of architectures for more
comparisons, but this is outside the scope of our study. CNN Architectures used in the experiments
are the following:
1. Encoder-Decoder Architecture : This architecture simply consists of the encoder and de-
coder part as illustrated in Figure 1, without considering red skip connections. The ï¬lter size in
all the layer are 33and each encoder and decoder part include 5CNN blocks and each CNN
blocks consist of different number of layers as mentioned in Table 1. Also, the number of ï¬lters in
each CNN block are a ï¬xed number and they are mentioned in Table 1 for each layer. Each layer
within the CNN block includes Convolution+Batch normalization+ReLu as activation function
(CBR ).
2. U-Net Architecture : U-Net is particularly popular in medical image analysis. The U-Net
model is based on a fully convolutional network, which means that it is built entirely out of con-
volutional layers and does not contain any fully connected layers. This makes it well-suited for
image segmentation tasks, as it can process input images of any size and output a corresponding
segmentation map. The U-Net model is known for its ability to handle small, sparsely annotated
training datasets, making it a useful tool for medical image analysis where such datasets are com-
mon. This architecture is similar to the Encoder-Decoder architecture as illustrated in Figure 1
10with red skip connections from encoder to decoder. The number of layers and ï¬lters for each
block are mentioned in Table 1.
3. DenseNet Architecture : DenseNet is another convolutional neural network architecture
that was developed to improve upon the efï¬ciency of training deep networks. The key idea behind
DenseNet is to connect all layers in the network directly to every other layer, rather than only
connecting each layer to its immediate neighbors as is done in traditional convolutional networks.
This allows the network to learn more efï¬cient feature representations and reduces the risk of
overï¬tting. DenseNets have been successful in a number of applications and have achieved state-
of-the-art performance on image classiï¬cation and segmentation tasks. We will use two different
DenseNet architectures in our experiments. First, the architecture in Figure 1 with dense blocks
(DBs) and skip connections is DenseNet 1. Then, in order to use higher growth rate (GR), in
DenseNet 2, at the end of each block a convolution layer with kernel size of 11is used to decrease
number of its input ï¬lters by Crate, which Cis equal 2 in this paper. The GR in DenseNet 2
increased to 24 (from 16 in DenseNet 1) while the number of parameters decreased (Table 1).The
number of CBR layers and also the number of parameters are mentioned in Table 1.
3.1 Dense Block
Within the DB, a concatenation operation is done for combining the feature maps (through direc-
tion (axis) of the channels) for the last three layers. So, if the input to lthlayer is Xl, then the
output of lthlayer is:
F(Xl) =CBR (Xl): (7)
11Table 1 Number of layers in each block of different architectures and number of parameters.
EncDec U-NetDenseNet 1
(GR=16)DenseNet 2
(GR=24)
Block 1 6 layers,#ï¬lters=32 6 layers,#ï¬lters=32 6 layers 6 layers
Block 2 8 layers,#ï¬lters=64 8 layers,#ï¬lters=64 8 layers 8 layers
Block 3 11 layers,#ï¬lters=128 11 layers,#ï¬lters=128 11 layers, 11 layers
Block 4 15 layers,#ï¬lters=256 15 layers,#ï¬lters=256 15 layers 15 layers
Block 5 20 layers,#ï¬lters=512 20 layers,#ï¬lters=512 20 layers 20 layers
Block 6 20 layers,#ï¬lters=512 20 layers,#ï¬lters=512 20 layers 20 layers
Block 7 15 layers,#ï¬lters=256 15 layers,#ï¬lters=256 15 layers 15 layers
Block 8 11 layers,#ï¬lters=128 11 layers,#ï¬lters=128 11 layers 11 layers
Block 9 8 layers,#ï¬lters=64 8 layers,#ï¬lters=64 8 layers 8 layers
Block 10 6 layers,#ï¬lters=32 6 layers,#ï¬lters=32 6 layers 6 layers
# of params
(in million):77.5 79.1 7.7 8.8
Since we are doing concatenation before each layer (except the ï¬rst one), the output of each layer
can be calculated only by considering the input and output of ï¬rst layer as following:
F(Xl) =F(l0=l 1_
l0=0F(Xl0)) for l1
and l=f1;2;:::; L;g;(8)
where_is concatenation operation. In addition, for initialization F(X 1)andF(X0)are con-
sidered asfgandX1respectively which fgis an empty set and there are Llayers inside of the
block.
Assuming the number of output features for each layer is Kout(channel out) and the number
of input features for ï¬rst layer is Kin1(channel in). Then, the feature maps growth (channel out)
for second, third, . . . , and Lthlayer are Kout+Kin1, 2Kout+Kin1, . . . , and (L 1)Kout+Kin1
respectively. The growth rate for the DB is the same as fourth layer.
12Fig 1 CNN Architecture is used for pixel-wise segmentation. The architecture with CNN blocks without red skip
connections is Encoder-Decoder architecture. The architecture with red skip connection (Fig. 2a) is called U-Net , if
connections are with Dense block (Fig. 2b), it is called Tiramisu (DenseNet for segmentation)
Fig 2 (a) CNN block used in Enc-Dec and U-Net architectures, (b) Dense block used in Tiramisu architecture.
3.2 Cyclic Learning/Momentum Rate Optimizer
Smith et al7argued that a cycling learning may be a more effective alternative to adaptive optimiza-
tions especially from generalization perspective. Basically, cyclic learning includes a pre-deï¬ned
cycle (such as triangle or Gaussian function) that learning rate is changing according to that cycle.
Here, we hypothesize (and show later in the results section) that having a cyclic momentum in
Nesterov optimizer (Eq. 2) can lead to a better accuracy in segmentation task in generalization
phase. As a reminder, momentum in Eq. 2 was used to consider the past iterations by a coefï¬cient
13called momentum . So, choosing the proper value for momentum is challenging. To this end, we
propose changing the MR in the same way that we changed the LR, and we considered the cyclic
triangle function for both MR and LR as illustrated in the Figure 3. cyclelrandcyclemrdetermine
the period of triangle function for LR and MR are deï¬ned by:
cyclelr=ClrIt; (9)
cyclemr=CmrIt; (10)
whereClrandCmrare positive even integer numbers, Itis number of iteration per each epoch.
In Figures 3a and 3b, the cyclic function for different values of ClrandCmrare illustrated. LR
during whole training can be determined from equation 11:
LR =8
>>><
>>>:2max lr min lr
ClrIti+minlr; for N cyclelri<2N+1
2cyclelr
 2max lr min lr
ClrIti+ 2maxlr minlr; for2N+1
2cyclelri<(N+ 1)cyclelr;
(11)
wheremaxlrandminlrare maximum and minimum values of LR function, respectively. iis the
iteration indicator during whole training process and i2f1;2;:::;ItEpg, whichEpis total
number of epochs in training and Nis a set of natural number. MR can also be determined as:
MR =8
>>><
>>>:2max mr min mr
CmrIti+minmr; for N cyclemri<2N+1
2cyclemr
 2max mr min mr
CmrIti+ 2maxmr minmr; for2N+1
2cyclemri<(N+ 1)cyclemr;
(12)
14wheremaxmrandminmrare maximum and minimum values of MR function, respectively.
Equations 11 and 12 are used to determine the values of LR and MR in each iteration during
training. One of the challenges in using these cyclic LR and MR functions are determining the
values of some variables in the equations including maxlr,minlr, andClrfor LR; and also maxmr,
minmr, andCmrfor MR. For ï¬nding the the maxlrandminlrvalues, as it suggested in,7one can
run the networks with different LR values for a few epochs and then these values are chosen
according to how network accuracy changes. Since, when both LR and MR change dynamically
and the one value can affect the other one (considering the optimizer formula), it makes more
challenging to ï¬nd the CLMR optimum parameters by proposed solution. It means we need to
train large number of networks in order to determine the optimum values of maxlr,minlr,max;r,
andminmrwhich is not computationally feasible. Also, a heuristic method was suggested in7to
ï¬nd the best value of Clr.
In this paper we propose an alternative way to ï¬nd best cyclic functions with minimum compu-
tational cost. We set ï¬xed values for maxlr,minlr,max;r, andminmrparameters and make sure
that the selected values cover a good range of values for both LR and MR in practice (illustrated in
Figure 3). Then, we did a computationally reasonable heuristic search for ï¬nding the appropriate
amount ofClrandCmrfrom the values shown in Figure 3. Since, changing the values of Clrand
Cmrleads to change in the values of LR and MR in different iterations, there is no need to ï¬nd the
optimum values for minimum and maximum, we did search in 2D space of ClrandCmrto ï¬nd
their optimal values.
15Fig 3 CLR and MLR functions. (a) Learning rate triangle function for different Clrvalues with min lr= 0:0005 and
max lr= 0:05. (b) Momentum rate triangle function for different Cmrvalues with min mr= 0:85andmax mr=
0:95.
164 Experiments and results
4.1 Data
For investigating the performance of proposed method, a dataset from Automatic Cardiac Di-
agnosis Challenge (ACDC-MICCAI Workshop 2017) were used.27These data set includes 150
cine-MR images: 30 normal cases, 30 patients with myocardial infarction, 30 patients with dilated
cardiomyopathy, 30 patients with hypertrophic cardiomyopathy, and the remaining 30 patients
with abnormal RV . While 100 cine-MR images were used for training (80) and validation (20), the
remaining 50 images were used for testing with online evaluation by the challenge organizers. For
a fair validation in training procedures, four subjects from each category have been chosen. The
binary masks for ground truths of three substructures were provided by the challenge organizers
for training and validation while test set was evaluated online (unseen test set. Three substructures
are right ventricle (RV), myocardium of left ventricle(Myo.), and left ventricle (LV) at two time
points of end-systole (ES) and end-diastole (ED).
The MRIs were obtained using two MRI scanners of different magnetic strengths (1.5T and
3.0T). Cine-MR images were acquired with a SSFP sequence in short axis while on breath hold
(and gating). In particular, a series of short axis slices cover the LV from the base to the apex, with
a thickness of 5 mm (or sometimes 8 mm) and sometimes an inter-slice gap of 5 mm. The spatial
resolution goes from 1.37 to 1.68 mm2=pixel and 28 to 40 volumes cover completely or partially
the cardiac cycle.
4.2 Implementation details
The networks were trained for a ï¬xed number of epochs (100) and it was conï¬rmed that they are
fully trained. All the images were resized to 200200in short axis by using B-spline interpolation.
17Then, as a preprocessing step, we applied anisotropic ï¬ltering and histogram matching to the whole
data set. The total number of 2D slices for training was about 1690 and batch size of 10 were
chosen for training. Hence, the number of iteration per epoch is1690
10= 169 and we have a total
number of iteration 100169 = 16;900in training. The Cross Entropy loss function was chosen
for minimization. All the networks were implemented on Tensorï¬‚ow with using NVIDIA TitanXP
GPUs.
4.3 Results
We calculated Dice Index (DI) and also Cross Entropy (CE) loss on validation set for investigating
our proposed optimizer along with other optimizers. In Figures 5a and b, the CE and DI curves
versus iterations for U-Net architecture for different optimizers are illustrated. As these curves
show, the DI in U-Net with ADAM optimizer is increasing rapidly and sharply at the very begin-
ning and then it is almost ï¬xed afterwards. Although our proposed optimizer (CLMR(C lr=20,
Cmr=20)) is not learning as fast as ADAM optimizer at very beginning in U-Net , it gets better
accuracy than ADAM ï¬nally. This phenomenon is clearer in CE curves. The quantitative results on
test set in Table 2 support the same observation and conclusion. Further, the same pattern happens
forDenseNet 2 architecture in Figure 6. This conï¬rms our hypothesis that adaptive optimizers
converges faster but to potentially to different local minimas in comparison with classical SGD
optimizers.
Figure 5 shows the behavior of U-Net architecture with CLMR optimizer performing 2% in-
crease in dice index (in all three substructures as well as average) than its CRL optimizer counter-
part. This proves that having a cyclic momentum rate can yield to better efï¬ciency and accuracy
than having a simple cyclic learning rate. The results on the test set comparing CLR and CLMR
18optimizers in Table 2 support this conclusion too.
Moreover, the curves of DI and CE among different architectures, trained by ADAM andCLMR ,
are demonstrated in Figure 4 a and b. Although the DenseNet 2 has less parameters in comparison
with other architectures, it gets better results than the other architectures. These curves reveals
some other important points about using different architectures: ï¬rst, for all different architec-
tures, the proposed CLMR optimizer works better than ADAM optimizer, indicating the power
of proposed cyclic optimizer. Second, DenseNet architectures are getting better results than U-
Netand Enc Dec architectures, which are highly over-parameterized architectures than DenseNet
and their saturation can be linked to this information too. Third, comparison between curves of
DenseNet 1 and DenseNet 2 shows that having a higher GR (growth rate) in dense connections is
more important than having dense block with high number of parameters. Since, DenseNet 2, with
GR=24 reached better results in comparison with DenseNet 1 with twice of number of parameters
in end of each dense block in comparison to DenseNet 2 and GR=16. These results are supported
by the dice metric obtained from test data and are mentioned in Table 2.
Finally, the DI on test data with online evaluation for different architectures with different
optimizers are summarized in Table 2. In order to have a better comparison, the box plot of all
methods are drawn in Figure 7. As the ï¬gure shows, the dice statistic obtained from CLMR is
better than other optimizers most of the time in addition to its superior efï¬ciency. In addition,
qualitative results for different methods are shown in Figures 8 and 9: the contours for RV , Myo.,
and LV in ED for different methods and architectures and also ground-truth across four slices from
Apex to Base. Usually, segmentation of RV near the Apex is harder than others because RV is
almost vanishing at this point. As a result, some methods may not even detect the RV at slices
near the Apex. Figure 9 shows the contours for RV , Myo., and LV in ES for different methods
19Fig 4 Validation loss and dice index for four different architectures with ADAM andCLMR optimizers. (Upper) Cross
Entropy loss in validation set for four different architectures. (Lower) Dice index in validation set for four different
architectures (zoomed).
and architectures and also ground-truth across four slices from Apex to Base. Since at ES heart
is at minimum volume; thus, it is more difï¬cult to segment substructures. The contours generated
20Fig 5 Validation loss and dice index for DenseNet 2 architecture with different values of ClrandCmr. (Upper) Cross
Entropy loss in validation set for DenseNet 2 architecture. (Lower) Dice index in validation set for U-Net architecture
(zoomed).
with DenseNet 2 method is more similar to ground-truth in both ED and ES, which shows the
generalizability of the proposed method with an efï¬cient architecture choice.
21Fig 6 Validation loss and dice index for U-Net architecture with different values of ClrandCmr. (Upper) Cross
Entropy loss in validation set for DenseNet 2 architecture. (Lower) Dice index in validation set for U-Net architecture
(zoomed).
22Fig 7 Box plots for DI in test data set for RV , Myo., LV and also average of them (Ave).
5 Discussions and Conclusions
We proposed a new cyclic optimization method ( CLMR ) to address the efï¬ciency and accuracy
problems in deep learning based medical image segmentation. We hypothesized that having a
cyclic learning/momentum function can yield better generalization in comparison to adaptive op-
timizers. We showed that CMLR is signiï¬cantly better than adaptive optimizers by considering
momentum changes inside the Nesterov optimizer as a cyclic function. Finding the parameters of
these cyclic functions are complicated due to the correlation existing between LR and MR function.
Thus, we formulated both LR and MR functions and we suggested a method to ï¬nd the parameters
of these cyclic functions with reasonable computational cost.
Our proposed method is just a beginning of a new generation of optimizers which can general-
ize better than adaptive ones. One of the challenges in designing such optimizers is to set up the
23Table 2 DI in the test data set with online evaluation.
Adam Nesterov CLR CLMR
EncDec
RV0.3272 0.1309 0.3833 0.4336
U-Net 0.8574 0.5968 0.8618 0.8820
DenseNet 1 0.8802 0.6936 0.8961 0.8957
DenseNet 2 0.8781 0.7232 0.8910 0.9049
EncDec
Myo0.1473 0.1492 0.1692 0.1686
U-Net 0.8628 0.6486 0.8588 0.8631
DenseNet 1 0.8787 0.7170 0.8834 0.8960
DenseNet 2 0.8796 0.7196 0.8904 0.8999
EncDec
LV0.4950 0.3260 0.4972 0.5418
U-Net 0.9238 0.7670 0.8936 0.9360
DenseNet 1 0.9376 0.8465 0.9351 0.9393
DenseNet 2 0.9196 0.8449 0.9378 0.9478
EncDec
Ave.0.3232 0.1687 0.3499 0.3814
U-Net 0.8813 0.6708 0.8714 0.8937
DenseNet 1 0.8988 0.7524 0.9049 0.9103
DenseNet 2 0.8924 0.7626 0.9064 0.9176
parameters of cyclic functions which need further investigation in a broad sense. One can learn
these parameters with a neural network or reinforcement learning in an efï¬cient manner: i.e., the
maxlr,minlr,max;r,minmr,Clr, andCmrcan be learned by an policy gradient reinforcement
learning approach. In this study, our focus was only on supervised learning methods. However,
proposed method can be generalized to semi-supervised or self-supervised methods as well. This
is outside the scope of the current paper and can be thought of as a follow-up to what we proposed
here.
In our study, our focus was in a particular clinical imaging problem: segmenting cardiac MRI
scans. We assessed the optimization problem with single and multi-object settings. One may
consider different imaging modalities and with different, and perhaps with newer, architectures
to explore the architecture choices versus optimization functions. We believe that, based on our
comparative studies, the architecture choice can affect the segmentation results such that more
complex architectures require optimization algorithms to be selected wisely.
24Fig 8 Qualitative results for ground-truth and different methods for same subject in end-diastole from Apex to Base
for four slices (from right to left). Green, yellow, and brown contours are showing RV , myo, and LV , respectively.
25Fig 9 Qualitative results for ground-truth and different methods for same subject in end-systole from Apex to Base for
four slices (from right to left). Green, yellow, and brown contours are showing RV , myo, and LV , respectively.
26The choice of optimization algorithm can depend on the speciï¬c characteristics of the dataset
and the model being trained, as well as the computational resources available. Therefore, our
results may not be generalizable to every situation in medical image analysis tasks. For instance, if
the medical data is noisy or uncertain, it may be more difï¬cult for the model to accurately predict
the labels. This can make the optimization process more sensitive to the choice of optimization
algorithm and may require the use of regularization techniques to prevent overï¬tting. For another
example, if the dataset is highly imbalanced, with many more examples of one class than the
other, it may be more difï¬cult for the model to accurately predict the minority class. This can
make the optimization process more challenging and may require the use of techniques such as
class weighting or oversampling to improve the performance of the model. Last, but not least, if
the dataset has a large number of features or the features are highly correlated, it may be more
difï¬cult to ï¬nd a good set of weights and biases that accurately model the data. This can make
the optimization process more challenging and may require the use of more advanced optimization
algorithms.
Our study has some other limitations too. The use of second-order optimization methods are in
high demands recently. However, it was not our focus on such methods due to their high burden in
computational cost. Second-order optimization methods, which take into account the curvature of
the loss function, have shown promising results in a variety of deep learning applications. These
methods can be more computationally expensive than ï¬rst-order methods, which only consider the
gradient of the loss function, but may be more effective in certain situations. Further, we focused on
the segmentation problem with traditional deep network architectures while reinforcement learning
and generative models can require development of new algorithms tailored to speciï¬c types of
problem.
275.1 Disclosures
No Conï¬‚ict of Interest.
5.2 Acknowledgments
This project is supported by NIH funding: R01-CA246704, R01-CA240639, U01-DK127384-
02S1, R03-EB032943-02, and R15-EB030356.
5.3 Data, Materials, and Code Availability
Data is available under MICCAI 2017 ACDC challenge.
References
1 D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning Internal Representations by
Error Propagation , 318â€“362. MIT Press, Cambridge, MA, USA (1986).
2 D. P. Kingma and J. Ba, â€œAdam: A method for stochastic optimization,â€ arXiv preprint
arXiv:1412.6980 (2014).
3 J. Duchi, E. Hazan, and Y . Singer, â€œAdaptive subgradient methods for online learning and
stochastic optimization,â€ Journal of Machine Learning Research 12(Jul), 2121â€“2159 (2011).
4 G. Hinton, N. Srivastava, and K. Swersky, â€œNeural networks for machine learning lecture 6a
overview of mini-batch gradient descent,â€ Cited on 14(8), 2 (2012).
5 N. Qian, â€œOn the momentum term in gradient descent learning algorithms,â€ Neural networks
12(1), 145â€“151 (1999).
6 A. C. Wilson, R. Roelofs, M. Stern, et al. , â€œThe marginal value of adaptive gradient methods
in machine learning,â€ in Advances in Neural Information Processing Systems , 4151â€“4161
(2017).
287 L. N. Smith, â€œCyclical learning rates for training neural networks,â€ in Applications of Com-
puter Vision (WACV), 2017 IEEE Winter Conference on , 464â€“472, IEEE (2017).
8 J. Zhang and I. Mitliagkas, â€œYellowï¬n and the art of momentum tuning,â€ arXiv preprint
arXiv:1706.03471 (2017).
9 Y . Nesterov, â€œA method for unconstrained convex minimization problem with the rate of
convergence o (1/kË† 2),â€ in Doklady AN USSR ,269, 543â€“547 (1983).
10 â€œCardiovascular Diseases (cvds).â€ http://www.who.int/mediacentre/
factsheets/fs317/en/ (2007). [Online; accessed 30-June-2017].
11 C. Xu, L. Xu, Z. Gao, et al. , â€œDirect delineation of myocardial infarction without contrast
agents using a joint motion feature learning architecture,â€ Medical image analysis 50, 82â€“94
(2018).
12 A. Mortazi, R. Karim, K. Rhode, et al. , â€œCardiacnet: Segmentation of left atrium and proxi-
mal pulmonary veins from mri using multi-view cnn,â€ in International Conference on Medi-
cal Image Computing and Computer-Assisted Intervention , 377â€“385, Springer (2017).
13 A. Mortazi, N. Khosravan, D. A. Torigian, et al. , â€œWeakly supervised segmentation by a
deep geodesic prior,â€ in International Workshop on Machine Learning in Medical Imaging ,
238â€“246, Springer (2019).
14 O. Oktay, E. Ferrante, K. Kamnitsas, et al. , â€œAnatomically constrained neural networks (ac-
nns): application to cardiac image enhancement and segmentation,â€ IEEE transactions on
medical imaging 37(2), 384â€“395 (2017).
15 P. Bizopoulos and D. Koutsouris, â€œDeep learning in cardiology,â€ IEEE reviews in biomedical
engineering 12, 168â€“193 (2018).
2916 X. Zhuang, L. Li, C. Payer, et al. , â€œEvaluation of algorithms for multi-modality whole heart
segmentation: An open-access grand challenge,â€ arXiv preprint arXiv:1902.07880 (2019).
17 A. Srivastava, D. Jha, B. Aydogan, et al. , â€œMulti-scale fusion methodologies for head and
neck tumor segmentation,â€ arXiv preprint arXiv:2210.16704 (2022).
18 N. K. Tomar, D. Jha, and U. Bagci, â€œDilatedsegnet: A deep dilated segmentation network for
polyp segmentation,â€ arXiv preprint arXiv:2210.13595 (2022).
19 A. Srivastava, D. Jha, E. Keles, et al. , â€œAn efï¬cient multi-scale fusion network for 3d organ
at risk (oar) segmentation,â€ arXiv preprint arXiv:2208.07417 (2022).
20 Z. Zhang and U. Bagci, â€œDynamic linear transformer for 3d biomedical image segmentation,â€
arXiv preprint arXiv:2206.00771 (2022).
21 U. Demir, Z. Zhang, B. Wang, et al. , â€œTransformer based generative adversarial network for
liver segmentation,â€ arXiv preprint arXiv:2205.10663 (2022).
22 V . Badrinarayanan, A. Kendall, and R. Cipolla, â€œSegnet: A deep convolutional encoder-
decoder architecture for image segmentation,â€ arXiv preprint arXiv:1511.00561 (2015).
23 K. He, X. Zhang, S. Ren, et al. , â€œDeep residual learning for image recognition,â€ in Proceed-
ings of the IEEE conference on computer vision and pattern recognition , 770â€“778 (2016).
24 O. Ronneberger, P. Fischer, and T. Brox, â€œU-net: Convolutional networks for biomedical im-
age segmentation,â€ in International Conference on Medical Image Computing and Computer-
Assisted Intervention , 234â€“241, Springer (2015).
25 G. Huang, Z. Liu, K. Q. Weinberger, et al. , â€œDensely connected convolutional networks,â€
arXiv preprint arXiv:1608.06993 (2016).
3026 S. J Â´egou, M. Drozdzal, D. Vazquez, et al. , â€œThe one hundred layers tiramisu: Fully convo-
lutional densenets for semantic segmentation,â€ in Computer Vision and Pattern Recognition
Workshops (CVPRW), 2017 IEEE Conference on , 1175â€“1183, IEEE (2017).
27 O. Bernard, A. Lalande, C. Zotti, et al. , â€œDeep learning techniques for automatic mri cardiac
multi-structures segmentation and diagnosis: Is the problem solved?,â€ IEEE transactions on
medical imaging 37(11), 2514â€“2525 (2018).
Biographies and photographs of the other authors are not available.
List of Figures
1 CNN Architecture is used for pixel-wise segmentation. The architecture with CNN
blocks without red skip connections is Encoder-Decoder architecture. The archi-
tecture with red skip connection (Fig. 2a) is called U-Net , if connections are with
Dense block (Fig. 2b), it is called Tiramisu (DenseNet for segmentation)
2 (a) CNN block used in Enc-Dec and U-Net architectures, (b) Dense block used in
Tiramisu architecture.
3 CLR and MLR functions. (a) Learning rate triangle function for different Clrval-
ues withminlr= 0:0005 andmaxlr= 0:05. (b) Momentum rate triangle function
for different Cmrvalues with minmr= 0:85andmaxmr= 0:95.
4 Validation loss and dice index for four different architectures with ADAM and
CLMR optimizers. (Upper) Cross Entropy loss in validation set for four different
architectures. (Lower) Dice index in validation set for four different architectures
(zoomed).
315 Validation loss and dice index for DenseNet 2 architecture with different values of
ClrandCmr. (Upper) Cross Entropy loss in validation set for DenseNet 2 archi-
tecture. (Lower) Dice index in validation set for U-Net architecture (zoomed).
6 Validation loss and dice index for U-Net architecture with different values of Clr
andCmr. (Upper) Cross Entropy loss in validation set for DenseNet 2 architecture.
(Lower) Dice index in validation set for U-Net architecture (zoomed).
7 Box plots for DI in test data set for RV , Myo., LV and also average of them (Ave).
8 Qualitative results for ground-truth and different methods for same subject in end-
diastole from Apex to Base for four slices (from right to left). Green, yellow, and
brown contours are showing RV , myo, and LV , respectively.
9 Qualitative results for ground-truth and different methods for same subject in end-
systole from Apex to Base for four slices (from right to left). Green, yellow, and
brown contours are showing RV , myo, and LV , respectively.
List of Tables
1 Number of layers in each block of different architectures and number of parame-
ters.
2 DI in the test data set with online evaluation.
32