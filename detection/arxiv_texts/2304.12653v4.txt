Received: Added at production Revised: Added at production Accepted: Added at production
DOI: xxx/xxxx
ARTICLE TYPE
Partially Observable Mean Field Multi-Agent Reinforcement
Learning Based on Graph–Attention
Min Yang | Guanjun Liu* | Ziyuan Zhou
1Department of Computer Science, Tongji
University, Shanghai, China Summary
Traditional multi-agent reinforcement learning algorithms are difficultly applied in
a large-scale multi-agent environment. The introduction of mean field theory has
enhanced the scalability of multi-agent reinforcement learning in recent years. This
paper considers partially observable multi-agent reinforcement learning (MARL),
where each agent can only observe other agents within a fixed range. This partial
observability affects the agent’s ability to assess the quality of the actions of sur-
roundingagents.Thispaperfocusesondevelopingamethodtocapturemoreeffective
information from local observations in order to select more effective actions. Pre-
vious work in this field employs probability distributions or weighted mean field
to update the average actions of neighborhood agents, but it does not fully con-
siderthefeatureinformationofsurroundingneighborsandleadstoalocaloptimum.
In this paper, we propose a novel multi-agent reinforcement learning algorithm,
Partially Observable Mean Field Multi-Agent Reinforcement Learning based on
Graph–Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention
module and a mean field module to describe how an agent is influenced by the
actions of other agents at each time step. This graph attention module consists of a
graph attention encoder and a differentiable attention mechanism, and this mecha-
nismoutputsadynamicgraphtorepresenttheeffectivenessofneighborhoodagents
against central agents. The mean–field module approximates the effect of a neigh-
borhood agent on a central agent as the average effect of effective neighborhood
agents.WeevaluateGAMFQonthreechallengingtasksintheMAgentsframework.
ExperimentsshowthatGAMFQoutperformsbaselinesincludingthestate-of-the-art
partiallyobservablemean-fieldreinforcementlearningalgorithms.Thecodeforthis
paper is here https://github.com/yangmin32/GPMF.
KEYWORDS:
Graph–Attention, Multi-agent reinforcement learning, Mean field theory, Partial observation
1INTRODUCTION
Reinforcement learning has been widely used in video games [26] and recently in education [7]. For multi-agent reinforce-
ment learning (MARL) [33], it involves multiple autonomous agents that make autonomous decisions to accomplish some
specificcompetitiveorcooperativetasksbymaximizingglobalreward,ithasbeenappliedinsomereal-worldscenariossuchasarXiv:2304.12653v4  [cs.AI]  9 Sep 20242
autonomous mobile [21] drone swarm confrontation[1] and multi-UAV collaboratively delivering goods [22]. For example, in
some of the drone swarm adversarial tasks, drones need to make actions based on autonomous decisions. Due to the inevitable
death of some drones in the confrontation environment [38], the surviving drones must constantly evolve their strategies in
real-timeduringtheinteractionwiththeenvironmenttoobtaintheoverallmaximumreward.Inordertomakebetterinteraction
among agents, it is required that each agent in the multi-agent system can effectively perceive environmental information and
fully acquire the information of surrounding agents.
However,theglobalcommunicationcostamongmultipleagentsishigh,andinmanypracticaltasks,eachagentonlyobserves
part of the environmental information. Take the task of Autonomous Driving as an example, each vehicle makes decision
in the limited observation space which is a typical local observation scene. Each agent can only rely on limited observation
information in the local observation environment, therefore the agent needs to learn a decentralized strategy. There are two
common decentralization strategies. One is Centralized Training and Decentralized Execution (CTDE), which requires agents
to communicate with each other during training and to independently make decisions based on their own observations during
testing in order to adapt to large-scale multi-agent environments. Some classic algorithms using the CTDE framework such
as MADDPG [15], QMIX [19] and MAVEN [16]. Another one takes the policy of decentralized training and decentralized
execution, in which each agent can only observe part of the information during the training and testing phases, which is closer
to the real environment with limited communication. Especially large-scale multi-agent environments are complex and non-
stationary [10], it is difficult for agents to observe the entire environment globally, limiting theirability to find the best actions.
Furthermore, as the number of agents increases, joint optimization of all information in a multi-agent environment may result
in a huge joint state-action space, which also brings scalability challenges. This paper focuses on the second strategy.
Traditionalmulti-agentreinforcementlearningalgorithmsaredifficulttobeappliedinlarge-scalemulti-agentenvironments,
especiallywhenthenumberofagentsisexponential.Recentstudiesaddressthescalabilityissuesofmulti-agentreinforcement
learning [31, 30, 12] by introducing mean-field theory, i.e., the multi-agent problem is reduced to a simple two-agent problem.
However,Yangetal.[31]assumesthateachagentcanobserveglobalinformation,whichisdifficulttoapplyinsomerealtasks.
Therefore, it is necessary to study large-scale multi-agent reinforcement learning algorithms in partially observable cases [3].
In addition, researchers have intensively studied mean-field-based multi-agent reinforcement learning algorithms to improve
performance in partially observable cases. One way is to further decompose the Q-function of the mean field-based multi-
agent reinforcementlearning algorithm[34, 6].Another wayuses probabilitydistribution or weightedmean fieldto updatethe
mean action of neighborhood agents [5, 37, 23, 28]. Hao [8] combined the graph attention with the mean field to calculate the
interaction strength between agents when agents interact, but only considered the scene where the agent has a fixed relative
position, and the agents can observe the global information. The difference is that when the agent is partially observable, we
consider the dynamic change of the agent’s position and the death scene of the agent, and construct a more flexible partial
observable graph attention network based on the mean field.
However, for partially observable multi-agent mean field reinforcement learning, the existing methods do not fully consider
the feature information of the surrounding neighbors, which will lead to falling into local optimum. This paper focuses on
identifying the neighborhood agents that may have the greater influence on the central agent in a limited observation space,
in order to avoid the local optimum issue. Since the graph neural network [29] can fully aggregate the relationship between
the central agent and its surrounding neighbors, we propose a graph attention-based mechanism to calculate the importance of
neighbor agents to estimate the average action more efficiently.
The main contributions of this paper are as follows:
•Weproposeapartiallyobservablemean–fieldreinforcementlearningbasedonthegraph–attention(GAMFQ),whichcan
learn a decentralized agent policy from an environment without requiring global information of an environment. In the
case of partially observable large-scale agents, the judgment of the importance of neighbor agents is insufficient in our
GAMFQ.
•We theoretically demonstrate that the settings of the GAMFQ algorithm are close to Nash equilibrium.
•Experiments on three challenging tasks in the MAgents framework show that GAMFQ outperforms two baseline
algorithms as well as the state-of-the-art partially observable mean-field reinforcement learning algorithms.3
2RELATED WORK
MostoftherecentMARLalgorithmsforpartialobservabilityresearcharemodel-freereinforcementlearningalgorithmsbased
on the CTDE framework. The most classic algorithm MADDPG [15] introduces critics that can observe global information
in training to guide actor training, but only use actors with local observation information to take actions in testing. QMIX[19]
usesahybridnetworktocombinethelocalvaluefunctionsofasingleagent,andaddsglobalstateinformationassistanceinthe
training and learning process to improve the performance of the algorithm. MAVEN [16] is able to solve complex multi-agent
tasksbyintroducinglatentspacesforhierarchicalcontrolbyvalue-mixingandpolicy-basedapproaches.However,thesemulti-
agentreinforcementlearningalgorithmusingtheCTDEframeworkisdifficulttoscaletolarge-scalemulti-agentenvironments,
because there will be hard-to-observe global information that prevents the agents from training better policies.
For large-scale multi-agent environments, Yang et al. [31] introduced the mean–field theory, which approximates the inter-
action of many agents as the interaction between the central agent and the average effects from neighboring agents. However,
partially observed multi-agent mean–field reinforcement learning algorithms still have a space to improve. Some researchers
further decompose the Q-function of the mean field based multi-agent reinforcement learning algorithm. Zhang et al. [34]
trained agents through the CTDE paradigm, transforming each agent’s Q-function into its local Q-function and its mean field
Q-function, but this approach is not strictly partially observable. Gu et al. [6] proposes a mean field multi-agent reinforcement
learning algorithm with local training and decentralized execution. The Q-function is decomposed by grouping the observable
neighborstatesofeachagentinamulti-agentsystem,sothattheQ-functioncanbeupdatedlocally.Inaddition,someresearchers
have focused on improving the mean action in mean field reinforcement learning. Fang et al. [5] adds the idea of mean field to
MADDPG,andproposesamulti-agentreinforcementlearningalgorithmbasedonweightedmeanfield,sothatMADDPGcan
adapttolarge-scalemulti-agentenvironment.Wangetal.[28]proposeaweightedmean-fieldmulti-agentreinforcementlearn-
ing algorithm based on reward attribution decomposition by approximating the weighted mean field as a joint optimization of
implicitrewarddistributionbetweenacentralagentanditsneighbors.Zhouetal.[37]usestheaverageactionofneighboragents
asalabel,andtrainedameanfieldpredictionnetworktoreplacetheaverageaction.Subramanianetal.[23]proposedtwomulti-
agentmeanfieldreinforcementlearningalgorithmsbasedonpartiallyobservablesettings:POMFQ(FOR)andPOMFQ(PDO),
extracting partial samples from Dirichlet or Gamma distribution to estimate partial observable mean action. Although these
methods achieve good results, they do not fully consider the feature information of surrounding neighbors.
GraphNeuralNetworks(GNNs)areabletominegraphstructuresfromdataforlearning.Inmulti-agentreinforcementlearn-
ing, GNNs can be used to model interactions between agents. In recent work, graph attention mechanisms have been used for
multi-agentreinforcementlearning.Zhangetal.[32]integratedtheimportanceoftheinformationofsurroundingagentsbased
onthemulti-headattentionmechanism,effectivelyintegratethekeyinformationofthegraphtorepresenttheenvironmentand
improvethecooperationstrategyofagentswiththehelpofmulti-agentreinforcementlearning.DCG[2]decomposedthejoint
value function of all agents into gains between pairs of agents according to the coordination graph, which can flexibly balance
the performance and generalization ability of agents. Li et al. [13] proposed a deep implicit coordination graph (DICG) struc-
ture that can adapt to dynamic environments and learn implicit reasoning about joint actions or values through graph neural
networks.Ruanetal.[20]proposedagraph-basedcoordinationstrategy,whichdecomposesthejointteamstrategyintoagraph
generatorandagraph-basedcoordinationstrategytorealizethecoordinationbehaviorbetweenagents.MAGIC[17]moreaccu-
rately represented the interactions between agents during communication by modifying the standard graph attention network
and compatible with differentiable directed graphs.
In the dynamic MARL system where competition and confrontation coexist, it is very difficult to directly apply the graph
neural network, because the agent will die, the graph structure of the constructed large-scale agent system has the problem of
largespatialdimension.However,graphneuralnetworkscanbetterminetherelationshipbetweenfeatures,andtheintroduction
of mean-field theory can further improve the advantages of mean-field multi-agent reinforcement learning.
Our approach differs from related work above in that it uses a graph attention mechanism to select surrounding agents that
are more important to the central agent in a partially observable environment. GAMFQ uses a graph attention module and
a mean field module to describe how an agent is influenced by the actions of other agents at each time step, where graph
attentionconsistsofagraphattentionencoderandadifferentiableattentionmechanism,andfinallyoutputsadynamicgraphto
representtheeffectivenessoftheneighborhoodagenttothecentralagent.Themeanfieldmoduleapproximatestheinfluenceof
aneighborhoodagentonacentralagentastheaverageinfluenceoftheeffectiveneighborhoodagents.Usingthesetwomodules
togetherisabletoefficientlyestimatetheaverageactionofsurroundingagentsinpartiallyobservablesituations.GAMFQdoes
not require global information about the environment to learn decentralized agent policies from the environment.4
3MOTIVATION & PRELIMINARIES
In this section, we represent discrete-time non-cooperative multi-agent task modeling as a stochastic game (SG). SG can be
defined as a tuple < 𝑆,𝐴1,…,𝐴𝑁,𝑟1,…,𝑟𝑁,𝑝,𝛾 >, where𝑆represents the true state of the environment. Each agent 𝑗∈
{1,…,𝑁}chooses an action at each time step 𝑎𝑗∈𝐴𝑗. The reward function for agent 𝑗is𝑟𝑗∶𝑆×𝐴1×⋯×𝐴𝑁→𝑅.
State transitions are dynamically represented as 𝑝∶𝑆×𝐴1×⋯×𝐴𝑁→Ω(𝑆).𝛾is a constant representing the discount
factor. It represents a stable state, and in this stable state, all agents will not deviate from the best strategy given to others. The
disadvantageisthatitcannotbeappliedtothecoexistenceofmultipleagents.Yangetal.[31]introducedmeanfieldtheory,which
approximates the interaction of many agents as the interaction between the average effect of a central agent and neighboring
agents, and solves the scalability problem of SG.
The Nash equilibrium of general and random games can be defined as a strategy tuple(𝜋1
∗,⋯,𝜋𝑁
∗), for all𝑠∈𝑆and
∀𝜋𝑖∈Π𝑖,thereis𝑣𝑗(𝑠,𝜋1
∗,⋯,𝜋𝑖
∗,⋯,𝜋𝑁
∗)≥𝑣𝑗(𝑠,𝜋1
∗,⋯,𝜋𝑖,⋯,𝜋𝑁
∗).Thisshowsthatwhenallotheragentsareimplementing
theirequilibriumstrategy,nooneagentwilldeviatefromthisequilibriumstrategyandreceiveastrictlyhigherreward.Whenall
agentsfollowtheNashequilibriumstrategy,theNashQ-functionofagent 𝑗is𝑄𝑗
∗(𝑠,𝑎).Partiallyobservablestochasticgamescan
generate a partially observable Markov decision process (POMDP), we review the partially observable Markov decision (Dec-
POMDP) in Section 3.1 and analyze the partially observable model from a theoretical perspective. Section 3.2 first introduces
thegloballyobservablemean-fieldmulti-agentreinforcement learning,andthenintroducesthepartiallyobservablemean-field
reinforcementlearningalgorithm(POMFQ)basedonthePOMDPframework,andanalyzestheexistingpartoftheobservable
indetail.Thelimitationofmean-fieldreinforcementlearningPOMFQ(FOR)[23]isthatthefeatureinformationofsurrounding
neighbors is not fully considered. In a partially observable setting, each agent 𝑗observable neighborhood agent information
𝑜𝑗can be used to better mine the relationship between features through a graph attention network. Introducing graph attention
networks into partially observable mean-field multi-agent reinforcement learning can further improve their performance, and
Section 3.3 briefly introduces graph attention networks.
3.1Partially observable Markov decision process
We mainly study partially observable Markov decisions (Dec-POMDP) [3, 18, 33]. The partially observable Markov decision
process of𝑛agents can be represented as a tuple⟨𝑁,𝑆,{𝐴𝑖}𝑛
𝑖=1,𝑇,𝑍,𝑅,𝑂,𝛾⟩, where𝑁= {1,…,𝑛}represents the set of
agents,𝑆representstheglobalstate, 𝐴𝑗representsthesetofactionspacesofthe 𝑗-thagent,𝑍representstheobservationspace
of the agents, and the agent 𝑗receives observation 𝑜𝑗∈𝑂𝑗through the observation function 𝑍(𝑠,𝑗) ∶𝑆×𝑁→𝑂, and the
transition function 𝑇∶𝑆×𝐴1×…×𝐴𝑛×𝑆→[0,1]represents the environment transitions from a state to another one. At
each time step 𝑡, the agent𝑗chooses an action 𝑎𝑗
𝑡∈𝐴𝑗, gets a reward 𝑟𝑗
𝑡∶𝑆×𝐴𝑗→𝑅w.r.t. a state and an action. 𝛾∈[0,1]
is a reward discount factor. Agent 𝑗has a stochastic policy 𝜋𝑗conditioned on its observation 𝑜𝑗or action observation history
𝜏𝑗∈(𝑍×𝐴𝑗), and according to the all agents’s joint policy 𝜋Δ=[𝜋1,…,𝜋𝑁], The value function of agent 𝑗under the joint
strategy𝜋is the value function 𝑣𝑗
𝜋(𝑠)=∑∞
𝑡=0𝛾𝑡𝐸𝜋,𝑝[𝑟𝑗
𝑡|𝑠0=𝑠]can be obtained, and then the Q-function can be formalized as
𝑄𝑗
𝜋(𝑠,𝑎)=𝑟𝑗(𝑠,𝑎)+𝛾𝐸𝑠′∼𝑝[𝑣𝑗
𝜋(𝑠′)]. Our work is based on the POMDP framework.
3.2Partially Observable Mean Field Reinforcement Learning
Mean-field theory-based reinforcement learning algorithm [31] approximates interactions among multiple agents as two-agent
interactions, where the second agent corresponds to the average effect of all other agents. Yang et al. [31] decomposes the
multi-agent Q-function into pairwise interacting local Q-functions as follows:
𝑄𝑗
𝜋(𝑠,𝑎)=1
𝑁𝑗∑
𝑘∈𝑁(𝑗)𝑄𝑗
𝜋(𝑠,𝑎𝑗,𝑎𝑘)(1)
where𝑁𝑗is the index set of the neighbors of the agent 𝑗and𝑎𝑗represents the discrete action of the agent 𝑗and is represented
by one-shot coding. Mean field Q-function is cyclically updated according to Eq.2-5:
𝑄𝑗
𝜋(𝑠𝑡,𝑎𝑗
𝑡, ̄ 𝑎𝑗
𝑡)=(1−𝛼)𝑄𝑗
𝜋(𝑠𝑡,𝑎𝑗
𝑡, ̄ 𝑎𝑗
𝑡)+𝛼[𝑟𝑗
𝑡+𝛾𝑣𝑗(𝑠𝑡+1)](2)5
where
𝑣𝑗(𝑠𝑡+1)=∑
𝑎𝑗
𝑡+1𝜋𝑗(𝑎𝑗
𝑡+1∣𝑠𝑡+1,̃ 𝑎𝑗
𝑡)𝑄𝑗
𝜋(𝑠𝑡+1,𝑎𝑗
𝑡+1,̃ 𝑎𝑗
𝑡)(3)
̄ 𝑎𝑗
𝑡=1
𝑁∑
𝑘≠𝑗𝑎𝑘
𝑡,𝑎𝑘
𝑡∼𝜋𝑘(⋅∣𝑠𝑡, ̄ 𝑎𝑘
𝑡−1)(4)
𝜋𝑗(𝑎𝑗
𝑡∣𝑠𝑡, ̄ 𝑎𝑗
𝑡−1)=exp(−𝛽𝑄𝑗
𝜋(𝑠𝑡,𝑎𝑗
𝑡, ̄ 𝑎𝑗
𝑡−1))
∑
𝑎𝑗′
𝑡∈𝐴𝑗exp(
−𝛽𝑄𝑗
𝜋(
𝑠𝑡,𝑎𝑗′
𝑡, ̄ 𝑎𝑗
𝑡−1)) (5)
wherē 𝑎𝑗
𝑡isthemeanactionoftheneighborhoodagent, 𝑟𝑗
𝑡istherewardforagent 𝑗attimestep𝑡,𝑣𝑗isthevaluefunctionofagent
𝑗, and𝛽is the Boltzmann parameter. Literature [31] assumes that each agent has global information, and for the central agent,
the average action of the neighboring agents is updated by Eq. 4. However, in a partially observable multi-agent environment,
the way of calculating the average action in Eq. 4 is no longer applicable.
Inthecaseofpartialobservability,Subramanianetal.[23]take 𝑈samplesfromtheDirichletdistributiontoupdatetheaverage
actionofEq.4,andachievebetterperformancethanthemeanfieldreinforcementlearningalgorithm.Theformulaisasfollows:
𝐷𝑗(𝜃)∝𝜃𝜂1−1+𝑐1
1⋯𝜃𝜂𝐿−1+𝑐𝐿
𝐿;
̃ 𝑎𝑗
𝑖,𝑡∼𝐷𝑗(𝜃;𝜂+𝑐);̃ 𝑎𝑗
𝑡=1
𝑈𝑖=𝑈∑
𝑖=1̃ 𝑎𝑗
𝑖,𝑡(6)
where𝐿denotes the size of the action space, 𝑐1,…,𝑐𝐿denotes the number of occurrences of each action, 𝜂is the Dirichlet
parameter,𝜃istheclassificationdistribution.ButthepremiseoftheDirichletdistributionistoassumethatthecharacteristicsof
eachagentareindependenttoachievebetterclusteringbasedonthecharacteristicsofneighboringagents.Infact,inmanymulti-
agent environments, the characteristics of each agent has a certain correlation, but the Dirichlet distribution does not consider
thiscorrelation,whichmakesitunabletoaccuratelydescribethecentralagentandtheneighborhoodagents.Therewillbesome
deviations in the related information. Figure 1 shows the process of a battle between the red and green teams, in which each
agentcanobservetheinformationofthefriendlyagent,andtheactionspaceoftheagentis {𝑢𝑝,𝑑𝑜𝑤𝑛,𝑙𝑒𝑓𝑡,𝑟𝑖𝑔ℎ𝑡 }.Thecentral
agentenclosedbytheredcircleisaffectedbythesurroundingfriendlyagents.WeusetheDirichletdistributiontosimulateand
calculate the probability of the central agent moving in each direction, as shown below:
⎧
⎪
⎪
⎨
⎪
⎪⎩𝑝𝑢𝑝=0.31
𝑝𝑑𝑜𝑤𝑛=0.42
𝑝𝑙𝑒𝑓𝑡=0.14
𝑝𝑟𝑖𝑔ℎ𝑡=0.13(7)
It can be obtained that the probability of the agent moving down is the highest, which is essentially due to the large number
ofagentsmoving 𝑑𝑜𝑤𝑛.However,moving 𝑢𝑝istheoptimalactionfortheagenttoformanencirclementtrendwithfriends.The
Dirichlet distribution results in a local optimal solution rather than finding the optimal action.
Zhangetal.[35]believesthatthecorrelationbetweentwoagentsiscrucialformulti-agentreinforcementlearning.First,the
papercalculatesthecorrelationcoefficientbetweeneachpairofagents,andthenshieldsthecommunicationamongweaklycor-
related agents, thereby reducing the dimensionality of the state-action value network in the input space. Inspired by Zhang et
al. [35], for large-scale partially observable multi-agent environments, it is more necessary to select the importance of neigh-
borhoodagents.Inourpaper,wewilladoptagraphattentionmethodtofilteroutmoreimportantneighborhoodagents,discard
unimportant agent information, and achieve more accurate estimation of the average actions of neighborhood agents.
3.3Graph Attention Network
Graph neural network [29] can better mine the graph structure form between data. Graph Attention Network (GAT) [25] is
composed of a group of graph attention layers, each graph attention layer acts on the node feature vector of node 𝑖denoting as
𝑚𝑖through a weight matrix 𝑊, and then uses softmax to normalize the neighbor nodes of the central node:
𝑒𝑖𝑗=(𝑊𝑚𝑖‖𝑊𝑚𝑗) (8)6
left
0.420.31
0.130.14up
rightdown
FIGURE 1 A battle environment of the red and blue groups, where the red agent in the center is distributed by Dirichlet to
calculate the action.
𝛼𝑖𝑗=softmax𝑗(𝑒𝑖𝑗)=exp(𝑒𝑖𝑗)
∑
𝑘∈𝑁𝑗exp(𝑒𝑗𝑘) (9)
where𝑒𝑖𝑗istheattentioncoefficientofeachnode,indicatingtheimportanceofnode 𝑖tonode𝑗.Finally,theoutputfeaturesare
obtained by weighting the input features ℎ𝑖, and the update rule for each node 𝑗is:
𝑒𝑗=𝜎⎛
⎜
⎜⎝∑
𝑖∈𝑁𝑗𝛼𝑖𝑗𝑊ℎ𝑖⎞
⎟
⎟⎠(10)
where𝑒𝑗represents the feature of node 𝑗,𝑁𝑗is the set of adjacent nodes of node 𝑗, and𝜎(⋅)is a nonlinear activation function.
4APPROACH
Inthissection,weproposeanovelmethodcalledPartiallyObservableMeanFieldMulti-AgentReinforcementLearningbased
on Graph–Attention (GAMFQ), which can be applied to large-scale partially observable MARL tasks, where the observation
range of each agent is limited, and the feature information of other agents in the fixed neighborhood is intelligently observed.
The overall architecture of the GAMFQ algorithm is depicted in Figure 2, including two important components: the Graph
Attention Module and the Mean Field Module: (i) In our Graph–Attention Module, the information observed locally by each
agentissplicedfirstly.Thenthehigh-dimensionalfeaturerepresentationsareobtainedbyalatentspacemappingprocesswhich
followedbyaone-layerLSTMnetworktoobtainthetime-seriescorrelationofthetargetagent,andthehiddenlayeroftheLSTM
isusedastheinputofthegraphattentionmoduletoinitializetheconstructedgraphnodes.Thentoenhancetheaggregationof
neighboragentstotargetagent,asimilarprocessisimplementedasaFCmappingnetworkfollowedbyaGATlayer.Afterthat,
the final representation of agents are obtained by a MLP layer with the input of the representations of target agent and other
observableagents.Finally,weadoptlayer-normalizedmethodtoobtaintheadjacencymatrix{𝐺𝑡}𝑁
1viaGumbelSoftmax.(ii)
The Mean Field Module utilizes the adjacency matrix{𝐺𝑡}𝑁
1from Graph Attention Module to obtain adopting action from
important neighbor agents, in which the joint Q-function of each agent 𝑗approximates the Mean-Field Q-function 𝑄𝑗(𝑠,𝑎) ≈
𝑄𝑗
POMF(𝑠,𝑎𝑗,̃ 𝑎𝑗)of important neighbor agents, where the Q-value is partially observable mean-field(POMF) Q-value, and ̃ 𝑎𝑗
istheaverageactionoftheimportantneighborhoodagentsthatispartiallyobservablebyagent 𝑗.Eachcomponentisdescribed
in detail below.7
Graph -Attention
ModuleFeature Vector
Effective Feature VectorAgent1
Agent𝑗
Agent𝑁
……
…
…Get  𝑴Effective neighbors
𝐺𝑡FC
LSTM
FCGumbel SoftmaxMLP GAT Encoder
… …𝑒1𝑡
𝑒𝑖𝑡
𝑒𝑁𝑗𝑡 …
𝐸𝑡
…
Agent1
Agent𝑗
Agent𝑁
…𝑥1𝑡𝑥𝑁𝑗𝑡𝑁𝑗neighbors
… …
… …
… …Local Observable
Get  Mean  action
Mean Field Module
…
𝑥𝑖𝑡
FIGURE 2 Schematic of GAMFQ. Each agent can observe the feature information of other agents within a fixed range, input
itintotheGraph–AttentionModule,andoutputanadjacencymatrixtorepresenttheeffectivenessoftheneighborhoodagentto
the central agent.
4.1Graph–Attention Module
To more accurately re-determine the influence of agent 𝑗’s neighbor𝑁𝑗on itself, we need to be able to extract useful infor-
mation from the local observations of agent 𝑗. The local observations of each agent include the embedding information of
neighboring agents. For each agent 𝑗and each time step 𝑡, the information of a local observation of length 𝐿𝑗, is expressed as
𝑜𝑡
𝑗=(
𝑥𝑡
1,𝑥𝑡
2,⋯,𝑥𝑡
𝑁𝑗)
, where𝑥𝑡
𝑁𝑗represents the feature of the 𝑁𝑗-th neighbor agent of agent 𝑗, and𝑜𝑡
𝑗∈𝑅𝑁𝑗×𝐷,𝑥𝑡
𝑖∈𝑅1×𝐷.
𝐿𝑗is concatenated from the embedding features of each neighbor. Our goal is to learn an adjacency matrix{𝐺𝑡}𝑁
1to extract
moreimportantembeddinginformationfortheagent 𝑗fromlocalobservationsateachtimestep 𝑡.Sincegraphneuralnetworks
can better mine the information of neighbor nodes, we propose a graph attention structure suitable for large-scale multi-agent
systems. This structure focuses on information from different agents by associating weights to observations based on the rel-
ative importance of other agents in their local observations. The Graph–Attention structure is constructed by concatenating a
graphattentionencoderandadifferentiableattentionmechanism.Forthelocalobservation 𝑜𝑡
𝑗ofagent𝑗attimestep𝑡,𝑜𝑡′
𝑗isfirst
encoded using a fully connected layer (FC) , and is passed to the LSTM layerin order to generate the hidden state ℎ𝑡
𝑗and cell
state𝑐𝑡
𝑗of agent𝑗, whereℎ𝑡
𝑗serves as the input of the graph attention module to initialize the constructed graph nodes:
ℎ𝑡
𝑗,𝑐𝑡
𝑗=𝐿𝑆𝑇𝑀(
𝑒(
𝑜𝑡
𝑗)
,ℎ𝑡
𝑗,𝑐𝑡
𝑗)
(11)
where𝑒(⋅)is a fully connected layer representing the observed encoder. ℎ𝑡
𝑗is encoded as a message:
𝑚𝑡
𝑗=𝑒(
ℎ𝑡
𝑗)
(12)
where𝑚𝑡
𝑗is the aggregated information of the neighborhood agents observed by agent 𝑗at time step 𝑡. The input encoding
information 𝑀𝑡is passed to the GAT encoder and hard attention mechanism, where the hard attention mechanism consists
of MLP and Gumbel Softmax function. Finally, the output adjacency matrix{𝐺𝑡}𝑁
1is used to determine which agents in the
neighborhoodhaveaninfluenceonthecurrentagent.TheGATencoderhelpstoefficientlyencodetheagent’slocalinformation,
which is expressed as:
{𝑀𝑡}𝑁
1=𝑓𝑆𝑐ℎ𝑒𝑑(𝑚𝑡
1,⋯,𝑚𝑡
𝑁)(13)
Additionally, we take the form of the same attention mechanism as GAT [25], expressed as:
𝛼𝑆
𝑖𝑗=exp(
𝐿𝑒𝑎𝑘𝑦Re𝐿𝑈(
𝑎𝑇
𝑆[
𝑊𝑆𝑚𝑡
𝑖||𝑊𝑆𝑚𝑡
𝑗]))
∑
𝑘∈𝑁𝑡
𝑗∪{𝑗}exp(
𝐿𝑒𝑎𝑘𝑦Re𝐿𝑈(
𝑎𝑇
𝑆[
𝑊𝑆𝑚𝑡
𝑗||𝑊𝑆𝑚𝑡
𝑘])) (14)8
where𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (⋅)is the activation function, 𝑎𝑆∈𝑅𝐷is the weight vector, 𝑁𝑡
𝑗∪{𝑗}represents the central agent 𝑗and its
observable neighborhood agent set, and 𝑊𝑆∈𝑅𝐷×𝐷is the weight matrix. The node feature of agent 𝑗is expressed as:
𝑒𝑡
𝑗=𝐸𝐿𝑈⎛
⎜
⎜⎝∑
𝑖∈𝑁𝑡
𝑗∪𝑗𝛼𝑆
𝑖𝑗𝑊𝑆𝑚𝑡
𝑖⎞
⎟
⎟⎠(15)
where𝐸𝐿𝑈(⋅)isanexponentiallinearunitfunction.Connectingthefeaturesofeachnodeinpairs: 𝐸𝑡
𝑖,𝑗=(
𝑒𝑡
𝑖||𝑒𝑡
𝑗)
,wecangeta
matrix𝐸𝑡∈𝑅𝑁×𝑁𝑗×2𝐷,where𝐸𝑡
𝑖,𝑗representstherelevantfeaturesofagent 𝑗.Taking𝐸𝑡astheinputofMLPwhichisfollowed
by a Gumbel Softmax function, the connected vector 𝐺𝑡
𝑗can be obtained. The connected vector 𝐺𝑡
𝑗consists of elements 𝑔𝑖𝑗,
where𝑖representstheneighborsofthecentralagent 𝑗.Theelement 𝑔𝑡
𝑖𝑗=1intheadjacencymatrixindicatesthattheactionof
the agent𝑖will have an impact on the agent 𝑗. Conversely, 𝑔𝑡
𝑖𝑗=0means that the agent’s actions have no effect on the agent 𝑗.
4.2Mean Field Module
This Graph-Attention method selects important 𝑀𝑗agents from the neighbors 𝑁𝑗of agent𝑗, and compute the average of the
actions of the choosed neighbor agents:
̃ 𝑎𝑗
𝑡=1
𝑀𝑗∑
𝑘∈𝑁𝑗𝑎𝑘
𝑡⋅𝐺𝑡
𝑗, 𝑎𝑘
𝑡∼𝜋𝑘(⋅∣𝑠𝑡,̃ 𝑎𝑘
𝑡)(16)
where⋅is the element-wise multiplication.
In the above formula, 𝑎𝑘represents the important neighborhood agent for agent 𝑗. Then the Q–value of each agent is shown
in Eq. 17. Note that the Q–value here is a partially observable Q–value.
𝑄𝑗
GAMF(𝑠𝑗
𝑡,𝑎𝑗
𝑡,̃ 𝑎𝑗
𝑡)=(1−𝛼)𝑄𝑗
GAMF(𝑠𝑗
𝑡,𝑎𝑗
𝑡,̃ 𝑎𝑗
𝑡)+𝛼[𝑟𝑗
𝑡+𝛾𝑣(𝑠𝑗
𝑡+1)](17)
where the value function 𝑣𝑗is expressed as
𝑣𝑗(𝑠𝑗
𝑡+1)=∑
𝑎𝑗
𝑡+1𝜋𝑗(𝑎𝑗
𝑡+1∣𝑠𝑗
𝑡+1,̃ 𝑎𝑗
𝑡)𝑄𝑗
GAMF(𝑠𝑗
𝑡+1,𝑎𝑗
𝑡+1,̃ 𝑎𝑗
𝑡)(18)
According to the above graph attention mechanism, more important neighborhood agents are obtained. The new average
actioñ 𝑎𝑗
𝑡is calculated by Eq.16, and then the strategy 𝜋𝑗
𝑡of agent𝑗is updated by the following formula:
𝜋𝑗(𝑎𝑗
𝑡∣𝑠𝑗
𝑡,̃ 𝑎𝑗
𝑡−1)=exp(−𝛽𝑄𝑗
GAMF(𝑠𝑗
𝑡,𝑎𝑗
𝑡,̃ 𝑎𝑗
𝑡−1))
∑
𝑎𝑗′
𝑡∈𝐴𝑗exp(
−𝛽𝑄𝑗
GAMF(
𝑠𝑗
𝑡,𝑎𝑗′
𝑡,̃ 𝑎𝑗
𝑡−1)) (19)
4.3Theoretical Proof
This subsection is devoted to proving that the setting of GAMFQ is close to the Nash equilibrium. Subramanian et al. [23]
showed that in partially observable cases, the fixed observation radius (FOR) setting is close to a Nash equilibrium, where the
mean action of each agent’s neighborhood agents is approximated by a dirichlet distribution. First, we state some assumptions,
which are the same as literature[23], and are followed by all the theorems and analyses below.
Assumption 1. For any𝑖and𝑗, there islim𝑡→∞𝜏𝑖
𝑗(𝑡)=∞.𝑤.𝑝.1.
This assumption guarantees a probability of 1 that old information is eventually discarded.
Assumption2. Supposesomemeasurabilityconditionsareasfollow:(1) 𝑥(0)is(0)-measurable.(2)Foreach 𝑖,𝑗and𝑡,𝑤𝑖(𝑡)
is(𝑡+1)-measurable.(3)Foreach 𝑖,𝑗and𝑡,𝛼𝑖(𝑡)and𝜏𝑖
𝑗(𝑡)are(𝑡)-measurable.(4)Foreach 𝑖and𝑡,satisfyB[𝑤𝑖(𝑡)|(𝑡)]=0.
(5)B[𝑤2
𝑖(𝑡)|(𝑡)]≤𝐴+𝐵max𝑗max𝜏≤𝑡|||𝑥𝑗(𝜏)|||2
, where𝐴and𝐵are deterministic constants.
Assumption 3. The learning rates satisfy 0≤𝛼𝑖(𝑡)<1.
Assumption 4. Suppose some conditions for the 𝐹mapping are as follows: (1) If 𝑥≤𝑦, then𝐹(𝑥)≤𝐹(𝑦), that is,𝐹is
monotonic; (2) 𝐹is continuous; (3) When 𝑡→∞,𝐹is limited to the interval [𝑥∗−𝐷,𝑥∗+𝐷], where𝑥∗is some arbitrary9
point;(4)If𝑒∈𝑛isavectorthatsatisfiesallcomponentsequalto1,then 𝐹(𝑥)−𝑝𝑒≤𝐹(𝑥+𝑝𝑒)≤𝐹(𝑥+𝑝𝑒)+𝑝𝑒,where𝑝
is a positive scalar.
Assumption 5. Each action-value pair can be accessed indefinitely, and the reward is limited.
Assumption 6. Under the limit 𝑡→∞of infinite exploration, the agent’s policy is greedy.
This assumption ensures that the agent is rational.
Assumption 7. In each stage of a stochastic game, a Nash equilibrium can be regarded as a global optimum or saddle point.
Based on these assumptions, Subramanian et al. [23] give the following lemma.
Lemma 1. [23] When the Q-function is updated using the partially observable update rule in Eq.2, and assumptions 3, 5, and
7 hold, the following holds for 𝑡→∞:
|𝑄∗(𝑠𝑡,𝑎𝑡)−𝑄𝑃𝑂𝑀𝐹(𝑠𝑡,𝑎𝑡,̃ 𝑎𝑡)|≤2𝐷 (20)
where𝑄∗istheNashQ-value, 𝑄𝑃𝑂𝑀𝐹isthepartiallyobservablemean-fieldQ-function,and 𝐷istheboundofthe 𝐹map.The
probability that the above formula holds is at least 𝛿𝐿−1, where𝐿=|𝐴|.
InourGAMFQsetting,forpartiallyobservableneighborhoodagents,wechoosetoselectalimitednumberofimportantagents
by using graph attention, and then update the POMF Q function. The following theorem proves that the setting of GAMFQ is
close to Nash equilibrium.
Theorem1. ThedistancebetweentheMFQ(globallyobservable)meanaction ̄ 𝑎andtheGAMFQ(partiallyobservable)mean
actioñ 𝑎satisfies the following formula:
|||̃ 𝑎𝑗
𝑡−̄ 𝑎𝑗
𝑡|||≤√
1
2𝑁𝑗log2
𝛿(21)
When𝑡→∞, the probability >=𝛿, where𝑁𝑗is the number of observed neighbor agents, ̃ 𝑎is the partially observable mean
action obtained by graph attention in Eq. 16, ̄ 𝑎is the globally observable mean action in Eq. 4.
Assumingthateachagentisgloballyobservable,themeanofimportantagentsselectedbygraphattentionisclosetothetrue
underlying global observable ̄ 𝑎. Since the GAMF Q-function is updated by taking finite samples through graph attention, the
empirical mean is ̃ 𝑎.
Theorem2. IftheQ-functionisLipschitzcontinuouswithrespecttothemeanaction,i.e. 𝑀isconstant,thentheMFQ-function
𝑄𝑀𝐹and GAMF Q-function 𝑄𝐺𝐴𝑀𝐹satisfy the following relation:
|||𝑄𝐺𝐴𝑀𝐹(𝑠𝑡,𝑎𝑡,̃ 𝑎𝑡−1)−𝑄𝑀𝐹(𝑠𝑡,𝑎𝑡, ̄ 𝑎𝑡−1)|||≤𝑀×𝐿×log2
𝛿×1
2𝑁𝑗(22)
When the limit 𝑡→∞, the probability is ≥(𝛿)𝐿−1, where𝐿=|𝐴|,𝐴is the action space of the agent.
Intheproofoftheorem2,firstconsideraQ-functionthatisLipschitzcontinuousforall ̄ 𝑎and̃ 𝑎.Accordingtotheorem1,the
above formula can further deduce the result of theorem 2. The total number of components is equal to the action space 𝐿. The
boundoftheorem1isprobability >=𝛿,andsincethereare 𝐿randomvariables,theprobabilityoftheorem2isatleast (𝛿)𝐿−1.
When the first 𝐿−1random variable is fixed, the deterministic last ̄ 𝑎component satisfies the relationship that the sum of the
individual components is 1. Since each agent’s action is represented by a one-hot encoding, the ̃ 𝑎′component of GAMFQ also
satisfies the relationship that the sum of the individual components is 1, and the component of the agent’s average action does
not change due to the application of graph attention. The proof of theorem 2 ends.
Theorem 3. A stochastic process in form 𝑥𝑖(𝑡+1) =𝑥𝑖(𝑡)+𝛼𝑖(𝑡)(𝐹𝑖(𝑥𝑖(𝑡))−𝑥𝑖(𝑡)+𝑤𝑖(𝑡))remains bounded in the range
[𝑥∗−2𝐷,𝑥∗+2𝐷]onlimit𝑡→∞ifassumptions1,2,3and4aresatisfied,andareguaranteednottodivergetoinfinity.Where
𝐷is the boundary of the 𝐹map in assumption 4(4).
This theorem can be proved in terms of Tsitsiklis[24] and by extension. The result of theorem 3 can then be used to derive
theorem 4.10
Theorem 4. When the Q-function is updated using the partially observable update rule in Eq.17, and assumptions 3, 5, and 7
hold, the following holds for 𝑡→∞:
|𝑄∗(𝑠𝑡,𝑎𝑡)−𝑄𝐺𝐴𝑀𝐹(𝑠𝑡,𝑎𝑡,̃ 𝑎𝑡)|≤2𝐷 (23)
where𝑄∗istheNashQ-value, 𝑄𝐺𝐴𝑀𝐹isthepartiallyobservablemean-fieldQ-function,and 𝐷istheboundofthe 𝐹map.The
probability that the above formula holds is at least 𝛿𝐿−1, where𝐿=|𝐴|.
Theorem 4 shows that the GAMFQ update is very close to the Nash equilibrium at the limit 𝑡→∞, i.e. reaching a plateau
for stochastic policies. Therefore, the strategy of Eq.19 is approximately close to this plateau. Theorem 4 is an application of
theorem3,usingassumptions3,5and7.However,inMARL,reachingaNashequilibriumisnotoptimal,butonlyafixed-point
guarantee. Therefore, to achieve better performance, each selfish agent will still tend to pick a limited number of samples. To
balancetheoryandperformancewhenselectingagentsfromtheneighborhood,anappropriatenumberofagents(moreefficient
agents) need to be used for better multi-agent system performance. This paper uses the graph attention structure to filter out
more important proxies, which can better approximate the Nash equilibrium.
4.4Algorithm
TheimplementationofGAMFQfollowstherelatedworkofthepreviousPOMFQ[23],thedifferenceisthatthegraphattention
structureisusedtoselecttheneighborhoodagentsthataremoreimportanttothecentralagentwhenupdatingtheaverageaction.
Algorithm1givesthepseudocodeoftheGAMFQalgorithm.Itobtainseffectiveneighboragentsbycontinuouslyupdatingthe
adjacency matrix 𝐺𝑡
𝑗to update the agent’s strategy.
Algorithm 1 Partially Observable Mean Field MARL Based on Graph–Attention
Initialize the weights of Q-function 𝑄𝜙𝑗,𝑄𝜙𝑗
−, replay buffer 𝐵, GAT encoder, MLP layers and mean action ̄ 𝑎𝑗for each agent
𝑗∈1,…,𝑁.
for𝑒𝑝𝑖𝑠𝑜𝑑𝑒=1,2,…,𝐸do
for𝑡≤𝑇and not terminal do
For each agent 𝑗, calculate the hidden state ℎ𝑡
𝑗according to Eq.11, and encode ℎ𝑡
𝑗as a message 𝑚𝑡
𝑗(Eq.12).
For each agent 𝑗, sample𝑎𝑗fron policy induced by 𝑄𝜙𝑗(Eq.19).
For each agent 𝑗, pass the encoded information 𝑚𝑡
𝑗to the GAT encoder and hard attention mechanism to output the
adjacency matrix 𝐺𝑡
𝑗.
For each agent 𝑗, calculate the new neighborhood agent mean action ̄ 𝑎𝑗by Eq.16.
Receive the full state of environment 𝑠𝑡, action𝑎=[𝑎1,…,𝑎𝑁], reward[𝑟=𝑟1,…,𝑟𝑁], and the next state 𝑠′=[𝑠1,…,𝑠𝑁].
Store transition ⟨𝑠,𝑎,𝑟,𝑠′, ̄ 𝑎⟩in𝐵, wherē 𝑎=[̄ 𝑎1,…, ̄ 𝑎𝑁]is the mean action.
end for
for𝑗=1,…,𝑁do
Sample a minibatch of K experiences ⟨𝑠,𝑎,𝑟,𝑠′, ̄ 𝑎⟩from replay buffer 𝐵.
Set𝑦𝑗=𝑟𝑗+𝛾𝑣𝜙(𝑠′)according to Eq.18.
minimize the loss 𝐿(𝜙𝑗)=(𝑦𝑗−𝑄𝜓𝑗(𝑠′,𝑎𝑗, ̄ 𝑎𝑗))2
to update Q network.
end for
For each agent 𝑗, update params of target network : 𝜙𝑗←𝜏𝜙𝑗+(1−𝜏)𝜙𝑗.
end for
5EXPERIMENTS
Inthissection,wedescribethreedifferenttasksbasedontheMAgentframeworkandgivesomeexperimentalsetupandtraining
details for evaluating the GAMFQ performance.11
5.1Environments and Tasks
Subramanianetal.[23]designedthreedifferentcooperative-competitivestrategiesintheMAgentframework[36]asexperimen-
talenvironments,andourexperimentsadoptthesameenvironments.Inthesethreetasks,themapsizeissetto28*28,wherethe
observationrangeofeachagentis6units.Thestatespaceistheconcatenationofthefeatureinformationofotheragentswithin
eachagent’sfieldofview,includinglocation,health,andgroupinginformation.Theactionspaceincludes13moveactionsand
8 attack actions. In addition, each agent is required to handle at most 20 other agents that are closest. We will evaluate against
these three tasks:
•Multibattleenvironment: Therearetwogroupsofagentsfightingeachother,eachcontaining25agents.Theagentgets
-0.005pointsforeachmove,-0.1pointsforattackinganemptyarea,200pointsforkillinganenemyagent,and0.2points
for a successful attack. Each agent is 2*2 in size, has a maximum health of 10 units, and a speed of 2 units. After the
battle, the team with the most surviving agents wins. If both teams have the same number of surviving agents, the team
with the highest reward wins. The reward for each team is the sum of the rewards for the individual agents in the team.
•Battle-Gathering environment: There is a uniform distribution of food in the environment, each agent can observe the
locationofallthefood.Inadditiontoattackingtheenemytogetrewards,eachagentcanalsoeatfoodtogetrewards.Agents
get 5 points for attacking enemy agents, and the rest of the reward settings are the same as the Multibattle environment.
•Predator-Preyenvironment: Thereare40predatorsand20prey,whereeachpredatorisasquaregridofsize2*2witha
maximumhealthof10unitsandaspeedof2units.Preyisa1*1squarewithamaximumhealthof2unitsandaspeedof
2.5units.Towinthegame,thepredatormustkillmoreprey,andthepreymustfindawaytoescape.Inaddition,predators
and prey have different reward functions, predators get -0.3 points for attacking space, 1 point for successfully attacking
prey, 100 points for killing prey, -1 point for attacked prey, and 0.5 points for dying. Unlike the Multibattle environment,
when the round ends for a fairer duel, if the two teams have the same number of surviving agents, it is judged as a draw.
5.2Evaluation
We consider four algorithms for the above three games: MFQ, MFAC [31], POMFQ(FOR) and GAMFQ, where MFQ and
MFAC are baselines and POMFQ(FOR) [23] is the state-of-the-art algorithm.
TheoriginalbaselinesMFQandMFACwereproposedbyYangetal.[31]basedonglobalobservability,andtheideawasto
approximatetheinfluenceoftheneighborhoodagentsonthecentralagentastheiraverageactions,therebyupdatingtheactions
oftheneighborhoodagents.WefixtheobservationradiusofeachagentinthebaselineMFQandMFACandapplyittoapartially
observableenvironment,whereneighboragentsareagentswithinafixedrange.ThePOMFQ(FOR)algorithmintroducesnoise
inthemeanactionparameterstoencourageexploration,usesBayesianinferencetoupdatetheDirichletdistribution,andsamples
100 samples from the Dirichlet distribution to estimate partially observable mean field actions. The GAMFQ algorithm judges
the effectiveness of neighborhood agents within a fixed range through the graph attention mechanism, selects more important
neighborhood agents, and updates the average action by averaging the actions of these agents.
5.3Hyperparameters
Inthethreetasks,eachalgorithmwastrainedfor2000epochsinthetrainingphase,generatingtwosetsofAandBsetsofmodels.
Inthe testphase,1000 roundsofconfrontation wereconducted,ofwhich thefirst500 roundswerethe firstgroupA ofthefirst
algorithmagainstthesecondgroupBofthesecondalgorithm,andthelast500groupsweretheopposite.Thehyperparameters
of MFQ, MFAC, POMFQ(FOR) and GAMFQ are basically the same. Table 1 lists the hyperparameters during training of the
four algorithms, and the remaining parameters can be seen in [23].
6RESULTS AND DISCUSSION
Inthissection,weevaluatetheperformanceofGMAFQinthreedifferentenvironments,includingMultibattle,Battle-Gathering,
and Predator-Prey. We benchmark against two algorithms, MFQ and MFAC, and compare with the state-of-the-art POMFQ12
TABLE 1 Hyperparameters for four algorithms training.
Parameter Value Description
𝛼 10−4learning rate
𝛽 decays linearly from 1 to 0 exploration rate
𝛾 0.95 discount rate
𝐵 1024 replay buffer
ℎ 64 the hidden layer size in GAT
𝐾 64 mini-batch
𝑡𝑒𝑚𝑝𝑒𝑟𝑎𝑡𝑢𝑟𝑒 0.1 the soft-max layer temperature of the actor in MFAC
(FOR). We implement our method and comparative methods on three different tasks. Note that we only used 50 agents in our
experimentsanddidnottestmoreagents,thisisbecausetheproportionofotheragentsthateachagentcanseeismoreimportant
than the absolute number.
6.1Reward
Figure 3 shows how the reward changes as the number of iterations increases during training. We plot the reward changes for
the four algorithms in different game environments during the first 1000 iterations. Since each algorithm is self-training which
resultsinalargechangeintherewardofthealgorithm,weusetheleastsquaresmethodtofittherewardchangegraph.InFigure
3, the solid black line represents the reward change graph of the GAMFQ algorithm. From Figure 3 (a), (b) and (c), it can be
seenthattherewardoftheGAMFQalgorithmcanincreaserapidly,indicatingthattheGAMFQalgorithmcanconvergerapidly
in the early stage, and the convergence performance is better than the other three algorithms.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000014/uni00000013/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034
(a) Train results of Multibattle game.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (b) Train results of Battle-Gathering game.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000014/uni00000013/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (c) Train results of Predator-Prey game.
FIGURE 3 Train results of three games. The reward curve for each algorithm is fitted by the least squares method.
6.2Elo Calculation
We use ELO Score [11] to evaluate the performance of the two groups of agents, the advantage of which is that it takes into
account the strength gap between the opponents themselves. ELO ratings are commonly used in chess to evaluate one-on-one
situations, and this approach can similarly be extended to N-versus-N situations. For the algorithm proposed in the paper, we
record the total rewards of the two teams of agents during each algorithm confrontation, which are 𝑅1and𝑅2, respectively.
Then the expected win rates of the two groups of agents are:
𝐸1=1
1+10(𝑅2−𝑅1)∕400,𝐸2=1
1+10(𝑅1−𝑅2)∕400(24)13
where𝐸1+𝐸2= 1. By analyzing the actual and predicted winning rates of the two groups of agents, the new ELO score of
each team after the game ends can be obtained:
𝑅1′=𝑅1+𝐾(𝑆1−𝐸1),𝑅2′=𝑅2+𝐾(𝑆2−𝐸2) (25)
where𝑅1representstheactualwinningorlosingvalue,1meanstheteamwins,0.5meansthetwoteamsaretied,and0means
the team loses. 𝐾is represented as a floating coefficient. To create a gap between agents, we set 𝐾to 32. For each match, we
faced off 500 times and calculated the average ELO value for all matches.
TABLE 2 The ELO Score of four algorithms in Multibattle environment.
Task Algorithm1 Algorithm2 ELO Score1 ELO Score2
GAMFQ vs POMFQ(FOR)GAMFQ-1 POMFQ(FOR)-2 3579 820
GAMFQ-2 POMFQ(FOR)-1 2696 2838
GAMFQ vs MFQGAMFQ-1 MFQ-2 2098 1508
GAMFQ-2 MFQ-1 2535 1695
GAMFQ vs MFACGAMFQ-1 MFAC-2 1350 -49
GAMFQ-2 MFAC-1 -856 -78
POMFQ(FOR) vs MFQPOMFQ(FOR)-1 MFQ-2 3145 2577
POMFQ(FOR)-2 MFQ-1 2569 2857
POMFQ(FOR) vs MFACPOMFQ(FOR)-1 MFAC-2 -205 -64
POMFQ(FOR)-2 MFAC-1 826 -42
MFQ vs MFACMFQ-1 MFAC-2 -142 -49
MFQ-2 MFAC-1 610 -46
TABLE 3 The ELO Score of four algorithms in Battle-Gathering environment.
Task Algorithm1 Algorithm2 ELO Score1 ELO Score2
GAMFQ vs POMFQ(FOR)GAMFQ-1 POMFQ(FOR)-2 7770 8931
GAMFQ-2 POMFQ(FOR)-1 8293 9310
GAMFQ vs MFQGAMFQ-1 MFQ-2 6374 10870
GAMFQ-2 MFQ-1 8510 8313
GAMFQ vs MFACGAMFQ-1 MFAC-2 5525 10
GAMFQ-2 MFAC-1 10751 -31
POMFQ(FOR) vs MFQPOMFQ(FOR)-1 MFQ-2 8526 8760
POMFQ(FOR)-2 MFQ-1 8632 8227
POMFQ(FOR) vs MFACPOMFQ(FOR)-1 MFAC-2 12722 0
POMFQ(FOR)-2 MFAC-1 12171 -88
MFQ vs MFACMFQ-1 MFAC-2 12649 49
MFQ-2 MFAC-1 13788 -4814
Table 2, 3, 4 shows the ELO scores of the four algorithms on the three tasks. It can be seen from Table 2 that in Multibattle
environment, the GAMFQ algorithm has the highest ELO score of 3579, which is significantly better than the other three
algorithms.AsshowninTable3,inBattle-Gatheringenvironment,theELOscoreoftheMFQalgorithmisthehighest,andthe
ELOscoreoftheGAMFQalgorithmisaverage.Thisisbecausethecollectionenvironmentcontainsfood,andsomealgorithms
tendtoeatfoodtogetrewardsquickly,ratherthanattackingenemyagents.However,thefinalgamewinningorlosingdecision
ismadebycomparingthenumberofremainingagentsbetweenthetwoteamsofagents.AsshowninTable4,inPredator-Prey
environment, the ELO score of the GAMFQ algorithm has the highest ELO score of 860, which is significantly better than
the other three algorithms. From the experimental results in the three environments, we can summarize that ELO score of the
GAMFQ algorithm is better than other three algorithms, showing better performance.
TABLE 4 The ELO Score of four algorithms in Predator-Prey environment.
Task Algorithm1 Algorithm2 ELO Score1 ELO Score2
GAMFQ vs POMFQ(FOR)GAMFQ-1 POMFQ(FOR)-2 421 -32
GAMFQ-2 POMFQ(FOR)-1 16 7
GAMFQ vs MFQGAMFQ-1 MFQ-2 714 -27
GAMFQ-2 MFQ-1 -15 -94
GAMFQ vs MFACGAMFQ-1 MFAC-2 860 -28
GAMFQ-2 MFAC-1 16 16
POMFQ(FOR) vs MFQPOMFQ(FOR)-1 MFQ-2 66 18
POMFQ(FOR)-2 MFQ-1 13 24
POMFQ(FOR) vs MFACPOMFQ(FOR)-1 MFAC-2 16 -16
POMFQ(FOR)-2 MFAC-1 47 16
MFQ vs MFACMFQ-1 MFAC-2 16 -16
MFQ-2 MFAC-1 174 17
6.3Results
Figure 4 shows the face-off results of the four algorithms in the three tasks. Figure 4(a) shows the faceoff results of Multibattle
game. The different colored bars for each algorithm represent the results of an algorithm versus others. We do not conduct
adversarialexperimentsbetweenthesamealgorithmsbecauseweconsiderthattheadversarialpropertiesofthesamealgorithms
are equal. The vertical lines in the bar graph represent the standard deviation of wins for groups A and B over 1,000 face-offs.
Figure 4(a) shows GAMFQ against three other algorithms, all with a win rate above 0.7.
Figure4(b)showsthefaceoffresultsofBattle-Gatheringgame.Inadditiontogettingrewardsforkillingenemies,agentscan
alsogetrewardsfromfood.ItcanbeseenthatMFQlosestoallotheralgorithms,MFACandPOMFQ(FOR)performingeneral,
and our GAMFQ is clearly ahead of other algorithms.
Figure4(c)showsthwefaceoffresultsofPredator-Preygame.Thestandarddeviationofthisgameissignificantlyhigherthan
theprevioustwogames,duetothefactthatbothgroupsAandBaretryingtobeateachotherintheenvironment.Itcanbeseen
that the GAMFQ algorithm is significantly better than other three algorithms, reaching a winning rate of 1.0.
Experimentsintheabovethreemulti-agentcombatenvironmentsshowthatGAMFQcanshowgoodperformanceoverMFQ,
MFAC and POMFQ(FOR).15
/uni00000030/uni00000029/uni00000034 /uni00000030/uni00000029/uni00000024/uni00000026 /uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c /uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000044/uni00000050/uni00000048/uni00000056/uni00000003/uni0000005a/uni00000052/uni00000051/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034
(a) Faceoff results of Multibattle game.
/uni00000030/uni00000029/uni00000034 /uni00000030/uni00000029/uni00000024/uni00000026 /uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c /uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000044/uni00000050/uni00000048/uni00000056/uni00000003/uni0000005a/uni00000052/uni00000051/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (b) Faceoff results of Battle-Gathering game.
/uni00000030/uni00000029/uni00000034 /uni00000030/uni00000029/uni00000024/uni00000026 /uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c /uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000044/uni00000050/uni00000048/uni00000056/uni00000003/uni0000005a/uni00000052/uni00000051/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (c) Faceoff results of Predator-Prey game.
FIGURE 4 Faceoff results of three games. The * in the legend indicates the enemy. For example, the first blue bar in the bar
graph corresponding to the GAMFQ algorithm is the result of the confrontation between GAMFQ and MFQ, and we do not
conduct confrontation experiments between the same algorithms.
6.4Visualization
(a) Multibattle begin (b) Multibattle process (c)Multibattle end cooperative 
attackattackescape
GAMFQ   alive ：22 POMFQ(FOR)    alive ：2
FIGURE 5 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Multibattle game.
(a) Battle -Gathering begin (b) Battle -Gathering process (c)Battle -Gathering end 
Besiege 
beginBesiege 
end
food GAMFQ   alive ：24 POMFQ( FOR)    alive ：9
FIGURE 6 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Battle-Gathering game.16
(a) Predator -Prey begin (b1) Predator -Prey process (b2) Predator -Prey process 
GAMFQ   alive ：25
POMFQ(FOR)    alive ：21POMFQ(FOR)    alive ：25
GAMFQ   alive ：25encircleescape
FIGURE 7 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Predator-Prey game.
TovisualizetheeffectivenessoftheGAMFQalgorithm,wevisualizetheconfrontationbetweenGAMFQandPOMFQ(FOR)
in a Multibattle environment, as shown in Figure 5, where the red side is GMAFQ and the blue side is POMFQ (FOR). It can
beseenfromtheconfrontationprocessthatfortheGAMFQalgorithm,whenanagentdecidestoattack,thesurroundingagents
will also decide to attack under its influence, forming a good cooperation mechanism. On the contrary, for the POMFQ (FOR)
algorithm, some blue-side agents are chosen to attack, some are chosen to escape, and no common fighting mechanism was
formed.Similarly,intheBattle-GatheringenvironmentofFigure6,GAMFQcanlearnthesurroundingmechanismwell.Inthe
Predator-PreyenvironmentofFigure7,whenGAMFQactsasapredator,thetechniqueofsurroundingthepreyPOMFQ(FOR)
can be learned. On the contrary, when POMFQ (FOR) acted as a predator, it failed to catch the prey GMAFQ.
6.5Ablation study
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000010/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000035/uni00000020/uni00000015
/uni00000035/uni00000020/uni00000017/uni00000035/uni00000020/uni00000019
/uni00000035/uni00000020/uni0000001b/uni00000035/uni00000020/uni00000014/uni00000013
FIGURE 8 Ablation study. R represents the observation radius of the agent.
Figure8isanablationstudythatinvestigatestheperformanceoftheGAMFQalgorithmfordifferentobservationradiusina
Multibattleenvironment.wherethesolidlinerepresentstheleastsquaresfitoftherewardchange.Itcanbeseenfromthefigure
that when the number of training is small, the performance of the algorithm is better as the observation distance increases. But
with the increase of training times, when R=4, the performance of the algorithm is the best, so the appropriate observation
distancecanachievebetterperformance.Whatismoreimportantinthispaperistoexploretheeffectoftheratioofobservable
distance to the number of agents on the performance of the algorithm, so there is no experiment with more agents.17
7CONCLUSION
In this paper, we proposed a new multi-agent reinforcement learning algorithm, Graph Attention-based Partially Observable
MeanReinforcementLearning(GAMFQ),toaddresstheproblemoflarge-scalepartiallyobservablemulti-agentenvironments.
Although existing methods are close to Nash equilibrium, they do not take into account the direct correlation of agents. Based
onthecorrelationbetweenagents,GAMFQusesaGraph-Attentionmoduletodescribehoweachagentisaffectedbytheactions
of other agents at each time step. Experimental results on three challenging tasks in the MAgents framework illustrate that,
our proposed method outperforms baselines in all these games and outperforms the state-of-the-art partially observable mean-
field reinforcement learning algorithms. In the future, we will further explore the correlation between agents to extend to more
common cooperation scenarios.
Conflict of interest
The authors declare no potential conflict of interests.
Article Description
The expanded version of this article is published in Drones 2023, 7(7), 476, with a DOI of
https://doi.org/10.3390/drones7070476.
References
[1] A. T. Azar et al., Drone deep reinforcement learning: A review , Electronics 10(2021), no. 9, 999.
[2] W.Boehmer,V.Kurin,andS.Whiteson, Deepcoordinationgraphs ,Proceedingsofthe37thInternationalConferenceon
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event ,Proceedings of Machine Learning Research , vol. 119,
PMLR, 980–991.
[3] Q. Cai, Z. Yang, and Z. Wang, Reinforcement learning from partial observation: Linear function approximation with
provable sample efficiency ,International Conference on Machine Learning , PMLR, 2485–2522.
[4] J. Fan et al., A theoretical analysis of deep q-learning ,Learning for Dynamics and Control , PMLR, 486–489.
[5] B.Fangetal., Large-scalemulti-agentreinforcementlearningbasedonweightedmeanfield ,CognitiveSystemsandSignal
Processing-5thInternationalConference,ICCSIP2020,Zhuhai,China,December25-27,2020,RevisedSelectedPapers ,
Communications in Computer and Information Science , vol. 1397, Springer, 309–316.
[6] H. Gu et al., Mean-field multi-agent reinforcement learning: A decentralized network approach , arXiv preprint
arXiv:2108.02731 (2021).
[7] J. Gu et al., A metaverse-based teaching building evacuation training system with deep reinforcement learning , IEEE
Transactions on Systems, Man, and Cybernetics: Systems (2023).
[8] Q.Hao, Verylargescalemulti-agentreinforcementlearningwithgraphattentionmeanfield ,https://openreview.net/forum?
id=MdiVU9lMmVS (2023).
[9] K. He, P. Doshi, and B. Banerjee, Reinforcement learning in many-agent settings under partial observability ,The 38th
Conference on Uncertainty in Artificial Intelligence .
[10] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, A survey and critique of multiagent deep reinforcement learning ,
Autonomous Agents and Multi-Agent Systems 6(2019), no. 33, 750–797.
[11] M.Jaderbergetal., Human-levelperformanceinfirst-personmultiplayergameswithpopulation-baseddeepreinforcement
learning, ArXivabs/1807.01281 (2018).18
[12] M. Laurière et al., Learning mean field games: A survey , arXiv preprint arXiv:2205.12944 (2022).
[13] S. Li et al., Deep implicit coordination graphs for multi-agent reinforcement learning ,AAMAS ’21: 20th International
Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021 , ACM, 764–
772.
[14] M.L.Littman, Markovgamesasaframeworkformulti-agentreinforcementlearning ,Machinelearningproceedings1994 ,
Elsevier, 1994. 157–163.
[15] R. Lowe et al., Multi-agent actor-critic for mixed cooperative-competitive environments ,Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA , 6379–6390.
[16] A. Mahajan et al., Maven: Multi-agent variational exploration , Advances in Neural Information Processing Systems 32
(2019), 7611–7622.
[17] Y. Niu, R. R. Paleja, and M. C. Gombolay, Multi-agent graph-attention communication and teaming ,AAMAS ’21: 20th
InternationalConferenceonAutonomousAgentsandMultiagentSystems,VirtualEvent,UnitedKingdom,May3-7,2021 ,
ACM, 964–973.
[18] F. A. Oliehoek and C. Amato, A concise introduction to decentralized POMDPs , Springer, 2016.
[19] T.Rashidetal., QMIX:monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning ,Proceedingsof
the35thInternationalConferenceonMachineLearning,ICML2018,Stockholmsmässan,Stockholm,Sweden,July10-15,
2018,Proceedings of Machine Learning Research , vol. 80, PMLR, 4292–4301.
[20] J. Ruan et al., Gcs: Graph-based coordination strategy for multi-agent reinforcement learning , arXiv preprint
arXiv:2201.06257 (2022).
[21] L. M. Schmidt et al., An introduction to multi-agent reinforcement learning and review of its application to autonomous
mobility, arXiv preprint arXiv:2203.07676 (2022).
[22] H. Shi et al., Marl sim2real transfer: Merging physical reality with digital virtuality in metaverse , IEEE Transactions on
Systems, Man, and Cybernetics: Systems (2022).
[23] S. G. Subramanian et al., Partially observable mean field reinforcement learning ,AAMAS ’21: 20th International
Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021 , ACM,
537–545.
[24] J. N. Tsitsiklis, Asynchronous stochastic approximation and q-learning , Machine learning 16(1994), no. 3, 185–202.
[25] P. Velickovic et al., Graph attention networks , stat1050(2017), 20.
[26] O.Vinyalsetal., Grandmasterlevelinstarcraftiiusingmulti-agentreinforcementlearning ,Nature575(2019),no.7782,
350–354.
[27] L. Wang et al., Neural policy gradient methods: Global optimality and rates of convergence , arXiv preprint
arXiv:1909.01150 (2019).
[28] T.Wuetal., Weightedmean-fieldmulti-agentreinforcementlearningviarewardattributiondecomposition ,International
Conference on Database Systems for Advanced Applications , Springer, 301–316.
[29] Z.Wuetal., Acomprehensivesurveyongraphneuralnetworks ,IEEEtransactionsonneuralnetworksandlearningsystems
32(2020), no. 1, 4–24.
[30] Q. Xie et al., Learning while playing in mean-field games: Convergence and optimality ,International Conference on
Machine Learning , PMLR, 11436–11447.19
[31] Y. Yang et al., Mean field multi-agent reinforcement learning ,Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 ,Proceedings of Machine
Learning Research , vol. 80, PMLR, 5567–5576.
[32] H. Zhang et al., H2gnn: Hierarchical-hops graph neural networks for multi-robot exploration in unknown environments ,
IEEE Robotics and Automation Letters 7(2022), no. 2, 3435–3442.
[33] K. Zhang, Z. Yang, and T. Başar, Multi-agent reinforcement learning: A selective overview of theories and algorithms ,
Handbook of Reinforcement Learning and Control (2021), 321–384.
[34] T.Zhangetal., MFVFD:Amulti-agentq-learningapproachtocooperativeandnon-cooperativetasks ,Proceedingsofthe
Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27
August 2021 , ijcai.org, 500–506, .
[35] Y.Zhangetal., Coordinationbetweenindividualagentsinmulti-agentreinforcementlearning ,Thirty-FifthAAAIConfer-
ence on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,
IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event,
February 2-9, 2021 , AAAI Press, 11387–11394.
[36] L. Zheng et al., Magent: A many-agent reinforcement learning platform for artificial collective intelligence ,Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial
Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018 , AAAI Press, 8222–8223.
[37] S. Zhou et al., Multi-agent mean field predict reinforcement learning ,2020 IEEE International Conference on Advances
in Electrical Engineering and Computer Applications (AEECA) , IEEE, 625–629.
[38] Z.ZhouandG.Liu, Romfac:Arobustmean-fieldactor-criticreinforcementlearningagainstadversarialperturbationson
states, arXiv preprint arXiv:2205.07229 (2022).
How to cite this article: Min Yang, Guanjun Liu, Ziyuan Zhou, Jiacun Wang (2023), Partially Observable Mean Field Multi-
Agent Reinforcement Learning Based on Graph Attention Network for UAV Swarms, Drones,2023;7(7):476 . https://www.
mdpi.com/2504-446X/7/7/476