Received: Added at production Revised: Added at production Accepted: Added at production
DOI: xxx/xxxx
ARTICLE TYPE
Partially Observable Mean Field Multi-Agent Reinforcement
Learning Based on Graphâ€“Attention
Min Yang | Guanjun Liu* | Ziyuan Zhou
1Department of Computer Science, Tongji
University, Shanghai, China Summary
Traditional multi-agent reinforcement learning algorithms are difficultly applied in
a large-scale multi-agent environment. The introduction of mean field theory has
enhanced the scalability of multi-agent reinforcement learning in recent years. This
paper considers partially observable multi-agent reinforcement learning (MARL),
where each agent can only observe other agents within a fixed range. This partial
observability affects the agentâ€™s ability to assess the quality of the actions of sur-
roundingagents.Thispaperfocusesondevelopingamethodtocapturemoreeffective
information from local observations in order to select more effective actions. Pre-
vious work in this field employs probability distributions or weighted mean field
to update the average actions of neighborhood agents, but it does not fully con-
siderthefeatureinformationofsurroundingneighborsandleadstoalocaloptimum.
In this paper, we propose a novel multi-agent reinforcement learning algorithm,
Partially Observable Mean Field Multi-Agent Reinforcement Learning based on
Graphâ€“Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention
module and a mean field module to describe how an agent is influenced by the
actions of other agents at each time step. This graph attention module consists of a
graph attention encoder and a differentiable attention mechanism, and this mecha-
nismoutputsadynamicgraphtorepresenttheeffectivenessofneighborhoodagents
against central agents. The meanâ€“field module approximates the effect of a neigh-
borhood agent on a central agent as the average effect of effective neighborhood
agents.WeevaluateGAMFQonthreechallengingtasksintheMAgentsframework.
ExperimentsshowthatGAMFQoutperformsbaselinesincludingthestate-of-the-art
partiallyobservablemean-fieldreinforcementlearningalgorithms.Thecodeforthis
paper is here https://github.com/yangmin32/GPMF.
KEYWORDS:
Graphâ€“Attention, Multi-agent reinforcement learning, Mean field theory, Partial observation
1INTRODUCTION
Reinforcement learning has been widely used in video games [26] and recently in education [7]. For multi-agent reinforce-
ment learning (MARL) [33], it involves multiple autonomous agents that make autonomous decisions to accomplish some
specificcompetitiveorcooperativetasksbymaximizingglobalreward,ithasbeenappliedinsomereal-worldscenariossuchasarXiv:2304.12653v4  [cs.AI]  9 Sep 20242
autonomous mobile [21] drone swarm confrontation[1] and multi-UAV collaboratively delivering goods [22]. For example, in
some of the drone swarm adversarial tasks, drones need to make actions based on autonomous decisions. Due to the inevitable
death of some drones in the confrontation environment [38], the surviving drones must constantly evolve their strategies in
real-timeduringtheinteractionwiththeenvironmenttoobtaintheoverallmaximumreward.Inordertomakebetterinteraction
among agents, it is required that each agent in the multi-agent system can effectively perceive environmental information and
fully acquire the information of surrounding agents.
However,theglobalcommunicationcostamongmultipleagentsishigh,andinmanypracticaltasks,eachagentonlyobserves
part of the environmental information. Take the task of Autonomous Driving as an example, each vehicle makes decision
in the limited observation space which is a typical local observation scene. Each agent can only rely on limited observation
information in the local observation environment, therefore the agent needs to learn a decentralized strategy. There are two
common decentralization strategies. One is Centralized Training and Decentralized Execution (CTDE), which requires agents
to communicate with each other during training and to independently make decisions based on their own observations during
testing in order to adapt to large-scale multi-agent environments. Some classic algorithms using the CTDE framework such
as MADDPG [15], QMIX [19] and MAVEN [16]. Another one takes the policy of decentralized training and decentralized
execution, in which each agent can only observe part of the information during the training and testing phases, which is closer
to the real environment with limited communication. Especially large-scale multi-agent environments are complex and non-
stationary [10], it is difficult for agents to observe the entire environment globally, limiting theirability to find the best actions.
Furthermore, as the number of agents increases, joint optimization of all information in a multi-agent environment may result
in a huge joint state-action space, which also brings scalability challenges. This paper focuses on the second strategy.
Traditionalmulti-agentreinforcementlearningalgorithmsaredifficulttobeappliedinlarge-scalemulti-agentenvironments,
especiallywhenthenumberofagentsisexponential.Recentstudiesaddressthescalabilityissuesofmulti-agentreinforcement
learning [31, 30, 12] by introducing mean-field theory, i.e., the multi-agent problem is reduced to a simple two-agent problem.
However,Yangetal.[31]assumesthateachagentcanobserveglobalinformation,whichisdifficulttoapplyinsomerealtasks.
Therefore, it is necessary to study large-scale multi-agent reinforcement learning algorithms in partially observable cases [3].
In addition, researchers have intensively studied mean-field-based multi-agent reinforcement learning algorithms to improve
performance in partially observable cases. One way is to further decompose the Q-function of the mean field-based multi-
agent reinforcementlearning algorithm[34, 6].Another wayuses probabilitydistribution or weightedmean fieldto updatethe
mean action of neighborhood agents [5, 37, 23, 28]. Hao [8] combined the graph attention with the mean field to calculate the
interaction strength between agents when agents interact, but only considered the scene where the agent has a fixed relative
position, and the agents can observe the global information. The difference is that when the agent is partially observable, we
consider the dynamic change of the agentâ€™s position and the death scene of the agent, and construct a more flexible partial
observable graph attention network based on the mean field.
However, for partially observable multi-agent mean field reinforcement learning, the existing methods do not fully consider
the feature information of the surrounding neighbors, which will lead to falling into local optimum. This paper focuses on
identifying the neighborhood agents that may have the greater influence on the central agent in a limited observation space,
in order to avoid the local optimum issue. Since the graph neural network [29] can fully aggregate the relationship between
the central agent and its surrounding neighbors, we propose a graph attention-based mechanism to calculate the importance of
neighbor agents to estimate the average action more efficiently.
The main contributions of this paper are as follows:
â€¢Weproposeapartiallyobservablemeanâ€“fieldreinforcementlearningbasedonthegraphâ€“attention(GAMFQ),whichcan
learn a decentralized agent policy from an environment without requiring global information of an environment. In the
case of partially observable large-scale agents, the judgment of the importance of neighbor agents is insufficient in our
GAMFQ.
â€¢We theoretically demonstrate that the settings of the GAMFQ algorithm are close to Nash equilibrium.
â€¢Experiments on three challenging tasks in the MAgents framework show that GAMFQ outperforms two baseline
algorithms as well as the state-of-the-art partially observable mean-field reinforcement learning algorithms.3
2RELATED WORK
MostoftherecentMARLalgorithmsforpartialobservabilityresearcharemodel-freereinforcementlearningalgorithmsbased
on the CTDE framework. The most classic algorithm MADDPG [15] introduces critics that can observe global information
in training to guide actor training, but only use actors with local observation information to take actions in testing. QMIX[19]
usesahybridnetworktocombinethelocalvaluefunctionsofasingleagent,andaddsglobalstateinformationassistanceinthe
training and learning process to improve the performance of the algorithm. MAVEN [16] is able to solve complex multi-agent
tasksbyintroducinglatentspacesforhierarchicalcontrolbyvalue-mixingandpolicy-basedapproaches.However,thesemulti-
agentreinforcementlearningalgorithmusingtheCTDEframeworkisdifficulttoscaletolarge-scalemulti-agentenvironments,
because there will be hard-to-observe global information that prevents the agents from training better policies.
For large-scale multi-agent environments, Yang et al. [31] introduced the meanâ€“field theory, which approximates the inter-
action of many agents as the interaction between the central agent and the average effects from neighboring agents. However,
partially observed multi-agent meanâ€“field reinforcement learning algorithms still have a space to improve. Some researchers
further decompose the Q-function of the mean field based multi-agent reinforcement learning algorithm. Zhang et al. [34]
trained agents through the CTDE paradigm, transforming each agentâ€™s Q-function into its local Q-function and its mean field
Q-function, but this approach is not strictly partially observable. Gu et al. [6] proposes a mean field multi-agent reinforcement
learning algorithm with local training and decentralized execution. The Q-function is decomposed by grouping the observable
neighborstatesofeachagentinamulti-agentsystem,sothattheQ-functioncanbeupdatedlocally.Inaddition,someresearchers
have focused on improving the mean action in mean field reinforcement learning. Fang et al. [5] adds the idea of mean field to
MADDPG,andproposesamulti-agentreinforcementlearningalgorithmbasedonweightedmeanfield,sothatMADDPGcan
adapttolarge-scalemulti-agentenvironment.Wangetal.[28]proposeaweightedmean-fieldmulti-agentreinforcementlearn-
ing algorithm based on reward attribution decomposition by approximating the weighted mean field as a joint optimization of
implicitrewarddistributionbetweenacentralagentanditsneighbors.Zhouetal.[37]usestheaverageactionofneighboragents
asalabel,andtrainedameanfieldpredictionnetworktoreplacetheaverageaction.Subramanianetal.[23]proposedtwomulti-
agentmeanfieldreinforcementlearningalgorithmsbasedonpartiallyobservablesettings:POMFQ(FOR)andPOMFQ(PDO),
extracting partial samples from Dirichlet or Gamma distribution to estimate partial observable mean action. Although these
methods achieve good results, they do not fully consider the feature information of surrounding neighbors.
GraphNeuralNetworks(GNNs)areabletominegraphstructuresfromdataforlearning.Inmulti-agentreinforcementlearn-
ing, GNNs can be used to model interactions between agents. In recent work, graph attention mechanisms have been used for
multi-agentreinforcementlearning.Zhangetal.[32]integratedtheimportanceoftheinformationofsurroundingagentsbased
onthemulti-headattentionmechanism,effectivelyintegratethekeyinformationofthegraphtorepresenttheenvironmentand
improvethecooperationstrategyofagentswiththehelpofmulti-agentreinforcementlearning.DCG[2]decomposedthejoint
value function of all agents into gains between pairs of agents according to the coordination graph, which can flexibly balance
the performance and generalization ability of agents. Li et al. [13] proposed a deep implicit coordination graph (DICG) struc-
ture that can adapt to dynamic environments and learn implicit reasoning about joint actions or values through graph neural
networks.Ruanetal.[20]proposedagraph-basedcoordinationstrategy,whichdecomposesthejointteamstrategyintoagraph
generatorandagraph-basedcoordinationstrategytorealizethecoordinationbehaviorbetweenagents.MAGIC[17]moreaccu-
rately represented the interactions between agents during communication by modifying the standard graph attention network
and compatible with differentiable directed graphs.
In the dynamic MARL system where competition and confrontation coexist, it is very difficult to directly apply the graph
neural network, because the agent will die, the graph structure of the constructed large-scale agent system has the problem of
largespatialdimension.However,graphneuralnetworkscanbetterminetherelationshipbetweenfeatures,andtheintroduction
of mean-field theory can further improve the advantages of mean-field multi-agent reinforcement learning.
Our approach differs from related work above in that it uses a graph attention mechanism to select surrounding agents that
are more important to the central agent in a partially observable environment. GAMFQ uses a graph attention module and
a mean field module to describe how an agent is influenced by the actions of other agents at each time step, where graph
attentionconsistsofagraphattentionencoderandadifferentiableattentionmechanism,andfinallyoutputsadynamicgraphto
representtheeffectivenessoftheneighborhoodagenttothecentralagent.Themeanfieldmoduleapproximatestheinfluenceof
aneighborhoodagentonacentralagentastheaverageinfluenceoftheeffectiveneighborhoodagents.Usingthesetwomodules
togetherisabletoefficientlyestimatetheaverageactionofsurroundingagentsinpartiallyobservablesituations.GAMFQdoes
not require global information about the environment to learn decentralized agent policies from the environment.4
3MOTIVATION & PRELIMINARIES
In this section, we represent discrete-time non-cooperative multi-agent task modeling as a stochastic game (SG). SG can be
defined as a tuple < ğ‘†,ğ´1,â€¦,ğ´ğ‘,ğ‘Ÿ1,â€¦,ğ‘Ÿğ‘,ğ‘,ğ›¾ >, whereğ‘†represents the true state of the environment. Each agent ğ‘—âˆˆ
{1,â€¦,ğ‘}chooses an action at each time step ğ‘ğ‘—âˆˆğ´ğ‘—. The reward function for agent ğ‘—isğ‘Ÿğ‘—âˆ¶ğ‘†Ã—ğ´1Ã—â‹¯Ã—ğ´ğ‘â†’ğ‘….
State transitions are dynamically represented as ğ‘âˆ¶ğ‘†Ã—ğ´1Ã—â‹¯Ã—ğ´ğ‘â†’Î©(ğ‘†).ğ›¾is a constant representing the discount
factor. It represents a stable state, and in this stable state, all agents will not deviate from the best strategy given to others. The
disadvantageisthatitcannotbeappliedtothecoexistenceofmultipleagents.Yangetal.[31]introducedmeanfieldtheory,which
approximates the interaction of many agents as the interaction between the average effect of a central agent and neighboring
agents, and solves the scalability problem of SG.
The Nash equilibrium of general and random games can be defined as a strategy tuple(ğœ‹1
âˆ—,â‹¯,ğœ‹ğ‘
âˆ—), for allğ‘ âˆˆğ‘†and
âˆ€ğœ‹ğ‘–âˆˆÎ ğ‘–,thereisğ‘£ğ‘—(ğ‘ ,ğœ‹1
âˆ—,â‹¯,ğœ‹ğ‘–
âˆ—,â‹¯,ğœ‹ğ‘
âˆ—)â‰¥ğ‘£ğ‘—(ğ‘ ,ğœ‹1
âˆ—,â‹¯,ğœ‹ğ‘–,â‹¯,ğœ‹ğ‘
âˆ—).Thisshowsthatwhenallotheragentsareimplementing
theirequilibriumstrategy,nooneagentwilldeviatefromthisequilibriumstrategyandreceiveastrictlyhigherreward.Whenall
agentsfollowtheNashequilibriumstrategy,theNashQ-functionofagent ğ‘—isğ‘„ğ‘—
âˆ—(ğ‘ ,ğ‘).Partiallyobservablestochasticgamescan
generate a partially observable Markov decision process (POMDP), we review the partially observable Markov decision (Dec-
POMDP) in Section 3.1 and analyze the partially observable model from a theoretical perspective. Section 3.2 first introduces
thegloballyobservablemean-fieldmulti-agentreinforcement learning,andthenintroducesthepartiallyobservablemean-field
reinforcementlearningalgorithm(POMFQ)basedonthePOMDPframework,andanalyzestheexistingpartoftheobservable
indetail.Thelimitationofmean-fieldreinforcementlearningPOMFQ(FOR)[23]isthatthefeatureinformationofsurrounding
neighbors is not fully considered. In a partially observable setting, each agent ğ‘—observable neighborhood agent information
ğ‘œğ‘—can be used to better mine the relationship between features through a graph attention network. Introducing graph attention
networks into partially observable mean-field multi-agent reinforcement learning can further improve their performance, and
Section 3.3 briefly introduces graph attention networks.
3.1Partially observable Markov decision process
We mainly study partially observable Markov decisions (Dec-POMDP) [3, 18, 33]. The partially observable Markov decision
process ofğ‘›agents can be represented as a tupleâŸ¨ğ‘,ğ‘†,{ğ´ğ‘–}ğ‘›
ğ‘–=1,ğ‘‡,ğ‘,ğ‘…,ğ‘‚,ğ›¾âŸ©, whereğ‘= {1,â€¦,ğ‘›}represents the set of
agents,ğ‘†representstheglobalstate, ğ´ğ‘—representsthesetofactionspacesofthe ğ‘—-thagent,ğ‘representstheobservationspace
of the agents, and the agent ğ‘—receives observation ğ‘œğ‘—âˆˆğ‘‚ğ‘—through the observation function ğ‘(ğ‘ ,ğ‘—) âˆ¶ğ‘†Ã—ğ‘â†’ğ‘‚, and the
transition function ğ‘‡âˆ¶ğ‘†Ã—ğ´1Ã—â€¦Ã—ğ´ğ‘›Ã—ğ‘†î‚¶â†’[0,1]represents the environment transitions from a state to another one. At
each time step ğ‘¡, the agentğ‘—chooses an action ğ‘ğ‘—
ğ‘¡âˆˆğ´ğ‘—, gets a reward ğ‘Ÿğ‘—
ğ‘¡âˆ¶ğ‘†Ã—ğ´ğ‘—î‚¶â†’ğ‘…w.r.t. a state and an action. ğ›¾âˆˆ[0,1]
is a reward discount factor. Agent ğ‘—has a stochastic policy ğœ‹ğ‘—conditioned on its observation ğ‘œğ‘—or action observation history
ğœğ‘—âˆˆ(ğ‘Ã—ğ´ğ‘—), and according to the all agentsâ€™s joint policy ğœ‹Î”=[ğœ‹1,â€¦,ğœ‹ğ‘], The value function of agent ğ‘—under the joint
strategyğœ‹is the value function ğ‘£ğ‘—
ğœ‹(ğ‘ )=âˆ‘âˆ
ğ‘¡=0ğ›¾ğ‘¡ğ¸ğœ‹,ğ‘[ğ‘Ÿğ‘—
ğ‘¡|ğ‘ 0=ğ‘ ]can be obtained, and then the Q-function can be formalized as
ğ‘„ğ‘—
ğœ‹(ğ‘ ,ğ‘)=ğ‘Ÿğ‘—(ğ‘ ,ğ‘)+ğ›¾ğ¸ğ‘ â€²âˆ¼ğ‘[ğ‘£ğ‘—
ğœ‹(ğ‘ â€²)]. Our work is based on the POMDP framework.
3.2Partially Observable Mean Field Reinforcement Learning
Mean-field theory-based reinforcement learning algorithm [31] approximates interactions among multiple agents as two-agent
interactions, where the second agent corresponds to the average effect of all other agents. Yang et al. [31] decomposes the
multi-agent Q-function into pairwise interacting local Q-functions as follows:
ğ‘„ğ‘—
ğœ‹(ğ‘ ,ğ‘)=1
ğ‘ğ‘—âˆ‘
ğ‘˜âˆˆğ‘(ğ‘—)ğ‘„ğ‘—
ğœ‹(ğ‘ ,ğ‘ğ‘—,ğ‘ğ‘˜)(1)
whereğ‘ğ‘—is the index set of the neighbors of the agent ğ‘—andğ‘ğ‘—represents the discrete action of the agent ğ‘—and is represented
by one-shot coding. Mean field Q-function is cyclically updated according to Eq.2-5:
ğ‘„ğ‘—
ğœ‹(ğ‘ ğ‘¡,ğ‘ğ‘—
ğ‘¡, Ì„ ğ‘ğ‘—
ğ‘¡)=(1âˆ’ğ›¼)ğ‘„ğ‘—
ğœ‹(ğ‘ ğ‘¡,ğ‘ğ‘—
ğ‘¡, Ì„ ğ‘ğ‘—
ğ‘¡)+ğ›¼[ğ‘Ÿğ‘—
ğ‘¡+ğ›¾ğ‘£ğ‘—(ğ‘ ğ‘¡+1)](2)5
where
ğ‘£ğ‘—(ğ‘ ğ‘¡+1)=âˆ‘
ğ‘ğ‘—
ğ‘¡+1ğœ‹ğ‘—(ğ‘ğ‘—
ğ‘¡+1âˆ£ğ‘ ğ‘¡+1,Ìƒ ğ‘ğ‘—
ğ‘¡)ğ‘„ğ‘—
ğœ‹(ğ‘ ğ‘¡+1,ğ‘ğ‘—
ğ‘¡+1,Ìƒ ğ‘ğ‘—
ğ‘¡)(3)
Ì„ ğ‘ğ‘—
ğ‘¡=1
ğ‘âˆ‘
ğ‘˜â‰ ğ‘—ğ‘ğ‘˜
ğ‘¡,ğ‘ğ‘˜
ğ‘¡âˆ¼ğœ‹ğ‘˜(â‹…âˆ£ğ‘ ğ‘¡, Ì„ ğ‘ğ‘˜
ğ‘¡âˆ’1)(4)
ğœ‹ğ‘—(ğ‘ğ‘—
ğ‘¡âˆ£ğ‘ ğ‘¡, Ì„ ğ‘ğ‘—
ğ‘¡âˆ’1)=exp(âˆ’ğ›½ğ‘„ğ‘—
ğœ‹(ğ‘ ğ‘¡,ğ‘ğ‘—
ğ‘¡, Ì„ ğ‘ğ‘—
ğ‘¡âˆ’1))
âˆ‘
ğ‘ğ‘—â€²
ğ‘¡âˆˆğ´ğ‘—exp(
âˆ’ğ›½ğ‘„ğ‘—
ğœ‹(
ğ‘ ğ‘¡,ğ‘ğ‘—â€²
ğ‘¡, Ì„ ğ‘ğ‘—
ğ‘¡âˆ’1)) (5)
whereÌ„ ğ‘ğ‘—
ğ‘¡isthemeanactionoftheneighborhoodagent, ğ‘Ÿğ‘—
ğ‘¡istherewardforagent ğ‘—attimestepğ‘¡,ğ‘£ğ‘—isthevaluefunctionofagent
ğ‘—, andğ›½is the Boltzmann parameter. Literature [31] assumes that each agent has global information, and for the central agent,
the average action of the neighboring agents is updated by Eq. 4. However, in a partially observable multi-agent environment,
the way of calculating the average action in Eq. 4 is no longer applicable.
Inthecaseofpartialobservability,Subramanianetal.[23]take ğ‘ˆsamplesfromtheDirichletdistributiontoupdatetheaverage
actionofEq.4,andachievebetterperformancethanthemeanfieldreinforcementlearningalgorithm.Theformulaisasfollows:
ğ·ğ‘—(ğœƒ)âˆğœƒğœ‚1âˆ’1+ğ‘1
1â‹¯ğœƒğœ‚ğ¿âˆ’1+ğ‘ğ¿
ğ¿;
Ìƒ ğ‘ğ‘—
ğ‘–,ğ‘¡âˆ¼ğ·ğ‘—(ğœƒ;ğœ‚+ğ‘);Ìƒ ğ‘ğ‘—
ğ‘¡=1
ğ‘ˆğ‘–=ğ‘ˆâˆ‘
ğ‘–=1Ìƒ ğ‘ğ‘—
ğ‘–,ğ‘¡(6)
whereğ¿denotes the size of the action space, ğ‘1,â€¦,ğ‘ğ¿denotes the number of occurrences of each action, ğœ‚is the Dirichlet
parameter,ğœƒistheclassificationdistribution.ButthepremiseoftheDirichletdistributionistoassumethatthecharacteristicsof
eachagentareindependenttoachievebetterclusteringbasedonthecharacteristicsofneighboringagents.Infact,inmanymulti-
agent environments, the characteristics of each agent has a certain correlation, but the Dirichlet distribution does not consider
thiscorrelation,whichmakesitunabletoaccuratelydescribethecentralagentandtheneighborhoodagents.Therewillbesome
deviations in the related information. Figure 1 shows the process of a battle between the red and green teams, in which each
agentcanobservetheinformationofthefriendlyagent,andtheactionspaceoftheagentis {ğ‘¢ğ‘,ğ‘‘ğ‘œğ‘¤ğ‘›,ğ‘™ğ‘’ğ‘“ğ‘¡,ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ }.Thecentral
agentenclosedbytheredcircleisaffectedbythesurroundingfriendlyagents.WeusetheDirichletdistributiontosimulateand
calculate the probability of the central agent moving in each direction, as shown below:
â§
âª
âª
â¨
âª
âªâ©ğ‘ğ‘¢ğ‘=0.31
ğ‘ğ‘‘ğ‘œğ‘¤ğ‘›=0.42
ğ‘ğ‘™ğ‘’ğ‘“ğ‘¡=0.14
ğ‘ğ‘Ÿğ‘–ğ‘”â„ğ‘¡=0.13(7)
It can be obtained that the probability of the agent moving down is the highest, which is essentially due to the large number
ofagentsmoving ğ‘‘ğ‘œğ‘¤ğ‘›.However,moving ğ‘¢ğ‘istheoptimalactionfortheagenttoformanencirclementtrendwithfriends.The
Dirichlet distribution results in a local optimal solution rather than finding the optimal action.
Zhangetal.[35]believesthatthecorrelationbetweentwoagentsiscrucialformulti-agentreinforcementlearning.First,the
papercalculatesthecorrelationcoefficientbetweeneachpairofagents,andthenshieldsthecommunicationamongweaklycor-
related agents, thereby reducing the dimensionality of the state-action value network in the input space. Inspired by Zhang et
al. [35], for large-scale partially observable multi-agent environments, it is more necessary to select the importance of neigh-
borhoodagents.Inourpaper,wewilladoptagraphattentionmethodtofilteroutmoreimportantneighborhoodagents,discard
unimportant agent information, and achieve more accurate estimation of the average actions of neighborhood agents.
3.3Graph Attention Network
Graph neural network [29] can better mine the graph structure form between data. Graph Attention Network (GAT) [25] is
composed of a group of graph attention layers, each graph attention layer acts on the node feature vector of node ğ‘–denoting as
ğ‘šğ‘–through a weight matrix ğ‘Š, and then uses softmax to normalize the neighbor nodes of the central node:
ğ‘’ğ‘–ğ‘—=(ğ‘Šğ‘šğ‘–â€–ğ‘Šğ‘šğ‘—) (8)6
left
0.420.31
0.130.14up
rightdown
FIGURE 1 A battle environment of the red and blue groups, where the red agent in the center is distributed by Dirichlet to
calculate the action.
ğ›¼ğ‘–ğ‘—=softmaxğ‘—(ğ‘’ğ‘–ğ‘—)=exp(ğ‘’ğ‘–ğ‘—)
âˆ‘
ğ‘˜âˆˆğ‘ğ‘—exp(ğ‘’ğ‘—ğ‘˜) (9)
whereğ‘’ğ‘–ğ‘—istheattentioncoefficientofeachnode,indicatingtheimportanceofnode ğ‘–tonodeğ‘—.Finally,theoutputfeaturesare
obtained by weighting the input features â„ğ‘–, and the update rule for each node ğ‘—is:
ğ‘’ğ‘—=ğœâ›
âœ
âœââˆ‘
ğ‘–âˆˆğ‘ğ‘—ğ›¼ğ‘–ğ‘—ğ‘Šâ„ğ‘–â
âŸ
âŸâ (10)
whereğ‘’ğ‘—represents the feature of node ğ‘—,ğ‘ğ‘—is the set of adjacent nodes of node ğ‘—, andğœ(â‹…)is a nonlinear activation function.
4APPROACH
Inthissection,weproposeanovelmethodcalledPartiallyObservableMeanFieldMulti-AgentReinforcementLearningbased
on Graphâ€“Attention (GAMFQ), which can be applied to large-scale partially observable MARL tasks, where the observation
range of each agent is limited, and the feature information of other agents in the fixed neighborhood is intelligently observed.
The overall architecture of the GAMFQ algorithm is depicted in Figure 2, including two important components: the Graph
Attention Module and the Mean Field Module: (i) In our Graphâ€“Attention Module, the information observed locally by each
agentissplicedfirstly.Thenthehigh-dimensionalfeaturerepresentationsareobtainedbyalatentspacemappingprocesswhich
followedbyaone-layerLSTMnetworktoobtainthetime-seriescorrelationofthetargetagent,andthehiddenlayeroftheLSTM
isusedastheinputofthegraphattentionmoduletoinitializetheconstructedgraphnodes.Thentoenhancetheaggregationof
neighboragentstotargetagent,asimilarprocessisimplementedasaFCmappingnetworkfollowedbyaGATlayer.Afterthat,
the final representation of agents are obtained by a MLP layer with the input of the representations of target agent and other
observableagents.Finally,weadoptlayer-normalizedmethodtoobtaintheadjacencymatrix{ğºğ‘¡}ğ‘
1viaGumbelSoftmax.(ii)
The Mean Field Module utilizes the adjacency matrix{ğºğ‘¡}ğ‘
1from Graph Attention Module to obtain adopting action from
important neighbor agents, in which the joint Q-function of each agent ğ‘—approximates the Mean-Field Q-function ğ‘„ğ‘—(ğ‘ ,ğ‘) â‰ˆ
ğ‘„ğ‘—
POMF(ğ‘ ,ğ‘ğ‘—,Ìƒ ğ‘ğ‘—)of important neighbor agents, where the Q-value is partially observable mean-field(POMF) Q-value, and Ìƒ ğ‘ğ‘—
istheaverageactionoftheimportantneighborhoodagentsthatispartiallyobservablebyagent ğ‘—.Eachcomponentisdescribed
in detail below.7
Graph -Attention
ModuleFeature Vector
Effective Feature VectorAgent1
Agentğ‘—
Agentğ‘
â€¦â€¦
â€¦
â€¦Get  ğ‘´Effective neighbors
ğºğ‘¡FC
LSTM
FCGumbel SoftmaxMLP GAT Encoder
â€¦ â€¦ğ‘’1ğ‘¡
ğ‘’ğ‘–ğ‘¡
ğ‘’ğ‘ğ‘—ğ‘¡ â€¦
ğ¸ğ‘¡
â€¦
Agent1
Agentğ‘—
Agentğ‘
â€¦ğ‘¥1ğ‘¡ğ‘¥ğ‘ğ‘—ğ‘¡ğ‘ğ‘—neighbors
â€¦ â€¦
â€¦ â€¦
â€¦ â€¦Local Observable
Get  Mean  action
Mean Field Module
â€¦
ğ‘¥ğ‘–ğ‘¡
FIGURE 2 Schematic of GAMFQ. Each agent can observe the feature information of other agents within a fixed range, input
itintotheGraphâ€“AttentionModule,andoutputanadjacencymatrixtorepresenttheeffectivenessoftheneighborhoodagentto
the central agent.
4.1Graphâ€“Attention Module
To more accurately re-determine the influence of agent ğ‘—â€™s neighborğ‘ğ‘—on itself, we need to be able to extract useful infor-
mation from the local observations of agent ğ‘—. The local observations of each agent include the embedding information of
neighboring agents. For each agent ğ‘—and each time step ğ‘¡, the information of a local observation of length ğ¿ğ‘—, is expressed as
ğ‘œğ‘¡
ğ‘—=(
ğ‘¥ğ‘¡
1,ğ‘¥ğ‘¡
2,â‹¯,ğ‘¥ğ‘¡
ğ‘ğ‘—)
, whereğ‘¥ğ‘¡
ğ‘ğ‘—represents the feature of the ğ‘ğ‘—-th neighbor agent of agent ğ‘—, andğ‘œğ‘¡
ğ‘—âˆˆğ‘…ğ‘ğ‘—Ã—ğ·,ğ‘¥ğ‘¡
ğ‘–âˆˆğ‘…1Ã—ğ·.
ğ¿ğ‘—is concatenated from the embedding features of each neighbor. Our goal is to learn an adjacency matrix{ğºğ‘¡}ğ‘
1to extract
moreimportantembeddinginformationfortheagent ğ‘—fromlocalobservationsateachtimestep ğ‘¡.Sincegraphneuralnetworks
can better mine the information of neighbor nodes, we propose a graph attention structure suitable for large-scale multi-agent
systems. This structure focuses on information from different agents by associating weights to observations based on the rel-
ative importance of other agents in their local observations. The Graphâ€“Attention structure is constructed by concatenating a
graphattentionencoderandadifferentiableattentionmechanism.Forthelocalobservation ğ‘œğ‘¡
ğ‘—ofagentğ‘—attimestepğ‘¡,ğ‘œğ‘¡â€²
ğ‘—isfirst
encoded using a fully connected layer (FC) , and is passed to the LSTM layerin order to generate the hidden state â„ğ‘¡
ğ‘—and cell
stateğ‘ğ‘¡
ğ‘—of agentğ‘—, whereâ„ğ‘¡
ğ‘—serves as the input of the graph attention module to initialize the constructed graph nodes:
â„ğ‘¡
ğ‘—,ğ‘ğ‘¡
ğ‘—=ğ¿ğ‘†ğ‘‡ğ‘€(
ğ‘’(
ğ‘œğ‘¡
ğ‘—)
,â„ğ‘¡
ğ‘—,ğ‘ğ‘¡
ğ‘—)
(11)
whereğ‘’(â‹…)is a fully connected layer representing the observed encoder. â„ğ‘¡
ğ‘—is encoded as a message:
ğ‘šğ‘¡
ğ‘—=ğ‘’(
â„ğ‘¡
ğ‘—)
(12)
whereğ‘šğ‘¡
ğ‘—is the aggregated information of the neighborhood agents observed by agent ğ‘—at time step ğ‘¡. The input encoding
information ğ‘€ğ‘¡is passed to the GAT encoder and hard attention mechanism, where the hard attention mechanism consists
of MLP and Gumbel Softmax function. Finally, the output adjacency matrix{ğºğ‘¡}ğ‘
1is used to determine which agents in the
neighborhoodhaveaninfluenceonthecurrentagent.TheGATencoderhelpstoefficientlyencodetheagentâ€™slocalinformation,
which is expressed as:
{ğ‘€ğ‘¡}ğ‘
1=ğ‘“ğ‘†ğ‘â„ğ‘’ğ‘‘(ğ‘šğ‘¡
1,â‹¯,ğ‘šğ‘¡
ğ‘)(13)
Additionally, we take the form of the same attention mechanism as GAT [25], expressed as:
ğ›¼ğ‘†
ğ‘–ğ‘—=exp(
ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦Reğ¿ğ‘ˆ(
ğ‘ğ‘‡
ğ‘†[
ğ‘Šğ‘†ğ‘šğ‘¡
ğ‘–||ğ‘Šğ‘†ğ‘šğ‘¡
ğ‘—]))
âˆ‘
ğ‘˜âˆˆğ‘ğ‘¡
ğ‘—âˆª{ğ‘—}exp(
ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦Reğ¿ğ‘ˆ(
ğ‘ğ‘‡
ğ‘†[
ğ‘Šğ‘†ğ‘šğ‘¡
ğ‘—||ğ‘Šğ‘†ğ‘šğ‘¡
ğ‘˜])) (14)8
whereğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ğ‘…ğ‘’ğ¿ğ‘ˆ (â‹…)is the activation function, ğ‘ğ‘†âˆˆğ‘…ğ·is the weight vector, ğ‘ğ‘¡
ğ‘—âˆª{ğ‘—}represents the central agent ğ‘—and its
observable neighborhood agent set, and ğ‘Šğ‘†âˆˆğ‘…ğ·Ã—ğ·is the weight matrix. The node feature of agent ğ‘—is expressed as:
ğ‘’ğ‘¡
ğ‘—=ğ¸ğ¿ğ‘ˆâ›
âœ
âœââˆ‘
ğ‘–âˆˆğ‘ğ‘¡
ğ‘—âˆªğ‘—ğ›¼ğ‘†
ğ‘–ğ‘—ğ‘Šğ‘†ğ‘šğ‘¡
ğ‘–â
âŸ
âŸâ (15)
whereğ¸ğ¿ğ‘ˆ(â‹…)isanexponentiallinearunitfunction.Connectingthefeaturesofeachnodeinpairs: ğ¸ğ‘¡
ğ‘–,ğ‘—=(
ğ‘’ğ‘¡
ğ‘–||ğ‘’ğ‘¡
ğ‘—)
,wecangeta
matrixğ¸ğ‘¡âˆˆğ‘…ğ‘Ã—ğ‘ğ‘—Ã—2ğ·,whereğ¸ğ‘¡
ğ‘–,ğ‘—representstherelevantfeaturesofagent ğ‘—.Takingğ¸ğ‘¡astheinputofMLPwhichisfollowed
by a Gumbel Softmax function, the connected vector ğºğ‘¡
ğ‘—can be obtained. The connected vector ğºğ‘¡
ğ‘—consists of elements ğ‘”ğ‘–ğ‘—,
whereğ‘–representstheneighborsofthecentralagent ğ‘—.Theelement ğ‘”ğ‘¡
ğ‘–ğ‘—=1intheadjacencymatrixindicatesthattheactionof
the agentğ‘–will have an impact on the agent ğ‘—. Conversely, ğ‘”ğ‘¡
ğ‘–ğ‘—=0means that the agentâ€™s actions have no effect on the agent ğ‘—.
4.2Mean Field Module
This Graph-Attention method selects important ğ‘€ğ‘—agents from the neighbors ğ‘ğ‘—of agentğ‘—, and compute the average of the
actions of the choosed neighbor agents:
Ìƒ ğ‘ğ‘—
ğ‘¡=1
ğ‘€ğ‘—âˆ‘
ğ‘˜âˆˆğ‘ğ‘—ğ‘ğ‘˜
ğ‘¡â‹…ğºğ‘¡
ğ‘—, ğ‘ğ‘˜
ğ‘¡âˆ¼ğœ‹ğ‘˜(â‹…âˆ£ğ‘ ğ‘¡,Ìƒ ğ‘ğ‘˜
ğ‘¡)(16)
whereâ‹…is the element-wise multiplication.
In the above formula, ğ‘ğ‘˜represents the important neighborhood agent for agent ğ‘—. Then the Qâ€“value of each agent is shown
in Eq. 17. Note that the Qâ€“value here is a partially observable Qâ€“value.
ğ‘„ğ‘—
GAMF(ğ‘ ğ‘—
ğ‘¡,ğ‘ğ‘—
ğ‘¡,Ìƒ ğ‘ğ‘—
ğ‘¡)=(1âˆ’ğ›¼)ğ‘„ğ‘—
GAMF(ğ‘ ğ‘—
ğ‘¡,ğ‘ğ‘—
ğ‘¡,Ìƒ ğ‘ğ‘—
ğ‘¡)+ğ›¼[ğ‘Ÿğ‘—
ğ‘¡+ğ›¾ğ‘£(ğ‘ ğ‘—
ğ‘¡+1)](17)
where the value function ğ‘£ğ‘—is expressed as
ğ‘£ğ‘—(ğ‘ ğ‘—
ğ‘¡+1)=âˆ‘
ğ‘ğ‘—
ğ‘¡+1ğœ‹ğ‘—(ğ‘ğ‘—
ğ‘¡+1âˆ£ğ‘ ğ‘—
ğ‘¡+1,Ìƒ ğ‘ğ‘—
ğ‘¡)ğ‘„ğ‘—
GAMF(ğ‘ ğ‘—
ğ‘¡+1,ğ‘ğ‘—
ğ‘¡+1,Ìƒ ğ‘ğ‘—
ğ‘¡)(18)
According to the above graph attention mechanism, more important neighborhood agents are obtained. The new average
actionÌƒ ğ‘ğ‘—
ğ‘¡is calculated by Eq.16, and then the strategy ğœ‹ğ‘—
ğ‘¡of agentğ‘—is updated by the following formula:
ğœ‹ğ‘—(ğ‘ğ‘—
ğ‘¡âˆ£ğ‘ ğ‘—
ğ‘¡,Ìƒ ğ‘ğ‘—
ğ‘¡âˆ’1)=exp(âˆ’ğ›½ğ‘„ğ‘—
GAMF(ğ‘ ğ‘—
ğ‘¡,ğ‘ğ‘—
ğ‘¡,Ìƒ ğ‘ğ‘—
ğ‘¡âˆ’1))
âˆ‘
ğ‘ğ‘—â€²
ğ‘¡âˆˆğ´ğ‘—exp(
âˆ’ğ›½ğ‘„ğ‘—
GAMF(
ğ‘ ğ‘—
ğ‘¡,ğ‘ğ‘—â€²
ğ‘¡,Ìƒ ğ‘ğ‘—
ğ‘¡âˆ’1)) (19)
4.3Theoretical Proof
This subsection is devoted to proving that the setting of GAMFQ is close to the Nash equilibrium. Subramanian et al. [23]
showed that in partially observable cases, the fixed observation radius (FOR) setting is close to a Nash equilibrium, where the
mean action of each agentâ€™s neighborhood agents is approximated by a dirichlet distribution. First, we state some assumptions,
which are the same as literature[23], and are followed by all the theorems and analyses below.
Assumption 1. For anyğ‘–andğ‘—, there islimğ‘¡â†’âˆğœğ‘–
ğ‘—(ğ‘¡)=âˆ.ğ‘¤.ğ‘.1.
This assumption guarantees a probability of 1 that old information is eventually discarded.
Assumption2. Supposesomemeasurabilityconditionsareasfollow:(1) ğ‘¥(0)isîˆ²(0)-measurable.(2)Foreach ğ‘–,ğ‘—andğ‘¡,ğ‘¤ğ‘–(ğ‘¡)
isîˆ²(ğ‘¡+1)-measurable.(3)Foreach ğ‘–,ğ‘—andğ‘¡,ğ›¼ğ‘–(ğ‘¡)andğœğ‘–
ğ‘—(ğ‘¡)areîˆ²(ğ‘¡)-measurable.(4)Foreach ğ‘–andğ‘¡,satisfyB[ğ‘¤ğ‘–(ğ‘¡)|îˆ²(ğ‘¡)]=0.
(5)B[ğ‘¤2
ğ‘–(ğ‘¡)|îˆ²(ğ‘¡)]â‰¤ğ´+ğµmaxğ‘—maxğœâ‰¤ğ‘¡|||ğ‘¥ğ‘—(ğœ)|||2
, whereğ´andğµare deterministic constants.
Assumption 3. The learning rates satisfy 0â‰¤ğ›¼ğ‘–(ğ‘¡)<1.
Assumption 4. Suppose some conditions for the ğ¹mapping are as follows: (1) If ğ‘¥â‰¤ğ‘¦, thenğ¹(ğ‘¥)â‰¤ğ¹(ğ‘¦), that is,ğ¹is
monotonic; (2) ğ¹is continuous; (3) When ğ‘¡â†’âˆ,ğ¹is limited to the interval [ğ‘¥âˆ—âˆ’ğ·,ğ‘¥âˆ—+ğ·], whereğ‘¥âˆ—is some arbitrary9
point;(4)Ifğ‘’âˆˆîˆ¾ğ‘›isavectorthatsatisfiesallcomponentsequalto1,then ğ¹(ğ‘¥)âˆ’ğ‘ğ‘’â‰¤ğ¹(ğ‘¥+ğ‘ğ‘’)â‰¤ğ¹(ğ‘¥+ğ‘ğ‘’)+ğ‘ğ‘’,whereğ‘
is a positive scalar.
Assumption 5. Each action-value pair can be accessed indefinitely, and the reward is limited.
Assumption 6. Under the limit ğ‘¡â†’âˆof infinite exploration, the agentâ€™s policy is greedy.
This assumption ensures that the agent is rational.
Assumption 7. In each stage of a stochastic game, a Nash equilibrium can be regarded as a global optimum or saddle point.
Based on these assumptions, Subramanian et al. [23] give the following lemma.
Lemma 1. [23] When the Q-function is updated using the partially observable update rule in Eq.2, and assumptions 3, 5, and
7 hold, the following holds for ğ‘¡â†’âˆ:
|ğ‘„âˆ—(ğ‘ ğ‘¡,ğ‘ğ‘¡)âˆ’ğ‘„ğ‘ƒğ‘‚ğ‘€ğ¹(ğ‘ ğ‘¡,ğ‘ğ‘¡,Ìƒ ğ‘ğ‘¡)|â‰¤2ğ· (20)
whereğ‘„âˆ—istheNashQ-value, ğ‘„ğ‘ƒğ‘‚ğ‘€ğ¹isthepartiallyobservablemean-fieldQ-function,and ğ·istheboundofthe ğ¹map.The
probability that the above formula holds is at least ğ›¿ğ¿âˆ’1, whereğ¿=|ğ´|.
InourGAMFQsetting,forpartiallyobservableneighborhoodagents,wechoosetoselectalimitednumberofimportantagents
by using graph attention, and then update the POMF Q function. The following theorem proves that the setting of GAMFQ is
close to Nash equilibrium.
Theorem1. ThedistancebetweentheMFQ(globallyobservable)meanaction Ì„ ğ‘andtheGAMFQ(partiallyobservable)mean
actionÌƒ ğ‘satisfies the following formula:
|||Ìƒ ğ‘ğ‘—
ğ‘¡âˆ’Ì„ ğ‘ğ‘—
ğ‘¡|||â‰¤âˆš
1
2ğ‘ğ‘—log2
ğ›¿(21)
Whenğ‘¡â†’âˆ, the probability >=ğ›¿, whereğ‘ğ‘—is the number of observed neighbor agents, Ìƒ ğ‘is the partially observable mean
action obtained by graph attention in Eq. 16, Ì„ ğ‘is the globally observable mean action in Eq. 4.
Assumingthateachagentisgloballyobservable,themeanofimportantagentsselectedbygraphattentionisclosetothetrue
underlying global observable Ì„ ğ‘. Since the GAMF Q-function is updated by taking finite samples through graph attention, the
empirical mean is Ìƒ ğ‘.
Theorem2. IftheQ-functionisLipschitzcontinuouswithrespecttothemeanaction,i.e. ğ‘€isconstant,thentheMFQ-function
ğ‘„ğ‘€ğ¹and GAMF Q-function ğ‘„ğºğ´ğ‘€ğ¹satisfy the following relation:
|||ğ‘„ğºğ´ğ‘€ğ¹(ğ‘ ğ‘¡,ğ‘ğ‘¡,Ìƒ ğ‘ğ‘¡âˆ’1)âˆ’ğ‘„ğ‘€ğ¹(ğ‘ ğ‘¡,ğ‘ğ‘¡, Ì„ ğ‘ğ‘¡âˆ’1)|||â‰¤ğ‘€Ã—ğ¿Ã—log2
ğ›¿Ã—1
2ğ‘ğ‘—(22)
When the limit ğ‘¡â†’âˆ, the probability is â‰¥(ğ›¿)ğ¿âˆ’1, whereğ¿=|ğ´|,ğ´is the action space of the agent.
Intheproofoftheorem2,firstconsideraQ-functionthatisLipschitzcontinuousforall Ì„ ğ‘andÌƒ ğ‘.Accordingtotheorem1,the
above formula can further deduce the result of theorem 2. The total number of components is equal to the action space ğ¿. The
boundoftheorem1isprobability >=ğ›¿,andsincethereare ğ¿randomvariables,theprobabilityoftheorem2isatleast (ğ›¿)ğ¿âˆ’1.
When the first ğ¿âˆ’1random variable is fixed, the deterministic last Ì„ ğ‘component satisfies the relationship that the sum of the
individual components is 1. Since each agentâ€™s action is represented by a one-hot encoding, the Ìƒ ğ‘â€²component of GAMFQ also
satisfies the relationship that the sum of the individual components is 1, and the component of the agentâ€™s average action does
not change due to the application of graph attention. The proof of theorem 2 ends.
Theorem 3. A stochastic process in form ğ‘¥ğ‘–(ğ‘¡+1) =ğ‘¥ğ‘–(ğ‘¡)+ğ›¼ğ‘–(ğ‘¡)(ğ¹ğ‘–(ğ‘¥ğ‘–(ğ‘¡))âˆ’ğ‘¥ğ‘–(ğ‘¡)+ğ‘¤ğ‘–(ğ‘¡))remains bounded in the range
[ğ‘¥âˆ—âˆ’2ğ·,ğ‘¥âˆ—+2ğ·]onlimitğ‘¡â†’âˆifassumptions1,2,3and4aresatisfied,andareguaranteednottodivergetoinfinity.Where
ğ·is the boundary of the ğ¹map in assumption 4(4).
This theorem can be proved in terms of Tsitsiklis[24] and by extension. The result of theorem 3 can then be used to derive
theorem 4.10
Theorem 4. When the Q-function is updated using the partially observable update rule in Eq.17, and assumptions 3, 5, and 7
hold, the following holds for ğ‘¡â†’âˆ:
|ğ‘„âˆ—(ğ‘ ğ‘¡,ğ‘ğ‘¡)âˆ’ğ‘„ğºğ´ğ‘€ğ¹(ğ‘ ğ‘¡,ğ‘ğ‘¡,Ìƒ ğ‘ğ‘¡)|â‰¤2ğ· (23)
whereğ‘„âˆ—istheNashQ-value, ğ‘„ğºğ´ğ‘€ğ¹isthepartiallyobservablemean-fieldQ-function,and ğ·istheboundofthe ğ¹map.The
probability that the above formula holds is at least ğ›¿ğ¿âˆ’1, whereğ¿=|ğ´|.
Theorem 4 shows that the GAMFQ update is very close to the Nash equilibrium at the limit ğ‘¡â†’âˆ, i.e. reaching a plateau
for stochastic policies. Therefore, the strategy of Eq.19 is approximately close to this plateau. Theorem 4 is an application of
theorem3,usingassumptions3,5and7.However,inMARL,reachingaNashequilibriumisnotoptimal,butonlyafixed-point
guarantee. Therefore, to achieve better performance, each selfish agent will still tend to pick a limited number of samples. To
balancetheoryandperformancewhenselectingagentsfromtheneighborhood,anappropriatenumberofagents(moreefficient
agents) need to be used for better multi-agent system performance. This paper uses the graph attention structure to filter out
more important proxies, which can better approximate the Nash equilibrium.
4.4Algorithm
TheimplementationofGAMFQfollowstherelatedworkofthepreviousPOMFQ[23],thedifferenceisthatthegraphattention
structureisusedtoselecttheneighborhoodagentsthataremoreimportanttothecentralagentwhenupdatingtheaverageaction.
Algorithm1givesthepseudocodeoftheGAMFQalgorithm.Itobtainseffectiveneighboragentsbycontinuouslyupdatingthe
adjacency matrix ğºğ‘¡
ğ‘—to update the agentâ€™s strategy.
Algorithm 1 Partially Observable Mean Field MARL Based on Graphâ€“Attention
Initialize the weights of Q-function ğ‘„ğœ™ğ‘—,ğ‘„ğœ™ğ‘—
âˆ’, replay buffer ğµ, GAT encoder, MLP layers and mean action Ì„ ğ‘ğ‘—for each agent
ğ‘—âˆˆ1,â€¦,ğ‘.
forğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’=1,2,â€¦,ğ¸do
forğ‘¡â‰¤ğ‘‡and not terminal do
For each agent ğ‘—, calculate the hidden state â„ğ‘¡
ğ‘—according to Eq.11, and encode â„ğ‘¡
ğ‘—as a message ğ‘šğ‘¡
ğ‘—(Eq.12).
For each agent ğ‘—, sampleğ‘ğ‘—fron policy induced by ğ‘„ğœ™ğ‘—(Eq.19).
For each agent ğ‘—, pass the encoded information ğ‘šğ‘¡
ğ‘—to the GAT encoder and hard attention mechanism to output the
adjacency matrix ğºğ‘¡
ğ‘—.
For each agent ğ‘—, calculate the new neighborhood agent mean action Ì„ ğ‘ğ‘—by Eq.16.
Receive the full state of environment ğ‘ ğ‘¡, actionğ‘=[ğ‘1,â€¦,ğ‘ğ‘], reward[ğ‘Ÿ=ğ‘Ÿ1,â€¦,ğ‘Ÿğ‘], and the next state ğ‘ â€²=[ğ‘ 1,â€¦,ğ‘ ğ‘].
Store transition âŸ¨ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€², Ì„ ğ‘âŸ©inğµ, whereÌ„ ğ‘=[Ì„ ğ‘1,â€¦, Ì„ ğ‘ğ‘]is the mean action.
end for
forğ‘—=1,â€¦,ğ‘do
Sample a minibatch of K experiences âŸ¨ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€², Ì„ ğ‘âŸ©from replay buffer ğµ.
Setğ‘¦ğ‘—=ğ‘Ÿğ‘—+ğ›¾ğ‘£ğœ™(ğ‘ â€²)according to Eq.18.
minimize the loss ğ¿(ğœ™ğ‘—)=(ğ‘¦ğ‘—âˆ’ğ‘„ğœ“ğ‘—(ğ‘ â€²,ğ‘ğ‘—, Ì„ ğ‘ğ‘—))2
to update Q network.
end for
For each agent ğ‘—, update params of target network : ğœ™ğ‘—â†ğœğœ™ğ‘—+(1âˆ’ğœ)ğœ™ğ‘—.
end for
5EXPERIMENTS
Inthissection,wedescribethreedifferenttasksbasedontheMAgentframeworkandgivesomeexperimentalsetupandtraining
details for evaluating the GAMFQ performance.11
5.1Environments and Tasks
Subramanianetal.[23]designedthreedifferentcooperative-competitivestrategiesintheMAgentframework[36]asexperimen-
talenvironments,andourexperimentsadoptthesameenvironments.Inthesethreetasks,themapsizeissetto28*28,wherethe
observationrangeofeachagentis6units.Thestatespaceistheconcatenationofthefeatureinformationofotheragentswithin
eachagentâ€™sfieldofview,includinglocation,health,andgroupinginformation.Theactionspaceincludes13moveactionsand
8 attack actions. In addition, each agent is required to handle at most 20 other agents that are closest. We will evaluate against
these three tasks:
â€¢Multibattleenvironment: Therearetwogroupsofagentsfightingeachother,eachcontaining25agents.Theagentgets
-0.005pointsforeachmove,-0.1pointsforattackinganemptyarea,200pointsforkillinganenemyagent,and0.2points
for a successful attack. Each agent is 2*2 in size, has a maximum health of 10 units, and a speed of 2 units. After the
battle, the team with the most surviving agents wins. If both teams have the same number of surviving agents, the team
with the highest reward wins. The reward for each team is the sum of the rewards for the individual agents in the team.
â€¢Battle-Gathering environment: There is a uniform distribution of food in the environment, each agent can observe the
locationofallthefood.Inadditiontoattackingtheenemytogetrewards,eachagentcanalsoeatfoodtogetrewards.Agents
get 5 points for attacking enemy agents, and the rest of the reward settings are the same as the Multibattle environment.
â€¢Predator-Preyenvironment: Thereare40predatorsand20prey,whereeachpredatorisasquaregridofsize2*2witha
maximumhealthof10unitsandaspeedof2units.Preyisa1*1squarewithamaximumhealthof2unitsandaspeedof
2.5units.Towinthegame,thepredatormustkillmoreprey,andthepreymustfindawaytoescape.Inaddition,predators
and prey have different reward functions, predators get -0.3 points for attacking space, 1 point for successfully attacking
prey, 100 points for killing prey, -1 point for attacked prey, and 0.5 points for dying. Unlike the Multibattle environment,
when the round ends for a fairer duel, if the two teams have the same number of surviving agents, it is judged as a draw.
5.2Evaluation
We consider four algorithms for the above three games: MFQ, MFAC [31], POMFQ(FOR) and GAMFQ, where MFQ and
MFAC are baselines and POMFQ(FOR) [23] is the state-of-the-art algorithm.
TheoriginalbaselinesMFQandMFACwereproposedbyYangetal.[31]basedonglobalobservability,andtheideawasto
approximatetheinfluenceoftheneighborhoodagentsonthecentralagentastheiraverageactions,therebyupdatingtheactions
oftheneighborhoodagents.WefixtheobservationradiusofeachagentinthebaselineMFQandMFACandapplyittoapartially
observableenvironment,whereneighboragentsareagentswithinafixedrange.ThePOMFQ(FOR)algorithmintroducesnoise
inthemeanactionparameterstoencourageexploration,usesBayesianinferencetoupdatetheDirichletdistribution,andsamples
100 samples from the Dirichlet distribution to estimate partially observable mean field actions. The GAMFQ algorithm judges
the effectiveness of neighborhood agents within a fixed range through the graph attention mechanism, selects more important
neighborhood agents, and updates the average action by averaging the actions of these agents.
5.3Hyperparameters
Inthethreetasks,eachalgorithmwastrainedfor2000epochsinthetrainingphase,generatingtwosetsofAandBsetsofmodels.
Inthe testphase,1000 roundsofconfrontation wereconducted,ofwhich thefirst500 roundswerethe firstgroupA ofthefirst
algorithmagainstthesecondgroupBofthesecondalgorithm,andthelast500groupsweretheopposite.Thehyperparameters
of MFQ, MFAC, POMFQ(FOR) and GAMFQ are basically the same. Table 1 lists the hyperparameters during training of the
four algorithms, and the remaining parameters can be seen in [23].
6RESULTS AND DISCUSSION
Inthissection,weevaluatetheperformanceofGMAFQinthreedifferentenvironments,includingMultibattle,Battle-Gathering,
and Predator-Prey. We benchmark against two algorithms, MFQ and MFAC, and compare with the state-of-the-art POMFQ12
TABLE 1 Hyperparameters for four algorithms training.
Parameter Value Description
ğ›¼ 10âˆ’4learning rate
ğ›½ decays linearly from 1 to 0 exploration rate
ğ›¾ 0.95 discount rate
ğµ 1024 replay buffer
â„ 64 the hidden layer size in GAT
ğ¾ 64 mini-batch
ğ‘¡ğ‘’ğ‘šğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ 0.1 the soft-max layer temperature of the actor in MFAC
(FOR). We implement our method and comparative methods on three different tasks. Note that we only used 50 agents in our
experimentsanddidnottestmoreagents,thisisbecausetheproportionofotheragentsthateachagentcanseeismoreimportant
than the absolute number.
6.1Reward
Figure 3 shows how the reward changes as the number of iterations increases during training. We plot the reward changes for
the four algorithms in different game environments during the first 1000 iterations. Since each algorithm is self-training which
resultsinalargechangeintherewardofthealgorithm,weusetheleastsquaresmethodtofittherewardchangegraph.InFigure
3, the solid black line represents the reward change graph of the GAMFQ algorithm. From Figure 3 (a), (b) and (c), it can be
seenthattherewardoftheGAMFQalgorithmcanincreaserapidly,indicatingthattheGAMFQalgorithmcanconvergerapidly
in the early stage, and the convergence performance is better than the other three algorithms.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000014/uni00000013/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034
(a) Train results of Multibattle game.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (b) Train results of Battle-Gathering game.
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000014/uni00000013/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (c) Train results of Predator-Prey game.
FIGURE 3 Train results of three games. The reward curve for each algorithm is fitted by the least squares method.
6.2Elo Calculation
We use ELO Score [11] to evaluate the performance of the two groups of agents, the advantage of which is that it takes into
account the strength gap between the opponents themselves. ELO ratings are commonly used in chess to evaluate one-on-one
situations, and this approach can similarly be extended to N-versus-N situations. For the algorithm proposed in the paper, we
record the total rewards of the two teams of agents during each algorithm confrontation, which are ğ‘…1andğ‘…2, respectively.
Then the expected win rates of the two groups of agents are:
ğ¸1=1
1+10(ğ‘…2âˆ’ğ‘…1)âˆ•400,ğ¸2=1
1+10(ğ‘…1âˆ’ğ‘…2)âˆ•400(24)13
whereğ¸1+ğ¸2= 1. By analyzing the actual and predicted winning rates of the two groups of agents, the new ELO score of
each team after the game ends can be obtained:
ğ‘…1â€²=ğ‘…1+ğ¾(ğ‘†1âˆ’ğ¸1),ğ‘…2â€²=ğ‘…2+ğ¾(ğ‘†2âˆ’ğ¸2) (25)
whereğ‘…1representstheactualwinningorlosingvalue,1meanstheteamwins,0.5meansthetwoteamsaretied,and0means
the team loses. ğ¾is represented as a floating coefficient. To create a gap between agents, we set ğ¾to 32. For each match, we
faced off 500 times and calculated the average ELO value for all matches.
TABLE 2 The ELO Score of four algorithms in Multibattle environment.
Task Algorithm1 Algorithm2 ELO Score1 ELO Score2
GAMFQ vs POMFQ(FOR)GAMFQ-1 POMFQ(FOR)-2 3579 820
GAMFQ-2 POMFQ(FOR)-1 2696 2838
GAMFQ vs MFQGAMFQ-1 MFQ-2 2098 1508
GAMFQ-2 MFQ-1 2535 1695
GAMFQ vs MFACGAMFQ-1 MFAC-2 1350 -49
GAMFQ-2 MFAC-1 -856 -78
POMFQ(FOR) vs MFQPOMFQ(FOR)-1 MFQ-2 3145 2577
POMFQ(FOR)-2 MFQ-1 2569 2857
POMFQ(FOR) vs MFACPOMFQ(FOR)-1 MFAC-2 -205 -64
POMFQ(FOR)-2 MFAC-1 826 -42
MFQ vs MFACMFQ-1 MFAC-2 -142 -49
MFQ-2 MFAC-1 610 -46
TABLE 3 The ELO Score of four algorithms in Battle-Gathering environment.
Task Algorithm1 Algorithm2 ELO Score1 ELO Score2
GAMFQ vs POMFQ(FOR)GAMFQ-1 POMFQ(FOR)-2 7770 8931
GAMFQ-2 POMFQ(FOR)-1 8293 9310
GAMFQ vs MFQGAMFQ-1 MFQ-2 6374 10870
GAMFQ-2 MFQ-1 8510 8313
GAMFQ vs MFACGAMFQ-1 MFAC-2 5525 10
GAMFQ-2 MFAC-1 10751 -31
POMFQ(FOR) vs MFQPOMFQ(FOR)-1 MFQ-2 8526 8760
POMFQ(FOR)-2 MFQ-1 8632 8227
POMFQ(FOR) vs MFACPOMFQ(FOR)-1 MFAC-2 12722 0
POMFQ(FOR)-2 MFAC-1 12171 -88
MFQ vs MFACMFQ-1 MFAC-2 12649 49
MFQ-2 MFAC-1 13788 -4814
Table 2, 3, 4 shows the ELO scores of the four algorithms on the three tasks. It can be seen from Table 2 that in Multibattle
environment, the GAMFQ algorithm has the highest ELO score of 3579, which is significantly better than the other three
algorithms.AsshowninTable3,inBattle-Gatheringenvironment,theELOscoreoftheMFQalgorithmisthehighest,andthe
ELOscoreoftheGAMFQalgorithmisaverage.Thisisbecausethecollectionenvironmentcontainsfood,andsomealgorithms
tendtoeatfoodtogetrewardsquickly,ratherthanattackingenemyagents.However,thefinalgamewinningorlosingdecision
ismadebycomparingthenumberofremainingagentsbetweenthetwoteamsofagents.AsshowninTable4,inPredator-Prey
environment, the ELO score of the GAMFQ algorithm has the highest ELO score of 860, which is significantly better than
the other three algorithms. From the experimental results in the three environments, we can summarize that ELO score of the
GAMFQ algorithm is better than other three algorithms, showing better performance.
TABLE 4 The ELO Score of four algorithms in Predator-Prey environment.
Task Algorithm1 Algorithm2 ELO Score1 ELO Score2
GAMFQ vs POMFQ(FOR)GAMFQ-1 POMFQ(FOR)-2 421 -32
GAMFQ-2 POMFQ(FOR)-1 16 7
GAMFQ vs MFQGAMFQ-1 MFQ-2 714 -27
GAMFQ-2 MFQ-1 -15 -94
GAMFQ vs MFACGAMFQ-1 MFAC-2 860 -28
GAMFQ-2 MFAC-1 16 16
POMFQ(FOR) vs MFQPOMFQ(FOR)-1 MFQ-2 66 18
POMFQ(FOR)-2 MFQ-1 13 24
POMFQ(FOR) vs MFACPOMFQ(FOR)-1 MFAC-2 16 -16
POMFQ(FOR)-2 MFAC-1 47 16
MFQ vs MFACMFQ-1 MFAC-2 16 -16
MFQ-2 MFAC-1 174 17
6.3Results
Figure 4 shows the face-off results of the four algorithms in the three tasks. Figure 4(a) shows the faceoff results of Multibattle
game. The different colored bars for each algorithm represent the results of an algorithm versus others. We do not conduct
adversarialexperimentsbetweenthesamealgorithmsbecauseweconsiderthattheadversarialpropertiesofthesamealgorithms
are equal. The vertical lines in the bar graph represent the standard deviation of wins for groups A and B over 1,000 face-offs.
Figure 4(a) shows GAMFQ against three other algorithms, all with a win rate above 0.7.
Figure4(b)showsthefaceoffresultsofBattle-Gatheringgame.Inadditiontogettingrewardsforkillingenemies,agentscan
alsogetrewardsfromfood.ItcanbeseenthatMFQlosestoallotheralgorithms,MFACandPOMFQ(FOR)performingeneral,
and our GAMFQ is clearly ahead of other algorithms.
Figure4(c)showsthwefaceoffresultsofPredator-Preygame.Thestandarddeviationofthisgameissignificantlyhigherthan
theprevioustwogames,duetothefactthatbothgroupsAandBaretryingtobeateachotherintheenvironment.Itcanbeseen
that the GAMFQ algorithm is significantly better than other three algorithms, reaching a winning rate of 1.0.
Experimentsintheabovethreemulti-agentcombatenvironmentsshowthatGAMFQcanshowgoodperformanceoverMFQ,
MFAC and POMFQ(FOR).15
/uni00000030/uni00000029/uni00000034 /uni00000030/uni00000029/uni00000024/uni00000026 /uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c /uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000044/uni00000050/uni00000048/uni00000056/uni00000003/uni0000005a/uni00000052/uni00000051/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034
(a) Faceoff results of Multibattle game.
/uni00000030/uni00000029/uni00000034 /uni00000030/uni00000029/uni00000024/uni00000026 /uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c /uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000044/uni00000050/uni00000048/uni00000056/uni00000003/uni0000005a/uni00000052/uni00000051/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (b) Faceoff results of Battle-Gathering game.
/uni00000030/uni00000029/uni00000034 /uni00000030/uni00000029/uni00000024/uni00000026 /uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c /uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000044/uni00000050/uni00000048/uni00000056/uni00000003/uni0000005a/uni00000052/uni00000051/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000034
/uni0000000d/uni00000003/uni00000030/uni00000029/uni00000024/uni00000026/uni0000000d/uni00000003/uni00000033/uni00000032/uni00000030/uni00000029/uni00000034/uni0000000b/uni00000029/uni00000032/uni00000035/uni0000000c
/uni0000000d/uni00000003/uni0000002a/uni00000024/uni00000030/uni00000029/uni00000034 (c) Faceoff results of Predator-Prey game.
FIGURE 4 Faceoff results of three games. The * in the legend indicates the enemy. For example, the first blue bar in the bar
graph corresponding to the GAMFQ algorithm is the result of the confrontation between GAMFQ and MFQ, and we do not
conduct confrontation experiments between the same algorithms.
6.4Visualization
(a) Multibattle begin (b) Multibattle process (c)Multibattle end cooperative 
attackattackescape
GAMFQ   alive ï¼š22 POMFQ(FOR)    alive ï¼š2
FIGURE 5 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Multibattle game.
(a) Battle -Gathering begin (b) Battle -Gathering process (c)Battle -Gathering end 
Besiege 
beginBesiege 
end
food GAMFQ   alive ï¼š24 POMFQ( FOR)    alive ï¼š9
FIGURE 6 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Battle-Gathering game.16
(a) Predator -Prey begin (b1) Predator -Prey process (b2) Predator -Prey process 
GAMFQ   alive ï¼š25
POMFQ(FOR)    alive ï¼š21POMFQ(FOR)    alive ï¼š25
GAMFQ   alive ï¼š25encircleescape
FIGURE 7 Visualization of the standoff between GAMFQ and POMFQ (FOR) in a Predator-Prey game.
TovisualizetheeffectivenessoftheGAMFQalgorithm,wevisualizetheconfrontationbetweenGAMFQandPOMFQ(FOR)
in a Multibattle environment, as shown in Figure 5, where the red side is GMAFQ and the blue side is POMFQ (FOR). It can
beseenfromtheconfrontationprocessthatfortheGAMFQalgorithm,whenanagentdecidestoattack,thesurroundingagents
will also decide to attack under its influence, forming a good cooperation mechanism. On the contrary, for the POMFQ (FOR)
algorithm, some blue-side agents are chosen to attack, some are chosen to escape, and no common fighting mechanism was
formed.Similarly,intheBattle-GatheringenvironmentofFigure6,GAMFQcanlearnthesurroundingmechanismwell.Inthe
Predator-PreyenvironmentofFigure7,whenGAMFQactsasapredator,thetechniqueofsurroundingthepreyPOMFQ(FOR)
can be learned. On the contrary, when POMFQ (FOR) acted as a predator, it failed to catch the prey GMAFQ.
6.5Ablation study
/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000010/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000035/uni00000020/uni00000015
/uni00000035/uni00000020/uni00000017/uni00000035/uni00000020/uni00000019
/uni00000035/uni00000020/uni0000001b/uni00000035/uni00000020/uni00000014/uni00000013
FIGURE 8 Ablation study. R represents the observation radius of the agent.
Figure8isanablationstudythatinvestigatestheperformanceoftheGAMFQalgorithmfordifferentobservationradiusina
Multibattleenvironment.wherethesolidlinerepresentstheleastsquaresfitoftherewardchange.Itcanbeseenfromthefigure
that when the number of training is small, the performance of the algorithm is better as the observation distance increases. But
with the increase of training times, when R=4, the performance of the algorithm is the best, so the appropriate observation
distancecanachievebetterperformance.Whatismoreimportantinthispaperistoexploretheeffectoftheratioofobservable
distance to the number of agents on the performance of the algorithm, so there is no experiment with more agents.17
7CONCLUSION
In this paper, we proposed a new multi-agent reinforcement learning algorithm, Graph Attention-based Partially Observable
MeanReinforcementLearning(GAMFQ),toaddresstheproblemoflarge-scalepartiallyobservablemulti-agentenvironments.
Although existing methods are close to Nash equilibrium, they do not take into account the direct correlation of agents. Based
onthecorrelationbetweenagents,GAMFQusesaGraph-Attentionmoduletodescribehoweachagentisaffectedbytheactions
of other agents at each time step. Experimental results on three challenging tasks in the MAgents framework illustrate that,
our proposed method outperforms baselines in all these games and outperforms the state-of-the-art partially observable mean-
field reinforcement learning algorithms. In the future, we will further explore the correlation between agents to extend to more
common cooperation scenarios.
Conflict of interest
The authors declare no potential conflict of interests.
Article Description
The expanded version of this article is published in Drones 2023, 7(7), 476, with a DOI of
https://doi.org/10.3390/drones7070476.
References
[1] A. T. Azar et al., Drone deep reinforcement learning: A review , Electronics 10(2021), no. 9, 999.
[2] W.Boehmer,V.Kurin,andS.Whiteson, Deepcoordinationgraphs ,Proceedingsofthe37thInternationalConferenceon
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event ,Proceedings of Machine Learning Research , vol. 119,
PMLR, 980â€“991.
[3] Q. Cai, Z. Yang, and Z. Wang, Reinforcement learning from partial observation: Linear function approximation with
provable sample efficiency ,International Conference on Machine Learning , PMLR, 2485â€“2522.
[4] J. Fan et al., A theoretical analysis of deep q-learning ,Learning for Dynamics and Control , PMLR, 486â€“489.
[5] B.Fangetal., Large-scalemulti-agentreinforcementlearningbasedonweightedmeanfield ,CognitiveSystemsandSignal
Processing-5thInternationalConference,ICCSIP2020,Zhuhai,China,December25-27,2020,RevisedSelectedPapers ,
Communications in Computer and Information Science , vol. 1397, Springer, 309â€“316.
[6] H. Gu et al., Mean-field multi-agent reinforcement learning: A decentralized network approach , arXiv preprint
arXiv:2108.02731 (2021).
[7] J. Gu et al., A metaverse-based teaching building evacuation training system with deep reinforcement learning , IEEE
Transactions on Systems, Man, and Cybernetics: Systems (2023).
[8] Q.Hao, Verylargescalemulti-agentreinforcementlearningwithgraphattentionmeanfield ,https://openreview.net/forum?
id=MdiVU9lMmVS (2023).
[9] K. He, P. Doshi, and B. Banerjee, Reinforcement learning in many-agent settings under partial observability ,The 38th
Conference on Uncertainty in Artificial Intelligence .
[10] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, A survey and critique of multiagent deep reinforcement learning ,
Autonomous Agents and Multi-Agent Systems 6(2019), no. 33, 750â€“797.
[11] M.Jaderbergetal., Human-levelperformanceinfirst-personmultiplayergameswithpopulation-baseddeepreinforcement
learning, ArXivabs/1807.01281 (2018).18
[12] M. LauriÃ¨re et al., Learning mean field games: A survey , arXiv preprint arXiv:2205.12944 (2022).
[13] S. Li et al., Deep implicit coordination graphs for multi-agent reinforcement learning ,AAMAS â€™21: 20th International
Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021 , ACM, 764â€“
772.
[14] M.L.Littman, Markovgamesasaframeworkformulti-agentreinforcementlearning ,Machinelearningproceedings1994 ,
Elsevier, 1994. 157â€“163.
[15] R. Lowe et al., Multi-agent actor-critic for mixed cooperative-competitive environments ,Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA , 6379â€“6390.
[16] A. Mahajan et al., Maven: Multi-agent variational exploration , Advances in Neural Information Processing Systems 32
(2019), 7611â€“7622.
[17] Y. Niu, R. R. Paleja, and M. C. Gombolay, Multi-agent graph-attention communication and teaming ,AAMAS â€™21: 20th
InternationalConferenceonAutonomousAgentsandMultiagentSystems,VirtualEvent,UnitedKingdom,May3-7,2021 ,
ACM, 964â€“973.
[18] F. A. Oliehoek and C. Amato, A concise introduction to decentralized POMDPs , Springer, 2016.
[19] T.Rashidetal., QMIX:monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning ,Proceedingsof
the35thInternationalConferenceonMachineLearning,ICML2018,StockholmsmÃ¤ssan,Stockholm,Sweden,July10-15,
2018,Proceedings of Machine Learning Research , vol. 80, PMLR, 4292â€“4301.
[20] J. Ruan et al., Gcs: Graph-based coordination strategy for multi-agent reinforcement learning , arXiv preprint
arXiv:2201.06257 (2022).
[21] L. M. Schmidt et al., An introduction to multi-agent reinforcement learning and review of its application to autonomous
mobility, arXiv preprint arXiv:2203.07676 (2022).
[22] H. Shi et al., Marl sim2real transfer: Merging physical reality with digital virtuality in metaverse , IEEE Transactions on
Systems, Man, and Cybernetics: Systems (2022).
[23] S. G. Subramanian et al., Partially observable mean field reinforcement learning ,AAMAS â€™21: 20th International
Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021 , ACM,
537â€“545.
[24] J. N. Tsitsiklis, Asynchronous stochastic approximation and q-learning , Machine learning 16(1994), no. 3, 185â€“202.
[25] P. Velickovic et al., Graph attention networks , stat1050(2017), 20.
[26] O.Vinyalsetal., Grandmasterlevelinstarcraftiiusingmulti-agentreinforcementlearning ,Nature575(2019),no.7782,
350â€“354.
[27] L. Wang et al., Neural policy gradient methods: Global optimality and rates of convergence , arXiv preprint
arXiv:1909.01150 (2019).
[28] T.Wuetal., Weightedmean-fieldmulti-agentreinforcementlearningviarewardattributiondecomposition ,International
Conference on Database Systems for Advanced Applications , Springer, 301â€“316.
[29] Z.Wuetal., Acomprehensivesurveyongraphneuralnetworks ,IEEEtransactionsonneuralnetworksandlearningsystems
32(2020), no. 1, 4â€“24.
[30] Q. Xie et al., Learning while playing in mean-field games: Convergence and optimality ,International Conference on
Machine Learning , PMLR, 11436â€“11447.19
[31] Y. Yang et al., Mean field multi-agent reinforcement learning ,Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, StockholmsmÃ¤ssan, Stockholm, Sweden, July 10-15, 2018 ,Proceedings of Machine
Learning Research , vol. 80, PMLR, 5567â€“5576.
[32] H. Zhang et al., H2gnn: Hierarchical-hops graph neural networks for multi-robot exploration in unknown environments ,
IEEE Robotics and Automation Letters 7(2022), no. 2, 3435â€“3442.
[33] K. Zhang, Z. Yang, and T. BaÅŸar, Multi-agent reinforcement learning: A selective overview of theories and algorithms ,
Handbook of Reinforcement Learning and Control (2021), 321â€“384.
[34] T.Zhangetal., MFVFD:Amulti-agentq-learningapproachtocooperativeandnon-cooperativetasks ,Proceedingsofthe
Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27
August 2021 , ijcai.org, 500â€“506, .
[35] Y.Zhangetal., Coordinationbetweenindividualagentsinmulti-agentreinforcementlearning ,Thirty-FifthAAAIConfer-
ence on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence,
IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event,
February 2-9, 2021 , AAAI Press, 11387â€“11394.
[36] L. Zheng et al., Magent: A many-agent reinforcement learning platform for artificial collective intelligence ,Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial
Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New
Orleans, Louisiana, USA, February 2-7, 2018 , AAAI Press, 8222â€“8223.
[37] S. Zhou et al., Multi-agent mean field predict reinforcement learning ,2020 IEEE International Conference on Advances
in Electrical Engineering and Computer Applications (AEECA) , IEEE, 625â€“629.
[38] Z.ZhouandG.Liu, Romfac:Arobustmean-fieldactor-criticreinforcementlearningagainstadversarialperturbationson
states, arXiv preprint arXiv:2205.07229 (2022).
How to cite this article: Min Yang, Guanjun Liu, Ziyuan Zhou, Jiacun Wang (2023), Partially Observable Mean Field Multi-
Agent Reinforcement Learning Based on Graph Attention Network for UAV Swarms, Drones,2023;7(7):476 . https://www.
mdpi.com/2504-446X/7/7/476