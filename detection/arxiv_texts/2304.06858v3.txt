arXiv:2304.06858v3  [cs.SI]  11 Jun 2023Vax-Culture: A Dataset for Studying Vaccine
Discourse on Twitter
Mohammad Reza Zarei∗, Michael Christensen†, Sarah Everts‡and Majid Komeili§
∗ §School of Computer Science,†Department of Law and Legal Studies,‡School of Journalism and Communication
Carleton University
Ottawa, Canada
Email:∗Mohammadrezazarei@cmail.carleton.ca,†Michael.Christensen@carleton.ca,‡Sarah.Everts@carleton.ca,
§Majid.Komeili@carleton.ca
Abstract —Vaccine hesitancy continues to be a main challenge
for public health ofﬁcials during the COVID-19 pandemic. As
this hesitancy undermines vaccine campaigns, many researc hers
have sought to identify its root causes, ﬁnding that the incr easing
volume of anti-vaccine misinformation on social media plat forms
is a key element of this problem. We explored Twitter as a sour ce
of misleading content with the goal of extracting overlappi ng
cultural and political beliefs that motivate the spread of v accine
misinformation. To do this, we have collected a data set of
vaccine-related Tweets and annotated them with the help of a
team of annotators with a background in communications and
journalism. Ultimately we hope this can lead to effective an d
targeted public health communication strategies for reach ing
individuals with anti-vaccine beliefs. Moreover, this inf ormation
helps with developing Machine Learning models to automatic ally
detect vaccine misinformation posts and combat their negat ive
impacts. In this paper, we present Vax-Culture, a novel Twit ter
COVID-19 dataset consisting of 6373 vaccine-related tweet s
accompanied by an extensive set of human-provided annotati ons
including vaccine-hesitancy stance, indication of any mis infor-
mation in tweets, the entities criticized and supported in e ach
tweet and the communicated message of each tweet. Moreover,
we deﬁne ﬁve baseline tasks including four classiﬁcation an d one
sequence generation tasks, and report the results of a set of recent
transformer-based models for them. The dataset and code are
publicly available at https://github.com/mrzarei5/Vax- Culture.
Index Terms —natural language processing, vaccine misinfor-
mation, vaccine hesitancy, Twitter dataset
I. I NTRODUCTION
When the novel coronavirus known as COVID-19 emerged
in 2019, it spread rapidly enough to be classiﬁed as a global
pandemic by the World Health Organization in early 2020.
Almost as rapidly, medical researchers began developing va c-
cines, culminating in multiple effective options authoriz ed by
national drug safety bodies around the world. To decrease
morbidity and to catalyze population-wide immunity, publi c
health agencies in many countries began to launch vaccine
campaigns. However, despite the wide availability in some
countries, a subset of many populations avoided vaccinatio n,
a phenomenon known as vaccine hesitancy [1].
Vaccine hesitancy undermines important public health cam-
paigns. Over the past decade, social scientists have found
that an increasing volume of anti-vaccine misinformation a nd
disinformation spread online is a key element of this proble m
[2]–[4]. Speciﬁcally, social media platforms have been wid elyused to spread anti-vaccine misinformation. These platfor ms
are among the main tools connecting people (particularly in
a pandemic) and they have signiﬁcant inﬂuence on decision-
making. Yet, as social scientists have pointed out, misinfo r-
mation and conspiracy theories are not spread by people who
are simply uninformed. For many, misinformation can tap int o
deep cultural narratives or the stories that people use to ma ke
sense of their lives and social contexts. Misinformation ca n
therefore feel true because it is interesting and entertain ing [5],
[6], or even products of what believers see as critical think ing
and research [7]. From this perspective, if misinformation
feels true because it speaks to social anxieties or seems
more consistent with peoples’ political identities, comba ting
misinformation may not simply be a problem of education and
fact-checking. Therefore, successful communication stra tegies
for combating vaccine hesitancy require a sophisticated un der-
standing of the underlying factors motivating misinformat ion
spread.
In one of the most recent broad manifest of vaccine hesi-
tancy, social media platforms were widely used to propagate
misinformation and negative tendencies against COVID-19
vaccines. Hence, these sources can be used to explore COVID-
19 vaccine misinformation posts and search for clues about
how best to reach anti-vaccine or vaccine hesitant members
of the public and understand the cultural/media context in
which hesitancy has grown. Moreover, the availability of hi gh-
quality information in this area can aid in developing Machi ne
Learning models for automatically identifying vaccine mis in-
formation posts and their attributes that can lead to combat ing
their negative impacts. However, reaching these objective s
depends on collecting relevant data.
Numerous Covid-19 vaccine datasets have been recently
collected from social media and made publicly available.
These datasets are mainly gathered on the Twitter platform d ue
to the simplicity of working with Twitter API and the wide
range of functionalities this tool provides for pulling Twe ets.
Moreover, Twitter is one of the most important sources of
misleading content in the media ecosystem [8], [9] making it
the center of attention in this area.
Although the published datasets have expedited the access
of researchers to vaccine-related data, they are usually co l-
lected and published without any quality control. Furtherm ore,annotations regarding vaccine-related attributes such as vac-
cine hesitancy and any misleading information in the text
are missing from these datasets, making them unsuitable for
training models on supervised tasks. Moreover, this missin g
information adds an additional process for performing stud ies
that require annotation. Finally, since only tweet IDs can b e
published due to Twitter policies and these Twitter dataset s
are not published with additional information regarding th e
meaning or communicated message in each tweet, if the
majority of the tweets get deleted over time, the dataset wil l
become useless.
In this paper, we present a Twitter COVID-19 vaccine mis-
information dataset (Vax-Culture) pulled from Twitter usi ng
the Twitter API with vaccine-related keywords, then anno-
tated by a team of trained annotators with a background in
journalism and communication. Each tweet is accompanied
by a set of complete attributes including vaccine hesitancy
stance (whether a Tweet is pro- or anti-vaccine), whether an y
information is misleading or inaccurate in the tweet, and th e
entities that are supported or criticized in the tweet. Sinc e
a small proportion of tweets in Twitter datasets might get
deleted over time, we also provide the intended communicate d
meaning of each tweet, mitigating the problem of deleted
tweets. Moreover, we deﬁne ﬁve baseline tasks including two
multi-class classiﬁcation tasks, two multi-label classiﬁ cation
tasks, and one text generation task on our dataset, and repor t
the performance of a set of state-of-the-art transformer-b ased
baseline models on these tasks. This evaluation can give
insights to researchers about developing models using our
dataset.
II. R ELATED WORK
Since the advent of COVID-19 virus, many Twitter datasets
related to this disease have been collected and published.
Lamsal [10] presented a COVID-19 dataset consisting of
310 million Covid-19 English language tweets collected wit h
covid-related keywords starting from March 20, 2020. The
sentiment of each tweet was also presented which was com-
puted using TextBlob. A multilingual COVID-19 dataset was
introduced in [11], comprised of 123 million tweets, with
over 60% of the tweets in English by the time of publishing
its paper. A set of basic analyses was also presented in the
paper that showed a correlation between coronavirus-relat ed
events and Twitter activity. The ﬁrst tweet of the presented
dataset backs to January 21, 2020 and its collection is still in
progress. Another large-scale multilingual COVID-19 data set
was presented in [12] containing a total of 2,996,610,622
tweets as of November 30, 2022 that were collected using
Twitter’s trending topics and selected keywords. This data set
is augmented with Twitter Named Entity Recognition and
Sentiment Analysis algorithms. In [13], a dataset consisti ng
of 252,600,524 tweets was presented, collected from Januar y
28, 2020 to June 1, 2022 and published with the emotion
and sentiment of each tweet. This additional information wa s
extracted using pre-trained Machine Learning-based emoti onrecognition algorithms. Topic Modeling was also performed
to extract the topic of each tweet.
All of the above datasets, however, either do not have any
annotation for the tweets or are accompanied by information
that is obtainable without any manual annotation such as
topic, sentiment and emotion. Such annotations are not dire ctly
related to the COVID-19 vaccine subject. Therefore, they
could not be directly used for tasks such as vaccine hesitanc y
detection that requires customized annotations. Moreover , In
[14], a dataset with respect to the context of vaccination fo r
the COVID-19 pandemic was presented, consisting of 40,268
and 98,385 tweets from the United Kingdom and the United
States, respectively. Although the sentiment of a tweet may
not necessarily be an indicator of vaccine hesitancy, this i nfor-
mation is directly used to specify whether the communicated
message in each tweet is vaccine-hesitant or not. COVID-19
vaccine hesitancy detection with human-provided annotati ons
has also been considered in recent research. However, most o f
these papers have not made their annotated datasets publicl y
available [1], [15], [16].
In [17], a Twitter dataset for vaccine misinformation de-
tection was introduced. This dataset comprised a total of
15,073 tweets, with 5,751 labeled as misinformation. The
tweets were annotated using veriﬁed sources and then the
labels were validated by public health experts. Three diffe rent
vaccine misinformation detection models including XGBoos t,
LSRTM, and BERT were trained and evaluated on the col-
lected dataset. Although the spread of misinformation can
be a sign of vaccine hesitancy, the presence or absence of
misinformation alone is not adequate for developing models
for vaccine hesitancy detection. Our collected dataset con tains
an extensive set of annotations related to vaccine misin-
formation and vaccine hesitancy that enables performing a
thorough analysis regarding the spread of vaccine misinfor -
mation and understanding the context in which hesitancy has
grown. Moreover, this dataset opens the door for developing
Machine Learning-based models on several natural language
processing tasks related to vaccine misinformation includ ing
vaccine hesitancy detection, vaccine misinformation dete ction,
criticized and supported entities detection as well as twee t
communicated meaning generation. In the paper, we focus on
the latter, as the former goes beyond the machine learning an d
natural language processing area and we leave it for future
work.
III. D ATASET COLLECTION AND ANNOTATION
In this section, we describe the data collection process usi ng
Twitter API and the tweet annotation process performed by a
trained team of annotators with a background in Journalism
and Communication.
Tweet data collection: We collected vaccine-related tweets
containing speciﬁc keywords using Twitter API for seven
months from September 2021 to March 2022 with a rate of
5000 tweets every day. Each tweet contained at least one of th e
following keywords: vaccine, vaccines, vaccination, vacc , vax,
vaxx and vaxxed. We further processed the collected tweets t oremove duplicates and non-English language tweets, result ing
in a collection of 500k tweets.
Annotation Process: A web dashboard was created to an-
notate tweets and manage assignment of tweets to annotators .
The dashboard allows annotators to assess an individual twe et
as it appears on the Twitter platform and annotate it accordi ng
to the following criteria: (1) whether it is anti-vaccine, n eutral,
or pro-vaccine, (2) whether any information in the tweet see ms
misleading or inaccurate (3) a list of who or what is the subje ct
of criticism in the tweet, (4) a list of who or what is supporte d
or promoted in the tweet (5) A free text response explaining
the intended meaning of the tweet.
This evaluation is performed by answering ﬁve questions,
each reﬂecting one of the mentioned criteria. For question
2, annotators were not asked to verify the factual accuracy
of any given tweet, but only to evaluate the presentation of
the information. The complete list of questions and possibl e
answer choices are shown in Table I. Since there may be an
entity criticized or promoted in the corresponding tweet wh ich
is not among the provided answer options in questions 3 and
4, we let the annotators add additional entities by consider ing
a free-text box for questions 3 and 4 to enter other answers.
The intended meaning of each tweet is requested in question
5 to ﬁrst complement the text of each tweet for analysis tasks ,
and second to keep some records and information about the
content of each tweet, in case the tweet was deleted.
The process of identifying misinformation is highly sub-
jective, and therefore requires a baseline literacy of the t opic
under consideration. Our early efforts to test the annotati on
system using the average score of three annotators on Ama-
zon’s MTurk platform1returned very poor results. As such we
adopted a quality control protocol focused on hiring annota tors
with existing English-language media literacy, in this cas e,
graduate students and fourth-year advanced undergraduate
students from Carleton University’s School of Journalism a nd
Communication. We then hosted three training sessions with
annotators designed to ensure interpretive alignment for e ach
of our questions on the topic of vaccines, as well as our
expectations for the free-text annotations (including min imum
word count, level of generality, and a focus on subtextual in -
terpretation). We also spot-checked the work of each annota tor
by monitoring word counts, time per annotation, and answer
frequency for each question. The results made clear that one
trained annotator produced far richer and more accurate dat a
than the crowd-sourced annotators were able to produce.
Each annotator is assigned a disjoint subset of random
tweets from the tweets pool for annotation. Since the conten t
of all tweets pulled by keywords may not be relevant, we allow
the annotators to skip any of the tweets if they found the con-
tent is not informative. Overall, 6,373 tweets were annotat ed
by the team of our annotators that forms our dataset. The
main statistics of the annotated dataset are reported in Tab le
II. According to the statistics, 43.8% and 40.5% of the tweet s
are anti-vaccine and pro-vaccine, respectively. This show s a
1https://www.mturk.com/good balance between positive and negative tendencies over
the annotated tweets with respect to communicated messages .
Also, 15.8% of the tweets communicate uncertainty about
vaccine. Moreover, 38.9% of the tweets contain misleading
or inaccurate information. Finally, 77.3% of tweets critic ise at
least one of the possible answer options such as vaccine man-
date, anti-vaxxers, safety of vaccines, etc.. This number i s 87%
for supporting entities such as Vaccines, freedom of choice ,
public health interventions, etc.. We present example pair s of
tweets and their corresponding communicated meaning of the
tweets that are collected by annotators in Table III.
IV. B ASELINE TASKS
In this section, we introduce a set of baseline tasks.
Communicated message prediction: This task consists of
predicting the communicated message of each tweet and is
deﬁned as a three-way classiﬁcation task: anti-vaccine, pr o-
vaccine or unsure about vaccine.
Misleading or inaccurate information detection: This
task is deﬁned as a binary classiﬁcation and consists of pre-
dicting whether any information in the tweet seems misleadi ng
or inaccurate.
Subjects of criticism prediction: This task is deﬁned as
a multilabel classiﬁcation problem and consists of predict ing
all entities that have been criticized. This task is a 12-cla ss
multilabel classiﬁcation problem. The available classes a re the
same as the answer options for question 3 as shown in Table
I.
Subjects of support/promote prediction: Similar to the
previous tasks, this one is also a multilabel classiﬁcation
problem. It consists of predicting the entities that each tw eet
supports or promotes. This task consists of 11 classes which
are the same as the answer options for question 4 as shown
in Table I.
Tweet meaning generation: This task is deﬁned as a text
generation task to explain the meaning each tweet intents to
communicate either implicitly or explicitly as provided by the
annotators in response to question 5. The input is the text of
the corresponding tweet. Since the text of tweet may not be
sufﬁcient for this task, we consider two additional version s of
this task with extended inputs. In the ﬁrst extended version ,
we append the text of the replied tweet in case the tweet
is a reply. This includes 31.1% of all the tweets. The new
tweet is preceded with the token “ < rep > ” in the input
to separate it from the text of the original tweet. We also
append the title and description of the links that were refer red
in tweet and parsed by Twitter API. This was applicable to
13.3% of tweets. The titles are preceded with “ < urlt >”
and the descriptions are preceded with “ < urld >”. In the
second extended version, we expand the input by adding the
information collected by the annotators from questions 1 to
4. To this end, we transform the answers for each tweet to
sentences and append them to the original tweet to form an
extended input text. This information is separated from the
tweet text with the token < sep > . The following outlines the
transformation process for each question:TABLE I: List of the questions and possible answer choices us ed to annotate each tweet. For questions 3 and 4, in addition t o
the predeﬁned set of answer choices, annotators were provid ed with the possibility of entering additional answers in a f ree-text
response form.
Question Answer Options
1. What is the message communicated in this Tweet? 1. Anti-vaccine 2. Pro-vaccine 3. Unsure about the vaccine
2. Does any information in the Tweet seem misleading or inacc urate? 1. Yes 2. No
3. Who or what is the subject of criticism in the tweet? [choos e all that apply]1. Vaccine mandates 2. Anti-vaxxers 3. The safety of vaccine s
4. Vaccine effectiveness 5. Public health policy 6. Politic ians
7. Government 8. Public health ofﬁcials 9. Pharmaceutical c ompanies
10. Democrats or Liberals 11. Conservative media 12. Mainst ream
media (Additional answers in the form of free text can be prov ided)
4. Who or what does the tweet support or promote? [choose all t hat apply]1. Vaccines 2. Freedom of choice 3. Public health interventi ons
4. A more relaxed approach 5. Science 6. Natural health
7. Global response 8. Waiting for more information 9. Altern ative
remedies 10. Small business 11. Religious beliefs (Additio nal answers
in the form of free text can be provided)
5. Please explain the meaning this tweet intends to communic ate (either
implicitly or explicitly). Please also include any importa nt keywords related Free text answer should be provided by annotator
to this meaning.
TABLE II: Statistics of the dataset
Number of tweets 6,373
Percentage of anti-vaccine tweets 43.8%
Percentage of pro-vaccine tweets 40.5%
Percentage of tweets with uncertainty about vaccine 15.8%
Percentage of tweets with misleading/inaccurate informat ion 38.9%
Percentage of tweets criticizing at least one entity 77.3%
Percentage of tweets supporting at least one entity 87%
Q1. The template sentence “The message communicated in
this tweet is . . .” is completed with the answer.
Q2. If the answer to this question is positive, the sentence
“This tweet contains misleading or inaccurate informa-
tion.” is appended to input
Q3. For any option selected by the annotators, we add a
sentence by completing the template “This tweet criticises
. . .”.
Q4. Similar to the previous question, we append a sentence
for any selected option by completing the template “This
tweet supports or promotes . . .”.
V. B ASELINE TASKS EVALUATION
A. Experimental Setup
Classiﬁcation Models: We used ﬁve different pre-trained
language models for baseline classiﬁcation tasks (commu-
nicated Message prediction, misleading/inaccurate infor ma-
tion detection, subjects of criticism prediction and subje ct
of support/promote prediction) including BERT-base [18],
BERT-large [18], RoBERTa-base [19], RoBERTa-large [19]
and BERTweet-covid19 [20]. BERT-base and BERT-large
are pre-trained on BookCorpus [21] and English Wikipedia.
RoBERTa-base and RoBERTa-large are pre-trained on over
160GB of uncompressed text from BookCorpus, English
Wikipedia, CC-News containing 63 million English news
articles collected from the CommonCrawl News dataset [22],
OpenWebText [23] and Stories [24]. BERTweet-covid19 has
the same architecture as BERT-base but is speciﬁcally pre-
trained on English tweets. This model uses 850M English
tweets plus 23M COVID-19 English tweets for pre-training.Text Generation Models: To perform baseline experiments
for tweet meaning generation task, we used Bart-large2[25]
and T5-large [26]3which are two large-scale language models
suitable for text generation tasks.
Data Split: We use the same train, validation, and test sets
in all experiments. To create these sets, we perform a data
split with a stratiﬁed sampling over the ﬁeld ”communicated
massage” to ensure the same distribution in terms of vaccine
hesitancy along all train, validation and test sets. We use 2 0%
of the dataset as test set and keep 25% of the remaining
tweets to perform validation and select the best model in eac h
experiment. The remainder is used for training.
Experimental Settings: For all experiments, we substitute
mentions with the token “@USER” and URL links with
“HTTPURL” in all tweets. Moreover, we use the emoji pack-
age of Python to replace each emoji with a text representatio n.
We also use NLTK TweetTokenizer [27] to remove redundant
characters.
For all tasks, the tokens “HTTPURL” and “@USER” are
added to the vocabulary of each model before being ﬁne-tuned
on the dataset. Additional to these two tokens, we add the
tokens< rep > ,< urlt >, and< urld > for the tweet
meaning generation task with the ﬁrst version of extended
input. For the second version, we add the token < sep >
to the vocabulary of the corresponding models.
Training Settings: Adam optimizer with a learning rate
of1e−5and a weight decay of 0.01 is used to ﬁne-tune all
classiﬁcation models. The same optimizer with a learning ra te
of3e−4and1e−5and no weight decay is used to ﬁne-tune
T5-large and Bart-large, respectively. All the models are ﬁ ne-
tuned for 40 epochs with a batch size of 16. After each epoch,
the model is evaluated on the validation set and the model wit h
the best performance is evaluated on the test set.
Evaluation Metrics: We report macro-averaged precision,
recall, and F1 score for the classiﬁcation tasks with the exc ep-
tion of the binary task of misleading/inaccurate informati on
2https://huggingface.co/facebook/bart-large
3https://huggingface.co/t5-largeTABLE III: Examples of tweets and the meaning they intend to c ommunicate, as provided by the annotators.
Tweet Meaning
@USER All these conspiracy theorists are more likely to get s eriously
ill, plus there’s more vaccine for the thinking population. We call that
win/win.The tweet is promoting the fact that anti-vaxxers will get si ck from not
getting vaccinated. The tweet is in support of this, and spec iﬁes that
there will be more vaccines for the thinking population.
When a totalitarian government and big pharma have a lovechi ld you
get mandatory vaccination.This tweet is saying that governments and big pharma are work ing together
and that by creating vaccine mandates they are both beneﬁtti ng,
which is not true.
So people who have never had COVID are getting the vaccine and
now they are coming down with COVID. Anyone care to explain th at?The tweet is questioning the safety of vaccines, as he is skep tical that
people who are getting the vaccine, are now contracting COVI D-19
as well.
So many questions for @USER. If the vaccine is safe and effect ive,
why won’t you, doctors, manufacturers take responsibility if
something goes wrong? HTTPURLThis tweet is referring to the idea that pharmaceutical manu facturers
are not responsible for any vaccine-related deaths and incl udes a video
claiming that 40% of the people who are dying from COVID are
completely vaccinated. The tweeter is pointing to the idea t hat if
ofﬁcials truly believed vaccines were safe and effective th en they
would take accountability when things go wrong.
TABLE IV: Evaluation of the baseline models on the commu-
nicated message prediction task
Model Accuracy Precision Recall F1
BERTweet-cov19 63.2 57.5 57.6 57.3
BERT-base 60.1 53.8 53.8 53.7
BERT-large 60.8 54.3 54 54
RoBERTa-base 60.2 55.6 55.6 55.2
RoBERTa-large 69.6 61.9 61.8 61.8
TABLE V: Results of the baseline models on the mislead-
ing/inaccurate information detection task.
Model Accuracy Precision Recall F1
BERTweet-cov19 73.4 65.2 71.8 68.3
BERT-base 71 61.7 72.4 66.6
BERT-large 71.5 61.8 75.7 68
RoBERTa-base 72.9 66.3 65.7 66
RoBERTa-large 75.6 68 73.7 70.7
detection where we report precision, recall, and F1 over
the class with label ”Yes”. Tweet meaning generation task
is evaluated by three ROUGE metrics including ROUGE-1,
ROUGE-2, and ROUGE-L. The length of the generated text
is also reported in this task.
B. Baseline Results
Communicated Message Prediction: The results of this
task are shown in Table IV. In all cases, the difference betwe en
precision and recall was negligible leading to obtaining a
value for F1 score close to both precision and recall. The
best performance belongs to RoBERTa-large with respect to a ll
metrics. The performance of this model was 69.6 and 61.8 in
terms of accuracy and F1 score, respectively. This superior ity
is due to using a more effective pre-training procedure and a
larger set of data for pre-training, compared to Bert models ,
and a larger network compared to RoBERTa-base. BERTweet-
covid19 which is a baseline speciﬁcally pre-trained on Engl ish
tweets ranked second among top-performing models. This
model achieved an accuracy of 63.2 and F1 score of 57.3.
Misleading/Inaccurate Information Detection: We
present the results of this task in Table V. Similar to the
communicated message prediction task, RoBERTa-large
outperformed other baselines in terms of all metrics with th eTABLE VI: Evaluating baselines on the subjects of criticism
prediction task.
Model Accuracy Precision Recall F1
BERTweet-cov19 88.1 47.9 28.9 33.9
BERT-base 88.2 54.5 25.3 31.7
BERT-large 88.7 57 32 38
RoBERTa-base 88.2 48.8 31.9 37
RoBERTa-large 89.2 51.6 41.2 44.7
exception of best recall score that was attained by BERT-lar ge
with 2% margin. BERTweet-covid19 is the runner-up with
respect to accuracy and F1 score. Although BERTweet-
covid19 placed third among best-performing models on both
precision and recall, a more balanced performance with
respect to these two metrics led to achieving the second
highest F1 score with just 0.3% difference with BERT-large a s
the third top-performer. BERTweet-covid19 is the runner-u p
also in terms of accuracy.
Subjects of Criticism Prediction: The results of this task
are shown in Table VI. RoBERTa-large obtained the highest
accuracy, recall and F1. The best performance in terms of
precision belongs to BERT-large. This model also holds the
second highest accuracy and F1. We observed a considerable
gap between the accuracy performance and other metrics.
While the baselines were able to achieve almost a high
accuracy of around 88%, they could not perform comparably
well on other metrics. To further investigate the results, w e
present ﬁne-grained F1 scores and the ratio of the positive
class for each answer choice in Table VII.
We can observe that the F1 score on some labels including
conservative media, mainstream media, public health ofﬁci als,
and public health policy is noticeably low for all baseline
models. If we take into consideration the ratio of positive
labels, we can see that the ratio of positive labels is below
10% for these labels which makes them severely imbalanced
problems. Although we can see an exception F1 score higher
than the average for the label Democrats or liberals that has
only 5.6% instances with positive class, this performance i s
due to the fact the baselines are not trained from scratch
and the pre-trained models perform differently depending o n
the data used for pre-training. Despite such exceptions inTABLE VII: Per-label F1 score and the ratio of positive class for each answer choice in the Subjects of Criticism task.
Label BERTweet Bert-base Bert-large RoBERTa-base RoBERTa large %Pos Class
Politicians 44.8 36.5 37.7 47.9 52.8 10.6
Pharmaceutical companies 39.7 38.2 53.5 44.9 52.4 5.8
Public health ofﬁcials 11.7 6.1 20.5 23 29.3 6.4
Anti-vaxxers 40.8 34.8 38 35.1 52.1 16.9
Vaccine mandates 54.7 53.6 58.2 55.1 64 33.6
Vaccine safety 47.7 49.5 49.5 51.9 62.2 18
Conservative media 8 8.3 8.3 8 7.7 1.3
Mainstream media 11.6 15.2 29.3 27.3 35.6 4
Public health policy 17.9 15.9 17.3 20.7 24.4 9.8
Democrats or liberals 42 44.7 50.4 37.1 46.2 5.6
Government 44.2 34.7 43.1 45.1 50.3 13.1
Vaccine effectiveness 43.6 42.6 50 47.6 55.6 17.6
Average 33.9 31.7 38 37 44.7 11.9
TABLE VIII: Evaluating baselines on the subject of support
prediction task.
Model Accuracy Precision Recall F1
BERTweet-cov19 88.3 38 15.4 18.6
BERT-base 88.5 38.5 15.8 20.3
BERT-large 88.7 37.4 19.8 23.6
RoBERTa-base 88.4 34.6 19.5 24
RoBERTa-large 88.8 38.6 29.5 32.8
labels with low positive class ratio, we can see that for labe ls
with a more balanced class ratio such as vaccine mandates,
performance is signiﬁcantly higher.
Vaccine mandates was the label with the highest proportion
of positive class. This phenomenon was criticised in about
one third of the tweets. Next most criticised entities were
vaccine safety, vaccine effectiveness and anti-vaxxers wi th a
criticizing ratio of 18%, 17.6% and 16.9%, respectively. Al so,
conservative media and mainstream media were the rarest
among critisized entities with a positive class ratio of 1.3 %
and 4%, respectively.
Subjects of Support/Promote Prediction: The results of
this task are shown in Table VIII. Similar to the subjects of
criticism prediction task, we also report per-label F1 scor e and
the ratio of positive class for each label in Table IX. Accord ing
to Table VIII, the best performance with respect to all four
metrics belongs to RoBERTa-large. This model attained a
macro-averaged F1 score of 32.8. Similar to the subjects
of criticism prediction task, the macro-average accuracy i s
around 88 in all cases. This high value is mainly due to the
spareness of positive classes where models achieve a high
accuracy by simply predicting all samples to be from the
majority class. The imbalanced essence of this task should
be taken into account to pick the proper model and training
procedure. According to Table IX, the correlation between l ow
performance in terms of F1 score and the low ratio of the
samples with positive class is more tangible as the labels wi th
an F1 score equal to 0 such as religious beliefs and small
business hold a positive label ratio less than 5. The highest
F1 score for all baselines is achieved for vaccine label that
holds the highest ratio of positive class (i.e. 36.2%) among all
labels.
Tweet Meaning Generation: The results of this task arepresented in Table X. As explained in the previous section, w e
consider three versions of the tweet meaning generation tas k
with respect to model input. The simple version that its inpu t
is just the text of the tweet is denoted by V0. We also report
the results of the models with the ﬁrst and second versions of
the extended input (denoted by V1 and V2, respectively) as
explained in Section IV.
Although it was anticipated that the models with extended
input versions, V1 and V2, would perform better, in our
experiments they did not improve V0. The ﬁrst and the second
extended inputs were able to improve the performance of
Bart by just 0.7% and 1.2% on average, respectively (Average
of Rouge-1, Rouge-2, and Rouge-l). Conversely, the average
performance of T5 declined by 0.7% and 0.3% on V1 and
V2, respectively. Through a qualitative analysis of the tex ts
generated by different versions of the models, we observed
that none of them was a clear winner in all cases. An example
of the input to each version of T5 and the generated meaning
is shown in Table XI. Although some useful information
was inferred and meaningful texts were generated from the
tweets, the complex nature of this task led to generating som e
incomplete meanings in some cases.
C. Discussion on Available Subset
Since a proportion of the tweets may not be accessible on
Twitter temporarily or permanently due to tweet removal or
user account deactivation, we create a subset of tweets that
were still available around 9 months after the Twitter data
collection was concluded. This accounts for around 73% of
the tweets. We compare the main statistics and results on thi s
still-available subset, with the complete dataset; herein after
referred to as AandC, respectively. For the still-available
subset, we follow the same train, validation, and test data s plit
procedure as previously described for the complete set.
We report the results of RoBERTa-large on the classiﬁcation
tasks on the still-available subset and compare them with
the results on the complete dataset in Table XII. RoBERTa-
large is selected due to its overall performance regarding t he
comparisons on the complete dataset. Surprisingly, althou gh
the number of tweets has decreased in the still-available su bset,
the performance was increased on communicated message
prediction task across all metrics. The increase in perform anceTABLE IX: Per-label F1 score and the ratio of positive class f or each answer choice in the Subjects of Support/Promote tas k.
Label BERTweet Bert-base Bert-large RoBERTa-base RoBERTa large %Pos Label
Science 20.1 24 26 28.4 33.5 17.7
Freedom of choice 36.6 37.5 43.5 45.5 52.5 31.3
Natural health 0 0 0 12.2 21.6 2.7
Vaccines 62.1 60.2 61.3 62.7 71.1 36.2
Small business 0 0 0 0 0 0.4
Alternative remedies 8.3 13.8 15.4 20.7 28.6 1.9
A more relaxed approach 9.7 16.4 16.7 15.5 21.8 17.3
Waiting for more information 3.6 7.1 13.1 5.6 17.3 3.1
Public health interventions 28.6 32.2 35.2 31.5 44.9 19.9
Global response 35.3 31.8 48.1 41.7 50 2.9
Religious beliefs 0 0 0 0 20 0.7
Average 18.6 20.3 23.6 24 32.8 12.2
TABLE X: The results of tweet meaning generation task using B art and T5 with different input as explained in Section IV
ModelRouge-1 Rouge-2 Rouge-l Length
V0 V1 V2 V0 V1 V2 V0 V1 V2 V0 V1 V2
T5 28.9 28 28.5 8.6 8 8.5 24 23.2 23.5 18.8 18.8 18.9
Bart 27.4 28.3 28.6 8.5 9 9.8 23.1 23.8 24.2 19.9 19.9 20
TABLE XI: An example of input and generated text of T5 model wi th different versions of input for Tweet Meaning Generation
task. The ﬁrst row corresponds to the model with just the text of the tweet as input. The second and the third rows belong to
the model with the ﬁrst and second versions of extended input , respectively.
Input Output
@USER Also Neil I love the way you throw about the word
vaccines, I know vaccines work.. I am pro vaccine.. My proble m
is speciﬁcally the Covid19 jab.. I know ye like to throw every one
in the anti vaxx bucket..This tweeter says they are pro vaccines and that they are agai nst the COVID-19
@USER Also Neil I love the way you throw about the word
vaccines, I know vaccines work.. I am pro vaccine.. My proble m
is speciﬁcally the Covid19 jab.. I know ye like to throw every one
in the anti vaxx bucket.. < rep > @USER The truth is vaccines
work, I talk to them on a personal level. They are passionate
about their work. Believe me there’s no conspiracy from scie nce.
Pharma makes proﬁt but that’s capitalism someone will.This tweeter is pro-vaccine and says that he is pro-vaccine.
@USER Also Neil I love the way you throw about the word
vaccines, I know vaccines work.. I am pro vaccine.. My proble m
is speciﬁcally the Covid19 jab.. I know ye like to throw every one
in the anti vaxx bucket.. < sep > The message communicated
in this tweet is pro-vaccine. < sep > This tweet criticises
vaccine mandates. < sep > This tweet supports or promotes
freedom of choice < sep > This tweet supports or
promotes vaccines < sep > This tweet supports or promotes
a more relaxed approach.This tweeter says they are pro vaccines and that they are agai nst the vaccine
mandates
was observed on other models too. This improvement may be
due to the decrease in the proportion of anti-vaccine tweets
from 43.8% to 38.6% and the subsequent increase in the ratio
of pro-vaccine tweets from 40.5% to .45.4%. This interestin g
change may be a sign of revision in the attitude of anti-
vaxxers over time. Although the results of RoBERTa-large
were improved also on the misleading/inaccurate informati on
detection task, this improvement was not observed in other
models showing the capability of RoBERTa-large and the
importance of model selection in this task. Also, the propor tion
of tweets with misleading or inaccurate information decrea sed
from 38.9% to 34.2%. This may be related to an increase in
public awareness over time.
The performance on subjects of criticism prediction and
subjects of support/promote prediction tasks has decrease d
compared to the performance on the complete dataset. This
decline was 2.7% and 3.4% in terms of average F1 score onthese tasks, respectively. A similar trend was observed for
the other models too. Regarding the ratio of positive class,
while this number was similar for most of the labels, the
highest increase was 1.4% belonged to anti-vaxxers while th e
highest decline was 3.3% held by vaccine safety. On subjects
of support/promote prediction task, the maximum rise and
decline in the positive class ratio were 4.3% for vaccine and
3% for freedom of choice, respectively. As for the tweet
meaning generation task, we did not observe a noticeable
difference between the results of the still-available subs et and
the complete dataset.
VI. C ONCLUSION
In this paper, we presented a novel Twitter COVID-19
vaccine misinformation dataset collected using Twitter AP I
and manually annotated by a team of annotators with a
background in communications and journalism. Each tweetTABLE XII: Comparison between the performance of RoBERTa-l arge on the complete dataset ( C) and still-available subset
(A) on the classiﬁcation tasks.
TaskAccuracy Precision Recall F1
T A T A T A T A
Communicated Message Prediction 69.6 71.7 61.9 65 61.8 63.6 61.8 64
Misleading Information Detection 75.6 76.5 68 68.8 73.7 75.3 70.7 71.9
Subjects of criticism prediction 89.2 89.9 51.6 48.3 41.2 39 44.7 42
Subjects of support/promote prediction 88.8 88.5 38.6 33.9 29.5 26.6 32.8 29.4
of the dataset accompanies by an extensive set of human-
provided annotations including vaccine hesitancy stance, indi-
cation of any misinformation in tweets, the entities critic ized
and supported in each tweet and the communicated message of
each tweet. We also deﬁned ﬁve baseline tasks on our dataset
and reported the results of a set of recent NLP models for
them. Furthermore, the provided dataset can be used to study
the vaccine hesitancy and spread of vaccine misinformation
problems.
REFERENCES
[1] S. Nyawa, D. Tchuente, and S. Fosso-Wamba, “Covid-19 vac cine
hesitancy: a social media analysis using deep learning,” Annals of
Operations Research , pp. 1–39, 2022.
[2] J. D. Featherstone and J. Zhang, “Feeling angry: the effe cts of vaccine
misinformation and refutational messages on negative emot ions and
vaccination attitude,” Journal of Health Communication , vol. 25, no. 9,
pp. 692–702, 2020.
[3] A. Jamison, D. A. Broniatowski, M. C. Smith, K. S. Parikh, A. Malik,
M. Dredze, and S. C. Quinn, “Adapting and extending a typolog y to
identify vaccine misinformation on twitter,” American Journal of Public
Health , vol. 110, no. S3, pp. S331–S339, 2020.
[4] N. Smith and T. Graham, “Mapping the anti-vaccination mo vement on
facebook,” Information, Communication & Society , vol. 22, no. 9, pp.
1310–1327, 2019.
[5] A. R. Hochschild, Strangers in their own land: Anger and mourning on
the American right . The New Press, 2018.
[6] F. Polletta and J. Callahan, “Deep stories, nostalgia na rratives, and fake
news: Storytelling in the trump era,” in Politics of meaning/meaning of
politics . Springer, 2019, pp. 55–73.
[7] D. Boyd, “You think you want media literacy. . . do you,” Retrived
from https://points. datasociety. net/you-think-you-wa nt-medialiteracy-
do-you-7cad6af18ec2 , 2018.
[8] M. Hindman and V . Barash, “Disinformation,’fake news’ a nd inﬂuence
campaigns on twitter,” 2018.
[9] W. Phillips, “The oxygen of ampliﬁcation,” Data & Society , vol. 22, pp.
1–128, 2018.
[10] R. Lamsal, “Design and analysis of a large-scale covid- 19 tweets
dataset,” applied intelligence , vol. 51, no. 5, pp. 2790–2804, 2021.
[11] E. Chen, K. Lerman, E. Ferrara et al. , “Tracking social media discourse
about the covid-19 pandemic: Development of a public corona virus
twitter data set,” JMIR public health and surveillance , vol. 6, no. 2,
p. e19273, 2020.
[12] C. E. Lopez and C. Gallemore, “An augmented multilingua l twitter
dataset for studying the covid-19 infodemic,” Social Network Analysis
and Mining , vol. 11, no. 1, pp. 1–14, 2021.
[13] R. K. Gupta, A. Vishwanath, and Y . Yang, “Covid-19 twitt er dataset
with latent topics, sentiments and emotions attributes,” arXiv preprint
arXiv:2007.06954 , 2020.
[14] A. Hussain, A. Tahir, Z. Hussain, Z. Sheikh, M. Gogate, K . Dashtipour,
A. Ali, A. Sheikh et al. , “Artiﬁcial intelligence–enabled analysis of
public attitudes on facebook and twitter toward covid-19 va ccines in
the united kingdom and the united states: Observational stu dy,” Journal
of medical Internet research , vol. 23, no. 4, p. e26627, 2021.
[15] L.-A. Cotfas, C. Delcea, and R. Gherai, “Covid-19 vacci ne hesitancy in
the month following the start of the vaccination process,” International
Journal of Environmental Research and Public Health , vol. 18, no. 19,
p. 10438, 2021.[16] L.-A. Cotfas, C. Delcea, I. Roxin, C. Ioan˘ as ¸, D. S. Ghe rai, and
F. Tajariol, “The longest month: analyzing covid-19 vaccin ation opin-
ions dynamics from tweets in the month following the ﬁrst vac cine
announcement,” Ieee Access , vol. 9, pp. 33 203–33 223, 2021.
[17] K. Hayawi, S. Shahriar, M. A. Serhani, I. Taleb, and S. S. Mathew,
“Anti-vax: a novel twitter dataset for covid-19 vaccine mis information
detection,” Public health , vol. 203, pp. 23–30, 2022.
[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understan ding,” arXiv
preprint arXiv:1810.04805 , 2018.
[19] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optim ized bert
pretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.
[20] D. Q. Nguyen, T. Vu, and A. T. Nguyen, “Bertweet: A pre-tr ained
language model for english tweets,” arXiv preprint arXiv:2005.10200 ,
2020.
[21] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun , A. Torralba,
and S. Fidler, “Aligning books and movies: Towards story-li ke visual
explanations by watching movies and reading books,” in Proceedings of
the IEEE international conference on computer vision , 2015, pp. 19–27.
[22] S. Nagel, “Cc-news,” URL: http://web. archive.
org/save/http://commoncrawl. org/2016/10/newsdataset available ,
2016.
[23] A. Gokaslan, V . Cohen, E. Pavlick, and S. Tellex, “Openw ebtext corpus,”
2019.
[24] T. H. Trinh and Q. V . Le, “A simple method for commonsense reason-
ing,” arXiv preprint arXiv:1806.02847 , 2018.
[25] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed , O. Levy,
V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence -to-sequence
pre-training for natural language generation, translatio n, and comprehen-
sion,” arXiv preprint arXiv:1910.13461 , 2019.
[26] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. M atena,
Y . Zhou, W. Li, P. J. Liu et al. , “Exploring the limits of transfer learning
with a uniﬁed text-to-text transformer.” J. Mach. Learn. Res. , vol. 21,
no. 140, pp. 1–67, 2020.
[27] S. Bird, E. Loper, and E. Klein, “Natural language proce ssing with
python o’reilly media inc,” 2009.