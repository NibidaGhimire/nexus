This article has been accepted for publication in the proceedings of the
2023 IEEE International Workshop on Advances in Sensors and Interfaces (IWASI).
¬©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.A Fast and Accurate Optical Flow Camera for
Resource-Constrained Edge Applications
Jonas K ¬®uhne, Michele Magno, Luca Benini
Dept. of Information Technology and Electrical Engineering, ETH Z ¬®urich, Z ¬®urich, Switzerland
kuehnej@ethz.ch
Abstract ‚ÄîOptical Flow (OF) is the movement pattern of pixels
or edges that is caused in a visual scene by the relative motion
between an agent and a scene. OF is used in a wide range
of computer vision algorithms and robotics applications. While
the calculation of OF is a resource-demanding task in terms of
computational load and memory footprint, it needs to be executed
at low latency, especially in robotics applications. Therefore, OF
estimation is today performed on powerful CPUs or GPUs to
satisfy the stringent requirements in terms of execution speed
for control and actuation. On-sensor hardware acceleration is a
promising approach to enable low latency OF calculations and
fast execution even on resource-constrained devices such as nano
drones and AR/VR glasses and headsets. This paper analyzes the
achievable accuracy, frame rate, and power consumption when
using a novel optical Ô¨Çow sensor consisting of a global shutter
camera with an Application SpeciÔ¨Åc Integrated Circuit (ASIC) for
optical Ô¨Çow computation. The paper characterizes the optical Ô¨Çow
sensor in high frame-rate, low-latency settings, with a frame rate
of up to 88 fps at the full resolution of 1124 by 1364 pixels and
up to 240 fps at a reduced camera resolution of 280 by 336, for
both classical camera images and optical Ô¨Çow data.
Index Terms ‚ÄîImage sensors, Optical Ô¨Çow, Hardware acceler-
ation, Low-power electronics
I. I NTRODUCTION
One important operation in visual perception pipelines is the
tracking of image features using a descriptor-based approach.
While descriptors allow the tracking of a single feature over
arbitrary frames with no strict succession in time [1], there is
a subgroup of these algorithms that tracks features only over
consecutive frames. This approach is called optical Ô¨Çow. It
tracks the movement of single features in an image sequence
or stream. Optical Ô¨Çow can also be applied in a dense manner
to track the movement of every pixel in a frame [2].
Tracking the movement of features between consecutive
frames compared to feature matching in arbitrary scenes has
the beneÔ¨Åt that if the movement characteristic of the agent
carrying the camera is known, then a maximum displacement
can be deÔ¨Åned to reduce the search range in feature matching
[3]. On the other hand, this task is confronted with some chal-
lenges compared to arbitrary feature matching. For example,
if a feature is lost from one frame to the next, but appears in
a later frame again, the inference that this is the same feature
as was tracked before, would not be possible with only optical
Ô¨Çow.
This work was supported by the European Union‚Äôs ERA-NET CHIST-ERA
2018 Research and Innovation Programme APROVIS3D under the Grant SNF-
19 20CH21 186991.On resource-constrained hardware, we face additional chal-
lenges in the computation of OF due to the small form factor, a
constrained energy budget, weight limitations, limited comput-
ing power, and often a requirement for low-cost solutions [4].
For instance, small-scale UA Vs and AR/VR glasses and head-
sets, impose high requirements on their respective perception
pipeline. On nano UA Vs, a low-latency perception pipeline is
equally important for fast Ô¨Çight as on its bigger counterparts
[5], [6]. Similarly, compact and inconspicuous AR glasses
also require a low-latency perception of the environment and
wearer to appropriately display rendered information.
This work introduces and characterizes a camera with an
embedded OF accelerator that has the potential to address
these problems, by offering low-latency OF tracking with mod-
erate power requirements due to its domain-speciÔ¨Åc accelerator
in the form of an ASIC.
The calculation of optical Ô¨Çow is a repetitive task that needs
to be done several times per frame (usually for a Ô¨Åxed number
of features) in a perception pipeline. This paper exploits a
not yet commercially available optical sensor, designed by
STMicroelectronics1, that embeds hardware acceleration for
the computation of optical Ô¨Çow directly in the sensor. We
present an accurate characterization of this sensor named
VD56G3, evaluating its beneÔ¨Åts in the low latency calculation
of optical Ô¨Çow, which can reach up to 300 frames per second.
The paper proposes and evaluates different use cases where
this sensor might be useful, especially in the domain of high-
speed autonomous vehicles and drones. Although we motivate
the use of optical Ô¨Çow for small-scale UA Vs we do not limit
our analysis to UA Vs. We also characterize the sensor in both
high frame rate and large Ô¨Åeld of view settings that are more
typical to other use cases such as speech recognition and eye-
tracking.
In detail, the paper contributes the following results:
A characterization of the VD56G3 sensor with different
sensor settings on different application scenarios.
An analysis of the tracking accuracy of the VD56G3
sensor in comparison to a Vicon ground truth.
An analysis of the re-detectability of features across
multiple frames.
An investigation of the consumed power when running
the optical Ô¨Çow pipeline on the camera, to assess if it is
viable compared to running it on a microprocessor.
1www.st.comarXiv:2305.13087v1  [eess.IV]  22 May 2023A dataset containing both the optical Ô¨Çow and image
data for different camera movements, as generated by the
VD56G3 sensor.
II. R ELATED WORK
In this section, we discuss related work in the area of
feature-based optical Ô¨Çow predictors by looking at feature
detection, description, and matching algorithms. Furthermore,
we look at the efforts that have been made to build hardware
accelerators for the computation of optical Ô¨Çow.
A. Feature Detection, Description, and Matching
The detection of features is often achieved with a layered
approach, where Ô¨Årst a fast algorithm is run to get some
candidate features. In the second step, a more sophisticated
algorithm is used to determine if the candidate point is suitable
as a feature point. These algorithms usually calculate the
image gradient using either the Shi-Tomasi [7] or the Harris
[8] corner detector algorithm. If the image gradient is steep
enough, then a point is selected as a feature and the feature
descriptor is calculated. As a last step, other nearby feature
candidates are being suppressed to avoid describing the same
feature multiple times. Modern algorithms [9] use feature
detectors that are rotation invariant as well as scaling invariant
within a certain range by determining a dominant orientation
and scale.
The feature descriptors, which are calculated on the de-
rotated and re-scaled feature points utilizing the dominant
orientation and scale respectively, are usually either composed
of a binary gradient representation as in SIFT [10], BRIEF
[11], or ORB [9], or composed of the actual de-rotated and
re-scaled image patch.
The matching of features is done by Ô¨Ånding the most
similar correspondence between two feature descriptors of
two different frames, which can be consecutive (as in the
calculation of optical Ô¨Çow) or not (as in place recognition
or key-frame-based VIO). As a similarity metric for binary
descriptors, the Hamming distance is used. For image patches,
the similarity between the template and the target patch is
calculated using the normalized cross-correlation (NCC) or a
similar metric [12].
B. Optical Flow Prediction
Optical Ô¨Çow describes the displacement of a pixel or a
feature from one image frame to the next one. It can be
therefore calculated by feature detection and matching as
described previously. As an alternative method, it can be
directly calculated by minimizing the photometric error, under
the assumption that the brightness of the pixels in an image
is nearly constant and the displacement between two images
is small [13].
Additionally, the emergence of event cameras and the
differential nature of event data has also led to research in
optical Ô¨Çow prediction from event data. Both optical Ô¨Çow
predictors on pure event data [14], [15] and predictors that use
a combination of conventional camera frames and event datahave been demonstrated [16]. As conventional cameras and
event cameras have some opposing strengths and weaknesses,
for instance, optical Ô¨Çow from event frames suffers from drift,
due to the integration of differential data, whereas conventional
camera frames are unaffected by this drift, but exhibit a lower
dynamic range and a lower frame rate than event camera, [17]
proposes to use a combination of event cameras, conventional
cameras, and IMU data for the calculation of optical Ô¨Çow and
general feature tracking.
C. Hardware Acceleration for Optical Flow Prediction
Computer vision tasks have been a popular target for
hardware acceleration, especially stereo-vision on drones has
been approached by various FPGA implementations. Although
the computing platforms have become much more powerful
ever since, the interest in smaller and smaller drones has led
to continuous research in this area [18], [19].
In the area of optical Ô¨Çow prediction, multiple accelerators
have been developed in recent years. PX4FLOW a near-sensor
accelerator, which consists of a camera and an ARM Cortex
M4F micro-controller for optical Ô¨Çow prediction has been
published almost a decade ago [20]. Recent works show FPGA
implementations for the prediction of optical Ô¨Çow applying
both traditional computer vision algorithms [2] as well as
machine learning algorithms [21]. Furthermore, [22] presents
a hardware accelerator for the prediction of optical Ô¨Çow from
event camera data.
Fig. 1. The VD56G3 image sensor includes an ASIC for the calculation
of optical Ô¨Çow. The shown PCB on the left is part of the evaluation kit. The
sensor returns the captured image and optical Ô¨Çow data as shown on the right.
III. H ARDWARE SETUP AND BACKGROUND
In our characterization, we used the VD56G3 image sensor
of STMicroelectronics depicted in Figure 1, which apart from
the image sensor and optics also hosts an ASIC for the
calculation of optical Ô¨Çow.
A. The VD56G3 Image Sensor
The VD56G3 is a 1.5-megapixel global shutter image sensor
with integrated optical Ô¨Çow motion vector computation. The
image sensor has an 1124-pixel by 1364-pixel resolution and
produces monochrome images at up to 88 frames per second
at the full resolution. At lower resolutions, the image sensor
can reach frame rates as high as 300 frames per second.
To get to lower-resolution images, the original image can be
either cropped or sub-sampled, or a combination of both. The
image sensor supports 2x and 4x sub-sampling and binning, as
well as almost arbitrary cropping, through an area of interest
setting. The optical Ô¨Çow unit can operate on input images upTABLE I
ACHIEVABLE FRAME RATES FOR A GIVEN FRAME HEIGHT AND A GIVEN
NUMBER OF OPTICAL FLOW VECTORS WHEN USING THE 10-BITADC
MODE .
Format Frame Height [pixel] # OF Vectors Frame Rate [1/s]
QVGA 240 1024 338
QVGA 240 2048 288
VGA 480 0 229
VGA 480 1024 205
VGA 480 2048 186
FULL 1364 0 88
FULL 1364 1024 84
FULL 1364 2048 80
to VGA size (640 by 480 pixels), therefore the input image is
automatically down-sampled if it is bigger than the supported
size.
The camera implements the MIPI CSI-2 interface, where
either one or two data lanes can be used. Both the image data
and the optical Ô¨Çow data are transmitted via this interface. The
user can selectively only send image data, optical Ô¨Çow data,
or both. Multiple regions of interest (ROI) can be deÔ¨Åned,
and with those settings, distinct image regions can be selected
for the creation of the image that will be transmitted, the
optical Ô¨Çow calculation, and the auto exposure controller. In
our experiments, we kept all three ROIs identical.
B. Optical Flow Pipeline
The VD56G3 camera employs the FAST algorithm as a
feature detector and the BRIEF algorithm [11] to generate
oriented descriptors on one image level that either corresponds
to the full image or a 2x subsampled image if the recorded
resolution is bigger than VGA. Furthermore, the VD56G3
camera enforces a spread of descriptors across the entire ROI
by limiting the number of descriptors per 16- by 16-pixel
patch, this limit can be in the range of 2 to 8.
The optical Ô¨Çow unit can predict up to 2048 motion vectors
across the full image, which can be achieved by setting the
desired number of BRIEF descriptors up to 2048. This setting
and the actual number of produced BRIEF descriptors are then
fed into a controller that regulates the contrast threshold of
the FAST detector to reach the desired number of descriptors.
After the matching with the BRIEF descriptors of the previous
frame, while adhering to a maximum displacement setting, the
number of effectively found matches, and therefore optical
Ô¨Çow vectors, is typically signiÔ¨Åcantly lower than the number
of the BRIEF descriptors as not all the features can be
matched. This can be due to certain features moving out of the
visible frame, or becoming occluded, additionally, the feature
appearance could also change signiÔ¨Åcantly due to lighting
changes, such that it can no longer be matched.
Using the Hamming distance, the best and second best
match are determined and the displacement to the best Ô¨Åt as
well as both Hamming scores are stored. The ratio between
the two Hamming scores can be used by a later processing
step, to suppress features that are not distinctive enough [10].
One optical Ô¨Çow vector is represented with the following six
X
YZ
Movement of CameraCamera FOVReference Image  
for OF T racking
Rail
Camera  
Connected to PCFig. 2. The setup that was used for the recording of the sequences where
the camera moves parallel to the image plane. To make sure the optical Ô¨Çow
camera has enough features to track, two different printed images were used.
values: the x and y coordinates of the feature in the previous
frame, the difference in coordinates from the previous to the
current frame, and the best and second best Hamming score.
This information is then transmitted via MIPI CSI-2 in lines
of 16 optical Ô¨Çow vectors.
In Table I we indicate the achievable frame rate for a given
frame height and a given number of optical Ô¨Çow vectors in
the 10-bit ADC mode, which ranges between 80 and over 300
frames per second.
IV. D ATASET
To be able to evaluate the VD56G3 sensor, we created
a dataset that contains linear and rotational movement pat-
terns. To better understand the effect of separate movement
directions, we built a setup that creates movement in one of
the degrees of freedom of the camera. Furthermore, to assess
the tracking accuracy in different scenarios, we used multiple
images and color patterns to generate features for the optical
Ô¨Çow unit of the camera.
A. Test Case
For the recording of the dataset, we deÔ¨Åned four different
movement patterns:
linear movement parallel to a planar scene,
linear movement away from a planar scene,
rotation around the focal axis,
stand-still.
To ensure that the camera has enough features to track and
to evaluate the repeatability of the feature detector we used
printed images in A0-size. For the translation experiments, we
used an image of a city as an example of easily distinguishable
features, and an image of a forest with foliage as an example of
features that are hard to track. For the rotation experiments, we
used two different color-patterned wheels. For the linear move-
ments, the camera was moved, for the rotational experiments
we rotated the pattern at variable speeds instead. The setup
where the camera moves parallel to the image plane is depicted
in Fig. 2. For every combination of camera movement, image
for the feature generation, and camera setting we recorded a
10-second sequence of optical Ô¨Çow and image data.TABLE II
PARAMETER SETS OF CAMERA SETTINGS THAT WERE APPLIED DURING THE RECORDING OF THE DATASET .
Parameter Set Camera Resolution X Camera Resolution Y Cropa/Sub-sample Frame Rate BRIEF Target BRIEF Max OF Spatial Point
1 1124 pixel 1364 pixel None 60 FPS 1536 2048 2
2 1120 pixel 1344 pixel Crop (0,0) 60 FPS 1536 2048 2
3b640 pixel 480 pixel Crop (240,432) 140 FPS 768 1024 4
4 560 pixel 672 pixel Crop (280,336) 140 FPS 768 1024 4
5 560 pixel 672 pixel 2x sub-sample 140 FPS 768 1024 4
6 272 pixel 336 pixel Crop (420,504) 240 FPS 384 512 8
7 280 pixel 336 pixel 4x sub-sample 240 FPS 384 512 8
aIf cropping is applied the numbers in brackets indicate the top left corner of the cropped image relative to the full 1124 by 1364 image.
bOperate the optical Ô¨Çow unit at its maximum resolution.
B. Image Sensor Settings
We show a wide range of settings that cover different
potential use cases such as visual (inertial) odometry that
requires a large Ô¨Åeld of view in combination with a moderate
frame rate, but also eye tracking for example for the use in
virtual or augmented reality glasses, which requires a small
Ô¨Åeld of view in combination with a fast frame rate to track
the eye movements. The parameter sets are given in Table II.
As the image sensor has a multitude of possible settings that
can be combined in various ways, seven Ô¨Åxed settings were
chosen for our experiments. Those seven sets of settings were
all selected close to the maximum resolution for a given frame
rate as indicated in Table I while keeping a Ô¨Åxed aspect ratio.
The dimensions indicated in Table II describe the resulting
resolutions of the output image after sub-sampling or cropping.
We select a target for the number of BRIEF descriptors
(BRIEF Target ) that is roughly scaled proportionally to the
image length. Furthermore, if more BRIEF descriptors than
speciÔ¨Åed in the BRIEF Max setting would be generated, those
descriptors are dropped. As the feature detector processes an
image frame from top to bottom, this can result in a lack
of features in the bottom part of the image frame. The OF
Spatial Point setting indicates how many features shall be
detected within every 16 pixels by 16 pixels patch. With this
setting, one can either enforce a more spread-out distribution
of feature points or a high concentration on very distinctive
features depending on the application.
It is important to note that in parameter sets 1 and 2 the
optical Ô¨Çow output is calculated on a 2x down-sampled image
as the original image has a width bigger than 640 pixels. As
a reference, we added parameter set 3 which is exactly 640
pixels wide and therefore utilizes the maximum width that can
be processed by the optical Ô¨Çow unit without down-sampling
the image.
V. E XPERIMENTAL SETUP AND CHARACTERIZATION
In the following, we are describing different test scenar-
ios that were used to verify the accuracy of the hardware-
accelerated optical Ô¨Çow prediction compared to ofÔ¨Çine optical
Ô¨Çow calculation.
A. Dataset Validation
During the acquisition of the linear movement dataset par-
allel to the image plane, we additionally recorded the absoluteposition of the camera setup using a Vicon motion capture
system2. To evaluate the tracking accuracy of the VD56G3
sensor, we compare the actual displacement of the features in
the scene to the estimate of the optical Ô¨Çow unit.
Additionally, we use the standstill dataset to have a look at
the length of the feature tracks, i.e. the concatenation of the
optical over multiple frames Ô¨Çow for a speciÔ¨Åc feature. We
check for how many frames a feature can be tracked and if it
is re-detected in a later frame.
B. Power ProÔ¨Åling
As a last experiment, we measure the power utilization for
three extreme cases of the parameter sets deÔ¨Åned in Table II
namely parameter sets 1, 4, and 6. For better comparison, we
additionally run the parameter sets 4 and 6 at 60 frames per
second. For all cases, we analyze the power draw when optical
Ô¨Çow is being calculated and compare it to the case where the
optical Ô¨Çow calculation is turned off.
The camera sensor is supplied by three voltage rails, an
analog supply with 2.8 volts, a digital supply with 1.8 volts,
and a supply voltage for the sensor core with 1.2 volts. The
speciÔ¨Åc hardware variant of the VD56G3 sensor that we used
(the module of the evaluation kit) hosts an onboard LDO for
the 1.2-volt supply. We therefore only measured the current
on the 2.8-volt and 1.8-volt supplies.
VI. E XPERIMENTAL RESULTS
This section presents the accurate characterization of the
compact and low-power optical Ô¨Çow camera, in terms of
tracking accuracy, tracking repeatability, and power draw.
A. Accuracy
The tracking accuracy was quantitatively analyzed by com-
paring the Vicon ground truth to the average optical Ô¨Çow
that was produced by the VD56G3 sensor. The results of the
linear movement parallel to the scene for the forest image
are given in Figure 3. Generally, one can observe, that the
tracking is most accurate when the camera is conÔ¨Ågured to
have a large resolution and the frame rates and sub-sampling
factors are selected, such that the average optical Ô¨Çow is above
one pixel. As the resolution in parameter set 1 is bigger than
VGA resolution, it is 2x subsampled before the optical Ô¨Çow
2https://www.vicon.com/0 2 4 6 8 10
time [s]10
0speed [px/frame]Estimated Movement in Parameter Set 1
Optical Flow Camera
Vicon Groundtruth
0 2 4 6 8 10
time [s]0500distance [px]Displacement in Pixels using Parameter Set 1
0 2 4 6 8 10
time [s]10
0speed [px/frame]Estimated Movement in Parameter Set 3
0 2 4 6 8 10
time [s]05001000distance [px]Displacement in Pixels using Parameter Set 3
0 2 4 6 8 10
time [s]5
0speed [px/frame]Estimated Movement in Parameter Set 5
0 2 4 6 8 10
time [s]0500distance [px]Displacement in Pixels using Parameter Set 5Fig. 3. The plots on the left show a comparison of the tracked movement speed when moving the camera parallel to the image plane for the parameter sets
1, 3, and 5 when utilizing the image of the forest to generate features. The plots on the right show the total traveled distance in pixels of both the estimate
from the optical Ô¨Çow camera and the Vicon ground truth.
is predicted, whereas the image in parameter set 3 is not
subsampled, therefore although the frame rate of parameter
set 1 is 60 frames per second and 140 frames per second for
parameter set 3 the average movement speeds of the optical
Ô¨Çow vectors are almost the same. From the plot in Figure
3, we can see that the parameter set 1 has slightly lower
tracking accuracy which is caused by the loss in precision
by the subsampling process. In parameter set 5, where both
subsampling and a high frame rate of 140 frames per second
are applied, one can observe an even bigger decrease in
tracking accuracy. The remaining paramter sets conÔ¨Årm the
previous Ô¨Åndings, with parameter set 7 performing worst in the
used scenario due to both the high frame rate and subsampling
factor of 4x.
B. Feature Track Length
In a standstill image with constant illumination, the re-
peatability of the feature tracker of the optical Ô¨Çow unit is
slightly lower than in a reference implementation where the
ORB descriptor is used. Both descriptors are able to produce
long feature tracks with lengths of 30 frames and above,
whereas the density of long feature tracks generated by the
ORB tracker is higher than the one of feature tracks generated
by the tracker of the optical Ô¨Çow sensor. It is worth noting,
that the reference ORB implementation iteratively optimizes
the threshold to select corners for a given frame, whereas the
equivalent threshold of the optical Ô¨Çow sensor is only changed
between two frames.
C. Power ProÔ¨Åle
We measured the 1.8 V and the 2.8 V supplies for the image
sensor. We conducted the analyses on the parameter sets 1, 4,
and 6 of Table II and additionally ran the parameter sets 4 and
6 at 60 frames per second, both with the optical Ô¨Çow prediction
enabled and disabled. We ran the current measurements again
for 10 seconds. When streaming data we arrive at the averagecurrent draw for the 1.8 V and 2.8 V supply as given in
Table III, additionally, the resulting power consumption is also
shown.
The 1.8 V supply is used for the digital logic in the camera,
therefore, we can see a clear increase in current draw, when
the optical Ô¨Çow unit is turned on. Furthermore, the current
draw on the 1.8 V supply depends on the frame rate and the
total number of pixels being processed per second. For the
60 FPS high-resolution case, the camera needs to process 92
Megapixels of image data per second, whereas, in the 240
FPS low-resolution case, only 22 Megapixels of image data
are being processed per second.
On the 2.8 V supply, which is used for supplying the analog
components, like the pixel array, we can see no signiÔ¨Åcant
change in the current draw when enabling the optical Ô¨Çow
unit. When operating the camera at higher frame rates, we
can see a very light increase in the current draw.
For the overall power draw, we can see an increase of 24.3%
(43.05 mW) when enabling the optical Ô¨Çow unit in the 60
FPS high-resolution case. A slightly lower 20.8% (35.48 mW)
increase in power draw is measured for the 240 FPS low-
resolution case. We can observe, that the change in frame rate
has almost no impact on the overall power consumption. One
has to note that with both settings we are operating the camera
close to the maximum speciÔ¨Åcation.
VII. D ISCUSSION
The qualitative rotation experiments show the drawback of a
non-rotation-invariant feature descriptor. Rotations around the
focal axis can only be tracked if they are sufÔ¨Åciently small,
or the camera frame rate is sufÔ¨Åciently high. Otherwise, the
descriptors of the rotated features are altered too much, such
that they can no longer be matched.
The experiments show, that the frame rate of the sensor
needs to be matched with the movement speed of the scene
to obtain satisfying results. If the scene is moving, but theTABLE III
CURRENT DRAW OF THE 1.8 V AND 2.8 V POWER RAIL RESPECTIVELY AS WELL AS THE TOTAL POWER DRAW FOR THE SELECTED PARAMETER SETS .
1.8 V Power Rail 2.8 V Power Rail Total Power Draw
FPS Resolution Optical Flow Off Optical Flow On Optical Flow Off Optical Flow On Optical Flow Off Optical Flow On
60 high 64.10 mA 88.00 mA 21.94 mA 21.95 mA 176.81 mW 219.86 mW
60 medium 56.90 mA 76.23 mA 18.17 mA 18.19 mA 153.30 mW 188.15 mW
60 low 53.77 mA 68.80 mA 16.32 mA 16.37 mA 142.48 mW 169.68 mW
140 medium 65.73 mA 93.79 mA 23.84 mA 23.91 mA 185.07 mW 235.77 mW
240 low 58.20 mA 77.74 mA 23.65 mA 23.76 mA 170.98 mW 206.46 mW
movement is below a pixel per frame, poor tracking accuracy
is reached. For optimal results, one can adjust the frame
rate and the sub-sampling factor, depending on the average
movement speed of the scene. As the sensitivity of the feature
detector is updated only once per frame and not iteratively on a
single frame, it sometimes fails to detect features at all, which
results in a movement speed prediction of zero in Figure 3 for
parameter sets 3 and 5.
If we compare the added power draw of the optical Ô¨Çow
unit of 43.05 mW against other low-power, high-frame-rate
optical Ô¨Çow implementations, such as [20], which is an MCU
implementation with a similar power envelope targeted at
drone applications, we can conÔ¨Årm that the VD56G3 sensor
is suitable for drone applications. Furthermore, thanks to its
ASIC implementation of a feature-based optical Ô¨Çow estimator
the VD56G3 sensor produces signiÔ¨Åcantly more optical Ô¨Çow
data than the implementation proposed in [20], which applies
patch matching on a 64- by 64-pixel image at up to 250 frames
per second.
VIII. C ONCLUSION
This paper presented and characterized a novel optical Ô¨Çow
camera that leverages on-sensor acceleration. With accurate
experimental evaluation, the paper demonstrated the low la-
tency, the low power, and the accuracy of the optical Ô¨Çow
prediction. Furthermore, guidelines on optimal conÔ¨Åguration
and indications on how to adapt them are provided. We
have shown the potential in low-latency applications and
especially for low-power edge applications, where strict energy
constraints and fast movements are demanding an energy-
efÔ¨Åcient and low-latency solution.
REFERENCES
[1] S. Jiang, C. Jiang, and W. Jiang, ‚ÄúEfÔ¨Åcient structure from motion for
large-scale uav images: A review and a comparison of sfm tools,‚Äù ISPRS
Journal of Photogrammetry and Remote Sensing , vol. 167, pp. 230‚Äì251,
2020.
[2] K. Blachut and T. Kryjak, ‚ÄúReal-time efÔ¨Åcient fpga implementation of
the multi-scale lucas-kanade and horn-schunck optical Ô¨Çow algorithms
for a 4k video stream,‚Äù Sensors , vol. 22, no. 13, p. 5017, 2022.
[3] J. Delmerico, T. Cieslewski, H. Rebecq, M. Faessler, and D. Scaramuzza,
‚ÄúAre we ready for autonomous drone racing? the uzh-fpv drone racing
dataset,‚Äù in 2019 International Conference on Robotics and Automation
(ICRA) . IEEE, 2019, pp. 6713‚Äì6719.
[4] D. Palossi, A. Loquercio, F. Conti, E. Flamand, D. Scaramuzza, and
L. Benini, ‚ÄúA 64-mw dnn-based visual navigation engine for autonomous
nano-drones,‚Äù IEEE Internet of Things Journal , vol. 6, no. 5, pp. 8357‚Äì
8371, 2019.
[5] Y . Lu, Z. Xue, G.-S. Xia, and L. Zhang, ‚ÄúA survey on vision-based uav
navigation,‚Äù Geo-spatial Information Science , vol. 21, no. 1, pp. 21‚Äì32,
2018.[6] A. Merzlyakov and S. Macenski, ‚ÄúA comparison of modern general-
purpose visual slam approaches,‚Äù in 2021 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems (IROS) . IEEE, 2021, pp.
9190‚Äì9197.
[7] J. Shi et al. , ‚ÄúGood features to track,‚Äù in 1994 Proceedings of IEEE
conference on computer vision and pattern recognition , IEEE. Seattle,
WA, USA: IEEE, 1994, pp. 593‚Äì600.
[8] C. Harris, M. Stephens et al. , ‚ÄúA combined corner and edge detector,‚Äù
inAlvey vision conference , vol. 15, Citeseer. Manchester, UK: Alvey
Vision Club, 1988, pp. 10‚Äì5244.
[9] E. Rublee, V . Rabaud, K. Konolige, and G. Bradski, ‚ÄúOrb: An efÔ¨Åcient
alternative to sift or surf,‚Äù in 2011 International Conference on Computer
Vision . Barcelona,Spain: IEEE, 2011, pp. 2564‚Äì2571.
[10] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant keypoints,‚Äù
International journal of computer vision , vol. 60, no. 2, pp. 91‚Äì110,
2004.
[11] M. Calonder, V . Lepetit, C. Strecha, and P. Fua, ‚ÄúBrief: Binary robust
independent elementary features,‚Äù in Computer Vision ‚Äì ECCV 2010 .
Springer, 2010, pp. 778‚Äì792.
[12] F. Fraundorfer and D. Scaramuzza, ‚ÄúVisual odometry : Part ii: Matching,
robustness, optimization, and applications,‚Äù IEEE Robotics & Automa-
tion Magazine , vol. 19, no. 2, pp. 78‚Äì90, 2012.
[13] Z. Wang and X. Yang, ‚ÄúMoving target detection and tracking based
on pyramid lucas-kanade optical Ô¨Çow,‚Äù in 2018 IEEE 3rd Interna-
tional Conference on Image, Vision and Computing (ICIVC) , IEEE.
Chongqing, China: IEEE, 2018, pp. 66‚Äì69.
[14] M. Z. Khairallah, F. Bonardi, D. Roussel, and S. Bouchafa, ‚ÄúPca event-
based optical Ô¨Çow: A fast and accurate 2d motion estimation,‚Äù in 2022
IEEE International Conference on Image Processing (ICIP) , IEEE.
Bordeaux, France: IEEE, 2022, pp. 3521‚Äì3525.
[15] G. Gallego, H. Rebecq, and D. Scaramuzza, ‚ÄúA unifying contrast
maximization framework for event cameras, with applications to mo-
tion, depth, and optical Ô¨Çow estimation,‚Äù in Proceedings of the IEEE
conference on computer vision and pattern recognition . Salt Lake
City, UT, USA: IEEE/CVF, 2018, pp. 3867‚Äì3876.
[16] A. S. Lele and A. Raychowdhury, ‚ÄúFusing frame and event vision for
high-speed optical Ô¨Çow for edge application,‚Äù in 2022 IEEE Interna-
tional Symposium on Circuits and Systems (ISCAS) . IEEE, 2022, pp.
804‚Äì808.
[17] A. R. Vidal, H. Rebecq, T. Horstschaefer, and D. Scaramuzza, ‚ÄúUltimate
slam? combining events, images, and imu for robust visual slam in
hdr and high-speed scenarios,‚Äù IEEE Robotics and Automation Letters ,
vol. 3, no. 2, pp. 994‚Äì1001, 2018.
[18] Z. Lu, J. Wang, Z. Li, S. Chen, and F. Wu, ‚ÄúA resource-efÔ¨Åcient
pipelined architecture for real-time semi-global stereo matching,‚Äù IEEE
Transactions on Circuits and Systems for Video Technology , vol. 32,
no. 2, pp. 660‚Äì673, 2021.
[19] Z. Wan, B. Yu, T. Y . Li, J. Tang, Y . Zhu, Y . Wang, A. Raychowdhury,
and S. Liu, ‚ÄúA survey of fpga-based robotic computing,‚Äù IEEE Circuits
and Systems Magazine , vol. 21, no. 2, pp. 48‚Äì74, 2021.
[20] D. Honegger, L. Meier, P. Tanskanen, and M. Pollefeys, ‚ÄúAn open source
and open hardware embedded metric optical Ô¨Çow cmos camera for in-
door and outdoor applications,‚Äù in 2013 IEEE International Conference
on Robotics and Automation . Karlsruhe, Germany: IEEE, 2013, pp.
1736‚Äì1741.
[21] Y . Yan, Y . Ling, K. Huang, and G. Chen, ‚ÄúAn efÔ¨Åcient real-time
accelerator for high-accuracy dnn-based optical Ô¨Çow estimation in fpga,‚Äù
Journal of Systems Architecture , vol. 136, p. 102818, 2023.
[22] D. C. Stumpp, H. Akolkar, A. D. George, and R. B. Benosman, ‚ÄúHarms:
A hardware acceleration architecture for real-time event-based optical
Ô¨Çow,‚Äù IEEE Access , vol. 10, pp. 58 181‚Äì58 198, 2022.