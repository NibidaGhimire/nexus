On the feasibility of attacking Thai LPR systems
with adversarial examples
Chissanupong Jiamsuchon
College of Computing
Prince of Songkla University
Phuket, Thailand
s6230613001@phuket.psu.ac.thJakapan Suaboot
College of Computing
Prince of Songkla University
Phuket, Thailand
jakapan.su@phuket.psu.ac.thNorrathep Rattanavipanon
College of Computing
Prince of Songkla University
Phuket, Thailand
norrathep.r@phuket.psu.ac.th
Abstract —Recent advances in deep neural networks (DNNs)
have signiﬁcantly enhanced the capabilities of optical character
recognition (OCR) technology, enabling its adoption to a wide
range of real-world applications. Despite this success, DNN-
based OCR is shown to be vulnerable to adversarial attacks, in
which the adversary can inﬂuence the DNN model’s prediction
by carefully manipulating input to the model. Prior work has
demonstrated the security impacts of adversarial attacks on
various OCR languages. However, to date, no studies have been
conducted and evaluated on an OCR system tailored speciﬁcally
for the Thai language. To bridge this gap, this work presents a
feasibility study of performing adversarial attacks on a speciﬁc
Thai OCR application – Thai License Plate Recognition (LPR).
Moreover, we propose a new type of adversarial attack based
on the semi-targeted scenario and show that this scenario is
highly realistic in LPR applications. Our experimental results
show the feasibility of our attacks as they can be performed on a
commodity computer desktop with over 90% attack success rate.
Index Terms —adversarial attacks, Thai OCR systems, Thai
LPR systems, machine learning security
I. I NTRODUCTION
Optical character recognition (OCR) is a technology to
recognize characters from printed or handwritten images. In
the last few decades, OCR has been adopted in many real-
world applications mainly due to the rise of deep neural
network (DNN) development. With DNN, OCR can now
perform the character recognition task at high speed, enabling
its use in many mission-critical and time-sensitive applications.
For instance, an OCR system can be deployed in an airport to
recognize passport information automatically [1]; or modern
license plate recognition systems employed by law enforce-
ment rely heavily on OCR in their core engine [9].
Besides the timing performance, the security of OCR is also
paramount to the underlying application. Unfortunately, OCRinherits the same security weakness as DNN since it is also
vulnerable to an attack based on adversarial examples [8]. The
aim of this attack is to confuse the DNN model, causing it to
misclassify a speciﬁc input image. It is typically carried out by
introducing subtle but deliberate changes to the input. These
changes can be in the form of noise perturbation or small pixel
images that are carefully crafted in such a way that they do
not look suspicious to the human eyes. As OCR has become
widely adopted, it presents more incentives for an adversary to
use this type of attack for his/her own beneﬁt. This attack, for
instance, can cause the OCR model to misinterpret passport
data, license plate numbers, or ﬁnancial documents, resulting
in ﬁnancial damages or crime detection avoidance.
A number of prior works explore different techniques to
generate adversarial examples in black-box [10] and white-
box [7] environments, in targeted [15] and untargetd [11] sce-
narios, and with different OCR languages, e.g., English [14],
Chinese [5], and Arabic [2]. Despite this rich literature, to
the best of our knowledge, there has been no prior work to
demonstrate the attack success on an OCR system based on
Thai language. Due to the idiosyncratic features of the Thai
alphabet (e.g., some letters contain an upper/lower symbol
–/uni0E2E//uni0E0D), it remains unclear whether these existing attack
techniques are still effective for Thai OCR systems.
To this end, we set out to answer this question by demon-
strating whether it is feasible to generate adversarial examples
that can be used to fool the state-of-the-art Thai OCR system.
To achieve this goal, we turn our attack focus to a speciﬁc
but widely-used OCR application – License Plate Recognition
(LPR) system. In particular, our attack targets an LPR system
based on Google Tesseract [13] with Thai language support.
Contrary to the previous works in [15] or [11], we consider
our LPR attack scenario semi-targeted , in which a successfularXiv:2301.05506v1  [cs.CR]  13 Jan 2023adversarial example can mislead the LPR model to output any
element in the set of adversary-chosen incorrect classes (e.g.,
a set of valid license numbers other than the true number).
This is distinct from the targeted scenario, which aims to
misguide the model to return a particular adversary-chosen
incorrect class (e.g., a speciﬁc fake license number), or the
untargeted scenario, which tricks the model into predicting
any of the incorrect classes (e.g., any sequence of Thai
characters/digits other than the true license number). We also
propose a transformation that converts the existing targeted
attack into the semi-targeted attack considered in this work.
Finally, we perform implementation experiments to evaluate
our proposed LPR attack. The results indicate the realism of
our attack as it obtains a high attack success rate and requires
only a reasonable amount of resources (i.e., runtime and RAM
usage) that can feasibly be acquired from a regular desktop
computer. Overall, we believe this work represents the ﬁrst
step towards raising awareness of the threats posed by Thai
OCR systems and eventually towards securing these systems
against adversarial examples.
The contribution of our work can be summarized as follows:
(i) We present a systematic approach to demonstrate the
feasibility of constructing adversarial examples to fool
the state-of-the-art Thai OCR-based LPR system.
(ii) We explore an alternative attack scenario, called semi-
targeted, and show it is highly realistic for attacking LPR
applications.
(iii) Our evaluation results show the feasibility of our attack; it
can achieve up to 91% attack success rate and can be car-
ried out realistically using only a commodity computer.
II. B ACKGROUND AND RELATED WORK
A. License Plate Recognition (LPR)
LPR is the process that automatically reads and extracts
vehicle license plate information from an image. It typically
consists of three steps: localization, segmentation, and iden-
tiﬁcation. In the ﬁrst step, an LPR system scans through
the entire image to detect and locate a license plate. Then,
the segmentation step extracts the regions from the detected
license plate where each region contains exactly a single
character. Finally, LPR leverages OCR technology to classify
and recognize each character and outputs the digitized license
information in the identiﬁcation step.
While numerous OCR techniques have been proposed for
LPR systems, the most common one used by modern LPR
systems is based on DNNs. For example, Tesseract [13] is the
state-of-the-art DNN-based OCR engine developed by Googleand has been used in many LPR systems [12]. The current
version of Tesseract uses LSTM DNNs and supports more
than 50 languages, including Thai. Besides LPR, Tesseract has
been adopted to recognize Thai characters in other settings,
e.g., Thai document digitization [6].
B. Adversarial Attacks
An adversarial attack was ﬁrst introduced and investigated
by Szegedy et al. in 2013 [15]. They show that by optimizing
DNN’s prediction error, an adversary can generate a small
perturbation that can be applied to an input image in such a
way that the resulting image (called an adversarial example )
is misclassiﬁed by the DNN model. The work in [15] has
inspired many subsequent studies to improve upon, and/or
proposed different settings for, adversarial attacks. Techniques
in adversarial attacks can often be categorized using two
orthogonal dimensions – adversarial knowledge and goal:
1)Adversarial knowledge can be further divided into
white-box and black-box environments. White-box at-
tacks assume a powerful adversary that has complete
knowledge of the DNN model’s architecture, including
parameters, weight values, and/or its training dataset.
Black-box attacks, on the other hand, consider a weaker
adversary which can only query the DNN model but has
no access to the model’s internal information.
2)Adversarial goal is often classiﬁed as either targeted
or untargeted scenarios. Targeted attacks aim to deceive
the model into classifying an adversarial example as a
targeted adversarial class, whereas an untargeted attack
misleads the classiﬁcation to an arbitrary class other than
the correct one.
Prior works have explored various techniques for adversarial
example generation targeting OCR systems with: (i) black-
box [3] and white-box [14] environments, (ii) targeted [5] and
untargeted [16] scenarios, and (iii) English [14], Chinese [5],
and Arabic [2] languages. In this work, we aim to assess
the feasibility of performing an adversarial attack in Thai
LPR systems with a realistic black-box and semi-targeted
adversarial setting.
III. A DVERSARY ’SGOAL & T HREAT MODEL
We consider a realistic adversary which aims to trick an
automatic LPR system to misclassify a speciﬁc potentially
illegal license plate into a different but still valid (i.e., well-
formed) license number. The adversary is assumed to have or-
acle access to the black-box LPR model, i.e., he/she can query
for the model’s prediction output on any given image input.However, as the model is usually proprietary and conﬁdential,
he/she has no access to the model’s internal parameters.
Figure 1 shows a scenario for performing an adversarial
attack on a Thai LPR system. The attack is carried out by
generating an adversarial example from an illegal license plate.
Then, it is considered a successful attack if the following
requirements hold:
I l l e g a l  
L i c e n s e  P l a t e
A d v e r s a r i a l  
A t t a c kA d v e r s a r i a l  
E x a m p l eA d v e r s a r yU n s u s p i c i o u sC r i m e  R e c o r d    
    U n s u s p i c i o u s             C r i m e                   
    R e c o r d
D e t e c t e d
C l e a n
ก ข  4 5 2 3จ ช  1 6 4 5จ ช  1 6 4 5จ ช  1 6 4 5
L P R  s y s t e m  c o m p r o m i s e d !
Figure 1. Adversarial attacks on Thai LPR systems
[R1] The generated adversarial example looks similar to
the illegal license plate input in human eyes. This is to ensure
that only a small change needs to be applied on the physical
license plate, and as a result, the modiﬁed license plate can
still fool the LPR system without being noticed by humans.
[R2] The adversarial example’s prediction class is differ-
ent from its true class but still considered a valid license
number. The rationale behind this requirement is that to
better evade detection, the adversary wants to avoid the DNN
model returning an invalid and thus suspicious class, e.g., a
malformed/unassigned license number since it can easily be
detected in software or by police ofﬁcers.
Without loss of generality, we simplify [R2] by considering
a license number valid if it consists of two Thai consonants
followed by a four-digit number. For example, /uni0E21/uni0E043456 is valid
but/uni0E21/uni0E01/uni0E381234 or/uni0E21/uni0E04123 are not. In practice, [R2] can be satisﬁed
by using a database of legal license plate numbers.
Due to [R2] , it becomes clear that the traditional targeted
and untargeted scenarios are not directly suitable in this attack
setting. Speciﬁcally, the untargeted scenario could return an
invalid number (e.g., /uni0E21/uni0E04123 ), violating [R2] ; whereas the
targeted scenario can be too restrictive. Hence, in this work,
we introduce a relaxed concept of the targeted scenario, called
semi-targeted , which accepts an adversarial example if its
prediction class falls into a speciﬁc adversary-chosen set (as
opposed to a speciﬁc class in the targeted scenario), e.g., a set
of valid license numbers in the LPR application.IV. M ETHODOLOGY
A. Overview
Our methodology for attacking Thai OCR systems consists
of two phases, as shown in Figure 2. The ﬁrst phase performs
the black-box semi-targeted adversarial attack on an input
license plate image and outputs an adversarial example.
Figure 2. Methodology for attacking Thai OCR systems
The second phase takes as input, the adversarial example,
and evaluates whether this adversarial example constitutes a
successful attack or not. We now discuss each phase in detail.
B. Phase-1: Black-box Semi-targeted Adversarial Attack
As illustrated in Figure 3, our black-box semi-targeted
attack requires three input parameters: (1) an original image
–img; (2) a set of valid classes – s; and (3) the number of
candidates to be considered in this attack – n. In the context
of LPR, imgrepresents a license plate image; scorresponds to
a set of valid license numbers, where, in this work, sis set to
common license patterns in Thailand with two Thai consonants
followed by a four-digit number.
The attack starts in Ê. It generates nclasses from the
given input with a constraint that all of these nclasses
must: (1) be non-repetitive and (2) contain at least one Thai
consonant different from the imgclass. Then, we can apply the
state-of-the-art black-box targeted attack for each individual
class, resulting in ncandidates for adversarial examples in Ë.
Finally, in Ì, we display these ncandidates to the user, ask
the user to select the one that is closely similar to img, and
output it as the adversarial example.
Note that this phase will always yield the adversarial
example satisfying [R2] . This is because the targeted attack
inËguarantees to produce an adversarial example that will be
classiﬁed as the targeted class class i, which, by construction
inÊ, is valid (i.e., class i2s) and different from the imgclass.
C. Phase-2: Adversarial Example Assessment
To assess the generated adversarial example, we recruit par-
ticipants from our university, present them with the adversarial
example image, and interview them with two questions:
Q1: Are all characters legible in the presented image?
Q2: What license number can you read from the image?Figure 3. Black-box semi-targeted attacks
The attack is considered successful if the participant re-
sponds “yes” to the ﬁrst question and the answer from the
second question matches the license number in img. If any of
these conditions are not fulﬁlled, we return “Attack failure”. As
a result of these two carefully-crafted questions, the adversarial
example can only pass this phase when still resembling img,
thus satisfying [R1] .
V. F EASIBILITY RESULTS
A. Experimental Setup
All of our experiments were conducted on an Ubuntu
20.04 machine with an Intel i7-11700k CPU@3.60 GHz. To
measure the attack success rate, we performed our attack on
100 unique software-generated Thai license plate images. The
OCR system used in our attack was based on Tesseract v5.2.0
and ran with the following parameters: psm=10,oem=1 .
Lastly, we used HopSkipJumpAttack [4] as the underlying
black-box targeted attack algorithm; for each sample, we ran
this attack until it reached 300 iterations.
Ethics. Our experiments were conducted using synthetic,
instead of real, license plates for ethical reasons. This work
was conducted solely for academic purposes and we do
not condone using it for real-world attacks. Further, we did
not gather any personally identiﬁable information during our
interviews with participants.
B. Experimental Results
Attack Success Rate (ASR). Figure 4 shows ASR of our
attack while varying n. ASR improved drastically as we moved
from the targeted attack ( n=1) to the semi-targeted attack
(n>1), with ASR=91% for n=10, compared to ASR=
70% for n=1. This highlights the effectiveness of the semi-
target scenario for attacking Thai OCR systems. We present
a selection of generated adversarial examples for various n
values in Table I, where Suc. refers to “Attack success".
Figure 4. Attack success rate and execution time
Attack Resource Consumption. In terms of resource con-
sumption, generating adversarial examples requires a moderate
amount of RAM (1:8 2GB) on our machine, independent
of the nvalue. On the other hand, the runtime for adversar-
ial example generation linearly depends on n, as shown in
Figure 4. For n=10, the attack takes less than 2 hours to
complete, which we consider to be reasonable because it only
needs to be done once for any given license plate.
VI. C ONCLUSION
This paper presents the ﬁrst feasibility study of performing
adversarial attacks on Thai OCR-based LPR systems. In
addition, it proposes a new type of attack scenario, called
semi-targeted , and argues that this scenario is more practical
for attacking LPR systems than the traditional targeted and
untargeted scenarios. Our experiments demonstrate the feasi-
bility of our attack as it achieves a high success rate and can
be carried out only using a commodity computer.Table I
SAMPLES OF ADVERSARIAL EXAMPLES
Sample n=1 n=5 n=10
Input Image Adv. Ex. OCR Out. Suc. Adv. Ex. OCR Out. Suc. Adv. Ex. OCR Out. Suc.
/uni0E21/uni0E044364 7
 /uni0E21/uni0E044364 7
 /uni0E21/uni0E284364 7
/uni0E25/uni0E281805 7
 /uni0E25/uni0E2B1805 7
 /uni0E25/uni0E211805 3
/uni0E08/uni0E2A1645 7
 /uni0E08/uni0E0B1645 3
 /uni0E08/uni0E0B1645 3
/uni0E0B/uni0E1D9597 3
 /uni0E0B/uni0E1D9597 3
 /uni0E0B/uni0E1D9597 3
REFERENCES
[1] Airport Supplier. Passport & ID VIZ OCR and authentication
software. https://www.airport-suppliers.com/product/passport-id-viz-ocr-
and-authentication-software/, 2022.
[2] Basemah Alshemali and Jugal Kalita. Adversarial examples in arabic.
InCSCI , pages 371–376, Las Vegas, NV , USA, 2019.
[3] Samet Bayram and Kenneth Barner. A black-box attack on optical
character recognition systems. arXiv:2208.14302 , 2022.
[4] Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hop-
skipjumpattack: A query-efﬁcient decision-based attack. In 2020 ieee
symposium on security and privacy (sp) , pages 1277–1294. IEEE, 2020.
[5] Lu Chen and Wei Xu. Attacking optical character recognition (ocr)
systems with adversarial watermarks. arXiv:2002.03095 , 2020.
[6] Todsanai Chumwatana and Waramporn Rattana-umnuaychai. Using ocr
framework and information extraction for thai documents digitization.
IniEECON2021 , pages 440–443, Pattaya, Thailand, 2021.
[7] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotﬂip:
White-box adversarial examples for text classiﬁcation. arXiv preprint
arXiv:1712.06751 , 2017.
[8] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining
and harnessing adversarial examples. arXiv:1412.6572 , 2014.
[9] IACP. Automated license plate recognition.
https://www.theiacp.org/projects/automated-license-plate-recognition,
2022.
[10] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-
box adversarial attacks with limited queries and information. In ICML ,
pages 2137–2146, 2018.
[11] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal
Frossard. Deepfool: a simple and accurate method to fool deep neural
networks. In CVPR , pages 2574–2582, Las Vegas, NV , USA, 2016.
[12] Rahul R Palekar, Sushant U Parab, Dhrumil P Parikh, and Vijaya N
Kamble. Real time license plate detection using opencv and tesseract.
InICCSP , pages 2111–2115, Chennai, India, 2017.
[13] Ray Smith. An overview of the tesseract ocr engine. In ICDAR , pages
629–633, Curitiba, Brazil, 2007.
[14] Congzheng Song and Vitaly Shmatikov. Fooling ocr systems with
adversarial text images. arXiv:1802.05385 , 2018.
[15] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties
of neural networks. arXiv:1312.6199 , 2013.
[16] Mingming Zha, Guozhu Meng, Chaoyang Lin, Zhe Zhou, and Kai Chen.
Rolma: a practical adversarial attack against deep learning-based lpr
systems. In Inscrypt , pages 101–117, Guangzhou, China, 2020.