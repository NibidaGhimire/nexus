Ambient Intelligence for Next-Generation AR
Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
Abstract Next-generation augmented reality (AR) promises a high degree of context-awareness – a detailed knowledge
of the environmental, user, social and system conditions in which an AR experience takes place. This will facilitate both
the closer integration of the real and virtual worlds, and the provision of context-speciﬁc content or adaptations. However,
environmental awareness in particular is challenging to achieve using AR devices alone; not only are these mobile devices’
view of an environment spatially and temporally limited, but the data obtained by onboard sensors is frequently inaccurate
and incomplete. This, combined with the fact that many aspects of core AR functionality and user experiences are impacted
by properties of the real environment, motivates the use of ambient IoT devices , wireless sensors and actuators placed in the
surroundingenvironment,forthemeasurementandoptimizationofenvironmentproperties.Inthisbookchapterwecategorize
and examine the wide variety of ways in which these IoT sensors and actuators can support or enhance AR experiences,
including quantitative insights and proof-of-concept systems that will inform the development of future solutions. We outline
the challenges and opportunities associated with several important research directions which must be addressed to realize the
full potential of next-generation AR.
1 Introduction
While virtual content in the metaverse is designed to be immersive, it will not be experienced in isolation. In particular for
augmented reality (AR), in which virtual content is integrated with our real-world surroundings, an accurate and complete
understanding of the real environment is a prerequisite for high quality experiences. Obtaining this using AR devices alone
is infeasible in many scenarios, raising the potential for employing external sensors placed in the surrounding environment, a
form ofambient intelligence for AR. As well as sensing the properties of an environment, it is also desirable to control them,
for example to optimize the performance of core AR algorithms, or to generate stimuli in sensory modalities that are beyond
the capabilities of AR devices. In this book chapter we explore the potential for wireless Internet of Things (IoT) devices to
provide this type of ambient intelligence, and thereby support next-generation AR experiences in the metaverse.
More well-studied than AR is virtual reality (VR), in which users are immersed in a fully virtual environment through
visual and auditory stimuli, usually delivered via a headset (e.g., Meta Quest 2, HTC Vive), or alternatively a desktop or
mobiledevice.Toprovidetactilefeedbackandinteractionmethodstotheuser,VRheadsetsarefrequentlyusedincombination
with specialized controllers or other handheld devices, and less commonly with other peripherals such as gloves, body suits,
helmets[76]andmouthaccessories[186].ThereisworkonintegratingexternaldevicesintoVRexperiencestofurtherincrease
the levels of immersion, beyond the capabilities of wearable devices. For example, fans have been used for representations of
wind[43,62],whilemultipleworks(e.g.,[170])haveused‘climatechambers’withHVACsystemstostudythermalperception
Tim Scargill
Electrical and Computer Engineering Department, Duke University, Durham, NC, USA e-mail: ts352@duke.edu
Sangjun Eom
Electrical and Computer Engineering Department, Duke University, Durham, NC, USA, e-mail: sangjun.eom@duke.edu
Ying Chen
Electrical and Computer Engineering Department, Duke University, Durham, NC, USA, e-mail: ying.chen151@duke.edu
Maria Gorlatova
Electrical and Computer Engineering Department, Duke University, Durham, NC, USA, e-mail: maria.gorlatova@duke.edu
1arXiv:2303.12968v2  [cs.HC]  24 Mar 20232 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
in immersive virtual environments. External devices have also been employed to capture contextual data from users or the
ambient environment to enhance VR experiences (e.g., [109,161]).
Herewefocusontheless-studiedtopicof ambientintelligencetosupportorenhanceAR ,inwhichvirtualcontentisoverlaid
onto and integrated with a user’s real-world surroundings. Speciﬁcally, we examine this in the context of mobile AR, when
virtual content is presented through handheld devices such as a smartphone or tablet, or wearable devices such as a headset.
As in VR, current AR devices deliver virtual content primarily in the form of visual and auditory stimuli, with limited tactile
feedback sometimes available in the case of handheld devices or headsets with handheld controllers (e.g., the Magic Leap 2).
Thisagainraisesthepossibilityofusingexternaldevicestoadjusttherealenvironmentinothersensorymodalitiesortoextend
possibleinteractionmethods.However,thereisalsoanimportantdistinctionbetweentwomethodsofdeliveringvisualcontent
inAR;handhelddevicesandsomeheadsets(e.g.,theVarjo XR-3)usevideosee-through(VST,sometimesreferredtoasvideo
pass-through) displays, while other headsets (e.g., the Microsoft HoloLens 2 and the Magic Leap 2) use optical see-through
(OST) HMDs. These diﬀerent designs have important implications for how the properties of real environments aﬀect a user’s
perceptionofvirtualvisualcontent.Ingeneral,aknowledgeofhowtherealenvironmentaﬀectsbothsystemfunctionalityand
humanperception,alongwiththeabilitytosenseandcontrolenvironmentproperties,willenabletheprovisionofoptimalAR
experiences in diverse scenarios.
Furthermotivationforpursuingambientintelligencecomesfromthenatureofnext-generationAR.Thecomingyearshold
the promise of virtual content that is not only more closely integrated with our real surroundings, but which also adapts to
the current context in which it is presented. This context-awareness is key to realizing the full potential of AR; to deliver
virtual content that provides a high-quality user experience, and enables optimal task performance, we require information
on the speciﬁc circumstances in which it is presented [69]. At a high level, we can break down this contextual information
into environmental, user, social, and system awareness. Environmental awareness, including the environment understanding
requiredforcloseintegrationofrealandvirtualcontent,isparticularlychallengingtoobtainusingthesensorsonboardmobile
ARdevicesalonebecausetheirviewofanenvironmentis spatiallyandtemporallylimited ;theytypicallyonlycaptureasmall
portion of an environment for a short period of time. Furthermore, due to restrictions on the quality of onboard sensors, and
the fact that they are frequently in motion, the data they capture is often inaccurate. External devices on the other hand can
help to address these deﬁciencies and generate more accurate, more complete environmental awareness, across a wider range
of conditions.
The desire to both improve the accuracy and completeness of environmental awareness in AR, and control environment
properties,motivatestheuseofInternetofThings(IoT)devices.Thesewirelesslyconnecteddeviceshavebecomeubiquitous
acrossdiversesettings(globallytherewere11billionactiveIoTdevicesattheendof2021,andthisisforecasttobe29billion
by 2030 [137]) and include a wide range of sensors and actuators that are suitable for detecting and adjusting environmental
conditions. The widespread availability and relatively low cost of IoT sensors like smart cameras and IoT actuators like smart
lightsanddisplays,aswellastheprevalenceofsupportinginfrastructure(e.g.,Wi-Finetworks),meansthattheycanbereadily
leveraged for AR. Our overall vision, illustrated in Figure 1, is for multi-device AR architectures, in which experiences on
mobile AR devices are enhanced or supported by a set of connected devices. Contextual data are provided by IoT sensors
such as smart cameras and wearables, while IoT actuators such as smart lights, shades, and displays optimize environmental
conditionsforAR,orenhanceARexperiencesbyprovidingadditionalstimuli.Anedgeserverorthecloudprovidesthestorage
and resources to aggregate these data, compute context, and control IoT devices.
Fig.1:Ahigh-levelviewofamulti-devicenext-generationARarchitecture,inwhichdatafrommobileARdevices,wearables,
and IoT sensors like smart cameras are aggregated to compute context (e.g., environment properties) on an edge or cloud
server.ThesedataarealsousedtoinformthecontrolofIoTactuatorssuchassmartlights,shades,anddisplays,whichoptimize
environmental conditions for AR experiences.Ambient Intelligence for Next-Generation AR 3
We deﬁne the scope of the IoT devices we cover in this book chapter through two important distinctions. Firstly, our focus
is onIoT devices that support or enhance AR , rather than simply any IoT device whose data could be displayed in AR or be
controlled through an AR interface. For example, while there are interesting and useful ways in which devices such as air
qualitysensorsandroboticarmscouldbecombinedwithAR,theyareexcludedhere.Examplesofworkswhichproposedthe
use of AR as a tool for visualizing IoT-generated data and interacting with IoT nodes include [84,127,151,211]. Secondly,
we only consider truly external or ambientIoT devices; we exclude sensors that are attached to AR devices, such as inertial
sensors or eye tracking cameras, as well as wearable sensors that may capture additional biometrics from humans in an AR
environment.ForarecentreviewofwearablesensorsforARapplications,see[92].GiventheplacementofambientIoTdevices
in the wider environment they are particularly beneﬁcial for environment awareness, though we note cases in which they can
supply other types of context-awareness; for example, while user context data is most often captured through on-device or
wearable sensors, ambient IoT cameras can also capture visual data pertinent to activity or emotion recognition [175].
The contents of this book chapter are as follows. In Section 2, we cover related work on combining AR and IoT devices,
methods of communication between them, and a network architecture that will support the implementation of ambient
intelligenceforAR,edgecomputing.InSection3,wecategorizethediﬀerentwaysinwhichambientIoTdevicescansupport
or enhance AR experiences, including relevant sensors and actuators for each use case; then in Sections 4 and Sections 5, we
discussinmoredetailthepossibilitiesforIoTsensorsandactuatorsrespectively,organizedbyusecase.InSection6wecover
open challenges and research directions, and in Section 7 we provide a conclusion.
2 Related Work
In thissection wereview existing workon systemswhich incorporate bothAR andIoT devices, diﬀerentmethods ofcommu-
nication that have been used to connect AR and the IoT, and a network architecture that will support the implementation of
ambient intelligence for next-generation AR.
SystemsincorporatingARandIoTdevices: ARcanbeusedtoenhanceinteractionswithIoTdevices,includingboththe
visualization of IoT data and the provision of an immersive interface to access and control IoT devices. For example, in [154]
theauthorsdescribedanAR-IoTsystemthatdisplaysreal-timeIoTdataasholographiccontenttoenhanceobjectinteractions;
this system is applied to crop monitoring, to provide object coordinates of plants and information collected from IoT devices
suchasthefertilizersused.Similarly,Sunetal.[190]presentedMagicHand,anARsystemthatallowsuserstointeractwithIoT
devicesbydetecting,localizing,andenablingaugmentedhandcontrols;thesystemwasimplementedusinga2Dconvolutional
neural network (CNN) and achieved high gesture recognition accuracy. In [191] IoT sensor data were overlaid onto industrial
machinesusingAR,withmoreaccurateposeestimates(andtherebybetteralignedoverlays)obtainedbyapplyingdeeplearning
to RGB and depth images of the machine. Visualizing and identifying IoT objects using an AR interface has also been shown
to improve shopping experiences, by increasing perceived usability and satisfaction in user interactions [85].
Communication between AR and IoT devices: Prior works have demonstrated how AR devices can communicate with
IoT devices through the Internet; examples include an AR application that displays agricultural data from temperature or
moisture sensors for crop monitoring [115], and a web-based AR application framework to visualize the state changes of
a coﬀeemaker through a visual tag [104]. Others have highlighted the importance of scalable systems with eﬃcient data
management for AR applications [31,158]; to this end the authors of [211] proposed an AR-based browsing architecture that
identiﬁesnewIoTdevicesandenablesimmediateinteractionswitheasilycontrollableuserinterfaces.Similarly,Blanco-Novoa
et al. proposed a framework that allows AR and IoT devices to communicate dynamically in real time through standard and
open-source protocols such as MQTT, HTTPS, or Node-RED [24]. Developing systems which incorporate AR devices into a
network of IoT devices remains an important topic of research; for example, VisIoT [151] supports tracking and visualizing
thelocationofIoTnodesinrealtimethroughacombinationofdatacollectedfromcamera,inertialmeasurementunit(IMU),
and radio frequency (RF) modules, while Scenariot [80] integrates the discovery and localization of IoT devices into an AR
system by embedding RF modules into both AR and IoT devices. These works highlight the need for further research on
device localization and calibration, as well as system scalability (with respect to e.g., bandwidth consumption), to inform the
development of IoT-supported AR systems.
EdgecomputingforAR: InordertoleveragethelargeamountsofdatafromARandambientIoTdevices,context-awareAR
systemswillrequirestorageandcomputationalresourcesbeyondtheconstraintsofthesedevicesalone.Giventhelowlatency
requirements of many aspects of context-aware AR, along with the privacy concerns associated with transferring sensitive
information about users or environments to the cloud, many have deemed edge computing a particularly promising network4 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
architecture[70,114,201,217,223].Inthisarchitecture,aserverisplacedphysicallyclosetomobileARdevicesandambient
IoTdevices(e.g.,inthesamebuilding),helpingtoaddresstheaforementionedlatencyandprivacyrequirements.Existingwork
hasalreadydemonstratedthebeneﬁtsofoﬄoadingtaskstotheedgeforvariousaspectsofARsystemfunctionality,including
elements of SLAM pipelines [5,204,207], lighting estimation [223] and object detection [74,112,162], and in our ongoing
work we have developed multiple edge architectures for context-aware AR. For example, in [178] we developed a system to
predict the quality of virtual content positioning (a function of AR device pose estimate error) from environment properties,
in which we transmitted data collected on the AR device to the edge for the computationally expensive pre-processing and
model inference. In [114] we presented an edge-assisted collaborative image recognition system, in [179] we demonstrated
an edge-supported AR application that analyzed user eye movements to recognize common activities in a museum scenario,
and we have developed multiple systems that provide edge-based provisioning of contextual virtual content [63]. We see the
incorporation of IoT devices that provide additional contextual data as a natural extension to these edge architectures for
context-aware AR, and we recently presented an example of this in [177] (see Section 5.1 for further details).
3 Ambient IoT for AR
In this section, we categorize the diﬀerent ways in which ambient IoT devices can support or enhance AR experiences
(Section 3.1). We then provide an overview of the diﬀerent types of IoT devices that may be employed, along with their
associated uses, and examples of their use for AR or VR (Section 3.2).
3.1 Uses of Ambient IoT for AR
Central to next-generation AR is the incorporation of context-awareness – adapting virtual content according to the environ-
mental, user, social, and system context in which it is presented. Ambient IoT sensors are able to collect environment data
that is more accurate and complete than AR devices alone, while ambient IoT actuators can be used to adjust environment
propertiesforbettersystemperformanceorahigherqualityuserexperience.Inordertocategorizethewaysinwhichdetecting
and adjusting environment properties using IoT devices can support or enhance AR, here we deﬁne ﬁve high-level aspects of
AR which contribute to the quality of a user’s experience:
Spatialunderstanding concernsinformationaboutthephysicalpropertiesoftheenvironment,whichincludesrepresentations
ofthereal-worldsurfacespresent,thedetectionofﬁducialorimage-basedmarkers,andreal-timeposeestimatesfortheAR
device; this information is required for accurate spatial registration of virtual content.
Semanticunderstanding takesthisenvironmentalawarenessastepfurther,andprovidesaknowledgeofthetype,poses,and
statesofobjectsandsurfacespresent,whichmaybeusedtoinformspatialunderstandingordisplaycontext-awarecontent.
Contextualizedcontent referstothewaysinwhicheitherthespatialorsemanticunderstandingobtainedthroughIoTdevices,
or environment properties such as light and visual texture directly, may be used to inform adaptations to virtual content.
Interaction covers how current interaction methods in AR (e.g., hand gestures, eye tracking) may be enhanced or extended
using IoT devices.
Immersion relatestohowIoTactuatorscanbeusedtoincreaseanARuser’ssenseofimmersion(i.e.,thesensethatvirtual
content is truly present in their real environment).
3.2 Ambient IoT Devices for AR
InTable1,wepresentanoverviewofambientIoTdevicesthatmaybeusedtosupportorenhanceARexperiences.Wegroup
included devices by the type of information they collect from or convey to AR devices or users (e.g., visual, auditory), rather
thantheunderlyingfunctionalityofthedevice.Forexample,manymotionsensorsdetectchangesinthermalenergy,butgiven
thatinthiscasetheyprovideinformationregardingthestateofthevisualenvironment(i.e.,humanpresence),weclassifythem
hereunder‘visual’.Similarly,straingauges,whichdetectchangesinelectricalresistance,areclassiﬁedasvisualbecausethey
provide information about the deformation of an object, eﬀectively enhancing the visual perception and acuity of an AR user.
For each type of information, we divide IoT devices into sensors or actuators, list example devices, state their possible uses
from the categories we deﬁned in Section 3.1, and provide examples of related work for these use cases.Ambient Intelligence for Next-Generation AR 5
Table1:OverviewofambientIoTdevices(sensorsandactuators)withpotentialusesforAR.Sensorsandactuatorsaregrouped
by the type of information they collect from or convey to AR devices or users (e.g., visual, auditory).
Type of
information
providedSensors Actuators
Devices UsesExamples of use
for AR or VRDevices UsesExamples of use
for AR or VR
VisualCameras, depth
sensors, motion
sensors, light
sensors, strain
gauges, pressure
sensorsSpatial understanding,
semantic understanding,
contextualized content,
interaction[111,123,132,
167,178,210]Light bulbs,
projectors,
electronic
displays (e.g.,
LCD, E-Ink)Spatial understanding,
semantic understanding,
interaction, immersion[4,177]
Auditory MicrophonesSemantic understanding,
contextualized content,
interaction[60,136] Speakers Immersion [101]
HapticWind sensors,
physical buttons
and proxies,
touchscreensContextualized content,
interaction[21,44,94]Haptic surfaces,
fansInteraction, immersion [43,62]
OlfactoryNanomechnical
sensorsSemantic understanding -Diﬀusers,
olfactometersImmersion [16]
ThermalThermocouples,
resistance
temperature
detectors, infrared
temperature
sensorsSemantic understanding,
contextualized content[25,140,154]Heaters, HVAC
systemsImmersion [43,170]
ConsideringﬁrstIoTsensors,thereisawidevarietyofusefulinformationwecancollectfromthesurroundingenvironment,
andweidentifyfourkeywaysinwhichthesedatamaybeleveraged.Firstly,wecanenhancethespatialunderstanding,semantic
understanding,andinteractioncapabilitiesofARsystemsbysensingfromadditionalandmoreadvantageousposes.Secondly,
withsuﬃcientdataonhowenvironmentalconditionsimpactARsystems,wecanpredictthecurrentlevelofperformanceand
overall quality of an AR experience. Thirdly, we can use the data collected from sensors to adapt AR system functionality or
thepresentationofvirtualcontentforthecurrentenvironment.Finally,wecancontextualizevirtualcontent,inthatitisrelated
to or reacts to current conditions. Throughout Section 4, we discuss in more detail how IoT sensors can be used to support or
enhance AR.
Beyond detecting the properties of the real environment using IoT sensors, we can also adjustthe properties of the real
environment using IoT actuators. The motivation for this is threefold. Firstly, because key aspects of AR system performance
and user perception are aﬀected by environment properties, we can optimize environments to achieve the best possible
performance. For example, the accuracy of spatial understanding is dependent on suﬃcient levels of light and visual texture,
light levels aﬀect the performance of algorithms related to semantic understanding and interaction, and human perception of
virtual content is also aﬀected by the properties of both light and textures. Secondly, we can improve the quality of a user’s
interactions in AR, by providing alternative interaction methods (e.g., electronic displays) or enhancing existing ones (e.g.,
adding tactile feedback using haptic surfaces). Thirdly, we can enhance a user’s sense of immersion in an AR experience.
This may involve the generation of visual and auditory stimuli to extend what is possible on AR devices (using e.g., light
projectors or speakers), or the generation of sensory stimuli in modalities that cannot be generated on AR devices (using e.g.,
fans, diﬀusers, or heaters). We discuss the possibilities for ambient IoT actuators in more detail in Section 5.
4 IoT-based Sensing for AR
In this section we discuss in more detail how ambient IoT sensors could be used to support or enhance AR experiences.
Each subsection examines a diﬀerent use which we deﬁned in Section 3.1 and listed in Table 1; for sensors we cover spatial
understanding (Section 4.1), semantic understanding (Section 4.2), contextualized content (Section 4.3), and interaction
(Section 4.4).6 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
4.1 Spatial Understanding
AfundamentalcomponentofallARsystemswhichpositionvirtualcontentrelativetotherealworldisspatialunderstanding.
Indeedanaccurateanddetailedknowledgeofourphysicalsurroundingsisessentialfornext-generationARsystemswhichaim
to closely integrate the real and virtual worlds. In this subsection, we ﬁrst provide background information on the techniques
behindspatialunderstandinginAR.Wethenexaminehowenvironmentpropertiesaﬀectspatialunderstanding,andhencewhy
obtainingknowledgeofthesepropertiesthroughIoTsensorsisuseful,beforeexploringhowIoTsensorsmaybeuseddirectly
in spatial understanding algorithms.
4.1.1 Background on Spatial Understanding in AR
Understanding one’s physical surroundings is a fundamental component of both marker-based and markerless AR, in order
to accurately overlay virtual content onto a view of the real-world environment. A marker-based AR system uses a marker,
commonly a paper-printed static image with distinct features, to obtain information about the position and orientation of an
object in the surrounding space. On the other hand, a markerless AR system is based on the use of simultaneous localization
and mapping (SLAM) to understand the surrounding space without the use of markers.
Marker-based AR: Marker detection based on image processing has been a popular approach to enable marker-based
applicationsinARduetoitseaseofuseandaccuratetrackingofanobject[59].Amultitudeofmarkerswithdiﬀerentpatterns
andfeatureshasbeenusedinmarker-basedARsystems,includingbinaryﬁducialmarkers(e.g.,ARToolkit[17],ArUco[18],
ARTag [51]), a variation of ﬁducial markers specialized for robotics applications (e.g., AprilTag [146]), and image-based
markers with a high number of unique feature points (e.g., Vuforia [200]). The detection of these markers is based on the
processing of the image frames captured by the camera on an AR device – the pose of the device is estimated by ﬁnding
contours,features,orlines[75]withinthemarker.However,theaccuracyandreliabilityofmarkerdetectioninARarelargely
determined by the performance of the camera capturing images of the scene and the environmental conditions of the scene
where the marker is located. Poor camera calibration or focus, or low image resolution, can potentially result in low pose
estimation accuracy of the marker in the scene [218]. Additionally, environmental properties such as lighting or the distance
fromthecameratothemarkerareotherfactorsthatcanaﬀectposeestimationaccuracy.WediscusstheuseofIoTsensorsand
actuatorstoaddresschallengesrelatedtomarkerdetectioninSection4.2.3fortheuseofIoTsensorsinobjectstatedetection,
and in Section 5.1.1 for environment optimization using IoT actuators.
Markerless AR: SLAM is a key enabling technology for markerless AR. Visual and visual-inertial SLAM (V-SLAM and
VI-SLAM), using cameras either alone or in combination with inertial sensors, have demonstrated remarkable progress over
the last three decades [27]. Due to the aﬀordability of cameras and the richness of information provided by them, V-SLAM
using monocular [138,168], RGB-D [138], and stereo [138] cameras has been widely studied. To provide robustness to
textureless areas, motion blur, illumination changes, there is a growing trend of employing VI-SLAM, that assists cameras
with an IMU [28,159,203]; VI-SLAM has become the de-facto standard SLAM method for modern augmented reality
platforms [13,65]. In VI-SLAM, visual information is fused with IMU data to achieve more accurate and robust localization
and mapping performance [28,159,203]. Due to the high computational demands incurred by V- and VI-SLAM on mobile
devices, oﬄoading parts of the workload to edge servers has recently emerged as a promising solution for lessening the
loads on mobile devices and improving overall performance [5,33,207,208]. A standard approach [5,33,208] is to oﬄoad
computationally expensive tasks, such as place recognition, the process of taking a sensor snapshot (e.g., an image) of a
location and querying it in a large, geotagged database gathered from prior measurements, and loop closings, the process of
determining whether AR users are revisiting the same place. Both the aforementioned papers [5,28,33,159,207,208] and
commercialARdevicesthatemploymarkerlessAR(e.g.,AndroiddeviceswithARCore[65],iOSdeviceswithARKit[13],or
headsets such as the Microsoft HoloLens 2 [129]) implement VI-SLAM using sensor data captured onboard mobile devices,
and these data are both spatially and temporally limited. To address this limitation, in Section 4.1.3 we discuss methods to
increase the accuracy and completeness of spatial understanding by integrating the sensor data obtained onboard AR devices
with data obtained from IoT sensors.
4.1.2 Estimating the Quality of Spatial Understanding Using IoT Sensors
Becauseoftheroleofvision-basedsensinginspatialunderstandinginAR,andthenatureofVI-SLAMalgorithmsinparticular,
thepropertiesofthevisualenvironmentimpacttheaccuracyandcompletenessofspatialunderstanding.ThereforeathoroughAmbient Intelligence for Next-Generation AR 7
knowledge of those properties, obtained through ambient IoT sensors, is highly useful in identifying problematic regions for
tracking, estimating current levels of spatial understanding, and informing system adaptations for current environmental con-
ditions.Untilrecently,knowledgeoftheimpactofenvironmentalpropertieswaslimitedtoqualitativeguidelines;however,our
recentwork[175,176]providesquantitativeinsightsontheimpactofbothlightandvisualtextureonVI-SLAMperformance.
To obtain these quantitative insights on the impact of environmental conditions on VI-SLAM performance, we developed
and implemented two methodologies. In one [176], we measure the pose estimate error of open-source VI-SLAM algorithms
(e.g., ORB-SLAM3 [28]) using a game engine-based emulator; we use the trajectories from existing VI-SLAM datasets
(e.g. TUM VI [181], SenseTime [83]) to create new camera images in realistic virtual environments, and combine that with
the original inertial data from those datasets. In the other [174], we measure virtual object position error (determined by
pose estimate error) on commercial AR platforms (e.g., ARCore [65], ARKit [13]), by aligning virtual objects with a real
world reference point using our open-source AR app. Our game engine-based methodology facilitates ﬁne-grained control of
environmentproperties(i.e.,theexactpropertiesofthelightsourcesandtexturesinavirtualenvironment),whileourmethod-
ologyforcommercialARsupportsmonitoringofenvironmentconditionswitheitherARdevicesensorsorambientIoTsensors.
Light:Illuminance,theamountoflightincidentonenvironmentsurfacesperunitarea,determinestheaccuracywithwhich
environment surfaces can be mapped or tracked, because it determines the extent to which visual features are detectable for
tracking. We illustrate an example of this in Figure 2; in these experiments we used our game engine-based emulator to run
twoSenseTime[83]trajectoriesina6m 6m4mvirtualconcreteroom,with10diﬀerentoverheadlightintensities[176],and
10trialsperlight setting.WethenplottedVI-SLAMposeestimation performance(intermsofrelativeerror,thetranslational
component of relative pose error) against the 10 light intensity settings. Figure 2a shows the results for SenseTime A1, a
trajectory involving slow side-to-side motion facing a wall (as if inspecting a virtual object at head height), followed by
repeated walking away and returning with the camera angled more towards the ﬂoor (described in [83] as ‘inspect+patrol’).
Figure 2b shows the results for SenseTime A4, with slow motion focused on a small area of the ﬂoor, followed by the same
slow side-to-side motion facing the wall (described as ‘aiming+inspect’).
For SenseTime A1, optimal performance is obtained at a medium light intensity (750 lumens), at which there is suﬃcient
lighttoensurerecognizablefeaturesarevisible,butthosefeaturesarenotobscuredbyspecularreﬂections–thisisparticularly
a factor during the ‘walking away and returning’ portion of the sequence. For the less challenging inertial data in SenseTime
A4, performance is largely determined by the illuminance of the small area of ﬂoor at the start of the sequence, and specular
reﬂections are not a major factor; as such performance is poor at low light levels when the small area of ﬂoor is too dark, and
optimal performance is obtained at the highest light intensity. These results illustrate that optimal environment illuminance
diﬀers depending on which regions of an environment user trajectories cover, and that monitoring of illuminance in speciﬁc
environment regions of interest will be informative for the estimation of current VI-SLAM performance.
(a) SenseTime A1
 (b) SenseTime A4
Fig. 2: VI-SLAM pose estimation error for two SenseTime [83] trajectories in a 6m 6m4m virtual concrete room [176],
showing the translational component of relative error when diﬀerent light intensities are emitted by a light source (100 trials,
10ateachlightintensity).OptimallightintensityforVI-SLAMperformancedependsonthestructure,textures,andreﬂectance
properties of an environment, as well as the camera trajectory. While a medium light intensity will be optimal when specular
reﬂections due to high illuminance are a factor (e.g., Figure 2a), trajectories with views of environment regions where light
sources are distant or occluded will result in optimal performance at high light intensities (e.g., Figure 2b).8 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
WeenvisionmultipleIoTambientlightsensorsbeingemployedtoaccuratelymeasurelightingpropertiesindiﬀerentparts
oftheenvironment,withoutrequiringanARdevice.Thiswillenableboththeproactiveidentiﬁcationofenvironmentregions
where lighting may cause tracking errors, and a more complete understanding of how lighting conditions change over time.
Data on the position of virtual content or user trajectories will inform the most relevant positions for ambient light sensors to
be placed. The output from these sensors is also highly useful for environment optimization systems which control properties
such as illuminance based on occupant needs, as we show in Section 5.1.
Visualtexture: Giventherelianceoffeature-basedSLAMonrecognizablevisualtexturesinthesurroundingenvironment,
estimating the properties of visual textures present is also a valuable predictor of device tracking performance in markerless
AR. In quantitative experiments on a state-of-the-art open-source VI-SLAM algorithm (ORB-SLAM3 [28]) using our game
engine-basedemulator,weshowedthatboththeedgestrengthandcomplexityofatexture,aswellashowatextureisimpacted
by motion blur, aﬀect pose estimate error magnitude [176]. For example, Figure 3 shows the VI-SLAM pose estimation
performance(intermsofrelativeerror,thetranslationalcomponentofrelativeposeerror,androbustness,themeanpercentage
of tracked frames) we obtained when running the ‘room5’ trajectory from the TUM VI dataset [181] in 6m 6m4m cuboid
environments covered with diﬀerent visual textures. The textures are ordered according to their edge strength (variance of
the Laplacian), with higher numbers indicating higher edge strength values. Three out of four of the textures with high edge
strength, stone (R7), plants wallpaper (R8), and brick (R9) resulted in median relative error 5cm, compared to ¡20cm for
all other textures. The notable exception was the speckled marble texture (R10), with a median relative error of 95cm. In this
case, the ﬁne texture was greatly aﬀected by motion blur, resulting in less recognizable texture (low edge strength) in camera
images from dynamic portions of the trajectory.
Fig. 3: VI-SLAM pose estimate performance for TUM VI room5 [181] with various visual textures, ordered by the edge
strengthofthetexture.Theinteriorofa6m 6m4mvirtualroomwascoveredineachtexture,and100trialswereconducted
foreach(10ateachof10diﬀerentlightlevels,lightlevelsshowninFigure2).Performancewasmeasuredusingrelativeerror,
the translational component of relative pose error, and robustness, the mean percentage of tracked frames over all trials.
In our experiments on commercial AR platforms we have demonstrated that in terms of virtual object position error (a
functionofposeestimateerrormagnitude),somevisualtexturesaremorerobusttolowilluminancethanothers[177].Inthese
experimentswemeasuredvirtualobjectpositionerror,the3DEuclideandistancebetweenwhereavirtualobjectwasoriginally
placed and where it appears after walking away approximately 7m and returning, in diﬀerent environmental conditions usingAmbient Intelligence for Next-Generation AR 9
ouropen-sourceapp[174].Weconductedourexperimentsinauniversitylab,andtestedtwotextureswherethevirtualobject
wasplaced,acheckerboardandanacademicpaper,atthreeambientlightlevels(low,50-100lux;medium,150-450lux;high,
500-1000 lux), with 10 trials for each of the six settings. Figure 4 shows our results for these experiments on the Samsung
GalaxyNote10+smartphone(ARCorev1.28).Ourresultsillustratehowerrorincreasesatlowerambientlightlevels,butthat
thecheckerboardtexturewasmorerobusttothiseﬀectthantheacademicpaper(meanerrorsof4.1cmand12.0cmrespectively
atthemediumlightlevel).Weobservedthatatthemediumlightlevel,noiseinsmartphonecameraimageshasminimaleﬀect
on the checkerboard texture, but obscures the ﬁner texture of the academic paper, making VI-SLAM-based place recognition
more challenging, and resulting in greater error.
Fig. 4: Virtual object position error on the Samsung Galaxy Note 10+ smartphone after walking away approximately 7m and
returning, when the virtual object was placedon a checkerboard or academic paper texture, with a low (50–100 lux), medium
(150–450 lux) or high (500–1000 lux) ambient light level. The ﬁne texture of the academic paper is less robust to lower light
levels due to it being more easily obscured by noise in the AR device camera image [177].
These relationships between tracking quality and visual texture motivate the proactive monitoring of texture properties;
images of the environment periodically captured by IoT cameras can be transmitted to an edge server for the required image
processing. Again, not only does this allow us to identify and manually address problematic regions that might cause poor
trackingqualitybeforetheyareencounteredbyARdevices,butwecanautomaticallyoptimizeilluminanceortexturebasedon
thecurrenttypesofvisualtexturepresent(seeSection5.1.2).Anotherpromisingdirectionisimplementingaformofadaptive
SLAM by adjusting feature extraction parameters based on visual texture; for example, it may be beneﬁcial to lower corner
detection thresholds when low-contrast textures are detected.
4.1.3 Enhancing Spatial Understanding Using IoT Sensors
Aswellasmeasuringenvironmentconditionstoestimatecurrentlevelsofspatialunderstanding,wecanalsousethedatafrom
ambientIoTsensorsasinputtospatialunderstandingalgorithmsdirectly.Thereareseveralpromisingpossibilitiesinthisarea,
which we discuss below.
CollaborativeSLAM: ThevisualdataobtainedfromcamerasonboardARdevicesisbothspatiallyandtemporallylimited,
andfrequentlysuﬀersfromdistortionssuchasnoiseormotionblur.Tohelpalleviatesomeoftheseissues,wecanadditionally
employ the visual data from ambient IoT cameras. This use of multiple vantage points can be seen as a type of collaborative
SLAM,wherecapturesofdiﬀerentdevicescanbecombinedtoobtainahigher-qualityoverallmap,thatleadstohigher-quality
poseestimationbytheARdevice[89,180,208].Tofurtherextendthesensingcapabilitiesusingexternalsensors,inourplanned
work we will use IoT sensors (e.g., a surveillance camera) located in the vicinity of AR devices to provide extra information
for pose estimation in SLAM. Speciﬁcally, we will use deep learning-based object detection and person re-identiﬁcation for
obtaining semantic information, that is, localizing and tracking users wearing mobile AR devices in the surveillance camera10 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
frames. We will fuse the semantic information obtained from the surveillance camera with VI-SLAM running onboard AR
devices to achieve more robust and accurate pose estimation.
Collaborative depth mapping : In addition to VI-SLAM, modern AR devices are increasingly relying on time-of-ﬂight
depthsensors toaidwithspatialunderstanding,andsensingfrommultiplevantagepointscanalsobeadvantageousforobtaining
accurate depth maps. Time-of-ﬂight sensors struggle to obtain reliable data when the observed scene contains materials with
low reﬂectivity, strongly specular objects, and reﬂections from multiple objects [73,195]. Not only that, but indirect time-of-
ﬂight depth sensors such as those on the Microsoft HoloLens 2, Magic Leap 2 and some high-end Android smartphones are
limited in range, while the direct time-of-ﬂight depth sensors (marketed as LiDAR) on high-end iOS devices produce sparse
readingsthatmaybeinaccurateatshorterdistances.Thisleadstodepthmapsthataremissingvalidestimatesinlargepartsof
a frame, and incomplete spatial understanding.
In our evaluation of depth estimates obtained by a Microsoft HoloLens 2 (in the long throw mode) across a range of
representative indoor environments, we found that on average 30% of depth pixels in a frame were missing [220]. We also
collected anindoor dataset of18.6K depth mapson a SamsungGalaxy Note 10+smartphone, of which58% had greater than
40% missing pixels [220]. Based on the properties of these missing pixels, we determined that the range of indirect time-of-
ﬂight depth sensors on current AR devices is approximately 5m. We also observed that smaller angles ( 60or less) between
the sensor’s optical axis and the target surface resulted in large numbers of missing pixels. These problematic conditions are
prevalent in AR scenarios – in larger rooms a distance of greater than 5m between an AR device and the nearest surface is
common, while a depth sensor naturally faces outward toward a wall (due to how a smartphone is usually held or a headset is
worn), but target surfaces in AR are often horizontal planes such as a table.
Ambient IoT depth sensors can help to increase the completeness of raw depth maps by capturing data from poses that are
not accessible to or normally covered by AR devices. To address limitations in sensor range, ambient depth sensors can be
positionedclosertosurfacesthatareonlyobservedfromlargedistancesbyARdevices.Horizontalplanes,frequentlyincom-
pleteduetotheirsimilarangleoforientationtotheopticalaxisofARdevicesensors,canbecapturedbydownward-facingIoT
depth sensors. Even some challenging reﬂections may be avoided from diﬀerent viewpoints. We envision this collaborative
sensing approach being combined with existing techniques for depth map completion such as [126,160,219,220], with the
more complete depth data from multiple sensors combined on the edge server for a less challenging depth inpainting task.
Scenechangedetection: AmbientIoTcamerascanalsobeusedtoestablishwhethertheenvironmenthaschangedbetween
diﬀerent AR sessions, to trigger SLAM remapping as required to improve the quality of spatial scene understanding (and
conversely to avoid unnecessary time- and resource-consuming remapping if the environment did not change). Scene change
detection based on stationary cameras’ inputs is a long-examined, well-formulated problem, for which many solutions have
been proposed [202]. Extending existing solutions to incorporate the speciﬁc constraints of heterogeneous multi-device plat-
formsweenvision(IoTandAR,stationaryandmobile,devices),forthespeciﬁccaseofscenechangedetectioninthecontext
of VI-SLAM, has the potential to signiﬁcantly reduce the extent of mapping that would be required to achieve high-quality,
spatially-aware AR experiences.
4.2 Semantic Understanding
Akeyelementofnext-generationARistheuseofsemanticalgorithmsthatdetectthetype,position,orientation,andeventhe
currentstateofobjectsandsurfaceswithinanenvironment.NotonlycantheybecombinedwithSLAMtoextendorenhance
spatial understanding (e.g., [215]), but they also enable the delivery of various types of content to the user. Firstly, directly
annotating a visual display with semantic information has a wide variety of applications, from language learning [81] to ﬁre-
ﬁghting[23].Secondly,thisknowledgecanbeusedtoinformuserinteractions,e.g.,suggestingappropriateplacesfortheuser
topositionvirtualcontent,orobjectsthatcanbeinteractedwithinaspeciﬁcapplication.Finally,semanticunderstandingalso
enablestheprovisionofmoreintelligentcontent,suchasavatarsorvirtualcharactersthatinteractnaturallyandautonomously
with real-world objects. Here we focus on the topic of object detection to illustrate the role of ambient IoT sensors, however
the techniques we describe may also be applied to other types of semantic understanding, such as semantic segmentation.Ambient Intelligence for Next-Generation AR 11
4.2.1 Background on Object Detection in AR
By running object detection models on images captured by AR devices, we can detect the type, pose and extents of common
objects that are present in real world environments, which provides us with a more in-depth understanding of environmental
context and informs the rendering of virtual content. Although current advancements in deep neural networks (DNNs) have
shownsuperiorperformanceinobjectdetection[12,78,99,105],executinglargenetworksoncomputation-constraineddevices
such as AR devices and IoT sensors with low latency remains a challenge. To address this, edge-supported architectures are
neededtooﬄoadcomputationfromtheARdevicesandIoTsensorsandimprovetheend-to-endlatency[70,114,201,205,216].
As the pervasive deployment of mobile AR will oﬀer numerous opportunities for multi-user collaboration, prior works have
also studied object detection that exploits the visual information captured by diﬀerent AR devices [37,217]. Nevertheless,
collaborativeobjectdetectionforARdevicesandIoTsensors,wherevisualinformationiscapturedfromhighlydistinctvantage
points, is an unexplored area of research. The depth information that is obtained by specialized AR headsets and high-end
smartphones equipped with time-of-ﬂight depth sensors may also be employed to aid in object detection [147], while recent
workhasinvestigatedtheuseofpointcloudsgeneratedonthesetypesofARdevicesasinputtoobjectdetectionmodels[37,74].
Another approach is to detect and track objects in the environment by matching current input data – either 2D feature
points in captured images, or the 3D data in a generated point cloud – with a predeﬁned reference. In the case of images, the
marker detection techniques we described in Section 4.1.1 may be used by attaching a printed marker to a real-world object.
Alternatively, feature points can be matched across input and reference images [45,102]. By comparing the features extracted
on reference images (i.e., images of objects that need to be detected or tracked) to the features detected on input images (i.e.,
images of AR scenes where the desired objects are present) using descriptors such as ORB, SIFT, or SURF [122], the pose
estimationofadesiredobjectcanbefoundbycomputingthehomographymatrix.Eﬀortstoenhancethismarkerlessregistration
through improved feature matching include [32,98,103]. Similarly for point clouds, input data is matched with the 3D points
captured in a prior scan of the object, through point cloud registration [79]. This approach is used to enable the detection
of previously scanned objects on ARKit [15]. In general, feature matching approaches require fewer computational resources
when compared to neural-network-based object detection, but are less robust to environmental factors such as lighting, image
distortion, the distance of the AR device to an object, and background textures.
4.2.2 Enhancing Object Detection Using IoT Sensors
Current techniques for semantic understanding, including both object detection and semantic segmentation, rely on inputs
of 2D images [112,162], image and depth data [147], or 3D point clouds [74]. When these data are collected from sensors
onboard AR devices, they are frequently subject to distortions due to device motion, occlusions, and resolution limitations
(i.e., targets must be observed at an appropriate distance to capture informative data), resulting in incomplete and incorrect
knowledge of the environment. Thankfully, just as for spatial understanding, ambient IoT sensors such as cameras and depth
sensors can help address these issues by capturing data from additional and more favorable vantage points (e.g., a stationary
downward-facingcamera).Giventhatmanysemanticalgorithmsarecomputationallyexpensiveandoftenbestoﬄoadedfrom
AR devices to an edge server, data from IoT sensors can also be transmitted to the edge for processing.
One way in which the data sourced from IoT sensors can be used is to combine them with the data from AR devices, to
achieve more robust detection. For example, in [114] we developed CollabAR, a collaborative image recognition system in
which the camera image from an AR device is combined with spatially and temporally correlated images stored on the edge.
The architecture for this system is shown in Figure 5. Initial inference results based on the device image are provided by
the distortion-tolerant image recognizer, then aggregated with the inference results from spatially and temporally correlated
images by our auxiliary-assisted multi-view ensembler module, which outputs the ﬁnal object detection result. This enables
CollabAR to achieve over 96% recognition accuracy for images with severe distortions, with an end-to-end system latency as
lowas17.8ms.WithambientIoTcameras,wecanquicklyprovisionalargesetofhigh-qualityimagesforthecorrelatedimage
lookup step, without having to rely on data from AR devices. In general IoT sensors can supplement the data obtained by AR
devices to achieve more robust object detection, by providing images or point clouds of the same environment region that are
of higher quality, or that contain a more complete view of an object.
Alternatively,thedatasourcedfromIoTsensorsmaybeusedtoextendtheperceptualﬁeldoftheARdevice,todetectobjects
or surfaces which are outside the ﬁeld of view or range of device sensors, or objects or surfaces which are entirely occluded.
Thisgivesrisetoexcitingpossibilitiesregardingtheextensionofthehumanperceptualﬁeldtotheentireenvironmentinwhich
an AR user is located. Not only can these IoT sensor data include data from cameras and depth sensors, but one can also
incorporateothermodalities,towardsmorereliablerecognitionthatisrobusttoconditionsinwhichvision-basedsensorsmay
perform poorly. For example, passive infrared motion sensors which detect heat can detect human or animal presence in dark
environments, while recent works have demonstrated tactile-olfactory-based detection of humans [113] and chemical sensing12 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
Distortion
classifierPristine 
expert Motion blur 
expert
Gaussian 
blur expertGaussian 
noise expertRecognition moduleEdge Server
Resized 
ImageDistort ion-tolerant
image recognizer
Multi-view 
ensemble learning
Results
cache
Correlated Image lookupSpatial -temporal 
image lookup
Anchor -time
cache
Cloud
anchors
ResultAuxiliary -assisted 
multi-view ensemble ranchors 
timestamps image IDsPose 
estimationMobile 
client
Fig.5:Anexampleofanedgecomputing-basedarchitecturewehavedevelopedforsemanticunderstandinginAR,CollabAR
[114],whichprovidesdistortion-tolerant,collaborativeimagerecognition.AsanexampleofcombiningdatafromARdevices
andIoTsensorstoachievebetterrobustness,wecanusehigh-qualitydatafromIoTcamerastoprovidetheexistingviewpoints
used in the auxiliary-assisted multi-view ensembler step.
of illegal drugs [57] and explosives [106], which could all be incorporated into ambient IoT sensors. IoT-based monitoring of
ambient acoustic signals [141] could be used to improve acoustic-based context detection for audio sources that are located
far from the microphones of the AR devices. Central to realizing this vision of semantic understanding through an ambient
multimodal sensor network will be solving challenges related to device localization and calibration, variable signal quality,
and combining multimodal signals, which we discuss further in Section 6.3.
4.2.3 Enhancing Object State Awareness Using IoT Sensors
In addition to established types of semantic understanding – i.e., a knowledge of the type and position of objects present
in an environment – we propose that a knowledge of the current stateof objects present can also enhance AR applications.
This is particularly useful for AR applications that directly interact with physical objects in the surroundings, e.g., overlaying
holograms related to an object’s position, orientation, or other properties. These types of AR applications can be enhanced
by the understanding of object states, and reﬂecting changes in real time. While information about some properties (e.g.,
pose) can be gathered through the processing of images captured by an AR device, IoT sensors incorporated into objects can
provide greater robustness to environment properties, as well as data on other types of object states (e.g., strain). Below, we
cover several properties of objects that can be obtained through ambient IoT sensors, and discuss potential use cases in AR
applications.
Pose:While information about the position and orientation of objects in the surrounding environment can be obtained
through marker detection (see Section 4.1.1) or object detection (see Section 4.2.1), these vision-based approaches are often
dependentonenvironmentalfactors(e.g.,lightingconditionsorimageresolution).Incorporatingambientinertialsensors(e.g.,
the accelerometer and gyroscope in an IMU) into objects on the other hand provides position and orientation estimates for an
objectwithoutthisdependencyonenvironmentalfactors.UsecasesforintegratinginertialsensorsintoARforunderstanding
object pose can be seen across various applications that use wearable devices; examples include tracking the orientation of
a glove [132], estimating a user’s location and orientation through IMUs in earphones [210], detecting head movements for
face-related gestures through smart glasses [123], and enabling sensing through haptic devices [150,187].
Strain:Strain of an object refers to its deformation due to stress, and can be measured by strain gauges. Strain gauges
change the electrical resistance based on the magnitude of the deformation, thus providing knowledge about the deformationAmbient Intelligence for Next-Generation AR 13
Fig.6:TheregistrationofacatheterholograminanAR-assistedexternalventriculardrain(EVD)procedurecanbeneﬁtfrom
the use of IoT sensors for understanding of object state. We used strain gauges to detect the degree of bending on the catheter
to align the catheter hologram in AR.
of an object such as the bending or external pressure applied. In our recent work, we have been examining this property to
enhance the image registration of a catheter hologram in AR-assisted neurosurgery, as shown in Figure 6. We have developed
an AR-assisted guidance system for neurosurgery by tracking the position and orientation of the catheter and overlaying a
catheter hologram to guide surgeons in targeting the brain ventricle [46,47]. However, due to the deformable shape of the
catheter, there were often misalignments between the hologram and the catheter object. Use of ﬁber Bragg grating (FBG)
sensors has been proposed in multiple lines of work for shape detection in medical applications [108,182,183], however the
integrationofFBGsensorsintoanARsystemischallengingduetoitshighcostandrequirementofanadditionalmeasurement
device. To address this challenge, we are currently experimenting with strain gauges as low-cost IoT sensors that can be used
to estimate the deformed shape of the catheter. The strain data collected from the catheter are sent to the HoloLens 2 from an
edge server to display and align the correct shape of the catheter hologram onto the object. Figure 6 shows the enhancement
of catheter hologram alignment by detecting the bending of the catheter using strain gauges. We believe that this will reduce
the misguidance that can occur from the misalignment of the catheter hologram in our AR-assisted neurosurgical guidance
system, and further enhance the accuracy of catheter placement during the external ventricular drain procedure.
Other properties: Ambient IoT sensors could also be employed to detect other ‘non-spatial’ aspects of object state, as
wellasposeandstrain.ThisgoesbeyondsimplyvisualizingthedatafromexistingIoTsensors,toenhancingsemanticunder-
standing for AR in new ways. For example, one AR use case is cleaning applications, which inform users if an area requires
attention, even if that area does not appear dirty to the human eye. Nanomechanical and electrochemical sensors have been
developed whichdetect pathogens[29,157],and ifintegrated into IoTdevices, thesesensors couldprovide informationabout
whether objects or surfaces in an environment are contaminated. Similarly, recent works [91,135,143] have shown that food
spoilagecanbedetectedusingavarietyofdiﬀerentdevices,fromgas,humidity,andtemperaturesensorstomorenoveldesigns
using nanomaterials. Deployed in distribution, retail, or culinary environments, these sensors could increase the speed and
accuracyofAR-assistedfoodinventorymanagement,byquicklyindicatingtoworkerswhichitemsareunsafeforconsumption.
Extending the perceptive capabilities of users to a wider range of object properties in this manner will provide opportuni-
tiestoimprovebothproductivityandsafetyinmanyindustries,andislikelytobeakeymotivationforthewideradoptionofAR.
4.3 Contextualized Content
In the previous two subsections, we explored how ambient IoT sensors may be used to support or enhance two core aspects
of next-generation AR, spatial and semantic understanding. We now consider how the virtual content that is presented to
the user may be adapted according to the data obtained from IoT sensors. We start by covering adaptive user interfaces ,
the adjustment of virtual content to improve visibility and intelligibility for the user. We then cover the established ﬁeld of
photometricregistration inAR,thematchingofvirtualcontentlightingtorealenvironmentalconditions,beforeextendingthis
toenvironment-awareintelligentvirtualcontent ,theprovisionofvirtualavatarsorcharacterswhichareawareofandrespond
to a wide range of environmental conditions.14 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
4.3.1 Adaptive User Interfaces
The development of adaptive user interfaces, the properties of which are adjusted according to the context they are presented
inandtheneedsoftheuser,isalong-standingﬁeldofresearchinhuman-computerinteraction(forarecentreviewsee[133]).
Whiletoalargeextenttheliteratureontraditional2Dinterfaceshasfocusedoncontextualinformationrelatedtotheuser(in-
cludingsocialandculturalcontext)orsystemcapabilities,theimpactofthediverseanddynamicrealenvironmentswhichwill
host AR on both human perception of virtual content and system functionality means that environment-aware user interfaces
are a vital consideration [54,110,117]. Below we examine diﬀerent properties which will inform environment-adaptive user
interfaces in AR, and how this will be enabled by ambient IoT sensors.
Spatialandsemanticunderstanding: Anumberofworksonenvironment-adaptiveARhaveconsideredadaptationsbased
on spatial or semantic properties. For example, in [56] the authors developed a rule-based framework that enables designers
to ﬁt their application components to environments with diﬀerent geometries, in this case, sourced from the RGB and depth
streamsofaMicrosoftKinectcamera.Similarly,[145]presentedaprototypesystemthatallowsusersto‘snap’virtualcontent
to planar surfaces and edges, with environment data again extracted using RGB and depth images from the Kinect. More
recently, the work of Lindlbauer et al. [110] considers adapting AR user interfaces based on a combination of environment
geometry (checking whether a virtual object will be occluded using depth data), user task (which may in part be determined
by semantic understanding), and user cognitive load.
Regardingsemanticunderstandingspeciﬁcally,wemaywishtoplaceuserinterfaceelementsnearreal-worldobjectsthatare
semantically related, in order to either anticipate a user’s current needs, or to place AR-based tasks in context for a smoother
experience, as proposed in [34]. For example, access to AR-guided recipes may appear above the stove, or an in-progress
packinglistmightappearnexttoasuitcase.Alternatively,wemaywishtoblocktheviewofrealobjectswithvirtualobjects–
for instance, in the context of just-in-time adaptive interventions (JITAI) [139] it may support a user’s personal development
and change to cover a cigarette packet or mobile phone with another object such as a plant. A mock-up of this on the Magic
Leap One headset is shown in Figure 7, and we discuss this topic further in [175]. Incorporating ambient IoT cameras and
depth sensors will enable suﬃcient levels of spatial and semantic understanding to realize these visions reliably in practical
scenarios; accurate, information on the contents of an environment, along with data gathered on how users interact with that
environment, will enable the provision of optimally positioned virtual content for AR users.
Fig. 7: A Magic Leap-based mock-up of an environment-adaptive user interface, which uses semantic understanding of the
environmenttoidentifyadistractingitem(aphone),thencoversitwithavirtualplant.Tofurthermotivatetheusertostudy,a
motivational hologram (a diploma) is presented [175].
Light,texture,andcolor: However,thephysicalattributesofanenvironmentarenottheonlyfactorswhichshouldinform
the positioning and properties of AR user interface elements – the light, texture, and colors present also aﬀect visibility and
intelligibility. For example, on the OST displays employed on a number of state-of-the-art AR headsets, visibility of virtual
content (and as a result usability, user comfort, and sense of presence) is lower at higher levels of illuminance, even thoseAmbient Intelligence for Next-Generation AR 15
commonly encountered in indoor environments [49,86,96]. Not only does the additive nature of these displays mean that
any virtual content added is less perceptible, but real-world background textures that are visible through dark, transparent
regionsofvirtualcontentaremorevisible.Therefore,wemaywishtodetectwhenhighambientlightlevelsorhighlytextured
backgrounds are present and increase the brightness (pixel intensity) of virtual content, or adjust the amount of ambient light
that is allowed through the headset lenses, to improve content visibility. While state-of-the-art headsets support automatic
adjustment of display brightness based on ambient light settings, and the Magic Leap 2 allows users to reduce the amount of
ambientlightthatpassesthroughallorpartsofthedisplay[119],theultimategoalhereisautomatic,ﬁne-grainedadjustment
based on both the nature of virtual content and the presence of background textures.
Furthermore,existingliteraturehasestablishedtheeﬀectofcontrastratioandcolorcombinationsoncontentlegibilityand
aesthetics for both traditional (e.g., [107]) and AR displays (e.g., [213]). Given that virtual content may be presented in front
of a wide variety of real-world backgrounds in AR, content that is automatically optimized for the current background color
and texture is naturally of interest, and was proposed in [54]. On OST displays, blending eﬀects occur in that the perceived
colorofvirtualcontentisaﬀectedbythecolorofthereal-worldbackground[53,55];theautomaticselectionofvirtualcontent
colors that will either be minimally aﬀected, or aﬀected towards a desired result by blending with the current background, is
an important direction for future work.
While light levels, background textures, and colors can be detected through the ambient light sensors or cameras on an
AR device, this requires user interfaces (or other content) to be optimized in real-time. This is feasible for the types of back-
grounds and content that have been studied so far in AR (the vast majority of existing works consider text readability against
backgroundswithasinglecolororpattern,e.g.,[40,54]),butformorecomplexcasesitwilllikelybeadvantageoustoprepare
andprovisionoptimizedcontentinadvance.Especiallywhenweconsiderthecomplexwaysinwhichlight,texture,andcolor
interact to determine visibility, and the potential for combining this information with spatial and semantic understanding of
an environment, the use of ambient IoT sensors becomes a necessary addition. To this end, we envision the installation of
IoT cameras that capture the texture and color of the real-world surfaces which frequently appear behind virtual content from
the user’s perspective, with camera poses informed by both analyses of user trajectories and the positions of virtual content.
Automaticcontentoptimization,informedbyexistingworkondeeplearningforadaptiveuserinterfaces(e.g.,[188])canthen
be performed on the edge, and the result provisioned to an AR user when they enter an environment.
Spatial and semantic algorithm performance: Less considered in the literature is how user interfaces might be adapted
based on the current performance of algorithms for spatial and semantic understanding, which is essential to ensure users
do not rely on incorrect virtual content. Several pioneering works examined how virtual content may be adjusted in the
presence of registration (tracking) error; for example, MacIntyre et al. introduced and applied a level-of-error rendering
technique [116,117], in which a virtual convex hull outlining a real object was expanded according to estimated registration
error. This was extended in [38] to show 2D convex hulls representing possible registration error of virtual objects. In [72],
Hallaway et al. demonstrated switching between spatially registered object labels and unregistered augmentations depending
on the level of tracking accuracy currently available (this AR system employed a hybrid tracking solution incorporating both
ultrasonicandGPS-basedtracking,ratherthanVI-SLAM).Sincethen,otherworkshaveproposedandevaluatedvisualization
techniques to mitigate the eﬀect of registration error [165], as well as alternative ways of visualizing error in navigation tasks
(e.g.,coloredarrows,virtualcharacterexpressions)[149].Wearebuildinguponthislineofresearchbyexploringnewmethods
of displaying registration error, such as 3D convex hulls around virtual objects.
In particular, our ongoing work examines how to establish and convey the relationship between environmental conditions
andtrackingorregistrationerror–aswecoveredinSections4.1and4.2,propertiessuchaslightandvisualtexturecanimpact
the performance of the underlying algorithms. Beyond visualizing an estimate of current error, this will enable users and
environmentdesignerstotakestepstoreduceerrorbyalteringtheirenvironment,beforethemainARexperiencecommences.
While commercial AR platforms such as ARKit [13] and ARCore [65] indicate when tracking results are unavailable or
questionable, along with possible high-level causes, these causes lack granularity in terms of environmental conditions. To
addressthiswedevelopedaninterpretablepredictivemodel(adecisiontree)forbinaryclassiﬁcationoftrackingperformance
in[178],alongwithanexampleofhowenvironmentratingsmightbedisplayed(Figure8a).Wehavesinceextendedthedesign
of this user interface to use the model output to provide the user with extra guidance, as shown in Figure 8b. In these cases
the input data for the tracking quality prediction were obtained by the AR device alone, but we envision them being sourced
from ambient IoT sensors in the future. We are now developing prediction models which provide more ﬁne-grained estimates
of error magnitude, as well as methods to visualize the error estimates associated with diﬀerent environment regions.16 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
(a)ARuserinterfaceshowingbinaryclassiﬁcationoftrackingperformanceusing
symbols and colored planes [178].
(b) AR user interface showing colored binary classiﬁ-
cation of tracking performance, plus user guidance.
Fig. 8: Two of our prototype adaptive user interfaces for communicating the current level of spatial understanding (device
tracking performance) in an environment when placing a virtual object (hologram). Notiﬁcation of tracking performance
classiﬁcation (a) can be extended to include user guidance on how to improve unsuitable environments (b).
4.3.2 Photometric Registration
Currently,themostestablished(andwell-supported)typeofenvironment-basedvirtualcontentadaptioninARisphotometric
registration – rendering virtual content such that it is consistent with the lighting in the real environment. Lighting properties
of interest include illuminance, light direction, and color temperature, and can be used to more accurately render reﬂections,
specular highlights (the bright regions of a surface that reﬂect a light source directly), and directional shadows, which in turn
increases the level of realism, a user’s sense that a virtual object is really present in an environment. Photometric registration
for AR has been an active research topic for more than two decades (e.g., [68,88]), enabling its recent support in commercial
AR platforms such as ARCore and ARKit. State-of-the-art methods in commercial AR (e.g., [66]) use machine learning to
analyze camera images and synthesize environment lighting, similar to recent work on using CNNs for this task (e.g., [121]).
Otherrecentworkincludesthedevelopmentofanedge-assistedframeworktosupportreal-timelightestimation[223],andthe
use of physical, geometrically-tracked reﬂective light probes that are placed in the environment [155].
We envisage ambient IoT cameras and light sensors being employed to enhance photometric registration for two main
reasons. Firstly, IoT sensors are able to provide views of the environment not available to AR devices due to occlusions or a
limited ﬁeld of view. This will enable a more complete understanding of lighting within an environment, such as the exact
positionoflightsourceswhicharenotvisiblefromtheperspectiveoftheARdevice.Secondly,unlikeARdevices,IoTdevices
areabletoobtainrelevantinformationonenvironmentlightingpriortoanARexperience.Thismeansthatallthecomputation
related to light estimation does not have to be done in real-time on the AR device; instead, it can be precomputed (and
continuously updated) on the edge, both saving resources on the AR device and allowing for more complex light estimation
methods to be implemented. Indeed, an example of this type of solution was developed in [167], which the authors termed
‘distributedillumination’,toaddressthelackofcomputingpowerinmobiledevicesatthattime(2014).Inthisimplementation
highdynamicrangecameraswereconnectedtoastationaryPCviaawiredconnection;weproposeextendingthistowireless
connections using the IoT sensors now available, as well as incorporating more advanced light rendering techniques such as
indirectillumination(reﬂectionsfromthevirtualobjectontotheenvironment)andtheuseofsphericalharmonics(e.g.,[222]).
4.3.3 Environment-Aware Intelligent Virtual Content
Environment awareness in next-generation AR should also extend to the provision of intelligent virtual humans, animals, or
othercharactersthatrespondnaturallytoenvironmentalconditions,includingthepresenceandstateofrealhumansorobjects,
as well as properties such as light and temperature. This has the potential to make virtual content appear even more realistic
to users, and further increase a user’s sense of presence or immersion. Indeed, recent work has shown that virtual humans
whose gaze is directed towards real physical objects supported more eﬀective communication with real participants [8], andAmbient Intelligence for Next-Generation AR 17
that a virtual human that is able to inﬂuence physical objects (e.g., turning oﬀ a light) may be seen as more trustworthy by
users,andcanresultinusersperceivingagreaterdegreeofsocialpresence[93].In[193]theauthorslookedtodevelopamore
sophisticated understanding of semantic context to inform this type of intelligent virtual content, with semantic information
ﬁrst extracted from RGB and depth images, then represented as a 3D scene graph.
While less explored, a virtual character’s apparent awareness of and ability to respond to environmental properties such as
light, noise, wind, or temperature also holds great potential for increasing levels of realism and immersion. For example, one
can imagine a virtual cat that basks in a patch of sunshine, a virtual human that turns its head in the direction of a slammed
door, or a virtual character that warms itself next to a heater. In [94] the authors demonstrated the use of IoT wind sensors for
AR, with virtual papers that ﬂuttered in response to the airﬂow generated by a real fan, along with a virtual human that put
their hand out to stop the ﬂuttering. IoT sensors will be particularly useful in cases such as this, when detecting or localizing
these types of contextual information is beyond the capabilities of the sensors onboard an AR device. Contextual data from
diﬀerent parts of an environment may also be required in advance in order to provision speciﬁc animations associated with
responsive or intelligent content (e.g., the virtual cat rolling around in a patch of sunshine).
4.4 Interaction
Currently, the primary methods of interaction for user input in AR are tactile, via a touchscreen on the device (e.g., a
smartphone), a controller (e.g., the Magic Leap 2), or mid-air gestures via hand tracking (e.g., the Microsoft HoloLens 2,
the Varjo XR-3). Some devices also facilitate gaze-based interactions, supported via video-oculography-based eye tracking,
or auditory interactions via speech recognition. One important property of all these methods is that their performance can
be aﬀected by noise sources in the surrounding environment; for example, the presence of high intensity light sources or
high illuminance in general can cause issues for infrared sensor-based depth sensing [195], resulting in lower quality eye
tracking [172,177] and hand tracking [71]. 3D gaze point estimation may also be inaccurate at low illuminance due to lower
quality spatial mapping [71,177], while acoustic noise is known to be problematic for speech recognition [221]. Another
limitation of tactile and mid-air interaction methods at present is that they only capture gestures made using the hands, and in
thecaseofmid-airgesturescapturedviahandtracking,gesturesareonlyrecognizedwhenthehandsareintheﬁeldofviewof
thehead-mounteddepthsensor.Ifwecomparethistonaturalhumaninteractions,thisisseverelyrestricted;humansfrequently
express themselves with hand gestures outside this region, as well as gestures with the head, upper and lower body, and facial
expressions.
There are two ways in which ambient IoT sensors could be used to manage or address the reduced performance of AR
interactionmethodsduetoenvironmentalconditions.Oneoptionistouseambientlightsensorsormicrophonestopredictthe
current level of performance for each interaction method; the most appropriate method could then be suggested to the user,
or virtual content could even be adapted to address current limitations (e.g., elements might be increased in size to support
gaze-based interactions when gaze estimation accuracy is lower). Another option particularly applicable to speech-based
interaction is to use ambient IoT sensors to inform noise cancellation techniques. Such a method was developed in [185], the
coreideabeingthatanIoTmicrophonecapturesambientsoundsandforwardsthemtothespeakeroveritswirelessradio,with
the information arriving faster than if the sound was captured next to the speaker. This solution is readily applicable to AR
audio systems, especially if IoT microphones are already placed in the environment for other purposes.
There are also a number of possibilities for extending the interaction methods available in AR using IoT sensors. For
example,camerasanddepthsensorshavethepotentialtogreatlyexpandtherangeofgesturesARuserscanuseforinteraction
by capturing a view of a user’s entire body. Building upon existing work in human-robot interaction (for a review see [111])
and human activity recognition (for a review see [39]), we can develop solutions to recognize natural human gestures as well
as relevant activities in AR scenarios. Alternatively, tangible user interfaces allow users to manipulate virtual content using a
physical proxy, tracked using optical or inertial sensors – for example in [44] a Vive Tracker [77] equipped with infrared and
inertial sensors was placed inside a physical sphere used for interaction. One could also leverage IoT tactile sensors present
in the environment, such as touchscreens, physical buttons, or pressure sensors; a recent example of the latter, particularly
relevant to robotic and biomedical applications, was developed in [36], and is able to detect contact pressure, contact shape,
and shear direction. Finally, higher-quality signals available from ambient sensors will also be useful in some circumstances;
for example, although the microphones embedded in AR devices are of suﬃcient quality for speech-based interactions, we
may require microphones with a better dynamic range and frequency response for AR applications that present feedback on
musical performances to support learning.18 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
5 IoT-based Actuation for AR
Next, we detail how ambient IoT actuators may be used to support or enhance AR experiences. As in Section 4, we examine
the diﬀerent uses which we deﬁned in Section 3.1, though given that the role of IoT actuators for AR is less studied than IoT
sensors, this section is more exploratory in nature. In this section we combine related uses into one of two subsections; the
ﬁrst covers how IoT actuators can enhance the performance of spatial and semantic understanding algorithms by optimizing
environmental conditions, and the second covers how IoT actuators can be used to extend interaction methods or enhance
immersion.
5.1 Spatial and Semantic Understanding
Optimizing environment properties in order to achieve higher quality spatial understanding is an important direction for both
marker-based and markerless AR, that will enable both more accurate spatial registration of virtual content and more stable
virtual content. Here we cover the use of IoT actuators for marker-based AR in our dynamic marker system (Section 5.1.1),
and for markerless AR in our environment illuminance optimization system (Section 5.1.2). One alternative to these spatial
understanding methods was recently introduced in [4], and involves the modulation of ambient light sources in a predeﬁned
manner; this could be achieved using ambient IoT actuators such as light bulbs, however we focus on established techniques
for spatial understanding here. Finally, we also cover the possible use of IoT actuators to optimize environments for semantic
understanding (Section 5.1.3).
5.1.1 Environment Optimization for Marker-based AR
As introduced in Section 4.1.1, marker detection is a common technique used for object detection, with the position and
orientation of an object obtained through the detection of a printed marker attached to the physical object. To address the
challengesofenvironmentalfactors(e.g.,environmentlighting,thedistanceandanglebetweenanARdeviceandthemarker),
wedevelopedadynamicmarkersystemthatcontrolsIoTactuatorsincludinganE-Inkdisplayandasmartlightbulbtooptimize
the environment for marker detection.
Fig. 9: The IoT-based architecture of our dynamic marker system, an example of environment optimization for marker-based
AR.EnvironmentimagesarecapturedbytheARheadsetandprocessedontheedgeserverforfeaturedetectionandmatching,
to inform environment optimization through IoT actuators.
Dynamicmarkerswereﬁrstexploredbymanipulatingthesizeofprojectionsinaprojection-basedARsystem[124],however
with recent innovations in low-cost and low-power display hardware (e.g., an E-Ink display), dynamic markers became more
practicalformanyapplications.Inpriorstudies,dynamicmarkerswereusedinroboticapplicationssuchasenhancinghuman-
robotswarmmanagement[131],oraidingUnmannedAerialVehicle(UAV)landingbyscalingthesizeofamarkerbasedonthe
distance to the UAV [1]. Similarly, a dynamic marker has the potential to enhance marker-based AR applications by changing
the properties of the marker. Compared to a static printed marker, dynamic markers using an E-Ink display can enhance user
interactions by adapting to various situations (e.g., changing the size, shape, or pattern of the marker on the display) [152].
However,therearechallengesassociatedwithadynamicmarker,suchastheglareobservedontheE-Inkdisplaydependingon
the environment lighting, which may result in the marker being undetectable by the AR system. Our dynamic marker systemAmbient Intelligence for Next-Generation AR 19
forARfurtherenablesthesystemtocontrolthepropertiesofIoTactuatorssuchasthebrightnessofasmartlightbulb,andthe
sizeandshapeofamarkershownonanE-Inkdisplay,inordertooptimizetheenvironmentformarkerdetection(asshownin
Figure 9).
The hardware setup of our system (illustrated in Figure 10) includes a 7.5-inch Waveshare E-Ink display that can show
images in 8-bit grayscale with a Raspberry Pi 3, a HoloLens 2 AR headset with Vuforia marker detection, and a LIFX smart
light bulb for controlling environment illuminance. As shown in Figure 9, we collect scene (environment) images from the
HoloLens 2 and process them on an edge server, to characterize the environment with feature points, an important metric
for marker detection algorithms, that provides information about the quality of the marker image. Due to the relatively low
computational complexity of this task, we use a Raspberry Pi 3 as our edge server. The scene images are ﬁrst cropped to a
region of interest through image processing by detecting the square shape of the marker. We then detect the feature points in
the cropped scene image and match them with the reference marker image to quantify the percentage of features available in
the scene image. Our system considers three environmental properties (the lighting condition of the scene, the distance from
the camera to the marker, and the viewing angle of the camera to the marker) to optimize for marker detection. Environment
lighting impacts the quality of the printed marker, with marker detection less robust in darker or brighter conditions. The
distanceandangleofthecameraontheARheadset,determinedbytheuser’smovement,impactsmarkershape-relatedfactors
such as the black and white ratio, edge sharpness, or the information complexity of the marker [87,90]. To optimize the
environment, we control the IoT actuators to change the brightness of the LIFX bulb, and the size and shape of the marker
shownontheE-Inkdisplay,untilthepercentageofmatchedfeaturepointsreachestheoptimallevelformarkerdetection.The
latencyassociatedwithchanginganewimagebasedontheenvironmentcharacterizationinourdynamicmarkersystemtakes
about one second, due to updating all pixels on the E-Ink display.
Fig. 10: Hardware setup of our environment optimzation system for marker-based AR: an E-Ink-based dynamic marker, a
HoloLens 2, and IoT actuators (a), an undetected dynamic marker in an unoptimized environmental condition (b), an overlay
of virtual content through detection of the dynamic marker in an optimized environmental condition (c).
To inform this system, we ﬁrst investigated the relationship between the three aforementioned environment properties
and characterization metrics of the scene (i.e., results of feature point matching). We conducted experiments by computing
feature matching with four diﬀerent marker patterns under diﬀerent environmental conditions. The four markers comprised
two ﬁducial markers in 1-bit grayscale with a similar number of available features from ARToolkit [17] and ArUco [18]
open-sourcelibraries,andtwoimagemarkers;onewithauniformpatternoffeaturesandanotherwithaninconsistentpattern
of features. We analyzed the changes in the percentage of matched feature points by testing each marker pattern at 9 diﬀerent
viewing distances (20-90cm with an increment of 10cm), 5 diﬀerent viewing angles (0-60 degrees with an increment of 15
degrees),and9diﬀerentilluminancelevels.Eachsetofenvironmentalconditionswasrunfor20trials.Ourresultsshowedthat
therewasacorrelationbetweenthenumberofmatchedfeaturepointsandtheenvironmentproperties.Morefeaturepointswere
matchedonthedynamicmarkerwhentheviewingdistanceandanglewerelower.Ontheotherhand,fewerfeaturepointswere
matchedonthedynamicmarkeratlowerilluminancelevels.Wealsoobservedadissimilitudeinthechangesinthepercentage
of the matched feature points among diﬀerent marker patterns. The matched feature points on the ﬁducial markers (those
from ARToolkit and ArUco) were more dependent on environmental conditions than image-based markers due to their lower
numberofavailablefeaturepoints.Thispromptsfurtherinvestigationtodiscoverthediﬀerentoptimalconditionsrequiredfor
each marker pattern, to achieve more robust and accurate marker detection in AR.
Theserelationshipsbetweenenvironmentpropertiesandmarkerdetectionperformancemotivatetheuseofdynamicmarkers
with IoT-based actuators for various marker-based AR applications. Our dynamic marker system achieves more accurate and
robust marker detection by optimizing the environment. For instance, we can change the brightness of a smart light bulb to
optimizethelevelofilluminance,andchangethesizeofthemarkershownontheE-Inkdisplaybasedonthedistancefromthe20 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
user to the marker. This environmental optimization enhances user interactions in marker-based AR applications by allowing
the system to detect a marker at ease even under initially challenging conditions. Furthermore, a dynamic marker provides
muchgreaterﬂexibilityinchangingthemarkerpattern,shape,andsize,whencomparedtoaconventionalprintedmarker.This
holds potential for marker-based image registration in medical applications by showing diﬀerent anatomies in AR holograms
basedonthemarkertype[46,52]orinhuman-robotapplicationsbyprovidingdiﬀerentfeedbackontheE-Inkdisplaytoenable
simpler debugging for users [131].
5.1.2 Environment Optimization for Markerless AR
Given the increasing popularity of markerless AR applications (e.g., [6,82,144]), and the fact that virtual object instability
due to incorrect spatial understanding is still a prevalent issue on state-of-the-art platforms and devices [174], it is also of
interest to consider how IoT actuators can be used to optimize environments for better spatial understanding in markerless
AR. As we covered in Section 4.1.2, environment illuminance (ambient light level) plays a central role in determining the
performanceofVI-SLAM,themethodunderpinningspatialunderstandinginmarkerlessAR,becauseitdeterminestheextent
to which visual textures are distinguishable for feature-based mapping and tracking. If the tracking cameras on an AR device
are pointed towards regions of an environment with suﬃciently low or (less commonly in indoor scenarios) high levels of
ambientlight,devicetrackingmayfailtoinitialize,beoflowerquality,orbelost,resultingintheincorrectspatialregistration
ofvirtualcontent.Thankfully,illuminanceisalsoreadilycontrolledthroughIoTactuatorswhichemitlight(e.g.,smartbulbs)
or block light (e.g., smart blinds), leading us to focus our initial eﬀorts on environment illuminance optimization. Additional
complexitycomesfromtheneedtoconsidertheimpactoflightonotherARsystemelements,includingthequalityofsemantic
understanding, the performance of eye and hand-tracking algorithms, and the visibility of virtual content on OST displays.
In [177], we developed a proof-of-concept environment illuminance optimization system for AR, which automatically
maintains illuminance at a suﬃcient level for high virtual object stability, and where possible, accurate and precise eye
tracking. It uses both IoT sensors to detect current levels of illuminance and visual texture in an environment, and an IoT
actuator to control the level of illuminance in that environment. Our results on virtual object position error for diﬀerent
visual textures and illuminance levels (see Section 4.1.2) showed that the robustness of spatial understanding of diﬀerent
illuminancelevelsisdependentonthepropertiesofthevisualtexturespresent,withﬁnetexturesrequiringgreaterilluminance
tosupportgoodperformance.Therefore,whileoursystem’sdefaultoptimumilluminancelevelis300lux(tosupportaccurate
eye tracking and low virtual object position error for coarse textures), when the environment contains ﬁne visual textures
the core AR functionality, virtual object stability, is prioritized and that optimal level is increased to 750 lux. To oﬄoad
the computationally expensive environment texture characterization task, while avoiding the transfer of potentially sensitive
images to the cloud, we implement an edge-computing architecture, with optimization controlled by a server on the same
wireless local area network, as shown in Figure 11.
Fig.11:Theedge-basedarchitectureforourenvironmentilluminanceoptimizationsystemformarkerlessAR[177].Environ-
ment images and illuminance levels are periodically captured by IoT sensors (a Raspberry Pi camera and an ambient light
sensor)connectedtoaRaspberryPi,whichtransfersthesedatatotheedgeserver.Environmentcharacterizationisperformed
on the edge, and instructions are sent to an IoT actuator, a LIFX bulb, to optimize illuminance levels.
Our environment illuminance optimization system employs two IoT sensors, an 8MP Raspberry Pi Camera Module 2 and
an ambient light sensor (TSL25911FN), connected to a Raspberry Pi 4 (as shown in Figure 12), to record images of the
environmentandilluminanceevery5s.ThisenvironmentdataissavedtolocalstorageandthensentviaanHTTPPUTrequest
to the edge server (a Lenovo ThinkCentre M910T desktop computer). The characterization module determines the optimalAmbient Intelligence for Next-Generation AR 21
light level by detecting the number of FAST corners in the environment image using OpenCV: if more than 250 corners are
detected the environment texture is classiﬁed as ﬁne, and the optimal light level is raised. If a light level change is required,
the characterization module sends the optimal and current light levels to the light level control module. Our system employs
one IoT actuator, a LIFX bulb, and the light level control module adjusts the brightness value of the LIFX bulb accordingly,
atlowlatency( 0.5s).Environmentcharacterizationmetrics(illuminance,plusimagebrightness,contrast,edgestrength,and
corners)arealsostoredontheedgeforlong-termtrendanalysis,whilethelatestmetricscanberequestedbyanARdevicevia
anHTTPGETrequest.Thecomputationalandstoragerequirementsofthislong-termenvironmentcharacterizationmotivated
our use of a higher class of edge server (the desktop computer) than the Raspberry Pi 3 used in our optimization system for
marker-based AR (Section 5.1.1).
In future work, we will extend our environment illuminance optimization system to employ multiple sets of sensors and
smart bulbs to control illuminance in diﬀerent environment regions. One associated challenge will be device localization
and calibration, which we discuss further in Section 6.2. It will be desirable for the pose of IoT cameras to be automatically
adjustable,suchthattheycaptureregionswhereARusersoftenpointtheirdevicecamera,orwheredevicetrackingislost.To
this end, while our current system uses a ﬁxed camera mount, future implementations could be replaced by a programmable
camera mount, controlled according to data gathered on the edge server (the Arducam B0227 Pan Tilt Platform for example
providesalow-costsolutionforaRaspberryPiCamera).WewillalsoinvestigatehowotherIoTactuatorssuchassmartblinds
can be incorporated, as alternative methods of controlling illuminance.
Fig. 12: A proof-of-concept installation of our IoT-enabled environment illuminance optimization system [177], viewed
through a custom AR application on a Samsung Galaxy Note 10+ smartphone. In order to capture the appropriate region of
the environment with the Raspberry Pi Camera Module 2 we manufactured a custom 3D-printed camera mount, which could
be replaced by a programmable mount in future implementations.
Anotherdirectionforfutureworkistoexplorehowthe visualtexture inanenvironmentcouldbeadjustedtoenhancespatial
understanding in markerless AR. VI-SLAM algorithms rely on suﬃcient texture for accurate and robust pose tracking [176],
butthisisoftennotpresentinbuiltenvironments,duetohumanpreferencesforminimalisminarchitectureandinteriordesign.
One possible solution would be to detect when an AR experience is taking place in an environment, and use ambient IoT
actuators such as electronic or E-Ink displays, or light projectors, to provide additional visual texture during that period. In
somecases,additionaltexturesprovidedbyambientactuatorsmightevenbematchedwiththecontentsoftheARapplication,
towardsgreateruserimmersion(seeSection5.2.2).Inongoingwork,weareinvestigatingmethodsthatautomaticallydetermine
where,foragivenenvironmentandapplicationscenario,theapplicationofvisualtexturewillbemostbeneﬁcial.However,any
environment texture optimization system must also take into account the impact texture can have on the visual perception of
AR users – greater visual texture can impair visibility on OST displays or distract users from a task [176]. This is particularly
pertinent because some of the environment regions in which applying visual texture is likely to be most beneﬁcial for spatial
understanding (i.e., the regions which a device camera is most often facing towards), are frequently the regions where AR
contentisplaced,soadjustmentoftexturescouldinterferewithhumanperception.Toinformtheoptimizationofvisualtexture
in AR environments, further work is required to investigate the trade-oﬀs between spatial understanding and human visual
perception of diﬀerent visual textures.22 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
5.1.3 Environment Optimization for Semantic Understanding
Inasimilarmannerasforspatialunderstanding,onecanalsouseambientIoTactuatorstooptimizeenvironmentsforsemantic
understanding.Speciﬁcally,IoTlightbulbsandblindscanbeusedtoadjustenvironmentilluminancetolevelswhichmaximize
the performance of vision-based object detection and semantic segmentation algorithms. For example, the guidelines for
detectingpreviouslyscannedobjectsonARKit(describedinSection4.2.1)recommendanilluminanceof250to400lux[15].
Atlowlevelsofilluminance,noiseisintroducedtothecameraimagesusedasinputforsemanticunderstanding,especiallyon
mobiledeviceswithasmallcamerasensorsize[7]–evensmallamountsofnoisehavebeenshowntoresultinlargeaccuracy
drops for leading image recognition algorithms [114]. Although we covered techniques to obtain images with less distortion
using ambient IoT cameras in Section 4.2.2, one can make this task less challenging by ensuring IoT light bulbs provide
suﬃcient illuminance. On the other hand, high intensity light sources may cause specular reﬂections on objects or surfaces
made of glossy or metallic materials; not only do these artifacts reduce the performance of image-based object detection or
semanticsegmentationalgorithms[10],buttheyalsodegradethequalityandcompletenessofdataavailablefromtime-of-ﬂight
depth sensors used to support semantic understanding [195]. IoT light bulbs should be set to intensities that minimize these
issues, and IoT blinds should be employed when necessary to block strong sunlight.
Critical to implementing environment optimization systems for semantic understanding will be establishing ‘regions of
interest’, where illuminance-related issues are likely to occur. Regions of interest will be areas of an environment where both
(1) illuminance-based distortion or artifacts could occur (e.g., dark corners, or a metallic object close to a light source), and
(2) AR users are likely to view, particularly with the expectation of semantic information being provided (e.g., an object of
interest).TheseregionsofinterestcanthenbemonitoredbyambientIoTsensorstoinformthecontrolofnearbyIoTactuators.
Forexample,ifanambientlightsensoronadeskorworkbenchdetectsalowlightlevel,thelightintensityofanearbyIoTlight
bulbcouldberaised.Alternatively,onecoulddetectilluminance-basedissuesinIoTsensordatadirectly;forexample,specular
reﬂectiondetection[10]couldberunontheimagescapturedbyanIoTcamerafocusedonametallicobjectnearawindow,and
whenspecularreﬂectionsaredetectedtheIoTblindonthatwindowwouldbelowered.Weenvisiontheseoptimizationsystems
beingreadilycombinedwiththeilluminanceoptimizationsystemsforspatialunderstandingthatwedescribedinSections5.1.1
and5.1.2;notonlydotheysharethecommongoalofminimizingdistortionandartifactsinvisualdata,butregionsofinterest
in need of IoT sensor-based monitoring will frequently overlap.
5.2 Interaction and Immersion
As well as being used to optimize the performance of spatial understanding algorithms, IoT actuators may also be leveraged
toenhancetheuser-facingelementsofanARsystemdirectly.Belowweconsiderthewaysinwhichauser’sinteractionswith
virtual content (Section 5.2.1) and their immersion or sense of presence in it (Section 5.2.2) may be improved using ambient
IoT actuators.
5.2.1 Interaction
Environmentalconditions(e.g.,lighting)canimpactboththeperformanceofinteractionmethodssuchashandandeyetracking
in AR (see Section 4.4), and an AR user’s perception of visual virtual content (see Section 4.3), which in turn aﬀects their
ability to interact with it. Ideally, we wish to incorporate these constraints into the type of environment optimization systems
we introduced in Section 5.1, although this will not be without challenges due to conﬂicting requirements, as we discuss in
Section6.4.Incertaincases,forexamplewhenconditionscannotbeadjusted,orinsafety-criticalscenarioswheretheaccuracy
andspeedoftaskcompletionareparamount,itmaybebeneﬁcialtopresentinformationontraditionalelectronicdisplaysthat
are not as aﬀected by environmental conditions. Users could choose to switch to a nearby ambient visual actuator (e.g., a
tablet, smartphone, or smart display such as an Echo Show) to help them complete a speciﬁc task, before switching back to
the main AR display. Indeed, similar to how screen mirroring is used to share content with a wider audience, this can also be
used to support interactions that involve non-AR users. We envision that knowledge of alternative displays that can currently
be leveraged will become an important element of environmental awareness in AR.
AnotherlimitationofcurrentARheadsetdesignsisthattheyfacilitatethedeliveryofvisualandauditorysensoryinformation
to users, but are not well suited to providing realistic tactile feedback when users interact with AR content. The controllers
included with some devices (e.g., the Magic Leap 2) provide uniform tactile feedback and support vibration but do not allow
users to actually ‘touch’ AR content, while hand tracking-based interactions allow for natural gestures but do not provide
any haptic feedback when users touch virtual content. Recent work has sought to address this latter issue by providing hapticAmbient Intelligence for Next-Generation AR 23
feedback through a vibrotactile ring [125,192] or ﬁnger-side or nail actuators [118,156]; however, all solutions involving
wearables place extra requirements on user equipment and setup that may not be practical in many scenarios. One potential
alternative for visual AR content that is overlaid on a real-world surface is the use of surface haptics, to produce variable
tactile sensations and perceived levels of friction on the same surface through electroadhesion [134,206]. By incorporating
these devices into an AR environment as ambient IoT actuators, we can program them according to the virtual content that is
displayed in a particular region, and thereby provide more realistic haptic feedback to users without the need for wearables.
5.2.2 Immersion
There are a variety of possibilities for using ambient IoT actuators to enhance an AR user’s sense of immersion. One type of
enhancementstemsfromthefactthatcurrentARdevicesarelimitedtodeliveringvisualandauditorysensoryinformation;this
canleadtoamismatchwithothersensoryinputssuchasthesensationoftemperature(thermoception),forexample,ifavirtual
ﬁreispresent,buttheroomiscold.Thislimitsauser’ssenseofimmersion,thefeelingthatvirtualARcontentistrulypresent
intherealenvironment–researchindicatesthatincreasingthemodalitiesofsensoryinputinvirtualenvironmentsincreasesa
user’ssenseofpresenceinthatenvironment(e.g.,[42,64,153]),consistentwithobservationsthatthehumanperceptualsystem
evolved in multisensory environments [189]. Recent work in VR has explored the potential for diﬀerent types of external
actuators to deliver a range of stimuli speciﬁc to the current environment, including speakers for 3D audio [101], heat lamps
for temperature control, and fans for haptic wind representations [43] (the authors use the term ‘smart substitutional reality’
to describe their system here), and an olfactometer that provides diﬀerent scents or smells [16]. We propose applying and
evaluatingthesetypesofsystemsinAR;forexample,ifweimagineatherapeuticARapplicationthatrelaxesusersbyplacing
virtual ﬂowers in their environment, incorporating the scents associated with those plants may increase the application’s
eﬀectiveness. Given the variable level of virtual content that may be incorporated into an AR experience, an important
consideration will be how a sense of virtual content realism in AR (i.e., a sense that a virtual object is present in a user’s
physical environment) diﬀers from a user’s sense of presence in VR (the feeling that one is actually present in a new virtual
environment).
Alternatively,anothertypeofpossibleenhancementusingIoTactuatorsinvolvesincreasingimmersionthroughtheaddition
of visual and auditory stimuli beyond the current limitations of AR device displays and speakers. In particular, the limited
ﬁeldofviewonOSTdisplaysandsmartphonesmaybecounteractedsomewhatbyaddingvisualstimuli,orcontrollingvisual
conditions,inthesurroundingenvironmenttomatchARcontent.Forexample,whileanARapplicationdisplayingtheplanets
of the solar system is being used, the environment light level might be lowered using IoT bulbs, and distant stars represented
on the surrounding surfaces using IoT light projectors. Existing immersive art experiences which oﬀer separate projection
and VR exhibits (e.g., [50]) could provide AR experiences which combine 3D virtual content with 2D projections, while still
allowingvisitorstointeractnaturallyinthesamespace.Forauditoryinformation,althoughspatialaudiousingheadphonesor
the speakers available on some headsets is well-developed, there will be cases (e.g., for a smartphone without headphones, or
a headset with a smaller form factor) in which ambient IoT speakers can replicate environmental sound sources much more
realistically. Indeed, the opportunities to support smaller AR device form factors by using external devices for virtual content
presentation (as well as for computation), in this type of ‘mixed media AR,’ is another important direction for future work.
6 Challenges and Research Directions
In this section we lay out a set of research directions associated with employing ambient IoT devices to support or enhance
ARexperiences.First,wedescribetheuseofagameengine-basedemulator,requiredtogathersuﬃcientdataontheeﬀectof
environmentpropertiestoinformthedesignofIoT-supportedARsystems,andprototypetheminknown,controlledconditions
(Section6.1).Second,welayoutthechallengesassociatedwithARandIoTdevicelocalizationandcalibrationthatariseonce
we implement these systems in the real world, along with potential solutions (Section 6.2). We then consider how, given a set
of localized and calibrated devices, one might tackle combining the sensor data from those devices (Section 6.3). Next, we
discuss the challenges of implementing the IoT actuator-based environment optimization systems we described in Section 5,
with a focus on the conﬂicting requirements of diﬀerent aspects of AR experiences, as well as those that come about once
weintroduceheterogeneoususersanddevices(Section6.4).Finally,wecoversecurityandprivacyissuesrelatedtoplatforms
which combine AR and ambient IoT devices (Section 6.5), a vital consideration for the wider acceptance and deployment of
ambient IoT-supported AR systems.24 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
6.1 Game Engine-based Emulations of IoT-supported AR Systems
Thevisionwehavelaidoutinthisbookchapterrequiresbothanin-depthunderstandingofhowenvironmentpropertiesimpact
ARalgorithmperformanceindiversescenarios,andthedevelopmentofdeeplearningmodelstopredictARalgorithmperfor-
mance. This is challenging for two reasons. Firstly, we require large amounts of data with accurate ground truth information
in a diverse set of environments, which is time-consuming or even infeasible to obtain. For example, obtaining ground truth
pose data for VI-SLAM algorithm evaluations in real environments necessitates the use of optical tracking systems such as
OptiTrack[148]andVicon[199],whichinvolvesconsiderablesetupandcalibrationtimeforeachnewenvironment.Secondly,
werequireﬁne-grained,systematicmanipulationofenvironmentpropertiesinordertoperformexperimentsinrepeatableand
controlled conditions; this is also diﬃcult to achieve in most real environments because they are often subject to external
inﬂuences, such as daylight entering through a window, or objects being moved. Indeed, these challenges are why existing
datasetsforVI-SLAMevaluations(e.g.,EuRoC[26],TUM VI [181],SenseTime[83])orobjectdetectioninAR(e.g.,[2,105])
cover a small range of environments or provide no information on environment properties. Instead, we propose the use of
highly realistic synthetic data, recently made possible through high-deﬁnition rendering techniques in game engines (e.g.,
Unity [197] and Unreal [48]) and other rendering software (e.g., V-Ray [30]). We developed a methodology for using virtual
environmentsforVI-SLAMevaluationsin[176],andtheprocessforgeneratingsyntheticdatasetsforsemanticunderstanding
was described in [164]. We cover the key elements of this approach for ambient IoT-supported AR below.
AR environment and AR user emulation: In ongoing work we are using the Unity and Unreal game engines, de-facto
standardARandVRdevelopmentplatforms,tocreateemulatorsofARenvironmentsandusers.Weareusingtheseemulators
to generate diverse photorealistic scenes, represent mobile AR user behaviors, and test the performance of a wide range of
IoT-supported AR solutions. An example of a scene we have generated in our initial Unreal-based emulator is shown in
Figure 13. Within the generated scenes, we have ﬁne-grained control over multiple parameters related to AR environments
(e.g., physical layouts, lighting, reﬂections, textures), as well as parameters related to AR users and devices (e.g., trajectories,
camera properties, motion blur levels). In future we envision the inclusion of other factors to emulate AR environments and
users at higher ﬁdelity, such as animating AR users to move and behave like real humans.
/g83/g72/g85/g86/g82/g81/g3/g20/g83/g72/g85/g86/g82/g81/g3/g21
/g83/g72/g85/g86/g82/g81/g3/g22
Fig. 13: An image captured by an IoT camera in the Unreal-based emulator we have created. The IoT camera overlooks a
photorealistic scene with three AR users.
IoTsensorandactuatoremulation: Toenhancethecapabilitiesofourgameengine-basedemulators,wewillalsoemulate
ambientIoTsensorsandactuatorsinARenvironments.Forexample,wewillemulatelightsensorstomeasurelightingproper-
ties(illuminance,lightdirection,andcolortemperature)ofthevirtualenvironment.Inourpreliminaryinvestigationswehave
added an IoT camera in our emulator: Figure 13 shows an image captured by this IoT camera, overlooking a space with three
AR users. The camera has also identiﬁed and tracked the AR users in its view. We are currently exploiting the emulation of
IoTcamerastodevelopalgorithmsthatincreasetheaccuracyofspatialandsemanticunderstanding,byfusingtheimagesfrom
these IoT cameras with the sensor data captured by the AR devices worn by emulated AR users. Adding the emulation of IoT
actuators(e.g.,bycreatingandmodifyinglightsources)willenablethemodelingandprototypingofenvironmentoptimizationAmbient Intelligence for Next-Generation AR 25
systems, such as those we described in Section 5.1.
Ground truth generation: As well as controlling parameters related to environmental conditions, we envision building
pipelinesthatautomaticallygeneratethegroundtruthdatarequiredforalgorithmevaluation.Weobtainedthegroundtruthof
thestates(e.g.,cameraposes,sensorpositions)ofARdevicesandIoTsensorsinourpriorwork[176],andwillusethepipelines
togeneratepixel-wisesemanticinstanceannotationsforimagescapturedbyARdevicesandIoTsensors.Forexample,in[176]
our VI-SLAM evaluation methodology used existing SLAM dataset ground truth trajectories to generate sequences in new
virtual environments, while preserving the use of real inertial data, as shown in Figure 14. In our future work, we envision
exploiting the automatic generation of ground truth data, especially ﬁne-grained pixel-wise data, to evaluate IoT-supported
ARsystemsunderdiverseenvironmentsettings,suchasassessingmappingaccuracyinSLAM,whichisotherwisediﬃcultto
evaluate in real environments [97].
Fig. 14: An overview of our methodology (left) and a screenshot of the SLAM sequence execution step (right) for our game
engine-based emulator for VI-SLAM using virtual environments [176].
6.2 AR and IoT Device Localization and Calibration
In the game engine-based emulators described above we have access to the ground truth poses of AR and IoT devices,
along with the parameters of IoT sensors and actuators (e.g., correct information on environment regions captured by sensors
and aﬀected by actuators such as light sources). However, a critical challenge for multi-device, IoT-supported AR in real
environments is how we establish all AR and IoT devices’ poses within the same frame of reference, and determine the
environment regions either in view of sensors or which will be adjusted by an actuator. Existing literature provides us with
potentialwaystoaddressthisproblem;forexample,thelocalizationofIoTdevicesisaknownissue,andavarietyofdiﬀerent
solutions have been proposed, including the use of ultra-wideband signals [22] and retroreﬂectors [184] – for a survey of
indoor localization systems and technologies see [212], and for discussion of the challenges speciﬁc to IoT devices, see [61].
EstablishedcameracalibrationtechniquesarealsoanoptionforIoTcameras,suchastheOCamCalibtoolbox[173]employed
in [167] for photometric registration using multiple cameras. For localizing light sources a possible approach is to build on
the work in [58], and apply DNNs to images obtained by AR devices or IoT cameras. Methods are also available to localize
multiple AR devices; cross-platform solutions include Azure Spatial Anchors [128] and Google Cloud Anchors [67], while
the ‘ARWorldMap’ feature can be used on ARKit [14], and ‘Shared Spaces’ on the Magic Leap [120]. The above techniques
provide both inspiration and readily implementable solutions for systems which employ AR and IoT devices.
Basedonouranalysisofexistingtechniques,weidentifyanumberofpossibleapproachesforlocalizingandcalibratingboth
AR and IoT devices, and consider their advantages and disadvantages. The solution that is perhaps most readily implemented
for current commercial AR devices is to identify IoT devices within the frame of reference established by an AR platform.
For example, the pose of devices could be recognized by displaying markers on them or attaching printed markers to them
(see Section 4.1.1 for background on spatial understanding using markers), although this may not be feasible for many small
devices, and has downsides in terms of environment aesthetics. Alternatively, spatial anchors could be manually aligned with
IoT devices [130]; however, this approach could be both labor-intensive and time-consuming, and the accuracy that can be
achieved requires further investigation, as it may be susceptible to human error or easily impacted by environmental factors.
On the other hand, a diﬀerent research direction is to identify AR device poses in the views obtained by IoT cameras, aided
by motion and visual cues from the AR device – this may be particularly useful for collaborative SLAM techniques (e.g.,
to inform loop closing). For the mapping of available environment control actions (e.g., increasing the brightness of a light
bulb or lowering a smart blind) to environment properties as detected by an IoT sensor or AR device, we plan to develop26 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
eﬃcientsamplingstrategiesforlong-termmonitoringofenvironmentproperties,suchthatwecanautomaticallydeterminethe
necessary control actions to achieve speciﬁc conditions, and thereby optimal AR experiences.
6.3 Combining Data from Multiple AR Users and IoT Sensors
Along with localizing and calibrating AR devices, IoT sensors and actuators, we must also consider how to combine the
data from the variety of diﬀerent sensors that are available in the environments that host AR experiences. This sensor data is
diverse, including multimodal sensor data of AR devices and IoT sensors captured from diﬀerent vantage points. Below we
discussthreechallengesrelatedtocombiningdatafrommultipleARusersandIoTsensors,andoutlinetheassociatedresearch
directions.
Communication-eﬃcient AR with multiple users and multiple IoT sensors: In a multi-user, multi-sensor AR scenario,
as the number of AR users and IoT sensors grows, the operational overhead of the IoT-supported system such as bandwidth
consumptionalsoscales[163],duetothetransmissionoflargeamountsofcontextualdatafrommultipleusersortheirambient
environments. The system also needs to maintain a low user-perceived latency [11] to ensure seamless integration with the
realworldandsynchronousperceptionofcontextualinformationamongdiﬀerentusers.Toimprovecommunicationeﬃciency
and reduce user-perceived latency, we envision developing intelligent network-adaptive AR-IoT solutions that adapt the size
and frequency of data transmissions under changing network conditions. Since multiple users and IoT sensors that are in
close proximity usually exhibit a strong correlation in the perceived contextual information, we also envision exploiting the
collaboration among multiple AR users and IoT sensors for communication redundancy elimination.
Assessment and processing of data with diﬀerent signal quality levels: Another research direction in this space is the
qualityassessmentofdatafromdiﬀerentdevicesandthedevelopmentofrobustapproachesforcombiningthisdata.Thesignal
qualitylevelsobtainedfromdiﬀerentvantagepointswillchangeovertimeduetoavarietyoffactors;forexample,othernoise
sources will be interfering with signals captured by diﬀerent microphones, users of AR devices will move closer and farther
from diﬀerentIoT-based cameras,and IoTsensorsin unusualposes willcause the misbehaviorof theobject detectionmodels
that perform well for IoT sensors in canonical poses. We envision the real-time and in-situ evaluation of the quality levels of
data captured by diﬀerent devices at diﬀerent times. After the data quality assessment, we will seek to combine signals with
diﬀerentqualitylevels;weenvisiondesigningalgorithmsthatwilladaptivelyamplify‘good’signalsandplacelessweighton
signals that are less useful, e.g., by integrating the existing design of appropriate attention mechanisms [198].
Multimodaldata: InIoT-supportedARsystems,ARdevicesandIoTsensorscontinuouslycaptureandprocessmultimodal
data(e.g.,visual,auditory,haptic,olfactory,andthermalsensoryinformation).Onepotentialresearchdirectionismultimodal
sensing and learning in which the visual modality enhances, or is enhanced by, other sensing modalities; another is fusing
sensor data with diﬀerent temporal and spatial traits. Given the heterogeneity of the collected data, we envision capturing
correspondences and transferring knowledge between modalities to improve the performance of various aspects of AR func-
tionality, including using gesture and speech for precise multimodal interaction, and using thermal, light, and visual sensors
for detecting scene changes throughout the day.
6.4 Optimizing Environments with Conﬂicting Requirements
Environmentoptimizationsystemswhichautomaticallyadjustpropertiessuchaslightandvisualtexture(seeSection5.1)have
thepotentialtofacilitateARexperiencesofbothhigherandmoreconsistentquality.However,beforethesetypesofsystemsare
readyfordeployment,considerationmustbegiventohowwemanageconﬂictingenvironmentalrequirements.Theseconﬂicts,
alreadyachallengefordesignersofspaceswhichhostAR,willbecommonplaceinpracticalscenarios,aswedescribebelow.
Conﬂicting environmental requirements: Even in an environment in which just one AR user is present, adjusting envi-
ronmental conditions to optimize one aspect of an AR experience may have negative eﬀects on another element of system
functionality or the overall user experience. For example, the addition of visual texture to an environment may improve
spatial understanding, but be distracting for the user [176]. For headsets with an OST display, the low level of illuminance
which maximizes content visibility may result in a reduced level of performance for spatial understanding or eye-trackingAmbient Intelligence for Next-Generation AR 27
algorithms [177]. Furthermore, environmental requirements related to AR may also be at odds with constraints related to
energyeﬃciencyorthecomfortofhumanoccupantsingeneral.Anyenvironmentoptimizationsystemwillneedto(1)deﬁne
the characteristics of the AR devices, applications, and users to be served, in order to deﬁne a reasonable ‘operating range’
in which the system is functional and usable; (2) apply other environmental constraints, e.g., those imposed by building
managers;(3)implementreal-timeanalysisofthesystemelementsandenvironmentregionsusersareinteractingwith,inorder
to determine and prioritize environment adjustments. This latter element is a particularly complex challenge, necessitating
innovative solutions to manage continuously updating environment maps and device poses, and to avoid oscillations between
environment states that may occur due to conﬂicting adjustments. Some of these issues were recently tackled in the context
of shared control of public IoT actuators [95], including device discovery, establishing ‘areas of interest’ for IoT devices, and
aggregating conﬂicting requirements; this work may serve as a valuable starting point for developing AR-speciﬁc systems.
Heterogeneous AR users and devices: The environment optimization challenge becomes even more complex when we
consider the diﬀering requirements of heterogeneous AR devices and users. Diﬀerent devices are equipped with diﬀerent
sensors, run diﬀerent marker detection or VI-SLAM algorithms, and employ diﬀerent types of displays: smartphones may
require greater levels of illuminance and texture to achieve acceptable tracking performance [174], while OST displays may
require lower illuminance than VST displays to achieve the same level of content visibility [20]. Heterogeneous AR users
will have diﬀerent behavior patterns (e.g., mobility characteristics, eye gaze patterns, virtual content interactions), as well as
diﬀerent expectations and requirements for the properties of virtual content. For example, school children exploring an AR
science exhibit may move rapidly, prompting the addition of environmental textures to maintain high-quality pose tracking.
On the other hand, an academic carefully examining virtual content in the same environment might require a view that is
uninhibitedbyatexturedbackground,andalsobemoreconcernedwiththestabilityofvirtualcontent.Whileknowledgeofthe
type of AR devices present in an environment is likely to be relatively straightforward in most cases, capturing and analyzing
informationonthepropertiesofARusersisamorecomplextask,whichwillrequireongoinganalysisofbothindividualusers
anduserpopulations.Thisinturnwillhavesigniﬁcantsecurityandprivacyimplications,whichwediscussnextinSection6.5.
6.5 Secure and Privacy-Preserving AR-IoT Platforms
The development of secure and privacy-preserving AR-IoT platforms is paramount for achieving widespread societal accep-
tance of AR systems incorporated with ambient intelligence technology.
Security: The increase in the number of connected AR and IoT devices and the richness of data collected by them bring
securityconcernstoAR-IoTplatforms.Buildinguponthecategorizationadoptedin[100,166],weclassifyconcernsasrelated
tosecurityofARdevices[41,100,166,169]andIoTsensorsandactuators[9,142]asrelatedto input security oroutput security .
Input security challenges are well known in traditional arenas of networked and computing systems, but become even more
important for AR-IoT platforms with rich multimodal inputs (e.g., RGB, depth, inertial, haptic, auditory sensor readings, and
even gaze tracking data) of AR and IoT devices. Compromised inputs will degrade the performance of AR-IoT platforms,
including localization and tracking accuracy, rendering quality, and the accuracy and completeness of environmental aware-
ness.Tomitigateinputsecurityrisks,potentialresearchdirectionsincludedesigninginputvalidationandsanitizationmethods
that apply to multimodal data from AR and IoT devices, and building fault-tolerant AR-IoT platforms robust to corrupted
input data. AR outputs produced by malicious or bug-ridden applications can also be potentially harmful or distracting. For
example,inthecaseofindustrialARinasmartwarehouse,itwouldbedangerousforavisualoverlaytoobstructtheoperator’s
view; IoT actuators that generate ﬂashing visuals, shrill sounds, or intense haptic signals could cause physiological damage
to the user. To mitigate output security risks, inspired by existing works on adaptive policies to secure visual outputs in
ARdevices[3,41],weenvisionthedevelopmentofreinforcementlearning-basedpoliciesthatpreventdistractionduetotam-
peredcontentofARdevicesorIoTactuators,withthesepoliciesabletoadapttodynamicenvironmentsthroughtrial-and-error.
Privacy: AR-IoTsystems’needforrich,continuoussensordata(frombothARandIoTdevices)raisesprivacyconcernsfor
bothusersandbystandersintheARenvironment.Buggyormaliciousapplicationsmayrecordprivacy-sensitiveinformationon
userstatesorthesurroundingARenvironmentbycompromisingtheARdevices[209]orIoTsensors[194].Tomitigateprivacy
risks, we envision enforcing privacy-preserving policies to limit access to potentially sensitive sensor data. For example, we
will anonymize human faces with an extremely low resolution before feeding images to object detection applications running
onARorIoTdevices;wewillseektopreventapplicationsfrominferringsensitiveinformationaboutanARenvironment(e.g.,
the contents of a home) by concealing information contained in the point cloud constructed by VI-SLAM algorithms.28 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
7 Conclusion
In this book chapter, we explored the variety of ways in which ambient IoT sensors and actuators could be used to support
next-generationAR.WecategorizedtheseusesbydeﬁningﬁvediﬀerentaspectsofARsystemfunctionalityoruserexperience
that may be enhanced – spatial understanding, semantic understanding, contextualized content, interaction, and immersion –
andprovidedanoverviewofrelevantIoTdevices(Section3).Wethenexaminedthepossibilitiesforeachofthesecategoriesin
detail,whenemployingIoTsensors(Section4)andactuators(Section5).Finally,wediscussedanumberofresearchdirections
related to implementing ambient IoT-supported AR systems, along with associated challenges and opportunities for future
work (Section 6).
One important point to make, having reviewed multiple diﬀerent uses of IoT sensors and actuators, is that the devices
physically deployed in a given environment will likely be used for multiple purposes concurrently (e.g., images from a single
IoT camera may serve as input to spatial understanding, semantic understanding, photometric registration, and visual texture
estimation algorithms), so consideration will need to be given to balancing the needs of each use in these cases. Similarly,
our vision for ambient IoT for AR in general, is that it is combined with the use of ambient IoT devices for other purposes;
for example, IoT-based systems incorporating devices also applicable to AR have been proposed for energy eﬃciency [171],
occupant comfort and productivity [19], surveillance [214] and building ventilation [35], while in [196] the authors consider
the role of ubiquitous displays in environment aesthetics. Moreover, as AR becomes more and more integrated into our lives
in the coming years, we envision that built environments, products and materials will be designed with ambient intelligence
for AR in mind, in order to support the use cases we have laid out in this book chapter.
8 Acknowledgements
We thank Guohao Lan, Zida Liu, Yunfan Zhang, Jovan Stojkovic, Achilles Dabrowski, Alex Xu, Ritvik Janamsetty, Tiﬀany
Ma, Owen Gibson, Michael Glushakov and Joseph DeChicchis for their contributions to this work. This work was supported
inpartbyNSFCAREERAwardIIS-2046072,NSFgrantsCNS-2112562andCNS-1908051,FacebookResearchAward,IBM
Research Award, and a Thomas Lord Educational Innovation Grant.
References
1. R. Acuna and V. Willert. Dynamic markers: UAV landing proof of concept. In Proceedings of IEEE Latin American Robotic Symposium,
Brazilian Symposium on Robotics (SBR) and Workshop on Robotics in Education (WRE) , 2018.
2. A.Ahmadyan,L.Zhang,A.Ablavatski,J.Wei,andM.Grundmann. Objectron:Alargescaledatasetofobject-centricvideosinthewildwith
pose annotations. In Proceedings of IEEE/CVF CVPR , 2021.
3. S. Ahn, M. Gorlatova, P. Naghizadeh, M. Chiang, and P. Mittal. Adaptive fog-based output security for augmented reality. In Proceedings of
ACM SIGCOMM VR/AR Network Workshop , 2018.
4. K. Ahuja, S. Pareddy, R. Xiao, M. Goel, and C. Harrison. LightAnchors: Appropriating point lights for spatially-anchored augmented reality
interfaces. In Proceedings of ACM UIST , 2019.
5. A. J. B. Ali, Z. S. Hashemifar, and K. Dantu. Edge-SLAM: Edge-assisted visual simultaneous localization and mapping. In Proceedings of
ACM MobiSys , 2020.
6. Amazon. Amazon AR view. https://www.amazon.com/adlp/arview , 2023.
7. J.AnayaandA.Barbu. Renoir–adatasetforreallow-lightimagenoisereduction. JournalofVisualCommunicationandImageRepresentation ,
51:144–154, 2018.
8. S.Andrist,M.Gleicher,andB.Mutlu.Lookingcoordinated:Bidirectionalgazemechanismsforcollaborativeinteractionwithvirtualcharacters.
InProceedings of ACM CHI , 2017.
9. M.Antonakakis,T.April,M.Bailey,M.Bernhard,E.Bursztein,J.Cochran,Z.Durumeric,J.A.Halderman,L.Invernizzi,M.Kallitsis,etal.
Understanding the Mirai botnet. In Proceedings of USENIX Security , 2017.
10. A. Anwer, S. Ainouz, M. N. M. Saad, S. S. A. Ali, and F. Meriaudeau. Specseg network for specular highlight detection and segmentation in
real-world images. Sensors, 22(17):6552, 2022.
11. K. Apicharttrisorn, B. Balasubramanian, J. Chen, R. Sivaraj, Y.-Z. Tsai, R. Jana, S. Krishnamurthy, T. Tran, and Y. Zhou. Characterization of
multi-user augmented reality over cellular networks. In Proceedings of IEEE SECON , 2020.
12. K. Apicharttrisorn, X. Ran, J. Chen, S. V. Krishnamurthy, and A. K. Roy-Chowdhury. Frugal following: Power thrifty object detection and
tracking for mobile augmented reality. In Proceedings of ACM SenSys , 2019.
13. Apple. ARKit. https://developer.apple.com/augmented-reality/ , 2023.
14. Apple. ARWorldMap. https://developer.apple.com/documentation/arkit/arworldmap , 2023.
15. Apple. Scanning and detecting 3D objects. https://developer.apple.com/documentation/arkit/content_anchors/scanning_
and_detecting_3d_objects , 2023.
16. N. S. Archer, A. Bluﬀ, A. Eddy, C. K. Nikhil, N. Hazell, D. Frank, and A. Johnston. Odour enhances the sense of presence in a virtual reality
environment. Plos one, 17(3):e0265039, 2022.Ambient Intelligence for Next-Generation AR 29
17. ARToolkit. ARToolkit. http://www.artoolkitx.org/ , 2023.
18. ArUco. ArUco. https://pypi.org/project/aruco/ , 2023.
19. A.Aryal,F.Anselmo,andB.Becerik-Gerber. SmartIoTdeskforpersonalizingindoorenvironmentalconditions. In ProceedingsofACMIoT ,
2018.
20. G. Ballestin, F. Solari, and M. Chessa. Perception and action in peripersonal space: A comparison between video and optical see-through
augmented reality devices. In Proceedings of ISMAR-Adjunct , 2018.
21. J. Baumeister, S. Y. Ssin, N. A. ElSayed, J. Dorrian, D. P. Webb, J. A. Walsh, T. M. Simon, A. Irlitti, R. T. Smith, M. Kohler, et al. Cognitive
cost of using augmented reality displays. IEEE Transactions on Visualization and Computer Graphics , 23(11):2378–2388, 2017.
22. P. N. Beuchat, H. Hesse, A. Domahidi, and J. Lygeros. Enabling optimization-based localization for IoT devices. IEEE Internet of Things
Journal, 6(3):5639–5650, 2019.
23. M. Bhattarai, A. R. Jensen-Curtis, and M. Martínez-Ramón. An embedded deep learning system for augmented reality in ﬁreﬁghting
applications. In Proceedings of IEEE ICMLA , 2020.
24. Ó. Blanco-Novoa, P. Fraga-Lamas, M. A Vilar-Montesinos, and T. M. Fernández-Caramés. Creating the Internet of augmented things: An
open-source framework to make IoT devices and augmented and mixed reality systems talk to each other. Sensors, 20(11):3328, 2020.
25. L. Bonanni, C.-H. Lee, and T. Selker. Attention-based design of augmented reality interfaces. In CHI Extended Abstracts , 2005.
26. M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart. The EuRoC micro aerial vehicle datasets.
The International Journal of Robotics Research , 35(10):1157–1163, 2016.
27. C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard. Past, present, and future of simultaneous
localization and mapping: Toward the robust-perception age. IEEE Transactions on Robotics , 32(6):1309–1332, 2016.
28. C. Campos, R. Elvira, J. J. G. Rodríguez, J. M. Montiel, and J. D. Tardós. ORB-SLAM3: An accurate open-source library for visual,
visual–inertial, and multimap SLAM. IEEE Transactions on Robotics , pp. 1–17, 2021.
29. L. M. Castle, D. A. Schuh, E. E. Reynolds, and A. L. Furst. Electrochemical sensors to detect bacterial foodborne pathogens. ACS sensors ,
6(5):1717–1730, 2021.
30. Chaos. V-Ray 3D rendering software. https://www.chaos.com/3d-rendering-software , 2022.
31. D. Chatzopoulos, C. Bermejo, Z. Huang, and P. Hui. Mobile augmented reality survey: From where we are to where we go. IEEE Access ,
5:6917–6950, 2017.
32. P.Chen,Z.Peng,D.Li,andL.Yang. AnimprovedaugmentedrealitysystembasedonAndAR. JournalofVisualCommunicationandImage
Representation , 37:63–69, 2016.
33. Y.Chen,H.Inaltekin,andM.Gorlatova. AdaptSLAM:Edge-assistedadaptiveSLAMwithresourceconstraintsviauncertaintyminimization.
InProceedings of IEEE INFOCOM , 2023.
34. Y. Cheng, Y. Yan, X. Yi, Y. Shi, and D. Lindlbauer. SemanticAdapt: Optimization-based adaptation of mixed reality layouts leveraging
virtual-physical semantic connections. In Proceedings of ACM UIST , 2021.
35. B. Chhaglani, C. Zakaria, A. Lechowicz, J. Gummeson, and P. Shenoy. FlowSense: Monitoring airﬂow in building ventilation systems using
audio sensing. Proceedings of ACM IMWUT , 6(1):1–26, 2022.
36. E.Choi,S.Kim,J.Gong,H.Sun,M.Kwon,H.Seo,O.Sul,andS.-B.Lee. Tactileinteractionsensorwithmillimetersensingacuity. Sensors,
21(13):4274, 2021.
37. S.Choudhary,N.Sekhar,S.Mahendran,andP.Singhal. Multi-user,scalable3DobjectdetectioninARcloud. In ProceedingsofIEEECVPR
Workshop on Computer Vision for Augmented and Virtual Reality , 2020.
38. E.M.Coelho,S.Julier,andB.Maclntyre. Osgar:Ascenegraphwithuncertaintransformations. In ProceedingsofIEEE/ACMISMAR ,2004.
39. L.M.Dang,K.Min,H.Wang,M.J.Piran,C.H.Lee,andH.Moon.Sensor-basedandvision-basedhumanactivityrecognition:Acomprehensive
survey.Pattern Recognition , 108:107561, 2020.
40. S.Debernardis,M.Fiorentino,M.Gattullo,G.Monno,andA.E.Uva. Textreadabilityinhead-worndisplays:Colorandstyleoptimizationin
video versus optical see-through devices. IEEE Transactions on Visualization and Computer Graphics , 20(1):125–139, 2013.
41. J.DeChicchis,S.Ahn,andM.Gorlatova. AdaptiveARvisualoutputsecurityusingreinforcementlearningtrainedpolicies:Demoabstract. In
Proceedings of ACM SenSys , 2019.
42. H.Q.Dinh,N.Walker,L.F.Hodges,C.Song,andA.Kobayashi. Evaluatingtheimportanceofmulti-sensoryinputonmemoryandthesense
of presence in virtual environments. In Proceedings of IEEE VR , 1999.
43. B. Eckstein, E. Krapp, A. Elsässer, and B. Lugrin. Smart substitutional reality: Integrating the smart home into virtual reality. Entertainment
Computing , 31:100306, 2019.
44. D.Englmeier,J.Dörner,A.Butz,andT.Höllerer. Atangiblesphericalproxyforobjectmanipulationinaugmentedreality. In IEEEVR,2020.
45. S. Eom, M. Hadziahmetovic, M. Pajic, and M. Gorlatova. Demo Abstract: Through an AR lens: Augmented reality magniﬁcation through
feature detection and matching. In Proceedings of ACM SenSys , 2022.
46. S. Eom, S. Kim, S. Rahimpour, and M. Gorlatova. AR-assisted surgical guidance system for ventriculostomy. In Proceedings of IEEE VR
Workshops , 2022.
47. S. Eom, D. Sykes, S. Rahimpour, and M. Gorlatova. NeuroLens: Augmented reality-based contextual guidance through surgical tool tracking
in neurosurgery. In Proceedings of IEEE ISMAR , 2022.
48. Epic Games. Unreal engine. https://www.unrealengine.com/ , 2023.
49. A. Erickson, K. Kim, G. Bruder, and G. F. Welch. Exploring the limitations of environment lighting on optical see-through head-mounted
displays. In Proceedings of ACM SUI , 2020.
50. Fever. Van Gogh: The immersive experience. https://vangoghexpo.com/ , 2023.
51. M. Fiala. Designing highly reliable ﬁducial markers. IEEE Transactions on Pattern Analysis and Machine Intelligence , 32(7):1317–1324,
2009.
52. T. Frantz, B. Jansen, J. Duerinck, and J. Vandemeulebroucke. Augmenting Microsoft’s HoloLens with Vuforia tracking for neuronavigation.
Healthcare Technology Letters , 5(5):221–225, 2018.
53. J. L. Gabbard, M. Smith, C. Merenda, G. Burnett, and D. R. Large. A perceptual color-matching method for examining color blending in
augmented reality head-up display graphics. IEEE Transactions on Visualization and Computer Graphics , 28:2834–2851, 2022.
54. J. L. Gabbard, J. E. Swan, D. Hix, S.-J. Kim, and G. Fitch. Active text drawing styles for outdoor augmented reality: A user-based study and
design implications. In Proceedings of IEEE VR , 2007.30 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
55. J. L. Gabbard, J. E. Swan, and A. Zarger. Color blending in outdoor optical see-through AR: The eﬀect of real-world backgrounds on user
interface color. In Proceedings of IEEE VR , 2013.
56. R. Gal, L. Shapira, E. Ofek, and P. Kohli. FLARE: Fast layout for augmented reality applications. In Proceedings of IEEE ISMAR , 2014.
57. J.Gao,N.Zhang,D.Ji,H.Song,Y.Liu,L.Zhou,Z.Sun,J.M.Jornet,A.C.Thompson,R.L.Collins,etal. Superabsorbingmetasurfaceswith
hybrid ag–au nanostructures for surface-enhanced raman spectroscopy sensing of drugs and chemicals. Small Methods , 2(7):1800045, 2018.
58. M.-A. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gambaretto, C. Gagné, and J.-F. Lalonde. Learning to predict indoor illumination from
a single image. arXiv preprint arXiv:1704.00090 , 2017.
59. S. Garrido-Jurado, R. Muñoz-Salinas, F. J. Madrid-Cuevas, and M. J. Marín-Jiménez. Automatic generation and detection of highly reliable
ﬁducial markers under occlusion. Pattern Recognition , 47(6):2280–2292, 2014.
60. F. Ghorbani, A. Ahmadi, M. Kia, Q. Rahman, and M. Delrobaei. A decision-aware ambient assisted living system with IoT embedded device
for in-home monitoring of older adults. Sensors, 23(5):2673, 2023.
61. S. Ghorpade, M. Zennaro, and B. Chaudhari. Survey of localization for Internet of Things nodes: Approaches, challenges and open issues.
Future Internet , 13(8):210, 2021.
62. G. Giraldo, M. Servières, and G. Moreau. Towards a sensitive urban wind representation in virtual reality. ISPRS International Journal of
Geo-Information , 11(4):239, 2022.
63. M.Glushakov,Y.Zhang,Y.Han,T.J.Scargill,G.Lan,andM.Gorlatova. Edge-basedprovisioningofholographiccontentforcontextualand
personalized augmented reality. In Proceedings of IEEE PerCom Workshops , 2020.
64. D. Gooch. An Investigation into Communicating Social Presence With Thermal Devices . PhD thesis, MSc Dissertation, 2009.
65. Google. ARCore. https://arvr.google.com/arcore/ , 2023.
66. Google. ARCore: Get the lighting right. https://developers.google.com/ar/develop/lighting-estimation , 2023.
67. Google. Cloud Anchors. https://developers.google.com/ar/develop/cloud-anchors , 2023.
68. L. Gruber, T. Richter-Trummer, and D. Schmalstieg. Real-time photometric registration from arbitrary geometry. In Proceedings of IEEE
ISMAR, 2012.
69. J. Grubert, T. Langlotz, S. Zollmann, and H. Regenbrecht. Towards pervasive augmented reality: Context-awareness in augmented reality.
IEEE Transactions on Visualization and Computer Graphics , 23(6):1706–1724, 2017.
70. Y. Guan, X. Hou, N. Wu, B. Han, and T. Han. DeepMix: Mobility-aware, lightweight, and hybrid 3D object detection for headsets. In
Proceedings of ACM MobiSys , 2022.
71. H.-J. Guo and B. Prabhakaran. HoloLens 2 technical evaluation as mixed reality guide. arXiv preprint arXiv:2207.09554 , 2022.
72. D.Hallaway,S.Feiner,andT.Höllerer.Bridgingthegaps:Hybridtrackingforadaptivemobileaugmentedreality. AppliedArtiﬁcialIntelligence ,
18(6):477–500, 2004.
73. M. Hansard, S. Lee, O. Choi, and R. Horaud. Time-of-ﬂight Cameras: Principles, Methods and Applications . Springer, 2012.
74. S. Herrmann. Object detection with Microsoft HoloLens 2. A comparison between image and point cloud based algorithms . PhD thesis, TU
Wien, 2021.
75. M. Hirzer. Marker detection for augmented reality applications. In Seminar/Project Image Analysis Graz , vol. 25, 2008.
76. M.Hoppe,D.Oskina,A.Schmidt,andT.Kosch. Odin’shelmet:Ahead-wornhapticfeedbackdevicetosimulateg-forcesonthehumanbody
in virtual reality. Proceedings of ACM on Human-Computer Interaction , 5(EICS):1–15, 2021.
77. HTC. Vive tracker. https://www.vive.com/us/accessory/tracker3/ , 2023.
78. M. Hu, D. Weng, F. Chen, and Y. Wang. Object detecting augmented reality system. In Proceedings of IEEE ICCT , 2020.
79. X. Huang, G. Mei, J. Zhang, and R. Abbas. A comprehensive survey on point cloud registration. arXiv preprint arXiv:2103.02690 , 2021.
80. K. Huo, Y. Cao, S. H. Yoon, Z. Xu, G. Chen, and K. Ramani. Scenariot: Spatially mapping smart things within augmented reality scenes. In
Proceedings of ACM CHI , 2018.
81. B. Huynh, J. Orlosky, and T. Höllerer. In-situ labeling for augmented reality language learning. In Proceedings of IEEE VR , 2019.
82. IKEA. IKEA Place app. https://www.ikea.com/us/en/customer-service/mobile-apps/ , 2023.
83. L.Jinyu,Y.Bangbang,C.Danpeng,W.Nan,Z.Guofeng,andB.Hujun. Surveyandevaluationofmonocularvisual-inertialSLAMalgorithms
for augmented reality. Virtual Reality & Intelligent Hardware , 1(4):386–410, 2019.
84. D. Jo and G. J. Kim. ARIoT: Scalable augmented reality framework for interacting with Internet of Things appliances everywhere. IEEE
Transactions on Consumer Electronics , 62(3):334–340, 2016.
85. D. Jo and G. J. Kim. IoT+ AR: Pervasive and augmented environments for “Digi-log” shopping experience. Human-centric Computing and
Information Sciences , 9(1):1–17, 2019.
86. D.Kahl,M.Ruble,andA.Krüger. Theinﬂuenceofenvironmentallightingonsizevariationsinopticalsee-throughtangibleaugmentedreality.
InProceedings of IEEE VR , 2022.
87. M.Kalaitzakis,B.Cain,S.Carroll,A.Ambrosi,C.Whitehead,andN.Vitzilaios. Fiducialmarkersforposeestimation. JournalofIntelligent
& Robotic Systems , 101(4):1–26, 2021.
88. M. Kanbara and N. Yokoya. Real-time estimation of light source environment for photorealistic augmented reality. In Proceedings of ICPR ,
2004.
89. M.Karrer,P.Schmuck,andM.Chli. CVI-SLAM-collaborativevisual-inertialSLAM. IEEERoboticsandAutomationLetters ,3(4):2762–2769,
2018.
90. D.Khan,S.Ullah,andI.Rabbi. FactorsaﬀectingthedesignandtrackingofARToolKitmarkers. ComputerStandards&Interfaces ,41:56–66,
2015.
91. D. Kim, Y. Cao, D. Mariappan, M. S. Bono Jr, A. J. Hart, and B. Marelli. A microneedle technology for sampling and sensing bacteria in the
food supply chain. Advanced Functional Materials , 31(1):2005370, 2021.
92. H.Kim,Y.-T.Kwon,H.-R.Lim,J.-H.Kim,Y.-S.Kim,andW.-H.Yeo. Recentadvancesinwearablesensorsandintegratedfunctionaldevices
for virtual and augmented reality applications. Advanced Functional Materials , 31(39):2005692, 2021.
93. K. Kim, L. Boelling, S. Haesler, J. Bailenson, G. Bruder, and G. F. Welch. Does a digital assistant need a body? the inﬂuence of visual
embodiment and social behavior on the perception of intelligent virtual agents in AR. In Proceedings of IEEE ISMAR , 2018.
94. K. Kim, G. Bruder, and G. F. Welch. Blowing in the wind: Increasing copresence with a virtual human via airﬂow inﬂuence in augmented
reality. In Proceedings of ICAT-EGVE , 2018.
95. W.Kim,S.Lee,Y.Chang,T.Lee,I.Hwang,andJ.Song. Hivemind:Socialcontrol-and-useofIoTtowardsdemocratizationofpublicspaces.
InProceedings of ACM MobiSys , 2021.Ambient Intelligence for Next-Generation AR 31
96. Y.-J. Kim, R. Kumaran, E. Sayyad, A. Milner, T. Bullock, B. Giesbrecht, and T. Höllerer. Investigating search among physical and virtual
objects under diﬀerent lighting conditions. IEEE Transactions on Visualization and Computer Graphics , 28(11):3788–3798, 2022.
97. K. Koide, J. Miura, M. Yokozuka, S. Oishi, and A. Banno. Interactive 3D graph SLAM for map correction. IEEE Robotics and Automation
Letters, 6(1):40–47, 2021.
98. D. Kurz and S. Benhimane. Gravity-aware handheld augmented reality. In Proceedings of IEEE ISMAR , 2011.
99. H. Le, M. Nguyen, W. Q. Yan, and H. Nguyen. Augmented reality and machine learning incorporation using YOLOv3 and ARKit. Applied
Sciences, 11(13):6006, 2021.
100. K.Lebeck,K.Ruth,T.Kohno,andF.Roesner. Towardssecurityandprivacyformulti-useraugmentedreality:Foundationswithendusers. In
Proceedings of IEEE S&P , 2018.
101. C. H. Lee. Location-aware speakers for the virtual reality environments. IEEE Access , 5:2636–2640, 2017.
102. J. Lee, J. Bang, and S.-I. Yang. Object detection with sliding window in images including multiple similar objects. In Proceedings of IEEE
ICTC, 2017.
103. T. Lee and T. Hollerer. Hybrid feature tracking and user interaction for markerless augmented reality. In Proceedings of IEEE VR , 2008.
104. T. Leppänen, A. Heikkinen, A. Karhu, E. Harjula, J. Riekki, and T. Koskela. Augmented reality web applications with mobile agents in the
Internet of Things. In Proceedings of IEEE International Conference on Next Generation Mobile Apps, Services and Technologies , 2014.
105. X. Li, Y. Tian, F. Zhang, S. Quan, and Y. Xu. Object detection in the context of mobile augmented reality. In Proceedings of IEEE ISMAR ,
2020.
106. Y. Li, W. Zhou, B. Zu, and X. Dou. Qualitative detection toward military and improvised explosive vapors by a facile tiO2 nanosheet-based
chemiresistive sensor array. Frontiers in Chemistry , 8:29, 2020.
107. C.-C. Lin. Eﬀects of contrast ratio and text color on visual performance with TFT-LCD. International journal of industrial ergonomics ,
31(2):65–72, 2003.
108. M. A. Lin, A. F. Siu, J. H. Bae, M. R. Cutkosky, and B. L. Daniel. HoloNeedle: Augmented reality guidance system for needle placement
investigatingtheadvantagesofthree-dimensionalneedleshapereconstruction. IEEERoboticsandAutomationLetters ,3(4):4156–4162,2018.
109. P. Lin, Q. Song, D. Wang, F. R. Yu, L. Guo, and V. C. Leung. Resource management for pervasive-edge-computing-assisted wireless VR
streaming in industrial Internet of Things. IEEE Transactions on Industrial Informatics , 17(11):7607–7617, 2021.
110. D. Lindlbauer, A. M. Feit, and O. Hilliges. Context-aware online adaptation of mixed reality interfaces. In Proceedings of ACM UIST , 2019.
111. H.LiuandL.Wang.Gesturerecognitionforhuman-robotcollaboration:Areview. InternationalJournalofIndustrialErgonomics ,68:355–367,
2018.
112. L.Liu,H.Li,andM.Gruteser. Edgeassistedreal-timeobjectdetectionformobileaugmentedreality. In ProceedingsofACMMobiCom ,2019.
113. M. Liu, Y. Zhang, J. Wang, N. Qin, H. Yang, K. Sun, J. Hao, L. Shu, J. Liu, Q. Chen, et al. A star-nose-like tactile-olfactory bionic sensing
array for robust object recognition in non-visual environments. Nature Communications , 13(1):1–10, 2022.
114. Z. Liu, G. Lan, J. Stojkovic, Y. Zhang, C. Joe-Wong, and M. Gorlatova. CollabAR: Edge-assisted collaborative image recognition for mobile
augmented reality. In Proceedings of IEEE IPSN , 2020.
115. J.A.G.Macias,J.Alvarez-Lozano,P.Estrada,andE.A.Lopez. BrowsingtheInternetofThingswithsentientvisors. Computer ,44(5):46–52,
2011.
116. B. MacIntyre and E. M. Coelho. Adapting to dynamic registration errors using level of error (LOE) ﬁltering. In Proceedings of IEEE/ACM
ISMAR, 2000.
117. B. MacIntyre, E. M. Coelho, and S. J. Julier. Estimating and adapting to registration errors in augmented reality systems. In Proceedings of
IEEE VR, 2002.
118. T. Maeda, S. Yoshida, T. Murakami, K. Matsuda, T. Tanikawa, and H. Sakai. Fingeret: A wearable ﬁngerpad-free haptic device for mixed
reality. In Proceedings of ACM SUI , 2022.
119. Magic Leap. Magic Leap 2 device. https://www.magicleap.com/magic-leap-2 , 2023.
120. Magic Leap. Shared Spaces. https://www.magicleap.com/spatial-mapping-ml2 , 2023.
121. D. Mandl, K. M. Yi, P. Mohr, P. M. Roth, P. Fua, V. Lepetit, D. Schmalstieg, and D. Kalkofen. Learning lightprobes for mixed reality
illumination. In Proceedings of IEEE ISMAR , 2017.
122. E. Marchand, H. Uchiyama, and F. Spindler. Pose estimation for augmented reality: A hands-on survey. IEEE Transactions on Visualization
and Computer Graphics , 22(12):2633–2651, 2015.
123. K.Masai,K.Kunze,D.Sakamoto,Y.Sugiura,andM.Sugimoto. Facecommands-user-deﬁnedfacialgesturesforsmartglasses. In Proceedings
of IEEE ISMAR , 2020.
124. K. Matkovic, T. Psik, I. Wagner, and D. Gracanin. Dynamic texturing of real objects in an augmented reality system . IEEE, 2005.
125. L.Meli,C.Pacchierotti,G.Salvietti,F.Chinello,M.Maisto,A.DeLuca,andD.Prattichizzo.Combiningwearableﬁngerhapticsandaugmented
reality:UserevaluationusinganexternalcameraandtheMicrosoftHoloLens. IEEERoboticsandAutomationLetters ,3(4):4297–4304,2018.
126. N. Merrill, P. Geneva, and G. Huang. Robust monocular visual-inertial depth completion for embedded systems. In Proceedings of IEEE
ICRA, 2021.
127. K. Michalakis, J. Aliprantis, and G. Caridakis. Visualizing the Internet of Things: Naturalizing human-computer interaction by incorporating
AR features. IEEE Consumer Electronics Magazine , 7(3):64–72, 2018.
128. Microsoft. Azure Spatial Anchors. https://azure.microsoft.com/en-us/services/spatial-anchors/ , 2023.
129. Microsoft. Microsoft HoloLens. https://www.microsoft.com/en-us/hololens , 2023.
130. Microsoft. Spatial Anchors. https://learn.microsoft.com/en-us/windows/mixed-reality/design/spatial-anchors , 2023.
131. A. G. Millard, R. A. Joyce, and I. Gray. Human-swarm interaction via E-Ink displays. In Proceedings of ICRA Workshops , 2020.
132. V. T. Minh, N. Katushin, and J. Pumwa. Motion tracking glove for augmented reality and virtual reality. Paladyn, Journal of Behavioral
Robotics, 10(1):160–166, 2019.
133. M. H. Miraz, M. Ali, and P. S. Excell. Adaptive user interfaces and universal usability through plasticity of user interface design. Computer
Science Review , 40:100363, 2021.
134. A. Mishra, P. Kumar, J. Shukla, and A. Parnami. HaptiDrag: A device with the ability to generate varying levels of drag (friction) eﬀects on
real surfaces. Proceedings of ACM IMWUT , 6(3):1–26, 2022.
135. Z. Mohammadi and S. M. Jafari. Detection of food spoilage and adulteration by novel nanomaterial-based sensors. Advances in Colloid and
Interface Science , 286:102297, 2020.32 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
136. E. Morotti, L. Donatiello, and G. Marﬁa. Fostering fashion retail experiences through virtual reality and voice assistants. In Proceedings of
IEEE VR Abstracts and Workshops , 2020.
137. J. Morrish and M. Arnott. Global IoT forecast report, 2021-2030. https://transformainsights.com/research/reports/
global-iot-forecast-report-2030 , 2022.
138. R.Mur-ArtalandJ.D.Tardós. ORB-SLAM2:Anopen-sourceSLAMsystemformonocular,stereo,andRGB-Dcameras. IEEETransactions
on Robotics , 33(5):1255–1262, 2017.
139. I. Nahum-Shani, S. N. Smith, B. J. Spring, L. M. Collins, K. Witkiewitz, A. Tewari, and S. A. Murphy. Just-in-time adaptive interventions
(JITAIs) in mobile health: Key components and design principles for ongoing health behavior support. Annals of Behavioral Medicine ,
52(6):446–462, 2018.
140. W.NatephraandA.Motamedi. LivedatavisualizationofIoTsensorsusingaugmentedreality(AR)andBIM. In ProceedingsofInternational
Symposium on Automation and Robotics in Construction (ISARC) , 2019.
141. B. W. Nelson and N. B. Allen. Extending the passive-sensing toolbox: Using smart-home technology in psychological science. Perspectives
on Psychological Science , 13(6):718–733, 2018.
142. N.Neshenko,E.Bou-Harb,J.Crichigno,G.Kaddoum,andN.Ghani. DemystifyingIoTsecurity:AnexhaustivesurveyonIoTvulnerabilities
and a ﬁrst empirical look on internet-scale IoT exploitations. IEEE Communications Surveys & Tutorials , 21(3):2702–2733, 2019.
143. L. H. Nguyen, S. Naﬁcy, R. McConchie, F. Dehghani, and R. Chandrawati. Polydiacetylene-based sensors to detect food spoilage at low
temperatures. Journal of Materials Chemistry C , 7(7):1919–1926, 2019.
144. Niantic. Pokemon GO app. https://www.pokemongo.com/ , 2023.
145. B. Nuernberger, E. Ofek, H. Benko, and A. D. Wilson. SnapToReality: Aligning augmented reality to the real world. In Proceedings of ACM
CHI, 2016.
146. E. Olson. AprilTag: A robust and ﬂexible visual ﬁducial system. In Proceedings of IEEE ICRA , 2011.
147. T. Ophoﬀ, K. Van Beeck, and T. Goedemé. Exploring RGB+Depth fusion for real-time object detection. Sensors, 19(4):866, 2019.
148. OptiTrack. OptiTrack. https://optitrack.com/ , 2023.
149. F. Pankratz, A. Dippon, T. Coskun, and G. Klinker. User awareness of tracking uncertainties in ar navigation scenarios. In Proceedings of
IEEE ISMAR , 2013.
150. G. Paolocci, T. L. Baldi, D. Barcelli, and D. Prattichizzo. Combining wristband display and wearable haptics for augmented reality. In
Proceedings of IEEE VR Workshops , 2020.
151. Y. Park, S. Yun, and K.-H. Kim. When IoT met augmented reality: Visualizing the source of the wireless signal in AR view. In Proceedings
of ACM MobiSys , 2019.
152. R. L. Peiris, O. N. N. Fernando, C. S. Bee, A. D. Cheok, A. G. Ganesan, and P. Kumarasinghe. dMarkers: Ubiquitous dynamic makers for
augmented reality. In Proceedings of ACM VRCAI , 2011.
153. S. Persky and A. P. Dolwick. Olfactory perception and presence in a virtual reality food environment. Frontiers in Virtual Reality , 1:571812,
2020.
154. P. Phupattanasilp and S.-R. Tong. Augmented reality in the integrative Internet of Things (AR-IoT): Application for precision farming.
Sustainability , 11(9):2658, 2019.
155. S. Prakash, A. Bahremand, L. D. Nguyen, and R. LiKamWa. GLEAM: An illumination estimation framework for real-time photorealistic
augmented reality on mobile devices. In Proceedings of ACM MobiSys , 2019.
156. P. Preechayasomboon and E. Rombokas. Haplets: Finger-worn wireless and low-encumbrance vibrotactile haptic feedback for virtual and
augmented reality. Frontiers in Virtual Reality , 2:738613, 2021.
157. F.Pujol-Vila,R.Villa,andM.Alvarez. Nanomechanicalsensorsasatoolforbacteriadetectionandantibioticsusceptibilitytesting. Frontiers
in Mechanical Engineering , 6:44, 2020.
158. X.Qiao,P.Ren,S.Dustdar,L.Liu,H.Ma,andJ.Chen. WebAR:Apromisingfutureformobileaugmentedreality—stateoftheart,challenges,
and insights. Proceedings of the IEEE , 107(4):651–666, 2019.
159. T. Qin, P. Li, and S. Shen. VINS-Mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics ,
34(4):1004–1020, 2018.
160. J.Qiu,Z.Cui,Y.Zhang,X.Zhang,S.Liu,B.Zeng,andM.Pollefeys. DeepLiDAR:Deepsurfacenormalguideddepthpredictionforoutdoor
scene from sparse LiDAR data and single color image. In Proceedings of IEEE CVPR , 2019.
161. F. Rabbi, T. Park, B. Fang, M. Zhang, and Y. Lee. When virtual reality meets Internet of Things in the gym: Enabling immersive interactive
machine exercises. Proceedings of ACM IMWUT , 2(2):1–21, 2018.
162. X. Ran, H. Chen, X. Zhu, Z. Liu, and J. Chen. DeepDecision: A mobile deep learning framework for edge video analytics. In Proceedings of
IEEE INFOCOM , 2018.
163. X.Ran,C.Slocum,Y.-Z.Tsai,K.Apicharttrisorn,M.Gorlatova,andJ.Chen. Multi-useraugmentedrealitywithcommunicationeﬃcientand
spatially consistent virtual objects. In Proceedings of ACM CoNEXT , 2020.
164. M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind. Hypersim: A photorealistic
synthetic dataset for holistic indoor scene understanding. In Proceedings of IEEE/CVF ICCV , 2021.
165. C. M. Robertson. Using graphical context to reduce the eﬀects of registration error in augmented reality . PhD thesis, Georgia Institute of
Technology, 2007.
166. F. Roesner, T. Kohno, and D. Molnar. Security and privacy for augmented reality systems. Communications of the ACM , 57(4):88–96, 2014.
167. K.Rohmer,W.Büschel,R.Dachselt,andT.Grosch. Interactivenear-ﬁeldilluminationforphotorealisticaugmentedrealityonmobiledevices.
InProceedings of IEEE ISMAR , 2014.
168. E. Rublee, V. Rabaud, K. Konolige, and G. Bradski. ORB: An eﬃcient alternative to SIFT or SURF. In Proceedings of IEEE ICCV , 2011.
169. K.Ruth,T.Kohno,andF.Roesner. Securemulti-usercontentsharingforaugmentedrealityapplications. In ProceedingsofUSENIXSecurity
Symposium , 2019.
170. S. Saeidi, G. Rentala, T. Rizzuto, T. Hong, N. Johannsen, and Y. Zhu. Exploring thermal state in mixed immersive virtual environments.
Journal of Building Engineering , 44:102918, 2021.
171. L.Salman,S.Salman,S.Jahangirian,M.Abraham,F.German,C.Blair,andP.Krenz. EnergyeﬃcientIoT-basedsmarthome. In Proceedings
of IEEE WF-IoT , 2016.
172. T. Santini, W. Fuhl, and E. Kasneci. Pure: Robust pupil detection for real-time pervasive eye tracking. Computer Vision and Image
Understanding , 170:40–50, 2018.Ambient Intelligence for Next-Generation AR 33
173. D. Scaramuzza. OCamCalib: Omnidirectional camera calibration toolbox for Matlab. https://sites.google.com/site/scarabotix/
ocamcalib-omnidirectional-camera-calibration-toolbox-for-matlab , 2023.
174. T.Scargill,J.Chen,andM.Gorlatova. Heretostay:Measuringhologramstabilityinmarkerlesssmartphoneaugmentedreality. arXivpreprint
arXiv:2109.14757 , 2021.
175. T. Scargill, Y. Chen, S. Eom, J. Dunn, and M. Gorlatova. Environmental, user, and social context-aware augmented reality for supporting
personal development and change. In Proceedings of IEEE VR Workshops , 2022.
176. T.Scargill,Y.Chen,N.Marzen,andM.Gorlatova. Integrateddesignofaugmentedrealityspacesusingvirtualenvironments. In Proceedings
of IEEE ISMAR , 2022.
177. T.Scargill,A.Dabrowski,A.Xu,andM.Gorlatova. IoT-enabledenvironmentilluminanceoptimizationforaugmentedreality. In Proceedings
of ACM UbiComp , 2022.
178. T. Scargill, S. Hurli, J. Chen, and M. Gorlatova. Demo: Will it move? Indoor scene characterization for hologram stability in mobile AR. In
Proceedings of ACM HotMobile , 2021. Demo video available at https://sites.duke.edu/timscargill/sceneit-prototype/ .
179. T. Scargill, G. Lan, and M. Gorlatova. Demo abstract: Catch my eye: Gaze-based activity recognition in an augmented reality art gallery. In
Proceedings of ACM/IEEE IPSN , 2022. Demo video available at https://sites.duke.edu/timscargill/catchmyeye-demo/ .
180. P. Schmuck and M. Chli. CCM-SLAM: Robust and eﬃcient centralized collaborative monocular simultaneous localization and mapping for
robotic teams. Journal of Field Robotics , 36(4):763–781, 2019.
181. D.Schubert,T.Goll,N.Demmel,V.Usenko,J.Stückler,andD.Cremers. TheTUMVIbenchmarkforevaluatingvisual-inertialodometry. In
Proceedings of IEEE/RSJ IROS , 2018.
182. S. Sefati, C. Gao, I. Iordachita, R. H. Taylor, and M. Armand. Data-driven shape sensing of a surgical continuum manipulator using an
uncalibrated ﬁber bragg grating sensor. IEEE Sensors Journal , 21(3):3066–3076, 2020.
183. S. Sefati, R. Hegeman, F. Alambeigi, I. Iordachita, P. Kazanzides, H. Khanuja, R. H. Taylor, and M. Armand. A surgical robotic system
for treatment of pelvic osteolysis using an FBG-equipped continuum manipulator and ﬂexible instruments. IEEE/ASME Transactions on
Mechatronics , 26(1):369–380, 2020.
184. S. Shao, A. Khreishah, and I. Khalil. RETRO: Retroreﬂector based visible light indoor localization for real-time tracking of IoT devices. In
Proceedings of IEEE INFOCOM , 2018.
185. S.Shen,N.Roy,J.Guan,H.Hassanieh,andR.R.Choudhury. MUTE:BringingIoTtonoisecancellation. In ProceedingsofACMSIGCOMM ,
2018.
186. V. Shen, C. Shultz, and C. Harrison. Mouth haptics in VR using a headset ultrasound phased array. In Proceedings of ACM CHI , 2022.
187. Y. Shi, H. Zhang, K. Zhao, J. Cao, M. Sun, and S. Nanayakkara. Ready, steady, touch! sensing physical contact with a ﬁnger-mounted IMU.
Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies , 4(2):1–25, 2020.
188. H. Soh,S. Sanner,M.White, andG. Jamieson. Deepsequential recommendationforpersonalized adaptiveuser interfaces. In Proceedingsof
ACM IUI, 2017.
189. J. Steuer. Deﬁning virtual reality: Dimensions determining telepresence. Journal of communication , 42(4):73–93, 1992.
190. Y. Sun, A. Armengol-Urpi, S. N. R. Kantareddy, J. Siegel, and S. Sarma. MagicHand: Interact with IoT devices in augmented reality
environment. In Proceedings of IEEE VR , 2019.
191. Y. Sun, S. N. R. Kantareddy, J. Siegel, A. Armengol-Urpi, X. Wu, H. Wang, and S. Sarma. Towards industrial IoT-AR systems using deep
learning-based object pose estimation. In Proceedings of IEEE IPCCC , 2019.
192. Z. Sun, M. Zhu, X. Shan, and C. Lee. Augmented tactile-perception and haptic-feedback rings as human-machine interfaces aiming for
immersive interactions. Nature communications , 13(1):1–13, 2022.
193. T. Tahara, T. Seno, G. Narita, and T. Ishikawa. Retargetable AR: Context-aware augmented reality in indoor scenes based on 3D scene graph.
InProceedings of IEEE ISMAR-Adjunct , 2020.
194. L.Tawalbeh,F.Muheidat,M.Tawalbeh,andM.Quwaider. IoTprivacyandsecurity:Challengesandsolutions. AppliedSciences ,10(12):4102,
2020.
195. Texas Instruments. Introduction to time-of-ﬂight long range proximity and distance sensor system design (Rev. B). https://www.ti.com/
lit/pdf/sbau305 , 2019.
196. N. Tractinsky and E. Eytam. Considering the aesthetics of ubiquitous displays. In Ubiquitous Display Environments , pp. 89–104. Springer,
2012.
197. Unity. Unity real-time development platform. https://unity.com/ , 2023.
198. A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polosukhin. Attentionisallyouneed. In Proceedings
of NIPS, 2017.
199. Vicon. Vicon. https://www.vicon.com/ , 2023.
200. Vuforia. Vuforia. https://library.vuforia.com/objects/image-targets , 2023.
201. H. Wang, B. Kim, J. L. Xie, and Z. Han. LEAF+ AIO: Edge-assisted energy-aware object detection for mobile augmented reality. IEEE
Transactions on Mobile Computing , 2022.
202. Y. Wang, P.-M. Jodoin, F. Porikli, J. Konrad, Y. Benezeth, and P. Ishwar. CDnet 2014: An expanded change detection benchmark dataset. In
Proceedings of IEEE CVPR Workshops , 2014.
203. S.Weiss,M.W.Achtelik,S.Lynen,M.Chli,andR.Siegwart. Real-timeonboardvisual-inertialstateestimationandself-calibrationofMAVs
in unknown environments. In Proceedings of IEEE ICRA , 2012.
204. S. Wen, J. Chen, F. R. Yu, F. Sun, Z. Wang, and S. Fan. Edge computing-based collaborative vehicles 3D mapping in real time. IEEE
Transactions on Vehicular Technology , 69(11):12470–12481, 2020.
205. D. Xu,A. Zhou,G. Wang, H.Zhang, X.Li, J.Pei, andH. Ma. Tutti: Coupling5G RANand mobileedge computing forlatency-critical video
analytics. In Proceedings of ACM MobiCom , 2022.
206. H.Xu,M.A.Peshkin,andJ.E.Colgate. UltraShiver:Lateralforcefeedbackonabareﬁngertipviaultrasonicoscillationandelectroadhesion.
IEEE Transactions on Haptics , 12(4):497–507, 2019.
207. J. Xu, H. Cao, D. Li, K. Huang, C. Qian, L. Shangguan, and Z. Yang. Edge assisted mobile semantic visual SLAM. In Proceedings of IEEE
INFOCOM , 2020.
208. J. Xu, H. Cao, Z. Yang, L. Shangguan, J. Zhang, X. He, and Y. Liu. SwarmMap: Scaling up real-time collaborative visual SLAM at the edge.
InProceedings of USENIX NSDI , 2022.34 Tim Scargill, Sangjun Eom, Ying Chen, Maria Gorlatova
209. X. Yang and X. Zhang. A study of user privacy in Android mobile AR apps. In Proceedings of IEEE/ACM International Conference on
Automated Software Engineering , 2022.
210. Z.Yang,Y.-L.Wei,S.Shen,andR.R.Choudhury.Ear-AR:Indooracousticaugmentedrealityonearphones.In ProceedingsofACMMobiCom ,
2020.
211. T. Zachariah and P. Dutta. Browsing the web of things in mobile augmented reality. In Proceedings of ACM HotMobile , 2019.
212. F.Zafari,A.Gkelias,andK.K.Leung. Asurveyofindoorlocalizationsystemsandtechnologies. IEEECommunicationsSurveys&Tutorials ,
21(3):2568–2599, 2019.
213. T. Zhan, K. Yin, J. Xiong, Z. He, and S.-T. Wu. Augmented reality and virtual reality displays: Perspectives and challenges. Iscience,
23(8):101397, 2020.
214. T. Zhang, A. Chowdhery, P. Bahl, K. Jamieson, and S. Banerjee. The design and implementation of a wireless video surveillance system. In
Proceedings of ACM MobiCom , 2015.
215. T.Zhang,H.Zhang,Y.Li,Y.Nakamura,andL.Zhang. FlowFusion:DynamicdenseRGB-DSLAMbasedonopticalﬂow. In Proceedingsof
IEEE ICRA , 2020.
216. W.Zhang,B.Han,andP.Hui. Onthenetworkingchallengesofmobileaugmentedreality. In ProceedingsofACMWorkshoponVirtualReality
and Augmented Reality Network , 2017.
217. W.Zhang,B.Han,andP.Hui. SEAR:Scalingexperiencesinmulti-useraugmentedreality. IEEETransactionsonVisualizationandComputer
Graphics, 28(5):1982–1992, 2022.
218. X. Zhang, S. Fronz, and N. Navab. Visual marker detection and decoding in AR systems: A comparative study. In Proceedings of IEEE
ISMAR, 2002.
219. Y. Zhang and T. Funkhouser. Deep depth completion of a single RGB-D image. In Proceedings of IEEE CVPR , 2018.
220. Y. Zhang, T. Scargill, A. Vaishnav, G. Premsankar, M. Di Francesco, and M. Gorlatova. InDepth: Real-time depth inpainting for mobile
augmented reality. In Proceedings of ACM IMWUT , 2022.
221. Z.Zhang,J.Geiger,J.Pohjalainen,A.E.-D.Mousa,W.Jin,andB.Schuller. Deeplearningforenvironmentallyrobustspeechrecognition:An
overview of recent developments. ACM Transactions on Intelligent Systems and Technology (TIST) , 9(5):1–28, 2018.
222. Y. Zhao and T. Guo. PointAR: Eﬃcient lighting estimation for mobile augmented reality. In Proceedings of ECCV , 2020.
223. Y. Zhao and T. Guo. Xihe: A 3D vision-based lighting estimation framework for mobile augmented reality. In Proceedings of ACM MobiSys ,
2021.