JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
SVFAP: Self-supervised Video Facial Affect
Perceiver
Licai Sun, Zheng Lian, Kexin Wang, Yu He, Mingyu Xu, Haiyang Sun, Bin Liu, Member, IEEE,
and Jianhua Tao, Senior Member, IEEE
Abstract —Video-based facial affect analysis has recently attracted increasing attention owing to its critical role in human-computer
interaction. Previous studies mainly focus on developing various deep learning architectures and training them in a fully supervised
manner. Although significant progress has been achieved by these supervised methods, the longstanding lack of large-scale
high-quality labeled data severely hinders their further improvements. Motivated by the recent success of self-supervised learning in
computer vision, this paper introduces a self-supervised approach, termed Self-supervised Video Facial Affect Perceiver (SVFAP), to
address the dilemma faced by supervised methods. Specifically, SVFAP leverages masked facial video autoencoding to perform
self-supervised pre-training on massive unlabeled facial videos. Considering that large spatiotemporal redundancy exists in facial
videos, we propose a novel temporal pyramid and spatial bottleneck Transformer as the encoder of SVFAP , which not only largely
reduces computational costs but also achieves excellent performance. To verify the effectiveness of our method, we conduct
experiments on nine datasets spanning three downstream tasks, including dynamic facial expression recognition, dimensional emotion
recognition, and personality recognition. Comprehensive results demonstrate that SVFAP can learn powerful affect-related
representations via large-scale self-supervised pre-training and it significantly outperforms previous state-of-the-art methods on all
datasets. Code is available at https://github.com/sunlicai/SVFAP.
Index Terms —Video-based facial affect analysis, self-supervised learning, masked autoencoding,Transformer, spatial bottleneck,
temporal pyramid
✦
1 I NTRODUCTION
VIDEO -BASED facial affect analysis, which aims to auto-
matically detect and understand human affective states
from facial videos, has recently gained considerable atten-
tion due to its great potential in developing natural and har-
monious human-computer interaction systems [1], [2], [3],
[4]. Early attempts for this task focus on designing advanced
handcrafted features and machine learning algorithms on
small lab-controlled datasets. With the advent of deep learn-
ing and larger labeled datasets, the research paradigm has
changed to train supervised deep neural networks in an
end-to-end manner. Researchers have developed a variety of
deep architectures to improve model performance, includ-
ing convolutional neural networks (CNN) [5], [6], recurrent
neural networks (RNN) [7], [8], Transformers [9], [10], and
their combinations [6], [11], [12], [13], [14].
Although tremendous progress has been achieved by
supervised learning, there are still two major obstacles that
•Licai Sun, Kexin Wang, Yu He, Mingyu Xu, Haiyang Sun, and Biu
Liu are with the School of Artificial Intelligence, University of Chinese
Academy of Sciences, Beijing, China, 100049, and the National Laboratory
of Pattern Recognition, Institute of Automation, Chinese Academy of
Sciences, Beijing, China, 100190.
E-mail: sunlicai2019@ia.ac.cn, liubin@nlpr.ia.ac.cn
•Zheng Lian is with the National Laboratory of Pattern Recognition,
Institute of Automation, Chinese Academy of Sciences, Beijing, China,
100190.
E-mail: lianzheng2016@ia.ac.cn
•Jianhua Tao is with the Department of Automation, Tsinghua University,
and Beijing National Research Center for Information Science and Tech-
nology, Tsinghua University, Beijing, China, 100084.
E-mail: jhtao@tsinghua.edu.cn
Manuscript received February 8, 2023, revised April 24, 2024. (Corresponding
authors: Zheng Lian, Bin Liu and Jianhua Tao)impede its further development: 1) Supervised learning
methods are prone to overfitting due to limited training data
and the existence of label noise in current datasets [15], [16],
thus having poor generalization ability on the unseen test
set. 2) Collecting large-scale and high-quality labeled data
is extremely time-consuming and labor-intensive because
of the sparsity and imbalanced distribution of emotional
moments in videos [15], and also the subjectivity and am-
biguity in human emotion perception [17], [18]. These two
irreconcilable factors make video-based facial affect analysis
still far from real-world applications.
As an alternative to supervised learning, self-supervised
learning has drawn massive attention recently due to its
strong generalization ability and data efficiency [19]. Specif-
ically, in contrast to supervised learning, self-supervised
learning leverages input data itself as the supervisory signal
for self-supervised pre-training and thus can learn powerful
representations from large-scale data without using any
human-annotated labels [19], [20]. Self-supervised learning,
especially generative self-supervised learning, has shown
unprecedented success in lots of deep learning fields [21],
including natural language processing and computer vi-
sion. For instance, BERT [22] introduces masked language
modeling as the pre-training objective for language repre-
sentation learning and achieves state-of-the-art results on
over ten natural language processing tasks. Similarly, MAE
[23] utilizes masked image autoencoding to perform self-
supervised visual pre-training and outperforms its super-
vised counterpart in many downstream vision tasks.
Despite its great success in many deep learning fields,
self-supervised learning has rarely been explored in video-arXiv:2401.00416v2  [cs.CV]  1 Oct 2024JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2
based affective computing. To this end, this paper presents
a self-supervised learning method, named Self-supervised
Video Facial Affect Perceiver (SVFAP), to unleash the power
of large-scale self-supervised learning for video-based facial
affect analysis. As shown in Fig. 1, SVFAP involves two-
stage training, i.e., self-supervised pre-training and down-
stream fine-tuning. Its whole pipeline is conceptually simple
and generally inherits those of MAE [23] and its video
versions [24], [25] considering their big success in computer
vision. Concretely, during the pre-training stage, SVFAP uti-
lizes masked facial video autoencoding to learn useful and
transferable spatiotemporal representations from a large
amount of unlabeled facial video data. The architecture
adopts an asymmetric encoder-decoder design [23] to enable
efficient pre-training, in which a high-capacity encoder only
processes limited visible input (since the masking ratio is
very high) and a lightweight decoder operates all input and
reconstructs the masked part. During fine-tuning, it discards
the decoder and fine-tunes the pre-trained high-capacity en-
coder on downstream datasets. Note that, since the vanilla
Vision Transformer (ViT) [26] is typically employed as the
encoder in MAE and its video versions, the computational
costs are very expensive during downstream fine-tuning
(especially for videos) despite the architecture efficiency
in pre-training. Considering that large redundancy (e.g.,
facial symmetry and temporal correlation) exists in 3D facial
video data, we thus propose a novel Temporal Pyramid
and Spatial Bottleneck Transformer (TPSBT) as the high-
capacity encoder of SVFAP to achieve both efficient pre-
training and fine-tuning. As the name suggests, TPSBT
(Fig. 2) utilizes spatial bottleneck mechanism and temporal
pyramid learning to minimize redundant information in
spatial and temporal dimensions, which not only reduces
the computational costs greatly (about 43% FLOPs reduction
during fine-tuning) but also leads to superior performance.
To verify the effectiveness of SVFAP , we conduct ex-
periments on nine datasets from three video-based facial
affect analysis tasks, including six datasets for dynamic
facial expression recognition, two datasets for dimensional
emotion recognition, and one dataset for personality recog-
nition. Comprehensive experimental results demonstrate
that our SVFAP can learn powerful affect-related represen-
tations from large-scale unlabeled facial video data via self-
supervised pre-training and significantly outperforms pre-
vious state-of-the-art methods on all downstream datasets.
For instance, on three in-the-wild dynamic facial expression
recognition datasets, our best model surpasses the previous
best by 5.72% UAR and 5.02% WAR on DFEW [6], 4.38%
UAR and 3.75% WAR on FERV39k [27], and 7.91% UAR
and 6.10% WAR on MAFW [28]. To sum up, the main
contributions of this paper are three-fold:
•We introduce a self-supervised learning approach,
SVFAP , to address the dilemma faced by supervised
learning methods in video-based facial affect anal-
ysis. It leverages masked facial video autoencoding
as the pre-training objective and can learn powerful
affect-related representations from large-scale unla-
beled facial video data.
•We propose a novel TPSBT model as the encoder
of SVFAP to eliminate large spatiotemporal redun-dancy in facial videos, which not only enjoys lower
computational costs but also has superior perfor-
mance when compared with the vanilla ViT.
•Comprehensive experiments on nine downstream
datasets demonstrate that our SVFAP achieves state-
of-the-art performance in three popular video-based
facial affect analysis tasks.
2 R ELATED WORK
2.1 Video-based Facial Affect Analysis
Most studies on video-based facial affect analysis fall into
the supervised learning paradigm. They mainly concen-
trate on developing more advanced deep learning archi-
tectures to extract discriminative spatiotemporal represen-
tations from raw facial videos. Generally, there are two lines
of research. The first line of research treats the spatial and
temporal dimension of 3D video data in an independent
manner [6], [11], [12], [27], [28], [29]. Typically, 2D CNN
(e.g., VGGNet [30] and ResNet [31]) is first used to extract
spatial features from each static frame and then RNN (e.g.,
LSTM [32] and GRU [33]) runs over them to integrate the
dynamic temporal information across all frames. Recently,
inspired by the great success of Transformer [34] in natural
language processing and computer vision, there are also
several studies that utilize the global dependency modeling
ability of Transformer to enhance spatial and temporal fea-
ture extraction of traditional CNN and RNN [9], [10], [13],
[14], [35]. Another line of research tries to simultaneously
encode spatial appearance features and dynamic motion in-
formation by extending the 2D convolution kernel to the 3D
convolution kernel along the temporal axis. With the help of
3D kernels, 3D CNN (e.g., C3D [36], R(2+1)D [37], P3D [38],
and 3D ResNet [39]) is expected to extract discriminative
spatiotemporal representations from raw videos.
Although the above supervised deep learning methods
have achieved remarkable improvement over traditional
machine learning methods, they still suffer from the no-
torious overfitting issue due to limited training data and
label noise in current datasets [15], [16]. In contrast to these
supervised methods, we propose a self-supervised learning
method in this study to address their dilemma by pre-
training on abundantly available unlabeled facial videos.
2.2 Self-supervised Learning
Self-supervised learning aims to solve the data-hungry issue
in supervised learning by exploiting massive unlabeled
data. It can be roughly divided into two categories: dis-
criminative and generative [21]. The discriminative method
generally follows the supervised counterpart by designing
a discriminative loss. Early studies focused on developing
various geometry-based pretext tasks, such as predicting
image rotation [40] and sorting shuffled video frames [41].
In recent years, the trend has shifted from handcrafted
tasks to contrastive learning methods, e.g., MoCo [42], and
SwAV [43]. Contrastive learning has been the dominant self-
supervised pre-training framework in computer vision until
the more recent reviving success of generative learning.
The generative method typically involves an autoencoding
process, in which an encoder maps the input into a latentJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3
representation and a decoder reconstructs the original input
from the encoded representation. Early generative work can
be dated back to denoising autoencoder [44], i.e., recon-
structing the clean input from a partially corrupted one.
Recently, motivated by the unprecedented success in natural
language processing (e.g., BERT [22] and GPT [45]), genera-
tive methods have also achieved impressive results in com-
puter vision [23], [46]. One of the most representative meth-
ods is masked autoencoder (MAE) [23]. MAE and its video
versions [24], [25] utilize an asymmetric encoder-decoder ar-
chitecture to efficiently pre-train vanilla ViT and outperform
its supervised counterpart and state-of-the-art contrastive
learning methods by large margins in many downstream
tasks (e.g., object recognition/detection/segmentation, and
action recognition/detection).
Despite the great success of self-supervised learning in
many deep learning fields, it has rarely been explored in
video-based facial affect analysis. Roy et al. [47] present a
spatiotemporal contrastive learning method pre-trained on
a small lab-controlled labeled dataset. Unlike it, the pro-
posed SVFAP is built upon more advanced generative meth-
ods (i.e., MAE and its video versions) and leverages large-
scale in-the-wild unlabeled facial videos for self-supervised
pre-training. There are also several self-supervised stud-
ies in the relevant research field (i.e., facial action unit
detection). For instance, FAb-Net [48] exploits relative fa-
cial movements between adjacent frames as free supervi-
sory signals to perform self-supervised pre-training. TCAE
[49] improves FAb-Net by disentangling the head pose-
related movements and facial action-related ones. FaceCycle
[50] further introduces facial motion and identity cycle-
consistency to promote facial representation learning. Lu et
al. [51] propose a triplet-based frame ranking method for
temporal consistency modeling. Recently, there are also a
few studies focusing on general self-supervised facial repre-
sentation learning. Bulat et al. [52] pre-train SwAV [43] on
large-scale face recognition datasets and find that it achieves
significant improvements over previous supervised meth-
ods on five face analysis tasks. FaRL [53] utilizes both low-
level masked facial image modeling and high-level face-
text contrastive learning for self-supervised pre-training and
outperforms state-of-the-art methods on many face analysis
tasks. Although achieving promising results, these methods
use 2D models for self-supervised pre-training, thus could
not capture rich spatiotemporal information in 3D facial
videos.
2.3 Vision Transformer
Originated from natural language processing, Transformer
[34] based architectures have recently revolutionized var-
ious computer vision tasks by means of its strong long-
range dependency modeling ability [54]. Among them, the
pioneering work of ViT [26] directly applies the standard
Transformer to a sequence of image patches and performs
very well on image classification tasks, challenging the
dominant paradigm of CNN in computer vision. Since ViT
relies on large amounts of labeled data (i.e., JFT-300M)
to achieve successful supervised pre-training, DeiT [55]
introduces several training strategies to allow it can be
trained on a much smaller dataset (i.e., ImageNet-1K). Afterthat, numerous ViT variants have emerged by incorporating
more or less vision-friendly priors (e.g., local self-attention
and hierarchical design) to reduce the quadratic scaling
cost of vanilla ViT and improve model performance in
vision tasks [56], [57], [58], [59], [60]. For instance, in the
video domain, TimeSformer [57] applies factorized temporal
and spatial attention in the standard Transformer block to
achieve spatiotemporal representation learning. MViT [58]
introduces multi-scale feature hierarchies to the vanilla ViT.
Video Swin Transformer [60] utilizes 3D-shifted window
attention to inject spatiotemporal locality inductive bias
into video Transformers. Although these variants usually
perform better than vanilla ViT in the supervised setting,
they are typically not suitable for masked autoencoding in
self-supervised pre-training. This is because they could not
drop masked tokens due to the incorporation of local oper-
ations (e.g., window-based attention and patch merging) in
their architectures. Thus, they can not enjoy the efficiency
of vanilla ViT during masked autoencoding. This is why
MAE and its video versions all employ ViT as the encoder.
Nevertheless, ViT still suffers from expensive computational
costs during downstream fine-tuning. To this end, in this
paper, we introduce temporal pyramid learning and spatial
bottleneck mechanism to ViT to enable both efficient pre-
training and fine-tuning.
3 M ETHOD
To tackle the longstanding data scarcity issue in video-
based affective computing, we propose Self-supervised
Video Facial Affect Perceiver (SVFAP) in this paper, with
its whole pipeline illustrated in Fig. 1. Specifically, it first
utilizes masked facial video autoencoding to perform self-
supervised pre-training on massive unlabeled facial video
data, then discards the decoder and fine-tunes the pre-
trained encoder in downstream video-based facial affect
analysis tasks. In the following sections, we elaborate on
both pre-training and fine-tuning details.
3.1 Self-supervised Pre-training
The idea of masked facial video autoencoding is simple,
i.e., reconstructing the original facial video given partially
observed input. Concretely, SVFAP consists of an encoder
to project the original video to a high-level spatiotemporal
representation in the latent space, and then a decoder to
reconstruct the original input from the encoded representa-
tion. Moreover, similar to MAE [23] and its video versions
[24], [25], SVFAP adopts an asymmetric encoder-decoder
architecture, where a high-capacity encoder only accepts
limited visible tokens (e.g., 10%) as input and a lightweight
decoder operates on all tokens to perform the video recon-
struction. With this special design, the computational costs
could be largely reduced to allow fast pre-training.
3.1.1 Patch Embedding
We employ a Transformer-based encoder to encode raw
facial videos. Considering that Transformer accepts a se-
quence of tokens as input, we thus split the 3D video clip
Vwith a spacetime shape of T×H×W×3(Tis the
number of frames, HandWare frame height and widthJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4
(a) Pre
-
training
Patch 
Embedding
Tube
Masking
Encoder
Decoder
Time
…
Input 
Video 
C
lip (Unlabeled)
Add 
Mask 
T
okens
Visible Tokens
Reconstruct
(b) Fine
-
tuning
Patch 
Embedding
Encoder
Time
…
Input Video Clip 
(Labeled
)
Pooling
FC



: Positional Embedding
Fig. 1. An overview of the proposed method (i.e., SVFAP). It consists of two stages, including self-supervised pre-training (a) and downstream
fine-tuning (b). During pre-training, SVFAP utilizes masked facial video autoencoding as the training objective. Following previous studies [23], [24],
[25], it adopts an asymmetric encoder-decoder architecture and a high masking ratio (e.g., 90%) to enable fast pre-training on large-scale unlabeled
facial video data. After pre-training, the lightweight decoder is discarded and the high-capacity encoder is fine-tuned in downstream tasks.
respectively) into a regular grid of non-overlapping spa-
tiotemporal patches (Fig. 1 (a)), then flatten and embed them
to tokens by linear projection. Following VideoMAE [24],
the patch size is set to 2×16×16. Therefore, the raw video
clip is transformed to token embeddings with the shape of
T
2×H
16×W
16×C(Cis the embedding dimension) after
patch embedding. Finally, to inject the position information
into the sequence, we add sinusoidal positional embeddings
[34] to the embedded tokens.
3.1.2 Tube Masking
In order to make video reconstruction a challenging and
meaningful pre-training task, we need to mask some embed-
ded patches. Different modalities usually require different
masking ratios. For the high-level and information-dense
text modality, BERT [22] chooses to randomly mask 15% of
tokens in the sequence. For the low-level and information-
redundant image modality, MAE advocates a masking ratio
of 75%. For video data, it has one more temporal dimension
compared with static images and has high spatiotemporal
redundancy. The empirical findings in previous studies sug-
gest that 90% works best for video data [24], [25]. Following
them, we also adopt a masking ratio of ρ= 90% . With such
a high masking ratio, most masked tokens can be dropped
and only a small subset (i.e., 10%) of visible tokens will be
processed by the encoder, leading to large computational
cost reduction and efficient pre-training.
Moreover, considering that consecutive frames in videos
are highly similar, spacetime-agnostic random masking
might incur information leakage and make masked video
reconstruction too easy. The reason is that the model could
easily reconstruct the masked patches by simply finding
similar visible ones in adjacent frames. Thus, the shortcutsignal will encourage the model to capture low-level tem-
poral correspondence and undermine the desired high-level
spatiotemporal structure learning. To address this issue, we
follow VideoMAE to employ a simple masking strategy,
termed tube masking or space-only masking, i.e., first mask-
ing spatial patches in one temporal slice and then sharing it
along the temporal axis (Fig. 1 (a)).
3.1.3 Encoder
Typically, the vanilla ViT [26] is employed as the encoder
in masked autoencoding [23], [24], [25]. Although it enjoys
much efficiency during pre-training by discarding many
masked tokens, its computational costs are still very ex-
pensive during downstream fine-tuning as it must take as
input all embedded tokens. Meanwhile, we notice that large
redundancy exists in 3D facial videos, such as facial symme-
try and temporal correlation. Motivated by this, we propose
the Temporal Pyramid and Spatial Bottleneck Transformer
(TPSBT) as the encoder of our SVFAP to enable both fast
pre-training and fine-tuning. As shown in Fig. 2, our TPSBT
has three stages. The first stage is composed of several
standard Transformer blocks, while in the last two stages,
they are replaced by spatial bottleneck Transformer blocks to
reduce spatial redundancy. Moreover, benefiting from tube
masking, we could further utilize temporal pyramid learn-
ing to reduce redundancy in the temporal dimension. For
convenience, in the following part, we denote as X∈RN×C
the reshaped input token embeddings after tube masking,
where N=T
2·Sis the total number of visible tokens and
S=H
16·W
16·(1−ρ) denotes the spatial token number. And
Xi∈RNi×Crepresents the output of stage i(Niis the
number of tokens in each stage, i∈ {1,2,3}).JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5
Transformer
Block
Stage 1
Spatial 
Bottleneck 
Transformer
Block
Stage 2
Spatial 
Bottleneck 
Transformer
Block
Stage 3
T
↓
T
↑
Fine
-
tuning
Pre
-
training
T
↑
T
↓
MHSA
Norm
Norm
FFN
Transformer Block
Spatial 
Attention
Norm
MHCA
MHSA
Norm
Norm
FFN
Norm
MHCA
MHSA
Norm
Norm
FFN
Spatial Bottleneck 
Transformer Blocks
T
↓
: Temporal Downsample
T
↑
: 
Temporal 
Upsample
MHSA: Multi
-
Head 
Self
-
Attention 
MHCA
: Multi
-
Head 
Cross
-
Attention 
FFN: Feed
-
Forward 
Network
Norm: Layer 
Norm
R
: Reshape
: Matrix Addition
: Matrix Multiplication
Input Tokens
Notation
or
R
FFN
Spatial 
Attention
R
R
Fig. 2. The encoder in SVFAP , i.e., TPSBT. We perform summation-based multi-scale fusion for features from three stages during pre-training
and empirically do not use it in fine-tuning. The temporal length in each stage Ti=T
2ki−1(kis the downsampling rate and empirically k= 2,
i∈ {1,2,3}). The spatial size S=H
16·W
16·(1−ρ)(ρis the masking ratio and ρ= 90% ) during pre-training and S=H
16·W
16during fine-tuning.
Standard Transformer . The standard Transformer in
stage 1 is used to retain the global spatiotemporal represen-
tation learning ability of vanilla ViT. It consists of a sequence
ofM1Transformer blocks, each of which is composed of
alternating layers of Multi-Head Self-Attention (MHSA) and
Feed-Forward Network (FFN), with Layer Normalization
(LN) applied before each layer and residual connections
after each layer. Formally, we define a standard Transformer
block as follows:
Y(j−1)
1 =MHSA (LN(X(j−1)
1 )) +X(j−1)
1
X(j)
1=FFN(LN(Y(j−1)
1 )) +Y(j−1)
1(1)
where the superscript j∈ {1, ..., M 1}is the block index,
X(0)
1=X, and the output of stage 1 is X1=X(M1)
1 .
The MHSA operation in Eq. (1) employs multi-head
scaled dot-product attention [34] to explore long-range spa-
tiotemporal dependencies in input tokens, i.e.,
MHSA (X) =Concat (head 1, ...,head H)WO
head h=Softmax (QhK⊤
h√dh)Vh(2)
where Qh=XWQ
his the query, Kh=XWK
his the key,
Vh=XWV
his the value, W∗
h∈RC×dh(∗ ∈ { Q, K, V }),
WO∈RC×C,His the number of heads, dh=C/H is the
feature dimension of head h, and the superscript⊤is matrix
transpose. Finally, FFN in Eq. (1) consists of two linear layers
with a GELU [61] non-linearity in between, i.e.,
FFN(X) =GELU (XW 1+b1)W2+b2 (3)
where W1∈RC×4C,b1∈R4C,W2∈R4C×C,b2∈RC.Temporal Pyramid . As illustrated in Fig. 2, a temporal
downsampling module is inserted before spatial bottleneck
Transformer blocks in the last two stages to achieve tempo-
ral redundancy reduction. We utilize a strided convolution
layer to implement this module:
T↓(Xi−1) =Conv (Xi−1, k)∈RNi×C(4)
where i∈ {2,3}is the stage index, Ni=Ti·S,Ti=T
2ki−1
is the temporal length in stage i, and kis the kernel
size and stride of the convolution layer. Empirically, kis
set to 2, which means that the temporal length is halved
after the temporal downsampling module. Alternatives to
convolution-based downsampling could be more simple
average pooling or max pooling, however, we observed
slight performance degradation in experiments.
Spatial Bottleneck Transformer . After temporal infor-
mation aggregation, we further employ the Spatial Bottle-
neck Transformer (SBT) in stage 2 and stage 3 to reduce
spatial redundancy. The main idea of SBT is to introduce a
small number of bottleneck tokens via spatial attention and
employ them instead of the original redundant embedded
tokens to perform further spatiotemporal interactions.
As shown in the bottom right of Fig. 2, SBT is composed
of a spatial attention module, Mi−1(i∈ {2,3}) SBT blocks,
and 1 reverse SBT block. To be specific, the spatial attention
module is used to compress the original fine-grained and
redundant spatial tokens into a few global semantic bottle-
neck tokens. For convenience, we reuse Xi−1∈RTi·S×C
(i∈ {2,3}) as the notation of the output of temporal
downsampling module in Eq. (4). Then, the spatial attentionJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6
is formulated as follows:
ˆXi−1=Reshape (Xi−1)∈RTi×S×C
Yi=GELU (ˆXi−1W1+b1)W2+b2∈RTi×S×G
˜Xi−1=Reshape (Xi−1)∈RTi×C×S
Zi=˜Xi−1Yi∈RTi×C×G
Bi=Reshape (Zi)∈RTi·G×C(5)
where W1,b1,W2, andb2have similar feature dimensions
to those in Eq. (3). After spatial attention, the redundant S
spatial tokens of each temporal slice in the original input
Xi−1are summarized into Gbottleneck tokens in Bi.
Subsequently, we utilize Mi−1SBT blocks to operate
on the summarized bottleneck tokens to achieve efficient
global spatiotemporal interactions. As shown in Fig. 2, an
SBT block mainly consists of a Multi-Head Cross-Attention
(MHCA) layer [62], an MHSA layer, and an FFN layer.
Formally, it can be defined as follows:
Y(j−1)
i =MHCA (LN(B(j−1)
i ),LN(Xi−1)) +B(j−1)
i
Z(j−1)
i =MHSA (LN(Y(j−1)
i )) +Y(j−1)
i
B(j)
i=FFN(LN(Z(j−1)
i )) +Z(j−1)
i(6)
where the superscript j∈ {1, ..., M i−1}is the block index,
B(0)
i=Bi. The MHCA layer iteratively distills necessary
information from the original input back into the bottleneck
embeddings to avoid key information being lost during
spatial compression. It is a variant of MHSA, whose query
is the bottleneck embeddings while the key and value come
from the original input to allow information flow from the
latter to the former, i.e.,
MHCA (X,Y) =Concat (head 1, ...,head H)WO
head h=Softmax (QX
hKY
h⊤
√dh)VY
h(7)
where QX
h=XWQ
h,KY
h=YWK
h,VY
h=YWV
h, other
notations are similar to those in Eq. (2).
Compared with a standard Transformer block, the com-
putational complexity of an SBT block is reduced from
the quadratic O(S2T2
i)toO(G(G+S)T2
i)thanks to the
introduction of spatial bottleneck tokens. Considering that
Gis a small constant, an SBT block thus enjoys linear
computational complexity with respect to the spatial size S.
Note that, empirically, we have G≈Sduring pre-training
andG≪Sduring fine-tuning (Sec. 4.1). Therefore, the
computational costs of pre-training remain approximately
unchanged (actually, slightly decrease) and large computa-
tions can be reduced during downstream fine-tuning, allow-
ing both efficient pre-training and fine-tuning (Sec. 4.2.2).
Ultimately, a reverse SBT block is employed to recover
the spatial resolution of the original input for the final
reconstruction. As shown in Fig .2, the query and key/value
of this reverse block are just the opposite of the normal
block, i.e.,
Yi=MHCA (LN(Xi−1),LN(B(Mi−1)
i )) +Xi−1
Zi=MHSA (LN(Yi)) +Yi
Xi=FFN(LN(Zi)) +Zi(8)
where Xi∈RNi×Cis the final output of stage i(i∈ {2,3}).3.1.4 Decoder
The decoder is only utilized during pre-training to recon-
struct the original input facial video. Therefore, it is flexible
to build the decoder architecture in a way that is indepen-
dent of the encoder design. For simplicity, we follow pre-
vious work [23], [24] to employ the standard Transformer.
Moreover, since it has demonstrated that the decoder can be
lightweight, the number of Transformer blocks is set to 4,
which is much smaller than that of the encoder (Sec. 4.1).
As illustrated in Fig. 1 (a), the input to the decoder is
the combination of visible and masked tokens. It should
be noted that, due to the existence of temporal downsam-
pling modules in the encoder, it is necessary to recover
the temporal resolution of visible tokens. Hence, we apply
corresponding temporal upsampling to the outputs of the
last two stages. To avoid increasing model parameters, we
simply use nearest-neighbor interpolation for upsampling:
T↑(Xi) =Interp (Xi, ki−1)∈RN×C(9)
where Xiis the output of stage i(i∈ {2,3}) in the
encoder, ki−1is the upscaling factor. Moreover, to enable the
decoder to be aware of spatiotemporal features at different
levels, summation-based multi-scale fusion is employed to
aggregate the outputs of three stages in the encoder, i.e.,
X=X1+T↑(X2) +T↑(X3) (10)
where X∈RN×Cdenotes the encoded visible tokens. After
multi-scale fusion, we first concatenate Xand the trainable
masked tokens, then add sinusoidal positional embeddings
to them, and finally pass them through the Transformer-
based decoder for video reconstruction. Finally, the mean
squared error between the original video Vand the re-
constructed video ˆVin the pixel space is calculated as the
reconstruction loss:
L=1
|M|X
m∈M||V(m)−ˆV(m)||2(11)
where Mis the set of masked positions.
3.2 Downstream Fine-tuning
After self-supervised pre-training, we then discard the
lightweight decoder and only use the pre-trained high-
capacity encoder for downstream fine-tuning (Fig. 1 (b)).
The main difference in the encoder behavior between fine-
tuning and pre-training is that we do not perform multi-
scale fusion to aggregate the outputs of three stages (i.e.,
only use the high-level spatiotemporal representations in
the last stage, as shown in Fig. 2), as we observe worse
results in our experiments.
Based upon the pre-trained encoder, we apply global
average pooling to the extracted spatiotemporal feature and
append a fully connected network for final prediction. We
use different loss functions for different types of video-based
facial affect analysis tasks. For the classification task, the
cross-entropy loss is employed, i.e.,
Lcls=−KX
k=1yklog ˆyk (12)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7
160x160
224x224
160x160
160x160
Fig. 3. The illustration of the face patch location for VoxCeleb2 videos.
where ˆy∈RKdenotes the prediction, y∈RKis the
target, and Kis the number of emotion classes. For the
regression task, we compute the mean square error between
the prediction and the target:
Lreg=1
K||y−ˆy||2(13)
where Kis the number of emotion dimensions.
4 E XPERIMENTS
4.1 Implementation Details
TPSBT architecture. To meet different needs in real-world
applications, we build two versions (i.e., base and small)
of TPSBT as the encoder of SVFAP . For the small version
TPSBT-S, C= 384 ,M1= 8 ,M2= 4 ,M3= 2 . For the
base version TPSBT-B, C= 512 ,M1= 12 ,M2= 6,M3= 3.
The model size and computational costs of the small version
are approximately half of the base version. Besides, for both
versions, the spatial bottleneck token number Gis set to 8.
Self-supervised pre-training. We pre-train TPSBT on
a large-scale audio-visual dataset of human speech, Vox-
Celeb2 [63]. It includes more than 1 million video clips
for over 6K celebrities extracted from about 150K videos
uploaded to YouTube. We use its development set for self-
supervised pre-training, which has 1,092,009 video clips
from 145,569 videos. As shown in Fig. 3, the resolution of
the original videos in VoxCeleb2 is 224×224, where faces
typically appear in the upper-central part of the video. The
remaining parts display the shoulders and neck, along with
extraneous background information. Therefore, we only use
a 160x160 patch from the upper-central location of the video,
allowing us to remove irrelevant information while also re-
ducing the model’s input size, thus lowering computational
costs. We sample 16 frames from each video clip using a
temporal stride of 4, resulting in 8×10×10input tokens after
patch embedding when using the patch size of 2×16×16.
We conduct experiments using the PyTorch framework
with 4 Nvidia GeForce RTX 3090 GPUs. For the hyperpa-
rameter setting, we mainly follow VideoMAE [24]. The main
differences include the learning rate, batch size, and training
epochs. Specifically, we adopt an AdamW optimizer with
β1= 0.9andβ2= 0.95. The base learning rate is 3e−4
and the weight decay is 0.05. The overall batch size is 256.
We linearly scale the base learning rate with respect to the
overall batch size, i.e., lr =base learning rate ×batch size
256.
Besides, we employ a cosine decay learning rate scheduler.
By default, we pre-train the model for 100 epochs with 5
warmup epochs and it takes about 5-6 days in our setting.
Downstream fine-tuning. The input video clip size is
also 16×160×160 and the temporal stride is 4 in mostcases. We adopt an AdamW optimizer with β1= 0.9and
β2= 0.999. The base learning rate is 1e−3and the overall
batch size is 96. Other hyperparameters are the basically
same as those in pre-training and can also refer to [24] for
more details. We fine-tune the pre-trained model for 100
epochs with 5 warmup epochs. For inference, we uniformly
sample two clips along the temporal axis for each video
sample and then compute the average score as the final
prediction.
4.2 Dynamic Facial Expression Recognition
4.2.1 Datasets
We conduct experiments on 6 dynamic facial expression
recognition datasets, including 3 large-scale in-the-wild
datasets (i.e., DFEW [6], MAFW [28], and FERV39k [27])
and 3 small lab-controlled datasets (i.e., CREMA-D [64],
RAVDESS [65], and eNTERFACE05 [66]). We briefly intro-
duce each of them below.
DFEW consists of 16,372 video clips which are extracted
from more than 1,500 high-definition movies. Each video
clip is annotated by 10 well-trained annotators with 7 basic
emotions (i.e., anger, disgust, fear, happy, sad, surprise, and
neutral). To align with previous studies [6], [13], we evaluate
the proposed method on 11,697 single-labeled clips using
the default 5-fold cross-validation protocol.
MAFW is a multimodal compound affective dataset in
the wild. It is composed of 10,045 video clips annotated
with 11 compound emotions, including contempt, anxiety,
helplessness, disappointment, and 7 basic emotions. In this
paper, we only consider the video modality and conduct
experiments on 9,172 single-labeled video clips. For evalu-
ation, we follow the original paper [28] to adopt the 5-fold
cross-validation protocol.
FERV39k is currently the largest real-world dynamic fa-
cial expression recognition dataset. It includes 38,935 video
clips which belong to 22 representative scenes in 4 different
scenarios. Each sample is annotated by 30 professional an-
notators with 7 basic emotions. The whole dataset has been
officially split into 80% for training and the rest 20% for test.
CREMA-D is a high-quality audio-visual dataset for
multimodal expression and perception of acted emotions.
It consists of 7,442 video clips from 91 actors. Each video
clip is labeled with 6 emotions, including happy, sad, anger,
fear, disgust, and neutral. Since there is no official split for
this dataset, we employ a 5-fold subject-independent cross-
validation protocol.
RA VDESS is an audio-visual dataset of emotional
speech and song. It consists of 2,880 video clips from 24
professional actors, each of which is labeled with 8 emotions
(i.e., 7 basic emotions and calm). In this paper, we only use
the speech part with 1,440 video clips. This dataset has no
official split, we thus follow [67], [68] to adopt the same 6-
fold subject-independent cross-validation protocol.
eNTERFACE05 is also an audio-visual emotion dataset
that contains about 1,200 video clips from more than 40
subjects. Each subject is asked to simulate six emotions,
including anger, disgust, fear, happy, sad, and surprise. To
make a fair comparison with previous work [9], we employ
a 5-fold subject-independent cross-validation protocol.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8
UAR WAR020406080100Scores (%)
3.2122.46 20.4529.2262.6374.81DFEW
Chance level
From scratch
SVFAP
UAR WAR020406080100Scores (%)
3.5624.9521.3731.9942.1452.29FERV39k
Chance level
From scratch
SVFAP
Fig. 4. Ablation study of training from scratch.
Following previous work [6], [13], [27], [28], we use the
weighted average recall (WAR, i.e., the accuracy) and un-
weighted average recall (UAR, i.e., the mean class accuracy)
as evaluation metrics for all datasets. Note that, for cross-
validation, we aggregate the predictions and labels from all
splits and then report the overall UAR and WAR.
4.2.2 Ablation Studies
In this section, we conduct in-depth ablation experiments
to investigate the impacts of several key factors in our pro-
posed SVFAP . By default, we use TPSBT-B as the encoder.
During downstream fine-tuning, all models share the same
evaluation protocol. In addition to fine-tuning performance,
we also show the number of model parameters and com-
putational overhead measured in Floating Point Operations
(FLOPs)1for efficiency comparison. Note that we denote
pre-training FLOPs as FLOPs-P . Unless otherwise stated, all
experiments are conducted on split 1 of DFEW.
Training from scratch. We first show the superiority of
the proposed self-supervised pre-training method SVFAP
by comparing it with training from scratch. Fig. 4 presents
the comparison results on DFEW and FERV39k. From the
figure, we first observe that it is hard to achieve good results
when training TPSBT-B from scratch. This observation is
consistent with previous findings (i.e., vision Transformers
are data-hungry) in computer vision [26], [69], and can be
largely attributed to the lack of inductive bias and the small
size of training datasets. Moreover, we find that our SVFAP
significantly outperforms training from scratch, achieving
about 42% UAR and 45% WAR improvements on DFEW,
and 21% UAR and 20% WAR improvements on FERV39k.
These encouraging results demonstrate that SVFAP pro-
vides an effective self-supervised pre-training mechanism
for video-based facial affect analysis. We also notice that the
performance gap between SVFAP and training from scratch
on FERV39k is smaller than that on DFEW, probably due to
the larger dataset size of the former (39K vs. 13K).
Training schedule. We then explore the effect of training
schedule length. As shown in Fig. 5, we find that as the
pre-training process goes on, the pre-training loss decreases
1. https://github.com/facebookresearch/fvcore
20 40 60 80 100
epochs0.360.400.440.48Pre-training loss
20 40 60 80 100
epochs56596265UAR
57.5558.1559.9461.462.63
20 40 60 80 100
epochs70727476WAR
70.5771.6873.7374.2474.81Fig. 5. Ablation study of training schedule.
steadily and the downstream fine-tuning performance im-
proves consistently. This finding is in line with previous
works on self-supervised learning [23], [24], [25]. It also
should be noted that we do not observe clear performance
saturation when reaching default maximum epochs (i.e.,
100), which indicates that longer pre-training could further
improve model performance in downstream fine-tuning.
However, due to limited computational resources, we leave
it for future work and hope the community can conduct
follow-up studies.
Moreover, we visualize several reconstructed video sam-
ples using our best pre-trained model in Fig. 6. Note that
these samples are randomly selected from the test set of
VoxCeleb2, whose speakers are disjoint with those from the
development set used for pre-training. Thus, they are not
seen by our model during pre-training. We find that even
under such a high masking ratio (i.e., 90%), SVFAP still
can generate satisfying reconstruction results, especially for
dynamic facial expressions. This indicates that, benefiting
from the challenging masked facial video autoencoding
task, our model can reason over high-level and meaning-
ful spatiotemporal semantics from limited visible input to
recover masked information.
Dataset scale. We investigate how the model behaves
when pre-trained with different dataset scales. For this pur-
pose, we randomly generate a range of subsets with increas-
ing sizes from the whole dataset, i.e., 1%, 5%, 10%, and 20%.
Note that we proportionally increase the pre-training epochs
for these subsets to ensure the same pre-training cost. The
results are shown in Table 1. We can observe that larger
pre-training datasets generally lead to better fine-tuning
results. This is expected as more diversified training samples
typically result in better generalization. It is also worth
noting that, even with 11K unlabeled data, our method still
achieves promising performance, which demonstrates that
our SVFAP is a data-efficient self-supervised video facial
affect learner.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9
Fig. 6. Reconstruction results of three randomly selected video samples from the test set of VoxCeleb2 with a masking ratio of 90%. For each
sample, we show the original video (top), masked input video (middle), and the reconstructed video (bottom).
TABLE 1
Ablation study of pre-training dataset scale.
Percentage Size Epochs UAR WAR
1% 11K 10000 57.65 70.61
5% 55K 2000 60.34 73.00
10% 110K 1000 61.09 73.56
20% 220K 500 61.54 73.76
100% 1.1M 100 62.63 74.81
TABLE 2
Ablation study of the masking ratio. FLOPs-P: Pre-training FLOPs.
Percentage#Params
(M)FLOPs
(G)FLOPs-P
(G)UAR WAR
75% 77.6 43.6 18.4 60.89 73.91
85% 77.6 43.6 14.7 61.87 74.77
90% 77.6 43.6 12.9 62.63 74.81
95% 77.6 43.6 11.2 59.93 73.17
Masking ratio. The influence of the masking ratio is
presented in Table 2. We can find that the masking ratio
of 90% has the best performance. The lower masking ratios
of 75% and 85% achieve worse performance, although the
encoder accepts more video tokens as input and has higher
computational costs during pre-training. A higher masking
ratio of 95% results in lower pre-training cost, however,
the performance degrades significantly. These results are
consistent with previous findings [24], [25]. Therefore, we
set 90% as the default masking ratio.
Model variants. We ablate different model variants ofTABLE 3
Ablation study of model variants. TP: Temporal Pyramid. SBT: Spatial
Bottleneck Transformer. FLOPs-P: Pre-training FLOPs.
TP SBT#Params
(M)FLOPs
(G)FLOPs-P
(G)UAR WAR
× × 76.4 76.9 14.9 61.71 74.41
✓ × 77.5 53.1 13.1 60.84 74.28
×✓ 76.6 49.9 13.5 62.66 75.02
✓ ✓ 77.6 43.6 12.9 62.63 74.81
TPSBT, including 1) no TP and SBT, i.e., the vanilla ViT
baseline, by removing temporal pyramid learning modules
and replacing SBT with a standard Transformer of simi-
lar size in the last two stages. 2) only TP , by replacing
SBT with a standard Transformer of similar size. 3) only
SBT, by removing temporal pyramid learning modules.
As presented in Table 3, we have the following observa-
tions: 1) When compared with the vanilla ViT baseline,
TP largely reduces computational overhead (about 30%
and 14% FLOPs reduction for fine-tuning and pre-training
respectively) and achieves comparable performance, with
only 1.1M additional parameters. 2) SBT not only reduces
large computations (about 35% FLOPs and 11% FLOPs-P
reduction) but also achieves the best performance among
all variants. 3) When combining TP and SBT together, our
default full model achieves the lowest computational costs
while almost maintaining the best performance of SBT.
Specifically, TPSBT significantly reduces about 43% and 15%
FLOPs during fine-tuning and pre-training and outperforms
the vanilla ViT baseline by 0.92% UAR and 0.40% WAR,JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10
TABLE 4
Ablation study of multi-scale fusion.
Pre-training Fine-tuning UAR WAR
× × 61.78 74.43
× ✓ 61.24 74.17
✓ × 62.63 74.81
✓ ✓ 62.39 74.74
TABLE 5
Ablation study of spatial bottleneck tokens. FLOPs-P: Pre-training
FLOPs.
Number#Params
(M)FLOPs
(G)FLOPs-P
(G)UAR WAR
4 77.6 43.2 12.6 61.76 74.08
8 77.6 43.6 12.9 62.63 74.81
16 77.6 44.4 13.7 62.86 74.69
with the sacrifice of a slight increase of model parameters
(1.2M). These results indicate that temporal pyramid learn-
ing and spatial bottleneck mechanism can greatly remove
redundant spatiotemporal information in 3D facial videos
(from temporal and spatial perspectives respectively) and
help the model to concentrate on the informative one, thus
contributing to lower computational costs and better model
performance.
Multi-scale fusion. We explore the effect of multi-scale
spatiotemporal feature fusion during both self-supervised
pre-training and downstream fine-tuning. As shown in Ta-
ble 4, we find that employing multi-scale fusion during pre-
training achieves better results than those that do not use it,
which shows that integrating spatiotemporal features in dif-
ferent levels for decoder reconstruction can help the encoder
to learn more useful representations. However, we observe
slight performance degradation when using it during fine-
tuning. This result could be partly ascribed to the simple and
parameter-free temporal upsampling method (i.e., nearest-
neighbor interpolation) used for multi-scale fusion. Another
reason is that, compared to low-level details, high-level
emotional semantics are more crucial for downstream affect
analysis tasks. Therefore, we only use the high-level feature
from the last stage of the encoder during fine-tuning.
Spatial bottleneck tokens. SBT utilizes spatial attention
to generate several global semantic bottleneck tokens for
spatial redundancy elimination and computational cost re-
duction, thus it is necessary to investigate how the number
of spatial bottleneck tokens influence model performance.
Table 5 presents the ablation results. We see that too few
tokens (i.e., 4) hurt the model performance as it might be
too aggressive to perform spatial compression and incur
critical information loss. Besides, too many tokens (i.e., 16)
increase model computational costs but do not contribute
to significantly better results. Therefore, we set the spatial
bottleneck token number to 8 by default.
Temporal downsampling. We explore the effect of three
different methods for temporal downsampling in temporal
pyramid learning, including simple average pooling, max
pooling, and the default strided convolution. As presented
in Tab. 6, we find that the convolution-based method out-
performs the other two pooling methods with the sacrificeTABLE 6
Ablation study of temporal downsampling. FLOPs-P: Pre-training
FLOPs.
Type#Params
(M)FLOPs
(G)FLOPs-P
(G)UAR WAR
Avg 76.6 43.3 12.9 62.04 74.42
Max 76.6 43.3 12.9 62.25 74.16
Conv 77.6 43.6 12.9 62.63 74.81
of slightly more model parameters and negligible FLOPs
increase, which justifies our default design choice.
4.2.3 Comparison with Previous Pre-trained Models
In this section, we show the effectiveness of the proposed
self-supervised learning method by comparing it with pre-
vious state-of-the-art supervised and self-supervised pre-
trained models. The supervised models contain four ad-
vanced video Transformers pre-trained on large-scale la-
beled video or image datasets, including TimeSformer [57],
MViT [58], MViTv2 [59], and Video Swin Transformer [60].
The self-supervised part involves eight cutting-edge mod-
els pre-trained on massive unlabeled data (most are facial
images or videos), including FAb-Net [48], TCAE [49], Face-
Cycle [50], BMVC’20 [51], MoCo [70], SwAV [52], ρBYOL
[71], SVT [72], VideoMAE [24], and FaRL [53]. It should be
noted, all self-supervised models except ρBYOL, SVT, and
VideoMAE are 2D models, i.e., they can not process video
inputs directly. Therefore, we add a standard Transformer
block on top of them to enable temporal sequential model-
ing. Besides, all models share the same evaluation protocol
to ensure a fair comparison.
The comprehensive comparison results on the split 1 of
DFEW are presented in Table 7. From the table, we have the
following key observations:
•Our SVFAP outperforms all supervised pre-trained
video Transformers. Specifically, SVFAP-B surpasses
the best-performing Video Swin Base (Swin-B) model
by 3.25% UAR and 2.91% WAR, although Swin-B has
a larger model size (87.6M vs. 77.6M) and more than
double FLOPs (92.5G vs. 43.6G), and is pre-trained
on the combination of large-scale labeled images and
videos. The more encouraging thing is that even
our small model SVFAP-S still outperforms Swin-B
slightly, thus amply demonstrating the remarkable
superiority of our proposed method. We also no-
tice that TimeSformer also achieves satisfying results
but it suffers from huge computational costs and
has much more parameters. To sum up, the com-
parison results with supervised models show that
our method can learn strong and transferable affect-
related facial representations from large-scale video
data without using any human-annotated labels.
•Compared with self-supervised pre-trained models,
SVFAP also achieves the best performance. Notably,
when using the same Voxceleb2 dataset for pre-
training, both our base and small model improve
FAb-Net, TCAE, FaceCycle, and BMVC’20, by a sig-
nificant margin, which indicates that masked facial
video autoencoding is an effective task for self-JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11
TABLE 7
Comparison with state-of-the-art supervised and self-supervised pre-trained models on split 1 of DFEW. SSL: Self-Supervised Learning or not.
Method SSL Pre-training Dataset Dataset Type Architecture#Params
(M)FLOPs
(G)UAR WAR
TimeSformer [57] × ImageNet-21K+Kinetics-400 Image+Video TimeSformer 121 198 58.90 71.48
TimeSformer [57] ×ImageNet-21K+Kinetics-400
+HowTo100MImage+Video TimeSformer 121 198 59.13 71.62
MViT [58] × Kinetics-400 Video MViT-B 53 45 54.19 65.27
MViT [58] × Kinetics-600 Video MViT-B 53 45 55.80 66.85
MViTv2 [59] × Kinetics-400 Video MViTv2-B 51 42 55.12 67.88
Video Swin [60] × ImageNet-1K+Kinetics-400 Image+Video Swin-S 50 55 57.13 69.96
Video Swin [60] × ImageNet-21K+Kinetics-400 Image+Video Swin-B 88 93 59.38 71.90
FAb-Net [48] ✓ VoxCeleb1&2 Video ConvNet 6 33 45.52 56.01
TCAE [49] ✓ VoxCeleb1&2 Video ConvNet 6 33 45.29 56.35
FaceCycle [50] ✓ VoxCeleb1&2 Video ConvNet 4 42 42.27 54.05
BMVC’20 [51] ✓ VoxCeleb2 Video ResNet-18 12 15 56.55 67.12
MoCo [70] ✓ CelebA Image ResNet-50 32 34 53.47 67.45
SwAV [52] ✓ VGGFace2 Image ResNet-50 32 34 58.18 68.99
ρBYOL [71] ✓ Kinetics-400 Video SlowOnly-R50 32 43 58.60 69.81
SVT [72] ✓ Kinetics-400 Video TimeSformer 121 198 57.07 70.01
VideoMAE [24] ✓ Kinetics-400 Video ViT-B 86 81 58.32 70.94
FaRL [53] ✓ LAION-FACE Image+Text ViT-B 93 141 58.91 72.15
SVFAP-S (ours) ✓ VoxCeleb2 Video TPSBT-S 30 18 59.70 72.70
SVFAP-B (ours) ✓ VoxCeleb2 Video TPSBT-B 78 44 62.63 74.81
TABLE 8
Comparison with state-of-the-art methods on DFEW.
Method#Params
(M)FLOPs
(G)Accuracy of Each Emotion (%) Metric (%)
Happy Sad Neutral Anger Surprise Disgust Fear UAR WAR
3D ResNet-18 [39] - 8 76.32 50.21 64.18 62.85 47.52 0.00 24.56 46.52 58.27
EC-STFL [6] - 8 79.18 49.05 57.85 60.98 46.15 2.76 21.51 45.35 56.51
ResNet-18+LSTM [13] - 8 83.56 61.56 68.27 65.29 51.26 0.00 29.34 51.32 63.85
ResNet-18+GRU [13] - 8 82.87 63.83 65.06 68.51 52.00 0.86 30.14 51.68 64.02
Former-DFER [13] 18 9 84.05 62.57 67.52 70.03 56.43 3.45 31.78 53.69 65.70
CEFLNet [73] 13 - 84.00 68.00 67.00 70.00 52.00 0.00 17.00 51.14 65.35
EST [10] 43 - 86.87 66.58 67.18 71.84 47.53 5.52 28.49 53.43 65.85
STT [14] - - 87.36 67.90 64.97 71.24 53.10 3.49 34.04 54.58 66.65
DPCNet [29] 51 10 - - - - - - - 57.11 66.32
NR-DFERNet [74] - 6 88.47 64.84 70.03 75.09 61.60 0.00 19.43 54.21 68.19
IAL [35] - - 87.95 67.21 70.10 76.06 62.22 0.00 26.44 55.71 69.24
M3DFEL [75] - 2 89.59 68.38 67.88 74.24 59.69 0.00 31.63 56.10 69.25
SVFAP-S (ours) 30 18 92.39 74.92 70.40 76.90 62.70 8.28 37.58 60.45 72.67
SVFAP-B (ours) 78 44 93.13 76.98 72.31 77.54 65.42 15.17 39.25 62.83 74.27
TABLE 9
Comparison with state-of-the-art methods on FERV39k.
Method#Params
(M)FLOPs
(G)Accuracy of Each Emotion (%) Metric (%)
Happy Sad Neutral Anger Surprise Disgust Fear UAR WAR
C3D [36] 78 - 48.20 35.53 52.71 13.72 3.45 4.93 0.23 22.68 31.69
P3D [38] - - 61.85 42.21 49.80 42.57 10.50 0.86 5.57 30.48 40.81
R(2+1)D [76] - - 59.33 42.43 50.82 42.57 16.30 4.50 4.87 31.55 41.28
3D ResNet-18 [39] 33 - 57.64 28.21 59.60 33.29 4.70 0.21 3.02 26.67 37.57
ResNet-18+LSTM [27] - - 61.91 31.95 61.70 45.93 14.26 0.00 0.70 30.92 42.59
VGG-13+LSTM [27] - - 66.26 51.26 53.22 37.93 13.64 0.43 4.18 32.42 43.37
Two C3D [27] - - 54.85 52.91 60.67 31.34 5.96 2.36 6.96 30.72 41.77
Two ResNet-18+LSTM [27] - - 59.00 45.87 61.90 40.15 9.87 1.71 0.46 31.28 43.20
Two VGG-13+LSTM [27] - - 69.65 47.31 52.55 47.88 7.68 1.93 2.55 32.79 44.54
Former-DFER [13] 18 9 65.65 51.33 56.74 43.64 21.94 8.57 12.53 37.20 46.85
STT [14] - - 69.77 47.81 59.14 47.41 20.22 10.49 9.51 37.76 48.11
NR-DFERNet [74] - 6 69.18 54.77 51.12 49.70 13.17 0.00 0.23 33.99 45.97
IAL [35] - - - - - - - - - 35.82 48.54
M3DFEL [75] - 2 - - - - - - - 35.94 47.67
SVFAP-S (ours) 30 18 75.02 52.12 61.34 48.69 23.04 12.85 15.31 41.19 51.34
SVFAP-B (ours) 78 44 74.00 53.34 62.26 51.11 25.24 13.28 15.78 42.14 52.29JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B404550556065UAR10%
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B404550556065UAR20%
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B404550556065UAR50%
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B404550556065UAR100%
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B50556065707580WAR10%
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B50556065707580WAR20%
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B50556065707580WAR50%
TimeSformerMViT
MViTv2
Video SwinBMVC'20MoCo SwAV
VideoMAEFaRL
SVFAP-B50556065707580WAR100%
Fig. 7. Comparisons with state-of-the-art supervised and self-supervised pre-trained models on the split 1 of DFEW in the few-shot setting.
TABLE 10
Comparison with state-of-the-art methods on MAFW. AN: Anger. DI: Disgust. FE: Fear. HA: Happiness. NE: Neutral. SA: Sadness. SU: Surprise.
CO: Contempt. AX: Anxiety. HL: Helplessness. DS: Disappointment.
Method#Params
(M)FLOPs
(G)Accuracy of Each Emotion (%) Metric (%)
AN DI FE HA NE SA SU CO AX HL DS UAR WAR
ResNet-18 [31] 11 - 45.02 9.25 22.51 70.69 35.94 52.25 39.04 0.00 6.67 0.00 0.00 25.58 36.65
ViT [26] 86 - 46.03 18.18 27.49 76.89 50.70 68.19 45.13 1.27 18.93 1.53 1.65 32.36 45.04
C3D [36] 78 - 51.47 10.66 24.66 70.64 43.81 55.04 46.61 1.68 24.34 5.73 4.93 31.17 42.25
ResNet-18+LSTM [28] - - 46.25 4.70 25.56 68.92 44.99 51.91 45.88 1.69 15.75 1.53 1.65 28.08 39.38
ViT+LSTM [28] - - 42.42 14.58 35.69 76.25 54.48 68.87 41.01 0.00 24.40 0.00 1.65 32.67 45.56
C3D+LSTM [28] - - 54.91 0.47 9.00 73.43 41.39 64.92 58.43 0.00 24.62 0.00 0.00 29.75 43.76
Former-DFER [13] 18 9 - - - - - - - - - - - 31.16 43.27
T-ESFL [28] - - 62.70 2.51 29.90 83.82 61.16 67.98 48.50 0.00 9.52 0.00 0.00 33.28 48.18
SVFAP-S (ours) 30 18 63.88 19.56 30.88 84.46 62.83 68.37 59.61 1.27 31.88 7.63 7.69 39.82 53.89
SVFAP-B (ours) 78 44 64.60 25.20 35.68 82.77 57.12 70.41 58.58 8.05 32.42 8.40 9.89 41.19 54.28
supervised pre-training. Our method also beats con-
trastive learning-based methods (i.e., MoCo, SwAV ,
and SVT), verifying the advantage of the gener-
ative paradigm in self-supervised learning. When
compared to VideoMAE, SVFAP-S still shows better
performance while having 2.9 ×fewer parameters
and 4.5 ×fewer FLOPs. Finally, our method also
outperforms the best-performing FaRL (i.e., 3.72%
UAR and 2.66% WAR improvements for SVFAP-
B, 0.32% UAR and 0.80% WAR improvements for
SVFAP-S), although FaRL has much more FLOPs and
parameters and is pre-trained on a huge multimodal
dataset. To summarize, the above comparison results
with self-supervised models show that our SVFAP is
an effective and efficient self-supervised video facial
affect perceiver.
In addition to the evaluation in full data regime, we also
conduct experiments to investigate the generalization ability
of SVFAP under few-shot settings. To this end, we randomly
select 50%, 20%, and 10% samples from the training set
of DFEW split 1 to obtain a series of new training sets
while keeping the test set unchanged. For simplicity, we
only choose several representative models in Table 7 forevaluation. Note that, for the method with more than one
model, we only use the best one. The results are reported in
Fig. 7. We can observe that: 1) As expected, the performance
of all methods degrades accordingly when fewer training
samples are used. 2) SVFAP consistently outperforms all
compared methods under each few-shot setting, which ver-
ifies the strong adaptation ability of the proposed methods
in the low data regime. This ability is particularly important
considering the longstanding data scarcity issue in affective
computing. Notably, even with 10% (about 935 samples)
training data, our method still achieves promising results
(more than 50% UAR and 63% WAR).
4.2.4 Comparisons with State-of-the-art Methods
In this section, we compare the proposed method with state-
of-the-art methods on both in-the-wild and lab-controlled
dynamic facial expression recognition datasets.
We first show the comparison results on three large in-
the-wild datasets, including DFEW, FERV39k, and MAFW.
The results on three datasets are reported in Table 8, 9, and
10, respectively. The comparison baselines can be roughly
divided into three categories: 1) the combination of convo-
lution neural network (CNN) and recurrent neural network
(RNN), i.e., CNN+RNN, such as ResNet+LSTM [31], [32]. 2)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13
TABLE 11
Comparison with state-of-the-art methods on CREMA-D.
Method Modality UAR WAR
VO-LSTM [77] Video - 66.80
Goncalves et al. [78] Video - 62.20
Lei et al. [79] Video 64.68 64.76
AV-LSTM [77] Video+Audio - 72.90
AV-Gating [77] Video+Audio - 74.00
TFN [80] Video+Audio - 63.09
EF-GRU [81] Video+Audio - 57.06
LF-GRU [81] Video+Audio - 58.53
MulT Base [81] Video+Audio - 68.87
MulT Large [81] Video+Audio - 70.22
Goncalves et al. [78] Video+Audio - 77.30
SVFAP-S (ours) Video 74.58 74.58
SVFAP-B (ours) Video 77.31 77.37
classic 3D CNNs, including 3D ResNet [39], C3D [36], P3D
[38], and R(2+1)D [76]. 3) hybrid architectures of CNN and
Transformer, e.g., Former-DFER [13], STT [14], NR-DFERNet
[74], IAL [35], and T-ESFL [28].
As shown in Table 8, we observe that SVFAP-B out-
performs previous state-of-the-art methods on DFEW sig-
nificantly (i.e., 5.72% UAR and 5.02% WAR improvement),
setting a new record on this dataset. Besides, the small ver-
sion, SVFAP-S, also surpasses the best-performing methods
by a large margin, achieving a better accuracy-complexity
trade-off. When comparing the fine-grained performance
of each class, we find that our methods achieve remark-
able improvements for most emotions (e.g., happy and sad).
Notably, for the rare disgust emotion which only accounts
for 1.2% (about 146 samples) in the whole dataset, most
baseline methods fail to classify its samples correctly. Never-
theless, our SVFAP-B improves the previous best performer
by about 10%. This result demonstrates that the proposed
method can learn generic affect-related representations via
large-scale self-supervised pre-training, thus alleviating the
unbalanced learning in minority classes. Moreover, we have
similar observations on the other two datasets. On the
largest DFER dataset FERV39k, as shown in Table 9, SVFAP-
B achieves 42.14% UAR and 52.29% WAR, outperforming
the best baselines by 4.38% UAR and 3.75% WAR. On the
MAFW dataset, as given in Table 10, SVFAP-B improves
over the state-of-the-art T-ESFL by 7.91% UAR and 6.10%
WAR. Besides, a slight performance drop is also observed
for both datasets. To sum up, the above encouraging results
on three in-the-wild datasets verify the strong generalization
ability of our SVFAP in real-world scenarios.
Finally, we present the comparison results on three small
lab-controlled datasets, including CREMA-D, RAVDESS,
and eNTERFACE05. The results are reported in Table 11,
12, and 13, respectively. Similarly, we observe consistently
significant performance improvements on these datasets.
For instance, as shown in Table 11, SVFAP-B outperforms
the best-performing unimodal methods by about 12% UAR
and 10% WAR on CREMA-D. We also report the results of
several multimodal methods. Compared with them, SVFAP-
B still shows slightly better performance without using the
audio information, which demonstrates the overwhelming
superiority of the proposed method again.TABLE 12
Comparison with state-of-the-art methods on RAVDESS.
Method Modality UAR WAR
VO-LSTM [77] Video - 60.50
3D ResNeXt-50 [67] Video - 62.99
AV-LSTM [77] Video+Audio - 65.80
AV-Gating [77] Video+Audio - 67.70
MCBP [67] Video+Audio - 71.32
MMTM [67] Video+Audio - 73.12
MSAF [67] Video+Audio - 74.86
CFN-SR [68] Video+Audio - 75.76
SVFAP-S (ours) Video 73.59 73.90
SVFAP-B (ours) Video 75.15 75.01
TABLE 13
Comparison with state-of-the-art methods on eNTERFACE05.
Method UAR WAR
Mansoorizadeh et al. [82] - 37.00
3DCNN [83] - 41.05
3DCNN-DAP [83] - 41.36
Zhalehpour et al. [84] - 42.16
FAN [85] - 51.44
STA-FER [86] - 42.98
TSA-FER [87] - 43.72
C-LSTM [88] - 45.29
EC-LSTM [89] - 49.26
Graph-Tran [9] - 54.62
SVFAP-S (ours) 57.16 57.12
SVFAP-B (ours) 60.58 60.54
4.3 Dimensional Emotion Recognition
4.3.1 Datasets
Werewolf-XL [90] is a spontaneous audio-visual dataset
with a total of 890 minutes of videos recorded during com-
petitive group interactions in Werewolf games. It contains
131,688 video clips from 129 subjects in 30 game sessions,
including 14,632 samples from active speakers and the rest
from listeners in the game. Werewolf-XL provides both self-
reported categorical emotion labels and externally assessed
dimensional emotion scores. In this paper, we only use
14,632 speaker samples and dimensional annotations. As
the goal is to predict continuous scores, we formulate it
as a regression problem as stated in Section 3.2. Besides,
we adopt a 5-fold session-independent cross-validation pro-
tocol for evaluation. Two types of standard metrics, i.e.,
Concordance Correlation Coefficient (CCC) and Pearson
Correlation Coefficient (PCC), are reported in this paper.
They are computed as follows:
PCC =cov(y,ˆy)
σˆyσy
CCC =2cov(y,ˆy)
σ2
ˆy+σ2y+ (µˆy−µy)2(14)
where µˆyandµyare mean values of the overall predictions
ˆyand overall labels yrespectively, σˆyandσyare their
standard deviations, and cov (y,ˆy)calculates the covariance
between predictions and labels.
A VCAffe [91] is currently the largest audio-visual
dataset with both affect and cognitive load attributes. It is
recorded by simulating a remote work setting and containsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14
TABLE 14
Comparison with state-of-the-art methods on Werewolf-XL.
Method Modality Dimension PCC CCC
HOG [92] VideoArousal 0.2082 0.1443
Valence 0.5254 0.3456
Dominance 0.2476 0.1690
VGGFace-LSTM [90] VideoArousal 0.0724 0.0461
Valence 0.6296 0.6038
Dominance 0.1430 0.0820
Zhang et al. [90] Video+AudioArousal 0.1641 0.2770
Valence 0.6314 0.6234
Dominance 0.3540 0.3840
SVFAP-S (ours) VideoArousal 0.2211 0.1786
Valence 0.6566 0.6374
Dominance 0.3202 0.2799
SVFAP-B (ours) VideoArousal 0.2351 0.1896
Valence 0.6711 0.6427
Dominance 0.3461 0.2969
TABLE 15
Comparison with state-of-the-art methods on AVCAffe.
Method Modality Arousal Valence
MC3-18 [76] Video 34.00 38.80
3D ResNet-18 [39] Video 30.90 39.50
R(2+1)D-18 [76] Video 33.30 34.90
MC3-18+VGG-16 [91] Video+Audio 38.90 41.70
3D ResNet-18+VGG-16 [91] Video+Audio 37.30 39.40
R(2+1)D-18+VGG-16 [91] Video+Audio 40.50 39.50
MC3-18+ResNet-18 [91] Video+Audio 36.00 39.20
3D ResNet-18+ResNet-18 [91] Video+Audio 35.10 39.10
R(2+1)D-18+ResNet-18 [91] Video+Audio 39.50 37.70
SVFAP-S (ours) Video 39.24 40.22
SVFAP-B (ours) Video 40.36 41.49
more than 108 hours of video from 106 subjects. Each subject
is asked to participate in 7 specially designed tasks. After
each task, self-reported affect (i.e., arousal and valence) and
cognitive load scores are collected as ground truth labels.
We only predict arousal and valence scores on a scale of 0-4
and formulate it as a classification problem according to the
original paper. Since the duration of each task is too long (7.5
minutes), each video has been segmented into multiple 6-
second short clips for model training. We follow the paper to
obtain video-level predictions by averaging clip-level scores
and employ the weighted F1 score as the evaluation metric.
The dataset provides an official split: 86 subjects for training
and 20 subjects for test.
4.3.2 Comparison with State-of-the-art Methods
The results on Werewolf-XL are shown in Table 14. As we
can see, our method achieves much better performance than
unimodal baseline methods in three emotion dimensions.
Specifically, SVFAP-B outperforms the best-performing ones
by about 0.04 CCC and 0.03 PCC in arousal, 0.04 CCC
and 0.04 PCC in valence, and 0.13 CCC and 0.10 PCC in
dominance. Besides, we also show the multimodal baseline
in the table. Compared with it, SVFAP-B still shows superior
performance in valence, although worse results are achieved
in arousal and dominance. Besides, we only observe a mod-
erate performance degradation for SVFAP-S on this dataset.
We further present the comparison results on AVCAffe
in Table 15. Similarly, we observe that both SVFAP-B and
SVFAP-S surpass the state-of-the-art unimodal methods byTABLE 16
Comparison with state-of-the-art methods on ChaLearn 2016 First
Impression in terms of CCC. O: Openness. C: Conscientiousness. E:
Extraversion. A: Agreeableness. N: Neuroticism.
Method O C E A N Average
DAN [93] 0.5693 0.6254 0.6070 0.4855 0.6025 0.5779
ResNet [94] 0.1561 0.1902 0.1355 0.0838 0.1373 0.1406
CRNet [95] 0.3748 0.3646 0.3987 0.2390 0.3226 0.3399
CAM-DAN +[96] 0.5882 0.6550 0.6326 0.5003 0.6199 0.5992
PersEmoN [97] 0.2067 0.2441 0.2675 0.1369 0.1768 0.2064
Amb-Fac [98] 0.5858 0.6750 0.5997 0.4971 0.5765 0.5868
SENet [99] 0.5300 0.5580 0.5815 0.4493 0.5708 0.5379
HRNet [100] 0.5923 0.6912 0.6436 0.5195 0.6273 0.6148
Swin [56] 0.2223 0.2426 0.2531 0.1224 0.1942 0.2069
3D ResNet [39] 0.3248 0.3601 0.3601 0.2120 0.3352 0.3185
Slow-Fast [101] 0.0256 0.0320 0.0185 0.0105 0.0184 0.0210
TPN [102] 0.4427 0.4767 0.4998 0.3230 0.4675 0.4420
VAT [103] 0.6216 0.6753 0.6836 0.5228 0.6456 0.6298
SVFAP-S (ours) 0.6313 0.6974 0.7210 0.5427 0.6648 0.6514
SVFAP-B (ours) 0.6511 0.7141 0.7351 0.5498 0.6724 0.6645
TABLE 17
Comparison with state-of-the-art methods on ChaLearn 2016 First
Impression in terms of ACC. O: Openness. C: Conscientiousness. E:
Extraversion. A: Agreeableness. N: Neuroticism.
Method O C E A N Average
DAN [93] 0.9098 0.9106 0.9096 0.9102 0.9061 0.9093
CNN-LSTM [104] 0.8832 0.8742 0.8778 0.8933 0.8770 0.8811
ResNet [94] 0.8896 0.8835 0.8837 0.8968 0.8830 0.8873
CRNet [95] 0.8987 0.8932 0.8952 0.9018 0.8908 0.8960
CAM-DAN +[96] 0.9115 0.9139 0.9126 0.9118 0.9089 0.9118
PersEmoN [97] 0.8934 0.8893 0.8913 0.8994 0.8866 0.8920
Amb-Fac [98] 0.9101 0.9141 0.9082 0.9095 0.9038 0.9091
SENet [99] 0.9076 0.9060 0.9080 0.9097 0.9061 0.9075
HRNet [100] 0.9101 0.9154 0.9111 0.9113 0.9084 0.9113
Swin [56] 0.8937 0.8870 0.8893 0.8983 0.8860 0.8909
3D ResNet [39] 0.8964 0.8921 0.8933 0.9008 0.8915 0.8948
Slow-Fast [101] 0.8780 0.8604 0.8443 0.8809 0.8613 0.8650
TPN [102] 0.9025 0.8963 0.9019 0.9013 0.8992 0.9003
VAT [103] 0.9115 0.9123 0.9153 0.9099 0.9098 0.9118
SVFAP-S (ours) 0.9144 0.9175 0.9208 0.9135 0.9134 0.9159
SVFAP-B (ours) 0.9162 0.9187 0.9227 0.9152 0.9145 0.9175
a large margin (especially in arousal). For instance, SVFAP-
B achieves about 6% F1-score improvement in arousal and
about 2% in valence. Moreover, when compared with multi-
modal methods, our base model still presents a comparable
performance in both arousal and valence, which verifies the
effectiveness of large-scale self-supervised pre-training for
dimension emotion recognition.
4.4 Personality Recognition
Finally, to further verify the general applicability of the pro-
posed method, we evaluate it on the personality recognition
task. Different from the above two tasks we have explored,
this task requires stable and prototypical facial behavior
modeling to capture relevant features that reflect personality
traits. Thus, experiments on this task can provide a more
comprehensive evaluation of our proposed method.
The classic ChaLearn First Impression dataset [105] is
selected for evaluation. It consists of 10,000 talking-to-
the-camera clips extracted from over 3,000 high-definition
YouTube videos. Each video clip has a duration of about 15
seconds and is annotated with the Big Five personality traits
(i.e., openness, conscientiousness, extraversion, agreeable-
ness, and neuroticism). This dataset has been split into three
sets: 6000 videos for training, 2000 videos for validation,JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15
and 2000 videos for the final test. Since the task is to predict
continuous scores in different personality dimensions, we
formulate it as a regression problem. Following [106], we
use two standard evaluation metrics, i.e., CCC and the
Accuracy (ACC). CCC is defined in Eq. (14). The definition
of ACC is given as follows:
ACC = 1−1
NNX
i=1|yi−ˆyi| (15)
where ˆyis the prediction, yis the label, and Nis the number
of test videos.
The comparison results in terms of CCC and ACC are
shown in Table 16 and Table 17, respectively. For CCC, we
observe that two versions of our models achieve similar
performance. Both of them outperform the best-performing
supervised baseline largely, achieving about 0.02-0.03 CCC
improvement. For ACC, we find that the variation be-
tween different methods is relatively small. Nevertheless,
our methods still show the best performance.
5 C ONCLUSION
In this paper, we have presented a self-supervised learning
method, termed Self-supervised Video Facial Affect Per-
ceiver (SVFAP), to unleash the power of large-scale self-
supervised pre-training for video-based facial affect anal-
ysis. SVFAP utilizes masked facial video autoencoding as
the objective to perform self-supervised pre-training on a
large amount of unlabeled facial videos. Besides, it employs
a novel TBSBT model as the encoder to minimize large
redundancy in 3D facial video data from both spatial and
temporal perspectives, leading to significantly lower com-
putational costs and superior performance. To verify the ef-
fectiveness of SVFAP , we conduct extensive experiments on
nine datasets in three popular downstream tasks, including
dynamic facial expression recognition, dimensional emo-
tion recognition, and personality recognition. The results
demonstrate that SVFAP can learn powerful affect-related
representations via large-scale self-supervised pre-training.
Specifically, it largely outperforms previous pre-trained su-
pervised and self-supervised models and also shows strong
adaptation ability in the low data regime. Moreover, our
SVFAP achieves significant improvements over state-of-the-
art methods in three downstream tasks, setting new records
on all datasets.
In future work, we plan to investigate the scaling be-
havior of SVFAP using larger models and more unlabeled
data. Besides, it is also interesting to evaluate SVFAP in
other downstream tasks, such as dynamic micro-expression
recognition and depression level detection. We also hope
our work can inspire more relevant research to further ad-
vance the development of video-based facial affect analysis.
ACKNOWLEDGMENTS
This work is supported by the National Natural Science
Foundation of China (NSFC) ( No.62276259, No.62201572,
No.U21B2010, No.62271083, No.62306316).REFERENCES
[1] M. Pantic and L. J. M. Rothkrantz, “Automatic analysis of facial
expressions: The state of the art,” IEEE Transactions on pattern
analysis and machine intelligence , vol. 22, no. 12, pp. 1424–1445,
2000.
[2] P . V . Rouast, M. T. Adam, and R. Chiong, “Deep learning for
human affect recognition: Insights and new developments,” IEEE
Transactions on Affective Computing , vol. 12, no. 2, pp. 524–543,
2019.
[3] E. Sariyanidi, H. Gunes, and A. Cavallaro, “Automatic analysis
of facial affect: A survey of registration, representation, and
recognition,” IEEE transactions on pattern analysis and machine
intelligence , vol. 37, no. 6, pp. 1113–1133, 2014.
[4] S. Zhao, X. Yao, J. Yang, G. Jia, G. Ding, T.-S. Chua, B. W. Schuller,
and K. Keutzer, “Affective image content analysis: Two decades
review and new perspectives,” IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , vol. 44, no. 10, pp. 6729–6751, 2021.
[5] Y. Fan, X. Lu, D. Li, and Y. Liu, “Video-based emotion recognition
using cnn-rnn and c3d hybrid networks,” in Proceedings of the
18th ACM international conference on multimodal interaction , 2016,
pp. 445–450.
[6] X. Jiang, Y. Zong, W. Zheng, C. Tang, W. Xia, C. Lu, and J. Liu,
“Dfew: A large-scale database for recognizing dynamic facial ex-
pressions in the wild,” in Proceedings of the 28th ACM International
Conference on Multimedia , 2020, pp. 2881–2889.
[7] S. Ebrahimi Kahou, V . Michalski, K. Konda, R. Memisevic, and
C. Pal, “Recurrent neural networks for emotion recognition in
video,” in Proceedings of the 2015 ACM on international conference
on multimodal interaction , 2015, pp. 467–474.
[8] L. Chao, J. Tao, M. Yang, Y. Li, and Z. Wen, “Long short term
memory recurrent neural network based multimodal dimen-
sional emotion recognition,” in Proceedings of the 5th international
workshop on audio/visual emotion challenge , 2015, pp. 65–72.
[9] R. Zhao, T. Liu, Z. Huang, D. P . Lun, and K.-M. Lam, “Spatial-
temporal graphs plus transformers for geometry-guided facial
expression recognition,” IEEE Transactions on Affective Computing ,
2022.
[10] Y. Liu, W. Wang, C. Feng, H. Zhang, Z. Chen, and Y. Zhan,
“Expression snippet transformer for robust video-based facial
expression recognition,” arXiv preprint arXiv:2109.08409 , 2021.
[11] D. Kollias and S. Zafeiriou, “Exploiting multi-cnn features in cnn-
rnn based dimensional emotion recognition on the omg in-the-
wild dataset,” IEEE Transactions on Affective Computing , vol. 12,
no. 3, pp. 595–606, 2020.
[12] L. Sun, Z. Lian, J. Tao, B. Liu, and M. Niu, “Multi-modal con-
tinuous dimensional emotion recognition using recurrent neural
network and self-attention mechanism,” in Proceedings of the 1st
International on Multimodal Sentiment Analysis in Real-life Media
Challenge and Workshop , 2020, pp. 27–34.
[13] Z. Zhao and Q. Liu, “Former-dfer: Dynamic facial expression
recognition transformer,” in Proceedings of the 29th ACM Inter-
national Conference on Multimedia , 2021, pp. 1553–1561.
[14] F. Ma, B. Sun, and S. Li, “Spatio-temporal transformer for dy-
namic facial expression recognition in the wild,” arXiv preprint
arXiv:2205.04749 , 2022.
[15] S. Li and W. Deng, “Deep facial expression recognition: A sur-
vey,” IEEE transactions on affective computing , vol. 13, no. 3, pp.
1195–1215, 2020.
[16] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Under-
standing deep learning (still) requires rethinking generalization,”
Communications of the ACM , vol. 64, no. 3, pp. 107–115, 2021.
[17] R. Lotfian and C. Busso, “Formulating emotion perception as
a probabilistic model with application to categorical emotion
classification,” in 2017 Seventh International Conference on Affective
Computing and Intelligent Interaction (ACII) . IEEE, 2017, pp. 415–
420.
[18] V . Sethu, E. M. Provost, J. Epps, C. Busso, N. Cummins, and
S. Narayanan, “The ambiguous world of emotion representa-
tion,” arXiv preprint arXiv:1909.00360 , 2019.
[19] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and
J. Tang, “Self-supervised learning: Generative or contrastive,”
IEEE Transactions on Knowledge and Data Engineering , vol. 35, no. 1,
pp. 857–876, 2021.
[20] L. Jing and Y. Tian, “Self-supervised visual feature learning with
deep neural networks: A survey,” IEEE transactions on pattern
analysis and machine intelligence , vol. 43, no. 11, pp. 4037–4058,
2020.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16
[21] C. Zhang, C. Zhang, J. Song, J. S. K. Yi, K. Zhang, and I. S. Kweon,
“A survey on masked autoencoder for self-supervised learning in
vision and beyond,” arXiv preprint arXiv:2208.00173 , 2022.
[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805 , 2018.
[23] K. He, X. Chen, S. Xie, Y. Li, P . Doll ´ar, and R. Girshick, “Masked
autoencoders are scalable vision learners,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 16 000–16 009.
[24] Z. Tong, Y. Song, J. Wang, and L. Wang, “VideoMAE: Masked
autoencoders are data-efficient learners for self-supervised video
pre-training,” in Advances in Neural Information Processing Systems ,
2022.
[25] C. Feichtenhofer, H. Fan, Y. Li, and K. He, “Masked autoencoders
as spatiotemporal learners,” arXiv preprint arXiv:2205.09113 , 2022.
[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly
et al. , “An image is worth 16x16 words: Transformers for image
recognition at scale,” arXiv preprint arXiv:2010.11929 , 2020.
[27] Y. Wang, Y. Sun, Y. Huang, Z. Liu, S. Gao, W. Zhang, W. Ge, and
W. Zhang, “Ferv39k: A large-scale multi-scene dataset for facial
expression recognition in videos,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022, pp.
20 922–20 931.
[28] Y. Liu, W. Dai, C. Feng, W. Wang, G. Yin, J. Zeng, and S. Shan,
“Mafw: A large-scale, multi-modal, compound affective database
for dynamic facial expression recognition in the wild,” in Proceed-
ings of the 30th ACM International Conference on Multimedia , 2022,
pp. 24–32.
[29] Y. Wang, Y. Sun, W. Song, S. Gao, Y. Huang, Z. Chen, W. Ge, and
W. Zhang, “Dpcnet: Dual path multi-excitation collaborative net-
work for facial expression representation learning in videos,” in
Proceedings of the 30th ACM International Conference on Multimedia ,
2022, pp. 101–110.
[30] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” arXiv preprint
arXiv:1409.1556 , 2014.
[31] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in Proceedings of the IEEE conference on
computer vision and pattern recognition , 2016, pp. 770–778.
[32] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural computation , vol. 9, no. 8, pp. 1735–1780, 1997.
[33] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evalua-
tion of gated recurrent neural networks on sequence modeling,”
arXiv preprint arXiv:1412.3555 , 2014.
[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems , vol. 30, 2017.
[35] H. Li, H. Niu, Z. Zhu, and F. Zhao, “Intensity-aware loss for
dynamic facial expression recognition in the wild,” in Proceedings
of the AAAI Conference on Artificial Intelligence , vol. 37, no. 1, 2023,
pp. 67–75.
[36] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri,
“Learning spatiotemporal features with 3d convolutional net-
works,” in Proceedings of the IEEE international conference on com-
puter vision , 2015, pp. 4489–4497.
[37] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a
new model and the kinetics dataset,” in proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2017, pp.
6299–6308.
[38] Z. Qiu, T. Yao, and T. Mei, “Learning spatio-temporal representa-
tion with pseudo-3d residual networks,” in proceedings of the IEEE
International Conference on Computer Vision , 2017, pp. 5533–5541.
[39] K. Hara, H. Kataoka, and Y. Satoh, “Can spatiotemporal 3d cnns
retrace the history of 2d cnns and imagenet?” in Proceedings of the
IEEE conference on Computer Vision and Pattern Recognition , 2018,
pp. 6546–6555.
[40] N. Komodakis and S. Gidaris, “Unsupervised representation
learning by predicting image rotations,” in International conference
on learning representations (ICLR) , 2018.
[41] H.-Y. Lee, J.-B. Huang, M. Singh, and M.-H. Yang, “Unsupervised
representation learning by sorting sequences,” in Proceedings of
the IEEE international conference on computer vision , 2017, pp. 667–
676.
[42] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum
contrast for unsupervised visual representation learning,” in Pro-ceedings of the IEEE/CVF conference on computer vision and pattern
recognition , 2020, pp. 9729–9738.
[43] M. Caron, I. Misra, J. Mairal, P . Goyal, P . Bojanowski, and
A. Joulin, “Unsupervised learning of visual features by contrast-
ing cluster assignments,” Advances in Neural Information Process-
ing Systems , vol. 33, pp. 9912–9924, 2020.
[44] P . Vincent, H. Larochelle, Y. Bengio, and P .-A. Manzagol, “Ex-
tracting and composing robust features with denoising autoen-
coders,” in Proceedings of the 25th international conference on Ma-
chine learning , 2008, pp. 1096–1103.
[45] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. ,
“Improving language understanding by generative pre-training,”
2018.
[46] H. Bao, L. Dong, S. Piao, and F. Wei, “Beit: Bert pre-training of
image transformers,” arXiv preprint arXiv:2106.08254 , 2021.
[47] S. Roy and A. Etemad, “Spatiotemporal contrastive learning of
facial expressions in videos,” in 2021 9th International Conference
on Affective Computing and Intelligent Interaction (ACII) . IEEE,
2021, pp. 1–8.
[48] O. Wiles, A. Koepke, and A. Zisserman, “Self-supervised learn-
ing of a facial attribute embedding from video,” arXiv preprint
arXiv:1808.06882 , 2018.
[49] Y. Li, J. Zeng, S. Shan, and X. Chen, “Self-supervised represen-
tation learning from videos for facial action unit detection,” in
Proceedings of the IEEE/CVF Conference on Computer vision and
pattern recognition , 2019, pp. 10 924–10 933.
[50] J.-R. Chang, Y.-S. Chen, and W.-C. Chiu, “Learning facial repre-
sentations from the cycle-consistency of face,” in Proceedings of
the IEEE/CVF International Conference on Computer Vision , 2021,
pp. 9680–9689.
[51] L. Lu, L. Tavabi, and M. Soleymani, “Self-supervised learning for
facial action unit recognition through temporal consistency,” in
BMVC , 2020.
[52] A. Bulat, S. Cheng, J. Yang, A. Garbett, E. Sanchez, and G. Tz-
imiropoulos, “Pre-training strategies and datasets for facial rep-
resentation learning,” in European Conference on Computer Vision .
Springer, 2022, pp. 107–125.
[53] Y. Zheng, H. Yang, T. Zhang, J. Bao, D. Chen, Y. Huang, L. Yuan,
D. Chen, M. Zeng, and F. Wen, “General facial representation
learning in a visual-linguistic manner,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 18 697–18 709.
[54] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,
A. Xiao, C. Xu, Y. Xu et al. , “A survey on vision transformer,” IEEE
transactions on pattern analysis and machine intelligence , vol. 45,
no. 1, pp. 87–110, 2022.
[55] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and
H. J ´egou, “Training data-efficient image transformers & distil-
lation through attention,” in International conference on machine
learning . PMLR, 2021, pp. 10 347–10 357.
[56] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
“Swin transformer: Hierarchical vision transformer using shifted
windows,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision , 2021, pp. 10 012–10 022.
[57] G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention
all you need for video understanding?” in ICML , vol. 2, no. 3,
2021, p. 4.
[58] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and
C. Feichtenhofer, “Multiscale vision transformers,” in Proceedings
of the IEEE/CVF International Conference on Computer Vision , 2021,
pp. 6824–6835.
[59] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and
C. Feichtenhofer, “Mvitv2: Improved multiscale vision trans-
formers for classification and detection,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 4804–4814.
[60] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, “Video
swin transformer,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022, pp. 3202–3211.
[61] D. Hendrycks and K. Gimpel, “Gaussian error linear units
(gelus),” arXiv preprint arXiv:1606.08415 , 2016.
[62] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and
J. Carreira, “Perceiver: General perception with iterative atten-
tion,” in International conference on machine learning . PMLR, 2021,
pp. 4651–4664.
[63] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep
speaker recognition,” Proc. Interspeech 2018 , pp. 1086–1090, 2018.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17
[64] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,
and R. Verma, “Crema-d: Crowd-sourced emotional multimodal
actors dataset,” IEEE transactions on affective computing , vol. 5,
no. 4, pp. 377–390, 2014.
[65] S. R. Livingstone and F. A. Russo, “The ryerson audio-visual
database of emotional speech and song (ravdess): A dynamic,
multimodal set of facial and vocal expressions in north american
english,” PloS one , vol. 13, no. 5, p. e0196391, 2018.
[66] O. Martin, I. Kotsia, B. Macq, and I. Pitas, “The enterface’05
audio-visual emotion database,” in 22nd International Conference
on Data Engineering Workshops (ICDEW’06) . IEEE, 2006, pp. 8–8.
[67] L. Su, C. Hu, G. Li, and D. Cao, “Msaf: Multimodal split attention
fusion,” arXiv preprint arXiv:2012.07175 , 2020.
[68] Z. Fu, F. Liu, H. Wang, J. Qi, X. Fu, A. Zhou, and Z. Li, “A
cross-modal fusion network based on self-attention and residual
structure for multimodal emotion recognition,” arXiv preprint
arXiv:2111.02172 , 2021.
[69] Y. Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, and M. Nadai,
“Efficient training of visual transformers with small datasets,”
Advances in Neural Information Processing Systems , vol. 34, pp.
23 818–23 830, 2021.
[70] N. Zhao, Z. Wu, R. W. Lau, and S. Lin, “What makes instance
discrimination good for transfer learning?” in International Con-
ference on Learning Representations , 2020.
[71] C. Feichtenhofer, H. Fan, B. Xiong, R. Girshick, and K. He, “A
large-scale study on unsupervised spatiotemporal representation
learning,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2021, pp. 3299–3309.
[72] K. Ranasinghe, M. Naseer, S. Khan, F. S. Khan, and M. S.
Ryoo, “Self-supervised video transformer,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 2874–2884.
[73] Y. Liu, C. Feng, X. Yuan, L. Zhou, W. Wang, J. Qin, and Z. Luo,
“Clip-aware expressive feature learning for video-based facial
expression recognition,” Information Sciences , vol. 598, pp. 182–
195, 2022.
[74] H. Li, M. Sui, Z. Zhu et al. , “Nr-dfernet: Noise-robust net-
work for dynamic facial expression recognition,” arXiv preprint
arXiv:2206.04975 , 2022.
[75] H. Wang, B. Li, S. Wu, S. Shen, F. Liu, S. Ding, and A. Zhou,
“Rethinking the learning paradigm for dynamic facial expres-
sion recognition,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2023, pp. 17 958–17 968.
[76] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri,
“A closer look at spatiotemporal convolutions for action recogni-
tion,” in Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition , 2018, pp. 6450–6459.
[77] E. Ghaleb, M. Popa, and S. Asteriadis, “Multimodal and temporal
perception of audio-visual cues for emotion recognition,” in 2019
8th International Conference on Affective Computing and Intelligent
Interaction (ACII) . IEEE, 2019, pp. 552–558.
[78] L. Goncalves and C. Busso, “Robust audiovisual emotion recog-
nition: Aligning modalities, capturing temporal information, and
handling missing features,” IEEE Transactions on Affective Com-
puting , vol. 13, no. 04, pp. 2156–2170, 2022.
[79] Y. Lei and H. Cao, “Audio-visual emotion recognition with pref-
erence learning based on intended and multi-modal perceived
labels,” IEEE Transactions on Affective Computing , pp. 1–16, 2023.
[80] A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P . Morency,
“Tensor fusion network for multimodal sentiment analysis,” in
Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing , 2017, pp. 1103–1114.
[81] M. Tran and M. Soleymani, “A pre-trained audio-visual trans-
former for emotion recognition,” in ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2022, pp. 4698–4702.
[82] M. Mansoorizadeh and N. Moghaddam Charkari, “Multimodal
information fusion application to human emotion recognition
from face and speech,” Multimedia Tools and Applications , vol. 49,
no. 2, pp. 277–297, 2010.
[83] Y.-H. Byeon and K.-C. Kwak, “Facial expression recognition
using 3d convolutional neural network,” International journal of
advanced computer science and applications , vol. 5, no. 12, 2014.
[84] S. Zhalehpour, O. Onder, Z. Akhtar, and C. E. Erdem, “Baum-1: A
spontaneous audio-visual face database of affective and mental
states,” IEEE Transactions on Affective Computing , vol. 8, no. 3, pp.
300–313, 2016.[85] D. Meng, X. Peng, K. Wang, and Y. Qiao, “Frame attention
networks for facial expression recognition in videos,” in 2019
IEEE international conference on image processing (ICIP) . IEEE,
2019, pp. 3866–3870.
[86] X. Pan, G. Ying, G. Chen, H. Li, and W. Li, “A deep spatial and
temporal aggregation framework for video-based facial expres-
sion recognition,” IEEE Access , vol. 7, pp. 48 807–48 815, 2019.
[87] X. Pan, W. Guo, X. Guo, W. Li, J. Xu, and J. Wu, “Deep temporal–
spatial aggregation for video-based facial expression recogni-
tion,” Symmetry , vol. 11, no. 1, p. 52, 2019.
[88] R. Miyoshi, N. Nagata, and M. Hashimoto, “Facial-expression
recognition from video using enhanced convolutional lstm,” in
2019 Digital Image Computing: Techniques and Applications (DICTA) .
IEEE, 2019, pp. 1–6.
[89] R. Miyoshi, N. Nagata, and M. Hashimoto, “Enhanced convo-
lutional lstm with spatial and temporal skip connections and
temporal gates for facial expression recognition from video,”
Neural Computing and Applications , vol. 33, no. 13, pp. 7381–7392,
2021.
[90] K. Zhang, X. Wu, X. Xie, X. Zhang, H. Zhang, X. Chen, and
L. Sun, “Werewolf-xl: A database for identifying spontaneous
affect in large competitive group interactions,” IEEE Transactions
on Affective Computing , vol. 14, no. 02, pp. 1201–1214, 2023.
[91] P . Sarkar, A. Posen, and A. Etemad, “Avcaffe: A large scale audio-
visual dataset of cognitive load and affect for remote work,” arXiv
preprint arXiv:2205.06887 , 2022.
[92] N. Dalal and B. Triggs, “Histograms of oriented gradients for
human detection,” in 2005 IEEE computer society conference on
computer vision and pattern recognition (CVPR’05) , vol. 1. Ieee,
2005, pp. 886–893.
[93] X.-S. Wei, C.-L. Zhang, H. Zhang, and J. Wu, “Deep bimodal
regression of apparent personality traits from short video se-
quences,” IEEE Transactions on Affective Computing , vol. 9, no. 3,
pp. 303–315, 2017.
[94] Y. G ¨uc ¸l¨ut¨urk, U. G ¨uc ¸l¨u, M. A. van Gerven, and R. van Lier, “Deep
impression: Audiovisual deep residual networks for multimodal
apparent personality trait recognition,” in European conference on
computer vision . Springer, 2016, pp. 349–358.
[95] Y. Li, J. Wan, Q. Miao, S. Escalera, H. Fang, H. Chen, X. Qi, and
G. Guo, “Cr-net: A deep classification-regression network for
multimodal apparent personality analysis,” International Journal
of Computer Vision , vol. 128, no. 12, pp. 2763–2780, 2020.
[96] C. Ventura, D. Masip, and A. Lapedriza, “Interpreting cnn
models for apparent personality trait regression,” in Proceedings
of the IEEE conference on computer vision and pattern recognition
workshops , 2017, pp. 55–63.
[97] L. Zhang, S. Peng, and S. Winkler, “Persemon: a deep network
for joint analysis of apparent personality, emotion and their
relationship,” IEEE Transactions on Affective Computing , 2019.
[98] C. Suman, S. Saha, A. Gupta, S. K. Pandey, and P . Bhattacharyya,
“A multi-modal personality prediction system,” Knowledge-Based
Systems , vol. 236, p. 107715, 2022.
[99] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,”
inProceedings of the IEEE conference on computer vision and pattern
recognition , 2018, pp. 7132–7141.
[100] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu,
Y. Mu, M. Tan, X. Wang et al. , “Deep high-resolution representa-
tion learning for visual recognition,” IEEE transactions on pattern
analysis and machine intelligence , vol. 43, no. 10, pp. 3349–3364,
2020.
[101] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks
for video recognition,” in Proceedings of the IEEE/CVF international
conference on computer vision , 2019, pp. 6202–6211.
[102] C. Yang, Y. Xu, J. Shi, B. Dai, and B. Zhou, “Temporal pyramid
network for action recognition,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , 2020, pp. 591–
600.
[103] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video
action transformer network,” in Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , 2019, pp. 244–253.
[104] A. Subramaniam, V . Patel, A. Mishra, P . Balasubramanian, and
A. Mittal, “Bi-modal first impressions recognition using tem-
porally ordered deep audio and stochastic visual features,” in
European conference on computer vision . Springer, 2016, pp. 337–
348.
[105] V . Ponce-L ´opez, B. Chen, M. Oliu, C. Corneanu, A. Clap ´es,
I. Guyon, X. Bar ´o, H. J. Escalante, and S. Escalera, “ChalearnJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18
lap 2016: First round challenge on first impressions-dataset and
results,” in European conference on computer vision . Springer, 2016,
pp. 400–418.
[106] R. Liao, S. Song, and H. Gunes, “An open-source benchmark
of deep learning models for audio-visual apparent and self-
reported personality recognition,” IEEE Transactions on Affective
Computing , 2024.
Licai Sun received the B.S. degree from Bei-
jing Forestry University, Beijing, China, in 2016,
and the M.S. degree from University of Chinese
Academy of Sciences, Beijing, China, in 2019.
He is currently working toward the Ph.D. degree
with the School of Artificial Intelligence, Univer-
sity of Chinese Academy of Sciences, Beijing,
China. His current research interests include
affective computing, deep learning, and multi-
modal representation learning.
Zheng Lian received the B.S. degree from the
Beijing University of Posts and Telecommuni-
cations (BUPT), Beijing, China, in 2016. And
he received the Ph.D. degree from the Institute
of Automation, Chinese Academy of Sciences,
Beijing, China, in 2021. He is currently an As-
sistant Professor at National Laboratory of Pat-
tern Recognition, Institute of Automation, Chi-
nese Academy of Sciences, Beijing, China. His
current research interests include affective com-
puting, deep learning, and multimodal emotion
recognition.
Kexin Wang received the B.S. degree from Bei-
jing University of Aeronautics and Astronautics,
Beijing, China, in 2021. She is currently working
toward the M.S. degree with the Institute of Au-
tomation, China Academy of Sciences, Beijing,
China. Her current research interests include
multi-label emotion recognition and noisy label
learning.
Yu He received the B.S. degree from Hunan
University, China, in 2013. He is currently work-
ing toward the M.S. degree with the Univer-
sity of Chinese Academy of Sciences, Beijing,
China. His current research interests include
multimodal affective computing and physiologi-
cal signal prediction.
Mingyu Xu received the B.S. degree from
Peking University, Beijing, China, in 2021. He is
currently working toward the M.S. degree with
the Institute of Automation, China Academy of
Sciences, Beijing, China. His current research
interests include uncertainty learning and partial
label learning.
Haiyang Sun received the B.E. degree from
Shandong University of Science and Technol-
ogy, China, in 2021. He is currently working to-
ward the M.S. degree with the Institute of Au-
tomation, China Academy of Sciences, Beijing,
China. His current research interests include
multimodal emotion recognition and neural ar-
chitecture search.
Bin Liu received his the B.S. degree and the
M.S. degree from Beijing institute of technology
(BIT), Beijing, China, in 2007 and 2009 respec-
tively. He received Ph.D. degree from the Na-
tional Laboratory of Pattern Recognition, Insti-
tute of Automation, Chinese Academy of Sci-
ences, Beijing, China, in 2015. He is currently an
Associate Professor in the National Laboratory
of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, China.
His current research interests include affective
computing and audio signal processing.
迅捷PDF编辑器
Jianhua Tao received the Ph.D. degree from
Tsinghua University, Beijing, China, in 2001, and
the M.S. degree from Nanjing University, Nan-
jing, China, in 1996. He is currently a Professor
with Department of Automation, Tsinghua Uni-
versity, Beijing, China. He has authored or coau-
thored more than eighty papers on major jour-
nals and proceedings. His current research in-
terests include speech recognition, speech syn-
thesis and coding methods, human–computer
interaction, multimedia information processing,
and pattern recognition. He is the Chair or Program Committee Member
for several major conferences, including ICPR, ACII, ICMI, ISCSLP , etc.
He is also the Steering Committee Member for the IEEE Transactions
on Affective Computing, an Associate Editor for Journal on Multimodal
User Interface and International Journal on Synthetic Emotions, and the
Deputy Editor-in-Chief for Chinese Journal of Phonetics.