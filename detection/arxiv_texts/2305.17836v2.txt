Data-driven Optimal Filtering for
Linear Systems with Unknown Noise Covariances
Shahriar Talebi1,2Amirhossein Taghvaei1Mehran Mesbahi1
1University of Washington, Seattle, WA, 98105
2Harvard University, Cambridge, MA, 02138
talebi@seas.harvard.edu amirtag@uw.edu mesbahi@uw.edu
Abstract
This paper examines learning the optimal filtering policy, known as the Kalman
gain, for a linear system with unknown noise covariance matrices using noisy
output data. The learning problem is formulated as a stochastic policy optimiza-
tion problem, aiming to minimize the output prediction error. This formulation
provides a direct bridge between data-driven optimal control and, its dual, op-
timal filtering. Our contributions are twofold. Firstly, we conduct a thorough
convergence analysis of the stochastic gradient descent algorithm, adopted for
the filtering problem, accounting for biased gradients and stability constraints.
Secondly, we carefully leverage a combination of tools from linear system theory
and high-dimensional statistics to derive bias-variance error bounds that scale
logarithmically with problem dimension, and, in contrast to subspace methods, the
length of output trajectories only affects the bias term.
1 Introduction
The duality of control and estimation plays a crucial role in system theory, linking two distinct
synthesis problems [5, 15, 20, 21, 35, 39]. This duality is an effective bridge between two distinct
disciplines, facilitating development of theoretical and computational techniques in one domain
and then adopting them for use in the other. For example, the stability proof of the Kalman filter
relies on the stabilizing characteristic of the optimal feedback gain in the dual Linear Quadratic
Regulator ( LQR ) optimal control problem [51, Ch. 9]. In this paper, we leverage this duality to learn
optimal filtering policies using recent advances in data-driven algorithms for optimal control.
We consider the estimation problem for a system with a known linear dynamic and observation model,
but unknown process and measurement noise covariances. Our objective is to learn the optimal steady-
state Kalman gain using a training dataset comprising independent realizations of the observation
signal. This problem has a rich history in system theory, often explored within the context of
adaptive Kalman filtering [4,9,31,32,36,42]. A comprehensive summary of four solution approaches
to this problem can be found in the classical reference [32]. These approaches include Bayesian
inference [17, 29, 30], Maximum likelihood [22, 41], covariance matching [36], and innovation
correlation methods [9,31]. While Bayesian and maximum likelihood approaches are computationally
intensive, and covariance matching introduces biases in practice, the innovation correlation-based
approaches have gained popularity and have been the subject of recent research [1, 12, 38]. For an
excellent survey on this topic, refer to the article [52]. However, it is important to note that these
approaches often lack non-asymptotic guarantees and heavily depend on statistical assumptions about
the underlying model.
In the realm of optimal control, significant progress has been made in the development of data-driven
synthesis methods. Notably, recent advances have focused on the adoption of first-order methods for
state-feedback LQR problems [6, 8]. The direct optimization of policies from a gradient-dominant
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.17836v2  [eess.SY]  26 Oct 2023perspective has first proven in [14] to be remarkably effective with global convergence despite
non-convex optimization landscape. It has been demonstrated that despite the non-convex nature of
the cost function, when expressed directly in terms of the policy, first-order methods exhibit global
convergence to the optimal policy. Building upon this line of work, the use of first-order methods for
policy optimization has been explored in variants of the LQR problem. These include Output-feedback
Linear Quadratic Regulators ( OLQR ) [13], model-free setup [33], risk-constrained setup [54], Linear
Quadratic Gaussian ( LQG ) [45], and most recently, Riemannian constrained LQR [43]. These
investigations have expanded the scope of data-driven optimal control, demonstrating the versatility
and applicability of first-order methods for a wide range of synthesis problems.
The objective of this paper is to provide fresh insights into the classical estimation problem by
leveraging the duality between control and estimation and incorporating recent advances in data-
driven optimal control. Specifically, building on the fundamental connection between the optimal
mean-squared error estimation problem and the LQR problem (Prop. 1), we reformulate determining
the optimal Kalman gain as a problem of synthesizing an optimal policy for the adjoint system, under
conditions that differ from those explored in the existing literature (see (10) and Remark 3). Upon
utilizing this relationship, we propose a Stochastic Gradient Descent ( SGD ) algorithm for learning
the optimal Kalman gain, accompanied by novel non-asymptotic error guarantees in presence of
biased gradient and stability constraint. Our approach opens up promising avenues for addressing the
estimation problem with robust and efficient data-driven techniques. The following is an informal
statement of our main results (combination of Thm. 1 and Thm. 2), and missing proofs appear in the
supplementary materials.
Theorem 3 (Informal) .Suppose the system is observable and both dynamic and measurement noise
are bounded. Then, with high probability, direct policy updates using stochastic gradient descent
with small stepsize converges linearly andglobally (from any initial stabilizing policy) to the optimal
steady-state Kalman gain.
More recently, the problem of learning the Kalman gain has been considered from a system identifi-
cation perspective, for completely unknown linear systems [26, 47, 48, 50]. In [47] and [48], subspace
system identification methods are used to obtain error bounds for learning the Markov parameters
of the model over a time horizon and establish logarithmic regret guarantee for output prediction
error. Due to the inherent difficulty of learning a completely unknown stochastic system from partial
observations, subspace methods assume marginal stability of the unknown system , and lead to sub-
optimal sample complexity bounds that grow with the number of Markov parameters, instead of the
number of unknowns [49, pp. 14]. Alternatively, [50] considers minimizing the output prediction
error and introduces a model-free policy gradient approach, under the same stability assumptions,
that achieves sublinear convergence rate . This paper provides a middle ground between completely
known and completely unknown systems, for a learning scenario that not only has relevant practical
implications, but also utilizes the duality relationship to LQR to establish linear convergence rates
even for unstable systems as long as they are observable . See the Appendix A for the discussion of
additional related works [28, 53, 55].
2 Background and Problem Formulation
Herein, first we propose the model setup in detail and discuss the Kalman filter as the estimation
strategy. Consider the discrete-time filtering problem given by the stochastic difference equations,
x(t+ 1) = Ax(t) +ξ(t),and y(t) =Hx(t) +ω(t), (1)
where x(t)∈Rnis the state of the system, y(t)∈Rmis the observation signal, and {ξ(t)}t∈Zand
{ω(t)}t∈Zare the uncorrelated zero-mean random vectors, that represent the process and measure-
ment noise respectively, with the following covariances,
E[ξ(t)ξ(t)⊺] =Q∈Rn×n,E[ω(t)ω(t)⊺] =R∈Rm×m,
for some positive (semi-)definite matrices Q, R⪰0. Let m0andP0⪰0denote the mean and
covariance of the initial condition x0.
In the filtering setup, the state x(t)is hidden, and the objective is to estimate it given the history of
the observation signal Y(t) ={y(0), y(1), . . . , y (t−1)}. The best linear mean-squared error (MSE)
2estimate of x(t)is defined according to
ˆx(t) = argmin
ˆx∈L(Y(t))E
∥x(t)−ˆx∥2
(2)
where L(Y(t))denotes the space of all linear functions of the history of the observation signal Y(t).
If the model parameters (A, H, Q, R )are known, the optimal MSE estimate ˆx(t)can be recursively
computed by the Kalman filter algorithm [20]:
ˆx(t+ 1) = Aˆx(t) +L(t)(y(t)−Hˆx(t)),ˆx(0) = m0, (3a)
P(t+ 1) = AP(t)A⊺+Q−AP(t)H⊺S(t)−1HP(T)A⊺, P (0) = P0, (3b)
where S(t) =HP(t)H⊺+R,L(t) :=AP(t)H⊺S(t)−1is the Kalman gain, and P(t) :=E[(x(t)−
ˆx(t))(x(t)−ˆx(t))⊺]is the error covariance matrix.
Assumption 1. The pair (A, H)is detectable, and the pair (A, Q1
2)is stabilizable, where Q1
2is the
unique positive semidefinite square root of Q.
Under this assumption, the error covariance P(t)converges to a steady-state value P∞, resulting
in a unique steady-state Kalman gain L∞=AP∞H⊺(HP∞H⊺+R)−1[25, 27]. It is common to
evaluate the steady-state Kalman gain L∞offline and use it, instead of L(t), to update the estimate in
real-time. Furthermore, we note that the assumption of uncorrelated random vectors is sufficient to
establish that Kalman filter provides the best linear estimate of the states given the observations for
minimizing the MSE criterion [20, Theorem 2].
2.1 Learning problem
Now, we describe our learning setup: 1) The system matrices AandHare known, but the process and
the measurement noise covariances, QandR, are notavailable. 2) We have access to an oracle that
generates independent realizations of the observation signal for given length T:{y(t)}T
t=0. However,
ground-truth measurements of the state x(t)isnotavailable.
Remark 1.Our proposed learning setup arises in various important engineering applications where
merely approximate or reduced-order linear models are available due to difficulty in analytically
capturing the effect of complex dynamics or disturbances, hence represented by noise with unknown
covariance matrices. Additionally, the system identification procedure often occurs through the
application of physical principles and collection of data from experiments in a controlled environment
(e.g., in a wind tunnel). However, identifying the noise covariance matrices strongly depends on the
operating environment which might be significantly different than the experimental setup. Therefore,
it is common engineering practice to use the learned system matrices and tune the Kalman gain to
improve the estimation error. We refer to [18] for the application of this procedure for gust load
alleviation in wings and [37] for estimation in chemical reactor models. We also emphasize that this
learning setup has a rich history in adaptive filtering with numerous references with a recent survey
on this topic [52]. As part of our future research, we will carry-out a robustness analysis, similar to
its LQR dual counterpart [10, 40], to study the effect of the error in system matrices on the learning
performance.
Inspired by the structure of the Kalman filter, our goal is to learn the steady-state Kalman gain L∞
from the data described in the learning setup:
Given: independent random realizations of {y(0), . . . , y (T)}with the parameters A, H
Learn: steady-state Kalman gain L∞
For that, we formulate the learning problem as a stochastic optimization described next.
2.2 Stochastic optimization formulation
Define ˆxL(T)to be the estimate given by the Kalman filter at time Trealized by the constant gain
L. Rolling out the update law (3a)fort= 0tot=T−1, and replacing L(t)withL, leads to the
following expression for the estimate ˆxL(T)as a function of L,
ˆxL(T) =AT
Lm0+PT−1
t=0AT−t−1
L Ly(t), (4)
3where AL:=A−LH. Note that evaluating this estimate does not require knowledge of QorR.
However, it is not possible to directly aim to learn the gain Lby minimizing the MSE (2)because the
ground-truth measurement of the state x(T)is not available. Instead, we propose to minimize the
MSE in predicting the observation y(T)as a surrogate objective function:1
minLJest
T(L) :=E
∥y(T)−ˆyL(T)∥2
(5)
where ˆyL(T) :=HˆxL(T). Note that while the objective function involves finite time horizon T, our
goal is to learn the steady-state Kalman gain L∞.
The justification for using the surrogate objective function in (5)instead of MSE error (2)lies in
the detectability assumption 1. Detectability implies that all unobservable states are stable; in other
words, their impact on the output signal vanishes quickly—depending on their stability time constant.
Numerically, the optimization problem (5)falls into the category of stochastic optimization and
can be solved by algorithms such as Stochastic Gradient Descent ( SGD ). Such an algorithm would
need access to independent realizations of the observation signal which are available. Theoretically
however, it is not yet clear if this optimization problem is well-posed and admits a unique minimizer.
This is the subject of following section where certain properties of the objective function, such as
its gradient dominance and smoothness, are established. These theoretical results are then used to
analyze first-order optimization algorithms and provide stability guarantees of the estimation policy
iterates. The results are based on the duality relationship between estimation and control that is
presented next.
3 Estimation-Control Duality Relationship
The stochastic optimization problem (5)is related to an LQR problem through the application of
the classical duality relationship between estimation and control [2, Ch.7.5]. In order to do so, we
introduce the adjoint system (dual to (1)) according to:
z(t) =A⊺z(t+ 1)−H⊺u(t+ 1), z(T) =a (6)
where z(t)∈Rnis the adjoint state and U(T) :={u(1), . . . , u (T)} ∈RmTare the control variables
(dual to the observation signal Y(T)). The adjoint state is initialized at z(T) =a∈Rnand simulated
backward in time starting with t=T−1. We introduce an LQR cost for the adjoint system:
JLQR
T(a,UT) :=z⊺(0)P0z(0) +PT
t=1[z⊺(t)Qz(t) +u⊺(t)Ru(t)], (7)
and formalize a relationship between linear estimation policies for the system (1)and linear control
policies for the adjoint system (6). A linear estimation policy takes the observation history YT∈RmT
and outputs an estimate ˆxL(T) :=L(YT)where L:RmT→Rnis a linear map. The adjoint of this
linear map, denoted by L†:Rn→RmT, is used to define a control policy for the adjoint system (6)
which takes the initial condition a∈Rnand outputs the control signal UL†=L†(a); i.e.,
{y(0), . . . , y (T−1)}L−→ˆxL(T) and {u(1), . . . , u (T)}L†
←−a.
The duality relationship between optimal MSE estimation and LQR control is summarized in the
following proposition. The proof is presented in the supplementary material.
Proposition 1. Consider the estimation problem for the system (1)and the LQR problem (7)subject
to the adjoint dynamics (6). For any linear estimation policy ˆxL(T) =L(YT), and for any a∈Rn,
we have the identity
E
|a⊺x(T)−a⊺ˆxL(T)|2
=JLQR
T(a,UL†(T)), (8)
whereUL†(T) =L†(a). In particular, for a Kalman filter with constant gain L, the output prediction
error (5)
E
∥y(T)−ˆyL(T)∥2
=Pm
i=1JLQR
T(Hi,UL⊺(T)) + tr [ R], (9)
where UL⊺(T) ={L⊺z(1), L⊺z(2), . . . , L⊺z(T)}, i.e., the feedback control policy with constant
gainL⊺, and H⊺
i∈Rnis the i-th row of the m×nmatrix Hfori= 1, . . . , m .
1The expectation in (5) is taken over all the random variables; consisting of the initial state x0, dynamic
noise ξ(t), and measurement noise ω(t)fort= 0,···, T. A conditional expectation is not necessary as the
estimate is constrained to be measurable with respect to the history of observation.
4Remark 2.The duality is also true in the continuous-time setting where the estimation problem is
related to a continuous-time LQR . Recent extensions to the nonlinear setting appears in [24] with a
comprehensive study in [23]. This duality is distinct from the maximum likelihood approach which
involves an optimal control problem over the original dynamics instead of the adjoint system [5].
3.1 Duality in steady-state regime
Using the duality relationship (9), the MSE in prediction (5) is expressed as:
Jest
T(L) = tr [ XT(L)H⊺H] + tr [ R],
where XT(L):=AT
LP0(A⊺
L)T+PT−1
t=0At
L(Q+LRL⊺)(A⊺
L)t. Define the set of Schur stabilizing
gains
S:={L∈Rn×m:ρ(AL)<1}.
For any L∈ S, in the steady-state, the mean-squared prediction error assumes the form,
limT→∞Jest
T(L) = tr [ X∞(L)H⊺H] + tr [ R],
where X∞(L) := lim T→∞XT(L)and coincides with the unique solution Xof the discrete Lya-
punov equation X=ALXA⊺
L+Q+LRL⊺(existence of unique solution follows from ρ(AL)<1).
Given the steady-state limit, we formally analyze the following constrained optimization problem:
min
L∈S←J(L):= tr
X(L)H⊺H
,s.t. X(L)=ALX(L)A⊺
L+Q+LRL⊺. (10)
Remark 3.Note that the latter problem is technically the dual of the optimal LQR problem as
formulated in [6] by relating A↔A⊺,−H↔B⊺,L↔K⊺, and H⊺H↔Σ. However, the
main difference here is that the product H⊺Hmay notbe positive definite, for example, due to rank
deficiency in Hspecially when m < n (whereas λ(Σ)>0appears in all of the bounds in [6, 14]).
Thus, in general, the cost function J(L)is not necessarily coercive in L, which can drastically effect
the optimization landscape. For the same reason, in contrast to the LQR case [6, 14], the gradient
dominant property of J(L)is not clear in the filtering setup. In the next section, we show that such
issues can be avoided as long as the pair (A, H)is observable. Also, the learning problem posed here
is distinct from its LQR counterpart (see Table 1).
3.2 Optimization landscape
The first result is concerned with the behaviour of the objective function at the boundary of the
optimization domain. It is known [7] that the set of Schur stabilizing gains Sis regular open,
contractible, and unbounded when m≥2and the boundary ∂Scoincides with the set {L∈Rn×m:
ρ(A−LH) = 1}. For simplicity of presentation, we consider a slightly stronger assumption:
Assumption 2. The pair (A, H)is observable, and the noise covariances Q≻0andR≻0.
Lemma 1. The function J(.) :S →Ris real-analytic and coercive with compact sublevel sets; i.e.,
L→∂Sor∥L∥ → ∞ each implies J(L)→ ∞ ,
andSα:={L∈Rn×m:J(L)≤α}is compact and contained in Sfor any finite α >0.
The next result establishes the gradient dominance property of the objective function. While this result
is known in the LQR setting ( [6, 14]), the extension to the estimation setup is not trivial as H⊺H,
which takes the role of the covariance matrix of the initial state in LQR , may not be positive definite
(instead, we only assume (A, H)is observable). This, apparently minor issue, hinders establishing
the gradient dominated property globally. However, we recover this property on every sublevel sets
ofJwhich is sufficient for the subsequent convergence analysis.
Lemma 2. Consider the constrained optimization problem (10).Then,
•The explicit formula for the gradient of Jis:∇J(L) = 2 Y(L) 
−LR+ALX(L)H⊺
,where
Y(L)=Yis the unique solution of Y=A⊺
LY AL+H⊺H.
•The global minimizer L∗= arg min L∈SJ(L)satisfies L∗=AX∗H⊺(R+HX∗H⊺)−1,with
X∗being the unique solution of X∗=AL∗X∗A⊺
L∗+Q+L∗R(L∗)⊺.
5•The function J(.) :Sα→R, for any non-empty sublevel set Sαfor some α > 0, satisfies the
following inequalities; for all L, L′∈ Sα:
c1[J(L)−J(L∗)] +c2∥L−L∗∥2
F≤ ⟨∇ J(L),∇J(L)⟩, (11a)
c3∥L−L∗∥2
F≤J(L)−J(L∗), (11b)
∥∇J(L)− ∇J(L′)∥F≤ℓ∥L−L′∥F, (11c)
for positive constants c1, c2, c3andℓthat are only a function of αand independent of L.
Note that the expression for the gradient is consistent with Proposition 3.8 in [6] after applying the
duality relationship explained in Remark 3.
Remark 4.The proposition above implies that J(.)has the Polyak-Łojasiewicz (PL) property (aka gra-
dient dominance) on every Sα; i.e., for any L∈ Sαwe have J(L)−J(L∗)≤1
c1(α)⟨∇J(L),∇J(L)⟩.
The inequality (11a) is more general as it characterizes the dominance gap in terms of the iterate error
from the optimality. This is useful in obtaining the iterate convergence results in the next section.
Also, the Lipschitz bound resembles its “dual” counterpart in [6, Lemma 7.9], however, it is not
implied as a simple consequence of duality because H⊺Hmay not be positive definite.
4 SGD for Learning the Kalman Gain
In order to emphasize on the estimation time horizon Tfor various measurement sequences, we use
YT:={y(t)}T
t=0to denote the measurement time-span. Note that, any choice of L∈ S corresponds
to a filtering strategy that outputs the following prediction,
ˆyL(T) =HAT
Lm0+PT−1
t=0HAT−t−1
L Ly(t).
We denote the squared-norm of the estimation error for this filtering strategy as,
ε(L,YT):=∥eT(L)∥2,
where eT(L):=y(T)−ˆyL(T). We also define the truncated objective function as
JT(L) :=E[ε(L,YT)],
where the expectation is taken over all possible random measurement sequences, and note that, at the
steady-state limit, we obtain limT→∞JT(L) =J(L).
TheSGD algorithm aims to solve this optimization problem by replacing the gradient, in the Gradient
Descent ( GD) update, with an unbiased estimate of the gradient in terms of samples from the
measurement sequence. In particular, with access to an oracle that produces independent realization
of the measurement sequence, say Mrandom independent measurements sequences {Yi
T}M
i=1, the
gradient can be approximated as follows: denote the approximated cost value
bJT(L):=1
MPM
i=1ε(L,Yi
T),
then the approximate gradient with batch-size Mis∇bJT(L) =1
MPM
i=1∇Lε(L,Yi
T). This forms
an unbiased estimate of the gradient of the “truncated objective”, i.e., Eh
∇bJT(L)i
=∇JT(L).
Next, for implementation purposes, we compute the gradient estimate explicitly in terms of the
measurement sequence and the filtering policy L.
Lemma 3. Given L∈ S and a sequence of measurements Y={y(t)}T
t=0, we have,
∇Lε(L,Y) =−2H⊺eT(L)y(T−1)⊺
+2PT−1
t=1−(A⊺
L)tH⊺eT(L)y(T−t−1)⊺+Pt
k=1(A⊺
L)t−kH⊺eT(L)y(T−t−1)⊺L⊺(A⊺
L)k−1H⊺.
Finally, using this approximate gradient, the so-called SGD update proceeds as,
Lk+1=Lk−ηk∇LbJT(L),
fork∈Z, where ηk>0is the step-size. Numerical results of the application of this algorithm
appears in Appendix G.
6Table 1: Differences between SGD algorithms for optimal LQR and optimal estimation problems
Problem Parameters Constraints Gradient Oracle
cost value QandR A andH stability S model biased
LQR [14] known known unknown yes E[J(L+r∆)∆] yes
∆∼U(Smn)
Estimation unknown unknown known yes E[∇ε(L,Y)] yes
(this work) Y ∼output data
Vanila SGD * * * no E[∇ε(L,Y)] no
Y ∼ data dist.
Remark 5.Computing this approximate gradient only requires the knowledge of the system matrices
AandH, and does notrequire the noise covariance information QandR. Simulation results for the
SGD algorithm are provided in the supplementary material.
Although the convergence of the SGD algorithm is expected to follow similar to the GDalgorithm
under the gradient dominance condition and Lipschitz property, the analysis becomes complicated due
to the possibility of the iterated gain Lkleaving the sub-level sets. It is expected that a convergence
guarantee would hold under high-probability due to concentration of the gradient estimate around the
true gradient. The complete analysis in this direction is provided in the subsequent sections.
We first provide sample complexity and convergence guarantees for SGD with a biased estima-
tion of gradient for locally Lipschitz objective functions and in presence of stability constraint S.
Subsequently, we study the stochastic problem of estimating the gradient for the estimation prob-
lem. Distinct features of our approach as compared with similar formulations in the literature are
highlighted in Table 1.
We now provide a road map to navigate the technical results that concludes with Theorem 3: Section
4.1 is concerned with the convergence analysis of the SGD algorithm under an assumption for
the biased gradient oracle, summarized in Theorem 1 which concludes the linear convergence of
the iterates for sufficiently small stepsize. Section 4.2 is concerned with the bias-variance error
analysis of the gradient estimate, summarized in Theorem 2 providing the sufficient values for the
batch-size and trajectory length that guarantees the desired bound on the gradient oracle required in
Theorem 1. Finally, combining the results from Theorem 1 and Theorem 2 concludes our main result
in Theorem 3.
4.1 SGD with biased gradient and stability constraint
First, we characterize the “robustness” of a policy at which we aim to estimate the gradient. This is
formalized in the following lemma which is a consequence of [43, Lemma IV .1].
Lemma 4. Consider any L∈ Sand let Zbe the unique solution of Z=ALZA⊺
L+Λfor any Λ≻0.
Then, L+ ∆∈ S for any ∆∈Rn×msatisfying 0≤ ∥∆∥F≤λ(Λ)
2λ(Z)∥H∥
.
Second, we provide a uniform lowerbound for the stepsize of gradient descent for an approximated
direction “close” to the true gradient direction.
Lemma 5 (Uniform Lower Bound on Stepsize) .LetL0∈ Sαfor some α≥α∗:=J(L∗), and
choose any finite β≥α. Consider any direction Esuch that ∥E− ∇J(L0)∥F≤γ∥∇J(L0)∥Ffor
some γ∈[0,1], then we have J(L0−ηE)≤βfor any ηsatisfying:
0≤η≤1−γ
(γ+ 1)2·1
ℓ(β)+c3(α)
ℓ(α)[α−α∗]s
β−α
2ℓ(β).
Remark 6.Note that for the case of exact gradient direction, i.e. when E=∇J(L0), we have γ= 0
and choosing β=αimplies the known uniform bound of η≤1
ℓ(β)for feasible stepsizes as expected.
Also, by this choice of β, this guarantees that the next iterate remains in sublevel set Sα. This lemma
generalizes this uniform bound for general directions and (potentially) larger sublevel set.
7The next proposition provides a decay guarantee for one iteration of gradient descent with an
approximate direction which will be used later for convergence of SGD with a biased gradient
estimate.
Proposition 2 (Linear Decay in Cost Value) .Suppose L0∈ Sαfor some α >0and a direction
E̸= 0is given such that ∥E−∇J(L)∥F≤γ∥∇J(L)∥Ffor some γ <1. Let¯η:=(1−γ)/(γ+1)2ℓ(α).
Then, L1:=L0−¯ηEremains in the same sublevel set, i.e., L1∈ Sα. Furthermore, we obtain the
following linear decay of the cost value:
J(L1)−J(L∗)≤[1−c1(α)¯η(1−γ)/2] [J(L0)−J(L∗)].
Next, we guarantee that SGD algorithm with this biased estimation of gradient obtains a linear
convergence rate outside a small set Cτaround optimality defined as
Cτ:={L∈ S∥∇J(L)∥F≤s0/τ},
for some τ∈(0,1)and arbitrarily small s0>0. First, we assume access to the following oracle that
provides a biased estimation of the true gradient.
Assumption 3. Suppose, for some α >0, we have access to a biased estimate of the gradient ∇bJ(L)
such that, there exists constants s, s0>0implying ∥∇bJ(L)− ∇J(L)∥F≤s∥∇J(L)∥F+s0for
allL∈ Sα\ Cτ.
Theorem 1 (Convergence) .Suppose Assumption 3 holds with small enough sands0such that
s≤γ/2andSα\ Cγ/2is non-empty for some γ∈(0,1). Then, SGD algorithm starting from any
L0∈ Sα\ Cγ/2with fixed stepsize ¯η:=(1−γ)
(γ+1)2ℓ(α)generates a sequence of policies {Lk}that are
stable (i.e. each Lk∈ Sα) and cost values decay linearly before entering Cγ/2; i.e.,
J(Lk)−J(L∗)≤[1−c1(α)¯η(1−γ)/2]k[J(L0)−J(L∗)],
for each k≥0unless Lj∈ Cγ/2for some j≤k.
Remark 7.By combining Theorem 1 and the PL property of the cost in (11a), we immediately obtain
a sample complexity for our algorithm; e.g., choose γ= 1/2, then in order to guarantee an εerror on
the cost J(Lk)−J(L∗)≤ε, it suffices to have a small enough bias term s0≤√
c1(α)ε
4and variance
coefficient s≤1
4for the oracle, and run the SGD algorithm for k >ln(ε
α)/ln(1−c1(α)
18ℓ(α)))steps.
4.2 Observation model for the estimation problem
Herein, first we show that the estimation error and its differential can be characterized as a “simple
norm” of the concatenated noise (Proposition 3). This norm is induced by a metric that encapsulates
the system dynamics which is explained below. Before proceeding to the results of this section, we
assume that both the process and measurement noise are bounded:
Assumption 4. Assume that (almost surely) ∥x0∥,∥ξ(t)∥ ≤κξand∥ω(t)∥ ≤κωfor all t. Also, for
simplicity, suppose the initial state has zero mean, i.e., m0= 0n.
For two vectors ⃗ v, ⃗ w∈R(T+1)n, we define
⟨⃗ v, ⃗ w⟩AL:= tr [ ⃗ v ⃗ w⊺A⊺
LH⊺HAL],
where AL:= 
A0
LA1
L. . . AT
L
.Also, define ML[E]:= (M0[E]M1[E]··· MT[E])
withM0[E] = 0 ,M1[E] =EH andMi+1[E] =Pi
k=0Ai−k
LEHAk
Lfori= 1,···, T−1.
Proposition 3. The estimation error ε(L,YT)takes the following form
ε(L,YT) =∥⃗ ηL∥2
AL,
where ⃗ ηL:=⃗ξ−(I⊗L)⃗ ωwith ⃗ξ⊺= (ξ(T−1)⊺. . . ξ (0)⊺x(0)⊺)and⃗ ω⊺=
(ω(T−1)⊺. . . ω (0)⊺0⊺
m).Furthermore, its differential acts on small enough E∈Rn×mas,
dε(·,YT)
L(E) =−2⟨⃗ ηL,(I⊗E)⃗ ω⟩AL+ tr [XLNL[E]],
where XL:=⃗ ηL⃗ η⊺
LandNL[E]:=ML[E]⊺H⊺HAL+A⊺
LH⊺HML[E].
8Now, we want to bound the error in the estimated gradient ∇bJT(L)by considering the concentration
error (on length Ttrajectories) and truncation error separately as follows:
∥∇bJT(L)− ∇J(L)∥ ≤ ∥∇ bJT(L)− ∇JT(L)∥+∥∇JT(L)− ∇J(L)∥,
recalling that JT(L) =E[ε(L,YT)]by definition.
Next, we aim to provide the analysis of concentration error on trajectories of length Twith probability
bounds. However, for any pair of real (T×T)-matrices MandN, by Cauchy Schwartz inequality
we obtain that |tr [MN]| ≤ ∥M∥F∥N∥F≤√
T∥M∥∥N∥F.This bound becomes loose (in terms
of dimension T) as the condition number of Nincreases2.
Nonetheless, we are able to provide concentration error bounds that “scale well with respect to the
length T” that hinges upon the following idea: from von Neumann Trace Inequality [19, Theorem
8.7.6] one obtains,
|tr [MN]| ≤PT
i=1σi(M)σi(N)≤ ∥M∥∥N∥∗, (12)
where ∥N∥∗:= trh√
N⊺Ni
=P
iσi(N)is the nuclear norm withσi(N)denoting the i-th largest
singular value of N. Additionally, the same inequality holds for non-square matrices of appropriate
dimension which is tight in terms of dimension.
Proposition 4 (Concentration Error Bound) .Consider Mindependent length Ttrajectories {Yi
T}M
i=1
and suppose Assumption 4 holds. Let ∇bJT(L):=1
MPM
i=1∇ε(L,Yi
T), then for any s >0,
Ph
∥∇bJT(L)− ∇JT(L)∥ ≥si
≤2nexp−Ms2/2
ν2
L+ 2νLs/3
,
where νL:= 4κ2
LC3
L∥H∥2∥H∥∗
[1−p
ρ(AL)]3withκL=κξ+∥L∥κω.
We can also show how the truncation error decays linearly as Tgrows, with constants that are
independent of the system dimension n:
Proposition 5 (Truncation Error Bound) .Under Assumption 4, the truncation error is bounded:
∥∇J(L)− ∇JT(L)∥ ≤¯γLp
ρ(AL)T+1,
where ¯γL:= 10κ4
LC6
L∥H∥2∥H∥∗
[1−ρ(AL)]2.
Remark 8.Notice how the trajectory length Tdetermines the bias in the estimated gradient. However,
the concentration error bound is independent of Tand only depends on the noise bounds proportionate
toκ4
ξ,κ4
ωand the stability margin of ALproportionate to C6
L
(1−p
ρ(AL))6.
Finally, by combining the truncation bound in Proposition 5 with concentration bounds in Proposi-
tion 4 we can provide probabilistic bounds on the “estimated cost” bJT(L)and the “estimated gradient”
∇bJT(L). Its precise statement is deferred to the supplementary materials (Theorem 5).
4.3 Sample complexity of SGD for Kalman Gain
Note that the open-loop system may be unstable. Often in learning literature, it is assumed that the
closed-loop system can be contractible ( i.e., the spectral norm ∥AL∥<1) which is quite convenient
for analysis, however, it is not a reasonable system theoretic assumption. Herein, we emphasize that
we only require the close-loop system to be Schur stable, meaning that ρ(AL)<1; yet, it is very
well possible that the system is not contractible. Handling systems that are merely stable requires
more involved system theoretic tools that are established in the following lemma.
Lemma 6 (Uniform Bounds for Stable Systems) .Suppose L∈ S, then there exit a constants CL>0
such that
∥Ak
L∥ ≤CLp
ρ(AL)k+1,∀k≥0.
Furthermore, consider Sαfor some α > 0, then there exist constants Dα>0,Cα>0and
ρα∈(0,1)such that ∥L∥ ≤Dα, CL≤Cα,andρ(AL)≤ραfor all L∈ Sα.
2The reason is that the first equality is sharp whenever Mis in the “direction” of N, while the second
inequality is sharp whenever condition number of Mis close to one.
9The following result provides sample complexity bounds for this stochastic oracle to provide a biased
estimation of the gradient that satisfies our oracle model of SGD analysis in Assumption 3.
Theorem 2. Under the premise of Proposition 4, consider Sαfor some α >0and choose any s, s0>
0andτ∈(0,1). Suppose the trajectory length T≥ln
¯γαp
min(n, m)/s0.
ln 
1/√ρα
and the
batch size M≥4ν2
αmin(n, m) ln(2 n/δ)
(s s0)2,where ¯γα:= 10( κξ+Dακω)4C6
α∥H∥2∥H∥∗
andνα:= 5C3
α∥H∥2∥H∥∗(κξ+Dακω)2
[1−√ρα]3. Then, with probability no less than 1−δ,
Assumption 3 holds.
Proof. First, note that for any L∈ Sαby Lemma 6 we have that ¯γL≤¯γαandνL≤να. Then,
note that the lower bound on Timplies that ¯γαp
min(n, m)√ραT+1≤s0.The claim then follows
by applying Theorem 5 and noting that for any L̸∈ Cτthe gradient is lowerbound as ∥∇J(L)∥>
s0/τ.
Theorem 3. Consider the linear system (1) under Assumptions 2 and 4. Suppose the SGD algorithm is
implemented with initial stabilizing gain L0, the step-size ¯η:=2
9ℓ(J(L0)), forkiterations, a batch-size
ofM, and data-length T. Then, ∀ε >0and with probability larger than 1−δ,J(Lk)−J(L∗)≤ε
if
T≥O(ln(1
ε)), M ≥O(1
εln(1
δ) ln(ln(1
ε))),and k≥O(ln(1
ε)). (13)
Proof of Theorem 3. Recall that according to Remark 7, in order to obtain εerror on the cost value
we require k≥O(ln(1/ε))of SGD algorithm using gradient estimates with small enough bias
terms0≤√
c1(α)ε
4and variance coefficient s <1
4. Now, we can guarantee Assumption 3 holds
for such s0ands(with probability at least 1−δ) by invoking Theorem 2. In fact, it suffices to
ensure that the length of data trajectories Tand the batch size Mare large enough; specifically,
T≥O(ln(1/s0)) = O(ln(1/ε))and by using union bound for bounding the failure probability
M≥O(ln(1/δ) ln(k)
s2
0) =O(ln(1/δ) ln(ln(1 /ε))
ε).
5 Conclusions, Broader Impact, and Limitations
In this work, we considered the problem of learning the optimal (steady-state) Kalman gain for linear
systems with unknown process and measurement noise covariances. Our approach builds on the
duality between optimal control and estimation, resulting in a direct stochastic Policy Optimization
(PO) algorithm for learning the optimal filter gain. We also provided convergence guarantees and
finite sample complexity with bias and variance error bounds that scale well with problem parameters.
In particular, the variance is independent of the length of data trajectories and scales logarithmically
with problem dimension, and bias term decreases exponentially with the length.
This work contributes a generic optimization algorithm and introduces a filtering strategy for estimat-
ing dynamical system states. While theoretical, it raises privacy concerns similar to the model-based
Kalman filter. Limitations include the need for prior knowledge of system parameters; nonetheless,
parameter uncertainties can be treated practically as process and measurement noise. Finally, sample
complexities depend on the stability margin 1−p
ρ(AL), inherent to the system generating the data.
Finally, a direction for future research is to study how to adapt the proposed algorithm and error
analysis for the setting when a single long trajectory is available as opposed to several independent
trajectories of finite length. Another interesting direction is to carry out a robustness analysis, similar
to its LQR dual counterpart, to study the effect of the error in system parameters on the policy learning
accuracy. Of course, the ultimate research goal is to use the recently introduced duality in nonlinear
filtering [23] as a bridge to bring tools from learning to nonlinear filtering.
Acknowledgments and Disclosure of Funding
The research of A. Taghvaei has been supported by NSF grant EPCN-2318977; S. Talebi and M.
Mesbahi have been supported by NSF grant ECCS-2149470 and AFOSR grant FA9550-20-1-0053.
The authors are also grateful for constructive feedback from the reviewers.
10References
[1]Bernt M Åkesson, John Bagterp Jørgensen, Niels Kjølstad Poulsen, and Sten Bay Jørgensen. A
generalized autocovariance least-squares method for Kalman filter tuning. Journal of Process
Control , 18(7-8):769–779, 2008.
[2] Karl J Åström. Introduction to Stochastic Control Theory . Courier Corporation, 2012.
[3]Amir Beck. First-Order Methods in Optimization . Society for Industrial and Applied Mathe-
matics, Philadelphia, PA, 2017.
[4]Pierre R Belanger. Estimation of noise covariance matrices for a linear time-varying stochastic
process. Automatica , 10(3):267–275, 1974.
[5] Alain Bensoussan. Estimation and control of dynamical systems , volume 48. Springer, 2018.
[6]Jingjing Bu, Afshin Mesbahi, Maryam Fazel, and Mehran Mesbahi. LQR through the lens of
first order methods: Discrete-time case. arXiv preprint arXiv:1907.08921 , 2019.
[7]Jingjing Bu, Afshin Mesbahi, and Mehran Mesbahi. On topological and metrical properties of
stabilizing feedback gains: The MIMO case. arXiv preprint arXiv:1904.02737 , 2019.
[8]Jingjing Bu, Afshin Mesbahi, and Mehran Mesbahi. Policy gradient-based algorithms for
continuous-time linear quadratic control. arXiv preprint arXiv:2006.09178 , 2020.
[9]Burian Carew and P Belanger. Identification of optimum filter steady-state gain for systems
with unknown noise covariances. IEEE Transactions on Automatic Control , 18(6):582–587,
1973.
[10] Ci Chen and Anthony Holohan. Stability robustness of linear quadratic regulators. International
Journal of Robust and Nonlinear Control , 26(9):1817–1824, 2016.
[11] Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal
Talwar. Online linear quadratic control. In Jennifer Dy and Andreas Krause, editors, Proceedings
of the 35th International Conference on Machine Learning , volume 80 of Proceedings of
Machine Learning Research , pages 1029–1038. PMLR, 10–15 Jul 2018.
[12] Jindrich Dunik, Miroslav Simandl, and Ondrej Straka. Methods for estimating state and
measurement noise covariance matrices: Aspects and comparison. IFAC Proceedings Volumes ,
42(10):372–377, 2009.
[13] Ilyas Fatkhullin and Boris Polyak. Optimizing static linear feedback: Gradient method. SIAM
Journal on Control and Optimization , 59(5):3887–3911, 2021.
[14] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. In Proceedings of the 35th International
Conference on Machine Learning , volume 80, pages 1467–1476. PMLR, 2018.
[15] Wendell H Fleming and Sanjoy K Mitter. Optimal control and nonlinear filtering for nondegen-
erate diffusion processes. Stochastics: An International Journal of Probability and Stochastic
Processes , 8(1):63–77, 1982.
[16] Nicholas J Higham. Functions of Matrices: Theory and Computation . SIAM, 2008.
[17] Charles G Hilborn and Demetrios G Lainiotis. Optimal estimation in the presence of unknown
parameters. IEEE Transactions on Systems Science and Cybernetics , 5(1):38–43, 1969.
[18] Kimberly A Hinson, Kristi A Morgansen, and Eli Livne. Autocovariance least squares noise
covariance estimation for a gust load alleviation test-bed. In AIAA SCITECH 2022 Forum , page
0617, 2022.
[19] Roger A Horn and Charles R Johnson. Matrix Analysis . Cambridge University Press, 2012.
[20] R. E. Kalman. A new approach to linear filtering and prediction problems. ASME. Journal of
Basic Engineering , 82(1):35–45, 03 1960.
[21] Rudolf E Kalman. On the general theory of control systems. In Proceedings First International
Conference on Automatic Control, Moscow, USSR , pages 481–492, 1960.
[22] R Kashyap. Maximum likelihood identification of stochastic linear systems. IEEE Transactions
on Automatic Control , 15(1):25–34, 1970.
[23] Jin Won Kim. Duality for nonlinear filtering. arXiv preprint arXiv:2207.07709 , 2022.
11[24] Jin-Won Kim, Prashant G Mehta, and Sean P Meyn. What is the lagrangian for nonlinear
filtering? In 2019 IEEE 58th Conference on Decision and Control (CDC) , pages 1607–1614.
IEEE, 2019.
[25] Huibert Kwakernaak and Raphael Sivan. Linear Optimal Control Systems , volume 1072.
Wiley-interscience, 1969.
[26] Sahin Lale, Kamyar Azizzadenesheli, Babak Hassibi, and Anima Anandkumar. Logarithmic
regret bound in partially observable linear dynamical systems. Advances in Neural Information
Processing Systems , 33:20876–20888, 2020.
[27] Frank Lewis. Optimal Estimation with an Introduction to Stochastic Control Theory . New York,
Wiley-Interscience, 1986.
[28] Wenjie Liu, Jian Sun, Gang Wang, Francesco Bullo, and Jie Chen. Learning robust data-based
LQG controllers from noisy data. arXiv preprint arXiv:2305.01417 , 2023.
[29] D Magill. Optimal adaptive estimation of sampled stochastic processes. IEEE Transactions on
Automatic Control , 10(4):434–439, 1965.
[30] Peter Matisko and Vladimír Havlena. Noise covariances estimation for Kalman filter tuning.
IFAC Proceedings Volumes , 43(10):31–36, 2010.
[31] Raman Mehra. On the identification of variances and adaptive Kalman filtering. IEEE Transac-
tions on Automatic Control , 15(2):175–184, 1970.
[32] Raman Mehra. Approaches to adaptive filtering. IEEE Transactions on Automatic Control ,
17(5):693–698, 1972.
[33] Hesameddin Mohammadi, Mahdi Soltanolkotabi, and Mihailo R. Jovanovic. On the linear
convergence of random search for discrete-time LQR. IEEE Control Systems Letters , 5(3):989–
994, 2021.
[34] Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R Jovanovi ´c. Con-
vergence and sample complexity of gradient methods for the model-free linear–quadratic
regulator problem. IEEE Transactions on Automatic Control , 67(5):2435–2450, 2021.
[35] RE Mortensen. Maximum-likelihood recursive nonlinear filtering. Journal of Optimization
Theory and Applications , 2(6):386–394, 1968.
[36] Kenneth Myers and BD Tapley. Adaptive sequential estimation with unknown noise statistics.
IEEE Transactions on Automatic Control , 21(4):520–523, 1976.
[37] Brian J Odelson, Alexander Lutz, and James B Rawlings. The autocovariance least-squares
method for estimating covariances: Application to model-based control of chemical reactors.
IEEE Transactions on Control Systems Technology , 14(3):532–540, 2006.
[38] Brian J Odelson, Murali R Rajamani, and James B Rawlings. A new autocovariance least-
squares method for estimating noise covariances. Automatica , 42(2):303–308, 2006.
[39] JD Pearson. On the duality between estimation and control. SIAM Journal on Control , 4(4):594–
600, 1966.
[40] MG Safonov and WEIZHENG Wang. Singular value properties of LQ regulators. IEEE
Transactions on Automatic Control , 37(8):1210–1211, 1992.
[41] Robert H Shumway and David S Stoffer. An approach to time series smoothing and forecasting
using the EM algorithm. Journal of Time Series Analysis , 3(4):253–264, 1982.
[42] Koji Tajima. Estimation of steady-state Kalman filter gain. IEEE Transactions on Automatic
Control , 23(5):944–945, 1978.
[43] Shahriar Talebi and Mehran Mesbahi. Policy optimization over submanifolds for constrained
feedback synthesis. IEEE Transactions on Automatic Control (to appear), arXiv preprint
arXiv:2201.11157 , 2022.
[44] Shahriar Talebi, Amirhossein Taghvaei, and Mehran Mesbahi. SGD for Filtering, October 2023.
Available on GitHub at https://github.com/shahriarta/sgd4filtering .
[45] Yujie Tang, Yang Zheng, and Na Li. Analysis of the optimization landscape of linear quadratic
gaussian (LQG) control. In Proceedings of the 3rd Conference on Learning for Dynamics and
Control , volume 144, pages 599–610. PMLR, June 2021.
12[46] Joel A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends ®
in Machine Learning , 8(1-2):1–230, 2015.
[47] Anastasios Tsiamis and George J Pappas. Finite sample analysis of stochastic system identifica-
tion. In 2019 IEEE 58th Conference on Decision and Control (CDC) , pages 3648–3654. IEEE,
2019.
[48] Anastasios Tsiamis and George J. Pappas. Online learning of the Kalman filter with logarithmic
regret. IEEE Transactions on Automatic Control , 68(5):2774–2789, 2023.
[49] Anastasios Tsiamis, Ingvar Ziemann, Nikolai Matni, and George J Pappas. Statistical learning
theory for control: A finite sample perspective. arXiv preprint arXiv:2209.05423 , 2022.
[50] Jack Umenberger, Max Simchowitz, Juan Perdomo, Kaiqing Zhang, and Russ Tedrake. Globally
convergent policy search for output estimation. In S. Koyejo, S. Mohamed, A. Agarwal,
D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems ,
volume 35, pages 22778–22790. Curran Associates, Inc., 2022.
[51] Jie Xiong. An Introduction to Stochastic Filtering Theory , volume 18. OUP Oxford, 2008.
[52] Lingyi Zhang, David Sidoti, Adam Bienkowski, Krishna R Pattipati, Yaakov Bar-Shalom, and
David L Kleinman. On the identification of noise covariances and adaptive Kalman filtering: A
new look at a 50 year-old problem. IEEE Access , 8:59362–59388, 2020.
[53] Xiangyuan Zhang, Bin Hu, and Tamer Ba¸ sar. Learning the Kalman filter with fine-grained
sample complexity. In 2023 American Control Conference (ACC) , pages 4549–4554, 2023.
[54] Feiran Zhao, Keyou You, and Tamer Ba¸ sar. Global convergence of policy gradient primal-dual
methods for risk-constrained LQRs. arXiv preprint arXiv:2104.04901 , 2021.
[55] Yang Zheng, Luca Furieri, Maryam Kamgarpour, and Na Li. Sample complexity of linear
quadratic gaussian (LQG) control for output feedback systems. In Ali Jadbabaie, John Lygeros,
George J. Pappas, Pablo A.;Parrilo, Benjamin Recht, Claire J. Tomlin, and Melanie N. Zeilinger,
editors, Proceedings of the 3rd Conference on Learning for Dynamics and Control , volume 144
ofProceedings of Machine Learning Research , pages 559–570. PMLR, 07 – 08 June 2021.
13Contents
1 Introduction 1
2 Background and Problem Formulation 2
2.1 Learning problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Stochastic optimization formulation . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 Estimation-Control Duality Relationship 4
3.1 Duality in steady-state regime . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Optimization landscape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4 SGD for Learning the Kalman Gain 6
4.1 SGD with biased gradient and stability constraint . . . . . . . . . . . . . . . . . . 7
4.2 Observation model for the estimation problem . . . . . . . . . . . . . . . . . . . . 8
4.3 Sample complexity of SGD for Kalman Gain . . . . . . . . . . . . . . . . . . . . 9
5 Conclusions, Broader Impact, and Limitations 10
A Discussion of additional related works 15
B Summary of Assumptions 15
C Proof of the duality relationship: Proposition 1 16
D Proofs for the results for the analysis of the optimization landscape 17
D.1 Preliminary lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
D.2 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
D.3 Derivation of the gradient formula in Lemma 2 . . . . . . . . . . . . . . . . . . . 18
D.4 Proof of existence of global minimizer in Lemma 2 . . . . . . . . . . . . . . . . . 19
D.5 Proof of gradient dominance property in Lemma 2 . . . . . . . . . . . . . . . . . . 19
D.6 Proof of Lipschitz property in Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . 21
D.7 Gradient Flow (GF) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.8 Gradient Descent (GD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
E Proofs for the analysis of the constrained SGD algorithm 24
E.1 Proof of lemma 3: Derivation of stochastic gradient formula . . . . . . . . . . . . 24
E.2 Proof of Lemma 4: Robustness of the policy with respect to perturbation . . . . . . 24
E.3 Proof of Lemma 5: Uniform lower-bound on stepsize . . . . . . . . . . . . . . . . 24
E.4 Proof of Proposition 2: Linear decay in cost value . . . . . . . . . . . . . . . . . . 25
E.5 Proof of Theorem 1: Convergence of SGD algorithm . . . . . . . . . . . . . . . . 26
F Proofs of the result for observation model and sample complexity 27
F.1 Preliminary lemmas and their proofs . . . . . . . . . . . . . . . . . . . . . . . . . 27
14F.2 Proof of Proposition 3: The Observation model . . . . . . . . . . . . . . . . . . . 30
F.3 Proof of Proposition 4: Concentration bounds . . . . . . . . . . . . . . . . . . . . 30
F.4 Proof of the Proposition 5: Truncation error bound . . . . . . . . . . . . . . . . . 33
F.5 Complete version of Theorem 2: Sample complexity bounds for the stochastic oracle 34
F.6 Additional concentration bound results . . . . . . . . . . . . . . . . . . . . . . . . 35
G Numerical Results 37
H Nomenclature 37
Appendix A Discussion of additional related works
After the initial submission of this manuscript, we encountered the following related works, and we
will now briefly discuss their connections and differences in relation to our work: [55] establishes
an end-to-end sample complexity bound on learning a robust LQG controller (which is different
than our filtering problem). As the system parameters are also unknown, this work only considers
theopen-loop stable systems. Additionally, it establishes a nice trade-off between optimality and
robustness characterized by a suboptimality performance bound as a result of the robust LQG
design. While our filtering design is based on the knowledge of system parameters, we do not
consider the open-loop stability assumption; and the robust synthesis is included as one of the
main future directions of this work. Furthermore, the complexity bounds in [55] depends on the
length of trajectory, whereas ours does not. Also, their suboptimality bound scale as O(1/√
N)
in the number of trajectories, whereas ours scales as O(1/N). Next, [53] similarly considers the
problem of learning the steady-state Kalman gain but in a different setup: The model is assumed
to be completely unknown. However, the algorithm requires access to a simulator that provides
noisy measurement of the MSE E[∥X(t)−ˆX(t)∥2]which requires generation of ground-truth state
trajectories X(t)(see Assumption 3.2 therein). The proposed approaches are different: zeroth-order
global optimization (theirs) vs first-order stochastic gradient descent (ours). The analysis reports
aO(ε−2)sample complexity on optimal policy error which aligns with our result. However, it is
difficult to provide more detailed comparison as explicit dependence of the error terms on problem
dimension is not provided. Finally, [28] considers the problem of simultaneously learning the optimal
Kalman filter and linear feedback control policy in the LQG setup. Their approach involves solving
Semidefinite Programmings (SDP) using input-state-output trajectories. Their result, for the the case
when trajectories involve noise, relies on the assumption that the magnitude of the noise can be made
arbitrary small. This is in contrast to our setup where we only assume a bound on the noise level and
do not require access to state trajectories.
Appendix B Summary of Assumptions
Herein, we provide a summary of all assumptions considered in our work and their relation to our
results.
Note that if the given pair (A, H)of system parameters is not detectable, then the estimation problem
is not well-posed, simply because there may not exist any stabilizing policy L0. Therefore, we
consider the minimum required assumption for well-posedness of the problem as follows:
Assumption 1. The pair (A, H)is detectable, and the pair (A, Q1
2)is stabilizable, where Q1
2is the
unique positive semidefinite square root of Q.
It is indeed possible to work with the weaker conditions compared with Assumption 1 and obtain
similar results presented here—by incorporating more system theoretic tools. However, in order to
improve the clarity of the learning problem with less system theoretic technicalities, we work with
the following stronger assumption in lieu of Assumption 1.
Assumption 2. The pair (A, H)is observable, and the noise covariances Q≻0andR≻0.
It is clear that Assumption 2 implies Assumption 1.
15Next, in order to provide an independent analysis of the SGD algorithm for locally Lipschitz functions
in presence of gradient bias, we consider the following assumption on the gradient oracle.
Assumption 3. Suppose, for some α >0, we have access to a biased estimate of the gradient ∇bJ(L)
such that, there exists constants s, s0>0implying ∥∇bJ(L)− ∇J(L)∥F≤s∥∇J(L)∥F+s0for
allL∈ Sα\ Cτ.
Next, in order to utilize the complexity analysis of SGD algorithm provided in Theorem 1, we need
to build an oracle that satisfies Assumption 3. This is oracle is built in Section 4.2, building on the
following assumptions on the process and observation noise.
Assumption 4. Assume that (almost surely) ∥x0∥,∥ξ(t)∥ ≤κξand∥ω(t)∥ ≤κωfor all t. Also, for
simplicity, suppose the initial state has zero mean, i.e., m0= 0n.
Then, based on this assumption, Theorem 2 provides lowerbounds on the trajectory length and the
batch-size in order for the oracle to satisfy Assumption 3. Finally, the combined version of our results
is presented in Theorem 3.
Appendix C Proof of the duality relationship: Proposition 1
1. By pairing the original state dynamics (1) and its dual (6):
z(t+ 1)⊺x(t+ 1)−z(t)⊺x(t) =z(t+ 1)⊺ξ(t) +u(t+ 1)⊺Hx(t).
Summing this relationship from t= 0tot=T−1yields,
z(T)⊺x(T) =z(0)⊺x(0) +T−1X
t=0z(t+ 1)⊺ξ(t) +u(t+ 1)⊺Hx(t).
Upon subtracting the estimate a⊺ˆxL(T), using the adjoint relationship
T−1X
t=0u(t+ 1)⊺y(t) =a⊺ˆxL(T). (14)
andz(T) =a, lead to
a⊺x(T)−a⊺ˆxL(T) =z(0)⊺x(0) +PT−1
t=0z(t+ 1)⊺ξ(t)−u(t+ 1)⊺w(t).
Squaring both sides and taking the expectation concludes the duality result in (8).
2. Consider the adjoint system (6) with the linear feedback law u(t) =L⊺z(t). Then,
z(t) = (A⊺
L)T−ta, for t= 0,1, . . . , T. (15)
Therefore, as a function of a,u(t) =L⊺(A⊺
L)T−ta. These relationships are used to identify the
control policy
L†(a) = (u(1), . . . , u (T)) = ( L⊺(A⊺
L)T−1a, . . . , L⊺a).
This control policy corresponds to an estimation policy by the adjoint relationship (14):
a⊺ˆxL(T) =T−1X
t=0a⊺AT−t−1
L Ly(t),∀a∈Rn.
This relationship holds for all a∈Rn. Therefore,
ˆxL(T) =T−1X
t=0AT−t−1
L Ly(t),
which coincides with the Kalman filter estimate with constant gain Lgiven by the formula (4)(with
m0= 0). Therefore, the adjoint relationship (14) relates the control policy with constant gain L⊺to
the Kalman filter with constant gain L. The result (9) follows from the identity
E
∥y(T)−ˆyL(T)∥2
=E
∥Hx(T)−HˆxL(T)∥2+∥w(T)∥2
=Pm
i=1E
|H⊺
ix(T)−H⊺
iˆxL(T)|2
+ tr [ R],
and the application of the first result (9) with a=Hi.
16Appendix D Proofs for the results for the analysis of the optimization
landscape
D.1 Preliminary lemma
The following lemmas are a direct consequence of duality and useful for our subsequent analysis.
Lemma 7. The set of Schur stabilizing gains Sis regular open, contractible, and unbounded when
m≥2and the boundary ∂Scoincides with the set {L∈Rn×m:ρ(A−LH) = 1}. Furthermore,
J(.)is real analytic on S.
Proof. Consider the duality described in Remark 3. The proof then follows identical to [6, Lemmas
3.5 and 3.6] by noting that the spectrum of a matrix is identical to the spectrum of its transpose.
Next, we present the proof of Lemma 1 that provides sufficient conditions to recover the coercive
property of J(.)which resembles Lemma 3.7 in [6] (but extended for the time-varying parameters).
The proof is removed on the account of space and appears in the extended version of this work.
Remark 9.This approach recovers the claimed coercivity also in the control setting with weaker
assumptions. In particular, using this result, one can replace the positive definite condition on the
covariance of the initial condition in [6], i.e., Σ≻0, with just the controllability of (A,Σ1/2).
D.2 Proof of Lemma 1
Proof. Consider any L∈ S and note that the right eigenvectors of AandALthat are annihilated by
Hare identical. Thus, by Popov-Belevitch-Hautus ( PBH ) test, observability of (A, H)is equivalent
to observability of (AL, H). Therefore, there exists a positive integer n0≤nsuch that
H⊺
n0(L) := 
H⊺A⊺
LH⊺. . . (A⊺
L)n0−1H⊺
is full-rank, implying that H⊺
n0(L)Hn0(L)is positive definite. Now, recall that for any such stabilizing
gainL, we compute
J(L) = tr"∞X
t=0(AL)t(Q+LRL⊺)(A⊺
L)tH⊺H#
= tr"∞X
t=0n0−1X
k=0(AL)n0t+k(Q+LRL⊺)(A⊺
L)n0t+kH⊺H#
= tr"∞X
t=0(AL)n0t(Q+LRL⊺)(A⊺
L)n0tH⊺
n0(L)Hn0(L)#
=:tr
Xn0(L)H⊺
n0(L)Hn0(L)
,
where we used the cyclic property of trace and the inequality follows because for any PSD matrices
P1, P2⪰0we have
tr [P1P2] = trh
P1
2
2P1P1
2
2i
≥0. (16)
Also, Xn0(L)is well defined because ALis Schur stable if and only if (AL)n0is. Moreover, Xn0(L)
coincides with the unique solution to the following Lyapunov equation
Xn0(L) = (AL)n0Xn0(L)(A⊺
L)n0+Q+LRL⊺.
Next, as R⪰0,
J(L)≥λ(H⊺
n0(L)Hn0(L))tr [Xn0(L)]
≥λ(H⊺
n0(L)Hn0(L))tr"∞X
t=0(AL)n0tQ(A⊺
L)n0t#
≥λ(H⊺
n0(L)Hn0(L))λ(Q)∞X
t=0tr
(A⊺
L)n0t(AL)n0t
≥λ(H⊺
n0(L)Hn0(L))λ(Q)∞X
t=0ρ(AL)2n0t, (17)
17where the last inequality follows by the fact that
tr
(A⊺
L)n0t(AL)n0t
=∥(AL)n0t∥2
F≥ ∥(AL)n0t∥2
op≥ρ((AL)n0t)2=ρ(AL)2n0t,
with∥ · ∥ opdenoting the operator norm induced by 2-norm. Now, by Lemma 7 and continuity of
the spectral radius, as Lk→∂Swe observe that ρ(ALk)→1. But then, the obtained lowerbound
implies that J(Lk)→ ∞ . On the other hand, as Q≻0,R≻0are both time-independent, by using
a similar technique we also provide the following lowerbound
J(L)≥tr"
(Q+LRL⊺)∞X
t=0(A⊺
L)n0tH⊺
n0(L)Hn0(L)(AL)n0t#
≥λ(H⊺
n0(L)Hn0(L))tr [Q+LRL⊺]
≥λ(H⊺
n0(L)Hn0(L))tr [RLL⊺]
≥λ(H⊺
n0(L)Hn0(L))λ(R)∥L∥2
F,
where ∥ · ∥Fdenotes the Frobenius norm. Therefore, by equivalency of norms on finite dimensional
spaces, ∥Lk∥ → ∞ implies that J(Lk)→ ∞ which concludes that J(.)is coercive on S. Finally,
note that for any L̸∈ S, by (17) we can argue that J(L) =∞, therefore the sublevel sets Sα⊂ S
whenever αis finite. The compactness of Sαis then a direct consequence of the coercive property
and continuity of J(.)(Lemma 7).
D.3 Derivation of the gradient formula in Lemma 2
Next, we aim to compute the gradient of Jin a more general format. We do the derivation for
time-varying RandQwith specialization to time-invariant setting at the end. For any admissible ∆,
we have
X∞(L+ ∆)−X∞(L) =∞X
t=1(AL)t(Qt+LRtL⊺) (⋆)⊺+ (⋆) (Qt+LRtL⊺) (A⊺
L)t
−∞X
t=0(AL)t(∆RtL⊺+LRt∆⊺) (A⊺
L)t+o(∥∆∥),
where the ⋆is hiding the following term
tX
k=1(AL)t−k∆H(AL)k−1.
Therefore, by linearity and cyclic permutation property of trace, we get that
J(L+ ∆)−J(L) =tr"
∆H∞X
t=1tX
k=12 (AL)k−1(Qt+LRtL⊺) (A⊺
L)tH⊺H(AL)t−k#
−tr"
∆∞X
t=02RtL⊺(A⊺
L)tH⊺H(AL)t#
+o(∥∆∥).
Finally, by considering the Euclidean metric on real matrices induced by the inner product ⟨Q, P⟩=
tr [Q⊺P], we obtain the gradient of Jas follows
∇J(L) =−2∞X
t=0(A⊺
L)tH⊺H(AL)tLRt
+2∞X
t=1tX
k=1(A⊺
L)t−kH⊺H(AL)t(Qt+LRtL⊺) (A⊺
L)k−1H⊺,
18whenever the series are convergent! And, by switching the order of the sums it simplifies to
∇J(L) =−2∞X
t=0(A⊺
L)tH⊺H(AL)tLRt
+ 2∞X
k=1∞X
t=kh
(A⊺
L)t−kH⊺H(AL)t−ki
ALh
(AL)k−1(Qt+LRtL⊺) (A⊺
L)k−1i
H⊺.
=−2∞X
t=0(A⊺
L)tH⊺H(AL)tLRt+ 2∞X
t=0h
(A⊺
L)tH⊺H(AL)ti
·AL"∞X
k=0(AL)k(Qt+k+1+LRt+k+1L⊺) (A⊺
L)k#
H⊺.
For the case of time-independent QandR, this reduces to
∇J(L) =−2Y(L)LR+ 2Y(L)AL"∞X
k=0(AL)k(Q+LRL⊺) (A⊺
L)k#
H⊺
=2Y(L)
−LR+ALX(L)H⊺
.
where Y(L)=Yis the unique solution of
Y=A⊺
LY AL+H⊺H.
D.4 Proof of existence of global minimizer in Lemma 2
The domain Sis non-empty whenever (A, H)is observable. Thus, by continuity of L→J(L), there
exists some finite α >0such that the sublevel set Sαis non-empty and compact. Therefore, the
minimizer is an interior point and thus must satisfy the first-order optimality condition ∇J(L∗) = 0 .
Therefore, by coercivity, it is stabilizing and unique which satisfies
L∗=AX∗H⊺(R+HX∗H⊺)−1,
withX∗being the unique solution of
X∗=AL∗X∗A⊺
L∗+Q+L∗R(L∗)⊺. (18)
As expected, the global minimizer L∗is equal to the steady-state Kalman gain, but explicitly
dependent on the noise covariances QandR.
D.5 Proof of gradient dominance property in Lemma 2
Note that X=X(L)satisfies
X=ALXA⊺
L+Q+LRL⊺. (19)
Then, by combining (18) and (19), and some algebraic manipulation, we recover part of the gradient
information, i.e., (−LR+ALXH⊺), in the gap of cost matrices by arriving at the following identity
X−X∗−AL∗(X−X∗)A⊺
L∗
=(LR−ALXH⊺)(L−L∗)⊺+ (L−L∗)(RL⊺−HXA⊺
L)
−(L−L∗)R(L−L∗)⊺−(L−L∗)HXH⊺(L−L∗)⊺
⪯1
a(LR−ALXH⊺)(RL⊺−HXA⊺
L) +a(L−L∗)(L−L∗)⊺
−(L−L∗)(R+HXH⊺)(L−L∗)⊺(20)
where the upperbound is valid for any choice of a >0. Now, as R≻0, we choose a=λ(R)/2. As
X⪰0, it further upperbounds
X−X∗−AL∗(X−X∗)A⊺
L∗
⪯2
λ(R)(−LR+ALXH⊺)(−RL⊺+HXA⊺
L)
−λ(R)
2(L−L∗)(L−L∗)⊺.
19Now, let ˜XandbXbe, respectively, the unique solution of the following Lyapunov equations
˜X=AL∗˜XA⊺
L∗+ (−LR+ALXH⊺)(−RL⊺+HXA⊺
L),
bX=AL∗bXA⊺
L∗+ (L−L∗)(L−L∗)⊺.
Then by comparison, we conclude that
X−X∗⪯2
λ(R)˜X−λ(R)
2bX.
Recall that by the fact in (16),
J(L)−J(L∗) = tr [( X−X∗)H⊺H]≤2
λ(R)trh
˜XH⊺Hi
−λ(R)
2trh
bXH⊺Hi
.(21)
LetY∗≻0be the unique solution of
Y∗=A⊺
L∗Y∗AL∗+H⊺H,
then, by cyclic permutation property
trh
˜XH⊺Hi
= tr [( −LR+ALXH⊺)(−RL⊺+HXA⊺
L)Y∗]
≤λ(Y∗)
λ2(Y(L))tr
Y(L)(−LR+ALXH⊺)(−RL⊺+HXA⊺
L)Y(L)
=λ(Y∗)
4λ2(Y(L))⟨∇J(L),∇J(L)⟩ (22)
where the last equality follows by the obtained formula for the gradient ∇J(L). Similarly, we obtain
that
trh
bXH⊺Hi
= tr [( L−L∗)(L−L∗)⊺Y∗]≥λ(Y∗)∥L−L∗∥2
F. (23)
Notice that the mapping L→Y(L)is continuous on S ⊃ S α, and also by observability of (A, H),
Y(L)≻0for any L∈ S. To see this, let Hn0(L)≻0be as defined in Lemma 1. Then,
Y(L)=∞X
t=0(A⊺
L)t(H⊺H)(AL)t
=∞X
t=0n0−1X
k=0(A⊺
L)n0t+k(H⊺H)(AL)n0t+k
=∞X
t=0(A⊺
L)n0tH⊺
n0(L)Hn0(L)(AL)n0t
⪰H⊺
n0(L)Hn0(L)≻0.
Now, by Lemma 1, Sαis compact and therefore we claim that the following infimum is attained with
some positive value κα:
inf
L∈Sαλ(Y(L)) =:κα>0. (24)
Finally, the first claimed inequality follows by combining the inequalities (21), (22) and (23), with
the following choice of
c1(α):=2λ(R)
λ(Y∗)κ2
α,and c2(α):=λ(Y∗)λ(R)2
λ(Y∗)κ2
α.
For the second claimed inequality, one arrive at the following identity by similar computation to (20):
X−X∗−AL(X−X∗)A⊺
L
=(L∗R−AL∗X∗H⊺)(L−L∗)⊺+ (L−L∗)(RL∗⊺−HX∗A⊺
L∗)
+ (L−L∗)R(L−L∗)⊺+ (L−L∗)HX∗H⊺(L−L∗)⊺
=(L−L∗)(R+HX∗H⊺)(L−L∗)⊺
20where the second equality follows because Y(L)≻0and thus
L∗R−AL∗X∗H⊺=−Y−1
(L)∇J(L∗) = 0 .
Recall that
J(L)−J(L∗) = tr [( X−X∗)H⊺H],
then by the equality in (20) and cyclic property of trace we obtain
J(L)−J(L∗) = tr
ZY(L)
,
where
Z:=(L−L∗)(R+HX∗H⊺)(L−L∗)⊺
⪰λ(R)(L−L∗)(L−L∗)⊺.
Therefore, for any L∈ Sα, we have
J(L)−J(L∗)≥λ(Y(L))tr [Z]≥λ(R)κα∥L−L∗∥2
F,
and thus, we complete the proof of first part by the following choice of
c3(α) =λ(R)κα.
D.6 Proof of Lipschitz property in Lemma 2
Next, we provide the proof of locally Lipschitz property. Notice that the mappings L→X(L),
L→Y(L)andL→ALare all real-analytic on the open set S ⊃ S α, and thus so is the mapping
L→ ∇J(L) = 2 Y(L)
−LR+ALX(L)H⊺
. Also, by Lemma 1, Sαis compact and therefore the
mapping L→ ∇ J(L)isℓ-Lipschitz continuous on Sαfor some ℓ=ℓ(α)>0. In the rest of the
proof, we attempt to characterize ℓ(α)in terms of the problem parameters. By direct computation we
obtain
∇J(L1)− ∇J(L2) = (2 Y(L1)−2Y(L2))
−L1R+AL1X(L1)H⊺
+ 2Y(L2) 
−L1R+AL1X(L1)H⊺
−
−L2R+AL2X(L2)H⊺
= 2(Y(L1)−Y(L2))
−L1(R+HX (L1)H⊺) +AX(L1)H⊺
+ 2Y(L2)
(L2−L1)(R+HX (L1)H⊺) +AL2(X(L1)−X(L2))H⊺
.
Therefore,
∥∇J(L1)− ∇J(L2)∥2
F≤ℓ2
1∥Y(L1)−Y(L2)∥2
F+ℓ2
2∥L1−L2∥2
F+ℓ2
3∥X(L1)−X(L2)∥2
F(25)
where
ℓ1=ℓ1(L1):= 2∥ −L1(R+HX (L1)H⊺) +AX(L1)H⊺∥op,
ℓ2=ℓ2(L1, L2):= 2∥Y(L2)∥op∥R+HX (L1)H⊺∥op,
ℓ3=ℓ3(L2):= 2∥Y(L2)∥op∥AL2∥op∥H⊺∥op.
On the other hand, by direct computation we obtain
Y(L1)−Y(L2)−A⊺
L1(Y(L1)−Y(L2))AL1
=(L2−L1)⊺H⊺Y(L2)AL2+A⊺
L2Y(L2)H(L2−L1)
+ (L1−L2)⊺H⊺Y(L2)H(L1−L2)
⪯∥L1−L2∥Fℓ4I(26)
where
ℓ4=ℓ4(L1, L2):= 2∥H⊺Y(L2)AL2∥op+∥H⊺Y(L2)H(L1−L2)∥op.
Now, consider the mapping L→Z(L)where Z(L)=Zis the unique solution of the following
Lyapunov equation:
Z=A⊺
LZAL+I,
which is well-defined and continuous on S ⊃ S α. Therefore, by comparison, we claim that
∥Y(L1)−Y(L2)∥F⪯ ∥L1−L2∥Fℓ4∥Z(L1)∥F.
21By a similar computation to that of (20), we obtain that
X(L1)−X(L2)−AL2(X(L1)−X(L2))A⊺
L2
=(L1R−AL1X(L1)H⊺)(L1−L2)⊺
+ (L1−L2)(RL⊺
1−HX (L1)A⊺
L1)
−(L1−L2)R(L1−L2)⊺
−(L1−L2)HX (L1)H⊺(L1−L2)⊺
⪯∥L1−L2∥Fℓ5(Q+L2RL⊺
2)(27)
where
ℓ5=ℓ5(L1):= 2∥ −L1R+AL1X(L1)H⊺∥op/ λ(Q).
Therefore, by comparison, we claim that
∥X(L1)−X(L2)∥F⪯ ∥L1−L2∥Fℓ5∥X(L2)∥F.
Finally, by compactness of Sα, we claim that the following supremums are attained and thus, are
achieved with some finite positive values:
¯ℓ1(α):= sup
L1,L2∈Sαℓ1(L1)ℓ4(L1, L2)∥Z(L1)∥F,
¯ℓ2(α):= sup
L1,L2∈Sαℓ2(L1, L2),
¯ℓ3(α):= sup
L1,L2∈Sαℓ3(L2)ℓ5(L1)∥X(L2)∥F.
Then, the claim follows by combining the bound in (25) with (26) and (27), and the following choice
of
ℓ(α):=q
¯ℓ2
1(α) +¯ℓ2
2(α) +¯ℓ2
3(α).
D.7 Gradient Flow (GF)
For completeness, in the next two sections, we anlayze first-order methods in order to solve the
minimization problem (10), although they are not part of the main paper. In this section, we consider
a policy update according to the the GF dynamics:
[GF] ˙Ls=−∇J(Ls).
We summarize the convergence result in the following proposition which is a direct consequence of
Lemma 2.
Proposition 6. Consider any sublevel set Sαfor some α >0. Then, for any initial policy L0∈ Sα,
theGFupdates converges to optimality at a linear rate of exp(−c1(α))(in both the function value
and the policy iterate). In particular, we have
J(Ls)−J(L∗)≤(α−J(L∗)) exp( −c1(α)s),
and
∥Ls−L∗∥2
F≤α−J(L∗)
c3(α)exp(−c1(α)s).
Proof. Consider a Lyapunov candidate function V(L):=J(L)−J(L∗). Under the GF dynamics
˙V(Ls) =−⟨∇J(Ls),∇J(Ls)⟩ ≤0.
Therefore, Ls∈ Sαfor all s >0. But then, by Lemma 2, we can also show that
˙V(Ls)≤ −c1(α)V(Ls)−c2(α)∥Ls−L∗∥2
F,fors >0.
By recalling that c1(α)>0is a positive constant independent of L, we conclude the following
exponential stability of the GF:
V(Ls)≤V(L0) exp(−c1(α)s),
for any L0∈ Sαwhich, in turn, guarantees convergence of J(Ls)→J(L∗)at the linear rate of
exp(−c1(α)). Finally, the linear convergence of the policy iterates follows directly from the second
bound in Lemma 2:
∥Ls−L∗∥2
F≤1
c3(α)V(Ls)≤V(L0)
c3(α)exp(−c1(α)s).
The proof concludes by noting that V(L0)≤α−J(L∗)for any such initial value L0∈ Sα.
22D.8 Gradient Descent (GD)
Here, we consider the GD policy update:
[GD] Lk+1=Lk−ηk∇J(Lk),
fork∈Zand a positive stepsize ηk. Given the convergence result for the GF, establishing con-
vergence for GDrelies on carefully choosing the stepsize ηk, and bounding the rate of change of
∇J(L)—at least on each sublevel set. This is achieved by the Lipschitz bound for ∇J(L)on any
sublevel set.
In what follows, we establish linear convergence of the GDupdate. Our convergence result only
depends on the value of αfor the initial sublevel set Sαthat contains L0. Note that our proof
technique is distinct from those in [6] and [34]; nonetheless, it involves a similar argument using the
gradient dominance property of J.
Theorem 4. Consider any sublevel set Sαfor some α >0. Then, for any initial policy L0∈ Sα, the
GDupdates with any fixed stepsize ηk=η∈(0,1/ℓ(α)]converges to optimality at a linear rate of
1−ηc1(α)/2(in both the function value and the policy iterate). In particular, we have
J(Lk)−J(L∗)≤[α−J(L∗)](1−ηc1(α)/2)k,
and
∥Lk−L∗∥2
F≤α−J(L∗)
c3(α)
(1−ηc1(α)/2)k,
withc1(α)andc3(α)as defined in Lemma 2.
Proof. First, we argue that the GDupdate with such a step size does not leave the initial sublevel
setSαfor any initial L0∈ Sα. In this direction, consider L(η) =L0−η∇J(L0)forη≥0where
L0̸=L∗. Then, by compactness of Sαand continuity of the mapping η→J(L(η))onS ⊃ S α, the
following supremum is attained with a positive value ¯η0:
¯η0:= sup{η:J(L(ζ))≤α,∀ζ∈[0, η]},
where positivity of ¯η0is a direct consequence of the strict decay of J(L(η))for sufficiently small η
as∇J(L0)̸= 0. This implies that L(η)∈ Sα⊂ S for all η∈[0,¯η0]andJ(L(¯η0)) =α. Next, by
the Fundamental Theorem of Calculus and smoothness of J(·)(Lemma 7), for any η∈[0,¯η0]we
have that,
J(L(η))−J(L0)− ⟨∇ J(L0), L(η)−L0⟩=Z1
0⟨∇J(L(ηs))− ∇J(L0), L(η)−L0⟩ds
≤ ∥L(η)−L0∥FZ1
0∥∇J(L(ηs))− ∇J(L0)∥Fds
≤ℓ(α)∥L(η)−L0∥FZ1
0∥L(ηs)−L0∥Fds
=1
2ℓ(α)η∥L(η)−L0∥F∥∇J(L0)∥F,
where ∥ · ∥Fdenotes the Frobenius norm, the first inequality is a consequence of Cauchy-Schwartz,
and the second one is due to (11c) and the fact that L(ηs)remains in Sαfor all s∈[0,1].3By the
definition of L(η), it now follows that,
J(L(η))−J(L0)≤η∥∇J(L0)∥2
Fℓ(α)η
2−1
. (28)
This implies J(L(η))≤J(L0)for all η≤2/ℓ(α), and thus concluding that ¯η0≥2/ℓ(α). This
justifies that L(η)∈ Sαfor all η∈[0,2/ℓ(α)]. Next, if we consider the GDupdate with any
3Note that a direct application of Descent Lemma [3, Lemma 5.7] may not be justified as one has to argue
about the uniform bound for the Hessian of Jover the non-convex set Sαwhere Jisℓ(α)-Lipschitz only on Sα.
Also see the proof of [34, Theorem 2].
23fixed stepsize η∈(0,1/ℓ(α)]and apply the bound in (28) and the gradient dominance property in
Lemma 2, we obtain
J(L1)−J(L0)≤ηc1(ℓ(α)η
2−1)[J(L0)−J(L∗)],
which by subtracting J(L∗)results in
J(L1)−J(L∗)≤ 
1−ηc1
2
[J(L0)−J(L∗)],
asηc1(ℓ(α)η/2−1)≤ −ηc1/2for all η∈(0,1/ℓ(α)]. By induction, and the fact that both c1(α)
and the choice of ηonly depends on the value of α, we conclude the convergence in the function
value at a linear rate of 1−(ηc1/2)and the constant coefficient of α−J(L∗)≥J(L0)−J(L∗).
To complete the proof, the linear convergence of the policy iterates follows directly from the second
bound in Lemma 2.
Appendix E Proofs for the analysis of the constrained SGD algorithm
E.1 Proof of lemma 3: Derivation of stochastic gradient formula
Proof. For small enough ∆∈Rn×m,
ε(L+ ∆,Y)−ε(L,Y) =∥eT(L+ ∆)∥2− ∥eT(L)∥2
= 2tr [( eT(L+ ∆)−eT(L))e⊺
T(L)] +o(∥∆∥)).
The difference
eT(L+ ∆)−eT(L) =E1(∆) + E2(∆) + o(∥∆∥),
with the following terms that are linear in ∆:
E1(∆):=−PT−1
t=0H(AL)t∆y(T−t−1),
E2(∆):=PT−1
t=1Pt
k=1H(AL)t−k∆H(AL)k−1Ly(T−t−1).
Therefore, combining the two identities, the definition of gradient under the inner product ⟨A, B⟩:=
tr [AB⊺], and ignoring the higher order terms in ∆yields,
⟨∇Lε(L, y),∆⟩= 2tr [( E1(∆) + E2(∆))e⊺
T(L)],
which by linearity and cyclic permutation property of trace reduces to:
⟨∇Lε(L, y),∆⟩=−2tr"
∆ T−1X
t=0y(T−t−1)e⊺
T(L)H(AL)t!#
+2tr"
∆ T−1X
t=1tX
k=1H(AL)k−1Ly(T−t−1)e⊺
T(L)H(AL)t−k!#
.
This holds for all admissible ∆, concluding the formula for the gradient.
E.2 Proof of Lemma 4: Robustness of the policy with respect to perturbation
Proof. Recall the stability certificate sKas proposed in [43, Lemma IV .1] for a choice of constant
mapping Q:K→Λ≻0and dual problem parameters as discussed in Remark 3. Then, we arrive at
sK=λ(Λ)/(2λ(Z)∥H⊺∆⊺∥),for which ρ(A⊺+H⊺(L⊺+η∆⊺))<1for any η∈[0, sK]. But,
the spectrum of a square matrix and its transpose are identical, thus L+η∆∈ S for any such η. The
claim then follows by noting that the operator norm of a matrix and its transpose are identical, and
the resulting lowerbound as follows sK≥λ(Λ)/(2λ(Z)∥H∥∥∆∥F).
E.3 Proof of Lemma 5: Uniform lower-bound on stepsize
Proof. Without loss of generality, suppose L0̸=L∗andE̸= 0, and let L(η):=L0−ηE. By
compactness of Sβand continuity of the mapping η→J(L(η))onS ⊃ S β, the following supremum
is attained by ηβ:
ηβ:= sup{η:J(L(ζ))≤β,∀ζ∈[0, η]}. (29)
24Note that ηβis strictly positive for β > α because L0∈ Sα⊂ SβandJ(·)is coercive (Lemma 1)
and its domain is open (Lemma 4). This implies that L(η)∈ Sβ⊂ S for all η∈[0, ηβ]and
J(L(ηβ)) =β.
Next, we want to show that ηβis uniformly lower bounded with high probability. By the Fundamental
Theorem of Calculus, for any η∈[0, ηβ]we have
J(L(η))−J(L0)− ⟨∇ J(L0), L(η)−L0⟩=Z1
0⟨∇J(L(ηs))− ∇J(L0), L(η)−L0⟩ds
≤ ∥L(η)−L0∥FZ1
0∥∇J(L(ηs))− ∇J(L0)∥Fds
≤ℓ(β)∥L(η)−L0∥FZ1
0∥L(ηs)−L0∥Fds
=1
2ℓ(β)η2∥E∥2
F, (30)
where the first inequality is a consequence of Cauchy-Schwartz, and the second one is due to (11c)
and the fact that L(ηs)remains in Sβfor all s∈[0,1]by definition of ηβ. Note that the assumption
implies ∥E∥F≤(γ+ 1)∥∇J(L0)∥F. Thus, (30) implies that
J(L(η))≤J(L0)−η⟨∇J(L0), E⟩+1
2ℓ(β)η2∥E∥2
F
≤J(L0)−η⟨∇J(L0), E− ∇J(L0)⟩ −η∥∇J(L0)∥2
F+1
2ℓ(β)η2∥E∥2
F
≤J(L0) +∥∇J(L0)∥2
F1
2(γ+ 1)2ℓ(β)η2+ (γ−1)η
.(31)
Therefore, for ηto be a feasible point in the supremum in (29), it suffices to satisfy:
1
2∥∇J(L0)∥2
F
(γ+ 1)2ℓ(β)η2+ 2(γ−1)η
≤β−α,
or equivalently,

(γ+ 1)ℓ(β)η+γ−1
γ+ 12
≤γ−1
γ+ 12
+2ℓ(β)[β−α]
∥∇J(L0)∥2
F.
But then it suffices to have
(γ+ 1)ℓ(β)η+γ−1
γ+ 1≤p
2ℓ(β)[β−α]
∥∇J(L0)∥F.
Finally, note that by Lemma 2 and (11c) we have
∥∇J(L0)∥F≤ℓ(α)
c3(α)[J(L0)−J(L∗)]≤ℓ(α)
c3(α)[α−α∗]. (32)
Using this uniform bound of gradient on sublevel set Sαand noting that γ∈[0,1], we can obtain the
sufficient condition for ηto be feasible. This completes the proof.
E.4 Proof of Proposition 2: Linear decay in cost value
Proof. Suppose L0̸=L∗and let L(η):=L0−ηE. Note that Emay not be necessarily in the
direction of decay in J(L), however, we can argue the following:
Choose β=αin Lemma 5 and note that ηβas defined in (29) will be lower bounded as ηβ≥¯η0,
and thus ¯η0is feasible. Recall that L(η)∈ Sβ⊂ S for all η∈[0, ηβ]. Also, for any η∈[0,¯η0]and
γ∈[0,1), from (31) we obtain that:
J(L(η))−J(L0)≤ ∥∇ J(L0)∥2
F1
2(γ+ 1)2ℓ(α)η2+ (γ−1)η
≤c1(α)[J(L0)−J(L∗)]1
2(γ+ 1)2ℓ(α)η2+ (γ−1)η
25where, as γ <1, the last inequality follows by (11a) for any η≤min{2¯η0, ηβ}. By the choice of ¯η0,
then we obtain that
J(L(¯η0))−J(L0)≤ −c1(α)(γ−1)2
2(γ+ 1)2ℓ(α)
[J(L0)−J(L∗)]
This implies that
J(L(¯η0))−J(L∗)≤
1−c1(α)(γ−1)2
2(γ+ 1)2ℓ(α)
[J(L0)−J(L∗)].
E.5 Proof of Theorem 1: Convergence of SGD algorithm
Before proving this result, we first discuss that the claim of Theorem 1 is sufficient for establishing
the complexity result of Theorem 3; i.e., it suffices to guarantee convergence from every initial
(stabilizing) policy to a neighborhood of optimality where norm of the gradient is controlled.
As shown in Lemma 2 (and discussed in Remark 4), the cost maintains the PL property on each
sublevel set. In particular (11a) implies that on each Sα,∥∇J(L)∥characterizes the optimality gap
J(L)−J(L∗)by:
c1(α)[J(L)−J(L∗)]≤ ∥∇ J(L)∥2
for some constant c1(α). This implies that if we have arrived at a candidate policy Lkfor which
the gradient is small, then the optimality gap should be small (involving the constant c1(α)that is
independent of Lk). This is the reason that in Theorem 1, it suffices to argue about the generated
sequence Lkto have a linear decay unless entering a neighborhood of L∗containing policies with
small enough gradients (denoted by Cτ). In particular, if for some j < k , we arrive at some policy
Lj∈ Cτ, then by (11a) we can conclude that:
J(Lj)−J(L∗)≤1
c1(α)∥∇J(Lj)∥2
F≤s2
0
c1(α)τ2,
which is directly controlled by the bias term s0. This is the bound used also in the proof of Theorem 3.
Finally, recall that every (stabilizing) initial policy L0amounts to a finite value of J(L0)and thus
lies in some sublevel set Sα. So, starting from such L0, Theorem 1 guarantees linear decay of the
optimality gap till the trajectory enters that small neighborhood. Finally, note that the radius of this
neighborhood Cτis characterized by the bias term s0which itself is exponentially decaying to zero in
the trajectory length T.
Next, we provide the proof of this result that is essentially an induction argument using Proposition 2.
Proof of Theorem 1. The first step of the proof is to show that the assumption of Proposition 2 is
satisfied for E=∇bJ(L)for all L∈ Sα\ Cγ/2. This is true because
∥∇bJ(L)− ∇J(L)∥ ≤s∥∇J(L)∥+s0
≤s∥∇J(L)∥+γ
2∥∇J(L)∥
≤γ∥∇J(L)∥
where the first inequality follows from Assumption 3, the second inequality follows from L̸∈Cγ/2
(i.e.s0∥∇J(L)∥ ≥γ/2), and the last step follows from the assumption s≤γ/2.
The rest of the proof relies on repeated application of Proposition 2. In particular, starting from
L0∈ Sα\ Cγ/2, the application of Proposition 2 implies that L1:=L0−¯η∇bJ(L0)remains in the
same sublevel set, i.e., L1∈ Sα, and we obtain the following linear decay of the cost value:
J(L1)−J(L∗)≤[1−c1(α)¯η(1−γ)/2] [J(L0)−J(L∗)].
Now, if L1∈ Cγ/2then we stop; otherwise L1∈ Sα\ Cγ/2and we can repeat the above process to
arrive at L2:=L1−¯η∇bJ(L1)∈ Sα, with a guaranteed linear decay
J(L2)−J(L∗)≤[1−c1(α)¯η(1−γ)/2] [J(L1)−J(L∗)].
26Combining the last two linear decays yields
J(L2)−J(L∗)≤[1−c1(α)¯η(1−γ)/2]2[J(L0)−J(L∗)].
Repeating the process generates a sequence of policies L0, L1, L2...with a combined linear decay of
J(Lk)−J(L∗)≤[1−c1(α)¯η(1−γ)/2]k[J(L0)−J(L∗)],
unless at some iteration j, we arrive at a policy Ljsuch that Lj∈ Cγ/2. This completes the proof.
Appendix F Proofs of the result for observation model and sample complexity
F.1 Preliminary lemmas and their proofs
First, we provide the proof for the complete version of Lemma 6:4
Lemma 6’ (Uniform Bounds for Stable Systems) .Suppose L∈ S, then there exit a constants
CL>0such that
∥Ak
L∥ ≤CLp
ρ(AL)k+1
,∀k≥0,
whenever ρ(AL)>0, and otherwisep
ρ(AL)is replaced with any arbitrarily small r∈(0,1).
Additionally,
∞X
i=0∥Ai
L∥ ≤CL
1−p
ρ(AL)
∞X
i=0∥Mi[E]∥ ≤1 + 2 C2
Lρ(AL)3/2
[1−p
ρ(AL)]2∥EH∥
Furthermore, consider Sαfor some α > 0, then there exist constants Dα>0,Cα>0and
ρα∈(0,1)such that ∥L∥ ≤Dα, CL≤Cαandρ(AL)≤ρα,∀L∈ Sα.
Proof of Lemma 6. Recall the Cauchy Integral formula for matrix functions [16, Theorem 1.12]: for
any matrix M∈Cn×n,
f(M) =1
2πiI
Γf(z)(zI−M)−1dz,
whenever fis real analytic on and inside a closed contour Γthat encloses spectrum of M. Note
thatL∈ Simplying that ρ(AL)<1. Now, fix some r∈(ρ(AL),1)and define Γ(θ) =reiθwithθ
ranging on [0,2π]. Therefore, Cauchy Integral formula applies to f(z) =zkfor any positive integer
kand the contour Γdefined above. So, for matrix AL, we obtain
Ak
L=1
2πiI
Γzk(zI−AL)−1dz=1
2πiZ2π
0rkeikθ(reiθI−A)−1r deiθ,
implying that
∥Ak
L∥ ≤rk+1
2πZ2π
0∥(reiθI−AL)−1∥dθ≤rk+1max
θ∈[0,2π]∥(reiθI−AL)−1∥.
Finally, the first claim follows by choosing r=p
ρ(AL)(whenever ρ(AL)>0, otherwise r∈(0,1)
can be chosen arbitrarily small) and defining
CL:= max
θ∈[0,2π]∥(p
ρ(AL)eiθI−AL)−1∥
which is attained and bounded.
4See [11, Definition 3.1] for an alternative notation; however, we prefer the explicit and simple form of
expressing spectral radius in the bounds established in our work, which also facilitates the comparison to
literature on first order methods for stabilizing policies.
27Next, by applying the first claim, we have
TX
i=0∥Ai
L∥ ≤CL1−p
ρ(AL)T
1−p
ρ(AL),
implying the second bound. For the third claim, note that for i= 1,2,···we obtain
∥Mi+1[E]∥ ≤iX
k=0∥Ai−k
L∥∥Ak
L∥∥EH∥ ≤ ∥ EH∥C2
LiX
k=0hp
ρ(AL)i(i−k+1)+( k+1)
≤ ∥EH∥C2
Lp
ρ(AL)h
(i+ 1)·ρ(AL)(i+1)/2i
.(33)
But, then by recalling that M0[E] = 0 and∥M1[E]∥=∥EH∥we have
∞X
i=0∥Mi[E]∥ ≤ ∥ EH∥+∥EH∥"
2C2
Lρ(AL)3/2
[1−p
ρ(AL)]2#
where we used the following convergent sum for any ρ∈(0,1):
∞X
i=1(i+ 1)·ρi+1=(2−ρ)ρ2
(1−ρ)2≤2ρ2
(1−ρ)2.
This implies the third bound.
The final claim follows directly from compactness of sublevel set Sα(Lemma 1) and continuity of the
mappings (L, θ)7→(ρ(AL), θ)7→ ∥(p
ρ(AL)eiθI−AL)−1∥onSα×[0,2π]whenever ρ(AL)>0
(and otherwise considering the mapping (L, θ)7→ ∥(reiθI−AL)−1∥for arbitrarily small and fixed
r∈(0,1)).
As mentioned in Section 4.2, a key idea behind these error bounds that scale well with respect to the
length T is the following consequence of von Neumann Trace Inequality [19, Theorem 8.7.6]:
|tr [MN]| ≤PT
i=1σi(M)σi(N)≤ ∥M∥∥N∥∗,
with∥ · ∥∗denoting the nuclear norm. Additionally, as a direct consequence of Courant-Fischer
Theorem, one can also show that nuclear norm is sub-multiplicative. More precisely,
∥AB∥∗≤ ∥A∥ ∥B∥∗≤ ∥A∥∗∥B∥∗.
Next, we require the following lemma to bound these errors.
Lemma 8. For any L∈ Sα, we have
∥A⊺
LH⊺HAL∥∗≤C2
L∥H⊺H∥∗
1−ρ(AL),
∥NL[E]∥∗≤
2CL+ 4C3
Lρ(AL)3/2
∥H∥∥H⊺H∥∗
[1−ρ(AL)]2∥E∥.
Proof of Lemma 8. For the first claim, note that A⊺
LH⊺HALis positive semi-definite, so
∥A⊺
LH⊺HAL∥∗= tr [A⊺
LH⊺HAL]
≤tr [H⊺H]∥ALA⊺
L∥
≤ ∥H⊺H∥∗TX
i=0Ai
L(A⊺
L)i
≤ ∥H⊺H∥∗TX
i=0∥Ai
L∥2
≤ ∥H⊺H∥∗C2
L
1−ρ(AL)
28where the last inequality follows by Lemma 6. Next, we have
∥ML[E]∥=∥ML[E]ML[E]⊺∥1/2
≤"TX
i=0∥Mi[E]∥2#1/2
≤ ∥EH∥+∥EH∥C2
Lp
ρ(AL)"TX
i=0(i+ 1)2·ρ(AL)(i+1)#1/2
≤ ∥EH∥+∥EH∥C2
Lp
ρ(AL)2ρ(AL)
[1−ρ(AL)]3/2
≤ ∥EH∥1 + 2 C2
Lρ(AL)3/2
[1−ρ(AL)]3/2
where the second inequality follows by (33) and the third one by the following convergent sum for
anyρ∈(0,1):
∞X
i=1(i+ 1)2·ρi+1=ρ2(ρ2−3ρ+ 4)
(1−ρ)3≤4ρ2
(1−ρ)3.
Also, by the properties of nuclear norm
∥H⊺HAL∥∗= trq
H⊺HALA⊺
LH⊺H
≤ ∥A LA⊺
L∥1/2∥H⊺H∥∗
≤"∞X
i=0∥Ai
L∥2#1/2
∥H⊺H∥∗
≤C2
L
1−ρ(AL)1/2
∥H⊺H∥∗,
where the last inequality follows by Lemma 6. Finally, notice that
∥NL[E]∥∗≤2∥A⊺
LH⊺HML[E]∥∗
≤2∥ML[E]∥∥H⊺HAL∥∗
and thus combining the last three bounds implies the second claim. This completes the proof.
The next tool we will be using is the following famous bound on random matrices which is a variant
of Bernstein inequality:
Lemma 9 (Matrix Bernstein Inequality [46, Corollary 6.2.1]) .LetZbe ad1×d2random ma-
trices such that E[Z] =¯Zand∥Z∥ ≤Kalmost surely. Consider Mindependent copy of Zas
Z1,···, ZM, then for every t≥0, we have
P1
MP
iZi−¯Z≥t
≤(d1+d2) exp−Mt2/2
σ2+ 2Kt/3
where σ2= max {∥E[ZZ⊺]∥,∥E[Z⊺Z]∥}is the per-sample second moment. This bound can be ex-
pressed as the mixture of sub-gaussian and sub-exponential tail as (d1+d2) expn
−cmin{t2
σ2,t
2K}o
for some c.
We are now well-equipped to provide the main proofs.
29F.2 Proof of Proposition 3: The Observation model
Proof. Recall that
ε(L,YT) =∥Hx(T)−Hˆx(T)∥2=mX
i=1|H⊺
ix(T)−H⊺
iˆx(T)|2
where H⊺
iis the i-th row of H. Also, by duality, if z(t) = ( A⊺
L)T−tHiis the adjoint dynamics’
closed-loop trajectory with control signal u(t) =L⊺z(t)then
H⊺
ix(T)−H⊺
iˆx(T) =⃗ z⊺
i⃗ξ−⃗ u⊺⃗ ω
where
⃗ z⊺
i= (z(T)⊺z(T−1)⊺. . . z (1)⊺z(0)⊺),
⃗ u⊺= (u(T)⊺u(T−1)⊺. . . u (1)⊺0⊺
m).
But⃗ u= (I⊗L⊺)⃗ ziand then ⃗ zi=A⊺
LHi. Therefore,
ε(L,YT) =mX
i=1|H⊺
ix(T)−H⊺
iˆx(T)|2
=mX
i=1trh
⃗ξ⃗ξ⊺⃗ zi⃗ z⊺
ii
−trh
⃗ξ ⃗ ω⊺(I⊗L⊺) + (I⊗L)⃗ ω⃗ξ⊺
⃗ zi⃗ z⊺
ii
+ tr [ ⃗ ω ⃗ ω⊺(I⊗L⊺)⃗ zi⃗ z⊺
i(I⊗L)]
Then, by using the fact thatPm
i=1HiH⊺
i=H⊺H, we obtain that
ε(L,YT) = tr [ XLA⊺
LH⊺HAL].
Thus, we can rewrite the estimation error as
ε(L,YT) =D
⃗ξ ,⃗ξE
AL+⟨(I⊗L)⃗ ω ,(I⊗L)⃗ ω⟩AL−2D
⃗ξ ,(I⊗L)⃗ ωE
AL=∥⃗ ηL∥2
AL
Next, we can compute that for small enough E
AL+E− A L=ML[E] +o(∥E∥).
This implies that
d(A⊺
LH⊺HAL)
L[E] =ML[E]⊺H⊺HAL+A⊺
LH⊺HML[E]
On the other hand,
XL+E− XL= (I⊗E)⃗ ω ⃗ ω⊺(I⊗L⊺)
+ (I⊗L)⃗ ω ⃗ ω⊺(I⊗E⊺)
−⃗ξ ⃗ ω⊺(I⊗E⊺) + (I⊗E)⃗ ω⃗ξ⊺+o(∥E∥).
Therefore, the second claim follows by the chain rule.
F.3 Proof of Proposition 4: Concentration bounds
We provide the proof for a detailed version of Proposition 4:
Proposition 4’ (Concentration independent of length T).Consider length Ttrajectories
{Yi
[t0,t0+T]}M
i=1and let bJT(L):=1
MPM
i=1ε(L,Yi
T). Then, under Assumption 4, for any s >0
Ph
|bJT(L)−JT(L)| ≤si
≥1−2nexp−Ms2/2
µ2
L+ 2µLs/3
,
Ph
∥∇bJT(L)− ∇JT(L)∥ ≤si
≥1−2nexp−Ms2/2
ν2
L+ 2νLs/3
30where κL=κξ+∥L∥κωand
µL:=κ2
LC2
L
[1−p
ρ(AL)]2∥H⊺H∥∗
νL:=2κLκωC2
L+
CL+ 2C3
Lρ(AL)3/2
∥H∥κ2
L
[1−p
ρ(AL)]3∥H⊺H∥∗.
Proof of Proposition 4. Note that Eh
⃗ξ⃗ξ⊺i
=
I⊗Q0
0 P0
=:Q,E[⃗ ω ⃗ ω⊺] =
I⊗R0
0 0 m
=:
R, andEh
⃗ξ ⃗ ω⊺i
= 0. Assume m0= 0 and recall that ⟨∇ε(L,YT), E⟩= dε(·,YT)
L(E)thus,
using Proposition 3, we can rewrite the JT(L)and its gradient as
JT(L) =E[ε(L,YT)] =E
∥⃗ ηL∥2
AL
= tr [E[XL]A⊺
LH⊺HAL]
where E[XL] =Q+ (I⊗L)R(I⊗L⊺). Therefore, by definition of bJT(L)we obtain
bJT(L) = tr [ ZLA⊺
LH⊺HAL]
withZL=1
MPM
i=1XL(Yi)which can be expanded as
ZL=Z1+ (I⊗L)Z2(I⊗L⊺)− Z3(I⊗L⊺)−(I⊗L)Z⊺
3,
where
Z1=1
MMX
i=1⃗ξi⃗ξ⊺
i,Z2=1
MMX
i=1⃗ ωi⃗ ω⊺
i,Z3=1
MMX
i=1ξi⃗ ω⊺
i,
andE[ZL] =Q+ (I⊗L)R(I⊗L⊺). Therefore,
bJT(L)−JT(L) = tr [( ZL−E[ZL])A⊺
LH⊺HAL].
Thus, by cyclic permutation property of trace and (12) we obtain
|bJT(L)−JT(L)| ≤∥A L(ZL−E[ZL])A⊺
L∥∥H⊺H∥∗. (34)
Next, we consider the symmetric random matrix AL(ZL−E[ZL])A⊺
L. Note that ∥ξ(t)−Lω(t)∥ ≤
κLalmost surely and thus
∥ALXLA⊺
L∥=∥AL⃗ ηL∥2≤κ2
L"∞X
i=0∥Ai
L∥#2
≤µL/∥H⊺H∥∗.
It then follows that
E
(ALXLA⊺
L)2≤E
∥ALXLA⊺
L∥2
≤µ2
L/∥H⊺H∥2
∗.
Therefore, by Lemma 9 we obtain that
P[∥AL(ZL−E[ZL])A⊺
L∥ ≥t]≤2nexp−M∥H⊺H∥2
∗t2/2
µ2
L+ 2µL∥H⊺H∥∗t/3
.
Substituting twitht/∥H⊺H∥∗together with (34) implies the first claim.
Similarly, we can compute that
⟨∇JT(L), E⟩=⟨E[∇ε(L,YT)], E⟩
=2tr [( I⊗L)R(I⊗E⊺)A⊺
LH⊺HAL] + tr [( Q+ (I⊗L)R(I⊗L⊺))NL[E]],
and thus
⟨∇bJT(L)− ∇JT(L), E⟩=−2tr [Z3(I⊗E⊺)A⊺
LH⊺HAL]
+ 2tr [( I⊗L)(Z2− R)(I⊗E⊺)A⊺
LH⊺HAL]
+ tr [(ZL−E[ZL])NL[E]].
31Thus, by cyclic permutation property of trace and (12) we obtain that
|⟨∇bJT(L)− ∇JT(L), E⟩| ≤ ∥1
MMX
i=1SL(E,Yi
T)−E
SL(E,Yi
T)
∥∥H⊺H∥∗ (35)
where SL(E,Y)is the symmetric part of the following random matrix
−2AL⃗ξ ⃗ ω⊺(I⊗E⊺)A⊺
L+ 2AL(I⊗L)⃗ ω ⃗ ω⊺(I⊗E⊺)A⊺
L+ 2ALXLML[E]⊺.
Next, we provide the following almost sure bounds for each term: first,
∥ALZ3(I⊗E⊺)A⊺
L∥ ≤ ∥A L⃗ξ∥∥A L(I⊗E)⃗ ω∥
≤κξ"TX
i=0∥Ai
L∥#
κω∥E∥"TX
i=0∥Ai
L∥#
≤κξκωC2
L
[1−p
ρ(AL)]2∥E∥
where the last equality follows by Lemma 6; second, similarly
∥AL(I⊗L)⃗ ω ⃗ ω⊺(I⊗E⊺)A⊺
L∥
≤ ∥A L(I⊗L)⃗ ω∥∥A L(I⊗E)⃗ ω∥
≤κ2
ω∥L∥∥E∥"TX
i=0∥Ai
L∥#2
≤κ2
ωC2
L
[1−p
ρ(AL)]2∥L∥ ∥E∥;
and finally
∥ALXLML[E]⊺∥ ≤∥A L⃗ ηL∥∥M L[E]⃗ ηL∥
≤κ2
L"TX
i=0∥Ai
L∥# "TX
i=0∥Mi[E]∥#
≤κ2
L"
CL+ 2C3
Lρ(AL)3/2
[1−p
ρ(AL)]3#
∥EH∥
where the last inequality follows by Lemma 6. Now, by combining the last three bounds we can
claim that almost surely
∥SL(E,Y)∥ ≤νL
∥H⊺H∥∗∥E∥.
This also implies that
∥E
SL(E,Y)2
∥ ≤E
∥SL(E,Y)∥2
≤ν2
L
∥H⊺H∥2∗∥E∥2.
Therefore, by Lemma 9 we obtain that
P"
∥1
MMX
i=1SL(E,Yi
T)−E
SL(E,Yi
T)
∥ ≥t#
≤2nexp−M∥H⊺H∥2
∗t2/2
ν2
L∥E∥2+ 2νL∥H⊺H∥∗∥E∥t/3
Thus, by substituting twitht∥E∥
∥H⊺H∥∗and applying this bound to (35) we obtain that
Ph
|⟨∇bJT(L)− ∇JT(L), E⟩| ≤t∥E∥i
≥1−2nexp−Mt2/2
ν2
L+ 2νLt/3
Finally, choosing E=∇bJT(L)− ∇JT(L)proves the second claim.
32F.4 Proof of the Proposition 5: Truncation error bound
We provide the proof for a detailed version of Proposition 5:
Proposition 5’ (Truncation Error Bound) .Suppose m0= 0, then under Assumption 4 we have
|J(L)−JT(L)| ≤¯ξLρ(AL)T+1
1−ρ(AL),
and
∥∇J(L)− ∇JT(L)∥ ≤¯γLp
ρ(AL)T+1
[1−ρ(AL)]2
where
¯ξL:=
κ2
ξ+ (κ2
ξ+κ2
ω∥L∥2)C2
L
∥H⊺H∥∗C2
L,
¯γL:=2
κ2
ξ+C2
L(κ2
ξ+κ2
ω∥L∥2)
C2
L∥H∥∥H⊺H∥∗
+ 2κ2
ω(κ2
ξ+κ2
ω∥L∥2)∥L∥∥H∥∥H⊺H∥∗
CL+ 2C3
Lρ(AL)3/2
C3
Lp
ρ(AL)T+1.
Proof of Proposition 5. For the purpose of this proof, we denote the same matrices by AL,Tand
ML,T[E]in order to emphasize on length T. Recall that
JT(L) =E[ε(L,Y)] = trh
E[XL]A⊺
L,TH⊺HAL,Ti
,
where E[XL] =Q+ (I⊗L)R(I⊗L⊺), which implies
JT(L) = trh
[I⊗(Q+LRL⊺)]A⊺
L,T−1H⊺HAL,T−1i
+ tr
P0(A⊺
L)TH⊺HAT
L
,
On the other hand,
J(L) = lim
t→∞trh
[I⊗(Q+LRL⊺)]A⊺
L,tH⊺HAL,ti
,
and thus
J(L)−JT(L) =−tr
P0(A⊺
L)TH⊺HAT
L
+ lim
t→∞trh
[I⊗(Q+LRL⊺)][I⊗(A⊺
L)T]A⊺
L,tH⊺HAL,t[I⊗AT
L]i
=−tr
AT
LP0(A⊺
L)TH⊺H
+ lim
t→∞trh
[I⊗AT
L(Q+LRL⊺)(A⊺
L)T]A⊺
L,tH⊺HAL,ti
.
Therefore, by the properties of trace and Lemma 6 we obtain that
|J(L)−JT(L)| ≤∥P0∥∥AT
L∥2tr [H⊺H]
+∥Q+LRL⊺∥∥(AL)T∥2lim
t→∞trh
A⊺
L,tH⊺HAL,ti
≤ ∥P0∥C2
Lρ(AL)T+1∥H⊺H∥∗
+ (∥Q∥+∥R∥∥L∥2)C2
Lρ(AL)T+1C2
L∥H⊺H∥∗
1−ρ(AL),
where the last line follows by Lemma 8. So,
|J(L)−JT(L)| ≤
∥P0∥+(∥Q∥+∥R∥∥L∥2)C2
L
1−ρ(AL)
∥H⊺H∥∗C2
Lρ(AL)T+1
This, together with Assumption 4 imply the first claim.
Next, for simplicity we adopt the notation AL,∞to interpret the limit as t→ ∞ , then similar to the
proof of Proposition 4 we can compute that
⟨∇J(L)− ∇JT(L), E⟩=−2tr
MT[E]P0(A⊺
L)TH⊺H
+ tr
[I⊗AT
L(Q+LRL⊺)(A⊺
L)T]NL,∞[E]
+ 2trh
[I⊗MT[E](Q+LRL⊺)(A⊺
L)T]A⊺
L,∞H⊺HAL,∞i
+ 2trh
[I⊗AT
LERL⊺(A⊺
L)T]A⊺
L,∞H⊺HAL,∞i
.
33Therefore, using (12) and Lemma 6 we have the following bound
|⟨∇J(L)− ∇JT(L), E⟩| ≤2∥EH∥C2
L(T+ 1)ρ(AL)T+1∥P0∥∥H⊺H∥∗
+∥Q+LRL⊺∥C2
Lρ(AL)T+1∥NL,∞[E]∥∗
+ 2∥Q+LRL⊺∥∥EH∥C2
L(T+ 1)ρ(AL)T+1∥A⊺
L,∞H⊺HAL,∞∥∗
+ 2∥ERL⊺∥C2
Lρ(AL)T+1∥A⊺
L,∞H⊺HAL,∞∥∗
which by Lemma 8 is bounded as follows
|⟨∇J(L)− ∇JT(L), E/∥E∥⟩| ≤ 2∥P0∥∥H∥∥H⊺H∥∗C2
L(T+ 1)ρ(AL)T+1
+∥Q+LRL⊺∥∥H∥∥H⊺H∥∗
2C3
L+ 4C5
Lρ(AL)3/2
ρ(AL)T+1
[1−ρ(AL)]2
+ 2∥Q+LRL⊺∥∥H∥∥H⊺H∥∗C4
L(T+ 1)ρ(AL)T+1
1−ρ(AL)
+ 2∥R∥∥L∥∥H⊺H∥∗C4
Lρ(AL)T+1
1−ρ(AL).
Finally, choosing E=∇J(L)− ∇JT(L)together with Assumption 4 implies
∥∇J(L)− ∇JT(L)∥ ≤2"
κ2
ξ+C2
L(κ2
ξ+κ2
ω∥L∥2)
1−ρ(AL)#
C2
L∥H∥∥H⊺H∥∗(T+ 1)ρ(AL)T+1
+ 2"
κ2
ω(κ2
ξ+κ2
ω∥L∥2)∥L∥∥H∥ 
CL+ 2C3
Lρ(AL)3/2
1−ρ(AL)#
∥H⊺H∥∗C3
Lρ(AL)T+1
1−ρ(AL).
Finally, the second claim follows by the following simple facts:
(T+ 1)ρ(AL)T+1≤p
ρ(AL)T+1
1−ρ(AL),∀T >0,
asmax t≥0tρt=2
eln 1/ρ≤1
ln 1/ρ≤1
1−ρfor any ρ∈(0,1). This completes the proof.
F.5 Complete version of Theorem 2: Sample complexity bounds for the stochastic oracle
The following is a detailed version of Theorem 2:
Theorem 2’. Suppose m0= 0nand Assumption 4 holds for a data-set {Yi
T}M
i=1. Define ∇bJT(L):=
1
MPM
i=1∇ε(L,Yi
T),where ∇Lε(L,Y)is obtained in Lemma 3. Consider Sαfor some α >0and
anys, s0>0andτ∈(0,1). Suppose the trajectory length
T≥ln 
¯γαp
min(n, m)
s0!

ln1√ρα
and the batch size
M≥
2 
ναp
min(n, m)
s s0/τ!2
+4
3 
ναp
min(n, m)
s s0/τ!
ln(2n/δ),
where
¯γα:= 2
κ2
ξ+C2
α(κ2
ξ+κ2
ωD2
α)
C2
α∥H∥∥H⊺H∥∗
+ 2κ2
ω(κ2
ξ+κ2
ωD2
α)Dα∥H∥∥H⊺H∥∗
Cα+ 2C3
αρ3/2
α
C3
α√ραT+1,
να:=2(κξ+Dακω)κωC2
α+h
Cα+ 2C3
αρ3/2
αi
∥H∥(κξ+Dακω)2
[1−√ρα]3/∥H⊺H∥∗,
withρα,CαandDαdefined in Lemma 6. Then, with probability no less than 1−δ, Assumption 3
holds.
34F.6 Additional concentration bound results
Combining the truncation bound in Proposition 5 with concentration bounds in Proposition 4 we can
provide probabilistic bounds on the “estimated cost” bJT(L)and the “estimated gradient” ∇bJT(L).
The result involves the bound for the Frobenius norm of the error with probabilities independent of T.
Theorem 2, can be viewed as a simplified application of Theorem 5 to characterize the required
minimum trajectory length and minimum batch so that the approximate gradient satisfies Assumption
3, with a specific sands0.
Theorem 5. Suppose Assumption 4 holds. For any s >0andL∈ Sα, if
M≥
2"
νLp
min(n, m)
s∥∇J(L)∥F#2
+4
3"
νLp
min(n, m)
s∥∇J(L)∥F#
ln(2n/δ),
then with probability no less than 1−δ,
∥∇bJT(L)− ∇J(L)∥F≤s∥∇J(L)∥F+ ¯γLp
min(n, m)p
ρ(AL)T+1,
withνLand¯γLdefined in Proposition 4 and Proposition 5, respectively.
Proof of Theorem 5. Recall that for any L∈ Sαfor some α >0we have
∥∇bJT(L)− ∇J(L)∥ ≤ ∥∇ bJT(L)− ∇JT(L)∥+∥∇JT(L)− ∇J(L)∥.
Thus, by Proposition 4 with sreplaced by s∥∇J(L)∥F/p
min(n, m)and applying Proposition 5 to
the second term, we obtain that with probability at least 1−δ:
∥∇bJT(L)− ∇J(L)∥ ≤s∥∇J(L)∥Fp
min(n, m)+ ¯γLp
ρ(AL)T+1
[1−ρ(AL)]2,
where
δ≥2nexp
−Ms2/2

νL√
min(n,m)
∥∇J(L)∥F2
+ 2
νL√
min(n,m)
∥∇J(L)∥F
s/3
.
Noticing ∥∇bJT(L)− ∇J(L)∥F≤p
min(n, m)∥∇bJT(L)− ∇J(L)∥and rearranging terms will
complete the proof.
One can also provide the analogous concentration error bounds where the probabilities are independent
of the system dimension n.
Proposition 7 (Concentration independent of system dimension n).Under the same hypothesis, we
have
Ph
|bJT(L)−JT(L)| ≤si
≥1−2Texp−Ms2/2
¯µ2
LT2+ 2¯µLTs/3
,
and
Ph
∥∇bJT(L)− ∇JT(L)∥ ≤si
≥1−2Texp−Ms2/2
¯ν2
LT2+ 2¯νLTs/3
−2Texp−Ms2/2
κ2ω¯µ2
LT2+ 2κω¯µLTs/3
where
¯µL:=C2
L∥H⊺H∥∗
1−ρ(AL)κ2
L
¯νL:=
2CL+ 4C3
Lρ(AL)3/2
∥H∥∥H⊺H∥∗
[1−ρ(AL)]2κ2
L.
35Proof of Proposition 7. Similar to the previous proof, we have
bJT(L)−JT(L) = tr [( ZL−E[ZL])A⊺
LH⊺HAL],
and thus, by (12) we obtain
|bJT(L)−JT(L)| ≤∥(ZL−E[ZL])∥∥A⊺
LH⊺HAL∥∗. (36)
Next, we consider the symmetric random matrix (ZL−E[ZL])and recall that ∥ξ(t)−Lω(t)∥ ≤κL
almost surely; thus
∥XL∥=∥⃗ ηL∥2≤κ2
LT2.
It then follows that E
X2
L≤E
∥XL∥2
≤κ4
LT4.
Therefore, by Lemma 9 we obtain that
P[∥(ZL−E[ZL])∥ ≥t]≤2Texp−Mt2/2
κ4
LT4+ 2κ2
LT2t/3
(37)
Substituting twitht/∥A⊺
LH⊺HAL∥∗together with (36) implies the first claim because by Lemma 8
κ2
LT2∥A⊺
LH⊺HAL∥∗≤¯µLT.
Again, similar to (35) in the previous proof, by (12) we obtain that
|⟨∇bJT(L)− ∇JT(L), E⟩| ≤ ∥1
MMX
i=1SL(E,Yi
T)−E
SL(E,Yi
T)
∥∥A⊺
LH⊺HA∥∗
+∥1
MMX
i=1XL(Yi)−E
XL(Yi)
∥∥N L[E]∥∗(38)
where SL(E,Y)is the symmetric part of the following random matrix
−2⃗ξ ⃗ ω⊺(I⊗E⊺) + 2( I⊗L)⃗ ω ⃗ ω⊺(I⊗E⊺).
So, we claim that almost surely
∥SL(E,Y)∥ ≤2∥(I⊗E)⃗ ω∥(∥⃗ξ∥+∥(I⊗L)⃗ ω∥)
≤2∥E∥κωT(κξT+∥L∥κωT)
=κLκωT2∥E∥,
and thus
∥E
SL(E,Y)2
∥ ≤E
∥SL(E,Y)∥2
≤κ2
Lκ2
ωT4∥E∥2.
Therefore, by Lemma 9 we obtain that
P"
∥1
MMX
i=1SL(E,Yi
T)−E
SL(E,Yi
T)
∥ ≥t#
≤2Texp−Mt2/2
κ2ωκ2
LT4∥E∥2+ 2κLκωT2∥E∥t/3
Substituting twitht/∥A⊺
LH⊺HAL∥∗implies that
P"
∥1
MMX
i=1SL(E,Yi
T)−E
SL(E,Yi
T)
∥∥A⊺
LH⊺HAL∥∗≥t#
≤2Texp−Mt2/2
κ2ω¯µ2
LT2∥E∥2+ 2κω¯µLT∥E∥t/3
because by Lemma 8 we have κ2
LT2∥A⊺
LH⊺HAL∥∗≤¯µLT.
Next, by substituting twitht/∥NL[E]∥∗in (37) we have
P[∥ZL−E[ZL]∥∥N L[E]∥∗≥t]≤2Texp−Mt2/2
¯ν2
LT2∥E∥2+ 2¯νLT∥E∥t/3
36because Lemma 8 implies that κ2
LT2∥NL[E]∥∗≤¯νLT∥E∥. Thus, by combining the last two
inequalities and using the union bound for (38) we obtain that
Ph
|⟨∇bJT(L)− ∇JT(L), E⟩| ≥ti
≤2Texp−Mt2/2
¯ν2
LT2∥E∥2+ 2¯νLT∥E∥t/3
+ 2Texp−Mt2/2
κ2ω¯µ2
LT2∥E∥2+ 2κω¯µLT∥E∥t/3
Finally, substituting twitht∥E∥and choosing E=∇bJT(L)−∇JT(L)proves the second claim.
Remark 10.We obtained a better bound for truncation of the gradient as
∥∇J(L)− ∇JT(L)∥ ≤¯γ1(L)p
ρ(AL)T+1
[1−ρ(AL)] ln(1 /ρ(AL))+ ¯γ2(L)ρ(AL)T+1
[1−ρ(AL)]2,
which has been simplified for clarity of the presentation.
Appendix G Numerical Results
Herein, we showcase the application of the developed theory for improving the estimation policy for
an LTI system. Specifically, we consider an undamped mass-spring system with known parameters
(A, H)withn= 2andm= 1. In the hindsight, we consider a variance of 0.1for each state dynamic
noise, a state covariance of 0.05and a variance of 0.1for the observation noise. Assuming a trajectory
of length Tat every iteration, the approximate gradient is obtained as in Lemma 3, only requiring an
output data sequence collected from the system in (1). Then, the progress of policy updates using the
SGD algorithm for different values of trajectory length Tand batch size Mare depicted in Figure 1
where each figure shows average progress over 50 rounds of simulation. The figure demonstrates a
linear convergence outside of a neighborhood of global optimum that depends on the bias term in the
approximate gradient (due to truncated data trajectories). The rate then drops when the policy iterates
enter into this neighborhood which is expected as every update only relies on a biased gradient —in
contrast to the linear convergence established for deterministic GD(to the exact optimum) using the
true gradient.
Specifically, recall that our convergence guarantee to a small neighborhood around the optimal value
is due to the finite-length of the data trajectories. The region can be made arbitrary small by choosing
larger trajectory length. In particular, to achieve εerror, we only require the length T≥O(ln(1/ε))—
see Theorem 3. Also, Fig 1(d) is illustrating that optimality gap at the final iteration (i.e. the radius
of the small neighborhood around optimality) which is decaying linearly as a function of trajectory
length T≤50—until the variance error dominates beyond T= 50 . It is clear that, increasing the
batch size Mwill allow further decrease of this optimality gap beyond T= 50 .
The code for regenerating these results is available online at this GitHub repository [44].
Appendix H Nomenclature
370 250 500 750 1000 1250 1500 1750 2000
iteration104
103
102
101
100
J(Lk)J(L)
J(L)
M=10
M=20
M=30
M=40
M=80(a)
101102
M106
105
104
103
J(LK)J(L)
J(L)
 (b)
0 250 500 750 1000 1250 1500 1750 2000
iteration103
102
101
100J(Lk)J(L)
J(L)
T=10
T=20
T=30
T=40
T=50
(c)
20 40 60 80 100
T107
106
105
104
103
102
101
 J(LK)J(L)
J(L)
 (d)
Figure 1: Simulation result of the SGD algorithm to learn the steady-state Kalman gain for the
mass-spring example. (a) The optimality gap as a function of iterations kfor different value of
batch-size Maveraged over 50simulations; (b) The optimality gap at final iteration as a function of
batch-size Mfor all 50simulations; (c) The optimality gap as a function of iterations kfor different
value trajectory length T. The figure depicts the linear decay of the optimality gap, with respect to
the iteration k, before the iterate enters the small neighborhood of optimality where the direction of
the oracle gradient is not informative anymore. The neighborhood shrinks as Tincreases; (d) The
optimality gap at final iteration as a function of trajectory length T. The gap is decaying linearly as a
function of trajectory length Tuntil the variance error dominates beyond T= 50 .
38