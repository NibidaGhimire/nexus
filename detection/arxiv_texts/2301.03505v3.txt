Advances in Medical Image Analysis with Vision Transformers: A Comprehensive
Review
Reza Azad1, Amirhossein Kazerouni2, Moein Heidari2, Ehsan Khodapanah Aghdam3, Amirali Molaei4, Yiwei Jia1, Abin Jose1,
Rijo Roy1, Dorit Merhof†,5,6∗
aFaculty of Electrical Engineering and Information Technology, RWTH Aachen University, Aachen, Germany
bSchool of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran
cDepartment of Electrical Engineering, Shahid Beheshti University, Tehran, Iran
dSchool of Computer Engineering, Iran University of Science and Technology, Tehran, Iran
eFaculty of Informatics and Data Science, University of Regensburg, Regensburg, Germany
fFraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany
Abstract
The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad
interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependen-
cies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de
facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image
analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifi-
cally, we present a systematic and thorough review of relevant recent Transformer literature for di fferent medical image analysis
tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these
applications, we investigate the novelty, strengths and weaknesses of the di fferent proposed strategies and develop taxonomies
highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on di fferent datasets. Finally,
we summarize key challenges and discuss di fferent future research directions. In addition, we have provided cited papers with
their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer .
Keywords: Transformers, Medical Image Analysis, Vision Transformers, Deep Neural Networks
1. Introduction
Convolutional neural networks (CNNs) have been an integral
part of research in the field of medical image analysis for many
years. By virtue of convolutional filters whose primary func-
tion is to learn and extract necessary features from medical im-
ages, a wealth of research has been dedicated to CNNs ranging
from tumor detection and classification [1], detection of skin
lesions [2, 3] to brain tumor segmentation [4], to name only a
few. CNNs have also contributed significantly to the analysis
of different imaging modalities in clinical medicine, including
X-ray radiography, computed tomography (CT), magnetic res-
onance imaging (MRI), ultrasound (US), and digital pathology.
Despite their outstanding performance, CNNs su ffer from con-
ceptual limitations and are innately unable to model explicit
long-distance dependencies due to the limited receptive field of
convolution kernels. Moreover, the convolutional operator suf-
fers from the fact that at inference time, it applies fixed weights
regardless of any changes to the visual input. To mitigate the
aforementioned problems, there have been great research ef-
forts to integrate attention mechanisms, which can be regarded
as a dynamic weight adjustment process based on input features
∗Corresponding author: Dorit Merhof, Tel.: +49 (941) 943-68509, E-mail:
dorit.merhof@ur.de.to the seminal CNN-based structures to improve the non-local
modeling capability [5, 6, 7].
To this end, Wang et al. [8] designed a non-local flexible
building block, which can be plugged into multiple intermedi-
ate convolution layers. SENet [9] suggested a channel atten-
tion squeeze-and-excitation (SE) block, which collects global
information in order to recalibrate each channel accordingly
to create a more robust representation. Inspired by this line
of research, there has been an overwhelming influx of mod-
els with attention variants proposed in the medical imaging
field [10, 11, 12]. Although these attention mechanisms al-
low the modeling of full image contextual information, as the
computational complexity of these approaches typically grows
quadratically with respect to spatial size, they imply an in-
tensive computational burden, thus making them ine fficient in
the case of medical images that are dense in pixel resolution
[13]. Moreover, despite the fact that the combination of the
attention mechanism with the convolutional operation leads to
systematic performance gains, these models inevitably su ffer
from constraints in learning long-range interactions. Trans-
formers [14] have demonstrated exemplary performance on a
broad range of natural language processing (NLP) tasks, e.g.,
machine translation, text classification, and question answer-
ing. Inspired by the eminent success of Transformer architec-
tures in the field of NLP, they have become a widely applied
Elsevier November 7, 2023arXiv:2301.03505v3  [cs.CV]  5 Nov 2023 Transformers Classification Pure Transformers
 Hybrid Models
 Segmentation Pure Transformers
 Hybrid Models
 Other Architectu res
 Reconstruction Low Dose Enhancement
 Sparse-View Reconstruction
 Undersampled Reconstruction
 Super Resolution Reconstruction
 Report Generation Reinforcement Learning-based Systems
 Graph-based Systems
 Memory-based Systems
 Other Systems Registration Deformable Registration
 Affine Registration
 Rigid Registration Detection Backbone
 Neck
 Head Synthesizing Intra-Modality
 Inter-Modality
 Self-supervised ReconstructionFigure 1: Overview of the applications covered in this review.
technique in modern Computer Vision (CV) models. Since the
establishment of Vision-Transformers (ViTs) [15], Transform-
ers proved to be valid alternatives to CNNs in diverse tasks
ranging from image recognition [15], object detection [16], im-
age segmentation [17] to video understanding [18] and image
super-resolution [19]. As a central piece of the Transformer,
the self-attention mechanism comes with the ability to model
relationships between elements of a sequence, thereby learning
long-range interactions [20]. Moreover, Transformers allow for
large-scale pre-training for specific downstream tasks and ap-
plications and are capable of dealing with variable-length in-
puts. The immense interest in Transformers has also spurred
research into medical imaging applications (see Figure 1). Be-
ing dominant in reputable top-tier medical imaging conferences
and journals, it is extremely challenging for researchers and
practitioners to keep up with the rate of innovation. The rapid
adoption of Transformers in the medical imaging field necessi-
tates a comprehensive summary and outlook, which is the main
scope of this review. Specifically, this review provides a holis-
tic overview of the Transformer models developed for medical
imaging and image analysis applications. We provide a taxon-
omy of the network design, highlight the major strengths and
deficiencies of the existing approaches, and introduce the cur-
rent benchmarks in each task. We inspect several key tech-
nologies that arise from the various medical imaging applica-
tions, including medical image segmentation, medical image
registration, medical image reconstruction, and medical image
classification. So far, review papers related to Transformers do
not concentrate on applications of Transformers in the medi-
cal imaging and image analysis domain [21]. The few liter-
ature reviews that do focus on the medical domain [22, 23],
despite being very comprehensive, do not necessarily discuss
the drawbacks and merits of each method. In our work, we
explicitly cover this aspect and also provide a taxonomy that
comprises the imaging modality, organ of interest, and type oftraining procedure each paper has selected. More specifically,
in Section 3 (Medical Image Classification), we comprehen-
sively elaborate on the most promising networks along with
their key ideas, limitations, the number of parameters, and the
specific classification task they are addressing. In Section 4
(Medical Image Segmentation), we analyze network architec-
tures in terms of their design choice and propose a detailed
taxonomy to categorize each network to provide insight for the
reader to understand the current limitations and progress in seg-
mentation networks based on the Transformer architecture. In
Section 5 (Medical Image Reconstruction), we take a di ffer-
ent perspective to categorize networks based on their network
structure and the imaging modality they are built upon. We cat-
egorize the synthesis methods in Section 6 based on their ob-
jective (intra-modality or inter-modality) and then provide de-
tailed information regarding the network architecture, parame-
ters, motivations, and highlights. In the sections related to de-
tection (Section 7), registration (Section 8), and report genera-
tion (Section 9) we review in detail the state-of-the-art (SOTA)
networks and provide detailed information regarding the net-
work architectures, advantages, and drawbacks. Moreover, due
to the swift development of the field, we believe that the com-
munity requires a more recent overview of the literature.
We hope this work will point out new research options and
provide a guideline for researchers and initiate further inter-
est in the vision community to leverage the potential of Trans-
former models in the medical domain. Our major contributions
are as follows:
•We systematically and comprehensively review the ap-
plications of Transformers in the medical imaging do-
main and provide a comparison and analysis of SOTA ap-
proaches for each task. Specifically, more than 200 papers
are covered in a hierarchical and structured manner.
•Our work provides a taxonomized (Figure 1), in-depth
2Linear Pr ojection of Flattened PatchesPatch + Position
Embedding
Extra learnable
[class] embedding
Embedded
PatchesLNMulti-Head  
Self-Attention  LN MLPClass
Benign
Malignant
Transformer EncoderMLP HeadFigure 2: Architecture of the Vision Transformer as proposed in [15] and the detailed structure of the Vision Transformer encoder block. In the Vision Transformer,
sequential image patches are used as the input and processed using a Transformer encoder to produce the final classification output.
analysis (e.g. task-specific research progress and limita-
tions), as well as a discussion of various aspects.
•Finally, We discuss challenges and open issues and also
identify new trends, raise open questions and identify fu-
ture directions.
Search Strategy . We conducted a thorough search using
DBLP, Google Scholar, and Arxiv Sanity Preserver, utilizing
customized search queries that allowed us to obtain lists of
scholarly publications. These publications included peer-
reviewed journal papers, conference or workshop papers, non-
peer-reviewed papers, and preprints. Our search queries con-
sisted of keywords (transformer* |deep*|medical*|
{Task}*), (transformer |medical*), (transformer*
|medical*|image*|model*), (convolution* |
vision*|transformer*|medical*) , where {Task }
refers to one application covered in this review (see Figure 1).
To ensure the selection of relevant papers, we meticulously
evaluated their novelty, contribution, and significance, and
prioritized those that were the first of their kind in the field of
medical imaging. Following these criteria, we chose papers
with the highest rankings for further examination. It is worth
noting that our review may have excluded other significant
papers in the field, but our goal was to provide a comprehensive
overview of the most important and impactful ones.
Paper Organizations. The remaining sections of the paper
are organized as follows. In Section 2, we provide an overview
of the key components of the well-established Transformer ar-chitecture and its clinical importance. Moreover, this section
clarifies the categorization of neural network variants in terms
of the position where the Transformer is located. Section 3 to
Section 9 comprehensively review the applications of Trans-
formers in diverse medical imaging tasks as depicted in Fig-
ure 1. For each task, we propose a taxonomy to characterize
technical innovations and major use cases. Section 10 presents
open challenges and future perspectives of the field as a whole,
while finally, Section 11 concludes this work.
2. Background
In this section, we first provide an overview of the Trans-
former module and the key ideas behind its feasible design.
Then, we outline a general taxonomy of Transformer-based
models, characterized by their core techniques of using Trans-
formers, i.e., whether they are purely Transformer-based, or
whether the Transformer module is either used in the encoder,
decoder, bottleneck, or skip connection, respectively.
2.1. Transformers
The original Transformer [14] was first applied to the task for
machine translation as a new attention-driven building block.
The vanilla Transformer consists of an encoder and a decoder,
each of which is a stack of Ltandem of consecutive identi-
cal blocks. The Transformer module is convolutional-free and
3solely based on the self-attention mechanism or attention mech-
anism in short. Specifically, these attention blocks are neu-
ral network layers that relate di fferent positions of a single se-
quence to compute the sequence’s representation. Since the es-
tablishment of Transformer models, they have attained remark-
able performance in diverse natural language processing tasks
[24]. Inspired by this, Dosovitskiy et al. proposed the Vision
Transformer (ViT) [15] model as illustrated in Figure 2. When
trained on large datasets, for instance, JFT-300M, ViT outper-
forms the then state-of-the-art, namely ResNet-based models
like BiT [25]. In their approach, an image is turned into fixed-
sized patches before being flattened into vectors. These vectors
are then passed through a trainable linear projection layer that
maps them into Nvectors with the dimensionality of D×Nis
the number of patches. The outputs of this stage are referred
to as patch embeddings. To preserve the positional information
present within each patch, they add positional embeddings to
the patch embeddings. In addition to this, a trainable class em-
bedding is also appended to the patch embeddings before going
through the Transformer encoder. The Transformer encoder is
comprised of multiple Transformer encoder blocks. There is
one multi-head self-attention (MSA) block and an MLP block
in each Transformer encoder block. The activations are first
normalized using LayerNorm (LN) before going into these
blocks in the Transformer encoder block. Furthermore, there
are skip connections before the LN that add a copy of these ac-
tivations to the corresponding MSA or MLP block outputs. In
the end, there is an MLP block used as a classification head that
maps the output to class predictions. The self-attention mech-
anism is a key defining characteristic of Transformer models.
Hence, we start by introducing the core principle of the atten-
tion mechanism.
2.1.1. Self-Attention
In a self-attention layer (Figure 3a), the input vector is firstly
transformed into three separate vectors, i.e., the query vector
q, the key vector k, and the value vector vwith a fixed dimen-
sion. These vectors are then packed together into three di fferent
weight matrices, namely WQ,WK, and WV. A common form
ofQ,K, and Vcan be formulated as Equation (1) for an input
X
K=WKX,Q=WQX,V=WVX, (1)
where WK,WQ, and WVrefers to the learnable parameters. The
scaled dot-product attention mechanism is then formulated as
Attention( Q,K,V)=Softmax QKT
√dk!
V, (2)
where√dkis a scaling factor, and a softmax operation is ap-
plied to the generated attention weights to translate them into a
normalized distribution.
2.1.2. Multi-Head Self-Attention
The multi-head self-attention (MHSA) mechanism (Fig-
ure 3b) has been proposed [14] to model the complex relation-
ships of token entities from di fferent aspects. Specifically, the
MHSA block helps the model to jointly attend to informationfrom multiple representation sub-spaces, as the modeling ca-
pability of the single-head attention block is quite coarse. The
process of MHSA can be formulated as
MultiHead( Q,K,V)=[Concat (head 1,..., head h)]WO,(3)
where head i=Attention
QWQ
i,KWK
i,VWV
i
, and WOindi-
cates a linear mapping function to combine multi-head repre-
sentation. Note that his a hyper-parameter set to h=8 in the
original paper.
MatMulScaleSoftMaxMatMul
(a) Self-Attention
Linear LinearScaled Dot-Pr oduct  
Attention  
LinearConcatLinear (b) Multi-Head Self-Attention
Figure 3: (a) The process of self-attention. (b) Multi-head attention. The MSA
consists of multiple SA blocks (heads) concatenated together channel-wise as
proposed in [14].
2.2. Transformer modes
While the Transformer was originally introduced with an
encoder-decoder pipeline, many modern architectures gener-
ally exploit the Transformer architecture in di fferent fashions,
which generally depend on the target application. The usage of
Transformers in vision tasks can broadly be classified into pure
and hybrid designs.
2.2.1. Pure Transformers
Due to the deficiency of CNN-based architectures in learning
global and long-range semantic information interactions, which
stems from the locality of convolution operation, a cohort study
has investigated the purely Transformer-based models without
any convolution layer. These models usually consist of encoder,
bottleneck, decoder, and skip connections directly built upon
the ViT or its variants. In this criteria, there are usually multiple
multi-head self-attention modules in both encoding and decod-
ing sections that allow the decoder to utilize information from
the encoder. Examples of such methods are the Swin-Unet [26]
and the TransDeepLab [27] networks which, as their name sug-
gests, try to model the seminal U-Net [28], and DeepLab [29]
architectures.
2.2.2. Transformer: Hybrid
The hybrid Transformer models usually modify the base
CNN structure by replacing the encoder or decoder modules.
4Encoder : Encoder-only models such as the seminal BERT
[30] are designed to make a single prediction per input or a sin-
gle prediction for an entire input sequence. In the computer
vision era, these models are applicable for classification tasks.
Moreover, as utilizing a pure Transformer can result in lim-
ited localization capacity stemming from inadequate low-level
features, many cohort studies try to combine CNN and Trans-
former in the encoding section [17]. Such a design can enhance
finer details by recovering localized spatial information.
Decoder: Transformers can also be used in a decoding fash-
ion. Such a causal model is typically used for generation tasks
such as language modeling. Besides that, the modification can
apply to the skip connections of the decoder module. Skip con-
nection is a widely used technique to improve the performance
and the convergence of deep neural networks. It can also serve
as a modulating mechanism between the encoder and the de-
coder. To e ffectively provide low-level spatial information for
the decoding path, the idea of exploiting Transformers in de-
signing skip connections has emerged. This notable idea can
lead to finer feature fusion and recalibration while guaranteeing
the aggregation scheme of using both high-level and low-level
features [17, 31].
2.3. Clinical Importance
In practice, medical professionals conduct medical image
analysis qualitatively in real-world scenarios. This can lead to
different understandings and levels of precision due to varia-
tions in the reader’s expertise or di fferences in image quality,
as well as being timely and labor-expensive. Therefore, deep
learning techniques have gained extensive attraction in medi-
cal image analysis aiming to reduce inter-reader variation and
decrease the expenses associated with time and workforce [22].
The fundamental question that emerges is What are the
motivations behind using Transformers in the medical do-
main?
Advances in adversarial attacks create an unavoidable danger
in which potential attackers might attempt to gain benefits by
exploiting vulnerabilities in the healthcare system. While there
is substantial literature available concerning the robustness of
CNNs in the field of medical imaging, it is still a challenging
direction to explore. Recent studies [32] prove that ViTs are
more robust to adversarial attacks than CNNs. Specifically, ViT
is significantly more robust than CNN in a wide range of white-
box attacks. A similar trend is also observed in the query-based
and transfer-based black-box attacks which can be attributed to
the fact that ViTs are more reliant on low-frequency (robust)
features while CNNs are more sensitive to high-frequency fea-
tures.
Besides, due to strict privacy rules, low occurrence rates of
certain diseases, concerns about data ownership, and a limited
number of patients, providing suitable data have always been a
paramount concern in the medical domain. Recently, the idea
of exploiting the inherent structure of ViT in distributed med-
ical imaging applications has emerged. Owning to the inher-
ent structure of ViT (as opposed to CNN), it can be split into
shared body and task-specific heads which demonstrate that a
ViT body with su fficient capacity can be shared across relevanttasks by leveraging di fferent strategies such as multi-task learn-
ing [33]. This way, ViTs can be a paramount application in
decentralized medical imaging solutions.
Moreover, Transformers excel at capturing long-range de-
pendencies in data, making them well-suited for analyzing
complex medical images where contextual information plays
a crucial role. Transformers o ffer greater interpretability than
CNNs, as they provide attention maps that highlight the re-
gions of an image that contribute most to the model’s decision-
making. This transparency is crucial in medical settings where
understanding the reasoning behind a diagnosis is important.
The flexible design of ViTs can be used on edge devices to
speed diagnosis, improve clinician workflows, enhance patient
privacy, and save time. Below, we provide recent examples of
clinical use cases of Transformers in the medical domain.
Med-PaLM 2 [34], a Transformer-based model, by Google
demonstrated a feasible comparison between physicians and the
application of Transformers in the medical domain questioning.
The answers provided by Med-PaLM 2 are pretty impressive,
and the authors provided a metric to assess model performance
by exposing the clinician answers and the model’s output to
other physicians and lay-persons to rate them in nine categories
such as factuality, medical reasoning capability, etc. As a result,
the quantitative results endorse that 72.9% of the time, answers
provided by Med-PaLM 2 are preferable regarding reporting
the medical consensus.
Seenivasan et al. [35] addressed the knowledge transfer’s
steep learning curve from surgical experts to medical trainees
and patients and proposed the SurgicalGPT , a Transformer-
based multi-modal visual question-answering framework that
utilizes the large language models with visual cues. Surgical-
GPT demonstrated outperforming results over several (robotic)
surgical datasets in terms of accuracy compared to the uni-
modality text generation models.
Consequently, the clinical use cases of Med-PaLM 2 and Sur-
gicalGPT have demonstrated the feasibility and benefits of em-
ploying Transformers in the medical domain, showing their ef-
fectiveness and potential applications.
3. Medical Image Classification
Image classification is still one of the challenging problems
in computer vision, which aids in segregating extensive quanti-
ties of data into meaningful categories [57]. Vision Transform-
ers (ViT) have recently demonstrated outstanding results in var-
ious image classification tasks and o ffer significant advantages
over conventional CNNs [58, 59, 60, 61, 62, 63]. These advan-
tages include long-range relationships, adaptive modeling, and
attention maps that yield intuition on what the model deems
more important inside an image [39]. Due to these alluring ad-
vantages, there is rising interest in building Transformer-based
models for medical image classification. Therefore, highly pre-
cise classification is becoming increasingly vital for facilitating
clinical care.
In this section, we exhaustively examine ViTs in medical im-
age classification. As illustrated in Figure 4, we have broadly
classified these methods based on the role ViT plays in their
5 Medical Image 
 Classification
 Pure
 Original ViT Struture
 1. COVID-VIT
 2. MIL-ViT
 3. XViTCOS
 4. FESTA
 5. ViT-vs-CNN
 6. ViT-BUS
 7. Covid-Transformer
 8. Self MedFed Other ViTs
 9. POC-Former
 10. RadioTransformer
 11. COVID-VOLO Hybrid
 Original ViT Structure
 12. TransMed
 13. 3DMET
 14. Femur-ViT
 15. Hybrid-Covid-ViT Other ViTs
 16. LAT
 17. DT-MIL
 18. TransMIL
 19. HATNet
 20. GTP
 21. HiFuse
 22. MedViTFigure 4: Taxonomy of ViT-based approaches in medical image classification. Methods are categorized based on their proposed architecture into pure and hybrid
methods, in which they adopt the vanilla ViT or present a new type of vision Transformer in medical image classification. Notably, we utilize the prefix numbers
in the paper’s name in ascending order and denote the reference for each study as follows: 1. [36], 2. [37], 3. [38], 4. [33], 5. [39], 6. [40], 7. [41], 8. [42], 9.
[43], 10. [44], 11. [45], 12. [46], 13. [47], 14. [48], 15. [49], 16. [50], 17. [51], 18. [52], 19. [53], 20. [54], 21. [55], 22.[56].
architecture. These categories include pure Transformers and
Hybrid Models. Generally, a vision Transformer-based clas-
sification architecture consists of three modules: (1) a back-
bone for capturing input features, (2) an encoder for model-
ing the information, and (3) a classification head for generat-
ing output based on the specified task. Therefore, the Trans-
former can be adopted in each module. However, some works,
including Lesion Aware Transformer (LAT) [50] and De-
formable Transformer for Multi-Instance Learning (DT-MIL)
[51], take a di fferent approach and utilize encoder-decoder
structures. LAT proposes a unified encoder-decoder system for
Diabetic Retinopathy (DR) grading, and DT-MIL introduces
a Transformer-based encoder-decoder architecture for classi-
fying histopathological images, where the deformable Trans-
former was embraced for the encoder part. In the following, we
will go into great depth on both hybrid and pure models.
3.1. Pure Transformers
Since the emergence of Transformers, there has been a grow-
ing debate regarding whether it is time to switch entirely from
CNNs to Transformers. Matsoukas et al. [39] conduct a se-
ries of experiments to answer this critical question. They take
ResNet50 [64] and the DeiT-S [65] models to represent CNN
and ViT models, respectively. They train each of these two
models in 3 di fferent fashions: a) randomly initialized weights,
b) pre-trained on ImageNet (transfer learning), and c) pre-
training on the target dataset in a self-supervised learning (SSL)
scheme using DINO [66]. Their findings show that when utiliz-
ing random initialization, ViTs are inferior to CNNs. In the case
of transfer learning, the results are similar for both models, with
ViT being superior for two out of three datasets. Additionally,
ViT performs better when self-supervision on the target data is
applied. They conclude that Vision Transformers, indeed, are
suitable replacements for CNNs.Transformers have had a profound e ffect on medical devel-
opment. Researchers have thoroughly investigated adopting the
ViT in medical image classification tasks since its introduction.
However, the limited number of medical images has hindered
Transformers from replicating their success in medical image
classification. ViT-BUS [40] studies the use of ViTs in medi-
cal ultrasound (US) image classification for the first time. They
propose to transfer pre-trained ViT models based on the breast
US dataset to compensate for the data-hunger of ViTs. Eval-
uated results on B [67], BUSI [68], and B +BUSI datasets in-
dicate the predominance of attention-based ViT models over
CNNs on US datasets. Likewise, COVID-Transformer [41]
utilizes ViT-L /16 to detect COVID from Non-COVID based on
CXR images. Due to the limitation of su fficient data, they intro-
duce a balanced dataset containing 30K chest X-ray images for
multi-class classification and 20K images for binary classifica-
tion. The published dataset is created by merging datasets [69],
[70], and [71]. They fine-tune the model on the dataset with
a custom MLP block on top of ViT to classify chest X-ray
(CXR) images. Moreover, COVID-Transformer exploits the
GradCAM Map [72] to visualize a ffected lung areas that are
significant for disease prediction and progression to display the
model interpretability. Similarly, Mondal et al. [38] present
xViTCOS for detecting COVID-19 in CTs and CXRs. xViT-
COS employs a model that has been pre-trained on ImageNet-
21k [73]. Nevertheless, the training data capacity might over-
shadow the generalization performance of the pre-trained ViT
to transfer the knowledge from the learned domain to the tar-
get domain. By training the model on the COVIDx-CT-2A
dataset [74], a moderately-sized dataset, xViTCOS overcomes
this problem. However, due to the shortage of the insu fficient
amount of CXR images, the pre-trained ViT model is fine-
tuned using the CheXpert dataset [75]. In addition, xViTCOS
6leverages the Gradient Attention Rollout algorithm [76] to vi-
sually demonstrate the model’s prediction on the input image
for clinically interpretable and explainable visualization. In
experiments using COVID CT-2A and their custom-collected
Chest X-ray dataset, xViTCOS significantly outperforms con-
ventional COVID-19 detection approaches. MIL-VT [37] sim-
ilarly suggests pre-training the Transformer on a fundus image
large dataset beforehand, initialized by the pre-trained weight
of ImageNet, then fine-tuning it on the downstream retinal dis-
ease classification task in order to encourage the model to learn
global information and achieve generalization. Unlike previ-
ous approaches, they apply some modifications to the vanilla
ViT structure. In the classic ViT, embedded features are ne-
glected for classification; instead, only the class token, which
retains the summarization of embedded features’ information,
is used. Yu et al. [37] propose a novel multiple-instance learn-
ing (MIL)-head module to exploit those embedded features to
complement the class token. This head comprises three sub-
modules that attach to the ViT in a plug-and-play manner: 1)
the MIL embedding submodule that maps the feature embed-
dings to a low-dimensional embedding vector, 2) the attention
aggregation submodule that outputs a spatial weight matrix for
the low-dimensional patch embeddings; this weight matrix is
then applied to the low-dimensional embeddings to ascertain
each instance’s importance, 3) the MIL classifier submodule
that determines the probability of each class through aggre-
gated features. In the downstream task, both MLP and MIL
heads use the weighted cross-entropy loss function for train-
ing. The outputs of both heads are then weight-averaged for
the inference time. Results indicate the e ffectiveness of the
proposed training strategy and the MIL-head module by dra-
matically boosting the performance over APTOS2019 [77] and
RFMiD2020 [78] datasets when compared to CNN-based base-
lines. In contrast to the previous 2D-based methods that employ
transfer learning, COVID-ViT [36] proposes training ViT to
classify COVID and non-COVID cases using 3D CT lung im-
ages. Given that a COVID volume may contain non-COVID 2D
slices, COVID-ViT applies a slice voting mechanism after the
ViT classification result in which the subject is categorized as
having COVID if more than a certain percentage of slices (e.g.,
25%) are predicted to be COVID. The findings reported for the
MIA-COVID19 competition [79] confirm that ViT outperforms
CNN-based approaches such as DenseNet [80] in identifying
COVID from CT images.
Besides the remarkable accuracy of Transformers compared
to CNNs, one of their major drawbacks is their high computa-
tional cost, thereby making them less e ffective for real-world
applications, such as detecting COVID-19 in real time. In light
of the prevalence of COVID-19, rapid diagnosis will be bene-
ficial for starting the proper course of medical treatment. CXR
and lung CT scans are the most common imaging techniques
employed. However, CT imaging is a time-consuming process,
and using CXR images is unreliable in identifying COVID-19
in the early stage. In addition, vision Transformers are compu-
tationally expensive to deploy on mobile devices for real-time
COVID-19 classification. Therefore, Perera et al. [43] present a
lightweight Point-of-Care Transformer (POCFormer) . Thecompactness of POCFormer allows for real-time diagnosis of
COVID-19 utilizing commercially accessible POC ultrasound
devices. POCFormer reduces the complexity of the vanilla ViT
self-attention mechanism from quadratic to linear using Lin-
former [81]. The results display the superiority of POCFormer
in the real-time detection of COVID-19 over the CNN-based
SOTAs on the POCUS dataset [82].
In addition, despite the great potential shown by ViTs in Im-
ageNet classification, their performance is still lower than the
latest SOTA CNNs without additional data. These Transform-
ers mainly focus on a coarse level by adopting a self-attention
mechanism to establish global dependency between input to-
kens. However, relying only on a coarse level restricts the
Transformer’s ability to achieve higher performance. Thus, Liu
et al. [45] leverage a pre-trained version of VOLO for an X-ray
COVID-19 classification. VOLO [83] first encodes fine-level
information into the token representations through proposed
outlook attention, alleviating the limitations of Transformers
that require a large amount of data for training, and second ag-
gregates the global features via self-attention at the coarse level.
Through the outlook attention mechanism, VOLO dynamically
combines fine-level features by treating each spatial coordinate
(i,j) as the center of a K×Klocal window and calculating its
similarity with all its neighbors. The findings indicate that fine-
tuning VOLO on Dataset-1 [84] leads to 99 .67% top1 accuracy
on Dataset-1 test cases and 98 .98% top1 accuracy on unseen
Dataset-2 [85], which demonstrates the generality of the ap-
proach.
Furthermore, accessible labeled images have considerably
influenced research on the use of Transformers to diagnose
COVID-19. Considering the shortage of labeled data, data shar-
ing between hospitals is needed so as to create a viable central-
ized dataset. However, such collaboration is challenging due
to privacy concerns and patient permission. Motivated by Fed-
erated Learning (FL) and Split Learning (SL), Park et al. [33]
present a Federated Split Task-Agnostic (FESTA) framework
that uses ViT for multi-task learning of classification, detection,
and segmentation of COVID-19 CXR images. FESTA benefits
from the decomposable modular design of ViT to train heads
and tails via clients and share the server-side Transformer body
across clients to aggregate extracted features and process each
task. The embedded features from the body Transformer are
then passed to their task-specific tail on the client side to pro-
duce the final prediction (Figure 5(a)). Figure 5(b) illustrates
the single-task learning scheme and (c) the multi-task learning
scheme. In multi-task learning, heads, tails, and a task-agnostic
Transformer body are first jointly trained for 6000 rounds (see
Figure 5(c)). Then, heads and tails are fine-tuned according to
the desired specific task while freezing the weights of the Trans-
former body. FESTA merits from 220000 decentralized CXR
images and attains competitive results compared to the data-
centralized training approaches. The experimental results also
demonstrate the stable generalization performance of FESTA,
where multi-task learning enhances the performance of the in-
dividual tasks through their mutual e ffect during training.
Most attention-based networks utilized for detection and
classification rely on the neural network to learn the neces-
7Figure 5: Overview of the FESTA framework [33], which utilizes ViT for
multi-task learning of COVID-19 CXR classification, detection, and segmenta-
tion. (a) FESTA leverages ViT’s decomposable modular design to train heads
(H) and tails (T) via clients while sharing the server-side Transformer body
(B) between clients to integrate retrieved features. Final predictions are then
derived by feeding embedded features to their task-specific tails on the client
side. (b) illustrates the single-task learning scheme, and (c) two steps multi-task
learning scheme. The former is trained for 12000 rounds, while the latter un-
dergoes two training steps. First, the whole parts train in 6000 rounds. Then by
freezing the weights of the Transformer body, the heads and tails are fine-tuned
for 6000 steps based on the desired specific task.
sary regions of interest. Bhattacharya et al. [44] in Radio-
Transformer argue that in certain applications, utilizing ex-
perts’ opinions can prove beneficial. Specifically, they apply
this notion to leverage radiologists’ gaze patterns while di-
agnosing di fferent diseases on medical images; then, using a
teacher-student architecture, they teach a model to pay attention
to regions of an image that a specialist is most likely to examine.
As illustrated in Figure 6, The teacher and the student networks
consist of two main components: global and focal. The global
component learns coarse representation while the focal module
works on low-level features, and both these segments are com-
prised of Transformer blocks with shifting windows. In addi-
tion, the global and focal components are interconnected using
two-way lateral connections to form the global-focal module;
this is to address the inherent attention gap between the two.
The teacher network is first directly pre-trained on human vi-
sual attention maps. Then, the entire model is trained for di ffer-
ent downstream tasks, e.g., object detection and classification.
Furthermore, the authors propose a self-supervised Visual At-
tention Loss (V AL) that incorporates both GIoU and MSE loss.
The student network is trained to predict probability values for
different classes and attention regions. These attention regions
are then compared to those obtained from the teacher model,
and the weights are optimized using V AL.
3.2. Hybrid Models
In spite of the vision Transformers’ ability to model global
contextual representations, the self-attention mechanism un-
dermines the representation of low-level details. CNN-
Transformer hybrid approaches have been proposed to ame-
liorate the problem above by encoding both global and local
features using the locality of CNNs and the long-range depen-
dency of Transformers.
Visual Attention Loss  
TEACHER STUDENT
Global-Focal Global-Focal
Human V isual Attention
Training
Input Image  
Visual Attention Predicted Attention  TEACHERGlobal-Focal
HVAT
Disease Classification
Eye gaze points  Figure 6: Overview of RadioTransformer [44]. Human Visual Attention Train-
ing (HV AT) block first uses radiologists’ visual observations of chest radio-
graphs to train a global-focal teacher network. The pre-trained teacher network
is then utilized to distill the teacher’s knowledge to a global-focal student net-
work through visual attention loss, enabling the student to learn visual infor-
mation. Following the teacher-student strategy and incorporating radiologist
visual examinations leads to an improvement in the classification of disease on
chest radiographs.
TransMed [46] proposes a hybrid CNN-Transformer net-
work that leverages the locality of CNNs and the long-range
dependency character of Transformers for parotid gland tumor
and knee injury classification. Multimodal medical images pri-
marily have long-range interdependencies, and improving per-
formance requires an e ffective fusion strategy. TransMed pro-
poses a novel image fusion strategy. Firstly, three neighboring
2D slices of a multimodal image are overlaid to create three-
channel images. Then, each image is partitioned into K×K
patches. This fusion approach allows the following network
to learn mutual information from images of di fferent modali-
ties. Patch tokens are fed into a CNN network to capture their
low-level features and generate patch embeddings. The classic
ViT is then used to determine the relationship between patch se-
quences. TransMed’s final results verify the e ffectiveness of hy-
brid models in classifying multimodal medical images by out-
performing all its counterparts by a large margin. TransMed-S
enhances average accuracy on the PGT dataset by about 10 .1%
over its nearest counterpart, BoTNet [87], while requiring fewer
parameters and FLOP count. Comparably, Tanzi et al. [48] de-
velop a new CAD system (Femur-ViT) based on Vision Trans-
formers for diagnosing femoral fractures. First, YOLOv3 [88]
is utilized to detect and crop the left and right femur regions.
Afterward, a CNN (InceptionV3 [89]) and a hierarchical CNN
(different InceptionV3 networks in cascade) [90] are applied to
the dataset, and the results serve as baselines for the classifi-
cation. Then, they use a modified ViT to classify seven di ffer-
ent fracture types. Finally, a clustering approach is proposed
as an evaluation technique for the ViT encoder. This study
highlights the power of using ViT models for medical image
classification and the ability of the proposed CAD system to
significantly increase clinicians’ diagnostic accuracy. 3DMeT
[47] proposes applying a 3D medical image Transformer for
assessing knee cartilage defects in three grades: grade 0 (no de-
fect), grade 1 (mild defect), and grade 2 (severe defect). Primar-
8Image
Bags
Words
n
B1I
W0
m
 W1
3
 W1
2
 W1
1n
CNN
n
mdB1
cnnWord-to-word
attentionBw2wWord-to-bag
AttentionWord-to-bag
attention
n
d
n
d Bw2b/hatwideBw2bBag-to-bag
attentionBb2bBag-to-image
attention
Ib2i∈Rd
Classifier Benign
Atypia
DCIS
Invasive
Bi
cnnMulti-head
attentionFeed forward
network (FFN) Bi
w2w
Word-to-word attention/hatwideBw2b
Bw2bMulti-head
attention
Multi-head
attentionFeed forward
network (FFN) Bb2b/hatwideBb2bBag-to-bag attention
Bi
w2wFunction
ΨLinear Softmax
Dot-product Bi
w2b
Word-to-bag attention
(a)(d)
(c)
(b)Figure 7: The overall architecture of [86]. HATNet hierarchically divides an input image into n×mwords, which are then fed into the CNN encoder to provide
word-level representations for each bag. Then by performing a bottom-up decoding strategy and applying a linear classifier, breast biopsy classification results
are obtained. Notably, bag-to-image attention has the same procedure as word-to-bag attention, shown in (c).
ily, using medical 3D volumes as an input to the Transformer
is computationally expensive, thereby making it impractical.
3DMeT resolves the high computational cost problem by re-
placing conventional linear embedding with 3D convolutional
layers. The weights of convolutional layers are adopted using
the teacher-student training strategy. 3DMeT takes an exponen-
tial moving average from the first one /few-layer(s) of the CNN
teacher’s weights and uses it as convolutional layers’ weights.
This method enables Transformers to be compatible with small
medical datasets and to benefit from CNNs’ spatial inductive
biases. Lastly, the Transformer and CNN teacher’s outputs are
combined in order to derive the classification results.
Operating Transformers over Whole Slide Images (WSIs)
is computationally challenging since WSI is a gigapixel im-
age that retains the original structure of the tissue. MIL and
CNN backbones have demonstrated practical tools for acting
on WSI. MIL is a weakly supervised learning approach that
enables deep learning methods to train high-resolution images
like WSI. Since annotating such images at the pixel level is
impractical, MIL proposes to divide an input WSI into a bag
of instances and assign a single label to the bag of each im-
age based on pathology diagnosis. The bag has a positive label
if it contains at least one positive instance, and it is consid-
ered negative if all the instances in the bag are negative. Then
CNN backbones are employed to down-sample and extract the
features of each instance and allow Transformers to operate
according to the generated feature maps and currently avail-
able hardware. Therefore, DT-MIL [51] proposes to compress
WSIs into compact feature images by embedding each patch of
the original WSI into a super-pixel at its corresponding posi-
tion using E fficientNet-B0 [91]. The resulting thumbnail image
feed into a 1×1 Conv for feature reduction, followed by a de-
formable Transformer encoder that aggregates instance repre-
sentations globally. A similar approach is adopted by Holistic
ATtention Network (HATNet) [53], where they first divide an
input image into nnon-overlapping bags, each broken down
intomnon-overlapping words (or patches). n×mwords are
fed into the CNN encoder to obtain word-level representationsfor each bag. HATNet aims to develop a computer-aided di-
agnosis system to help pathologists in reducing breast cancer
detection errors. According to the World Health Organization
(WHO), breast cancer is the most frequent non-skin cancer in
women, accounting for one out of every four new female can-
cers annually [92]. As illustrated in Figure 7, HATNet follows
a bottom-up decoding strategy such that it first performs multi-
head attention to words in a word-to-word attention block, then
considers the relationship between words and bags in word-to-
bag attention , followed by bag-to-bag attention to attain inter-
bag representations. The acquired bag features are then ag-
gregated in bag-to-image attention to build image-level repre-
sentations. A linear classifier is ultimately applied to achieve
the final results. Furthermore, unlike most MIL methods that
take all the instances in each bag independent and identically
distributed [93, 94, 95], TransMIL [52] suggests that it is es-
sential to consider the correlation between di fferent instances
and explore both morphological and spatial information. Two
Transformer layers address the morphological information, and
a conditional position encoding layer named Pyramid Position
Encoding Generator (PPEG) addresses the spatial information.
The proposed PPEG module has two merits: 1) It handles posi-
tional encoding of sequences with a variant number of instances
by using group convolution over the 2D reshaped patch tokens,
and 2) It enriches the features of tokens by capturing more con-
text information through convolutions. In contrast to conven-
tional IID-based MIL methods requiring many epochs to con-
verge, TransMIL converges two to three times faster by using
morphological and spatial information. TransMIL also outper-
forms all the latest MIL methods [96, 97, 98, 99, 93] in terms of
accuracy and AUC by a significant margin in binary and multi-
ple classification tasks and exhibits the superiority of taking the
correlation between di fferent instances into account and con-
sidering both morphological and spatial information.
Previous methods mainly rely on weakly supervised learn-
ing or dividing WSIs into image patches and using supervised
learning to assess the overall disease grade. Nevertheless, these
approaches overlook WSI contextual information. Thus, Zheng
9Table 1: An overview of the reviewed Transformer-based medical image classification models.
Method Modality Organ Type Pre-trained Module: Type Datasets Metrics Year
Pure
ViT-vs-CNN [39]Fundus
Dermoscopy
X-rayEye
Skin
Breast2D ViT: Self-supervised & Supervised1APTOS-2019 [77]
2ISIC-2019 [100]
3CBIS-DDSM [101]Kappa
Recall
ROC-AUC2021
ViT-BUS [40] Ultrasound Breast 2D ViT: Supervised1B [67]
2BUSI [68]ACC
AUC2022
POCFormer [43] Ultrasound Chest 2D ✗ POCUS [82]Recall, F1
SP, SE, ACC2021
MIL-VT [37] Fundus Eye 2D ViT: Supervised1Private Dataset
2APTOS-2019 [77]
3RFMiD-2020 [78]Recall, F1
ACC, AUC
Precision2021
COVID-VIT [36] CT Chest 3D ✗ MIA-COV19 [79] ACC, F1 2021
xViTCOS [38]X-ray
CTChest 2D ViT: Supervised1COVID CT-2A [74]
2CheXpert [75]Recall, F1
Precision
SP, NPV2021
FESTA [33] X-ray Chest 2D ViT: Supervised1Four Private Datasets
2CheXpert [75],3BIMCV [102]
4Brixia [103],5NIH [104]
6SIIM-ACR [105],7RSNA [106]Recall, F1
SP, SE, AUC2021
COVID-Transformer [41] X-ray Chest 2D ViT: Supervised1[69],2[71]
3[70]Recall, F1
ACC, AUC
Precision2021
COVID-VOLO [45] X-ray Chest 2D ViT: Supervised1[84]
2[85]ACC 2021
RadioTransformer [44] X-ray Chest 2D ViT: Supervised1RSNA [107],2Cell Pneumonia [108]
3COVID-19 Radiography [84, 109]
4NIH [104],5VinBigData [110]
6SIIM-FISABIO-RSNA [111]
7RSNA-MIDRC [112, 113]
8TCIA-SBU COVID-19 [114, 115]Recall, F1
ACC, AUC
Precision2022
Self MedFed [42]X-ray
Dermoscopy
FundusChest
Skin
Eye2D ViT: Self-supervised1EyePACKS [116]
2ISIC 2017 [117],3ISIC 2018 [118]
4ISIC 2020 [119]
5COVID-FL [42]ACC 2023
Hybrid
TransMIL [52] Microscopy Multi-organ 2D CNN: Supervised1Camelyon16 [120]
2TCGA-NSCLC [121, 122]
3TCGA-RCC [123]ACC
AUC2021
LAT [50] Fundus Eye 2D CNN: Supervised1Messidor-1 [124]
2Messidor-2 [125]
3EyePACKS [116]AUC & Kappa 2021
TransMed [46] MRIEar
Knee3DViT: Supervised
CNN: Supervised1PGT [46]
2MRNET [126]Precision
ACC2021
3DMeT [47] MRI Knee 3D CNN: Supervised Private dataset ACC, Recall, F1 2021
Hybrid-COVID-ViT [49] X-ray Chest 2D CNN: Supervised CheXpert [75]AUC, ACC
SP, SE2021
Femur-ViT [48] X-ray Femur 2DViT: Supervised
CNN: UnsupervisedPrivate datasetRecall, F1
Precision, ACC2022
DT-MIL [51] MicroscopyLung
Breast2D CNN: Supervised1CPTAC-LUAD [114]
2BREAST-LNM [51]Recall, F1
AUC, precision2021
GTP [54] Microscopy Lung 2D CNN: Self-supervised1NLST [127],2CPTAC [128]
3TCGA [129]Precision, Recall
SP, SE, ACC, AUC2022
HATNet [53] Microscopy Breast 2D CNN: Supervised Breast Biopsy WSI Dataset [130]ACC, ROC-AUC
F1, SP, SE2022
HiFuse [55]CT
Dermoscopy
EndoscopySkin
Chest
Stomach2D ✗1ISIC 2018 [118]
2COVID19-CT [131]
3Kvasir [132]Recall, ACC
F1, Precision2022
MedViT [56]CT
X-ray
ultrasound
OCTMulti-organ 2D ✗ MedMNIST [133]AUC, ACC
Top-12023
et al. [54] propose a Graph-based Vision Transformer (GTP)
framework for predicting disease grade using both morphologi-
cal and spatial information at the WSIs. The graph term allowsfor the representation of the entire WSI, and the Transformer
term allows for computationally e fficient WSI-level analysis.
The input WSI is first divided into patches, and those that con-
10Table 2: A brief description of the reviewed Transformer-based medical image classification models. The unreported number of parameters indicates that the
value was not mentioned in the paper, and the code was unavailable.
Method # Params Contributions Highlights
Pure
ViT-vs-CNN [39] 22M •They investigate three di fferent weight initialization approaches on three medical datasets: (1) random
initialization, (2) transfer learning using supervised ImageNet pre-trained weights, and (3) self-supervised
pretraining on the target dataset using DINO [66]. Their final verdict is that ViTs can replace CNNs.•Utilize three di fferent training schemes on three di fferent datasets to conclude whether ViT can replace
CNN.
•They repeat their processes five times to be certain of the outcome.
•Comparing only two models, DeiT-S and Resnet-50, on only three datasets cannot generalize the conclu-
sion of the superiority of each of the Transformer and CNN.
ViT-BUS [40]ViT-Ti /16: 5.53M
ViT-S /32: 22.87M
ViT-B /32: 87.44M•Proposes the use of ViT for the classification of breast ultrasound images for the first time. •Transferring pre-trained ViT models based on small ultrasound datasets yields much higher accuracy than
CNN models.
POCFormer [43]Binary CLS: 2.8M
Multiclass CLS: 6.9M•Proposes a lightweight Transformer architecture that uses lung ultrasound images for real-time detection
of COVID-19.•POCFormer can perform in real-time and be deployed to portable devices.
•POCFormer can be used for rapid mass testing of COVID-19 due to its compactness.
MIL-VT [37] 22.12M •Proposes to first pre-train the Vision Transformer on a fundus image large dataset and then fine-tune it on
the downstream task of the retinal disease classification.
•Introduces the MIL-VT framework with a novel Multiple Instance Learning(MIL)-head to e ffectively
utilize embedding features to improve the ViT performance.•The MIL head can significantly enhance the performance by easily attaching to the Transformer in a
plug-and-play manner.
•MIL-VT e fficiently exploits the embedding features overlooked in the final prediction of the ViT.
COVID-VIT [36] 52.81M •Offers to utilize ViT to classify COVID and non-COVID patients using 3D CT lung images. •COVID-ViT performs better in classifying COVID from Non-COVID compared to DenseNet.
•The reported result is not enough to conclude. They only compare ViT with DenseNet.
xViTCOS [38] 85.99 •Proposes and explores using ViT for detecting COVID-19 from CXR and CT images.
•xViTCOS makes use of the Gradient Attention Rollout algorithm [76] for visualization and clinical inter-
pretability of the output.•Uses a heatmap plot to demonstrate the model’s explainability.
FESTA [33]Body: 66.37M
(CLS) Head: 13.31M, Tail: 2k
(SEG) Head: 15.04M, Tail: 7.39M
(DET) Head: 25.09M, Tail: 19.77M•Proposes a Federated Split Task-Agnostic (FESTA) framework that leverages ViT to merit from federated
learning and split learning.
•They use multi-task learning to classify, detect, and segment COVID-19 CXR images.•The proposed FESTA Transformer improved individual task performance when combined with multi-task
learning.
•Experimental results demonstrate stable generalization and SOTA performance of FESTA in the external
test dataset even under non-independent and non-identically distributed (non-IID) settings.
•FESTA eliminates the need for data exchange between health centers while maintaining data privacy.
•Using FESTA in the industry may not be safe because it may encounter external attacks on the server
that may lose the parameters of the entire network. In addition, using this method may jeopardize patient
information through privacy attacks.
•Authors did not evaluate the robustness of their approach to di fficulties through communication, strag-
glers, and fault tolerance.
COVID-Transformer [41] ViT-L /16: 307M •COVID-Transformer investigates using ViT for detecting COVID-19 from CXR images.
•COVID-Transformer introduces a new balanced chest X-ray dataset containing 30K images for multi-
class classification and 20K for binary classification.•Uses a heatmap plot to demonstrate the model’s explainability.
COVID-VOLO [45] 86.3M •Proposes fine-tuning the pre-trained VOLO [83] model for COVID-19 diagnosis. •Using VOLO enables capturing both fine-level and coarse-level features resulting in higher performance
in COVID-19 binary classification.
RadioTransformer [44] 3.93M •Presents a novel global-focal RadioTransformer architecture, including Transformer blocks with shifting
windows, to improve diagnosis accuracy by leveraging the knowledge of experts.
•Introduces an innovative technique for training student networks by utilizing visual attention regions
generated by teacher networks.•outperforms counterpart backbones on multiple datasets.
•Model’s explainability
Self MedFed [42] 85.2M •Self MedFed is a novel privacy-preserving federated self-supervised pre-training framework designed to
learn visual representations from decentralized data by leveraging masked image modeling.
•Introduces a benchmark dataset called COVID-FL, consisting of federated chest X-ray data from eight
different medical datasets.•The proposed network simultaneously tackles the dual challenges of data heterogeneity and label defi-
ciency.
•Experimental results on various medical datasets validate the superiority of the proposed method com-
pared to ImageNet-supervised baselines and existing federated learning algorithms in terms of label e ffi-
ciency and robustness to non-IID data.
Hybrid
TransMIL [52] 2.67M •Presents a Transformer-based Multiple Instance Learning (MIL) approach that uses both morphological
and spatial information for weakly supervised WSI classification.
•Proposes to consider the correlation between di fferent instances of WSI instead of assuming them inde-
pendently and identically distributed
•Proposes a CNN-based PPEG module for conditional position encoding, which is adaptive to the number
of tokens in the corresponding sequence•Converges two to three times faster than SOTA MIL methods.
•The proposed method can be employed for unbalanced /balanced and binary /multiple classification with
great visualization and interpretability.
•TransMIL is adaptive for positional encoding as token numbers in the sequences changes.
•It needs further improvement to handle higher magnification than ×20 of WSIs - Higher magnification
means longer sequences, which in turn require more memory and computational costs to process.
LAT [50] - •Proposes a unified Transformer-based encoder-decoder structure capable of DR grading and lesion detec-
tion simultaneously.
•Proposes a Transformer-based decoder to formulate lesion discovery as a weakly supervised lesion local-
ization problem.
•Proposes lesion region importance mechanism to determine the importance of lesion-aware features.
•Proposes lesion region diversity mechanism to diversify and compact lesion-aware features.•Unlike most approaches that confront lesion discovery and diabetic retinopathy grading tasks indepen-
dently, which may generate suboptimal results, the proposed encoder-decoder structure is jointly optimized
for both tasks.
•Despite existing methods that only perform well for discovering explicit lesion regions, LAT can also
detect less dense lesion areas.
•The proposed LAT is capable of identifying Grades 0 and 1, which are hard to distinguish.
TransMed [46]TransMed-Tu: 17M
TransMed-S: 43M
TransMed-B: 110M
TransMed-L: 145M•Proposes a hybrid CNN-Transformer network for multimodal medical image classification.
•Proposes a novel image fusion strategy for 3D MRI data.•TransMed achieves much higher accuracy in classifying parotid tumors and knee injuries than CNN
models.
•Requiring fewer computational resources compared to SOTA CNNs.
3DMeT [47] - •Replaces conventional linear embedding with 3D convolution layers to reduce the computational cost of
using 3D volumes as the Transformer’s inputs.
•Obtains weights for 3D convolution layers by using a teacher-student training strategy.•The proposed method makes the Transformers capable of using 3D medical images as input.
•3DMeT Uses significantly fewer computational resources.
•Adopting CNN as a teacher assists in inheriting CNN’s spatial inductive biases.
Hybrid-COVID-ViT [49] - •Proposes a vision Transformer that embeds features for high-level COVID-19 diagnosis classification
using a backbone trained to spot low-level abnormalities in CXR images.•Different from SOTA models, the proposed model does not use the ImageNet pre-trained weights while
archiving significantly better results.
•They examine the interpretability of the proposed model.
Femur-ViT [48] - •Investigates using ViT for classifying femur fractures.
•proposes using unsupervised learning to evaluate the ViT results.•Achieves SOTA results compared to the CNN models.
GTP [54] - •Proposes a graph-based vision Transformer (GTP) framework for predicting disease grade using both
morphological and spatial information at the WSIs.
•Proposes a graph-based class activation mapping (GraphCAM) method that captures regional and contex-
tual information and highlights the class-specific regions.
•They use a self-supervised contrastive learning approach to extract more robust and richer patch features.
•They exploit a mincut pooling layer [134] before the vision Transformer layer to lessen the number of
Transformer input tokens and reduce the model complexity.•In contrast to SOTA approaches, the proposed GTP can operate on the entire WSI by taking advantage
of graph representations. In addition, GTP can e fficiently classify disease gade by leveraging a vision
Transformer.
•The proposed GTP is interpretable so that it can identify salient WSI areas associated with the Transformer
output class.
•GTP obviates the need for adding extra learnable positional embeddings to nodes by using the graph
adjacency matrix. It enables diminishing the complexity of the model.
•The proposed GTP takes both morphological and spatial information into account.
DT-MIL [51] 10.88M •Presents a novel embedded-space MIL approach incorporated with an encoder-decoder Transformer for
histopathological image analysis. Encoding is done with a deformable Transformer, and decoding with a
classic ViT.
•An efficient method to render a huge WSI is proposed, which encodes the WSI into a position-encoded
feature image.
•The proposed method selects the most discriminative instances simultaneously by utilizing associated
attention weights and calibrating instance features using the deformable self-attention.•The proposed method e fficiently embeds instances’ position relationships and context information into
bag embedding.
An extensive analysis of four di fferent bag-embedding modules is presented on two datasets.
HATNet [53](w/MobileNetv2): 5.59M
(w/ESPNetv2): 5.58M
(w/MNASNet): 5.47M•Presents a novel end-to-end hybrid method for classifying histopathological images. •HATNet surpasses the bag-of-words models by following a bottom-up strategy and taking into account
inter-word, word-to-bag, inter-bag, and bag-to-image representations, respectively. (word →bag→image)
HiFuse [53]HiFuse-Ti: 82.49M
HiFuse-S: 93.82M
HiFuse-B: 127.80M•A parallel framework is developed to e ffectively capture local spatial context features and global semantic
information representation at various scales.
•An adaptive hierarchical feature fusion (HFF) block is created to selectively blend semantic information
from various scale features of each branch using spatial attention, channel attention, residual inverted MLP,
and shortcut connection.•The HiFuse model yields favorable outcomes on the three di fferent ISIC 2018 [117], Covid19-CT [131],
and Kvasir [132] datasets.
•HiFuse incorporates a modular design that o ffers rich scalability and linear computational complexity.
MedViT [56]MedViT-Ti: 10.2M
MedViT-S: 23M
MedViT-B: 45M•Proposes a hybrid model that combines the locality of CNNs and the global connectivity of vision Trans-
formers, aiming to improve the reliability of deep medical diagnosis systems.
•By utilizing an e fficient convolution operation, the proposed attention mechanism enables joint attention
to the information in various representation subspaces, thereby reducing computational complexity while
maintaining performance.
•Introduces modifying the shape information in the high-level feature space by permuting the feature mean
and variance within mini-batches to improve the model’s resilience against adversarial attacks.•The proposed attention mechanism is e fficient.
•MedViT enhances the robustness of the Transformer models against adversarial attacks.
tain more than 50% of the background are eliminated and not
considered for further processing. Selected patches are fed
forward through a contrastive learning-based patch embeddingmodule for feature extraction. A graph is then built via a graph
construction module utilizing patch embeddings as nodes of the
graph. In the graph Transformer section, a graph convolution
11layer followed by a mincut pooling layer [134] is applied first to
learn and enrich node embeddings and then lessen the number
of Transformer input tokens. Since the graph adjacency matrix
contains spatial information of nodes, by adding an adjacency
matrix to node features, GTP obviates the need for adding ex-
tra learnable positional embeddings to nodes. The final Trans-
former layer predicts the WSI-level class label for three lung tu-
mor classes: Normal, LUAD, and LSCC. GTP also introduces a
graph-based class activation mapping (GraphCAM) technique
that highlights the class-specific regions. GraphCAM exploits
attention maps from multi-head self-attention (MHSA) blocks
in the Transformer layer and maps them to the graph space
to create a heatmap for the predicted class. The experiments
show that GTP performs as a superior interpretable and e ffi-
cient framework for classifying WSI images while considering
morphological and spatial information.
Diabetic Retinopathy (DR) is an eye disorder that can cause
impaired vision and sightlessness by damaging blood vessels
in the retina. Most deep-learning approaches view lesion dis-
covery and DR grading as independent tasks that may produce
suboptimal results. In contrast to conventional methods, LAT
[50] proposes a unified encoder-decoder structure that com-
prises a pixel relation-based encoder to capture the image con-
text information and a lesion filter-based decoder to discover le-
sion locations, which the whole network jointly optimized and
complemented during training. The encoder is particularly in
charge of modeling the pixel correlations, and the Transformer-
based decoder part is formulated as a weakly supervised lo-
calization problem to detect lesion regions and categories with
only DR severity level labels. In addition, LAT proposes two
novel mechanisms to improve the e ffectiveness of lesion-aware
filters: 1) Lesion region importance mechanism, g(·|Φ), to de-
termine the contribution of each lesion-aware feature, and 2)
Lesion region diversity mechanism to diversify and compact
lesion-aware features. The former is a linear layer followed by
a sigmoid activation function that generates importance weights
for lesion-aware features, and the latter adopts a triplet loss
[135] to encourage lesion filters to find diverse lesion regions.
In the DR grading branch, LAT presents a DR grading classi-
fication module that calculates a global consistency loss based
on the lesion-aware features, indicated as h(·|σ). Eventually,
the final DR grading prediction is achieved by calculating the
cross-entropy loss between the predicted labels obtained from
the fusion of g(·|Φ) and h(·|σ) and the ground truth. The total
loss is the aggregation of cross-entropy loss, global consistency
loss, and triplet loss. Visual results of LAT regarding the lesion
discovery are depicted in Figure 8.
The current success of deep learning in medical image inter-
pretation relies heavily on supervised learning, which requires
large labeled datasets. However, annotating medical imag-
ing data is costly and time-consuming. Additionally, models
struggle to generalize across external institutions or di fferent
tasks. Self-supervised learning o ffers a solution by leverag-
ing unlabeled medical data to develop robust models without
costly annotations [137]. Chen et al. [138] propose a valuable
empirical study of training self-supervised vision Transform-
ers, examining their e ffectiveness and performance. Accord-
Ground -truth LAT CAMFigure 8: LAT [50] vs. CAM [136] visual comparison. The ground truth con-
sists of microaneurysms, hemorrhages, soft exudates, and hard exudates, which
are colored as green, yellow, green, and blue dots, respectively (zoom in on the
ground truth image for better clarity)
.
ing to their findings, self-supervised Transformers demonstrate
strong performance when trained using contrastive learning,
outperforming ImageNet-supervised ViT models as their size
increases. For example, self-supervised pre-training surpasses
supervised pre-training in certain cases, particularly with the
very large ViT-Large. Furthermore, self-supervised ViT mod-
els achieve competitive results compared to prominent convo-
lutional ResNets in prior studies [139, 140], showcasing the po-
tential of ViT with relatively fewer inductive biases [15]. No-
tably, they discovered that removing the position embedding
in ViT only has a slight negative impact on accuracy, indicat-
ing that self-supervised ViT can learn powerful representations
without relying heavily on positional information, but also sug-
gesting that the positional information may not have been fully
utilized. Therefore, these findings indicate that the combination
of self-supervised learning and vision Transformers can be ben-
eficial to adopt for di fferent medical image analysis tasks. For
example, Yan et al. [42] propose a robust self-supervised feder-
ated learning framework, Self MedFed , for medical image clas-
sification. Their approach combines masked image modeling
with Transformers to pre-train models directly on decentralized
target task datasets. This enables more robust representation
learning on heterogeneous data and e ffective knowledge trans-
fer. Experimental results on public medical imaging datasets
[116, 117, 118] and their created COVID-FL dataset demon-
strate the framework’s e ffectiveness in improving model robust-
ness and learning visual representations across non-IID clients.
The research addresses data heterogeneity and label deficiency,
showcasing the broad applicability of their self-supervised fed-
erated learning framework in medical image analysis.
3.3. Discussion and Conclusion
Section 3 thoroughly outlines 22 distinctive Transformer-
based models in medical image classification. We have cat-
egorized the introduced models based on their architectures
12Table 3: Comparison of Transformer-based models on di fferent medical image classification datasets. ( †) and (⋆) indicate that results are adopted from [55] and
[56].
ISIC 2018†[118] TissueMNIST ⋆[141]
Method Params (M) FLOPs (G) Accuracy % F1 % Precision % Recall % Method Params (M) FLOPs (G) Top-1%
HiFuse-S [55] 93.82 8.84 83.59 72.70 72.70 73.14 MedViT-S [56] 23.6 4.9 73.1
Conformer-B /16 [142] 83.29 22.89 82.66 72.44 73.31 71.66 Twins-SVT-S [143] 24.0 2.9 72.1
Swin-B [58] 87.77 15.14 79.79 63.95 65.09 63.65 PoolFormer-S36 [144] 31.2 5.0 71.8
ViT-B /32 [15] 88.30 8.56 77.92 57.52 58.74 56.90 Swin-T [58] 29.0 4.5 71.7
COVID19-CT†[131] CvT-13 [145] 20.1 4.5 71.6
HiFuse-S [55] 93.82 8.84 76.88 76.31 77.78 76.19 RVT-S [146] 22.1 4.7 71.2
Conformer-B /16 [142] 83.29 22.89 75.81 75.60 76.81 77.81 MedViT-T [56] 10.8 1.3 70.3
ViT-B /32 [15] 88.30 8.56 61.83 60.59 61.89 60.94 MedViT-L [56] 45.8 13.4 69.9
Swin-B [58] 87.77 15.14 60.75 56.36 63.20 58.95 RVT-Ti [146] 8.6 1.3 69.6
Kvasir†[132] CoaT Tiny [147] 5.5 4.4 69.3
HiFuse-S [55] 93.82 8.84 85.00 84.96 85.08 85.00 RVT-B [146] 86.2 17.7 69.3
Conformer-B /16 [142] 83.29 22.89 84.25 84.27 84.45 84.37 DeiT-S [65] 22.0 4.6 67.0
Swin-B [58] 87.77 15.14 77.30 77.29 77.74 77.44 PiT-S [148] 23.5 2.9 66.9
ViT-B /32 [15] 88.30 8.56 73.80 73.50 74.24 73.72 PVT-S [149] 25.4 4.0 66.7
into hybrid and pure. These approaches di ffer according to
whether they adhere to the original structure of the vanilla
ViT or provide a new variant of the vision Transformer that
can be applied to medical applications. In addition, we have
presented details on the studied classification methods re-
garding their architecture type, modality, organ, pre-trained
strategy, datasets, metrics, and the year of publication in
Table 1. Additional descriptions of the methods, including
their model size, contributions, and highlights, are described
in Table 2. Table 3 presents a comparison of Transformer-
based models across four distinct medical image classifica-
tion datasets. The evaluation primarily focuses on assessing
the accuracies achieved by various models, as well as ana-
lyzing their Flop count and number of parameters.
Additionally, time inference and GPU usage are pivotal fac-
tors when evaluating method performance. Inference time
is a crucial metric for real-time or time-critical applications,
and e fficient GPU utilization ensures optimal resource allo-
cation, preventing undue stress on the hardware. We con-
ducted experiments using di fferent MedViT models (small,
base, and large) on an NVIDIA Tesla T4 GPU, assessing
GPU usage and inference times for input images of size
3×224×224. We performed 1000 model inferences af-
ter a warm-up period of 50 iterations to gauge performance.
For a batch size of 8, MedViT-S consumes 3.92 GB of
GPU memory, while MedViT-B and MedViT-L require 5.04
GB and 6.06 GB, respectively. In terms of inference time
(batch size 1), MedViT-S, -B, and -L averaged 49.60 ms,
66.89 ms, and 87.95 ms, respectively. These insights pro-
vide valuable information for choosing the right model for
specific applications, balancing performance with GPU re-
source constraints, and enabling informed comparisons with
other models when considering factors like the number of
parameters and FLOPs. By examining these metrics, re-
searchers can gain insights into the performance and e ffi-
ciency of di fferent models in the context of medical imageclassification tasks.
As is evident in the storyline of this section, we have dis-
cussed methods in each paragraph regarding the underlying
problems in medical image classification and introduced so-
lutions and how they address such issues. However, the need
for more research on these problems is crucial to making
such approaches widely applicable.
Data availability in the medical domain is one of the most
challenging aspects of developing Transformer-based mod-
els since Transformer models are known for being data-
hungry to generalize. Reasons for data scarcity in the med-
ical field can be referred to as privacy concerns of patients,
the time-consuming and costly process of annotation, and
the need for expert sta ff. To this end, the use of genera-
tive models [150, 151, 152] and their integration with Trans-
former models can become prominent since they are capable
of creating synthetic data that is comparable to genuine data.
In addition, another way to attack this problem is by utiliz-
ing federated learning, such as [33]. Nevertheless, there is
still room for improvement when it comes to privacy con-
cerns since, in federated learning, communication between
the client and server is required.
Despite their SOTA performance, Transformer-based net-
works still face challenges in deploying their models in the
real world due to computational limitations. As shown in
Table 2, most approaches have a high number of param-
eters which provokes a serious problem. Di fferent novel
approaches have been introduced to reduce the quadratic
complexity of self-attention, which can be leveraged in the
medical domain. Furthermore, though ViTs have shown im-
pressive capabilities in ImageNet classification, their perfor-
mance is still lower than the latest SOTA CNNs without ad-
ditional data [45]. Hence, existing methods mostly follow
pre-training strategies on the ImageNet dataset to build the
pre-trained weights for the subsequent downstream tasks.
However, despite the enhancement, the domain of natural
13images is significantly di fferent from medical data, thereby
restricting the performance of further improvement. There-
fore, we believe e fficient Transformers will considerably in-
fluence the future research of Transformer-based models.
4. Medical Image Segmentation
Medical segmentation is a significant sub-field of image seg-
mentation in digital image processing. It aims to extract fea-
tures from a set of regions partitioned from the entire image
and segment the key organs simultaneously, which can assist
physicians in making an accurate diagnosis in practice. X-ray,
positron emission tomography (PET), computed tomography
(CT), magnetic resonance imaging (MRI), and ultrasound are
common imaging modalities used to collect data. The CNN-
based U-Net [28, 4, 166, 167] has been the main choice in this
field due to its e ffective performance and high accuracy. Nev-
ertheless, it cannot extract long-range dependencies in high-
dimensional and high-resolution medical images [168, 169].
Therefore, the flexible combination of the U-Net structure with
Transformers become a prevalent solution to the segmentation
problem at present. Take the multi-organ segmentation task as
an example: some networks can achieve state-of-the-art multi-
organ segmentation performance on the Synapse dataset (as
shown in Figure 10) for abdominal images.
In this section, we present the application of ViTs in seg-
mentation tasks. First, we divide the approaches into two cat-
egories: pure Transformers andhybrid Transformers , where
thepure Transformer denotes the use of the multiple multi-
head self-attention modules in both the encoder and decoder.
Hybrid architecture-based approaches fuse the ViTs with con-
volution modules as the encoder, bottleneck, decoder, or skip
connection part to leverage information about the global con-
text and local details. Furthermore, we review some methods
with other architectures that propose several novel manners for
self-supervised learning. Figure 9 demonstrates the di fferent di-
rections of the methods employing Transformers in the U-Net
architecture.
4.1. Pure Transformers
In this section, we review several networks referred to as
pure Transformers , which employ Transformer blocks in both
the encoding and the decoding paths. Despite the great suc-
cess of CNN-based approaches in medical segmentation tasks,
these models still have limitations in learning long-range se-
mantic information of medical images. The authors proposed
Swin-Unet , a symmetric Encoder-Decoder architecture moti-
vated by the hierarchical Swin Transformer [58], to improve
segmentation accuracy and robust generalization capability. In
contrast to the closest approaches [158, 157, 156, 172] using
integrations of CNN with Transformer, Swin-Unet explores the
possibility of pure Transformer applied to medical image seg-
mentation.
As shown in Figure 11, Swin-Unet consists of encoder, bot-
tleneck, decoder, and skip connections utilizing the Swin Trans-
former block with shifted windows as the basic unit. For theencoder, the sequence of embeddings transformed from im-
age patches is fed into multiple Swin Transformer blocks and
patch merging layers, with Swin Transformer blocks perform-
ing feature learning, and patch merging layers downsampling
the feature resolution and unifying the feature dimension. The
designed bottleneck comprises two consecutive Swin Trans-
former blocks to learn the hierarchical representation from the
encoder with feature resolution and dimension unchanged.
Swin Transformer blocks and patch-expanding layers con-
struct the symmetric Transformer-based decoder. In contrast to
the patch merging layers in the encoder, each patch expanding
layer is responsible for upsampling the feature maps into double
resolutions and halving the corresponding feature dimension.
The final reshaped feature maps pass through a linear projec-
tion to produce the pixel-wise segmentation outputs. Inspired
by the U-Net, the framework also employs skip connections
to combine multi-scale features with the upsampled features at
various resolution levels to reduce the loss of fine-grained con-
textual information caused by down-sampling.
In contrast to the CNN-based methods showing over-
segmentation issues, the proposed U-shape pure Transformer
presents better segmentation performance resulting from learn-
ing both local and long-range dependencies. Compared to the
previous methods [173, 17], the HD evaluation metric of Swin-
Unet shows an improvement in accuracy for better edge predic-
tion. The experiments on the Synapse multi-organ CT dataset
and ACDC dataset from MRI scanners also demonstrate the ro-
bustness and generalization ability of the method.
Compared to Swin-Unet and DS-TransUNet, nnFormer
[153] proposed by Zhou et al. preserves the superior perfor-
mance of convolution layers for local detail extraction and em-
ploys a hierarchical structure to model multi-scale features. It
utilizes the volume-based multi-head self-attention (V-MSA)
and the shifted version (SV-MSA) in the Transformer blocks
instead of processing 2D slices of the volume. The overall ar-
chitecture of nnFormer is composed of an encoder and a de-
coder. Each stage in the encoder and decoder consists of a
Transformer block applying V-MSA and SV-MSA and a suc-
cessive upsampling or downsampling block built upon convo-
lution layers, which is referred to as the interleaved architec-
ture. V-MSA conducts self-attention within 3D local volumes
instead of 2D local windows to reduce the computational com-
plexity by approximately 98% and 99.5% on the Synapse and
ACDC datasets, respectively. nnFormer is first pre-trained on
the ImageNet dataset [73] and utilizes symmetrical initializa-
tion to reuse the pre-trained weights of the encoder in the de-
coder. It employs the self-supervised learning strategy, deep
supervision, to train the network. Specifically, the output fea-
ture maps of all resolutions in the decoder are passed to the
corresponding expanding block. It applies cross-entropy loss
and dice loss to evaluate the loss for each stage. Accord-
ingly, the resolution of ground truth is downsampled to match
the predictions of all the resolutions. The weighted losses of
all three stages comprise the final training objective function.
The results of experiments that compare nnFormer with prior
Transformer-based [17, 26] and CNN-based arts [174] illustrate
nnFormer makes significant progress on the segmentation task.
14 Medical Image 
 Segmentation
 Pure
 1. Swin-Unet
 2. nnFormer
 3. MISSFormer
 4. TransDeep Lab Hybrid
 Decoder
 5. SegTran Encoder
 6. Trans-UNet
 7. TransBTS
 8. TransFuse
 9. MedT
 10. UNETR
 11. Swin UNETR Skip Connection
 13. CoTr
 14. HiFormer Other Architectures
 15. T-AutoML
 16. Cross Teaching
 17. Self-pretraining with 
 MAE
 12. Swin UNETR++Figure 9: An overview of ViTs in medical image segmentation. Methods are classified into the pure Transformer, hybrid Transformer, and other architectures
according to the positions of the Transformers in the entire architecture. The prefix numbers of the methods denote 1. [26], 2. [153], 3. [154], 4. [27], 5. [155],
6. [17], 7. [156], 8. [157], 9. [158], 10. [159], 11. [160], 12. [161], 13. [162], 14. [31], 15. [163], 16. [164], 17. [165].
.
Figure 10: Transformer-based models can perform image segmentation on
medical image datasets. Figure aandcillustrate two 2D slices of raw im-
ages with the labels from Synapse dataset [170]. Figure banddshow the 3D
visualization of the labeled organs from di fferent angles. These images were
generated with MITK Workbench [171].
Although the recent Transformer-based methods improve the
problem that CNN methods cannot capture long-range depen-
dencies, they show the limitation of the capability of modeling
local details. Some methods directly embedded the convolu-
tion layers between fully connected layers in the feed-forward
network. Such structure supplements the low-level information
but limits the discrimination of features. Huang et al. propose
MISSFormer [154], a hierarchical encoder-decoder network,
which employs the Transformer block named Enhanced Trans-
former Block and equips the Enhanced Transformer Context
Bridge.
The Enhanced Transformer Block utilizes a novel e fficient
self-attention module that illustrates the e ffectiveness of spa-
tial reduction for better usage of the high-resolution map. The
original multi-head self-attention can be formulated as follows:
Attention (Q,K,V)=S o f tmax (QKT
√dhead)V, (4)
where Q,K, and Vrefer to query, key, and value, respectively,
Linear Embedding
Swin T ransformer
Block x2
Swin T ransformer
Block x1Skip Connection
1/4
EncoderPatch Merging
Swin T ransformer
Block x2
Patch Merging
Swin T ransformer
Block x2Patch Partition
Patch MergingPatch ExpandingSwin T ransformer
Block x2
Swin T ransformer
Block x1Patch ExpandingSwin T ransformer
Block x2
Patch ExpandingSwin T ransformer
Block x2Linear Projection
Patch Expanding
BottleneckSkip Connection
1/16Skip Connection
1/8
DecoderWxHxC(4x)WxHxClassFigure 11: The architecture of the Swin-Unet [26] which follows the U-Shape
structure. It contains the encoder, the bottleneck, and the decoder part, which
are built based on the Swin Transformer block. The encoder and the decoder
are connected with skip connections.
and have the same shape of N×C,dheaddenotes the number
of heads. The computational complexity is O(N2). In e fficient
self-attention, the KandVare reshaped by a spatial reduction
ratio R. Take K, for example:
new K=Reshape (N
R,C·R)W(C·R,C) (5)
Kis first resized from N×CtoN
R×(C·R) and then pro-
jected linearly to restore the channel depth from C·RtoC.
The computational cost reduces to O(N2
R) accordingly. Further-
more, the structure of the Enhanced Mix Feed-forward network
(Mix-FFN) extended from [175] introduces recursive skip con-
15nections to make the model more expressive and consistent with
each recursive step.
The U-shaped architecture of the MISSFormer contains the
encoder and decoder built on the Enhanced Transformer blocks
connected with an enhanced Transformer context bridge.
Multi-scale features produced from the encoder are flattened
and concatenated together and passed through the Enhanced
Transformer Context Bridge. The pipeline of the Enhanced
Transformer Context Bridge is based on the Enhanced Trans-
former Block to fuse the hierarchical features. The output of the
bridge is split and recovered to each original spatial dimension
to pass through the corresponding stage in the decoder. The
results of experiments show a robust capacity of the method to
capture more discriminative details in medical image segmen-
tation. It is worth mentioning that MISSFormer trained from
scratch even outperforms state-of-the-art methods pre-trained
on ImageNet.
The results in Figure 12 show that the performance of
MISSFormer for prediction and segmentation of edges in pure
Transformer network structures is more accurate compared
to TransUNet and Swin-Unet. Comparing MISSFormer and
MISSFormer-S (MISSFormer without bridge), MISSFormer
has fewer segmentation errors because the bridge is e ffective
for integrating multi-scale information.
Figure 12: A visual comparison with the state-of-the-art approaches on Synapse
dataset. Above the red line shows the successful cases of segmentation, and
below the red line are the failed cases with relatively large errors [154]
.
Inspired by the notable DeepLabv3 [176] which utilizes the
Atrous Spatial Pyramid Pooling (ASPP) to learn multi-scale
feature representations and depth-wise separable convolution
to reduce the computational burden, the authors propose Trans-
DeepLab [27] to combine the DeepLab network with the Swin-
Transformer blocks. Applying the Swin-Transformer module
with windows of multiple sizes enables the fusion of multi-
scale information with a lightweight model.
TransDeepLab is a pure Transformer-based DeepLabv3 +ar-
chitecture, as shown in Figure 13. The model builds a hi-
erarchical architecture based on the Swin-Transformer mod-ules. TransDeepLab first employs Nstacked Swin-Transformer
blocks to model the input embedded images into a deep-level
feature space. 2D medical images are first to split into non-
overlapping patches of dimension Cand size 4×4. The ensu-
ing Swin-Transformer block learns local semantic information
and global contextual dependencies of the sequence of patches.
Then, the authors introduce windows with di fferent sizes to pro-
cess the output of the Swin-Transformer and fuse the resulting
multi-scale feature layers, which are then passed through Cross
Contextual attention. This design, referred to as Spatial Pyra-
mid Pooling (SSPP) block, replaces the original Atrous Spa-
tial Pyramid Pooling (ASPP) module exploited in DeepLabV3.
A cross-contextual attention mechanism is utilized to explore
the multi-scale representation after fusion. This attention mod-
ule applies channel attention and spatial attention to the out-
put from windows of each size (from each layer of the spatial
pyramid). Finally, in the decoder part, the low-level features
from the encoder are concatenated with the multi-scale features
extracted by Cross Contextual Attention after bilinear upsam-
pling. The last two Swin-Transformer blocks and the patch ex-
panding module generate the final prediction masks.
Stacked N blocks
…
…Cross Contextual 
AttentionMulti -scale RepresentationEncoder
Low-level 
Features
ConcatUpsampleDecoder
Idea : Pure transformer model to model DeepLab 3 model with additional attention mechanism
1. Using swim transformer strategy to reduce time complexity 
2. Transformer structure to better model long -range contextual dependency
3. Novel structure using transformer Swin Transformer 
Block ×𝟐
Patch MergingPatch Partition
Linear Embedding2×2Window
7×7Window
𝐼𝑚𝑎𝑔𝑒𝑃𝑜𝑜𝑙
Swin 
Transformer 
Block ×𝟐Patch 
Expanding…
…
Figure 13: The overview architecture of TransDeepLab, which comprises
encoder and decoder built on Swin-Transformer blocks. It is the pure
Transformer-based extension of DeepLabv3 ++[27].
4.2. Hybrid Models
Hybrid Transformers concatenate Transformer blocks with
convolution layers to extract local details and long-range de-
pendencies. We further classify this category into Transformer:
Encoder ,Transformer: Decoder andTransformer: skip con-
nection according to the position of the combined module in
the U-Net architecture.
4.2.1. Transformer: Encoder
Starting with TransUNet [17], multiple methods in the med-
ical image segmentation field adopt the self-attention mecha-
nism in the encoder.
Transformers have developed as an alternative architecture
for modeling global context that exclusively relies on attention
mechanisms instead of convolution operators. However, its in-
ner global self-attention mechanism induces missing low-level
details. Direct upsampling cannot retrieve the local informa-
tion, which results in inaccurate segmentation results. The au-
thors propose the TransUNet architecture, a hybrid approach
that integrates CNN-Transformer hybrid as the encoder and
16cascaded upsampler as the decoder, combining the advantages
of Transformer and U-Net to boost the segmentation perfor-
mance by recovering localized spatial information.
The framework of the TransUNet is illustrated in Figure 14.
The proposed encoder initially employs CNN as a feature ex-
tractor to build a feature map for the Transformer input layer,
rather than the Transformer directly projecting the raw tok-
enized picture patches to latent embedding space. In this way,
the intermediate CNN feature maps of di fferent resolutions can
be saved and utilized in the following process.
For the decoder, the Cascaded Upsampler (CUP) is proposed
to replace naive bilinear upsampling, applying several upsam-
pling blocks to decode the hidden feature and output the final
segmentation result. Finally, the hybrid encoder and the CUP
constitute the overall architecture with skip connections to fa-
cilitate feature aggregation at di fferent resolution levels. This
strategy can compensate for the loss of local fine-grained de-
tails caused by the Transformer encoder and merge the encoded
global information with the local information contained in in-
termediate CNN feature maps.
The experiments show that TransUNet significantly outper-
forms the model consisting of pure Transformer encoder and
naive upsampling, as well as the ViT-hybrid model without
skip connections [17]. Comparisons with prior work [28, 177]
also demonstrate the superiority of TransUNet over competing
CNN-based approaches in terms of both qualitative visualiza-
tion and the quantitative evaluation criteria (i.e.average DSC
and HD). TransUNet integrates the benefits of both high-level
global contextual information and low-level details as an alter-
native approach for medical image segmentation.
reshape1/41/81/2 
Conv3x3, ReLUUpsampleSegmentation head(n_patch, D)(D, H/16, W/16)(512, H/16, W/16)(256, H/8, W/8)(128, H/4, W/4)(64, H/2, W/2)(16, H, W)
Transformer Layer…(n = 12)Hidden FeatureLinear ProjectionCNN
Hidden FeatureDownsampleFeature ConcatenationTransformer LayerEmbeddedSequence𝒙𝒑𝟏,𝒙𝒑𝟐,…,𝒙𝒑𝑵LayerNormMSALayerNormMLP++𝒛𝟏(a) (b) 
Figure 14: The overview architecture of the TransUNet [17]. The Transformer
layers are employed in the encoder part. The schematic of the Transformer is
shown on the left.
Wang et al. [156] propose the encoder-decoder architecture,
TransBTS , which leverages Transformer on learning global
contextual information and merits the 3D CNN for modeling
local details. In contrast to the concurrent Transformer-based
model [17], which analyzes 3D medical volumetric data in a
slice-by-slice manner, TransBTS also explores the local fea-
tures along the depth dimension by processing all the image
slices at once.
The network encoder initially employs a 3D CNN to capture
volumetric spatial features, simultaneously downsampling the
input 3D images, yielding compact volumetric feature maps.
Each feature map is projected into a token and fed into theTransformer encoder to investigate the global relationships.
The full-resolution segmentation maps are generated by the
3D CNN decoder after the progressive upsampling while us-
ing the feature embedding from the Transformer. For the en-
coder part, TransBTS first utilizes the 3 ×3×3 convolution
blocks with downsampling to process the 3D input medical im-
age data, which boosts the e ffective embedding of rich local
3D features a cross spatial and depth dimensions into the low-
resolution feature representation F. They apply a linear projec-
tion to the feature representation Fto obtain the sequence f,
which is then integrated with position embeddings, as the input
for the Transformer encoder. The Transformer encoder con-
sists of multiple Transformer layers, each of which comprises
a Multi-Head Attention(MHA) block and a Feed-Forward Net-
work(FFN). The output sequence of the Transformer encoder
passes through the feature mapping module to be reshaped to a
4D feature map Zof the same dimension as F. The approach
employs cascaded upsampling and convolution blocks to pro-
gressively restore the segmentation predictions at the original
resolution. Furthermore, skip connections combine the fine-
grained details of local information with the decoder modules,
resulting in more accurate segmentation masks.
The authors conduct comparisons between the proposed
TransBTS and the closest method TransUNet [17]. TransUNet
essentially processes 3D medical images slice by slice, while
TransBTS is a 3D model that explores the continuous interac-
tion through the depth dimension by processing a 3D medical
image in a single pass. In contrast to TransUNet, which adopts
pre-trained ViT models on other large-scale datasets, TransBTS
is trained on the dataset for the specified task without relying on
pre-trained weights.
The framework is evaluated on the Brain Tumor Segmenta-
tion (BraTS) 2019 challenge and 2020 challenge. Compared
to the 3D U-Net baseline, TransBTS achieves a significant en-
hancement in segmentation. The prediction results indicate the
improved accuracy and the superiority of modeling long-range
dependencies.
Previous approaches [157] primarily focus on replacing
convolution operation with Transformer layers or consecu-
tively stacking the two together to address the inherent lack
of pure Transformer-based models to learn local information.
In this study, the authors propose a new strategy- TransFuse
which consists of the CNN-based encoder branch and the
Transformer-based branch in parallel fused with the proposed
BiFusion module, thus further exploring the benefits of CNNs
and Transformers. The construction of the Transformer branch
is designed in the typical encoder-decoder manner. The input
images are first split into non-overlapped patches. The linear
embedding layer then projects the flattened patches into the raw
embedding sequence which is added to the learnable position
embedding of the same dimension. The obtained embeddings
are fed into the Transformer encoder, which comprises Llayers
of MSA and MLP. The output of the last layer of the Trans-
former encoder is passed through layer normalization to obtain
the encoded sequence.
The decoder part utilizes the same progressive upsampling
(PUP) approach as SETR [178]. The encoded sequence is first
17reshaped back to a sequence of 2D feature maps. Then they
employ two stacked upsampling-convolution layers to restore
the feature scales. The feature maps with di fferent spatial res-
olutions generated by each upsampling-convolution layer are
retained for the subsequent fusion operation. For the CNN
branch, the approach discards the last layer of the traditional
CNNs architecture and combines the information extracted
from the CNNs with the global contextual features obtained
from the Transformer branch. A shallower model is yielded as
a result of this design, avoiding the requirement for extremely
deep models that exhaust resources to get long-range depen-
dencies. For instance, there are five blocks in a typical ResNet-
based network where only the outputs of the 4th, 3rd, and 2nd
layers are saved for the following fusion with the feature maps
from the Transformer branch.
The BiFusion module is proposed to fuse the features ex-
tracted from the two branches mentioned above to predict the
segmentation results of medical images. The global features
from the Transformer branch are boosted by the channel at-
tention proposed in SE-Block [9]. Meanwhile, the feature
maps from the CNN branch are filtered by the spatial atten-
tion which is adopted in the CBAM [179] block to suppress the
irrelevant and noisy part and highlight local interaction. Then
the Hadamard product is applied to the features from the two
branches to learn the interaction between them. They concate-
nate the interaction feature biwith attended features ˆtiand ˆgi
and feed the results through a residual block to produce the
feature fi, which successfully models both the global and lo-
cal features at the original resolution. Finally, the segmenta-
tion prediction is generated by integrating the fifrom di fferent
BiFusion modules via the attention-gated (AG) [180] skip con-
nection.
They evaluate the performance of three variants of Trans-
Fuse on four segmentation tasks with di fferent imaging modal-
ities and target sizes. TransFuse-S is constructed with
ResNet-34 (R34) and 8-layer DeiT-Small (DeiT-S) [65]. Be-
sides, TransFuse-L is composed of Res2Net-50 and 10-layer
DeiT-Base (DeiT-B). TransFuse-L* is implemented based
on ResNetV2-50 and ViT-B [15]. For polyp segmenta-
tion, Transfuse-S /L outperforms significantly the CNN base-
line models with fewer parameters and faster running time.
TransFuse-L* also achieves the best performance among the
previous SOTA Transformer-based methods with a faster speed
for inference. It runs at 45.3 FPS and about 12% faster than
TransUNet. The experiments for other segmentation tasks also
show the superiority of the segmentation performance.
Despite the powerful results of applying Transformers to seg-
mentation tasks [181, 178], the dilemma is that properly train-
ing existing Transformer-based models requires large-scale
datasets, whereas the number of images and labels available
for medical image segmentation is relatively limited. To over-
come the di fficulty, MedT [158] proposes a gated position-
sensitive axial attention mechanism where the introduced gates
are learnable parameters to enable the model to be applied to a
dataset of arbitrary size. Furthermore, they suggested a Local-
Global(LoGo) training strategy to improve the segmentation
performance by operating on both the original image and the
Global Branch
Local Branch
Encoder
BlockDecoder
BlockImage
Segmentation 
Mask
1x1
ConvAdd
Conv
1x1NormGated
Multi-
Head
Attn
HeightGated
Multi-
Head
Attn
WidthConv
1x1 + Norm InputEncoder - Gated Axial T ransformer Layer(a)
(b)Conv
Block
XWV WK WQrQ rKG Q G KrVG V 1 G V 2
softmaxy Y
Gates
Positional 
Embeddings
Weights
Matrix
Multiplication
Addition
(c)Gated Axial Attention LayerResample
Patches
Patches
Figure 15: Overview of the MedT [158] architecture. The network uses the
LoGo strategy for training. The upper global branch utilizes the first fewer
blocks of the Transformer layers to encode the long-range dependency of the
original image. In the local branch, the images are converted into small patches
and then fed into the network to model the local details within each patch. The
output of the local branch is re-sampled relying on the location information.
Finally, a 1×1 convolution layer fuses the output feature maps from the two
branches to generate the final segmentation mask.
local patches.
The main architecture of MedT, as shown in Figure 15 (a),
is composed of 2 branches: a shallow global branch that works
on the original resolution of the entire image, and a deep local
branch that acts on the image patches. Two encoder blocks and
two decoder blocks comprise the global branch, which is suf-
ficient to model long-range dependencies. In the local branch,
the original image is partitioned into 16 patches and each patch
is feed-forwarded through the network. The output feature
maps are re-sampled based on their locations to obtain the out-
put feature maps of the branch. Then the results generated from
both branches are added and fed into a 1 ×1 convolution layer
to produce the output segmentation mask. The LoGo training
strategy enables the global branch to concentrate on high-level
information and allows the local branch to learn the finer inter-
actions between pixels within the patch, resulting in improved
segmentation performance.
Figure 15 (b) and (c) illustrate the gated axial Transformer
layer, which is used as the main building block in MedT, and
the feed-forward structure in it. They introduced four learnable
gates GV1,GV2,GQ,GK∈Rthat control the amount of informa-
tion the positional embeddings supply to key, query, and value.
Based on whether a relative positional encoding is learned ac-
curately or not, the gate parameters will be assigned weights
either converging to 1 or some lower value. The gated mecha-
nism can control the impact of relative positional encodings on
the encoding of non-local context and allows the model to work
well on any dataset regardless of size.
Unlike the fully-attended baseline [181], MedT trained on
even smaller datasets outperforms the convolutional baseline
and other Transformer-based methods. In addition, improve-
ments in medical segmentation are also observed since the pro-
posed method takes into account pixel-level dependencies.
In contrast to multiple proposed methods [178, 17, 158, 157]
18that investigate the task of 2D medical image segmentation,
UNETR [159] proposes a novel Transformer-based architec-
ture for 3D segmentation which employs the Transformer as
the encoder to learn global contextual information from the
volumetric data. In addition, unlike the previous frameworks
proposed for 3D medical image segmentation [162, 156], the
encoded feature from the Transformer of this proposed model
is directly connected to a CNN-based decoder via skip connec-
tions at di fferent resolution levels. The U-shaped UNETR com-
prises a stack of Transformers as the encoder and a decoder
coupling with it by skip connections. They begin by generating
the 1D sequence of patches by splitting the 3D input volume
in a non-overlapping manner. The flattened input patches are
then passed through a linear projection layer to yield Kdimen-
sional patch embeddings. They attach a 1D learnable positional
embedding to each patch embedding taking into account the
spatial information of the extracted patches. After the embed-
ding layer, the global multi-scale representation is captured us-
ing Transformer blocks composed of multi-head self-attention
modules and multilayer perceptron layers. They resize and
project the sequence representation extracted from the Trans-
former at di fferent resolutions for use in the decoder in order to
retrieve spatial information of the low-level details.
In the expanding pattern of the framework, the proposed
CNN-based decoder combines the output feature of di fferent
resolutions from the Transformer with upsampled feature maps
to properly predict the voxel-wise segmentation mask at the
original input resolution.
The paper claims UNETR achieves new state-of-the-art per-
formance on all organs compared against CNN [182, 29, 183,
184] and competing for Transformer-based [162, 17, 178] base-
lines on BTCV dataset, with significant improvement in perfor-
mance on small organs in particular. In addition, it outperforms
the closest methodologies on brain tumor and spleen segmen-
tation tasks in the MSD dataset. UNETR shows the superiority
of learning both global dependencies and fine-grained local re-
lationships in medical images.
Figure 16 presents qualitative segmentation comparisons for
brain tumor segmentation on the MSD dataset between UNETR
[159], TransBTS [156], CoTr [162] and U-Net [28]. It can be
seen that the details of the brain tumor are captured well by
UNETR [159].
As opposed to other methods that attempted to utilize the
Transformer module as an additional block beside the CNN-
based components in the architectures, UNETR [159] lever-
ages the Transformer as the encoder instead of the CNN-based
encoder. The Swin Transformer [58] is a hierarchical visual
Transformer featuring an e fficient shift-window partitioning
scheme for computing self-attention. Inspired by these two ap-
proaches, a novel model termed Swin Unet Tr ansformer ( Swin
UNETR ) [160] is proposed for brain tumor segmentation in
this work.
The proposed framework applies a U-shape architecture with
the Swin Transformers as the encoder and a CNN-based mod-
ule as the decoder connected to the encoder via skip connec-
tions at di fferent resolution levels. The model initially converts
3D MRI images with four channels to non-overlapping patches
Ground Truth
0.86UNet TransBTS CoTr UNETR
0.83
Figure 16: Comparison of visualization of brain tumor segmentation on the
MSD dataset. The whole tumor (WT) includes a combination of red, blue, and
green regions. The union of red and blue regions demonstrates the tumor core
(TC). The green regions indicate the enhanced tumor core (ET) [159].
and creates windows of a specific size with a patch partition
layer.
The Swin UNETR encoder is composed of 4 stages. Each
stage comprises 2 Transformer blocks and a patch merging
layer. In the Transformer blocks, the self-attention is computed
with a shifted windowing mechanism. Swin UNETR employs
the windows of size M×M×Mto partition the 3D token with
resolution of H′×W′×D′into regions of⌈H′
M×W′
M×D′
M⌉at
layer l. The partitioned window regions are then shifted by
(⌊M
2⌋,⌊M
2⌋,⌊M
2⌋) voxels at the following l+1 layer. The patch
merging layer after the Transformer components reduces the
resolution of feature maps by a factor of two and concatenates
them to form a feature embedding with the doubled dimension-
ality of the embedding space.
For the decoder of the architecture, the output feature repre-
sentations of the bottleneck are reshaped and passed through the
residual block containing two convolutional layers. The subse-
quent deconvolutional layer increases the resolution of feature
maps by a factor of 2. The outputs are then concatenated with
the outputs of the previous stage and fed into another residual
block. After the resolutions of the feature maps are restored to
the original H′×W′×D′, a head is utilized to generate the final
segmentation predictions.
The authors conduct the experiments to compare Swin UN-
ETR against the previous methodologies SegResNet [185], nn-
UNet [174]and TransBTS [156] in this work. The results
demonstrate that the proposed model has prominence as one
of the top-ranking approaches in the BraTS 2021 challenge.
It is due to the better capability of learning multi-scale con-
textual information and modeling long-range dependencies by
Swin Transformers in comparison to regular Transformers with
a fixed resolution of windows.
Apart from the architecture of Swin UNETR [160], they
also proposed a hierarchical encoder for pre-training the en-
coder of Swin UNETR [161], called Swin UNETR ++. The
pre-training tasks are tailored to learning the underlying pat-
tern of human anatomy. Various proxy tasks such as image
inpainting, 3D rotation prediction, and contrastive learning are
19leveraged to extract the features from the naturally consistent
contextual information in 3D radiographic images such as CT.
Specifically, inpainting enables learning the texture, structure,
and relationship of masked regions to their surrounding con-
text. Contrastive learning is utilized to recognize separate re-
gions of interest (ROIs) of di fferent body compositions. The
rotation task creates a variety of sub-volumes that can be used
for contrastive learning. To validate the e ffectiveness of the pre-
trained encoder, they fine-tuned the pre-trained Swin UNETR
on the downstream segmentation task and achieved SOTA per-
formance on BTCV dataset [170].
4.2.2. Transformer: Decoder
Another direction is to modify the decoder of the U-shape
structure to aggregate the Transformer-CNN-based modules.
In the Segtran framework [155], as illustrated in Figure 17,
Squeeze-and-Expansion Transformer is proposed to ”squeeze”
the attention matrix and aggregate multiple sets of contextual-
ized features from the output. A novel Learnable Sinusoidal
Position Encoding is also employed to impose the continuity
inductive bias for images. The Segtran consists of five compo-
nents: a CNN backbone to extract image features, 2) input /out-
put feature pyramids to do upsampling, 3) the Learnable Sinu-
soidal Positional Encoding, 4) Squeeze-and-Expansion Trans-
former layers to contextualize features, and 5) a segmentation
head. The pretrained CNN backbone is first utilized to learn
feature maps from the input medical images. Since the input
features to Transformers are of a low spatial resolution, the
authors increase their spatial resolutions with an input Feature
Pyramid Network (FPN) [186] to upsample the feature maps by
bilinear interpolation. Then the proposed Learnable Sinusoidal
Positional Encoding is added to the visual features to inject spa-
tial information. In contrast to the previous two mainstream
PE schemes [187, 15], the new positional embedding vector,
a combination of sine and cosine functions of linear transfor-
mations of ( x,y), brings in the continuity bias with adaptabil-
ity. The equation of the encoding strategy varies gradually with
pixel coordinates. Thus, close units receive similar positional
encodings, increasing the attention weights between them to-
wards higher values. The encoding vectors generated from the
addition of positional encodings and visual features are then fed
into the Transformer.
The novel Transformer architecture combines Squeezed At-
tention Block (SAB) [188] with an Expanded Attention Block.
Here this method employs the Induced Set Attention Block
(ISAB) proposed by [188] as a squeezed attention block. The
Squeezed Attention Block computes attention between the in-
put and inducing points and compresses the attention matrices
to lower rank matrices, reducing noises and overfitting. The
Expanded Attention Block (EAB), a mixture-of-experts model,
outputs Nmsets of complete contextualized features from Nm
modes. Each mode is an individual single-head Transformer
and shares the same feature space with each other. That is as
opposed to multi-head attention in which each head outputs an
exclusive feature subset. All features are then aggregated into
one set using dynamic mode attention. The dynamic mode at-
tention can be obtained by doing a linear transformation of each
mode feature and taking softmax over all the modes.Compared with representative existing methods in the exper-
iments, Segtran consistently achieved the highest segmentation
accuracy and exhibited good cross-domain generalization capa-
bilities.
skip connection
conv + max pool 2x2
up-conv 2x2Downsampling 
pathUpsampling 
path
Figure 17: Segtran network extracts image features using a CNN backbone and
combines the features with the position encoding of pixels flattened into a series
of local feature vectors. Multiple squeezed and extended transform layers are
stacked to process the local feature vectors. Finally, an output FPN after the
Transformer upsamples the features to generate the final prediction [155].
4.2.3. Transformer: Skip Connection
In this section, Transformer blocks are incorporated into the
skip connections to facilitate the transmission of detailed infor-
mation from the encoder to the decoder.
Although Transformer-based methods overcome the limi-
tation of capturing long-range dependency, they present ex-
treme computational and spatial complexity in analyzing high-
resolution volumetric image data. Some studies [187, 17] em-
ploy hybrid structures, fusing CNN with Transformer in an at-
tempt to reduce the training requirement on huge datasets. The
recent approach, TransUNet [17], shows good performance.
However, it is di fficult to optimize the model due to the inner
self-attention mechanism of the vanilla Transformer. First, it
takes a long time to train the attention, which is caused by ini-
tially distributing attention uniformly to each pixel within the
salient regions [14]. Second, a vanilla Transformer struggles to
handle multi-scale and high-resolution feature maps due to its
high computational cost.
Motivated by this, [162] proposes a novel encoder-decoder
framework, CoTr , which bridges CNN and Transformer. The
architecture exploits CNN to learn feature representations. An
efficient deformable self-attention mechanism in the Trans-
former is designed to model the global context from the ex-
tracted feature maps, which reduces the computational com-
plexity and enables the model to process high-resolution fea-
tures. The final segmentation results are generated by the de-
coder.
As shown in Figure 18, the DeTrans-encoder consists of
an input-to-sequence layer and multiple DeTrans Layers. The
input-to-sequence layer first flattens the feature maps at di ffer-
ent resolutions extracted from the CNN-encoder into 1D se-
20quences{fl}L
l=1. Then the corresponding 3D positional encod-
ing sequence plis supplemented with each of the flattened se-
quences flto complement the spatial information. The com-
bined sequence is fed as the input into the DeTrans Layers.
Each of the DeTrans Layers is a composition of an MS-DMSA
and a Feed-Forward Network (FFN). In contrast to the self-
attention mechanism which casts attention to all the possible
locations, the proposed MS-DMSA layer only attends to a small
set of key sampling locations around a reference location. As
a result, it can achieve faster convergence and lower compu-
tational complexity. The skip connection is utilized after each
DeTrans Layer to preserve the low-level details of local infor-
mation. The output of the DeTrans-encoder is successively up-
sampled by the pure CNN encoder to restore the original reso-
lution. Besides, they apply skip connections and a deep super-
vision strategy to add fine-grained details and auxiliary losses
to the prediction outputs.
The results of experiments indicate that CoTr with the hybrid
architecture has superior of performance over the models with
pure CNN encoder or pure Transformer encoder. It also outper-
forms other hybrid methods like TransUNet [17] in processing
multi-scale 3D medical images with reduced parameters and
complexity.
DeTransLayerDeTransLayer...FFFRRR
DeTransLayerPositional encodingUpsamplingFlattenReshapeCNN-encoderDeTrans-encoderDecoder
MS-DMSAFeed Forward
ReferencepointLayerNormLayerNormDeformableTransformerLayer
Figure 18: Overview of the CoTr [162] architecture. It is composed of a CNN-
encoder, a DeTrans-encoder and a decoder. The CNN-encoder models the lo-
cal information of the input images and provides the outputs at each stage.
The outputs of di fferent resolutions are flattened, fused and passed through the
Deformable Transformer Layers along with positional encoding. The decoder
reshapes the processed sequences from the DeTrans-encoder and produces the
final predictions.
HiFormer [31] is proposed to aggregate a fusion module
in the skip connections to learn richer representations. Fig-
ure 19 demonstrates the end-to-end network structure of the
strategy that incorporates the global dependencies learned with
the Swin Transformer and the detailed local features extracted
by the CNN modules. The encoder is composed of two hierar-
chical CNN, Swin Transformer modules and the novel Double-
Level Fusion module (DLF module). First, medical images
are fed into a CNN module to obtain a local fine-grained se-
mantic representation. After the CNN layer catches the shal-
low feature layers, HiFormer introduces the Swin Transformer
modules to complement the global feature information. The
Swin Transformer module employs windows of di fferent sizes
to learn the dependencies between multiple scales. To reuse
the shallow and deep multi-scale feature information in the en-
coder, HiFormer designs a novel skip connection module, the
DLF module. The deep-level semantic and shallow-level lo-calization information are fed into the DLF module and fused
by the cross-attention mechanism. Finally, both generated fea-
ture maps are passed into the decoder to produce the final
segmentation prediction results. The experiments conducted
on the Synapse dataset [170], SegPC [189], and ISIC 2017
dataset [117] demonstrate the superiority of the learning abil-
ity of HiFormer. Moreover, the lightweight model with fewer
parameters also exceeds CNN-based methods and previous
Transformer-based approaches with lower computational com-
plexity.
𝐶𝑜𝑛𝑣1×1
Patch merging
Patch merging
DLF Module
Conv Block
Segmentation 
Head
Transformer 
Encoder ×𝑺
GAP
Cross 
Attention
Transformer 
Encoder ×𝑳
GAP
𝐶𝑜𝑛𝑣1×1
𝐶𝑜𝑛𝑣1×1
𝐇/𝟒×𝐖/𝟒,𝐃𝐇/𝟖×𝐖/𝟖,𝟐𝐃𝐇/𝟏𝟔×𝐖/𝟏𝟔,𝟒𝐃
𝐇/𝟒×𝐖/𝟒,𝐃′𝐇/𝟖×𝐖/𝟖,𝟐𝐃′𝐇/𝟏𝟔×𝐖/𝟏𝟔,𝟒𝑫′
𝑯×𝑾×3
ConvUp
P
s
P
l×6
×2
×2
ConvUp
Figure 19: HiFormer comprises the CNN-Transformer encoder, the CNN-based
decoder and the Double-Level Fusion Module (DLF). The feature layers of the
shallowest level pland of the deepest level psare fed into the DLF module for
the fusion of hierarchical information. Blue blocks and orange blocks refer to
Swin Transformer and CNN modules, respectively [31].
4.3. Other Architectures
Most ViT-based models rely on pre-training of large natu-
ral image datasets to obtain pre-weights and then solve down-
stream tasks by transfer learning. Several works explore train-
ing in a self-supervised or semi-supervised manner to e ffi-
ciently utilize medical image datasets of limited size or datasets
without manual labels. Furthermore, some approaches apply
Transformers to seek the design of architectures that implement
medical image segmentation, instead of using the Transformers
to act directly on the input image.
Unlike the previously proposed methods that employ the
Transformers to act directly on the medical image for feature
extraction, this method [163] adopts the AutoML for automat-
ically designing the network architecture without much human
heuristics or assumptions, where the Transformer is applied to
encode the embedding vector regarding the architecture config-
urations. The approach reduces the workload of algorithm de-
sign by automatically estimating ”almost” all the components
of the framework instead of manually designing for the network
and training strategies. That improves the model performance
of segmentation simultaneously.
The proposed Transformer-based T-AutoML inspired by
SpineNet [190] leverages neural architecture search (NAS) with
a larger search space to optimize the selection of the network
connections. This framework can connect the feature maps at
different spatial levels of the network with another one arbitrar-
ily, compared with the previous methods that only search for the
encoder-decoder U-shape networks [191, 192, 193]. The can-
didates of di fferent blocks in the network consist of 3D residual
21blocks, 3D bottleneck blocks, and 3D axial-attention blocks.
The residual blocks and bottleneck blocks are e ffective in alle-
viating the vanishing gradient. The axial-attention blocks are
applied to model the long-range dependency in the 2D medi-
cal images. Another upsampling layer (linear interpolation) is
utilized at the end of the architecture to produce the results of
feature maps at the original volume size.
To search for the optimal architecture and training configu-
ration, the authors first encode the necessary components in the
search space to form a one-dimensional vector v. The search
space contains candidates of di fferent configurations with re-
gard to data augmentation, learning rates, learning rate sched-
ulers, loss function, the optimizer, the number and spatial reso-
lution of blocks, and block types.
After the obtainment of the encoding vector v, the proposed
new predictor predicts the binary relation of validation accuracy
values between viandvj. The predictor employs the Trans-
former encoder to encode the vector vof varying lengths into
feature maps of a fixed resolution. Then the feature maps are
passed through the Multiple FC layers to generate the binary
relation predictions denoted as GTvi,vj. Since the predictor is
designed for ranking the vectors with respect to the accuracy
values and estimating the relations, the actual values of the pre-
dicted accuracy are not necessary to be calculated for each vec-
tor. Thus, the new predictor requires less overall training time.
The experiments indicate that the proposed method can
achieve the state of the art(SOTA) in lesion segmentation tasks
and shows superiority in transferring to di fferent datasets.
Despite the promising results achieved by the CNNs and
Transformer-based methods with large-scale images, these ap-
proaches require expert labeling at the pixel /voxel level. Expen-
sive time costs and manual annotation limit the size of the med-
ical image dataset. Due to this dilemma, the proposed semi-
supervised segmentation [164] provides a low-cost and practi-
cal scheme, called Cross Teaching between CNN and Trans-
former, to train e ffective models using a little amount of cor-
rectly labeled data and a large amount of unlabeled or coarsely
labeled data.
Inspired by the existing works [194, 195, 196] for semi-
supervised learning which introduce perturbation at di fferent
levels and encourage prediction to be consistent during the
training stage, the designed cross teaching introduces the per-
turbation in both learning paradigm-level and output-level. As
shown in Figure 20, each image within the training set con-
taining labeled and unlabeled images is fed into two di fferent
learning paradigms: a CNN and a Transformer respectively.
For the unlabeled dataset with raw images, the cross teaching
scheme allows the cross supervision between a CNN ( fc
ϕ(.)) and
a Transformer( ft
ϕ(.)), which aims at integrating the properties
of the Transformer modeling the long-range dependency and
CNN t0 learn local information in the output level.
The unlabeled data initially passes through a CNN and a
Transformer respectively to generate predictions pc
iandpt
i.
pc
i=fc
ϕ(xi);pt
i=ft
ϕ(xi); (6)
Then the pseudo labels plc
iandplt
iare produced in this manner:
plc
i=argmax (pt
i);plt
i=argmax (pc
i); (7)The pseudo label plc
iused for the CNN training is generated by
the Transformer. Similarly, the CNN model provides pseudo
labels for Transformer training. The cross-teaching loss for the
unlabeled data is defined as follows:
Lctl=LDice(pc
i,plc
i)|          {z          }
supervision f or CNNs+ LDice(pt
i,plt
i)|         {z         }
supervision f or Trans f ormers(8)
which is a bidirectional loss function. One direction of the data
stream is from the CNN to the Transformer, and the other di-
rection is from the Transformer to the CNN. For the labeled
data, the CNN and Transformer are supervised by the ground
truth. The commonly-used supervised loss functions, i.e. the
cross-entropy loss and Dice loss, are employed to update model
parameters.
Lsup=Lce(pi,yi)+LDice(pi,yi) (9)
where pi,yirepresent the prediction and the label of image xi.
And the overall objective combining the cross-teaching branch
and supervised branch is defined as:
Ltotal=Lsup+λLctl (10)
whereλis a weight factor, which is defined by time-
dependent Gaussian warming up function [197, 164]:
λ(t)=0.1·e
−5
1−ti
ttotal2
(11)
The results of the ablation study indicate that the combina-
tion of CNN and Transformer in a cross-teaching way shows
superiority over the existing semi-supervised methods. Further-
more, the novel method has the potential to reduce the label
cost by learning from limited data and large-scale unlabeled
data. However, it is observed that achieving SOTA via semi-
supervised approaches remains a significant challenge.
UNet (CNN)
Transformer 
EncoderTransformer 
Decoder
Training setMini-batch
LL L
LUU
UU
LL:Labeled image.
U: Unlabeled image.
:Stop-gradient.
Swin -UNet (Transformer)
U
LLabel
Figure 20: The model performs the semi-supervised medical image segmenta-
tion task. The regularization scheme between CNN and Transformer is referred
to as Cross Teaching. L denotes the labeled data and U denotes the unlabeled
data. The cross-teaching employs a bidirectional loss function: one path is
from the CNN branch to the Transformer branch, and the other is from the
Transformer to the CNN. A Transformer is applied for complementary training
instead of prediction generation [164].
Zhou et al. [165] hypothesize that the ability to aggre-
gate contextual information is imperative to improve the per-
formance of medical image analysis. Nonetheless, there is no
ImageNet-scale medical image dataset for pre-training. There-
fore, they investigate a novel self-pre-training paradigm based
22Table 4: An overview of the reviewed Transformer-based medical image Segmentation models.
Method Modality Organ Type Pre-trained Module: Type Datasets Metrics Year
Pure
Swin-Unet [26] CT Multi-organ 2D ViT: Supervised1Synapse [170]
2ACDC [198]Dice 2021
nnFormer [153]CT
MRIMulti-organ 3D ViT: Supervised1Synapse [170]
2ACDC [198]Dice 2021
MISSFormer [154]CT
MRIMulti-organ 2D ✗1Synapse [170]
2ACDC [198]Dice
Hausdor ffdistance2021
TransDeepLab [27]CT
DermoscopyMulti-organ
Skin2D ViT: Supervised1Synapse [170]
2ISIC 2017, 2018 [117, 118]
3PH2 [199]Dice
Hausdor ffdistance2022
Encoder
TransUNet [17]CT
MRIMulti-organ 2D ViT: Supervised1Synapse [170]
2ACDC [198]Dice
Hausdor ffdistance2021
TransBTS [156] MRI Brain 3D ✗ BraTS 19-20 [200, 201, 202]Dice
Hausdor ffdistance2021
TransFuse [157] Colonoscopy Multi-organ2D
3DViT: Supervised1Synapse [170]
2ACDC [198]Dice
Hausdor ffdistance2021
MedT [158]Microscopy
UltrasoundMulti-organ 2D ✗1Brain US (Private)
2 GLAS [203]
3MoNuSeg [204]F1 2021
UNETR [159]CT
MRIBrain, Spleen 3D ✗1Synapse [170]
2MSD [205]Dice
Hausdor ffdistance2021
Swin UNETR [160] MRI Brain 3D ViT: Supervised BraTS 21 [206]Dice
Hausdor ffdistance2022
Swin UNETR ++[161] CT Multi-organ 3D ViT: Self-supervised1BTCV [170]
2MSD [205]Dice
NSD2022
Skip connection
CoTr [162] CT Multi-organ 3D ✗ Synapse [170] Dice 2021
HiFormer [31]MRI
Dermoscopy
MicroscopicMulti-organ
Skin
Cells2DViT: Supervised
CNN: Supervised1Synapse [170]
2ISIC 2017, 2018 [117, 118]
3SegPC 2021 [207, 208, 209]Dice
Hausdor ffdistance2022
Decoder
SegTran [155]Fundus
MRI
X-ColonoscopyMulti-organ2D
3DCNN: SupervisedREFUGE 20 [210]
1BraTS 19 [200, 201, 202]
2X-CVC [211]
3KV ASIR [212]Dice 2021
Other architectures
T-AutoML [163] CT Liver and lung tumor 3D ✗ MSD 2019 [213]Dice
NSD2021
Cross Teaching [164] MRI Multi-organ 2D ✗ ACDC [198]Dice
Hausdor ffdistance2022
Self-pretraining with MAE [165]CT
MRI
X-rayLung
Brain
Multi-organ3D ViT: supervised1ChestX-ray14 [104]
2Synapse [213]
3MSD 2019 [205]Dice
Hausdor ffdistance2022
on Masked Autoencoder (MAE), MAE self pre-training , for
medical image analysis, one of the masked image modeling
(MIM) frameworks [215] [216] [217] [218]. MIM encourages
the framework to restore the masked target by integrating infor-
mation from the context, where the main idea of MIM is mask-
ing and reconstructing: masking a set of image patches before
input into the Transformer and reconstructing these masked
patches at the output.
The pipeline for segmentation with MAE self-pre-training
contains two stages. In the first stage (as shown on the left of
Figure 21), ViT is pre-trained with MAE as the encoder. The
input patches are divided into visible ones and masked ones.
The ViT encoder only acts on the visible patches. Compared
to other MIM methods, MAE does not employ mask tokens in
the encoder, which saves time and allows for faster pre-training.
A lightweight Transformer decoder is appended to reconstruct
the full image. The decoder is only an auxiliary part used for
pre-training and will not be applied in downstream tasks.
In the second stage (as shown on the right of Figure 21), thepre-trained ViT weights are transferred to initialize the segmen-
tation encoder. Then, the task-specific heads are appended to
perform downstream tasks. The whole segmentation network,
e.g., UNETR, is finetuned to perform the segmentation task.
The experiments, including the MAE pre-training and the
downstream task, are conducted to evaluate the performance of
the proposed method. The results show that MAE can recover
the lost information in the masked input patches. MAE pre-
training enables the model to improve its classification and seg-
mentation performance on medical image analysis tasks, sur-
passing the ImageNet pre-trained model to SOTA.
4.4. Discussion and Conclusion
This section comprehensively investigates the overview of
around 17 Transformer-based models for medical image
segmentation presented in Section 4.1 to Section 4.3. Some
representative self-supervised strategies [161, 153] for train-
23Table 5: A brief description of the reviewed Transformer-based medical image segmentation models. The unreported number of parameters indicates that the
value was not mentioned in the paper, and the code was unavailable.
Method # Params Contributions Highlights
Pure
Swin-Unet[26] - •Builds a pure Transformer model with symmetric Encoder-Decoder architecture based on the Swin-
Transformer block connected via skip connections.
•Proposes patch merging layers and patch expanding layers to perform downsampling and upsampling
without convolution or interpolation operation.•The results of extensive experiments on multi-organ and multi-modal datasets show the good generalization ability of the model.
•Pre-trained on ImageNet rather than medical image data, which may result in sub-optimal performance.
nnFormer [153] 158.92M •Proposes a powerful segmentation model with an interleaved architecture (stem) based on the empirical
combination of self-attention and convolution.
•Proposes a volume-based multi-head self-attention (V-MSA) to reduce computational complexity.•V olume-based operations help to reduce the computational complexity.
•Pre-trained on ImageNet rather than medical image data, which may result in sub-optimal performance.
MISSFormer [154] - •Proposes the Enhanced Transformer Block based on the Enhanced Mix-FFN and the E fficient Self-
attention module.
•Proposes the Enhanced Transformer Context Bridge built on the Enhanced Transformer Block to model
both the local and global feature representation and fuse multi-scale features.•The model can be trained from scratch without the pretraining step on ImageNet.
•Less computational burden due to the novel design of the E fficient Self-attention module.
TransDeepLab [27] 21.14M •Proposes the encoder-decoder DeepLabv3 +architecture based on Swin-Transformer.
•Proposes the cross-contextual attention to adaptively fuse multi-scale representation.•The first attempt to combine the Swin-Transformer with DeepLab architecture for medical image segmentation.
•A lightweight model with only 21.14M parameters compared with Swin-Unet[26], the original DeepLab model [214] and TransUNet[17].
Encoder
TransUNet [17] 96.07M •Proposes the first CNN-Transformer hybrid network for medical image segmentation, which establishes
self-attention mechanisms from the perspective of sequence-to-sequence prediction.
•Proposes a cascaded upsampler (CUP) which comprises several upsampling blocks to generate the pre-
diction results.•TransUNet fully exploits the strong global context encoded from the Transformer and local semantics from the CNN module.
•It presents the generalization ability on multi-modalities.
•The approach allows the segmentation of 2D and 3D medical images.
TransBTS [156]Moderate TransBTS: 32.99M
Lightweight TransBTS: 15.14M•Proposes a novel encoder-decoder framework TransBTS that integrates Transformer with 3D CNN for
MRI Brain Tumor Segmentation.•The method can model the long-range dependencies not only in spatial but also in the depth dimension for 3D volumetric segmentation.
•TransBTS can be trained on the task-specific dataset without the dependence on pre-trained weights.
•TransBTS is a moderate-size model that outperforms in terms of model complexity with 32.99M parameters and 33G FLOPs. Furthermore,
the vanilla Transformer can be replaced with other variants to reduce the computation complexity.
TransFuse [157] TransFuse-S: 26.3M •Proposes the first parallel-in-branch architecture — TransFuse to capture both low-level global features
and high-level fine-grained semantic details.
•Proposes BiFusion module in order to fuse the feature representation from the Transformer branch with
the CNN branch.•The architecture does not require very deep nets, which alleviates gradient vanishing and feature diminishing reuse problems.
•It improves performance by reducing parameters and increasing inference speed, allowing deployment on both the cloud and the edge.
•The CNN branch is flexible to use any o ff-the-shelf CNN network.
4It can be applied to both 2D and 3D medical image segmentation.
MedT [158] 1.4M •Proposes a gated axial-attention model that introduces an additional control mechanism to the self-
attention module.
•Proposes a LoGo (Local-Global) training strategy for boosting segmentation performance by simultane-
ously training a shallow global branch and a deep local branch.•The proposed method does not require pre-training on large-scale datasets compared to other transform-based models.
•The results of predictions are more precise compared to the full attention model.
UNETR [159] 92.58M •Proposes a novel architecture to address the task of 3D volumetric medical image segmentation.
•Proposes a new architecture where the Transformer-based encoder learns long-range dependencies and
the CNN-based decoder utilizes skip connections to merge the outputs of a Transformer at each resolution
level with the upsampling part.•UNETR shows moderate model complexity while outperforming these Transformer-based and CNN-based models.
•The inference time of UNETR is significantly faster than Transformer-based models.
•They did not use any pre-trained weights for the Transformer backbone.
Swin UNETR [160] 61.98M •Proposes a novel segmentation model, Swin UNETR, based on the design of UNETR and Swin Trans-
formers.•The FLOPs of Swin UNETR significantly grow compared to that of UNETR and TransBTS.
•The Swin Transformer is suitable for the downstream tasks.
Swin UNETR ++[160] 61.98M •Presents a new self-supervised learning approach that capitalizes on the Swin UNETR model, harnessing
its multi-resolution encoder for e fficient pre-training.
•Introduces a pre-trained robust model using the proposed encoder and proxy tasks on a dataset of 5,050
publicly available CT images, resulting in a powerful feature representation for various medical image
analysis tasks.•Fine-tuning the pretrained Swin UNETR model leads to improved accuracy, faster convergence, and reduced annotation e ffort compared to
training from randomly initialized weights.
•Researchers can benefit from the pre-trained encoder in transfer learning for various medical imaging analysis tasks, including classification
and detection.
Skip connection
CoTr [162] 46.51M •Proposes a hybrid framework that bridges a convolutional neural network and a Transformer for accurate
3D medical image segmentation.
•Proposes the deformable Transformer (DeTrans) that employs the multi-scale deformable self-attention
mechanism (MS-DMSA) to model the long-range dependency e fficiently.•The deformable mechanism in CoTr reduces computational and spatial complexities, allowing the network to model high-resolution and
multi-scale feature maps.
HiFormer [31]HiFormer-S: 23.25M
HiFormer-B: 25.51M
HiFormer-L: 29.52M•Proposes an encoder-decoder architecture that bridges a CNN and a Transformer for medical image seg-
mentation.
•Proposes a Double-level Fusion module in the skip connection.•Fewer parameters and lower computational cost.
Decoder
SegTran [155] 86.03M •Proposes a novel Transformer design, Squeeze-and-Expansion Transformer, in which a squeezed attention
block helps regularize the huge attention matrix, and an expansion block learns diversified representations.
•Proposes a learnable sinusoidal positional encoding that imposes a continuity inductive bias for the Trans-
former.•Compared to U-Net and DeepLabV3 +, Segtran has the least performance degradation, showing the best cross-domain generalization when
evaluated on datasets of drastically di fferent characteristics.
Other architectures
T-AutoML [163] 16.96M •Proposes the first automated machine learning algorithm, T-AutoML, which automatically estimates “al-
most” all components of a deep learning solution for lesion segmentation in 3D medical images.
•Proposes a new predictor-based search method in a new search space that searches for the best neural
architecture and the best combination of hyperparameters and data augmentation strategies simultaneously.•The method is e ffectively transferable to di fferent datasets.
•The applied AutoML alleviates the need for the manual design of network structures.
•The intrinsic limitations of AutoML.
Cross Teaching [164] - •Proposes an e fficient regularization scheme for semi-supervised medical image segmentation where the
prediction of a network serves as the pseudo label to supervise the other network.
•The proposed method is the first attempt to apply the Transformer to perform the semi-supervised medical
segmentation utilizing the unlabeled data.•The training process requires less data cost by semi-supervision.
•The framework contains components with low complexity and simple training strategies compared to other semi-supervised learning methods.
•The proposed semi-supervised segmentation still can not achieve the state-of-the-art (SOTA) compared with the fully-supervised approaches.
Self-pretraining with MAE [165] - •Proposes a self-pre-training paradigm with MAE for medical images where the pre-training process of
the model uses the same data as the target dataset.•The proposed paradigm demonstrates its e ffectiveness in limited data scenarios.
Table 6: Comparison results of some segmentation methods on the Synapse [170] dataset. The methods are ordered based on their dice score.
Methods Params (M) FLOPs (G) Dice↑HD95↓Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach
TransUNet [17] 96.07 88.91 77.49 31.69 87.23 63.16 81.87 77.02 94.08 55.86 85.08 75.62
Swin-Unet [26] 27.17 6.16 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60
TransDeepLab [27] 21.14 16.31 80.16 21.25 86.04 69.16 84.08 79.88 93.53 61.19 89.00 78.40
HiFormer [31] 25.51 8.05 80.39 14.70 86.21 65.23 85.23 79.77 94.61 59.52 90.99 81.08
PVT-CASCADE [219] 35.28 6.40 81.06 20.23 83.01 70.59 82.23 80.37 94.08 64.43 90.10 83.69
MISSFormer [154] 42.46 9.89 81.96 18.20 86.99 68.65 85.21 82.00 94.41 65.67 91.92 80.81
DAEFormer [220] 48.07 27.89 82.63 16.39 87.84 71.65 87.66 82.39 95.08 63.93 91.82 80.77
TransCASCADE [219] 123.49 - 82.68 17.34 86.63 68.48 87.66 84.56 94.43 65.33 90.79 83.52
ScaleFormer [221] 111.60 48.93 82.86 16.81 88.73 74.97 86.36 83.31 95.12 64.85 89.40 80.14
D-LKA Net [222] 101.64 19.92 84.27 20.04 88.34 73.79 88.38 84.92 94.88 67.71 91.22 84.94
MERIT [223] 147.86 33.31 84.90 13.22 87.71 74.40 87.79 84.85 95.26 71.81 92.01 85.38
Table 7: Performance comparison of several Transformer-based segmentation approaches on ISIC 2017 [117], ISIC 2018 [118], and PH2[224].
Methods Params (M) FLOPs (G)ISIC 2017 ISIC 2018 PH2
DSC SE SP ACC DSC SE SP ACC DSC SE SP ACC
TransUNet [17] 96.07 88.91 0.8123 0.8263 0.9577 0.9207 0.8499 0.8578 0.9653 0.9452 0.8840 0.9063 0.9427 0.9200
TransNorm [172] 117.98 33.41 0.8933 0.8532 0.9859 0.9582 0.8951 0.8750 0.9790 0.9580 0.9437 0.9438 0.9810 0.9723
Swin-Unet [26] 27.17 6.16 0.9183 0.9142 0.9798 0.9701 0.8946 0.9056 0.9798 0.9645 0.9449 0.9410 0.9564 0.9678
D-LKA Net [222] 101.64 19.92 0.9254 0.9327 0.9793 0.9705 0.9177 0.9164 0.9773 0.9647 0.9490 0.9430 0.9775 0.9659
HiFormer [31] 25.51 8.05 0.9253 0.9155 0.9840 0.9702 0.9102 0.9119 0.9755 0.9621 0.9460 0.9420 0.9772 0.9661
24ViT Encoder
Transformer Decoder
ViT Encoder
.
 .
UNETR Decoder
Transfer  
 
WeightsMAE Self Pre-training UNETR Segmentation
z3 z6 z9z12
[mask token]
[patch feature]Figure 21: Illustration of MAE self pre-training. First, MAE is pre-trained as
an encoder for ViT. The ViT encoder is fed with a random subset of patches
and the decoder of the Transformer reconstructs the complete image as shown
on the left. Then, the pre-trained ViT weights are transferred to the initialized
segmentation encoder, as shown on the right. Finally, the whole segmenta-
tion network, such as UNETR, is fine-tuned to perform the segmentation task.
[165].
ing transformer modules are also introduced in this sec-
tion. We provide information on the reviewed segmenta-
tion approaches about the architecture type, modality, or-
gan, input size, the pre-trained manner, datasets, metrics,
and the year in Table 4. Table 5 also lists the methods
along with the number of parameters, contributions, and
highlights. The comparison results of some SOTA meth-
ods on the Synapse dataset are also presented in Table 6,
showcasing their respective performance metrics. Addition-
ally, the evaluation of SOTA approaches on the ISIC 2017,
ISIC 2018, and PH2datasets are demonstrated in Table 7.
The tables serve as valuable references for evaluating the
relative performance and suitability of di fferent approaches
in medical image segmentation. Moreover, inference time
and GPU usage are crucial performance factors for model
selection. DAE-Former excels with a fast 46.01 ms infer-
ence and 6.42 GB GPU memory. MERIT, while e ffective,
takes longer at 93.13 ms and uses 7.78 GB. D-LKA, with
113.18 ms inference, balances GPU memory well at 5.22
GB. TransCASCADE o ffers a compromise with 53.18 ms
inference and 3.75 GB GPU memory. These metrics along-
side the parameter numbers and FLOP count guide model
selection for e fficient medical image segmentation. It’s im-
portant to note that these assessments were conducted using
the same test setup and configurations as those employed in
image classification Section 3.2.
ViT-based works o ffer solutions in a broad range of mul-
timodal tasks of 2D or 3D. Most of the approaches demon-
strate superior results over CNN-based segmentation models
on benchmark medical datasets. Despite the SOTA perfor-
mance Transformer-based networks have achieved, there are
some challenges in deploying the Transformer-based mod-
els at present. The first challenge is the high computational
burden due to the relatively large number of parameters of
the Transformer-based models [220]. The reason is that thetime and space complexity of the attention mechanism is
quadratic to the sequence length. For example, the CNN-
based models such as U-Net [28] require 3.7M parameters
[158] to reach Dice Score 74.68 [17]. However, TransUNet,
which achieves Dice Score 77.48 needs 96.07M [159] pa-
rameters. The researchers have to meet the high demand
for GPU resources. Thus, several novel approaches such
as Swin Transformer employed in Swin-Unet [26], volume-
based Transformer utilized in nnFormer [153] and e fficient
self-attention module in MISSFormer [154] are proposed to
simplify the computation of the Transformer models. The
direction of facilitating the e fficiency of models will play a
crucial role in future research. We also note that most exist-
ing methods require pre-training strategies on the ImageNet
dataset to obtain the pre-trained weights for the following
downstream tasks. However, the natural image datasets and
medical datasets di ffer dramatically from one another, which
may impact the final performance of extracting the medical
features. Meanwhile, pre-training leads to high computa-
tional costs, which hinders the training of models in prac-
tice. Multiple segmentation networks that can be trained
from scratch on the medical dataset are suggested as the so-
lutions, such as MISSFormer [154]. We expect more ap-
proaches to exploring more e fficient pre-training strategies
or without pre-training. Furthermore, considering the lim-
ited size of some medical datasets, some approaches propose
semi-supervised technologies or self-pre-training paradigms
to reduce the dataset burden of training or pre-training. Nev-
ertheless, the performance is still not comparable to that of
fully-supervised models. Designing semi-supervised mod-
els with improved accuracy in this direction requires more
attention.
5. Medical Image Reconstruction
3D Medical imaging is a clinical breakthrough and is very
popular in medical diagnosis and follow-up after treatment. In
Computed Tomography (CT), Single Photon Emission Tomog-
raphy (SPECT), and Positron Emission Tomography (PET), the
imaging process relies on ionizing radiation [242, 243], which
implies a potential risk for the patient [244]. A non-invasive
3D imaging technique is Magnetic Resonance Imaging (MRI),
which does not rely on ionizing radiation. However, image ac-
quisition may take longer and confines the patient in a discom-
forting narrow tube [245]. In order to reconstruct 3D volumet-
ric datasets from the acquired data, Medical image reconstruc-
tion is one of the essential components of 3D medical imaging.
The primary objective of 3D image reconstruction is to gener-
ate high-quality volumetric images for clinical usage at mini-
mal cost and radiation exposure, whilst also addressing poten-
tial artifacts inherent to the physical acquisition process. Image
reconstruction solves an inverse problem that is generally chal-
lenging due to its large-scale and ill-posed nature [246].
In medical imaging, there are ongoing research e fforts to
reduce the acquisition time (i.e. to reduce cost and potential
25 Medical Image 
 Reconstruction
 Computed Tomography (CT)
 Low Dose Enhancement
 1. TransCT
 2. TED-net
 3.- Eformer
 4. 3D Transformer-GAN
 5. STFNet
 6. CTformer
 7. SIST Sparse-View Reconstruction
 8. DuDoTrans
 9. FIT
 10. CTTR
 11. ARMLUT Magnetic Resonance Imaging (MRI)
 Undersampled Reconstruction
 12. ViT-Rec
 13. T2Net Super Resolution 
 Reconstruction
 14. DisCNN-ViT
 15. Cohf-T(a) Taxonomy structure for medical image reconstruction. Methods in this field are categorized by their functionality in
addressing issues in consensus imaging modalities, not how the Transformer is integrated with the architecture like in
the previous sections. The prefix numbers in the paper’s name in ascending order denote the reference for each study as
follows: 1. [225], 2. [226], 3. [227], 4. [228], 5. [229], 6. [230], 7. [231], 8. [232], 9. [233], 10. [234], 11. [235], 12.
[236], 13. [237], 14. [238], 15. [239], 16. [240], 17. [241].
Modification
Stage
Presence in
Architecture
Design
Encoder
TransCT
(Zhang et
al., 2021e)
STFNet
(Zhang et
al., 2022)
SIST
(Yang et
al., 2022)
DuDoTrans
(Wang et
al., 2021a)CTTR
(Shi et
al., 2022a)Pure
TED-net
(Wang et
al., 2021b)
Eformer
(Luthra et
al., 2021)CTformer
(Wang et
al., 2022a)FIT
(Buchholz
and Jug,
2021)ViT-Rec
(Lin and
Heckel,
2021)Skip
Connection
Cohf-T
(Fang et
al., 2022)
Other
Architectures
3D T-GAN
(Luo et
al., 2021b)ARMLUT
(Wu et
al., 2022)T2Net
(Feng et
al., 2021)
DisCNN-
ViT
(Mahapatra
and Ge,
2021)
SLATER
(Korkmaz
et al., 2022)
DSFormer
(Zhou et
al., 2023)(b) We also presented a second taxonomy due to the
presence of the Transformer in the reviewed studies.
Figure 22: An overview of medical image reconstruction taxonomies either as categorizing by the task or the location of using Transformer in an architecture.
movement artifacts) as well as radiation dose. However, lower-
ing the radiation dose results in higher noise levels and reduced
contrast, which poses a challenge for 3D image reconstruction.
Vision Transformers (ViTs) have e ffectively demonstrated
possible solutions to address these challenges. We categorize
the literature in this domain into low dose enhancement ,sparse-
view reconstruction ,undersampled reconstruction , and super-
resolution reconstruction . This section will overview some of
the SOTA Transformer-based studies that fit into our taxonomy.
Figure 22a and Figure 22b demonstrate our proposed taxonomy
for this field of study. Figure 22a indicates the diversity of our
taxonomy based on the medical imaging modalities we studied
in this research. Figure 22b endorses the usage of the Trans-
former within the overviewed studies’ pipelines.
5.1. Low Dose Enhancement
Zhang et al. [225] used a very general intuition about image
denoising: the noisy image constructed with high-frequency
and low-frequency counterparts as X=XH+XLin a study,
namely, TransCT . Zhang et al. [225] claim that the noisy im-
age’s low counterpart contains two sub-components of main
image content and weakened image textures, which are entirely
noise-free. They applied a Gaussian filter on the input image
to decompose an image into a high-frequency sub-band and a
low-frequency sub-band. After this, they extracted XLccon-
tent features and XLtlatent texture features by applying two
shallow CNNs on the low-frequency counterpart of the input
image. Simultaneously, they applied a sub-pixel layer on a
high-frequency counterpart to transform it into a low-resolution
image and extracted embedding features ( XHf) by applying a
shallow CNN. Then the resultant latent texture features ( XLt)
and corresponding high-frequency representation are fed to the
Transformer for noise removal from a high-frequency repre-
sentation. Ultimately, they reconstruct the high-quality image
piecewise. They showed that the latent texture features are ben-
eficial in screening noise from the high-frequency domain.
Figure 23: An overview of TED-net [226]. Tokenize and DeToken blocks are
invertible operations that apply the process of patch embedding and converting
patches again to image, respectively. TB represents a standard Transformer
block. (I)CSB denotes the (inverse) Cyclic Shift block to modify the feature
map, nonetheless, the reverse operation avoids pixel shifts in the final result.
T2T block represents the Token-to-Token process [247] to improve the spatial
inductive bias of Transformers by merging the neighboring tokens. The Dilated
T2T (T2TD) block is used to refine contextual information further.
Despite the TransCT [225], Wang et al. proposed
a convolution-free Token-to-Token vision Transformer-based
Encoder-decoder Dilation network ( TED-net ) design for CT
image denoising [226]. Their approach is based on a U-Net
encoder-decoder scheme enriched by di fferent modules, i.e.,
Basic Vision Transformer, Token-to-Token Dilation (T2TD),
and (Inverse) Cyclic Shift blocks. Consider y∈RN×Na clean
natural dose CT image, x∈RN×Nnoisy low dose CT im-
age, and T:RN×N→RN×Nis a Transformer-based denois-
ing model. According to the Figure 23 after tokenization of x
and passing through the Vision Transformer block to capture
long-range dependencies and alleviate the absence of local in-
ductive bias in Transformers, they employed Token-to-Token
serialization [247]. Also, they utilized feature re-assembling
26with a Cyclic Shift block (CSB) to integrate more information.
Obvious from Figure 23, all of these blocks are replicated in
a symmetric decoder path, but instead of the CSB, the Inverse
Cyclic Shift block (ICSB) is implemented to avoid pixel shifts
in the final denoising results ( y=x+T(x)). They reached
SOTA results compared to CNN-based methods and a compet-
itive benchmark with regard to the TransCT [225].
Luthra et al. [227] proposed a Transformer-based network,
Eformer , to deal with low-dose CT images while concurrently
using the edge enhancement paradigm to deliver more accu-
rate and realistic denoised representations. Their architecture
builds upon the LeWin (Locally-enhanced Window) Trans-
former block [248], which is accompanied by an edge enhance-
ment module. The success of the Swin Transformer [58] in
capturing the long-range dependencies with the window-based
self-attention technique makes it a cornerstone in designing
new Transformer blocks due to its linear computational com-
plexity. LeWin Transformer is one of these blocks that capture
the global contextual information and, due to the presence of a
depth-wise block in its structure, could also capture a local con-
text. Eformer’s first step is through the Sobel edge enhancement
filter. In every encoder-decoder stage, convolutional features
pass through the LeWin Transformer block, and downsampling
and upsampling procedures are done by convolution and de-
convolution layers. Eformer’s learning paradigm is a resid-
ual learning scheme, meaning it learns the noise representation
rather than a denoised image due to the ease of optimization in
predicting a residual mapping.
Akin to Low Dose CT (LDCT), Low-Dose PET (LDPET)
is preferable to avoid the radiation risk, especially for cancer
patients with a weakened immune system who require multi-
ple PET scans during their treatment at the cost of sacrific-
ing diagnosis accuracy in Standard-Dose PET (SDPET). Luo
et al. [228] proposed an end-to-end Generative Adversarial
Network (GAN) based method integrated with a Transformer
block, namely 3D Transformer-GAN , to reconstruct SDPET
images from the corresponding LDPET images. To alleviate the
inter-slice discontinuity problem of existing 2D methods, they
designed their network to work with 3D PET data. Analogous
to any GAN network, they used a generator network, encoder-
decoder, with a Transformer placed in the bottleneck of the
generator network to capture contextual information. Due to
the computational overhead of Transformers, they did not build
their proposed method solely on it. Therefore, they were sat-
isfied to place a Transformer counterpart across CNN layers of
the generator to guarantee to extract low-level spatial feature
extraction and global semantic dependencies. They also intro-
duced adversarial loss term to their voxel-wise estimation error
to produce more realistic images.
In contradiction with other works, Zhang et al. [229] pro-
posed leveraging the PET /MRI data simultaneously for denois-
ing low-count PET images, which is a crucial assessment for
cancer treatment. PET scan is an emission Computed Tomog-
raphy (CT) operating by positron annihilation radiation. Due to
the foundation and requirements of PET scans, there is a severe
risk of getting infected with secondary cancer by radiotracers.
So to degrade the side e ffects of this imaging process, thereare two potential methods: reduction in radiotracer dose and
lessening the patient’s bedtime duration. The aforementioned
approaches, without a doubt, a ffect the imaging result quality
with decreased contrast to noise ratio and bias in texture. The
traditional low-count PET denoising approaches are based on
Non-Local Means (NLM) [249], Block Matching 3D (BM3D)
[250], and Iterative methods [251], etc., which are firmly in
bond with hyperparameter tuning for new data or result in un-
natural smoothings over denoised images. Zhang et al. [229]
testify that simultaneous PET /MRI could boost one modality in
terms of correct attenuation, motion, and partial volume e ffects,
and also, due to the high contrast among soft tissues in MRI, the
denoising process of PET images is preferably straightforward.
STFNet [229] is a U-Net based structure with di fferent medica-
tions. They proposed a new Siamese encoder comprising dual
input flow for each modality in the encoding path. To obtain
sufficient features from di fferent modalities, they used the Spa-
tial Adaptive (SA) block, a dual path in each block with the
residual block design, which consists of di fferent consecutive
convolutional blocks and deformable convolution with fusion
modulation. This module aims to learn more contextual fea-
tures from each modality. To leverage global attention, they
used a Transformer to produce a pixel-to-pixel interaction be-
tween the PET and the MRI modality. After this integration, the
fused features are input to the two branches based on residual
convolution blocks for PET denoising.
Wang et al. [230] proposed the enhancement for their previ-
ous work, TED-net [226] convolution-free, solely Transformer-
based network, namely CTformer . From Figure 24, it is ap-
parent that their network is an unsupervised residual learning,
U-Net-like encoder-decoder structure, rather than direct map
learning of LDCT to Natural Dose CT (NDCT). The CTformer
tries to compensate for the Transformers’ deficiency in cap-
turing path inter-boundary information and spatial inductive
bias with token rearrangement, T2T [247]. To do so, analo-
gously like TED-net, they used dilation and cyclic shift blocks
in the Token2Token block to broaden the receptive field to cap-
ture more contextual information and not increase the compu-
tational cost.
Table 8: Comparison result on NIH-AAPM-Mayo [252] dataset in low dose
enhancement task. LDE indicates the Low Dose Enhancement task.
Methods Task Dataset SSIM↑RMSE↓
Eformer
[227]LDE NIH-AAPM-Mayo
[252]0.9861 0.0067
TransCT
[225]LDE NIH-AAPM-Mayo
[252]0.923 22.123
CTformer
[230]LDE NIH-AAPM-Mayo
[252]0.9121 9.0233
Yang et al. [231] were inspired by how sinogram works and
proposed Singoram Inner-Structure Transformer ( SIST ) (Fig-
ure 25). This inner structure of the sinogram contains the
unique characteristics of the sinogram domain. To do so, they
mimic the global and local characteristics of sinogram in a
loss function based on sinogram inner-structure, namely Sino-
27512 × 51264 × 64
CTformer
Module ATB
TBDeTokenization
TB
TBEncoder    Decoder
TBTokenization
29×29
25×2529×29
25×25CTformer
Module D
CTformer
Module CCTformer
Module BT2TD IT2TD
T2TD IT2TD512 × 51264 × 64Figure 24: An overview of CTformer [230]. This structure is analogous to the
TED-net [226] structure, the previous study by the same authors.
gram Inner-Structure Loss (SISL). The global inner-structure
loss utilizes conjugate sampling pairs in CT, and local inner-
structure loss considers the second-order sparsity of sinograms.
The amalgamation of these two terms could be beneficial in re-
constructing NDCT images while retaining the noise. Due to
the CT imaging mechanism, each row of the sinogram repre-
sentation denotes the projection at a certain view. Naturally,
this procedure is suitable for leveraging the Transformer block
for modeling the interaction between di fferent projections of di-
verse angles to capture contextual information. Therefore, the
SIST module applies to raw sinogram input and captures struc-
tural information. Afterward, the unified network reconstructs
the high-quality images in a residual policy with the image re-
construction module.
HeadTail
Split
𝑆𝑙𝑑𝑠𝑙𝑑1
𝑠𝑙𝑑𝑃 …Multi -Head
Self-AttentionAdd & NormMLPAdd & Norm Image 
Reconstruction 
Module
Total 
LossNoise 
LossImage 
Loss
Conv1DResidualConv 
Conv
Conv
Conv
Conv
Conv
𝑆𝑙𝑑 𝐼𝑙𝑑መ𝑆𝑛𝑜𝑖𝑠𝑒 መ𝑆
Minus Minus𝐼𝑙𝑑መ𝑆
Image 
LossNoise 
Loss
መ𝐼𝑛𝑜𝑖𝑠𝑒መ𝐼
Sinogram Transformer ModuleInner -Structure Loss
𝑆𝑙𝑑Sinogram 
Loss
×N
Figure 25: The overall architecture of SIST [231] pipeline. SldandIldare
the LDCT sinogram and image, ˆSand ˆIdenote the output sinogram and im-
age, ˆSnoise andˆInoise are the sinogram noise and image noise. First, the LDCT
sinogram feed to the Transformer for sinogram domain denoising, then the de-
noised sinogram ˆSinput to the image reconstruction module for image domain
denoising. Within the image reconstruction module, the sinogram noise ˆSnoise
with the usage of residual CNN block generates image domain ˆInoise. NDCT ˆI,
outputs from applying refinement steps on Ildminus ˆInoise.Table 8 represents the benchmark results in the LDCT task
over the NIH-AAPM-Mayo [252] dataset respecting SSIM and
RMSE metrics on overviewed methods in this study. For clar-
ification, TED-net [226] achieved better results than CTformer
[230], but due to two studies originating from the same au-
thors and the resemblance between architectures, we preferred
to mention CTformer to count in the comparison table. This
result endorses the capability of the pure Transformer-based
Eformer [227] method in reconstructing natural dose CT im-
ages.
5.2. Sparse-View Reconstruction
Due to the customary usage of CT images in medical diag-
nosis, another policy to lessen the side e ffects of X-ray radia-
tion is acquiring fewer projections, known as sparse-view CT,
which is a very feasible and e ffective method rather than ma-
nipulating the standard radiation dose [253, 254]. However,
the resultant images from this method su ffer from severe arti-
facts, and decreasing the number of projections demands pro-
found techniques to reconstruct high-quality images. Wang et
al. [232] is the first paper that inspected the usage of Trans-
formers in this field which was quite successful, namely DuDo-
Trans . Their intuition was to shed light on the globality nature
of the sinogram sampling process, which the previous CNN ar-
chitectures neglected. DuDoTrans, unlike the conventional iter-
ative methods in this literature, does not provide blocky e ffects
in reconstructed images. This method simultaneously benefits
from enhanced and raw sinogram streams to restore informa-
tive sinograms via long-range dependency modeling in a super-
vised policy. DuDoTrans from Figure 26 is built on three main
modules, namely Singoram Restoration Transformer (SRT), the
DuDo Consistency layer, and the Residual Image Reconstruc-
tion Module (RIRM). SRT block consists of successive hybrid
Swin Transformer modules and convolutional layers to model
local semantic features and inherent global contextual informa-
tion in the sinogram to produce the enhanced sinogram.
Residual Image Reconstruction Module(RIRM)
Shallow LayerDeep Feature 
Extraction LayersRecon Layer
DuDo 
Consistency LayerFBPNormSW-MSANormMLP
Input OutputSwin-Transformer Module (STM)
 ConvSTM…STM Conv Conv
 Conv…STM…STM ConvSinogram Restoration Transformer (SRT)
Figure 26: DuDoTrans [232] framework for sparse-view CT image reconstruc-
tion. First, the sparse-view sinogram Ymaps to a low-quality image eX1and
other estimation eX2generated by SRT module’s enhanced sinogram output eY
followed by DuDo Consistency Layer. Lastly, the predicted estimations are
concatenated and fed to the RIRM module that outputs the CT image of eXin a
supervised manner.
Buchholz et al. [233] presented the Fourier Image Trans-
former ( FIT) that operates on the image frequency representa-
tion, especially the Fourier description of the image, which in
their study is known as Fourier Domain Encoding (FDE), that
encodes the entire image at lower resolution. The intuition in
their idea is underlying the CT’s acquisition process physics.
CT utilizes a rotating 1D detector array around the patient body
28to calculate the Radon transform [255] of a 2D object, which
leads to a sequence of density measurements at di fferent pro-
jection angles, namely sinogram as a 2D image in which each
column of this representation corresponds to one 1D measure-
ment. The Filtered Back Projection (FBP) [256, 255] is a re-
construction method to map sinograms to tangible CT images.
FBP is based on the Fourier slice theorem; hence, computing
the 1D Fourier transform of 1D projection and rearranging them
by their projection angle in Fourier space, followed by an in-
verse Fourier transformation, results in a reconstructed 2D CT
image slice. Limiting the number of projections leads to miss-
ing Fourier measurements, which ultimately conduce to recon-
struction artifacts. FIT is the first study that uses a Transformer
to query arbitrary Fourier coe fficients and fill the unobserved
Fourier coe fficients to conceal or avoid the probable artifacts
in reconstruction within sparse-view CT reconstruction litera-
ture. From Figure 27 this procedure starts with calculating the
FDE of the raw sinogram. To do so, first, the discrete Fourier
transform (DFT) of the sinogram will be calculated. Secondly,
after dropping half of the coe fficients on the Fourier rings of
the resultant Fourier representation, it preserves the lower fre-
quency counterparts to recover the lower resolution of the raw
sinogram. Afterward, the complex coe fficients convert into 1D
sequences by unrolling the Fourier rings. These complex values
convert to normalized amplitudes and phases. Therefore, each
complex coe fficient has its own polar representation, which is
a normalized real-valued matrix with N×2 entries ( Nis equal
to half of the DFT coe fficients number). A linear layer applies
on this tensor to upsample the feature dimensionality toF
2. Fi-
nally, a 2D positional encoding concatenates to this tensor and
produces a 2D FDE image with the size of N×F.
F C - L o s s MSE-Loss1D FFT
2D iFFTEncoder Decoder
2D FFT
2D FFTFBP
Figure 27: FIT [233] framework for sparse-view CT reconstruction. FDE rep-
resentation of the sinogram calculates that serves as an input to an encoder
of Transformer design. The decoder predicts the Fourier coe fficients from the
encoder’s latent space. The Fourier coe fficients of applying the FBP [256] al-
gorithm on sinogram information are fed into a Transformer’s decoder to enrich
the Fourier query representation. A shallow CNN block applies after inverse
FFT to hamper the frequency oscillations.
Shi et al. [234] presented a CT reconstruction network
with Transformers ( CTTR ) for sparse-view CT reconstruction.
In contrast to DuDoTrans [232], CTTR enhances low-quality
reconstructions directly from raw sinograms and focuses on
global features in a simple policy in an end-to-end architecture.
CTTR contains four parts: two CNN-based residual blocks ex-
tracting local features from FBP [256] images reconstruction
and sinograms, an encoder-decoder Transformer for long-rangemodeling dependencies, and contextual information between
features, and a CNN block to map features to a high-quality
reconstruction.
Cone-Beam Computed Tomography (CBCT) is a conven-
tional way of dental and maxillofacial imaging; due to its fast
3D imaging qualifications, its popularity has extended to lung
imaging. However, studies approved that its radiation dose is
higher than plain radiographs [258] hence sparse-view CBCT
could be a suitable method to lower radiation dose. Wu et
al. [235] proposed a novel untrained 3D Transformer-based
architecture, namely ARMLUT , with a multi-level loss func-
tion for CBCT reconstruction. While the Transformer mod-
ule, especially the UNETR [159] in this study, captures long-
range contextual information and enhances the resulting image.
The intuition behind this strategy is Deep Image Prior (DIP)
[259] to succeed in the reconstruction field. From Figure 28a,
ARMLUT is an iterative optimization problem between the Im-
age Reconstructor module and Image Generator module to fit a
CBCT inverse solver without a large number of data or ground
truth images. The multi-level loss function comprises a Mean
Squared Error (MSE) and Perceptual Loss (PL) [260] to recon-
struct smooth and streak artifact-free outputs. The entire frame-
work (Figure 28) has three main counterparts: Image Recon-
structor, Image Generator, and Feature Extractor. Image Re-
constructor uses Feldkamp-Davis-Kress (FDK) algorithm [257]
to produce a low enhanced reconstruction from M-view mea-
surements, and the Image generator module maps the noisy
voxel inputs to reconstruct a regularised image. The Feature
Extractor module applies the VGG-11 pre-trained network on
two representations and produces a PL paradigm. To minimize
the distance between these two reconstructions, ARMLUT uti-
lizes an adaptively re-weight multi-loss technique to stabilize
the convergence of the Transformer in the optimization.
5.3. Undersampled Reconstruction
Magnetic Resonance Imaging (MRI) is a dominant technique
for assistive diagnosis. However, due to the physics behind its
operation, the scanning time can take longer and be very te-
dious, a ffecting the patient experience and leading to inevitable
artifacts in images [269]. Hence, reducing the number of MRI
measurements can result in faster scan times and artifacts re-
duction due to the patient’s movement at the cost of aliasing
artifacts in the image [245].
Lin et al. [236] proposed a comprehensive analytical study
to investigate the usage of ViT in a pure (CNN-free modules)
and most straightforward Transformer design. This study is ev-
idence of the prominent e ffect of ViTs in medical image recon-
struction. For this work, they adopted the original ViT [15] for
image reconstruction by discarding the classification token and
replacing the classification head with the reconstruction head,
which is comprised of successive Norm and Linear layers to
map the Transformer output to a visual image. They performed
a complete ablation study with di fferent ViT settings, from the
number of stacked Transformers to embedding dimension and
the number of heads in Multi-Head Self-Attention (MHSA).
Their results were quite e ffective and proved that trained ViT
on su fficient data from natural images like ImageNet or medical
29(a) ARMLUT [235] pipeline. The collaboration of three distinct modules—FDK algorithm
[257], prior embedding with Transformer, and VGG-11 network for extracting hierarchical
features—in this pipeline generates the reconstructed CBCT image. Red texts in the figure
denote the variable weights that contribute to the iterative optimization step.
(b) UNETR [159] used as an image generator module in the ARMLUT paradigm.
Figure 28: (a) represents multi-loss untrained network for sparse-view CBCT reconstruction. (b) architecture of UNETR [159], as a Transformer module in a
ARMLUT.
data could perform better or achieve on-par reconstruction ac-
curacies compared to CNN baselines such as U-Net [28]. The
proposed design’s distinguished power based on the mean at-
tention distance metric [270] proves that it e ffectively mimics
the convolutional receptive fields and could concurrently cap-
ture local and global dependencies. In addition, they showed
that the ViT benefits from two times faster inference times and
fewer memory requirements compared to the U-Net.
Feng et al. [237] address the particular issue in this do-
main by designing an end-to-end multi-task learning paradigm
to boost feature learning between two sub-tasks, MRI recon-
struction, and super-resolution, which have a high overlap
with each other named Task Transformer Network ( T2Net) is
showed in Figure 29. Their network consists of two branches,
each for a specific task. T2Net utilizes a Transformer between
two branches to share feature representation and transmission.
T2Net applies a convolution layer and EDSR [271] backbone
to extract task-specific features in each task branch. To share
information between two branches and benefit from the inter-
actions of these two task-specific features concerning the na-
ture of the Transformer’s globality, T2Net uses a unique Trans-
former design to learn a generalized representation. Since the
reconstruction branch has more potential in artifact removal ca-
pacity than the super-resolution branch, the task Transformer
module guides the super-resolution branch into high-quality
representation from the reconstruction branch. The Trans-
former module inherits the query ( Q: from super-resolution
branch), key ( K: from reconstruction branch), and value ( V:
from reconstruction branch) from each scale’s two branches’
output. It forms three main concepts: relevance embedding,
Transfer attention, and soft attention, which di ffer from the
original Transformer blocks. Relevance embedding tries to en-
close the correlated features from the reconstruction branch to
the super-resolution branch. Transfer attention aims to transmit
the anatomical and global features between two branches, and
last but not least, soft attention amalgamates features from the
previous two steps. Ultimately, this module lets the whole net-work transfer and synthesize the representative and anatomical
features to produce a high-quality, artifacts-free representation
from highly undersampled measurements. The experimental
results on two datasets expressed the high potential of this ap-
proach rather than conventional algorithms.
Conv Conv
Upsamplelong skip connection
long skip connection
Element -wise sum
super -resolution branch
reconstruction branchˆxLR
x′LRx′HRB
SR1HRB
SR2HRB
SRN
HRB
Rec1HRB
Rec2HRB
RecNHtt
1Htt
2Htt
NF0
SR
F0
RecF1
SR
F1
RecF2
SR
F2
RecFN
SR
FN
RecF1
TTF2
TTFN
TT
(a) An overview of T2Net [272], a multi-task learning framework that consists of a super-
resolution branch and reconstruction branch. The reconstruction branch embraces the
stronger capability of artifact removal therefore, the task Transformer module is fed with
the reconstruction branch.
Transfer  AttentionRelevance 
Embeding
Concanate
Conv
Soft Attention Element -wise 
multiplication
ConvQ
K
VFi
SR
Fi
Rec
Fi
Rec↑↓TS
C
ZFTT
(b) inner design of proposed Htttask Transformer module. Analogous to Figure 2, the
design of T2module follows the naive design with some modifications, and in contrast to
seminal design, all Q,K, and Ventities do not originate from the same representation— Q
comes from the super-resolution branch and the rest from the reconstruction branch.
Figure 29: An overview of T2Net [272]. (a) Multi-Task T2Net pipeline and (b)
Task Transformer Module—T2Module
5.4. Super-Resolution Reconstruction
Improving the resolution of images leads to the more detailed
delineation of objects. Increasing the medical image resolu-
tion plays a crucial role in computer-aided diagnosis due to
its rich anatomical and textural representation. Based on the
aforementioned fact and the MRI’s pipeline physics during the
image acquisition process for having high-resolution images,
a patient needs to lie a long time in the MRI tube. Hereupon
30Table 9: Medical Image Reconstruction. LDE ,SVR,USR, and SRR stand for Low Dose Enhancement, Sparse-View Reconstruction, Undersampled Reconstruc-
tion, and Super Resolution Reconstruction, respectively. †indicates that this network uses a pre-trained perceptual loss (loss network).
Method Task(s) Modality Type Pre-trained Module: Type Dataset(s) Metrics Year
Pure
TED-net [226] LDE CT 2D ✗ NIH-AAPM-Mayo Clinical LDCT [252]SSIM
RMSE2021
Eformer [227] LDE CT 2D ✗†NIH-AAPM-Mayo Clinical LDCT [252]PSNR, SSIM
RMSE2021
CTformer [230] LDE CT 2D ✗ NIH-AAPM-Mayo Clinical LDCT [252]SSIM
RMSE2023
FIT [233] SVR CT 2D ✗ LoDoPaB [261] PSNR 2021
ViT-Rec [236] USR MRI 2D Supervised fastMRI [262] SSIM 2021
Encoder
TransCT [225] LDE CT 2D ✗1NIH-AAPM-Mayo Clinical LDCT [252]
2Private clinical pig head CBCTRMSE
SSIM
VIF2021
STFNet [229] LDEPET
MRI2D ✗ Private DatasetRMSE, PSNR
SSIM, PCC2022
SIST [231] LDE CT 2D ✗1LDCT Dataset [263]
2Private datasetPSNR, SSIM
RMSE2022
DuDoTrans [232] SVR CT 2D ✗ NIH-AAPM-Mayo Clinical LDCT [252]PSNR, SSIM
RMSE2021
CTTR [234] SVR CT 2D ✗ LIDC-IDRI [264]RMSE, PSNR
SSIM2022
Skip Connection
Cohf-T [239] SRR MRI 2D ✗1BraTS2018 [202]
2IXI [265]PSNR
SSIM2022
Other Architectures
3D T-GAN [228] LDE PET 3D ✗ Private DatasetPSNR, SSIM
NMSE2021
ARMLUT [235] SVR CT 3D ViT: Supervised†1SPARE Challenge Dataset [266]
2Walnut dataset [267]PSNR, SSIM 2022
T2Net [237]USR
SRRMRI 2D ✗1IXI [265]
2Private DatasetPSNR, SSIM
NMSE2021
DisCNN-ViT [238] SRR MRI 3D ViT: Self-Supervised1fastMRI [262]
2IXI [265]PSNR, SSIM
NMSE2021
SLATER [240] USR MRI 2D ViT: Self-Supervised1fastMRI [262]
2IXI [265]PSNR, SSIM 2022
DSFormer [241] USR MRI 2D ViT: Self-Supervised IXI [265] PSNR, SSIM 2023
lower signal-to-noise ratio and more minor spatial coverage
drawbacks are inevitable [269]. Therefore in this section, we
investigate Transformer-utilized algorithms that try to alleviate
this problem. Of note, due to the analogy between MRI and
super-resolution reconstruction, some studies investigate these
two tasks in conjunction with each other.
Mahapatra et al. [238] proposed the GAN-based model with
structural and textural information preservation done by mul-
tiple loss function terms. Their pipeline included two pre-
trained modules named feature disentanglement module, a con-
ventional autoencoder, and a Transformer-based feature en-
coder, UNETR [159]. UNETR captures the global and local
context of the original low-resolution image and induces the
high-resolution image to preserve these contexts too. These
two modules fine-tune on a di fferent medical dataset, and after-
ward, the low-resolution input plus the intermediate generator
produced image feed to these modules. The disentanglementnetwork contains two autoencoders to learn two counterparts,
latent space, structural and textural space, with fed medical im-
ages. In an end-to-end setting, these two pre-trained assistive
modules help to generate more realistic and structural and tex-
tural preserving high-resolution images by imposing module-
related loss terms such as adversarial loss to constrain for pro-
ducing realistic images and cosine similarity loss for each men-
tioned module. Results on the IXI dataset proved that Ma-
hapatra et al.’s [238] network outperformed a couple of the
CNN-based attention mechanism networks and T2Net [237].
Maintaining structural information during the acquiring high-
resolution images plays a crucial role. Hence, the structure
information is embedded in an image’s high-frequency coun-
terpart, like in an image’s gradients. In addition, due to the less
time-consuming nature of obtaining MR T1WI (T1 Weighted
Image), it is wise to use it as an inter-modality context prior
to producing a high-resolution image. Accordingly, Fang et
31Table 10: A brief description of the reviewed Transformer cooperated in the medical image reconstruction field.
Method Contributions Highlights
Low Dose Enhancement
TransCT [225] •The proposed prototype was the first successful implementation of a Transformer complement to CNN in
the Low Dose CT reconstruction domain by exploring its revenue within high-frequency and low-frequency
counterparts.•The Transformer e ffectively could learn the embedded texture representation from the noisy counterpart.
•This paradigm is not convolution-free and uses Transformer as a complement.
TED-net [226] •Convolution-free U-Net based Transformer model. •Introduced Dialted Token-to-Token-based token serialization for an improved receptive field in Trans-
formers.
•Using Cyclic Shift block for feature refinement in tokenization.
Eformer [227] •Incorporate the learnable Sobel filters into the network for preserving edge reconstruction and improving
the overall performance of the network.
•Conduct diverse experiments to validate that the residual learning paradigm is much more e ffective than
other learning techniques, such as deterministic learning approaches.•Successfully imposed the Sobel-Feldman generated low-level edge features with intermediate network
layers for better performance.
•To guarantee the convergence of the network, they used the Multi-scale Perceptual (MSP) loss alongside
Mean Squared error (MSE) to hinder the generation of disfavored artifacts.
3D T-GAN [228] •It is a 3D-based method rather than conventional 2D methods.
•First LDPET enhancement study that leveraged from Transformer to model long-range contextual infor-
mation.•To produce more reliable images with a generator they used an adversarial loss to make the data distribu-
tion the same as real data.
STFNet [229] •Proposed a dual input U-Net-based denoising structure for low-count PET images with excessive MRI
modality contribution.
•Used a Transformer block as a hybrid add-on for feature fusion to make a pixel-to-pixel translation of
PET and MRI modalities.•In comparison with the U-Net and residual U-Net structures due to the di fferent training strategy which is
roughly named Siamese structure has a low computational burden and simplified network.
•This network successfully handled the disparity and nonuniformity of shape and modality of PET and
MRI.
•The visual results of denoised images testify that the proposed method could recover the detail of texture
more clearly than other networks.
CTformer [230] •Convolution-free, computational e fficient design.
•Introduce a new inference mechanism to address the boundary artifacts.
•Proposed interpretability method to follow each path resultant attention map through the model to under-
stand how the model is denoising.•Alleviate receptive filed deficiency with the token rearrangement.
SIST [231] •Proposed inner-structure loss to mimic the physics of the functionality of sinogram processing by CT
devices to restrain the noise.
•Extracting the long-range dependencies between distinct sinogram angles of views via Transformer.•Utilizing the image reconstruction module to alleviate the artifacts that could happen in sinogram domain
denoising by transferring the sinogram noise into the image domain.
•Image domain loss back-propagates into the sinogram domain for complementary optimization.
Sparse-View Reconstruction
DuDoTrans [232] •To cope with the global nature of the sinogram sampling process introduced, the SRT module, a hybrid
Transformer-CNN to capture long-range dependencies.
•Utilizing a dual domain model to simultaneously enrich raw sinograms and reconstruct CT images with
both enhanced and raw sinograms.
•To compensate for the drift error between raw and enhanced sinogram representation employs DuDo
Consistency Layer.•Utilizing a residual learning paradigm for image-domain reconstruction.
•Fewer parameters in comparison with other structures, e.g., DuDoNet [268] with better performance.
FIT [233] •Introduced the Fourier Domain Encoding to encode the image to a lower resolution representation for
feeding to the encoder-decoder Transformer for reconstructing the sparse-view CT measurements.•Introduced the Fourier coe fficient loss as a multiplicative combination of amplitude loss and phase loss in
the complex domain.
CTTR [234] •Introduced the encoder-decoder Transformer pipeline to utilize dual-domain information, raw sinogram,
and primary reconstruction of CT via FBP [256] algorithm for sparse-view CT measurement reconstruction.•In contrast to DuDoTrans [232], CTTR directly utilizes raw sinograms to enhance reconstruction perfor-
mance.
ARMLUT [235] •Proposed a paradigm for CT image reconstruction in a non-trainable manner.
•Extending the most DIP research on 2D to 3D medical imaging scenario.•Optimising the large-scale 3D Transformer with only one reference data in an unsupervised manner.
•Stabilising the iterative optimization of multi-loss untrained Transformer via re-weighting technique.
Undersampled Reconstruction
ViT-Rec [236] •This study investigated the advantage of pure Transformer framework, ViT, in fastMRI reconstruction
problems in comparison with the baseline U-Net.
•ViT benefits from less inference time and memory consumption compared to the U-Net.•Utilizing pre-training weights, e.g., ImageNet, extensively improves the performance of ViT in the low-
data regime for fastMRI reconstruction, a widespread concept in the medical domain.
•ViTs that accompany pre-training weights demonstrate more robust performance toward anatomy shifts.
T2Net [237] •Introduce the first Transformer utilized multi-task learning network in the literature.
•Designed the task Transformer for maintaining and feature transforming between branches in the network.
•Outperformed the sequentially designed networks for simultaneous MRI reconstruction and super-
resolution with T2Net.•Used the same backbone for feature extraction in branches, however, the purpose of the branches is
diverse.
Super Resolution Reconstruction
DisCNN-ViT [238] •Using a Transformer-based network to capture global contextual cues and amalgamate them with CNN’s
local information results in the superior quality of high-resolution images in super-resolution literature.
•Creating realistic images is just not a burden on an adversarial loss function, in addition, multiple loss
functions incorporate extra constraints that preserve anatomical and textural information in the begotten
image.•Multi prerequisite steps are required to train the experiments; however, the super-resolution step is a
straightforward end-to-end network.
•Need fine-tuning steps for two disentanglement and UNETR networks.
•The computational burden of UNETR is high and could use new e fficient transformer-designed networks.
Cohf-T [239] •Leverage the high-resolution T1WI due to its rich structural information for super-resolving T2-weighted
MR images.
•Introduced the high-frequency structure prior and intra-modality and inter-modality attention paradigms
within the Cohf-T framework.•Assess prior knowledge into super-resolution paradigm successfully.
•Competitive number of FLOPS in reaching SOTA PSNR results in comparison with other attention-based
networks.
•End-to-end pipeline for training the network.
Self-supervised Reconstruction
SLATER [240] •A Self-supervised MRI reconstruction framework that uses unconditional adversarial network and cross-
attention transformers for high-quality zero-shot reconstruction.
•The first work that proposes an adversarial vision transformer framework designed for the task of MRI
reconstruction.•SLATER outperforms CNN and self-attention GAN models, showing better invertibility and improved
reconstruction performance with cross-attention transformer blocks and unsupervised pretraining.
•Prior adaptation is computationally burdensome but improves across-domain reconstructions.
DSFormer [241] •Proposes a deep learning model for fast MRI reconstruction that uses a deep conditional cascade trans-
former and self-supervised learning to exploit contrast redundancy and reduce data requirements.•Both k-space and image-domain self-supervision independently produce strong reconstruction quality,
showing the benefits of self-supervision in both domains.
al. [239] devised a network (see Figure 30) to leverage these
two concerns in their super-resolution pipeline: Cross-Modality
High-Frequency Transformer ( Cohf-T ). This network is di-
vided into two streams, the first stream is applied on low-
resolution T2WI, and the second one manipulates T2WI’s gra-
dient and high-resolution T1WI. The Cohf-T module interacts
between two streams to embed the prior knowledge in the
super-resolution stream’s features. The Cohf-T module con-
sists of three di fferent attention modules: short-distance and
long-distance window attention and inter-modality attention.
The first two attention modules help to model intra-modality
dependency. To be precise, the short-distance window helps re-
cover the local discontinuity in boundaries with the help of sur-
rounding structure information, and the long-distance window
can capture the textural and structural patterns for enhanced re-sults. Due to the discrepancy in intensity levels between T1WI
and T2WI, it is vital to make an alignment between these two
domains, and Fang et al. [239] presented a Feature Align-
ment (FA) module to reduce the cross-modality representation
gap. They compared their results with T2Net [237] and MTrans
[273], which outperformed both approaches by ∼1% in terms
of PSNR.
5.5. Self-supervised Reconstruction
Recently, self-supervised reconstruction methods requiring
only undersampled k-space data have been proposed for single-
contrast MRI reconstruction, enabling the autonomous learn-
ing of high-quality image reconstructions without the need for
fully sampled data or manual annotations. SLATER [240]
is introduced as a self-supervised MRI reconstruction method
32Conv
5xRRDBConv
5xRRDBConv
5xRRDB
MLP
MLP
MLPConv
Cross -modality High -frequency Transformer ( Cohf -T)FA
Cohf -T5xRRDB𝐈𝑖𝑛
Conv
𝐑𝑐𝐅0𝐄1
𝐅1
𝐅𝑠0
𝐅𝑐0𝐅𝑠1
Cohf -T5xRRDB𝐄2
𝐅2𝐅3
𝐏1𝐄4
𝐅4
Output Gate
𝐏4𝐏2ത𝐅𝑠1
𝐑𝑠Conv
𝐅𝑐0
Cohf -T5xRRDB𝐄3
𝐏3
𝐅𝑐0𝐓1
Input Gateaddition
concatenation
broadcast element -wise 
product
Sigmoid function𝐈𝑜𝑢𝑡
𝐑𝑜𝑢𝑡Short -distance
Window Attention
Long-distance
Window Attention
Inter -modality 
AttentionFigure 30: The pipeline of Cohf-T [239] consists of three main branches with
the corresponding input modalities as follows: Iin,Rs, and Rcdenote the
low-resolution T2WI, the gradient of low-resolution T2WI and high-resolution
T1WI, respectively. A fully-convolutional branch for density-domain super-
resolution, a Transformer-based branch for restoring high-frequency signals in
the gradient domain, and a guidance branch for extracting priors from the T1
modality. Conv ,RRDB andMLP represent a 3×3 convolution operation and
residual-in-residual dense block and multi-layer perceptron, respectively.
that combines an unconditional adversarial network with cross-
attention transformers. It learns a high-quality MRI prior
during pre-training and performs zero-shot reconstruction by
optimizing the prior to match undersampled data. SLATER
improves contextual feature capture and outperforms existing
methods in brain MRI. It has potential for other anatomies and
imaging modalities, o ffering reduced supervision requirements
and subject-specific adaptation for accelerated MRI. Likewise,
DSFormer [241] is presented for accelerated multi-contrast
MRI reconstruction. It uses a deep conditional cascade trans-
former (DCCT) with Swin transformer blocks. DSFormer is
trained using self-supervised learning in both k-space and im-
age domains. It exploits redundancy between contrasts and
models long-range interactions. It also reduces the need for
paired and fully-sampled data, making it more cost-e ffective.
5.6. Discussion and Conclusion
In this section, we outline di fferent Transformer-based ap-
proaches for medical image reconstruction and present a de-
tailed taxonomy of reconstruction approaches. We reviewed
17 studies that profit from the Transformer design to com-
pensate for the deficiency of CNN’s limited receptive field,
with two of them incorporating self-supervised strategies
[240, 241] for training transformer modules. We investigate
each study in depth and represent Table 9 for detailed in-
formation about the dataset, utilized metrics, modality, and
objective tasks. In Table 10, we provide the main contri-
bution of each study and the prominent highlight of each
method, and in Table 8, we present a comparison of vari-
ous methods for the low-dose enhancement task using the
NIH-AAPM-Mayo [252] benchmark medical reconstruc-
tion dataset, which shows that Eformer [227] is superior as
compared to TransCT [225] and CTformer [230], consider-
ing its higher SSIM and lower RMSE scores.
Most of the studies in this domain use the original Trans-
former as a plug-and-play module in their design and only
a limited number of studies utilize hierarchical and e ffi-
cient Transformers. However, the criteria for using multi-
scale and hierarchical architectures are generally important
for dense prediction tasks, e.g. image reconstruction, andshould be considered further. Also, another direction to fol-
low for future research could be to investigate the influence
of using pre-training weights on Transformers due to the
need for a large amount of data for better convergence re-
sults in Transformers, which contradicts the nature of the
medical domain, due to the scarceness of annotated medical
data.
In addition, we noticed that most of the studies focus on
MRI and CT image reconstruction tasks. So there is a need
for evaluating the applicability of these methods on other
modalities, too.
6. Medical Image Synthesis
In this section, we will overview several instances of Trans-
formers in the medical image synthesis task. The scarcity of
medical data and the high cost of acquisition processes make
this task very valuable in the medical field. Some studies aim
to synthesize missing slices from MRI and CT sequences. In
addition, some methods target capturing the structural infor-
mation in diverse modalities, e.g., CT to MRI image-to-image
translation and vice versa. Figure 31 shows our taxonomy for
the image-synthesized methods.
 Medical Image Synthesis
 Intra-Modality
 1. PTNet
 2. ResViT
 3. MMT Inter-Modality
 2. ResViT
 4. CyTran
 5. VTGAN
 6. SLMT-Net
Figure 31: An overview of ViTs in medical image synthesis. Methods are
categorized by target and source modality. The prefix numbers in the paper’s
name in ascending order denote the reference for each study as follows: 1.
[274], 2. [275], 3. [276], 4. [277], 5. [278], 6. [279].
6.1. Intra-Modality
The main objective of the intra-modality methods is to syn-
thesize high-quality images using low-quality samples from
the same modality. In this respect, several Transformer-based
approaches are presented to formulate the synthesis task as
a sequence-to-sequence matching problem to generate fine-
grained features. In this section, we will briefly present some
recent samples [274].
Brain development monitoring is a de facto standard in pre-
dicting later risks; hence it is critical to screen brain biomark-
ers via available imaging tools from early life stages. Due to
33this concern and the nature of MRI and subjective infants’ rest-
lessness, it is not relatively straightforward to take all the MR
modalities during the MRI acquisition. Zhang et al. [274] pro-
posed a Pyramid Transformer Net (PTNet) as a tool to re-
construct realistic T1WI images from T2WI. This pipeline is an
end-to-end Transformer-based U-Net-like and multi-resolution
structure network utilizing an e fficient Transformer, Performer
[280], in its encoder (PE) and decoder (PD). Analogously to the
original U-Net [28], they used skip connection paths for pre-
serving fine-grained features and accurate localization features
for reconstruction. Moreover, the paradigm’s two-level pyra-
midal design helps the network capture local and global infor-
mation in a multi-resolution fashion. They achieved the SOTA
results on the dHCP [281] dataset compared with the flagship
GAN-based image generation method pix2pix (HD) [282, 283].
Figure 32: The ResViT [275] framework for multi-modal medical image syn-
thesis. The bottleneck of this encode-decoder comprises successively residual
Transformers and residual convolutions layers for synergistically capturing the
fine-grained global and local context.
Dalmaz et al. [275] introduced a conditional generative ad-
versarial network based on the cooperation of Transformers
and CNN operators, namely ResViT . This paradigm addresses
the issue of needing to rebuild separate synthesis models for
varying source-target modality settings and represents a unified
framework as a single model for elevating its practicality. The
ResViT (Figure 32) pervasively refers to the generator of its
pipeline, whereby it leverages a hybrid pipeline of residual con-
volutional operations and Transformer blocks that enable ef-
fective aggregation of local and long-range dependencies. The
discriminator is based on a conditional PatchGAN framework
[282]. Utilizing standalone Transformer architectures (e.g., PT-
Net [274]) in pixel-to-pixel tasks is quite challenging due to
the quadratic complexity, which limits its usage to fixed-size
patches that hamper its e ffectiveness. From Figure 32, it is
evident that residual Transformer blocks stacked successively,known as aggregated residual Transformer (ART) blocks, in the
bottleneck of the encoder-decoder design of the generator to ex-
tract the hidden contextual information of input features. The
primary motivation of ART blocks is to learn an integrated rep-
resentation that combines contextual, local, and hybrid local-
contextual features underhood from the input flow. Channel
Compression (CC) module recalibrates the concatenated fea-
tures from the previous ART block and Transformer module to
select the most discriminant representations. Due to the cas-
cade of Transformers in design, to decrease the model com-
plexity and computational burden, ResViT utilizes weight shar-
ing strategy among projection tensors for Query, Key, value,
and attention heads besides weight matrices for multi-layer per-
ceptron operation. The superiority of this method has been
proved over several MRI datasets in multi-contrast MRI synthe-
sis and MRI to CT experiments with high PSNR and SSIM met-
rics over the conventional SOTA methods, e.g., pGAN [284],
SAGAN [285], pix2pix [282] and PTNet [274].
Likewise, Liu et al. [276] addressed the issue of missing
contrasts in MRI imaging and proposed a multi-contrast multi-
scale Transformer ( MMT ) framework to handle the unavail-
ability of this information by synthesizing the existing con-
trasts as a means to substitute the missing data. To achieve
efficient contrast synthetization, the task is considered as a seq-
to-seq problem, in which the model learns to generate missing
contrasts by leveraging the existing contrast in the following
manner: A Swin multi-contrast Transformer encoder is imple-
mented that creates hierarchical representation from the input
MRI image. Then, a Swin Transformer-based architecture de-
codes the provided representation at multiple scales to perform
medical image synthesis. Both the encoder and decoder are
composed of two sequential swin blocks that capture contrast
dependencies e ffectively. Conducted experiments on the IXI
[265] and BraTS [206] datasets demonstrated MMT’s advan-
tage compared to previous methods.
Table 11: Comparison results on BraTS [206] dataset for medical image syn-
thesis, focusing on one-to-one and many-to-one tasks with di fferent input to
output combinations of T1, T2, and FLAIR modalities. The symbol ( †) indi-
cates that the values are taken from [279]. The GFLOPs of all methods are
calculated with an input image size of 256 ×256.
Methods Design Task SSIM↑PSNR↑Params (M) FLOPs (G)
PTNet†[279] Hybrid T1, T2 →FLAIR
T1, FLAIR→T2
T2, FLAIR→T1
T2→FLAIR
FLAIR→T20.851
0.905
0.920
0.851
0.89423.78
25.09
22.19
23.01
24.7827.69 233.1
MMT [276] Pure T1, T2 →FLAIR
T1, FLAIR→T2
T2, FLAIR→T1
T2→FLAIR
FLAIR→T20.909
0.934
0.922
0.891
0.90226.20
26.74
25.43
25.00
24.84138.2 221.5
ResViT [275] Bottleneck T1, T2 →FLAIR
T1, FLAIR→T2
T2, FLAIR→T1
T2→FLAIR
FLAIR→T20.886
0.938
0.924
0.870
0.90825.84
26.90
26.20
24.97
25.78123.4 486.1
6.2. Inter-Modality
Unlike the intra-modality strategies, the inter-modality meth-
ods are designed to learn the mapping function between two
different modalities. This approach allows the network to con-
vert the samples from the base modality into a new modality
34and leverage the generated samples in the training process for
the sake of performance gain. In this section, we will elaborate
on two Transformer-based [277, 278] strategies.
Several medical conditions may prevent patients from re-
ceiving intravenous contrast agents while getting CT screen-
ing. However, the contrast agent is crucial in assisting medi-
cal professionals in identifying some specific lesions. There-
fore, CyTran [277] is proposed as an unsupervised generative
adversarial convolutional Transformer for translating between
contrast and non-contrast CT scans and image alignment of
contrast-enhanced CT scans to non-enhanced. Its unsupervised
part is also derived from its cyclic loss. CyTran is composed
of three main modules: I)A downsample CNN-based mod-
ule designed for handling high-resolution images, II)A con-
volutional Transformer module tailored for incorporating both
local and global features, and III)An upsampling module de-
veloped to revert the transformation of the downsampling block
through transpose-convolution. Additionally, the authors intro-
duce a new dataset, Coltea-Lung-CT-100W, comprised of 100
3D anonymized triphasic lung CT scans of female patients.
Furthermore, Kamran et al. [278] trained a ViT-based gener-
ative adversarial network ( VTGAN ) in a semi-supervised fash-
ion on the Fundus Fluorescein Angiography (FFA) dataset pro-
vided by [288] via incorporating multiple modules, including
residual blocks as generators for coarse and fine image gener-
ation, and two ViT architectures consisting of identical trans-
former encoder blocks for concurrent retinal abnormality clas-
sification and FA image synthesis.
To embed self-supervised training in medical image syn-
thesis, SLMT-Net [279] is presented for generating missing
modalities in magnetic resonance (MR) images. It tackles the
limited paired data challenge by combining paired and unpaired
data. The method involves edge-aware pre-training using an
Edge-preserving Masked Auto-Encoder (Edge-MAE) model
that predicts missing information and estimates the edge map.
A patch-wise loss enhances the performance. In the multi-
scale fine-tuning stage, a DSF module is integrated to synthe-
size missing-modality images using multi-scale features. The
pre-trained encoder extracts high-level features to ensure con-
sistency between synthesized and ground-truth images during
training.
6.3. Discussion and Conclusion
This section covers the adoption of ViT architectures in
medical image synthesis applications. We explored the pro-
posed methods based on two synthesis approaches: (I)inter-
modality, in which the target modality is synthesized in a
way that encapsulates crucial diagnostic features from dif-
ferent source images; and (II)intra-modality, with the ob-
jective of yielding target images with better quality by in-
tegrating information from lower resolution source images.
To demonstrate their e ffectiveness, these approaches usu-
ally rely on SSIM, PSNR, and LPIPS as the evaluation
metrics, since they are designed to measure the similarity
between images. We also reviewed a ViT-based synthe-
sis model [278] that operates in a decoder fashion for thetask of fundus-to-angiogram translation with di fferent eval-
uation measurements, including Fr ´echet Inception Distance
(FID) and Kernel Inception Distance (KID). A combination
of self-supervised learning with transformer-based medical
synthesis is further reviewed to emphasize the cost-saving
benefits of reduced labeling [279]. We have additionally
provided the architectural type, modality, input size, training
setting, datasets, metrics, and year for every medical regis-
tration technique analyzed in Table 12. Furthermore, Ta-
ble 13 lists the contributions and highlights of the proposed
works. To facilitate comparison, table Table 11 includes
performance evaluation of multiple methods on the BraTS
dataset [206]. In terms of SSIM and PSNR scores, MMT
[276] typically achieves superior performance, while PTNet
[279] falls slightly behind when compared to the other two
methods. Moreover, we have provided information about
the model size and computational cost for three distinct ap-
proaches: PTNet, MMT, and ResViT. PTNet, with 27.69
million parameters and 233.1 GFLOPs, strikes a balance be-
tween model size and computational demand. Meanwhile,
MMT o ffers a larger model with 138.2 million parameters
but lower computational requirements at 221.5 GFLOPs,
emphasizing e fficiency. On the other hand, ResViT pro-
vides a mid-sized model with 123.4 million parameters but
a notably higher computational load of 486.1 GFLOPs. The
choice among these methods should be guided by specific
task requirements, weighing the trade-o ffbetween model
size and computational cost to achieve optimal performance.
However, it is important to note that all these models require
a significant computational cost.
With the scarcity of works with ViT implementations and
the recent advancement in the medical synthesis field with
Transformer models, we believe that these systems require
more research e ffort to be put into them. For example,
Transformers have much room for improvement to generate
more realism and high-quality synthesized medical images.
One way to achieve this is by incorporating more detailed
anatomy and physiology features using more e fficient and
effective attention mechanisms. Additionally, while much
of the current research in this area has focused on 2D med-
ical images and CT and MRI modalities, there is potential
to apply these techniques to other types of medical images,
including 3D and microscopy images.
7. Medical Image Detection
Object detection remains one of the challenging problems in
computer vision, especially detection in the medical image do-
main has its own challenges. Current SOTA architectures that
work on 2D natural images use Vision Transformers. The Vi-
sion Transformers used in the detection task can be classified
into two Transformer backbones and detection Transformers.
In addition, the Transformer module can be used in a hybrid
manner. Detection Transformers generally represent an end-to-
end detection pipeline with an encoder-decoder structure, while
the Transformer backbone solely utilizes the Transformer en-
35Table 12: An overview of the reviewed Transformer-based medical image synthesizing approaches.
Method Concept(s) Modality Type Pre-trained Module: Type Dataset(s) Metrics Year
Pure
PTNet [274] Intra-Modality MRI 2D ✗ dHCP dataset [281]SSIM
PSNR2021
MMT [276]Intra-Modality
Inter-ModalityMRI 2D ✗1IXI [265]
2BraTS [206]SSIM
PSNR
LPIPS2022
Bottleneck
ResViT [275]Intra-Modality
Inter-ModalityCT
MRI2D ViT: Supervised1IXI [265]
2BraTS [200, 201, 202]
3Multi-modal pelvic MRI-CT [286]PSNR
SSIM2022
CyTran [277] Inter-Modality CT2D
3D✗ Coltea-Lung-CT-100W [277]MAE
RMSE
SSIM2022
Encoder
SLMT-Net [279] Inter-Modality MRI 2D ViT: Self-supervised BraTS [200, 201, 202]2ISLES [287]SSIM
PSNR
NMSE2022
Decoder
VTGAN [278] Inter-Modality Angiography 2D ✗ Fundus Fluorescein Angiography [288]Fr´echet inception distance
Kernel Inception distance2021
Table 13: A brief description of the reviewed Transformer-based medical image synthesizing models.
Method Contributions Highlights
Pure
PTNet [274] •Introduced the pure Transformer-based network with linear computational complexity for image-
synthesizing context•Practical inference time around 30 image /s
MMT [276] •Proposed a pure Transformer-based architecture that incorporates Swin Transformer blocks to perform
missing data imputation by leveraging the existing MRI contrasts.
•Conducted experiments on the IXI and BraTS datasets to perform qualitative and quantitative analysis
and confirm their model’s e fficiency.•Since the attention mechanism can be utilized to pinpoint influential features in the model’s reasoning
and decision-making, the attention scores of the Transformer decoder in MMT make it interpretable by
capturing information in di fferent contrasts that play an important role in generating the output sequence.
•The framework can be applied to a variety of medical analysis tasks, including image segmentation
and cross-modality synthesis.
Bottleneck
ResViT [275] •First conditional adversarial model for medical image-to-image translation with hybrid CNN-
Transformer generator
•Introduced a new module, ART block, for simultaneously capturing localization and contextual infor-
mation•Utilized weight sharing strategy among Transformers to hinder the computational overhead and lessen
the model complexity
•An end-to-end design for the synthesized model that generalizes through multiple settings of source-
target modalities, e.g., one-to-one and many-to-one tasks
CyTran [277] •Proposing a generative adversarial convolutional Transformer for two tasks of image translation and
image registration.
•Introducing a new dataset, named Coltea-Lung-CT-100W, comprised of 100 3D anonymized triphasic
lung CT scans of female patients.•The presented method can handle high-resolution images due to its hybrid structure
•Utilized style transfer techniques to improve alignment between contrast and non-contrast CT scans
Encoder
SLMT-Net [279] •Proposed a self-supervised method for cross-modality MR image synthesis to generate missing modal-
ities based on available modalities.
•Addresses the challenge of limited paired data by combining both paired and unpaired data.•Despite using only 70% paired data, MT-Net’s performance remains on par with comparable models.
•Yields Enhanced PSNR scores when utilized for MR image synthesis.
•The combination of edge-aware pre-training and multi-scale fine-tuning stages contributes to the suc-
cess of SLMT-Net.
Decoder
VTGAN [278] •Proposed a synthesis model for the task of fundus-to-angiogram that incorporates ViT architecture in
the decoder section of the system to concurrently classify retinal abnormalities and synthesize FA im-
ages.
•Prepared experimental data based on quantitative and qualitative metrics regarding the model’s gener-
alization ability under the influence of spatial and radial transformations.•Has the potential to be adopted as a tool for tracking disease progression.
•The system is designed to operate on non-invasive and low-cost fundus data to generate FA images.
coder for feature refinement. In order to increase detection per-
formance, object detectors combine variants of vision Trans-
formers with classical convolutional neural networks (CNNs).
Quite recently, Carion et al. [187] introduced the concept
of DETR, which forms a foundation for Detection Transform-
ers. DETR uses a ResNet backbone to create a lower-resolution
representation of the input images. Even though this approach
achieves very good 2D detection results, comparable to the
R-CNN backbone, high computational complexity is a down-
side of this method. The deformable DETR [16] approach
has improved DETR’s detection performance overcoming the
problem of high computational complexity. Many recent ap-
proaches have tried to improve DETR’s detection concept over
time. E fficient DETR [297] eliminated DETR’s requirement for
iterative refinement. Conditional DETR [298] introduced the
concept of a conditional cross-attention module. DN-DETR
[299] introduced a denoising strategy, and DINO [300] im-proved many aspects, such as denoising training, etc. Recently,
some studies performed experiments on 2D medical data such
as [301], [293], etc. However, only very few attempts tried to
adapt it to 3D. Spine-Transformer was proposed by Tao et al.
[295] for sphere-based vertebrae detection. Another approach
in 3D detection was proposed by Ma et al. [289], which in-
troduced a novel Transformer that combines convolutional lay-
ers and Transformer encoders for automatically detecting coro-
nary artery stenosis in Coronary CT angiography (CCTA). An
approach to better extract complex tooth decay features was
proposed by [290]. For end-to-end polyp detection, Shen et
al. [293] proposed an approach that was based on the DETR
model. Kong et al. have proposed the approach CT-CAD [294],
context-aware Transformers for end-to-end chest abnormality
detection. As illustrated in Figure 33, we have broadly clas-
sified these methods based on the role ViT plays in the detec-
tion’s commonly accepted pipeline. Table 14 indicates details
36 Medical Image 
 Detection
 Backbone
 1. TR-Net
 2. RDFNet
 3. CellCentroidFormer Neck
 5. COTR
 6. CT-CAD
 7. Spine-Transformer Head
 8. Focused Decoder
 4. Joint-2D3D-SSLFigure 33: An overview of Transformers in medical image detection. Methods are classified into the backbone, neck, and head according to the positions of the
Transformers in their architecture. The prefix numbers in the paper’s name in ascending order denote the reference for each study as follows: 1. [289], 2. [290],
3. [291], 4. [292], 5. [293], 6. [294], 7. [295], 8. [296].
on modalities, organs, datasets, metrics, etc. The highlights of
different approaches are summarized in Table 15. Some of the
aforementioned detection papers in the medical image domain
are reviewed in detail in this section.
7.1. Backbone
This section explains Transformer networks using only the
Transformer encoder layers for object detection. The work pro-
posed by Ma et al. [289] uses a Transformer network (TR-Net)
for identifying stenosis. A leading threat to the lives of cardio-
vascular disease patients globally is Coronary Artery Disease
(CAD). Hence, the automatic detection of CAD is quite signif-
icant and is considered a challenging task in clinical medicine.
The complexity of coronary artery plaques, which result in
CAD, makes the detection of coronary artery stenosis in Coro-
nary CT angiography (CCTA) challenging.
The architecture introduces a Transformer and combines the
feature extraction capability of convolutional layers and Trans-
former encoders. TR-Net can easily analyze the semantic in-
formation of the sequences and can generate the relationship
between image information in each position of a multiplayer
reformatted (MPR) image. This model can e ffectively detect
stenosis based on both local and global features. The CNN eas-
ily extracts the local semantic information from images, and
the Transformer captures global semantic details more easily.
A 3D-CNN is employed to capture the local semantic features
from each position in an MPR image. After this step, the Trans-
former encoders are mainly used to analyze feature sequences.
The main advantage here is that this helps in mining the depen-
dency of local stenosis on each position of the coronary artery.
The architecture of the TR-Net is given in Figure 34. One part
of the figure indicates the 3D-CNN. This module extracts the
local features. The other part indicates the Transformer encoder
structure. This module associates the local feature maps of each
position and helps in analyzing the dependency between di ffer-
ent positions, which in turn is helpful for classifying the signif-
icant stenosis at each position. The CNN part mainly has twomain advantages: it prevents the overfitting of semantic infor-
mation and improves the model’s e fficiency. The input to the
network architecture is the coronary artery MPR image.
Figure 34: Proposed architecture of TR-Net model [289].
The 3D-CNN module has four sequentially connected sub-
structures, which consist of a convolutional kernel of size
3×3×3, a non-linear ReLU layer and a 2 ×2×2 max-pooling
layer. The number of filters is 16 in the first part, and in sub-
sequent parts, the number of filters is double the number in the
previous part. Since Transformers have 1D vector sequences
as input, the feature maps are flattened. The Transformer in
the proposed architecture consists of 12 Transformer encoders.
Each Transformer encoder mainly consists of two sub-blocks -
multi-head self-attention (MSA) and the feed-forward network
(FFN), which are connected sequentially. Layer normal (LN)
and residual connections are employed before and after two
sub-blocks. In order to ensure the consistency of the encoders,
the size of the input is the same as the size of the output. The
output of the previous encoder is given as input to the next en-
coder. In the final layer, the embeddings are fed into softmax
classifiers to detect significant stenosis.
RDFNet approach proposed by Jiang et al. [290] basically
incorporates the Transformer mechanism in order to better ex-
tract the complex tooth decay features. The incorporation of the
37Transformer has improved the detection accuracy. The main
three modules of the network are the backbone, neck, and pre-
diction modules. The backbone module is mainly used to ex-
tract the features from caries images. In the backbone module,
the focus operation is a slicing operation that could easily re-
place the convolution operation and reduce the loss of feature
information. The C3Modified layer is a convolution module ac-
tivated by the FReLU function, which extracts complex visual-
spatial information of the caries images. SPP [302] module
has a spatial pyramid structure that could expand the percep-
tual field, fusing the local and global features and enhancing
the feature maps. After the SPP structure, RDFNet appends an
improved Transformer-encoder module to improve the feature
extraction capability. The main functionality of the neck mod-
ule is to fuse the feature maps of di fferent sizes and extract high-
level semantic structures. This module mainly uses the struc-
ture of the feature pyramid network (FPN) proposed in [303],
and path aggregation network (PAN) proposed in [304]. The
FPN approach is employed in a top-down fashion, and PAN is
performed in a bottom-up fashion to generate the feature pyra-
mids. In order to prevent information loss, feature fusion is
performed using both bottom-up and top-down approaches. An
improved C3Modified convolutional module is adopted into the
neck module to better extract the semantic features of caries
images. The high-level features generated by the neck mod-
ule are used by the prediction module, which in turn is used to
classify and regress the location and class of the objects. To
overcome the problems of the single-stage detection method,
which has quite a low detection accuracy, it mainly has three
detection heads for detecting large, medium, and small objects.
As Transformers have proved to have strong feature extraction
capability, in order to extract complex features, they utilized the
Transformer model. To better extract the features, three Trans-
former encoders were stacked together. To simplify the model,
the authors removed the original normalization layer from the
Transformer encoder. In order to extract the deep features, the
feature map was fed into this structure. For each head, the at-
tention values were calculated independently and later concate-
nated.
Wagner et al. [291] proposed a novel hybrid cell detection
approach (CellCentroidFormer) in microscopic images that
combines the advantages of vision Transformers (ViTs) and
convolutional neural networks (CNNs). Authors show that the
combined use of convolutional and Transformer layers is ad-
vantageous as the convolutional layers can focus on the local in-
formation (cell centroids), and the Transformer layers can focus
on the global information ( overall shapes of a cell). The pro-
posed centroid-based approach represents the cells as ellipses
and is trainable in an end-to-end fashion. Four di fferent 2D mi-
croscopic datasets were used for experimental evaluations, and
the results outperformed the fully convolutional architecture-
based methods. Figure 35 shows the architecture. The encoder
is then folded into a 3D tensor, which is afterward concatenated
with the input tensor. The MobileViT block is a lightweight al-
ternative to the actual encoder-decoder approach using a Trans-
former [14]. Due to the multi-head self-attention layers, the
MobileViT block causes a much higher computational com-
Figure 35: Proposed architecture of CellCentroidFormer model [291].
plexity than convolutional layers. To not increase the computa-
tional complexity excessively, the MobileViT blocks are com-
bined in the neck part of the proposed model. Layer normal-
ization is added for regularization and to allow higher learning
rates. The backbone module of the proposed model is the E ffi-
cientNetV2S [305] CNN model. This block mainly consists of
six high-level blocks, out of which five blocks are used to ex-
tract image features. To use the advantage of transfer learning,
the backbone module is initialized with weights learned from
training on ImageNet. This, in turn, reduces the amount of re-
quired training data. The E fficientNetV2S [305] CNN models
are generally optimized for a fixed input size. Therefore the in-
put images need to be resized to this input size. The cells are
represented mainly by the centroid, width, and height param-
eters. Mainly, two fully convolutional heads are used to pre-
dict these cell parameters in the paper. These heads contain 2D
convolution, batch normalization, and bilinear upsampling lay-
ers. More MobileViT blocks are not used as it will increase the
computational complexity. Later convolutional layers have a
bigger receptive field which helps in capturing the global infor-
mation [306] e ffectively. The first convolutional head predicts a
heatmap for detecting the cell centroids, and the second head is
used for predicting the cell dimensions. The output dimensions
of this model are 384 ×384. The authors use one decoder of
the Dual U-Net to predict the centroid heatmap, and the sec-
ond branch predicts the dimensions of the detected cells. The
shapes of the cells are focused on by the Transformer layers in
the network.
Nguyen et al. [292] present a novel self-supervised learn-
ing (SSL) framework, Joint-2D3D-SSL , designed to address
the scarcity of labeled training data for joint learning on 2D
and 3D medical data modalities. The framework constructs an
SSL task based on a 2D contrastive clustering problem for dis-
tinct classes using 2D images or 2D slices extracted from 3D
volumes. In particular, the framework leverages 3D volumes
by computing feature embeddings for each slice and then con-
structing a comprehensive feature representation through a de-
formable self-attention mechanism. This approach enables the
capture of correlations among local slices, resulting in a holistic
understanding of the data. The global embedded features de-
rived from this transformer are subsequently utilized to define
38an agreement clustering for 3D volumes and a masked encoding
feature prediction. By employing this methodology, the frame-
work enables the learning of feature extractors at both the local
and global levels, ensuring consistency and enhancing perfor-
mance in downstream tasks. Experimental results across vari-
ous downstream tasks, including abnormal chest X-ray detec-
tion, lung nodule classification, 3D brain segmentation, and 3D
heart structures segmentation, illustrate the e fficacy of this joint
2D and 3D SSL approach, surpassing plain 2D and 3D SSL
approaches, as well as improving upon SOTA baselines. Ad-
ditionally, this method overcomes limitations associated with
fine-tuning pre-trained models with di fferent dimensionality,
providing versatile pre-trained weights suitable for both 2D and
3D applications.
7.2. Head
Detection Transformers based on Transformer encoder-
decoder architecture require a large amount of training data to
deliver the highest performance. However, this is not feasible in
the medical domain, where access to labeled data is limited. To
address this problem, for the detection of 3D anatomical struc-
tures from the human body, Wittmann et al. [296] proposed a
detection Transformer network with a focused decoder . This
network considers the relative position of the anatomical struc-
tures and thus requires less training data. The focused decoder
uses an anatomical region atlas to deploy query anchors to fo-
cus on the relevant anatomical structures. The proposed net-
work omits the Transformer encoder network and consists of
only Transformer decoder blocks. The authors show that in
3D datasets, avoiding the encoder can reduce the complexity of
modeling relations with a self-attention module.
The model architecture contains a backbone network for fea-
ture extraction, a focus decoder network for providing well-
defined detection results, a classification network to predict the
classes, and a bounding box regression network to output the
best possible bounding box. The feature extraction backbone
network is a feature pyramid network (FPN) inspired by the
RetinaNet [307]. Features from the second layer (P2) are flat-
tened before being given as input to the focus decoder. A spe-
cific anatomical region atlas [308] containing regions of interest
(RoI) is determined for each dataset. Then to each RoI, uni-
formly spaced query anchors are placed, and a dedicated object
query is assigned to each. Such an object query will restrict the
focus decoder network to predict solely within their respective
RoI.
The focused decoder network contains a self-attention mod-
ule, a focused cross-attention module, and a feedforward net-
work (FFN). The self-attention module encodes strong posi-
tional inter-dependencies among object queries. The focused
cross-attention module matches the input sequence to object
queries to regulate the individual feature map for prediction via
attention. The FFN network then enables richer feature repre-
sentation. Also, residual skip connections and normalizations
are used to increase gradient flow. The classification network
consists of a single fully-connected layer, and the bounding box
regression network consists of three layers. The bounding box
predictions are combined with query anchors to get the bound-
ing box together with class-specific confidence scores. The net-work is trained to predict 27 candidates predictions per class.
Dynamic labeling with the help of generalized intersection over
union (GIoU) is created during training to get 27 predictions.
During inference, the prediction with the highest confidence
score indicates the best candidate. The model is trained end-
to-end with the above GIoU loss, binary cross-entropy loss for
the classification network, and L1 loss for the bounding box
predictions.
7.3. Neck
Detection methods using region-based approaches need to
generate anchor boxes to encode their prior knowledge and use
a non-maximum suppression to filter the resulting bounding
boxes after prediction. These pre-and post-processing steps re-
markably reduce the detection performance. To bypass these
surrogate tasks, Carion et al. [187] proposed Detection Trans-
former (DETR), which views the object detection task as a
direct set prediction problem using an encoder-decoder archi-
tecture using Transformers. The self-attention mechanism of
the Transformers, which explicitly models all pairwise interac-
tions between elements in a sequence, helps to predict the set
of detections with absolute prediction boxes directly from the
image rather than using an anchor. For the end-to-end detec-
tion of polyp lesions, Shen et al. [293] proposed a convolution
in Transformer (COTR) network based on the DETR model.
COTR consists of 4 main layers: 1) a CNN backbone network
used for extracting features, 2) Transformer encoder layers em-
bedded with convolutional layers used for feature encoding and
reconstruction, 3) Transformer decoder layers used for object
querying, and 4) a feed-forward network used for detecting pre-
diction. Embedding convolutional layers into the Transformer
encoder layer leads to convergence acceleration compared to
the slow convergence of the DETR model.
The CNN backbone uses a pre-trained model with ResNet18
[64] architecture for feature extraction. This layer converts in-
put medical images to a high-level feature map. The authors
then use a 1×1 convolution to reduce the channel dimensions.
In the Transformer encoder layers, they used six convolution-
in-Transformer encoders to collapse this spatial structure into a
sequence. Then they use a convolution layer to reconstruct the
sequential layer back to the spatial one. In the encoder layer,
each Transformer has a standard architecture with a multi-head
self-attention module and a feed-forward network. To the in-
put of each attention layer, a positional embedding [14] is also
introduced. In the Transformer decoder layers, they used six
decoders which follow the standard architecture of the Trans-
former except that it also decodes object queries in parallel.
Each object query will correspond to a particular object in the
image. The decoders take these object queries with position
embeddings as well as output embeddings from the encoder
network and convert them into decoded embeddings. Then
they used a feed-forward network with two fully connected lay-
ers for converting these decoded embeddings into object pre-
dictions. The first fully connected layer is a box regression
layer to predict object location, and the second one is a box-
classification layer to predict object scores. Therefore, the ob-
ject queries are independently decoded into box coordinates
39Table 14: An overview of the reviewed Transformer-based medical image detection approaches.
Method Modality Organ Type Pre-trained Module: Type Datasets Metrics Year
Backbone
TR-Net [289] MPR Heart (Coronary Artery) 3D Supervised Private datasetAccuracy,
Sensitivity,
Specificity,
PPV , NPV ,
F1-score2021
RDFNet [290] Dental Caries Teeth 2D Supervised Private datasetPrecision,
Recall,
mAP@0:52021
CellCentroidFormer [291] Microscopy Cells 2D Supervised1Fluo-N2DL-HeLa (HeLa) [309] ,
2Fluo-N2DH-SIM +(SIM +) [309],
3Fluo-N2DH-GOWT1 (GOWT1) [309] ,
4PhC-C2DH-U373 (U373) [309].Mean-IoU,
SSIM2022
Joint-2D3D-SSL [292] X-ray Chest2D
3DSelf-supervised VinDr-CXR [110] mAP@0.5 2022
Head
Focused decoder [296] CT Multi-organ 3D Semi-Supervised1VISCERAL anatomy benchmark [310],
2AMOS22 challenge [311].mAP,
AP50,
AP752023
Neck
COTR [187] Colonoscopy Colon 2D Supervised1CVC-ClinicDB [312],
2ETIS-LARIB [313],
3CVC-ColonDB [314].Precision,
Sensitivity,
F1-score2021
CT-CAD [294] X-ray Chest 2D Supervised1Vinbig Chest X-Ray dataset [315]
2ChestXDet-10 dataset [316]AP50 2021
Spine-Transformer [295] CT Vertebra 3D Supervised1VerSe 2019 [317],
2MICCAI-CSI 2014 [318],
3Private datasetId-Rate,
L-Error2021
and classes by the feed-forward network, which results in fi-
nal predictions, including object and no object (background)
predictions. This model transforms the object detection prob-
lem into a direct set prediction problem by training end-to-end
by calculating bipartite matching loss (Hungarian algorithm)
between predictions and ground truth for each query. If the
number of queries exceeds the number of objects in the image,
the remaining boxes are annotated as no object class. Thus,
the model is trained to predict output for each query as an ob-
ject or no object detection. For the class prediction, they used
negative log-likelihood loss, and for the bounding box local-
ization, they used an L1 loss with generalized intersection over
union (GIOU) [323] loss. The experiments demonstrated that
the proposed model achieved comparable performance against
state-of-the-art methods.
Many deep learning detection methods lack using context-
relevant information for improved accuracy, and they also gen-
erally su ffer from slower convergence issues and high compu-
tational costs. The proposed CT-CAD [294], context-aware
Transformers for end-to-end chest abnormality detection, ad-
dress these problems. The model consists of two main modules:
1) a context-aware feature extractor module for enhancing the
features, and 2) a deformable Transformer detector module for
detection prediction and to accelerate the convergence speed.
The context-aware feature extractor network uses a ResNet50
backbone, dilated context encoding (DCE) blocks, and posi-
tional encoding structure. The deformable Transformer detec-
tor contains a Transformer encoder-decoder architecture and
a feed-forward network. The proposed design of the context-
aware feature extractor is inspired by the feature fusion scheme
from DetectoRS [324] which is based on the Feature Pyramid
Networks (FPN) [325]. The feature fusion scheme iterativelyenhances the features of the FPN to powerful feature represen-
tations. Likewise, the DCE blocks enhance the features ex-
tracted from the ResNet50 backbone by expanding the recep-
tive fields to fuse multiscale context information using dilated
convolution filters of di fferent sizes. This powerful feature map
benefits in detecting objects across various scales. Inspired by
YOLOF [326] the DCE block uses dilated convolution and skip
connections to achieve a larger receptive field and acquire more
local context information. Finally, all the features from di ffer-
ent DCE blocks computed at di fferent scales are summed up to
get the feature map for the output.
The proposed design of the deformable Transformer detector
contains single-scale and multi-head attention properties. The
deformable attention block attends to a small set of key sam-
pling points, thus allowing the Transformer to focus on the fea-
ture space and accelerate the convergence. The authors used
six encoder and decoder layers with positional encoding to ob-
tain the decoder outputs. The outputs from the decoder are the
number of abnormalities detected and the dimension of the de-
coder layers. Finally, a feed-forward network is used to out-
put the category classification and location regression results.
The model is trained end-to-end with a combination of bound-
ing box loss and classification (cross-entropy) loss. The authors
adopted GIoU [327] to balance the loss between large and small
object bounding boxes.
The attention module in the detection Transformers com-
putes similarity scores between elements of each input data
to identify complex dependencies within these data. Calcu-
lating similarities of all possible positional pairs in the in-
put data scales quadratically with the number of positions and
thus becomes computationally very expensive. For this reason,
the Transformer-based object detection model from 3D images
40Table 15: A brief description of the reviewed Transformer-based medical image detection models. The unreported number of parameters indicates that the value
was not mentioned in the paper.
Method # Params Contributions Highlights
Backbone
TR-Net [289] - •This work is the first attempt to detect coronary artery stenosis more accu-
rately by employing Transformers.
•To detect significant stenosis, local and global features are e ffectively inte-
grated into this approach, which has resulted in more accurate results.•While detecting significant stenosis, the TR-Net architecture is capa-
ble of combining the information of local areas near stenoses and the
global information of coronary artery branches.
•Compared to state-of-the-art methods, the TR-Net model has better
results on multiple indicators.
•The shallow CNN layer prevents the overfitting of semantic informa-
tion and improves the overall e fficiency.
•The gain in performance comes with a trade-o ffin the number of pa-
rameters, which a ffects the computational complexity.
RDFNet [290] - •An image dataset of caries is created, which is annotated by professional
dentists.
•For better extraction of the complex features of dental caries, the Transformer
mechanism is incorporated.
•In order to increase the inference speed significantly, the FReLU activation
function is adopted.•Compared with existing approaches, the accuracy and speed of caries
detection are better.
•Method is applicable to portable devices.
•The method does not work really well when the illumination of the
oral image is insu fficient.
•Even though detection accuracy and speed are improved compared to
the original approach, the detection speed is not the fastest.
CellCentroidFormer [291] 11.5M •A novel deep learning approach that combines the self-attention of Trans-
formers and the convolution operation of convolutional neural networks is pro-
posed.
•A centroid-based cell detection method, denoting the cells as ellipses is pro-
posed.•Pseudocoloring in combination with pre-trained backbones shows im-
proved cell detection performance.
•The model outperforms other state-of-the-art fully convolutional one-
stage detectors on four microscopy datasets, despite having a lower
number of parameters.
•Larger output strides worsen the performance.
Joint-2D3D-SSL [292] 31.16M •Introduces a self-supervised learning framework capable of leveraging both
2D and 3D data for downstream applications.
•Proposes a deformable self-Attention mechanism that captures flexible corre-
lations between 2D slices, leading to powerful global feature representations.
•Extends the SSL tasks by proposing a novel 3D agreement clustering and
masking embedding prediction•Overcomes limitations in fine-tuning pre-trained models with di ffer-
ent dimensionality.
•Produces versatile pre-trained weights for both 2D and 3D applica-
tions.
Head
Focused Decoder [296]VISCERAL Dataset - 41.8M
AMOS22 Dataset - 42.6M•First detection Transformer model for 3D anatomical structure detection.
•Introduced a focused decoder to focus the predictions on RoI.•Better results compared to existing detection models using a Trans-
former network like DETR [187] and deformable DETR [16].
•Comparable results to the RetinaNet [307].
•Varying anatomical fields of view (FoVs) can a ffect the robustness of
the model.
Neck
COTR [162] - •Proposed a convolution layer embedded into the Transformer encoder for
better feature reconstruction and faster convergence compared to DETR.•COTR has comparable results with state-of-the-art methods like Mask
R-CNN [319] and MDeNet-plus [320].
•This approach produces low confidence for a particular type of lesion.
CT-CAD [294] - •Proposed a context-aware feature extractor, which enhances the receptive
fields to encode multi-scale context-relevant information.
•Proposed a deformable Transformer detector that attends to a small set of key
sampling locations and then the Transformers can focus to feature subspace and
accelerate the convergence speed.•CT-CAD outperforms the existing methods in Cascade R-CNN [321],
YoLo [322], and DETR [16].
•CT-CAD is capable to detect hard cases, such as nodules that are ig-
nored by Faster R-CNN.
•Compared to the ChestXDet-10 dataset, this model has a lower perfor-
mance on the Vinbig Chest X-Ray dataset which has higher categories
of abnormalities with more complex patterns.
Spine-Transformers [295] - •Proposed a 3D object detection model based on the Transformer’s architec-
ture.
•Proposed a one-to-one set global loss that enforces unique prediction and
preserves the sequential order of vertebrae.
•Proposed a Sphere-based bounding box to enforce rotational invariance.•Obtained better results for all datasets compared to state-of-the-art
methods.
•The model has a higher Id-Rate on both the datasets, but a higher L-
Error compared to the benchmark by [317].
has never been applied. Tao et al. [295] proposed a novel
Transformer-based 3D object detection model as a one-to-one
set prediction problem for the automatic detection of verte-
brae in arbitrary Field-Of-View (FOV) scans, called the Spine-
Transformers . Here the authors used a one-to-one set-based
global loss that compels a unique prediction for preserving the
sequential order of di fferent levels of vertebrae and eliminated
bipartite matching between ground truth and prediction. The
main modules of the Spine-Transformer are (1) a backbone
network to extract features, (2) a light-weighted Transformer
encoder-decoder network using positional embeddings and a
skip connection, and (3) two feed-forward networks for detec-
tion prediction. The authors used a ResNet50 [64] architec-
ture without the last SoftMax layer as the backbone network to
extract high-level features. These features are passed through
a 1×1×1 convolutional layer to reduce the channel dimen-
sions and then flattened to get a feature sequence to feed as the
input for the Transformer network. The light-weighted Trans-former encoder-decoder network contains only a two-layer en-
coder and two-layer decoder to balance between feature res-
olution and memory constraint. In both the encoder and de-
coder layers of the network, learnable positional embeddings
are used. The authors found that using a skip connection across
the Transformer encoder-decoder network will help in the prop-
agation of context and gradient information during training and
thus improves performance. The two feed-forward networks
are then used to predict the existence of the objects and regress
their coordinates. The authors also proposed a sphere-based
bounding box detector to replace the rectangular-based bound-
ing box to introduce rotational invariance called InSphere de-
tector. The Spine-Transformer is trained end-to-end with fixed-
size patch images to predict all the vertebrae objects in parallel
by forcing one-to-one matching. Binary cross-entropy loss is
used as classification loss, and to enforce the order of the pre-
dicted vertebrae objects, an edge loss is introduced, which is
an L1 distance loss introduced between the centers of the top
41and bottom neighborhood vertebrae objects. For better local-
ization accuracy of the bounding sphere detection, the authors
used generalized inception-over-union (GIoU) [323] loss. The
results of this model showed superior results to all the state-
of-the-art methods. The authors also claim that by using a 3D
CNN-based landmark regression [328], the localization accu-
racy can be further improved.
7.4. Discussion and Conclusion
In this chapter, several well-known Transformer architec-
tures are analyzed to address the automatic detection chal-
lenge. Based on the Transformer model contribution to the
network structure, we grouped the set of literature work into
the backbone, neck, or head strategies and for each category,
we provided sample works. We also described the details re-
garding self-supervised learning in [292]. In this respect, the
core idea behind each network design along with the pros
and cons of the strategies are highlighted in the summary ta-
bles. Vision Transformers have been shown to make more
accurate diagnoses compared to traditional methods of ana-
lyzing medical images. These deep learning models can be
trained on large datasets, such as ImageNet, and fine-tuned
on medical image datasets to improve their performance in
detecting abnormalities in X-rays, CT scans, and MRIs. By
incorporating information from multiple modalities, Trans-
formers can further enhance their ability to identify and de-
tect rare or subtle abnormalities in medical images. Many
medical images are often taken over time, and incorporating
temporal information into the model can improve its perfor-
mance. For example, the model can be designed to take into
account the temporal evolution of diseases or conditions.
Overall, Transformers have demonstrated their capabilities
to significantly improve the accuracy and e fficiency of med-
ical image analysis, leading to advances in healthcare.
8. Medical Image Registration
Medical image registration is the task of transforming a set
of two or more images of an organ or a biological process taken
with di fferent poses, time stamps, or modalities (e.g., CT and
MRI) into a geometrically aligned and spatially corresponding
image that can be utilized for medical analysis. The transfor-
mation can be discovered by solving an optimization problem
that maximizes the similarity between the images to be regis-
tered [329]. A pair-wise registration of two MRI brain scans is
shown in Figure 36 for illustration.
Despite remarkable advancements in the quality of medical
imaging techniques that aid professionals in better visualiza-
tion and analysis of image data, a prominent challenge prevails
in developing a system capable of e ffective integration of visual
data that captures useful information from original images with
high precision. Most registration procedures take into account
the whole image as input by utilizing global information for
spatial transformation, which leads to ine fficient and slow inte-
gration of data. Furthermore, the collection process of medical
images for training is slow and toilsome, performance degrades
Moving Image (M)
Fixed Image (F)Spatial TransformationRegistered ImageFigure 36: An example of pair-wise medical image registration. The goal of
image registration is to geometrically align the moving image with the target or
fixed image by performing the spatial transformation.
due to the presence of outliers, and local maxima entail neg-
ative e ffects on performance during optimization [330, 331].
The emergence of deep learning methods alleviated these prob-
lems by automatic extraction of features utilizing convolutional
neural networks (CNN), optimizing a global function, and im-
proving registration accuracy. For instance, Balakrishnan et al.
[332] utilized a CNN to achieve unsupervised deformable reg-
istration by treating it as a parametric function to be optimized
during training. Furthermore, Chen et al. [333] presented an
unsupervised CNN-based registration algorithm to produce an-
thropomorphic phantoms. However, there are still limitations
in capturing long-range spatial correspondence in CNN-based
frameworks [334, 39].
Fueled by the strong ability of Transformers to model long-
range dependencies and detect global information [335, 21,
336], they have gained the attention of researchers in the med-
ical image registration domain in recent years. In this section,
we review Transformer-based methods in medical image reg-
istration that ameliorate the aforementioned shortcomings of
previous systems by utilizing the self-attention mechanism. We
have organized the relevant approaches based on their type of
registration:
(a) Deformable registration, which employs an optimization
algorithm to tune the transformation model, is a way that
maximizes the similarity measure function for the images
of interest [337];
(b) Rigid registration, which achieves correspondence by
maintaining the relative distance between each pair of
points between the patient’s anatomy images [337].
(c) A ffine registration, which contains the same operations as
rigid registration plus non-isometric scaling.
8.1. Deformable Registration
Most existing Transformer-based algorithms focus on de-
formable transformation to perform medical image registra-
tion. Vit-V-Net [338] is the earliest work that incorporates
42 Medical Image 
 Registration
 Rigid
 1. SVoRT Deformable
 2. ViT-V-Net
 3. TransMorph
 4. DTN
 5. XMorpher Affine
 7. C2FViT
 6. SPANFigure 37: Taxonomy of Transformer-based image registration based on their
transformation type. We use the prefix numbers in the figure in ascending order
and reference the corresponding paper as follows: 1. [336], 2. [338], 3. [339],
4. [340], 5. [341], 6. [342], 7. [343].
Transformers to perform medical image registration in a self-
supervised fashion. It is inspired by the integration of vision
Transformer-based segmentation methods with convolutional
neural networks to enhance the localization information recov-
ered from the images. Unlike previous research that employed
2D images for spatial correspondence, Vit-V-net stepped to-
wards utilizing ViT [15] as the first study for volumetric med-
ical image registration (i.e., 3D image registration). As illus-
trated in Figure 38, the images are first encoded into high-level
feature representations by implementing multiple convolution
blocks; then, these features get split into Ppatches in the ViT
block. Next, the patches are mapped to a D-dimensional em-
bedding space to provide patch embeddings, which are then
integrated with learnable positional encodings to retain posi-
tional information. Next, these patches are passed into the
encoder block of the Transformer, followed by multiple skip
connections to retain localization information, and then de-
coded employing a V-Net style decoder [344]. Finally, a spa-
tial Transformer [345] warps the moving image by utilizing
the final output of the network. TransMorph [339] extended
ViT-V-Net and proposed a hybrid Transformer ConvNet frame-
work that utilizes the Swin Transformer [58] as the encoder
and a ConvNet as the decoder to provide a dense displacement
field. Like ViT-V-Net, it employed long skip connections to
retain the flow of localization information that may enhance
registration accuracy. The output of the network, which is a
nonlinear warping function, gets applied to the moving image
with the deformation field utilizing the spatial transformation
function proposed in [345]. An a ffine transformation Trans-
former network is incorporated to align the moving image with
the fixed image before feeding it to the deformable registra-
tion network. This work also proposed two variants of Trans-
Morph: di ffeomorphic TransMorph (TransMorph-di ff) to fa-
cilitate topology-preserving deformations and Bayesian Trans-
Morph (TransMorph-Bayes) to promote a well-calibrated reg-
istration uncertainty estimate.
Likewise, Zhang et al. [340] introduced the dual Transformer
network ( DTN ) framework to perform di ffeomorphic registra-
tion. It is composed of a CNN-based 3D U-Net encoder [328]
for the embedding of separate and concatenated volumetric im-
ages and a dual Transformer to capture the cross-volume depen-
Figure 38: Overview of ViT-V-Net. Multiple convolution blocks encode images
into high-level features, which the Vit block splits into patches. These patches
are then mapped to D-dimensional patch embeddings that get integrated with
learnable positional encodings to retain positional information. Next, these
patches are passed into the Transformer encoder block, followed by multiple
skip connections to retain localization information, and decoded using a V-Net
style decoder. Using the network’s final output, a spatial Transformer warps the
moving image. Figure taken from [338].
dencies. One of the Transformers is responsible for modeling
the inter- and intra-image dependencies, and the other one han-
dles the modeling of the global dependencies by employing the
self-attention mechanism. The concatenation of the generated
features from these Transformers results in enhanced feature
embeddings, which are utilized by the CNN-based decoder to
provide a di ffeomorphic deformation field. The evaluation of
the framework was conducted on the brain MRI scans of the
OASIS dataset [346], which substantiates their improvements
in diffeomorphic registration compared to the existing deep-
learning-based approaches.
Furthermore, XMorpher [341] put emphasis on the signifi-
cance of backbone architectures in feature extraction and match
of pair-wise images, and proposed a novel full Transformer net-
work as the backbone, which consists of two parallel U-Net
structures [28] as the sub-networks with their convolutions re-
placed by the introduced Cross Attention Transformer for fea-
ture extraction of moving and fixed images, and cross-attention-
based fusion modules that utilize these features for generating
the feature representation of moving-fixed correspondence and
fine-grained multi-level semantic information that contributes
to a fine registration.
The latest developments in self-supervised representation
learning have primarily aimed at eliminating inductive biases
in training processes. Nonetheless, these biases can still serve a
purpose in scenarios with scarce data or when they o ffer further
understanding of the underlying data distribution. To mitigate
this problem, [342] introduces spatial prior attention ( SPAN ),
a framework that leverages the consistent spatial and semantic
structure found in unlabeled image datasets to guide the atten-
tion mechanism of vision Transformers. SPAN integrates do-
main expertise to enhance the e ffectiveness and interpretability
of self-supervised pretraining specifically for medical images
using image registration and alignment methodologies. Specif-
ically, they exploit image registration to align the attention
maps with an inductive bias corresponding to a salient region
so only a single representative sample is required. When utiliz-
ing deformable image registration templates, SPAN achieves
the highest mAUC score, followed by the predicted global tem-
plates and the triangular spatial heuristic resulting in improved
performance in downstream tasks.
43Table 16: An overview of the reviewed Transformer-based medical image registration approaches.
Method Modality Organ Type Datasets Metrics Year
Deformable
ViT-V-Net [338] MRI Brain 3D Private Dataset Dice 2021
TransMorph [339]MRI
CTBrain
Chest-Abdomen-Pelvis region3D1IXI [265]
2T1-weighted brain MRI scans from Johns Hopkins University
3Chest-Abdomen-Pelvis CT [347]Dice
% of|JΦ|≤0
SSIM2022
DTN [340] MRI Brain 3D OASIS [346]Dice
|JΦ|≤02021
XMorpher [341]CT
MRIHeart 3D1MM-WHS 2017 [348]
2ASOCA [349]Dice
% of|JΦ|≤02022
SPAN [342]CT
MRILung 3DCheXpert [75]
JSRT [350]mAUC 2022
Affine
C2FViT [343] MRI Brain 3D1OASIS [346]
2LPBA [351]Dice
Hausdor ffdistance2022
Rigid
SV oRT [336] MRI Brain 3D FeTA [352]PSNR
SSIM2022
Table 17: A brief description of the reviewed Transformer-based medical image registration techniques.
Method Contributions Highlights
Deformable
ViT-V-Net [338] •Contributed to the medical image registration domain as the first work to exploit ViTs to develop a volu-
metric (3D) registration.
•Integrated Transformers with CNNs to build a hybrid architecture for self-supervised brain MRI registra-
tion•Employed a hybrid architecture to incorporate long-range and local information in the registration process.
•Attempted to preserve the localization data with the help of long skip connections between the encoder and decoder stages.
TransMorph [339] •Proposed a Transformer-based unsupervised registration approach for a ffine and deformable objectives.
•Conducted experiments on two brain MRI datasets and in a phantom-to-CT registration task to demon-
strate their superior performance compared to traditional approaches.•They additionally proposed two distinguishable versions of their model: a di ffeomorphic variant to facilitate the topology-preserving defor-
mations and a Bayesian variant to promote a well-calibrated registration uncertainty estimate.
•Studied the e ffect of receptive fields by comparing TransMorph with CNNs and addressed that while the receptive field of ConvNets only
increases with the layer depth, their presented model takes into account the whole image at each due to the self-attention mechanism.
DTN [340] •Proposed a dual Transformer architecture to capture semantic correspondence of anatomical structures.
•The suggested DTN demonstrated remarkable results in di ffeomorphic registration and atlas-based seg-
mentation of multi-class anatomical structures.•The Dual Transformer is capable of reducing the negative Jacobian determinant while preserving the atlas-based registration quality.
•The qualitative and quantitative analysis of their method on the OASIS dataset indicates that di ffeomorphic registration fields are e ffective.
XMorpher [341] •Devised a deformable registration system consisting of dual parallel feature extraction networks which
facilitate the association of representative features between moving and fixed images.
•Proposed cross-attention Transformer that establishes spatial correspondences through computation of
bilateral information in the attention mechanism.•Promotes visual superiority by presenting fine-grained visual results in terms of boundary smoothness, adjacent regions’ resolution quality,
and deformation grid polishness.
•Demonstrated the model’s great diagnostic potential by conducting experiments with di fferent training regimes.
SPAN [342] •Developed a self-supervised transformer-based registration method that leverages consistent spatial and
semantic structure in unlabeled image datasets to guide the attention mechanism of vision Transformers.•Integrates domain expertise and uses image registration and alignment methodologies to enhance the e ffectiveness and interpretability of
self-supervised pretraining, particularly for medical images.
Affine
C2FViT [343] •Presented a method in order to learn the global a ffine registration by taking advantage of the strong long-
range dependency recognition and locality of the hybrid Transformer and the multi-resolution strategy.•The suggested training framework can be extended to a number of parametric-based registration approaches by removing or scaling the
geometrical transformation matrices.
Rigid
SV oRT [336] •Devised an approach for the task of V olumetric reconstruction of fetal brains based on Transformer archi-
tectures.
•Employed a Transformer network trained on artificially sampled 2D MR slices that estimate the underly-
ing 3D volume from the input slices to more accurately predict transformation.•Experimental procedures on the FeTA dataset [352] represented the model’s ability in high-quality volumetric reconstruction.
•The volumetric reconstruction associated with the transformations of the proposed method displays higher visual quality.
Figure 39: The model has Lstages with convolutional patch embedding lay-
ers and NTransformer encoder blocks to learn the optimal a ffine registration
matrix. In each stage, fixed and moving images are downsampled and con-
catenated, then passed to the convolutional patch embedding layer to produce
image patch embeddings. The Transformer then produces the input feature em-
bedding from the embeddings [343].
8.2. Affine Registration
To perform a ffine medical image registration with Trans-
formers, Mok et al. [343] proposed C2FViT , a coarse-to-
fine vision Transformer that performs a ffine registration, a ge-
ometric transformation that preserves points, straight lines, and
planes while registering 3D medical images. Former studies
have relied on CNN-based a ffine registration that focuses on lo-
cal misalignment or global orientation [353, 354], which limits
the modeling of long-range dependencies and hinders high gen-
eralizability. C2FVit, as the first work that takes into accountthe non-local dependencies between medical images, leverages
vision Transformers instead of CNNs for 3D registration. As
depicted in Figure 39, the model is split into Lstages, each con-
taining a convolutional patch embedding layer and NiTrans-
former encoder blocks (i indicates the stage number), intending
to learn the optimal a ffine registration matrix. In each stage, the
fixed and moving images are downsampled and concatenated
with each other, then the new representation gets passed to the
convolutional patch embedding layer to produce image patch
embeddings. Next, the Transformer receives the embeddings
and produces the feature embedding of the input. Conducted
experiments on OASIS [346] and LPBA [351] demonstrated
their superior performance compared to existing CNN-based
affine registration techniques in terms of registration accuracy,
robustness, and generalization ability.
8.3. Rigid Registration
SVoRT [336] addressed the necessity of slice-to-volume reg-
istration before volumetric reconstruction for the task of volu-
metric fetal brains reconstruction, and employed a Transformer
network trained on artificially sampled 2D MR slices that learn
to predict slice transformation based on the information gained
44from other slices. The model also estimates the underlying 3D
volume from the input slices to promote higher accuracy in
transformation prediction. The superiority of their proposed
method in terms of registration accuracy and reconstruction
based on the evaluation of synthetic data and their experiments
on real-world MRI scans demonstrated the ability of the model
in high-quality volumetric reconstruction.
8.4. Discussion and Conclusion
According to the research discussed in this section, vision
Transformers are prominent tools in image registration tasks
due to their training capability on large-scale data, which
is made feasible by parallel computing and self-attention
mechanisms. Leveraging Transformers to encourage bet-
ter global dependency identification improves registration in
terms of dice scores and Jacobian matrix determinants com-
pared to CNNs.
To mitigate the burden of quadratic complexity when pro-
cessing images at high resolution and modeling local rela-
tionships, reviewed studies usually employ CNNs to provide
feature maps or dense displacement fields [338, 339, 340].
C2FViT [343] disregarded convolutional networks and im-
plemented convolutional patch embeddings to promote lo-
cality. However, in deformably registering medical con-
tent, XMorpher recently demonstrated the power of cross-
attention in better capturing spatial relevancy without a CNN
implementation [341], and SV oRT purely utilized Trans-
formers to perform rigid registration [336]. Regarding the
challenges caused by the data-hungry nature of deep neural
networks and the di fficulties in gathering annotated train-
ing data, studies in medical image registration have also
focused on integrating self-supervised learning with trans-
former models [342].
The notable experimental attempts on brain MRI scan data,
such as OASIS [346] and FeTA [352], show the importance
of accurate automatic registration for neuroimaging data.
One particular work [341] proposed to evaluate their regis-
tration on images of cardiac region datasets including MM-
WHO-2017 [348] and ASOCA [349]. To further clarify the
modality type used in the aforementioned proposed meth-
ods, all works conducted their evaluations on 3D or volu-
metric imaging modalities.
Based on the brief review of Transformer-based medical im-
age registration research, we believe that other regions of in-
terest (ROI) such as neurons, retina, and neck area are worth
exploring to facilitate diagnostic operations in di fferent do-
mains with more precise registration models.
We have also specified the architectural type, modality, or-
gan, data size, training paradigm, datasets, metrics, and
year for each medical registration technique reviewed in Ta-
ble 16. Furthermore, Table 17 provides a list of the contri-
butions and highlights of the proposed works. We also elab-
orate on the use cases of self-supervision in medical image
registration.9. Medical Report Generation
Medical report generation focuses on producing comprehen-
sive captions and descriptions pivoting on medical images for
diagnostic purposes. Designing automatic methods capable of
performing this task can alleviate tedious and time-consuming
work in producing medical reports and promote medical au-
tomation [372]. Recently, advancements in deep learning have
brought the attention of researchers to employing an intelligent
system capable of understanding the visual content of an im-
age and describing its comprehension in natural language for-
mat [373]. Research e fforts in improving this area can be em-
ployed in medical imaging by implementing systems capable
of providing descriptions and captions (i.e., generating medical
reports) concerning medical images. These captioning systems
usually utilize encoder-decoder models that encode medical im-
ages and decode their understandings to provide diagnostic in-
formation in a natural language format.
Despite the success of deep learning, limitations including
reliability on an immense amount of data, unbalanced data in
radiology datasets (e.g., IU X-ray chest X-Ray [374]), and the
black box nature of DL models entail challenges in medical
report generation [375]. The success of Transformer models
in many vision-and-language tasks has drawn the attention of
researchers in the medical report generation domain to the em-
ployment of this architecture. In this section, we discuss ap-
proaches that utilize Transformers to promote e ffective capture
of long-range context dependencies and better report genera-
tion. As illustrated in Figure 40, the following is our taxonomy
of these systems according to the mechanism by which they
produce accurate and reliable clinical reports:
(I)Reinforcement Learning-based. The ultimate goal of a
medical report generation system is to provide clinically
accurate and reliable reports. In reinforcement learning,
the MRG system is considered an agent with the objec-
tive of maximizing clinical accuracy based on the feed-
back given by the reward signal, which is directly calcu-
lated by the evaluation metric score (e.g., CIDEr [376]).
(II)Graph-based. Radiology reports are typically composed
of a long finding section with multiple sentences that
make report generation a challenging task. Therefore,
the inclusion of prior information is beneficial for facili-
tating the generation of long narratives from visual data.
Knowledge graphs, which are powerful models that can
capture domain-specific information in a structured man-
ner, can be used to exploit prior information for medical
report generation [377, 362, 378].
(III) Memory-based. Memory is a resource through which im-
portant information is recorded. In designing a proper
MRG system, it is crucial to store vital and diagnostic
information that can benefit the generation process by
incorporating prior knowledge and experience. Hence,
configuring a memory mechanism with Transformers
as a report generation framework facilitates longer and
more coherent text generation by sharing information
gained through the process [357, 361].
(IV) Other Systems. Systems that introduce di fferent ideas
from previous categories to improve clinical accuracy,
45 Medical Report 
 Generation
 RL-based
 1. RTMIC
 2. SIG Memory-based
 3. MDT
 4. AlignTransformer
 5. M2 TR Progressive 
 6. MDT-WCL
 7. CMN Graph-based
 8. KERP
 9. PPKED Other Systems
 10. CRG
 11. Medical-VLBERT
 12. CDGPT2
 13. CGI
 14. CCRG
 15. CXR-RePaiR
 16. BioViL-T 
 17. ChatCADFigure 40: Taxonomy of Transformer-based medical report generation approaches based on the mechanism by which they generate clinical reports. We reference
the papers in ascending order corresponding to their prefix number: 1. [355], 2. [356], 3. [357], 4. [358], 5. [359], 6. [360]. 7. [361], 8. [362], 9. [363], 10.
[364], 11. [365], 12. [366], 13. [367], 14. [368], 15. [369], 16. [370], 17. [371].
such as curriculum learning, contrastive learning, and al-
ternate learning, belong to this group.
9.1. Reinforcement Learning-based Systems
The first work to implement a Transformer architecture for
medical report generation is RTMIC [355]. It used the rein-
forcement learning strategy in training to mitigate the problem
of exposure bias prevailing in Seq2Seq models [393]. In their
approach, the original images are fed into a DenseNet [379] as
the region detector to extract bottom-up visual features. These
features are then passed into a visual encoder to generate visual
representations from the detected regions, which the caption-
ing detector then utilizes to generate captions for the specified
regions. The proposed method was experimented on the IU
X-Ray dataset [374] and achieved state-of-the-art results. In-
tegration of RL and Transformers was also applied in surgical
instruction generation since the joint understanding of surgical
activity along with modeling relations linking visual and tex-
tual data is a challenging task. Zhang et al. [356] employed a
Transformer-backboned encoder-decoder architecture and ap-
plied the self-critical reinforcement learning [394] approach to
optimize the CIDEr score [376] as the reward. Their approach
surpasses existing models in performance on the DAISI dataset
[381] with caption evaluation metrics applied to the model.
This work’s key di fference from others is that their model is
proposed to generate instructions instead of descriptions.
9.2. Graph-based Systems
In graph-based medical report generation, Li et al. [362] pro-
posed KERP , a Graph Transformer implementation to generate
robust graph structures from visual features that are extracted
by a DenseNet [379] backbone. This approach is composed
of three modules: Encode, Retrieve and Paraphrase. First, it
constructs an abnormality graph by converting the visual fea-
tures extracted from the medical images via an encoder mod-
ule. Next, a sequence of templates is retrieved considering thedetected abnormalities by utilizing a retrieve module. Subse-
quently, the terms of the produced templates are paraphrased
into a report by employing the paraphrase module. The KERP’s
workflow is illustrated in Figure 41.
Pleural 
effusion Consolidation 
Encode
GTRi2g Retrieve
GTRg2s Paraphrase
GTRgs2s 
GTRg2g
Abnormality graph Disease graph 
Visual feature 
CNNTemplates Report Degenerative changes in the spine. 
No pleural effusion. 
There is hyperexpansion of the lungs 
suggesting underlying emphysema. 
No focal airspace consolidation. 
Heart size is normal. 
Emphysema Degenerative 
disease Degenerative change 
of spine (0.66) Focal airspace 
consolidation 
(0.01) Hyperexpansion of 
lungs (0.78) 
Enlarged 
heart size 
(0.04) Tortuous 
aorta (0.12) 
Low lung 
volumes (0.00) 0.030.19
0.12
0.00
Figure 41: Using an encoder module, KERP creates an abnormality graph from
the extracted visual features. Then, a retrieval module retrieves a sequence
of templates based on detected abnormalities. Next, the paraphrase module
paraphrases the templates’ terms into a report [362].
Additionally, Liu et al. [363] addressed the visual and tex-
tual data biases and their consequences in generating radiology
reports and proposed the PPKED framework to alleviate these
challenges. Their work introduced three modules to perform re-
port generation: (1) Prior Knowledge Explorer (PrKE), which
obtains relevant prior information for the input images; (2) Pos-
terior Knowledge Explorer (PoKE), which extracts the poste-
rior information, including the abnormal regions of the medi-
cal image; and (3) Multi-domain Knowledge Distiller (MKD),
which distills the obtained information from the previous mod-
ules to perform the final report generation. PPKED then formu-
lated the problem by employing the presented modules in the
following manner: PoKE first extracts the image features cor-
responding to the relevant disease topics by taking the visual
features extracted by ResNet-152 [64] from the input image
and abnormal topic word embeddings as the input. Next, the
PrKE module filters the prior knowledge from the introduced
prior working experience (a BERT encoder) and prior medical
knowledge component that is relevant to the abnormal regions
of the input image by utilizing the output of the PoKE module.
Next, the MKD module generates the final medical report by
using this obtained information, which is implemented based
46Table 18: An overview of the reviewed Transformer-based Medical Report Generation approaches.
Method Modality Organ Type Visual Backbone Datasets Metrics Year
Reinforcement Learning
RTMIC [355] X-ray Lung 2D DenseNet-121 [379] IU Chest X-ray [374]BLEU [380]
CIDEr [376]2019
SIG [356]Ultrasound
ColonoscopyMulti-organ 3D ResNet-101 [64] DAISI [381]BLUE [380]
Meteor [382]
CIDEr [376]
ROUGE [383]
SPICE [384]2021
Graph
KERP [362] X-ray Lung 2D DenseNet-121 [379]1IU Chest X-ray [374]
2CX-CHR (private dataset)BLEU [380]
CIDEr [376]
ROUGE [383]2019
PPKED [363] X-ray Lung 2D ResNet-152 [64]1IU Chest X-ray [374]
2MIMIC-CXR [385]BLUE [380]
Meteor [382]
CIDEr [376]
ROUGE [383]2021
Memory
MDT [357] X-ray Lung 2D ResNet-121 [64]1IU Chest X-ray [374]
2MIMIC-CXR [385]BLEU [380]
Meteor [382]
ROUGE [383]2020
AlignTransformer [358] X-ray Lung 2D ResNet-50 [64]1IU Chest X-ray [374]
2MIMIC-CXR [385]BLEU [380]
Meteor [382]
ROUGE [383]2021
M2TR. progressive [359] X-ray Lung 2D DenseNet-121 [379]1IU Chest X-ray [374]
2MIMIC-CXR [385]BLEU [380]
Meteor [382]
ROUGE [383]2021
MDT-WCL [360] X-ray Lung 2D ResNet [64]1MIMIC-ABN [386]
2MIMIC-CXR [385]BLUE [380]
Meteor [382]
ROUGE [383]2021
CMN [361] X-ray Lung 2D ResNet-101 [64]1IU Chest X-ray [374]
2MIMIC-CXR [385]BLEU [380]
Meteor [382]
ROUGE [383]2022
Other
CRG [364] X-ray Lung 2D DenseNet-121 [379] MIMIC-CXR [385]BLUE [380]
Meteor [382]
CIDEr [376]
ROUGE [383]2020
Medical-VLBERT [365]CT
X-rayLung 2D DenseNet-121 [379]1Chinese Covid-19 CT [387]
2CX-CHR (private dataset)BLUE [380]
CIDEr [376]
ROUGE [383]2021
CDGPT2 [366] X-ray Lung 2D DenseNet-121 [379] IU chest X-ray [374]BLUE [380]
Meteor [382]
CIDEr [376]
ROUGE [383]2021
CGI [367] X-ray Lung 2D DenseNet-121 [379]1MIMIC-CXR [385]
2IU chest X-ray [374]BLUE [380]
Meteor [382]
ROUGE [383]2021
CGRG [368] X-ray Lung 2D ResNet-101 [64]1IU Chest X-ray [374]
2COV-CTR [388]BLUE [380]
Meteor [382]
ROUGE [383]2021
CXR-RePaiR [369] X-ray Lung 2D ✗ MIMIC-CXR [385]BLUE [380]
F12021
BioViL-T [370] X-ray Lung 2D ResNet-50 [64]1MIMIC-CXR [385]
2MS-CXR-T [370]BLUE [380]
ROUGE [383]
CHEXBERT [389]2023
ChatCAD [371] X-ray Lung 2D ✗ MIMIC-CXR [385] Precision, Recall, F1 2023
on the decoder part of the Transformers equipped with Adap-
tive Distilling Attention.
47Table 19: A brief summary of the reviewed Transformer-based medical report generation methods.
Method Contributions Highlights
Reinforcement Learning
RTMIC [355] •Presented a novel Hierarchical Reinforced Transformer for producing comprehensible, informative med-
ical reports by training through reinforcement learning-based training.
•The initial attempt at incorporating Transformers to develop a medical report generation system.•Utilized reinforcement learning to ameliorate the exposure bias problem.
•Enhanced clinical report coherence by employing Transformers to capture long-range dependencies.
•The selected metric (CIDEr) as a reward signal is not designed for the medical domain [390].
SIG [356] •Generated surgical instructions from multiple clinical domains by utilizing a Transformer-based Encoder-
decoder architecture.•The proposed method is able to produce multimodal dependencies, form pixel-wise patterns, and develop textual associations for the masked
self-attention decoder.
•Utilizing self-critical reinforcement learning to perform optimization increased the performance of surgical instruction generation.
•The selected metric (CIDEr) as a reward signal is not designed for the medical domain [390].
Graph
KERP [362] •Developed an MRG system using a hybrid retrieval-generation technique that unifies standard retrieval-
based and recent visual text generation methods.
•Introduced Graph Transformer (GTR) as the first research to employ an attention mechanism to convert
different data types formulated as a graph.•Aligning the generated reports with abnormality attention maps by providing location reference facilitates medical diagnosis.
•Since KERP is designed based on abnormality detection, it may disregard other valuable information [367].
PPKED [363] •Proposed a three-module system that mimics the working habits of radiologists by extracting abnormal
regions, encoding prior information, and distilling the useful knowledge to generate accurate reports.•Provides abnormal descriptions and locations to facilitate medical diagnosis.
•Capable of extracting relevant information from the explored posterior and prior multi-domain knowledge.
•Some mistakes, such as duplicate reports and inaccurate descriptions, are present in the generated reports. [391]
Memory
MDT [357] •Introduced Memory-Driven Transformer for radiology report generation
•Developed a relational memory to retain essential knowledge gathered through the previous generations.•To facilitate medical diagnosis, visual-textual attention mappings were incorporated to capture correspondence with essential medical terms.
•Dataset imbalance with dominating normal findings hinders the model’s generalizability.
AlignTransformer [358] •Introduced an MRG framework that mitigates the problem of data bias by hierarchically aligning visual
abnormality regions and illness tags in an iterative fashion.•Conducted experiments on MIMIC-CXR and IU-Xray datasets and demonstrated the capability of the model in ameliorating the data bias
problem
M2TR. progressive [359] •Developed a progressive text generation model for medical report generation by incorporating high-level
concepts into the process of generation.•The division of report generation into two steps enhanced the performance in terms of language generation and clinical e fficacy metrics.
•The progressive generation process increases the false positive rate by including abnormality mentions in negation mode. [390].
MDT-WCL [360] •Introduced the contrastive learning technique into chest X-ray report generation by proposing a weakly
supervised approach that contrasts report samples against each other to better identify abnormal findings.•Optimization with contrastive loss facilitates generalizability in comparison to contrastive retrieval-based methods.
CMN [361] •Cross-modal memory networks were introduced to improve report generation based on encoder-decoder
architectures by incorporating a shared memory to capture multi-modal alignment.•Capable of properly aligning data from radiological images and texts to aid in the preparation of more precise reports in terms of clinical
accuracy.
Other
CRG [364] •Formulated the problem in two steps: (1) a report generation phase incorporating a standard language
generation objective to train a Transformer model, and (2) a sampling phase that includes sampling a report
from the model and extracting clinical observations from it.•Transformers’ ability to provide more coherent and fluent reports was demonstrated.
•Due to the biased nature of the dataset caused by dominant normal findings, the algorithm tends to generate reports that lack essential
descriptions of abnormal sections [392].
Medical-VLBERT [365] •Proposed a framework as the first work that generates medical reports for the COVID-19 CT scans
•Devised an alternate learning strategy to minimize the inconsistencies between the visual and textual data.•Alleviated the shortage of COVID-19 data by employing the transfer learning strategy.
•Capable of e ffective terminology prediction
•Overreliance on predetermined terminologies undermines robustness and generalizability.
CDGPT2 [366] •Presented a conditioning mechanism to improve radiology report generation in terms of word-overlap
metrics and time complexity.
•Utilized a pre-trained GPT2 conditioned on visual and weighted semantic features to promote faster
training, eliminate vocabulary selection, and handle punctuation.
•The first study to employ semantic similarity metrics to quantitatively analyze medical report generation
results.•Conditioning mechanism tackled punctuations, vocabulary collection, and reduced training duration.
•The architecture does not require modification to be trained on distinct data sets.
•Incorporating semantic similarity in addition to word overlap metrics improved medical report evaluation.
4The model’s generalization ability and robustness against over-fitting are both hindered when the size of the dataset is small.
CGI [367] •Provides cohesive and precise X-ray reports in a fully di fferentiable manner by dividing the report gener-
ation system into a classifier, generator, and interpreter.
•Their conducted experiments revealed that incorporating additional scans besides clinical history can be
beneficial in providing higher-quality X-ray reports.•Flexibility in processing additional input data, such as clinical documents and extra scans, which also contributes to performance improve-
ment.
•The model doesn’t provide vital information, such as illness orientation and time-series correlations, which facilitates more reliable reports.
CGRG [368] •Presented a Transformer-based method that estimates report uncertainty to develop a more reliable MRG
system and facilitate diagnostic decision-making.
•Introduced the Sentence Matched Adjusted Semantic Similarity (SMAS) to capture vital and relevant
features in radiology report generation more e ffectively.•Assessing visual and textual uncertainties leads to more reliable reports in medical diagnosis.
•Measuring uncertainties can properly provide correlated confidence between various reports, which is beneficial to aiding radiologists in
clinical report generation [392].
CXR-RePaiR [369] •Employs contrastive language image pre-training (CLIP) to generate chest X-ray reports.
•The utilization of representations pairs of X-ray reports aids the process of retrieving unstructured radiol-
ogy reports written in natural language.•Accuracy is evaluated by comparing it with the ground truth using a labeler, and performance scores are calculated accordingly.
•Their experiments indicate that despite generating precise diagnostic explanations, it may not consistently employ identical wording as the
initial report.
BioViL-T [370] •Provides a hybrid design that aligns X-ray report pairs using temporal information and prior knowledge.
•Combines CNN and transformer encoders to extract image representations and matches them with text
representations using a chest X-ray domain-specific transformer model.
•Introduced MS-CXR-T, a benchmark dataset that facilitates the evaluation of vision-language representa-
tions in capturing temporal semantics.•Prior report is more crucial for optimal performance compared to prior image as it outlines the image content and enhances the signal clarity.
•A significant factor in e ffective language supervision during the pre-training phase is the utilization of contrastive loss.
ChatCAD [371] •Integrates MRG with large language models such as ChatGPT to improve diagnosis accuracy
•The LLM summarizes and corrects errors in the generated reports via the concatenation of textual repre-
sentations from CAD networks.•The experiments show that model size and complexity have a significant impact on generation accuracy, as larger models generally result in
improved F1-scores.
•Due to the reports being less similar to human-written text, the system’s BLEU score is lower compared to previous methods.
9.3. Memory-based Systems
Concerning the development of systems that rely on a mem-
ory mechanism to generate medical reports, Chen et al. [357]
presented a Memory-Driven Transformer ( MDT ), a model suit-
able for the generation of long informative reports and one of
the first works on the MIMIC-CXR dataset [385]. MDT em-
ploys a relational memory to exploit characteristics prevailing
in reports of similar images, and then the memory is incorpo-
rated into the decoder section of the Transformer by implement-
ing a memory-driven conditional layer normalization (MCLN).
Likewise, Nooralahzadeh et al. [359] introduced M2TR.
progressive , a report generation approach that utilizes curricu-
lum learning, which is a strategy of training machine learning
models by starting with easy samples and gradually increasing
the samples’ di fficulty [395]. Instead of directly generating full
reports from medical images, their work formulates the prob-
lem into two steps: first, the Meshed-Memory Transformer (M2
TR.) [396], as a powerful image captioning model, receives the
visual features extracted by a DenseNet [379] backbone and
generates high-level global context. Second, BART [397], as
a Transformer-based architecture, encodes these contexts with
a bidirectional encoder and decodes its output using a left-to-
right decoder into coherent reports. The overview of the pro-
cess is depicted in Figure 42.
Figure 42: Workflow of the M2Tr. Progressive framework. The task is ac-
complished in two stages: First, the Meshed-Memory Transformer (M2TR.)
receives visual features extracted by a DenseNet [379] backbone and generates
high-level global context. Second, the BART [397] architecture encodes con-
texts with a bidirectional encoder and decodes them with a left-to-right decoder
to produce coherent reports [359].
Additionally, You et al. [358] proposed AlignTransformer ,
a framework composed of two modules: Align Hierarchical At-
tention (AHA) and Multi-Grained Transformer (MGT). In their
approach, first visual features and disease tags are extracted
from the medical image by an image encoder, then they get
aligned hierarchically to obtain multi-grained disease-grounded
visual features in the AHA module. The obtained grounded fea-
tures are capable of tackling the data bias problem by promot-
ing a better representation of abnormal sections. Next, these
grounded visual features are exploited by an adaptive exploit-
ing attention (AEA) [396] mechanism in the MGT module for
48Table 20: Comparison of Transformer-based medical report generation systems in terms of NLG performance metrics. The methods are ordered by BL-4 score,
which captures more precise phrase and sentence structure.
IU Chest X-ray [374]
Method BL-1 BL-2 BL-3 BL-4 METEOR ROUGE CIDEr
RTMIC [355] 0.350 0.234 0.143 0.096 - - 0.323
CDGPT2 [366] 0.387 0.245 0.166 0.111 0.164 0.289 0.257
KERP [362] 0.482 0.325 0.226 0.162 - 0.339 0.280
MDT [357] 0.470 0.304 0.219 0.165 0.187 0.371 -
PPKED [363] 0.483 0.315 0.224 0.168 0.190 0.376 0.351
CMN [361] 0.475 0.309 0.222 0.170 0.191 0.375 -
AlignTransformer [358] 0.484 0.313 0.225 0.173 0.204 0.379 -
M2TR. progressive [359] 0.486 0.317 0.232 0.173 0.192 0.390 -
CGRG [368] 0.497 0.357 0.279 0.225 0.217 0.408 -
MIMIC-CXR [385]
Method BL-1 BL-2 BL-3 BL-4 METEOR ROUGE CIDEr
MDT [357] 0.353 0.218 0.145 0.103 0.142 0.277 -
PPKED [363] 0.360 0.224 0.149 0.106 0.149 0.284 0.237
CMN [361] 0.353 0.218 0.148 0.106 0.142 0.278 -
M2TR. progressive [359] 0.378 0.232 0.154 0.107 0.145 0.272 -
AlignTransformer [358] 0.378 0.235 0.156 0.112 0.158 0.283 -
CRG [364] 0.415 0.272 0.193 0.146 0.159 0.318 -
CGI [367] 0.495 0.360 0.278 0.224 0.222 0.390 -
the generation of the medical reports. They also justified their
model’s e fficiency through the manual evaluation of clinical ra-
diologists.
InMDT-WCL [360], the problem is approached with a
weakly supervised contrastive loss, which lends more weight
to the reports that are semantically close to the target reports,
and a memory-driven Transformer is adopted as the backbone
model to store key information in its memory module. To aid
the contrastive learning during training, after clustering the re-
ports into groups with the K-Means algorithm, each report is
assigned a label corresponding to its cluster, and the semanti-
cally closed ones are considered to be in the same cluster.
Although previous approaches have achieved promising re-
sults, they lack the ability to generate mappings between im-
ages and texts to align visual-textual information and assist
medical diagnosis. In order to facilitate visual-textual align-
ment, the Cross-modal Memory Network ( CMN ) [361] ex-
tended encoder-decoder methods by utilizing a shared memory
for better alignment of information between images and texts.
It uses a pre-trained ResNet [64] as the visual extractor to out-
put visual features, then passes them to the cross-modal mem-
ory network that utilizes a matrix to store information where
each row represents the embedding of information linking im-ages and texts. To access the stored information aligning the
modalities, memory querying and responding are implemented
in a multi-threaded manner.
9.4. Other Systems
Other MRG systems focus on solving the problem with dif-
ferent ideas. Lovelace et al. [364] proposed a generation frame-
work composed of two stages. In the first stage, a Transformer
model is adopted to map the input image features extracted by
a DenseNet-121 [379] to contextual annotations and learn re-
port generation. In the second stage, a procedure is introduced
to differentiably sample a clinical report from the Transformer
decoder and obtain observational clinical information from the
sample. This di fferentiability is further employed to fine-tune
the model for improving clinical coherence by applying their
differentiable CheXpert to the sampled reports. Fueled by re-
cent progress in explainable artificial intelligence and the in-
troduction of algorithms that attempt to provide interpretable
prediction in DL-based systems, Likewise, in CDGPT2 [366],
the medical image is passed into a Chexnet [398] to provide
localizations of 14 types of diseases from the images as vi-
sual features. To implement better semantic features, the model
was fine-tuned as a multi-label classification problem to extract
49manual tags from the IU-Xray dataset [374] by replacing the
final layer of the model with a layer containing 105 neurons to
produce 105 tags. The vector representation of the tags is then
fed into a pre-trained distilGPT2 [399] as the decoder to gener-
ate medical reports. Moreover, Wang et al. [368] presented a
confidence-guided report generation ( CGRG ) approach to sup-
port reliability in report generation by quantifying visual and
textual uncertainties. It’s comprised of an auto-encoder that
reconstructs images, a Transformer encoder that encodes the
input visual feature extracted by ResNet-101 [64], and a Trans-
former decoder for report generation. Visual uncertainty is ob-
tained by the AutoEncoder, which acts as a guide for the visual
feature extractor, and textual uncertainty is quantified based on
the introduced Sentence Matched Adjusted Semantic Similar-
ity (SMAS) which captures the similarity between the gener-
ated reports. These uncertainties are further utilized to aid the
model optimization process.
The recent outbreak of COVID-19, one of the deadliest pan-
demics, has influenced the research community to alleviate the
tedious and time-consuming work of producing medical re-
ports. VL-BERT, [400] as an extension of BERT, [30] can be
employed as an intelligent medical report generation system to
expedite the diagnosis process. Medical-VLBERT [365] in-
troduced VL-BERT to the medical report generation domain.
It defines the problem as a two-step procedure: First, it uti-
lizes two distinct VL-BERTs as terminology encoders to pro-
duce terminology-related features (textual and visual), and then
these features are fed into a shared language decoder to produce
medical textbooks and reports. The proposed method takes into
account predefined terminology word embeddings that repre-
sent medical domain knowledge. These embeddings are paired
distinctly with two other embeddings as an input to the en-
coders: textbook embeddings, which are generated by employ-
ing a lookup table, and spatial feature embeddings (termed ”vi-
sual context”) that are extracted from medical images by im-
plementing DenseNet-121 [379]. The encoders then integrate
this pairwise information separately to produce textual and vi-
sual terminological features. Subsequently, a shared language
decoder is trained by utilizing an alternate approach to properly
exchange the knowledge captured by the encoders.
Furthermore, in the work of Nguyen et al. [367], a clas-
sification, generation, and interpretation framework ( CGI ) is
proposed to address clinical accuracy. Each term of the frame-
work’s name represents a di fferent module to perform the task.
The classification module learns how to discover diseases and
generate their embeddings, which consist of an image and
text encoder to extract the global visual features from medi-
cal images and obtain text-summarized embeddings from clini-
cal documents. The generation module is a Transformer model
that takes the disease embeddings as input and generates med-
ical reports from them. The interpretation module then takes
these reports for evaluation and fine-tuning.
Deep learning methods in medical report analysis require
extensive amounts of annotated training data, which requires
a significant amount of time and e ffort. To this end, self-
supervised report generation approaches use a large amount of
unlabeled text data without explicit supervision. For instance,CXR-RePaiR [369] utilizes self-supervised contrastive lan-
guage image pre-training. It encodes reports with the CheXbert
transformer network[389], and leverages the X-ray report pair
representations to retrieve unstructured radiology reports with a
contrastive scoring function trained on chest X-ray-report pairs
to rank the similarity between a test dataset of X-rays and a
large corpus of reports. Likewise, BioViL-T [370] is a hybrid
design that improves the alignment of X-ray report pairs by in-
corporating temporal information and prior knowledge to sup-
port complementary self-supervision and exploit the strengths
of each modality. It compares prior images with given reports to
exploit temporal correlations and enhance representation learn-
ing. BioViL-T uses a multi-image encoder to handle the ab-
sence of prior images and spatial misalignment, benefiting both
image and text models. Image representations are extracted uti-
lizing a hybrid CNN and transformer encoder, which are then
matched with corresponding text representations obtained with
CXR-BERT [401] by training with contrastive objectives.
Recently, large language models (LLMs), which are AI sys-
tems that learn the patterns of human language from mas-
sive amounts of text data, have revolutionized human-like text
generation and natural language understanding. Consequently,
they hold great potential in clinical applications, such as under-
standing medical texts and report generation [402, 403]. One
of the successful large language models based on the GPT-
3.5 architecture is ChatGPT [404], with extensive knowledge
and abilities to perform a wide range of tasks such as question
answering, text summarization, and generating textual instruc-
tions. Wang et al. [371] presented a scheme called ChatCAD
that integrates LLMs such as GPT-3 [405] and ChatGPT with
Computer-Aided Diagnosis (CAD) models to perform medical
report generation. CAD systems are computer programs that
assist doctors in interpreting medical images, such as X-rays or
MRI scans. To generate reports via ChatCAD, the medical im-
age is passed to a segmentation, image classification, and a re-
port generation network. Since the outputs of segmentation and
classification networks are a mask and a vector, respectively,
they are transformed into textual format to be understandable
by LLMs. These textual representations are then concatenated
and presented to the LLM, which then summarizes the results
from all the CAD networks and exploits them to correct the
errors in the generated report. Their experiments indicate that
LLMs can be more e ffective than conventional CAD systems in
enhancing medical report quality.
9.5. Discussion and Conclusion
This section o ffers a systematic review of the Trans-
former architectures configured for medical report gener-
ation. Compared to previous sections that reviewed ViT-
based frameworks to tackle di fferent medical tasks and prob-
lems, this section focuses mostly on using standard Trans-
formers as the core of a medical report generation sys-
tem. A common theme prevailing in these systems is to
solve the problem with an encoder-decoder architecture sup-
ported by a CNN-based visual backbone. As mentioned in
previous sections, the self-attention mechanism undermines
50the representation of low-level details. On the other hand,
since medical reports consist of long and multiple sentences,
Transformers are of great significance to model long-term
dependencies, which assists clinically accurate report gen-
eration [406, 367]. To exploit the power of both CNNs
and Transformers simultaneously, state-of-the-art MRG sys-
tems usually embed CNNs along with Transformers in their
frameworks [355, 366, 368]. We have provided information
in Table 18 on the reviewed report generation methods con-
cerning their architectural type, modality, organ, pre-trained
strategy, datasets, metrics, and year. Table 19 contains
summarized information about the methodologies, includ-
ing their contributions and highlights. In addition, it should
be noted that several survey publications have been pub-
lished in this field of medicine [375, 390, 407], and the most
recent one provided a technical overview of Transformer-
based clinical report generation [22]. We approach our
review di fferently by distinguishing the proposed methods
based on the mechanism they used to support the prevailing
concerns such as long and coherent text generation, reliabil-
ity, and visual-textual biases.
The ultimate goal of these frameworks is to increase clin-
ical accuracy to expedite the diagnosis process and reduce
the workloads in radiology professions [364, 367]. Numer-
ous works have attempted to facilitate diagnostic decision-
making by aligning correlated sections of medical image
and textual report that provide valuable information for de-
tecting abnormalities [358, 361]. Also, multiple studies em-
phasized the importance of universal knowledge, and de-
signed a system to incorporate prior information for detect-
ing disease [362, 357]. Some research e ffort was also put
into better representation learning by contrasting normal and
abnormal samples against each other in representation space
by utilizing a contrastive loss as the objective [360]. One re-
cent work was inspired by curriculum learning to imitate the
order of the human learning process [359].
As for the cumbersomeness of accessing annotated textual
training data, research in MRG also focused on consolidat-
ing self-supervised learning and transformer architectures
for report generation, which are discussed in Section 9.4
through the analysis of two specific works [408, 369]. Chat-
CAD [371] was also introduced recently to integrate MRG
with large language models like ChatGPT to improve di-
agnosis accuracy, but it achieves a lower BLEU score as it
generates reports that are less similar to human-written text.
The Table 20 includes performance values of BLEU, ME-
TEOR, ROUGE, and CIDEr for each method. Among the
methods evaluated on the IU Chest X-ray, KERP and CGRG
perform the best across most metrics, while RTMIC and
CDGPT2 are the weakest. MDT has the highest METEOR
score, CGRG achieves the highest ROUGE score, and PP-
KED demonstrates the highest CIDEr score. In contrast, for
the MIMIC-CXR dataset, CRG and CGI demonstrate the
best performance, while MDT shows the weakest overall
performance.
Additionally, Table 20 compares the performance of the re-viewed methods Natural Language Generation (NLG) met-
rics. The methods are presented in separate sections based
on the dataset they were evaluated on, with one section ded-
icated to the IU Chest X-ray [374], and the other to the
MIMIC-CXR [385].
Overall, we believe that MRG systems need more research
and progression to be robustly incorporated in a practical
setting.
10. Open Challenges and Future Perspectives
So far, we discussed the application of Transformers (espe-
cially vision Transformers) and reviewed state-of-the-art mod-
els in medical image analysis. Even though their e ffective-
ness is exemplified in previous sections by delicately present-
ing their ideas and analyzing the significant aspects that were
addressed in their proposed methods, there is still room for
improvement in many areas to devise a more practical and
medically accurate system by leveraging Transformers. Conse-
quently, we discuss the challenges and future directions hoping
to help researchers gain insight into the limitations and develop
more convenient automatic medical systems based on Trans-
formers.
10.1. Explainability
Fueled by recent progress in XAI (explainable artificial intel-
ligence) and the introduction of algorithms that attempt to pro-
vide interpretable prediction in DL-based systems, researchers
are putting e ffort into incorporating XAI methods into con-
structing Transformer-based models to promote a more reliable
and understandable system in di fferent areas, including medi-
cal analysis [409, 410]. Existing approaches usually highlight
important regions of the medical image that contribute to the
model prediction by employing attention maps [411, 38]. Fur-
thermore, Vision Transformers (ViTs) have the ability to pro-
vide attention maps that indicate the relevant correlations be-
tween the regions of the input and the prediction. However,
the challenge of numerical instabilities in using propagation-
based XAI methods such as LRP [412] and the vagueness of
the attention maps, which leads to inaccurate token associations
[76, 413], makes interpretable ViTs an open research opportu-
nity in computer vision, especially in medical image analysis.
We believe that including interpretable vision Transformers,
such as ViT-NeT [413], in various medical applications can pro-
mote user-friendly predictions and facilitate decision-making in
the diagnosis of medical conditions, and is a promising direc-
tion in medical research problems.
10.2. Richer Feature Representation
An effective and suitable representation space is substantially
influential in building medical analysis systems. Transformers
have demonstrated their e fficiency in obtaining global informa-
tion and capturing long-term dependencies in many areas, such
as Natural Language Processing (NLP), Computer Vision, and
Speech Recognition [335], and CNNs have proven to be e ffec-
tive in extracting local context from visual data [414]. However,
51this locality usually enables these networks to capture rich lo-
cal texture representation and lacks model global dependency.
As a result, many approaches stack Transformers along with
CNNs to leverage both local and global information simultane-
ously in clinical applications (e.g., medical report generation)
[358, 364, 48]. Recent studies stated that the single-scale rep-
resentation of ViTs hinders improvement in dense prediction
tasks, so a multi-scaled feature representation is implemented
which achieves better performance in computer vision tasks,
including image classification, object detection, and image seg-
mentation [415, 416]. Generalizing this idea to medical appli-
cations of ViTs to facilitate devising a clinically suitable system
can be considered as future work.
10.3. Video-based analysis
There has been an increasing interest in the vision commu-
nity in extending ViT architectures to video recognition tasks.
Recently, a handful of papers have integrated standard Trans-
formers with their models in AI-assisted dynamic clinical tasks
[417, 418, 419, 420]. However, the scarcity of the proposed ap-
proaches puts video-based medical analysis in an infancy stage
and open for future investigations. Another potential research
direction is to explore the power of video vision Transformer
variants, such as Video Swin Transformer [421], in clinical
video understanding and to facilitate automatic robotic surgery.
10.4. High Computational Complexity
The robustness of Transformer models in layouts that im-
plement large numbers of parameters is one of their strengths.
While this is a beneficial trait that makes it possible to train
models of enormous scale, it leads to the requirement of large
resources for training and inferencing [21]. Particularly dis-
advantageous to medical image analysis is that expanding the
use of ViTs for pretraining in new tasks and datasets comes
with substantial expenses and burdens. Additionally, gathering
medical samples can be di fficult and the dataset scale is often
limited. For instance, according to empirical studies in [15],
pretraining a ViT-L /16 model on the large-scale dataset of Ima-
geNet takes approximately 30 days employing a standard cloud
TPUv3 with 8 cores. As a result, a notable number of papers
utilized the pre-trained weights of ViT models to exploit the
transfer learning strategy to alleviate training load [41, 17, 40],
but in some cases, such as dealing with volumetric medical
images, where transfer learning doesn’t demonstrate any im-
provements [159, 338], the pretraining process is necessary to
capture domain-specific features for generalization and better
performance.
In addition, the standard self-attention mechanism su ffers
from a significant drawback: its computational complexity
grows quadratically with the number of tokens ( n), making it
impractical for tasks where input images can be millions of
pixels in size or when dealing with volumetric data. To ad-
dress this issue, several approaches have been proposed to re-
duce the computational burden of self-attention. They either
reduce the number of tokens by windowing [422, 58], shift the
calculation to the channel dimension [423, 424], or change the
order of multiplying query, key, and value [425]. In the contextof 3D biomedical image segmentation, Zhang et al. [426] in-
troduced a dynamic linear Transformer algorithm that achieves
linear complexity by focusing computations only on the region
of interest (ROI). This significantly reduces the overall com-
putational requirements. Shen et al. [425] proposed an e ffi-
cient attention mechanism that avoids the pairwise similarity
computations of dot-product attention. Instead, it normalizes
the keys and queries first, performs multiplication between the
keys and values, and then multiplies the resulting global con-
text vectors with the queries. This approach reduces the com-
putational complexity of self-attention from O(dn2) toO(d2n),
where dis the embedding dimension. XCiT, [423] another
approach, addresses the complexity challenge by introducing
cross-covariance attention, which has a complexity of O(d2n
h),
where his the number of attention heads. EdgeNext, an archi-
tecture proposed by Maaz et al. [424] for edge devices, op-
timizes the number of Multiplication-Addition (MAdd) opera-
tions required. It employs a variation of the attention mecha-
nism used in XCiT called split depth-wise transpose attention
(SDTA). Huang et al. [154] introduce e fficient self-attention
(ESA), specifically for the spatial reduction in 3D data. ESA re-
duces the number of tokens by a spatial reduction ratio Rwhile
expanding the channel dimension by the same ratio. As a result,
the complexity of ESA is reduced to O(n2
R).
Despite progress in making self-attention more e fficient for
computer vision tasks, further work is needed to develop e ffi-
cient Transformer models tailored to the medical domain, par-
ticularly for handling challenging 3D data or large-scale im-
ages (e.g., WSI). Moreover, the ultimate goal is to deploy these
models on low-cost edge devices or in resource-constrained
environments. Therefore, the focus should be on designing
Transformer systems that strike a balance between computa-
tional complexity and clinical accuracy while ensuring robust-
ness. This area of research holds great promise and should be
pursued further.
10.5. Transformer-based Registration
As reviewed in Section 8, the idea of employing Transform-
ers to support e fficient medical image registration has become
popular in recent years. The ability of the self-attention mecha-
nism assists the learning of long-term visual correlations since
their unlimited receptive field promotes a more accurate under-
standing of the spatial relationship between moving and fixed
images [338, 339]. However, registration systems composed of
Transformer architectures are still in their infancy and require
more research e ffort to be put into them.
10.6. Data-Driven Predictions
With supervised learning as a popular fashion in building in-
telligent systems, the model learns features based on the pro-
vided annotations that are suitable to accomplish a specific
task, which hinders generalizability. In other words, super-
vised learning modifies the bias-variance trade-o ffin favor of
the strong inductive biases that lead to making assumptions as a
means to aid the model in learning a particular task quicker and
with higher sample e fficiency. However, these hard assump-
tions sacrifice adaptability to other settings and unseen datasets,
52and the model learns to accomplish its task without having an
innate understanding of the data. To tackle this issue, unsu-
pervised regimes enable the algorithms to act as general de-
scriptors and capture features that will assist them in perform-
ing efficiently in a wide range of tasks. Similarly, in medical
image analysis, adopting Transformer networks with unsuper-
vised learning algorithms promotes robustness and generaliz-
ability to other datasets and tasks.
10.7. Medical Software Ecosystems
A future direction for advancing in the automatic medical
analysis is to provide an open-source environment that contains
libraries suitable for solving multiple medical tasks and chal-
lenges with Transformer architectures. Developers can further
contribute to the ecosystem by updating and adding additional
tasks, bringing novelty, and proposing ideas to enhance perfor-
mance and accuracy [152]. Companies and organizations can
support the system by preparing the necessary computational
resources and hardware requirements. Sample of software pro-
totypes in this direction are nnU-Net [427], Ivadomed [428],
and preliminary works such as [166], which provides an end-
to-end pipeline for implementing deep models on medical data.
11. Discussion and Conclusion
In this paper, we presented a comprehensive encyclopedic
review of the applications of Transformers in medical imag-
ing. First, we provided preliminary information regarding the
Transformer structures and the idea behind the self-attention
mechanism in the introduction and background sections. Start-
ing from Section 3, we reviewed the literature on Transformer
architecture in diverse medical imaging tasks, namely, classifi-
cation, segmentation, detection, reconstruction, synthesis, reg-
istration, and clinical report generation. For each application,
we provided a taxonomy and high-level abstraction of the core
techniques employed in these models along with the SOTA ap-
proaches. We also provided comparison tables to highlight the
pros and cons, network parameters, type of imaging modality
they are considering, organ, and the metrics they are using. Fi-
nally, we outlined possible avenues for future research direc-
tions.
Acknowledgments This work was funded by the German Re-
search Foundation (Deutsche Forschungsgemeinschaft, DFG)
under project number 191948804. We thank Johannes
Stegmaier for his contribution to the proofreading of this docu-
ment.
References
[1] J. Arevalo, F. A. Gonz ´alez, R. Ramos-Poll ´an, J. L.
Oliveira, M. A. G. Lopez, Representation learning for
mammography mass lesion classification with convolu-
tional neural networks, Computer methods and programs
in biomedicine 127 (2016) 248–257.
[2] S. Karimijafarbigloo, R. Azad, A. Kazerouni, D. Mer-
hof, MS-Former: Multi-scale self-guided transformerfor medical image segmentation, in: Medical Imaging
with Deep Learning, 2023.
[3] R. Azad, M. Asadi-Aghbolaghi, M. Fathy, S. Escalera,
Bi-directional ConvLSTM U-Net with densley con-
nected convolutions, in: 2019 IEEE /CVF International
Conference on Computer Vision Workshop (ICCVW),
2019, pp. 406–415.
[4] R. Azad, N. Khosravi, D. Merhof, SMU-Net: Style
matching U-Net for brain tumor segmentation with miss-
ing modalities, in: International Conference on Medical
Imaging with Deep Learning, PMLR, 2022, pp. 48–62.
[5] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello,
A. Levskaya, J. Shlens, Stand-alone self-attention in vi-
sion models, Advances in Neural Information Processing
Systems 32 (2019).
[6] I. Bello, B. Zoph, A. Vaswani, J. Shlens, Q. V . Le, Atten-
tion augmented convolutional networks, in: Proceedings
of the IEEE /CVF international conference on computer
vision, 2019, pp. 3286–3295.
[7] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar,
B. Hechtman, J. Shlens, Scaling local self-attention for
parameter e fficient visual backbones, in: Proceedings of
the IEEE /CVF Conference on Computer Vision and Pat-
tern Recognition, 2021, pp. 12894–12904.
[8] X. Wang, R. Girshick, A. Gupta, K. He, Non-local neu-
ral networks, in: Proceedings of the IEEE conference
on computer vision and pattern recognition, 2018, pp.
7794–7803.
[9] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation net-
works, in: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2018, pp. 7132–
7141.
[10] M. Al-Shabi, K. Shak, M. Tan, ProCAN: Progressive
growing channel attentive non-local network for lung
nodule classification, Pattern Recognition 122 (2022)
108309.
[11] D. V . Sang, T. Q. Chung, P. N. Lan, D. V . Hang,
D. Van Long, N. T. Thuy, Ag-curesnest: A novel
method for colon polyp segmentation, arXiv preprint
arXiv:2105.00402 (2021).
[12] C. Yao, J. Tang, M. Hu, Y . Wu, W. Guo, Q. Li, X.-P.
Zhang, Claw U-Net: A UNet variant network with deep
feature concatenation for scleral blood vessel segmen-
tation, in: CAAI International Conference on Artificial
Intelligence, Springer, 2021, pp. 67–78.
[13] T. Gonc ¸alves, I. Rio-Torto, L. F. Teixeira, J. S. Cardoso,
A survey on attention mechanisms for medical applica-
tions: are we moving towards better algorithms? (2022).
53[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin, At-
tention is all you need, Advances in neural information
processing systems 30 (2017).
[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-
senborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
derer, G. Heigold, S. Gelly, et al., An image is worth
16x16 words: Transformers for image recognition at
scale, arXiv preprint arXiv:2010.11929 (2020).
[16] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai,
Deformable{detr}: Deformable transformers for end-
to-end object detection, in: International Conference on
Learning Representations, 2021.
URL https://openreview.net/forum?id=
gZ9hCDWe6ke
[17] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu,
A. L. Yuille, Y . Zhou, Transunet: Transformers make
strong encoders for medical image segmentation, arXiv
preprint arXiv:2102.04306 (2021).
[18] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c,
C. Schmid, Vivit: A video vision transformer, in: Pro-
ceedings of the IEEE /CVF International Conference on
Computer Vision, 2021, pp. 6836–6846.
[19] X. Chen, X. Wang, J. Zhou, Y . Qiao, C. Dong, Activating
more pixels in image super-resolution transformer, in:
Proceedings of the IEEE /CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 22367–22377.
[20] B. Azad, R. Azad, S. Eskandari, A. Bozorgpour,
A. Kazerouni, I. Rekik, D. Merhof, Foundational models
in medical imaging: A comprehensive survey and future
vision, arXiv preprint arXiv:2310.18689 (2023).
[21] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan,
M. Shah, Transformers in vision: A survey, ACM com-
puting surveys (CSUR) 54 (10s) (2022) 1–41.
[22] F. Shamshad, S. Khan, S. W. Zamir, M. H. Khan,
M. Hayat, F. S. Khan, H. Fu, Transformers in medi-
cal imaging: A survey, Medical Image Analysis (2023)
102802.
[23] K. He, C. Gan, Z. Li, I. Rekik, Z. Yin, W. Ji, Y . Gao,
Q. Wang, J. Zhang, D. Shen, Transformers in medical
image analysis: A review, Intelligent Medicine (2022).
[24] K. S. Kalyan, A. Rajasekharan, S. Sangeetha, Am-
mus: A survey of transformer-based pretrained mod-
els in natural language processing, arXiv preprint
arXiv:2108.05542 (2021).
[25] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung,
S. Gelly, N. Houlsby, Big transfer (bit): General vi-
sual representation learning, in: European conference on
computer vision, Springer, 2020, pp. 491–507.[26] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang,
Q. Tian, M. Wang, Swin-unet: Unet-like pure trans-
former for medical image segmentation, in: Proceedings
of the European Conference on Computer Vision Work-
shops(ECCVW), 2022.
[27] R. Azad, M. Heidari, M. Shariatnia, E. K. Aghdam,
S. Karimijafarbigloo, E. Adeli, D. Merhof, Trans-
DeepLab: Convolution-free transformer-based deeplab
v3+for medical image segmentation, arXiv preprint
arXiv:2208.00713 (2022).
[28] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolu-
tional networks for biomedical image segmentation, in:
International Conference on Medical image computing
and computer-assisted intervention, Springer, 2015, pp.
234–241.
[29] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schro ff, H. Adam,
Encoder-decoder with atrous separable convolution for
semantic image segmentation, in: Proceedings of the Eu-
ropean conference on computer vision (ECCV), 2018,
pp. 801–818.
[30] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding, arXiv preprint arXiv:1810.04805
(2018).
[31] M. Heidari, A. Kazerouni, M. Soltany, R. Azad, E. K.
Aghdam, J. Cohen-Adad, D. Merhof, Hiformer: Hier-
archical multi-scale representations using transformers
for medical image segmentation, in: Proceedings of the
IEEE /CVF Winter Conference on Applications of Com-
puter Vision (WACV), 2023, pp. 6202–6212.
[32] P. Benz, S. Ham, C. Zhang, A. Karjauv, I. S. Kweon,
Adversarial robustness comparison of vision transformer
and mlp-mixer to cnns, arXiv preprint arXiv:2110.02797
(2021).
[33] S. Park, G. Kim, J. Kim, B. Kim, J. C. Ye, Federated
split task-agnostic vision transformer for COVID-19
CXR diagnosis, in: A. Beygelzimer, Y . Dauphin,
P. Liang, J. W. Vaughan (Eds.), Advances in Neural
Information Processing Systems, 2021.
URL https://openreview.net/forum?id=
Ggikq6Tdxch
[34] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wul-
czyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis,
D. Neal, et al., Towards expert-level medical question
answering with large language models, arXiv preprint
arXiv:2305.09617 (2023).
[35] L. Seenivasan, M. Islam, G. Kannan, H. Ren, Sur-
gicalGPT: End-to-end language-vision GPT for vi-
sual question answering in surgery, arXiv preprint
arXiv:2304.09974 (2023).
54[36] X. Gao, Y . Qian, A. Gao, COVID-VIT: Classification of
COVID-19 from CT chest images based on vision trans-
former models, arXiv preprint arXiv:2107.01682 (2021).
[37] S. Yu, K. Ma, Q. Bi, C. Bian, M. Ning, N. He, Y . Li,
H. Liu, Y . Zheng, Mil-vt: Multiple instance learning
enhanced vision transformer for fundus image classi-
fication, in: International Conference on Medical Im-
age Computing and Computer-Assisted Intervention,
Springer, 2021, pp. 45–54.
[38] A. K. Mondal, A. Bhattacharjee, P. Singla, A. Prathosh,
xViTCOS: explainable vision transformer based
COVID-19 screening using radiography, IEEE Journal
of Translational Engineering in Health and Medicine 10
(2021) 1–10.
[39] C. Matsoukas, J. F. Haslum, M. S ¨oderberg, K. Smith,
Is it time to replace cnns with transformers for medical
images?, arXiv preprint arXiv:2108.09038 (2021).
[40] B. Gheflati, H. Rivaz, Vision transformers for classifi-
cation of breast ultrasound images, in: 2022 44th An-
nual International Conference of the IEEE Engineering
in Medicine & Biology Society (EMBC), IEEE, 2022,
pp. 480–483.
[41] D. Shome, T. Kar, S. N. Mohanty, P. Tiwari, K. Muham-
mad, A. AlTameem, Y . Zhang, A. K. J. Saudagar, Covid-
transformer: Interpretable COVID-19 detection using
vision transformer for healthcare, International Journal
of Environmental Research and Public Health 18 (21)
(2021) 11086.
[42] R. Yan, L. Qu, Q. Wei, S.-C. Huang, L. Shen, D. Rubin,
L. Xing, Y . Zhou, Label-e fficient self-supervised feder-
ated learning for tackling data heterogeneity in medical
imaging, IEEE Transactions on Medical Imaging (2023).
[43] S. Perera, S. Adhikari, A. Yilmaz, POCFormer: A
lightweight transformer architecture for detection of
COVID-19 using point of care ultrasound, in: 2021 IEEE
International Conference on Image Processing (ICIP),
IEEE, 2021, pp. 195–199.
[44] M. Bhattacharya, S. Jain, P. Prasanna, Radiotransformer:
a cascaded global-focal transformer for visual attention–
guided disease classification, in: European Conference
on Computer Vision, Springer, 2022, pp. 679–698.
[45] C. Liu, Q. Yin, Automatic diagnosis of COVID-19 us-
ing a tailored transformer-like network, in: Journal of
Physics: Conference Series, V ol. 2010, IOP Publishing,
2021, p. 012175.
[46] Y . Dai, Y . Gao, F. Liu, Transmed: Transformers advance
multi-modal medical image classification, Diagnostics
11 (8) (2021) 1384.
[47] S. Wang, Z. Zhuang, K. Xuan, D. Qian, Z. Xue, J. Xu,
Y . Liu, Y . Chai, L. Zhang, Q. Wang, et al., 3DMET:3D medical image transformer for knee cartilage de-
fect assessment, in: International Workshop on Machine
Learning in Medical Imaging, Springer, 2021, pp. 347–
355.
[48] L. Tanzi, A. Audisio, G. Cirrincione, A. Aprato,
E. Vezzetti, Vision transformer for femur fracture clas-
sification, Injury (2022).
[49] S. Park, G. Kim, Y . Oh, J. B. Seo, S. M. Lee, J. H.
Kim, S. Moon, J.-K. Lim, J. C. Ye, Vision transformer
for COVID-19 CXR diagnosis using chest X-ray feature
corpus, arXiv preprint arXiv:2103.07055 (2021).
[50] R. Sun, Y . Li, T. Zhang, Z. Mao, F. Wu, Y . Zhang,
Lesion-aware transformers for diabetic retinopathy grad-
ing, in: Proceedings of the IEEE /CVF Conference on
Computer Vision and Pattern Recognition, 2021, pp.
10938–10947.
[51] H. Li, F. Yang, Y . Zhao, X. Xing, J. Zhang, M. Gao,
J. Huang, L. Wang, J. Yao, DT-MIL: Deformable trans-
former for multi-instance learning on histopathologi-
cal image, in: International Conference on Medical
Image Computing and Computer-Assisted Intervention,
Springer, 2021, pp. 206–216.
[52] Z. Shao, H. Bian, Y . Chen, Y . Wang, J. Zhang, X. Ji,
et al., Transmil: Transformer based correlated multiple
instance learning for whole slide image classification,
Advances in Neural Information Processing Systems 34
(2021).
[53] S. Mehta, X. Lu, W. Wu, D. Weaver, H. Hajishirzi, J. G.
Elmore, L. G. Shapiro, End-to-end diagnosis of breast
biopsy images with transformers, Medical Image Anal-
ysis 79 (2022) 102466.
[54] Y . Zheng, R. H. Gindra, E. J. Green, E. J. Burks,
M. Betke, J. E. Beane, V . B. Kolachalama, A graph-
transformer for whole slide image classification, IEEE
transactions on medical imaging 41 (11) (2022) 3003–
3015.
[55] X. Huo, G. Sun, S. Tian, Y . Wang, L. Yu, J. Long,
W. Zhang, A. Li, HiFuse: Hierarchical multi-scale fea-
ture fusion network for medical image classification,
arXiv preprint arXiv:2209.10218 (2022).
[56] O. N. Manzari, H. Ahmadabadi, H. Kashiani, S. B.
Shokouhi, A. Ayatollahi, MedViT: A robust vision trans-
former for generalized medical image classification,
Computers in Biology and Medicine 157 (2023) 106791.
[57] A. Aminimehr, A. Molaei, E. Cambria, Entri: Ensem-
ble learning with tri-level representations for explain-
able scene recognition, arXiv preprint arXiv:2307.12442
(2023).
[58] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin,
B. Guo, Swin transformer: Hierarchical vision trans-
former using shifted windows, in: Proceedings of the
55IEEE /CVF International Conference on Computer Vi-
sion, 2021, pp. 10012–10022.
[59] Z. Xia, X. Pan, S. Song, L. E. Li, G. Huang, Vision trans-
former with deformable attention, in: Proceedings of the
IEEE /CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 4794–4803.
[60] M. Fayyaz, S. Abbasi Kouhpayegani, F. Rezaei Jafari,
E. Sommerlade, H. R. Vaezi Joze, H. Pirsiavash, J. Gall,
Adaptive token sampling for e fficient vision transform-
ers, European Conference on Computer Vision (ECCV)
(2022).
[61] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan,
D. Chen, B. Guo, Cswin transformer: A general vision
transformer backbone with cross-shaped windows, in:
Proceedings of the IEEE /CVF Conference on Computer
Vision and Pattern Recognition, 2022, pp. 12124–12134.
[62] W. Li, X. Wang, X. Xia, J. Wu, X. Xiao, M. Zheng,
S. Wen, Sepvit: Separable vision transformer, arXiv
preprint arXiv:2203.15380 (2022).
[63] T. Yao, Y . Li, Y . Pan, Y . Wang, X.-P. Zhang, T. Mei, Dual
vision transformer, IEEE transactions on pattern analysis
and machine intelligence (2023).
[64] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learn-
ing for image recognition, in: Proceedings of the IEEE
conference on computer vision and pattern recognition,
2016, pp. 770–778.
[65] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-
rolles, H. J ´egou, Training data-e fficient image transform-
ers & distillation through attention, in: International
Conference on Machine Learning, PMLR, 2021, pp.
10347–10357.
[66] M. Caron, H. Touvron, I. Misra, H. J ´egou, J. Mairal,
P. Bojanowski, A. Joulin, Emerging properties in self-
supervised vision transformers, in: Proceedings of the
IEEE /CVF International Conference on Computer Vi-
sion, 2021, pp. 9650–9660.
[67] M. H. Yap, G. Pons, J. Mart ´ı, S. Ganau, M. Sentis,
R. Zwiggelaar, A. K. Davison, R. Marti, Automated
breast ultrasound lesions detection using convolutional
neural networks, IEEE journal of biomedical and health
informatics 22 (4) (2017) 1218–1226.
[68] W. Al-Dhabyani, M. Gomaa, H. Khaled, A. Fahmy,
Dataset of breast ultrasound images, Data in brief 28
(2020) 104863.
[69] X. Qi, L. G. Brown, D. J. Foran, J. Nosher, I. Haci-
haliloglu, Chest X-ray image phase features for im-
proved diagnosis of COVID-19 using convolutional neu-
ral network, International journal of computer assisted
radiology and surgery 16 (2) (2021) 197–206.[70] W. El-Shafai, F. Abd El-Samie, Extensive COVID-19 X-
ray and CT chest images dataset, Mendeley data 3 (10)
(2020).
[71] U. Sait, K. Lal, S. Prajapati, R. Bhaumik, T. Kumar,
S. Sanjana, K. Bhalla, Curated dataset for COVID-19
posterior-anterior chest radiography images (X-Rays),
Mendeley Data 1 (2020).
[72] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, D. Batra, Grad-cam: Visual explanations from
deep networks via gradient-based localization, in: Pro-
ceedings of the IEEE international conference on com-
puter vision, 2017, pp. 618–626.
[73] J. Deng, A large-scale hierarchical image database, Proc.
of IEEE Computer Vision and Pattern Recognition, 2009
(2009).
[74] H. Gunraj, COVIDx CT-2A: A large-scale chest CT
dataset for COVID-19 detection (2021).
[75] J. Irvin, P. Rajpurkar, M. Ko, Y . Yu, S. Ciurea-Ilcus,
C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpan-
skaya, et al., Chexpert: A large chest radiograph dataset
with uncertainty labels and expert comparison, in: Pro-
ceedings of the AAAI conference on artificial intelli-
gence, V ol. 33, 2019, pp. 590–597.
[76] H. Chefer, S. Gur, L. Wolf, Transformer interpretabil-
ity beyond attention visualization, in: Proceedings of the
IEEE /CVF Conference on Computer Vision and Pattern
Recognition, 2021, pp. 782–791.
[77] Asia Pacific Tele-Ophthalmology Society, Aptos 2019
blindness detection: Detect diabetic retinopathy to stop
blindness before it’s too late., https://www.kaggle.
com/c/aptos2019-blindness-detection/ (2019).
[78] S. Pachade, P. Porwal, D. Thulkar, M. Kokare, G. Desh-
mukh, V . Sahasrabuddhe, L. Giancardo, G. Quellec,
F. M ´eriaudeau, Retinal fundus multi-disease image
dataset (RFMiD): a dataset for multi-disease detection
research, Data 6 (2) (2021) 14.
[79] D. Kollias, A. Arsenos, L. Soukissian, S. Kollias,
MIA-COV19D: COVID-19 detection through 3-D chest
CT image analysis, in: 2021 IEEE /CVF International
Conference on Computer Vision Workshops (ICCVW),
IEEE, 2021, pp. 537–544.
[80] F. Iandola, M. Moskewicz, S. Karayev, R. Girshick,
T. Darrell, K. Keutzer, Densenet: Implementing ef-
ficient convnet descriptor pyramids, arXiv preprint
arXiv:1404.1869 (2014).
[81] S. Wang, B. Z. Li, M. Khabsa, H. Fang, H. Ma, Lin-
former: Self-attention with linear complexity, arXiv
preprint arXiv:2006.04768 (2020).
56[82] J. Born, G. Br ¨andle, M. Cossio, M. Disdier, J. Goulet,
J. Roulin, N. Wiedemann, POCOVID-Net: auto-
matic detection of COVID-19 from a new lung ul-
trasound imaging dataset (POCUS), arXiv preprint
arXiv:2004.12084 (2020).
[83] L. Yuan, Q. Hou, Z. Jiang, J. Feng, S. Yan, V olo: Vision
outlooker for visual recognition, IEEE transactions on
pattern analysis and machine intelligence 45 (5) (2022)
6575–6586.
[84] M. E. Chowdhury, T. Rahman, A. Khandakar,
R. Mazhar, M. A. Kadir, Z. B. Mahbub, K. R. Islam,
M. S. Khan, A. Iqbal, N. Al Emadi, et al., Can AI help in
screening viral and COVID-19 pneumonia?, IEEE Ac-
cess 8 (2020) 132665–132676.
[85] J. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Duong,
M. Ghassem, COVID-19 image data collection:
Prospective predictions are the future, Machine Learn-
ing for Biomedical Imaging 1 (2020) 1–38. doi:https:
//doi.org/10.59275/j.melba.2020-48g7 .
URL https://melba-journal.org/2020:002
[86] S. Mehta, X. Lu, D. Weaver, J. G. Elmore, H. Ha-
jishirzi, L. Shapiro, Hatnet: an end-to-end holistic at-
tention network for diagnosis of breast biopsy images,
arXiv preprint arXiv:2007.13007 (2020).
[87] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel,
A. Vaswani, Bottleneck transformers for visual recog-
nition, in: Proceedings of the IEEE /CVF conference
on computer vision and pattern recognition, 2021, pp.
16519–16529.
[88] J. Redmon, A. Farhadi, Yolov3: An incremental im-
provement, arXiv preprint arXiv:1804.02767 (2018).
[89] C. Szegedy, V . Vanhoucke, S. Io ffe, J. Shlens, Z. Wo-
jna, Rethinking the inception architecture for computer
vision, in: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2016, pp. 2818–
2826.
[90] L. Tanzi, E. Vezzetti, R. Moreno, A. Aprato, A. Audisio,
A. Mass `e, Hierarchical fracture classification of proxi-
mal femur X-Ray images using a multistage deep learn-
ing approach, European journal of radiology 133 (2020)
109373.
[91] M. Tan, Q. Le, E fficientnet: Rethinking model scaling
for convolutional neural networks, in: International con-
ference on machine learning, PMLR, 2019, pp. 6105–
6114.
[92] World-Health-Organization, Breast cancer,
https://www.who.int/news-room/fact-sheets/
detail/breast-cancer (2021).
[93] M. Y . Lu, D. F. Williamson, T. Y . Chen, R. J. Chen,
M. Barbieri, F. Mahmood, Data-e fficient and weaklysupervised computational pathology on whole-slide im-
ages, Nature biomedical engineering 5 (6) (2021) 555–
570.
[94] Y . Sharma, A. Shrivastava, L. Ehsan, C. A. Moskaluk,
S. Syed, D. Brown, Cluster-to-conquer: A framework for
end-to-end multi-instance learning for whole slide image
classification, in: Medical Imaging with Deep Learning,
PMLR, 2021, pp. 682–698.
[95] N. Naik, A. Madani, A. Esteva, N. S. Keskar, M. F. Press,
D. Ruderman, D. B. Agus, R. Socher, Deep learning-
enabled breast cancer hormonal receptor status determi-
nation from base-level h&e stains, Nature communica-
tions 11 (1) (2020) 1–8.
[96] M. Ilse, J. Tomczak, M. Welling, Attention-based deep
multiple instance learning, in: International conference
on machine learning, PMLR, 2018, pp. 2127–2136.
[97] B. Li, Y . Li, K. W. Eliceiri, Dual-stream multiple in-
stance learning network for whole slide image classifi-
cation with self-supervised contrastive learning, in: Pro-
ceedings of the IEEE /CVF Conference on Computer Vi-
sion and Pattern Recognition, 2021, pp. 14318–14328.
[98] G. Campanella, M. G. Hanna, L. Geneslaw, A. Mi-
raflor, V . Werneck Krauss Silva, K. J. Busam, E. Brogi,
V . E. Reuter, D. S. Klimstra, T. J. Fuchs, Clinical-grade
computational pathology using weakly supervised deep
learning on whole slide images, Nature medicine 25 (8)
(2019) 1301–1309.
[99] W. Li, V .-D. Nguyen, H. Liao, M. Wilder, K. Cheng,
J. Luo, Patch transformer for multi-tagging whole slide
histopathology images, in: International Conference on
Medical Image Computing and Computer-Assisted In-
tervention, Springer, 2019, pp. 532–540.
[100] M. Combalia, N. C. Codella, V . Rotemberg, B. Helba,
V . Vilaplana, O. Reiter, C. Carrera, A. Barreiro, A. C.
Halpern, S. Puig, et al., Bcn20000: Dermoscopic lesions
in the wild, arXiv preprint arXiv:1908.02288 (2019).
[101] R. S. Lee, F. Gimenez, A. Hoogi, K. K. Miyake,
M. Gorovoy, D. L. Rubin, A curated mammography data
set for use in computer-aided detection and diagnosis re-
search, Scientific data 4 (1) (2017) 1–9.
[102] M. D. L. I. Vay ´a, J. M. Saborit, J. A. Montell, A. Pertusa,
A. Bustos, M. Cazorla, J. Galant, X. Barber, D. Orozco-
Beltr ´an, F. Garc ´ıa-Garc ´ıa, et al., BIMCV COVID-19 +:
a large annotated dataset of RX and CT images from
COVID-19 patients, arXiv preprint arXiv:2006.01174
(2020).
[103] A. Signoroni, M. Savardi, S. Benini, N. Adami,
R. Leonardi, P. Gibellini, F. Vaccher, M. Ravanelli,
A. Borghesi, R. Maroldi, et al., BS-Net: Learning
COVID-19 pneumonia severity on a large chest X-ray
dataset, Medical Image Analysis 71 (2021) 102046.
57[104] X. Wang, Y . Peng, L. Lu, Z. Lu, M. Bagheri, R. M. Sum-
mers, Chestx-ray8: Hospital-scale chest X-ray database
and benchmarks on weakly-supervised classification and
localization of common thorax diseases, in: Proceedings
of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 2097–2106.
[105] SIIM-ACR, SIIM-ACR Pneumothorax Seg-
mentation, https://www.kaggle.com/c/
siim-acr-pneumothorax-segmentation (2019).
[106] RSNA, RSNA Pneumonia Detection Chal-
lenge, https://www.kaggle.com/c/
rsna-pneumonia-detection-challenge (2018).
[107] R. S. of North America, RSNA Pneumonia De-
tection Challenge, https://www.kaggle.com/c/
rsna-pneumonia-detection-challenge/ (2018).
[108] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valen-
tim, H. Liang, S. L. Baxter, A. McKeown, G. Yang,
X. Wu, F. Yan, et al., Identifying medical diagnoses and
treatable diseases by image-based deep learning, Cell
172 (5) (2018) 1122–1131.
URL https://www.kaggle.com/datasets/
paultimothymooney/chest-xray-pneumonia
[109] T. Rahman, A. Khandakar, Y . Qiblawey, A. Tahir, S. Ki-
ranyaz, S. B. A. Kashem, M. T. Islam, S. Al Maadeed,
S. M. Zughaier, M. S. Khan, et al., Exploring the e ffect
of image enhancement techniques on COVID-19 detec-
tion using chest X-ray images, Computers in biology and
medicine 132 (2021) 104319.
[110] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q.
Tran, D. B. Nguyen, D. D. Le, C. M. Pham, H. T. Tong,
D. H. Dinh, et al., Vindr-cxr: An open dataset of chest X-
rays with radiologist’s annotations, Scientific Data 9 (1)
(2022) 1–7.
[111] P. Lakhani, J. Mongan, C. Singhal, Q. Zhou, K. P. An-
driole, W. F. Au ffermann, P. Prasanna, T. Pham, M. Pe-
terson, P. J. Bergquist, et al., The 2021 SIIM-FISABIO-
RSNA machine learning COVID-19 challenge: Annota-
tion and standard exam classification of COVID-19 chest
radiographs. (2021).
[112] E. B. Tsai, S. Simpson, M. P. Lungren, M. Hersh-
man, L. Roshkovan, E. Colak, B. J. Erickson, G. Shih,
A. Stein, J. Kalpathy-Cramer, et al., The RSNA interna-
tional COVID-19 open radiology database (RICORD),
Radiology 299 (1) (2021) E204–E213.
[113] E. B. Tsai, S. Simpson, M. P. Lungren, M. Hersh-
man, L. Roshkovan, E. Colak, B. J. Erickson, G. Shih,
A. Stein, J. Kalpathy-Cramer, et al., Data from medical
imaging data resource center (MIDRC) - RSNA interna-
tional COVID radiology database (RICORD) release 1c
- chest X-ray, covid +(MIDRC-RICORD-1c), The Can-
cer Imaging Archive (2021).[114] K. Clark, B. Vendt, K. Smith, J. Freymann, J. Kirby,
P. Koppel, S. Moore, S. Phillips, D. Ma ffitt, M. Pringle,
et al., The cancer imaging archive (TCIA): maintaining
and operating a public information repository, Journal of
digital imaging 26 (6) (2013) 1045–1057.
[115] J. Saltz, M. Saltz, P. Prasanna, R. Mo ffitt, J. Hajagos,
E. Bremer, J. Balsamo, T. Kurc, Stony brook university
COVID-19 positive cases [data set] (2021). doi:10.
7937/TCIA.BBAG-2923 .
[116] EyePACKS, Kaggle diabetic retinopathy detec-
tion competition., https://www.kaggle.com/c/
diabetic-retinopathy-detection (2015).
[117] N. C. Codella, D. Gutman, M. E. Celebi, B. Helba,
M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris,
N. Mishra, H. Kittler, et al., Skin lesion analysis toward
melanoma detection: A challenge at the 2017 interna-
tional symposium on biomedical imaging (isbi), hosted
by the international skin imaging collaboration (isic), in:
2018 IEEE 15th international symposium on biomedical
imaging (ISBI 2018), IEEE, 2018, pp. 168–172.
[118] N. Codella, V . Rotemberg, P. Tschandl, M. E. Celebi,
S. Dusza, D. Gutman, B. Helba, A. Kalloo, K. Li-
opyris, M. Marchetti, et al., Skin lesion analysis to-
ward melanoma detection 2018: A challenge hosted by
the international skin imaging collaboration (isic), arXiv
preprint arXiv:1902.03368 (2019).
[119] V . Rotemberg, N. Kurtansky, B. Betz-Stablein, L. Caf-
fery, E. Chousakos, N. Codella, M. Combalia, S. Dusza,
P. Guitera, D. Gutman, et al., A patient-centric dataset of
images and metadata for identifying melanomas using
clinical context, Scientific data 8 (1) (2021) 34.
[120] B. E. Bejnordi, M. Veta, P. J. Van Diest, B. Van Gin-
neken, N. Karssemeijer, G. Litjens, J. A. Van Der Laak,
M. Hermsen, Q. F. Manson, M. Balkenhol, et al., Di-
agnostic assessment of deep learning algorithms for de-
tection of lymph node metastases in women with breast
cancer, Jama 318 (22) (2017) 2199–2210.
[121] B. Albertina, M. Watson, C. Holback, R. Jarosz, S. Kirk,
Y . Lee, K. Rieger-Christ, J. Lemmerman, The cancer
genome atlas lung adenocarcinoma collection (TCGA-
LUAD) (version 4) [data set] (2016). doi:10.7937/
K9/TCIA.2016.JGNIHEP5 .
[122] S. Kirk, Y . Lee, P. Kumar, J. Filippini, B. Albertina,
M. Watson, K. Rieger-Christ, J. Lemmerman, The can-
cer genome atlas lung squamous cell carcinoma collec-
tion (TCGA-LUSC) (version 4) (2016). doi:10.7937/
K9/TCIA.2016.TYGKKFMQ .
[123] S. AB, V . R, J. C, A. O, K. J, H. E, F. J, S. NI, S. CA,
B. TK, R. DL, O. A, H. MT, S. VR, K. V , S. SG, Ra-
diogenomics of clear cell renal cell carcinoma: Prelimi-
nary findings of the cancer genome atlas-renal cell carci-
noma (tcga-rcc) research group (2014). doi:10.7937/
K9/TCIA.2014.K6M61GDW .
58[124] E. Decenci `ere, X. Zhang, G. Cazuguel, B. Lay, B. Coch-
ener, C. Trone, P. Gain, R. Ordonez, P. Massin,
A. Erginay, et al., Feedback on a publicly distributed im-
age database: the messidor database, Image Analysis &
Stereology 33 (3) (2014) 231–234.
[125] J. Krause, V . Gulshan, E. Rahimy, P. Karth, K. Widner,
G. S. Corrado, L. Peng, D. R. Webster, Grader variability
and the importance of reference standards for evaluating
machine learning models for diabetic retinopathy, Oph-
thalmology 125 (8) (2018) 1264–1272.
[126] N. Bien, P. Rajpurkar, R. L. Ball, J. Irvin, A. Park,
E. Jones, M. Bereket, B. N. Patel, K. W. Yeom, K. Sh-
panskaya, et al., Deep-learning-assisted diagnosis for
knee magnetic resonance imaging: development and ret-
rospective validation of MRNet, PLoS medicine 15 (11)
(2018) e1002699.
[127] N. L. S. T. R. Team, Reduced lung-cancer mortality with
low-dose computed tomographic screening, New Eng-
land Journal of Medicine 365 (5) (2011) 395–409.
[128] N. J. Edwards, M. Oberti, R. R. Thangudu, S. Cai, P. B.
McGarvey, S. Jacob, S. Madhavan, K. A. Ketchum, The
CPTAC data portal: a resource for cancer proteomics
research, Journal of proteome research 14 (6) (2015)
2707–2713.
[129] N. I. of Health, et al., National cancer institute. the can-
cer genome atlas program (2019).
[130] J. G. Elmore, G. M. Longton, P. A. Carney, B. M.
Geller, T. Onega, A. N. Tosteson, H. D. Nelson, M. S.
Pepe, K. H. Allison, S. J. Schnitt, et al., Diagnostic con-
cordance among pathologists interpreting breast biopsy
specimens, Jama 313 (11) (2015) 1122–1132.
[131] X. He, X. Yang, S. Zhang, J. Zhao, Y . Zhang, E. Xing,
P. Xie, Sample-e fficient deep learning for COVID-19 di-
agnosis based on CT scans, medrxiv (2020) 2020–04.
[132] K. Pogorelov, K. R. Randel, C. Griwodz, S. L. Eskeland,
T. de Lange, D. Johansen, C. Spampinato, D.-T. Dang-
Nguyen, M. Lux, P. T. Schmidt, et al., Kvasir: A multi-
class image dataset for computer aided gastrointestinal
disease detection, in: Proceedings of the 8th ACM on
Multimedia Systems Conference, 2017, pp. 164–169.
[133] J. Yang, R. Shi, B. Ni, Medmnist classification de-
cathlon: A lightweight automl benchmark for medical
image analysis, in: 2021 IEEE 18th International Sym-
posium on Biomedical Imaging (ISBI), IEEE, 2021, pp.
191–195.
[134] F. M. Bianchi, D. Grattarola, C. Alippi, Spectral clus-
tering with graph neural networks for graph pooling, in:
International Conference on Machine Learning, PMLR,
2020, pp. 874–883.[135] Z. Zhong, L. Zheng, S. Li, Y . Yang, Generalizing a per-
son retrieval model hetero-and homogeneously, in: Pro-
ceedings of the European conference on computer vision
(ECCV), 2018, pp. 172–188.
[136] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba,
Learning deep features for discriminative localization,
in: Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, 2016, pp. 2921–2929.
[137] S.-C. Huang, A. Pareek, M. Jensen, M. P. Lungren,
S. Yeung, A. S. Chaudhari, Self-supervised learning for
medical image classification: a systematic review and
implementation guidelines, NPJ Digital Medicine 6 (1)
(2023) 74.
[138] X. Chen, S. Xie, K. He, An empirical study of train-
ing self-supervised vision transformers, in: Proceedings
of the IEEE /CVF International Conference on Computer
Vision, 2021, pp. 9640–9649.
[139] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, G. E.
Hinton, Big self-supervised models are strong semi-
supervised learners, Advances in neural information pro-
cessing systems 33 (2020) 22243–22255.
[140] J.-B. Grill, F. Strub, F. Altch ´e, C. Tallec, P. Richemond,
E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo,
M. Gheshlaghi Azar, et al., Bootstrap your own latent-
a new approach to self-supervised learning, Advances in
neural information processing systems 33 (2020) 21271–
21284.
[141] V . Ljosa, K. L. Sokolnicki, A. E. Carpenter, Annotated
high-throughput microscopy image sets for validation.,
Nature methods 9 (7) (2012) 637–637.
[142] Z. Peng, W. Huang, S. Gu, L. Xie, Y . Wang, J. Jiao,
Q. Ye, Conformer: Local features coupling global repre-
sentations for visual recognition, in: Proceedings of the
IEEE /CVF international conference on computer vision,
2021, pp. 367–376.
[143] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei,
H. Xia, C. Shen, Twins: Revisiting the design of spa-
tial attention in vision transformers, Advances in Neural
Information Processing Systems 34 (2021) 9355–9366.
[144] W. Yu, M. Luo, P. Zhou, C. Si, Y . Zhou, X. Wang,
J. Feng, S. Yan, Metaformer is actually what you need
for vision, in: Proceedings of the IEEE /CVF conference
on computer vision and pattern recognition, 2022, pp.
10819–10829.
[145] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan,
L. Zhang, Cvt: Introducing convolutions to vision trans-
formers, in: Proceedings of the IEEE /CVF International
Conference on Computer Vision, 2021, pp. 22–31.
[146] X. Mao, G. Qi, Y . Chen, X. Li, R. Duan, S. Ye, Y . He,
H. Xue, Towards robust vision transformer, in: Proceed-
ings of the IEEE /CVF Conference on Computer Vision
and Pattern Recognition, 2022, pp. 12042–12051.
59[147] W. Xu, Y . Xu, T. Chang, Z. Tu, Co-scale conv-attentional
image transformers, in: Proceedings of the IEEE /CVF
International Conference on Computer Vision, 2021, pp.
9981–9990.
[148] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, S. J. Oh,
Rethinking spatial dimensions of vision transformers, in:
Proceedings of the IEEE /CVF International Conference
on Computer Vision, 2021, pp. 11936–11945.
[149] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang,
T. Lu, P. Luo, L. Shao, Pyramid vision transformer: A
versatile backbone for dense prediction without convo-
lutions, in: Proceedings of the IEEE /CVF international
conference on computer vision, 2021, pp. 568–578.
[150] W. H. Pinaya, P.-D. Tudosiu, J. Da fflon, P. F. Da Costa,
V . Fernandez, P. Nachev, S. Ourselin, M. J. Cardoso,
Brain imaging generation with latent di ffusion models,
in: MICCAI Workshop on Deep Generative Models,
Springer, 2022, pp. 117–126.
[151] P. A. Moghadam, S. Van Dalen, K. C. Martin,
J. Lennerz, S. Yip, H. Farahani, A. Bashashati, A
morphology focused di ffusion probabilistic model for
synthesis of histopathology images, arXiv preprint
arXiv:2209.13167 (2022).
[152] A. Kazerouni, E. K. Aghdam, M. Heidari, R. Azad,
M. Fayyaz, I. Hacihaliloglu, D. Merhof, Di ffusion mod-
els in medical imaging: A comprehensive survey, Medi-
cal Image Analysis (2023) 102846.
[153] H.-Y . Zhou, J. Guo, Y . Zhang, L. Yu, L. Wang, Y . Yu, nn-
former: Interleaved transformer for volumetric segmen-
tation, arXiv preprint arXiv:2109.03201 (2021).
[154] X. Huang, Z. Deng, D. Li, X. Yuan, Y . Fu, Missformer:
An effective transformer for 2d medical image segmen-
tation, IEEE Transactions on Medical Imaging (2022) 1–
1doi:10.1109/TMI.2022.3230943 .
[155] S. Li, X. Sui, X. Luo, X. Xu, L. Yong, R. S. M.
Goh, Medical image segmentation using squeeze-and-
expansion transformers, in: The 30th International Joint
Conference on Artificial Intelligence (IJCAI), 2021.
[156] W. Wang, C. Chen, M. Ding, H. Yu, S. Zha, J. Li,
Transbts: Multimodal brain tumor segmentation using
transformer, in: International Conference on Medical
Image Computing and Computer-Assisted Intervention,
Springer, 2021, pp. 109–119.
[157] Y . Zhang, H. Liu, Q. Hu, Transfuse: Fusing transform-
ers and cnns for medical image segmentation, in: Inter-
national Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2021, pp. 14–
24.
[158] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, V . M. Pa-
tel, Medical transformer: Gated axial-attention for medi-
cal image segmentation, in: International Conference onMedical Image Computing and Computer-Assisted In-
tervention, Springer, 2021, pp. 36–46.
[159] A. Hatamizadeh, Y . Tang, V . Nath, D. Yang, A. Myro-
nenko, B. Landman, H. R. Roth, D. Xu, Unetr: Trans-
formers for 3D medical image segmentation, in: Pro-
ceedings of the IEEE /CVF winter conference on appli-
cations of computer vision, 2022, pp. 574–584.
[160] A. Hatamizadeh, V . Nath, Y . Tang, D. Yang, H. R. Roth,
D. Xu, Swin unetr: Swin transformers for semantic seg-
mentation of brain tumors in MRI images, in: Brainle-
sion: Glioma, Multiple Sclerosis, Stroke and Traumatic
Brain Injuries: 7th International Workshop, BrainLes
2021, Held in Conjunction with MICCAI 2021, Virtual
Event, September 27, 2021, Revised Selected Papers,
Part I, Springer, 2022, pp. 272–284.
[161] Y . Tang, D. Yang, W. Li, H. R. Roth, B. Landman, D. Xu,
V . Nath, A. Hatamizadeh, Self-supervised pre-training
of swin transformers for 3D medical image analysis, in:
Proceedings of the IEEE /CVF Conference on Computer
Vision and Pattern Recognition, 2022, pp. 20730–20740.
[162] Y . Xie, J. Zhang, C. Shen, Y . Xia, CoTr: E fficiently
bridging CNN and transformer for 3D medical image
segmentation, arXiv preprint arXiv:2103.03024 (2021).
[163] D. Yang, A. Myronenko, X. Wang, Z. Xu, H. R. Roth,
D. Xu, T-AutoML: Automated machine learning for
lesion segmentation using transformers in 3D medical
imaging, in: Proceedings of the IEEE /CVF International
Conference on Computer Vision, 2021, pp. 3962–3974.
[164] X. Luo, M. Hu, T. Song, G. Wang, S. Zhang, Semi-
supervised medical image segmentation via cross teach-
ing between CNN and transformer, in: International
Conference on Medical Imaging with Deep Learning,
PMLR, 2022, pp. 820–833.
[165] L. Zhou, H. Liu, J. Bae, J. He, D. Samaras, P. Prasanna,
Self pre-training with masked autoencoders for med-
ical image analysis, arXiv preprint arXiv:2203.05573
(2022).
[166] R. Azad, E. K. Aghdam, A. Rauland, Y . Jia, A. H.
Avval, A. Bozorgpour, S. Karimijafarbigloo, J. P. Co-
hen, E. Adeli, D. Merhof, Medical image segmen-
tation review: The success of U-Net, arXiv preprint
arXiv:2211.14830 (2022).
[167] M. Asadi-Aghbolaghi, R. Azad, M. Fathy, S. Es-
calera, Multi-level context gating of embedded collec-
tive knowledge for medical image segmentation, arXiv
preprint arXiv:2003.05056 (2020).
[168] S. Karimijafarbigloo, R. Azad, A. Kazerouni, S. Ebadol-
lahi, D. Merhof, MMCFormer: Missing modality com-
pensation transformer for brain tumor segmentation, in:
Medical Imaging with Deep Learning, 2023.
60[169] E. K. Aghdam, R. Azad, M. Zarvani, D. Merhof, At-
tention swin u-net: Cross-contextual attention mech-
anism for skin lesion segmentation, arXiv preprint
arXiv:2210.16898 (2022).
[170] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak,
A. Klein, Miccai multi-atlas labeling beyond the cranial
vault–workshop and challenge, in: Proc. MICCAI Multi-
Atlas Labeling Beyond Cranial Vault—Workshop Chal-
lenge, V ol. 5, 2015, p. 12.
[171] M. Nolden, S. Zelzer, A. Seitel, D. Wald, M. M ¨uller,
A. M. Franz, D. Maleike, M. Fangerau, M. Baumhauer,
L. Maier-Hein, et al., The medical imaging interaction
toolkit: challenges and advances, International journal
of computer assisted radiology and surgery 8 (4) (2013)
607–620.
[172] R. Azad, M. T. Al-Antary, M. Heidari, D. Merhof,
Transnorm: Transformer provides a strong spatial nor-
malization mechanism for a deep segmentation model,
IEEE Access 10 (2022) 108205–108215.
[173] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Hein-
rich, K. Misawa, K. Mori, S. McDonagh, N. Y .
Hammerla, B. Kainz, B. Glocker, D. Rueckert, Atten-
tion U-Net: Learning where to look for the pancreas, in:
Medical Imaging with Deep Learning, 2018.
URL https://openreview.net/forum?id=
Skft7cijM
[174] F. Isensee, P. F. J ¨ager, P. M. Full, P. V ollmuth, K. H.
Maier-Hein, nnU-net for brain tumor segmentation, in:
International MICCAI Brainlesion Workshop, Springer,
2020, pp. 118–132.
[175] F. Liu, X. Ren, Z. Zhang, X. Sun, Y . Zou, Rethinking
skip connection with layer normalization in transformers
and resnets, arXiv preprint arXiv:2105.07205 (2021).
[176] L.-C. Chen, G. Papandreou, F. Schro ff, H. Adam, Re-
thinking atrous convolution for semantic image segmen-
tation, arXiv preprint arXiv:1706.05587 (2017).
[177] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Hein-
rich, K. Misawa, K. Mori, S. McDonagh, N. Y .
Hammerla, B. Kainz, et al., Attention u-net: Learn-
ing where to look for the pancreas, arXiv preprint
arXiv:1804.03999 (2018).
[178] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang,
Y . Fu, J. Feng, T. Xiang, P. H. Torr, et al., Rethink-
ing semantic segmentation from a sequence-to-sequence
perspective with transformers, in: Proceedings of the
IEEE /CVF conference on computer vision and pattern
recognition, 2021, pp. 6881–6890.
[179] S. Woo, J. Park, J.-Y . Lee, I. S. Kweon, Cbam: Convo-
lutional block attention module, in: Proceedings of the
European conference on computer vision (ECCV), 2018,
pp. 3–19.[180] J. Schlemper, O. Oktay, M. Schaap, M. Heinrich,
B. Kainz, B. Glocker, D. Rueckert, Attention gated net-
works: Learning to leverage salient regions in medical
images, Medical image analysis 53 (2019) 197–207.
[181] H. Wang, Y . Zhu, B. Green, H. Adam, A. Yuille, L.-
C. Chen, Axial-deeplab: Stand-alone axial-attention for
panoptic segmentation, in: European Conference on
Computer Vision, Springer, 2020, pp. 108–126.
[182] F. Isensee, P. F. Jaeger, S. A. A. Kohl, J. Petersen, K. H.
Maier-Hein, nnu-net: a self-configuring method for deep
learning-based biomedical image segmentation, Nature
Methods 18 (2) (2021) 203– +.
URL <GotoISI>://WOS:000599000100001
[183] Y . Tang, R. Gao, H. H. Lee, S. Han, Y . Chen, D. Gao,
V . Nath, C. Bermudez, M. R. Savona, R. G. Abramson,
et al., High-resolution 3d abdominal segmentation with
random patch network fusion, Medical Image Analysis
69 (2021) 101894.
[184] Y . Zhou, Z. Li, S. Bai, C. Wang, X. Chen, M. Han,
E. Fishman, A. L. Yuille, Prior-aware neural network for
partially-supervised multi-organ segmentation, in: Pro-
ceedings of the IEEE /CVF International Conference on
Computer Vision, 2019, pp. 10672–10681.
[185] A. Myronenko, 3D MRI brain tumor segmentation using
autoencoder regularization, in: International MICCAI
Brainlesion Workshop, Springer, 2018, pp. 311–320.
[186] S. Liu, L. Qi, H. Qin, J. Shi, J. Jia, Path aggregation net-
work for instance segmentation, in: Proceedings of the
IEEE conference on computer vision and pattern recog-
nition, 2018, pp. 8759–8768.
[187] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kir-
illov, S. Zagoruyko, End-to-end object detection with
transformers, in: European conference on computer vi-
sion, Springer, 2020, pp. 213–229.
[188] J. Lee, Y . Lee, J. Kim, A. Kosiorek, S. Choi, Y . W.
Teh, Set transformer: A framework for attention-based
permutation-invariant neural networks, in: International
Conference on Machine Learning, PMLR, 2019, pp.
3744–3753.
[189] A. Gupta, S. Gehlot, S. Goswami, S. Motwani, R. Gupta,
´A. G. Faura, D. ˇStepec, T. Martin ˇciˇc, R. Azad, D. Mer-
hof, et al., SegPC-2021: A challenge & dataset on seg-
mentation of multiple myeloma plasma cells from mi-
croscopic images, Medical Image Analysis 83 (2023)
102677.
[190] X. Du, T.-Y . Lin, P. Jin, G. Ghiasi, M. Tan, Y . Cui, Q. V .
Le, X. Song, Spinenet: Learning scale-permuted back-
bone for recognition and localization, in: Proceedings of
the IEEE /CVF conference on computer vision and pat-
tern recognition, 2020, pp. 11592–11601.
61[191] C. Liu, L.-C. Chen, F. Schro ff, H. Adam, W. Hua, A. L.
Yuille, L. Fei-Fei, Auto-deeplab: Hierarchical neural ar-
chitecture search for semantic image segmentation, in:
Proceedings of the IEEE /CVF conference on computer
vision and pattern recognition, 2019, pp. 82–92.
[192] W. Bae, S. Lee, Y . Lee, B. Park, M. Chung, K.-H.
Jung, Resource optimized neural architecture search for
3D medical image segmentation, in: International Con-
ference on Medical Image Computing and Computer-
Assisted Intervention, Springer, 2019, pp. 228–236.
[193] S. Kim, I. Kim, S. Lim, W. Baek, C. Kim, H. Cho,
B. Yoon, T. Kim, Scalable neural architecture search for
3D medical image segmentation, in: International Con-
ference on Medical Image Computing and Computer-
Assisted Intervention, Springer, 2019, pp. 220–228.
[194] S. Qiao, W. Shen, Z. Zhang, B. Wang, A. Yuille, Deep
co-training for semi-supervised image recognition, in:
Proceedings of the european conference on computer vi-
sion (eccv), 2018, pp. 135–152.
[195] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang,
M. Sugiyama, Co-teaching: Robust training of deep neu-
ral networks with extremely noisy labels, Advances in
neural information processing systems 31 (2018).
[196] X. Chen, Y . Yuan, G. Zeng, J. Wang, Semi-supervised
semantic segmentation with cross pseudo supervision,
in: Proceedings of the IEEE /CVF Conference on Com-
puter Vision and Pattern Recognition, 2021, pp. 2613–
2622.
[197] L. Yu, S. Wang, X. Li, C.-W. Fu, P.-A. Heng,
Uncertainty-aware self-ensembling model for semi-
supervised 3D left atrium segmentation, in: Interna-
tional Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2019, pp.
605–613.
[198] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky,
X. Yang, P.-A. Heng, I. Cetin, K. Lekadir, O. Camara,
M. A. G. Ballester, et al., Deep learning techniques
for automatic MRI cardiac multi-structures segmentation
and diagnosis: is the problem solved?, IEEE transactions
on medical imaging 37 (11) (2018) 2514–2525.
[199] T. Mendonc ¸a, P. M. Ferreira, J. S. Marques, A. R. S.
Marcal, J. Rozeira, Ph2 - a dermoscopic image database
for research and benchmarking, in: 2013 35th An-
nual International Conference of the IEEE Engineering
in Medicine and Biology Society (EMBC), 2013, pp.
5437–5440. doi:10.1109/EMBC.2013.6610779 .
[200] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer,
K. Farahani, J. Kirby, Y . Burren, N. Porz, J. Slotboom,
R. Wiest, et al., The multimodal brain tumor image seg-
mentation benchmark (BRATS), IEEE transactions on
medical imaging 34 (10) (2014) 1993–2024.[201] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki,
J. S. Kirby, J. B. Freymann, K. Farahani, C. Davatzikos,
Advancing the cancer genome atlas glioma MRI collec-
tions with expert segmentation labels and radiomic fea-
tures, Scientific data 4 (1) (2017) 1–13.
[202] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler,
A. Crimi, R. T. Shinohara, C. Berger, S. M. Ha, M. Rozy-
cki, et al., Identifying the best machine learning algo-
rithms for brain tumor segmentation, progression assess-
ment, and overall survival prediction in the brats chal-
lenge, arXiv preprint arXiv:1811.02629 (2018).
[203] K. Sirinukunwattana, J. P. Pluim, H. Chen, X. Qi, P.-
A. Heng, Y . B. Guo, L. Y . Wang, B. J. Matuszewski,
E. Bruni, U. Sanchez, et al., Gland segmentation in colon
histology images: The glas challenge contest, Medical
image analysis 35 (2017) 489–502.
[204] N. Kumar, R. Verma, S. Sharma, S. Bhargava, A. Va-
hadane, A. Sethi, A dataset and a technique for gen-
eralized nuclear segmentation for computational pathol-
ogy, IEEE transactions on medical imaging 36 (7) (2017)
1550–1560.
[205] A. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Fara-
hani, B. Van Ginneken, A. Kopp-Schneider, B. Land-
man, G. Litjens, B. Menze, et al., A large annotated med-
ical image dataset for the development and evaluation
of segmentation algorithms. arxiv 2019, arXiv preprint
arXiv:1902.09063.
[206] U. Baid, S. Ghodasara, S. Mohan, M. Bilello, E. Cal-
abrese, E. Colak, K. Farahani, J. Kalpathy-Cramer, F. C.
Kitamura, S. Pati, et al., The rsna-asnr-miccai brats
2021 benchmark on brain tumor segmentation and radio-
genomic classification, arXiv preprint arXiv:2107.02314
(2021).
[207] A. Gupta, P. Mallick, O. Sharma, R. Gupta, R. Dug-
gal, PCSeg: Color model driven probabilistic multiphase
level set based tool for plasma cell segmentation in mul-
tiple myeloma, PloS one 13 (12) (2018) e0207908.
[208] A. Gupta, R. Duggal, S. Gehlot, R. Gupta, A. Man-
gal, L. Kumar, N. Thakkar, D. Satpathy, GCTI-SN:
Geometry-inspired chemical and tissue invariant stain
normalization of microscopic medical images, Medical
Image Analysis 65 (2020) 101788.
[209] S. Gehlot, A. Gupta, R. Gupta, Ednfc-net: Convolutional
neural network with nested feature concatenation for
nuclei-instance segmentation, in: ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), IEEE, 2020, pp. 1389–
1393.
[210] J. I. Orlando, H. Fu, J. B. Breda, K. van Keer, D. R.
Bathula, A. Diaz-Pinto, R. Fang, P.-A. Heng, J. Kim,
J. Lee, et al., Refuge challenge: A unified framework
62for evaluating automated methods for glaucoma assess-
ment from fundus photographs, Medical image analysis
59 (2020) 101570.
[211] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen,
L. Shao, Pranet: Parallel reverse attention network for
polyp segmentation, in: International conference on
medical image computing and computer-assisted inter-
vention, Springer, 2020, pp. 263–273.
[212] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. d.
Lange, D. Johansen, H. D. Johansen, Kvasir-seg: A seg-
mented polyp dataset, in: International Conference on
Multimedia Modeling, Springer, 2020, pp. 451–462.
[213] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello,
K. Farahani, B. Van Ginneken, A. Kopp-Schneider, B. A.
Landman, G. Litjens, B. Menze, et al., A large anno-
tated medical image dataset for the development and
evaluation of segmentation algorithms, arXiv preprint
arXiv:1902.09063 (2019).
[214] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy,
A. L. Yuille, Deeplab: Semantic image segmentation
with deep convolutional nets, atrous convolution, and
fully connected crfs, IEEE transactions on pattern anal-
ysis and machine intelligence 40 (4) (2017) 834–848.
[215] H. Bao, L. Dong, S. Piao, F. Wei, BEit: BERT
pre-training of image transformers, in: International
Conference on Learning Representations, 2022.
URL https://openreview.net/forum?id=
p-BhZSz59o4
[216] Z. Xie, Z. Zhang, Y . Cao, Y . Lin, J. Bao, Z. Yao, Q. Dai,
H. Hu, Simmim: A simple framework for masked image
modeling, in: Proceedings of the IEEE /CVF Conference
on Computer Vision and Pattern Recognition, 2022, pp.
9653–9663.
[217] A. El-Nouby, G. Izacard, H. Touvron, I. Laptev,
H. Jegou, E. Grave, Are large-scale datasets neces-
sary for self-supervised pre-training?, arXiv preprint
arXiv:2112.10740 (2021).
[218] K. He, X. Chen, S. Xie, Y . Li, P. Doll ´ar, R. Girshick,
Masked autoencoders are scalable vision learners, in:
Proceedings of the IEEE /CVF Conference on Computer
Vision and Pattern Recognition, 2022, pp. 16000–16009.
[219] M. M. Rahman, R. Marculescu, Medical image segmen-
tation via cascaded attention decoding, in: Proceedings
of the IEEE /CVF Winter Conference on Applications of
Computer Vision, 2023, pp. 6222–6231.
[220] R. Azad, R. Arimond, E. K. Aghdam, A. Kazerouni,
D. Merhof, DAE-Former: Dual attention-guided e ffi-
cient transformer for medical image segmentation, arXiv
preprint arXiv:2212.13504 (2022).[221] H. Huang, S. Xie, L. Lin, Y . Iwamoto, X. Han,
Y .-W. Chen, R. Tong, Scaleformer: Revisiting the
transformer-based backbones from a scale-wise perspec-
tive for medical image segmentation, arXiv preprint
arXiv:2207.14552 (2022).
[222] R. Azad, L. Niggemeier, M. Huttemann, A. Kazerouni,
E. K. Aghdam, Y . Velichko, U. Bagci, D. Merhof, Be-
yond self-attention: Deformable large kernel attention
for medical image segmentation (2023). arXiv:2309.
00121 .
[223] M. M. Rahman, R. Marculescu, Multi-scale hierarchical
vision transformer with cascaded attention decoding for
medical image segmentation, in: Medical Imaging with
Deep Learning, 2023.
URL https://openreview.net/forum?id=
u0MHV19E2n
[224] T. Mendonc ¸a, P. M. Ferreira, J. S. Marques, A. R. Mar-
cal, J. Rozeira, PH 2-A dermoscopic image database for
research and benchmarking, in: 2013 35th annual inter-
national conference of the IEEE engineering in medicine
and biology society (EMBC), IEEE, 2013, pp. 5437–
5440.
[225] Z. Zhang, L. Yu, X. Liang, W. Zhao, L. Xing, Tran-
sCT: dual-path transformer for low dose computed to-
mography, in: Medical Image Computing and Computer
Assisted Intervention–MICCAI 2021: 24th International
Conference, Strasbourg, France, September 27–October
1, 2021, Proceedings, Part VI 24, Springer, 2021, pp.
55–64.
[226] D. Wang, Z. Wu, H. Yu, TED-net: Convolution-free T2T
vision transformer-based encoder-decoder dilation net-
work for low-dose CT denoising, in: Machine Learn-
ing in Medical Imaging: 12th International Workshop,
MLMI 2021, Held in Conjunction with MICCAI 2021,
Strasbourg, France, September 27, 2021, Proceedings
12, Springer, 2021, pp. 416–425.
[227] A. Luthra, H. Sulakhe, T. Mittal, A. Iyer, S. Yadav,
Eformer: Edge enhancement based transformer for med-
ical image denoising, arXiv preprint arXiv:2109.08044
(2021).
[228] Y . Luo, Y . Wang, C. Zu, B. Zhan, X. Wu, J. Zhou,
D. Shen, L. Zhou, 3D transformer-GAN for high-quality
PET reconstruction, in: International Conference on
Medical Image Computing and Computer-Assisted In-
tervention, Springer, 2021, pp. 276–285.
[229] L. Zhang, Z. Xiao, C. Zhou, J. Yuan, Q. He, Y . Yang,
X. Liu, D. Liang, H. Zheng, W. Fan, et al., Spatial adap-
tive and transformer fusion network (STFNet) for low-
count PET blind denoising with MRI, Medical Physics
49 (1) (2022) 343–356.
63[230] D. Wang, F. Fan, Z. Wu, R. Liu, F. Wang, H. Yu,
CTformer: convolution-free token2token dilated vi-
sion transformer for low-dose CT denoising, Physics in
Medicine & Biology 68 (6) (2023) 065012.
[231] L. Yang, D. Zhang, et al., Low-dose CT denoising
via sinogram inner-structure transformer, arXiv preprint
arXiv:2204.03163 (2022).
[232] C. Wang, K. Shang, H. Zhang, Q. Li, Y . Hui, S. K. Zhou,
Dudotrans: Dual-domain transformer provides more at-
tention for sinogram restoration in sparse-view CT re-
construction, arXiv preprint arXiv:2111.10790 (2021).
[233] T.-O. Buchholz, F. Jug, Fourier image transformer, in:
Proceedings of the IEEE /CVF Conference on Computer
Vision and Pattern Recognition, 2022, pp. 1846–1854.
[234] C. Shi, Y . Xiao, Z. Chen, Dual-domain sparse-view CT
reconstruction with transformers, Physica Medica 101
(2022) 1–7.
[235] M. Wu, Y . Xu, Y . Xu, G. Wu, Q. Chen, H. Lin, Adap-
tively re-weighting multi-loss untrained transformer
for sparse-view cone-beam CT reconstruction, arXiv
preprint arXiv:2203.12476 (2022).
[236] K. Lin, R. Heckel, Vision transformers enable fast and
robust accelerated MRI, in: Medical Imaging with Deep
Learning, 2021.
[237] C.-M. Feng, Y . Yan, H. Fu, L. Chen, Y . Xu, Task
transformer network for joint MRI reconstruction and
super-resolution, in: International Conference on Med-
ical Image Computing and Computer-Assisted Interven-
tion, Springer, 2021, pp. 307–317.
[238] D. Mahapatra, Z. Ge, MR image super resolution by
combining feature disentanglement CNNs and vision
transformers, in: Medical Imaging with Deep Learning,
2021.
[239] C. Fang, D. Zhang, L. Wang, Y . Zhang, L. Cheng,
J. Han, Cross-modality high-frequency transformer for
MR image super-resolution, in: Proceedings of the 30th
ACM International Conference on Multimedia, 2022,
pp. 1584–1592.
[240] Y . Korkmaz, S. U. Dar, M. Yurt, M. ¨Ozbey, T. Cukur,
Unsupervised MRI reconstruction via zero-shot learned
adversarial transformers, IEEE Transactions on Medical
Imaging 41 (7) (2022) 1747–1763.
[241] B. Zhou, N. Dey, J. Schlemper, S. S. M. Salehi, C. Liu,
J. S. Duncan, M. Sofka, DSFormer: a dual-domain self-
supervised transformer for accelerated multi-contrast
MRI reconstruction, in: Proceedings of the IEEE /CVF
Winter Conference on Applications of Computer Vision,
2023, pp. 4966–4975.
[242] E. Seeram, Computed Tomography-E-Book: Physical
Principles, Clinical Applications, and Quality Control,
Elsevier Health Sciences, 2015.[243] J. P. Mathews, Q. P. Campbell, H. Xu, P. Halleck, A re-
view of the application of X-ray computed tomography
to the study of coal, Fuel 209 (2017) 10–24.
[244] D. J. Brenner, E. J. Hall, Computed tomography—an
increasing source of radiation exposure, New England
journal of medicine 357 (22) (2007) 2277–2284.
[245] C. M. Hyun, H. P. Kim, S. M. Lee, S. Lee, J. K. Seo,
Deep learning for undersampled MRI reconstruction,
Physics in Medicine & Biology 63 (13) (2018) 135007.
[246] H.-M. Zhang, B. Dong, A review on deep learning in
medical image reconstruction, Journal of the Operations
Research Society of China 8 (2) (2020) 311–340.
[247] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z.-H. Jiang,
F. E. Tay, J. Feng, S. Yan, Tokens-to-token vit: Training
vision transformers from scratch on imagenet, in: Pro-
ceedings of the IEEE /CVF International Conference on
Computer Vision, 2021, pp. 558–567.
[248] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, H. Li,
Uformer: A general u-shaped transformer for image
restoration, in: Proceedings of the IEEE /CVF Confer-
ence on Computer Vision and Pattern Recognition, 2022,
pp. 17683–17693.
[249] A. Buades, B. Coll, J.-M. Morel, A non-local algorithm
for image denoising, in: 2005 IEEE computer society
conference on computer vision and pattern recognition
(CVPR’05), V ol. 2, Ieee, 2005, pp. 60–65.
[250] K. Dabov, A. Foi, V . Katkovnik, K. Egiazarian, Im-
age denoising with block-matching and 3D filtering, in:
Image processing: algorithms and systems, neural net-
works, and machine learning, V ol. 6064, SPIE, 2006, pp.
354–365.
[251] C. Wang, Z. Hu, P. Shi, H. Liu, Low dose PET recon-
struction with total variation regularization, in: 2014
36th Annual International Conference of the IEEE En-
gineering in Medicine and Biology Society, IEEE, 2014,
pp. 1917–1920.
[252] C. H. McCollough, A. C. Bartley, R. E. Carter, B. Chen,
T. A. Drees, P. Edwards, D. R. Holmes III, A. E. Huang,
F. Khan, S. Leng, et al., Low-dose CT for the detection
and classification of metastatic liver lesions: results of
the 2016 low dose CT grand challenge, Medical physics
44 (10) (2017) e339–e352.
[253] J. Bian, J. H. Siewerdsen, X. Han, E. Y . Sidky, J. L.
Prince, C. A. Pelizzari, X. Pan, Evaluation of sparse-
view reconstruction from flat-panel-detector cone-beam
CT, Physics in Medicine & Biology 55 (22) (2010) 6575.
[254] Y . Han, J. C. Ye, Framing U-Net via deep convolutional
framelets: Application to sparse-view ct, IEEE transac-
tions on medical imaging 37 (6) (2018) 1418–1429.
64[255] A. C. Kak, M. Slaney, Principles of computerized tomo-
graphic imaging, SIAM, 2001.
[256] X. Pan, E. Y . Sidky, M. Vannier, Why do commer-
cial CT scanners still employ traditional, filtered back-
projection for image reconstruction?, Inverse problems
25 (12) (2009) 123009.
[257] L. A. Feldkamp, L. C. Davis, J. W. Kress, Practical cone-
beam algorithm, Josa a 1 (6) (1984) 612–619.
[258] S. Patel, J. Brown, T. Pimentel, R. Kelly, F. Abella,
C. Durack, Cone beam computed tomography in
endodontics–a review of the literature, International en-
dodontic journal 52 (8) (2019) 1138–1152.
[259] D. Ulyanov, A. Vedaldi, V . Lempitsky, Deep image prior,
in: Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, 2018, pp. 9446–9454.
[260] J. Johnson, A. Alahi, L. Fei-Fei, Perceptual losses for
real-time style transfer and super-resolution, in: Euro-
pean conference on computer vision, Springer, 2016, pp.
694–711.
[261] J. Leuschner, M. Schmidt, D. O. Baguer, P. Maaß,
The lodopab-ct dataset: A benchmark dataset for
low-dose CT reconstruction methods, arXiv preprint
arXiv:1910.01113 (2019).
[262] J. Zbontar, F. Knoll, A. Sriram, T. Murrell, Z. Huang,
M. J. Muckley, A. Defazio, R. Stern, P. Johnson,
M. Bruno, M. Parente, K. J. Geras, J. Katsnelson,
H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero,
M. Rabbat, P. Vincent, N. Yakubova, J. Pinkerton,
D. Wang, E. Owens, C. L. Zitnick, M. P. Recht, D. K.
Sodickson, Y . W. Lui, fastMRI: An open dataset and
benchmarks for accelerated MRI, 2018. arXiv:1811.
08839 .
[263] T. R. Moen, B. Chen, D. R. Holmes III, X. Duan, Z. Yu,
L. Yu, S. Leng, J. G. Fletcher, C. H. McCollough, Low-
dose CT image and projection dataset, Medical physics
48 (2) (2021) 902–911.
[264] S. G. Armato III, G. McLennan, L. Bidaut, M. F. McNitt-
Gray, C. R. Meyer, A. P. Reeves, B. Zhao, D. R. Aberle,
C. I. Henschke, E. A. Ho ffman, et al., The lung im-
age database consortium (LIDC) and image database re-
source initiative (IDRI): a completed reference database
of lung nodules on CT scans, Medical physics 38 (2)
(2011) 915–931.
[265] B. I. A. Group, Ixi dataset, http://
brain-development.org/ixi-dataset/ .
[266] C.-C. Shieh, Y . Gonzalez, B. Li, X. Jia, S. Rit, C. Mory,
M. Riblett, G. Hugo, Y . Zhang, Z. Jiang, et al., SPARE:
Sparse-view reconstruction challenge for 4D cone-beam
CT from a 1-min scan, Medical physics 46 (9) (2019)
3799–3811.[267] H. Der Sarkissian, F. Lucka, M. van Eijnatten, G. Colaci-
cco, S. B. Coban, K. J. Batenburg, A cone-beam X-ray
computed tomography data collection designed for ma-
chine learning, Scientific data 6 (1) (2019) 1–8.
[268] W.-A. Lin, H. Liao, C. Peng, X. Sun, J. Zhang, J. Luo,
R. Chellappa, S. K. Zhou, Dudonet: Dual domain net-
work for ct metal artifact reduction, in: Proceedings of
the IEEE /CVF Conference on Computer Vision and Pat-
tern Recognition, 2019, pp. 10512–10521.
[269] E. Plenge, D. H. Poot, M. Bernsen, G. Kotek, G. Hous-
ton, P. Wielopolski, L. van der Weerd, W. J. Niessen,
E. Meijering, Super-resolution methods in MRI: can
they improve the trade-o ffbetween resolution, signal-to-
noise ratio, and acquisition time?, Magnetic resonance
in medicine 68 (6) (2012) 1983–1993.
[270] S. d’Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos,
G. Biroli, L. Sagun, Convit: Improving vision transform-
ers with soft convolutional inductive biases, in: Interna-
tional Conference on Machine Learning, PMLR, 2021,
pp. 2286–2296.
[271] B. Lim, S. Son, H. Kim, S. Nah, K. Mu Lee, En-
hanced deep residual networks for single image super-
resolution, in: Proceedings of the IEEE conference
on computer vision and pattern recognition workshops,
2017, pp. 136–144.
[272] C.-M. Feng, Y . Yan, H. Fu, L. Chen, Y . Xu, Task trans-
former network for joint MRI reconstruction and super-
resolution, in: Medical Image Computing and Computer
Assisted Intervention–MICCAI 2021: 24th International
Conference, Strasbourg, France, September 27–October
1, 2021, Proceedings, Part VI 24, Springer, 2021, pp.
307–317.
[273] C.-M. Feng, Y . Yan, G. Chen, Y . Xu, Y . Hu, L. Shao,
H. Fu, Multi-modal transformer for accelerated MR
imaging, IEEE Transactions on Medical Imaging (2022).
[274] X. Zhang, X. He, J. Guo, N. Ettehadi, N. Aw, D. Se-
manek, J. Posner, A. Laine, Y . Wang, PTNet: a high-
resolution infant MRI synthesizer based on transformer,
arXiv preprint arXiv:2105.13993 (2021).
[275] O. Dalmaz, M. Yurt, T. C ¸ ukur, ResViT: Residual vision
transformers for multimodal medical image synthesis,
IEEE Transactions on Medical Imaging 41 (10) (2022)
2598–2614. doi:10.1109/TMI.2022.3167808 .
[276] J. Liu, S. Pasumarthi, B. Du ffy, E. Gong, G. Zaharchuk,
K. Datta, One model to synthesize them all: Multi-
contrast multi-scale transformer for missing data impu-
tation, arXiv preprint arXiv:2204.13738 (2022).
[277] N.-C. Ristea, A.-I. Miron, O. Savencu, M.-I. Georgescu,
N. Verga, F. S. Khan, R. T. Ionescu, CyTran: Cycle-
consistent transformers for non-contrast to contrast CT
translation, arXiv preprint arXiv:2110.06400 (2021).
65[278] S. A. Kamran, K. F. Hossain, A. Tavakkoli, S. L. Zucker-
brod, S. A. Baker, Vtgan: Semi-supervised retinal image
synthesis and disease prediction using vision transform-
ers, in: Proceedings of the IEEE /CVF International Con-
ference on Computer Vision, 2021, pp. 3235–3245.
[279] Y . Li, T. Zhou, K. He, Y . Zhou, D. Shen, SLMT-Net:
A self-supervised learning based multi-scale transformer
network for cross-modality mr image synthesis, arXiv
preprint arXiv:2212.01108 (2022).
[280] K. M. Choromanski, V . Likhosherstov, D. Dohan,
X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis,
A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell,
A. Weller, Rethinking attention with performers, in:
International Conference on Learning Representations,
2021.
URL https://openreview.net/forum?id=
Ua6zuk0WRH
[281] A. Makropoulos, E. C. Robinson, A. Schuh, R. Wright,
S. Fitzgibbon, J. Bozek, S. J. Counsell, J. Steinweg,
K. Vecchiato, J. Passerat-Palmbach, et al., The devel-
oping human connectome project: A minimal process-
ing pipeline for neonatal cortical surface reconstruction,
Neuroimage 173 (2018) 88–112.
[282] P. Isola, J.-Y . Zhu, T. Zhou, A. A. Efros, Image-to-
image translation with conditional adversarial networks,
in: Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, 2017, pp. 1125–1134.
[283] T.-C. Wang, M.-Y . Liu, J.-Y . Zhu, A. Tao, J. Kautz,
B. Catanzaro, High-resolution image synthesis and se-
mantic manipulation with conditional gans, in: Proceed-
ings of the IEEE conference on computer vision and pat-
tern recognition, 2018, pp. 8798–8807.
[284] S. U. Dar, M. Yurt, L. Karacan, A. Erdem, E. Erdem,
T. Cukur, Image synthesis in multi-contrast MRI with
conditional generative adversarial networks, IEEE trans-
actions on medical imaging 38 (10) (2019) 2375–2388.
[285] H. Zhang, I. Goodfellow, D. Metaxas, A. Odena, Self-
attention generative adversarial networks, in: Interna-
tional conference on machine learning, PMLR, 2019, pp.
7354–7363.
[286] T. Nyholm, S. Svensson, S. Andersson, J. Jonsson,
M. Sohlin, C. Gustafsson, E. Kjell ´en, K. S ¨oderstr ¨om,
P. Albertsson, L. Blomqvist, et al., MR and CT data
with multiobserver delineations of organs in the pelvic
area—part of the gold atlas project, Medical physics
45 (3) (2018) 1295–1300.
[287] O. Maier, B. H. Menze, J. von der Gablentz, L. H ¨ani,
M. P. Heinrich, M. Liebrand, S. Winzeck, A. Basit,
P. Bentley, L. Chen, et al., ISLES 2015-A public eval-
uation benchmark for ischemic stroke lesion segmenta-
tion from multispectral MRI, Medical Image Analysis 35
(2017) 250–269.[288] S. Hajeb Mohammad Alipour, H. Rabbani, M. R.
Akhlaghi, Diabetic retinopathy grading by digital
curvelet transform, Computational and mathematical
methods in medicine 2012 (2012).
[289] X. Ma, G. Luo, W. Wang, K. Wang, Transformer net-
work for significant stenosis detection in ccta of coro-
nary arteries, in: Medical Image Computing and Com-
puter Assisted Intervention–MICCAI 2021: 24th Inter-
national Conference, Strasbourg, France, September 27–
October 1, 2021, Proceedings, Part VI 24, Springer,
2021, pp. 516–525.
[290] H. Jiang, P. Zhang, C. Che, B. Jin, RDFNet: A fast caries
detection method incorporating transformer mechanism,
Computational and Mathematical Methods in Medicine
2021 (2021).
[291] R. Wagner, K. Rohr, Cellcentroidformer: Combining
self-attention and convolution for cell detection, in: An-
nual Conference on Medical Image Understanding and
Analysis, Springer, 2022, pp. 212–222.
[292] D. M. Nguyen, H. Nguyen, M. T. Truong, T. Cao, B. T.
Nguyen, N. Ho, P. Swoboda, S. Albarqouni, P. Xie,
D. Sonntag, Joint self-supervised image-volume repre-
sentation learning with intra-inter contrastive clustering,
in: 37th AAAI Conference on Artificial Intelligence,
AAAI, 2022.
[293] Z. Shen, R. Fu, C. Lin, S. Zheng, COTR: Convolution
in transformer network for end to end polyp detection,
in: 2021 7th International Conference on Computer and
Communications (ICCC), IEEE, 2021, pp. 1757–1761.
[294] Q. Kong, Y . Wu, C. Yuan, Y . Wang, CT-CAD: Context-
aware transformers for end-to-end chest abnormality de-
tection on X-Rays, in: 2021 IEEE International Confer-
ence on Bioinformatics and Biomedicine (BIBM), IEEE,
2021, pp. 1385–1388.
[295] R. Tao, G. Zheng, Spine-transformers: Vertebra detec-
tion and localization in arbitrary field-of-view spine ct
with transformers, in: International Conference on Med-
ical Image Computing and Computer-Assisted Interven-
tion, Springer, 2021, pp. 93–103.
[296] B. Wittmann, F. Navarro, S. Shit, B. Menze, et al.,
Focused decoding enables 3D anatomical detection by
transformers, Machine Learning for Biomedical Imag-
ing 2 (February 2023 issue) (2023) 72–95.
[297] Z. Yao, J. Ai, B. Li, C. Zhang, E fficient detr: improv-
ing end-to-end object detector with dense prior, arXiv
preprint arXiv:2104.01318 (2021).
[298] A. Criminisi, J. Shotton, S. Bucciarelli, Decision forests
with long-range spatial context for organ localization
in CT volumes, in: Medical Image Computing and
Computer-Assisted Intervention (MICCAI), Citeseer,
2009, pp. 69–80.
66[299] F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, L. Zhang,
Dn-detr: Accelerate detr training by introducing query
denoising, in: Proceedings of the IEEE /CVF Conference
on Computer Vision and Pattern Recognition, 2022, pp.
13619–13627.
[300] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. Ni,
H.-Y . Shum, DINO: DETR with improved denoising
anchor boxes for end-to-end object detection, in: The
Eleventh International Conference on Learning Repre-
sentations, 2023.
URL https://openreview.net/forum?id=
3mRwyG5one
[301] T. Prangemeier, C. Reich, H. Koeppl, Attention-based
transformers for instance segmentation of cells in mi-
crostructures, in: 2020 IEEE International Conference
on Bioinformatics and Biomedicine (BIBM), IEEE,
2020, pp. 700–707.
[302] K. He, X. Zhang, S. Ren, J. Sun, Spatial pyramid pool-
ing in deep convolutional networks for visual recogni-
tion, IEEE transactions on pattern analysis and machine
intelligence 37 (9) (2015) 1904–1916.
[303] T.-Y . Lin, P. Doll ´ar, R. Girshick, K. He, B. Hariharan,
S. Belongie, Feature pyramid networks for object detec-
tion, in: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2017, pp. 2117–
2125.
[304] S. Liu, L. Qi, H. Qin, J. Shi, J. Jia, Path aggregation net-
work for instance segmentation, in: Proceedings of the
IEEE conference on computer vision and pattern recog-
nition, 2018, pp. 8759–8768.
[305] M. Tan, Q. Le, E fficientnetv2: Smaller models and
faster training, in: International Conference on Machine
Learning, PMLR, 2021, pp. 10096–10106.
[306] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang,
A. Dosovitskiy, Do vision transformers see like convo-
lutional neural networks?, Advances in Neural Informa-
tion Processing Systems 34 (2021) 12116–12128.
[307] O. Schoppe, C. Pan, J. Coronel, H. Mai, Z. Rong,
M. I. Todorov, A. M ¨uskes, F. Navarro, H. Li, A. Ert ¨urk,
et al., Deep learning-enabled multi-organ segmentation
in whole-body mouse scans, Nature communications
11 (1) (2020) 1–14.
[308] K. H. Hohne, M. Bomans, M. Riemer, R. Schubert,
U. Tiede, W. Lierse, A volume-based anatomical at-
las, IEEE Computer Graphics and Applications 12 (04)
(1992) 73–77.
[309] V . Ulman, M. Ma ˇska, K. E. Magnusson, O. Ronneberger,
C. Haubold, N. Harder, P. Matula, P. Matula, D. Svo-
boda, M. Radojevic, et al., An objective comparison of
cell-tracking algorithms, Nature methods 14 (12) (2017)
1141–1152.[310] O. Jimenez-del Toro, H. M ¨uller, M. Krenn, K. Gru-
enberg, A. A. Taha, M. Winterstein, I. Eggel,
A. Foncubierta-Rodr ´ıguez, O. Goksel, A. Jakab, et al.,
Cloud-based evaluation of anatomical structure segmen-
tation and landmark detection algorithms: VISCERAL
anatomy benchmarks, IEEE transactions on medical
imaging 35 (11) (2016) 2459–2475.
[311] Y . Ji, H. Bai, C. Ge, J. Yang, Y . Zhu, R. Zhang, Z. Li,
L. Zhanng, W. Ma, X. Wan, et al., Amos: A large-scale
abdominal multi-organ benchmark for versatile medical
image segmentation, Advances in Neural Information
Processing Systems 35 (2022) 36722–36732.
[312] J. Bernal, F. J. S ´anchez, G. Fern ´andez-Esparrach, D. Gil,
C. Rodr ´ıguez, F. Vilari ˜no, WM-DOV A maps for accu-
rate polyp highlighting in colonoscopy: Validation vs.
saliency maps from physicians, Computerized medical
imaging and graphics 43 (2015) 99–111.
[313] J. Silva, A. Histace, O. Romain, X. Dray, B. Granado,
Toward embedded detection of polyps in wce images for
early diagnosis of colorectal cancer, International journal
of computer assisted radiology and surgery 9 (2) (2014)
283–293.
[314] J. Bernal, J. S ´anchez, F. Vilarino, Towards automatic
polyp detection with a polyp appearance model, Pattern
Recognition 45 (9) (2012) 3166–3182.
[315] N. T. Nguyen, P. T. Truong, V . T. Ho, T. V . Nguyen,
H. T. Pham, M. T. Nguyen, L. T. Dam, H. Q. Nguyen,
VinDr Lab: A data platform for medical AI, URL:
https: //github. com /vinbigdata-medical /vindr-lab (2021).
[316] J. Liu, J. Lian, Y . Yu, ChestX-det10: chest X-ray dataset
on detection of thoracic abnormalities, arXiv preprint
arXiv:2006.10550 (2020).
[317] A. Sekuboyina, A. Bayat, M. E. Husseini, M. L ¨offler,
M. Rempfler, J. Kuka ˇcka, G. Tetteh, A. Valentinitsch,
C. Payer, M. Urschler, et al., Verse: a vertebrae labelling
and segmentation benchmark, arXiv. org e-Print archive
(2020).
[318] B. Glocker, D. Zikic, E. Konukoglu, D. R. Haynor,
A. Criminisi, Vertebrae localization in pathological spine
CT via dense classification from sparse annotations, in:
International conference on medical image computing
and computer-assisted intervention, Springer, 2013, pp.
262–270.
[319] H. A. Qadir, I. Balasingham, J. Solhusvik, J. Bergsland,
L. Aabakken, Y . Shin, Improving automatic polyp de-
tection using CNN by exploiting temporal dependency
in colonoscopy video, IEEE journal of biomedical and
health informatics 24 (1) (2019) 180–193.
[320] H. A. Qadir, Y . Shin, J. Solhusvik, J. Bergsland,
L. Aabakken, I. Balasingham, Toward real-time polyp
detection using fully CNNs for 2D gaussian shapes pre-
diction, Medical Image Analysis 68 (2021) 101897.
67[321] Z. Cai, N. Vasconcelos, Cascade r-cnn: Delving into
high quality object detection, in: Proceedings of the
IEEE conference on computer vision and pattern recog-
nition, 2018, pp. 6154–6162.
[322] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only
look once: Unified, real-time object detection, in: Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition, 2016, pp. 779–788.
[323] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid,
S. Savarese, Generalized intersection over union: A met-
ric and a loss for bounding box regression, in: Proceed-
ings of the IEEE /CVF conference on computer vision
and pattern recognition, 2019, pp. 658–666.
[324] S. Qiao, L.-C. Chen, A. Yuille, Detectors: Detecting
objects with recursive feature pyramid and switchable
atrous convolution, in: Proceedings of the IEEE /CVF
conference on computer vision and pattern recognition,
2021, pp. 10213–10224.
[325] T.-Y . Lin, P. Doll ´ar, R. Girshick, K. He, B. Hariharan,
S. Belongie, Feature pyramid networks for object detec-
tion, in: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2017, pp. 2117–
2125.
[326] Q. Chen, Y . Wang, T. Yang, X. Zhang, J. Cheng, J. Sun,
You only look one-level feature, in: Proceedings of the
IEEE /CVF conference on computer vision and pattern
recognition, 2021, pp. 13039–13048.
[327] Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, D. Ren,
Distance-iou loss: Faster and better learning for bound-
ing box regression, in: Proceedings of the AAAI confer-
ence on artificial intelligence, V ol. 34, 2020, pp. 12993–
13000.
[328] ¨O. C ¸ ic ¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox,
O. Ronneberger, 3D U-Net: learning dense volumetric
segmentation from sparse annotation, in: International
conference on medical image computing and computer-
assisted intervention, Springer, 2016, pp. 424–432.
[329] G. Haskins, U. Kruger, P. Yan, Deep learning in medical
image registration: a survey, Machine Vision and Appli-
cations 31 (1) (2020) 1–18.
[330] F. Alam, S. U. Rahman, S. Ullah, K. Gulati, Medical im-
age registration in image guided surgery: Issues, chal-
lenges and research opportunities, Biocybernetics and
Biomedical Engineering 38 (1) (2018) 71–89.
[331] F. Alam, S. U. Rahman, Challenges and solutions in mul-
timodal medical image subregion detection and registra-
tion, Journal of medical imaging and radiation sciences
50 (1) (2019) 24–30.
[332] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag,
A. V . Dalca, An unsupervised learning model for de-
formable medical image registration, in: Proceedingsof the IEEE conference on computer vision and pattern
recognition, 2018, pp. 9252–9260.
[333] J. Chen, Y . Li, Y . Du, E. C. Frey, Generating anthropo-
morphic phantoms using fully unsupervised deformable
image registration with convolutional neural networks,
Medical physics 47 (12) (2020) 6366–6380.
[334] L. Chen, Q. T. Yu, Transformers make strong encoders
for medical image segmentation. arxiv 2021, arXiv
preprint arXiv:2102.04306.
[335] T. Lin, Y . Wang, X. Liu, X. Qiu, A survey of transform-
ers, arXiv preprint arXiv:2106.04554 (2021).
[336] J. Xu, D. Moyer, P. E. Grant, P. Golland, J. E. Igle-
sias, E. Adalsteinsson, SV oRT: iterative transformer for
slice-to-volume registration in fetal brain MRI, in: Inter-
national Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2022, pp. 3–
13.
[337] Y . Rong, M. Rosu-Bubulac, S. H. Benedict, Y . Cui,
R. Ruo, T. Connell, R. Kashani, K. Latifi, Q. Chen,
H. Geng, et al., Rigid and deformable image registra-
tion for radiation therapy: a self-study evaluation guide
for NRG oncology clinical trial participation, Practical
Radiation Oncology 11 (4) (2021) 282–298.
[338] J. Chen, Y . He, E. Frey, Y . Li, Y . Du, ViT-V-Net: Vision
transformer for unsupervised volumetric medical image
registration, in: Medical Imaging with Deep Learning,
2021.
URL https://openreview.net/forum?id=
h3HC1EU7AEz
[339] J. Chen, E. C. Frey, Y . He, W. P. Segars, Y . Li,
Y . Du, Transmorph: Transformer for unsupervised medi-
cal image registration, Medical image analysis 82 (2022)
102615.
[340] Y . Zhang, Y . Pei, H. Zha, Learning dual transformer
network for di ffeomorphic registration, in: Interna-
tional Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2021, pp.
129–138.
[341] J. Shi, Y . He, Y . Kong, J.-L. Coatrieux, H. Shu, G. Yang,
S. Li, XMorpher: Full transformer for deformable med-
ical image registration via cross attention, in: Interna-
tional Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2022, pp.
217–226.
[342] K. Miao, A. Gokul, R. Singh, S. Petryk, J. Gonzalez,
K. Keutzer, T. Darrell, Prior knowledge-guided attention
in self-supervised vision transformers, arXiv preprint
arXiv:2209.03745 (2022).
[343] T. C. Mok, A. Chung, A ffine medical image registration
with coarse-to-fine vision transformer, in: Proceedings
of the IEEE /CVF Conference on Computer Vision and
Pattern Recognition, 2022, pp. 20835–20844.
68[344] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully con-
volutional neural networks for volumetric medical image
segmentation, in: 2016 fourth international conference
on 3D vision (3DV), IEEE, 2016, pp. 565–571.
[345] M. Jaderberg, K. Simonyan, A. Zisserman, et al., Spatial
transformer networks, Advances in neural information
processing systems 28 (2015).
[346] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky,
J. C. Morris, R. L. Buckner, Open access series of imag-
ing studies (OASIS): cross-sectional MRI data in young,
middle aged, nondemented, and demented older adults,
Journal of cognitive neuroscience 19 (9) (2007) 1498–
1507.
[347] W. Segars, J. Bond, J. Frush, S. Hon, C. Eckersley, C. H.
Williams, J. Feng, D. J. Tward, J. Ratnanather, M. Miller,
et al., Population of anatomically variable 4D XCAT
adult phantoms for imaging research and optimization,
Medical physics 40 (4) (2013) 043701.
[348] X. Zhuang, J. Shen, Multi-scale patch and multi-
modality atlases for whole heart segmentation of MRI,
Medical image analysis 31 (2016) 77–87.
[349] R. Gharleghi, D. G. Samarasinghe, P. A. Sowmya,
D. S. Beier, Automated segmentation of coronary arter-
ies (Mar. 2020). doi:10.5281/zenodo.3819799 .
URL https://doi.org/10.5281/zenodo.3819799
[350] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto,
T. Kobayashi, K.-i. Komatsu, M. Matsui, H. Fujita,
Y . Kodera, K. Doi, Development of a digital image
database for chest radiographs with and without a lung
nodule: receiver operating characteristic analysis of ra-
diologists’ detection of pulmonary nodules, American
Journal of Roentgenology 174 (1) (2000) 71–74.
[351] D. W. Shattuck, M. Mirza, V . Adisetiyo, C. Ho-
jatkashani, G. Salamon, K. L. Narr, R. A. Poldrack,
R. M. Bilder, A. W. Toga, Construction of a 3D prob-
abilistic atlas of human cortical structures, Neuroimage
39 (3) (2008) 1064–1080.
[352] K. Payette, P. de Dumast, H. Kebiri, I. Ezhov, J. C. Paet-
zold, S. Shit, A. Iqbal, R. Khan, R. Kottke, P. Grehten,
et al., An automatic multi-tissue human fetal brain seg-
mentation benchmark using the fetal tissue annotation
dataset, Scientific Data 8 (1) (2021) 1–14.
[353] B. D. De V os, F. F. Berendsen, M. A. Viergever,
H. Sokooti, M. Staring, I. I ˇsgum, A deep learning frame-
work for unsupervised a ffine and deformable image reg-
istration, Medical image analysis 52 (2019) 128–143.
[354] S. Zhao, T. Lau, J. Luo, I. Eric, C. Chang, Y . Xu, Unsu-
pervised 3D end-to-end medical image registration with
volume tweening network, IEEE journal of biomedical
and health informatics 24 (5) (2019) 1394–1404.[355] Y . Xiong, B. Du, P. Yan, Reinforced transformer for
medical image captioning, in: International Workshop
on Machine Learning in Medical Imaging, Springer,
2019, pp. 673–680.
[356] J. Zhang, Y . Nie, J. Chang, J. J. Zhang, Surgical instruc-
tion generation with transformers, in: International Con-
ference on Medical Image Computing and Computer-
Assisted Intervention, Springer, 2021, pp. 290–299.
[357] Z. Chen, Y . Song, T.-H. Chang, X. Wan, Generating ra-
diology reports via memory-driven transformer, in: Pro-
ceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), 2020, pp.
1439–1449.
[358] D. You, F. Liu, S. Ge, X. Xie, J. Zhang, X. Wu, Align-
transformer: Hierarchical alignment of visual regions
and disease tags for medical report generation, in: Inter-
national Conference on Medical Image Computing and
Computer-Assisted Intervention, Springer, 2021, pp. 72–
82.
[359] F. Nooralahzadeh, N. P. Gonzalez, T. Frauenfelder,
K. Fujimoto, M. Krauthammer, Progressive transformer-
based generation of radiology reports, arXiv preprint
arXiv:2102.09777 (2021).
[360] A. Yan, Z. He, X. Lu, J. Du, E. Chang, A. Gentili,
J. McAuley, C.-n. Hsu, Weakly supervised contrastive
learning for chest X-Ray report generation, in: Find-
ings of the Association for Computational Linguistics:
EMNLP 2021, 2021, pp. 4009–4015.
[361] Z. Chen, Y . Shen, Y . Song, X. Wan, Cross-modal mem-
ory networks for radiology report generation, arXiv
preprint arXiv:2204.13258 (2022).
[362] C. Y . Li, X. Liang, Z. Hu, E. P. Xing, Knowledge-driven
encode, retrieve, paraphrase for medical image report
generation, in: Proceedings of the AAAI Conference on
Artificial Intelligence, V ol. 33, 2019, pp. 6666–6673.
[363] F. Liu, X. Wu, S. Ge, W. Fan, Y . Zou, Exploring and dis-
tilling posterior and prior knowledge for radiology report
generation, in: Proceedings of the IEEE /CVF Confer-
ence on Computer Vision and Pattern Recognition, 2021,
pp. 13753–13762.
[364] J. Lovelace, B. Mortazavi, Learning to generate clini-
cally coherent chest x-ray reports, in: Findings of the As-
sociation for Computational Linguistics: EMNLP 2020,
2020, pp. 1235–1243.
[365] G. Liu, Y . Liao, F. Wang, B. Zhang, L. Zhang, X. Liang,
X. Wan, S. Li, Z. Li, S. Zhang, et al., Medical-vlbert:
Medical visual language bert for covid-19 ct report gen-
eration with alternate learning, IEEE Transactions on
Neural Networks and Learning Systems 32 (9) (2021)
3786–3797.
69[366] O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal,
A. Fahmy, Automated radiology report generation using
conditioned transformers, Informatics in Medicine Un-
locked 24 (2021) 100557.
[367] H. T. Nguyen, D. Nie, T. Badamdorj, Y . Liu, Y . Zhu,
J. Truong, L. Cheng, Automated generation of accu-
rate & fluent medical X-ray reports, arXiv preprint
arXiv:2108.12126 (2021).
[368] Y . Wang, Z. Lin, J. Tian, Z. Shi, Y . Zhang, J. Fan,
Z. He, Confidence-guided radiology report generation,
arXiv preprint arXiv:2106.10887 (2021).
[369] M. Endo, R. Krishnan, V . Krishna, A. Y . Ng, P. Ra-
jpurkar, Retrieval-based chest X-ray report generation
using a pre-trained contrastive language-image model,
in: Machine Learning for Health, PMLR, 2021, pp. 209–
219.
[370] S. Bannur, S. Hyland, Q. Liu, F. Perez-Garcia, M. Ilse,
D. C. Castro, B. Boecking, H. Sharma, K. Bouzid,
A. Thieme, et al., Learning to exploit temporal structure
for biomedical vision-language processing, in: Proceed-
ings of the IEEE /CVF Conference on Computer Vision
and Pattern Recognition, 2023, pp. 15016–15027.
[371] S. Wang, Z. Zhao, X. Ouyang, Q. Wang, D. Shen,
Chatcad: Interactive computer-aided diagnosis on med-
ical image using large language models, arXiv preprint
arXiv:2302.07257 (2023).
[372] B. Jing, P. Xie, E. Xing, On the automatic gen-
eration of medical imaging reports, arXiv preprint
arXiv:1711.08195 (2017).
[373] M. Stefanini, M. Cornia, L. Baraldi, S. Cascianelli,
G. Fiameni, R. Cucchiara, From show to tell: a survey
on deep learning-based image captioning, IEEE Trans-
actions on Pattern Analysis and Machine Intelligence
(2022).
[374] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman,
S. E. Shooshan, L. Rodriguez, S. Antani, G. R. Thoma,
C. J. McDonald, Preparing a collection of radiology
examinations for distribution and retrieval, Journal of
the American Medical Informatics Association 23 (2)
(2016) 304–310.
[375] M. M. A. Monshi, J. Poon, V . Chung, Deep learning in
generating radiology reports: A survey, Artificial Intelli-
gence in Medicine 106 (2020) 101878.
[376] R. Vedantam, C. Lawrence Zitnick, D. Parikh, Cider:
Consensus-based image description evaluation, in: Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition, 2015, pp. 4566–4575.
[377] Y . Li, X. Liang, Z. Hu, E. P. Xing, Hybrid retrieval-
generation reinforced agent for medical image report
generation, Advances in neural information processing
systems 31 (2018).[378] Y . Zhang, X. Wang, Z. Xu, Q. Yu, A. Yuille, D. Xu,
When radiology report generation meets knowledge
graph, in: Proceedings of the AAAI Conference on Ar-
tificial Intelligence, V ol. 34, 2020, pp. 12910–12917.
[379] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger,
Densely connected convolutional networks, in: Proceed-
ings of the IEEE conference on computer vision and pat-
tern recognition, 2017, pp. 4700–4708.
[380] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, Bleu: a
method for automatic evaluation of machine translation,
in: Proceedings of the 40th annual meeting of the As-
sociation for Computational Linguistics, 2002, pp. 311–
318.
[381] E. Rojas-Mu ˜noz, K. Couperus, J. Wachs, Daisi:
Database for ai surgical instruction, arXiv preprint
arXiv:2004.02809 (2020).
[382] S. Banerjee, A. Lavie, METEOR: An automatic met-
ric for mt evaluation with improved correlation with hu-
man judgments, in: Proceedings of the acl workshop on
intrinsic and extrinsic evaluation measures for machine
translation and /or summarization, 2005, pp. 65–72.
[383] C.-Y . Lin, Rouge: A package for automatic evaluation of
summaries, in: Text summarization branches out, 2004,
pp. 74–81.
[384] P. Anderson, B. Fernando, M. Johnson, S. Gould,
Spice: Semantic propositional image caption evaluation,
in: European conference on computer vision, Springer,
2016, pp. 382–398.
[385] A. E. Johnson, T. J. Pollard, N. R. Greenbaum, M. P.
Lungren, C.-y. Deng, Y . Peng, Z. Lu, R. G. Mark, S. J.
Berkowitz, S. Horng, MIMIC-CXR-JPG, a large pub-
licly available database of labeled chest radiographs,
arXiv preprint arXiv:1901.07042 (2019).
[386] J. Ni, C.-N. Hsu, A. Gentili, J. McAuley, Learn-
ing visual-semantic embeddings for reporting ab-
normal findings on chest X-rays, arXiv preprint
arXiv:2010.02467 (2020).
[387] G. Liu, Y . Liao, Z. Li, Covid-19ct dataset, https://
covid19ct.github.io/ .
[388] M. Li, R. Liu, F. Wang, X. Chang, X. Liang, Auxiliary
signal-guided knowledge encoder-decoder for medical
report generation, World Wide Web (2022) 1–18.
[389] A. Smit, S. Jain, P. Rajpurkar, A. Pareek, A. Y . Ng, M. P.
Lungren, CheXbert: combining automatic labelers and
expert annotations for accurate radiology report labeling
using BERT, arXiv preprint arXiv:2004.09167 (2020).
[390] P. Messina, P. Pino, D. Parra, A. Soto, C. Besa, S. Uribe,
M. And ´ıa, C. Tejos, C. Prieto, D. Capurro, A survey
on deep learning and explainability for automatic report
generation from medical images, ACM Computing Sur-
veys (CSUR) 54 (10s) (2022) 1–40.
70[391] F. Liu, X. Ren, G. Zhao, X. Sun, Layer-wise cross-
view decoding for sequence-to-sequence learning, arXiv
preprint arXiv:2005.08081 (2020).
[392] F. Liu, S. Ge, X. Wu, Competence-based multimodal
curriculum learning for medical report generation, arXiv
preprint arXiv:2206.14579 (2022).
[393] R. H. Zhang, Q. Liu, A. X. Fan, H. Ji, D. Zeng, F. Cheng,
D. Kawahara, S. Kurohashi, Minimize exposure bias of
seq2seq models in joint entity and relation extraction,
arXiv preprint arXiv:2009.07503 (2020).
[394] S. J. Rennie, E. Marcheret, Y . Mroueh, J. Ross, V . Goel,
Self-critical sequence training for image captioning, in:
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 7008–7024.
[395] X. Wang, Y . Chen, W. Zhu, A survey on curriculum
learning, IEEE Transactions on Pattern Analysis and
Machine Intelligence (2021).
[396] M. Cornia, M. Stefanini, L. Baraldi, R. Cucchiara,
Meshed-memory transformer for image captioning, in:
Proceedings of the IEEE /CVF conference on computer
vision and pattern recognition, 2020, pp. 10578–10587.
[397] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V . Stoyanov, L. Zettlemoyer, Bart:
Denoising sequence-to-sequence pre-training for natu-
ral language generation, translation, and comprehension,
arXiv preprint arXiv:1910.13461 (2019).
[398] P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta,
T. Duan, D. Ding, A. Bagul, C. Langlotz, K. Shpan-
skaya, et al., Chexnet: Radiologist-level pneumonia
detection on chest X-rays with deep learning, arXiv
preprint arXiv:1711.05225 (2017).
[399] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
I. Sutskever, et al., Language models are unsupervised
multitask learners, OpenAI blog 1 (8) (2019) 9.
[400] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, J. Dai,
VL-BERT: Pre-training of generic visual-linguistic rep-
resentations, in: International Conference on Learning
Representations, 2020.
URL https://openreview.net/forum?id=
SygXPaEYvH
[401] V . Ramesh, N. A. Chi, P. Rajpurkar, Improving radiol-
ogy report generation systems by removing hallucinated
references to non-existent priors, in: Machine Learning
for Health, PMLR, 2022, pp. 456–473.
[402] J. Zhou, X. He, L. Sun, J. Xu, X. Chen, Y . Chu, L. Zhou,
X. Liao, B. Zhang, X. Gao, SkinGPT-4: An interac-
tive dermatology diagnostic system with visual large lan-
guage model, medRxiv (2023) 2023–06.[403] C. Ma, Z. Wu, J. Wang, S. Xu, Y . Wei, Z. Liu,
L. Guo, X. Cai, S. Zhang, T. Zhang, et al., Impres-
sionGPT: an iterative optimizing framework for radiol-
ogy report summarization with chatGPT, arXiv preprint
arXiv:2304.08448 (2023).
[404] OpenAI, Chatgpt: Optimizing language models for dia-
logue (2023).
URL https://openai.com/blog/chatgpt/
[405] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al., Language models are few-shot learners,
Advances in neural information processing systems 33
(2020) 1877–1901.
[406] T. Lin, Y . Wang, X. Liu, X. Qiu, A survey of transform-
ers, AI Open (2022).
[407] J. Pavlopoulos, V . Kougia, I. Androutsopoulos, D. Pa-
pamichail, Diagnostic captioning: a survey, Knowledge
and Information Systems (2022) 1–32.
[408] X. Meng, C. H. Ganoe, R. T. Sieberg, Y . Y . Cheung,
S. Hassanpour, Self-supervised contextual language rep-
resentation of radiology reports to improve the identi-
fication of communication urgency, AMIA Summits on
Translational Science Proceedings 2020 (2020) 413.
[409] G. Alicioglu, B. Sun, A survey of visual analytics for
explainable artificial intelligence methods, Computers &
Graphics 102 (2022) 502–520.
[410] A. Singh, S. Sengupta, V . Lakshminarayanan, Explain-
able deep learning models in medical image analysis,
Journal of Imaging 6 (6) (2020) 52.
[411] B. Hou, G. Kaissis, R. M. Summers, B. Kainz, Ratchet:
Medical transformer for chest X-ray diagnosis and re-
porting, in: International Conference on Medical Im-
age Computing and Computer-Assisted Intervention,
Springer, 2021, pp. 293–303.
[412] A. Binder, G. Montavon, S. Lapuschkin, K.-R. M ¨uller,
W. Samek, Layer-wise relevance propagation for neu-
ral networks with local renormalization layers, in: In-
ternational Conference on Artificial Neural Networks,
Springer, 2016, pp. 63–71.
[413] S. Kim, J. Nam, B. C. Ko, Vit-net: Interpretable vi-
sion transformers with neural tree decoder, in: Interna-
tional Conference on Machine Learning, PMLR, 2022,
pp. 11162–11172.
[414] Z. Li, F. Liu, W. Yang, S. Peng, J. Zhou, A survey of
convolutional neural networks: analysis, applications,
and prospects, IEEE transactions on neural networks and
learning systems (2021).
[415] J. Gu, H. Kwon, D. Wang, W. Ye, M. Li, Y .-H.
Chen, L. Lai, V . Chandra, D. Z. Pan, Multi-scale high-
resolution vision transformer for semantic segmentation,
71in: Proceedings of the IEEE /CVF Conference on Com-
puter Vision and Pattern Recognition, 2022, pp. 12094–
12103.
[416] Y . Lee, J. Kim, J. Willette, S. J. Hwang, MPViT: Multi-
path vision transformer for dense prediction, in: Pro-
ceedings of the IEEE /CVF Conference on Computer Vi-
sion and Pattern Recognition, 2022, pp. 7287–7296.
[417] H. Reynaud, A. Vlontzos, B. Hou, A. Beqiri, P. Lee-
son, B. Kainz, Ultrasound video transformers for car-
diac ejection fraction estimation, in: International Con-
ference on Medical Image Computing and Computer-
Assisted Intervention, Springer, 2021, pp. 495–505.
[418] Y . Long, Z. Li, C. H. Yee, C. F. Ng, R. H. Taylor,
M. Unberath, Q. Dou, E-dssr: e fficient dynamic surgi-
cal scene reconstruction with transformer-based stereo-
scopic depth perception, in: International Conference on
Medical Image Computing and Computer-Assisted In-
tervention, Springer, 2021, pp. 415–425.
[419] T. Czempiel, M. Paschali, D. Ostler, S. T. Kim,
B. Busam, N. Navab, Opera: Attention-regularized
transformers for surgical phase recognition, in: In-
ternational Conference on Medical Image Computing
and Computer-Assisted Intervention, Springer, 2021, pp.
604–614.
[420] Z. Zhao, Y . Jin, P.-A. Heng, TraSeTR: track-to-segment
transformer with contrastive query for instance-level
instrument segmentation in robotic surgery, in: 2022
International Conference on Robotics and Automation
(ICRA), IEEE, 2022, pp. 11186–11193.
[421] Z. Liu, J. Ning, Y . Cao, Y . Wei, Z. Zhang, S. Lin,
H. Hu, Video swin transformer, in: Proceedings of the
IEEE /CVF Conference on Computer Vision and Pattern
Recognition, 2022, pp. 3202–3211.
[422] M. Ding, B. Xiao, N. Codella, P. Luo, J. Wang, L. Yuan,
Davit: Dual attention vision transformers, in: Com-
puter Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
XXIV , Springer, 2022, pp. 74–92.
[423] A. Ali, H. Touvron, M. Caron, P. Bojanowski, M. Douze,
A. Joulin, I. Laptev, N. Neverova, G. Synnaeve, J. Ver-
beek, et al., Xcit: Cross-covariance image transformers,
Advances in neural information processing systems 34
(2021) 20014–20027.
[424] M. Maaz, A. Shaker, H. Cholakkal, S. Khan, S. W.
Zamir, R. M. Anwer, F. Shahbaz Khan, Edgenext: ef-
ficiently amalgamated cnn-transformer architecture for
mobile vision applications, in: European Conference on
Computer Vision, Springer, 2022, pp. 3–20.
[425] Z. Shen, M. Zhang, H. Zhao, S. Yi, H. Li, E fficient atten-
tion: Attention with linear complexities, in: Proceedings
of the IEEE /CVF winter conference on applications of
computer vision, 2021, pp. 3531–3539.[426] Z. Zhang, U. Bagci, Dynamic linear transformer for
3D biomedical image segmentation, in: Machine Learn-
ing in Medical Imaging: 13th International Workshop,
MLMI 2022, Held in Conjunction with MICCAI 2022,
Singapore, September 18, 2022, Proceedings, Springer,
2022, pp. 171–180.
[427] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, K. H.
Maier-Hein, nnU-Net: a self-configuring method for
deep learning-based biomedical image segmentation,
Nature methods 18 (2) (2021) 203–211.
[428] C. Gros, A. Lemay, O. Vincent, L. Rouhier, A. Buc-
quet, J. P. Cohen, J. Cohen-Adad, Ivadomed: A med-
ical imaging deep learning toolbox, arXiv preprint
arXiv:2010.09984 (2020).
72