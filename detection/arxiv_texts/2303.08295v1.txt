A large-scale multimodal dataset of human speech
recognition
Yao Ge1, Chong Tang1,3, Haobo Li2, Zikang Chen1, Wenda Li4, Kevin Chetty3, Daniele
Faccio2, Qammer H. Abbasi1,*, and Muhammad Imran1
1James Watt School of Engineering, University of Glasgow, Glasgow, G12 8QQ,UK
2School of Physics & Astronomy, University of Glasgow, Glasgow, G12 8QQ,UK
3Department of Security and Crime Science, University College London, London, WC1E 6BT, UK
4School of Science and Engineering, University of Dundee, Dundee, DD1 4HN, UK
*corresponding author(s): Qammer H. Abbasi (qammer.abbasi@glasgow.ac.uk)
ABSTRACT
Nowadays, non-privacy small-scale motion detection has attracted an increasing amount of research in remote sensing in
speech recognition. These new modalities are employed to enhance and restore speech information from speakers of multiple
types of data. In this paper, we propose a dataset contains 7.5 GHz Channel Impulse Response (CIR) data from ultra-wideband
(UWB) radars, 77-GHz frequency modulated continuous wave (FMCW) data from millimetre wave (mmWave) radar, and laser
data. Meanwhile, a depth camera is adopted to record the landmarks of the subject’s lip and voice. Approximately 400 minutes
of annotated speech proﬁles are provided, which are collected from 20 participants speaking 5 vowels, 15 words and 16
sentences. The dataset has been validated and has potential for the research of lip reading and multimodal speech recognition.
1 Background & Summary
In general speech recognition tasks, acoustic information from microphones is the main source for analysing the verbal
communication of human beings1. The speech process is not just a means of conveying linguistic information, which can also
provide valuable insight into the speaker’s characteristics such as gender, age, social and regional origin, health, emotional state,
and in some cases even their identity. Recently, the automatic speech recognition (ASR) technique has already matured and
been marketed2. In addition to sound signals, the series of physiological processes that produce sound, such as lip movement,
vocal cord vibration, and head movement, also retain semantic and speaker information to some extent. On the other hand, there
are two main limitations in speciﬁc environments that only audio information can not perfectly work for ASR: silent speech
recognition (SSR) and multiple speakers environments. Both issues can be solved if considering the speaker physics properties.
First, SSR can be considered a signiﬁcant branch of speech recognition that provides understandable and enhancing
communication methods to assist patients with severe speech disorders. Recently, the directions of SSR is mainly focused on
wearable sensors, which detect brain and muscle activity with electroencephalogram (EEG) sensor, articulator movements
headset. and other kinds of implantable sensors3. However, these methods are highly dependent on wearable and implant
sensors, which is dedicated to patients but does not collect a large dataset from a normal person. Meanwhile, users should
consider the potential health risk of contactable devices. For voice disorder and other patients who maintain the capability
to control the vibration of vocal folds and face muscles, non-invasive SSR has the potential to improve their quality of life
compared to electronic sensors.
In addition, in scenarios with multiple speakers, the microphone captures the sounds from the surroundings without
distinguishing the person’s identity, which seriously lowers the accuracy of speech recognition. This issue is similar to the
cocktail party effect4, which is a phenomenon in which an individual can focus on one conversation despite being surrounded by
several other simultaneous conversations. The effect is mainly attributed to the brain’s ability to process auditory frequency and
highlight certain sounds, allowing the individual to focus in on the source of interest without being easily distracted. However,
it is a challenge to separate different sources only using acoustic data. In this case, additional radar or laser devices can assist
the model in distinguishing the audio according to the physical information. For example, the proposed work5combined the
audio and radar signals to ﬁlter after added noise. And Secondly, the voice information including tone and speaking habits of
individual contains a variety of personal data that can be used to create a unique voice ﬁngerprint, such as speaking habits and
intonation. This will cause a risk of sensitive data leakage, as the voice ﬁngerprint could be used for identiﬁcation. For the
wireless sensing side-based algorithm, vocal folds vibration only focuses on the tone of speech, which does not include privacyarXiv:2303.08295v1  [eess.SP]  15 Mar 2023information. Third, previous speech recognition research has focused mainly on visual-based mouth movements, posing a risk
of lack of privacy and overlooking internal mouth movements.
In this paper, we proposed a dataset of human speech by collecting data from multiple sensors information while people are
speaking speciﬁc corpus. The contributions of our dataset are concluded in following points:
1.To the best of our knowledge, we ﬁrstly proposed a modality of silent speech recognition and collect 6 hours dataset
including the information from FMCW and UWB radar, laser, audio, video of mouth motion and processed mouth
skeleton points. The dataset is expected to reduce the labour for researchers who expect to work on SSR or speech
enhancement.
2.Our system takes into account the physical movements of all parts of the head during human speech, including mouth
movements and vibrations of the vocal cord, which is illustrated in Fig. 1.
3.The diverse range of modalities in our dataset offers ample opportunities for conducting research in the ﬁeld of
speech recognition. The range contains but is not limited to the following application: radar-based vowels and words
classiﬁcation, speaker identiﬁcation, speech enhancement in noisy environment, radar-based lip reconstruction, etc.
Figure 1. Schematic diagram of the Multimodal signals
2 Methods
Firstly, we conducted a literature review to establish the necessary sensors and experimental setup for radar-based speech
recognition, given the absence of a standard and corpus. Meanwhile, we demonstrate the availability of all the sensors we
adopted and then establish our data collection approach, referencing previous work.
Literature survey of radar-enabled speech recognition There are various kinds of sensors has been adopted for speech
research: UWB, mmWave radar and laser speckle detector6. The work of UWB demonstrated the lip reading work with
the vowels of A, E, I, O, U and static scenario, with a face mask. The result of 95% approves that the mouth motion brings
informative signal for UWB sensing7. To expand the work and exploit more possibilities, we added words and sentences
for data collection regarding the reference. Besides, mmWave FMCW radar has been used for speech enhancement in5, 6.
These two researches has distinct focus directions:5considered the distance coefﬁcient for radar signal and successfully make
speech enhancement system implementation in 7 meters; and6, 8works on audio separation of multiple speakers with radar
based spacial information. For laser-related information,9proposed a remote measurement technique for healthy individuals
that involves capturing the reﬂected laser speckle from the surface of the neck skin. This system is capable of capturing the
micro-vibration on the surface of the neck produced by blood pressure, which can also be adopted for the extraction of voice
signals without audio signals through detecting the vibration caused by throat. Therefore, we adopted these devices as the core
in our experiments.
Data acquisition scheme The overall data collection system was organised by four laptops and four types of sensors:
Microsoft Kinect V2 for audio and video including mouth landmark, X4M03 UWB radar from NOVELDA, AWR2243
mmWave radar from Texas Instrument, and laser measurement system for physical vibration of human speech. To keep the data
synchronised with different sensors, we utilised TCP/IP connection to control the distinct host laptops with the same network
time protocol (NTP) for recording the time stamp while data collection. Once we run script on master laptop, the master will
2/13send the commends to other three sockets in series. The mean delay from master to sockets of other devices is around 80 ms,
which is considered in our post synchronisation processing. Considering the potential research for speech recognition, we
designed three data collection schemes shown in the following. The adopted corpus is recorded in an additional folder in our
dataset.
• Single person speech of vowels, words and sentences.
• Dual-person speech simultaneously of complex sentences.
• Single person speech of vowels, words and sentences with different distance from radar to speakers.
The details of data collection from speciﬁc sensor are demonstrated below, with experiment setup shown in Fig. 2.
Figure 2. Data collection setup with device label in the real scenario
Speech voice We used Kinect v2 for collecting vocalised speech. Kinect v2 contains a 4-mic Microphone array. With the
enable Kinect v2 to collect the accurate acoustic information. The sample rate of audio data is 16000Hz, bit depth is 16-bit.
The frequency range of recording audio is up to 8000Hz, that can cover the frequency range of human voice.
Mouth skeletal points The Kinect v2 is also used in collecting the facial landmark information. A RGB camera and an
infrared camera are intergrated in kinect v2. By measuring the time of ﬂight (ToF) using the IR camera Kinect can get the depth
image. Meanwhile, we use the lip recognition method proposed in10for extraction of the lip skeleton, which is provided as part
of our dataset.
IR-UWB radar Like Wi-Fi and Bluetooth, UWB is a short-range wireless communication protocol. The UWB was deﬁned as
the wireless transmission system of which bandwidth exceeds 500 MHz, and each transmit pulse of this communication system
can occupy at least 500 MHz bandwidth. Instead of modulating with a carrier wave, IR-UWB relies on nanosecond (ns) to
picosecond (ps) non-sinusoidal narrow impulse radio signals to transmit data. The time-based modulation technology increases
transmission speed and reduces power consumption. For speech recognition, the UWB system has the following advantages:
1.Strong anti-interference ability: From the RF mechanism, the pulse wave emitted by UWB is more resistant to interference
than the continuous electromagnetic waves in short range. Speciﬁcally, the permitted work frequency band of UWB is
from 3 GHz to 10 GHz, which suffers less disturbance from general 2.4 GHz WiFi system and other telecommunication
signals.
2.The protocol yielded positive results, resulting in a reduction in power consumption for short-range communication
applications. The transmit power of the UWB transmitters was found to be typically less than 1 mW, which extended the
system’s operating time and minimised electromagnetic wave radiation to the human body.
After careful consideration of cost and feasibility, we have selected the XeThru X4M03, an IR-UWB radar system on chip,
as our UWB radar. The UWB RF speciﬁcations of this radar have been approved by ETSI (European Telecommunications
3/13Standards Institute) in Europe, and FCC (Federal Communications Commission) in the USA for commercial use in human
living circumstances11.This device is a highly reliable sensor that is capable of detecting objects at a range of up to 10 meters.
It is also capable of detecting objects in a wide range of angles, up to 180 degrees. This radar system has been adopted in a
variety of research projects, ranging from human vital sign detection12, 13to activity recognition14.
For pulsed radars, the range between radar and target can be calculated by R=cDT
2, where crepresents microwave speed,
DTrepresents the round-trip time of a single pulse, called time of ﬂight (ToF). The signal of IR-UWB can be represented in Eq.
1, where the trepresents the ToF of signal impulses in fast-time range, trepresents receiving time of frame in slow-time domain,
Ndis the index of the dynamic path, ai(t;t)represents the complex attenuation factor of the ithpath; e j2pd(t)
lrepresents the
phase change of ithpath; di(t)andda(t)are the static length and vibrating length of ithpath. lrepresents the wavelength of the
UWB signal.
s(t;t) =Nd
å
i=1ai(t;t)e j2p(di(t)+da(t))
l (1)
The format of the received signals is a ﬁxed set of bins determined by the of the transmitting pulses indexed by fast-time
and slow-time shown in Fig. 3. Fast-time and slow-time are two dimensions that are used to describe the format of the received
signals in a UWB radar system. Fast-time is the time it takes for the radar to transmit a pulse and receive the reﬂected signal.
Slow-time is the time it takes for the radar to transmit multiple pulses and receive the reﬂected signals. The fast-time and
slow-time dimensions are used to determine the format of the received signals, which is a ﬁxed set of bins. These bins are
used to store the information about the objects detected by the radar. The fast-time and slow-time dimensions are also used to
determine the range of objects detected by the radar. By using the fast-time and slow-time dimensions, the UWB radar system
can accurately detect objects in a wide range of distances. This makes the UWB radar system an ideal choice for a variety of
applications, including human vital sign detection and activity recognition.
Figure 3. IR-UWB impulse signal format
For data collection, we adopted Moduleconnector API which is supported by the radar on Windows MATLAB 2021b. The
detailed parameters of radar is listed in the Table 1a.
mmWave FMCW radar Although the IR-UWB radar is able to capture the vibration of sub-centimeter motion. The angle
resolution is limited by the number of antennas. Meanwhile, for comparison of speech recognition performance using different
modulation-based radars, we added one commercial off-the-shelf (COTS) 77 GHz FMCW radar, AWR2243, for data collection.
This high frequency enables radar signal to capture motion in millimeter, that can be used for both lip motion and vocal folds
detection. FMCW radar signal can be illustrated as Fig. 4.
Unlike IR-UWB radars that measure distance using the ToF of instantaneous impulses, FMCW radars rely on the difference
in frequency between the transmitted and received signal from a linear variation of signal frequency. In other words, Doppler
effect of moving target can be explored from difference between transmission frequency ( ft) and shifted frequency ( fs). The
formula shows Doppler velocity of target relative to radar: v=c(fs ft)
2ft.
Except for modulation methods, AWR2243 radar contains 4 receive antennas and 3 transmit antennas, which is possible to
adopt angle of arrival (AoA) for research in horizontal plane. In our experiment, we adopt 4 receive and 1 transmit antennas
4/13Figure 4. The plot of mmWave FMCW radar chirp signal in time and frequency domain
Parameter Value
Center frequency 8.745 GHz
Sampling frequency 23.328 GHz
Frame rate 300 Hz
Bandwidth 1.5 GHz
Number of antennas 1 Tx and 1 Rx
(a)X4M03 setupParameter Value
Center frequency 8.745 GHz
Sampling frequency 23 GHz
Frame rate 1018 Hz
Bandwidth 4GHz
Number of antennas 1 Tx and 4 Rx
No.of Sample per chirp 512
No. of Chirp per frame 1
(b)AWR2243 setup
Table 1. Recognition accuracy vs User identity
to increase the sample rate. In data collection, we utilised mmWaveStudio API on Windows with the conﬁguration ﬁle. The
parameters were set to the value shown in Table 1b.
Laser The laser measurement system consists of a 532nm green laser diode (DJ532-40, Thorlabs) as transmitter and a
high-speed CMOS camera from Basler as receiver, where the laser diode emits a laser beam pointing to the face outline of the
testing subject and the camera captures the reﬂected laser speckle patterns. Both transmitter and receiver are ﬁxed on a 1.2m
tripod, and the camera is connected with a laptop via an USB 3.0 cable for powering and data transferring. The green laser
diode has a distance of approximately 1 m to the participants, it will produce an illumination spot of around 5mm diameter on
the human skin by considering the beam divergence. For the laser safety, laser power exposed on human skin is controlled
to be less than 0.5 mw (CLASS 1), therefore it is safe for long-term eye and skin exposure. The focal length and f-stop of
the camera objective are set as 25 mm and 0.95, respectively, allowing the camera system to detect the laser speckle from a
very close range (0.1 m) to a relative far range (up to 3 m). Additionally, the size of the region of interest (ROI) window is
chosen as 128x128 pixels, and the camera exposure time is set as 600 µs. The laser and camera are carefully aligned before the
experiments to ensure that the selected ROI includes the movements of speckles. For every single measurement, the collected
data is in a format of WHN, where W and H is represents the width and height of the ROI, respectively, and the N equals
to the number of frames within the measuring period. In our case, N is correlated with the sampling frequency of the CMOS
camera, which is set as 1.47 kHz.
Participant There are 20 volunteers contributing to our experiment, whom come from different country regions including
Europe, China, Pakistan and UK. Our dataset for speech recognition presents both opportunities for generalization and
challenges due to the volunteers’ diverse backgrounds, resulting in distinct accents. To the issue bring from body size, we
adopted an adjustable table for subjects that can keep the relative distance same between the head of speaker and different
sensors. The feature of accents and speech habits can be extracted from our dataset with lip motion, vocal folds vibration, and
audio, which is potential for related multimodal ASR research.
5/13(a)Top View
 (b)Front View
Figure 5. Detailed setup schematic diagram for single person scenario (Laser’s location is not ﬁxed due to the camera based
signal process only require the laser directly point to skin of subjects. The UWB radar facing to subject directly was called
’xe2’ in dataset folder, another is called ’xe1’)
Meanwhile, all participants were informed about the purpose of the study and what was expected of them. Experiment
consent forms were obtained from each participant prior to the experiments. Whole project of dataset collection was ethical
approved by University of Glasgow College of science and engineering (approval no: 300210309).
3 Data Record
The multimodal speech detection dataset is accessible with the link of https://nextcloud.gla.ac.uk/s/LJHKyBxLHXdk4xZ. The
corpus of the single person scenario is listed in Table 2. During data collection, we asked volunteers to pronounce a speciﬁc
vowel / word / sentence with timestamps on laptops. All laptops were synchronised using the same NTP server. During
collection, volunteers were guided by automatic voice instruction to read the corpus and relax. The timestamps of audio
instruction were instantaneously recoreded in kinect/timestamp. However, there is a few seconds of uncontrolled latency in
activating all radars and laser equipment, which disrupted the devices’ ability to synchronise acquisition data. In this case, we
decided to keep these devices recording for one minute and write timestamp while data collection is activated so that the signal
can be cropped according to kinect timestamps. All were recorded alongside the data, which reduces the effort for manually
separating the data.
Meanwhile, we also collected data via different distances in the single-person scenario and the two-person scenario, with
corpus listed in Table 3. Instead of original 60cm, we asked volunteers to sit 1:2mand2:2maway from radar equipment,
respectively, which is a potential for researchers to explore the relationship of radar-based audio detection with distance.
Besides, in two-person experiment, we kept one volunteer sitting in the same place as the single-person scenario shown in
Fig. 5, then let another speaker sit on the left side of the ﬁrst-mentioned volunteer. The two subjects were asked to normally
read different corpus shown in Table 4, which was shown on the screen in one minute, without repeating words. The laser
equipment was pointed to the ﬁrst volunteer, and the kinect camera only took information from another subject. This kind of
dataset will contribute to multiple audio source separation.
Data storage structure After saving the data, all ﬁles were integrated into speciﬁc folders according to the data class, which
is illustrated in Fig. 6. The entire dataset was divided into raw data and processed data to match the limitation of ﬁle size.
Firstly, due to the data size limitation of ﬁle, we put mmWave FMCW radar data and laser data in a separate folder and other
sensors in another. The radar signal ﬁles were kept in binary format with radar timestamps in text format. We provided the
script for reading the timestamps along with radar signal in the code section demonstrated in Section 6. Meanwhile, information
from kinect and two UWB radars was kept in same folder as the similar storage structure, which contains timestamps in JSON
format, audio in WA V , landmarkers of user’s head, and UWB radar signals in MAT format. Additionally, to ensure licence-free
distribution of the dataset, the preprocessed data was converted to .npy, .csv and .wav ﬁles regarding the usages.
4 Technical validation
The effectiveness of the collected data is validated in two parts for validation and benchmark: signal processing and analysis
and UWB radar-based classiﬁcation.
6/13Type Corpus Index Participants
vowel a, e, i, o, u1-5
in sequenceUser 1 - 20
wordorder, assist, help, ambulance, bleed,
fall, shock, medical, sanitize, doctor,
accident, rescue, emergency, heart, break1-15
in sequenceUser 1 - 20
sentencesI need help. 1
User 1 - 20Call for an ambulance. 2
The building’s on ﬁre. 3
Can you smell smoke. 4
Where’s the ﬁre escape. 5
There’s been an accident. 6
Is there a doctor here? 8
The staff sanitized the sickroom. 7
User 1,2,4,6,7 Medical care is important. 9
Don’t worry about bleeding. 10
I am having trouble breathing. 7
User 3, 8 - 13 I think I’m having a heart attack. 9
My heart is failling. 10
Need emergency treatment at shock stage. 7
User 5, 14 - 20 He need a rescue for a heart attack. 9
Don’t worry about falling. 10
Table 2. Corpus list for single subject experiment. The index of each participant is identical to the user label.
Type Corpus Label Participants
vowels a, e, i, o, uv1-5 in
sequence
User 4 of 1.2m and 2.2m
(Index No. 24 and 25 in dataset),
User 5 of 1.2m and 2.2m
(Index No. 26 and 27 in dataset)wordsorder, ambulance, medical,
sanitize, accidentw1-5 in
sequence
sentencesCall for an ambulance s1
There’s been an accident s2
The staff sanitized the sickroom s3
Is there a doctor here? s4
Medical care is important. s5
Table 3. Corpus list for supplementary experiments of changing the distance.
Type Corpus Label Participants
articleFrom view of Kinect, volunteer
on the left side read ’Mr Sticky’,
on the right side read
’The king of the birds’.b1-11 User 6 (Left) and User 4 (Right), recorded in Index 21
b12-22 User 4 (Left) and User 6 (Right), recorded in Index 21
b1-11 User 4 (Left) and User 5 (Right), recorded in Index 22
b12-23 User 5 (Left) and User 4 (Right), recorded in Index 22
b1-11 User 5 (Left) and User 1 (Right), recorded in Index 23
b12-23 User 1 (Left) and User 5 (Right), recorded in Index 23
static sitting without speaking b24-26 User 5 (Left) and User 4 (Right), recorded in Index 22
Table 4. Corpus list for supplementary experiments of two-person scenario. The reading materials are referred from corpus
publication15.
7/13Figure 6. The structure of the multimodal speech dataset
8/13Signal analysis In this section, we analyse the entire process of lip motion and vibration of the vocal fold combined with
video frames of the skeletal mouth and information about the voice, as shown in Fig. 7.
For UWB and FMCW radar signals, we transferred the raw data to the Doppler spectrum, shown with the speech spectrum
and skeleton motion. The Fig. 7. shows all synchronised data types that were collected in the dataset. For UWB data, to sanitise
the stationary object, the raw signal was ﬁrst multiplied by a moving target indication (MTI) ﬁlter, which is a radar process
method that allows the UWB radar to detect and track targets that are moving in relation to the radar devices. From Eq. 1, we
know that the channel impulses indicate different ranges. To consider all channel vibrations, we calculate the short-time Fourier
transformation (STFT) result on each channel and then add all channels together, which is shown in Fig. 7.
For FMCW radar, we ﬁrst transfer the IQ data to range-bin data through 1D-FFT. Then, instead of adopting the velocity
dimension with multiple chirps, we increase the frame rate to obtain continuous and smooth radar signals. Because in our case,
we are not interested in the velocity of mouth and vocal folds, but in the frequency response reﬂected by radar signals. The
frequency range of vocal folds is from 80Hzto400Hz16. Our radar’s frame rate is adjusted to 1017 Hz, which ﬁts the minimum
requirement of Nyquist criterion, which is the sampling rate should be more than twice of the highest frequency contained in
the signal. Next, considering the wide range of radar that can be obtained, we introduce the FFT in the AoA dimension to
obtain angle information regarding the location of the radar, which can be demonstrated in17. Meanwhile, MTI was applied to
ﬁlter out the noise reﬂected by static object. We subtract the two spectrograms at a speciﬁc time interval which can reduce the
occurrence of false alarms resulting from signiﬁcant indoor clutter to a certain degree. Through getting the radar strength of the
range of interest, which represents the location of the human, we get the information about the movement of the mouth and the
vibration of the vocal folds.
In addition, we transfer the video to images with 30 frame per second and voice signal in spectrum, shown in Fig. 7 together
with Doppler spectrograms of UWB and FMCW radar signals. To retrieve the sound signal from raw laser data, an optical
ﬂow-based method, notably, the Farneback algorithm, is utilised to estimate the displacement of laser speckles on participants’
faces. The input of this algorithm is every frame, denoted as a 2-D function f(x;y), whereas a quadratic polynomial expansion
is adopted to approximate the grey value of each pixel and its neighbours. The signal model based on the local coordinates of
the selected pixel could be written as the Eq. 2.
f(x) =xTMx+nT+q (2)
where x is the local coordinate (x;y),Mis a symmetric matrix equal to
C4C6=2
C6=2 C5
, n is a vector equal to
C2
C3
and q is a
scalar equal to C1,C1toC6are the coefﬁcients of the quadratic polynomial expansion. The new signal could be expressed
using a displacement index Ddas the Eq. 3 indicates.
f(x Dd) = ( x Dd)TM1(x Dd) +nT
1(x Dd) +q1 g(x) =xTM2x+nT
2+q2 (3)
Simply let f(x Dd) =g(x), then we can get n2=n1 2M1Dd, leading to the solution of displacement index. Then the
computed optical ﬂow needs to be ﬁltered with a band-pass ﬁlter. The cut-off frequency of the ﬁlter is chosen to be 80 and
255 Hz for removing the frequency components caused by non-speaking activities such as head and skin movement. Then
we integrate all sorts of cropped data that were mentioned above and observe that the speech features in different ranges are
synchronised among all kinds of signals by intuition.
Multimodal words recognition For the benchmark of multimodal speech classiﬁcation, we gathered UWB radar, mmWave
radar and audio data of 20 subjects and established CNN-based ResNet classiﬁcation network. In this case we only adopt one
UWB radar that faces the person. Meanwhile, we further consider a sensor fusion scheme that combines data from mmWave
radar and UWB radar for radio-based word recognition. We employ a multi-input ResNet18 for this task, which includes
two input blocks consisting of a convolutional layer, a batch normalisation layer, and a ReLU activation, as shown in Fig. 8.
The initial feature extraction is completed by feeding spectrograms from the mmWave and UWB radar into their respective
input blocks. The resulting feature maps are then stacked along the channel axis and processed by ResNet18 for ﬁnal analysis.
Then, we also applied audio data on word recognition as comparison with radio-based methods. The performance of the four
mentioned systems are shown in Fig. 9. They illustrated that the combination of two radar modalities can assist system to get
better results. It is also a challenge from the data fusion side that how to combine multi-modality information for different tasks.
5 Usage Notes
The reader is encouraged to analyse the data with the example script provided with the dataset. The functionalities of the script
are described in the following section. Meanwhile, processing methods are not limited to the script we provide. We hope that
readers can utilise the dataset regarding speciﬁc target. We would like to recommend that the user who wants to process their own
9/13Figure 7. Multimodal data illustration including UWB and mmWave radar signal, laser, audio, image, and mouth skeleton
points. From left to right columns, the ﬁrst represents the volunteer is speaking of vowel ’o’, second is speaking of word
’bleed’, and the third is speaking of sentence ’There’s been an accident’. The last row illustrates the camera vision of the
volunteer’s mouth with the processed skeleton.
10/13Figure 8. Multi-modal sensor fusion scheme for word speech classiﬁcation.
(a)Confusion matrix of word classiﬁcation based on UWB radar.
 (b)Confusion matrix of word classiﬁcation based on mmWave radar.
(c)Confusion matrix of word classiﬁcation based on two modalities of
UWB and mmWave radar.
(d)Confusion matrix of word classiﬁcation based on audio signal.
Figure 9. Results of human speech classiﬁcation task among 15 words labelled with UWB radar, mmWave radar, combination
of two modalities of radars, and audio signals, respectively.
11/13algorithm on raw data, pay attention to the synchronisation among multiple sources with provided timestamps. Meanwhile, we
created a github Repo for dataset code update of: https://github.com/G-Bob/Multimodal-dataset-for-human-speech-recognition.
The video resources cannot be accessed in this dataset because they pertain to privacy concerns of vol. Please contact us by
email to acquire further dataset with proper usage and data management licence.
6 Code availability
Matlab and Python scripts are provided in the codes directory of dataset for the users to replicate some of the ﬁgures:
 FMCW_Radar_process.m This script is used to load the raw signal recorded by the AWR2243 radar. Then it is used to
visualise the ﬁrst and second FFT through distance dimension and angle dimension, respectively. Lastly, by reading the
human location’s phase variation, we can get human-related signals, including lip motion.
 UWB_radar_process.m This script is used to load the raw signal recorded by the Xethru X4M03 radar and process the
data to STFT spectrums.
 plot_InParallel.m This script provides a template to plot the spectrums that are shown on paper. First, the preprocessing
data is needed to be downloaded.
 uwb_cutting.py; mmWave_cutting.py; kinect_cutting.py; laser_cutting.py; This script can be utilised to cut the sequence
with given Kinect timestamp. In advance of this step, the radar signal should be processed to spectrums. This step can be
used with the radar scripts we provide.
References
1.Cai, C., Zheng, R. & Luo, J. Ubiquitous acoustic sensing on commodity iot devices: A survey. IEEE Commun. Surv. &
Tutorials 24, 432–454, 10.1109/COMST.2022.3145856 (2022).
2.Benzeghiba, M. et al. Automatic speech recognition and speech variability: A review. Speech Commun. 49, 763–786,
https://doi.org/10.1016/j.specom.2007.02.006 (2007). Intrinsic Speech Variations.
3.Gonzalez-Lopez, J. A., Gomez-Alanis, A., Martín Doñas, J. M., Pérez-Córdoba, J. L. & Gomez, A. M. Silent speech
interfaces for speech restoration: A review. IEEE Access 8, 177995–178021, 10.1109/ACCESS.2020.3026579 (2020).
4.Bednar, A. & Lalor, E. C. Where is the cocktail party? decoding locations of attended and unattended moving sound
sources using eeg. NeuroImage 205, 116283, https://doi.org/10.1016/j.neuroimage.2019.116283 (2020).
5.Liu, T. et al. Wavoice: A noise-resistant multi-modal speech recognition system fusing mmwave and audio signals. In
Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems , SenSys ’21, 97–110, 10.1145/3485730.
3485945 (Association for Computing Machinery, New York, NY , USA, 2021).
6.Ozturk, M. Z., Wu, C., Wang, B. & Liu, K. J. R. Radiomic: Sound sensing via radio signals. IEEE Internet Things J. 10,
4431–4448, 10.1109/JIOT.2022.3217968 (2023).
7.Hameed, H. et al. Pushing the limits of remote rf sensing by reading lips under the face mask. Nat. Commun. 13, 5168
(2022).
8.Ozturk, M. Z., Wu, C., Wang, B., Wu, M. & Liu, K. R. Beyond microphone: mmwave-based interference-resilient voice
activity detection. In Proceedings of the 1st ACM International Workshop on Intelligent Acoustic Systems and Applications ,
7–12 (2022).
9.Cester, L. et al. Remote laser-speckle sensing of heart sounds for health assessment and biometric identiﬁcation. Biomed.
Opt. Express 13, 3743–3750 (2022).
10.Haliassos, A., V ougioukas, K., Petridis, S. & Pantic, M. Lips don’t lie: A generalisable and robust approach to face forgery
detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 5039–5049 (2021).
11.Thullier, F., Beaulieu, A., Maître, J., Gaboury, S. & Bouchard, K. A systematic evaluation of the xethru x4 ultra-wideband
radar behavior. Procedia Comput. Sci. 198, 148–155, https://doi.org/10.1016/j.procs.2021.12.222 (2022). 12th International
Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and
Future Trends of Information and Communication Technologies in Healthcare.
12.Yang, Z., Bocca, M., Jain, V . & Mohapatra, P. Contactless breathing rate monitoring in vehicle using uwb radar. In
Proceedings of the 7th International Workshop on Real-World Embedded Wireless Systems and Networks , RealWSN’18,
13–18, 10.1145/3277883.3277884 (Association for Computing Machinery, New York, NY , USA, 2018).
12/1313.Zheng, T., Chen, Z., Cai, C., Luo, J. & Zhang, X. V2iﬁ: In-vehicle vital sign monitoring via compact rf sensing. Proc.
ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 10.1145/3397321 (2020).
14.Li, H., Shrestha, A., Heidari, H., Le Kernec, J. & Fioranelli, F. Bi-lstm network for multimodal continuous human activity
recognition and fall detection. IEEE Sensors J. 20, 1191–1201, 10.1109/JSEN.2019.2946095 (2020).
15.Futrell, R. et al. The natural stories corpus: a reading-time corpus of english texts containing rare syntactic constructions.
Lang. Resour. Eval. 55, 63–77 (2021).
16.Hollien, H. V ocal fold dynamics for frequency change. J. Voice 28, 395–405, https://doi.org/10.1016/j.jvoice.2013.12.005
(2014).
17.Kim, S. & Lee, K.-K. Low-complexity joint extrapolation-music-based 2-d parameter estimator for vital fmcw radar. IEEE
Sensors J. 19, 2205–2216, 10.1109/JSEN.2018.2877043 (2019).
Acknowledgements
This work was supported in parts by Engineering and Physical Sciences Research Council (EPSRC) grants EP/T021020/1 and
EP/T021063/1 and by the RSE SAPHIRE grant.
Author contributions statement
Y .G., C.T. and H.L. conceived the whole experiment, Q.A., D.F., K.C., W.L. and M.I. reviewed the experiment plan and
supervised the whole experiment. H.L., C.T., Y .G. and Q.A. made the necessary applications and obtained the risk assessment
and ethics approvals prior to the experiment. Y .G., H.L. and Z.C. conducted the experiment and recorded the experimental data
from mentioned multiple modalities. C.T. and Y .G. analyzed the datasets. Y .G., C.T. and H.L. wrote the scripts in Matlab and
Python for loading and analyzing the dataset from each modality. Y .G. lead to sort the time synchronisation and document
experiments in this manuscript with the assistance by C.T., H.L. and Z.C. C.T. and Z.C. managed to write the classiﬁcation
network as part of validation. All authors reviewed the manuscript, provided constructive feedback and assisted with the editing
of the manuscript prior to submission.
Competing interests
The authors declare no competing interests.
13/13