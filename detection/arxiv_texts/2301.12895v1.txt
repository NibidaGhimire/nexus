DEEP LEARNING NUMERICAL METHODS FOR
HIGH-DIMENSIONAL FULLY NONLINEAR PIDES AND COUPLED
FBSDES WITH JUMPS
WANSHENG WANGy, JIE WANGz, JINPING LIx, FEIFEI GAO{,AND YI FUk
Abstract. We propose a deep learning algorithm for solving high-dimensional parabolic integro-
dierential equations (PIDEs) and high-dimensional forward-backward stochastic dierential equa-
tions with jumps (FBSDEJs), where the jump-diusion process are derived by a Brownian motion
and an independent compensated Poisson random measure. In this novel algorithm, a pair of deep
neural networks for the approximations of the gradient and the integral kernel is introduced in a
crucial way based on deep FBSDE method. To derive the error estimates for this deep learning algo-
rithm, the convergence of Markovian iteration, the error bound of Euler time discretization, and the
simulation error of deep learning algorithm are investigated. Two numerical examples are provided
to show the eciency of this proposed algorithm.
Key words. parabolic integro-dierential equations, forward-backward stochastic dierential
equations with jumps, deep learning, error estimates
AMS subject classications. 60H35, 65C20, 65M15, 65C30, 60H10, 65M75
1. Introduction. The purpose of this paper is to derive error estimates for
the proposed deep learning algorithm for solving high-dimensional parabolic integro-
partial dierential equations (PIDEs) which can be represented by high-dimensional
forward-backward stochastic dierential equations with jumps (FBSDEJs), because
of the generalized nonlinear Feynman-Kac formula [2].
PIDEs and FBSDEJs mathematical models have been widely employed in various
applications such as stochastic optimal control [29, 30, 35, 3, 34], mathematical nance
[14, 11, 34], and so on. The existence, uniqueness and regularity of the solution to the
two classes of equations have been also examined by many researchers at about the
same time (see, for example, [29, 30, 35, 3, 34, 14, 33, 42, 39]). Due to the complex
solution structure, however, explicit solutions of PIDEs and FBSDEJs can seldom
be found. Consequently, one usually resorts to numerical methods to solve the two
kinds of equations, and a volume of work has been performed on their numerical
solutions. IMEX time discretizations combined with nite dierence method, nite
element method, or spectral method, have been used to solve low-dimensional PIDEs
(see, for example, [1, 31, 26, 36, 37, 28]), and multistep and prediction-correction
schemes have been used to low-dimensional FBSDEJs (see, for example, [41, 40, 15]).
With the increase of dimensionality, the traditional grid-based numerical method
is no longer suitable for high-dimensional problems, and its computational complexity
will increase exponentially, resulting in the so-called \curse of dimensionality" [5].
Therefore, the resolution of nonlinear partial dierential equations (PDEs) in high
This work was supported by grants from the National Natural Science Foundation of China
(Grant Nos. 12271367, 11771060), Science and Technology Innovation Plan Of Shanghai Science
and Technology Commission (No. 20JC1414200), and sponsored by Natural Science Foundation of
Shanghai, China (No. 20ZR1441200).
yCorresponding author, Department of Mathematics, Shanghai Normal University, Shanghai,
200234, China ( w.s.wang@163.com ).
zDepartment of Mathematics, Shanghai Normal University, Shanghai, 200234, China.
xSchool of Science, Hainan University, Haikou, China
{Department of Mathematics, Shanghai Normal University, Shanghai, 200234, China.
kSchool of Finance and Business, Shanghai Normal University, Shanghai, 200234, China.
1arXiv:2301.12895v1  [math.NA]  30 Jan 20232 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
dimension has always been a challenge for scientists. Recently, based on the Feyman-
Kac representation of the PDEs, branch diusion process method and Monte Carlo
method have been studied; see, for example, [16, 21, 22, 38].
In recent years, machine learning and deep learning have played a great role in
many elds, such as as image recognition, automatic driving, natural language pro-
cessing and so on. This also provides a new idea for numerical approximation of
high-dimensional functions, which has attracted more and more scholars' attention,
since these approximation methods can overcome the problem of \curse of dimension-
ality". Still based on Feyman-Kac representation of the PDEs, some machine learning
techniques (see, for example, [10]) have proposed to solve the high-dimensional prob-
lems. With multilevel techniques and automatic dierentiation, multi-layer Picard
iterative methods have been developed for handling some high-dimensional PDEs
with nonlinearity (see, for example, [13, 24, 25]). Using machine learning representa-
tion of the solution, the so-called Deep Galerkin method has proposed to solve PDEs
on a nite domain in [32]. On basis of the backward stochastic dierential equation
(BSDE) approach rst developed in [29], a neutral network method was proposed
to solve high-dimensional PDEs in the pioneering papers [19, 12]. The idea of this
algorithm is to view the BSDE as a stochastic control problem with the gradient of
the solution being the policy function, which can be approximated by a deep neural
network by minimizing a global loss function. Deep learning backward dynamic pro-
gramming (DBDP) methods, including DBDP1 scheme and DBDP2 scheme, in which
some machine learning techniques are used to estimate simultaneously the solution
and its gradient by minimizing a loss function on each time step, were proposed in [23].
The DBDP1 algorithm has been extended to the case of semilinear parabolic nonlocal
integro-dierential equations in [9]. Quite recently, a new deep learning algorithm
was proposed to solve fully nonlinear PDEs and nonlinear second-order backward
stochastic dierential equations (2BSDE) by exploiting a connection between PDEs
and 2BSDEs [4].
It is worth noting that most of the above-named approximation methods are only
applicable in the case of semilinear PIDEs or nonlinear PDEs. To the best of our
knowledge, only the papers by Gonon and Schwab [17, 18], and Castro [9] are devoted
to the deep learning approximations of the numerical solution of linear and semilinear
PIDEs. At the moment there exists no practical algorithm for high-dimensional fully
nonlinear PIDEs in the scientic literature. Consequently, the numerical solution of
high-dimensional nonlinear PIDEs remains an exceedingly dicult task and deserves
further study. In this work, we propose a new algorithm for solving fully nonlinear
PIDEs and nonlinear FBSDEJs. The proposed algorithm exploits a connection be-
tween PIDEs and FBSDEJs to obtain a merged formulation of the nonlinear PIDE
and the coupled FBSDEJs, whose solution is then approximated by combining a Euler
time discretization with a Markovian iteration [6] and a neural network-based deep
learning procedure. The error estimates of this new FBSDE algorithm (we refer to
the algorithm as FBSDE since it is based on forward-backward stochastic dierential
equations but not only backward stochastic dierential equation) are then derived by
bounding the time discretization error and deep learning error, and by showing the
convergence of Markovian iteration.
The paper is organized as follows. We start by introducing the deep learning-
based algorithm for FBSDEJs and related PIDEs in Section 2. In Section 3, the
assumptions for theoretical analysis are made and the main error estimates are given.
To prove this main results, we show the convergence of Markovian iteration, bound theDEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 3
time discretization error, and derive the simulation error of deep learning in Sections
4, 5, and 6, respectively. Several numerical experiments with the proposed scheme
are presented in Section 7. In Section 8 we nally conclude with some remarks.
2. Deep learning-based schemes for nonlinear PIDEs and coupled FB-
SDEJs. In this section, we introduce the details about deep learning-based schemes
for solving coupled FBSDEJs and the associated nonlinear PIDEs. We deal with
nonlinear PIDEs in three steps.
We formulate the PIDEs as FBSDEJs.
By taking \control part" and \integral kernel" as policy functions, we view
FBSDEJs as a stochastic control problem.
We use a deep neural network to approximate high-dimensional policy func-
tion.
2.1. Nonlinear PIDEs and coupled FBSDEJs. Letjjdenote the Euclidean
norm in the Euclidean space, and Cl;kdenote the set of functions f(t;x) with con-
tinuous partial derivatives up to lwith respect to tand up tokwith respect to x.
Let (
;F;F;P),F= (Ft)0t<T, be a stochastic basis such that F0contain all zero
P-measure sets, and Ft+,T
>0Ft+=Ft. The ltration Fis generated by a d-
dimension Brownian motion (BM) fWtg0t<Tand a Poisson random measure on
R+E, independent of W. In this subsection, we establish a connection between
nonlinear PIDEs and coupled FBSDEJs.
Let us consider the following nonlinear PIDEs
(
@tu+Lu+f(t;x;u;T(t;x;u )rxu;B[u]) = 0;(t;x)2[0;T)Rd;
u(T;x) =g; x2Rd;(2.1)
whered1 andT > 0,g:Rd!Ris terminal condition, the second-order nonlocal
operatorLis dened as follows:
Lu=1
2Tr(T(t;x;u )@2
xu) +hb(t;x;u );rxui
+Z
E(u(t;x+(t;x;u;e )) u(t;x) hrxu;(t;x;u;e )i)(de);
andBis an integral operator
B[u] =Z
E(u(t;x+(t;x;e )) u(t;x))(e)(de):
Hereb(t;x;y ): [0;T]RdR!Rd,(t;x;y ): [0;T]RdR!Rdd,(t;x;y;e ):
[0;T]RdRE!Rd, andf: [0;T]RdRRdR!Rare deterministic and
Lipschitz continuous functions of linear growth which are additionally supposed to
satisfy some weak coupling or monotonicity conditions, ATdenotes the transpose of a
vector or matrix A,E,Rdnf0gis equipped with its Borel eld E, with compensator
(de;dt ) =(de)dt, for some measurable functions :E!Rsatisfying
sup
e2Ej(e)jK; (2.2)
and(de) is assumed to be a -nite measure on ( E;E) satisfying
Z
E(1^jej2)(de)<1:4 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
Letu(t;x)2C1;2([0;T]Rd) be the unique viscosity solution of (2.1). Then by
the It^ o formula, one can show that the solution uadmits a probabilistic representation,
i.e., we have (see [27] and [2]),
u(t;Xt) =Yt; (2.3)
and furthermore, the following relationship holds
8
><
>:Zt=rxu(t;Xt)(t;Xt;u(t;Xt));
Ut=u(t;Xt +(t;Xt ;u(t;Xt );e)) u(t;Xt )
 t=B[u(t;Xt)];(2.4)
where the quadruplet ( Xt;Yt;Zt; t) is the solution of the coupled FBSDEJs
8
>>><
>>>:Xt=+Zt
0b(s;Xs;Ys)ds+Zt
0(s;Xs;Ys)dWs+Zt
0Z
E(Xs ;Ys ;e)~(de;ds );
Yt=g(XT) +ZT
tf(s;Xs;Ys;Zs; s)ds ZT
tZsdWs ZT
tZ
EUs(e)~(de;ds );(2.5)
The quadruplet ( Xt;Yt;Zt; t) are called the \forward part", the \backward part",
the \control part" and the \jump part", respectively. The presence of the control part
Ztis crucial to nd a nonanticipative solution. The above formulas (2.3)-(2.4) are the
so-called nonlinear Feynman-Kac formulas, and such formulas indicate an interesting
relationship between solutions of FBSDEJs and PIDEs.
Note that the FBSDEJs (2.5) is coupled since the coecients b;  anddepend
onYt. When the coecients b;  andare independent of Yt, the FBSDEJs (2.5)
is called decoupled and can be solved in sequence. Using nonlinear Feynman-Kac
formulas (2.3)-(2.4) and the decoupled FBSDEJs, the deep learning algorithm DBDP1
proposed in [23] has been extended to the semilinear PIDEs in [9].
2.2. Deep neural network (DNN). In this subsection, we give a brief intro-
duction about Deep neural networks (DNN). DNN provides eective method to solve
high-dimensional approximation problems, and it is a combination of simple func-
tions. In the past decades, there exist several type of neutral network, including Deep
feedfoward neutral network, convolutional neural network (CNN) and the recurrent
neural network (RNN), et.al. Deep feedforward neutral network is the simplest neural
network, but it is sucient for most PDE problems. Since it is a class of universal
neural network, we consider Deep feedforward neural network in this paper.
Letm`(`= 0;:::;L ) be the number of neurous in the `th layers,Lis layer of
neural network. The rst layer is the input layer, the last layer is the output layer,
and another layers are the hidden layers. A feedforward neural network can be dened
as the composition
x2Rd!NLNL 1:::N1(x)2Rd1; (2.6)
wheredis the dimension of xand the output dimension d1=k;k2R+. We x
d1= 1 in this paper, and
8
><
>:N0(x) =x2Rd;
N`(x) =%(w`N` 1(x) +b`)2Rm`;for 1`L 1;
NL(x) =wLNL 1(x) +bL2R;DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 5
where w`2Rm`m` 1andb`2Rm`denote the weight matrix and bias vector,
respectively, %is a nonlinear activation function such as the logistic sigmoid function,
the hyperbolic tangent (tanh) function, the rectied linear unit (ReLU) function and
other similar functions. We use the ReLU function for all the hidden layers in this
paper. The nal layers NL(x) is typically linear.
Letdenote the parameters of the neural network:
:=fw`;b`g; ` = 1;:::;L:
The DNN is trained by optimizing over the parameters by (2.6).
2.3. Time discretization of the coupled FBSDEJs. We rst need to dis-
cretize equation (2.5). We consider a partition of the time interval [0 ;T]:
: 0 =t0<t1<<tN=T; N2N
with modulus h= maxn=0;1;Ntn, tn=tn+1 tn. Then a natural time dis-
cretization of equation (2.5) is by classical Euler scheme:
X
tn+1=X
tn+b(tn;X
tn;Y
tn)tn+(tn;X
tn;Y
tn)Wtn
+Z
E(tn;X
t 
n;Y
tn;e)~(de;(tn;tn+1]); (2.7)
Y
tn+1=Y
tn f(tn;X
tn;Y
tn+1;Z
tn; 
tn)tn+Z
tnWtn
+Z
EU
tn(e)~(de;(tn;tn+1]); (2.8)
where Wtn=Wtn+1 Wtnand  t=R
EU
t(e)(e)(e). Note that (2.8) is an
explicit discretization. For the implicit discretization, which is formulated as replacing
f(tn;X
tn;Y
tn+1;Z
tn; 
tn) withf(tn;X
tn;Y
tn;Z
tn; 
tn), the same conclusions hold as
we state in Theorem 3.1 for the explicit discretization.
2.4. Deep learning-based approximations of coupled FBSDEJs. We al-
ready formulate the PIDEs equivalently as FBSDEJs by nonlinear Feyman-Kac for-
mula. Let the quadruplet ( X
t;Y
t;Z
t;U
t) be the solution of (2.7)-(2.8) with
Y
t=u(t;X
t) (2.9)
and
(
Z
t=T(t;X
t;Y
t)rxu(t;X
t);
U
t(e) =u(t;X
t +(t;X
t ;Y
t;e)) u(t;X
t );(2.10)
whereu(t;x) is the solution to nonlinear PIDEs (2.1). We can approximate Zt,Utby
a deep learning algorithm. We employ the following formulas as the policy functions:
T(tn;X
tn;Y
tn)rxu(tn;X
tn)2R1d; x2Rd; n20;1;:::;N; (2.11)
u(tn;X
t 
n+(t;X
t 
n;Y
tn;e)) u(tn;X
t 
n)2R; x2Rd; n20;1;:::;N: (2.12)
More specicity, letting 2Nbe the number of parameters in the neural network and
2R, our goal becomes nding appropriate functions A;:Rd!R,B;
tn:Rd6 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
R!Rd,n2f0;1;;N 1g, andC;
tn(e) :RdR!R,n2f0;1;;N 1g, such
thatA;,B;
tnandC;
tncan serve as good surrogates of Y0,ZtandUt, respectively.
For all appropriate 2R, we deneA;:Rd!Ras suitable approximation of
u(0;):
A;u(0;): (2.13)
and
B;
tnT(tn;X
tn)rxu(tn;X
tn); (2.14)
C;
tn(e)u(tn;X
t 
n+(tn;X
t 
n;e)) u(tn;X
t 
n): (2.15)
Combining (2.7), (2.8), (2.13), (2.14) and (2.15) leads to
8
>>>>>>>>>>>>>>><
>>>>>>>>>>>>>>>:X
0=; Y
0=A;;
X
tn+1=X
tn+b(tn;X
tn;Y
tn)tn+(tn;X
tn;Y
tn)Wtn
+Z
E(X
tn;Y
tn;e)~(de;(tn;tn+1]);
Z
tn=B;
tn(X
tn;Y
tn); U
tn(e) =C;
tn(X
tn;Y
tn;e);
Y
tn+1=Y
tn f
tn;X
tn;Y
tn+1;Z
tn;Z
EU
tn(e)(e)(de)
tn
+Z
tnWtn+Z
EU
tn(e)~(de;(tn;tn+1]):(2.16)
Now, we set the loss function as squared approximation error
7!inf
Eh
jY
T g(XT)j2i
; (2.17)
associated to the terminal condition of the FBSDEJs. We then obtain the appropriate
by minimizing the expected loss function through stochastic gradient descent-type
algorithms (SGD).
3. Assumptions and main results. In this paper, we will derive the a poste-
riori error estimates for the deep learning algorithm proposed in (2.16). To do this,
we need to introduce some notations:  x=x1 x2, y=y1 y2, z=z1 z2,
v=v1 v2, and make several assumptions.
assumption 1.
(i)There exist constants k1<0,k2<0andkg>0such that
Txk1jxj2;
[f(t;x;y 1;z;v) f(t;x;y 2;z;v)]yk2(jyj2;
[g(t;x1) g(t;x2)]xkgjGxj2:
(ii)The functions b,,,fandgare uniformly Lipschitz continuous with respect
to(x;y;z;v ). In particular, there are constants bx,by,x,y,x,y,fx,DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 7
fz,f andgxsuch that
jb(t;x1;y1) b(t;x2;y2)j2bxjxj2+byjyj2;
j(t;x1;y1) (t;x2;y2)j2xjxj2+yjyj2;
Z
E(t;x1;y1;e)(de) Z
E(t;x2;y2;e)(de)2
xjxj2+yjyj2;
jf(t;x1;y1;z1;v1) f(t;x2;y2;z2;v2)j2fxjxj2+fyjyj2
+fzjzj2+f jvj2;
jg(t;x1) g(t;x2)j2gxjxj2:
(iii)b(t;0;0),(t;0;0)and(t;0;0)are bounded. In particular, there are con-
stantsb0,0,0,f0andg0such that
jb(t;x;y )j2b0+bxjxj2+byjyj2;
j(t;x;y )j20+xjxj2+yjyj2;
Z
E(t;x;y;e )(de)2
0+xjxj2+yjyj2;
jf(t;x;y;z;v )j2f0+fxjxj2+fyjyj2+fzjzj2+f jvj2;
jg(x)j2g0+gxjxj2:
It should be emphasized that here bxet al. are constants, not partial derivatives.
For convenience, we also suppose that Mis an upper bound for all these constants
above.
The following assumption will be used in bounding the time discretization error.
assumption 2. The coecients b,,,fare uniformly H older-1
2continuous
with respect to t. We also assume the same constant Mto be the upper bound of the
square of the H oder constants.
Now we state an assumption which plays a key role in error analysis of numerical
methods for coupled FBSDEJs problems.
assumption 3. One of the following ve cases holds:
(i)Small time duration, that is, Tis small.
(ii)Weak coupling of Yinto the forward SDEJ (2.5), that is by,yandyare
small. In particular, if by=y=y= 0, then the forward equation does not
depend on the backward one.
(iii) Weak coupling of Xinto the backward SDEJ (2.5), that is, fxandgxare
small. In particular, if fx=gx= 0, then the backward equation does not
depend on the forward one and, thus, the backward SDEJ (2.5) are also de-
coupled.
(iv)bis strongly decreasing in x, that is,k1is very negative.
(iv)fis strongly decreasing in y, that is,k2is very negative.
Finally, we make an assumption on the neural network approximation functions,
which makes sure the systems in (2.16) is well-known.
assumption 4. Then functionsA;
0,B;
tnandC;
tnare measurable with linear
growth.
It is easy to verify that neural networks with common activation functions, in-
cluding ReLU and sigmoid function, satisfy this assumption.
Assumption 1 is usually called the Lipschitz continuity and monotonicity condi-
tions, and Assumption 3 is called weak coupling conditions (Bender and Zhang [6]).8 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
We can give more specic expression later. With these assumptions, we will prove
the following main theorem in this paper.
Theorem 3.1 ( Error estimates for deep learning algorithm ).Under Assumptions
1, 2, 3, and 4, there exists a constant C, independent of h,d, such that for suciently
small h, for any >0, we have
max
0n<Nsup
tnttn+1(EjXt X
tnj2+EjYt Y
tnj2) +N 1X
n=0Ztn+1
tnEjZt Z
tnj2dt
+N 1X
n=0Ztn+1
tnEj t  
tnj2dtC[h1 +Ejg(X
T) Y
Tj2]:(3.1)
In particular, if one of the following two conditions holds, we have = 0:
(i)Coecient b,,,fandghave bounded derivative function with K-Lipchitz
derivatives.
(ii)For eachy2Rande2E, the mapx2Rd!(x;y;e )admits a Jacobian
matrixrx(x;y;e )such that the function a(x;y; ;e) :=T(rx(x;y;e ) +
Id)satises one of the following condition uniformly in (x;y; )2RdRRd
a(x;y; ;e)j2jK 1ora(x;y; ;e) j2jK 1; e2E;(x;y)2RdRd:
Theorem 3.1 allows us to state that the simulation error (left side of equation
(3.1)) of deep learning FBSDE method can be bounded through the value of the
objective function (2.17) and the time discretization error h1 . It is illuminating to
note that for = 0, that is, the no-jump case, it has been shown that = 0 for the
deep BSDE method in [20]. Note also that the appearance of is mainly due to the
errorPN 1
n=0Rtn+1
tnEjZt Z
tnj2dt. Under some additional conditions such as (i), (ii)
in Theorem 3.1 or similar conditions (see, e.g., [8]), we can prove that the estimate
(3.1) holds true for = 0.
The constant Cin Theorem 3.1 depends on the constants in Assumptions 1, 2, 3,
and 4, related to the coecients b,,,f,g, andT, but is independent of d,h, and
N. As will be seen in the proof, roughly speaking, the weaker the coupling (resp., the
stronger the monotonicity, the smaller the time horizon) is, the easier the condition
is satised, and the smaller the constants Crelated with error estimates are.
In what follows, we concentrate on the proof of Theorem 3.1, which is divided
in the three sections. The next section will be of great utility in order to prove the
convergence of Markovian iteration.
From now on we denote by Ca generic constant that only depends on Ejj2,T
and the coecients b,,,f,g, but is independent of d,h,Nandx. The value of
Cmay change from line to line when there is no need to distinguish.
4. Convergence of Markovian iteration. To prove Theorem 3.1, we need
several theorems and lemmas. Recalling the discrete form (2.16) and taking condi-
tional expectations Etnfgon both sides of the last equation in (2.16), we obtain
^Y
tn=Etnf^Y
tn+1+f(tn;^X
tn;^Y
tn;^Z
tn;^ 
tn)g: (4.1)
Multiplying ( Wtn)Tand taking condition expectations Etnfgon both sides of the
last equation in (2.16) again, one gets
^Z
tn=1
tnEtnf^Y
tn+1Wtng: (4.2)DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 9
Similarly, multiplyingRtn+1
tnR
E(e)~(de;dt ) and taking condition expectations Etnfg
on both sides of the last equation in (2.16) again yield
^ 
tn=1
tnEtn
^Y
tn+1Z
E(e)~(de;(tn;tn+1])
: (4.3)
From (2.16) and (4.1)-(4.3), we can get another discrete system without deep learning
as follows:
8
>>>>>>>>>>>>>><
>>>>>>>>>>>>>>:^X
0=;
^X
tn+1=^X
tn+b(tn;^X
tn;^Y
tn)tn+(tn;^X
tn;^Y
tn)Wtn
+Z
E(^X
tn;^Y
tn;e)~(de;(tn;tn+1]);
^Y
T=g(^X
T); ^Z
tn+1=1
tnEtnf^Y
tn+1Wtng;
^ 
tn=1
tnEtnn
^Y
tn+1R
E(e)~(de;(tn;tn+1])o
;
^Y
tn=Etnf^Y
tn+1+f(tn;^X
tn;^Y
tn+1;^Z
tn;^ 
tn)g:(4.4)
On the solution ( ^X
tn,^Y
tn,^Z
tn,^ 
tn) of (4.4), we have the following theorem whose
proof will be given later.
According to [42] and [2], we know that under Assumption 1, the solution ( ^X
tn,
^Y
tn,^Z
tn,^ 
tn) to FBSDEJs (2.5) is unique. Although the algorithm (4.4) is explicit
with respect to ^X
tn+1and ^Y
tn, it cannot be implemented directly because of their
coupling. To decouple the equations (4.4) in practical computation, we can introduce
a \Markovian" iteration [6] with u
tn;0= 0,80nN,
8
>>>>>>>>>>>>>>>>><
>>>>>>>>>>>>>>>>>:^X
0;m=;
^X
tn+1;m=^X
tn;m+b(tn;^X
tn;m;u
tn;m 1(^X
tn;m))tn
+(tn;^X
tn;m;u
tn;m 1(^X
tn;m))Wtn
+Z
E(^X
tn;m;u
tn;m 1(^X
tn;m);e)~(de;(tn;tn+1]);
^Y
T;m=g(^X
T;m); ^Z
tn+1;m=1
tnEtnf^Y
tn+1;mWtng; m = 1;2;:::;
^ 
tn;m=1
tnEtnn
^Y
tn+1;mR
E(e)~(de;(tn;tn+1])o
;
^Y
tn;m=Etnf^Y
tn+1;m+f(tn;^X
tn;m;^Y
tn+1;m;^Z
tn;m;^ 
tn;m)g;
u
tn;m(^X
tn;m) =^Y
tn;m:(4.5)
For proving the convergence of the \Markovian" iteration (4.5), we estimate u
tn;m+1(^X
tn;m) 
u
tn;m(^X
tn;m) in terms of ^X
tn;m+1 ^X
tn;m, and then ^X
tn;m+1 ^X
tn;min terms of
u
tn;m(^X
tn;m) u
tn;m 1(^X
tn;m), and obtain
sup
xju
tn;m+1(x) u
tn;m(x)j2(L(u
tn;m)) sup
xju
tn;m(x) u
tn;m 1(x)j2;
where(L(u
tn;m)) depends on the coecients of the equation and the Lipschitz con-
stantL(u
tn;m) ofu
tn;m.L(u
tn;m) will be estimated in following lemma, Lemma 4.1. If10 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
the \Markovian" iteration converges, we need to control (L(u
tn;m))<1. In addition,
we will show that u
tn;mis linearly growing and satises
ju
tn;m(x)j2G(u
tn;m)jxj2+H(u
tn;m): (4.6)
To estimate the Lipschitz constant L(u
tn;m) and the constants G(u
tn;m) andH(u
tn;m)
in the linear growth condition (4.6), let us dene
L0=[by+y+y][gx+fxT]Texp([2 + 2k1+ 2k2+x+x+fz+K2
f ]T
+ [by+y+y][gx+fxT]T);
L1=[gx+fxT][exp([2 + 2 k1+ 2k2+x+x+fz+K2
f ]T
+ [by+y+y][gx+fxT]T+ 1)_1];
c0(L1) :=T[gx1([1 + 2k2+fz+K2
f ]T;[1 + 2k1+x+x]T+ [by+y+y]L1T)
+fxT0([1 + 2k2+fz+K2
f ]T)
0([1 + 2k1+x+x]T+ [by+y+y]L1T)];
c1(L1) :=[by+y+y]c0(L1);
L2(L1) :=[b0+0+0]c0(L1) +e[1+2k2+fz+K2
f ]+Tg0
+f0T0([1 + 2k2+fz+K2
f ]T);
0(x) :=ex 1
x;(x>0); 1(x;y) := sup
0<<1ex0(y):
Then we have the following lemma.
Lemma 4.1. If
L0e 1; c 1(L1)<c1<1; (4.7)
then for any L>L 1,G>L 1,L2>L 2(L1), and forhsmall enough, we have
L(u
tn;m)L; G (u
tn;m)G; H (u
tn;m)L2
1 c1:
remark 1. If any of the ve conditions of Assumption 3 holds true, then (4.7)
hold.
4.1. Estimates for the dierence of solutions. In order to prove Lemma
4.1, we consider the following system of equations
^X
tn+1=^X
tn+b(tn;^X
tn;'(^X
tn))tn+(tn;^X
tn;'(^X
tn))Wtn
+Z
E(tn;^X
tn;'(^X
tn);e)~(de;(tn;tn+1]); (4.8)
^Y
tn+1=^Y
tn f(tn;^X
tn;^Y
tn;^Z
tn;^ 
tn)tn+Ztn+1
tnZi
tndWt
+Ztn+1
tnZ
EUi
tn(e)~(de;dt ); (4.9)
with ^X
t0=, and
^Z
tn:=1
tnEtnf^Y
tn+1Wtng;^ 
tn:=1
tnEtn
^Y
tn+1Z
E(e)~(de;(tn;tn+1])
;DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 11
where'is uniformly Lipschitz continuous with L(') denoting the square of the Lip-
schitz constant of '.
Since the terminal condition of ^Y
Tis not specied, the system of (4.8)-(4.9) has
innitely many solutions. The dierence between two such solutions is bounded by
the following lemma.
Lemma 4.2 ( Estimates for the dierence of two solutions ).Fori= 1;2, let
(^X;i
tn,^Y;i
tn,^Z;i
tn,^ ;i
tn)be two solutions of (4.8) -(4.9) , with'replacing by 'iand
^X;i
tn,^Y;i
tn2L2(
;Ftn;P). Suppose condition (2.2) holds. For suciently small h,
denote
A1:=2k1+x+1+bxh+x;
A2:= 1
1by+y+y+byh;
A3:=2+4+ (1 + 1
2+1
K2Z
E2(e)(de) 1
3)fztn;
A4:=3+K2
5+ (1 + 1
2+1
K2Z
E2(e)(de) 1
3)f tn;
A5:=1 + 2k2+ 1
4fz+ 1
5f + (1 + 1
2+1
K2Z
E2(e)(de) 1
3)fyh;
A6:=fx+
1 + 1
2+ 1
3
K2Z
E2(e)(de)
fxh:
wherei>0,i= 1;:::; 5, are chosen such that
A31andA41: (4.10)
Then for 0nNand any6>0, we have
Etnj^Xn+1j2[1 +A1h+ (1 +6)A2hL('1)]j^Xnj2
+ (1 + 1
6)A2hj'1(^X;2
n) '2(^X;2
n)j2;(4.11)
j^Ynj2+ (1 A3)tnj^Znj2+1
K2(1 A4)tnj^ nj2
(1 +A5h)Etnj^Yn+1j2+A6hj^Xnj2; (4.12)
where
^Xn: =^X;1
tn ^X;2
tn;^Yn:=^Y;1
tn ^Y;2
tn;
^Zn: =^Z;1
tn ^Z;2
tn;^ n:=^ ;1
tn ^ ;2
tn:
Proof . For simplicity, let us denote
bn=b(tn;^X;1
tn;'1(^X;1
tn)) b(tn;^X;2
tn;'2(^X;2
tn));
n=(tn;^X;1
tn;'1(^X;1
tn)) (tn;^X;2
tn;'2(^X;2
tn));
n=(tn;^X;1
tn;'1(^X;1
tn);e) (tn;^X;2
tn;'2(^X;2
tn);e);
fn=f(tn;^X;1
tn;^Y;1
tn;Z;1
tn; ;1
tn) f(tn;^X;2
tn;^Y;2
tn;Z;2
tn; ;2
tn);
Zn:=Z1
tn Z2
tn;Un:=U1
tn U2
tn:12 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
Then from (4.8), we obtain
Etnfj^Xn+1j2g=Etn(^Xn+ bntn+ nWtn+Z
En~(de;(tn;tn+1])2)
[1 + (bxtn+x+ 2k1+1+x)tn]Etnn
j^Xnj2o
+ ( 1
1by+y+bytn+y)tnEtnn
j'1(^X;1
tn) '2(^X;2
tn)j2o
(1 +A1h)Etnn
j^Xnj2o
+A2hEtnn
j'1(^X;1
tn) '1(^X;2
tn) +'1(^X;2
tn) '2(^X;2
tn)j2o
[1 +A1h+ (1 +6)A2hL('1)]Etnn
j^Xnj2o
+ (1 + 1
6)A2hEtnn
j'1(^X;2
tn) '2(^X;2
tn)j2o
;(4.13)
and therefore (4.11) is proved.
Similarly, from (4.9), we have
^Yn+Ztn+1
tnZndWn+Ztn+1
tnZ
EUn(e)~(de;dt ) =  ^Yn+1+ fntn:(4.14)
Squaring and taking conditional expectation on both sides of equation (4.14), we
obtain
j^Ynj2+EtnZtn+1
tnjZnj2dt
+EtnZtn+1
tnZ
EjUn(e)j2(de)dt
=Etnfj^Yn+1j2+ 2 ^Yn+1fntn+jfnj2t2
ng:(4.15)
Next we estimate the second and third terms on the left-hand side of (4.15). The
second term can be estimated by the following inequality, in view of Cauchy-Schwarz
inequality,
EtnZtn+1
tnjZnj2dt
tnj^Znj2+ 2tn^ZnEtnffWtng
(1 2)tnj^Znj2  1
2t2
nEtnfjfnj2g:(4.16)
To estimate the third term on the left-hand side of (4.15), we use the condition (2.2)
to get
Ztn+1
tnZ
Ej(e)Un(e)j2(de)dtK2
Ztn+1
tnZ
EjUn(e)j2(de)dt:
As a consequence, we have
EtnZtn+1
tnZ
EjUn(e)j2(de)dt
1
K2EtnZtn+1
tnZ
Ej(e)Un(e)j2(de)dt
1
tnK2Etn(Ztn+1
tnZ
E(e)Un(e)(de)dt2)
:
(4.17)DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 13
On the other hand, from (4.14), we also have
EtnZtn+1
tnZ
E(e)Un(e)(de)dt
=EtnZtn+1
tnZ
EUn(e)~(de;dt )Z
E(e)~(de;(tn;tn+1])
=Etnf[^Yn+1+ fntn]Z
E(e)~(de;(tn;tn+1])g
= tn
^ n+Etn
fnZ
E(e)~(de;(tn;tn+1])
:(4.18)
Substituting (4.18) into (4.17) yields
EtnZtn+1
tnZ
EjUn(e)j2(de)dt
tn
K2j^ nj2+2tn
K2^ nEtn
fnZ
E(e)~(de;(tn;tn+1])
1
K2(1 3)tnj^ j2  1
31
K2Z
E2(e)(de)t2
nEtnfjfnj2g:(4.19)
Combining (4.15), (4.16) and (4.19) leads to
j^Ynj2+(1 2)tnj^Znj2+1
K2(1 3)tnj^ nj2
Etnn
j^Yn+1j2+ 2 ^Yn+1fntn
+
1 + 1
2+ 1
3
K2Z
E2(e)(de)
t2
njfnj2
:(4.20)
Finally, we substitute
2^Yn+1fnfxj^Xnj2+ (1 + 2k2+ 1
4fz+ 1
5f )j^Yn+1j2
+4j^Znj2+5j^ nj2
into (4.20) and obtain (4.12). The proof is completed.
We need the following a priori estimates for the solution of (4.8)-(4.9).
Lemma 4.3 ( A priori estimates ).Let(^X
tn,^Y
tn,^Z
tn,^ 
tn)be the solution of
(4.8) -(4.9) with ^Y
T=g(^X
T)and'replacing by 'n, where'nis linearly growing
functions satisfying
j'n(x)j2G('n)jxj2+H('n):
Let
G(') := sup
n0G('n); H (') := sup
n0H('n):14 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
Suppose condition (2.2) holds. Then for 0nN, we have
Etnj^X
tn+1j2[1 +A1h+A2hG(')]j^X
tnj2
+ [b0+0+0+b0h+A2H(')]h; (4.21)
j^Y
tnj2+(1 A3)tnj^Z
tnj2+1
K2(1 A4)tnj^ tnj2
(1 +A5h)Etnj^Y
tn+1j2+A6hj^Xtnj2
+
1 + (1 + 1
2+ 1
31
K2Z
E2(e)(de))h
f0h: (4.22)
Proof . From (4.8) and the assumption of the lemma, one gets
Etnj^X
tn+1j2=Etn^X
tn+b(tn;^X
tn;'n(^X
tn))tn+(tn;^X
tn;'n(^X
tn))Wtn
+Z
E(^X
tn;'n(^X
tn);e)~(de;(tn;tn+1])2
Etnj^X
tnj2+ 2tnEtn
b(tn;^X
tn;'n(^X
tn))T^X
tn
+Etnn
b2(tn;^X
tn;'n(^X
tn))(tn)2o
+Etnn
2(tn;^X
tn;'n(^X
tn))tno
+Z
E2(^X
tn;'n(^X
tn);e)(de)tn
[1 +A1h]j^X
tnj2+A2hj'n(^X
tn)j2+ [b0+0+0+b0h]h
[1 +A1h+A2hG(')]j^X
tnj2+ [b0+0+0+b0h+A2H(')]h;
which implies that (4.21) holds.
Next, applying Lemma 4.2 yields
j^Ytnj2+ (1 2)tnj^Ztnj2+1
K2(1 3)tnj^ tnj2
Etnn
j^Ytn+1j2+ 2^Ytn+1ftn+ (1 + 1
2
+1
K2 1
3Z
E2(e)(de))tnjftnj2
:
In view of
^Yn+1fnfxj^Xnj2+ (1 + 2k2+ 1
4fz+ 1
5f )j^Yn+1j2+4j^Znj2+5j^ nj2:
using (iii) of Assumption 1, we obtain (4.22) and therefore complete the proof.
Now we employ Lemmas 4.2 and 4.3 to show Lemma 4.1.
Proof . [of Lemma 4.1]. To show Lemma 4.1, let us introduce
^X;1
tn=^X
tn;m+1; ^Y;1
tn=^Y
tn;m+1; '1(^X;1
tn) =u
tn;m 1(^X;2
tn);
^X;2
tn=^X
tn;m; ^Y;2
tn=^Y
tn;m; '2(^X;2
tn) =u
tn;m(^X;1
tn):
In view of the above notations, we can set 6= 0 in (4.13) and therefore obtain from
Lemmas 4.2 and 4.3 that
EtnjXn+1j2[1 +A1h+A2hL(u
tn;m 1)]jXnj2;DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 15
and
jYnj2+ (1 A3)tnjZnj2+1
K2(1 A4)tnj nj2
(1 +A5h)EtnjYn+1j2+A6hjXnj2:
In view of (4.10) and jYnj=ju
tn;m+1 u
tn;mj, we can get
ju
tn;mj2(1 +A5h)EtnjYn+1j2+A6hjXnj2
[1 +A5h][1 +A1h+A2hL(u
tn;m 1)]L(u
tn+1;m)jXnj2
+A6hjXnj2:
Then we have
L(u
tn;m)[1 +A5h][1 +A1h+A2hL(u
tn;m 1)]L(u
tn+1;m) +A6h

1 +
A1+A5+A1A5h+ (A2+A2A5h)L(u
tn;m 1)
h	
L(u
tn+1;m)
+A6h

1 +
(A1+A5+A1A5h+ (A2+A2A5h)L(u
tn;m 1))_0
h	
L(u
tn+1;m)
+A6h:
Applying discrete Gronwall inequality and L(u
tN;m) =gxleads to
L(u
tn;m)[gx+A6T]
[exp((A1+A5+A1A5h)T+ (A2+A2A5h)TL(u
tn;m 1))_1]:(4.23)
Multiplying ( A2+A2A5tn)Ton the both sides of equation (4.23) and using L(u
tn;0) =
0, we get
(A2+A2A5h)TL(u
tn;m)
[A2+A2A5h]T[gx+A6T]

exp([A1+A5+A1A5h]T+ [A2+A2A5h]TL(u
tn;m 1))_1	
[A2+A2A5h]T[gx+A6T]
fexp([A1+A5+A1A5h]T+ [A2+A2A5h][gx+A6T]T
+ [A2+A2A5h][gx+A6T]T[exp([A1+A5+A1A5h]T
+[A2+A2A5h]TL(u
tn;m 2))])_1	
:(4.24)
Now we set
L0(;h) :=[A2+A2A5h]T[gx+A6T]
exp([A1+A5+A1A5h]T+ [A2+A2A5h][gx+A6T]T);
L1(;h) :=[gx+A6T][exp([A1+A5+A1A5h]T
+ [A2+A2A5h][gx+A6T]T+ 1)_1];
and choose
2:=p
tn;  4:= 1 
1 +fz+fz
K2Z
E(e)2(de)p
tn fztn;
3:=p
tn;  5:=1
K2
1 
1 +f +f 
K2Z
E(e)2(de)p
tn f tn
:(4.25)16 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
ThenA3= 1,A4= 1 and
lim
h!0L0(;h) =L0; lim
h!0L1(;h) =L1:
Hence, for any small enough hand any L>L 1, ifL0e 1, from (4.24) we have
L(u
tn;m)L1(;h);
and therefore L(u
tn;m)L.
Now, we study the linear growth condition (4.6) on u
tn;m. Similarly, it is easy
get the following inequalities,
G(u
tn;m)[gx+A6T][exp([A1+A5+A1A5h]T+ [A2+A2A5h]TG(u
tn;m 1))_1];
H(u
tn;m)[eA5T_1]g0+
f0+f0(1 + 1
2+ 1
31
K2Z
E(e)2(de))h
n
0(A5)
+ [b0+0+0+b0h+A2H(u
tn;m 1)]

gxn
1(A5;A1+A2G(u
tn;m 1)) +A6n
0(A5)n
0(A1+A2G(u
tn;m 1))
=c1 
;h;G (u
tn;m 1)
H(u
tn;m 1) +L2 
;h;G (u
tn;m 1)
:
Here
c0(;h;G (u
tn;m 1))
:=
gxn
1(A5;A1+A2G(u
tn;m 1)) +A6n
0(A5)n
0(A1+A2G(u
tn;m 1))
;
c1(;h;G (u
tn;m 1)) :=A2c0(;h;G (u
tn;m 1));
L2(;h;G (u
tn;m 1) := [b0+0+0+b0h]c0(;h;G (u
tn;m 1)) + [eA5T_1]g0
+
f0+f0(1 + 1
2+ 1
31
K2Z
E(e)2(de))h
n
0(A5):
We still choose ias in (4.25), i= 2;3;4;5. Noting that
lim
h!0c1(;h;G (u
tn;m 1)) =c1(G); lim
h!0L2(;h;G (u
tn;m 1)) =L2(G);
because of L0e 1,c1(L1)1, for any small enough h, and for any G > L 1,
c1(G)c1<1,L2(G)L2, we haveG(u
tn;m)G,H(u
tn;m)L2
1 c1. This
completes the proof of Lemma 4.1.
Now we dene
c2(1;L1;L1)
:=h
e[1+2k2+x+x+[by+y+y]L1]T_1i
(1 + 1
1)[by+y+y]T[gx
1([1 + 2k2+fz+f ]T;[2 + 2k1+x+x]T+ (1 +1)[by+y+y]L1T)
+fxT0([2 +k2+fz+f ]T)0([1 +k1+x+x]T
+ (1 +1)[by+y+y]L1T)];
and
c2(L1;L1) := inf
1>1c2(1;L1;L1):DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 17
Then we have the following theorem which implies the convergence of the Markovian
iteration (4.5).
Theorem 4.4. AssumeL0e 1holds true and
c2(L1;L1)<1: (4.26)
(i)For any L>L 1,G>L 1,c1(L1)<c1<1,L2>L 2(L1), we have
L(u
tn)L; G (u
tn)G; H (u
tn)H=L2
1 c1;
(ii)For anyc2(L1;L1)<c21, and suciently small h, whenm!1 , we have
max
0nNju
tn;m u
tnj2!0:
Proof . Employing Lemma 4.1, the proof of above theorem is similar to that of
Theorem 5.1 in [6], we are not going to repeat this proof.
5. Error estimates for time discretization. We now study the error due
to the time discretization. We rst present the following theorem which gives the
connections between coupled FBSDEJs and nonlinear PIDEs under weaker conditions.
Theorem 5.1. Under Assumptions 1, 2 and 3, there exist a function u:RRd!
Rthat satises the following statements
(i)ju(t;x1) u(t;x2)j2L1jx1 x2j2,ju(t;x)j2L1jxj2+L2(L1)
1 c1L1.
(ii)ju(t;x) u(s;x)j2C(1 +jxj2)jt sjfor some constant C.
(iii)uis a viscosity solution of the PIDEs (2.1) .
(iv) The FBSDEJs (2.5) has a unique solution (Xt;Yt;Zt; t)andYt=u(t;Xt).
Thus, (Xt;Yt;Zt; t)also solve the following decoupled FBSDEJs
8
>>>>>>>>>>>><
>>>>>>>>>>>>:Xt=+Zt
0b(s;Xs;u(s;Xs))ds+Zt
0(s;Xs;u(s;Xs))dWs
+Zt
0Z
E(Xs ;u(s;Xs);e)~(de;ds );
Yt=g(XT) +ZT
tf(s;Xt;Ys;Zs; s)ds ZT
tZsdWs
 ZT
tZ
EUs(e)~(de;ds ):(5.1)
(v)Furthermore, we have the following estimates:
j~u
tn(t;x) u(tn;x)j2C[1 +jj2]h; (5.2)
and for any >0
max
0n<Nsup
tnttn+1(EjXt ~Xtnj2+EjYt ~Ytnj2) +N 1X
n=0Ztn+1
tnEjZt ~Z
tnj2dt
+N 1X
n=0Ztn+1
tnEj t ~ 
tnj2dtC[1 +Ejj2]h1 ;(5.3)18 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
where ~u
tn(t;~X
tn) =~Y
tnand
8
>>>>>>>>>>><
>>>>>>>>>>>:~X
tn+1=~X
tn+b(tn;~X
tn;u(tn;~X
tn))tn+(tn;~X
tn;u(tn;~X
tn))Wtn
+Z
E(~X
tn;u(tn;~X
tn);e)~(de;(tn;tn+1]);
~Z
tn+1=1
tnEtnf~Y
tn+1Wtng;
~ 
tn=1
tnEtnn
~Y
tn+1R
E(e)~(de;(tn;tn+1])o
;
~Y
tn=Etnf~Y
tn+1+f(tn;~X
tn;~Y
tn;~Z
tn;~ 
tn)g:(5.4)
In particular, we have = 0under the conditions (i) or (ii) in Theorem 3.1.
Proof . The proof of (i), (ii) and (iii) is similar to that of Theorem 4.4 and also
similar to that of Theorem 6.1 of [6]. Since (5.1) is decoupled, (iv) and (5.3) in (v)
can be obtained directly from references [7]. The estimate (5.2) in (v) can be obtained
from (i), (ii) and (iv), and the proof is similar to that of Corollary 6.2 in Bender and
Zhang [6].
Now, we are ready to derive the error estimates for standard time discretization.
Theorem 5.2 ( Error estimates for time discretization ).Under assumptions 1,
2, 3 and 4, for suciently small h, equation (4.4) has a solution (^X
tn,^Y
tn,^Z
tn,^ 
tn)
such that
max
n<Nsup
tnttn+1(EjXt ^X
tnj2+EjYt ^Y
tnj2) +N 1X
n=0Ztn+1
tnEjZt ^Z
tnj2dt
+N 1X
n=0Ztn+1
tnEj t ^ 
tnj2dtC[1 +Ejj2]h1 :(5.5)
In particular, = 0under the conditions (i) or (ii) in Theorem 3.1.
Proof . Reviewing discrete schemes (4.4) and (5.4), applying Cauchy-Schwarz
inequality and (5.3), we have
max
0n<Nsup
tnttn+1(EjXt ^Xtnj2+EjYt ^Ytnj2)
+N 1X
n=0Ztn+1
tn(EjZt ^Z
tnj2dt+Ej t ^ 
tnj2)dt
max
0n<Nsup
tnttn+1(EjXt ~Xtnj2+EjYt ~Ytnj2) +N 1X
n=0Ztn+1
tnEjZt ~Z
tnj2dt
+N 1X
n=0Ztn+1
tnEj t ~ 
tnj2dt+ max
0n<Nsup
tnttn+1(Ej~X
tn ^X
tnj2+Ej~Y
tn ^Y
tnj2)
(5.6)
+N 1X
n=0Ztn+1
tnEj~Z
t ^Z
tnj2dt+N 1X
n=0Ztn+1
tnEj~ 
t ^ 
tnj2dt
C(1 +jj2)h1 + max
0n<Nsup
tnttn+1(Ej~X
tn ^X
tnj2+Ej~Y
tn ^Y
tnj2)
+N 1X
n=0Ztn+1
tnEj~Z
t ^Z
tnj2dt+N 1X
n=0Ztn+1
tnEj~ 
t ^ 
tnj2dt:DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 19
Choose1= 1, we obtain from (4.11) and (iii) of Theorem 5.1,
Ej~X
tn+1 ^X
tn+1j2Ef(1 +Ch)j~X
tn ^X
tnj2+Chju(tn;X
tn) ^u(tn;X
tn)j2g
Ef(1 +Ch)j~X
tn ^X
tnj2+C(1 +jj2)hg:
Using Growall inequality and ~X
t0 ^X
t0= 0, we have
X
0nNEfj~X
tn ^X
tnj2gC(1 +jj2)h: (5.7)
Choose2=3=4=1
5andhsmall enough so that A31
2andA41
2. Then we
obtain
En
j~Y
tn ^Y
tnj2+1
2hj~Z
tn ^Z
tnj2+1
2hj~ 
tn ^ 
tnj2
En
(1 +Ch)j~Y
tn+1 ^Y
tn+1j2+Chj~X
tn ^X
tnj2o
:
Since
j~Y
tN ^Y
tNj2=jg(~X
tN) g(X
tN)j2Cj~X
tN ^X
tNj2;
we can get
sup
0nNEfj~Y
tn ^Y
tnj2g+hN 1X
n=0Ej~Z
tn ^Z
tnj2+hN 1X
n=0Ej~ 
tn ^ 
tnj2
Csup
0nNEfj~X
tn ^X
tnj2
C(1 +jj2)h:(5.8)
Substituting (5.7) and (5.8) into (5.6) yields the desired results. This completes the
proof.
We conclude this section with a remark. Theorem 5.2 allow us to state that the
Euler time discretization scheme achieves a rate of convergence of at least h(1 )=2for
any >0 under the standard Lipschitz conditions, and the optimal rate h1=2under
the additional assumption.
6. Error estimates for deep learning approximation. In this section, we
derive the simulation error of deep learning method. We rst consider algorithm (4.4)
without the terminal condition of ^Y
T. This implies the system has innitely many
solutions, as the case of (4.8)-(4.9). We have the following estimates whose proof is
based on Lemma 4.2 as well.
Lemma 6.1. For suciently small h, for any7>0and8>maxffz;f 
K2g, let
B1:=( 1
7+h)by+y+y; B 2:= ln[1 (1 +fx+8)h]
h;
B3:=fx
[1 (1 +fx+8)h]8:
Suppose equation (4.4) without the terminal condition of ^Y
Thas two solutions (^X;j
tn,
^Y;j
tn,^Z;j
tn,^ ;j
tn),j= 1;2. Let Xtn=^X;1
tn ^X;2
tn,Ytn=^Y;1
tn ^Y;2
tn. Then we
have
EjXtnj2B1n 1P
i=0e(A1+7)(n 1 i)hEjYtij2h; (6.1)20 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
EjYtnj2eB2(N n)hjYtNj2+B3N 1P
i=neB2(i n)hEjXtij2h: (6.2)
Proof . Similar to (4.13), it follows that
EfjXtn+1j2g[1 + (A1+7)]hEfjXtnj2g+B1hEfjYtnj2g:
Using discrete Gr onwall inequality yields the estimate (6.1).
By the martingale representation theorem (see, for example, [34]), there exists an
F-adapted square integrable process fZtgtitti+1,fUt(e)gtitti+1such that
Ytn+1=EtnfYtn+1g+Ztn+1
tn(Zt)TdWt+Ztn+1
tnZ
EUt(e)~(de;(tn;tn+1]):
Then, similar to (4.15), we have
EfjYtn+1j2gEjYtnj2+Ztn+1
tnEjZtj2dt+Ztn+1
tnZ
EEjU(e)j2(de)dt
 1 +fxtnEfjYtnj2g [8EfjYtnj2g+ 1
8(fxEfjXtnj2g
+fzEfjZnj2g+f Efj nj2)]tng: (6.3)
Substituting (4.17) and
EZtn+1
tnjZtj2dt
1
tnE(Ztn+1
tnZtdt2)
into (6.3), we have
EfjYtn+1j2g[1 (1 +fx+8)tn]EfjYtnj2g+ (1 fz 1
8)tnEfjZndtj2g
+ (1
K2 f  1
8)tnEfj nj2g fx 1
8tnEfjXtnj2g:(6.4)
For any8>maxffz;f 
K2g, and suciently small hsatisfying (2 kf+8)h < 1, we
then have
EjYtnj2[1 (1 +fx+8)tn] 1(EjYtn+1j2+fx 1
8tnEjXtnj2):
By induction, we obtain (6.2) and therefore complete the proof of the theorem.
Now we are ready to bound the simulation error of deep learning algorithm.
Theorem 6.2. Suppose Assumptions 1, 2, 3 and 4, hold true and there exist
7>0and8maxffz;f 
K2gsuch thatA0<1with
A0= (by 1
7+y+y)1 e B4T
B4
gx(1 +9)eB4T+fx 1
8eB4T 1
B4
;
where
B4= 1 + 2k1+x+x+fx+1+7+8:DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 21
If (X
tn,Y
tn,Z
tn, 
tn) is a solution of equation (2.16) and ( ^X
tn,^Y
tn,^Z
tn,^ 
tn) is a
solution of equation (4.4) , then we have
sup
0nN(EjX
tn ^X
tnj2+EjY
tn ^Y
tnj2)
+N 1X
n=0EjZ
tn ^Z
tnj2tn+N 1X
n=0Ej 
tn ^ 
tnj2tnCEjg(X
T) Y
Tj2:
Proof . Let ^X;1
tn=X
tn,^Y;1
tn=Y
tn,^Z;1
tn=Z
tn,^ ;1
tn=  
tn,^X;2
tn=^X
tn,
^Y;2
tn=^Y
tn,^Z;2
tn=^Z
tnand^ ;2
tn=^ 
tn. Then using Lemma 6.1 we can bound the
dierence between ( X
tn,Y
tn,Z
tn,  
tn) and ( ^X
tn,^Y
tn,^Z
tn,^ 
tn) by the objective
function Ejg(X
T) Y
Tj2.
To begin with, for any 9>0, we set
P:= max
n2[0;N]e (A1+7)nhEjXtnj2; S := max
n2[0;N]eB2nhEjYtnj2;
A(h) :=B1he (A1+7)he (A1+7+B2)T 1
e (A1+7+B2)h 1

gx(1 +9)e(A1+7+B2)T+B3he(A1+7+B2)T 1
e(A1+7+B2)h 1
:
Then when A(h)<1, applying Lemma 6.1 yields
P[1 A(h)] 1eB2T(1 + 1
9)B1he (A1+7)he (A1+7+B2)T 1
e (A1+7+B2)h 1Ejg(X
T) Y
Tj2;
and
S[1 A(h)] 1eB2T(1 + 1
9)Ejg(X
T) Y
Tj2:
Noting lim
h!0A(h) =A0, we have
lim
h!0P2[1 A0] 1e(1+fx+8)T
(1 + 1
9)(by 1
7+y+y)1 e B4T
B4Ejg(X
T) Y
Tj2
and
lim
h!0S2[1 A0] 1e(1+fx+8)T(1 + 1
9)Ejg(X
T) Y
Tj2:
We then obtain our error estimates of max
n2[0;N]EjXtnj2and max
n2[0;N]EjYtnj2as
max
n2[0;N]EjXtnj2C(7;8)Ejg(X
T) Y
Tj2; (6.5)
max
n2[0;N]EjYtnj2C(7;8)Ejg(X
T) Y
Tj2: (6.6)22 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
To estimate EjZnj2andEj nj2, for maxffz;f g6= 0, we choose 8= 2 maxffz;f 
K2g
in (6.4) to get
1
2tnEjZnj2+1
2tnEj nj2
(1 fz 1
8)tnEjZtnj2+1
K2 f  1
8
tnEj tnj2
fx
2 maxffz;f 
K2gEjXtnj2tn+EjYtn+1j2
 
1 
1 +fx+ 2 max
fz;f 
K2
tn
EjYtnj2;
which further implies
N 1X
n=0tnEjZnj2+N 1X
n=0tnEj nj2
fxT
maxffz;f gmax
n2[0;N]EjXtnj2
+ [2 + (2 + 2 fx+ 4 maxffz;f g)T_0] max
n2[0;N]EjYtnj2
C(7;8)Ejg(X
T) Y
Tj2:(6.7)
Note the estimate (6.7) is trivial for the case of max ffz;f g= 0. Then combined
(6.5), (6.6) and (6.7) leads to the desired results. This completes the proof.
Finally, combining with Theorems 5.2 and 6.2, we obtain our main Theorem 3.1.
It is essential to bear in mind that there must exist 7and8satisfying the condi-
tions in Theorem 6.2, provided any of the weak coupling and monotonicity conditions
introduced in Assumption 3 holds to a sucient extent.
7. Numerical Experiments. In this section, we will use two numerical exam-
ples to illustrate the eectiveness of the deep learning-based algorithms.
7.1. One-dimensional problem. We rst consider a one-dimensional problem
(see example 1 of [40]). Let g(T;x) = sin(XT+T) + 2 and
b(t;Xt) = 0; (t;Xt) = 1; (Xt ;e) =e;  = 1; d= 1; (7.1)
f(t;x;u;Trxu;B[u]) =(u 2) exp(u)
2 exp(sin(x+t) + 2) urxu
sin(x+t) + 2
 Z
E(u(t;x+e) u(t;x))(de);(7.2)
such that the exact solution of (2.1) is u(t;x) = sin(x+t) + 2. The compensated
Poisson random measure:
(de) =(e)de:=X[ ;](e)de;
whereX[ ;]is the characteristic function of the interval [  ;],= 2is the jump
intensity and (e) =1
2X[ ;](e) is the density function of a uniform distribution onDEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 23
[ ;]. Then the FBSDEJ corresponding to (7.1)-(7.2) is
8
>>>>>>><
>>>>>>>:dXt=dWt+Z
Ee~(de;dt );
 dYt=(Yt 2) exp(Yt)
2 exp[sin(Xt+t) + 2] YtZt
sin(Xt+t) + 2  t
dt
 ZtdWt Z
EUt(e)~(de;dt ):
Now, let us set N= 20 and set 2 hidden layers, both of which are 1 + 10 dimen-
sional. Input layer and output layer are chosen as 1-dimensional. Table 4.1 depicts
average value of u(0;X0) and standard deviation of u(0;X0) based on 256 Monte Carlo
samples and 5 independent runs. From Table 4.1, we observe that we can obtain a
good approximation of u(0;X0) by using the deep learning-based algorithm.
Table 7.1
Estimate of u(0; X0)where X0= 0,d= 1.
Averaged
valueStandard
deviationLoss
function
0 1.63119 0.18441 0.81499
1000 1.91521 0.04239 0.17286
2000 1.96693 0.01654 0.14002
3000 1.98162 0.00939 0.13258
4000 1.99324 0.00639 0.12275
To further demonstrate the eectiveness of this algorithm for decoupled FBSDEJs,
the relative L1-approximation error of u(0;X0) and mean of the loss function are
presented in Fig. 4.1. It is observed from Fig. 4.1 that the relative L1-approximation
error ofu(0;X0) and mean of the loss function drop signicantly as the number of
iteration steps increase from 0 to 1500, but is extremely slow as the number of iteration
steps increase from 1500 to 4000.
(a) Relative L1-approximation error
 (b) Mean of the loss function
Fig. 7.1 .Relative L1-approximation error of u(0; X0)and mean of the loss function.24 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
7.2. High-dimensional problems. In the second example, we consider a high-
dimensional problems with
b(t;Xt) = 0; (t;Xt) =1p
dId; (Xt ;e) =e;  = 1; d= 100; (7.3)
f(t;x;u;Trxu;B[u]) =(u 2) exp(u)
2 exp(sin(x+t) + 2) u(Idrxu)
sin(x+t) + 2
 Z
E(u(t;x+e) u(t;x))(de);(7.4)
where x=Pd
i=1xi. We choose g(T;x) = sin(XT+T)+2 such that the exact solution
of the associated PIDEs is u(t;x) = sin(x+t) + 2. The compensated Poisson random
measure is the same as in the rst example:
(de) =(e)de:=X[ ;](e)de:
Similarly, we can obtain the corresponding FBSDEJs to (7.3)-(7.4). Let us still set
N= 20 and 2 hidden layers. Both of hidden layers are 100 + 10 dimensional, input
layer is 100-dimensional, and output layer is 1-dimensional. Table 4.2 depicts average
value ofu(0;X0) and standard deviation of u(0;X0) based on 256 Monte Carlo samples
and 5 independent runs. From Table 4.2, we still observe that the deep learning-based
algorithm can produce a good approximation of u(0;X0).
Table 7.2
Estimate of u(0; X0)where X0= 0,d= 100 .
Averaged
valueStandard
deviationLoss
function
0 1.96204 0.01881 0.50262
500 1.99250 0.00716 0.50266
1000 1.99322 0.00683 0.50098
1500 1.99942 0.00602 0.50026
2000 2.00047 0.00714 0.49976
The relative L1-approximation error of u(0;X0) and mean of the loss function are
presented in Fig. 4.2 from which we observe that the relative L1-approximation error
ofu(0;X0) oscillates when the number of iteration steps becomes larger. We also see
that the mean of the loss function decays as the number of iteration steps increase.
(a) Relative L1-approximation error
 (b) Mean of the loss function
Fig. 7.2 .Relative L1-approximation error of u(0; X0)and mean of the loss function.DEEP LEARNING ALGORITHMS FOR PIDES AND FBSDEJS 25
8. Concluding remarks. In this work, we popularized the deep BSDE schemes
for high-dimensional forward-backward stochastic dierential equations with jumps
(FBSDEJs) and related high-dimensional parabolic integral-partial dierential equa-
tions (PIDEs). We constructed the deep FBSDE scheme in which deep neural net-
works are used to approximate the gradient and the integral kernel. Then the error
estimates for this deep FBSDE algorithm were obtained based on the optimal error es-
timates of Euler time discretization and deep learning error estimate which is bounded
by the objective function in the variational problems. To establish the convergence
relationship between the solutions of FBSDEJs and PIDEs, the Markovian iteration
has been introduced and its convergence was established. We have implemented this
deep FBSDE scheme for low and high dimensional FBSDEJs problems and numerical
results showed that this scheme is eective. We also realized that our scheme cannot
reach the accuracy of the classical numerical schemes which are not available for high
dimensional problems. To improve the accuracy, extending our scheme to DBDP2 in
which the loss function will be minimized on each time step will be our future work.
REFERENCES
[1]Y. Achdou and O. Pironneau ,Computational methods for option pricing , Frontiers Appl.
Math., 30, SIAM, Philadelphia, PA, 2005.
[2]G. Barles, R. Buckdahn, and E. Pardoux ,Backward stochastic dierential equations and
integral-partial dierential equations , Stoch. Stoch. Rep., 60(1997), pp. 57{83.
[3]D. Becherer ,Bounded solutions to backward SDEs with jumps for utility optimization and
indierence hedging , Ann. Appl. Probab., 16(2006), pp. 2027{2054.
[4]C. Beck, W. E, and A. Jentzen ,Machine learning approximation algorithms for high-
dimensional fully nonlinear partial dierential equations and second-order backward
stochastic dierential equations , J. Nonlinear Sci., 29 (2019), pp. 1563-1619.
[5]R. Bellman ,Dynamic programming , Princeton Landmarks in Mathematics. Princeton Uni-
versity Press, Princeton, Nj, (2010).
[6]C. Bender and J. Zhang ,Time discretization and Markovian iteration for coupled FBSDEs ,
Ann. Appl. Probab., 18(2008), pp. 143-177.
[7]B. Bouchard and R. Elie ,Discrete-time approximation of decoupled forward{backward SDE
with jumps , Stochastic Process. Appl., 118(2008), pp. 53{75.
[8]B. Buchdahn and E. Pardoux ,BSDE's with jumps and associated integro-partial dierential
equations ,SFB 373 Discussion Papers 1994, 41, Humboldt University of Berlin, Interdisci-
plinary Research Project 373: Quantication and Simulation of Economic Processes..
[9]J. Castro ,Deep learing schemes for parabolic nonlocal integro-dieretial equations , arXiv
preprint arXiv: 2103.15008v1 (2021).
[10]Q. Chan-Wai-Nam, J. Mikael, and X. Warin ,Machine learing for semi-linear PDEs , J. Sci.
Comput., 79 (2019), pp. 1667-1712.
[11]R. Cont and P. Tankov ,Financial modelling with jump processes , Chanpman and Hall/CRC
Press, London, 2004.
[12]W. E, J. Han, and A. Jentzen ,Deep learning-based numerical methods for high-dimensional
parabolic partial dierential equations and backward stochastic dierential equations , Com-
mun. Math. Stat., 5(2017), pp. 349{380.
[13]W. E, M. Hutzenthaler, A. Jentzen, and T. Kruse ,On multilevel Picard numerical ap-
proximations for high-dimensional nonlinear parabolic partial dierential equations and
high-dimensional nonlinear backward stochastic dierential equations , J. Sci. Comput.,
79(2019), pp. 1534{1571.
[14]N. El Karoui, S. Peng, and M. C. Quenez ,Backward stochastic dierential equations in
nance , Math. Finance., 7(1997), pp. 1{71.
[15]Y. Fu, J. Yang, and W. Zhao ,Prediction-Correction scheme for decoupled forward backward
stochastic dierential equations with jumps , East Asian J. Appl. Math., 6(2016), pp. 253-
277.
[16]E. Gobet, J.-P. Lemor, and X. Warin ,A regression-based Monte Carlo method to solve
backward stochastic dierential equations , Ann. Appl. Problb., 15 (2005), pp. 2172-2202.
[17]L. Gonon and C. Schwab ,Deep ReLU network ecpression rates for option prices in high-26 W. S. WANG, J. WANG, J. P. Li, F. F. GAO AND Y. FU
dimensional, exponential L evy models , arXiv preprint arXiv: 2101.11897v2, (2021).
[18]L. Gonon and C. Schwab ,Deep ReLU neural network overcome the curse of dimensionality
for partial integrodierential equations , arXiv preprint arXiv: 2102.11707v2, (2021).
[19]J. Han, J. Arnulf, W. E ,Solving high-dimensional partial dierential equations using deep
learning , Proc. Natl. Acad. Sci. USA, 115(2018), pp. 8505-8510.
[20]J. Han and J. Long ,Convergence of the deep BSDE method for coupled FBSDEs , Probab.
Uncertain. Quant. Risk, 5(2020), pp. 1-33.
[21]P. Henry-Labordere ,Counterparty risk valuation: A marked branching diusion approach ,
arXiv preprint arXiv:1203.2369, (2012).
[22]P. Henry-Labordere, N. Oudjane, X. Tan, N. Touzi, and X. Warin ,Branching diu-
sion representation of semilinear PDEs and Monte Carlo approximation , Ann. Inst Henri
Poincar e Probab. Stat., 55(2019), pp. 184{210.
[23]C. Hur e, H. Pham, and X. Warin ,Deep backward schemes for high-dimensional nonlinear
PDEs , Math. Comp., 89(2020), pp. 1547{1580.
[24]M. Hutzenthaler, A. Jentzen, T. Kruse, T. A. Nguyen, and P. von Wurstemberger ,
Overcoming the curse of dimensionality in the numerical approximation of semilinear
parabolic partial dierential equations , Proc. R. Soc. A., 476 (2020).
[25]M. Hutzenthaler and T. Kruse ,Multi-level Picard approximations of high-dimensional
semilinear parabolic dierential equations with gradient-dependent nonlinearities , SIAM
J. Numer. Anal., 58 (2020), pp. 929-961.
[26]M. K. Kadalbajoo, L. P.Tripathi and A. Kumar ,An error analysis of a nite element
method with IMEX-time semidiscretizations for some partial integro-dierential inequal-
ities arising in the pricing of American options . SIAM J. Numer. Anal., 55 (2017), pp.
869-891.
[27]J. Ma and J. Yong ,forward-backward stochastic dierential equations and their applicatiions ,
Springer, Berlin Heidelberg, 2007.
[28]M. L. Mao, W. S. Wang and X. Jiang ,An extrapolated Crank-Nicolson method for option
pricing under stochastic volatility model with jump , Submitted.
[29]E. Pardoux and S. Peng ,Adapted solution of a backward stochastic dierential equation ,
Systems Control Lett., 14(1990), pp. 55-61.
[30]S. Peng ,Backward stochastic dierential equations and applications to optimal control , Appl.
Math. Optim., 27(1993), pp. 125-144.
[31]E. Pindza, K. C. Patidar and E. Ngounda ,Robust spectral method for numerical valua-
tion of European options under Merton's jump-diusion model , Numer. Methods Partial
Dierential Equations, 30 (2014), pp. 1169-1188.
[32]J. Sirignano and K. Spiliopoulos ,DGM: A deep learning algorithm for solving partial dif-
ferential equations , J. Comput. Phys., 375(2018), pp. 1339{1364.
[33]R. Situ ,On solutions of backward stochastic dierential equations with jumps and applications ,
Stochastic Process. Appl., 66(1997), pp. 209{236.
[34]R. Situ ,Theory of stochastic dierential equations with jumps and applications , Springer,
Berlin, 2005.
[35]S. Tang and X. Li ,Necessary conditions for optimal control of stochastic systems with random
jumps , SIAM J. Control Optim., 32(1994), pp. 1447-1475.
[36]W. S. Wang, Y. Z. Chen and H. Fang ,On the variable two-step IMEX BDF method for
parabolic integro-dierential equations with nonsmooth initial data arising in nance ,
SIAM J. Numer. Anal., 57 (2019), pp. 1289-1317.
[37]W. Wang, M. Mao and Z. Wang ,An ecient variable step-size method for options pric-
ing under jump-diusion models with nonsmooth payo function , ESAIM Math. Model.
Numer. Anal., 55(2021), pp. 913-938.
[38]X. Warin ,Nesting Monte Carlo for high-dimensional non-linear PDEs , Monte Carlo Methods
Appl., 24(2018), pp. 225-247.
[39]Z. Wu ,Fully coupled FBSDE with Brownian motion and Poisson process in stopping time
duration , J. Aust. Math. Soc., 74(2003), pp. 249{266.
[40]W. Zhao, Z. Wei, and G. Zhang ,Second-order numerical schemes for decoupled forward-
backward stochastic dierential equations with jumps , J. Comput. Math., 35(2017), pp.
213-244.
[41]W. Zhao, Fu. Y, and T. Zhou ,Multistep schemes for forward backward stochastic dierential
equations with jumps , J. Sci. Comput., 69(2016), pp. 1-22.
[42]W. Zhen ,Forward-backward stochastic dierential equations with Brownian motion and Pois-
son process , Acta Math. Appl. Sin., 15(1999), pp. 433{443.