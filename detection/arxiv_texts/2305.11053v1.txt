arXiv:2305.11053v1  [cs.DS]  18 May 2023(Noisy) Gap Cycle Counting Strikes Back: Random Order
Streaming Lower Bounds for Connected Components and Beyond∗
Sepehr Assadi†Janani Sundaresan†
Abstract
We continue the study of the communication complexity of gap cycle c ounting problems.
These problems have been introduced by Verbin and Yu [SODA 2011] a nd have found numerous
applicationsinprovingstreaminglowerbounds. Inthenoisygapcycle countingproblem(NGC),
there is a small integer k/greaterorequalslant1 and an n-vertex graph consisted of vertex-disjoint union of either
k-cycles or 2 k-cycles, plus O(n/k) disjoint paths of length k−1 in both cases (“noise”). The
edges of this graph are partitioned between Alice and Bob whose goa l is to decide which case
the graph belongs to with minimal communication from Alice to Bob.
We study the robust communication complexity —` a la Chakrabarti, Cormode, and
McGregor [STOC 2008]—of NGC, namely, when edges are partitioned r andomly between the
players. This is in contrast to all prior work on gap cycle counting pro blems in adversarial
partitions. While NGC can be solved trivially with zero communication whe nk <logn, we
prove that when kis a constant factor larger than log n, the robust (one-way) communication
complexity of NGC is Ω( n) bits.
As a corollary of this result, we can prove several new graph strea ming lower bounds for
random order streams . In particular, we show that any streaming algorithm that for ever y
ǫ >0 estimates the number of connected components of a graph pres ented in a random order
stream to within an ǫ·nadditive factor requires 2Ω(1/ǫ)space, settling a conjecture of Peng and
Sohler [SODA 2018]. We further discuss new implications of our lowerbo unds to other problems
such as estimating size of maximum matchings and independent sets o n planar graphs, random
walks, as well as to stochastic streams.
∗An extended abstract of this paper appears at ACM Symposium o n Theory of Computing (STOC) 2023.
†Department of Computer Science, Rutgers University. Resea rch supported in part by an NSF CAREER Grant
CCF-2047061, a Sloan Research Fellowship, a Google Researc h gift, and a Fulcrum award from Rutgers Research
Council. Emails: sepehr@assadi.info andsun.j@rutgers.edu .
iContents
1 Introduction 1
1.1 Local Exploration in Graph Streams and (Noisy) Gap Cycle Counting . . . . . . . . 2
1.2 Random Order Streaming Lower Bounds from NGC . . . . . . . . . . . . . . . . . . 4
1.3 Our Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2 Preliminaries 6
2.1 Robust Communication Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Biases and XORs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7
3 A Robust Lower Bound for Noisy Gap Cycle Counting 8
3.1 Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.2 A Hard Distribution for NGC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.3 The Distributional Hidden-XOR ( DHX) Problem . . . . . . . . . . . . . . . . . . . 13
3.4 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4 A Lower Bound for Distributional Hidden-XOR 18
4.1 Input Partitioning and Active Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.2 The Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21
5 Implications to Random Order Streaming Algorithms 26
5.1 Number of Connected Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.2 Minimum Spanning Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5.3 Maximum Matching and Matrix Rank . . . . . . . . . . . . . . . . . . . . . . . . . . 28
5.4 Planar Maximum Independent Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
5.5 Random Walk Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
5.6 Lower Bounds for Stochastic Streams . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A Noisy Gap Cycle Counting in Stochastic Streams 37
ii1 Introduction
Thestreamingmodelofcomputation, introducedby[ AMS96], isstudiedextensively intheliterature
as a way of capturing some challenges of processing massive d atasets, in particular, space eﬃciency.
Inthismodel, theinputispresentedtothealgorithminastr eam, andthealgorithmneedstoprocess
this stream on the ﬂy with minimal space, ideally polylogari thmic in the input size. Numerous such
eﬃcient algorithms are known for various statistical estim ation problems like frequency moments,
heavy hitters, quantiles, and alike (see, e.g. [ Mut05] for a summary of early work on this model).
But, estimating graph parameters, say, the number of connec ted components or size of maximum
matchings, fromastreamoftheiredgesturnedouttobequite morechallenging, andformanygraph
problemsofinterest, onecanruleoutexistenceofsuchspac eeﬃcientalgorithms(see, e.g.,[ McG14]).
This provable impossibility of eﬃcient streaming algorith ms for most graph problems has led
researchers to consider natural relaxations of the model, e .g., by allowing more memory or a few
more passes1. A particularly successful relaxation has been to go beyond the doubly worst case
analysis of streaming algorithms and instead consider random order streams, wherein the input
graph is still adversarial, but its edges are presented in a u niformly random order to the streaming
algorithm [ KKS14,KKS15,MMPS17 ,PS18,KMNT20 ,CFPS20,KKP22,CKKP22 ,SSSV23]. Our
goal in this paper is to study the inherent limitations of ran dom order graph streaming algorithms.
One of the most successful techniques in designing random or der graph streaming algorithms is
through implementing local exploration (query) algorithms. For instance, consider the problem of
estimating the number of connected components in an n-vertex graph to within an ǫ·nfactor. A
seminal work of [ CRT01] gave a local exploration query algorithm for this problem b y performing
truncated BFS of size O(1/ǫ) from few random vertices of the graph. In general, computin g BFS
trees in the (adversarial arrival) streaming model is quite challenging (see, e.g., [ FKM+08,AKSY20 ,
AN21]). Yet, [ PS18] gave an elegant implementation of this algorithm in random order streams
by crucially exploiting the fact that in such streams, edges of a BFS tree of size O(1/ǫ) may come
in the “right” order of exploration in the stream with probab ility (1/ǫ)−Θ(1/ǫ). Thus, by sampling
roughly (1 /ǫ)O(1/ǫ)vertices and performing a BFS from them by extending each one directly given
the arriving edges, [ PS18], and later [ CKKP22 ], showed that one can indeed estimate the number
of connected components in random order streams in small spa ce2.
While the random order streaming algorithms of [ PS18,CKKP22 ] for connected components
are quite eﬃcient for constant values of ǫ >0, their exponential dependence on ǫin the space can
become prohibitive even for moderately small values of ǫ. Yet, these exponential-in- ǫ(or even much
larger) dependences are quite common among random order gra ph streaming algorithms such as
ǫ-property testers in [ MMPS17 ,CFPS20],ǫ·nadditive approximation of matching size in bounded
degree graphs [ MMPS17 ], (1+ǫ)-approximation of minimum spanning tree weight [ PS18], (1+ǫ)-
approximation of maximum independent set in planar graphs [ PS18], and random walks [ KKP22].
But, are such exponential dependences also necessary? This question was originally posed
by [PS18] who stated that:
“It seems to be plausible to conjecture that approximating th e number of connected com-
ponents requires space exponential in 1/ǫ. It would be nice to have lower bounds that
conﬁrm this conjecture.”
Very recently [ CKKP22 ] provided a strong evidence in favor of this conjecture by re laxing the
1Often in the semi-streaming model [ FKM+05] wherein the memory is proportional to the number of vertice sn.
In this work, we do notconsider semi-streaming algorithms and solely focus on alg orithms with o(n) space.
2The algorithm of [ PS18] required 2O(1/ǫ3)·(logn) space but it was improved to (1 /ǫ)O(1/ǫ)·(logn) by [CKKP22 ].
1question. They showed that: ( i) if we slightly relax the random order streams by allowing so me
correlation in the ordering of the edges3, and (ii) if we further relax our goal in ﬁndinga connected
component of size O(1/ǫ) (as opposed to estimating their numbers), then they can ind eed prove an
exponential (even factorial) lower bound on ǫin the space of such algorithms4.
Nevertheless, the original conjecture for (truly) random o rder streams and, more importantly,
for estimation problem remains unanswered. This state-of- aﬀairs is the starting point of our work.
We develop a new tool for proving random order streaming lowe r bounds that leads to 2Ω(1/ǫ)
space lower bounds for a wide range of graph estimation probl ems, including the connected com-
ponent problem, settling the conjecture of [ PS18]. Similar to most other graph streaming lower
bounds, our proofs are through communication complexity. I n the following, we ﬁrst deﬁne the
main communication problem we work with and the lower bound w e establish for it. We then
present rather direct implications of this communication l ower bound to random order streams.
1.1 Local Exploration in Graph Streams and (Noisy) Gap Cycle Counting
At its core, the question we study in this paper boils down to u nderstanding the power of random
order streaming algorithms in performing local exploratio n. In general, such questions have a rich
history in the graph streaming literature. Already more tha n a decade ago, [ VY11] introduced gap
cycle counting problems in communication complexity as a wa y to prove lower boundsfor streaming
problems that rely on these explorations. In this problem, A lice and Bob are given edges of a graph
Gand an integer k/greaterorequalslant1 and want to test if Gis a disjoint union of k-cycles or 2 k-cycles. By building
on the Fourier-analytic approach of [ GKK+07], [VY11] proved that any one-way communication
protocol wherein Alice sends a single message to Bob require sn1−O(1/k)communication to solve
this problem. As communication complexity implies streami ng lower bounds, this leads to an
n1−O(1/k)-space lower bounds for single-pass streaming algorithms f or gap cycle counting.
Gap cycle counting is a highly versatile problem that captur es the diﬃculty in solving various
graph streaming problems. For instance, since the number of connected components in the two
cases of this problem diﬀers by an n/2kadditive factor, one can use the lower bound of [ VY11]
to also obtain that estimating the number of connected compo nents to within an ǫ·nfactor in
single-pass (adversarial order) streams requires n1−O(ǫ)space; see [ HP16] for this lower bound
and its optimality. Gap cycle counting problems have been ex tensively used for proving space
lower bounds for various streaming problems, e.g., in [ KKS15,KK15,BS15,EHL+15,HP16,LW16,
GVV17,BCK+18,KMT+22] (see [AKSY20 , Appendix B] for a survey of these results). As a result,
this problem has found its way among the few canonical commun ication problems—alongside, say,
Index, Disjointness, and Gap Hamming Distance—for proving streaming lower bounds.
Given the canonical role of gap cycle counting in streaming l ower bounds, recent years have
witnessedseveralattemptsinextendingthelowerboundsfo rthisproblem(oritsnaturalextensions)
to other streaming settings as well. In particular, [ AKSY20 ] extended the lower bounds for this
problem to multi-round communication (multi-pass streami ng) algorithms, proving that Ω(log k)
rounds/passes or nΩ(1)communication/space is needed. Subsequently [ AN21] extended these lower
bounds to an optimal Ω( k)-pass for no(1)-space streaming algorithms for a “noisy” variant of this
3[CKKP22 ] deﬁned the hidden-batch random order model wherein the edges of the graph are ﬁrst adv ersarially
batched together in groups of O(1) size, and then the groups are randomly permuted and prese nted to the algorithm.
4Among these two relaxations, the latter seems to us a stronge r condition to impose on algorithms as proving
searchlower bounds can be quite diﬀerent than decision lower bounds in this context; see, e.g., the classical com-
munication/streaming lower bounds on the Hidden Matching p roblem (search) [ BJK04] and Boolean Hidden Match-
ing (decision) [ GKK+07], or recent advances on search lower bounds for better-than -2-approximation of multi-pass
streaming MAX-CUT [ CKP+23] in absence of any decision lower bounds.
2problemwherein, theinputgraphadditionally containsΘ( n/k)pathsoflength kineithercases5–in
other words, depth- kexploration requires Ω( k) passes in adversarially ordered streams. Combining
this with prior reductions, the results in [ AKSY20 ,AN21] provided the ﬁrst, and in many cases the
only, known lower bounds for multi-pass streaming algorith ms for graph estimation problems.
(Noisy) gap cycle counting in random order streams. All prior lower bounds for gap cycle
counting problems were focused on adversarial ordered stre ams6. However, given the previous
successes in using this problem for addressing the role of lo cal exploration in streaming algorithms
and the myriad of reductions known already from this problem , it is quite natural to wonder if we
can also prove an analogue of these lower bounds for random or der streaming algorithms. This is
precisely the approach we take in this paper.
To be able to lift our communication lower bounds to random or der streaming algorithms, we
work with the notion of robust communication complexity introduced by [ CCM08]. In this model,
a graphGis adversarially chosen but then each of its edges is sent to A lice and Bob uniformly at
random. This random partitioning of the input then allows on e to extend communication lower
bounds in this model to streaming algorithms on random order streams (see Proposition 2.5 ). This
motivates the deﬁnition of our main communication problem ( similar-in-spirit to [ AN21]).
Problem 1 (Noisy Gap Cycle Counting (NGC) ).For integers n,k/greaterorequalslant1, inNGCn,k, we have
a graphG= (V,E)onnvertices such that Geither contains: (i) (n/4k)vertex-disjoint cycles of
length2k, or(ii) (n/2k)vertex-disjoint cycles of length k. In addition, in both cases Gcontains
(n/2k)vertex-disjoint paths of length k−1.7
Each edge e∈Eis sent uniformly at random to one of the sets EAandEB, given to Alice
and Bob, respectively. Alice can send one message to Bob, and Bob outputs which of the above two
cases the graph belongs to.
It is easy to see that when kis suﬃciently smaller than log n,NGCn,k, admits a trivial zero
communication protocol. In this case, one of the ( n/4k) vertex-disjoint cycles of length 2 kof case
(i) appear entirely in EBgiven to Bob with constant probability, which allows him to d istinguish
this case from the other one.
Our main technical result shows that, in contrast, the momen tkbecomes suﬃciently larger
than log n, this problem requires a large communication.
Result 1. There exists an absolute constant b >0such that the following is true. Any one-way
protocol for NGCn,kfork/greaterorequalslantb·lognrequiresΩ(n)communication from Alice to Bob to solve
the problem with probability of success at least 2/3.
Result 1 establishes that while “short-range” exploration, namely , depth-o(logn) exploration,
can be trivial in the robust communication complexity model , exploring slightly longer ranges
requires exponential-in-range communication. Prior to ou r work, no non-trivial robust communi-
cation lower bounds were known for this or any other gap cycle counting problems. We hope this
result paves the path toward a more systematic study of lower bounds in random order streams.
5The lower bound of [ AN21] is not based on two-player communication complexity and is proven directly for
streaming algorithms but can also be stated as a (NIH) multi- party communication lower bound.
6The only exception we are aware of is the Ω(√n)-space lower bound of [ KKS15] for better-than-2-approximation
of MAX-CUT in random order stream and its very recent extensi on in [SSSV23] to other streaming CSPs. These
results however do not imply random order lower bounds for ga p cycle counting and instead borrow ideas from this
problem to establish their own lower bounds using several ot her technical ingredients.
7This choice of number of vertex-disjoint paths is for simpli city of exposition and can be replaced with any o(n/k)
or even smaller; see Remark 3.17 .
31.2 Random Order Streaming Lower Bounds from NGC
We can now use Result 1 alongside existing reductions in [ VY11,AKSY20 ,AN21], as well as some
new ones, to establish several new random order graph stream ing lower bounds (see Section 5 for
the deﬁnition and background on each problem).
Result 2. None of the following problems admit an algorithm in random o rder streams that
for every ǫ >0uses2o(1/ǫ)space and computes the answer with probability at least 2/3(the
references list the known random order streaming algorithm s for these problems):
(i)estimating connected components to within an ǫ·nadditive factor (cf. [ PS18,CKKP22 ]);
(ii) (1+ǫ)-approximation of weight of the minimum spanning tree (cf. [ PS18]);
(iii) (1+ǫ)-approximation of matching size in bounded degree graphs (c f. [MMPS17 ]);
(iv) (1+ǫ)-approximation of maximum independent size in planar graph s (cf. [PS18]);
(v)random walk generation; see Section 5.5 for the precise deﬁnition (cf. [ KKP22]).
Prior to our work, no non-trivial random order streaming low er bounds were known for any of
these problems (but as mentioned earlier, [ CKKP22 ] proved lower bounds for the search version of
someof theseproblemsandinalmost randomorderstreams). O urResult 2 establishesthenecessity
of an exponential-in- ǫspace-dependence in general, for all of these problems. In p articular, part
(i) of this result now fully settles the conjecture of [ PS18] mentioned earlier.
Animportantremarkon Result 2 isinorder. DirectreductionsfromNGCwhenproving Result 2
typically require setting ǫ≈1/lognso that 2o(1/ǫ)=o(n) to apply Result 1. This is reminiscent of
other communication complexity lower bounds, most promine ntly for (1+ ǫ)-approximation of the
number of distinct elements from the Gap Hamming Distance pr oblem [IW03,CR11]. Similar to
those cases, for some problems (e.g., parts ( iii) and (iv) ofResult 2), this can be easily extended
to other choices of ǫ >0 through a simple padding argument, but this approach does n ot work for
all problems, in particular for part ( i). Thus, for some problems, our lower bound only holds for
certain values of ǫand not all choices (see Remark 5.1 for a detailed discussion of this topic)8.
We note that our lower bounds in Result 2 are somewhat stronger than what is stated and in
most cases rule out even 2o(1/ǫ)·no(1)space algorithms. This is necessary to argue near-optimali ty
of these results as all existing algorithms also have some mi ld dependence on n, typically O(logn),
in addition to ǫ, simply for storing the edges or some counters.
Finally, we also extend our lower bounds to stochastic graph streams deﬁned in [ CMVW16 ],
wherein the stream consists of independently chosen random samples from the edges of the graph,
and the goal is to solve the problem with minimal space and min imal number of samples. To
obtain this, we prove an analogue of Result 1 but for a communication model that corresponds
to stochastic streams, and use that to obtain similar lower b ounds as in Result 2 for stochastic
streams as well; see Section 5.6 for more information on these results.
8This in particular implies that while our lower bounds using communication complexity bypasses the relaxations
imposed by [ CKKP22 ] and target the original estimation problem on random order streams, unlike [ CKKP22 ], our
results cannot be applied to the full range of parameter ǫ >0. It is however also worth mentioning that the (1 /ǫ)Ω(1/ǫ)
lower bounds in [ CKKP22 ] for constant ǫ >0 (or even ǫ≈1/loglogn) are subsumed by the Ω(log n) bits lower bound
needed simply to read an edge from the stream. Thus, even in [ CKKP22 ], one needs slightly sub-constant values of
ǫto obtain non-trivial lower bounds, although those bounds a re exponentially better than ours.
41.3 Our Techniques
The bulk of the technical eﬀort of our paper is in proving Result 1 and the lower bounds in Result 2
follow easily fromthisresult. So, inthissection, weonlyf ocusontheformerresult, namely, arobust
communication lower bound for NGC, and postpone the details of the latter one to Section 5 .
The ﬁrst step of our lower bound follows a by-now familiar “de correlation” step to break the
strong promise in the input graphs (that the cycles are eithe r all short or all long). This approach
dates all the way back to [ GKK+07,VY11] and was done more explicitly in [ AKSY20 ,AN21,
KMT+22,ACL+22]. At a high level, the intuition is as follows. A natural stra tegy for a protocol
to solveNGCis to attempt to solve the problem for “many” cycles (i.e., de termine whether they
arek-cycles or 2 k-cycles), but each one with only a “minor” chance of success – given the strong
correlation of the cycles in the input, the protocol is likel y to solve the problem correctly for one
of the cycles and use that to infer which case it belongs to. Th ese decorrelation steps use a hybrid
argument to show that the converse is also true: any protocol forNGCleads to a protocol for a
“hybrid problem” that roughly speaking corresponds to the f ollowing: given a ﬁxed vertex v∈V,
is this vertex part of a k-cycle or a 2 k-cycle without the strong promise on remaining cycles (as in ,
now, combination of length- kand length-2 kcycles are also possible). Nevertheless, the challenge is
that this new protocol only has a “tiny” advantage over rando m guessing the answer, i.e., it solves
this new problem with probability only 1 /2+O(k/n). Thus, for the next step of the argument, we
should prove a “low probability of success” lower bound for o ur new hybrid problem.
The next step, and the main part of the argument, is to prove th is low probability of success
lower bound. The main recipe for this part is to apply some “ha rdness ampliﬁcation” technique,
typically an XOR-lemma [ GKK+07,VY11,AN21] or a direct product argument [ AKSY20 ]. This
allows one to obtain a low probability of success lower bound from a more typical lower bound
by amplifying the hardness often through certain gadgets em bedded in the input. This is where
we entirely deviate from these prior works, as unlike all of t hem, our goal is to prove a robust
communication complexity lower bound, i.e., when the input s are partitioned randomly between
Alice and Bob. This random partitioning of the input has this undesired side eﬀect that whenever
we ﬁx a certain gadget in our input, it is quite likely that the edges of this gadget are partitioned
the “wrong way” across the players, entirely destroying the role of this gadget.
To address this challenge, we design a hard input distributi on forNGCwhich is supported on
highly structured input graphs. This then allows us to argue that after this decorrelation step, the
new hybrid problem we have to deal with is also highly structu red. Quite informally speaking, this
means that the answer to our hybrid problem now is a function o f a series of “localized” gadgets
that we can keep track of. In particular, we can show that even under random partitioning of
the input, with high probability, many of these gadgets “sur vive”, namely, retain their hardness
amplifying property. Moreover, the “failed” gadgets now ar e not able to leak any information
about the surviving ones or negate their eﬀects. This then, in a highly non-black-box way, bring
us to the more familiar setting of proving low probability of success lower bounds using hardness
ampliﬁcation ideas, and more speciﬁcally, an XOR-lemma-ty pe argument (although we emphasize
that we neither prove nor use any form of a generic XOR-lemma, say, as in [ AN21] in this work, as
the random partitioning of the input does not allow using the se results in a black-box way).
The ﬁnal step will be then to establish the lower bound using t hese surviving gadgets which is
based on a white-box application of the Fourier analytic app roaches in [ GKK+07,VY11] (although
interestingly, a technical corollary of the main Fourier an alytic tool used in these works, namely,
the KKL inequality [ KKL88] suﬃces for our purpose and thus we only work with this coroll ary,
without us performing any Fourier analysis in this paper; se eSection 2.2 for more details).
52 Preliminaries
Notation. For any integer m/greaterorequalslant1, let [m] :={1,2,...,m}andSmbe the set of all permutations
on [m]. We use σ−1to deﬁne the inverse of permutation σ∈ Sm. In any graph G= (V,E), we use
Vto denote the vertices and Efor the edges. Throughout, we set n:=|V|to denote the number of
vertices. For any two vertices s,t∈V, we write s→tto denote that there is a path of length one,
namely, an edge, from stot, ands/squigglerighttto denote there is a path of arbitrary length (the direction
of the path will be clear from the context).
Thetotal variation distance of any two distributions µandνover the same support Ω is:
/bardblµ−ν/bardbltvd= max
Ω′⊆Ω(µ(Ω′)−ν(Ω′)) =1
2/summationdisplay
s∈Ω|µ(s)−ν(s)|.
Fact 2.1. Given a single sample schosen uniformly at random from one of the distributions µ
andν, the best probability of successfully deciding whether scame from µorνis1
2+1
2·/bardblµ−ν/bardbltvd
(obtained via the maximum likelihood estimator).
Proposition 2.2 (cf. [DP09]).For any mindependent 0/1-random variables X1,X2,...,X mwith
X=/summationtextm
i=1Xi, and0/lessorequalslantǫ/lessorequalslant1,
Pr(X/lessorequalslant(1−ǫ)·E[X])/lessorequalslantexp/parenleftbigg−ǫ2·E[X]
3/parenrightbigg
.
2.1 Robust Communication Complexity
Our lower bounds for random order streaming algorithms are p roven through (one-way) robust
communication complexity introduced by [ CCM08]. We provide some brief context here and refer
the interested reader to [ CCM08] for more information.
LetGnbe any subset of n-vertex graphs and f:Gn→ {0,1}be a function. In this model, we
pick a graph GfromGnadversarially, and then each edge of Gis assigned uniformly at random to
eitherGAorGB, given to Alice and Bob, respectively. The players have acce ss to a shared public
tape of random bits Rand follow a protocol Π. In the protocol Π, Alice sends a singl e message π
based on ( GA,R) to Bob, and Bob needs to output f(GA∪GB) based on π,GB, andR. We use
Π(GA,GB,R) to denote the output of the protocol on inputs GA,GBand random bits R. Protocol
Π is said to be a δ-error protocol for 0 /lessorequalslantδ/lessorequalslant1 for function fif for any graph GinGn, we have,
Pr(Π(GA,GB,R) =f(G))/greaterorequalslant1−δ,
where the probability is over the random partitioning into ( GA,GB) and random bits of R. The
communication cost of a protocol Π for fis the worst-case length of its message.
Deﬁnition 2.3. Therandomized one-way communication complexity of a function f
with error at most δ, denoted by− →Rδ(f) is the minimum possible communication cost of a
protocol for fwith error at most δfor every graph Gchosen from Gn.
Thedistributional one-way communication complexity of a function fwith error at
mostδover a distribution µonGn, denoted by− →Dδ,µ(f) is the minimum possible communication
cost of a deterministic protocol for fwith error at most δover graphs Gchosen from µ.
Proposition 2.4 (Yao’s minimax principle [ Yao77]).For any function fand error 0/lessorequalslantδ/lessorequalslant1,
− →Rδ(f) = max
µ− →Dδ,µ(f).
6Thefollowingstandardresultrelates robustcommunicatio n complexity tostreamingalgorithms.
Proposition 2.5 (cf. [CCM08]).Any single-pass streaming algorithm that given edges of a gr aph
GfromGnin a random-order stream computes f(G)w.p. at least 1−δrequiresΩ(− →Rδ(f))space.
Proof.Consider the following protocol Π: Alice randomly permutes the edges of GA, runs the
streaming algorithm over them, and sends the memory content as the message πto Bob. Bob takes
a random permutation of GB, continues running the algorithm, and outputs the answer. T he claim
follows since the random partitioning of the input plus the r andom permutation of the edges leads
to a random order stream, and size of πis bounded by the communication cost of Π.
2.2 Biases and XORs
We use a basic application of the KKL inequality [KKL88] from Fourier analysis in our results.
This approach has been at the core of various Fourier analyti c lower bounds in streaming (see,
e.g. [GKK+07,VY11,KKS15]). In this work, we directly apply a technical corollary of t his result
stated in [ Wol08] without relying on the Fourier analytic parts explicitly. As such, in the following,
we only provide the necessary background for this corollary .
LetXbe any random variable with support {0,1}. ThebiasofXis deﬁned as:
bias(X) :=|Pr(X= 1)−Pr(X= 0)|.
Notice that this way, for any b∈ {0,1}, Pr(X=b)/lessorequalslant(1+bias(X))/2.
LetA⊆ {0,1}mandYbe a random variable denoting a string chosen uniformly at ra ndom
fromA. For any set S⊆[m], we slightly abuse the notation and write:
biasA(S) =bias/parenleftBigg/circleplusdisplay
i∈SYi/parenrightBigg
,
where/circleplustextdenotes the XOR function. In words, biasA(S) is the bias of XOR of indices in Sof a
random string from A.
The following corollary of KKL inequality [ KKL88] captures the property we need from biases.
Proposition 2.6 (cf. [Wol08, Section 4.2]) .For any integers m,k/greaterorequalslant1, letA⊆ {0,1}mbe arbitrary
andSbe chosen uniformly at random from k-subsets of [m]. Then,
E
S/bracketleftBig
biasA(S)2/bracketrightBig
=O/parenleftbigg1
m·log/parenleftbigg2m
|A|/parenrightbigg/parenrightbiggk
.
An interpretation of Proposition 2.6 is in order. Suppose |A|= 2m−tfor some (small) t/greaterorequalslant1,
thus the RHS above is O(t
m)k. When k= 1, the bound above is rather straightforward as we
expect the bias-squared of a single coordinate in Ychosen randomly from Ato be roughly O(t/m).
This is because the entropy of a random coordinate of Yis at least 1 −t/m, making this bit
quite close to uniform. But for k= 2, already Proposition 2.6 becomes quite interesting. It is
an easy exercise to show that XOR of independent bits has a dampened bias compared to each
bit, i.e., bias(P⊕Q) =bias(P)·bias(Q) for two independent random variables P,Q∈ {0,1}.
Proposition 2.6 extends this property to random indices ( i,j) ofYchosen uniformly at random
fromA, even though coordinates YiandYjarenotindependent anymore! Thus, Proposition 2.6 is
a vast generalization of the bias-dampening property of XOR to anyarbitrary setA(of suﬃciently
large size).
73 A Robust Lower Bound for Noisy Gap Cycle Counting
The following theorem is our main technical contribution wh ich formalizes Result 1.
Theorem 1. For suﬃciently large n/greaterorequalslant1andk/greaterorequalslant1600·lnn,
− →R1/3(NGCn,k) = Ω(n).
To prove Theorem 1 , we want to give a hard distribution µngcsuch that the deterministic com-
munication complexity of NGCn,kover distribution µngcis Ω(n). First, we describe the primitive
constructs needed to deﬁne our hard distribution µngc. Once we have these constructs, we deﬁne
µngcand give a reduction from NGCn,koverµngcto a new intermediate problem we introduce
here, called the distributional hidden XOR ( DHX) problem. We conclude by showing how a lower
bound for the DHXproblem gives a lower bound for NGCn,k. The lower bound for DHXitself
is proved in the next section.
3.1 Building Blocks
In this section, we deﬁne the basic constructs needed to desc ribe our hard distribution for NGCn,k.
We start by deﬁning a group-layered graph (see Section 3.1 for an illustration).
Deﬁnition 3.1 (Grouped-Layered Graph ).For integers w,d/greaterorequalslant1, we deﬁne a group-
layered graph as any graph G= (V,E) satisfying the following properties:
(i) Vertices of Gare partitioned into dequal-size layersV1,...,Vd, each of size 2 w. We
identify each of these vertex-sets by [2 w].
(ii) The vertices in each layer Vifori∈[d] are partitioned into wgroups
gi
1= (ai
1,bi
1), gi
2= (ai
2,bi
2),···, gi
w= (ai
w,bi
w).
(we always pick the lexicographically-ﬁrst way of grouping the vertices).
(iii) The edges of Gared−1perfect matchings M1,...,Md−1between consecutive layers such
that for any i∈[d−1] andj∈[w], both vertices in group gi
jare matched to vertices of a
single group gi+1
j′forj′∈[w] in the next layer.
We refer to was thewidthof the layered graph and das itsdepth. We use Lw,dto denote
the set of all layered graphs with width wand depth d.
Finally, for any i∈[d] andj∈[w], we deﬁne PG(j) =j′wherej′is the index of the unique
groupgd
j′in the last layer whose vertices are reachable from those of g roupg1
jin the ﬁrst layer.
We need a simple notation for concatenating group-layered graphs. For any w,d1,d2/greaterorequalslant1, let
G1∈ Lw,d1andG2∈ Lw,d2be two layered graphs on layers ( V1,...,Vd1) and (U1,...,Ud2). We
useG1/bardblG2to denote the graph consisting of d1+d2−1 layers ( W1,...,Wd1+d2−1) such that:
(W1,...,Wd1) = (V1,...,Vd1) and ( Wd1,...,Wd1+d2−1) = (U1,...,Ud2).
We are setting Wd1=Vd1=U1(as both sets are identiﬁed by [2 w]). The edges of G1/bardblG2are the
union of edges of G1andG2. This makes G1/bardblG2a grouped-layered graph in Lw,d1+d2−1. We also
observe the following about how the edges and functions PG1(·),PG2(·) interact in G=G1/bardblG2.
8V1V3V4V2
g1
1
g2
1
g3
1
g4
1
Figure 1: Illustration of a group layered graph in L4,4fromDeﬁnition 3.1 .
Observation 3.2. LetG=G1/bardblG2forG1∈ Lw,d1,G2∈ Lw,d2; forj∈[w],PG(j) =PG2(PG1(j)).
Proof.PG1(j) takes the group g1
jto a group gd1
j′in the “middle” layer of G(the shared layer of G1
andG2inG), andPG2(j′) then takes gd1
j′togd1+d2−1
j′′, which makes PG(j) =PG2(j′) =j′′.
We need some more structure on the edges in a group-layered gr aph in our hard distribution,
which motivates the following deﬁnitions (see Figure 2).
Deﬁnition 3.3 (XOR-Matching ).Letx∈ {0,1}wbe a string. We deﬁne XOR-matching
ofxas the grouped-layered graph G∈ Lw,2on two layers, denoted by G=XOR-Matching (x),
so that its single perfect matching Mis deﬁned as follows:
For any j∈[w] and groups g1
j= (a1
j,b1
j) andg2
j= (a2
j,b2
j), ifxj= 0, then both edges
(a1
j,a2
j),(b1
j,b2
j) belong to M, and otherwise both edges ( a1
j,b2
j),(b1
j,a2
j) belong to M.
Observation 3.4. InG=XOR-Matching (x), for any j∈[w],PG(j) =j.
Proof.For any group g1
j∈V1, irrespective of xj, the vertices a1
j,b1
jare connected by edges only to
vertices of the group g2
j∈V2. Thus,PG(j) =jfor each j∈[w].
Deﬁnition 3.5 (Perm-Matching ).Letσ∈ Swbeapermutation. Wedeﬁne perm-matching
ofσas the grouped-layered graph G∈ Lw,2on two layers, denoted by G=Perm-Matching (σ),
so that its single perfect matching Mis deﬁned as follows:
For anyj∈[w] and group g1
j= (a1
j,b1
j), the edges ( a1
j,a2
σ(j)) and (b1
j,b2
σ(j)) belong to M.
Observation 3.6. InG=Perm-Matching (x), for any j∈[w],PG(j) =σ(j).
Proof.The vertices in group g1
jfor anyj∈[w] are mapped by edges to vertices in group g2
σ(j)in
Perm-Matching (σ), making PG(j) =σ(j).
The basic structure in our hard distribution is a block which is described next. It is constructed
usingDeﬁnition 3.3 andDeﬁnition 3.5 , and is used extensively throughout the rest of the paper.
9(a)XOR-Matching (x) forx= 1001. (b)Perm-Matching (σ) forσ= (3,1,2,4).
Figure 2: An illustration of XOR-matchings and perm-matchings in L4,2deﬁned in Deﬁnitions 3.3 and3.5.
Deﬁnition 3.7 (Block).Letx∈ {0,1}w,σ∈ Sw, and
G1=Perm-Matching (σ), G2=XOR-Matching (x), G3=Perm-Matching (σ−1).
We deﬁne blockB:=Block(x,σ)∈ Lw,4asB=G1/bardblG2/bardblG3.
σ σ−1 x
Figure 3: Illustration of Block(x,σ) forx= 1001 and σ= (3,1,2,4) fromDeﬁnition 3.7 .
A few useful properties obeyed by paths in blocks are stated n ow.
Observation 3.8. In anyB=Block(x,σ)∈ Lw,4, we have,
(i)Forj∈[w],PB(j) =j;
(ii)Forj∈[w], ifxσ(j)= 0, thena1
j/squigglerighta4
jandb1
j/squigglerightb4
j; else,a1
j/squigglerightb4
jandb1
j/squigglerighta4
j.
Proof.We have,
PB(j) =PG3(PG2(PG1(j))) =σ−1(σI(σ(j))) =j,
whereσIis the identity permutation, the ﬁrst equality is by Observation 3.2 , and the second one
is by applying Observation 3.6 ,Observation 3.4 , andObservation 3.6 again. This proves part ( i).
Ifxσ(j)= 0, (a1
j→a2
σ(j)→a3
σ(j)→a4
j) is a path from a1
jtoa4
j, and (b1
j→b2
σ(j)→b3
σ(j)→b4
j)
is a path from b1
jtob4
j. Ifxσ(j)= 1, (a1
j→a2
σ(j)→b3
σ(j)→b4
j) is a path from a1
jtob4
j, and
(b1
j→b2
σ(j)→a3
σ(j)→a4
j) is a path from b1
jtoa4
j, proving part ( ii).
We can concatenate multiple blocks to get larger group-laye red graphs as follows.
10Deﬁnition 3.9 (Multi-Block Graph ).Letw,t/greaterorequalslant1,X= (x1,...,xt)∈({0,1}w)t, and
Σ = (σ1,...,σt)∈(Sw)t. We deﬁne multi-block graph G:=Multi-Block (X,Σ) as the group-
layered graph G∈ Lw,3t+1obtained as G:=B1/bardblB2/bardbl.../bardblBt, whereBi=Block(xi,σi).
For anyj∈[w], we deﬁne zG(j) :=/circleplustext
i∈[t]xi
σi(j).
σ1σ1
−1σ2 σ2
−1 x1x2
Figure 4: An illustration of Multi-Block (X,Σ) forX= (1001,0110), and Σ = ((3 ,1,2,4),(2,1,4,3)).
Observation 3.8 on blocks can be extended to multi-block graphs as well.
Observation 3.10. In anyG=Multi-Block (X,Σ)∈ Lw,d, we have,
(i)Forj∈[w],PG(j) =j;
(ii)Forj∈[w], ifzG(j) = 0, thena1
j/squigglerightad
jandb1
j/squigglerightbd
j; else,a1
j/squigglerightbd
jandb1
j/squigglerightad
j.
Proof.LetGi=B1/bardblB2.../bardblBibe used to denote the concatenation of the ﬁrst iblocks for i∈[t]
withG=Gt. EachGi∈ Lw,3i+1fori/greaterorequalslant2 andG1∈ Lw,4. These statements can be proven by a
straightforward induction on i.
To prove part ( i), we know PB1(j) =jfromObservation 3.8 . Let us assume PGi−1(j) =j.
Observation 3.2 givesPGi(j) =PBi(PGi−1(j)) =PBi(j) =jfor each i∈[t].
We now prove part ( ii). It is easy to see that zGi(j) =zGi−1(j)⊕xi
σi(j)andzB1(j) =x1
σ1(j).
FromObservation 3.8 ,a1
j/squigglerighta4
jandb1
j/squigglerightb4
jifzG1(σ(j)) = 0. If zG1(σ(j)) = 1,a1
j/squigglerightb4
jand
b1
j/squigglerighta4
j. Let us assume that the statement is true for the graph Gi−1with 1/lessorequalslanti < t. We know
Gi=Gi−1/bardblBi, hence the layer V3(i−1)+1is treated as the ﬁrst layer of Bi. We analyze the two
possible cases for the value taken by zGi(j) as follows.
•IfzGi−1(j) =xi
σi(j), thenzGi(j) = 0. If zGi−1(j) =xi
σi(j)= 0, we have paths from a1
j/squiggleright
a3(i−1)+1
j/squigglerighta3i+1
jandb1
j/squigglerightb3(i−1)+1
j/squigglerightb3i+1
j. IfzGi−1(j) =xi
σi(j)= 1, then we have paths
froma1
j/squigglerightb3(i−1)+1
j/squigglerighta3i+1
jandb1
j/squigglerighta3(i−1)+1
j/squigglerightb3i+1
j.
•IfzGi(j) = 1, then either zGi−1(j) = 0,xi
σi(j)= 1, where we have with paths from a1
j/squiggleright
a3(i−1)+1
j/squigglerightb3i+1
jandb1
j/squigglerightb3(i−1)+1
j/squigglerighta3i+1
j, orzGi−1(j) = 1,xi
σi(j)= 0, with paths from
a1
j/squigglerightb3(i−1)+1
j/squigglerightb3i+1
jandb1
j/squigglerighta3(i−1)+1
j/squigglerighta3i+1
j.
This concludes our setup. We now have all the constructs nece ssary to describe our hard
distribution for NGCn,k.
113.2 A Hard Distribution for NGC
In this section, we will describe our hard distribution µngcforNGCn,konly forcertain values ofk,
and prove the validity of the distribution (the extension to all choices of kis straightforward and
is done in the proof of Theorem 1 itself).
Our distribution will be a multi-block graph from Deﬁnition 3.9 with strings xiand permuta-
tionsσisampled at random from {0,1}(n/2k)andSn/2krespectively, conditioned on correlation of
the bitzG(j) for some subsetof groups jin the ﬁrst layer. We add additional edges to create cycles
so that the graph is a valid instance of NGCn,k. Formally,
A hard distribution µngcfor NGC n,k(whenk= 3t+1 fort/greaterorequalslant1 andn= 4k·mform/greaterorequalslant1).
(i) Sample θ∈ {0,1}uniformly at random.
(ii) Sample X=/parenleftbig
x1,...,xt/parenrightbig
∈/parenleftBig
{0,1}2m/parenrightBigt
and Σ = ( σ1,...,σt)∈(S2m)tuniformly at random
conditioned on the event that for all j∈[m]a:
/circleplusdisplay
i∈[t]xi
σi(j)=θ.
(iii) LetG=Multi-Block (X,Σ)∈ L2m,kplus theauxiliary edges (ak
j,a1
j) and (bk
j,b1
j) forj∈[m].
aWe emphasize that this property is only for the ﬁrst mindices and not all 2 mof them.
The following ﬁgure gives an illustration of the distributi onµngc.
σ1σ2x1x2σ1
−1 σ2
−1
Figure 5: An illustration of graph Gsampled from µngcwith strings X= (100101 ,011001), and permuta-
tions Σ = ((3 ,1,4,2,6,5),(2,1,4,5,3,6)) when θ= 1. The thicker edges show that zG(j) = 1 for j∈[3].
The dashed edges are the auxiliary edges added to Multi-Block (X,Σ).
12We prove that the graphs sampled from µngcsatisfy the guarantee of the NGCproblem.
Lemma 3.11. For any graph G∼µngcforNGCn,k:
(i)ifθ= 0,Ghas(n/2k)vertex-disjoint cycles of length k;
(ii)ifθ= 1,Ghas(n/4k)vertex-disjoint cycles of length 2k.
In either case, Gadditionally has (n/2k)vertex-disjoint paths of length k−1.
Proof.First let us show that irrespective of what θis,Ghas (n/2k) vertex-disjoint paths of length
k. Consider any group g1
jin layerV1ofGform < j/lessorequalslant2m. We know vertices of g1
jare connected
by a path to vertices of the group gk
jfromObservation 3.10 asPG(j) =j. Either paths ( a1
j/squiggleright
ak
j),(b1
j/squigglerightbk
j) exist, or paths ( a1
j/squigglerightbk
j),(b1
j/squigglerightak
j) exist (which of these two cases happens depends
onzG(j) which we have not conditioned on). The length of any such pat h isk−1 as edges in G
only go from one layer to the next. Moreover, this path is a sep arate connected component of G,
asGis a layered graph with no additional edges to any a1
j,ak
j,b1
jorbk
jform < j/lessorequalslant2m. There are
a total of 2 m= (n/2k) such vertices in layer V1, and if we consider all the paths connecting these
vertices to vertices of Vk, we get ( n/2k) vertex-disjoint paths.
Ifθ= 0, we know zG(j) = 0 for every j∈[m]. Hence, paths ( a1
j/squigglerightak
j),(b1
j/squigglerightbk
j) of length
k−1 exist by Observation 3.10 . But additional edges ( a1
j,ak
j) and (b1
j,bk
j) are added to G. Thus,
(a1
j/squigglerightak
j→a1
j) is a cycle in Gof length k. AsGis layered and the degree of each vertex is at most
2, this cycle is a connected component of G. In fact, there are 2 m=n/2kvertex-disjoint cycles of
lengthkas (a1
j/squigglerightak
j→a1
j) and (b1
j/squigglerightbk
j→b1
j) are cycles for each j∈[m].
Ifθ= 1, we know zG(j) = 1 for every j∈[m]. Paths ( a1
j/squigglerightbk
j),(b1
j/squigglerightak
j) of length k−1
are present in G, again by Observation 3.10 . These paths are similarly vertex-disjoint from other
paths in Gas the degree of every vertex is at most 2. Thus, for each j∈[m] we get the cycle
(a1
j/squigglerightbk
j→b1
j/squigglerightak
j→a1
j) of length 2 kif we incorporate the auxiliary edges, giving us m= (n/4k)
vertex disjoint cycles of length 2 k.
For any graph Gsampled from distribution µngc, we have zG(j) =θfor each j∈[m]. Thus,
theX-bits on the paths from vertices of the ﬁrst mgroups in layer V1to vertices in layer Vkare
correlated. We remove this correlation as the next step to pr oving our lower bound.
3.3 The Distributional Hidden-XOR (DHX) Problem
In this section, we deﬁne the distributional hidden XOR ( DHX) problem and give a reduction to
it fromNGCn,kover distribution µngcwith a decorrelation step. The intuition behinds this step i s
as follows. For any algorithm that attempts to solve NGCn,kon distribution µngc, it is suﬃcient
to ﬁndzG(j) forsomej∈[m] with probability at least 2/3. However, we will show that su ch an
algorithm also has to ﬁnd zG(h∗) for aﬁxedh∗∈[m] albeit with a lower probability of (roughly)
at least1
2+1
m. The following problem and the subsequent lemma capture thi s intuition.
Problem 2 ((Distributional) Hidden-XOR Problem (DHX)).InDHXw,t, we have a graph
G=Multi-Block (X,Σ)∈ Lw,3t+1forX∈({0,1}w)tandΣ∈(Sw)tchosen independently and
uniformly at random. The goal is to output zG(1)in this graph when the edges of the graph are
randomly partitioned between Alice and Bob, and Alice can send a singl e message to Bob.
This is the main lemma of this subsection, giving a lower boun d forNGCn,kusingDHX.
13Lemma 3.12. For any suﬃciently large n/greaterorequalslant1,k= 3t+1for some t/greaterorequalslant1andw= (n/4k)+1,
− →Dµngc,1/3(NGCn,k)/greaterorequalslant− →Dµdhx,1
2−1
6w(DHXw,t),
whereµdhxis the implicit uniform distribution in the deﬁnition of DHX.
We design a way for Alice and Bob to solve DHXw,tusingany protocol for NGCn,kby a hybrid
argument. Consider the following series of distributions f or 0/lessorequalslanth/lessorequalslantm.
Distribution H(h).SampleX=/parenleftbig
x1,x2,...,xt/parenrightbig
∈/parenleftBig
{0,1}2m/parenrightBigt
, Σ =/parenleftbig
σ1,σ2,...,σt/parenrightbig
∈(S2m)t
independently and uniformly at random conditioned on
zG(j) =/circleplusdisplay
i∈[t]xi
σi(j)=/braceleftBigg
0 ifj/lessorequalslanth,
1 otherwise .
Notice that the distribution µngcis just1
2(H(0)+H(m)) along with the ﬁxed auxiliary edges.
We say that a protocol Π distinguishes between two distributions D1andD2with probability
p∈[0,1], if given a sample schosen uniformly from D1andD2, running Π on sallows us to
identify which distribution swas sampled from with probability at least p.9Let Π be a protocol
forNGCn,kwith probability of success at least 2 /3. We use Π in a non-black-box way to identify
an index h∗∈[m] such that Π distinguishes samples from H(h∗−1) andH(h∗) w.p. above 1 /2.
Lemma 3.13. There exists an index 1/lessorequalslanth∗/lessorequalslantmsuch that protocol Πcan distinguish the samples
fromH(h∗−1)andH(h∗)with probability at least1
2+1
6m.
Proof.With a slight abuse of notation, let out(Π) denote the set of random variables used by Bob
to determine the output of the protocol, namely, the message received by Alice and his own input.
Protocol Π identiﬁes whether any sample from µngcis sampled from H(0) orH(m) with probability
at least 2/3. Consider two distributions out(Π)| H(0) andout(Π)| H(m). As we can distinguish
these distributions with probability at least2
3or advantage1
6, byFact 2.1,
/bardblout(Π)| H(0)−out(Π)| H(m)/bardbltvd/greaterorequalslant1
6.
However from the triangle inequality property of the total v ariation distance, we also have that,
/bardblout(Π)| H(0)−out(Π)| H(m)/bardbltvd/lessorequalslantm/summationdisplay
i=1/bardblout(Π)| H(i−1)−out(Π)| H(i)/bardbltvd.
By a simple averaging argument, there exists an index 1 /lessorequalslanth∗/lessorequalslantmsuch that,
/bardblout(Π)| H(h∗)−out(Π)| H(h∗−1)/bardbltvd/greaterorequalslant1
6m.
That is, out(Π) can be used to idenﬁty samples from H(h∗−1) andH(h∗) with advantage at least
1/6m, again by Fact 2.1. Thus, we can use Π to distinguish between samples from H(h∗) and
H(h∗−1) with probability at least1
2+1
6m.
9To clarify, in general, the protocol Π may not have been desig ned for this task and thus we cannot solely rely on
its output (which may not even be well-deﬁned in certain case s). Thus, we rather focus on the maximum likelihood
estimator for the joint distribution of the message πand Bob’s input to distinguish between the two distribution s.
14Now that we have an index h∗∈[m], our plan to solve DHXw,tis to embed the starting vertex
of any instance of DHXgiven to Alice and Bob in h∗ofHand use protocol Π. To do so, Alice
and Bob need a way to sample from the distribution H∗=1
2(H(h∗)+H(h∗−1)) and we will show
that such a sampling process exists. We need to deﬁne some not ation on partial assignments and
permutations before we proceed.
Notation. For anyI⊆[w],XI
wdenotes the set of all x∈ {0,1,⋆}wsuch that xi∈ {0,1}ifi∈I
andxi=⋆otherwise. That is, the indices in subset Iare ﬁxed to be either 0 or 1, whereas the
indices outside Iare not ﬁxed yet, with the ambiguous value represented by ⋆. Similarly, deﬁne SI
w
as the set of all permutations σof [w] which ﬁx σ(i)∈[w] fori∈I, andσ(i) =⋆for anyi /∈I. For
anyσ∈ SI
w, there are ( w−|I|)! ways to extend σto a permutation σ′∈ Swby ﬁxing indices i /∈I.
Wecannowdescribethealgorithm usedbyAliceandBobtosamp lefromH∗andsolve DHXw,t.
In the following, to avoid ambiguity, we use Yand Φ as the inputs to Multi-Block (Y,Φ) for the
DHXproblem, and Xand Σ as inputs to some Multi-Block (X,Σ) forNGCthat Π operates on.
Algorithm 1. A protocol for DHXw,tusing protocol Π for NGCn,kwithw=n
4k+1,k= 3t+1.
Input:An instance of DHXw,t,G=Multi-Block (Y,Φ),Y=/parenleftbig
y1,y2,...,yt/parenrightbig
∈({0,1}w)t, and
Φ = (φ1,φ2,...,φt)∈(Sw)t.
Output: Answer to DHXw,tonG, i.e.,zG(1).
(i) Sample Σ [m]\{h∗}:= (σ1,...,σt)∈/parenleftBig
S[m]\{h∗}
2m/parenrightBigt
uniformly at random.
(ii) LetIi
◦=/braceleftbig
σi(j)|j∈([m]\{h∗})/bracerightbig
fori∈[t], that is, Ii
◦is the set of indices that have a
non-⋆pre-image under permutation σi. The set Ii
◦hasm−1 elements for all i∈[t].
(iii) Sample X[m]\{h∗}= (x1,x2,...,xt)∈/parenleftBig
XI1
◦
2m×XI2
◦
2m×...XIt
◦
2m/parenrightBig
uniformly at random condi-
tioned on the event that,
/circleplusdisplay
i∈[t]xi
σi(j)=/braceleftBigg
0 if 1/lessorequalslantj/lessorequalslanth∗−1,
1 ifh∗+1/lessorequalslantj/lessorequalslantm.
(iv) Lexicographically map indices [ m+ 1] to [2 m]\Ii
◦for each i∈[t]. This is possible since/vextendsingle/vextendsingle[2m]\Ii
◦/vextendsingle/vextendsingle=m+1 =w. Denote this mapping by fi: [m+1]→[2m]\Ii
◦.
(v) Update the positions with ⋆value in X[m]\{h∗}and Σ[m]\{h∗}to getXand Σ respectively
as follows. For each i∈[t],
σi(j) =/braceleftBigg
fi(φi(1)) if j=h∗
fi(φi(j−m+1)) if m < j/lessorequalslant2mandxi
fi(j)=yi
jforj∈[m+1].
(vi) FixG′=Multi-Block (X,Σ).
(vii) Run protocol Π on G′to identify whether G′∼ H(h∗−1) orG′∼ H(h∗); in the former
case output 0 and in the latter output 1.
First, we prove that the sampling process used by Algorithm 1 gives a graph sampled from H∗.
15Lemma 3.14. Given an instance GofDHXw,t, step(vi)ofAlgorithm 1 samplesG′∼ H∗without
any communication using its randomness and the randomness o f instance G.
Proof.We can view H∗as ﬁrst sampling tpermutations from Swindependently and uniformly at
random, and then sampling telements of {0,1}wconditioned on the choice of the permutations to
give appropriate target zG(i) for each i∈[m]\{h∗}.
•Permutations ΣofG′:In step (i),σi(j) is ﬁxed for m−1 values of j∈[m]\[h∗], but these
indices are picked uniformly at random from [2 m], and independently. It is enough if the
remaining m+1 indices in ([2 m]\[m])∪{h∗}are uniformly at random mapped to the set of
indices in [2 m] with no pre-image under σi, i.e., [2m]\Ii
◦. This is true because irrespective of
the mapping fi, we know that φiis chosen uniformly at random from Swand independently.
•Strings XofG′:From step ( iii), we know that X[m]\{h∗}is picked at random and indepen-
dently, conditioned on the choice of Σ [m]\{h∗}withzG′(j) = 0 for j < h∗, andzG′(j) = 1 for
h∗+ 1/lessorequalslantj/lessorequalslantmas required by H∗. The rest of the indices in Xare assigned based on Y,
butYis picked uniformly at random from ( {0,1}w)t. Hence the rest of the indices in Xare
chosen with equal probability from {0,1}, and independently of each other.
Finally, the fact that no communication is needed for the sam pling follows by construction: the
players sample Σ [m]\{h∗}andX[m]\{h∗}using public randomness, which ﬁxes the functions fi’s as
well. The rest is obtained privately by each player embeddin g the parts of their own private inputs
Yand Φ in proper places of Xand Σ.
Next, we show that Algorithm 1 successfully embeds the starting group 1 of instance Gof
DHXw,tin the group h∗of the sampled graph G′as required.
Claim 3.15. In graph G′in step (vi) of Algorithm 1 ,zG′(h∗) =zG(1).
Proof.We know for each i∈[t], from step ( v),
xi
σi(h∗)=xi
fi(φi(1))=yi
φi(1).
Hence,
zG′(h∗) =/circleplusdisplay
i∈[t]xi
σi(h∗)=/circleplusdisplay
i∈[t]yi
φi(1)=zG(1).
We can now use Lemma 3.14 andClaim 3.15 to complete the proof of Lemma 3.12 .
Proof of Lemma 3.12 .LetΠbeanyprotocolthatsolves NGCondistribution µngcwithprobability
at least 2/3. By Lemma 3.13 , we know that there exists an h∗∈[m] such that Π distinguishes
samples from H(h∗−1) andH(h∗) with probability at least1
2+1
6m.
Given an instance GofDHXw,twithw=n/4k+ 1 and t= (k−1)/3, byLemma 3.14 , in
Algorithm 1 graphG′is sampled from H∗=1
2(H(h∗−1)+H(h∗)).Claim 3.15 implies that iden-
tifying whether G′is sampled from H(h∗−1) orH(h∗) is identical to solving DHXon the instance
G. Hence Alice and Bob can run Algorithm 1 to solve instances of DHXw,twith probability at
least1
2+1
6m/greaterorequalslant1
2+1
6wsincew=m+1.
163.4 Proof of Theorem 1
So far, we have proved that a lower bound for DHXw,twith a low probability of success of1
2+1
6w
gives a lower bound for the distributional communication co mplexity of NGCn,kfor some values
ofk. We will state a lower bound for DHXw,teven against this low probability, and use this to
prove our main result Theorem 1 .
Lemma 3.16. For suﬃciently large w/greaterorequalslant1andt/greaterorequalslant512·lnw,
− →Dµdhx,1
2−1
6w(DHXw,t) = Ω(w·t),
whereµdhxis the implicit uniform distribution in the deﬁnition of DHX.
The proof of Lemma 3.16 is the main technical step of our paper and is postponed to the next
section. Here, we show that this lemma plus the previous step s concludes the proof of our main
lower bound in Theorem 1 .
Proof of Theorem 1 .Our hard distribution µngconly gives instances with k= 3t+1 for some t >0.
However, we can extend it to a hard distribution for any NGCn,kby a simple paddingargument. If
k≡0 mod 3, then we pick a sample from µngcforNGCn−2n/k,k−2, and add two additional layers
(V−1,V0) withn/kvertices in each. We add identity perfect matchings from V−1toV0and from
V0toV1. The auxiliary edges are added from layer Vk−2to layerV−1. Similarly if k≡2 mod 3,
we sample from NGCn−n/k,k−1and add one additional layer V0ofn/kvertices with an identity
perfect matching from V0toV1, and auxiliary edges from Vk−1toV0. Thus, we can focus on µngc
for the rest of the proof.
Forw= (n/4k)+1 and t/greaterorequalslant(k−3)/3/greaterorequalslant512·lnn/greaterorequalslant512·lnw, we have,
− →D1/3,µngc(NGCn,k)/greaterorequalslant− →Dµdhx,1
2−1
6w(DHXw,t) = Ω(n/k·k) = Ω(n),
wheretheﬁrstinequality followsfrom Lemma 3.12 , andthesecondboundfollows from Lemma 3.16 .
Applying Yao’s minimax principle in Proposition 2.4 now implies that
− →R1/3(NGCn,k)/greaterorequalslant− →D1/3,µngc(NGCn,k) = Ω(n),
concluding the proof.
Remark 3.17 (“Reducing the noise”) .The notion of “noise” in NGC, namely, the extra ( k−1)-
length paths, is used in our decorrelation step. In particul ar, using mnoisy paths allowed us to
reduce the lower bound for NGCto a proving a “low-probability” lower bound for DHXon layers
of sizem+ 1 each. On the other hand, the lower bound for DHXstated in Lemma 3.16 (and
proven in the next section), is Ω( w·t) as long as the layers are of size w(and has some large
deptht). Thus, as evident from the proof, the choice of mnoisy paths in our deﬁnition of NGCis
not sacrosanct and can be replaced by any other Θ( m) and still lead to the same asymptotic lower
bound (we only chose mfor simplicity of exposition). As such, we can eﬀectively obt ain the same
lower bound of Ω( n) forNGCeven if we reduce the number of noisy paths to o(n/k).
We could have alternatively entirely removed the noisy path s using a similar reduction as
in [AKSY20 , Section 3.5]. That however would forced us to replace the ro le of XOR in our con-
struction with gadgets that implement arithmetic modulo co nstants larger than 2 (say, 5). This
in turn, requires an analogue of Proposition 2.6 for larger ﬁelds to prove our lower bound (which
do exist, see, e.g. [ GT19, Lemma 2.3]). While this approach seems plausible , given the benign role
of the noise and the fact that it virtually does not change any of the subsequent reductions from
NGC, we opted to forego this step in favor of a simpler and more dir ect proof.
174 A Lower Bound for Distributional Hidden-XOR
This section focuses on our lower bound for DHXw,tinLemma 3.16 , restated below.
Lemma 4.1 (Restatement of Lemma 3.16 ).For suﬃciently large w/greaterorequalslant1andt/greaterorequalslant512·lnw,
− →Dµngc,1/3(NGCn,k)/greaterorequalslant− →Dµdhx,1
2−1
6w(DHXw,t),
whereµdhxis the implicit uniform distribution in the deﬁnition of DHX.
Our one-way communication lower bound for DHXw,tuses the random partitioning of edges
between Alice and Bob crucially. We will ﬁrst show an alterna tive way of viewing the partitioning
the inputs ( X,Σ) to the multi-block graph in DHXw,t. Then, we describe conﬁgurations of parti-
tions that are favorable to us in that they force Alice to send longer messages and show that these
partitions occur with a high probability in instances of DHXw,t. We use these to prove that the
messages of Alice to Bob must have length Ω( wt) to solve DHXand obtain the ﬁnal lower bound.
4.1 Input Partitioning and Active Blocks
In this section, we argue that the partitioning of the edges i nGcan be done independently of
(X,Σ), and deﬁne some partitions for which Alice has to send a lot of information about the edges
inEA. We will also prove that such partitions of Eoccur with a high probability.
LetG=Multi-Block (X,Σ)∈ Lw,3t+1forX∈({0,1}w)tand Σ∈(Sw)tbe chosen independently
and uniformly at random (as in the distribution of DHXw,t). LetB1,...,B tbe the blocks in G.
We consider the following process for partitioning the edge s of each block between Alice and Bob.
An alternative way of partitioning edges in a block B(seeFigure 6 ).
(i) Let (V1,V2,V3,V4) be the layers in B, each a copy of [2 w], andfL,fM,fR: [2w]→ {α,β}
be three given functions.
(ii) Partition the edges of Bbetween Alice and Bob as follows:
(a) For any u∈V2, send the edge ( w,u) forw∈V1to Alice if fL(u) =α, else to Bob;
(b) For any u∈V2, send the edge ( u,v) forv∈V3to Alice if fM(u) =α, else to Bob;
(c) For any v∈V3, send the edge ( v,z) forz∈V4to Alice if fR(v) =α, else to Bob.
fL fM fR
Alice Bob Bob
Figure 6: An illustration of a partition for j∈[2w] in block BforfL(j) =α,fM(j) =βandfR(j) =β.
These functions, if picked at random, can be used to give a par tition of the edges in Gobeying
the conditions in Problem 2 , as shown in the following.
Observation 4.2. LetG=Multi-Block (X,Σ)∈ Lw,3t+1and pick ttuples of functions
F= (FL,FM,FR) := (fi
L,fi
M,fi
R: [2w]→ {α,β})t
i=1
independently and uniformly at random. The partitioning pro cess above using these functions leads
to a uniform partitioning of edges of Gbetween Alice and Bob.
18Proof.The partitioning of edges only depends on the three function sfL,fMandfR, and these
functions are picked uniformly at random and independently .
The importance of these partitions is that F(henceforth referred to as partition functions )
are chosen independently of ( X,Π). We now use this to state our main deﬁnitions in this sectio n.
Deﬁnition 4.3 (Clean Indices ).LetBbeablock and fL,fM,fR: [2w]→ {α,β}bepartition-
functions. We say that an index j∈[w] iscleaniﬀ the following holds for vertices of groups
g2
j= (a2
j,b2
j) andg3
j= (a3
j,b3
j) in layers two and three of B:
fL(a2
j) =fL(b2
j) =β, fM(a2
j) =fM(b2
j) =α,andfR(a3
j) =fR(b3
j) =β.
We letclean(B) denote the set of clean indices in Bunder (fL,fM,fR).
fL fM fR
Bob Bob Alice
Bob Bob Alice
Figure 7: An illustration of the partitions for a clean index from Deﬁnition 4.3 .
For any clean index j∈[w] in block B=Block(x,σ), it is clear that only Alice has access to
xj, and Bob knows the inverse of junderσ. Alice may know the inverse based on the other edges,
but the edges from g1
σ−1(j)tog2
jand the edges from g3
jtog4
σ−1(j)are given only to Bob. We prove
that there are a lot of clean indices in any block Busing a simple application of Chernoﬀ bound.
Lemma 4.4. For any ﬁxed block Band a random choice of F= (fL,fM,fR: [2w]→ {α,β}):
Pr/parenleftBig
|clean(B)|/greaterorequalslantw/100/parenrightBig
/greaterorequalslant1−1/w4.
Proof.LetRjbe the indicator random variable for whether index j∈[w] is clean in block Bunder
the choice of F. We want to prove that R=/summationtext
j∈[w]Rjis at least w/100 with high probability.
There are six edges associated with each index j, and each of these edges must be distributed
appropriately for jto be clean. The functions are chosen independently and unif ormly at random.
HenceRj= 1 with probability 1 /26, and 0 otherwise for each j∈[w] independently. Chernoﬀ
bound from Proposition 2.2 is applicable here. We know E[R] =/summationtextw
j=1E[Rj] =w/26. Thus,
Pr/parenleftBig
R <w
100/parenrightBig
= Pr/parenleftbigg
R </parenleftbigg
1−36
100/parenrightbigg
·E[R]/parenrightbigg
/lessorequalslantexp/parenleftbigg−1
3·0.362·w
64/parenrightbigg
/lessorequalslant1
w4.
To make the analysis simpler, we restrict the number of clean indices in any block Bifori∈[t]
that has at least w/100 clean indices to be at most
wc:=w
100, (1)
by picking the lexicographically-ﬁrst wcclean indices and discarding the rest (namely, no longer
call them as clean indices). This process again depends only on the partition functions F.
19In any block B, if there are a lot of clean indices, from the perspective of A lice, a lot of edges are
missing from V1toV2and from V3toV4. Intuitively, any input index jmapped to a clean index
underσis tough for Bob to track, as he does not have any information a boutxσ(j). Alice cannot
sendxσ(j)because she has multiple choices for what σ(j) could be. Thus, if the starting index is
mapped to a clean index, the value of xat a lot of indices should be sent to Bob to communicate
xσ(1). We formalize this intuition with active blocks as deﬁned be low.
Deﬁnition 4.5 (Active Blocks ).LetB=Block(x,σ) andfL,fM,fR: [2w]→ {α,β}be
partition-functions. We say that Bisactiveif the index σ(1) is clean.
ForG=Multi-Block (X,Σ) and (FL,FM,FR), we deﬁne active(G) as the active blocks in G.
g1
1 g4
1Bob Bob
Aliceg2
σ(1) g3
σ(1)
Figure 8: Illustration of an active block where the dashed (red) edges are giv en to Bob and the normal
(green) edges are given to Alice from Deﬁnition 4.5 .
In an active block B, from the perspective of Alice, there are no edges out of grou pg1
1and no
edges into group g4
1. She does not know the index σ(1), and all indices j∈clean(B) are viable
choices for σ(1), as they do not have any edges into g2
jor out of g3
j. The following lemma proves
thatσ(1) can be any of these choices with equal probability.
Lemma 4.6. Suppose we sample B=Block(x,σ)andfL,fM,fR: [2w]→ {α,β}conditioned on
Bbeing active, and the partition functions F= (fL,fM,fR). Then, σ(1)is chosen uniformly at
random from clean(B).
Proof.First observe that any block Bis active with probability clean(B)/w, since index σ(1) is
chosen uniformly at random from [ w] andσis sampled independently of partition functions F. The
setclean(B) depends only on F.
Letcbe any index in clean(B). Then,
Pr(σ(1) =c|Bis active) =Pr(σ(1) =c)
Pr(Bis active)(c∈clean(B),σ(1) =cimpliesBis active)
= Pr(σ(1) =c)·w
clean(B)(asBis active with probabilityclean(B)
w)
=1
w·w
clean(B)(asσ(1) is uniformly random over [ w])
=1
clean(B).
We can also prove that the total number of active blocks in any G=Multi-Block (X,Σ) is high.
20Lemma 4.7. ForG=Multi-Block (X,Σ)andFsampled in DHX, witht/greaterorequalslant512·lnw:
Pr/parenleftBig
|active(G)|/greaterorequalslantlnw/parenrightBig
/greaterorequalslant1−1/w2.
Proof.BlockBibeing active is independent of other blocks for i∈[t], as it depends only on σiand
fi
L,fi
M,fi
R. For any i∈[t],
Pr(Biis active) = Pr/parenleftbig
σi(1) is clean/parenrightbig
=1
26,
as there are 6 edges associated with whether any index is clea n, and all these edges must be
partitioned according to Deﬁnition 4.3 . LetRibe the indicator variable for whether block Biis
active. We want to prove that R=/summationtextt
i=1Riis larger than ln wwith high probability. By linearity
of expectation, E[R] =t·1
26/greaterorequalslant8lnw. Chernoﬀ bound from Proposition 2.2 can be used here as
Ris are independent.
Pr(R/greaterorequalslantlnw)/greaterorequalslantPr/parenleftbigg
R/greaterorequalslant/parenleftbigg
1−7
8/parenrightbigg
·E[R]/parenrightbigg
/greaterorequalslant1−exp/parenleftbigg−49
64·8lnw
3/parenrightbigg
/greaterorequalslant1−1
w2.
We get our lower bound from graphs with a lot of active blocks, and we have proved that all
graphs have at least ln wactive blocks with high probability. In any active block Bi, Bob has no
information about xi
σ(1)as argued next.
Observation 4.8. Conditioned on a choice of active(G)forG=Multi-Block (X,Σ)∼DHX,Xis
still chosen uniformly at random.
Proof.Whetherablock isactive dependsonly onΣandthepartition f unctions F. Itisindependent
ofX, thus conditioned on a choice of active blocks, Xis still chosen uniformly at random.
This concludes our section on the partitioning of the edges i nG. In the next section, we will
use the presence of a lot of active blocks to prove Lemma 3.16 .
4.2 The Lower Bound
In this section we prove a lower bound for DHXw,t, as stated in Lemma 3.16 against a probability
of success1
2+1
6w. First, we condition the input graph so that only the edges co nnected to the clean
indices remain random, decreasing the probability of succe ss only slightly. Then we show that even
with this conditioning, Bob can only have a very low chance of guessing zG(1) correctly, by relying
onProposition 2.6 .
We know that a lot of active blocks exist and each of these bloc ks contains a lot of clean indices
in any sample GfromDHXw,twith high probability from the previous section. First, let us
condition on this event as the ﬁrst step towards proving Lemma 3.16 .
Claim 4.9. Any protocol ΠsolvingDHXw,tfort/greaterorequalslant29lnwwith probability at least1
2+1
6w, solves
DHXw,teven conditioned on input graph Gsatisfying |active(G)|/greaterorequalslantlnwand|clean(Bi)|/greaterorequalslantw
100for
eachi∈[t]with probability at least1
2+1
8wfor suﬃciently large w.
Proof.Let Π(G) denote the output of protocol Π on input graph G. Deﬁne:
•eventEactive: the event that there are at least ln wactive blocks in G;
21•eventEclean: the event that each block in Gcontains at least wc=w/100 clean indices.
Then, for any input G,
Pr(Π(G) =zG(1))/lessorequalslantPr(¬Eactiveor¬Eclean)+Pr(Eactive,Eclean)·Pr(Π(G) =zG(1)| Eactive,Eclean)
/lessorequalslantPr(¬Eactive)+Pr(¬Eclean)+Pr(Π( G) =zG(1)| Eactive,Eclean)
/lessorequalslant1
w2+Pr(¬Eclean)+Pr(Π( G) =zG(1)| Eactive,Eclean) (by Lemma 4.7 )
/lessorequalslant1
w2+t·Pr/parenleftBig
|clean(Bi)|<w
100/parenrightBig
+Pr(Π(G) =zG(1)| Eactive,Eclean)
(by union bound on number of blocks)
/lessorequalslant2
w2+Pr(Π(G) =zG(1)| Eactive,Eclean). (byLemma 4.4 andt < w)
Hence, for suﬃciently large w,
Pr(Π(G) =zG(1)| Eactive,Eclean)/greaterorequalslant1
2+1
6w−2
w2/greaterorequalslant1
2+1
8w.
Let thetotal numberofactive blocksingraph Gbeta.Claim 4.9 states that even ifwecondition
onta/greaterorequalslantlnwand that each block has at least wc=w
100clean indices, the probability of success
reduces only by a 1 /24wadditive factor. Thus, throughout the rest of this section, we condition
on these events happening in our input graph.
For simplicity of exposition, let us assume active(G) ={B1,B2,...,B ta}, that is only the ﬁrst
tablocks are active (which is without loss of generality by ren aming the blocks). Our next step is to
condition on all the permutations σiand strings xifori > ta, and some parts of the permutations
σi, and some indices in xifor active blocks i∈[ta]. We need some notation for these partial
permutations and strings before we proceed.
Notation. For any i∈[t],j∈[w], we deﬁne the tuple Q(i,j) := (j′,j,xi
j) as two indices in [ w]
followed by a value from string xisuch that σi(j′) =j. Deﬁne
Quc(i) := (Q(i,j))j∈[w]\clean(Bi) and Qc(i) := (Q(i,j))j∈clean(Bi),
as tuples of Q(i,j) for indices that are not clean, and those which are clean in Birespectively
(seeFigure 9). Letclean−1(Bi) denote the set of indices jsuch that σi(j)∈clean(Bi) fori∈[t].
There are exactly wc=w
100indices in clean−1(Bi) since we have restricted the size of each clean(Bi)
to be exactly wcin the previous section. By deﬁnition, if Biis an active block, 1 ∈clean−1(Bi).
Observe that all the edges of block Biare ﬁxed if both Qc(i) andQuc(i) are ﬁxed since they
cover the entirety of permutation σiand all the indices in xi. Our aim is to condition on speciﬁc
choices of both Qc(i),Quc(i) for all blocks Bi/∈active(G), and only Quc(i) forBi∈active(G). This
leaves only Qc(i) as random for Bi∈active(G). The following claim shows that this is possible.
Claim 4.10. For any protocol Πwhich solves DHXw,ton instance Gwith probability of success
at least1
2+1
8wconditioned on |active(G)|/greaterorequalslantlnwand|clean(Bi)|=wcfor each i∈[t], there exists
a choice of tuples from the set
/braceleftBig/parenleftbig
Quc(i),Qc(i′),Quc(i′)/parenrightbig
i∈[ta],ta<i′/lessorequalslantt/bracerightBig
such that the probability of success of ΠonGconditioned on this choice over the randomness of
Qc(i),i∈[ta]is at least1
2+1
8w.
22(a)Qc(i). The dashed (red) edges are sent to Bob
and normal (green) edges are sent to Alice.(b)Quc(i).
Figure 9: An illustration of Qc(i) andQuc(i) for an inactive block Bi=Block(x,σ) forx= (00010101) and
σ= (3,1,4,2,7,5,6,8). The gray vertices in layers V2andV3correspond to clean indices.
Proof.We know that all σiandxiare chosen uniformly and independently for i∈[t]. Moreover,
eachxi
jis chosen uniformly at random and independently for j∈[w],i∈[t]. Also, permutation
σirestricted from clean−1(Bi) toclean(Bi) is uniform over Swc(the indices may be numbered
diﬀerently, but there are wcof them) conditioned on some choice of the rest of permutatio nσito
indices [w]\clean(Bi) for each i∈[t].
Hence by the independence of these inputs, there exists a cho ice of permutations and strings
in blocks that are not active, Qc(i),Quc(i) forta< i/lessorequalslantt, and partial permutations and strings in
active blocks Quc(i) fori∈[ta] such that the probability of success of Π on Gover the randomness
ofQc(i),i∈[ta] is at least1
2+1
8w, proving the claim.
Henceforth, we assume that the tuples Quc(i),Qc(i′),Quc(i′) fori∈[ta] andta< i′/lessorequalslanttare ﬁxed
for input graph Gas the choice from Claim 4.10 . We have conditioned our input on the partition
functions F, the active blocks active(G) ={B1,B2,...,B ta}and a speciﬁc choice of the tuples
fromClaim 4.10 , resulting in the following partition of edges of Qc(i) fori∈[ta] (which are still
random).
•Alice has tastrings of length wceach,xi
jfori∈[ta],j∈clean(Bi) picked uniformly at random
and independently.
•Bob has tapermutations from Swc(though the numbering of the indices may diﬀer), σi
restricted from clean−1(Bi) toclean(Bi) fori∈[ta] chosen uniformly and independently.
In fact, these are the only edges in Gwhich are not ﬁxed by our conditioning of the input graph.
By a reordering of the indices in clean(Bi) fori∈[ta], we refer to Alice’s input as ( x1,x2,...,xta)∈
({0,1}wc)ta. Similarly by a reordering of the indices in clean−1(Bi) fori∈[ta], we can view Bob’s
input as ( σ1,σ2,...,σta)∈(Swc)ta.
23Alice and Bob are interested in
zG(1) =/circleplusdisplay
i∈[t]xi
σi(1)=
/circleplusdisplay
i∈[ta]xi
σi(1)
⊕/parenleftBigg/circleplusdisplay
ta<i/lessorequalslanttxi
σi(1)/parenrightBigg
.
The second term/circleplustext
ta<i/lessorequalslanttxi
σi(1)is ﬁxed and known to both Alice and Bob based on the choice of
these parameters from Claim 4.10 . However, by Lemma 4.6 , we know that σi(1) is uniform over
clean(Bi) for each i∈[ta]. We will prove Lemma 3.16 usingProposition 2.6 , by showing that any
protocol with o(wc·ta) communication for DHXw,tincurs only a low bias on/circleplustext
i∈[ta]xi
σi(1). We
need some more notation about protocols and their messages f or the rest of this section.
Notation. Let Π be any protocol for DHXw,twith success probability at least1
2+1
6w, and let
Π(G) denote the output of Π on input graph G. With a slight abuse of notation, we also use Π to
denote the random variable for the message πsent by the protocol. For any message π, deﬁneI(π)
as the set of inputs on which Alice sends π. We assume that all the messages sent by protocol Π
are of the same length c, by a standard padding argument.
Lemma 4.11. For any protocol ΠsolvingDHXw,toncbits with probability of success at least
1
2+1
8wover randomness of Qc(i)fori∈[ta], there exists a message πsuch that for at least 2wcta−4c
inputs Alice sends π, and Bob succeeds with probability at least1
2+1
8w−1
23con outputting the
correct answer given π.
Proof.The proof is by an averaging argument over the total number of possible inputs to Alice and
the number of possible messages. Alice has tastrings of wclength each so 2wc·tapossible inputs.
Denote by GoodMsg the set of messages πwith|I(π)|/greaterorequalslant2wcta−4c. The probability that Alice sends
a message π /∈GoodMsg can be bounded as follows.
Pr(Π =π,π /∈GoodMsg ) =/summationdisplay
π/∈GoodMsg|I(π)|
2wcta(as inputs to Alice are uniform)
<2c·2wcta−4c
2wcta= 2−3c.
The total probability of success is at most,
Pr(Π(G) =zG(1))/lessorequalslantPr(Π(G) =zG(1)|Π =π,π∈GoodMsg )+Pr(Π = π,π /∈GoodMsg )
/lessorequalslantPr(Π(G) =zG(1)|Π =π,π∈GoodMsg )+1
23c.
Conditioned on the message πhaving at least 2wcta−4cinputs mapped to it, the probability of
success is at least1
2+1
8w−1
23c. Hence there exists a message with at least 2wcta−4cinputs mapped
to it when protocol Π succeeds with probability at least1
2+1
8w−1
23c.
We are now ready to prove Lemma 3.16 . We know a lot of inputs are mapped to πfrom
Lemma 4.11 , and the input for Alice is chosen uniformly at random from st rings of length 2wcta.
The input to Bob is tapermutations of Swcchosen independently and randomly. We will prove
that only a low bias is present on zG(1) for inputs mapped to πusingProposition 2.6 .
Proof of Lemma 3.16 .Firstly,Claims 4.9 and4.10allow us to condition the input graph such that
only the parameters in Qc(i) fori∈[ta] are not ﬁxed while keeping the probability of success at
24least1
2+1
8w. FromLemma 4.11 , we know there exists a πsuch that I(π) has at least 2wcta−4c
elements on which the protocol succeeds with probability at least1
2+1
8w−1
23c. We want to prove
thatc= Ω(w·t). We assume towards a contradiction that c=1
32e·wcta.
Conditioned on Π = π, the input to Alice is uniformly distributed over the set I(π), as the
input is chosen uniformly at random and independently from {0,1}wcta. ByProposition 2.6 , the
XOR of any tasize subset Jof the indices [ wcta] picked uniformly at random of an element from
I(π), again sampled uniformly at random incurs a bias biasI(π)(J) of at most,
E
J/bracketleftBig
biasI(π)(J)2/bracketrightBig
=O/parenleftbigg1
wcta·log/parenleftbigg2wcta
|I(π)|/parenrightbigg/parenrightbiggta
. (2)
We are interested in the bias incurred when J=/braceleftbig
σ1(1),σ2(1),...,σta(1)/bracerightbig
. ByLemma 4.6 , we
know that σi(1) is uniform over the set {wc(i−1)+1,wc(i−1)+2,...,w c·i}fori∈[ta]. We call
ata-size subset Jof indices validif for all i∈[ta],
|J∩{wc(i−1)+1,wc(i−1)+2,...,w c·i}|= 1.
That is, it contains one index from each substring xifori∈[ta] of the input to Alice, which are
the only possible choices for the set/braceleftbig
σi(1)|i∈[ta]/bracerightbig
. We can write the expectation over a random
tasize subset Jof [wcta] as,
E
J/bracketleftBig
biasI(π)(J)2/bracketrightBig
/greaterorequalslantPr(Jis valid)·E
J/bracketleftBig
biasI(π)(J)2|Jis valid/bracketrightBig
= (wc)ta·1/parenleftbigwcta
ta/parenrightbig·E
J/bracketleftBig
biasI(π)(J)2|Jis valid/bracketrightBig
,
wherewehaveusedthat Jisvalidwithprobability( wc)ta//parenleftbigwcta
ta/parenrightbig
. Bob’sinputindices/braceleftbig
σi(1)|i∈[ta]/bracerightbig
are random over the set of all valid ta-size subsets. Therefore the bias over these valid subsets i s
at most,
E
J/bracketleftBig
biasI(π)(J)2|Jis valid/bracketrightBig
/lessorequalslant/parenleftbiggwcta
ta/parenrightbigg
·1
(wc)ta·O/parenleftbigg1
wcta·log/parenleftbigg2wcta
|I(π)|/parenrightbigg/parenrightbiggta
(byEq (2))
/lessorequalslanteta·O/parenleftbigg1
wcta·log/parenleftbigg2wcta
|I(π)|/parenrightbigg/parenrightbiggta
(as/parenleftbigwcta
ta/parenrightbig
/lessorequalslant/parenleftBig
ewcta
ta/parenrightBigta)
/lessorequalslantO/parenleftbigg4ec
wcta/parenrightbiggta
(as|I(π)|/greaterorequalslant2wcta−4c)
/lessorequalslantO(1/8)ta=o(1/w2).(asc= (1/32e)·wctaandta/greaterorequalslantlnw)
However, protocol Π succeeds with probability at least1
2+1
8w−1
23cwhen the transcript is π,
giving a lower bound on the bias:
Pr(Π is correct) =1
2+EJ/bracketleftbig
biasI(π)(J)|Jis valid/bracketrightbig
2/greaterorequalslant1
2+1
8w−1
23c;
E
J/bracketleftbig
biasI(π)(J)|Jis valid/bracketrightbig
/greaterorequalslant1
4w−1
23c−1/greaterorequalslant1
8w.(asc/greaterorequalslantlogw)
Comparing the two bounds on the expected bias, we get a contra diction as follows.
1
64w2/lessorequalslantE
J/bracketleftbig
biasI(π)(J)|Jis valid/bracketrightbig2/lessorequalslantE
J/bracketleftBig
biasI(π)(J)2|Jis valid/bracketrightBig
/lessorequalslanto/parenleftbigg1
w2/parenrightbigg
.
This concludes the proof.
255 Implications to Random Order Streaming Algorithms
In this section, we show some implications of Theorem 1 to proving space lower bounds for random
orderstreamingalgorithms, formalizing Result 2. Wealsoextendourlowerboundstothestochastic
streaming model deﬁned in [ CMVW16 ]. The reductions in this section are standard (or follow by
easy modiﬁcations of known ones), see, e.g. [ VY11,AKSY20 ,AN21,KMT+22], and are provided
here for completeness. The main diﬀerence with all these prio r work is that since we proved a
robustcommunication complexity lower bound for NGC, we can obtain streaming lower bounds
forrandom-order streams instead of adversarially ordered, as in all these pr ior work.
Before moving on, the following important remark is in order .
Remark 5.1. Our goal in our paper is to establish an exponential-in- ǫ-dependency on the
spacecomplexity ofrandom-orderstreamingalgorithmstha t obtain(1+ ǫ) multiplicative oradditive
approximation for various graph problems. Hence, in many of our lower bounds, we take ǫto be
sub-constant, typically ǫ= 1/Θ(logn), andthusobtain lower boundson speciﬁcpoints of thespace -
approximation tradeoﬀ curve.10For multiplicative approximation lower bounds, this is not at all
a problem as we can use a simple padding argument (by embeddin g the lower bound on a smaller
part of the graph), but this approach does not work when it com es to additive (in n) approximation
lower bounds. As a result, for additive approximation resul ts, our lower bounds inherently apply
only to certain points of the tradeoﬀ curve. We leave open the possibility of extending our results
to the entire space-approximation tradeoﬀ curve as an inter esting research question.
Finally, we note that given most algorithms for the problems we consider have some (mild)
dependence on nas well, say, polylog( n) to store counters or edges, we state our lower bounds by
even including some no(1)-dependence on the space; in other words, the exponential-d ependence
onǫin our lower bounds continues to hold even when one allows no(1)-space dependence (which is
necessary to prove near-optimality of our bounds).
5.1 Number of Connected Components
We start with one of our main results which is a lower bound for estimating the number of con-
nected components. Given a graph G= (V,E) in a random-order stream, estimate the number of
connected components of Gto within an ǫ·nadditive factor.
[PS18] gave an algorithm for this problem with space (1 /ǫ)O(1/ǫ3)·(logn) which was improved
later by [ CKKP22 ] to (1/ǫ)O(1/ǫ)·(logn) space. As stated earlier in theintroduction, thelatter wo rk
also showed that (1 /ǫ)Ω(1/ǫ)space is needed for the component ﬁndingproblem, namely, ﬁnding a
component of size Θ(1 /ǫ) in a graph that contains Θ( ǫ·n) of them (while also requiring the stream
to be almost-random, or rather in the batch random-order mod el they introduced; see [ CKKP22 ]).
Weproveanear-optimal lowerboundfortheoriginalestimat ion problemandon(truly)random-
order streams, settling a conjecture of [ PS18] in the necessity of exponential dependence on (1 /ǫ)
for any algorithm for this problem.
Corollary 5.2. Any single-pass streaming algorithm that for every given ǫ >0processes edges of
any graph Gwithnvertices in a random-order stream in 2o(1/ǫ)·no(1)space cannot estimate the
number of connected components in Gto within an ǫ·nadditive factor with probability at least 2/3.
Proof.Consider the NGCn,kproblem for k/greaterorequalslant1600lnn. In the two possible cases of NGCn,k, the
10This means our lower bounds may leave out the possibility of h aving an algorithm that for constant/small values
ofǫhave a better space-dependence on ǫ, but once ǫgets smaller, their space-dependence “switches” to expone ntial.
Whether such algorithms can exist for these problems howeve r seems quite unlikely to the authors.
26number of connected components of Gdiﬀers by n/4k: the number of noisy paths is the same in
both cases of NGCand thek-cycle case has n/4kmore cycles/connected components compared to
the 2k-cycle case. Thus, if we take ǫ <1/8k, then estimating the number of connected components
ofGinNGCn,kup to an ǫ·nadditive factor would also solve this problem.
Combining this with Proposition 2.5 and our robust communication lower bound of Ω( n) for
NGCn,kinTheorem 1 , we get that any random order streaming algorithm for estima ting the
number of connected components up to ǫ·nadditive factor for ǫ <1/8krequires Ω( n) space. Given
the choice of ǫ= 1/Θ(logn), a space of 2o(1/ǫ)·no(1)for the algorithm translates to o(n) space,
which contradicts the above lower bound. This concludes the proof.
5.2 Minimum Spanning Tree
In this section, we prove a lower bound for random-order stre aming algorithms which estimate the
size of the minimum spanning tree up to an (1+ ǫ) multiplicative approximation factor. Given a
connected weighted graph G= (V,E) with integral weights from the set [ W], estimate the weight
of the minimum spanning tree of Gto within an (1+ ǫ) multiplicative approximation factor.
[PS18] gave an algorithm for this problem which uses O/parenleftbig1
ǫ/parenrightbig/tildewideO(W3/ǫ3)space. We give the ﬁrst
non-trivial lower bound for this problem.
Corollary 5.3. Any single-pass streaming algorithm that for every given ǫ >0processes edges of
any graph Gwithnvertices and integer weights in [W]in a random-order stream in 2o(W/ǫ)·no(1)
space cannot estimate the weight of minimum spanning trees t o within an (1 +ǫ)multiplicative
approximation factor with probability at least 2/3.
Proof.Ourhardinstances for NGCn,karesampledfromdistribution µngc. Recall that µngcoutputs
validinstancesof NGCn,kbyaddingauxiliaryedgestomulti-block graphssuchthatth erearecycles
of length 2 kwhenθ= 1 and cycles of length kotherwise (see Figure 5).
LetGbe a graph sampled from µngcfork/lessorequalslantW−1
12ǫ. The weights of all the initial edges in Gare
set as 1. We add the following additional edges so that there i s a large diﬀerence in the weight of
the minimum spanning tree when θ= 1 and when θ= 0.
•Add edge ( ak
j,bk
j) of weight Wfor each j∈[m].
•Add edges ( a1
j,a1
m+j),(a1
j,b1
m+j) of weight 1 for each j∈[m].
•Add edges ( a1
j,b1
j+1) for each j∈[m−1], and edge ( a1
m,b1
1) of weight 1.
LetG′be used to denote Gafter these edges are added.
Whenθ= 1, that is the instance Gconsists of cycles of length 2 k, adding these additional
edges makes G′connected through edges of weight 1. There are n/4kcycles of length 2 k, and
all the edges in these cycles have weight 1. Each path of lengt hk−1 from vertex a1
m+jorb1
m+j
is connected to a cycle of length 2 kcontaining vertex a1
jfor each j∈[m] by edges of weight 1.
The cycles are all connected to each other through edges ( a1
1,b1
2),(a1
2,b1
3),...,(a1
m−1,b1
m),(a1
m,b1
1)
of weight 1. Hence, the weight of the minimum spanning tree is n−1.
Whenθ= 0 and the instance Gconsists of cycles of length k, the weight of any spanning tree of
G′is at least n−m+W(m−1). If we consider only edges of weight 1, there are at least mconnected
components since, for each j∈[m−1], the component with the vertex a1
jonly contains 4 k=n/m
vertices. It is only connected to the cycle of length kpassing through ak
jwithkvertices, two paths
27of length kbeginning at a1
m+jandb1
m+jrespectively of kvertices each, and the cycle containing
b1
j+1withkvertices. Similarly, the component containing a1
mcontains only n/mvertices. At least
m−1 edges of weight Wconnecting ak
jtobk
jforj∈[m] are added to any spanning tree of G′.
From our choice of k, we know that (1+ ǫ)(n−1)<(1−ǫ)(n−W+m(W−1)). Estimating the
weight of a minimum spanning tree of G′to within an (1+ ǫ) multiplicative approximation would
also solve NGCn,kover the hard distribution µngc. Thus by Proposition 2.5 andTheorem 1 , we
know that the space required by such an algorithm is Ω( n) bits. For k= Θ(logn) andǫ= Θ(W/k),
the algorithm requires Ω( n) = 2Ω(W/ǫ)space.
5.3 Maximum Matching and Matrix Rank
We next prove a lower bound for random-order streaming algor ithms for estimating the size of the
maximum matching even in bounded-degree graphs (and planar ones). Given a graph G= (V,E)
in a random-order stream with the promise that maximum degre e ofGis bounded by some given
d=O(1), estimate the size of a maximum matching in Gto within an ǫ·nadditive factor.
Maximum matchings have been studied extensively in the stre aming model and listing the prior
results on this problem is beyond the scope of our work. We onl y note that [ KKS14,KMNT20 ] gave
polylog(n)-space streaming algorithms for polylog( n) (multiplicative) approximation of matching
size on arbitrary graphs in random-order streams, and [ EHL+15,MV16,CJMM17 ,MV18] gave
polylog(n)-space streaming algorithms for O(1) (multiplicative) approximation of matching size on
planar graphs in adversarial-order streams. Most related t o us however is the work of [ MMPS17 ]
who gave an algorithm for estimating the size of maximum matc hing to within an ǫ·nadditive
factor on bounded-degree graphs (the problem we started wit h above) in random-order streams
using 22poly(1/ǫ)·(logn) space, i.e., with doubly-exponential dependence on ǫ.
We prove theﬁrstlower boundforthis problemshowingthat at least an exponential dependence
onǫis necessary even on an extremely simple family of graphs.
Corollary 5.4. Any single-pass streaming algorithm that for every given ǫ >0processes edges of
any graph Gwithnvertices in a random-order stream in 2o(1/ǫ)·no(1)space cannot estimate the
size of maximum matching in Gto within an ǫ·nadditive factor with probability at least 2/3.
The lower bound holds even for planar graphs of maximum degree two, namely, on disjoint-
unions of cycles and paths.
Proof.Firstly, it is easy to see that any instance of NGCn,kis a planar graph with degree two for
anyn,ksince it is just a collection of cycles and paths.
For anyoddk, in any instance of NGCn,k, each path of length k−1 has a matching of ( k−1)/2
size. If theinstance contains cycles of length 2 k, each such cycle has a matching with kedges, giving
us a maximum matching of size ( n/4k)·k+(n/2k)·(k−1)/2. If the instance instead has cycles of
lengthk, then each such cycle has a matching of ( k−1)/2 edges, and the maximum matching size
is (n/2k)·(k−1)/2+(n/2k)·(k−1)/2. There is a diﬀerence of ( n/4k) in the maximum matching
size in either case of NGCn,k.
The rest of the lower bound follows exactly as in Corollary 5.2 by combining Proposition 2.5
with our Theorem 1 .
Remark 5.5. For(1 +ǫ)-multiplicative approximation instead, we can prove a lowe r bound of
2Ω(1/ǫ)on space even for constant choices of ǫ >0, by simply using NGC2Θ(1/ǫ),Θ(1/ǫ)and pad the
remaining graph with singleton vertices.
28As a corollary of the standard equivalence between estimati ng matching size and computing the
rank of the Tutte matrix [ Tut47] with random entries established by [ Lov79] (see [BS15] streaming
implementation of the reduction), we get the following resu lt as well.
Corollary 5.6. Any single-pass streaming algorithm that for every given ǫ >0processes entries of
anyn-times-nmatrixAin a random-order stream in 2o(1/ǫ)·no(1)space cannot estimate the rank
ofAto within an ǫ·nadditive factor with probability at least 2/3.
The lower bound holds even for sparse matrices with two non-ze ro entries per row and column.
5.4 Planar Maximum Independent Set
We can also prove a similar lower bound for the problem of esti mating the maximum independent
set size on planar graphs. Given a planar graph G= (V,E) in a random-order stream, estimate
the size of the largest independent set in Gto within an additive ǫ·nfactor.
[PS18] gave an algorithm for this problem that outputs a (1+ ǫ) multiplicative approximation
using 222logO(1)(1/ǫ)
·(logn) space. We prove the ﬁrst lower bound for this problem showin g that at
least an exponential dependence on ǫis necessary even on an extremely simple family of graphs.
Corollary 5.7. Any single-pass streaming algorithm that for every given ǫ >0processes edges of
any graph Gwithnvertices in a random-order stream in 2o(1/ǫ)·no(1)space cannot estimate the
size of maximum independent set in Gto within an ǫ·nadditive factor with probability at least 2/3.
The lower bound holds even for planar graphs of maximum degree two, namely, on disjoint-
unions of cycles and paths.
Proof.For any odd k, in an instance of NGCn,k, each path of length k−1 has an independent set
of size at most ( k+1)/2 vertices. If the instance contains cycles of length 2 k, each such cycle has an
independent set with kvertices. The size of the maximum independent set is ( n/4k)·k+(n/2k)·
((k+1)/2). If the instance instead has cycles of length k, then each such cycle has an independent
set of (k−1)/2 vertices, and the maximum independent set is of size ( n/2k)·((k+1)/2)+(n/2k)·
(k−1)/2. There is a diﬀerence of ( n/4k) in either case of NGCn,k.
The rest of the lower bound follows exactly as in Corollary 5.2 by combining Proposition 2.5
with our Theorem 1 .
We note that Remark 5.5 applies to this problem as well.
5.5 Random Walk Generation
Next, we show that we also get an exponential lower bound for r andom walk generation in random
order streams. To deﬁne this problem, we need some further no tation.
Deﬁnition 5.8 (Pointwise ǫ-closeness of distributions) .We say that a distribution pover support
Ωisǫ-close to distribution qover the same support Ω, if for each s∈Ω,p(s)∈[1−ǫ,1+ǫ]·q(s).
Deﬁnition 5.9 (ǫ-approximate sample) .Given a graph G= (V,E)and a vertex u∈V, we say
that(X0,X1,...,X ℓ)is anǫ-approximate sample of the ℓstep random walk starting at uif the
distribution of (X0,...,X ℓ)isǫ-close pointwise (from Deﬁnition 5.8 ) to the distribution of the
ℓ-step random walk starting at vertex u.
Problem 3 ((ℓ,b,ǫ,δ)-Random Walk ).Given a graph G= (V,E)in a random order stream,
a length ℓ, a budget b, and error parameters ǫ,δ, generate bindependent ǫ-approximate samples of
29theℓ-step random walk started at a vertex picked uniformly at ran dom from Vwith error bounded
byδin total variation distance.
[KKP22] gives an upper bound of (1 /ǫ)O(ℓ)·2O(ℓ2)·bspace for this problem for δ= 1/10. For
constant ǫ=δ= 1/10, [CKKP22 ] gives a lower bound of ℓΩ(√
ℓ)whenb= 1, and a lower bound of
ℓΩ(ℓ)whenb= Ω(4ℓ) in the batch random order model. We give exponential in ℓlower bounds in
the random stream model for generating random walks.
Corollary 5.10. Any algorithm for generating (ℓ,c1,1/10,1/10)-random walks requires 2Ω(√
ℓ)
space and generating (ℓ,c2·4ℓ,1/10,1/10)-random walks requires 2Ω(ℓ)space for suﬃciently large
constants c1,c2, andℓ= Θ(logn).
To prove this result, we need the following classical result on the cover time of random walks.
Fact 5.11 (cf. [MR95, Chapter 6]) .Any random walk in a connected graph G= (V,E)visits all
the vertices in Gin expected time O(|V|·|E|).
Proof of Corollary 5.10 .For the ﬁrst lower bound, given an instance GofNGCn,k, we pick ℓ=
c3k2. We generate c1independent 1 /10-approximate random walks of length ℓwith 1/10 error in
total variation distance. We know that if c1vertices are picked uniformly at random, at least one
vertex is a part of a cycle in Gwith probability at least 1 −1
2c1. This is because only half of the
vertices in Gare present in vertex disjoint paths of length k−1.
For suﬃciently large constant c3, any random walk beginning at v∈Vof length at least c3k2
visits all the vertices in the connected component of Gcontaining vbyFact 5.11 with constant
probability. Since we generate 1 /10-approximate random walks, the probability that any such walk
visits all the vertices reduces by at most a 9 /10 multiplicative factor. The 1 /10 error in total
variation distance reduces this probability by at most a 1 /10 additive factor.
Ifvis a part of a cycle in G, Alice and Bob can use an algorithm for generating random
walks to ﬁnd if vis part of a k-cycle or a 2 k-length cycle with probability at least 2/3. As
k=/radicalbig
ℓ/c3= Θ(lnn), we require Ω( n) = 2Ω(√
ℓ)space to do so by Theorem 1 .
For the next lower bound, we pick ℓ= 2kand generate c2·4ℓ1/10-approximate random walks
with error at most 1 /10. As we are ﬁnding c24ℓindependent 1 /10-approximate random walks, with
high probability, at least a constant fraction of these rand om walks begin with cycles in G.
The probability that a truly random walk of length at least 2 kin a cycle with at most 2 k
vertices visits all the vertices in the cycle is at least 2−2k(the random walk has to always pick the
unvisited edge which happens with probability 1 /2 for 2ksteps). For a 1 /10-approximate random
walk, this happens with probability at least 9 /10·2−2k= 9/10·2−ℓ.
For a large constant c3, at least one of the 1 /10-approximate random walks visits all the vertices
in the cycle with constant probability as the error due to the total variation distance is at most
1/10. A cycle of Gis found with probability at least 2/3 and Alice and Bob can us e this algorithm
to ﬁnd whether Ghas cycles of length kor 2k. Forℓ= 2k= Θ(lnn), we know that we require
Ω(n) = 2Ω(ℓ)space, again by Theorem 1 .
5.6 Lower Bounds for Stochastic Streams
In the stochastic stream model, the algorithm receives c·medges picked uniformly at random and
independently from the graph for some c >0, where mdenotes the number of edges in G. The
goal as before is to estimate a certain parameter of the under lying graph with minimal space, and
30now additionally, minimal number of samples also, i.e., by m inimizing the parameter c. To our
knowledge, this model was ﬁrst studied by [ CMVW16 ].
We show that our lower bounds continue to hold on stochastic s treams of c·medges for any
constant c >0. We will begin by deﬁning the SGCn,kcommunication problem, an adaptation of
NGCn,kfrom random streams for a stochastic stream of cmedges.
Problem 4 (Stochastic Noisy Gap Cycle Counting (SGC) ).For any integers n,k/greaterorequalslant1, In
SGCn,k, we have a graph G= (V,E)onnvertices such that Geither contains: (i) (n/4k)vertex-
disjoint cycles of length 2k, or(ii) (n/2k)vertex-disjoint cycles of length k. In addition, in both
cases,Gcontains (n/2k)vertex-disjoint paths of length k−1.
We pick c· |E|/2edges uniformly and independently from E(with repetition) and add them
toEA, and similarly c· |E|/2edges are picked at random and added to EB(here,c, is the same
constant as the one deﬁned above for stochastic streams). Al ice is given EAand Bob is given EB.
Their goal is to decide which case the graph belongs to by Alice sending a single message to Bob.
Lower bounds on the communication of SGCn,ktranslate into lower bounds for stochastic
streams, as follows.
Proposition 5.12. Any single-pass streaming algorithm that given a graph G= (V,E)fromGn
withn/2kpaths of length k−1, and either
(i)n/2kcycles of length k, or,
(ii)n/4kcycles of length 2k,
in a stochastic stream with c·|E|edges, computes which case the graph belongs to w.p. at least 1−δ
requires at least as much space as the communication complex ity ofSGCn,kwith error at most δ.
Proof.Alice and Bob can use the streaming algorithm to solve SGCn,k: Alice randomly permutes
the edges of GA, runs the algorithm over them, and sends the memory content a s the message π
to Bob; Bob takes a random permutation of GB, continues running the algorithm, and outputs the
answer. This simulates a stochastic stream of c|E|samples as each edge is sampled independently
and given to Alice and Bob. The algorithm solves SGCn,kwith error at most δ.
By building on the approach in Theorem 1 , we further show that SGCn,krequires Ω( n) bits of
communication using the same hard distribution µngcfor the underlying graphs.
Lemma 5.13. For suﬃciently large n/greaterorequalslant1and constant c=O(1)withk/greaterorequalslant24·e9c·lnn, any
one-way communication protocol for SGCn,kwith probability of success at least 2/3requiresΩ(n)
communication.
The proof of Lemma 5.13 can be found in Appendix A .
All our lower bounds in for random order streams ( Corollaries 5.2 to5.4,5.6,5.7and5.10)
apply for stochastic streams as well, as a direct consequenc e ofLemma 5.13 andProposition 5.12 .
In particular,
Corollary 5.14. Letc >0be a ﬁxed constant. Any streaming algorithm that for every gi venǫ >0
and graph G= (V,E)processes c·|E|uniform samples of edges from Ein a stochastic stream and
uses2o(1/ǫ)·no(1)space cannot solve any of the following problems with probab ility of success at
least2/3:
31(i)estimating the number of connected components in Gto within an ǫ·nadditive factor;
(ii)estimating the weight of minimum spanning trees to within an (1+ǫ)multiplicative approxi-
mation factor (even when the weights are integers and consta nt);
(iii)estimating the size of maximum matching in Gto within an ǫ·nadditive factor (even on
bounded-degree planar G);
(iv)estimating the size of maximum independent set in Gto within an ǫ·nadditive factor (even
on bounded-degree planar G).
This concludes our list of implications of Theorem 1 to random order (and stochastic) streams.
Acknowledgement
We thank Michael Kapralov and Huacheng Yu for helpful discus sions.
References
[ACL+22] Sepehr Assadi, Vaggos Chatziafratis, Jakub Lacki, Vaha b Mirrokni, and Chen Wang.
Hierarchical clustering in graph streams: Single-pass alg orithms and space lower
bounds. InPo-LingLohandMaximRaginsky, editors, Conference on Learning Theory,
2-5 July 2022, London, UK , volume 178 of Proceedings of Machine Learning Research ,
pages 4643–4702. PMLR, 2022. 5
[AKSY20] Sepehr Assadi, Gillat Kol, Raghuvansh R. Saxena, a nd Huacheng Yu. Multi-pass
graph streaming lower bounds for cycle counting, max-cut, m atching size, and other
problems. In Sandy Irani, editor, 61st IEEE Annual Symposium on Foundations of
Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2 020, pages
354–364. IEEE, 2020. 1,2,3,4,5,17,26
[AMS96] Noga Alon, Yossi Matias, and Mario Szegedy. The spac e complexity of approximating
the frequency moments. In Gary L. Miller, editor, Proceedings of the Twenty-Eighth
Annual ACM Symposium on the Theory of Computing, Philadelphia, Pennsylvania,
USA, May 22-24, 1996 , pages 20–29. ACM, 1996. 1
[AN21] Sepehr Assadi and Vishvajeet N. Graph streaming lowe r bounds for parameter es-
timation and property testing via a streaming XOR lemma. In S amir Khuller and
Virginia Vassilevska Williams, editors, STOC ’21: 53rd Annual ACM SIGACT Sympo-
sium on Theory of Computing, Virtual Event, Italy, June 21-25, 2 021, pages 612–625.
ACM, 2021. 1,2,3,4,5,26
[BCK+18] Vladimir Braverman, Stephen R. Chestnut, Robert Krauth gamer, Yi Li, David P.
Woodruﬀ, and LinF. Yang. Matrix normsindata streams: Faster , multi-pass and row-
order. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th Interna-
tional Conference on Machine Learning, ICML 2018, Stockholmsm ¨ assan, Stockholm,
Sweden, July 10-15, 2018 , volume 80 of Proceedings of Machine Learning Research ,
pages 648–657. PMLR, 2018. 2
32[BJK04] Ziv Bar-Yossef, T. S. Jayram, and Iordanis Kerenidi s. Exponential separation of quan-
tum and classical one-way communication complexity. In L´ a szl´ o Babai, editor, Pro-
ceedings of the 36th Annual ACM Symposium on Theory of Computing , Chicago, IL,
USA, June 13-16, 2004 , pages 128–137. ACM, 2004. 2
[BS15] Marc Bury and Chris Schwiegelshohn. Sublinear estim ation of weighted matchings
in dynamic data streams. In Nikhil Bansal and Irene Finocchi , editors, Algorithms
- ESA 2015 - 23rd Annual European Symposium, Patras, Greece, September 14-16,
2015, Proceedings , volume 9294 of Lecture Notes in Computer Science , pages 263–274.
Springer, 2015. 2,29
[CCM08] Amit Chakrabarti, Graham Cormode, and Andrew McGre gor. Robust lower bounds
for communication and stream computation. In Proceedings of the 40th Annual ACM
Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20,
2008, pages 641–650, 2008. 3,6,7
[CFPS20] Artur Czumaj, Hendrik Fichtenberger, Pan Peng, an d Christian Sohler. Testable prop-
erties in general graphs and random order streaming. In Jaro slaw Byrka and Raghu
Meka, editors, Approximation, Randomization, and Combinatorial Optimiza tion. Algo-
rithms and Techniques, APPROX/RANDOM 2020, August 17-19, 2 020, Virtual Con-
ference, volume 176 of LIPIcs, pages 16:1–16:20. Schloss Dagstuhl - Leibniz-Zentrum
f¨ ur Informatik, 2020. 1
[CJMM17] Graham Cormode, Hossein Jowhari, Morteza Monemiz adeh, and S. Muthukrishnan.
The sparse awakens: Streaming algorithms for matching size estimation in sparse
graphs. InKirkPruhsandChristianSohler, editors, 25th Annual European Symposium
on Algorithms, ESA 2017, September 4-6, 2017, Vienna, Austr ia, volume 87 of LIPIcs,
pages 29:1–29:15. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2017. 28
[CKKP22] Ashish Chiplunkar, John Kallaugher, Michael Kapr alov, and Eric Price. Factorial
lower bounds for (almost) random order streams. In 63rd IEEE Annual Symposium
on Foundations of Computer Science, FOCS 2022 . IEEE, 2022. 1,2,4,26,30
[CKP+23] Lijie Chen, Gillat Kol, Dmitry Paramonov, Raghuvansh Sa xena, Zhao Song, and
Huacheng Yu. Towards multi-pass streaming lower bounds for optimal approxima-
tion of max-cut. In Proceedings of the Thirty-Fourth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2023 . SIAM, 2023. 2
[CMVW16] Michael S. Crouch, Andrew McGregor, Gregory Valia nt, and David P. Woodruﬀ.
Stochastic streams: Sample complexity vs. space complexit y. In Piotr Sankowski and
Christos D. Zaroliagis, editors, 24th Annual European Symposium on Algorithms, ESA
2016, August 22-24, 2016, Aarhus, Denmark , volume 57 of LIPIcs, pages 32:1–32:15.
Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2016. 4,26,31
[CR11] Amit Chakrabarti and Oded Regev. An optimal lower bou nd on the communication
complexity of gap-hamming-distance. In Lance Fortnow and S alil P. Vadhan, editors,
Proceedings of the 43rd ACM Symposium on Theory of Computing, STO C 2011, San
Jose, CA, USA, 6-8 June 2011 , pages 51–60. ACM, 2011. 4
[CRT01] Bernard Chazelle, Ronitt Rubinfeld, and Luca Trevi san. Approximating the minimum
spanning tree weight in sublinear time. In Fernando Orejas, Paul G. Spirakis, and
33Jan van Leeuwen, editors, Automata, Languages and Programming, 28th International
Colloquium, ICALP 2001, Crete, Greece, July 8-12, 2001, Proceedi ngs, volume 2076
ofLecture Notes in Computer Science , pages 190–200. Springer, 2001. 1
[DP09] Devdatt P. Dubhashi and Alessandro Panconesi. Concentration of Measure for the
Analysis of Randomized Algorithms . Cambridge University Press, 2009. 6
[EHL+15] Hossein Esfandiari, Mohammad Taghi Hajiaghayi, Vahid L iaghat, Morteza Monem-
izadeh, and Krzysztof Onak. Streaming algorithms for estim ating the matching size
in planar graphs and beyond. In Piotr Indyk, editor, Proceedings of the Twenty-Sixth
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA,
USA, January 4-6, 2015 , pages 1217–1233. SIAM, 2015. 2,28
[FKM+05] Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Sidd harth Suri, and Jian
Zhang. On graph problems in a semi-streaming model. Theor. Comput. Sci. , 348(2-
3):207–216, 2005. 1
[FKM+08] Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Sidd harth Suri, and Jian
Zhang. Graph distances in the data-stream model. SIAM J. Comput. , 38(5):1709–
1727, 2008. 1
[GKK+07] Dmitry Gavinsky, Julia Kempe, Iordanis Kerenidis, Ran R az, and Ronald de Wolf.
Exponential separations for one-way quantum communicatio n complexity, with appli-
cations to cryptography. In David S. Johnson and Uriel Feige , editors, Proceedings of
the 39th Annual ACM Symposium on Theory of Computing, San Diego, California,
USA, June 11-13, 2007 , pages 516–525. ACM, 2007. 2,5,7
[GT19] Venkatesan Guruswami and Runzhou Tao. Streaming har dness of unique games. In
Dimitris Achlioptas and L´ aszl´ o A. V´ egh, editors, Approximation, Randomization, and
Combinatorial Optimization. Algorithms and Techniques, AP PROX/RANDOM 2019,
September 20-22, 2019, Massachusetts Institute of Technolo gy, Cambridge, MA, USA ,
volume 145 of LIPIcs, pages 5:1–5:12. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur I nfor-
matik, 2019. 17
[GVV17] Venkatesan Guruswami, Ameya Velingker, and Santho shini Velusamy. Streaming com-
plexity of approximating max 2csp and max acyclic subgraph. In Klaus Jansen, Jos´ e
D. P. Rolim, David Williamson, and Santosh S. Vempala, edito rs,Approximation,
Randomization, and Combinatorial Optimization. Algorithm s and Techniques, AP-
PROX/RANDOM2017, August16-18, 2017, Berkeley, CA,USA ,volume81of LIPIcs,
pages 8:1–8:19. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur In formatik, 2017. 2
[HP16] Zengfeng Huang and Pan Peng. Dynamic graph stream alg orithms in o(n) space. In
Ioannis Chatzigiannakis, Michael Mitzenmacher, Yuval Rab ani, and Davide Sangiorgi,
editors,43rd International Colloquium on Automata, Languages, and Pr ogramming,
ICALP 2016, July 11-15, 2016, Rome, Italy , volume 55 of LIPIcs, pages 18:1–18:16.
Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2016. 2
[IW03] Piotr Indyk and David P. Woodruﬀ. Tight lower bounds fo r the distinct elements
problem. In 44th Symposium on Foundations of Computer Science (FOCS 2003) , 11-
14 October 2003, Cambridge, MA, USA, Proceedings , pages 283–288. IEEE Computer
Society, 2003. 4
34[KK15] Dmitry KoganandRobertKrauthgamer. Sketchingcuts ingraphsandhypergraphs. In
Tim Roughgarden, editor, Proceedings of the 2015 Conference on Innovations in The-
oretical Computer Science, ITCS 2015, Rehovot, Israel, January 1 1-13, 2015 , pages
367–376. ACM, 2015. 2
[KKL88] JeﬀKahn,GilKalai, andNathanLinial. Theinﬂuence ofvariablesonbooleanfunctions
(extended abstract). In 29th Annual Symposium on Foundations of Computer Science,
White Plains, New York, USA, 24-26 October 1988 , pages 68–80. IEEE Computer
Society, 1988. 5,7
[KKP22] John Kallaugher, Michael Kapralov, and Eric Price. Simulating random walks in
random streams. In Joseph (Seﬃ) Naor and Niv Buchbinder, edi tors,Proceedings
of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 20 22, Virtual
Conference / Alexandria, VA, USA, January 9 - 12, 2022 , pages 3091–3126. SIAM,
2022.1,4,30
[KKS14] Michael Kapralov, Sanjeev Khanna, and Madhu Sudan. Approximating matching
size from random streams. In Chandra Chekuri, editor, Proceedings of the Twenty-
Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland,
Oregon, USA, January 5-7, 2014 , pages 734–751. SIAM, 2014. 1,28
[KKS15] Michael Kapralov, Sanjeev Khanna, and Madhu Sudan. Streaming lower bounds for
approximating MAX-CUT. In Piotr Indyk, editor, Proceedings of the Twenty-Sixth
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2015, San Diego, CA,
USA, January 4-6, 2015 , pages 1263–1282. SIAM, 2015. 1,2,3,7
[KMNT20] Michael Kapralov, Slobodan Mitrovic, Ashkan Noro uzi-Fard, and Jakab Tardos. Space
eﬃcient approximation to maximum matching size from unifor m edge samples. In
Shuchi Chawla, editor, Proceedings of the 2020 ACM-SIAM Symposium on Discrete
Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 20 20, pages 1753–
1772. SIAM, 2020. 1,28
[KMT+22] Michael Kapralov, Amulya Musipatla, Jakab Tardos, Davi d P. Woodruﬀ, and Samson
Zhou. Noisy boolean hidden matching with applications. In M ark Braverman, editor,
13th Innovations in Theoretical Computer Science Conference, ITCS 2022, January
31 - February 3, 2022, Berkeley, CA, USA , volume 215 of LIPIcs, pages 91:1–91:19.
Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2022. 2,5,26
[Lov79] L´ aszl´ o Lov´ asz. On determinants, matchings, and random algorithms. In Lothar Bu-
dach, editor, Fundamentals of Computation Theory, FCT 1979, Proceedings of t he
Conference on Algebraic, Arthmetic, and Categorial Methods i n Computation Theory,
Berlin/Wendisch-Rietz, Germany, September 17-21, 1979 , pages 565–574. Akademie-
Verlag, Berlin, 1979. 29
[LW16] Yi Li and David P. Woodruﬀ. On approximating functions of the singular values in a
stream. In Daniel Wichs and Yishay Mansour, editors, Proceedings of the 48th Annual
ACM SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, M A,
USA, June 18-21, 2016 , pages 726–739. ACM, 2016. 2
[McG14] Andrew McGregor. Graph stream algorithms: a survey .SIGMOD Rec. , 43(1):9–20,
2014.1
35[MMPS17] Morteza Monemizadeh, S. Muthukrishnan, Pan Peng, and Christian Sohler. Testable
bounded degree graph properties are random order streamabl e. In Ioannis Chatzi-
giannakis, Piotr Indyk, Fabian Kuhn, and Anca Muscholl, edi tors,44th International
Colloquium on Automata, Languages, and Programming, ICALP 2017 , July 10-14,
2017, Warsaw, Poland , volume 80 of LIPIcs, pages 131:1–131:14. Schloss Dagstuhl -
Leibniz-Zentrum f¨ ur Informatik, 2017. 1,4,28
[MR95] Rajeev Motwani and Prabhakar Raghavan. Randomized Algorithms . Cambridge Uni-
versity Press, 1995. 30
[Mut05] S. Muthukrishnan. Data streams: Algorithms and app lications. Found. Trends Theor.
Comput. Sci. , 1(2), 2005. 1
[MV16] Andrew McGregor and Sofya Vorotnikova. Planar match ing in streams revisited. In
Klaus Jansen, Claire Mathieu, Jos´ e D. P. Rolim, and Chris Um ans, editors, Approxi-
mation, Randomization, and Combinatorial Optimization. Al gorithms and Techniques,
APPROX/RANDOM 2016, September 7-9, 2016, Paris, France , volume 60 of LIPIcs,
pages 17:1–17:12. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur Informatik, 2016. 28
[MV18] Andrew McGregor and Sofya Vorotnikova. A simple, spa ce-eﬃcient, streaming algo-
rithm for matchings in low arboricity graphs. In Raimund Sei del, editor, 1st Sympo-
sium on Simplicity in Algorithms, SOSA 2018, January 7-10, 2 018, New Orleans, LA,
USA, volume 61 of OASIcs, pages 14:1–14:4. Schloss Dagstuhl - Leibniz-Zentrum f¨ ur
Informatik, 2018. 28
[PS18] Pan Peng and Christian Sohler. Estimating graph para meters from random order
streams. In Artur Czumaj, editor, Proceedings of the Twenty-Ninth Annual ACM-
SIAM Symposium on Discrete Algorithms, SODA 2018, New Orlean s, LA, USA, Jan-
uary 7-10, 2018 , pages 2449–2466. SIAM, 2018. 1,2,4,26,27,29
[SSSV23] Raghuvansh R. Saxena, Noah Singer, Madhu Sudan, an d Santhoshini Velusamy.
Streaming complexity of csps with randomly ordered constra ints. InProceedings of the
Thirty-Fourth Annual ACM-SIAM Symposium on Discrete Algorith ms, SODA 2023 .
SIAM, 2023. 1,3
[Tut47] William T Tutte. The factorization of linear graphs .Journal of the London Mathe-
matical Society , 1(2):107–111, 1947. 29
[VY11] Elad Verbin and Wei Yu. The streaming complexity of cy cle counting, sorting by
reversals, and other problems. In Dana Randall, editor, Proceedings of the Twenty-
Second Annual ACM-SIAM Symposium on Discrete Algorithms, SOD A 2011, San
Francisco, California, USA, January 23-25, 2011 , pages 11–25. SIAM, 2011. 2,4,5,
7,26
[Wol08] Ronald Wolf. A brief introduction to fourier analys is on the boolean cube. Theory of
Computing, Graduate Surveys , 1:1–20, 01 2008. 7
[Yao77] Andrew Chi-Chin Yao. Probabilistic computations: Toward a uniﬁed measure of com-
plexity. In 18th Annual Symposium on Foundations of Computer Science (sf cs 1977),
pages 222–227, 1977. 6
36A Noisy Gap Cycle Counting in Stochastic Streams
This section is dedicated to proving Lemma 5.13 . First, let us deﬁne the SHXw,tproblem, similar
toDHXw,texcept for how the edges in EA,EBare distributed for a stochastic stream of cmedges.
Problem 5 (Stochastic Hidden-XOR Problem (SHX)).InSHXw,t, we have a graph G=
Multi-Block (X,Σ)∈ Lw,3t+1forX∈({0,1}w)tandΣ∈(Sw)tchosen independently and uniformly
at random. The goal is to output zG(1)in this graph.
We pick c· |E|/2edges uniformly and independently from E(with repetition) and add them
toEA, and similarly c· |E|/2edges are picked at random and added to EB(here,c, is the same
constant as the one deﬁned above for stochastic streams). Al ice is given EAand Bob is given EB.
Their goal is to decide which case the graph belongs to by Alice sending a single message to Bob.
We can prove that SHXw,treduces to SGCn,kin stochastic streams as well.
Lemma A.1. For any suﬃciently large n/greaterorequalslant1andk= 3t+1for some t/greaterorequalslant1andw= (n/4k)+1,
the distributional communication complexity of SHXw,tfor probability of success at least1
2+1
6wis
at most as much as the determinisitic communication complex ity ofSGCn,kover distribution µngc
for probability of success at least 2/3.
Proof.Alice and Bob use the ﬁrst ( vi) steps of Algorithm 1 to get an instance of SGCn,kgiven
an instance G= (V,E) ofSHXw,t.Lemmas 3.13 and3.14still hold for SGCn,kandSHXw,tas
well because they are only about distribution Hand the sampling process in Algorithm 1 . Given a
streaming algorithm for SGCn,k, Alice and Bob have an instance G′= (V′,E′) ofSGCn,kwhere
the edges added to G′which are not in Gare public. c|E|/2 edges in Gare given privately to
both Alice and Bob, with each picked independently and rando mly. They can each generate a
stochastic stream of c|E′|/2 edges in G′with public randomness and the edges given to them from
Gwithout any additional communication. By Claim 3.15 , we know that they can output zG(1)
with probability at least1
2+1
6w.
If we prove a lower bound on the distributional communicatio n complexity of SHXw,tagainst
probability of success1
2+1
6wof Ω(n), we are done. We will show that all of the results in Section 4
can be applied to stochastic streams as well.
Wewillstartbyupdatingourdeﬁnitionsofcleanindicesand activeblocksforstochasticstreams.
Wewillthenshowthat thesecleanindicesandactive blocks a ppearwithhighprobabilityin SHXw,t
problem.
Deﬁnition A.2. We say that an index j∈[w]in some block B=Block(x,σ)iscleaniﬀ the
following holds for vertices of groups g2
j= (a2
j,b2
j)andg3
j= (a3
j,b3
j)in layers V2andV3ofB.
•Edges(a1
σ−1(j),a2
j),(b1
σ−1(j),b2
j),(a3
j,a4
σ−1(j)),(b3
j,b4
σ−1(j))do not belong to EA, and belong to
EB.
•Edges from group g2
jto group g3
jdo not belong to EB, and belong to EA.
We letclean(B)denote the set of clean indices in B.
To analyze the number of clean indices, we will ﬁrst lower bou nd the probability that an index
is clean.
Claim A.3. For any index j∈[w], the probability that jis a clean index is at least/parenleftbig1
e9c/parenrightbig
.
37Proof.For any edge ein blockB, sincec|E|/2 edges are given to Alice and c|E|/2 edges are given
to Bob, for suﬃciently large |E|,
Pr(e /∈EB) =/parenleftbigg
1−1
|E|/parenrightbiggc|E|/2
/greaterorequalslant1
ec,
Pr(e /∈EB,e∈EA)/greaterorequalslant1
ec·/parenleftBigg
1−/parenleftbigg
1−1
|E|/parenrightbiggc|E|/2/parenrightBigg
/greaterorequalslant1
ec·/parenleftbigg
1−1
ec/2/parenrightbigg
/greaterorequalslant1
e3c/2
Pr(e /∈EA,e∈EB)/greaterorequalslant1
e3c/2,
where we have used that e−2x/lessorequalslant1−x/lessorequalslante−xfor 0< x <1/2. For index jto be clean, the 6 edges
associated with it must be distributed according to Deﬁnition A.2 . As the process of distributing
edges is independent, the probability that index jis clean is at least1
e9c.
We can prove that a constant fraction of the indices are clean even with our updated deﬁnition.
Lemma A.4. For any ﬁxed block B, over the randomness of edges given to EAandEB,
Pr/parenleftBig
|clean(B)|/greaterorequalslantw/2e9c/parenrightBig
/greaterorequalslant1−1/w4,
for some constant c.
Proof.LetRjbe the indicator function for whether index j∈[w] is clean in block B. We want to
prove that R=/summationtext
j∈[w]Rjis at least w/2e9cwith high probability.
For each j∈[w],Rj= 1 with probability at least 1 /e9cand 0 otherwise for each j∈[w]
independently by Claim A.3 . Chernoﬀ bound from Proposition 2.2 can be applied here. We know
E[R] =/summationtextw
j=1E[Rj] =w/e9c. Thus, for large w,
Pr/parenleftBig
R <w
2e9c/parenrightBig
= Pr/parenleftbigg
R </parenleftbigg
1−1
2/parenrightbigg
·E[R]/parenrightbigg
/lessorequalslantexp/parenleftbigg−1
3·w
2e9c·1
4/parenrightbigg
/lessorequalslant1
w4.
We retain our deﬁnition for active blocks, that is blocks whe re index σi(1) is clean are active.
It is easy to see that even with our new notion of clean indices ,σ(1) is uniform over clean(B)
conditioned on Bbeing active.
Lemma A.5. Suppose we sample B=Block(x,σ)conditioned on Bbeing active. Then, σ(1)is
chosen uniformly at random from clean(B).
Proof.In any block, whether or not an index j∈[w] is clean depends only on how the edges in
blockBare given to Alice and Bob in EAandEB. It does not depend on choice of permutation
σ(j). Any block is active with probability clean(B)/w, since index σ(1) is chosen uniformly at
random from [ w].
38Letℓbe any index in clean(B). Then,
Pr(σ(1) =ℓ|Bis active) =Pr(σ(1) =ℓ)
Pr(Bis active)(ℓ∈clean(B),σ(1) =ℓimpliesBis active)
= Pr(σ(1) =ℓ)·w
clean(B)(asBis active with probabilityclean(B)
w)
=1
w·w
clean(B)(asσ(1) is uniformly random over [ w])
=1
clean(B).
We are able to prove that the number of active blocks is high in any multi-block graph in
stochastic streams as well.
Lemma A.6. ForG=Multi-Block (X,Σ)andFsampled in SHXw,twitht/greaterorequalslant8·e9c·lnw:
Pr/parenleftBig
|active(G)|/greaterorequalslantlnw/parenrightBig
/greaterorequalslant1−1/w2.
Proof.BlockBibeing active is independent of other blocks for i∈[t], as it depends only on σiand
how the edges in block Biare distributed in EAandEB. For any i∈[t], byClaim A.3 ,
Pr(Biis active) = Pr/parenleftbig
σi(1) is clean/parenrightbig
/greaterorequalslant1/e9c.
LetRibe the indicator variable for whether block Biis active. We want to prove that R=/summationtextt
i=1Ri
is larger than ln wwith high probability. By linearity of expectation, E[R]/greaterorequalslantt·1
e9c/greaterorequalslant8lnw.
Chernoﬀ bound from Proposition 2.2 can be used here as Ris are independent.
Pr(R/greaterorequalslantlnw) = Pr/parenleftbigg
R/greaterorequalslant/parenleftbigg
1−7
8/parenrightbigg
·E[R]/parenrightbigg
/greaterorequalslant1−exp/parenleftbigg−49
64·8lnw
3/parenrightbigg
/greaterorequalslant1−1
w2.
Claims 4.9 and4.10still hold here as they are only related to the probability of success of
the protocol and not how the edges are distributed. Hence, af ter conditioning on the input graph
havingta/greaterorequalslantlnwactive blocks, wc=w/2e9cclean indices in each block, ( xi,σi) forBi/∈active(G),
and all the edges associated with indices [ w]\clean(Bi) forBi∈active(G), Alice and Bob are again
left with the following scenario.
•Alice has ta/greaterorequalslantlnwstrings of length wc=w/2e9ceach,xi
jfori∈[ta],j∈clean(Bi) picked
uniformly at random and independently.
•Bob has tapermutations from Swc(though the numbering of the indices may diﬀer), σi
restricted from clean−1(Bi) toclean(Bi) fori∈[ta] chosen uniformly at random and indepen-
dently.
Lemma 4.11 holds here as well for the updated number of clean indices. We can prove the following
lemma for stochastic streams.
39Lemma A.7. For suﬃciently large w/greaterorequalslant1and constant c=O(1)witht/greaterorequalslant8·e9c·lnw, the
distributional communication complexity of SHXw,tfor probability of success at least1
2+1
6wis
Ω(w·t).
The proof of Lemma A.7 is identical to the proof of Lemma 3.16 forwc=w/2e9candta/greaterorequalslantlnw.
Putting things together, we prove Lemma 5.13 to ﬁnish this section.
Proof of Lemma 5.13 .Wecanextendourharddistribution µngcforanyn,kbyapaddingargument
described in the proof of Theorem 1 . We know by Proposition 2.4 that the randomized communi-
cation complexity of SGCn,kis lower bounded by the distributional communication compl exity of
SGCn,kover distribution µngc.
We also know that, for w= (n/4k) + 1 and t/greaterorequalslant(k−3)/3/greaterorequalslant8·e9c·lnw, the deterministic
communication complexity of SGCn,kover distribution µngcfor probability of success at least 2 /3
is lower bounded by the distributional communication compl exity of SHXw,tfor probability of
success at least1
2+1
6wbyLemma A.1 .
We also know that SHXw,trequires Ω( w·t) = Ω(n/k·k) = Ω(n) space by Lemma A.7 , which
completes the proof.
40