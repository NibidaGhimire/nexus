State Derivative Normalization for
Continuous-Time Deep Neural Networks
Jonas Weigand∗Gerben I. Beintema∗∗Jonas Ulmen∗∗∗
Daniel G¨ orges∗∗∗Roland T´ oth∗∗Maarten Schoukens∗∗
Martin Ruskowski∗
∗Chair of Machine Tools and Control Systems, RPTU Kaiserslautern, and the
German Research Center for Artificial Intelligence, Kaiserslautern, Germany
(e-mail: jonas@weigand-weigand.de and martin.ruskowski@rptu.de, ORCID:
0000-0001-5835-3106, 0000-0002-6534-9057)
∗∗Control Systems (CS) Group at the Department of Electrical Engineering,
Eindhoven University of Technology, Netherlands. R. T´ oth is also affiliated to the
Systems and Control Laboratory at the Institute for Computer Science and
Control, Budapest, Hungary (e-mail: g.i.beintema@tue.nl, r.toth@tue.nl and
m.schoukens@tue.nl, ORCID: 0000-0002-7822-6283, 0000-0001-7570-6129,
0000-0002-4904-1255)
∗∗∗Institute for Electromobility, RPTU Kaiserslautern, Germany (e-mail:
jonas.ulmen@rptu.de and daniel.goerges@rptu.de, ORCID: 0000-0003-1597-1523,
0000-0001-5504-0972)
This work has been accepted for presentation at the 20thIFAC
Symposium on System Identification 2024.
Abstract: The importance of proper data normalization for deep neural networks is well known.
However, in continuous-time state-space model estimation, it has been observed that improper
normalization of either the hidden state or hidden state derivative of the model estimate, or
even of the time interval can lead to numerical and optimization challenges with deep learning
based methods. This results in a reduced model quality. In this contribution, we show that
these three normalization tasks are inherently coupled. Due to the existence of this coupling, we
propose a solution to all three normalization challenges by introducing a normalization constant
at the state derivative level. We show that the appropriate choice of the normalization constant
is related to the dynamics of the to-be-identified system and we derive multiple methods of
obtaining an effective normalization constant. We compare and discuss all the normalization
strategies on a benchmark problem based on experimental data from a cascaded tanks system
and compare our results with other methods of the identification literature.
Keywords: System Identification, Continuous-Time, Neural Ordinary Differential Equations,
Nonlinear State-Space
1. INTRODUCTION
Machine learning is a powerful tool for time series mod-
eling and system identification (Schoukens and Ljung,
2019). Models can be categorized into Discrete-Time (DT)
models and Continuous-Time (CT) models such as Neural
Ordinary Differential Equations (NODE) (Chen et al.,
2018), Runge-Kutta Neural Networks (RKNN) (Wang and
Lin, 1998), Deep Encoder Networks (Beintema et al.,
2023) or Liquid Time Constant Neural Networks (LTC)
(Hasani, 2020). In control engineering, CT models are
often preferred, as they are closely related to physical
models, they are often found more attractive for designing
controllers since shaping performance with such models
is more intuitive, and they can be used under irregular
sampling.
⋆The project RACKET supported this research under grant
01IW20009 by the German Federal Ministry of Education and Re-
search.Data normalization is a key data preprocessing step in
modern machine learning approaches. The core idea of
normalization is to map the input and output data to a
numerically favorable range (e.g. zero-mean and standard
deviation of 1) (Ba et al., 2016). This is important to ( i)
comply with the underlying assumption in many param-
eter initialization strategies for Neural Networks (NN’s)
(e.g., Xavier initialization (Glorot and Bengio, 2010)) and
to (ii) numerically improve the effective numerical range of
the gradients for the nonlinear activation functions (Ioffe
and Szegedy, 2015).
Prior research demonstrates the significant enhancement
in model performance through State Derivative Normal-
ization (SDN) as evidenced by (Weigand et al., 2021;
Beintema et al., 2023). Despite these advancements, a
comprehensive exploration and analysis of SDN’s mecha-
nisms remain absent from published literature. In addition,
practical methodologies to estimate the normalization fac-
tor are missing in the literature.arXiv:2401.02902v2  [eess.SY]  14 May 2024•Consequently, this paper analyses the state, the state
derivative, and the time normalization as well as their
coupling. Additional graphical interpretation and the
connection to Ordinary Differential Equations (ODE)
solvers are presented.
•Furthermore, three approaches are proposed to esti-
mate this normalization factor and these approaches
are compared on a well-studied benchmark example.
The remainder of the paper is structured as follows. Sec-
tion 2 introduces the considered identification problem.
We present multiple interpretations of the use of a nor-
malization constant in Section 3. Section 4 defines the
criteria for an effective normalization and provides three
methods for practical implementation. Section 5 applies
the method to a real-world benchmark problem, and in
Section 6 concluding remarks on the established results
are made.
2. THE IDENTIFICATION PROBLEM
2.1 Data-generating System
Consider a nonlinear system that is represented by a
nonlinear state-space representation
˙x(t) =f(x(t), u(t)) (1a)
yk=h(xk, uk) +wk, (1b)
x(0) = x0, (1c)
where t∈R≥0is the continuous time, x(t)∈Rnxis
the state associated with (1) with nx∈N,x0∈Rnx
being the initial state, u(t)∈Rnuis the exogenous input,
andf:Rnx×nu→Rnxis locally Lipschitz continuous
ensuring that all solutions of xare forward complete. With
respect to the output equation, y(k)∈Rnystands for the
sampled system output at time moments kTswith Ts>0
being the sampling interval and k∈Z≥0,xk=x(kTs)
anduk=u(kTs), while wk∈Rnyis an i.i.d. zero-mean
white noise process with finite variance Σ w. Such a hybrid
process and noise model is often used in CT nonlinear
system identification (Schoukens and Ljung, 2019).
2.2 Identification
For the considered system class, the CT model identifi-
cation problem can be expressed for a given data set of
measurements
DN={(u0, y0),(u1, y1), ...,(uN−1, yN−1)}, (2)
generated by (1), with unknown wk,x(t), ˙x(t), and initial
state x0, as the following optimization problem (a.k.a. ℓ2
simulation loss minimization)
min
θ,ˆx01
NN−1X
k=0∥yk−ˆyk∥2
2,
s.t. ˆ yk=hθ(ˆx(kTs)),
˙ˆx(t) =fθ(ˆx(t), u(t)),(3)
where ˆ x(t)∈Rnxis the model state, hθand fθare
the output and state-derivative functions parameterized
byθ∈Rnθ. These two functions are represented by
multi-layer feedforward NN’s within this paper. The state
network fθ(·) input layer nodes are considered as model
state ˆ x(t) and exogenous variables u(t), and the outputlayer nodes are considered as model state derivative ˙ˆx(t)
(Schoukens, 2021; Suykens et al., 1995). However, the pre-
sented results also hold when other function approximators
such as polynomials are considered.
3. INTERPRETATION OF THE NORMALIZATION
State Derivative Normalization (SDN) The idea is
the introduction of a normalization factor τ, which scales
the state-derivative equation of the state-space model
during training:
˙ˆx(t) =1
τfNN(ˆx(t), u(t)) (4)
The normalization can be implemented as a scalar nor-
malization τ∈R>0, or as a vector τ∈Rnx
>0corre-
sponding to elements of the output of the hidden state
network fNN. In the latter case, (4) is defined as ˙ˆx(t) =
diag(1
τ1, . . . ,1
τnx)fθ(ˆx(t), u(t)). This linear scaling can also
be interpreted as changing the weight initialization of the
output layer of fNN(·,·).
It is clear that τin (4) scales the hidden state derivative.
Hence, it can be used to normalize the state derivatives.
Next, we show that the derivative normalization is inher-
ently coupled to the scaling of the state xand the scaling
of the time t.
State Normalization. It is also possible to rewrite (4)
as:
d(τˆx(t))
dt=fNN(ˆx(t), u(t)) (5a)
˜x(t)≜τˆx(t) (5b)
d˜x(t)
dt=fNN(˜x(t)/τ, u(t)) (5c)
In this way, normalization can be interpreted as the
normalization of the magnitude of the state.
Time-Based Normalization. We can rewrite (4) as
dˆx(t)
d(t/τ)=fNN(ˆx(t), u(t)), (6a)
˜t≜t/τ, (6b)
dˆx(˜tτ)
d˜t=fNN(ˆx(˜tτ), u(˜tτ)), (6c)
which suggests that the normalization can be viewed as a
scaling of time by a factor τ.
This time rescaling can also be viewed as changing the
effective integration length in ODE solvers. For instance,
ˆx(t) = ˆx(0) +Zt
01
τfNN(ˆx(t′), u(t′))dt′(7a)
= ˆx(0) +Zt/τ
0fNN(ˆx(τ˜t′), u(τ˜t′))d˜t′(7b)
using the substitution of ˜t′=t′/τ. This integration by
substitution shows that the normalization can be viewed as
rescaling the effective integration time from ttot/τ. This
can be translated directly to a wide range of numerical
integration schemes by rescaling the integration length and
step size simultaneously.Graphical intuition about the time rescaling can be gained
from output data of the Cascaded Tank System (CTS)
benchmark in Fig. 1 (Schoukens et al., 2016). The mea-
sured data (black) of the water level is given at a sample
rate of Ts= 4.0 s, the scaled data (red) is transferred to a
sample time of Tm=Ts/τ= 1.0, while the number of time
steps remains unchanged. The original data is transferred
to the scaled time domain, where the model is trained
and evaluated, and the results are transferred back to the
original time grid.
0 200 400 600 800 1,0001,2001,40046810
Time (s)/UnitlessWaterlevel (V)
Fig. 1. Time scaling of the output measurement of the
Cascaded Tank System (CTS) benchmark (Schoukens
et al., 2016). Black: Original measurement (unit sec-
onds). Red: Time-domain scaled data (unitless).
It is known that the stiffness of an ODE system is subject
to temporal variation. However, employing a constant (lin-
ear) normalization across all stiffness regions is essential
from a practical model inference point of view. Moreover,
averaging across multiple regions increases the robustness
of the estimation by providing a broader range of suit-
able normalization factors. Our subsequent experiments
demonstrate that there exists a broad valley of good nor-
malization factors (Fig. 3). Remarkably, all normalization
factors within this range lead to high-performance models.
The key takeaway of representations (4), (6) and (5) is that
SDN collectively influences the hidden state, the hidden
state derivative, and the scaling of time t.
4. ESTIMATION OF THE NORMALIZATION
FACTOR
Since the system dynamics are unknown a priori, selecting
a normalization factor τthat results in a favorable numer-
ical performance of the training algorithm is challenging
a priori, it has to be estimated during training, from the
available data, or by heuristics. We propose three methods:
•make the normalization factor a trainable parameter,
•estimate τusing cross-validation,
•use a heuristic-based approach starting from a linear
approximation of the system.
Trainable Parameter. The normalization factor τcan
be added to the set of parameters present in the op-
timization problem (3). This results in a simultaneous
optimization of τ,θ, and ˆ x0under the ℓ2simulation loss.
For this method, a vectorized normalization leads to more
optimization parameters than using a single scalar.
In terms of implementation, τ >0 should hold to fulfil its
role as a magnitude normalization. As most NN libraries
do not provide constrained optimization one can imple-
ment
ˆτ= max( ϵ, τ) (8)with a small positive number ϵ∈R>0. Empirical evidence
demonstrates that the trainable normalization quickly
reaches a stationary value.
From a pure mathematical perspective, making τa train-
able parameter may seem to have little impact because the
expressive power of the NN could easily compensate for
it. Practically, the optimizer would need to adapt a large
quantity of parameters instead of a single normalization
factor. Additionally, traditional NN initialization methods
assume inputs and outputs are within a specific range. We
consolidated the normalization dependency into a single
trainable parameter τ, making the training process simpler
and more robust, as demonstrated in our experiments.
Overall, this method offers high adaptability across differ-
ent data sets and NN configurations, an easy implemen-
tation, and demands only little additional computational
load.
Cross-validation. The normalization factor τcan be
considered as a hyperparameter and estimated using cross-
validation methods. For example, (3) is optimized using
a training data set for a chosen set of values of τ, a
validation data set is used subsequently to determine
the best performing τvalue. This method is universally
applicable to nonlinear systems, is easy to implement if
a hyperparameter tuning is already available, and, given
sufficient function evaluations, ensures global optimality.
Heuristic approach based on an approximate model.
This heuristic is derived based on a guarantee of the
existence of a normalized model formalized by (Beintema
et al., 2023) which states;
Theorem 1. Given a input trajectory u(t), a non-constant
state trajectory x(t) which satisfies ˙ x(t) =f(x(t), u(t)) as
in (1) for all t∈[0, L] than there exists a τ∈R+and a
scalar state transformation γˆx(t) =x(t) such that both
the model state trajectory ˆ x(t) and model derivative fNN
given by ˙ˆx(t) =1
τfNN(ˆx(t), u(t)) as in (4) are normalized
as
var(ˆx)≜1
LZL
01
nx∥ˆx(t)∥2
2dt= 1, (9a)
var(fNN(ˆx, u)) = 1 . (9b)
Proof. With
γ=p
var(x) (10)
τ=p
var(x)/var( ˙x) (11)
the normalization conditions are satisfied, as shown below
var(ˆx) = var( x/γ) = var( x)/γ2= 1,
var(fNN(ˆx, u)) = var( τ˙ˆx) = var( τ˙x/γ) =τ2/γ2var( ˙x) = 1 .
We base our heuristic on the relationship of (11). We
make two alterations (i)the integral over time can be
approximated by a summation over the time samples to
make this tractable and (ii)in (11) we use an approximate
model instead of the system equations.
We have observed that using a linear approximate model
based on the Best Linear Approximation (BLA) of a non-
linear system can get a sufficiently accurate estimate of the
optimal τ. The BLA offers a linear approximation of a non-
linear system, best in the mean-squared-error sense, based
on measured input-output data (Pintelon and Schoukens,2012). Estimating a BLA of a potentially nonlinear system
is commonly done using Prediction Error Minimization
(PEM), minimizing a least squares prediction error. Using
an estimated BLA, an estimate of the state and state-
derivatives, xBLAand ˙xBLA, can be obtained to compute
τthrough (11):
τBLA=p
var(xBLA)/var( ˙xBLA). (12)
This method is especially valuable for dynamic systems
that can be reasonably well represented by a linear model
for the considered range of excitation. Also note that any
linear transformation that can be present in the BLA state
and state derivative estimate does not affect the resulting
τestimate in (12). Additionally, besides normalization,
the BLA can be used as a good NN weight initialization
candidate (Schoukens and T´ oth, 2020).
Frequency-domain analysis of this approach offers addi-
tional insight into the optimal choice of the normalization
factor. Consider that the input signal has a periodicity
ofNsamples, then one can decompose the input into its
Discrete Fourier Transform (DFT) components as:
uk=1
NN−1X
m=0Umej2π
Nmk(13)
Um=N−1X
k=0uke−j2π
Nmk(14)
Both the variance of the state and state-derivative can be
expressed using these DFT components as:
var(x) =1
LZL
0∥x(t)∥2
2dt∼∞X
m=−∞∥Xm∥2
2 (15a)
var( ˙x) =1
LZL
0∥˙x(t)∥2
2dt∼∞X
m=−∞ω2
m∥Xm∥2
2(15b)
where ωm= 2πm
NTs.
Since we are considering the BLA, the Fourier components
of the state can be written in terms of the input-to-state
frequency response function Xm=G(jωm)Um, assuming
a single input system for simplicity. Therefore, substituting
this and (15) into (12) results in
τBLA=sP∞
m=−∞UHmGH(jωm)G(jωm)UmP∞
m=−∞ω2mUHmGH(jωm)G(jωm)Um,(16)
where ·Hdenotes the Hermitian operation. Hence, if the
signal u(t) only consists of a single sine wave (i.e. only
one nonzero Um) then τBLA= 1/ωm. Furthermore, if u(t)
orG(jωm) has a finite bandwidth bounded by Ω then
τBLA≥1/Ω.
5. EXPERIMENTAL RESULTS
Benchmark. We apply the proposed methods to the CTS
benchmark (Schoukens et al., 2016). The setup is depicted
in Fig. 2. It consists of two vertically mounted tanks, where
the upper one has a water inflow using a pump, and the
water flows from the upper tank into the lower one. The
task is to estimate a model that can predict the water
level of the lower tank given a pump input sequence. The
experiment incorporates an overflow of the tank, which
introduces a hard saturation function.
Fig. 2. Picture of the Cascaded Tank System (CTS)
(Schoukens et al., 2016).
Model Configuration. We chose the fixed-step Runge-
Kutta 4 ODE solver. The network fNN(·,·) is chosen to
consist of linear layers with matrices A∈Rnx×nx,B∈
Rnx×nuand two residual hidden layers with a Leaky ReLU
activation function σ(·) and 64 hidden units for each layer.
Weight matrices are denoted as W(·)and bias terms as
b(·)with appropriate dimensions. Bias terms for the linear
layer are disabled. An output network gNN(·,·) is also used
with the same structure as the state network fNN(·,·). The
overall NN model can be written as:
dx(t)
dt=fNN(x(t), u(t)) =Ax(t) +Bu(t) (17a)
+WF1σ
WF2σ
WF3
x(t)
u(t)
+bF3
+bF2
y(t) =gNN(x(t), u(t)) =Cx(t) +Du(t) (17b)
+WG1σ
WG2σ
WG3
x(t)
u(t)
+bG3
+bG2
More details on the general model structure are given in
(Schoukens, 2021; Suykens et al., 1995). White box model-
ing of the CTS would lead to 2 states. Nevertheless, we set
the number of states nx= 4. It is observed that this results
in better learning behavior. This is motivated by the effect
of state augmentation (Dupont et al., 2019), corresponding
to the fact that increasing the state dimension allows to
describe the system behavior with less complex nonlinear-
ities involved (also expressed by the immersion concept
in nonlinear system theory). The initial hidden states are
obtained using a Deep Encoder Network (Beintema et al.,
2023) eNN(·,·) with na=nb= 5. Weight matrices are
initialized with a small random number chosen from a
uniform distribution U[−0.01,0.01], while bias terms are
initialized to zero. The scalar normalization is initialized
with 1 and the vectorized normalization with 1 /nx. A bar-
rier function LNis applied to the linear layer Aof the state
network to ensure negative definiteness (Weigand et al.,
2021). This barrier minimizes drifts over long simulation
horizons and is estimated using the differentiable Sylvester
Criterion. Furthermore, we apply a Differential Algebraic
Equations (DAE) network
dNN(x(t), u(t)) = (18)
WD1σ
WD2σ
WD3
x(t)
u(t)
+bD3
+bD2
+bD1
to account for the hard state saturation, which is trained
using an additional penalty function LD. It does not affectthe forward model evaluation. The optimization problem
is given by
min
θLN+LW+
K−JX
k=max( na,nb)
J−1X
j=0∥yk+j−ˆyk+j|k∥2
2+LD,j
,
s.t. ˆ yk+j|k=gNN(xk+j|k),
xk+j+1|k= ODE Solve
xk+j|k, uk+j,1
τfNN(·,·), Ts
,
xk|k=eNN(uk−1, ..., u k−nb, yk−1, ..., y k−na),
LD,j=λD 
dNN(xj|j, uj|j)2,
LN=0 if A≺0
λNotherwise .,
LW=λW∥θ∥2
2
with λN= 1012andλD= 103. The bar notation xk+j|k
reads as ”The simulated state at xk+jstarting at kwith
initial state xk|k”. The notation ∥·∥2
2stands for the squared
Euclidean norm. The sampling time is Ts= 4 s.
The implementation is written in Python, using PyTorch.
Furthermore, the ADAM optimizer is applied with unmod-
ified configuration except for the learning rate, which is
set to 0 .003 for the first 1 ,000 steps, and to 0 .0009 until
step 3 ,000, and to 0 .00027 for all subsequent iterations.
The maximum number of optimization steps is set to
20,000. Regularization is obtained using weight decay of
λW= 10−8and early stopping when the best validation
error does not improve for 2 ,000 iterations. We do not
apply Batch-Norm orDropout . Input and output data are
z-score normalized. As no explicit validation data set is
given for CTS, we apply the first 512 time steps of the
test data for early stopping. Test data is not accessed for
any other reason. Training is performed in 64 mini-batches
with a sequence length of J= 128 steps using Truncated
Simulation Error Minimization (TSEM) (Forgione and
Piga, 2021).
Effect of Normalization. We analyze the effect of
the normalization factor τgiven the CTS benchmark,
a fixed NN configuration in simulation mode (free-run
simulation/simulation error), and a fixed training pipeline
throughout all experiments. In addition to the proposed
state derivative normalization, the magnitude of the data
(waterlevel) is normalized, too. Performance is measured
in terms of the Root Mean Squared Error (RMSE) of the
simulation error w.r.t. the test data:
eRMSE =vuut1
K−max( na, nb)K−1X
k=max( na,nb)∥yk−ˆyk∥2
2.
Fig. 3 displays a grid search for different scalar values
ofTs/τfixed prior to the model training on a log scale
(Ts= 4 s for CTS). Each experiment is repeated 20 times
to account for and observe the effect of randomness in
the initial weights. We observe in Fig. 3 that both the
performance and variance of the results get worse for large
and small values of τ. Furthermore, Fig. 3 indicates that
there exists a desirable optimum. The optimum is different
from the unscaled function estimation with τ= 1, which
corresponds to Ts/τ= 4.0 for CTS in Fig. 3. The medianRMSE for Ts/τ= 4.0 is 0 .75(V) and the mean RMSE
is 1.98(V), considerably larger than the normalized result.
The model outputs obtained with the proposed estimators
forτare displayed in Figure 4.
0.0001 0.002 0.030.10.52.34.09.54000.20.40.60.81
Normalized time Ts/τ(unitless)Error eRMSE (V)
Fig. 3. 200 independent experiments, each with the same
configuration except for the random weight initializa-
tion and a fixed scalar normalization factor. We tested
10 different normalization factors repeated 20 times
each. The box plot displays the median, lower quar-
tile, upper quartile, minimum and maximum values.
Note that experiments with a normalization Ts/τ=
40 sometimes lead to unstable results, with RMSE
>109.
Using the cross-validation method to estimate the nor-
malization, we observe in Fig. 3 good results between
Ts/τ= 0.031 and Ts/τ= 2.276, with a best RMSE at
Ts/τ= 0.543. For the trainable normalization method, we
define a normalization vector τas a trainable parameter
and implement (8). It is not optimized with hyperparam-
eter tuning but jointly trained with all NN weights, using
the same learning rate. The normalization is initialized to
Ts/τ= 0.1. After training 20 models, we obtain an average
normalized time constant Ts/τ= 0.072±0.048 (mean ±
std). Using the BLA and (12), we estimate a normalization
factor Ts/τ= 0.054, which matches the results in Fig. 3.
0 1,000 2,000 3,000 4,00046810
Time (s)Waterlevel (V)Target
Trained
Cross-Val
BLA
Fig. 4. Simulation results on the CTS benchmark
(Schoukens et al., 2016). Results of the best models
obtained with the trained normalization factor, the
cross-validation method, and the Best Linear Approx-
imation are displayed.
Comparison to Literature. Since several black-box
identification methods have been applied to the CTS in
the literature, we can compare our method to the black-
box approaches with the best performance (in terms of
RMSE on the test data). This comparison is provided in
Table 1. The following approaches are compared: (Relan
et al., 2017) estimates a BLA and develops an unstructuredNonlinear State Space Model (NLSS) with different initial-
ization schemes. A nonparametric Volterra series model
is estimated in (Birpoutsoukis et al., 2018). A nonlinear
state-space model based on Gaussian processes is applied
in (Svensson and Sch¨ on, 2017). As a direct comparison,
continuous-time NN on this modeling problem has been
applied in (Beintema et al., 2023; Forgione and Piga,
2021; Mavkov et al., 2020; Weigand et al., 2021). These
works emphasize initial state estimation, fitting criteria,
and model stability differently. It can be observed that
the proposed approach outperforms all other black-box
identification approaches.
Table 1. Results for the Cascaded Tank Bench-
mark.
Test data
Method eRMSE (V)
Best Linear Approximation 0.75
Truncated Volterra Model 0.54
State-space with GP-inspired Prior 0.45
Integrated Neural Networks 0.41
Soft-constrained Integration Method 0.40
Stable Runge-Kutta Neural Network 0.39
Nonlinear State Space Model 0.34
Truncated Simulation Error Minimization 0.33
Deep Subspace Encoder 0.22
ours (SDN, Trained Parameter, best model) 0.2151
ours (SDN, Trained Parameter, mean ±std) 0.2977 ±0.1259
ours (SDN, Cross-Validation, best model) 0.2054
ours (SDN, Cross-Validation, mean ±std) 0.2777 ±0.052
ours (SDN, BLA, best model) 0.2253
ours (SDN, BLA, mean ±std) 0.2633 ±0.0284
6. CONCLUSION
We have shown the importance of proper state normal-
ization when considering continuous-time modeling with
state-space NN’s. This is handled by introducing a normal-
ization constant in front of the state derivative network.
We have provided a state domain, a state derivative do-
main, and a time domain interpretation of this concept,
and showed the beneficial effects of such a normaliza-
tion. To estimate the appropriate normalization constant,
three approaches based on a trainable parameter, cross-
validation, and BLA have been proposed to ensure the
practical applicability of the normalization. Based on sim-
ulation studies, we have shown that the proposed method-
ologies enable to improve model estimation with NN-based
state-space modeling methods.
REFERENCES
Ba, J.L., Kiros, J.R., and Hinton, G.E. (2016). Layer
normalization. URL http://arxiv.org/pdf/1607.
06450v1 .
Beintema, G.I., Schoukens, M., and T´ oth, R. (2023).
Continuous-time identification of dynamic state-space
models by deep subspace encoding. In The Eleventh
International Conference on Learning Representations .
Birpoutsoukis, G., Csurcsia, P.Z., and Schoukens, J.
(2018). Efficient multidimensional regularization for
volterra series estimation. Mechanical Systems and Sig-
nal Processing , 104, 896–914. doi:10.1016/j.ymssp.2017.
10.007.
Chen, R.T.Q., Rubanova, Y., Bettencourt, J., and Duve-
naud, D. (2018). Neural ordinary differential equations.Advances in neural information processing systems , 31.
URL http://arxiv.org/pdf/1806.07366v5 .
Dupont, E., Doucet, A., and Teh, Y.W. (2019). Aug-
mented neural ODEs. Advances in neural information
processing systems , 32. URL http://arxiv.org/pdf/
1904.01681v3 .
Forgione, M. and Piga, D. (2021). Continuous-time system
identification with neural networks: Model structures
and fitting criteria. European Journal of Control , 59,
69–81. doi:10.1016/j.ejcon.2021.01.008.
Glorot, X. and Bengio, Y. (2010). Understanding the
difficulty of training deep feedforward neural networks.
International Conference on Artificial Intelligence and
Statistics (AISTATS) .
Hasani, R. (2020). Interpretable Recurrent Neural Net-
works in Continuous-time Control Environments . Ph.d.
thesis, TU Wien.
Ioffe, S. and Szegedy, C. (2015). Batch normalization:
Accelerating deep network training by reducing internal
covariate shift. In International conference on machine
learning , 448–456. PMLR.
Mavkov, B., Forgione, M., and Piga, D. (2020). Integrated
neural networks for nonlinear continuous-time system
identification. IEEE Control Systems Letters , 1. doi:
10.1109/LCSYS.2020.2994806.
Pintelon, R. and Schoukens, J. (2012). System identifica-
tion: a frequency domain approach . John Wiley & Sons.
Relan, R., Tiels, K., Marconato, A., and Schoukens, J.
(2017). An unstructured flexible nonlinear model for the
cascaded water-tanks benchmark. IFAC-PapersOnLine ,
50(1), 452–457. doi:10.1016/j.ifacol.2017.08.074.
Schoukens, J. and Ljung, L. (2019). Nonlinear system
identification: A user-oriented road map. IEEE Control
Systems Magazine , 39(6), 28–99.
Schoukens, M. (2021). Improved initialization of state-
space artificial neural networks. In 2021 European
Control Conference (ECC) , 1913–1918. IEEE.
Schoukens, M., Mattsson, P., Wigren, T., and Noel, J.P.
(2016). Cascaded tanks benchmark combining soft and
hard nonlinearities. Workshop on Nonlinear System
Identification Benchmarks, Brussels, Belgium .
Schoukens, M. and T´ oth, R. (2020). On the initialization of
nonlinear LFR model identification with the best linear
approximation. IFAC-PapersOnLine , 53(2), 310–315.
doi:10.1016/j.ifacol.2020.12.142.
Suykens, J.A., De Moor, B.L., and Vandewalle, J. (1995).
Nonlinear system identification using neural state space
models, applicable to robust control design. Interna-
tional Journal of Control , 62(1), 129–152.
Svensson, A. and Sch¨ on, T.B. (2017). A flexible state–
space model for learning nonlinear dynamical systems.
Automatica , 80, 189–199. doi:10.1016/j.automatica.
2017.02.030.
Wang, Y.J. and Lin, C.T. (1998). Runge-kutta neural
network for identification of dynamical systems in high
accuracy. IEEE Transactions on Neural Networks , 9(2),
294–307.
Weigand, J., Deflorian, M., and Ruskowski, M. (2021).
Input-to-state stability for system identification with
continuous-time runge–kutta neural networks. Interna-
tional Journal of Control , 1–17. doi:10.1080/00207179.
2021.1978555.