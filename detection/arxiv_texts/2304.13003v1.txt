Functional Individualized Treatment Regimes with Imaging Features
Xinyi Liaand Michael R. Kosorokb
aClemson University,bUniversity of North Carolina at Chapel Hill
Abstract: Precision medicine seeks to discover an optimal personalized treatment plan and thereby provide
informed and principled decision support, based on the characteristics of individual patients. With recent
advancements in medical imaging, it is crucial to incorporate patient-speciÔ¨Åc imaging features in the study
of individualized treatment regimes. We propose a novel, data-driven method to construct interpretable im-
age features which can be incorporated, along with other features, to guide optimal treatment regimes. The
proposed method treats imaging information as a realization of a stochastic process, and employs smoothing
techniques in estimation. We show that the proposed estimators are consistent under mild conditions. The
proposed method is applied to a dataset provided by the Alzheimer‚Äôs Disease Neuroimaging Initiative.
Key words and phrases: Precision medicine; Functional data; Bivariate spline; Imaging data; Treatment
regime.
1. Introduction
Precision medicine is an approach to medical treatment that takes into account individual variability
in various features, such as genomic characteristics, environment factors, medical history, lifestyle,
and current health status. In contrast to traditional treatment strategies, which tend to be uniform and
one-size-Ô¨Åts-all, precision medicine aims to provide optimal treatment that is tailored to the unique
characteristics of each individual. To achieve this goal, individualized treatment rules are developed
to formalize the decision-making process that translates patient information into recommended treat-
ment. The identiÔ¨Åcation and application of individualized treatment regimes (ITRs) are crucial areas of
investigation in precision medicine research, particularly in the study of chronic diseases or disorders
that necessitate diverse medical interventions. Notably, ITRs have been successfully applied in vari-
ous clinical contexts, including but not limited to Alzheimer‚Äôs Disease (Isaacson et al., 2019), diabetes
(Luckett et al., 2020), cancers (Thall et al., 2007; Zhao et al., 2011), and attention deÔ¨Åcit hyperactivity
disorder (Pelham and Fabiano, 2008).
The Ô¨Åeld of medical imaging has experienced signiÔ¨Åcant advancements that have had a notable
impact on disease and health studies. In addition to characterizing abnormalities and providing mor-
phologic disease information, imaging can also deÔ¨Åne the imaging phenotype of the disease, using an
increasingly diverse set of modern imaging tools and imaging biomarkers (Herold et al., 2016). The
Address forcorrespondence: Xinyi Li, School of Mathematical and Statistical Sciences, Clemson University, Clemson,
SC 29634, USA. Email: lixinyi@clemson.eduarXiv:2304.13003v1  [stat.ME]  25 Apr 2023integration of imaging features in precision medicine research has the potential to signiÔ¨Åcantly enhance
patient care by enabling effective early detection and diagnosis tools, as well as guiding optimal treat-
ments and lifestyle interventions. For instance, Alzheimer‚Äôs disease-related pathophysiologic changes
can be visualized up to 15 years before the onset of clinical dementia (Bateman et al., 2012). Thus,
biomedical imaging and imaging-guided interventions, which provide multiparametric morphologic
and functional information, constitute crucial elements in the infrastructure of precision medicine. Con-
sequently, it is essential to develop strategies that incorporate imaging data in conjunction with other
abundant information in precision medicine research.
In this work, we propose a novel and Ô¨Çexible approach, Functional Individualized Treatment
Regimes with Imaging features (FITRI), to adapt semiparametric learning and functional data analysis
(FDA) frameworks to precision medicine with abundant features, including medical images, genetic
features, environment factors, etc. Semiparametric modeling and FDA offer powerful tools to deal with
image features, allowing for a Ô¨Çexible range of possible model speciÔ¨Åcations. SpeciÔ¨Åcally, we apply
FDA for low-dimensional representation of high-resolution and high-dimensional imaging features. In
other words, the imaging features are represented as functional terms in our analysis. We Ô¨Årst de-
velop a multi-dimensional functional principal component (FPC) basis expansion tool to approximate
the functional imaging features, and then conduct a Q-learning (Clifton and Laber, 2020; Kosorok and
Laber, 2019) type analysis to estimate the optimal treatment regimes. We also investigate the theoret-
ical properties of the proposed bases and estimators, and develop computationally efÔ¨Åcient algorithms
accordingly.
The proposed work presents a signiÔ¨Åcant contribution to the Ô¨Åeld of precision medicine research
in several ways. First and foremost, the FITRI approach bridges the gap in the current literature in
precision medicine studies that incorporate imaging features, which is not a trivial task due to the
large volume and complexity of image data. The development and analysis of FITRI require efÔ¨Åcient
statistical methods and decision-support algorithms, which will be discussed later. And the proposed
estimates provide an efÔ¨Åcient data-driven summary statistic for the information contained in the imag-
ing features. Moreover, the proposed work expands the existing methods that use functional data to
construct optimal ITRs. Prior studies, including McKeague and Qian (2014), Ciarleglio et al. (2015),
Ciarleglio et al. (2016), Ciarleglio et al. (2018), Laber and Staicu (2018) and Park et al. (2021), have
developed methods for various FDA models for dense or sparse functional data. All of these works
focus on functional data with a one-dimensional index continuum. In contrast, the proposed basis tool
demonstrates both theoretical superiority and computational efÔ¨Åciency in dealing with imaging data,
which can be widely used in the FDA approach dealing with image data (Nathoo et al., 2019; Zhu
et al., 2022). Such a smoothing tool is lacking in the current literature on functional data indexed by
two or higher-dimensional continuums. Not only do the proposed multi-dimensional FPC bases work
for index continuum dimensions greater than 1, they also provide practical, data-driven interpretation
2for the dominant information in the imaging features.
The rest of the paper is organized as follows. Section 2 presents the construction methods for
the proposed multi-dimensional FPC basis for multivariate functional data, with a particular focus on
the two-dimensional (2D) case. In Section 3, we apply the proposed 2D-FPC basis to the functional
model and describe the corresponding estimator. Section 4 provides a presentation of the theoretical
Ô¨Åndings, while Section 5 elaborates on the implementation aspects. In Section 6, we furnish empirical
investigations to showcase the performance of the proposed estimator. Additionally, Section 7 provides
an illustrative example of real-world data in the context of Alzheimer‚Äôs Disease. The conclusion of
the paper is presented in Section 8, which discusses open issues and future work. The supplemental
materials provide theoretical details.
2. Bases approximation for imaging features
We assume the observed data are fXi;Zi(s);Ai;Yign
i=1, which comprise nindependent, identi-
cally distributed (iid) copies of a trajectory fX;Z(s);A;Yg, where X2X  Rqdenotes patient
information,fZ(s) :s2Vg is a stochastic process indexed by VRmwithm1,A2A=f0;1g
is a binary treatment, and Yis real-valued outcome for which higher values are more desirable. For the
ith subject, the real-valued imaging measure Zi(sj)is observed at point sj2Vforj= 1;:::;Ns. For
example, when m= 2,sj= (sj1;sj2)>could be the coordinates of a voxel center in a two-dimensional
brain image. Let be a nonnegative Ô¨Ånite measure on V, and deÔ¨Åne L2(V;) =ff:kfk;2<1g,
wherekfk2
;2=R
Vjf(s)j2d(s). Note thatL2(V;)is a closed linear space, speciÔ¨Åcally, a Hilbert
space. We assume that Z1;:::;Znare iid with distribution fsuch thatkZk;2<1almost surely.
We employ a Q-learning type analysis. SpeciÔ¨Åcally, deÔ¨Åne the Q-function as
Qfx;z(s);ag= EfYjX=x;Z(s) =z(s);A=ag: (2.1)
Assume that we want to parametrize all linear functionals of Z, which can be characterized as
Z
VZ(s)g(s)d(s) (2.2)
for someg2L2(V;). To be speciÔ¨Åc, we assume a semiparametric working model:
EfYjX=x;Z(s) =z(s);A=ag
=>
1X+Z
V1(s)Z(s)ds+A
>
2X+Z
V2(s)Z(s)ds
; (2.3)
where= (>
1;>
2)>are linear coefÔ¨Åcients, and 1(s)and2(s)are coefÔ¨Åcient functions/maps.
EfÔ¨Åcient tools are essential for addressing coefÔ¨Åcient maps in estimating the Q-function and, sub-
sequently, determining the optimal treatment regime. The analysis of imaging data presents challenges
3such as high dimensionality and resolution, complex geometric structures, including complex bound-
aries, and spatial heterogeneity. A variety of nonparametric methods have been developed for ana-
lyzing images; see for example, tensor-product-based kernel smoothing (Zhu et al., 2014), thin plate
spline smoothing (Ramsay, 2002), soap Ô¨Ålm smoothing (Wood et al., 2008), and multivariate spline
over triangulation (MST; Lai and Schumaker, 2007). MST has demonstrated superiority in analyzing
multi-dimensional (MD) imaging data, as evidenced by its application in bivariate spline analysis of
2D images (Lai and Wang, 2013) and trivariate spline analysis of 3D images (Li et al., 2023). The con-
struction of MST involves two main steps: constructing triangulation to approximate the entire domain
and constructing multivariate splines based on the triangulation. In dealing with MD data, the complex
and irregular shape poses a primary challenge compared to 1D data, which conventional nonparametric
methods struggle to estimate accurately along the boundary, commonly known as the ‚Äúleakage‚Äù issue
(Ramsay, 2002; Wang and Ranalli, 2007). However, the triangulation, a set of MD simplices whose
union approximates the domain Vwell, provides an efÔ¨Åcient tool to address the ‚Äúleakage‚Äù issue. Based
on the constructed triangulation, the multivariate spline can be constructed with explicit formulas, al-
lowing efÔ¨Åcient approximation of the MD images. One can refer to Lai and Schumaker (2007) for
additional technical details.
The use of MST presents numerous advantages in the analysis of MD imaging data. However,
when applied to the model (2.3), theoretical and computational limitations emerge. SpeciÔ¨Åcally, to
establish the asymptotic consistency of the proposed coefÔ¨Åcients estimates and, consequently, the esti-
mated optimal regime, the stability condition for the bases inside the integral is required. Unfortunately,
such a condition is lacking in the literature for MST. Moreover, the construction of multivariate splines
typically involves hundreds or thousands of bases, rendering the whole system underdetermined, given
the limited sample size. To overcome these limitations, we propose an MD-FPC basis based on MST, in
a parallel fashion to the construction of 1D FPC bases. By constructing a Reproducing Kernel Hilbert
Space (RKHS) based on the multivariate spline space, the low-dimensional representation of MD-FPC
can approximate the multivariate spline space, and thus provide a good approximation to the origi-
nal spaceL2(V;). This approach offers theoretical and computational advantages and allows for the
handling of the complicated MD image domain.
2.1. Construction of MD-FPC basis
In the following, while we develop the methodology for general L2spaces, includingVRm, for
m1; we also will specialize some of the results for VR2, that is, for the 2D-FPC basis.
We Ô¨Årst need the following assumptions:
(A1) ( Space ) Suppose we have a Hilbert space H0L2(V;), and assume Z2H 0with probability
one.
4(A2) ( Covariance function ) Assume EkZk2
;2<1. LetV0(s;s0) = EfZ(s) EZ(s)gfZ(s0) 
EZ(s0)g, and suppose V0(s;s0) =P1
k=1kk(s)k(s0), wherefk;k1gis an orthonormal
basis contained inH0, with corresponding eigenvalues 0< 2< 1<1.
(A3) ( Eigenvalues )P1
k=1k<1.
Remark 1. By Assumptions (A1) and (A2), we have the Karhunen-Lo ¬¥eve expansion Z(s) = EZ(s) +P1
k=1kk(s), wherek=R
Vk(s)fZ(s) EZ(s)gd(s),Ek= 0,Var(k) =k, and E(kk0) =
0, for 1k < k0. LetH1be the closed linear span of the basis functions fk;k1g, then con-
sequentlyH1H 0is also a Hilbert space, and Assumption (A3) will then imply that Pr(Z EZ2
H1) = 1 .
LetB1=fb(s)2H 0:kbk;21g,F1=fR
Vb(s)fZ(s) EZ(s)gd(s) :b(s)2B 1g, and
F2=F1F1=fR
VVa(s)b(s0)fZ(s) EZ(s)gfZ(s0) EZ(s0)gd(s)d(s0) :a(s);b(s0)2B 1g.
Theorem A.1 in supplemental materials shows that both F1andF2are Glivenko-Cantelli classes of
functions of Z. LetSbe the closed linear space spanned by MST in H0. LetGNbe the operator that
projects ontoS, where the index Nis the number of spline bases that grow with the sample size n. We
need the following additional assumption:
(A4) ( Convergence of spline space ) Assume that for any g2H 1such thatkgk;21, we have that
kGNg gk;2!0asn!1 .
Remark 2. Assumption (A4) assumes the convergence of the spline space, which is a regular conclusion
in the MST literature; see, for example, Lai and Wang (2013) and Li et al. (2023).
Denote the empirical variance function as Vn(s;s0) =n 1Pn
i=1fZi(s) Zn(s)gfZi(s0) 
Zn(s0)g, where Zn=n 1Pn
i=1Zi. We Ô¨Årst obtain the sequence of theoretical bases based on
Vn(s;s0). DeÔ¨Åne
bn1(s) = arg max
f2B1\SZ
VVf(s)f(s0)Vn(s;s0)d(s)d(s0): (2.4)
As we show in Theorem A.2 in Section A in supplemental materials, bn1converges to 1up to sign
almost surely, as n!1 . In addition, we deÔ¨Åne
bHn1=fclosed linear span of fbn1(s)ginH0g;
bn2(s) = arg max
f2B1\S\bH?
n1Z
VVf(s)f(s0)Vn(s;s0)d(s)d(s0);
wherebH?
n1denotes the closed orthocomplements of bHn1inL2(V;). Similarly,bn2converges to 2
5up to sign almost surely, as n!1 . Repeating the construction recursively, we have for k= 1;:::;K ,
bHnk=fclosed linear span of fbn1;:::;bnkginH0g;
H0k=fclosed linear span of f1;:::;kginH0g; (2.5)
bn(k+1)(s) = arg max
f2B1\S\bH?
nkZ
VVf(s)f(s0)Vn(s;s0)d(s)d(s0):
As shown in Theorem 1 in Section 4.1, fbn1;:::;bnKgform an orthonormal system on S\H 0, and
fbnkgkconverge tofkgkup to sign almost surely, as n!1 .
In practice, in the 2D-FPC setting, we obtain the measurements of the image data fZi(sj)gn;Ns
i=1;j=1
on a Ô¨Ånite grid or random set of points, instead of the whole continuum. That means, Vn(s;s0)in (2.4)
and (2.5) is not computable for practical data settings; consequently, fbnkgkis not directly accessible.
Therefore, we propose to employ bivariate splines over triangulation (BST) as an initial smoothing tool
to construct the 2D-FPC basis function. In 2D-FPC, we are specializing to the setting where VR2.
SpeciÔ¨Åcally, let B(s) =fBJ(s)2SgJ2Jbe the BST basis of S, and let H=R
VB(s)B>(s)d(s)
with dimensionjJjjJj , wherejJjis the cardinality of the bivariate basis index set J. DeÔ¨Åne a ‚Äúpre-
smoothed‚Äù variance-covariance matrix by bivariate spline as
Kn=H 1=2Z
VVB(s)Vn(s;s0)B(s0)>d(s)d(s0)
H 1=2:
Letfb'nkgpn
k=1be the eigenvectors of Kn, ordered from largest eigenvalue to smallest, where pn=
jJj^n. Then fork1, we deÔ¨Åne
bS
nk(s) =b'>
nkH 1=2B(s);bnk=Z
VVbnk(s)bnk(s0)Vn(s;s0)d(s)d(s0): (2.6)
As shown in Theorem 1 (iii) in Section 4.1, fbnk(s)gkandfbS
nk(s)gkare asymptotically equal
up to sign almost surely, as n!1 . Also, by Theorem 1 (iv) in Section 4.1, we have that fbnkgk
converge tofkgkalmost surely. Note that the bnkvalues are also the eigenvalues of Kn. Therefore,
we obtain that thefbS
nk(s)gkfunctions are the constructed orthonormal bases.
3. Functional learning with imaging features for estimating optimal ITRs
With all the preparation in Section 2, we are ready to implement functional learning with imaging
features for estimating the optimal ITRs.
An individualized treatment rule is a function :XV!A so that under , a patient presenting
withX=xandZ(s) =z(s)will be assigned to treatment (x;z(s)). We employ the potential
outcome framework to deÔ¨Åne an optimal ITR. Let Y(a)be the potential outcome under treatment
a2A, and under any regime ,Y() =P
a2AY(a)1f(X;Z(s)) =ag. For a decision rule ,
6letV() = EY()be the value of . An optimal regime for Y, denoted as opt, satisÔ¨ÅesV(opt)
V()for any other regime .
Throughout the paper, we use capital letters, like X, to denote random variables, and lower cases
likexfor instances of corresponding random variables. To identify the optimal regimes in terms of the
data-generating model, we make the following assumptions:
(A5) (Consistency) Y=Y(A).
(A6) (Positivity) There exists some constant csuch that PrfA=ajX;Z(s)gc>0for eacha2A,
X2X andZ(s)2V almost surely.
(A7) (Ignorability) fY( 1);Y(1)g?AjX;Z(s).
Remark 3. Assumptions (A5)‚Äì(A7) are standard in the context of precision medicine (Hern ¬¥an and
Robins, 2010; Robins, 2004). Assumption (A6) is veriÔ¨Åable in the study. Assumption (A7) implies there
are no unmeasured confounders, which generally holds by construction in randomized studies but is not
in general empirically veriÔ¨Åable in observational studies (Rosenbaum, 1984; Rosenbaum and Rubin,
1983).
Under Assumptions (A5)‚Äì(A7), and recalling the deÔ¨Ånition of the Q-function in (2.1), the optimal
ITR isoptfx;z(s)g= arg maxa2AQfx;z(s);ag. LetbQfx;z(s);agbe the functional-regression-
based estimators Ô¨Åt using the data. Then the optimal ITR can be estimated through bQfx;z(s);agvia
bQfx;z(s)g= arg max
a2AbQfx;z(s);ag:
Switch back now to functional regression modeling. Recall our model in (2.3), or equivalently,
assume
Y=>
1X+A>
2X+Z
V1(s)Z(s)d(s) +AZ
V2(s)Z(s)d(s) +";
where Xincludes af1gterm (intercept) and is in Rq,1,22Rq,Ais binary treatment indicator (can
assumeA2f0;1g),Z(s)is a stochastic process on H0, and1(s),2(s)2H 0. Assume EkXk2
2<1,
E("jX;A;Z ) = 0 almost surely, EfVar("jX;A;Z )g<1andVar(") =2.
Denote= (>
1;>
2;1;2)>. Let the generic covariate vector W= (X>;AX>;Z;AZ )>, and
deÔ¨Åne(W) =>
1X+>
2AX+R
V1(s)Z(s)d(s) +R
V2(s)AZ(s)d(s). Also, for subject i,
the observed covariate is Wi= (X>
i;AiX>
i;Zi;AiZi)>. Note thatis a linear operator, instead of
a pure linear coefÔ¨Åcient; and WandWiare ‚Äúfunctional matrices‚Äù, namely a hybrid of both matrices
and functional covariates. We need the following Assumption (A8) for the linear operator .
(A8) ( Linear operator )(Wi) = 0 a:s:)kk= 0, wherekk=k1k2+k2k2+k1k;2+
k2k;2.
7Remark 4. Assumption (A8) can be implied by EWiW>
ibeing positive deÔ¨Ånite, since Ef(Wi)g2=
>EWiW>
i= 0if and only if=0.
We employ the 2D-FPC basis proposed in Section 2 to approximate the covariates ZandAZ. To be
speciÔ¨Åc, denote the 2D-FPC bases for Z(s)andAZ(s)asfb1;nk(s)gkandfb2;nk(s)gk, respectively,
then
Z(s)K1X
k=1bU1;kb1;nk(s); AZ (s)K2X
k=1bU2;kb2;nk(s); (3.1)
wherebU1;k=R
Vb1;nk(s)Z(s)d(s),bU2;k=R
Vb2;nk(s)AZ(s)d(s), andK1andK2are basis
numbers. Note that we exclude the expectation terms in Equation (3.1), which are absorbed into the
intercept terms and have no effect on the estimation procedure. Consequently, 8k,bU1;kandbU2;kdoes
not necessarily have mean zero. Fix 1K1;K2<1. Note thatK1andK2can be zero, in which case
the corresponding terms vanish in the functional regression model. To avoid such trivialities, we will
requireK1;K21. As for the selection of K1andK2, we discuss this in detail in Section 5.1. Accord-
ingly, the coefÔ¨Åcient functions 1(s)and2(s)have the representations `(s) =PK`
k=1`;kb`;nk(s)
for`= 1;2, where`;k=R
Vb`;nk(s)`(s)d(s).
For1K`pn, letbU`(K`) = (bU`;1;:::;bU`;K`)>andbU`;i(K`) = (bU`;i1;:::;bU`;iK`)>,`=
1;2, where for 1in,
bU1;ik=Z
Vb1;nk(s)Zi(s)d(s);1kK1;
bU2;ik=Z
Vb2;nk(s)AiZi(s)d(s);1kK2:
Note that Var(bU`;ik) =b`;nk, whereb`;nkare deÔ¨Åned in (2.6) with V`;n(s;s0),`= 1;2, respectively,
with
V1;n(s;s0) =1
nnX
i=1fZi(s) Zn(s)gfZi(s0) Zn(s0)g;
V2;n(s;s0) =1
nnX
i=1fAiZi(s) (AZ)n(s)gfAiZi(s0) (AZ)n(s0)g: (3.2)
In a parallel fashion to WandWi, for1in, letcW= (X>;AX>;bU>
1(K1);AbU>
2(K2))>and
cWi= (X>
i;AiX>
i;bU>
1;i(K1);AibU>
2;i(K2))>. Assume the full parameter space is =f1;22
Rq;1;22H 0g. Recall the deÔ¨Ånitions of H0KandbHnKin (2.5), which are the closed linear spaces
spanned by basesfbnkgK
k=1andfkgK
k=1, respectively. DeÔ¨Åne the parameter spaces
K1;K2=f1;22Rq;12H 0K1;22H 0K2g; (3.3)
bK1;K2=f1;22Rq;12bHnK1;22bHnK2g:
8As shown in Theorem 1 in Section 4.1, bK1;K2!K1;K2almost surely, as n! 1 . Since
K1;K2, we also have bK1;K2almost surely, as n!1 . Let1= (1;1;:::; 1;K1)>
and2= (2;1;:::; 2;K2)>. With a slight abuse of notation, denote e= (>
1;>
2;>
1;>
2)>, and
the corresponding parameter space as eK1;K2=f1;22Rq;12RK1;22RK2g. Obvi-
ously, the closed linear span of bK1;K2is equal to the closed linear span of eK1;K2almost surely, as
n!1 ; in the sense that, for each b2bK1;K2, there exists one and only one e2eK1;K2such that
e(cW) =b(W)almost surely. Actually, this is an isomorphism by Assumption (A8).
Combining the model (2.3) and the basis expansion with 2D-FPC basis in (3.1), we are able to
obtain the estimates
bn= arg min
2bK1;K2n 1nX
i=1fYi b(Wi)g2: (3.4)
Consequently, the optimal decision rule is estimated as
b= arg max
AbQfbn(W)g=Ih
bQfbn(W)gj(A= 1) bQfbn(W)gj(A= 0)i
>0
=I
b2X+Z
Vb2(s)Z(s)d(s)>0
: (3.5)
Combining all the aforementioned steps, the complete algorithm, which we refer to as the semi-
parametric functional learning algorithm, is given below in Algorithm 1.
Input :fXi;Zi(v);Ai;Yign
i=1.
Output : bnandb.
Step 1. Based onfZi(sj)gn;Ns
i=1;j=1, construct 2D-FPC basis functions fbS
`;nk(s)gK`
k=1by (2.6)
with respect to covariance functions V`;n(s;s0)deÔ¨Åned in (3.2), `= 1;2.
Step 2. Obtain the estimates bnby (3.4).
Step 3. Obtain the estimated optimal regime bthrough (3.5).
Algorithm 1: Semiparametric Functional Learning Algorithm.
4. Theoretical results
In this section, we establish the asymptotic properties of the bases fbnk(s)gkandfbS
nk(s)gkpro-
posed in Section 2, and the estimated band estimated optimal regime bproposed in Section 3. Our
main results are summarized in Theorems 1 and 2.
4.1. Theoretical results for 2D-FPC basis
The main results are stated below. Theorem 1 states that as n!1 , the proposed bases fbnk(s)gk
converge to the theoretical ones up to sign almost surely, the projection of the closed linear span of the
9proposed basesfbnk(s)gkconverge to the theoretical ones almost surely, the proposed ‚Äúpre-smoothed‚Äù
basesfbS
nk(s)gkare asymptotically equal to fbnk(s)gkup to sign, and the eigenvalues fbnkgkcon-
verge to their theoretical values almost surely.
Theorem 1. Under Assumption (A3), there exists a sequence Kn!1 asn!1 , such that
(i) for each k, there exists a sign sequence fSnkgsuch that max 1kKnkSnkbnk kk;2a:s:  !0;
(ii) for any gn2 H 0with lim supn!1kgnk;2<1,kbHnKngn H 0Kngnk;2a:s:  ! 0and
kbH?
nKngn H?
0Kngnk;2a:s:  !0;
(iii)fbnk(s)gkandfbS
nk(s)gkare asymptotically equal up to sign;
(iv)maxkjbnk kja:s:  !0.
Sketch Proof of Theorem 1. To show Theorem 1, we need to show the following conclusions in se-
quence:
(a)F1andF2are Glivenko-Cantelli classes;
(b) Recall that bn1= arg max f2B1\SR
VVf(s)f(s0)Vn(s;s0)d(s)d(s0). Then9a sign sequence
fSn1:8n;Sn12f  1;1ggsuch thatkSn1bn1 1k;2a:s:  !0, asn!1 .
(c) Suppose for some 1K <1,9sign sequencesfSn1;Sn2;:::;SnK:Snk2f  1;1g;k=
1;:::;Kgnsuch that max 1kKkSnkbnk kk;2a:s:  ! 0, asn!1 , wherebnk2B 1\S,
81kKandfbn1;:::;bnKgare orthogonal inH0. Letfgng2H 0be a sequence satisfying
lim supn!1kgnk;2<1, then both
kbHnKgn H 0Kgnk;2a:s:  !0andkbH?
nKgn H?
0Kgnk;2a:s:  !0;asn!1:
(d) Assume for some 1K <1, thatfbn1;:::;bnKgform an orthonormal system on S\H 0with
max 1kKkSnkbnk kk;2a:s:  !0, asn!1 , for some sign sequences fSn1;:::;SnK:Snk2
f 1;1g;k= 1;:::;Kgn. Letbn(K+1)= arg maxf2B1\S\bH?
nKR
VVf(s)f(s0)Vn(s;s0)d(s)d(s0).
Then9a sign sequencefSn(K+1)2f  1;1ggnsuch thatkSn(K+1)bn(K+1) (K+1)k;2a:s:  !0,
asn!1 .
The detailed proof and more technical details are given in Section A in supplemental materials.
4.2. Theoretical results for individualized treatment regimes estimation
Let0= (01;02;01;02)2be the true parameter value. We require further the following
Assumptions (A9)‚Äì(A10) to develop the consistency results of the estimates:
10(A9) (Covariates) EkXk2<1. The eigenvalues of E(XX>)are bounded away from 0 and inÔ¨Ånity.
(A10) (CoefÔ¨Åcients) k01k<1,k02k<1,01;022H 0,k01k2
;2<1andk02k2
;2<1.
In what follows, Pdenotes taking the expectation over a single observation (W;Y). The following
Theorem 2 (i) gives the consistency result for bn, and Theorem 2 (ii), which is similar to results found
in Qian and Murphy (2011) but generalized to our setting, establishes asymptotic optimality of the
treatment regime estimated from the forgoing regression model.
Theorem 2. Under Assumptions (A5) ‚Äì (A10), as n!1 ,
(i)Pn
bn(W) 0(W)o2a:s:  !0;
(ii)V(opt) V(b)a:s:  !0.
Sketch Proof of Theorem 2. The conclusion in (i) follows from the facts of the convergence of bases, the
negligibility of the basis approximation error caused by the basis cutoff, and the fact that lim supkbnk
is bounded. The conclusion in (ii) now follows from Qian and Murphy (2011) directly. See the detailed
proof and more technical details in Section B in supplemental materials.
5. Implementation
This section outlines the practical implementation of the proposed procedure. We begin by dis-
cussing the selection criteria for bases numbers, followed by the implementation details of BST.
5.1. Selection Criteria for Bases Numbers
Selecting the leading principal components (PC) is a popular topic in FDA, particularly for func-
tional data with univariate indexes. To this end, two methods have gained favor in the literature: ranking
PC based on the percentage of variance explained (PVE; Kong et al., 2016), and the percentage of asso-
ciation‚Äìvariation explained (PA VE; Su et al., 2017), due to their high testing power and computational
efÔ¨Åciency. In this study, we adopt both PVE and PA VE criteria to select the leading PC bases, similar
to the univariate functional linear model setting. SpeciÔ¨Åcally, for a given threshold , we deÔ¨Åne the
eigenvaluesbnk,k= 1;:::;Knas in (2.6). Then
‚Ä¢ PVE selects the number of bases Knsuch that
PKn
k=1bnkP1
k=1bnkandPKn 1
k=1bnkP1
k=1bnk<;
‚Ä¢ PA VE selects the number of bases Knsuch that
PKn
k=1bnkb2
kPKn
k=1bnkb2
kandPKn 1
k=1bnkb2
kPKn
k=1bnkb2
k<;
11whereK
nis selected by pre-Ô¨Åtting the truncated model with a high threshold of PVE, i.e., some-
where in the range [0:95;0:99], and wherefbkgK
n
k=1are the corresponding PC basis coefÔ¨Åcients.
The threshold is usually chosen to be within [0:95;0:99]in practice. In our work, we set = 0:99
for PVE, PA VE, and the threshold of PVE in the pre-Ô¨Åtting step for PA VE.
In the following, we adopt both PVE and PA VE as selection criteria for bases numbers, denoting
the results obtained by the selection criteria PVE and PA VE with the superscripts ‚ÄúPVE‚Äù and ‚ÄúPA VE‚Äù,
respectively.
5.2. Construction of bivariate spline bases over triangulation
The proposed 2D-FPC bases are constructed using the BST as the initial basis. The construction
of BST involves the input of parameters for triangulation and spline bases. To investigate the impact of
BST on the performance of the proposed 2D-FPC bases, we conducted the simulation study in various
settings, such as coarse or Ô¨Åne grids for triangulation, and different smoothness conditions for the
splines, with larger degree d= 5 or smaller degree d= 3. The simulation results indicate that the
proposed method is robust regarding the initial bases. We brieÔ¨Çy discuss the implementation of BST in
this section.
Triangulation. Optimal triangulation involves determining both the number and shape of the tri-
angles. According to the literature (Li et al., 2021; Mu et al., 2018; Yu et al., 2021), BST performs
consistently well when an adequate number of triangles is used. In practice, it is suggested that var-
ious numbers of triangles with coarse or Ô¨Åne grids can be attempted to select the optimal number of
triangles. Once the number of triangles is determined, triangulated meshes can be constructed using
typical triangulation methods, such as Delaunay Triangulation (Bern and Eppstein, 1995), which is
implemented in the Rpackage Triangulation (Wang and Lai, 2019).
BST parameters. BST with a higher degree dis expected to provide a more accurate approximation
and requires more computational power. Throughout, we use (d;r) = (5;1)for BST, as it attains full
approximation power asymptotically (Lai and Schumaker, 2007). After the triangulation is constructed,
BST can be generated using the Rpackage BPST (Wang et al., 2019).
6. Empirical study
In this section, we investigate the Ô¨Ånite sample performance of the proposed method. The experi-
mental data is generated from the underlying model:
Yi=>
1Xi+Z
V1(s)Zi(s)ds+Ai
>
2Xi+Z
V2(s)Zi(s)ds
+"i; i= 1;:::;n:
For each subject i, we generate Xi2Rqindependently from MVN (0q;
q(r)), wheref
q(r)g`;`0=
rj` `0j, and the dependence structure in Xiis indexed by the autocorrelation parameter r; the error
12"iis independently generated from N(0;1). To simulate the within-image dependence, we generate
the imaging data Zi(s) =P2
k=1ikZ
k(s)at a grid of Ns= 4040pixels, where (i1;i2)>
MVN (02;I2), andZ
1()andZ
2()are quadratic and exponential functions, respectively, with forms
Z
1(s) = 20f(s1 0:5)2+ (s2 0:5)2gandZ
2(s) = exp[ 15f(s1 0:5)2+ (s2 0:5)2g]. The
contour plots of Z
1()andZ
2()are illustrated in Figure 6.1, left panel. Throughout, we take q= 5,
Z
1() bn1()forZ() bn1()forAZ()
Z
2() bn2()forZ() bn2()forAZ()
Figure 6.1: Contour plots of Z
j()(left panel) and the corresponding bases functions bnj()forZ()
(middle panel) and AZ()(right panel), j= 1;2.
1=2=1>
5, and the coefÔ¨Åcient maps 1(s) =2(s) = 1 , and we consider the autocorrelation r=
0andr= 0:5, which corresponds to the independent and dependent covariate structures, respectively.
We evaluate the methods on the accuracy of the estimated ITR with respect to the average marginal
mean outcome V(bPVE)andV(bPA VE), for PVE and PA VE criteria, respectively. For comparison,
we also compute the average optimal marginal mean outcome V(opt), which works as a benchmark
and is only computable in simulations. We also evaluate the methods on the accuracy of coefÔ¨Åcient
estimation by computing the mean squared errors (MSEs). All the results are based on 100 Monte
Carlo replications.
As illustrated in Figure 6.1, the generated bases functions are the normalized true function up to
a sign difference, which validates the conclusion in Theorem 1. Table 6.1 presents the marginal mean
outcome for the proposed methods. In all settings, the average marginal mean outcomes of PVE and
PA VE criteria perform similarly, and both are close to those with optimal treatment regime, opt. Table
6.2 shows the MSEs for linear coefÔ¨Åcients 1and2. With the increase of sample size, the MSEs of
all estimates decrease signiÔ¨Åcantly. And the estimates with bases selected by PVE and PA VE perform
13similarly, regardless of the settings.
Table 6.1: Estimated mean outcome for optimal and proposed method with PVE and PA VE criteria.
r= 0 r= 0:5
n= 100n= 200n= 500n= 100n= 200n= 500
V(bopt) 2.791 2.814 2.798 3.433 3.442 3.456
V(bPVE) 2.763 2.808 2.804 3.431 3.455 3.459
V(bPA VE) 2.763 2.808 2.804 3.431 3.455 3.459
Table 6.2: MSEs for linear coefÔ¨Åcients 1and2.
r n CriteriaMSE of1(10 2) MSE of 2(10 2)
11121314152122232425
0 100 PVE 2.53 1.29 1.30 1.19 1.42 6.85 1.08 1.35 1.31 1.47
PA VE 2.53 1.29 1.30 1.19 1.42 6.85 1.08 1.35 1.31 1.47
200 PVE 1.13 0.65 0.68 0.47 0.60 3.73 0.54 0.54 0.58 0.61
PA VE 1.13 0.65 0.68 0.47 0.60 3.73 0.54 0.54 0.58 0.61
500 PVE 0.46 0.19 0.20 0.22 0.22 1.48 0.23 0.21 0.17 0.20
PA VE 0.46 0.19 0.20 0.22 0.22 1.48 0.23 0.21 0.17 0.20
0.5 100 PVE 2.53 2.01 2.56 1.89 1.27 6.85 1.71 2.01 1.79 1.89
PA VE 2.53 2.01 2.56 1.89 1.27 6.85 1.70 2.01 1.79 1.89
200 PVE 1.13 0.67 1.03 1.03 1.02 3.73 0.67 0.78 1.08 0.78
PA VE 1.13 0.67 1.03 1.03 1.02 3.73 0.67 0.78 1.08 0.78
500 PVE 0.46 0.29 0.31 0.34 0.25 1.48 0.28 0.32 0.39 0.27
PA VE 0.46 0.29 0.31 0.34 0.25 1.48 0.28 0.32 0.39 0.27
7. Application to ADNI data
The data that drives our research comes from the large neuroimaging datasets in the Alzheimer‚Äôs
Disease Neuroimaging Initiative (ADNI, http://adni.loni.usc.edu ). The longitudinal cohort
study in ADNI, which has gone through three phases including ADNI1, ADNI GO, and ADNI2, is a
comprehensive neuroimaging study that collected a variety of necessary phenotypic measures, includ-
ing structural, functional, and molecular neuroimaging, biomarkers, clinical and neuropsychological
variables, and genomic information (Petersen et al., 2010; Weiner and Veitch, 2015). These data pro-
vide unprecedented resources for statistical methods development and scientiÔ¨Åc discovery.
We now analyze the records from 441 participants through the ADNI1 and ADNI GO phases. The
data contains the following variables:
‚Ä¢ Mini-mental state examination (MMSE) scores: response variable, ranging from 15 to 30, where
lower values indicate a more severe AD status.
14‚Ä¢ Fludeoxyglucose positron emission tomography (PET) scans: neuroimaging representing brain
metabolism activity level and can be used to make early diagnoses of AD, with 7995pixels,
with the measurements ranging from 0.013 to 2.149. The left panel in Figure 7.3 shows the PET
images for four randomly selected subjects.
‚Ä¢ Age: the participants‚Äô ages, ranging from 55 to 89 years.
‚Ä¢ Education: the participants‚Äô educational status, ranging from 4 to 20 years.
‚Ä¢ Gender: the participants‚Äô gender, with 169 female and 278 male. We created a dummy variable
with value 1 representing female and 0 for male.
‚Ä¢ Ethnicity: the participants‚Äô ethnic categories, with 12 Hispanic/Latino, 429 not Hispanic/Latino,
and 6 unknown. We created a dummy variable with value 1 representing Hispanic/Latino and 0
for others.
‚Ä¢ Race: the participants‚Äô racial categories, with 1 Indian/Alaskan, 7 Asians, 24 Blacks, 413 Whites,
and 2 more than one category. We created a dummy variable with value 1 representing white and
0 for others.
‚Ä¢ Marriage: the participants‚Äô marital status, with 35 divorced, 344 married, 12 never married, and
56 widowed. We created a dummy variable with value 1 representing married and 0 for others.
‚Ä¢ Apolipoprotein (APOE) gene: the number of copies of APOE4 gene, the most prevalent genetic
risk factor for AD (Ashford and Mortimer, 2002), ranging from 0 to 2. We created two dummy
variables, APOE1 and APOE2, to denote those with one and two copies of APOE4 gene, respec-
tively.
‚Ä¢ Treatment: During the ADNI1 and ADNI GO study periods, the US FDA-approved therapies
for AD symptoms included cholinesterase inhibitors and the NMDA-partial receptor antagonist
memantine. Cholinesterase inhibitors, including donepezil, galantamine, and rivastigmine, are
prescribed for mild-to-moderate-stage AD. Memantine is prescribed for the treatment of AD
either as monotherapy or in combination with one of the cholinesterase inhibitors for moderate-
to-severe stage AD (Schneider et al., 2011). We denote by A= 1 those participants taking one
or more combinations of Donepezil (Aricept), Galantamine (Razadyne), Rivastigmine (Exelon),
and Memantine (Namenda), while we use A= 0 for those wihout concurrent medical records,
or taking some other treatments or supplements. The distribution of participants with A= 1 is
illstruated in Figure 7.1.
We apply the proposed method to those data using both PVE and PA VE criteria. The estimated
coefÔ¨Åcient map for 1()and2()are shown in the top line of Figure 7.2, in which PVE and PA VE
15Figure 7.1: Distribution of patients by treatments (total sample size 441, with 247 A= 1).
criteria produce similar results, and the estimates of 2()illustrate brain structures. We also display
visually the three leading principal component basis maps in the middle and bottom lines of Figure 7.2.
All these estimated basis maps illustrate brain structures.
Tables 7.1 presents the estimated coefÔ¨Åcients for the nonfunctional predictors, along with the cor-
responding 95% bootstrap conÔ¨Ådence intervals. The main effect of the treatment can improve the
performance in MMSE by around 46units on average. With the increase of age, the general MMSE
scores will decrease; and with the increase of education level, the MMSE scores also increase in gen-
eral: both are supported by studies on cognitive reserve in aging and AD (Fratiglioni et al., 2007; Stern,
2012). As for the well-known risk genetic factor APOE gene, with more copies of epsilon 4 alleles in
the APOE gene, the MMSE scores decrease, which means a higher risk for the onset of AD and agrees
with other current studies (Schneider et al., 2011). The difference between females and males is not
signiÔ¨Åcant.
When one patient enters into the database with the imaging feature and other features, we can
utilize the estimated coefÔ¨Åcients to provide the optimal treatment regime based on Equation (3.5). To
be speciÔ¨Åc, for the imaging feature, for Subjects 27, 48, 55, and 160, the process can be visualized as
depicted in the following Figure 7.3. The estimated optimal ITRs for these four subjects are A= 1,
A= 0,A= 0, andA= 1, respectively. The differences in the images are subtle, note, for example,
that the blue and orange parts are a little darker for those assigned A= 0versus those assigned A= 1.
16bPVE
1() bPA VE
1() bPVE
2() bPA VE
2()
bn1()forZ()bn2()forZ()bn3()forZ()
bn1()forAZ()bn2()forAZ()bn3()forAZ()
Figure 7.2: Top: Estimated coefÔ¨Åcient maps for 1()and2()using PVE and PA VE criteria, respec-
tively. Middle: The three leading PC basis maps (top three ranked by both PVE and PA VE) for Z().
Bottom: The three leading PC basis maps (top three ranked by both PVE and PA VE) for AZ().
17Table 7.1: Estimated coefÔ¨Åcients and 95% bootstrap conÔ¨Ådence intervals for the linear covariates using
PVE and PA VE criteria, respectively.
PVE PA VE
Term Estimate 95% Bootstrap CI Estimate 95% Bootstrap CI
Intercept 20.0650 (8.655, 31.93) 19.8176 (8.663, 31.562)
Age -0.0560 (-0.113, -0.004) -0.0568 (-0.111, -0.005)
Education 0.2225 (0.137, 0.340) 0.2196 (0.137, 0.334)
Gender -0.1998 (-0.909, 0.707) -0.1811 (-0.896, 0.643)
APOE1 -0.3260 (-1.079, 0.318) -0.3201 (-1.080, 0.272)
APOE2 -0.9185 (-2.059, 0.321) -0.9003 (-2.022, 0.333)
Ethinicity 0.2363 (-1.513, 2.580) 0.2427 (-1.536, 2.451)
Race 1.2455 (0.256, 2.664) 1.3468 (0.312, 2.527)
Marriage 0.1760 (-0.711, 0.837) 0.1451 (-0.703, 0.801)
Treatment 4.1362 (-6.893, 15.867) 6.0930 (-6.759, 12.850)
TreatmentAge -0.0594 (-0.107, -0.006) -0.0643 (-0.107, -0.014)
TreatmentEducation 0.0001 (-0.104, 0.076) 0.0056 (-0.102, 0.072)
TreatmentGender -0.0302 (-0.711, 0.704) -0.0279 (-0.620, 0.710)
TreatmentAPOE1 0.4237 (-0.312, 1.089) 0.4050 (-0.298, 1.036)
TreatmentAPOE2 -0.7102 (-1.767, 0.576) -0.8079 (-1.764, 0.508)
TreatmentEthinicity -0.0115 (-2.067, 2.216) 0.0661 (-1.957, 2.263)
TreatmentRace 1.3543 (0.368, 2.536) 1.3859 (0.381, 2.477)
TreatmentMarriage -0.2662 (-0.998, 0.552) -0.3088 (-0.963, 0.524)
18Zi(s)bPVE
2(s)!Zi(s)bPVE
2(s)!R
VZi(s)bPVE
2(s)d(s)
Subject
27
-1.36
Subject
48
-1.68
Subject
160
-1.66
Subject
433
-1.61
Figure 7.3: An illustration of procedures for dealing with imaging features to obtain ITRs. The left
panel represents the imaging features of four randomly selected subjects, Subjects 27,48,160, and 433,
with MMSE scores of 30,27,15, and 23, respectively. The multiplication of the imaging feature Zi(s)
and the coefÔ¨Åcient map bPVE
2(s), as shown in the third column, represents the data-driven summary
of the imaging features, which differs in the activity level in the blue region and orange region in the
bottom. The integral of the multiplication gives the image contribution of the interaction term; adding
Xband taking the positivity indicator will result in the optimal ITR.
198. Discussion
We proposed semiparametric functional learning with imaging features to estimate the optimal
ITRs. Overall, the proposed approach can efÔ¨Åciently and precisely estimate the optimal treatment
regime with abundant features. It can overcome the ‚Äúleakage‚Äù problem in handling the complex do-
main of imaging data and is also computationally efÔ¨Åcient. This approach provides an efÔ¨Åcient and
powerful tool to estimate optimal treatment regimes by incorporating abundant features into the preci-
sion medicine framework.
There are some natural extensions based on the proposed work. One extension is to extend the
current regression model to a generalized regression model, in which the response is allowed to be
discrete or categorical. For example, for the study of Alzheimer‚Äôs disease, one could take the disease
stage as the response. Another extension could be to generalize the current single-decision setting to
the multi-stage decision setting, which is more common in chronic disease settings such as Alzheimer‚Äôs
disease.
Acknowledgements
Research reported in this publication was supported by the National Institute Of General Medical
Sciences of the National Institutes of Health under Award Number P20GM139769 (Xinyi Li), National
Science Foundation awards DMS-2210658 (Xinyi Li) and DMS-2210659 (Michael Kosorok). The
content is solely the responsibility of the authors and does not necessarily represent the ofÔ¨Åcial views
of the National Institutes of Health. The investigators within the ADNI contributed to the design and
implementation of ADNI and/or provided data but did not participate in analysis or writing of this
report. A complete listing of ADNI investigators can be found at http://adni.loni.usc.edu/
wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf .
20Supplemental Materials for ‚ÄúFunctional Individualized Treatment Regimes with Imaging Features‚Äù
In this document, we provide the technical details of the conclusions presented in the main paper.
SpeciÔ¨Åcally, we give detailed proofs of Theorems 1 and 2 in the paper, and present a number of technical
lemmas and other supporting results used in the proofs.
A. Proof of 2D-FPC Bases Properties
We Ô¨Årst develop the properties of the proposed 2D-FPC bases. We generally follow the
sketch proof of Theorem 1 in Section 4.1 in the paper, and provide a detailed proof for each
conclusion, with corresponding supporting lemmas.
Lemma A.1.For anyb(s)2H?
0,VarfR
Vb(s)Z(s)d(s)g= 0.
Proof. For anyb(s)2H?
0,
VarZ
Vb(s)Z(s)d(s)
= EZ
Vb(s)fZ(s) EZ(s)gd(s)2
= E(1X
k=1kZ
Vb(s)k(s)d(s))2
= 0:
Remark A.1. From Lemma A.1, for any linear functionalR
Vb(s)Z(s)d(s), we can assume
without loss of generality that b2H 0.
In the following, we denote the theoretical and empirical expectation operators as Pand
Pn, respectively. The following Theorem A.1 shows that both F1andF2are Glivenko-Cantelli
classes, which can be further applied for consistency proofs.
Theorem A.1.Suppose Assumption (A3) holds. Then the following two conclusions hold:
(i)F1is a Glivenko-Cantelli class with envelope
F1(Z) = 1X
k=1Z
Vk(s)fZ(s) EZ(s)gd(s)2!1=2
; (A.1)
such thatF1kZ EZk;2andEF2
1<1.
(ii)F2is also aP-Glivenko-Cantelli class with envelope F2
1, where EF2
1<1.
Proof. (i) By Assumption (A3), Ô¨Åx " >0,9K <1, such thatP1
k=K+1k"2. Since
B1H 0,b2B 1has a unique Fourier representation b(s) =P1
k=1akk(s), where
21R
Vb2(s)d(s) =P1
k=1a2
k1by deÔ¨Ånition ofB1and projection of a basis. Then,
letting`2=fa:kak21g , we can writeF1as
F1=(1X
k=1akZ
Vk(s)fZ(s) EZ(s)gd(s) :a2`2andkak21)
:
LetA1=fa2`2andkak21gand recall Zn(s) =n 1Pn
i=1Zi(s). Thus,8f2F 1,
sup
f2F1kPnf Pfk= sup
a2A 11X
k=1akZ
Vk(s)fZn(s) EZ(s)gd(s)
sup
a2A 11X
k=K+1akZ
Vk(s)fZn(s) EZ(s)gd(s)+ sup
a2A 1KX
k=1akZ
Vk(s)fZn(s) EZ(s)gd(s)
An+Bn:
By the Cauchy-Schwarz inequality,
An= sup
a2A 1n 1nX
i=11X
k=K+1akZ
Vk(s)fZi(s) EZ(s)gd(s)
sup
a2A 1n 1nX
i=1 1X
k=K+1a2
k!1=2 1X
k=K+1Z
Vk(s)fZi(s) EZ(s)gd(s)2!1=2
n 1nX
i=1 1X
k=K+1Z
Vk(s)fZi(s) EZ(s)gd(s)2!1=2
:
LetUi= (P1
k=K+1[R
Vk(s)fZi(s) EZ(s)gd(s)]2)1=2. Note that (EUi)2EU2
i=P1
k=K+1k. Since this is an iid sum, we have that lim supn!1An= lim supn!1n 1Pn
i=1Ui
"almost surely. LetA2=fa2RKandkak21g. Then
Bn= sup
a2A 2KX
k=1akZ
Vk(s)fZn(s) EZ(s)gd(s):
SinceA2is a compact ball,9Ô¨Ånite subsetM2A 2, such that supa2A 2infea2M 2ka 
eak". Accordingly,
Bnmax
ea2M 2KX
k=1eakZ
Vk(s)fZn(s) EZ(s)gd(s)
+ sup
a2A 2;ea2M 2:ka eak"KX
k=1(ak eak)Z
Vk(s)fZn(s) EZ(s)gd(s)
B1n+B2n:
22By the standard strong law of large numbers, B1na:s:  !0asn!1 . As forB2n, by the
Cauchy-Schwarz inequality,
sup
a2A 2;ea2M 2:ka eak"KX
k=1(ak eak)Z
Vk(s)fZn(s) EZ(s)gd(s)
= sup
a2A 2;ea2M 2:ka eak"n 1nX
i=1KX
k=1(ak eak)Z
Vk(s)fZi(s) EZ(s)gd(s)
"n 1nX
i=1 KX
k=1Z
Vk(s)fZi(s) EZ(s)gd(s)2!1=2
"n 1nX
i=11X
k=1Z
Vk(s)fZi(s) EZ(s)gd(s)21=2
:
LetVi=P1
k=1R
Vk(s)fZi(s) EZ(s)gd(s)2. Note thatVidoes not depend on ",
and
EVi=1X
k=1k<1) lim sup
n!1B2n"C0a:s:;
whereC0=jP1
k=1kj1=2does not depend on ". Therefore, supf2F1kPnf Pfka:s:  !0,
thus,F1is aP-Glivenko-Cantelli class by deÔ¨Ånition. Also note that F1is enveloped by
F1(Z) = 1X
k=1Z
Vk(s)fZ(s) EZ(s)gd(s)2!1=2
 1X
k=1Z
V2
k(s)d(s)Z
VfZ(s) EZ(s)g2d(s)!1=2
=kZ EZk;2;
and
EF2
1(Z)E 1X
k=1Z
Vk(s)fZi(s) EZ(s)gd(s)2!
=1X
k=1k<1:
(ii) By (i) and Corollary 9.27 of Kosorok (2008), we can conclude that F2is also aP-
Glivenko-Cantelli class, with envelope F2
1.
Given the conclusions in Theorem A.1, we can therefore develop the convergence results
for the basesfbnkgkthrough the following Theorems A.2 ‚Äì A.4.
23Theorem A.2.Letbn1= arg max f2B1\SR
VVf(s)f(s0)Vn(s;s0)d(s)d(s0). Suppose As-
sumption (A3) holds. Then 9a sign sequencefSn1:8n;Sn12f  1;1ggsuch that
kSn1bn1 1k;2a:s:  !0;asn!1:
Proof of Theorem A.2.Let
B1n= sup
f2B1Z
VVf(s)f(s0)fVn(s;s0) V0(s;s0)gd(s)d(s0): (A.2)
By Theorem A.1 (ii), B1na:s:  !0asn!1 . Then
Z
VVbn1(s)bn1(s0)Vn(s;s0)d(s)d(s0)Z
VVbn1(s)bn1(s0)V0(s;s0)d(s)d(s0) +B1n
sup
f2B1Z
VVf(s)f(s0)V0(s;s0)d(s)d(s0) +B1na:s:  !1;asn!1:
Leten1(s)be the projection of 1(s)ontoB1\S. Then on the other side, as n!1 ,
Z
VVbn1(s)bn1(s0)Vn(s;s0)d(s)d(s0)Z
VVen1(s)en1(s0)Vn(s;s0)d(s)d(s0)
Z
VVen1(s)en1(s0)V0(s;s0)d(s)d(s0) B1na:s:  !Z
VV1(s)1(s0)V0(s;s0)d(s)d(s0) =1:
Therefore, if we deÔ¨Åne bank=R
Vbn1(s)k(s)d(s), we can conclude that as n!1 ,
Z
VVbn1(s)bn1(s0)V0(s;s0)d(s)d(s0) =Z
VV1X
k=1bn1(s)bn1(s0)kk(s)k(s0)d(s)d(s0)
=1X
k=1ba2
nkka:s:  !1:
Note thatP1
k=1ba2
nk1sincebn12B 1, and since1>  1>  2> ::: : this now leads to
ba2
n1a:s:  !1andP1
j=2ba2
nka:s:  !0. Hence there exists a sign sequence fSn12f  1;1ggnsuch
thatkSn1bn1 1k;2a:s:  !0asn!1 .
Theorem A.3.Suppose for some 1K <1,9sign sequencesfSn1;Sn2;:::;SnK:Snk2
f 1;1g;k= 1;:::;Kgnsuch that
max
1kKkSnkbnk kk;2a:s:  !0;asn!1;
wherebnk2B 1\S,81kKandfbn1;:::;bnKgare orthogonal inH0. Recall the
deÔ¨Ånitions ofH0KandbHnKin (2.5), and let bH?
nKandH?
0Kdenote the respective closed
24orthocomplements in H0. Letfgng2H 0be a sequence satisfying lim supn!1kgnk;2<1.
Suppose Assumption (A3) holds, then both
kbHnKgn H 0Kgnk;2a:s:  !0andkbH?
nKgn H?
0Kgnk;2a:s:  !0;asn!1:
Proof of Theorem A.3.
bHnKgn(s) =KX
k=1bnk(s)Z
Vbnk(s0)gn(s0)d(s0) =KX
k=1Snkbnk(s)Z
VSnkbnk(s0)gn(s0)d(s0)
=H0Kgn(s) +KX
k=1fSnkbnk(s) k(s)gZ
Vk(s0)gn(s0)d(s0)
+KX
k=1Snkbnk(s)Z
VfSnkbnk(s0) k(s0)ggn(s0)d(s0)
=H0Kgn(s) +F1n(s) +F2n(s):
Note that as n!1 ,
kF1nk;2KX
k=1kSnkbnk kk;2kgnk;2a:s:  !0;kF2nk;2KX
k=1kSnkbnk kk;2kgnk;2a:s:  !0:
Thus,kbHnKgn H 0Kgnk;2a:s:  !0, asn!1 . Since
kbH?
nKgn H?
0Kgnk;2=k(I bHnK)gn (I H 0K)gnk;2=kbHnKgn H 0Kgnk;2;
where Iis the identity operator, we now have the second conclusion of the theorem.
Theorem A.4.Suppose Assumption (A3) holds. Assume for some 1K <1, that
fbn1;:::;bnKgform an orthonormal system on S\H 0with
max
1kKkSnkbnk kk;2a:s:  !0;asn!1;
for some sign sequences fSn1;:::;SnK:Snk2f  1;1g;k= 1;:::;Kgn. Let
bn(K+1)= arg max
f2B1\S\bH?
nKZ
VVf(s)f(s0)Vn(s;s0)d(s)d(s0):
Then9a sign sequencefSn(K+1)2f  1;1ggnsuch that
kSn(K+1)bn(K+1) (K+1)k;2a:s:  !0;asn!1:
25Proof of Theorem A.4.LetV0K(s;s0) =P1
k=K+1kk(s)k(s0). Recall the deÔ¨Ånition of
B1ngiven in (A.2) and B1na:s:  !0as shown in Theorem A.2. Again by Theorem A.1 (ii), we
have asn!1 ,
Z
VVbn(K+1)(s)bn(K+1)(s0)Vn(s;s0)d(s)d(s0) (A.3)
Z
VVbn(K+1)(s)bn(K+1)(s0)V0(s;s0)d(s)d(s0) +B1n
Z
VVbn(K+1)(s)bn(K+1)(s0)V0K(s;s0)d(s)d(s0) +B1n+B2n;
sup
f2B1Z
VVf(s)f(s0)V0K(s;s0)d(s)d(s0) +B1n+B2na:s:  !K+1;
whereB2n=jPK
k=1kfR
Vbn(K+1)(s)k(s)d(s)g2ja:s:  !0by Theorem A.3. Let en(K+1)=
B1SK+1, then
(A:3)Z
VVen(K+1)(s)en(K+1)(s0)Vn(s;s0)d(s)d(s0)
Z
VVen(K+1)(s)en(K+1)(s0)V0(s;s0)d(s)d(s0) B1na:s:  !K+1;asn!1:
Thus,
Z
VVbn(K+1)(s)bn(K+1)(s0)V0K(s;s0)d(s)d(s0)a:s:  !K+1;asn!1: (A.4)
Letbank=R
Vbn(K+1)(s)k(s)d(s). Using arguments similar to those in the proof of The-
orem A.2, sinceP1
k=1ba2
nk1and by (A:4), we haveP1
k=K+1ba2
nkka:s:  !K+1, which
forcesba2
n(K+1)a:s:  !1andPK
k=1ba2
nk+P1
k=K+2ba2
nka:s:  !0. Then there exists a sign sequence
fSn(K+1)2f  1;1ggnsuch thatkSn(K+1)bn(K+1) K+1k;2a:s:  !0asn!1 .
Proof of Theorem 1. Based on these theorems and lemmas introduced above, we are ready to
give the detailed proof of Theorem 1.
(i) The conclusion of (i) follows by Theorems A.2 ‚Äì A.4 directly.
(ii) The conclusion of (ii) has been shown in Theorem A.3.
(iii) It is obvious that for 1kKn^pn,
Z
VVbnk(s)bnk(s0)Vn(s;s0)d(s)d(s0)Z
VVbS
nk(s)bS
nk(s0)Vn(s;s0)d(s)d(s0):
26On the other side, for any f2B 1\S,9a2RJn\A 1, such thatf(s) =a>H 1=2B(s).
Thus
Z
VVf(s)f(s0)Vn(s;s0)d(s)d(s0)
=a>H 1=2Z
VVB(s)Vn(s;s0)B>(s0)d(s)d(s0)H 1=2a
=a>Knab'>
n1Knb'n1=Z
VVbS
n1(s)bS
n1(s0)Vn(s;s0)d(s)d(s0):
We therefore obtain bS
n1(s) = arg maxf2B1\SR
VVf(s)f(s0)Vn(s;s0)d(s)d(s0)by
taking the supremum over B1\S. Thus,bn1(s)is equal tobS
n1(s)up to sign. Suppose
8k<Kn^pn, we havebn(k 1)(s) =bS
n(k 1)(s). Similarly, we can obtain
bS
nk(s) = arg max
f2B1\S\bH?
n(k 1)Z
VVf(s)f(s0)Vn(s;s0)d(s)d(s0)
by noticing that b'nk= arg maxa2RJn\A 1a>bKnka, where
bKnk=H 1=2Z
VVB(s)X
k0kbnk0bnk(s)bnk(s0)B>(s0)d(s)d(s0)H 1=2:
Hence, for 1kKn^pn,bnk(s)andbS
nk(s)are equal up to sign.
(iv) By the deÔ¨Ånitions of bnkandk, for1kKn^pn, we have
bnk k=Z
VVn
bnk(s)bnk(s0)Vn(s;s0) k(s)k(s0)V0(s;s0)o
d(s)d(s0)
Z
VVbnk(s)bnk(s0)fVn(s;s0) V0(s;s0)gd(s)d(s0)
+Z
VVn
bnk(s)bnk(s0) k(s)k(s0)o
V0(s;s0)d(s)d(s0):
From the proofs of Theorem A.1 and Theorem A.4, as n!1 ,
Z
VVbnk(s)bnk(s0)fVn(s;s0) V0(s;s0)gd(s)d(s0)a:s:  !0:
SinceP1
k=1k<1and1> 2>:::0,R
VVV2
0(s;s0)d(s)d(s0) =P1
k=12
k<
271. Thus,
Z
VVn
Snkbnk(s)Snkbnk(s0) k(s)k(s0)o
V0(s;s0)d(s)d(s0)
Z
VVSnkbnk(s)n
Snkbnk(s0) k(s0)o
V0(s;s0)d(s)d(s0)
+Z
VVk(s0)n
Snkbnk(s) k(s)o
V0(s;s0)d(s)d(s0)
kSnkbnkk;2kSnkfnj kk;2Z
VVV2
0(s;s0)d(s)d(s0)1=2
+kkk;2kSnkfnj kk;2Z
VVV2
0(s;s0)d(s)d(s0)1=2
2kSnkfnj kk;2Z
VVV2
0(s;s0)d(s)d(s0)1=2
!0
almost sure as n!1 . The conclusion follows.
B. Proof of Convergence of Estimators
In this section, we develop the convergence results of the coefÔ¨Åcient estimator based on
the proposed 2D-FPC bases. Following the sketch proof of Theorem 2 in Section 4.2 in the
paper, we provide the supporting lemmas and the detailed proof.
We Ô¨Årst deÔ¨Åne those coefÔ¨Åcients and operators that will be employed in the proof. Recall
thate= (>
1;>
2;>
1;>
2)>2eK1;K2, where`2Rq,`= 1;2, are the linear coefÔ¨Åcients,
and`2RK`,`= 1;2, are the basis coefÔ¨Åcients. Let en= arg min e2eK1;K2n 1Pn
i=1fYi 
e(cWi)g2. We writeen= (e1n;e2n;e1;e2), withe1= (e1;1;:::;e1;K1)>ande2= (e2;1;:::;e2;K2)>,
then
bn= 
e1n;e2n;K1X
k=1e1;kb1;nk(s);K2X
k=1e2;kb2;nk(s)!>
:
Recall that0= (01;02;01;02)2is the true parameter value. Let 0`;k=R
V`;k(s)0`(s)d(s),
and thus0`(s) =P1
k=10`;k`;k(s),`= 1;2, wheref1;k(s)gkandf2;k(s)gkare theoretical
orthonormal bases for Z(s)andAZ(s), respectively. DeÔ¨Åne
0n= 
>
01;>
02;K1X
k=101;kS1;nk1;k(s);K2X
k=102;kS2;nk2;k(s)!>
;
28and
e0n= 
>
01;>
02;(01;1;:::; 01;K1);(02;1;:::; 02;K2)>:
We Ô¨Årst explore the properties of the truncated terms. Let
0n(Wi) =1X
k=K1+101;kZ
VSnkk(s)Zi(s)d(s) +1X
k=K2+102;kZ
VSnkk(s)AiZi(s)d(s):
(B.1)
We use the following Lemma B.1 to state that the truncated terms are negligible.
Lemma B.1.Under Assumptions (A3) ‚Äì (A8), as n!1 ,n 1Pn
i=12
0n(Wi)!0.
Proof. Decompose 0n(Wi) =I1+I2, where
I1=1X
k=K1+101;kZ
VSnkk(s)Zi(s)d(s); I 2=1X
k=K2+102;kZ
VSnkk(s)AiZi(s)d(s):
We have
EI2
1=1X
k=K1+11X
k0=K1+101;k01;k0Z
VVSnkSnk0k(s)k0(s0)fEZi(s)Zi(s0)gd(s)d(s0)
=1X
k=K1+11X
k0=K1+101;k01;k0Z
VVSnkSnk0k(s)k0(s0)(1X
k00=1k00k00(s)k00(s0))
d(s)d(s0)
=1X
k=K1+11X
k0=K1+11X
k00=101;k01;k0k00SnkSnk0Z
Vk(s)k00(s)d(s)Z
Vk0(s0)k00(s0)d(s0)
=1X
k=K1+12
01;kk:
Similarly, EI2
2P1
k=K2+12
02;kk, and EI1I2P1
k=K1_K2+101;k02;kk. Then asn!1 ,
n 1nX
i=12
0n(Wi)!E2
0n(W)2 1X
k=K1+12
01;kk+1X
k=K2+12
02;kk!
2 1X
k=K1+12
01;k+1X
k=K2+12
02;k! 1X
k=K1^K2+1k!
E
1n(K1;K2):
Thus, we have1>E
1n(K1;K2)!0asK1^K2!1 .
Next, we use the following Lemma B.2 to state that the errors induced by the approxima-
tion offbnkgtofkgare negligible.
29Lemma B.2.Under Assumptions (A3) ‚Äì (A8), as n!1 ,n 1Pn
i=1f0n(Wi) e0n(cWi)g2a:s:  !
0.
Proof. By the deÔ¨Ånitions of 0n(Wi)ande0n(cWi),
0n(Wi) e0n(cWi) =K1X
k=101;kZ
Vn
Snkk(s) bnk(s)o
Zi(s)d(s)
+K2X
k=102;kZ
Vn
Snkk(s) bnk(s)o
AiZi(s)d(s):
Soj0n(Wi) b0n(cWi)j PK1
k=101;kkSnkk bnkk;2kZik;2+PK2
k=102;kkSnkk 
bnkk;2kZik;2sincejAij1. By Theorem 1, there exist a sequence frng, such thatrnkSnkk 
bnkk;2!0. TakeK1_K2=r 1=2
n, asn!1 ,
n 1nX
i=1n
0n(Wi) e0n(cWi)o2
 K1_K2X
j=12
01;k+2
02;k! 
2K1_K2X
j=1kSnkk bnkk2
;2!
n 1nX
i=1kZik2
;2a:s:  !0:
Combining Lemmas B.1 and B.2, we use the following Lemma B.3 to illuminate the
convergence rate of e0nto the observed Yi.
Lemma B.3.Under Assumptions (A3) ‚Äì (A8), as n!1 ,n 1Pn
i=1fYi e0n(cWi)g2a:s:  !C1
for some constant C1.
Proof. By deÔ¨Ånition,
n 1nX
i=1n
Yi e0n(cWi)o2
=n 1nX
i=1n
0(Wi) +"i e0n(cWi)o2
=n 1nX
i=1n
"i+  0n(Wi) +0n(Wi) e0n(cWi)o2
a:s:;
where 0n(Wi)is given in (B.1). Since n 1Pn
i=1"2
i!Var(") =2<1, combined with
Lemmas B.1 and B.2, we have
n 1nX
i=1n
Yi e0n(cWi)o2a:s:  !C1<1 (B.2)
for some constant C1.
30Note thatn 1Pn
i=1fYi en(cWi)g2n 1Pn
i=1fYi e0n(cWi)g2:Consequently,
 2
nnX
i=1n
Yi e0n(cWi)on
en(cWi) e0n(cWi)o
+1
nnX
i=1n
en(cWi) e0n(cWi)o2
0:
(B.3)
Leten=en e0nandern=en=kenk2. We use the following Lemma B.4 to build the
boundedness of en.
Lemma B.4.Under Assumptions (A3) ‚Äì (A10), lim supn!1kenk2C2<1almost surely
for some constant C2.
Proof.
(B:3))kenk2
2
1 +kenk21
nnX
i=1fern(cWi)g22
nnX
i=1n
Yi e0n(cWi)o
ern(cWi)
2"
1
nnX
i=1n
Yi e0n(cWi)o2#1=2"
1
nnX
i=1n
ern(cWi)o2#1=2
;
)kenk2
2
1 +kenk22
1
nPn
i=1n
Yi e0n(cWi)o21=2

1
nPn
i=1n
ern(cWi)o21=2: (B.4)
By Lemma B.3, the numerator on the right-hand-side (RHS) of (B.4)a:s:  !2C1=2
1asn!1 .
As for the denominator on the RHS of (B:4), we Ô¨Årst deÔ¨Åne
bDn=1
nnX
i=1cWicW>
i=0
BBB@bD11bD12bD13bD14
bD>
12bD22bD23bD24
bD>
13bD>
23bD33bD34
bD>
14bD>
24bD>
34bD441
CCCA;
where for each entry of bDn,
bD11=n 1Pn
i=1XiX>
i, bD12=bD22=n 1Pn
i=1AiXiX>
i,
bD13=n 1Pn
i=1XibU>
1;i(K1),bD14=bD24=n 1Pn
i=1AiXibU>
2;i(K2),
bD23=n 1Pn
i=1AiXibU>
1;i(K1),bD33=n 1Pn
i=1bU1;i(K1)bU>
1;i(K1),
bD34=n 1Pn
i=1AibU1;i(K1)bU>
2;i(K2),bD44=n 1Pn
i=1AibU2;i(K2)bU>
2;i(K2).
Then we have n 1Pn
i=1fern(cWi)g2=er>
nbDnern.
31In a parallel fashion to cWiandbDn, fori= 1;:::;n , letfWi= (X>
i;AiX>
i;eU>
1;i(K1);AieU>
2;i(K2))>,
whereeU`;i(K`) = (eU`;i1;:::;eU`;iK`)>,`= 1;2, and
eU1;ik=Z
VS1;nk1;k(s)Zi(s)d(s);1kK1;
eU2;ik=Z
VS2;nk2;k(s)AiZi(s)d(s);1kK2:
Then deÔ¨Åne
D= EfWifW>
i=0
BBB@D11D12D13D14
D>
12D22D23D24
D>
13D>
23D33D34
D>
14D>
24D>
34D441
CCCA;
where for each entry of D,
D11= E( XiX>
i);D12=D22= E(AiXiX>
i);
D13= EfXieU>
1;i(K1)g=Z
VEn
XiZi(s)
S1;n11;1(s);:::;S 1;nK 11;K1(s)o
d(s);
D14=D24= EfAiXieU>
2;i(K2)g
=Z
VEh
AiXifAiZi(s)g
S2;n12;1(s);:::;S 2;nK 22;K2(s)i
d(s);
D23= EfAiXieU>
1;i(K1)g=Z
VEn
AiXiZi(s)
S1;n11;1(s);:::;S 1;nK 11;K1(s)o
d(s);
D33= EfeU1;i(K1)eU>
1;i(K1)g=Z
VVEfZi(s)Zi(s0)
0
B@S1;n11;1(s)
...
S1;nK 11;K1(s)1
CA
S1;n11;1(s0);:::;S 1;nK 11;K1(s0)9
>=
>;d(s)d(s0);
D34= EfAieU1;i(K1)eU>
2;i(K2)g=Z
VVE [AifZi(s)gfAiZi(s0)g
0
B@S1;n11;1(s)
...
S1;nK 11;K1(s)1
CA
S2;n12;1(s0);:::;S 2;nK 22;K2(s0)3
75d(s)d(s0);
32D44= EfAieU2;i(K2)eU>
2;i(K2)g=Z
VVE [AifAiZi(s)gfAiZi(s0)g
0
B@S2;n12;1(s)
...
S2;nK 22;K2(s)1
CA
S2;n12;1(s0);:::;S 2;nK 22;K2(s0)3
75d(s)d(s0):
By strong law of large numbers and Theorem A.1, we can recycle previous arguments to
verify thatkbDn Dk1a:s:  !0. From Assumptions (A3) and (A9), we know that the minimum
eigenvalue of Dc1for some constant c1>0. Hence
lim inf
n!11
nnX
i=1n
ern(cWi)o2
c1>0;a:s:
LetC2= 2C1=2
1c 1
1, then the lim supn!1of the RHS of (B.4) C2<1.
Now, ifa2
1+akfor alla0and somek<1, thenak+p
k(follows from algebra).
Thus,
lim sup
n!1kenk2C2+p
C2<1:
The following Theorem B.1 gives the tightness of bn.
Theorem B.1. Under Assumptions (A5) ‚Äì (A10), for Ô¨Åxed 1K1;K2<1,9C<1, such
that
lim sup
n!1kbnkCa:s: (B.5)
Proof of Theorem B.1. The conclusion follows directly from Lemmas B.2 and B.4, and the
isomorphism between bK1;K2andeK1;K2.
Lemma B.5.Under Assumptions (A3) ‚Äì (A10), as n!1 ,
Pnn
Y bn(W)o2
 Pn
Y bn(W)o2a:s:  !0: (B.6)
Proof. By the isomorphism between bK1;K2andeK1;K2, it is equivalent to show
Pnn
Y en(cW)o2
 Pn
Y en(cW)o2a:s:  !0:
33Note that
en(cW) =e>
1nX+e>
2nAX+K1X
k=1e1;kbU1;k+K2X
k=1e2;kbU2;k
=e>
1nX+e>
2nAX+Z
VK1X
k=1e1;kb1;nk(s)Z(s)d(s) +Z
VK2X
k=1e2;kb2;nk(s)AZ(s)d(s)
,bfn(W):
By Theorem A.1 and Lemma B.4, bfnis contained in a Glivenko-Cantelli (G-C) class with
probability going to one as n!1 . By Glivenko-Cantelli Preservation, Y en(cW)is also a
G-C class, and by Theorem A.1, fY en(cW)g2is also a G-C class, and thus the conclusion
follows.
With all these preparations, we can show the detailed proof of Theorem 2.
Proof of Theorem 2. Let
be the set of!for which both (B.5) and (B.6) hold. Fix !2
.
By deÔ¨Ånition of 
,9a subsequencefn0gsuch that along this subsequence
bn0!2K1;K2;
for some limit , where K1;K2is given in (3.3). Note that by Assumption (A8), PfY 
(W)g2has a unique minimizer over K1;K2,
0. Let
r1n=PnfY 
0(W)g2 PfY 
0(W)g2;
r2n=Pnn
Y bn(W)o2
 Pn
Y bn(W)o2:
By Lemma B.5, r2n!0asn!1 . Thus,
PfY 
0(W)g2PnfYi 
0(W)g2 r1nPnfYi bn(W)g2 r1n
PfY bn(W)g2 r1n r2n
!PfY (W)g2 limr1nPfY 
0(W)g2;
asn!1 , where the convergence is attained along the subsequence n0. Since the subsequence
is arbitrary, and with the help of the strong law, we can conclude that bn!
0andr1n!0
asn!1 . Thus, for Ô¨Åxed 1K1;K2<1, integratesPover!in
, we have
Pn
Y bn(W)o2
 PfY 
0(W)g2a:s:  !0: (B.7)
34However, this is also true if we allow K1n;K2n!1 sufÔ¨Åciently slowly. Recycling previous
arguments as in the proofs of Lemmas B.1 ‚Äì B.4, we can now show that
Pf0(W) 
0(W)g2!0;asK1n;K2n!1: (B.8)
Then
PfY bn(W)g2 PfY 
0(W)g2
=Ph
fY 
0(W)g n
bn(W) 
0(W)oi2
 PfY 
0(W)g2
=Pn
bn(W) 
0(W)o2
 2Ph
fY 
0(W)gn
bn(W) 
0(W)oi
:
LetEn= 2P[fY 
0(W)gfbn(W) 
0(W)g]. By (B.8), we have
jEnj2Ph
f"+0(W) 
0(W)gn
bn(W) 
0(W)oi
=2Ph
f0(W) 
0(W)gn
bn(W) 
0(W)oi

Pf0(W) 
0(W)g21=2
Pn
bn(W) 
0(W)o21=2
a:s:  !0;
asn!1 . Combining with (B.7) above, we obtain that Pfbn(W) 
0(W)g2a:s:  !0. Thus
combining with (B.8), as n!1 , the conclusion follows by
Pn
bn(W) 0(W)o2
=Pn
bn(W) 
0(W) +
0(W) 0(W)o2
a:s:  !Pn
bn(W) 
0(W)o2
+Pf
0(W) 0(W)g2a:s:  !0:
References
Ashford, J. W. and Mortimer, J. A. (2002), ‚ÄúNon-familial Alzheimer‚Äôs disease is mainly due
to genetic factors,‚Äù Journal of Alzheimer‚Äôs disease , 4, 169‚Äì177.
Bateman, R. J., Xiong, C., Benzinger, T. L., Fagan, A. M., Goate, A., Fox, N. C., Marcus,
D. S., Cairns, N. J., Xie, X., Blazey, T. M., Holtzman, D. M., Santacruz, A., Buckles, V .,
Oliver, A., Moulder, K., Aisen, P. S., Ghetti, B., Klunk, W. E., McDade, E., Martins, R. N.,
Masters, C. L., Mayeux, R., Ringman, J. M., Rossor, M. N., SchoÔ¨Åeld, P. R., Sperling,
R. A., Salloway, S., and Morris, J. C. (2012), ‚ÄúClinical and biomarker changes in dominantly
inherited Alzheimer‚Äôs disease,‚Äù New England Journal of Medicine , 367, 795‚Äì804.
35Bern, M. and Eppstein, D. (1995), ‚ÄúMesh generation and optimal triangulation,‚Äù in Computing
in Euclidean geometry , World ScientiÔ¨Åc, pp. 47‚Äì123.
Ciarleglio, A., Petkova, E., Ogden, R. T., and Tarpey, T. (2015), ‚ÄúTreatment decisions based
on scalar and functional baseline covariates,‚Äù Biometrics , 71, 884‚Äì894.
Ciarleglio, A., Petkova, E., Ogden, T., and Tarpey, T. (2018), ‚ÄúConstructing treatment decision
rules based on scalar and functional predictors when moderators of treatment effect are
unknown,‚Äù Journal of the Royal Statistical Society Series C , 67, 1331‚Äì1356.
Ciarleglio, A., Petkova, E., Tarpey, T., and Ogden, R. T. (2016), ‚ÄúFlexible functional regression
methods for estimating individualized treatment rules,‚Äù Stat, 5, 185‚Äì199.
Clifton, J. and Laber, E. (2020), ‚ÄúQ-learning: Theory and applications,‚Äù Annual Review of
Statistics and Its Application , 7, 279‚Äì301.
Fratiglioni, L., Winblad, B., and von Strauss, E. (2007), ‚ÄúPrevention of Alzheimer‚Äôs disease
and dementia. Major Ô¨Åndings from the Kungsholmen Project,‚Äù Physiology & behavior , 92,
98‚Äì104.
Hern ¬¥an, M. A. and Robins, J. M. (2010), Causal Inference , CRC Boca Raton, FL;.
Herold, C. J., Lewin, J. S., Wibmer, A. G., Thrall, J. H., Krestin, G. P., Dixon, A. K., Schoen-
berg, S. O., Geckle, R. J., Muellner, A., and Hricak, H. (2016), ‚ÄúImaging in the age of
precision medicine: summary of the proceedings of the 10th Biannual Symposium of the
International Society for Strategic Studies in Radiology,‚Äù Radiology , 279, 226‚Äì238.
Isaacson, R. S., Hristov, H., Saif, N., Hackett, K., Hendrix, S., Melendez, J., Safdieh, J.,
Fink, M., Thambisetty, M., Sadek, G., et al. (2019), ‚ÄúIndividualized clinical management of
patients at risk for Alzheimer‚Äôs dementia,‚Äù Alzheimer‚Äôs & Dementia , 15, 1588‚Äì1602.
Kong, D., Staicu, A.-M., and Maity, A. (2016), ‚ÄúClassical testing in functional linear models,‚Äù
Journal of nonparametric statistics , 28, 813‚Äì838.
Kosorok, M. R. (2008), Introduction to Empirical Processes and Semiparametric Inference ,
Springer: New York.
Kosorok, M. R. and Laber, E. B. (2019), ‚ÄúPrecision medicine,‚Äù Annual review of statistics and
its application , 6, 263‚Äì286.
Laber, E. B. and Staicu, A.-M. (2018), ‚ÄúFunctional feature construction for individualized
treatment regimes,‚Äù Journal of the American Statistical Association , 113, 1219‚Äì1227.
36Lai, M.-J. and Schumaker, L. L. (2007), Spline functions on triangulations. , Cambridge Uni-
versity Press.
Lai, M.-J. and Wang, L. (2013), ‚ÄúBivariate penalized splines for regression,‚Äù Statistica Sinica ,
23, 1399‚Äì1417.
Li, X., Wang, L., and Wang, H. J. (2021), ‚ÄúSparse Learning and Structure IdentiÔ¨Åcation for
Ultrahigh-Dimensional Image-on-Scalar Regression,‚Äù Journal of the American Statistical
Association , 116, 1994‚Äì2008.
Li, X., Yu, S., Wang, Y ., Wang, G., Wang, L., and Lai, M.-J. (2023), ‚ÄúNonparametric Regres-
sion for 3D Point Cloud Learning,‚Äù arXiv:2106.04255 .
Luckett, D. J., Laber, E. B., Kahkoska, A. R., Maahs, D. M., Mayer-Davis, E., and Kosorok,
M. R. (2020), ‚ÄúEstimating dynamic treatment regimes in mobile health using V-learning,‚Äù
Journal of the American Statistical Association , 115, 692‚Äì706.
McKeague, I. W. and Qian, M. (2014), ‚ÄúEstimation of treatment policies based on functional
predictors,‚Äù Statistica Sinica , 24, 1461‚Äì1485.
Mu, J., Wang, G., and Wang, L. (2018), ‚ÄúEstimation and inference in spatially varying coefÔ¨Å-
cient models,‚Äù Environmetrics , 29, e2485.
Nathoo, F. S., Kong, L., Zhu, H., and Initiative, A. D. N. (2019), ‚ÄúA review of statistical
methods in imaging genetics,‚Äù Canadian Journal of Statistics , 47, 108‚Äì131.
Park, H., Petkova, E., Tarpey, T., and Ogden, R. T. (2021), ‚ÄúFunctional additive models for
optimizing individualized treatment rules,‚Äù Biometrics , 1‚Äì14.
Pelham, W. E. and Fabiano, G. A. (2008), ‚ÄúEvidence-based psychosocial treatments for
attention-deÔ¨Åcit/hyperactivity disorder,‚Äù Journal of Clinical Child & Adolescent Psychol-
ogy, 37, 184‚Äì214.
Petersen, R. C., Aisen, P., Beckett, L. A., Donohue, M., Gamst, A., Harvey, D. J., Jack, C.,
Jagust, W., Shaw, L., Toga, A., and Trojanowski, J. (2010), ‚ÄúAlzheimer‚Äôs disease neuroimag-
ing initiative (ADNI): clinical characterization,‚Äù Neurology , 74, 201‚Äì209.
Qian, M. and Murphy, S. A. (2011), ‚ÄúPerformance guarantees for individualized treatment
rules,‚Äù Annals of statistics , 39, 1180.
Ramsay, T. (2002), ‚ÄúSpline smoothing over difÔ¨Åcult regions.‚Äù Journal of the Royal Statistical
Society: Series B (Statistical Methodology) , 64, 307‚Äì319.
37Robins, J. M. (2004), ‚ÄúOptimal Structural Nested Models for Optimal Sequential Decisions,‚Äù
inProceedings of the Second Seattle Symposium in Biostatistics , Springer, pp. 189‚Äì326.
Rosenbaum, P. R. (1984), ‚ÄúFrom association to causation in observational studies: The role
of tests of strongly ignorable treatment assignment,‚Äù Journal of the American Statistical
Association , 79, 41‚Äì48.
Rosenbaum, P. R. and Rubin, D. B. (1983), ‚ÄúAssessing sensitivity to an unobserved binary
covariate in an observational study with binary outcome,‚Äù Journal of the Royal Statistical
Society: Series B (Methodological) , 45, 212‚Äì218.
Schneider, L. S., Insel, P. S., Weiner, M. W., Initiative, A. D. N., et al. (2011), ‚ÄúTreatment with
cholinesterase inhibitors and memantine of patients in the Alzheimer‚Äôs Disease Neuroimag-
ing Initiative,‚Äù Archives of neurology , 68, 58‚Äì66.
Stern, Y . (2012), ‚ÄúCognitive reserve in ageing and Alzheimer‚Äôs disease,‚Äù The Lancet Neurol-
ogy, 11, 1006‚Äì1012.
Su, Y .-R., Di, C.-Z., and Hsu, L. (2017), ‚ÄúHypothesis testing in functional linear models,‚Äù
Biometrics , 73, 551‚Äì561.
Thall, P. F., Wooten, L. H., Logothetis, C. J., Millikan, R. E., and Tannir, N. M. (2007),
‚ÄúBayesian and frequentist two-stage treatment strategies based on sequential failure times
subject to interval censoring,‚Äù Statistics in medicine , 26, 4687‚Äì4702.
Wang, G., Wang, L., Lai, M.-J., Kim, M., Li, X., Mu, J., Wang, Y ., and Yu, S. (2019), ‚ÄúBPST:
Bivariate Spline over Triangulation,‚Äù R package version 1.0, https://github.com/
funstatpackages/BPST .
Wang, H. and Ranalli, M. G. (2007), ‚ÄúLow-rank smoothing splines on complicated domains.‚Äù
Biometrics , 63, 209‚Äì217.
Wang, L. and Lai, M.-J. (2019), ‚ÄúTriangulation,‚Äù R package version 1.0, https://
github.com/funstatpackages/Triangulation .
Weiner, M. W. and Veitch, D. P. (2015), ‚ÄúIntroduction to special issue: overview of
Alzheimer‚Äôs Disease Neuroimaging Initiative,‚Äù Alzheimer‚Äôs & Dementia , 11, 730‚Äì733.
Wood, N. S., Bravington, V . M., and Hedley, L. S. (2008), ‚ÄúSoap Ô¨Ålm smoothing.‚Äù Journal of
the Royal Statistical Society: Series B (Statistical Methodology) , 70, 931‚Äì955.
38Yu, S., Wang, G., Wang, L., and Yang, L. (2021), ‚ÄúMultivariate spline estimation and inference
for image-on-scalar regression,‚Äù Statistica Sinica , 31, 1463‚Äì1487.
Zhao, Y ., Zeng, D., Socinski, M. A., and Kosorok, M. R. (2011), ‚ÄúReinforcement learning
strategies for clinical trials in nonsmall cell lung cancer,‚Äù Biometrics , 67, 1422‚Äì1433.
Zhu, H., Fan, J., and Kong, L. (2014), ‚ÄúSpatially varying coefÔ¨Åcient model for neuroimaging
data with jump discontinuities,‚Äù Journal of the American Statistical Association , 109, 1084‚Äì
1098.
Zhu, H., Li, T., and Zhao, B. (2022), ‚ÄúStatistical learning methods for neuroimaging data
analysis with applications,‚Äù .
39