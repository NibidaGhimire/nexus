Connectivity-Aware Semi-Decentralized Federated Learning over
Time-Varying D2D Networks
Rohit Parasnisâˆ—
Purdue University
West Lafayette, Indiana, USA
rparasni@purdue.eduSeyyedali Hosseinalipour
University at Buffaloâ€“SUNY
Buffalo, New York, USA
alipour@buffalo.eduYun-Wei Chu
Purdue University
West Lafayette, Indiana, USA
chu198@purdue.edu
Mung Chiang
Purdue University
West Lafayette, USA
chiang@purdue.eduChristopher G. Brinton
Purdue University
West Lafayette, USA
cgb@purdue.edu
ABSTRACT
Semi-decentralized federated learning blends the conventional device-
to-server (D2S) interaction structure of federated model training
with localized device-to-device (D2D) communications. We study
this architecture over practical edge networks with multiple D2D
clusters modeled as time-varying and directed communication
graphs. Our investigation results in an algorithm that controls
the fundamental trade-off between (a) the rate of convergence of
the model training process towards the global optimizer, and (b)
the number of D2S transmissions required for global aggregation.
Specifically, in our semi-decentralized methodology, D2D consen-
sus updates are injected into the federated averaging framework
based on column-stochastic weight matrices that encapsulate the
connectivity within the clusters. To arrive at our algorithm, we
show how the expected optimality gap in the current global model
depends on the greatest two singular values of the weighted adja-
cency matrices (and hence on the densities) of the D2D clusters.
We then derive tight bounds on these singular values in terms of
the node degrees of the D2D clusters, and we use the resulting
expressions to design a threshold on the number of clients required
to participate in any given global aggregation round so as to ensure
a desired convergence rate. Simulations performed on real-world
datasets reveal that our connectivity-aware algorithm reduces the
total communication cost required to reach a target accuracy sig-
nificantly compared with baselines depending on the connectivity
structure and the learning task.
CCS CONCEPTS
â€¢Computer systems organization â†’Distributed architec-
tures ;Peer-to-peer architectures ;â€¢Networksâ†’Topology
analysis and generation .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MobiHoc â€™23, June 03â€“05, 2023, Washington, DC
Â©2023 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXXKEYWORDS
connectivity, semi-decentralized, federated learning
ACM Reference Format:
Rohit Parasnis, Seyyedali Hosseinalipour, Yun-Wei Chu, Mung Chiang,
and Christopher G. Brinton. 2023. Connectivity-Aware Semi-Decentralized
Federated Learning over Time-Varying D2D Networks. In Proceedings of
Submitted (MobiHoc â€™23). ACM, New York, NY, USA, 10 pages. https://doi.
org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Federated learning (FL) [ 14,21] is a popular paradigm for distribut-
ing machine learning (ML) tasks over a network of centrally coor-
dinated devices. By not requiring the devices to share any training
data with the central coordinator (server), FL improves privacy
and communication efficiency. The first FL technique, known as
federated averaging (FedAvg), was proposed in [ 14,21] as a dis-
tributed optimization algorithm for a â€œstarâ€ topology-based network
architecture. In each iteration of the FedAvg algorithm, (i) devices
individually performs a number of local stochastic gradient descent
(SGD) iterations and transmit their cumulative stochastic gradients
to the central server, which then (ii) aggregates a random subset
of these gradients to estimate the globally optimal ML model. In
recent years, several variants of FedAvg have been proposed to
address the challenges encountered by FL at the wireless edge, in-
cluding different dimensions of heterogeneity in dataset statistics
(e.g., varying local data distributions) and in the network system
itself (e.g., varying communication and computation capabilities).
An emerging arch of work has been exploring FL under edge
networks that diverge from the star learning topology between the
devices and the server. This had led to varying degrees of decentral-
ization in FL, reaching fully decentralized, serverless settings that sit
at the opposite extreme of the star topology [ 4,10,15,16,18,27,37].
In between these two extremes is semi-decentralized FL , where
device-to-device (D2D) communications complement device-to-
server (D2S) interactions [ 6,9,19,36]. These D2D interactions
occur locally within clusters of devices, with each cluster forming
a connected component. In semi-decentralized FL, D2D transmis-
sions are less energy-consuming than D2S interactions and can help
reduce the frequency of D2S communications through localized
synchronizations of the ML model updates.
Despite these recent investigations, we still do not have a clear
understanding of how different D2D topology properties impactarXiv:2303.08988v2  [cs.DC]  20 Jul 2023MobiHoc â€™23, June 03â€“05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
Figure 1: Conventional Federated Learning vs. Connectivity-
Aware Semi-Decentralized Learning Architecture
the learning process. For instance, the ratio of the number of D2D
interactions to that of D2S interactions will impact the training effi-
ciency differently over different topologies. This becomes especially
important in the presence of constraints such as upload/download
bandwidths, and stochastic uncertainties such as data heterogeneity,
client mobility, and communication link failures. On one hand, edge
devices in clustered D2D networks that have little to no cross-cluster
interactions are typically in contact with only a small fraction of
the rest of the network at any given time instant (e.g., networks of
unmanned aerial vehicle (UAV) swarms spread over geo-distributed
regions separated by long distances). In such networks, if there
is no central coordinator (implying zero D2S interactions) and if
the training data are distributed heteregeneously among the edge
devices, no practically feasible number of D2D interactions is likely
to aggregate a set of local ML models that are diverse enough to
approximate the global data distribution [3].
On the other hand, having a high number of D2D interactions is
advantageous when D2S interactions take the form of high-latency,
high-energy transmissions (e.g., if the UAV swarms in the previous
example are miles away from the nearest base station). Moreover,
classical star-topology-based FL architectures miss out on an im-
portant benefit of D2D cooperation: devices acting as information
relays between other devices and the server, effectively sharing
with the server more information than it would expect to receive.
We are thus motivated to conduct a formal study of semi-decentralized
FL, and reveal the combined impact of D2S and D2D interactions
on the training process. After building an understanding of the
D2D topologies on which the D2D interactions occur, we propose a
novel FL technique that enables us to take into account the degree
distributions of the D2D clusters and use this knowledge to tune
the number of expensive D2S transmissions while simultaneously
ensuring a minimum rate of global training convergence. As shown
in Fig. 1, we incorporate two scales of model aggregations: on the
first scale, the edge devices perform intra-cluster model aggrega-
tions with their one-hop neighbors via distributed averaging, and
on the second scale, a central server samples a random set of clients
(as in the classical FedAvg architecture [ 21]) for global aggregation.
Our methodology has several potential use-cases, including the
following that we will refer to as examples throughout the paper:
(1)UAV Networks for ISR: UAVs are being increasingly de-
ployed for intelligence, surveillance, and reconnaissance
(ISR) operations in defense settings [ 26,31]. With the UAVspartitioned into D2D-enabled swarms deployed across dif-
ferent areas, our connectivity-aware algorithm can facilitate
intra-cluster communications and reduce the over-reliance
of the model training process on D2S transmissions.
(2)Self-driving cars: Many learning tasks for self-driving cars
call for vehicles to communicate over short distances. In such
settings, geographical proximity can be used to partition the
traffic network into clusters, which would enable us to design
intra-cluster D2S communications that turn out to be more
efficient than D2S communications with a far-away server.
1.1 Summary of Contributions
Our key contributions are summarized below:
(1)Analysis with Time-Varying and Directed Cluster Topolo-
gies: We consider that each D2D cluster in general is a time-
varying directed graph (digraph). We show how the expected
optimality gap of the learning process depends on the great-
est two singular values of the weighted adjacency matrices
used for local aggregations in the clusters. Our analysis is
applicable to edge networks with asymmetric D2D commu-
nications subjected to link failures.
(2)Singular Value Bounds in terms of Node Degrees: We
derive bounds on the singular values of the cluster-specific
weighted adjacency matrices in terms of the degree distri-
bution of every cluster. This introduces new technical chal-
lenges as described in Section 1.2, since it is a stark departure
from existing analyses of consensus-based FL algorithms
that rely heavily on the spectral gaps of symmetric weight
matrices (e.g., see [5, 9, 12, 13, 19, 34, 37]).
(3)Connectivity-Aware Learning Algorithm: We use our sin-
gular value bounds to design a time-varying threshold on
the number of clients required to be sampled by the cen-
tral server for global aggregation so as to enforce a desired
convergence rate while simultaneously reducing the num-
ber of D2S communications. This tradeoff results in a novel
connectivity-aware algorithm with significant energy sav-
ings, as validated subsequently by our numerical results.
(4)Effect of Data Heterogeneity under Mild Gradient Di-
versity Assumptions: We derive a bound on the expected
optimality gap that captures the effects of cluster densities
as well as the extent of data heterogeneity across the de-
vices. In doing so, we employ a milder definition of gradient
diversity [19] than what is typically assumed in literature.
Notation: We denote the set of real numbers by Rand the set of
positive integers by N. For anyğ‘›âˆˆN, we define[ğ‘›]:={1,2,...,ğ‘›}.
For a finite set ğ‘†, we denote its cardinality by |ğ‘†|.
We denote the vector space of ğ‘›-dimensional real-valued column
vectors by Rğ‘›. We use the superscript notationâŠ¤to denote the
transpose of a vector or a matrix. All matrix and vector inequalities
are assumed to hold entry-wise. We use ğ¼to denote the identity
matrix (of the known dimension) and 1to denote the column vector
(of the known dimension) that has all entries equal to 1. Similarly, 0
denotes the all-zeroes vector. In addition, we use âˆ¥Â·âˆ¥to denote the
Euclidean norm of a square matrix or a vector, and for any vector
ğ‘£âˆˆRğ‘›we use diag(ğ‘£)to denote the diagonal matrix whose ğ‘–-th
diagonal entry is ğ‘£ğ‘–.Connectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc â€™23, June 03â€“05, 2023, Washington, DC
We say that a vector ğ‘£âˆˆRğ‘›isstochastic ifğ‘£â‰¥0andğ‘£âŠ¤1=1,
and a matrix ğ´iscolumn-stochastic ifğ´is non-negative and if each
column ofğ´sums to 1, i.e., if ğ´â‰¥0andğ´âŠ¤1=1.
1.2 Related Work
Several different FL approaches with varying degrees of decentral-
ization have been proposed to date. In this section, we focus on
those which are most relevant to the present work.
Semi-decentralized FL: [19] proposes a semi-decentralized learn-
ing methodology in which the D2D network is partitioned into
clusters, as in our paper. The key differences between [ 19] and the
present work are (a) we do not assume the D2D communications to
be bidirectional (equivalently, the cluster graphs in our model are
not undirected), and (b) our analysis uses column-stochastic con-
sensus matrices that need not satisfy the standard but unrealistic
assumption of symmetry (which leads to double stochasticity and
may not hold if the cluster graphs are directed). This leads to two
significant technical challenges. First, we cannot use standard eigen-
value results in our analysis since we must focus on singular values,
which generally differ from eigenvalues for asymmetric matrices.
Second, unlike doubly stochastic matrices, column-stochastic aggre-
gation matrices in general do not ensure convergence to consensus
in the absence of a central coordinator, which means our analysis
must account for the combined effect of global aggregations and
column-stochasticity. We address these challenges in this work.
Another closely related semi-decentralized learning methodol-
ogy is [ 36]. In [ 36], the goal is to enable edge devices to compute
weighted sums of their neighborsâ€™ scaled cumulative gradients in
order to reduce the dependence of the global training process on un-
reliable D2S links. [ 36], however, assumes the D2D communication
network to be time-invariant and undirected, thereby disregarding
potential communication link failures and client mobility.
Learning over Clustered D2D Networks: Recently, [ 2] proposed
fully decentralized learning over D2D networks in which a small
subset of nodes act as bridges between different clusters for cross-
cluster model transmission, thereby obviating the role of a server.
Their topology design, however, results in a static rather than a
dynamic D2D network. Reference [ 6] also focuses on clustered
networks, but it provides a semi-decentralized learning methodol-
ogy where the basis for clustering is data similarity, whereas our
methodology makes no assumptions on the basis for clustering. A
complementary approach is proposed in [ 3], where every cluster is
assumed to be a clique and the D2D network is partitioned in such
a way that each local dataset is representative of the global data.
Network clusters also form the focus of another recent work, [ 28],
which proposes having one edge server per cluster so as to elimi-
nate the need for a central server. Its learning algorithm assumes
the edge network topology to be undirected, which gives rise to a
symmetric adjacency matrix.
Other Consensus-based Algorithms: Reference [ 12] provides
improved bounds on the convergence rates of certain gradient
tracking methods used in decentralized learning by enhancing
the analysis of the consensus matrix (referred to as the mixing
matrix therein) and its spectral gap. However, similar to [ 19], this
work assumes the consensus matrix to be row-stochastic as well
as symmetric, and hence, doubly stochastic. In this respect, [ 7]relaxes the assumptions of symmetry as well as double stochasticity
in an online learning setting. However, the matrices therein are
row-stochastic, which are not average-preserving and hence, they
are not as suitable as column-stochastic matrices for minimizing
the average of all the local loss functions. Finally, we remark that
there exists abundant literature on distributed optimization over
time-varying digraphs characterized by consensus matrices that
are not necessarily doubly stochastic (e.g., see [ 1,17,23â€“25,30,33]).
However, the effects of both data heterogeneity (or non-i.i.d. data
distribution) and graph-theoretic properties (such as the degree
distribution of the network in question) on the convergence rate of
these algorithms have remained largely unexplored.
2 SEMI-DECENTRALIZED FL SETUP
We now introduce the system model, the learning objective, and
the network model in semi-decentralized FL.
2.1 System Model and Learning Objectives
We consider a collaborative learning environment consisting of ğ‘›
edge devices, or clients , and a central parameter server (PS) that is
tasked with aggregating all the local model updates generated by
the clients. We use [ğ‘›]to denote the set of clients.
Each clientğ‘–âˆˆ[ğ‘›]has a local datasetDğ‘–, which is a collection of
data samples of the form ğœ‰=(ğ‘¢,ğ‘¦)whereğ‘¢âˆˆRğ‘is the feature vec-
torof the sample and ğ‘¦is its label . On this basis, for any model ğ‘¥âˆˆ
Rğ‘, we define the loss function ğ¿:Rğ‘Ã—âˆªğ‘›
ğ‘–=1Dğ‘–â†’Rso thatğ¿(ğ‘¥;ğœ‰)
denotes the loss incurred by ğ‘¥on a sample ğœ‰âˆˆâˆªğ‘›
ğ‘–=1Dğ‘–(where
âˆªğ‘›
ğ‘–=1Dğ‘–is the global dataset). The average loss incurred by ğ‘¥over
the local dataset of client ğ‘–is given byğ‘“ğ‘–(ğ‘¥):=1
|Dğ‘–|Ã
ğœ‰âˆˆDğ‘–ğ¿(ğ‘¥;ğœ‰),
whereğ‘“ğ‘–:Rğ‘â†’Rdenotes the local loss function of clientğ‘–.
In collaboration with the PS, the clients seek to minimize the
global loss function ğ‘“:Rğ‘â†’R, defined as the unweighted arith-
metic mean ğ‘“(ğ‘¥):=1
ğ‘›Ãğ‘›
ğ‘–=1ğ‘“ğ‘–(ğ‘¥)of all the local loss functions. The
learning objective, therefore, is to determine the global optimum
ğ‘¥âˆ—:=arg minğ‘¥âˆˆRğ‘ğ‘“(ğ‘¥).
2.2 D2D and D2S Network Models
We model two types of interactions among the network elements:
(i) D2S and (ii) D2D. For D2S interactions, the devices can engage in
uplink communications to the PS if prompted by the server, which
happens through a sampling procedure explained later.
We model the D2D network as a time-varying directed graph
ğº(ğ‘¡)=([ğ‘›],ğ¸(ğ‘¡)), where[ğ‘›]denotes the vertex set andğ¸(ğ‘¡)the
edge set of the digraph. The existence of a directed edge from a node
ğ‘–âˆˆ[ğ‘›]to another node ğ‘—âˆˆ[ğ‘›]inğº(ğ‘¡)denotes the existence of a
communication link from the ğ‘–-th client to the ğ‘—-th client in the D2D
network. In this case, we refer to client ğ‘–(respectively, client ğ‘—) as
the in-neighbor (respectively, out-neighbor) of client ğ‘—(respectively,
clientğ‘–). The set of in-neighbors (respectively, out-neighbors) of a
clientğ‘–âˆˆ[ğ‘›]at timeğ‘¡is denoted byNâˆ’
ğ‘–(ğ‘¡)(respectively,N+
ğ‘–(ğ‘¡)).
The number of in-neighbors (respectively, out-neighbors) is called
the in-degree (respectively, out-degree) and is denoted by ğ‘‘âˆ’
ğ‘–(ğ‘¡)(re-
spectively,ğ‘‘+
ğ‘–(ğ‘¡)). We letğ‘‘âˆ’max(ğ‘¡),ğ‘‘+
min(ğ‘¡), andğ‘‘+max(ğ‘¡)denote the
maximum in-degree, the minimum out-degree, and the maximum
out-degree, respectively.MobiHoc â€™23, June 03â€“05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
Unlike standard works on distributed learning [ 1,23â€“25], we
do not assume the D2D network to be strongly connected or even
uniformly strongly connected [ 23,24] over time. This gives rise to
a numberğ‘>1of strongly connected components of ğº(ğ‘¡), denoted
{(ğ‘‰1(ğ‘¡),ğ¸1(ğ‘¡)),(ğ‘‰2(ğ‘¡),ğ¸2(ğ‘¡)),...,(ğ‘‰ğ‘(ğ‘¡),ğ¸ğ‘(ğ‘¡))}which we refer to
asclusters of the D2D network. Here, we make the following mild
assumptions that apply to many cellular networks:
(1) The number of clusters, ğ‘, is time-invariant.
(2)There does not exist any communication link between any
two clusters. In other words, ğ¸(ğ‘¡)=âˆªğ‘
â„“=1ğ¸â„“(ğ‘¡).
(3)Regardless of any movement of clients from one cluster to
another over time, as of time ğ‘¡, the server has full knowledge
of the vertex sets{ğ‘‰â„“(ğ‘¡)}ğ‘
â„“=1of all theğ‘clusters.
The third condition is satisfied in practice since the base station
(which acts as the PS) is aware of the users in its coverage area.
3 PROPOSED METHOD FOR
CONNECTIVITY-AWARE LEARNING
We now present our methodology for connectivity-aware learning
over the semi-decentralized setup from Sec. 2. Our technique will
enable the central server to use limited knowledge of the cluster
degree distributions to tune a communication-efficiency trade-off.
3.1 Local Model Updates
As in many FL schemes, we assume every client performs multiple
rounds of local SGD iterations between any two consecutive rounds
of global aggregation. Let ğ‘¥(ğ‘¡)denote the global model that all the
clients possess at the end of the ğ‘¡-th round of global aggregation.
Then, each client ğ‘–âˆˆ[ğ‘›]performsğ‘‡âˆˆNiterations of local SGD.
In other words, for each ğ‘˜âˆˆ{0,1,...,ğ‘‡âˆ’1}, we have
ğ‘¥(ğ‘¡,ğ‘˜+1)
ğ‘–=ğ‘¥(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœ‚ğ‘¡eâˆ‡ğ‘“ğ‘–(ğ‘¥(ğ‘¡,ğ‘˜)
ğ‘–), (1)
whereğœ‚ğ‘¡>0is the learning rate or the step-size, and eâˆ‡ğ‘“ğ‘–(ğ‘¥):=
1
|ğœ’ğ‘–|Ã
ğœ‰âˆˆğœ’ğ‘–âˆ‡ğ¿(ğ‘¥;ğœ‰)is the stochastic gradient computed by client ğ‘–
by sampling a mini-batch or a random subset ğœ’ğ‘–âŠ‚Dğ‘–of its local
samples. Note that ğ‘¥(ğ‘¡,0)
ğ‘–:=ğ‘¥(ğ‘¡).
3.2 Intra-Cluster Model Aggregations
The next step involves all the clients aggregating their scaled cumu-
lative gradients with their neighbors. This aggregation takes the
form of weighted sums. Every client ğ‘–âˆˆ[ğ‘›]first transmits its scaled
cumulative stochastic gradient ğ‘¥(ğ‘¡,ğ‘‡)
ğ‘–âˆ’ğ‘¥(ğ‘¡)=âˆ’ğœ‚ğ‘¡Ãğ‘‡âˆ’1
ğ‘˜=0eâˆ‡ğ‘“ğ‘–(ğ‘¥(ğ‘¡,ğ‘˜)
ğ‘–)
to each of its out-neighbors ğ‘—âˆˆN+
ğ‘–(ğ‘¡)before theğ‘¡-th global ag-
gregation round. To facilitate this, we assume that every cluster
â„“âˆˆ[ğ‘]contains an access point to which every client ğ‘–âˆˆğ‘‰â„“(ğ‘¡)
sends a list of its in-neighbors (clients whose gradients ğ‘–has re-
ceived). The access point then announces the end of the concerned
D2D communication round, determines the out-degree sequence
{ğ‘‘+
ğ‘—(ğ‘¡):ğ‘—âˆˆğ‘‰â„“(ğ‘¡)}of the cluster, and broadcasts this sequence to
every client in the cluster.
Subsequently, the client computes the following weighted sum of
all the scaled cumulative gradients it receives from its in-neighbors:
Î”ğ‘–(ğ‘¡)=âˆ‘ï¸
ğ‘—âˆˆNâˆ’
ğ‘–(ğ‘¡)1
ğ‘‘+
ğ‘—(ğ‘¡)
ğ‘¥(ğ‘¡,ğ‘‡)
ğ‘—âˆ’ğ‘¥(ğ‘¡)
. (2)This rule can be expressed compactly in matrix form as
âˆ†(ğ‘¡)=ğ´(ğ‘¡)ğ‘‹âŠ¤
diff(ğ‘¡), (3)
where âˆ†(ğ‘¡):=[Î”1(ğ‘¡)Î”2(ğ‘¡) Â·Â·Â· Î”ğ‘›(ğ‘¡)]âŠ¤,
ğ‘‹diff(ğ‘¡):=h
ğ‘¥(ğ‘¡,ğ‘‡)
1âˆ’ğ‘¥(ğ‘¡)ğ‘¥(ğ‘¡,ğ‘‡)
2âˆ’ğ‘¥(ğ‘¡)Â·Â·Â·ğ‘¥(ğ‘¡,ğ‘‡)
ğ‘›âˆ’ğ‘¥(ğ‘¡)i
, and
ğ´(ğ‘¡)âˆˆRğ‘›Ã—ğ‘›is a matrix whose(ğ‘–,ğ‘—)-th entry equals ğ‘ğ‘–ğ‘—(ğ‘¡)=1
ğ‘‘+
ğ‘—(ğ‘¡)
for allğ‘–âˆˆ[ğ‘›]andğ‘—âˆˆNâˆ’
ğ‘–(ğ‘¡).
Fact 1.ğ´(ğ‘¡)is a column-stochastic matrix because the following
holds for all ğ‘—âˆˆ[ğ‘›]:
ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘ğ‘–ğ‘—(ğ‘¡)=âˆ‘ï¸
ğ‘–âˆˆ[ğ‘›]:ğ‘—âˆˆNâˆ’
ğ‘–(ğ‘¡)1
ğ‘‘+
ğ‘—(ğ‘¡)=âˆ‘ï¸
ğ‘–âˆˆN+
ğ‘—(ğ‘¡)1
|N+
ğ‘—(ğ‘¡)|=1.
It can be verified that ğ´(ğ‘¡)is a block-diagonal matrix with its blocks
{ğ´â„“(ğ‘¡)}ğ‘
â„“=1being the equal-neighbor adjacency matrices of the ğ‘
clusters in the D2D network.
Henceforth, we refer to ğ´(ğ‘¡)as the equal-neighbor adjacency ma-
trixofğº(ğ‘¡)because it represents every client ğ‘–âˆˆ[ğ‘›]transmitting
an equal share (a fraction1
ğ‘‘+
ğ‘–(ğ‘¡)) of its scaled cumulative gradient
to itsğ‘‘+
ğ‘–(ğ‘¡)out-neighbors.
3.3 Global Aggregation at the PS
For the global aggregation step, the PS samples a random subset of
clientsS(ğ‘¡)âŠ‚[ğ‘›]. The cardinality ğ‘š(ğ‘¡)â‰¤ğ‘›of this set is carefully
chosen by our algorithm such that the resulting number of D2S
interactions is just enough to complement the intra-cluster aggre-
gations without excessively slowing down the training process.
Specifically, this involves three broad steps: (a) The PS first learns
the degree distribution of each cluster. (b) It then computes an
upper bound on an error quantity ğœ™(ğ‘¡)that captures the combined
effect of random sampling and the cluster degree distributions on
the convergence rate. (c) It computes the minimum value of ğ‘š(ğ‘¡)
required to keep ğœ™(ğ‘¡)below a desired threshold. More specifically:
(1)For the(ğ‘¡+1)-th round of global aggregation, the server
usesğ‘š(ğ‘¡)(computed in the previous iteration) to selectlğ‘š(ğ‘¡)
ğ‘›
ğ‘›â„“(ğ‘¡)m
clients uniformly at random from the ğ‘›â„“(ğ‘¡):=
|ğ‘‰â„“(ğ‘¡)|clients that constitute cluster â„“âˆˆ[ğ‘]. This ensures
that every cluster has a representation in the global aggre-
gation that is proportionate to its size. The resulting set of
randomly sampled clients is denoted by S(ğ‘¡). The server
then updates the global model as follows:
ğ‘¥(ğ‘¡+1)=ğ‘¥(ğ‘¡)+1
ğ‘š(ğ‘¡)âˆ‘ï¸
ğ‘–âˆˆS(ğ‘¡)Î”ğ‘–(ğ‘¡)=ğ‘¥(ğ‘¡)+1
ğ‘š(ğ‘¡)ğ‘›âˆ‘ï¸
ğ‘–=1ğœğ‘–(ğ‘¡)Î”ğ‘–(ğ‘¡),
(4)
whereğœğ‘–(ğ‘¡):=|{ğ‘–}âˆ©S(ğ‘¡)|is an indicator random variable
that takes the value 1when client ğ‘–is sampled and the value
0otherwise. Note thatÃğ‘›
ğ‘–=1ğœğ‘–(ğ‘¡)=|S(ğ‘¡)|=ğ‘š(ğ‘¡).
(2)The current round is now ğ‘¡â†ğ‘¡+1. All the cluster access
points send their respective out-degree sequences to the
server. Using this information, the server computes ğ›¼â„“(ğ‘¡):=
1
ğ‘›â„“(ğ‘¡)minğ‘–âˆˆğ‘‰â„“(ğ‘¡)ğ‘‘+
ğ‘–(ğ‘¡), the minimum out-degree fraction of
clusterâ„“âˆˆ [ğ‘]. The server then uses either of the two
sets of singular value bounds that we later derive in Sec.Connectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc â€™23, June 03â€“05, 2023, Washington, DC
5 (either (10)-(11)or(15)-(16)) to compute an upper bound
ğœ“(ğ‘š(ğ‘¡),ğ›¼1(ğ‘¡),...,ğ›¼ğ‘(ğ‘¡))on the connectivity factor affecting
the convergence rate. This connectivity factor is defined as
ğœ™(ğ‘¡):=ğ‘›
ğ‘š(ğ‘¡)âˆ’1ğ‘âˆ‘ï¸
â„“=1ğ‘›â„“(ğ‘¡)
ğ‘›ğœ™â„“(ğ‘¡), (5)
whereğœ™â„“(ğ‘¡):=ğœ2
1(ğ´â„“(ğ‘¡))+ğœ2
2(ğ´â„“(ğ‘¡))âˆ’ 1depends on the
greatest two singular values ğœ1(ğ´â„“(ğ‘¡))â‰¥ğœ2(ğ´â„“(ğ‘¡))of the
equal-neighbor adjacency matrix ğ´â„“(ğ‘¡)of clusterâ„“. For the
upper bound, we will show that
ğœ“(ğ‘š(ğ‘¡),ğ›¼1(ğ‘¡),...,ğ›¼ğ‘(ğ‘¡))=ğ‘›
ğ‘š(ğ‘¡)âˆ’1ğ‘âˆ‘ï¸
â„“=1ğ‘›â„“(ğ‘¡)
ğ‘›ğœ“â„“(ğ‘¡),(6)
where either of the following holds (with the indexing (ğ‘¡)
on the right hand side omitted for brevity):
ğœ“â„“(ğ‘¡)=1+ğœ€â„“+1
ğ›¼â„“âˆ’12
+2ğœ€â„“ 
1+2
ğ›¼â„“âˆ’1
ğ›¼2
â„“!
,
ğœ“â„“(ğ‘¡)=2+2ğœ‘â„“
âˆ’(1âˆ’ğœ€â„“)2(1âˆ’ğ›¼2
âˆ’â„“) (1âˆ’ğœ€â„“)2(1âˆ’ğ›¼2
âˆ’â„“)âˆ’ğ›¼âˆ’â„“
ğ‘›â„“(ğœ€net,â„“+1)
ğœ€net,â„“âˆ’ğ›¼âˆ’â„“+1
ğ›¼â„“ğ‘›â„“ (7)
withğœ€â„“(ğ‘¡):=ğ‘‘+
max(ğ‘¡)âˆ’ğ‘‘+
min(ğ‘¡)
ğ‘‘+
min(ğ‘¡),ğœ‘â„“(ğ‘¡):=ğ‘‘âˆ’
max(ğ‘¡)âˆ’ğ‘‘+
min(ğ‘¡)
ğ‘‘+
min(ğ‘¡),
ğ›¼âˆ’â„“(ğ‘¡):=1
ğ›¼â„“(ğ‘¡)âˆ’1andğœ€net,â„“(ğ‘¡)=ğœ‘â„“(ğ‘¡)+ğœ€â„“(ğ‘¡)
ğ›¼â„“(ğ‘¡).
(3) Finally, the server sets
ğ‘š(ğ‘¡+1):=min{ğ‘Ÿâˆˆ[ğ‘›]:ğœ“(ğ‘Ÿ,ğ›¼1(ğ‘¡+1),...,ğ›¼ğ‘(ğ‘¡+1))â‰¤ğœ™max}
whereğœ™maxis a threshold given as an input to the algorithm.
This step ensures that ğœ™(ğ‘¡)remains below the threshold
ğœ™max, thereby preserving the convergence rate.
Our algorithm for ğ‘¡maxglobal rounds is summarized in Alg. 1.
4 CONVERGENCE ANALYSIS
We now provide theoretical performance guarantees for Algorithm
1. We also explain how the effect of D2D cluster connectivity on
the convergence rate of the algorithm is captured by the singular
values of the equal-neighbor adjacency matrices of the clusters. All
the proofs except that of Proposition 5.1 are available in the
appendix .
4.1 Assumptions and Preliminaries
4.1.1 Loss Functions. We start by making the following standard
assumptions on the local loss functions:
Assumption 1 ( Strong Convexity ).All the local loss func-
tions{ğ‘“ğ‘–}ğ‘›
ğ‘–=1areğœ‡-strongly convex, i.e., there exists ğœ‡>0such that
(âˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¦))âŠ¤(ğ‘¥âˆ’ğ‘¦)â‰¥ğœ‡âˆ¥ğ‘¥âˆ’ğ‘¦âˆ¥2for allğ‘¥,ğ‘¦âˆˆRğ‘and all
ğ‘–âˆˆ[ğ‘›].
Assumption 2 ( Smoothness ).All the local loss functions {ğ‘“ğ‘–}ğ‘›
ğ‘–=1
areğ›½-smooth, i.e., there exists a finite ğ›½such thatâˆ¥âˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¦)âˆ¥â‰¤
ğ›½âˆ¥ğ‘¥âˆ’ğ‘¦âˆ¥for allğ‘¥,ğ‘¦âˆˆRğ‘and allğ‘–âˆˆ[ğ‘›].
As shown in [ 19], Assumptions 1 and 2 imply that the global
loss function ğ‘“is bothğœ‡-strongly convex and ğ›½-smooth.Algorithm 1 Connectivity-Aware Semi-Decentralized Learning
Input:ğ‘›,ğ‘,ğ‘‡,ğœ™max,ğ‘¡max,ğ‘š(0),{ğ‘›â„“(ğ‘¡)}ğ‘¡max
ğ‘¡=0,ğ‘¥(0)
Output:ğ‘¥(ğ‘¡max)
1:forğ‘¡âˆˆ{0,1,...,ğ‘¡ maxâˆ’1}do
2: Clientğ‘–âˆˆ[ğ‘›]setsğ‘¥(ğ‘¡,0)
ğ‘–â†ğ‘¥(ğ‘¡)
3: forğ‘˜âˆˆ{0,1,...,ğ‘‡âˆ’1}do
4: Clientğ‘–âˆˆ[ğ‘›]computesğ‘¥(ğ‘¡,ğ‘˜+1)
ğ‘–â†ğ‘¥(ğ‘¡,ğ‘˜)
ğ‘–âˆ’ğœ‚ğ‘¡eâˆ‡ğ‘“ğ‘–(ğ‘¥(ğ‘¡,ğ‘˜)
ğ‘–)
5: end
6: Clientğ‘–âˆˆ [ğ‘›]transmits its scaled cumulative local gradient
âˆ’ğœ‚ğ‘¡Ãğ‘‡âˆ’1
ğ‘˜=0eâˆ‡ğ‘“ğ‘–(ğ‘¥(ğ‘¡,ğ‘˜)
ğ‘–)=ğ‘¥(ğ‘¡,ğ‘‡)
ğ‘–âˆ’ğ‘¥(ğ‘¡)to its out-neighbors N+
ğ‘–(ğ‘¡)
7: Clientğ‘–âˆˆ [ğ‘›]computes the following weighted sum of its in-
neighborsâ€™ cumulative local gradients:
Î”ğ‘–(ğ‘¡)â†âˆ‘ï¸
ğ‘—âˆˆNâˆ’
ğ‘–(ğ‘¡)1
ğ‘‘+
ğ‘—(ğ‘¡)
ğ‘¥(ğ‘¡,ğ‘‡)
ğ‘—âˆ’ğ‘¥(ğ‘¡)
8: PS samplesğ‘šâ„“(ğ‘¡)=ğ‘›â„“(ğ‘¡)
ğ‘›ğ‘š(ğ‘¡)clients uniformly at random from
clusterâ„“âˆˆ[ğ‘]
9: PS computes ğ‘¥(ğ‘¡+1)â†ğ‘¥(ğ‘¡)+1
ğ‘š(ğ‘¡)Ãğ‘›
ğ‘–=1ğœğ‘–(ğ‘¡)Î”ğ‘–(ğ‘¡)and broadcasts
ğ‘¥(ğ‘¡+1)to all clients
10: PS computes
11:ğ‘š(ğ‘¡+1)â† min{ğ‘Ÿâˆˆ[ğ‘›]:ğœ“(ğ‘Ÿ,ğ›¼ 1(ğ‘¡+1),...,ğ›¼ğ‘(ğ‘¡+1))â‰¤ğœ™max}
12:end
13:returnğ‘¥(ğ‘¡max)
4.1.2 SGD Iterations. Additionally, we make the following stan-
dard assumption on the stochastic gradients generated through the
SGD procedure for each client:
Assumption 3 ( Unbiasedness and Bounded Variance ).
The SGD noise associated with every client is unbiased, i.e.,
E[eâˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¥)|ğ‘¥]=0, and it has a bounded variance, i.e., there
exists a constant ğœš>0such that Eâˆ¥eâˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ¥2â‰¤ğœš2for all
modelsğ‘¥âˆˆRğ‘and allğ‘–âˆˆ[ğ‘›].
In addition, we assume that the SGD noise is independent across
clients, i.e., for all ğ‘¥âˆˆRğ‘, the random vectorsn
eâˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¥)oğ‘›
ğ‘–=1
are mutually conditionally independent given ğ‘¥.
4.1.3 Gradient Diversity. Furthermore, we assume that the training
data are not distributed uniformly at random among the clients,
which gives rise to data heterogeneity among the clients. Unlike the
standard assumption on data heterogeneity that imposes a uniform
upper bound onâˆ¥âˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ’âˆ‡ğ‘“(ğ‘¥)âˆ¥(see [ 29] for example), we make
a weaker assumption on the diversity of local gradients. In fact, this
assumption, which was first proposed in [ 19], can be derived as a
consequence of Assumptions 1 and 2, as shown in [ 19]. Below, we
formally state this observation.
Lemma 4.1 ( Gradient diversity [19]).For allğ‘–âˆˆ[ğ‘›]and
ğ‘¥âˆˆRğ‘, we haveâˆ¥âˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ’âˆ‡ğ‘“(ğ‘¥)âˆ¥â‰¤ğ›¿+2ğ›½âˆ¥ğ‘¥âˆ’ğ‘¥âˆ—âˆ¥, where
ğ›¿:=ğ›½max
ğ‘–âˆˆ[ğ‘›]âˆ¥ğ‘¥âˆ—âˆ’ğ‘¥âˆ—
ğ‘–âˆ¥=ğ›½max
ğ‘–âˆˆ[ğ‘›]âˆ¥ğ‘¥âˆ—âˆ’arg min
ğ‘¦âˆˆRğ‘ğ‘“ğ‘–(ğ‘¦)âˆ¥ (8)
As argued in [ 19], the standard assumption (which is a special
case of the above inequality with ğ›½=0) is unrealistic as it does not
apply to quadratic and super-quadratic loss functions unless the
upper bound ğ›¿is chosen to be unreasonably large.MobiHoc â€™23, June 03â€“05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
4.2 Results
We now quantify how the singular values of the equal-neighbor
matrices and the number of clients sampled by the PS affect the
efficiency of our algorithm in terms of its optimality gap.
We first show how the expected optimality gap of our algo-
rithm depends on the expected deviation of the global average
ğ‘¥(ğ‘¡+1)âˆ’ğ‘¥(ğ‘¡)(i.e., the random vector computed by the PS using
the aggregation rule (4)) from the true average of all the scaled
cumulative gradients.
Lemma 4.2. At the end of the(ğ‘¡+1)-th round of global aggregation,
the expected optimality gap of Algorithm 1 is given by
Eğ‘¥(ğ‘¡+1)âˆ’ğ‘¥âˆ—2
=Eğ‘¥(ğ‘¡+1)âˆ’Â¯ğ‘¥(ğ‘¡+1)2
+EÂ¯ğ‘¥(ğ‘¡+1)âˆ’ğ‘¥âˆ—2
,
where Â¯ğ‘¥(ğ‘¡+1):=ğ‘¥(ğ‘¡)+1
ğ‘›Ãğ‘›
ğ‘–=1(ğ‘¥(ğ‘¡,ğ‘‡)
ğ‘–âˆ’ğ‘¥(ğ‘¡))is a vector that would
equal the global model if the PS were to sample all the ğ‘›clients.
Observe that the first term on the RHS depends on ğ‘¥(ğ‘¡+1)âˆ’Â¯ğ‘¥(ğ‘¡+1),
which can be easily shown to be the difference between the random
average1
ğ‘š(ğ‘¡)Ã
ğ‘–âˆˆS(ğ‘¡)Î”ğ‘–(ğ‘¡)and the true average1
ğ‘›Ãğ‘›
ğ‘–=1
ğ‘¥(ğ‘¡,ğ‘‡)
ğ‘–âˆ’ğ‘¥(ğ‘¡)
.
Thus, this term captures the error due to random sampling. As the
next result shows, this difference depends on the network topol-
ogy as well as on ğ‘š(ğ‘¡), the number of clients selected for global
aggregation uniformly at random by the PS.
Proposition 4.3. Letğ›¿be the constant defined in (8). Then Algo-
rithm 1 satisfies the following for every ğ‘¡âˆˆNâˆª{0}:
Eğ‘¥(ğ‘¡+1)âˆ’Â¯ğ‘¥(ğ‘¡+1)2
â‰¤
2ğ‘‡ğœš2ğœ‚2
ğ‘¡+4ğ‘’ğ‘‡(ğœš2+2ğ›¿2)ğœ‚2
ğ‘¡+6ğ›¿2ğ‘‡2ğœ‚2
ğ‘¡
+(27+4ğ‘’)ğ‘‡2ğ›½2ğœ‚2
ğ‘¡Eğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—2
ğœ™(ğ‘¡),
whereğœ™(ğ‘¡)is the connectivity factor defined in (5).
In other words, Eğ‘¥(ğ‘¡+1)âˆ’Â¯ğ‘¥(ğ‘¡+1)depends on the previous op-
timality gap Eğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—2
viağœ™(ğ‘¡), i.e., the connectivity factor
that captures the combined effect of global aggregation (via ğ‘š(ğ‘¡))
and the D2D network topology within each cluster (via ğœ™â„“(ğ›¼â„“(ğ‘¡))).
Moreover, Lemma 4.2 and Proposition 4.3 together show that
the singular values of the equal-neighbor adjacency matrices can
be used to derive an upper bound on the expected optimality gap
(and ultimately establish theoretical performance guarantees) for
our connectivity-aware algorithm. Doing so yields the following.
Proposition 4.4. Letğ›¿be as defined in (5), letğœ™(ğ‘¡)be the connec-
tivity factor defined in (5), letÎ“:=ğ‘“(ğ‘¥âˆ—)âˆ’1
ğ‘›Ãğ‘›
ğ‘–=1minğ‘¥âˆˆRğ‘ğ‘“ğ‘–(ğ‘¥),
and letğ‘’denote the exponential constant. Then the expected optimality
gap of Algorithm 1 satisfies the following for all ğ‘¡âˆˆN0:
Eğ‘¥(ğ‘¡+1)âˆ’ğ‘¥âˆ—2
â‰¤
(1âˆ’ğœ‡ğœ‚ğ‘¡)ğ‘‡+(27+4ğ‘’)ğ‘‡2ğ›½2ğœ‚2
ğ‘¡(2ğ‘‡+ğœ™(ğ‘¡))
Eğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—2
+ğ‘‡ğœš2
ğ‘›+6ğ›½Î“+4ğ‘‡ğœš2+8ğ‘’ğ‘‡(ğœš2+2ğ›¿2)+12ğ›¿2ğ‘‡2
ğœ‚2
ğ‘¡
+
2ğ‘‡ğœš2+4ğ‘’ğ‘‡(ğœš2+2ğ›¿2)+6ğ›¿2ğ‘‡2
ğœ™(ğ‘¡)ğœ‚2
ğ‘¡.A recursive expansion on the inequality stated by Proposition 4.4
results in our main theoretical result, which we state below.
Theorem 4.5. Consider a connectivity factor threshold ğœ™maxâ‰¥0,
and suppose that ğœ™(ğ‘¡)â‰¤ğœ™maxfor all timesğ‘¡â‰¥0. In addition, suppose
ğœ‚ğ‘¡=4
ğ‘‡ğœ‡(ğ‘¡+ğ‘¡1), where
ğ‘¡1:=$
4
1âˆ’1
ğ‘‡
+(16ğ‘‡+8ğœ™max)ğ›½
ğœ‡2
+1%
.
Then the expected optimality gap of Algorithm 1 satisfies the following
for allğ‘¡â‰¥0:
Eğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—2
â‰¤ğ‘¡1
ğ‘¡+ğ‘¡12
Eğ‘¥(0)âˆ’ğ‘¥âˆ—2
+16
1
ğ‘›ğ‘‡ğœš
ğœ‡2
+6ğ›½Î“
ğ‘‡ğœ‡2
ğ‘¡+ğ‘¡1
+(32ğ‘‡+16ğœ™max)
2
ğ‘‡ğœš
ğœ‡2
+4ğ‘’
ğ‘‡ğœš
ğœ‡2
+2
ğ›¿
ğœ‡2
+6
ğ›¿
ğœ‡2
ğ‘¡+ğ‘¡1.(9)
Theorem 4.5 reveals that the convergence rate of our algorithm is
O(1/ğ‘¡), which coincides with that of FedAvg and its semi-decentralized
variants such as [ 19]. In fact,O(1/ğ‘¡)resembles the convergence
rate of vanilla centralized SGD. It also shows that suitably tuning
the connectivity factor (by choosing an appropriate value of ğœ™max)
is critical to the efficiency of the algorithm: as ğœ™maxincreases the
bound gets worse/larger; however, ğœ™max, by its definition, is non-
negative, which means it can at best be made equal to 0, which
forcesğ‘š=ğ‘›, in which case the inequality boils down to an upper
bound on the convergence rate of FedAvg with full device sampling.
At the other extreme, setting ğœ™maxtoâˆresults inğ‘š=1, which
happens when our semi-decentralized FL architecture collapses to
full decentralization.
Moreover, Theorem 4.5 jointly captures the effect of the follow-
ing factors on the expected instantaneous optimality gap and hence
on the convergence rate: (i) the initial optimality gap Eâˆ¥ğ‘¥(0)âˆ’ğ‘¥âˆ—âˆ¥2
(via the first term), (ii) The SGD noise variance ğœš2and the strong
convexityğœ‡and smoothness parameters ğ›½(via the second term),
and finally, (iii) the combined effect of cluster connectivity levels
and random sampling-based global aggregations (via the third term,
which depends on ğœ™max, which in turn prevents the connectivity
factorğœ™(ğ‘¡)from becoming too large). It can be seen that higher
values of the SGD noise variance ğœš2and the data heterogeneity
measure Î“lead to a larger value of the bound, implying that our
algorithm is sensitive to the size of the mini-batches used for com-
puting the stochastic gradients as well as to the non-i.i.d.-ness of
the local datasets.
5 SINGULAR VALUE BOUNDS
Having established the role of the connectivity factor ğœ™(ğ‘¡)in the
performance of Algorithm 1, we now analyze two important quan-
tities associated with ğœ™(ğ‘¡): the top two singular values of the equal-
neighbor adjacency matrices of the clusters. Since a precise estima-
tion of these singular values requires full knowledge of the cluster
topologies, which is challenging to obtain in practice, we are mo-
tivated to derive a set of novel upper bounds on these values inConnectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc â€™23, June 03â€“05, 2023, Washington, DC
terms of the node degrees of the cluster digraphs, which are easy
to obtain/measure in practice. To the best of our knowledge, this
is one of the first attempts at connecting the singular values of
adjacency matrices with minimal topological information such as
node degrees of the digraphs.
To conduct our analysis, for any digraph ğº=([ğ‘ ],ğ¸), we first
defineğœ€=ğœ€ğº:=ğ‘‘+
max(ğº)âˆ’ğ‘‘+
min(ğº)
ğ‘‘+
min(ğº),which quantifies the hetero-
geneity of out-degree of the nodes across the digraph. We also
letğ›¼(ğº):=ğ‘‘+
min(ğº)
ğ‘ capture the minimum fraction of the node
population that any node is out-connected to. In addition, we let
ğ‘Š(ğº)=(ğ‘¤ğ‘–ğ‘—)andğ·+(ğº):=diag([ğ‘‘+
1ğ‘‘+
2Â·Â·Â·ğ‘‘+ğ‘ ]âŠ¤)denote the
binary adjacency matrix and the out-degree matrix of ğº, respec-
tively. In the sequel, we drop the indexing (ğº)for brevity.
We are now equipped to state our first set of bounds on the great-
est two singular values of ğºunder certain regularity assumptions
on the digraph.
Proposition 5.1. Supposeğº=([ğ‘ ],ğ¸)is a directed graph in
which every node has its in-degree equal to its out-degree, i.e., ğ‘‘+
ğ‘–=ğ‘‘âˆ’
ğ‘–
for allğ‘–âˆˆ[ğ‘›]. Then the greatest two singular values ğœ1andğœ2of
the equal-neighbor adjacency matrix ğ´ofğºsatisfy the following
inequalities for ğ›¼>1
2andğœ€â‰ª1:
ğœ2
1â‰¤1+ğœ€+O(ğœ€2), (10)
ğœ2
2â‰¤1
ğ›¼âˆ’12
+2ğœ€
1+2
ğ›¼âˆ’1
ğ›¼2
+O(ğœ€2), (11)
whereO(Â·) is the big-ğ‘‚notation used in the context of ğœ€â†’0.
Proof. To simplify our notation, we define ğ·:=ğ·+for the
remainder of this proof. Observe that
ğ´âŠ¤=ğ·âˆ’1ğ‘Š=ğ·âˆ’1
2(ğ·âˆ’1
2ğ‘Šğ·âˆ’1
2)ğ·1
2,
which means ğ´âŠ¤is similar to the normalized adjacency matrix
defined asğ´ğ‘:=ğ·âˆ’1
2ğ‘Šğ·âˆ’1
2.
On the other hand, we have ğ‘‘+
min=ğ‘‘+max(1âˆ’ğœ€)â‰¤ğ‘‘+
ğ‘–â‰¤ğ‘‘+maxfor
allğ‘–âˆˆ[ğ‘ ], which implies the existence of a diagonal matrix ğ¸3such
thatğ‘‚â‰¤ğ¸3â‰¤ğ¼andğ·=ğ‘‘+max((1âˆ’ğœ€)ğ¼+ğœ€ğ¸3). Using similar argu-
ments, it can be easily shown that there exist diagonal matrices ğ¸1
andğ¸2such thatğ‘‚â‰¤ğ¸1,ğ¸2â‰¤ğ¼,ğ·1
2=âˆšï¸
ğ‘‘+max (1âˆ’ğœ€
2)ğ¼+ğœ€
2ğ¸1+
O(ğœ€2), andğ·âˆ’1
2=1âˆš
ğ‘‘+max ğ¼+ğœ€
2ğ¸2+O(ğœ€2). As a result, the follow-
ing holds up to an additive error of O(ğœ€2):
ğ´ğ‘=ğ·1
2ğ´âŠ¤ğ·âˆ’1
2=âˆšï¸ƒ
ğ‘‘+max
(1âˆ’ğœ€
2)ğ¼+ğœ€
2ğ¸1ğ´âˆšï¸
ğ‘‘+max
ğ¼+ğœ€
2ğ¸2
=ğ´âŠ¤+ğœ€
2((ğ¸1âˆ’ğ¼)ğ´+ğ´ğ¸2),
i.e.,ğ´âŠ¤âˆ’ğ´ğ‘=âˆ’ğœ€
2 (ğ¸1âˆ’ğ¼)ğ´âŠ¤+ğ´âŠ¤ğ¸2+O(ğœ€2). In conjunction
with standard bounds on singular value perturbations (e.g., see [ 22]),this implies the following up to an additive error of O(ğœ€2):
ğœğ‘—(ğ´)=ğœğ‘—(ğ´âŠ¤)
â‰¤ğœğ‘—(ğ´ğ‘)+ğœ€
2(ğ¸1âˆ’ğ¼)ğ´âŠ¤+ğ´âŠ¤ğ¸2
â‰¤âˆšï¸ƒ
ğœ†ğ‘—(ğ´ğ‘ğ´âŠ¤
ğ‘)+ğœ€
2 âˆ¥ğ¸1âˆ’ğ¼âˆ¥ğ´âŠ¤+ğ´âŠ¤âˆ¥ğ¸2âˆ¥
(ğ‘)
â‰¤âˆšï¸ƒ
ğœ†ğ‘—(ğ·âˆ’1
2ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤ğ·âˆ’1
2)+ğœ€
2(1Â·ğ´âŠ¤+ğ´âŠ¤Â·1)
(ğ‘)=âˆšï¸ƒ
ğœ†ğ‘—(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤)+ğœ€ğœ1(ğ´âŠ¤). (12)
Here,(ğ‘)holds because ğ¼âˆ’ğ¸1being a diagonal matrix along with
ğ‘‚â‰¤ğ¼âˆ’ğ¸1â‰¤ğ¼implies thatâˆ¥ğ¼âˆ’ğ¸1âˆ¥=maxğ‘–âˆˆ[ğ‘ ]|(ğ¼âˆ’ğ¸1)ğ‘–ğ‘–|â‰¤1.
(ğ‘)holds because ğœ1(ğ´âŠ¤)=ğ´âŠ¤and because ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤and
ğ·âˆ’1
2ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤ğ·âˆ’1
2, being similar, have the same eigenvalues.
We now bound ğœ1(ğ´)andğœ2(ğ´)individually. As for ğœ1(ğ´), the
derivation (12) and the fact that ğœ1(ğ´)=ğœ1(ğ´âŠ¤)imply that
ğœ1(ğ´)â‰¤âˆšï¸
ğœ†1(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘Šğ‘‡)
1âˆ’ğœ€=âˆšï¸ƒ
ğœ†1(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘Šğ‘‡)(1+ğœ€)
+O(ğœ€2). (13)
So, it is enough to bound ğœ†1(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘Šğ‘‡). For this purpose, note
thatğ´being column-stochastic implies that ğ·âˆ’1ğ‘Š1=1and hence
also thatğ‘Š1=ğ·1. Besides, our assumption on in-degrees and out-
degrees can be expressed asÃğ‘ 
ğ‘—=1ğ‘¤ğ‘–ğ‘—=Ãğ‘ 
ğ‘—=1ğ‘¤ğ‘—ğ‘–for eachğ‘–âˆˆ[ğ‘ ], or
equivalently, ğ‘ŠâŠ¤1=ğ‘Š1=ğ·1. As a result, we have ğ·âˆ’1ğ‘ŠâŠ¤1=1.
Thus,ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤=ğ´âŠ¤ğ·âˆ’1ğ‘ŠâŠ¤is a product of row-stochastic
matrices and hence, it is row-stochastic in itself. Consequently,
ğœ†1(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘Šğ‘‡)=1. In light of this, (13) implies (10).
It remains to prove (11). We do this by using Theorem 2.2 of [ 20],
which helps derive a bound in terms of ğœ1and the minimum positive
entryğ›¿of the matrix ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘Šğ‘‡. We first note that
(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘Šğ‘‡)ğ‘–ğ‘—(ğ‘)
â‰¥1
(ğ‘‘+max)2ğ‘ âˆ‘ï¸
ğ‘˜=1(ğ‘Š)ğ‘–ğ‘˜(ğ‘ŠâŠ¤)ğ‘˜ğ‘—
=1
(ğ‘‘+max)2|{ğ‘˜âˆˆ[ğ‘ ]:ğ‘¤ğ‘–ğ‘˜=ğ‘¤ğ‘—ğ‘˜=1}|(ğ‘)
â‰¤(2ğ›¼âˆ’1)ğ‘ 
(ğ‘‘+max)2,
where(ğ‘)follows from the fact that ğ·âˆ’1â‰¥1
ğ‘‘+maxğ¼and(ğ‘)holds
because the number of common out-neighbors of any two nodes
ğ‘–,ğ‘—âˆˆ[ğ‘ ]is at least(2ğ›¼âˆ’1)ğ‘ . We can now apply Theorem 2.2 of [ 20]
by settingğ‘¥=1âˆšğ‘ in the theorem (because1âˆšğ‘ 1, as explained above,
is the unit-norm principal eigenvector of ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤). Thus,
ğœ†2(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤)â‰¤ğœ†1(ğ·âˆ’1ğ‘Šğ·âˆ’1ğ‘ŠâŠ¤)âˆ’(2ğ›¼âˆ’1)ğ‘ 2
(ğ‘‘+max)2
=1âˆ’2
ğ›¼âˆ’1
ğ›¼2
(1âˆ’2ğœ€)+O(ğœ€2), (14)
where the last step follows from the observation that ğ‘‘+max=ğ›¼ğ‘ 
1âˆ’ğœ€.
Combining (10), (12) and (14) now gives
ğœ2(ğ´)â‰¤âˆšï¸„
1âˆ’2
ğ›¼âˆ’1
ğ›¼2
(1âˆ’2ğœ€)+O(ğœ€2)+ğœ€
1+ğœ€+O(ğœ€)2
.
Squaring both sides and rearranging the terms results in (11). â–¡MobiHoc â€™23, June 03â€“05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
Remark 1. Observing the bounds in Proposition 5.1, we can see
that setting ğ›¼=1, which corresponds to ğºbeing a clique, in the
bounds yields ğœ1â‰¤1+O(ğœ€)andğœ2=O(ğœ€). These inequalities, for
ğœ€â‰ª1, are tight with respect to the well-known lower bounds ğœ1â‰¥1
andğœ2â‰¥0. This implies that the bounds (10)and(11)can be expected
to be reasonably tight for high edge density (i.e., whenever ğ›¼â‰ˆ1).
Another implication of the bounds is decreasing ğœ€, which inherently
measures how irregular the digraph is, leads to (11)becoming sharper.
The above singular value bounds are especially tight for digraphs
that are approximately regular (or digraphs that do not exhibit
significant variations in their in-degrees and out-degrees) since
such digraphs satisfy ğœ€â‰ª1. This happens in practice, when the
D2D clusters are dense (e.g., in the wireless setting, when the nodes
are closer to each other or when they can move and communicate
over time). Furthermore, the same holds for the condition on ğ›¼in
Proposition 5.1 (i.e., ğ›¼>1
2), which is always met when the clusters
are dense.
However, the bounds (10)and(11)are obtained under the as-
sumption that every node has its in-degree equal to its out-degree,
which can be restrictive in practical settings. This observation fur-
ther motivates us to find a new set of singular value bounds that
work well under milder assumptions. We thus provide the following
bounds, which not only relax the said restrictive assumption, but
also apply to digraphs with more general out-degree distributions
(and hence subsume digraphs with wider out-degree variations).
Proposition 5.2. Letğœ‘=ğ‘‘in
maxâˆ’ğ‘‘min
ğ‘‘min, whereğ‘‘inmax denotes the
maximum in-degree of the digraph ğº. Ifğ›¼â‰¥1
2, we have the following
bounds:
ğœ2
1â‰¤1+ğœ‘, (15)
ğœ2
2â‰¤1+ğœ‘âˆ’(1âˆ’ğœ€)2(1âˆ’ğ›¼2
âˆ’1) (1âˆ’ğœ€)2(1âˆ’ğ›¼2
âˆ’1)âˆ’ğ›¼âˆ’1
ğ‘ (ğœ€net+1)
ğœ€netâˆ’ğ›¼âˆ’1+1
ğ›¼ğ‘ ,(16)
whereğœ€net:=ğœ‘+ğœ€
ğ›¼andğ›¼âˆ’1:=1
ğ›¼âˆ’1.
The bounds obtained in Proposition 5.2 ( proved in the appen-
dix) are particularly effective when the D2D cluster digraphs are
dense but irregular. This is often the case in practical systems, when
there is communication heterogeneity (e.g., in wireless sensor net-
works consisting of sensors with different radii).
In conjunction with Theorem 4.5, the bounds derived in Propo-
sitions 5.1 and 5.2 capture the inherent dependence of the expected
optimality gap, and hence that of the convergence rate, on the de-
gree distributions of the D2D clusters. In particular, upon having
approximately regular D2D clusters, the bounds in Propositions 5.1
along with the result of Theorem 4.5 determine the convergence rate
of Algorithm 1. The same holds when using the result of Proposi-
tion 5.2 with Theorem 4.5, which will characterize the convergence
rate of Algorithm 1 upon having irregular D2D clusters.
6 NUMERICAL VALIDATION
We now conduct numerical experiments to validate our methodol-
ogy. Overall, our simulations show that compared with baselines,
Algorithm 1 obtains significant reductions in total communication
cost for the same or similar levels of testing accuracy.6.1 Implementation
6.1.1 Network Architecture. We simulate a network consisting of
ğ‘›=70edge devices partitioned into ğ‘=7clusters with ğ‘›â„“=10
nodes per cluster. In every global aggregation round, the digraph
for each cluster â„“âˆˆ[ğ‘]is constructed as follows: (i) we generate
ağ‘˜-regular directed graph (a digraph in which every node has its
in-degree and out-degree equal to ğ‘˜) with the value of ğ‘˜being
chosen uniformly at random from the set {6,..., 9}; (ii) we delete
a fractionğ‘âˆˆ(0,1)of the directed edges uniformly at random
so as to incorporate D2D link failures due to client mobility and
bandwidth issues. The result is an approximately regular digraph
whose degree distribution may deviate significantly from that of
regular digraphs, while satisfying ğ›¼â„“(ğ‘¡)>1
2.
6.1.2 Datasets. All our simulations are performed on MNIST [ 35]
and Fashion-MNIST (F-MNIST) [ 32] datasets. The MNIST dataset
consists of 70K images (60K for training and 10K for testing), and
each image is a hand-written digit between 0 to 9 (i.e., the dataset
has 10 labels). The same applies to the FMNIST dataset, the only
difference being that it consists of images of fashion products.
6.1.3 ML Models and Implementation. We use the neural network
model from [ 21] in our simulations. In particular, we use a convolu-
tional neural network (CNN) with two 5Ã—5convolution layers, the
first of which has 32 channels and the second 64 channels, where
each of these layers precedes a 2Ã—2max pooling, resulting in a total
model dimension of 1,663,370. We use the PyTorch implementation
of this setup provided in [ 11] with cross-entropy loss. Each dataset
is distributed among the clients in a non-i.i.d. manner: the samples
(from either of the two datasets) are first sorted by their labels,
partitioned into chunks of equal size, and each of the 70 clients is
assigned only two chunks (i.e., each client will end up having only
two labels). This results in extreme data heterogeneity, which leads
to strong empirical guarantees for our approach.
All of our simulations are performed using the following hyper-
parameter values/ranges: ğ‘‡=5,ğ‘¡maxâˆˆ{15,30},ğ‘âˆˆ{0.1,0.2}, and
ğœ‚ğ‘¡=0.02(0.1)ğ‘¡whereğ‘¡is the global aggregation index.
6.2 Results
We compare the energy vs. accuracy trade-offs associated with
Algorithm 1 with those associated with two baselines, FedAvg [ 21]
and collaborative relaying (COLREL) [ 36]. The second baseline is
a recently proposed semi-decentralized FL algorithm that incor-
porates single-step consensus updates. Under the D2D and D2S
connectivity constraints introduced in Section 2, COLREL is a vari-
ant of FedAvg that incorporates one round of column-stochastic
D2D aggregations before every global aggregation round but does
not provide any criterion to control the sampling size ğ‘š, which we
assume to be fixed throughout its implementation. The fundamen-
tal difference between our method and COLREL is that our method
takes into account the change in the connectivity of D2D clusters,
optimally tuning the value of ğ‘šaccording to the set of novel upper
bounds on the singular values we obtained in Section 5.
We consider these tradeoffs under different D2S connectivity
levels. Intuitively speaking, on one hand, as the D2S connectivity
improves, we expect to see that our algorithm leads to a lower
energy and cost savings as compared to FedAvg. This is becauseConnectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc â€™23, June 03â€“05, 2023, Washington, DC
Figure 2: Communication cost vs. testing accuracy under high
D2S connectivity (Dataset: MNIST).
Figure 3: Communication cost vs. testing accuracy under high
D2S connectivity (Dataset: F-MNIST).
our algorithm will naturally collapse to FedAvg and D2D commu-
nications will become less useful since more devices would engage
in uplink communications, which by itself degrades the benefit of
D2D local aggregations. On the other hand, as D2S connectivity
improves, we expect to see that our algorithm achieves significant
energy savings as compared to COLREL. This is because the impact
of tuningğ‘šbecomes more prominent when there is a possibility
of D2S communications.
All of the following plots and discussion are based on the as-
sumption that the ratio of the energy required for D2D commu-
nication to that of up-link (D2S) transmission, denoted byğ¸D2D
ğ¸Glob,
equals 0.1. This is a pessimistic estimate in favor of D2S consid-
ering that most ratios reported in the literature [ 8,19,38] take
values less than 0.1. Thus, the communication costs reported are
(#D2S transmissions)+0.1Ã—(#D2D transmissions ).
6.2.1 Case 1: Cost savings under high D2S connectivity and a low link
failure probability. When the PS has a high downlink bandwidth
and the connectivity between the devices and the PS is reliable,
implementing FedAvg or COLREL has the effect of setting ğ‘što a
value close to ğ‘›. As an example, we implement FedAvg and COLREL
withğ‘š=57andğ‘š=52, respectively (note that COLREL requires
fewer up-link transmissions because it uses D2D consensus up-
dates in addition to global aggregations). The results for MNIST are
shown in Fig. 2: choosing ğœ™max=0.06and a low D2D link failure
Figure 4: Communication cost vs. testing accuracy under low
D2S connectivity (Dataset: MNIST).
Figure 5: Communication cost vs. testing accuracy under low
D2S connectivity (Dataset: F-MNIST).
probabilityğ‘=0.1results in Algorithm 1 achieving a testing accu-
racy of 90% while consuming about 46%less energy than FedAvg
(thereby incurring proportionately lower communication costs).
With respect to COLREL, the energy saving is even higher because
COLREL also expends energy on D2D aggregations with relatively
little gain in testing accuracy.
Repeating this experiment on FMNIST results in a similar per-
formance, depicted in Fig. 3. We see that Algorithm 1 (with ğœ™max=
0.06) consumes about 30% less energy than COLREL for achieving
a testing accuracy of 70%.
6.2.2 Case 2: Cost savings under low D2S connectivity and a high
link failure probability. When the connectivity between the devices
and the PS is poor, implementing FedAvg or COLREL has the effect
of settingğ‘što a value significantly smaller than ğ‘›. As an example,
we implement FedAvg and COLREL with ğ‘š=26andğ‘š=15,
respectively. Choosing ğœ™max=0.2and a high D2D link failure
probability ğ‘=0.2results in Algorithm 1 consuming about 30%
less energy than FedAvg for achieving a testing accuracy of 90% on
MNIST, as shown in Fig. 4. The cost saving is lower than in Case 1
as we expect because the singular value bounds incorporated by
our algorithm into its choice of ğ‘š(ğ‘¡)are looser for higher values of
the link failure probability ğ‘. Repeating this experiment on FMNIST
results in a similar performance, depicted in Fig. 5.MobiHoc â€™23, June 03â€“05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
7 CONCLUSION
We have investigated consensus-based semi-decentralized learning
over clustered D2D networks modeled as time-varying digraphs.
We first revealed the connection between the singular values of
the column-stochastic matrices used for D2D model aggregations
and the convergence rate of the learning process. We then derived
a set of novel upper bounds on these singular values in terms of
the degree distributions of the cluster digraphs, and we used the
resulting bounds to design a novel connectivity-aware FL algorithm
that enables the central parameter server to tune the number of
up-link transmissions by using its knowledge of the time-varying
degree distributions of clusters. Our algorithm maintains a continu-
ous balance between the number of model aggregations occurring
at the server and those occurring over the edge network, thereby
enhancing the resource efficiency of the learning process without
compromising convergence.
Future works include obtaining upper bounds on singular values
under more general assumptions, and obtaining optimal device
sampling schemes for irregular clusters.
REFERENCES
[1]Mohammad Akbari, Bahman Gharesifard, and TamÃ¡s Linder. 2015. Distributed
online convex optimization on time-varying directed graphs. IEEE Transactions
on Control of Network Systems 4, 3 (2015), 417â€“428.
[2]Mohammed S Al-Abiad, Mohanad Obeed, Md Hossain, Anas Chaaban, et al .2022.
Decentralized aggregation for energy-efficient federated learning via overlapped
clustering and D2D communications. arXiv preprint arXiv:2206.02981 (2022).
[3]AurÃ©lien Bellet, Anne-Marie Kermarrec, and Erick Lavoie. 2022. D-cliques: Com-
pensating for data heterogeneity with topology in decentralized federated learn-
ing. In 2022 41st International Symposium on Reliable Distributed Systems (SRDS) .
IEEE, 1â€“11.
[4]Enrique TomÃ¡s MartÃ­nez BeltrÃ¡n, Mario Quiles PÃ©rez, Pedro Miguel SÃ¡nchez
SÃ¡nchez, Sergio LÃ³pez Bernal, GÃ©rÃ´me Bovet, Manuel Gil PÃ©rez, Grego-
rio MartÃ­nez PÃ©rez, and Alberto Huertas CeldrÃ¡n. 2022. Decentralized Federated
Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges.
arXiv preprint arXiv:2211.08413 (2022).
[5]Aleksandr Beznosikov, Pavel Dvurechensky, Anastasia Koloskova, Valentin
Samokhin, Sebastian U Stich, and Alexander Gasnikov. 2021. Decentralized
local stochastic extra-gradient for variational inequalities. arXiv preprint
arXiv:2106.08315 (2021).
[6]Christopher Briggs, Zhong Fan, and Peter Andras. 2020. Federated learning with
hierarchical clustering of local updates to improve training on non-IID data. In
2020 International Joint Conference on Neural Networks (IJCNN) . IEEE, 1â€“9.
[7]Chaoyang He, Conghui Tan, Hanlin Tang, Shuang Qiu, and Ji Liu. 2019. Central
server free federated learning over single-sided trust social networks. arXiv
preprint arXiv:1910.04956 (2019).
[8]Mariem Hmila, Manuel FernÃ¡ndez-Veiga, Miguel Rodriguez-Perez, and Sergio
HerrerÃ­a-Alonso. 2019. Energy efficient power and channel allocation in underlay
device to multi device communications. IEEE transactions on communications 67,
8 (2019), 5817â€“5832.
[9]Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G Brinton, Nicolo
Michelusi, Vaneet Aggarwal, David J Love, and Huaiyu Dai. 2022. Multi-stage
hybrid federated learning over large-scale D2D-enabled fog networks. IEEE/ACM
Transactions on Networking 30, 4 (2022), 1569â€“1584.
[10] Yifan Hua, Kevin Miller, Andrea L Bertozzi, Chen Qian, and Bao Wang. 2022.
Efficient and reliable overlay networks for decentralized federated learning. SIAM
J. Appl. Math. 82, 4 (2022), 1558â€“1586.
[11] Shaoxiong Ji. 2018. A PyTorch Implementation of Federated Learning.
[12] Anastasiia Koloskova, Tao Lin, and Sebastian U Stich. 2021. An improved analysis
of gradient tracking for decentralized machine learning. Advances in Neural
Information Processing Systems 34 (2021), 11422â€“11435.
[13] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian
Stich. 2020. A unified theory of decentralized sgd with changing topology and
local updates. In International Conference on Machine Learning . PMLR, 5381â€“5393.
[14] Jakub KoneÄn `y, H Brendan McMahan, Felix X Yu, Peter RichtÃ¡rik,
Ananda Theertha Suresh, and Dave Bacon. 2016. Federated learning: Strategies
for improving communication efficiency. arXiv preprint arXiv:1610.05492 (2016).[15] Anusha Lalitha, Shubhanshu Shekhar, Tara Javidi, and Farinaz Koushanfar. 2018.
Fully decentralized federated learning. In Third workshop on bayesian deep learn-
ing (NeurIPS) , Vol. 2.
[16] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. Federated
learning: Challenges, methods, and future directions. IEEE signal processing
magazine 37, 3 (2020), 50â€“60.
[17] Shu Liang, George Yin, et al .2019. Dual averaging push for distributed convex
optimization over time-varying directed graph. IEEE Trans. Automat. Control 65,
4 (2019), 1785â€“1791.
[18] Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam, Christo-
pher G Brinton, and NicolÃ² Michelusi. 2021. Federated learning beyond the star:
Local D2D model consensus with global cluster sampling. In 2021 IEEE Global
Communications Conference (GLOBECOM) . IEEE, 1â€“6.
[19] Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam, Christo-
pher G Brinton, and Nicolo Michelusi. 2021. Semi-decentralized federated learn-
ing with cooperative D2D local model aggregations. IEEE Journal on Selected
Areas in Communications 39, 12 (2021), 3851â€“3869.
[20] M Stuart Lynn and William P Timlake. 1969. Bounds for Perron eigenvectors and
subdominant eigenvalues of positive matrices. Linear Algebra Appl. 2, 2 (1969),
143â€“152.
[21] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics . PMLR,
1273â€“1282.
[22] Carl D Meyer. 2000. Matrix analysis and applied linear algebra . Vol. 71. Siam.
[23] Angelia NediÄ‡ and Alex Olshevsky. 2014. Distributed optimization over time-
varying directed graphs. IEEE Trans. Automat. Control 60, 3 (2014), 601â€“615.
[24] Angelia NediÄ‡ and Alex Olshevsky. 2016. Stochastic gradient-push for strongly
convex functions on time-varying directed graphs. IEEE Trans. Automat. Control
61, 12 (2016), 3936â€“3947.
[25] Angelia Nedic, Alex Olshevsky, and Wei Shi. 2017. Achieving geometric conver-
gence for distributed optimization over time-varying graphs. SIAM Journal on
Optimization 27, 4 (2017), 2597â€“2633.
[26] Dan Shen, Genshe Chen, Jose B Cruz, and Erik Blasch. 2008. A game theoretic
data fusion aided path planning approach for cooperative UAV ISR. In 2008 IEEE
Aerospace Conference . IEEE, 1â€“9.
[27] Tao Sun, Dongsheng Li, and Bao Wang. 2022. Decentralized federated averaging.
IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
[28] Bin Wang, Jun Fang, Hongbin Li, Xiaojun Yuan, and Qing Ling. 2022. Confed-
erated Learning: Federated Learning with Decentralized Edge Servers. arXiv
preprint arXiv:2205.14905 (2022).
[29] Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian
Makaya, Ting He, and Kevin Chan. 2019. Adaptive federated learning in re-
source constrained edge computing systems. IEEE journal on selected areas in
communications 37, 6 (2019), 1205â€“1221.
[30] Zheng Wang and Huaqing Li. 2019. Edge-based stochastic gradient algorithm for
distributed optimization. IEEE Transactions on Network Science and Engineering
7, 3 (2019), 1421â€“1430.
[31] Zutong Wang, Mingfa Zheng, Jiansheng Guo, and Hanqiao Huang. 2017. Un-
certain UAV ISR mission planning problem with multiple correlated objectives.
Journal of Intelligent & Fuzzy Systems 32, 1 (2017), 321â€“335.
[32] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[33] Ran Xin and Usman A Khan. 2018. A linear algorithm for optimization over
directed graphs with geometric convergence. IEEE Control Systems Letters 2, 3
(2018), 315â€“320.
[34] Hong Xing, Osvaldo Simeone, and Suzhi Bi. 2021. Federated learning over wireless
device-to-device networks: Algorithms and convergence analysis. IEEE Journal
on Selected Areas in Communications 39, 12 (2021), 3723â€“3741.
[35] L Yan, C Corinna, and CJ Burges. 1998. The MNIST dataset of handwritten digits.
[36] Michal Yemini, Rajarshi Saha, Emre Ozfatura, Deniz GÃ¼ndÃ¼z, and Andrea J Gold-
smith. 2022. Semi-decentralized federated learning with collaborative relaying. In
2022 IEEE International Symposium on Information Theory (ISIT) . IEEE, 1471â€“1476.
[37] Shahryar Zehtabi, Seyyedali Hosseinalipour, and Christopher G Brinton. 2022.
Event-Triggered Decentralized Federated Learning over Resource-Constrained
Edge Devices. arXiv preprint arXiv:2211.12640 (2022).
[38] Aiqing Zhang and Xiaodong Lin. 2017. Security-aware and privacy-preserving
D2D communications in 5G. IEEE Network 31, 4 (2017), 70â€“77.