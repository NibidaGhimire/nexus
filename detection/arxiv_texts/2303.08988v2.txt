Connectivity-Aware Semi-Decentralized Federated Learning over
Time-Varying D2D Networks
Rohit Parasnis∗
Purdue University
West Lafayette, Indiana, USA
rparasni@purdue.eduSeyyedali Hosseinalipour
University at Buffalo–SUNY
Buffalo, New York, USA
alipour@buffalo.eduYun-Wei Chu
Purdue University
West Lafayette, Indiana, USA
chu198@purdue.edu
Mung Chiang
Purdue University
West Lafayette, USA
chiang@purdue.eduChristopher G. Brinton
Purdue University
West Lafayette, USA
cgb@purdue.edu
ABSTRACT
Semi-decentralized federated learning blends the conventional device-
to-server (D2S) interaction structure of federated model training
with localized device-to-device (D2D) communications. We study
this architecture over practical edge networks with multiple D2D
clusters modeled as time-varying and directed communication
graphs. Our investigation results in an algorithm that controls
the fundamental trade-off between (a) the rate of convergence of
the model training process towards the global optimizer, and (b)
the number of D2S transmissions required for global aggregation.
Specifically, in our semi-decentralized methodology, D2D consen-
sus updates are injected into the federated averaging framework
based on column-stochastic weight matrices that encapsulate the
connectivity within the clusters. To arrive at our algorithm, we
show how the expected optimality gap in the current global model
depends on the greatest two singular values of the weighted adja-
cency matrices (and hence on the densities) of the D2D clusters.
We then derive tight bounds on these singular values in terms of
the node degrees of the D2D clusters, and we use the resulting
expressions to design a threshold on the number of clients required
to participate in any given global aggregation round so as to ensure
a desired convergence rate. Simulations performed on real-world
datasets reveal that our connectivity-aware algorithm reduces the
total communication cost required to reach a target accuracy sig-
nificantly compared with baselines depending on the connectivity
structure and the learning task.
CCS CONCEPTS
•Computer systems organization →Distributed architec-
tures ;Peer-to-peer architectures ;•Networks→Topology
analysis and generation .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MobiHoc ’23, June 03–05, 2023, Washington, DC
©2023 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/XXXXXXX.XXXXXXXKEYWORDS
connectivity, semi-decentralized, federated learning
ACM Reference Format:
Rohit Parasnis, Seyyedali Hosseinalipour, Yun-Wei Chu, Mung Chiang,
and Christopher G. Brinton. 2023. Connectivity-Aware Semi-Decentralized
Federated Learning over Time-Varying D2D Networks. In Proceedings of
Submitted (MobiHoc ’23). ACM, New York, NY, USA, 10 pages. https://doi.
org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Federated learning (FL) [ 14,21] is a popular paradigm for distribut-
ing machine learning (ML) tasks over a network of centrally coor-
dinated devices. By not requiring the devices to share any training
data with the central coordinator (server), FL improves privacy
and communication efficiency. The first FL technique, known as
federated averaging (FedAvg), was proposed in [ 14,21] as a dis-
tributed optimization algorithm for a “star” topology-based network
architecture. In each iteration of the FedAvg algorithm, (i) devices
individually performs a number of local stochastic gradient descent
(SGD) iterations and transmit their cumulative stochastic gradients
to the central server, which then (ii) aggregates a random subset
of these gradients to estimate the globally optimal ML model. In
recent years, several variants of FedAvg have been proposed to
address the challenges encountered by FL at the wireless edge, in-
cluding different dimensions of heterogeneity in dataset statistics
(e.g., varying local data distributions) and in the network system
itself (e.g., varying communication and computation capabilities).
An emerging arch of work has been exploring FL under edge
networks that diverge from the star learning topology between the
devices and the server. This had led to varying degrees of decentral-
ization in FL, reaching fully decentralized, serverless settings that sit
at the opposite extreme of the star topology [ 4,10,15,16,18,27,37].
In between these two extremes is semi-decentralized FL , where
device-to-device (D2D) communications complement device-to-
server (D2S) interactions [ 6,9,19,36]. These D2D interactions
occur locally within clusters of devices, with each cluster forming
a connected component. In semi-decentralized FL, D2D transmis-
sions are less energy-consuming than D2S interactions and can help
reduce the frequency of D2S communications through localized
synchronizations of the ML model updates.
Despite these recent investigations, we still do not have a clear
understanding of how different D2D topology properties impactarXiv:2303.08988v2  [cs.DC]  20 Jul 2023MobiHoc ’23, June 03–05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
Figure 1: Conventional Federated Learning vs. Connectivity-
Aware Semi-Decentralized Learning Architecture
the learning process. For instance, the ratio of the number of D2D
interactions to that of D2S interactions will impact the training effi-
ciency differently over different topologies. This becomes especially
important in the presence of constraints such as upload/download
bandwidths, and stochastic uncertainties such as data heterogeneity,
client mobility, and communication link failures. On one hand, edge
devices in clustered D2D networks that have little to no cross-cluster
interactions are typically in contact with only a small fraction of
the rest of the network at any given time instant (e.g., networks of
unmanned aerial vehicle (UAV) swarms spread over geo-distributed
regions separated by long distances). In such networks, if there
is no central coordinator (implying zero D2S interactions) and if
the training data are distributed heteregeneously among the edge
devices, no practically feasible number of D2D interactions is likely
to aggregate a set of local ML models that are diverse enough to
approximate the global data distribution [3].
On the other hand, having a high number of D2D interactions is
advantageous when D2S interactions take the form of high-latency,
high-energy transmissions (e.g., if the UAV swarms in the previous
example are miles away from the nearest base station). Moreover,
classical star-topology-based FL architectures miss out on an im-
portant benefit of D2D cooperation: devices acting as information
relays between other devices and the server, effectively sharing
with the server more information than it would expect to receive.
We are thus motivated to conduct a formal study of semi-decentralized
FL, and reveal the combined impact of D2S and D2D interactions
on the training process. After building an understanding of the
D2D topologies on which the D2D interactions occur, we propose a
novel FL technique that enables us to take into account the degree
distributions of the D2D clusters and use this knowledge to tune
the number of expensive D2S transmissions while simultaneously
ensuring a minimum rate of global training convergence. As shown
in Fig. 1, we incorporate two scales of model aggregations: on the
first scale, the edge devices perform intra-cluster model aggrega-
tions with their one-hop neighbors via distributed averaging, and
on the second scale, a central server samples a random set of clients
(as in the classical FedAvg architecture [ 21]) for global aggregation.
Our methodology has several potential use-cases, including the
following that we will refer to as examples throughout the paper:
(1)UAV Networks for ISR: UAVs are being increasingly de-
ployed for intelligence, surveillance, and reconnaissance
(ISR) operations in defense settings [ 26,31]. With the UAVspartitioned into D2D-enabled swarms deployed across dif-
ferent areas, our connectivity-aware algorithm can facilitate
intra-cluster communications and reduce the over-reliance
of the model training process on D2S transmissions.
(2)Self-driving cars: Many learning tasks for self-driving cars
call for vehicles to communicate over short distances. In such
settings, geographical proximity can be used to partition the
traffic network into clusters, which would enable us to design
intra-cluster D2S communications that turn out to be more
efficient than D2S communications with a far-away server.
1.1 Summary of Contributions
Our key contributions are summarized below:
(1)Analysis with Time-Varying and Directed Cluster Topolo-
gies: We consider that each D2D cluster in general is a time-
varying directed graph (digraph). We show how the expected
optimality gap of the learning process depends on the great-
est two singular values of the weighted adjacency matrices
used for local aggregations in the clusters. Our analysis is
applicable to edge networks with asymmetric D2D commu-
nications subjected to link failures.
(2)Singular Value Bounds in terms of Node Degrees: We
derive bounds on the singular values of the cluster-specific
weighted adjacency matrices in terms of the degree distri-
bution of every cluster. This introduces new technical chal-
lenges as described in Section 1.2, since it is a stark departure
from existing analyses of consensus-based FL algorithms
that rely heavily on the spectral gaps of symmetric weight
matrices (e.g., see [5, 9, 12, 13, 19, 34, 37]).
(3)Connectivity-Aware Learning Algorithm: We use our sin-
gular value bounds to design a time-varying threshold on
the number of clients required to be sampled by the cen-
tral server for global aggregation so as to enforce a desired
convergence rate while simultaneously reducing the num-
ber of D2S communications. This tradeoff results in a novel
connectivity-aware algorithm with significant energy sav-
ings, as validated subsequently by our numerical results.
(4)Effect of Data Heterogeneity under Mild Gradient Di-
versity Assumptions: We derive a bound on the expected
optimality gap that captures the effects of cluster densities
as well as the extent of data heterogeneity across the de-
vices. In doing so, we employ a milder definition of gradient
diversity [19] than what is typically assumed in literature.
Notation: We denote the set of real numbers by Rand the set of
positive integers by N. For any𝑛∈N, we define[𝑛]:={1,2,...,𝑛}.
For a finite set 𝑆, we denote its cardinality by |𝑆|.
We denote the vector space of 𝑛-dimensional real-valued column
vectors by R𝑛. We use the superscript notation⊤to denote the
transpose of a vector or a matrix. All matrix and vector inequalities
are assumed to hold entry-wise. We use 𝐼to denote the identity
matrix (of the known dimension) and 1to denote the column vector
(of the known dimension) that has all entries equal to 1. Similarly, 0
denotes the all-zeroes vector. In addition, we use ∥·∥to denote the
Euclidean norm of a square matrix or a vector, and for any vector
𝑣∈R𝑛we use diag(𝑣)to denote the diagonal matrix whose 𝑖-th
diagonal entry is 𝑣𝑖.Connectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc ’23, June 03–05, 2023, Washington, DC
We say that a vector 𝑣∈R𝑛isstochastic if𝑣≥0and𝑣⊤1=1,
and a matrix 𝐴iscolumn-stochastic if𝐴is non-negative and if each
column of𝐴sums to 1, i.e., if 𝐴≥0and𝐴⊤1=1.
1.2 Related Work
Several different FL approaches with varying degrees of decentral-
ization have been proposed to date. In this section, we focus on
those which are most relevant to the present work.
Semi-decentralized FL: [19] proposes a semi-decentralized learn-
ing methodology in which the D2D network is partitioned into
clusters, as in our paper. The key differences between [ 19] and the
present work are (a) we do not assume the D2D communications to
be bidirectional (equivalently, the cluster graphs in our model are
not undirected), and (b) our analysis uses column-stochastic con-
sensus matrices that need not satisfy the standard but unrealistic
assumption of symmetry (which leads to double stochasticity and
may not hold if the cluster graphs are directed). This leads to two
significant technical challenges. First, we cannot use standard eigen-
value results in our analysis since we must focus on singular values,
which generally differ from eigenvalues for asymmetric matrices.
Second, unlike doubly stochastic matrices, column-stochastic aggre-
gation matrices in general do not ensure convergence to consensus
in the absence of a central coordinator, which means our analysis
must account for the combined effect of global aggregations and
column-stochasticity. We address these challenges in this work.
Another closely related semi-decentralized learning methodol-
ogy is [ 36]. In [ 36], the goal is to enable edge devices to compute
weighted sums of their neighbors’ scaled cumulative gradients in
order to reduce the dependence of the global training process on un-
reliable D2S links. [ 36], however, assumes the D2D communication
network to be time-invariant and undirected, thereby disregarding
potential communication link failures and client mobility.
Learning over Clustered D2D Networks: Recently, [ 2] proposed
fully decentralized learning over D2D networks in which a small
subset of nodes act as bridges between different clusters for cross-
cluster model transmission, thereby obviating the role of a server.
Their topology design, however, results in a static rather than a
dynamic D2D network. Reference [ 6] also focuses on clustered
networks, but it provides a semi-decentralized learning methodol-
ogy where the basis for clustering is data similarity, whereas our
methodology makes no assumptions on the basis for clustering. A
complementary approach is proposed in [ 3], where every cluster is
assumed to be a clique and the D2D network is partitioned in such
a way that each local dataset is representative of the global data.
Network clusters also form the focus of another recent work, [ 28],
which proposes having one edge server per cluster so as to elimi-
nate the need for a central server. Its learning algorithm assumes
the edge network topology to be undirected, which gives rise to a
symmetric adjacency matrix.
Other Consensus-based Algorithms: Reference [ 12] provides
improved bounds on the convergence rates of certain gradient
tracking methods used in decentralized learning by enhancing
the analysis of the consensus matrix (referred to as the mixing
matrix therein) and its spectral gap. However, similar to [ 19], this
work assumes the consensus matrix to be row-stochastic as well
as symmetric, and hence, doubly stochastic. In this respect, [ 7]relaxes the assumptions of symmetry as well as double stochasticity
in an online learning setting. However, the matrices therein are
row-stochastic, which are not average-preserving and hence, they
are not as suitable as column-stochastic matrices for minimizing
the average of all the local loss functions. Finally, we remark that
there exists abundant literature on distributed optimization over
time-varying digraphs characterized by consensus matrices that
are not necessarily doubly stochastic (e.g., see [ 1,17,23–25,30,33]).
However, the effects of both data heterogeneity (or non-i.i.d. data
distribution) and graph-theoretic properties (such as the degree
distribution of the network in question) on the convergence rate of
these algorithms have remained largely unexplored.
2 SEMI-DECENTRALIZED FL SETUP
We now introduce the system model, the learning objective, and
the network model in semi-decentralized FL.
2.1 System Model and Learning Objectives
We consider a collaborative learning environment consisting of 𝑛
edge devices, or clients , and a central parameter server (PS) that is
tasked with aggregating all the local model updates generated by
the clients. We use [𝑛]to denote the set of clients.
Each client𝑖∈[𝑛]has a local datasetD𝑖, which is a collection of
data samples of the form 𝜉=(𝑢,𝑦)where𝑢∈R𝑝is the feature vec-
torof the sample and 𝑦is its label . On this basis, for any model 𝑥∈
R𝑝, we define the loss function 𝐿:R𝑝×∪𝑛
𝑖=1D𝑖→Rso that𝐿(𝑥;𝜉)
denotes the loss incurred by 𝑥on a sample 𝜉∈∪𝑛
𝑖=1D𝑖(where
∪𝑛
𝑖=1D𝑖is the global dataset). The average loss incurred by 𝑥over
the local dataset of client 𝑖is given by𝑓𝑖(𝑥):=1
|D𝑖|Í
𝜉∈D𝑖𝐿(𝑥;𝜉),
where𝑓𝑖:R𝑝→Rdenotes the local loss function of client𝑖.
In collaboration with the PS, the clients seek to minimize the
global loss function 𝑓:R𝑝→R, defined as the unweighted arith-
metic mean 𝑓(𝑥):=1
𝑛Í𝑛
𝑖=1𝑓𝑖(𝑥)of all the local loss functions. The
learning objective, therefore, is to determine the global optimum
𝑥∗:=arg min𝑥∈R𝑝𝑓(𝑥).
2.2 D2D and D2S Network Models
We model two types of interactions among the network elements:
(i) D2S and (ii) D2D. For D2S interactions, the devices can engage in
uplink communications to the PS if prompted by the server, which
happens through a sampling procedure explained later.
We model the D2D network as a time-varying directed graph
𝐺(𝑡)=([𝑛],𝐸(𝑡)), where[𝑛]denotes the vertex set and𝐸(𝑡)the
edge set of the digraph. The existence of a directed edge from a node
𝑖∈[𝑛]to another node 𝑗∈[𝑛]in𝐺(𝑡)denotes the existence of a
communication link from the 𝑖-th client to the 𝑗-th client in the D2D
network. In this case, we refer to client 𝑖(respectively, client 𝑗) as
the in-neighbor (respectively, out-neighbor) of client 𝑗(respectively,
client𝑖). The set of in-neighbors (respectively, out-neighbors) of a
client𝑖∈[𝑛]at time𝑡is denoted byN−
𝑖(𝑡)(respectively,N+
𝑖(𝑡)).
The number of in-neighbors (respectively, out-neighbors) is called
the in-degree (respectively, out-degree) and is denoted by 𝑑−
𝑖(𝑡)(re-
spectively,𝑑+
𝑖(𝑡)). We let𝑑−max(𝑡),𝑑+
min(𝑡), and𝑑+max(𝑡)denote the
maximum in-degree, the minimum out-degree, and the maximum
out-degree, respectively.MobiHoc ’23, June 03–05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
Unlike standard works on distributed learning [ 1,23–25], we
do not assume the D2D network to be strongly connected or even
uniformly strongly connected [ 23,24] over time. This gives rise to
a number𝑐>1of strongly connected components of 𝐺(𝑡), denoted
{(𝑉1(𝑡),𝐸1(𝑡)),(𝑉2(𝑡),𝐸2(𝑡)),...,(𝑉𝑐(𝑡),𝐸𝑐(𝑡))}which we refer to
asclusters of the D2D network. Here, we make the following mild
assumptions that apply to many cellular networks:
(1) The number of clusters, 𝑐, is time-invariant.
(2)There does not exist any communication link between any
two clusters. In other words, 𝐸(𝑡)=∪𝑐
ℓ=1𝐸ℓ(𝑡).
(3)Regardless of any movement of clients from one cluster to
another over time, as of time 𝑡, the server has full knowledge
of the vertex sets{𝑉ℓ(𝑡)}𝑐
ℓ=1of all the𝑐clusters.
The third condition is satisfied in practice since the base station
(which acts as the PS) is aware of the users in its coverage area.
3 PROPOSED METHOD FOR
CONNECTIVITY-AWARE LEARNING
We now present our methodology for connectivity-aware learning
over the semi-decentralized setup from Sec. 2. Our technique will
enable the central server to use limited knowledge of the cluster
degree distributions to tune a communication-efficiency trade-off.
3.1 Local Model Updates
As in many FL schemes, we assume every client performs multiple
rounds of local SGD iterations between any two consecutive rounds
of global aggregation. Let 𝑥(𝑡)denote the global model that all the
clients possess at the end of the 𝑡-th round of global aggregation.
Then, each client 𝑖∈[𝑛]performs𝑇∈Niterations of local SGD.
In other words, for each 𝑘∈{0,1,...,𝑇−1}, we have
𝑥(𝑡,𝑘+1)
𝑖=𝑥(𝑡,𝑘)
𝑖−𝜂𝑡e∇𝑓𝑖(𝑥(𝑡,𝑘)
𝑖), (1)
where𝜂𝑡>0is the learning rate or the step-size, and e∇𝑓𝑖(𝑥):=
1
|𝜒𝑖|Í
𝜉∈𝜒𝑖∇𝐿(𝑥;𝜉)is the stochastic gradient computed by client 𝑖
by sampling a mini-batch or a random subset 𝜒𝑖⊂D𝑖of its local
samples. Note that 𝑥(𝑡,0)
𝑖:=𝑥(𝑡).
3.2 Intra-Cluster Model Aggregations
The next step involves all the clients aggregating their scaled cumu-
lative gradients with their neighbors. This aggregation takes the
form of weighted sums. Every client 𝑖∈[𝑛]first transmits its scaled
cumulative stochastic gradient 𝑥(𝑡,𝑇)
𝑖−𝑥(𝑡)=−𝜂𝑡Í𝑇−1
𝑘=0e∇𝑓𝑖(𝑥(𝑡,𝑘)
𝑖)
to each of its out-neighbors 𝑗∈N+
𝑖(𝑡)before the𝑡-th global ag-
gregation round. To facilitate this, we assume that every cluster
ℓ∈[𝑐]contains an access point to which every client 𝑖∈𝑉ℓ(𝑡)
sends a list of its in-neighbors (clients whose gradients 𝑖has re-
ceived). The access point then announces the end of the concerned
D2D communication round, determines the out-degree sequence
{𝑑+
𝑗(𝑡):𝑗∈𝑉ℓ(𝑡)}of the cluster, and broadcasts this sequence to
every client in the cluster.
Subsequently, the client computes the following weighted sum of
all the scaled cumulative gradients it receives from its in-neighbors:
Δ𝑖(𝑡)=∑︁
𝑗∈N−
𝑖(𝑡)1
𝑑+
𝑗(𝑡)
𝑥(𝑡,𝑇)
𝑗−𝑥(𝑡)
. (2)This rule can be expressed compactly in matrix form as
∆(𝑡)=𝐴(𝑡)𝑋⊤
diff(𝑡), (3)
where ∆(𝑡):=[Δ1(𝑡)Δ2(𝑡) ··· Δ𝑛(𝑡)]⊤,
𝑋diff(𝑡):=h
𝑥(𝑡,𝑇)
1−𝑥(𝑡)𝑥(𝑡,𝑇)
2−𝑥(𝑡)···𝑥(𝑡,𝑇)
𝑛−𝑥(𝑡)i
, and
𝐴(𝑡)∈R𝑛×𝑛is a matrix whose(𝑖,𝑗)-th entry equals 𝑎𝑖𝑗(𝑡)=1
𝑑+
𝑗(𝑡)
for all𝑖∈[𝑛]and𝑗∈N−
𝑖(𝑡).
Fact 1.𝐴(𝑡)is a column-stochastic matrix because the following
holds for all 𝑗∈[𝑛]:
𝑛∑︁
𝑖=1𝑎𝑖𝑗(𝑡)=∑︁
𝑖∈[𝑛]:𝑗∈N−
𝑖(𝑡)1
𝑑+
𝑗(𝑡)=∑︁
𝑖∈N+
𝑗(𝑡)1
|N+
𝑗(𝑡)|=1.
It can be verified that 𝐴(𝑡)is a block-diagonal matrix with its blocks
{𝐴ℓ(𝑡)}𝑐
ℓ=1being the equal-neighbor adjacency matrices of the 𝑐
clusters in the D2D network.
Henceforth, we refer to 𝐴(𝑡)as the equal-neighbor adjacency ma-
trixof𝐺(𝑡)because it represents every client 𝑖∈[𝑛]transmitting
an equal share (a fraction1
𝑑+
𝑖(𝑡)) of its scaled cumulative gradient
to its𝑑+
𝑖(𝑡)out-neighbors.
3.3 Global Aggregation at the PS
For the global aggregation step, the PS samples a random subset of
clientsS(𝑡)⊂[𝑛]. The cardinality 𝑚(𝑡)≤𝑛of this set is carefully
chosen by our algorithm such that the resulting number of D2S
interactions is just enough to complement the intra-cluster aggre-
gations without excessively slowing down the training process.
Specifically, this involves three broad steps: (a) The PS first learns
the degree distribution of each cluster. (b) It then computes an
upper bound on an error quantity 𝜙(𝑡)that captures the combined
effect of random sampling and the cluster degree distributions on
the convergence rate. (c) It computes the minimum value of 𝑚(𝑡)
required to keep 𝜙(𝑡)below a desired threshold. More specifically:
(1)For the(𝑡+1)-th round of global aggregation, the server
uses𝑚(𝑡)(computed in the previous iteration) to selectl𝑚(𝑡)
𝑛
𝑛ℓ(𝑡)m
clients uniformly at random from the 𝑛ℓ(𝑡):=
|𝑉ℓ(𝑡)|clients that constitute cluster ℓ∈[𝑐]. This ensures
that every cluster has a representation in the global aggre-
gation that is proportionate to its size. The resulting set of
randomly sampled clients is denoted by S(𝑡). The server
then updates the global model as follows:
𝑥(𝑡+1)=𝑥(𝑡)+1
𝑚(𝑡)∑︁
𝑖∈S(𝑡)Δ𝑖(𝑡)=𝑥(𝑡)+1
𝑚(𝑡)𝑛∑︁
𝑖=1𝜏𝑖(𝑡)Δ𝑖(𝑡),
(4)
where𝜏𝑖(𝑡):=|{𝑖}∩S(𝑡)|is an indicator random variable
that takes the value 1when client 𝑖is sampled and the value
0otherwise. Note thatÍ𝑛
𝑖=1𝜏𝑖(𝑡)=|S(𝑡)|=𝑚(𝑡).
(2)The current round is now 𝑡←𝑡+1. All the cluster access
points send their respective out-degree sequences to the
server. Using this information, the server computes 𝛼ℓ(𝑡):=
1
𝑛ℓ(𝑡)min𝑖∈𝑉ℓ(𝑡)𝑑+
𝑖(𝑡), the minimum out-degree fraction of
clusterℓ∈ [𝑐]. The server then uses either of the two
sets of singular value bounds that we later derive in Sec.Connectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc ’23, June 03–05, 2023, Washington, DC
5 (either (10)-(11)or(15)-(16)) to compute an upper bound
𝜓(𝑚(𝑡),𝛼1(𝑡),...,𝛼𝑐(𝑡))on the connectivity factor affecting
the convergence rate. This connectivity factor is defined as
𝜙(𝑡):=𝑛
𝑚(𝑡)−1𝑐∑︁
ℓ=1𝑛ℓ(𝑡)
𝑛𝜙ℓ(𝑡), (5)
where𝜙ℓ(𝑡):=𝜎2
1(𝐴ℓ(𝑡))+𝜎2
2(𝐴ℓ(𝑡))− 1depends on the
greatest two singular values 𝜎1(𝐴ℓ(𝑡))≥𝜎2(𝐴ℓ(𝑡))of the
equal-neighbor adjacency matrix 𝐴ℓ(𝑡)of clusterℓ. For the
upper bound, we will show that
𝜓(𝑚(𝑡),𝛼1(𝑡),...,𝛼𝑐(𝑡))=𝑛
𝑚(𝑡)−1𝑐∑︁
ℓ=1𝑛ℓ(𝑡)
𝑛𝜓ℓ(𝑡),(6)
where either of the following holds (with the indexing (𝑡)
on the right hand side omitted for brevity):
𝜓ℓ(𝑡)=1+𝜀ℓ+1
𝛼ℓ−12
+2𝜀ℓ 
1+2
𝛼ℓ−1
𝛼2
ℓ!
,
𝜓ℓ(𝑡)=2+2𝜑ℓ
−(1−𝜀ℓ)2(1−𝛼2
−ℓ) (1−𝜀ℓ)2(1−𝛼2
−ℓ)−𝛼−ℓ
𝑛ℓ(𝜀net,ℓ+1)
𝜀net,ℓ−𝛼−ℓ+1
𝛼ℓ𝑛ℓ (7)
with𝜀ℓ(𝑡):=𝑑+
max(𝑡)−𝑑+
min(𝑡)
𝑑+
min(𝑡),𝜑ℓ(𝑡):=𝑑−
max(𝑡)−𝑑+
min(𝑡)
𝑑+
min(𝑡),
𝛼−ℓ(𝑡):=1
𝛼ℓ(𝑡)−1and𝜀net,ℓ(𝑡)=𝜑ℓ(𝑡)+𝜀ℓ(𝑡)
𝛼ℓ(𝑡).
(3) Finally, the server sets
𝑚(𝑡+1):=min{𝑟∈[𝑛]:𝜓(𝑟,𝛼1(𝑡+1),...,𝛼𝑐(𝑡+1))≤𝜙max}
where𝜙maxis a threshold given as an input to the algorithm.
This step ensures that 𝜙(𝑡)remains below the threshold
𝜙max, thereby preserving the convergence rate.
Our algorithm for 𝑡maxglobal rounds is summarized in Alg. 1.
4 CONVERGENCE ANALYSIS
We now provide theoretical performance guarantees for Algorithm
1. We also explain how the effect of D2D cluster connectivity on
the convergence rate of the algorithm is captured by the singular
values of the equal-neighbor adjacency matrices of the clusters. All
the proofs except that of Proposition 5.1 are available in the
appendix .
4.1 Assumptions and Preliminaries
4.1.1 Loss Functions. We start by making the following standard
assumptions on the local loss functions:
Assumption 1 ( Strong Convexity ).All the local loss func-
tions{𝑓𝑖}𝑛
𝑖=1are𝜇-strongly convex, i.e., there exists 𝜇>0such that
(∇𝑓𝑖(𝑥)−∇𝑓𝑖(𝑦))⊤(𝑥−𝑦)≥𝜇∥𝑥−𝑦∥2for all𝑥,𝑦∈R𝑝and all
𝑖∈[𝑛].
Assumption 2 ( Smoothness ).All the local loss functions {𝑓𝑖}𝑛
𝑖=1
are𝛽-smooth, i.e., there exists a finite 𝛽such that∥∇𝑓𝑖(𝑥)−∇𝑓𝑖(𝑦)∥≤
𝛽∥𝑥−𝑦∥for all𝑥,𝑦∈R𝑝and all𝑖∈[𝑛].
As shown in [ 19], Assumptions 1 and 2 imply that the global
loss function 𝑓is both𝜇-strongly convex and 𝛽-smooth.Algorithm 1 Connectivity-Aware Semi-Decentralized Learning
Input:𝑛,𝑐,𝑇,𝜙max,𝑡max,𝑚(0),{𝑛ℓ(𝑡)}𝑡max
𝑡=0,𝑥(0)
Output:𝑥(𝑡max)
1:for𝑡∈{0,1,...,𝑡 max−1}do
2: Client𝑖∈[𝑛]sets𝑥(𝑡,0)
𝑖←𝑥(𝑡)
3: for𝑘∈{0,1,...,𝑇−1}do
4: Client𝑖∈[𝑛]computes𝑥(𝑡,𝑘+1)
𝑖←𝑥(𝑡,𝑘)
𝑖−𝜂𝑡e∇𝑓𝑖(𝑥(𝑡,𝑘)
𝑖)
5: end
6: Client𝑖∈ [𝑛]transmits its scaled cumulative local gradient
−𝜂𝑡Í𝑇−1
𝑘=0e∇𝑓𝑖(𝑥(𝑡,𝑘)
𝑖)=𝑥(𝑡,𝑇)
𝑖−𝑥(𝑡)to its out-neighbors N+
𝑖(𝑡)
7: Client𝑖∈ [𝑛]computes the following weighted sum of its in-
neighbors’ cumulative local gradients:
Δ𝑖(𝑡)←∑︁
𝑗∈N−
𝑖(𝑡)1
𝑑+
𝑗(𝑡)
𝑥(𝑡,𝑇)
𝑗−𝑥(𝑡)
8: PS samples𝑚ℓ(𝑡)=𝑛ℓ(𝑡)
𝑛𝑚(𝑡)clients uniformly at random from
clusterℓ∈[𝑐]
9: PS computes 𝑥(𝑡+1)←𝑥(𝑡)+1
𝑚(𝑡)Í𝑛
𝑖=1𝜏𝑖(𝑡)Δ𝑖(𝑡)and broadcasts
𝑥(𝑡+1)to all clients
10: PS computes
11:𝑚(𝑡+1)← min{𝑟∈[𝑛]:𝜓(𝑟,𝛼 1(𝑡+1),...,𝛼𝑐(𝑡+1))≤𝜙max}
12:end
13:return𝑥(𝑡max)
4.1.2 SGD Iterations. Additionally, we make the following stan-
dard assumption on the stochastic gradients generated through the
SGD procedure for each client:
Assumption 3 ( Unbiasedness and Bounded Variance ).
The SGD noise associated with every client is unbiased, i.e.,
E[e∇𝑓𝑖(𝑥)−∇𝑓𝑖(𝑥)|𝑥]=0, and it has a bounded variance, i.e., there
exists a constant 𝜚>0such that E∥e∇𝑓𝑖(𝑥)−∇𝑓𝑖(𝑥)∥2≤𝜚2for all
models𝑥∈R𝑝and all𝑖∈[𝑛].
In addition, we assume that the SGD noise is independent across
clients, i.e., for all 𝑥∈R𝑝, the random vectorsn
e∇𝑓𝑖(𝑥)−∇𝑓𝑖(𝑥)o𝑛
𝑖=1
are mutually conditionally independent given 𝑥.
4.1.3 Gradient Diversity. Furthermore, we assume that the training
data are not distributed uniformly at random among the clients,
which gives rise to data heterogeneity among the clients. Unlike the
standard assumption on data heterogeneity that imposes a uniform
upper bound on∥∇𝑓𝑖(𝑥)−∇𝑓(𝑥)∥(see [ 29] for example), we make
a weaker assumption on the diversity of local gradients. In fact, this
assumption, which was first proposed in [ 19], can be derived as a
consequence of Assumptions 1 and 2, as shown in [ 19]. Below, we
formally state this observation.
Lemma 4.1 ( Gradient diversity [19]).For all𝑖∈[𝑛]and
𝑥∈R𝑝, we have∥∇𝑓𝑖(𝑥)−∇𝑓(𝑥)∥≤𝛿+2𝛽∥𝑥−𝑥∗∥, where
𝛿:=𝛽max
𝑖∈[𝑛]∥𝑥∗−𝑥∗
𝑖∥=𝛽max
𝑖∈[𝑛]∥𝑥∗−arg min
𝑦∈R𝑝𝑓𝑖(𝑦)∥ (8)
As argued in [ 19], the standard assumption (which is a special
case of the above inequality with 𝛽=0) is unrealistic as it does not
apply to quadratic and super-quadratic loss functions unless the
upper bound 𝛿is chosen to be unreasonably large.MobiHoc ’23, June 03–05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
4.2 Results
We now quantify how the singular values of the equal-neighbor
matrices and the number of clients sampled by the PS affect the
efficiency of our algorithm in terms of its optimality gap.
We first show how the expected optimality gap of our algo-
rithm depends on the expected deviation of the global average
𝑥(𝑡+1)−𝑥(𝑡)(i.e., the random vector computed by the PS using
the aggregation rule (4)) from the true average of all the scaled
cumulative gradients.
Lemma 4.2. At the end of the(𝑡+1)-th round of global aggregation,
the expected optimality gap of Algorithm 1 is given by
E𝑥(𝑡+1)−𝑥∗2
=E𝑥(𝑡+1)−¯𝑥(𝑡+1)2
+E¯𝑥(𝑡+1)−𝑥∗2
,
where ¯𝑥(𝑡+1):=𝑥(𝑡)+1
𝑛Í𝑛
𝑖=1(𝑥(𝑡,𝑇)
𝑖−𝑥(𝑡))is a vector that would
equal the global model if the PS were to sample all the 𝑛clients.
Observe that the first term on the RHS depends on 𝑥(𝑡+1)−¯𝑥(𝑡+1),
which can be easily shown to be the difference between the random
average1
𝑚(𝑡)Í
𝑖∈S(𝑡)Δ𝑖(𝑡)and the true average1
𝑛Í𝑛
𝑖=1
𝑥(𝑡,𝑇)
𝑖−𝑥(𝑡)
.
Thus, this term captures the error due to random sampling. As the
next result shows, this difference depends on the network topol-
ogy as well as on 𝑚(𝑡), the number of clients selected for global
aggregation uniformly at random by the PS.
Proposition 4.3. Let𝛿be the constant defined in (8). Then Algo-
rithm 1 satisfies the following for every 𝑡∈N∪{0}:
E𝑥(𝑡+1)−¯𝑥(𝑡+1)2
≤
2𝑇𝜚2𝜂2
𝑡+4𝑒𝑇(𝜚2+2𝛿2)𝜂2
𝑡+6𝛿2𝑇2𝜂2
𝑡
+(27+4𝑒)𝑇2𝛽2𝜂2
𝑡E𝑥(𝑡)−𝑥∗2
𝜙(𝑡),
where𝜙(𝑡)is the connectivity factor defined in (5).
In other words, E𝑥(𝑡+1)−¯𝑥(𝑡+1)depends on the previous op-
timality gap E𝑥(𝑡)−𝑥∗2
via𝜙(𝑡), i.e., the connectivity factor
that captures the combined effect of global aggregation (via 𝑚(𝑡))
and the D2D network topology within each cluster (via 𝜙ℓ(𝛼ℓ(𝑡))).
Moreover, Lemma 4.2 and Proposition 4.3 together show that
the singular values of the equal-neighbor adjacency matrices can
be used to derive an upper bound on the expected optimality gap
(and ultimately establish theoretical performance guarantees) for
our connectivity-aware algorithm. Doing so yields the following.
Proposition 4.4. Let𝛿be as defined in (5), let𝜙(𝑡)be the connec-
tivity factor defined in (5), letΓ:=𝑓(𝑥∗)−1
𝑛Í𝑛
𝑖=1min𝑥∈R𝑝𝑓𝑖(𝑥),
and let𝑒denote the exponential constant. Then the expected optimality
gap of Algorithm 1 satisfies the following for all 𝑡∈N0:
E𝑥(𝑡+1)−𝑥∗2
≤
(1−𝜇𝜂𝑡)𝑇+(27+4𝑒)𝑇2𝛽2𝜂2
𝑡(2𝑇+𝜙(𝑡))
E𝑥(𝑡)−𝑥∗2
+𝑇𝜚2
𝑛+6𝛽Γ+4𝑇𝜚2+8𝑒𝑇(𝜚2+2𝛿2)+12𝛿2𝑇2
𝜂2
𝑡
+
2𝑇𝜚2+4𝑒𝑇(𝜚2+2𝛿2)+6𝛿2𝑇2
𝜙(𝑡)𝜂2
𝑡.A recursive expansion on the inequality stated by Proposition 4.4
results in our main theoretical result, which we state below.
Theorem 4.5. Consider a connectivity factor threshold 𝜙max≥0,
and suppose that 𝜙(𝑡)≤𝜙maxfor all times𝑡≥0. In addition, suppose
𝜂𝑡=4
𝑇𝜇(𝑡+𝑡1), where
𝑡1:=$
4
1−1
𝑇
+(16𝑇+8𝜙max)𝛽
𝜇2
+1%
.
Then the expected optimality gap of Algorithm 1 satisfies the following
for all𝑡≥0:
E𝑥(𝑡)−𝑥∗2
≤𝑡1
𝑡+𝑡12
E𝑥(0)−𝑥∗2
+16
1
𝑛𝑇𝜚
𝜇2
+6𝛽Γ
𝑇𝜇2
𝑡+𝑡1
+(32𝑇+16𝜙max)
2
𝑇𝜚
𝜇2
+4𝑒
𝑇𝜚
𝜇2
+2
𝛿
𝜇2
+6
𝛿
𝜇2
𝑡+𝑡1.(9)
Theorem 4.5 reveals that the convergence rate of our algorithm is
O(1/𝑡), which coincides with that of FedAvg and its semi-decentralized
variants such as [ 19]. In fact,O(1/𝑡)resembles the convergence
rate of vanilla centralized SGD. It also shows that suitably tuning
the connectivity factor (by choosing an appropriate value of 𝜙max)
is critical to the efficiency of the algorithm: as 𝜙maxincreases the
bound gets worse/larger; however, 𝜙max, by its definition, is non-
negative, which means it can at best be made equal to 0, which
forces𝑚=𝑛, in which case the inequality boils down to an upper
bound on the convergence rate of FedAvg with full device sampling.
At the other extreme, setting 𝜙maxto∞results in𝑚=1, which
happens when our semi-decentralized FL architecture collapses to
full decentralization.
Moreover, Theorem 4.5 jointly captures the effect of the follow-
ing factors on the expected instantaneous optimality gap and hence
on the convergence rate: (i) the initial optimality gap E∥𝑥(0)−𝑥∗∥2
(via the first term), (ii) The SGD noise variance 𝜚2and the strong
convexity𝜇and smoothness parameters 𝛽(via the second term),
and finally, (iii) the combined effect of cluster connectivity levels
and random sampling-based global aggregations (via the third term,
which depends on 𝜙max, which in turn prevents the connectivity
factor𝜙(𝑡)from becoming too large). It can be seen that higher
values of the SGD noise variance 𝜚2and the data heterogeneity
measure Γlead to a larger value of the bound, implying that our
algorithm is sensitive to the size of the mini-batches used for com-
puting the stochastic gradients as well as to the non-i.i.d.-ness of
the local datasets.
5 SINGULAR VALUE BOUNDS
Having established the role of the connectivity factor 𝜙(𝑡)in the
performance of Algorithm 1, we now analyze two important quan-
tities associated with 𝜙(𝑡): the top two singular values of the equal-
neighbor adjacency matrices of the clusters. Since a precise estima-
tion of these singular values requires full knowledge of the cluster
topologies, which is challenging to obtain in practice, we are mo-
tivated to derive a set of novel upper bounds on these values inConnectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc ’23, June 03–05, 2023, Washington, DC
terms of the node degrees of the cluster digraphs, which are easy
to obtain/measure in practice. To the best of our knowledge, this
is one of the first attempts at connecting the singular values of
adjacency matrices with minimal topological information such as
node degrees of the digraphs.
To conduct our analysis, for any digraph 𝐺=([𝑠],𝐸), we first
define𝜀=𝜀𝐺:=𝑑+
max(𝐺)−𝑑+
min(𝐺)
𝑑+
min(𝐺),which quantifies the hetero-
geneity of out-degree of the nodes across the digraph. We also
let𝛼(𝐺):=𝑑+
min(𝐺)
𝑠capture the minimum fraction of the node
population that any node is out-connected to. In addition, we let
𝑊(𝐺)=(𝑤𝑖𝑗)and𝐷+(𝐺):=diag([𝑑+
1𝑑+
2···𝑑+𝑠]⊤)denote the
binary adjacency matrix and the out-degree matrix of 𝐺, respec-
tively. In the sequel, we drop the indexing (𝐺)for brevity.
We are now equipped to state our first set of bounds on the great-
est two singular values of 𝐺under certain regularity assumptions
on the digraph.
Proposition 5.1. Suppose𝐺=([𝑠],𝐸)is a directed graph in
which every node has its in-degree equal to its out-degree, i.e., 𝑑+
𝑖=𝑑−
𝑖
for all𝑖∈[𝑛]. Then the greatest two singular values 𝜎1and𝜎2of
the equal-neighbor adjacency matrix 𝐴of𝐺satisfy the following
inequalities for 𝛼>1
2and𝜀≪1:
𝜎2
1≤1+𝜀+O(𝜀2), (10)
𝜎2
2≤1
𝛼−12
+2𝜀
1+2
𝛼−1
𝛼2
+O(𝜀2), (11)
whereO(·) is the big-𝑂notation used in the context of 𝜀→0.
Proof. To simplify our notation, we define 𝐷:=𝐷+for the
remainder of this proof. Observe that
𝐴⊤=𝐷−1𝑊=𝐷−1
2(𝐷−1
2𝑊𝐷−1
2)𝐷1
2,
which means 𝐴⊤is similar to the normalized adjacency matrix
defined as𝐴𝑁:=𝐷−1
2𝑊𝐷−1
2.
On the other hand, we have 𝑑+
min=𝑑+max(1−𝜀)≤𝑑+
𝑖≤𝑑+maxfor
all𝑖∈[𝑠], which implies the existence of a diagonal matrix 𝐸3such
that𝑂≤𝐸3≤𝐼and𝐷=𝑑+max((1−𝜀)𝐼+𝜀𝐸3). Using similar argu-
ments, it can be easily shown that there exist diagonal matrices 𝐸1
and𝐸2such that𝑂≤𝐸1,𝐸2≤𝐼,𝐷1
2=√︁
𝑑+max (1−𝜀
2)𝐼+𝜀
2𝐸1+
O(𝜀2), and𝐷−1
2=1√
𝑑+max 𝐼+𝜀
2𝐸2+O(𝜀2). As a result, the follow-
ing holds up to an additive error of O(𝜀2):
𝐴𝑁=𝐷1
2𝐴⊤𝐷−1
2=√︃
𝑑+max
(1−𝜀
2)𝐼+𝜀
2𝐸1𝐴√︁
𝑑+max
𝐼+𝜀
2𝐸2
=𝐴⊤+𝜀
2((𝐸1−𝐼)𝐴+𝐴𝐸2),
i.e.,𝐴⊤−𝐴𝑁=−𝜀
2 (𝐸1−𝐼)𝐴⊤+𝐴⊤𝐸2+O(𝜀2). In conjunction
with standard bounds on singular value perturbations (e.g., see [ 22]),this implies the following up to an additive error of O(𝜀2):
𝜎𝑗(𝐴)=𝜎𝑗(𝐴⊤)
≤𝜎𝑗(𝐴𝑁)+𝜀
2(𝐸1−𝐼)𝐴⊤+𝐴⊤𝐸2
≤√︃
𝜆𝑗(𝐴𝑁𝐴⊤
𝑁)+𝜀
2 ∥𝐸1−𝐼∥𝐴⊤+𝐴⊤∥𝐸2∥
(𝑎)
≤√︃
𝜆𝑗(𝐷−1
2𝑊𝐷−1𝑊⊤𝐷−1
2)+𝜀
2(1·𝐴⊤+𝐴⊤·1)
(𝑏)=√︃
𝜆𝑗(𝐷−1𝑊𝐷−1𝑊⊤)+𝜀𝜎1(𝐴⊤). (12)
Here,(𝑎)holds because 𝐼−𝐸1being a diagonal matrix along with
𝑂≤𝐼−𝐸1≤𝐼implies that∥𝐼−𝐸1∥=max𝑖∈[𝑠]|(𝐼−𝐸1)𝑖𝑖|≤1.
(𝑏)holds because 𝜎1(𝐴⊤)=𝐴⊤and because 𝐷−1𝑊𝐷−1𝑊⊤and
𝐷−1
2𝑊𝐷−1𝑊⊤𝐷−1
2, being similar, have the same eigenvalues.
We now bound 𝜎1(𝐴)and𝜎2(𝐴)individually. As for 𝜎1(𝐴), the
derivation (12) and the fact that 𝜎1(𝐴)=𝜎1(𝐴⊤)imply that
𝜎1(𝐴)≤√︁
𝜆1(𝐷−1𝑊𝐷−1𝑊𝑇)
1−𝜀=√︃
𝜆1(𝐷−1𝑊𝐷−1𝑊𝑇)(1+𝜀)
+O(𝜀2). (13)
So, it is enough to bound 𝜆1(𝐷−1𝑊𝐷−1𝑊𝑇). For this purpose, note
that𝐴being column-stochastic implies that 𝐷−1𝑊1=1and hence
also that𝑊1=𝐷1. Besides, our assumption on in-degrees and out-
degrees can be expressed asÍ𝑠
𝑗=1𝑤𝑖𝑗=Í𝑠
𝑗=1𝑤𝑗𝑖for each𝑖∈[𝑠], or
equivalently, 𝑊⊤1=𝑊1=𝐷1. As a result, we have 𝐷−1𝑊⊤1=1.
Thus,𝐷−1𝑊𝐷−1𝑊⊤=𝐴⊤𝐷−1𝑊⊤is a product of row-stochastic
matrices and hence, it is row-stochastic in itself. Consequently,
𝜆1(𝐷−1𝑊𝐷−1𝑊𝑇)=1. In light of this, (13) implies (10).
It remains to prove (11). We do this by using Theorem 2.2 of [ 20],
which helps derive a bound in terms of 𝜎1and the minimum positive
entry𝛿of the matrix 𝐷−1𝑊𝐷−1𝑊𝑇. We first note that
(𝐷−1𝑊𝐷−1𝑊𝑇)𝑖𝑗(𝑎)
≥1
(𝑑+max)2𝑠∑︁
𝑘=1(𝑊)𝑖𝑘(𝑊⊤)𝑘𝑗
=1
(𝑑+max)2|{𝑘∈[𝑠]:𝑤𝑖𝑘=𝑤𝑗𝑘=1}|(𝑏)
≤(2𝛼−1)𝑠
(𝑑+max)2,
where(𝑎)follows from the fact that 𝐷−1≥1
𝑑+max𝐼and(𝑏)holds
because the number of common out-neighbors of any two nodes
𝑖,𝑗∈[𝑠]is at least(2𝛼−1)𝑠. We can now apply Theorem 2.2 of [ 20]
by setting𝑥=1√𝑠in the theorem (because1√𝑠1, as explained above,
is the unit-norm principal eigenvector of 𝐷−1𝑊𝐷−1𝑊⊤). Thus,
𝜆2(𝐷−1𝑊𝐷−1𝑊⊤)≤𝜆1(𝐷−1𝑊𝐷−1𝑊⊤)−(2𝛼−1)𝑠2
(𝑑+max)2
=1−2
𝛼−1
𝛼2
(1−2𝜀)+O(𝜀2), (14)
where the last step follows from the observation that 𝑑+max=𝛼𝑠
1−𝜀.
Combining (10), (12) and (14) now gives
𝜎2(𝐴)≤√︄
1−2
𝛼−1
𝛼2
(1−2𝜀)+O(𝜀2)+𝜀
1+𝜀+O(𝜀)2
.
Squaring both sides and rearranging the terms results in (11). □MobiHoc ’23, June 03–05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
Remark 1. Observing the bounds in Proposition 5.1, we can see
that setting 𝛼=1, which corresponds to 𝐺being a clique, in the
bounds yields 𝜎1≤1+O(𝜀)and𝜎2=O(𝜀). These inequalities, for
𝜀≪1, are tight with respect to the well-known lower bounds 𝜎1≥1
and𝜎2≥0. This implies that the bounds (10)and(11)can be expected
to be reasonably tight for high edge density (i.e., whenever 𝛼≈1).
Another implication of the bounds is decreasing 𝜀, which inherently
measures how irregular the digraph is, leads to (11)becoming sharper.
The above singular value bounds are especially tight for digraphs
that are approximately regular (or digraphs that do not exhibit
significant variations in their in-degrees and out-degrees) since
such digraphs satisfy 𝜀≪1. This happens in practice, when the
D2D clusters are dense (e.g., in the wireless setting, when the nodes
are closer to each other or when they can move and communicate
over time). Furthermore, the same holds for the condition on 𝛼in
Proposition 5.1 (i.e., 𝛼>1
2), which is always met when the clusters
are dense.
However, the bounds (10)and(11)are obtained under the as-
sumption that every node has its in-degree equal to its out-degree,
which can be restrictive in practical settings. This observation fur-
ther motivates us to find a new set of singular value bounds that
work well under milder assumptions. We thus provide the following
bounds, which not only relax the said restrictive assumption, but
also apply to digraphs with more general out-degree distributions
(and hence subsume digraphs with wider out-degree variations).
Proposition 5.2. Let𝜑=𝑑in
max−𝑑min
𝑑min, where𝑑inmax denotes the
maximum in-degree of the digraph 𝐺. If𝛼≥1
2, we have the following
bounds:
𝜎2
1≤1+𝜑, (15)
𝜎2
2≤1+𝜑−(1−𝜀)2(1−𝛼2
−1) (1−𝜀)2(1−𝛼2
−1)−𝛼−1
𝑠(𝜀net+1)
𝜀net−𝛼−1+1
𝛼𝑠,(16)
where𝜀net:=𝜑+𝜀
𝛼and𝛼−1:=1
𝛼−1.
The bounds obtained in Proposition 5.2 ( proved in the appen-
dix) are particularly effective when the D2D cluster digraphs are
dense but irregular. This is often the case in practical systems, when
there is communication heterogeneity (e.g., in wireless sensor net-
works consisting of sensors with different radii).
In conjunction with Theorem 4.5, the bounds derived in Propo-
sitions 5.1 and 5.2 capture the inherent dependence of the expected
optimality gap, and hence that of the convergence rate, on the de-
gree distributions of the D2D clusters. In particular, upon having
approximately regular D2D clusters, the bounds in Propositions 5.1
along with the result of Theorem 4.5 determine the convergence rate
of Algorithm 1. The same holds when using the result of Proposi-
tion 5.2 with Theorem 4.5, which will characterize the convergence
rate of Algorithm 1 upon having irregular D2D clusters.
6 NUMERICAL VALIDATION
We now conduct numerical experiments to validate our methodol-
ogy. Overall, our simulations show that compared with baselines,
Algorithm 1 obtains significant reductions in total communication
cost for the same or similar levels of testing accuracy.6.1 Implementation
6.1.1 Network Architecture. We simulate a network consisting of
𝑛=70edge devices partitioned into 𝑐=7clusters with 𝑛ℓ=10
nodes per cluster. In every global aggregation round, the digraph
for each cluster ℓ∈[𝑐]is constructed as follows: (i) we generate
a𝑘-regular directed graph (a digraph in which every node has its
in-degree and out-degree equal to 𝑘) with the value of 𝑘being
chosen uniformly at random from the set {6,..., 9}; (ii) we delete
a fraction𝑝∈(0,1)of the directed edges uniformly at random
so as to incorporate D2D link failures due to client mobility and
bandwidth issues. The result is an approximately regular digraph
whose degree distribution may deviate significantly from that of
regular digraphs, while satisfying 𝛼ℓ(𝑡)>1
2.
6.1.2 Datasets. All our simulations are performed on MNIST [ 35]
and Fashion-MNIST (F-MNIST) [ 32] datasets. The MNIST dataset
consists of 70K images (60K for training and 10K for testing), and
each image is a hand-written digit between 0 to 9 (i.e., the dataset
has 10 labels). The same applies to the FMNIST dataset, the only
difference being that it consists of images of fashion products.
6.1.3 ML Models and Implementation. We use the neural network
model from [ 21] in our simulations. In particular, we use a convolu-
tional neural network (CNN) with two 5×5convolution layers, the
first of which has 32 channels and the second 64 channels, where
each of these layers precedes a 2×2max pooling, resulting in a total
model dimension of 1,663,370. We use the PyTorch implementation
of this setup provided in [ 11] with cross-entropy loss. Each dataset
is distributed among the clients in a non-i.i.d. manner: the samples
(from either of the two datasets) are first sorted by their labels,
partitioned into chunks of equal size, and each of the 70 clients is
assigned only two chunks (i.e., each client will end up having only
two labels). This results in extreme data heterogeneity, which leads
to strong empirical guarantees for our approach.
All of our simulations are performed using the following hyper-
parameter values/ranges: 𝑇=5,𝑡max∈{15,30},𝑝∈{0.1,0.2}, and
𝜂𝑡=0.02(0.1)𝑡where𝑡is the global aggregation index.
6.2 Results
We compare the energy vs. accuracy trade-offs associated with
Algorithm 1 with those associated with two baselines, FedAvg [ 21]
and collaborative relaying (COLREL) [ 36]. The second baseline is
a recently proposed semi-decentralized FL algorithm that incor-
porates single-step consensus updates. Under the D2D and D2S
connectivity constraints introduced in Section 2, COLREL is a vari-
ant of FedAvg that incorporates one round of column-stochastic
D2D aggregations before every global aggregation round but does
not provide any criterion to control the sampling size 𝑚, which we
assume to be fixed throughout its implementation. The fundamen-
tal difference between our method and COLREL is that our method
takes into account the change in the connectivity of D2D clusters,
optimally tuning the value of 𝑚according to the set of novel upper
bounds on the singular values we obtained in Section 5.
We consider these tradeoffs under different D2S connectivity
levels. Intuitively speaking, on one hand, as the D2S connectivity
improves, we expect to see that our algorithm leads to a lower
energy and cost savings as compared to FedAvg. This is becauseConnectivity-Aware Semi-Decentralized Federated Learning over Time-Varying D2D Networks MobiHoc ’23, June 03–05, 2023, Washington, DC
Figure 2: Communication cost vs. testing accuracy under high
D2S connectivity (Dataset: MNIST).
Figure 3: Communication cost vs. testing accuracy under high
D2S connectivity (Dataset: F-MNIST).
our algorithm will naturally collapse to FedAvg and D2D commu-
nications will become less useful since more devices would engage
in uplink communications, which by itself degrades the benefit of
D2D local aggregations. On the other hand, as D2S connectivity
improves, we expect to see that our algorithm achieves significant
energy savings as compared to COLREL. This is because the impact
of tuning𝑚becomes more prominent when there is a possibility
of D2S communications.
All of the following plots and discussion are based on the as-
sumption that the ratio of the energy required for D2D commu-
nication to that of up-link (D2S) transmission, denoted by𝐸D2D
𝐸Glob,
equals 0.1. This is a pessimistic estimate in favor of D2S consid-
ering that most ratios reported in the literature [ 8,19,38] take
values less than 0.1. Thus, the communication costs reported are
(#D2S transmissions)+0.1×(#D2D transmissions ).
6.2.1 Case 1: Cost savings under high D2S connectivity and a low link
failure probability. When the PS has a high downlink bandwidth
and the connectivity between the devices and the PS is reliable,
implementing FedAvg or COLREL has the effect of setting 𝑚to a
value close to 𝑛. As an example, we implement FedAvg and COLREL
with𝑚=57and𝑚=52, respectively (note that COLREL requires
fewer up-link transmissions because it uses D2D consensus up-
dates in addition to global aggregations). The results for MNIST are
shown in Fig. 2: choosing 𝜙max=0.06and a low D2D link failure
Figure 4: Communication cost vs. testing accuracy under low
D2S connectivity (Dataset: MNIST).
Figure 5: Communication cost vs. testing accuracy under low
D2S connectivity (Dataset: F-MNIST).
probability𝑝=0.1results in Algorithm 1 achieving a testing accu-
racy of 90% while consuming about 46%less energy than FedAvg
(thereby incurring proportionately lower communication costs).
With respect to COLREL, the energy saving is even higher because
COLREL also expends energy on D2D aggregations with relatively
little gain in testing accuracy.
Repeating this experiment on FMNIST results in a similar per-
formance, depicted in Fig. 3. We see that Algorithm 1 (with 𝜙max=
0.06) consumes about 30% less energy than COLREL for achieving
a testing accuracy of 70%.
6.2.2 Case 2: Cost savings under low D2S connectivity and a high
link failure probability. When the connectivity between the devices
and the PS is poor, implementing FedAvg or COLREL has the effect
of setting𝑚to a value significantly smaller than 𝑛. As an example,
we implement FedAvg and COLREL with 𝑚=26and𝑚=15,
respectively. Choosing 𝜙max=0.2and a high D2D link failure
probability 𝑝=0.2results in Algorithm 1 consuming about 30%
less energy than FedAvg for achieving a testing accuracy of 90% on
MNIST, as shown in Fig. 4. The cost saving is lower than in Case 1
as we expect because the singular value bounds incorporated by
our algorithm into its choice of 𝑚(𝑡)are looser for higher values of
the link failure probability 𝑝. Repeating this experiment on FMNIST
results in a similar performance, depicted in Fig. 5.MobiHoc ’23, June 03–05, 2023, Washington, DC Parasnis, Hosseinalipour, Chu, Brinton, Chiang.
7 CONCLUSION
We have investigated consensus-based semi-decentralized learning
over clustered D2D networks modeled as time-varying digraphs.
We first revealed the connection between the singular values of
the column-stochastic matrices used for D2D model aggregations
and the convergence rate of the learning process. We then derived
a set of novel upper bounds on these singular values in terms of
the degree distributions of the cluster digraphs, and we used the
resulting bounds to design a novel connectivity-aware FL algorithm
that enables the central parameter server to tune the number of
up-link transmissions by using its knowledge of the time-varying
degree distributions of clusters. Our algorithm maintains a continu-
ous balance between the number of model aggregations occurring
at the server and those occurring over the edge network, thereby
enhancing the resource efficiency of the learning process without
compromising convergence.
Future works include obtaining upper bounds on singular values
under more general assumptions, and obtaining optimal device
sampling schemes for irregular clusters.
REFERENCES
[1]Mohammad Akbari, Bahman Gharesifard, and Tamás Linder. 2015. Distributed
online convex optimization on time-varying directed graphs. IEEE Transactions
on Control of Network Systems 4, 3 (2015), 417–428.
[2]Mohammed S Al-Abiad, Mohanad Obeed, Md Hossain, Anas Chaaban, et al .2022.
Decentralized aggregation for energy-efficient federated learning via overlapped
clustering and D2D communications. arXiv preprint arXiv:2206.02981 (2022).
[3]Aurélien Bellet, Anne-Marie Kermarrec, and Erick Lavoie. 2022. D-cliques: Com-
pensating for data heterogeneity with topology in decentralized federated learn-
ing. In 2022 41st International Symposium on Reliable Distributed Systems (SRDS) .
IEEE, 1–11.
[4]Enrique Tomás Martínez Beltrán, Mario Quiles Pérez, Pedro Miguel Sánchez
Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Grego-
rio Martínez Pérez, and Alberto Huertas Celdrán. 2022. Decentralized Federated
Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges.
arXiv preprint arXiv:2211.08413 (2022).
[5]Aleksandr Beznosikov, Pavel Dvurechensky, Anastasia Koloskova, Valentin
Samokhin, Sebastian U Stich, and Alexander Gasnikov. 2021. Decentralized
local stochastic extra-gradient for variational inequalities. arXiv preprint
arXiv:2106.08315 (2021).
[6]Christopher Briggs, Zhong Fan, and Peter Andras. 2020. Federated learning with
hierarchical clustering of local updates to improve training on non-IID data. In
2020 International Joint Conference on Neural Networks (IJCNN) . IEEE, 1–9.
[7]Chaoyang He, Conghui Tan, Hanlin Tang, Shuang Qiu, and Ji Liu. 2019. Central
server free federated learning over single-sided trust social networks. arXiv
preprint arXiv:1910.04956 (2019).
[8]Mariem Hmila, Manuel Fernández-Veiga, Miguel Rodriguez-Perez, and Sergio
Herrería-Alonso. 2019. Energy efficient power and channel allocation in underlay
device to multi device communications. IEEE transactions on communications 67,
8 (2019), 5817–5832.
[9]Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G Brinton, Nicolo
Michelusi, Vaneet Aggarwal, David J Love, and Huaiyu Dai. 2022. Multi-stage
hybrid federated learning over large-scale D2D-enabled fog networks. IEEE/ACM
Transactions on Networking 30, 4 (2022), 1569–1584.
[10] Yifan Hua, Kevin Miller, Andrea L Bertozzi, Chen Qian, and Bao Wang. 2022.
Efficient and reliable overlay networks for decentralized federated learning. SIAM
J. Appl. Math. 82, 4 (2022), 1558–1586.
[11] Shaoxiong Ji. 2018. A PyTorch Implementation of Federated Learning.
[12] Anastasiia Koloskova, Tao Lin, and Sebastian U Stich. 2021. An improved analysis
of gradient tracking for decentralized machine learning. Advances in Neural
Information Processing Systems 34 (2021), 11422–11435.
[13] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian
Stich. 2020. A unified theory of decentralized sgd with changing topology and
local updates. In International Conference on Machine Learning . PMLR, 5381–5393.
[14] Jakub Konečn `y, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon. 2016. Federated learning: Strategies
for improving communication efficiency. arXiv preprint arXiv:1610.05492 (2016).[15] Anusha Lalitha, Shubhanshu Shekhar, Tara Javidi, and Farinaz Koushanfar. 2018.
Fully decentralized federated learning. In Third workshop on bayesian deep learn-
ing (NeurIPS) , Vol. 2.
[16] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. Federated
learning: Challenges, methods, and future directions. IEEE signal processing
magazine 37, 3 (2020), 50–60.
[17] Shu Liang, George Yin, et al .2019. Dual averaging push for distributed convex
optimization over time-varying directed graph. IEEE Trans. Automat. Control 65,
4 (2019), 1785–1791.
[18] Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam, Christo-
pher G Brinton, and Nicolò Michelusi. 2021. Federated learning beyond the star:
Local D2D model consensus with global cluster sampling. In 2021 IEEE Global
Communications Conference (GLOBECOM) . IEEE, 1–6.
[19] Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam, Christo-
pher G Brinton, and Nicolo Michelusi. 2021. Semi-decentralized federated learn-
ing with cooperative D2D local model aggregations. IEEE Journal on Selected
Areas in Communications 39, 12 (2021), 3851–3869.
[20] M Stuart Lynn and William P Timlake. 1969. Bounds for Perron eigenvectors and
subdominant eigenvalues of positive matrices. Linear Algebra Appl. 2, 2 (1969),
143–152.
[21] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Artificial intelligence and statistics . PMLR,
1273–1282.
[22] Carl D Meyer. 2000. Matrix analysis and applied linear algebra . Vol. 71. Siam.
[23] Angelia Nedić and Alex Olshevsky. 2014. Distributed optimization over time-
varying directed graphs. IEEE Trans. Automat. Control 60, 3 (2014), 601–615.
[24] Angelia Nedić and Alex Olshevsky. 2016. Stochastic gradient-push for strongly
convex functions on time-varying directed graphs. IEEE Trans. Automat. Control
61, 12 (2016), 3936–3947.
[25] Angelia Nedic, Alex Olshevsky, and Wei Shi. 2017. Achieving geometric conver-
gence for distributed optimization over time-varying graphs. SIAM Journal on
Optimization 27, 4 (2017), 2597–2633.
[26] Dan Shen, Genshe Chen, Jose B Cruz, and Erik Blasch. 2008. A game theoretic
data fusion aided path planning approach for cooperative UAV ISR. In 2008 IEEE
Aerospace Conference . IEEE, 1–9.
[27] Tao Sun, Dongsheng Li, and Bao Wang. 2022. Decentralized federated averaging.
IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
[28] Bin Wang, Jun Fang, Hongbin Li, Xiaojun Yuan, and Qing Ling. 2022. Confed-
erated Learning: Federated Learning with Decentralized Edge Servers. arXiv
preprint arXiv:2205.14905 (2022).
[29] Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian
Makaya, Ting He, and Kevin Chan. 2019. Adaptive federated learning in re-
source constrained edge computing systems. IEEE journal on selected areas in
communications 37, 6 (2019), 1205–1221.
[30] Zheng Wang and Huaqing Li. 2019. Edge-based stochastic gradient algorithm for
distributed optimization. IEEE Transactions on Network Science and Engineering
7, 3 (2019), 1421–1430.
[31] Zutong Wang, Mingfa Zheng, Jiansheng Guo, and Hanqiao Huang. 2017. Un-
certain UAV ISR mission planning problem with multiple correlated objectives.
Journal of Intelligent & Fuzzy Systems 32, 1 (2017), 321–335.
[32] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[33] Ran Xin and Usman A Khan. 2018. A linear algorithm for optimization over
directed graphs with geometric convergence. IEEE Control Systems Letters 2, 3
(2018), 315–320.
[34] Hong Xing, Osvaldo Simeone, and Suzhi Bi. 2021. Federated learning over wireless
device-to-device networks: Algorithms and convergence analysis. IEEE Journal
on Selected Areas in Communications 39, 12 (2021), 3723–3741.
[35] L Yan, C Corinna, and CJ Burges. 1998. The MNIST dataset of handwritten digits.
[36] Michal Yemini, Rajarshi Saha, Emre Ozfatura, Deniz Gündüz, and Andrea J Gold-
smith. 2022. Semi-decentralized federated learning with collaborative relaying. In
2022 IEEE International Symposium on Information Theory (ISIT) . IEEE, 1471–1476.
[37] Shahryar Zehtabi, Seyyedali Hosseinalipour, and Christopher G Brinton. 2022.
Event-Triggered Decentralized Federated Learning over Resource-Constrained
Edge Devices. arXiv preprint arXiv:2211.12640 (2022).
[38] Aiqing Zhang and Xiaodong Lin. 2017. Security-aware and privacy-preserving
D2D communications in 5G. IEEE Network 31, 4 (2017), 70–77.