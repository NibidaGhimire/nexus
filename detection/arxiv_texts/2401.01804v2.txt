Efficient Computation of Confidence Sets Using
Classification on Equidistributed Grids
Lujie Zhou *
First version: October 2023
This version: November 2024
Abstract
Economic models produce moment inequalities, which can be used to form tests of the true
parameters. Confidence sets (CS) of the true parameters are derived by inverting these tests.
However, they often lack analytical expressions, necessitating a grid search to obtain the CS nu-
merically by retaining the grid points that pass the test. When the statistic is not asymptotically
pivotal, constructing the critical value for each grid point in the parameter space adds to the com-
putational burden. In this paper, we convert the computational issue into a classification problem
by using a support vector machine (SVM) classifier. Its decision function provides a faster and
more systematic way of dividing the parameter space into two regions: inside vs. outside of the
confidence set. We label those points in the CS as 1 and those outside as -1. Researchers can train
the SVM classifier on a grid of manageable size and use it to determine whether points on denser
grids are in the CS or not. We establish certain conditions for the grid so that there is a tuning that
allows us to asymptotically reproduce the test in the CS. This means that in the limit, a point is
classified as belonging to the confidence set if and only if it is labeled as 1 by the SVM.
1 Introduction
The confidence regions are of central importance in a wide range of applied work spanning many
fields of studies. While its computation is not the focus of in the context of economics and econo-
metrics as much as its theoretical construction, to be able to construct the confidences with modern
computing tools is a necessary component of the empirical economic analysis.
We see that oftentimes structural modeling leads to a characterization of the confidence region as
a set of parameter values that satisfy a certain inequality or a vector of inequalities (i.e. the criterion
inequalities, or simply the test). To perform the pointwise testing when inverting the test to obtain an
analytical expression of the confidence region for the parameter of interest is not feasible, one would
break the parameter space, typically a subset that lies in some Euclidean space, into very dense grid
*Department of Economics, Pennsylvania State University. lbz5158@psu.edu
- 1 -arXiv:2401.01804v2  [econ.EM]  13 Nov 2024points and evaluate the criterion to keep the points accepted by the test. This approach is known as the
exhaustive searching. For example, if one is interested in the inference of a scalar-valued parameter,
the easiest thing to do is to cut (maybe some segment of, based on the prior knowledge) the real line
into, say, 10,000 points of equal gaps, followed by plugging those values into the criterion inequality
to obtain the confidence interval(s) once we have the criterion inequality defined. Now imagine if we
would like to apply the same idea to the computation of parameter vectors of higher dimensions. With
two or three parameters, so 10,0002and10,0003grid points, we would probably do just as well in
that we can easily visualize the regions in which parameter vectors all satisfy the criterion inequality,
provided that computation efficiency is not really a concern to economists. But we can already see
that the computation complexity grows exponentially fast in the number of parameters of interest, and
very quickly it will expand out of a manageable scale if one would like to maintain the same precision
along each dimension. Especially when the test statistic is not asymptotically pivotal, as we will see
in the setup in the next section, an additional computation of the cutoff value has to be done within
the evaluation of every grid point.
A second issue that arises is the reporting of these confidence sets. Even though we have formed
fine grids and obtained those regions after having the program run for hours or days, the researcher
would have hard time describing those regions or present them simply because we cannot visualize
objects beyond three dimensions. After all, the grid points along do not tell us about the shape, angle,
or connectedness of the confidence regions.
In this paper, we will explore the number-theoretic notion of equidistributed sequences and inte-
grate it with a modern machine learning method, support vector machine (SVM), originally proposed
in Cortes and Vapnik (1995), to address the computation challenge described above. The essence of
this approach is, by turning the computation problem into a classification problem, we build a clas-
sifier that is fast to evaluate and mimics the decision behavior of the test using a moderately-sized
grid, and apply it to a much larger grid to get fast results, hence accelerating the computation of deal-
ing with the gigantic number of grid points while still sustaining the coverage of the underlying the
confidence regions. Additionally, this approach is also compatible with parallel computing in that we
can parallelize the computation of both grid evaluation and the fitting of the SVM classifier (see, e.g.
Zhu et al. (2007)), should this situation become relevant. The main contribution of this paper includes
introducing the general notion of equidistributed sequences to the problems of grid search; most im-
portantly, we convert the problem into a classification task and show that it is possible to tune the
SVM classifier, which is fast and easy to use once trained on a pre-specified grid, such that it exactly
preserves the asymptotic coverage of the confidence sets on the equidistributed grids. Although this
paper does not aim to solve the issue of reporting the confidence sets, we hope to venture a different
angle around this issue via the use of the decision function associated with the SVM classifier. One
can conservatively report the smallest box that contains all grid points predicted by SVM classifier
to be in the confidence set, which is characterized by 2 times the dimension of the space numbers,
i.e. maximum and minimum along each dimension. Alternatively, we propose reporting the decision
function after training the classifier, which is concise, very easy to work with, and conserves all the
information in the classifier.
The rest of this paper proceeds as follows. The first prat of Section 2 introduces the framework
- 2 -where a computational approach becomes necessary when obtaining confidence sets. The second
part then provides a discussion of some work that involves moment inequality confidence sets along-
side the methods used in the econometrics literature. Section 3 describe the idea of support vector
machine classifier and its asymptotic behavior, followed by the conditions and tuning that ensure the
preservation of coverage, which are the main contribution of this paper. Section 4 provides simulation
results in graphs to help visually illustrate the performance of SVM classifiers. Section 5 concludes.
Details of the equidistributed sequences and all proofs can be found respectively in Appendix A and
Appendix B.
2 Setup and Related Literature
2.1 Confidence Sets
The general setup follows the framework provided in Rosen (2008) that describes the construction
of the confidence regions from models based on moment inequalities. And as is mentioned in the
paper, similar construction is also seen in a number of other papers, such as Wolak (1991), Ciliberto
and Tamer (2009), Aradillas-Lopez and Gandhi (2016), Li and Henry (2022), etc. In particular, Rosen
defines the set to be all parameter values 𝜃∈Θ, the parameter space, at which some criterion value is
less than a threshold value, 𝑍1−𝛼, corresponding to the significance level 𝛼, and it is given as follows.
𝐶𝑅 1−𝛼≡
𝜃∈Θ:𝑛ˆ𝑄𝑛(𝜃)≤𝑍1−𝛼	
, (1)
where the parameter-dependent component of the criterion is given by
ˆ𝑄𝑛(𝜃)=min
𝑡≥0ˆ𝐸𝑛[𝑚(𝑦,𝑥,𝜃)]−𝑡′ˆ𝑉−1
𝜃ˆ𝐸𝑛[𝑚(𝑦,𝑥,𝜃)]−𝑡
, (2)
and𝑛ˆ𝑄𝑛(𝜃)typically follows some chi-bar-square distribution, a mixture of chi-square distributions.
Here in this Wald-type statistic, 𝑚(·)denotes the finite-dimensional vector of moment conditions
which the economic model predicts to be non-negative, and ˆ𝑉𝜃is the sample variance of 𝑚(𝑦,𝑥,𝜃)
with𝑥and𝑦being the covariate and outcome variables. The idea of this statistics is that if 𝜃belongs
to the identified region of the model, Θ∗, then this quantity should be very small. The cutoff value is
computed in a way such that
inf
𝜃∈Θ∗lim
𝑛→∞Pr
𝑛ˆ𝑄𝑛(𝜃)≤𝑍1−𝛼	
≥1−𝛼 (3)
The computational complication arises for the models where the test statistic is not asymptotically
pivotal. Specifically, the asymptotic distribution of the test statistic may depend on those moments
with expected value 0. This is resolved by taking upper bounds of the number of such moment
conditions and construct conservative confidence sets for 𝜃0. While this is a very clear setup in theory,
the specific steps of computation given the precision of the confidence region and the parameter set
to consider are less obvious. The quote below from Rosen (2008) discusses the computation of the
confidence sets, and shows the sense in which the choices can be ad hoc.
Appropriate choice of grid values 𝐺depends on the particular application. How fine the
grid should be depends on the desired level of precision for 𝐶𝑅 1−𝛼. IfΘ∗is known to
- 3 -be sufficiently regular (e.g. closed and convex), certain values of 𝜃may be able to be
included or discarded without explicitly evaluating 𝑛ˆ𝑄𝑛(𝜃).
Our approach on the computation is relatively more straightforward and less case dependent compared
to Rosen (2008), so long as a concrete and meaningful definition of the critical region can be written
down both on paper and in computer code.
2.2 Literature Review
In a recent work, Kalouptsidi et al. (2020) employ a slight variation of the standard grid search;
that is, they limit the searching to certain regions of the real line (the parameter space in their case).
This limited grid search scheme begins with, say, ˆ𝜃𝑈+0.01where ˆ𝜃𝑈is the estimated upper bound of
the identified set for the true parameter 𝜃, proceeds to ˆ𝜃𝑈+0.02if some null hypothesis is not rejected,
and stops when it is rejected. Similar procedures are also applied to the estimated lower bound. This
guarantees that all points beyond the stopping values, 𝜃𝑙and𝜃𝑢, are ruled out, hence the asymptoti-
cally uniformly valid 1−𝛼confidence set for 𝜃is given by the interval [𝜃𝑙,𝜃𝑢]. They also propose
that the stochastic search can be used to find good directions for the parameter of interest in the higher
dimensional setting. This algorithm adds perturbation during each iteration of the searching. A risk
that the authors recognize is that there is a real risk for the random algorithm to exit the identified set,
without exploiting the specific structure of the problems. However, it is not immediately clear how
to extend this method to parameters in higher dimensional spaces because, as is mentioned above,
putting together the confidence intervals along each individual dimension does not give us the correct
coverage probability (e.g. the “rectangle” versus the “ellipse”). On the contrary, our approach han-
dles higher dimensions especially well with the use of SVM because the hyperplane it draws lives in
a transformed space and is not constructed in a successive manner.
The family of intelligent algorithms can be useful when solving for global maximum or minimum,
such as genetic algorithm, particle swarm optimization algorithm, artificial fish swarm algorithm,
artificial bee colony algorithm, and firefly algorithm. The wolfpack algorithm introduced in Wu and
Zhang (2014), for example, first obtain the minimizer of as in equation (1), and then perform a grid
search near the global minimizer. The obvious advantage is the massive parallelizability in that we
can multi-process the same algorithm to visit various regions of the space via different initializations.
Nonetheless, there is no guarantee that all disconnected regions of the confidence set will be visited
since one starts grid searching only around the global extremum after discarding other information
along the search of the extremum, and one certainly does not get around the issue of performing a
grid search even after nailing the global minimum. Working equally well with this idea is another
family of algorithms, the derivative-free optimization (DFO) algorithms. In comparison, our new
approach does not hinge on the fact that a global extremum is found first as a guiding direction; more
importantly, through the use of the equidistributed sequences, we have control over the way we visit
different regions of the parameter space. We can just as easily parallelize the computation, too. The
algorithm below describes the detailed steps of the intelligence algorithm, where “wolves” refer to
the nodes that run in a parallel manner, and the “lead wolf” is defined to be the node with the best
objective value in the current iteration.
- 4 -Algorithm Wu and Zhang (2014): Wolf Pack Algorithm
1:Initialize the algorithm parameters: positions/number of wolves, maximum iteration, etc.
2:All but the lead wolf search until a higher objective function value is attained (then replace the
lead wolf) or maximum iteration is reached.
3:All wolves move toward the lead wolf. If a new lead wolf emerges, go to step 2; otherwise keep
moving until they are “close” to the lead wolf.
4:All but the lead wolf take besieging behavior in the vicinity of the lead wolf.
5:Update the leader when appropriate. And update the wolf population to open up new search
space.
6:Stop if one stopping condition is met; otherwise, go to step 2.
Moment inequalities arise in many different scenarios, such as relaxing model assumptions, miss-
ing data, models with multiple predictions or solutions, etc. Manski and Tamer (2002) represent a
particular missing data issue in terms of moment inequalities. Specifically, when upper and lower
bounds of outcome data are observed rather than the actual outcome, a set of conditional moment in-
equalities can be derived that must hold with probability 1. Large amount of papers have focused on
the models with partially identified parameters. In the case where the univariate parameter of interest
is not point-identified in the model, Imbens and Manski (2004) provides a way of constructing confi-
dence intervals, for the particular parameter rather than the entire identified region, that has the correct
pre-specified coverage probability. Chernozhukov et al. (2007) and Romano and Shaikh (2010) were
the earliest attempts who separately develop inference procedures for both the multi-dimensional set
of identified parameter vectors and the true parameter vector of interest, both of which rely on sub-
sampling methods that may be computationally difficult. The recent work by Li and Henry (2022)
proposes a framework with incomplete models in which the confidence set obtained by inverting the
test displays exact finite sample coverage. Notably, the test statistic and critical value are computed
through a discrete optimal transport problem, and they also propose a fast preliminary search method-
ology that locates the region in which the true parameter vector lies, thus alleviating the cumbersome
search over the entire parameter space.
There is a literature on sub-vector inference for moment inequalities when the parameters of in-
terest form only a sub-vector aside from the nuisance parameters. In particular, if the conditional
moment inequalities satisfies linearity conditions, one can exploit this structure and efficiently com-
pute the profiled maximum statistic as in Andrews et al. (2019) or the profiled quasi-likelihood ratio
statistic proposed in Cox and Shi (2022), regardless of the dimension of the nuisance parameters.
Aradillas-Lopez and Gandhi (2016) develop a structural model of chain-story entry with discrete
intensity and obtain the asymptotic properties of the pivotal test statistic via kernel-based estimation,
which is then used to construct the confidence set. A grid search is then applied in the parameter space
to determine numerically the region of acceptance via the criterion defined by the authors. Ciliberto
and Tamer (2009) study the effect of general firm heterogeneity on the market structure in the context
of U.S. airline industry, and propose a moment inequalities-based procedure to conduct inference on
the parameters of interest. They employ simulated annealing algorithm to approximate the global
minimum of the objective criterion along with subsampling to obtain the critical value (as described
- 5 -in Chernozhukov et al. (2007)) in the construction of the confidence region. We discuss more details
of their approach in Subsection 4.3.
Finally, equidistributed sequences are no strangers to the studies of computation in economics.
The computational methods textbook, Judd (1998), introduced this concept in the context of numerical
integration. It is noted that equidistributed sequences display ex post uniformity, which is a property
that yields asymptotically valid approximation to any integral. Judd (1998) also discusses that the
precision is lost as more points along the same sequences are generated because they are sensitive to
the round-off errors due to the machine precision. Additionally, Miranda and Fackler (2004) builds
on the aforementioned formulation for Monte Carlo integration and offers a function that computes
the equidistributed sequences along with their weights in the well-known CompEcon toolbox. They
point out that certain such sequences achieve uniformity in both an ex ante and an ex post senses,
hence better representing the regions under investigation than truly random sequences.
3 Methodologies
In this section, we will first describe the mechanism of the support vector machine classifiers.
Then, we introduce the asymptotic property of the SVM classifiers when the tuning parameters take
certain values. And lastly, we provide conditions based on the equidistributed sequences which guar-
antee that our procedure replicates the behavior of the limiting confidence sets. Our complete ap-
proach would be to first generate a moderately-sized set of grid points (or more, depending on the
researcher’s time budget) using one of the equidistributed sequences, evaluate these points using the
criterion inequalities, assign binary labels to these grid points (i.e. 1 if in the confidence region; -1
otherwise), and fit a SVM classifier to these labeled grid points. The tuning of the SVM parameters
will always be such that the SVM classifier predicts all training examples correctly. As a result, we
end up with a trained SVM classifier on the grid points generated with the equidistributed sequences,
with which we can then assign binary labels to more grid points that are outside of the grid we use
to train the classifier. This approach is much more efficient compared to exhaustively evaluating the
grids, because training the SVM is a one-time investment and its computation is relatively negligible
compared to evaluating the grid points in most applications. So in the extreme case where we evaluate
just as many grid points as exhaustive search, SVM gives us a systematic way of replicating the con-
fidence set and extrapolating the prediction to larger grids via its decision function. Most importantly,
it allows us to re-produce the confidence sets with precision in the limit if the grids meet the very mild
conditions described below.
Other classification methods could work too in this setting, but there are other drawbacks. For
example, convolutional neural networks are also excellent at approximating functions but the theoret-
ical result puts restrictions on the type of functions it can perfectly approximate, such as continuity
on a box in R𝑑(see Guliyev and Ismailov (2018)). Another example is the nearest-neighbor classi-
fiers, which inefficiently computes the distance between a new point and all training points although
it requires no fitting. Note that logistic regression does not apply because this is a non-linear problem
in general.
- 6 -3.1 Support Vector Machine Classifiers
Support vector machines, which is of core importance to this paper, are ideal methods for classifi-
cation tasks. In a simple case of points with binary labels (hence defining two groups) along the real
line, SVM finds the boundary point such that it best separates the two groups; mathematically, it finds
the point that minimizes the incorrect labels on both sides of the point. When there are disconnected
groups, this problem becomes non-linear. The kernel method is applied then to transform the problem
into a linear problem in other spaces. Essentially, we transform the data via a kernel function with
tuning parameter such that the data becomes linearly separable in the new space. To exemplify its
working, let’s consider a 2-dimensional case where one group is surrounded by another group, as in
the left panel of Figure 1 below. Clearly, one cannot draw a single straight line to separate the two
groups. Kernel method adds a third dimension, altitude, to the data, and as we see in the right panel of
Figure 1, it is straightforward to draw a plane (i.e. a line equivalent in 3-dimensional space) that com-
pletely separates the two groups. In fact, there are a few possible planes one can draw to accomplish
the same result. SVM finds the one that maximizes the minimum distance to both groups. Now if we
project this plane back to the original data space, we would end up with a closed curve resembling
that one would draw to separate the groups by hand. This is called the decision boundary . We will
explore more of this in Section 4
Figure 1: The mechanism of SVM classifiers in a simple 2D example. ( Source )
Since in reality, the kernel method only runs the computations as if the data are in the higher
dimensional space without actually transforming them, this improves the efficiency and makes non-
linear classification possible computationally. Aside from the fast computation, SVM classifiers also
have great performances separating the groups very consistently. And for the same reason, it is very
suitable when our object lives in highers dimensions. Again, as is discussed earlier, the appealing
visual representation is limited to spaces of dimensions no more than three. But it does allow us to
have an alternative nonparametric and computational characterization of the very same confidence
regions defined by our economic models. Future research along this line is appropriate to exploit the
value of the decision boundaries created by the SVM classifiers.
To fully understand the way SVM works and discuss the ways in which we can adapt it to our ap-
- 7 -plications, we will look into the mathematics in the background next. Once we equip the mechanism
with computation tools, we should be able to relax the standard approach and modify it with our own
criteria. The following parts build the support vector network progressively in three steps, after which
a brief discussion of the computational implementation is provided; and at last, we will see where the
adaptation is appropriate to extend it in the economic models. For our purposes, it suffices to discuss
the cases with two classes labeled 1and−1, hence a binary classification problem.
Optimal Hyperplane
Let us first consider the case where the data are linearly separable, i.e. can be separated perfectly
with a straight line or its equivalent in higher dimensions. Denote the features x𝑖∈R𝑑and the labels
𝑦𝑖∈{− 1,1}, for𝑖=1,···,𝑆. Formally, we say the classes are linearly separable if there exist a
vector w∈R𝑑and a scalar𝑏such that
w·x𝑖+𝑏≥1,if𝑦𝑖=1
w·x𝑖+𝑏≤−1,if𝑦𝑖=−1.(4)
Define the margin to be the smallest distance from one class to the other, and the decision boundary is
used to separate the classes. In Figure 2 below, an example with 2-dimensional features is presented.
One class is labelled as circles and the other is crosses. The margin is given by the perpendicular
distance from the highlighted circle to the highlighted crosses. And the decision boundary is the
dotted line in between the two solid lines.
Figure 2: Decision boundary and margin.
Having this in mind, we proceed to define the following concepts. Since there are infinitely many
lines one can draw, in the above example, to separate the two classes, we take the line that yields
the maximal margin to be our optimal hyperplane , and call the largest margin the optimal margin .
In other words, optimal hyperplane maximizes its distance to the nearest data point of either class.
Mathematically, we write equation (4) as, for 𝑖=1,···,𝑆,
𝑦𝑖(w·x𝑖+𝑏)≥1, (5)
and the optimal hyperplane
w0·x+𝑏0=0, (6)
- 8 -which uniquely separates the two classes and maximizes the margin, i.e.
(w0,𝑏0)=arg max
min
{x:𝑦=1}x·w
||w||−max
{x:𝑦=−1}x·w
||w||
(7)
By equation (5), the maximal margin is given by2
||w0||=2√w0·w0. So, constructing the optimal hyper-
plane is a quadratic programming problem that minimizes w·wunder constraint (5). Equivalently,
we have the following convex minimization problem
min1
2||w||2subject to𝑦𝑖(w·x𝑖+𝑏)≥1,∀𝑖.
The Lagrangian is given by
𝐿(w,𝑏,α)=1
2||w||2−𝑛∑︁
𝑖=1𝛼𝑖(𝑦𝑖(w·x𝑖+𝑏)−1) (8)
Importantly, the Lagrange multiplier 𝛼𝑖=0for all data point 𝑖that satisfies the constraint (5) with
strict inequality, by Karush-Kuhn-Tucker Theorem. Furthermore, the KKT first-order conditions are
as follows
w=∑︁
𝑖𝛼𝑖𝑦𝑖x𝑖
0=∑︁
𝑖𝛼𝑖𝑦𝑖. (9)
Put together the above, we have that the optimal hyperplane is only determined by the points that sat-
isfy constraint (5) with equality, i.e. the support vectors . Plugging equations (9) into the Lagrangian
(8), we have the Lagrangian dual below that only operates over values of 𝛼𝑖under constraint (5).
𝐿(α)=∑︁
𝑖𝛼𝑖−1
2∑︁
𝑖∑︁
𝑗𝛼𝑖𝛼𝑗𝑦𝑖𝑦𝑗(x𝑖·x𝑗) (10)
Moreover, denote the maximizer α0, the separating hyperplane is given by
0=∑︁
𝑖𝛼0
𝑖𝑦𝑖(x𝑖·x)+𝑏0 (11)
Note that w0can be solved from top equation in (9), and 𝑏0is typically solved using the average of
all points such that (5) holds with equality for numerical stability (Hastie et al. (2001)). Once the
solution is obtained, we take the sign of the function 𝑓(x†)=w0·x†+𝑏0as the classification of an
new vector x†with unknown label. We will see next that the dot product terms in equations (10) and
(11) are replaced with some kernel function to extend this idea to nonlinear classes.
Soft Margin Classifier
To make our technique even more flexible and handle cases where classes overlap, we will allow
for some tolerance of error. Define 𝜉𝑖≥0to be the tolerance associated with classifying each observa-
tion incorrectly. Our objective becomes to minimize the errors while finding the optimal hyperplane.
This implies that we can combine the old problem with the new component into the following formu-
lation.
min1
2w2+𝐶·𝐹 ∑︁
𝑖𝜉𝑖!
subject to𝑦𝑖(w·x𝑖+𝑏)≥1−𝜉𝑖, 𝑖=1,···,𝑆 (12)
𝜉𝑖≥0, 𝑖=1,···,𝑆 (13)
- 9 -Here, the coefficient 𝐶is a tuning parameter which determines the trade-off between maximizing
the margin and making sure that all features x𝑖lie on the correct sides of the margin. Function 𝐹
is a monotone convex function that takes form of some loss function, such as hinge loss given by
max{0,1−𝑦𝑖(w·x𝑖+𝑏)}. If we think of the first component of this objective as the 𝐿2regularization
term (or penalty term), then SVM can also be viewed as penalized regression under the hinge loss
function. The above problem is known as the primal problem . The classical approach to computing
the soft margin classifier is to solve the Lagrangian dual of the above problem and turn it into a
quadratic programming problem. There are more recent approaches such as sub-gradient descent and
coordinate descent developed for this problem, whose details I have not gotten into yet. Note it can
be showed that the solution w0, same as the optimal hyperplane problem, can be written as a linear
combination of the support vectors, i.e.
w0=∑︁
𝑖𝛼0
𝑖𝑦𝑖x𝑖 (14)
whereα0is solved from the dual problem.
Support Vector Machine
Suppose we have a non-linear problem. For some given transformation 𝜙:R𝑑→R𝑝, which
we know/choose a priori, that maps from the original input space to some feature space, we could
separate the classes linearly in the feature space very easily. As discussed above, a coherent way
to extend the above method is to replace the x𝑖with𝜙(x𝑖), thus the dot products x𝑖·x𝑗replaced
by𝐾(x𝑖,x𝑗) ≡ ⟨𝜙(x𝑖),𝜙(x𝑗)⟩. We call this generalized inner product, 𝐾,kernel function . More
specifically, we would like to learn some normal vector w∈R𝑝and scalar𝑏, and according to the
above formulation, we have
w=∑︁
𝑖𝛼𝑖𝑦𝑖𝜙(x𝑖), (15)
which then implies
𝑓(x)=w·𝜙(x)+𝑏
=∑︁
𝑖𝛼𝑖𝑦𝑖𝜙(x𝑖)·𝜙(x)+𝑏
=∑︁
𝑖𝛼𝑖𝑦𝑖𝐾(x𝑖,x)+𝑏.(16)
While the computation takes place in the transformed feature space now, the steps to compute the
classifier (i.e. the support vectors and the weights 𝛼𝑖) follow exactly from before. We require this
kernel𝐾to be a symmetric, positive semi-definite function so that Mercer’s theorem applies. This
is because only kernels that satisfy Mercer’s condition can be decomposed into inner products of
some underlying map 𝜙(i.e. such𝜙exists). This is known as the “kernel method”, and it allows
us to work with potentially infinite dimensional transformation of the input data without having to
explicitly transform them. Some common choices of the kernel function include the polynomial
kernel of degree 𝑑1, which is given by
𝐾(u,v)=(u·v+1)𝑑; (17)
1Derivation: here
- 10 -and radial basis function kernel (Gaussian)2
𝐾(u,v)=exp
−||u−v||2
2𝜎2
(18)
where𝜎 > 0is a free parameter.
The implementation of the SVM has been well developed and packaged in a number of machine
learning toolkits, including LIBSVM, Matlab, scikit-learn in Python, etc. For the simulation exercises
in section 4 below, I used the “e1071”3library in R for convenience. When implementing our own
variant of the SVM, we can make use of the source code of some of the these packages.
3.2 Asymptotic Behavior
Because of some properties of the radial basis function (RBF) kernel, we should pair our method
with this choice of kernel. The most desirable property is that we can achieve 100% perfect classi-
fication accuracy on the training data with the RBF kernel, provided that no two points of different
classes lie on top of each other. Keerthi and Lin (2003) show that for a fixed parameter value 𝐶that is
sufficiently large, so long as we make the 𝜎parameter really small (e.g. much smaller than the small-
est distance between any two data points), we have that for any 𝑖, the predicted label is ˆ𝑦(𝑥𝑖)=𝑦𝑖.
The classifier classifies the training example correctly in an asymptotic sense of 𝜎, given a fixed 𝐶.
Formally, we have the following result.
Assumption 1.
1.𝑙0> 𝑙 1+1>2where𝑙0and𝑙1are the number of training examples in class 0 and class 1,
respectively.
2. For𝑖≠𝑗,𝑥𝑖≠𝑥𝑗. That is, no two examples have identical 𝑥vectors.
Lemma 1. Under Assumption 1, if 𝐶 >𝑙 0/(𝑙0+𝑙1), then, as𝜎2→0, the classifier function
𝑓(x)=w0·𝜙(x)+𝑏0
where𝜙(·)is implicitly defined through the RBF kernel, i.e. 𝜙(x)·𝜙(x′)=exp
−||x−x′||2
2𝜎2
, classifies
all training examples correctly and classifies the rest of the space as class 0.
As a consequence of the Lemma, we will always pair our SVM classifier with the RBF kernel.
Throughout this paper we will assume Assumption 1 holds and that the tuning parameter 𝐶satisfies
the condition given in Lemma 1, hence the tuning will be focused on the parameter 𝜎2. Albeit, the
above result creates an issue of overfitting, which extensively undermines the out-of-sample predictive
power. However, as far as our problem is concerned, this becomes an ideal attribute that allows us
to capture the confidence regions. To illustrate the idea of this result, let us consider the following
special case of the classification function
𝑓(x)=∑︁
𝑖𝛼𝑖𝑦𝑖𝐾(x𝑖,x), (19)
2Derivation: here
3See https://cran.r-project.org/web/packages/e1071/e1071.pdf.
- 11 -Figure 3: Overfitting issue with the Gaussian RBF kernel. ( Source )
where the kernel is given by
𝐾(x𝑖,x)=exp
−||x𝑖−x||2
2𝜎2
.
When we make 𝜎really small, it follows that 𝐾(x𝑖,x) ≈ 0; except when x=x𝑖,𝐾(x𝑖,x𝑖)=1.
Therefore, the predicted label of the 𝑗th training point is
ˆ𝑦(x𝑗)=sgn 
𝛼0
𝑗𝑦𝑗𝐾(x𝑗,x𝑗)+∑︁
𝑖≠𝑗𝛼0
𝑖𝑦𝑖𝐾(x𝑖,x𝑗)!
≈sgn(𝛼0
𝑗𝑦𝑗)
Now if we simplify 𝛼0
𝑖to1s, then clearly ˆ𝑦(x𝑗)would have the same sign as 𝑦𝑗. The immediate
implication for our purposes is that as we increase the number of training points 𝑛in our setup, since
we can always completely classify the two groups (in and outside the confidence set) by taking a
sequence of scalars {𝜎𝑛}𝑛∈Nwith𝜎𝑛→0as𝑛→∞ , we can perfectly recover the true underlying
confidence set. We will formally state this result in the next subsection. Figure 3 shows the issue of
overfitting when the 𝜎parameter value is too small, which puts too much weight on correctly classi-
fying the training example. Whereas this can be mitigated usually via cross validation in practice, we
will take full advantage of this behavior to obtain the confidence sets of interest.
Furthermore, RBF can be interpreted as if it maps a vector in the the original data space to
an infinite-dimensional feature space where a separating hyperplane that perfectly divides the two
groups is guaranteed to exist, although we never need to explicitly transform the data into the infinite-
dimensional features due to the kernel method. To see this, let uandvbe any two vectors, setting
2𝜎2=1, we have
𝐾(u,v)=exp
−||u−v||2
=exp(−⟨u−v,u−v⟩)
=exp
−||u||2−||v||2+2u·v
=exp
−||u||2−||v||2∞∑︁
𝑛=0(2u·v)𝑛
𝑛!,
- 12 -by Maclaurin series. Therefore, denoting 𝑀=exp −||u||2−||v||2, we can define the transformation
function from
𝜙(u)·𝜙(v)=𝑀∞∑︁
𝑛=0(2u·v)𝑛
𝑛!.
In the case where 𝑢and𝑣are scalars, we can write it out as follows
𝜙(𝑢)=√
𝑀·
1,√︂
2
1!𝑢,√︂
22
2!𝑢2,√︂
23
3!𝑢3,···
.
3.3 Coverage
With the above theoretical guarantee, we now show that the desired coverage can be achieved
by the SVM via proper tuning of the tuning parameters, along with proper design of the grid in the
absence of further knowledge about the true paremeter. In this subsection, we provide the conditions
that the grid and the values of the tuning parameters need to satisfy in order to preserve the asymptotic
coverage of the underlying confidence sets. Moreover, when we tune the SVM classifier to behave
exactly like the confidence set as we will show below, it not only maintains the asymptotic size, but
also inherits the power in that any point outside the confidence set will be be labelled −1by the post-
tuning SVM classifier. We will look at different cases when sample size 𝑛goes to infinity and when
it is some finite number. As the sample size grows, we would like to generate increasingly more grid
points as well over which we evaluate the test, and feed all grid points to train the SVM classifier.
However, this must happen in a way that sufficiently explores the parameter space. The reason is that
if the grid points are clustered in some regions, then we cannot precisely learn the boundary of the
confidence sets in the limit even as the total number of grid points tends to infinity. More formally, we
state in the following definitions the ways in which the grid points should be formed in the absence
of any knowledge of the parameter of interest.
Definition 1. Γ⊂R𝑑is a𝑑-dimensional boxif it is given by Γ=Î𝑑
𝑖=1[𝑎𝑖,𝑏𝑖], where𝑎𝑖< 𝑏𝑖, for all
𝑖=1,···,𝑑.
Definition 2. A grid𝑆fully explores the space of interest Θif it is the first|𝑆|terms of an equidis-
tributed sequence on Γ(Θ), the smallest box in R𝑑that contains Θ.
We borrow this number-theoretic notion of equidistribution to generalize the formation of grids and
ensure that any point in the space can be approached arbitrarily closely by our grid points, an essential
component in the theoretical result below. A uniform grid is formed from evenly-spaced equidis-
tributed sequences. Another example of the equidistributed sequences is the Monte Carlo sequences,
which is nothing other than putting together independent random sequences along each dimension of
the parameter space. We leave the full details and further discussion of the equidistributed sequences
along with more examples in Appendix A. We acknowledge that if prior knowledge of the true pa-
rameter is available, it would be more efficient to avoid visiting all regions of the parameter space and
concentrate the grid points in regions that align with the prior information.
We will focus on the classifier with corresponding decision function given by
𝑓(𝜃)=sgn ∑︁
𝑖𝛼0
𝑖𝑦𝑖𝐾(𝑠𝑖,𝜃)+𝑏0!
, (20)
- 13 -setting the non-zero 𝛼0
𝑖’s to 1 and𝑏0=0for simplicity, so only the effect of the kernel is present.
This can be extended to include the optimal values 𝛼0
𝑖and𝑏0without qualitatively changing the
following results. Below is another useful definition in finding the sufficient condition of preserving
the asymptotic coverage of the confidence sets.
Definition 3. The influence of a grid point 𝑠on another point 𝜃dominates the influence of all other
grid points if 𝑘(𝑠,𝜃)>Í
𝑠𝑗≠𝑠𝑘(𝑠𝑗,𝜃), where𝑘(·,𝜃)measures the similarity between a point and 𝜃.
3.3.1 Large Sample
First, we provide the conditions as the sample size goes to infinity. Given the decision function
in (20), we see that any point 𝜃∈𝐶𝑆𝑛will be predicted to have label 1by the SVM classifier if the
nearest grid point has label 1and its influence dominates the influence of all grid points with label
−1, using Gaussian RBF kernel as the measure of similarity. Therefore, we require the following to
be true for the grid.
Assumption 2. Grid𝑆𝑛fully explores the parameter space Θ, for each𝑛, based on the same equidis-
tributed sequence. That is, 𝑆𝑛⊂𝑆𝑛+1.
As an immediate result of Assumption 2, we have the following lemma which states that eventually
the closest grid point to any 𝜃in the confidence set will be a grid point with label 1; whereas the
distance from any grid point outside the confidence set is bounded away from 0, for any sample size.
Given a particular 𝜃, we denote the closest grid point with label 1, i.e. interior grid point, as 𝐼𝑛(𝜃),
and the closest grid point with label −1as𝐸𝑛(𝜃).
Lemma 2. Under Assumption 2, for any 𝜃∈𝐶𝑆≡lim
𝑛→∞𝐶𝑆𝑛,||𝐼𝑛(𝜃)−𝜃||→ 0and||𝐸𝑛(𝜃)−𝜃||→
𝑀 > 0as𝑛→∞ , where𝑀is some fixed number and ||·|| is the Euclidean norm.
Therefore, we have the first main result of this paper as follows.
Theorem 1. Suppose Assumption 2 holds. Then, for any 𝛼∈ (0,1), there exists tuning such that
P(𝜃0∈𝑆𝑉𝑀+)≥1−𝛼, where𝜃0∈𝑆𝑉𝑀+means SVM classifier predicts 𝜃0to have label 1.
This tells us that the SVM classifier can be tuned to preserve the asymptotic coverage of the confidence
set of interest, so we know using SVM classifier in place of the test with which we evaluate the
parameter values would be correct in the limit. Furthermore, as Lemma 1 dictates that SVM classifiers
can achieve perfect classification on training examples, we have that it is possible tune the SVM
classifier in a way that it completely re-produces the classification behavior of the test from which we
construct the confidence set. Formally, we state this result in the following corollary.
Corollary 1. Under the above assumptions, given any confidence set 𝐶𝑆⊂Θ, there exists tuning of
the SVM classifier such that a point 𝜃∈𝐶𝑆if and only if 𝜃∈𝑆𝑉𝑀+.
This finding is very useful in that once we finish training the SVM classifier on some moderately
sized grid, we can rely on it to determine the confidence regions on much denser grids and produce
about the same results as the original confidence sets. Evaluating points with an SVM is a simple
computation that is much more desirable than with the original test involving moment inequalities,
which can become very burdensome when the critical values depend on the parameter values.
- 14 -3.3.2 Fixed Sample Size
Let us now fix the sample size to some finite 𝑛. An easy thing to do is the analogous construction
to the above result, which will yield the same re-producing behavior of the SVM classifier. But instead
of the correct size 𝛼, the coverage is only lower bounded by the coverage of the corresponding 𝐶𝑆𝑛,
i.e.inf
𝜃∈Θ0P(𝜃∈𝐶𝑆𝑛|𝜃)(Θ0is the collection of parameters values consistent with the economic model),
which might not be at least 1−𝛼for any pre-specified 𝛼. And likewise, we require that the grid fully
exploresΘas it grows in cardinality. This is formalized in the theorem below.
Theorem 2. Given some finite sample size 𝑛, if we let the grid 𝑆grow, i.e.|𝑆|→∞ , in a way that
fully explores Θbased on an equidistributed sequence (as in Definition 2), then there exists tuning
such that{𝜃∈𝐶𝑆𝑛}implies{𝜃∈𝑆𝑉𝑀+}.4
Regardless of the preservation of coverage, this serves better as a theoretical check than being
a useful result in practice because it defeats the purpose of coming up with efficient computational
method if we actually evaluate the test on an infinite grid. Therefore, we would preferably like
to have some procedure that is more computationally manageable without inflating the size of the
uninformative grid to infinity, while trying to maintain the coverage probability of 𝐶𝑆𝑛. We can
continue to rely on the leading terms of some equidistributed sequence as the grid since this procedure
gives the correct asymptotic coverage as discussed above. Alternatively, we can iteratively add grid
points in regions with grid points of both classes to pin down the boundary for a better recovery of
the finite-sample confidence set. Once we initialize the grid with an equidistributed sequence, which
should have sufficient points along the boundary of Θ, we can investigate the neighborhoods of each
grid point with a pre-specified radius (such that there is at least one neighboring grid point in the
neighborhood for every grid point). If a neighbor has a label different from the centroid, we add a
new grid point between the centroid and the neighbor and obtain the label of this new point. In the
next iteration, we repeat the same step for the expanded grid. This continues until some stopping
criterion is met, and then we train the SVM classifier on the resulting grid. While this way we have
to introduce more parameters such as the initial grid, the neighborhood radius, maximum number
of iterations, etc., it allows for more flexibility and leaves case-dependent choices to the researcher,
hence could achieve better finite-sample results. It is worth pointing out that, as a drawback, regions
of the confidence set 𝐶𝑆𝑛that are undiscovered initially are unlikely to be visited later on, hence how
the coverage of the SVM classifier following this procedure compares to that of 𝐶𝑆𝑛is ambiguous.
Having and incorporating information about the true parameter 𝜃0would help reduce the occurrence
of such event. Without knowing better, our hope is that the initial grid would pick up all disconnected
regions of the underlying confidence set.
4 Simulation
In this section, we demonstrate the use as well as performance of the SVM classifiers under
the Gaussian RBF kernel in two simulation studies where we fit SVM classifiers to (i) a consistent
4Proof is a special case of Theorem 1 and Corollary 1, hence is omitted in this version.
- 15 -estimator for some pre-specified set and (ii) coefficient estimates of an ordinary least squares (OLS)
regression model, using arguably coarse grids especially in the latter case. We will showcase that the
effectiveness of our proposed method relies not only on the density of the grid, but more importantly
the properties of the inference procedure being reproduced, specifically its convergence rate as a
function of sample size. Furthermore, we will validate the idea that, even though the one-time training
of the SVM classifier may take some time (which is still trivial compared to evaluating the original
criterion), evaluating new grid points using the trained classifier takes negligible amount of time in
spaces of at least a few dimensions.
4.1 Consistent Estimate for a Set
In this first simulation study, we focus on parameter vectors of two dimensions for the purpose of
visualization. We first make up some true confidence region of interest and a consistent estimate of
the true region, followed by generating a grid which we evaluate using the consistent region. Then,
Figure 4: Sample sizes range from 20 to 5,000 and the corresponding grid sizes are between 1956 and
4259. By design, the estimated region (black) becomes very close to the true region (red) as sample
size increases. Green grid points are determined based on the estimated region.
- 16 -we fit the SVM classifier using the labeled grid and compare its predictions with the true region. We
also let the cardinality of the grid grow with sample size.
To begin with, let the true region be given by the union of (i) the red circle and (ii) the red ellipse
minus the two diamonds inside the ellipse as showed in the top-left and bottom-right panels of Figure
4. Pretending that we have no prior information, we use a non-informative grid generated using Monte
Carlo sequences whose cardinality is a function of the sample size. Particularly,
|𝑆|=
500∗log(𝑛)
,
where
·
rounds the number inside to the nearest integer. For demonstration purposes, the consistent
estimate of the true region obtained from our procedure, color-coded black in Figure 4, is constructed
by adding some noise to the true region which vanishes as the sample size increases, hence the es-
timate becomes more accurate with larger sample sizes as we see in Figure 4. We plug each grid
point into the analytical expression of the consistent estimate to assign a label to it. Subsequently, we
Figure 5: Sample size 𝑛=50. Left panels correspond to the same tuning and the right panels are
governed by another set of tuning. Perfect training classification is achieved in the top-right plot;
however, this does not shield us from making misclassification errors due to the small sample size.
- 17 -fit an SVM classifier with the coordinates of the grid points being the features and the labels being
the classes of the observations, to phrase it in the machine learning terms. And lastly, we compare
the labels of the grid points predicted by the trained SVM classifier with the labels dictated by the
true region and mark the mistakes in the plots. We repeat the exercise described here for different
sample sizes. The results when sample sizes are 50 and 5,000 are presented in Figure 5 and Figure 6,
respectively.
The SVM classifiers are trained under different sets of tuning thus display different classification
performance on the training grids. In the top panels, the black regions denote the consistent estimate
of the true region which we use to label the grid points, red dots are the grid points predicted by the
trained SVM classifiers to be “inside” the confidence set, and blue dots are predicted to be “outside”.
Furthermore, the crosses denote the mistakes by the SVM classifier. Specifically, red crosses are
outside grid points that are predicted to be inside the confidence regions, and the blue crosses are
inside grid points that are mistakenly predicted to be outside the region. The green regions in the
bottom panels are the true confidence region. The predictions are similarly labeled when assessed
Figure 6: Sample size 𝑛=5000 . Note that tuning for the right panels attains perfect classification
(see top-right plot). For larger sample sizes and denser grids, achieving high training accuracy is more
likely to make SVM classifier reproduce the true region.
- 18 -against the true region in the bottom panels. We see that although we can easily achieve perfect
classification on the training grid as is the case in the top-right panel of Figure 5, the bias due to the
small sample size still results in tons of misclassification underneath the surface with respect to the
true confidence region. Nonetheless, this procedure proves to be valid in that when we repeat the
same steps under a sample size of 5,000, not only can we attain perfection on the training grid, the
SVM classifier also largely captures the underlying true region as a composite result of the finer grid
and the consistency of the estimate.
4.2 OLS Coefficients
In the second simulation study, we introduce a simple parametric OLS model and explore the out-
of-sample performance of the SVM classifier. Specifically, we simulate data, including the covariates
and shocks, based on the model we make up, and generate grid points which we label using the
analytical formula for the confidence sets of OLS coefficients and which we use to train the SVM
classifier. We examine the correct prediction rates over data that our classifier has not seen and record
the computation time. The detailed steps are described as follows.
Consider a OLS model with an intercept and four covariates, so the parameter of interest is 5-
dimensional provided that everything else is known and not of interest. So, we draw a random vector
with five numbers taken as the true coefficients, 𝛽0. With a multivariate design matrix and some error
of length𝑛=500, it is easy to generate the dependent variable, hence the estimate of 𝛽0as well as its
covariance matrix. Next, we construct the Monte Carlo grid of cardinality |𝑆|=
6log(𝑛)
=68,534
and assign the labels with the following criterion. For each grid point 𝑠, we assign label 1if
(𝑠−ˆ𝛽)⊤ˆ𝑉−1(𝑠−ˆ𝛽)< 𝜒2
5(0.95),
where ˆ𝛽is the usual estimator for 𝛽0,ˆ𝑉is its covariance matrix, and 𝜒2
5(0.95)denotes the 95th
percentile of the 𝜒2distribution with 5degrees of freedom. Since a grid of such cardinality is very
sparse in the 5-dimensional real space, we end up with only 853grid points with label 1, which is
about 1.2%of all grid points. For different ratios between the two sub-grids, we repeat the following
2,000times and compute the average outcomes. We split the grid into training grid and test grid.
Then, we fit an SVM classifier using the training grid and their corresponding labels, and make it
predict the labels of the test grid to see its prediction accuracy. Additionally, we also test whether
𝛽0is predicted to have label 1, i.e. covered by the regions reproduced by the SVM classifier. Note
that𝛽0satisfies the above inequality, thus lies in the sample 95% confidence set. For the purpose
of comparison, we hold the tuning fixed throughout this subsection. The total run-time of 2,000
iterations is reported for each configuration (i.e. training-test split ratio). Results are summarized in
Table 1.
- 19 -Training Test Test accuracy (%) % Capturing 𝛽0Time (sec)
54,827 13,707 99.15 100.00 23,630
28,784 39,750 98.99 94.65 8,146
13,707 54,827 98.86 45.65 3,120
3,427 65,107 98.77 8.8 1,140
Table 1: Results from 2000 iterations of the described simulation study over a grid of 68,534points
on the Roar cluster.
Overall, both the prediction accuracy and coverage on 𝛽0grow when the classifier is trained with
more data, i.e. grid points, ceteris paribus. Moreover, it only takes small training sizes to yield high
test accuracies. On the other hand though, since incidentally very few grid points lie in the vicinity
of𝛽0in the original grid, coverages are low when the training-test ratio is low. This should raise no
concern because as the grid size grows in an equidistributed way, gaps are gradually filled with grid
points. We see that the coverage of 𝛽0very quickly picks up as we train the SVM classifier with more
grid points. While run-time appears to grow exponentially with training data, the testing process takes
almost no additional time. The implication of this in practice is, assigning labels on the denser grid
using the trained SVM classifier will be very fast.
4.3 Moment Condition Models
We simulate the construction of the confidence set based on moment conditions in this subsection.
In particular, we follow the approach proposed in Chernozhukov et al. (2007) (henceforth CHT)
and employed in Ciliberto and Tamer (2009) where the confidence set is based on keeping the all
parameter vectors that satisfy an objective function resulted from moment conditions. That is, the
collection of parameter values the evaluation of the criterion at which meets the moment conditions,
i.e.
𝐶𝑛(𝑐)={𝜃∈Θ:𝑛[ˆ𝑄𝑛(𝜃)−min
𝑡ˆ𝑄(𝑡)]≤𝑐} (21)
Note that this approach involves minimization of the objective function for the entire data followed
by minimization for each bootstrap subsample using some off-the-shelf solver, such as Nealder-Mead
and other genetic algorithm like simulated annealing in Ciliberto and Tamer (2009). Subsequently,
a grid search over 𝐶𝑛(𝑐(0))given some initial 𝑐(0)is performed to find the maximum of 𝑏(𝑄𝑏(𝜃)−
min𝑡𝑄𝑏(𝑡))for each bootstrap sample of size 𝑏. Once all bootstrap optimal values are computed, the
empirical distribution of the values is formed and the empirical 𝛼-quantile is chosen to replace 𝑐in 7.
Ciliberto and Tamer (2009) repeats the above construction of critical value 𝑐one more time, and then
retains all qualified parameter vectors via another grid search.
Consider a standard least squares problem with 3 parameter of interest, an intercept and two slope
coefficients, as the objective. We set the sample size 𝑛=1,000and bootstrap sample size 𝑏=250,
and draw some 𝑋∈R3to make𝑌for each observation. For simplicity, we assume the model is
correctly specified and hence obviating the inner minimization in the the above construct (7), which
reduces to the set{𝜃∈Θ:𝑛ˆ𝑄𝑛(𝜃)≤𝑐}where ˆ𝑄𝑛(𝜃)=1
𝑛Í
𝑖(𝑌𝑖−𝑋⊤
𝑖𝜃)2. We use Nealder-Mead
- 20 -Figure 7: The accepted grid points (blue) form a non-rectangular area. Reporting interval for each
parameter leads readers to potentially falsely accept points outside of the joint confidence set (red).
solver to obtain the minimal values of ˆ𝑄𝑛(𝜃)using the entire sample as well as in the bootstrap sub-
routine. The parameter space, a subset of R3, is divided into a uniform grid on the cube consisting of
101 values for each parameter and hence a total of 1013grid points. This is arguably a very coarse grid
since the points along each parameter dimension are fairly sparsely located, containing only 5 values
between each pair of integers. Nonetheless, performing the brute-force CHT routine on such grid,
with a programming choice of R without parallelization, runs for 3 hours on Penn State Roar Collab
server with 16GB memory. We repeat the same exercise on a smaller grid with 51 points along each
dimension and use the larger grid as benchmark to examine the difference between standard procedure
and the use of SVM classifier.
The relationship between the two slope coefficients is depicted in Figure 7 based on the search on
the513grid. The blue dots are accepted according to CHT procedure and the red crosses are rejected.
We note that once we report the confidence interval for each parameter, information is lost in that
both blue dots and red crosses appear to be in the joint rectangle. For the SVM classifier, we train the
model based on the coarser grid and examine its ability to capture the underlying confidence region
using the large grid of 1013points. As described above, labels for the purpose of classification are
assigned according to each grid point’s membership to the confidence set, i.e. 1 if the grid point is
accepted according to CHT and -1 otherwise. We calculate the percentages of grid points classified
to be in the CHT CS out of all grid points on both the training grid (i.e. the 513grid) and the test grid
(i.e. the 1013grid), denoted as “% Labeled CS”. We also compare the predicted labels against the
labels dictated by the CHT procedure, and denote the percentage of labels agreed by both approaches
as “% Correct”. Results are reported in Table 2. The same calculations for the CHT procedure with
rectangular region is also included for exposition. We emphasize again that, rather than making wrong
predictions, the collection of individual intervals does not tell us anything about the confidence set
- 21 -Method % Labeled CS ( 513) % Correct ( 513) % Labeled CS ( 1013) % Correct ( 1013)
Rectangle 86.275 58.758 87.128 44.695
SVM (no tuning) 45.053 99.928 45.165 98.540
SVM (tuned) 45.032 100.000 44.681 99.024
Table 2: Comparison between standard reporting of intervals and SVM classifiers.
but its minimum bounding box, which the SVM classifier is able to overcome.
An immediate observation is the rectangle falsely includes many grid points that do not belong
to the CS and leads to lower statistical power due to the fact that the joint confidence region does
not admit a rectangular shape. To correct this means we would have to store all the accepted grid
points; and even so, there is generally no prescription on new parameter vectors, e.g. a point that lies
in the convex hull but outside of the concave hull of the accepted grid points, without evaluating the
criterion that is based on moment conditions. Therefore, it would be much more concise and precise
to consider instead the decision function of the form in equation (20) for 𝐾(𝜃1,𝜃2)=exp{||𝜃1−
𝜃2||2/𝜎2}with a properly tuned 𝜎2. This allows us to easily determine if any parameter vector
including those outside the grid points belongs to the joint confidence region by simply evaluating the
function at the parameter vector. Lastly, we note that while such grid of 513points is fairly coarse, it is
already so abundant that tuning of the kernel is essentially unnecessary if time is of utmost important.
In the un-tuned classifier, the number of support vectors is 4,901, which amount to summing up
4,901 evaluations of simple operations. It takes about 16 seconds in the aforementioned computing
environment for the trained classifier to classify each 10,000 grid points. The properly tuned classifier
is comprised of 56,004 support vectors in this case.
5 Conclusion
In this paper, we devise an efficient and reliable way of reproducing the confidence sets in the
context of moment conditions. These problems typically involve inverting the test and sub-sampling
or re-sampling when constructing critical values, and are computationally complex especially when
the dimensionalily of the parameter grows. Combining the uses of the support vector machine and the
equidistributed sequences, we are able to consistently re-produce the limiting confidence regions as
the sample size grows under common assumptions and mild conditions. This procedure is very easy
to implement in practice. Researchers can generate grid points and fit the SVM classifier utilizing the
existing packages in standard programming languages. Researchers can also incorporate an adaptive
procedure into the generation of grid points for better approximation of the confidence sets when the
sample size is fixed. Additionally, the use of decision functions provides much higher efficiency and
precision than reporting the rectangular regions from the exhaustive search. This approach might also
be used to compute and characterize the identified sets, or the confidence sets of the identified sets, so
long as they have a similar formulation.
- 22 -References
Andrews, I., Roth, J., and Pakes, A. (2019). Inference for linear conditional moment inequalities.
Aradillas-Lopez, A. and Gandhi, A. (2016). Estimation of games with ordered actions: An application
to chain-store entry: Estimation of games with ordered actions. Quantitative Economics , 7:727–
780.
Chandrasekharan, K. (1969). Introduction to analytic number theory.
Chernozhukov, V ., Hong, H., and Tamer, E. (2007). Estimation and confidence regions for parameter
sets in econometric models. Econometrica , 75(5):1243–1284.
Ciliberto, F. and Tamer, E. (2009). Market structure and multiple equilibria in airline markets. Econo-
metrica , 77(6):1791–1828.
Cortes, C. and Vapnik, V . (1995). Support vector networks. Machine Learning , 20:273–297.
Cox, G. and Shi, X. (2022). Simple Adaptive Size-Exact Testing for Full-Vector and Subvector
Inference in Moment Inequality Models. The Review of Economic Studies , 90(1):201–228.
Guliyev, N. J. and Ismailov, V . E. (2018). Approximation capability of two hidden layer feedforward
neural networks with fixed weights. Neurocomputing , 316:262–269.
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The Elements of Statistical Learning . Springer
Series in Statistics. Springer New York Inc., New York, NY , USA.
Imbens, G. W. and Manski, C. F. (2004). Confidence intervals for partially identified parameters.
Econometrica , 72(6):1845–1857.
Judd, K. L. (1998). Numerical Methods in Economics , volume 1 of MIT Press Books . The MIT Press.
Kalouptsidi, M., Kitamura, Y ., Lima, L., and Souza-Rodrigues, E. A. (2020). Partial identification
and inference for dynamic models and counterfactuals. Working Paper 26761, National Bureau of
Economic Research.
Keerthi, S. S. and Lin, C.-J. (2003). Asymptotic behaviors of support vector machines with gaussian
kernel. Neural Comput. , 15(7):1667–1689.
Li, L. and Henry, M. (2022). Finite Sample Inference in Incomplete Models. Papers 2204.00473,
arXiv.org.
Manski, C. F. and Tamer, E. (2002). Inference on regressions with interval data on a regressor or
outcome. Econometrica , 70(2):519–546.
Miranda, M. J. and Fackler, P. L. (2004). Applied Computational Economics and Finance , volume 1
ofMIT Press Books . The MIT Press.
- 23 -Romano, J. P. and Shaikh, A. M. (2010). Inference for the identified set in partially identified econo-
metric models. Econometrica , 78(1):169–211.
Rosen, A. (2008). Confidence sets for partially identified parameters that satisfy a finite number of
moment inequalities. Journal of Econometrics , 146(1):107–117.
Wolak, F. A. (1991). The local nature of hypothesis tests involving inequality constraints in nonlinear
models. Econometrica , 59(4):981–995.
Wu, H.-S. and Zhang, F.-M. (2014). Wolf pack algorithm for unconstrained global optimization.
Mathematical Problems in Engineering .
Zhu, K., Wang, H., Bai, H., Li, J., Qiu, Z., Cui, H., and Chang, E. (2007). Parallelizing support vector
machines on distributed computers. In Platt, J., Koller, D., Singer, Y ., and Roweis, S., editors,
Advances in Neural Information Processing Systems , volume 20. Curran Associates, Inc.
Appendix A Equidistributed Sequences
The equidistributed sequences are designed to best explore the spaces of our interest. Some se-
quences are random in nature, such as the Monte Carlo sequences, whereas some others display
patterns as we will see below. We first introduce the definition of equidistributed sequences, and
then provide an argument that they leave ”no gap” in the space. In other words, for any point in the
𝑑-dimensional Euclidean space, we can approach the point arbitrarily closely with equidistributed
sequences. Denote 𝜆𝑑as the𝑑-dimensional Lebesgue measure. The definition of equidistributed se-
quences is given below, extending the 1-dimensional definition introduced in Chandrasekharan (1969)
to potentially high-dimensional space R𝑑.
Definition 4. A sequence𝑆=(𝑠1,𝑠2,𝑠3,···) inR𝑑is said to be equidistributed on the boxΓ⊂R𝑑if
for every box Δ⊆Γ,
lim
𝑛→∞Í𝑛
𝑖=11{𝑠𝑖∈Δ}
𝑛=𝜆𝑑(Δ)
𝜆𝑑(Γ).
With this, we can proceed to show that grids generated using equidistributed sequences explore
the space in an exhaustive manner. Specifically, for any point in the space, we will show that, in the
limit, there will always exist a point from an equidistributed sequence which is in any arbitrarily small
neighborhood of the point.
Lemma 3. Let𝑆=(𝑠1,𝑠2,𝑠3,···) be an equidistributed sequence in Γ⊂R𝑑. Then,∀𝜃∈Γand
∀𝜖 >0, there exists some 𝑠∗∈𝑆such that||𝜃−𝑠∗||≤𝜖.
Proof. For some𝜃∈Γ, let𝐵𝜖(𝜃)be the ball centered at 𝜃with radius𝜖. First, we consider 𝜃∈int(Γ),
and assume for now 𝐵𝜖(𝜃)⊂Γ. Let𝐶𝜖(𝜃)be the hypercube such that the distance from any vertex
to its center𝜃is𝜖. In 2 dimensions, 𝐶𝜖(𝜃)is the square centered at 𝜃whose vertices lie on the circle
𝐵𝜖(𝜃); and in 3 dimensions, 𝐶𝜖(𝜃)is a cube (whose edges are of equal length) centered at 𝜃with
- 24 -all vertices lying on the ball 𝐵𝜖(𝜃). Since𝐶𝜖(𝜃)is a box inΓand𝜆𝑑(𝐶𝜖(𝜃))>0, it follows that
𝜆𝑑(𝐶𝜖(𝜃))
𝜆𝑑(Γ)>0. Therefore, by definition of equidistributed sequences, we have that
𝜆𝑑(𝐶𝜖(𝜃))
𝜆𝑑(Γ)=lim
𝑛→∞Í𝑛
𝑖=11{𝑠𝑖∈𝐶𝜖}
𝑛>0.
This implies that 𝑆Ñ𝐶𝜖(𝜃)≠∅, because there must be a positive number of points in 𝑆that lie in
𝐶𝜖(𝜃). Take𝑠∗∈(𝑆Ñ𝐶𝜖(𝜃)), and we have 𝑠∗∈𝐵𝜖(𝜃), or equivalently||𝜃−𝑠∗||≤𝜖.
To complete the proof, if 𝐵𝜖(𝜃)⊊Γ, we take instead ¯𝜖∈(0,𝜖)such that𝐵¯𝜖(𝜃)⊂(𝐵𝜖(𝜃)ÑΓ)
(which cannot be empty since 𝜃∈int(Γ)), and apply the above analysis to 𝐵¯𝜖(𝜃), hence the point
𝑠∗∈𝐵¯𝜖(𝜃)⊂𝐵𝜖(𝜃). And lastly, if 𝜃lies on a face (or edge) of Γ, we construct a hyperrectangle
inside the intersection of 𝐵𝜖(𝜃)andΓwith positive volume (with respect to 𝜆𝑑) such that𝜃lies on the
face (or edge) of the hyperrectangle. Then we apply the same argument as above. ■
Now we present how the grid points look under different generating sequences. Figures 8 through
11 show in the two-dimensional space the grid points generated from Monte Carlo sequences, Sobol
sequences, Weyl sequences, Bakers sequences, respectively. Going from the top-left panel to the
bottom-right, we present the grids of 20, 100, 500, and 2000 grid points in each of the following four
figures.
Figure 8: Monte Carlo Points
We see that the Monte Carlo random sequences, although random in nature, leave out a lot of
blank regions, hence might not buy us the optimal performance sampling the space. The uniformity
issue is addressed better by the Sobol sequences. Besides the pattern, we see that the grid points are
- 25 -Figure 9: Sobol Points
Figure 10: Weyl Points
- 26 -Figure 11: Baker Points
pretty evenly spread out in the space. Furthermore, Weyl and Baker points display patterns which can
be use when we believe certain regions of the parameter space are under-visited.
Appendix B Proof of the Main Results
B.1 Proof of Lemma 2
Proof. We will use a standard argument based on the definition of the interior points. By definition,
for every𝜃∈𝐶𝑆, there exists a neighborhood 𝐵(𝜃)(with positive radius ¯𝜖 >0) such that𝐵(𝜃)⊂𝐶𝑆.
Since for every 𝑛,𝐸𝑛(𝜃)∉𝐶𝑆𝑛, in the limit, it follows that lim
𝑛→∞𝐸𝑛(𝜃)∉𝐶𝑆, thus the nearest exterior
point will be at least ¯𝜖away from𝜃. We can simply set 𝑀=¯𝜖.
Moreover, by Assumption 2 and Lemma 3, the grid will grow in a way that gradually fills the entire
parameter space, i.e. there are grid points arbitrarily close to any point in the space. Then, it follows
that
||𝐼𝑛+1−𝜃||≤||𝐼𝑛−𝜃||
where
𝐼𝑛≡arg min
𝐼∈𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟𝑛||𝐼−𝜃||,
and𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟
𝑛 denotes those grid points of 𝑆𝑛that are inside 𝐶𝑆, because𝐼𝑛∈𝑆𝑛+1. We evaluate the
growing grid using the same test given in 𝐶𝑆to avoid the issue of evolving boundaries, which can
- 27 -cause certain 𝜃to be inside the 𝐶𝑆𝑛but outside𝐶𝑆𝑛′for some𝑛≠𝑛′. So, we have that
lim
𝑛→∞||𝐼𝑛−𝜃||=0.
■
B.2 Proof of Theorem 1
Proof. The idea of this proof is to find a sufficient condition which implies the dominant influence of
the nearest interior grid point over all exterior grid points by the construction of the grid points. Here
we will assume that 𝜃lies exactly on the boundary of 𝐶𝑆w.p.0, so we can focus on the case where
𝜃∈int(𝐶𝑆). Since we want the following to be true
max
𝑖∈𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟𝑛𝐾(𝑖,𝜃)>∑︁
𝑗∈𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟𝑛𝐾(𝑗,𝜃),
where𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟
𝑛 denotes those grid points of 𝑆𝑛that are outside 𝐶𝑆, if we can show the following
sufficient condition,
max
𝑖∈𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟𝑛𝐾(𝑖,𝜃)>|𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟
𝑛|·max
𝑗∈𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟𝑛𝐾(𝑗,𝜃),
where|·|denotes the cardinality of the collection of grid points, we would establish that the influence
of the nearest interior grid point dominates the combined influence of all exterior grid points, which
then implies that our SVM classifier labels this point 𝜃as 1. Equivalently, we would like to show the
following condition holds true
max
𝑖∈𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟𝑛exp
−||𝑖−𝜃||2
2𝜎2𝑛
>|𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟
𝑛|·max
𝑗∈𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟𝑛exp
−||𝑗−𝜃||2
2𝜎2𝑛
⇔exp
−||𝐼𝑛(𝜃)−𝜃||2
2𝜎2𝑛
>|𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟
𝑛|·exp
−||𝐸𝑛(𝜃)−𝜃||2
2𝜎2𝑛
,
which requires that
exp||𝐸𝑛(𝜃)−𝜃||2−||𝐼𝑛(𝜃)−𝜃||2
2𝜎2𝑛
>|𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟
𝑛|,
or
0<2𝜎2
𝑛<||𝐸𝑛(𝜃)−𝜃||2−||𝐼𝑛(𝜃)−𝜃||2
log|𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟𝑛|. (22)
By Lemma 2,∀𝜃∈int(𝐶𝑆),∃𝑁∈Nsuch that∀𝑛> 𝑁 ,||𝐼𝑛(𝜃)−𝜃||2<||𝐸𝑛(𝜃)−𝜃||2, and hence such
𝜎2
𝑛exists that satisfies 22. Therefore, choosing the tuning parameter this way for each 𝑛 > 𝑁 makes
the influence of the nearest interior grid dominate the influence of all exterior grid points in the limit.
Consequently, the SVM classifier trained over these grids under such tuning classifies any 𝜃∈𝐶𝑆to
have label 1; that is, {𝜃∈𝐶𝑆}⇒{𝜃∈𝑆𝑉𝑀+}. Because P(𝜃0∈𝐶𝑆)≥1−𝛼, it follows that
P(𝜃0∈𝑆𝑉𝑀+)≥P(𝜃0∈𝐶𝑆)≥1−𝛼
■
- 28 -B.3 Proof of Corollary 1
Proof. The forward direction of this result is showed in the proof in the above section of Theorem 1.
It only remains to show the backward direction; that is, if a point 𝜃is classified by the SVM classifier
to have label 1, then it must be in 𝐶𝑆, too. We will show the contrapositive statement of the backward
direction holds true.
Take a point ˜𝜃∉𝐶𝑆, or𝜃∈𝐶𝑆≡Θ\𝐶𝑆, we want to show that we can find values of the tuning
parameter𝜎2such that SVM classifier classifies ˜𝜃to have label−1. Following the same argument in
the proof of Lemma 2, as we expand the grid, the Euclidean distance from ˜𝜃to any interior grid point
of𝐶𝑆(with label 1) is bounded away from 0; whereas the distance from ˜𝜃to the nearest exterior grid
point (with label−1) decreases to 0in the limit. Notice that the interior and exterior grid points are
defined in reference to 𝐶𝑆, the same way as above. Mathematically, we have
1.∃˜𝑀 > 0such that||𝐼𝑛(˜𝜃)−˜𝜃||≥ ˜𝑀 > 0, for all𝑛; and
2.lim
𝑛→∞||𝐸𝑛(˜𝜃)−˜𝜃||→ 0.
Now, in order for SVM to classify ˜𝜃as−1, a sufficient condition is
max
𝑠𝑖∈𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟𝑛𝐾(𝑠𝑖,˜𝜃)>|𝑆+|· max
𝑠𝑗∈𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟𝑛𝐾(𝑠𝑗,˜𝜃)
⇔ max
𝑠𝑖∈𝑆𝑒𝑥𝑡𝑒𝑟𝑖𝑜𝑟𝑛exp
−||𝑠𝑖−˜𝜃||2
2𝜎2𝑛
>|𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟
𝑛|· max
𝑠𝑗∈𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟𝑛exp 
−||𝑠𝑗−˜𝜃||2
2𝜎2𝑛!
Solving the above inequality yields
0<2𝜎2
𝑛<||𝐼𝑛(˜𝜃)−˜𝜃||2−||𝐸𝑛(˜𝜃)−˜𝜃||2
log|𝑆𝑖𝑛𝑡𝑒𝑟𝑖𝑜𝑟𝑛|(23)
Since for large enough sample size 𝑛,||𝐼𝑛(˜𝜃)−˜𝜃||2>||𝐸𝑛(˜𝜃)−˜𝜃||2, the chain of inequalities in 23 is
well-defined. Therefore, we can find values for the tuning parameter 𝜎2
𝑛for all𝑛’s that are sufficiently
large such that a point ˜𝜃in the complement of 𝐶𝑆will be labelled−1by the SVM classifier.
Combining this with Theorem 1, it follows that when the sample size is large enough, by taking the
values in the intersection of 22 and 23 for the tuning parameter 𝜎2, we have that a point 𝜃∈Θbelongs
to𝐶𝑆if and only if the SVM classifier under such tuning classifies it to have label 1. ■
- 29 -