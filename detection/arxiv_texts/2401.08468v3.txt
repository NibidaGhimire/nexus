Nonparametric Evaluation of Noisy ICA Solutions
Syamantak Kumar∗1, Purnamrita Sarkar†2, Peter Bickel‡3, and Derek Bean§4
1Department of Computer Science, University of Texas at Austin
2Department of Statistics and Data Sciences, University of Texas at Austin
3Department of Statistics, University of California, Berkeley
4Department of Statistics, University of Wisconsin, Madison
Abstract
Independent Component Analysis (ICA) was introduced in the 1980’s as a model for Blind Source
Separation (BSS), which refers to the process of recovering the sources underlying a mixture of signals,
with little knowledge about the source signals or the mixing process. While there are many sophisticated
algorithms for estimation, different methods have different shortcomings. In this paper, we develop a
nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise. The
novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses
the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge
of the parameters of the noise distribution. In addition, we propose some new contrast functions and
algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE
but work in domains where the former may fail. While these also may have weaknesses, our proposed
diagnostic, as shown by our simulations, can remedy them. Finally, we propose a theoretical framework
to analyze the local and global convergence properties of our algorithms.
1 Introduction
Independent Component Analysis (ICA) was introduced in the 1980’s as a model for Blind Source Separation
(BSS) [ 13,12], which refers to the process of recovering the sources underlying a mixture of signals, with
little knowledge about the source signals or the mixing process. It has become a powerful alternative to the
Gaussian model, leading to PCA in factor analysis. A good account of the history, many results, and the role
of dimensionality in noiseless ICA can be found in [ 28,5]. In one of its first forms, ICA is based on observing
nindependent samples from a k-dimensional population:
xxx=Bzzz+ggg (1)
whereB∈Rd×k(d≥k) is an unknown mixing matrix, zzz∈Rkis a vector of statistically independent
mean zero “sources” or “factors” and ggg∼N(000,Σ) is addimensional mean zero Gaussian noise vector with
covariance matrix Σ, independent of zzz. We denote diag (aaa):=cov (zzz)andS:=cov (xxx)=Bdiag (aaa)BT+ Σ.
The goal of the problem is to estimate B.
ICA was initially developed in the context of ggg= 0, now called noiseless ICA. In that form, it spawned an
enormous literature [ 17,39,40,1,8,27,35,37,23,11,6,42] and at least two well-established algorithms
FASTICA [ 29] and JADE [ 9] which are widely used. The general model with nonzero gggand unknown
noise covariance matrix Σ, known as noisy ICA, has developed more slowly, with its own literature
[4,50,49,30,31,41]. In the following sections, we will show that various ICA and noisy ICA methods
∗syamantak@utexas.edu
†purna.sarkar@utexas.edu
‡bickel@stat.berkeley.edu
§derekb@stat.wisc.edu
1arXiv:2401.08468v3  [math.ST]  5 Nov 2024have distinct shortcomings. Some struggle with heavy-tailed distributions and outliers, while others require
approximations of entropy-based objectives, which have their own challenges (see eg. [ 32]). Although
methods for noiseless cases may sometimes work in noisy settings [ 49], they are not always reliable (e.g., see
Figure 1 in [ 49] and Theorem 2 in [ 11]). Despite the plethora of algorithms for noisy and noiseless ICA, the
literature has largely been missing a diagnostic to decide which algorithm to pick for a given dataset. This is
our primary goal.
The paper is organized as follows. Section 2 contains background and notation. Section 3.1 contains the
independence score, its properties, and a Meta algorithm 1 that utilizes this score. Section 3.3 introduces
new contrast functions based on the natural logarithm of the characteristic function and moment generating
function of xxxTuuu. Section 4 presents the global and local convergence results for noisy ICA. Section 5
demonstrates the empirical performance of our new contrast functions as well as the Meta algorithm applied
to a variety of methods for inferring B.
2 Background and overview
This section contains a general overview of the main concepts of ICA.
Notation: We denote vectors using bold font as vvv∈Rd. The dataset is represented as{︁
xxx(i)}︁
i∈[n]for
xxx(i)∈Rd. Scalar random variables and matrices are represented using upper case letters, respectively. Ik
denotes the k- dimensional identity matrix. Entries of vector vvv(matrixM) are represented as vi(Mij).
M(:,i) (M(i,:)) represents the ithcolumn (row) of matrix M.M†represents the Moore–Penrose inverse of
M.∥.∥represents the Euclidean l2norm for vectors and operator norm for matrices. ∥.∥Fsimilarly represents
the Frobenius norm for matrices. E[.](ˆ︁E[.]) denotes the expectation (empirical average); xxx⊥ ⊥yyydenotes
statistical independence. ∥.∥ψ2represents the subgaussian Orlicz norm (see Def. 2.5.6 in [ 46]).Xn=OP(an)
implies thatXn
anis stochastically bounded i.e, ∀ϵ>0, there exists a finite Msuch that supnP(︂⃓⃓⃓Xn
an⃓⃓⃓>M)︂
≤ϵ
(See [45]).
Identifiability: One key issue in Eq 1 is the identifiability of A:=B−11, which holds up to permutation
and scaling of the coordinates of zzzif, at most, one component of zzzis Gaussian (see [ 14,43]). Ifd=kand Σ
are unknown, it is possible, as we shall see later, to identify the canonical versions of Band Σ. For d>k and
unknown Σ, it is possible (see [ 15]) to reduce to the d=kcase. In this work, we assume d=k. For classical
factor models, when zzzis Gaussian, identifiability can only be identified up to rotation and scale, leading
to non-interpretability. This suggests fitting strategies focusing on recovering B−1through nongaussianity
as well as independence. In the noisy model (Eq 1), an additional difficulty is that recovery of the source
signals becomes impossible even if Bis known exactly. This is because, the ICA models x=B(z+r) +g
andx=Bz+ (Br+g) are indistinguishable (see [ 48] pages 2-3). This is resolved in [ 48] via maximizing the
Signal-to-Interference-plus-Noise ratio (SINR). In classical work on ICA, a large class of algorithms (see [ 37])
choose to optimize a measure of non-gaussianity, referred to as contrast functions.
Contrast functions: Methods for estimating Btypically optimize an empirical contrast function.
Formally, we define a contrast function f(uuu|P) byf:Bk×P→ RwhereBkis the unit ball in Rkand
Pis essentially the class of mean zero distributions on Rd. More concretely, for suitable choices of g,
f(uuu|P)≡f(uuu|xxx) =Exxx∼P[g(uuuTxxx)] for any random variable xxx∼P. The most popular example of a contrast
function is the kurtosis, which is defined as f(uuu|P) =Exxx∼P(uuuTxxx)4/︁
E[︁
(uuuTxxx)2]︁2−3. The aim is to maximize
an empirical estimate, fˆ(uuu|P). Notable contrast functions are the negentropy, mutual information (used by
INFOMAX [7]), the tanh function, etc. (See [37] for further details).
Fitting strategies: There are two broad categories of the existing fitting strategies. (I) Find Asuch
that the components of Axxxare both as nongaussian and as independent as possible. See, for example, the
multivariate cumulant-based method JADE [ 9] and characteristic function-based methods [ 21,11]. The latter
we shall call the PFICA method. (II) Find successively the jthrow ofA, denoted by A(j,:)∈Rd,j= 1,...,k
such thatA(j,:)Txxxare independent and each as nongaussian as possible, that is, estimate A(1,:), project,
and then apply the method again on the residuals successively. The chief representative of these methods is
FastICA [27] based on univariate kurtosis.
1IfBis not invertible, this can be replaced by the Moore-Penrose inverse.
2From noiseless to noisy ICA: In the noiseless case one can first prewhiten the dataset using the
empirical variance-covariance matrix, Σ ˆ, ofxxx(i), using Σ ˆ−1/2. Therefore, WLOG, Bcan be assumed to be
orthogonal. Searching over orthogonal matrices not only simplifies algorithm design for fitting strategy (I)
but also makes strategy (II) meaningful. It also greatly simplifies the analysis of kurtosis-based contrast
functions (see [ 18]). For noisy ICA, prewhitening requires knowledge of the noise covariance, and therefore
many elegant methods [4, 48, 49] avoid this step.
Individual shortcomings of different contrast functions and fitting strategies: To our knowledge,
no single ICA algorithm or contrast function works universally across all source distributions. Adaptively
determining which algorithm is useful in a given setting would be an important tool in a practitioner’s toolbox.
Key challenges include:
Cumulants: Even though contrast functions based on even cumulants have no spurious local optima
(Theorem 3, [ 49]), they can vanish for some non-Gaussian distributions. Our experiments (Section 5) show
that kurtosis-based contrast functions can perform poorly even when there are a few independent components
with zero kurtosis. They also suffer under heavy-tailed source distributions [3, 11, 26].
PFICA: PFICA can outperform cumulant-based methods in some situations (Section 5). However, it is
computationally much slower, and poorly performs for Bernoulli( p) sources with small p.
FastICA: [32] show that FastICA’s use of Negentropy approximations for computational efficiency may
result in a contrast function where optimal directions do not correspond to directions of low entropy. tanh is
another popular smooth contrast function used by FastICA. However, in [ 52], the authors show that this
tends to return spurious solutions for some distributions [52].
Contrast functions are usually nonconvex and may have spurious local optima. However, they may still,
under suitable conditions, converge to maximal directions. We introduce such a contrast function which
vanishes iff xxxis Gaussian in Section 3.3 and does not require higher moments.
Our contributions:
1) Using Theorem 1 we obtain a computationally efficient scoring method for evaluating the Bestimated
by different algorithms. Our score can also be extended to evaluate each demixing direction in sequential
settings, which we do not pursue here.
2) We propose new contrast functions that work under scenarios where cumulant-based methods fail. We
propose a fast sequential algorithm as in [ 49] to optimize these and further provide theoretical guarantees for
convergence.
3 Main contributions
In this section, we present the “Independence score” and state the key result that justifies it. We conclude
with two new contrast functions.
3.1 Independence score
While there are many tests for independence [ 22,34,42,6], we choose the Characteristic function-based one
because it is easy to compute and has been widely used in normality testing [ 19,20,25] and ICA [ 11,21,44].
Let us start with noiseless ICA to build intuition. Say we have estimated the inverse of the mixing matrix, i.e.
B−1using a matrix F. Ideally, if F=B−1, thenFxxx=zzz, wherezzzis a vector of independent random variables.
Kac’s theorem [ 33] tells us that E[︁
exp(itTzzz)]︁
=∏︁k
j=1E[exp(itjzj)]if and only if ziare independent. In [ 21],
a novel characteristic function-based objective (CHFICA), is further analyzed and studied by [ 11] (PFICA).
Here one minimizes |E[︁
exp(itTFxxx)]︁
−∏︁k
j=1E[exp(itj(Fxxx)j)]|overF. We refer to this objective as the
uncorrected independence score. We propose to adapt the CHFICA objective using estimable parameters ,S,
to the noisy ICA setting. We will minimize:
∆(ttt,F|P) :=⃓⃓⃓⃓⃓⃓⃓⃓E[︁
exp(itttTFxxx)]︁
exp(︃
−tttTdiag(FSFT)ttt
2)︃
⏞ ⏟⏟ ⏞
Joint−k∏︂
j=1E[exp(itj(Fxxx)j)] exp(︃
−tttTFSFTttt
2)︃
⏞ ⏟⏟ ⏞
Product⃓⃓⃓⃓⃓⃓⃓⃓⃓⃓(2)
3whereS=E[xxxxxxT] and can be estimated using the sample covariance matrix. Hence, we do not require
knowledge of any model parameters. We refer to this score as the ( corrected ) independence score. The
second terms in JOINT and PRODUCT (Eq 2) corrects the original score by canceling out the additional
terms resulting from the Gaussian noise using the covariance matrix S of the data (estimated via the sample
covariance). See [ 21] for a related score requiring knowledge of the unknown Gaussian covariance matrix Σ.
We are now ready to present our first theoretical result for consistency of ∆( ttt,F|P).
Theorem 1. IfF∈Rk×kis invertible and the joint and marginal characteristic functions of all independent
components,{zi}i∈[k], are twice-differentiable, then ∀ttt∈Rk,∆(ttt,F|P) = 0 iffF=DB−1whereDis a
permutation of an arbitrary diagonal matrix.
Unfortunately, Fis not uniquely defined if Σ is unknown as we have noted in Section 1. The proof of
Theorem 1 is provided in the Appendix Section A.1. When using this score in practice, Sis replaced with
the sample covariance matrix of the data, and the expectations are replaced by the empirical estimates of
the characteristic function. The convergence of this empirical score, ∆( t,F|Pˆ), to the population score,
∆(t,F|P), is given by the following theorem.
Theorem 2. LetF:={F∈Rk×k:∥F∥≤1},xxx∼subgaussian (σ)andCk:= max(1,klog (n) Tr(S)). Then,
we have
sup
F∈F|Ettt∈N(0,Ik)∆(ttt,F|P)−Ettt∈N(0,Ik)∆(ttt,F|Pˆ)|=OP⎛
⎝√︄
k2∥S∥max(k,σ4∥S∥) log2(nCk)
n⎞
⎠
This bound shows that uniformly over F, the empirical average Ettt∈N(0,Ik)∆(ttt,F|Pˆ), is close to the
population score. This guarantees that as long as the difference between the population scores of the two
candidate algorithms is not too small, the meta-algorithm can pick up the better score.
Remark 1 (Subgaussianity assumption) .The subgaussianity assumption in Theorem 2 simply ensures the
concentration of the sample covariance matrix since it is used in the score (see Theorem A.3, line 825). It can
be relaxed if the concentration in the operator norm is ensured. Appendix Section A.2.1 contains experiments
with more super-Gaussian source signals.
The proof of this result is deferred to the Appendix section A.1.1. It is important to note that ∆( ttt,F|Pˆ)
is not easy to optimize. As we show in Section 3.2, objective functions that are computationally more
amenable to optimize for ICA, e.g., cumulants, satisfy some properties (see Assumption 1). The independence
score does not satisfy the first property (Assumption 1 (a)). In the noiseless case, whitening reduces
the problem to Bbeing orthogonal. This facilitates efficient gradient descent in the Stiefel manifold of
orthogonal matrices [ 11]. However, in the noisy setting, the prewhitening matrix contains the unknown noise
covariance Σ, making optimization hard. Furthermore, as noted in Remark 2, in practice, it is better to
useEttt∼N(0,Ik)∆(ttt,F|Pˆ) [11,21,25] instead of using a fixed vector, ttt, to evaluate ∆( ttt,F|Pˆ). Any iterative
optimization method requires repeatedly estimating this expectation at each gradient step. Therefore, instead
of using this score directly as the optimization objective, we use it to choose between different contrast
functions after extracting the demixing matrix with each. This process is referred to as the Meta algorithm
(Algorithm 1).
Remark 2 (Averaging over ttt).Algorithm 1 averages the independence score over ttt∼N (0,Ik). While Eq 2
defines the independence score for one direction ttt, in practice, however, there may be some directions such that
a non-gaussian signal has a small score in that direction. Hence, it is desirable to average over ttt, following
the convention in [11, 21, 25].
To gain an intuition of the independence score, we conduct an experiment where we use the dataset
mentioned in Section 5 (Figure 2b) and compute the independence score for B′=ϵB+(1−ϵ)I,ϵ∈(0.5,1).
As we increase ϵ,B′approaches B, and hence the Amari error dB′,B(see Eq 8) decreases. Figure 1 shows the
independence score versus Amari error, indicating that the independence score accurately predicts solution
quality even without knowing the true B.
4Algorithm 1 Meta-algorithm for choosing best
candidate algorithm.
Input : Algorithm listL, DataX∈Rn×k
forjin range[1, size (L)]do
Bj← Lj(X){Extract mixing matrix Bj
usingjthcandidate algorithm }
δj←ˆ︁Ettt∼N(0,Ik)[︂
∆(︂
t,B−1
j|Pˆ)︂]︂
end for
i∗←arg minj∈[size(L)][δj]
returnBi∗
Figure 1: Correlation of independence score (with std.
dev.) with Amari error between B′=ϵB+(1−ϵ)I
andB, averaged over 10 random runs.
3.2 Desired properties of contrast functions
The following properties are often useful for designing provable optimization algorithms for ICA.
Assumption 1 (Properties of contrast functions) .Letfbe a contrast function defined for a mean zero
random variable X. Then,
(a)f(u|X+Y) =f(u|X) +f(u|Y)forX⊥ ⊥Y
(b)f(0|X) = 0,f′(0|X) = 0 ,f′′(0|X) = 0
(c)f(u|G) = 0,∀uforG∼N (0,1)
(d) WLOG, f(u|X) =f(−u|X)(symmetry)
Properties (a) and (c) ensure that f(u|G+X) =f(u|X) for non-gaussian Xand independent non-gaussian
G, which means that the additive independent Gaussian noise does not change f. Property (c) ensures that
|f(u|G)|is minimal for a Gaussian. Property (d) holds without loss of generality because one can always
symmetrize the distribution.2
3.3 New contrast functions
In Section 2, we provided a discussion of the individual shortcomings of different contrast functions for existing
contrast functions. Before we introduce new contrast functions in this section, we revisit the algorithmic
issues posed by the added Gaussian noise with unknown Σ in Eq 1.
Prewhitening the data is challenging for noisy ICA because E[xxxxxxT] includes the unknown Gaussian
covariance matrix Σ. [ 4] show that it is enough to quasi orthogonalize the data, i.e., multiply it by a
matrix, which makes the data have a diagonal covariance matrix (not necessarily the identity). Subsequent
work [ 50,49] uses cumulants and the Hessian of f(uuu) to construct a matrix C:=BDBTwhereDis a
diagonal matrix, and then use this matrix to achieve quasi-orthogonalization. In general, Dmay not be
positive semidefinite (e.g. when components of zzzhave kurtosis of different signs). To remedy this, in [ 49],
the authors propose computing the Cmatrix using the Hessian of f(uuu) at some fixed unit vector and then
perform an elegant power method in the pseudo-Euclidean space:
uuut←∇f(C†uuut−1|P)/︁
∥∇f(C†uuut−1|P)∥ (3)
A pseudo-Euclidean space is a generalization of the Euclidean space used by [ 48]. Here the produce between
vectorsuuu,vvvis given asu⊤Av,In this section, we present two new contrast functions based on the logarithm of the characteristic function
(CHF) and the cumulant generating function (CGF). These do not depend on a particular cumulant, and
the first only requires a finite second moment, which makes it suitable for heavy-tailed source signals. We
will usef(uuu|P) orf(uuu|xxx) wherexxx∼Pinterchangeably to represent the population contrast function. In
constructing both of the following contrast functions, we use the fact that, like the cumulants, the CGF and
2Note thatyi:=xi−x⌊n/2⌋+i, i= 1···⌊n/2⌋has a symmetric distribution, and also remains a noisy version of a linear
combination of source signals with the same mixing matrix.
5Algorithm 2 Pseudo-Euclidean Power Iteration for ICA (Algorithm 2 in [ 48])B˜is the recovered matrix for
the ICA model in Eq 1. A˜is a running estimate of B˜†.
Input :C∈Rk×k,∇f
A˜←0,B˜←0
forjin range[1, k]do
Draw uuniformly at random from Sk−1
while Convergence (up to sign) do
u←B˜A˜u
u←∇f(︁
C†u)︁
/⃦⃦∇f(︁
C†u)︁⃦⃦
2end while
B˜j←u,A˜j←[︂
C†B˜j/(︂(︁
C†B˜j)︁⊤B˜j)︂]︂⊤
end for
returnB˜,A˜
CHF-based contrast functions satisfy Assumption 1 (a). To satisfy Assumption 1 (c), we subtract out the
part resulting from the Gaussian noise, which leads to the additional terms involving uuu⊤Suuu.
CHF-based contrast function: Recall that Sdenotes the covariance matrix of xxx. We maximize the
absolute value of following:
f(uuu|P) = log Eexp(iuuuTxxx) + log Eexp(−iuuuTxxx) +uuuTSuuu
= log( Ecos(uuuTxxx))2+ log( Esin(uuuTxxx))2+uuuTSuuu (4)
The intuition is that this is exactly zero for zero mean Gaussian data and maximizing this leads to extracting
non-gaussian signal from the data. It also satisfies Assumption 1 a), b) and c).
CGF-based contrast function: The cumulant generating function also has similar properties (see [ 53]
for the noiseless case). In this case, we maximize the absolute value of following:
f(uuu|P) = log Eexp(uuuTxxx)−1
2uuuTSuuu (5)
Like CHF, this vanishes iff xxxis mean zero Gaussian, and satisfies Assumption 1 a), b) and c). However, it
cannot be expected to behave well in the heavy-tailed case. In the Appendix (Section A.1, Theorem A.1), we
show that the Hessian of both functions obtained from Eq 4 and Eq 5 evaluated at some uuuis, in fact, of the
formBDBTwhereDis some diagonal matrix.
4 Global and local Convergence: loss landscape of noisy ICA
Here we present sufficient conditions for global and local convergence of a broad class of contrast functions.
This covers the cumulant-based contrast functions and our CHF and CGF-based proposals.
4.1 Global convergence
The classical argument for global optima of kurtosis for noiseless ICA in [ 18] assumes WLOG that Bis
orthogonal. This reduces the optimization over uuuinto one for vvv=Buuu. SinceBis orthogonal,∥uuu∥= 1
translates into∥vvv∥= 1. It may seem that this idea should extend to the noisy case due to property 1 a) and
c) by optimizing over vvv=Buuuover potentially non-orthogonal mixing matrices B.
However, for non-orthogonal B, the norm constraint on vvvis no longer vvvTvvv= 1, rendering the original
argument invalid . In what follows, we extend the original argument to the noisy ICA setting by introducing
pseudo-euclidean constraints. Our framework includes a large family of contrast functions, including cumulants.
To our knowledge, this is the first work to characterize the loss functions in the pseudo-euclidean space.
In contrast, [ 50] provides global convergence of the cumulant-based methods by a convergence argument of
the power method itself.
6Consider the contrast function f(uuu|P):=Exxx∼P[︁
g(︁
xxxTuuu)︁]︁
and recall the definition of the quasi orthogonalization
matrixC=BDBT, whereDis a diagonal matrix. For simplicity, WLOG let us assume that Dis invertible.
We now aim to find the optimization objective that leads to the pseudo-Euclidean update in Eq 3. For
f(︁
C−1uuu|P)︁
=Exxx∼P[︁
g(︁
uuuTC−1xxx)︁]︁
, consider the following:
f(︁
C−1uuu|P)︁
=∑︂
i∈[k]E[︁
g(︁(︁
B−1uuu)︁
izi/Dii)︁]︁
=∑︂
i∈[k]E[g(αiz˜i)] =∑︂
i∈[k]f(αi/Dii|zi) (6)
where we define ααα:=B−1uuuandz˜i=zi/Dii. We now examine f(C−1uuu) subject to the “pseudo” norm
constraintuuuTC−1uuu= 1. The key point is that for suitably defined f, one can construct a matrix C=BDBT
from the data even when Bis unknown. So our new objective is to optimize
f(︁
C−1uuu|P)︁
s.t.uuuTC−1uuu= 1
Using the Lagrange multiplier method, optimizing the above simply gives the power method update (Eq 3)
uuu∝∇f(C−1uuu). Furthermore, optimizing Eq 6 leads to the following transformed objective:
max
ααα⃓⃓⃓⃓⃓∑︂
if(αi/Dii|zi)⃓⃓⃓⃓⃓s.t.∑︂
iα2
i/Dii= 1 (7)
Now we provide our theorem about maximizing f(C−1uuu|P). Analogous results hold for minimization. We
will denote the corresponding derivatives evaluated at t0asf′(t0|zi) andf′′(t0|zi).
Theorem 3. LetCbe a matrix of the form BDBT, wheredi:=Dii=f′′(ui|zi)for some random ui. Let
S+={i:di>0}. Consider a contrast function f:R→R. Assume that for every non-Gaussian independent
component, Assumption 1 holds and the third derivative, f′′′(u|X), does not change the sign in the half-lines
[0,∞),(∞,0]. Thenf(︁
C−1uuu⃓⃓xxx)︁
with the constraint of ⟨uuu,uuu⟩C−1= 1has local maxima at B−1uuu=eeei,i∈S+.
All solutions satisfying eeeT
iB−1uuu̸=0,∀i∈[1,k]are minima, and all solutions with eeeT
iB−1uuu̸=0fori∈(1,k)
are saddle points. These are the only fixed points.
Note that the above result immediately implies that for C˜=−C, the same optimization in Theorem 3
will have maxima at all uuusuch thatB−1uuu=eeei,i∈S−.
Corollary 3.1. 2kthcumulant-based contrast functions for k>1, have maxima and minima at the directions
of the columns of B, provided{zzzi}i∈[k]have a corresponding non-zero cumulant.
Proof. This proof (see Appendix Section A.1) is immediate since cumulants satisfy (a), (c). For (c) and (d),
note thatf(u|Z) is simply u2jκ2j(Z), whereκ2j(Z) is the 2jthcumulant of Z.
Theorem 3 can be easily extended to the case where Cis not invertible. The condition on the third
derivative,f′′′(u|X), may seem strong, and it is not hard to construct examples of random variables Zwhere
this fails for suitable contrast functions (see [ 36]). However, for such settings, it is hard to separate Zfrom a
Gaussian. See the Appendix A.4.1 for more details.
4.2 Local convergence
In this section, we establish a local convergence result for the power iteration-based method described in
Eq 3. Define∀t∈R,∀i∈[d], the functions qi(t):=f′(︂
αi
Dii⃓⃓⃓zi)︂
. Then, without loss of generality, we have
the following result for the first corner:
Theorem 4. Letαi=B−1uuuiandααα∗:=eee1. Let the contrast function f(.|X), satisfy assumptions
1 (a), (b), (c), and (d). Let B:= [−∥B−1∥2,∥B−1∥2]and define c1,c2,c3>0such that∀i∈[d],
supx∈B{︂
|qi(x)|
c1,|q′
i(x)|
c2,|q′′
i(x)|
c3}︂
≤1. Defineϵ:=∥B∥F
∥Be1∥2max{︃
c3+c2∥Be1∥
|q1(α∗
1)|,c2
2
|q1(α∗
1)|2,c3∥Be1∥
|q′
1(α∗
1)|}︃
. Let∥ααα0−
ααα∗∥2≤R,R≤max{︂
c2
c3,1
5ϵ}︂
. Then, we have∀t≥1,∥αααt+1−ααα∗∥
∥αααt−ααα∗∥≤1
2.
7Therefore, we can establish linear convergence without the third derivative constraint required for global
convergence (see Theorem 3), by assuming some mild regularity conditions on the contrast functional and a
sufficiently close initialization. The proof is included in Appendix Section A.1.2.
Remark 3. Letααα=B−1uuudenote the demixed direction, uuu. Theorem 4 shows that if the initial uuu0is such that
ααα0is close to the ground truth ααα∗(which is WLOG taken to be eee1), then there is geometric convergence to ααα∗.
|q1(α∗
1)|and|q′
1(α∗
1)|essentially quantify how non-gaussian the corresponding independent component is since
they are zero for a mean zero Gaussian. When these are large quantities, ϵis small, and the initialization
radius∥ααα0−ααα∗∥2can be relatively larger.
5 Experiments
In this section, we provide experiments to compare the fixed-point algorithms based on the characteristic
function (CHF), the cumulant generating function (CGF) (Eqs 4 and 5) with the kurtosis-based algorithm
(PEGI-κ4[49]). We also compare against noiseless ICA3algorithms - FastICA, JADE, and PFICA. These
six algorithms are included as candidates for the Meta algorithm (see Algorithm 1). We also present the
Uncorrected Meta algorithm ( Unc. Meta ) to denote the Meta algorithm with the uncorrected independence
score proposed in CHFICA [ 21]. Table 1 and Figure 2 provide experiments on simulated data, and Figure 3
provides an application for image demixing [ 16]. We provide additional experiments on denoising MNIST
images in the Appendix Section A.2.
Experimental setup: Similar to [ 49], the mixing matrix Bis constructed as B=UΛVT, whereU,V
arekdimensional random orthonormal matrices, and Λ is a diagonal matrix with Λ ii∈[1,3]. The covariance
matrix Σ of the noise gggfollows the Wishart distribution and is chosen to beρ
kRRT, wherekis the number of
sources, and Ris a random Gaussian matrix. Higher values of the noise power ρcan make the noisy ICA
problem harder. Keeping Bfixed, we report the median of 100 random runs of data generated from a given
distribution (different for different experiments). The quasi-orthogonalization matrix for CHF and CGF is
initialized as BˆBˆTusing the mixing matrix, Bˆ, estimated via PFICA. The performance of CHF and CGF is
based on a single random initialization of the vector in the power method (see Algorithm 2). Our experiments
were performed on a Macbook Pro M2 2022 CPU with 8 GB RAM.
Error Metrics: Due to the inherent ambiguity in signal recovery discussed in Section 2, we measure
error using the accuracy of estimating B. We report the widely used Amari error [ 2,24,11,10] for our results.
For estimated demixing matrix ˆ︁B, defineW=ˆ︁B−1B, after normalizing the rows of ˆ︁B−1andB−1. Then we
measuredˆ︁B,Bas -
dˆ︁B,B:=1
k⎛
⎝k∑︂
i=1∑︁k
j=1|Wij|
maxj|Wij|+k∑︂
j=1∑︁k
i=1|Wij|
maxi|Wij|⎞
⎠−2 (8)
Variance reduction using Meta: We first show that the independence score can also be used to pick the
best solution from many random initializations. In Figure 2 (a), the top panel has a histogram of 40 runs of
CHF, each with one random initialization. The bottom panel shows the histogram of 40 experiments, where,
for each experiment, the best out of 30 random initializations are picked using the independence score. The
top and bottom panels have (mean, standard deviation) (0.51, 0.51) and (0.39,0.34) respectively. This shows
a reduction in variance and overall better performance.
Effect of varying kurtosis: For our second experiment (see Table 1), we use k= 5 independent
components, sample size n= 105and noise power ρ= 0.2 from a Bernoulli( p) distribution, where we
varypfrom 0.001 to 0 .5−1/√
12. The last parameter makes kurtosis zero. Different algorithms perform
differently (best candidate algorithm is highlighted in bold font) for different p. In particular, characteristic
function-based methods like PFICA and CHF perform poorly for small values of p. We attribute this to the
fact that the characteristic function is close to one for small p. Kurtosis-based algorithms like PEGI, JADE,
and FASTICA perform poorly for kurtosis close to zero. Furthermore, the Uncorrected Meta algorithm
performs worse than the Meta algorithm since it shadows PFICA.
3MATLAB implementations (under the GNU General Public License) can be found at - FastICA and JADE. The code for
PFICA was provided on request by the authors.
8AlgorithmScaledκ4994 194 95 15 5 2 0.8 0.13 0
Meta 0.007 0.010 0.011 0.010 0.011 0.011 0.0128 0.01981 0.023
CHF 1.524 0.336 0.011 0.010 0.011 0.011 0.0129 0.0213 0.029
CGF 0.007 0.011 0.011 0.016 0.029 0.044 0.05779 0.06521 0.071
PEGI 0.007 0.010 0.011 0.010 0.012 0.017 0.02795 0.13097 1.802
PFICA 1.525 0.885 0.540 0.024 0.023 0.023 0.0212 0.0224 0.024
JADE 0.021 0.022 0.021 0.022 0.022 0.023 0.029 0.089 1.909
FastICA 0.024 0.027 0.026 0.026 0.026 0.027 0.02874 0.0703 -
Unc. Meta 1.52 0.049 0.041 0.0408 0.0413 0.0414 0.0413 0.0416 0.0419
Table 1: Median Amari error with varying pin Bernoulli( p) data. The scaled kurtosis is given as κ4:=
(1−6p(1−p))/(p(1−p)). Observe that the Meta algorithm (shaded in red) performs at par or better than
the best candidate algorithms. FastICA did not converge for zero-kurtosis data.
Effect of varying noise power: For our next two experiments, we use k= 9, out of which 3 are from
Uniform(︁
−√
3,√
3)︁
, 3 are from Exponential(5) and 3 are from(︂
Bernoulli(︂
1
2−√︂
1
12)︂)︂
and hence have zero
kurtosis. In these experiments, we vary the sample size (Figure 2b), nfixingρ= 0.2, and the noise power
(Figure 2a) , ρfixingn= 105, respectively. Note that with most mixture distributions, it is easily possible
to have low or zero kurtosis. We include such signals in our data to highlight some limitations of PEGI- κ4
and show that Algorithm 1 can choose adaptively to obtain better results. Figure 2a shows that large noise
power leads to worse performance for all methods. PEGI, JADE, and FastICA perform poorly consistently,
because of some zero kurtosis components. CGF suffers because of heavy-tailed components. However, CHF
and PFICA perform well consistently. The Meta algorithm mostly picks the best algorithm except for a few
points, where the difference between the two leading algorithms is small. The Uncorrected Meta algorithm
(which uses the independence score without the additional correction term introduced for noise) performs
identically to PFICA.
Effect of varying sample size: Figure 2b shows that the noiseless ICA methods have good performance
at smaller sample sizes. However, they suffer a bias in performance compared to their noiseless counterparts as
the sample size increases. The Meta algorithm can consistently pick the best option amongst the candidates,
irrespective of the distribution, leading to significant improvements in performance. What is interesting is
that up to a sample size of 104, PFICA dominates other algorithms and Meta performs like PFICA. However,
after that CHF dominates, and one can see that Meta starts to have a similar trend as CHF. We also see
that the Uncorrected Meta algorithm (which uses the independence score without the additional correction
term introduced for noise) has a near identical error as PFICA and has a bias for large n.
6 Conclusion
ICA is a classical problem that aims to extract independent non-Gaussian components. The vast literature
on both noiseless and noisy ICA introduces many different inference methods based on a variety of contrast
functions for separating the non-Gaussian signal from the Gaussian noise. Each has its own set of shortcomings.
We aim to identify the best method for a given dataset in a data-driven fashion. In this paper, we propose a
nonparametric score, which is used to evaluate the quality of the solution of any inference method for the noisy
ICA model. Using this score, we design a Meta algorithm, which chooses among a set of candidate solutions
of the demixing matrix. We also provide new contrast functions and a computationally efficient optimization
framework. While they also have shortcomings, we show that our diagnostic can remedy them. We provide
uniform convergence properties of our score and theoretical results for local and global convergence of our
methods. Simulated and real-world experiments show that our Meta-algorithm matches the accuracy of the
best candidate across various distributional settings.
9(a) Variation of performance with noise power
 (b) Variation of performance with sample size
(c) Histograms of Amari error with n= 104and noise power
ρ= 0.2
Figure 2: Amari error in the log-scale onyaxis and varying noise powers (for n= 105) and varying sample sizes (for
ρ= 0.2) onxaxis for figures 2a and 2b respectively. For figure 2c, the top panel contains a histogram of 40 runs with
one random initialization. The bottom panel contains a histogram of 40 runs, each of which is the best independence
score out of 30 random initializations.
Acknowledgments
The authors thank Aiyou Chen for many important discussions that helped shape this paper and for sharing
his code of PFICA. The authors also thank Joe Neeman for sharing his valuable insights and the anonymous
reviewers for their helpful suggestions in improving the exposition of the paper. SK and PS were partially
supported by NSF grants 2217069, 2019844, and DMS 2109155.
10(a) Source Images
(b) Mixed Images
(c) Demixed Images using CHF-based contrast function (∆( ttt,F|P) = 5.26×10−3).
(d) Demixed Images using Kurtosis-based contrast function (∆( ttt,F|P) = 2.48×10−2)
Figure 3: We demix images using ICA by flattening and linearly mixing them with a 4 ×4 matrixB(i.i.d entries
∼N(0,1)) and Wishart noise ( ρ= 0.001). The CHF-based method (c) recovers the original sources well, upto sign.
The Kurtosis-based method (d) fails to recover the second source. This is consistent with its higher independence score.
The Meta algorithm selects CHF from candidates CHF, CGF, Kurtosis, FastICA, and JADE. Appendix Section A.2
provides results for other contrast functions and their independence scores.
References
[1]Shun-Ichi Amari and J.-F. Cardoso. Blind source separation-semiparametric statistical approach. IEEE
Transactions on Signal Processing , 45(11):2692–2700, 1997.
[2]Shun-ichi Amari, Andrzej Cichocki, and Howard Yang. A new learning algorithm for blind signal
separation. Advances in neural information processing systems , 8, 1995.
[3]Joseph Anderson, Navin Goyal, Anupama Nandi, and Luis Rademacher. Heavy-tailed analogues of the
covariance matrix for ica. In Satinder P. Singh and Shaul Markovitch, editors, AAAI , pages 1712–1718.
11AAAI Press, 2017.
[4]Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. ”provable ica with unknown gaussian
noise, with implications for gaussian mixtures and autoencoders”. In Peter L. Bartlett, Fernando
C. N. Pereira, Christopher J. C. Burges, L´ eon Bottou, and Kilian Q. Weinberger, editors, NIPS , pages
2384–2392, 2012.
[5]Arnab Auddy and Ming Yuan. Large dimensional independent component analysis: Statistical optimality
and computational tractability, 2023.
[6]F.R. Bach and M.I. Jordan. Kernel independent component analysis. In 2003 IEEE International
Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP ’03). , volume 4,
pages IV–876, 2003.
[7]Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separation
and blind deconvolution. Neural computation , 7(6):1129–1159, 1995.
[8]Jean-Fran¸ cois Cardoso. High-order contrasts for independent component analysis. Neural computation ,
11(1):157–192, 1999.
[9]J.F. Cardoso and A. Souloumiac. Blind beamforming for non-gaussian signals. IEE Proceedings F (Radar
and Signal Processing) , 140:362–370(8), December 1993.
[10]Aiyou Chen and Peter J. Bickel. Efficient independent component analysis. The Annals of Statistics ,
34(6):2825 – 2855, 2006.
[11]Aiyou Chen and P.J. Bickel. Consistent independent component analysis and prewhitening. IEEE
Transactions on Signal Processing , 53(10):3625–3632, 2005.
[12]Pierre Comon. Independent component analysis, a new concept? Signal processing , 36(3):287–314, 1994.
[13]Pierre Comon and Christian Jutten. Handbook of Blind Source Separation: Independent component
analysis and applications . Academic press, 2010.
[14]George Darmois. Analyse g´ en´ erale des liaisons stochastiques: etude particuli` ere de l’analyse factorielle
lin´ eaire. Revue de l’Institut international de statistique , pages 2–8, 1953.
[15] M. Davies. Identifiability issues in noisy ica. IEEE Signal Processing Letters , 11(5):470–473, 2004.
[16]Laurent de Vito. Ica for demixing images. https://github.com/ldv1/ICA_for_demixing_images ,
2024. Accessed: 2024-05-20.
[17]N. Delfosse and P. Loubaton. Adaptive separation of independent sources: a deflation approach. In
Proceedings of ICASSP ’94. IEEE International Conference on Acoustics, Speech and Signal Processing ,
volume iv, pages IV/41–IV/44 vol.4, 1994.
[18]Nathalie Delfosse and Philippe Loubaton. Adaptive blind separation of convolutive mixtures. In ICASSP ,
pages 2940–2943. IEEE Computer Society, 1996.
[19]Bruno Ebner and Norbert Henze. Bahadur efficiencies of the epps–pulley test for normality. arXiv
preprint arXiv:2106.13962 , 2021.
[20]Bruno Ebner and Norbert Henze. On the eigenvalues associated with the limit null distribution of the
epps-pulley test of normality. Statistical Papers , 64(3):739–752, 2023.
[21]Jan Eriksson and Visa Koivunen. Characteristic-function-based independent component analysis. Signal
Processing , 83(10):2195–2208, 2003.
[22]Arthur Gretton, Kenji Fukumizu, Choon Hui Teo, Le Song, Bernhard Sch¨ olkopf, and Alexander J. Smola.
A kernel statistical test of independence. In Proceedings of the 20th International Conference on Neural
Information Processing Systems , NIPS’07, page 585–592, Red Hook, NY, USA, 2007. Curran Associates
Inc.
12[23]Trevor Hastie and Rob Tibshirani. Independent components analysis through product density estimation.
Advances in neural information processing systems , 15, 2002.
[24]Trevor Hastie and Rob Tibshirani. Independent components analysis through product density estimation.
InProceedings of the 15th International Conference on Neural Information Processing Systems , NIPS’02,
page 665–672, Cambridge, MA, USA, 2002. MIT Press.
[25]Norbert Henze and Thorsten Wagner. A new approach to the bhep tests for multivariate normality.
Journal of Multivariate Analysis , 62(1):1–23, 1997.
[26]A. Hyvarinen. One-unit contrast functions for independent component analysis: a statistical analysis.
InNeural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society
Workshop , pages 388–397, 1997.
[27]Aapo Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE
transactions on Neural Networks , 10(3):626–634, 1999.
[28]Aapo Hyv¨ arinen, J. Karhunen, and Erkki Oja. Independent component analysis . John Wiley & Sons,
2001.
[29]Aapo Hyv¨ arinen and Erkki Oja. A fast fixed-point algorithm for independent component analysis. Neural
Computation , 9(7):1483–1492, 1997.
[30]Pauliina Ilmonen and Davy Paindaveine. Semiparametrically efficient inference based on signed ranks in
symmetric independent component models. Annals of Statistics , 39:2448–2476, 2011.
[31]Pauliina Ilmonen and Davy Paindaveine. Semiparametrically efficient inference based on signed ranks in
symmetric independent component models. 2011.
[32]Elena Issoglio, Paul Smith, and Jochen Voss. On the estimation of entropy in the fastica algorithm.
Journal of Multivariate Analysis , 181:104689, 2021.
[33] M. Kac. Sur les fonctions ind´ ependantes (i) (propri´ et´ es g´ en´ erales). Studia Mathematica , 6:46–58, 1936.
[34]Sergey Kirshner and Barnab´ as P´ oczos. Ica and isa using schweizer-wolff measure of dependence. In
Proceedings of the 25th International Conference on Machine Learning , ICML ’08, page 464–471, New
York, NY, USA, 2008. Association for Computing Machinery.
[35]Te-Won Lee, Mark Girolami, and Terrence J Sejnowski. Independent component analysis using an
extended infomax algorithm for mixed subgaussian and supergaussian sources. Neural computation ,
11(2):417–441, 1999.
[36]Peter McCullagh. Does the moment-generating function characterize a distribution? The American
Statistician , 48(3):208–208, 1994.
[37]Erkki Oja and A Hyvarinen. Independent component analysis: algorithms and applications. Neural
networks , 13(4-5):411–430, 2000.
[38]Erkki Oja, Aapo Hyv¨ arinen, and Patrik Hoyer. Image feature extraction and denoising by sparse coding.
Pattern Analysis & Applications , 2:104–110, 1999.
[39]Dinh Tuan Pham. Blind separation of instantaneous mixture of sources via an independent component
analysis. IEEE Transactions on Signal Processing , 44(11):2768–2779, 1996.
[40]Dinh Tuan Pham and P. Garat. Blind separation of mixture of independent sources through a quasi-
maximum likelihood approach. IEEE Transactions on Signal Processing , 45(7):1712–1725, 1997.
[41]Karl Rohe and Muzhe Zeng. Vintage factor analysis with varimax performs statistical inference. arXiv
preprint arXiv:2004.05387 , 2020.
13[42]Hao Shen, Stefanie Jegelka, and Arthur Gretton. Fast kernel-based independent component analysis.
IEEE Transactions on Signal Processing , 57(9):3498–3511, 2009.
[43] Viktor P Skitovitch. On a property of the normal distribution. DAN SSSR , 89:217–219, 1953.
[44]Fabian J. Theis. Multidimensional independent component analysis using characteristic functions. In
2005 13th European Signal Processing Conference , pages 1–4, 2005.
[45] Aad W. Van der Vaart. Asymptotic statistics , volume 3. Cambridge university press, 2000.
[46] Roman Vershynin. High-Dimensional Probability . Cambridge University Press, Cambridge, UK, 2018.
[47]Roman Vershynin. High-dimensional probability: An introduction with applications in data science ,
volume 47. Cambridge university press, 2018.
[48]James R. Voss, Mikhail Belkin, and Luis Rademacher. Optimal recovery in noisy ICA. CoRR ,
abs/1502.04148, 2015.
[49]James R Voss, Mikhail Belkin, and Luis Rademacher. A pseudo-euclidean iteration for optimal recovery
in noisy ica. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in
Neural Information Processing Systems , volume 28. Curran Associates, Inc., 2015.
[50]James R Voss, Luis Rademacher, and Mikhail Belkin. Fast algorithms for gaussian noise invariant
independent component analysis. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q.
Weinberger, editors, Advances in Neural Information Processing Systems , volume 26. Curran Associates,
Inc., 2013.
[51]Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48. Cambridge
university press, 2019.
[52]Tianwen Wei. A study of the fixed points and spurious solutions of the deflation-based fastica algorithm.
Neural Comput. Appl. , 28(1):13–24, jan 2017.
[53]Arie Yeredor. Blind source separation via the second characteristic function. Signal Processing , 80(5):897–
902, 2000.
14A Appendix
The Appendix is organized as follows -
•Section A.1 proves Theorems 1, 2, A.1, 3 and 4
•Section A.2 provides additional experiments for noisy ICA
•Section A.3 provides the algorithm to compute independence scores via a sequential procedure
•Section A.4 explores the third derivative constraint in Theorem 3 and provides examples where it holds
•Section A.5 contains surface plots of the loss landscape for noisy ICA using the CHF-based contrast
functions
A.1 Proofs
Proof of Theorem 1. We will give an intuitive argument for the easy direction for k= 2. We will show
that ifF=DB−1for some permutation of a diagonal matrix D, then after projecting using that matrix,
the data is of the form zzz+ggg′for some Gaussian vector ggg′. Letk= 2. Let the entries of this vector be
z1+g′
1andz2+g′
2, wherezi,i∈{1,2}are mean zero independent non-Gaussian random variables and g′
i,
i∈{1,2}are mean zero possibly dependent Gaussian variables. The Gaussian variables are independent of
the non-Gaussian random variables. Let ti,i∈{1,2}be arbitrary but fixed real numbers. Let ttt= (t1,t2)
and let Σ g′denote the covariance matrix of the Gaussian g′. Assume for simplicity var(zi) = 1 fori∈{1,2}.
Denote by Λ := cov( Fxxx) =FSFT=I+ Σg′. The Joint part of the score is given by:
Joint =E[it1z1]E[it2z2] exp(︃
−tttT(Σg′+ diag(Λ))ttt
2)︃
TheProduct part of the score is given by
Product =E[it1z1]E[it2z2] exp(︃
−tttT(diag(Σg′) + Λ)ttt
2)︃
It is not hard to see that Joint equals Product . The same argument generalizes to k>2. We next provide
proof for the harder direction here. Suppose that ∆ F(ttt|P) = 0, i.e,
E[︁
exp(itttTFxxx)]︁
exp(︄
−tttTdiag(︁
FSFT)︁
ttt
2)︄
−k∏︂
j=1E[exp(itj(Fxxx)j)] exp(︃
−tttTFSFTttt
2)︃
= 0 (A.9)
We then prove that Fmust be of the form DB−1forDbeing a permutation of a diagonal matrix. Then,
taking the logarithm, we have
ln(︁
E[︁
exp(itttTFxxx)]︁)︁
−k∑︂
j=1ln (E[exp(itj(Fxxx)j)]) =1
2tttT(︁
diag(︁
FSFT)︁
−FSFT)︁
ttt
=1
2tttT(︁
diag(︁
FΣFT)︁
−FΣFT)︁
ttt
Under the ICA model we have, xxx=Bzzz+ggg. Therefore, from Eq A.9, using the definition of the characteristic
function of a Gaussian random variable and noting that cov ( ggg) = Σ, we have
ln(︁
E[︁
exp(itttTFxxx)]︁)︁
= ln(︁
E[︁
exp(itttTFBzzz)]︁)︁
+ ln(︁
E[︁
exp(itttTFggg)]︁)︁
= ln(︁
E[︁
exp(itttTFBzzz)]︁)︁
−1
2tttTFΣFTttt
15and similarly,
k∑︂
j=1ln (E[exp(itj(Fxxx)j)]) =k∑︂
j=1ln (E[exp(itj(FBzzz)j)])−1
2tttTdiag(︁
FΣFT)︁
ttt
Therefore, from Eq A.9,
gF(ttt) := ln(︁
E[︁
exp(itttTFBzzz)]︁)︁
−k∑︂
j=1ln (E[exp(itj(FBzzz)j)])
must be a quadratic function of ttt. It is important to note that the second term is an additive function w.r.t
t1,t2,···tk. For simplicity, consider the case of k= 2 and assume that the joint and marginal characteristic
functions of all signals are twice differentiable. Consider∂2(gF(ttt))
∂t1∂t2, then
2∑︂
k=1∂2(gF(ttt))
∂t1∂t2≡const. (A.10)
To simplify the notation, let ψj(t) :=E[︁
eitzj]︁
,fj≡logψj,M:=FB. The above is then equivalent to
2∑︂
k=1M1kM2kf′′
k(M1kt1+M2kt2)≡const. (A.11)
Note thatMis invertible since F,B are invertible. Therefore, let s≡MTt, then
2∑︂
k=1M1kM2kf′′
k(sk)≡const.
which implies that either M1kM2k= 0 orf′′
k(sk)≡const , fork= 1,2. For any k∈{1,2}, sincezkis
non-Gaussian, its logarithmic characteristic function, i.e. fkcannot be a quadratic function, so
M1kM2k= 0
which implies that each column of Mhas a zero. Since Mis invertible, thus Mis either a diagonal matrix or
a permutation of a diagonal matrix. Now to consider k>2, we just need to apply the above k= 2 argument
to pairwise entries of {t1,···,tk}with other entries fixed. Hence proved.
Proof of Theorem 3. We start with considering the following constrained optimization problem -
sup
uuuTC−1uuu=1f(︁
C−1uuu|P)︁
By an application of lagrange multipliers, for the optima uuuopt, we have
νC−1uuuopt=C−1∇f(︁
(C−1)Tuuuopt|P)︁
, uuuT
optC−1uuuopt= 1
with multiplier ν∈R. This leads to a fixed-point iteration very similar to the one in [ 49], which is the
motivation for considering such an optimization framework. We now introduce some notations to simplify
the problem.
Letuuu=Bααα,z′
i:=zi
dianda′
i:= var(z′
i) =ai
d2
i, whereai:= var(zi). Then,
sup
uuuTC−1uuu=1f(︁
C−1uuu|P)︁
= sup
αααTD−1ααα=1f(︁
D−1ααα|zzz)︁
We will use f(uuu|P) orf(uuu|xxx) wherexxx∼Pinterchangeably. By Assumption 1-(a), we see that f(︁
D−1ααα|zzz)︁
=
∑︁k
i=1f(︂
αi
di|zi)︂
. For simplicity of notation, we will use hi(αi/di) =f(αi/di|zi), since the functional form
16can be different for different random variables zi. Therefore, we seek to characterize the fixed points of the
following optimization problem -
sup
αααk∑︂
i=1hi(αi/di) (A.12)
s.tk∑︂
i=1α2
i
di= 1,
di̸= 0∀i∈[k]
whereααα,d∈Rk. We have essentially moved from optimizing in the ⟨.,.⟩C−1space to the⟨.,.⟩D−1space. We
find stationary points of the Lagrangian
L(ααα,λ) :=k∑︂
i=1hi(︃αi
di)︃
−λ(︄k∑︂
i=1α2
i
di−1)︄
(A.13)
The components of the gradient of L(ααα,λ) w.r.tαααare given as
∂
∂αjL(ααα,λ) =1
djh′
j(︃αj
dj)︃
−λαj
dj,
At the fixed point ( ααα,λ) we have,
∀j∈[k],h′
j(︃αj
dj)︃
−λαj= 0
By Assumption 1-(b), for αi= 0, the above is automatically zero. However, for {j:αj̸= 0}, we have,
λ=h′
j(︂
αj
dj)︂
αj(A.14)
So, for all{j:αj̸=0}, we must have the same value and sign of1
αjh′
j(︂
αj
dj)︂
. By assumption in the theorem
statement,h′′′
i(x) does not change sign on the half-lines x>0 andx<0. Along with Assumption 1-(b), this
implies that
∀x∈[0,∞),sgn(hi(x)) = sgn(h′
i(x)) = sgn(h′′
i(x)) = sgn(h′′′
i(x)) =κ1 (A.15)
∀x∈(−∞,0],sgn(hi(x)) =−sgn(h′
i(x)) = sgn(h′′
i(x)) =−sgn(h′′′
i(x)) =κ2 (A.16)
whereκ1,κ2∈{− 1,1}are constants. Furthermore, we note that Assumption 1-(d) ensures that hi(x) is a
symmetric function. Therefore, ∀x∈R,sgn(hi(x)) = sgn(h′′
i(x)) =κ,κ∈{− 1,1}. Then, since di=h′′
i(ui),
sgn (di) = sgn (hi(x)),∀x∈R (A.17)
Now, using A.14,
sgn(λ) = sgn(︃
h′
j(︃αj
dj)︃)︃
×sgn(αj)
= sgn(︃αj
dj)︃
×sgn(︃
hj(︃αj
dj)︃)︃
×sgn(αj),using EqA.15 andA.16
= sgn (dj)×sgn(︃
hj(︃αj
dj)︃)︃
= 1,using EqA.17 (A.18)
17Keeping this in mind, we now compute the Hessian, H∈Rk×k, of the lagrangian, L(ααα,λ) at the fixed point,
(ααα,λ). Recall that, we have h′
i(0) = 0 and h′′
i(0) = 0.Thus, for{i:αi= 0},
Hii=−λ/di (A.19)
This implies that sgn ( diHii) = sgn (λ) =−1 for{i:αi= 0}, using Eq A.18.
For{i:αi̸= 0}, we have
Hij=∂2
∂αi∂αjL(ααα,λ)⃓⃓⃓⃓
ααα= 1(i=j)⎡
⎣h′′
i(︂
αi
di)︂
d2
i−λ
di⎤
⎦
= 1(i=j)⎡
⎣h′′
i(︂
αi
di)︂
d2
i−h′
i(︂
αi
di)︂
αidi⎤
⎦, forαi̸= 0, usingA.14
= 1(i=j)1
diαi[︃αi
dih′′
i(︃αi
di)︃
−h′
i(︃αi
di)︃]︃
, forαi̸= 0
We consider the pseudo inner product space ⟨.,.⟩D−1for optimizing ααα. Furthermore, since we are in this
pseudo-inner product space, we have
⟨vvv,Hvvv⟩D−1=vvvD−1Hvvv
So we will consider the positive definite-ness of the matrix H˜:=D−1Hto characterize the fixed points.
Recall that for a differentiable convex function f, we have∀x,y∈dom(f)⊆Rn
f(y)≥f(x) +∇f(x)T(y−x)
Therefore, for{i:αi̸= 0}, we have
sgn (diHii) = sgn (di)×sgn (diαi)×sgn(︃αi
dih′′
i(︃αi
di)︃
−h′
i(︃αi
di)︃)︃
= sgn (αi)×sgn(︃
h′′′
i(︃αi
di)︃)︃
,using convexity/concavity of h′(.)
= sgn (αi)×sgn(︃
h′
i(︃αi
di)︃)︃
,usingA.15 andA.16
= sgn (αi)×sgn (λ)×sgn (αi),usingA.14
= sgn (λ)
= 1,using EqA.17 (A.20)
LetS:={i:αi̸= 0}. We then have the following cases -
1.Assume that only one αiis nonzero, then∀vvvorthogonal to this direction, ⟨vvv,Hvvv⟩D−1<0 by Eq A.19.
Thus this gives a local maxima.
2.Assume more than one αiare nonzero, but |S|<k. Then we have for i̸∈S,H˜jj<0 using A.19. For
i∈S,H˜ii>0 from Eq A.20. Hence these are saddle points.
3. Assume we have S= [k], i.e.∀i,αi̸= 0. In this case, H˜≻0. So, we have a local minima.
This completes our proof.
Theorem A.1. Consider the data generated from the noisy ICA model (Eq 1). If f(uuu|P)is defined as in
Eq 4 or Eq 5, we have ∇2f(uuu|P) =BDuBT, for some diagonal matrix Du, which can be different for the
differerent contrast functions.
18Remark 4. The above theorem is useful because it shows that the Hessian of the contrast functions based
on the CHF (eq 4 or CGF (eq 5) is of the form BDBT, whereDis some diagonal matrix. This matrix
can be precomputed at some vector uuuand used as the matrix Cin the power iteration update (see eq 3) in
Algorithm 2 of [49].
Proof of Theorem A.1. First, we show that the CHF-based contrast function (see Eq 4), the Hessian, is of
the correct form.
CHF-based contrast function
f(uuu|P) = log Eexp(iuuuTBzzz) + log Eexp(−iuuuTBzzz) +uuuTBdiag(aaa)BTuuu,
where diag( aaa) is the covariance matrix of zzz. For simplicity of notation, define ϕ(zzz;uuu) :=E[︁
exp(iuuuTBzzz)]︁
.
∇f(uuu|P) =iB⎛
⎜⎜⎜⎝E[︁
exp(iuuuTBzzz)zzz]︁
ϕ(zzz;uuu)⏞⏟⏟⏞
µ(uuu;zzz)−iE[︁
exp(−iuuuTzzz)zzz]︁
ϕ(zzz;−uuu)⎞
⎟⎟⎟⎠+ 2Bdiag(aaa)BTuuu
Define
H(uuu;zzz) :=E[︁
exp(iuuuTBzzz)zzzzzzT]︁
ϕ(zzz;uuu)−µ(uuu;zzz)µ(uuu;zzz)T.
Taking a second derivative, we have:
∇2f(uuu|P) =B(−H(uuu;zzz)−H(zzz;−uuu) + 2 diag(aaa))BT
All we have to show at this point is that H(uuu;zzz) is diagonal. We will evaluate the k,ℓentry. LetBidenote
theithcolumn ofB. LetY\k,ℓ:=uuuTB∑︁
j̸=k,ℓzj. Using independence of the components of zzz, we have, for
k̸=ℓ,
E[︁
exp(iuuuTBkzk+iuuuTBℓzℓ+Y\kℓ)zkzℓ]︁
ϕ(zzz;uuu)=E[︁
exp(iuuuTBkzk)zk]︁
E[︁
exp(iuuuTBℓzℓ)zℓ]︁
E[exp(iuuuTBkzk)]E[exp(iuuuTBℓzℓ)](A.21)
Now we evaluate:
eeeT
kµ(uuu;zzz) :=E[︁
exp(iuuuTBzzz)zk]︁
ϕ(zzz;uuu)=E[︁
exp(iuuuTBkzk)zk]︁
E[exp(iuuuTBkzk)](A.22)
Using Eqs A.21 and A.22, we see that indeed H(uuu;zzz) is diagonal.
Now, we prove the statement about the CGF-based contrast function.
CGF-based contrast function
We have,
∇f(uuu|P) =E[︁
exp(︁
uuuTBzzz)︁
Bzzz]︁
E[exp (uuuTBzzz)]−Bdiag (aaa)BTuuu,
∇2f(uuu|P) =E[︁
exp(︁
uuuTBzzz)︁
BzzzzzzTBT]︁
E[exp (uuuTBzzz)]−E[︁
exp(︁
uuuTBzzz)︁
Bzzz]︁
E[︁
exp(︁
uuuTBzzz)︁
zzzTBT]︁
E[exp (uuuTBzzz)]2−Bdiag (aaa)BT
=B⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣E[︁
exp(︁
uuuTBzzz)︁
zzzzzzT]︁
E[exp (uuuTBzzz)]−E[︁
exp(︁
uuuTBzzz)︁
zzz]︁
E[︁
exp(︁
uuuTBzzz)︁
zzzT]︁
E[exp (uuuTBzzz)]2
⏞ ⏟⏟ ⏞
Covariance of zzz∼P(zzz).exp(︁
uuuTBzzz)︁
E[exp (uuuTBzzz)]−diag (aaa)⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦BT
The new probability density P(zzz).exp(uuuTBzzz)
E[exp(uuuTBzzz)]is an exponential tilt of the original pdf, and since {zi}d
i=1
are independent, and the new tilted density also factorizes over the zi’s, therefore, the covariance under this
tilted density is also diagonal.
19A.1.1 Uniform convergence
In this section, we provide the proof for Theorem 2. First, we state some preliminary results about the
uniform convergence of smooth function classes which will be useful for our proofs.
A.1.1.1 Preliminaries
LetD:=supθ,θ˜∈TρX(︁
θ,θ˜)︁
denote the diameter of set T, and letNX(δ;T)denote the δ-covering number of
Tin theρXmetric. Then, we have the following standard result:
Proposition A.2. (Proposition 5.17 from [ 51]) Let{Xθ,θ∈T}be a zero-mean subgaussian process with
respect to the metric ρX. Then for any δ∈[0,D]such thatNX(δ;T)≥10, we have
E[︄
sup
θ,θ˜∈T(Xθ−Xθ˜)]︄
≤2E[︄
sup
γ,γ′∈T;ρX(γ,γ′)≤δ(Xθ−Xθ˜)]︄
+ 4√︁
D2log (NX(δ;T))
Remark 5. For zero-mean subgaussian processes, Proposition A.2 implies, E[supθ∈TXθ]≤E[︂
supθ,θ˜∈T(Xθ−Xθ˜)]︂
Proposition A.3. (Theorem 4.10 from [ 51]) For anyb-uniformly bounded class of functions C, any positive
integern≥1, any scalar δ≥0, and a set of i.i.d datapoints{︁
X(i)}︁
i∈[n]we have
sup
f∈C⃓⃓⃓⃓⃓1
nn∑︂
i=1f(︂
X(i))︂
−E[f(X)]⃓⃓⃓⃓⃓≤21
nEX,ϵ[︄
sup
f∈C⃓⃓⃓⃓⃓n∑︂
i=1ϵif(︂
X(i))︂⃓⃓⃓⃓⃓]︄
+δ
with probability at least 1−exp(︂
−nδ2
2b2)︂
. Here{ϵi}i∈[n]are i.i.d rademacher random variables.
A.1.1.2 Proof of theorem 2
For dataset{︁
xxx(i)}︁
i∈[n],xxx(i)∈Rk, consider the following definitions:
ϕ(ttt,F|P) :=Exxx[︁
exp(itttTFxxx)]︁
, ϕ (ttt,F|Pˆ) :=1
nn∑︂
j=1exp(itttTFxxx(j)) (A.23)
ψ(ttt,F|P) :=k∏︂
j=1Exxx[exp(itj(Fxxx)j)], ψ (ttt,F|Pˆ) :=1
nk∏︂
j=1n∑︂
r=1exp(itj(Fxxx(r))j) (A.24)
Let ∆(ttt,F|Pˆ) be the empirical version where the expectation is replaced by sample averages. Now define:
∆(ttt,F|P) =⃓⃓ϕ(ttt,F|P) exp(︁
−tttTdiag(FSFT)ttt)︁
−ψ(ttt,F|P) exp(︁
−tttTFSFTttt)︁⃓⃓
∆(ttt,F|Pˆ) =⃓⃓⃓ϕ(ttt,F|Pˆ) exp(︂
−tttTdiag(Fˆ︁SFT)ttt)︂
−ψ(ttt,F|Pˆ) exp(︂
−tttTFˆ︁SFTttt)︂⃓⃓⃓
Theorem A.4. LetF:={F∈Rk×k:∥F∥≤1}. Assume that xxx∼subgaussian (σ). We have:
sup
F∈F|Ettt∈N(0,Ik)∆(ttt,F|P)−Ettt∈N(0,Ik)∆(ttt,F|Pˆ)|=OP⎛
⎝√︄
k2∥S∥max(k,σ4∥S∥) log2(nCk)
n⎞
⎠
whereCk:= max(1,klog (n) Tr(S)).
20Proof. DefineE={ttt:∥ttt∥≤√2klogn}. We will use the fact that P(Ec)≤2/n(Example 2.12, [ 51]). Also
note that by construction, ∆( ttt,F|P)≤1. Observe that:
sup
F∈F|Ettt∈N(0,Ik)∆(ttt,F|P)−Ettt∈N(0,Ik)∆(ttt,F|Pˆ)|
≤sup
F∈FEttt∈N(0,Ik)|∆(ttt,F|P)−∆(ttt,F|Pˆ)|
≤Ettt∈N(0,Ik)sup
F∈F|∆(ttt,F|P)−∆(ttt,F|Pˆ)|
≤Ettt∈N(0,Ik)[︃
sup
F∈F⃓⃓⃓∆(ttt,F|P)−∆(ttt,F|Pˆ)⃓⃓⃓⃓⃓⃓⃓E]︃
+Ettt∈N(0,Ik)[︃
sup
F∈F|∆(ttt,F|P)−∆(ttt,F|Pˆ)⃓⃓⃓⃓|Ec]︃
P(Ec)
≤Ettt∈N(0,Ik)[︃
sup
F∈F⃓⃓⃓∆(ttt,F|P)−∆(ttt,F|Pˆ)⃓⃓⃓⃓⃓⃓⃓E]︃
+ 2/n
=OP⎛
⎝√︄
k2∥S∥max(k,σ4∥S∥) log2(nCk)
n⎞
⎠+ 2/n,using Theorem A.5
The second inequality follows from Jensen’s inequality, the fourth follows from E[sup(.)]≤sup(E[.]).
Theorem A.5. LetF:={F∈Rk×k:∥F∥≤1}. Assume that xxx∼subgaussian (σ). We have:
sup
F∈F|∆(ttt,F|P)−∆(ttt,F|Pˆ)|=OP(︄
∥ttt∥√︃
k∥S∥max(k,σ4∥S∥) log(nCt)
n)︄
whereCt:= max(1,∥ttt∥2Tr(S)).
Proof.
∆(ttt,F|P)−∆(ttt,F|Pˆ)≤⃓⃓⃓ϕ(ttt,F|P)−ϕ(ttt,F|Pˆ)⃓⃓⃓exp(︁
−tttTdiag(FSFT)ttt)︁
+ϕ(ttt,F|Pˆ)⃓⃓⃓exp(︂
−tttTdiag(Fˆ︁SFT)ttt)︂
−exp(︁
−tttTdiag(FSFT)ttt)︁⃓⃓⃓
+⃓⃓⃓ψ(ttt,F|P)−ψ(ttt,F|Pˆ)⃓⃓⃓exp(︁
−tttTFSFTttt)︁
+ψ(ttt,F|Pˆ)⃓⃓⃓exp(︂
−tttTFˆ︁SFTttt)︂
−exp(︁
−tttTFSFTttt)︁⃓⃓⃓
Finally, for some S′=λS+ (1−λ)Sˆ,
|exp(−tttTFSFTttt)−exp(−tttTFSˆFTttt)|=⃓⃓⃓⟨︂
∂Sexp(−tttTFSFTttt)⃓⃓
S′,Sˆ−S⟩︂⃓⃓⃓
≤exp(−tttTFS′FTttt)⃓⃓⃓tttTF(S−Sˆ)FTttt⃓⃓⃓
≤∥ttt∥2∥S−Sˆ∥=∥ttt∥2OP(︄
σ2∥S∥√︃
k
n)︄
where the last result follows from Theorem 4.7.1 in [47]. Now, for the second term, observe that:
⃓⃓⃓ϕ(ttt,F|Pˆ)(︂
exp(︂
−tttTdiag(Fˆ︁SFT)ttt)︂
−exp(︁
−tttTdiag(FSFT)ttt)︁)︂⃓⃓⃓
≤exp(︁
−tttTdiag(FSFT)ttt)︁⃓⃓⃓exp(︂
−tttTdiag(F(ˆ︁S−S)FT)ttt)︂
−1⃓⃓⃓ (A.25)
We next have that, with probability at least 1 −1/n,
⃦⃦⃦diag(︂
F(S−ˆ︁S)FT)︂⃦⃦⃦
2≤⃦⃦⃦F(S−ˆ︁S)FT⃦⃦⃦
2≤C(︄
σ2∥S∥√︃
klogn
n)︄
21Thus with probability at least 1 −1/n, using the inequality |1−ex|≤2|x|,∀x∈[−1,1], Eq A.25 leads to:
⃓⃓⃓ϕ(ttt,F|Pˆ)(︂
exp(︂
−tttTdiag(Fˆ︁SFT)ttt)︂
−exp(︁
−tttTdiag(FSFT)ttt)︁)︂⃓⃓⃓≤2C∥ttt∥2(︄
σ2∥S∥√︃
klogn
n)︄
Note that the matrices diag(︂
F(S−ˆ︁S)FT)︂
andFSFTare positive semi-definite and tttis unit-norm.
Observe that, using Lemmas A.6 and A.7, we have:
sup
F∈F|∆(ttt,F|P)−∆(ttt,F|Pˆ)|
≤OP(︄
∥ttt∥√︃
kTr(S) log (nCt)
n)︄
+OP(︄
∥ttt∥σ2∥S∥√︃
klogn
n)︄
+OP(︄
∥ttt∥√︃
k2∥S∥logn
n)︄
=OP(︄
∥ttt∥√︃
k∥S∥max(k,σ4∥S∥) log(nCt)
n)︄
Lemma A.6. DefineF:={F∈Rk×k:∥F∥≤1}. Let{︁
xxx(i)}︁
i∈[n]be i.i.d samples from a subgaussian (σ)
distribution. We have:
sup
F∈F⃓⃓⃓⃓⃓⃓1
nn∑︂
j=1exp(itttTFxxx(j))−Exxx[︁
exp(itttTFxxx)]︁⃓⃓⃓⃓⃓⃓=OP(︄
∥ttt∥√︃
kTr(S) log (nCt)
n)︄
whereCt:= max(1,∥ttt∥2Tr(S)).
Proof. Letϕ(ttt,F;xxx(i)) = exp(itttTFxxx(i)). Next, we note that ∥F∥2≤1 imply⃦⃦FTttt⃦⃦≤∥ttt∥. Therefore,
sup
F∈F⃓⃓⃓⃓⃓⃓1
nn∑︂
j=1exp(itttTFxxx(j))−Exxx[︁
exp(itttTFxxx)]︁⃓⃓⃓⃓⃓⃓≤ sup
uuu∈Rk,∥uuu∥≤1⃓⃓⃓⃓⃓⃓1
nn∑︂
j=1exp(iuuuTxxx(j))−Exxx[︁
exp(iuuuTxxx)]︁⃓⃓⃓⃓⃓⃓
LetfX(uuu) =∑︁
iϵiξ(uuu;xxx(i)). We will argue for unit vectors uuu, and later scale them by ∥ttt∥. DefineBk:={︁
uuu∈Rk,∥uuu∥≤1}︁
,ξ(uuu;xxx) := exp(iuuuTxxx) and let
d(uuu,uuu′)2:=∑︂
i(ξ(uuu;xxx(i))−ξ(uuu′;xxx(i)))2
Then we have,
∥∇uuuexp(iuuuTxxx)∥2=∥xxx∥|−sin(uuuTxxx) +icos(uuuTxxx)|≤ ∥xxx∥
Then,
⃓⃓⃓ξ(uuu;xxx(i))−ξ(ttt;xxx(i))⃓⃓⃓≤min(︂
∥∇uuu′′ξ(uuu;xxx(i))∥2∥uuu−uuu′∥,2)︂
≤min(︂
∥xxx(i)∥2∥uuu−uuu′∥,2)︂
Letˆ︁S:=∑︁
ixxx(i)(︁
xxx(i))︁T/nandτn:=nTr(︂
ˆ︁S)︂
. We next have,
D2:= max
uuu,uuu′d(uuu,uuu′)2≤min{︄
4n,max
uuu,uuu′∑︂
i∥xxx(i)∥2
2∥uuu−uuu′∥2}︄
≤4 min{n,τn}
22Next, we bound the covering number N(δ;Bk,duuu). Note that N(δ;Bk,duuu)≤N(δ/√τn;Bk,∥.∥2) since,
d(uuu,uuu′)2=∑︂
i(ξ(uuu;xxx(i))−ξ(uuu′;xxx(i)))2
≤∑︂
i∥xxx(i)∥2
2∥uuu−uuu′∥2
=∑︂
iTr(︃
xxx(i)(︂
xxx(i))︂T)︃
∥uuu−uuu′∥2
Using Cauchy-Schwarz, we have:
|fX(uuu)−fX(uuu′)|≤n∑︂
i=1ϵi|ξ(uuu;xxx(i))−ξ(uuu;xxx(i))|≤√︁
nd(uuu,uuu′)2
Therefore, we have N(δ;Bk,duuu)≤(︂
3√τn
δ)︂k
. Therefore, using Proposition A.2,
Exxx,ϵ[︃
sup
F∈F|fX(uuu)−fX(uuu′)|]︃
≤2Exxx[︄
sup
d(uuu,uuu′)≤δ|fX(uuu)−fX(uuu′)|]︄
+ 4Exxx[︂
D√︁
logN(δ;Bk,duuu)]︂
≤2Exxx[︄
inf
δ>0{︄
δ√n+ 8√︁
min{n,τn}√︄
2klog(︃3√τn
δ)︃}︄]︄
=O(︂
Exxx√︁
(kmin(n,τn) log (max(τn,n)))︂
,settingδ:=√︁
min(1,τn/n)
=√
knO(︂√︁
min(1,Tr(S)) log (nmax(1,Tr(S))))︂
,Cauchy-Schwarz inequality
Now we recall that ∥FTttt∥≤∥ttt∥, since all vectors tttappear astttTFTxxx(i), this simply leads to scaling τand
∥S∥by∥ttt∥2. Dividing both sides by n, we have
1
nExxx,ϵ[︃
sup
F∈F|fX(ttt)−fX(ttt′)|]︃
≤√︃
k
nO(︂√︁
min(1,∥ttt∥2Tr(S)) log (nmax(1,∥ttt∥2Tr(S))))︂
Thus, using Proposition A.3 and using the definition of Ct, we have,
sup
F∈F|ϕ(ttt,F|Pˆ)−ϕ(ttt,F|P)|=√︃
k
nO(︂√︁
∥ttt∥2Tr(S) log (nCt))︂
=∥ttt∥√︃
k
nO(︂√︁
Tr(S) log (nCt))︂
Lemma A.7. LetF={F∈Rk×k:∥F∥≤1}. We have:
sup
F∈F⃓⃓⃓ψ(ttt,F|Pˆ)−ψ(ttt,F|P)⃓⃓⃓≤OP(︄
∥ttt∥√︃
k2∥S∥logn
n)︄
Proof. LetBk:={︁
uuu∈Rk,∥uuu∥≤1}︁
Now define ψj(ttt,F;xxx) =E[exp(itjFT
jxxx)]. Also define f(j)
X(F) =
1
n∑︁
iϵiψj(ttt,F;xxx(i)). Thus, using the same argument as in Lemma A.6 for ttt←∥ttt∥eeejandxxx(i)←tjxxx(i),
sup
F∈F|ψj(ttt,F|Pˆ)−ψj(ttt,F|P)|=OP(︄
|tj|√︃
k∥S∥logn
n)︄
Finally, we see that:
sup
F∈F⃓⃓⃓ψ(ttt,F|Pˆ)−ψ(ttt,F|P)⃓⃓⃓≤∑︂
jOP(︄
|tj|√︃
k∥S∥logn
n)︄
=OP(︄√︃
k∥S∥logn
n)︄⎛
⎝k∑︂
j=1|tj|⎞
⎠ =∥ttt∥OP(︄√︃
k2∥S∥logn
n)︄
23The above is true because, for |ai|,|bi|≤1,i= 1,...,k ,
⃓⃓⃓⃓⃓k∏︂
i=1ai−k∏︂
i=1bi⃓⃓⃓⃓⃓=⃓⃓⃓⃓⃓⃓k−1∑︂
j=0∏︂
i≤jbi(aj+1−bj+1)k∏︂
i=j+2ai⃓⃓⃓⃓⃓⃓≤k−1∑︂
j=0|aj+1−bj+1|
A.1.2 Local convergence
In this section, we provide the proof of Theorem 4. Recall the ICA model from Eq 1,
xxx=Bzzz+ggg,
diag (aaa) := cov (zzz),Σ := cov (ggg) =Bdiag (aaa)BT+ Σ
We provide the proof for the CGF-based contrast function. The proof for the CHF-based contrast function
follows similarly. From the Proof of Theorem A.1 we have,
∇f(uuu|P) =E[︁
exp(︁
uuuTBzzz)︁
Bzzz]︁
E[exp (uuuTBzzz)]−Bdiag (aaa)BTuuu,
∇2f(uuu|P) =E[︁
exp(︁
uuuTBzzz)︁
BzzzzzzTBT]︁
E[exp (uuuTBzzz)]−E[︁
exp(︁
uuuTBzzz)︁
Bzzz]︁
E[︁
exp(︁
uuuTBzzz)︁
zTBT]︁
E[exp (uuuTBzzz)]2−Bdiag (aaa)BT
=B⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣E[︁
exp(︁
uuuTBzzz)︁
zzzzzzT]︁
E[exp (uuuTBzzz)]−E[︁
exp(︁
uuuTBzzz)︁
zzz]︁
E[︁
exp(︁
uuuTBzzz)︁
zzzT]︁
E[exp (uuuTBzzz)]2
⏞ ⏟⏟ ⏞
Covariance of zzz∼P(zzz) exp(︁
uuuTBzzz)︁
E[exp (uuuTBzzz)]−diag (aaa)⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦BT
The new probability densityP(zzz) exp(︁
uuuTBzzz)︁
E[exp (uuuTBzzz)]is an exponential tilt of the original pdf, and since {zi}d
i=1
are independent, and the new tilted density also factorizes over the zi’s, therefore, the covariance under this
tilted density is also diagonal.
LetC:=BD 0BT. We denote the ithcolumn of B as B.i. Define functions
ri(uuu) := ln(︁
E[︁
exp(︁
uuuTB.izi)︁]︁)︁
−Var (zi)uuuTB.iBT
.iuuu
2
gi(uuu) :=∇ri(uuu)
Thenf(uuu|P)=∑︁d
i=1ri(uuu),∇f(uuu|P)=∑︁d
i=1gi(uuu). For fixed point iteration, uuuk+1=∇f(C−1uuuk|P)
∥∇f(C−1uuuk|P)∥.
Consider the function ∇f(︁
C−1uuu|P)︁
. Leteidenote the ithaxis-aligned unit basis vector of Rn. SinceBis
24full-rank, we can denote ααα=B−1uuu. We have,
∇f(︁
C−1uuu|P)︁
=d∑︂
i=1gi(︁
C−1uuu)︁
=d∑︂
i=1E[︂
exp(︂
uuuT(︁
C−1)︁TB.izi)︂
B.izi]︂
E[︂
exp(︂
uuuT(C−1)TB.izi)︂]︂−Var (zi)B.iBT
.iC−1uuu
=d∑︂
i=1E[︂
exp(︂
uuuT(︁
BT)︁−1D−1
0eeeizi)︂
B.izi]︂
E[︂
exp(︂
uuuT(BT)−1D−1
0eeeizi)︂]︂−Var (zi)B.ieeeT
iD−1
0B−1u
=d∑︂
i=1⎛
⎝E[︂
exp(︂
αi
(D0)iizi)︂
zi]︂
E[︂
exp(︂
αi
(D0)iizi)︂]︂−Var (zi)αi
(D0)ii⎞
⎠B.i
Fort∈R, define the function qi(t) :R→Ras -
qi(t) :=E[︂
exp(︂
t
(D0)iizi)︂
zi]︂
E[︂
exp(︂
t
(D0)iizi)︂]︂−Var (zi)t
(D0)ii
Note thatqi(0) = E[zi] = 0. For t= (t1,t2,···td)∈Rd, letq(t) := [q1(t1),q2(t2)···qd(td)]T∈Rd. Then,
∇f(︁
C−1u|P)︁
=Bq(ααα)
Therefore, if uuut+1=Bαααt+1, then
Bαααt+1=Bq(αααt)
∥Bq(αααt)∥
=⇒αααt+1=q(αααt)
∥Bq(αααt)∥
=⇒ ∀i∈[d],(αααt+1)i=qi((αααt)i)
∥Bq(αααt)∥
At the fixed point, ααα∗=eee1
∥Beee1∥andBqqq(ααα∗)
∥Bqqq(ααα∗)∥2=Beee1. Therefore,
∀i∈[2,d],(αααt+1)i−α∗
i= (αααt+1)i=qi((αααt)i)
∥Bq(αααt)∥(A.26)
fori= 1,(αααt+1)1−α∗
1=q1((αααt)i)
∥Bq(αααt)∥−1
∥Beee1∥(A.27)
Note the smoothness assumptions on qi(.) mentioned in Theorem 4, ∀i∈[d],
1. supt∈[−∥B−1∥2,∥B−1∥2]|qi(t)|≤c1
2. supt∈[−∥B−1∥2,∥B−1∥2]|q′
i(t)|≤c2
3. supt∈[−∥B−1∥2,∥B−1∥2]|q′′
i(t)|≤c3
Since∀k,∥uuuk∥= 1, therefore,∀k,∥αααt∥≤∥B−1∥2. We seek to prove that the sequence {αααt}n
k=1converges to
ααα∗.
25A.1.2.1 Taylor expansions
Consider the function gi(y) :=qi(yi)
∥Bq(yyy)∥foryyy∈Rd. We start by computing the gradient, ∇ygi(y).
Lemma A.8. Letgi(y) :=qi(yi)
∥Bq(yyy)∥, then we have
[∇ygi(y)]j=⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩1
∥Bq(yyy)∥∂qi(yi)
∂yi−qi(yi)
∥Bq(yyy)∥2∂∥Bq(yyy)∥
∂qi(yi)∂qi(yi)
∂yi,forj=i
−q′
j(yj)
∥Bq(yyy)∥qi(yi)eeeT
jBTBq(yyy)
∥Bq(yyy)∥2,forj̸=i
Proof. The derivative w.r.t yiis given as -
∂gi(yyy)
∂yi=1
∥Bq(yyy)∥∂qi(yi)
∂yi−qi(yi)
∥Bq(yyy)∥2∂∥Bq(yyy)∥
∂yi
=1
∥Bq(yyy)∥∂qi(yi)
∂yi−qi(yi)
∥Bq(yyy)∥2∂∥Bq(yyy)∥
∂qi(yi)∂qi(yi)
∂yi
Note that
∇yyy∥Ayyy∥2=1
∥Ayyy∥ATAyyy
Therefore,
∂gi(yyy)
∂yi=1
∥Bq(yyy)∥∂qi(yi)
∂yi−qi(yi)
∥Bq(yyy)∥21
∥Bq(yyy)∥eeeT
iBTBq(yyy)∂qi(yi)
∂yi
=q′
i(yi)
∥Bq(yyy)∥[︃
1−qi(yi)eeeT
iBTBq(yyy)
∥Bq(yyy)∥2]︃
(A.28)
Forj̸=i, the derivative w.r.t yjis given as -
∂gi(yyy)
∂yj=−qi(yi)
∥Bq(yyy)∥2∂∥Bq(yyy)∥
∂qj(yj)∂qj(yj)
∂yj
=−q′
j(yj)
∥Bq(yyy)∥qi(yi)eeeT
jBTBq(yyy)
∥Bq(yyy)∥2(A.29)
Next, we bound qi(t) andq′
i(t).
Lemma A.9. Under the smoothness assumptions on qi(.)mentioned in Theorem 4, we have for t∈
[−∥B−1∥2,∥B−1∥2],
1.|q1(t)−q1(α∗
1)|≤c2|t−α∗
1|,|q′
1(t)−q′
1(α∗
1)|≤c3|t−α∗
1|
2.∀i,|qi(t)|≤c3t2
2,|q′
i(t)|≤c3|t|
Proof. First consider qi(t)andq′
i(t)fori̸=1. Using Taylor expansion around t= 0, we have for some
c∈(0,t) and∀i̸= 1,
qi(t) =qi(0) +tq′
i(0) +t2
2q′′
i(c) and
q′
i(t) =q′
i(0) +tq′′
i(0)
26Now, we know that qi(0) =q′
i(0) = 0. Then using Assumption 1, we have, for t∈[−∥B−1∥2,∥B−1∥2],
|qi(t)|≤c3t2
2and (A.30)
|q′
i(t)|≤c3|t| (A.31)
Similarly, using Taylor expansion around α∗
1=1
∥Beee1∥2forq1(.) we have, for some c′∈(0,t)
q1(t) =q1(α∗
1) + (t−α∗
1)q′
1(c′) and
q′
1(t) =q′
1(α∗
1) + (t−α∗
1)q′′
1(c′)
Therefore, using Assumption 4, we have, for t∈[−∥B−1∥2,∥B−1∥2]
|q1(t)−q1(α∗
1)|≤c2|t−α∗
1|and (A.32)
|q′
1(t)−q′
1(α∗
1)|≤c3|t−α∗
1| (A.33)
A.1.2.2 Using convergence radius
In this section, we use the Taylor expansion results for qi(.)andq′
i(.)in the previous section to analyze the
following functions for yyy∈Rd,
1.w(yyy) :=∥Byyy∥
2.v(yyy;i) :=eeeT
iBTBq(yyy)
∥Bq(yyy)∥2
Under the constraints specified in the theorem statement we have,
∥yyy−ααα∗∥2≤R, R≤max{c2/c3,1},
ϵ:=∥B∥F
∥Beee1∥max{︃c2
|q1(α∗
1)|,c3
|q′
1(α∗
1)|}︃
, ϵR≤1
10(A.34)
We start with w(yyy).
Lemma A.10.∀yyy∈Rd,satisfying (A.34) , letδ(yyy) :=q(yyy)−q1(α∗
1)eee1. Then, we have
1.(1−ϵ∥yyy−ααα∗∥)|q1(α∗
1)|∥Beee1∥≤∥Bq(yyy)∥≤(1 +ϵ∥yyy−ααα∗∥)|q1(α∗
1)|∥Beee1∥
2.∥δ(yyy)∥≤c2∥yyy−ααα∗∥
Proof. Consider the vector δ(yyy) :=q(yyy)−q1(α∗
1)eee1. Then,
|(δ(yyy))ℓ|≤{︄
c2|y1−α∗
1|,forℓ= 1,using Eq A.32
c3
2(y)2
ℓ,forℓ̸= 1,using Eq A.30(A.35)
Note that
∥δ(yyy)∥≤c2∥yyy−ααα∗∥ (A.36)
Now consider w(yyy). Using the mean-value theorem on the Euclidean norm we have,
∥Bq(yyy)∥=|q1(α∗
1)|∥Beee1∥+1
∥Bγ(yyy)∥(︁
BTBγ(yyy))︁Tδ(yyy), (A.37)
27whereγ(yyy) =µq(yyy) + (1−µ)q1(α∗
1)eee1, µ∈(0,1). Then,
⃦⃦⃦⃦1
∥Bγ(yyy)∥(︁
BTBγ(yyy))︁Tδ(yyy)⃦⃦⃦⃦≤1
∥Bγ(yyy)∥⃦⃦BTBγ(yyy)⃦⃦∥δ(yyy)∥
≤1
∥Bγ(yyy)∥∥B∥∥Bγ(yyy)∥∥δ(yyy)∥
=∥B∥∥δ(yyy)∥
≤c2∥B∥∥yyy−ααα∗∥,using Eq A.36
≤c2∥B∥F∥yyy−ααα∗∥,
≤ϵ|q1(α∗
1)|∥Beee1∥∥yyy−ααα∗∥,using Eq A.34 (A.38)
Therefore using A.37,
(1−ϵ∥yyy−ααα∗∥)|q1(α∗
1)|∥Beee1∥≤∥Bq(yyy)∥≤(1 +ϵ∥yyy−ααα∗∥)|q1(α∗
1)|∥Beee1∥ (A.39)
Finally, we consider the function v(yyy;i) :=eeeT
iBTBq(yyy)
∥Bq(yyy)∥2.
Lemma A.11.∀yyy∈Rdsatisfying (A.34) , we have
1. Fori= 1,1−5ϵ∥yyy−ααα∗∥≤q1(α∗
1)v(yyy|1)≤1 + 5ϵ∥yyy−ααα∗∥
2. Fori̸= 1,|q1(α∗
1)v(yyy;i)|≤(1 + 5ϵ∥yyy−ααα∗∥)∥Beeei∥
∥Beee1∥
Proof. Defineθ:=ϵ∥yyy−ααα∗∥for convenience of notation. We have,
eeeT
iBTBq(yyy)
∥Bq(yyy)∥2=q1(α∗
1)eeeT
iBTBeee1
∥Bq(yyy)∥2+eeeT
iBTBδ(yyy)
∥Bq(yyy)∥2,using definition of δ(yyy) (A.40)
Therefore, if i= 1, then using Lemma A.10,
1
(1 +θ)2+q1(α∗
1)eeeT
1BTBδ(yyy)
∥Bq(yyy)∥2≤q1(α∗
1)eeeT
1BTBq(yyy)
∥Bq(yyy)∥2≤1
(1−θ)2+q1(α∗
1)eeeT
1BTBδ(yyy)
∥Bq(yyy)∥2(A.41)
Using the following inequalities -
1
(1 +θ)2≥1−2θ,θ≥0,and,1
(1−θ)2≤1 + 5θ,θ≤1
5
and observing that using Eq A.34, and Lemma A.10,
⃓⃓⃓⃓q1(α∗
1)eeeT
1BTBδ(yyy)
∥Bq(yyy)∥2⃓⃓⃓⃓≤|q1(α∗
1)|∥Beee1∥
(1−θ)2|q1(α∗
1)|2∥Beee1∥2∥B∥∥δ(yyy)∥
≤1
(1−θ)2c2∥B∥
|q1(α∗
1)|∥Beee1∥∥yyy−ααα∗∥
≤ϵ
(1−θ)2∥yyy−ααα∗∥
Therefore we have from Eq A.41,
1−5θ≤q1(α∗
1)v(yyy;i)≤1 + 5θ (A.42)
28where we used θ:=ϵ∥yyy−ααα∗∥. For the case of i̸= 1, using Lemma A.10 we have
|q1(α∗
1)||v(yyy;i)|=|q1(α∗
1)|⃓⃓⃓⃓eeeT
iBTBq(yyy)
∥Bq(yyy)∥2⃓⃓⃓⃓
≤|q1(α∗
1)|⃓⃓⃓⃓∥Beeei∥
∥Bq(yyy)∥⃓⃓⃓⃓
≤1
(1−θ)2∥Beeei∥
∥Beee1∥
≤(1 + 5θ)∥Beeei∥
∥Beee1∥(A.43)
We now operate under the assumption that Eq A.34 holds for yyy=αααtand inductively show that it holds
foryyy=αααt+1as well. Recall the function gi(yyy):=qi(yi)
∥Bq(yyy)∥. By applying the mean-value theorem for gi(.)
for the points αααtandααα∗, we have from Eq A.26 and A.27 -
|(αααt+1)i−α∗
i|=|gi(αααt)−gi(ααα∗)|
=⃓⃓⃓∇gi(βββi)T(αααt−ααα∗)⃓⃓⃓forβββi:= (1−λi)αααt+λiααα∗, λi∈(0,1)
≤∥∇gi(βββi)∥∥αααt−ααα∗∥ (A.44)
Note the induction hypothesis assumes that Eq A.34 is true for x=αααt. Since∀i,λi∈(0,1), therefore Eq A.34
holds for all βββias well. Squaring and adding Eq A.44 for i∈[d] and taking a square-root, we have
∥αααt+1−ααα∗∥≤∥αααt−ααα∗∥⌜⃓⃓⎷k∑︂
i=1∥∇gi(βββi)∥2
≤∥αααt−ααα∗∥⌜⃓⃓⃓⎷k∑︂
i=1k∑︂
j=1⎛
⎝∂gi(yyy)
∂yj⃓⃓⃓⃓⃓
βββi⎞
⎠2
(A.45)
Let us consider the expression Gij:=∂gi(yyy)
∂yj⃓⃓⃓⃓⃓
βββi,∀i,j∈[k]. For the purpose of the subsequent analysis, we
defineθ:=ϵ∥αααt−ααα∗∥. We divide the analysis into the following cases -
Case 1 :i= 1,j= 1
Using Lemma A.8,
|G11|=⃓⃓⃓⃓q′
1((βββ1)1)
∥Bw(βββ1)∥[︃
1−q1((βββ1)1)eeeT
1BTBqqq(βββ1)
∥Bqqq(βββ1)∥2]︃⃓⃓⃓⃓
From Lemma A.9,
|q′
1((βββ1)1)|≤|q′
1(α∗
1)|+c3|(βββ1)1−α∗
1|
=|q′
1(α∗
1)|(︃
1 +c3|(βββ1)1−α∗
1|
|q′
1(α∗
1)|)︃
≤|q′
1(α∗
1)|(︃
1 +c3∥αααt−ααα∗∥
|q′
1(α∗
1)|)︃
≤|q′
1(α∗
1)|(1 +θ)
29and,
q1((βββ1)1)≤|q1(α∗
1)|+c2|(βββ1)1−α∗
1|
=|q1(α∗
1)|(︃
1 +c2|(βββ1)1−α∗
1|
|q1(α∗
1))︃
≤|q1(α∗
1)|(︃
1 +c2∥αααt−ααα∗∥
|q1(α∗
1)|)︃
≤|q1(α∗
1)|(1 +θ)
From Lemma A.10,
∥Bqqq(βββ1)∥≥(1−θ)|q1(α∗
1)|∥Beee1∥
From Lemma A.11 and θ≤1
10,
−6θ≤1−q1((βββ1)1)eeeT
1BTBw(βββ1)
∥Bw(βββ1)∥2≤6θ
Therefore,
|G11|≤6(︃1 +θ
1−θ)︃|q′
1(α∗
1)|θ
∥Beee1∥|q1(α∗
1)|
≤7.5ϵ|q′
1(α∗
1)|
∥Beee1∥|q1(α∗
1)|∥αααt−ααα∗∥,sinceθ≤1
10
Case 2 :i= 1,j̸= 1
Using Lemma A.8,
|G1j|=⃓⃓⃓⃓⃓⃓q′
j(︂
(βββ1)j)︂
∥Bqqq(βββ1)∥q1((βββ1)1)eeeT
jBTBqqq(βββ1)
∥Bqqq(βββ1)∥2⃓⃓⃓⃓⃓⃓
From Lemma A.9,
⃓⃓⃓q′
j(︂
(βββ1)j)︂⃓⃓⃓≤c3⃓⃓⃓(βββ1)j−α∗
j⃓⃓⃓≤c3⃓⃓⃓(αααt)j−α∗
j⃓⃓⃓,sinceα∗
j= 0
From Lemma A.10,
∥Bqqq(βββ1)∥≥(1−θ)|q1(α∗
1)|∥Beee1∥
From Lemma A.9,
|q1((βββ1)1)|≤|q1(α∗
1)|+c2|(βββ1)1−α∗
1|
=|q1(α∗
1)|(︃
1 +c2|(βββ1)1−α∗
1|
|q1(α∗
1)|)︃
≤|q1(α∗
1)|(1 +θ)
From Lemma A.11,
⃓⃓⃓⃓⃓eeeT
jBTBqqq(βββ1)
∥Bqqq(βββ1)∥2⃓⃓⃓⃓⃓≤(1 + 5θ)
|q1(α∗
1)|∥Beeej∥
∥Beee1∥
Therefore,
|G1j|≤c3(︃1 +θ
1−θ)︃
(1 + 5θ)∥Beeej∥
|q1(α∗
1)|∥Beee1∥2⃓⃓⃓(αααt)j−α∗
j⃓⃓⃓≤2c3∥Beeej∥
|q1(α∗
1)|∥Beee1∥2⃓⃓⃓(αααt)j−α∗
j⃓⃓⃓
30Case 3 :i̸= 1,j= 1
Using Lemma A.8,
|Gi1|=⃓⃓⃓⃓q′
1((βββi)1)
∥Bqqq(βββi)∥qi((βββi)i)eeeT
1BTBqqq(βββi)
∥Bqqq(βββi)∥2⃓⃓⃓⃓
From Lemma A.9,
|q′
1((βββi)1)|≤|q′
1(α∗
1)|+c3|(βββi)1−α∗
1|
=|q′
1(α∗
1)|(︃
1 +c3|(βββi)1−α∗
1|
|q′
1(α∗
1)|)︃
≤|q′
1(α∗
1)|(1 +θ)
From Lemma A.10,
∥Bqqq(βββi)∥≥(1−θ)|q1(α∗
1)|∥Beee1∥
From Lemma A.9,
|qi((βββi)i)|≤c3
2((βββi)i−α∗
i)2≤c3
2((αααt)i−α∗
i)2
From Lemma A.11,
⃓⃓⃓⃓eeeT
1BTBqqq(βββi)
∥Bqqq(βββi)∥2⃓⃓⃓⃓≤1 + 5θ
|q1(α∗
1)|
Therefore,
|Gi1|≤c3
2(︃(1 +θ) (1 + 5θ)
1−θ)︃|q′
1(α∗
1)|
|q1(α∗
1)|2∥Beee1∥((αααt)i−α∗
i)2
≤c3|q′
1(α∗
1)|
|q1(α∗
1)|2∥Beee1∥((αααt)i−α∗
i)2
≤c3R|q′
1(α∗
1)|
|q1(α∗
1)|2∥Beee1∥|(αααt)i−α∗
i|
≤c2|q′
1(α∗
1)|
|q1(α∗
1)|2∥Beee1∥|(αααt)i−α∗
i|
Case 4 :i̸= 1,j̸= 1,i=j
Using Lemma A.8,
|Gii|=⃓⃓⃓⃓q′
i((βββi)i)
∥Bqqq(βββi)∥[︃
1−qi((βββi)i)eeeT
iBTBqqq(βββi)
∥Bqqq(βββi)∥2]︃⃓⃓⃓⃓
From Lemma A.9,
|q′
i((βββi)i)|≤c3⃓⃓(βββi)i−α∗
j⃓⃓≤c3|(αααt)i−α∗
i|
From Lemma A.10,
∥Bqqq(βββi)∥≥(1−θ)|q1(α∗
1)|∥Beee1∥
From Lemma A.9,
|qi((βββi)i)|≤c3
2((βββi)i−α∗
i)2≤c3
2((αααt)i−α∗
i)2≤c3R∥αααt−ααα∗∥≤c2∥αααt−ααα∗∥
31From Lemma A.11,
⃓⃓⃓⃓eeeT
iBTBqqq(βββi)
∥Bqqq(βββi)∥2⃓⃓⃓⃓≤(1 + 5θ)∥Beeei∥
|q1(α∗
1)|∥Beee1∥
Then,
qi((βββi)i)eeeT
iBTBqqq(βββi)
∥Bqqq(βββi)∥2≤(1 + 5θ)∥αααt−ααα∗∥∥Beeei∥
∥Beee1∥c2
|q1(α∗
1)|
≤(1 + 5θ)∥αααt−ααα∗∥∥B∥F
∥Beee1∥c2
|q1(α∗
1)|
≤(1 + 5θ)ϵ∥αααt−ααα∗∥
≤2θ
Therefore,
|Gii|≤c3|(αααt)i−α∗
i|
(1−θ)|q1(α∗
1)|∥Beee1∥≤2c3
|q1(α∗
1)|∥Beee1∥|(αααt)i−α∗
i|
Case 5 :i̸= 1,j̸= 1,i̸=j
Using Lemma A.8,
|Gij|=⃓⃓⃓⃓⃓⃓q′
j(︂
(βββi)j)︂
∥Bqqq((βββi))∥qi((βββi)i)eeeT
jBTBqqq((βββi))
∥Bqqq((βββi))∥2⃓⃓⃓⃓⃓⃓
From Lemma A.9,
⃓⃓⃓q′
j(︂
(βββi)j)︂⃓⃓⃓≤c3⃓⃓⃓(βββi)j−α∗
j⃓⃓⃓
≤c3⃓⃓⃓(αααt)j−α∗
j⃓⃓⃓
From Lemma A.10,
∥Bqqq(βββi)∥≥(1−θ)|q1(α∗
1)|∥Beee1∥
From Lemma A.9,
|qi((βββi)i)|≤c3
2((βββi)i−α∗
i)2≤c3
2((αααt)i−α∗
i)2≤c3R
2|(αααt)i−α∗
i|
From Lemma A.11,
⃓⃓⃓⃓⃓eeeT
jBTBqqq(βββi)
∥Bqqq(βββi)∥2⃓⃓⃓⃓⃓≤(1 + 5θ)
|q1(α∗
1)|∥Beeej∥
∥Beee1∥
Therefore,
|Gij|≤c2
3R
2(︃1 + 5θ
1−θ)︃∥Beeej∥
|q1(α∗
1)|2∥Beee1∥2⃓⃓⃓(αααt)j−α∗
j⃓⃓⃓|(αααt)i−α∗
i|≤c2
3R∥Beeej∥
|q1(α∗
1)|2∥Beee1∥2⃓⃓⃓(αααt)j−α∗
j⃓⃓⃓|(αααt)i−α∗
i|
A.1.2.3 Putting everything together
Putting all the cases together in Eq A.45, we have∥αααt+1−ααα∗∥
∥αααt−ααα∗∥≤C∥αααt−ααα∗∥, where
C:=⌜⃓⃓⃓⃓⃓⃓⃓⃓⎷(︃7.5ϵ|q′
1(α∗
1)|
∥Beee1∥|q1(α∗
1)|)︃2
+(︃2c3∥B∥F
|q1(α∗
1)|∥Beee1∥2)︃2
+(︄
c2|q′
1(α∗
1)|
|q1(α∗
1)|2∥Beee1∥)︄2
+(︃2c3
|q1(α∗
1)|∥Beee1∥)︃2
+(︄
c2
3R2∥B∥F
|q1(α∗
1)|2∥Beee1∥2)︄2
32Then we have, C≤5∥B∥F
∥Be1∥2max{︃
c3
|q1(α∗
1)|,c2
2
|q1(α∗
1)|2}︃
. For linear convergence, we require CR< 1, which is
ensured by the condition mentioned in Theorem 4.
A.2 Additional experiments for noisy ICA
A.2.1 Experiments with super-gaussian source signals
Table 2 shows the Amari error in the presence of super-Gaussian signals. The signals are 1) a Uniform
distribution U(−√
3,√
3), 2) Bernoulli(︂
1
2+1√
12)︂
, 3) Laplace(0, 0.05) (mean 0 and standard deviation 0.05),
4) Exponential(5), and 5) and 6) a Student’s t-distribution with 3 and 5 degrees of freedom, respectively. The
Meta algorithm closely follows the best candidate algorithm even in the presence of many super-Gaussian
signals.
Table 2: Variation of Amari error with Sample Size for Heavy-Tailed distributions, averaged over 100 random
runs. Noise power ρ= 0.001, number of sources k= 6.
Algorithmn200 500 1000 5000 10000
Meta 0.44376 0.25215 0.20222 0.11435 0.0838
CHF 1.10103 0.70823 0.52529 0.21344 0.09194
CGF 1.84266 1.51216 1.3702 0.84351 0.58753
PEGI 2.27873 1.86561 1.71474 1.33709 1.23322
PFICA 0.39237 0.25468 0.21222 0.12878 0.12347
JADE 0.70174 0.39246 0.28199 0.12652 0.09686
FastICA 0.66441 0.3869 0.28419 0.13215 0.09946
A.2.2 Image demixing experiments
In this section, we provide additional experiments for image-demixing using ICA. We mix images using
flattening and linearly mixing them with a 4 ×4 matrixB(i.i.d entries∼N (0,1)) and Wishart noise
(ρ= 0.001). Demixing is performed using the SINR-optimal demixing matrix (see Section 2) and the results
are shown in Figure A.1 along with their corresponding independence scores. The CHF-based method recovers
the original sources well, upto sign. The Kurtosis and CGF-based method fails to recover the second source.
This is consistent with their higher independence score. The Meta algorithm selects CHF from candidates
CHF, CGF, Kurtosis, FastICA, and JADE.
A.2.3 Image denoising experiments
In this experiment, we use the ICA-based denoising technique proposed in [ 38] to compare candidate Noisy
ICA algorithms and show that the Meta algorithm can pick the best-denoised image based on the independence
score proposed in our work.
We use the noisy MNIST dataset and further add entrywise Gaussian noise with variance proportional
tosin2(c1π(i+j))(c1= 50) to the ( i,j)thpixel. Training images are flattened to create a 784-dimensional
vector, and PCA is performed to reduce dimensionality to 25, on which subsequently ICA is performed. The
original and denoised images along with their independence score are shown in Figure A.2. We note that
CHF-based denoising provides qualitatively better results, while CGF-based denoising provides the worst
results. This is consistent with their corresponding independence scores.
33(a) Source Images
(b) Mixed Images
(c) Demixed Images using CHF-based contrast function (∆( ttt,F|P) = 5.26×10−3).
(d) Demixed Images using Kurtosis-based contrast function (∆( ttt,F|P) = 2.48×10−2)
(e) Demixed Images using CGF-based contrast function (∆( ttt,F|P) = 4×10−2).
(f) Demixed Images using FastICA (∆( ttt,F|P) = 7.1×10−3).
(g) Demixed Images using JADE (∆( ttt,F|P) = 6.8×10−3).
Figure A.1: Image-Demixing using ICA34(a) Original Images
(b) CHF-based denoising (∆( ttt,F|P) = 4.4×10−6)
(c) Kurtosis-based denoising (∆( ttt,F|P) = 1.7×10−4)
(d) CGF-based denoising (∆( ttt,F|P) = 9.2×10−6)
(e) FastICA-based denoising (∆( ttt,F|P) = 5.9×10−6)
(f) JADE-based denoising (∆( ttt,F|P) = 4.6×10−6)
Figure A.2: Image Denoising using ICA
35Figure A.3: Mean independence score with errorbars from 50 random runs
A.3 Algorithm for sequential calculation of independence scores
In this section, we provide an algorithm for the computation of the independence score proposed in the
manuscript, but in a sequential manner. The detailed algorithm is provided as Algorithm 3. We assume that
we have access to a matrix of the form C=BDBT.
The power method (see Eq 3) essentially extracts one column of B(up to scaling) at every step. [ 49]
provides an elegant way to use the pseudo-Euclidean space to successively project the data onto columns
orthogonal to the ones extracted. This enables one to extract each column of the mixing matrix. For
completeness, we present the steps of this projection. After extracting the ithcolumnuuu(i), the algorithm
maintains two matrices. The first, denoted by U, estimates the mixing matrix Bone column at a time
(up to scaling). The second, denoted by VestimatesB−1one row at a time. It is possible to extend the
independence score in Eq 2 to the sequential setting as follows. Let uuu(j)be thejthvector extracted by the
power method (Eq 3) until convergence. After extracting ℓcolumns, we project the data using min(ℓ+ 1,k)
projection matrices. These would be mutually independent if we indeed extracted different columns of B.
LetC=BDBTfor some diagonal matrix D. For convenience of notation, denote Bi≡B(:,i). Set -
vvv(j)=C†uuu(j)
uuu(j)TC†uuu(j)(A.46)
Whenuuu(j)=Bj/∥Bj∥,vvv(j)=(BT)−1D†eeej
eeeT
jD†eeej∥Bj∥= (BT)−1eeej∥Bj∥.Letxxxdenote an arbitrary datapoint. Thus
U(:,j)V(j,:)xxx=Bjzj+U(:,j)V(j,:)ggg (A.47)
Thus the projection on all other columns j >ℓ is given by:
(I−UV)xxx=k∑︂
i=1Bizi−ℓ∑︂
j=1Bjzj+ggg˜=k∑︂
j=ℓ+1Bjzj+ggg˜
whereggg˜=Fggg, whereFis somek×kmatrix. So we have min(ℓ,k) vectors of the form ziBi+ggg˜i, and when
ℓ<k an additional vector which contains all independent random variables zj,j >ℓ along with a mean-zero
Gaussian vector. Then we can project each vector to a scalar using unit random vectors and check for
independence using the Independence Score defined in 2. When ℓ=k, then all we need are the j= 1,...,k
projections on the kdirections identified via ICA in Eq A.47.
As an example, we conduct an experiment (Figure A.3), where we fix a mixing matrix Busing the same
36generating mechanism in Section 5. Now we create vectors qwhich interpolate between B(:,1) and a fixed
arbitrary vector orthogonal to B(:,1). As the interpolation changes, we plot the independence score for
direction q. The dataset is the 9-dimensional dataset, which has independent components from many different
distributions (see Section 5).
This plot clearly shows that there is a clear negative correlation between the score and the dot product of
a vector with the column B(:,1). To be concrete, when qhas a small angle with B(:,1), the independence
score is small, and the error bars are also very small. However, as the dot product decreases, the score grows
and the error bars become larger.
Algorithm 3 Independence Score after extracting ℓcolumns.UandVare running estimates of BandB−1
uptoℓcolumns and rows respectively.
Input
X∈Rn×k,U∈Rk×ℓ,V∈Rℓ×k, Number of random projections M
k0←min(ℓ+ 1,k)
forjin range[1, ℓ]do
Yj←X(U(:,j)V(j,:))T
end for
ifℓ<k then
Yℓ+1←X(︁
I−VTUT)︁
end if
forjin range[1, M]do
ttt←random unit vector in Rk
forain range[1, k0]do
W(:,j)←Yjttt
end for
Letααα∈R1,k0represent a row of W
S˜←cov(W)
γ←∑︁
i,jS˜ij
βββ←∑︁
iS˜ii
s(j) =|Eˆexp(∑︁
jiαj) exp(−βββ)−∏︁k0
j=1Eˆexp(iαj) exp(−γ)|
end for
Return mean( s),stdev(s)
We refer to this function as ∆ (X,U,V,ℓ )which takes as input, the data X, the number of columns ℓand
matricesU,Vwhich are running estimates of BandB−1uptoℓcolumns and rows respectively.
A.4 More details for third derivative condition in theorem 3
Theorem 3 requires the condition “The third derivative of hX(u) does not change the sign in the half line
[0,∞) for the non-Gaussian random variable considered in the ICA problem.”. In this section, we provide
sufficient conditions with respect to contrast functions and datasets where this holds. Further, we also provide
an interesting example to demonstrate that our Assumption 1-(d) might not be too far from being necessary.
CGF-based contrast function. Consider the cumulant generating function of a random variable X, i.e.
the logarithm of the moment generating function. Now consider the contrast function
gX(t) =CGF (t)−var(X)t2/2.
We first note that it satisfies Assumption 1(a)-(d). Next, we observe that it is enough for a distribution
to have all cumulants of the same sign to satisfy Assumption 1(d) for the CGF. For example, the Poisson
distribution has all positive cumulants. Figure A.4 depicts the third derivative of the contrast function for a
Bernoulli(1/2), Uniform, Poisson, and Exponential.
Logarithm of the symmetrized characteristic function. Figure A.5 depicts the third derivative of
this contrast function for the logistic distribution where Assumption 1(d) holds. Although the Assumption
37(a)
 (b)
(c)
 (d)
Figure A.4: Plots of the third derivative of the CGF-based contrast function for different datasets - Bernoulli( p= 0.5)
A.4a, Uniform(U(︁
−√
3,√
3)︁
) A.4b, Poisson( λ= 1) A.4c and Exponential( λ= 1) A.4d. Note that the sign stays the
same in each half-line
38Figure A.5: Plots of the third derivative of the CHF-based contrast function for Logistic(0 ,1) dataset.
does not hold for a large class of distributions here, we believe that the notion of having the same sign can be
relaxed up to some bounded parameter values instead of the entire half line, so that the global convergence
results of Theorem 3 still hold. This belief is further reinforced by Figure A.6 which shows that the loss
landscape of functions where Assumption 1(d) doesn’t hold is still smooth.
A.4.1 Towards a necessary condition
Consider the probability distribution function (see [ 36])f(x) =ke−x2
2(a+bcos(cπx)) for constants a,b,c> 0.
Forf(x) to be a valid pdf, we require that f(x)≥0, and∫︁∞
−∞f(x)dx= 1. Therefore, we have,
∫︂∞
−∞ke−x2
2(a+bcos(cπx))dx= 1 (A.48)
Noting standard integration results, we have that,
∫︂∞
−∞e−x2
2dx=√
2πand∫︂∞
−∞e−x2
2cos(cπx)dx=√
2πe−c2π2
2 (A.49)
Therefore,k=1
√
2π(a+be−c2π2
2). Some algebraic manipulations yield the following:
MGFf(x)(t) =1
(a+be−c2π2
2)et2
2(︂
a+be−c2π2
2cos(cπt))︂
Therefore, ln( MGFf(x)(t)) =−ln(a+be−c2π2
2) +t2
2+ ln(a+be−c2π2
2cos(cπt)).
We now compute the mean, µ, and variance σ2.
The mean, µcan be given as :
µ=∫︂∞
−∞f(x)xdx = 0 sincef(x) is an even function.
The variance, σ2can therefore be given as :
σ2=k(︃
a∫︂∞
−∞e−x2
2x2dx+b∫︂∞
−∞e−x2
2x2cos(cπx)dx)︃
Now, we note that∫︁∞
−∞e−x2
2x2dx=√
2πand∫︁∞
−∞e−x2
2x2cos(cπx)dx=e−c2π2
2√
2π(1−c2π2) Therefore,
σ2=k(︂
a√
2π+be−c2π2
2√
2π(1−c2π2))︂
= 1−bc2π2e−c2π2
2
a+be−c2π2
2
39The symmetrized CGF can therefore be written as
symCGF (t) := ln(MGFf(x)(t)) + ln(MGFf(x)(−t))
=−2 ln(a+be−c2π2
2) +t2+ 2 ln(a+be−c2π2
2cos(cπt))
Therefore,g(t) = symCGF (t)−(1−bc2π2e−c2π2
2
a+be−c2π2
2)t2can be written as :
g(t) =−2 ln(a+be−c2π2
2) + 2 ln(a+be−c2π2
2cos(cπt)) +bc2π2e−c2π2
2
a+be−c2π2
2t2
Finally,g′(t) can be written as :
g′(t) =d
dt(︄
2 ln(a+be−c2π2
2cos(cπt)) +bc2π2e−c2π2
2
a+be−c2π2
2t2)︄
=−2bcπe−c2π2
2sin(cπt)
a+be−c2π2
2cos(cπt)+2bc2π2e−c2π2
2
a+be−c2π2
2t
g′′(t) can be written as :
g′′(t) =−2bc2π2e−c2π2
2acos(cπt) +be−c2π2
2
(a+be−c2π2
2cos(cπt))2+2bc2π2e−c2π2
2
a+be−c2π2
2
andg′′′(t) can be evaluated as:
g′′′(t) = 2bc3π3e−c2π2
2sin(cπt)(︂
a2−2b2e−c2π2−abe−c2π2
2cos(cπt))︂
(a+be−c2π2
2cos(cπt))3
Lets seta= 2,b=−1,c=4
π. Then, we have that the pdf, f(x) =ke−x2
2(2−cos(4x)) =ke−x2
2(1 +
2 sin2(2x)). The corresponding functions are :
E[exp(tX)] =1
(2−e−8)et2
2(︁
2−e−8cos(4t))︁
g(t) := symCGF (t)−var(X)t2=−2 ln(2−e−8) + 2 ln(2−e−8cos(4t))−16e−8
2−e−8t2
g′(t) =8e−8sin(4t)
2−e−8cos(4t)−32e−8
2−e−8t
g′′(t) = 32e−82 cos(4t)−e−8
(2−e−8cos(4t))2−32e−8
2−e−8
g′′′(t) =−128e−8sin(4t)(︁
4−2e−16+ 2e−8cos(4t))︁
(2−e−8cos(4t))3
Closeness to a Gaussian MGF: It can be seen there that g′′′(t) changes sign in the half-line. However,
the key is to note that the MGF of this distribution is very close to that of a Gaussian and can be made
arbitrarily small by varying the parameters aandb. This renders the CGF-based contrast function ineffectual.
This leads us to believe that Assumption 1(d) is not far from being necessary to ensure global optimality of
the corresponding objective function.
A.5 Surface plots of the CHF-based contrast functions
Figure A.6 depicts the loss landscape of the Characteristic function (CHF) based contrast function described in
Section 3.3 of the manuscript. We plot the value of the contrast function evaluated at B−1uuu,uuu=1√
x2+y2(︃x
y)︃
forx,y∈[−1,1]. As shown in the figure. We have rotated the data so that the columns of Balign with the
XandYaxes. The global maxima occur at uuualigned with the columns of B.
40(a)
 (b)
(c)
 (d)
Figure A.6: Surface plots for zero-kurtosis (A.6c and A.6d) and Uniform (A.6a, A.6b) data with n= 10000 points,
noise-power ρ= 0.1 and number of source signals, k= 2.
41