‚ÄúGet ready for a party‚Äù: Exploring smarter smart
spaces with help from large language models
Evan King, Haoxiang Yu, Sangsu Lee, Christine Julien
The University of Texas at Austin
fe.king, hxyu, sethlee, c.julien g@utexas.edu
GPT models can infer meaning behind ambiguous user commands and control smart home devices in response. When told to
‚Äúset up for a party‚Äù, GPT-3 produces valid JSON that sets the lights to a color loop and turns on the stereo for music playback.
Abstract ‚ÄîThe right response to someone who says ‚Äúget ready
for a party‚Äù is deeply inÔ¨Çuenced by meaning and context. For
a smart home assistant (e.g., Google Home), the ideal response
might be to survey the available devices in the home and change
their state to create a festive atmosphere. Current practical
systems cannot service such requests since they require the
ability to (1) infer meaning behind an abstract statement and
(2) map that inference to a concrete course of action appropriate
for the context (e.g., changing the settings of speciÔ¨Åc devices).
In this paper, we leverage the observation that recent task-
agnostic large language models (LLMs) like GPT-3 embody a
vast amount of cross-domain, sometimes unpredictable contextual
knowledge that existing rule-based home assistant systems lack,
which can make them powerful tools for inferring user intent
and generating appropriate context-dependent responses during
smart home interactions. We Ô¨Årst explore the feasibility of a
system that places an LLM at the center of command inference
and action planning, showing that LLMs have the capacity to
infer intent behind vague, context-dependent commands like ‚Äúget
ready for a party‚Äù and respond with concrete, machine-parseable
instructions that can be used to control smart devices. We
furthermore demonstrate a proof-of-concept implementation that
puts an LLM in control of real devices, showing its ability to infer
intent and change device state appropriately with no Ô¨Åne-tuning
or task-speciÔ¨Åc training . Our work hints at the promise of LLM-
driven systems for context-awareness in smart environments,
motivating future research in this area.
Index Terms ‚Äîhuman-centered computing, artiÔ¨Åcial intelli-
gence, internet of thingsI. I NTRODUCTION
An exciting prospect of smart homes at their advent was
the potential to reduce user burden by providing seamless,
unobtrusive, and ‚Äúsmart‚Äù interfaces to everyday devices. While
smart assistants have improved signiÔ¨Åcantly over the years
with respect to speech recognition [25, 24] and user satis-
faction [20, 16], a central challenge remains: how can these
assistants be made to respond appropriately to ambiguous user
commands that may be inÔ¨Çuenced by context or are otherwise
impossible for system developers to anticipate beforehand? An
example of such a command might be a user preparing their
home to entertain for guests, who asks their smart assistant to
‚Äúget ready for a party‚Äù. The hope is that the assistant‚Äîif it is
truly smart‚Äîmight be able to help by inferring the meaning
of the statement and determining how to change the state of
available devices in response: perhaps to start up the user‚Äôs
party playlist on a smart speaker and change their smart lights
to a festive color scheme. In practice, however, such a request
is beyond the capacity of current smart home systems. Google
Home will sadly admit: ‚ÄúI‚Äôm sorry, I didn‚Äôt understand.‚Äù
In this paper, we are motivated by the observation that large
language models (LLMs) like OpenAI‚Äôs GPT-3 [3] have shown
an impressive ability to generalize to new tasks with high zero-
shot performance, as well as the capacity to infer meaning
behind semantically complex or abstract statements [15]. We
thus ask the question: can this powerful capacity for cross-arXiv:2303.14143v1  [cs.HC]  24 Mar 2023domain contextual reasoning be applied to practical smart
home applications?
To explore this question, we carry out a feasibility study that
places GPT-3 in control of a smart home. We evaluate GPT-
3‚Äôs ability to provide high-quality responses to user commands
of varying ambiguity given only a simple prompt and a data
structure containing information about devices that it can con-
trol. Our results demonstrate that LLMs like GPT can infer the
meaning behind ambiguous user commands like ‚Äúget ready for
a party‚Äù or ‚ÄúI am tired and I want to sleep‚Äù and respond with
properly-formatted data describing courses of action, enabling
more intuitive control of smart devices. We furthermore build
a proof-of-concept implementation that puts GPT-3 in control
of real devices, showing LLM-driven command inference and
action planning can function in practice with no Ô¨Åne-tuning or
task-speciÔ¨Åc training required . Motivated by our results, we
propose future work that can further leverage the power of
LLMs toward building smarter smart home applications.
Our key contributions are as follows:
An experimental setup and study results that show LLMs
can infer meaning behind abstract user commands like ‚ÄúI
am tired and I have to work‚Äù and, in response, quickly
and appropriately change the state of the smart devices
available in the home, with no task-speciÔ¨Åc training .
An implementation that puts a GPT model in control of
real devices, showing that it can intuitively respond to a
variety of commands. When told to ‚Äúset up for a party‚Äù, it
responds by turning on a stereo and conÔ¨Åguring a group
of Hue lights to loop through a festive set of colors;
given the command ‚ÄúI‚Äôm leaving‚Äù, it turns off all available
devices. We trigger these actions by inputting the LLM‚Äôs
response directly into smart device APIs .
Analytical results that suggest responses are variable in
quality, dependent on both the devices available and the
nature of the user‚Äôs command. In essence, further system
design is necessary to manage the LLM‚Äôs tendency to
‚Äúnot know what it doesn‚Äôt know‚Äù in order to produce
consistently high-quality responses.
The following describes the structure of this paper. Sec-
tion II situates our work with related research. Section III
describes the experimental setup that we use to demonstrate
the feasibility of LLMs as smart home controllers. Section IV
presents the results of our exploratory study, while Section V
demonstrates a proof-of-concept implementation. Section VI
offers avenues for future work. Section VII concludes.
II. B ACKGROUND & R ELATED WORK
This section provides a high-level overview of LLMs and
their applications before situating our work with related efforts
in context-aware smart spaces.
Large language models (LLMs) have gained signiÔ¨Åcant
attention in recent years due to their impressive performance
on a wide range of natural language processing tasks. In
2018, Devlin proposed BERT, a language representation model
that uses Bidirectional Encoder Representations from Trans-
formers [6] and can be Ô¨Åne-tuned for a variety of NLPtasks, such as text classiÔ¨Åcation and sentiment analysis. In
the same year, OpenAI proposed GPT (Generative Pre-trained
Transformer) [22]. Both models use a transformer architec-
ture [27] that was pre-trained on a massive corpus of text data,
including books, articles, and websites. The resulting models
demonstrate impressive results on a wide range of natural
language processing tasks, including language translation,
text generation, and the ability to translate natural language
descriptions into program implementations.
Following the success of the transformer-based model, sub-
sequent studies have explored ways to improve and expand
the model‚Äôs performance. In 2019, Radford et al., published
an updated version of GPT and called GPT-2 [23]. Building
on the success of GPT-2, Brown et al. released GPT-3 in 2020
[4]. After that, in 2023, GPT-4 was introduced. It is currently
one of the largest and most powerful language models, with
more than 1 trillion parameters [18]. At time of writing, access
to GPT-4 is limited‚Äîwe therefore base our study on GPT-3.
Two popular approaches exist for adapting task-agnostic
LLMs to new applications: prompt engineering and Ô¨Åne-
tuning . Prompt engineering refers to the process of designing
a task-speciÔ¨Åc prompt or template that guides the model to
produce relevant outputs for a particular task [29]. These
prompts generally contain instructions to the model written
in natural language‚Äîe.g., ‚Äúexplain the following passage
of text‚Äù. Fine-tuning, on the other hand, involves directly
training the model on a new task by providing task-speciÔ¨Åc
examples [23]. The key advantage of prompt engineering over
Ô¨Åne-tuning is that it does not require task-speciÔ¨Åc data‚Äî
we therefore adopt that approach here. Within the realm of
prompt engineering, there are zero-shot andfew-shot learning
approaches. Zero-shot approaches provide the model with a
single prompt containing instructions and task-speciÔ¨Åc infor-
mation; few-shot approaches provide examples to the model
of correct input/output pairs. We focus on zero-shot learning.
Context-aware spaces leverage sensor information, user
data (including past behaviors and preferences), and de-
vice state to inÔ¨Çuence system actions toward meeting user
needs [2]. The notion of ‚Äúcontext-awareness‚Äù in this sense
has roots in research on ambient intelligence [5]‚Äîthat is, the
development of built environments that sense and adapt to
users. A concrete example of this concept is a home that lever-
ages contextual information to improve energy efÔ¨Åciency [11,
7]. In an early paper, Yamazaki suggested that smart homes
should go beyond automation and instead integrate expressive
interfaces between the user and system [28], a goal that is
partially realized in smart assistants [20], but with limited
ability to adapt to more complex user commands [13]. Ample
prior work has approached the issue of context-awareness us-
ing task-speciÔ¨Åc models [21, 12, 17, 14]. While these methods
can achieve high performance given ample task-speciÔ¨Åc data,
we believe that the high zero-shot performance of LLMs could
hint at better generalizability without a need for training data.
However, we are aware of no work to-date that has explored
the use of task-agnostic LLMs for deeper contextual reasoning
in smart environments. This motivates our feasibility study.III. S YSTEM DESIGN
In this section, we introduce the system design that we use
to explore the feasibility of LLM-driven smart home control.
We Ô¨Årst assume the use of an LLM like GPT-3 that provides
responses to user prompts written in natural language. These
LLM models are not task-speciÔ¨Åc, rather, they are trained
on an immense amount of cross-domain textual information
and, depending on the structure of the prompt, can provide
responses suited to a variety of different use cases (e.g., writing
a poem, writing code in response to a high-level program
description, etc.). We opt to adapt the model‚Äôs outputs to our
task using zero-shot learning through prompt engineering.
Our challenge is therefore to package relevant context and
user commands into a concise prompt issued to the model,
such that its responses include concrete, machine-parseable
changes to device state that can be passed off to the appropriate
smart device APIs. Qualitatively, we want these courses of
action to be shaped by the model successfully inferring (1) the
intent behind the user command and (2) the manner in which
the state of available devices can be changed to meet the user‚Äôs
intent. To that end, we Ô¨Årst deÔ¨Åne an abstract schema for
capturing smart home context before describing a method for
engineering prompts to conversational LLMs that elicit useful,
actionable responses.
A. Context Schema
In order for the model to ‚Äúknow‚Äù what actions are available
to it, we need to package the available devices, their states, and
other relevant information‚Äîi.e., the context‚Äîinto a machine-
parseable format. This package effectively describes the action
space available to the model: the knobs it can turn, and
information (e.g., which room the user is in) that might
inÔ¨Çuence how it turns them. It also provides a hint about
how the model should format its response. Representations
of context can be complex and have been explored in the
literature [9, 1]. Since our goal is to conduct an exploratory
study rather than design an end-to-end solution, we use a
schema that is simple but adequate for our experimental setup.
We choose JSON for structuring this data since it is the de-
facto data interchange format for RESTful APIs used by many
smart home devices [19, 8, 10]. Leveraging a common format
is also advantageous since there is a high likelihood that the
LLM has been trained on source material that contains it,
which beneÔ¨Åts the model‚Äôs ability to converse in it.
At the top level, context is a collection of ‚Äúkey, value‚Äù pairs.
There are two relevant contexts: ‚Äúuser‚Äù context that contains
immutable information about the user‚Äôs state, e.g., which room
they are in, and ‚Äúdevice‚Äù context, which contains mutable and
immutable information about the devices in the home. Each
top-level key in the collection of device context deÔ¨Ånes a room
in the home, and within each room we deÔ¨Åne collections of
devices organized by type, e.g., ‚Äúlights‚Äù, ‚Äútvs‚Äù, etc. Within
a collection of devices, we deÔ¨Åne each individual device as
a collection of properties about that device, e.g., for a light
we can deÔ¨Åne its ‚Äústate‚Äù property and its ‚Äúr‚Äù, ‚Äúg‚Äù, and ‚Äúb‚Äù
Fig. 1: Data structures for expressing smart home device and
user context in prompts to an LLM.
color values. This overall structure is depicted in Fig. 1 and
illustrated by the example in the following:
{
"user": {
"location": "living_room"
}
}
{
"devices": {
"bedroom": {
"lights": {
"bedside_lamp": {
"state": "off"
}
}
},
"living_room": {
"lights": {
"overhead": {
"state": "on"
},
"lamp": {
"state": "off"
}
},
"tvs": {
"living_room_tv": {
"state": "off",
"volume": 20
}
}
}
}
}
In this example, the user‚Äôs home has two rooms‚Äîa bedroom
and living room‚Äîand the user is currently located in the living
room. The bedroom has one light turned off, and the living
room has two lights (one turned on) and a television.
B. Prompt Engineering
Having developed a structure for storing context, we now
move to the practical challenge of engineering prompts that
elicit useful responses from the model.Fig. 2: An example prompt and response from ChatGPT,
demonstrating its ability to change device state in response to
ambiguous user commands like ‚Äúget ready for a party‚Äù. JSON
is omitted from this Ô¨Ågure in favor of a visual depiction.
Our prompts consist of four segments, as follows:
Framing. This portion of the prompt provides direction to
the conversational agent about its role in the interaction‚Äî
it is being asked to make decisions as an AI that controls
a smart home. We open with the phrase ‚ÄúYou are an AI
that controls a smart home.‚Äù
Context. This informs the agent of the user context and
devices available in the environment, which scopes the
space of its actions and provides a hint as to the structure
of our desired response. We continue the prompt: ‚ÄúHere
is the state of the devices in the home, in JSON format:
fdevices gHere is information about the user: fuserg‚Äù,
where both contexts are formatted as shown earlier.
Command. This portion inserts the user command and
directs the agent to manipulate the state of devices in
response, as follows: ‚ÄúThe user issues the command:
fcommand g. Change the device state as appropriate.‚Äù The
command is written in natural language, as a user might
utter to their smart assistant.
Formatting. We close the prompt by requesting the
response in JSON format so that it can be easily parsed
and input to a relevant smart device API: ‚ÄúProvide your
response in JSON format.‚Äù
An example prompt with this structure and the correspond-
ing response from ChatGPT 3.5 are depicted in Figure 2. We
can see that by using the proposed context structure inside the
the prompt, we are able to elicit responses from the model
that contain changes to the underlying JSON that accurately
reÔ¨Çect what a user‚Äôs intent might be. In essence, GPT-3.5 isable to relate the meaning of ‚Äúparty‚Äù to the devices available,
as well as alter their speciÔ¨Åc settings in desirable ways. In the
next section, we use this system design to perform qualitative
analysis of the model‚Äôs responses.
IV. E VALUATION
This section describes the results of our feasibility study
using the experimental setup described in the previous section.
Our evaluations address two high-level questions:
1)How good are the agent‚Äôs responses? We measure the
quality of the agent‚Äôs responses, in the sense that they
include courses of action that can reasonably be thought
to meet the user‚Äôs request and can be easily machine-
parsed and executed.
2)How timely are the agent‚Äôs responses? We also
measure the round-trip response latency. This hints at
how feasible a practical system is with respect to user
experience and responsiveness.
To better understand the system from these two perspectives,
we design scenarios of increasing complexity and ambiguity
ofcontext and command . This captures the intuition that
(1) different smart homes can have different complexity of
context, from an apartment with a few smart lights to a large
home with many devices and (2) different user commands can
have different levels of ambiguity, from direct commands like
‚Äúturn on the light‚Äù to wholly ambiguous statements like ‚ÄúI am
tired‚Äù. Evaluating agent responses under these circumstances
allows us to identify the failure modes of LLM-driven smart
home control given increasingly challenging prompts with
respect to both the context and the nature of the command.
We use three contexts of increasing complexity, as follows:
Simple: Describes a home with a bedroom and living
room that have one and two lights, respectively, all
initially off. Lights can either be on or off but have no
other state (e.g. color).
Medium: Same as above, but adds red, green, and blue
color state to each of the lights, with expected values in
the range [0, 255].
Complex: Same as above, but adds a TV with on/off
and volume state to the bedroom, as well as a TV and
smart speaker to the living room (each also with on/off
and volume state).
Each of these contexts is expressed in the schema described
in Section III. We combine these contexts with three user
prompts of increasing ambiguity, as follows:
Direct: ‚ÄúTurn on the light. ‚Äù This command is simple
since it directly expresses a state change, as well as
a relevant device. Existing home assistants can easily
respond to this type of command.
Indirect: ‚ÄúGet ready for a party. ‚Äù This command is more
ambiguous since it expresses a desired state change, but
provides no information about which devices are relevant.
Ambiguous: ‚ÄúI am tired. ‚Äù This command is completely
ambiguous since it expresses neither a state change, nor
which devices might be relevant.We run our tests with each possible combination of these
three contexts and commands (9 total), each for 10 trials. We
save the agent‚Äôs response for each trial in a human-readable
format, then perform manual rating to measure the quality of
the responses. Our process for rating the quality of responses
is based on a binary label, where each is assigned one of the
following labels based on its quality:
Poor (0): ‚ÄúThe changes to the devices do not at all reÔ¨Çect
the intent behind the user command, or the response is
malformed/garbled.‚Äù
Good (1): ‚ÄúThe changes to the devices are reasonable for
the command. You can imagine someone being satisÔ¨Åed
with the result, even if it is somewhat subjective (e.g.,
based on different personal preferences).‚Äù
Three researchers independently reviewed all responses and
assigned them a label. We report the aggregate score for each
trial as the average across all assigned scores. We also note the
average latency for each trial‚Äîthis includes both the network
transmission time of the request, as well as the inference time
taken by the model. Since this time is subject to network
conditions and API demand, it should be taken as a rough
estimate rather than a concrete benchmark. Our results are
summarized in Table I.
Response time is a function of context complexity. With
respect to latency, we can see that responses generally arrive
on the order of seconds, meaning that a practical system
could feasibly leverage an LLM for ambiguous command
inference and action planning without signiÔ¨Åcant detriments
to user experience or responsiveness. For direct commands, a
2 to 3 second response time may be too long‚Äîfuture system
designs could thus leverage the LLM only for commands that
require it. This may entail a hybrid of rule-based inference
for common commands, along with LLM inference for less
familiar commands. It is also worth noting that as the context
increases in complexity, the response latency also increases‚Äî
this motivates future work to develop methods for Ô¨Åltering
context prior to prompting the agent so that only the most
relevant information is provided.
Response quality is a function of context and command
ambiguity. With respect to response quality, we Ô¨Ånd that the
LLM approach provides good responses given the same direct
and simple commands that current home assistants are able to
service. Note, however, that unlike existing home assistants,
the LLM approach utilizes a much simpler system architecture
that performs command inference and action planning in
the same pass. These results are consistent given increasing
degrees of context complexity, suggesting that the model was
not overwhelmed by the growth in the decision space that
comes with adding new devices and possible state changes. On
the contrary, the model provides better responses when given
more context that might be relevant to the user‚Äôs command,
as is apparent when comparing the low response quality of
the Simple/Indirect and Medium/Indirect experiments against
the Complex/Indirect experiment. Upon inspection of the
responses, the reason for this is clear: given minimal contextand a subjective command like ‚Äúget ready for a party‚Äù, the
LLM simply makes up a response‚ÄîspeciÔ¨Åcally, it turns on
all the lights in the house, to include the bedroom. When
we add a speaker and a television to the context, the model
now has more relevant knobs to turn, and produces a higher
quality response. The tendency to make something up when
the answer is unknown or requires more context is an open
problem and motivates the development of application-speciÔ¨Åc
methods to mediate between the user and the model.
For the most ambiguous command (‚ÄúI am tired‚Äù), we note
that the model delivers poor responses regardless of context.
In all but a few cases, the LLM simply turns on all of the
home‚Äôs lights. The exception is in a Medium/Ambiguous trial,
where it only turns on the bedroom lamp, perhaps to help the
user prepare for bed. This is to be expected: an individual‚Äôs
intent and preference in this case are highly subjective (are
they, e.g., tired and ready for bed or tired but they have a
pressing deadline?) and the LLM ultimately cannot read the
user‚Äôs mind. However, since the LLM does not ‚Äúknow what it
doesn‚Äôt know‚Äù, it does not ask for clariÔ¨Åcation or go with the
safest choice, which is likely to do nothing. Instead, it makes
something up.
Since our previous results suggested more context is often
beneÔ¨Åcial, we dig deeper to see if we can help it make a better
choice. We amend the vague command ‚ÄúI am tired‚Äù to offer
hints at the user‚Äôs intent:
Ambiguous*: ‚ÄúI am tired and I need to work.‚Äù
Ambiguous**: ‚ÄúI am tired and I want to sleep.‚Äù
Provided this added hint at the user‚Äôs context, the response
quality improves signiÔ¨Åcantly. For Ambiguous*, the model
consistently responds by turning on the living room lights
while leaving all other devices off; for Ambiguous**, the
model turns on only the user‚Äôs bedside lamp and, in some
cases, reduces the volume on their speaker and TV . Note that
although the amended statement includes additional context,
it still requires the model to infer meaning in a way that more
rigid or rule-based approaches cannot.
V. I MPLEMENTATION
To demonstrate LLM-driven smart home control in prac-
tice, we built a proof-of-concept implementation in Python.
Our implementation accepts user commands as strings, pack-
ages them into prompts along with contextual information
about real devices, then processes responses from OpenAI‚Äôs
text-davinci-003 model into smart home API calls that
change the device state as speciÔ¨Åed by the LLM. We scope
the application to one room (a researcher‚Äôs living room) with
three Philips Hue color smart lights [19] and one TP-Link
Kasa smart plug [26] that controls a stereo. We store the
device context in JSON as in our experimental setup, with
the difference that the device state for the Hue lights is pulled
directly from the Hue API without modiÔ¨Åcation. Our code is
open source and available online.1
1https://github.com/UT-MPC/homegptContext Command Avg. Quality Avg Latency (sec)
Simple Direct 1.00 2.42
Indirect 0.67 2.31
Ambiguous 0.00 2.22
Medium Direct 1.00 4.56
Indirect 0.63 4.70
Ambiguous 0.17 4.97
Complex Direct 1.00 7.90
Indirect 1.00 7.25
Ambiguous 0.00 7.04
Complex Ambiguous* 1.00 7.49
Ambiguous** 1.00 8.09
TABLE I: Results for experiments given various combina-
tions of different context complexity and command ambiguity.
Higher quality responses suggest the model produced a course
of action that would be desirable for an end user (e.g., turning
on the bedroom light when receiving the command ‚ÄúI am tired
and I want to sleep‚Äù). Lower latency suggests better system
responsiveness.
The teaser Ô¨Ågure depicts the result when issuing the com-
mand ‚Äúset up for a party‚Äù. We include the JSON context of
the light group2, along with a Ô¨Åeld for the plug powering
the stereo. The model mutates the parameters in the JSON to
change the stereo state to ‚Äúon‚Äù and, impressively, also changes
the ‚Äúeffect‚Äù parameter of the Hue light group from ‚Äúnone‚Äù to
‚Äúcolorloop‚Äù to create a looping color effect. The latter change
suggests that GPT-3 may have been trained on material about
the speciÔ¨Åc features of the Hue API and can leverage that
along with the inferred meaning of the user command to
trigger more intuitive changes than existing systems.
We brieÔ¨Çy list multiple other commands we tested in our
implementation, along with responses from the model:
‚Äúmake it bright in here‚Äù ‚Äì sets lights to full brightness
‚Äúmake it groovy‚Äù ‚Äì sets lights to color loop; adds invalid
‚Äúgenre‚Äù Ô¨Åeld to stereo and sets it to ‚Äùgroovy‚Äù
‚Äúgotta relax‚Äù ‚Äì dims lights, turns stereo on
‚ÄúI‚Äôm cold‚Äù ‚Äì sets lights to warm white, turns stereo on
‚ÄúI‚Äôm leaving‚Äù ‚Äì turns off lights and stereo
‚ÄúI‚Äôm home‚Äù ‚Äì turns on lights and stereo
We note, of course, that these tests are far from exhaustive.
We observed high variability in responses, meaning the same
command can elicit many responses: some good, some bad.
A more robust system design will be necessary to tackle the
inconsistencies present in current LLM model outputs. We
address this in our discussion in the following section.
VI. D ISCUSSION & F UTURE WORK
Our efforts in this paper hint at exciting opportunities for
future work. We suggest several avenues for further research.
Managing contextual information. We found that includ-
ing more context can improve the quality of the model‚Äôs
responses, but at the expense of response latency. To effec-
tively navigate this tradeoff in an end-to-end solution, a more
involved approach for storing, pre-processing, and expressing
2https://developers.meethue.com/develop/hue-api/groupds-api/context will be necessary. This will also become essential
as the amount of context grows to include sensor data, user
preference data, and a growing and more diverse set of
controllable devices. We note that in our experiments, we did
not attempt to test the limits of how much context a model
can receive before the quality or latency of responses degrades
substantially. This should be considered in future work.
Robust system design. While we were able to leverage
a simple system design in this paper, an end-to-end system
will need a more robust design to account for several factors.
First, since LLMs do not yet ‚Äúknow what they don‚Äôt know‚Äù,
the likelihood of invalid or low-quality responses remains
high. In the case of responses where the model makes invalid
changes to device state (e.g, to add new settings to a device),
a full system should include a way to enforce a set of formal
properties for device states. In the case of unsatisfactory
responses, it would be beneÔ¨Åcial to develop a method for
learning user preferences or seeking clarifying information
(e.g., ‚Äúare you tired and want to sleep, or are you tired but
need an energy boost?‚Äù).
From commands to automation. Our primary focus in
this exploratory study was on immediate commands‚Äîthe user
makes a request and the model immediately responds with
a state change. Future work could investigate the use of
LLMs for more intuitive automation planning. A user could,
for instance, ask their smart assistant to ‚Äúplay jazz when it
rains‚Äù and the model could leverage contextual information to
put in place an automation sequence that meets their needs.
This would obviate the need for pre-programmed automation
routines and could substantially improve user satisfaction with
smart assistant systems.
VII. C ONCLUSION
In this paper, we explored the feasibility of smarter smart
home control using large language models (LLMs). We pro-
posed a simple system design for capturing smart home con-
text (i.e., information about the user and controllable devices
in the environment) in engineered prompts to GPT-3, showing
that the model has the ability to infer meaning behind indirect
and ambiguous user commands like ‚ÄúI am tired and I need to
work‚Äù and, in response, generate changes to smart device state.
We implemented our system design, giving GPT-3 control
of real devices and Ô¨Ånding that it is able to quickly and
appropriately control them in response to user commands with
no Ô¨Åne tuning and no post-processing of its responses. By
simply telling GPT-3 what devices are available and what the
user wants, it can generate courses of action in response.
Our work hints at the capability of GPT-3 and similar
models to go far beyond the current abilities of smart space
control and motivates future work with context modeling, end-
to-end system design, and approaches for further leveraging
GPT-3‚Äôs capabilites to develop complex automation routines
in response to user commands.REFERENCES
[1] Hamim Md Adal et al. ‚ÄúThe Space Broker: A Middleware for
Mediating Interactions in Smart IoT Spaces‚Äù. In: Proceedings of the
8th ACM International Conference on Systems for Energy-EfÔ¨Åcient
Buildings, Cities, and Transportation . BuildSys ‚Äô21. Coimbra, Portu-
gal: Association for Computing Machinery, 2021, pp. 101‚Äì110. ISBN :
9781450391146. DOI: 10.1145/3486611.3486664. URL: https://doi.
org/10.1145/3486611.3486664.
[2] Muhammad Raisul Alam, Mamun Bin Ibne Reaz, and Mohd Alauddin
Mohd Ali. ‚ÄúA Review of Smart Homes‚ÄîPast, Present, and Future‚Äù.
In:IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews) 42.6 (2012), pp. 1190‚Äì1203. DOI: 10.1109/
TSMCC.2012.2189204.
[3] Tom Brown et al. ‚ÄúLanguage models are few-shot learners‚Äù. In: Ad-
vances in neural information processing systems 33 (2020), pp. 1877‚Äì
1901.
[4] Tom B. Brown et al. ‚ÄúLanguage Models are Few-Shot Learners‚Äù. In:
CoRR abs/2005.14165 (2020). arXiv: 2005.14165. URL: https://arxiv.
org/abs/2005.14165.
[5] Diane J Cook, Juan C Augusto, and Vikramaditya R Jakkula. ‚ÄúAm-
bient intelligence: Technologies, applications, and opportunities‚Äù. In:
Pervasive and mobile computing 5.4 (2009), pp. 277‚Äì298.
[6] Jacob Devlin et al. ‚ÄúBert: Pre-training of deep bidirectional transform-
ers for language understanding‚Äù. In: arXiv preprint arXiv:1810.04805
(2018).
[7] PR Geraldo Filho et al. ‚ÄúEnergy-efÔ¨Åcient smart home systems: Infras-
tructure and decision-making process‚Äù. In: Internet of Things 5 (2019),
pp. 153‚Äì167.
[8] Google Nest API Documentation . 2022. URL: https : / / developers .
google.com/nest/device-access.
[9] Jie Hua et al. ‚ÄúCoPI: Enabling Probabilistic ConÔ¨Çict Prediction
in Smart Space Through Context-awareness‚Äù. In: 2022 IEEE/ACM
Seventh International Conference on Internet-of-Things Design and
Implementation (IoTDI) . IEEE. 2022, pp. 30‚Äì42.
[10] Insteon REST API Documentation . 2022. URL: https://www.insteon.
com/developer.
[11] Marco Jahn et al. ‚ÄúThe energy aware smart home‚Äù. In: 2010 5th
international conference on future information technology . IEEE. 2010,
pp. 1‚Äì8.
[12] M Humayun Kabir et al. ‚ÄúMachine learning based adaptive context-
aware system for smart home environment‚Äù. In: International Journal
of Smart Home 9.11 (2015), pp. 55‚Äì62.
[13] Julia Kiseleva et al. ‚ÄúUnderstanding user satisfaction with intelligent
assistants‚Äù. In: Proceedings of the 2016 ACM on Conference on Human
Information Interaction and Retrieval . 2016, pp. 121‚Äì130.
[14] Tiankai Liang et al. ‚ÄúAn unsupervised user behavior prediction algo-
rithm based on machine learning and neural network for smart home‚Äù.
In:IEEE Access 6 (2018), pp. 49237‚Äì49247.
[15] Xiao Liu et al. ‚ÄúGPT understands, too‚Äù. In: arXiv preprint
arXiv:2103.10385 (2021).
[16] Irene Lopatovska et al. ‚ÄúTalk to me: Exploring user interactions with
the Amazon Alexa‚Äù. In: Journal of Librarianship and Information
Science 51.4 (2019), pp. 984‚Äì997.
[17] Isaac Machorro-Cano et al. ‚ÄúHEMS-IoT: A big data and machine
learning-based smart home system for energy saving‚Äù. In: Energies
13.5 (2020), p. 1097.
[18] OpenAI. GPT-4 .URL: https://openai.com/product/gpt-4.
[19] Philips Hue API Documentation . 2022. URL: https : / / developers .
meethue.com/.
[20] Amanda Purington et al. ‚Äú‚Äù Alexa is my new BFF‚Äù Social Roles, User
Satisfaction, and PersoniÔ¨Åcation of the Amazon Echo‚Äù. In: Proceedings
of the 2017 CHI conference extended abstracts on human factors in
computing systems . 2017, pp. 2853‚Äì2859.
[21] Basheer Qolomany et al. ‚ÄúLeveraging machine learning and big data
for smart buildings: A comprehensive survey‚Äù. In: IEEE Access 7
(2019), pp. 90316‚Äì90356.
[22] Alec Radford et al. ‚ÄúImproving language understanding by generative
pre-training‚Äù. In: (2018).
[23] Alec Radford et al. ‚ÄúLanguage models are unsupervised multitask
learners‚Äù. In: OpenAI blog 1.8 (2019), p. 9.
[24] Anirudh Raju et al. ‚ÄúScalable multi corpora neural language models
for asr‚Äù. In: arXiv preprint arXiv:1907.01677 (2019).[25] Prakhar Swarup et al. ‚ÄúImproving ASR conÔ¨Ådence scores for Alexa
using acoustic and hypothesis embeddings‚Äù. In: (2019).
[26] TP-Link Kasa Smart Plugs . 2023. URL: https://www.kasasmart.com/
us/products/smart-plugs.
[27] Ashish Vaswani et al. ‚ÄúAttention is all you need‚Äù. In: Advances in
neural information processing systems 30 (2017).
[28] Tatsuya Yamazaki. ‚ÄúBeyond the Smart Home‚Äù. In: 2006 International
Conference on Hybrid Information Technology . V ol. 2. 2006, pp. 350‚Äì
355. DOI: 10.1109/ICHIT.2006.253633.
[29] Tony Z. Zhao et al. Calibrate Before Use: Improving Few-Shot Per-
formance of Language Models . 2021. arXiv: 2102.09690 [cs.CL] .