â€œGet ready for a partyâ€: Exploring smarter smart
spaces with help from large language models
Evan King, Haoxiang Yu, Sangsu Lee, Christine Julien
The University of Texas at Austin
fe.king, hxyu, sethlee, c.julien g@utexas.edu
GPT models can infer meaning behind ambiguous user commands and control smart home devices in response. When told to
â€œset up for a partyâ€, GPT-3 produces valid JSON that sets the lights to a color loop and turns on the stereo for music playback.
Abstract â€”The right response to someone who says â€œget ready
for a partyâ€ is deeply inï¬‚uenced by meaning and context. For
a smart home assistant (e.g., Google Home), the ideal response
might be to survey the available devices in the home and change
their state to create a festive atmosphere. Current practical
systems cannot service such requests since they require the
ability to (1) infer meaning behind an abstract statement and
(2) map that inference to a concrete course of action appropriate
for the context (e.g., changing the settings of speciï¬c devices).
In this paper, we leverage the observation that recent task-
agnostic large language models (LLMs) like GPT-3 embody a
vast amount of cross-domain, sometimes unpredictable contextual
knowledge that existing rule-based home assistant systems lack,
which can make them powerful tools for inferring user intent
and generating appropriate context-dependent responses during
smart home interactions. We ï¬rst explore the feasibility of a
system that places an LLM at the center of command inference
and action planning, showing that LLMs have the capacity to
infer intent behind vague, context-dependent commands like â€œget
ready for a partyâ€ and respond with concrete, machine-parseable
instructions that can be used to control smart devices. We
furthermore demonstrate a proof-of-concept implementation that
puts an LLM in control of real devices, showing its ability to infer
intent and change device state appropriately with no ï¬ne-tuning
or task-speciï¬c training . Our work hints at the promise of LLM-
driven systems for context-awareness in smart environments,
motivating future research in this area.
Index Terms â€”human-centered computing, artiï¬cial intelli-
gence, internet of thingsI. I NTRODUCTION
An exciting prospect of smart homes at their advent was
the potential to reduce user burden by providing seamless,
unobtrusive, and â€œsmartâ€ interfaces to everyday devices. While
smart assistants have improved signiï¬cantly over the years
with respect to speech recognition [25, 24] and user satis-
faction [20, 16], a central challenge remains: how can these
assistants be made to respond appropriately to ambiguous user
commands that may be inï¬‚uenced by context or are otherwise
impossible for system developers to anticipate beforehand? An
example of such a command might be a user preparing their
home to entertain for guests, who asks their smart assistant to
â€œget ready for a partyâ€. The hope is that the assistantâ€”if it is
truly smartâ€”might be able to help by inferring the meaning
of the statement and determining how to change the state of
available devices in response: perhaps to start up the userâ€™s
party playlist on a smart speaker and change their smart lights
to a festive color scheme. In practice, however, such a request
is beyond the capacity of current smart home systems. Google
Home will sadly admit: â€œIâ€™m sorry, I didnâ€™t understand.â€
In this paper, we are motivated by the observation that large
language models (LLMs) like OpenAIâ€™s GPT-3 [3] have shown
an impressive ability to generalize to new tasks with high zero-
shot performance, as well as the capacity to infer meaning
behind semantically complex or abstract statements [15]. We
thus ask the question: can this powerful capacity for cross-arXiv:2303.14143v1  [cs.HC]  24 Mar 2023domain contextual reasoning be applied to practical smart
home applications?
To explore this question, we carry out a feasibility study that
places GPT-3 in control of a smart home. We evaluate GPT-
3â€™s ability to provide high-quality responses to user commands
of varying ambiguity given only a simple prompt and a data
structure containing information about devices that it can con-
trol. Our results demonstrate that LLMs like GPT can infer the
meaning behind ambiguous user commands like â€œget ready for
a partyâ€ or â€œI am tired and I want to sleepâ€ and respond with
properly-formatted data describing courses of action, enabling
more intuitive control of smart devices. We furthermore build
a proof-of-concept implementation that puts GPT-3 in control
of real devices, showing LLM-driven command inference and
action planning can function in practice with no ï¬ne-tuning or
task-speciï¬c training required . Motivated by our results, we
propose future work that can further leverage the power of
LLMs toward building smarter smart home applications.
Our key contributions are as follows:
An experimental setup and study results that show LLMs
can infer meaning behind abstract user commands like â€œI
am tired and I have to workâ€ and, in response, quickly
and appropriately change the state of the smart devices
available in the home, with no task-speciï¬c training .
An implementation that puts a GPT model in control of
real devices, showing that it can intuitively respond to a
variety of commands. When told to â€œset up for a partyâ€, it
responds by turning on a stereo and conï¬guring a group
of Hue lights to loop through a festive set of colors;
given the command â€œIâ€™m leavingâ€, it turns off all available
devices. We trigger these actions by inputting the LLMâ€™s
response directly into smart device APIs .
Analytical results that suggest responses are variable in
quality, dependent on both the devices available and the
nature of the userâ€™s command. In essence, further system
design is necessary to manage the LLMâ€™s tendency to
â€œnot know what it doesnâ€™t knowâ€ in order to produce
consistently high-quality responses.
The following describes the structure of this paper. Sec-
tion II situates our work with related research. Section III
describes the experimental setup that we use to demonstrate
the feasibility of LLMs as smart home controllers. Section IV
presents the results of our exploratory study, while Section V
demonstrates a proof-of-concept implementation. Section VI
offers avenues for future work. Section VII concludes.
II. B ACKGROUND & R ELATED WORK
This section provides a high-level overview of LLMs and
their applications before situating our work with related efforts
in context-aware smart spaces.
Large language models (LLMs) have gained signiï¬cant
attention in recent years due to their impressive performance
on a wide range of natural language processing tasks. In
2018, Devlin proposed BERT, a language representation model
that uses Bidirectional Encoder Representations from Trans-
formers [6] and can be ï¬ne-tuned for a variety of NLPtasks, such as text classiï¬cation and sentiment analysis. In
the same year, OpenAI proposed GPT (Generative Pre-trained
Transformer) [22]. Both models use a transformer architec-
ture [27] that was pre-trained on a massive corpus of text data,
including books, articles, and websites. The resulting models
demonstrate impressive results on a wide range of natural
language processing tasks, including language translation,
text generation, and the ability to translate natural language
descriptions into program implementations.
Following the success of the transformer-based model, sub-
sequent studies have explored ways to improve and expand
the modelâ€™s performance. In 2019, Radford et al., published
an updated version of GPT and called GPT-2 [23]. Building
on the success of GPT-2, Brown et al. released GPT-3 in 2020
[4]. After that, in 2023, GPT-4 was introduced. It is currently
one of the largest and most powerful language models, with
more than 1 trillion parameters [18]. At time of writing, access
to GPT-4 is limitedâ€”we therefore base our study on GPT-3.
Two popular approaches exist for adapting task-agnostic
LLMs to new applications: prompt engineering and ï¬ne-
tuning . Prompt engineering refers to the process of designing
a task-speciï¬c prompt or template that guides the model to
produce relevant outputs for a particular task [29]. These
prompts generally contain instructions to the model written
in natural languageâ€”e.g., â€œexplain the following passage
of textâ€. Fine-tuning, on the other hand, involves directly
training the model on a new task by providing task-speciï¬c
examples [23]. The key advantage of prompt engineering over
ï¬ne-tuning is that it does not require task-speciï¬c dataâ€”
we therefore adopt that approach here. Within the realm of
prompt engineering, there are zero-shot andfew-shot learning
approaches. Zero-shot approaches provide the model with a
single prompt containing instructions and task-speciï¬c infor-
mation; few-shot approaches provide examples to the model
of correct input/output pairs. We focus on zero-shot learning.
Context-aware spaces leverage sensor information, user
data (including past behaviors and preferences), and de-
vice state to inï¬‚uence system actions toward meeting user
needs [2]. The notion of â€œcontext-awarenessâ€ in this sense
has roots in research on ambient intelligence [5]â€”that is, the
development of built environments that sense and adapt to
users. A concrete example of this concept is a home that lever-
ages contextual information to improve energy efï¬ciency [11,
7]. In an early paper, Yamazaki suggested that smart homes
should go beyond automation and instead integrate expressive
interfaces between the user and system [28], a goal that is
partially realized in smart assistants [20], but with limited
ability to adapt to more complex user commands [13]. Ample
prior work has approached the issue of context-awareness us-
ing task-speciï¬c models [21, 12, 17, 14]. While these methods
can achieve high performance given ample task-speciï¬c data,
we believe that the high zero-shot performance of LLMs could
hint at better generalizability without a need for training data.
However, we are aware of no work to-date that has explored
the use of task-agnostic LLMs for deeper contextual reasoning
in smart environments. This motivates our feasibility study.III. S YSTEM DESIGN
In this section, we introduce the system design that we use
to explore the feasibility of LLM-driven smart home control.
We ï¬rst assume the use of an LLM like GPT-3 that provides
responses to user prompts written in natural language. These
LLM models are not task-speciï¬c, rather, they are trained
on an immense amount of cross-domain textual information
and, depending on the structure of the prompt, can provide
responses suited to a variety of different use cases (e.g., writing
a poem, writing code in response to a high-level program
description, etc.). We opt to adapt the modelâ€™s outputs to our
task using zero-shot learning through prompt engineering.
Our challenge is therefore to package relevant context and
user commands into a concise prompt issued to the model,
such that its responses include concrete, machine-parseable
changes to device state that can be passed off to the appropriate
smart device APIs. Qualitatively, we want these courses of
action to be shaped by the model successfully inferring (1) the
intent behind the user command and (2) the manner in which
the state of available devices can be changed to meet the userâ€™s
intent. To that end, we ï¬rst deï¬ne an abstract schema for
capturing smart home context before describing a method for
engineering prompts to conversational LLMs that elicit useful,
actionable responses.
A. Context Schema
In order for the model to â€œknowâ€ what actions are available
to it, we need to package the available devices, their states, and
other relevant informationâ€”i.e., the contextâ€”into a machine-
parseable format. This package effectively describes the action
space available to the model: the knobs it can turn, and
information (e.g., which room the user is in) that might
inï¬‚uence how it turns them. It also provides a hint about
how the model should format its response. Representations
of context can be complex and have been explored in the
literature [9, 1]. Since our goal is to conduct an exploratory
study rather than design an end-to-end solution, we use a
schema that is simple but adequate for our experimental setup.
We choose JSON for structuring this data since it is the de-
facto data interchange format for RESTful APIs used by many
smart home devices [19, 8, 10]. Leveraging a common format
is also advantageous since there is a high likelihood that the
LLM has been trained on source material that contains it,
which beneï¬ts the modelâ€™s ability to converse in it.
At the top level, context is a collection of â€œkey, valueâ€ pairs.
There are two relevant contexts: â€œuserâ€ context that contains
immutable information about the userâ€™s state, e.g., which room
they are in, and â€œdeviceâ€ context, which contains mutable and
immutable information about the devices in the home. Each
top-level key in the collection of device context deï¬nes a room
in the home, and within each room we deï¬ne collections of
devices organized by type, e.g., â€œlightsâ€, â€œtvsâ€, etc. Within
a collection of devices, we deï¬ne each individual device as
a collection of properties about that device, e.g., for a light
we can deï¬ne its â€œstateâ€ property and its â€œrâ€, â€œgâ€, and â€œbâ€
Fig. 1: Data structures for expressing smart home device and
user context in prompts to an LLM.
color values. This overall structure is depicted in Fig. 1 and
illustrated by the example in the following:
{
"user": {
"location": "living_room"
}
}
{
"devices": {
"bedroom": {
"lights": {
"bedside_lamp": {
"state": "off"
}
}
},
"living_room": {
"lights": {
"overhead": {
"state": "on"
},
"lamp": {
"state": "off"
}
},
"tvs": {
"living_room_tv": {
"state": "off",
"volume": 20
}
}
}
}
}
In this example, the userâ€™s home has two roomsâ€”a bedroom
and living roomâ€”and the user is currently located in the living
room. The bedroom has one light turned off, and the living
room has two lights (one turned on) and a television.
B. Prompt Engineering
Having developed a structure for storing context, we now
move to the practical challenge of engineering prompts that
elicit useful responses from the model.Fig. 2: An example prompt and response from ChatGPT,
demonstrating its ability to change device state in response to
ambiguous user commands like â€œget ready for a partyâ€. JSON
is omitted from this ï¬gure in favor of a visual depiction.
Our prompts consist of four segments, as follows:
Framing. This portion of the prompt provides direction to
the conversational agent about its role in the interactionâ€”
it is being asked to make decisions as an AI that controls
a smart home. We open with the phrase â€œYou are an AI
that controls a smart home.â€
Context. This informs the agent of the user context and
devices available in the environment, which scopes the
space of its actions and provides a hint as to the structure
of our desired response. We continue the prompt: â€œHere
is the state of the devices in the home, in JSON format:
fdevices gHere is information about the user: fusergâ€,
where both contexts are formatted as shown earlier.
Command. This portion inserts the user command and
directs the agent to manipulate the state of devices in
response, as follows: â€œThe user issues the command:
fcommand g. Change the device state as appropriate.â€ The
command is written in natural language, as a user might
utter to their smart assistant.
Formatting. We close the prompt by requesting the
response in JSON format so that it can be easily parsed
and input to a relevant smart device API: â€œProvide your
response in JSON format.â€
An example prompt with this structure and the correspond-
ing response from ChatGPT 3.5 are depicted in Figure 2. We
can see that by using the proposed context structure inside the
the prompt, we are able to elicit responses from the model
that contain changes to the underlying JSON that accurately
reï¬‚ect what a userâ€™s intent might be. In essence, GPT-3.5 isable to relate the meaning of â€œpartyâ€ to the devices available,
as well as alter their speciï¬c settings in desirable ways. In the
next section, we use this system design to perform qualitative
analysis of the modelâ€™s responses.
IV. E VALUATION
This section describes the results of our feasibility study
using the experimental setup described in the previous section.
Our evaluations address two high-level questions:
1)How good are the agentâ€™s responses? We measure the
quality of the agentâ€™s responses, in the sense that they
include courses of action that can reasonably be thought
to meet the userâ€™s request and can be easily machine-
parsed and executed.
2)How timely are the agentâ€™s responses? We also
measure the round-trip response latency. This hints at
how feasible a practical system is with respect to user
experience and responsiveness.
To better understand the system from these two perspectives,
we design scenarios of increasing complexity and ambiguity
ofcontext and command . This captures the intuition that
(1) different smart homes can have different complexity of
context, from an apartment with a few smart lights to a large
home with many devices and (2) different user commands can
have different levels of ambiguity, from direct commands like
â€œturn on the lightâ€ to wholly ambiguous statements like â€œI am
tiredâ€. Evaluating agent responses under these circumstances
allows us to identify the failure modes of LLM-driven smart
home control given increasingly challenging prompts with
respect to both the context and the nature of the command.
We use three contexts of increasing complexity, as follows:
Simple: Describes a home with a bedroom and living
room that have one and two lights, respectively, all
initially off. Lights can either be on or off but have no
other state (e.g. color).
Medium: Same as above, but adds red, green, and blue
color state to each of the lights, with expected values in
the range [0, 255].
Complex: Same as above, but adds a TV with on/off
and volume state to the bedroom, as well as a TV and
smart speaker to the living room (each also with on/off
and volume state).
Each of these contexts is expressed in the schema described
in Section III. We combine these contexts with three user
prompts of increasing ambiguity, as follows:
Direct: â€œTurn on the light. â€ This command is simple
since it directly expresses a state change, as well as
a relevant device. Existing home assistants can easily
respond to this type of command.
Indirect: â€œGet ready for a party. â€ This command is more
ambiguous since it expresses a desired state change, but
provides no information about which devices are relevant.
Ambiguous: â€œI am tired. â€ This command is completely
ambiguous since it expresses neither a state change, nor
which devices might be relevant.We run our tests with each possible combination of these
three contexts and commands (9 total), each for 10 trials. We
save the agentâ€™s response for each trial in a human-readable
format, then perform manual rating to measure the quality of
the responses. Our process for rating the quality of responses
is based on a binary label, where each is assigned one of the
following labels based on its quality:
Poor (0): â€œThe changes to the devices do not at all reï¬‚ect
the intent behind the user command, or the response is
malformed/garbled.â€
Good (1): â€œThe changes to the devices are reasonable for
the command. You can imagine someone being satisï¬ed
with the result, even if it is somewhat subjective (e.g.,
based on different personal preferences).â€
Three researchers independently reviewed all responses and
assigned them a label. We report the aggregate score for each
trial as the average across all assigned scores. We also note the
average latency for each trialâ€”this includes both the network
transmission time of the request, as well as the inference time
taken by the model. Since this time is subject to network
conditions and API demand, it should be taken as a rough
estimate rather than a concrete benchmark. Our results are
summarized in Table I.
Response time is a function of context complexity. With
respect to latency, we can see that responses generally arrive
on the order of seconds, meaning that a practical system
could feasibly leverage an LLM for ambiguous command
inference and action planning without signiï¬cant detriments
to user experience or responsiveness. For direct commands, a
2 to 3 second response time may be too longâ€”future system
designs could thus leverage the LLM only for commands that
require it. This may entail a hybrid of rule-based inference
for common commands, along with LLM inference for less
familiar commands. It is also worth noting that as the context
increases in complexity, the response latency also increasesâ€”
this motivates future work to develop methods for ï¬ltering
context prior to prompting the agent so that only the most
relevant information is provided.
Response quality is a function of context and command
ambiguity. With respect to response quality, we ï¬nd that the
LLM approach provides good responses given the same direct
and simple commands that current home assistants are able to
service. Note, however, that unlike existing home assistants,
the LLM approach utilizes a much simpler system architecture
that performs command inference and action planning in
the same pass. These results are consistent given increasing
degrees of context complexity, suggesting that the model was
not overwhelmed by the growth in the decision space that
comes with adding new devices and possible state changes. On
the contrary, the model provides better responses when given
more context that might be relevant to the userâ€™s command,
as is apparent when comparing the low response quality of
the Simple/Indirect and Medium/Indirect experiments against
the Complex/Indirect experiment. Upon inspection of the
responses, the reason for this is clear: given minimal contextand a subjective command like â€œget ready for a partyâ€, the
LLM simply makes up a responseâ€”speciï¬cally, it turns on
all the lights in the house, to include the bedroom. When
we add a speaker and a television to the context, the model
now has more relevant knobs to turn, and produces a higher
quality response. The tendency to make something up when
the answer is unknown or requires more context is an open
problem and motivates the development of application-speciï¬c
methods to mediate between the user and the model.
For the most ambiguous command (â€œI am tiredâ€), we note
that the model delivers poor responses regardless of context.
In all but a few cases, the LLM simply turns on all of the
homeâ€™s lights. The exception is in a Medium/Ambiguous trial,
where it only turns on the bedroom lamp, perhaps to help the
user prepare for bed. This is to be expected: an individualâ€™s
intent and preference in this case are highly subjective (are
they, e.g., tired and ready for bed or tired but they have a
pressing deadline?) and the LLM ultimately cannot read the
userâ€™s mind. However, since the LLM does not â€œknow what it
doesnâ€™t knowâ€, it does not ask for clariï¬cation or go with the
safest choice, which is likely to do nothing. Instead, it makes
something up.
Since our previous results suggested more context is often
beneï¬cial, we dig deeper to see if we can help it make a better
choice. We amend the vague command â€œI am tiredâ€ to offer
hints at the userâ€™s intent:
Ambiguous*: â€œI am tired and I need to work.â€
Ambiguous**: â€œI am tired and I want to sleep.â€
Provided this added hint at the userâ€™s context, the response
quality improves signiï¬cantly. For Ambiguous*, the model
consistently responds by turning on the living room lights
while leaving all other devices off; for Ambiguous**, the
model turns on only the userâ€™s bedside lamp and, in some
cases, reduces the volume on their speaker and TV . Note that
although the amended statement includes additional context,
it still requires the model to infer meaning in a way that more
rigid or rule-based approaches cannot.
V. I MPLEMENTATION
To demonstrate LLM-driven smart home control in prac-
tice, we built a proof-of-concept implementation in Python.
Our implementation accepts user commands as strings, pack-
ages them into prompts along with contextual information
about real devices, then processes responses from OpenAIâ€™s
text-davinci-003 model into smart home API calls that
change the device state as speciï¬ed by the LLM. We scope
the application to one room (a researcherâ€™s living room) with
three Philips Hue color smart lights [19] and one TP-Link
Kasa smart plug [26] that controls a stereo. We store the
device context in JSON as in our experimental setup, with
the difference that the device state for the Hue lights is pulled
directly from the Hue API without modiï¬cation. Our code is
open source and available online.1
1https://github.com/UT-MPC/homegptContext Command Avg. Quality Avg Latency (sec)
Simple Direct 1.00 2.42
Indirect 0.67 2.31
Ambiguous 0.00 2.22
Medium Direct 1.00 4.56
Indirect 0.63 4.70
Ambiguous 0.17 4.97
Complex Direct 1.00 7.90
Indirect 1.00 7.25
Ambiguous 0.00 7.04
Complex Ambiguous* 1.00 7.49
Ambiguous** 1.00 8.09
TABLE I: Results for experiments given various combina-
tions of different context complexity and command ambiguity.
Higher quality responses suggest the model produced a course
of action that would be desirable for an end user (e.g., turning
on the bedroom light when receiving the command â€œI am tired
and I want to sleepâ€). Lower latency suggests better system
responsiveness.
The teaser ï¬gure depicts the result when issuing the com-
mand â€œset up for a partyâ€. We include the JSON context of
the light group2, along with a ï¬eld for the plug powering
the stereo. The model mutates the parameters in the JSON to
change the stereo state to â€œonâ€ and, impressively, also changes
the â€œeffectâ€ parameter of the Hue light group from â€œnoneâ€ to
â€œcolorloopâ€ to create a looping color effect. The latter change
suggests that GPT-3 may have been trained on material about
the speciï¬c features of the Hue API and can leverage that
along with the inferred meaning of the user command to
trigger more intuitive changes than existing systems.
We brieï¬‚y list multiple other commands we tested in our
implementation, along with responses from the model:
â€œmake it bright in hereâ€ â€“ sets lights to full brightness
â€œmake it groovyâ€ â€“ sets lights to color loop; adds invalid
â€œgenreâ€ ï¬eld to stereo and sets it to â€groovyâ€
â€œgotta relaxâ€ â€“ dims lights, turns stereo on
â€œIâ€™m coldâ€ â€“ sets lights to warm white, turns stereo on
â€œIâ€™m leavingâ€ â€“ turns off lights and stereo
â€œIâ€™m homeâ€ â€“ turns on lights and stereo
We note, of course, that these tests are far from exhaustive.
We observed high variability in responses, meaning the same
command can elicit many responses: some good, some bad.
A more robust system design will be necessary to tackle the
inconsistencies present in current LLM model outputs. We
address this in our discussion in the following section.
VI. D ISCUSSION & F UTURE WORK
Our efforts in this paper hint at exciting opportunities for
future work. We suggest several avenues for further research.
Managing contextual information. We found that includ-
ing more context can improve the quality of the modelâ€™s
responses, but at the expense of response latency. To effec-
tively navigate this tradeoff in an end-to-end solution, a more
involved approach for storing, pre-processing, and expressing
2https://developers.meethue.com/develop/hue-api/groupds-api/context will be necessary. This will also become essential
as the amount of context grows to include sensor data, user
preference data, and a growing and more diverse set of
controllable devices. We note that in our experiments, we did
not attempt to test the limits of how much context a model
can receive before the quality or latency of responses degrades
substantially. This should be considered in future work.
Robust system design. While we were able to leverage
a simple system design in this paper, an end-to-end system
will need a more robust design to account for several factors.
First, since LLMs do not yet â€œknow what they donâ€™t knowâ€,
the likelihood of invalid or low-quality responses remains
high. In the case of responses where the model makes invalid
changes to device state (e.g, to add new settings to a device),
a full system should include a way to enforce a set of formal
properties for device states. In the case of unsatisfactory
responses, it would be beneï¬cial to develop a method for
learning user preferences or seeking clarifying information
(e.g., â€œare you tired and want to sleep, or are you tired but
need an energy boost?â€).
From commands to automation. Our primary focus in
this exploratory study was on immediate commandsâ€”the user
makes a request and the model immediately responds with
a state change. Future work could investigate the use of
LLMs for more intuitive automation planning. A user could,
for instance, ask their smart assistant to â€œplay jazz when it
rainsâ€ and the model could leverage contextual information to
put in place an automation sequence that meets their needs.
This would obviate the need for pre-programmed automation
routines and could substantially improve user satisfaction with
smart assistant systems.
VII. C ONCLUSION
In this paper, we explored the feasibility of smarter smart
home control using large language models (LLMs). We pro-
posed a simple system design for capturing smart home con-
text (i.e., information about the user and controllable devices
in the environment) in engineered prompts to GPT-3, showing
that the model has the ability to infer meaning behind indirect
and ambiguous user commands like â€œI am tired and I need to
workâ€ and, in response, generate changes to smart device state.
We implemented our system design, giving GPT-3 control
of real devices and ï¬nding that it is able to quickly and
appropriately control them in response to user commands with
no ï¬ne tuning and no post-processing of its responses. By
simply telling GPT-3 what devices are available and what the
user wants, it can generate courses of action in response.
Our work hints at the capability of GPT-3 and similar
models to go far beyond the current abilities of smart space
control and motivates future work with context modeling, end-
to-end system design, and approaches for further leveraging
GPT-3â€™s capabilites to develop complex automation routines
in response to user commands.REFERENCES
[1] Hamim Md Adal et al. â€œThe Space Broker: A Middleware for
Mediating Interactions in Smart IoT Spacesâ€. In: Proceedings of the
8th ACM International Conference on Systems for Energy-Efï¬cient
Buildings, Cities, and Transportation . BuildSys â€™21. Coimbra, Portu-
gal: Association for Computing Machinery, 2021, pp. 101â€“110. ISBN :
9781450391146. DOI: 10.1145/3486611.3486664. URL: https://doi.
org/10.1145/3486611.3486664.
[2] Muhammad Raisul Alam, Mamun Bin Ibne Reaz, and Mohd Alauddin
Mohd Ali. â€œA Review of Smart Homesâ€”Past, Present, and Futureâ€.
In:IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews) 42.6 (2012), pp. 1190â€“1203. DOI: 10.1109/
TSMCC.2012.2189204.
[3] Tom Brown et al. â€œLanguage models are few-shot learnersâ€. In: Ad-
vances in neural information processing systems 33 (2020), pp. 1877â€“
1901.
[4] Tom B. Brown et al. â€œLanguage Models are Few-Shot Learnersâ€. In:
CoRR abs/2005.14165 (2020). arXiv: 2005.14165. URL: https://arxiv.
org/abs/2005.14165.
[5] Diane J Cook, Juan C Augusto, and Vikramaditya R Jakkula. â€œAm-
bient intelligence: Technologies, applications, and opportunitiesâ€. In:
Pervasive and mobile computing 5.4 (2009), pp. 277â€“298.
[6] Jacob Devlin et al. â€œBert: Pre-training of deep bidirectional transform-
ers for language understandingâ€. In: arXiv preprint arXiv:1810.04805
(2018).
[7] PR Geraldo Filho et al. â€œEnergy-efï¬cient smart home systems: Infras-
tructure and decision-making processâ€. In: Internet of Things 5 (2019),
pp. 153â€“167.
[8] Google Nest API Documentation . 2022. URL: https : / / developers .
google.com/nest/device-access.
[9] Jie Hua et al. â€œCoPI: Enabling Probabilistic Conï¬‚ict Prediction
in Smart Space Through Context-awarenessâ€. In: 2022 IEEE/ACM
Seventh International Conference on Internet-of-Things Design and
Implementation (IoTDI) . IEEE. 2022, pp. 30â€“42.
[10] Insteon REST API Documentation . 2022. URL: https://www.insteon.
com/developer.
[11] Marco Jahn et al. â€œThe energy aware smart homeâ€. In: 2010 5th
international conference on future information technology . IEEE. 2010,
pp. 1â€“8.
[12] M Humayun Kabir et al. â€œMachine learning based adaptive context-
aware system for smart home environmentâ€. In: International Journal
of Smart Home 9.11 (2015), pp. 55â€“62.
[13] Julia Kiseleva et al. â€œUnderstanding user satisfaction with intelligent
assistantsâ€. In: Proceedings of the 2016 ACM on Conference on Human
Information Interaction and Retrieval . 2016, pp. 121â€“130.
[14] Tiankai Liang et al. â€œAn unsupervised user behavior prediction algo-
rithm based on machine learning and neural network for smart homeâ€.
In:IEEE Access 6 (2018), pp. 49237â€“49247.
[15] Xiao Liu et al. â€œGPT understands, tooâ€. In: arXiv preprint
arXiv:2103.10385 (2021).
[16] Irene Lopatovska et al. â€œTalk to me: Exploring user interactions with
the Amazon Alexaâ€. In: Journal of Librarianship and Information
Science 51.4 (2019), pp. 984â€“997.
[17] Isaac Machorro-Cano et al. â€œHEMS-IoT: A big data and machine
learning-based smart home system for energy savingâ€. In: Energies
13.5 (2020), p. 1097.
[18] OpenAI. GPT-4 .URL: https://openai.com/product/gpt-4.
[19] Philips Hue API Documentation . 2022. URL: https : / / developers .
meethue.com/.
[20] Amanda Purington et al. â€œâ€ Alexa is my new BFFâ€ Social Roles, User
Satisfaction, and Personiï¬cation of the Amazon Echoâ€. In: Proceedings
of the 2017 CHI conference extended abstracts on human factors in
computing systems . 2017, pp. 2853â€“2859.
[21] Basheer Qolomany et al. â€œLeveraging machine learning and big data
for smart buildings: A comprehensive surveyâ€. In: IEEE Access 7
(2019), pp. 90316â€“90356.
[22] Alec Radford et al. â€œImproving language understanding by generative
pre-trainingâ€. In: (2018).
[23] Alec Radford et al. â€œLanguage models are unsupervised multitask
learnersâ€. In: OpenAI blog 1.8 (2019), p. 9.
[24] Anirudh Raju et al. â€œScalable multi corpora neural language models
for asrâ€. In: arXiv preprint arXiv:1907.01677 (2019).[25] Prakhar Swarup et al. â€œImproving ASR conï¬dence scores for Alexa
using acoustic and hypothesis embeddingsâ€. In: (2019).
[26] TP-Link Kasa Smart Plugs . 2023. URL: https://www.kasasmart.com/
us/products/smart-plugs.
[27] Ashish Vaswani et al. â€œAttention is all you needâ€. In: Advances in
neural information processing systems 30 (2017).
[28] Tatsuya Yamazaki. â€œBeyond the Smart Homeâ€. In: 2006 International
Conference on Hybrid Information Technology . V ol. 2. 2006, pp. 350â€“
355. DOI: 10.1109/ICHIT.2006.253633.
[29] Tony Z. Zhao et al. Calibrate Before Use: Improving Few-Shot Per-
formance of Language Models . 2021. arXiv: 2102.09690 [cs.CL] .