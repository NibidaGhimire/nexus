Single-subject Multi-contrast MRI
Super-resolution via Implicit Neural
Representations
Julian McGinnis⋆1,2,3, Suprosanna Shit⋆1,4,5, Hongwei Bran Li5,
Vasiliki Sideri-Lampretsa1, Robert Graf1,4, Maik Dannecker1,
Jiazhen Pan1, Nil Stolt-Ansó1, Mark Mühlau2,3, Jan S. Kirschke4,
Daniel Rueckert1,6, and Benedikt Wiestler4
1School of Computation, Information and Technology, TU Munich, Germany
2TUM-Neuroimaging Center, TU Munich, Germany
3Department of Neurology, TU Munich, Germany
4Department of Neuroradiology, TU Munich, Germany
5Department of Quantitative Biomedicine, University of Zurich, Switzerland
6Department of Computing, Imperial College London, United Kingdom
{julian.mcginnis,suprosanna.shit}@tum.de
Abstract. Clinical routine and retrospective cohorts commonly include
multi-parametricMagneticResonanceImaging;however,theyaremostly
acquired in different anisotropic 2D views due to signal-to-noise-ratio
and scan-time constraints. Thus acquired views suffer from poor out-of-
plane resolution and affect downstream volumetric image analysis that
typically requires isotropic 3D scans. Combining different views of multi-
contrast scans into high-resolution isotropic 3D scans is challenging due
to the lack of a large training cohort, which calls for a subject-specific
framework. This work proposes a novel solution to this problem lever-
aging Implicit Neural Representations (INR). Our proposed INR jointly
learns two different contrasts of complementary views in a continuous
spatial function and benefits from exchanging anatomical information
between them. Trained within minutes on a single commodity GPU, our
model provides realistic super-resolution across different pairs of con-
trasts in our experiments with three datasets. Using Mutual Information
(MI) as a metric, we find that our model converges to an optimum MI
amongst sequences, achieving anatomically faithful reconstruction.7
Keywords: Multi-contrastSuper-resolution ·ImplicitNeuralRepresen-
tations ·Mutual Information.
1 Introduction
In clinical practice, Magnetic Resonance Imaging (MRI) provides important in-
formationfordiagnosingandmonitoringpatientconditions[17,5].Tocapturethe
⋆equal contribution
7Code is available at: https://github.com/jqmcginnis/multi_contrast_inr/arXiv:2303.15065v3  [eess.IV]  5 Jan 20242 McGinnis and Shit et al.
complex pathophysiological aspects during disease progression, multi-parametric
MRI (such as T1w, T2w, DIR, FLAIR) is routinely acquired. Image acquisition
inherently poses a trade-off between scan time, resolution, and signal-to-noise
ratio (SNR) [20]. To maximize the source of information within a reasonable
time budget, clinical protocol often combines anisotropic 2D scans of different
contrastsincomplementaryviewingdirections.Althoughacquired2Dscansoffer
an excellent in-plane resolution, they lack important details in the orthogonal
out-of-plane. For a reliable pathological assessment, radiologists often resort to a
second scan of a different contrast in the orthogonal viewing direction. Further-
more, poor out-of-plane resolution significantly affects the accuracy of volumet-
ric downstream image analysis, such as radiomics and lesion volume estimation,
which usually require isotropic 3D scans. As multi-parametric isotropic 3D scans
are not always feasible to acquire due to time-constraints [20], motion [10], and
patient’s condition [11], super-resolution offers a convenient alternative to obtain
the same from anisotropic 2D scans. Recently, it has been shown that acquir-
ing three complementary 2D views of the same contrast may yield higher SNR
at reduced scan time [20,30]. However, it remains under-explored if orthogonal
anisotropic 2D views of different contrasts can benefit from each other based on
the underlying anatomical consistency. Additionally, whether such strategies can
further decrease scan times while preserving similar resolution and SNR remains
unanswered. Moreover, unlike conventional super-resolution models trained on
a cohort, a personalized model is of clinical relevance to avoid the danger of
potential misdiagnosis caused by cohort-learned biases. In this work, we miti-
gate these gaps by proposing a novel multi-contrast super-resolution framework
that only requires the patient-specific low-resolution MR scans of different se-
quences (and views) as supervision. As shown in various settings, our approach
is not limited to specific contrasts or views but provides a generic framework for
super-resolution. The contributions in this paper are three-fold:
1. To the best of our knowledge, our work is the first to enable subject-specific
multi-contrast super-resolution from low-resolution scans without needing
any high-resolution training data. We demonstrate that Implicit Neural Rep-
resentations (INR) are good candidates to learn from complementary views
of multi-parametric sequences and can efficiently fuse low-resolution images
into anatomically faithful super-resolution.
2. We introduce Mutual Information (MI) [27] as an evaluation metric and find
that our method preserves the MI between high-resolution ground truths in
its predictions. Further observation of its convergence to the ground truth
value during training motivates us to use MI as an early stopping criterion.
3. We extensively evaluate our method on multiple brain MRI datasets and
show that it achieves high visual quality for different contrasts and views
and preserves pathological details, highlighting its potential clinical usage.
Related Work. Single-image super-resolution (SISR) [2] aims at restoring a
high-resolution (HR) image from a low-resolution (LR) input from a single se-
quenceandtargetsapplicationssuchaslow-fieldMRupsamplingoroptimizationSingle-subject Multi-contrast MRI Super-resolution via INRs 3
of MRI acquisition [4]. Recent methods [4,9] incorporate priors learned from a
training set [4], which is later combined with generative models [3]. On the other
hand, multi-image super-resolution (MISR) relies on the information from com-
plementaryviewsofthesamesequence[30]andisespeciallyrelevanttocapturing
temporal redundancy in motion-corrupted low-resolution MRI [10,28].
Multi-contrast Super-resolution (MCSR) targets using inter-contrast priors
[21]. In conventional settings [16], an isotropic HR image of another contrast
is used to guide the reconstruction of an anisotropic LR image. Zeng et al.
[31] use a two-stage architecture for both SISR and MCSR. Utilizing a feature
extraction network, Lyu et al. [15] learn multi-contrast information in a joint
feature space. Later, multi-stage integration networks [7], separatable attention
[8] and transformers [14] have been used to enhance joint feature space learning.
However, all current MCSR approaches are limited by their need for a large
training dataset. Consequently, this constrains their usage to specific resolutions
andfurtherharborsthedangerofhallucinationoffeatures(e.g.,lesions,artifacts)
present in the training set and does not generalize well to unseen data.
Originating from shape reconstruction [19] and multi-view scene representa-
tions [18], Implicit Neural Representations (INR) have achieved state-of-the-art
results by modeling a continuous function on a space from discrete measure-
ments. Key reasons behind INR’s success can be attributed to overcoming the
low-frequency bias of Multi-Layer Perceptrons (MLP) [26,25,22]. Although MRI
isadiscretemeasurement,theunderlyinganatomyisacontinuousspace.Wefind
INR to be a good fit to model a continuous intensity function on the anatomical
space. Once learned, it can be sampled at an arbitrary resolution to obtain the
super-resolved MRI. Following this spirit, INRs have recently been successfully
employed in medical imaging applications ranging from k-space reconstruction
[12] to SISR [30]. Unlike [30,23], which learn anatomical priors in single con-
trasts, and [29,1], which leverage INR with latent embeddings learned over a
cohort, we focus on employing INR in subject-specific, multi-contrast settings.
2 Methods
In this section, we first formally introduce the problem of joint super-resolution
of multi-contrast MRI from only one image per contrast per patient. Next, we
describe strategies for embedding information from two contrasts in a shared
space.Subsequently,wedetailourmodelarchitectureandtrainingconfiguration.
Problem Statement. We denote the collection of all 3D coordinates of interest
in this anatomical space as Ω={(x, y, z )}with anatomical function q:Ω→
A. The image intensities are a function of the underlying anatomical proper-
tiesA. Two contrasts C1andC2can be scanned in a low-resolution subspace
Ω1, Ω2⊂Ω. Let us consider g1, g2:A→Rthat map from anatomical properties
to contrast intensities C1andC2, respectively. We obtain sparse observations
I1={g1(q(x)) = f1(x);∀x∈Ω1}andI2={g2(q(x)) = f2(x);∀x∈Ω2},
where fiis composition of giandq. However, one can easily obtain the global4 McGinnis and Shit et al.
MSE  LossMutual
Information
LR Contrast 1In-plane In-plane Out-of-plane Out-of-plane
a) LR MRI Acquisition/Retrospective Cohort
c) Inference (Joint Multi-Contrast SR)b) Training (from LR Images Only)
Fourier Features
Fourier FeaturesMSE  Loss
LR Contrast 2LR Contrast 1 LR Contrast 2
Epoch
SR Contrast 1 SR Contrast 2Optimum
Pred Contrast 2 Pred Contrast 1
Fig.1.Overview of our proposed approach (best viewed in full screen). a) Given a
realistic clinical scenario, two MRI contrasts are acquired in complementary 2D views.
b) Our proposed INR models both contrast from the supervision available in the 2D
scans and, by doing so, learn to transfer knowledge from in-plane measurements to
out-of-plane of the other contrast. Although our model is trained on MSELoss only for
the observed coordinates, it constructs a continuous function space, converging to an
optimum state of mutual information between the contrasts on the global space of Ω.
c) Once learned, we can sample an isotropic grid and obtain the anatomically faithful
and pathology-preserving super-resolution.
anatomical space Ωby knowing Ω1andΩ2, e.g., by rigid registration between
the two images. In this paper, we aim to estimate f1, f2:Ω→Rgiven I1andI2.
Joint Multi-contrast Modelling. Since both component-functions f1andf2
operate on a subset of the same input space, we argue that it is beneficial to
model them jointly as a single function f:Ω→R2and optimize it based on
their estimation error incurred in their respective subsets. This will enable infor-
mation transfer from one contrast to another, thus improving the estimation and
preventing over-fitting in single contrasts, bringing consistency to the prediction.
To this end, we propose to leverage INR to model a continuous multi-contrast
function ffrom discretely sampled sparse observations I1andI2.Single-subject Multi-contrast MRI Super-resolution via INRs 5
MCSR Setup. Without loss of generalization, let us consider two LR input
contrasts scanned in two orthogonal planes p1andp2, where p1, p2∈{axial,
sagittal, coronal}. We assume they are aligned by rigid registration requiring no
coordinate transformation. Their corresponding in-plane resolutions are (s1×s1)
and(s2×s2)and slice thickness is t1andt2, respectively. Note that s1< t1and
s2< t2imply high in-plane and low out-of-plane resolution. In the end, we aim
to sample an isotropic (s×s×s)grid for both contrasts where s≤s1, s2.
Implicit Neural Representations for MCSR. We intend to project the in-
formationavailableinonecontrastintoanotherbyembeddingbothintheshared
weight space of a neural network. However, a high degree of weight sharing could
hinder contrast-specific feature learning. Based on this reasoning, we aim to hit
thesweetspotwheremaximuminformationexchangecanbeencouragedwithout
impeding contrast-specific expressiveness. We propose a split-head architecture,
as shown in Fig. 1, where the initial layers jointly learn the common anatomical
features, and subsequently, two heads specialize in contrast-specific information.
The model takes Fourier [26] Features v= [cos(2πBx), sin(2πBx)]Tas input
and predicts [ˆI1,ˆI2] =f(v), where x= (x, y, z )andBis sampled from a Gaus-
sian distribution N(µ, σ2). We use mean-squared error loss, LMSE, for training.
LMSE =αX
x∈Ω1(ˆI1(x)−I1(x))2+βX
x∈Ω2(ˆI2(x)−I2(x))2(1)
where αandβare coefficients for the reconstruction loss of two contrasts. Note
that for points {(x, y, z )} ∈Ω2\Ω1, there is no explicit supervision coming from
low resolution C1. For these points, one can interpret learning C1from the loss
inC2, and vice versa, to be a weakly supervised task.
Implementation and Training. Given the rigidly registered LR images, we
compute Ω1,Ω2∈Ωin the scanner reference space using their affine matrices.
Subsequently, we normalize Ωto the interval [−1,1]3and independently normal-
ize each contrast’s intensities to [0,1]. We use 512-dimensional Fourier Features
in the input. Our model consists of a four-layer MLP with a hidden dimension
of 1024 for the shared layers and two layers with a hidden dimension of 512 for
the heads. We use Adam optimizer with a learning rate of 4e-4 and a Cosine
annealing rate scheduler with a batch size of 1000. For the multi-contrast INR
models, we use MI as in Eq. 2 for early stopping. Implemented in PyTorch, we
train our model on a single A6000 GPU. Please refer to Tab. 3 in supplementary
for an exhaustive hyper-parameter search.
Model Selection and Inference. Since our model is trained on sparse sets of
coordinates, it is prone to overfitting them and has little incentive to generalize
in out-of-plane predictions for single contrast settings. A remedy to this is to
hold random points as a validation set. However, this will reduce the number of
training samples and hinder the reconstruction of fine details. For multi-contrast
settings, one can exploit the agreement between the two predicted contrasts.6 McGinnis and Shit et al.
Ideally, the network should reach an equilibrium between the contrasts over
the training period, where both contrasts optimally benefit from each other.
We empirically show that Mutual Information (MI) [27] is a good candidate to
capture such an equilibrium point without the need for ground truth data in its
computation. For two predicted contrasts ˆI1andˆI2, MI can be expressed as:
MI(ˆI1,ˆI2) =X
y∈ˆI2X
x∈ˆI1P(ˆI1,ˆI2)(x, y) log 
P(ˆI1,ˆI2)(x, y)
PˆI1(x)PˆI2(y)!
(2)
Compared to image registration, we do not use MI as a loss for aligning two
images; instead, we use it as a quantitative assessment metric. Given two ground
truth HR images for a subject, one can compute the optimum state of MI. We
observe that the MI between our model predictions converges close to such an
optimum state over the training period without any explicit knowledge about
it, c.f. Fig. 3 in the supplementary. This observation motivates us to detect a
plateau in MI between the predicted contrasts and use it as a stopping criterion
for model selection in multi-contrast INR.
3 Experiments and Results
Datasets. To enable fair evaluation between our predictions and the reference
HR ground truths, the in-plane SNR between the LR input scan and corre-
sponding ground truth has to match. To synthetically create 2D LR images, it is
necessary to downsample out-of-plane in the image domain anisotropically [33]
while preserving in-plane resolution. Consequently, to mimic realistic 2D clinical
protocol, which often has higher in-plane details than that of 3D scans, we use
splineinterpolationtomodelpartialvolumeanddownsampling.Wedemonstrate
our network’s modeling capabilities for different contrasts (T1w, T2w, FLAIR,
DIR), views (axial, coronal, sagittal), and pathologies (MS, brain tumor). We
conduct experiments on two public datasets, BraTS [17], and MSSEG [5], and an
in-house clinical MS dataset (cMS). In each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth HR scans. Note
that we only use the ground truth HR for evaluation, not anywhere in training.
We optimize separate INRs for each subject with supervision from only its two
LR scans. If required, we employ skull-stripping [13] and rigid registration to
the MNI152 (MSSEG, cMS) or SRI24 (BraTS) templates. For details, we refer
to Table 2 in the supplementary.
Metrics. We evaluate our results by employing common SR [6,15,30] quality
metrics, namely PSNR and SSIM. To showcase perceptual image quality, we
additionally compute the Learned Perceptual Image Patch Similarity (LPIPS)
[32] and measure the absolute error ϵMIin mutual information of two upsampled
images to their ground truth counterparts as follows:
εCi
MI=1
NNX
k=1|MI(ˆIk
i, Ik
j)−MI(Ik
i, Ik
j)|; ˆεMI=1
NNX
k=1|MI(ˆIk
1,ˆIk
2)−MI(Ik
1, Ik
2)|Single-subject Multi-contrast MRI Super-resolution via INRs 7
Table 1. Quantitative results for MCSR on two public and one in-house datasets. All
metrics consistently show that our split-head INR performs the best for MCSR.
BraTS 2019Contrasts T1w T2w T1w & T2w
Methods PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓εT1
MI↓εT2
MI↓ˆεMI↓
Cubic Spline 21.201 0.896 0.09826.201 0.932 0.0580.0960.0870.145
LRTV 21.328 0.919 0.05224.206 0.915 0.0530.1260.1270.203
SMORE 26.266 0.942 0.03028.466 0.942 0.0300.1570.1270.225
Single Contrast INR 26.168 0.952 0.03629.920 0.957 0.0280.0510.0300.079
Our vanilla INR 26.196 0.960 0.03229.777 0.962 0.0260.0080.0150.065
Our split-head INR 28.7460.9650.02831.8020.9660.0240.0070.0140.062MSSEG 2016Contrasts T1w Flair T1w & Flair
Methods PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓εT1
MI↓εFlair
MI↓ˆεMI↓
Cubic Spline 30.102 0.953 0.05128.724 0.945 0.0540.0620.0870.115
LRTV 22.848 0.860 0.05023.920 0.872 0.0440.0680.0520.095
SMORE 25.729 0.937 0.03027.430 0.940 0.0290.1380.1000.183
Single Contrast INR 30.852 0.956 0.02931.156 0.955 0.0300.0470.0740.095
Our vanilla INR 31.599 0.9660.01932.312 0.969 0.0190.008 0.0250.024
Our split-head INR 31.7690.9670.01932.5140.9700.0180.0080.0230.024cMSContrasts DIR Flair DIR & Flair
Methods PSNR ↑SSIM↑LPIPS ↓PSNR ↑SSIM↑LPIPS ↓εDIR
MI↓εFlair
MI↓ˆεMI↓
Cubic Spline 28.106 0.929 0.06526.545 0.923 0.0790.0830.0960.136
LRTV 28.725 0.904 0.03322.766 0.835 0.0570.2690.0880.312
SMORE 28.933 0.926 0.04025.336 0.921 0.0390.1240.0790.139
Single Contrast INR 29.941 0.937 0.03728.655 0.936 0.0410.0630.0720.096
Our vanilla INR 30.8160.956 0.02429.749 0.950 0.0290.0220.0330.009
Our split-head INR 31.6860.9560.02330.2460.9520.0280.0210.0330.009
Baselines and Ablation. To the best of our knowledge, there are no prior
data-driven methods that can perform MCSR on a single-subject basis. Hence,
we provide single-subject baselines that operate solely on single contrast and
demonstrate the benefit of information transfer from other contrasts with our
proposed models. In addition, we show ablations of our proposed split head
model compared to our vanilla INR. Precisely, our experiments include:
Baseline 1 :Cubic-spline interpolation is applied on each contrast separately.
Baseline 2 :LRTV [24] applied on each contrast separately.
Baseline 3 :SMORE (v3.1.2) [33] applied on each contrast separately.
Baseline 4 :Two single-contrast INRs with one output channel each.
Our vanilla INR (ablation) : Single INR with two output channels that
jointly predicts the two contrast intensities.
Our proposed split-head INR : Single INR with two separate heads that
jointly predicts the two contrast intensities (cf. Fig. 1).
Quantitative Analysis. Table 1 demonstrates that our proposed framework
poses a trustworthy candidate for the task of MCSR. As observed in [33], LRTV
struggles for anisotropic up-sampling while SMORE’s overall performance is bet-
ter than cubic-spline, but slightly worse to single-contrast INR. However, the8 McGinnis and Shit et al.
Spline
 Single-contrast INR
 Split-head INR
 HR GT
Fig.2.Qualitative results for MCSR for cMS. The predictions of the split-head INR
demonstratethetransferofanatomicalandlesionknowledgefromcomplementingviews
and sequences. Yellow boxes highlight details recovered by the split-head INR in the
out-of-plane reconstructions, where others struggle.
benefit of single-contrast INR may be limited if not complemented by additional
views as in [30]. For MCSRfrom single-subject scans, we achieve encouraging
results across all metrics for all datasets, contrasts, and views. Since T1w and
T2w both encode anatomical structures, the consistent improvement in BraTS
for both sequences serves as a proof-of-concept for our approach. As FLAIR is
the go-to-sequence for MS lesions, and T1w does not encode such information,
the results are in line with the expectation that there could be a relatively higher
transfer of anatomical information to pathologically more relevant FLAIR than
vice-versa. Lastly, given their similar physical acquisition and lesion sensitivity,
we note that DIR/FLAIR benefit to the same degree in the cMS dataset.
Qualitative Analysis. Fig. 2 shows the typical behavior of our models on cMS
dataset, where one can qualitatively observe that the split-head INR preserves
the lesions and anatomical structures shown in the yellow boxes, which other
models fail to capture. While our reconstruction is not identical to the GT HR,
the coronal view confirms anatomically faithful reconstructions despite not re-
ceiving any in-plane supervision from any contrast during training. We refer to
Fig. 4 in the supplementary for similar observations on BraTS and MSSEG.Single-subject Multi-contrast MRI Super-resolution via INRs 9
4 Discussion and Conclusion
Given the importance and abundance of large multi-parametric retrospective co-
horts [17,5], our proposed approach will allow the upscaling of LR scans with the
help of other sequences. Deployment of such a model in clinical routine would
likely reduce acquisition time for multi-parametric MRI protocols maintaining
an acceptable level of image fidelity. Importantly, our model exhibits trustwor-
thiness in its clinical applicability being 1) subject-specific, and 2) as its gain
in information via super-resolution is validated by MI preservation and is not
prone to hallucinations that often occur in a typical generative model.
In conclusion, we propose the first subject-specific deep learning solution for
isotropic3Dsuper-resolutionfromanisotropic2Dscansoftwodifferentcontrasts
of complementary views. Our experiments provide evidence of inter-contrast in-
formation transfer with the help of INR. Given the supervision of only single
subject data and trained within minutes on a single GPU, we believe our frame-
work to be potentially suited for broad clinical applications. Future research will
focus on prospectively acquired data, including other anatomies.
Acknowledgement
JM, MM and JSK are supported by Bavarian State Ministry for Science and Art
(Collaborative Bilateral Research Program Bavaria – Québec: AI in medicine,
grant F.4-V0134.K5.1/86/34). SS, RG and JSK are supported by European
Research Council (ERC) under the European Union’s Horizon 2020 research
and innovation program (101045128-iBack-epic-ERC2021-COG). MD and DR
are supported by ERC (Deep4MI - 884622) and ERA-NET NEURON Cofund
(MULTI-FACT - 8810003808). HBL is supported by an Nvidia GPU grant.
References
1. Amiranashvili, T., Lüdke, D., Li, H.B., Menze, B., Zachow, S.: Learning shape
reconstructionfromsparsemeasurementswithneuralimplicitfunctions.In:MIDL.
pp. 22–34. PMLR (2022)
2. Bhowmik, A., Shit, S., Seelamantula, C.S.: Training-free, single-image super-
resolution using a dynamic convolutional network. IEEE signal processing letters
25(1), 85–89 (2017)
3. Chen, Y., Shi, F., Christodoulou, A.G., Xie, Y., Zhou, Z., Li, D.: Efficient and
accurateMRIsuper-resolutionusingagenerativeadversarialnetworkand3dmulti-
level densely connected network. In: MICCAI. pp. 91–99. Springer (2018)
4. Chen, Y., Xie, Y., Zhou, Z., Shi, F., Christodoulou, A.G., Li, D.: Brain mri super
resolution using 3d deep densely connected neural networks. In: ISBI. pp. 739–742.
IEEE (2018)
5. Commowick, O., Cervenansky, F., Ameli, R.: MSSEG challenge proceedings: mul-
tiple sclerosis lesions segmentation challenge using a data management and pro-
cessing infrastructure. In: MICCAI (2016)
6. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convo-
lutional networks. IEEE TPAMI 38(2), 295–307 (2015)10 McGinnis and Shit et al.
7. Feng, C.M., Fu, H., Yuan, S., Xu, Y.: Multi-contrast MRI super-resolution via a
multi-stage integration network. In: MICCAI. pp. 140–149. Springer (2021)
8. Feng,C.M.,Yan,Y.,Yu,K.,Xu,Y.,Shao,L.,Fu,H.:Exploringseparableattention
for multi-contrast MR image super-resolution. arXiv preprint arXiv:2109.01664
(2021)
9. Georgescu, M.I., Ionescu, R.T., Verga, N.: Convolutional neural networks with
intermediatelossfor3dsuper-resolutionofctandmriscans.IEEEAccess 8,49112–
49124 (2020)
10. Gholipour, A., Estroff, J.A., Warfield, S.K.: Robust super-resolution volume re-
construction from slice acquisitions: application to fetal brain MRI. IEEE TMI
29(10), 1739–1758 (2010)
11. Ha, J.Y., Baek, H.J., Ryu, K.H., Choi, B.H., Moon, J.I., Park, S.E., Kim, T.B.:
One-minute ultrafast brain MRI with full basic sequences: can it be a promising
way forward for pediatric neuroimaging? AJR 215(1), 198–205 (2020)
12. Huang, W., Li, H.B., Pan, J., Cruz, G., Rueckert, D., Hammernik, K.: Neural
implicit k-space for binning-free non-cartesian cardiac MR imaging. In: IIPMI. pp.
548–560. Springer (2023)
13. Isensee, F., Schell, M., Pflueger, I., Brugnara, G., Bonekamp, D., Neuberger, U.,
Wick, A., Schlemmer, H.P., Heiland, S., Wick, W., et al.: Automated brain extrac-
tion of multisequence MRI using artificial neural networks. Human brain mapping
40(17), 4952–4964 (2019)
14. Li,G.,Lv,J.,Tian,Y.,Dou,Q.,Wang,C.,Xu,C.,Qin,J.:Transformer-empowered
multi-scale contextual matching and aggregation for multi-contrast MRI super-
resolution. In: CVPR. pp. 20636–20645. IEEE (2022)
15. Lyu, Q., Shan, H., Steber, C., Helis, C., Whitlow, C., Chan, M., Wang, G.: Multi-
contrast super-resolution MRI through a progressive network. IEEE TMI 39(9),
2738–2749 (2020)
16. Manjón, J.V., Coupé, P., Buades, A., Collins, D.L., Robles, M.: MRI superresolu-
tion using self-similarity and image priors. J. Biomed. Imaging 2010, 1–11 (2010)
17. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,
Burren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor
image segmentation benchmark (brats). IEEE TMI 34(10), 1993–2024 (2014)
18. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: NeRF: Representing scenes as neural radiance fields for view synthesis. In:
ECCV 2020. pp. 405–421. Springer (2020)
19. Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: DeepSDF: Learn-
ing continuous signed distance functions for shape representation. In: CVPR. pp.
165–174. IEEE (2019)
20. Plenge, E., Poot, D.H., Bernsen, M., Kotek, G., Houston, G., Wielopolski, P.,
van der Weerd, L., Niessen, W.J., Meijering, E.: Super-resolution methods in MRI:
can they improve the trade-off between resolution, signal-to-noise ratio, and acqui-
sition time? Magnetic Resonance in Medicine 68(6), 1983–1993 (2012)
21. Rousseau, F., Initiative, A.D.N., et al.: A non-local approach for image super-
resolution using intermodality priors. Med. Image Anal. 14(4), 594–605 (2010)
22. Saragadam, V., LeJeune, D., Tan, J., Balakrishnan, G., Veeraraghavan, A., Bara-
niuk, R.G.: WIRE: Wavelet implicit neural representations. In: CVPR. pp. 18507–
18516. IEEE (2023)
23. Shen, L., Pauly, J., Xing, L.: NeRP: implicit neural representation learning with
prior embedding for sparsely sampled image reconstruction. IEEE TNNLS (2022)Single-subject Multi-contrast MRI Super-resolution via INRs 11
24. Shi, F., Cheng, J., Wang, L., Yap, P.T., Shen, D.: LRTV: MR image super-
resolution with low-rank and total variation regularizations. IEEE TMI 34(12),
2459–2466 (2015)
25. Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neu-
ral representations with periodic activation functions. In: NeurIPS. pp. 7462–7473
(2020)
26. Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Sing-
hal, U., Ramamoorthi, R., Barron, J., Ng, R.: Fourier features let networks learn
high frequency functions in low dimensional domains. In: NeurIPS. pp. 7537–7547
(2020)
27. Wells III, W.M., Viola, P., Atsumi, H., Nakajima, S., Kikinis, R.: Multi-modal
volume registration by maximization of mutual information. Med. Image Anal.
1(1), 35–51 (1996)
28. Wesarg,S.,etal.:Combiningshort-axisandlong-axiscardiacMRimagesbyapply-
ing a super-resolution reconstruction algorithm. In: Medical Imaging 2010: Image
Processing. vol. 7623, pp. 187–198. SPIE (2010)
29. Wu, Q., Li, Y., Sun, Y., Zhou, Y., Wei, H., Yu, J., Zhang, Y.: An arbitrary scale
super-resolution approach for 3d mr images via implicit neural representation.
IEEE JBHI 27(2), 1004–1015 (2023)
30. Wu, Q., Li, Y., Xu, L., Feng, R., Wei, H., Yang, Q., Yu, B., Liu, X., Yu, J., Zhang,
Y.: IREM: high-resolution magnetic resonance image reconstruction via implicit
neural representation. In: MICCAI. pp. 65–74. Springer (2021)
31. Zeng, K., Zheng, H., Cai, C., Yang, Y., Zhang, K., Chen, Z.: Simultaneous single-
and multi-contrast super-resolution for brain MRI images based on a convolutional
neural network. Computers in biology and medicine 99, 133–141 (2018)
32. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR. pp. 586–595. IEEE
(2018)
33. Zhao, C., Dewey, B.E., Pham, D.L., Calabresi, P.A., Reich, D.S., Prince, J.L.:
SMORE: a self-supervised anti-aliasing and super-resolution algorithm for MRI
using deep learning. IEEE TMI 40(3), 805–817 (2020)12 McGinnis and Shit et al.
Table 2. Summary of dataset statistics used in our experiments.
DatasetContrast-1 Contrast-2
NameInplane Dim Res. (mm) NameInplane Dim Res. (mm)
BraTS 2019 T1wAxial(192,192,40) 1×1×4T2wCoronal(192,48,160) 1×4×1
MSSEG 2016 T1wAxial(160,224,40) 1×1×4FlairSagittal(40,224,160) 4×1×1
cMS DIRAxial(160,224,40) 1×1×4FlairSagittal(40,224,160) 4×1×1
Table 3. To estimate the correct hyperparameters, we perform linear and grid searches
on a hold-out set of subjects across all datasets. We list the sweeped hyperparameter
ranges and the configurations for the final experiments.
Hyperparameter Sweep Range Final
Fourier Features, Distribution Scale [3.5, 5.0, step=0.1] 4.0
Fourier Features, Scaling Factor [0.5, 1.5, step=0.1] & [1.0, 10, step=1.0] 1.0
Dimension of Fourier Features [256, 512, 1024] 512
Batch Size [1000, 3000, 5000, 10000] 1000
Learning Rate [1e−4, 2e−4, 4e−4] 4e−4
Epochs [30, 40, 50, 80, 100] 50
Num of Layers [4,5,6] 5
Num of neurons [256,512,1024,2048] 1024
0 10 20 30 40 50
Epoch0.30.40.50.60.70.8Mutual InformationMSSEG 2016 Dataset
0 10 20 30 40 50
Epoch0.250.300.350.400.450.50Mutual InformationcMS Dataset
Fig.3.Convergence of predicted MI(ˆI1,ˆI2)shown in a dashed line to the ground
truthstate MI(I1, I2)showninsolidline forfiverandomlyselectedsubjects(shownin
adifferent color ) for two datasets. Note that initially, the MI between two predicted
contrasts is high because of randomly initialized shared weights, and over the training
period reaches a plateau close to the true equilibrium.Single-subject Multi-contrast MRI Super-resolution via INRs 13
Spline
 Single-contrast INR
 Split-head INR
 HR GT
Spline
 Single-contrast INR
 Split-head INR
 HR GT
Fig.4.(Best viewed in fullscreen.) Qualitative comparisons of different models for a
typical subject of the MSSEG (upper part) and BraTS (lower part) dataset. Starting
from limited out-of-plane information of the input LR scans, the split-head INR is
capable of retrieving recoverable anatomical structures providing truthfulness to its
prediction. Exploiting the consistency and mutual anatomical information, the split-
headINRcanresolveambiguitiesinjointreconstruction,ashighlightedinyellowboxes,
which is impossible if trained in a single contrast setting.