1 
 Universal Linear Intensity Transformations 
Using Spatially -Incoherent  Diffractive 
Processor s 
 
Md Sadman Sakib Rahman1,2,3  mssr@ucla.edu  
Xilin Yang1,2,3    mikexlyang@ucla.edu  
Jingxi Li1,2,3    jxlli@ucla.edu 
Bijie Bai1,2,3    baibijie@ucla.edu  
Aydogan Ozcan1,2,3,*  ozcan@ucla.edu   
1Electrical and Computer Engineering Department, University of California, Los Angeles, CA, 90095, USA  
2Bioengineering Department, University of California, Los Angeles, CA, 90095, USA  
3California NanoSystems Institute (CNSI), University of California, Los Angeles, CA, 90095, USA  
*Corresponding author: ozcan@ucla.edu  
 
Abstract  
Under spatially -coherent light , a diffractive optica l network composed  of structured  surfaces can be 
designed  to perform any arbitrary complex -valued linear transformation between its input and output 
fields-of -view (FOV s) if the total number  (𝑁𝑁) of optimizable phase-only diffractive features  is ≥~2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜, 
where 𝑁𝑁𝑖𝑖 and 𝑁𝑁𝑜𝑜 refer to  the number of useful pixels at the input and the output FOVs , respectively. 
Here we report the design of a spatially -incoherent diffractive optical processor that can approximate 
any arbitrary linear transformation in tim e-average d intensity between its input and output FOVs . Under  
spatially-incoherent monochromatic light,  the spatially -varying intensity point  spread  function ( 𝐻𝐻) of a 
diffractive network , corresponding to a  given, arbitrarily -selected linear intensity transformation , can be 
written as 𝐻𝐻(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)=|ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)|2, where ℎ is the spatially-coherent point -spread  function 
of the same diffractive network , and (𝑚𝑚,𝑛𝑛) and (𝑚𝑚′,𝑛𝑛′) define the coordinates of the output and input 
FOVs, respectively. Using deep learning, supervised through examples of input -output profiles, we 
numerically demonstrate that a  spatially -incoherent  diffractive network can  be trained to all-optically 
perform any arbitrary linear intensity transformation between its input and output  if 𝑁𝑁 ≥ ~2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜. 
These results constitute the first demonstration of univers al linear intensity transformations  performed 
on an input FOV  under spatially-incoherent illumination and will be useful for designing  all-optical visual  
processors that can work with incoherent , natural  light.  
 
Introduction  
Spatial information processing with free-space optics has been widely explored  for a long time and 
predates the proliferation of electronic computing  1–18. The inherent transformation of  optical fields as 
they propagate through free space , known as diffraction,  togeth er with the ability for wavefront 2 
 modulation with compact hardware, makes low-cost and passive spatial information processing at the 
speed of light propagation possible 19–21. In recent years , diffractive optical network s comprising a set of 
spatially -engineered surfaces to  perform computation through passive light -matter -interaction have 
emerged as  powerful  all-optical processor s 22,23. Designed utilizing deep learning  24, such  coherent  
diffractive optical  processors have demonstrated versatile applications,  including  statistical inference as 
well as deterministic tasks23,25– 31 across the spectrum  from terahertz to near -infrared32 and visible33,34. 
Information processing with a diffractive network involves local modulation of the amplitude and/or the 
phase of the incident optical wave by structu red surfaces containing  diffractive neurons/ features , each 
with a lateral size  of ~λ/2, where λ  is the wavelength of the spatially -coherent  illumination light . The 
entire propagation of a spatially -coherent  wave from the input plane to the output FOV compr ises such  
optical  modulations by 𝐾𝐾 spatially -optimized diffractive surfaces , which in total contain  𝑁𝑁 independent 
diffractive features  (for example, evenly  distributed over the 𝐾𝐾 diffractive surfaces ). These 𝑁𝑁 diffractive 
features  represent the complex -valued transmission coefficients, forming  the independent degrees  of 
freedom of the diffractive processor, which can be  optimized to all -opticall y execute  different tasks22,26–
30,35,36. It was shown that a spatially -coherent diffractive optical network c ould  be trained to perform any 
arbitrary complex -valued  linear transformation between its  input and output FOVs  if 𝑁𝑁≥𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜, where 
𝑁𝑁𝑖𝑖 and 𝑁𝑁𝑜𝑜 refer to  the number of useful  (diffraction -limited)  pixels  at the input  and the output FOVs  21. 
For a phase -only diffractive network where the transmission coefficients of the diffractive features of 
each structured surface only modulate the phase information of light , the requirement  for universal 
linear transformations increase s to 𝑁𝑁≥2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 due to  the reduce d degrees of freedom that can be 
optimized independently .  
For a given complex -valued linear transformation that a coherent diffractive network is designed  to 
approximate, any arbitrary point on th e input plane defined by (𝑚𝑚′,𝑛𝑛′) will result in a  unique  complex -
valued coherent point spread function ( ℎ) at the output FOV defined by (𝑚𝑚,𝑛𝑛). This 4 -dimensional 
complex-valued function,  ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′), that map s the input and output FOVs  represents a spatially -
varying coherent point spread function. Stated differently, unlike traditional spatially -invariant imaging 
systems, a coherent diffractive optical network provides a framework to approximate any arbitrary 
ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′) that corresponds to an arbitrar ily-selected complex- valued linear transformation 
between its input and output FOVs. It was also shown that different /independent  complex -valued linear  
transformations  could  be multiplexed in a single spatially -coherent diffractive processor by utilizing 
polarization and wavelength diversity37,38. 
All of these earlier studies on universal linear transformations implemented  in free -space  through 
diffractive processors  were based on spatially -coherent illumination. In this paper, we report the first 
demonstration of universal linear transformations in optical intensity performed under spatially -
incoherent monochromatic illumination  of an input FOV . We show that, u nder  spatially -incoherent  light,  
a diffractive optical processor can perform any arbitrary linear transformation of time -average d 
intensities between its  input and the output FOVs . Our numerical analyses  revealed that phase-only 
diffractive optical processors  with  a shallow architecture ( for example,  having  a single trainable 
diffractive surface ) are unable to accurately  approximate an arbitrary intensity transformation 
irrespective of the total number  (𝑁𝑁) of diffractive features available for optimization; on the contrary,  
phase-only diffractive optical processors  with deeper architectures  (one  diffractive  layer  following 
others ) can perform an arbitrary intensity linear transformation using spatially -incoherent  illumination  
with  a negligible error when  𝑁𝑁≥2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜. These analyses and conclusions are important for  all-optical 3 
 information processing and visual co mputing systems that us e spatially -incoherent  light , such as in 
natural scenes ; this framework  can also find applications in  computational  microscopy  and incoherent  
imagin g through point spread function engineering, among others . 
Results  
Theoretical analysis  
Spatially -coherent  monochromatic d iffractive optical networks can be characterized by a 4 -dimensional 
complex-valued coherent  impulse response function  (i.e., the point spread function) that is spatially -
varying , connecting the input and output FOVs : ℎ(𝑥𝑥,𝑦𝑦;𝑥𝑥′,𝑦𝑦′). Stated differently, e ach arbitrarily-
selected complex -valued linear transformation that is desired between the pixels of an input FOV and 
output FOV results in a spatially -varying impulse response function ℎ(𝑥𝑥,𝑦𝑦;𝑥𝑥′,𝑦𝑦′), where (𝑥𝑥′,𝑦𝑦′) and 
(𝑥𝑥,𝑦𝑦) define the input and output FOVs, respectively. Based on this definition, the complex -valued 
output field  𝑜𝑜𝑐𝑐(𝑥𝑥,𝑦𝑦) of a spatially -coherent  diffractive processor  is related to the complex -valued input 
field 𝑖𝑖𝑐𝑐(𝑥𝑥′,𝑦𝑦′) by: 
𝑜𝑜𝑐𝑐(𝑥𝑥,𝑦𝑦)=�ℎ𝑐𝑐(𝑥𝑥,𝑦𝑦;𝑥𝑥′,𝑦𝑦′)𝑖𝑖𝑐𝑐(𝑥𝑥′,𝑦𝑦′)𝑑𝑑𝑥𝑥′𝑑𝑑𝑦𝑦′ (1) 
The subscript 𝑐𝑐  indicates  that the quantities are functions of continuous spatial variables 𝑥𝑥, 𝑦𝑦, 𝑥𝑥′, 𝑦𝑦′, 
representing the transverse coordinates on the output and input planes . If these optical fields are  
sampled at an interval (𝛿𝛿) sufficiently small to preserve the spatial variations,  satisfying the Nyquist 
criterion39, one can write : 
𝑜𝑜(𝑚𝑚,𝑛𝑛)=�ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′) 𝑖𝑖(𝑚𝑚′,𝑛𝑛′)
𝑚𝑚′,𝑛𝑛′(2) 
Here, 𝑚𝑚, 𝑛𝑛, 𝑚𝑚′,𝑛𝑛′ refer to  discrete indices such that  𝑜𝑜(𝑚𝑚,𝑛𝑛)=𝑜𝑜𝑐𝑐(𝑚𝑚𝛿𝛿,𝑛𝑛𝛿𝛿) and 𝑖𝑖(𝑚𝑚′,𝑛𝑛′)=
𝑖𝑖𝑐𝑐(𝑚𝑚′𝛿𝛿,𝑛𝑛′𝛿𝛿). The instantaneous output intensity can be written as : 
|𝑜𝑜(𝑚𝑚,𝑛𝑛)|2=�ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′) ℎ∗(𝑚𝑚,𝑛𝑛;𝑚𝑚′′,𝑛𝑛′′) |𝑖𝑖(𝑚𝑚′,𝑛𝑛′)| |𝑖𝑖(𝑚𝑚′′,𝑛𝑛′′)| 𝑒𝑒𝑗𝑗�𝜑𝜑�𝑚𝑚′,𝑛𝑛′�−𝜑𝜑�𝑚𝑚′′,𝑛𝑛′′��
𝑚𝑚′,𝑛𝑛′,𝑚𝑚′′,𝑛𝑛′′(3) 
where 𝜑𝜑(.) is the phase function  of the input field 𝑖𝑖, i.e., 𝑖𝑖=|𝑖𝑖|𝑒𝑒𝑗𝑗𝜑𝜑, and ℎ∗ denotes the complex 
conjugate of  ℎ. The time -average d output intensity can be written as :  
𝑂𝑂(𝑚𝑚,𝑛𝑛)=〈|𝑜𝑜(𝑚𝑚,𝑛𝑛)|2〉=�ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′) ℎ∗(𝑚𝑚,𝑛𝑛;𝑚𝑚′′,𝑛𝑛′′)|𝑖𝑖(𝑚𝑚′,𝑛𝑛′)||𝑖𝑖(𝑚𝑚′′,𝑛𝑛′′)|〈𝑒𝑒𝑗𝑗∆𝜑𝜑〉
𝑚𝑚′,𝑛𝑛′,𝑚𝑚′′,𝑛𝑛′′(4) 
where 〈∙〉 denotes  time -average  operation  and ∆𝜑𝜑=𝜑𝜑(𝑚𝑚′,𝑛𝑛′)−𝜑𝜑(𝑚𝑚′′,𝑛𝑛′′). Since  the illumination 
light is  spatially -incoherent , the phases at different spatial points of the input vary randomly  over time 
and are independent  of each  other .40 Stated differently for stationary objects/scenes that are uniformly 
illuminated with a spatially -incoherent light, ∆𝜑𝜑 varies randomly between 0  and 2𝜋𝜋 over time, yielding 
〈𝑒𝑒𝑗𝑗∆𝜑𝜑〉=0 for (𝑚𝑚′,𝑛𝑛′)≠(𝑚𝑚′′,𝑛𝑛′′). As a result of this , under spatially-incoherent il lumination, Eq. (4) 
can be written as : 
𝑂𝑂(𝑚𝑚,𝑛𝑛)=�|ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)|2 〈|𝑖𝑖(𝑚𝑚′,𝑛𝑛′)|2〉
𝑚𝑚′,𝑛𝑛′=�𝐻𝐻(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′) 𝐼𝐼(𝑚𝑚′,𝑛𝑛′)
𝑚𝑚′,𝑛𝑛′(5) 4 
 where 𝐼𝐼=  〈|𝑖𝑖|2〉 is the time -average d input intensity and 𝐻𝐻(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)=|ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)|2 is the 
intensity impulse response  of the diffractive optical processor under spatially-incoherent illumination. 
From now on, unless otherwise stated, we use the term optical ‘intensity’ to imply  time -average d 
intensity  functions . Similarly, whenever all-o ptical linear transformation of intensity is mentioned, 
spatially-incoherent  monochromatic  illumination is implied  unless stated otherwise . 
We should emphasize that while 𝐻𝐻(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)=|ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)|2, we have  in general 𝑂𝑂(𝑚𝑚,𝑛𝑛)≠ 
|𝑜𝑜(𝑚𝑚,𝑛𝑛)|2. Therefore, the output intensity of a spatially -incoherent diffractive network cannot be 
calculated as |𝑜𝑜(𝑚𝑚,𝑛𝑛)|2=�∑ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′) 𝑖𝑖(𝑚𝑚′,𝑛𝑛′)𝑚𝑚′,𝑛𝑛′ �2. For the numerical forward model  
corresponding to  each input object, as will be detailed in the next section, we used a large number of 
random phase distributions at the input plane to approximate  𝑂𝑂(𝑚𝑚,𝑛𝑛)=〈|𝑜𝑜(𝑚𝑚,𝑛𝑛)|2〉 under spatially-
incoherent  illumination.  
Numerical analysis  
In this subsection, we numerically explore the design of diffractive optical processors to perform an 
arbitrary linear intensity  transformation between the input and the output FOVs  under  spatially -
incoherent  illumination. We assume, as shown in Fig. 1a, 𝑁𝑁  independent diffractive features  (phase -only 
elements) that are distributed over 𝐾𝐾 diffractive surfaces, each with 𝑁𝑁 𝐾𝐾⁄ diffractive features , between 
the input and output planes. Following from Eq. (5), i f we rearrange the pixel intensities of  𝐼𝐼(𝑚𝑚′,𝑛𝑛′) and 
𝑂𝑂(𝑚𝑚,𝑛𝑛) as column vector s 𝒊𝒊 and 𝒐𝒐, then we can write 𝒐𝒐=𝑨𝑨′ 𝒊𝒊, where  𝑨𝑨′ represents the linear intensity 
transformation  performed  by the diffractive optical network  under spatially-incoherent illumination. 
The elements of 𝑨𝑨′ correspond to the elements of the intensity impulse response 𝐻𝐻 (𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′); see 
Eq. (5) . Note  that all the  elements of 𝑨𝑨′ are real and nonnegative  since it represents a linear intensity 
transformation  with 𝐻𝐻(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)=|ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)|2. Hence, in the  context of arbitrary linear 
transformation s in intensity, only real transformation matrices with nonnegative elements are 
consi dered.  
For our  target linear transformation that is  to be approximated by the spatially- incoherent  diffractive 
processor, initially, we selected an arbitrary matrix  𝑨𝑨, as shown in Fig. 1b . In the following numerical 
analysis, we optimize 𝑁𝑁 diffractive features  of a phase -only diffractive processor so that  𝑨𝑨′≈ 𝑨𝑨 under 
spatially-incoherent  illuminatio n. The size of 𝑨𝑨 is chosen as  𝑁𝑁𝑜𝑜×𝑁𝑁𝑖𝑖=64×64, i.e., the number of 
pixels at  both the input ( 𝑁𝑁𝑖𝑖) and the output ( 𝑁𝑁𝑜𝑜) FOVs  are 8 × 8 , arranged in a  square grid.  Each element 
of the matrix 𝑨𝑨 is randomly sampled from a uniform probability distribution between 0 and 1, i.e., 
𝑨𝑨[𝑝𝑝,𝑞𝑞]~𝑈𝑈𝑛𝑛𝑖𝑖
𝑈𝑈𝑜𝑜𝑈𝑈𝑚𝑚(0, 1) wh
ere 𝑨𝑨[𝑝𝑝,𝑞𝑞] is the element at 𝑝𝑝-th row and 𝑞𝑞-th column of 𝑨𝑨, 𝑝𝑝= 1, … ,𝑁𝑁𝑜𝑜 
and 𝑞𝑞= 1, … ,𝑁𝑁𝑖𝑖.  
For the deep learning-based optimiz ation of  the design of a phase- only diffractive processor to achieve 
𝑨𝑨′≈ 𝑨𝑨 , we followed two different  data -driven supervised learning approach es: (1) the indirect 
approach  and (2) the direct approach. In the indirect  approach,  instead of directly training the diffractive 
network to perform the linear intensity transformation 𝑨𝑨, we trained the network , under spatially-
coherent  illumination, to perform the complex-value d linear transformation 𝑨𝑨 � between the input and 
output FOVs  such that �𝑨𝑨 �[𝑝𝑝,𝑞𝑞]�=�𝑨𝑨[𝑝𝑝,𝑞𝑞], which would result in  an intensity linear transformation 
�𝑨𝑨�[𝑝𝑝,𝑞𝑞]�2=𝑨𝑨[𝑝𝑝,𝑞𝑞] under spatially -incoherent  illumination. For the purpose of the training, we defined 
the phase of 𝑨𝑨�[𝑝𝑝,𝑞𝑞] to be zero, i.e., 𝑨𝑨 �[𝑝𝑝,𝑞𝑞]=�𝑨𝑨[𝑝𝑝,𝑞𝑞]exp(𝑗𝑗0); however, any other phase distribution 
could also be used since the design space is not unique. Stated differently, i n this indirect approach, we 5 
 design a diffractive network that can achieve a spatially-coherent  impulse response  ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′), 
which will ensure that the same design has a spatially -incoherent  impulse response of  𝐻𝐻(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)=
|ℎ(𝑚𝑚,𝑛𝑛;𝑚𝑚′,𝑛𝑛′)|2 such that 𝑨𝑨′≈𝑨𝑨 can be satisfied under spatially -incoherent  illumination.  To achieve 
this goal,  we used the relationship 𝒐𝒐�=𝑨𝑨�𝒊𝒊̃ to generate a large set of input -target complex -valued optical 
field pairs (𝒊𝒊̃,𝒐𝒐�), and used deep learning to optimize  the phase values of the diffractive feature s by 
minimizing the mean  squared  error (MSE) loss between the target  complex field 𝒐𝒐� and the complex  field  
𝒐𝒐�′ obtained by coherent ly propagati ng 𝒊𝒊̃ through the diffractive network  (see the Methods section) . In 
other words , spatially -coherent  design of a diffractive network is used here as a proxy for th e design of a 
spatially -incoherent  diffractive network that can achieve any arbitrary intensity linear transformation 
between its input and output FOVs.  
In the second  approach  (termed the direct approach),  we trained the diffractive network to perform the 
desired intensity linear transformation 𝑨𝑨 between the input and the output FOVS, by directly using the 
relationship  𝒐𝒐=𝑨𝑨 𝒊𝒊 to generate a large set of input -target intensity  pairs (𝒊𝒊,𝒐𝒐). Using this large training 
set of input/output intensity patterns , we optimize d the transmission phase values of the diffractive 
layers  using deep learning,  by minimizing the MSE  loss between the output  pixel intensities of  the 
diffractive processor  𝒐𝒐′ and the ground -truth intensit ies 𝒐𝒐 (see the Methods section).  During the training 
phase, t he output intensity of the diffractive processor was simulated through the incoherent 
propagation of the input intensity  patterns , 𝒊𝒊 or 𝐼𝐼(𝑚𝑚′,𝑛𝑛′). To numerically simulat e the spatially -
incoherent  propagation of  𝐼𝐼(𝑚𝑚′,𝑛𝑛′), we assume d the input optical field to be √𝐼𝐼𝑒𝑒𝑗𝑗𝜑𝜑 where  𝜑𝜑 is a 
random 2D phase  distribution, i.e., 𝜑𝜑(𝑚𝑚′,𝑛𝑛′)~𝑈𝑈𝑛𝑛𝑖𝑖𝑈𝑈𝑜𝑜𝑈𝑈𝑚𝑚(0,2𝜋𝜋) for each (𝑚𝑚′,𝑛𝑛′). This input  field with 
the random phase distribution 𝜑𝜑 was coherently propagated through the diffractive surfaces  to the 
output plane, using the angular spectrum approach22. We repeat ed this coherent w ave propagation 𝑁𝑁𝜑𝜑 
times  for every 𝒊𝒊 , each time with a different random phase 𝜑𝜑 (𝑚𝑚′,𝑛𝑛′) distribution , and average d the 
resulting 𝑁𝑁𝜑𝜑 output intensities. As 𝑁𝑁𝜑𝜑→∞, the average intensity would approach the theoretical time -
average d output intensity for spatially -incoherent  illumination , i.e., 𝑂𝑂(𝑚𝑚,𝑛𝑛)=〈|𝑜𝑜(𝑚𝑚,𝑛𝑛)|2〉. Due to the 
limited availability of  computational resources,  for the direct training  (the second design appr oach)  of 
the spatially -incoherent  diffractive optical processors , we used  𝑁𝑁𝜑𝜑=𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=1000 . 
The diffractive models reported in Figs. 1 -5 and 10 were trained using the indirect approach while the 
ones in Figs. 6 -9 were trained using the direct approach. All the diffractive networks reported in this 
work, after their training using either the direct or indirect design approaches,  were evaluated and  
blindly tested through the incoherent propagation of input intensities  with 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20,000. Since the 
testing is computationally less cumbersome compared to the training,  we used 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20,000≫𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡.  
After the training phase, we tested the resulting diffractive processor designs using 20,000 test intensity 
patterns 𝒊𝒊 that were never used during training; the size of this testing intensity set (20,000) should not 
be confused with 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20,000 since for each input intensity test pattern  of this set , we used  𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=
20,000 random 2D phase patterns to compute the corresponding spatially -incoherent  output intensity . 
In Fig. 1c , the approximation error s of eight different phase-only diffractive processors trained using the 
indirect approach, each with  𝐾𝐾=5 diffractive layers , are reported  as a function of 𝑁𝑁. The mean error  
(Fig. 1c ) for  each diffractive des ign w as calculated at the output intensity  patterns 𝒐𝒐′ with respect to the 
ground truth 𝒐𝒐 =𝑨𝑨𝒊𝒊, by averaging over  the 20,000 test intensity patterns . Figure 1c reveals  that the 
approximation error of the spatially-incoherent  diffractive processors  reaches a minimum level  as 𝑁𝑁
2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 
approaches 1, and stays at the same level for  𝑁𝑁≥2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜.  6 
 To understand the impact  of 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡 on these approximation  error calculations , we took the diffractive 
processor design #  1E shown in Fig. 1c  (i.e., 𝐾𝐾=5,𝑁𝑁≈2.1×2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜), and used different 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡 values at 
the blind testing phase for evaluating the average test error on the same intensity test set  composed of 
20,000 patterns 𝒊𝒊. As shown  in Fig. 1d, the computed error values  decrease as 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡 increases , as 
expected . On the right y -axis of the s ame Figure  1d, we also show , as a function of 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡, the 
expec tation  value of  �1
𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡∑𝑒𝑒𝑗𝑗𝜃𝜃𝑖𝑖𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡
𝑖𝑖=1�, where 𝜃𝜃𝑖𝑖~𝑈𝑈𝑛𝑛𝑖𝑖𝑈𝑈𝑜𝑜𝑈𝑈𝑚𝑚(0,2𝜋𝜋). This expect ation value of the  
residual magnitude of 1
𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡∑𝑒𝑒𝑗𝑗𝜃𝜃𝑖𝑖𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡
𝑖𝑖=1 decreases as 𝑁𝑁 𝜑𝜑,𝑡𝑡𝑡𝑡 increases and would approach zero  as 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡→
∞. The numerically simulated output intensity of a diffractive processor design approach es the true 
time -averaged intensity of the spatially -incoherent  wave  as 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡 gets larger , following a similar trend as 
�1
𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡∑𝑒𝑒𝑗𝑗𝜃𝜃𝑖𝑖𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡
𝑖𝑖=1�, reported in Fig. 1d. This comparison also highlights the fact that our choice of using 
𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20,000 random 2D phase patterns to compute the spatially-incoherent  output intensit y 
patterns  in the blind testing phase is an accurate approximation.  
Next , we show in Fig. 2  the scaled intensity linear transformations, 𝑨𝑨 �, that were approximated by five  of 
the trained diffractive networks of Fig. 1c . 𝑨𝑨� is related to the physical transformation 𝑨𝑨′ by a scalar 
factor 𝜎𝜎𝐴𝐴 (see the ‘Evaluation’ subsection in  ‘Methods ’ section ) which  compensates  for diffraction 
efficiency-related optical losses. We also show the error matrix with respect to the target 𝑨𝑨, i.e., 𝜺𝜺=
�𝑨𝑨−𝑨𝑨��, and report the average of the error matrix elements in the table on the right.  Here |∙| denotes 
the elementwise operation.  As 𝑁𝑁 increases, t he diffractive networks’ resulting  matrices resemble the 
ground truth  target better and the approximation  error decreases steadily ; however , the improvement 
is more prominent as 𝑁𝑁 approaches 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 and stagnates beyond 𝑁𝑁≈2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜.  
To provide visually more noticeable  illustrations of  the diffractive network s’ all -optical intensity  
transformations  under spatially -incoherent  illumination , we use d structured intensity patterns such as 
the letters U, C, L, and A as input  intensity  to the diffractive networks (see Fig. 3 ). Because of the 
randomness of the elements of the intensity transformation matrix, the output pixel intensities also 
appear random (harder to compare visually against the ground truth) . However, t he reappearance of 
the letters after a numerical inversion thr ough  the multiplication of the  scaled  output intensity 𝒐𝒐 �  by the 
inverse of the target transformation, 𝑨𝑨−1, would indicate  𝑨𝑨�≈𝑨𝑨 and validat e the correctness of the 
diffractive network s’ approximations  in a visually noticeable  manner  (see the ‘Evaluation’ subsection of 
the Methods section  for the definition of 𝒐𝒐 �). In the case of  the diffractive network  # 1A (𝐾𝐾=5, 𝑁𝑁=
5×382≈0.88×2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜), the result of such  an inversion does not quite reveal  any recognizable 
patterns, indicating the near -failure of the all-optical approximation  of this design # 1A . However , such 
inversion reveals the  recognizable patterns (U, C, L , and A) as 𝑁𝑁 approaches 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 (design  # 1B) and 
becomes identical to the in puts as 𝑁𝑁  exceeds 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 (e.g., design # 1C). These results  show  that for the 
𝐾𝐾=5 phase-only diffractive networks  with a sufficient ly large  𝑁𝑁≥~2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜, we have  𝑨𝑨�≈𝑨𝑨, indicating 
that these  networks could  faithfully approximate the target  intensity linear  transformation  under 
spatially -incoherent  illumination.  
For computational imaging and sensing applications , such as in microscopy, exploring patterns of closely 
spaced lines and points would be interestin g. Motivated by this , we repeat ed the same procedures 
outlined in Fig. 3 for various intensity patterns consisting of  closely separated  line pairs  and sets of 7 
 points , the results  of which are summarized in  Fig. 4. The same  conclusions drawn previously  in Fig. 3  
hold : for 𝑁𝑁 ≥ ~2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 we have 𝑨𝑨�≈ 𝑨𝑨.  
We also investigated the dependence of the all-optical approximation of intensity linear transformation s 
on the number of  diffractive  layers 𝐾𝐾; see  Fig. 5 . The results of this analysis reveal that even with 𝑁𝑁 ≈
2 × 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜, 𝐾𝐾= 1 and 𝐾𝐾= 2 diffractive designs failed to approximate the target linear transformation  
despite having a  large 𝑁𝑁, whereas the designs with 𝐾𝐾> 2 succe ssfully approximated the target 
transformation under spatially-incoherent illumination . This confirms  that the depth of the diffractive 
network  design  is a key architectural  factor in the  computational  capacity of diffractive processors to 
perform arbitrary linear transformation s21,22,37,38.  
Next, we present the blind testing  results of the diffractive processors  that were trained  using the 
second design approach (i.e., direct approach ), to perform the same arbitrary intensity linear 
transformation as has been considered so far.  In Fig. 6a, the approximation error s of eight different 
phase-only diffractive processors trained using the direct approach, each with  𝐾𝐾= 5 diffractive layers , 
are reported as a function of 𝑁𝑁 . The mean error was calculated over the same 20,000 test intensity 
patterns used in  Fig.1c;  for each test intensity pattern, the incoherent output intensity 𝒐𝒐′ was calculated 
using 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20000  (same as before ). In these alternative diffractive designs , the approximation error 
of the diffractive processors  reaches a minimum level as 𝑁𝑁
2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 approaches 1, and stays at the same level 
for 𝑁𝑁 ≥ 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 – the same conclusion that we reached for the indirect designs reported  earlier . 
However, compared with the previous designs that used the indirect approach, here, the minimum  error 
level obtained using the direct approach is approximately three times  higher . This can be attributed to 
the use of a relatively small 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=1000  during the training, and these designs can be further 
improved by increasing 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡 using a longer training effort with more computational resources . 
In Fig. 7 , we show the scaled li near intensity transformations, 𝑨𝑨�, that were approximated by f ive of the 
trained diffractive networks of Fig. 6a . For each case, we also show the error matrix with respect to the 
target 𝑨𝑨, i.e., 𝜺𝜺=�𝑨𝑨−𝑨𝑨��, and report the average of the error matrix elements in the table on the right.  
As 𝑁𝑁 increases, the mean intensity transformation error decreases, except for design # 2B which we 
believe is an outlier resulting from poor convergence. The relatively large error  of the  design  # 2B is due 
to the diffr action efficiency imbalance  among the individual input pixels, as evident from the uneven 
magnitudes across the columns of 𝑨𝑨�. Similarly, the other  designs  of the direct approach  reveal uneven 
magnitudes across the columns of 𝜺𝜺, indicating some diffraction efficiency im balance among the 
individual input pixels, albeit not as severe as the  design  # 2B. Despite such imperfections , these 
diffractive  network s designed using the direc t approach  effectively learned the target intensity 
transformation, as evident from Figs. 8 and 9. Figure 8 reveals that , for all the designs,  the multiplication 
of the output intensity patterns  𝒐𝒐� by the inverse of the target transformation, 𝑨𝑨−1 brings back the 
patterns U, C, L, A. Although, the reconstruction quality is better for  𝑁𝑁 ≈ 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 and remains similar 
beyond 𝑁𝑁> 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜, the improvement is not as sharp as it was with the indirect approach  (see Fig. 8 vs. 
Fig. 3 and Fig. 9 vs. Fig. 4) . In contrast with  the diffractive networks designed using the indirect 
approach , here in this case, the diffractive networks  with 𝑁𝑁 < 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 (e.g., design # 2A ) succeeded in 
approximating the linear transformation to the extent of revealing recognizable patterns after a 
numerical inverse mapping. These same observations also hold for the intensity patterns that consist of  
closely spaced lines and points , as shown in Fig. 9.  8 
 Finally , we report in Fig. 10  the performance of a diffractive network  (𝐾𝐾=5, 𝑁𝑁≈2×2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜) trained 
using the indirect approach to approximate another arbitrary intensity linear transformation, defined by 
a non-invertible  matrix.  The target transformation 𝑨𝑨, the approximate all-optical transformation 𝑨𝑨�, and 
the error matrix 𝜺𝜺=�𝑨𝑨−𝑨𝑨�� are shown in  Fig. 10a , revealing that  the diffractive  network  design 
performed the target intensity transformation with negligible error. We also show the performance of 
this diffract ive network  design  on test patterns (U, C, L, and A as well as line  pairs and point s) in Fig. 10b. 
The all -optical outputs are identical to the ground truth outputs , further confirming  that we have 𝑨𝑨�≈𝑨𝑨. 
Another example of the all-optical  approximation of an arbitrary  intensity transformation (defined by a 
random permutation  matrix)  is also reported in Supplementary Figure S1. 
Discussion  
We demonstrated  that phase-only diffractive networks  under spatially -incoherent  illumination  could  
perform arbitrary linear transformation s of optical intensity with a negligible error if 𝑁𝑁 ≥2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜. The 
same conclusions would be applicable to complex-valued diffractive networks where the phase and 
amplitude of each diffractive fea ture could be independently optimized;  in that case, the critical number 
of complex-valued diffractive features  for approximating an  arbitrary linear transformation of optical 
intensity  would reduce by half to  𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 due to the increased degrees of freedom  per diffractive layer . 
Because of the practical advantages of phase -only diffractive networks,  without loss of generality, we 
limit ed our analyses  in this work to phase-only modulation at each diffractive surface . 
Our results  suggest that the two differ ent training approaches  (indirect vs. direct design)  converge 
differently. If 𝑁𝑁 is comparable to or larger than 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜, the indirect approach results in significantly 
better  and faster  convergence  and accurate approximation 𝑨𝑨�≈𝑨𝑨; on the other hand,  the direct design 
approach works better when 𝑁𝑁 is considerably less than 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜, even if  its approximation error is larger. 
For example, although the designs  # 2A and  # 2B have higher errors than the design # 1A, the 
performance s of the former on  various test patterns are manifestly better  as compared in Figs. 3, 4, 8 
and 9 . These direct designs can be further improved in their approximation power by increasing 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡≫
1000  through  a longer training phase,  utilizing  more computational resources. 
An important  advantage of the direct approach over the indirect one is that the former  is compatible 
with data -driven design and can be applied even if the only information available  to the designer  is the 
sample data represe nting the target incoherent linear process , without a priori  knowledge of  the 
transformation matrix itself. By the same token, the direct approach also lends itself to data -driven 
optimization of incoherent diffractive processors  for all -optical linear  approximation of some  nonlinear 
processes. As a c onsequen ce of this , data-driven design of incoherent processors for  performing  other  
inference  tasks such as e.g.,  all-optical image classification under spatially-incoherent  illumination, can 
be accomplished u sing the direct approach.  
The failure of shallow diffractive networks to perform an arbitrary intensity transformation ( see e.g., 
𝐾𝐾=1 and 𝐾𝐾=2 designs shown in Fig. 5 ) indicates that  shallow architectures  with phase-only 
diffractive layers  are unable to effectively balance  the ballistic photons  that are transmitted from the 
sample/input  FOV over a low numerical aperture; as a result of this,  the lower spatial frequencies of the 
input intensity patterns dominate the output intensity patterns  of a shallow diffractive network , 
sacrificing th e approximation accuracy . Therefore, shallow diffractive  network  architectures , even with 
large numbers of trainable diffractive features ( 𝑁𝑁), fail to approximate an arbitrary intensity 9 
 transformation , as shown in Fig. 5. Deeper architectures, on the other hand, utilize their trainable 
diffractive features more effectively by distributing  them across several layers/surfaces, one following 
another,  and mixing the propagating modes of the input FOV over a seri es of  layers that are optimized 
using deep learning.  
Spatially-incoherent diffractive processor  designs  can also be extended to  temporally incoherent 
broadband illumination light. In fact, multiplexing of >100 arbitrary complex-valued linear 
transformations for complex optical fields was shown to be possible under  spatially-coherent but 
broadband illumination light38. Following a similar multi-wavelength optimization process and the 
indirect design principles  outlined earlier , one can design  a diffractive network to simultaneously 
approximate  a group of arbitrarily -selected linear intensity transformations  (𝑨𝑨𝝀𝝀𝟏𝟏,𝑨𝑨𝝀𝝀𝟐𝟐,… 𝑨𝑨𝝀𝝀𝑴𝑴) under 
spatially-incoherent  illumination, where each intensity transformation is assigned to a unique 
wavelength 𝝀𝝀𝒊𝒊 {𝒊𝒊=𝟏𝟏:𝑴𝑴}. The success of such a spatially- and temporally-incoherent diffractive optical 
network to accurately perform all the target intensity transformation s will require an increase in the 
number of trainable features within the diffractive volume, i.e.,  𝑁𝑁 ≥ 𝑀𝑀 × 2𝑁𝑁𝑖𝑖𝑁𝑁𝑜𝑜 would be needed for a 
phase-only diffra ctive network . Such diffractive processor designs that work  under spatially- and 
temporally-incoherent light can be useful for a number of applications , including fluorescence and 
brightfield microscopy and the processing of natural scenes .  
Methods  
Model for the propagation of spatially- coherent light through a diffractive  optical  
network  
Propagation of  spatially-coherent  complex optical fields through  a diffractive processor 𝔇𝔇{∙} constitutes 
successive amplitude and/or phase modulation by diffractive surfaces, each followed by  coherent 
propagation through the free space  separating  consecutive diffractive surfaces.  The diffractive features  
of a surface locally modulate the incident optical field 𝑢𝑢(𝑥𝑥,𝑦𝑦). For this paper, the trainable diffrac tive 
features are phase-only , i.e., only the phase, but not the amplitude, of the incident field is modulated by 
the diffractive surface. In other words, the field immediately after the surfaces would be 
𝑢𝑢(𝑥𝑥,𝑦𝑦)exp�𝑗𝑗𝜙𝜙𝑀𝑀(𝑥𝑥,𝑦𝑦)� where  the local  phase change 𝜙𝜙𝑀𝑀(𝑥𝑥,𝑦𝑦) induced by the surface  is related to its  
height ℎ(𝑥𝑥,𝑦𝑦) as 𝜙𝜙𝑀𝑀=2𝜋𝜋
𝜆𝜆(𝑛𝑛− 1)ℎ. Here 𝑛𝑛 is the refractive index of the diffractive surface material.  
Free-space propagation of an optical field between consecutive diffractive surfaces  was modeled using 
the a ngular spectrum method 8, according to which the propagation of an optical field 𝑢𝑢(𝑥𝑥,𝑦𝑦) by 
distance 𝑑𝑑 can be computed as follows:  
𝑢𝑢(𝑥𝑥,𝑦𝑦;𝑧𝑧=𝑧𝑧0+𝑑𝑑)=ℱ−1�ℱ{𝑢𝑢(𝑥𝑥,𝑦𝑦;𝑧𝑧=𝑧𝑧0)}×𝐻𝐻�𝑈𝑈𝑥𝑥,𝑈𝑈𝑦𝑦;𝑑𝑑�� (6) 
where ℱ (ℱ−1) is the two -dimensional Fourier (Inverse Fourier) transform and 𝐻𝐻�𝑈𝑈𝑥𝑥,𝑈𝑈𝑦𝑦;𝑑𝑑� is the free -
space transfer function for an axial  propagation distance 𝑑𝑑: 
𝐻𝐻�𝑈𝑈𝑥𝑥,𝑈𝑈𝑦𝑦;𝑑𝑑�=�exp�𝑗𝑗2𝜋𝜋
𝜆𝜆𝑑𝑑�1−(𝜆𝜆𝑈𝑈𝑥𝑥)2−�𝜆𝜆𝑈𝑈𝑦𝑦�2�,𝑈𝑈𝑥𝑥2+𝑈𝑈𝑦𝑦2< 1/𝜆𝜆2
0, otherwise(7) 
where  𝜆𝜆 is the wavelength of light.  10 
 Model for the propagation of spatially- incoherent  light through a diffractive  optical  
network  
With spatially-incoherent  light, the (average ) output optical intensity 𝑂𝑂 (𝑥𝑥,𝑦𝑦) of a diffractive network , 
for a given input intensity 𝐼𝐼 (𝑥𝑥,𝑦𝑦), can be written as  
𝑂𝑂(𝑥𝑥,𝑦𝑦)=〈�𝔇𝔇��𝐼𝐼(𝑥𝑥,𝑦𝑦)exp�𝑗𝑗𝜑𝜑(𝑥𝑥,𝑦𝑦)���2
〉=lim
𝑁𝑁𝜑𝜑→∞1
𝑁𝑁𝜑𝜑��𝔇𝔇��𝐼𝐼(𝑥𝑥,𝑦𝑦)exp�𝑗𝑗𝜑𝜑𝑡𝑡(𝑥𝑥,𝑦𝑦)���2𝑁𝑁𝜑𝜑
𝑡𝑡=1(8) 
where 𝔇𝔇{∙} denotes the coherent propagation of the optical field through the diffractive processor  as 
described in the preceding subsection,  and  〈∙〉 denotes the statistical average , over all the realizations of 
the spatially-independen t random process 𝜑𝜑 (𝑚𝑚,𝑛𝑛) representing the 2D phase of the input optical field, 
i.e., 𝜑𝜑(𝑚𝑚,𝑛𝑛)~𝑈𝑈(0,2𝜋𝜋) for all 𝑚𝑚,𝑛𝑛40.  
As for the spatially-incoherent propagation of average intensity , it is on ly possible to approximate the 
true  average  (Eq. 8) by averaging over a finite number 𝑁𝑁 𝜑𝜑 of samples of 𝜑𝜑 (𝑥𝑥,𝑦𝑦), i.e.,  
𝑂𝑂(𝑥𝑥,𝑦𝑦)≈1
𝑁𝑁𝜑𝜑��𝔇𝔇��𝐼𝐼(𝑥𝑥,𝑦𝑦)exp�𝑗𝑗𝜑𝜑𝑡𝑡(𝑥𝑥,𝑦𝑦)���2𝑁𝑁𝜑𝜑
𝑡𝑡=1(9) 
In the training phase of the direct training approach,  incoherent propagation of intensities through the 
diffractive processors was simulated with 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=1000 . However , in the blind testing phase we used 
𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20000  while evaluating the diffractive processors once they were trained, irrespective of 
whether the indirect or the direct approach of training was used . 
In our numerical  simulations , the fields/intensities were discretized using 𝛿𝛿 ≈ 0.53𝜆𝜆 along both 𝑥𝑥 and 𝑦𝑦, 
e.g., 𝑢𝑢(𝑚𝑚,𝑛𝑛)≜ 𝑢𝑢(𝑚𝑚𝛿𝛿,𝑛𝑛𝛿𝛿) and sufficiently zero -padded before evaluating the Fourier transform , as in 
Eq. 6, using Fast Fourier Transform (FFT) algorithm.  
Diffractive network architecture  
The heights ℎ (𝑚𝑚,𝑛𝑛)≜ ℎ(𝑚𝑚𝛿𝛿,𝑛𝑛𝛿𝛿) of the  𝑁𝑁 diffractive features  distributed over 𝐾𝐾  surfaces were 
optimized for designing the diffractive processors to perform the desired transformation.  To keep the 
connectivity between successive diffractive layers22 the same across the trained diffractive networks 
with different 𝑁𝑁 , the layer-to -layer separation was set as 𝑑𝑑 =𝑊𝑊𝛿𝛿
𝜆𝜆, where 𝑊𝑊=�𝑁𝑁
𝐾𝐾𝛿𝛿 is the width of each  
diffractive layer . The distances between  the input FOV and layer -1 and between layer- 𝐾𝐾 and the output 
FOV were also set as 𝑑𝑑. The pixel size on both the input and the output FOVs was ~ 2.13𝜆𝜆 × 2. 13𝜆𝜆, i.e., 
4𝛿𝛿× 4𝛿𝛿. 
Linear transformation matri x 
In this paper, the input and  the output of the diffractive networks have dimensions  of 𝑁𝑁𝑖𝑖=𝑁𝑁𝑜𝑜= 8 × 8 , 
i.e.,  𝐼𝐼,𝑂𝑂 ∈ ℝ+8×8 and 𝒊𝒊,𝒐𝒐 ∈ ℝ+64. To clarify, 𝒊𝒊 and 𝒐𝒐 are one-dimensional  (column) vectors obtained by 
rearranging the intensity values 𝐼𝐼(𝑚𝑚,𝑛𝑛) and 𝑂𝑂(𝑚𝑚,𝑛𝑛) of the input and the output pixels arranged in a 
two-dimensional 8 × 8  square grid . Accordingly, the target transformation matrix 𝑨𝑨 has a size of 
𝑁𝑁𝑜𝑜×𝑁𝑁𝑖𝑖=64×64, i.e., 𝑨𝑨 ∈ ℝ+64×64. 11 
 Traini ng details  
The height  ℎ of the diffractive features at each layer  was confined between zero  and a maximum value 
ℎ𝑚𝑚𝑚𝑚𝑥𝑥 by using a latent variable ℎ𝑙𝑙𝑚𝑚𝑡𝑡𝑡𝑡𝑛𝑛𝑡𝑡 : 
ℎ=ℎ𝑚𝑚𝑚𝑚𝑥𝑥
2×[sin(ℎ𝑙𝑙𝑚𝑚𝑡𝑡𝑡𝑡𝑛𝑛𝑡𝑡 )+1]  
We chose ℎ𝑚𝑚𝑚𝑚𝑥𝑥≈𝜆𝜆
𝑛𝑛−1 so that the corresponding phase modulation depth is 2𝜋𝜋. The diffractive layers  
were  optimized using the AdamW  optimizer 41 for 50  epochs with a minibatch size of 8 and an initial 
learning rate of 10−3. The learning rate was decayed by a factor of 0.7 every five epoch s. The latent 
variables were initialized randomly from the standard normal distribution 𝒩𝒩(0,1). We evaluated t he 
mean loss of the trained model on the validation set after the completion of each epoch and selected 
the trained model state  at the end of the epoch corresponding to the lowest validation loss.  These 
details were the same for both the indirect and the direct training approach es. 
The diffractive processor models were implemented and trained using PyTorch (v1.10) 42 with Compute 
Unified Device Architecture (CUDA) version 11.3.1 . Training and testing were done on GeForce RTX 3090 
graphics processing units (GPU) in workstations with 256GB of random-access memory (RAM) and Intel 
Core i9 central processing unit (CPU). The training time of the models varied with the training approach 
as well as  the size of  the models in terms of 𝐾𝐾  and 𝑁𝑁. For example,  the indirect training of 𝐾𝐾=5, 𝑁𝑁=
5×522 diffractive network model took  around 5 hours  , whereas with the direct  approach , the  training 
time for  the 𝐾𝐾=5, 𝑁𝑁=5×522 model with 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=1000  was around 58 hours.  
Evaluation  
The evaluation procedure was the same across all the trained diffractive networks irrespective of 
whether the direct approach or the indirect approach was used to train them. To evaluate the trained 
diffractive networks, we generated  a test set comprising  20,000 pairs of  input and target intensity 
vector s 𝒐𝒐=𝑨𝑨𝒊𝒊. Note that these 20,000 test examples were generated using a different random seed 
from the ones used to generate the training and the validation sets to ensure they were not represented 
during the training. For a given 𝒊𝒊, the corresponding  input  intensity pattern was incoherently propagated 
through the trained diffractive network (as in Eq . 9) using 𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20,000 to compute the output 
intensity  𝒐𝒐′. The mean of the error between 𝒐𝒐′ and 𝒐𝒐  over the 20,000 test examples was used to 
quantify the output error of the diffractive network for comparing different designs , as in Figs. 1 and 6 . 
For comparison between the ground truth  and the all -optical output intensities, e.g., in Figs. 3, 4, 8, 9 , 
10, we define d the scaled  all-optical output intensity vector  𝒐𝒐�=𝜎𝜎′
𝜎𝜎𝒐𝒐′ (see Supplementary Information  
for details ). 
For evaluating the intensity transformation 𝑨𝑨′ performed by the diffractive networks  at the end of the ir 
training, we used 𝑁𝑁𝑖𝑖 intensity vectors {𝒊𝒊𝑡𝑡}𝑡𝑡=1𝑁𝑁𝑖𝑖 where 𝒊𝒊𝑡𝑡[𝑙𝑙]=1 if 𝑙𝑙=𝑡𝑡 and 0 otherwise. In other words, 
{𝒊𝒊𝑡𝑡}𝑡𝑡=1𝑁𝑁𝑖𝑖 are unit impulse functions where the impulses are located at different in put pixels. We simulated 
the all-optical output intensity vectors {𝒐𝒐𝑡𝑡′}𝑡𝑡=1𝑁𝑁𝑖𝑖 corresponding to these input intensity vectors by 
incoherent propagation  and stacked  them  column by column, i.e.,   
𝑨𝑨′=�𝒐𝒐1′|𝒐𝒐2′|⋯|𝒐𝒐𝑁𝑁𝑖𝑖′� (10) 12 
 Considering the diffraction -efficiency-associated scaling mismatch between 𝑨𝑨′ and the target 
transformation 𝑨𝑨, we defined t he scaled diffractive network intensity transformation 𝑨𝑨�=𝜎𝜎𝐴𝐴𝑨𝑨′, where:  
𝜎𝜎𝐴𝐴=�∑ ∑ (𝑨𝑨[𝑝𝑝,𝑞𝑞])2𝑁𝑁𝑜𝑜
𝑝𝑝=1𝑁𝑁𝑖𝑖
𝑞𝑞=1
∑ ∑ (𝑨𝑨′[𝑝𝑝,𝑞𝑞])2𝑁𝑁𝑜𝑜
𝑝𝑝=1𝑁𝑁𝑖𝑖
𝑞𝑞=1(11) 
 
Supplementar y Information i ncludes:  
• The i ndirect approach  of training  
• The direct approach  of training  
• Supplementary Figure S1  
 
 
References: 
1. Lugt, A. V. Signal detection by complex spatia l filtering. IEEE Trans. Inf. Theory 10, 139–145 (1964). 
2. Heinz, R. A., Artman, J. O. & Lee, S. H. Matrix Multiplication by Optical Methods. Appl. Opt.  9, 2161–
2168 (1970).  
3. Goodman, J. W. & Woody, L. M. Method for performing complex- valued linear oper ations on 
complex-valued data using incoherent light. Appl. Opt.  16, 2611– 2612 (1977).  
4. Goodman, J. W., Dias, A. R. & Woody, L. M. Fully parallel, high -speed incoherent optical method for 
performing discrete Fourier transforms. Opt. Lett.  2, 1–3 (1978).  
5. Tamura, P. N. & Wyant, J. C. Two -Dimensional Matrix Multiplication using Coherent Optical 
Techniques. Opt. Eng.  18, 198– 204 (1979). 
6. Farhat, N. H., Psaltis, D., Prata, A. & Paek, E. Optical implementation of the Hopfield model. Appl. 
Opt.  24, 1469 –1475 (1985). 
7. Hotate, K. & Okugawa, T. Optical information processing by synthesis of the coherence function. J. 
Light. Technol.  12, 1247– 1255 (1994).  
8. Goodman, J. W. Introduction to Fourier Optics . (W. H. Freeman, 2005).  13 
 9. Silva, A. et al.  Performing Mathematical Operations with Metamaterials. Science  343, 160– 163 
(2014).  
10. Solli, D. R. & Jalali, B. Analog optical computing. Nat. Photonics  9, 704– 706 (2015).  
11. Athale, R. & Psaltis, D. Optical Computing: Past and Future. Opt. Photonics N ews 27, 32– 39 (2016).  
12. Wu, W., Jiang, W., Yang, J., Gong, S. & Ma, Y. Multilayered analog optical differentiating device: 
performance analysis on structural parameters. Opt. Lett.  42, 5270 –5273 (2017).  
13. Kwon, H., Sounas, D., Cordaro, A., Polman, A. & Alù, A. Nonlocal Metasurfaces for Optical Signal 
Processing. Phys. Rev. Lett.  121, 173004 (2018).  
14. Zuo, Y. et al.  All-optical neural network with nonlinear activation functions. Optica  6, 1132– 1137 
(2019).  
15. Hughes, T. W., Williamson, I. A. D., Minkov, M. & Fan, S. Wave physics as an analog recurrent neural 
network. Sci. Adv.  5, eaay6946 (2019).  
16. Spall, J., Guo, X., Barrett, T. D. & Lvovsky, A. I. Fully reconfigurable coherent optical vector –matrix 
multiplication. Opt. Lett.  45, 5752– 5755 (2020).  
17. Zangeneh -Nejad, F., Sounas, D. L., Alù, A. & Fleury, R. Analogue computing with metamaterials. Nat. 
Rev. Mater.  6, 207– 225 (2021).  
18. Mengu, D. et al.  At the intersection of optics and deep learning: stati stical inference, computing, 
and inverse design. Adv. Opt. Photonics  14, 209– 290 (2022).  
19. Stark, H. Application of Optical Fourier Transforms . (Elsevier Science, 2012).  
20. Yu, N. & Capasso, F. Flat optics with designer metasurfaces. Nat. Mater.  13, 139 –150 (2014).  
21. Kulce, O., Mengu, D., Rivenson, Y. & Ozcan, A. All -optical synthesis of an arbitrary linear 
transformation using diffractive surfaces. Light Sci. Appl.  10, 196 (2021).  
22. Lin, X. et al.  All-optical machine learning using diffractive deep neural networks. Science  361, 1004–
1008 (2018).  14 
 23. Mengu, D., Luo, Y., Rivenson, Y. & Ozcan, A. Analysis of Diffractive Optical Neural Networks and 
Their Integration With Electronic Neural Networks. IEEE J. Sel. Top. Quantum Electron.  26, 1 –14 
(2020).  
24. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature  521, 436– 444 (2015).  
25. Rahman, M. S. S., Li, J., Mengu, D., Rivenson, Y. & Ozcan, A. Ensemble learning of diffractive optical networks. Light Sci. Appl.  10, 14 (2021).  
26. Li, J. et al.  Spectrally  encoded single-pixel machine vision using diffractive networks. Sci. Adv.  7, 
eabd7690 (2021).  
27. Luo, Y. et al.  Computational imaging without a computer: seeing through random diffusers at the 
speed of light. eLight  2, 4 (2022).  
28. Rahman, M. S. S. & Ozcan, A. Computer -Free, All -Optical Reconstruction of Holograms Using 
Diffractive Networks. ACS Photonics  8, 3375– 3384 (2021).  
29. Mengu, D. & Ozcan, A. All -Optical Phase Recovery: Diffractive Computing for Quantitative Phase 
Imaging. Adv. Opt. Mater.  10, 2200281 (2022).  
30. Bai, B. et al.  To image, or not to image: class -specific diffractive cameras with all- optical erasure of 
undesired objects. eLight  2, 14 (2022).  
31. Rahman, M. S. S. & Ozcan, A. Time- lapse image classification using a diffractive neural network. 
Preprint at https://doi.org/10.48550/arXiv.2208.10802 (2022).  
32. Bai, B., Wei, H., Yang, X., Mengu, D. & Ozcan, A. Data class-specific all-optical transformations and 
encryption. Preprint at https://doi.org/10.48550/arXiv.2212.12873 (2022).  
33. Goi, E., Schoenhardt, S. & Gu, M. Direct retrieval of Zernike -based pupil functions using integrated 
diffractive deep neural networks. Nat. Commun.  13, 7531 (2022).  
34. Luo, X. et al.  Metasurface -enabled on-chip multiplexed diffractive neural networks in the visible. 
Light Sci. Appl.  11, 158 (2022).  15 
 35. Luo, Y. et al.  Design of task -specific optical systems using broadband diffractive neural networks. 
Light Sci. Appl.  8, 112 (2019).  
36. Veli, M. et al.  Terahertz pulse shaping using diffr active surfaces. Nat. Commun.  12, 37 (2021).  
37. Li, J., Hung, Y. -C., Kulce, O., Mengu, D. & Ozcan, A. Polarization multiplexed diffractive computing: 
all-optical implementation of a group of linear transformations through a polarization -encoded 
diffractive network. Light Sci. Appl.  11, 153 (2022).  
38. Li, J. et al.  Massively parallel universal linear transformations using a wavelength -multiplexed 
diffractive optical network. Adv. Photonics  5, 016003 (2023).  
39. Shannon, C. E. Communication in the Presence of Noise. Proc. IRE  37, 10– 21 (1949).  
40. Saleh, B. E. A. & Teich, M. C. Fundamentals of Photonics . (Wiley, 2007).  
41. Loshchilov, I. & Hutter, F. Decoupled Weight Decay Regularization. Preprint at 
https://doi.org/10.48550/arXiv.1711.05101 (2019).  
42. Paszke, A. et al.  PyTorch: An Imperative Style, High -Performance Deep Learning Library. in Advances 
in Neural Information Processing Systems  vol. 32 (Curran Associates, Inc., 2019).  
 
 
  16 
 Figures and Figure captions:  
 
Fig. 1:  All-optical  linear transformation  of intensity  performed by diffractive networks  under  spatially -
incoherent  illumination . (a) Schematic of a diffractive network formed by 𝐾𝐾=5 diffractive surfaces that  
all-optical ly perform a linear transformation of intensity between the input and output FOV s. The 𝑁𝑁 
diffractive features  are distributed evenly among the 𝐾𝐾=5 surfaces. (b) An arbitrary 𝑁𝑁𝑜𝑜×𝑁𝑁𝑖𝑖 matrix 𝑨𝑨, 
representing the target  intensity transformation to be  performed  all-optically by the diffractive network. 
Here 𝑁𝑁𝑖𝑖=82 and 𝑁𝑁𝑜𝑜=82 are the number of pixels at the input and the output FOVs of the diffractive 
network, respectively. (c) The expectation value of  the MSE between the all-optical output intensit y 𝒐𝒐′ 
and the ground -truth output intensit y 𝒐𝒐, as a function of 𝑁𝑁  for different  diffractive networks  trained 
using the indirect  approach . To simulate the incoherent propagation of intensity  for each  test input , we 
used  𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡=20000 . (d) Dependence of the calculated  output MSE on 𝑁𝑁𝜑𝜑, 𝑡𝑡𝑡𝑡, demonstrated  for network  
17 
 # 1E of Fig. 1c . The right y -axis shows the expec tation value of the residual magnitude of  1
𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡∑𝑒𝑒𝑗𝑗𝜃𝜃𝑖𝑖𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡
𝑖𝑖=1, 
where 𝜃𝜃𝑖𝑖~𝑈𝑈𝑛𝑛𝑖𝑖𝑈𝑈𝑜𝑜𝑈𝑈𝑚𝑚(0,2𝜋𝜋). 18 
  
Fig. 2:  All-optical  linear  transformations  of intensity , 𝑨𝑨�, performed  under spatially-incoherent 
19 
 illumination,  by five of the diffractive network d esigns shown in  Fig. 1c , together with the corresponding 
error matrices  with respect to the target transformation, 𝜺𝜺 =�𝑨𝑨−𝑨𝑨��. Here |∙| denotes elementwise 
operation.  The means of the error matrix elements are listed in t he table on the right . 20 
  
Fig. 3:  All-optical  linear  transformation of structured intensity patterns such as letters U, C, L, and A by  
the same diffractive networks  as in Fig. 2 , accompanied by the patterns  resulting from the numerical 
21 
 inverse mapping of  the all -optical outputs through multiplication by 𝑨𝑨−𝟏𝟏.  
 
Fig. 4:  Same as Figure 3, except for  the test intensity  patterns formed by closely separated  lines  and 
points.  
22 
  
Fig. 5:  Effect of the diffractive network ’s depth, i.e., the number of diffractive surfaces (𝐾𝐾), on the 
23 
 approximation perform ance  for an arbitrary intensity linear transformation  under spatially-incoherent 
illumination.   All-optical  linear  transformations  of intensity , 𝑨𝑨�, performed by four diffractive network 
designs  with approximately equal 𝑁𝑁 and increasing 𝐾𝐾, are shown, together with the corresponding error  
matrices  with respect to the target transformation,  i.e., 𝜺𝜺=�𝑨𝑨−𝑨𝑨��. Here |∙| denotes elementwise 
operation.  The mean  values of the error matrix elements are listed in the table on the right.  
 
 
Fig. 6:  All-optical  linear transformation  of intensity  under spatially -incoherent illumination,  by diffractive 
networks  trained using the direct  approach . (a) The e xpectation value of the MSE between the all-
optical output intensity  𝒐𝒐′ and the ground -truth output intensit y 𝒐𝒐, as a function of 𝑁𝑁 for different  
diffractive networks  trained using the direct approach . (b) Dependence of the calculated  output MSE on 
𝑁𝑁𝜑𝜑, 𝑡𝑡𝑡𝑡, demonstrated for network # 2 E of Fig. 6a . The right y -axis shows the expec tation value of the 
residual magnitude of  1
𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡∑𝑒𝑒𝑗𝑗𝜃𝜃𝑖𝑖𝑁𝑁𝜑𝜑,𝑡𝑡𝑡𝑡
𝑖𝑖=1, where 𝜃𝜃𝑖𝑖~𝑈𝑈𝑛𝑛𝑖𝑖𝑈𝑈𝑜𝑜𝑈𝑈𝑚𝑚(0,2𝜋𝜋). 
24 
  
Fig. 7:  All-optical  linear  transformations  of intensity , 𝑨𝑨�, performed under spatially -incoherent 
25 
 illumination by five of the diffractive network d esigns shown in  Fig. 6a, together with the corresponding 
error matrices  with respect to the target transformation, 𝜺𝜺 =�𝑨𝑨−𝑨𝑨��. Here |∙| denotes elementwise 
operation.  The mean  values of the error matrix elements are listed in the table on the right.  26 
  
Fig. 8:  All-optical  linear transformation of structured intensity patterns such as letters U, C, L, and A by  
the same diffractive networks  as in Fig. 7, accompanied by the patterns  resulting from the numerical 
27 
 inverse mapping of  the all -optical outputs through multiplication  by 𝑨𝑨−𝟏𝟏. 
 
 
Fig. 9:  Same as Figure 8, except for  the test intensity  patterns formed by closely separated  lines  and 
points . 
28 
  
Fig. 10:  Approximation of an arbitrary non -invertible  linear  transformation (𝑨𝑨) of intensity, under 
spatially -incoherent illumination, by a diffractive network ( 𝐾𝐾=5, 𝑁𝑁=5×582) trained using the 
29 
 indirect approach. (a) The target transformation  𝑨𝑨, the all-optical intensity transformation  𝑨𝑨� performed 
by the  trained  diffractive network  and the error  matrix 𝜺𝜺=�𝑨𝑨−𝑨𝑨��. Here |∙| denotes elementwise 
operation. (b) All-optical transformation of different test intensity patterns  by the trained  diffractive 
network , together with the corresponding ground truths . 
 