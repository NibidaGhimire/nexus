1 
 Universal Linear Intensity Transformations 
Using Spatially -Incoherent  Diffractive 
Processor s 
 
Md Sadman Sakib Rahman1,2,3  mssr@ucla.edu  
Xilin Yang1,2,3    mikexlyang@ucla.edu  
Jingxi Li1,2,3    jxlli@ucla.edu 
Bijie Bai1,2,3    baibijie@ucla.edu  
Aydogan Ozcan1,2,3,*  ozcan@ucla.edu   
1Electrical and Computer Engineering Department, University of California, Los Angeles, CA, 90095, USA  
2Bioengineering Department, University of California, Los Angeles, CA, 90095, USA  
3California NanoSystems Institute (CNSI), University of California, Los Angeles, CA, 90095, USA  
*Corresponding author: ozcan@ucla.edu  
 
Abstract  
Under spatially -coherent light , a diffractive optica l network composed  of structured  surfaces can be 
designed  to perform any arbitrary complex -valued linear transformation between its input and output 
fields-of -view (FOV s) if the total number  (ğ‘ğ‘) of optimizable phase-only diffractive features  is â‰¥~2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ, 
where ğ‘ğ‘ğ‘–ğ‘– and ğ‘ğ‘ğ‘œğ‘œ refer to  the number of useful pixels at the input and the output FOVs , respectively. 
Here we report the design of a spatially -incoherent diffractive optical processor that can approximate 
any arbitrary linear transformation in tim e-average d intensity between its input and output FOVs . Under  
spatially-incoherent monochromatic light,  the spatially -varying intensity point  spread  function ( ğ»ğ») of a 
diffractive network , corresponding to a  given, arbitrarily -selected linear intensity transformation , can be 
written as ğ»ğ»(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)=|â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)|2, where â„ is the spatially-coherent point -spread  function 
of the same diffractive network , and (ğ‘šğ‘š,ğ‘›ğ‘›) and (ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) define the coordinates of the output and input 
FOVs, respectively. Using deep learning, supervised through examples of input -output profiles, we 
numerically demonstrate that a  spatially -incoherent  diffractive network can  be trained to all-optically 
perform any arbitrary linear intensity transformation between its input and output  if ğ‘ğ‘ â‰¥ ~2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ. 
These results constitute the first demonstration of univers al linear intensity transformations  performed 
on an input FOV  under spatially-incoherent illumination and will be useful for designing  all-optical visual  
processors that can work with incoherent , natural  light.  
 
Introduction  
Spatial information processing with free-space optics has been widely explored  for a long time and 
predates the proliferation of electronic computing  1â€“18. The inherent transformation of  optical fields as 
they propagate through free space , known as diffraction,  togeth er with the ability for wavefront 2 
 modulation with compact hardware, makes low-cost and passive spatial information processing at the 
speed of light propagation possible 19â€“21. In recent years , diffractive optical network s comprising a set of 
spatially -engineered surfaces to  perform computation through passive light -matter -interaction have 
emerged as  powerful  all-optical processor s 22,23. Designed utilizing deep learning  24, such  coherent  
diffractive optical  processors have demonstrated versatile applications,  including  statistical inference as 
well as deterministic tasks23,25â€“ 31 across the spectrum  from terahertz to near -infrared32 and visible33,34. 
Information processing with a diffractive network involves local modulation of the amplitude and/or the 
phase of the incident optical wave by structu red surfaces containing  diffractive neurons/ features , each 
with a lateral size  of ~Î»/2, where Î»  is the wavelength of the spatially -coherent  illumination light . The 
entire propagation of a spatially -coherent  wave from the input plane to the output FOV compr ises such  
optical  modulations by ğ¾ğ¾ spatially -optimized diffractive surfaces , which in total contain  ğ‘ğ‘ independent 
diffractive features  (for example, evenly  distributed over the ğ¾ğ¾ diffractive surfaces ). These ğ‘ğ‘ diffractive 
features  represent the complex -valued transmission coefficients, forming  the independent degrees  of 
freedom of the diffractive processor, which can be  optimized to all -opticall y execute  different tasks22,26â€“
30,35,36. It was shown that a spatially -coherent diffractive optical network c ould  be trained to perform any 
arbitrary complex -valued  linear transformation between its  input and output FOVs  if ğ‘ğ‘â‰¥ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ, where 
ğ‘ğ‘ğ‘–ğ‘– and ğ‘ğ‘ğ‘œğ‘œ refer to  the number of useful  (diffraction -limited)  pixels  at the input  and the output FOVs  21. 
For a phase -only diffractive network where the transmission coefficients of the diffractive features of 
each structured surface only modulate the phase information of light , the requirement  for universal 
linear transformations increase s to ğ‘ğ‘â‰¥2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ due to  the reduce d degrees of freedom that can be 
optimized independently .  
For a given complex -valued linear transformation that a coherent diffractive network is designed  to 
approximate, any arbitrary point on th e input plane defined by (ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) will result in a  unique  complex -
valued coherent point spread function ( â„) at the output FOV defined by (ğ‘šğ‘š,ğ‘›ğ‘›). This 4 -dimensional 
complex-valued function,  â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²), that map s the input and output FOVs  represents a spatially -
varying coherent point spread function. Stated differently, unlike traditional spatially -invariant imaging 
systems, a coherent diffractive optical network provides a framework to approximate any arbitrary 
â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) that corresponds to an arbitrar ily-selected complex- valued linear transformation 
between its input and output FOVs. It was also shown that different /independent  complex -valued linear  
transformations  could  be multiplexed in a single spatially -coherent diffractive processor by utilizing 
polarization and wavelength diversity37,38. 
All of these earlier studies on universal linear transformations implemented  in free -space  through 
diffractive processors  were based on spatially -coherent illumination. In this paper, we report the first 
demonstration of universal linear transformations in optical intensity performed under spatially -
incoherent monochromatic illumination  of an input FOV . We show that, u nder  spatially -incoherent  light,  
a diffractive optical processor can perform any arbitrary linear transformation of time -average d 
intensities between its  input and the output FOVs . Our numerical analyses  revealed that phase-only 
diffractive optical processors  with  a shallow architecture ( for example,  having  a single trainable 
diffractive surface ) are unable to accurately  approximate an arbitrary intensity transformation 
irrespective of the total number  (ğ‘ğ‘) of diffractive features available for optimization; on the contrary,  
phase-only diffractive optical processors  with deeper architectures  (one  diffractive  layer  following 
others ) can perform an arbitrary intensity linear transformation using spatially -incoherent  illumination  
with  a negligible error when  ğ‘ğ‘â‰¥2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ. These analyses and conclusions are important for  all-optical 3 
 information processing and visual co mputing systems that us e spatially -incoherent  light , such as in 
natural scenes ; this framework  can also find applications in  computational  microscopy  and incoherent  
imagin g through point spread function engineering, among others . 
Results  
Theoretical analysis  
Spatially -coherent  monochromatic d iffractive optical networks can be characterized by a 4 -dimensional 
complex-valued coherent  impulse response function  (i.e., the point spread function) that is spatially -
varying , connecting the input and output FOVs : â„(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦;ğ‘¥ğ‘¥â€²,ğ‘¦ğ‘¦â€²). Stated differently, e ach arbitrarily-
selected complex -valued linear transformation that is desired between the pixels of an input FOV and 
output FOV results in a spatially -varying impulse response function â„(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦;ğ‘¥ğ‘¥â€²,ğ‘¦ğ‘¦â€²), where (ğ‘¥ğ‘¥â€²,ğ‘¦ğ‘¦â€²) and 
(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦) define the input and output FOVs, respectively. Based on this definition, the complex -valued 
output field  ğ‘œğ‘œğ‘ğ‘(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦) of a spatially -coherent  diffractive processor  is related to the complex -valued input 
field ğ‘–ğ‘–ğ‘ğ‘(ğ‘¥ğ‘¥â€²,ğ‘¦ğ‘¦â€²) by: 
ğ‘œğ‘œğ‘ğ‘(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)=ï¿½â„ğ‘ğ‘(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦;ğ‘¥ğ‘¥â€²,ğ‘¦ğ‘¦â€²)ğ‘–ğ‘–ğ‘ğ‘(ğ‘¥ğ‘¥â€²,ğ‘¦ğ‘¦â€²)ğ‘‘ğ‘‘ğ‘¥ğ‘¥â€²ğ‘‘ğ‘‘ğ‘¦ğ‘¦â€² (1) 
The subscript ğ‘ğ‘  indicates  that the quantities are functions of continuous spatial variables ğ‘¥ğ‘¥, ğ‘¦ğ‘¦, ğ‘¥ğ‘¥â€², ğ‘¦ğ‘¦â€², 
representing the transverse coordinates on the output and input planes . If these optical fields are  
sampled at an interval (ğ›¿ğ›¿) sufficiently small to preserve the spatial variations,  satisfying the Nyquist 
criterion39, one can write : 
ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)=ï¿½â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) ğ‘–ğ‘–(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)
ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²(2) 
Here, ğ‘šğ‘š, ğ‘›ğ‘›, ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€² refer to  discrete indices such that  ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)=ğ‘œğ‘œğ‘ğ‘(ğ‘šğ‘šğ›¿ğ›¿,ğ‘›ğ‘›ğ›¿ğ›¿) and ğ‘–ğ‘–(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)=
ğ‘–ğ‘–ğ‘ğ‘(ğ‘šğ‘šâ€²ğ›¿ğ›¿,ğ‘›ğ‘›â€²ğ›¿ğ›¿). The instantaneous output intensity can be written as : 
|ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)|2=ï¿½â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) â„âˆ—(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²) |ğ‘–ğ‘–(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)| |ğ‘–ğ‘–(ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²)| ğ‘’ğ‘’ğ‘—ğ‘—ï¿½ğœ‘ğœ‘ï¿½ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²ï¿½âˆ’ğœ‘ğœ‘ï¿½ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²ï¿½ï¿½
ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²,ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²(3) 
where ğœ‘ğœ‘(.) is the phase function  of the input field ğ‘–ğ‘–, i.e., ğ‘–ğ‘–=|ğ‘–ğ‘–|ğ‘’ğ‘’ğ‘—ğ‘—ğœ‘ğœ‘, and â„âˆ— denotes the complex 
conjugate of  â„. The time -average d output intensity can be written as :  
ğ‘‚ğ‘‚(ğ‘šğ‘š,ğ‘›ğ‘›)=âŒ©|ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)|2âŒª=ï¿½â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) â„âˆ—(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²)|ğ‘–ğ‘–(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)||ğ‘–ğ‘–(ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²)|âŒ©ğ‘’ğ‘’ğ‘—ğ‘—âˆ†ğœ‘ğœ‘âŒª
ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²,ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²(4) 
where âŒ©âˆ™âŒª denotes  time -average  operation  and âˆ†ğœ‘ğœ‘=ğœ‘ğœ‘(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)âˆ’ğœ‘ğœ‘(ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²). Since  the illumination 
light is  spatially -incoherent , the phases at different spatial points of the input vary randomly  over time 
and are independent  of each  other .40 Stated differently for stationary objects/scenes that are uniformly 
illuminated with a spatially -incoherent light, âˆ†ğœ‘ğœ‘ varies randomly between 0  and 2ğœ‹ğœ‹ over time, yielding 
âŒ©ğ‘’ğ‘’ğ‘—ğ‘—âˆ†ğœ‘ğœ‘âŒª=0 for (ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)â‰ (ğ‘šğ‘šâ€²â€²,ğ‘›ğ‘›â€²â€²). As a result of this , under spatially-incoherent il lumination, Eq. (4) 
can be written as : 
ğ‘‚ğ‘‚(ğ‘šğ‘š,ğ‘›ğ‘›)=ï¿½|â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)|2 âŒ©|ğ‘–ğ‘–(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)|2âŒª
ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²=ï¿½ğ»ğ»(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) ğ¼ğ¼(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)
ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²(5) 4 
 where ğ¼ğ¼=  âŒ©|ğ‘–ğ‘–|2âŒª is the time -average d input intensity and ğ»ğ»(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)=|â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)|2 is the 
intensity impulse response  of the diffractive optical processor under spatially-incoherent illumination. 
From now on, unless otherwise stated, we use the term optical â€˜intensityâ€™ to imply  time -average d 
intensity  functions . Similarly, whenever all-o ptical linear transformation of intensity is mentioned, 
spatially-incoherent  monochromatic  illumination is implied  unless stated otherwise . 
We should emphasize that while ğ»ğ»(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)=|â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)|2, we have  in general ğ‘‚ğ‘‚(ğ‘šğ‘š,ğ‘›ğ‘›)â‰  
|ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)|2. Therefore, the output intensity of a spatially -incoherent diffractive network cannot be 
calculated as |ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)|2=ï¿½âˆ‘â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) ğ‘–ğ‘–(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€² ï¿½2. For the numerical forward model  
corresponding to  each input object, as will be detailed in the next section, we used a large number of 
random phase distributions at the input plane to approximate  ğ‘‚ğ‘‚(ğ‘šğ‘š,ğ‘›ğ‘›)=âŒ©|ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)|2âŒª under spatially-
incoherent  illumination.  
Numerical analysis  
In this subsection, we numerically explore the design of diffractive optical processors to perform an 
arbitrary linear intensity  transformation between the input and the output FOVs  under  spatially -
incoherent  illumination. We assume, as shown in Fig. 1a, ğ‘ğ‘  independent diffractive features  (phase -only 
elements) that are distributed over ğ¾ğ¾ diffractive surfaces, each with ğ‘ğ‘ ğ¾ğ¾â„ diffractive features , between 
the input and output planes. Following from Eq. (5), i f we rearrange the pixel intensities of  ğ¼ğ¼(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) and 
ğ‘‚ğ‘‚(ğ‘šğ‘š,ğ‘›ğ‘›) as column vector s ğ’Šğ’Š and ğ’ğ’, then we can write ğ’ğ’=ğ‘¨ğ‘¨â€² ğ’Šğ’Š, where  ğ‘¨ğ‘¨â€² represents the linear intensity 
transformation  performed  by the diffractive optical network  under spatially-incoherent illumination. 
The elements of ğ‘¨ğ‘¨â€² correspond to the elements of the intensity impulse response ğ»ğ» (ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²); see 
Eq. (5) . Note  that all the  elements of ğ‘¨ğ‘¨â€² are real and nonnegative  since it represents a linear intensity 
transformation  with ğ»ğ»(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)=|â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)|2. Hence, in the  context of arbitrary linear 
transformation s in intensity, only real transformation matrices with nonnegative elements are 
consi dered.  
For our  target linear transformation that is  to be approximated by the spatially- incoherent  diffractive 
processor, initially, we selected an arbitrary matrix  ğ‘¨ğ‘¨, as shown in Fig. 1b . In the following numerical 
analysis, we optimize ğ‘ğ‘ diffractive features  of a phase -only diffractive processor so that  ğ‘¨ğ‘¨â€²â‰ˆ ğ‘¨ğ‘¨ under 
spatially-incoherent  illuminatio n. The size of ğ‘¨ğ‘¨ is chosen as  ğ‘ğ‘ğ‘œğ‘œÃ—ğ‘ğ‘ğ‘–ğ‘–=64Ã—64, i.e., the number of 
pixels at  both the input ( ğ‘ğ‘ğ‘–ğ‘–) and the output ( ğ‘ğ‘ğ‘œğ‘œ) FOVs  are 8 Ã— 8 , arranged in a  square grid.  Each element 
of the matrix ğ‘¨ğ‘¨ is randomly sampled from a uniform probability distribution between 0 and 1, i.e., 
ğ‘¨ğ‘¨[ğ‘ğ‘,ğ‘ğ‘]~ğ‘ˆğ‘ˆğ‘›ğ‘›ğ‘–ğ‘–
ğ‘ˆğ‘ˆğ‘œğ‘œğ‘ˆğ‘ˆğ‘šğ‘š(0, 1) wh
ere ğ‘¨ğ‘¨[ğ‘ğ‘,ğ‘ğ‘] is the element at ğ‘ğ‘-th row and ğ‘ğ‘-th column of ğ‘¨ğ‘¨, ğ‘ğ‘= 1, â€¦ ,ğ‘ğ‘ğ‘œğ‘œ 
and ğ‘ğ‘= 1, â€¦ ,ğ‘ğ‘ğ‘–ğ‘–.  
For the deep learning-based optimiz ation of  the design of a phase- only diffractive processor to achieve 
ğ‘¨ğ‘¨â€²â‰ˆ ğ‘¨ğ‘¨ , we followed two different  data -driven supervised learning approach es: (1) the indirect 
approach  and (2) the direct approach. In the indirect  approach,  instead of directly training the diffractive 
network to perform the linear intensity transformation ğ‘¨ğ‘¨, we trained the network , under spatially-
coherent  illumination, to perform the complex-value d linear transformation ğ‘¨ğ‘¨ ï¿½ between the input and 
output FOVs  such that ï¿½ğ‘¨ğ‘¨ ï¿½[ğ‘ğ‘,ğ‘ğ‘]ï¿½=ï¿½ğ‘¨ğ‘¨[ğ‘ğ‘,ğ‘ğ‘], which would result in  an intensity linear transformation 
ï¿½ğ‘¨ğ‘¨ï¿½[ğ‘ğ‘,ğ‘ğ‘]ï¿½2=ğ‘¨ğ‘¨[ğ‘ğ‘,ğ‘ğ‘] under spatially -incoherent  illumination. For the purpose of the training, we defined 
the phase of ğ‘¨ğ‘¨ï¿½[ğ‘ğ‘,ğ‘ğ‘] to be zero, i.e., ğ‘¨ğ‘¨ ï¿½[ğ‘ğ‘,ğ‘ğ‘]=ï¿½ğ‘¨ğ‘¨[ğ‘ğ‘,ğ‘ğ‘]exp(ğ‘—ğ‘—0); however, any other phase distribution 
could also be used since the design space is not unique. Stated differently, i n this indirect approach, we 5 
 design a diffractive network that can achieve a spatially-coherent  impulse response  â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²), 
which will ensure that the same design has a spatially -incoherent  impulse response of  ğ»ğ»(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)=
|â„(ğ‘šğ‘š,ğ‘›ğ‘›;ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)|2 such that ğ‘¨ğ‘¨â€²â‰ˆğ‘¨ğ‘¨ can be satisfied under spatially -incoherent  illumination.  To achieve 
this goal,  we used the relationship ğ’ğ’ï¿½=ğ‘¨ğ‘¨ï¿½ğ’Šğ’ŠÌƒ to generate a large set of input -target complex -valued optical 
field pairs (ğ’Šğ’ŠÌƒ,ğ’ğ’ï¿½), and used deep learning to optimize  the phase values of the diffractive feature s by 
minimizing the mean  squared  error (MSE) loss between the target  complex field ğ’ğ’ï¿½ and the complex  field  
ğ’ğ’ï¿½â€² obtained by coherent ly propagati ng ğ’Šğ’ŠÌƒ through the diffractive network  (see the Methods section) . In 
other words , spatially -coherent  design of a diffractive network is used here as a proxy for th e design of a 
spatially -incoherent  diffractive network that can achieve any arbitrary intensity linear transformation 
between its input and output FOVs.  
In the second  approach  (termed the direct approach),  we trained the diffractive network to perform the 
desired intensity linear transformation ğ‘¨ğ‘¨ between the input and the output FOVS, by directly using the 
relationship  ğ’ğ’=ğ‘¨ğ‘¨ ğ’Šğ’Š to generate a large set of input -target intensity  pairs (ğ’Šğ’Š,ğ’ğ’). Using this large training 
set of input/output intensity patterns , we optimize d the transmission phase values of the diffractive 
layers  using deep learning,  by minimizing the MSE  loss between the output  pixel intensities of  the 
diffractive processor  ğ’ğ’â€² and the ground -truth intensit ies ğ’ğ’ (see the Methods section).  During the training 
phase, t he output intensity of the diffractive processor was simulated through the incoherent 
propagation of the input intensity  patterns , ğ’Šğ’Š or ğ¼ğ¼(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²). To numerically simulat e the spatially -
incoherent  propagation of  ğ¼ğ¼(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²), we assume d the input optical field to be âˆšğ¼ğ¼ğ‘’ğ‘’ğ‘—ğ‘—ğœ‘ğœ‘ where  ğœ‘ğœ‘ is a 
random 2D phase  distribution, i.e., ğœ‘ğœ‘(ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²)~ğ‘ˆğ‘ˆğ‘›ğ‘›ğ‘–ğ‘–ğ‘ˆğ‘ˆğ‘œğ‘œğ‘ˆğ‘ˆğ‘šğ‘š(0,2ğœ‹ğœ‹) for each (ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²). This input  field with 
the random phase distribution ğœ‘ğœ‘ was coherently propagated through the diffractive surfaces  to the 
output plane, using the angular spectrum approach22. We repeat ed this coherent w ave propagation ğ‘ğ‘ğœ‘ğœ‘ 
times  for every ğ’Šğ’Š , each time with a different random phase ğœ‘ğœ‘ (ğ‘šğ‘šâ€²,ğ‘›ğ‘›â€²) distribution , and average d the 
resulting ğ‘ğ‘ğœ‘ğœ‘ output intensities. As ğ‘ğ‘ğœ‘ğœ‘â†’âˆ, the average intensity would approach the theoretical time -
average d output intensity for spatially -incoherent  illumination , i.e., ğ‘‚ğ‘‚(ğ‘šğ‘š,ğ‘›ğ‘›)=âŒ©|ğ‘œğ‘œ(ğ‘šğ‘š,ğ‘›ğ‘›)|2âŒª. Due to the 
limited availability of  computational resources,  for the direct training  (the second design appr oach)  of 
the spatially -incoherent  diffractive optical processors , we used  ğ‘ğ‘ğœ‘ğœ‘=ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=1000 . 
The diffractive models reported in Figs. 1 -5 and 10 were trained using the indirect approach while the 
ones in Figs. 6 -9 were trained using the direct approach. All the diffractive networks reported in this 
work, after their training using either the direct or indirect design approaches,  were evaluated and  
blindly tested through the incoherent propagation of input intensities  with ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20,000. Since the 
testing is computationally less cumbersome compared to the training,  we used ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20,000â‰«ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡.  
After the training phase, we tested the resulting diffractive processor designs using 20,000 test intensity 
patterns ğ’Šğ’Š that were never used during training; the size of this testing intensity set (20,000) should not 
be confused with ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20,000 since for each input intensity test pattern  of this set , we used  ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=
20,000 random 2D phase patterns to compute the corresponding spatially -incoherent  output intensity . 
In Fig. 1c , the approximation error s of eight different phase-only diffractive processors trained using the 
indirect approach, each with  ğ¾ğ¾=5 diffractive layers , are reported  as a function of ğ‘ğ‘. The mean error  
(Fig. 1c ) for  each diffractive des ign w as calculated at the output intensity  patterns ğ’ğ’â€² with respect to the 
ground truth ğ’ğ’ =ğ‘¨ğ‘¨ğ’Šğ’Š, by averaging over  the 20,000 test intensity patterns . Figure 1c reveals  that the 
approximation error of the spatially-incoherent  diffractive processors  reaches a minimum level  as ğ‘ğ‘
2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ 
approaches 1, and stays at the same level for  ğ‘ğ‘â‰¥2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ.  6 
 To understand the impact  of ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡ on these approximation  error calculations , we took the diffractive 
processor design #  1E shown in Fig. 1c  (i.e., ğ¾ğ¾=5,ğ‘ğ‘â‰ˆ2.1Ã—2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ), and used different ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡ values at 
the blind testing phase for evaluating the average test error on the same intensity test set  composed of 
20,000 patterns ğ’Šğ’Š. As shown  in Fig. 1d, the computed error values  decrease as ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡ increases , as 
expected . On the right y -axis of the s ame Figure  1d, we also show , as a function of ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡, the 
expec tation  value of  ï¿½1
ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡âˆ‘ğ‘’ğ‘’ğ‘—ğ‘—ğœƒğœƒğ‘–ğ‘–ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡
ğ‘–ğ‘–=1ï¿½, where ğœƒğœƒğ‘–ğ‘–~ğ‘ˆğ‘ˆğ‘›ğ‘›ğ‘–ğ‘–ğ‘ˆğ‘ˆğ‘œğ‘œğ‘ˆğ‘ˆğ‘šğ‘š(0,2ğœ‹ğœ‹). This expect ation value of the  
residual magnitude of 1
ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡âˆ‘ğ‘’ğ‘’ğ‘—ğ‘—ğœƒğœƒğ‘–ğ‘–ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡
ğ‘–ğ‘–=1 decreases as ğ‘ğ‘ ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡ increases and would approach zero  as ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡â†’
âˆ. The numerically simulated output intensity of a diffractive processor design approach es the true 
time -averaged intensity of the spatially -incoherent  wave  as ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡ gets larger , following a similar trend as 
ï¿½1
ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡âˆ‘ğ‘’ğ‘’ğ‘—ğ‘—ğœƒğœƒğ‘–ğ‘–ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡
ğ‘–ğ‘–=1ï¿½, reported in Fig. 1d. This comparison also highlights the fact that our choice of using 
ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20,000 random 2D phase patterns to compute the spatially-incoherent  output intensit y 
patterns  in the blind testing phase is an accurate approximation.  
Next , we show in Fig. 2  the scaled intensity linear transformations, ğ‘¨ğ‘¨ ï¿½, that were approximated by five  of 
the trained diffractive networks of Fig. 1c . ğ‘¨ğ‘¨ï¿½ is related to the physical transformation ğ‘¨ğ‘¨â€² by a scalar 
factor ğœğœğ´ğ´ (see the â€˜Evaluationâ€™ subsection in  â€˜Methods â€™ section ) which  compensates  for diffraction 
efficiency-related optical losses. We also show the error matrix with respect to the target ğ‘¨ğ‘¨, i.e., ğœºğœº=
ï¿½ğ‘¨ğ‘¨âˆ’ğ‘¨ğ‘¨ï¿½ï¿½, and report the average of the error matrix elements in the table on the right.  Here |âˆ™| denotes 
the elementwise operation.  As ğ‘ğ‘ increases, t he diffractive networksâ€™ resulting  matrices resemble the 
ground truth  target better and the approximation  error decreases steadily ; however , the improvement 
is more prominent as ğ‘ğ‘ approaches 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ and stagnates beyond ğ‘ğ‘â‰ˆ2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ.  
To provide visually more noticeable  illustrations of  the diffractive network sâ€™ all -optical intensity  
transformations  under spatially -incoherent  illumination , we use d structured intensity patterns such as 
the letters U, C, L, and A as input  intensity  to the diffractive networks (see Fig. 3 ). Because of the 
randomness of the elements of the intensity transformation matrix, the output pixel intensities also 
appear random (harder to compare visually against the ground truth) . However, t he reappearance of 
the letters after a numerical inversion thr ough  the multiplication of the  scaled  output intensity ğ’ğ’ ï¿½  by the 
inverse of the target transformation, ğ‘¨ğ‘¨âˆ’1, would indicate  ğ‘¨ğ‘¨ï¿½â‰ˆğ‘¨ğ‘¨ and validat e the correctness of the 
diffractive network sâ€™ approximations  in a visually noticeable  manner  (see the â€˜Evaluationâ€™ subsection of 
the Methods section  for the definition of ğ’ğ’ ï¿½). In the case of  the diffractive network  # 1A (ğ¾ğ¾=5, ğ‘ğ‘=
5Ã—382â‰ˆ0.88Ã—2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ), the result of such  an inversion does not quite reveal  any recognizable 
patterns, indicating the near -failure of the all-optical approximation  of this design # 1A . However , such 
inversion reveals the  recognizable patterns (U, C, L , and A) as ğ‘ğ‘ approaches 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ (design  # 1B) and 
becomes identical to the in puts as ğ‘ğ‘  exceeds 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ (e.g., design # 1C). These results  show  that for the 
ğ¾ğ¾=5 phase-only diffractive networks  with a sufficient ly large  ğ‘ğ‘â‰¥~2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ, we have  ğ‘¨ğ‘¨ï¿½â‰ˆğ‘¨ğ‘¨, indicating 
that these  networks could  faithfully approximate the target  intensity linear  transformation  under 
spatially -incoherent  illumination.  
For computational imaging and sensing applications , such as in microscopy, exploring patterns of closely 
spaced lines and points would be interestin g. Motivated by this , we repeat ed the same procedures 
outlined in Fig. 3 for various intensity patterns consisting of  closely separated  line pairs  and sets of 7 
 points , the results  of which are summarized in  Fig. 4. The same  conclusions drawn previously  in Fig. 3  
hold : for ğ‘ğ‘ â‰¥ ~2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ we have ğ‘¨ğ‘¨ï¿½â‰ˆ ğ‘¨ğ‘¨.  
We also investigated the dependence of the all-optical approximation of intensity linear transformation s 
on the number of  diffractive  layers ğ¾ğ¾; see  Fig. 5 . The results of this analysis reveal that even with ğ‘ğ‘ â‰ˆ
2 Ã— 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ, ğ¾ğ¾= 1 and ğ¾ğ¾= 2 diffractive designs failed to approximate the target linear transformation  
despite having a  large ğ‘ğ‘, whereas the designs with ğ¾ğ¾> 2 succe ssfully approximated the target 
transformation under spatially-incoherent illumination . This confirms  that the depth of the diffractive 
network  design  is a key architectural  factor in the  computational  capacity of diffractive processors to 
perform arbitrary linear transformation s21,22,37,38.  
Next, we present the blind testing  results of the diffractive processors  that were trained  using the 
second design approach (i.e., direct approach ), to perform the same arbitrary intensity linear 
transformation as has been considered so far.  In Fig. 6a, the approximation error s of eight different 
phase-only diffractive processors trained using the direct approach, each with  ğ¾ğ¾= 5 diffractive layers , 
are reported as a function of ğ‘ğ‘ . The mean error was calculated over the same 20,000 test intensity 
patterns used in  Fig.1c;  for each test intensity pattern, the incoherent output intensity ğ’ğ’â€² was calculated 
using ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20000  (same as before ). In these alternative diffractive designs , the approximation error 
of the diffractive processors  reaches a minimum level as ğ‘ğ‘
2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ approaches 1, and stays at the same level 
for ğ‘ğ‘ â‰¥ 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ â€“ the same conclusion that we reached for the indirect designs reported  earlier . 
However, compared with the previous designs that used the indirect approach, here, the minimum  error 
level obtained using the direct approach is approximately three times  higher . This can be attributed to 
the use of a relatively small ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=1000  during the training, and these designs can be further 
improved by increasing ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡ using a longer training effort with more computational resources . 
In Fig. 7 , we show the scaled li near intensity transformations, ğ‘¨ğ‘¨ï¿½, that were approximated by f ive of the 
trained diffractive networks of Fig. 6a . For each case, we also show the error matrix with respect to the 
target ğ‘¨ğ‘¨, i.e., ğœºğœº=ï¿½ğ‘¨ğ‘¨âˆ’ğ‘¨ğ‘¨ï¿½ï¿½, and report the average of the error matrix elements in the table on the right.  
As ğ‘ğ‘ increases, the mean intensity transformation error decreases, except for design # 2B which we 
believe is an outlier resulting from poor convergence. The relatively large error  of the  design  # 2B is due 
to the diffr action efficiency imbalance  among the individual input pixels, as evident from the uneven 
magnitudes across the columns of ğ‘¨ğ‘¨ï¿½. Similarly, the other  designs  of the direct approach  reveal uneven 
magnitudes across the columns of ğœºğœº, indicating some diffraction efficiency im balance among the 
individual input pixels, albeit not as severe as the  design  # 2B. Despite such imperfections , these 
diffractive  network s designed using the direc t approach  effectively learned the target intensity 
transformation, as evident from Figs. 8 and 9. Figure 8 reveals that , for all the designs,  the multiplication 
of the output intensity patterns  ğ’ğ’ï¿½ by the inverse of the target transformation, ğ‘¨ğ‘¨âˆ’1 brings back the 
patterns U, C, L, A. Although, the reconstruction quality is better for  ğ‘ğ‘ â‰ˆ 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ and remains similar 
beyond ğ‘ğ‘> 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ, the improvement is not as sharp as it was with the indirect approach  (see Fig. 8 vs. 
Fig. 3 and Fig. 9 vs. Fig. 4) . In contrast with  the diffractive networks designed using the indirect 
approach , here in this case, the diffractive networks  with ğ‘ğ‘ < 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ (e.g., design # 2A ) succeeded in 
approximating the linear transformation to the extent of revealing recognizable patterns after a 
numerical inverse mapping. These same observations also hold for the intensity patterns that consist of  
closely spaced lines and points , as shown in Fig. 9.  8 
 Finally , we report in Fig. 10  the performance of a diffractive network  (ğ¾ğ¾=5, ğ‘ğ‘â‰ˆ2Ã—2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ) trained 
using the indirect approach to approximate another arbitrary intensity linear transformation, defined by 
a non-invertible  matrix.  The target transformation ğ‘¨ğ‘¨, the approximate all-optical transformation ğ‘¨ğ‘¨ï¿½, and 
the error matrix ğœºğœº=ï¿½ğ‘¨ğ‘¨âˆ’ğ‘¨ğ‘¨ï¿½ï¿½ are shown in  Fig. 10a , revealing that  the diffractive  network  design 
performed the target intensity transformation with negligible error. We also show the performance of 
this diffract ive network  design  on test patterns (U, C, L, and A as well as line  pairs and point s) in Fig. 10b. 
The all -optical outputs are identical to the ground truth outputs , further confirming  that we have ğ‘¨ğ‘¨ï¿½â‰ˆğ‘¨ğ‘¨. 
Another example of the all-optical  approximation of an arbitrary  intensity transformation (defined by a 
random permutation  matrix)  is also reported in Supplementary Figure S1. 
Discussion  
We demonstrated  that phase-only diffractive networks  under spatially -incoherent  illumination  could  
perform arbitrary linear transformation s of optical intensity with a negligible error if ğ‘ğ‘ â‰¥2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ. The 
same conclusions would be applicable to complex-valued diffractive networks where the phase and 
amplitude of each diffractive fea ture could be independently optimized;  in that case, the critical number 
of complex-valued diffractive features  for approximating an  arbitrary linear transformation of optical 
intensity  would reduce by half to  ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ due to the increased degrees of freedom  per diffractive layer . 
Because of the practical advantages of phase -only diffractive networks,  without loss of generality, we 
limit ed our analyses  in this work to phase-only modulation at each diffractive surface . 
Our results  suggest that the two differ ent training approaches  (indirect vs. direct design)  converge 
differently. If ğ‘ğ‘ is comparable to or larger than 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ, the indirect approach results in significantly 
better  and faster  convergence  and accurate approximation ğ‘¨ğ‘¨ï¿½â‰ˆğ‘¨ğ‘¨; on the other hand,  the direct design 
approach works better when ğ‘ğ‘ is considerably less than 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ, even if  its approximation error is larger. 
For example, although the designs  # 2A and  # 2B have higher errors than the design # 1A, the 
performance s of the former on  various test patterns are manifestly better  as compared in Figs. 3, 4, 8 
and 9 . These direct designs can be further improved in their approximation power by increasing ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡â‰«
1000  through  a longer training phase,  utilizing  more computational resources. 
An important  advantage of the direct approach over the indirect one is that the former  is compatible 
with data -driven design and can be applied even if the only information available  to the designer  is the 
sample data represe nting the target incoherent linear process , without a priori  knowledge of  the 
transformation matrix itself. By the same token, the direct approach also lends itself to data -driven 
optimization of incoherent diffractive processors  for all -optical linear  approximation of some  nonlinear 
processes. As a c onsequen ce of this , data-driven design of incoherent processors for  performing  other  
inference  tasks such as e.g.,  all-optical image classification under spatially-incoherent  illumination, can 
be accomplished u sing the direct approach.  
The failure of shallow diffractive networks to perform an arbitrary intensity transformation ( see e.g., 
ğ¾ğ¾=1 and ğ¾ğ¾=2 designs shown in Fig. 5 ) indicates that  shallow architectures  with phase-only 
diffractive layers  are unable to effectively balance  the ballistic photons  that are transmitted from the 
sample/input  FOV over a low numerical aperture; as a result of this,  the lower spatial frequencies of the 
input intensity patterns dominate the output intensity patterns  of a shallow diffractive network , 
sacrificing th e approximation accuracy . Therefore, shallow diffractive  network  architectures , even with 
large numbers of trainable diffractive features ( ğ‘ğ‘), fail to approximate an arbitrary intensity 9 
 transformation , as shown in Fig. 5. Deeper architectures, on the other hand, utilize their trainable 
diffractive features more effectively by distributing  them across several layers/surfaces, one following 
another,  and mixing the propagating modes of the input FOV over a seri es of  layers that are optimized 
using deep learning.  
Spatially-incoherent diffractive processor  designs  can also be extended to  temporally incoherent 
broadband illumination light. In fact, multiplexing of >100 arbitrary complex-valued linear 
transformations for complex optical fields was shown to be possible under  spatially-coherent but 
broadband illumination light38. Following a similar multi-wavelength optimization process and the 
indirect design principles  outlined earlier , one can design  a diffractive network to simultaneously 
approximate  a group of arbitrarily -selected linear intensity transformations  (ğ‘¨ğ‘¨ğ€ğ€ğŸğŸ,ğ‘¨ğ‘¨ğ€ğ€ğŸğŸ,â€¦ ğ‘¨ğ‘¨ğ€ğ€ğ‘´ğ‘´) under 
spatially-incoherent  illumination, where each intensity transformation is assigned to a unique 
wavelength ğ€ğ€ğ’Šğ’Š {ğ’Šğ’Š=ğŸğŸ:ğ‘´ğ‘´}. The success of such a spatially- and temporally-incoherent diffractive optical 
network to accurately perform all the target intensity transformation s will require an increase in the 
number of trainable features within the diffractive volume, i.e.,  ğ‘ğ‘ â‰¥ ğ‘€ğ‘€ Ã— 2ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘œğ‘œ would be needed for a 
phase-only diffra ctive network . Such diffractive processor designs that work  under spatially- and 
temporally-incoherent light can be useful for a number of applications , including fluorescence and 
brightfield microscopy and the processing of natural scenes .  
Methods  
Model for the propagation of spatially- coherent light through a diffractive  optical  
network  
Propagation of  spatially-coherent  complex optical fields through  a diffractive processor ğ”‡ğ”‡{âˆ™} constitutes 
successive amplitude and/or phase modulation by diffractive surfaces, each followed by  coherent 
propagation through the free space  separating  consecutive diffractive surfaces.  The diffractive features  
of a surface locally modulate the incident optical field ğ‘¢ğ‘¢(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦). For this paper, the trainable diffrac tive 
features are phase-only , i.e., only the phase, but not the amplitude, of the incident field is modulated by 
the diffractive surface. In other words, the field immediately after the surfaces would be 
ğ‘¢ğ‘¢(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)expï¿½ğ‘—ğ‘—ğœ™ğœ™ğ‘€ğ‘€(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)ï¿½ where  the local  phase change ğœ™ğœ™ğ‘€ğ‘€(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦) induced by the surface  is related to its  
height â„(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦) as ğœ™ğœ™ğ‘€ğ‘€=2ğœ‹ğœ‹
ğœ†ğœ†(ğ‘›ğ‘›âˆ’ 1)â„. Here ğ‘›ğ‘› is the refractive index of the diffractive surface material.  
Free-space propagation of an optical field between consecutive diffractive surfaces  was modeled using 
the a ngular spectrum method 8, according to which the propagation of an optical field ğ‘¢ğ‘¢(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦) by 
distance ğ‘‘ğ‘‘ can be computed as follows:  
ğ‘¢ğ‘¢(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦;ğ‘§ğ‘§=ğ‘§ğ‘§0+ğ‘‘ğ‘‘)=â„±âˆ’1ï¿½â„±{ğ‘¢ğ‘¢(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦;ğ‘§ğ‘§=ğ‘§ğ‘§0)}Ã—ğ»ğ»ï¿½ğ‘ˆğ‘ˆğ‘¥ğ‘¥,ğ‘ˆğ‘ˆğ‘¦ğ‘¦;ğ‘‘ğ‘‘ï¿½ï¿½ (6) 
where â„± (â„±âˆ’1) is the two -dimensional Fourier (Inverse Fourier) transform and ğ»ğ»ï¿½ğ‘ˆğ‘ˆğ‘¥ğ‘¥,ğ‘ˆğ‘ˆğ‘¦ğ‘¦;ğ‘‘ğ‘‘ï¿½ is the free -
space transfer function for an axial  propagation distance ğ‘‘ğ‘‘: 
ğ»ğ»ï¿½ğ‘ˆğ‘ˆğ‘¥ğ‘¥,ğ‘ˆğ‘ˆğ‘¦ğ‘¦;ğ‘‘ğ‘‘ï¿½=ï¿½expï¿½ğ‘—ğ‘—2ğœ‹ğœ‹
ğœ†ğœ†ğ‘‘ğ‘‘ï¿½1âˆ’(ğœ†ğœ†ğ‘ˆğ‘ˆğ‘¥ğ‘¥)2âˆ’ï¿½ğœ†ğœ†ğ‘ˆğ‘ˆğ‘¦ğ‘¦ï¿½2ï¿½,ğ‘ˆğ‘ˆğ‘¥ğ‘¥2+ğ‘ˆğ‘ˆğ‘¦ğ‘¦2< 1/ğœ†ğœ†2
0, otherwise(7) 
where  ğœ†ğœ† is the wavelength of light.  10 
 Model for the propagation of spatially- incoherent  light through a diffractive  optical  
network  
With spatially-incoherent  light, the (average ) output optical intensity ğ‘‚ğ‘‚ (ğ‘¥ğ‘¥,ğ‘¦ğ‘¦) of a diffractive network , 
for a given input intensity ğ¼ğ¼ (ğ‘¥ğ‘¥,ğ‘¦ğ‘¦), can be written as  
ğ‘‚ğ‘‚(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)=âŒ©ï¿½ğ”‡ğ”‡ï¿½ï¿½ğ¼ğ¼(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)expï¿½ğ‘—ğ‘—ğœ‘ğœ‘(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)ï¿½ï¿½ï¿½2
âŒª=lim
ğ‘ğ‘ğœ‘ğœ‘â†’âˆ1
ğ‘ğ‘ğœ‘ğœ‘ï¿½ï¿½ğ”‡ğ”‡ï¿½ï¿½ğ¼ğ¼(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)expï¿½ğ‘—ğ‘—ğœ‘ğœ‘ğ‘¡ğ‘¡(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)ï¿½ï¿½ï¿½2ğ‘ğ‘ğœ‘ğœ‘
ğ‘¡ğ‘¡=1(8) 
where ğ”‡ğ”‡{âˆ™} denotes the coherent propagation of the optical field through the diffractive processor  as 
described in the preceding subsection,  and  âŒ©âˆ™âŒª denotes the statistical average , over all the realizations of 
the spatially-independen t random process ğœ‘ğœ‘ (ğ‘šğ‘š,ğ‘›ğ‘›) representing the 2D phase of the input optical field, 
i.e., ğœ‘ğœ‘(ğ‘šğ‘š,ğ‘›ğ‘›)~ğ‘ˆğ‘ˆ(0,2ğœ‹ğœ‹) for all ğ‘šğ‘š,ğ‘›ğ‘›40.  
As for the spatially-incoherent propagation of average intensity , it is on ly possible to approximate the 
true  average  (Eq. 8) by averaging over a finite number ğ‘ğ‘ ğœ‘ğœ‘ of samples of ğœ‘ğœ‘ (ğ‘¥ğ‘¥,ğ‘¦ğ‘¦), i.e.,  
ğ‘‚ğ‘‚(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)â‰ˆ1
ğ‘ğ‘ğœ‘ğœ‘ï¿½ï¿½ğ”‡ğ”‡ï¿½ï¿½ğ¼ğ¼(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)expï¿½ğ‘—ğ‘—ğœ‘ğœ‘ğ‘¡ğ‘¡(ğ‘¥ğ‘¥,ğ‘¦ğ‘¦)ï¿½ï¿½ï¿½2ğ‘ğ‘ğœ‘ğœ‘
ğ‘¡ğ‘¡=1(9) 
In the training phase of the direct training approach,  incoherent propagation of intensities through the 
diffractive processors was simulated with ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=1000 . However , in the blind testing phase we used 
ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20000  while evaluating the diffractive processors once they were trained, irrespective of 
whether the indirect or the direct approach of training was used . 
In our numerical  simulations , the fields/intensities were discretized using ğ›¿ğ›¿ â‰ˆ 0.53ğœ†ğœ† along both ğ‘¥ğ‘¥ and ğ‘¦ğ‘¦, 
e.g., ğ‘¢ğ‘¢(ğ‘šğ‘š,ğ‘›ğ‘›)â‰œ ğ‘¢ğ‘¢(ğ‘šğ‘šğ›¿ğ›¿,ğ‘›ğ‘›ğ›¿ğ›¿) and sufficiently zero -padded before evaluating the Fourier transform , as in 
Eq. 6, using Fast Fourier Transform (FFT) algorithm.  
Diffractive network architecture  
The heights â„ (ğ‘šğ‘š,ğ‘›ğ‘›)â‰œ â„(ğ‘šğ‘šğ›¿ğ›¿,ğ‘›ğ‘›ğ›¿ğ›¿) of the  ğ‘ğ‘ diffractive features  distributed over ğ¾ğ¾  surfaces were 
optimized for designing the diffractive processors to perform the desired transformation.  To keep the 
connectivity between successive diffractive layers22 the same across the trained diffractive networks 
with different ğ‘ğ‘ , the layer-to -layer separation was set as ğ‘‘ğ‘‘ =ğ‘Šğ‘Šğ›¿ğ›¿
ğœ†ğœ†, where ğ‘Šğ‘Š=ï¿½ğ‘ğ‘
ğ¾ğ¾ğ›¿ğ›¿ is the width of each  
diffractive layer . The distances between  the input FOV and layer -1 and between layer- ğ¾ğ¾ and the output 
FOV were also set as ğ‘‘ğ‘‘. The pixel size on both the input and the output FOVs was ~ 2.13ğœ†ğœ† Ã— 2. 13ğœ†ğœ†, i.e., 
4ğ›¿ğ›¿Ã— 4ğ›¿ğ›¿. 
Linear transformation matri x 
In this paper, the input and  the output of the diffractive networks have dimensions  of ğ‘ğ‘ğ‘–ğ‘–=ğ‘ğ‘ğ‘œğ‘œ= 8 Ã— 8 , 
i.e.,  ğ¼ğ¼,ğ‘‚ğ‘‚ âˆˆ â„+8Ã—8 and ğ’Šğ’Š,ğ’ğ’ âˆˆ â„+64. To clarify, ğ’Šğ’Š and ğ’ğ’ are one-dimensional  (column) vectors obtained by 
rearranging the intensity values ğ¼ğ¼(ğ‘šğ‘š,ğ‘›ğ‘›) and ğ‘‚ğ‘‚(ğ‘šğ‘š,ğ‘›ğ‘›) of the input and the output pixels arranged in a 
two-dimensional 8 Ã— 8  square grid . Accordingly, the target transformation matrix ğ‘¨ğ‘¨ has a size of 
ğ‘ğ‘ğ‘œğ‘œÃ—ğ‘ğ‘ğ‘–ğ‘–=64Ã—64, i.e., ğ‘¨ğ‘¨ âˆˆ â„+64Ã—64. 11 
 Traini ng details  
The height  â„ of the diffractive features at each layer  was confined between zero  and a maximum value 
â„ğ‘šğ‘šğ‘šğ‘šğ‘¥ğ‘¥ by using a latent variable â„ğ‘™ğ‘™ğ‘šğ‘šğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘›ğ‘›ğ‘¡ğ‘¡ : 
â„=â„ğ‘šğ‘šğ‘šğ‘šğ‘¥ğ‘¥
2Ã—[sin(â„ğ‘™ğ‘™ğ‘šğ‘šğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘›ğ‘›ğ‘¡ğ‘¡ )+1]  
We chose â„ğ‘šğ‘šğ‘šğ‘šğ‘¥ğ‘¥â‰ˆğœ†ğœ†
ğ‘›ğ‘›âˆ’1 so that the corresponding phase modulation depth is 2ğœ‹ğœ‹. The diffractive layers  
were  optimized using the AdamW  optimizer 41 for 50  epochs with a minibatch size of 8 and an initial 
learning rate of 10âˆ’3. The learning rate was decayed by a factor of 0.7 every five epoch s. The latent 
variables were initialized randomly from the standard normal distribution ğ’©ğ’©(0,1). We evaluated t he 
mean loss of the trained model on the validation set after the completion of each epoch and selected 
the trained model state  at the end of the epoch corresponding to the lowest validation loss.  These 
details were the same for both the indirect and the direct training approach es. 
The diffractive processor models were implemented and trained using PyTorch (v1.10) 42 with Compute 
Unified Device Architecture (CUDA) version 11.3.1 . Training and testing were done on GeForce RTX 3090 
graphics processing units (GPU) in workstations with 256GB of random-access memory (RAM) and Intel 
Core i9 central processing unit (CPU). The training time of the models varied with the training approach 
as well as  the size of  the models in terms of ğ¾ğ¾  and ğ‘ğ‘. For example,  the indirect training of ğ¾ğ¾=5, ğ‘ğ‘=
5Ã—522 diffractive network model took  around 5 hours  , whereas with the direct  approach , the  training 
time for  the ğ¾ğ¾=5, ğ‘ğ‘=5Ã—522 model with ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=1000  was around 58 hours.  
Evaluation  
The evaluation procedure was the same across all the trained diffractive networks irrespective of 
whether the direct approach or the indirect approach was used to train them. To evaluate the trained 
diffractive networks, we generated  a test set comprising  20,000 pairs of  input and target intensity 
vector s ğ’ğ’=ğ‘¨ğ‘¨ğ’Šğ’Š. Note that these 20,000 test examples were generated using a different random seed 
from the ones used to generate the training and the validation sets to ensure they were not represented 
during the training. For a given ğ’Šğ’Š, the corresponding  input  intensity pattern was incoherently propagated 
through the trained diffractive network (as in Eq . 9) using ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20,000 to compute the output 
intensity  ğ’ğ’â€². The mean of the error between ğ’ğ’â€² and ğ’ğ’  over the 20,000 test examples was used to 
quantify the output error of the diffractive network for comparing different designs , as in Figs. 1 and 6 . 
For comparison between the ground truth  and the all -optical output intensities, e.g., in Figs. 3, 4, 8, 9 , 
10, we define d the scaled  all-optical output intensity vector  ğ’ğ’ï¿½=ğœğœâ€²
ğœğœğ’ğ’â€² (see Supplementary Information  
for details ). 
For evaluating the intensity transformation ğ‘¨ğ‘¨â€² performed by the diffractive networks  at the end of the ir 
training, we used ğ‘ğ‘ğ‘–ğ‘– intensity vectors {ğ’Šğ’Šğ‘¡ğ‘¡}ğ‘¡ğ‘¡=1ğ‘ğ‘ğ‘–ğ‘– where ğ’Šğ’Šğ‘¡ğ‘¡[ğ‘™ğ‘™]=1 if ğ‘™ğ‘™=ğ‘¡ğ‘¡ and 0 otherwise. In other words, 
{ğ’Šğ’Šğ‘¡ğ‘¡}ğ‘¡ğ‘¡=1ğ‘ğ‘ğ‘–ğ‘– are unit impulse functions where the impulses are located at different in put pixels. We simulated 
the all-optical output intensity vectors {ğ’ğ’ğ‘¡ğ‘¡â€²}ğ‘¡ğ‘¡=1ğ‘ğ‘ğ‘–ğ‘– corresponding to these input intensity vectors by 
incoherent propagation  and stacked  them  column by column, i.e.,   
ğ‘¨ğ‘¨â€²=ï¿½ğ’ğ’1â€²|ğ’ğ’2â€²|â‹¯|ğ’ğ’ğ‘ğ‘ğ‘–ğ‘–â€²ï¿½ (10) 12 
 Considering the diffraction -efficiency-associated scaling mismatch between ğ‘¨ğ‘¨â€² and the target 
transformation ğ‘¨ğ‘¨, we defined t he scaled diffractive network intensity transformation ğ‘¨ğ‘¨ï¿½=ğœğœğ´ğ´ğ‘¨ğ‘¨â€², where:  
ğœğœğ´ğ´=ï¿½âˆ‘ âˆ‘ (ğ‘¨ğ‘¨[ğ‘ğ‘,ğ‘ğ‘])2ğ‘ğ‘ğ‘œğ‘œ
ğ‘ğ‘=1ğ‘ğ‘ğ‘–ğ‘–
ğ‘ğ‘=1
âˆ‘ âˆ‘ (ğ‘¨ğ‘¨â€²[ğ‘ğ‘,ğ‘ğ‘])2ğ‘ğ‘ğ‘œğ‘œ
ğ‘ğ‘=1ğ‘ğ‘ğ‘–ğ‘–
ğ‘ğ‘=1(11) 
 
Supplementar y Information i ncludes:  
â€¢ The i ndirect approach  of training  
â€¢ The direct approach  of training  
â€¢ Supplementary Figure S1  
 
 
References: 
1. Lugt, A. V. Signal detection by complex spatia l filtering. IEEE Trans. Inf. Theory 10, 139â€“145 (1964). 
2. Heinz, R. A., Artman, J. O. & Lee, S. H. Matrix Multiplication by Optical Methods. Appl. Opt.  9, 2161â€“
2168 (1970).  
3. Goodman, J. W. & Woody, L. M. Method for performing complex- valued linear oper ations on 
complex-valued data using incoherent light. Appl. Opt.  16, 2611â€“ 2612 (1977).  
4. Goodman, J. W., Dias, A. R. & Woody, L. M. Fully parallel, high -speed incoherent optical method for 
performing discrete Fourier transforms. Opt. Lett.  2, 1â€“3 (1978).  
5. Tamura, P. N. & Wyant, J. C. Two -Dimensional Matrix Multiplication using Coherent Optical 
Techniques. Opt. Eng.  18, 198â€“ 204 (1979). 
6. Farhat, N. H., Psaltis, D., Prata, A. & Paek, E. Optical implementation of the Hopfield model. Appl. 
Opt.  24, 1469 â€“1475 (1985). 
7. Hotate, K. & Okugawa, T. Optical information processing by synthesis of the coherence function. J. 
Light. Technol.  12, 1247â€“ 1255 (1994).  
8. Goodman, J. W. Introduction to Fourier Optics . (W. H. Freeman, 2005).  13 
 9. Silva, A. et al.  Performing Mathematical Operations with Metamaterials. Science  343, 160â€“ 163 
(2014).  
10. Solli, D. R. & Jalali, B. Analog optical computing. Nat. Photonics  9, 704â€“ 706 (2015).  
11. Athale, R. & Psaltis, D. Optical Computing: Past and Future. Opt. Photonics N ews 27, 32â€“ 39 (2016).  
12. Wu, W., Jiang, W., Yang, J., Gong, S. & Ma, Y. Multilayered analog optical differentiating device: 
performance analysis on structural parameters. Opt. Lett.  42, 5270 â€“5273 (2017).  
13. Kwon, H., Sounas, D., Cordaro, A., Polman, A. & AlÃ¹, A. Nonlocal Metasurfaces for Optical Signal 
Processing. Phys. Rev. Lett.  121, 173004 (2018).  
14. Zuo, Y. et al.  All-optical neural network with nonlinear activation functions. Optica  6, 1132â€“ 1137 
(2019).  
15. Hughes, T. W., Williamson, I. A. D., Minkov, M. & Fan, S. Wave physics as an analog recurrent neural 
network. Sci. Adv.  5, eaay6946 (2019).  
16. Spall, J., Guo, X., Barrett, T. D. & Lvovsky, A. I. Fully reconfigurable coherent optical vector â€“matrix 
multiplication. Opt. Lett.  45, 5752â€“ 5755 (2020).  
17. Zangeneh -Nejad, F., Sounas, D. L., AlÃ¹, A. & Fleury, R. Analogue computing with metamaterials. Nat. 
Rev. Mater.  6, 207â€“ 225 (2021).  
18. Mengu, D. et al.  At the intersection of optics and deep learning: stati stical inference, computing, 
and inverse design. Adv. Opt. Photonics  14, 209â€“ 290 (2022).  
19. Stark, H. Application of Optical Fourier Transforms . (Elsevier Science, 2012).  
20. Yu, N. & Capasso, F. Flat optics with designer metasurfaces. Nat. Mater.  13, 139 â€“150 (2014).  
21. Kulce, O., Mengu, D., Rivenson, Y. & Ozcan, A. All -optical synthesis of an arbitrary linear 
transformation using diffractive surfaces. Light Sci. Appl.  10, 196 (2021).  
22. Lin, X. et al.  All-optical machine learning using diffractive deep neural networks. Science  361, 1004â€“
1008 (2018).  14 
 23. Mengu, D., Luo, Y., Rivenson, Y. & Ozcan, A. Analysis of Diffractive Optical Neural Networks and 
Their Integration With Electronic Neural Networks. IEEE J. Sel. Top. Quantum Electron.  26, 1 â€“14 
(2020).  
24. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature  521, 436â€“ 444 (2015).  
25. Rahman, M. S. S., Li, J., Mengu, D., Rivenson, Y. & Ozcan, A. Ensemble learning of diffractive optical networks. Light Sci. Appl.  10, 14 (2021).  
26. Li, J. et al.  Spectrally  encoded single-pixel machine vision using diffractive networks. Sci. Adv.  7, 
eabd7690 (2021).  
27. Luo, Y. et al.  Computational imaging without a computer: seeing through random diffusers at the 
speed of light. eLight  2, 4 (2022).  
28. Rahman, M. S. S. & Ozcan, A. Computer -Free, All -Optical Reconstruction of Holograms Using 
Diffractive Networks. ACS Photonics  8, 3375â€“ 3384 (2021).  
29. Mengu, D. & Ozcan, A. All -Optical Phase Recovery: Diffractive Computing for Quantitative Phase 
Imaging. Adv. Opt. Mater.  10, 2200281 (2022).  
30. Bai, B. et al.  To image, or not to image: class -specific diffractive cameras with all- optical erasure of 
undesired objects. eLight  2, 14 (2022).  
31. Rahman, M. S. S. & Ozcan, A. Time- lapse image classification using a diffractive neural network. 
Preprint at https://doi.org/10.48550/arXiv.2208.10802 (2022).  
32. Bai, B., Wei, H., Yang, X., Mengu, D. & Ozcan, A. Data class-specific all-optical transformations and 
encryption. Preprint at https://doi.org/10.48550/arXiv.2212.12873 (2022).  
33. Goi, E., Schoenhardt, S. & Gu, M. Direct retrieval of Zernike -based pupil functions using integrated 
diffractive deep neural networks. Nat. Commun.  13, 7531 (2022).  
34. Luo, X. et al.  Metasurface -enabled on-chip multiplexed diffractive neural networks in the visible. 
Light Sci. Appl.  11, 158 (2022).  15 
 35. Luo, Y. et al.  Design of task -specific optical systems using broadband diffractive neural networks. 
Light Sci. Appl.  8, 112 (2019).  
36. Veli, M. et al.  Terahertz pulse shaping using diffr active surfaces. Nat. Commun.  12, 37 (2021).  
37. Li, J., Hung, Y. -C., Kulce, O., Mengu, D. & Ozcan, A. Polarization multiplexed diffractive computing: 
all-optical implementation of a group of linear transformations through a polarization -encoded 
diffractive network. Light Sci. Appl.  11, 153 (2022).  
38. Li, J. et al.  Massively parallel universal linear transformations using a wavelength -multiplexed 
diffractive optical network. Adv. Photonics  5, 016003 (2023).  
39. Shannon, C. E. Communication in the Presence of Noise. Proc. IRE  37, 10â€“ 21 (1949).  
40. Saleh, B. E. A. & Teich, M. C. Fundamentals of Photonics . (Wiley, 2007).  
41. Loshchilov, I. & Hutter, F. Decoupled Weight Decay Regularization. Preprint at 
https://doi.org/10.48550/arXiv.1711.05101 (2019).  
42. Paszke, A. et al.  PyTorch: An Imperative Style, High -Performance Deep Learning Library. in Advances 
in Neural Information Processing Systems  vol. 32 (Curran Associates, Inc., 2019).  
 
 
  16 
 Figures and Figure captions:  
 
Fig. 1:  All-optical  linear transformation  of intensity  performed by diffractive networks  under  spatially -
incoherent  illumination . (a) Schematic of a diffractive network formed by ğ¾ğ¾=5 diffractive surfaces that  
all-optical ly perform a linear transformation of intensity between the input and output FOV s. The ğ‘ğ‘ 
diffractive features  are distributed evenly among the ğ¾ğ¾=5 surfaces. (b) An arbitrary ğ‘ğ‘ğ‘œğ‘œÃ—ğ‘ğ‘ğ‘–ğ‘– matrix ğ‘¨ğ‘¨, 
representing the target  intensity transformation to be  performed  all-optically by the diffractive network. 
Here ğ‘ğ‘ğ‘–ğ‘–=82 and ğ‘ğ‘ğ‘œğ‘œ=82 are the number of pixels at the input and the output FOVs of the diffractive 
network, respectively. (c) The expectation value of  the MSE between the all-optical output intensit y ğ’ğ’â€² 
and the ground -truth output intensit y ğ’ğ’, as a function of ğ‘ğ‘  for different  diffractive networks  trained 
using the indirect  approach . To simulate the incoherent propagation of intensity  for each  test input , we 
used  ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡=20000 . (d) Dependence of the calculated  output MSE on ğ‘ğ‘ğœ‘ğœ‘, ğ‘¡ğ‘¡ğ‘¡ğ‘¡, demonstrated  for network  
17 
 # 1E of Fig. 1c . The right y -axis shows the expec tation value of the residual magnitude of  1
ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡âˆ‘ğ‘’ğ‘’ğ‘—ğ‘—ğœƒğœƒğ‘–ğ‘–ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡
ğ‘–ğ‘–=1, 
where ğœƒğœƒğ‘–ğ‘–~ğ‘ˆğ‘ˆğ‘›ğ‘›ğ‘–ğ‘–ğ‘ˆğ‘ˆğ‘œğ‘œğ‘ˆğ‘ˆğ‘šğ‘š(0,2ğœ‹ğœ‹). 18 
  
Fig. 2:  All-optical  linear  transformations  of intensity , ğ‘¨ğ‘¨ï¿½, performed  under spatially-incoherent 
19 
 illumination,  by five of the diffractive network d esigns shown in  Fig. 1c , together with the corresponding 
error matrices  with respect to the target transformation, ğœºğœº =ï¿½ğ‘¨ğ‘¨âˆ’ğ‘¨ğ‘¨ï¿½ï¿½. Here |âˆ™| denotes elementwise 
operation.  The means of the error matrix elements are listed in t he table on the right . 20 
  
Fig. 3:  All-optical  linear  transformation of structured intensity patterns such as letters U, C, L, and A by  
the same diffractive networks  as in Fig. 2 , accompanied by the patterns  resulting from the numerical 
21 
 inverse mapping of  the all -optical outputs through multiplication by ğ‘¨ğ‘¨âˆ’ğŸğŸ.  
 
Fig. 4:  Same as Figure 3, except for  the test intensity  patterns formed by closely separated  lines  and 
points.  
22 
  
Fig. 5:  Effect of the diffractive network â€™s depth, i.e., the number of diffractive surfaces (ğ¾ğ¾), on the 
23 
 approximation perform ance  for an arbitrary intensity linear transformation  under spatially-incoherent 
illumination.   All-optical  linear  transformations  of intensity , ğ‘¨ğ‘¨ï¿½, performed by four diffractive network 
designs  with approximately equal ğ‘ğ‘ and increasing ğ¾ğ¾, are shown, together with the corresponding error  
matrices  with respect to the target transformation,  i.e., ğœºğœº=ï¿½ğ‘¨ğ‘¨âˆ’ğ‘¨ğ‘¨ï¿½ï¿½. Here |âˆ™| denotes elementwise 
operation.  The mean  values of the error matrix elements are listed in the table on the right.  
 
 
Fig. 6:  All-optical  linear transformation  of intensity  under spatially -incoherent illumination,  by diffractive 
networks  trained using the direct  approach . (a) The e xpectation value of the MSE between the all-
optical output intensity  ğ’ğ’â€² and the ground -truth output intensit y ğ’ğ’, as a function of ğ‘ğ‘ for different  
diffractive networks  trained using the direct approach . (b) Dependence of the calculated  output MSE on 
ğ‘ğ‘ğœ‘ğœ‘, ğ‘¡ğ‘¡ğ‘¡ğ‘¡, demonstrated for network # 2 E of Fig. 6a . The right y -axis shows the expec tation value of the 
residual magnitude of  1
ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡âˆ‘ğ‘’ğ‘’ğ‘—ğ‘—ğœƒğœƒğ‘–ğ‘–ğ‘ğ‘ğœ‘ğœ‘,ğ‘¡ğ‘¡ğ‘¡ğ‘¡
ğ‘–ğ‘–=1, where ğœƒğœƒğ‘–ğ‘–~ğ‘ˆğ‘ˆğ‘›ğ‘›ğ‘–ğ‘–ğ‘ˆğ‘ˆğ‘œğ‘œğ‘ˆğ‘ˆğ‘šğ‘š(0,2ğœ‹ğœ‹). 
24 
  
Fig. 7:  All-optical  linear  transformations  of intensity , ğ‘¨ğ‘¨ï¿½, performed under spatially -incoherent 
25 
 illumination by five of the diffractive network d esigns shown in  Fig. 6a, together with the corresponding 
error matrices  with respect to the target transformation, ğœºğœº =ï¿½ğ‘¨ğ‘¨âˆ’ğ‘¨ğ‘¨ï¿½ï¿½. Here |âˆ™| denotes elementwise 
operation.  The mean  values of the error matrix elements are listed in the table on the right.  26 
  
Fig. 8:  All-optical  linear transformation of structured intensity patterns such as letters U, C, L, and A by  
the same diffractive networks  as in Fig. 7, accompanied by the patterns  resulting from the numerical 
27 
 inverse mapping of  the all -optical outputs through multiplication  by ğ‘¨ğ‘¨âˆ’ğŸğŸ. 
 
 
Fig. 9:  Same as Figure 8, except for  the test intensity  patterns formed by closely separated  lines  and 
points . 
28 
  
Fig. 10:  Approximation of an arbitrary non -invertible  linear  transformation (ğ‘¨ğ‘¨) of intensity, under 
spatially -incoherent illumination, by a diffractive network ( ğ¾ğ¾=5, ğ‘ğ‘=5Ã—582) trained using the 
29 
 indirect approach. (a) The target transformation  ğ‘¨ğ‘¨, the all-optical intensity transformation  ğ‘¨ğ‘¨ï¿½ performed 
by the  trained  diffractive network  and the error  matrix ğœºğœº=ï¿½ğ‘¨ğ‘¨âˆ’ğ‘¨ğ‘¨ï¿½ï¿½. Here |âˆ™| denotes elementwise 
operation. (b) All-optical transformation of different test intensity patterns  by the trained  diffractive 
network , together with the corresponding ground truths . 
 