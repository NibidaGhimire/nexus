Entropy-dissipation Informed Neural Network
for McKean-Vlasov Type PDEs
Zebang Shen∗
ETH Z ¨urich
zebang.shen@inf.ethz.chZhenfu Wang∗
Peking University
zwang@bicmr.pku.edu.cn
Abstract
The McKean-Vlasov equation (MVE) describes the collective behavior of parti-
cles subject to drift, diffusion, and mean-field interaction. In physical systems, the
interaction term can be singular, i.e. it diverges when two particles collide. No-
table examples of such interactions include the Coulomb interaction, fundamental
in plasma physics, and the Biot-Savart interaction, present in the vorticity formu-
lation of the 2D Navier-Stokes equation (NSE) in fluid dynamics. Solving MVEs
that involve singular interaction kernels presents a significant challenge, especially
when aiming to provide rigorous theoretical guarantees. In this work, we propose
a novel approach based on the concept of entropy dissipation in the underlying
system. We derive a potential function that effectively controls the KL divergence
between a hypothesis solution and the ground truth. Building upon this theoret-
ical foundation, we introduce the Entropy-dissipation Informed Neural Network
(EINN ) framework for solving MVEs. In EINN , we utilize neural networks (NN)
to approximate the underlying velocity field and minimize the proposed poten-
tial function. By leveraging the expressive power of NNs, our approach offers a
promising avenue for tackling the complexities associated with singular interac-
tions. To assess the empirical performance of our method, we compare EINN with
SOTA NN-based MVE solvers. The results demonstrate the effectiveness of our
approach in solving MVEs across various example problems.
1 Introduction
Scientists use Partial Differential Equations (PDEs) to describe natural laws and predict the dynam-
ics of real-world systems. As PDEs are of fundamental importance, a growing area in machine
learning is the use of neural networks (NN) to solve these equations [Han et al., 2018, Zhang et al.,
2018, Raissi et al., 2020, Cai et al., 2021, Karniadakis et al., 2021, Cuomo et al., 2022]. An im-
portant category of PDEs is the McKean-Vlasov equation (MVE), which models the dynamics of a
stochastic particle system with mean-field interactions
dXt=−∇V(Xt)dt+K∗¯ρt(Xt)dt+√
2νdBt,¯ρt=Law(Xt). (1)
HereXt∈Xdenotes a random particle’ position, Xis either Rdor the torus Πd(a cube [−L, L]d
with periodic boundary condition), V∶Rd→Rdenotes a known potential, K∶Rd→Rddenotes
some interaction kernel and the convolution operation is defined as h∗ϕ=∫Xh(x−y)ϕ(y)dy,
{Bt}t≥0is the standard d-dimensional Wiener process with ν≥0being the diffusion coefficient,
and¯ρt∶X→Ris the law or the probability density function of the random variable Xtand the
initial data ¯ρ0is given. Under mild regularity conditions, the density function ¯ρtsatisfies the MVE
(MVE) ∂t¯ρt(x)+div(¯ρt(−∇V(x)+K∗¯ρt(x)))=ν∆¯ρt(x), (2)
∗Authors are listed in alphabetic order.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2303.11205v2  [math.NA]  27 Oct 2023where divdenotes the divergence operator, divh(x)=∑d
i=1∂hi/∂xifor a velocity field h∶Rd→
Rd,∆denotes the Laplacian operator defined as ∆ϕ=div(∇ϕ), where∇ϕdenotes the gradient of
a scalar function ϕ∶Rd→R. Note that all these operators are applied only on the spatial variable x.
In order to describe dynamics in real-world phenomena such as electromagnetism [Golse, 2016] and
fluid mechanics [Majda et al., 2002], the interaction kernels Kin the MVE can be highly singular ,
i.e.∥K(x)∥→∞when∥x∥→0. Two of the most notable examples are the Coulomb interactions
(Coulomb Kernel) K(x)=−∇g(x),withg(x)={((d−2)Sd−1(1))−1∥x∥−(d−2), d≥3,
−(2π)−1log∥x∥, d =2,(3)
withSd−1(1)denoting the surface area of the unit sphere in Rd, and the vorticity formulation of the
2D Navier-Stokes equation (NSE) where the interaction kernel Kis given by the Biot-Savart law
(Biot-Savart Kernel) K(x)=1
2πx⊥
∥x∥2=1
2π(−x2
∥x∥2,x1
∥x∥2), (4)
where x=(x1, x2)and∥x∥denotes the Euclidean norm of a vector.
Classical methods for solving MVEs, including finite difference, finite volume, finite element, spec-
tral methods, and particle methods, have been developed over time. A common drawback of these
methods lies in the constraints of their solution representation: Sparse representations, such as less
granular grids, cells, meshes, fewer basis functions, or particles, may lead to an inferior solution
accuracy; On the other hand, dense representations incur higher computational and memory costs.
As a potent tool for function approximation, NNs are anticipated to overcome these hurdles and
handle higher-dimensional, less regular, and more complex systems efficiently [Weinan et al., 2021].
The most renowned NN-based algorithm is the Physics Informed Neural Network (PINN) [Raissi
et al., 2019]. The philosophy behind the PINN method is that solving a PDE system is equivalent
to finding the root of the corresponding differential operators. PINN tackles the latter problem by
directly parameterizing the hypothesis solution with an NN and training it to minimize the L2func-
tional residual of the operators. As a versatile PDE solver, PINN may fail to exploit the underlying
dynamics of the PDE, which possibly leads to inferior performance on task-specific solvers. For
example, on the 2D NSE problem, a recent NN-based development Zhang et al. [2022] surpasses
PINN and sets a new SOTA empirical performance, which however lacks rigorous theoretical sub-
stantiation. Despite the widespread applications of PINN, rigorous error estimation guarantees are
scarce in the literature. While we could not find results on the MVE with the Coulomb interaction,
only in a very recent paper [De Ryck et al., 2023], the authors establish for NSE that the PINN loss
controls the discrepancy between a candidate solution and the ground truth. We highlight that their
result holds average-in-time , meaning that at a particular timestamp t∈[0, T], a candidate solution
with small PINN loss may still significantly differ from the true solution. In contrast, all guarantees
in this paper are uniform-in-time . Moreover, there is a factor in the aforementioned guarantee that
exponentially depends on the total evolving time T, while the factor in our guarantee for the NSE
isindependent ofT. We highlight that these novel improvements are achieved for the proposed
EINN framework since we take a completely different route from PINN: Our approach is explicitly
designed to exploit the underlying dynamics of the system, as elaborated below.
Our approach Define the operator
A[ρ]def=−∇V+K∗ρ−ν∇logρ. (5)
By noting ∆¯ρt=div(¯ρt∇log ¯ρt), we can rewrite the MVE in the form of a continuity equation
∂t¯ρt(x)+div(¯ρt(x)A[¯ρt](x))=0. (6)
For simplicity, we will refer to A[¯ρt]as the underlying velocity . Consider another time-varying
hypothesis velocity fieldf∶R×Rd→Rand let ρf
tbe the solution to the continuity equation
(hypothesis solution) ∂tρf
t(x)+div(ρf
t(x)f(t,x))=0, ρf
0=¯ρ0 (7)
fort∈[0, T], where we recall that the initial law ¯ρ0is known. We will refer to ρf
tas the hypothesis
solution and use the superscript to emphasize its dependence on the hypothesis velocity field f. We
propose an Entropy-dissipation Informed Neural Network framework ( EINN ), which trains an NN
parameterized hypothesis velocity field fθby minimizing the following EINN loss
(EINN loss) R(fθ)def=∫T
0∫X∥fθ(t,x)−A[ρfθ
t](x)∥2ρfθ
t(x)dxdt. (8)
2The objective (8) is obtained by studying the stability of carefully constructed Lyapunov functions.
These Lyapunov functions draw inspiration from the concept of entropy dissipation in the system,
leading to the name of our framework. We highlight that we provide a rigorous error estimation
guarantee for our framework for MVEs with singular kernels (3) and (4), showing that when R(fθ)
is sufficiently small, ρfθ
trecovers the ground truth ¯ρtin the KL sense, uniform in time.
Theorem 1 (Informal) .Suppose that the initial density function ¯ρ0is sufficiently regular and the
hypothesis velocity field ft(⋅)=f(t,⋅)is at least three times continuously differentiable both in t
andx. We have for the MVE with a bounded interaction kernel Kor with the singular Coulomb (3)
or Biot-Savart (4) interaction, the KL divergence between the hypothesis solution ρf
tand the ground
truth ¯ρtis controlled by the EINN loss for any t∈[0, T], i.e. there exists some constant C>0,
sup
t∈[0,T]KL(ρf
t,¯ρt)≤CR(f). (9)
Having stated our main result, we elaborate on the difference between EINN and PINN in terms
of information flow over time, which explains why EINN achieves better theoretical guarantees:
In PINN, the residuals at different time stamps are independent of each other and hence there is no
information flow from the residual at time t1to the one at time t2(>t1). In contrast, in the EINN loss
(8), incorrect estimation made in t1will also affect the error at t2through the hypothesis solution ρf
t.
Such an information flow gives a stronger gradient signal when we are trying to minimize the EINN
loss, compared to the PINN loss. It partially explains why we can obtain the novel uniform-in-time
estimation as opposed to the average-in-time estimation for PINN and why the constant Cin the
NSE case is independent of TforEINN (Theorem 2), but exponential in Tfor PINN.
Contributions. In summary, we present a novel NN-based framework for solving the MVEs. Our
method capitalizes on the entropy dissipation property of the underlying system, ensuring robust
theoretical guarantees even when dealing with singular interaction kernels. We elaborate on the
contributions of our work from theory, algorithm, and empirical perspectives as follows.
1. (Theory-wise) By studying the stability of the MVEs with bounded interaction kernels or with
singular interaction kernels in the Coulomb (3) and the Biot-Savart case (4) (the 2D NSE) via en-
tropy dissipation, we establish the error estimation guarantee for the EINN loss on these equations.
Specifically, we design a potential function R(f)of a hypothesis velocity fsuch that R(f)controls
the KL divergence between the hypothesis solution ρf
t(defined in equation (7)) and the ground truth
solution ¯ρtfor any time stamp within a given time interval [0, T]. A direct consequence of this
result is that R(f)can be used to assess the quality of a generic hypothesis solution to the above
MVEs and ρf
texactly recovers ¯ρtin the KL sense given that R(f)=0.
2. (Algorithm-wise) When the hypothesis velocity field is parameterized by an NN, i.e. f=fθwith
θbeing some finite-dimensional parameters, the EINN lossR(fθ)can be used as the loss function
of the NN parameters θ. We discuss in detail how an estimator of the gradient ∇θR(fθ)can be
computed so that stochastic gradient-based optimizers can be utilized to train the NN. In particular,
for the 2D NSE (the Biot-Savart case (4)), we show that the singularity in the gradient computation
can be removed by exploiting the anti-derivative of the Biot-Savart kernel.
3. (Empirical-wise) We compare the proposed approach, derived from our novel theoretical guar-
antees, with SOTA NN-based algorithms for solving the MVE with the Coulomb interaction and
the 2D NSE (the Biot-Savart interaction). We pick specific instances of the initial density ¯ρ0, under
which explicit solutions are known and can be used as the ground truth to test the quality of the
hypothesis ones. Using NNs with the same complexity (depth, width, and structure), we observe
that the proposed method significantly outperforms the included baselines.
2 Entropy-dissipation Informed Neural Network
In this section, we present the proposed EINN framework for the MVE. To understand the intuition
behind our design, we first write the continuity equation (7) in a similar form as the MVE (6):
∂tρf
t(x)+div(ρf
t(x)(A[ρf
t](x)+δt(x)))=0, (10)
where fis the hypothesis velocity (recall that ft(⋅)=f(t,⋅)) and
(Perturbation) δt(x)def=ft(x)−A[ρf
t](x) (11)
can be regarded as a perturbation to the original MVE system. Consequently, it is natural to study
the deviation of the hypothesis solution ρf
tfrom the true solution ¯ρtusing an appropriate Lyapunov
3function L(ρf
t,¯ρt). The functional relation between this deviation and the perturbation is termed
as the stability of the underlying dynamical system, which allows us to derive the EINN loss (8).
Following this idea, the design of the EINN loss can be determined by the choice of the Lyapunov
function Lused in the stability analysis. In the following, we describe the Lyapunov function used
for the MVE with the Coulomb interaction and the 2D NSE (MVE with Biot-Savart interaction).
The proof of the following results are the major theoretical contributions of this paper and will be
elaborated in the analysis section 3.
• For the MVE with the Coulomb interaction, we choose Lto be the modulated free energy E
(defined in equation (23)) which is originally proposed in [Bresch et al., 2019a] to establish the
mean-field limit of a corresponding interacting particle system. We have (setting L=E)
d
dtE(ρf
t,¯ρt)≤1
2∫Xρf
t(x)∥δt(x)∥2dx+C E(ρf
t,¯ρt), (12)
where Cis a universal constant depending on νand(¯ρt)t∈[0,T].
• For the 2D NSE (MVE with the Biot-Savart interaction), we choose Las the KL divergence. Our
analysis is inspired by [Jabin and Wang, 2018] which for the first time establishes the quantitative
mean-field limit of the stochastic interacting particle systems where the interaction kernel can be
in some negative Sobolev space. We have
d
dtKL(ρf
t,¯ρt)≤−ν
2∫Xρf
t(x)∥∇logρf
t
¯ρt(x)∥2+CKL(ρf
t,¯ρt)+1
ν∫Xρf
t(x)∥δt(x)∥2dx,(13)
where again Cis a universal constant depending on νand(¯ρt)t∈[0,T].
After applying Gr ¨onwall’s inequality on the above results, we can see that the EINN loss (8) is
precisely the term derived by stability analysis of the MVE system with an appropriate Lyapunov
function. In the next section, we elaborate on how a stochastic approximation of ∇θR(fθ)can be
efficiently computed for a parameterized hypothesis velocity field f=fθso that stochastic opti-
mization methods can be utilized to minimize R(fθ).
2.1 Stochastic Gradient Computation with Neural Network Parameterization
While the choice of the EINN loss (8) is theoretically justified through the above stability study, in
this section, we show that it admits an estimator which can be efficiently computed. Define the flow
mapXtvia the ODE dx(t)=ft(x(t);θ)dtwithx(0)=x0such that x(t)=Xt(x0). From the
definition of the push-forward measure, one has ρf
t=Xt♯¯ρ0. Recall the definitions of the EINN loss
R(f)in equation (8) and the perturbation δtin equation (11). Use the change of variable formula
of the push-forward measure in (a) and Fubini’s theorem in (b). We have
R(f)=∫T
0∥δt∥2
ρf
tdt(a)=∫T
0∥δt○Xt∥2
ρ0dt(b)=∫ ∫T
0∥δt○Xt(x0)∥2dtd¯ρ0(x0). (14)
Consequently, by defining the trajectory-wise loss (recall x(t)=Xt(x0))
R(f;x0)=∫T
0∥δt○Xt(x0)∥2dt=∫T
0∥δt(x(t))∥2dt, (15)
we can write the potential function (8) as an expectation R(f)=Ex0∼¯ρ0[R(f;x0)]. Similarly, when
fis parameterized as f=fθ, we obtain the expectation form ∇θR(fθ)=Ex0∼¯ρ0[∇θR(fθ;x0)].
We show∇θR(fθ;x0)can be computed accurately, via the adjoint method (for completeness see
the derivation of the adjoint method in appendix D). As a recap, suppose that we can write R(fθ;x0)
in a standard ODE-constrained form R(fθ;x0)=ℓ(θ)=∫T
0g(t,s(t), θ)dt, where {s(t)}t∈[0,T]is
the solution to the ODEd
dts(t)=ψ(t,s(t);θ)withs(0)=s0, andψis a known transition function.
The adjoint method states that the gradientd
dθℓ(θ)can be computed as
(Adjoint Method)dℓ
dθ=∫T
0a(t)⊺∂ψ
∂θ(t,s(t);θ)+∂g
∂θ(t,s(t);θ)dt. (16)
where a(t)solves the final value problemsd
dta(t)⊺+a(t)⊺∂ψ
∂s(t,s(t);θ)+∂g
∂s(t,s(t);θ)=0, a(T)=
0. In the following, we focus on how R(fθ;x0)can be written in the above ODE-constrained form.
4Write R(fθ;x0)in ODE-constrained form Expanding the definition of δtin equation (11) gives
δt(x(t))=ft(x(t))−(−∇V(x(t))+K∗ρf
t(x(t))−ν∇logρf
t(x(t))). (17)
Note that in the above quantity, fandVare known functions. Moreover, it is known that
∇logρf
t(x(t))admits a closed form dynamics (e.g. see Proposition 2 in [Shen et al., 2022])
d
dt∇logρf
t(x(t))=−∇(divft(x(t);θ))−(Jft(x(t);θ))⊺∇logρf
t(x(t)), (18)
which allows it to be explicitly computed by starting from ∇log ¯ρ0(x0)and integrating over time
(recall that ¯ρ0is known). Here Jftdenotes the Jacobian matrix of ft. Consequently, all we need to
handle is the convolution term K∗ρf
t(x(t)).
A common choice to approximate the convolution operation is via Monte-Carlo integration: Let
yi(t)iid∼ρf
tfori=1, . . . , N and denote an empirical approximation of ρf
tbyµρf
t
N=1
N∑N
i=1δyi(t),
where δyi(t)denotes the Dirac measure at yi(t). We approximate the convolution term in equation
(17) in different ways for the Coulomb and the Biot-Savart interactions:
1. For the Coulomb type kernel (3), we first approximate K∗ρf
twithKc∗ρf
t, where
Kc(x)def={K(x)if∥x∥>c,
0 if∥x∥≤c.(19)
Ifρf
tis bounded in X, we have
sup
x∈X∥(K−Kc)∗ρf
t(x)∥=sup
x∈X∥∫∥x−y∥≤cx−y
∥x−y∥dρf
t(y)dy∥≤∥ρf
t∥L∞(X)∫∥y∥≤c1
∥y∥d−1dy.
To compute the integral on the right-hand side, we will switch to polar coordinates (r, ψ):
∫∣y∣≤c1
∣y∣d−1dy=∫c
0dr1
rd−1∫Ψdψ J(r,ψ)≤∫c
0dr=c. (20)
Here, J(r,ψ)denotes the determinant of the Jacobian matrix resulting from the transformation from
the Cartesian system to the polar coordinate system. In inequality (20), we utilize the fact that
J(r,ψ)≤rd−1, which allows us to cancel out the factor 1/rd−1. Now that Kcis bounded by c−d+1, we
can further approximate Kc∗ρf
tusing Kc∗µρf
t
Nwith error of the order O(c−d+1/√
N). Altogether,
we have supx∈X∥K∗ρf
t(x)−Kc∗µρf
t
N(x)∥=O(c+c−d+1/√
N)which can be made arbitrarily
small for a sufficiently small cand a sufficiently large N.
2. For Biot-Savart interaction (2D Navier-Stokes equation), there are more structures to exploit and
we can completely avoid the singularity: As noted by Jabin and Wang [2018], the convolution kernel
Kcan be written in a divergence form:
K=∇⋅U,withU(x)=1
2π[−arctan(x1
x2), 0
0, arctan(x2
x1)], (21)
where the divergence of a matrix function is applied row-wise, i.e. [K(x)]i=divUi(x). Using
integration by parts and noticing that the boundary integration vanishes on the torus, one has
K∗ρf
t(x)=∫K(y)ρf
t(x−y)dy=∫∇⋅U(y)ρf
t(x−y)dy=∫U(y)∇ρf
t(x−y)dy
=∫U(x−y)ρf
t(y)∇logρf
t(y)dy=Ey∼ρf
t(y)[U(x−y)∇logρf
t(y)].
If the score function ∇logρf
tis bounded, then the integrand in the expectation is also bounded.
Therefore, we can avoid integrating singular functions and the Monte Carlo-type estimation
1
N∑N
i=1U(x−yi(t))∇logρf
t(yi(t))is accurate for a sufficiently large value of N.
With the above discussion, we can write R(fθ;x0)in an ODE-constrained form in a standard way,
which due to space limitation is deferred to Appendix D.1.
Remark 1. LetℓN(θ)be the function we obtained using the above approximation of the convolution,
where Nis the number of Monte-Carlo samples. The above discussion shows that ℓN(θ)and
R(fθ;x0)are close in the L∞sense, which is hence sufficient when the EINN loss is used as error
quantification since only function value matters. When both ℓN(θ)andR(fθ;x0)are in C2, one
can translate the closeness in function value to the closeness of their gradients. In our experiments,
using∇ℓN(θ)as an approximation of ∇θR(fθ;x0)gives very good empirical performance already.
53 Analysis
In this section, we focus on the torus case, i.e. X=Πdis a box with the periodic boundary condition.
This is a typical setting considered in the literature as the universal function approximation of NNs
only holds over a compact set. Moreover, the boundary integral resulting from integration by parts
vanishes in this setting, making it amenable for analysis purposes. For completeness, we provide
a discussion on the unbounded case, i.e. X=Rdin the Appendix G, which requires additional
regularity assumptions. Given the MVE (2), if Kis bounded, it is sufficient to choose the Lyapunov
functional L(ρf
t,¯ρt)as the KL divergence (please see Theorem 6 in the appendix). But for the
singular Coulomb kernel, we need also to consider the modulated energy as in [Serfaty, 2020]
(Modulated Energy) F(ρ,¯ρ)def=1
2∫X2g(x−y)d(ρ−¯ρ)(x)d(ρ−¯ρ)(y), (22)
where gis the fundamental solution to the Laplacian equation in Rd, i.e.−∆g=δ0, and the Coulomb
interaction reads K=−∇g(see its closed form expression in equation (3)). If we are only interested
in the deterministic dynamics with Coulomb interactions, i.e. ν=0in equation (2), it suffices to
choose L(ρf
t,¯ρ)asF(ρf
t,¯ρt)(please see Theorem 3). But if we consider the system with Coulomb
interactions and diffusions, i.e. ν>0, we shall combine the KL divergence and the modulated
energy to form the modulated free energy as in Bresch et al. [2019b], which reads
(Modulated Free Energy) E(ρ,¯ρ)def=νKL(ρ,¯ρ)+F(ρ,¯ρ). (23)
This definition agrees with the physical meaning that “Free Energy = Temperature ×Entropy +
Energy”, and we note that the temperature is proportional to the diffusion coefficient ν. We remark
also for two probability densities ρand¯ρ,F(ρ,¯ρ)≥0since by looking in the Fourier domain
F(ρ,¯ρ)=∫ˆg(ξ)∣̂ρ−¯ρ(ξ)∣2dξ≥0asˆg(ξ)≥0. Moreover, F(ρ,¯ρ)can be regarded as a negative
Sobolev norm for ρ−¯ρ, which metricizes weak convergence.
To obtain our main stability estimate, we first obtain the time evolution of the KL divergence.
Lemma 1 (Time Evolution of the KL divergence) .Given the hypothesis velocity field f=f(t, x)∈
C1
t,x. Assume that (ρf
t)t∈[0,T]and(¯ρt)t∈[0,T]are classical solutions to equation (7) and equation
(6) respectively. It holds that (recall the definition of δtin equation (11))
d
dt∫Xρf
tlogρf
t
¯ρt=−ν∫Xρf
t∣∇logρf
t
¯ρt∣2+∫Xρf
tK∗(ρf
t−¯ρt)⋅∇logρf
t
¯ρt+∫Xρf
tδt⋅∇logρf
t
¯ρt,
whereXis the tours Πd. All the integrands are evaluated at x.
We refer the proof of this lemma and all other lemmas and theorems in this section to the appendix
E. We remark that to have the existence of classical solution (¯ρt)t∈[0,T], we definitely need the
regularity assumptions on −∇Vand on K. But the linear term −∇Vwill not contribute to the
evolution of the relative entropy. See [Jabin and Wang, 2018] for detailed discussions.
Similarly, we have the time evolution of the modulated energy as follows.
Lemma 2 (Time evolution of the modulated energy) .Under the same assumptions as in Lemma 1,
given the diffusion coefficient ν≥0, it holds that (recall the definition of δtin equation (11))
d
dtF(ρf
t,¯ρt)=−∫Xρf
t∥K∗(ρf
t−¯ρt)∥2−∫Xρf
tδt⋅K∗(ρf
t−¯ρt)+ν∫Xρf
tK∗(ρf
t−¯ρt)⋅∇logρf
t
¯ρt
−1
2∫X2K(x−y)⋅(A[¯ρt](x)−A[¯ρt](y))d(ρf
t−¯ρt)⊗2(x, y)
where we recall that the operator Ais defined in equation (5).
By Lemma 1 and careful analysis, in particular by rewriting the Biot-Savart law in the divergence of
a bounded matrix-valued function (21), we obtain the following estimate for the 2D NSE.
Theorem 2 (Stability estimate of the 2D NSE) .Notice that when Kis the Biot-Savart kernel,
divK=0. Assume that the initial data ¯ρ0∈C3(Πd)and there exists c>1such that1
c≤¯ρ0≤c.
Assume further the hypothesis velocity field f(t, x)∈C1
t,x. Then it holds that
sup
t∈[0,T]∫Πdρf
tlogρf
t
¯ρtdx≤eC
νR(f),
where C=∫∞
0M(t)dt<∞withM(t)def=∥∇log ¯ρt∥2
L∞/2ν+2∥∇2¯ρt/¯ρt∥
L∞.
6We remark that given ¯ρ0is smooth enough and fully supported on X, one can propagate the regu-
larity to finally show the finiteness of C. See detailed computations as in Guillin et al. [2021]. We
give the complete proof in the appendix E. This theorem tells us that as long as R(f)is small, the
KL divergence between ρf
tand¯ρtis small and the control is uniform in time t∈[0, T]for any T.
Moreover, we highlight that Cis independent of T, and our result on the NSE is significantly better
than the average-in-time and exponential-in- Tresults from [De Ryck et al., 2023].
To treat the MVE (2) with Coulomb interactions, we exploit the time evolution of the modulated free
energy E(ρf
t,¯ρt). Indeed, combining Lemma 1 and Lemma 2, we arrive at the following identity.
Lemma 3 (Time evolution of the modulated free energy) .Under the same assumptions as in Lemma
1, one has (recall the definitions of δtandAin (11) and (5) respectively)
d
dtE(ρf
t,¯ρt)=−∫Xρf
t∣K∗(ρf
t−¯ρt)−ν∇logρf
t
¯ρt∣2
−∫Xρf
tδt⋅(K∗(ρf
t−¯ρt)−ν∇logρf
t
¯ρt)
−1
2∫X2K(x−y)⋅(A[¯ρt](x)−A[¯ρt](y))d(ρf
t−¯ρt)⊗2(x, y).
Inspired by the mean-field convergence results as in Serfaty [2020] and Bresch et al. [2019b], we
finally can control the growth of E(ρf
t,¯ρt)in the case when ν>0, and F(ρf
t,¯ρt)in the case when
ν=0. Note also that E(ρf
t,¯ρt)can also control the KL divergence when ν>0.
Theorem 3 (Stability estimate of MVE with Coulomb interactions) .Assume that for t∈[0, T], the
underlying velocity field A[¯ρt](x)is Lipschitz in xandsupt∈[0,T]∥∇A[¯ρt](⋅)∥L∞=C1<∞.Then
there exists C>0such that
sup
t∈[0,T]νKL(ρf
t,¯ρt)≤sup
t∈[0,T]E(ρf
t,¯ρt)≤exp(CC1T)R(f).
In the deterministic case when ν=0, under the same assumptions, it holds that
sup
t∈[0,T]F(ρf
t,¯ρt)≤exp(CC1T)R(f).
Recall the definition of the operator Ain equation 5. Given that X=Πd, and ¯ρ0is smooth enough
and bounded from below, one can propagate regularity to obtain the Lipschitz condition for A[¯ρt].
See the proof and the discussion on the Lipschitz assumptions on A[¯ρt](⋅)in the appendix E.
Approximation Error of Neural Network Theorems 2 and 3 provide the error estimation guar-
antee for the proposed EINN loss (8). Suppose that we parameterize the velocity field f=fθwith
an NN parameterized by θ, as we did in Section 2.1 and let ˜fbe the output of an optimization pro-
cedure when R(fθ)is used as objective. In order the explicitly quantify the mismatch between ρ˜f
t
and¯ρt, we need to quantify two errors: (i) Approximation error, reflecting how well the ground truth
solution can be approximated among the NN function class of choice; (ii) Optimization error, in-
volving minimization of a highly nonlinear non-convex objective. In the following, we show that for
a function class Fwith sufficient capacity, there exists at least one element ˆf∈Fthat can reduce the
loss function R(ˆf)as much as desired. We will not discuss how to identify such an element in the
function class Fas it is independent of our research and remains possibly the largest open problem
in modern AI research. To establish our result, we make the following assumptions.
Assumption 1. ρ0is sufficiently regular, such that ∇logρ0∈L∞(X)and¯ft=A[¯ρt]∈W2,∞(X).
∇Vis Lipschitz continuous. Here W2,∞(X)stands for the Sobolev norm of order (2,∞)overX.
We here again need to propagate the regularity for ftat least for a time interval [0, T]. It is easy
to do so for the torus case, but for the unbounded domain, there are some technical issues to be
overcome. Similar assumptions are also needed in some mathematical works for instance in Jabin
and Wang [2018]. We also make the following assumption on the capacity of the function class F,
which is satisfied for example by NNs with tanh activation function [De Ryck et al., 2021].
Assumption 2. The function class is sufficiently large, such that there exists ˆf∈Fsatisfying ˆft∈
C3(X)and∥ˆft−¯ft∥W2,∞(X)≤ϵfor all t∈[0, T].
Theorem 4. Consider the case where the domain is the torus. Suppose that Assumptions 1 and
2 hold. For both the Coulomb and the Biot-Savart cases, there exists ˆf∈Fsuch that R(ˆf)≤
C(T)⋅(ϵ⋅ln 1/ϵ)2, where C(T)is some constant independent of ϵ. Here Ris the EINN loss (8).
7The major difficulty to overcome is the lack of Lipschitz continuity due to the singular interaction.
We successfully address this challenge by establishing that the contribution of the singular region
(∥x∥≤ϵ)toR(ˆf)can be bounded by O((ϵlog1
ϵ)2). Please see the detailed proof in Appendix F.
4 Related Works on NN-based PDE solvers
Solving PDEs is a key aspect of scientific research, with a wealth of literature [Evans, 2022]. Due
to space limitations, a detailed discussion about the classical PDE solvers is deferred to Appendix
A. In this section, we focus on the NN-based approaches as they are more related to our research.
As previously mentioned, PINN is possibly the most well-known method of this type. PINN regards
the solution to a PDE system as the root of the corresponding operators {Di(g)}n
i=1, and expresses
the time and space boundary conditions as B(g)=0, where gis a candidate solution and DiandB
are operators acting on g. Parameterizing g=gθusing an NN, PINN optimizes its parameters θby
minimizing the residual L(θ)def=∑n
i=1λi∥Di(gθ)∥2
L2(X)+λ0∥B(gθ)∥2
L2(X). The hyperparameters
λibalance the validity of PDEs and boundary conditions under consideration and must be adjusted
for optimal performance. In contrast, EINN requires no hyperparameter tuning. PINN is versatile
and can be applied to a wide range of PDEs, but its performance may not be as good as other NN-
based solvers tailored for a particular class of PDEs, as it does not take into account other in-depth
properties of the system, a phenomenon observed in the literature [Krishnapriyan et al., 2021, Wang
et al., 2022]. [Shin and Em Karniadakis, 2020] initiates the work of theoretically establishing the
consistency of PINN by considering the linear elliptic and parabolic PDEs, for which they prove that
a vanishing PINN loss L(θ)asymptotically implies gθrecovers the true solution. A similar result
is extended to the linear advection equations in [Shin et al., 2020]. Leveraging the stability of the
operators Di(corresponding to PDEs of interest), non-asymptotic error estimations are established
for linear Kolmogorov equations in [De Ryck and Mishra, 2022], for semi-linear and quasi-linear
parabolic equations and the incompressible Euler in [Mishra and Molinaro, 2022], and for the NSE
in [De Ryck et al., 2023]. We highlight these non-asymptotic results are all average-in-time, meaning
that even when the PINN loss is small the deviation of the candidate solution to the true solution
may be significant at a particular timestamp t∈[0, T]. In comparison, our results are uniform-
in-time, i.e. the supremum of the deviation is strictly bounded by the EINN loss. Moreover, we
show in Theorem 2, for the NSE our error estimation holds for any Tuniformly, while the results in
[De Ryck et al., 2023] have an exponential dependence on T.
Recent work from Zhang et al. [2022] proposes the Random Deep V ortex Network (RDVN) method
for solving the 2D NSE and achieves SOTA performance for this task. Let uθ
tbe an estimation of
the interaction term K∗ρtin the SDE (1) and use ρθ
tto denote the law of the particle driven by the
SDEdXt=uθ
t(Xt)dt+√
2νdBt. To train uθ
t, RDVN minimizes the loss L(θ)=∫T
0∫X∥uθ
t(x)−
K∗ρθ
t(x)∥2
L2dxdt. Note that in order to simulate the SDE, one needs to discretize the time variable
in loss function L. After training θ, RDVN outputs ρθ
tas a solution. However, no error estimation
guarantee is provided that controls the discrepancy between ρθ
tandρtusing L(θ).
Shen et al. [2022] propose the concept of self-consistency for the FPE. However, unlike our work
where the EINN loss is derived via the stability analysis, they construct the potential R(f)for
the hypothesis velocity field fby observing that the underlying velocity field f∗is the fixed point
of some velocity-consistent transformation Aand they construct R(f)to be a more complicated
Sobolev norm of the residual f−A(f). In their result, they bound the Wasserstein distance between
ρfandρbyR(f), which is weaker than our KL type control. The improved KL type control for the
Fokker-Planck equation has also been discussed in [Boffi and Vanden-Eijnden, 2023]. A very recent
work [Li et al., 2023] extends the self-consistency approach to compute the general Wasserstein
gradient flow numerically, without providing further theoretical justification.
5 Experiments
To show the efficacy and efficiency of the proposed approach, we conduct numerical studies on
example problems that admit explicit solutions and compare the results with SOTA NN-based PDE
solvers. The included baselines are PINN [Raissi et al., 2019] and DRVN [Zhang et al., 2022].
Note that these baselines only considered the 2D NSE. We extend them to solve the MVE with the
Coulomb interaction for comparison, and the details are discussed in Appendix C.1.
80 2000 4000 6000 8000 10000
# iterations (Lamb-Oseen)0.00.10.20.30.40.5objective valuePINN (Raissi et al., 2019)
EINN (Our method)
DRVN (Zhang et al., 2022)
0 2000 4000 6000 8000 10000
# iterations (Lamb-Oseen)0.00.20.40.60.81.0average rela. l2 error
0 2000 4000 6000 8000 10000
# iterations (Lamb-Oseen)0.00.20.40.60.81.0rela. l2 error at time T
0 2000 4000 6000 8000 10000
# iterations (Barenblatt)0.00.20.40.60.81.0objective value
0 2000 4000 6000 8000 10000
# iterations (Barenblatt)0.000.250.500.751.001.251.501.752.00average rela. l2 error
0 2000 4000 6000 8000 10000
# iterations (Barenblatt)0.000.250.500.751.001.251.501.752.00rela. l2 error at time TFigure 1: The first row contains results for the 2D NSE and the second row contains the results
for the 3D MVE with Coulomb interaction. The first column reports the objective losses, while the
second and third columns report the average and last-time-stamp relative ℓ2error.
Equations with an explicit solution We consider the following two instances that admit explicit
solutions. We verify these solutions in Appendix C.2.
Lamb-Oseen Vortex (2D NSE) [Oseen, 1912]: Consider the whole domain case where X=R2and
the Biot-Savart kernel (4). Let N(µ,Σ)be the Gaussian distribution with mean µand covariance
Σ. Ifρ0=N(0,√2νt0I2)for some t0≥0, then we have ρt(x)=N(0,√
2ν(t+t0)I2).
Barenblatt solutions (MVE) [Serfaty and V ´azquez, 2014]: Consider the 3D MVE with the Coulomb
interaction kernel (3) with the diffusion coefficient set to zero, i.e. d=3andν=0. LetUniform [A]
be the uniform distribution over a set A. Consider the whole domain case where X=R3. Ifρ0=
Uniform [∥x∥≤(3
4πt0)1/3]for some t0≥0, then we have ρt=Uniform [∥x∥≤(3
4π(t+t0))1/3].
Numerical results We present the results of our experiments in Figure 1, where the first row
contains the result for the Lamb-Oseen vortex (2D NSE) and the second row contains the result
for the Barenblatt model (3D MVE). The explicit solutions of these models allow us to assess the
quality of the outputs of the included methods. Specifically, given a hypothesis solution ρf
t, the
ground truth ¯ρtand the interaction kernel K, define the relative ℓ2error at timestamp tasQ(t)def=
∫Ω∥K∗(ρf
t−¯ρt)(x)∥/∥K∗¯ρt(x)∥dx, where Ωis some domain where ρthas non-zero density.
We are particularly interested in the quality of the convolution term K∗ρf
tsince it has physical
meanings. In the Biot-Savart kernel case, it is the velocity of the fluid, while in the Coulomb case, it
is the Coulomb field. We set Ωto be[−2,2]2for the Lamb-Oseen vortex and to [−0.1,0.1]3for the
Barenblatt model. For both models, we take ν=0.1,t0=0.1, and T=1. The neural network that
we use is an MLP with 7hidden layers, each of which has 20 neurons.
From the first column of Figure 1, we see that the objective loss of all methods has substantially
reduced over a training period of 10000 iterations. This excludes the possibility that a baseline has
worse performance because the NN is not well-trained, and hence the quality of the solution now
solely depends on the efficacy of the method. From the second and third columns, we see that
the proposed EINN method significantly outperforms the other two methods in terms of the time-
average relative ℓ2error, i.e.1
T∫T
0Q(t)dtand the relative ℓ2error at the last time stamp Q(T).
This shows the advantage of our method.
Conclusion By employing entropy dissipation of the MVE, we design a potential function for a
hypothesis velocity field such that it controls the KL divergence between the corresponding hypoth-
esis solution and the ground truth, for any time stamp within the period of interest. Built on this
potential, we proposed the EINN method for MVEs with NN and derived the detailed computa-
tion method of the stochastic gradient, using the classic adjoint method. Through empirical studies
on examples of the 2D NSE and the 3D MVE with Coulomb interactions, we show the significant
advantage of the proposed method, when compared with two SOTA NN based MVE solvers.
9Acknowledgments and Disclosure of Funding
Zhenfu Wang is supported by the National Key R&D Program of China, Project Number
2021YFA1002800, NSFC grant No.12171009, Young Elite Scientist Sponsorship Program by China
Association for Science and Technology (CAST) No. YESS20200028 and the Fundamental Re-
search Funds for the Central Universities (the start-up fund), Peking University. Zebang Shen’s
work is supported by ETH research grant and Swiss National Science Foundation (SNSF) Project
Funding No. 200021-207343.
References
N. M. Boffi and E. Vanden-Eijnden. Probability flow solution of the fokker–planck equation. Ma-
chine Learning: Science and Technology , 4(3):035012, 2023.
D. Bresch, P.-E. Jabin, and Z. Wang. On mean-field limits and quantitative estimates with a large
class of singular kernels: application to the patlak–keller–segel model. Comptes Rendus Mathe-
matique , 357(9):708–720, 2019a.
D. Bresch, P.-E. Jabin, and Z. Wang. Modulated free energy and mean field limit. S´eminaire Laurent
Schwartz—EDP et applications , pages 1–22, 2019b.
S. Cai, Z. Mao, Z. Wang, M. Yin, and G. E. Karniadakis. Physics-informed neural networks (pinns)
for fluid mechanics: A review. Acta Mechanica Sinica , 37(12):1727–1738, 2021.
J. A. Carrillo, K. Craig, and F. S. Patacchini. A blob method for diffusion. Calculus of Variations
and Partial Differential Equations , 58:1–53, 2019.
J. A. Carrillo, K. Craig, L. Wang, and C. Wei. Primal dual methods for wasserstein gradient flows.
Foundations of Computational Mathematics , pages 1–55, 2022.
S. Cuomo, V . S. Di Cola, F. Giampaolo, G. Rozza, M. Raissi, and F. Piccialli. Scientific machine
learning through physics–informed neural networks: where we are and what’s next. Journal of
Scientific Computing , 92(3):88, 2022.
T. De Ryck and S. Mishra. Error analysis for physics-informed neural networks (pinns) approximat-
ing kolmogorov pdes. Advances in Computational Mathematics , 48(6):1–40, 2022.
T. De Ryck, S. Lanthaler, and S. Mishra. On the approximation of functions by tanh neural net-
works. Neural Networks , 143:732–750, 2021. ISSN 0893-6080. doi: https://doi.org/10.1016/j.
neunet.2021.08.015. URL https://www.sciencedirect.com/science/article/
pii/S0893608021003208 .
T. De Ryck, A. Jagtap, and S. Mishra. Error estimates for physics-informed neural networks ap-
proximating the navier–stokes equations. IMA Journal of Numerical Analysis , 2023.
L. Erd ˝os and H.-T. Yau. A dynamical approach to random matrix theory , volume 28. American
Mathematical Soc., 2017.
L. C. Evans. Partial differential equations , volume 19. American Mathematical Society, 2022.
F. Golse. On the dynamics of large particle systems in the mean field limit. Macroscopic and large
scale phenomena: coarse graining, mean field limits and ergodicity , pages 1–144, 2016.
A. Guillin, P. L. Bris, and P. Monmarch ´e. Uniform in time propagation of chaos for the 2d vortex
model and other singular stochastic systems. arXiv preprint arXiv:2108.08675 , 2021.
G. Gupta, X. Xiao, and P. Bogdan. Multiwavelet-based operator learning for differential equations.
Advances in neural information processing systems , 34:24048–24062, 2021.
J. Han, A. Jentzen, and W. E. Solving high-dimensional partial differential equations using deep
learning. Proceedings of the National Academy of Sciences , 115(34):8505–8510, 2018.
P.-E. Jabin and Z. Wang. Quantitative estimates of propagation of chaos for stochastic systems with
W−1,∞kernels. Inventiones mathematicae , 214(1):523–591, 2018.
10C. Johnson. Numerical solution of partial differential equations by the finite element method . Courier
Corporation, 2012.
R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the fokker–planck equation.
SIAM journal on mathematical analysis , 29(1):1–17, 1998.
G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-informed
machine learning. Nature Reviews Physics , 3(6):422–440, 2021.
N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandkumar.
Neural operator: Learning maps between function spaces. arXiv preprint arXiv:2108.08481 ,
2021.
A. Krishnapriyan, A. Gholami, S. Zhe, R. Kirby, and M. W. Mahoney. Characterizing possible
failure modes in physics-informed neural networks. Advances in Neural Information Processing
Systems , 34:26548–26560, 2021.
L. Li, S. Hurault, and J. Solomon. Self-consistent velocity matching of probability flows. arXiv
preprint arXiv:2301.13737 , 2023.
Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anand-
kumar. Fourier neural operator for parametric partial differential equations. arXiv preprint
arXiv:2010.08895 , 2020a.
Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandku-
mar. Neural operator: Graph kernel network for partial differential equations. arXiv preprint
arXiv:2003.03485 , 2020b.
D.-G. Long. Convergence of the random vortex method in two dimensions. Journal of the American
Mathematical Society , 1(4):779–804, 1988.
A. J. Majda, A. L. Bertozzi, and A. Ogawa. V orticity and incompressible flow. cambridge texts in
applied mathematics. Appl. Mech. Rev. , 55(4):B77–B78, 2002.
S. Mishra and R. Molinaro. Estimates on the generalization error of physics-informed neural net-
works for approximating pdes. IMA Journal of Numerical Analysis , 43(1):1–43, 01 2022. ISSN
0272-4979. doi: 10.1093/imanum/drab093. URL https://doi.org/10.1093/imanum/
drab093 .
F. Moukalled, L. Mangani, M. Darwish, F. Moukalled, L. Mangani, and M. Darwish. The finite
volume method . Springer, 2016.
C. Oseen. Uber die wirbelbewegung in einer reibenden flussigkeit. Ark. Mat. Astro. Fys. , 7, 1912.
M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learn-
ing framework for solving forward and inverse problems involving nonlinear partial differential
equations. Journal of Computational physics , 378:686–707, 2019.
M. Raissi, A. Yazdani, and G. E. Karniadakis. Hidden fluid mechanics: Learning velocity and
pressure fields from flow visualizations. Science , 367(6481):1026–1030, 2020.
M. Rosenzweig. Mean-field convergence of point vortices to the incompressible euler equation with
vorticity in L∞.Archive for Rational Mechanics and Analysis , 243(3):1361–1431, 2022.
S. Serfaty. Mean field limit for coulomb-type flows. Duke Math. J. , 169(15):2887–2935, 2020.
S. Serfaty and J. L. V ´azquez. A mean field equation as limit of nonlinear diffusions with fractional
laplacian operators. Calculus of Variations and Partial Differential Equations , 49:1091–1120,
2014.
J. Shen, T. Tang, and L.-L. Wang. Spectral methods: algorithms, analysis and applications , vol-
ume 41. Springer Science & Business Media, 2011.
11Z. Shen, Z. Wang, S. Kale, A. Ribeiro, A. Karbasi, and H. Hassani. Self-consistency of the fokker
planck equation. In P.-L. Loh and M. Raginsky, editors, Proceedings of Thirty Fifth Conference
on Learning Theory , volume 178 of Proceedings of Machine Learning Research , pages 817–841.
PMLR, 02–05 Jul 2022. URL https://proceedings.mlr.press/v178/shen22a.
html .
J. Shin, YeonjongDarbon and G. Em Karniadakis. On the convergence of physics informed neural
networks for linear second-order elliptic and parabolic type pdes. Communications in Computa-
tional Physics , 28(5):2042–2074, 2020.
Y . Shin, Z. Zhang, and G. E. Karniadakis. Error estimates of residual minimization using neural
networks for linear pdes. arXiv preprint arXiv:2010.08019 , 2020.
G. D. Smith, G. D. Smith, and G. D. S. Smith. Numerical solution of partial differential equations:
finite difference methods . Oxford university press, 1985.
C. Villani et al. Optimal transport: old and new , volume 338. Springer, 2009.
C. Wang, S. Li, D. He, and L. Wang. Is L2physics informed loss always suitable for training physics
informed neural network? In Advances in Neural Information Processing Systems , 2022.
E. Weinan, J. Han, and A. Jentzen. Algorithms for solving high dimensional pdes: from nonlinear
monte carlo to machine learning. Nonlinearity , 35(1):278, 2021.
X. Xiao, D. Cao, R. Yang, G. Gupta, G. Liu, C. Yin, R. Balan, and P. Bogdan. Coupled mul-
tiwavelet neural operator learning for coupled partial differential equations. arXiv preprint
arXiv:2303.02304 , 2023.
L. Zhang, J. Han, H. Wang, and R. Car. Deep potential molecular dynamics: a scalable model with
the accuracy of quantum mechanics. Physical review letters , 120(14):143001, 2018.
R. Zhang, P. Hu, Q. Meng, Y . Wang, R. Zhu, B. Chen, Z.-M. Ma, and T.-Y . Liu. Drvn (deep random
vortex network): A new physics-informed machine learning method for simulating and inferring
incompressible fluid flows. Physics of Fluids , 34(10):107112, 2022.
12A Classical Methods for Solving MVEs
Solving partial differential equations (PDEs) is a key aspect of scientific research, with a wealth of
literature in the field [Evans, 2022]. For the interest of this paper, we will only consider the methods
that can be used to solve the MVE under consideration.
Categorize PDE solvers via solution representation. To better understand the benefits of neural
network (NN) based PDE solvers and to compare our approach with others, we categorize the liter-
ature based on the representation of the solution to the PDE. These representations can be roughly
grouped into four categories:
•1. Discretization-based representation: The solution to the PDE is represented as discrete func-
tion values at grid points, finite-size cells, or finite-element meshes.
•2. Representation as a combination of basis functions: The solution to the PDE is approximated
as a sum of basis functions, e.g. Fourier series, Legendre polynomials, or Chebyshev polynomials.
•3. Representation using a collection of particles: The solution to the PDE is represented as a
collection of particles, each described by its weight, position, and other relevant information.
•4. NN-based representation: NNs offer many strategies for representing the solution to the PDE,
such as using the NN directly to represent the solution, using normalizing flow or GAN-based
parameterization to ensure the non-negativity and conservation of mass of the solution or using
the NN to parameterize the underlying dynamics of the PDE, such as the time-varying velocity
field that drives the evolution of the system.
The drawback of the first three strategies is that a sparse representation2leads to reduced solution
accuracy, while a dense representation results in increased computational and memory cost. NNs,
as powerful function approximation tools, are expected to surpass these strategies by being able to
handle higher-dimensional, less regular, and more complicated systems [Weinan et al., 2021].
Given a representation strategy of the solution, an effective solver must exploit the underlying prop-
erties of the system to find the best candidate solution. Three-= notable properties that are utilized
to design solvers are
(A) the PDE definition or weak formulation of the system,
(B) the SDE interpretation of the system,
(C) the variational interpretation, particularly the Wasserstein gradient flow interpretation.
These properties are combined with the solution representations mentioned earlier to form different
methods. For example, the Finite Difference method [Smith et al., 1985], Finite V olume method
[Moukalled et al., 2016], and Finite Element method [Johnson, 2012] represent the solution of par-
tial differential equations (PDEs) by discretizing the solution and utilize the property (A), at least
in their original form. On the other hand, recent work by Carrillo et al. [2022] solves PDEs admit-
ting a Wasserstein gradient flow structure using the classic JKO scheme [Jordan et al., 1998], which
is based on the property (C), and the solution is also represented via discretization. The Spectral
method [Shen et al., 2011] is a class of methods that exploits property (A) by representing the so-
lution as a combination of basis functions. The Random V ortex Method [Long, 1988] is a highly
successful method for solving the vorticity formulation of the 2D Navier-Stokes equation by exploit-
ing property (C) and representing the solution with particles. The Blob method from Carrillo et al.
[2019] is another particle-based method for solving PDEs that describe diffusion processes, which
also exploits property (C).
B Comparison with Neural Operator
We thank the anonymous reviewers for pointing out the interesting research direction of neural op-
erators [Xiao et al., 2023, Gupta et al., 2021, Li et al., 2020b, Kovachki et al., 2021, Li et al., 2020a].
However, to highlight the major difference between EINN and the approach of the Neural Operator,
2For example, sparser grid, cell or mesh with less granularity, fewer basis functions, fewer particles.
13it’s worth noting that they consider completely different problem settings: EINN requires no pre-
existing data and the goal is to obtain the solution to a PDE by solely exploiting the structure of the
equation itself. In contrast, the neural operator approach is data-driven , i.e. it relies on the existence
of configuration-solution pairs. Here, by configuration-solution pairs, we mean the correspondence
between some configurations that determine the PDE, e.g. the initial condition or the viscosity
parameter in the fluid dynamics problems, and the pre-existing solution to the PDE given the afore-
mentioned configurations. Consequently, the neural operator approach is more like a regression
problem where a neural network is trained to learn the abstract map between the configuration and
the solution. In contrast, EINN is more like a numerical PDE solver.
Consequently, EINN and the approach of neural operator are two related but quite distinct research
directions. They are related in the sense that EINN can provide the data (configuration-solution
pairs) required by the neural operator approach. They are distinct since EINN requires no data a
priori, while the neural operator approach is built on the supervised learning paradigm.
C More Details on the Experiments
C.1 Implementations of Baselines
Objectives of PINN
• For the vorticity equation of the 2D Navier-Stokes equation, let u∶[0, T]×R2→R2be the
velocity field (this should not be confused with the velocity field of the continuity equation) such
that∇⋅u=0, i.e.uis divergence-free, and let ω=∇×u∈Rbe the vorticity. We have
∂ω
∂t+∇⋅(ωu)=ν∆ω, (24)
ω=∇×u. (25)
We use this form to construct the objective for the PINN method
∫T
0∥∂ω
∂t+∇⋅(ωu)−ν∆ω∥2
L(Ω)2+∥ω−∇×u∥L(Ω)2dt, (26)
whereL2(Ω)denotes the functional L2norm on the domain Ω=[−2,2]2.
• For the MVE with Coulomb interaction, let gbe the Coulomb potential defined in equation 3. We
have that ψ=g∗ρis the solution to the Poisson equation ∆ψ=−ρandK∗ρ=−∇ψ. We have
∂ρ
∂t+∇⋅(ρ⋅(−∇ψ))=ν∆ρ (27)
∆ψ=−ρ. (28)
Expand the the divergence to obtain
∂ρ
∂t+∇ρ⋅(−∇ψ)+ρ⋅(−∆ψ)=ν∆ρ (29)
∆ψ=−ρ. (30)
Now plug in the ∆ψ=−ρto arrive at the following equivalent form
∂ρ
∂t+∇ρ⋅−∇ψ+ρ2=ν∆ρ (31)
∆ψ=−ρ. (32)
We use this form to construct the objective for the PINN method.
∫T
0∥∂ρ
∂t+∇ρ⋅−∇ψ+ρ2−ν∆ρ∥2
L2(Ω)+∥∆ψ+ρ∥2
L2, (33)
whereL2(Ω)denotes the functional L2norm on the domain Ω=[−1,1]2.
DRVN In the original paper [Zhang et al., 2022], only the Biot-Savart kernel is concerned. We
extend the DRVN method to the Coulomb case by setting Kto be the kernel defined in equation 3.
14C.2 Examples with an Explicit Solution
In this section, we verify the explicit solutions discussed in the experiment section.
Lamb-Oseen Vortex on the whole domain R2.Recall that we consider the 2D Navier-Stokes
equation (the MVE with the Biot-Savart interaction kernel (4)). The Lamb-Oseen V ortex model
states that, if ρ0=N(0,√2νt0I2)for some t0≥0, then we have ρt(x)=N(0,√
2ν(t+t0)I2).
To verify this, define ut(x)=1√
ν(t+t0)v(x√
ν(t+t0)), where
v(x)=1
2πx⊥
∥x∥2(1−exp(−1
4∥x∥2)). (34)
One can easily check that ∇⋅ut≡0and hence there exists a function ψtsuch that∇⊥ψt=−ut,
where∇⊥denotes the perpendicular gradient, defined as ∇⊥=(−∂x2, ∂x2), and ψtis called the
stream function in the literature of fluid dynamics. Moreover, one can verify that ∇×ut=ρtwhere
∇×denotes the curl of a 2D velocity field, defined as ∇×u(x)=∂u2/∂x1−∂u1/∂x2. Together
we have
∆ψt=−ρt, (35)
i.e., the stream function ψtis the solution to the 2D Poisson equation with a source term ρt.
Under the boundary condition that ψt(x)→0for∥x∥→∞, we can express ψtvia the unique Green
function G(x)=1
2πln∥x∥as
ψt(x)=G∗ρt=1
2π∫ln∥x−y∥ρt(y)dy. (36)
Consequently, by taking the perpendicular gradient, we obtain
ut=∇⊥ψt=1
2π∫(x−y)⊥
∥x−y∥2ρt(y)dy=K∗ρt. (37)
Finally, by plugging the expressions of ρtandut=K∗ρtin the MVE (2), we verified the Lamb-
Oseen vortex.
Barenblatt solutions for the MVE with Coulomb Interaction. Recall that we consider the MVE
with the Coulomb interaction kernel (3) for d=3and set the diffusion coefficient ν=0, i.e.
∂ρt
∂t+∇⋅(ρt⋅−∇ψt)=0 (38)
where ψtis the solution to the Poisson equation ∆ψt=−ρt. The Barenblatt solution of the above
MVE is stated as follows: If ρ0=Uniform [∥x∥≤(3
4πt0)1/3]for some t0≥0, then we have
ρt=Uniform [∥x∥≤(3
4π(t+t0))1/3] (39)
We now verify this solution.
Recall that the volume of a three dimensional Euclidean ball with radius Ris4π
3R3. Hence we can
write the density function as ρt(x)=1
t+t0χ∥x∥≤(3
4π(t+t0))1/3, where χXis a function that takes value
1forx∈Xand takes value 0forx∉X. Take
ψt(x)=⎧⎪⎪⎨⎪⎪⎩2(3
4π(t+t0))2/3−∥x∥2
6(t+t0),∥x∥≤(3
4π(t+t0))1/3,
1
8π∥x∥, ∥x∥>(3
4π(t+t0))1/3.(40)
It can be verified that the Poisson equation ∆ψt=−ρtholds (note that ∆∥x∥−1=0, i.e.∥x∥−1is a
harmonic function for d=3). Consequently, for a fixed time stamp tand any ∥x∥≤(3
4π(t+t0))1/3
we have
∂ρt
∂t(x)+∇⋅(ρt(x)⋅−∇ψt(x))=−1
(t+t0)2+1
(t+t0)2=0, (41)
which verifies this solution.
15D Adjoint Method
Consider the ODE system
˙s(t)=ψ(s(t), t, θ)
s(0)=s0,
and the objective loss
ℓ(θ)=∫T
0g(s(t), t, θ)dt. (42)
The following proposition computes the gradient of ℓw.r.t. θ. We omit the parameters of the
functions for succinctness. We note that all the functions in the integrands should be evaluated at
the corresponding time stamp t, e.g. b⊺∂h
∂θdtabbreviates for b(t)⊺∂
∂θh(ξ(t), x(t), t, θ)dt.
Proposition 1.
dℓ
dθ=∫T
0a⊺∂ψ
∂θ+∂g
∂θdt. (43)
where a(t)is solution to the following final value problems
˙a⊺+a⊺∂ψ
∂s+∂g
∂s=0, a(T)=0, (44)
Proof. Let us define the Lagrange multiplier function (or the adjoint state) a(t)dual to s(t). More-
over, let Lbe an augmented loss function of the form
L=ℓ−∫T
0a⊺(˙s−ψ)dt. (45)
Since we have ˙s(t)=ψ(s(t), t, θ)by construction, the integral term in Lis always null and acan
be freely assigned while maintaining dL/dθ=dℓ/dθ. Using integral by part, we have
∫T
0a⊺˙sdt=a(t)⊺s(t)∣T
0−∫T
0s⊺˙adt. (46)
We obtain
L=−a(t)⊺s(t)∣T
0+∫T
0˙a⊺s+a⊺ψ+gdt. (47)
Now we compute the gradient of Lw.r.t. θas
dℓ
dθ=dL
dθ=−a(T)⊺dx(T)
dθ+∫T
0˙a⊺ds
dθ+a⊺(∂ψ
∂θ+∂ψ
∂sds
dθ)dt+∫T
0∂g
∂sds
dθ+∂g
∂θdt,
which by rearranging terms yields to
dℓ
dθ=dL
dθ=−a(T)⊺dx(T)
dθ+∫T
0a⊺∂ψ
∂θ+∂g
∂θdt+∫T
0(˙a⊺+a⊺∂ψ
∂s+∂g
∂s)ds
dθdt.
Now by taking asatisfying the final value problems
˙a⊺+a⊺∂ψ
∂s+∂g
∂s=0, a(T)=0, (48)
we derive the result
dℓ
dθ=∫T
0a⊺∂ψ
∂θ+∂g
∂θdt. (49)
D.1 Writing the Trajectory-wise Loss (15) in an ODE-constrained form
We are now ready to write R(fθ;x0)in an ODE-constrained form. Define the state s(t), the initial
condition s0and the transition function ψas follows: Let
s(t)=[x(t), ξ(t),{yi(t)}N
i=1,{ζi(t)}N
i=1], (50)
16withξ(t)=∇logρf
t(x(t))andζi(t)=∇logρf
t(yi(t)). Take the initial condition
s0=[x0, ξ0,{yi(0)}N
i=1,{ζi(0)}N
i=1] (51)
withξ0=∇log ¯ρ0(x0),ζi(0)=∇log ¯ρ0(yi(0)), andyi(0)iid∼¯ρ0; and define the function
ψ(t, s(t);θ)=[ft(x(t);θ), ht(x(t), ξ(t);θ),{ft(yi(t);θ)}N
i=1,{ht(yi(t), ζi(t);θ)}N
i=1],(52)
where h(a,b;θ)=−∇(divft(a;θ))−J⊺
ft(a;θ)b(derived from equation 18). Finally, define
g(t,s(t);θ)=∥f(t,x(t);θ)−(−∇V(x(t))+E(t,s(t))−νξ(t))∥2, (53)
where the estimator E(t,s)of the convolution term is defined as
E(t,s(t))={1
N∑N
i=1Kc(x(t)−yi(t)) the Coulomb case ,
1
N∑N
i=1U(x−yi(t))ζi(t)the Biot-Savart case .(54)
We recall the definition of Uin equation 21 and the definition of Kcin equation 19.
17E Detailed Proofs
Proof of Lemma 1. Recall the McKean-Vlasov equation 6 and the continuity equation 10. We sim-
ply write that ρt=ρf
tand omit the integration domain X. Then
d
dt∫ρtlogρt
¯ρt=∫∂tρtlogρt
¯ρt+∫ρt∂tlogρt−∫ρt∂tlog ¯ρt
=−∫div(ρt([−∇V(x)+K∗ρt−ν∇logρt]+δt))logρt
¯ρt
+∫ρt
¯ρtdiv(¯ρt(−∇V(x)+K∗¯ρt−ν∇log ¯ρt)),
where we note that ∫ρt∂tlogρt=∫∂tρt=0since the total mass is preserved over time. By
integration by parts, one has
d
dt∫ρtlogρt
¯ρt=I1+I2+I3+∫ρtδt⋅∇logρt
¯ρt,
where I1, I2, I3denote the linear, nonlinear interaction, and diffusion parts separately. More pre-
cisely, by integration by parts,
I1=∫div(ρt∇V(x))logρt
¯ρt−∫ρt
¯ρtdiv(¯ρt∇V(x))
=−∫ρt∇V(x)⋅∇logρt
¯ρt+∫¯ρt∇ρt
¯ρt⋅∇V(x)=0.
And
I2=−∫div(ρtK∗ρt)logρt
¯ρt+∫ρt
¯ρtdiv(¯ρtK∗¯ρt)
=∫ρtK∗ρt∇logρt
¯ρt−∫¯ρtK∗¯ρt⋅∇ρt
¯ρt
=∫ρt∇logρt
¯ρt⋅K∗(ρt−¯ρt).
Given that the kernel Kis divergence free, that is divK=0, one further has
I2=−∫ρt∇log ¯ρt⋅K∗(ρt−¯ρt)+∫∇ρt⋅K∗(ρt−¯ρt)
=−∫ρt∇log ¯ρt⋅K∗(ρt−¯ρt).(55)
Note that this modification will be used in the proof in the 2D Navier-Stokes case. Finally, all
diffusion terms sum up to I3which can be further simplified as
I3=ν∫div(ρt∇logρt)logρt
¯ρt−ν∫ρt
¯ρtdiv(¯ρt∇log ¯ρt)
=−ν∫ρt∇logρt⋅∇logρt
¯ρt+ν∫¯ρt∇log ¯ρt⋅∇ρt
¯ρt
=−ν∫ρt∣∇logρt
¯ρt∣2.
We thus complete the proof of Lemma 1.
Proof of Lemma 2. Recall that K=−∇g. For simplicity, we write that ρt=ρf
t. Then
d
dtF(ρt,¯ρt)=d
dt1
2∫X2g(x−y)d(ρt−¯ρt)⊗2(x, y)
=∫Xg∗(ρt−¯ρt)(x)(∂tρt(x)−∂t¯ρt(x))dx
=∫g∗(ρt−¯ρt)(x)div{ρt([∇V(x)−K∗ρt+ν∇logρt]−δt)
−¯ρt(∇V(x)−K∗¯ρt+νlog ¯ρt)}
=J1+J2+J3+J4,
18where J1, J2, J3, J4denote the perturbation term, the linear difference term, the nonlinear difference
term, and the diffusion term respectively. The perturbation term J1reads
J1=−∫Xg∗(ρt−¯ρt)div(ρtδt)=−∫XρtK∗(ρt−¯ρt)⋅δt.
By integration by parts, the linear difference term can be written as
J2=∫Xg∗(ρt−¯ρt)div((ρt−¯ρt)∇V)=∫XK∗(ρt−¯ρt)(ρt−¯ρt)∇V
=1
2∫X2K(x−y)(∇V(x)−∇V(y))d(ρt−¯ρt)⊗2(x, y),
where the last equality is true since K=−∇gis an odd function and we do the symmetrization trick,
i.e. exchanging the role of xandyto another term and then taking the average.
The nonlinear difference term reads
J3=−∫Xg∗(ρt−¯ρt)div(ρtK∗ρt−¯ρtK∗¯ρt)
=−∫XK∗(ρt−¯ρt)(ρtK∗(ρt−¯ρt)−∫XK∗(ρt−¯ρt)(ρt−¯ρt)K∗¯ρt
=−∫Xρt∣K∗(ρt−¯ρt)∣2−1
2∫K(x−y)(K∗¯ρt(x)−K∗¯ρt(y))d(ρt−¯ρt)⊗2(x, y),
where again in the last term we do the symmetrization.
The diffusion term reads
J4=ν∫g∗(ρt−¯ρt)div(ρt∇logρt−¯ρt∇log ¯ρt)
=ν∫K∗(ρt−¯ρt)ρt∇logρt
¯ρt+ν∫K∗(ρt−¯ρt)(ρt−¯ρt)∇log ¯ρt
=ν∫XρtK∗(ρt−¯ρt)⋅∇logρt
¯ρt
+ν
2∫X2K(x−y)(∇log ¯ρt(x)−∇log ¯ρt(y))d(ρt−¯ρt)⊗2.
To sum it up, we prove the thesis.
E.1 Proof of the 2D Navier-Stokes case
Now we proceed to control the growth of the KL divergence KL(ρf
t∣¯ρt)for the 2D Navier-Stokes
case. Since the Biot-Savart law is divergence free, by equation 55 in the proof of Lemma 1, one has
d
dt∫Πdρtlogρt
¯ρt=−ν∫Πdρt∣∇logρt
¯ρt∣2−∫ΠdρtK∗(ρt−¯ρt)⋅∇log ¯ρt+∫Πdρtδt⋅∇logρt
¯ρt.(56)
Recall that we write the kernel K=(K1,⋯, Kd)and its component Ki=∑d
j=1∂xjUij(x), where
U=(Uij)1≤i,j≤dis a matrix-valued potential function for instance can be defined as in equation 21.
Consequently
−∫ρtK∗(ρt−¯ρt)⋅∇log ¯ρt=−d
∑
i,j=1∫ρt∂xjUij∗(ρt−¯ρt)∂xilog ¯ρt,
which equals to
d
∑
i,j=1∫Uij∗(ρt−¯ρt)∂xj(ρt
¯ρt∂xi¯ρt)=A+B
by integration by parts, where further
A=d
∑
i,j=1∫Vij∗(ρt−¯ρt)∂xi¯ρt∂xjρt
¯ρt=∫U∗(ρt−¯ρt)∶∇¯ρt⊗∇ρt
¯ρt,
19and
B=d
∑
i,j=1∫ρtUij∗(ρt−¯ρt)∂2
xixj¯ρt
¯ρt=∫ρtU∗(ρt−¯ρt)∶∇2¯ρt
¯ρt.
Noticing that ∇ρt
¯ρt=ρt
¯ρt∇logρt
¯ρt, one estimates Aas follows
A=∫ρtU∗(ρt−¯ρt)∶∇log ¯ρt⊗∇logρt
¯ρt
≤ν
4∫ρt∣∇logρt
¯ρt∣2+1
ν∫ρt∣(∇log ¯ρt)⊺U∗(ρ−¯ρ)∣2
≤ν
4∫ρt∣∇logρt
¯ρt∣2+1
ν∥U∥2
L∞∥∇log ¯ρt∥2
L∞∥ρt−¯ρt∥2
L1,
and again by Csisz ´ar–Kullback–Pinsker inequality, one has that
A≤ν
4∫ρt∣∇logρt
¯ρt∣2+2
ν∥U∥2
L∞∥∇log ¯ρt∥2
L∞∫ρtlogρt
¯ρt.
Now it only remains to control B. Recall the following famous Gibbs inequality
Lemma 4 (Gibbs inequality) .For any parameter η>0, and probability measures ρ,¯ρ∈P(X)∩
L1(X), and ϕa real-valued function defined on X, one has the following change of reference
measure inequality
∫Xρ(x)ϕ(x)dx≤1
η(∫Xρ(x)logρ(x)
¯ρ(x)dx+log∫X¯ρ(x)exp(ηϕ(x))dx).
The proof of this inequality can be found in section 13.1 in [Erd ˝os and Yau, 2017].
To control B, we write that ϕ=U∗(ρt−¯ρt)∶∇2¯ρt
¯ρtand thus B=∫ρtϕ. We choose a positive
parameter η>0such that
1
η=2∥U∥L∞∥∇2¯ρt
¯ρt∥
L∞.
Now we apply Lemma 4 to obtain that
B=∫ρtϕ≤1
η(∫ρtlogρt
¯ρt+log∫¯ρtexp(ηϕ)).
Note that η>0is chosen so small such that
η∥ϕ∥L∞≤1
2∥U∥L∞∥∇2¯ρt
¯ρt∥
L∞∥U∥L∞∥ρt−¯ρt∥L1∥∇2¯ρt
¯ρt∥
L∞
≤1
2∥ρt−¯ρt∥L1≤1,
since for two probability densities it always holds ∥ρt−¯ρt∥L1≤2. Consequently, applying the
inequality exp(x)≤1+x+e
2x2for∣x∣≤1, we have
∫¯ρtexp(ηϕ)≤∫¯ρt(1+ηϕ+e
2η2ϕ2)≤1+0+e
2(1
2∥ρt−¯ρt∥L1)2
≤1+e
4KL(ρt∣¯ρt),
where
∫¯ρtϕ=∫U∗(ρt−¯ρt)∶∇2¯ρt=∫d
∑
i,j=1∂xixjU∗(ρt−¯ρt)¯ρt=∫divK∗(ρt−¯ρt)¯ρt=0,
since divK=0.
To sum it up, in particular since log(1+x)≤xforx>0, one has
B≤1
η(1+e
4)KL(ρt∣¯ρt)≤4∥U∥L∞∥∇2¯ρt
¯ρt∥
L∞KL(ρt∣¯ρt).
20Combining equation 56, the estimates for AandB, one has
d
dt∫ρtlogρt
¯ρt≤−3ν
4∫ρt∣∇logρt
¯ρt∣2+M(t)∫ρtlogρt
¯ρt+∫ρtδt⋅∇logρt
¯ρt
≤−ν
2∫ρt∣∇logρt
¯ρt∣2+M(t)∫ρtlogρt
¯ρt+1
ν∫ρt∣δt∣2(57)
where
M(t)=2
ν∥U∥2
L∞∥∇log ¯ρt∥2
L∞+4∥U∥L∞∥∇2¯ρt
¯ρt∥
L∞=M(t;ν, U,¯ρt).
Since the matrix-valued potential function Uis bounded ( ∥U(x)∥op≤1/4when Utakse the form
(21)), and under suitable assumptions for the initial data ¯ρ0(for instance ¯ρ0∈C3and there exists
c>1s.t.1
c≤¯ρ≤c), one can obtain supt∈[0,T]M(t)≤M<∞. We recall Theorem 2 in [Guillin
et al., 2021] as below for completeness.
Theorem 5. Given the initial data ¯ρ0∈C∞(Πd), such that there exists c>1,1
c≤¯ρ0≤c. Then the
vorticity formulation of the 2D Navier-Stokes equation
∂t¯ρt+div(¯ρtK∗¯ρt)=ν∆¯ρt,¯ρ(0, x)=¯ρ0(x),
has a unique bounded solution ¯ρ(t, x)∈C∞([0,∞)×Πd), and for any t>0, for any x∈Πd, it
holds that1
c≤¯ρ(t, x)≤c.
Finally, we simplify equation 57 to obtain that
d
dt∫ρtlogρt
¯ρt≤M∫ρtlogρt
¯ρt+1
ν∫ρt∣δt∣2,
where M=supt∈[0,T]M(t;ν, U,¯ρt)<∞. By Gronwall inequality, one finally obtains that
sup
t∈[0,T]∫Πdρtlogρt
¯ρtdx≤1
νexp(MT)R(θ).
As noted in [Guillin et al., 2021],in particular Corollary 2 there, one can improve the above time-
dependent estimate ( exp(MT)) to uniform-in-time estimate by using Logarithmic Sobolev inequal-
ity. Indeed, given that1
c≤¯ρt≤c, one has that
∫Πdρtlogρt
¯ρtdx≤c2
8π2∫Πdρt∣∇xlogρt
¯ρt∣2dx. (58)
Combining equation 58 and equation 57, one obtains that
d
dtKL(ρt∣¯ρt)≤(M(t)−4π2ν
c2)KL(ρt∣¯ρt)+1
ν∫ρt∣δt∣2.
Multiplying the factor exp(4π2ν
c2t−∫t
0M(s)ds)and noting in particular KL(ρ0∣¯ρ0)=0, one obtains
that
KL(ρt∣¯ρt)≤∫t
0exp(4π2ν
c2(s−t)+∫t
sM(u)du)f(s)ds.
Indeed, under the assumptions as in Theorem 2, one has that there exists a universal C>0, such that
∫∞
0M(t)dt=C<∞.
We thus immediately obtain that
sup
t∈[0,T]KL(ρt∣¯ρt)≤eC
ν∫T
0∫Πdρt∣δt∣2dxdt.
This completes the proof of Theorem 2.
21The McKean-Vlasov PDEs, i.e. equation 2, with bounded interactions K∈L∞As mentioned
in the main body of this article, it is much easier to obtain the stability estimate for the McKean-
Vlasov PDE with bounded interactions.
Theorem 6 (Stability Estimate for McKean-Vlasov PDE with K∈L∞).Assume that K∈L∞. One
has the estimate that
sup
t∈[0,T]KL(ρf
t∣¯ρt)≤1
νexp(2∥K∥2
L∞
νT)R(f),
where we recall the self-consitency potential/loss function R(θ)reads
R(f)=∫T
0∫X∣f(t, x)+∇V(x)−K∗ρf
t+ν∇logρθ
t∣2dρf
t(x)dt.
Proof. Here we give the control of the growth of the KL divergence for systems with bounded
kernels. Applying Cauchy-Schwarz inequality twice for the entropy dissipation terms in Lemma 1
to obtain
∫ΠdρtK∗(ρt−¯ρt)⋅∇logρt
¯ρt≤ν
4∫ρt∣∇logρt
¯ρt∣2+1
ν∫ρt∣K∗(ρt−¯ρt)∣2,
and
∫Πdρtδt⋅∇logρt
¯ρt≤ν
4∫ρt∣∇logρt
¯ρt∣2+1
ν∫ρt∣δt∣2.
Furthermore,
∫ρt∣K∗(ρt−¯ρt)∣2≤∥K∥2
L∞∥ρt−¯ρt∥2
L1≤2∥K∥2
L∞∫ρtlogρt
¯ρt,
where the last inequality is simply the Csisz ´ar–Kullback–Pinsker inequality [Villani et al., 2009].
Combining the above estimates, we obtain that given that K∈L∞,
d
dt∫Πdρtlogρt
¯ρt=−ν
2∫Πdρt∣∇logρt
¯ρt∣2+2∥K∥2
L∞
ν∫ρtlogρt
¯ρt+1
ν∫ρt∣δt∣2.
Currently, we are not interested in the long time behavior, so we first ignore the negative term above
to obtain that
d
dt∫Πdρtlogρt
¯ρt≤2∥K∥2
L∞
ν∫ρtlogρt
¯ρt+1
ν∫ρt∣δt∣2.
By Gronwall inequality, we obtain that
∫Πdρtlogρt
¯ρt≤1
νexp(2∥K∥2
L∞
νt)∫t
0∫ρs∣δs∣2dxds.
E.2 The McKean-Vlasov equation with Coulomb interactions
Proof of Theorem 3. We first prove the case when ν>0. Applying Cauchy-Schwarz inequality to
the right-hand side ofd
dtE(ρf
t,¯ρt)in Lemma 3, one has
d
dtE(ρf
t,¯ρt)≤1
2∫Xρf
t∣δt∣2dx
−1
2∫X2K(x−y)⋅(A[¯ρt](x)−A[¯ρt](y))d(ρf
t−¯ρt)⊗2(x, y).
By Lemma 5.2 in Bresch et al. [2019b], as long as the ground truth “velocity field” A[¯ρt]is Lips-
chitz, i.e. A[¯ρ]∈W1,∞, or equivalently ∇2V∈W1,∞,∇2log ¯ρt∈L∞, K∗¯ρt∈W1,∞, using the
particular structure introduced by the Coulomb interactions (note that −∆g=δ0andK=−∇g), we
have the estimate
−1
2∫X2K(x−y)⋅(A[¯ρt](x)−A[¯ρt](y))d(ρf
t−¯ρt)⊗2(x, y)
≤C∥∇A[¯ρt]∥L∞F(¯ρf
t,¯ρt).
22This estimate can be obtained either by Fourier method [Bresch et al., 2019b] or by the stress-energy
tensor approach as in [Serfaty, 2020]. We emphasize that those assumptions made on (¯ρt)t∈[0,T]can
be obtained by propagating similar conditions on the initial data ¯ρ0. This estimate actually holds
for more general choices of gorK. See more examples including Riesz kernels in [Bresch et al.,
2019b]. Moreover, the Lipschitz regularity of A[¯ρt]can also be relaxed a bit. See for instance in
[Rosenzweig, 2022].
Combining previous two estimates, one has
d
dtE(ρf
t,¯ρt)≤1
2∫Xρf
t∣δt∣2dx+CC1F(ρf
t,¯ρt)≤1
2∫Xρf
t∣δt∣2dx+CC1E(ρf
t,¯ρt).
Then applying Gronwall inequality concludes the proof of the case when ν>0.
Now we prove the deterministic case when ν=0. Now the relative entropy or KL divergence does
not play a role since there is no Laplacian term in equation 2. Lemma 2 now reads
d
dtF(ρf
t,¯ρt)=−∫Xρf
t∣K∗(ρf
t−¯ρ)∣2−∫Xρf
tδt⋅K∗(ρf
t−¯ρt)
−1
2∫X2K(x−y)⋅(A[¯ρt](x)−A[¯ρt](y))d(ρf
t−¯ρt)⊗2(x, y).
Applying Cauchy-Schwarz to the 2nd term in the right-hand side above, we obtain that
d
dtF(ρf
t,¯ρt)≤1
2∫Xρf
t∣δt∣2−1
2∫X2K(x−y)⋅(A[¯ρt](x)−A[¯ρt](y))d(ρf
t−¯ρt)⊗2(x, y).
Again assuming that the “velocity field” A[¯ρt](⋅)is Lipschitz will give us
d
dtF(ρf
t,¯ρt)≤1
2∫Xρf
t∣δt∣2+CC1F(ρf
t,¯ρt).
Applying Gronwall inequality again conclude all the proof.
23F Approximation Error of Neural Network
We show that in a function class Fwith sufficient capacity, there exists at least one element ˆf∈F
such that R(ˆf)is small. In particular, we are interested in the function class of neural networks.
We will focus on the case where the domain is the torus X=Πd, i.e. a ddimensional box with size
Lendowed with the periodic boundary condition. For the simplicity of notations, we denote the
underlying velocity by ¯ft=A[¯ρt], where the operator Ais defined in equation 5.
In the following, we focus on the Coulomb case where Kis defined in equation 3. The Biot-Savart
case (4) can be treated similarly.
Proof of Theorem 4. In equation 14, we showed that for any hypothesis velocity f, theEINN loss
R(f)admits the trajectory-wise reformulation:
R(f)=∫Xdx0¯ρ0(x)∫T
0dt∥δf
t○Xf
t(x0)∥2, (59)
where we recall the definition of δf
tin equation 11. Note that, as a general principle, in this proof,
we will use the superscript to emphasize the dependence on a velocity f, e.g. the flow map Xf
t.
From Assumption 2 we know that there exists ˆf∈Fsuch that ∥¯f−ˆf∥W2,∞(X)≤ϵ. In the following,
we show that R(ˆf)is small.
Define
Af
x(t)def=∫t
0∥δf
s○Xf
s(x)∥2ds. (60)
We have
R(f)=∫XAf
x(T)d¯ρ0(x). (61)
Recall that ¯fdenotes the underlying velocity and hence δ¯f
t≡0andρ¯f
t≡¯ρt, where we recall that ρ¯f
t
is the solution to the continuity equation (7) with velocity field ¯f. We can bound
∂
∂tAˆf
x(t)=∥δˆf
t○Xˆf
t(x)∥2=∥δˆf
t○Xˆf
t(x)−δ¯f
t○X¯f
t(x)∥2
≤4∥(ˆft○Xˆf
t−¯ft○X¯f
t)(x)∥2+4∥(∇V○Xˆf
t−∇V○X¯f
t)(x)∥2
+4∥((K∗ρˆf
t)○Xˆf
t−(K∗¯ρt)○X¯f
t)(x)∥2+4ν2∥(∇logρˆf
t○Xˆf
t−∇log ¯ρt○X¯f
t)(x)∥2
=1⃝+2⃝+3⃝+4⃝. (62)
We will bound each term on the R.H.S. individually. The following lemmas will be useful:
Lemma 5. For two Lipschitz continuous velocity field f1, f2∈C1(X), we have for any t∈[0, T]
∥Xf1
t(x)−Xf2
t(x)∥2≤A1(T)∥f1−f2∥2
L∞(X).
Proof. Denote xi(t)=Xfi
t(x0)fori=1,2.
d
dt∥x1(t)−x2(t)∥2≤∥x1(t)−x2(t)∥2+∥f1(t,x1(t))−f2(t,x2(t))∥2
≤C∥x1(t)−x2(t)∥2+∥f1(t,x2(t))−f2(t,x2(t))∥2
≤C∥x1(t)−x2(t)∥2+∥f1−f2∥2
L∞(X).
Using the Gr ¨onwall’s inequality, we have the result.
Lemma 6. Suppose that f∈C1is Lipschitz continuous. We have that Xf
tis an A2(T)-Lipschitz
continuous map. For f∈L∞(X), we have ∥Xf
t(x)∥2≤∥x∥2+t∥f∥2
L∞.
24Proof. Denote xi(t)=Xf
t(xi
0)fori=1,2.
d
dt∥x1(t)−x2(t)∥2≤∥x1(t)−x2(t)∥2+∥f(t,x1(t))−f(t,x2(t))∥2
≤C∥x1(t)−x2(t)∥2.
Using the Gr ¨onwall’s inequality, we have that Xf
tis Lipschitz continuous.
Lemma 7. Forf∈C2(X), suppose that ∇(divf)∈L∞(X)andJf∈L∞(X). Further suppose
that the initial distribution ¯ρ0satisfies∇log ¯ρ0∈L∞(X). We have that ∇logρf
t○Xf
t∈L∞(X).
Proof. Denote x(t)=Xf
t(x0). From equation 18, we have
d
dt∥∇logρf
t(x(t))∥2≤C(1+∥∇logρf
t(x(t))∥2). (63)
Using the Gr ¨onwall’s inequality, we have ∥∇logρf
t(x(t))∥2<∞forx0∈X.
Bounding 1⃝in equation 62 We have
∥(ˆft○Xˆf
t−¯ft○X¯f
t)(x)∥
≤∥(ˆft○Xˆf
t−¯ft○Xˆf
t)(x)∥+∥(¯ft○Xˆf
t−¯ft○X¯f
t)(x)∥
≤ϵ+ϵ⋅∥Xˆf
t(x)−X¯f
t(x)∥≤C1(T)ϵ.
Bounding 2⃝in equation 62 We have from Assumption 2 and Lemma 5
∥(∇V○Xˆf
t−∇V○X¯f
t)(x)∥≤C2(T)ϵ.
Bounding 3⃝in equation 62 Bounding 3⃝in equation 62 requires a more sophisticated analysis
which is the major technical challenge of this proof. For the simplicity of notations, for a fixed x
andy, denote ˆx(t)=Xˆf
t(x),¯x(t)=X¯f
t(x),ˆy(t)=Xˆf
t(y),¯y(t)=X¯f
t(y). For any ϵ′which is to
be determined later, we have
∥((K∗ρˆf
t)○Xˆf
t−(K∗¯ρt)○X¯f
t)(x)∥2=∥∫XK(ˆx(t)−ˆy(t))−K(¯x(t)−¯y(t))d¯ρ0(y)∥2
≤2∥∫∥ˆx(t)−ˆy(t)∥≤ϵ′K(ˆx(t)−ˆy(t))−K(¯x(t)−¯y(t))d¯ρ0(y)∥2(A⃝)
+2∥∫A2(T)L≥∥ˆx(t)−ˆy(t)∥≥ϵ′K(ˆx(t)−ˆy(t))−K(¯x(t)−¯y(t))d¯ρ0(y)∥2. (B⃝)
Note that the upper bound on ∥ˆx(t)−ˆy(t)∥inB⃝comes from Lemma 6 and the facts that X=Πd
is bounded with size L. To bound A⃝, we have
∥∫∥ˆx(t)−ˆy(t)∥≤ϵ′K(ˆx(t)−ˆy(t))−K(¯x(t)−¯y(t))d¯ρ0(y)∥
≤∫∥ˆx(t)−ˆy(t)∥≤ϵ′∥K(ˆx(t)−ˆy(t))∥+∥K(¯x(t)−¯y(t))∥d¯ρ0(y)
=∫∥ˆx(t)−ˆy(t)∥≤ϵ′1
∥ˆx(t)−ˆy(t)∥d−1+1
∥¯x(t)−¯y(t)∥d−1d¯ρ0(y)=C⃝+D⃝.
We can bound C⃝by
∫∥ˆx(t)−ˆy(t)∥≤ϵ′1
∥ˆx(t)−ˆy(t)∥d−1d¯ρ0(y)=∫∥ˆx(t)−y∥≤ϵ′1
∥ˆx(t)−y∥d−1dρˆf
t(y)≤∥ρˆf
t∥∞⋅ϵ′,(64)
where in the above inequality we remove the singular term by using transforming to the polar coor-
dinate system. To bound D⃝, we pick ϵ′=dA1(T)ϵ, so that Lemma 5 implies
{y∈X∣∥ˆx(t)−ˆy(t)∥≤ϵ′}⊆{y∈X∣∥¯x(t)−¯y(t)∥≤d+2
dϵ′},
25and consequently
∫∥ˆx(t)−ˆy(t)∥≤ϵ′1
∥¯x(t)−¯y(t)∥d−1d¯ρ0(y)≤∫∥¯x(t)−¯y(t)∥≤2ϵ′1
∥¯x(t)−¯y(t)∥d−1d¯ρ0(y)≤d+2
d∥ρˆf
t∥∞⋅ϵ′.
(65)
To bound B⃝, note that
∇K(x)=1
∥x∥d+2(∥x∥2I−d⋅x⊗x)⇒∥∇K(x)∥≤d
∥x∥d. (66)
Denote z(t)=min(∥ˆx(t)−ˆy(t)∥,∥¯x(t)−¯y(t)∥). Recall the choice of ϵ′=dA1(T)ϵ. Using
Lemma 5, we have that
∥z(t)∥≥d−2
d∥ˆx(t)−ˆy(t)∥.
Using the triangle inequality, we have
∥∫A2(T)L≥∥ˆx(t)−ˆy(t)∥≥ϵ′K(ˆx(t)−ˆy(t))−K(¯x(t)−¯y(t))d¯ρ0(y)∥
≤∫A2(T)L≥∥ˆx(t)−ˆy(t)∥≥ϵ′∥K(ˆx(t)−ˆy(t))−K(¯x(t)−¯y(t))∥d¯ρ0(y)
≤2ϵ′d∫A2(T)L≥∥ˆx(t)−ˆy(t)∥≥ϵ′1
∥z(t)∥dd¯ρ0(y)
≤2ϵ′d∫A2(T)L≥∥ˆx(t)−ˆy(t)∥≥ϵ′(d
d−2)d 1
∥ˆx(t)−ˆy(t)∥dd¯ρ0(y)
≤2eϵ′d∥ρˆf
t∥∞∫A2(T)L≥∥ˆx(t)−y∥≥.5ϵ′1
∥y∥ddy
=2eϵ′d∥ρˆf
t∥∞ln(A2(T)L/ϵ′).
Combining the bounds of A⃝andB⃝, we have that
3⃝≤C3(T)(ϵln1
ϵ)2. (67)
Bounding 4⃝in equation 62 Denote ˆx(t)=Xˆf
t(x)and¯x(t)=X¯f
t(x). Define
Bx(t)def=∥(∇logρˆf
t○Xˆf
t−∇log ¯ρt○X¯f
t)(x)∥2=∥∇logρˆf
t(ˆx(t))−∇log ¯ρt(¯x(t))∥2.(68)
Computing its dynamics
d
dtBx(t)≤Bx(t)+∥d
dt(∇logρˆf
t(ˆx(t))−∇log ¯ρt(¯x(t)))∥2(69)
Recall equation 18. We have that
d
dt∇logρˆf
t(ˆx(t))=−∇(∇⋅ˆft(ˆx(t)))−(Jˆft(ˆx(t)))⊺
∇logρˆf
t(ˆx(t)), (70)
d
dt∇logρ¯f
t(¯x(t))=−∇(∇⋅¯ft(¯x(t)))−(J¯ft(¯x(t)))⊺∇logρ¯f
t(¯x(t)), (71)
and hence
∥d
dt(∇logρˆf
t(ˆx(t))−∇log ¯ρt(ˆx(t)))∥2
≤2∥∇(∇⋅ˆft(ˆx(t)))−∇(∇⋅¯ft(¯x(t)))∥2+2∥(Jˆft(ˆx(t)))⊺
∇logρˆf
t(ˆx(t))−(J¯ft(¯x(t)))⊺∇logρ¯f
t(¯x(t))∥2
=E⃝+F⃝.
We now bound these two terms individually. To bound E⃝,
∥∇(∇⋅ˆft(ˆx(t)))−∇(∇⋅¯ft(¯x(t)))∥
≤∥∇(∇⋅ˆft(ˆx(t)))−∇(∇⋅ˆft(¯x(t)))∥+∥∇(∇⋅ˆft(¯x(t)))−∇(∇⋅¯ft(¯x(t)))∥≤ϵ+LA1(T)ϵ.
26To bound F⃝
∥(Jˆft(ˆx(t)))⊺
∇logρˆf
t(ˆx(t))−(J¯ft(¯x(t)))⊺∇logρ¯f
t(¯x(t))∥2
≤2∥((Jˆft(ˆx(t)))⊺
−(J¯ft(¯x(t)))⊺)∇logρˆf
t(ˆx(t))∥2+2∥(J¯ft(¯x(t)))⊺(∇logρˆf
t(ˆx(t))−∇logρ¯f
t(¯x(t)))∥2
≤2(1+LA1(T))2ϵ2+2L2Bx(t).
Consequently, using Gr ¨onwall’s inequality, we have that
4⃝≤C4(T)ϵ2. (72)
Combining all the estimations for 1⃝to4⃝, we have that
R(ˆf)≤C(T)ϵ2(ln1
ϵ)2, (73)
for some constant C(T)independent of ϵ.
27G Discussion on the Unbounded Case
In Section 3, we considered the torus case, i.e. Xis ad-dimensional box with size Lwith a periodic
boundary condition. In this section, we consider the unbounded case, i.e. X=Rd. There are two
major differences:
1. The first difference is that when X=Rd, we would obtain an additional integral-of-divergence
term from the operation of integration by parts. When Xis a torus, using Gauss’s divergence
theorem and the periodic boundary condition, this term immediately vanishes, which simplifies
the analysis. In contrast, for the unbounded case, we need to handle this term by assuming some
additional regularity conditions.
2. The second difference is that for the torus, it is reasonable to assume that the initial distribution
¯ρ0is fully supported, which is equivalent to the existence of some constant c>0such that
¯ρ0(x)≥cfor all x∈X. Such an assumption will allow us to propagate the regularity of the
initial distribution ¯ρ0to the solution at time t, i.e. ¯ρt. In contrast, for the unbounded case, such
an assumption clearly does not hold since otherwise ¯ρ0would not be integrable. Consequently,
we can no long propagate the regularity of the initial distribution and hence we need to directly
make regularity assumptions on ¯ρt.
In the following, we will focus on addressing the first point and provide sufficient conditions such
that Lemmas 1 and 2 can be recovered even in the unbounded case. To elaborate a bit on the second
point, the theorems that are derived in the main body of the submission remain valid under the
regularity assumptions given therein. However, unlike the torus case, it is difficult to establish these
regularity results for the unbounded case by assuming the regularity of the initial distribution ¯ρ0.
Lemma 8 (Analogy of Lemma 1 in the unbounded case) .Given the hypothesis velocity field f=
f(t, x)∈C1
t,x. Assume that (ρf
t)t∈[0,T]and(¯ρt)t∈[0,T]are classical solutions to equation (7) and
equation (6) respectively. It holds that (recall the definition of δtin equation (11))
d
dt∫Xρf
tlogρf
t
¯ρt=−ν∫Xρf
t∣∇logρf
t
¯ρt∣2+∫Xρf
tK∗(ρf
t−¯ρt)⋅∇logρf
t
¯ρt
+∫Xρf
tδt⋅∇logρf
t
¯ρt−∫div(ρf
t(ftlogρf
t
¯ρt−¯ft)).
whereXis the tours Πd. All the integrands are evaluated at x.
Proof. Recall the McKean-Vlasov equation 6 and the continuity equation 10. For simplicity, we
write that ρt=ρf
tand¯ft=A[¯ρt]. Then
d
dt∫ρtlogρt
¯ρt=−∫div(ρtft)logρt
¯ρt+∫ρt
¯ρtdiv(¯ρt¯ft)
=∫ρtft∇logρt
¯ρt−∫∇ρt
¯ρt¯ρt¯ft−∫div(ρt(ftlogρt
¯ρt−¯ft)).
We handle the first two terms on the R.H.S. just like the torus case and we can have the result.
Lemma 9 (Analogy of Lemma 2 in the unbounded case) .Under the same assumptions as in Lemma
1, given the diffusion coefficient ν≥0, it holds that (recall the definition of δtin equation (11))
d
dtF(ρf
t,¯ρt)=−∫Xρf
t∥K∗(ρf
t−¯ρt)∥2−∫Xρf
tδt⋅K∗(ρf
t−¯ρt)+ν∫Xρf
tK∗(ρf
t−¯ρt)⋅∇logρf
t
¯ρt
−1
2∫X2K(x−y)⋅(A[¯ρt](x)−A[¯ρt](y))d(ρf
t−¯ρt)⊗2(x, y)
−∫div{g∗(ρf
t−¯ρt)(x)(ρf
t(x)ft(x)−¯ρt(x)¯ft(x))}dx
where we recall that the operator Ais defined in equation (5).
28Proof. Recall that K=−∇g. For simplicity, we write that ρt=ρf
t. Then
d
dtF(ρt,¯ρt)=d
dt1
2∫X2g(x−y)d(ρt−¯ρt)⊗2(x, y)
=∫Xg∗(ρt−¯ρt)(x)(∂tρt(x)−∂t¯ρt(x))dx
=−∫g∗(ρt−¯ρt)(x)div{ρt(x)ft(x)−¯ρt(x)¯ft(x)}dx
=∫∇g∗(ρt−¯ρt)(x){ρt(x)ft(x)−¯ρt(x)¯ft(x)}dx
−∫div{g∗(ρt−¯ρt)(x)(ρt(x)ft(x)−¯ρt(x)¯ft(x))}dx
We handle the first term on the R.H.S. just like the torus case and we can have the result.
G.1 Handling the Integral of the Divergence
Given a vector field, the following lemma provides a sufficient condition for the volume integral
of its divergence over Xto be zero. The idea is to construct a sequence of approximations to the
integral of interest, each of which involves integration over a compact set. Consequently, Gauss’s
divergence theorem can be applied. We then utilize the dominant convergence theorem to exchange
the order of the limit and integral.
Lemma 10. For a vector function g∶Rd→Rdwhich satisfies
∫Rddx∣divg(x)∣<∞ and∫Rddx∥g(x)∥<∞, (74)
we have
∫Rddxdivg(x)=0. (75)
Proof. Choose a cut-off function, indexed by r>1, satisfying
Φr(x)=⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩1, if∥x∥≤r,
1
2(1+cos(π∥x∥/r−1)),ifr<∥x∥≤2r,
0, if2r<∥x∥.(76)
We have ∥∇Φr∥L∞=O(1/r). Using the chain rule of divergence, we have that
divx(g⋅Φr)=divx(g)⋅Φr+g⋅∇Φr. (77)
We have ∫Rddxdivx(gΦr)(x)=0for all randx, by noting gΦr(x)=0for∥x∥>2rand using
Gauss’s divergence theorem on the xvariable. Using conditions (74) and the dominated convergence
theorem, we have
0=lim
r→∞∫Xdx[divx(g)⋅Φr](x)+lim
r→∞∫Xdx[g⋅∇Φr](x)
=∫Xdxlim
r→∞[divx(g)⋅Φr](x)+∫Xdxlim
r→∞[g⋅∇Φr](x)
=∫Xdxdivx(g)(x),
where in the last equality, we use g⋅∇Φr(x)≤∥g(x)∥∥∇Φr(x)∥→0asr→∞.
We now show that the divergence integrals in Lemmas 8 and 9 satisfy the requirements (74), under
the following regularity assumptions on the hypothesis velocity field f, initial distribution ¯ρ0, and
the ground truth solution ¯ρ.
Assumption 3. f∈Lip(X)and there exists some constant L, such that for all t∈[0, T]andx∈X
∥[∇(divf)](t,x)∥≤L.
Assumption 4. The initial distribution ¯ρ0satisfies
∫Xdx¯ρ0(x)(∣log ¯ρ0(x)∣+1)(∥x∥+1)α+1(∥∇log ¯ρ0(x)∥+1)<∞ (78)
29Assumption 5. Suppose that the ground truth ¯ρt∈L∞is sufficiently regular such that
∣log ¯ρt(x)∣+∥∇log ¯ρt(x)∥≤L(1+∥x∥)αand∥¯ft(x)∥+∣div¯ft(x)∣≤L(1+∥x∥)α(79)
holds for all x∈Xandt∈[0, T]with some constant αandL. Here we denote ¯ft=A[ρt].
The following estimations of regularity will be helpful. The proof is deferred to the end of this
section.
Lemma 11. Under Assumption 3, we have the following estimations
∥xt∥2≤exp(t(1+2L2))(∥x0∥2+1)
∣logρf
t(xt)∣≤∣log ¯ρ0(x0)∣+Lt
∥∇logρf
t(xt)∥2≤exp(t(1+2L2))(∥∇log ¯ρ0(x0)∥2+1).
We now show that the integrals of the divergence in Lemmas 12 and 13 are zero.
Lemma 12. Under Assumptions 3 to 5, we have
∫Xdiv(ρt(ftlogρf
t
¯ρt−¯ft))=0. (80)
Proof. To establish Lemma 12, we need to show that all the terms inside the divergence of equa-
tion 80 satisfy the integrability requirements (74) in Lemma 10, which are handled one by one in
the following.
• We handle the term ρf
tlogρf
tft.
–To show that ∫Xdx∥ρf
tlogρf
tft(x)∥<∞
∫Xdx∥[ρf
tlogρf
tft](x)∥=∫Xdxρf
t(x)∥[logρf
tft](x)∥
=∫Xdx¯ρ0(x0)⋅∣logρf
t(xt)∣⋅∥ft(xt)∥
≤∫Xdx¯ρ0(x0)⋅(∣log ¯ρ0(x0)∣+Lt)⋅exp(t(1+2L2))(∥x0∥+1)<∞.
–To show that ∫Xdx∣div(ρf
tlogρf
tft)(x)∣<∞
div(ρf
tlogρf
tft)=divft⋅ρf
tlogρf
t+ft⋅∇(ρf
tlogρf
t)
=divft⋅ρf
t⋅logρf
t+(ft⋅∇ρf
t)⋅logρf
t+(ft⋅∇logρf
t)⋅ρf
t
=ρf
t(divft⋅logρf
t+(ft⋅∇logρf
t)⋅(1+logρf
t))
We now bound
∫Xdxρf
t(x)∣[divft⋅logρf
t](x)∣
=∫Xdx¯ρ0(x0)∣[divft⋅logρf
t](xt)∣≤∫Xdx¯ρ0(x0)⋅L⋅(tL+∣log ¯ρ0(x0)∣)<∞.
and
∫Xdxρf
t(x)∣[(ft⋅∇logρf
t)⋅(1+logρf
t)](x)∣
=∫Xdx¯ρ0(x0)∣[(ft⋅∇logρf
t)⋅(1+logρf
t)](xt)∣
≤∫Xdx¯ρ0(x0)L(1+∥x0∥)exp(t(1+2L2))(∥∇log ¯ρ0(x0)∥+1)(1+Lt+∣log ¯ρ0(x0)∣)<∞.
• We handle the term ρf
tlog ¯ρtft.
30–To show that ∫Xdx∥ρf
tlog ¯ρtft(x)∥<∞
∫Xdxρf
t∥log ¯ρtft(x)∥=∫Xdx¯ρ0∥[log ¯ρtft](xt)∥
≤∫Xdx¯ρ0(x0)∣log ¯ρt(xt)∣∥ft(xt)∥≤∫Xdx¯ρ0(x0)L2(1+∥xt∥)α+1<∞.
–To show that ∫Xdx∣div(ρf
tlog ¯ρtft)(x)∣<∞
div(ρf
tlog ¯ρtft)=divft⋅ρf
tlog ¯ρt+ft⋅∇(ρf
tlogρt)
=divft⋅ρf
t⋅log ¯ρt+(ft⋅∇ρf
t)⋅log ¯ρt+(ft⋅∇logρt)⋅ρf
t
=ρf
t(divft⋅log ¯ρt+(ft⋅∇logρf
t)log ¯ρt+ft⋅∇logρt)
We now bound
∫Xdxρf
t(x)∣divft(x)∣⋅∣log ¯ρt(x)∣=∫Xdx¯ρ0(x0)∣divft(xt)∣⋅∣log ¯ρt(xt)∣
≤∫Xdx¯ρ0(x0)L(1+∥xt∥)(1+∥xt∥)α<∞.
∫Xdxρf
t(x)∣[(ft⋅∇logρf
t)logρt](x)∣=∫Xdx¯ρ0(x0)∣[(ft⋅∇logρf
t)logρt](xt)∣
≤∫Xdx¯ρ0(x0)∥ft(xt)∥∥∇logρf
t(xt)∥∣log ¯ρt(xt)∣
≤∫Xdxexp(t(1+2L2))(∥∇log ¯ρ0(x0)∥+1)L(1+∥xt∥)(1+∥xt∥)α<∞.
∫Xdxρf
t(x)∣[ft⋅∇logρt](x)∣=∫Xdx¯ρ0(x0)∥ft(xt)∥∥∇log ¯ρt(xt)∥
≤∫Xdx¯ρ0(x0)∥ft(xt)∥∥∇log ¯ρt(xt)∥≤∫Xdx¯ρ0(x0)L(1+∥xt∥)(1+∥xt∥)α<∞.
• We handle the term ρf
t¯ft.
–To show that ∫Xdx∥[ρf
t¯ft](x)∥<∞
∫Xdxρf
t(x)∥¯ft(x)∥=∫Xdx¯ρ0(x0)∥¯ft(xt)∥<∞
–To show that ∫Xdx∣div(ρf
t¯ft)(x)∣<∞
∫Xdiv(ρf
t¯ft)=∫X∇ρf
t⋅¯ft+ρf
tdiv¯ft=∫Xρf
t(∇logρf
t⋅¯ft+div¯ft)
=∫Xdx0¯ρ0(x0)(∇logρf
t(xt)⋅¯ft(xt)+div¯ft(xt))<∞,
using the polynomial growth assumption on the ground truth velocity field ¯ft.
We now focus on addressing the integral-of-divergence term in Lemma 9. The following result will
be useful.
Remark 2. LetXf
tbe the flow map generated by the velocity field f∈Lip(Rd). We have that
Xf
t∈Lip(Rd)and that ρf
t=Xf
t♯¯ρ0remains bounded for t∈[0, T]if¯ρ0is bounded on Rd. This
can be established using the change-of-variable formula of the probability density function.
Lemma 13. Under Assumptions 3 to 5, we have
∫Xdxdiv{g∗(ρf
t−¯ρt)(x)(ρf
t(x)ft(x)−¯ρt(x)¯ft(x))}=0. (81)
31Proof. Denote h=g∗(ρt−¯ρt)(x)(ρt(x)ft(x)−¯ρt(x)¯ft(x)). To show that h∈L1(X), we can
show that, after splitting into simple terms, every term from his inL1. In the following, we show
g∗ρt(x)ρt(x)ft(x)∈L1(X).
Other terms can be proved similarly. First, we show that g∗ρt∈L∞(X)ifρt∈L∞(X). For any
constant C, we have
g∗ρt(x)=∫Xg(x−y)ρt(y)dy
=∫∥x−y∥≤Cg(x−y)ρt(y)dy+∫∥x−y∥>Cg(x−y)ρt(y)dy
≤∥ρt∥L∞(X)∫∥x−y∥≤C∥x−y∥2−ddy+C2−d∫∥x−y∥>Cρt(y)dy
≤∥ρt∥L∞(X)C2+C2−d,
where in the last inequality, we use
∫∥x−y∥≤C∥x−y∥2−ddy=∫∥y∥≤C∥y∥2−ddy≤∫0≤r≤Cr2−ddr∫Jθdθ≤∫0≤r≤Cdr r≤C2.
Here Jθdenotes the determinant of the Jacobian obtained from changing to the polar coordinate,
which is bounded by rd−1. We hence obtain
∫Xdx∥g∗ρt(x)ρt(x)ft(x)∥≤C′∫Xdxρt(x)∥ft(x)∥=C′∫Xdx0¯ρ0(x0)∥ft(xt)∥<∞,
where we use ft∈Lip(X)and the estimation in Lemma 11.
Similarly, to show that div(h)∈L1(X), we can show that, after splitting into simple terms, every
term from div(h)is inL1. In the following, we show that
∇g∗ρt(x)ρt(x)ft(x)∈L1(X)andg∗ρt(x)∇ρt(x)⋅ft(x)∈L1(X).
Other terms can be proved similarly.
To show that ∇g∗ρt(x)ρt(x)ft(x)∈L1(X), we first show that ∇g∗ρt(x)∈L∞(X)forρt∈
L∞(X). We can then apply the same argument as above to establish the absolute integrability of
the whole term.
∥∇g∗ρt(x)∥≤∫X∥∇g(x−y)∥ρt(y)dy
=∫∥x−y∥≤C∥∇g(x−y)∥ρt(y)dy+∫∥x−y∥>C∥∇g(x−y)∥ρt(y)dy
≤∥ρt∥L∞(X)∫∥x−y∥≤C∥x−y∥1−ddy+C1−d∫∥x−y∥>Cρt(y)dy
≤∥ρt∥L∞(X)C+C1−d.
To show that g∗ρt(x)∇ρt(x)⋅ft(x)∈L1(X), we use the fact that g∗ρt∈L∞(X)and that
∫Xdx∇ρt(x)⋅ft(x)=∫Xdx0¯ρ0(x0)∇logρt(xt)⋅ft(xt). (82)
Using the estimation in Lemma 11 and that ft∈Lip(X), we obtain the result.
Proof of Lemma 11.
d
dt∥xt∥2≤∥xt∥2+∥¯ft(xt)∥2≤∥xt∥2+2L2(1+∥xt∥2)=(1+2L2)∥xt∥2+2L2. (83)
Using Gr ¨onwall’s inequality, we have
∥xt∥2≤exp(t(1+2L2))(∥x0∥2+2L2/(1+2L2))≤exp(t(1+2L2))(∥x0∥2+1). (84)
We have
d
dtlogρf
t(xt)=−div¯ft(xt) (85)
32We have
d
dt∇logρf
t(xt)=−∇(div¯ft(xt))−(J¯ft(xt))⊺∇logρf
t(xt) (86)
d
dt∥∇logρf
t(xt)∥2≤∥∇logρf
t(xt)∥2+2∥∇(div¯ft(xt))∥2+2∥(J¯ft(xt))⊺∇logρf
t(xt)∥2
(87)
≤∥∇logρf
t(xt)∥2(1+2∥J¯ft(xt)∥2)+2∥∇(div¯ft(xt))∥2(88)
≤∥∇logρf
t(xt)∥2(1+2L2)+2L2(89)
Using Gr ¨onwall’s inequality, we have
∥∇logρf
t(xt)∥2≤exp(t(1+2L2))(∥∇log ¯ρ0(x0)∥2+1). (90)
33