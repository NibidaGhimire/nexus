arXiv:2302.13849v3  [cs.LG]  17 Aug 2023Optimal Prediction Using Expert Advice
and
Randomized Littlestone Dimension
Yuval Filmus1,2, Steve Hanneke3, Idan Mehalel1, and Shay Moran2,1,4
1The Henry and Marilyn Taub Faculty of Computer Science, Technion, Israel
2Faculty of Mathematics, Technion, Israel
3Department of Computer Science, Purdue University, USA
4Google Research, Israel
August 21, 2023
Abstract
A classical result in online learning characterizes the optimal mistake bound achievable
by deterministic learners using the Littlestone dimension (Littleston e ’88). We prove an
analogous result for randomized learners: we show that the optima lexpected mistake bound
in learning a class Hequals its randomized Littlestone dimension , which we deﬁne as follows:
it is the largest dfor which there exists a tree shattered by Hwhoseaveragedepth is 2 d. We
further study optimal mistake bounds in the agnostic case, as a fu nction of the number of
mistakesmade bythe best function in H, denotedby k. Towardsthis end weintroduce the k-
Littlestone dimension and its randomized variant, and use them to ch aracterize the optimal
deterministic and randomized mistake bounds. Quantitatively, we sh ow that the optimal
randomizedmistakebound forlearningaclasswith Littlestone dimens iondisk+Θ(√
kd+d)
(equivalently, the optimal regret is Θ(√
kd+d)). This also implies an optimal deterministic
mistake bound of 2 k+Θ(d)+O(√
kd), thus resolving an open question which was studied
by Auer and Long [’99].
As an application of our theory, we revisit the classical problem of pr ediction using
expert advice: about 30 years ago Cesa-Bianchi, Freund, Haussle r, Helmbold, Schapire and
Warmuthstudied predictionusingexpertadvice, providedthatthe best amongthe nexperts
makes at most kmistakes, and asked what are the optimal mistake bounds (as a fun ction
ofnandk). Cesa-Bianchi, Freund, Helmbold, and Warmuth [’93, ’96] provided a nearly
optimal bound for deterministic learners, and left the randomized c ase as an open problem.
We resolve this question by providing an optimal learning rule in the ran domized case, and
showing that its expected mistake bound equals half of the determin istic bound of Cesa-
Bianchi et al. [’93, ’96], up to negligible additive terms. In contrast with previous works
by Abernethy, Langford, and Warmuth [’06], and by Brˆ anzei and P eres [’19], our result
applies to all pairs n,k, and does so via a uniﬁed analysis using the randomized Littlestone
dimension.
In our proofs we develop and use optimal learning rules, which can be seen as natural
variants of the Standard Optimal Algorithm ( SOA) of Littlestone: a weighted variant in the
agnostic case, and a probabilistic variant in the randomized case. We conclude the paper
with suggested directions for future research and open question s.
1Contents
1 Introduction 3
2 Main results 3
2.1 Realizable Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Agnostic Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.3 Prediction Using Expert Advice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.4 Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3 Technical Overview 10
3.1 Combinatorial Characterizations . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.2 Quasi-balanced Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.3 Prediction using Expert Advice . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4 Background and Basic Deﬁnitions 14
5 Randomized Littlestone Dimension and Optimal Expected Mistake Bound 16
5.1 Proof of Characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6 Quasi-balanced Trees 20
6.1 Deﬁnition and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
6.2 A Concentration Lemma for Quasi-Balanced Trees . . . . . . . . . . . . . . . . . 22
7 Bounded Horizon 23
7.1 Proof of Theorem 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
7.2 Approaching RL(H) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
7.3 Mistake Bound for Few Rounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
8 Mistake Bounds in the k-Realizable Setting 27
8.1 Weighted Hypothesis Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
8.2 Proof of Optimal Deterministic Mistake Bound . . . . . . . . . . . . . . . . . . . 29
8.3 Proof of Optimal Randomized Mistake Bound . . . . . . . . . . . . . . . . . . . . 30
8.4 Explicit Bounds in Terms of Littlestone Dimension . . . . . . . . . . . . . . . . . 30
8.5 Adapting to k. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
9 Prediction using Expert Advice 33
9.1 Optimal Mistake Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
9.2 Proof of the Upper Bound on M⋆(n,k) . . . . . . . . . . . . . . . . . . . . . . . . 35
9.3 Lower bounding M⋆(2,k) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
10 Open Questions 39
21 Introduction
A recurring phenomenon in learning theory is that diﬀerent no tions of learnability are captured
by combinatorial parameters. Notable examples include the Vapnik–Chervonenkis (VC) dimen-
sionwhichcharacterizes PAClearnability [VC74,BEHW89]a ndtheLittlestone dimensionwhich
characterizes online learnability [Lit88,BDPSS09]. Othe r examples include the Daniely–Shalev-
Shwartz and Natarajan dimensions in multiclass PAC learnin g [Nat89,DSS14,BCD+22], the
star number, disagreement coeﬃcient, and inference dimens ion in interactive learning [Han14,
HY15,KLMZ17], the statistical query dimension in learning with statistical queries [Fel17], the
representation dimension, one-way communication complex ity, and Littlestone dimension in
diﬀerentially private learning [FX15,BNS19,ABL+22], and others.
One of the simplest and most appealing characterizations is that of online learnability by the
Littlestone dimension. In his seminal work, Nick Littlesto ne proved that the optimal mistake-
bound in online learning a class Hisexactlythe Littlestone dimension of H[Lit88]. Thus, not
only does the Littlestone dimension qualitatively capture s online learnability, it also provides an
exact quantitative characterization of the best possible m istake bound. This distinguishes the
Littlestone dimension from other dimensions in learning th eory, which typically only provide
asymptotic bounds on the learning complexity.
However, the exact quantitative characterization of the op timal mistake bound by the Lit-
tlestone dimension applies only in the noiseless realizable setting and only for deterministic
learners. In particular, it does not apply in the more genera l and well-studied setting of agnos-
ticonline learning. The reason it does not apply is twofold: (i) because the agnostic setting
allows for non-realizable sequences, and (ii) because rand omized learners are in fact necessary.1
This suggests the following question, which guides this wor k:
Is there a natural dimension which captures the optimal expe cted mistake bound in
learning a class Husing randomized learners? How about the agnostic setting w hen
there is no h∈ Hwhich is consistent with input data?
The main contribution of this work formalizes and proves aﬃr mative answers to these ques-
tions. Some of the technical material is omitted from this ma nuscript and can be found in the
full version which is accessible online in [FHMM22].
Organization. Inthenextsection wepresentthemainresultsofthiswork. T hen, inSection3
we provide a short technical overview, where we outline the m ain ideas we use in our proofs.
The remaining sections contain the complete proofs.
2 Main results
This section assumes familiarity with standard deﬁnitions and terminology from online learning.
We refer the unfamiliar reader to Section 4, which introduce s the online learning model and
related basic deﬁnitions in a self-contained manner.
2.1 Realizable Case
In his seminal work from 1988, Nick Littlestone studied the o ptimal mistake bound in online
learning an hypothesis class Hby deterministic learning rules in the realizable setting [ Lit88];
1Randomized learners are necessary in the following sense: a ny agnostic online learner for a class Hmust be
randomized, provided that Hcontains at least two functions [Cov65], see also [SSBD14, C hapter 21.2].
3that is, under the assumption that the input data sequence is consistent with a function h∈ H.
Littlestone dimension. LetXbe the domain, and let Hbe a class of “ X → {0,1}” predic-
tors. The Littlestone dimension of H, denoted L(H), is the maximal depth of a binary complete
decision tree Twhich is shattered by H. That is, a decision tree Twhose nodes are associated
with points from Xand whose edges are associated with labels from {0,1}such that each of
the branches (root-to-leaf paths) in Tis realized by some h∈ H.
Littlestone proved that the optimal mistake bound achievab le by deterministic learners
equals the Littlestone dimension:
Theorem 2.1 (Deterministic Mistake Bound[Lit88]) .The optimal deterministic mistake bound
in online learning Hin the realizable setting is equal to its Littlestone dimensi on,L(H).
Littlestone further described a natural deterministic lea rning rule, which he dubbed the
Standard Optimal Algorithm (SOA), that makes at most L(H) mistakes on every realizable
input sequence.
Randomized Littlestone dimension. Our ﬁrst main result shows that a natural probabilis-
tic variant of the Littlestone dimension characterizes the optimal expected mistake bound for
randomized learners.
Deﬁnition 2.2 (Randomized Littlestone Dimension) .LetTbebinary tree, and consider
a random walk on Tthat starts at the root, goes to the left or right child with pr oba-
bility 1/2, and continues recursively in the same manner until reachi ng a leaf. Let ET
denote the expected length of a random branch which is produc ed by this process.
Therandomized Littlestone dimension of a class H, denoted by RL(H), is deﬁned by
RL(H) =1
2sup
TshatteredET.
To compare the randomized Littlestone dimension with the Li ttlestone dimension, notice
that the Littlestone dimension is equal to sup {mT:Tshattered }, wheremTis the minimum
length of a branch in T. Thus, the diﬀerence is that in RL(H) we take the expected depth rather
than the minimal depth, and multiply by a factor of 1 /2.2
Theorem 2.3 (Main Result (i): Randomized Mistake Bound) .The optimal randomized
mistake bound in online learning Hin the realizable setting is equal to its randomized
Littlestone dimension, RL(H).
We also provide an optimal randomized learning rule which ca n be seen as a probabilistic
adaptation of Littlestone’s classical SOAalgorithm. See Section 3.1 for a brief overview, and
Section 5.1 for the proof.
The connection between online learning problems and random walks was identiﬁed in the
online learning literature [AWY08,LS14a,GPS16]. [AWY08] asked, conceptually, how general
2From a learning theoretic perspective it is easy to see that RL(H)≤L(H), because randomized learners
are more general than deterministic ones. Interestingly, t his inequality is less obvious from a combinatorial
perspective: indeed, for every ﬁxed tree Twe have that ET≥mT(because the expected length of a branch is
at least the minimal length), but it is not a priori clear why t he inequality is reversed when ETis replaced by
ET/2 and we take supremum over all shattered trees.
4is this connection. Our results show that it is indeed quite g eneral, in the sense that it yields
the optimal algorithm for every hypothesis class.
2.2 Agnostic Case
We next consider the agnostic setting, in which we no longer a ssume that the input sequence
of examples is consistent with H. Our second main result characterizes the optimal expected
mistake bound in this setting.
A common approach for handling the agnostic case is to assume abounded horizon and
analyze the regret. That is, it is assumed that the length of the input sequence ( called the
horizon) is a given parameter T∈N, and the goal is to design learning rules whose mistake
bound is competitive with that of the best h∈ Hup to an additive term which is negligible
inT(this term is called the regretof the algorithm).
The bounded horizon assumption simpliﬁes the design of lear ning rules, by allowing them to
depend on T. A notable example is the celebrated Multiplicative Weights (MW) learning rule,
whose learning rate depends on T. This assumption can then be lifted by standard doubling
tricks.3
Thek-realizable setting. In this work we consider an alternative approach: instead of
assuming a bound Ton the horizon, we assume a bound kon the number of mistakes made by
thebest functionin theclass. Notice that this assumptionc an alsobelifted by suitabledoubling
tricks as we demonstrate inSection 2.4, wherewealso extend ourresults to thebounded-horizon
setting.
The upshot of this approach is that it allows for a precise com binatorial characterization of
the optimal mistake bound via a natural generalization of th e Littlestone dimension.
2.2.1k-Littlestone Dimension
LetHbe an hypothesis class, and let k∈N. A sequence of examples S={(xi,yi)}t
i=1is
k-realizable byHif there exists h∈ Hsuch that h(xi)/n⌉}ationslash=yifor at most kindicesi. In the k-
realizable setting we assume that the input sequence given t o the learner is k-realizable. Notice
that the case k= 0 amounts to realizability by H. We say that a decision tree is k-shattered
byHif every branch is k-realizable by H. The corresponding deterministic and randomized
k-Littlestone dimensions of a class Hare
Lk(H) = sup
T k-shatteredmTandRLk(H) =1
2sup
T k-shatteredET.
Theorem 2.4 (Main Result (ii): k-Littlestone Dimension) .LetHbe an hypothesis class.
1. The optimal deterministic mistake bound in online learnin gHin thek-realizable
setting equals its k-Littlestone dimension, Lk(H).
2. The optimal randomized mistake bound in online learning Hin thek-realizable
setting equals its k-randomized Littlestone dimension, RLk(H).
Wealsoprovideoptimal learningruleswhichcanbeseen aswe ighted variants of Littlestone’s
classical SOAalgorithm. See Section 3.1 for a brief overview and Section 8 for the proof.
3E.g. start by running the algorithm with T= 2, and double Twhen reaching the ( T+1)’st example.
5As a consequence of this perspective, we prove the following theorem which provides tight
regret bounds in terms of the Littlestone dimension.
Theorem 2.5 (Main Result (iii): Optimal Regret Bounds for Littlestone C lasses).Let
Hbe an hypothesis class and let k∈N. Then
RLk(H) =k+Θ/parenleftBig/radicalbig
k·L(H)+L(H)/parenrightBig
.
In particular, the optimal regret in online learning HisΘ/parenleftBig/radicalbig
k·L(H)+L(H)/parenrightBig
, wherek
is the number of mistakes made by the best function in H.
This improves and reﬁnes over results by [AL99,ABED+21]. The work by [ABED+21]
determined an optimal regret bound of Θ/parenleftBig
L(H)+/radicalbig
T·L(H)/parenrightBig
, whereTis the time horizon.
The above bound reﬁnes it by replacing Twithk≤T. The work by [AL99] studies the optimal
deterministic mistake bound in online learning Hin thek-realizable setting. Theorem 2.5 and
the deterministic lower bound of [LW94] imply that the deter ministic mistake bound is
Lk(H) = 2k+Θ(L(H))+O/parenleftBig/radicalbig
k·L(H)/parenrightBig
.
This improves over [AL99, Theorem 4.4], which gives an upper bound of (2 + 2 .5ǫ)k+
O/parenleftbig1
ǫlog1
ǫ/parenrightbig
L(H) for every 0 < ǫ≤1/20. See Section 8.4 for the proof of Theorem 2.5.
2.3 Prediction Using Expert Advice
In this section, we consider the problem of prediction using expert advice [Vov90,LW94]. This
problem studies a repeated guessing game between a learner a nd an adversary. In each round
of the game, the learner needs to guess the label that the adve rsary chooses. In order to do so,
the learner can use the advice of nexperts. Formally, each round iin the game proceeds as
follows:
(i) The experts present predictions ˆ y(1)
i,...,ˆy(n)
i∈ {0,1}.
(ii) The learner predicts a value pi∈[0,1].
(iii) The adversary reveals the true answer yi∈ {0,1}, and the learner suﬀers the loss |yi−pi|.
The value pishould be understood as the probability (over the learner’s randomness) of pre-
dictingyi= 1. Notice that the adversary only gets to see pi, which reﬂects the assumption that
the adversary does not know the learner’s internal randomne ss. Notice also that the suﬀered
loss|yi−pi|exactly captures the probability that the learner makes a mi stake. The above is a
standard way to model randomized learners in online learnin g, see e.g. [Sha12,Haz19,CBL06].
Ifpi∈ {0,1}for alli, then the learner is deterministic , in which case |yi−pi|is the binary
indicator for whether the learner made a mistake.
We focus here on the k-realizable setting, which was suggested by [CBFHW96,CBFH+97]
and further studied by [ALW06,MS10,BP19]. Here, the advers ary must choose the answers so
that at least one of the experts makes at most kmistakes. That is, there must exist an expert
jsuch that yi/n⌉}ationslash= ˆy(j)
ifor at most kmany indices i.
6The goal is to determine the optimal loss of the learner as a fu nction of nandk. Let
M⋆
D(n,k) denote the optimal loss of a deterministic learner and M⋆(n,k) denote the optimal loss
of a (possibly) randomized learner.4
The starting point is the basic fact5that
M⋆
D(n,k)
2≤M⋆(n,k)≤M⋆
D(n,k). (1)
In their seminal work, Cesa-Bianchi et al. [CBFH+97] exhibited a randomized algorithm which
witnesses that in the regime when k≫lognork≪logn, the lower bound in Equation 1 is
tight up to a relative factor of o(1), (See their Theorem 4.4.3).
In a follow up work, [CBFHW96] aimed to ﬁnd optimal determini stic and randomized algo-
rithms. They found a nearly optimal deterministic algorith m called binomial weights , which is
optimal (up to an additive constant) when kis small enough. The main problem they left open
is whether there is a randomized learner with loss exactly ha lf the loss of their binomial weights
algorithm (plus, maybe, a constant). Below we show that thea nswer to this question is negative,
and ﬁnd tight guarantees on the second-order term in M⋆(n,k), in terms of the performances of
their algorithm.
Nearly 10 years later, Abernathy, Langford and Warmuth [ALW 06] showed that M⋆(n,k)≤
M⋆
D(n,k)/2+Cfor every kand every n≥N(k), where Cis a universal constant (independent
ofn,k), thus showing that in the regime when k=O(1) the additive negligible term is indeed
a universal constant (independent of n,k).
More recently, Brˆ anzei and Peres [BP19] showed that M⋆(n,k)≤(1
2+o(1))M⋆
D(n,k) fork=
o(logn), while quantitatively improving upon the bounds given by [ CBFH+97] in this regime.
In the next theorem we provide guarantees on M⋆(n,k) foralln≥2 andk≥0, which are
tight when n= 2, thus fully resolving the question raised by [CBFHW96].6Our lower bound
shows that the second-order term tends to inﬁnity when n= 2. The latter shows that the result
by [ALW06] does not apply for general n,k.
Theorem 2.6 (Main Result (iv): Bounds for Randomized Predictors) .LetM⋆(n,k)
denote the optimal expected mistake bound for prediction us ing expert advice in the k-
realizable setting when there are nexperts, and let D(n,k)denote the mistake bound of
the binomial weights algorithm. For all n≥2andk≥0,
M⋆(n,k) =D(n,k)
2+O/parenleftBig/radicalbig
D(n,k)/parenrightBig
.
Furthermore, the error term cannot be improved for n= 2:
M⋆(2,k) =D(2,k)
2+Ω/parenleftBig/radicalbig
D(2,k)/parenrightBig
.
We prove the upper bound in Section 9.2, and the lower bound in Section 9.3. Both bounds
4Note that we assume here that kis known to the learner and that the horizon (i.e. number of ro unds in the
game) might be unbounded. In Section 2.4.2 below we explain h ow to extend our results to the complementing
cases.
5One might be tempted to interpret these inequalities as impl ying that M⋆(n,k) andM⋆
D(n,k) are nearly the
same. However, the multiplicative gap of 1 /2 can be signiﬁcant. For example, a randomized learner with a non-
trivial error rate of 25% corresponds to adeterministic lea rner with 50% error-rate. The latter is trivially achieved
by a random guess. For the same reason, sublinear regret guar antees can only be achieved by randomized learners,
although they are “just” a factor of 1 /2 better than deterministic learners, see e.g. [CBL06,Sha1 2,Haz19].
6Whenn= 1,M⋆(1,k) =M⋆
D(1,k) =k.
7are proved using the randomized k-Littlestone dimension. A special case of Theorem 2.5 state s
thatM⋆(n,k) =k+Θ/parenleftbig√klogn+logn/parenrightbig
, where the upper bound in this quantitative bound was
ﬁrst proved in [CBFH+97].
Using Theorem 2.6 and the bounds of [CBFHW96], we obtain the f ollowing corollary.
Corollary 2.7. For alln≥2andk≥0,
M⋆(n,k) =/parenleftbigg1
2+o(1)/parenrightbigg
M⋆
D(n,k).
This also follows from the results of [Vov90] for randomized learners.7
Additional Related Work. Diﬀerent variants of the experts problem have been extensive ly
studied in the past 30 years and various techniques for bound ing the optimal regret and mistake
boundsweredeveloped throughout theyears, suchas sequential Rademacher complexity [RSS12,
RS14],drifting games [MS10,LS14b], and the Hedge setting [AWY08,FS97]. However, those
techniquesareseemingly tailoredforrandomized properlearners(i.e., learnersthatpredictusing
a distribution over the experts which is updated at the end of each round), and proper learners
are inherently suboptimal for the experts problem, even in t he realizable case, as proven in the
full version of this paper [FHMM22]. [AWY08] identiﬁed the o ptimal proper algorithm, using a
random walk analysis, which is similar to our characterizat ion results. It will be interesting to
investigate whether variations of these techniques can rep roduce or even improve the bounds in
this work.
2.4 Variations
2.4.1 Bounded Horizon
Consider learning Hin thek-realizable setting, and let M⋆
k=M⋆
k(H) denote the optimal expected
mistakebound. Inparticular, thismeansthattheadversary canforce M⋆
kmistakes inexpectation
on any randomized learner. This would be tolerable if in orde r to do so the adversary must use
many examples, say 1000 M⋆
k. Indeed, this would mean that the learner makes only one mist ake
per a thousand examples (amortized), which is rather good.
This raises the question to what extent does M⋆
kcapture the optimal mistake bound under
the additional assumption that the horizon is bounded by a gi venT∈N. A bounded horizon
is often assumed in the online learning literature, and in fa ct this question was explicitly asked
by [CBFH+97] in the special case of prediction using expert advice.
LetM⋆
k(T)denotetheoptimalexpectedmistakeboundinthe k-realizablesettingwithhorizon
boundedby T. Thefollowing resultshows that M⋆
kprovides an excellent approximation of M⋆
k(T);
in particular, the scenario described above is impossible.
7In the COLT 2023 proceedings version of this paper, this coro llary was unintentionally presented as brand
new.
8Theorem 2.8 (Main Result (v): Bounded vs Unbounded Horizon) .LetHbe an hypoth-
esis class. Let M⋆
kdenote the optimal expected mistake bound in online learnin gHin the
k-realizable setting, and let M⋆
k(T)denote the optimal expected mistake bound under the
additional assumption that the input sequence has length at mostT. Then,
1.Long horizon. IfT>2M⋆
kthen
M⋆
k−/radicalBig
8M⋆
klnM⋆
k−1≤M⋆
k(T)≤M⋆
k.
2.Short Horizon. IfT≤2M⋆
kthen
T
2−√
8TlnT−1≤M⋆
k(T)≤T
2,
and ifT≤M⋆
kthenM⋆
k(T) =T
2.
The upper bounds in Theorem 2.8 follow from basic facts: inde ed,M⋆
k(T)≤M⋆
kholds because
assuming a bounded horizon restricts the adversary, and M⋆
k(T)≤T
2follows by guessing each
label uniformly at random. The lower bounds are more challen ging, and our proofs of them
relies heavily on the randomized Littlestone dimension.
Our proof of Theorem 2.8 appears in Section 7.3. The proof rel ies on a simple extension
of our characterization to this setting: consider the follo wing modiﬁcation of the Littlestone
dimension and its randomized variant:
Lk(H,T) = sup
Tshattered
depth(T)≤TmTandRLk(H,T) =1
2sup
Tshattered
depth(T)≤TET.
The bounded randomized Littlestone dimension gives the pre cise mistake bound in this setting:
Theorem 2.9 (Optimal Mistake Bounds: Bounded Horizon) .LetHbe an hypothesis class.
1. The optimal deterministic mistake bound in online learnin gHin thek-realizable setting
with horizon Tequals its bounded k-Littlestone dimension, Lk(H,T).8
2. The optimal randomized mistake bound in online learning Hin thek-realizable setting with
horizonTequals its bounded k-randomized Littlestone dimension, RLk(H,T).
We prove Theorem 2.9 in Section 7.1.
Prediction using Expert Advice. Also the problem of prediction using expert advice is
often considered when the number of rounds is bounded (e.g. [ CBFH+97]). Let M⋆(n,k,T) be
the optimal loss of the learner when the number of rounds is T. By a simple reduction to
Theorem 2.9 we show that
M⋆(n,k,T)≈/braceleftBigg
M⋆(n,k) ifT≥2M⋆(n,k),
T
2ifT<2M⋆(n,k).
The exact bounds are as in Theorem 2.9 when replacing M⋆(n,k,T) andM⋆(n,k) withM⋆
k(T)
andM⋆
k.
8Trivially, Lk(H,T) = min{T,Lk(H)}.
92.4.2 Adaptive Algorithms
The analysis in much of this work considers the case where the learning algorithm may depend
explicitly on a bound kon the number of mistakes of the best hypothesis (or expert). However,
it is also desirable to study mistake bounds achievable adaptively : that is, by a single algorithm
that applies to all k. We present here one simple approach to obtaining such an alg orithm, with
a corresponding mistake bound. However, the bound we obtain may likely be improvable, and
generally we leave the question of obtaining a tightest poss ible adaptively-achievable mistake
bound as an open problem.
Theorem 2.10. There is an adaptive algorithm (i.e., which has no knowledge o fk⋆) such that,
for every k⋆-realizable sequence for H, its expected number of mistakes is at most
M⋆
k⋆+O/parenleftbigg/radicalBig
M⋆
k⋆log/parenleftbig
(k⋆+1)logM⋆
k⋆/parenrightbig/parenrightbigg
.
In the special case of the general expertssetting, since we know that M⋆(n,k⋆) = Ω(k⋆+
log(n)), we obtain the following bound on the expected number of mi stakes:
M⋆(n,k⋆)+O/parenleftBig/radicalbig
M⋆(n,k⋆)logM⋆(n,k⋆)/parenrightBig
= (1+o(1))M⋆(n,k⋆).
In particular, combining this with Theorem 2.6, we ﬁnd that t his algorithm adaptively still
achieves an expected number of mistakes/parenleftbig1
2+o(1)/parenrightbig
M⋆
D(n,k⋆).
On the other hand, in the case of concept classes Hwith a bounded Littlestone dimen-
sionL(H), we know from Theorem 2.5 that
M⋆
k⋆≤k⋆+O/parenleftBig/radicalbig
k⋆L(H)+L(H)/parenrightBig
.
Theorem2.10 implies that theadaptive procedurenearly pre servestheform of thisupperbound,
guaranteeing a slightly larger bound of the form
k⋆+O/parenleftBig/radicalbig
k⋆L(H)log(k⋆logL(H))+L(H)/parenrightBig
.
Our proof of Theorem 2.10 appears in Section 8.5. The adaptiv e technique we propose
involves using an experts algorithm of [KvE15] named Squint, with experts deﬁned by the
optimal randomized algorithm for the k-realizable setting, for all values of k.
3 Technical Overview
In its greatest generality, online prediction is a game invo lving two randomized parties, an
adversary who is producing examples, and a learner who is try ing to correctly predict the labels
of all or most of these examples. In the realizable case, the a dversary is moreover constrained
by an hypothesis class which must be adhered to.
Various techniques are used in the literature to analyze thi s sophisticated setting. On the
one hand, learning rules show which hypothesis classes lend themselves to learning, and on the
other hand, strategies for the adversary put limitations on what can be learned, and at what
cost.
In this work, we identify the combinatorial core behind many settings of online learning. In
this, we follow up on Nick Littlestone’s classical work on de terministic online learning, as well
as on other classical work in learning theory such as that the foundational work of Vapnik and
Chervonenkis.
Reducingthe messy probabilistic setting of onlinelearnin g to the clean combinatorial setting
ofshatteredtreesenablesustotackleopenquestionsabout predictionusingexpertadvice, which
are hard to approach directly.
103.1 Combinatorial Characterizations
The Littlestone dimension of an hypothesis class His the maximal depth of a complete binary
treewhichisshatteredby H. A treeof depth Deasily translates intoastrategy fortheadversary
which forces thelearner to make Dmistakes. In other words, a tree shattered by His an obvious
obstacle to learning H.
The magic of Littlestone dimension is the opposite directio n: Littlestone’s SOAlearning
rule makes at most L(H) mistakes, showing that trees shattered by Hare theonlyobstacle for
learning H. This is a common phenomenon in mathematics: an obvious nece ssary condition
turns out to be (less obviously) suﬃcient.
Deﬁning the randomized Littlestone dimension. In order to motivate the deﬁnition
of the randomized Littlestone dimension, let us ﬁrst examin e the (deterministic) Littlestone
dimension. Given a tree Tshattered by H, the adversary executes the following strategy,
starting at the root:
At an internal node labeled x, ask the learner for the label of x, and follow the opposite edge.
This strategy follows a branch of T, and forces the learner to make a mistake in each round.
The total number of mistakes which the adversary can guarant ee is precisely mT, the minimum
length of a branch in T. The resulting input sequence is realizable by HsinceTis shattered
byH.
The deﬁnition of the randomized Littlestone dimension foll ows a similar approach, but uses
a diﬀerent strategy for the adversary:
At an internal node labeled x, ask the learner for the label of x, and follow a randomedge.
This strategy also follows a branch of T, and it forces the learner to make halfa mistake in
each round, in expectation.9The total expected number of mistakes is ET/2, where ETis the
expected length of a random branch of T.
We deﬁnethe randomized Littlestone dimension by consideri ng all such adversary strategies:
RL(H) =1
2sup
TshatteredET.
Extending the Standard Optimal Algorithm. Littlestone’s StandardOptimal Algorithm
(SOA) makes at most L(H) mistakes on any realizable input sequence. The algorithm i s very
simple. It maintains a subset VofHwhich consists of all hypotheses which are consistent with
the data seen so far. Given a sample x, one of the following must hold, where Vx→yis the subset
ofVconsisting of all hypotheses assigning to xthe label y:
1.L(Vx→0)<L(V). The learner predicts ˆ y= 1.
2.L(Vx→1)<L(V). The learner predicts ˆ y= 0.
One of these cases must hold, since otherwise we could constr uct a tree of depth L(V) + 1
shattered by V. Each time that the learner makes a mistake, L(V) decreases by 1, and so the
learner makes at most L(H) mistakes.
9Recall that we model a randomized learner as a learner which m akes a “soft” prediction p∈[0,1]; if the
true label is y, then the learner’s loss is |p−y|. When we choose the label yat random, the expected loss is
E[|p−y|] =1
2regardless of p.
11Our randomized extension of SOA, which we call RandSOA , follows a very similar strategy.
It maintains Vin the same way. Given a sample x, we want to make a prediction pwhich
“covers all bases”, that is, results in a good outcome for the learner whatever the correct label
yis. Given a prediction p, the adversary can guarantee a loss of
max{p+RL(Vx→0),1−p+RL(Vx→1)}.
For the optimal choice of p, this quantity is at most RL(V), as we show in Section 5.1.
Thek-realizable setting and weighted SOA.Thek-realizable setting is handled simi-
larly. In the deﬁnition of randomized Littlestone dimensio n, instead of requiring the tree to be
shattered, it suﬃces for it to be k-shattered, since the adversary need only produce an input
sequence which is k-realizable.
Themain novelty in this setting is a weighted analog of the SOAlearningrule. Thisweighted
SOArule relates to the classical SOAin a similar way like the Weighted Majority algorithm
relates to Halving. In particular, it keeps track, for each hypothesis, how man y more mistakes
areallowed. Accordingly, weconsiderthemoregeneralized setting of weighted hypothesis classes .
These are hypothesis classes in which each hypothesis has a “ mistake budget”. The deﬁnition of
randomized Littlestone dimension extends to this setting, and allows us to generalize RandSOA
to the randomized agnostic setting.
3.2 Quasi-balanced Trees
Given an hypothesis class H, how does an optimal strategy for the adversary look like? Su ch a
strategy must make the analysis of RandSOA tight, and in particular, if the ﬁrst sample it asks
isx, then
RL(H) =p+RL(Hx→0) = 1−p+RL(Hx→1),
wherepis the prediction of the learner.10
The strategy of the adversary naturally corresponds to a tre e which is shattered by H: the
root is labeled x, and the edge labeled yleads to a tree corresponding to an optimal strategy
forHx→y. Suppose that we further assign weights to the edges touchin g the root: the 0-edge
gets the weight p, and the 1-edge gets the weight 1 −p. If we assign weights to the remaining
edges recursively then the resulting tree satisﬁes the foll owing property:
Every branch has the same total weight RL(H).
More generally, a tree Tisquasi-balanced if we can assign non-negative weight to its edges
such that (i) theweights of thetwo edges emanating froma ver tex sumto1, and(ii) all branches
have the same total weight (which must be ET/2). If a tree is quasi-balanced then the weight
assignment turns out to be unique.
A tree in which all branches have the same depth is quasi-bala nced, but the class of quasi-
balanced trees is a lot richer, including for example the pat h appearing in Figure 1.
There is a simple criterion for quasi-balancedness:
A treeTis quasi-balanced if and only if it is monotone : ifwis a descendant of vthen
ETw≤ETv, whereTuis the subtree rooted at u.
10Strictly optimal strategies do not always exist, and even wh en they do, they might require an unbounded
number of rounds. For the sake of exposition we gloss over the se diﬃculties.
121
8
1
4
1
21
23
47
8
Figure 1: A quasi-balanced tree. The edges are labeled with t he unique weights. The sum of
weights in each branch is7
8, which is half the expected branch length7
4.
Since the loss guaranteed by an adversary following the stra tegy corresponding to a tree T
isET/2, it is clear that the best strategy is always monotone. This argument shows that
RL(H) =1
2sup
Tshattered, monotoneET.
In other words, it suﬃces to consider only quasi-balanced tr ees when deﬁning the randomized
Littlestone dimension. Thisistherandomizedcounterpart ofatrivial propertyoftheLittlestone
dimension: in order to deﬁne the Littlestone dimension, it s uﬃces to consider balanced trees,
that is, trees in which all branches have the same length. We c an view quasi-balancedness as a
relaxation of strict balancedness.
Concentration of expected branch length. The randomized Littlestone dimension is de-
ﬁned in terms of the expected branch length. However, severa l of our results require knowledge
of the distribution of the branch length.
For example, Theorem 2.8 states that 2 RL(H)+O(/radicalbig
RL(H)log(RL(H)/ǫ)) rounds are needed
in order for the adversary to guarantee a loss of RL(H)−ǫ. The number of rounds corresponds
to the depth of the tree, and so the natural way to prove such a r esult would be to start with
a treeTsatisfying ET/2 =RL(H), and prune it to depth 2 RL(H) +O(/radicalbig
RL(H)log(RL(H)/ǫ)).
We would like to say that this does not reduce the expected bra nch length by much, since the
length of most branches does not exceed ETby much. Other applications such as prediction
using expert advice need concentration from the other side ( the length of most branches does
not fall behind ETby much).
It is possible to construct trees for which the length of a ran dom branch isn’t concentrated
around its expectation. For example, we can take an inﬁnite p ath which, every so often, splits
into a deep complete binary tree. If we are careful, we can gua rantee that the expected branch
length is ﬁnite but its variance is inﬁnite.
At this point, quasi-balancedness comes to the rescue. The m onotonicity property of quasi-
balanced trees implies that the choice of an edge at every ste p of a random branch does not
aﬀect the ﬁnal length by much. Consequently, Azuma’s inequal ity (a version of Chernoﬀ’s
inequality for martingales) shows that for quasi-balanced trees, the length of a random branch
is strongly concentrated around its expectation. This simp le observation drives several of our
strongest results.
3.3 Prediction using Expert Advice
At ﬁrst, the setting of prediction using expert advice looks similar, but not identical, to our
setting. However, it turns out that it is actually a special case of our setting, for a speciﬁc
hypothesis class known as the universal hypothesis class Un.
13The class Uncontains ndiﬀerent hypotheses, which correspond to the experts. For ea ch
possible set of predictions ˆ y(1),...,ˆy(n)there is a corresponding element in the domain. In
other words, the domain is X={0,1}n, and the hypotheses in Unare thenprojections
hi(x1,...,x n) =xi.
With this equivalence in place, we can apply the theory we hav e developed so far to analyze
prediction using expert advice. Our main result concerning this setting, Theorem 2.6, consists
of an upper bound on M⋆(n,k), and a lower bound on M⋆(2,k).
We start with the upper bound on M⋆(n,k). In view of the equivalence above, we want to
bound the expected branch length of any tree Twhich is k-shattered by Un. We can assume
thatTis quasi-balanced, and so the length of a random branch of Tis roughly ET. IfTwere
strictly balanced, then a random branch would be k-realizable by Unwith probability at most
n/parenleftbigET
≤k/parenrightbig
2ET.
[CBFHW96] have shown that the largest value of ETfor which this quantity is at least 1, which
we denote by D(n,k), provides the state-of-the-art upper bound on M⋆
D(n,k). Since Tis only
quasi-balanced, we get a slightly worse bound.
A nice proof of the lower bound on M⋆(2,k) is given by identifying the optimal tree. In-
tuitively, it seems obvious that rounds in which both expert s make the same prediction are
“wasteful”, and we can show this formally. By symmetry, we ca n assume that the ﬁrst ex-
pert always predicts 0 and that the second expert always pred icts 1. We can construct the
corresponding tree explicitly, and conclude that
M⋆(2,k) =k+(k+1/2)/parenleftbig2k
k/parenrightbig
4k.
The proof of this result can be found in the full version [FHMM 22] of this paper.
4 Background and Basic Deﬁnitions
Unless stated otherwise, our logarithms are base 2.
Online Learning. LetXbe a set called the domain, andYbe a set called the label set. In
this work we focus on binary classiﬁcation , and thus Y={0,1}. A pair ( x,y)∈ X ×Y is called
anexample, and an element x∈ Xis called an instance or anunlabeled example . A function
h:X → Yis called a hypothesis or aconcept. Ahypothesis class , or aconcept class , is a set
H ⊂ YX. A sequence of examples S={(xi,yi)}t
i=1is said to be realizable byHif there exists
h∈ Hsuch that h(xi) =yifor all 1≤i≤t.
Online learning [SSBD14,CBFH+97] is a repeated game between a learner and an adversary.
Each round iin the game proceeds as follows:
(i) The adversary sends the learner an unlabeled example xi∈ X.
(ii) The learner predicts a value pi∈[0,1] and reveals it to the adversary.
(iii) The adversary reveals the true label yi, and the learner suﬀers the loss|yi−pi|.
The value pishould be understood as the probability (over the learner’s randomness) of pre-
dictingyi= 1. Notice that the adversary only gets to see pi, which reﬂects the assumption that
the adversary does not know the learner’s internal randomne ss. Notice also that the suﬀered
14loss|yi−pi|exactly captures the probability that the learner makes a mi stake. The above is a
standard way to model randomized learners in online learnin g, see e.g. [Sha12]. If pi∈ {0,1}for
alli, then the learner is deterministic , in which case |yi−pi|is the binary indicator for whether
the learner made a mistake.
We model learners as functions Lrn: (X ×Y)⋆×X →[0,1]. Given a learning rule Lrnand
an input sequence of examples S= (x1,y1),...,(xt,yt), we denote the (expected) number of
mistakes Lrnmakes on Sby
M(Lrn;S) =t/summationdisplay
i=1|yi−pi|,
wherepi=Lrn((x1,y1),...,(xi−1,yi−1),xi) is the prediction of the learner on the i’th example.
An hypothesis class Hisonline learnable (orlearnable ) if there exists a ﬁnite bound Mand
a learning rule Lrnsuch that for any input sequence Swhich is realizable by Hit holds that
M(Lrn;S)≤M. We deﬁne the optimalrandomized mistake bound of Hto be
M⋆(H) = inf
Lrnsup
SM(Lrn;S) (2)
where the inﬁmum is taken over all learning rules, and the sup remum is taken over all realizable
input sequences S.
We denote by M⋆
D(H) the optimal deterministic mistake bound of H. That is, M⋆
D(H) is
deﬁned in the same way as M⋆(H), with the additional restriction that Lrnmust be deterministic
(that is, the output must be in {0,1}).
WhenH=∅, the set of realizable input sequences is empty, and therefo re the supre-
mum is not deﬁned. It is technically convenient to deal with t his special case by deﬁning
M⋆
D(∅) =M⋆(∅) =−1. When the context is clear, we may sometimes refer to the det erministic
or randomized mistake bound as the accumulating loss of the learner through the entire game,
or simply as the learner’s lossthrough the entire game.
Decision Trees and the Littlestone Dimension. In this paper, a treeTrefers to a ﬁnite
full rooted ordered binary tree (that is, a rooted binary tre e where each node which is not a
leaf has a left child and a right child), equipped with the fol lowing information:
1. Each internal node vis associated with an instance x∈ X.
2. For every internal node v, the left outgoing edge is associated with the label 0, and th e
right outgoing edge is associated with the label 1.
We stress that by default, the trees we consider are ﬁnite and their vertices are labeled.
Whenever we consider inﬁnite trees or unlabeled trees, we sp eciﬁcally mention these attributes.
The tree is directed from the root towards the leaves.
Apreﬁxof the tree Tis any path that starts at the root. In this paper, a path is deﬁ ned by
a sequence of consecutive vertices. If a path is not empty, we may refer it by the sequence of
consecutive edges corresponding with the sequence of conse cutive vertices deﬁning it. A preﬁx
v0,v1,...,vtdeﬁnes a sequence of examples ( x1,y1),...,(xt,yt) in a natural way: for every
i∈[t],xiis the instance corresponding to the node vi−1, andyiis the label corresponding to
the edge vi−1→vi. A preﬁx is called maximal if it is maximal with respect to containment,
that is, there is no preﬁx in the tree that strictly contains i t. This is equivalent to requiring
thatvtbe a leaf. A maximal preﬁx is called a branch, and the set of branches of Tis denoted
byB(T). The length of a preﬁx is the number of edges in it (so, the len gth is equal to the size
of the corresponding sequence of examples).
15A preﬁx in the tree is said to be realizable byHif the corresponding sequence of examples
is realizable by H. A tree Tisshattered byHif all branches in Tare realizable by H. The
Littlestone dimension of an hypothesis class H, denoted by L=L(H), is the maximal depth of
acomplete (also known as perfect, orbalanced) binary tree (that is, a tree in which all branches
have the same depth) shattered by HifH /n⌉}ationslash=∅, and−1 whenH=∅. If the maximum does not
exist, then L=∞.
Littlestone Dimension ≡Optimal Deterministic Mistake Bound. In his seminal work
from 1988, Nick Littlestone proved that the optimal mistake bound of a deterministic learner
is characterized by the Littlestone dimension:
Theorem 4.1 (Optimal Deterministic Mistake Bound [Lit88]) .LetHbe an hypothesis class.
Then,His online learnable if and only if L(H)<∞. Further, the optimal deterministic mistake
bound satisﬁes M⋆
D(H) =L(H).
Doob’s Exposure Martingales. Letf:{0,1}N→R. Consider the random variable X=
f(/vectorb), where /vectorbis sampled uniformly at random. Deﬁne a sequence L0,L1,L2,..., each deﬁned
byLi=E[X|b1,...,bi−1] (soL0=E[X]). The sequence L0,L1,L2,...is called an exposure
martingale . It is well-known that an exposure martingale is indeed a mar tingale [Doo53].
5 Randomized Littlestone Dimension and Optimal Expected
Mistake Bound
In this section we study the randomized Littlestone dimensi on. We shall deﬁne the randomized
Littlestone dimension and prove that it characterizes the o ptimal randomized mistake bound
exactly.
The randomized Littlestone dimension is deﬁned using trees , which correspond to strategies
of the adversary. We study a special class of trees, quasi-balanced trees , in Section 6.1. Such
trees deﬁne optimal strategies for the adversary (more deta ils on such strategies are found in
the full version of this paper [FHMM22]). Several other appl ications of quasi-balanced trees are
presented in the full version of this paper [FHMM22]; more ap plications are found throughout
the paper.
Theﬁrstmain contribution of this paperis acharacterizati on of theoptimal randomized mis-
take bound in terms of a combinatorial parameter we call the randomized Littlestone dimension
and denote by RL=RL(H).
We deﬁne RL(H) using a natural distribution on the branches of trees (a bra nch is a root-
to-leaf path). Given a tree T, arandom branch is chosen by starting at the root, and at each
step, picking an edge leaving the current vertex uniformly a t random, until reaching a leaf. We
denote the expected length of a random branch by ET. It is given explicitly by the formula
ET=/summationdisplay
b∈B(T)|b|·2−|b|,
whereB(T) is the set of branches of T. If we think of a random branch as a distribution over
B(T), thenETis its entropy.
It is convenient to deﬁne the length of the empty branch to be −1. With this convention,
the expected branch length in Tsatisﬁes the recursion
ET= 1+ET0+ET1
2, (3)
whereT0,T1are the subtrees of the root of T, which are empty when Tis a leaf.
16Deﬁnition 5.1 (Randomized Littlestone Dimension) .LetHbe an hypothesis class. The
randomized Littlestone dimension ofH, denoted by RL(H), is deﬁned by
RL(H) =1
2sup
TshatteredET.
In the special case when H=∅, deﬁneRL(H) =−1.
To compare RL(H) withL(H), let us consider the following equivalent way of deﬁning L(H):
L(H) = sup
TshatteredmT,
wheremTis the minimum length of a branch in T. Thus, the diﬀerence is that in RL(H) we
take the expected depth rather than the minimal depth, and mu ltiply by a factor of 1 /2.
Theorem 5.2 (Optimal Randomized Mistake Bound) .LetHbe an hypothesis class. Then,
M⋆(H) =RL(H).
We prove the theorem in Subsection 5.1 using randomized SOA, a randomized adaptation of
Littlestone’s classical SOAalgorithm. This shows that the inﬁmum in Equation (2) is real ized
by a minimizer.
5.1 Proof of Characterization
The case H=∅holds by deﬁnition. Therefore we assume that H /n⌉}ationslash=∅. The lower bound
“RL(H)≤M⋆(H)” boils down to the following lemma:
Lemma 5.3. LetHbe an hypothesis class, and let Tbe a ﬁnite tree which is shattered by H.
Then, for every learning rule Lrnthere exists a realizable sequence Sso thatM(Lrn;S)≥ET/2.
Moreover, there exists such a sequence Swhich corresponds to one of the branches of T.
Proof.The proof is given by a simple probabilistic argument. Suppo se that we pick a random
branch in the tree according to the random branch distributi on: begin at the root, pick a
random child of the root uniformly at random, and recursivel y pick a random branch in the
corresponding subtree. Consider the random variable
LT=M(Lrn;S),
whereSis the sequence of examples corresponding to a random branch drawn as above. It
suﬃces to show that E[LT] =ET/2. We prove this by induction on the depth of T.
In the base case, Tis a single leaf, and there are no internal nodes. Hence Sis always the
empty sequence, and E[LT] = 0 =ET/2, as required.
For the induction step, let T0andT1be the left and right subtrees of T, respectively. The
expected loss of Lrnon the ﬁrst example in Sis 1/2, because the label y∈ {0,1}is chosen
uniformly at random, independently of the learner’s predic tion (formally,|0−p|+|1−p|
2= 1/2 for
allp∈[0,1]). Therefore, by linearity of expectation,
E[LT] =1+E[LT0]+E[LT0]
2
=1+ET0/2+ET1/2
2(by the induction hypothesis)
=ET/2, (by Eq. (3))
as required.
17By applying Lemma 5.3 on every shattered tree and taking the s upremum, we conclude the
lower bound:
Corollary 5.4 (Lower bound) .For every hypothesis class Hit holds that M⋆(H)≥RL(H).
We now turn to prove the upperbound “ RL(H)≥M⋆(H)”. This is achieved via the RandSOA
learning rule, described in Figure 2.
We begin with the following useful property of RL:
Observation 5.5. LetHbe a non-empty hypothesis class. Then,
RL(H) =1
2sup
x∈X/parenleftbig
1+RL(Hx→0)+RL(Hx→1)/parenrightbig
.
Proof.Observation 5.5 follows from Equation (3): let S(H) denote the set of trees that are
shattered by H, and for x∈ X, letSx(H)⊆ S(H) denote the set of trees that are shattered by
Hwhose root is labeled by x. Then,
RL(H) =1
2sup
T∈S(H)ET=1
2sup
xsup
T∈Sx(H)ET.
By Equation (3),
sup
T∈Sx(H)ET= 1+supT1∈S(Hx→1)ET1+supT0∈S(Hx→0)ET0
2= 1+RL(Hx→1)+RL(Hx→0),
which ﬁnishes the proof.
Notice that the classical Littlestone dimension satisﬁes a similar recursion:
L(H) = sup
x∈X/parenleftbig
1+min{L(Hx→1),L(Hx→0)}/parenrightbig
.
The following lemma is the crux of the analysis: it guides the choice of the prediction piin each
round.
Lemma 5.6 (Optimal prediction for each round) .LetHbe an hypothesis class, and let x∈ X.
Then there exists p∈[0,1]so that
p+RL(Hx→0)≤RL(H)and(1−p)+RL(Hx→1)≤RL(H).
Proof of Lemma 5.6. IfRL(H) =∞then the lemma is trivial. Therefore we assume that
RL(H)<∞. Assume ﬁrst that |RL(Hx→0)−RL(Hx→1)|>1. IfRL(Hx→0) + 1<RL(Hx→1),
then by choosing p= 1 and applying the fact that RL(H′)≤RL(H) ifH′⊆ Hwe get
p+RL(Hx→0) = 1+RL(Hx→0)<RL(Hx→1)≤RL(H),
1−p+RL(Hx→1) =RL(Hx→1)≤RL(H),
as desired. The case RL(Hx→1)+1<RL(Hx→0) is treated similarly.
It remains to handle the case when |RL(Hx→0)−RL(Hx→1)| ≤1. Set
p:=1+RL(Hx→1)−RL(Hx→0)
2.
By assumption, p∈[0,1], and also
p+RL(Hx→0) = 1−p+RL(Hx→1)
18RandSOA :Randomized SOA
Input:An hypothesis class H.
Initialize: LetV(1)=H.
Fori= 1,2,...
1. Receive xi.
2. Predict pi∈[0,1] such that the value
max/braceleftBig
pi+RL/parenleftBig
V(i)
xi→0/parenrightBig
,1−pi+RL/parenleftBig
V(i)
xi→1/parenrightBig/bracerightBig
(4)
is minimized, where V(i)
xi→b={h∈V(i):h(xi) =b}.
3. Receive true label yi.
4. Update V(i+1)=V(i)
xi→yi.
Figure2: Therandomized SOAisavariationof SOAthatﬁndsanoptimalrandomizedprediction
in every round. SOAis the name of the original deterministic algorithm by Littl estone [Lit88],
and it stands for “Standard Optimal Algorithm”.
=1+RL(Hx→0)+RL(Hx→1)
2
≤RL(H). (Observation 5.5)
Lemma 5.7 (Upperbound) .LetHbe an hypothesis class. Then the RandSOA learner described
in Figure 2 has expected mistake bound
M(RandSOA ;S)≤RL(H)
for every realizable input sequence S.
Proof.Theproofisbyinductiononthelengthoftheinputsequence. LetS= (x1,y1),...,(xt,yt)
be a realizable sequence. In the base case t= 0 we have M(RandSOA ;S) = 0≤RL(H). For
the induction step, assume that t≥1, and let S′= (x2,y2),...,(xt,yt) be the input sequence
without the ﬁrst example. In the ﬁrst round, the learner pred ictsp1∈[0,1] as deﬁned in step 2
ofRandSOA . Thus, the learner’s expected accumulated loss on Sis
M(RandSOA ;S) =|p1−y1|+M(RandSOA ;S′). (5)
By the induction hypothesis we have
M(RandSOA ;S′)≤RL(Hx1→y1). (6)
Also, by Lemma 5.6 it holds that p1+RL(Hx1→0)≤RL(H) and 1−p1+RL(Hx1→1)≤RL(H),
which is equivalent to
|p1−y1|+RL(Hx1→y1)≤RL(H). (7)
19Therefore, overall we get that
M(RandSOA ;S) =|p1−y1|+M(RandSOA ;S′) (Eq. (5))
≤ |p1−y1|+RL(Hx1→y1) (Eq. (6))
≤RL(H), (Eq. (7))
as required.
6 Quasi-balanced Trees
6.1 Deﬁnition and Basic Properties
The classical deﬁnition of the Littlestone dimension of a cl assHis the maximum depth of a
balanced (or complete) shattered tree. In contrast, the ran domized Littlestone dimension is
deﬁned via quantifying over allshattered trees. Further, in the deterministic case, balan ced
treesnaturallydescribeoptimaldeterministicstrategie s fortheadversarywhichforceanylearner
to make a mistake on every example along a branch of the tree.
Itis thereforenatural toask whether thereis atypeof shatt ered trees, analogous tobalanced
trees, which can be used to deﬁne the randomized Littlestone dimension. In this subsection, we
show that such an analog exists: a type of trees which we call quasi-balanced ; roughly speaking,
quasi-balanced trees can be seen as a fractional relaxation of balanced trees. We further use
this section to prove some useful properties of these trees, which will be used later on.
Informally, quasi-balanced trees are balanced under some w eight function deﬁned on the
edges. To formally deﬁne quasi-balanced trees, we need to de ﬁneweight functions for trees.
LetTbe a non-empty tree with edge set E. LetW=W(T) be the set of all functions
w:E→[0,1], such that for every internal node with outgoing edges e0,e1it holds that w(e0)+
w(e1) = 1. Each function in Wis called a weight function forT.
For every branch b∈B(T) deﬁned by a sequence of consecutive edges, deﬁne the weight of
the branch bwith respect to wbyw(b) =/summationtext
e∈bw(e).
The expected weight of a random branch is always half the expe cted length of a random
branch, as a simple inductive argument shows.
Lemma 6.1. For every non-empty tree Tand every weight function w∈ W(T), the expected
weight of a random branch is ET/2.
Proof.The proof is by induction on the depth of the tree. If Tis a leaf then the expected
weight of a random branch is 0 = ET/2. IfTis not a leaf, let e0,e1be the edges emanating
from the root, and let T0,T1be the corresponding subtrees. Applying the inductive hypo thesis,
the expected weight of a random branch in Tunderwis
w(e0)+ET0/2
2+w(e1)+ET1/2
2=1+ET0/2+ET1/2
2=ET/2,
usingw(e0)+w(e1) = 1 and Equation (3).
This lemma prompts the following deﬁnition.
Deﬁnition 6.2. A treeTisquasi-balanced if it is non-empty and there is a weight function
w∈ W(T) under which all branches have weight ET/2.
We callET/2 theweightof the tree, and denote it by λT.
20Lemma 6.3. If a treeTis quasi-balanced then there is a unique weight function wunder which
all branches have the same weight. Explicitly, if T′is a subtree of Twhose root is connected via
edgese0,e1to the subtrees T0,T1, then
w(e0) =1+λT1−λT0
2andw(e1) =1+λT0−λT1
2.
Proof.The trees T′,T0,T1are necessarily quasi-balanced, and in particular
w(e0)+λT0=w(e1)+λT1.
Sincew(e0)+w(e1) = 1, we can solve for w(e0),w(e1), obtaining the claimed formula.
Quasi-balanced trees are a generalization of balanced tree s: every tree Twhich is balanced
is also quasi-balanced with weight λT=d/2, where dis the depth of T. This weight is realized
by the (unique) constant weight function that gives weight 1 /2 to all edges. The family of
quasi-balanced trees is, however, much broader than the fam ily of balanced trees.
Recall the deﬁnition of the randomized Littlestone dimensi on of the class H:
RL(H) =1
2sup
TshatteredET.
It turns out that in this deﬁnition, it suﬃces to take the supr emum only over quasi-balanced
trees. Thiswill beeasier toseethroughthecharacterizati on ofquasi-balanced trees as monotone
trees.
Deﬁnition 6.4 (Monotone Trees) .A non-empty tree Tisweakly monotone if
ET≥max{ET0,ET1},
whereT0andT1are the subtrees rooted at the children of the root of T. A tree is monotone if
it is non-empty and all of its subtrees are weakly monotone.
It is not hard to see that non-monotone trees need not be consi dered when computing the
randomized Littlestone dimension.
Lemma 6.5. For any non-empty hypothesis class H,
RL(H) =1
2sup
Tshattered, monotoneET.
Proof.Consider a tree Tshattered by Hwhich is not monotone. Then there exists a vertex v
such that ETv< ETw, whereTwis a tree rooted at a child of v. If we replace the subtree rooted
atvwith the subtree Tw, we get a tree which is also shattered by H, and has higher expected
branch length.
Repeating this process ﬁnitely many times, for each tree Tshattered by Hwe obtain a
monotone tree T′shattered by Hsatisfying ET′≥ET, and the lemma follows.
Thefollowing theorem asserts that monotone and quasi-bala nced trees are indeed equivalent.
Theorem 6.6. A tree is quasi-balanced if and only if it is monotone.
Corollary 6.7. For any non-empty hypothesis class H,
RL(H) =1
2sup
Tshattered, quasi-balancedET.
21To prove Theorem 6.6, we use the following simple observatio n.
Observation 6.8. LetTbe a non-empty tree. Then Tis weakly monotone if and only if
|ET0−ET1| ≤2, whereT0andT1are the subtrees rooted at the children of the root of T.
Proof.Equation (3) states that 2 ET= 2 +ET0+ET1, and so ET0≤ETis equivalent to
ET0−ET1≤2. Similarly, ET1≤ETis equivalent to ET1−ET0≤2. Hence Tis weakly
monotone iﬀ |ET0−ET1| ≤2.
Proof of Theorem 6.6. An empty tree is neither quasi-balanced nor monotone. Suppo se there-
fore that we are given a non-empty tree T. We prove the equivalence by proving both implica-
tions separately.
Monotone =⇒Quasi-balanced. The proof is by induction on the depth of the tree. A
tree of depth 0 (the base case) is quasi-balanced with weight ET/2 = 0. For the induction step,
letT0,T1be the subtrees rooted at the root’s children. They are clear ly monotone, and so by
induction, there are weight functions w0∈ W(T0) andw1∈ W(T1) under which all branches
inT0have weight λT0=ET0/2 and all branches in T1have weight λT1=ET1/2.
Lete0,e1be the edges connecting the root of Tto the roots of T0,T1, respectively. Deﬁne
a weight function w∈ W(T) by deﬁning w(e) =w0(e) ife∈T0,w(e) =w1(e) ife∈T1,
w(e0) =1+λT1−λT0
2,andw(e1) =1+λT0−λT1
2.
Clearlyw(e0) +w(e1) = 1. Observation 6.8 implies that w(e0),w(e1)∈[0,1], and so indeed
w∈ W(T). Since w(e0) +λT0=w(e1) +λT1, the weight function wshows that Tis quasi-
balanced.
Quasi-balanced =⇒Monotone. The proof is by induction on the depth of the tree. A
tree of depth 0 is monotone. For the induction step, we ﬁrst ob serve that every proper subtree
ofTis quasi-balanced, and so monotone by the inductive hypothe sis. Hence it suﬃces to show
thatTis weakly monotone.
Letwbe the unique weight function for Tunder which each branch has weight λT. Let
e0,e1be the edges connecting the root of Tto the two subtrees T0,T1. According to Lemma 6.3,
the weights of these edges are
w(e0) =1+λT1−λT0
2andw(e1) =1+λT0−λT1
2.
Since the weights are non-negative, we deduce that |λT0−λT1| ≤1, and so |ET0−ET1| ≤2. We
conclude that Tis weakly monotone by Observation 6.8.
6.2 A Concentration Lemma for Quasi-Balanced Trees
Another interesting property of quasi-balanced trees is th at the length of a random branch
concentrates around its expectation. This property will be important for deriving tight bounds
in Section 9.
Proposition 6.9 (Concentration of branch lengths) .LetTbe a quasi-balanced tree, and let X
be the length of a random branch. Then for any ǫ >0,
Pr[X <(1−ǫ)ET]≤exp(−ǫ2ET/4)andPr[X >(1+ǫ)ET]≤exp(−ǫ2ET/4(1+ǫ)).
22Proof.IfTis a single leaf then the result trivially holds since there i s a single random branch.
Therefore we can assume that Tis not a single leaf, and in particular, ET≥1.
Letb0,b1,b2,...be an inﬁnite sequence of random coin tosses. We can choose a r andom
branch of Tas follows. Let v0bethe root of T. Fori∈N, ifviis not a leaf, then vi+1is obtained
by following the edge labeled bi. Otherwise, we deﬁne vi+1=vi. The resulting random branch
has exactly the same distribution that we have been consider ing so far.
LetLibetheexpectedlengthofthebranchgiven b0,...,bi−1. Thisisanexposuremartingale,
as deﬁned in Section 4.
In order to apply Azuma’s inequality, we need to bound the ran dom diﬀerence |Li−Li+1|.
Ifviis a leaf, then Li+1=Li. Otherwise, let T′be the subtree rooted at vi, and let T′
0,T′
1be the
subtrees rooted at the children of vi. ThusLi+1is either λ0:=i+1+ET′
0orλ1:=i+1+ET′
1,
depending on the value of bi. Moreover, Li= (λ0+λ1)/2 is the average of these two values.
Theorem 6.6 shows that T′is weakly monotone, and so Observation 6.8 shows that |ET′
0−
ET′
1| ≤2. Consequently,
|Li−Li+1|=1
2|λ0−λ1| ≤1.
The deﬁnition of Liimplies that Lβ=Xfor allβ≥X. In particular, if X <(1−ǫ)ET
thenL⌈ET⌉<(1−ǫ)ET. Applying Azuma’s inequality and using L0=ET, it follows that
Pr[X <(1−ǫ)ET]≤Pr[L⌈ET⌉−ET<−ǫET]≤exp/parenleftbigg−ǫ2E2
T
2⌈ET⌉/parenrightbigg
≤exp(−ǫ2ET/4),
where the ﬁnal inequality uses ⌈ET⌉ ≤ET+1≤2ET.
Thedeﬁnitionof Lialsoimpliesthat Lβ≥βwhenever β≤X. Inparticular, if X >(1+ǫ)ET
thenL⌈(1+ǫ)ET⌉≥ ⌈(1+ǫ)ET⌉. Therefore
Pr[X >(1+ǫ)ET]≤Pr[L⌈(1+ǫ)ET⌉−ET> ǫET]≤exp/parenleftbigg−ǫ2E2
T
2⌈(1+ǫ)ET⌉/parenrightbigg
≤exp(−ǫ2ET/4(1+ǫ)),
using (1+ ǫ)ET≥1 as before.
7 Bounded Horizon
So far we have not put any restrictions on the number of rounds . However, in many circum-
stances we are interested in the online learning game when th e number of rounds is bounded.
We model this by assuming that the learner knows an upper boun d on the number of rounds.
We deﬁne M⋆(H,T) to be the optimal randomized mistake bound when the number o f rounds
is at most T.
WecangeneralizeTheorem5.2tothissetting. Therequiredn otionofrandomizedLittlestone
dimension is
RL(H,T) =1
2sup
Tshattered
depth(T)≤TET.
The bounded randomized Littlestone dimension gives the pre cise mistake bound in this
setting.
Theorem 7.1 (Optimal Randomized Mistake Bound with Finite Horizon) .LetHbe an hy-
pothesis class, and let T∈N. Then,
M⋆(H,T) =RL(H,T).
23BoundedRandSOA :Bounded Randomized SOA
Input:An hypothesis class Hand number of rounds T.
Initialize: LetV(1)=H.
Fori= 1,2,...,T
1. Receive xi.
2. Predict pi∈[0,1] such that the value
max/braceleftBig
pi+RL/parenleftBig
V(i)
xi→0,T−i/parenrightBig
,1−pi+RL/parenleftBig
V(i)
xi→1,T−i/parenrightBig/bracerightBig
is minimized, where V(i)
xi→b={h∈V(i):h(xi) =b}.
3. Receive true label yi.
4. Update V(i+1)=V(i)
xi→yi.
Figure 3: BoundedRandSOA is a bounded variant of RandSOA .
We prove Theorem 7.1 in Section 7.1. This theorem immediatel y suggests the following
questions:
1. How many rounds are needed in order for the adversary to gua rantee that the loss of the
learner is at least RL(H)−ǫ?
We prove in Section 7.2 that 2 RL(H) +O(/radicalbig
RL(H)log(RL(H)/ǫ)) rounds always suﬃce,
andO(log(1/ǫ)) rounds suﬃce as long as ǫis small enough.
2. What can we say about the loss of the learner when there are f ewer than 2 RL(H) rounds?
A trivial upper bound on RL(H,T) isT/2. In Section 7.3 we show that this bound is
nearly optimal when T≤2RL(H).
The proofs of these results use concentration bounds on the d epth of quasi-balanced trees,
which we prove in Section 6.2.
7.1 Proof of Theorem 7.1
In this section we indicate how to generalize the proof of The orem 5.2 to the ﬁnite horizon
setting, proving Theorem 7.1.
The lower bound RL(H,T)≤M⋆(H,T) follows directly from the statement of Lemma 5.3,
since the length of Sis at most depth(T).
For the upper bound, we use a straightforward modiﬁcation of algorithm RandSOA , which
appears in Figure 3.
We start by extending Observation 5.5: if His a non-empty hypothesis class and T>0
then
RL(H,T) =1
2max
x∈X/parenleftbig
1+RL(Hx→0,T−1)+RL(Hx→1,T−1)/parenrightbig
.
The proof is identical. Since there are only ﬁnitely many unl abeled trees of depth at most T,
we can replace the supremum with a maximum.
24The next step is to generalize Lemma 5.6, which now states tha t for every hypothesis class
H, instance x∈ X, andT>0, there exists p∈[0,1] so that
p+RL(Hx→0,T−1)≤RL(H,T) and (1 −p)+RL(Hx→1,T−1)≤RL(H,T).
The proof is identical, using the generalized Observation 5 .5.
Finally, we prove the following generalization of Lemma 5.7 : for every hypothesis class H,
any parameter T, and any realizable input sequence Sof length at most T,
M(BoundedRandSOA ;S)≤RL(H,T).
The proof is identical, using the generalized Lemma 5.6.
7.2 Approaching RL(H)
As a simple consequence of the concentration bound proved in Proposition 6.9, we show that
we can approach RL(H) using relatively shallow trees, quantiﬁed as follows.
Proposition 7.2. LetHbe a non-empty hypothesis class with ﬁnite randomized Little stone
dimension RL(H).
For every ǫ >0there is a tree Tshattered by Hsatisfying ET/2≥RL(H)−ǫwhose depth
is at most
2RL(H)+O/parenleftBigg/radicalbigg
RL(H)logRL(H)
ǫ+log1
ǫ/parenrightBigg
= 2RL(H)+O/parenleftbigg/radicalbig
RL(H)logRL(H)
ǫ/parenrightbigg
.
This means that the adversary can force the learner to suﬀer a l oss ofRL(H)−ǫafter only
2RL(H)+O(/radicalbig
RL(H)log(RL(H)/ǫ)+log(1/ǫ)) rounds. In contrast, at least 2 RL(H)−2ǫrounds
are clearly needed, since a learner who predicts 1 /2 at each round suﬀers a loss of R/2 afterR
rounds.
We prove Proposition 7.2 via the following technical estima te.
Lemma 7.3. LetTbe a monotone tree, and let T≤kresult from truncating it to the ﬁrst k
levels (all branches in T≤khave length at most k). Ifk≥ETthen
ET≤k≥ET−15/radicalbig
ETexp/parenleftbigg
−(k−ET)2
8ET/parenrightbigg
−10exp/parenleftbigg
−k−ET
8/parenrightbigg
.
Proof.LetXbe the length of a random branch of T. UsingX, we can express the diﬀerence
between ET≤kandETexplicitly:
ET−ET≤k=∞/summationdisplay
t=kPr[X > t].
Applying Proposition 6.9, we deduce that the diﬀerence is at m ost
∆ :=∞/summationdisplay
t=kexp/parenleftbigg
−(t−ET)2
4t/parenrightbigg
≤exp/parenleftbigg
−(k−ET)2
4k/parenrightbigg
+/integraldisplay∞
kexp/parenleftbigg
−(t−ET)2
4t/parenrightbigg
dt.
Ifk≤2ETthen
∆≤exp/parenleftbigg
−(k−ET)2
4k/parenrightbigg
+/integraldisplay2ET
kexp/parenleftbigg
−(t−ET)2
8ET/parenrightbigg
dt+/integraldisplay∞
2ETexp/parenleftbigg
−t−ET
8/parenrightbigg
dt
25≤5/radicalbig
ETexp/parenleftbigg
−(k−ET)2
8ET/parenrightbigg
+8exp/parenleftbigg
−ET
8/parenrightbigg
≤15/radicalbig
ETexp/parenleftbigg
−(k−ET)2
8ET/parenrightbigg
,
using the well-known Gaussian tail bound,
/integraldisplay∞
ke−(t−µ)2/2σ2dt=√
2πσ2Pr[N(µ,σ)> k]≤√
2πσ2e−(k−µ)2/2σ2(k≥µ),
to bound the ﬁrst integral.
Ifk≥2ETthen
∆≤exp/parenleftbigg
−k−ET
8/parenrightbigg
+/integraldisplay∞
kexp/parenleftbigg
−t−ET
8/parenrightbigg
dt≤9exp/parenleftbigg
−k−ET
8/parenrightbigg
.
We can now prove Proposition 7.2.
Proof of Proposition 7.2. Applying Lemma 6.5, we can ﬁnd a monotone tree Tshattered by H
such that 2 RL(H)−ǫ/2≤ET≤2RL(H). Let
k=ET+/radicalbigg
8ETlog60√ET
ǫ+8log20
ǫ=ET+O/parenleftBigg/radicalbigg
ETlogET
ǫ+log1
ǫ/parenrightBigg
.
Lemma 7.3 implies that ET≤k≥ET−ǫ/2, and so ET≤k≥2RL(H)−ǫ.
7.3 Mistake Bound for Few Rounds
Another truncation argument allows us to estimate RL(H,T) for small T.
Proposition 7.4. LetHbe a non-empty hypothesis class with ﬁnite randomized Little stone
dimension RL(H).
IfT≤RL(H)thenRL(H,T) =T/2.
IfT≤2RL(H)then
T
2−O(/radicalbig
TlogT)≤RL(H,T)≤T
2.
Furthermore, if T≤2RL(H)−/radicalbig
8RL(H)lnRL(H)then
T
2−1<RL(H,T)≤T
2,
and ifT≥2RL(H)−/radicalbig
8RL(H)lnRL(H)then
RL(H)−O/parenleftBig/radicalbig
RL(H)logRL(H)/parenrightBig
≤RL(H,T)≤RL(H).
Proof.A learner that always predicts 1 /2 suﬀers a loss of exactly 1 /2 each round, showing that
RL(H,T)≤T/2 for each T. In contrast, if Tis a tree shattered by Hthen Theorem 7.1 shows
thatRL(H,T)≥ET≤T/2, and we will use this to give lower bounds on RL(H,T).
Suppose ﬁrst that T≤RL(H). Our characterization shows that mT≥ET/2. IfETis close
enough to 2 RL(H) thenmT≥RL(H) (sincemTis an integer), and so T≤Tis a complete tree of
depthT. This shows that RL(H,T)≥T/2.
In order to prove the remaining results, suppose that T≤2RL(H), and consider a tree T
shattered by Hsatisfying ET= 2RL(H)−δ≥T. Proposition 6.9 shows that a random branch
ofT≤Thas depth Twith probability at least 1 −exp/parenleftbig
−(ET−T)2
4ET/parenrightbig
, and so
RL(H,T)≥/parenleftbigg
1−exp/parenleftbigg
−(ET−T)2
4ET/parenrightbigg/parenrightbigg
·T
2−→/parenleftbigg
1−exp/parenleftbigg
−(2RL(H)−T)2
8RL(H)/parenrightbigg/parenrightbigg
·T
2,
26where the limit is taken along a sequence of trees shattered b yHand satisfying ET→2RL(H).
IfT≤T0:= 2RL(H)−/radicalbig
8RL(H)lnRL(H) then this gives
RL(H,T)≥/parenleftbigg
1−1
RL(H)/parenrightbigg
·T
2>T
2−1.
IfT0≤T≤2RL(H) then
RL(H,T)≥RL(H,T0)≥T0−2
2≥T−2
2−/radicalbig
8RL(H)lnRL(H)≥T
2−O(/radicalbig
TlogT).
Finally, if we only assume that T≥T0then
RL(H,T)≥RL(H,T0)≥T0−2
2≥RL(H)−/radicalbig
8RL(H)lnRL(H)−1.
8 Mistake Bounds in the k-Realizable Setting
So far we have considered online learning when the adversary is restricted to choose labels
which are consistent with one of the hypotheses in the hypoth esis class, a setting known as the
realizable setting. This is a quite restrictive assumption, and there a re many ways to relax it.
In this section we concentrate on the k-realizable setting, in which the answers of the ad-
versary are consistent with one of the hypotheses in the clas sup to at most kmistakes. Our
goal is to characterize the optimal mistake bounds in this se tting, for both deterministic and
randomized learners, generalizing Theorems 4.1 and 5.2. Ou r characterizations are based on
k-shattered trees , in which each branch is consistent with one of the hypothese s in the class up
to at most kmistakes.
If all instances in a sequence of examples are distinct, then the sequence is k-realizable by
Hif and only if it is realizable by the k-expansion ofH, consisting of all hypotheses h′which
disagree with some hypothesis h∈ Hon at most kinstances. However, this need not be the
case. For example, the sequence ( x,0),(x,1) is 1-realizable by the hypothesis class Hconsisting
of all constant functions.
Nevertheless, the arguments in this section are very simila r to their counterparts in the
realizable setting.
8.1 Weighted Hypothesis Classes
While we are interested mainly in the k-realizable setting, we consider a more general setting
in which the number of allowed mistakes can depend on the hypo thesis. This will be useful in
the subsequent proofs.
A weighted hypothesis class Wis a collection of pairs ( h,w), where h:X → Yis an hypoth-
esis and w∈Nis the allowed number of mistakes (possibly zero). Furtherm ore, all hypotheses
are distinct (that is, Wcannot contain two diﬀerent pairs ( h,w1),(h,w2)). An input sequence
(x1,y1),...,(xt,yt) isrealizable by a weighted hypothesis class Wif there exists ( h,w)∈ W
such that h(xi)/n⌉}ationslash=yifor at most wmany examples in the sequence. A tree is shattered byWif
each of its branches is realized by W.
Given an hypothesis class H, a learning rule which observes the labeled example ( x,y) can
restrict itself to Hx→y={h∈ H:h(x) =y}. The corresponding operation for weighted
hypothesis classes is
Wx→y={(h,w) : (h,w)∈ W,h(x) =y}∪{(h,w−1) : (h,w)∈ W,h(x)/n⌉}ationslash=y,w >0}.
27In words, we decrease the allowed number of mistakes for each hypothesis inconsistent with the
given example ( x,y), removing hypotheses which has zero mistakes left.
For every weighted hypothesis class W, we deﬁne its Littlestone dimension and its random-
ized Littlestone dimension by
L(W) = sup
TshatteredmTandRL(W) =1
2sup
TshatteredET,
where the supremum is taken over all trees shattered by W. As in the realizable setting, we
deﬁneL(∅) =RL(∅) =−1 for convenience.
Our main results in this section extend Theorems 4.1 and 5.2 t o this more general setting.
Theorem 8.1 (Optimal Deterministic Mistake Bound) .LetWbe a weighted hypothesis class.
Then,
M⋆
D(W) =L(W).
Theorem 8.2 (Optimal Randomized Mistake Bound) .LetWbe a weighted hypothesis class.
Then,
M⋆(W) =RL(W).
We prove these theorems in the following subsections, makin g use of the following funda-
mental observation, which follows directly from the deﬁnit ions:
Observation 8.3. LetWbe a weighted hypothesis class. The sequence (x1,y1),...,(xt,yt)is
realizable by Wiﬀ the sequence (x2,y2),...,(xt,yt)is realizable by Wx1→y1.
Similarly, let Tis a tree whose root is labeled by x, and let T0,T1be the subtrees rooted at the
children of the root. Then Tis realizable by WiﬀT0is realizable by Wx→0andT1is realizable
byWx→1.
Thek-realizable setting. LetHbe an hypothesis class, and let k∈N. A sequence of
examples S={(xi,yi)}t
i=1isk-realizable byHif there exists h∈ Hsuch that h(xi)/n⌉}ationslash=yifor at
mostkindicesi. We denote the correspondingmistake boundsby M⋆(H,k),M⋆
D(H,k). These are
deﬁned just as in the realizable setting, the only diﬀerence b eing that the sequence of examples
provided by the adversary need only be k-realizable by H.
Wesaythatatreeis k-shattered by Hifeverybranchis k-realizable by H. Thecorresponding
deterministic and randomized k-Littlestone dimension of a class Hare
Lk(H) = sup
T k-shatteredmTandRLk(H) =1
2sup
T k-shatteredET.
If we deﬁne WH,k={(h,k) :h∈ H}, then a sequence of examples is k-realizable by Hif
it is realizable by WH,k. In other words, the k-realizable setting is a special case of weighted
hypothesis classes, where all weights are equal to k. Therefore we immediately conclude the
following theorems, by applying the preceding theorems to WH,k:
Theorem 8.4 (Optimal Deterministic Mistake Bound) .LetHbe an hypothesis class, and let
k∈N. Then,
M⋆
D(H,k) =Lk(H).
Theorem 8.5 (Optimal Randomized Mistake Bound) .LetHbe an hypothesis class, and let
k∈N. Then,
M⋆(H,k) =RLk(H).
28Using the classic lower bounds of [LW94,BDPSS09] and recent results of [ABED+21], we
can bound the optimal mistake bound in terms of the realizable Littlestone dimension:
Theorem 8.6. LetHbe an hypothesis class with at least two hypotheses, and let k∈N. Then,
M⋆(H,k) =k+Θ/parenleftBig/radicalbig
k·L(H)+L(H)/parenrightBig
.
We prove this result in Section 8.4. Note that since L(H) andRL(H) diﬀer by at most a
constant factor, the theorem still holds if we replace L(H) byRL(H).
Using the experts algorithm of [KvE15], we can construct an a lgorithm which works in the
adaptive setting, that is, without knowledge of k:
Theorem 8.7. LetHbe an hypothesis class. There is an algorithm Squintsuch that for every
input sequence Swhich is k⋆-realizable by H,
M(Squint;S)≤M⋆(H,k⋆)+O/parenleftBig/radicalbig
M⋆(H,k)log((k⋆+1)logM⋆(H,k⋆))/parenrightBig
.
Furthermore, Squintis adaptive, that is, it has no knowledge of k⋆.
We describe and analyze the algorithm in Section 8.5.
8.2 Proof of Optimal Deterministic Mistake Bound
The case W=∅holds by deﬁnition. Therefore we assume that W /n⌉}ationslash=∅. The lower bound
“L(W)≤M⋆
D(W)” boils down to the following lemma:
Lemma 8.8. LetWbe a weighted hypothesis class, and let Tbe a ﬁnite tree which is shattered
byW. Then, for every deterministic learning rule Lrnthere exists a realizable sequence Sso
thatM(Lrn;S)≥mT. Furthermore, Scorresponds to one of the branches in T.
Proof.We construct the sequence Sby traversing T, starting at the root v1. At step i, we send
Lrnthe instance xilabelling vi. If the learner predicts ˆ yi, we set the true label to yi= 1−ˆyi,
and letvi+1be the vertex obtained from viby following the leaf labeled yi. We stop once the
process reaches a leaf.
By construction, Scorresponds to one of the branches of T, and the number of mistakes is
|S| ≥mT. SinceTis shattered by W, thenSis realizable by W.
By applying the lemma on every shattered tree and taking the s upremum, we conclude the
lower bound:
Corollary 8.9 (Lower bound) .For every weighted hypothesis class Wit holds that M⋆
D(W)≥
L(W).
We now turn to prove the upper bound “ L(W)≥M⋆
D(W)”. This is achieved via the
WeightedSOA learning rule, depicted in Figure 4.
Lemma 8.10 (Upperbound) .LetWbe a non-empty weighted hypothesis class. The WeightedSOA
learner described in Figure 4 has the mistake bound
M(WeightedSOA ;S)≤L(W)
for every input sequence Srealizable by W.
29WeightedSOA
Input:A weighted hypothesis class W.
Initialize: LetV(1)=W.
fori= 1,2,...
1. Receive xi.
2. Predict
ˆyi= argmax
b∈YL/parenleftBig
V(i)
xi→b/parenrightBig
.
3. Receive true label yi.
4. Update V(i+1)=V(i)
xi→yi.
Figure 4: The weighted version of SOA.
Proof.We will show that each time that WeightedSOA makes a mistake, the Littlestone dimen-
sion drops by at least 1. That is, if ˆ yi/n⌉}ationslash=yithenL(V(i+1))<L(V(i)). Since the Littlestone
dimension is always non-negative, it follows that WeightedSOA makes at most L(W) mistakes.
Suppose that ˆ yi/n⌉}ationslash=yiyetL(V(i+1)) =L(V(i)). The choice of ˆ yishows that L(V(i)
xi→0) =
L(V(i)
xi→1) =L(V(i)). Thisis, however, impossible. Indeed,taketrees T0,T1shattering V(i)
xi→0,V(i)
xi→1
withmT0=mT1=L(V(i)). Observation8.3showsthat thetree Twhoserootislabeled xiandin
whichT0,T1are the subtrees of the root’s children is shattered by V(i). SincemT=L(V(i))+1,
we reach a contradiction.
8.3 Proof of Optimal Randomized Mistake Bound
The proof of the optimal mistake bound in the randomized sett ing, Theorem 8.2, is very similar
to the proof of its counterpart in the realizable setting, Th eorem 5.2.
The proof of the lower bound “ RL(W)≤M⋆(W)” is virtually identical to the proof of
Lemma 5.3.
The proof of the upper bound “ RL(W)≥M⋆(W)” usesWeightedRandSOA , the weighted
counterpart of RandSOA , which appears in Figure 5. The proof of Lemma 5.7 extends, wi th
virtually no changes, to show that M(WeightedRandSOA ;S)≤RL(W) for every input sequence
Srealizable by W.
8.4 Explicit Bounds in Terms of Littlestone Dimension
Here we prove Theorem 8.6, which bounds M⋆(H,k) in terms of kandL(H) (orRL(H)). In the
proof, we use the notation M⋆(H,k,T) for the optimal mistake bound in the k-realizable setting
when the number of rounds is bounded by T, and the notation M⋆
Agn(H,T) for the optimal
mistake bound when the number of rounds is bounded by T, but there is no limitation on the
number of mistakes made by the best hypothesis in H.
We ﬁrst prove the upper bound. [ABED+21] have shown that, for any time horizon T,
we always have M⋆(H,k,T)≤k+O/parenleftBig/radicalbig
T·L(H)/parenrightBig
. By Proposition 7.2, time horizon T=
30WeightedRandSOA
Input:A weighted hypothesis class W.
Initialize: LetV(1)=W.
fori= 1,2,...
1. Receive xi.
2. Predict pi∈[0,1] such that the value
max/braceleftBig
pi+RL/parenleftBig
V(i)
xi→0/parenrightBig
,1−pi+RL/parenleftBig
V(i)
xi→1/parenrightBig/bracerightBig
is minimized.
3. Receive true label yi.
4. Update V(i+1)=V(i)
xi→yi.
Figure 5: The weighted version of RandSOA .
O(M⋆(H,k)) suﬃces to guarantee M⋆(H,k)≤M⋆(H,k,T) + 1. Plugging this time horizon into
the result of [ABED+21] reveals that
M⋆(H,k)≤k+O/parenleftBig/radicalbig
M⋆(H,k)·L(H)/parenrightBig
.
Solving this quadratic inequality in/radicalbig
M⋆(H,k) yields the upper bound claimed in the theorem.
We now turn to prove the lower bound. We consider two cases. If k≤L(H), it suﬃces
to prove a lower bound of k+ Ω(L(H)). The lower bound k+L(H)/2 of [LW94] establishes
that. In the complementary case, suppose that k >L(H). Therefore, we only need to prove
a lower bound of k+Ω/parenleftBig/radicalbig
k·L(H)/parenrightBig
. This follows from the following adaptation of the classic
regret bound of [BDPSS09]. They showed that M⋆
Agn(H,T)≥b+Ω/parenleftBig/radicalbig
L(H)·T/parenrightBig
, wherebis the
minimal number of mistakes made by a best hypothesis h⋆∈ H. To adapt this bound to our
setting, ﬁrst play the game for T=krounds, forcing a loss of at least b+Ω/parenleftBig/radicalbig
k·L(H)/parenrightBig
on the
learner. Now, as we prove in Theorem 9.3,11the adversary can further force the learner a loss
arbitrarily close to k−b, using an input sequence which is ( k−b)-realizable by h⋆. Overall, the
input sequence is k-realizable by h⋆, and we get the desired lower bound k+Ω/parenleftBig/radicalbig
k·L(H)/parenrightBig
.
8.5 Adapting to k
This section presents our proof of Theorem 8.7, showing that it is possible to adapt to the value
ofkwithout sacriﬁcing too signiﬁcantly in the expected mistak e bound.
The adaptive technique we propose uses an experts algorithm of [KvE15] named Squint,
with experts deﬁned by the optimal randomized algorithm for thek-realizable setting, for all
values of k∈N(including k= 0).
11We prove this result in the setting of prediction with expert advice, but it holds for general hypothesis classes
(as long as the domain is non-empty).
31The experts algorithm Squintaccepts an input sequence S= (x1,y1),...,(xn,yn) and a list
of learners Lrnk, each with an associated weight πk. The weights πkshould form a probabil-
ity distribution. With an appropriate choice of parameters ,Squinthas the following guaran-
tee [KvE15, Theorem 3]:
M(Squint;S)≤min
k/braceleftBigg
M(Lrnk;S)+O/parenleftBigg/radicalBigg
VkloglogVk
πk+log1
πk/parenrightBigg/bracerightBigg
, (8)
whereVkis an uncentered variance term given by
Vk=n/summationdisplay
i=1(|Squint(x1,y1,...,x i−1,yi−1,xi)−yi|−|Lrnk(x1,y1,...,x i−1,yi−1,xi)−yi|)2.
Since both absolute values are in the range [0 ,1], we have
Vk≤n/summationdisplay
i=1|Squint(x1,y1,...,x i−1,yi−1,xi)−yi|+n/summationdisplay
i=1|Lrnk(x1,y1,...,x i−1,yi−1,xi)−yi|
=M(Squint;S)+M(Lrnk;S).
For any given k, ifM(Squint;S)>M(Lrnk;S), then we have Vk≤2M(Squint;S), so that (8)
implies
M(Squint;S)≤M(Lrnk;S)+O
/radicalBigg
M(Squint;S)loglogM(Squint;S)
πk+log1
πk
.
This inequality trivially holds as well in the case M(Squint;S)≤M(Lrnk;S) due to the ﬁrst term
on the right hand side. Moreover, this inequality further im plies
M(Squint;S) =O/parenleftbigg
M(Lrnk;S)+log1
πk+1/parenrightbigg
.
To see this, note that were it not the case, we could upper boun d eachM(Lrnk;S) and log1
πkon the right hand side by M(Squint;S)/cfor some large constant c, making the right hand side
strictly less than M(Squint;S): a contradiction. Plugging in this upper bound on M(Squint;S)
into the loglog M(Squint;S) term and simplifying with elementary inequalities reveal s
M(Squint;S)≤M(Lrnk;S)+O
/radicalBigg
M(Squint;S)loglogM(Lrnk;S)
πk+log1
πk
.
Thisisaquadraticinequalityin/radicalbig
M(Squint;S). Solvingthequadraticfortherangeof M(Squint;S)
where the inequality holds, we have
M(Squint;S)≤M(Lrnk;S)+O
/radicalBigg
M(Lrnk;S)loglogM(Lrnk;S)
πk+loglogM(Lrnk;S)
πk
.
Since this holds for any k, we conclude that
M(Squint;S)≤min
k

M(Lrnk;S)+O
/radicalBigg
M(Lrnk;S)loglogM(Lrnk;S)
πk+loglogM(Lrnk;S)
πk


.
(9)
32We instantiate Squintwith algorithm WeightedRandSOA of Figure 5. Namely, for every k,
we letLrnkbe the instantiation of WeightedRandSOA withWH,k. We use the weights πk=
1
(k+1)(k+2). Since πk=1
k+1−1
k+2, they indeed constitute a probability distribution. Since
WeightedRandSOA achieves the optimal mistake bound (see Section 8.3), Eq. (9 ) shows that if
Sisk⋆-realizable by Hthen
M(Squint;S)≤M⋆(H,k⋆)+O/parenleftbigg/radicalBig
M⋆(H,k⋆)log/parenleftbig
(k⋆+1)logM⋆(H,k⋆)/parenrightbig
+log/parenleftbig
(k⋆+1)M⋆(H,k⋆)/parenrightbig/parenrightbigg
.
SinceM⋆(H,k⋆)≥k⋆/2, the term log/parenleftbig
(k⋆+ 1)M⋆(H,k⋆)/parenrightbig
can be swallowed by the preceding
term.
9 Prediction using Expert Advice
In this section, we consider the problem of prediction using expert advice , which was raised
in[Vov90,LW94]. Speciﬁcally, weconsiderthe k-realizablesetting, whichwassuggestedin[CBFHW96,
CBFH+97] and further studied in [ALW06,MS10,BP19].
The problem concerns a repeated game which has the same ﬂavor as the online learning
game of Section 4. The game is between a learner and an adversa ry. Additionally, there are n
experts. Each round iin the game proceeds as follows:
(i) The experts present predictions ˆ y(1)
i,...,ˆy(n)
i∈ {0,1}.
(ii) The learner predicts a value pi∈[0,1].
(iii) The adversary reveals the true answer yi∈ {0,1}, and the learner suﬀers the loss |yi−pi|.
The adversary must choose the answers so that at least one of t he experts makes at most k
mistakes. That is, there must exist an expert jsuch that yi/n⌉}ationslash= ˆy(j)
ifor at most kmany indices
i. We call such an adversary k-consistent .
The goal is to determine the optimal loss of the learner as a fu nction of nandk. We denote
the optimal loss of the learner by M⋆(n,k), and the optimal loss when the learner is constrained
to output predictions in {0,1}byM⋆
D(n,k).
The game underlying prediction using expert advice is quite similar to the online learning
game. In fact, we can relate the two.
LetXn={0,1}n, and consider the hypothesis class Unon the domain Xnconsisting of the
projection functions hi(x1,...,x n) =xi. We can simulate the game of prediction using expert
advice by the online learning game as follows: whenever the e xperts predict x1,...,x n, the
adversary sends the instance ( x1,...,x n). The adversary in the original game is k-consistent if
and only if the sequence ( xi,yi) isk-realizable by Un.
This simulation goes both ways, and so the two games are actua lly equivalent. The upshot
is that we can express M⋆(n,k) andM⋆
D(n,k) in terms of quantities we have already considered:
M⋆(n,k) =M⋆(Un,k) =RLk(Un) andM⋆
D(n,k) =M⋆(Un,k) =Lk(Un).
The equivalence above shows that Unis the “hardest” hypothesis class of size n, in the sense
that it maximizes both M⋆(H,k) andM⋆
D(H,k) over all hypothesis classes Hof sizen. Indeed,
M⋆(H,k) andM⋆
D(H,k) are equal to the optimal loss in the game of prediction using expert
advice when the answers of the experts must belong to {(h1(x),...,h n(x)) :x∈ X}, where
H={h1,...,h n}has domain X.
33Bounded horizon. Prediction using expert advice is often considered when the number of
rounds is bounded. Let M⋆(n,k,T) be the optimal loss of the learner when the number of rounds
isT.
ClearlyM⋆(n,k,T)≤M⋆(n,k). InviewofTheorem7.1, Proposition7.2showsthat M⋆(n,k,T)≥
M⋆(n,k)−ǫalready for T= 2M⋆(n,k) +O(/radicalbig
M⋆(n,k)log(M⋆(n,k)/ǫ)). In contrast, since a
learner can always guarantee a loss of at most 1 /2 per round by predicting 1 /2, we have
M⋆(n,k,T)≤T/2, and so M⋆(n,k,T)≥M⋆(n,k)−ǫrequires T≥2M⋆(n,k)−2ǫ.
(The deterministic case is not interesting, since triviall yM⋆
D(n,k,T) = min{T,M⋆
D(n,k)}.)
9.1 Optimal Mistake Bounds
For every n≥1 andk≥0, let
D(n,k) = max/braceleftbigg
d:d≤logn+log/parenleftbiggd
≤k/parenrightbigg/bracerightbigg
.
The value of D(n,k) plays a central role in the problem of prediction using expe rt advice:
[CBFHW96]showedthat M⋆
D(n,k)≤D(n,k)usingthe Binomial Weights learningrule, andcom-
plemented this with an asymptotically matching lower bound M⋆
D(n,k)≥D(n,k)−o(D(n,k)).
Thelower boundis proved by constructing a k-covering code of size nthat simulates the experts.
Whenkis ﬁxed and nis large enough, it can be further improved to M⋆
D(n,k)≥D(n,k)−1, as
shown in [CBFHW96].
Thepaper[CBFHW96] leaves opentheproblem of determiningw hetherM⋆(n,k)≤D(n,k)
2+c
for some universal constant c. [CBFH+97] showed that M⋆(n,k)≤M⋆
D(n,k)/2 +o(M⋆
D(n,k))
whenever k=o(logn) ork=ω(logn).12[ALW06] showed that for large enough n(as a
function of k),M⋆(n,k)≤M⋆
D(n,k)/2 +O(1). [BP19] showed that for k=o(logn),M⋆(n,k)≤
(1 +o(1))M⋆
D(n,k)/2 even in the multiclass setting where the experts’ predicti ons are chosen
from some ﬁnite set {1,...,d}. In this section, we remove any assumptions on n,k, proving the
following theorem:
Theorem 9.1. Letn≥2andk≥0. Then
M⋆(n,k)≤D(n,k)/2+O/parenleftBig/radicalbig
D(n,k)/parenrightBig
.
The error term is tight for n= 2:
Theorem 9.2. Letk≥0. Then
M⋆(2,k) =D(2,k)/2+Ω/parenleftBig/radicalbig
D(2,k)/parenrightBig
.
Allofourboundsareattainedusingtherandomized k-Littlestonedimensionof Un. Notethat
asaspecialcaseofTheorem8.6,onecanalsoderivethebound sM⋆(n,k) =k+Θ/parenleftbig√klogn+logn/parenrightbig
,
usingL(Un) =⌊logn⌋. The upperboundwas proved by [CBFH+97]. We prove the upper bound
in Section 9.2, and the lower bound in Section 9.3. In the full version of the paper, we use our
techniques to determine M⋆(2,k) exactly.
All results we stated so far concern n≥2. The case n= 1 is diﬀerent, and much simpler:
Theorem 9.3. Letk≥0. Then
M⋆(1,k) =M⋆
D(1,k) =D(1,k) =k.
12More precisely, they showed that M⋆(n,k)≤k+logn
2+√
klnn. Together with the bound M⋆
D(n,k)≥
2k+⌊logn⌋of [LW94], this implies that M⋆(n,k)≤M⋆
D(n,k)/2+o(M⋆
D(n,k)) whenever k=o(logn) ork=ω(logn).
34Proof.According to the deﬁnition, D(1,k) is the maximum dsuch that 2d≤/parenleftbigd
≤k/parenrightbig
. Since/parenleftbigd
≤d/parenrightbig
= 2dwhereas/parenleftbigd+1
≤d/parenrightbig
<2d+1, we see that D(1,k) =k.
The complete tree of depth k, labeled arbitrarily, is k-shattered by U1. In contrast, a tree
of depth k+ 1 cannot be k-shattered by U1, since there exists a branch on which the unique
hypothesis makes k+1 mistakes. Therefore M⋆
D(1,k) =k.
For the randomized case,it is convenient to consider an opti mal inﬁnite tree Tksuch that
M⋆(1,k) =RLk(U1) =ETk/2 (proof of existence of Tkcan be found in the full version of the
paper [FHMM22]). Denote the unique hypothesis in U1byh. By possibly switching the order of
children, we can assume that all vertices in Tkare labeled by an instance xsuch that h(x) = 0.
We can then identify vertices of Tkwith binary strings.
SinceTkis optimal, it contains all strings which contain at most kmany 1s. A string is a
leaf it it contains exactly kmany 1s and it ends with 1. The length of a random branch has the
distribution of a sum of kmany Geom(1 /2) random variables, and so M⋆(1,k) =ETk/2 =k.
In contrast, [LW94] shows that M⋆
D(n,k)≥2k+⌊logn⌋forn≥2, highlighting the diﬀerence
between n= 1 andn >1. This immediately implies the following corollary, which will be useful
in the sequel:
Corollary 9.4. Letn≥2andk≥0. ThenD(n,k)≥2k+1.
Proof.ClearlyD(n,k)≥D(2,k). Theorem 9.6 shows that D(2,k)≥M⋆
D(2,k), which is at
least 2k+1 by the result of [LW94].
9.2 Proof of the Upper Bound on M⋆(n,k)
Westartbyprovingaprobabilisticversionofthe sphere packing bound forcoveringcodes[CHLL97].
Lemma 9.5. LetHbe a ﬁnite hypothesis class of size n≥1. Lett≥k≥0, and let Tbe a
tree whose minimum depth is at least t.
LetS= (x1,y1),...,(xt,yt)be therandom preﬁx of length t, consisting of the ﬁrst tsteps
in a random branch of T. The probability that Sisk-realizable by His at most
n/parenleftbiggt
≤k/parenrightbigg
/2t.
Proof.For each hypothesis h∈ Hand set of indices I⊆[t], the probability that yi/n⌉}ationslash=h(xi) for
all indices in Iandyi=h(xi) for all indices outside of Iis 2−t.
Thesequence Sisk-realizable by Hif theevent above happensfor some h∈ Hand some Iof
size at most k. Applying the union bound, we get that the probability is at m ostn/parenleftbigt
≤k/parenrightbig
/2t.
As a warm-up, we use this lemma together with the k-Littlestone dimension to reprove the
upper bound M⋆
D(n,k)≤D(n,k), ﬁrst proved in [CBFHW96].
Theorem 9.6. Letn≥1andk≥0. ThenM⋆
D(n,k)≤D(n,k).
Proof.SinceM⋆
D(n,k) =Lk(Un), it suﬃces to bound Lk(Un).
LetTbe a tree satisfying mT=Lk(Un) which is k-shattered by Un. A random preﬁx of
lengthLk(Un) isk-realizable by Un, and so 2Lk(Un)≤n/parenleftbigLk(Un)
≤k/parenrightbig
by Lemma 9.5. Taking the
logarithm, we deduce that Lk(Un)≤D(n,k) by the deﬁnition of D(n,k).
We now prove Theorem 9.1. The main tools are concentration of the random branch length
in quasi-balanced trees (Lemma 6.9), and the following lemm a.
35Lemma 9.7. LetHbe a ﬁnite hypothesis class of size n≥1. LetD=D(n,k), and let Tbe
a tree of minimum depth at least (1+ǫ)D, where0< ǫ <1/3. The probability that a random
preﬁx of length (1+ǫ)Disk-realizable by His at most
21−ǫ2D/9.
Furthermore, if k≤cDfor some constant c <1/2then the probability is at most
21−c′ǫD,
wherec′>0is a constant depending only on c.
The proof of this lemma will require some elementary estimat es on binomial coeﬃcients,
summarized in the following technical lemma.
Lemma 9.8. LetD≥k≥1andǫ >0. Then
/parenleftbigg(1+ǫ)D
≤k/parenrightbigg
≤2ǫD·log(D/(D−k))·/parenleftbiggD
≤k/parenrightbigg
.
If furthermore k≤D/2andǫ≤1/3then
/parenleftbigg(1+ǫ)D
≤k/parenrightbigg
≤2ǫD−ǫ2k/3·/parenleftbiggD
≤k/parenrightbigg
.
We prove this lemma in Subsection 9.2.1.
Proof of Lemma 9.7. We start by observing that
n/parenleftbiggD
≤k/parenrightbigg
/2D≤2, (10)
Indeed, the maximality of Dshows that
1> n/parenleftbiggD+1
≤k/parenrightbigg
/2D+1≥1
2n/parenleftbiggD
≤k/parenrightbigg
/2D,
from which Eq. (10) immediately follows.
Denote by pthe probability we wish to bound. Lemma 9.5 shows that
p≤n/parenleftbigg(1+ǫ)D
≤k/parenrightbigg
/2(1+ǫ)D=/parenleftbig(1+ǫ)D
≤k/parenrightbig
/parenleftbigD
≤k/parenrightbig·2−ǫD·n/parenleftbiggD
≤k/parenrightbigg
/2D≤21−ǫD·/parenleftbig(1+ǫ)D
≤k/parenrightbig
/parenleftbigD
≤k/parenrightbig,
using Eq. (10). It remains to estimate the ratio using Lemma 9 .8.
We start by proving the “furthermore” part. By assumption, w e havek≤cD. Applying
Lemma 9.8, we deduce that
p≤21−(1−log(D/(D−k)))ǫD.
Since
c′= 1−logD
D−k= 1−log1
1−k/D≥1−log1
1−c>0,
this completes the proof of the “furthermore” part.
In order to prove the main part of the lemma, we distinguish be tween two cases. If k≤D/3
then the “furthermore” bound shows that
p≤21−c′ǫD,
36wherec′= log(4/3). Since ǫ≤1/3, we have c′ǫ≥ǫ2/9, completing the proof in this case.
Otherwise, k≥D/3. In this case, noting that k≤D/2 by Corollary 9.4, we apply the
“furthermore” part of Lemma 9.8 to obtain
p≤21−ǫ2k/3≤21−ǫ2D/9.
We can now prove the upper bound on M⋆(n,k). The idea is simple. Let Tbe a tree which
isk-shattered by Un. Using Corollary 6.7 , we can assume that Tis quasi-balanced, and so the
length of a random branch is concentrated around ET.
This implies that Trealizes almost all sequences of size (1 −ǫ)ET. These sequences are
k-realized by Un, and we obtain an upper bound on ETvia Lemma 9.7.
Proof of Theorem 9.1. SinceM(n,k) =RLk(Un), we bound the latter. It is convenient (although
not necessary) to assume that there is an inﬁnite tree Twhich is k-shattered by Unand satisﬁes
ET/2 =RLk(Un)13. Furthermore, Tis monotone, and so Proposition 6.9 applies to it (while
the proposition is formulated for ﬁnite quasi-balanced tre es, the proof actually directly uses
monotonicity, and is valid for inﬁnite trees).
In order to bound ET, we will show that for small enough ǫ >0, the assumption (1+ ǫ)D≤
(1−ǫ)ETleads to a contradiction.
ExtendTarbitrarily to a tree T′of minimum depth (1+ ǫ)D, and let Sbe a random preﬁx
ofT′of length (1+ ǫ)D. IfSlies completely within Tthen it is k-realizable by Un, and so
Pr[Slies within T]≤Pr[Sisk-realizable by Un].
The probability that Slies within Tis precisely the probability that a random branch of T
has length at least (1+ ǫ)D. Since we assume that (1+ ǫ)D≤(1−ǫ)ET, this probability is at
least 1−e−ǫ2ET/4by Proposition 6.9, and so at least 1 −e−ǫ2D/4.
In contrast, the probability that Sisk-realizable by Unis at most 21−ǫ2D/9by Lemma 9.7.
Therefore
1≤e−ǫ2D/4+21−ǫ2D/9.
Letǫ=C/√
D. AsC→ ∞, the right-hand side tends to 0, and in particular, we obtain a
contradiction for some constant C >0.
It follows that (1+ ǫ)D >(1−ǫ)ETforǫ=C/√
D, and so
ET<1+ǫ
1−ǫD= (1+O(1/√
D))D=D+O(√
D).
9.2.1 Proof of Technical Estimate
In this section we complete the proof of Theorem 9.1 by provin g Lemma 9.8.
We start with estimates on the ratio of individual binomial c oeﬃcients.
Lemma 9.9. LetD≥ℓ≥1andǫ >0. Then
/parenleftbigg(1+ǫ)D
ℓ/parenrightbigg
≤2ǫD·log(D/(D−ℓ))·/parenleftbiggD
ℓ/parenrightbigg
.
If furthermore ℓ≤D/2andǫ≤1/3then
/parenleftbigg(1+ǫ)D
ℓ/parenrightbigg
≤2ǫD·log(D/(D−ℓ))−ǫ2ℓ/3·/parenleftbiggD
ℓ/parenrightbigg
.
13This assumption is proved to be true in the full version of thi s paper [FHMM22]
37Proof.We can calculate the ratio between the binomials explicitly :
Rℓ:=/parenleftbigg(1+ǫ)D
ℓ/parenrightbigg/slashbigg/parenleftbiggD
ℓ/parenrightbigg
=ℓ−1/productdisplay
r=0(1+ǫ)D−r
D−r=ℓ−1/productdisplay
r=0/parenleftbigg
1+ǫD
D−r/parenrightbigg
.
Applying the well-known estimate ln(1+ x)≤x, we obtain
lnRℓ≤ℓ−1/summationdisplay
r=0ǫD
D−r≤ǫD·/integraldisplayD
D−ℓdx
x=ǫD·lnD
D−ℓ,
and so
Rℓ≤2ǫD·log(D/(D−ℓ)).
Now suppose that ℓ≤D/2 andǫ≤1/3. Forr∈ {0,...,ℓ−1}we have
ǫD
D−r≤ǫD
D−ℓ=ǫ
1−ℓ/D≤2ǫ≤2/3.
Since 1+ x≤ex−x2/3forx≤0.787, we can improve the estimate on Rℓ:
lnRℓ≤ǫD·lnD
D−ℓ−1
3ℓ−1/summationdisplay
r=0ǫ2D2
(D−r)2≤ǫD·lnD
D−ℓ−1
3ǫ2ℓ.
We can now prove Lemma 9.8.
Proof of Lemma 9.8. The ratio between/parenleftbig(1+ǫ)D
≤k/parenrightbig
and/parenleftbigD
≤k/parenrightbig
is clearly at most max( R0,...,R k),
whereRℓis the ratio between the binomials in Lemma 9.9.
If we only assume that D≥ℓ≥1 andǫ >0, then Lemma 9.9 states that
logRℓ≤ǫD·logD
D−ℓ,
which is clearly monotone increasing in ℓ. Therefore
logmax(R0,...,R k)≤ǫD·logD
D−k.
If we furthermore assume that k≤D/2 andǫ≤1/3, then Lemma 9.9 states that
logRℓ≤ǫD·logD
D−ℓ−1
3ǫ2ℓ.
The derivative of the upper bound with respect to ℓis
ǫD
D−ℓ−1
3ǫ2≥ǫ−1
3ǫ2>0,
sinceǫ≤1/3. Therefore the upper bound is maximized at ℓ=k, and we conclude that
logmax(R0,...,R k)≤ǫD·logD
D−k−1
3ǫ2k.
Sincek≤D/2, we can further estimate
logD
D−k= log1
1−k/D≤log2 = 1 .
389.3 Lower bounding M⋆(2,k)
We prove Theorem 9.2 by applying the lower bound of Theorem 8. 6. In the full version of this
paper [FHMM22], we also show how our techniques can be used to determine the exact value
ofM⋆(2,k). Concretely, we are able to identify the optimal shattered tree and to compute its
expected branch length.
Proof of Theorem 9.2. SinceL(U2) = 1, Theorem 8.6 implies that
M⋆(2,k) =RLk(U2) =k+Ω(k).
On the other hand, it is easy to see that D(2,k) = 2k+1. Indeed, if d≥2k+1 then
log2+log/parenleftbiggd
≤k/parenrightbigg
≤log2+log2d−1=d,
with equality if and only if d= 2k+1. Therefore D(2,k) = 2k+1.
10 Open Questions
Our work naturally raises many directions for future resear ch (more can be found in the full
version paper [FHMM22]).
Adaptive algorithms. Algorithm WeightedRandSOA gives the optimal mistake bound, but
requires knowledge of k. Theorem 8.7 gives an algorithm which doesn’t require knowl edge ofk,
and has a regret bound of ˜O(/radicalbig
M⋆(H,k)·logk) (this is the loss beyond M⋆(H,k)). What is the
optimal regret bound?
Quantitative bounds. The ﬁrst part in Theorem 2.6 asserts that M⋆(n,k)≤1
2D(n,k) +
O/parenleftBig/radicalbig
D(n,k)/parenrightBig
. It will be interesting to get quantitative bounds on the sec ond-order term in
terms ofM⋆
D(n,k). By the second part of Theorem 2.6 we know that in some cases ( n= 2) it is
Ω/parenleftbig/radicalbig
M⋆
D(n,k)/parenrightbig
. Does an upper bound of
M⋆(n,k)≤1
2M⋆
D(n,k)+O/parenleftbigg/radicalBig
M⋆
D(n,k)/parenrightbigg
hold for all n,k?
Inaddition, itwillbeinterestingtoﬁndexplicitboundson M⋆(n,k),M⋆
D(n,k); byTheorem2.5,
we know that when k≫lognthen14
M⋆(n,k) =k+Θ/parenleftBig/radicalbig
klogn/parenrightBig
.
How about for other values of n,k? Brˆ anzei and Peres [BP19] have the state-of-the-art bound s
in the regime k≪logn, but we are not aware of any results in other regimes, e.g. whe n
k= Θ(log n). As a matter of fact, to the best of our knowledge, even the le ading asymptotic
terms in the regime k= Θ(log n) were unknown prior to this work (we included them in the
full version of this work [FHMM22]).
14The upper bound is also given in [CBFH+97].
39Proper predictions and repeated game playing. Considerthepredictionusingtheexpert
advice problem, when the learner is restricted to predict wi th a convex combination of the
experts. That is, at the beginning of each round (before seei ng the advice of the nexperts),
the learner picks a convex combination of the experts and pre dicts accordingly. What is the
optimal expected number of mistakes in this game?15
The optimal algorithm for this problem was identiﬁed in [AWY 08]. We comment that this
game can also be presented in the language of game theory: ass ume a repeated zero-sum game
with 0/1 values, where each round is played as follows: player (i) ch ooses a (mixed) strategy
and reveals it to player (ii), who then replies with a strateg y of his own. What is the optimal
accumulated payoﬀ that player (i) can guarantee provided th at she has nstrategies and that
the sequence of strategies chosen by player (ii) is such that player (i) has a pure strategy that
loses to at most kof them? Proper predictions in the prediction with expert ad vice setting are
equivalent to mixed strategies here.
Acknowledgments
Shay Moran is a Robert J. Shillman Fellow; he acknowledges su pport by ISF grant 1225/20, by
BSF grant 2018385, by an Azrieli Faculty Fellowship, by Isra el PBC-VATAT, by the Technion
Center for Machine Learning and Intelligent Systems (MLIS) , and by the the European Union
(ERC, GENERALIZATION, 101039692). Views and opinions expr essed are however those of
the author(s) only and do not necessarily reﬂect those of the European Union or the European
Research Council Executive Agency. Neither the European Un ion nor the granting authority
can be held responsible for them.
References
[ABED+21] Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Mo ni Naor, and Eylon
Yogev. Adversarial laws of large numbers and optimal regret in online classiﬁca-
tion. InProceedings of the 53rd Annual ACM SIGACT Symposium on Theory of
Computing , pages 447–455, 2021.
[ABL+22] Noga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Shay Moran. Pri-
vate and online learnability are equivalent. J. ACM, 69(4):28:1–28:34, 2022.
doi:10.1145/3526074 .
[AL99] Peter Auer and Philip M Long. Structural results abou t on-line learning models
with and without queries. Machine Learning , 36(3):147–181, 1999.
[ALW06] Jacob Abernethy, John Langford, and Manfred K Warmu th. Continuous experts
and the binning algorithm. In International Conference on Computational Learn-
ing Theory , pages 544–558. Springer, 2006.
[AWY08] Jacob Abernethy, Manfred K Warmuth, and Joel Yellin . Optimal strategies from
random walks. In Proceedings of The 21st Annual Conference on Learning Theory ,
pages 437–446, 2008.
[BCD+22] Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, a nd Amir
Yehudayoﬀ. A characterization of multiclass learnability, 2022. URL:
https://arxiv.org/abs/2203.01550 .
15We answer this question only for the case k= 0 in the full version of this paper [FHMM22].
40[BDPSS09] Shai Ben-David, D´ avid P´ al, and Shai Shalev-Shw artz. Agnostic online learning.
InCOLT, 2009.
[BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussl er, and Manfred K War-
muth. Learnability and the Vapnik–Chervonenkis dimension .Journal of the ACM
(JACM), 36(4):929–965, 1989.
[BNS19] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Charact erizing the sample com-
plexity of pureprivate learners. Journal of Machine Learning Research , 20(146):1–
33, 2019. URL: http://jmlr.org/papers/v20/18-269.html .
[BP19] Simina Brˆ anzei and Yuval Peres. Online learning wit h an almost perfect expert.
Proceedings of the National Academy of Sciences , 116(13):5949–5954, 2019.
[CBFH+97] Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, Davi d P Helmbold, Robert E
Schapire, and Manfred K Warmuth. How to use expert advice. Journal of the
ACM (JACM) , 44(3):427–485, 1997.
[CBFHW96] Nicolo Cesa-Bianchi, Yoav Freund, David P Helmbo ld, and Manfred K Warmuth.
On-line prediction and conversion strategies. Machine Learning , 25(1):71–110,
1996.
[CBL06] Nicolo Cesa-Bianchi and G´ abor Lugosi. Prediction, learning, and games . Cam-
bridge university press, 2006.
[CHLL97] G´ erard Cohen, Iiro Honkala, Simon Litsyn, and Ant oine Lobstein. Covering codes .
Elsevier, 1997.
[Cov65] T. Cover. Behavior of sequential predictors of bina ry sequences. In Proc. of the
4th Prague Conference on Information Theory, Statistical Deci sion Functions and
Random Processes , pages 263–272. PublishingHouseoftheCzechoslovak Acade my
of Sciences, 1965.
[Doo53] J. L. Doob. Stochastic processes . John Wiley & Sons, Inc., New York; Chapman
& Hall, Ltd., London, 1953.
[DSS14] Amit Daniely and Shai Shalev-Shwartz. Optimal lear ners for multiclass problems.
InCOLT, pages 287–316, 2014.
[Fel17] Vitaly Feldman. A general characterization of the s tatistical query complexity.
In Satyen Kale and Ohad Shamir, editors, Proceedings of the 30th Conference
on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-1 0 July 2017 ,
volume 65 of Proceedings of Machine Learning Research , pages 785–830. PMLR,
2017. URL: http://proceedings.mlr.press/v65/feldman17c.html .
[FHMM22] Yuval Filmus, Steve Hanneke, Idan Mehalel, and Sha y Moran. Optimal prediction
using expert advice and randomized Littlestone dimension. Preprint, 2022. URL:
https://technionmail-my.sharepoint.com/:b:/g/person al/yuvalfi_technion_ac_il/Eb0yAGvdYnBIlZl2fT0DIVIB4 lFRF4MisFbcxYSa8sDXQQ?e=KHoiNS .
[FS97] Yoav Freund and Robert E Schapire. A decision-theore tic generalization of on-line
learning and an application to boosting. Journal of computer and system sciences ,
55(1):119–139, 1997.
41[FX15] Vitaly Feldman and David Xiao. Sample complexity bou nds on diﬀerentially
private learning via communication complexity. SIAM J. Comput. , 44(6):1740–
1764, 2015. doi:10.1137/140991844 .
[GPS16] Nick Gravin, Yuval Peres, and Balasubramanian Siva n. Towards optimal algo-
rithms for prediction with expert advice. In Proceedings of the twenty-seventh an-
nual ACM-SIAM symposium on Discrete algorithms , pages 528–547. SIAM, 2016.
[Han14] Steve Hanneke. Theory of disagreement-based activ e learning. Foun-
dations and Trends ®in Machine Learning , 7(2-3):131–309, 2014. URL:
http://dx.doi.org/10.1561/2200000037 ,doi:10.1561/2200000037 .
[Haz19] Elad Hazan. Introduction to online convex optimiza tion.arXiv preprint
arXiv:1909.05207 , 2019.
[HY15] Steve Hanneke and Liu Yang. Minimax analysis of ac-
tive learning. J. Mach. Learn. Res. , 16:3487–3602, 2015.
URL: https://dl.acm.org/doi/10.5555/2789272.2912111 ,
doi:10.5555/2789272.2912111 .
[KLMZ17] Daniel M. Kane, Shachar Lovett, Shay Moran, and Jia peng Zhang. Active clas-
siﬁcation with comparison queries. In Chris Umans, editor, 58th IEEE An-
nual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley,
CA, USA, October 15-17, 2017 , pages 355–366. IEEE Computer Society, 2017.
doi:10.1109/FOCS.2017.40 .
[KvE15] Wouter M. Koolen and Tim van Erven. Second-order qua ntile methods for experts
and combinatorial games. In Proceedings of the 28th Conference on Learning
Theory, 2015.
[Lit88] Nick Littlestone. Learning quickly when irrelevan t attributes abound: A new
linear-threshold algorithm. Machine learning , 2(4):285–318, 1988.
[LS14a] Haipeng Luo and Robert Schapire. Towards minimax on line learning with un-
known time horizon. In International Conference on Machine Learning , pages
226–234. PMLR, 2014.
[LS14b] Haipeng Luo and Robert E Schapire. A drifting-games analysis for online learning
and applications to boosting. Advances in Neural Information Processing Systems ,
27, 2014.
[LW94] Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm.
Information and computation , 108(2):212–261, 1994.
[MS10] Indraneel Mukherjee and Robert E Schapire. Learning with continuous experts
using drifting games. Theoretical Computer Science , 411(29-30):2670–2683, 2010.
[Nat89] BalasK.Natarajan. Onlearningsetsandfunctions. Machine Learning , 4(1):67–97,
1989.
[RS14] Alexander Rakhlin and Karthik Sridharan. Online non -parametric regression. In
Conference on Learning Theory , pages 1232–1264. PMLR, 2014.
42[RSS12] Alexander Rakhlin, Ohad Shamir, and Karthik Sridha ran. Relax and randomize:
From value to algorithms. Advances in Neural Information Processing Systems ,
25, 2012.
[Sha12] Shai Shalev-Shwartz. Online learning and online co nvex optimization. Found.
Trends Mach. Learn. , 4(2):107–194, 2012. doi:10.1561/2200000018 .
[SSBD14] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From
theory to algorithms . Cambridge university press, 2014.
[VC74] VladimirVapnik andAlexey Chervonenkis. Theory of Pattern Recognition . Nauka,
1974.
[Vov90] Volodimir G Vovk. Aggregating strategies. Proc. of Computational Learning
Theory, 1990 , 1990.
43