DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery
Chaofan Ma1, Yuhuan Yang1, Chen Ju1, Fei Zhang1, Jinxiang Liu1,
Yu Wang1, Ya Zhang1;2, Yanfeng Wang1;2B
1Coop. Medianet Innovation Center, Shanghai Jiao Tong University2Shanghai AI Laboratory
fchaofanma, yangyuhuan, ju chen, ferenas, jinxliu, yuwangsjtu, ya zhang, wangyanfeng g@sjtu.edu.cn
Pixel -level Object Discovery
A photo of {dog}
Pre-
trained
Diffusion
Figure 1: Left: Insight for extracting pixel-level object masks by leveraging the visual knowledge from pre-trained text-to-
image diffusion models. Right : Qualitative visualization for extensive synthetic data and corresponding object masks.
Abstract
Learning from a large corpus of data, pre-trained models
have achieved impressive progress nowadays. As popular
generative pre-training, diffusion models capture both low-
level visual knowledge and high-level semantic relations. In
this paper, we propose to exploit such knowledgeable diffu-
sion models for mainstream discriminative tasks, i.e., unsu-
pervised object discovery: saliency segmentation and ob-
ject localization. However, the challenges exist as there is
one structural difference between generative and discrimi-
native models, which limits the direct use. Besides, the lack
of explicitly labeled data signiÔ¨Åcantly limits performance in
unsupervised settings. To tackle these issues, we introduce
DiffusionSeg , one novel synthesis-exploitation framework
containing two-stage strategies. To alleviate data insufÔ¨Å-
ciency, we synthesize abundant images, and propose a novel
training-free AttentionCut to obtain masks in the Ô¨Årst syn-
thesis stage. In the second exploitation stage, to bridge the
structural gap, we use the inversion technique, to map the
given image back to diffusion features. These features can
be directly used by downstream architectures. Extensive ex-
periments and ablation studies demonstrate the superiority
of adapting diffusion for unsupervised object discovery.1. Introduction
To date in the literature, large-scale pre-trained models,
i.e., foundation models, have swept the CV domain for their
remarkable progress. One general trend is pre-training then
application, i.e., given a large corpus of data, Ô¨Årst optimizes
large-scale models to learn valuable prior knowledge about
practical scenarios; then extracts some speciÔ¨Åc knowledge
from pre-trained models for various downstream tasks.
SpeciÔ¨Åcally, existing foundation models can be grouped
into two branches, namely, discriminative ( e.g., MoCo [20],
DINO [6], CLIP [53]) and generative ( e.g., MAE [19], Dif-
fusion [68, 22]). The two branches have their own advan-
tages. Discriminative-based models are trained to align im-
ages within the same class or with corresponding captions,
thus they are aware of ‚Äúwhat‚Äù the object is, i.e., better at
high-level semantic tasks ,e.g., classiÔ¨Åcation and retrieval.
While generative-based models are trained to capture both
low-level visual knowledge (textures, edges, structures) and
high-level semantic relations, thus they are aware of ‚Äúwhat‚Äù
and ‚Äúwhere‚Äù the object is, i.e., better at pixel-level process-
ing tasks ,e.g., reconstruction and segmentation. In terms
of applications to downstream tasks, these two foundation
models have large gaps. Discriminative-based models have
been explored for both discriminative and generative tasks,
1arXiv:2303.09813v1  [cs.CV]  17 Mar 2023e.g., detection [87], segmentation [42], image synthesis [77]
and video understanding [27, 30]. However, since discrimi-
native pre-training focuses more on high-level semantics, it
is difÔ¨Åcult to deal with dense prediction tasks well. While
generative pre-training, with both low-level and high-level
visual knowledge, is now still stuck in the limited applica-
tions of low-level tasks, e.g., image generation [48], col-
orization [57], visual inpainting [13].
Hence, a novel question naturally raises: is generative-
based pre-training also or even more valuable for the main-
stream discriminative tasks? This paper makes a step to-
wards positively answering the question, i.e., we adopt pop-
ular diffusion models to solve object discovery, i.e., saliency
segmentation and object localization. The unsupervised set-
ting is explored to clearly evaluate the effectiveness.
To adapt pre-trained diffusion models for downstream
tasks, a vanilla idea is to directly use the feature inside the
model. However, it is infeasible, as there are considerable
gaps lying across diffusion models and discriminative ob-
ject discovery. (1) The structural difference between gener-
ative and discriminative models limits the direct transfer,
i.e., diffusion turns noise into random images, while ob-
ject discovery Ô¨Ånds masks from given images. (2) Lacking
explicitly labeled data signiÔ¨Åcantly limits training perfor-
mance of downstream tasks, especially for unsupervision.
In this paper, we design one novel synthesis-exploitation
framework, containing two-stage strategies to tackle the
above two issues respectively. SpeciÔ¨Åcally, the Ô¨Årst syn-
thesis stage is designed to tackle the issue of insufÔ¨Åcient la-
beled data. We propose novel training-free AttentionCut to
obtain masks during synthesizing sufÔ¨Åcient images. Images
are synthesized using text-to-image diffusion model with
random noise and category as inputs. Masks are generated
leveraging cross- and self- attention in this diffusion model.
As shown in Fig. 1, these synthetic images are realistic with
accurate mask, which is impressive and demonstrates the
quality. The second exploitation stage is proposed to bridge
the structural gap. We combine inversion technique with
diffusion models, to deterministically map the given im-
age back to diffusion features. This allows the diffusion
model to be regarded as a universal knowledge extractor,
which can be directly used by any downstream architecture.
Results show the strong capabilities of this knowledge and
training a lightweight decoder can unify the utilization of
diffusion pre-training and object discovery.
On six standard benchmarks, namely, ECSSD, DUTS,
DUT-OMRON for segmentation, while VOC07, VOC12,
COCO20K for detection, our method signiÔ¨Åcntly outper-
forms existing state-of-the-art methods. We also conduct
extensive ablation studies to reveal the effectiveness of each
component, both quantitatively and qualitatively.
To sum up, our contributions lie three fold:
We pioneer the early exploration in adapting free pixel-level knowledge from pre-trained diffusion models to facil-
itate unsupervised object discovery;
We design a novel synthesis-exploitation framework
that explicitly extracts knowledge through data synthesis
and leverages implicit knowledge by diffusion inversion;
We conduct extensive experiments and ablations to re-
veal the signiÔ¨Åcance of adapting diffusion knowledge and
our superior performance on six public benchmarks.
2. Related Work
Generative Models can roughly be classiÔ¨Åed into two main
branches: GANs and diffusions. As the early representa-
tives, GANs [18, 46, 25, 91, 32, 4, 33] have the advan-
tage of generating realistic and diverse data that are simi-
lar to the original. They can also learn complex and high-
dimensional distributions without explicit density estima-
tion. Such properties allow GANs to enjoy success in image
generation [32], image-to-image translation [91, 24, 25].
However, they are hard to train stably. Without careful tun-
ing of hyperparameters, they usually suffer from mode col-
lapse, i.e., only generating a few modes of data distribution.
In contrast, diffusion models [68, 22, 70, 71, 11, 49, 69]
have recently broken the long-term dominance of GANs
and raised the bar for generative modeling. Compared
with GANs, they are friendly for using, without need for
adversarial training. Besides, they also achieve state-of-
the-art image quality and Ô¨Ådelity on various datasets [11].
BeneÔ¨Åted from such advantages, diffusion models have
been applied to various generative tasks, such as text-to-
image generation [48, 54, 58, 55], colorization [57], super-
resolution [59], inpainting [13, 41], and semantic edit-
ing [9, 45].
Nevertheless, all above methods focus on preliminary
generation tasks. In this paper, we explore the signiÔ¨Åcance
of generative pre-training models for discriminative tasks.
The insight is that generative models are pre-trained to con-
tain both low-level knowledge and high-level semantic re-
lations. Among all generative models, we choose diffusion
models as representatives, for their impressive performance.
Object Discovery aims at detecting and segmenting salient
objects in the natural scenes, consisting of two popular sub-
tasks: saliency segmentation and object localization. Ex-
isting methods have two settings: supervised and unsuper-
vised. The supervised methods [23, 90, 52, 86] are trained
with large-scale pixel-level human annotations, which are
time-consuming and expensive to acquire.
By contrast, the unsupervised setting without any labor
labels, has received increasing attentions. Concretely, most
methods embrace discriminative-based pre-trained models
for help. LOST [66], Deep Spectral [44], and TokenCut [80]
leverage features from self-supervised ViTs [6] with con-
trastive learning [51, 26, 28, 29] that exhibit object segmen-
2Noise ùëßùëá~
Diffusion
Model
Up
sample
Inner CoherenceSynthetic 
Dataùê¥ùëê‚àà‚Ñù‚Ñé√óùë§
ùê¥ùë†‚àà‚Ñù‚Ñé√óùë§√ó‚Ñé√óùë§
ObjectnessClass {bird}
Self AttnFFN
Res BlockCross AttnBlock
Up
sample
Indexed from ùê¥ùë†
‚Ä¶
 ùëü(ùëù)
AttentionCutMask Generation
Diffusion 
Inversion
CLIP
input image
Class
{person}
CLIP-Classifiable PriorBlock Block Block Block
Diffusion Model
Noise
1  Synthesis Stage 2  Exploitation Stage
predicted
mask√ó2 √ó2 √ó2
Segment Decodersemantic coherence (self -attn)
spatial coherence (geodesic dist)cross -attnconfidence maps
Figure 2: We use pre-trained diffusion models to synthesize extensive data, helping the model training, then evaluated on real
images. (1) The Synthesis Stage. We synthesize free image-mask pairs, where mask generation is solved leveraging cross-
and self- attention by AttentionCut. (2) The Exploitation Stage. We extract knowledge using diffusion inversion, then only
one lightweight decoder is trained for object discovery on synthetic data.
tation potential, after which a heuristic strategy or a graph-
based method [63] is employed. SelfMask [65] revisits
the spectral clustering on image features from various self-
supervised models, e.g., MoCo [20], SwA V [5], DINO [6],
to obtain pseudo-labels, which are then used to train one
salient object detector. FreeSOLO [79] proposes to gener-
ate correlation maps which are then ranked and Ô¨Åltered by
maskness scores. DINOSAUR [61] reconstructs features
from self-supervised models for object-centric representa-
tions. Different with all above methods only using discrim-
inative pre-training, this paper proves that generative-based
pre-training is also or even more valuable for mainstream
discriminative tasks, by a synthesis-exploitation strategy.
3. Methods
This paper aims to utilize pre-trained diffusion genera-
tion models for downstream tasks by proposing a two-stage
synthesis-exploitation framework. In Sec. 3.1, we start by
describing the preliminary. In Sec. 3.2, we detail the syn-
thesis stage to generate sufÔ¨Åcient labeled data. In Sec. 3.3,
we detail the exploitation stage to close the structural gap
between generative models and discriminative tasks.
3.1. Preliminary and Overview
Problem DeÔ¨Ånition. Object Discovery (OD), i.e., saliency
segmentation and object localization, as a fundamental and
typical discriminative task, is studied in this paper. Con-
cretely, object discovery aims to train one pixel-level seg-
mentation model ODthat partitions one image Iinto twodisjoint groups, namely, foreground and background.
Mseg=  OD(I)2f0;1gHW1;I2RHW3;(1)
whereMsegrefers to the binary segmentation mask.
Here, to clearly evaluate the effectiveness of our method,
we focus on the strict unsupervised setting, i.e., the model
is trained without any manually annotated data.
Motivation. This paper aims to exploit pixel-level visual
knowledge from pre-trained diffusion generation models,
for downstream discriminative tasks, e.g., OD. To achieve
this goal, we design a novel synthesis-exploitation frame-
work (Fig. 2). SpeciÔ¨Åcally, at the synthesis stage, we ex-
plicitly construct one free (inÔ¨Ånite-size) discriminative syn-
thetic dataset, to obtain sufÔ¨Åcient labeled samples. At the
exploitation stage, we enable diffusion to be compatible
with OD tasks, by extracting implicit diffusion features, and
training one discovery decoder with the synthetic dataset.
Diffusion [68, 22] is one recently popular generative idea,
containing forward and reverse processes. The forward pro-
cessis a Markov chain where noise is gradually added to the
data. The reverse process is a denoising procedure that can
be decomposed into a linear combination of a noisy image
xtand a noise approximator ().t= 1;:::;T refers to
the denoising timesteps. The key to diffusion models is to
learn the function (), typically using a UNet [56].
Particularly, we build on a variant of the text-to-image
diffusion model, namely, Stable Diffusion [55]. During
the synthesis process, it‚Äôs sampled by iteratively denois-
ingxtconditioned on the input text prompt yfor timestep
t= 1;:::;T . The conditional denoising UNet (xt;t;y)
3stacks layers of self- and cross-attentions. yis Ô¨Årst encoded
to text embeddings by a pre-trained text encoder, then text
embeddings are mapped to intermediate layers as Kand
Vvia the attention mechanism, and the noisy image xtis
mapped asQ. For steptand layerl, we call cross-attention
asAt;l
c, self-attention asAt;l
s, and intermediate features as
Ft;l.Note that , this paper freezes Stable Diffusion pre-
trained on LAION-5B [60] (5 billion image-text pairs), as
a knowledge provider. This diffusion model involves both
low-level object details and high-level class semantics, en-
abling us to achieve unsupervised object discovery.
3.2. Synthesis Stage: Free Data Generation
As illustrated in Fig. 2 (1), this stage aims to synthesize
large and free image-mask pairs through Stable Diffusion,
solving the lack of labeled training data under unsupervised
settings. We detail image synthesis in Sec. 3.2.1, and mask
generation in Sec. 3.2.2.
3.2.1 Image Generation
For one pre-trained text-to-image Stable Diffusion [55], we
here freeze it, then generate images through inputting ran-
dom Gaussian noise and class text prompts. Class names
are sampled from ImageNet [10].
For text input, a simple way is to simply use class names,
but this may limit diversity and cause bottlenecks for down-
stream tasks. Hence, to adaptively generate various text
prompts for each class, we interact with ChatGPT [50].
For example, we ask ChatGPT to list prompts about ‚Äúaero-
plane‚Äù, then it could give some generative-style prompts
like: ‚Äú A aeroplane soaring through a vibrant sunset sky,
Ô¨Çuffy clouds, warm lighting, viewed from a low angle, real-
istic style. ‚Äù The generated prompts introduce richer context,
thus can better unleash the potential of the Stable Diffusion
to synthesis high-Ô¨Ådelity, more diverse images. One noise
reduction strategy is also applied following [21].
3.2.2 Mask Generation
Here, we generate high-quality masks by leveraging atten-
tions in pre-trained diffusion models as clues, following two
non-trivial observations. (1) Cross-attention Acindicates
locality between the conditioning text and noisy image, thus
Accan coarsely describe objectness . (2) Self-attention As
inside one image indicates pairwise semantic similarity be-
tween pixels, thus Ascould roughly describe coherence .
Inspired by these, we propose AttentionCut, a training-free
strategy to generate masks guided by attention maps.
Preparations. We Ô¨Årst extractAcandAsat the position
of category token in the prompt sentence, then aggregate
different resolutions and timesteps considering multi-scale
objects and avoiding focus shift during diffusion. Formally,Ac=1
kTkX
l=1T 1X
t=0At;l
c;As=1
LTLX
l=1T 1X
t=0At;l
s;(2)
wheret=T 1;:::; 0is for each reverse step and l=
1;:::;L is for intermediate layers. Acis averaged among
the top-kof the standard variation from all Al
c, whileAsis
averaged among all layers and time steps.
Objectness. Intuitively, the pixel-level cross-attention Ac
under a speciÔ¨Åc category can roughly be seen as segmenta-
tion masks, as it indicates how likely a pixel belongs to the
category. However, in practice we found Acis sparse and
inattentive near the boundary, which can seriously damage
segmentation results. To handle this issue, we improve Ac
by strengthening the edge area with the self-attention As. It
indicates semantic connectivity, i.e., how semantically two
pixels belong to one group. SpeciÔ¨Åcally, we Ô¨Årst randomly
select a set of initial seeds Bfrom the boundary of the binary
mask [Ac>]. Then each selected seed b2B can expand
as a conÔ¨Ådence map As(b;), which is the self-attention be-
tweenband other pixels, indicating weights of the boundary
area. We assumeAs(;b) =As(b;), asAsis symmetric
theoretically. For pixel p, these maps are averaged as a re-
Ô¨Åned mapr(p), to reinforce the boundary pixels:
r(p) = 1=jBjX
b2BAs(p;b): (3)
Combining cross-attention Acand the reÔ¨Åned map r(p)
with a balance weight , the pixel-level objectness are:
(p) =(
 log(Ac(p) +r(p));ifp2foreground;
log(1 Ac(p) r(p));ifp2background;
(4)whereAc(p)is the cross-attention at pixel p.
Inner Coherence. With only objectness, we found that the
masks tend to lose local information, for example, irregular
corners, mis-segmented holes, or jagged contours. This can
be solved by taking local consistency into account, i.e., how
likely two neighboring pixels belong to one group. Here we
design an inner coherence term that can help to enforce con-
tinuity, proximity and smoothness of segments belonging to
the same object, and penalize those who deviate.
The proposed inner coherence consists of two parts: se-
mantic and spatial. As mentioned above, Ascan indicate
semantic coherence, as self-attention is calculated in se-
mantic feature space. Spatial coherence is designed to in-
dicate pixels pairwise distance in both RGB and Euclidian
space. This coherence is obtained by absorbing the form of
geodesic distance on the surface of image intensity, then by
negative exponential transformation. The inner coherence
 can be formalized as:
 (p;q) =As(p;q) + e D(p;q);
D(p;q) = min
PZ1
0krI(P(s))v(s)kds;(5)
4where for pixel pandq,As(p;q)is the self-attention and
D(p;q)is the geodesic distance; Pis an arbitrary path from
ptoqparameterized by s2[0;1];v(s)denotes the unit
vectorP0(s)=kP0(s)kthat is tangent to the path direction;
I()is image RGB intensity.
Calculating Mask. Given objectness and inner coherence,
we deÔ¨Åne an energy function Efor each potential mask M:
E(M) =X
p(p) +X
M(p)6=M(q) (p;q);(6)
wheredenotes the weight between and ;M()2
f0;1gmeans the pixel in this mask. The binary mask Mis
generated by minimizing E(M),i.e., use Ford-Fulkerson
algorithm [17] to Ô¨Ånd a minimum cut in the image graph.
And after further post-processing and denoising [1, 89, 37],
we can obtain the Ô¨Ånal synthetic mask (see Fig. 1 Right for
some examples).
Discussion. Compared with other training-free mask gener-
ation methods like NCut [63] and K-means [40], they only
consider pairwise similarly, thus cannot decide fore/back-
ground for each partition. Compared with DenseCRF [36],
AttentionCut has well-designed objectness and inner coher-
ence terms, which is more suitable for diffusion models and
guarantees convergence. In Tab. 5, we have conducted ex-
periments to validate the superiority of AttentionCut.
3.3. Exploitation Stage: Diffusion Knowledge
This stage aims to bridge the architectural gap between
pre-trained diffusion models and discriminative tasks, e.g.,
object discovery. As shown in Fig. 2 (2), we achieve this in
two steps: in Sec. 3.3.1, we treat diffusion models as a uni-
versal feature extractor to distill explicit visual knowledge;
in Sec. 3.3.2, we feed diffusion features into one Ô¨Çexible
decoder, and train with ‚ÄúinÔ¨Ånite‚Äù synthetic data.
3.3.1 Extracting Diffusion Knowledge
For diffusion models, they are fed with noise and text to
output synthesis images; while for object discovery models,
they are fed with images to output pixel-level masks. Such
an architectural gap blocks direct feature extraction from
diffusion. To solve this, given one image, we are required
to Ô¨Ånd the corresponding input noise of diffusion models
under some conditioning text, then features can be extracted
through diffusion reverse process. To get input noise, we
combine diffusion inversion [69] with the conditional UNet.
To get the conditioning text, we simply classify images by
CLIP [53].
Diffusion Inversion and Feature Extraction. Given pre-
trained diffusion models, we here inverse one image back
to its corresponding noise under the conditioning text. This
diffusion inversion can be seen as a special forward process.One trivial solution is to use the typical DDPM [22]. Al-
though it can yield latent variables ( i.e., noise) through the
forward process, these variables are stochastic and cannot
reconstruct the image through the reverse process. So it is
not suitable for feature extraction. Inspired by DDIM [69],
we modify each step by combining it with conditional
denoising UNet (xt;t;y)in Stable Diffusion, making
the forward/reverse non-Markovian to enjoy deterministic.
Now the forward/reverse process for each step is:
xt+1=pt+1f(xt;t;y) +p
1 t+1(xt;t;y);
xt 1=pt 1f(xt;t;y) +p
1 t 1(xt;t;y);(7)
wheref(xt;t;y) = 
xt p1 t(xt;t;y)
=pt,
t= 1 t,t=Qt
s=1(1 s),tis a variance schedule.
ydenotes the conditional text, and tmeans timesteps.
After diffusion inversion, to get the corresponding noise,
featuresFt;lcan be extracted from (xt;t;y)during each
reverse step t=T 1;:::; 0and intermediate layer l=
1;:::;L . To cover long range and multi-level features of
multi-scale objects, they are aggregated in all time steps:
Fl= 1=TXT 1
t=0Ft;l: (8)
In practice, we choose the output of the ‚ÄúSpatialTrans-
former‚Äù block in Stable Diffusion, where L= 6with reso-
lutions 1616,3232, and 6464, two of each.
CLIP-classiÔ¨Åable Prior. Notice that in Eq. (7), the diffu-
sion inversion should be done under some conditional text
y. We choose yto be the CLIP-classiÔ¨Åed category of the
input image, because of the following observations: (1) hu-
mans take pictures by naturally framing an object of in-
terest near the center of the image [31] (center prior); (2)
most background regions can be easily connected to image
boundaries, while difÔ¨Åcult for object regions [82] (back-
ground prior); (3) CLIP is pre-trained on a large corpus of
web-curated data, and most of which is human-token im-
ages with saliency objects [53] (source prior). It is easy to
classify images with the center and background priors, and
the source prior enables us to classify using CLIP [53]. We
summarize this as CLIP-classiÔ¨Åable prior .
In practice, we choose the label set in ImageNet [10], and
combine semantically similar classes, e.g., poodle and Chi-
huahua as dogs, etc. Besides, multiple prompt templates are
used, e.g., ‚ÄúA photo offcategoryg‚Äù to boost performance.
3.3.2 Segment Decoder
To enable diffusion compatible with object discovery, we
here propose two options for preference. One is to attach
a Ô¨Çexible decoder to the pre-trained diffusion models, and
train using the synthesised data to achieve object discovery.
This option costs many parameters and rich training data,
bringing superior performance, and we denote it as Diffu-
sionSeg in Tab. 1. The other is to extract cross- and self-
5ModelDUT-OMRON [85] DUTS-TE [78] ECSSD [64]
Acc"IoU"maxF"Acc"IoU"maxF"Acc"IoU"maxF"
HS [84] .843 .433 .561 .826 .369 .504 .847 .508 .673
wCtr [92] .838 .416 .541 .835 .392 .522 .862 .517 .684
WSC [38] .865 .387 .523 .862 .384 .528 .852 .498 .683
DeepUSPS [47] .779 .305 .414 .773 .305 .425 .795 .440 .584
BigBiGAN [75] .856 .453 .549 .878 .498 .608 .899 .672 .782
E-BigBiGAN [75] .860 .464 .563 .882 .511 .624 .906 .684 .797
Melas-Kyriazi et al. [43] .883 .509 - .893 .528 - .915 .713 -
LOST [66] .797 .410 .473 .871 .518 .611 .895 .654 .758
Deep Spectral [44] - .567 - - .514 - - .733 -
TokenCut [80] .880 .533 .600 .903 .576 .672 .918 .712 .803
FreeSOLO [79] .909 .560 .684 .924 .613 .750 .917 .703 .858
SelfMask (pseudo) [65] .811 .403 - .845 .466 - .893 .646 -
SelfMask [65] .901 .582 .680 .923 .626 .750 .944 .781 .889
FOUND-single [67] .920 .586 .683 .993 .637 .733 .912 .793 .946
FOUND-multi [67] .912 .578 .663 .938 .645 .715 .949 .807 .955
LOST [66] +BS .818 .489 .578 .887 .572 .697 .916 .723 .837
TokenCut [80] +BS .897 .618 .697 .914 .624 .755 .934 .772 .874
SelfMask [65] +BS .919 .655 .771 .933 .660 .819 .955 .818 .911
FOUND-single [67] +BS .921 .608 .706 .941 .654 .733 .912 .793 .946
FOUND-multi [67] +BS .922 .613 .708 .942 .663 .763 .951 .813 .935
AttentionCut .905 .536 - .914 .608 - .924 .710 -
DiffusionSeg .948 .661 .772 .959 .704 .829 .964 .831 .955
(a)Comparisons for unsupervised saliency segmentation on three standard
benchmarks DUT-OMRON [85], DUTS [78] and ECSSD [64]. +BS means
Bilateral Solver [1] for post-processing. The max Fon SelfMask has been re-
evaluated for fair comparisons, as we found SelfMask computed max Fwith
various optimal thresholds, while other methods only use one uniÔ¨Åed threshold.Method VOC07 [14] VOC12 [15] COCO20K [39, 73]
Selective Search [72, 66] 18.8 20.9 16.0
EdgeBoxes [93, 66] 31.1 31.6 28.8
Kim et al. [34, 66] 43.9 46.4 35.1
Zhange et al. [88, 66] 46.2 50.5 34.8
DDT+ [81, 66] 50.2 53.1 38.2
rOSD [73, 66] 54.5 55.3 48.5
LOD [74, 66] 53.6 55.1 48.5
DINO-seg [66] 45.8 46.2 42.1
FreeSOLO [79] 56.1 56.7 52.8
LOST [66] 61.9 64.0 50.7
Deep Spectral [44] 62.7 66.4 52.2
TokenCut [80] 68.8 72.1 58.8
AttentionCut 67.5 70.2 54.9
DiffusionSeg 75.2 78.3 63.1
(b)Single object localization. We extract the tight bound-
ing box for saliency mask as our box prediction.
Model Acc IoU max F
PertGAN [3] - .380 -
ReDO [8] .845 .426 -
OneGAN [2] - .555 -
Melas-Kyriazi [43] .921 .664 .783
BigBiGAN [75] .930 .683 .794
E-BigBiGAN [75] .940 .710 .834
AttentionCut .946 .695 .838
DiffusionSeg .963 .726 .852
(c)Compare with GAN-based methods on CUB [76] .
Table 1: Comparison with state-of-the-art methods on object discovery. Our DiffusionSeg outperforms previous state-of-
the-art approaches across all benchmarks.
attention during diffusion inversion, and generate pseudo-
masks using AttentionCut in Sec. 3.2.2. Such an option
costs no trainable parameters and data, thus showing faster
inference speeds, and we call it AttentionCut in Tab. 1.
3.4. Discussion
This paper uses pre-trained diffusion models for unsu-
pervised object discovery. Comparing with discriminative
pre-training [66, 80, 65], generative pre-training has addi-
tional pixel-level understanding, which is more suitable for
object discovery. Compared with MAE-style [19] gener-
ative pre-training, which learns reconstruction representa-
tions to help object discovery, diffusion models show a clear
advantage, i.e., synthesis abundant data, which is valuable
to improve performance (see Tab. 3 and Fig. 6). Comparing
with GANs in image synthesizing, diffusion models have
signiÔ¨Åcant advantages in higher sample quality and diver-
sity, more stability and robustness [11]. Compared to a few
early GAN-based works that struggle to synthesise mask
with manual annotations [89, 37], diffusion model can ob-
tain mask using AttentionCut, without manually labeling.
4. Experiments
4.1. Experimental Setup
Datasets & Evaluations. For unsupervised saliency seg-
mentation, we evaluate on three standard benchmarks: EC-
SSD [64], DUTS [78] and DUT-OMRON [85]. We also useCUB [76] to compare with some generative-based segmen-
tation models [8, 75, 43]. For metrics, we report pixel-wise
accuracy (Acc), intersection-over-union (IoU), and max F
for2to0:3following conventions [75, 43, 80, 65].
For unsupervised single object localization, we evaluate
on VOC07 [14], VOC12 [15] and COCO20K [39, 73]. We
evaluate using correct localization (CorLoc) [80], i.e., the
percentage of images, where the IoU >0:5of a predicted
single bounding box with at least one of the ground truth.
Implementation Details. We adopt the publicly released
sd-v1-4.ckpt of Stable Diffusion1for image genera-
tion, and it remains frozen throughout. We set image reso-
lution 512512, timestepsT= 40 , channel num C= 4,
sample frequency f= 8 and ddim eta is 0:0. For At-
tentionCut, we set = 0:16, = 2:5and= 0:1.
Our synthetic dataset contains about 50;000 image-mask
pairs. We use a three-layer FCN as segment decoder, and
set lr = 0:001on Adam [35] for optimization. The batch
size is set to 10.
4.2. Comparison with the State-of-the-art
Unsupervised Saliency Segmentation. Tab. 1a com-
pares for unsupervised object segmentation. DiffusionSeg
reached a new SOTA, and largely improves AttentionCut by
12.5%, 9.6%, 12.1% in IoU after training on our synthetic
dataset, which proves the value of synthesis data.
1https://huggingface.co/CompVis/stable-diffusion-v-1-4-original
620 40 60 80 100
stdprobabilityours
DUTS-TRFigure 3: Color contrast.
Our synthetic data shows
a similar distribution of
color contrast with real-
world dataset DUTS-TR.
0.0 0.2 0.4 0.6 0.8 1.0
Object sizeprobabilityours
DUTS-TRFigure 4: Object size.
Our synthetic data has a
broader scale of salient
objects (object sizes rang-
ing from 0:1to0:5).
0.2 0.4 0.6 0.80.20.40.60.8
Ours
0.2 0.4 0.6 0.80.20.40.60.8
DUTS-TRFigure 5: Center bias scatter plot for
our synthetic dataset (left) and DUTS-TR
(right). DUTS-TR is object-centric, while our
dataset is more diverse in center distribution
and contains more hard samples.DUTS-TR Ours
SC 27.1 29.9
PL 2.96 2.78
SD 2.31 1.91
Table 2: Geometry
statistics, in terms of
shape complexity (SC),
polygon length (PL)
and shape diversity
(SD).
Besides, we also compared with some GAN-based unsu-
pervised object segmentation methods on CUB benchmark,
as shown in Tab. 1c. Our DiffusionSeg, utilizing diffusion
model, can largely outperform all GAN-based method even
without the need of training on the synthetic data.
Unsupervised Single Object Localization. Given the pre-
dicted segmentation mask from our model, we convert it
to a bounding box by Ô¨Årst connecting components, then
choosing the tight outline of the largest components from
the top, bottom, left and right sides. As shown in Tab. 1b,
our method reaches new state-of-the-art on all three bench-
marks.
4.3. Synthesized Data Analysis
This section provides a thorough analysis of our syn-
thesized dataset. The results show that with sufÔ¨Åcient data
scale, our dataset is a reliable simulation of the real world.
4.3.1 Data Statistics
We compare our synthetic dataset to a real dataset DUTS-
TR [78]. For most key properties, statistics show that our
synthetic dataset has a similar distribution to DUTS-TR.
Color Contrast. As shown in Fig. 3, our dataset has almost
the same distribution of color contrast as DUTS-TR, which
can make the model easy to transfer to the real world.
Object Size. DeÔ¨Åning object size as the ratio of foreground
pixels to full image pixels, Fig. 4 shows the comparison.
Compared to DUTS-TR, our synthetic data has a broader
scale of salient objects (object sizes ranging from 0:1to
0:5), which is suitable for training the object discovery task.
Center Bias. Fig. 5 draws the scatter plot for each object
using bounding box centers. In comparison with object-
centric DUTS-TR, a more diverse center distribution con-
tains more hard samples, and can improve generalizability
of the model.
Geometry Statistics. Tab. 2 shows shape complexity (SC),
polygon length (PL) and shape diversity (SD) of our dataset,which are close with real DUTS-TR. Following [37], we
convert masks into polygons and deÔ¨Åne SC as vertice num-
ber, PL as perimeter. SD is deÔ¨Åned as averaging pairwise
Chamfer distance between two polygons.
4.3.2 Training Performance
We train two typical segmentation pipeline on different
scale of synthetic dataset as well as real ones. Synthetic
data is a replacement for real data with sufÔ¨Åcient scale.
Compared with Real Dataset. Ideally, a well-established
dataset should be capable of training on arbitrary architec-
tures. To reveal such ability, we compare performances of
training on our synthetic dataset and DUTS-TR. SpeciÔ¨Å-
cally, we deal with saliency segmentation in an end-to-end
manner. We select two widely used segmentation archi-
tectures, UNet [56] and DeepLabV3 [7] as representatives.
Note that , training on real data makes use of both image
andground truth annotations, which can be seen as fully
supervised in this scenario.
Model DatasetDUT-OMRON DUTS-TE ECSSD
Acc IoU Acc IoU Acc IoU
UNet [56]Real 1k .853 .471 .875 .489 .912 .678
Syn 1k .841 .419 .846 .401 .853 .615
Syn 10k .864 .475 .872 .493 .908 .681
DeepLabV3 [7]Real 1k .910 .565 .900 .512 .899 .670
Syn 1k .878 .479 .866 .454 .862 .612
Syn 10k .916 .573 .906 .521 .904 .669
Table 3: Performance of segmentation models trained on
synthetic and real dataset. Real 1k refers to 1000 ran-
dom selected images from DUTS-TR. It shows synthetic
data can replace real data with sufÔ¨Åcient samples provided.
Tab. 3 shows experimental results. The gap between syn-
thetic and real data can be observed on the same data scale.
However, performance could be boosted by adding more
synthetic data. With an increase of 10in scale, the model
is trained to be comparable with that on real data. Consid-
ering the inÔ¨Ånite generating capability of diffusion models,
71k 20k 40k 60k 80k 100k
data scale.575.600.625.650.675700
IoU
(50k,.704)Figure 6: Results of increas-
ing the data scale from 1k
to100kon DUTS-TE. The
data scale of 50kis Ô¨Ånally
used, since it offers a goodish
trade-off between synthetic
cost and performance at the
same time.
we come to the conclusion that our synthetic data is a viable
alternative to real ones.
Data Scale. Fig. 6 answers for ‚Äúhow much synthetic data
is enough for training?‚Äù We increase the data scale from
1kto100kand report IoU on DUTS-TE, and Ô¨Ånd there‚Äôs
a decrease marginal effect as scale increases. We keep the
scale to 50k, for its good trade-off between synthesizing
cost and performance.
4.4. Diffusion Features Analysis
Model DatasetDUT-OMRON DUTS-TE ECSSD
Acc IoU Acc IoU Acc IoU
DINO [6]
Syn 5k.897 .528 .883 .502 .869 .701
MoCo [20] .882 .534 .912 .596 .915 .699
CLIP [53] .897 .523 .924 .563 .920 .728
Ours .895 .532 .916 .594 .923 .730
DINO [6]
Syn 10k.930 .589 .919 .576 .907 .750
MoCo [20] .925 .586 .915 .623 .930 .766
CLIP [53] .931 .592 .913 .628 .933 .771
Ours .932 .598 .933 .637 .952 .790
Table 4: Comparing diffusion models (Ours) with other
pre-trained models. Diffusion features show privilege.
To show the superiority of pre-trained diffusion features,
we compare them with some discriminative pre-training
methods (DINO [6], MoCo [20], CLIP [53]). During train-
ing, we freeze all pre-trained models as feature extractor,
attaching a same segment decoder for mask prediction.
Diffusion Pre-training vs.Discriminative Pre-training.
Tab. 4 shows the privilege of diffusion features, compared
with discriminative ones. Noticing that pixel-wise recon-
struction is the most informative pre-training task among
the three, it‚Äôs not surprising to have such results. Although
both diffusion and CLIP are pre-trained using text-image
pairs, discriminative pre-training like image-caption align-
ment focuses mainly on global features and loses detail.
4.5. Ablation Study
Mask Generation Methods. Besides our AttentionCut,
Tab. 5 also compares with other training-free segmentation
methods. They are usually used in RGB space. Here, weapply them to diffusion features with minor modiÔ¨Åcations.
Overall, AttentionCut far outperforms these methods.
MethodDUT-OMRON DUTS-TE ECSSD
Acc IoU Acc IoU Acc IoU
K-means clustering [40] .802 .413 .834 .462 .885 .628
DenseCRF [36] .872 .497 .883 .522 .902 .692
NCut [63] .860 .503 .872 .528 .899 .690
AttentionCut .905 .536 .914 .608 .924 .710
Table 5: Comparisons of training-free mask generation
methods. All are conducted in diffusion feature space.
Model(p) (p;q) DUT-OMRON DUTS-TE ECSSD
Acr(p)AsD Acc IoU Acc IoU Acc IoU
¬¨ X - - - .881 .480 .902 .535 .903 .637
¬≠ -X - - .885 .486 .896 .533 .915 .691
¬Æ X X - - .892 .502 .910 .561 .905 .643
¬Ø X X -X .896 .512 .910 .582 .919 .699
¬∞ X X X - .894 .508 .909 .579 .912 .657
¬± X X X X .905 .536 .914 .608 .924 .710
Table 6: Ablation on the components of AttentionCut. D
means spatial coherence in Eq. 5.
AttentionCut Components. Tab. 6 ablates the Attention-
Cut formulation to show their effectiveness. Acalone (¬¨)
provides a reasonable mask prediction and can be enhanced
byr(p)(¬≠,¬Æ). Both semantic and spatial coherence im-
prove results ( ¬Ø,¬∞). The beneÔ¨Åts of two coherences are
addable, and combining them all performs best ( ¬±).
Effectiveness of CLIP-classiÔ¨Åable Prior. Tab. 7 ablates
MethodDUT-OMRON DUTS-TE ECSSD
Acc IoU Acc IoU Acc IoU
AttentionCut ( w/ prior) .905 .536 .914 .608 .924 .710
AttentionCut ( w/o prior) .831 .392 .816 .329 .851 .466
DiffusionSeg ( w/ prior) .948 .661 .959 .704 .964 .831
DiffusionSeg ( w/o prior) .929 .628 .943 .672 .952 .801
Table 7: Ablation on CLIP-classiÔ¨Åcation prior. w/o prior
means using empty string in place of category label.
the CLIP-classiÔ¨Åable prior. Under w/o prior setting, the
category label is replaced by an empty string. It shows At-
tentionCut heavily relies on this prior, but DiffusionSeg is
robust. As AttentionCut is based on attention maps, it is
sensitive to the given label. However, in DiffusionSeg, the
network is trained, potentially having the ability to under-
stand diffusion features without CLIP.
5. Conclusion
Diffusion model has shown remarkable success on gen-
erative tasks. In this paper, we propose DiffusionSeg to fur-
ther explore its ability on discriminative tasks. We build
8a synthetic dataset using AttentionCut to generate image-
mask pairs, and use diffusion inversion to exploit diffusion
features for training a segment decoder. Our DiffusionSeg
shows privilege and achieves new SOTA on all benchmarks.
We expect our work to make a positive step towards unify-
ing generative and discriminative tasks in one model.
References
[1] Jonathan T Barron and Ben Poole. The fast bilateral solver.
InEur. Conf. Comput. Vis. , pages 617‚Äì632. Springer, 2016.
5, 6
[2] Yaniv Benny and Lior Wolf. Onegan: Simultaneous un-
supervised learning of conditional image generation, fore-
ground segmentation, and Ô¨Åne-grained clustering. In Eur.
Conf. Comput. Vis. , pages 514‚Äì530. Springer, 2020. 6
[3] Adam Bielski and Paolo Favaro. Emergence of object seg-
mentation in perturbed generative models. Adv. Neural In-
form. Process. Syst. , 32, 2019. 6
[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high Ô¨Ådelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018. 2
[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learning
of visual features by contrasting cluster assignments. Adv.
Neural Inform. Process. Syst. , 33:9912‚Äì9924, 2020. 3
[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv‚Äôe J‚Äôegou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. Int.
Conf. Comput. Vis. , pages 9630‚Äì9640, 2021. 1, 2, 3, 8
[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for semantic
image segmentation. ArXiv , abs/1706.05587, 2017. 7
[8] Micka ¬®el Chen, Thierry Arti `eres, and Ludovic Denoyer. Un-
supervised object segmentation by redrawing. Adv. Neural
Inform. Process. Syst. , 32, 2019. 6, 14
[9] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune
Gwon, and Sungroh Yoon. Ilvr: Conditioning method for
denoising diffusion probabilistic models. arXiv preprint
arXiv:2108.02938 , 2021. 2
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
248‚Äì255. IEEE, 2009. 4, 5, 14
[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Adv. Neural Inform. Process.
Syst., 34:8780‚Äì8794, 2021. 2, 6
[12] David H Douglas and Thomas K Peucker. Algorithms for
the reduction of the number of points required to represent a
digitized line or its caricature. Cartographica: the interna-
tional journal for geographic information and geovisualiza-
tion, 10(2):112‚Äì122, 1973. 13
[13] Patrick Esser, Robin Rombach, Andreas Blattmann, and
Bjorn Ommer. Imagebart: Bidirectional context with multi-
nomial diffusion for autoregressive image synthesis. Adv.
Neural Inform. Process. Syst. , 34:3518‚Äì3532, 2021. 2[14] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The PASCAL visual
object classes challenge 2007 (VOC2007) results, 2007. 6,
14
[15] Mark Everingham, Luc Van Gool, Christopher K. I.
Williams, John Winn, and Andrew Zisserman. The PASCAL
Visual Object Classes Challenge 2012 (VOC2012) Results.
6, 14
[16] Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set
generation network for 3d object reconstruction from a single
image. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
605‚Äì613, 2017. 13
[17] Lester Randolph Ford and Delbert R Fulkerson. Maximal
Ô¨Çow through a network. Canadian journal of Mathematics ,
8:399‚Äì404, 1956. 5
[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139‚Äì144, 2020. 2
[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll¬¥ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 16000‚Äì16009, 2022. 1, 6
[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross B. Girshick. Momentum contrast for unsupervised vi-
sual representation learning. IEEE Conf. Comput. Vis. Pat-
tern Recog. , pages 9726‚Äì9735, 2020. 1, 3, 8
[21] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing
Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic
data from generative models ready for image recognition?
arXiv preprint arXiv:2210.07574 , 2022. 4
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Adv. Neural Inform. Process.
Syst., 33:6840‚Äì6851, 2020. 1, 2, 3, 5
[23] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, A. Borji, Z. Tu,
and Philip H. S. Torr. Deeply supervised salient object detec-
tion with short connections. IEEE Conf. Comput. Vis. Pattern
Recog. , 2016. 2
[24] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz.
Multimodal unsupervised image-to-image translation. In
Eur. Conf. Comput. Vis. , pages 172‚Äì189, 2018. 2
[25] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 1125‚Äì1134, 2017. 2
[26] Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant
information clustering for unsupervised image classiÔ¨Åcation
and segmentation. In Int. Conf. Comput. Vis. , pages 9865‚Äì
9874, 2019. 2
[27] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi
Xie. Prompting visual-language models for efÔ¨Åcient video
understanding. In Eur. Conf. Comput. Vis. , pages 105‚Äì124.
Springer, 2022. 2
[28] Chen Ju, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Ya
Zhang, Peisen Zhao, Jianlong Chang, and Qi Tian. Con-
straint and union for partially-supervised temporal sentence
grounding. arXiv preprint arXiv:2302.09850 , 2023. 2
9[29] Chen Ju, Peisen Zhao, Siheng Chen, Ya Zhang, Xiaoyun
Zhang, Yanfeng Wang, and Qi Tian. Adaptive mutual super-
vision for weakly-supervised temporal action localization.
IEEE Transactions on Multimedia , 2022. 2
[30] Chen Ju, Kunhao Zheng, Jinxiang Liu, Peisen Zhao, Ya
Zhang, Jianlong Chang, Yanfeng Wang, and Qi Tian. Distill-
ing vision-language pre-training to collaborate with weakly-
supervised temporal action localization. arXiv preprint
arXiv:2212.09335 , 2022. 2
[31] Tilke Judd, Krista Ehinger, Fr ¬¥edo Durand, and Antonio Tor-
ralba. Learning to predict where humans look. In Int. Conf.
Comput. Vis. , pages 2106‚Äì2113. IEEE, 2009. 5
[32] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
IEEE Conf. Comput. Vis. Pattern Recog. , pages 4401‚Äì4410,
2019. 2
[33] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of stylegan. In IEEE Conf. Comput. Vis.
Pattern Recog. , pages 8110‚Äì8119, 2020. 2
[34] Gunhee Kim and Antonio Torralba. Unsupervised detection
of regions of interest using iterative link analysis. In Adv.
Neural Inform. Process. Syst. , 2009. 6
[35] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 6
[36] Philipp Kr ¬®ahenb ¬®uhl and Vladlen Koltun. EfÔ¨Åcient inference
in fully connected crfs with gaussian edge potentials. In Adv.
Neural Inform. Process. Syst. , 2011. 5, 8
[37] Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis,
Sanja Fidler, and Antonio Torralba. Bigdatasetgan: Synthe-
sizing imagenet with pixel-wise annotations. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 21330‚Äì21340, 2022. 5,
6, 7, 13
[38] Nianyi Li, Bilin Sun, and Jingyi Yu. A weighted sparse cod-
ing framework for saliency detection. IEEE Conf. Comput.
Vis. Pattern Recog. , pages 5216‚Äì5223, 2015. 6
[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and Lawrence
Zitnick. Microsoft COCO: common objects in context. In
Eur. Conf. Comput. Vis. , 2014. 6, 14
[40] Stuart Lloyd. Least squares quantization in pcm. IEEE trans-
actions on information theory , 28(2):129‚Äì137, 1982. 5, 8
[41] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpaint-
ing using denoising diffusion probabilistic models. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 11461‚Äì11471,
2022. 2
[42] Chaofan Ma, Yuhuan Yang, Yanfeng Wang, Ya Zhang, and
Weidi Xie. Open-vocabulary semantic segmentation with
frozen vision-language models. Brit. Mach. Vis. Conf. , 2022.
2
[43] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
Andrea Vedaldi. Finding an unsupervised image segmenter
in each of your deep generative models. arXiv preprint
arXiv:2105.08127 , 2021. 6, 13, 14[44] Luke Melas-Kyriazi, C. Rupprecht, Iro Laina, and Andrea
Vedaldi. Deep spectral methods: A surprisingly strong base-
line for unsupervised semantic segmentation and localiza-
tion. IEEE Conf. Comput. Vis. Pattern Recog. , pages 8354‚Äì
8365, 2022. 2, 6
[45] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and
editing with stochastic differential equations. arXiv preprint
arXiv:2108.01073 , 2021. 2
[46] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. arXiv preprint arXiv:1411.1784 , 2014. 2
[47] Duc Tam Nguyen, Maximilian Dax, Chaithanya Kumar
Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong
Nguyen, Zhongyu Lou, and Thomas Brox. Deepusps:
Deep robust unsupervised saliency prediction with self-
supervision. In Adv. Neural Inform. Process. Syst. , 2019.
6
[48] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2
[49] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In Int. Conf. Mach.
Learn. , pages 8162‚Äì8171. PMLR, 2021. 2
[50] OpenAI. Chatgpt: Optimizing language models for dialogue,
2022. 4
[51] Yassine Ouali, C ¬¥eline Hudelot, and Myriam Tami. Autore-
gressive unsupervised image segmentation. In Eur. Conf.
Comput. Vis. , pages 142‚Äì158. Springer, 2020. 2
[52] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-
hghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Go-
ing deeper with nested u-structure for salient object detec-
tion. Pattern Recognition , 106:107404, 2020. 2
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In Int. Conf. Mach. Learn. , pages 8748‚Äì8763. PMLR,
2021. 1, 5, 8
[54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 2
[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In IEEE Conf. Comput.
Vis. Pattern Recog. , pages 10684‚Äì10695, 2022. 2, 3, 4
[56] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention , pages 234‚Äì241.
Springer, 2015. 3, 7
[57] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In
ACM SIGGRAPH 2022 Conference Proceedings , pages 1‚Äì
10, 2022. 2
10[58] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487 , 2022. 2
[59] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative reÔ¨Ånement. IEEE Trans. Pattern Anal.
Mach. Intell. , 2022. 2
[60] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for
training next generation image-text models. arXiv preprint
arXiv:2210.08402 , 2022. 4
[61] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Do-
minik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel,
Tong He, Zheng Zhang, Bernhard Sch ¬®olkopf, Thomas
Brox, and Francesco Locatello. Bridging the gap to real-
world object-centric learning. arXiv preprint arXiv: Arxiv-
2209.14860 , 2022. 3
[62] Xi Shen, Alexei A Efros, Armand Joulin, and Mathieu
Aubry. Learning co-segmentation by segment swapping for
retrieval and discovery. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 5082‚Äì5092, 2022. 14
[63] Jianbo Shi and Jitendra Malik. Normalized cuts and im-
age segmentation. IEEE Trans. Pattern Anal. Mach. Intell. ,
22(8):888‚Äì905, 2000. 3, 5, 8
[64] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hierarchi-
cal image saliency detection on extended cssd. IEEE Trans.
Pattern Anal. Mach. Intell. , 38(4):717‚Äì729, 2015. 6, 14, 15
[65] Gyungin Shin, Samuel Albanie, and Weidi Xie. Unsuper-
vised salient object detection with spectral cluster voting. In
IEEE Conf. Comput. Vis. Pattern Recog. , pages 3971‚Äì3980,
2022. 3, 6, 13, 14
[66] Oriane Sim ¬¥eoni, Gilles Puy, Huy V V o, Simon Roburin,
Spyros Gidaris, Andrei Bursuc, Patrick P ¬¥erez, Renaud
Marlet, and Jean Ponce. Localizing objects with self-
supervised transformers and no labels. arXiv preprint
arXiv:2109.14279 , 2021. 2, 6
[67] Oriane Sim ¬¥eoni, Chlo ¬¥e Sekkat, Gilles Puy, Antonin V obecky,
¬¥Eloi Zablocki, and Patrick P ¬¥erez. Unsupervised object local-
ization: Observing the background to discover objects. arXiv
preprint arXiv:2212.07834 , 2022. 6
[68] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Int. Conf. Mach. Learn. ,
pages 2256‚Äì2265. PMLR, 2015. 1, 2, 3
[69] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 2, 5
[70] Yang Song and Stefano Ermon. Generative modeling by esti-
mating gradients of the data distribution. Adv. Neural Inform.
Process. Syst. , 32, 2019. 2
[71] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-basedgenerative modeling through stochastic differential equa-
tions. arXiv preprint arXiv:2011.13456 , 2020. 2
[72] Jasper Uijlings, Karin van de Sande, Theo Gevers, and
Arnold Smeulders. Selective search for object recognition.
Int. J. Comput. Vis. , 2013. 6
[73] Huy V . V o, Patrick P ¬¥erez, and Jean Ponce. Toward unsu-
pervised, multi-object discovery in large-scale image collec-
tions. In Eur. Conf. Comput. Vis. , 2020. 6, 14
[74] Huy V . V o, Elena Sizikova, Cordelia Schmid, Patrick P ¬¥erez,
and Jean Ponce. Large-scale unsupervised object discovery.
InarXiv , 2021. 6
[75] Andrey V oynov, Stanislav Morozov, and Artem Babenko.
Object segmentation without labels with large-scale genera-
tive models. In Int. Conf. Mach. Learn. , pages 10596‚Äì10606.
PMLR, 2021. 6, 13, 14
[76] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 6, 14, 15
[77] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Clip-nerf: Text-and-image driven manipula-
tion of neural radiance Ô¨Åelds. In IEEE Conf. Comput. Vis.
Pattern Recog. , 2022. 2
[78] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,
Dong Wang, Baocai Yin, and Xiang Ruan. Learning to de-
tect salient objects with image-level supervision. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 136‚Äì145, 2017. 6,
7, 14, 15
[79] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz,
Anima Anandkumar, Chunhua Shen, and Jos ¬¥e Manuel
¬¥Alvarez. Freesolo: Learning to segment objects without an-
notations. IEEE Conf. Comput. Vis. Pattern Recog. , pages
14156‚Äì14166, 2022. 3, 6
[80] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James L
Crowley, and Dominique Vaufreydaz. Self-supervised trans-
formers for unsupervised object discovery using normalized
cut. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
14543‚Äì14553, 2022. 2, 6, 13, 14
[81] Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chunhua Shen,
and Zhi-Hua Zhou. Unsupervised object discovery and co-
localization by deep descriptor transforming. Pattern Recog-
nition , 2019. 6
[82] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian Sun.
Geodesic saliency using background priors. In Eur. Conf.
Comput. Vis. , pages 29‚Äì42. Springer, 2012. 5
[83] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In IEEE Conf. Comput. Vis.
Pattern Recog. Worksh. , pages 3485‚Äì3492. IEEE, 2010. 14
[84] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical
saliency detection. IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 1155‚Äì1162, 2013. 6
[85] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and
Ming-Hsuan Yang. Saliency detection via graph-based man-
ifold ranking. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 3166‚Äì3173, 2013. 6, 14, 15
[86] Yi Ke Yun and Weisi Lin. Selfreformer: Self-reÔ¨Åned network
with transformer for salient object detection. arXiv preprint
arXiv: Arxiv-2205.11283 , 2022. 2
11[87] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-
Fu Chang. Open-vocabulary object detection using captions.
InIEEE Conf. Comput. Vis. Pattern Recog. , 2021. 2
[88] Runsheng Zhang, Yaping Huang, Mengyang Pu, Jian Zhang,
Qingji Guan, Qi Zou, and Haibin Ling. Object discovery
from a single unlabeled image by mining frequent itemsets
with multi-scale features. IEEE Trans. Image Process. , 29,
2020. 6
[89] Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-
Francois LaÔ¨Çeche, Adela Barriuso, Antonio Torralba, and
Sanja Fidler. Datasetgan: EfÔ¨Åcient labeled data factory with
minimal human effort. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 10145‚Äì10155, 2021. 5, 6
[90] Ting Zhao and Xiangqian Wu. Pyramid feature attention net-
work for saliency detection. In IEEE Conf. Comput. Vis. Pat-
tern Recog. , pages 3085‚Äì3094, 2019. 2
[91] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Int. Conf. Comput. Vis. ,
pages 2223‚Äì2232, 2017. 2
[92] Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun.
Saliency optimization from robust background detection. In
IEEE Conf. Comput. Vis. Pattern Recog. , pages 2814‚Äì2821,
2014. 6
[93] Lawrence Zitnick and Piotr Doll ¬¥ar. Edge boxes: Locating
object proposals from edges. In Eur. Conf. Comput. Vis. ,
2014. 6
126. Appendix
In this supplementary material, we start by giving details about evaluation metrics in Sec. 6.1, and datesets in Sec. 6.2. In
Sec. 6.3, some qualitative visualizations about four benchmarks and our synthetic dataset are displayed.
6.1. Evaluation Metrics
6.1.1 Saliency Segmentation Metrics
Here we deÔ¨Åne three metrics used for evaluating saliency segmentation performance:
‚Ä¢Accuracy (Acc) measures pixel-wise accuracy using ground-truth masks G2f 0;1gHWand binary predictions M2
f0;1gHW.
Acc=1
HWHX
i=1WX
j=1I(Gij=Mij); (9)
where I()is the indicator function.
‚Ä¢Intersection-over-union (IoU) is the overlapped size divided by the total size of the foreground regions from GandM.
IoU=jM\Gj
jM[Gj: (10)
‚Ä¢maximal-F(maxF)is the maximum score of Famong masks binarised using different thresholds. Given binarized
maskMand ground-truthG,Fis deÔ¨Åned as:
F=(1 +2)PrecisionRecall
2Precision +Recall; (11)
where Precision =tp
tp+fpand Recall =tp
tp+fn.tp;fp;fn represent true-positive, false-positive and false-negative
respectively. 2denotes weight. We set 2= 0:3in our experiments following [75, 43, 80, 65].
6.1.2 Single Object Localization Metrics
We report performance using CorLoc metric following [80]. CorLoc considers a predicted bounding box to be correct if the
intersection over union (IoU) score between this box and one of the ground-truth bounding boxes is greater than 0:5.
6.1.3 Geometry Metrics
In Tab. 2 we use three metrics to measure the dataset‚Äôs geometry statistics. Here we provide the implementation details of the
three metrics: shape complexity (SC), polygon length (PL) and shape diversity (SD).
Following [37], we use OpenCV‚Äôs findContours function with RETR EXTERNAL andCHAIN APPROX SIMPLE
Ô¨Çag to extract a simpliÔ¨Åed polygon for each mask. Then we normalize the polygon by pi= (pi pmin)=(pmax pmin). This
operation normalizes the polygon to a unit square in both horizontal and vertical directions. pminandpmax are the minimum
and maximum coordinates among the set of points. We further apply Douglas-Peucker algorithm [12] with a threshold of
0:01to simplify the polygon. After that, we deÔ¨Åne:
‚Ä¢Shape Complexity (SC) is the number of points in the normalized and simpliÔ¨Åed polygon.
‚Ä¢Polygon Length (PL) is deÔ¨Åned as the total length of the polygon.
‚Ä¢Shape Diversity (SD) is the average mean of pair-wise Chamfer distance [16] over all dataset. Chamfer distance is:
dCD(S1;S2) =X
p2S1min
q2S2kp qk2
2+X
q2S2min
p2S1kp qk2
2; (12)
whereS1andS2are two sets of points corresponding to different polygons.
136.2. Datesets Details
Here we present details of all benchmarks used in our experiments:
‚Ä¢ECSSD (Extended Complex Scene Saliency Dataset) [64] consists of 1,000 real-world images of complex scenes.
‚Ä¢DUT-OMRON [85] contains 5,168 high quality images with very challenging scenarios.
‚Ä¢DUTS [78] contains 10,553 training images (DUTS-TR) collected from the ImageNet [10] DET training/val sets, and
5,019 test images (DUTS-TE) collected from the ImageNet DET test set and the SUN [83] dataset. Following previous
works [62, 80, 65], the performance is reported only on DUTS-TE.
‚Ä¢CUB (Caltech-UCSD Birds-200-2011) [76] contains 11,788 images and segmentation masks of 200 subcategories be-
longing to birds. We follow [8, 75, 43] but only use the 1,000 images for the test subset from splits provided by [8].
‚Ä¢VOC07 [14] and VOC12 [15] correspond to the training and validation set of PASCAL VOC07 and PASCAL VOC12.
VOC07 and VOC12 contains 5,011 and 11,540 images respectively which belong to 20 categories.
‚Ä¢COCO20K contains 19,817 randomly chosen images from the COCO2014 dataset [39]. It is used as a benchmark
in [73] for a large scale evaluation.
146.3. Visualization
In this section, we Ô¨Årst present qualitative visualizations of CUB [76] on AttentionCut (Fig. 7), then visualizations of
ECSSD [64] (Fig. 8), DUTS-TE [78] (Fig. 9), DUT-OMRON [85] (Fig. 10) on both AttentionCut and DiffusionSeg, respec-
tively. Fig. 11 shows our synthetic dataset. Note that ¬¨,¬≠,¬Æ,¬±are the same meaning as in Tab. 6. ¬¨: onlyAc;¬≠: only
r(p);¬Æ:Acwithr(p),i.e.,(p);¬±:(p)with (p;q),i.e., AttentionCut.
Figure 7: Qualitative Results of AttentionCut on CUB. The columns from left to right are input image, ground-truth mask
and¬¨,¬≠,¬Æ,¬±are the same as deÔ¨Åned in Tab. 6 ( ¬¨: onlyAc;¬≠: onlyr(p);¬Æ:Acwithr(p),i.e.,(p);¬±:(p)with
 (p;q),i.e., AttentionCut).
15Figure 8: Qualitative Results on ECSSD. First three columns are input image, ground-truth mask and reconstructed image
by diffusion inversion. ¬¨,¬≠,¬Æ,¬±are the same as deÔ¨Åned in Tab. 6 ( ¬¨: onlyAc;¬≠: onlyr(p);¬Æ:Acwithr(p),i.e.,(p);
¬±:(p)with (p;q),i.e., AttentionCut). The last column is the prediction of DiffusionSeg.
16Figure 9: Qualitative Results on DUTS-TE. First three columns are input image, ground-truth mask and reconstructed
image by diffusion inversion. ¬¨,¬≠,¬Æ,¬±are the same as deÔ¨Åned in Tab. 6 ( ¬¨: onlyAc;¬≠: onlyr(p);¬Æ:Acwithr(p),i.e.,
(p);¬±:(p)with (p;q),i.e., AttentionCut). The last column is the prediction of DiffusionSeg.
17Figure 10: Qualitative Results on DUT-OMRON. First three columns are input image, ground-truth mask and reconstructed
image by diffusion inversion. ¬¨,¬≠,¬Æ,¬±are the same as deÔ¨Åned in Tab. 6 ( ¬¨: onlyAc;¬≠: onlyr(p);¬Æ:Acwithr(p),i.e.,
(p);¬±:(p)with (p;q),i.e., AttentionCut). The last column is the prediction of DiffusionSeg.
18Figure 11: Synthetic Image-mask Pairs. With zero human annotated required, DiffusionSeg can generate ‚ÄúinÔ¨Ånite‚Äù realistic
and diverse images together with impressive masks. Random samples are shown here.
19