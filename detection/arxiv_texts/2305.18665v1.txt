E-PANNs: Sound Recognition Using E fficient Pre-trained Audio
Neural Networks
Arshdeep Singh1, Haohe Liu2, Mark D. Plumbley3
Centre for Vision, Speech and Signal Processing (CVSSP),
University of Surrey, UK
ABSTRACT
Sounds carry an abundance of information about activities and events in our everyday environment,
such as tra ffic noise, road works, music, or people talking. Recent machine learning methods, such
as convolutional neural networks (CNNs), have been shown to be able to automatically recognize
sound activities, a task known as audio tagging. One such method, pre-trained audio neural networks
(PANNs), provides a neural network which has been pre-trained on over 500 sound classes from
the publicly available AudioSet dataset, and can be used as a baseline or starting point for other
tasks. However, the existing PANNs model has a high computational complexity and large storage
requirement. This could limit the potential for deploying PANNs on resource-constrained devices,
such as on-the-edge sound sensors, and could lead to high energy consumption if many such devices
were deployed. In this paper, we reduce the computational complexity and memory requirement of
the PANNs model by taking a pruning approach to eliminate redundant parameters from the PANNs
model. The resulting E fficient PANNs (E-PANNs) model, which requires 36% less computations and
70% less memory, also slightly improves the sound recognition (audio tagging) performance. The
code for the E-PANNs model has been released under an open source license.
1. INTRODUCTION
Everyday sound environments include a wide range of sound activities and events, such as tra ffic
noise, road works, key jangling, music, coughing or people talking. These environmental sound
activities contain an abundance of information and can potentially be used in various applications
including public security surveillance, monitoring activities in a home for assisted living, healthcare,
and improving the o ffice, workplace and urban environment.
Recent advances in machine learning infrastructure and the availability of large-scale dataset
such as AudioSet [1] have attracted artificial intelligence and machine learning (AI /ML) researchers
to develop methods for automatic sound activity recognition, commonly known as audio tagging. A
typical audio tagging system is shown in Figure 1. It takes audio recordings from the surrounding
using microphones and then, recognises various sound activities that occur in the surrounding
area. Audio tagging systems using convolutional neural networks (CNNs) have shown promising
performance compared to the traditional hand-crafted methods [2]. However, CNNs are resource-
hungry due to the high computational cost arising from multiply-accumulate operations (MACs) and
from the memory requirement for CNNs. For example, one of the best performing audio tagging
networks from the pre-trained audio neural networks (PANNs) [2] framework has approximately
1arshdeep.singh@surrey.ac.uk
2haohe.liu@surrey.ac.uk
3m.plumbley@surrey.ac.ukarXiv:2305.18665v1  [cs.SD]  30 May 2023Figure 1: An audio tagging system recognising various sound activities in a given audio recording [4].
81M parameters and requires more than 50G MACs for inference corresponding to a 10s audio clip.
Due to this, it may be challenging to deploy such large-scale CNNs on resource-constrained devices
having a limited power budget and limited memory, such as smart phones or internet of things (IoT)
devices. Moreover, when large-scale CNNs are used as a feature extractor or as a classifier for other
downstream tasks such as acoustic scene classification [3], the high computational cost of the CNNs
makes them slow during inference and particularly during the training process, where the large-scale
CNNs may consume more energy and emit more CO 2. For instance, an NVIDIA RTX-2080 Ti GPU
used to train machine learning models for 48 hours generates the equivalent amount of CO 2to that
emitted by an average car driven for 13 miles4. Therefore, large-scale CNNs are not e fficient and
environmental-friendly despite performing well.
Our contributions: To reduce computational complexity and memory storage of large-scale CNNs
such as PANNs, we eliminate redundant parameters and reduce computational complexity from the
PANNs model by taking a pruning approach. The resulting E fficient PANNs (E-PANNs) model has
significantly less MACs and memory storage with slightly improved performance compared to that
of the original PANNs model. We make a real-time sound recognition demonstration using E-PANNs
publicly available5.
The rest of the paper is organised as follows. Section 2 introduces some background including
a brief overview of convolutional neural networks, and background on existing audio tagging systems
and the PANNs architecture. Section 3 presents the proposed methodology used to obtain E-PANNs.
Next, the experimental setup and dataset used for experiments is explained in Section 4. Section 5
presents experimental analysis. Finally, Section 6 concludes the paper.
2. BACKGROUND
2.1. Convolutional Neural Networks (CNNs)
Convolutional neural networks (CNNs) are a type of artificial neural network inspired by biological
nervous systems such as the human brain. CNNs are designed to learn from examples or from a
dataset for an underlying task such as classification of sound activity in the surrounding area. They
learn through an optimization process to update their parameters including weights and filters across
various types of intermediate layers, such as convolutional, pooling, and dense layers. An architecture
of a CNN is shown in Figure 2. A convolutional layer has multiple feature maps, where each feature
map is produced by the convolutional operation on input and a filter.
In a CNN, filters are small matrices of size ( k×k) with cchannels that are convolved across
the input data. The convolution operation, as given in Equation 1, is a multiply-accumulate operation
(MAC) that involves sliding the filter, F, over the input, performing element-wise multiplications
4Machine learning CO 2estimator: https://mlco2.github.io/impact/#compute
5https://github.com/Arshdeep-Singh-Boparai/E-PANNs.gitbetween the filter and the corresponding input patch xof size ( c×k×k), and summing up the results
to produce a single value, y. This process is repeated for every possible position of the filter across
the input, resulting in a feature map . Similarly, other feature maps are produced using other filters in
a given convolutional layer. Subsequently a bias band a non-linear activation function f(.) is applied
to the elements of feature maps. Other than convolutional, pooling and dense layers, some CNNs
might have residual blocks, which contain skip or shortcut connections that bypass one or more of the
convolutional layers. For an introduction to various architectures of CNNs, see the article [5].
Classes
or
categoriesInputConvolutional
layerPooling
layerConvolutional
layerDense layer
Feature mapsFiltersFilters
Figure 2: Convolutional neural network (CNN) architecture comprising of convolutional, pooling and
dense layer.
y=f(cX
i=1kX
k1=1kX
k2=1(xi,k1,k2×Fi,k1,k2)+b). (1)
2.2. Existing Audio Tagging Frameworks
With the release of the large-scale AudioSet dataset [1], which has 2M audio examples and 527
classes, several researchers have conducted studies to improve the performance of neural networks on
AudioSet classification and in particular CNNs. Methods for AudioSet classification include CNNs
[2], CNNs with residual blocks [6] and and self-attention based Transformers [7]. A summary of
the existing methods alongwith their performance and the number of parameters is given in Table 1.
While Transformer-based methods perform better than CNNs, Transformers have high computational
complexity, which makes their deployment on low-powered devices di fficult compared to the CNNs.
We shall therefore focus on CNNs in this paper.
Table 1: Mean average precision (mAPs) obtained for AudioSet evaluation dataset using various
audio tagging systems with their number of parameters.
Method Neural Network type Parameters (x 106) mAPs
PANNs-CNN6 [2] Plain 4.8 0.343
PANNs-CNN10 [2] Plain 5.2 0.380
ERANNs-2-5 [6] Residual 38.2 0.446
ERANNS-1-6 [6] Residual 54.5 0.450
PANNs (ResNet38) [2] Residual 73.78 0.434
PANNs-CNN14 [2] Plain 80.75 0.431
PANNs (Wavegram-Logmel-CNN) [2] Plain 81.06 0.439
AST [8] Transformer 88.10 0.459
AST(ensemble) [8] Transformer 526.6 0.4852.3. PANNs-CNN14 Architecture for AudioSet Classification
The primary motivation for the development of PANNs was to provide pre-training systems for audio
pattern recognition on extensive datasets, in this case, the AudioSet dataset [1], which can be used
as a baseline network to extract features or for a classification task to other audio-related tasks. The
authors proposed several architectures for PANNs, including CNN-14, which demonstrated promising
performance on various audio pattern recognition tasks. Experiments demonstrated that PANNs can
generalize well to other tasks with limited training data and outperform models trained from scratch
on those tasks.
The architecture of the PANNs-CNN14 model, including the number of parameters across each
convolutional layers, the total number of parameters and the model size is shown in Figure 6 (a) in the
Appendix. It has six convolutional blocks, each with two convolutional layers, followed by a batch
normalization layer and a ReLU activation function. The number of filters in each convolutional block
gradually increases from 64 to 2048 from layer to layer. Finally, there is a dense layer with a sigmoid
activation function that outputs the predicted probabilities for each class.
The PANNs-CNN14 model takes a log-mel spectrogram of size (1000 ×64) computed from
the 10-second-length audio input. The model is trained with data augmentation techniques such as
Mixup [9] and SpecAugment [10] for 600k iterations. For more details on CNN14 and other PANNs
models, see [2].
3. PROPOSED METHODOLOGY TO REDUCE COMPLEXITY
To reduce the computational complexity and memory storage of CNNs such as PANNs-CNN14, we
use a filter pruning approach [11–13] that involves removing or “pruning” unimportant filters from
the network, i.e. these filters that contribute less to the performance of the CNN. Filter pruning is
inspired by the idea that some filters in a CNN are redundant or have a negligible contribution to
the overall accuracy of the network [14, 15]. These filters can be safely removed from the network
without significantly a ffecting its performance. For example, previous studies [16,17] found that 73%
of the filters in a SoundNet network [18] do not provide discriminative information across di fferent
acoustic scene classes, and eliminating such filters give similar performance to that of using all the
original filters in SoundNet. For a survey on pruning techniques, see [11].
The typical steps in a filter pruning process are:
1. For a baseline, train the original network to a desired level of accuracy, or utilise an already
existing pre-trained network;
2. Rank the filters based on a certain “importance” criterion, such as their relevance in contributing
to the performance of the network;
3. Remove the least important filters and their corresponding feature maps from the network to
obtain a pruned network;
4. Fine-tune the pruned network to recover some of the lost accuracy.
There are several benefits of filter pruning in CNNs. Firstly, it can significantly reduce the
size of the network, reducing the memory footprint and computation time required for inference by
removing filters and the corresponding feature maps generated by the filters. Secondly, the robustness
of the network in maintaining performance and generalization capabilities can improve by removing
filters that may be sensitive to small perturbations in the input data.
In this paper, we will apply the filter pruning approach to the PANNs-CNN14 model, to
produce an e fficient PANNs model, E-PANNs. To obtain E-PANNs, we follow a three step pipeline
as shown in Figure 3. A detailed description of the steps is given below,Pruning filters layer by layer
Baseline network
(Pre-trained
PANNs-CNN14)Pruned
PANNs-
CNN14(a) (b)
(c)Fine-tuningTake filters
from a given
convolutional
layerCompute
importance
for each filter
Remove p filters
with least
importance
E-PANNsStep 1
Step 2 & 3Figure 3: A flow diagram to obtain E-PANNs.
(Step 1) Take a baseline PANNs network : We take the publicly available pre-trained CNN14 from
PANNs and denote it as PANNs-CNN14. PANNs-CNN14 has approximately 81M parameters and
requires 21G MACs6for inference of a 10s audio clip with a performance metric, mAPs of 0.431.
(Step 2) Compute filter importance across convolutional layers of the baseline network: We
compute the relevance of the filters to decide whether to retain or prune the filter for each layer of
PANNs-CNN14 independently using (a), (b), and (c) steps as shown in Figure 3.
Assume that there are nfilters in a given convolutional layer. With these nfilters, first, we
measure how well a given filter produces the output in the convolutional layer. Our hypothesis is that
a filter producing significant output can capture specific patterns or features in the input significantly,
which would be useful for the subsequent convolutional layers of CNN to better understand the input
data at di fferent levels of abstraction. Therefore, we consider a filter producing significant output
more important than others. Then, we measure the importance of each filter and rank the filters
according to their importance in a given convolutional layer. Subsequently, the process (a)-(c) is
repeated for other convolutional layers as well. More information about the importance calculation
of the filters can be found in [19].
(Step 3) Obtaining E-PANNs: Once we obtain the importance of each filter in various convolutional
layers in PANNs-CNN14, we eliminate pleast important filters and their corresponding feature maps
from various convolutional layers. This results in a pruned network. The pruned network is then
re-trained or fine-tuned with same data as used for training the baseline PANNs-CNN14 network, to
regain some of the lost performance owing to elimination of the filters and their corresponding feature
map. Finally, an e fficient version of PANNs-CNN14, E-PANNs is obtained.
4. EXPERIMENTAL SETUP
In this section, we describe the experimental setup to prune the PANNs-CNN14 and then fine-tune
the pruned network.
In PANNs-CNN14 model, the last six convolutional layers (C7 to C12) yield approximately
99% of the total network parameters (See architecture in Figure 6 (a) in the Appendix), therefore we
prune p∈{25% ,50% ,75%}filters from the last six convolutional layers. The fine-tuning process of
the pruned PANNs-CNN14 is performed in the same way as that used to train the original PANNs-
CNN14 [2]. The optimization procedure uses the same loss function, data augmentation and batch
size etc, but with fewer iterations, which are approximately 450k for the pruned PANNs-CNN14. To
evaluate the performance of the proposed E-PANNs, we test it on the audio tagging problem using
the AudioSet evaluation dataset.
6MACs computation Pytorch package.: https://pypi.org/project/thop/5. PERFORMANCE ANALYSIS
Figure 4: Convergence during fine-tuning process of E-PANNs when di fferent number of filters are
pruned from the baseline network. In brackets, the maximum mAPs obtained is also given.
Figure 4 shows a convergence plot obtained during the fine-tuning process of E-PANNs, when
different numbers of filters are pruned from the baseline network. We find that E-PANNs recovers
the same performance as that of the baseline network, even after pruning 25% and 50% filters, with
only 100k and 200k iterations respectively. We also find that it uses at least 3 times fewer iterations
compared to the baseline network to achieve mAPs equal to 0.431. Therefore, E-PANNs can be
used as an alternative to the original PANNs with an advantage of less computation and memory
requirement.
Trade-of f
81MBaseline mAPs
0.431
Figure 5: mAPs and the number of parameters obtained using the proposed method and the other
existing CNN-based methods.
We find that pruning 25% of the filters across C7 to C12 layers of the baseline network removes
41% of the parameters and 24% of the MACs with an improved mAPs equal to 0.442, compared to
that of the baseline network. Also after pruning 50% of the filters, the mAPs obtained are better than
that of the baseline network, with 70% fewer parameters and 36% fewer MACs. The architecture
of the E-PANNs network obtained after pruning 50% of the filters is shown in Figure 6 (b) in the
Appendix. On the other hand, when 75% of the filters are removed, the mAPs is 0.420 with 78%
fewer parameters and 46% fewer MACs. This suggests that there is a trade-o ffbetween the number
of parameters pruned and the mAPs. As shown in Figure 5 (see arrow trend), we find that when the
number of parameters in E-PANNs is less than 25% of the total number of parameters in the baseline
network, and that the mAPs obtained using E-PANNs becomes less than the mAPs obtained using
the baseline network. Therefore, the maximum number of parameters that can be pruned from the
baseline network, PANNs-CNN14, without any degradation in performance are approximately 75%.
Figure 5 also shows the number of parameters and mAPs obtained using the other existing
CNN-based methods as given in Table 1. Our proposed method gives better mAPs with 70%fewer parameters and 60% fewer parameters respectively compared to the best performing PANNs-
(Wavegram-Logmel-CNN) and the PANNs-ResNet38 network. In general, compared to other
methods such as PANNs-CNN10 or ERANNs-1-6 [6], we find that there is a trade-o ffbetween the
number of parameters and the mAPs.
6. CONCLUSION
Convolutional neural networks (CNNs) are e ffective for audio recognition, but can be costly. We make
these CNNs more e fficient by reducing their computational cost and memory storage. We present a
framework to obtain E-PANNs from a large-scale pre-trained audio neural network (PANNs) for
audio tagging. We find that removing few of the unimportant filters from original PANNs reduces
the computational complexity by 36% and 70% fewer parameters with an improved performance.
Therefore, E-PANNs can be used as an alternative to the original PANNs for the downstream tasks
for feature extraction or classification due to the less computational complexity and low memory
requirement of E-PANNs compared to that of the PANNs.
7. ACKNOWLEDGEMENTS
This work was partly supported by Engineering and Physical Sciences Research Council (EPSRC)
Grant EP /T019751 /1 “AI for Sound (AI4S)”. For the purpose of open access, the authors have applied
a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising.
REFERENCES
1. Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing
Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset
for audio events. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pages 776–780. IEEE, 2017.
2. Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley.
PANNs: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE /ACM
Transactions on Audio, Speech, and Language Processing , 28:2880–2894, 2020.
3. Irene Martín-Morató, Toni Heittola, Annamaria Mesaros, and Tuomas Virtanen. Low-complexity
acoustic scene classification for multi-device audio: Analysis of DCASE 2021 Challenge
systems. DCASE Workshop , 2021.
4. Eduardo Fonseca, Manoj Plakal, Frederic Font, Daniel P. W. Ellis, and Xavier Serra. Audio
tagging with noisy labels and minimal supervision. In DCASE2019 Workshop , NY , USA, 2019.
5. Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting
Liu, Xingxing Wang, Gang Wang, Jianfei Cai, et al. Recent advances in convolutional neural
networks. In Pattern Recognition , volume 77, pages 354–377. Elsevier, 2018.
6. Sergey Verbitskiy, Vladimir Berikov, and Viacheslav Vyshegorodtsev. ERANNS: E fficient
residual audio neural networks for audio pattern recognition. Pattern Recognition Letters ,
161:38–44, 2022.
7. Yuan Gong, Andrew Rouditchenko, Alexander H Liu, David Harwath, Leonid Karlinsky, Hilde
Kuehne, and James R Glass. Contrastive audio-visual masked autoencoder. In International
Conference on Learning Representations , 2023.
8. Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc.
Interspeech 2021 , pages 571–575, 2021.
9. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond
empirical risk minimization. International Conference on Learning Representations (ICLR) ,
2017.10. Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk,
and Quoc V Le. SpecAugment: A simple data augmentation method for automatic speech
recognition. Interspeech , 2019.
11. Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and
quantization for deep neural network acceleration: A survey. Neurocomputing , 461:370–403,
2021.
12. Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A filter level pruning method for deep
neural network compression. In Proceedings of the IEEE International Conference on Computer
Vision , pages 5058–5066, 2017.
13. Haisong Ding, Kai Chen, and Qiang Huo. Compressing CNN-DBLSTM models for OCR with
teacher-student learning and tucker decomposition. In Pattern Recognition , volume 96, page
106957. Elsevier, 2019.
14. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations , 2019.
15. Misha Denil, Babak Shakibi, Laurent Dinh, M.A. Ranzato, and Nando De Freitas. Predicting
parameters in deep learning. In Advances in Neural Information Processing Systems , pages 2148–
2156, 2013.
16. Arshdeep Singh, Padmanabhan Rajan, and Arnav Bhavsar. Deep hidden analysis: A statistical
framework to prune feature maps. In IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pages 820–824. IEEE, 2019.
17. Arshdeep Singh, Padmanabhan Rajan, and Arnav Bhavsar. SVD-based redundancy removal in
1-D CNNs for acoustic scene classification. In Pattern Recognition Letters , volume 131, pages
383–389. Elsevier, 2020.
18. Yusuf Aytar, Carl V ondrick, and Antonio Torralba. SoundNet: Learning sound representations
from unlabeled video. Advances in Neural Information Processing Systems , 29, 2016.
19. Arshdeep Singh and Mark D Plumbley. E fficient CNNs via Passive Filter Pruning. arXiv preprint
arXiv:2304.02319 , 2023.APPENDIX
Architecture of baseline network: PANNs-CNN14 model and e fficient PANNs-CNN14.
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12Raw-audio to log-mel
spectrogram conversion
(a) PANNs-CNN14 (b) E-P ANNs-CNN14
(50% filters removed from C7 to C12 layers)
Figure 6: (a) The architecture of baseline network PANNs-CNN14. (b) The architecture of E-PANNs
obtained after removing 50% of the filters from C7 to C12 convolutional layers of PANNs-CNN14.
The number of parameters for each convolutional layer and the parameter (model) size is also shown.
The PANNs-CNN14 has 12 trainable convolutional layers, denoted as C1 to C12. The first two
convolutional layers (conv1d-1. conv1d-2) are non-trainable and their purpose is to convert the raw
audio signal to log-mel spectrogram.