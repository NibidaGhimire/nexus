Global optimization of MPS in quantum-inspired
numerical analysis
Paula Garc´ ıa-Molina
Institute of Fundamental Physics IFF-CSIC, Calle Serrano 113b, Madrid 28006, Spain
E-mail: paula.garcia@iff.csic.es
Luca Tagliacozzo
Institute of Fundamental Physics IFF-CSIC, Calle Serrano 113b, Madrid 28006, Spain
Juan Jos´ e Garc´ ıa-Ripoll
Institute of Fundamental Physics IFF-CSIC, Calle Serrano 113b, Madrid 28006, Spain
Abstract. This work discusses the solution of partial differential equations (PDEs)
using matrix product states (MPS). The study focuses on the search for the lowest
eigenstates of a Hamiltonian equation, for which five algorithms are introduced:
imaginary-time evolution, steepest gradient descent, an improved gradient descent,
an implicitly restarted Arnoldi method, and density matrix renormalization group
(DMRG) optimization. The first four methods are engineered using a framework
of limited-precision linear algebra, where operations between MPS and matrix
product operators (MPOs) are implemented with finite resources. All methods
are benchmarked using the PDE for a quantum harmonic oscillator in up to two
dimensions, over a regular grid with up to 228points. Our study reveals that all MPS-
based techniques outperform exact diagonalization techniques based on vectors, with
respect to memory usage. Imaginary-time algorithms are shown to underperform any
type of gradient descent, both in terms of calibration needs and costs. Finally, Arnoldi-
like methods and DMRG asymptotically outperform all other methods, including exact
diagonalization, as problem size increases, with an exponential advantage in memory
and time usage.
Keywords : quantum-inspired numerical optimization, quantum-inspired numerical
methods, tensor networks, numerical analysisarXiv:2303.09430v2  [quant-ph]  16 May 2024Global optimization of MPS in quantum-inspired numerical analysis 2
1. Introduction
Tensor network states (TNs) are a large family of quantum state representations which
use moderate classical resources—time and memory—to describe complex quantum
systems in scenarios of low entanglement. TNs have been successfully applied in the
study of quantum many-body physics [1, 2, 3, 4, 5], approximating the low-energy
properties of quantum Hamiltonians [6, 7, 8, 9, 10], enabling the study of quantum phases
of matter [11, 12, 13], or the simulation of spin, bosonic and fermionic systems in multiple
dimensions [14, 15, 16, 17, 18, 19, 20, 21]. The expressivity and efficiency of TNs have
made them ideal tools to develop new quantum-inspired algorithms that solve a large
variety of non-quantum problems. These applications include machine learning [22]—
e.g. unsupervised and supervised learning for the classification of images [23, 24, 25, 26],
generative modeling [26, 27, 28], reinforcement learning [29, 30], or the use of quantum
circuits based on TNs to machine learning tasks [31]—, the improvement of optimization
algorithms [32, 33] and, more recently, the development of novel approaches to large
numerical analysis problems [34].
Almost all TNs algorithms use these structures in variational schemes, many of
which translate to finding the lowest energy eigenstates of some Hermitian operator
H. The first TN algorithm for this task is the density matrix renormalization group
(DMRG) [9, 10]. This algorithm can be interpreted as a local optimization of the tensors
in a matrix product state (MPS), sweeping various times over the whole system, until
convergence [35, 36, 37]. A popular alternative to DMRG is solving the imaginary-time
evolution problem ∂t|ψ⟩=−H|ψ⟩, a task which is facilitated when the Hamiltonian is
local and one may apply time-evolving block decimation (TEBD) algorithm [6, 7, 8] for
a repeatedly local update of the matrix product state. Imaginary-time evolution can be
upgraded through the time-dependent variational principle (TDVP) [38, 39, 4], or by
combining DMRG-like methods with Taylor, Pad´ e and Arnoldi approximations of the
evolution operator [40, 41].
In this work, we revise the problem of operator diagonalization with TNs in
the context of quantum-inspired algorithms for numerical analysis [34]. In these
applications, sophisticated, highly non-local matrix product operators (MPO) can
represent partial differential equations (PDEs), while MPS or other TNs are used for
representing the solution of those equations. Focusing on a multidimensional harmonic
oscillator with squeezing, we study the four TNs techniques that can be most easily
applied to solving static PDEs: (i) imaginary-time evolution, (ii) gradient-descent type
methods, (iii) linear algebra approximate diagonalization techniques, and (iv) state-of-
the-art DMRG-like optimization of the tensors. Methods (i)-(iii) are all implemented in
a framework of approximate linear algebra, where MPS encode vectors and MPO encode
matrices, and matrix-vector products and vector-sum operations can be implemented
as fast, efficient optimization problems. In this framework, we find that imaginary-
time evolution algorithms are more costly than a simple gradient descent, in terms of
those elementary MPO-MPS operations. Moreover, we find that gradient descent canGlobal optimization of MPS in quantum-inspired numerical analysis 3
be upgraded to work in a Krylov basis of arbitrary size, formulating a variant of the
Arnoldi diagonalization method that outperforms methods (i) and (ii), even in scenarios
of limited precision. Finally, in comparing all methods with DMRG, we find that,
while the latter performs exponentially better in single-shot experiments, the Arnoldi
methods perform just as well if a renormalization strategy is applied—that is, solving the
same problem with increasing size—and can be generalized to MPOs of greater depth.
This study, therefore, establishes Arnoldi diagonalization as a powerful technique that
can address large-scale optimization problems, not only in quantum-inspired numerical
analysis but also possibly in other many-body and quantum chemistry applications.
The structure of the manuscript is as follows. Section 2 introduces the field of
quantum numerical analysis, motivating the efficient encoding of partial differential
equations and functions as MPO and MPS. Section 3 discusses the benchmark problem—
solving a harmonic oscillator with squeezing in multiple dimensions—and the methods
to solve it. These include different implementations of imaginary-time evolution (Section
3.1), methods for approximate diagonalization based on gradient descent (Section 3.2.1)
and improved gradient descent (Section 3.2.2), and finally an Arnoldi-like method with
implicit restart (Section 3.2.3). We analyze the performance of these methods in
Section 4 and benchmark them using the one-dimensional quantum harmonic oscillator
PDE, concluding the superiority of approximate diagonalization over imaginary-time
evolution. Finally, in Section 5 we compare the best methods with DMRG, over a large-
scale 2D problem, providing evidence of the advantages of quantum-inspired methods,
as well as the best strategies to implement them. Finally, in Section 6 we draw the
conclusions from this work and outline future research lines.
2. Quantum-inspired numerical analysis
Quantum computers have been proposed as a viable platform to solve complex numerical
analysis in a scalable way. For example, it has been shown how PDEs can be solved
using both fault-tolerant [42, 43, 44, 45] and noisy-intermediate scale quantum (NISQ)
algorithms [46, 47, 48, 49, 50, 51]. These are algorithms that benefit both from an
efficient compression of data into the quantum computer—the quantum register encodes
highly differentiable functions with a precision that increases exponentially with the
number of qubits—, and the intrinsic speed-up of quantum algorithms such as phase
estimation or matrix inversion. However, despite this algorithmic progress, the fact
is that contemporary quantum computers have insufficient accuracy to encode even
rather simple problems [47]. It therefore makes sense to look back at the progress in
these quantum algorithms and understand what strategies can be reused in a classical,
quantum-inspired scenario—in particular, with tensor-network methods in mind.
There is a growing body of literature that explores the encoding and solution of
numerical problems using tensor networks of various types. In particular, matrix product
states (MPS) [1]—also known as tensor trains in applied mathematics—have been used
to solve parabolic PDEs [52, 53], parametric PDEs combined with the Galerkin methodGlobal optimization of MPS in quantum-inspired numerical analysis 4
and Karhunen-Lo` eve expansion [54, 55], high-dimensional nonlinear PDEs [56] using the
functional tensor train (FTT) representation [57], Hamilton Jacobi Bellman equations
[58, 59, 60, 61, 62], the Schr¨ odinger equation using a gradient-descent algorithm [63],
among others. The FTT [57] stands out as an effective representation technique. Other
interesting TNs techniques for PDEs are the combination of Chebyshev interpolation and
spectral differentiation with low-rank tensor approximations to solve multidimensional
PDEs [64], or the use of automatic differentation [65].
In this section, we motivate a particular encoding of numerical analysis problems
using quantum registers and quantum operators, which can be upgraded to treat
numerous problems both in the quantum and quantum-inspired scenarios [34]. We
cover the representation of continuous functions using qubits (Section 2.1), and
the representation of potential and differential operators (Section 2.2) as linear
transformations of those quantum states. In a natural way, we also discuss how the
representation in terms of quantum states and operators may be transformed into a
representation in terms of MPS and MPOs, which can be more efficiently manipulated
in a classical computer, and become the basis of quantum-inspired algorithms (Section
4).
2.1. Representation of functions
Our representation of continuous functions using quantum registers is inspired by Refs.
[66, 67]. Given a d-dimensional function f(x) =f(x1, x1, . . . , x d), defined over intervals
xi∈[ai, bi) of size Lxi=|bi−ai|,i= 1, . . . , d , we discretize each dimension xiusing 2ni
points,
x(ni)
i,si=ai+si∆x(ni)
i. (1)
The grid positions xson the discretization grid are labeled by a set of integer si∈
{0,1, . . . , 2ni−1}, each of them encoded using niqubits out of a quantum register. This
allows us to store the discretized function {f(xs)}as the wavefunction of the quantum
register,
|f(n)⟩=1
N1/2
fX
{si}f(xs)|s⟩, (2)
up to a normalization factor Nf, and where n=P
ini.
This representation in position space can also be transformed into a representation
in frequency or momentum space using the multidimensional quantum Fourier transform
(QFT) operator F
|˜f(n)⟩=X
{si}˜f(n)(ps)|s⟩=1√
2nX
{si}ei2πrP
isi/2nif(x)|s⟩:=F|f(n)⟩,(3)
with
p(ni)
i,si=2π
∆x(ni)
i2ni×(
si for 0⩽si<2ni−1,
si−2niotherwise.(4)Global optimization of MPS in quantum-inspired numerical analysis 5
Figure 1. (a) Diagrammatic representation of an MPS with open boundary
conditions. (b) Scalar product ⟨ξ|ψ⟩of two states |ψ⟩,|ξ⟩in the MPS representation.
(c) Contraction of an MPS and an MPO.
When expressed on the basis of qubit states, the states introduced above |f(n)⟩
and|˜f(n)⟩both require an exponentially large number of parameters. This may be
a problem in a quantum computer, if the generation of those parameters requires
exponentially many operations, and is always a problem in classical algorithms, because
of the exponentially growing time and memory requirements. Fortunately, if the function
is differentiable enough, it will have a small entanglement content [34], satisfying an area
law, and becoming amenable to a tensor network representation.
The MPS representation we use is sketched in Figure 1(a). The exponentially many
coefficients of the wavefunction are recovered by contracting a set of tensors
|ψ⟩=X
{k}X
{α}(As1
α1As2
α1,α2As3
α2,α3. . . AsN
αN−1,αN)|s1⟩ ⊗ |s2⟩ ⊗...⊗ |sN⟩.(5)
Each tensor Askαk,αk+1∈C2×Dk×Dk+1has a bounded size, with dimensions Dkthat
depend on the entanglement content. Provided this is kept under bounds, the whole
representation requires only polynomial many resources O(N×2×D2).
2.2. Representation of operators
The encoding of functions in the quantum register induces a similar representation
for operators acting on those functions. The most common linear operators include
multiplication by some other function and differentiation. The multiplication by a
continuous function V(x) is a diagonal operator in the position basis
V(ˆx(n)) :=X
{si}V(x)|s⟩⟨s|. (6)
Differential operators of any order become diagonal operators in Fourier space
D(−i∇) :=F−1X
{si}D(p)|s⟩⟨s|F. (7)Global optimization of MPS in quantum-inspired numerical analysis 6
Finite differences are a popular alternative to the spectral differentiation method, which
avoids the cost of the QFT. Building on finite-order approximations of derivatives,
∂f(x)
∂xi=f(x+ ∆xiei)−f(x−∆xiei)
2∆xi+O(∆x2
i), (8)
∂2f(x)
∂x2
i=f(x+ ∆xiei)−2f(x) +f(x−∆xiei)
∆x2
i+O(∆x2
i), (9)
one creates first and second-order derivate operators as linear combinations of
displacements ˆS±on the quantum register [34]
|∂xif(n)⟩ ≃1
2∆xi
ˆS+
i−ˆS−
i
|f(n)⟩, (10)
|∂2
xif(n)⟩ ≃1
∆x2
i
ˆS+
i+ˆS−
i−2
|f(n)⟩. (11)
Just as we require MPS for compressing the wavefunction representation, the linear
operators for potentials and derivatives can be encoded using tensor-network methods.
Given a linear operator that has exponentially many coefficients in the qubit basis
ˆO=X
sk,s′
kOs1,...,sN
s′
1,...,s′
N|s1, . . . , s N⟩⟨s′
1, . . . , s′
N|, (12)
its matrix product operator (MPO) representation is a contraction of rank-4 tensors,
ˆO=X
sk,s′
kX
βkWs1,s′
1
β1Ws2,s′
2
β1,β2...WsN,s′
N
βN|s1, . . . , s N⟩⟨s′
1, . . . , s′
N|. (13)
Each tensor Wsk,s′
k
βn,βk+1has two physical indices, relating the qubit degrees of freedom sk
ands′
k, and two internal indices, βkandβk+1, that carry information about correlations.
When these correlations are small and independent of the problem size N, the complete
MPO requires only polynomially many components to be described O(N×2×D2).
Figure 2. MPO elements for the 1D ˆ xoperator with D= 2. The figure depicts the
non-zero tensors’ element of the MPO. Middle tensors (b) are rank-4 tensors, while
the first (a) and last (c) tensor have rank-3. If the tensor is equal to a delta function
it is represented as a straight line.Global optimization of MPS in quantum-inspired numerical analysis 7
Computing exactly the tensors for a generic linear operator is an exponentially
costly task. However, there exist finite-size, simple definitions for many operators, such
as the Fourier transform F, the differential operators D(−i∇) and the ˆS±introduced
above. For instance, as shown in Figure 2, the position operator ˆ xover a given set
of qubits uses tensors with bond dimension D= 2. This operator can be used as a
primitive to implement V(x) orD(−i∇). Similar representations are found for ˆS±, see
[34]. In general, we expect that a generalization of the T T-cross approximation can be
used to extrapolate some operators only using a polynomially large set of their elements
[68].
2.3. Finite-precision linear algebra
We can complete the MPS and MPO representations for vectors and operators in a
function space, with operations that allow us to implement linear algebra algorithms.
The first operations are the scalar product between states ⟨ψ|ξ⟩(Figure 1(b)) and the
matrix elements of an operator between possibly different states ⟨ψ|O|ξ⟩. When ψ,ξ
andOare expressed as tensor networks, these operations become contractions of quasi-
2D arrangements of tensors. These contractions—which enable us to compute distances
between vectors and projections and transformations of states—are stable if the MPS
and MPO are in canonical form, and are only limited by the finite precision of the
computer.
While scalar products are numerically exact operations, computing the action of
an MPO onto a state O|ψ⟩(Figure 1(c)) or estimating a linear combination of two
vectors α|ψ⟩+β|ξ⟩are tasks that, when implemented naively, can lead to a polynomial
increase in the tensors sizes. We address this in a canonical way, defining these problems
as optimization tasks, searching for the MPS that has a bounded tensor size, and which
best approximates either computation. As described in Appendix A, we minimize the
distance to
|θ⟩= argminθ∈MD∥|θ⟩ −O|ψ⟩∥2, (14)
or to
|θ⟩= argminθ∈MD∥|θ⟩ −α|ψ⟩ −β|ξ⟩∥2, (15)
with the space MDof MPS with bounded resources D.
As described below, the solution of these nonlinear optimization problems enables
us to implement a finite-precision linear algebra approximation to many other tasks:
from estimating time evolution of states under continuous equations, to implementing
gradient descent or approximate diagonalization of Hermitian operators. While similar
strategies have been developed in the context of DMRG [40], a crucial difference is
in that the optimizations described above the states under consideration do not share
any tensors with the target states, O|ψ⟩,|ψ⟩or|ξ⟩. This means that we can, using L
matrix product states of bond-dimension D, implement an optimization that in DMRG
would require environments of size L×Dand operations that scale as L2times worse,
in general [41].Global optimization of MPS in quantum-inspired numerical analysis 8
2.4. Benchmark problem
In the sections below, we explore various algorithms to solve static PDEs of Hamiltonian
type, searching for the function f(x) that satisfies
[D(−i∇) +V(x)]f(x) =E0f(x), (16)
where E0is the lowest eigenvalue of the Hamiltonian operator H=D(−i∇) +V(x).
In this work, we will explore algorithms for solving (16) using harmonic oscillator
potentials as benchmark problems
H=−1
2∇2+1
2x†Ax. (17)
Some benchmarks will be done in one dimension, where A=ω2gives the frequency of the
fundamental mode. However, more generally, we will study two-dimensional problems
where the matrix Acontains some degree of correlation. In this case, the matrix is a
two-dimensional squeezed harmonic oscillator, rotated an angle θandsqueezed a factor
σmin/σmax, where
A=OT(θ) 
1/σ4
max 0
0 1 /σ4
min!
O(θ), (18)
with the orthogonal transformation
O(θ) = 
cos(θ) sin( θ)
−sin(θ) cos( θ)!
. (19)
For this matrix, the ground state energy is given by
E0,0=1
21
σ2
max+1
σ2
min
. (20)
As we will see below, despite the apparent simplicity of the problem, which is gapped,
classical methods require an effort to compute the ground state f(x) and the eigenenergy
E0,0that scales exponentially with the problem discretization size. The use of DMRG
in the context of finding the lowest eigenfunction of a single particle complex potentials
was also explored in [69, 70]
3. Hamiltonian diagonalization algorithms
In this section, we explore and re-engineer a spectrum of optimization algorithms,
designed to find the lowest eigenvalue of a Hermitian operator and solve the quantum-
inspired version of the PDE introduced above (17). The first methods introduced are
based on imaginary-time evolution [6, 7, 8, 71], and they seek the ground state of the
problem by small evolution times in a dissipative equation. We derive methods for
imaginary-time evolution that are better suited for non-local problems, and which are
based on a finite-precision linear algebra approach—i.e. reimplementations of Euler,
Runge-Kutta and other Taylor expansions of the evolution operator. We then describe
methods that directly address the energy functional, minimizing it through gradientGlobal optimization of MPS in quantum-inspired numerical analysis 9
search. By upgrading a gradient descent algorithm, we connect these techniques to
Arnoldi diagonalizations, in which the eigenvalues are sought using not one direction,
but a Krylov subspace of fixed dimension. We discuss how these algorithms can
be interpreted under the light of optimization methods, and thus implemented in a
numerically stable way in a context of finite precision.
3.1. Imaginary-time evolution
Imaginary-time evolution implies the solution of a Schr¨ odinger equation in which the
change of variables t→(−iβ) has been performed
∂β|ψ(β)⟩=−H|ψ⟩. (21)
For a non-degenerate Hamiltonian with a gapped spectrum, the normalization of the
solution to this equation gives the solution to our Hamiltonian problem (16)
|f0⟩= lim
β→∞1
⟨ψ|ψ⟩|ψ(β)⟩, (22)
for any initial condition |ψ(0)⟩that has a nonzero overlap with the solution |f0⟩. This
may be seen from the expansion of the evolved state in the basis of eigenstates of the
Hamiltonian {fn}
|ψ(β)⟩=X
ne−βEn|φn⟩. (23)
Here, all states with energies En> E 0attenuate exponentially faster than the desired
state, which is the only one surviving in the limit β→ ∞ after normalization.
Formally, (21) is solved by the evolution operator U(β) =e−βH. In practice, the
computation of |ψ(β)⟩is implemented by repeated application of some linear operator
that approximates U(∆β) for brief periods of time. This is even more relevant in
scenarios where the MPO representation of Hmay be efficient, but its exponential may
have an exponential bond dimension [3].
Since we are focused on MPOs with long range and possibly multiple layers of
tensors (see for instance the QFT MPO in [34]), we will focus on algorithms that can
be approximated using the finite-precision linear algebra techniques described above. In
particular, we will compare four explicit methods that construct Taylor approximations
of the evolution operator at discrete times ψk=ψ(βk)
(i)Euler method. This is an explicit, first-order Taylor approximation of the
evolution, with an error O(∆β) and simple update with a fixed time-step βk=
k×∆β
ψ0=ψ(β0), (24)
ψk+1=ψk−∆βHψ k,fork= 0,1, . . . , N −1.
(ii)Improved Euler or Heun method. This is a second order, fixed-step explicit
method which uses two matrix-vector multiplications to achieve an error O(∆β2)
ψk+1=ψk−∆β
2[v1+H(ψk−∆βv1)],with (25)
v1=Hψk.Global optimization of MPS in quantum-inspired numerical analysis 10
(iii)Fourth-order Runge-Kutta method. This popular algorithm achieves an error
O(∆β4) using four matrix-vector multiplications and four linear combinations of
vectors
ψk+1=ψk+∆β
6(v1+ 2v2+ 2v3+v4),with (26)
v1=−Hψk,
v2=−H
ψk+∆β
2v1
,
v3=−H
ψk+∆β
2v2
,
v4=−H(ψk+ ∆βv3).
(iv)Runge-Kutta-Fehlberg method. This is an adaptative Runge-Kutta method
[72] that combines the fifth and fourth order Runge-Kutta methods to find the
error at each step, obtaining a method of order O(∆β4) with an error estimator
of order O(∆β5) used to tune ∆ β. Good convergence requires a good initial step
size. Moreover, the theoretical cost of Runge-Kutta-Fehlberg (RKF) starts with
six evaluations of matrix-vector multiplication, plus repetitions of evolution steps
when the step size is rejected.
Higher-order methods, while approximating the imaginary-time evolution with lower
errors, also require a larger number of evaluations. Understanding how many operations,
and how many integration steps, is crucial to decide among all methods. These cost
considerations also led us to discard implicit methods—e.g. an Euler implicit formula,
(1 + ∆ β/2H)ψk+1= (1−∆β/2)Hψk—which, even though can be implemented using
solvers [34], they have a very large and very uncontrolled cost per step.
Another important consideration of all explicit methods is stability. These methods
approximate U(∆β) for a short evolution step, in a way in which it is intrinsically
unstable. To be more precise, the eigenvalues of the approximate transformation
Wn(∆β)≃U(∆β) +O(∆βn) deviate from the contracting limit |λ(Wn)|≮1. Thus,
when ∆ βis not well calibrated, a repeated application of the integration rule will lead to
a blow-up of the eigenstates that we wish to exponentially attenuate. In Appendix B we
present a discussion of this limitation and the stability of the imaginary-time evolution
methods.
There are alternative imaginary-time evolution techniques, such as using the
Suzuki-Trotter approximation to separately apply the potential V(ˆx) and the differential
D(−i∇) terms [73] operators. However, this approximation is only useful if the
separate application of the operators is more efficient, which is not the case for our
non-local MPOs. Other possibilities are the application of second-order differencing
or Chebyshev polynomial expansion as propagation schemes for the time-dependent
Schr¨ odinger equation [73] or the use of real-time evolution techniques [7]. These time
evolution methods have also been combined with the DMRG method [9, 10] leading to
time-dependent DMRG methods, enhancing the TEBD techniques [74, 35]. Finally, the
implementation of Runge-Kutta methods to approximate the time evolution within theGlobal optimization of MPS in quantum-inspired numerical analysis 11
DMRG algorithm [75] has also been studied. These Runge-Kutta methods, together
with other approximations of the evolution such as Pad´ e, Arnoldi, or Lanczos methods,
have also been implemented for Matrix Product State variational ansatz (vMPS)
algorithms [41].
3.2. Approximate diagonalization methods
Imaginary-time evolution is designed to solve the imaginary-time equation. The fact
that the solution converges to the ground state of the problem is a lucky and useful
accident. However, we can find better optimization strategies by directly addressing the
energy functional associated to the equation we want to solve (16)
f0= argminψE[ψ] =⟨ψ|H|ψ⟩
⟨ψ|ψ⟩. (27)
Given this formulation of the problem, we can now devise various strategies to engineer
a trajectory |ψ(β)⟩that aims at minimizing E[ψ], such as the steepest gradient descent,
the momentum gradient descent, or the adaptative gradient algorithm (AdaGrad) [76].
3.2.1. Gradient descent Our first proposal is to use the steepest gradient descent
optimization method, updating our estimate of the solution along the direction of fastest
energy decrease
ψk+1=ψk+ ∆βδE
δψ. (28)
The step ∆ β < 0, known as the learning rate in machine learning, determines how
far we move along the direction of the functional gradientδE
δψ. For a normalized state
|ψ(β)⟩, the functional derivative of the energy has an analytical expression
δE
δψ= (H− ⟨H⟩I)ψ, (29)
giving us a closed algorithm for the function’s update.
Note that similar techniques have been used in other areas, such as MPS-based
machine learning [77, 78, 32]. However, while in machine learning the learning rate
∆βis a meta-parameter of the algorithm, with delicate tuning, our functional admits
an exact choice that eagerly optimizes the cost functional at each step. The optimum
step is computed by substituting the update rule (28) in the cost functional (27), and
minimizing analytically it with respect to ∆ β,
∆β−=⟨(H− ⟨H⟩I)3⟩ −p
⟨(H− ⟨H⟩I)3⟩2+ 4⟨(H− ⟨H⟩I)2⟩3
2⟨(H− ⟨H⟩I)2⟩2. (30)
Compared to the imaginary-time evolution, the steepest descent algorithm provides
us with automatic calibration of the solution’s update, which aims directly at the ground
state. Besides this, compared to DMRG [9, 10], this type of steepest descent is a global
update of the MPS that can be easily implemented with MPS linear algebra techniques,
adaptable to very sophisticated MPOs.Global optimization of MPS in quantum-inspired numerical analysis 12
3.2.2. Improved gradient descent The update rule (28) is a particular case of a more
general update that involves moving on the plane spanned by the previous solution ψk
and the derivative ξ=Hψ
ψk+1=v0ψ+v1ξ. (31)
In the steepest descent v0= 1−∆β⟨H⟩andv1= ∆β, but we can improve this algorithm
by searching the optimal vector vT= (v0, v1) that minimizes the total cost function.
Fortunately, the average energy on this two-dimensional subspace has a simple
expression, given by the ratio of two quadratic forms
E[χ] =E(v) =v†Av
v†Nv, (32)
with Hermitian matrices
A= 
⟨ψ|H|ψ⟩ ⟨ψ|H|ξ⟩
⟨ξ|H|ψ⟩ ⟨ξ|H|ξ⟩!
= 
⟨ψ|H|ψ⟩ ⟨ψ|H2|ψ⟩
⟨ψ|H2|ψ⟩ ⟨ψ|H3|ψ⟩!
, (33)
N= 
⟨ψ|ψ⟩ ⟨ψ|ξ⟩
⟨ξ|ψ⟩ ⟨ξ|ξ⟩!
= 
⟨ψ|ψ⟩ ⟨ ψ|H|ψ⟩
⟨ψ|H|ψ⟩ ⟨ψ|H2|ψ⟩!
. (34)
The critical points of the cost function (32) satisfy
δE
δv∗=1
v†Nv(Av−E(v)Nv) = 0 . (35)
This is a generalized eigenvalue equation
Av=λNv, (36)
where the minimum eigenvalue λ=E(v) gives the optimal energy for the k-th step,
and the associated direction vprovides the steepest descent on the plane.
This generalized eigenvalue problem (36) can be solved analytically or numerically,
giving us both a new estimate of the energy and a new state |ψk+1⟩. Unlike the gradient
descent, the cost of each step is dominated by the computation of the three expectation
values ⟨H⟩,⟨H2⟩and⟨H3⟩, plus the linear combination of MPS (31).
3.2.3. Arnoldi iteration The improved gradient descent is a nonlinear optimization
on a two-dimensional Krylov space, spanned by K2:= lin {|ψk⟩, H|ψk⟩}. We
can improve on this method by enlarging the size of the Krylov basis KL=
lin{|ψk⟩, H|ψk⟩, . . . , HL−1|ψk⟩}, implementing an Arnoldi-like diagonalization method
[79]. Just like in Section 3.2.2, the goal is to construct two matrices AandNthat
keep track of the matrix elements of the Hamiltonian operator, and the scalar products
between Krylov vectors. Solving the generalized eigenvalue equation (36) provides a
vector v∈CLwith which to compute the next approximation to the problem ψk+1.
Unlike conventional Arnoldi or Lanczos methods, we have to work in a scenario of
limited precision. This means that we cannot construct a perfectly orthogonal basis, or
orthogonalize a set of existing Krylov vectors in MPS form—any linear combination of
MPS is subject to some truncation and rounding errors—. The solution to this is to
keep track of the scalar products of the basis in a separate matrix N.Global optimization of MPS in quantum-inspired numerical analysis 13
However, there are other strategies from iterative methods that can be reused. In
particular, we implement the Arnoldi iteration with an implicit restart technique. In our
approach, the algorithm starts with one vector ψ0, and progressively we grow the Krylov
basisKLfrom L= 1 up to a size which may be either Lmax, or a smaller size, if we
detect that the approximate Krylov vector HL|ψk⟩is not exactly linearly independent
of the others, i.e., the matrix Nbecomes singular for size L+ 1. At this point, we solve
the generalized eigenvalue problem (36) and use the best eigenstate as |ψk+1⟩.
Lanczos and Arnoldi methods have been previously applied to MPS states in
different frameworks. These methods, in combination with DMRG, are useful to
compute dynamical correlations functions [80], a technique later improved by the
introduction of an adaptive Lanczos-vector method [81, 82]. Lanczos and Arnoldi
methods have also been used within the context of the variational DMRG algorithm
[40, 41] for the evolution of one-dimensional quantum states. All these frameworks
focused principally in DMRG-like methodologies, potentially requiring MPS of a larger
bond dimension than the techniques presented here. On the same spirit as in this work,
one must remark a complementary technique, which is the use of Chebyshev filters
expansions [83] as iterative schemes that enable approximate diagonalization around
regions of the spectrum.
4. Method calibration and comparison
In this section, we compare the methods presented in Sections 3.1 and 3.2, characterizing
their performance and practical cost. This study uses as benchmark the one-dimensional
quantum harmonic oscillator PDE (17), discretized using a finite-differences method of
order 2, over a spatial interval [ −L/2, L/2] with L= 10, and a symmetric choice of
points around x= 0.
As figures of merit for the benchmark, we compare four values against the exact
ones obtained from the finite difference solution of the problem for the same interval
and discretization: (i) the difference between the energy obtained by the method Eapprox
and the one from an exact diagonalization E0
ε=|E0−Eapprox|, (37)
(ii) the 1-norm distance of the approximated solution ψapprox to the numerically exact
ground state φ0
∥φ0−ψapprox∥1, (38)
(iii) the infidelity with respect to φ0
1−F= 1− |⟨φ0|ψapprox⟩|2, (39)
and (iv) the standard deviation of the energy on the final state,
σ=p
⟨H2⟩ − ⟨H⟩2. (40)
As mentioned before, different algorithms have a different cost in terms of expensive
MPS operations. Thus, rather than compare simulations in terms of steps, we compareGlobal optimization of MPS in quantum-inspired numerical analysis 14
0 25000 50000 75000 100000
Cost10−1310−1010−710−410−1ε
(a)(a)(a)(a)(a)(a)(a)
0 50000 100000
Cost10−610−410−2100/bardblϕ0−ψapprox/bardbl1
(b)(b)(b)(b)(b)(b)(b)Euler
Improved Euler
Runge-Kutta
Runge-Kutta-Fehlberg
Gradient descent
Arnoldi (nv= 2)
Arnoldi (nv= 3)
0 20000 40000 60000 80000
Cost10−510−310−1101σ
(c)(c)(c)(c)(c)(c)(c)
0 50000 100000
Cost10−1410−1110−810−510−21−F
(d)(d)(d)(d)(d)(d)(d)
Figure 3. Evolution of the figures of merit versus number of expensive MPS-MPS and
MPO-MPS operations, when solving the one-dimensional quantum harmonic oscillator
PDE over the interval x∈[−L/2, L/2], with L= 10, using a discretization with n= 8
qubits and ∆ x=L/(2n−1). We plot (a) the absolute error εin the estimation of the
eigenvalue (37), (b) the norm-1 distance (38) and (d) infidelity (39) with respect to
the numerically exact solution, and (c) the standard deviation (40) of the Hamiltonian
over the computed eigenstate.
them in terms of a rescaled cost ck=C×k, where Cis the number of MPS combinations
and MPO-MPS multiplications of each algorithm, and kis the steps so far executed. The
factors Care displayed in Table 1 for all imaginary-time and diagonalization methods.
EulerImproved
EulerRunge-Kutta RKFGradient
descentIRArnoldi
Cost 7 14 28 43 13 [6( nv−2) + 13] /(nv−1)
Table 1. Cost factors Cof the numerical methods.
Figure 3 illustrates the four metrics as a function of the number of operations
performed, when solving a harmonic oscillator with an 8-qubit discretization, and no
bounds on the MPS size. This comparison is biased towards favoring the imaginary-time
evolution methods, because we use as time step ∆ βan optimal value, without includingGlobal optimization of MPS in quantum-inspired numerical analysis 15
Figure 4. Computational cost of each algorithm, to estimate the Hamiltonian’s lowest
eigenvalue with an error (37) below 10−10, when solving one-dimensional harmonic
oscillator over the interval x∈[−L/2, L/2], with L= 10 and a discretization of n
qubits, ∆ x=L/(2n−1).
the cost of this calibration in the analysis. Since all metrics exhibit similar behaviors,
we will focus on the error in the estimation of the energy ε, a metric whose lowest value
is obtained for the implicitly restarted Arnoldi method with nv= 3.
We can reinterpret the same data, studying the cost to reach a given error in
the energy. Figure 4 explores the growth in the cost of all algorithms, for an error
tolerance ε < 10−10, as a function of the number of qubits in the discretization.
All the imaginary-time evolution algorithms perform worse than the approximate
diagonalization techniques, except the Euler method, which seems on par with the
improved gradient descent (or Arnoldi with nv= 2). However, the Euler method
requires extra fine-tuning since one need to find the optimum ∆ β. In our analysis, we
have neglected the cost of such extra fine-tuning step. Since approximate diagonalization
methods do not require any fine-tuning, we thus conclude that they constitute the best
overall choice for solving PDEs in a quantum-inspired way.Global optimization of MPS in quantum-inspired numerical analysis 16
0 1000 2000 3000
Steps10−1310−910−510−1ε
(a)10−6
10−810−10
10−1210−14
NE
10−1410−1110−8
Tolerance45678maxD
(b)Arnoldi (nv= 3)
Figure 5. Error in the estimation of the energy ε(37) for different truncation
tolerances in the MPS-MPS and MPO-MPS operations, for an Arnoldi diagonalization
with nv= 3 vectors and a discretization of n= 8 qubits. NE stands for numerically
exact, indicating that the truncation tolerance is the machine precision of floating point
operations.
The simulations in Figures 3 and 4 were derived using MPS with arbitrary bond
dimension, where the truncation tolerance is set to the machine’s floating point precision,
i.e., they are numerically exact (NE). Figure 5(a) shows that the methods remain stable
when we impose more strict truncation tolerances. A consequence of this reduction of
precision is also a reduction in the final error in the energy by a comparable magnitude.
However, the bond dimensions are also decreased, as shown in Figure 5(b). This reveals
that the eigenvalue structure of the highly-differentiable functions is very favorable, and
possibly formed by exponentially decaying Schmidt numbers, which justifies the good
precision in the final computation.
5. Benchmark: squeezed harmonic oscillator
The simulations in the previous section were intentionally small, focusing on the
comparison between the MPS methods developed before. In this section we move to
larger problems, comparing the best method so far—the Arnoldi diagonalization—to two
state-of-the-art methods: (i) open-source Arnoldi and Lanczos diagonalization packages
(ARPACK [84] and Primme [85, 86]), and (ii) a DMRG-like algorithm [87] which seeks
the ground state through a local optimization of the MPS—which is possible because
the finite-differences MPO representation of the PDE is small. As benchmarks, we use
much larger problems, solving the two-dimensional quantum harmonic oscillator (17),
with a rotation angle θ=π/4 and a large squeezing σmin/σmax= 0.5, both of which
increase the entanglement needs of the MPS function representation [34].
Since we do not have access to the intermediate steps of the open-source libraries,
we can only study the convergence of the energy for our Arnoldi method and our DMRGGlobal optimization of MPS in quantum-inspired numerical analysis 17
0 20000 40000 60000
Iterations10−910−610−3100εn= 9
DMRG
MPS,nv= 2
MPS,nv= 5
MPS,nv= 10
Vector,nv= 2
Vector,nv= 5
Vector,nv= 10
Figure 6. Error in the approximation of the ground state energy εwith the number
of steps for the squeezed harmonic oscillator with n= 9 qubits per dimension.
code. Figure 6 shows the decrease in the energy approximation error εas a function of
the algorithmic steps, for a problem discretized with n= 9 qubits per dimension. We
observe that both the DMRG and Arnoldi methods obtain low errors, but the Arnoldi
iteration needs more steps to converge. This is not intrinsic to the use of MPS, but it is
a consequence of the Arnoldi algorithm itself, as confirmed by implementing the same
computation with vectors instead of MPS. The minor discrepancies between MPS and
vectors can be attributed to the MPS truncation tolerance of 10−15, which is slightly
above the processor’s intrinsic precision.
Figures 7(a)-(b) present a more detailed comparison that takes into account the
relative costs, measured in comparison to the DMRG solution with n= 3 qubits
per dimension. Now, the ”vectors” algorithm refers to the state-of-the-art Arnoldi
algorithms in Arpack and Primme, both of which provide very similar metrics. The
time required by the MPS methods outperforms asymptotically the execution time of
the vector implementation (see figure 7(a)), which can be explained by exponential
growth in memory for the vector representation, as compared to the bounded needs of
the MPS states (see memory costs in figure 7(c)). Notably, in this problem in which the
MPO is very simple and does not have a large depth, the DMRG algorithm outperforms
all other methods, converging to the final solution in less steps and less time, and using
exponentially less memory.
We interpret this speed-up as a consequence of the renormalization steps implicit
in DMRG. Remember that DMRG solves the problem ”locally” in the tensor
representation, minimizing a quadratic representation of the energy functional with
respect to pairs of neighboring tensors. Mathematically this means that DMRG, when
sweeping from site 0 up to site 2n-1 is solving the problem along the X and the Y
directions, starting with the longest length scales first, and refining the solution within
each sweep, and between consecutive sweeps.Global optimization of MPS in quantum-inspired numerical analysis 18
34567891011121314
nper dimension10−1102105108Relative time (s)
34567891011121314
nper dimension101103105Relative steps
DMRG
MPS,nv= 2
MPS,nv= 5
MPS,nv= 10
Vector,nv= 2
Vector,nv= 5
Vector,nv= 10
34567891011121314
nper dimension103105107109Memory (ﬂoats)
Figure 7. Results of the resolution of the squeezed harmonic oscillator equation (17).
(a) Relative time, (b) relative states, (b) memory.
34567891011121314
nper dimension10−1410−1210−1010−8ε
DMRG
MPS,nv= 2
MPS,nv= 5
MPS,nv= 10
Vector,nv= 2
Vector,nv= 5
Vector,nv= 10
34567891011121314
nper dimension10−2100102Time toε<10−7(s)
Figure 8. Squeezed harmonic oscillator results using finite difference interpolation
(17). (a) ε, (b) time to ε <10−7.
Motivated by the DMRG success, we have repeated our simulations, but now using
the solution with n, after interpolation, as a better starting point for the problem
with n+ 1 qubits per dimension (see Appendix D for the interpolation algorithm). InGlobal optimization of MPS in quantum-inspired numerical analysis 19
Figure 8(a) we observe the errors for each method. All Arnoldi and Lanczos methods
exhibit comparable accuracies, which are slightly surpassed by the DMRG optimization.
However, now the MPS Arnoldi method becomes competitive with DMRG in terms
of execution time, and has better asymptotic behavior than the vector-based Arnoldi
methods—which exhibit exponential growth in time to solution, due to the unavoidable
exponential growth in the memory required to store the intermediate vectors.
6. Conclusions
This work has explored the solution of partial differential equations of Hamiltonian
form using quantum-inspired algorithms, where functions and operators are encoded as
MPS and MPOs. Focusing on the search of ground-states of Hamiltonian PDEs, we have
developed four algorithms based on imaginary-time evolution, steepest gradient descent,
improved nonlinear gradient descent and Arnoldi diagonalization. These methods have
been compared over large problems with DMRG optimization of the functions encoded in
MPS, and with state-of-the-art exact diagonalization techniques (Arpack and Primme)
that work with vector representations of the problem.
The main conclusion is that all tensor-network based algorithms work adequately
and exhibit exponential advantages in memory over vector representations of the same
numerical analysis problems. Among TN methods, we find a surprising result, which
is that imaginary-time evolution is less efficient than a self-calibrated gradient descent,
when one considers the cost of operations. Moreover, gradient descent can be improved,
including multiple states in the optimization, developing an implicitly restarted Arnoldi
method that is stable under the finite precision MPS algebra. We also find that
for problems where the PDE can be encoded in a small MPO, DMRG excels at the
optimization of the state. Arnoldi-like methods can achieve a comparable performance
to DMRG and generalize to much more complex MPOs with the use of interpolation.
And all MPS-based diagonalization techniques perform exponentially better than vector-
based methods also in time.
Indeed, a particularly relevant aspect of the evolution and optimization algorithms
explained in this work is that they are based on a general framework of approximate
MPS-MPS and MPO-MPS operations. This framework is flexible and enables working
with long-range interactions. Given the good performance exhibited in this work, we
believe that these methods will become useful also in the study of many-body physics
problems, joining other techniques used for long-range interactions, such as Chebyshev
expansions [88], the generalized TDVP algorithm [89, 90], the MPO WI,IImethod [71],
or the variational uniform matrix product state (VUMPS) [91] algorithm, that combines
the DMRG and MPS tangent space concepts.
This work also opens many different paths for optimization and generalization. A
rather obvious speedup would come from the parallelization of the MPS linear algebra
operations (e.g. linear combinations) or the construction of the Krylov basis in the
Arnoldi methods. These algorithms may also be improved in terms of precision andGlobal optimization of MPS in quantum-inspired numerical analysis 20
stability, extended to other tasks, such as diagonalization in other areas of the spectrum,
computation of excited states, or the inclusion of symmetries. The precision of the
quantum-inspired representation may also be enhanced, replacing finite differences with
Fourier interpolation [34] and an exponentially more efficient encoding of derivative
operators [47]. Finally, as described in Appendix E, the techniques put forward in
this work can be extended to other PDEs, such as equations with sources, via suitable
reformulations that convert those equations into optimization problems.
7. Acknowledgments
This work was funded “FSE invierte en tu futuro” through an FPU Grant FPU19/03590
and by MCIN/AEI/10.13039/501100011033. JJGR and PGM acknowledge support
from Proyecto Sin´ ergico CAM 2020 Y2020/TCS-6545 (NanoQuCoCM), Spanish project
PID2021-127968NB-I00 funded by MCIN/AEI/ 10.13039/501100011033/FEDER, UE,
and CSIC Interdisciplinary Thematic Platform (PTI) Quantum Technologies (PTI-
QTEP+). JJGR acknowledges support by grant NSF PHY-1748958 to the Kavli
Institute for Theoretical Physics (KITP). LT aknowledges the “Plan Nacional
Generaci´ on de Conocimiento” PGC2018-095862-B-C22. The authors also gratefully
acknowledge the Scientific computing Area (AIC), SGAI-CSIC, for their assistance
while using the DRAGO Supercomputer for performing the simulations, and Centro de
Supercomputaci´ on de Galicia (CESGA) for access to the supercomputer FinisTerrae.
Appendix A. MPS algebra
In this work we implement many algorithms using MPS and MPOs. We consider MPS
as vectors within an algebra, constituted by the minimum set of operations to implement
any algorithm based on the application of quantum operators on quantum states and
the linear combination of quantum states.
The contraction of an MPO with an MPS increases the bond dimension of the
resulting MPS, and for large bond dimensions, this operation becomes very costly, as the
number of coefficients of the MPS increases quadratically with the bond dimension [1].
Thus, to efficiently apply quantum operators on quantum states, truncation algorithms
have been proposed to truncate the bond dimension of the tensors of the MPS, while
still representing the same quantum state up to a certain error. The simplest approach
is to directly truncate the Schmidt coefficients while performing the SVD [4]. More
stable and precise approaches are based on a variational truncation [4].
In our MPS methods we implement a two-site simplification algorithm to
approximate an MPS quantum state |ψ⟩with bond dimension Dψby projecting it
in the subspace of MPS with bond dimension Dϕ, MPS Dϕ, such that Dϕ< D ψ. The
resulting MPS |ϕ⟩ ∈MPS Dϕis the state that minimizes the distance d(ψ, ϕ)
ϕ= argminϕ∈MPS Dϕd(ψ, ϕ), (A.1)Global optimization of MPS in quantum-inspired numerical analysis 21
where
d(ψ, ϕ) =∥ψ−ϕ∥2=⟨ψ|ψ⟩+⟨ϕ|ϕ⟩ − ⟨ψ|ϕ⟩ − ⟨ϕ|ψ⟩. (A.2)
This is a bilinear function with respect to any tensor in |ϕ⟩, leading to a functional that
can be efficiently optimized via an iterative algorithm.
In this algorithm, we locally optimize the MPS to minimize the distance (A.2) with
respect to a site Dusing its canonical form. The minimization condition is
∂
∂Di
αβd(ψ, ϕ) =∂
∂Di∗
αβd(ψ, ϕ) = 0 . (A.3)
Then we can compute
∂
∂Di∗
αβ⟨ψ|ϕ⟩=Ui
α,β,∂
∂Di∗
αβ⟨ϕ|ϕ⟩=Di
α,β, (A.4)
where Uiis constructed from the contraction of the left environment Li, the right
environment Riandψi, and this leads to the approximation of ϕifor that given iteration,
Di
αβ=Ui
αβ. (A.5)
A diagrammatic representation of one step of the algorithm is depicted in Figure A1. In
practice, we use a two-site local optimization algorithm by contracting two neighboring
sites, as it adapts the bond dimension at each step to achieve greater accuracy and
stability. Both algorithms are implemented in the same way, by expressing the solution
Di
αβ=Ui
αβas an antilinear form that maps the local tensor Di
αβof|ϕ⟩to the scalar
product of ⟨ϕ|ψ⟩. With this algorithm, we decrease the bond dimension of the MPS
while maintaining the precision up to a certain tolerance. This allows us to avoid the
exponential increase in the application of quantum operators as MPO.
We can extend this algorithm to the approximation of a linear combination of states,
i.e., to solve the problem
argminϕ∈MPSϕ−NX
n=1αnψn2
. (A.6)
In this case we have an antilinear form for every scalar product ⟨ϕ|ψn⟩, and the solution
is a weighted linear combination of the solution for each state
Di
α,β=NX
n=1αnU(n)i
αβ. (A.7)
Appendix B. Time step limitation in Runge-Kutta methods
In the imaginary-time evolution Runge-Kutta methods in Section 3.1, correctly choosing
the step size ∆ βis key to optimize the convergence to the ground state. This convergence
is determined by the contraction ratio rm,Global optimization of MPS in quantum-inspired numerical analysis 22
D∗D
⟨ϕ|ϕ⟩ ≡D∗C
⟨ϕ|ψ⟩ ≡L R
∂
∂D∗(⟨ψ|ψ⟩+⟨ϕ|ϕ⟩ − ⟨ϕ|ψ⟩ − ⟨ψ|ϕ⟩) = 0 DC
L R =D∗C A E
B∗F∗G
H∗→
D∗D B F
B∗F∗H
H∗→
→
Figure A1. Diagrammatic representation of one step of the simplification algorithm
for MPS.
rm=λm(β, E m)
λ0(β, E 0), m > 0, (B.1)
where λ(∆β, E n) is the eigenvalue of the Runge-Kutta method for the nth energy level
and step size ∆ β. The smaller this value, the faster the component of the corresponding
m-th energy level goes to zero. The form of the contraction ratio depends on how the
imaginary-time evolution method approximates the evolution operator, so the optimum
∆βvaries for each numerical method. To study this, let us plot the contraction ratios
of the energy levels corresponding to the quantum harmonic oscillator (17) 3-qubit
discretization for a range of values of ∆ β. We represent them for rm≤ |1|, as outside this
interval convergence is not assured. We observe that higher-order methods approximate
better the exact theoretical evolution, especially for the smallest ∆ βs, as expected due
to the smaller global error associated with them. However, although the Euler method
fails to reproduce the behavior of the evolution for larger values of ∆ β, it can achieve
smaller contraction ratios and consequently faster convergence.Global optimization of MPS in quantum-inspired numerical analysis 23
Figure B1. Contraction ratio rmof the one-dimensional quantum harmonic oscillator
(17) 3-qubit discretization for ∆ β∈[0,1]. The different line styles correspond to
the used methods: theoretical evolution (solid), Euler (dash-dotted), improved Euler
(dotted), and Runge-Kutta (dashed).
Not only the step size ∆ β, but also the initial state ψ(β0), plays a key role in the
convergence. For c0=⟨φ0|ψ(β0)⟩= 0, the state cannot evolve to the ground state, so
we need to make sure that the initial state has c0̸= 0. In addition, if the ground state
has some ci= 0, i̸= 0, their corresponding contraction ratios will not be considered for
the computation of ∆ βopt. Thus, the optimum step size will depend on the initial state.
In general, the spectrum of the problem that we are aiming to solve is unknown, so
we cannot compute the contraction ratios or the cncoefficients. In these cases, we will
use a minimization algorithm that finds the optimum step size according to a certain
figure of merit acting as cost function. This algorithm requires applying the method as
many times as optimization steps necessary to reach the optimum step size but allows
us to find a better approximation than simple inspection in fewer executions of the
numerical method when a good initial approximation of ∆ βoptis used.
Appendix C. Density matrix renormalization group
The density matrix renormalization group (DMRG) is a numerical algorithm originally
developed for the study of the low-energy physics of quantum many-body physics [9, 10].
This approach was adapted to the formalism of MPS [35, 36, 37], extending it to a
quantum information perspective. Since then, it constitutes one of the most important
MPS algorithms, as it performs an iterative variational optimization of the MPS. This
optimization procedure is mainly used to obtain the ground state of the Hamiltonian
of a quantum many-body system. Other applications are the computation of excited
states [92, 93], dynamical systems [94], or the real-time evolution of quantum systems
via time-dependent DMRG (tDMRG) [74, 95, 96].
The DMRG algorithm approximates the dominant eigenvector of the Hamiltonian
matrix H, where the eigenvector is written as an MPS. To perform this it uses the
variational method, i.e., for a given quantum state |ψ⟩,Global optimization of MPS in quantum-inspired numerical analysis 24
E0≤⟨ψ|H|ψ⟩
⟨ψ|ψ⟩, (C.1)
where E0is the ground state energy and the inequality is saturated for |ψ⟩=|ψ0⟩,
where |ψ0⟩is the ground state of H. The variation of the MPS is performed locally on
each site M, such that at each step the energy E
E=⟨ψ[M]|HM|ψ[M]⟩
⟨ψ[M]|NM|ψ[M]⟩, (C.2)
where |ψ[M]⟩is the MPS rewritten as a tensor, and HMandNMare the Hamiltonian
and normalization matrices, that correspond to the environments of tensors ψ[M]and
ψ∗[M]for⟨ψ|H|ψ⟩and⟨ψ|ψ⟩[1]. Then, we solve the minimization problem for site M
at each step given by
min
ψ[M] 
⟨ψ[M]|HM|ψ[M]⟩ −λ⟨ψ[M]|NM|ψ[M]⟩
. (C.3)
By taking the derivative of the previous expression with respect to ψ∗[M]we obtain that
the minimization problem is equivalent to the following generalized eigenvalue problem
HM|ψ[M]⟩=λNM|ψ[M]⟩. (C.4)
Therefore, we need to solve a system of equations at each step to obtain the optimized
site. This process is iteratively repeated on each site on the MPS, and several
sweeps of the MPS are performed until convergence, for which |ψ⟩is a high precision
approximation of the dominant eigenvector of H.
Appendix D. Interpolation
The previous description is a discrete approximation of continuous functions and
operators. We can increase its precision using interpolation. For functions that meet
the requirements for the spectral method, we can arbitrarily increase this precision—
up to O(e−r2n),for analytic functions, where ris a problem-dependent constant
[47]—using Fourier interpolation. This technique reconstructs the original continuous,
bandwidth-limited, infinitely differentiable function from the momentum space discrete
approximation
f(x)∝X
{si}e−ipsx⟨s|F|f(n)⟩. (D.1)
.
We can also use the finite difference method to perform the interpolation. We
can reconstruct the ( n+ 1)-qubit function from the n-qubit one by dividing the spatialGlobal optimization of MPS in quantum-inspired numerical analysis 25
discretization step ∆ xiby two, and approximating the middle points as
f(x+ ∆x/2)≈f(x) +∆x
2∇f(x+ ∆x/2) (D.2)
≈f(x) +f(x+ ∆x)−f(x)
2, (D.3)
where ∆ x= (∆ x1, . . . , ∆xd).
Appendix E. Extension of the global optimization to other PDEs
The global optimization methods can solve another type of PDEs as long as we can
rewrite them in the form of a cost function that will lead to the solution. A possible
application is the resolution of PDEs with a source term g(x),
Df(x) =g(x), f(x), g(x)∈CN. (E.1)
We solve (E.1) by minimizing the cost functional C[f],
C[f] =||Df(x)−g(x)||2. (E.2)
The gradient descent algorithm 3.2.1 then follows the optimization path
fn+1=fn+ ∆βD†(Df−g) =fn+ ∆βD†w, (E.3)
and using the optimum ∆ βfor each step,
∆βopt=−⟨w|DD†|w⟩
⟨w|DD†DD†|w⟩. (E.4)
The efficient resolution of PDEs with source terms is key, due to the multiple
applications of such equations. Some important source PDEs are Poisson’s equation
and the heat equation, which have many applications beyond their original use, as
many models can be reduced to them. An interesting example is the use of the heat
equation in finance, as the Black-Scholes equation [97] can be expressed in terms of it.
References
[1] Rom´ an Or´ us. A practical introduction to tensor networks: Matrix product states and projected
entangled pair states. Annals of Physics , 349:117–158, oct 2014.
[2] Garnet Kin-Lic Chan, Anna Keselman, Naoki Nakatani, Zhendong Li, and Steven R. White.
Matrix product operators, matrix product states, and ab initio density matrix renormalization
group algorithms, 2016.
[3] Jacob C Bridgeman and Christopher T Chubb. Hand-waving and interpretive dance: an
introductory course on tensor networks. Journal of Physics A: Mathematical and Theoretical ,
50(22):223001, may 2017.
[4] Sebastian Paeckel, Thomas K¨ ohler, Andreas Swoboda, Salvatore R. Manmana, Ulrich Schollw¨ ock,
and Claudius Hubig. Time-evolution methods for matrix-product states. Annals of Physics ,
411:167998, 2019.
[5] Shi-Ju Ran, Emanuele Tirrito, Cheng Peng, Xi Chen, Luca Tagliacozzo, Gang Su, and Maciej
Lewenstein. Tensor Network Contractions: Methods and Applications to Quantum Many-Body
Systems . Lecture Notes in Physics. Springer International Publishing, 2020.Global optimization of MPS in quantum-inspired numerical analysis 26
[6] Guifr´ e Vidal. Efficient simulation of one-dimensional quantum many-body systems. Phys. Rev.
Lett., 93:040502, Jul 2004.
[7] Michael Zwolak and Guifr´ e Vidal. Mixed-state dynamics in one-dimensional quantum lattice
systems: A time-dependent superoperator renormalization algorithm. Phys. Rev. Lett. ,
93:207205, Nov 2004.
[8] F. Verstraete, J. J. Garc´ ıa-Ripoll, and J. I. Cirac. Matrix product density operators: Simulation
of finite-temperature and dissipative systems. Phys. Rev. Lett. , 93:207204, Nov 2004.
[9] Steven R. White. Density matrix formulation for quantum renormalization groups. Physical
Review Letters , 69(19):2863–2866, nov 1992.
[10] Steven R. White. Density-matrix algorithms for quantum renormalization groups. Phys. Rev. B ,
48:10345–10356, Oct 1993.
[11] F. Verstraete, M. M. Wolf, D. Perez-Garcia, and J. I. Cirac. Criticality, the area law, and the
computational power of projected entangled pair states. Phys. Rev. Lett. , 96:220601, Jun 2006.
[12] Xie Chen, Zheng-Cheng Gu, and Xiao-Gang Wen. Classification of gapped symmetric phases in
one-dimensional spin systems. Phys. Rev. B , 83:035107, Jan 2011.
[13] Norbert Schuch, David P´ erez-Garc´ ıa, and Ignacio Cirac. Classifying quantum phases using matrix
product states and projected entangled pair states. Phys. Rev. B , 84:165139, Oct 2011.
[14] Thomas Barthel, Carlos Pineda, and Jens Eisert. Contraction of fermionic operator circuits and
the simulation of strongly correlated fermions. Phys. Rev. A , 80:042333, Oct 2009.
[15] Philippe Corboz and Guifr´ e Vidal. Fermionic multiscale entanglement renormalization ansatz.
Phys. Rev. B , 80:165129, Oct 2009.
[16] L. Tagliacozzo, G. Evenbly, and G. Vidal. Simulation of two-dimensional quantum systems using a
tree tensor network that exploits the entropic area law. Physical Review B , 80:235127, December
2009.
[17] Philippe Corboz, Glen Evenbly, Frank Verstraete, and Guifr´ e Vidal. Simulation of interacting
fermions with entanglement renormalization. Phys. Rev. A , 81:010303, Jan 2010.
[18] Philippe Corboz, Rom´ an Or´ us, Bela Bauer, and Guifr´ e Vidal. Simulation of strongly correlated
fermions in two spatial dimensions with fermionic projected entangled-pair states. Phys. Rev.
B, 81:165104, Apr 2010.
[19] Christina V. Kraus, Norbert Schuch, Frank Verstraete, and J. Ignacio Cirac. Fermionic projected
entangled pair states. Phys. Rev. A , 81:052338, May 2010.
[20] Iztok Piˇ zorn and Frank Verstraete. Fermionic implementation of projected entangled pair states
algorithm. Phys. Rev. B , 81:245110, Jun 2010.
[21] Rom´ an Or´ us. Advances on tensor network theory: symmetries, fermions, entanglement, and
holography. The European Physical Journal B , 87(11), nov 2014.
[22] Rom´ an Or´ us. Tensor networks for complex quantum systems. Nature Reviews Physics , 1(9):538–
550, aug 2019.
[23] Edwin Stoudenmire and David J Schwab. Supervised learning with tensor networks. Advances
in Neural Information Processing Systems , 29, 2016.
[24] E Miles Stoudenmire. Learning relevant features of data with multi-scale tensor networks.
Quantum Science and Technology , 3(3):034003, apr 2018.
[25] Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng, Raul Bl´ azquez Garc´ ıa, Gang Su, and Maciej
Lewenstein. Machine learning by unitary tensor network of hierarchical tree structure. New
Journal of Physics , 21(7):073059, 2019.
[26] Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang, and Pan Zhang. Unsupervised generative modeling
using matrix product states. Phys. Rev. X , 8:031012, Jul 2018.
[27] Song Cheng, Lei Wang, Tao Xiang, and Pan Zhang. Tree tensor networks for generative modeling.
Phys. Rev. B , 99:155131, Apr 2019.
[28] Tom Vieijra, Laurens Vanderstraeten, and Frank Verstraete. Generative modeling with projected
entangled-pair states. arXiv preprint arXiv:2202.08177 , 2022.
[29] Samuel T Wauthier, Bram Vanhecke, Tim Verbelen, and Bart Dhoedt. Learning generative modelsGlobal optimization of MPS in quantum-inspired numerical analysis 27
for active inference using tensor networks. arXiv preprint arXiv:2208.08713 , 2022.
[30] Friederike Metz and Marin Bukov. Self-correcting quantum many-body control using reinforcement
learning with tensor networks. arXiv preprint arXiv:2201.11790 , 2022.
[31] William Huggins, Piyush Patil, Bradley Mitchell, K Birgitta Whaley, and E Miles Stoudenmire.
Towards quantum machine learning with tensor networks. Quantum Science and technology ,
4(2):024001, 2019.
[32] Fergus Barratt, James Dborin, and Lewis Wright. Improvements to gradient descent methods for
quantum tensor network machine learning, 2022.
[33] Tianyi Hao, Xuxin Huang, Chunjing Jia, and Cheng Peng. A quantum-inspired tensor network
method for constrained combinatorial optimization problems. arXiv preprint arXiv:2203.15246 ,
2022.
[34] Juan Jos´ e Garc´ ıa-Ripoll. Quantum-inspired algorithms for multivariate analysis: from
interpolation to partial differential equations. Quantum , 5:431, apr 2021.
[35] F. Verstraete, D. Porras, and J. I. Cirac. Density matrix renormalization group and periodic
boundary conditions: A quantum information perspective. Phys. Rev. Lett. , 93:227205, Nov
2004.
[36] U. Schollw¨ ock. The density-matrix renormalization group. Rev. Mod. Phys. , 77:259–315, Apr
2005.
[37] Ulrich Schollw¨ ock. The density-matrix renormalization group in the age of matrix product states.
Annals of Physics , 326(1):96–192, jan 2011.
[38] Jutho Haegeman, J. Ignacio Cirac, Tobias J. Osborne, Iztok Piˇ zorn, Henri Verschelde, and Frank
Verstraete. Time-dependent variational principle for quantum lattices. Physical Review Letters ,
107(7), aug 2011.
[39] Laurens Vanderstraeten, Jutho Haegeman, and Frank Verstraete. Tangent-space methods for
uniform matrix product states. SciPost Physics Lecture Notes , jan 2019.
[40] Salvatore R. Manmana. Time evolution of one-dimensional quantum many body systems. In AIP
Conference Proceedings . AIP, 2005.
[41] Juan Jos´ e Garc´ ıa-Ripoll. Time evolution of matrix product states. New Journal of Physics ,
8(12):305, dec 2006.
[42] Dominic W. Berry. High-order quantum algorithm for solving linear differential equations. Journal
of Physics A: Mathematical and Theoretical , 47(10):105301, feb 2014.
[43] Dominic W. Berry, Andrew M. Childs, Aaron Ostrander, and Guoming Wang. Quantum
algorithm for linear differential equations with exponentially improved dependence on precision.
Communications in Mathematical Physics , 356(3):1057–1081, oct 2017.
[44] Ashley Montanaro and Sam Pallister. Quantum algorithms and the finite element method.
Physical Review A , 93(3), mar 2016.
[45] Andrew M. Childs, Jin-Peng Liu, and Aaron Ostrander. High-precision quantum algorithms for
partial differential equations. arXiv e-prints , page arXiv:2002.07868, February 2020.
[46] John Preskill. Quantum computing in the NISQ era and beyond. Quantum , 2:79, aug 2018.
[47] Paula Garc´ ıa-Molina, Javier Rodr´ ıguez-Mediavilla, and Juan Jos´ e Garc´ ıa-Ripoll. Quantum
fourier analysis for multivariate functions and applications to a class of schr¨ odinger-type partial
differential equations. Phys. Rev. A , 105:012433, Jan 2022.
[48] Michael Lubasch, Jaewoo Joo, Pierre Moinier, Martin Kiffner, and Dieter Jaksch. Variational
quantum algorithms for nonlinear problems. Physical Review A , 101(1), jan 2020.
[49] Sam McArdle, Tyson Jones, Suguru Endo, Ying Li, Simon C. Benjamin, and Xiao Yuan.
Variational ansatz-based quantum simulation of imaginary time evolution. npj Quantum
Information , 5(1), sep 2019.
[50] Oleksandr Kyriienko, Annie E. Paine, and Vincent E. Elfving. Solving nonlinear differential
equations with differentiable quantum circuits. Phys. Rev. A , 103:052416, May 2021.
[51] Martin Knudsen and Christian B. Mendl. Solving Differential Equations via Continuous-Variable
Quantum Computers. arXiv e-prints , page arXiv:2012.12220, December 2020.Global optimization of MPS in quantum-inspired numerical analysis 28
[52] S. V. Dolgov, B. N. Khoromskij, and I. V. Oseledets. Fast solution of parabolic problems in the
tensor train/quantized tensor train format with initial application to the fokker–planck equation.
SIAM Journal on Scientific Computing , 34(6):A3016–A3038, 2012.
[53] Lorenz Richter, Leon Sallandt, and Nikolas N¨ usken. Solving high-dimensional parabolic pdes using
the tensor train format, 2021.
[54] Sergey Dolgov, Boris N. Khoromskij, Alexander Litvinenko, and Hermann G. Matthies.
Polynomial chaos expansion of random coefficients and the solution of stochastic partial
differential equations in the tensor train format. SIAM/ASA Journal on Uncertainty
Quantification , 3(1):1109–1135, jan 2015.
[55] Martin Eigel, Max Pfeffer, and Reinhold Schneider. Adaptive stochastic galerkin FEM with
hierarchical tensor representations. Numerische Mathematik , 136(3):765–803, nov 2016.
[56] Alec Dektor, Abram Rodgers, and Daniele Venturi. Rank-adaptive tensor methods for high-
dimensional nonlinear PDEs. Journal of Scientific Computing , 88(2), jun 2021.
[57] Alex Gorodetsky, Sertac Karaman, and Youssef Marzouk. A continuous analogue of the tensor-
train decomposition. Computer methods in applied mechanics and engineering , 347:59–84, 2019.
[58] Matanya B. Horowitz, Anil Damle, and Joel W. Burdick. Linear hamilton jacobi bellman equations
in high dimensions. In 53rd IEEE Conference on Decision and Control . IEEE, dec 2014.
[59] Elis Stefansson and Yoke Peng Leong. Sequential alternating least squares for solving
high dimensional linear hamilton-jacobi-bellman equation. In 2016 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , pages 3757–3764, 2016.
[60] Alex Gorodetsky, Sertac Karaman, and Youssef Marzouk. High-dimensional stochastic optimal
control using continuous tensor decompositions. The International Journal of Robotics Research ,
37(2-3):340–377, 2018.
[61] Sergey Dolgov, Dante Kalise, and Karl K. Kunisch. Tensor decomposition methods for high-
dimensional hamilton–jacobi–bellman equations. SIAM Journal on Scientific Computing ,
43(3):A1625–A1650, 2021.
[62] Mathias Oster, Leon Sallandt, and Reinhold Schneider. Approximating the stationary bellman
equation by hierarchical tensor products, 2019.
[63] Rui Hong, Ya-Xuan Xiao, Jie Hu, An-Chun Ji, and Shi-Ju Ran. Functional tensor network solving
many-body schr¨ odinger equation. Phys. Rev. B , 105:165116, Apr 2022.
[64] Andrei Chertkov and Ivan Oseledets. Solution of the fokker–planck equation by cross
approximation method in the tensor train format. Frontiers in Artificial Intelligence , 4, 2021.
[65] Hai-Jun Liao, Jin-Guo Liu, Lei Wang, and Tao Xiang. Differentiable programming tensor
networks. Phys. Rev. X , 9:031041, Sep 2019.
[66] Christof Zalka. Simulating quantum systems on a quantum computer. Proceedings of the Royal
Society of London. Series A: Mathematical, Physical and Engineering Sciences , 454(1969):313–
322, jan 1998.
[67] Lov Grover and Terry Rudolph. Creating superpositions that correspond to efficiently integrable
probability distributions. arXiv e-prints , pages quant–ph/0208112, August 2002.
[68] Sergey Dolgov and Robert Scheichl. A hybrid Alternating Least Squares – TT Cross algorithm
for parametric PDEs, July 2018.
[69] M. A. Martin-Delgado, J. Rodriguez-Laguna, and G. Sierra. Single-Block Renormalization Group:
Quantum Mechanical Problems. Nuclear Physics B , 601(3):569–590, May 2001.
[70] Javier Rodriguez-Laguna. Real Space Renormalization Group Techniques and Applications, July
2002.
[71] Michael P. Zaletel, Roger S. K. Mong, Christoph Karrasch, Joel E. Moore, and Frank Pollmann.
Time-evolving a matrix product state with long-ranged interactions. Phys. Rev. B , 91:165112,
Apr 2015.
[72] Erwin Fehlberg. Classical fifth-, sixth-, seventh-, and eighth-order runge-kutta formulas with
stepsize control. NASA Technical Report , 287, 1968.
[73] C Leforestier, R.H Bisseling, C Cerjan, M.D Feit, R Friesner, A Guldberg, A Hammerich,Global optimization of MPS in quantum-inspired numerical analysis 29
G Jolicard, W Karrlein, H.-D Meyer, N Lipkin, O Roncero, and R Kosloff. A comparison
of different propagation schemes for the time dependent schr¨ odinger equation. Journal of
Computational Physics , 94(1):59–80, 1991.
[74] A J Daley, C Kollath, U Schollw¨ ock, and G Vidal. Time-dependent density-matrix
renormalization-group using adaptive effective hilbert spaces. Journal of Statistical Mechanics:
Theory and Experiment , 2004(04):P04005, apr 2004.
[75] Adrian E. Feiguin and Steven R. White. Time-step targeting methods for real-time dynamics
using the density matrix renormalization group. Phys. Rev. B , 72:020404, Jul 2005.
[76] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research , 12(61):2121–2159, 2011.
[77] Alex A. Gorodetsky and John D. Jakeman. Gradient-based optimization for regression in the
functional tensor-train format. Journal of Computational Physics , 374:1219–1238, 2018.
[78] Jinhui Wang, Chase Roberts, Guifre Vidal, and Stefan Leichenauer. Anomaly detection with
tensor networks, 2020.
[79] W. E. ARNOLDI. The principle of minimized iterations in the solution of the matrix eigenvalue
problem. Quarterly of Applied Mathematics , 9(1):17–29, 1951.
[80] Karen A. Hallberg. Density-matrix algorithm for the calculation of dynamical properties of low-
dimensional systems. Phys. Rev. B , 52:R9827–R9830, Oct 1995.
[81] P. E. Dargel, A. Honecker, R. Peters, R. M. Noack, and T. Pruschke. Adaptive lanczos-vector
method for dynamic properties within the density matrix renormalization group. Phys. Rev. B ,
83:161104, Apr 2011.
[82] P. E. Dargel, A. W¨ ollert, A. Honecker, I. P. McCulloch, U. Schollw¨ ock, and T. Pruschke. Lanczos
algorithm with matrix product states for dynamical correlation functions. Phys. Rev. B ,
85:205119, May 2012.
[83] Andreas Holzner, Andreas Weichselbaum, Ian P. McCulloch, Ulrich Schollw¨ ock, and Jan von Delft.
Chebyshev matrix product state approach for spectral functions. Phys. Rev. B , 83:195115, May
2011.
[84] R. B. Lehoucq, D. C. Sorensen, and C. Yang. ARPACK Users’ Guide . Society for Industrial and
Applied Mathematics, 1998.
[85] Andreas Stathopoulos and James R. McCombs. PRIMME: PReconditioned Iterative MultiMethod
Eigensolver: Methods and software description. ACM Transactions on Mathematical Software ,
37(2):21:1–21:30, 2010.
[86] Lingfei Wu, Eloy Romero, and Andreas Stathopoulos. Primme svds: A high-performance
preconditioned SVD solver for accurate large-scale computations. SIAM Journal on Scientific
Computing , 39(5):S248–S271, 2017.
[87] Juan Jos´ e Garc´ ıa-Ripoll. mps C++ GitHub repository, 2018.
[88] F. Alexander Wolf, Ian P. McCulloch, Olivier Parcollet, and Ulrich Schollw¨ ock. Chebyshev matrix
product state impurity solver for dynamical mean-field theory. Phys. Rev. B , 90:115124, Sep
2014.
[89] Thomas Koffel, M. Lewenstein, and Luca Tagliacozzo. Entanglement entropy for the long-range
ising chain in a transverse field. Phys. Rev. Lett. , 109:267203, Dec 2012.
[90] Jutho Haegeman, Christian Lubich, Ivan Oseledets, Bart Vandereycken, and Frank Verstraete.
Unifying time evolution and optimization with matrix product states. Phys. Rev. B , 94:165116,
Oct 2016.
[91] V. Zauner-Stauber, L. Vanderstraeten, M. T. Fishman, F. Verstraete, and J. Haegeman.
Variational optimization algorithms for uniform matrix product states. Phys. Rev. B , 97:045145,
Jan 2018.
[92] M. Chandross and J. C. Hicks. Density-matrix renormalization-group method for excited states.
Phys. Rev. B , 59:9699–9702, Apr 1999.
[93] Alberto Baiardi, Anna Kl´ ara Kelemen, and Markus Reiher. Excited-state dmrg made simple with
feast. Journal of Chemical Theory and Computation , 18(1):415–430, 2022. PMID: 34914392.Global optimization of MPS in quantum-inspired numerical analysis 30
[94] Eric Jeckelmann. Dynamical density-matrix renormalization-group method. Phys. Rev. B ,
66:045114, Jul 2002.
[95] Steven R. White and Adrian E. Feiguin. Real-time evolution using the density matrix
renormalization group. Phys. Rev. Lett. , 93:076401, Aug 2004.
[96] Ulrich Schollw¨ ock. Methods for time dependence in DMRG. In AIP Conference Proceedings . AIP,
2006.
[97] Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of
Political Economy , 81(3):637–654, may 1973.