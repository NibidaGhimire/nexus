Automatic Interaction and Activity Recognition from Videos of
Human Manual Demonstrations with Application to Anomaly Detection
Elena Merlo1,2, Marta Lagomarsino1,3, Edoardo Lamon1,4, and Arash Ajoudani1
Abstract — This paper presents a new method to describe
spatio-temporal relations between objects and hands, to recog-
nize both interactions and activities within video demonstrations
of manual tasks. The approach exploits Scene Graphs to
extract key interaction features from image sequences while
simultaneously encoding motion patterns and context. Addi-
tionally, the method introduces event-based automatic video
segmentation and clustering, which allow for the grouping of
similar events and detect if a monitored activity is executed
correctly. The effectiveness of the approach was demonstrated in
two multi-subject experiments, showing the ability to recognize
and cluster hand-object and object-object interactions without
prior knowledge of the activity, as well as matching the same
activity performed by different subjects.
I. INTRODUCTION
The comprehension of human activities enables machines
to understand and interpret the visual information they acquire
and make informed decisions based on what they see [1].
Many robotic applications rely on video comprehension,
such as autonomous navigation, interactions with objects,
and human-robot interaction. In particular, in the latter,
it is of paramount importance to be able to recognize
human activities and predict their outcomes, for instance
in scenarios where robots provide assistance in Activities
of Daily Living (ADL) [2] or collaborate with humans in
industrial settings to achieve a common task [3]. Moreover,
by understanding an activity demonstrated by humans, robots
could learn the task structure and replicate it [4]. Both
domestic and industrial activities are characterized by the
prominent presence of manual tasks. However, being able
to transfer detailed knowledge about human manipulation
activities is a challenging problem. When addressing such
a challenge, a question arises: ”What do we want the robot
to learn?”, and two are the potential answers [4]. The first
resides in teaching the robot to reproduce exact movements,
at the trajectory level [5], while the second is to replicate the
outcome of the activity, understanding the activity goal and
context, not only in physical but also in semantic terms [6]–
[9]. The first approach has been extensively studied within the
Learning by Demonstration paradigm and in general requires
This work was supported by the ERC-StG Ergo-Lean (Grant Agreement
No. 850932) and in part by the European Union under NextGenerationEU
(FAIR - Future AI Research - PE00000013).
1Human-Robot Interfaces and Interaction Laboratory, Istituto Italiano di
Tecnologia, Genoa, Italy. elena.merlo@iit.it
2Dept. of Informatics, Bioengineering, Robotics, and Systems Engineering,
University of Genoa, Genoa, Italy.
3Dept. of Electronics, Information and Bioengineering, Politecnico di
Milano, Milan, Italy.
4Dept. of Information Engineering and Computer Science, University of
Trento, Trento, Italy.accurate motion capture tools which might not always be
available, as in the case the activity demonstration consists of
pre-recorded videos. For these reasons, the manuscript will
focus on the second approach.
A pioneering study in this direction is [10], which leverages
the concept of active vision to comprehend the content of a
series of images, extracting regions of interest and specific
descriptive features. In our framework, the feature selection
has been carefully conducted to enable the description of
activities and facilitate their replication by the robot. Since
we are dealing with manual activities, the primary interest
lies in describing hand-object and object-object interactions.
Our key features, also employed in the cited works, include
the spatial location of hands and objects and their relative
distances. Moreover, we enclose more detailed information
about movements during an interaction, such as the velocity
and direction of hands and objects involved [11].
To provide a more detailed understanding of the interaction
flows, we propose a method to include all relevant features
through a scene encoder and to automatically segment
activities hierarchically, according to the definition of our
taxonomy. Then, we process the segmented time series of
features to capture the temporal patterns of the interactions
and recognize similar ones. The method we propose adopts
an unsupervised approach, which grants flexibility to describe
various types of interactions, avoids the challenge associated
with obtaining labeled data, and opens possibilities for
discovering hidden patterns and structures within the data. By
leveraging the intrinsic characteristics of interactions, our goal
is to capture the underlying similarities and differences among
them, enabling the recognition based on such properties rather
than relying on any prior knowledge [12], [13]. Since our
encoding can separate contextual information from motion
information, it enables us to recognize, as similar interactions,
analogous motion patterns with different objects and, at the
same time, distinguish different motion patterns executed with
the same objects. For example, assembling and disassembling
two pieces might involve the same objects but opposite
motions. Finally, individual activities are recognized by
identifying time sequences of frames corresponding to specific
interactions. In this way, by comparing the execution of an
activity with the learned ones through the proposed metrics,
the method could also be used to detect potential anomalies
in the activity’s progress.
The method has been tested in two multi-
subject experiments (see multimedia attachment and
youtu.be/Ftu EHAtH4k)) the capability of the method in
finding similarities in activities executed by different subjectsarXiv:2304.09789v2  [cs.CV]  7 Jul 2023and ii) the potential of the method in detecting in-progress
activity anomalies given a learned activity representation.
The results demonstrate the effectiveness of the proposed
approach in recognizing and clustering hand-object and
object-object interactions and in matching the same activity
executed by different subjects.
II. EVENT-BASED TAXONOMY
In this study, we focus on the analysis of manual activities
that involve the manipulation of objects through human hands.
Henceforth, we will refer to both objects and human hands
asvideo objects , which are, using terminology introduced in
[11], specific entities or regions of interest within a video
sequence that a computer system is able to identify and track
over time.
In our approach, we examine the spatio-temporal relation-
ships between video objects by analyzing the content of
smaller video segments obtained by video segmentation. By
segmenting the video into atomic units and then grouping
these interactions according to logic rules, it is possible to
describe and recognize the activity occurring in the video.
In other words, we are aiming to understand higher-level
activities starting from descriptions of low-level interactions,
adopting a bottom-up hierarchical approach. To further clarify
the problem and the goal of the work, we provide an event-
based taxonomy, which extends the one proposed by Fu et
al.[11] to define each different event:
•anElementary Reaction Unit (ERU) is a set of
consecutive frames within which video objects have
a specific spatio-temporal relationship. The onset of
a new ERU is due to a change in the video objects’
relationship;
•anInteraction Unit (IU) is a time-ordered sequence of
ERUs that involve the same video objects. By grouping
the ERUs, we are able to capture all of the changes that
occur in the spatio-temporal relationship between the
video objects;
•anactivity is a time-ordered sequence of IUs. The
IUs within an activity are logically connected, with the
successful completion of one IU leading to the start of
the next. When two IUs are not connected logically it
means that the activity is changed;
•ajobis a collection of activities which share an overall
common objective.
In particular, the definitions of ERU and IU are based on
[11] (see also Figure 2). However, the taxonomy lacks the
ability to capture the overall structure and context of the
interactions. Therefore, we introduced the activity and job
layers, which allow us to group hierarchically IUs into more
general, human-understandable units.
III. METHODOLOGY
The framework depicted in Figure 1 illustrates the various
stages of the proposed approach. The input of the framework
is a video demonstration of the manipulation task. The
perception module is responsible for extracting the 3D hands’
landmarks and the 3D pose of the objects in the scene. Itshould be noted that the presented method is not limited to
a particular technique to detect hand and object movements.
Instead, it can accommodate various approaches, including
inertial motion capture, marker-based motion capture with
multiple cameras, or RGBD-based markerless motion capture.
Additionally, information on the objects type and interaction
points are required as input. These points are chosen based on
the object properties such as shape, size, and affordance, and
correspond to the suitable grasp frames for object interactions
with hands or other objects [14]. Similarly, providing a
technique to automatically retrieve the object interaction
points is out of the scope of the manuscript. However, SoA
techniques in robotic manipulation and grasping can be
employed [15]. Once extracted, the perception module outputs
are organized per frame into a Scene Graph (SG) structure
[16], [17] that portrays the spatial semantic relationships
among the video objects in the scene. This representation,
enriched with local temporal features, such as video objects’
relative accelerations and velocities, could provide a detailed
and sensitive-to-variation scene description. In this way,
specific changes in the feature space represent variations in the
scene content, leading to the segmentation of the video into
ERUs, IUs, or activities. Once the video has been segmented,
it is possible to recognize and compare different patterns. In
particular, in this manuscript, we will focus on grouping IUs
using clustering techniques, such as centroid- or distribution-
based clustering [18], [19]. This involves comparing the IUs
based on similarity measurements and grouping together the
similar ones. The feature set consists of two components. The
first component encodes motion features at the interaction
level, regardless of which objects are involved. The second
one encodes contextual information, such as the identity
of the objects involved in the interaction. When clustering
motion features, we obtain clusters of context-free IUs that
are characterized by similar motion patterns. Instead, when
clustering contextual information, we group IUs that involve
the same objects being handled. The latter allows us to
identify interactions based on context that are equal in terms
of the specific objects involved, even if their motion features
differ. However, to discriminate IUs, both feature components
are necessary. Therefore, an ensemble-like approach is used,
where the results of each independent clustering are combined
into a single cluster.
A. Scene Encoding
By processing the perception data, each frame representing
a scene is mapped into a SG. A SG Gis defined as the tuple
G= (V,R,E).V={v1,..., v|v|}is the set of video objects
which are interacting with each other (foreground). We discard
not interacting objects since they are considered not relevant in
the scene description (background). Each object is represented
asvi= (ci,Ai), where ciand Airespectively indicate the
category and attributes of the video object. Rrepresents a set
of relationships between the nodes, while Edenotes the edges
between the two interacting video objects [17]. Furthermore,
in order to describe the interactions for each hand separately
as in [20], we will generate nSGs if there are nhandsFig. 1. Overview of the overall framework. Given a video demonstration of a human job, the perception module detects and extracts the 3D objects poses
and hands landmarks positions. In red, the interaction points for each object. This data is organized per frame into a Scene Graph structure, where objects
and hands are the nodes, and edges represent interactions between the connected nodes. The scene’s content is encoded into a feature space, capturing both
motion xxxmand context xxxcat the interaction level. Changes in the feature space indicate variations in the scene, leading to the segmentation of the video into
Interaction Units (IU)s. IUs can be recognized and compared using clustering techniques to group together similar ones.
interacting with the same object. This allows us to capture
the unique interactions of each hand with the objects.
1) Scene Graph nodes: Each node v∈Vin a scene
graph represents a video object, with different attributes
depending on the type. The i-thhand node is represented
byvhi= (chi,Ahi), with chi=IDhi, and Ahi= (LMhi,αhi),
where IDhis the hand identity, αhidenotes the tangential
component of the hand acceleration measured at the middle
finger knuckle, and LMhiis the set of hand landmarks which
represent key locations such as fingertips, knuckles, and wrist.
A subset of LMhi, in particular the landmarks corresponding
to the fingertips of the three middle fingers, represents
the hand interaction points IPhi. Instead, the j-thobject
node is represented by voj= (coj,Aoj), with coj=IDoj, and
Aoj= (pppoj,φφφoj,αoj,IPoj), where IDojis the object identity,
pppojis the position (based on the object centroid), φφφojis
the orientation, αojdenotes the tangential component of the
object acceleration measured at the object centroid, and IPoj
is the set of the interaction points position, computed with
respect to pppoj.
2) Scene Graph edges: An edge ei,j∈Econnects two
video objects viandvjif and only if
di,j=dist(IPk
vi,IPq
vj)≈0,∀k,q∈[1,|IPv|],k̸=q,
where dist(·,·)is the Euclidean distance. In other words,
we determine whether there is an interaction between two
video objects if the minimum Euclidean distance between
each pair of their respective interaction points IPviandIPvj
is reasonably small as in [4]. Each ei,j∈Eis described by a
relationship between the connected nodes viandvj,ri→j∈R.
Each ri→j∈Rincludes three attributes:
•thedistance d i,jbetween the two objects viandvj;
•thejoint motion : whether viandvjare moving jointly;
•therelative motion direction , expressing the direction
along which viis moving towards or away from vj.
To determine whether the two sufficiently close video
objects are moving jointly, we compare their tangential accel-
eration components. If sgn(αvi)andsgn(αvj)are concordant,
we can conclude that viandvjare moving together. In thecase where a hand and an object are jointly moving, we
obtain the in-hand relation between them, i.e. we assume that
the hand holds the object as in [21]. We expand the concept
of the in-hand relationship beyond just a hand and an object,
to determine if multiple objects function as an integral unit,
such as in the case of an assembly.
In addition, the interaction direction is obtained by project-
ing the velocity vector of vionto the frame Tvj= [pppvj,φφφvj]
of the approached/left video object vj; the resulting vector
is transformed into spherical coordinates, i.e., elevation θi,j
and azimuth ϕi,jangles, and radius ρi,j. These coordinates are
then quantized: θQ
i,j=Q(θi,j),ϕQ
i,j=Q(ϕi,j).As a result,
θQ
i,jandϕi,jQdescribe the interaction direction of video
objects using a finite number of integer values. Retrieving
the interaction direction helps to distinguish activities of the
same type, differentiated only by the direction in which the
objects interact with each other. This feature is also useful
to recognize if the interaction is well performed or warped.
This representation goes beyond the simple away/toward
dichotomy and encompasses additional dimensions such as
right/left, bottom/up, in front/behind, and all their possible
combinations [11]. Moreover, another robust point of our
design is the space invariance, by encoding in the same
way interactions that involve similar relative approaching
directions regardless of the video objects’ absolute poses.
3) Feature Couple: The scene representation provided
by the SG can be further reduced by means of a feature
couple, denoted as X= (xxxm,xxxc), where xxxmconveys semantic
motion information ( motion features ), while xxxcabout the
video objects IDs ( context features ). To generate the two
feature vectors, we employ a Dijkstra search algorithm [22]
on the SG, utilizing the distance edge attribute as the cost
metric. This process enables us to identify and consider the
hand-object and object-object interactions that exist along the
optimal path in the graph, starting from the hand node. By
traversing nodes and edges on this path, features are extracted
and stored in the xxxmandxxxcvectors. In the case of a hand-
object interaction between vhiandvoj, and an object-object
interaction between vojandvol, the two vectors will have thisFig. 2. Conceptual illustration of the proposed taxonomy and automatic
video segmentation criteria.
form:
xxxm=h
ahiθQ
hi,ojϕQ
hi,ojjmhi,ojθQ
oj,olϕQ
oj,oljmoj,oli
xxxc=IDhiIDojIDojIDol
Inxxxm, the first feature ahi=sgn(αhi)represents the hand
acceleration categorized into three states: accelerating, de-
celerating, or maintaining a constant velocity (if αhi≈0).
The elements θQ
hi,oj,ϕQ
hi,oj,θQ
oj,ol,ϕQ
oj,olrepresent the direction
of the interactions, while jmhi,oj,jmoj,olencode the joint
motion, i.e. whether the interacting video objects are integral.
Finally, xxxccontains their IDs. Note that all values in the
feature vectors are integers, and each element in xxxmbelongs
to an ordered range with a unit distance between consecutive
values.
B. Event-Based Video Segmentation
Using the proposed encoding from a time series of video
frames, we obtain a time series of X. The video segmentation
can then be easily automatized by examining specific changes
in the feature values. In this paper, we propose the following
rules for the automatic segmentation of ERUs, IUs, and
activities (see Figure 2):
•a new ERU begins when at least one element in X
changes its value;
•a new IU is initiated when there is a change in xxxc.
Specifically, if at least one video object starts or stops
interacting with others, a new IU arises (as in [8]);
•a new activity starts with a specific change in xxxc, i.e.
when the hand starts interacting with a new object.
Let’s explain the segmentation strategy with an example.
Consider the job of filling a box with five tools. The proposed
segmentation would return five similar activities characterized
by the hand interacting with a specific object and would
involve picking up a tool and placing it inside the box. Each
activity would be, in turn, composed of the following four
IUs: (i) the human hand grasping the tool from the storage
area (involving hand-tool and tool-storage area interactions),
(ii) the hand holding the tool in the air far from the storage
area (involving hand-tool interaction but no object-object
interaction), (iii) placing the tool in the box (involving hand-
tool and tool-box interactions), and (iv) the tool becoming
integral with the box (involving no hand-object interaction
but tool-box interaction). A new activity starts when a new
tool is grasped, and the four IUs are repeated.C. Similarity Measures and Clustering
Following the procedure above, motion features xxxmand
context features xxxccan be associated with each video
frame, enabling the description of each IU both in terms of
motion and of the objects involved in the interaction. In this
section, we describe the metrics used to find similar motion
patterns among IUs, and discern IUs that involve the same
objects. Moreover, we propose a machine learning approach
to automatically group IUs describing similar interactions
without previous knowledge about the accomplished activities.
1) Similarity between IUs motion patterns: To measure
the similarity between IUs in terms of motion, we utilize
the multi-dimensional Dynamic Time Warping (DTW). DTW
is a widely-used distance measure for time series data, and
it allows us to compare sequences of different lengths by
warping and stretching the time axis. The resulting distance
reflects the discrepancy between two context-free IUs, i.e.,
how different two IUs are based on their motion patterns.
The lower the distance, the higher the similarity. While the
distance provided by DTW is a promising candidate measure
of the similarity between IUs, determining the condition under
which two video segments should be considered as instances
of the same interaction is not trivial. To overcome this issue,
we exploit the K-means clustering algorithm [18], where a
suitable value of kcan be deduced with the elbow method
applied to the trend of the Within-Cluster Sum of Squares
(WCSS) over k.
2) Similarity between IUs contexts: By definition, each IU
is characterized by the same video objects in interaction, thus
by a constant xxxc. Hence, two distinct xxxcare representative of
different IUs. This means that two IUs context are similar
(actually identical) if their distance is smaller than 1 for each
possible distance metric. Therefore, to cluster all the IUs
involving the same video objects’ interactions, we utilize
DBSCAN [19] with Euclidean distance and ε=1as the
maximum cluster distance.
By combining the two clusterings, we can detect IUs that
present similarities both in terms of motion and context.
D. Anomaly Detection in Activity Execution
The proposed method to segment and distinguish activities
can be particularly useful in applications where the ability
to identify in-progress execution deviations is critical. For
instance in the anomaly detection in human job executions,
a prompt anomaly identification could trigger alerts or
corrective actions.
We assume that an activity is correctly performed when all
IUs are properly executed and in the correct time order. The
algorithm (see Algorithm 1) capitalizes on three lists J,L, and
C. The first one contains all the nominal activities required
to complete the selected job. The second one comprehends
the activities discarded as non-candidates. The latter instead
includes the completed IUs. Land Care initially empty.
During the job execution, the recorded scenes are encoded
and segmented using the above-mentioned procedure. Once a
new IU ui+1starts, the last completed one, ui, is added to list
Cand is compared to all the i-thIUs of each activity in the listFig. 3. Selection of candidates in anomaly detection algorithm. The first
completed IU was compared with the first IUs of all the candidate activities
inJ. This comparison resulted in a division between candidate {J∖L}
and non-candidate Lactivities. Second completed IU is compared with the
second IU of the remaining candidate activities. Once each IU is completed,
it is added to the list C. This process continues until the end of the activity,
when the list Cis checked if it perfectly matches with an activity in {J∖L}.
{J∖L}in terms of context and motion. First, it is checked if
uiand the candidateauihave identical context (i.e., xxxc,i==axxxc,i).
Then, the DTW algorithm is used for the comparison of the
motion features ( DTW (xxxm,i,axxxm,i)). Ifd>dth,i, that activity is
added to the list L, which means that it is no longer a candidate
activity. Otherwise, the activity remains a candidate. If the
current IU is not included in any of the activities belonging
to the list {J∖L}, an alert will be generated to indicate the
occurrence of an anomaly. The process is repeated for each IU
until the current activity ends. Figure 3 shows graphically the
selection of candidates. Once the activity ends, the algorithm
checks if the sequence of the executed IUs, stored in list
C, corresponds to the series of IUs of one of the candidate
activities remaining in {J∖L}(line 28). At this point, all the
activities contained in {J∖L}have the same time-ordered
sequence of iIUs. If a match is found, it means that the
activity was executed correctly. Otherwise, the activity was
not completed and the algorithm reports the anomaly.
IV. EXPERIMENTS
The evaluation of the method consisted of two experiments.
In the first one, we validated the proposed video segmentation
and the IUs similarity recognition, while in the second
experiment, we tested our anomaly detection algorithm on
the activity execution. The experimental setup envisioned an
RGB camera (Intel RealSense D435i) mounted in top shot
(bird’s eye) view, and the image plane was aligned with the
working plane (i.e., the tabletop of the workbench).
Since object detection and hand pose estimation are
extensively studied and beyond the scope of this paper,
in our experiments, we have opted for a reliable marker-
based object detection system and an open-source hand
detector. Specifically, we exploit ArUco markers as a 3D
object detection method. However, the same method did
not give satisfactory results in hand detection due to the
different configurations that the hand can have during the
manipulation task, eventually hiding the marker, and since the
marker hindered the natural movements of participants. For
this reason, we utilized the MediaPipe Hand Detector, whichAlgorithm 1 Anomaly Detection in Activity Execution
1:J←list of the nominal activities of a job
2:dth←distance thresholds for each IU
3:C← {} ▷List of completed IUs
4:L← {} ▷List of non-candidate activities
5:i←1
6:while job is not finished do
7: activity inJ←True
8: while activity is not finished do
9: ifuiis finished then
10: ui←just completed IU
11: xxxm,i,xxxc,i←just completed IU features
12: Add uitoC
13: for each activity a in{J∖L}do
14: ▷Computing similarity with reference IU
15:axxxm,i,axxxc,i←i-th IU of a features
16: ifxxxc,i==axxxc,ithen
17: d←DTW (xxxm,i,axxxm,i)
18: ifd>dth,ithen ▷Non-similar motion
19: Add atoL▷ais non-candidate activity
20: else ▷Non-similar context
21: Add a to L ▷ais non-candidate activity
22: if{J∖L}== / 0then
23: activity inJ←False ▷No such activity in set J
24: i←i+1
25: activity iscorrect ←False
26: ifactivity inJthen
27: fora∗in{J∖L}do
28: ifa∗≡Cthen ▷Activity executed correctly
29: L← {} ,C← {}
30: activity iscorrect ←True
31: break
32: if not activity iscorrect then ▷Activity not completed
features 21 hand landmarks. For simplicity, we detected only
the right hand and evaluated the method in 2D (in the image
plane, which is aligned with the workbench tabletop). As a
result, feature ϕwas not taken into account. The architecture
has been developed in Python, on Ubuntu 20.04 and ROS
Noetic, exploiting the DTAIdistance library [23] for the multi-
dimensional DTW and the k-means clustering and Scikit-
Learn library [24] for the DBSCAN with Euclidean distance.
A. Activity Recognition Validation (Exp 1)
In the first experiment, we asked Nsub=10subjects, 7
males and 3 females ( 25.6±1.2years old), to perform a set of
five activities for Nrep=4repetitions. These activities involved
the following objects: an aluminum profile, a corner joint, a
meter, a box, a polisher, and a black brick (shown in Figure 4).
After the completion of each activity, the associated features
X, normalized in [0,1], were filtered using the opening-closing
filter [25], then the segmentation was performed. This implies
that a new IU is generated only if a particular interaction
persists for a consecutive number of frames. The activities
and IUs segmented by the algorithm are listed below (the
semantic labels are not used by the algorithm and are reported
only to clarify the experiment):
I)Boxing : grasp the profile, put the profile inside the box,
leave the profile inside the box;
II)Measuring : grasp the profile, put the profile close to theFig. 4. Objects used in the experiments.
meter, leave the profile close to the meter;
III)Assembly : grasp the profile, interlock the profile and the
corner joint, leave the complete assembly;
IV)Disassembly : profile and corner joint already interlocked,
disassemble the profile and the corner joint, move away
from the corner while holding the profile;
V)Polishing : grasp the polisher, polish the black brick with
back and forth motions, move away from the black brick
surface while holding the polisher.
To further evaluate the similarity between IUs, we selected a
subset of size NIU=6 of representative IUs:
1) grasp the profile, from measuring ;
2) put the profile inside the box, from boxing ;
3) put the profile close to the meter, from measuring ;
4)interlock the profile and the corner joint, from assembly ;
5)disassemble the profile and the corner joint, from
disassembly ;
6)polish the black brick with back and forth motions, from
polishing .
We initially analyzed the IUs extracted from the Nrep=4
repetitions of each activity by a single subject, computing
similarities in terms of motion features and context fea-
tures. The similarities between the context-free IUs were
evaluated using DTW and a confidence matrix of size
(NIU×Nrep)x(NIU×Nrep)was generated to show the distances
between each couple of IUs. Figure 5 shows the Single
subject Confidence Matrix (SCM) for subject 5, which reports
similarities between context-free IUs. The distances obtained
from DTW were normalized to a range between 0and1, and
dark patches indicate lower distance, hence higher similarity
between IUs.
To evaluate instead the similarity of the same IU performed
by different subjects, we conducted a multi-subject analysis
by computing the distances between all the repetitions across
all the participants. We filled the corresponding confidence
matrix Multi subjects Confidence Matrix (MCM) with size
(NIU×Nsub)x(NIU×Nsub)(see Figure 6). Each element is
computed as:
MCM (a,x,b,y) =1
N2repNrep
∑
m=1Nrep
∑
n=1DTW ((ua,sx,rm),(ub,sy,rn))
where a,b∈[1,NIU],x,y∈[1,Nsub].uaand ubidentify the
couple of IUs we are comparing, while sxandsydenote the
Fig. 5. Single subject Confidence Matrix resulting from the comparison of
context-free IUs. The IUs are respectively: u1grasp the profile, u2put the
profile in the box, u3measure the profile, u4assemble the profile and corner,
u5disassemble the profile and corner, u6polish the black brick surface. ri
corresponds to the i-threpetition. A darker color in the matrix indicates a
higher similarity between the motion patterns of the IUs being compared.
subjects. In other words, each element of MCM represents
the distance between the average performance of all the Nrep
repetitions of IU uafor subject sxand the average performance
of all the Nreprepetitions of IU ubfor subject sy. Distances
were then normalized in [0,1].
We further analyzed the results by clustering all executions
of the IUs by all subjects using k-means with DTW as distance
metric. To determine the optimal number of clusters, we
ran the algorithm 10times for k∈[1,10]and selected the
minimum WCSS for each k. The best value of k=4was
given by the elbow method (see Figure 7 (left)). In particular,
IUs of the assembly ,measuring , and boxing activities were
grouped together in the same cluster (see Figure 7 (top-
center)). At the same time, to compare IUs context, we
used the DBSCAN algorithm with Euclidean distance and
maximum cluster distance ε=1. In this case, the clustering
algorithm identified a total of k=5clusters, one for each
type of IU except for those from assembly anddisassembly
activities, which were grouped into a single cluster (see
Figure 7 (bottom-center)). To obtain a unified combination of
motion and context features characterizing each IU, the two
clustering results are combined together as shown in Figure 7
(right).
Besides, we conducted an additional analysis to verify
the robustness of the detection with respect to variations of
the absolute poses of the involved video objects within the
same IUs. In particular, we asked the same participants to
perform a drilling activity where the IUs included: 1) grasp
the drill, 2) drill the black brick for 5 seconds, and 3) move
away from the surface while holding the drill. This activity
was carried out while changing the absolute position of the
drill and the black brick in three different configurations
(C1, C2, C3), as illustrated in Figure 8 (left), for a total of
30 activities executed. In Figure 8 (right) we reported the
confidence matrix of the second IU.Fig. 6. Multi-subject Confidence Matrix resulting from the comparison of
context-free IUs. The IUs are respectively: u1grasp the profile, u2put the
profile in the box, u3measure the profile, u4assemble the profile and corner,
u5disassemble the profile and corner, u6polish the black brick surface.
s1...s10corresponds to the subject number, which ranges from 1 to 10. A
darker color in the matrix indicates a higher similarity between the motion
patterns of the IUs being compared.
Fig. 7. Results of clustering IUs from multi-subject experiment. K-means
clustering is used for clustering IUs motion patterns. The best kvalue k=4
is found with the elbow method (left) and we obtain clusters m1,...,m4.
Results show u2,u3, and u4as a unique cluster (top-center). DBSCAN is
used for clustering IUs contexts and it results in 5 clusters ( c1,...,c5), one
for each IU except for u4andu5, which are grouped together in a single
cluster (bottom-center). Then the two clustering results are merged to obtain
a unique combination of motion and context features ( mi,ci) for each IU
(right).
B. Anomaly Detection Algorithm Validation (Exp 2)
Our second experiment focused on identifying anomalies
in a job that consisted of two distinct activities: I) polishing
the black brick surface, and II) measuring its thickness.
Activity I) consisted of three IUs: 1) grasp the polisher;
2) polish the brick surface employing back and forth motions;
3) move away from the surface and release the polisher.
Activity II) involved two IUs: 1) grasp the brick; 2) place
the brick close to the meter. The aim of this evaluation is
to demonstrate a potential application that utilizes the ability
to distinguish between similar and non-similar interactions.
While the algorithm is designed for operating online, its
assessment is conducted offline and no computational time-
related performance metrics are reported in this study.
We asked Nsub=7subjects, 6males and 1female ( 25.7±
1.1years old) to perform the job correctly 3times. Using the
procedure described in subsection III-B, we automatically seg-
mented each of the filtered Njobcorrect =21job executions into
Fig. 8. Drilling activity being performed in three different configurations
labeled as C1, C2, and C3 (left). Confidence matrix obtained by calculating
the distance between all the IUs from each configuration (right). s1...s10
corresponds to the subject number, which ranges from 1 to 10.
TABLE I
CONFUSION MATRIX OF ANOMALY DETECTION
IU level Predicted
Negative Positive
ActualNegative 858 52
Positive 0 140
Accuracy = 95 .0%±2.7%
Activity level Predicted
Negative Positive
ActualNegative 252 44
Positive 0 124
Accuracy = 89 .5%±5.1%
Nactivity =2andNIU=5IUs. Subsequently, we asked each
subject to repeat 2 flawed job executions ( J1andJ2), leading
toNjobflawed =14. InJ1, we instructed the subjects to fail the
second IU of activity I) by stopping halfway the polishing,
while in J2, we asked them to fail the second IU of activity
II) by not measuring the brick. The accuracy of the anomaly
detection algorithm presented in Algorithm 1 was evaluated
through a cross-validation repeated for Niterations =10rounds.
In each round, we randomly divided the set of correct
executions, which contains Njobcorrect samples, into a training
set and a test set. The training set contained Njobtraining =14
randomly selected samples, while the test set contained the
remaining correct samples plus all executions of J1and
J2, i.e., Njobtest=Njobcorrect +Njobflawed−Njobtraining =21.
The algorithm requires a nominal execution of the job and
the distance threshold vector dddth. The first was obtained by
calculating the barycenter buiof each IU cluster (containing
Njobtraining samples), i∈[1,NIU]. While each element of dddth
corresponds to dth,i=µui+2σui, where µuiandσuiare mean
and standard deviation of DTW distance on the motion
features of each IU from its corresponding barycenter. Finally,
the nominal activities generated by the sequence of buiare
stored in J. At each iteration, the selected Njobtraining were
used for retrieving buianddth,ifor each IU. The confusion
matrix in Table I shows the accuracy in recognizing correct
and warped executions in terms of IU and activities. Note that
the total number of IUs and activities is obtained using the
following formulas: NtotIU=Njobtest×Niterations ×NIU, and
Ntotactivity =Njobtest×Niterations ×Nactivity .V. DISCUSSION AND CONCLUSIONS
In this paper, we proposed a bottom-up approach for
recognizing activities by analyzing object-object and hand-
object interactions in terms of motion and context information.
Experiments in subsection IV-A demonstrated the capability
of the framework to identify and group similar context-
free interactions. This indicates that our scene encoding and
features-based representation succeeded in comprehensively
describing manual activities and identifying video objects
interaction’ changes during the execution. Strong points of
our encoding include the automatic segmentation of activities
and IUs, as well as the possibility of separating contextual
information from motion information. Considering the motion
features exclusively, we can recognize IUs that involve similar
motion patterns. Within our experiments, DTW identifies
similarities between boxing ,measuring , and assembly (see
SCM in Figure 5). This outcome is motivated by the fact
that the IUs mentioned above share (i) the hand holding of
the profile, (ii) the approaching of a tool (i.e. box, meter,
or corner joint), (iii) the hand release of the profile, and
(iv) the hand moving away. As shown in Figure 6, the IUs
were grouped in an analogous way to SCM, no matter who
executed the activity. We can deduce that our framework
is robust to the variability induced by different subjects in
the activity execution. Moreover, the algorithm was robust
to changes in the absolute poses of the video objects (see
Figure 8). Additionally, our method shows promising results
in leveraging the recognition of both type similar and non-
type similar interactions to ensure job performance without
anomalies. A preliminary experiment indicated a high success
rate and consistent results across iterations using different
references (Table I). Interestingly, the errors committed were
only false positive, meaning that the algorithm occasionally
failed in detecting correctly performed IUs. This was probably
due to the limited size of the training set, which may
not capture the full range of variations in correct activity
executions.
However, we acknowledge several limitations of our
approach, including heavy reliance on accurate object and
hand detection, the lack of occlusion handling, and the
evaluation in 2D with a fixed camera orientation. Moreover,
the current version of our method only describes interactions
for each hand separately and does not consider any link
between activities performed by the two hands, even when
they occur simultaneously. In future works, we plan to address
these limitations and further refine our method to enhance
its capabilities and relevance in robotics applications.
REFERENCES
[1]D. Weinland, R. Ronfard, and E. Boyer, “A survey of vision-based
methods for action representation, segmentation and recognition,”
Computer vision and image understanding , 2011.
[2]J. Massardi, M. Gravel, and ´E. Beaudry, “Parc: A plan and activity
recognition component for assistive robots,” in 2020 IEEE International
Conference on Robotics and Automation (ICRA) . IEEE, 2020.
[3]M. Lagomarsino, M. Lorenzini, P. Balatti, E. D. Momi, and A. Ajoudani,
“Pick the right co-worker: Online assessment of cognitive ergonomics in
human-robot collaborative assembly,” IEEE Transactions on Cognitive
and Developmental Systems , pp. 1–1, 2022.[4]K. Ramirez-Amaro, M. Beetz, and G. Cheng, “Transferring skills
to humanoid robots by extracting semantic representations from
observations of human activities,” Artificial Intelligence , 2017.
[5]S. Albrecht, K. Ram ´ırez-Amaro, F. Ruiz-Ugalde, D. Weikersdorfer,
M. Leibold, M. Ulbrich, and M. Beetz, “Imitating human reaching
motions using physically inspired optimization principles,” IEEE-RAS
International Conference on Humanoid Robots , pp. 602–607, 2011.
[6]D. J. Patterson, D. Fox, H. Kautz, and M. Philipose, “Fine-grained
activity recognition by aggregating abstract object usage,” Proceedings
- International Symposium on Wearable Computers, ISWC , 2005.
[7]E. E. Aksoy, A. Abramov, F. W ¨org¨otter, and B. Dellen, “Categorizing
object-action relations from semantic scene graphs,” Proceedings -
IEEE International Conference on Robotics and Automation , pp. 398–
405, 2010.
[8]M. W ¨achter, S. Schulz, T. Asfour, E. Aksoy, F. W ¨org¨otter, and
R. Dillmann, “Action sequence reproduction based on automatic
segmentation and Object-Action Complexes,” IEEE-RAS International
Conference on Humanoid Robots , pp. 189–195, 2015.
[9]A. Guha, Y . Yang, C. Fermuuller, and Y . Aloimonos, “Minimalist plans
for interpreting manipulation actions,” IEEE International Conference
on Intelligent Robots and Systems , pp. 5908–5914, 2013.
[10] Y . Kuniyoshi, M. Inaba, and H. Inoue, “Learning by Watching:
Extracting Reusable Task Knowledge from Visual Observation of
Human Performance,” IEEE Transactions on Robotics and Automation ,
vol. 10, no. 6, pp. 799–822, 1994.
[11] Y . Fu, A. Ekin, A. M. Tekalp, and R. Mehrotra, “Temporal segmentation
of video objects for hierarchical object-based motion description,” IEEE
Transactions on Image Processing , 2002.
[12] T. S. Kim, J. Jones, and G. D. Hager, “Motion guided attention fusion
to recognize interactions from videos,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2021, pp. 13 076–13 086.
[13] H. Ahn and D. Lee, “Refining action segmentation with hierarchical
video representations,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2021, pp. 16 302–16 310.
[14] P. Ard ´on,`E. Pairet, R. P. A. Petrick, S. Ramamoorthy, and K. S. Lohan,
“Reasoning on grasp-action affordances,” CoRR , 2019.
[15] G. e. Carbone, Grasping in Robotics , ser. Mechanisms and Machine
Science, G. Carbone, Ed. London: Springer London, 2013, vol. 10.
[16] J. Johnson, R. Krishna, M. Stark, L. J. Li, D. A. Shamma, M. S. Bern-
stein, and F. F. Li, “Image retrieval using scene graphs,” Proceedings
of the IEEE Computer Society Conference on Computer Vision and
Pattern Recognition , vol. 07-12-June-2015, pp. 3668–3678, 10 2015.
[17] X. Chang, P. Ren, P. Xu, Z. Li, X. Chen, and A. Hauptmann, “A
Comprehensive Survey of Scene Graphs: Generation and Application,”
IEEE Transactions on Pattern Analysis and Machine Intelligence ,
vol. 45, no. 1, pp. 1–26, 1 2023.
[18] D. Arthur and S. Vassilvitskii, “K-means++ the advantages of care-
ful seeding,” in Proceedings of the eighteenth annual ACM-SIAM
symposium on Discrete algorithms , 2007, pp. 1027–1035.
[19] E. Schubert, J. Sander, M. Ester, H. P. Kriegel, and X. Xu, “DBSCAN
Revisited, Revisited,” ACM Transactions on Database Systems (TODS) ,
vol. 42, no. 3, 7 2017.
[20] C. R. Dreher, M. W ¨achter, and T. Asfour, “Learning Object-Action Re-
lations from Bimanual Human Demonstration Using Graph Networks,”
IEEE Robotics and Automation Letters , 1 2020.
[21] K. Ramirez-Amaro, M. Beetz, and G. Cheng, “Automatic segmentation
and recognition of human activities from observation based on semantic
reasoning,” IEEE International Conference on Intelligent Robots and
Systems , pp. 5043–5048, 10 2014.
[22] E. W. Dijkstra, “A note on two problems in connexion with graphs,” in
Edsger Wybe Dijkstra: His Life, Work, and Legacy , 2022, pp. 287–290.
[23] W. Meert, K. Hendrickx, T. Van Craenendonck, P. Robberechts,
H. Blockeel, and J. Davis, “Dtaidistance,” Aug. 2020. [Online].
Available: https://doi.org/10.5281/zenodo.7158824
[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Van-
derplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of
Machine Learning Research , vol. 12, pp. 2825–2830, 2011.
[25] S. Bhutada, N. Yashwanth, P. Dheeraj, and K. Shekar, “Opening and
closing in morphological image processing,” World Journal of Advanced
Research and Reviews , vol. 14, no. 3, pp. 687–695.