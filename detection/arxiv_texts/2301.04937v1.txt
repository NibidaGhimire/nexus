Density-based clustering with fully-convolutional
networks for crowd ow detection from drones
Giovanna Castellanoa, Eugenio Cotardoa, Corrado Mencara, Gennaro
Vessioa
aDepartment of Computer Science, University of Bari Aldo Moro, Bari, Italy
Abstract
Crowd analysis from drones has attracted increasing attention in recent times
due to the ease of use and aordable cost of these devices. However, how
this technology can provide a solution to crowd ow detection is still an
unexplored research question. To this end, we propose a crowd ow detec-
tion method for video sequences shot by a drone. The method is based on
a fully-convolutional network that learns to perform crowd clustering in or-
der to detect the centroids of crowd-dense areas and track their movement
in consecutive frames. The proposed method proved eective and ecient
when tested on the Crowd Counting datasets of the VisDrone challenge,
characterized by video sequences rather than still images. The encouraging
results show that the proposed method could open up new ways of analyzing
high-level crowd behavior from drones.
Keywords: drones, drone vision, computer vision, deep learning, crowd
ow detection, crowd density estimation, clustering
1. Introduction
Due to population growth and the increasing degree of urbanization, more
and more people live in urban areas. Positive consequences of this trend are
the enrichment of cultural life and the full use of convenient urban infras-
tructure. At the same time, gatherings of people, which can occur for various
reasons, such as political demonstrations, festival celebrations, concerts, and
so on, pose serious challenges to urban security and management. In this per-
spective, automated crowd analysis methods, which typically involve crowd
counting and associated crowd density estimation, have attracted increasing
Accepted manuscript: 10. 1016/ j. neucom. 2023. 01. 059arXiv:2301.04937v1  [cs.CV]  12 Jan 2023attention for their many potential applications [1, 2]. These include the pre-
vention of crowd-induced disasters, such as stampedes, but also other less
critical objectives such as better crowd management at public events and the
design of public spaces and virtual environments.
A cost-eective way to perform automated crowd analysis is by using un-
manned aerial vehicles (UAVs), more commonly known as drones. Indeed,
once equipped with aordable but suciently powerful cameras and GPUs,
drones can become ying computer vision devices that can be rapidly de-
ployed for a wide range of applications, including crowd analysis for public
safety [3]. However, while these perspectives are fascinating, there are also
some drawbacks to be aware of. On the one hand, the computer vision algo-
rithms applied to aerial images are burdened with further diculties because
the problems of scale and point of view are taken to the extreme. On the
other hand, the sophisticated and computationally intensive methods com-
monly applied in this eld do not meet the stringent real-time requirements
imposed by the UAV. In other words, lightweight models that oer a good
compromise between eectiveness and eciency are essential [4].
Crowd analysis with drones has attracted attention in recent years [5].
However, despite signicant progress, the proposed methods still have room
for improvement to address the challenges posed by drones. In this article,
we want to contribute to this research eort by taking it one step further:
instead of considering crowd counting and density estimation in static frames,
we aim to detect crowd ow. This poses a new challenge as the goal is not
only to recognize the presence of people in a single high-altitude scene but
also to determine how crowds ow as a function of time. This is dierent
from people tracking|where the goal is to track a single person or groups of
people|and can lead to useful systems, as it can allow for crowd behavior
analysis for better logistics and disaster prevention [6].
To this end, we propose a method for crowd ow detection from drones
based on fully-convolutional networks (FCNs). The network is trained to rec-
ognize groups of people in each frame and, to do this, simultaneously learns to
perform crowd density estimation and crowd clustering. In this way, groups
of people are identied simply by their centroids, and these are used to trace
the trajectories of the identied groups, following their movement during the
shooting of the drone. We preferred FCNs over other architectures mainly be-
cause of their known eciency. Furthermore, direct learning of crowd clusters
was preferred to avoid a multi-step approach, based on performing density
estimation rst and clustering the resulting density maps later. This was
2done in our previous preliminary work [7] but, while eective, this approach
proved too demanding from a computational point of view. The method
was tested on the recently proposed Crowd Counting 2020 [8] and 2021 [9]
datasets, used annually for the international VisDrone challenge. The pecu-
liarity of these datasets is that they are not characterized by still images but
actually by frames of video sequences that are used here to perform the crowd
ow detection task. It is worth noting that what we actually do to deter-
mine crowd ow is to calculate the inter-frame dierence between centroids;
in other words, it is a kind of \inter-frame density clustering". However,
since this approach allows us to detect the movement of the crowd frame by
frame, we use the expression \ow detection" for the sake of simplicity.
The rest of this paper is structured as follows. Section 2 reviews related
work. Section 3 describes the datasets used. Section 4 presents the pro-
posed method. Section 5 describes the experimental setup and discusses the
results obtained. Section 6 concludes the paper and highlights the future
developments of our research.
2. Related work
There is a large body of knowledge about crowd counting and crowd den-
sity estimation in computer vision, but the trend today is density estimation.
Early work usually applied a person or head detector via a sliding window
on the image. However, although current implementations may be based on
state-of-the-art object detectors such as YOLO [10, 11], these approaches
still provide unsatisfactory results when asked to detect small objects in a
very dense crowd. To alleviate this problem, regression-based methods have
been introduced that directly learn the mapping from an image to the global
people count [1]. However, although these methods make the approach in-
dependent of the precise position of individuals in the crowd, which is very
complex, they ignore the spatial information that can be very useful for pre-
diction. To avoid the diculty of accurately detecting and locating people
in the scene, while using spatial information, the recent trend is to learn
density maps, thus incorporating spatial information directly into the learn-
ing process [12]. Successful solutions include methods that work rst at the
patch level and then fuse local features [13], methods that integrate atten-
tion mechanisms [14], cascade approaches that jointly learn people counting
and density maps [15], methods that improve performance through knowl-
edge distillation [16], and frameworks that simultaneously perform crowd
3counting and localization [17].
However, while eective, these approaches are generally computationally
demanding and do not meet the stringent requirements typically imposed
by UAVs (limited battery, need for real-time responses). How to ne-tune
deep neural architectures to achieve an optimal balance between precision
and performance is an active research area. The VisDrone Crowd Counting
challenge was introduced to encourage research in this direction [8, 9]; nev-
ertheless, the solutions proposed by the participants in the challenge are not
always focused on eciency but rather on eectiveness, as the goal is only
to obtain a low error in counting people. The lowest error was obtained with
TransCrowd [18], based on the increasingly popular Vision Transformer [19].
However, the proposed method only regresses the people count, not provid-
ing density maps that would be useful for detecting crowd ow; furthermore,
transformer-based solutions are known to be computationally expensive.
A promising way to address these problems is to use FCN models. Be-
cause they do not rely on fully-connected layers at all, which are the most
expensive part of processing a neural network, they are a candidate solu-
tion for nding an accurate model without damaging the inference time. An
FCN model for aerial drone imaging was presented in [4], and a similar so-
lution was also proposed in our previous work [20]. However, both methods
were aimed at crowd detection, i.e. discriminating between crowded and un-
crowded scenes; furthermore, they only provide coarse density maps, as the
models have not been trained on people labels.
Human tracking methods based on RGB cameras or other sensors that
use clustering or classication models to track motion have been investigated,
e.g. [21, 22, 23]. However, they are designed to work indoors or by involv-
ing a few people from a frontal perspective. The work most linked to ours,
which takes into account the images captured by drones, is [24] where the
same authors who propose the VisDrone Crowd Counting datasets present
a model that jointly solves density map estimation, localization, and track-
ing. This model diers from ours in that it uses a complex and expensive
pipeline aimed at tracking individual trajectories. Other authors have re-
cently proposed a method for periodic crowd tracking from UAVs based on
a binary linear programming model [25]. However, they experimented with
simulated scenarios that do not consider the crowd detection problem from
a computer vision perspective. As far as we know, there is no work in the
literature addressing crowd ow detection in drone videos, which poses sig-
nicantly dierent challenges than traditional settings. This paper aims to
4ll this gap; in particular, we aim to trace the centroids that identify groups
of people by exploiting the spatial information learned and expressed through
density maps.
3. Materials
The literature landscape is not as populated with datasets for crowd
counting and density estimation from drones with video sequences captured
by optical cameras. The datasets best suited to our purposes for evaluating
crowd ow detection were the VisDrone Crowd Counting 2020 [8] and 2021 [9]
datasets. The two benchmarks are characterized as follows:
•VisDrone Crowd Counting 2020 (CC2020) contains 82 video clips (2 ;460
frames in total) with a resolution of 1920 1080;
•VisDrone Crowd Counting 2021 (CC2021) contains 1 ;807 frames with
a resolution of 640 512. Unlike CC2020, the frames are not arranged
in precisely separated video clips, and this required manual separation
to split the sequences of dierent locations to avoid overtting.
The video sequences were acquired by various drone-mounted cameras,
which shot dierent scenarios in dierent cities in China to maintain diversity.
In both datasets, people were annotated manually with dots in each video
frame, expressed as ( x;y) coordinates in the bi-dimensional plane. How-
ever, CC2020 and CC2021 have signicant dierences. In CC2020, each of
the 82 sequences was captured by a drone hovering over the crowd, allow-
ing for rather static scenes. In CC2021, on the other hand, the drone ies
and sometimes rotates, shooting dierent scenes even if semantically linked.
Furthermore, it should be noted that in CC2020 there is a predominance of
daylight scenes, while in CC2021 there are many frames at night, making
the dataset much more variable in this respect. A nal dierence concerns
altitude: in CC2020 the frames appear to have been shot at a higher altitude
than in CC2021. For both datasets, we randomly held out (sequence-wise)
a fraction of 25% of the total frames evenly split to form validation and test
sets.
Compared to benchmark datasets focused on crowd counting in surveil-
lance scenes, both VisDrone datasets present particular challenges due to the
scenes captured by drones. Object scales can be extremely small due to the
high shooting altitude of drones. Crowds are sparse across the video frames,
5as each can hold a few to dozen people. Finally, the crowds are surrounded
by very dierent backgrounds in dierent sequences.
4. Methods
The proposed method for crowd ow detection is based on an FCN model,
which is used to estimate a \centroid density map"|a heatmap highlighting
crowd centroids|from pairs of consecutive frames of the same video sequence
shot by the drone. Since a crowd can be seen as multiple groups of people not
necessarily following the same direction, the predicted centroids are represen-
tative of these groups of people. The displacements of the centroids detected
between the pairs of frames are then calculated to identify the direction of
movement. Thus, the framework assumes video sequences shot by a drone,
but the network is fed one frame at a time.
The idea of combining a density estimation method with clustering, in-
stead of tracking the movement of each individual, is motivated by the com-
plexity and computational cost of this strategy. In fact, while the direct use
of the crowd density map based on the location of all individuals can retain
more information, it also carries the burden of tracking the trajectory of
each individual in the scene. Moreover, individual tracking can be not only
impractical but also non-essential, as in crowd management scenarios it is
important to recognize the overall ow of people rather than the precise loca-
tion of each person in the scene. A centroid density estimation method will
focus only on high-density areas, i.e. those corresponding to concentrations
of people, and will be inherently robust to occlusion, which would heavily
aect a people detector, especially from a high altitude.
The method is detailed in the following subsections, along with an expla-
nation of how the ground truth is obtained. An illustration schematizing the
overall processing is shown in Fig. 1.
4.1. Ground truth generation
Since we assume that there is no label regarding the location of the cen-
troids within the original drone shot, we followed a simple strategy to derive
a ground truth for the crowd centroids. We rst applied the well-known
Mean Shift clustering algorithm to the head annotations described above
to obtain centroids. Mean Shift is a centroid-based algorithm that updates
the candidates for the centroids as the average of the points within a given
6Figure 1: Schema of the overall method. Pairs of consecutive frames taken by a drone at
timet0andtk(withtk> t0and separated by any time interval) are fed into the neural
network model for crowd clustering. A synthetically generated ground truth helps guide
the learning of the network. The obtained centroid density maps are then thresholded,
and the displacement of the centroids, representing the groups of people, from t0totk, is
used to determine their direction of movement.
region. These candidates are then ltered in a post-processing step to elimi-
nate near-duplicates [26]. Specically, the Scikit-learn implementation of the
Mean Shift algorithm with the recommended default hyperparameters was
used.1Then, following the seminal paper by Zhang et al. [27], if there is a
centroid at pixel xi, we represented it as a delta function (x xi) and we
obtained a \centroid density map" C(x) convolving the delta function with
a Gaussian kernel:
C(x) =KX
i=1(x xi)G(x)
In the formula, Kis the number of centroids; equals 1 when x=xi, 0
otherwise; and Gis the Gaussian kernel. Since the sizes of the heads are
similar in each video sequence and there is no perspective problem as in
[27], we decided to use a xed for each frame. In particular, we have
1At the time of writing, the version of Scikit-learn is 1.1.2.
7Figure 2: Examples of frames and corresponding synthetically generated centroid density
maps.
empirically set = 10, since this value leads to better performance thanks
to the suciently large \condence" activation area. Examples of generated
centroid density maps are shown in Fig. 2.
We used Mean Shift as it is impossible to know the number of clusters
in the crowd in advance, so there was a need for an algorithm that did
not require a pre-specication of the number of clusters. In this way, an
unsupervised learning approach is followed to nd crowd centroids, but when
their location is found, it can be used as a synthetically generated annotation
to guide a supervised learning approach. Although this strategy strongly
depends on the results provided by the specic clustering algorithm chosen,
it allowed us to automate the generation of the ground truth and to guide and
quantitatively evaluate the clustering task performed by the neural network.
In addition, we also experimented with models aimed at performing the
more classic crowd density estimation task. To this end, \crowd density
maps"D(x) were obtained by convolving the same delta function with Gaus-
sian kernel as before but using the original people head annotations.
4.2. Fully-Convolutional Networks
The proposed method is based on an FCN architecture that recognizes
the crowd centroids within each frame and produces the related heatmap. In
8our previous preliminary work [7], the identication of the centroids was del-
egated to a classic clustering algorithm after the generation of crowd density
maps, signicantly penalizing the inference time. In the method proposed
here, instead, the task of nding crowd centroids is integrated directly into
the network training and is performed in a single step.
Fully-convolutional neural networks, originally proposed in [28], perform
only convolution and pooling operations and discard the fully-connected com-
ponent typical of CNNs. Instead of the fully-connected layer, there is a 1 1
convolution with stride 1, which allows, on the one hand, to have fewer pa-
rameters to estimate and, on the other hand, to be able to receive an image
of arbitrary size as an input. It is worth noting that although FCNs have this
desirable property, for better evaluation, we have set the input resolution of
each frame to 640 5123. The architecture of an FCN consists of two
parts: an encoder aims to downsample the input into a lower-dimensional
representation, a decoder aims to upsample the latent representation to the
desired output resolution.
In particular, we have experimented with two dierent neural networks:
anad-hoc FCN designed to be as simple as possible, and a state-of-the-art
FCN already used for crowd counting in more traditional contexts, i.e. Mo-
bileCount [29]. In the ad-hoc implementation, a rescaling layer normalizes
each pixel value in the [0 ;1] range. Then, the encoder part of the model
repeatedly applies four blocks consisting of a convolutional layer with kernel
33, batch normalization, and max pooling with kernel 2 2 until reaching
a latent space Z2R4032128. The decoder upsamples the feature maps with
transposed convolution and batch normalization. Finally, a 1 1 convolu-
tional layer produces the output density map of size 640 5121. The
commonly used ReLU was chosen as the activation function. MobileCount,
on the other hand, is similar but has some more sophisticated architectural
designs. To reduce the input resolution, a 3 3 max pooling layer with
stride 2 is added before the encoding part. The encoder is adapted from Mo-
bileNetV2 [30] by reducing the number of inverted residual blocks from 7 to
4. As for the decoding component, the lightweight ReneNet [31], originally
designed for semantic segmentation, is exploited.
Since we have two datasets with very dierent characteristics, as shown in
Fig. 3 we experimented with three main variants of the above architectures,
which were likely to produce dierent results:
•Single-branch, single-output (SBSO): this architecture uses one of the
9two FCNs described above and learns to directly estimate the centroid
density map. As a loss function, the network is trained to minimize
the mean squared error between the predicted and the ground truth
centroid density map:
L=1
NNX
i=1CP(i) CGT(i)2
2
whereNis the number of samples, CP(i) andCGT(i) are the predicted
and ground truth centroid density map, respectively, and kk2is the
Euclidean distance. Demanding the model to nd groups of people in a
completely unsupervised way would not have provided the network with
a guide to optimize weights to cluster only a specic type of objects,
in our case people, leaving it completely free to separate people from
trees, buildings, etc., which was not our goal.
•Single-branch, multi-output (SBMO): The task of nding crowd cen-
troids may be dicult for the model as the centroids do not represent
objects eectively present within the scene. To try to mitigate this
issue, we also experimented with a multi-output model, which extends
the previous one by learning to simultaneously estimate the centroid
density map and the more classic crowd density map. The main idea is
that the auxiliary task can help the overall network extract the features
that actually characterize the people in the scenes, thus supporting the
main task of density estimation, which only concerns the detection of
crowd centroids. In this case, the loss function is the sum of the above
and the mean squared error between the predicted and the ground truth
crowd density maps.
•Dual-branch, multi-output (DBMO): nally, to improve the capacity of
the previous variant, we also experimented with a multi-output model,
this time characterized by two branches, which are actually two \twin"
autoencoders with the same architecture as before (ad-hoc FCN or Mo-
bileCount). The two branches communicate with each other through
a concatenation of the feature maps produced immediately before the
desired output: one branch is totally dedicated to learning the crowd
density map; the concatenation of both branches contributes to esti-
mating the centroid density map. The loss function is the same as the
previous variant.
10In other words, the multi-output architectures are meant to help the model
learn more about the concept of \crowdedness". Because these variants in-
herently perform crowd counting, in addition to crowd clustering, it is worth
noting that they can be used for the more classical task of pedestrian counting
if needed.
Finally, it is important to emphasize that the neural network models
we have tested are a backbone of the overall framework we propose, which
can be replaced with similar models as desired. Table 1 compares the two
backbones used in this work in terms of the number of parameters. The
proposed FCN has been designed as a classic and straightforward baseline.
It is small, but deliberately without any sophistication to further improve
eectiveness or eciency, to show the feasibility of the proposed method
with a simple model. MobileCount, on the other hand, has more parameters
but was purposely made by the authors as an ecient framework for real-time
crowd counting. In fact, the MobileNetV2-based encoder and the ReneNet-
based decoder were carefully adapted to achieve a good balance between
accuracy and speed, as reported in the article [29].
11Figure 3: Schema of the proposed neural network architectures. The proposed ad-hoc
FCN is schematized in this gure but can be replaced by any similar model as desired.
12Ad-hoc FCN MobileCount
SBSO 0.35 25.199
SBMO 0.35 25.199
DBMO 0.7 50.398
Table 1: Comparison in terms of the number of parameters, in millions, between the pro-
posed ad-hoc FCN and MobileCount, used as the backbone of the proposed architectures.
4.3. Crowd ow detection
Although accurate, FCN-predicted density maps are characterized by
non-normalized values and may contain noise. In particular, while in the
ground truth the background has a value of 0 for construction, there is no
guarantee that its value will remain 0 in the predicted heatmap. To study
how this aects performance, we rst apply a min-max normalization to limit
their range to [0 ;1]; then, we threshold (with an empirically chosen threshold
) the pixel values, so that any value below the threshold is considered to
be the background. A higher threshold essentially maintains the areas where
the model is more condent about the presence of people in those areas.
After identifying the centroids and ltering the images through the empir-
ical threshold , to calculate the actual displacement of the recognized groups
of people, and to determine the direction of their movements, we calculate
the dierence in coordinates of the centroids predicted in dierent frames. In
other words, the displacement of the centroids in an ending frame at time tk
is calculated with respect to the centroids detected in a starting frame at time
t0, and so on. Since we are in two dimensions, this shift is simply calculated
using the Euclidean distance between the ( x;y) coordinates. The primary
assumption is that a given group of people, i.e. a centroid in this case, can
potentially move between successive frames, but its distance from its posi-
tion in the previous frame would be less than the distance from all other
centroids in the current frame. Therefore, to determine if we can associate
the newly predicted centroids to the existing ones, and thus in which direc-
tion they have moved, we calculate the Euclidean distance between each pair
of centroids in each frame and keep the minimum distances between the pairs
to match them. In this way, we can classify each shift as facing one of the
four typical cardinal points, namely North, South, East, and West, plus the
intermediate points North-East, North-West, South-East, and South-West.
The case in which the centroids remain stationary between the two frames is
also considered.
IfPiis the set of centroids predicted at time i, we can distinguish three
13scenarios:
•jP0j=jPkj: in this case, each centroid in P0is simply associated with
theclosest centroid in Pk;
•jP0j>jPkj: in this case, we have fewer centroids in the ending frame.
Indicated by d=jjP0j jPkjjthe dierence in absolute value between
the number of centroids in t0and intk, we will have that dcentroids
int0will remain unmatched. This could indicate a network prediction
error, the fusion of two centroids in t0into one intk, or centroids leaving
the visual eld at time tk;
•jP0j<jPkj: in this case, we have fewer centroids in the starting frame.
We will then have dmore centroids at time tk, which will remain un-
matched and could be indicative of a network prediction error, the
formation of new clusters in tk, or nally the input into the visual eld
of new centroids.
5. Experiments
The proposed method was implemented in Python, using TensorFlow for
the implementation of the deep learning models.2All models were trained
with stochastic gradient descent with the Adam optimizer and a learning
rate of 10 4. As for the threshold, we experimented with values ranging
from 0 to 1, reporting those that gave the best performance. As mentioned
above, all frames have been resized to 640 512. All experiments were
performed on Google Colab Pro, which mainly provides a T4 or P100 GPU.
The performance metrics considered, as well as the results obtained, are
described below.
5.1. Performance metrics
Applying traditional external clustering measures is not feasible in our
density-based context as they assume an exact match between discretized
category labels. Since we are interested in measuring how correctly the de-
tected centroids represent relevant groups of people within the scene, we
propose a new ad-hoc metric (already presented in [7]) which we called Mean
2https://github.com/evgenivs/crowd_flow_detection_drones
14Coordinate Matching Error (MCME). The metric measures the average dis-
tances between the ground truth centroids and the predicted centroids. For
a single frame, let:
A=(
 
CP;CGT
minCP2P;
CGT
min= arg min
CGT2GTCP CGT
2)
B=(
 
CP
min;CGTCGT2GT;
CP
min= arg min
CP2PCP CGT
2)
whereGT=fCGT
1;CGT
2;:::gandP=fCP
1;CP
2;:::gare the ground truth
and the predicted centroids, respectively. Then:
MCME =1
jA[BjX
(CP;CGT)2A[BCP CGT
2
Each centroid of a set (say P) is associated with the nearest neighbor centroid
of the other set ( GT) since both sets of centroids are assumed to represent the
same structure in the data. This association must be symmetric, i.e. from P
toGTand vice versa, because a centroid in Pcan represent part of a cluster
inGTthat has been split into two clusters in P, but it can also represent a
cluster inPthat has merged two clusters in GT. The overall score on a video
sequence can be obtained by averaging the individual scores. The proposed
metric aims to \punish" the model both when the predictions are very far
from the ground truth, and when no real groups are identied or groups that
do not exist are identied. Instead, lower values for MCME will indicate that
the predicted centroids match the ground truth centroids, i.e. they represent
the same clusters and are very close.
In addition, to have an easily interpretable metric from a supervised learn-
ing perspective, we present here another new metric that we call Multiple
Patch Precision-Recall (MPPR). It is based on the repeated generation of
a large number of patches from the predicted and ground truth centroid
density maps and a local comparison between them, allowing for a typical
classication-based evaluation mechanism. Instead of considering only the
coordinates of the centroids, MPPR considers a condence region given by
the size of each patch. Let GTiandPibe the ground truth and predicted
centroid density map for the i-th frame, respectively, and let lx;lybe the
width and height of the frame. Let H(x;y) be a randomly generated point
15in both maps based on whose coordinates a bounding box is drawn; this
bounding box acts as a sliding window that goes down and to the right. Let
w;h be the width and height of the rectangular bounding box, respectively.
Then, MPPR computes the following quantities:
•True positive ( TP): the patches in GTiandPiare both \active", which
means that they both contain (at least) a centroid;
•True negative ( TN): the patches in GTiandPiare both \inactive",
which means that they contain no centroid;
•False positive ( FP): the patch in GTiis inactive, while the patch in Pi
is active;
•False negative ( FN): the patch in GTiis active, while the patch in Pi
is inactive.
This process is repeated nptimes, where npis usually a large number to be
statistically condent that the entire frame is explored. Then, precision and
recall for thei-th frame can be computed as usual. The global MPPR is ob-
tained as the average over all video sequence frames. It should be noted that
we have no guarantee that the bounding box whis fully contained inside
the framelxly. The probability of obtaining a full bounding box depends
solely on the random coordinates of H(x;y). It can be shown that this prob-
ability amounts to(lx w)(ly h)
lxly. This behavior, together with the sampling
ofnpbounding boxes, helps us avoid the bias we would have introduced if
we had used a xed grid of patches or a small np: in fact, in our method,
a slight shift of the bounding box can result in a change of prediction (e.g.,
fromTPtoFN), and this has the benet of reducing the impact of a too
unrealistically optimistic or pessimistic classication. In our experiments, we
setnp= 1000 and w=h= 150.
Finally, the total inference time is calculated, which includes all the pro-
cessing, from estimating the centroid density maps of two consecutive frames
to thresholding and calculating the displacement of the centroids.
5.2. Results
Tables 2 and 4 report the accuracy results obtained by varying the FCN
backbone model and the threshold per dataset (VisDrone CC2020 and
CC2021). It is worth noting that to make the MCME metric independent of
16the resolution of the input frame and for a fair comparison with our previous
work [7], the table reports the normalized values for this metric, obtained
by dividing the original value by the diagonal of the frame on which it was
calculated, i.e. the maximum possible error. Tables 3 and 5, on the other
hand, report the average time taken by the method to process two consecutive
frames, depending on the backbone; having xed the resolution, there is no
dierence between the datasets in terms of inference time.
A rst observation that can be drawn is that not a single variant of the
neural architecture performs better than the others from the point of view
of predictive accuracy in all cases. Sometimes, the SBSO variant achieves
the highest performance; sometimes, the multi-output models surpass the
single-output one. Similarly, there is no predominant backbone between the
simple FCN and MobileCount, as they show very similar results. As for
MCME, it varies between 0.167 and 0.253 and 0.181 and 0.235 for the two
datasets: since 1 is the worst possible value and lower is better, these results
conrm the eectiveness of the density-based clustering strategy proposed
here. No single model exceeds the normalized MCME of 0.101 obtained in
our previous preliminary study on CC2020 only, but this was achieved with
a sophisticated two-stage pipeline (rst density estimation, then clustering)
that takes about 15 seconds to run on the same hardware [7]. The slightly
worse results obtained with the single-stage strategy are compensated by a
much lower inference time, which is approximately 88 times shorter than in
the previous work. The multi-output variants are, as expected, slightly slower
than the single-output variant; however, they all showed near real-time per-
formance. Notably, although MobileCount has many more parameters than
the proposed simple FCN, the overall processing of the method is relatively
stable, regardless of the backbone used. This suggests that the inference
time of the neural network contributes only marginally to the overall time
and conrms the better eciency obtained by using a single-stage learning
strategy to produce crowd density maps.
Although MCME remains fairly stable across the dierent thresholds,
this does not apply to precision and recall. As expected, there is a trade-o
between the two metrics, with precision increasing and recall decreasing while
increases. It is worth noting that the use of dierent background thresholds
to lter the density maps can be set according to the specic application.
For example, if the safe landing of the drone is a signicant concern (as
in [20]), then a lower threshold may be preferred, which excludes the risk
of running into false negatives. Conversely, a higher threshold can be used
17in video surveillance scenarios to promote better precision. The diculty
of accurately locating people in aerial scenes, which in our case translates
into the problem of simultaneously maximizing precision and recall, is well-
known to the community (see for example [32]). This is reected in the better
precision generally achieved by the models on CC2021, since in this dataset
the scenes were acquired at a relatively lower altitude than on CC2020.
Finally, from a qualitative point of view, we show in Figs. 4 and 5 ex-
amples of centroid density maps produced in output by the ad-hoc FCN
and MobileCount, respectively, by varying the threshold, given two ground
truth maps from both datasets. The maps are superimposed on the original
frames in RGB. As can be seen, they conrm the trend already observed
quantitatively: the recall, i.e. the number of centroids detected over all the
centroids, tends to decrease as increases. This eect is exacerbated with
MobileCount, especially with zero or low , where recall is maximum at the
expense of a drastic drop in precision. Indeed, in these cases, the maps
produced are pretty ineective. This drawback could be explained by con-
sidering that MobileCount was explicitly optimized for crowd counting, thus
locating each person rather than groups in the scene, which translates into
a more \conservative" approach. The qualitative analysis also shows how
SBSO tends to adhere more to the ground truth, while SBMO and DBMO
to a lesser extent. This could be explained considering that SBSO is specif-
ically dedicated to producing centroid density maps. In contrast, in SBMO
and DBMO, the crowd density estimation task can mislead the main network
task. Although clusters do not perfectly match the ground truth, SBSO can
still recognize groups of people within the scene, especially with higher .
18CC2020 CC2021
MCME Precision (%) Recall (%) MCME Precision (%) Recall (%)
= 0SBSO 0.178 26.8 90.9 0.198 51.8 83.7
SBMO 0.223 22.7 97.6 0.201 44.1 100
DBMO 0.203 22.51 97.6 0.201 44.5 99.8
=1
5SBSO 0.185 30 76.5 0.202 53.5 78
SBMO 0.229 22.7 90.3 0.181 52.3 86.8
DBMO 0.192 27.2 78.2 0.181 52.8 87.2
=1
3SBSO 0.187 32 68.1 0.208 54.1 70.2
SBMO 0.233 21.7 76.6 0.181 55.3 82.5
DBMO 0.195 29.9 70.6 0.185 55.7 74.3
=1
2SBSO 0.203 32.9 54 0.217 54.3 55.1
SBMO 0.236 20.7 52.6 0.187 59.8 70
DBMO 0.208 32.0 56.1 0.201 58.5 56.8
=2
3SBSO 0.230 34.5 41.5 0.235 54.3 38.1
SBMO 0.253 19.1 31.9 0.202 63.9 51.5
DBMO 0.236 31.9 38.5 0.227 58.7 40.2
Table 2: Ad-hoc FCN eectiveness results. The best results for each individual metric per
dataset are shown in bold.
Inference time [s]
SBSO 0.16
SBMO 0.17
DBMO 0.18
Table 3: Ad-hoc FCN eciency results.
19CC2020 CC2021
MCME Precision (%) Recall (%) MCME Precision (%) Recall (%)
= 0SBSO 0.199 22.5 100 0.205 44.2 100
SBMO 0.201 22.7 100 0.205 44.1 100
DBMO 0.206 22.6 100 0.204 44.3 100
=1
3SBSO 0.198 22.4 100 0.205 43.7 100
SBMO 0.201 22.6 100 0.198 44.3 99.1
DBMO 0.209 22.6 100 0.202 44.1 99.6
=2
3SBSO 0.188 25.7 95.2 0.203 45.3 98.1
SBMO 0.184 24.1 98.6 0.204 49.3 77.3
DBMO 0.231 23.1 98.0 0.186 59.1 64.8
=3
4SBSO 0.213 40.9 45.5 0.194 46.7 93.9
SBMO 0.170 27.3 93.1 0.215 47.9 62.5
DBMO 0.230 23.3 87.5 0.20 61.5 49.1
=4
5SBSO 0.176 30.6 88.3 0.193 48.1 85.7
SBMO 0.167 30.2 86.2 0.224 46.4 52.0
DBMO 0.225 23.7 79.0 0.211 64.4 41.9
Table 4: MobileCount eectiveness results. The best results for each individual metric per
dataset are shown in bold.
Inference time [s]
SBSO 0.15
SBMO 0.18
DBMO 0.16
Table 5: MobileCount eciency results.
6. Conclusion
In this article, we tackled the problem of crowd ow detection from
drones, which was still an unexplored research direction. Such a system
can be helpful for various security and management applications, especially
in smart city scenarios. In particular, the joint exploitation of crowd density
estimation and clustering within a video sequence shot by a drone provided
encouraging results, especially from the point of view of eciency, which can
be crucial in critical tasks.
Future development of the research presented in this paper could be to
increase the size of the dataset by integrating the available scenes with syn-
thetic data, as done, for example, in [33]. Such a strategy can help further
improve the robustness of the deep learning model. Second, it is worth not-
ing that the proposed framework assumes the availability of video sequences
shot by a drone, but the neural network is fed with one frame at a time. A
dierent strategy to be explored in the future could be to feed the network
with the video sequence to account for the temporal information directly in
20Figure 4: Examples of centroid density maps, corresponding to ground truth maps for both
CC2020 and CC2021, produced in output by the ad-hoc FCN varying the threshold.
the model. Third, provided adequate ground truth is available, the method
could also be used for other similar tasks, such as vehicle counting/tracking.
Finally, another future work concerns the experimentation of the method on
new real-world situations aboard a drone to best calibrate the parameters
considered. Testing the generalizability of the method to dierent urban and
non-urban contexts can increase condence in UAV technology.
Acknowledgment. This work was supported by the Italian Ministry of Uni-
versity and Research within the \RPASInAir" project under grant PON
ARS01 00820.
21Figure 5: Examples of centroid density maps, corresponding to ground truth maps for
both CC2020 and CC2021, produced in output by MobileCount varying the threshold.
References
[1] V. A. Sindagi, V. M. Patel, A survey of recent advances in CNN-based
single image crowd counting and density estimation, Pattern Recogni-
tion Letters 107 (2018) 3{16.
[2] B. Li, H. Huang, A. Zhang, P. Liu, C. Liu, Approaches on crowd count-
ing and density estimation: a review, Pattern Analysis and Applications
(2021) 1{22.
[3] Y. Akbari, N. Almaadeed, S. Al-maadeed, O. Elharrouss, Applications,
22databases and open computer vision research from drone videos and
images: a survey, Articial Intelligence Review 54 (5) (2021) 3887{3938.
[4] M. Tzelepi, A. Tefas, Graph embedded convolutional neural networks
in human crowd detection for drone ight safety, IEEE Transactions on
Emerging Topics in Computational Intelligence 5 (2) (2019) 191{204.
[5] P. Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, H. Ling, Detection and
tracking meet drones challenge (2021). arXiv:2001.06303 .
[6] V. J. Kok, M. K. Lim, C. S. Chan, Crowd behavior analysis: A review
where physics meets biology, Neurocomputing 177 (2016) 342{362.
[7] G. Castellano, C. Mencar, G. Sette, F. S. Troccoli, G. Vessio, Crowd ow
detection from drones with fully convolutional networks and clustering,
in: 2022 International Joint Conference on Neural Networks (IJCNN
2022), IEEE, 2022, pp. 1{8.
[8] D. Du, L. Wen, P. Zhu, H. Fan, Q. Hu, H. Ling, M. Shah, J. Pan,
A. Al-Ali, A. Mohamed, et al., Visdrone-cc2020: The vision meets drone
crowd counting challenge results, in: European Conference on Computer
Vision, Springer, 2020, pp. 675{691.
[9] Z. Liu, Z. He, L. Wang, W. Wang, Y. Yuan, D. Zhang, J. Zhang, P. Zhu,
L. Van Gool, J. Han, et al., VisDrone-CC2021: The vision meets drone
crowd counting challenge results, in: Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2021, pp. 2830{2838.
[10] W. Lan, J. Dang, Y. Wang, S. Wang, Pedestrian detection based on yolo
network model, in: 2018 IEEE international conference on mechatronics
and automation (ICMA), IEEE, 2018, pp. 1547{1551.
[11] V. Molchanov, B. Vishnyakov, Y. Vizilter, O. Vishnyakova, V. Knyaz,
Pedestrian detection in video surveillance using fully convolutional
YOLO neural network, in: Automated visual inspection and machine
vision II, Vol. 10334, International Society for Optics and Photonics,
2017, p. 103340Q.
[12] G. Gao, J. Gao, Q. Liu, Q. Wang, Y. Wang, CNN-based density esti-
mation and crowd counting: A survey, arXiv preprint arXiv:2003.12783
(2020).
23[13] L. Zhu, C. Li, Z. Yang, K. Yuan, S. Wang, Crowd density estimation
based on classication activation map and patch density level, Neural
Computing and Applications 32 (9) (2020) 5105{5116.
[14] G. Zhang, Y. Pan, L. Zhang, R. L. K. Tiong, Cross-scale generative ad-
versarial network for crowd density estimation from images, Engineering
Applications of Articial Intelligence 94 (2020) 103777.
[15] V. A. Sindagi, V. M. Patel, CNN-based cascaded multi-task learning of
high-level prior and density estimation for crowd counting, in: 2017 14th
IEEE International Conference on Advanced Video and Signal Based
Surveillance (AVSS), IEEE, 2017, pp. 1{6.
[16] M. Jiang, J. Lin, Z. J. Wang, ShueCount: Task-specic knowledge
distillation for crowd counting, in: 2021 IEEE International Conference
on Image Processing (ICIP), IEEE, 2021, pp. 999{1003.
[17] M. Jiang, J. Lin, Z. J. Wang, A smartly simple way for joint crowd
counting and localization, Neurocomputing 459 (2021) 35{43.
[18] D. Liang, X. Chen, W. Xu, Y. Zhou, X. Bai, TransCrowd:
Weakly-supervised crowd counting with transformer, arXiv preprint
arXiv:2104.09116 (2021).
[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,
An image is worth 16x16 words: Transformers for image recognition at
scale, arXiv preprint arXiv:2010.11929 (2020).
[20] G. Castellano, C. Castiello, C. Mencar, G. Vessio, Crowd detection in
aerial images using spatial graphs and fully-convolutional neural net-
works, IEEE Access 8 (2020) 64534{64544.
[21] V. Gajjar, A. Gurnani, Y. Khandhediya, Human detection and tracking
for video surveillance: A cognitive science approach, in: Proceedings of
the IEEE international conference on computer vision workshops, 2017,
pp. 2805{2809.
[22] Y. Xiao, V. R. Kamat, C. C. Menassa, Human tracking from single
RGB-D camera using online learning, Image and Vision Computing 88
(2019) 67{75.
24[23] Z. Yan, T. Duckett, N. Bellotto, Online learning for 3D LiDAR-based
human detection: Experimental analysis of point cloud clustering and
classication methods, Autonomous Robots 44 (2) (2020) 147{164.
[24] L. Wen, D. Du, P. Zhu, Q. Hu, Q. Wang, L. Bo, S. Lyu, Detection,
tracking, and counting meets drones in crowds: A benchmark, in: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2021, pp. 7812{7821.
[25] K. Chebil, S. Htiouech, M. Khemakhem, Toward optimal periodic crowd
tracking via unmanned aerial vehicles, Computers & Industrial Engi-
neering (2022).
[26] D. Comaniciu, P. Meer, Mean shift: A robust approach toward feature
space analysis, IEEE Transactions on pattern analysis and machine in-
telligence 24 (5) (2002) 603{619.
[27] Y. Zhang, D. Zhou, S. Chen, S. Gao, Y. Ma, Single-image crowd count-
ing via multi-column convolutional neural network, in: Proceedings of
the IEEE conference on computer vision and pattern recognition, 2016,
pp. 589{597.
[28] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for se-
mantic segmentation, in: Proceedings of the IEEE conference on com-
puter vision and pattern recognition, 2015, pp. 3431{3440.
[29] P. Wang, C. Gao, Y. Wang, H. Li, Y. Gao, MobileCount: An ecient
encoder-decoder framework for real-time crowd counting, Neurocomput-
ing 407 (2020) 292{299.
[30] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen, Mo-
bileNetV2: Inverted residuals and linear bottlenecks, in: Proceedings
of the IEEE conference on computer vision and pattern recognition,
2018, pp. 4510{4520.
[31] V. Nekrasov, C. Shen, I. Reid, Light-Weight ReneNet for real-time
semantic segmentation, arXiv preprint arXiv:1810.03272 (2018).
[32] Y. Cao, Z. He, L. Wang, W. Wang, Y. Yuan, D. Zhang, J. Zhang,
P. Zhu, L. Van Gool, J. Han, et al., VisDrone-DET2021: The vision
meets drone object detection challenge results, in: Proceedings of the
25IEEE/CVF International Conference on Computer Vision, 2021, pp.
2847{2854.
[33] Z. Zhao, T. Han, J. Gao, Q. Wang, X. Li, A ow base bi-path network
for cross-scene video crowd understanding in aerial view, in: European
Conference on Computer Vision, Springer, 2020, pp. 574{587.
26