arXiv:2301.06194v1  [q-bio.BM]  15 Jan 2023Geometric Graph Learning with Extended Atom–Types Feature s for
Protein–Ligand Binding Aﬃnity Prediction
Md Masud Rana1and Duc Duy Nguyen1∗
1Department of Mathematics, University of Kentucky, KY 40506, U SA
January 18, 2023
Abstract
Understanding and accurately predicting protein-ligand b inding aﬃnity are essential in the drug design
and discovery process. At present, machine learning-based methodologies are gaining popularity as a means
of predicting binding aﬃnity due to their eﬃciency and accur acy, as well as the increasing availability of
structural and binding aﬃnity data for protein-ligand comp lexes. In biomolecular studies, graph theory has
been widely applied since graphs can be used to model molecul es or molecular complexes in a natural manner.
In the present work, we upgrade the graph-based learners for the study of protein-ligand interactions by
integrating extensive atom types such as SYBYL and extended connectivity interactive features (ECIF) into
multiscale weighted colored graphs (MWCG). By pairing with the gradient boosting decision tree (GBDT)
machine learning algorithm, our approach results in two diﬀ erent methods, namelysybylGGL-Score and
ecifGGL-Score. Both of our models are extensively validated in t heir scoring power using three commonly
used benchmark datasets in the drug design area, namely CASF -2007, CASF-2013, and CASF-2016. The
performance of our best modelsybylGGL-Score is compared with other state-of-the-art models i n the binding
aﬃnity prediction for each benchmark. While both of our mode ls achieve state-of-the-art results, the SYBYL
atom-type modelsybylGGL-Score outperforms other methods by a wide margin in all b enchmarks.
Keywords— geometric graph learning, protein-ligand binding aﬃnity, atom-type interaction, weighted colored sub-
graph, machine learning
1 Introduction
In recent years, graph theories have been widely used in chem ical, biological, physical, social, and computer sciences . This
is because graphs are useful for representing and analyzing a wide range of practical problems. In molecular modeling,
graph representation is widely used since it is a natural way to model their structures, in which graph vertices represen t
atoms and graph edges represent possible interactions betw een them. In general, graph theories can be divided into thre e
categories: geometric graph theory, algebraic graph theor y, and topological graph theory. Geometric graph theory stu dies
a graph’s geometric connectivity, which refers to the pairw ise relations among graph nodes or vertices [ 1]. Algebraic graph
theory concerns the algebraic connectivity via the charact eristic polynomial, eigenvalues, and eigenvectors of matr ices
associated with the graph, such as the adjacency matrix or th e Laplacian matrix [ 2,3]. In topological graph theory,
embedding and immersion of graphs are studied along with the ir association with topological spaces, such as abstract
simplicial complexes [ 4,5].
There are numerous applications of graphs in chemical analy sis and biomolecular modeling [ 6,7,8,9], such as
normal-mode analysis (NMA) [ 10,11,12,13] and elastic network model (ENM) [ 14,15,16,17,18,19] used to study
protein B–factor prediction. Algebraic graph theory has be en utilized in some of the most popular elastic network model s
(ENMs) such as the Gaussian network model (GNM) and the aniso tropic network model (ANM). However, due to the
matrix-diagonalization procedure, these methods have a co mputational complexity of O(N3), withNbeing the number
of matrix elements. Furthermore, these methods suﬀer from l imited accuracy in protein B–factor prediction, with avera ge
Pearson correlation coeﬃcients less than 0.6 in all dataset s. A geometric graph theory-based weighted graph approach,
called ﬂexibility-rigidity index (FRI), was introduced to bypass matrix diagonalization in GNM [ 20,21,22,23]. FRI
assumes that protein interactions, including interaction s with its environment, completely determine its structure in a
given environment, which in turn, fully determines protein ﬂexibility and functions. Therefore, it is not necessary to
invoke a high-dimensional protein interaction Hamiltonia n as in spectral graph theory to analyze protein ﬂexibility w hen
the accurate structure of the protein and its environment ar e known. While the computational complexity of earlier FRI
[20] is ofO(N2), the fast FRI [ 21] is ofO(N). In order to capture multiscale interactions in macromole cules, multiscale
FRI (mFRI) was introduced [ 24], resulting in a number of graphs with parallel edges, i.e. m ultiple graphs. Despite the
fact that mFRI is about 20% more accurate than the GNM on a set o f 364 proteins, the average Pearson’s correlation
coeﬃcient in B–factor prediction falls below 0.7, which is i nsuﬃcient to provide a reliable assessment of protein ﬂexib ility.
The limited accuracy of these graph-based models is due to th e fact that they do not distinguish diﬀerent chemical elemen t
types in a molecule or biomolecule, resulting in a severe los s of important chemical and biological information.
∗Address correspondences to Duc Duy Nguyen. E-mail: ducnguy en@uky.edu
1To address the aforementioned problem, a multiscale weight ed colored graph (MWCG) model was introduced for
protein ﬂexibility analysis [ 25]. In MWCG, the graph of a protein structure is colored accord ing to the type of inter-
action between nodes in the graph, and subgraphs are deﬁned a ccording to colors. This process is commonly referred
to as graph coloring, which is an important technique in grap h theory that allows graph vertices or edges to be treated
diﬀerently. MWCG weights the importance of graph edges by sc aling their Euclidean distances in radial basis functions
so that the nearest neighbors have the strongest edges in the sense of the Euclidean metric. Mathematical properties
of MWCGs include low dimensionality, simplicity, robustne ss, and invariance of rotations, translations, and reﬂecti ons.
Subgraphs constructed from vertex-labeled and edge-label ed graphs provide powerful representations of intermolecu lar
and intramolecular interactions, such as hydrogen bonds, e lectrostatics, van der Waals interactions, hydrophilicit y, hy-
drophobicity, etc [ 1,25]. The MWCG models oﬀer 40% more accuracy than the GNM in prote in B–factor prediction
[25].
Molecular interactions between proteins and substrate mol ecules (ligands) are the principal determinant of many vita l
processes, such as cellular signaling, transcription, met abolism, and immunity. Therefore, understanding protein- ligand
interactions is a central issue in biochemistry, biophysic s, and molecular biology. Moreover, an accurate prediction of
protein-ligand binding aﬃnity plays a critical role in comp utational drug design, particularly in virtual screening a nd lead
optimization. Various scoring functions (SFs) have been de veloped over the past few decades to evaluate protein-ligan d
interactions in structure-based drug design. These SFs can be classiﬁed mainly into four categories: force-ﬁeld-base d
or physics-based SF, empirical SF, knowledge-based SF, and machine-learning-based SF. Force-ﬁeld-based SFs oﬀer
physical insight and are not dependent on existing data. Emp irical SFs utilizes a number of physical sub-models and use
regression to ﬁt existing data. The knowledge-based SF uses available datasets to derive binding patterns for proteins
and ligands without requiring further training. Finally, m achine learning-based SFs are data-driven, and are capable
of capturing non-linear and complex relationships in the da ta. They can also easily handle large and diverse datasets.
The performance of machine learning-based SFs strongly dep ends on the training set, in addition to their descriptors
and machine learning algorithms. These scoring functions o ften take the top place in several standard benchmarks and
community-wide competitions [ 26,27,28,29,30].
In recent years, due to the increasing availability of struc tural and binding aﬃnity data for protein-ligand complexes ,
machine-learning SFs have become increasingly popular for binding aﬃnity prediction. The RF–Score [ 31] is considered
one of the ﬁrst machine-learning-based SFs to outperform ot her SFs in the CASF–2007 benchmark dataset. The model
uses the random forest algorithm and employs element–type p air counts as features to describe protein-ligand complexe s.
The model was later extended to incorporate a more precise ch emical description, including SYBYL atom-type pair counts
features [ 32]. Including SYBYL atom types into the model permits deconvo luting the element into a hybridization state
and bonding environment. For example, instead of having a si ngle Carbon (C) element atom type, the SYBYL scheme
allows the following subtypes: C.1, C.2 C.3, C.ar, and C.cat . A number of SYBYL atom-type-based models have been
developed in the past years [ 33], including SYBYL::ChemScore , SYBYL::G-Score , and SYBYL ::D-Score. In a separate
study, it has been shown that the connectivity of the atoms [ 34] can improve the performance of a machine learning model
in the binding aﬃnity prediction task [ 35]. In [ 35], the authors used a set of protein-ligand atom-type pair co unts features,
called the extended connectivity interactive features (EC IF), that considers each atom’s connectivity to deﬁne the at oms
involved in the pairs. The atom deﬁnition in ECIF is based on t he atom environment concept initially introduced in
the development of Extended Connectivity Fingerprints (EC FP) [ 36]. Paired with a machine learning algorithm, the
ECIF model signiﬁcantly improves the performance of the bin ding aﬃnity prediction with Pearson’s correlation of 0.866
for the CASF–2016 benchmark. A number of machine-learning- based SF with diﬀerent types of descriptors including
diﬀerential geometry [ 37,38], persistent homology [ 39,5], and graph theory [ 1,2] have emerged in the past few years
for protein-ligand binding aﬃnity prediction. Among them, the element-type graph coloring-based MWCG descriptors
have particularly been successful in the task [ 1,2].
In the present work, we propose a geometric graph theory-bas ed multiscale weighted colored graph (MWCG) de-
scriptors for the protein-ligand complex where the graph co loring is based on SYBYL atom-type and ECIF atom-type
connectivity. By pairing with the advanced machine learnin g architectures, our approach results in two diﬀerent metho ds,
namelysybylGGL-Score andecifGGL-Score. We verify the scoring power of our proposed model against three commonly
used benchmarks in drug design, namely CASF-2007 [ 33], CASF-2013 [ 40], and CASF-2016 [ 41]. Several experiments
conﬁrm that both of our models achieve state-of-the-art res ults and outperform other models by a wide margin.
2 Methods and Materials
2.1 Multiscale Weighted Colored Geometric Subgraphs
A graph Gof a biomolecule consists of a set of vertices Vand edges Eand can be used to describe the noncovalent
interaction of atoms in the molecule. In recent years, graph theory descriptors of protein-ligand binding interaction s
have been developed for massive and diverse datasets [ 2,42]. To improve the graph theory representation, diﬀerent
types of elements are labeled, which is known as graph colori ng. A colored graph is used to encode diﬀerent types of
interactions between atoms and gives rise to a basis for the c ollective coarse-grained description of the dataset. Labe led
atoms of a molecule are classiﬁed into subgraphs where color ed edges correspond to element-speciﬁc interactions.
To account for details of physical interactions in protein- ligand complexes such as hydrophobic, hydrophilic, etc.,
we are interested in constructing the subgraphs in an atomic interactive manner. In our previous work [ 1,2], we used
the combination of the element symbols of the interacting pr otein-ligand atoms to classify the interaction, e.g., C–C o r
N–O. In the present work, instead of the element symbol, we co nsider the following two schemes to classify the atomic
interaction. In the ﬁrst approach, we consider atom name (ex cluding hydrogen) for protein and SYBYL atom type
2for the ligand to deﬁne a range of protein-ligand atom pairs, e.g CD1–C.2, CG–C.ar, OE1–N.am, etc. In the second
scheme, we consider the extended connectivity interaction features (ECIF) described in [ 35] to extract the protein-ligand
atom-type pair that takes each atom’s connectivity into acc ount. The ECIF atom type in a molecule is deﬁned by
considering six atomic features: atom symbol, explicit val ence, number of attached heavy atoms, number of attached
hydrogens, aromaticity, and ring membership. Each of these properties can be represented textually where each propert y
is separated by a semicolon. For example, the ECIF atom type f or theαcarbon CA is C;4;3;1;0;0.
For convenience, let Tbe the set of all interested atom types in a given biomolecula r dataset for either of the two
schemes described above. To reduce the notation complexity , we denote the atom type at the ith position in the set T
asTi. Assuming that a biomolecule has Natoms of interest, we denote
V={(ri,αi)|ri∈R3;αi∈ T;i= 1,2,···,N} (1)
a subset of Natoms (i.e. subgraph vertices) that are members of T. Note that the ith atom is labeled by both its
coordinate riand atom type αi. We assume that all the pairwise non-covalent interactions between atom types Tkand
Tk′in a molecule or molecular complex can be represented by fast -decay weight functions
E={Φ(/bardblri−rj/bardbl;ηkk′)|αi=Tk, αj=Tk′;
i,j= 1,2,···,N;/bardblri−rj/bardbl ≤c}, (2)
where/bardblri−rj/bardblis the Euclidean distance between the ith andjth atom and cis a predeﬁned cutoﬀ distance that deﬁnes
the binding site of the atom type TkandTk′. Hereηkk′is a characteristic distance between the atoms, and Φ is a
subgraph weight that satisﬁes the following admissibility conditions
Φ(/bardblri−rj/bardbl;ηkk′) = 1,as/bardblri−rj/bardbl →0, (3)
Φ(/bardblri−rj/bardbl;ηkk′) = 0,as/bardblri−rj/bardbl → ∞,
αi=Tk, αj=Tk′. (4)
Although most radial basis functions can be used as the subgr aph weight, the generalized exponential function
ΦE(/bardblri−rj/bardbl;ηkk′) =e−(/bardblri−rj/bardbl/ηkk′)κ, κ > 0,
and the generalized Lorentz function
ΦL(/bardblri−rj/bardbl;ηkk′) =1
1 + (/bardblri−rj/bardbl/ηkk′)κ, κ > 0,
were shown to work very well for biomolecules [ 21]. Now, we have a weighted colored subgraph G(V,E) for a molecule or
a molecular complex and we can use it to construct atomic-lev el collective molecular descriptors. We deﬁne the multisca le
weighted colored geometric subgraph (MWCGS) interaction b etweenkth atom type Tkandk′th atom type Tk′by
µG(ηkk′) =/summationdisplay
iµG
i(ηkk′) =/summationdisplay
i/summationdisplay
jΦ(/bardblri−rj/bardbl;ηkk′),
αi=Tk, αj=Tk′, (5)
whereµG
i(ηkk′) is the geometric subgraph centrality for the ith atom of type Tkand all atoms of type Tk′. The summation
over the geometric centrality µG
i(ηkk′) in equation ( 5) can be interpreted as the total interaction strength for th e selected
atom type pair TkandTk′, which provides the atomic-level coarse-grained descript ion of the molecular properties. The
equation ( 5) is a generalization of a bipartite subgraph discussed in [ 1] for the predictions of protein-ligand binding
aﬃnities and free energy ranking. A bipartite subgraph of a p rotein-ligand complex is a graph in which each of its edges
connects one atom in the protein and another in the ligand. We intend to capture the hydrogen bonds, polarization,
electrostatics, van der Waals interactions, hydrophilici ty, hydrophobicity, etc. of a protein-ligand complex throu gh the
bipartite graph coloring, i.e., atom-speciﬁc description s and subgraph weight.
The multiscale behavior of the MWCGS arises when a diﬀerent s election of the characteristic distance ηkk′for a
pair of atom types kandk′are considered. Therefore, for a molecule or a biomolecule, the MWCGS allows us to
systematically construct a family of collective, scalable , multiscale graph-based descriptors by an appropriate sel ection
of atom types pair kandk′, characteristic distance ηkk′, and subgraph weight Φ. An illustration of the weighted colo red
subgraph under the SYBYL atom-type system for the molecule x anthine (C 5H4N4O2) is presented in Figure 1.
2.2 Geometric Graph Learning
The multiscale weighted colored geometric subgraph (MWCGS ) descriptors for a molecule or molecular complex can be
paired with any machine learning or deep learning algorithm to predict molecular properties. In a supervised machine
learning algorithm (either classiﬁcation or regression), the labeled dataset is divided into two parts: a training set
and a test set. Let Xibe a labeled dataset for the ith molecule or molecular complex in the training set. Furthe rmore,
suppose G(Xi,λ) be a function that encodes the geometric information of the molecule or molecular complex into suitable
graph representations with a set of parameters λ. The training of a machine learning model can be translated i nto a
minimization problem,
min
λ,θ/summationdisplay
i∈IL(yi,G(Xi,λ);θ) (6)
3b
 a
Figure 1: Illustration of the weighted colored subgraph. Part (a) is a diagram of the structure of the xanthine
molecule (C 5H4N4O2; ligand name: XAN; PDB ID: 2uz9), and (b) the weighted colored sub graphs, from left
to right, G N.am–O.2, GN.pl3–O.2, and G N.2–O.2consisting of SYBYL atom-type pair N.am–O.2, N.pl3–O.2, and
N.2–O.2, respectively. The dashed line in (b) represents the edges o f the graph.
whereLis a scalar loss function to be minimized and yiis the labels of the ith sample in the training set I. Here,
θis the set of hyperparameters that depends on the chosen mach ine learning algorithm and typically be optimized for
optimal performance. A wide range of machine learning algor ithms, such as support vector machines, random forests,
gradient boosting trees, artiﬁcial neural networks, and co nvolution neural networks, can be implemented in conjugati on
with the present geometric subgraph descriptors. However, to focus on the descriptive power of the proposed geometric
subgraph features, we only employ gradient boosting decisi on trees (GBDT) in the present work and avoid optimizing
machine learning algorithm selections. Although relative ly simple, GBDT is still powerful, robust against overﬁttin g,
and a widely used ensemble algorithm. An illustration of the proposed geometric graph learning strategy is presented in
Figure 2.
We use GBDT module in scikit-learn v0.24.1 package with the f ollowing parameters: nestimators = 20000,
maxdepth = 8,minsamples split = 2,learning rate = 0.005, loss = ls,subsample = 0.7, and maxfeatures =
sqrt. These parameter values are selected from the extensiv e tests on PDBbind datasets and are uniformly used in all
our validation tasks in this work.
2.3 Dataset
For protein-ligand binding aﬃnity prediction, the most com monly used benchmarks are the PDBbind datasets. In this
work, we use the three most popular PDBbind benchmark datase ts, CASF–2007, CASF–2013, and CASF–2016, to test
the performance of our model. The PDBbind datasets consist o f a general set, a reﬁned set, and a core set, where the
latter set is a subset of the previous one. In the present work , we explore two diﬀerent training sets to build predictive
models for the binding aﬃnity of the complexes in the test set , which is the core set of the corresponding benchmark.
The ﬁrst training set, denoted by SR, is the reﬁned set excluding the core set of the correspondin g benchmark. As
a second training set, denoted by SG, we use the general set excluding the core set of the correspo nding benchmark.
More information about these datasets is oﬀered on the PDBbi nd website http://pdbbind.org.cn/ . A summary of the
dataset is provided in Table 1.
Table 1: Summary of PDBbind datasets used in the present work.
Dataset |SG| |SR| |SC|
CASF–2007 benchmark 2852 1105 195
CASF–2013 benchmark 11713 3516 195
CASF–2016 benchmark 12998 3772 285
|SG|: Number of complexes in the general set excluding the core set of t he corresponding benchmark.
|SR|: Number of complexes in the reﬁned set excluding the core set of th e corresponding benchmark.
|SC|: Number of complexes in the core set of the corresponding benchm ark.
2.4 Model Parametrization
For the sake of convenience, we use the notation GGLα
κ,τto indicate the geometric graph learning features generate d
by using kernel type αand corresponding kernel parameters κandτ. Here,α=Eandα=Lrefer to the generalized
exponential and generalized Lorentz kernels, respectivel y. Andτis used such that ηkk′=τ(¯rk+ ¯rk′), where ¯ rkand ¯rk′
4Protein-ligand 
complex Atom-type 
specific groups …
Weighted-
colored geometric 
subgraphs Statistics 
of subgraph 
rigidity Machine 
learning 
prediction 
Sum, Mean, 
Max, 
Deviation, 
etc. 
Sum, Mean, 
Max, 
Deviation, 
etc. 
Sum, Mean, 
Max, 
Deviation, 
etc. ……
…
Figure 2: Illustration of the geometric graph learning strategy usin g the molecular complex with PDBID: 5bwc
(ﬁrst column). The second column represents the protein-liganda tom-type pair CA–O.3, OE1–N.pl3, and NE1–
C.2, respectively from top to bottom. The corresponding weighted colored geometric subgraphs are shown in
the third column. The fourth column presents the statistics of the subgraph rigidity. In the ﬁnal column, the
advanced machine learning models such as the gradient boosting tre es integrate these statistical features for
training and prediction.
are the van der Waals radii of atom type kand atom type k′, respectively. Kernel parameters κandτare selected based
on the cross-validation with a random split of the training d ata. We propose a GGL representation in which multiple
kernels are parametrized at diﬀerent scale ( η) values. In this work, we consider at most two kernels. As a st raightforward
notation extension, two kernels can be parametrized by GGLα1,α2κ1,τ1;κ2,τ2. Each of these kernels gives rise to one set of
features. Finally, as we consider two diﬀerent schemes to ex tract the protein-ligand atom-type pair, we introduce the
following two notationssybylGGLα1,α2κ1,τ1;κ2,τ2andecifGGLα1,α2κ1,τ1;κ2,τ2.
3 Results and Discussion
In this section, we present the scoring power of our proposed geometric graph learning (GGL) model for the benchmark
datasets discussed above.
3.1 Hyperparameter Optimization and Model Performance
It is a well-known fact that the performance of a machine-lea rning model depends on the optimization of its essential
parameters. To achieve the best performance of our GGL model on each benchmark, we optimize the kernel parameters
κandτ. We use ﬁve-fold cross-validation (CV) and a grid search met hod to ﬁnd the optimal parameters τin the range
[0.5,10] andκin the range [0 .5,20] with an increment of 0.5 for both parameter ranges. The hi gh values of the power
parameter κare considered to approximate the ideal low–pass ﬁlter (ILF ) [43].
As a general strategy to optimize the model hyperparameters on each benchmark, we carry out a ﬁve-fold CV on
the training set SRwhich is the reﬁned set excluding the core set of the correspo nding benchmark. Once we ﬁnd the
best model for each benchmark dataset, we test the performan ce of the model on the test set SC(i.e., the core set of
the corresponding benchmark). For the prediction task, our ﬁrst strategy is to train the model using the training set SR
(i.e., the reﬁned set excluding the core set) and observe the performance on the test set. And secondly, we train the best
model using the training set SG(i.e., the general set excluding the core set) and test the pe rformance on the test set. As
the general set of each benchmark contains more diverse comp lexes than the reﬁned set, we expect our model performs
better when trained with the training set SG. Below we discuss the optimization of our model hyperparame tersτand
κand the model’s performance on each benchmark.
53.1.1 CASF–2016
The ﬁrst benchmark we consider is the CASF–2016, the latest o f the three benchmark datasets in the PDBbind database.
We carry out ﬁve-fold cross-validation (CV) on the ﬁrst trai ning set which is the reﬁned set excluding the core set of
this benchmark. The CV results for both the single-scale and two-scale SYBYL atom-type GGL models are presented
in Figure 3. The parameter set ( κ,τ) = (2.5,1.5) gives the best median Pearson’s correlation coeﬃcient Rp=0.795 for
the single-scale exponential kernel (Figure 3a). For the single-scale Lorentz kernel model the parameter s are (κ,τ) =
(14.0,1.5) with median Rp=0.795 (Figure 3b). The two-scale kernel model is built on top of the previous ly optimized
single-scale kernel parameters, so we only optimize the par ameters for the second kernel. Figure 3c and Figure 3d
plots the CV results for the second kernel parameters κ2andτ2of the two-scale kernel SYBYL atom-type model
sybylGGLα1,α2κ1,τ1;κ2,τ2withκ1andτ1ﬁxed at the optimal value from single-scale model. We observ e that the best two-scale
exponential kernel model issybylGGLE,E
2.5,1.5;15.0,8.5with median Rp=0.796 (Figure 3c) and the best two-scale Lorentz
kernel model issybylGGLL,L
14.0,1.5;16.0,0.5with median Rp=0.797 (Figure 3d).
To ﬁnd the optimal parameters for the ECIF atom-type models, we carry out a similar process discussed above.
Figure 4plots the CV performance of the single-scale kernel ECIF ato m-type modelecifGGLα
κ,τ. We ﬁnd that the best
parameters for the single-scale exponential kernel model a reκ=13.0 and τ=2.5 with median Rp=0.790 (Figure 4a) and
the best parameters for the single-scale Lorentz kernel mod el areκ=14.0 and τ=1.5 with median Rp=0.789 (Figure
4b). The optimal parameters for the two-scale kernel model ar e also explored in a similar fashion as above. The CV
results of each combination of the second kernel parameters are presented in Figure 4. The ﬁgure conﬁrms that the best
two-scale exponential kernel model isecifGGLE,E
13.0,2.5;15.0,9.0with median Rp=0.792 (Figure 4c) and the best two-scale
Lorentz kernel model isecifGGLL,L
14.0,1.5;13.5,9.0with median Rp=0.791 (Figure 4d).
After ﬁnding the best models for this benchmark, we are inter ested in validating their performance on the test set,
i.e., the CASF–2016 core set. The performance is measured us ing the Pearson 's correlation coeﬃcient between the
predicted and the experimental binding aﬃnities of the test set complexes. First, we train each model with the smaller
training set SR, i.e., the PDBbind v2016 reﬁned set excluding the CASF–2016 core set. Then we use the trained model
to predict the test set. To this end, we repeat the model up to 5 0 times and use the average of all predicted values as the
ﬁnal predicted set. As a second approach, we train the model w ith the bigger training set SG, i.e., the PDBbind v2016
general set excluding the CASF–2016 core set. For the predic tion task, we again repeat the trained model 50 times and
use the average of all predictions.
The performance of the best models (both SYBYL atom-type and ECIF atom-type) on the test set are listed
in Table 2. We ﬁnd that the performance of all models signiﬁcantly impr oved when the model is trained with the
bigger training data SG. The results in Table 2indicate that the two-scale models perform slightly better than their
single-scale counterparts as expected. We also observe tha t the SYBYL atom-type models, both single-scale and two-
scale, outperform their ECIF atom-type counterparts. The b est model for this benchmark is the two-scale Lorentz
kernel SYBYL atom-type modelsybylGGLL,L
14.0,1.5;16.0,0.5with reported Pearson 's correlation Rp=0.873. In addition, we
compare the scoring power of our proposed GGL-Score against various state-of-the-art scoring functions in the literat ure
[33,31,44,45,46,47]. Figure 9c illustrates such a comparison for CASF–2016 benchmark and clearly our model stands
in the top. The second best is the TopBP-DL with reported Rp=0.848. It must be stressed that the base geometric
and algebraic graph learning models that consider the eleme nt-speciﬁc interactions instead of the atom-type interact ions
have a comparatively lower performance with Rp=0.815 [ 1] andRp=0.835 [ 2] respectively. The above comparison and
Figure 9c conﬁrm the scoring power and the eﬀectiveness of consideri ng atom-type pair interactions in the present model.
Moreover, to highlight that the current model’s impressive performance is due to the incorporation of the atom-type
pair interactions and not because of the use of larger traini ng data SG, we explore the performance of the base GGL
models with element-speciﬁc interactions that are trained on the set SG. The details of this experiment and results are
presented in Appendix 5. While the use of the bigger training data improves the perfo rmance of the base GGL model,
our extended atom-type models still outperform them by a big margin (see Table A1).
Table 2: Performance of various GGL models on CASF–2016 test set .
Pearson 'sRpof single-scale Model Pearson 'sRpof two-scale Model
Model Trained with SRTrained with SGModel Trained with SG
sybylGGLE
2.5,1.5 0.838 0.872sybylGGLE,E
2.5,1.5;15.0,8.5 0.872
sybylGGLL
14.0,1.5 0.832 0.872sybylGGLL,L
14.0,1.5;16.0,0.5 0.873
ecifGGLE
13.0,2.5 0.824 0.867ecifGGLE,E
13.0,2.5;15.0,9.0 0.868
ecifGGLL
14.0,1.5 0.822 0.865ecifGGLL,L
14.0,1.5;13.5,9.0 0.868
3.1.2 CASF–2013
As a second benchmark dataset among the CASF family, we consi der the CASF–2013 benchmark. For both SYBYL
atom-type and ECIF atom-type models, we carry out a similar h yperparameter optimization to that of the CASF–2016
benchmark. We use the smaller training set SRof this benchmark which is the PDBbind v2015 reﬁned set exclu ding
6Figure 3: Optimized parameters forsybylGGL model for CASF–2016 benchmark. The best parameterslocat ions
are marked by “x”. The optimal parameters for, (a) single-scale e xponential kernel model are ( κ,τ) = (2.5,1.5)
with the corresponding median Rp= 0.795 and (b) single-scale Lorentz kernel model are ( κ,τ) = (14.0,1.5)
with corresponding median Rp= 0.795. The optimal second kernel parameters for (c) two-scale ex ponential
kernel model are ( κ,τ) = (15.0,8.5) with the correspondingmedian Rp= 0.796 and (d) two-scale Lorentz kernel
model are ( κ,τ) = (16.0,0.5) with the corresponding median Rp= 0.797.
Figure 4: Optimized parameters forecifGGL model for CASF–2016 benchmark. The best parameters locat ions
are marked by “x”. The optimal parameters for (a) single-scale ex ponential kernel model are ( κ,τ) = (13.0,2.5)
with the corresponding median Rp= 0.790 and (b) single-scale Lorentz kernel model are ( κ,τ) = (14.0,1.5)
with corresponding median Rp= 0.789. The optimal second kernel parameters for (c) two-scale ex ponential
kernel model are ( κ,τ) = (15.0,9.0) with the correspondingmedian Rp= 0.792 and (d) two-scale Lorentz kernel
model are ( κ,τ) = (13.5,9.0) with the corresponding median Rp= 0.791.
7the CASF–2013 core set for the cross-validation process. Fi gure 5reveals the optimal parameters for the SYBYL
atom-type model. The best parameters for the single-scale e xponential kernel are found to be κ=5.5 and τ=2.0 with
medianRp=0.796 (Figure 5a) and the best parameters for the single-scale Lorentz kern el areκ=5.5 and τ=0.5 with
medianRp=0.795 (Figure 5b). For the two-scale kernel models, we ﬁx the ﬁrst kernel par ameters at their optimal
value and optimize the second kernel parameter. Figure 5c shows that the best two-scale exponential kernel model
issybylGGLE,E
5.5,2.0;4.0,0.5with median Rp=0.798 and Figure 5d shows that the best two-scale Lorentz kernel model is
sybylGGLL,L
5.5,0.5;12.0,9.5with median Rp=0.798.
For the ECIF atom-type model hyperparameter optimization, we follow the same procedure as above. We found
that the best single-scale exponential kernel model isecifGGLE
12.0,2.5with median Rp=0.792 (Figure 6a) and the best
single-scale Lorentz kernel model is found to beecifGGLL
18.0,2.0with median Rp=0.791 (Figure 6b). For the two-scale
kernel model, the best two-scale exponential kernel model i s found to beecifGGLE,E
12.0,2.5;15.0,8.5with median Rp=0.795
(Figure 6c). Finally, from Figure 6d, we found that the best two-scale Lorentz kernel model isecifGGLL,L
18.0,2.0;15.0,8.5with
medianRp=0.795.
Furthermore, we utilize the best models of this benchmark to predict the binding aﬃnity of the 195 complexes in the
CASF–2013 test set. Like the CASF–2016 benchmark, we ﬁrst tr ain each model using the smaller training set of this
benchmark, i.e., the PDBbind v2015 reﬁned set excluding the CASF–2013 core set, and then we generate a prediction
for the test set from the average of 50 runs. Secondly, we use t he more extensive training set, PDBbind v2015 general
set, excluding the CASF–2013 core set to train the model and u se it to get the prediction for the test set.
The performance of all models on the CASF–2013 test set is rep orted in Table 3. It is interesting to see a similar
trend that the performance of all models improved signiﬁcan tly when the model is trained on the bigger training data SG.
We also observe that the SYBYL atom-type models consistentl y outperform their ECIF atom-type counterparts. With
the two-scale kernel model performing slightly better than the single-scale kernel model, the best-performing model
for this benchmark is the two-scale exponential kernel SYBY L atom-type modelecifGGLE,E
12.0,2.5;15.0,8.5with reported
Pearson 's correlation coeﬃcient Rp=0.848. Additionally, Figure 9b proves the dominance of our model in the scoring
power over other published models for this benchmark. The re portedRp=0.848 of our best model is signiﬁcantly higher
than the Rp=0.808 of the runner-up model TopBP. Furthermore, Table A1in Appendix 5conﬁrms that the outstanding
performance of our model is due to the incorporation of the at om-type interactions in the model.
Figure 5: Optimized parameters forsybylGGL model for CASF–2013 benchmark. The best parameterslocat ions
are marked by “x”. The optimal parameters for (a) single-scale ex ponential kernel model are ( κ,τ) = (5.5,2.0)
with the corresponding median Rp= 0.796 and (b) single-scale Lorentz kernel model are ( κ,τ) = (5.5,0.5) with
corresponding median Rp= 0.795. The optimal second kernel parameters for (c) two-scale ex ponential kernel
model are ( κ,τ) = (4.0,0.5) with the corresponding median Rp= 0.798 and (d) two-scale Lorentz kernel model
are (κ,τ) = (12.0,9.5) with the corresponding median Rp= 0.798.
Table 3: Performance of various GGL models on CASF–2013 test set .
Pearson 'sRpof single-scale Model Pearson 'sRpof two-scale Model
Model Trained with SRTrained with SGModel Trained with SG
sybylGGLE
5.5,2.0 0.797 0.846sybylGGLE,E
5.5,2.0;4.0,0.5 0.848
sybylGGLL
5.5,0.5 0.812 0.841sybylGGLL,L
5.5,0.5;12.0,9.5 0.844
ecifGGLE
12.0,2.5 0.797 0.826ecifGGLE,E
12.0,2.5;15.0,8.5 0.829
ecifGGLL
18.0,2.0 0.801 0.829ecifGGLL,L
18.0,2.0;15.0,8.5 0.833
8Figure 6: Optimized parameters forecifGGL model for CASF–2013 benchmark. The best parameters locat ions
are marked by “x”. The optimal parameters for (a) single-scale ex ponential kernel model are ( κ,τ) = (12.0,2.5)
with the corresponding median Rp= 0.792 and (b) single-scale Lorentz kernel model are ( κ,τ) = (18.0,2.0)
with corresponding median Rp= 0.791. The optimal second kernel parameters for (c) two-scale ex ponential
kernel model are ( κ,τ) = (15.0,8.5) with the correspondingmedian Rp= 0.795 and (d) two-scale Lorentz kernel
model are ( κ,τ) = (15.0,8.5) with the corresponding median Rp= 0.795.
3.1.3 CASF–2007
Our last benchmark is the CASF–2007. The hyperparameter opt imization for this benchmark is similar to the previous
two benchmarks. The smaller training set SR, which is the PDBbind v2007 reﬁned set excluding the CASF-20 07 core set,
is used for the ﬁve-fold CV. The CV performances of the SYBYL a tom-type models are plotted in Figure 7. The optimal
kernel parameters for the single-scale exponential model a reκ=2.5 and τ=0.5 (Figure 7a) with median Rp=0.745. For the
single-scale Lorentz kernel, Figure 7b, the best parameters are κ=13.5 and τ=0.5 with median Rp=0.746. The two-scale
models are built on top of the optimized single-scale model, we only search for the optimal second kernel parameters.
Figure 7c shows that the two-scale exponential modelsybylGGLE,E
2.5,0.5;19.0,9.0gives the best median Rp=0.747 while Figure
7d reveals that the best two-scale Lorentz kernel model issybylGGLL,L
13.5,0.5;13.0,9.5with median Rpbeing 0.747.
The hyperparameter optimization for the ECIF atom-type mod els is carried out in a similar fashion. Figure 8displays
the best parameters and the CV performance. We found that the best single-scale exponential model isecifGGLE
17.5,1.5
with median Rp=0.739 (Figure 8a) and the best single-scale Lorentz kernel model isecifGGLL
15.5,1.5with median Rp=0.738
(Figure 8b). The best two-scale exponential kernel model is found to b eecifGGLE,E
17.5,1.5;16.5,8.5with median Rp=0.741
(Figure 8c). Finally, (Figure 8d), shows that the best two-scale Lorentz kernel model isecifGGLL,L
15.5,1.5;15.0,7.5with median
Rp=0.742.
Having optimized the models’ hyperparameters, we now predi ct the binding aﬃnity of the 195 complexes in the
CASF-2007 test set. Just like in the previous two benchmarks , we ﬁrst train each model using the smaller training set
SRand produce a prediction for the test set from the average of 5 0 runs. Secondly, we use the bigger training set SGof
this benchmark, which is the PDBbind v2007 general set exclu ding the CASF-2007 core set, to train the model and use
the trained model to predict the binding aﬃnity of the test se t.
The performance of all our selected models for this benchmar k is reported in Table 4. We observe that all of
these models perform signiﬁcantly better when trained with the bigger training set SG. Following a similar trend as in
the previous two benchmarks, the SYBYL atom-type models of t his benchmark consistently perform better than their
ECIF atom-type counterparts. Also, the two-scale kernel mo del improves the performance compared to their single-
scale versions. The best-performing model for this benchma rk is the two-scale Lorentz kernel SYBYL atom-type model
sybylGGLL,L
13.5,0.5;13.0,9.5with Pearson 's correlation coeﬃcient Rp=0.834. Moreover, Figure 9a reveals the scoring power
of our model in this benchmark. Our proposed GGL model stands at the top with reported Rp=0.834 while AGL-Score
is the runner-up with Rp=0.830.
Table 4: Performance of various GGL models on CASF–2007 test set .
Pearson 'sRpof single-scale Model Pearson 'sRpof two-scale Model
Model Trained with SRTrained with SGModel Trained with SG
sybylGGLE
2.5,0.5 0.803 0.824sybylGGLE,E
2.5,0.5;19.0,9.0 0.833
sybylGGLL
13.5,0.5 0.807 0.827sybylGGLL,L
13.5,0.5;13.0,9.5 0.834
ecifGGLE
17.5,1.5 0.794 0.807ecifGGLE,E
17.5,1.5;16.5,8.5 0.811
ecifGGLL
15.5,1.5 0.792 0.805ecifGGLL,L
15.5,1.5;15.0,7.5 0.809
9Figure 7: Optimized parameters forsybylGGL model for CASF–2007 benchmark. The best parameterslocat ions
are marked by “x”. The optimal parameters for (a) single-scale ex ponential kernel model are ( κ,τ) = (2.5,0.5)
with the corresponding median Rp= 0.745 and (b) single-scale Lorentz kernel model are ( κ,τ) = (13.5,0.5)
with corresponding median Rp= 0.746. The optimal second kernel parameters for (c) two-scale ex ponential
kernel model are ( κ,τ) = (19.0,9.0) with the correspondingmedian Rp= 0.747 and (d) two-scale Lorentz kernel
model are ( κ,τ) = (13.0,9.5) with the corresponding median Rp= 0.747.
Figure 8: Optimized parameters forecifGGL model for CASF–2007 benchmark. The best parameters locat ions
are marked by “x”. The optimal parameters for (a) single-scale ex ponential kernel model are ( κ,τ) = (17.5,1.5)
with the corresponding median Rp= 0.739 and (b) single-scale Lorentz kernel model are ( κ,τ) = (15.5,1.5)
with corresponding median Rp= 0.738. The optimal second kernel parameters for (c) two-scale ex ponential
kernel model are ( κ,τ) = (16.5,8.5) with the correspondingmedian Rp= 0.741 and (d) two-scale Lorentz kernel
model are ( κ,τ) = (15.0,7.5) with the corresponding median Rp= 0.742.
10a
 b c
Figure 9: Performance comparison of diﬀerent scoring functions o n CASF benchmarks. Our proposed model
in this work, GGL-Score, is highlighted in red, and the rest is in purple. a) CASF–2007: the performances of
other methods taken from previous studies [ 33,31,44,45,46,47,48]. OursybylGGL-Score achieves Rp=0.834
b) CASF–2013: the other results are extracted from [ 48,40,45]. OursybylGGL-Score achieves Rp=0.848. c)
CASF–2016: oursybylGGL-Score achieves Rp=0.873, other scoring functions are discussed in [ 41,49,48].
4 Conclusion
The binding aﬃnity between a ligand and its receptor protein is a key component in structure-based drug design.
Although signiﬁcant progress has been made over the past dec ades, an accurate prediction of protein-ligand binding
aﬃnity remains a challenging task. Geometric graph theorie s are widely used in the study of molecular and biomolecular
systems. Furthermore, the element-type graph coloring-ba sed multiscale weighted colored graph (MWCG) approaches
have particularly shown success in the task of binding aﬃnit y prediction [ 1,2]. On the other hand, SYBYL atom-type
interaction and extended connectivity interactive featur es (ECIF) have enjoyed their success in molecular property
prediction [ 32,35]. Therefore, with an aim to develop robust and reliable scor ing functions for large and diverse
protein-ligand datasets, the present work combines the gra ph learning model and extended atom types to give rise
to novel geometric graph theory-based multiscale weighted colored graph (MWCG) descriptors for the protein-ligand
complex where the graph coloring is based on SYBYL atom-type and ECIF atom-type interactions. By pairing with
the gradient boosting decision tree (GBDT) machine learnin g algorithm, our approach results in two diﬀerent methods,
namelysybylGGL-Score andecifGGL-Score. We explore the optimal hyperparameters of our mo dels using a ﬁve-fold
cross-validation on the training set of three commonly used benchmarks in drug design area, namely CASF-2007 [ 33],
CASF-2013 [ 40], and CASF-2016 [ 41]. For the binding aﬃnity prediction task of each benchmark’ s test set complexes,
we consider two training sets– the reﬁned set excluding the c ore set and the general set excluding the core set. Our
model performs signiﬁcantly better in each benchmark when t rained with the larger training set. It is also found that
the SYBYL atom-type modelssybylGGL-Score outperform the ECIF atom-type modelsecifGGL-Score in most cases.
To demonstrate the scoring power of the proposed models, man y state-of-the-art scoring functions are considered in
each benchmark. Impressively, oursybylGGL-Score outperforms other models by a wide margin in all th ree PDBbind
benchmarks. In addition to the accuracy and robustness, our model is computationally inexpensive– the only required
structural input is the atom types and coordinates. Moreove r, our model can be applied in a vast majority of molecular
property predictions such as toxicity, solubility, protei n mutation, protein folding, and protein-nucleic acid inte ractions.
5 Appendix A
In this section, we explore the performance of the basic geom etric graph approach model that considers element-type
interactions presented in [ 1] using the bigger training set SG, i.e. the general set excluding the core set of each benchmar k.
We carry out a similar experiment as we did for our present mod el. For simplicity, we use the notation GGLα
κ,τfor a
single-scale kernel and GGLα1,α2κ1,τ1;κ2,τ2for a two-scale kernel basic element-type geometric graph l earning model. To ﬁnd
the optimized parameters for each benchmark, we carry out a ﬁ ve-fold CV on the training set SRi.e. the reﬁned set
excluding the core set of the corresponding benchmark. For C ASF–2016 benchmark, the best single kernel models are
11found to be GGLE
3.5,2.0(Figure A1a)and GGLL
16.0,2.0(Figure A1b) with median Pearson correlation Rp=0.769 for both
models. The best two kernel models for CASF–2016 are GGLE,E
3.5,2.0;16.0,3.0(Figure A1c) and GGLL,L
16.0,2.0;12.0,1.5(Figure
A1d) with median Rp=0.773 for both models. The performance of all models on the t est set of CASF–2016 benchmark
are reported in Table A1. It is interesting to ﬁnd that the performance of each model i mproved signiﬁcantly when trained
with the bigger training data SGi.e. PDBbind v2016 general set excluding the core set. The be st performing model
for this benchmark is the two-scale exponential kernel mode l GGLE,E
3.5,2.0;16.0,3.0withRp=0.859. We note that both of
our proposed GGL models, SYBYL atom-type and ECIF atom-type model, perform promisingly better (with reported
Rp=0.873 and 0.868 respectively) than the basic element-type model.
The CV performance of CASF–2013 benchmark (Figure A2), reveals that the best models for this benchmark are
GGLE
5.0,2.0and GGLL
16.0,2.0for the single kernel, and GGLE,E
5.0,2.0;15.0,3.0and GGLL,L
16.0,2.0;11.5,1.5for two kernels, with
medianRp= 0.774, 0.773, 0.778, and 0.776, respectively. Table A1indicates that the use of the bigger training set SG
improved the performance of these models. The best performi ng model for this benchmark is the single-scale exponential
model GGLE
5.0,2.0with reported Rp=0.821. However, our proposed SYBYL atom-type GGL model out performs the basic
GGL model by a huge margin with reported Rp=0.848 for this benchmark.
Figure A3plots the CV performance for CASF–2007 benchmark. The best m odels for this benchmark are GGLE
17.0,1.5
and GGLL
17.0,1.5for the single kernel, and GGLE,E
17.0,1.5;16.5,3.0and GGLL,L
17.0,1.5;6.5,10.0for two kernels, with median Rp=
0.724, 0.724, 0.733, and 0.730, respectively. The performa nce of these models is presented in Table A1. We observe that
the use of the bigger training data signiﬁcantly improves th e performance of each model for this benchmark as well. While
the best performing basic GGL model for this benchmark is the two-scale exponential kernel model GGLE,E
17.0,1.5;16.5,3.0
with Pearson’s Rp=0.833, our proposed SYBYL atom-type GGL model for this benc hmark perform slightly better with
Rp=0.834.
Figure A1: Optimized parameters for basic GGL model for CASF–201 6 benchmark. The best parameters
locations are marked by “x”. The optimal parameters for (a) single -scale exponential kernel model are ( κ,τ) =
(3.5,2.0) with the corresponding median Rp= 0.769 and (b) single-scale Lorentz kernel model are ( κ,τ) =
(16.0,2.0) with corresponding median Rp= 0.769. The optimal second kernel parameters for (c) two-scale
exponential kernel model are ( κ,τ) = (16.0,3.0) with the corresponding median Rp= 0.773 and (d) two-scale
Lorentz kernel model are ( κ,τ) = (12.0,1.5) with the corresponding median Rp= 0.773.
Figure A2: Optimized parameters for basic GGL model for CASF–201 3 benchmark. The best parameters
locations are marked by “x”. The optimal parameters for (a) single -scale exponential kernel model are ( κ,τ) =
(5.0,2.0) with the corresponding median Rp= 0.774 and (b) single-scale Lorentz kernel model are ( κ,τ) =
(16.0,2.0) with corresponding median Rp= 0.773. The optimal second kernel parameters for (c) two-scale
exponential kernel model are ( κ,τ) = (15.0,3.0) with the corresponding median Rp= 0.778 and (d) two-scale
Lorentz kernel model are ( κ,τ) = (11.5,1.5) with the corresponding median Rp= 0.776.
12Figure A3: Optimized parameters for basic GGL model for CASF–200 7 benchmark. The best parameters
locations are marked by “x”. The optimal parameters for (a) single -scale exponential kernel model are ( κ,τ) =
(17.0,1.5) with the corresponding median Rp= 0.724 and (b) single-scale Lorentz kernel model are ( κ,τ) =
(17.0,1.5) with corresponding median Rp= 0.724. The optimal second kernel parameters for (c) two-scale
exponential kernel model are ( κ,τ) = (16.5,3.0) with the corresponding median Rp= 0.733 and (d) two-scale
Lorentz kernel model are ( κ,τ) = (6.5,10.0) with the corresponding median Rp= 0.730.
Table A1: Performance of various basic GGL models on all benchmark test sets.
Pearson 'sRpof single-scale Model Pearson 'sRpof two-scale Model
Model Trained with SRTrained with SGModel Trained with SG
CASF–2016GGLE
3.5,2.0 0.843 0.856 GGLE,E
3.5,2.0;16.0,3.0 0.859
GGLL
16.0,2.0 0.839 0.848 GGLL,L
16.0,2.0;12.0,1.5 0.856
CASF–2013GGLE
5.0,2.0 0.794 0.821 GGLE,E
5.0,2.0;15.0,3.0 0.818
GGLL
16.0,2.0 0.793 0.809 GGLL,L
16.0,2.0;11.5,1.5 0.818
CASF–2007GGLE
17.0,1.5 0.809 0.828 GGLE,E
17.0,1.5;16.5,3.0 0.833
GGLL
17.0,1.5 0.815 0.830 GGLL,L
17.0,1.5;6.5,10.0 0.830
136 Data and Software Availability
The source code is available at Github: https://github.com/NguyenLabUKY/GGL-ETA-Score .
7 Competing interests
No competing interest is declared.
8 Acknowledgments
This work is supported in part by funds from the National Scie nce Foundation (NSF: # 2053284 and # 2151802), and
University of Kentucky Startup Fund.
References
[1] Duc D Nguyen, Tian Xiao, Menglun Wang, and Guo-Wei Wei. Ri gidity strengthening: A mechanism for protein–
ligand binding. Journal of chemical information and modeling , 57(7):1715–1721, 2017.
[2] Duc Duy Nguyen and Guo-Wei Wei. Agl-score: algebraic gra ph learning score for protein–ligand binding scoring,
ranking, docking, and screening. Journal of chemical information and modeling , 59(7):3291–3304, 2019.
[3] Dong Chen, Kaifu Gao, Duc Duy Nguyen, Xin Chen, Yi Jiang, G uo-Wei Wei, and Feng Pan. Algebraic graph-
assisted bidirectional transformers for molecular proper ty prediction. Nature Communications , 12(1):1–9, 2021.
[4] Rui Wang, Duc Duy Nguyen, and Guo-Wei Wei. Persistent spe ctral graph. International journal for numerical
methods in biomedical engineering , 36(9):e3376, 2020.
[5] Zhenyu Meng and Kelin Xia. Persistent spectral–based ma chine learning (perspect ml) for protein-ligand binding
aﬃnity prediction. Science Advances , 7(19):eabc5329, 2021.
[6] Nenad Trinajstic. Chemical graph theory . CRC press, 2018.
[7] Harry P Schultz. Topological organic chemistry. 1. grap h theory and topological indices of alkanes. Journal of
Chemical Information and Computer Sciences , 29(3):227–228, 1989.
[8] Dusanka Janezic, Ante Milicevic, Sonja Nikolic, and Nen ad Trinajstic. Graph-theoretical matrices in chemistry .
CRC Press, 2015.
[9] A Angeleska, N Jonoska, and M Saito. Dna rearrangement th rough assembly graphs. Discrete. Appl. Math ,
157:3020–3037, 2009.
[10] N. Go, T. Noguti, and T. Nishikawa. Dynamics of a small gl obular protein in terms of low-frequency vibrational
modes.Proc. Natl. Acad. Sci. , 80:3696 – 3700, 1983.
[11] M. Tasumi, H. Takenchi, S. Ataka, A. M. Dwidedi, and S. Kr imm. Normal vibrations of proteins: Glucagon.
Biopolymers , 21:711 – 714, 1982.
[12] Bernard R Brooks, Robert E Bruccoleri, Barry D Olafson, David J States, S a Swaminathan, and Martin Karplus.
Charmm: a program for macromolecular energy, minimization , and dynamics calculations. Journal of computational
chemistry , 4(2):187–217, 1983.
[13] Michael Levitt, Christian Sander, and Peter S Stern. Pr otein normal-mode dynamics: trypsin inhibitor, crambin,
ribonuclease and lysozyme. Journal of molecular biology , 181(3):423–447, 1985.
[14] Ivet Bahar, Ali Rana Atilgan, and Burak Erman. Direct ev aluation of thermal ﬂuctuations in proteins using a
single-parameter harmonic potential. Folding and Design , 2(3):173–181, 1997.
[15] Paul J Flory. Statistical thermodynamics of random net works.Proceedings of the Royal Society of London. A.
Mathematical and Physical Sciences , 351(1666):351–380, 1976.
[16] Ivet Bahar, Ali Rana Atilgan, Melik C Demirel, and Burak Erman. Vibrational dynamics of folded proteins:
signiﬁcance of slow and fast motions in relation to function and stability. Physical Review Letters , 80(12):2733,
1998.
[17] Ali Rana Atilgan, SR Durell, Robert L Jernigan, Melik C D emirel, O Keskin, and Ivet Bahar. Anisotropy of
ﬂuctuation dynamics of proteins with an elastic network mod el.Biophysical journal , 80(1):505–515, 2001.
[18] Konrad Hinsen. Analysis of domain motions by approxima te normal mode calculations. Proteins: Structure,
Function, and Bioinformatics , 33(3):417–429, 1998.
[19] Florence Tama and Y-H Sanejouand. Conformational chan ge of proteins arising from normal mode calculations.
Protein engineering , 14(1):1–6, 2001.
[20] Kelin Xia, Kristopher Opron, and Guo-Wei Wei. Multisca le multiphysics and multidomain models—ﬂexibility and
rigidity. The Journal of chemical physics , 139(19):11B614 1, 2013.
[21] Kristopher Opron, Kelin Xia, and Guo-Wei Wei. Fast and a nisotropic ﬂexibility-rigidity index for protein ﬂexibil ity
and ﬂuctuation analysis. The Journal of chemical physics , 140(23):06B617 1, 2014.
14[22] Kristopher Opron, Kelin Xia, Zach Burton, and Guo-Wei W ei. Flexibility–rigidity index for protein–nucleic acid
ﬂexibility and ﬂuctuation analysis. Journal of computational chemistry , 37(14):1283–1295, 2016.
[23] Duc Duy Nguyen, Kelin Xia, and Guo-Wei Wei. Generalized ﬂexibility-rigidity index. The Journal of chemical
physics , 144(23):234106, 2016.
[24] Kristopher Opron, Kelin Xia, and Guo-Wei Wei. Communic ation: Capturing protein multiscale thermal ﬂuctuations.
The Journal of chemical physics , 142(21):06B401 1, 2015.
[25] David Bramer and Guo-Wei Wei. Multiscale weighted colo red graphs for protein ﬂexibility and rigidity analysis.
The Journal of chemical physics , 148(5):054103, 2018.
[26] Duc Duy Nguyen, Zixuan Cang, Kedi Wu, Menglun Wang, Yin C ao, and Guo-Wei Wei. Mathematical deep learning
for pose and binding aﬃnity prediction and ranking in d3r gra nd challenges. Journal of computer-aided molecular
design , 33(1):71–82, 2019.
[27] Duc Duy Nguyen, Kaifu Gao, Menglun Wang, and Guo-Wei Wei . Mathdl: mathematical deep learning for d3r
grand challenge 4. Journal of computer-aided molecular design , 34(2):131–147, 2020.
[28] Zied Gaieb, Shuai Liu, Symon Gathiaka, Michael Chiu, Hu anwang Yang, Chenghua Shao, Victoria A Feher,
W Patrick Walters, Bernd Kuhn, and Markus G Rudolph. D3r gran d challenge 2: blind prediction of protein–
ligand poses, aﬃnity rankings, and relative binding free en ergies.Journal of computer-aided molecular design ,
32(1):1–20, 2018.
[29] Zied Gaieb, Conor Parks, Michael Chiu, Huanwang Yang, C henghua Shao, Patrick Walters, Millard Lambert, Neysa
Nevins, Scott D Bembenek, and Stephen K Burley. D3r grand cha llenge 3: Blind prediction of protein-ligand poses
and aﬃnity rankings. Journal of computer-aided molecular design , 33(1):1–18, 2018.
[30] Conor D Parks, Zied Gaieb, Michael Chiu, Huanwang Yang, Chenghua Shao, W Patrick Walters, Johanna M
Jansen, Georgia McGaughey, Richard A Lewis, and Scott D Bemb enek. D3r grand challenge 4: blind prediction
of protein–ligand poses, aﬃnity rankings, and relative bin ding free energies. Journal of computer-aided molecular
design , 34(2):99–119, 2020.
[31] Pedro J Ballester and John BO Mitchell. A machine learni ng approach to predicting protein–ligand binding aﬃnity
with applications to molecular docking. Bioinformatics , 26(9):1169–1175, 2010.
[32] Pedro J Ballester, Adrian Schreyer, and Tom L Blundell. Does a more precise chemical description of protein–ligand
complexes lead to more accurate prediction of binding aﬃnit y?Journal of chemical information and modeling ,
54(3):944–955, 2014.
[33] Tiejun Cheng, Xun Li, Yan Li, Zhihai Liu, and Renxiao Wan g. Comparative assessment of scoring functions on a
diverse test set. Journal of chemical information and modeling , 49(4):1079–1093, 2009.
[34] Maciej W´ ojcikowski, Micha/suppress l Kukie/suppress lka, Marta M Stepn iewska-Dziubinska, and Pawel Siedlecki. Development of a
protein–ligand extended connectivity (plec) ﬁngerprint a nd its application for binding aﬃnity predictions. Bioin-
formatics , 35(8):1334–1341, 2019.
[35] Norberto S´ anchez-Cruz, Jos´ e L Medina-Franco, Jordi Mestres, and Xavier Barril. Extended connectivity interac tion
features: improving binding aﬃnity prediction through che mical description. Bioinformatics , 37(10):1376–1382,
2021.
[36] David Rogers and Mathew Hahn. Extended-connectivity ﬁ ngerprints. Journal of chemical information and modeling ,
50(5):742–754, 2010.
[37] Duc Duy Nguyen and Guo-Wei Wei. DG-GL: diﬀerential geom etry-based geometric learning of molecular datasets.
International journal for numerical methods in biomedical engineering , 35(3):e3179, 2019.
[38] Md Masud Rana and Duc Duy Nguyen. Eisa-score: Element in teractive surface area score for protein-ligand binding
aﬃnity prediction. arXiv preprint arXiv:2206.00611 , 2022.
[39] Zixuan Cang and Guo-Wei Wei. Integration of element spe ciﬁc persistent homology and machine learning for
protein-ligand binding aﬃnity prediction. International journal for numerical methods in biomedical engineering ,
34(2):e2914, 2018.
[40] Yan Li, Li Han, Zhihai Liu, and Renxiao Wang. Comparativ e assessment of scoring functions on an updated
benchmark: 2. evaluation methods and general results. Journal of chemical information and modeling , 54(6):1717–
1736, 2014.
[41] Minyi Su, Qifan Yang, Yu Du, Guoqin Feng, Zhihai Liu, Yan Li, and Renxiao Wang. Comparative assessment of
scoring functions: the casf-2016 update. Journal of chemical information and modeling , 59(2):895–913, 2018.
[42] Jian Jiang, Rui Wang, and Guo-Wei Wei. Ggl-tox: geometr ic graph learning for toxicity prediction. Journal of
chemical information and modeling , 61(4):1691–1700, 2021.
[43] Kelin Xia, Kristopher Opron, and Guo-Wei Wei. Multisca le gaussian network model (mgnm) and multiscale
anisotropic network model (manm). The Journal of chemical physics , 143(20):11B616 1, 2015.
[44] Guo-Bo Li, Ling-Ling Yang, Wen-Jing Wang, Lin-Li Li, an d Sheng-Yong Yang. Id-score: a new empirical scoring
function based on a comprehensive set of descriptors relate d to protein–ligand interactions. Journal of chemical
information and modeling , 53(3):592–600, 2013.
15[45] Hongjian Li, Kwong-Sak Leung, Man-Hon Wong, and Pedro J Ballester. Improving autodock vina using random
forest: the growing accuracy of binding aﬃnity prediction b y the eﬀective exploitation of larger data sets. Molecular
informatics , 34(2-3):115–126, 2015.
[46] Hongjian Li, Kwong-Sak Leung, Man-Hon Wong, and Pedro J Ballester. Substituting random forest for multiple
linear regression improves binding aﬃnity prediction of sc oring functions: Cyscore as a case study. BMC bioinfor-
matics , 15(1):1–12, 2014.
[47] Yang Cao and Lei Li. Improved protein–ligand binding aﬃ nity prediction by using a curvature-dependent surface-
area model. Bioinformatics , 30(12):1674–1680, 2014.
[48] Cheng Wang and Yingkai Zhang. Improving scoring-docki ng-screening powers of protein–ligand scoring functions
using random forest. Journal of computational chemistry , 38(3):169–177, 2017.
[49] Marta M Stepniewska-Dziubinska, Piotr Zielenkiewicz , and Pawel Siedlecki. Development and evaluation of a deep
learning model for protein-ligand binding aﬃnity predicti on.Bioinformatics , 1:9, 2018.
16