Dynamic Background Reconstruction via MAE for Infrared Small Target
Detection
Jingchao Peng, Haitao Zhao, Kaijie Zhao, Zhongze Wang, and Lujian Yao
Automation Department, School of Information Science and Engineering,
East China University of Science and Technology
Abstract
Infrared small target detection (ISTD) under complex
backgrounds is a difﬁcult problem, for the differences
between targets and backgrounds are not easy to
distinguish. Background reconstruction is one of the
methods to deal with this problem. This paper proposes an
ISTD method based on background reconstruction called
Dynamic Background Reconstruction (DBR). DBR consists
of three modules: a dynamic shift window module (DSW),
a background reconstruction module (BR), and a detection
head (DH). BR takes advantage of Vision Transformers in
reconstructing missing patches and adopts a grid masking
strategy with a masking ratio of 50% to reconstruct clean
backgrounds without targets. To avoid dividing one target
into two neighboring patches, resulting in reconstructing
failure, DSW is performed before input embedding. DSW
calculates offsets, according to which infrared images
dynamically shift. To reduce False Positive (FP) cases
caused by regarding reconstruction errors as targets, DH
utilizes a structure of densely connected Transformer to
further improve the detection performance. Experimental
results show that DBR achieves the best F1-score on the two
ISTD datasets, MFIRST (64.10%) and SIRST (75.01%).
1. Introduction
In recent years, infrared small-target detection (ISTD) is
becoming more and more popular [5,17,30,31]. Compared
with visible light imaging, infrared imaging offers strong
anti-interference, has the ability of long-range imaging, and
can work around the clock [42]. Infrared imaging has
been widely used in various ﬁelds, especially in intelligent
robotics, autonomous driving, ﬁre warning, agricultural
production, leakage measurement, etc. [16, 27, 36, 42].
When the target is more than 10 kilometers away from the
infrared detector, it usually occupies a small region (the
target size is less than 9 9 pixels in 256256 images [3]).
In addition, due to atmospheric scattering and refraction,
(a) The structure of the naive background reconstruction method (NBR).
(b) Dividing one target into two patches will cause reconstruction failure. The
probability of dividing one target into two patches at different target sizes
Figure 1. An ISTD method based on background reconstruction
and its shortage.
optical defocusing, and various noises, infrared images have
a low signal-to-noise (SNR) ratio and low contrast with the
background [2]. These factors reduce the performance of
ISTD methods, and thus ISTD is still a challenge in the ﬁeld
of target detection.
Due to extremely small sizes and low SNR ratio,
targets in ISTD have inadequate detail information such
as contour, shapes, textures, and so on, which is what
visible light target detection methods rely on [24, 25, 27].
Therefore, most ISTD methods pay attention to differences
between targets and backgrounds. For example, ﬁlter-
based methods detect local gray differences between targets
and backgrounds [15, 18]; human-visual-system-based
(HVS) methods measure local contrast to detect targets
[4, 9–11, 33]; deep-learning-based (DL-based) methods
utilizing multiple nonlinear layers transform raw images
into high-dimensional representations, in which targets
and backgrounds have more signiﬁcant differences thanarXiv:2301.04497v2  [cs.CV]  9 Apr 2023in raw images [7, 14, 26, 29, 40, 41]. However, given
complex backgrounds, the differences between targets and
backgrounds are not easy to distinguish, resulting in poor
detection performance. Consequently, detecting small
targets under complex backgrounds is a difﬁcult problem.
Clean backgrounds without targets can alleviate the
detection problem under complex backgrounds [20]. Since
targets do not belong to backgrounds, if the regions of the
targets are masked, clean backgrounds can be obtained by
image inpainting [39], whose purpose is to produce visually
plausible structure and texture for the missing regions of
images [35]. In the last few years, the success of Vision
Transformers [8] has brought new opportunities to image
inpainting. Vision Transformers embed one input image
into 1616 patches. Compared with convolutional neural
networks, which process the whole feature map, Vision
Transformers have natural advantages in integrating mask
tokens into images. MAE [12] removes random patches
to reconstruct pixels under a high masking ratio (75%) and
works well. Therefore, the idea of masking patches with
targets and reconstructing backgrounds via MAE naturally
comes to us.
Inspired by MAE [12], we propose a naive background
reconstruction method (NBR). NBR adopts a grid masking
strategy and reconstructs one image twice with a masking
ratio of 50%. Two mask patches compose a “mutually
exclusive and collectively exhausting” (MECE) image. The
structure of NBR can be shown in Fig. 1a. NBR
assumes that masked backgrounds can be reconstructed
while masked targets cannot. The principle is that given
twice reconstruction with a masking ratio of 50%, the
probability of the target being masked is 100%. So that
NBR can reconstruct clean backgrounds without targets.
However, when a target is divided into two neighboring
patches, one-half of the target will be reconstructed based
on the other half of the target, resulting in the failure
of background reconstruction, as shown in Fig. 1b.
Furthermore, when the target size is 9 9, the probability
of dividing one target into two patches is up to 81%. It is
necessary for ISTD to avoid one target being divided into
two neighboring patches.
The existing methods of avoiding dividing one target
into two neighboring patches include enlarging the patch
window [32, 37], adopting deformable embedding methods
[34, 38], and shifting windows [19]. However, these
methods are not designed for background reconstruction
and cannot be directly used in ISTD. In speciﬁc,
the methods of avoiding dividing one target into two
neighboring patches, which can be used to reconstruct
backgrounds, need to meet the following conditions:
• The same target can only be presented in one patch.
Otherwise, the target will also be reconstructed.• The principle of MECE must be satisﬁed between
patches. Otherwise, the background will be
reconstructed incompletely.
Therefore, it is necessary to design a special method for
ISTD to avoid dividing one target into two patches.
Motivated by the above analysis, in this paper, we
propose a background-reconstruction-based ISTD method
called Dynamic Background Reconstruction (DBR), as
shown in Fig. 2. DBR consists of a dynamic shift window
module (DSW), a background reconstruction module (BR),
and a detection head (DH). First, DSW calculates an offset
base on how many pixels the target can move to the center
of the patch in raw infrared images. Then, according to
this offset, BR dynamically shifts windows before input
embedding and uses NBR to reconstruct clean backgrounds.
Finally, infrared images with targets, clean backgrounds
without targets, and their difference are concatenated and
fed into DH to improve the detection performance. Because
the detector is prone to regard reconstruction errors as
targets, the recall rate is greater than the precision rate.
We propose a weighted dice loss (WDLoss) to balance the
precision and recall rates.
In summary, our contributions are summarized below:
1. DBR is a background-reconstruction-based ISTD
method that can improve detection performance under
complex backgrounds. We propose a BR based
on Vision Transformer (MAE) to reconstruct clean
backgrounds without targets.
2. DBR can prevent the transformer from dividing one
target into two neighboring patches, which is harmful
to background reconstruction. We propose a DSW to
calculate offsets, according to which BR dynamically
shifts images before input embedding.
3. DBR is robust to reconstruction error. We propose
a DH and a WDLoss, which can separately reduce
the inﬂuence of reconstruction error on detection
performance from aspects of the network architecture
and loss function.
2. Related Works
In this section, we ﬁrst summarize ISTD methods,
including traditional and DL-based methods. Then we
introduce existing vision transformers and how they avoid
dividing one target into two patches.
2.1. Infrared Small-Target Detectio Methods
ISTD methods can be largely divided into traditional
methods and deep learning-based methods [42]. Traditional
methods utilize ﬁlters or the human visual system to detect
targets, assuming the target is brighter than its neighborFigure 2. The architecture of the Dynamic Background Reconstruction method (DBR).
pixels. To strengthen the difference between targets
and backgrounds, WSLCM [11] adopted a weighting
function and strengthened LCM [4]; AADCDD [1] took
advantage of local gray differences and employed weighting
coefﬁcients; ADMD [22] used directional information.
As for DL-based methods, due to small targets prone
to disappear in forward propagation, the key technology is
to enhance targets in complex backgrounds. For example,
ACM [5] and ALCNet [6] utilized a bottom-up local
attention module to transfer context information; IAANet
[31] leveraged a transformer encoder to obtain interior
relations between pixels; DNANet [17] proposed a dense
nested attention network to maintain target information
in deep layers; MDvsFA-cGAN [30] and CourtNet [23]
utilized two subnetworks to separately enhance targets
and suppress backgrounds. Existing ISTD methods rely
on differences between targets and backgrounds to detect
targets but cannot reconstruct clean backgrounds without
targets.
2.2. Vision Transformers
MAE [12] developed an asymmetric encoder-decoder
architecture, which masks random patches of the input
image and reconstructs the missing pixels. The
reconstruction performance is excellent when the masking
ratio is less than 75%. However, since MAE uses a ﬁxed
input embedding method, which divides the image into
1616 patches, targets are easily divided into two patches.
To avoid dividing one target into two patches, T2T-
ViT [37] and PVT-V2 [32] utilized an overlapping patch
embedding method. The overlapping patch embedding
method enlarges the patch window, and the input image
is split into patches with overlapping. Nevertheless,
this method makes the target present in two patches
simultaneously, so it is impossible to reconstruct a clean
background based on one patch with the target. PS-ViT [38] and DAT [34] build deformable input embedding
methods, focusing the model on complete regions. But this
method will destroy the complementation between patches,
breaking the principle of MECE. SwinTransformer [19]
adopted a shifted windowing scheme to connect cross-
window patches. However, the possibility of dividing
one target into two patches is doubled with two types of
shifted windows switchover. It is adverse to background
reconstruction for ISTD.
3. Proposed Method
In this section, we ﬁrst overview the architecture of
DBR. Then we introduce the main modules: DSW, BR, and
DH.
3.1. Overall Structure
The architecture of DBR can be shown in Fig. 2. Taking
an infrared image as an example, if the image is directly
embedded, the target will be divided into different patches.
To solve this problem, in DBR, the image is ﬁrst fed into
DSW to calculate an offset (x;y). The offset (x;y)
means that when the image horizontally shifts xpixels
and vertically shifts ypixels, the target moves to the
center of a patch instead of being divided into different
patches. In the background reconstruction phase, after
input embedding, the infrared image was masked twice in a
complementary way. The masking strategy is grid masking.
Theoretically, the target can be obtained by subtracting the
generated background from the original image. However,
because the generated background is not absolutely the
same as the factual background, there is inevitably a
reconstruction error. To minimize the inﬂuence of the
reconstruction error on detection performance, ﬁnally,
the original image, the generated background, and their
difference are concatenated and fed into DH.3.2. Dynamic Shift Window Module
3.2.1 Start from Single-Target
Figure 3. The structure of the dynamic shift window module
(DSW). The blue circle represents the target, and the sketch
represents the background.
The structure of DSW can be shown in Fig. 3. To
distinguish the target and the background, in Fig. 3, Fig.
6, and Fig. 7, the blue circle represents the target, and the
sketch represents the background. DSW adopts ResNet34
as the backbone and three fully-connected layers as the
head. Given an infrared images X, the backboneB(), and
headH(), the offset vector [x;y]can be obtained by:
[x;y] =H(B(X)); (1)
where x;y2R16, indicates the probability of pixels
the target should move horizontally or vertically within a
patch (patch size is 16).
In the test phase, the offset can be obtained by:
x;y= (24 argmax ([x;y]))%16; (2)
where argmax ([x;y])is the target center.
In the training phase, DSW treats the offset calculation
as a ﬁtting task. For any target, the distance from the target
to its nearest patch center is (cx;cy ). The distance vector
(cx;cy)can be obtained by rounding to the nearest integer
and one-hot encoding:
cx;cy=onehot (round (cx;cy )): (3)
DSW ﬁts the distance vector and the offset vector with MSE
loss. The speciﬁc process of generating distance vector can
be shown in Alg. 1.
Since the center vector is directional, i.e., the closer the
target is to the center, the better the effect of avoiding one
target into two patches is. Therefore, we improve one-hot
encoding by adding directional information. The difference
between the proposed encoding and one-hot encoding can
be shown in Fig. 4.
3.2.2 Extension to Multi-Targets
For multi-targets in one image, shifting the infrared image
cannot guarantee all target centers are close to their patchAlgorithm 1 The distance vector generation for a single
target.
Input: The binary image: Xgt2R208208.
Output: The distance vector: (cx;cy).
1:Obtain the target center (x; y):
(x; y) =cv.ﬁndcounters (Xgt);
where cv.ﬁndcounters is the function ﬁnding white objects
from a black background in OpenCV;
2:Calculate the distance from the target to its nearest patch
center:
(cx; cy ) = (8 x%16;8 y%16);
where “%” means modulo operation;
3:Obtain the distance vector:
(cx;cy) =onehot (round (cx; cy )):
Figure 4. The difference between one-hot encoding, label
smoothing, and ours.
centers. However, our purpose is to avoid dividing one
target into different patches, if all targets are located in
different but single patches, whether all target centers are
close to their patch centers is not essential. So we change
to another criterion which is shifting the infrared image to
make sure target centers are far from their path edges.
Compared with DSW for a single target, in the training
phase, DSW also ﬁts the distance vector and the offset
vector with MSE loss. But the process of generating
distance vector is quite different: for there are more than
one target, multiple distance vectors can be generated.
the process of generating distance vector for multi-targets
averages and weights all distance vectors according to their
target radius, which can be shown in Alg. 2, Step 35.
In the test phase, the offset can be obtained by:
x;y= 16 argmin ([x;y]); (4)
where argmin ([x;y])is the pixel that is the farthest
from the target centers. That means for multi-targets, DSW
maximizes the distance between target centers and patch
edges. The difference between the DSW of a single target
and multiple targets can be intuitively shown in Fig. 5. It isAlgorithm 2 The distance vector generation for multi-
targets.
Input: The binary image: Xgt2R208208.
Output: The distance vector: (cx;cy).
1:Obtain the target centers (xi; yi)and their radius ri:
(xi; yi); ri=cv.ﬁndcounters (Xgt); i= 1;2; : : : ; n;
where cv.ﬁndcounters is the function ﬁnding white objects
from a black background in OpenCV;
2:Calculate the distance from the target to its nearest patch
center:
(cxi; cyi) = (8 xi%16;8 yi%16);
where “%” means modulo operation;
3:foriin [1,2,. . . ,n] do
4: Obtain the distance vectors:
(cxi;cyi) =onehot (round (cxi; cyi)):
5:Average and weight all distance vectors:
(cx;cy) =((cxi;cyi)ri)
(ri);
Figure 5. The difference between DSW for a single target and
multiple targets.
worth noting that when there is only one target in an infrared
image, the target center being far from the path edges is
equivalent to the target center being close to the path center.
So there is no need to determine the number of targets in
the test image, we directly use DSW for multiple targets (as
shown in Fig. 5).
3.3. Background Reconstruction Module
The structure of BR can be shown in Fig. 6. Since
the image needs to be shifted, the down and right parts
of the image will be separately shifted to the top and left,
which makes the continuous image disconnected, resulting
in signiﬁcant differences between the two sides of theboundary. To deal with this problem, the infrared image is
ﬁrst padded to ensure no boundary after shifting. The image
is then shifted according to the offset calculated by DSW,
moving the target to the center of the patch. After that,
the infrared image is masked twice with the grid masking
strategy. The masking ratio is 50%. The two masked
images are complementary. In other words, they satisfy
the principle of MECE. After masking, MAE reconstructs
the masked part. Finally, the background of the original
image size can be generated by cropping according to the
offset calculated by DSW. Given an original image Xori2
R208208, the algorithm for reconstructing its background
can be shown in Alg. 3.
Algorithm 3 Background reconstruction.
Input: The original image: Xori2R208208;
the offset: (x;y).
Output: The generated background: Xgen2R208208.
1:Pad the left and top regions of Xoriwith a width of 16 pixels
to the right and bottom: Xori!X
ori2R224224;
2:Horizontally shift the padded image X
oribyxpixels, and
vertically shift by ypixels;
3:Embed X
orito the feature: X
ori!Fori2RBPC;
4:Complementarily grid-mask Foritwice with a masking ratio
of 50%: Fori!(F1; F2); Fi2RBPC
2; F1[F2=Fori;
5:forFiin(F1; F2)do
6: Reconstruct Fiby MAE [12]: Fi!FC
i2RBPC
2;
7:Combine the reconstructed patch to generate the background:
Fgen=FC
1[FC
2;
8:De-embed the feature to the image: Fgen!X
gen2
R224224;
9:Crop the image to get the generated background: Xgen=
X
gen[x: x+ 208 ;y: y+ 208] .
3.4. Detection Head
The structure of DH can be shown in Fig. 7. Because
the generated background is not absolutely the same as
the factual background, there is inevitably a reconstruction
error. The purpose of DH is to reduce the inﬂuence of the
reconstruction error on detection performance. Inspired by
CourtNet [23], DH utilizes a densely connected transformer
structure consisting of 12 blocks. Consider an original
imageXori2RB1HWand a generated background
Xgen2RB1HW, the batch size is B. Each image
has one gray channel, and its width and height are Wand
H, respectively. First, Xori,Xgen, and their difference are
concatenated and encoded to the feature F2RBPC,
whereP=pp,C=H
pW
pc.
Then the feature Fgoes through 12 blocks to get the
outputsY2RB1HW, which are binary maps with 0
and 1 indicating whether each pixel is a target. Suppose the
input of thei-th block isFi 12RBPCi 1, the output is
F(Fi 1)2RBP32. Then the input feature and outputFigure 6. The structure of the background reconstruction module (BR).
Figure 7. The structure of the detection head (DH).
feature are concatenated:
Fi=concat (Fi 1;F(Fi 1))2RBPCi; (5)
whereCi=Ci 1+32. For ViT, the input feature dimension
of each block is equal to the output feature dimension,
and the input feature is not preserved. The output feature
dimension of our block is 32. The input feature is
concatenated with the output feature. The advantage of
concatenation is to 1) reduce the amount of computation
and increase the detection speed; 2) preserve generated
backgrounds, so that the detector can take full advantage
of the background-reconstruction-based method.
Since the difference of XoriandXgencontains targets
(TP) and reconstruction error (FP), DH is prone to have
a high recall rate and a low precision rate. To balance
the precision and recall rates, inspired by Generalised Dice
Loss [28], we propose a weighted dice loss (WDLoss).
Given the result of DH as Yand the ground truth as ^Y,
WDLoss can be obtained by:
LWDL= log(2(Y^Y)
(Y) +(^Y)); (6)
wheremeans the weighting factor. When  >1, the recall
rate is greater than the precision rate, while 0<  < 1,
the precision rate is greater than the recall rate, here we set
= 2e 3.4. Experiments
We train and test DBR on the PyTorch platform with
I7-10700K CPU and RTX TITAN GPU. We use two ISTD
datasets, MFIRST [30] and SIRST [5], to train and evaluate
DBR. MFIRST contains 9900 training images and 100 test
images, and SIRST contains 341 training images and 86 test
images. For training settings, we utilize Adam and warm-
up to train DBR. The warm-up steps are 200, the beginning
learning rate is 4e-5, the max learning rate is 2.5e-4, and
the number of epochs is 150. More detail can be referred to
https://github.com/PengJingchao/DBR.
As for evaluation metrics, we use the precision rate, the
recall rate, and F1-score to evaluate the binary segmentation
result, which is the same as [30]. The precision rate, the
recall rate, and F1-score are calculated by:
Precision =TP
TP+FP; (7)
Recall =TP
TP+FN; (8)
F1 Score =2PrecisionRecall
Precision +Recall: (9)
Note that a high F-1 score indicates good performance
rather than only achieving high precision or recall rates [30].
4.1. The Precision of DSW
The precision of DSW is crucial for background
reconstruction, for if DSW cannot ﬁnd the real target
centers, offsets may be wrong, resulting in dividing one
target into different patches. To this end, we calculate the
precision rate under the different thresholds. The results can
be shown in Fig. 8. From the ﬁgure, we can ﬁnd that 1) all
target centers can be located within 8 pixels; 2) DSW can
ﬁnd target centers within 5 pixels with a precision rate of
75%. Considering the small target size ( 99) and patch
size ( 1616), this result is acceptable.
However, good performance does not mean that using
DSW alone can satisfy the ISTD problem. We change a
head of DSW that uses an upsampling layer and a 11Figure 8. The precision rate of DSW under different thresholds.
convolutional layer to directly detect targets, i.e., output
binary maps whose size is equal to the size of original
infrared images, with “1” representing the pixel belonging
to targets and “0” representing the target belonging to
backgrounds. The results can be shown in the ﬁrst line of
Tab. 1 (DBR_v0). Using DSW alone to detect targets, F1-
score is only 23.88%, which is lower compared with our
DBR (64.10%). This is because the ﬁnal purpose of ISTD
is to determine whether each pixel belongs to the target,
in other words, ISTD is a pixel-to-pixel task. Due to the
resolution of the feature map of DSW is gradually decrease,
DSW is not suitable to pixel-to-pixel tasks.
4.2. MAE Promotes Performance under Complex
Backgrounds
In traditional ISTD methods, background reconstruction,
background modeling, or background estimation belong to
the same kind of technology, which has long been studied
by scholars and belongs to a relatively classical technology.
From the information theory, the ideal feature extractor
decomposes the image information H(Image) consists of
two parts [13]:
H(Image) =H(Innovation) + H(Prior Knowledge) ;
(10)
whereH(Innovation) denotes the novelty part, say small
targets to be detected; and H(Prior Knowledge) is the
redundant information, say backgrounds which should be
suppressed by background-reconstruction-based methods.
The background-reconstruction-based methods had been
proven effective for small target detection when the detector
is stationary or the background is simple. On the contrary,
if the background is complex and the clutter interference is
serious, traditional background-reconstruction-based ISTD
methods would fail. That is because when the background
is complex, the information of the background does not
only inH(Prior Knowledge) , but also in H(Innovation) ,resulting in the target in H(Innovation) cannot be
distinguished from the background.
Deep-learning-based methods, however, have strong
representation abilities, which can encode background
information into H(Prior Knowledge) as much as
possible. And H(Innovation) only contains target
information, so the targets can be distinguished from
backgrounds. To this end, we select several infrared images
with complex backgrounds and plot the spectral residual
(SR) saliency maps of original images and generated
backgrounds, as shown in Fig. 9. From the ﬁgure, we can
ﬁnd that in the original images, not only targets but also
parts of backgrounds are prominent. Directly using original
images cannot distinguish targets from backgrounds. But
in the backgrounds generated by MAE, SR saliency maps
are quite smooth and small compared with that of original
infrared images, which is beneﬁcial to the ISTD problem.
4.3. Ablation Study
Table 1. Ablation study on the MFIRST dataset. “ ” indicates
using the corresponding module, while “ ” indicates not.
Methods DSW BR DH Precision Recall F1Flops ParamsFPS(GMac) (M)
DBR_v0   15.91 80.41 23.88 3.26 21.62 76.94
DBR_v1  27.72 32.53 20.91 70.30 329.24 26.39
DBR_v2   36.34 34.78 25.34 73.56 350.86 23.42
DBR_v3   57.37 70.71 58.66 5.61 34.26 55.56
DBR_v4   62.04 71.10 62.63 75.91 363.50 21.69
DBR    63.79 72.24 64.10 79.17 385.12 19.96
In order to demonstrate the effectiveness of our major
modules, we implement three variations, separately pruning
DSW, BR, or DH. The comparison results on MFIRST
dataset are shown in Tab. 1. Among them, DSW cannot
exist with DH, so we excluded this method in Tab. 1.
Compared DBR_v1 with DBR_v2, DSW improves
F1-score by 4.43%, compared DBR_v4 with DBR,
DSW improves F1-score by 1.47%. This is because,
without DSW, BR is prone to divide one target into
different patches, resulting in background reconstruction
failure, which affects the detection performance. To
visually demonstrate the impact of DSW on background
reconstruction performance, we separately generate
backgrounds with DSW and without DSW, as shown in
Fig. 10. When the target is divided into different patches,
directly reconstructing backgrounds will lead to targets
also being reconstructed (the second column). With DSW,
the target can be divided into one patch. Due to targets do
not belong to backgrounds, targets cannot be reconstructed
(the last column).
Compared DBR_v3 with DBR_v4, BR improves F1-
score by 3.97%, which means that BR can improve
detection performance under complex backgrounds. Small
and dim infrared targets are difﬁcult to distinguish fromFigure 9. The spectral residual saliency maps of original infrared images and generated backgrounds.
backgrounds under complex backgrounds. With BR, targets
can be easily distinguished by the difference between
original images and generated backgrounds.
Compared DBR_v2 with DBR, DH improves F1-score
by 38.76%. This is because the generated background
is not absolutely the same as the factual background. If
the differences between original images and generated
backgrounds are directly used as results, the detector
will regard reconstruction errors as targets, reducing the
detection performance. Our proposed DBR achieves the
best F1-score of 64.10%, which means that the combination
of DSW, BR, and DH can achieve the best detection
performance.
As for computational cost, DSW and DH have 4.12%
and 7.09% of all Flops; 5.61% and 8.90% of all Params,
respectively. Thus most of the computational cost comes
from BR. This is because Considering the reconstruction
quality will seriously affect the detection performance;
simple and small reconstruction models will lead to
signiﬁcant reconstruction errors. Therefore, we utilize
a mature but large Vision Transformer (MAE-ViT-large[12]) for background reconstruction. Given the best
performance of DBR and the detection speed (19.96 FPS),
this computational cost is acceptable.
4.4. Comparison with Other Methods
We compare DBR with traditional methods (WSLCM
[11], TLLCM [10], ADMD [22], MSPCM [21], and
AADCDD [1]) and DL-based methods (MDvsFA-cGAN
[30], ALCNet [6], ACM [5], DNANet [17], CourtNet
[23], and IAANet [31]). Wang et al. [30] emphasize that
F-measure indicates good performance rather than only
achieving high precision or recall rates. DBR gets the best
results for F1-score in two datasets over other methods,
as shown in Tab. 2. In speciﬁc, DBR achieves 64.10%
and 75.01% for F1-score, which is 0.2% and 1.75% higher
than the second-best method (IAANet). Among DL-based
methods, DBR has the closest precision rate and recall rate,
indicating the effectiveness of the proposed weighted dice
loss in balancing the precision rate and the recall rate.Figure 10. The performance comparison of background generation without DSW and with DSW.
Table 2. Comparison of different methods which were evaluated on MFIRST and SIRST.
MethodsMFIRST SIRST
Precision Recall F1 Precision Recall F1
ADMD [22] 40.75 44.21 35.77 57.09 69.03 54.35
MSPCM [21] 49.23 49.36 39.07 69.17 69.23 62.80
AADCDD [1] 50.16 56.19 42.12 78.94 62.63 64.04
TLLCM [10] 57.70 46.11 45.15 78.19 59.08 61.18
WSLCM [11] 68.66 61.40 58.08 74.86 71.89 66.72
MDvsFA-cGAN [30] 66.00 54.00 60.00 - - -
ALCNet [6] - - - 77.98 69.02 69.97
ACM [5] - - - 67.09 85.02 66.76
DNANet [17] 56.82 70.83 57.89 61.43 90.67 70.99
CourtNet [23] 60.87 72.61 61.80 69.60 83.33 72.81
IAANet [31] 60.60 81.78 63.90 69.25 87.98 73.26
DBR (Ours) 63.80 72.24 64.10 80.70 74.09 75.01
4.5. Qualitative Performances
In order to illustrate the detection performance of DBR,
as well as whether BR could generate clean backgrounds,
we select 10 samples from MFIRST for comparison. Fig.
11 shows infrared images (the ﬁrst column), generated
backgrounds (the second column), their differences (the
third column), ﬁnal results (the fourth column), and ground
truth (the last column).In the ﬁrst column, targets in the original infrared
images are dim and small, which is difﬁcult to distinguish
from the complex backgrounds. In the second column,
BR can generate clean backgrounds without targets. In
the differences between the original images and generated
backgrounds (the third column), the target is more apparent
than in the original images, which means that background
reconstruction beneﬁts the ISTD task. However, there are
lots of noises in images of the third column caused byFigure 11. Qualitative performances of DBR.
reconstruction errors, so the third column cannot be directly
used as the detection result. DH ﬁlters these noises, which
demonstrates the effectiveness of reducing the impact of
reconstruction errors on detection performance.
Based on all the experiments performed in this section,
we conclude that:
1. Compared with the existing ISTD methods (WSLCM
[11], TLLCM [10], ADMD [22], MSPCM [21], and
AADCDD [1], MDvsFA-cGAN [30], ALCNet [6],
ACM [5], DNANet [17], CourtNet [23], and IAANet
[31]), DBR achieves the best F1-score on the two ISTD
datasets, MFIRST (64.10%) and SIRST (75.01%), and
its speed is 19.96 FPS.
2. BR can generate clean backgrounds without targets.
The performance of the detector with BR is better
than that of the detector without BR. Background
reconstruction beneﬁts ISTD.
3. With the consideration of dynamically shifting
windows, DSW can avoid dividing one target into two
patches. DSW beneﬁts background reconstruction.
4. DH can ﬁlter noises in the differences between original
images and generated backgrounds. DH makes the
detector robust to reconstruction errors.5. Conclusion
In this paper, we detect infrared small targets under
complex backgrounds by background reconstruction. We
propose a novel ISTD method called Dynamic Background
Reconstruction (DBR). The principle of DBR is that targets
can be detected through infrared images with targets minus
clean backgrounds without targets. To this end, we
propose a background reconstruction module (BR) based
on Transformer (MAE). BR reconstructs twice grid-masked
images with a masking ratio of 50%.
If one target was divided into two neighboring patches,
BR would fail to reconstruct a clean background. To solve
this problem, we propose a dynamic shift window module
(DSW). DSW calculates offsets with that infrared images
dynamically shift before input embedding. So that can
prevent DBR divide one target into different patches.
In order to reduce the inﬂuence of reconstruction error
on detection performance, we propose a detection head
(DH) and a weighted dice loss (WDLoss). DH utilizes a
structure of the densely connected Transformer. Original
images, generated backgrounds, and their differences are
fed into DH to get the ﬁnal results. In the training
phase, WDLoss balances the precision and recall rates.
Extensive experiments and ablation studies demonstrate the
effectiveness of the proposed main modules.
In the future, we will design a dedicated background
reconstructor for ISTD. We will design the reconstructor asa “plug-and-play” module and try integrating BR into other
ISTD methods to improve the detection performance.
References
[1] Saeid Aghaziyarati, Saed Moradi, and Hasan Talebi. Small
infrared target detection using absolute average difference
weighted by cumulative directional derivatives. Infrared
Physics & Technology , 101:78–87, 2019. 3, 8, 9, 10
[2] Zhaoyang Cao, Xuan Kong, and Guanghui Wang. False
alarm sources detection based on LNIP and local probability
distribution in infrared image. In Zhigeng Pan and Xinhong
Hei, editors, Twelfth International Conference on Graphics
and Image Processing (ICGIP 2020) , volume 11720, pages
1 – 10. International Society for Optics and Photonics, SPIE,
2021. 1
[3] Philip B. Chapple, Derek C. Bertilone, Robert S. Caprari,
Steven Angeli, and Garry N. Newsam. Target detection
in infrared and SAR terrain images using a non-Gaussian
stochastic model. In Wendell R. Watkins, Dieter Clement,
and William R. Reynolds, editors, Targets and Backgrounds:
Characterization and Representation V , volume 3699, pages
122 – 132. International Society for Optics and Photonics,
SPIE, 1999. 1
[4] C. L. Philip Chen, Hong Li, Yantao Wei, Tian Xia, and
Yuan Yan Tang. A local contrast method for small infrared
target detection. IEEE Transactions on Geoscience and
Remote Sensing , 52(1):574–581, 2014. 1, 3
[5] Yimian Dai, Yiquan Wu, Fei Zhou, and Kobus Barnard.
Asymmetric contextual modulation for infrared small target
detection. In 2021 IEEE Winter Conference on Applications
of Computer Vision (WACV) , pages 949–958, 2021. 1, 3, 6,
8, 9, 10
[6] Yimian Dai, Yiquan Wu, Fei Zhou, and Kobus Barnard.
Attentional local contrast networks for infrared small target
detection. IEEE Transactions on Geoscience and Remote
Sensing , 59(11):9813–9824, 2021. 3, 8, 9, 10
[7] Lianghui Ding, Xin Xu, Yuan Cao, Guangtao Zhai, Feng
Yang, and Liang Qian. Detection and tracking of infrared
small target by jointly using ssd and pipeline ﬁlter. Digital
Signal Processing , 110:102949, 2021. 2
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An
image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on
Learning Representations , 2021. 2
[9] Jinhui Han, Yong Ma, Bo Zhou, Fan Fan, Kun Liang, and
Yu Fang. A robust infrared small target detection algorithm
based on human visual system. IEEE Geoscience and
Remote Sensing Letters , 11(12):2168–2172, 2014. 1
[10] Jinhui Han, Saed Moradi, Iman Faramarzi, Chengyin Liu,
Honghui Zhang, and Qian Zhao. A local contrast method for
infrared small-target detection utilizing a tri-layer window.
IEEE Geoscience and Remote Sensing Letters , 17(10):1822–
1826, 2020. 1, 8, 9, 10[11] Jinhui Han, Saed Moradi, Iman Faramarzi, Honghui Zhang,
Qian Zhao, Xiaojian Zhang, and Nan Li. Infrared small
target detection based on the weighted strengthened local
contrast measure. IEEE Geoscience and Remote Sensing
Letters , 18(9):1670–1674, 2021. 1, 3, 8, 9, 10
[12] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
DollÃ¡r, and Ross Girshick. Masked autoencoders are
scalable vision learners. In 2022 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
16000–16009, 2022. 2, 3, 5, 8
[13] Xiaodi Hou and Liqing Zhang. Saliency detection: A
spectral residual approach. In 2007 IEEE Conference on
Computer Vision and Pattern Recognition , pages 1–8, 2007.
7
[14] Lian Huang, Shaosheng Dai, Tao Huang, Xiangkang Huang,
and Haining Wang. Infrared small target segmentation
with multiscale feature representation. Infrared Physics &
Technology , 116:103755, 2021. 2
[15] Liangjie Jia, Peng Rao, Yuke Zhang, Yueqi Su, and Xin
Chen. Low-snr infrared point target detection and tracking
via saliency-guided double-stage particle ﬁlter. Sensors ,
22(7), 2022. 1
[16] Moran Ju, Jiangning Luo, Guangqi Liu, and Haibo Luo.
Istdet: An efﬁcient end-to-end neural network for infrared
small target detection. Infrared Physics & Technology ,
114:103659, 2021. 1
[17] Boyang Li, Chao Xiao, Longguang Wang, Yingqian Wang,
Zaiping Lin, Miao Li, Wei An, and Yulan Guo. Dense nested
attention network for infrared small target detection. IEEE
Transactions on Image Processing , pages 1–1, 2022. 1, 3, 8,
9, 10
[18] Chang Liu, Fengying Xie, Xiaomeng Dong, Hongxia
Gao, and Haopeng Zhang. Small target detection
from infrared remote sensing images using local adaptive
thresholding. IEEE Journal of Selected Topics in Applied
Earth Observations and Remote Sensing , 15:1941–1952,
2022. 1
[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2021. 2, 3
[20] Deyong Lu, Qiang Ling, Yuanyuan Zhang, Zaiping Lin, and
Wei An. Iistd: Image inpainting-based small target detection
in a single infrared image. IEEE Journal of Selected Topics in
Applied Earth Observations and Remote Sensing , 15:7076–
7087, 2022. 2
[21] Saed Moradi, Payman Moallem, and Mohamad Farzan
Sabahi. A false-alarm aware methodology to develop robust
and efﬁcient multi-scale infrared small target detection
algorithm. Infrared Physics & Technology , 89:387–397,
2018. 8, 9, 10
[22] Saed Moradi, Payman Moallem, and Mohamad Farzan
Sabahi. Fast and robust small infrared target detection
using absolute directional mean difference algorithm. Signal
Processing , 177:107727, 2020. 3, 8, 9, 10[23] Jingchao Peng, Haitao Zhao, Zhengwei Hu, Kaijie Zhao, and
Zhongze Wang. Courtnet for infrared small-target detection.
arXiv , 2209.13780, 2022. 3, 5, 8, 9, 10
[24] Junhwan Ryu and Sungho Kim. Heterogeneous gray-
temperature fusion-based deep learning architecture for
far infrared small target detection. Journal of Sensors ,
2019:4658068, Aug 2019. 1
[25] Manish Sharma, Mayur Dhanaraj, Srivallabha Karnam,
Dimitris G. Chachlakis, Raymond Ptucha, Panos P.
Markopoulos, and Eli Saber. Yolors: Object detection
in multimodal remote sensing imagery. IEEE Journal of
Selected Topics in Applied Earth Observations and Remote
Sensing , 14:1497–1508, 2021. 1
[26] Lars Sommer and Arne Schumann. Deep learning-based
drone detection in infrared imagery with limited training
data. In Henri Bouma, Radhakrishna Prabhu, Robert James
Stokes, and Yitzhak Yitzhaky, editors, Counterterrorism,
Crime Fighting, Forensics, and Surveillance Technologies
IV, volume 11542, pages 1 – 12. International Society for
Optics and Photonics, SPIE, 2020. 2
[27] Zizhuang Song, Jiawei Yang, Dongfang Zhang, Shiqiang
Wang, and Zheng Li. Semi-supervised dim and small
infrared ship detection network based on haar wavelet. IEEE
Access , 9:29686–29695, 2021. 1
[28] Carole H. Sudre, Wenqi Li, Tom Vercauteren, Sebastien
Ourselin, and M. Jorge Cardoso. Generalised dice overlap
as a deep learning loss function for highly unbalanced
segmentations. In Deep Learning in Medical Image Analysis
and Multimodal Learning for Clinical Decision Support ,
pages 240–248. Springer International Publishing, 2017. 6
[29] Huaichao Wang, Haifeng Li, Hai Zhou, and Xinwei
Chen. Low-altitude infrared small target detection based on
fully convolutional regression network and graph matching.
Infrared Physics & Technology , 115:103738, 2021. 2
[30] Huan Wang, Luping Zhou, and Lei Wang. Miss detection
vs. false alarm: Adversarial learning for small object
segmentation in infrared images. In 2019 IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
8508–8517, 2019. 1, 3, 6, 8, 9, 10
[31] Kewei Wang, Shuaiyuan Du, Chengxin Liu, and Zhiguo Cao.
Interior attention-aware network for infrared small target
detection. IEEE Transactions on Geoscience and Remote
Sensing , 60:1–13, 2022. 1, 3, 8, 9, 10
[32] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pvtv2: Improved baselines with pyramid vision transformer.
Computational Visual Media , 8(3):1–10, 2022. 2, 3
[33] Xin Wang, Guofang Lv, and Lizhong Xu. Infrared dim
target detection based on visual attention. Infrared Physics
& Technology , 55(6):513–521, 2012. 1
[34] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and
Gao Huang. Vision transformer with deformable attention.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 4794–4803,
June 2022. 2, 3
[35] Hanyu Xiang, Qin Zou, Muhammad Ali Nawaz, Xianfeng
Huang, Fan Zhang, and Hongkai Yu. Deep learningfor image inpainting: A survey. Pattern Recognition ,
134:109046, 2023. 2
[36] Dongfang Yang, Xing Liu, Hao He, and Yongfei Li.
Air-to-ground multimodal object detection algorithm based
on feature association learning. International Journal
of Advanced Robotic Systems , 16(3):1729881419842995,
2019. 1
[37] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers
from scratch on imagenet. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
558–567, October 2021. 2, 3
[38] Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei,
Philip Torr, Wayne Zhang, and Dahua Lin. Vision
transformer with progressive sampling. Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , 2021. 2, 3
[39] Vitjan Zavrtanik, Matej Kristan, and Danijel Skocaj.
Reconstruction by inpainting for visual anomaly detection.
Pattern Recognition , 112:107706, 2021. 2
[40] Yu Zhang, Yan Zhang, Zhiguang Shi, Jinghua Zhang, and
Ming Wei. Design and training of deep cnn-based fast
detector in infrared suav surveillance system. IEEE Access ,
7:137365–137377, 2019. 2
[41] Zhaoxiang Zhang, Akira Iwasaki, Guodong Xu, and Jianing
Song. Cloud detection on small satellites based on
lightweight U-net and image compression. Journal of
Applied Remote Sensing , 13(2):1 – 13, 2019. 2
[42] Mingjing Zhao, Wei Li, Lu Li, Jin Hu, Pengge Ma, and Ran
Tao. Single-frame infrared small-target detection: A survey.
IEEE Geoscience and Remote Sensing Magazine , 10(2):87–
119, 2022. 1, 2