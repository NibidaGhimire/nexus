Image background assessment as a novel technique for insect
microhabitat identification
Sesa Singha Roy1,*, Reid Tingley2, and Alan Dorin1,*
1Dept. of Data Science and AI, Faculty of IT, Monash University, Clayton 3800, VIC Australia
2School of Biological Sciences, Monash University, Clayton 3800, VIC, Australia
2EnviroDNA, 293 Royal Parade, Parkville, VIC 3052, Australia
*Corresponding authors : sesa.singharoy@monash.edu, alan.dorin@monash.edu
Abstract
The effects of climate change, habitat fragmentation under increased urbanisation, industrial agriculture and
land clearing, are changing the way insects occupy habitat. Some species are highly adaptable and may utilise
anthropogenic microhabitat features for aspects of their existence, either because they prefer them to natural
features, or because they have no choice. Other species are tenuously dependent on natural microhabitats, hav-
ing to locate these within increasingly hostile landscapes. Consequently, humans are encountering insects in new
settings. Identifying and analysing these insects’ use of natural and anthropogenic microhabitats is important
to assess their responses to a changing environment, for improving pollination and managing invasive pests. But
such studies are costly and time-consuming.
Traditional studies of insect microhabitat use can now be supplemented by machine learning-based insect
image analysis. Typically, research has focused on automatic insect classification, but valuable data appearing
in image backgrounds has been ignored. In this research, we analysed the backgrounds of insect images available
on the Atlas of Living Australia database to determine the microhabitats in which they are commonly pho-
tographed. We analysed the microhabitats of three globally distributed insect species that are common across
Australia: Drone flies ( Eristalis tenax ), European honey bees ( Apis mellifera ), and European wasps ( Vespula
germanica ). Image backgrounds were classified as either natural or anthropogenic microhabitat using computer
vision and machine learning tools benchmarked against a manual classification algorithm.
We found Drone flies and European honey bees to be most commonly photographed in natural microhabitats,
confirming their need for natural havens within our cities. European wasps were less likely to be seen in these
areas and were commonly seen in anthropogenic microhabitats. This data supports the view that these insects
are well adapted to survive in the built environment, and that the management of this invasive pest requires
thoughtful reduction of their access to human-provided resources. The assessment of insect image backgrounds is
instructive to document the use of microhabitats by insects, especially those encountered in urban environments
where they are commonly photographed. The method offers insight that is increasingly vital for biodiversity
management as urbanisation continues to encroach on natural ecosystems and we must consciously provide re-
sources within built environments to maintain insect biodiversity and manage invasive pests.
Keywords : Microhabitat, insects, image analysis, computer vision, machine learning
1 Introduction
Insects are major contributors to terrestrial biodiversity and underpin diverse ecological and economic activities.
For example, they act as pollinators (Losey and Vaughan, 2006), pests (Aukema et al., 2011), disease vectors
(Lounibos, 2002), and they are a direct food source for many organisms (Hallmann et al., 2017). Unfortunately,
insects are highly vulnerable to the current rapid increase in urbanisation, something that is having major im-
pacts on global biodiversity (Bidau, 2018; Cardoso et al., 2020; Sánchez-Bayo and Wyckhuys, 2019). Urbanisation
has lead to modification of insect habitats as humans make space for industry, our residences and our agriculture
(Sánchez-Bayo and Wyckhuys, 2019). These changes have lead a to loss of habitat area, habitat fragmentation
and increases in habitat isolation for many species (Zanette et al., 2005; Zschokke et al., 2000). Research has
highlighted a decrease in arthropod abundance by 78%, a reduction in species richness by 34%in German grass-
1arXiv:2305.18207v1  [q-bio.PE]  26 May 2023lands between 2008−2017(Seibold et al., 2019), a drop of more than 75%in flying insect biomass over a period
of27years in German nature reserves (Hall et al., 2017), a 33%decrease in the number of bees and hover-fly
species over a span of 33years in Britain (Powney et al., 2019), and a 71%decrease in native butterflies in Britain
over 20years (Thomas et al., 2004). This has been labelled by some an "Insect Armageddon" (Leather, 2017).
And although there are those who question the extent of the issue (Saunders et al., 2019), there is little doubt that
a potential decline in insect numbers and biodiversity, even if localised, is of serious concern and worthy of attention.
Some insect species are comfortable in, or adapt to, urban habitats (Fig. 1). In some cases, insect diversity might
be high in urban refuges (Baranová et al., 2015; Heneberg et al., 2016). Quarries and waste-dump sites for exam-
ple, provide homes for flies and soil dwelling invertebrates like beetles, spiders and harvestmen (Heneberg et al.,
2016; Heneberg and Řezáč, 2014; Smedt et al., 2017). Urban areas may also provide refuge to rare and threatened
species such as native bees, butterflies, ground beetles and weevils (Jones and Leather, 2012). An increase in urban
green spaces, parks, gardens, green infrastructure, even vacant wasteland, can improve the quality of life for local
(human) residents and help control air pollution and the urban heat island effect (Ballinas and Barradas, 2016;
Carrus et al., 2015). But also, the moderation of climate these areas produce, and the relative abundance of food
and water, may make cities "pseudo-tropical bubbles" (Shochat et al., 2006) that increase the abundance of some
insects including ants, bees, caterpillars / butterflies and hoverflies (Dylewski et al., 2019; Uno et al., 2010). Such
insects have therefore entered human residential areas where they are often encountered (Azmy et al., 2016).
The microhabitats occupied by urban insects can vary based on their community structures and environmental
interactions, which in turn effects local abundances and distributions. Consequently, the micro-climate in some
urban spaces fosters increased abundance of some insect species but can be simultaneously detrimental to others
(McKinney and Lockwood, 1999). Buildings may alter locallight conditions and effect temperature and moisture in
their vicinity (Arnfield, 2003). Urban structures in general can fragment green spaces and hinder species dispersal,
resulting in a shift in species composition (Hans et al., 1999; Kozlov, 1996). For all of these reasons, understanding
the microhabitats occupied by insects can therefore be beneficial to assess insect behaviour, understand their
physiology, phenology, and, importantly, to assist us to manage insects’ potential responses to environmental change
(Pincebourde and Woods, 2020). Hence, in this article we investigate the microhabitats of insects using novel image
background analysis techniques applied to insect images posted online.
Figure 1: Insects in urban green spaces. (a) Honey bees are common garden pollinators. (b) Caterpillars can be
backyard pests that consume spinach leaves and other vegetables (but their butterflies can, by contrast, be valuable
pollinators). Images ©Copyright SSR 2022.
Insect microhabitat studies are traditionally carried out by ecologists who collect data manually from a selected
study site. For example, (Kolenda et al., 2020) executed a study in south-western Poland to identify the microhab-
itats of ants in wasteland. Researchers on pollinators like wild bees, butterflies and hover-flies, may use hand nets
2or pan traps to capture insects (Dylewski et al., 2019) in different microhabitats. This is costly, time-consuming,
and the expertise required presents practical obstacles (Büchs, 2003). The automation of insect survey methods
can potentially mitigate a shortage of human expertise. Image-based techniques for monitoring vertebrates, and
invertebrate insects, have gained popularity recently (Norouzzadeh et al., 2018; Steen, 2017; Steenweg et al., 2017;
Yousif et al., 2019). A variety of image types are used in these surveys including remote-sensing images (Hall et al.,
2016; Torresan et al., 2017; Zhang et al., 2019), a combination of camera trap and remote sensing images (Ayres and
Lombardero, 2018; Choi and Park, 2019) and even web-based images (ElQadi et al., 2017). Image-based techniques
may require manual validation by experts (Preti et al., 2021), but improvements in machine learning and computer
vision mean that many insect studies can be automated to a large extent (Amarathunga et al., 2021; ElQadi et al.,
2017; Joly et al., 2019; Wäldchen and Mäder, 2018). Research methods that use such image-based techniques,
typically, as might be expected, segment insect pixels from the remainder of the image to identify the species (Deng
et al., 2018; Ebrahimi et al., 2017; Liu et al., 2016; Maharlooei et al., 2017; Qing et al., 2012). Several deep-learning
based methods have been used in insect classification and monitoring of this type (Buschbacher et al., 2020; Cheng
et al., 2017; Li et al., 2020; Ren et al., 2019; Tetila et al., 2020). However, as we demonstrate in this article, the
image background against which the insect subject appears contains potentially valuable information. To the best
of our knowledge, no research has yet analysed the backgrounds of insect images. Here we conduct such an anal-
ysis investigating clues about insect microhabitat to reveal how common species engage with their environments.
Specifically, we wish to study how insects seek out natural features (like flowers, leaves, grass, sticks, soil, tree
bark or fruits) or anthropogenic microhabitat features (like rubbish bins (garbage cans), food scraps, brickwork,
paving stones, fence palings, fly-screens, food containers, window panes, walls, metal rods, spoons, but also human
hands, human hair, clothing etc.). Our interests in these two broad classes of microhabitat utilisation is of current
importance as climate change alters insect habitat availability (Halsch et al., 2021), and increasing urbanisation
either forces insects to leave degraded natural habitat, or offers them an opportunity to adapt to resource rich urban
environments. Studies of insect microhabitat utilisation therefore improve our knowledge of insect-human encoun-
ters. Theypotentiallyenableustoshapeourenvironmenttoimproveinsectbiodiversityandreducepestabundance.
The study aim is to investigate insect microhabitat use by both manual and automatic analysis of insect image
backgrounds. We report on experiments using an analysis pipeline we developed to determine the relative extent
to which three species are observed occupying natural or anthropogenic microhabitats in Australia.
Our study was conducted using images posted online in the Atlas of Living Australia ( www.ala.org ), a web-based
database of Australian biodiversity. We devised a human/manual algorithm to classify insect image backgrounds as
either natural or anthropogenic (Section 2.3). Then, we devised computer vision and machine learning algorithms
(Section2.4.2, 2.4.3)toperformthissameclassificationautomaticallyandbenchmarkitagainstourmanualmethod.
We outline our methodology in Section 2 and present our results in Section 3 along with a discussion (Section 4) of
strengths and weaknesses of this new research approach.
2 Methodology
We studied images of insects acquired from the Atlas of Living Australia (ALA) ( ala.org.au ), a database of
Australian biodiversity, to determine the kinds of microhabitat in which insects were observed and photographed.
The process is first outlined in point form and then described in detail in the subsections below and Fig. 2.
1. We extracted from ALA images of 3 insect species that inhabit a variety of Australian urban and natural
environments to create a dataset.
2. A pre-processing step was performed to automatically segment out the pixels containing insects from each
image in our dataset, leaving a derivative dataset with only the insects’ backgrounds remaining in the images.
3. We developed, documented and refined a manual, repeatable process for classifying the insect image back-
grounds as containing either natural or anthropogenic microhabitat.
4. We trained and tested convolutional neural networks to extract features from insect image backgrounds that
would enable their classification into natural and anthropogenic microhabitats.
5. We developed a Support Vector Machine (SVM) to automatically classify features of the insect image back-
grounds into classes corresponding to natural or anthropogenic microhabitats and benchmarked this classifi-
3cation against the manually determined results.
We tested the hypothesis that an analysis of insect image backgrounds would reveal variation in the extent to
which insect species occupy natural and anthropogenic microhabitats when they are observed. We explored this
hypothesisforEuropeanhoneybees( Apis mellifera ),Europeanwasps( Vespula germanica ),anddroneflies( Eristalis
tenax). These species were selected as they are frequently observed in Australian urban and natural environments,
and are known to have a variety of foraging preferences likely to result in diverse but observable microhabitat
utilisation. Intuitively we expected that insects that forage from floral resources would be documented utilising
natural microhabitats more often than scavengers and predatory insects, but we were unsure of the extent to which
this would be true among the studied species, and whether or not significant differences would appear in their
photographic documentation. We tested this hypothesis using two different methods:
Manual microhabitat identification : We developed and applied a manual, repeatable method or algorithm to
classify insect image backgrounds.
Automated microhabitat identification : We developed and applied a machine learning tool to classify insect
image backgrounds automatically. This was benchmarked against the manual method.
In the following sections we outline our methods in detail.
Figure 2: Overview of the methodology: (a) Insect images are collected from ALA; (b) Insect pixels are removed
from the images to generate a dataset of images containing only insect backgrounds; (c) A Manual classification
algorithm is used to classify the insect image backgrounds; (d) Some manually classified images are taken to create
Training and validation datasets for the new automatic classifier model; Then, the automatic classifier model is
trained for background classification, tested and validated; (e) The trained automatic model isused to classify the
background-onlyimagesintotwoclasses-naturalandanthropogenicmicrohabitats. Thesearebenchmarkedagainst
the results obtained using Manual classification.(Images ©Copyright AD, RT 2022).
2.1 Insect image retrieval
We used ALA as our source of ecological entomological data since the data are often collected by experts and have
some chance of being evaluated for quality ( https://www.ala.org.au/data-quality-project/ ). ALA contains
4occurrence records with text and often image data. The platform offers an Application Programming Interface
(API) that makes the process of searching and downloading data for specific species and within specific geographic
areas straightforward. We used this API to extract taxon name, species class, land-use type, species id, observation
latitude and longitude, identification verification status and image URLs for each target insect species. Image URLs
for each record were stored in a file with the other data and later downloaded using a Python script within the
following constraints.
We constrained the software to download only images within the geographic region from −10.41to−43.38degrees
latitude and 113.78to153.45degrees longitude which comprises of the whole Australian mainland. Any records
missing latitudinal or longitudinal data were ignored. We have also restricted the software to download images
which have their identification verification status as ’research’ and ’confident’ to ensure the images that we used
are taxonomically confirmed and identified by experts. Records with other status or no status were discarded. Our
ecologist co-author visually "sanity checked" images to further ensure they were of the target species.
We downloaded image data for three insect species from ALA, honey bees ( Apis mellifera ), drone flies ( Eristalis
tenax) and European wasps ( Vespula germanica ). Honey bees were chosen as they are a recognisable, abundant,
and important invasive pollinator insect in Australia (Smith and Saunders, 2016). They are also relatively slow
moving insects that visit flowers, two traits that make them a popular subject for amateur photographers. Drone
flies also act as pollinators (Howlett and Gee, 2019) and, like bees, are commonly found in Australian urban gardens
and natural landscapes on and around flowers and natural vegetation. European wasps are an invasive introduced
species in Australia that is particularly abundant in eastern and southern regions (Spradbery et al., 1992). They are
a declared and prominent pest in Australian urban areas effecting people and animals (Cook, 2019). These wasps
are, intuitively speaking, often spotted near general waste, food and food waste as they are attracted by sugars
and meat. Due to their salient yellow and black patterning and their proximity to humans in urban areas, wasps
have also become a subject of photography (and are sometimes mistaken for bees). European wasp’s differences
in behaviour from bees and flies makes them a potentially interesting contrast to pollinators. They provide an
important opportunity for this study to test the clarity with which insect microhabitat occupation is evident in the
image records of common species.
Figure 3: Example insect segmentation using YOLACT. (a) Raw image. (b) The insect is segmented out from the
image to obtain the background. This provides evidence of the insect’s microhabitat at the time of observation
in a form that minimises the chance that a classifier might learn to identify the insect species and then infer the
microhabitat by association. (Removed pixels have been highlighted in red for illustrative purposes.) Image ©
Copyright RT 2022.
2.2 Image insect subject removal
In this pre-processing step we segment out insect pixels from images to isolate image background pixels from which
to classify observations of insect microhabitats. Failure to conduct this step may result in the classifier biasing its
5background classification based on its “knowledge” of the insect species contained within the image. Hence it plays
an important role in ensuring any conclusions are meaningful. For the insect removal step we used the real-time
instance segmentation algorithm, YOLACT ( YouOnlyLookAtCoefficienT’s) (Bolya et al., 2019), pre-trained
with the MS COCO dataset (Lin et al., 2014). This is a large-scale object-detection and segmentation dataset of
object classes including humans, animals, planes, ships, etc., but not insects – hence, YOLACT in its basic form
was unable to directly segment insects from our dataset. We therefore fine-tuned the pre-trained model with 200
labelled ALA images of European honey bees and 200 images of European wasps. Fifty images of these classes were
taken from the remaining dataset for fine-tuning the classifier. None of the images used in training were reused for
experimental processes. The training and validation was performed on an NVIDIA P100 GPU. Due to the visual
similarity between all insects in our study, we were able to successfully segment out their bodies using this trained
model (e.g. Fig. 3).
Anthropogenicimage setNoYesDiscarded image set
NoYes
Naturalimage setNoNoNoNoYesNoYesNoYes
NoYesYesNo
YesNo
YesYesYesYesNoYesYesYesYesYes
NoNoNoNoContains regular grid?Background uninterpretably unfocussed?Containstext?Contains repeating pattern?Contains human body parts?Contains vertex, edge, uniform surface or artiﬁcial texture?Contains image watermark or copyright overlay?Image set to classiﬁy
Background entirely uniform colour?Containswater ripples, honeycomb or wasp’s nest?Is the colour sky blue?Is the colour palette green, brown, sky blue or sunset hues?
Contains ﬂowers?Contains leaves?Contains pebble, rock, soil or sand?Contains natural wood, wood grain or bark?Contains water?NoContains clouds?YesContains uniform black background?
No
Figure 4: Manual image background classification algorithm based on human observation to create training and
validation datasets to train our software classification algorithm.
2.3 A manual algorithm to classify insect image backgrounds
We created an algorithm based on visual (human) observations of ALA image backgrounds (Fig. 4). We manually
selected a subset of 500 images of Apis mellifera (honey bees) with a variety of backgrounds to refine a human
classification procedure. This process was necessary for us to articulate our understanding of what constitutes
anthropogenic and natural microhabitats - a distinction without which training any software classifier would be at
best problematic, and at worst nonsensical. Once this process was formally documented (Fig. 4) it was methodically
followed to classify all images of the three insect species, drone flies, honey bees and European wasps.
62.4 A software algorithm to classify insect image backgrounds
In this step we develop a machine learning classifier to distinguish the two classes of image background that broadly
describe insect microhabitat as natural or anthropogenic (Fig. 5).
Figure 5: The software classifier showing the structure of ResNet-50 generating a feature vector of size 1000. This
is used to train the SVM classifier to classify the insect image backgrounds into two classes. The notation ‘ axa,
b’ in the convolution blocks 1-5 represent a filter of size a and b channels. The output of each convolution block is
represented by ‘Size x’. The repetition of each square shape in each convolution block represents the repetition of
each unit.
2.4.1 Creating a labelled dataset of image backgrounds for training
To create image sets with which to train and test a software microhabitat classifier, we used the manual algorithm
discussed in Section 2.3 and classified 500 images of European honeybee and European wasp backgrounds into
equal sets of 250 natural and 250 anthropogenic microhabitats. These, manually but methodically labelled, images
constituted the training data for the software classifier described in Section 2.4.2. A separate set of 100 Apis
mellifera images labelled using the same methodical process was set aside to test the trained software classifier.
2.4.2 Feature extraction of image background
The next step was to gather standard visual information and low-level image features from the datasets that include
traits such as colour, texture, shape, etc. (Shih et al., 2001). These features can be used by classifiers or deep
learning models for image segmentation, image classification and object detection. There are various techniques
for conducting image feature extraction. With advancements in machine learning, specifically deep learning and
convolution neural networks (CNNs), feature extraction from images can be performed automatically to obtain
high levels of classification accuracy (Acharya and Khoshelham, 2020). Therefore, we used a deep-learning model
to extract features of our image backgrounds for training a classifier model. We adopted a ResNet-50 model (He
et al., 2016) for extracting image background features for classification. ResNets (Residual Networks) are easy to
train with reduced complexity, even though they have deeper layers than CNN models, because of the presence of
skip connections between the input and output of each block (He et al., 2016). ResNet has different variants with
a variety of convolution layers. We trialled three depths, ResNet-18, -50 and -101, and compared their training
times and validation accuracies. They behaved similarly but ResNet-50 had a marginally higher validation accuracy
(Section 3.1.1)leading us to choose it for feature extraction.
7ResNet-50 is made up of five convolutional blocks stacked on top of one another (Fig. 5). The image features
are extracted from the fully connected layers pre-trained with the ImageNet database ( https://image-net.org/
index.php ). The input to the network is an image of 224x224 pixels. Features extracted from the deeper layers
class-specific properties such as shape, texture and colour, and hence provide a better classification performance
than features extracted from the shallower layers (Zeiler and Fergus, 2014). Therefore, we extracted features from
the last layer, a fully connected layer, that outputs a 1000-dimensional feature vector.
2.4.3 Training and validating the classifier model
Deep learning is increasingly used for image classification (Jena et al., 2021; Li et al., 2021; Tiwari et al., 2021).
The literature reports research where purpose-built datasets are used to train species-specific image-classifiers for
identifying insects, such as crop pests, in images (Ullah et al., 2022). However, training such models for specific
purposes requires relatively large datasets, it is time-consuming and computationally expensive. To overcome these
shortcomings, we used Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) as our classifier model. SVMs
aremachinelearningmodelsthattransformnon-linearseparableproblemstolinearlyseparableproblems. Theyhave
a high generalising ability compared to other classifier models (Cervantes et al., 2020) and are capable of delivering
high classification accuracy (Durgesh and Lekha, 2010) with small datasets and little time and computational
expense. We used the extracted background features generated in our previous step to train an SVM model with 5-
fold cross validation. The training and validation of our classifier was run on a basic laptop using its CPU (Intel(R)
Core(TM) i7-9850H, CPU clock speed=2.60 GHz).
3 Results
Section 3.1 presents the results of our manual classification algorithm. Sections 3.1.1 and 3.1.2 provide training and
validation results for our automatic classifier. Section 3.1.3 provides the results of our insect microhabitat studies.
3.1 The discovery of exceptions and special cases during the manual classification of
image backgrounds
While manually classifying image backgrounds we encountered some exceptions and edge cases that lead to difficul-
ties in labelling microhabitat types (Fig. 6). One edge case related to the presence of text in an image (Fig. 6d). If
text sits within the non-insect pixels of an image background, such as on a handwritten paper note, or printed words
on a page, packaging or signage, we would consider the insect to have been observed in an anthropogenic scenario.
However, watermarks and copyright claims were also digitally superimposed on some images. Such artefacts of
the image-making process (obviously) shouldn’t be considered components of a depicted insect’s microhabitat. To
simplify the process of correctly determining microhabitat, we subsequently excluded images containing overlaid
text from our study. Regular grids or other repeated patterns present in image backgrounds are usually classified as
anthropogenic according to our manual algorithm. However, exceptions to this rule were identified. For instance, we
discovered water ripples forming repeated patterns and classified these as natural (Fig. 6b). Similarly, insect nests,
such as honey bee honeycomb structures or some wasps nests, may be grid-like or exhibit other repeated patterns.
These may be either in anthropogenic or natural settings depending on the location (e.g. within a box-beehive,
attached to a tree branch, or under a building’s eaves Fig. 6c). As our dataset has only a few insect nest images, we
ignored the sub-classification of nests by location and classified them all as natural, even though typically, regular
grid-like patterns are indeed human artefacts. We also discarded all images with pure black backgrounds (Fig. 6d)
as the lack of features made it impossible to classify them.
8(a)
 (b)
(c)
 (d)
Figure 6: The kinds of exceptions encountered during manual microhabitat classification. (a) Watermarked im-
ages were discarded; (b) Regular water-ripples were classified as natural; (c) Insect nest structures were classified as
natural even if the nest was attached to artificial substrate; (d) Images with black or removed backgrounds were dis-
carded. Image (b) https://images.ala.org.au/image/f2e756f0-1e7e-4436-855c-e23d8b00b643 ©deborod
2020; licensed under CC-BY-NC 4.0; (a),(c),(d) ©AD 2022.
3.1.1 Results of feature extraction from image backgrounds
We tested the performance of pre-trained ResNet-18/50/101 models by training each with the same 500 honey
bee images. This dataset contained 250 images of natural backgrounds and 250 anthropogenic backgrounds as
determined by the manual algorithm. The training times for all three models were similar, with ResNet-50 being
marginally faster. A pre-trained ResNet-50 CNN model was therefore selected for our study, and the last layer
before its classification layer was used to extract 1000 features for training our classifier model. These features are
visualised in Fig. 7 using a t-SNE algorithm (Maaten et al., 2008). This is a non-linear dimensionality reduction
method for visualising high-dimensional data by providing each data-point a location in 2-D or 3-D space. In this
case it shows that our two classes, natural and anthropogenic microhabitats, are well structured.
ResNet-18 ResNet-50 ResNet-101
Training time 1.25 s 1.2 s 1.22 s
Table 1: Comparison of training time for pre-trained ResNet-18/50/101 models for extracting features of back-
grounds of 500 honey bee images.
9Figure7: Visualisationofextractedfeaturesusingapre-trainedResNet-50CNNmodelshowingthatthebackground
microhabitat classes are well structured. Green triangles represent ‘natural’ microhabitats and the light brown
circlesrepresent ‘anthropogenic’ mirohabitats.
3.1.2 Results of classifier model training and validation
After the SVM classifier was trained with image features (Section 2.4.2) extracted from 500 honey bee images
(training time = 30 secs, on laptop CPU), 88 random images were selected from the remaining honey bee images
to test the model’s classification performance (test time = 0.25 seconds on laptop CPU). We performed 5-fold cross
validation for our SVM model. We compared the validation inaccuracies of the SVM classifier model using the
features extracted from ResNet-18/50/101 models (Table 2). We found that features extracted from ResNet-50
generated marginally better validation accuracy than ResNet-18 or -101 and obtained a validation accuracy for
training the SVM of 96.4%and a test accuracy of 97.4%. This strengthens our motivation for using ResNet-50 as
our image feature extractor. Some classification results obtained from the trained SVM model did not match their
manual classifications, i.e. the method’s accuracy was not 100%. (Section 3.1.3)
ResNet-18 ResNet-50 ResNet-101
Validation accuracy 95% 96.4% 94%
Table 2: Comparison of the validation accuracy of the SVM classifier using the features of backgrounds of 500 honey
bee images extracted using ResNet-18/50/101 models
3.1.3 Results of comparison between manual and automated microhabitat identification methods
Following our download of ALA’s images of honey bees (7654 images), European wasps (1026 images) and drone
flies (706 images) within the extent of mainland Australia and manual removal of irrelevant images, over 9000
images of the target species remained (Table 3). The classification of these images’ backgrounds using both manual
and automated methods into natural and anthropogenic microhabitats is provided (Fig. 8).
Common name Search term (scientific name) Number of images returned
honey bee Apis mellifera 7669
european wasp Vespula germanica 1026
drone fly Eristalis tenax 706
Table 3: ALA images collected by searching for species’ scientific names on 21stNov 2022 and ascertained by
superficial visual inspection to be images of the target species.
10Figure 8: Plot showing the percentage of ALA target insect species images with ‘natural’ backgrounds calculated
using the manual classification method (solid green bars) and the automated classifier measurement of the same
value (red dashed lines). Error bars represent 95%confidence intervals calculated using the Wilson score interval
test (Wilson, 1927). Manual/automated method results: Drone flies = 95±3%/94%Honey bees = 89±2%/
87%, European wasps = 70±6%/63%.
The results from the automated method’s classification of natural (versus anthropogenic) backgrounds is slightly,
but consistently, lower than that produced by the manual algorithm across all species studied. This is discussed in
Section 4. Irrespective of the classification algorithm, the drone flies in our data were photographed almost entirely
in natural microhabitat, honey bees only slightly less so. By contrast, European wasps were photographed much
more prominently in anthropogenic microhabitats, even though the majority of their images did still show natural
microhabitats.
In order to determine if these three species of insects are part of significantly different distribution and find the
confidence interval of the population proportion ‘p’ for each distribution, we have performed a Wilson score interval
test (Wilson, 1927). This test provides 95%confidence values for the natural image background prediction using
our manual method.Results show 95±3%drone flies have natural backgrounds, honey bees and European wasps
have 89±2%and 70±6%natural backgrounds respectively when classified manually.
11Figure 9: Types of background mis-match between the automated software and the manual algorithm for each
insect species. Blue shades represent natural backgrounds misclassified as anthropogenic by the automated model.
Red shades represent anthropogenic backgrounds misclassified as natural by the automated model. ‘F’, ‘B’ and
‘W’ indicate classification discrepancies for drone Flies, honey Bees and European Wasps respectively. Detailed
descriptions of classification mismatch types are given in Section 3.1.3 and examples appear in Fig. 10.
Fig. 9 shows the results of comparing the software image background classifications to the manual method. For
10of the 12listed background types discrepencies were less than 0.25%. In the case of the wasps, natural ground
and wood backgrounds were misclassified as anthropogenic: 2%ground, 4.5%for wood (see Section 4). Twelve
mismatch types were identified in classifying the image backgrounds:
•Ceramic : a ceramic or glazed surface that is anthropogenic (Fig. 10a).
•Fabric : cloth or fabric that is anthropogenic (Fig. 10b).
•Fence : a garden or park fence, often made of metal, that is anthropogenic (Fig. 10c).
•Flower part : a close-up image of a small flower part, such as a petal or flower anthers, that is natural (Fig.
10d).
•Hive : an insect hive or nest, often with a repeated or grid-like pattern that is natural (Fig. 10e).
•Hybrid : an image background containing both natural and anthropogenic components, such as a background
showing a flower set against a wall (Fig. 10f). In such cases the human considered the location where the insect
was situated to be the best indicator of microhabitat use, but the software sometimes classifies in the opposite
way to the human, taking a broader view of the image background contents in making its classification (since
this was how it was trained).
•Leaf part : a close-up image of a leaf or parts of a leaf that is natural (Fig. 10g).
•Plastic : a plastic surface such as a serving plate, drink cup or plastic bag that is anthropogenic (Fig. 10h).
•Sky : the blue sky is natural (Fig. 10i).
•Ground : soil, mud, sand, gravel, etc. are natural (Fig. 10j).
•Water : water in a natural setting, such as a pond (Fig. 10k).
•Wood : tree trunks, split or natural logs, wooden tables and benches, etc. were often natural but classified
by the software as anthropogenic (Fig. 10l). In a couple of cases, the background was identified manually as
an anthropogenic picnic table or benchtop but the model classified it as natural.
12(a)
 (b)
 (c)
 (d)
 (e)
 (f)
(g)
 (h)
 (i)
 (j)
 (k)
 (l)
Figure 10: Examples of image background types occasionally mis-classified by the automated software: (a) ceramic;
(b) fabric; (c) fence; (d) flower part; (e) hive; (f) hybrid; (g) leaf part; (h) plastic; (i) sky; (j) ground; (k) water;
(l) wood. Images (a-g, j-k) used under license BY-NC 4.0, (h) under license BY-SA 4.0, (i,l) under license BY-
NC-SA 4.0. Image source (a) https://images.ala.org.au/image/67e833c5-e949-481f-9d39-90ec2d30d27b ©
Menura 2021; (b) https://images.ala.org.au/image/169abe6a-c292-472c-9228-020e3d73bb09 ©Kymelen
2021; (c) https://images.ala.org.au/image/b014ce62-0953-492e-a05f-4e29cd1bc09f ©Paula Rivera 2021;
(d)https://images.ala.org.au/image/689bf48f-3dbc-472c-9e0d-967fab1aa97c ©Wild Days Wildlife Shel-
ter 2021; (e) https://images.ala.org.au/image/4789cdba-dff3-4ffb-9bc9-0804560e5b2c ©Russell Cum-
ming 2018; (f) https://images.ala.org.au/image/5073019c-3e46-4daa-98e9-a9e7f7aae287 ©QuestaGame
2018; (g) https://images.ala.org.au/image/9c648e56-2de8-44db-ae91-5909ee779034 ©Rosemary Kidd,
2020; (h) https://images.ala.org.au/image/6b7525de-f44c-44c6-986e-5001713b4307 ©James K. Douch
2022; (i) https://images.ala.org.au/image/52cf6df3-609f-4b88-8efe-ee3583162798 ©Reiner Richter
2019; (j) https://images.ala.org.au/image/99c73250-7fd6-4628-914b-1f1ffdd19f49 ©ReinerRichter2017;
(k)https://images.ala.org.au/image/48736075-fd49-4b02-9deb-afc5072b4d19 ©Tim 2020; (l) https:
//images.ala.org.au/image/39ed3fda-c300-49c9-a7e7-37f373ab63bd ©Geofflot 2017.
4 Discussion
Our results demonstrate how image backgrounds can be analysed manually and automatically to generate important
data on the kinds of microhabitats in which humans encounter insects. This is a novel approach to insect microhab-
itat studies derived from biodiversity data that might otherwise go unanalysed. Images online provide a potentially
huge source of information on species occurrence (ElQadi et al., 2017), but as shown here, we can go further than
simply using these images as evidence of an individual insect’s presence. These images allow us to learn how species
occupy the environment when they are encountered by humans. Although we restricted this study to ALA images
for quality control reasons, previous work has demonstrated that machine learning and computer vision tools can
correctly identify some species from social media posts (Burke et al., 2022; ElQadi et al., 2017). Consequently, in
the future, our method could be applied to social media image data as “Incidental Citizen Science” to deduce insect
microhabitat, especially in urban areas where social media posts are most abundant. Such information can assist
in the control of invasive species like the European wasp (and the honey bee for that matter). But it also allows
us to obtain data on insect utilisation of urban habitats that might inform biodiversity management of pollinating
species like honey bees (these aren’t often viewed through the lens of invasive species, even in Australia) and drone
flies.
Analysis of the microhabitats of the study species shows the large extent to which drone flies and honey bees
are encountered by humans against natural backgrounds (Fig. 10). We found this to be true irrespective of the
classification method (flies 95%and bees 89%using manual classification, 94%for flies and 87%for bees using the
automated classifier). These insects are important pollinators. Their tendency to loiter on, in and around natural
microhabitat features such as plants, trees and flowers is strongly supported by the data we acquired through image
13analysis. This highlights also the need for the explicit provision of natural microhabitats within our urban built
environments to cater for these insects (Beaujour and Cézilly, 2022; Shrestha et al., 2021).
On the other hand, European wasps were much less often encountered against natural backgrounds than the pollina-
tors. Manual classification of wasp backgrounds revealed only a 70%encounter rate against natural microhabitats
with a 63%rate determined by automated classification. This confirms the wasps’ long-recognised pest status
in Australia (Crosland, 1991), with a diet that has come to include not only “natural” resources, but also human
food and waste, especially carbohydrates (Goodall and Smith, 2001). This diet ensures humans encounter European
wasps even in highly anthropogenic areas, and supports the reduction of access to human food and waste as a viable
way to reduce encounters with these stinging and aggressive insects (de Villiers et al., 2017). Future work would
be valuable to unravel the extent to which wasps utilise natural and anthropogenic microhabitats differentially in
natural, rural and urban regions.
The deep learning approach provided quick, reliable, automatic classification of image backgrounds. ResNet-50
classified with a very high test accuracy of 97.4%(Section 3.1.2) using only the CPU of a basic laptop computer.
Our trained ResNet-50 + SVM classifier took approx. 90sto classify 500image backgrounds on this modest hard-
ware, evidence of the relatively low computational requirements of the method.
It is interesting to consider the discrepancies between the results of our manual and automated methods for back-
groundclassification(Fig. 9). Asnoted, groundandwoodwerethebackgroundswhereclassifierimprovementwould
be most fruitful. Some ground cover, such as grit and stones embedded in finer soil, can be recognised by humans
as natural but further classifier training would improve the software’s ability to distinguish this from anthropogenic
conglomerate of concrete. Textured bark and naturally or roughly split wood backgrounds were sometimes classified
as artificial by the software - their similarity to fence palings and weathered outdoor furniture is obvious. But even
in these cases the deviation of the automated method from the manual classification was very low, did not impact
on the results of our study statistically, and therefore did not warrant further work for this project.
The difference in the discrepancy of background classification of ground and wood between manual and automated
methods across the three target species (flies seemingly showed the lowest discrepancy, followed by bees and wasps),
reflectsthefrequencywithwhichtheseinsectsappearedagainsteachbackground–aninsectthatisn’tphotographed
on the ground for instance, would not raise a histogram bar in this category.
In Section 3.1, we discussed how we discarded watermarked images from the ALA data before processing. If water-
marked images provided important data, automated software could be applied to remove the water marks (Yang
et al., 2021). A further expansion of our method relates to non-insect studies. In the case of insects, most useful
photographs are close-ups. But our method could potentially operate on larger animals as long as “hybrid” back-
grounds were carefully handled; the larger the animal in the image, the more likely diverse elements will appear in
the camera frame. It would then be essential to subdivide image backgrounds into regions to determine which ele-
ment was most relevant to an animal’s interaction with the location as specified in a particular research project. For
instance, if an image showed a kangaroo eating grass (natural) whilst standing in front of an oncoming motor vehicle
(anthropogenic), perhaps the food source is most relevant to the animal’s use of the locality. But perhaps this would
not be true if the study was unravelling why Australian native animals end up as road-kill. Along similar lines, in
our study all images of insect hives were classified as natural. Some images may have shown artificial hives though
- it can be impossible to tell from some images. In our dataset insect hives were few ( 0.09%of the total images),
and the significant amount of effort to elucidate their location was deemed insufficiently valuable to explore in the
current study. This may well be a fruitful avenue for future work exploring insect nest and hive locations specifically.
Microhabitats provide insects the resources to occupy an area that might, at a broader scale, seem inhospitable
to them. The availability and explicit provision of suitable microhabitats for desirable insects within our built
environments is increasingly important under a changing global environment and expanding urbanisation and
industrial agriculture. And the reverse is also true. We would be wise to reduce access of pest insects to resources
they depend on in our built environments to reduce the potential for humans to encounter dangerous or aggressive
pests. Insect microhabitat can, and should, continue to be studied in traditional ways using on-the-ground field
14ecologists and manual data collection. Only in this way will a deep understanding of insect interactions with
their environment be obtained. However, these painstaking, labour intensive, costly but essential projects can be
supplemented by the image analysis procedures detailed here. In fact, traditional ecological projects could support
our analysis approach by explicitly capturing relevant insect images for analysis.
5 Conclusion
The backgrounds of insect images provide important information from which to study the microhabitat use of
individual insect species. We have developed new algorithmic procedures and an image analysis pipeline to extract
insect pixels from an image, and successfully shown how manual approaches and automated machine learning can
be used to classify microhabitat appearing in image backgrounds into broad natural and anthropogenic classes. Our
findings suggest that a deep-learning based automated classifier model can quickly and accurately classify insect
image backgrounds as benchmarked against manual classifications.
We found pollinating drone flies and honey bees had clear signatures in the image record of their use of natural
microhabitats. This supports the findings of traditional studies that these insects require the provision of natural
resources within built environments for their continued well-being and survival. This need set the drone flies and
honey bees apart clearly from European wasps that are scavengers known to utilise human-provided resources in
anthropogenic microhabitat. The wasps were, by contrast, often documented in the image data against anthro-
pogenic backdrops. This finding supports the idea that depriving European wasps of access to human food, waste,
and other anthropogenic features they depend on, would be one way to reduce encounters with these insects in built
environments.
6 Acknowledgement
SSR was supported by the Faculty of Information Technology International Postgraduate Research Scholarship,
Monash University, Australia.
7 Author contributions
Allauthorsdevisedtheidea. SSRimplementedandconductedthemethodology, andadministeredtheanalysis. SSR
wrote the original draft of the manuscript. SSR and AD prepared the figures. All authors edited and contributed
critically to the drafts and provided final approval for publication. AD and RT provided supervision and acquired
the funding for this project.
8 Declaration of competing interest
The authors have no conflict of interest to declare.
References
D. Acharya and K. Khoshelham. Real-time image-based parking occupancy detection and automatic parking slot deliniation using deep
learning: A tutorial. 2020. Available online. Accessed on 11 December 2022.
D. C. Amarathunga et al. Methods of insect image capture and classification: A systematic literature review. Smart Agricultural
Technology , 1:100023, 2021. ISSN 2772-3755. doi: https://doi.org/10.1016/j.atech.2021.100023. URL https://www.sciencedirect.
com/science/article/pii/S277237552100023X .
A. John Arnfield. Two decades of urban climate research: a review of turbulence, exchanges of energy and water, and the urban
heat island. International Journal of Climatology , 23(1):1–26, 2003. doi: https://doi.org/10.1002/joc.859. URL https://rmets.
onlinelibrary.wiley.com/doi/abs/10.1002/joc.859 .
Juliann E. Aukema, Brian Leung, Kent Kovacs, Corey Chivers, Kerry O. Britton, Jeffrey Englin, Susan J. Frankel, Robert G. Haight,
Thomas P. Holmes, Andrew M. Liebhold, Deborah G. McCullough, and Betsy Von Holle. Economic impacts of non-native forest
insects in the continental united states. PLOS ONE , 6(9):1–7, 09 2011. doi: 10.1371/journal.pone.0024587. URL https://doi.org/
10.1371/journal.pone.0024587 .
M. P. Ayres and M. J. Lombardero. Forest pests and their management in the anthropocene. Canadian Journal of Forest Research , 48
(3):292–301, 2018. doi: 10.1139/cjfr-2017-0033. URL https://doi.org/10.1139/cjfr-2017-0033 .
15M. M. Azmy, T. Hosaka, and S. Numata. Responses of four hornet species to levels of urban greenness in nagoya city, japan: Implications
for ecosystem disservices of urban green spaces. Urban Forestry &Urban Greening , 18:117–125, 2016. ISSN 1618-8667. doi:
https://doi.org/10.1016/j.ufug.2016.05.014. URL https://www.sciencedirect.com/science/article/pii/S1618866716300164 .
M. Ballinas and V. L. Barradas. The urban tree as a tool to mitigate the urban heat island in mexico city: A simple phenomenological
model.Journal of Environmental Quality , 45(1):157–166, 2016. doi: https://doi.org/10.2134/jeq2015.01.0056. URL https://acsess.
onlinelibrary.wiley.com/doi/abs/10.2134/jeq2015.01.0056 .
B. Baranová, P. Manko, and T. Jászay. Waste dumps as local biodiversity hotspots for soil macrofauna and ground beetles (coleoptera:
Carabidae) in the agricultural landscape. Ecological Engineering , 81:1–13, 2015. ISSN 0925-8574. doi: https://doi.org/10.1016/j.
ecoleng.2015.04.023. URL https://www.sciencedirect.com/science/article/pii/S0925857415001330 .
PierreMichardBeaujourandFrankCézilly. TheImportanceofUrbanGreenSpacesforPollinatingInsects: TheCaseoftheMetropolitan
Area of Port-au-Prince, Haiti. Caribbean Journal of Science , 52(2):238 – 249, 2022. doi: 10.18475/cjos.v52i2.a11. URL https:
//doi.org/10.18475/cjos.v52i2.a11 .
C. J. Bidau. Doomsday for insects? the alarming decline of insect populations around the world. Entomol Ornithol Herpetol , 7(3):1–5,
2018. doi: 10.4172/2161-0983.1000e130.
Daniel Bolya et al. Yolact: Real-time instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV) , pages 9157–9166, October 2019.
H. M. Burke et al. Tag frequency difference: Rapid estimation of image set relevance for species occurrence data using general-purpose
image classifiers. Ecological Informatics , 69:101598, 2022. ISSN 1574-9541. doi: https://doi.org/10.1016/j.ecoinf.2022.101598. URL
https://www.sciencedirect.com/science/article/pii/S1574954122000474 .
K. Buschbacher et al. Image-based species identification of wild bees using convolutional neural networks. Ecological Informatics , 55:
101017, 2020. ISSN 1574-9541. doi: https://doi.org/10.1016/j.ecoinf.2019.101017. URL https://www.sciencedirect.com/science/
article/pii/S1574954119303280 .
W. Büchs. Biodiversity and agri-environmental indicators—general scopes and skills with special reference to the habitat level. Agri-
culture, Ecosystems &Environment , 98(1):35–78, 2003. ISSN 0167-8809. doi: https://doi.org/10.1016/S0167-8809(03)00070-7.
URL https://www.sciencedirect.com/science/article/pii/S0167880903000707 . Biotic Indicators for Biodiversity and Sustain-
able Agriculture.
Pedro Cardoso, Philip S. Barton, Klaus Birkhofer, Filipe Chichorro, Charl Deacon, Thomas Fartmann, Caroline S. Fukushima,
René Gaigher, Jan C. Habel, Caspar A. Hallmann, Matthew J. Hill, Axel Hochkirch, Mackenzie L. Kwak, Stefano Mammola,
Jorge Ari Noriega, Alexander B. Orfinger, Fernando Pedraza, James S. Pryke, Fabio O. Roque, Josef Settele, John P. Simaika,
Nigel E. Stork, Frank Suhling, Carlien Vorster, and Michael J. Samways. Scientists’ warning to humanity on insect extinc-
tions.Biological Conservation , 242:108426, 2020. ISSN 0006-3207. doi: https://doi.org/10.1016/j.biocon.2020.108426. URL
https://www.sciencedirect.com/science/article/pii/S0006320719317823 .
Giuseppe Carrus, Massimiliano Scopelliti, Raffaele Lafortezza, Giuseppe Colangelo, Francesco Ferrini, Fabio Salbitano, Mariagrazia
Agrimi, Luigi Portoghesi, Paolo Semenzato, and Giovanni Sanesi. Go greener, feel better? the positive effects of biodiversity on
the well-being of individuals visiting urban and peri-urban green areas. Landscape and Urban Planning , 134:221–228, 2015. ISSN
0169-2046. doi: https://doi.org/10.1016/j.landurbplan.2014.10.022. URL https://www.sciencedirect.com/science/article/pii/
S0169204614002552 .
Jair Cervantes et al. A comprehensive survey on support vector machine classification: Applications, challenges and trends. Neurocom-
puting, 408:189–215, 2020. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2019.10.118. URL https://www.sciencedirect.
com/science/article/pii/S0925231220307153 .
X. Cheng et al. Pest identification via deep residual learning in complex background. Computers and Electronics in Agriculture ,
141:351–356, 2017. ISSN 0168-1699. doi: https://doi.org/10.1016/j.compag.2017.08.005. URL https://www.sciencedirect.com/
science/article/pii/S0168169917304854 .
W. I. Choi and Y. S. Park. Monitoring, assessment and management of forest insect pests and diseases. Forests, 10(10), 2019. ISSN
1999-4907. doi: 10.3390/f10100865. URL https://www.mdpi.com/1999-4907/10/10/865 .
David C Cook. Quantifying the potential impact of the european wasp (vespula germanica) on ecosystem services in western australia.
NeoBiota , 50:55–74, 2019. doi: https://doi.org/10.3897/neobiota.50.37573.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning , 20(3):273–297, 1995. ISSN 1573-0565. doi: 10.1007/
BF00994018. URL https://doi.org/10.1007/BF00994018 .
M. W. J. Crosland. The spread of the social wasp, vespula germanica, in australia. New Zealand Journal of Zoology , 18(4):375–387,
1991. doi: 10.1080/03014223.1991.10422843. URL https://doi.org/10.1080/03014223.1991.10422843 .
Marelize de Villiers, Darren J. Kriticos, and Ruan Veldtman. Including irrigation in niche modelling of the invasive wasp vespula
germanica (fabricius) improves model fit to predict potential for further spread. PLOS ONE , 12(7):1–17, 07 2017. doi: 10.1371/
journal.pone.0181397. URL https://doi.org/10.1371/journal.pone.0181397 .
L. Deng et al. Research on insect pest image detection and recognition based on bio-inspired methods. Biosystems Engineering , 169:
139–148, 2018. ISSN 1537-5110. doi: https://doi.org/10.1016/j.biosystemseng.2018.02.008. URL https://www.sciencedirect.com/
science/article/pii/S1537511017302969 .
K SRIVASTAVA Durgesh and B Lekha. Data classification using support vector machine. Journal of theoretical and applied information
technology , 12(1):1–7, 2010.
16L. Dylewski, L. Maćkowiak, and W. Banaszak-Cibicka. Are all urban green spaces a favourable habitat for pollinator communities?
bees, butterflies and hoverflies in different urban green areas. Ecological Entomology , 44(5):678–689, 2019. doi: https://doi.org/10.
1111/een.12744. URL https://resjournals.onlinelibrary.wiley.com/doi/abs/10.1111/een.12744 .
M. A. Ebrahimi et al. Vision-based pest detection based on svm classification method. Computers and Electronics in Agriculture , 137:
52–58, 2017. ISSN 0168-1699. doi: https://doi.org/10.1016/j.compag.2017.03.016. URL https://www.sciencedirect.com/science/
article/pii/S016816991631136X .
M. M. ElQadi et al. Mapping species distributions with social media geo-tagged images: Case studies of bees and flowering plants
in australia. Ecological Informatics , 39:23–31, 2017. ISSN 1574-9541. doi: https://doi.org/10.1016/j.ecoinf.2017.02.006. URL
https://www.sciencedirect.com/science/article/pii/S1574954116302321 .
Sarah Goodall and Derek L Smith. The european wasp in metropolitan adelaide: it’s ecology, spread and impacts. South Australian
Geographical Journal , 100(2001):25–48, 2001.
Damon M. Hall, Gerardo R. Camilo, Rebecca K. Tonietto, Jeff Ollerton, Karin Ahrné, Mike Arduser, John S. Ascher, Katherine C. R.
Baldock, Robert Fowler, Gordon Frankie, Dave Goulson, Bengt Gunnarsson, Mick E. Hanley, Janet I. Jackson, Gail Langellotto,
David Lowenstein, Emily S. Minor, Stacy M. Philpott, Simon G. Potts, Muzafar H. Sirohi, Edward M. Spevak, Graham N. Stone,
and Caragh G. Threlfall. The city as a refuge for insect pollinators. Conservation Biology , 31(1):24–29, 2017. doi: https://doi.org/
10.1111/cobi.12840. URL https://conbio.onlinelibrary.wiley.com/doi/abs/10.1111/cobi.12840 .
R.J. Hall et al. Remote sensing of forest pest damage: a review and lessons learned from a canadian perspective. The Canadian
Entomologist , 148(S1):S296–S356, 2016. doi: 10.4039/tce.2016.11.
Caspar A. Hallmann, Martin Sorg, Eelke Jongejans, Henk Siepel, Nick Hofland, Heinz Schwan, Werner Stenmans, Andreas Müller,
Hubert Sumser, Thomas Hörren, Dave Goulson, and Hans de Kroon. More than 75 percent decline over 27 years in total flying insect
biomass in protected areas. PLOS ONE , 12(10):1–21, 10 2017. doi: 10.1371/journal.pone.0185809. URL https://doi.org/10.1371/
journal.pone.0185809 .
Christopher A. Halsch et al. Insects and recent climate change. Proceedings of the National Academy of Sciences , 118(2):e2002543117,
2021. doi: 10.1073/pnas.2002543117. URL https://www.pnas.org/doi/abs/10.1073/pnas.2002543117 .
Van Dyck Hans, Matthysen Erik, et al. Habitat fragmentation and insect flight: a changing ‘design’in a changing landscape? Trends in
Ecology & Evolution , 14(5):172–174, 1999. doi: 10.1016/S0169-5347(99)01610-9. URL https://doi.org/10.1016/S0169-5347(99)
01610-9.
Kaiming He et al. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 770–778, June 2016.
Petr Heneberg, Petr Hesoun, and Jiří Skuhrovec. Succession of arthropods on xerothermophilous habitats formed by sand quarrying:
Epigeic beetles (coleoptera) and orthopteroids (orthoptera, dermaptera and blattodea). Ecological Engineering , 95:340–356, 2016.
ISSN 0925-8574. doi: https://doi.org/10.1016/j.ecoleng.2016.06.022. URL https://www.sciencedirect.com/science/article/pii/
S0925857416303470 .
Petr Heneberg and Milan Řezáč. Dry sandpits and gravel–sandpits serve as key refuges for endangered epigeic spiders (araneae) and
harvestmen (opiliones) of central european steppes aeolian sands. Ecological Engineering , 73:659–670, 2014. ISSN 0925-8574. doi:
https://doi.org/10.1016/j.ecoleng.2014.09.101. URL https://www.sciencedirect.com/science/article/pii/S0925857414005126 .
Brad G. Howlett and Megan Gee. The potential management of the drone fly (eristalis tenax) as a crop pollinator in new zealand. New
Zealand Plant Protection , 72:221–230, Jul. 2019. doi: 10.30843/nzpp.2019.72.304. URL https://nzpps.org/_journal/index.php/
nzpp/article/view/304 .
B. Jena et al. Artificial intelligence-based hybrid deep learning models for image classification: The first narrative review. Computers
in Biology and Medicine , 137:104803, 2021. ISSN 0010-4825. doi: https://doi.org/10.1016/j.compbiomed.2021.104803. URL https:
//www.sciencedirect.com/science/article/pii/S0010482521005977 .
A. Joly et al. Overview of lifeclef 2019: Identification of amazonian plants, south & north american birds, and niche prediction. In Ex-
perimental IR Meets Multilinguality, Multimodality, and Interaction , pages 387–401, Cham, 2019. Springer International Publishing.
ISBN 978-3-030-28577-7. doi: 10.1007/978-3-030-28577-7_29. URL https://doi.org/10.1007/978-3-030-28577-7_29 .
L. E. Jones and R. S. Leather. Invertebrates in urban areas: A review. EJE, 109(4):463–478, 2012. ISSN 12105759. doi: 10.14411/eje.
2012.060. URL https://www.eje.cz/artkey/eje-201204-0001.php .
K. Kolenda et al. Deadly trap or sweet home? the case of discarded containers as novelty microhabitats for ants. Global
Ecology and Conservation , 23:e01064, 2020. ISSN 2351-9894. doi: https://doi.org/10.1016/j.gecco.2020.e01064. URL https:
//www.sciencedirect.com/science/article/pii/S2351989420303395 .
Mikhail Kozlov. Patterns of forest insect distribution within a large city: microlepidoptera in st peterburg, russia. Journal of Biogeog-
raphy, 23(1):95–103, 1996. doi: https://doi.org/10.1046/j.1365-2699.1996.d01-219.x. URL https://onlinelibrary.wiley.com/doi/
abs/10.1046/j.1365-2699.1996.d01-219.x .
S. R. Leather. “ecological armageddon”-more evidence for the drastic decline in insect numbers. Annals of Applied Biology , 172(1):1–3,
2017. doi: 10.1111/aab.12410. URL https://doi.org/10.1111/aab.12410 .
W. Li et al. Classification and detection of insects from field images using deep learning for smart pest management: A systematic
review.Ecological Informatics , 66:101460, 2021. ISSN 1574-9541. doi: https://doi.org/10.1016/j.ecoinf.2021.101460. URL https:
//www.sciencedirect.com/science/article/pii/S157495412100251X .
17Y. Li et al. Crop pest recognition in natural scenes using convolutional neural networks. Computers and Electronics in Agriculture ,
169:105174, 2020. ISSN 0168-1699. doi: https://doi.org/10.1016/j.compag.2019.105174. URL https://www.sciencedirect.com/
science/article/pii/S0168169919313638 .
T. Y. Lin et al. Microsoft coco: Common objects in context. In Computer Vision – ECCV 2014 , pages 740–755, Cham, 2014. Springer
International Publishing. ISBN 978-3-319-10602-1. doi: 10.1007/978-3-319-10602-1_48. URL https://doi.org/10.1007/978-3-
319-10602-1_48 .
T. Liu et al. Detection of aphids in wheat fields using a computer vision technique. Biosystems Engineering , 141:82–93, 2016. ISSN
1537-5110. doi: https://doi.org/10.1016/j.biosystemseng.2015.11.005. URL https://www.sciencedirect.com/science/article/pii/
S1537511015300866 .
John E. Losey and Mace Vaughan. The Economic Value of Ecological Services Provided by Insects. BioScience , 56(4):311–323, 04 2006.
ISSN 0006-3568. doi: 10.1641/0006-3568(2006)56[311:TEVOES]2.0.CO;2. URL https://doi.org/10.1641/0006-3568(2006)56[311:
TEVOES]2.0.CO;2 .
L. P. Lounibos. Invasions by insect vectors of human disease. Annual Review of Entomology , 47:233, 2002. Copyright - Copyright
Annual Reviews, Inc. 2002; Last updated - 2014-05-20.
V. D. Maaten et al. Visualizing data using t-sne. Journal of machine learning research , 9(11):2579–2605, 2008.
M. Maharlooei et al. Detection of soybean aphids in a greenhouse using an image processing technique. Computers and Elec-
tronics in Agriculture , 132:63–70, 2017. ISSN 0168-1699. doi: https://doi.org/10.1016/j.compag.2016.11.019. URL https:
//www.sciencedirect.com/science/article/pii/S0168169916310791 .
M. L. McKinney and J. L. Lockwood. Biotic homogenization: a few winners replacing many losers in the next mass extinction.
Trends in Ecology and Evolution , 14(11):450–453, 1999. ISSN 0169-5347. doi: https://doi.org/10.1016/S0169-5347(99)01679-1. URL
https://www.sciencedirect.com/science/article/pii/S0169534799016791 .
M.S. Norouzzadeh et al. Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning.
Proceedings of the National Academy of Sciences , 115(25):E5716–E5725, 2018. doi: 10.1073/pnas.1719367115. URL https://www.
pnas.org/doi/abs/10.1073/pnas.1719367115 .
S. Pincebourde and H. A. Woods. There is plenty of room at the bottom: microclimates drive insect vulnerability to climate change.
Current Opinion in Insect Science , 41:63–70, 2020. ISSN 2214-5745. doi: https://doi.org/10.1016/j.cois.2020.07.001. URL https:
//www.sciencedirect.com/science/article/pii/S2214574520300870 . Global change biology * Molecular physiology section.
G. D. Powney, C. Carvell, M. Edwards, Roger KA Morris, Helen E Roy, Ben A Woodcock, and Nick JB Isaac. Widespread losses of
pollinating insects in britain. Nature communications , 10(1):1018, 2019. doi: 10.1038/s41467-019-08974-9. URL https://doi.org/
10.1038/s41467-019-08974-9 .
M. Preti et al. Insect pest monitoring with camera-equipped traps: strengths and limitations. Journal of Pest Science , 94(2):203–217,
2021. doi: 10.1007/s10340-020-01309-4. URL https://doi.org/10.1007/s10340-020-01309-4 .
Y. Qing et al. An insect imaging system to automate rice light-trap pest identification. Journal of Integrative Agriculture , 11(6):978–
985, 2012. ISSN 2095-3119. doi: https://doi.org/10.1016/S2095-3119(12)60089-6. URL https://www.sciencedirect.com/science/
article/pii/S2095311912600896 .
Fuji Ren et al. Feature reuse residual networks for insect pest recognition. IEEE Access , 7:122758–122768, 2019. doi: 10.1109/ACCESS.
2019.2938194.
Manu E Saunders, Jasmine K Janes, and James C O’Hanlon. Moving On from the Insect Apocalypse Narrative: Engaging with
Evidence-Based Insect Conservation. BioScience , 70(1):80–89, 12 2019. ISSN 0006-3568. doi: 10.1093/biosci/biz143. URL https:
//doi.org/10.1093/biosci/biz143 .
S. Seibold, M. M. Gossner, N. K. Simons, Nico Blüthgen, Jörg Müller, Didem Ambarlı, Christian Ammer, Jürgen Bauhus, Markus
Fischer, Jan C Habel, et al. Arthropod decline in grasslands and forests is associated with landscape-level drivers. Nature, 574(7780):
671–674, 2019. doi: 10.1038/s41586-019-1684-3. URL https://doi.org/10.1038/s41586-019-1684-3 .
Timothy K Shih et al. An intelligent content-based image retrieval system based on color, shape and spatial relations. proceedings-
national science council republic of China part a physical science and engineering , 25(4):232–243, 2001.
Eyal Shochat, Paige S. Warren, Stanley H. Faeth, Nancy E. McIntyre, and Diane Hope. From patterns to emerging processes in
mechanistic urban ecology. Trends in Ecology &Evolution , 21(4):186–191, 2006. ISSN 0169-5347. doi: https://doi.org/10.1016/j.
tree.2005.11.019. URL https://www.sciencedirect.com/science/article/pii/S0169534705003848 .
Mani Shrestha, Jair E. Garcia, Freya Thomas, Scarlett R. Howard, Justin H. J. Chua, Thomas Tscheulin, Alan Dorin, Anders Nielsen,
and Adrian G. Dyer. Insects in the city: Does remnant native habitat influence insect order distributions? Diversity , 13(4), 2021.
ISSN 1424-2818. doi: 10.3390/d13040148. URL https://www.mdpi.com/1424-2818/13/4/148 .
P. D. Smedt, S Poel, et al. Succession in harvestman (opiliones) communities within an abandoned sand quarry in belgium. Belgian
Journal of Zoology , 147(2):155–169, 2017. doi: 10.26496/bjz.2017.13. URL https://doi.org/10.26496/bjz.2017.13 .
Tobias J. Smith and Manu E. Saunders. Honey bees: the queens of mass media, despite minority rule among insect pollinators.
Insect Conservation and Diversity , 9(5):384–390, 2016. doi: https://doi.org/10.1111/icad.12178. URL https://resjournals.
onlinelibrary.wiley.com/doi/abs/10.1111/icad.12178 .
18JP Spradbery, GF Maywald, et al. The distribution of the european or german wasp, vespula germanica (f.)(hymenoptera: Vespidae),
in australia: past, present and future. Australian journal of zoology , 40(5):495–510, 1992. URL https://www.researchgate.net/
profile/Philip-Spradbery/publication/304858842_The_distribution_of_the_european_or_german_wasp_vespula_germanica_F_
Hymenoptera_Vespidae_in_Australia_Past_present_and_future/links/57f5859a08ae91deaa5c8090/The-distribution-of-the-
european-or-german-wasp-vespula-germanica-F-Hymenoptera-Vespidae-in-Australia-Past-present-and-future.pdf .
R. Steen. Diel activity, frequency and visit duration of pollinators in focal plants: in situ automatic camera monitoring and data
processing. Methods in Ecology and Evolution , 8(2):203–213, 2017. doi: https://doi.org/10.1111/2041-210X.12654. URL https:
//besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12654 .
R. Steenweg et al. Scaling-up camera traps: monitoring the planet’s biodiversity with networks of remote sensors. Frontiers in Ecology
and the Environment , 15(1):26–34, 2017. doi: https://doi.org/10.1002/fee.1448. URL https://esajournals.onlinelibrary.wiley.
com/doi/abs/10.1002/fee.1448 .
Francisco Sánchez-Bayo and Kris A.G. Wyckhuys. Worldwide decline of the entomofauna: A review of its drivers. Biological Conser-
vation, 232:8–27, 2019. ISSN 0006-3207. doi: https://doi.org/10.1016/j.biocon.2019.01.020. URL https://www.sciencedirect.com/
science/article/pii/S0006320718313636 .
E. C. Tetila et al. Detection and classification of soybean pests using deep learning with uav images. Computers and Elec-
tronics in Agriculture , 179:105836, 2020. ISSN 0168-1699. doi: https://doi.org/10.1016/j.compag.2020.105836. URL https:
//www.sciencedirect.com/science/article/pii/S016816991831055X .
J. A. Thomas, M. G. Telfer, D. B. Roy, C. D. Preston, J. J. D. Greenwood, J. Asher, R. Fox, R. T. Clarke, and J. H. Lawton.
Comparative losses of british butterflies, birds, and plants and the global extinction crisis. Science, 303(5665):1879–1881, 2004. doi:
10.1126/science.1095046. URL https://www.science.org/doi/abs/10.1126/science.1095046 .
Vivek Tiwari et al. Real-time soybean crop insect classification using customized deep learning models. In Data Management, Analytics
and Innovation , pages 143–156, Singapore, 2021. Springer Singapore. ISBN 978-981-16-2934-1.
C. Torresan et al. Forestry applications of uavs in europe: a review. International Journal of Remote Sensing , 38(8-10):2427–2447,
2017. doi: 10.1080/01431161.2016.1252477. URL https://doi.org/10.1080/01431161.2016.1252477 .
Naeem Ullah et al. An efficient approach for crops pests recognition and classification based on novel deeppestnet deep learning model.
IEEE Access , 10:73019–73032, 2022. doi: 10.1109/ACCESS.2022.3189676.
S. Uno, J. Cotton, and S. M. Philpott. Diversity, abundance, and species composition of ants in urban green spaces. Urban Ecosystems ,
13(4):425–441, 2010. doi: 10.1007/s11252-010-0136-5. URL https://doi.org/10.1007/s11252-010-0136-5 .
E. B. Wilson. Probable inference, the law of succession, and statistical inference. Journal of the American Statistical Association , 22
(158):209–212, 1927. doi: 10.1080/01621459.1927.10502953. URL https://www.tandfonline.com/doi/abs/10.1080/01621459.1927.
10502953 .
J. Wäldchen and P. Mäder. Machine learning for image based species identification. Methods in Ecology and Evolution , 9(11):2216–
2225, 2018. doi: https://doi.org/10.1111/2041-210X.13075. URL https://besjournals.onlinelibrary.wiley.com/doi/abs/10.
1111/2041-210X.13075 .
L. Yang et al. Wdnet: Watermark-decomposition network for visible watermark removal. In Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision (WACV) , pages 3685–3693, January 2021.
H. Yousif, J. Yuan, et al. Animal scanner: Software for classifying humans, animals, and empty frames in camera trap images. Ecology
and Evolution , 9(4):1578–1589, 2019. doi: https://doi.org/10.1002/ece3.4747. URL https://onlinelibrary.wiley.com/doi/abs/
10.1002/ece3.4747 .
L.R.S.Zanette, R.P.Martins, andS.P.Ribeiro. Effectsofurbanizationonneotropicalwaspandbeeassemblagesinabrazilianmetropolis.
Landscape and Urban Planning , 71(2):105–121, 2005. ISSN 0169-2046. doi: https://doi.org/10.1016/j.landurbplan.2004.02.003. URL
https://www.sciencedirect.com/science/article/pii/S0169204604000325 .
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In David Fleet, Tomas Pajdla, Bernt Schiele,
and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014 , pages 818–833, Cham, 2014. Springer International Publishing. ISBN
978-3-319-10590-1.
J. Zhang et al. Monitoring plant diseases and pests through remote sensing technology: A review. Computers and Electronics in Agri-
culture, 165:104943, 2019. ISSN 0168-1699. doi: https://doi.org/10.1016/j.compag.2019.104943. URL https://www.sciencedirect.
com/science/article/pii/S016816991930290X .
S. Zschokke, C. Dolt, HP Rusterholz, Peter Oggier, Brigitte Braschler, G Heinrich Thommen, Eric Lüdin, Andreas Erhardt, and Bruno
Baur. Short-term responses of plants and invertebrates to experimental small-scale grassland fragmentation. Oecologia , 125(4):
559–572, 2000. doi: 10.1007/s004420000483. URL https://doi.org/10.1007/s004420000483 .
19