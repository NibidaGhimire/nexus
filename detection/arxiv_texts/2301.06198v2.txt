Generalized Neural Closure Models with Interpretability
Abhinav Gupta1and Pierre F.J. Lermusiaux1
1Department of Mechanical Engineering, Center for Computational Science and
Engineering, Massachusetts Institute of Technology, Cambridge, MA 02139,
guptaa@mit.edu,, pierrel@mit.edu (Corresponding author)
Friday 19thMay, 2023
Abstract
Improving the predictive capability and computational cost of dynamical models is often
at the heart of augmenting computational physics with machine learning (ML). However, most
learning results are limited in interpretability and generalization over different computational
grid resolutions, initial and boundary conditions, domain geometries, and physical or problem-
specific parameters. In the present study, we simultaneously address all these challenges by
developing the novel and versatile methodology of unified neural partial delay differential equa-
tions. We augment existing/low-fidelity dynamical models directly in their partial differential
equation (PDE) forms with both Markovian and non-Markovian neural network (NN) closure
parameterizations. The melding of the existing models with NNs in the continuous spatiotem-
poral space followed by numerical discretization automatically allows for the desired generaliz-
ability. The Markovian term is designed to enable extraction of its analytical form and thus
provides interpretability. The non-Markovian terms allow accounting for inherently missing
time delays needed to represent the real world. Our flexible modeling framework provides full
autonomy for the design of the unknown closure terms such as using any linear-, shallow-, or
deep-NN architectures, selecting the span of the input function libraries, and using either or both
Markovian and non-Markovian closure terms, all in accord with prior knowledge. We obtain
adjoint PDEs in the continuous form, thus enabling direct implementation across differentiable
and non-differentiable computational physics codes, different ML frameworks, and treatment of
nonuniformly-spaced spatiotemporal training data. We demonstrate the new generalized neural
closure models (gnCMs) framework using four sets of experiments based on advecting nonlinear
waves, shocks, and ocean acidification models. Our learned gnCMs discover missing physics,
find leading numerical error terms, discriminate among candidate functional forms in an inter-
pretable fashion, achieve generalization, and compensate for the lack of complexity in simpler
models. Finally, we analyze the computational advantages of our new framework.
Keywords— partial delay differential equations, reduced-order-model, turbulence closure, ecosystem mod-
eling, ocean acidification, data assimilation, deep learning.
1 Introduction
The field of Scientific Machine Learning (SciML; [46]) is burgeoning with innovative methods that combine
machine learning with existing scientifically-derived differential equation models and computational physics
schemes. This is in part because many realistic dynamical models are complex, and often truncated, coars-
ened, or aggregated due to computational cost constraints. Machine learning (ML) is then used to learn
and represent the neglected and unresolved terms in a data-driven fashion [23; 44; 45; 52; 62; 63; 55; 51].
1arXiv:2301.06198v2  [cs.LG]  18 May 2023Such techniques that express the missing dynamics as functions of modeled state variables and parameters
are referred to as closure models. Most ML closure models (and SciML results in general) are however often
limited both in interpretability as black-box ML models and in generalization over different computational
grid resolutions, initial conditions, boundary conditions, domain geometries, and physical or problem-specific
parameters. Addressing the challenges of interpretability and generalization is imperative to justify the costs
of training the SciML models using data sets obtained from expensive measurements or generated by solving
the complex dynamical models in the first place. The goal of the present study is to simultaneously address
these challenges and learn closure models which are both generalizable and interpretable.
The need for closure modeling arises for a variety of reasons, ranging from computational cost consid-
erations, preference for simpler models over complex ones due to overparameterization, or lack of scientific
understanding of processes and variables involved in the system of interest. The simpler or the known model
is often referred to as a low-fidelity model, while the complex counterpart in either models or observations is
then referred to as the high-fidelity model, reality, or real-world data. Low-fidelity models can be categorized
into three categories: (i) Reduced-order models, in which the original high-dimensional dynamical system is
projected and solved in a reduced space. While it is computationally cheaper to solve the low-dimensional
system, these models can quickly accumulate errors due to the missing interactions with the truncated di-
mensions [15; 14; 53]; (ii) Coarse-resolution models, in which we only resolve the scales of interest. In these
cases, the neglected and unresolved scales, along with their interactions with the resolved ones, can lead to
unintended or unacceptable effects at global scales [29; 66; 9; 37; 38]; (iii) Simplistic or speculative models,
in which an incomplete representation or understanding of processes and interactions occurs, and thus un-
certainty in the model formulations and even in the relevant state variables themselves. This can lead to a
gross or incorrect approximation of the real-world phenomena [13; 36; 42; 33; 30].
In [23], neural closure models (nCMs) are developed for low-fidelity models using neural delay differential
equations (nDDEs) and data from high-fidelity simulations. The need for time delays in closure parameteri-
zations is rooted in the presence of inherent delays in real-world systems [19; 59] and theoretically justifed by
the Mori-Zwanzig formulation [8; 20; 56; 64]. Using nDDEs for closure modeling has a number of advantages.
They allow for the use of smaller architectures and account for the accumulation of numerical time-stepping
error in the presence of neural networks (NNs) during training. Additionally, nDDEs are agnostic to the
time-integration scheme, handle unevenly-spaced training data, and have good performance over prediction
periods much longer than the training or validation periods. However, there are other highly-desirable prop-
erties, as mentioned above. Fundamental questions for neural closures include: Can they be interpretable
and lead to analytical expressions? Can they achieve generalization over many conditions and variables, as
physics-based models do? How can they be combined seamlessly with classic numerical schemes? A number
of recent approaches have aimed to address such questions, however, challenges remain especially for partial
differential equations (PDEs). This is often because NNs are used with the discretized ordinary differential
equation (ODE) form of the corresponding PDEs, which makes it inherently difficult to generalize to changes
in boundary conditions, domain geometry, and computational grid. Recently, a few studies have taken steps
at addressing these drawbacks. Sirignano et al. [55] augment the underlying PDE with a neural network,
however, they only learn a Markovian closure. The inputs to the neural network include the state, its spatial
derivatives, and a fixed number of neighboring grid points. They also provide an accompanying discrete
adjoint PDE for efficient training. Saha et al. [51] use a radial-basis-functions-based collocation method to
allow for mesh-free embedding of NNs. However, the resulting NNs also only learn a Markovian closure, do
not account for the accumulation of time-integration errors, and lack interpretability.
In the present study, we develop the unified neural partial delay differential equations (nPDDEs) that
augment existing/low-fidelity models in their PDE forms with both Markovian and non-Markovian closures
parameterized with deep-NNs. The neural closure terms then contain instantaneous and delayed contribu-
tions. Their inputs consist of the modeled states, their spatial derivatives, combinations of derivatives, and
any other problem-specific variables and parameters. The melding of the low-fidelity model and deep-NNs
in the continuous spatiotemporal space automatically allows for generalizability to computational grid reso-
lution, boundary conditions, and initial conditions. By design, the closure terms can also provide analytical
expressions of the missing terms, thus leading to interpretability. The resulting nPDDEs are discretized using
any numerical method relevant to the dynamical system studied. Further, we provide adjoint PDE deriva-
tions in the continuous form, thus allowing one to implement across differentiable and non-differentiable
computational physics codes, and also different machine learning frameworks. All our derivations and im-
2plementations are done considering deep-NN architectures, thus automatically encompassing linear- and
shallow-NNs, and providing the user or subject-matter-expert user with the flexibility of choosing the ar-
chitectural complexity in accord with the prior information available. We refer to the new methodology as
generalized neural closure models ( gnCM). Through a series of experiments, we demonstrate the flexibility of
gnCMs to learn closures either in an interpretable fashion, black-box fashion, or both simultaneously, using
the prior scientific knowledge about the problem at hand. The gnCMs can eliminate erroneous and redun-
dant input terms, or combine them to achieve increased accuracy. We also demonstrate the generalizability
of our learned closures to changes in physical parameters, grid resolution, initial conditions, and boundary
conditions. Our first class of simulation experiments uses nonlinear waves and advecting shocks problems
governed by the KdV-Burgers and classic Burgers PDEs. Our learned gnCM finds missing terms, discovers
the leading truncation error, and a correction to the non-linear advection term. We find that training on data
corresponding to just a few combinations of grid resolution and Reynolds number is sufficient to ensure that
the learned closures are generalizable over a range of grid resolution and Reynolds number combinations,
initial and boundary conditions, and also outperform the popular Smagorinsky subgrid-scale closure model.
Our second class of experiments is based on ocean acidification models, where we learn the functional form
of biological processes and compensate for the lack of complexity in simpler models obtained by aggregation
of components and other simplifications of processes and parameterizations. Finally, we comment on the
computational advantages of our new gnCM framework.
In what follows, Section 2 develops the theory and methodology for the gnCMs. Section 3 showcases the
generalization and interpretability properties of the gnCMs in experiments with nonlinear waves, advecting
shocks, and ocean acidification, and discusses computational advantages. Conclusions are in Section 4.
2 Theory and Methodology
The functional form of closure models representing missing dynamics can be derived by the Mori-Zwanzig
formulation [8; 20; 56; 64], which proves it to be dependent on the time-lagged state dynamics. Many systems
are modeled assuming smooth fields of state variables governed by advection-diffusion-reaction PDEs. Such
PDEs implicitly assume that local information between state variables is exchanged instantaneously at any
spatial location. In reality, however, time delays occur for several reasons. First, reactions or changes in
populations have non-negligible time scales. Such time delays are captured in more complex models by
modeling intermediate state variables. The time response of lower-complexity models can thus approximate
that of high-complexity models by explicitly introducing delays [19; 59]. Second, time delays arise due to
missing subgrid-scale processes and/or truncated modes in reduced-order models. For all of these reasons,
memory-based terms and thus non-Markovian closure terms are needed to augment low-fidelity models [23].
In general, low-fidelity models are also outright missing Markovian terms due to truncation, coarse
resolution, or incomplete and uncertain functional forms of some of the model terms. We will therefore use
both Markovian and non-Markovian terms to close low-fidelity models in their PDE forms. This leads to
partial delay differential equations (PDDEs) that are widely used in ecology, control theory, biology, and
climate dynamics, to name a few application areas [65].
In this study, the Markovian and non-Markovian closure terms will be modeled using deep-NNs. To
achieve full interpretability from the learned weights of the NNs of the closure models, we at times consider
single-layer linear-NNs. Closure terms in general depend on the state variables, their spatial derivatives,
and combinations of these belonging to a function library. As the presence of discrete delays can be seen as
a special case of distributed delays, the non-Markovian term is assumed to contain distributed delays and
have a maximum finite time-delay ( 𝜏). Given a continuous state vector comprising of 𝑁𝑠different states,
3𝑢(𝑥, 𝑡) :R×[0, 𝑇]→R𝑁𝑠, we thus consider a dynamical system belonging to domain Ω of the following form,
𝜕𝑢(𝑥, 𝑡)
𝜕𝑡=ℒ(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ..., 𝑥, 𝑡 ;𝜈)︂
⏟  ⏞  
𝐿𝑜𝑤−𝐹𝑖𝑑𝑒𝑙𝑖𝑡𝑦 / 𝐾𝑛𝑜𝑤𝑛 𝑀𝑜𝑑𝑒𝑙
+ℱ𝑁𝑁(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ..., 𝑥, 𝑡 ;𝜑)︂
⏟  ⏞  
𝑀𝑎𝑟𝑘𝑜𝑣𝑖𝑎𝑛 𝐶𝑙𝑜𝑠𝑢𝑟𝑒 𝑇𝑒𝑟𝑚
+∫︁𝑡
𝑡−𝜏𝒟𝑁𝑁(︂
𝑢(𝑥, 𝑠),𝜕𝑢(𝑥, 𝑠)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑠)
𝜕𝑥2, ..., 𝑥, 𝑠 ;𝜃)︂
𝑑𝑠
⏟  ⏞  
𝑁𝑜𝑛−𝑀𝑎𝑟𝑘𝑜𝑣𝑖𝑎𝑛 𝐶𝑙𝑜𝑠𝑢𝑟𝑒 𝑇𝑒𝑟𝑚, 𝑥∈Ω, 𝑡≥0,
𝑢(𝑥, 𝑡) =ℎ(𝑥, 𝑡),−𝜏≤𝑡≤0 andℬ(𝑢(𝑥, 𝑡)) =𝑔(𝑥, 𝑡), 𝑥∈𝜕Ω, 𝑡≥0,(1)
whereℒ,ℱ𝑁𝑁, and𝒟𝑁𝑁are nonlinear functions parameterized with 𝜈,𝜑, and 𝜃, respectively. 𝜈are problem-
specific parameters associated with the physical/biological/chemical phenomenon of interest, while 𝜑and𝜃
are the NN weights. When compared to PDEs, PDDEs require a history function ( ℎ(𝑥, 𝑡),−𝜏≤𝑡≤0) for
their initialization at 𝑡= 0. The operator ℬrepresents appropriate boundary conditions such as Dirichlet,
Neumann, etc. which are needed to solve the system uniquely. Furthermore, for ease of notation, we have
assumed a one-dimensional (1D) domain, however, the method directly extends to 2D and 3D domains.
2.1 Neural Partial Delay Differential Equations
We now obtain ML schemes that learn PDDEs parameterized using deep-NNs. They are referred to as
neural partial delay differential equations ( nPDDEs). Without loss of generality, and for brevity, we limit
ourselves to nPDDEs with only a Markovian term and a non-Markovian term with distributed delays. The
low-fidelity model can be considered to be absorbed in the Markovian closure term. Hence, the nPDDE is
of the form,
𝜕𝑢(𝑥, 𝑡)
𝜕𝑡=ℱ𝑁𝑁(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡)
𝜕𝑥𝑑, 𝑥, 𝑡;𝜑)︂
+∫︁𝑡
𝑡−𝜏𝒟𝑁𝑁(︂
𝑢(𝑥, 𝑠),𝜕𝑢(𝑥, 𝑠)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑠)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑠)
𝜕𝑥𝑑, 𝑥, 𝑠;𝜃)︂
𝑑𝑠 ,
𝑥∈Ω, 𝑡≥0,
𝑢(𝑥, 𝑡) =ℎ(𝑥, 𝑡),−𝜏≤𝑡≤0 andℬ(𝑢(𝑥, 𝑡)) =𝑔(𝑥, 𝑡)𝑥∈𝜕Ω, 𝑡≥0.(2)
The two deep-NNs, instantaneous ℱ𝑁𝑁(∙;𝜑) and delayed𝒟𝑁𝑁(∙;𝜃), remain parameterized by 𝜑and𝜃, and
for generality, they are considered to be functions of an arbitrary number of spatial derivatives, with the
highest order defined by 𝑑∈Z+. We can rewrite equation 2 as an equivalent system of coupled PDDEs with
discrete delays,
𝜕𝑢(𝑥, 𝑡)
𝜕𝑡=ℱ𝑁𝑁(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡)
𝜕𝑥𝑑, 𝑥, 𝑡;𝜑)︂
+𝑦(𝑥, 𝑡),
𝑥∈Ω, 𝑡≥0,
𝜕𝑦(𝑥, 𝑡)
𝜕𝑡=𝒟𝑁𝑁(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡)
𝜕𝑥𝑑, 𝑥, 𝑡;𝜃)︂
−𝒟𝑁𝑁(︂
𝑢(𝑥, 𝑡−𝜏),𝜕𝑢(𝑥, 𝑡−𝜏)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡−𝜏)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡−𝜏)
𝜕𝑥𝑑, 𝑥, 𝑡−𝜏;𝜃)︂
,
𝑥∈Ω, 𝑡≥0,
𝑢(𝑥, 𝑡) =ℎ(𝑥, 𝑡),−𝜏≤𝑡≤0 andℬ(𝑢(𝑥, 𝑡)) =𝑔(𝑥, 𝑡), 𝑥∈𝜕Ω, 𝑡≥0,
𝑦(𝑥,0) =∫︁0
−𝜏𝒟𝑁𝑁(︂
ℎ(𝑥, 𝑠),𝜕ℎ(𝑥, 𝑠)
𝜕𝑥,𝜕2ℎ(𝑥, 𝑠)
𝜕𝑥2, ...,𝜕𝑑ℎ(𝑥, 𝑠)
𝜕𝑥𝑑, 𝑥, 𝑠;𝜃)︂
𝑑𝑠 .(3)
4Let us assume that high-fidelity data is available at 𝑀discrete times, 𝑇1< ... < 𝑇 𝑀≤𝑇, and at 𝑁(𝑇𝑖)
spatial locations ( 𝑥𝑇𝑖
𝑘∈Ω,∀𝑘∈1, ..., 𝑁 (𝑇𝑖)) for each of the times. Thus, we define the scalar loss function
as,𝐿=1
𝑀∑︀𝑀
𝑖=11
𝑁(𝑇𝑖)∑︀𝑁(𝑇𝑖)
𝑘=1𝑙(𝑢(𝑥𝑇𝑖
𝑘, 𝑇𝑖))≡∫︀𝑇
01
𝑀∑︀𝑀
𝑖=1∫︀
Ω1
𝑁(𝑇𝑖)∑︀𝑁(𝑇𝑖)
𝑘=1𝑙(𝑢(𝑥, 𝑡))𝛿(𝑥−𝑥𝑇𝑖
𝑘)𝛿(𝑡−𝑇𝑖)𝑑𝑥𝑑𝑡
≡∫︀𝑇
01
𝑀∑︀𝑀
𝑖=11
|Ω|∫︀
Ω^𝑙(𝑢(𝑥, 𝑡))𝛿(𝑡−𝑇𝑖)𝑑𝑥𝑑𝑡, where 𝑙(∙) are scalar loss functions such as mean-absolute-error
(MAE), and 𝛿(∙) is the Kronecker delta function. In order to derive the adjoint PDEs, we start with the
Lagrangian corresponding to the above system,
L=𝐿(𝑢(𝑥, 𝑡)) +∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡) (𝜕𝑡𝑢(𝑥, 𝑡)−ℱ𝑁𝑁(∙, 𝑡;𝜑)−𝑦(𝑥, 𝑡))𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡) (𝜕𝑡𝑦(𝑥, 𝑡)−𝒟𝑁𝑁(∙, 𝑡;𝜃) +𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃))𝑑𝑥𝑑𝑡
+∫︁
Ω𝛼𝑇(𝑥)(︂
𝑦(𝑥,0)−∫︁0
−𝜏𝒟𝑁𝑁(ℎ(𝑥, 𝑡), 𝜕𝑥ℎ(𝑥, 𝑡), 𝜕𝑥2ℎ(𝑥, 𝑡), ..., 𝜕 𝑥𝑑ℎ(𝑥, 𝑡), 𝑥, 𝑡;𝜃)𝑑𝑡)︂
𝑑𝑥 ,(4)
where 𝜆(𝑥, 𝑡),𝜇(𝑥, 𝑡) and 𝛼(𝑥) are the Lagrangian variables. To find the gradients of Lw.r.t. 𝜑and𝜃, we
first solve the following adjoint PDEs (for brevity we denote, 𝜕/𝜕(∙)≡𝜕(∙), and 𝑑/𝑑(∙)≡𝑑(∙)),
0 =1
𝑀1
|Ω|𝑀∑︁
𝑘=1𝜕𝑢(𝑥,𝑡)^𝑙(𝑢(𝑥, 𝑡))𝛿(𝑡−𝑇𝑘)
−𝜕𝑡𝜆𝑇(𝑥, 𝑡)−𝜆𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡) +𝑑∑︁
𝑖=1(−1)𝑖+1𝜕𝑥𝑖(︁
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑖𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︁
−𝜇𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃) +𝑑∑︁
𝑖=1(−1)𝑖+1𝜕𝑥𝑖(︁
𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑖𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︁
+𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)−𝑑∑︁
𝑖=1(−1)𝑖+1𝜕𝑥𝑖(︁
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑖𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︁
,
𝑥∈Ω, 𝑡∈[0, 𝑇),
0 =−𝜆𝑇(𝑥, 𝑡)−𝜕𝑡𝜇𝑇(𝑥, 𝑡), 𝑥∈Ω, 𝑡∈[0, 𝑇),(5)
with initial conditions, 𝜆(𝑥, 𝑡) =𝜇(𝑥, 𝑡) = 0 , 𝑡≥𝑇. The boundary conditions are derived based on those of
the forward PDDE and they satisfy,
0 =𝑑∑︁
𝑖=0𝑑−𝑖−1∑︁
𝑗=0(−1)𝑗+1𝜕𝑥𝑗(︁
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑗+𝑖+1𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︁
𝑑𝜃𝜕𝑥𝑖𝑢(𝑥, 𝑡)
+𝑑∑︁
𝑖=0𝑑−𝑖−1∑︁
𝑗=0(−1)𝑗+1𝜕𝑥𝑗(︁
𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑗+𝑖+1𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡))︁
𝑑𝜃𝜕𝑥𝑖𝑢(𝑥, 𝑡)
−𝑑∑︁
𝑖=0𝑑−𝑖−1∑︁
𝑗=0(−1)𝑗+1𝜕𝑥𝑗(︁
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑗+𝑖+1𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡))︁
𝑑𝜃𝜕𝑥𝑖𝑢(𝑥, 𝑡),
𝑥∈𝜕Ω, 𝑡∈[𝑡, 𝑇).(6)
Details of the derivation of the above adjoint PDEs are in section SI-1. After solving for the Lagrangian
variables, 𝜆(𝑥, 𝑡) and 𝜇(𝑥, 𝑡), we compute the required gradients as,
𝑑𝜃ℒ=−∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜃𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝑥𝑑𝑡 +∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜃𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝑑𝑥𝑑𝑡
−∫︁
Ω𝜇𝑇(𝑥,0)∫︁0
−𝜏𝜕𝜃𝒟𝑁𝑁(ℎ(𝑥, 𝑡), 𝜕𝑥ℎ(𝑥, 𝑡), 𝜕𝑥𝑥ℎ(𝑥, 𝑡), 𝑥, 𝑡;𝜃)𝑑𝑡𝑑𝑥 ,
𝑑𝜑ℒ=−∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜑ℱ𝑁𝑁(∙, 𝑡;𝜑)𝑑𝑥𝑑𝑡 .(7)
5Finally, using a stochastic gradient descent algorithm, we find the optimal values of the weights 𝜑and𝜃.
2.2 Generalized Neural Closure Models: Properties
The gnCM framework is schematized in Figure 1. Next, we discuss some of its properties and variations.
Figure 1: Overview of the generalized neural closure models ( gnCM) framework. The blocks labeled DNN represent
any deep-neural-network architectures. The block labeled∫︀
symbolizes any time-integration scheme. DDE stands
for delay differential equation.
Interpretability. For interpretability – especially for the Markovian closure term – we can use a sim-
ple NN architecture with no hidden layers and linear activation. Nonlinearity can still be introduced by
having input features that are nonlinear combinations of the states and their derivatives belonging to a
function library. The result is a linear combination of these nonlinear input features. Along with this, a
𝐿1regularization on the NN weights and pruning below a threshold helps promote sparsity, thus allowing
for redundancy in the input function library. In practice, one can include as many input test functions
as computationally efficient and scientifically meaningful, and then adaptively augment and prune this li-
brary during the data-driven learning process, similar to that demonstrated in [28]. Although this approach
has similarities to SINDy [4; 50; 28], it is significantly different. SINDy requires training data to be rich
enough to allow for the computation of temporal and spatial derivatives, and solves a regression problem
to discover the governing dynamical system. Some successors of SINDy circumvent the need for calculating
spatio-temporal derivatives from training data by utilizing weak forms [39] and NNs to map coordinates of
the problem to the state variable [2]. Our gnCM method also does not require using the training data to
compute any temporal and spatial derivatives. It further accounts for the accumulation of time-integration
errors during training by numerically solving the PDE augmented with the Markovian closure term and its
corresponding adjoint PDE. Compared to other model discovery methods, gnCM seamlessly incorporates
and simultaneously learns a non-Markovian closure term without simplifying assumptions.
The use of an informative function library along with a simple NN architecture with no hidden layers and
linear activation is also applicable for the non-Markovian term for enhanced interpretability. In fact, in our
6derivation above (section 2.1) and framework implementation, we keep the possibility of using any general
deep-NN architectures for both Markovian and non-Markovian closure terms. This allows one to introduce
an arbitrary amount of nonlinearity, especially in cases when no prior information is available about the
functional form of the missing dynamics. The use of deep-NNs comes at the cost of full interpretability.
However, even in this case, some insight can be obtained, for example, by examining the weights of the
input layer of the learned deep-NN to determine the relative importance of different input features. This is
showcased in Experiments-1b (section 3.2) for the learned non-Markovian closure term.
Generalizability. The forward model (equation 1 or 2) and the adjoint PDEs (equation 5) are discretized
and integrated using numerical schemes [6], such as finite differences, finite volumes, collocation methods, etc.
This new approach, where we augment the PDEs with the NN-based Markovian and non-Markovian closures
first, before numerical discretization, ensures that the burden of generalization over boundary conditions,
domain geometry, and computational grid resolution, along with computing the relevant spatial derivatives
is handled by the numerical schemes, and not by the learned deep-NNs. This also automatically makes the
learning only dependent on local features and affine equivariant, similar to numerical schemes.
Backpropagation and Adjoint Equations. With the adjoint method , the adjoint PDEs (equations 5
& 6) are solved backward in time, and one would require access to 𝑢(𝑥, 𝑡),∀𝑥∈Ω,0≤𝑡≤𝑇. In the original
neural ODEs [7], the proposed adjoint method forgets the forward-time trajectory 𝑢(𝑥, 𝑡),∀𝑥∈Ω,0≤𝑡≤𝑇;
instead, it remembers only the state at the final time, 𝑢(𝑥, 𝑇), and then solves for 𝑢(𝑥, 𝑡) in reverse-time along
with the adjoint PDEs. This approach is known to suffer from inaccuracies and numerical instabilities [18; 68].
Thus, in our current implementation, we create and continuously update an interpolation function using the 𝑢
obtained at every time step as we solve the forward model (equation 2). For memory efficiency, one could also
use the method of checkpointing [21; 18; 68], or the interpolated reverse dynamic method (IRDM) [10]. Along
with this, using adaptive time-integration schemes leads to stable and accurate solutions for our forward and
adjoint PDEs, especially for stiff dynamical systems [67; 12]. The inherent inaccuracies and instabilities
of using continuous adjoint equations followed by discretization remain open questions [18; 67; 12; 68; 35]
and well-known issues in data assimilation [49; 48]. In this work, we found that the combination of the
continuous adjoint method followed by discretization and adaptive time-integration schemes is successful.
Another challenge that can occur is the feasibility of derivation of the continuous adjoint PDEs followed by
discretization, especially for known realistic (low-fidelity) models that are highly complex and nonlinear. In
such cases, the discrete adjoint method, i.e., the approach of deriving the adjoint equations for the discrete
forward model might be more useful [34; 40]. This makes it easier to utilize the vast array of tools developed
by the Automatic Differentiation community over the last several decades [5], specifically, the source-code-
transformation (source-to-source) methods [60; 61]. Finally, reduced-space adjoints as well as ensemble
approaches can be used to estimate gradients [17].
3 Application Results and Discussion
Using four sets of experiments, we now showcase and evaluate the capabilities of our new closure modeling
framework ( gnCM) in terms of generalizability over grid resolutions, boundary and initial conditions, and
problem-specific parameters. We also demonstrate the interpretability of the learned closures within PDEs.
In the first and second sets of experiments, we consider problems based on advecting nonlinear wave and
shock PDEs. We find that gnCMs can discriminate and discover processes such as dispersion, the leading
truncation error term, and a correction to the nonlinear advection term, all in an interpretable fashion with
the learned Markovian neural closure. Using both Markovian and non-Markovian neural closure terms, we
demonstrate the generalization of the gnCM over grid resolution, Reynolds number, and initial and boundary
conditions, along with superior performance compared to the popular Smagorinsky closure. In the third and
fourth sets of experiments, we consider problems based on coupled physical-biological-carbonate PDEs used
to study the threat of ocean acidification. We utilize the gnCMs to discriminate and discover the functional
form of uncertain terms with interpretability, and to augment a simpler model obtained by aggregation of
components and simplifications of processes and parameterizations, such that, it becomes as accurate as a
more complex model.
7Our training and evaluation protocol is similar to that in [23]. We use performance over the validation
period (past the period for which high-fidelity data snapshots are used for training) to fine-tune various
training-related hyperparameters. The final evaluation is based on continuous evolution through the training
and validation periods, followed by longer-term future predictions. We also compare the learned closure with
the known true model. For all the figure, table, and section references prefixed with “SI-”, we direct the
reader to the Supplementary Information .
3.1 Experiments 1a: Nonlinear Waves - Interpretable Model Discrimination
In the first set of experiments, we consider advecting nonlinear wave PDEs and discover missing/uncertain
physical processes, such as dispersion, in an interpretable fashion, using the learned Markovian neural closure.
Setup: True model, data generation, and low-fidelity model. Models for advecting shocks and nonlinear
waves are the backbone of various physical phenomena. The Korteweg de Vries (KdV)-Burgers PDE is often
used to describe weak effects of dispersion, dissipation, and non-linearity in such wave propagation [54].
Here, considering a 1D spatial domain, we select this KdV-Burgers PDE as the high-fidelity model (truth),
𝜕𝑢
𝜕𝑡=−6𝑢𝜕𝑢
𝜕𝑥−𝜕3𝑢
𝜕𝑥3. (8)
The data is generated from two solitary waves colliding with each other and that are exact solutions of
equation 8 with initial and boundary conditions given by,
𝑢(𝑥,0) = 2 𝜂2
1sech[𝜂1(𝑥−𝑥1)] + 2 𝜂2
2sech[𝜂2(𝑥−𝑥2)],
𝑢(−𝐿, 𝑡) = 0 ,𝜕𝑢(𝑥, 𝑡)
𝜕𝑥⃒⃒⃒⃒
𝑥=𝐿= 0,and𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2⃒⃒⃒⃒
𝑥=𝐿= 0,(9)
where 𝑥1is the location, 2 𝜂2
1is the amplitude, and 1 /𝜂1is the width of the first soliton wave, whereas 𝑥2is
the location, 2 𝜂2
2is the amplitude, and 1 /𝜂2is the width of the second soliton wave, initially. The parametric
analytical solution of the above system is given by,
𝑢(𝑥, 𝑡) =8(𝜂2
1−𝜂2
2)(𝜂2
1cosh𝜃2+𝜂2
2sinh𝜃1)
((𝜂1−𝜂2) cosh( 𝜃1+𝜃2) + (𝜂1+𝜂2) cosh( 𝜃1−𝜃2))2, (10)
where 𝜂1≥𝜂2, and 𝜃1and𝜃2are given by,
𝜃1=𝜂1(𝑥−𝑥1−4𝜂2
1𝑡),
𝜃2=𝜂1(𝑥−𝑥2−4𝜂2
2𝑡).(11)
We choose 𝐿= 10, maximum time 𝑇= 1.5,𝜂1= 1.2,𝜂2= 0.8,𝑥1=−6.0 and 𝑥2=−2.0.
For the closure learning experiments, we assume we only have prior knowledge about the existence of
the advection term and the low-fidelity model is thus,
𝜕𝑢
𝜕𝑡=−𝑢𝜕𝑢
𝜕𝑥. (12)
Other effects are unknown and need to be discovered. We assume these unknown effects to be mainly
Markovian in nature and that they can be modeled using a linear combination from a library of nonlinear
functions comprising terms up to 3rd order spatial derivatives:{︁
𝜕2𝑢
𝜕𝑥2,𝜕3𝑢
𝜕𝑥3, 𝑢𝜕𝑢
𝜕𝑥, 𝑢2𝜕𝑢
𝜕𝑥}︁
. Compared to the
true model (equation 8), our library contains two superfluous or redundant terms,𝜕2𝑢
𝜕𝑥2and𝑢2𝜕𝑢
𝜕𝑥. Of course,
it does not contain repetitive terms.
Numerics. All the numerical solutions of low-fidelity model augmented with the gnCM are obtained
using finite difference schemes. For the advection term, 2𝑛𝑑order accurate upwind [47] is used, while all
other spatial terms and derivatives are discretized with 4𝑡ℎorder accurate central-difference. For time-
marching, the Vode scheme [3] with adaptive time-stepping is used. Finally, we employ a fine grid with
𝑁𝑥= 200 number of grid points in the 𝑥−direction in order to keep low discretization and truncation errors.
Comparison LF-HF: In Figure 2, we compare the numerical solution of the low-fidelity model (equation
812) with the analytical solution of the high-fidelity model (equations 8, 9 & 10). The solutions of the two
models have the same initial condition, however, their evolutions are drastically different. With the high-
fidelity model, the two solitons interact elastically, i.e., their amplitudes and shapes are unchanged after
the interaction, however, they do experience a phase shift in their positions. With the low-fidelity model,
however, the two solitons do not even come close to interacting with each other.
Figure 2: Comparison of the numerical solution of the KdV-Burgers equation with only the advection term (equation
12; low-fidelity model; middle plot ), with the analytical solution corresponding to the equation with stronger advection
and 3𝑟𝑑order derivative term (equations 8, 9 & 10; high-fidelity model; left plot ). The low-fidelity model is solved on
a grid with 𝑁𝑥= 200 grid points. The absolute difference between the two solutions is provided in the right panel .
Training: NN architecture, data, and loss function. For the gnCM, we only consider the Markovian
term with a simple neural network with no hidden layer and only linear activation in the output layer, in-
effect equivalent to a linear combination of the inputs. The training data consists of the analytical solution
(equation 10) sampled at time intervals of 0.01 until time 𝑡= 1.0, with a validation period from 1 .0≤𝑡≤1.25.
In all the experiments, we use both ℒ1andℒ2regularization for the weights of the neural network, and
prune them if their value drops below a certain threshold (only if the weightage of ℒ1regularization is non-
zero), in order to promote sparsity. The set of tuned hyperparameters used to generate the results presented
next are provided in the supplementary information, section SI-2.2. Given the analytical solution data,
{𝑢𝑡𝑟𝑢𝑒(𝑥, 𝑇𝑖),−𝐿≤𝑥≤𝐿}𝑀
𝑖=1, the loss function is based on time and space averaged mean-absolute-error
(MAE),ℒ=1
𝑀∑︀𝑀
𝑖=1∫︀𝐿
−𝐿1
2𝐿|𝑢𝑝𝑟𝑒𝑑(𝑥, 𝑇𝑖)−𝑢𝑡𝑟𝑢𝑒(𝑥, 𝑇𝑖)|𝑑𝑥, where 𝑀= 100 is the number of high-fidelity
solution states at different times available for training.
Learning results. We perform 6 repeats of the experiment with exactly the same set of hyperparameters,
and the learned model with the mean and standard deviation of the weights is as follows,
𝜕𝑢
𝜕𝑡=−𝑢𝜕𝑢
𝜕𝑥−(4.9680±0.0008) 𝑢𝜕𝑢
𝜕𝑥−(1.0105±0.0002)𝜕3𝑢
𝜕𝑥3. (13)
The true coefficients corresponding to the learned 𝑢𝜕𝑢
𝜕𝑥and𝜕3𝑢
𝜕𝑥3terms are−5.0 and−1.0, respectively.
The learned closure is able to recover the true model, and the slight discrepancy in the learned coefficients
is to compensate for the very small discretization and truncation errors. To illustrate this, we compare
the root-mean-square-error (RMSE), ℒ=1
𝑀∑︀𝑀
𝑖=1√︁∑︀𝑁𝑥
𝑗=11
𝑁𝑥(𝑢𝑝𝑟𝑒𝑑(𝑥𝑗, 𝑇𝑖)−𝑢𝑡𝑟𝑢𝑒(𝑥𝑗, 𝑇𝑖))2, of the learned
closure and the true model solved using the same numerical schemes. The RMSE (mean and standard
deviation) obtained for the learned closure and the true model solved numerically is 0 .0063±0.0014 and
0.0251, respectively. Thus, on average, the learned closure leads to a smaller RMSE than the error of the
numerically-solved true model. We note that this excellent accuracy in the coefficients of the recovered
(learned) model compared to the true model is similar to that observed in SINDy and its variants for the
KdV PDE in [50; 39; 2].
Sensitivity. The learning was sensitive to batch-time, and higher values were especially detrimental to
convergence. This behavior is in general observed when the error between the low- and high-fidelity models
is large, e.g., when there is no low-fidelity model. Using a smaller batch size and regularization weights
lead to slightly different values of the learned coefficients. This is especially noted for the 𝑢2𝜕𝑢
𝜕𝑥term, whose
weight tends towards a non-zero value with a very small magnitude. For a study on the impact of different
hyperparameters (encountered specifically in the nCM framework and SciML in general) on training, we
9refer to [26]. In the current set of experiments, the learning framework is able to recover the known true
model and, due to this, we do not additionally focus on demonstrating generalization over initial conditions,
boundary conditions, and grid resolution.
3.2 Experiments 1b: Advecting Shock - Model Discovery and Generalization
In the second set of experiments, we employ the advecting shock PDE models. First, a gnCM discovers
the leading truncation term and a correction to the nonlinear advection term by interpreting the learned
Markovian neural closure. Second, we utilize both Markovian and non-Markovian gnCM terms trained on
data corresponding to just a few combinations of grid resolution and Reynolds number, and demonstrate
the generalization of the learned closure model over grid resolution, Reynolds number, initial and boundary
conditions, along with superior performance compared to the popular Smagorinsky closure model. We further
interpret the learned closure by analysing the weights of the learned neural networks, and find the closure
to be independent of the Reynolds number despite it being one of the functional inputs.
Setup: True model, data generation, and low-fidelity model. We consider the classic form of the Burgers
equation as the governing high-fidelity model,
𝜕𝑢
𝜕𝑡=−𝑢𝜕𝑢
𝜕𝑥+𝜈𝜕2𝑢
𝜕𝑥2,0≤𝑥≤𝐿, 𝑡∈(0, 𝑇], (14)
where 𝜈is the diffusion coefficient. The data is generated from an analytical solution of this Burgers equation
14 with initial and boundary conditions,
𝑢(𝑥,0) =𝑥
1 +√︁
1
𝑡0exp(︀
𝑅𝑒𝑥2
4)︀, 𝑢(0, 𝑡) = 0 ,and𝜕𝑢(𝑥, 𝑡)
𝜕𝑥⃒⃒⃒⃒
𝑥=𝐿= 0,(15)
where the Reynolds number 𝑅𝑒= 1/𝜈and𝑡0= exp( 𝑅𝑒/8). This solution is given by,
𝑢(𝑥, 𝑡) =𝑥/(𝑡+ 1)
1 +√︁
𝑡+1
𝑡0exp(︁
𝑅𝑒𝑥2
4𝑡+4)︁.(16)
However, when the discrete version of the above equation 14 is solved numerically, truncation and round-off
errors occur and the numerical solution incurs discretization errors [6; 31].
Numerics. We solve the Burgers equation (14) numerically with the following schemes: 1 𝑠𝑡order accurate
upwind for the advection term, 2𝑛𝑑order accurate central-difference for the diffusion term, and Vode scheme
for adaptive time-stepping. Thus, the leading order truncation error term is given by, −Δ𝑥
2𝑢𝜕2𝑢
𝜕𝑥2+𝒪(Δ𝑥2),
where Δ 𝑥is the uniform grid-spacing. The terms in 𝒪(Δ𝑥2) contain spatial derivatives of order 3 and above.
Comparison LF-HF: A comparison of the analytical (equation 16) and numerical solution of the Burgers
equation is provided in Figure 3. One can clearly notice the effects of numerical diffusion and the error in
the location of the shock peak at later times due to truncation errors.
3.2.1 Learning interpretable truncation errors and nonlinear flux corrections
Training: NN architecture, data, and loss function. First, we only consider a Markovian closure term based
on a library composed of second-degree combinations of 𝑢,𝜕𝑢
𝜕𝑥, and𝜕2𝑢
𝜕𝑥2. The library explicitly omits 𝑢𝜕𝑢
𝜕𝑥
because it is already known as part of the governing equation, and 𝑢2because it cannot be part of truncation
error due to the absence of any derivative. Hence, the Markovian closure term is assumed to be a linear
combination of{Δ𝑥(︀𝜕𝑢
𝜕𝑥)︀2,Δ𝑥3(︁
𝜕2𝑢
𝜕𝑥2)︁2
,Δ𝑥2(︁
𝜕𝑢
𝜕𝑥𝜕2𝑢
𝜕𝑥2)︁
,Δ𝑥(︁
𝑢𝜕2𝑢
𝜕𝑥2)︁
}, of which the fourth term is true but
unknown leading order truncation error term itself, and other terms are informed but still expected to be
redundant. Each of the terms is multiplied with appropriate powers of Δ 𝑥, such that the closure terms are
dimensionally consistent with the other terms of the Burgers equation. The 4𝑡ℎorder accurate central and
upwind finite-difference schemes [47] are used to compute the spatial derivatives in the Markovian closure,
so as to eliminate additional sources of truncation error from our analysis. The training data consists of
the analytical solution up until 𝑇= 4.0 solved in a domain of length 𝐿= 1.25 and saved at every 0.01
10Figure 3: Comparison of the numerical solution of the Burgers equation (with 𝑅𝑒= 1000) on a low-resolution
grid (equations 14 & 15; low-fidelity model; middle plot ), with its corresponding analytical solution (equation 16;
high-fidelity model; left plot ). The low-fidelity model is solved on a grid with 𝑁𝑥= 50 grid points, and the absolute
difference between the two solutions is provided in the right plot . We also provide a pair of time-averaged errors,
specifically: root-mean-squared-error (RMSE); and RMSE considering only the grid points where the error is at least
2% of the maximum velocity value, denoted by RMSE( >2%).
time-intervals, for three different combinations of 𝑁𝑥(number of grid points in 𝑥−direction) and 𝑅𝑒. The
chosen ( 𝑁𝑥,𝑅𝑒) pairs,{(100,50),(150,750),and (200 ,1250)}, are such that the −Δ𝑥
2𝑢𝜕2𝑢
𝜕𝑥2term is really the
leading source of error. In every epoch, we parse through the training data of each of these pairs, selected
in random order by sampling without replacement. We tune the hyperparameters based on performance in
the training period (0 .0≤𝑡≤4.0) and the validation period (4 .0≤𝑡≤6.0), and these are provided in
Section SI-2.2. The Markovian closure model is a simple neural network with no hidden layers and only
linear activation in the output layer, in-effect equivalent to a linear combination of the inputs. Given the
analytical solution, {𝑢𝑡𝑟𝑢𝑒(𝑥, 𝑇𝑖),0≤𝑥≤𝐿}𝑀
𝑖=1, the loss function is once again the time and space averaged
mean-absolute-error (MAE), ℒ=1
𝑀∑︀𝑀
𝑖=1∫︀𝐿
01
𝐿|𝑢𝑝𝑟𝑒𝑑(𝑥, 𝑇𝑖)−𝑢𝑡𝑟𝑢𝑒(𝑥, 𝑇𝑖)|𝑑𝑥, where 𝑀= 400 is the number
of high-fidelity solution states at different times available for training.
Learning results. We perform 8 repeats of the same experiment with the tuned hyperparameters. The
resulting learned model with the mean and standard deviation of the coefficients is as follows,
ℱ𝑁𝑁(︃
Δ𝑥(︂𝜕𝑢
𝜕𝑥)︂2
,Δ𝑥3(︂𝜕2𝑢
𝜕𝑥2)︂2
,Δ𝑥2(︂𝜕𝑢
𝜕𝑥𝜕2𝑢
𝜕𝑥2)︂
,Δ𝑥(︂
𝑢𝜕2𝑢
𝜕𝑥2)︂
;𝜑)︃
= (0.133±0.017)Δ 𝑥(︂𝜕𝑢
𝜕𝑥)︂2
+ (0.009±0.023)Δ 𝑥3(︂𝜕2𝑢
𝜕𝑥2)︂2
−(0.323±0.022)Δ 𝑥(︂
𝑢𝜕2𝑢
𝜕𝑥2)︂
.(17)
For evaluation, we first compare the performance of this learned gnCM w.r.t. using the true leading truncation
error term (−Δ𝑥
2𝑢𝜕2𝑢
𝜕𝑥2) as the closure itself. For both cases, we evolve the Burgers equation with the respective
closure terms up until 𝑇= 8.0 (beyond training and validation time-periods), for 35 ( 𝑁𝑥, 𝑅𝑒) pairs in the
2D domain spanned by 50 ≤𝑁𝑥≤200 and 50≤𝑅𝑒≤1500. In Figure 4 we provide the RMSE( >2%)
error (see Figure 3 for description). When the true leading truncation error term is used as the closure,
we find that increasing 𝑅𝑒and lowering 𝑁𝑥values leads to instabilities in the solution which causes it to
explode. On the contrary, in the learned gnCM case, even though it was not shown any training data in
the high 𝑅𝑒and low 𝑁𝑥regime, it still provides a stable solution, and, on average, performs better than its
counterpart in the other regions of the ( 𝑁𝑥, 𝑅𝑒) domain. To interpret the learned closure further, we rewrite
11it by substituting,𝜕
𝜕𝑥(︀
𝑢𝜕𝑢
𝜕𝑥)︀
=(︀𝜕𝑢
𝜕𝑥)︀2+(︁
𝑢𝜕2𝑢
𝜕𝑥2)︁
in equation 17,
ℱ𝑁𝑁(︃
Δ𝑥(︂𝜕𝑢
𝜕𝑥)︂2
,Δ𝑥3(︂𝜕2𝑢
𝜕𝑥2)︂2
,Δ𝑥2(︂𝜕𝑢
𝜕𝑥𝜕2𝑢
𝜕𝑥2)︂
,Δ𝑥(︂
𝑢𝜕2𝑢
𝜕𝑥2)︂
;𝜑)︃
= (0.133±0.017)Δ 𝑥𝜕
𝜕𝑥(︂
𝑢𝜕𝑢
𝜕𝑥)︂
+ (0.009±0.023)Δ 𝑥3(︂𝜕2𝑢
𝜕𝑥2)︂2
−(0.456±0.012)Δ 𝑥(︂
𝑢𝜕2𝑢
𝜕𝑥2)︂
.(18)
Thus, the learned gnCM contains the Δ 𝑥(︁
𝑢𝜕2𝑢
𝜕𝑥2)︁
term with a coefficient of correct sign but slightly smaller
value – in absolute value – in comparison to that of the true leading truncation error term. Along with
that, the other significant term, Δ 𝑥𝜕
𝜕𝑥(︀
𝑢𝜕𝑢
𝜕𝑥)︀
, corresponds to a first-order Taylor series correction to the
nonlinear advection term, and can help with mitigating the resolution error highlighted earlier. Finally, it
is remarkable that the important Δ 𝑥𝜕
𝜕𝑥(︀
𝑢𝜕𝑢
𝜕𝑥)︀
term was missing from the input features; however, to our
surprise, it is still accounted for indirectly in the learned closure, utilizing the redundant terms present in
the input feature library. This highlights a noteworthy learning capability of the gnCM.
(a)Leading truncation error term as closure
 (b)gnCM with only Markovian closure term
(c)Smagorinsky closure
 (d)gnCM
Figure 4: Performance of four closure models for the Burgers equation (equations 14 & 15) evaluated for various
(𝑁𝑥, 𝑅𝑒) pairs in the 2D domain spanned by 50 ≤𝑁𝑥≤200 and 50 ≤𝑅𝑒≤1500. The error provided is the
𝑅𝑀𝑆𝐸 (>2%) (see Figure 3 for description) computed w.r.t. the corresponding analytical solutions (equation 16)
for 0.0≤𝑡≤8.0 in a domain of length 𝐿= 1.25.(a): Leading truncation error term, −Δ𝑥
2𝑢𝜕2𝑢
𝜕𝑥2, as closure. The
white region in the top-left denotes an unconverged numerical solution; (b):Learned gnCM with only the Markovian
term, with the three red⋆’s marking the ( 𝑁𝑥, 𝑅𝑒) pairs used as training data.; (c): Smagorinsky LES model with
𝐶𝑠= 1.0;(d): Learned gnCM with both Markovian and non-Markovian closure terms, with the four red⋆’s marking
the (𝑁𝑥, 𝑅𝑒) pairs used as training data.
123.2.2 Learning generalizable and interpretable closures
Training: NN architecture, data, and loss function. Keeping the Markovian closure term formulation of
Section SI-2.2, we now add the non-Markovian closure term with inputs, {𝑢,𝜕𝑢
𝜕𝑥,𝜕2𝑢
𝜕𝑥2, 𝜈,Δ𝑥}, discretized
using 4𝑡ℎorder finite-difference schemes, and the deep-NN architecture given in Table SI-1. We utilize a
fully-connected deep-NN with four hidden-layers and the non-linear swish activation. The output of the NN
is multiplied with |𝑢|to ensure that the contribution of the non-Markovian closure term is zero in the right-
hand parts of the domain where the shock is yet to reach. As the non-Markovian closure term is nonlinear,
we do not explicitly make the inputs dimensionally consistent with other terms in the Burgers equation.
The overall training and evaluation setup are as in Section 3.2.1, however, this time four pairs of ( 𝑁𝑥, 𝑅𝑒)
are used such that all four combinations of high and low 𝑁𝑥and𝑅𝑒are contained in the training data.
The chosen pairs were, {(50,750),(200,750),(50,1250) ,(200,1250)}. The tuned set of hyperparameters is
provided in Section SI-2.2. The time-delay, 𝜏= 0.075, is based on the optimal-time delay established for the
Burgers equation experiments in [23].
Learning results. We perform 7 repeats of the experiment with exactly the same set of tuned hyperpa-
rameters. The learned coefficients for the Markovian term are different than those in equation 17 due to the
presence of the non-Markovian term, however, once again, the most weightage is given to the Δ 𝑥(︀𝜕𝑢
𝜕𝑥)︀2and
Δ𝑥(︁
𝑢𝜕2𝑢
𝜕𝑥2)︁
terms. Upon inspection, the weights of the input layer of the deep-NN in the non-Markovian
term being multiplied with 𝜈were consistently found to be particularly small ( 𝒪(10−4)), indicating that the
learned closure is independent of 𝜈. For one of the experiment runs, we show in Figure 4 the performance for
(𝑁𝑥, 𝑅𝑒) pairs in the 2D domain spanned by 50 ≤𝑁𝑥≤200 and 50≤𝑅𝑒≤1500, and compare it with that
of the popular Smagorinsky model used for subgrid-scale turbulence closure in large eddy simulations (LES).
To the Burgers equation (14), this model introduces a dynamic turbulent eddy viscosity ( 𝜈𝑒) resulting in,
𝜕𝑢
𝜕𝑡=−𝑢𝜕𝑢
𝜕𝑥+𝜈𝜕2𝑢
𝜕𝑥2+𝜕
𝜕𝑥(︂
𝜈𝑒𝜕𝑢
𝜕𝑥)︂
, (19)
where 𝜈𝑒= (𝐶𝑠Δ𝑥)2⃒⃒𝜕𝑢
𝜕𝑥⃒⃒and𝐶𝑠is the Smagorinsky constant. As the rectangle formed by the training
(𝑁𝑥, 𝑅𝑒) pairs is only a subset of the rectangle in which we evaluate the learned closure, we are testing both
interpolation and extrapolation performance w.r.t. changing the physical parameter governing the model
and grid resolution. We find that the learned gnCM clearly outperforms the Smagorinsky model. It should
be noted, that in Figure 4d, the bottom-right corner (low 𝑅𝑒and high 𝑁𝑥region) has inherently small errors
even without the presence of a closure. Further, the amount of error between low-fidelity and high-fidelity
solutions is different for the four training data ( 𝑁𝑥, 𝑅𝑒) combinations; for example, (50 ,1250) (coarsest
resolution, higher Re) should incur the most error. Thus, we notice a differential in the impact of learned
gnCM in reducing the error at and around different training data ( 𝑁𝑥, 𝑅𝑒) pairs.
As claimed earlier, we expect the learned gnCM to be also generalizable over different boundary condi-
tions. We tested this by modifying the boundary conditions. The analytical solution (equation 16) used in
training corresponded to Neumann boundary conditions on the right edge of the domain. This was changed
to a zero Dirichlet boundary condition. Furthermore, the length of the domain was decreased to 𝐿= 1, and
𝑁𝑥= 50 number of equally-spaced grid points were used in our low-fidelity model with 𝑅𝑒= 1000. Since
no closed-form analytical solution exists for the Dirichlet boundary conditions case, we solve the system
with 𝑁𝑥= 1000 grid points and use that as the true solution for comparing the performance of our learned
closure. In Figure 5, we find that the learned gnCM is able to keep the errors remarkably low throughout
the time period encompassing training, testing, and prediction.
Sensitivity. In general, the quality of learning was less sensitive to the batch-time hyperparameter,
however, higher values led to more interpretable closures. Using lower-order finite-difference schemes for the
closure inputs did not compromise the performance of the learned closures, however, it did lead to a decrease
in interpretability. Sensitivity to other hyperparameters was similar to that observed in Experiments-1a.
3.3 Experiments 2a: Ocean Acidification - Interpretable Model Discrimination
In the third set of experiments, we consider coupled physical-biological-carbonate PDE models that are
used to study ocean acidification (OA). We utilize the Markovian neural closure model to interpretably
discriminate between candidate functional forms of uncertain Zooplankton mortality term.
13(a)No closure
(b)gnCM
Figure 5: Solution of the Burgers equation with and without the learned generalized neural closure model ( gnCM)
for𝑅𝑒= 1000, a low-resolution grid ( 𝑁𝑥= 50), and zero Dirichlet boundary condition on the right edge. For each
case, we also provide the pair of time-averaged errors (see Figure 3 for description).
Setup: True model, data generation, and low-fidelity model. OA models are used to study and predict
essential carbonate chemistry and biological production cycles, and their interplay with global warming. A
plethora of biogeochemical models have been proposed. They differ in their complexity, or ability to resolve
different biological processes. A set of parameter values and functional forms that might work in a particular
ocean region may not apply anywhere else. As an additional source of complexity, there may be seasonal
variability in these functional forms [32; 58; 24].
For this set of experiments, the high-fidelity model is similar to the Hadley Centre Ocean Carbon Cycle
(HadOCC) model [43], where the biological part is a modified version of four components (nutrients (N),
phytoplankton (P), zooplankton (Z), and detritus (D)) developed in [57] for the Gulf of Maine, along with
dissolved inorganic carbon (DIC) and total alkalinity (TA) for the carbonate part. The NPZD model is,
𝑑𝑁
𝑑𝑡=−𝑈𝑃+𝜆𝐺𝑍+𝜀𝐷 ,
𝑑𝑃
𝑑𝑡=𝑈𝑃−𝐺𝑍−𝑚𝑃𝑃 ,
𝑑𝑍
𝑑𝑡=𝛾𝐺𝑧−𝑀𝑍(𝑍),
𝑑𝐷
𝑑𝑡= (1−𝛾−𝜆)𝐺𝑍+𝑚𝑃𝑃+𝑀𝑍(𝑍)−𝜀𝐷 ,(20)
where 𝑈𝑃is the phytoplankton growth, regulated by nitrogen limitation based on Michaelis-Menten kinetics
14(𝑓(𝑁)) and photosynthetically active radiation ( 𝑓(𝐼)), and 𝐺𝑍the zooplankton grazing, each given by,
𝑈𝑃=𝜇𝑚𝑎𝑥𝑓(𝑁)𝑓(𝐼)𝑃, 𝑓 (𝑁) =𝑁
𝑁+𝐾𝑁,
𝑓(𝐼) = (1−exp(𝛼𝐼/𝜇 𝑚𝑎𝑥)) exp(−𝛽𝐼/𝜇 𝑚𝑎𝑥)
𝐼(𝑧) =𝐼0exp(−𝑘𝑊𝑧), 𝐺 𝑍=𝑔𝑚𝑎𝑥𝑍𝑃2
𝑃2+𝐾2
𝑃,(21)
and𝑀𝑍(𝑍) is the to-be-learned zooplankton mortality. In these equations, the concentration of biological
variables is measured in nitrogen ( 𝑚𝑚𝑜𝑙 𝑁 𝑚−3),𝑧is depth, and the other parameters are: 𝜇𝑚𝑎𝑥, maximum
growth rate of phytoplankton; 𝐾𝑁, half-saturation constant; 𝛼and𝛽, light-growth slope and inhibition
coefficient; 𝐼0, photosynthetically active radiation (PAR) at the sea surface; 𝑘𝑊, attenuation coefficient of
water; 𝑔𝑚𝑎𝑥, zooplankton maximum grazing rate; 𝐾𝑃, half-saturation constant for zooplankton grazing; 𝛾,
assimilation coefficient; 𝑚𝑧, zooplankton mortality coefficient; 𝑚𝑝, phytoplankton mortality coefficient; 𝜆,
active respiration zooplankton expressed as a fraction of grazing; and 𝜀, remineralization rate of detritus.
The carbon in the system is coupled with the nitrogen by fixed carbon-nitrogen ratios, 𝐶𝑃,𝐶𝑍, and 𝐶𝐷,
𝑑(𝐷𝐼𝐶 )
𝑑𝑡=−𝐶𝑃𝑑𝑃
𝑑𝑡−𝐶𝑍𝑑𝑍
𝑑𝑡−𝐶𝐷𝑑𝐷
𝑑𝑡−𝛾𝑐𝐶𝑃𝑈𝑃,
𝑑(𝑇𝐴)
𝑑𝑡=−1
𝜌𝑤𝑑𝑁
𝑑𝑡−2𝛾𝑐𝐶𝑃𝑈𝑃
𝜌𝑤,(22)
and neither DIC nor TA has any effect on the biology because phytoplankton growth is not carbon limited.
The last term in the DIC equation represents the precipitation of calcium carbonate to form shells and other
hard body parts, which subsequently sink below the euphotic zone, also known as “hard flux”. This flux is
modeled to be proportional (and additional) to the uptake of carbon for primary production. The chemistry
dictates the decrease in total alkalinity by two molar equivalents for each mole of carbonate precipitated. In
general, since TA is measured in 𝑚𝑚𝑜𝑙 𝐶 𝑘𝑔−1(or𝜇𝑚𝑜𝑙 𝐶 𝑘𝑔−1), we divide the right-hand-side (RHS) of the
TA equation by the density of sea-water ( 𝜌𝑤). Moreover, the units of DIC concentration are 𝑚𝑚𝑜𝑙 𝐶 𝑚−3.
The above biological and carbonate models are often coupled with physical models to introduce both
spatial and temporal components. For our experiments, we use a 1-D diffusion-reaction PDE with vertical
eddy mixing parameterized by the operator 𝜕/𝜕𝑧 (𝐾𝑧(𝑧, 𝑀)𝜕/𝜕𝑧 (∙)), where 𝐾𝑧is a dynamic eddy diffusion
coefficient. A mixed layer of varying depth ( 𝑀=𝑀(𝑡)) is used as a physical input to the OA models. Thus,
each biological and carbonate state variable 𝐵(𝑧, 𝑡) is governed by the following non-autonomous PDE,
𝜕𝐵
𝜕𝑡=𝑆𝐵+𝜕
𝜕𝑧(︂
𝐾𝑧(𝑧, 𝑀(𝑡))𝜕𝐵
𝜕𝑧)︂
, (23)
𝐾𝑧(𝑧, 𝑀(𝑡)) =𝐾𝑧𝑏+(𝐾𝑧0−𝐾𝑧𝑏)(arctan(−𝛾𝑡(𝑀(𝑡)−𝑧))−arctan(−𝛾𝑡(𝑀(𝑡)−𝐷𝑧)))
arctan(−𝛾𝑡𝑀(𝑡))−arctan(−𝛾𝑡(𝑀(𝑡)−𝐷𝑧)), (24)
where 𝐾𝑧𝑏and𝐾𝑧0are the diffusion at the bottom and surface, respectively, 𝛾𝑡is the thermocline sharpness,
and𝐷𝑧is the total depth. The 1-D model and parameterizations are adapted from [11] and [41]. They
simulate the seasonal variability in upwelling, sunlight, and biomass vertical profiles. The dynamic mixed
layer depth, surface photosynthetically-available radiation 𝐼0(𝑡), and biomass fields 𝐵(𝑧, 𝑡) are shown in
Figure 6. The radiation 𝐼0(𝑡) and total biomass concentration, 𝑇𝑏𝑖𝑜(𝑧, 𝑡), affects 𝑆𝐵and the initial conditions.
To generate data, we first initialize the 𝑁state with the depth-varying total biomass concentration and
the𝑃,𝑍, and 𝐷states with zero concentrations, and then run a one-month spin-off of just the NPZD
model without the diffusion term and a constant sea-surface solar radiation in order to determine the stable
equilibrium of the biological states. These equilibrium states form the initial conditions for the respective
states in the NPZD-OA model. To initialize 𝐷𝐼𝐶 , we multiply the equilibrium state for 𝑁with the nitrogen-
to-carbon ratio that is considered nearly equal to the value of 𝐶𝑃.𝑇𝐴is often assumed to have a dependence
on salinity and biological processes [1]. The contribution from salinity ( 𝑆in𝑃𝑆𝑈 ) is modeled using a linear
relationship optimized for the Gulf of Maine, 𝑇𝐴={︃
(198.10 + 61 .75𝑆)/1000 , 𝑆 < 32.34
(744.41 + 44 .86𝑆)/1000 , 𝑆≥32.34(Dr. P.J. Haley
15Jr.,pers. comm. ), while the biological impact is given by equation 22. We assume a stationary salinity profile
described using a sigmoid function 𝑆(𝑧) =𝐴+𝐾−𝐴
(𝐶+𝑄exp(−𝐵𝑧))1/𝜈with 𝐴= 31 .4𝑃𝑆𝑈 ,𝐾= 32 .8𝑃𝑆𝑈 ,
𝐶= 1.0,𝑄= 0.5,𝐵= 0.25, and 𝜈= 2.0. Thus, we can initialize TA based on salinity and evolve it using
equation 22 coupled with equation 23.
For the low-fidelity model, we assume that we have only prior knowledge about the existence of a linear
zooplankton mortality term, i.e., 𝑀𝑍(𝑍) =𝑚𝑍
2𝑍. For the high-fidelity model, however, the true zooplankton
mortality contains an additional quadratic dependence, i.e., 𝑀𝑍(𝑍) =𝑚𝑍
2(𝑍+𝑍2).
Numerics. We use a 2𝑛𝑑order central difference scheme for the spatial discretization ( 𝑁𝑧= 20), and
dopri5 [25] scheme for time integration with adaptive time-stepping. Comparison LF-HF: In Figure 6-left-
and -mid-columns, we provide a year-long simulation for the NPZD-OA model with quadratic (truth) and
linear (prior) 𝑍mortality terms, respectively. We notice the low 𝑍concentration and enhanced 𝑃bloom in
the former case. Figure 6-right-column provides the absolute difference between the two cases. Values of the
model parameters are provided in section SI-2.
Training: NN architecture, data, and loss function. For the gnCM – we only consider the Markovian
term – belonging to a linear combination of a library of popular mortality functions [16], {𝑍, 𝑍2,𝑍2
1+𝑍,exp𝑍}.
Compared to the true zooplankton mortality term, our library contains three superfluous or redundant terms,
𝑍,𝑍2
1+𝑍, and exp 𝑍, noting that the 𝑍term is already a part of the low-fidelity model and completely known.
For the Markovian term, we use again a simple NN with no hidden layers and linear activation in the
output layer. Using weight constraints for the output layer, we enforce biomass conservation in the 𝑁,𝑃,
𝑍, and 𝐷equations and couple with 𝐷𝐼𝐶 and𝑇𝐴equations as in the known system (equations 20 and 22).
Architectural details are given in table SI-1 and the tuned set of training hyperparameters in section SI-2.2.
The training data consists of the true/high-fidelity model solution sampled at time intervals of 0.1 day, until
𝑡=30 days,{{𝐵𝑡𝑟𝑢𝑒(𝑧, 𝑇𝑖)}𝐵∈{𝑁,𝑃,𝑍,𝐷,𝐷𝐼𝐶,𝑇𝐴 }}𝑀
𝑖=1, i.e., a 𝑀= 300 high-fidelity solution states. We use
an MAE-based loss function, ℒ=1
𝑀∑︀𝑀
𝑖=1∫︀𝐷
01
𝐷√︁∑︀
𝐵∈{𝑁,𝑃,𝑍,𝐷,𝐷𝐼𝐶,𝑇𝐴 }1
𝜎𝐵|𝐵𝑝𝑟𝑒𝑑(𝑧, 𝑇𝑖)−𝐵𝑡𝑟𝑢𝑒(𝑧, 𝑇𝑖)|𝑑𝑧.
Here, 𝜎𝐵’s are hyperparameters to scale the importance of different state variables based on their magnitudes.
After multiple hyperparameter tuning experiments, values of 𝜎𝑁= 1, 𝜎𝑃= 0.25, 𝜎𝑍= 1, 𝜎𝐷= 1, 𝜎𝐷𝐼𝐶=
2, 𝜎𝑇𝐴= 0.1, were found to aid in learning.
Learning results. In 7 repeats of the experiment with exactly the same hyperparameters, the learned mod-
els consisted of no contribution of the closure to the 𝑁,𝑃, and 𝑇𝐴equations, while for the 𝑍,𝐷, and 𝐷𝐼𝐶
equations the contributions were found – with mean and standard deviation – to be ( −0.02996±0.00014) 𝑍2,
(0.03001±0.00013) 𝑍2, and (−0.05603±0.00136) 𝑍2, respectively. For reference, the true contribution of the
zooplankton quadratic mortality term to the 𝑍,𝐷, and 𝐷𝐼𝐶 equations are given as −0.02998 𝑍2, 0.02998 𝑍2,
and−0.05621 𝑍2, respectively.
Sensitivity. Multiple experiments were done to study the effects of hyperparameters, such as batch-
time, batch-size, regularization factors, etc., and the convergence to the true model was the most severely
compromised when increasing batch-time and changing the loss-scaling for individual state variables.
3.4 Experiments 2b: Ocean Acidification - Model Complexity
In the last set of experiments, we again consider the coupled physical-biological-carbonate PDE models,
however, this time we utilize the full generalized neural closure model to augment a simpler model obtained
by aggregation of components and other simplifications of processes and parameterizations, such that, it
becomes as accurate as the more complex model. Simultaneously, we also discriminate between the candidate
functional forms of the uncertain Zooplankton mortality term in an interpretable fashion.
Setup: True model, data generation, and low-fidelity model. The high-fidelity model and the data are
those used in Experiments-2a (section 3.3), where we model the intermediate state of detritus, thus capturing
processes such as remineralization and quadratic zooplankton mortality, i.e., 𝑀𝑍(𝑍) =𝑚𝑍
2(𝑍+𝑍2). The
16Figure 6: Solutions (in each column, concentration profiles of 𝑁,𝑃,𝑍,𝐷in𝑚𝑚𝑜𝑙 𝑁 𝑚−3,𝐷𝐼𝐶 in𝑚𝑚𝑜𝑙 𝐶 𝑚−3,
and𝑇𝐴in𝑚𝑚𝑜𝑙 𝐶 𝑘𝑔−1, all vs. time in days) of the OA model used in Experiments-2a, corresponding to different
functional forms for the zooplankton mortality term. Left-column: The top panel shows the yearly variation of solar
radiation and the subsequent panels depict the states from the NPZD-OA model with 𝑀𝑍(𝑍) =𝑚𝑍
2(𝑍+𝑍2) (ground
truth), overlaid with the dynamic mixed layer depth in dashed red lines; Middle-column: States from the NPZD-OA
model with 𝑀𝑍(𝑍) =𝑚𝑍
2𝑍(low-fidelity); Right-column: Absolute difference between the corresponding states in the
left- and middle- column. For each case, we also provide the pair of time-averaged errors (see Figure 3 for description).
17low-fidelity model is the less complex three-component NPZ model,
𝑑𝑁
𝑑𝑡=−𝑈𝑃+ (1−𝛾)𝐺𝑍+𝑚𝑃𝑃+𝑚𝑍
2𝑍 ,
𝑑𝑃
𝑑𝑡=𝑈𝑃−𝐺𝑍−𝑚𝑃𝑃 ,
𝑑𝑍
𝑑𝑡=𝛾𝐺𝑧−𝑚𝑍
2𝑍 ,(25)
coupled with the carbonate system using fixed carbon-nitrogen ratios, 𝐶𝑃, and 𝐶𝑍,
𝑑(𝐷𝐼𝐶 )
𝑑𝑡=−𝐶𝑃𝑑𝑃
𝑑𝑡−𝐶𝑍𝑑𝑍
𝑑𝑡−𝛾𝑐𝐶𝑃𝑈𝑃,
𝑑(𝑇𝐴)
𝑑𝑡=−1
𝜌𝑤𝑑𝑁
𝑑𝑡−2𝛾𝑐𝐶𝑃𝑈𝑃
𝜌𝑤,(26)
and with the 1-D diffusion-reaction PDE 23. The goal of these experiments is to use the gnCM to simulta-
neously learn the functional form of the zooplankton mortality term using the Markovian closure term, and
account for the missing intermediate state of detritus through the non-Markovian closure term.
Numerics. The numerical schemes used are as those of Experiments-2a in section 3.3. Comparison LF-
HF:Since the high-fidelity NPZD-OA model resolves more processes, the concentrations of 𝑁+𝐷(aggregated
state), 𝑃,𝑍,𝐷𝐼𝐶 , and 𝑇𝐴differ significantly from the 𝑁,𝑃,𝑍,𝐷𝐼𝐶 , and 𝑇𝐴of the low-fidelity NPZ-OA
model, as shown in Figure 7.
Training: NN architecture, data, and loss function. Our Markovian closure consists of a linear combi-
nation of a library of popular mortality functions [16], {𝑍, 𝑍2,𝑍2
1+𝑍,exp𝑍}. Once again, compared to the
true zooplankton mortality term, our library contains three redundant terms, 𝑍,𝑍2
1+𝑍, and exp 𝑍, where the
𝑍term is already a part of the low-fidelity model and completely known. Additionally, we use a deep-NN
for the non-Markovian closure term, with 𝑁(𝑧, 𝑡),𝑃(𝑧, 𝑡),𝑍(𝑧, 𝑡), and 𝐼(𝑧, 𝑡) as the input; the inclusion of
the photosynthetically active radiation, 𝐼(𝑧, 𝑡), makes this closure term non-autonomous. The architecture
for the fully-connected deep-NN used in the non-Markovian closure term is provided in table SI-1, and it
consists of two hidden-layers with the non-linear swish activation. We do not include the states 𝐷𝐼𝐶 (𝑧, 𝑡)
and𝑇𝐴(𝑧, 𝑡) among the inputs in order to preserve one-way coupling between the biological and carbonate
system. Along with this, biomass conservation and coupling of the carbonate system by nitrogen conversion
(as in equations 25 and 26) is maintained in the non-Markovian closure terms by manipulating the channels
of the output layer. On the other hand, in the Markovian layer, these constraints are imposed by constraining
the weights of the output layer. To help with learning, we further impose the condition that the contribution
of the Markovian closure term to the 𝑃equation is exactly equal to zero. See table SI-1 for implementational
details of these constraints.
The training data consists of solving the NPZD-OA model with 𝑀(𝑍) =𝑚𝑍
2(𝑍+𝑍2), and the solution
sampled at time intervals of 0.1 day until 𝑡= 60 𝑑𝑎𝑦𝑠,{{𝐵𝑡𝑟𝑢𝑒(𝑧, 𝑇𝑖)}𝐵∈{𝑁+𝐷,𝑃,𝑍,𝐷𝐼𝐶,𝑇𝐴}}𝑀
𝑖=1, i.e., 𝑀= 600
high-fidelity solution states at different times. Performance of the learned model in the validation interval
of 60 days≤𝑡≤120 days is used to tune the hyperparameters, provided in section SI-2.2. We again use a
MAE based loss function, ℒ=1
𝑀∑︀𝑀
𝑖=1∫︀𝐷
01
𝐷√︁∑︀
𝐵∈{𝑁,𝑃,𝑍,𝐷𝐼𝐶,𝑇𝐴}1
𝜎𝐵|𝐵𝑝𝑟𝑒𝑑(𝑧, 𝑇𝑖)−𝐵𝑡𝑟𝑢𝑒(𝑧, 𝑇𝑖)|𝑑𝑧, with
𝜎𝑁= 1, 𝜎𝑃= 0.25, 𝜎𝑍= 1, 𝜎𝐷𝐼𝐶 = 2, 𝜎𝑇𝐴= 0.1 (similar to those used in experiments-2a). A time
delay of 𝜏= 2.5𝑑𝑎𝑦𝑠 was used for the non-Markovian closure term based on the optimal delay value study
performed in [23].
Learning results. In 9 repeats of the experiment with exactly the same set of hyperparameters, the mean
and standard deviation of the learned contribution of the Markovian closure term to the 𝑍equation is given
by, (−0.03000±0.00067) 𝑍2. For reference, the true contribution of the quadratic mortality term to the 𝑍
equation is−0.02998 𝑍2. Due to the weight constraints, the contribution of the Markovian closure term to
other equations is exactly zero. We evaluate the performance of the learned neural closure model for long
predictions, spanning over 1 year (365 𝑑𝑎𝑦𝑠). The comparison with true/high-fidelity data for one of the
experiments is provided in Figure 7. Overall, the learned closure keeps the errors low throughout the 1-year
time period, apart from a slight increase observed for the OA states after ∼200𝑑𝑎𝑦𝑠.
18Figure 7: Comparison of the OA models used in Experiments-2b with and without closure models. The parameter
values and concentration units are as in Figure 6. For the gnCM, the training period is from t = 0 to 60 𝑑𝑎𝑦𝑠, the
validation period from t = 60 to 120 days, and the future prediction period from t = 120 to 364 days. Left-column:
The top panel shows the yearly variation of solar radiation and the subsequent panels depict the aggregated states
from the NPZD-OA model with 𝑀𝑍(𝑍) =𝑚𝑍
2(𝑍+𝑍2) (ground truth), overlaid with the dynamic mixed layer depth
in dashed red lines. Middle-column: Absolute difference between the corresponding states from the NPZ-OA model
with𝑀𝑍(𝑍) =𝑚𝑍
2𝑍(low-fidelity) and those in the left-column (high-fidelity ground truth). Right-column: Absolute
difference between the corresponding states from the low-fidelity model augmented with the learned gnCM and the
ground truth. For each case, we also provide the pair of time-averaged errors (see Figure 3 for description).
Sensitivity. Multiple experiments were done to study the effects of hyperparameters, such as batch-
time, batch-size, regularization factors, etc., and their effects were similar to those observed in previous
experiments. However, when using larger neural network architectures for the non-Markovian term, this
19led to high variability in the learned coefficients of the Markovian term on repeats of the experiments with
the same set of hyperparameters. This is probably because of the increased expressive power of the non-
Markovian term, which overshadows the significance of the learned Markovian term.
3.5 Remarks and Discussion
Computational advantages. In [23], through a flop-count analysis, we proved that the additional
computational cost due to the presence of neural closure models is of similar or lower complexity than the
existing low-fidelity model. However, in our current generalized framework, we have additional computational
advantages. First, the size of the neural network architecture is completely independent of the number of
discretized state variables and only dictated by the number of local features to be used as inputs to the
gnCM terms. Second, as the same neural networks are applied locally at every grid point, it is directly
possible to use batches of the size of the number of grid points. It has been reported that larger batch sizes
could lead to performance speed-ups in forward pass through neural networks during the inference stage
[27]. Estimating the leading flop-count order for training is non-trivial due to the presence of a number of
operations ranging from time-integration of the forward model and adjoint PDEs; automatic differentiation
through the neural networks; creation and use of interpolation functions; the integral to compute the final
derivatives; the gradient descent step, etc. All these operations lead to training costs that are non-negligible.
However, the generalizability and interpretability of our learned gnCMs over boundary conditions, initial
conditions, domain, problem-specific parameters, etc., help justify the one-time training cost.
Lack of prior knowledge. As showcased in the prior experiments and summarized in the corresponding
sensitivity studies, the lack of prior knowledge about the missing dynamics could manifest in many different
ways. This includes no known low-fidelity model, dynamics of the known low-fidelity model very different
from the high-fidelity model/data, no knowledge of potential candidate terms to create input function li-
braries, or even no information on the most relevant state variables themselves. To allow compensation for
this lack of prior knowledge, our gnCM framework is derived and implemented for any deep-neural-network
(DNN) architectures for both Markovian and non-Markovian closure terms (Figure 1). Our flexible mod-
eling framework provides full autonomy for the design of the unknown closure terms such as using linear-,
shallow-, or deep-NNs, selecting the span of the function libraries, and using either or both Markovian and
non-Markovian closure terms. All these decisions are made by the subject matter expert/user depending on
the problem at hand. For example, in all our experiments, fully-connected deep-NNs were utilized for the
non-Markovian closure terms, because in general, no prior knowledge is available for the same. Further, our
framework could be extended to allow for the adaptive increase of the input function library, for example
using the algorithm proposed in [28].
Non-Markovian term. In the current derivation of the gnCM framework, due to the mathematical
constructs, the non-Markovian term does not account for the possibility of memory decay contribution of the
𝒟𝑁𝑁(∙) function under the integral w.r.t. 𝑡−𝑠in equation 1. However, memory decay or other variations
can be a desired property for some problems. To allow for this, one can split the integral in equation 1 into
contiguous parts and multiply each of them with different weights. An alternate option is to consider discrete
delays utilizing recurrent neural networks as for the discrete-nDDEs in [23] and so implicitly incorporate the
desired memory decay. In general, the need for the non-Markovian closure term for a given problem should be
determined by the subject matter expert. However, in many cases, we anticipate the need for non-Markovian
closure term to be imperative, especially when the high-fidelity model/data accounts for intermediate state
variables not modeled in the known low-fidelity model, as in our Experiments-2b (section 3.4). Finally,
it is also desirable to allow learning an adaptive optimal delay ( 𝜏in equation 1), instead of treating it as
a hyperparameter. For such a possibility, we refer to Appendix E in [22] where we derive the theory for
learning the optimal delay for the nCM framework ([23]; DDE counterpart for gnCM).
204 Conclusions
In the present study, we develop neural closure models that are generalizable over computational grid res-
olution, boundary and initial conditions, domain geometries, and problem parameters, and also provide
interpretability. These generalized neural closure models ( gnCMs) are based on neural partial delay differen-
tial equations (nPDDEs) that augment existing/low-fidelity models in their PDE forms with both Markovian
and non-Markovian closures parameterized with deep-NNs. The melding in the continuous spatiotemporal
space is then followed by numerical discretization. This ensures that the burden of generalization, along
with computing the relevant spatial derivatives, is carried by the numerical schemes, and not by the learned
NNs. The space-time continuous form of the gnCMs also makes it easy to interpret the learned closures. For
efficient training, we derive the adjoint PDEs in the continuous form and discretize them with adaptive time-
integration schemes and employ interpolation functions constructed during forward integration to increase
numerical stability and accuracy. This enables implementation across differentiable and non-differentiable
computational physics codes, and different machine learning frameworks, all while being agnostic to the
numerical methods. It further removes any requirements on the availability of regularly spaced training data
in both space and time, and also accounts for errors in the time-evolution of the states in the presence of
NNs during training. Finally, all our derivations and implementations consider deep-NN architectures for
both Markovian and non-Markovian terms, thus automatically encompassing linear- and shallow-NNs, and
providing the user or subject-matter-expert with the flexibility of choosing the architectural complexity in
accord with prior knowledge.
Through a series of four sets of experiments, we demonstrate the interpretability and generalizability of
our learned gnCMs. Our first two sets of simulation experiments are based on advecting nonlinear waves
and shocks governed by the KdV-Burgers and classic Burgers PDEs, where the low-fidelity models are either
missing terms or contain errors due to unresolved subgrid-scale processes. When presented with a function
library containing terms of spatial derivatives of different orders and their combinations, grid-resolution, and
the Reynolds number as inputs to the closure terms, our learned gnCMs eliminate redundant terms and
discover missing physics, leading truncation error terms, and a correction to the nonlinear advection, all in
an interpretable fashion. The correction to the nonlinear advection term, despite being absent from the input
function library, is still accounted for and learned indirectly. Further, by analyzing the deep-NN weights, we
also notice the learned closure terms to be independent of the Reynolds number. We find that training on
data corresponding to just 3 −4 combinations of a number of grid points and Reynolds number is sufficient to
ensure that the learned closures are generalizable over large ranges of grid resolutions and Reynolds numbers,
initial and boundary conditions, and also outperform the popular Smagorinsky closure model. Our last two
sets of experiments are based on one-dimensional, non-autonomous ocean acidification PDE models, that
couple physical, biological, and carbonate states, processes, and interactions. In these experiments, the low-
fidelity models have uncertainty in the functional form of certain biological processes and lack complexity due
to a missing intermediate state variable. The learned gnCMs simultaneously discriminate between candidate
functional forms of the uncertain Zooplankton mortality term with the Markovian part of the closure,
and account for the missing intermediate state and processes with the non-Markovian part. In terms of
computational advantage, our new framework naturally lends itself to batching across computational grid
points during the forward pass through the NNs in the closure terms, thus leading to potential performance
speed-ups.
The gnCMs allow learning both Markovian and non-Markovian closure parameterization with deep-
NNs at the PDE level, thus addressing the issues of generalizability and interpretability that are often the
bottleneck when it comes to using machine learning for computational science and engineering problems.
The generalizability and interpretability properties also make it easier to justify the often computationally
expensive training stage, thus enabling wider adoption.
Data Accessibility. The codes and data used in this work are available in the GitHub repository: https:
//github.com/mit-mseas/generalized_nCMs.git
Author’s Contributions. A.G. conceived the idea of extending the existing neural closure models using
neural partial delay differential equations; augmentation of closure terms in the continuous spatio-temporal
space followed with numerical discretization for generalizability and interpretability; derived the adjoint PDE;
21implemented the neural network architectures and the simulation experiments; interpreted the computational
results; and wrote a first draft of the manuscript. P.F.J.L. supervised the work; provided ideas; interpreted
the results; and edited and wrote significant parts of the manuscript.
Competing Interests. We declare we have no competing interests.
Funding. We are grateful to the Office of Naval Research for partial support under grant N00014-20-1-2023
(MURI ML-SCOPE) to the Massachusetts Institute of Technology.
Acknowledgement. We are grateful to the members of our MSEAS group for their collaboration and
insights, especially Mr. Aman Jalan. We thank the members of our MURI ML-SCOPE research team for
many useful discussions. We also thank the three anonymous reviewers for their useful comments.
References
[1] Y. Artioli, J. C. Blackford, M. Butensch¨ on, J. T. Holt, S. L. Wakelin, H. Thomas, A. V. Borges, and
J. I. Allen. The carbonate system in the north sea: Sensitivity and model validation. Journal of Marine
Systems , 102:1–13, 2012.
[2] G.-J. Both, S. Choudhury, P. Sens, and R. Kusters. Deepmod: Deep learning for model discovery in
noisy data. Journal of Computational Physics , 428:109985, 2021.
[3] P. N. Brown, G. D. Byrne, and A. C. Hindmarsh. VODE: A variable-coefficient ODE solver. SIAM
journal on scientific and statistical computing , 10(5):1038–1051, 1989.
[4] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse
identification of nonlinear dynamical systems. PNAS , 113(15):3932–3937, 2016.
[5] M. B¨ ucker, P. Hovland, et al. Community Portal for Automatic Differentiation. https://www.
autodiff.org/ .
[6] S. C. Chapra and R. P. Canale. Numerical methods for engineers , volume 1221. Mcgraw-hill New York,
2011.
[7] T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations.
InAdvances in neural information processing systems , pages 6571–6583, 2018.
[8] A. J. Chorin, O. H. Hald, and R. Kupferman. Optimal prediction and the Mori–Zwanzig representation
of irreversible processes. PNAS , 97(7):2968–2973, 2000.
[9] D. P. Dauhajre, J. C. McWilliams, and L. Renault. Nearshore lagrangian connectivity: submesoscale
influence and resolution sensitivity. JGR: Oceans , 124(7):5180–5204, 2019.
[10] T. Daulbaev et al. Interpolation technique to speed up gradients propagation in neural ODEs. Advances
in Neural Information Processing Systems , 33, 2020.
[11] M. Eknes and G. Evensen. An ensemble kalman filter with a 1-d marine ecosystem model. Journal of
Marine Systems , 36(1-2):75–100, 2002.
[12] M. Enriquez. The Effects of Coupling Adaptive Time-Stepping and Adjoint-State Methods for Optimal
Control Problems . PhD thesis, Rice University, 2011.
[13] W. Fennel and T. Neumann. Introduction to the Modelling of Marine Ecosystems:(with MATLAB
programs on accompanying CD-ROM) , volume 72 of Oceanography . Elsevier, 2014.
[14] F. Feppon and P. F. J. Lermusiaux. Dynamically orthogonal numerical schemes for efficient stochastic
advection and Lagrangian transport. SIAM Review , 60(3):595–625, 2018. doi: 10.1137/16M1109394.
22[15] F. Feppon and P. F. J. Lermusiaux. The extrinsic geometry of dynamical systems tracking nonlinear
matrix projections. SIAM Journal on Matrix Analysis and Applications , 40(2):814–844, 2019. doi:
10.1137/18M1192780.
[16] P. J. S. Franks. NPZ models of plankton dynamics: their construction, coupling to physics, and
application. Journal of Oceanography , 58(2):379–387, 2002.
[17] A. Geer. Learning earth system models from observations: machine learning or data assimilation?
Philosophical Transactions of the Royal Society A , 379(2194):20200089, 2021.
[18] A. Gholami, K. Keutzer, and G. Biros. Anode: Unconditionally accurate memory-efficient gradients for
neural odes. arXiv preprint:1902.10298 , 2019.
[19] D. S. Glass, X. Jin, and I. H. Riedel-Kruse. Nonlinear delay differential equations and their application
to modeling biological network motifs. Nature communications , 12(1):1–19, 2021.
[20] A. Gouasmi, E. J. Parish, and K. Duraisamy. A priori estimation of memory effects in reduced-order
models of nonlinear systems using the Mori–Zwanzig formalism. Proceedings of the Royal Society A ,
473(2205):20170385, 2017.
[21] A. Griewank. Achieving logarithmic growth of temporal and spatial complexity in reverse automatic
differentiation. Optimization Methods and software , 1(1):35–54, 1992.
[22] A. Gupta. Scientific Machine Learning for Dynamical Systems: Theory and Applications to Fluid Flow
and Ocean Ecosystem Modeling . PhD thesis, Massachusetts Institute of Technology, Department of
Mechanical Engineering, Cambridge, Massachusetts, Sept. 2022.
[23] A. Gupta and P. F. J. Lermusiaux. Neural closure models for dynamical systems. Proceedings of The
Royal Society A , 477(2252):1–29, Aug. 2021. doi: 10.1098/rspa.2020.1004.
[24] A. Gupta and P. F. J. Lermusiaux. Bayesian learning of coupled biogeochemical-physical models.
Progress in Oceanography , 2022. URL https://arxiv.org/abs/2211.06714 . Sub-judice.
[25] E. Hairer, S. P. Norsett, and G. Wanner. Solving ordinary differential equations I . Springer, 1993.
[26] A. Jalan, A. Gupta, and P. F. J. Lermusiaux. Neural closure models for chaotic dynamical systems.
2023. In preparation.
[27] Y. Kochura, Y. Gordienko, V. Taran, N. Gordienko, A. Rokovyi, O. Alienin, and S. Stirenko. Batch
size influence on performance of graphic and tensor processing units during training and inference
phases. In International Conference on Computer Science, Engineering and Education Applications ,
pages 658–668. Springer, 2019.
[28] C. S. Kulkarni, A. Gupta, and P. F. J. Lermusiaux. Sparse regression and adaptive feature generation for
the discovery of dynamical systems. In F. Darema, E. Blasch, S. Ravela, and A. Aved, editors, Dynamic
Data Driven Application Systems. DDDAS 2020. , volume 12312 of Lecture Notes in Computer Science ,
pages 208–216. Springer, Cham, Nov. 2020. doi: 10.1007/978-3-030-61725-7 25.
[29] S. Laizet, J. Nedi´ c, and C. Vassilicos. Influence of the spatial resolution on fine-scale features in DNS of
turbulence generated by a single square grid. International Journal of Computational Fluid Dynamics ,
29(3-5):286–302, 2015.
[30] P. F. J. Lermusiaux. Adaptive modeling, adaptive data assimilation and adaptive sampling. Physica
D: Nonlinear Phenomena , 230(1):172–196, 2007. doi: 10.1016/j.physd.2007.02.014.
[31] P. F. J. Lermusiaux. Numerical fluid mechanics. MIT OpenCourseWare, May 2015. URL https://ocw.
mit.edu/courses/mechanical-engineering/2-29-numerical-fluid-mechanics-spring-2015/
lecture-notes-and-references/ .
23[32] P. F. J. Lermusiaux, C. Evangelinos, R. Tian, P. J. Haley, Jr, J. J. McCarthy, N. M. Patrikalakis,
A. R. Robinson, and H. Schmidt. Adaptive coupled physical and biogeochemical ocean predictions: A
conceptual basis. In Computational Science - ICCS 2004 , volume 3038 of Lecture Notes in Computer
Science , pages 685–692. Springer Berlin Heidelberg, 2004. ISBN 978-3-540-22116-6. doi: 10.1007/
978-3-540-24688-6 ∖89.
[33] P. F. J. Lermusiaux, C.-S. Chiu, G. G. Gawarkiewicz, P. Abbot, A. R. Robinson, R. N. Miller, P. J.
Haley, Jr, W. G. Leslie, S. J. Majumdar, A. Pang, and F. Lekien. Quantifying uncertainties in ocean
predictions. Oceanography , 19(1):92–105, 2006. doi: 10.5670/oceanog.2006.93.
[34] P. F. J. Lermusiaux, P. Malanotte-Rizzoli, D. Stammer, J. Carton, J. Cummings, and A. M. Moore.
Progress and prospects of U.S. data assimilation in ocean research. Oceanography , 19(1):172–183, 2006.
doi: 10.5670/oceanog.2006.102.
[35] S. Li and L. Petzold. Adjoint sensitivity analysis for time-dependent partial differential equations with
adaptive mesh refinement. Journal of Computational Physics , 198(1):310–325, 2004.
[36] R. M. May. Stability and complexity in model ecosystems , volume 1. Princeton U. press, 2019.
[37] J. C. McWilliams. Submesoscale currents in the ocean. Proceedings of the Royal Society A: Mathemat-
ical, Physical and Engineering Sciences , 472(2189):20160117, 2016.
[38] J. C. McWilliams. Submesoscale surface fronts and filaments: secondary circulation, buoyancy flux,
and frontogenesis. Journal of Fluid Mechanics , 823:391, 2017.
[39] D. A. Messenger and D. M. Bortz. Weak sindy for partial differential equations. Journal of Computa-
tional Physics , page 110525, 2021.
[40] A. M. Moore, M. Martin, S. Akella, H. Arango, M. Balmaseda, L. Bertino, S. Ciavatta, B. Cornuelle,
J. Cummings, S. Frolov, P. Lermusiaux, P. Oddo, P. R. Oke, A. Storto, A. Teruzzi, A. Vidard, and
A. T. Weaver. Synthesis of ocean observations using data assimilation for operational, real-time and
reanalysis systems: A more complete picture of the state of the ocean. Frontiers in Marine Science , 6
(90):1–6, Mar. 2019. doi: 10.3389/fmars.2019.00090.
[41] P. A. Newberger, J. S. Allen, and Y. H. Spitz. Analysis and comparison of three ecosystem models.
Journal of Geophysical Research: Oceans (1978–2012) , 108(C3), 2003.
[42] M. A. Nowak. Evolutionary dynamics: exploring the equations of life . Harvard U. press, 2006.
[43] J. Palmer and I. Totterdell. Production and export in a global ocean ecosystem model. Deep Sea
Research Part I: Oceanographic Research Papers , 48(5):1169–1198, 2001.
[44] S. Pan and K. Duraisamy. Data-driven discovery of closure models. SIAM Journal on Applied Dynamical
Systems , 17(4):2381–2413, 2018.
[45] S. Pawar, S. E. Ahmed, O. San, and A. Rasheed. Data-driven recovery of hidden physics in reduced
order modeling of fluid flows. Physics of Fluids , 32(3):036602, 2020.
[46] C. Rackauckas et al. SciML Scientific Machine Learning Software. https://sciml.ai/ .
[47] K. Rahul and S. Bhattacharyya. One-sided finite-difference approximations suitable for use with richard-
son extrapolation. Journal of Computational Physics , 219(1):13–20, 2006.
[48] A. R. Robinson and P. F. J. Lermusiaux. Data assimilation for modeling and predicting coupled
physical–biological interactions in the sea. In A. R. Robinson, J. J. McCarthy, and B. J. Rothschild,
editors, Biological-Physical Interactions in the Sea , volume 12 of The Sea , chapter 12, pages 475–536.
John Wiley and Sons, New York, 2002.
24[49] A. R. Robinson, P. F. J. Lermusiaux, and N. Q. Sloan III. Data assimilation. In K. H. Brink and A. R.
Robinson, editors, The Global Coastal Ocean-Processes and Methods , volume 10 of The Sea , chapter 20,
pages 541–594. John Wiley and Sons, New York, 1998.
[50] S. Rudy, A. Alla, S. L. Brunton, and J. N. Kutz. Data-driven identification of parametric partial
differential equations. SIAM Journal on Applied Dynamical Systems , 18(2):643–660, 2019.
[51] P. Saha and S. Mukhopadhyay. A deep learning approach for predicting spatiotemporal dynamics from
sparsely observed data. IEEE Access , 9:64200–64210, 2021.
[52] O. San and R. Maulik. Neural network closures for nonlinear model order reduction. Advances in
Computational Mathematics , 44(6):1717–1750, 2018.
[53] T. P. Sapsis and P. F. J. Lermusiaux. Dynamical criteria for the evolution of the stochastic di-
mensionality in flows with uncertainty. Physica D: Nonlinear Phenomena , 241(1):60–76, 2012. doi:
10.1016/j.physd.2011.10.001.
[54] Y. Shi, B. Xu, and Y. Guo. Numerical solution of korteweg-de vries-burgers equation by the compact-
type cip method. Advances in Difference Equations , 2015(1):1–9, 2015.
[55] J. Sirignano, J. F. MacArt, and J. B. Freund. Dpm: A deep learning pde augmentation method with
application to large-eddy simulation. Journal of Computational Physics , 423:109811, 2020.
[56] P. Stinis. Renormalized Mori–Zwanzig-reduced models for systems without scale separation. Proceedings
of the Royal Society A , 471(2176):20140446, 2015.
[57] R. Tian, C. Chen, J. Qi, R. Ji, R. C. Beardsley, and C. Davis. Model study of nutrient and phytoplankton
dynamics in the gulf of maine: patterns and drivers for seasonal and interannual variability. ICES
Journal of Marine Science , 72(2):388–402, 2015.
[58] R. C. Tian, P. F. J. Lermusiaux, J. J. McCarthy, and A. R. Robinson. A generalized prognostic model
of marine biogeochemical-ecosystem dynamics: Structure, parameterization and adaptive modeling.
Harvard Reports in Physical/Interdisciplinary Ocean Science 67, Department of Earth and Planetary
Sciences, Harvard University, Cambridge, MA, May 2004.
[59] I. T. Tokuda, O. E. Akman, and J. C. Locke. Reducing the complexity of mathematical models for the
plant circadian clock by distributed delays. J. of theoretical biology , 463:155–166, 2019.
[60] B. van Merri¨ enboer, A. B. Wiltschko, and D. Moldovan. Tangent: Automatic differentiation using
source code transformation in python. arXiv preprint arXiv:1711.02712 , 2017.
[61] B. Van Merri¨ enboer, O. Breuleux, A. Bergeron, and P. Lamblin. Automatic differentiation in ml: Where
we are and where we should be going. Advances in neural information processing systems , 31, 2018.
[62] Z. Y. Wan, P. Vlachas, P. Koumoutsakos, and T. Sapsis. Data-assisted reduced-order modeling of
extreme events in complex dynamical systems. PloS one , 13(5), 2018.
[63] Q. Wang, N. Ripamonti, and J. S. Hesthaven. Recurrent neural network closure of parametric POD-
galerkin reduced-order models based on the Mori-Zwanzig formalism. Journal of Computational Physics ,
page 109402, 2020.
[64] H. Whitney. Differentiable manifolds. Annals of Mathematics , pages 645–680, 1936.
[65] J. Wu. Theory and applications of partial functional differential equations , volume 119. Springer Science
& Business Media, 2012.
[66] P. Yeung et al. Effects of finite spatial and temporal resolution in direct numerical simulations of
incompressible isotropic turbulence. Physical Review Fluids , 3(6):064603, 2018.
25[67] H. Zhang and W. Zhao. A memory-efficient neural ordinary differential equation framework based on
high-level adjoint differentiation. IEEE Transactions on Artificial Intelligence , 2022.
[68] J. Zhuang, N. Dvornek, X. Li, S. Tatikonda, X. Papademetris, and J. Duncan. Adaptive checkpoint
adjoint method for gradient estimation in neural ode. In International Conference on Machine Learning ,
pages 11639–11649. PMLR, 2020.
26Supplementary Information
Generalized Neural Closure Models with Interpretability
Abhinav Gupta∗1and Pierre F.J. Lermusiaux†1
1Department of Mechanical Engineering, Massachusetts Institute of Technology,
Cambridge, MA 02139
Friday 19thMay, 2023
SI-1 Adjoint Equations for Neural Partial Delay Differential Equa-
tions
In this section, we provide a detailed derivation of adjoint equations for neural partial delay dif-
ferential equations (nPDDEs). This derivation is inspired by the adjoint equation derivation for a
general PDE by Li and Petzold, 2004 [1] and Cao et al., 2002 [2]. Our nPDDE is of the form,
𝜕𝑢(𝑥, 𝑡)
𝜕𝑡=ℱ𝑁𝑁(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡)
𝜕𝑥𝑑, 𝑥, 𝑡;𝜑)︂
+∫︁𝑡
𝑡−𝜏𝒟𝑁𝑁(︂
𝑢(𝑥, 𝑠),𝜕𝑢(𝑥, 𝑠)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑠)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑠)
𝜕𝑥𝑑, 𝑥, 𝑠;𝜃)︂
𝑑𝑠 ,
𝑥∈Ω, 𝑡≥0,
𝑢(𝑥, 𝑡) =ℎ(𝑥, 𝑡),−𝜏≤𝑡≤0 andℬ(𝑢(𝑥, 𝑡)) =𝑔(𝑥, 𝑡), 𝑥∈𝜕Ω, 𝑡≥0.(SI-1)
As compared to PDEs, PDDEs require the speciﬁcation of a history function ( ℎ(𝑥, 𝑡)) for the initial
conditions.ℱ𝑁𝑁(∙;𝜑) and𝒟𝑁𝑁(∙;𝜃) are two neural networks (NNs) parameterized by 𝜑and𝜃,
respectively. We consider the presence of an arbitrary number of spatial derivatives, with the
highest order deﬁned by 𝑑∈Z+. We can rewrite the above equation SI-1 as an equivalent system
∗guptaa@mit.edu
†pierrel@mit.edu
1arXiv:2301.06198v2  [cs.LG]  18 May 2023of coupled PDDEs with discrete delays,
𝜕𝑢(𝑥, 𝑡)
𝜕𝑡=ℱ𝑁𝑁(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡)
𝜕𝑥𝑑, 𝑥, 𝑡;𝜑)︂
+𝑦(𝑥, 𝑡),
𝑥∈Ω, 𝑡≥0,
𝜕𝑦(𝑥, 𝑡)
𝜕𝑡=𝒟𝑁𝑁(︂
𝑢(𝑥, 𝑡),𝜕𝑢(𝑥, 𝑡)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡)
𝜕𝑥𝑑, 𝑥, 𝑡;𝜃)︂
−𝒟 𝑁𝑁(︂
𝑢(𝑥, 𝑡−𝜏),𝜕𝑢(𝑥, 𝑡−𝜏)
𝜕𝑥,𝜕2𝑢(𝑥, 𝑡−𝜏)
𝜕𝑥2, ...,𝜕𝑑𝑢(𝑥, 𝑡−𝜏)
𝜕𝑥𝑑, 𝑥, 𝑡−𝜏;𝜃)︂
,
𝑥∈Ω, 𝑡≥0,
𝑢(𝑥, 𝑡) =ℎ(𝑥, 𝑡),−𝜏≤𝑡≤0 andℬ(𝑢(𝑥, 𝑡)) =𝑔(𝑥, 𝑡), 𝑥∈𝜕Ω, 𝑡≥0
𝑦(𝑥,0) =∫︁0
−𝜏𝒟𝑁𝑁(︂
ℎ(𝑥, 𝑠),𝜕ℎ(𝑥, 𝑠)
𝜕𝑥,𝜕2ℎ(𝑥, 𝑠)
𝜕𝑥2, ...,𝜕𝑑ℎ(𝑥, 𝑠)
𝜕𝑥𝑑𝑥, 𝑠;𝜃)︂
𝑑𝑠 .(SI-2)
Let us assume that high-ﬁdelity data is available at M discrete times, 𝑇1< ... < 𝑇 𝑀≤𝑇, and at
𝑁(𝑇𝑖) spatial locations ( 𝑥𝑇𝑖
𝑘∈Ω,∀𝑘∈1, ..., 𝑁 (𝑇𝑖)) for each of the times. We deﬁne the scalar loss
function as 𝐿=1
𝑀∑︀𝑀
𝑖=11
𝑁(𝑇𝑖)∑︀𝑁(𝑇𝑖)
𝑘=1𝑙(𝑢(𝑥𝑇𝑖
𝑘, 𝑇𝑖))≡∫︀𝑇
01
𝑀∑︀𝑀
𝑖=1∫︀
Ω1
𝑁(𝑇𝑖)∑︀𝑁(𝑇𝑖)
𝑘=1𝑙(𝑢(𝑥, 𝑡))𝛿(𝑥−
𝑥𝑇𝑖
𝑘)𝛿(𝑡−𝑇𝑖)𝑑𝑥𝑑𝑡≡∫︀𝑇
01
𝑀∑︀𝑀
𝑖=11
|Ω|∫︀
Ωˆ𝑙(𝑢(𝑥, 𝑡))𝛿(𝑡−𝑇𝑖)𝑑𝑥𝑑𝑡, where 𝑙(∙) are scalar loss functions
such as mean-squared-error (MSE), and 𝛿(∙) is the Kronecker delta function. In order to derive
the adjoint PDEs, we start with the Lagrangian corresponding to the above system,
L=𝐿(𝑢(𝑥, 𝑡)) +∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡) (𝜕𝑡𝑢(𝑥, 𝑡)−ℱ 𝑁𝑁(∙, 𝑡;𝜑)−𝑦(𝑥, 𝑡))𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡) (𝜕𝑡𝑦(𝑥, 𝑡)−𝒟 𝑁𝑁(∙, 𝑡;𝜃) +𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃))𝑑𝑥𝑑𝑡
+∫︁
Ω𝛼𝑇(𝑥)(︂
𝑦(𝑥,0)−∫︁0
−𝜏𝒟𝑁𝑁(ℎ(𝑥, 𝑡), 𝜕𝑥ℎ(𝑥, 𝑡), 𝜕𝑥2ℎ(𝑥, 𝑡), ..., 𝜕𝑥𝑑ℎ(𝑥, 𝑡), 𝑥, 𝑡;𝜃)𝑑𝑡)︂
𝑑𝑥 ,
(SI-3)
where 𝜆(𝑥, 𝑡),𝜇(𝑥, 𝑡) and 𝛼(𝑥) are the Lagrangian variables. We then take the derivative of the
Lagrangian (equation SI-3) w.r.t. 𝜃(for brevity we denote, 𝜕/𝜕(∙)≡𝜕(∙), and 𝑑/𝑑(∙)≡𝑑(∙)),
𝑑𝜃L=∫︁𝑇
0∫︁
Ω1
𝑀1
|Ω|𝑀∑︁
𝑖=1𝜕𝑢(𝑥,𝑡)ˆ𝑙(𝑢(𝑥, 𝑡))𝛿(𝑡−𝑇𝑖)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡 +∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡) (𝜕𝑡𝑑𝜃𝑢(𝑥, 𝑡)
−𝜕𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡;𝜑)𝑑𝜃𝑢(𝑥, 𝑡)−𝜕𝜕𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡;𝜑)𝑑𝜃𝜕𝑥𝑢(𝑥, 𝑡)
−𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡;𝜑)𝑑𝜃𝜕𝑥𝑥𝑢(𝑥, 𝑡)−𝑑𝜃𝑦(𝑥, 𝑡))︀
𝑑𝑥𝑑𝑡 +∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡) (𝜕𝑡𝑑𝜃𝑦(𝑥, 𝑡)
−𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝑢(𝑥, 𝑡)−𝜕𝜕𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝜕𝑥𝑢(𝑥, 𝑡)
−𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝜕𝑥𝑥𝑢(𝑥, 𝑡)
−𝜕𝜃𝒟𝑁𝑁(∙, 𝑡;𝜃) +𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝑑𝜃𝑢(𝑥, 𝑡−𝜏)
+𝜕𝜕𝑥𝑢(𝑥,𝑡−𝜏)𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝑑𝜃𝜕𝑥𝑢(𝑥, 𝑡−𝜏)
+𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡−𝜏)𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝑑𝜃𝜕𝑥𝑥𝑢(𝑥, 𝑡−𝜏) +𝜕𝜃𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃))︀
𝑑𝑥𝑑𝑡
+∫︁
Ω𝛼𝑇(𝑥)(︂
𝑑𝜃𝑦(𝑥,0)−∫︁0
−𝜏𝜕𝜃𝐺𝑁𝑁(ℎ(𝑥, 𝑡), 𝜕𝑥ℎ(𝑥, 𝑡), 𝜕𝑥𝑥ℎ(𝑥, 𝑡), 𝑥, 𝑡;𝜃)𝑑𝑡)︂
𝑑𝑥 .(SI-4)
2Using integration-by-parts, we get,
∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡)𝜕𝑡𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡 =∫︁
Ω[𝜆𝑇(𝑥, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)]⃒⃒⃒⃒𝑇
0−∫︁𝑇
0∫︁
Ω𝜕𝑡𝜆𝑇(𝑥, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡 , (SI-5)
∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
=∫︁𝑇
0∫︁
𝜕Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥(︀
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡 ,(SI-6)
∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝜕𝑥𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
=∫︁𝑇
0∫︁
𝜕Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥(︀
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︀
𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
=∫︁𝑇
0∫︁
𝜕Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−(︂∫︁𝑇
0∫︁
𝜕Ω𝜕𝑥(︀
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥𝑥(︀
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡)︂
,(SI-7)
∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡−𝜏)𝒟𝑁𝑁(∙, 𝑡−𝜏)𝑑𝜃𝑢(𝑥, 𝑡−𝜏)𝑑𝑥𝑑𝑡
=∫︁𝑇−𝜏
−𝜏∫︁
Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
=∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
𝑇−𝜏∫︁
Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁0
−𝜏∫︁
Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡 .(SI-8)
Using the fact that 𝜇𝑇(𝑥, 𝑡) = 0 ,∀𝑡≥𝑇and𝑑𝜃𝑢(𝑥, 𝑡) = 0 ,−𝜏≤𝑡≤0, we get,
∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡−𝜏)𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝑑𝜃𝑢(𝑥, 𝑡−𝜏)𝑑𝑥𝑑𝑡
=∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡 .(SI-9)
3Similarly, we also get,
∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡−𝜏)𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡−𝜏)𝑑𝑥𝑑𝑡
=∫︁𝑇
0∫︁
𝜕Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥(︀
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡 ,(SI-10)
∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡−𝜏)𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝜕𝑥𝑥𝑑𝜃𝑢(𝑥, 𝑡−𝜏)𝑑𝑥𝑑𝑡
=∫︁𝑇
0∫︁
𝜕Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−(︂∫︁𝑇
0∫︁
𝜕Ω𝜕𝑥(︀
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥𝑥(︀
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡)︂
.
(SI-11)
Plugging everything in yields,
𝑑𝜃L=∫︁𝑇
0∫︁
Ω1
𝑀1
|Ω|𝑀∑︁
𝑖=1𝜕𝑢(𝑥,𝑡)ˆ𝑙(𝑢(𝑥, 𝑡))𝛿(𝑡−𝑇𝑖)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑡𝜆𝑇(𝑥, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
𝜕Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
Ω𝜕𝑥(︀
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
𝜕Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
𝜕Ω𝜕𝑥(︀
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥𝑥(︀
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡)𝑑𝜃𝑦(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁
Ω𝜇𝑇(𝑥,0)𝑑𝜃𝑦(𝑥,0)𝑑𝑥−∫︁𝑇
0∫︁
Ω𝜕𝑡𝜇𝑇(𝑥, 𝑡)𝑑𝜃𝑦(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
𝜕Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
4+∫︁𝑇
0∫︁
Ω𝜕𝑥(︀
𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
𝜕Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
𝜕Ω𝜕𝑥(︀
𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥𝑥(︀
𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜃𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
𝜕Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
Ω𝜕𝑥(︀
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
𝜕Ω𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)𝜕𝑥𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
−∫︁𝑇
0∫︁
𝜕Ω𝜕𝑥(︀
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
Ω𝜕𝑥𝑥(︀
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑥𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︀
𝑑𝜃𝑢(𝑥, 𝑡)𝑑𝑥𝑑𝑡
+∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜃𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝑑𝑥𝑑𝑡
+∫︁
Ω𝛼𝑇(𝑥)𝑑𝜃𝑦(𝑥,0)
−∫︁
Ω𝛼𝑇(𝑥)∫︁0
−𝜏𝜕𝜃𝒟𝑁𝑁(ℎ(𝑥, 𝑡), 𝜕𝑥ℎ(𝑥, 𝑡), 𝜕𝑥𝑥ℎ(𝑥, 𝑡), 𝑥, 𝑡;𝜃)𝑑𝑡𝑑𝑥 . (SI-12)
Collecting all the terms with∫︀
Ω,𝑑𝜃𝑢(𝑥, 𝑡), and 𝑑𝜃𝑦(𝑥, 𝑡), we get the following adjoint PDEs,
0 =1
𝑀1
|Ω|𝑀∑︁
𝑘=1𝜕𝑢(𝑥,𝑡)ˆ𝑙(𝑢(𝑥, 𝑡))𝛿(𝑡−𝑇𝑘)
−𝜕𝑡𝜆𝑇(𝑥, 𝑡)−𝜆𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡) +𝑑∑︁
𝑖=1(−1)𝑖+1𝜕𝑥𝑖(︁
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑖𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︁
−𝜇𝑇(𝑥, 𝑡)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃) +𝑑∑︁
𝑖=1(−1)𝑖+1𝜕𝑥𝑖(︁
𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑖𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︁
+𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃)−𝑑∑︁
𝑖=1(−1)𝑖+1𝜕𝑥𝑖(︁
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑖𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡;𝜃))︁
,
𝑥∈Ω, 𝑡∈[0, 𝑇),
0 =−𝜆𝑇(𝑥, 𝑡)−𝜕𝑡𝜇𝑇(𝑥, 𝑡), 𝑥∈Ω, 𝑡∈[0, 𝑇),(SI-13)
5with initial conditions, 𝜆(𝑥, 𝑡) =𝜇(𝑥, 𝑡) = 0 , 𝑡≥𝑇. The boundary conditions are derived based on
that of the forward PDE such that they satisfy,
0 =𝑑∑︁
𝑖=0𝑑−𝑖−1∑︁
𝑗=0(−1)𝑗+1𝜕𝑥𝑗(︁
𝜆𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑗+𝑖+1𝑢(𝑥,𝑡)ℱ𝑁𝑁(∙, 𝑡))︁
𝑑𝜃𝜕𝑥𝑖𝑢(𝑥, 𝑡)
+𝑑∑︁
𝑖=0𝑑−𝑖−1∑︁
𝑗=0(−1)𝑗+1𝜕𝑥𝑗(︁
𝜇𝑇(𝑥, 𝑡)𝜕𝜕𝑥𝑗+𝑖+1𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡))︁
𝑑𝜃𝜕𝑥𝑖𝑢(𝑥, 𝑡)
−𝑑∑︁
𝑖=0𝑑−𝑖−1∑︁
𝑗=0(−1)𝑗+1𝜕𝑥𝑗(︁
𝜇𝑇(𝑥, 𝑡+𝜏)𝜕𝜕𝑥𝑗+𝑖+1𝑢(𝑥,𝑡)𝒟𝑁𝑁(∙, 𝑡))︁
𝑑𝜃𝜕𝑥𝑖𝑢(𝑥, 𝑡),
𝑥∈𝜕Ω, 𝑡∈[𝑡, 𝑇).(SI-14)
Note that adjoint PDE needs to be solved backward in time, and one would require access to
𝑢(𝑥, 𝑡),∀𝑥∈Ω,0≤𝑡≤𝑇. After solving for the Lagrangian variables, 𝜆(𝑥, 𝑡) and 𝜇(𝑥, 𝑡), we can
compute the required gradients as follows:
𝑑𝜃ℒ=−∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜃𝒟𝑁𝑁(∙, 𝑡;𝜃)𝑑𝑥𝑑𝑡 +∫︁𝑇
0∫︁
Ω𝜇𝑇(𝑥, 𝑡)𝜕𝜃𝒟𝑁𝑁(∙, 𝑡−𝜏;𝜃)𝑑𝑥𝑑𝑡
−∫︁
Ω𝜇𝑇(𝑥,0)∫︁0
−𝜏𝜕𝜃𝒟𝑁𝑁(ℎ(𝑥, 𝑡), 𝜕𝑥ℎ(𝑥, 𝑡), 𝜕𝑥𝑥ℎ(𝑥, 𝑡), 𝑥, 𝑡;𝜃)𝑑𝑡𝑑𝑥 .(SI-15)
If we restart the above derivation by taking derivative of the Lagrangian (equation SI-3) w.r.t. 𝜑,
we will arrive at the same adjoint PDEs (equations SI-13 & SI-14), and the required gradient will
be given by,
𝑑𝜑ℒ=−∫︁𝑇
0∫︁
Ω𝜆𝑇(𝑥, 𝑡)𝜕𝜑ℱ𝑁𝑁(∙, 𝑡;𝜑)𝑑𝑥𝑑𝑡 . (SI-16)
Finally, using any stochastic gradient descent algorithm, we can ﬁnd the optimal values of the
weights 𝜑and𝜃.
SI-2 Experimental Setup
SI-2.1 Architectures
The architectures used to generate the results corresponding to diﬀerent experiments are provided in
table SI-1. The implementation details of the various biological and carbonate constraints imposed
on the neural closure terms in experiments 2a & 2b are also provided.
SI-2.2 Hyperparameters
The values of the tuned training hyperparameters corresponding to diﬀerent experiments are listed
next. In all the experiments, the number of iterations per epoch is calculated by dividing the
number of time-steps in the training period by the batch-size multiplied by the length of short
time-sequences, adding 1, and rounding up to the next integer.
6SI-2.2.1 Experiments-1a:
For training, we randomly select short time-sequences spanning 3 time-steps (batch-time) and
extract data at every time-step to form batches of size 16; 4 iterations per epoch are used; an
exponentially decaying learning rate (LR) schedule is used with an initial LR of 0.075, the decay
rate of 0.97, and 4 decay steps; the RMSprop optimizer is employed; training is for a total of 150
epochs.ℒ1andℒ2regularization with factors of 1 .5×10−3and 1×10−5, respectively, is used; the
weights are pruned if the value drops below 5 ×10−3.
SI-2.2.2 Experiments-1b:
Only Markovian closure case: We randomly select short time-sequences spanning 30 time-
steps (batch-time) and extract data at every other time-step to form batches of size 2. In total
24 iterations per epoch are used, with every 8 of them belonging to one of the ( 𝑁𝑥, 𝑅𝑒) pairs. An
exponentially decaying learning rate (LR) schedule with an initial LR of 0.025, a decay rate of 0.95,
and 24 decay steps are used; the RMSprop optimizer is used; we train for a total of 30 epochs.
We also use both ℒ1andℒ2regularization for the weights of the neural network with factors of
5×10−4and 5×10−4, respectively, along with pruning of the weights if their value drops below
5×10−3.
Both Markovian and non-Markovian closures case: A batch-time of 30 time-steps is used
with data extracted at every other time-step to form batches of size 2; 32 iterations per epoch
are used, with every 8 of them belonging to one of the ( 𝑁𝑥, 𝑅𝑒) pairs; an exponentially decaying
learning rate (LR) schedule is employed with an initial LR of 0.01, the decay rate of 0.95, and 32
decay steps; the RMSprop optimizer is used; we train for a total of 30 epochs. We also use both ℒ1
andℒ2regularization for the weights of the neural network with factors of 1 .5×10−3and 1×10−5,
respectively, along with pruning of the weights if their value drops below 5 ×10−3.
SI-2.2.3 Experiments-2a:
Parameter values used in the ocean acidiﬁcation model are (adopted from [57; 3]): 𝑔𝑚𝑎𝑥= 0.7𝑑𝑎𝑦−1;
𝑘𝑊= 0.08𝑚−1;𝐾𝑁= 0.5𝑚𝑚𝑜𝑙 𝑁 𝑚−3;𝐾𝑃= 0.25𝑚𝑚𝑜𝑙 𝑁 𝑚−3;𝑚𝑃= 0.08𝑑𝑎𝑦−1(𝑚𝑚𝑜𝑙 𝑁 𝑚−3)−1;
𝑚𝑍= 0.030𝑑𝑎𝑦−1(𝑚𝑚𝑜𝑙 𝑁 𝑚−3)−1;𝜇𝑚𝑎𝑥 = 2.808𝑑𝑎𝑦−1;𝛼= 0.14 (𝑊 𝑚−2𝑑𝑎𝑦)−1;𝛽=
0.0028 ( 𝑊 𝑚−2𝑑𝑎𝑦)−1;𝜖= 0.015𝑑𝑎𝑦−1;𝜆= 0.3;𝛾= 0.4; a sinusoidal variation in 𝐼𝑜(𝑡); linear
vertical variation in total biomass 𝑇𝑏𝑖𝑜(𝑧) from 10 𝑚𝑚𝑜𝑙 𝑁 𝑚−3at the surface to 20 𝑚𝑚𝑜𝑙 𝑁 𝑚−3
at𝑧= 100 𝑚;𝐾𝑧𝑏= 0.0864 ( 𝑚2/𝑑𝑎𝑦);𝐾𝑧0= 8.64 (𝑚2/𝑑𝑎𝑦);𝛾𝑡= 0.1𝑚−1;𝐷𝑧=−100𝑚; and
𝜌𝑤= 1000 𝑘𝑔/𝑚3. For training, we randomly select short time-sequences spanning 3 time-steps
(batch-time) and extract data at every other time-step to form batches of size 4; we use 26 itera-
tions per epoch; an exponentially decaying learning rate (LR) schedule is used with an initial LR
of 0.075, the decay rate of 0.97, and 26 decay steps; the RMSprop optimizer is adopted; training is
terminated at 200 epochs. We also use both ℒ1andℒ2regularization for the weights of the neural
network with factors of 1 .5×10−3and 1×10−3, respectively, along with pruning of the weights if
their value drops below 5 ×10−3.
SI-2.2.4 Experiments-2b:
We use a batch-time of 3 time-steps with data extracted at every other time-step to form batches
of size 8; we use 26 iterations per epoch; an exponentially decaying learning rate (LR) schedule is
applied with an initial LR of 0.075, the decay rate of 0.97, and 26 decay steps; the RMSprop opti-
mizer is employed; training is terminated at 200 epochs. We also use both ℒ1andℒ2regularization
7for the weights of the Markovian closure with factors of 1 .5×10−3and 1×10−3, respectively, along
with pruning of the weights if their value drops below 5 ×10−3. For the neural network in the
non-Markovian closure term, only ℒ2regularization with a factor of 1 ×10−5is used.
Finally, for all the experiments and their multiple repeats with the exact same tuned hyper-
parameters, we provide variation of training and validation error as training progresses (ﬁgure
SI-1).
References
[1] Shengtai Li and Linda Petzold. Adjoint sensitivity analysis for time-dependent partial diﬀer-
ential equations with adaptive mesh reﬁnement. Journal of Computational Physics , 198(1):
310–325, 2004.
[2] Yang Cao, Shengtai Li, and Linda Petzold. Adjoint sensitivity analysis for diﬀerential-algebraic
equations: algorithms and software. Journal of computational and applied mathematics , 149(1):
171–191, 2002.
[3] Mette Eknes and Geir Evensen. An ensemble kalman ﬁlter with a 1-d marine ecosystem model.
Journal of Marine Systems , 36(1-2):75–100, 2002.
8Experi-
mentsMarkovian Term Non-Markovian Term
Architecture Act. Trainable Weights Architecture Act. DelaysTrainable
Weights
1aℱ𝑁𝑁
4 Input layer with 4
neuronsnone
Dense output layer
with 1 neuronslinear
1bℱ𝑁𝑁
4𝒢𝑁𝑁
0.075 198Input layer with 4
neuronsnone Input layer with 5
neuronsnone
Dense output layer
with 1 neuronslinear Dense hidden layer
with 10 neuronsswish
Dense hidden layer
with 7 neuronsswish
2 Dense hidden lay-
ers with 5 neuronsswish
Dense hidden layer
with 3 neuronsswish
Dense output layer
with 1 neuronlinear
Multiply output with |𝑢|
2aℱ𝑁𝑁 18 (eﬀective)
𝑤=⎡
⎢⎢⎢⎢⎢⎢⎣−(𝑤2+𝑤3+𝑤4)
𝑤2
𝑤3
𝑤4
−𝐶𝑃𝑤2−𝐶𝑍𝑤3−𝐶𝐷𝑤4
(𝑤2+𝑤3+𝑤4)/𝜌𝑤⎤
⎥⎥⎥⎥⎥⎥⎦Input layer with 6
neuronsnone
Dense output layer
with 6 neuronslinear
2bℱ𝑁𝑁 4 (eﬀective)
𝑤=⎡
⎢⎢⎢⎢⎣−𝑤3
0
𝑤3
−𝐶𝑍𝑤3
𝑤3/𝜌𝑤⎤
⎥⎥⎥⎥⎦𝒢𝑁𝑁
2.5 65Input layer with 4
neuronsnone Input layer with 4
neuronsnone
Dense output layer
with 5 neuronslinear 2 Dense hidden
layer with 5 neu-
ronsswish
Dense output layer
with 4 neuronslinear
𝒢𝑁𝑁=⎡
⎢⎢⎢⎢⎣−(𝒢1
𝑁𝑁+𝒢2
𝑁𝑁)
𝒢1
𝑁𝑁
𝒢2
𝑁𝑁
𝒢3
𝑁𝑁
𝒢4
𝑁𝑁/𝜌𝑤⎤
⎥⎥⎥⎥⎦
Table SI-1: Architectures for different generalized neural closure models used in the four sets of experiments. We
explicitly provide the constraints on the weights and output layer of neural networks used in different experiments.
{𝑤𝑖}4
𝑖=1are row vectors of the weight matrix. “Effective” number of trainable weights does not count the ones which
are not free or are overwritten due to the imposed constraints. 𝐶𝑃,𝐶𝑍, and 𝐶𝐷are the carbon-nitrogen ratios for
phytoplanktons, zooplanktons, and detritus, respectively. 𝜌𝑤is seawater density.
9(a)Experiments-1a
(b)Experiments-1b ( gnCM with only Markovian closure term)
(c)Experiments-1b ( gnCM with both Markovian and non-Markovian closure terms)
Figure SI-1: Variation of training (left column) and validation (right column) loss with epochs, for each of the
experiments-1a, 1b, 2a, and 2b. We use boxplots to provide statistical summaries for multiple training repeats done
for each set of experiments. The box and its whiskers provide a five-number summary: minimum, first quartile (Q1),
median (orange solid line), third quartile (Q3), and maximum, along with outliers (black circles) if any. These results
accompany the architectures detailed in table SI-1. (cont.)
10(d)Experiments-2a
(e)Experiments-2b
Figure SI-1: Variation of training (left column) and validation (right column) loss with epochs, for each of the
experiments-1a, 1b, 2a, and 2b. We use boxplots to provide statistical summaries for multiple training repeats done
for each set of experiments. The box and its whiskers provide a five-number summary: minimum, first quartile (Q1),
median (orange solid line), third quartile (Q3), and maximum, along with outliers (black circles) if any. These results
accompany the architectures detailed in table SI-1.
11