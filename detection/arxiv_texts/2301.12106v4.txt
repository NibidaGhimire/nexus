Covariate-assisted bounds on causal effects with instrumental
variables
Alexander W. Levis1, Matteo Bonvini1∗, Zhenghao Zeng1∗,
Luke Keele2, Edward H. Kennedy1
1Department of Statistics & Data Science,
Carnegie Mellon University
2Department of Surgery,
University of Pennsylvania
{alevis, mbonvini, zhenghaz} @ andrew.cmu.edu ;
luke.keele@gmail.com ;edward@stat.cmu.edu
Abstract
When an exposure of interest is confounded by unmeasured factors, an instrumental
variable(IV)canbeusedtoidentifyandestimatecertaincausalcontrasts. Identificationof
themarginalaveragetreatmenteffect(ATE)fromIVsreliesonstronguntestablestructural
assumptions. When one is unwilling to assert such structure, IVs can nonetheless be used
to construct bounds on the ATE. Famously, Balke and Pearl (1997) proved tight bounds
on the ATE for a binary outcome, in a randomized trial with noncompliance and no
covariate information. We demonstrate how these bounds remain useful in observational
settings with baseline confounders of the IV, as well as randomized trials with measured
baseline covariates. The resulting bounds on the ATE are non-smooth functionals, and
thus standard nonparametric efficiency theory is not immediately applicable. To remedy
this, we propose (1) under a novel margin condition, influence function-based estimators
of the bounds that can attain parametric convergence rates when the nuisance functions
are modeled flexibly, and (2) estimators of smooth approximations of these bounds. We
proposeextensionstocontinuousoutcomes, explorefinitesamplepropertiesinsimulations,
and illustrate the proposed estimators in an observational study targeting the effect of
higher education on wages.
Keywords: causal inference, instrumental variables, partial identification, nonparametric bounds,
nonparametric efficiency
∗MB and ZZ contributed equally.arXiv:2301.12106v4  [stat.ME]  29 Sep 20231 Introduction
A primary goal in many scientific endeavors is to determine whether an intervention has a
causal effect on an outcome. However, simple comparisons between the outcomes of treated
and control groups are often complicated by confounding: differences in outcomes between
those who are and are not treated due to pre-treatment differences rather than the effect
of the treatment itself. One solution to selection bias of this form is to allocate the treat-
ment via randomization, rendering pre-treatment distributions equal, on average. In many
circumstances, however, randomization is infeasible or unethical. When this is the case, one
alternative is to identify an instrumental variable (IV). For a variable to be an IV, it must
meet the three following conditions: (a) the IV must be associated with the exposure; (b) the
IV must be randomly or as-if randomly assigned; and (c) the IV cannot have a direct effect on
the outcome (Angrist et al., 1996a). Under these conditions, as well as some further structural
assumptions, an IV can provide a consistent estimate of a causal effect even in the presence
of unobserved confounding between the exposure and the outcome. See Baiocchi et al. (2014)
and Imbens (2014) for general reviews of IV methods.
In most analyses, investigators focus on point identification–asserting sufficient structure
so that a single parameter describing a causal effect can be expressed as a function of the
observed data distribution. In fact, assumptions (a)–(c), on their own, are insufficient to point
identify an average causal effect. Point identification requires investigators to assume either
some form of homogeneity (e.g., lack of effect modification by unmeasured confounders) or
an assumption known as monotonicity (Robins, 1994; Hernán and Robins, 2006; Tan, 2010;
Angrist et al., 1996b). Critically, these structural IV assumptions are untestable and may be
controversial in many applications. One alternative approach is to relax key IV assumptions
usingpartialidentification. Underthepartialidentificationapproach, analystsseektoestimate
bounds on the parameter of interest, which can typically be done with weaker assumptions
(Manski, 1990a, 1995). There is a large body of work on partial identification in IV designs
with foundational work done by, e.g., Balke and Pearl (1997) and Manski and Pepper (2009,
2000). See Swanson et al. (2018) for a general overview of partial identification approaches to
an IV analysis.
There exists a substantial literature studying bounds on causal effects, incorporating co-
variate adjustment under various sets of assumptions. Broadly, use of covariates is beneficial
in either (a) experimental settings, where baseline covariate adjustment can improve efficiency
(Yang and Tsiatis, 2001; Tsiatis et al., 2008) and bound width (Cai et al., 2007), or (b) ob-
servational settings, where confounder adjustment is necessary for key assumptions to hold
(Grilli and Mealli, 2008; Feller et al., 2017). Notably, Cai et al. (2007) illustrated possible im-
provements on natural bounds (Manski, 1990b) and tight bounds (Balke and Pearl, 1997) on
average treatment effects in a randomized trial using covariate adjustment. Long and Hudgens
(2013) showed covariate-assisted bounds on principal effects are narrower than the unadjusted
bounds. Such adjusted bounds are widely used in principal stratification analysis on evaluat-
ing the effects of training and school programs (Lee, 2005; Miratrix et al., 2018). However,
most existing work considers the discrete covariate setting (hence plug-in style empirical av-
erage estimators are applicable) and does not explore efficiency theory on the bounds. Grilli
and Mealli (2008) examined adjusted principal stratification-based bounds using continuous
covariates under strong parametric assumptions that can be hard to justify in practice.
In this paper, we extend the Balke-Pearl bounds to include baseline covariate information
to remedy these issues. Our proposed estimators are based on semiparametric efficiency theory
and use influence functions, which allow for flexible and efficient estimation via a variety of
2nonparametric machine learning based methods. The use of influence functions also allows us
to derive simple closed-form variance estimators that are consistent. Importantly, our work
may provide useful insights into estimation strategies for functionals that involve non-smooth
functions on observed data and are not pathwise-differentiable without further assumptions.
Such functionals commonly appear when targetting bounds on causal effects (Sachs et al.,
2022; Grilli and Mealli, 2008; Long and Hudgens, 2013).
The remainder of the paper is organized as follows. In Section 2, we define notation, review
key IV assumptions, and outline our target causal estimand: the average treatment effect. In
Section 3, we review extant methods for bounding the average treatment effect (ATE) based
on an IV. We also provide a simple illustration to demonstrate the benefits of incorporating
covariate information, and present a general result to guide which covariates to include in
the ensuing analyses. Importantly, the nonparametric bounds outlined in Section 3 are non-
smooth functionals of the observed data distribution, so standard semiparametric efficiency
theory does not immediately apply. In view of this challenging setting, Sections 4, 5, and
6 detail the three main methodological contributions of the paper: (i) efficient estimators of
the true non-smooth bound functionals under a margin condition (in Section 4); (ii) valid but
conservative bounds on the ATE based on estimators of smooth functional approximations
to the bounds (in Section 5); and (iii) extensions of the proposed methods to the case of a
general bounded outcome, discrete or continuous (in Section 6). In Section 7, we present a
simulationstudytoinvestigatethefinitesamplepropertiesoftheproposedestimators. Finally,
in Section 8, we apply our proposed methods to data from an observational study to assess
the effects of college education on wages. Proofs of all results can be found in the appendices.
2 Background
2.1 Notation and Assumptions
Consider the standard instrumental variable setup, in which the observed data are niid copies
ofO= (X, Z, A, Y )∼P, where X∈ X ⊆ Rdis a vector of covariates, and Z, A∈ {0,1}
are binary instrument and exposure variables, respectively. Our focus is mostly on binary
outcomes, Y∈ {0,1}; we will discuss extensions for non-binary outcomes in Section 6. We let
A(z)andY(z)denote the counterfactual exposure and outcome values, had the instrument
been set to Z=z, forz∈ {0,1}. Similarly, we also define Y(a)andY(z, a)to be the potential
outcomes under an intervention that sets A=a, and an intervention that sets both Z=z
andA=a, respectively. For z∈ {0,1}, let λz(X) =P[Z=z|X]. Next, we review the
set of assumptions that we take as given in the analysis. First, we make the two following
assumptions:
Assumption 1 (Consistency) .A(Z) =A, and Y(Z, A) =Y(A) =Y, almost surely.
Assumption 2 (Positivity) .For some ϵ >0,λ1(X)∈[ϵ,1−ϵ], almost surely.
In words, Assumption 1 asserts that interventions on ZandAare well-defined, and that
there is no interference between subjects, so that a unit’s potential treatment and outcome
can be linked to their observed variables; Assumption 2 asserts that either instrument value
can be realized, over all strata determined by X. These two assumptions are not unique to
the IV framework, and are commonly invoked in many causal inference settings. Next, we
make the following “core” IV assumptions:
Assumption 3 (Unconfoundedness) .Z⊥ ⊥(A(z), Y(z))|X.
3Assumption 4 (Exclusion Restriction) .Y(z, a)≡Y(a), for z, a∈ {0,1}.
Assumption 3 asserts that the effect of ZonAandYis unconfounded, given measured
covariates X. Note that in certain special cases, Assumption 3 holds unconditionally, and
we can identify the IV effect without conditioning on X. For example, when Zis marginally
randomized (e.g., in a trial), Assumption 3 holds by design. Alternatively, in certain natural
experiments, analysts may assert that Assumption 3 holds unconditionally, e.g. Angrist and
Evans (1998). Assumption 4 asserts that the effect of ZonYacts entirely through its effect on
A, i.e., Zhas no direct effect on Y. Note that most IV studies adopt an assumption of non-zero
association between ZandA(often referred to as a “relevance” assumption). However, this
assumption is not formally required for partial identification of the average treatment effect.
2.2 Estimands and Additional Structural Assumptions
In what follows, we focus on the average treatment effect (ATE) as the target causal estimand:
E(Y(a= 1)−Y(a= 0)) .
Critically, under the four assumptions introduced in the previous section, the ATE is not point
identified. Analysts typically take one of two approaches for point identification. The first
approach invokes some type of homogeneity assumptions and places various restrictions on
how the effects of AandZvary from unit to unit in the study population. See Hernan and
Robins (2019) and Wang and Tchetgen Tchetgen (2018) for prominent examples. However,
homogeneity assumptions are often implausible or difficult to verify in specific applications.
The second approach invokes an assumption known as monotonicity, which has the following
form: A(z= 1)≥A(z= 0), i.e., if A(z= 0) = 1 then A(z= 1) = 1 (Imbens and Angrist,
1994). Under monotonicity, the target estimand is no longer the ATE, but instead is the local
average treatment effect (LATE):
E(Y(a= 1)−Y(a= 0)|A(z= 1) > A(z= 0)) (1)
Here, investigators must be content with a more local estimand that may not generalize to the
ATE. In this paper, we only assume that Assumptions 1–4 hold, and aim to construct tight
bounds on the ATE parameter.
3 Partial Identification for IVs and the Role of Covariate Infor-
mation
Next, we provide an overview of one partial identification approach for the IV framework.
We then provide a brief illustration to demonstrate how covariate information can alter the
bounds on the ATE. We also present theoretical results to elucidate when we may expect the
inclusion of covariates to tighten the bounds on the ATE.
3.1 Review: Balke-Pearl Bounds
In their seminal work, Alexander Balke and Judea Pearl leveraged symbolic linear program-
ming to develop sharp nonparametric bounds on the ATE for a binary outcome (Balke and
Pearl, 1994; Balke, 1995; Balke and Pearl, 1997). Notably, their bounds only invoke As-
sumptions 1–4, and are provably tight under these assumptions. In both applications and
4simulations they demonstrated substantial narrowing of the bounds on the ATE compared to
partial identification results in Robins (1989) and Manski (1990b).
The results in Balke and Pearl (1997) focused exclusively on marginally randomized instru-
ments such that X=∅. As such, their methods do not make use of any measured covariates.
Nonetheless, their results hold just as well for bounding the conditional average treatment
effect (CATE), E[Y(a= 1)−Y(a= 0)|X=x], for any x∈ X, in two important set-
tings where Assumptions 1–4 may hold: (i) an experiment with Zmarginally randomized and
baseline covariates Xare measured, and (ii) an observational setting where Xare baseline
confounders required for Zto be a valid IV. In case (i), we will demonstrate that incorporating
baseline covariate information Xcan both provide tighter theoretical bounds, and improve
statistical precision. In case (ii), conditioning on Xis required for the IV assumptions to hold,
and thus for the bounds to be valid.
To begin, we review the main theoretical result from Balke and Pearl (1997). For each
y, a, z ∈ {0,1}, define πya.z(X) =P(Y=y, A =a|X, Z=z). Moreover, define the
two 8-dimensional vectors θℓ(X) = (θℓ,1(X), . . . , θ ℓ,8(X)),θu(X) = (θu,1(X), . . . , θ u,8(X))∈
[−1,1]8, where omitting inputs,
θℓ,1=π11.1+π00.0−1
θℓ,2=π11.0+π00.1−1
θℓ,3=−π01.1−π10.1
θℓ,4=−π01.0−π10.0
θℓ,5=π11.0−π11.1−π10.1−π01.0−π10.0
θℓ,6=π11.1−π11.0−π10.0−π01.1−π10.1
θℓ,7=π00.1−π01.1−π10.1−π01.0−π00.0
θℓ,8=π00.0−π01.0−π10.0−π01.1−π00.1(2)
and
θu,1= 1−π01.1−π10.0
θu,2= 1−π01.0−π10.1
θu,3=π11.1+π00.1
θu,4=π11.0+π00.0
θu,5=−π01.0+π01.1+π00.1+π11.0+π00.0
θu,6=−π01.1+π11.1+π00.1+π01.0+π00.0
θu,7=−π10.1+π11.1+π00.1+π11.0+π10.0
θu,8=−π10.0+π11.0+π00.0+π11.1+π10.1.(3)
Finally, define γℓ(X) = max 1≤j≤8θℓ,j(X)andγu(X) = min 1≤j≤8θu,j(X). Balke and Pearl
proved (e.g., see the main result in Balke and Pearl (1997), p. 1173) that the CATE is
bounded between γℓandγu, the tightest possible lower and upper bounds, respectively, under
Assumptions 1–4. We summarize their result in the following theorem.
Theorem 1 (Balke and Pearl (1997)) .Under Assumptions 1–4, for (Z, A, Y )∈ {0,1}3, the
CATE can be bounded as
γℓ(X)≤E(Y(a= 1)−Y(a= 0)|X)≤γu(X).
These bounds are tight in the nonparametric model. Moreover, marginalizing yields bounds on
the ATE:
EP(γℓ(X))≤E(Y(a= 1)−Y(a= 0))≤EP(γu(X)), (4)
5which are also tight in the nonparametric model.
The primary goal of this paper is to construct efficient estimators of the tight bounds on the
ATE given in Theorem 1, L(P):=EP(γℓ(X))andU(P):=EP(γu(X)). Statistically, this
is a challenging task, since these are means of a pointwise maximum and minimum, each of
which is a non-smooth function. We will outline two strategies to deal with this difficulty: (1)
invoking a margin condition (see Section 4), or (2) targeting a smooth approximation to the
Balke-Pearl bounds (see Section 5).
Before describing strategies for estimation, we first demonstrate the benefit of pursuing
covariate-assisted bounds. In Section 3.2, we give a simple illustration in the randomized
experimental setting, showing that covariate-assisted bounds can be substantially narrower
than unadjusted bounds. In Section 3.3, we provide a general result to guide the choice of
adjustment set Xwhen more than one is possible.
3.2 Motivating Illustration
Consider a hypothetical randomized experiment with arm assignment Z∼Bernoulli(0 .5), in-
dependent of baseline covariates X1∼Bernoulli(0 .7),X2∼Unif(−1,1), with X1⊥ ⊥X2. In
this example, X1represents a behavioral or demographic factor, and X2represents an under-
lying risk score (i.e., low values represent good health and low risk, high values represent poor
health and high risk). In this experiment, we suppose that there is a degree of noncompliance,
which is completely determined by the covariates X1andX2. Specifically, we set:
A(z= 0)A(z= 1) = 1(X2≥0.99)
{1−A(z= 0)}{1−A(z= 1)}=1(X2≤ −0.99)
A(z= 0){1−A(z= 1)}= (1−X1)1(X2∈(−0.5,0.5])
{1−A(z= 0)}A(z= 1) = 1 −1(|X2| ≥0.99)−(1−X1)1(X2∈(−0.5,0.5])
In words, we can define four principal strata: “always takers”, who are exposed regardless of
instrument status, are those with the very highest underlying risk; “never takers”, who are
not exposed regardless of instrument status, are those with the very lowest risk; “defiers”,
whose exposure value is opposite to that of the instrument, are those with intermediate risk
and behavioral factor X1= 0; and “compliers”, whose exposure value matches that of the
instrument, comprise the remainder of the population (Angrist et al., 1996a; Frangakis and
Rubin, 2002). We further suppose the potential outcomes Y(a)are completely determined by
the compliance classes U= (A(z= 0), A(z= 1)), and set Y(a)|U∼Bernoulli( pa(U)), with
pa=

0.20 + 0 .10a,ifA(z= 0)A(z= 1) = 1 ,
0.90 + 0 .05a,if{1−A(z= 0)}{1−A(z= 1)}= 1,
0.65 + 0 .05a,ifA(z= 0){1−A(z= 1)}= 1,
0.25 + 0 .10a,if{1−A(z= 0)}A(z= 1) = 1 ..
Figure 1 shows the covariate-agnostic and covariate-adjusted Balke-Pearl bounds on the
ATE resulting from Theorem 1. The true bounds are plotted in blue and estimated 95%
confidence intervals (from one simulated sample of size 5,000) for the bounds are plotted in
green. Thecovariate-agnosticbounds, whilesimpletocomputeasthemaximumandminimum
of 8 (true or empirical) probabilities, are quite wide and cover the null treatment effect of zero.
The covariate-adjusted bounds, on the other hand, are very narrow and are bounded away
6−0.1 0.0 0.1 0.2 0.3Average Treatment Effect
Covariate−Agnostic Covariate−AdjustedFigure 1: Balke-Pearl bounds for the ATE, with and without covariate adjustment. The red
dashed line represents the true ATE. Blue bars represent the true theoretical bounds, and
green bars are estimated 95% confidence intervals from a single sample of 5,000 subjects (with
10-fold cross-fitting for adjusted approach). The solid black line indicates the reference value
of zero average effect.
fromzero. EmployingtheestimatorweproposeinSection4, withflexibleregressiontree-based
nuisance function estimation, we obtain valid and narrow estimated bounds on the ATE.
Note that in this example, the covariates X1,X2happen to comprise all confounders of
the relationship between AandY, so that direct adjustment (e.g., with the g-formula) would
identify the ATE. A practitioner without this knowledge, but with the foresight to measure
X1andX2, could still use these covariates to construct informative bounds. We also note that
our example is somewhat extreme, as we would not typically expect the covariate-adjusted
bounds to always be so narrow. However, we show below that in this experimental design, the
covariate-adjusted bounds are guaranteed to be at least as narrow as unadjusted bounds, even
in the presence of residual exposure-outcome confounding given X. Indeed, one will want to
adjust for Xwhenever these covariates are predictive of AandY.
73.3 Width of the Bounds
In the illustration above, both choices of adjustment sets (no covariates or (X1, X2)) result in
valid bounds since Assumptions 1–4 are satisfied with either choice. In our example, the
covariate-assisted Balke-Pearl bounds improve substantially upon naïve covariate-agnostic
bounds. In fact, such additional adjustment in general cannot do worse, and can only im-
prove upon unadjusted bounds. We formalize this idea in the following result.
Proposition 1. Suppose Xrenders Za valid instrument according to Assumptions 1–4.
Suppose that Assumptions 1–4 also hold after augmenting XwithG, and Z⊥ ⊥G|X, i.e., G
doesn’t predict Z, except possibly through association with X. Then Balke-Pearl bounds based
on(X,G)are at least as narrow as those based on X, and may be strictly narrower.
According to Proposition 1, we would want to include in Xany pure predictors of (A, Y)
that are not direct confounders of the effect of ZonAandY, when constructing Balke-Pearl
bounds. This is somewhat analogous to Lemma 4 in Rotnitzky and Smucler (2020), which says
that influence function-based estimation of the ATE from an observational study is improved
by adding pure outcome predictors—so-called “precision variables”—to an already sufficient
set of confounders. Note, however, that in that setting improvement corresponded to lower
variance, whereas here we are concerned both with the width of the theoretical bounds, as well
as the variance of our estimators. Specializing Proposition 1 to the case where Zis marginally
randomized yields the following corollary:
Corollary 1. Suppose Z⊥ ⊥Xand Assumptions 1–4 hold unconditionally as well as given
X. Then Balke-Pearl bounds based on Xare at least as narrow as unadjusted bounds.
Corollary 1 explains what we observed in the motivating example of Section 3.2. That
is, a marginally randomized IV guarantees that inclusion of baseline covariates Xwill result
in bounds that are no worse than unadjusted bounds. The degree to which such inclusion
narrows the bounds is of great interest, and will help in deciding which covariates to measure
in practice—a more precise characterization of this improvement we leave for future research.
For the remainder of the paper, we focus on constructing valid and efficient estimators of the
covariate-assisted Balke-Pearl bounds.
4 Bound Estimation Under a Margin Condition
As mentioned in Section 3.1, the bound functionals L(P) =EP(γℓ(X))andU(P) =EP(γu(X))
are not pathwise differentiable without further restrictions, and therefore do not have influence
functions to enable estimation as described above. This is because the pointwise maximum,
γℓ(X) = max 1≤j≤8θℓ,j(X), and pointwise minimum, γu(X) = min 1≤j≤8θu,j(X), are not
differentiable everywhere. Indeed, we should not expect it to be possible in the nonparametric
model to estimate the covariate-adjusted bounds at parametric rates. In this section, we
introduce an additional assumption known as a margin condition that renders the bounds
pathwise differentiable. Further, we describe estimators of the bounds that exploit the margin
condition in order to achieve faster rates. For a broader overview of the use of influence
functions for efficient and robust estimation, see Appendix C.
4.1 An infeasible estimator
To motivate the proposed estimator, we first consider an infeasible estimator of the bounds.
Letdℓ(X)∈arg max1≤j≤8θℓ,j(X)anddu(X)∈arg min1≤j≤8θu,j(X). The bounds can thus
8be written as
L(P) =8X
j=1E[{dℓ(X) =j}θℓ,j(X)]andU(P) =8X
j=1E[{du(X) =j}θu,j(X)].
For each y, a, z ∈ {0,1}, define
ψya.z(O;P) =1(Z=z)
λz(X){1(Y=y, A=a)−πya.z(X)}.
Under the assumption that x7→dℓ(x)andx7→du(x)are known functions, and are unique
maximizers and minimizers, respectively, it can be shown that the uncentered influence func-
tions of L(P)andU(P)are
φℓ(O;P, dℓ) =8X
j=11{dℓ(X) =j}{Lj(O;P) +θℓ,j(X)}
φu(O;P, du) =8X
j=11{du(X) =j}{Uj(O;P) +θu,j(X)}
where Lj(O;P)andUj(O;P)are obtained by replacing πya.z(X)with ψya.z(O;P)inθℓ,j(X)
andθu,j(X)in display (2), respectively, and omitting the constant 1whenever it appears, e.g.,
L1=ψ11.1+ψ00.0, U1=−ψ01.1−ψ10.0.
Therefore, under minor regularity conditions, an “infeasible” estimator eL=Pn{φℓ(O;bP, dℓ)},
i.e., requiring complete knowledge of dℓ, would satisfy
√n(eL − L (P))⇝N(0,VarP{φℓ(O;P, dℓ)})
as long as P{φℓ(O;P, dℓ)−φℓ(O;bP, dℓ)}=oP(n−1/2). In this regime, eLwould be efficient.
4.2 Estimation & Inference
In light of the discussion in the section above, we propose estimating the bounds with
bL=8X
j=1Pnh
1{bdℓ(X) =j}{Lj(O;bP) +bθℓ,j(X)i
=Pn{φℓ(O;bP,bdℓ)},
bU=8X
j=1Pnh
1{bdu(X) =j}{Uj(O;bP) +bθu,j(X)i
=Pn{φu(O;bP,bdu)}.
That is, to estimate the non-smooth components of the bounds L(P)andU(P), namely the
indicators 1{dℓ(X) =j}and1{du(X) =j}, we use plug-in estimators 1{bdℓ(X) =j}and
1{bdu(X) =j}, where bdℓ∈arg max1≤j≤8bθℓ,jandbdu∈arg max1≤j≤8bθu,j. Anaturalquestionis
then under what conditions, if any, the estimators bLandbUbehave, at least asymptotically, like
their infeasible counterparts eLandeU. As shown in the next theorem, a sufficient condition
for such oracle behavior is captured by the following “margin” condition. This additional
assumption controls the probability that the minimum and maximum are near their points of
non-differentiability.
9Assumption 5 (Margin condition) .There exists α >0such that for any t≥0,
P
min
j̸=dℓ(X){θℓ,dℓ(X)(X)−θℓ,j(X)} ≤t
≲tα, (5)
and
P
min
j̸=du(X){θu,j(X)−θu,du(X)(X)} ≤t
≲tα. (6)
The margin condition in Assumption 5 is very similar to conditions that have been pro-
posed and leveraged in the classification literature (Audibert and Tsybakov, 2007), as well
as in dynamic treatment regimes (Luedtke and van der Laan, 2016) and other instrumental
variable problems (Kennedy et al., 2020). In words, condition (5) says that with high prob-
ability, θℓ,dℓis separated from non-maximal lower bound values θℓ,j. Similarly, condition (6)
limits how close non-minimal upper bound values θu,jare to the actual minimum θu,du. If
minj̸=dℓ(X){θℓ,dℓ(X)(X)−θℓ,j(X)}, for instance, has bounded density near zero, then (5) will
hold with α= 1. This is a relatively weak requirement which we expect to hold in many
cases. Under Assumption 5, we are able to derive sufficient conditions such that bLandbUare√n-consistent and asymptotically normal.
Theorem 2. Suppose that the nuisance functions bπya.zandbλzare estimated from a separate
independent sample. Moreover, suppose that Assumption 5 holds, P
ϵ≤bλ1(X)≤1−ϵ
= 1,
for some ϵ >0,bλ1−λ1=oP(1), and max y,a,z∈{0,1}∥bπya.z−πya.z∥=oP(1). Then, we have
bL − L = (Pn−P)φℓ(O;P, dℓ)
+OPbλ1−λ1·max
y,a,z∈{0,1}∥bπya.z−πya.z∥+ max
1≤j≤8bθℓ,j−θℓ,j1+α
∞
+oP(n−1/2),
and
bU − U = (Pn−P)φu(O;P, du)
+OPbλ1−λ1·max
y,a,z∈{0,1}∥bπya.z−πya.z∥+ max
1≤j≤8bθu,j−θu,j1+α
∞
+oP(n−1/2).
In this result, we assume the nuisance functions are estimated on separate independent
dataforsimplicity: inpracticewithonerandomsample, onecansplitthesampleandusecross-
fitting to achieve the same asymptotic behavior (Zheng and van der Laan, 2010; Chernozhukov
et al., 2018). Sample splitting allows us to avoid complicated empirical process conditions
(Chernozhukov et al., 2016; Kennedy et al., 2020) and derive bounds on the conditional bias
of the estimator in terms of the convergence rate of the nuisance functions.
Note that in Theorem 4 we do not require the individual nuisance functions to converge
at√n-rates. The conditions are on the product of convergence rates, or rates raised to a
power greater than 1. This is a key advantage of a robust estimator: after we correct for the
first-order bias, the remaining bias only involves higher-order terms and is much smaller. The
result of Theorem 2 establishes that bL, for instance, is√n-consistent as long as
bλ1−λ1·max
y,a,z∈{0,1}∥bπya.z−πya.z∥+ max
1≤j≤8bθℓ,j−θℓ,j1+α
∞=oP(n−1/2)
10The first product-bias term results from estimation of θℓ,jwith Lj(O;bP) +bθℓ,j. The second
term denotes a bound on the bias arising plug-in estimation of the indicators 1{dℓ(x) =j}
and depends on the exponent αfrom Assumption 5. For example, as long as the quantity
minj̸=dℓ(X){θℓ,dℓ(X)−θℓ,j(X)}has a bounded density near zero, then Assumption 5 holds with
α= 1. In this case, max y,a,z∈{0,1}∥bπya.z−πya.z∥∞=oP(n−1/4)is typically sufficient to imply
max
1≤j≤8bθℓ,j−θℓ,j1+α
∞=oP(n−1/2).
Finally, Theorem 2 outlines sufficient conditions for the asymptotic normality of bLandbUso
that Wald-type confidence intervals are straightforward to compute. In particular, under the
conditionsfor√n-consistency, wecanconstructanasymptoticallyvalid 100(1−δ)%confidence
interval for the ATE with

bL −z1−δ/2q
bV /n,bU+z1−δ/2q
cW/n
,
where bV=Pnn
φℓ(O;bP,bdℓ)−bLo2
,bV=Pnn
φu(O;bP,bdu)−bUo2
, and where zβis the β-
th quantile of the standard normal distribution. Moreover, the procedure proposed in Imbens
and Manski (2004) can be used to conduct more precise inferences (see also Theorem 3 in
Jiang and Ding (2018)).
We highlight that marginally randomized instruments present a special case of interest. In
this case, Z⊥ ⊥Xholds by randomization of treatment assignment Zandλz(X)is equal to
a known constant λzby design. Here, the requirement for the convergence rate is reduced to
max 1≤j≤8bθℓ,j−θℓ,j
∞=oP
n−1
2(1+α)
.
5 Targeting Smooth Approximations
As an alternative to direct estimation of the bounds under a margin condition, our sec-
ond proposal is to instead target approximations to the bounds, Lg(P) =EP(g(θℓ(X)))and
Uh(P) =EP(h(θu(X))), where g, h: [−1,1]8→Rare sufficiently smooth approximate point-
wise maximum and minimum functions, respectively.
5.1 Approximation Based on the LSE Function
While other approximations are possible, we will focus on the log-sum-exp (LSE) function as
an approximate maximum. Namely, for any t >0, define gt:Rk→Rvia
gt(v) =1
tlog
kX
j=1etvj
,forv∈Rk. (7)
The LSE is a smooth convex function (Boyd et al., 2004), with gradient and Hessian given by
∇gt(v) =z
1Tz,∇2gt(v) =t
(1Tz)2 
1Tz
diag(z)−zzT
,
where z= (etv1, . . . , etvk). Importantly, for our purposes, the following inequality shows that
LSE approximates the pointwise maximum function:
max{v1, . . . , v k}< gt(v)≤max{v1, . . . , v k}+logk
t.
11Increasing the tuning parameter tyields smaller approximation error. On the other hand, the
Hessian matrix of gtalso depends critically on t; as we will see in the following discussions,
astincreases, the operator norm∇2gt(v)
opincreases, which may induce larger estimation
error.
In the context of our problem, we replace the maximum function in EP(max 1≤j≤8θℓ,j(X))
with the LSE function and focus on estimating the smooth functional EP(gt(θℓ(X))). By the
approximation property, EP(gt(θℓ(X)))satisfies
E
max
1≤j≤8θℓ,j(X)
≤E(gt(θℓ(X)))≤E
max
1≤j≤8θℓ,j(X)
+log 8
t.
We can similarly define a smooth approximation for pointwise minimum function as ht=g−t
andestimatethesmoothfunctional EP(ht(θu(X)))fortheupperbound EP(min 1≤j≤8θu,j(X)).
The smooth approximation for the minimum function satisfies
E
min
1≤j≤8θu,j(X)
−log 8
t≤E(ht(θu(X)))≤E
min
1≤j≤8θu,j(X)
.
In the remaining part of this section, we develop efficiency theory for these smooth functional
approximations of the Balke-Pearl bounds, and propose robust and efficient estimators.
5.2 Efficiency Theory
Inthefollowingtheorem, wepresentthenonparametricefficientinfluencefunctionforfunction-
als of the form Lg(P) =EP(g(θℓ(X)))andUh(P) =EP(h(θu(X))), where g, h: [−1,1]87→R
are smooth functions with continuous first-order derivatives.
Theorem 3. The nonparametric influence function of Lg(P)is
˙Lg(O;P) =g(θℓ(X))− L g(P) +8X
j=1∂g(θℓ(X))
∂θℓ,j(X)Lj(O;P),
where Lj(O;P)is defined in Section 4.1. Similarly, the nonparametric influence function of
Uh(P)is
˙Uh(O;P) =h(θu(X))− Uh(P) +8X
j=1∂h(θu(X))
∂θu,j(X)Uj(O;P),
again with Uj(O;P)defined in Section 4.1.
Theorem 3 implies that there are two terms contributing to the influence functions (and
hence our proposed estimators to follow) of the smooth functionals Lg(P)andUh(P). The
first term for the lower bound, g(θℓ(X))− L g(P), is augmented with the second term,P8
j=1∂g(θℓ(X))
∂θℓ,j(X)Lj(O;P), which effectively reduces bias that results from estimating the un-
known functions πya.zandλz.
After characterizing the influence functions for Lg(P)andUh(P), we can use them to
correct for the first-order bias in the von Mises expansion and arrive at robust estimators,
which allows us to perform estimation and inference efficiently.
125.3 Estimation & Inference
Next, weproposeandanalyzearobustestimatorforthesmoothlowerboundfunctional, Lg(P).
Similar results hold for Uh(P)using the same arguments. As in Section 4, we assume that
we train models for the nuisance functions πya.z(X), λz(X)based on a separate independent
sample Dn. The robust estimator is defined as:
bLg=Lg(bP) +Pnh
˙Lg(O;bP)i
=Pn
g
bθℓ(X)
+8X
j=1∂g
bθℓ(X)
∂bθℓ,j(X)Lj(O;bP)

Thefollowingtheoremcharacterizestheconditionalbias(giventhetrainingdata)oftherobust
estimator bLgand establishes its asymptotic normality under additional conditions.
Theorem 4. Suppose g:R87→Ris twice continuously differentiable, such that ∥∇g(θ)∥∞≤
C1and∥∇2g(θ)∥op≤C2uniformly over θ, and the nuisance functions bπya.z,bλzare estimated
from a separate independent sample, Dn. Moreover, suppose there exists positive constant ϵ
such that
P
ϵ≤bλ1(X)≤1−ϵ
= 1.
Then the conditional bias of the robust estimator bLgcan be bounded as
E[bLg|Dn]− L g(P)
≲
max
y,a,z∈{0,1}∥bπya.z−πya.z∥
C1bλ1−λ1+C2max
y,a,z∈{0,1}∥bπya.z−πya.z∥
.
Letf(O) = ˙Lg(O;P) +Lg(P)be the non-centered influence function. If we further assumebf−f=oP(1)and the nuisance estimators satisfy
bλ1−λ1
max
y,a,z∈{0,1}∥bπya.z−πya.z∥
=oP(n−1/2),
max
y,a,z∈{0,1}∥bπya.z−πya.z∥2=oP(n−1/2),
then we have
bLg− L g(P) =Pnh
˙Lg(O;P)i
+oP(n−1/2),
implying the robust estimator is√n-consistent and achieves the nonparametric efficiency
bound.
Similar to the analysis following Theorem 2, Theorem 4 implies that if the nuisance es-
timators satisfybλ1−λ1=OP 
n−1/4
andmax y,a,z∈{0,1}∥bπya.z−πya.z∥=oP 
n−1/4
, the
robust estimator will be√n-consistent and achieve the efficiency bound. Comparing the direct
estimator from Section 4 with the approximation-based estimator here, when Assumption 5
holds with α= 1then the requirements on the bias of Theorem 2 are essentially the same as
those of Theorem 4—this is because estimation in L∞typically yields the same convergence
rate as estimation in L2up to a logfactor (Tsybakov, 2009).
13Remark 1. It is natural to consider, assuming that the margin condition described in As-
sumption 5 holds, (a) whether and how the behavior of the smooth approximation estimator
bLgmight improve, (b) whether and how to choose a sequence of tuning parameters tnin the
LSE function (or some other smooth approximation to the pointwise maximum) to minimize
the convergence rate and mean square error of bLgtnwith respect to L(P), and (c) if we can
describe minimax optimal estimators of L(P)under a large class of models (e.g., nuisance
functions belonging to Hölder smooth classes). Careful analysis in a simplified setting (omit-
ted here) reveals that while the approximation error |Lgtn(P)− L(P)|improves from O(1
tn)to
O(1
t1+α
n)under a margin condition, the direct estimator dominates the smooth approximation
even for optimally chosen data-adaptive tuning parameters tn. Indeed, we conjecture that the
direct estimator bL, modulo higher order influence function corrections (Robins et al., 2008,
2009, 2017), is rate-optimal (up to log factors) when Assumption 5 holds. We leave precise
minimax optimal rate characterization—for non-smooth functionals similar to L(P)andU(P)
under a margin condition akin to Assumption 5—to be investigated in future research.
5.4 Wald-type Confidence Interval for the ATE
Combining Theorem 1 with the smooth LSE-based approximation we have
Lgt(P)−log 8
t≤E[Y(a= 1)−Y(a= 0)]≤ Uht(P) +log 8
t.
Thus the interval

bLgt−log 8
t−z1−δ/2q
bVt/n,bUht+log 8
t+z1−δ/2q
cWt/n
is an asymptotically valid (though potentially conservative) 100(1−δ)%Wald-type confidence
interval for the ATE, where bVt,cWtare plug-in estimators of the nonparametric efficiency
bounds VarP
˙Lgt(O;P)
andVarP
˙Uht(O;P)
, respectively. The choice of the tuning pa-
rameter trequires balancing the smooth approximation error log 8/tand the conditional bias
termE[bLg|Dn]− L g(P). Note that ∥∇2gt(θ)∥op≤tand Theorem 4 implies the conditional
bias increases as we select a larger t. Hence a smaller tis preferred to reduce the conditional
bias. Yet reducing the approximation error log 8/trequires a larger t. How to choose tin a
data-driven fashion to arrive at the shortest confidence interval remains an open problem left
for future investigation.
6 Direct Estimation with Continuous Outcomes
Thus far, we have only focused on a binary outcome Y∈ {0,1}. In this section, we will
extend our approaches from the binary outcome case to construct valid bounds on the ATE
for continuous outcomes. In fact, we will assume only that the outcome Yis bounded; without
lossofgenerality, wemayassume Y∈[0,1](otherwiseonecanalwaysrescaletheoutcome). As
observed but not studied in Balke and Pearl (1997), the idea is to replace the binary outcome
in previous analyses with the indicator 1(Y≤t). Specifically, for each t∈[0,1],1(Y≤t)is a
binary outcome for which we can apply the bounds proposed in Section 3.1 to obtain pointwise
bounds on the difference in the distribution functions of potential outcomes P(Y(a)≤t). We
proceed by integrating the tail probabilities P(Y(a)> t)to bound the mean of potential
outcomes E(Y(a)), and finally arrive at bounds on the ATE, E(Y(a= 1)−Y(a= 0)).
14For any a, z∈ {0,1}andt∈R, define the probabilities π1a.z(t,X) =P(Y≤t, A=a|
X, Z=z)andπ0a.z(t,X) =P(Y > t, A =a|X, Z=z). Next, we define functions exactly
as in (2) and (3), replacing with πya.z(X)with πya.z(t,X)(note that θu,j,θℓ,j,γℓ, and γuare
then also functions of tandX). Assuming Zis a valid instrument given covariates X, for each
t∈Rwe view 1(Y≤t)as the binary outcome and apply the “Balke-Pearl” bounds to this
new outcome. By Theorem 1, we obtain the following bounds on the difference in distribution
functions of the individual potential outcomes:
γℓ(t,X)≤P[Y(a= 1)≤t|X]−P[Y(a= 0)≤t|X]≤γu(t,X).
Integrating with respect to Xand noting that E(Y(a)) =R1
0P[Y(a)> t]dt, we arrive at the
following valid (though not necessarily tight) bounds on the ATE:
−Z1
0E(γu(t,X))dt≤E(Y(a= 1)−Y(a= 0))≤ −Z1
0E(γℓ(t,X))dt. (8)
Onenaturalapproachwouldbetoestimatethefunctions t7→E(γu(t,X))andt7→E(γℓ(t,X))
bytheproposalsofSection4orSection5, overagridofvalues t∈[0,1], andusetheseestimates
to approximate the integrated bounds given in (8). Alternatively, one could use Monte Carlo
methods to approximate the integral. Either approach would require computing a binary-
outcome estimator at many different inputs t, which could be quite computationally intensive.
The following theorem characterizes a looser bound that can be computed more efficiently.
Theorem 5. LetW∼Uniform(0 ,1)be independent of O= (X, Z, A, Y ). Then
Z1
0E(γu(t,X))dt≤E
min
1≤j≤8eθu,j(X)
,
Z1
0E(γℓ(t,X))dt≥E
max
1≤j≤8eθℓ,j(X)
,
where we define eπ1a.z(X) =P(Y≤W, A =a|X, Z=z),eπ0a.z(X) =P(Y > W, A =a|
X, Z=z), and where eθℓ,j(X)andeθu,j(X)are defined exactly as in equations (2)and(3),
replacing πwitheπ.
Theorem 5 together with (8) implies the following valid bounds on the ATE:
−E
min
1≤j≤8eθu,j(X)
≤E[Y(1)−Y(0)]≤ −E
max
1≤j≤8eθℓ,j(X)
. (9)
Note that the bounds in (9) are of the same form as those in Theorem 1, but with a new binary
outcome 1(Y≤W). By introducing an independent uniform random variable, we obtain a
looser bound, but one that lends itself to more computationally feasible estimation. To oper-
ationalize these bounds, we can simulate ni.i.d. Uniform(0 ,1)random variables W1, . . . , W n,
independent of the data, and construct the augmented observation unit eO= (X, Z, A, Y, W ).
We then estimate the quantities in the bounds exactly as in the binary case using the methods
proposed in Section 4 or Section 5, with the new binary outcome being 1(Y≤W). Note that
the exact value of the final bounds will depend on the realization of W1, . . . , W n. To reduce
this variability and regain some efficiency, we may repeat this process mtimes and average
the resulting estimates of the bounds—see Proposition 3 in Appendix D for an illustration of
this behavior in a simplified setting.
157 Simulation Study
In order to assess the performance of the proposed estimators, we conducted a small simulation
study. In particular, we consider a simplified scenario where Assumption 5 holds with margin
parameter α= 1, and demonstrate for different sample sizes nhow mean squared errors of the
proposed lower bound estimators bLandbLgtcompare to that of a plug-in estimator. We find
that, as anticipated by our theoretical results, our proposed estimators generally have smaller
error, and achieve parametric levels of error when the root-mean-square error of nuisance
estimates is less than O(n−1/4). Details and full results are included in Appendix E.
8 Data Analysis
To illustrate the proposed methods on real data, we aimed to estimate the effect of higher
education on wages later in life using a subset of data from the National Longitudinal Survey
of Young Men (Card, 1995), as previously analyzed, for instance, in Tan (2006); Wang and
Tchetgen Tchetgen (2018). For ease of comparison, we replicate the setup of Wang and Tch-
etgen Tchetgen (2018), and use the n= 3010participants (males between 14 and 24 years in
1966) for whom survey responses for education and wage in 1976 were available. In particular,
the instrument Zis taken to be an indicator of proximity to a 4-year college, the exposure A
an indicator of education beyond high school, and the outcome Yan indicator of wages in 1976
exceeding the median value. In addition to (Z, A, Y ), we adjusted for a collection of baseline
covariates X: age, parental educational attainment, indicators for living in the south and a
metropolitan area, race, intelligence quotient scores, as well as variable-specific missingness
indicators. As in Card (1995) and Wang and Tchetgen Tchetgen (2018), mean imputation was
used for missing covariates, and, due to non-representativeness of the sample, the observations
were reweighted using sampling weights (results from unweighted analyses were qualitatively
similar).
Wang and Tchetgen Tchetgen (2018) consider the binary instrumental variable problem
(i.e., Z, A, Y ∈ {0,1}) as we do, and propose several estimators that respect that the CATE
and ATE must be bounded between −1and 1. Note that, in addition to the assumptions
adopted here which underpin the Balke-Pearl bounds, these authors introduced a structural
assumption—asserting no additive interaction between unmeasured confounders and either (i)
the instrument, on the treatment, or (ii) the treatment, on the outcome—which results in
point identification of the CATE and thus the population ATE. That is, their methods rely
on strictly stronger assumptions than ours.
We computed estimates of the covariate-adjusted bounds L(P)andU(P)using the direct
estimators bLandbUdevelopedinSection4, assumingthemargincondition(i.e., Assumption5).
Specifically, we used separate ensembles based on fits from glm,rpart,ranger, and polymars
using the SuperLearner package in R(Polley et al., 2019) to obtain bλ1(X)and{bπya.z(X) :
y, a, z ∈ {0,1}}. We also computed LSE-based estimators bLgtandbUhtbased on the same
nuisance function estimators, as described in Section 5, with the ad hoctuning parameter
choice t= 100 n1/4. For all estimators, we used cross-fitting with 5 folds.
Summarizing the results, the point estimates for the covariate-adjusted Balke-Pearl bound
estimators bLandbUwere (−0.405,0.546), andtheresultingWald-based95%confidenceinterval
for the ATE was (−0.454,0.597). These results indicate a wide range of possible effects com-
patible with the observed data, with the width perhaps demonstrating the potentially strong
effects of unmeasured confounders. Nevertheless, quite surprisingly, compared to the bounds
16reported in Wang and Tchetgen Tchetgen (2018), the confidence interval width is narrower
and the estimated upper bound is more informative. For instance, the point estimates and
95% confidence intervals for their proposed “ g-estimator” and “bounded multiply robust esti-
mator” were 0.079 (−0.355,1.000)and 0.344 (−0.373,0.938), respectively. This discrepancy
may arise as their estimators rely on unstable weights which do not feed into our approach
(i.e., the nonparametric efficiency bounds for the Balke-Pearl bound functionals are much less
than that for the functional targetted by these authors), or because the constraints implied
by the binary instrumental variable model are not explicitly incorporated in their approaches
(cf. Proposition 1 in Wang and Tchetgen Tchetgen (2018)).
The approximation-based estimators bLgtandbUhtwere nearly identical, (−0.406,0.544),
and the associated 95% confidence interval constructed as in Section 5.4 was (−0.506,0.646):
the extra width is due to the inclusion of the (potentially conservative) error bound log (8) /t.
9 Discussion
In this paper, we proposed estimators of nonparametric bounds on the average treatment
effect using an instrumental variable, avoiding strong structural or parametric assumptions
typically used for point identification. We extended the classic approach of Balke and Pearl
(1997) by incorporating baseline covariate information to (i) control for measured confounders
to render the instrument valid, and/or (ii) construct narrower and hence more informative
bounds. We also proposed a concrete extension to bound the ATE for a general bounded
outcome. Our estimators are based on influence functions, and as a result are robust and
can attain√n-consistency, asymptotic normality, and nonparametric efficiency, under non-
parametric convergence rates of the component nuisance functions estimation—these rates are
attainable under sparsity, smoothness or other structural conditions.
The key difficulty in estimating the covariate-adjusted bound functionals L(P)andU(P)
is that, as means of non-differentiable functions, they are not pathwise differentiable (Bickel
et al., 1993). As a result, these parameters do not—at least without further assumptions—
have an influence function to facilitate flexible and efficient estimation (e.g., see Section 5.3
of Kennedy (2022)). To make progress, in Section 4, we first proposed and invoked a margin
conditiontorendertheexactboundfunctionalspathwisedifferentiable, andproposedinfluence
function-based estimators under this condition. Second, in Section 5, we presented general
valid bounds on the ATE via estimation of pathwise differentiable approximations to L(P)and
U(P). The second approach has the advantage of not requiring the margin condition, but the
resulting bounds suffer from being slightly conservative, and not converging to the true exact
bounds at fast rates. The first approach, on the other hand, can achieve parametric rates
when the margin condition holds. As a general guideline, we recommend that practitioners
use the direct bound estimators proposed in Section 4, deferring to the approximate bounds
only when there is serious doubt on Assumption 5 arising due to subject matter knowledge.
In our view, it is difficult to imagine concrete scenarios in which Assumption 5 would be
violated. A similar margin condition is invoked in dynamic treatment regime problems, for
example when estimating the mean outcome under the optimal treatment policy in a point
treatment setting: E[max{E(Y(a= 1)|X),E(Y(a= 0)|X)}](Luedtke and van der Laan,
2016). In that context, one must argue that the CATE is well separated from zero, the value
representing no treatment effect. It is plausible that there is a subgroup for which the CATE
is exactly zero, which would violate the margin condition for that problem. In our setting,
Assumption 5 requires separation of the pointwise maximum (minimum) of the Balke-Pearl
17lower (upper) bound functions θℓ(X)(θu(X)) from the second largest (smallest) value. These
lower and upper bound functions are not readily interpretable like the CATE, and it would
seem to necessitate an unlikely confluence of factors to violate the required separation.
The approaches developed in this paper are not inherently tied to the specific bounds
we studied: the same ideas extend to many functionals defined as expectations of non-smooth
functions, whichariseofteninproblemstargettingboundsoncausaleffects. Indeed, onemonth
after our work was posted on arXiv, Semenova (2023) independently considered estimation of
bounds, and functionals theoreof, that can be expressed as the minimum of several nuisance
functions. They studied a wide range of bounds obtained from certain optimization problems
and proposed estimators under a margin condition similar to Assumption 5. The Balke-
Pearl bounds arise from a linear programming specification, which is a special case of bounds
based on general optimization problems in Semenova (2023). However, our work specifically
characterizes the nonparametric efficient estimator of the Balke-Pearl bounds, as well as its
asymptotic bias, in Theorem 2. Further, a direct application of the approach of Semenova
(2023) to our setting would require that instrument probabilities λ1(X)are known. We also
propose estimators based on smooth approximation—not considered in Semenova (2023)—and
establish corresponding efficiency theory. We expect that the methods discussed in this paper
can be combined with the insights from Semenova (2023) to yield efficient, debiased estimators
in more general settings as considered in their paper.
It is important to mention some limitations of the proposed methods, as well as possible
extensions and open problems. First, we have throughout assumed a fixed collection of mea-
sured covariates X. In Proposition 1, we provide a general criterion to justify adjusting for
certain covariates, however, it remains to characterize (i) the actual difference in the length
of the bounds based on two valid adjustment sets, and (ii) the effect of the adjustment set
on the variance of the proposed estimators. Second, we have focused primarily on the set-
ting with instrument, exposure, and outcome all binary variables, since this is the simplest
setting for partial identification with closed form bounds given by Balke and Pearl (1997).
In Section 6, we provided an extension to construct valid ATE bounds for continuous out-
comes, though an important open problem is determining the sharpest possible bounds for
such general outcomes, beyond the binary case. Along the same lines, it of course will also
be of interest to consider multi-valued or continuous instruments and exposures. In these
cases, there is generally no longer the same closed form solution for the theoretical bounds,
and alternative optimization specifications and/or symbolic bounds might be incorporated
(Sachs et al., 2022; Zhang and Bareinboim, 2021; Duarte et al., 2023). When closed-form
solutions are available (see examples in Sachs et al. (2022)), similar direct estimation and
targeting smooth approximation strategies can be applied to estimate the bounds efficiently.
Duarte et al. (2023) recently proposed a general framework to automatically compute sharp
bounds on causal effects under user-specified causal assumptions. Their idea is to formalize
the assumptions and observed evidence as polynomial constraints, and estimands of interest
as objective functions, following which the bounds are obtained by optimizing the objective.
In this regard the Balke-Pearl bounds can be viewed as a special case where the causal as-
sumptions are specified by an IV model. However, the Balke-Pearl bounds admit closed-form
solutions, based on which efficiency estimation theory can be derived, while there is no closed-
form solutions to the general optimization problem in Duarte et al. (2023). Moreover, they
only focus on the fully discrete setting since the optimization problem is intractable in greater
generality, while the methodology in our work naturally handles continuous covariates (and
outcomes using the extension in Section 6). Third, an interesting problem we will explore in
future research is to directly and efficiently estimate the conditional bounds γℓ(X),γu(X)
18for the CATE given in Theorem 1, and identify strata for whom we are confident that the
CATE is positive or negative. In doing so, one may extend the ideas in Kennedy et al. (2020)
to define “sharpness” of an instrument—the ability to tightly bound causal effects in certain
subgroups—without relying on the typical monotonicity assumption.
Acknowledgements
This work was supported by the Patient-Centered Outcomes Research Institute (PCORI
Awards ME-2021C1-22355) and the National Library of Medicine, #1R01LM013361-01A1.
All statements in this report, including its findings and conclusions, are solely those of the
authors and do not necessarily represent the views of PCORI or its Methodology Committee.
19References
Aldous, D. (2005), “Spin Glasses: A Challenge for Mathematicians,” .
Angrist, J. D. and Evans, W. N. (1998), “Children and Their Parents’ Labor Supply: Evidence
from Exogenous Variation in Family Size,” The American Economic Review , 88, 450–477.
Angrist, J.D., Imbens, G.W., andRubin, D.B.(1996a), “IdentificationofCausalEffectsUsing
Instrumental Variables,” Journal of the American Statistical Association , 91, 444–455.
— (1996b), “Identification of Causal Effects Using Instrumental Variables,” Journal of the
American Statistical Association , 91, 444–455.
Audibert, J.-Y. and Tsybakov, A. B. (2007), “Fast learning rates for plug-in classifiers,” The
Annals of statistics , 35, 608–633.
Baiocchi, M., Cheng, J., and Small, D. S. (2014), “Instrumental variable methods for causal
inference,” Statistics in medicine , 33, 2297–2340.
Balke, A. and Pearl, J. (1993), “Nonparametric bounds on causal effects from partial compli-
ance data,” Tech. Rep. Technical Report R–199–J, University of California, Los Angeles.
— (1994), “Counterfactual probabilities: Computational methods, bounds and applications,”
inUncertainty Proceedings 1994 , Elsevier, pp. 46–54.
— (1997), “Bounds on treatment effects from studies with imperfect compliance,” Journal of
the American Statistical Association , 92, 1171–1176.
Balke, A. A. (1995), Probabilistic counterfactuals: semantics, computation, and applications ,
University of California, Los Angeles.
Bickel, P. J., Klaassen, C. A., Bickel, P. J., Ritov, Y., Klaassen, J., Wellner, J. A., and Ritov,
Y. (1993), Efficient and adaptive estimation for semiparametric models , vol. 4, Springer.
Boyd, S., Boyd, S. P., and Vandenberghe, L. (2004), Convex optimization , Cambridge univer-
sity press.
Cai, Z., Kuroki, M., and Sato, T. (2007), “Non-parametric bounds on treatment effects with
non-compliance by covariate adjustment,” Statistics in medicine , 26, 3188–3204.
Calafiore, G. C., Gaubert, S., and Possieri, C. (2020), “A universal approximation result
for difference of log-sum-exp neural networks,” IEEE transactions on neural networks and
learning systems , 31, 5603–5612.
Card, D. (1995), “Using Geographic Variation in College Proximity to Estimate the Return to
Schooling,” in Aspects of Labour Market Behavior: Essays in Honour of John Vanderkamp ,
eds. L. N. Christophides, E. K. G. and Swidinsky, R., Toronto: University of Toronto Press,
pp. 201–222.
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and
Robins, J. (2016), “Double/debiased machine learning for treatment and causalparameters,”
arXiv preprint arXiv:1608.00060 .
20Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and
Robins, J. M. (2018), “Double/debiased machine learning for treatment and structural pa-
rameters,” The Econometrics Journal , 21, C1–C68.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2012), “Central limit theorems and multi-
plier bootstrap when p is much larger than n,” Tech. rep., cemmap working paper.
Duarte, G., Finkelstein, N., Knox, D., Mummolo, J., and Shpitser, I. (2023), “An automated
approach to causal inference in discrete settings,” Journal of the American Statistical Asso-
ciation, 1–25.
Feller, A., Mealli, F., and Miratrix, L. (2017), “Principal score methods: Assumptions, exten-
sions, and practical considerations,” Journal of Educational and Behavioral Statistics , 42,
726–758.
Frangakis, C. A. and Rubin, D. B. (2002), “Principal Stratification in Causal Inference,”
Biometrics , 58, 21–29.
Grilli, L. and Mealli, F. (2008), “Nonparametric bounds on the causal effect of university
studies on job opportunities using principal stratification,” Journal of Educational and Be-
havioral Statistics , 33, 111–130.
Hernán, M. A. and Robins, J. M. (2006), “Instruments for Causal Inference: An Epidemiolo-
gists Dream,” Epidemiology , 17, 360–372.
Hernan, M. A. and Robins, J. M. (2019), Causal inference , CRC Boca Raton, forthcoming.
Imbens, G. (2014), “Instrumental variables: An econometrician’s perspective,” Statistical Sci-
ence, 29, 323–358.
Imbens, G. W. and Angrist, J. D. (1994), “Identification and Estimation of Local Average
Treatment Effects,” Econometrica , 62, 467–475.
Imbens, G. W. and Manski, C. F. (2004), “Confidence intervals for partially identified param-
eters,”Econometrica , 72, 1845–1857.
Jiang, Z. and Ding, P. (2018), “Using missing types to improve partial identification with
application to a study of HIV prevalence in Malawi,” The Annals of Applied Statistics , 12,
1831–1852.
Kennedy, E. H. (2016), “Semiparametric theory and empirical processes in causal inference,”
inStatistical causal inferences and their applications in public health research , Springer, pp.
141–167.
— (2022), “Semiparametric doubly robust targeted double machine learning: a review,” arXiv
preprint arXiv:2203.06469 .
Kennedy, E. H., Balakrishnan, S., and G’Sell, M. (2020), “Sharp instruments for classifying
compliers and generalizing causal effects,” The Annals of Statistics , 48, 2008–2030.
Lee, D. S. (2005), “Training, wages, and sample selection: Estimating sharp bounds on treat-
ment effects,” .
21Long, D. M. and Hudgens, M. G. (2013), “Sharpening bounds on principal effects with covari-
ates,”Biometrics , 69, 812–819.
Luedtke, A. R. and van der Laan, M. J. (2016), “Statistical inference for the mean outcome
under a possibly non-unique optimal treatment strategy,” Annals of statistics , 44, 713.
Manski, C. F. (1990a), “Nonparametric bounds on treatment effects,” The American Economic
Review, 80, 319–323.
— (1990b), “Nonparametric bounds on treatment effects,” The American Economic Review ,
80, 319–323.
— (1995), Identification problems in the social sciences , Harvard University Press.
Manski, C. F. and Pepper, J. V. (2000), “Monotone instrumental variables: With an applica-
tion to the returns to schooling,” Econometrica , 68, 997–1010.
— (2009), “More on monotone instrumental variables,” The Econometrics Journal , 12, S200–
S216.
Miratrix, L., Furey, J., Feller, A., Grindal, T., and Page, L. C. (2018), “Bounding, an accessible
methodforestimatingprincipalcausaleffects, examinedandexplained,” Journal of Research
on Educational Effectiveness , 11, 133–162.
Polley, E., LeDell, E., Kennedy, C., Lendle, S., and van der Laan, M. (2019), “Package ‘Su-
perLearner’,” CRAN.
Robins, J. (1986), “A new approach to causal inference in mortality studies with a sustained
exposure period—application to control of the healthy worker survivor effect,” Mathematical
modelling , 7, 1393–1512.
Robins, J., Li, L., Mukherjee, R., Tchetgen, E. T., and van der Vaart, A. (2017), “Higher order
estimating equations for high-dimensional models,” Annals of statistics , 45, 1951.
Robins, J., Li, L., Tchetgen, E., van der Vaart, A., et al. (2008), “Higher order influence
functions and minimax estimation of nonlinear functionals,” in Probability and statistics:
essays in honor of David A. Freedman , Institute of Mathematical Statistics, pp. 335–421.
Robins, J., Li, L., Tchetgen, E., and van der Vaart, A. W. (2009), “Quadratic semiparametric
von mises calculus,” Metrika, 69, 227–247.
Robins, J. M. (1989), “The analysis of randomized and non-randomized AIDS treatment trials
using a new approach to causal inference in longitudinal studies,” Health Service Research
Methodology: a Focus on AIDS , 113–159.
— (1994), “Correcting for non-compliance in randomized trials using structural nested mean
models,” Communications in Statistics-Theory and methods , 23, 2379–2412.
Rotnitzky, A. and Smucler, E. (2020), “Efficient Adjustment Sets for Population Average
Causal Treatment Effect Estimation in Graphical Models,” Journal of Machine Learning
Research , 21, 1–86.
22Sachs, M. C., Jonzon, G., Sjölander, A., and Gabriel, E. E. (2022), “A general method for
deriving tight symbolic bounds on causal effects,” Journal of Computational and Graphical
Statistics , 1–23.
Scharfstein, D. O., Rotnitzky, A., and Robins, J. M. (1999), “Adjusting for nonignorable
drop-out using semiparametric nonresponse models,” Journal of the American Statistical
Association , 94, 1096–1120.
Semenova, V. (2023), “Adaptive Estimation of Intersection Bounds: a Classification Ap-
proach,” arXiv preprint arXiv:2303.00982 .
Swanson, S. A., Hernán, M. A., Miller, M., Robins, J. M., and Richardson, T. S. (2018),
“Partial identification of the average treatment effect using instrumental variables: review
of methods for binary instruments, treatments, and outcomes,” Journal of the American
Statistical Association , 113, 933–947.
Tan, Z. (2006), “Regression and weighting methods for causal inference using instrumental
variables,” Journal of the American Statistical Association , 101, 1607–1618.
— (2010), “Marginal and nested structural models using instrumental variables,” Journal of
the American Statistical Association , 105, 157–169.
Tsiatis, A. A. (2006), Semiparametric theory and missing data , Springer.
Tsiatis, A. A., Davidian, M., Zhang, M., and Lu, X. (2008), “Covariate adjustment for two-
sample treatment comparisons in randomized clinical trials: a principled yet flexible ap-
proach,” Statistics in medicine , 27, 4658–4677.
Tsybakov, A. B. (2009), Introduction to Nonparametric Estimation , Springer, 1st ed.
van der Vaart, A. W. (2000), Asymptotic statistics , vol. 3, Cambridge university press.
Vershynin, R. (2018), High-dimensional probability: An introduction with applications in data
science, vol. 47, Cambridge university press.
Wainwright, M. J. (2019), High-dimensional statistics: A non-asymptotic viewpoint , vol. 48,
Cambridge University Press.
Wang,L.andTchetgenTchetgen,E.(2018),“Bounded,efficientandmultiplyrobustestimation
of average treatment effects using instrumental variables,” Journal of the Royal Statistical
Society: Series B (Statistical Methodology) , 80, 531–550.
Yang, L. and Tsiatis, A. A. (2001), “Efficiency study of estimators for a treatment effect in a
pretest–posttest trial,” The American Statistician , 55, 314–321.
Zhang, J. and Bareinboim, E. (2021), “Bounding causal effects on continuous outcome,” in
Proceedings of the AAAI Conference on Artificial Intelligence , vol. 35, pp. 12207–12215.
Zheng, W. and van der Laan, M. J. (2010), “Asymptotic theory for cross-validated targeted
maximum likelihood estimation,” Tech. rep., U.C. Berkeley.
23Supporting Information
A Proofs of Results in Section 2
A.1 Proof of Theorem 1
The argument of Balke and Pearl (1997) is valid within levels of X, given Assumptions 1–4.
This implies that for each x∈ X,
γℓ(x)≤E(Y(a= 1)−Y(a= 0)|X=x)≤γu(x), (10)
and that these bounds are tight. It follows that for any ϵ >0, there exist joint distributions
P∗
1,ϵ,P∗
2,ϵon the full data (X, Z, A, Y (a= 0), Y(a= 1)), compatible with the observed data
lawPsuch that
P
EP∗
1,ϵ(Y(a= 1)−Y(a= 0)|X)< γℓ(X) +ϵ
= 1, (11)
and
P
EP∗
2,ϵ(Y(a= 1)−Y(a= 0)|X)> γu(X)−ϵ
= 1.
By iterated expectations, EP(γℓ(X))andEP(γu(X))represent valid lower and upper bounds,
respectively, on the ATE. To prove that these are also tight, we proceed by contradiction: we
focus on the lower bound without loss of generality, and assume there exists ϵ >0such that
for all full data laws P∗compatible with P,EP∗(Y(a= 1)−Y(a= 0)) ≥EP(γℓ(X)) +ϵ.
But (11) implies EP∗
1,ϵ(Y(a= 1)−Y(a= 0)) <EP(γℓ(X)) +ϵ, which yields a contradiction.
A.2 Proof of Proposition 1
The bounds based on (X,G)are of the same form as those for Xalone, but with underlying
probabilities π†
ya.z(X,G) =P[Y=y, A =a|X,G, Z=z]instead of πya.z(X). The key
observation is that for any y, a, z ∈ {0,1},
πya.z(X) =E(π†
ya.z(X,G)|X, Z=z) =E(π†
ya.z(X,G)|X),
where the first equality holds by the tower law, and the second holds by the assumption
Z⊥ ⊥G|X. Therefore, letting θ†
ℓ,j(X,G),θ†
u,j(X,G)be of the same for as θℓ,j(X),θu,j(X),
respectively, for j= 1, . . . , 8, but with π†
ya.zreplacing πya.zfor all y, a, z ∈ {0,1}, we have
EP
max
1≤j≤8θℓ,j(X)
=EP
max
1≤j≤8EP(θ†
ℓ,j(X,G)|X)
≤EP
max
1≤j≤8θ†
ℓ,j(X,G)
,
where we used a conditional version of Jensen’s inequality for the pointwise maximum, and
iterated expectations. The proof that the (X,G)-upper bound is lower follows by the same
logic, using concavity of the pointwise minimum. The example in Section 3.2 shows that the
lower (upper) bound based on (X,G)may be strictly greater (smaller).
A.3 Proof of Corollary 1
This follows immediately by Proposition 1, replacing Xwith∅, andGwithX.
24A.4 Simplification under Monotonicity
The bounds γℓ(X) = max 1≤j≤8θℓ,j(X)andγu(X) = min 1≤j≤8θu,j(X)on the CATE due to
Balke and Pearl (1997) are often compared to simpler valid bounds derived earlier in Robins
(1989); Manski (1990b): in our setting with baseline confounders, βℓ(X)≤E(Y(1)−Y(0)|
X)≤βu(X), where
βℓ(X) =EP(AY|X, Z= 1)−EP(Y(1−A) +A|X, Z= 0),
and
βu(X) =EP(AY+ 1−A|X, Z= 1)−EP(Y(1−A)|X, Z= 0).
Balke and Pearl (1997) show that their bounds on the treatment effect are in general tighter
than these simpler bounds, as seen in the following result.
Lemma 1. βℓ(X) =θℓ,1(X)≤γℓ(X)≤γu(X)≤θu,1(X) =βu(X).
Proof.Observe that, noting A=Y A+ (1−Y)A,
βℓ(X) =EP(AY|X, Z= 1)−EP(Y(1−A) +A|X, Z= 0)
=π11.1(X)−π10.0(X)−π11.0(X)−π01.0(X)
=π11.1(X)− {1−π00.0(X)}
=θℓ,1(X).
Similarly,
βu(X) =EP(AY+ 1−A|X, Z= 1)−EP(Y(1−A)|X, Z= 0)
=π11.1(X) +π10.1(X) +π00.1(X)−π10.0(X)
={1−π01.1(X)} −π10.0(X)
=θu,1(X).
These simpler bounds are known to be tight — see Theorem 7.3 of Balke and Pearl (1993) —
when we additionally assert the monotonicity assumption, i.e., A(z= 1)≥A(z= 0)almost
surely. Thus, somewhatsurprisingly, monotonicitydoesnotenableustoobtaintighterbounds,
but does simplify the structure in that βℓ(X) =γℓ(X)andβu(X) =γu(X). A consequence
of this analysis is that in the special case of a randomized trial with monotonicity, covariate-
assisted bounds offer no reduction in width for bounding the marginal ATE compared to
covariate-agnostic bounds
β∗
ℓ(P) =EP(AY|Z= 1)−EP(Y(1−A) +A|Z= 0),
and
β∗
u(P) =EP(AY+ 1−A|Z= 1)−EP(Y(1−A)|Z= 0),
since β∗
ℓ(P) =EP(βℓ(X))andβ∗
u(P) =EP(βu(X))by randomization, i.e., Z⊥ ⊥X. That
said, oneupsideisthatwecanleveragerandomizationtoobtainmoreefficientestimatorsofthe
bounds in this setting compared to nonparametric estimators of β∗
ℓ(P)andβ∗
u(P). Indeed, the
efficient influence functions of the bounds in this setting will be the nonparametric influence
functions of the functionals EP(βℓ(X))andEP(βu(X))—we omit this analysis here.
25B Proofs of Results in Section 4
B.1 Proof of Theorem 2
The proof structures follows that of the proof of Theorem 4, explained in greater detail below.
We prove the result for the lower bound as the result for the upper bound is analogous. By
the standard decomposition, we have
bL − L = (Pn−P){φℓ(O;bP,bdℓ)−φℓ(O;P, dℓ)}+P{φℓ(O;bP,bdℓ)−φℓ(O;P, dℓ)}
+ (Pn−P){φℓ(O;P, dℓ)}
≡R1+R2+ (Pn−P){φℓ(O;P, dℓ)}
We will show that R1=oP(n−1/2)and
R2=OPbλ1−λ1·max
y,a,z∈{0,1}∥bπya.z−πya.z∥+ max
1≤j≤8bθℓ,j−θℓ,j1+α
∞
under the conditions of the theorem.
B.1.1 Term R1
By Lemma 2 in Kennedy et al. (2020), R1=oP(n−1/2)if
Z
{φℓ(o;bP,bdℓ)−φℓ(o;P, dℓ)}2dP(o) =oP(1)
We haveZ
{φℓ(o;bP,bdℓ)−φℓ(o;P, dℓ)}2dP(o)≲Z
{φℓ(o;bP,bdℓ)−φℓ(o;P,bdℓ)}2dP(o)
+Z
{φℓ(o;P,bdℓ)−φℓ(o;P, dℓ)}2dP(o)
For the first term,
Z
{φℓ(o;bP,bdℓ)−φℓ(o;P,bdℓ)}2dP(o)≲8X
j=1Z
{Lj(o;bP)−Lj(o;P) +bθℓ,j−θℓ,j}2dP(o) =oP(1)
since for example for j= 1, we have
(bπ11.1−π11.1)
1−1(Z= 1)
bλ1
+1(Z= 1)
bλ1λ1{1(Y= 1, A= 1)−π11.1}
λ1−bλ1
+(bπ00.0−π00.0)
1−1(Z= 0)
bλ0
+1(Z= 0)
bλ0λ0{1(Y= 0, A= 0)−π00.0}
λ0−bλ0
≲bλ1−λ1+ max
y,a,z∈{0,1}∥bπya.z−πya.z∥=oP(1),
by our assumptions, using the fact that bλzandλzare bounded away from zero.
For the second term, we have
Z
{φℓ(o;P,bdℓ)−φℓ(o;P, dℓ)}2dP(o) =8X
j=1Z1{bdℓ(x) =j} −1{dℓ(x) =j}{Lj(o;P) +θℓ,j(x)}2dP(o)
≲Pn
θℓ,bd(X)(X)̸=θℓ,dℓ(X)(X)o
26since θℓ,j(X)andLj(O;P)are all uniformly bounded. Next, we show that
Pn
θℓ,bd(X)(X)̸=θℓ,dℓ(X)(X)o
=oP(1)
For any t >0, we have
Ph
θℓ,bdℓ(X)̸=θℓ,dℓ(X)i
=P
θℓ,bdℓ(X)̸=θℓ,dℓ(X),min
j̸=dℓ(X){θℓ,dℓ(X)(X)−θℓ,j(X)} ≤t
+P
θℓ,bdℓ(X)̸=θℓ,dℓ(X),min
j̸=dℓ(X){θℓ,dℓ(X)(X)−θℓ,j(X)}> t
≤P
min
j̸=dℓ(X){θℓ,dℓ(X)(X)−θℓ,j(X)} ≤t
+Ph
θℓ,dℓ(X)−θℓ,bdℓ(X)> ti
≤Ctα+Ph
θℓ,dℓ(X)−θℓ,bdℓ(X)+bθℓ,bdℓ(X)−bθℓ,dℓ(X)> ti
≤Ctα+P

28X
j=1|bθℓ,j−θℓ,j|> t


≤Ctα+2
t8X
j=1P|bθℓ,j(X)−θℓ,j(X)|
≤Ctα+2
t8X
j=1bθℓ,j−θℓ,j
where C > 0is the universal constant in Assumption 5. In the second line, we use that
θℓ,bdℓ(X)̸=θℓ,dℓ(X)implies bdℓ(X)̸=dℓ(X), soθℓ,dℓ(X)−θℓ,bdℓ(X)≥minj̸=dℓ(X){θℓ,dℓ(X)(X)−
θℓ,j(X)}. In the third line we use Assumption 5 and that bθℓ,bdℓ(X)(X)−bθℓ,dℓ(X)(X)≥0by
construction of bdℓ(X). The fourth line follows from Markov’s inequality, and the last line from
∥·∥L1≤ ∥·∥ L2. Since for each j∈ {1, . . . , 8}we havebθℓ,j−θℓ,j=oP(1), as each bθℓ,j−θℓ,j
is a linear combination of the differences {bπya.z−πya.z:y, a, z ∈ {0,1}}, we obtain the desired
result by invoking Lemma 2. Namely, we set Xn=Ph
θℓ,bdℓ(X)̸=θℓ,dℓ(X)i
, and for any ϵ >0,
choose tϵ= ϵ
C1/α>0andZ(ϵ)
n=2
tϵP8
j=1bθℓ,j−θℓ,j.
Lemma 2. Suppose that for a given sequence Xn, one can find for any ϵ >0another sequence
Z(ϵ)
n≥0such that |Xn| ≤ϵ+Z(ϵ)
nandZ(ϵ)
n=oP(1). Then Xn=oP(1).
Proof.Fixing ϵ >0, consider a non-negative sequence Z(ϵ/2)
n =oP(1)satisfying |Xn| ≤ϵ/2 +
Z(ϵ/2)
n. Then
P[|Xn|> ϵ]≤P[ϵ/2 +Z(ϵ/2)
n> ϵ] =P[Z(ϵ/2)
n> ϵ/2]→0asn→ ∞ ,
since Zn=oP(1), thus proving the result.
27B.1.2 Term R2
We decompose R2as
R2=8X
j=1n
Ph
1{bdℓ(X) =j}{Lj(O;bP) +bθℓ,j(X)−θℓ,j(X)}i
+Ph
1{bdℓ(X) =j} −1{dℓ(X) =j}i
θℓ,j(X)o
We have
Ph
1{bdℓ(X) =j}{Lj(O;bP) +bθℓ,j(X)−θℓ,j(X)}i
≲bλ1−λ1·max
y,a,z∈{0,1}∥bπya.z−πya.z∥
For the second term, observe that
Pn
θℓ,bdℓ(X)(X)−θℓ,dℓ(X)(X)o
=Ph
1{θℓ,dℓ(X)(X)> θℓ,bdℓ(X)(X)}n
θℓ,dℓ(X)(X)−θℓ,bdℓ(X)(X)oi
≤P
1
min
j̸=dℓ(X){θℓ,dℓ(X)−θℓ,j} ≤θℓ,dℓ(X)−θℓ,bdℓ(X)+bθℓ,bdℓ(X)−bθℓ,dℓ(X)
×
θℓ,dℓ(X)−θℓ,bdℓ(X)+bθℓ,bdℓ(X)−bθℓ,dℓ(X)
≤2 max
1≤j≤8bθℓ,j−θℓ,j
∞P
min
j̸=dℓ(X){θℓ,dℓ(X)−θℓ,j} ≤2 max
1≤j≤8bθℓ,j−θℓ,j
∞
≲max
1≤j≤8bθℓ,j−θℓ,j1+α
∞,
by Assumption 5, where we used the fact that θℓ,dℓ(X)(X)≥θℓ,bdℓ(X)(X)andbθℓ,dℓ(X)(X)≤
bθℓ,bdℓ(X)(X), by construction of dℓandbdℓ.
C Elaboration and Proofs of Results in Section 5
C.1 Doubly Robust Machine Learning Framework
We now briefly review the statistical framework we use to derive our estimators and to evaluate
thetheoreticalpropertiesofourmethods. Acentralgoalistodevelopmethodsthatareflexible
and resistant to bias from model misspecification.
Here, we reduce the potential for model misspecification by using flexible nonparametric
machine learning (ML) tools. More specifically, we aim to construct bias-corrected estimators
using influence functions—a central element of semiparametric theory. As an example, we
review estimation of the mean counterfactual, E(Y(a= 1))(i.e., the mean outcome if every
unit in the population were treated), from this perspective. For the purposes of illustration, we
assume (just for this paragraph) that the data consist of niid copies of O= (X, A, Y )∼P.
Under no unmeasured confounding and other assumptions, E(Y(a= 1))can be written as
an averaged regression function (Robins, 1986): ψ(P) =EP{EP(Y|A= 1,X)}. Estimating
EP(Y|A= 1,X)is a standard regression problem and flexible ML methods may be preferred
to more restrictive parametric estimation methods such as linear regression to avoid model
misspecification and reduce bias. However, if ψis estimated as the average of predictions from
28a ML model, rather than from a parametric model, the estimate will generally inherit first-
ordersmoothingbias fromthe nonparametric estimates. Instead, onecan find a function ofthe
data, which we can denote generically as φ, such that estimating ψas the average value of (an
estimated version of) φwill correct this first-order bias. An optimal choice of this function is
referred to as the influence function ofψ, and in this case is based on two regression functions,
EP(Y|A= 1,X)andP(A= 1|X). In practice, the analyst fits two models: a model
for the outcome regressed against the treatment and all confounders (the outcome model);
and a model regressing the treatment against all confounders (the propensity model). These
models are combined to estimate the effect of interest. This approach is “doubly robust”,
since it is consistent if either the propensity model or the outcome model is correctly specified
(Scharfstein et al., 1999), and also leads to “doubly reduced” second-order bias. Finally, the
estimation process is combined with sample-splitting or cross-fitting, to prevent over-fitting
(or formally, to avoid imposing complexity restrictions on the class of nuisance estimators) by
separating estimation of the components of the influence function from estimation of its mean
(Robins et al., 2008; Zheng and van der Laan, 2010; Chernozhukov et al., 2018).
In semiparametric efficiency theory (Bickel et al., 1993; Tsiatis, 2006; van der Vaart, 2000;
Kennedy, 2016), a fundamental goal is to characterize the (efficient) influence function. Math-
ematically, an influence function is the derivative in a von Mises expansion of the target
statistical functional (analogous to the usual derivative of a function in Taylor expansion). In
robust statistics, it coincides with the Gateaux derivative of the functional in the direction of
a point-mass contamination distribution. The influence function serves a number of purposes.
First, the variance of the efficient influence function is equal to the efficiency bound of the
target statistical functional, which serves as a lower bound of the variance for regular estima-
tors. It characterizes the inherent estimation difficulty of the target functional and provides
a benchmark to compare against when we construct estimators. Moreover, it enables us to
correct for first-order bias in the plug-in estimator and motivates the robust estimator, which
has a general second-order bias property so that nonparametric and flexible machine learning
methods with relatively slow rates can be used for estimating the nuisance functions.
C.2 Background on log-sum-exp Function
The log-sum-exp (LSE) function is commonly employed across a range of disciplines, including
statistical mechanics (Aldous, 2005) and machine learning (Calafiore et al., 2020). Using the
LSEfunctiontoapproximatethemaximumfunctionisalsocommoninthestatisticsliterature.
For instance, one way to prove Sudakov-Fernique’s inequality on Gaussian comparison is to
apply the functional form of Slepian’s inequality (which requires the function considered to be
twice-differentiable) to the LSE function and let t→ ∞(Vershynin, 2018; Wainwright, 2019).
As another example, in proving a high-dimensional Gaussian comparison version of the central
limit theorem, Chernozhukov et al. (2012) used Slepian’s Gaussian interpolation together with
Stein’s leave-one-out expansions. A typical tool to evaluate Stein’s leave-one-out expansion
is a Taylor’s expansion, which requires differentiability of the function. Hence, Chernozhukov
et al. (2012) also approximated the maximum function with the LSE function, and selected
the tuning parameter tto limit approximation error while controlling the derivatives of the
LSE function.
29C.3 Proof of Theorem 3
Recall an influence function of a pathwise differentiable functional χ(P), atPin a given
statistical model, is a zero-mean finite-variance function ˙χ(O;P)of observed data Osuch that
for any regular one-dimensional parametric submodel Pϵthrough P0=P, it holds that
d
dϵχ(Pϵ)
ϵ=0=EP[ ˙χ(O;P)u(O)]
where u(O)is the score function of the parametric submodel at P. For such a parametric
submodel, invoking the total derivative, we have (let θϵ,ℓ(X)be the nuisance function vector
at submodel Pϵ)
d
dϵLg(Pϵ)
ϵ=0
=d
dϵEPϵ[g(θϵ,ℓ(X))]
ϵ=0
=d
dϵEPϵ[g(θℓ(X))]
ϵ=0+8X
j=1d
dϵEP[g(θℓ,1(X), . . . , θ ϵ,ℓ,j(X), . . . , θ ℓ,8(X))]
ϵ=0
=EP[(g(θℓ(X))− L g(P))u(O)] +8X
j=1EP∂g(θℓ(X))
∂θℓ,j(X)d
dϵθϵ,ℓ,j(X)
ϵ=0(12)
using the fact that score function uhas mean zero to center the first term, and the chain
rule for the remaining eight terms. Next, observe that, for any (y, a, z )∈ {0,1}3, (denote in
general u(B|C)as the conditional score function for the distribution of Bgiven C.
d
dϵπϵ,ya.z(X)
ϵ=0=EP[(1(Y=y, A=a)−πya.z(X))u(Y, A|Z=z,X)|Z=z,X]
=EP1(Z=z)
λz(X){1(Y=y, A=a)−πya.z(X)}u(Y, A|Z=z,X)|X
=EP1(Z=z)
λz(X){1(Y=y, A=a)−πya.z(X)}u(Y, A|Z,X)|X
=EP(ψya.z(O;P)u(Y, A|Z,X)|X),
(13)
where the first equation follows from directly taking derivatives and centering with πya.zsince
u(Y, A|Z=z,X)has conditional mean zero. The second equation follows from conditioning
onZ,Xand using property of conditional expectations. The last equation follows from the
definition of ψya.z.
Since θℓ,j’s are linear combinations of πya.z(and constant 1), we can prove
EP∂g(θℓ(X))
∂θℓ,j(X)d
dϵθϵ,ℓ,j(X)
ϵ=0
=EP∂g(θℓ(X))
∂θℓ,j(X)Lj(O;P)u(O)
.
30For illustration, take j= 1(other j’s can be proved similarly) and we have
EP∂g(θℓ(X))
∂θℓ,1(X)d
dϵθϵ,ℓ,1(X)
ϵ=0
=EP∂g(θℓ(X))
∂θℓ,1(X)d
dϵ{πϵ,11.1(X) +πϵ,00.0(X)−1}
ϵ=0
=EP∂g(θℓ(X))
∂θℓ,1(X)EP[{ψ11.1(O;P) +ψ00.0(O;P)}u(Y, A|Z,X)|X]
=EP∂g(θℓ(X))
∂θℓ,1(X){ψ11.1(O;P) +ψ00.0(O;P)}u(Y, A|Z,X)
=EP∂g(θℓ(X))
∂θℓ,1(X)L1(O;P)u(O)
,
where the first equation follows from definition of θϵ,ℓ,1. The second equation follows from (13).
The third equation follows from property of conditional distribution. In the final equation we
added u(Z,X)and noted u(O) =u(Z,X) +u(Y, A|Z,X)— we are permitted to add this
term as it is a function only of (Z, X), and ψya.z(O;P)has mean zero given (Z,X).
Plugging these equations into (12) we have
d
dϵLg(Pϵ)
ϵ=0=EP


g(θℓ(X))− L g(P) +8X
j=1∂g(θℓ(X))
∂θℓ,j(X)Lj(O;P)
u(O)

.
The argument for deriving the influence function of Uh(P)uses the exact same logic.
C.4 Proof of Theorem 4
We first prove a proposition that characterizes the conditional bias of the robust estimator.
In the following derivations all the expectations are taken conditioning on training data Dn.
Proposition 2. Suppose gis a twice continuously differentiable function, then we have
E[bLg]− L g=EP
∇g
bθℓ(X)T
EP(L(O;bP)|X) +bθℓ(X)−θℓ(X)
−1
2EP
bθℓ(X)−θℓ(X)T
∇2g(θ∗
ℓ(X))
bθℓ(X)−θℓ(X)
,
where L(O;bP) =
L1(O;bP), . . . , L 8(O;bP)T
andθ∗
ℓ(X)is a point that lies on the line segment
between θℓ(X)andbθℓ(X).
31Proof.By definition of bLg,
E[bLg]− L g
=EP
g
bθℓ(X)
+8X
j=1∂g
bθℓ(X)
∂bθℓ,j(X)Lj(O;bP)−g(θℓ(X))

=EP
g
bθℓ(X)
+8X
j=1∂g
bθℓ(X)
∂bθℓ,j(X)EP
Lj(O;bP)|X
−g(θℓ(X))

=EP
∇g
bθℓ(X)T
EP(L(O;bP)|X)−
g(θℓ(X))−g(bθℓ(X))
=EP
∇g
bθℓ(X)T
EP[L(O;bP)|X] +bθℓ(X)−θℓ(X)
−1
2EP
bθℓ(X)−θℓ(X)T
∇2g(θ∗
ℓ(X))
bθℓ(X)−θℓ(X)
.
The second equality follows from conditioning on Xand the last equality follows from second-
order Taylor expansion.
For any (y, a, z )∈ {0,1}3, by conditioning on (Z,X)we have
EP
ψya.z(O;bP)|X
=E"
1(Z=z)
bλz(X)(P(Y=y, A=a|X, Z=z)−bπya.z(X))|X#
=λz(X)
bλz(X){πya.z(X)−bπya.z(X)}.
We want to bound each component of EP[L(O;bP)|X] +bθℓ(X)−θℓ(X). We only analyze
the first component as an illustration. All other components can be similarly analyzed.
EP
L1(O;bP)|X
+bθℓ,1(X)−θℓ,1(X)
=EP
ψ11.1(O;bP) +ψ00.0(O;bP)|X
+{bπ11.1(X)−π11.1(X)}+{bπ00.0(X)−π00.0(X)}
=(
1−λ1(X)
bλ1(X))
{bπ11.1(X)−π11.1(X)}+(
1−λ0(X)
bλ0(X))
{bπ00.0(X)−π00.0(X)}.
Note that
1−λ1(X)
bλ1(X)=bλ1(X)−λ1(X)
bλ1(X),1−λ0(X)
bλ0(X)=λ1(X)−bλ1(X)
1−bλ1(X)
By positivity assumption we have
EP
L1(O;bP)|X
+bθℓ,1(X)−θℓ,1(X)
≤1
ϵbλ1(X)−λ1(X)· {|bπ11.1(X)−π11.1(X)|+|bπ00.0(X)−π00.0(X)|}
32Similar inequalities can be obtained for 2≤j≤8. Hence, by Hölder’s inequality,EP
∇g
bθℓ(X)Tn
EP(L(O;bP)|X) +bθℓ(X)−θℓ(X)o
≤EPh∇g
bθℓ(X)
∞EP(L(O;bP)|X) +bθℓ(X)−θℓ(X)
1i
≤C1
ϵEP
bλ1(X)−λ1(X)X
(y,a,z )∈R|bπya.z(X)−πya.z(X)|
,
where Ris a multiset of elements in {0,1}3such that each (y, a, z )∈ {0,1}3appears in Ras
many times as πya.z(X)appears in θℓ(X). By the triangle inequality and Cauchy-Schwarz’s
inequality EP
∇g
bθℓ(X)Tn
EP(L(O;bP)|X) +bθℓ(X)−θℓ(X)o
≲C1bλ1−λ1X
(y,a,z )∈R∥bπya.z−πya.z∥
≲C1bλ1−λ1
max
y,a,z∈{0,1}∥bπya.z−πya.z∥
It remains to bound the second derivative term in the asymptotic bias expression derived in
Proposition 2. By properties of the operator norm we haveEP
bθℓ(X)−θℓ(X)T
∇2g(θ∗
ℓ(X))
bθℓ(X)−θℓ(X)
≤
sup
θ∥∇2g(θ)∥
EPbθℓ(X)−θℓ(X)2
2
≲C2max
y,a,z∈{0,1}∥bπya.z−πya.z∥2
The bound on conditional bias is established. For a general function fon the sample O, we
have
Pn[bf]−E[f] = (Pn−E)(bf−f) + (Pn−E)(f) +E(bf−f) (14)
We apply the decomposition of error (14) to
f(O) = ˙Lg(O;P) +Lg(P) =g(θℓ(X)) +8X
j=1∂g(θℓ(X))
∂θℓ,j(X)Lj(O;P),
Note that Pn[bf]is exactly the robust estimator. By Lemma 2 in Kennedy et al. (2020), if
∥bf−f∥2=oP(1)we have
(Pn−E)(bf−f) =oP(n−1/2).
Also note E(bf−f)is equal to the conditional bias and under the convergence rate assumption
we have E(bf−f) =oP(n−1/2). Finally note that f−Ef=˙Lg(O;P), the proof is completed.
D Proofs of Results in Section 6
D.1 Proof of Theorem 5
First by Fubini’s theorem and Jensen’s inequality we have
Z1
0E
min
1≤j≤8θu,j(t,X)
dt=EZ1
0min
1≤j≤8θu,j(t,X)dt
≤E
min
1≤j≤8Z1
0θu,j(t,X)dt
.
33The proof will be completed if we can show
Z1
0θu,j(t,X)dt=eθu,j(X).
Since θu,j’sarelinearcombinationsof π1a.zandπ0a.z, weonlyneedtoshowforany a, z∈ {0,1},
Z1
0π1a.z(t,X)dt=eπ1a.z(X),
Z1
0π0a.z(t,X)dt=eπ0a.z(X).
We condition on W, by property of conditional expectation,
eπ1a.z(X)
=P(Y≤W, A =a|Z=z,X)
=Z1
0P(Y≤t, A=a|Z=z,X, W=t)pw(t|Z=z,X)dt,
where pwdenotes the density of W. Since Wis independent of the data generating process of
O= (X, Z, A, Y ), we have
P(Y≤t, A=a|Z=z,X, W=t) =P(Y≤t, A=a|Z=z,X),
pw(t|Z=z,X) =pw(t) =1(0< t < 1).
Hence we conclude
Z1
0P(Y≤t, A=a|Z=z,X, W=t)pw(t|Z=z,X)dt=Z1
0P(Y≤t, A=a|Z=z,X)dt.
Note that by definition π1a.z(t,X) =P(Y≤t, A=a|X, Z=z), which implies
Z1
0π1a.z(t,X)dt=eπ1a.z(X).
We can similarly proveZ1
0π0a.z(t,X)dt=eπ0a.z(X).
which completes the proof for the first inequality in Theorem 5. The second inequality can be
proved by the same arguments.
D.2 Basic Efficiency Result
Proposition 3. Suppose we observe niid copies of T∼P, with T∈[0,1]. Letting µ=
EP(T),VarP(T) =σ2, construct mestimates of µby sampling m×nindependent Unif(0 ,1)
variates, {W(j)
i}j=1,...,m
i=1,...,n, and estimating bµj=Pn[T > W(j)] =1
nPn
i=11(Ti> W(j)
i). Then
the estimator that averages these mestimates,
bµ=1
mmX
j=1bµj,
is unbiased and has variance1
n 
σ2+1
m
µ(1−µ)−σ2	m→∞→σ2
n.
34Proof.Observe that EP(bµj) =P[T > W ] =R1
0P[T > w ]dw=EP(T) =µ, forj= 1, . . . , m,
sobµis unbiased. Further, VarP(bµj) =1
nVarP(1(T > W )) =1
nµ(1−µ), so by identical
distribution of each bµj,
VarP(bµ) =1
mVarP(bµ1) +
1−1
m
Cov P(bµ1,bµ2)
=1
nmµ(1−µ) +
1−1
m1
nCov P(1(T > W(1)),1(T > W(2)))
using the fact that {W(j)
i} ⊥ ⊥ (T1, . . . , T n). Next, see that
Cov P(1(T > W(1)),1(T > W(2))) =P[T >max{W(1), W(1)}]−µ2,
and finally, since V= max {W(1), W(1)}has density 2v1(v∈(0,1)),
P[T > V ] =Z1
02vP[T > v ]dv=Z1
0P[T >√u]du=Z1
0P[T2> u]du=EP(T2) =µ2+σ2,
where we made the substitution u=v2.
E Details of the Simulation Study
Consider the following simplified setting: with a single covariate X≡X∼Uniform(0 ,1),
we generated Z∼Bernoulli( λ1(X))based on λ1(X) = min {max{X2,0.10},0.90}. Next, we
assumed perfect compliance, i.e., A≡Z, then generated U∼Uniform(0 ,1), independent of
(X, A), and Y|X, A, U ∼Bernoulli ( U{A(1−X) + (1 −A)X}). This simplified setup, in
which Xis sufficient for controlling for A−Yconfounding, results in
min
j̸=dℓ(X)
θℓ,dℓ(X)(X)−θℓ,j(X)	
= min {µ0(X),1−µ0(X), µ1(X),1−µ1(X)}
=1
2min{X,1−X},
where µa(X) =P[Y= 1|X, A =a]. In turn, this guarantees that Assumption 5 holds with
α= 1, since {X,1−X} ∼Uniform 
0,1
2
:
P1
2min{X,1−X} ≤t
=

0ift <0
4tif0≤t≤1
4
1ift >1
4
which is at most 4tfor all t≥0. Moreover, the conditional bounds satisfy γℓ(X) =γu(X) =
1
2−X, and thus the marginal bounds are L(P) =U(P) = 0. In this scenario, the probabilities
{πya.z(X) :y, a, z ∈ {0,1}}are given by
π00.1(X) =π10.1(X) =π01.0(X) =π11.0(X) = 0 ,
π00.0(X) = 1−1
2X, π 10.0(X) =1
2X, π 01.1(X) = 1−1
2(1−X), π11.1(X) =1
2(1−X).
35To “estimate” the nuisance functions, we define bλ1(X) = expit (logit ( λ1(X)) +ϵλ),bπya.z≡0
when a̸=z, and otherwise bπya.z(X) = expit (logit ( πya.z(X)) +ϵya.z), where
ϵλ, ϵ00.0, ϵ10.0, ϵ01.1, ϵ11.1iid∼ N (h n−r, h2n−2r),
where we set h= 2.25, and vary the parameter r∈(0,0.5]in different scenarios. These choices
guaranteethatbλ1−λ1=O(n−r)and∥bπya.z−πya.z∥=O(n−r)fora=z—asaconsequence
we can study the performance of the proposed estimators under different nuisance estimation
convergence rates known to be O(n−r).
Our simulation study proceeded as follows: for n∈ {500,1000,5000}, we generated data
and computed nuisance estimates (bλ1,{bπya.z:y, a, z ∈ {0,1}})as described above, varying
r∈ {0.10 + 0 .05k:k∈ {0, . . . , 8}}. We then computed the direct lower bound estimator bLas
in Section 4, and the log-sum-exp smooth approximation-based estimator bLgtas described in
Section 5, taking t= 2hnr. Finally, we computed a plug-in estimator Pnh
max 1≤j≤8bθℓ,j(X)i
,
whose error is expected to be first order, i.e., on the order of the nuisance error O(n−r). Each
scenario was replicated 5,000times, and root-mean-square error (RMSE) of each estimator
relative to the lower true bound L(P) = 0was computed. Results are shown in Figure 2.
0.1 0.2 0.3 0.4 0.50.0 0.1 0.2 0.3 0.4 0.5n = 500
Error RateRMSEPlug−in
Smooth IF−based
Direct IF−based
0.1 0.2 0.3 0.4 0.50.000.050.100.150.200.250.30n = 1000
Error RateRMSEPlug−in
Smooth IF−based
Direct IF−based
0.1 0.2 0.3 0.4 0.50.000.050.100.150.200.250.30n = 5000
Error RateRMSEPlug−in
Smooth IF−based
Direct IF−based
Figure 2: Root-mean-square error versus nuisance function error rate parameter r
According to the results in Figure 2, the proposed estimators bLandbLgttend to outperform
the plug-in estimator when nuisance functions are estimated slower than the parametric rate
ofO(n−1/2), especially for larger sample sizes. Indeed, as predicted by our theoretical results,
the robust estimators bLandbLgtnearly attained the optimal performance of the plug-in esti-
mator (where nuisance error ≍n−1/2)when nuisance RMSE was only on the order O(n−1/4),
following which performance plateaued. As nuisance error on the order of O(n−1/2)would be
anticipated only in the unlikely scenario of a correctly specified parametric model, the results
support our recommendation to use the proposed estimators in practice. Note that in this
simulation, the ad hocchoice for the tuning parameter tinbLgtresulted in very little difference
compared to bL, though other choices might be considered.
In this simulation study, we considered only the case α= 1from Assumption 5. In future
work, we will consider simulation settings in which we can vary this margin parameter, and
pursue theoretical development of optimal choices (e.g., in terms of minimizing MSE) for the
tuning parameter tinbLgt.
36F Code for Reproducing Motivating Illustration
library(rpart)
X1.p <- 0.7
beta.AT.0 <- function(x1, x2) {
plogis(qlogis(0.20))
}
beta.AT.d <- function(x1, x2) {
plogis(qlogis(0.35))
}
beta.NT.0 <- function(x1, x2) {
plogis(qlogis(0.90))
}
beta.NT.d <- function(x1, x2) {
plogis(qlogis(0.95))
}
beta.DE.0 <- function(x1, x2) {
plogis(qlogis(0.65))
}
beta.DE.d <- function(x1, x2) {
plogis(qlogis(0.725))
}
beta.CO.0 <- function(x1, x2) {
plogis(qlogis(0.25))
}
beta.CO.d <- function(x1, x2) {
plogis(qlogis(0.375))
}
## we work with an 8-vector pi = {pi_{ya.z} : y, a, z \in {0,1}}
## specifically, take the following order:
## pi = (pi_{00.0}, pi_{01.0}, pi_{10.0}, pi_{11.0},
## pi_{00.1}, pi_{01.1}, pi_{10.1}, pi_{11.1})
## lower bound functions
p.l1 <- function(pi) { pi[8] + pi[1] - 1 }
p.l2 <- function(pi) { pi[4] + pi[5] - 1 }
p.l3 <- function(pi) { -pi[6] - pi[7] }
p.l4 <- function(pi) { -pi[2] - pi[3] }
p.l5 <- function(pi) { pi[4] - pi[8] - pi[7] - pi[2] - pi[3] }
p.l6 <- function(pi) { pi[8] - pi[4] - pi[3] - pi[6] - pi[7] }
p.l7 <- function(pi) { pi[5] - pi[6] - pi[7] - pi[2] - pi[1] }
p.l8 <- function(pi) { pi[1] - pi[2] - pi[3] - pi[6] - pi[5] }
gamma.l <- function(pi) { pmax(p.l1(pi), p.l2(pi), p.l3(pi), p.l4(pi),
p.l5(pi), p.l6(pi), p.l7(pi), p.l8(pi)) }
37## upper bound functions
p.u1 <- function(pi) { 1 - pi[6] - pi[3] }
p.u2 <- function(pi) { 1 - pi[2] - pi[7] }
p.u3 <- function(pi) { pi[8] + pi[5] }
p.u4 <- function(pi) { pi[4] + pi[1] }
p.u5 <- function(pi) { -pi[2] + pi[6] + pi[5] + pi[4] + pi[1] }
p.u6 <- function(pi) { -pi[6] + pi[2] + pi[1] + pi[8] + pi[5] }
p.u7 <- function(pi) { -pi[7] + pi[8] + pi[5] + pi[4] + pi[3] }
p.u8 <- function(pi) { -pi[3] + pi[4] + pi[1] + pi[8] + pi[7] }
gamma.u <- function(pi) { pmin(p.u1(pi), p.u2(pi), p.u3(pi), p.u4(pi),
p.u5(pi), p.u6(pi), p.u7(pi), p.u8(pi)) }
set.seed(476)
N <- 1000000
X1 <- rbinom(N, 1, X1.p)
X2 <- runif(N, -1, 1)
cmp <- factor(ifelse(X1 == 0 & X2 >=-0.5 & X2 <= 0.5, "DE",
ifelse(X2 >= 0.98, "AT",
ifelse(X2 <= -0.99, "NT", "CO"))))
ATE.true <- (1 - X1.p)*0.5*0.075 + 0.01 * 0.05 + 0.02 * 0.15 +
(1 - 0.02 - (1 - X1.p)*0.5)*0.125
## covariate-adjusted BP bounds
pi00.0 <- ((cmp == "NT") * (1 - beta.NT.0(X1, X2)) +
(cmp == "CO") * (1 - beta.CO.0(X1, X2)))
pi01.0 <- ((cmp == "AT") * (1 - beta.AT.d(X1, X2)) +
(cmp == "DE") * (1 - beta.DE.d(X1, X2)))
pi10.0 <- ((cmp == "NT") * beta.NT.0(X1, X2) +
(cmp == "CO") * beta.CO.0(X1, X2))
pi11.0 <- ((cmp == "AT") * beta.AT.d(X1, X2) +
(cmp == "DE") * beta.DE.d(X1, X2))
pi00.1 <- ((cmp == "NT") * (1 - beta.NT.0(X1, X2)) +
(cmp == "DE") * (1 - beta.DE.0(X1, X2)))
pi01.1 <- ((cmp == "AT") * (1 - beta.AT.d(X1, X2)) +
(cmp == "CO") * (1 - beta.CO.d(X1, X2)))
pi10.1 <- ((cmp == "NT") * beta.NT.0(X1, X2) +
(cmp == "DE") * beta.DE.0(X1, X2))
pi11.1 <- ((cmp == "AT") * beta.AT.d(X1, X2) +
(cmp == "CO") * beta.CO.d(X1, X2))
pi <- cbind(pi00.0, pi01.0, pi10.0, pi11.0,
pi00.1, pi01.1, pi10.1, pi11.1)
cov.lower <- apply(pi, 1, gamma.l)
cov.upper <- apply(pi, 1, gamma.u)
38pi.mean <- c(mean(pi00.0), mean(pi01.0), mean(pi10.0), mean(pi11.0),
mean(pi00.1), mean(pi01.1), mean(pi10.1), mean(pi11.1))
mean.lower <- gamma.l(pi.mean)
mean.upper <- gamma.u(pi.mean)
c(mean(cov.lower), mean(cov.upper)) ## theoretical covariate-assisted BP bounds
c(mean.lower, mean.upper) ## theoretical covariate-agnostic BP bounds
par(mar = c(3,4,1,1))
plot(NULL, ylim = c(ATE.true - 0.24,ATE.true + 0.21), xlim = c(0.2,0.8),
xlab = "", ylab = "Average Treatment Effect", xaxt = ’n’)
axis(1, at = c(0.35, 0.65), labels = c("Covariate-Agnostic", "Covariate-Adjusted"))
abline(h = ATE.true, col = ’red’, lty = ’dashed’)
abline(h = 0)
arrows(x0=0.33, y0=mean.lower, x1=0.33, y1=mean.upper,
code=3, angle=90, length=0.05, lwd=2,col = ’blue’)
arrows(x0=0.63, y0=mean(cov.lower)-0.0007, x1=0.63, y1=mean(cov.upper)+0.0007,
code=3, angle=90, length=0.05, lwd=2, col = ’blue’)
## Simulated data
set.seed(956)
n <- 5000
X1 <- rbinom(n, 1, X1.p)
X2 <- runif(n, -1, 1)
cmp <- factor(ifelse(X1 == 0 & X2 >=-0.5 & X2 <= 0.5, "DE", "CO"))
Z <- rbinom(n, 1, 0.5)
A <- ifelse(cmp == "NT", 0,
ifelse(cmp == "AT", 1,
ifelse(cmp == "CO", Z, 1 - Z)))
Y.0 <- ifelse(cmp == "NT", rbinom(n, 1, beta.NT.0(X1, X2)),
ifelse(cmp == "AT", rbinom(n, 1, beta.AT.0(X1, X2)),
ifelse(cmp == "CO", rbinom(n, 1, beta.CO.0(X1, X2)),
rbinom(n, 1, beta.DE.0(X1, X2)))))
Y.1 <- ifelse(cmp == "NT", rbinom(n, 1, beta.NT.d(X1, X2)),
ifelse(cmp == "AT", rbinom(n, 1, beta.AT.d(X1, X2)),
ifelse(cmp == "CO", rbinom(n, 1, beta.CO.d(X1, X2)),
rbinom(n, 1, beta.DE.d(X1, X2)))))
Y <- A*Y.1 + (1 - A)*Y.0
dat <- cbind.data.frame(X1, X2, Z, A, Y)
dat$Y.A <- factor(dat$Y):factor(dat$A)
## Estimated covariate-agnostic BP-bounds
## compute estimated probabilities
pi.hat <- c(mean((1 - dat$A[dat$Z == 0]) * (1 - dat$Y[dat$Z == 0])),
mean(dat$A[dat$Z == 0] * (1 - dat$Y[dat$Z == 0])),
39mean((1 - dat$A[dat$Z == 0]) * dat$Y[dat$Z == 0]),
mean(dat$A[dat$Z == 0] * dat$Y[dat$Z == 0]),
mean((1 - dat$A[dat$Z == 1]) * (1 - dat$Y[dat$Z == 1])),
mean(dat$A[dat$Z == 1] * (1 - dat$Y[dat$Z == 1])),
mean((1 - dat$A[dat$Z == 1]) * dat$Y[dat$Z == 1]),
mean(dat$A[dat$Z == 1] * dat$Y[dat$Z == 1]))
## natural bounds
round(c(p.l1(pi.hat), p.u1(pi.hat)), 3)
## Balke & Pearl bounds
round(c(gamma.l(pi.hat), gamma.u(pi.hat)), 3)
## bootstrap CI’s
B <- 1000
boot.res <- matrix(NA, nrow = B, ncol = 2)
for (b in 1:B) {
dat.b <- dat[sample(1:n, n, replace = T),]
pi.hat.b <- c(mean((1 - dat.b$A[dat.b$Z == 0]) * (1 - dat.b$Y[dat.b$Z == 0])),
mean(dat.b$A[dat.b$Z == 0] * (1 - dat.b$Y[dat.b$Z == 0])),
mean((1 - dat.b$A[dat.b$Z == 0]) * dat.b$Y[dat.b$Z == 0]),
mean(dat.b$A[dat.b$Z == 0] * dat.b$Y[dat.b$Z == 0]),
mean((1 - dat.b$A[dat.b$Z == 1]) * (1 - dat.b$Y[dat.b$Z == 1])),
mean(dat.b$A[dat.b$Z == 1] * (1 - dat.b$Y[dat.b$Z == 1])),
mean((1 - dat.b$A[dat.b$Z == 1]) * dat.b$Y[dat.b$Z == 1]),
mean(dat.b$A[dat.b$Z == 1] * dat.b$Y[dat.b$Z == 1]))
boot.res[b,] <- c(gamma.l(pi.hat.b), gamma.u(pi.hat.b))
}
arrows(x0=0.37, y0=quantile(boot.res[,1],0.025),
x1=0.37, y1=quantile(boot.res[,2],0.975),
code=3, angle=90, length=0.05, lwd=2,col = ’darkgreen’)
## Estimated covariate-assisted BP-bounds
M <- 10 ## number of data splits
test.indices <- list()
remaining <- 1:n
for (m in 1:(M-1)) {
test.indices[[m]] <- sample(remaining, floor(n/M), replace = F)
remaining <- remaining[! (remaining %in% test.indices[[m]])]
}
test.indices[[M]] <- remaining
analysis <- function(dat.train, dat.test) {
pi.ya.0.rf <- rpart(Y.A ~ X1 + X2, data = dat.train[dat.train$Z==0, ])
dat.test <- cbind.data.frame(dat.test, predict(pi.ya.0.rf, type = ’prob’,
newdata = dat.test))
40## compile pi.hat_{ya.0} estimates
colnames(dat.test)[(ncol(dat.test) - 3):ncol(dat.test)] <-
c("pi_00.0", "pi_01.0", "pi_10.0", "pi_11.0")
pi.ya.1.rf <- rpart(Y.A ~ X1 + X2, data = dat.train[dat.train$Z==1, ])
dat.test <- cbind.data.frame(dat.test, predict(pi.ya.1.rf, type = ’prob’,
newdata = dat.test))
## compile pi.hat_{ya.1} estimates
colnames(dat.test)[(ncol(dat.test) - 3):ncol(dat.test)] <-
c("pi_00.1", "pi_01.1", "pi_10.1", "pi_11.1")
lambda.1 <- 0.5
## compile all the IF contributions
dat.test$psi_00.0 <- dat.test$pi_00.0 +
(1 - dat.test$Z) * ((1 - dat.test$Y) *
(1 - dat.test$A) - dat.test$pi_00.0) /
(1 - lambda.1)
dat.test$psi_01.0 <- dat.test$pi_01.0 +
(1 - dat.test$Z) * ((1 - dat.test$Y) * dat.test$A - dat.test$pi_01.0) /
(1 - lambda.1)
dat.test$psi_10.0 <- dat.test$pi_10.0 +
(1 - dat.test$Z) * (dat.test$Y * (1 - dat.test$A) - dat.test$pi_10.0) /
(1 - lambda.1)
dat.test$psi_11.0 <- dat.test$pi_11.0 +
(1 - dat.test$Z) * (dat.test$Y * dat.test$A - dat.test$pi_11.0) /
(1 - lambda.1)
dat.test$psi_00.1 <- dat.test$pi_00.1 +
dat.test$Z *
((1 - dat.test$Y) * (1 - dat.test$A) - dat.test$pi_00.1) / lambda.1
dat.test$psi_01.1 <- dat.test$pi_01.1 +
dat.test$Z * ((1 - dat.test$Y) * dat.test$A - dat.test$pi_01.1) / lambda.1
dat.test$psi_10.1 <- dat.test$pi_10.1 +
dat.test$Z * (dat.test$Y * (1 - dat.test$A) - dat.test$pi_10.1) / lambda.1
dat.test$psi_11.1 <-dat.test$pi_11.1 +
dat.test$Z * (dat.test$Y * dat.test$A - dat.test$pi_11.1) / lambda.1
pi.hats <- dat.test[,(ncol(dat.test) - 15):(ncol(dat.test) - 8)]
psi.hats <- dat.test[,(ncol(dat.test) - 7):ncol(dat.test)]
p.l.hats <- cbind(p.l1(pi.hats)[,1], p.l2(pi.hats)[,1], p.l3(pi.hats)[,1],
p.l4(pi.hats)[,1], p.l5(pi.hats)[,1], p.l6(pi.hats)[,1],
p.l7(pi.hats)[,1], p.l8(pi.hats)[,1])
p.u.hats <- cbind(p.u1(pi.hats)[,1], p.u2(pi.hats)[,1], p.u3(pi.hats)[,1],
p.u4(pi.hats)[,1], p.u5(pi.hats)[,1], p.u6(pi.hats)[,1],
p.u7(pi.hats)[,1], p.u8(pi.hats)[,1])
L.hats <- cbind(p.l1(psi.hats)[,1], p.l2(psi.hats)[,1], p.l3(psi.hats)[,1],
p.l4(psi.hats)[,1], p.l5(psi.hats)[,1], p.l6(psi.hats)[,1],
p.l7(psi.hats)[,1], p.l8(psi.hats)[,1])
U.hats <- cbind(p.u1(psi.hats)[,1], p.u2(psi.hats)[,1], p.u3(psi.hats)[,1],
41p.u4(psi.hats)[,1], p.u5(psi.hats)[,1], p.u6(psi.hats)[,1],
p.u7(psi.hats)[,1], p.u8(psi.hats)[,1])
argmax.p.l <- apply(p.l.hats, MARGIN = 1, FUN = which.max)
argmin.p.u <- apply(p.u.hats, MARGIN = 1, FUN = which.min)
IF.l <- sapply(1:nrow(dat.test), function(i) { L.hats[i, argmax.p.l[i]] },
simplify = 0)
IF.u <- sapply(1:nrow(dat.test), function(i) { U.hats[i, argmin.p.u[i]] },
simplify = 0)
## IF-based Balke & Pearl bounds
return(c(lower = mean(IF.l), upper = mean(IF.u),
lower.var = var(IF.l), upper.var = var(IF.u)))
}
results.BP <- lapply(test.indices, function(inds) {
analysis(dat.train = dat[-inds,], dat.test = dat[inds,])
})
BP.low <- mean(sapply(results.BP, function(x) {x[1]}, simplify = 0))
BP.upp <- mean(sapply(results.BP, function(x) {x[2]}, simplify = 0))
BP.low.var <- mean(sapply(results.BP, function(x) {x[3]}, simplify = 0))
BP.upp.var <- mean(sapply(results.BP, function(x) {x[4]}, simplify = 0))
arrows(x0=0.67, y0=BP.low + qnorm(0.025) * sqrt(BP.low.var / n),
x1=0.67, y1=BP.upp + qnorm(0.975) * sqrt(BP.upp.var / n),
code=3, angle=90, length=0.05, lwd=2, col = ’darkgreen’)
42