arXiv:2301.11481v2  [cs.GT]  27 Apr 2023Are Equivariant Equilibrium Approximators Beneﬁcial?
Zhijian Duan1,Yunxuan Ma1,Xiaotie Deng1,2
1Center on Frontiers of Computing Studies, Peking University
2Center for Multi-Agent Research, Institute for AI, Peking Univer sity
{zjduan,charmingmyx,xiaotie }@pku.edu.cn
Abstract
Recently, remarkable progress has been made by approximati ng Nash equilibrium (NE), correlated equilib-
rium(CE), andcoarse correlated equilibrium(CCE) through functionapproximationthattrains aneuralnetwork
to predict equilibria from game representations. Furtherm ore, equivariant architectures are widely adopted in
designing such equilibrium approximators in normal-form g ames. In this paper, we theoretically characterize
beneﬁts and limitations of equivariant equilibrium approx imators. For the beneﬁts, we show that they enjoy
better generalizability than general ones and can achieve b etter approximations when the payoﬀ distribution
is permutation-invariant. For the limitations, we discuss their drawbacks in terms of equilibrium selection and
social welfare. Together, our results help to understand th e role of equivariance in equilibrium approximators.
1 Introduction
The equivariant equilibrium property states that, given a Nash Equilib rium (NE) solution of a game, the
permuted solution is also an NE for the game whose actions of repres entation are permuted in the same way. The
same property also holds in correlated equilibrium (CE) and coarse co rrelated equilibrium (CCE), as well as the
approximate solutions for all three solution concepts.
In this paper, we are interested in understanding the equivariant e quilibrium property in designing neural
networks that predict equilibria from game payoﬀs, following such re cent approaches in designing equivariant equi-
librium approximators [ Feng et al. ,2021,Marris et al. ,2022] in normal-form games. Informally, such equivariant
approximators keep the same permutation of the output strateg ies (represented as vectors or tensors) when the in-
put game representations (e.g., the game payoﬀ tensors) are per muted1. While equivariant approximators achieved
empirical success, little work has theoretically discussed whether t hey are beneﬁcial.
1.1 Our Contributions
We theoretically characterize beneﬁts and limitations of equivariant NE, CE and CCE approximators. For the
beneﬁts, we ﬁrst show that equivariant approximators enjoy bet ter generalizability, where we evaluate the approxi-
mators using the maximum exploitability [ Lockhart et al. ,2019,Goktas and Greenwald ,2022] over all players. To
get such a result, we derive the generalization bounds and the samp le complexities of the NE, CE, and CCE approx-
imators: The generalization bounds oﬀer conﬁdence intervals on th e expected testing approximations based on the
empirical training approximations; The sample complexities describe h ow many training samples the equilibrium
approximatorsneedtoachievedesirablegeneralizability. Thegener alizationboundsandsamplecomplexitiesinclude
the covering numbers [ Shalev-Shwartz and Ben-David ,2014], which measure the representativeness of the approxi-
mators’ function classes. Afterward, we provethat the equivar iant approximatorshave lower coveringnumbers than
the general models, therefore have lower generalization bounds a nd sample complexities. We then show that the
equivariant approximators can achieve better approximation when the payoﬀ distribution is permutation-invariant.
As for the limitations, we ﬁnd the equivariant approximators unable t o ﬁnd all the equilibria of some normal-
form games. Such a result is caused by the limited representativene ss of the equivariant approximators’ function
class. Besides, we ﬁnd that the equivariant NE approximator may los e social welfare. Speciﬁcally, in an example
we constructed, while the maximum NE social welfare is large, the max imum social welfare of NEs that the
equivariant NE approximators could ﬁnd can be arbitrary close to ze ro. Such a negative result inspires us to
balance generalizability and social welfare when we design the approx imators’ architectures.
1We will provide a formal deﬁnition of equivariance equilibr ium approximators in Section 3
11.2 Further Related Work
Solving(approximate)NE,CE,andCCEforasinglegamearewellstud ied[Fudenberg et al. ,1998,Cesa-Bianchi and Lugosi ,
2006]. However, many similar games often need to be solved [ ?] , both in practice and in some multi-agent learn-
ing algorithms [ Marris et al. ,2021,Liu et al. ,2022]. For instance, in repeated traﬃc routing games [ Sessa et al. ,
2020], the payoﬀs of games depend on the capacity of the underlying net work, which can vary with time and
weather conditions. In repeated sponsored search auctions, ad vertisers value diﬀerent keywords based on the
current marketing environment [ Nekipelov et al. ,2015]. In many multi-agent learning algorithms such as Nash Q-
learning [ Hu and Wellman ,2003], Correlated-Q learning [ Greenwald et al. ,2003], V-learning [ Jin et al. ,2022] and
PSRO [Lanctot et al. ,2017], an NE, CE or CCE of a normal-form game need to be solved in every up date step.
In these settings, it is preferred to accelerate the speed of game solving by function approximation: Marris et al.
[2022] introduces a neural equilibrium approximator to approximate CE an d CCE for n-player normal-form games;
Feng et al. [2021] proposes a neural NE approximator in PSRO [ Lanctot et al. ,2017];Wu and Lisser [2022] designs
a CNN-based NE approximator for zero-sum bimatrix games. Diﬀere ntiable approximators have also been devel-
oped to learn QREs [ Ling et al. ,2018], NE in chance-constrained games [ Wu and Lisser ,2023], and opponent’s
strategy [ Hartford et al. ,2016].
Equivariance is an ideal property of the equilibrium approximator. We will discuss the literates of equivariant
approximators after formally deﬁning them in Section 3 .
1.3 Organization
The rest of our paper is organized as follows: In Section 2 we introduce the preliminary of game theory and
equilibrium approximators. In Section 3 we formally deﬁne the equivariance of equilibrium approximators. We
present our theoretical analysis of beneﬁts in Section 4 and limitations in Section 5 . We conclude and point out
the future work in Section 6 .
2 Preliminary
In this section, we introduce the preliminary and notations of our pa per. We also provide a brief introduction
to equilibrium approximators.
2.1 Game Theory
Normal-Form Game Let a normal-form game with joint payoﬀ ube Γu= (n,A,u), in which
•n∈N≥2is the number of players. Each player is represented by the index i∈[n].
•A=×i∈[n]Aiis the product action space of all players, where Ai={1,2,...,m i}. For player i∈[n],
letai∈ Aibe a speciﬁc action of i(An action is also referred to as a pure strategy). A joint action
a= (a1,a2,...,a n)∈Arepresents one play of the game in which the player itakes action ai. The action
spaceAis a Cartesian product that contains all possible joint actions. We ha ve|A|=/producttext
i∈[n]|Ai|=/producttext
i∈[n]mi.
•u= (ui)i∈[n]isthejointpayoﬀorutilityofthegame. uiisann-dimensionaltensor(ormatrixif n= 2)describ-
ing player i’s payoﬀ on each joint action. In our paper, following previous literat ures [Tsaknakis and Spirakis ,
2007,Deligkas et al. ,2022], we normalize all the elements of payoﬀ into [0 ,1].
A joint (mixed) strategy is a distribution over A. Letσ=×i∈[n]σibe a product strategy and π∈∆Abe a joint
(possibly correlated) strategy. Denote πias the marginal strategy of player iinπ. The expected utility of player i
underπis
ui(π) =Ea∼π[ui(a)] =/summationdisplay
a∈Aπ(a)ui(a).
Besides, on behalf of player i, the other players’ joint strategy is denoted as π−i, so asa−iandσ−i.
Nash Equilibrium (NE) We say a product strategy σ∗= (σ∗
1,σ∗
2,...,σ∗
n) is a NE if each player’s strategy is
the best response given the strategies of others, i.e.,
ui(σi,σ∗
−i)≤ui(σ∗
i,σ∗
−i),∀i∈[n],σi∈∆Ai. (NE)
2Computing NE for even general 2-player or 3-player games is PPAD- hard [Chen et al. ,2009,Daskalakis et al. ,
2009], which leads to research on approximate solutions. For arbitrary ǫ >0, we say a product strategy ˆ σis an
ǫ-approximate Nash equilibrium (ǫ-NE) if no one can achieve more than ǫutility gain by deviating from her current
strategy. Formally,
ui(σi,ˆσ−i)≤ui(ˆσi,ˆσ−i)+ǫ,∀i∈[n],σi∈∆Ai. (ǫ-NE)
The deﬁnition of ǫ-NE reﬂects the idea that players might not be willing to deviate from t heir strategies when the
amount of utility they could gain by doing so is tiny (not more than ǫ).
Coarse Correlated Equilibrium (CCE) We say a joint (possibly correlated) strategy π∗is a CCE if no player
can receive a higher payoﬀ by acting independently, i.e.,
ui(σi,π∗
−i)≤ui(π∗),∀i∈[n],σi∈∆Ai, (CCE)
and we say ˆ πis anǫ-approximate coarse correlated equilibrium ( ǫ-CCE) for ǫ >0 if
ui(σi,ˆπ−i)≤ui(ˆπ)+ǫ,∀i∈[n],σi∈∆Ai, (ǫ-CCE)
The diﬀerence between NE and CCE is that in an NE, players execute t heir strategy individually in a decentralized
way, while in a CCE, players’ strategies are possibly correlated. A st andard technique to correlate the strategy is
sending each player a signal from a centralized controller [ Shoham and Leyton-Brown ,2008].
Correlated Equilibrium (CE) CE is similar to CCE, except that in a CE, each player can observe her r ecom-
mended action before she acts. Thus, player ideviates her strategy through strategy modiﬁcation φi:Ai→Ai.
φimaps actions inAito possibly diﬀerent actions in Ai. Based on strategy modiﬁcation, we say a joint (possibly
correlated) strategy π∗is a CE if
/summationdisplay
a∈Aπ∗(a)ui(φi(ai),a−i)≤ui(π∗),∀i,∀φi, (CE)
and a joint strategy ˆ πis anǫ-approximate correlated equilibrium ( ǫ-CE) for ǫ >0 if
/summationdisplay
a∈Aˆπ(a)ui(φi(ai),a−i)≤ui(ˆπ)+ǫ,∀i,∀φi, (ǫ-CE)
Note that for a ﬁnite n-player normal-form game, at least one NE, CE, and CCE must exist. This is because
NE always exists [ Nash et al. ,1950] and NE⊆CE⊆CCE.
Equilibrium Approximation To evaluate the quality of a joint strategy to approximate an equilibr ium, we
deﬁne approximation based on exploitability [ Lockhart et al. ,2019,Goktas and Greenwald ,2022].
Deﬁnition 2.1 (Exploitability and Approximation) .Given a joint strategy π, theexploitability (or regret)Ei(π,u)
of player iis the maximum payoﬀ gain of iby deviating from her current strategy, i.e.,
Ei(π,u):= max
σ′
iui(σ′
i,π−i)−ui(π) = max
a′
iui(a′
i,π−i)−ui(π)
and the exploitability under strategy modiﬁcation ECE
i(π,u) of player iis the maximum payoﬀgain of iby deviating
through strategy modiﬁcation, i.e.,
ECE
i(π,u):= max
φi/summationdisplay
a∈Aπ(a)ui(φi(ai),a−i)−ui(π).
Theequilibrium approximation is deﬁned as the maximum exploitability over all players2, i.e.,
E(π,u):=/braceleftBigg
maxi∈[n]Ei(π,u),for NE and CCE
maxi∈[n]ECE
i(π,u),for CE
Based on approximation, we can restate the deﬁnition of solution co ncepts. A product strategy σis an NE of
game Γ uifE(σ,u) = 0 and is an ǫ-NE ifE(σ,u)≤ǫ. A joint strategy πis a (C)CE of Γ uifE(π,u) = 0 and is an
ǫ-(C)CE ifE(π,u)≤ǫ.
2A similar metric of equilibrium approximation is called Nik aido-Isoda function [ Nikaidˆ o and Isoda ,1955] orNash-
Conv[Lockhart et al. ,2019], which is the sum of exploitability over all players, i.e.,/summationtext
i∈[n]Ei(π,u).
3Algorithm 1 Example: learning NE/CCE approximator via minibatch SGD
1:Input:Training set S
2:Parameters: Number of iterations T >0, batch size B >0, learning rate η >0, initial parameters w0∈Rdof
the approximator model.
3:fort= 0toTdo
4:Receive minibatch St={u(1),...,u(B)}⊂S
5:Compute the empirical average approximation of St:
6:LSt(fwt)←1
B/summationtextB
i=1E(fwt(u(i)),u(i))
7:Update model parameters:
8:wt+1←wt−η∇wtLSt(fwt)
9:end for
2.2 Equilibrium Approximator
The equilibrium approximators, including NE, CE, and CCE approximato rs, aim to predict solution concepts
from game representations. In our paper, we ﬁx the number of pla yersnand action space A. We denote byUthe
set of all the possible game payoﬀs. The NE approximator fNE:U→× i∈[n]∆Aimaps a game payoﬀ to a product
strategy, where fNE(u)i∈∆Aiis player i’s predicted strategy. We deﬁne FNEas the function class of the NE
approximator. Similarly, we deﬁne (C)CE approximator as f(C)CE:U→∆Aand (C)CE approximator class as
F(C)CE.
An equilibrium approximator can be learned through machine learning p aradigms based on empirical data. For
instance, Feng et al. [2021] learn the NE approximator using the game payoﬀs generated in the learning process
of PSRO, and optimize the approximator by gradient descent in explo itability. Marris et al. [2022] learn the CE
and CCE approximators using the games i.i.d. generated from a manua lly designed distribution, and optimize
the approximators using maximum welfare minimum relative entropy los s. Such a loss balances the equilibrium
approximation, the social welfare, and the relative entropy of the joint strategy. Additionally, another simple
way to learn NE or CCE equilibrium approximator is to apply minibatch sto chastic gradient descent (SGD) on
approximation. Speciﬁcally, we denote w∈Rdas thed-dimensional parameter of the approximator, such as the
weights of the neural network. We can optimize wby the standard minibatch SGD algorithm on approximation
(SeeAlgorithm 1 ).
3 Equivariant Equilibrium Approximator
In this section, we introduce the equivariance of the equilibrium appr oximators and the way how we apply orbit
averaging [ Elesedy and Zaidi ,2021] to construct equivariant approximators. Equivariant approxima tor has been
developed in many literatures [ Hartford et al. ,2016,Feng et al. ,2021,Marris et al. ,2022,Wu and Lisser ,2022],
which we will discuss latter.
We ﬁrst deﬁne the permutation of a game. Let ρi:Ai→Aibe a permutation function of player i, which is
a bijection fromAitoAiitself. DeﬁneGi∋ρias the class of permutation function for player i, which forms an
Abelian group.
Deﬁnition 3.1 (Permutation of a game) .For a normal-form game Γ u= (n,u,A), we deﬁne the ρi-permutation of
payoﬀ tensor uasρiu= (ρiuj)j∈[n], in which
(ρiuj)(ai,a−i) =uj(ρ−1
i(ai),a−i),∀a∈A.
We also deﬁne the ρi-permutation of joint strategy πasρiπ, where
(ρiπ)(ai,a−i) =π(ρ−1
i(ai),a−i),∀a∈A,
and theρi-permutation of product strategy σasρiσ= (ρiσj)j∈[n], where
∀aj∈Aj,ρiσj(aj) =/braceleftBigg
σj(aj),ifj/\e}atio\slash=i
σi(ρ−1
iai),ifj=i
4Equivariant NE Approximator Considering the relationship of ρi-permuted game and ρi-permuted product
strategy, we have the following result for NE:
Lemma 3.2. In a normal-form game Γu= (n,u,A), for arbitrary player i∈[n]and any ( ǫ-)NE strategy σ=
(σi,σ−i),ρiσ= (ρiσi,σ−i)is also an ( ǫ-)NE for the ρi-permuted game Γρiu.
Lemma 3.2 tells the unimportance of action order with respect to NE approxima tion. Based on it, we can
summarize two ideal properties for NE approximators, which are de ﬁned as follows:
Deﬁnition 3.3 (Player-Permutation-Equivariance, PPE) .We say an NE approximator fNEsatisﬁes playeri
permutation-equivariant ( i-PE)if for arbitrary permutation function ρi∈Giwe have
fNE(ρiu)i=ρifNE(u)i, (i-PE)
Moreover, we say fNEisplayer-permutation-equivariant (PPE) iffNEsatisﬁesi-PEfor all player i∈[n].
Deﬁnition 3.4 (Opponent-Permutation-Invariance,OPI) .WesayanNEapproximator fNEisopponent ipermutation-
invariant ( i-PI)if for any other player j∈[n]−{i}and arbitrary permutation function ρi∈Giwe have
fNE(ρiu)j=fNE(u)j,∀j/\e}atio\slash=i (i-PI)
and we say fNEisopponent-permutation-invariant (OPI) iffNEsatisﬁesi-PIfor all player i∈[n].
Equivariant (C)CE approximator Considering the relationship of ρi-permuted game and ρi-permuted joint
strategy, we have a similar result for CE and CCE:
Lemma 3.5. In a normal-form game Γu= (n,u,A), for an arbitrary player i∈[n]and any ( ε-)CE or ( ǫ-)CCE
strategyπ,ρiπis also an ( ε-)CE or ( ǫ-)CCE for the ρi-permuted game Γρiu.
Inspired by Lemma 3.5 , we can also summarize an ideal property for CE and CCE approximat ors deﬁned as
follows.
Deﬁnition 3.6 (Permutation-Equivariance,PE) .We say an (C)CE approximator f(C)CEisplayeripermutation-
equivariant ( i-PE)if for permutation function ρi∈Giwe have
f(C)CE(ρiu) =ρif(C)CE(u),
and we say f(C)CEispermutation-equivariant (PE) iff(C)CEsatisﬁesi-PE for all player i∈[n].
Equivariant Approximators in Literature For two-player games, Feng et al. [2021] propose an MLP-based
NE approximator that satisﬁes both PPE and OPI for zero-sum gam es. Additionally, they also design a Conv1d-
based NE approximator that satisﬁes PPE only. Hartford et al. [2016] give a PPE approximator to predict players’
strategies. The traditional algorithms Tsaknakis and Spirakis [2007] andDeligkas et al. [2022], which approximate
NE by optimization, are also PPE and OPI to payoﬀ and the initial strat egies. For n-player general games,
Marris et al. [2022] provide a permutation-equivariant approximator to approximate CE and CCE. Equivariant
architectures are also adopted in optimal auction design [ Rahme et al. ,2021,Duan et al. ,2022,Ivanov et al. ,2022],
andQin et al. [2022] theoretically characterize the beneﬁts of permutation-equivar iant in auction mechanisms. We
follow the rough idea of Qin et al. [2022] when we analyze the beneﬁts of equivariant equilibrium approximato rs.
3.1 Orbit Averaging
Orbit averaging is a well-known method to enforce equivariance or inv ariance for a function [ Schulz-Mirbach ,
1994]. It averages the inputs of a function over the orbit of a group (e.g ., the permutation group in our paper).
Orbit Averaging for NE Approximator Foran NEapproximator fNEand anyplayer i∈[n], wecanconstruct
ai-PIori-PENE approximator by averaging with respect to all the permutations of player i. Speciﬁcally, we
construct an i-PINE approximator by operator Oiwith
(OifNE)(u)j=/braceleftBigg
fNE(u)i ,ifj=i
1
|Ai|!/summationtext
ρi∈GifNE(ρiu)j,otherwise
5and we construct an i-PENE approximator by operator Piwith:
(PifNE)(u)j=/braceleftBigg
1
|Ai|!/summationtext
ρi∈Giρ−1
ifNE(ρiu)i,ifj=i
fNE(u)j ,otherwise
Lemma 3.7.OifNEisi-PIandPifNEisi-PE. Specially, if fNEis already i-PIori-PE, then we haveOifNE=fNE
orPifNE=fNE, respectively.
To construct a PPE or OPI NE approximator, we composite the oper ators with respect to all players. Let
O=O1◦O2◦···◦O nandP=P1◦P2◦···◦P n, we get the following corollary:
Lemma 3.8.OfNEis OPI andPfNEis PPE. If fNEis already OPI or PPE, we have OfNE=fNEorPfNE=
fNE, respectively.
Furthermore, we can also compose P◦Oto construct a NE approximator with both PPE and OPI.
Orbit Averaging for (C)CE Approximator For CE or CCE approximator f, we deﬁneQi-project for player
i∈[n] to construct an i-PE approximator, which averages with respect to all the permuta tions of player i.
(Qif(C)CE)(u) =1
|Ai|!/summationdisplay
ρi∈Giρ−1
if(C)CE(ρiu)
Similarly, we deﬁne Q=Q1◦Q2◦···◦Q nas the composite operator.
Lemma 3.9.Qif(C)CEisi-PE andQf(C)CEis PE. Speciﬁcally, If f(C)CEis already i-PE or PE, then we have
Qif(C)CE=f(C)CEorQf(C)CE=f(C)CE, respectively.
Combined with Lemma 3.7 ,Lemma 3.8 andLemma 3.9 , we can have the following corollary directly.
Corollary 3.10. O2=O,P2=P,Q2=Q.
The beneﬁt of using orbit averaging is shown in the following lemma:
Lemma 3.11. DenoteXas an idempotent operator, i.e. X2=X(e.g.O,PorQ). For function class Fof NE,
CE or CCE approximator, let FXbe any subset ofFthat is closed under X, thenXFXis the largest subset of FX
that is invariant under X.
According to Lemma 3.8 ,Lemma 3.9 andLemma 3.11 ,OFNE(orPFNE/QF(C)CE) is the largest subset of
FNE(orFNE/F(C)CE) with the corresponding property (OPI, PPE, or PE) if FNE(orFNE/F(C)CE) is closed
operator underO(orP/Q). The result tells that the orbit averaging operators, while enforc ing the operated
function to be equivariance or invariance, keep as large capacity of the function class as possible. Therefore, we
believe that orbit averaging is an ideal approach to constructing eq uivariant or invariant functions.
4 Theoretical Analysis of Beneﬁts
In this section, we theoretically analyzethe beneﬁts ofequivariant approximatorswith respect to generalizability
and approximation.
4.1 Beneﬁts for Generalization
We ﬁrst derive the generalization bound and sample complexity for ge neral approximator classes, and then we
show the beneﬁts of equivariant approximators by applying orbit av eraging to the approximators.
Therepresentativenessofanapproximatorclassismeasuredbyt hecoveringnumbers[ Shalev-Shwartz and Ben-David ,
2014] underℓ∞-distance, which are deﬁned as follows:
Deﬁnition 4.1 (ℓ∞-distance) .Theℓ∞-distance between two equilibrium approximators f,gis:
ℓ∞(f,g) = max
u∈U/ba∇dblf(u)−g(u)/ba∇dbl,
6where we deﬁne the distance of two product strategies σandσ′as
/ba∇dblσ1−σ2/ba∇dbl= max
i∈[n]/summationdisplay
ai∈Ai|σ1
i(ai)−σ2
i(ai)|
and the distance of two joint strategy πandπ′as
/ba∇dblπ1−π2/ba∇dbl=/summationdisplay
a∈A|π1(a)−π2(a)|
Deﬁnition 4.2 (r-covering number) .Forr >0, we say function class Frr-covers another function class Funder
ℓ∞-distance if for all function f∈F, there exists fr∈Frsuch that/ba∇dblf−fr/ba∇dbl∞≤r. Ther-covering number
N∞(F,r) ofFis the cardinality of the smallest function class Frthatr-coversFunderℓ∞-distance.
Based on covering numbers, we provide the generalization bounds o f NE, CE and CCE approximators. The
bounds describe the diﬀerence between the expected testing app roximation and empirical training approximation.
Theorem 4.3. [Generalization bound] For function class Fof NE, CE or CCE approximator, with probability at
least1−δover draw of the training set S(with size m) from payoﬀ distribution D, for all approximator f∈Fwe
have
Eu∼D[E(f(u),u)]−1
m/summationdisplay
u∈SE(f(u),u)≤2·inf
r>0{/radicalbigg
2lnN∞(F,r)
m+Lr}+4/radicalbigg
2ln(4/δ)
m,
whereL= 2nfor NE approximator, and L= 2for CE and CCE approximators.
Toget the theorem, weﬁrst showthat allthree equilibrium approxim ationsareLipschitz continuouswith respect
to strategies. Afterward, we derive the Rademacher complexity [ Bartlett and Mendelson ,2002] of the expected
approximation based on the Lipschitz continuity and covering numbe rs. SeeAppendix B.1 for the detailed proof.
We can see from Theorem 4.3 that, with a large enough training set, the generalization gaps of eq uilibrium
approximators go to zero if the covering number N∞(F,r) is bounded. As a result, we can estimate the expected
testing performance through the empirical training performance .
We canalsoderivethe samplecomplexitiesofequilibriumapproximators to achievethe desirablegeneralizability.
Theorem 4.4. [Sample complexity] For ǫ,δ∈(0,1), function classFof NE, CE or CCE approximator and
distributionD, with probability at least 1−δover draw of the training set Swith
m≥9
2ǫ2/parenleftbigg
ln2
δ+lnN∞(F,ǫ
3L)/parenrightbigg
games sampled from D,∀f∈Fwe have
Eu∼D[E(f(u),u)]≤1
m/summationdisplay
u∈SE(f(u),u)+ǫ,
whereL= 2nfor NE approximators, and L= 2for CE and CCE approximators.
The proof is based on the Lipschitz continuity of approximation, unif orm bound, and concentration inequality.
SeeAppendix B.2 for details. Theorem 4.4 is also called the uniform convergence of function class F, which is a
suﬃcient condition for agnostic PAC learnable [ Shalev-Shwartz and Ben-David ,2014].
As for the beneﬁts of equivariant approximators for generalizabilit y, the following result indicates that the
projected equilibrium approximators have smaller covering numbers .
Theorem 4.5. TheO-projected,P-projected andQ-projected approximator classes have smaller covering num bers,
i.e.,∀r >0we have
N∞(OFNE,r)≤N∞(FNE,r),
N∞(PFNE,r)≤N∞(FNE,r),
N∞(QF(C)CE,r)≤N∞(F(C)CE,r)
The proof is done by showing all the operators are contraction map pings. See Appendix B.3 for details.
Both the generalization bounds in Theorem 4.3 and the sample complexities in Theorem 4.4 decrease with the
decrease of covering numbers N∞(F,r). Thus, we can see from Theorem 4.5 that both PPE and OPI can improve
the generalizability of NE approximators, and PE can improve the gen eralizability of CE and CCE approximators.
74.2 Beneﬁts for Approximation
We then show the beneﬁts of equivariance for approximation when t he payoﬀ distribution is invariant under
permutation. The permutation-invariant distribution holds when th e action is anonymous or indiﬀerent or when
we pre-train the equilibrium approximators using a manually designed d istribution [ Marris et al. ,2022].
(C)CE Approximator The following theorem tells the beneﬁt of permutation-equivariance in decreasing the
exploitability of (C)CE approximators.
Theorem 4.6. When the payoﬀ distribution Dis invariant under the permutation of payoﬀs, the Q-projected
(C)CE approximator has a smaller expected equilibrium appr oximation. Formally, for all f(C)CE∈F(C)CEand
permutation-invariant distribution D, we have
Eu∼D[E(Qf(C)CE(u),u)]≤Eu∼D[E(f(C)CE(u),u)],
The proof is done by using the convexity of approximation. See Appendix B.4 for details. We can see from
Theorem 4.6 that, when payoﬀ distribution is invariant under permutation, it is be neﬁcial to use equivariant archi-
tecture as the CE or CCE approximators.
NE Approximator As for NE approximator, we have similar results.
Theorem 4.7. For bimatrix constant-sum games, when the payoﬀ distributi onDis invariant under the permutation
of payoﬀs, then the X-projected (X∈{O,P}) NE approximator has a smaller expected exploitability. Fo rmally, for
allfNE∈FNEand permutation-invariant distribution Dfor bimatrix constant-sum games, we have
Eu∼D[/summationdisplay
iEi((XfNE)(u),u)]≤Eu∼D[/summationdisplay
iEi(fNE(u),u)]
Theorem 4.8. When the payoﬀ distribution Dis invariant under the permutation of payoﬀs, and fNEsatisﬁes
OPI, then theP-projected NE approximator has a smaller expected NE approx imation. Formally, for all fNE∈FNE
that is OPI and permutation-invariant distribution D, we have
Eu∼D[E((PfNE)(u),u)]≤Eu∼D[E(fNE(u),u)].
Theorem 4.9. For bimatrix games, when the payoﬀ distribution Dis invariant under the permutation of payoﬀs,
andfNEsatisﬁes PPE, then the O-projected NE approximator has a smaller expected NE approx imation. Formally,
for allfNE∈FNEthat is PPE and permutation-invariant distribution Dof bimatrix games, we have
Eu∼D[E((OfNE)(u),u)]≤Eu∼D[E(fNE(u),u)].
Theorem 4.8 andTheorem 4.9 tell that PPE and OPI approximators can achieve better approxima tion than
ones with only PPE or OPI. Meanwhile, we can see from Theorem 4.7 that for bimatrix constant-sum games (such
as zero-sum games), it can be preferred to introduce PPE or OPI t o the architectures.
5 Theoretical Analysis of Limitations
As we discussed in Section 4 , equivariant approximators enjoy better generalizability and bett er approximation
sometimes. However, as we will show, they have some limitations rega rding equilibrium selection and social welfare.
Such limitations attribute to the limited representativeness caused by equivariance.
5.1 Equilibrium Selection
We ﬁrst show that there may be equilibria points that equivariant app roximators will never ﬁnd. We illustrate
such limitation in permutation-invariant games, which is deﬁned as follo ws:
Deﬁnition 5.1 (Permutation- ρ-InvariantGame) .We saya game Γ uis permutation- ρ-invariant, where ρ=◦i∈[n]ρi,
if the payoﬀ uis permutation-invariant with respect to ρ. That is, ρu=u.
8Permutation- ρ-invariance indicates that one cannot distinguish joint action afromρausing only the payoﬀ u.
We’d like to provide an example to show more insight of permutation- ρ-invariant games:
Example 5.2. For a2-player game Γu= (2,u= (u1,u2),A= ([m1],[m2])), Letρi= (mi,mi−1,...,1)and
ρ=ρ1◦ρ2. If one of the following conditions holds, then uis permutation- ρ-invariant:
1.u1andu2are symmetric and persymmetric (i.e., symmetric with respe ct to the northeast-to-southwest diago-
nal) squares.
2. Both u1andu2are centrosymmetric, i.e., ui(x,y) =ui(m1+1−x,m2+1−y)fori∈{1,2},x∈[m1]and
y∈[m2].
For permutation ρ= (◦i∈[n]ρi) and player k∈[n], we denote the set of non-ﬁxed actions of player kunderρkas
V(ρk):={ak|ak∈Ak,ρk(ak)/\e}atio\slash=ak}.
Basedon V(ρk), weﬁndsomeequilibriapointsofpermutation- ρ-invariantgamesthatanyequivariantapproximators
will never ﬁnd.
Theorem 5.3. For a permutation- ρ-invariant game Γu. if there is a pure NE a∗= (a∗
i)i∈[n]and at least one player
k∈[n]such that a∗
k∈V(ρk), thena∗will never be found by any NE approximator with both PPE and OP I. Besides,
a∗(as a pure CE or CCE) will also never be found by any CE or CCE app roximator with PE.
We illustrate Theorem 5.3 by the following example:
Example 5.4. Consider a bimatrix game with identity utility
u=/bracketleftbigg
1,10,0
0,01,1/bracketrightbigg
There are two pure NE (bolded in the above matrix) and one mixe d NE of σ1= (0.5,0.5)andσ2= (0.5,0.5). Let
ρibe the unique permute function (except for identity functio n) of player i∈[2], andρ=ρ1◦ρ2. The game is
permutation- ρ-invariant.
Case 1: Letfbe a permutation-equivariant CE or CCE approximator, and de noteπ=f(u). We have
π=f(u)(a)=f(ρu)(b)=ρf(u),
where(a)holds by permutation- ρ-invariance of u, and(b)holds by PE of f. Thus, we have π1,1=π2,2∈[0,1
2]and
π1,2=π2,1∈[0,1
2]. As a result, the two pure (C)CEs cannot be found.
Case 2: Letfbe a NE approximator that holds PPE and OPI. Denote f(u) = (σ1,σ2), whereσ1= (p1,1−p1)
andσ2= (p2,1−p2). By PPE and OPI of f, we have
f(u)1= (p1,1−p1)(a)=f(ρ1ρ2u)1(b)=ρ1f(ρ2u)1(c)=ρ1f(u)1= (1−p1,p1),
where(a)holds by permutaion- ρ-invariance of u,(b)holds by PPE of f, and(c)holds by OPI of f. As a result,
the only NE that fcould ﬁnd is the mixed NE.
As we can see from the example and Theorem 5.3 , the equivariance, while introducing inductive bias to the
approximator architecture, is also a strong constraint. Such a co nstraint is why the equivariant approximators
cannot ﬁnd all the equilibria points.
5.2 Social Welfare
The social welfare of a joint strategy πis deﬁned as the sum of all players’ utilities, i.e.,
SW(π,u) =/summationdisplay
i∈[n]ui(π).
The equilibrium with higher social welfare is usually preferred [ Marris et al. ,2022].
To analyze the social welfare of equivariant approximators, we deﬁ ne the worst social welfare ratio as follows:
9Deﬁnition 5.5. For anyN,M≥2 and two NE (or CE/CCE) approximator classes F1,F2that target on games
with number of players n≤Nand|Ai|≤M, we deﬁne the worst social welfare ratio of F1overF2as:
SWRN,M(F1,F2):= inf
Dmaxf1∈F1Eu∼DSW(f1(u),u)
maxf2∈F2Eu∼DSW(f2(u),u)
SWRN,M(F1,F2) measures the relative representativeness of F1overF2in terms of social welfare. Based on
that, we have the following result for equivariant CE and CCE approx imator classes:
Theorem 5.6. GivenN,M≥2, letF(C)CE
PEbe the function class (target on games with number of players n≤N
and|Ai|≤M) of all the (C)CE approximators with PE. Denote by F(C)CE
generalthe function class of all the (C)CE
approximators. Then we have
SWRN,M(F(C)CE
PE,F(C)CE
general) = 1.
Theorem 5.6 tells that, while the permutation-equivariant (C)CE approximator c lass may not be able to ﬁnd
all the (C)CE in a game, it can keep the social welfare of the output s olutions.
However, when considering equivariant NE approximators, we have the following negative result:
Theorem 5.7. GivenN,M≥2, letFNE
OPI,FNE
PPEandFNE
bothbe the function classes (target on games with number
of players n≤Nand|Ai|≤M) of all the NE approximators with OPI, PPE and both. Denote th e function class
of all the NE approximators as FNE
general. Then we have
SWRN,M(FNE
OPI,FNE
general) =1
MN−1, (1)
SWRN,M(FNE
PPE,FNE
general)≤1
M, (2)
SWRN,M(FNE
both,FNE
general) =1
MN−1. (3)
Additionally, when M≥3, denote by /tildewideFNE
boththe function class of all the NE oracles (functions that alwa ys output
exact NE solutions of the input games) with both PPE and OPI, a nd by/tildewideFNE
generalthe function class of all the NE
oracles. Then we have
SWRN,M(/tildewideFNE
both,/tildewideFNE
general) = 0. (4)
The proof is done by construction (See Appendix C.3 for details). As an illustration of Equation (4) , consider
a bimatrix game with the following payoﬀ:
u=
1,1 0 ,0 0,1
2+ε
0,0 1 ,1 0,1
2+ε
1
2+ε,01
2+ε,0ε,ε

forǫ∈(0,1
2). The maximum NE (the upper-left corner of u) social welfare is 2, which can be found by at least one
NE oracle in /tildewideFNE
general. However, the only NE (the lower-right corner of u) that the NE oracles in /tildewideFNE
bothcould ﬁnd
only has a social welfare of 2 ǫ. As a result,
SWR2,3(/tildewideFNE
both,/tildewideFNE
general)≤2ǫ
2=ǫ,
which goes to zero as ǫ→0. Recall that we always have SWR N,M≥0, thusEquation (4) holds when N= 2 and
M= 3.
Theorem 5.7 tells that equivariant NE approximatorsmay lose some social welfare while enjoying better general-
izability. Sucha resultinspiresusto balancegeneralizabilityand social welfarewhen designingthe NEapproximator
architecture.
6 Conclusion and Future Work
In this paper, we theoretically analyze the beneﬁts and limitations of equivariant equilibrium approximators,
including player-permutation-equivariant(PPE) and opponent-pe rmutation-invariant (OPI) NE approximator, and
permutation-equivariant (PE) CE and CCE approximators. For the beneﬁts, we ﬁrst show that these equivariant
10approximators enjoy better generalizability. To get the result, we derive the generalization bounds and sample
complexities based on covering numbers, and then we prove that th e symmetric approximators have lower covering
numbers. We then show that the equivariant approximators can de crease the exploitability when the payoﬀ distri-
bution is invariant under permutation. For the limitations, we ﬁnd the equivariant approximators may fail to ﬁnd
some equilibria points due to their limited representativeness caused by equivariance. Besides, while equivariant
(C)CE approximators can keep the social welfare, the equivariant NE approximators reach a small worst social
welfare ratio comparing to the general approximators. Such a res ult indicates that equivariance may reduce social
welfare; therefore, we’d better balance the generalizability and so cial welfare when we design the architectures of
NE approximators.
As for future work, since in our paper we assume the training and te sting payoﬀ distribution are the same, an
interesting topic is to study the beneﬁts of equivariant approximat ors under the payoﬀ distribution shift. Moreover,
since we consider ﬁxed and discrete action space, another interes ting future direction is to analyze the beneﬁts of
equivariant approximators in varying or continuous action space.
References
PeterLBartlettandShaharMendelson. Rademacherandgaussian complexities: Riskboundsandstructuralresults.
Journal of Machine Learning Research , 3(Nov):463–482, 2002.
Nicolo Cesa-Bianchi and G´ abor Lugosi. Prediction, learning, and games . Cambridge university press, 2006.
Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player Nash equilibria.
Journal of the ACM (JACM) , 56(3):1–57, 2009.
Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimit riou. The complexity of computing a Nash
equilibrium. SIAM Journal on Computing , 39(1):195–259, 2009.
Argyrios Deligkas, Michail Fasoulakis, and Evangelos Markakis. A poly nomial-time algorithm for 1/3-approximate
Nash equilibria in bimatrix games. In 30th Annual European Symposium on Algorithms, ESA , 2022.
Zhijian Duan, Dinghuai Zhang, Wenhan Huang, Yali Du, Jun Wang, Ya odong Yang, and Xiaotie Deng. Towards
the PAC learnability of Nash equilibrium. arXiv preprint arXiv:2108.07472 , 2021.
Zhijian Duan, Jingwu Tang, Yutong Yin, Zhe Feng, Xiang Yan, Manzil Z aheer, and Xiaotie Deng. A context-
integrated transformer-based neural network for auction des ign. InInternational Conference on Machine Learn-
ing, pages 5609–5626. PMLR, 2022.
Paul D¨ utting, Zhe Feng, Harikrishna Narasimhan, David Parkes, a nd Sai Srivatsa Ravindranath. Optimal auctions
through deep learning. In International Conference on Machine Learning , pages 1706–1715. PMLR, 2019.
Bryn Elesedy and Sheheryar Zaidi. Provably strict generalisation be neﬁt for equivariant models. In International
Conference on Machine Learning , pages 2959–2969. PMLR, 2021.
Xidong Feng, Oliver Slumbers, Ziyu Wan, Bo Liu, Stephen McAleer, Ying Wen, Jun Wang, and Yaodong Yang.
Neural auto-curricula in two-player zero-sum games. Advances in Neural Information Processing Systems , 34:
3504–3517, 2021.
Drew Fudenberg, FudenbergDrew, DavidKLevine, andDavidKLevin e.The theory of learning in games , volume2.
MIT press, 1998.
Denizalp Goktas and Amy Greenwald. Exploitability minimization in games a nd beyond. In Advances in Neural
Information Processing Systems , 2022.
Amy Greenwald, Keith Hall, Roberto Serrano, et al. Correlated Q-lea rning. In ICML, volume 3, pages 242–249,
2003.
JasonSHartford,JamesRWright,andKevinLeyton-Brown. Deep learningforpredictinghumanstrategicbehavior.
Advances in neural information processing systems , 29, 2016.
Junling Hu and MichaelPWellman. Nash Q-learningforgeneral-sumsto chasticgames. Journal of machine learning
research, 4(Nov):1039–1069, 2003.
11Dmitry Ivanov, Iskander Saﬁulin, Igor Filippov, and Ksenia Balabaev a. Optimal-er auctions through attention. In
Advances in Neural Information Processing Systems , 2022.
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning – a simple, eﬃcient, decentralized algorithm
for multiagent RL. In ICLR 2022 Workshop on Gamiﬁcation and Multiagent Solutions , 2022.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazarid ou, Karl Tuyls, Julien P´ erolat, David Silver,
and Thore Graepel. A uniﬁed game-theoretic approach to multiagen t reinforcement learning. Advances in neural
information processing systems , 30, 2017.
C. Ling, Fei Fang, and J. Z. Kolter. What game are we playing? End-t o-end learning in normal and extensive form
games. In IJCAI, pages 396–402, 2018.
Siqi Liu, Marc Lanctot, Luke Marris, and Nicolas Heess. Simplex neur al population learning: Any-mixture bayes-
optimality in symmetric zero-sum games. In International Conference on Machine Learning, ICML , 2022.
Edward Lockhart, Marc Lanctot, Julien P´ erolat, Jean-Baptiste Lespiau, Dustin Morrill, Finbarr Timbers, and Karl
Tuyls. Computing approximate equilibria in sequential adversarial ga mes by exploitability descent. In Sarit
Kraus, editor, IJCAI, pages 464–470. ijcai.org, 2019.
Luke Marris, Paul Muller, Marc Lanctot, Karl Tuyls, and Thore Gra epel. Multi-agent training beyond zero-sum
with correlated equilibrium meta-solvers. In International Conference on Machine Learning , pages 7480–7491.
PMLR, 2021.
Luke Marris, Ian Gemp, Thomas Anthony, Andrea Tacchetti, Siqi L iu, and Karl Tuyls. Turbocharging solution
concepts: Solving NEs, CEs and CCEs with neural equilibrium solvers. InAdvances in Neural Information
Processing Systems , 2022.
John F Nash et al. Equilibrium points in n-person games. Proceedings of the national academy of sciences , 36(1):
48–49, 1950.
Denis Nekipelov, VasilisSyrgkanis,andEvaTardos. Econometricsfo rlearningagents. In Proceedings of the sixteenth
acm conference on economics and computation , pages 1–18, 2015.
Hukukane Nikaidˆ o and Kazuo Isoda. Note on non-cooperative con vex games. Paciﬁc Journal of Mathematics , 5
(S1):807–815, 1955.
Tian Qin, Fengxiang He, Dingfeng Shi, Wenbing Huang, and Dacheng Ta o. Beneﬁts of permutation-equivariance
in auction mechanisms. In Advances in Neural Information Processing Systems , 2022.
Jad Rahme, Samy Jelassi, Joan Bruna, and S Matthew Weinberg. A pe rmutation-equivariant neural network
architecture for auction design. In Proceedings of the AAAI Conference on Artiﬁcial Intelligen ce, 2021.
Hanns Schulz-Mirbach. Constructing invariant features by avera ging techniques. In Proceedings of the 12th IAPR
International Conference on Pattern Recognition, Vol. 3-C onference C: Signal Processing (Cat. No. 94CH3440-5) ,
volume 2, pages 387–390. IEEE, 1994.
Pier Giuseppe Sessa, Ilija Bogunovic, Andreas Krause, and Maryam Kamgarpour. Contextual games: Multi-agent
learning with side information. Advances in Neural Information Processing Systems , 33:21912–21922, 2020.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms . Cambridge
university press, 2014.
Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and log ical foundations .
Cambridge University Press, 2008.
Haralampos Tsaknakis and Paul G Spirakis. An optimization approach for approximate Nash equilibria. In
International Workshop on Web and Internet Economics , pages 42–56. Springer, 2007.
Dawen Wu and Abdel Lisser. Using CNN for solving two-player zero-s um games. Expert Systems with Applications ,
page 117545, 2022.
Dawen Wu and Abdel Lisser. CCGnet: A deep learning approachto pr edict Nash equilibrium of chance-constrained
games.Information Sciences , 2023.
12A Omitted Proofs in Section 3
A.1 Useful Lemma
We ﬁrst introduce a lemma, which will be frequently used in the following proofs.
Lemma A.1.∀i,j∈[n],ρi∈Giwe have (ρiu)j(σi,σ−i) =uj(ρ−1
iσi,σ−i)and(ρiu)j(π) =uj(ρ−1
iπ)
Proof.Deﬁne/hatwideai:=ρ−1
iai. For product strategy σ= (σi)i∈[n],
(ρiu)j(σi,σ−i) =/summationdisplay
ai∈Ai/summationdisplay
a−i∈A−i(ρiu)j(ai,a−i)·σi(ai)·σ−i(a−i)
=/summationdisplay
ai∈Ai/summationdisplay
a−i∈A−iuj(ρ−1
iai,a−i)·σi(ai)·σ−i(a−i)
=/summationdisplay
ai∈Ai/summationdisplay
a−i∈A−iuj(ρ−1
iai,a−i)·(ρ−1
iσi)(ρ−1
iai)·σ−i(a−i)
=/summationdisplay
/hatwideai∈Ai/summationdisplay
a−i∈A−iuj(/hatwideai,a−i)·(ρ−1
iσi)(/hatwideai)·σ−i(a−i)
=uj(ρ−1
iσi,σ−i)
For joint strategy π,
(ρiu)j(π) =/summationdisplay
ai∈Ai/summationdisplay
a−i∈A−i(ρiuj)(ai,a−i)·π(ai,a−i)
=/summationdisplay
ai∈Ai/summationdisplay
a−i∈A−iuj(ρ−1
iai,a−i)·π(ai,a−i)
=/summationdisplay
ai∈Ai/summationdisplay
a−i∈A−iuj(ρ−1
iai,a−i)·(ρ−1
iπ)(ρ−1
iai,a−i)
=/summationdisplay
/hatwideai∈Ai/summationdisplay
a−i∈A−iuj(/hatwideai,a−i)·(ρ−1
iπ)(/hatwideai,a−i)
=uj(ρ−1
iπ)
A.2 Proof of Lemma 3.2
Lemma 3.2. In a normal-form game Γu= (n,u,A), for arbitrary player i∈[n]and any ( ǫ-)NE strategy σ=
(σi,σ−i),ρiσ= (ρiσi,σ−i)is also an ( ǫ-)NE for the ρi-permuted game Γρiu.
Proof.For player i, we have
Ei(ρiσ,ρiu) = max
ai∈Aiρiui(ai,ρiσ−i)−ρiui(ρiσ) = max
ai∈Aiρiui(ai,σ−i)−ρiui(ρiσi,σ−i)
= max
ai∈Aiui(ρ−1
iai,σ−i)−ui(ρ−1
iρiσi,σ−i)(a)= max
ai∈Aiui(ai,σ−i)−ui(σi,σ−i) =Ei(σ,u),
where (a) holds since ρiis a bijection onAi. For player j/\e}atio\slash=i, we have
Ej(ρiσ,ρiu) =max
aj∈Aρiuj(aj,ρiσ−j)−ρiuj(ρiσ) = max
aj∈Ajuj(aj,ρ−1
iρiσ−j)−uj(ρ−1
iρiσ)
= max
aj∈Ajuj(aj,σ−j)−uj(σ) =Ej(σ,u)
From above, we have E(ρiσ,ρiu) =E(σ,u), thus if σis aε-NE of Γ u, thenρiσmust be a ε-NE of Γ ρiu.
A.3 Proof of Lemma 3.5
Lemma 3.5. In a normal-form game Γu= (n,u,A), for an arbitrary player i∈[n]and any ( ε-)CE or ( ǫ-)CCE
strategyπ,ρiπis also an ( ε-)CE or ( ǫ-)CCE for the ρi-permuted game Γρiu.
13CCEFor player i, we have
Ei(ρiπ,ρiu) = max
ai∈Ai(ρiui)(ai,(ρiπ)−i)−(ρiui)(ρiπi)
= max
ai∈Ai(ρiui)(ai,(ρiπ)−i)−ui(ρ−1
iρiπi)
= max
ai∈Ai(ρiui)(ai,(ρiπ)−i)−ui(πi)
= max
ai∈Ai/summationdisplay
b∈A(ρiui)(ai,b−i)·(ρiπ)(b)−ui(πi)
= max
ai∈Ai/summationdisplay
bi∈Ai,b−i∈A−iui(ρ−1
iai,b−i)·π(ρ−1
ibi,b−i)−ui(πi)
= max
ai∈Ai/summationdisplay
bi∈Ai,b−i∈A−iui(ai,b−i)·π(bi,b−i)−ui(πi) ,ρiis a bijection onAi
=Ei(π,u)
For player j/\e}atio\slash=i, we have
Ej(ρiπ,ρiu) = max
aj∈Aj(ρiuj)(aj,(ρiπ)−j)−(ρiuj)(ρiπj)
= max
aj∈Aj(ρiuj)(aj,(ρiπ)−j)−uj(ρ−1
iρiπj)
= max
aj∈Aj(ρiuj)(aj,(ρiπ)−j)−uj(πj)
= max
aj∈Aj/summationdisplay
b∈A(ρiuj)(aj,b−j)·(ρiπ)(b)−uj(πj)
= max
aj∈Aj/summationdisplay
bi∈Ai,b−i∈A−iuj(aj,(b−j)−i,ρ−1
ibi)·π(ρ−1
ibi,b−i)−uj(πj)
= max
aj∈Aj/summationdisplay
bi∈Ai,b−i∈A−iuj(aj,(b−j)−i,bi)·π(bi,b−i)−uj(πj) ,ρiis a bijection onAi
=Ej(π,u)
Thus, we haveE(ρiπ,ρiu) =E(π,u). Thus, if πis aε-CCE of Γ u, thenρiπmust be a ε-CCE of Γ ρiu.
CEFor player j/\e}atio\slash=i, we have
ECE
j(ρiπ,ρiu) = max
φj:Aj→Aj/summationdisplay
a∈A(ρiπ)(a)·(ρiuj)(φj(aj),a−j)−(ρiuj)(ρiπ)
= max
φj:Aj→Aj/summationdisplay
a∈Aπ(ρ−1
iai,a−i)·uj(φj(aj),a−i,j,ρ−1
iai)−uj(π)
= max
φj:Aj→Aj/summationdisplay
a∈Aπ(ai,a−i)·uj(φj(aj),a−i,j,ai)−uj(π) ,ρiis a bijection onAi
=ECE
j(π,u)
For player i, we deﬁne operator ¯ ρias (¯ρiφi)(ai) =ρ−1
iφi(ρiai). We can verify that ¯ ρiis a bijection on {φi:
Ai→Ai}, because ¯·is a homomorphism in the sense that ρ1
i◦ρ2
i=ρ2
iρ1
iand¯·maps the identity mapping of Aito
the identity mapping of {Ai→Ai}. Speciﬁcally,
ρ1
i◦ρ2
iφi(ai) = (ρ1
i)−1(ρ2
iφi)(ρ1
iai) = (ρ1
i)−1(ρ2
i)−1φi(ρ2
iρ1
iai) =ρ2
iρ1
iφi(ai),
and
eiφi(ai) =e−1
iφi(eiai) =φi(ai).
14Based on ¯ ρi, we have
ECE
i(ρiπ,ρiu)
= max
φi:Ai→Ai/summationdisplay
a∈A(ρiπ)(a)·(ρiui)(φi(ai),a−i)−ui(π)
= max
φi:Ai→Ai/summationdisplay
a∈Aπ(ρ−1
iai,a−i)ui(ρ−1
iφi(ai),a−i)−ui(π)
= max
φi:Ai→Ai/summationdisplay
a∈Aπ(ρ−1
iai,a−i)ui(ρ−1
iφi(ρi(ρ−1
iai)),a−i)−ui(π)
= max
φi:Ai→Ai/summationdisplay
a∈Aπ(ai,a−i)ui(ρ−1
iφi(ρiai),a−i)−ui(π) ,ρiis a bijection onAi
= max
φi:Ai→Ai/summationdisplay
a∈Aπ(ai,a−i)ui((¯ρiφi)(ai),a−i)−ui(π)
= max
φi:Ai→Ai/summationdisplay
a∈Aπ(ai,a−i)ui(φi(ai),a−i)−ui(π) ,¯ρiis a bijection on{Ai→Ai}
=ECE
i(π,u)
Thus, we haveE(ρiπ,ρiu) =E(π,u), thus if πis aε-CE of Γ u, thenρiπmust be a ε-CE of Γ ρiu.
A.4 Proof of Lemma 3.7 toLemma 3.9
Lemma 3.7.OifNEisi-PIandPifNEisi-PE. Specially, if fNEis already i-PIori-PE, then we haveOifNE=fNE
orPifNE=fNE, respectively.
Proof.∀j/\e}atio\slash=i,ρ0∈Gi, for operatorOiwe have
(OifNE)(ρ0u)j=1
|Ai|!/summationdisplay
ρi∈GifNE(ρiρ0u)j(a)=1
|Ai|!/summationdisplay
/hatwideρi∈GifNE(/hatwideρiu)j= (OifNE)(u)j
where in ( a) we deﬁne /hatwideρi=ρiρ0, and (a) holds since ρ0is a bijection onGi. As a result,OifNEisi-PI.
For operatorPiwe have
(PifNE)(ρ0u)i=1
|Ai|!/summationdisplay
ρi∈Giρ−1
ifNE(ρiρ0u)j=ρ01
|Ai|!/summationdisplay
ρi∈Giρ−1
0ρ−1
ifNE(ρiρ0u)j
=ρ01
|Ai|!/summationdisplay
/hatwideρi∈Gi/hatwideρ−1
ifNE(/hatwideρiu)j=ρ0(PifNE)(u)i,
thereforePifNEisi-PE.
IffNEis already i-PI,∀j/\e}atio\slash=iwe have
OifNE(u)j=1
|Ai|!/summationdisplay
ρi∈GifNE(ρiu)j=1
|Ai|!/summationdisplay
ρi∈GifNE(u)j=fNE(u)j,
andOifNE(u)i=fNE(u)iaccording to deﬁnition of Oi. Therefore,OifNE=fNEfori-PIfNE.
IffNEis already i-PE, we have
PifNE(u)i=1
|Ai|!/summationdisplay
ρi∈Giρ−1
ifNE(ρiu)i=1
|Ai|!/summationdisplay
ρi∈Giρ−1
iρifNE(u)i=1
|Ai|!/summationdisplay
ρi∈GifNE(u)i=fNE(u)i,
and∀j/\e}atio\slash=i,PifNE(u)j=fNE(u)jaccording to deﬁnition of Pi. Therefore,PifNE=fNEfori-PEfNE.
Lemma 3.8.OfNEis OPI andPfNEis PPE. If fNEis already OPI or PPE, we have OfNE=fNEorPfNE=
fNE, respectively.
Proof.A direct inference from Lemma 3.7
15Lemma 3.9.Qif(C)CEisi-PE andQf(C)CEis PE. Speciﬁcally, If f(C)CEis already i-PE or PE, then we have
Qif(C)CE=f(C)CEorQf(C)CE=f(C)CE, respectively.
Proof.∀ρ0∈Gi, we have
(Qif(C)CE)(ρ0u) =1
|Ai|!/summationdisplay
ρi∈Giρ−1
if(C)CE(ρiρ0u) =ρ01
|Ai|!/summationdisplay
ρi∈Giρ−1
0ρ−1
if(C)CE(ρiρ0u)
=ρ01
|Ai|!/summationdisplay
/hatwideρi∈Gi/hatwideρ−1
if(C)CE(/hatwideρiu) =ρ0(Qif(C)CE)(u)
Iff(C)CEis already i-PE, we have
Qif(C)CE(u) =1
|Ai|!/summationdisplay
ρi∈Giρ−1
if(C)CE(ρiu) =1
|Ai|!/summationdisplay
ρi∈Giρ−1
iρif(C)CE(u) =1
|Ai|!/summationdisplay
ρi∈Gif(C)CE(u) =f(C)CE(u)
A.5 Proof of Lemma 3.11
Lemma 3.11. DenoteXas an idempotent operator, i.e. X2=X(e.g.O,PorQ). For function class Fof NE,
CE or CCE approximator, let FXbe any subset ofFthat is closed under X, thenXFXis the largest subset of FX
that is invariant under X.
Proof.We prove the three claims below.
1.XFX⊆FX.
2.X2FX=XFX.
3. IfXY=Y⊆F X, thenY⊆XF X
The ﬁrst claim holds because FXis closed underX, and the second claim holds because Xis idempotent. For
the third claim, from Y⊆F Xwe knowXY⊆XF X, thenY=XY⊆XF X.
We immediately know XFXis the largest subset of FXthat is invariant under X.
B Omitted Proofs in Section 4
B.1 Proof of Theorem 4.3
Theorem 4.3. [Generalization bound] For function class Fof NE, CE or CCE approximator, with probability at
least1−δover draw of the training set S(with size m) from payoﬀ distribution D, for all approximator f∈Fwe
have
Eu∼D[E(f(u),u)]−1
m/summationdisplay
u∈SE(f(u),u)≤2·inf
r>0{/radicalbigg
2lnN∞(F,r)
m+Lr}+4/radicalbigg
2ln(4/δ)
m,
whereL= 2nfor NE approximator, and L= 2for CE and CCE approximators.
Some of the proof techniques come from D¨ utting et al. [2019] andDuan et al. [2021]. We ﬁrst introduce some
useful lemmas. Denote ℓ:F×U→ Ras the loss function (such as ℓ(f,u):=E(f(u),u)). We measure the capacity
of the composite function class ℓ◦Fusing the empirical Rademacher complexity [ Bartlett and Mendelson ,2002]
on the training set S, which is deﬁned as:
RS(ℓ◦F):=1
mEx∼{+1,−1}m/bracketleftBig
sup
f∈Fm/summationdisplay
i=1xi·ℓ(f,u(i))/bracketrightBig
,
wherexis distributed i.i.d. according to uniform distribution in {+1,−1}. We have
16Lemma B.1 (Shalev-Shwartz and Ben-David [2014]).LetSbe a training set of size mdrawn i.i.d. from distribution
DoverU. Then with probability at least 1−δover draw of SfromD, for allf∈F,
Eu∼D[ℓ(f,u)]−1
m/summationdisplay
u∈Sℓ(l,u)≤2RS(ℓ◦F)+4/radicalbigg
2ln(4/δ)
m
Lemma B.2. If|ℓ(·)|≤cfor constant c >0and∀f,f′∈F,|ℓ(f,u)−ℓ(f′,u)|≤L/ba∇dblf−f′/ba∇dbl∞, then we have
Eu∼D[ℓ(f,u)]−1
m/summationdisplay
u∈Sℓ(l,u)≤2 inf
r>0/braceleftBigg
c/radicalbigg
2lnN∞(F,r)
m+Lr/bracerightBigg
+4/radicalbigg
2ln(4/δ)
m
Proof.For function class F, letFrwith|Fr|=N∞(F,r) be the function class that r-coversFfor some r >0.
Similarly,∀f∈F, denote fr∈Frbe the function that r-coversf. We have
RS(ℓ◦F) =1
mEx/bracketleftBig
sup
f∈Fm/summationdisplay
i=1xi·ℓ(f,u(i))/bracketrightBig
=1
mEx/bracketleftBig
sup
f∈Fm/summationdisplay
i=1xi·/parenleftbig
ℓ(fr,u(i))+ℓ(f,u(i))−ℓ(fr,u(i))/parenrightbig/bracketrightBig
≤1
mEx/bracketleftBig
sup
fr∈Frm/summationdisplay
i=1xi·ℓ(fr,u(i))/bracketrightBig
+1
mEx/bracketleftBig
sup
f∈Fm/summationdisplay
i=1|xi·Lr|/bracketrightBig
,|ℓ(f,u)−ℓ(fr,u)|≤L/ba∇dblf−fr/ba∇dbl∞=Lr
≤sup
fr∈Fr/radicaltp/radicalvertex/radicalvertex/radicalbtm/summationdisplay
i=1ℓ2(fr,u(i))·/radicalbig
2lnN∞(F,r)
m+Lr
mEx/ba∇dblx/ba∇dbl,the ﬁrst term holds by Massart’s lemma
≤√
c2m·/radicalbig
2lnN∞(F,r)
m+Lr
mEx/ba∇dblx/ba∇dbl
≤c/radicalbigg
2lnN∞(F,r)
m+Lr,
(5)
Combining Lemma B.1 andEquation (5) , we get
Eu∼D[ℓ(f,u)]−1
m/summationdisplay
u∈Sℓ(l,u)≤2 inf
r>0/braceleftBigg
c/radicalbigg
2lnN∞(F,r)
m+Lr/bracerightBigg
+4/radicalbigg
2ln(4/δ)
m
B.1.1 NE Approximator
Lemma B.3. For arbitrary product mixed strategy σandσ′, we have
|E(σ,u)−E(σ′,u)|≤2n/ba∇dblσ−σ′/ba∇dbl,
17Proof.∀σ,σ′, we deﬁne y−j:= (σ1,...,σ j−1,σ′
j+1,...,σ′
n). Then,∀i∈[n] we have
|ui(σ)−ui(σ′)|=|ui(σ1,σ2,...,σ n)−ui(σ′,σ′
2,...,σ′
n)|
=/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
j=1/parenleftBig
ui(σ1,...,σ j,σ′
j+1,...,σ′
n)−ui(σ1,...,σ′
j,σ′
j+1,...,σ′
n)/parenrightBig/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
j=1/parenleftBig
ui(σj,y−j)−ui(σ′
j,y−j)/parenrightBig/vextendsingle/vextendsingle/vextendsingle
=/vextendsingle/vextendsingle/vextendsinglen/summationdisplay
j=1/summationdisplay
aj(σj(aj)−σ′
j(aj))/summationdisplay
a−jui(aj,a−j)y−j(a−j)/vextendsingle/vextendsingle/vextendsingle
≤n/summationdisplay
j=1/summationdisplay
aj/vextendsingle/vextendsingle/vextendsingleσj(aj)−σ′
j(aj)/vextendsingle/vextendsingle/vextendsingle/summationdisplay
a−jui(aj,a−j)y−j(a−j)
≤n/summationdisplay
j=1/summationdisplay
aj/vextendsingle/vextendsingle/vextendsingleσj(aj)−σ′
j(aj)/vextendsingle/vextendsingle/vextendsingle/summationdisplay
a−jy−j(a−j) ,ui(·)∈[0,1]
≤n/summationdisplay
j=1/summationdisplay
aj∈Aj/vextendsingle/vextendsingle/vextendsingleσj(aj)−σ′
j(aj)/vextendsingle/vextendsingle/vextendsingle≤nmax
j∈[n]/summationdisplay
aj∈Aj/vextendsingle/vextendsingle/vextendsingleσj(aj)−σ′
j(aj)/vextendsingle/vextendsingle/vextendsingle
=n/ba∇dblσ−σ′/ba∇dbl,
Therefore,∀ai∈Ai,
ui(ai,σ−i)−ui(σ) =ui(ai,σ−i)−ui(ai,σ′
−i)+ui(ai,σ′
−i)−ui(σ′)+ui(σ′)−ui(σ)
≤n/ba∇dblσ−σ′/ba∇dbl+E(σ′,u)+n/ba∇dblσ−σ′/ba∇dbl
=E(σ′,u)+2n/ba∇dblσ−σ′/ba∇dbl.
Based on that, we get
E(σ,u) = max
i∈N,ai∈Ai[ui(ai,σ−i)−ui(σ)]≤E(σ′,u)+2n/ba∇dblσ−σ′/ba∇dbl
Similarly, we also have
E(σ′,u)≤E(σ,u)+2n/ba∇dblσ−σ′/ba∇dbl
Based on Lemma B.3 ,∀f,f′∈FNE, we have
E(f(u),u)−E(f′(u),u)≤2/ba∇dblf(u)−f′(u)/ba∇dbl≤2/ba∇dblf−f′/ba∇dbl∞
Considering that |E(·)|≤1, according to Lemma B.2 , we have:
Eu∼D[E(fNE(u),u)]−1
m/summationdisplay
u∈SE(fNE(u),u)≤2·inf
r>0/braceleftBig/radicalbigg
2lnN∞(FNE,r)
m+2nr/bracerightBig
+4/radicalbigg
2ln(4/δ)
m
B.1.2 CCE Approximator
Lemma B.4. For arbitrary joint mixed strategy πandπ′, we have
|E(π,u)−E(π′,u)|≤2/ba∇dblπ−π′/ba∇dbl,
Proof.∀π,π′,∀i∈[n] we have
|ui(π)−ui(π′)|=/summationdisplay
a∈A(π(a)−π′(a))ui(a)(a)
≤/summationdisplay
a∈A|π(a)−π′(a)|=/ba∇dblπ−π′/ba∇dbl (6)
18where (a) holds since ui(·)∈[0,1]. Therefore,∀ai∈Ai,
ui(ai,π−i)−ui(π) =ui(ai,π−i)−ui(ai,π′
−i)+ui(ai,π′
−i)−ui(π′)+ui(π′)−ui(π)
≤/ba∇dblπ−π′/ba∇dbl+E(π′,u)+/ba∇dblπ−π′/ba∇dbl
=E(π′,u)+2/ba∇dblπ−π′/ba∇dbl.
Based on that, we get
E(π,u) = max
i∈N,ai∈Ai[ui(ai,π−i)−ui(π)]≤E(π′,u)+2/ba∇dblπ−π′/ba∇dbl
Similarly, we also have
E(π′,u)≤E(π,u)+2/ba∇dblπ−π′/ba∇dbl
Based on Lemma B.4 ,∀f,f′∈FCCE, we have
E(f(u),u)−E(f′(u),u)≤2/ba∇dblf(u)−f′(u)/ba∇dbl≤2/ba∇dblf−f′/ba∇dbl∞
Considering that |E(·)|≤1, according to Lemma B.2 , we have:
Eu∼D[E(fCCE(u),u)]−1
m/summationdisplay
u∈SE(fCCE(u),u)≤2·inf
r>0/braceleftBig/radicalbigg
2lnN∞(FCCE,r)
m+2r/bracerightBig
+4/radicalbigg
2ln(4/δ)
m
B.1.3 CE Approximator
Lemma B.5. For arbitrary joint mixed strategy πandπ′, we have
|ECE(π,u)−ECE(π′,u)|≤2/ba∇dblπ−π′/ba∇dbl,
Proof.∀ai∈Ai,∀φi, we have
/summationdisplay
a∈Aπ(a)ui(φ(ai),a−i)−ui(π) =/summationdisplay
a∈Aπ(a)ui(φ(ai),a−i)−/summationdisplay
a∈Aπ′(a)ui(φ(ai),a−i)
+/summationdisplay
a∈Aπ′(a)ui(φ(ai),a−i)−ui(π′)+ui(π′)−ui(π)
≤/ba∇dblπ−π′/ba∇dbl+ECE(π′,u)+/ba∇dblπ−π′/ba∇dbl
=ECE(π′,u)+2/ba∇dblπ−π′/ba∇dbl.
Based on that, we get
ECE(π,u) = max
i∈Nmax
φi/summationdisplay
a∈Aπ(a)ui(φ(ai),a−i)−ui(π)≤ECE(π′,u)+2/ba∇dblπ−π′/ba∇dbl
Similarly, we also have
ECE(π′,u)≤ECE(π,u)+2/ba∇dblπ−π′/ba∇dbl
Based on Lemma B.4 ,∀f,f′∈FCE, we have
ECE(f(u),u)−ECE(f′(u),u)≤2/ba∇dblf(u)−f′(u)/ba∇dbl≤2/ba∇dblf−f′/ba∇dbl∞
Considering that |E(·)|≤1, according to Lemma B.2 , we have:
Eu∼D[ECE(fCE(u),u)]−1
m/summationdisplay
u∈SECE(fCE(u),u)≤2·inf
r>0/braceleftBig/radicalbigg
2lnN∞(FCE,r)
m+2r/bracerightBig
+4/radicalbigg
2ln(4/δ)
m
19B.2 Proof of Theorem 4.4
Theorem 4.4. [Sample complexity] For ǫ,δ∈(0,1), function classFof NE, CE or CCE approximator and
distributionD, with probability at least 1−δover draw of the training set Swith
m≥9
2ǫ2/parenleftbigg
ln2
δ+lnN∞(F,ǫ
3L)/parenrightbigg
games sampled from D,∀f∈Fwe have
Eu∼D[E(f(u),u)]≤1
m/summationdisplay
u∈SE(f(u),u)+ǫ,
whereL= 2nfor NE approximators, and L= 2for CE and CCE approximators.
Proof.Forfunctionclass FofNE,CEorCCEapproximators,accordingto Lemma B.3 ,Lemma B.4 andLemma B.5 ,
∀f,g∈Fwe have
E(CE)(f(u),u)−E(CE)(g(u),u)≤L/ba∇dblf(u)−g(u)/ba∇dbl≤L/ba∇dblf−g/ba∇dbl∞, (7)
whereL= 2nfor NE approximators, and L= 2 for CE and CCE approximators.
For simplicity, we denote LS(f) =1
m/summationtext
u∈SE(CE)(f(u),u) andLD(f) =Eu∼D[E(CE)(f(u),u)]. letFrwith
|Fr|=N∞(F,r) be the function class that r-coversFfor some r >0.∀ǫ∈(0,1), by setting r=ǫ
3Lwe have
PS∼Dm/bracketleftBig
∃f∈F,/vextendsingle/vextendsingleLS(f)−LD(f)/vextendsingle/vextendsingle> ǫ/bracketrightBig
≤PS∼Dm/bracketleftBig
∃f∈F,/vextendsingle/vextendsingleLS(f)−LS(fr)/vextendsingle/vextendsingle+/vextendsingle/vextendsingleLS(fr)−LD(fr)/vextendsingle/vextendsingle+/vextendsingle/vextendsingleLD(fr)−LD(f)/vextendsingle/vextendsingle> ǫ/bracketrightBig
(a)
≤PS∼Dm/bracketleftBig
∃f∈F,Lr+/vextendsingle/vextendsingleLS(fr)−LD(fr)/vextendsingle/vextendsingle+Lr > ǫ/bracketrightBig
≤PS∼Dm/bracketleftBig
∃fr∈Fr,/vextendsingle/vextendsingleLS(fr)−LD(fr)/vextendsingle/vextendsingle> ǫ−2Lr/bracketrightBig
(b)
≤N∞(F,r)PS∼Dm/bracketleftBig/vextendsingle/vextendsingleLS(f)−LD(f)/vextendsingle/vextendsingle> ǫ−2Lr/bracketrightBig
(c)
≤2N∞(F,r)exp(−2m(ǫ−2Lr)2),
=2N∞(F,ǫ
3L)exp(−2
9mǫ2)
where (a) holds by Equation (7) , (b) holds by union bound, and ( c) holds by Hoeﬀding inequality. As a result,
whenm≥9
2ǫ2/parenleftbig
ln2
δ+lnN∞(F,ǫ
3L)/parenrightbig
, we have PS∼Dm/bracketleftBig
∃f∈F,/vextendsingle/vextendsingle/vextendsingleLS(f)−LD(f)/vextendsingle/vextendsingle/vextendsingle> ǫ/bracketrightBig
< δ.
B.3 Proof of Theorem 4.5
Theorem 4.5. TheO-projected,P-projected andQ-projected approximator classes have smaller covering num bers,
i.e.,∀r >0we have
N∞(OFNE,r)≤N∞(FNE,r),
N∞(PFNE,r)≤N∞(FNE,r),
N∞(QF(C)CE,r)≤N∞(F(C)CE,r)
We ﬁrst provide an auxiliary lemma.
Lemma B.6. For function class Fand orbit averaging operator X, if∀f,g∈F,ℓ∞(Xf,Xg)≤ℓ∞(f,g), then
N∞(XF,r)≤N∞(F,r)for anyr >0.
Proof.∀r >0, DenoteFras the smallest r-covering set that covers Fwith sizeN∞(F,r).∀f∈F, letfr∈Frbe
the function that r-coversf. We have ℓ∞(Xfr,Xf)≤ℓ∞(fr,f)≤r. Therefore,XFris ar-covering set ofXF,
and we haveN∞(XF,r)≤|XF r|≤|Fr|=N∞.
20Proof of Theorem 4.5 .For player i∈[n] and∀fNE,gNE∈FNE, assumingUis closed under any ρi∈Gi. ForOi,
l∞(OifNE,OigNE) =max
u∈U/ba∇dblOifNE(u)−OigNE(u)/ba∇dbl
=max
j∈[n]max
u∈U/ba∇dbl(OifNE)(u)j−(OigNE)(u)j/ba∇dbl
=max/braceleftBig
max
u∈U/ba∇dblfNE(u)i−gNE(u)i/ba∇dbl,max
j/ne}ationslash=imax
u∈U/ba∇dbl1
|Ai|!/summationdisplay
ρi∈Gi(fNE(ρiu)j−gNE(ρiu)j)/ba∇dbl/bracerightBig
≤max/braceleftBig
max
u∈U/ba∇dblfNE(u)i−gNE(u)i/ba∇dbl,max
j/ne}ationslash=i1
|Ai|!/summationdisplay
ρi∈Gimax
u∈U/ba∇dblfNE(ρiu)j−gNE(ρiu)j/ba∇dbl/bracerightBig
=max/braceleftBig
max
u∈U/ba∇dblfNE(u)i−gNE(u)i/ba∇dbl,max
j/ne}ationslash=i1
|Ai|!/summationdisplay
ρi∈Gimax
u∈U/ba∇dblfNE(u)j−gNE(u)j/ba∇dbl/bracerightBig
=max/braceleftBig
max
u∈U/ba∇dblfNE(u)i−gNE(u)i/ba∇dbl,max
j/ne}ationslash=imax
u/ba∇dblfNE(u)j−gNE(u)j/ba∇dbl/bracerightBig
=l∞(fNE,gNE)
SinceO=O1◦···◦O n, we have
ℓ∞(OfNE,OgNE)≤ℓ∞(fNE,gNE). (8)
ForPi,
l∞(PifNE,PigNE) =max
u∈Umax
j∈[n]/ba∇dbl(PifNE)(u)j−(PigNE)(u)j/ba∇dbl
=max/braceleftBig
max
j/ne}ationslash=imax
u/ba∇dblfNE(u)j−gNE(u)j/ba∇dbl,max
u/ba∇dbl1
|Ai|!/summationdisplay
ρi∈Giρ−1
i(fNE(ρiu)i−gNE(ρiu)i)/ba∇dbl/bracerightBig
=max/braceleftBig
max
j/ne}ationslash=imax
u/ba∇dblfNE(u)j−gNE(u)j/ba∇dbl,max
u/ba∇dbl1
|Ai|!/summationdisplay
ρi∈Gi(fNE(ρiu)i−gNE(ρiu)i)/ba∇dbl/bracerightBig
≤max/braceleftBig
max
j/ne}ationslash=imax
u/ba∇dblfNE(u)j−gNE(u)j/ba∇dbl,1
|Ai|!/summationdisplay
ρi∈Gimax
u/ba∇dblfNE(ρiu)i−gNE(ρiu)i/ba∇dbl/bracerightBig
=max/braceleftBig
max
j/ne}ationslash=imax
u/ba∇dblfNE(u)j−gNE(u)j/ba∇dbl,1
|Ai|!/summationdisplay
ρi∈Gimax
u/ba∇dblfNE(u)i−gNE(u)i/ba∇dbl/bracerightBig
=max/braceleftBig
max
j/ne}ationslash=imax
u/ba∇dblfNE(u)j−gNE(u)j/ba∇dbl,max
u/ba∇dblfNE(u)i−gNE(u)i/ba∇dbl/bracerightBig
=l∞(fNE,gNE)
SinceP=P1◦···◦P n, we have
ℓ∞(PfNE,PgNE)≤ℓ∞(fNE,gNE). (9)
21For CE or CCE approximator f(C)CE∈F(C)CEandQi, we have
l∞(Qif(C)CE,Qig(C)CE) =max
u∈U/ba∇dbl(Qif(C)CE)(u)−(Qig(C)CE)(u)/ba∇dbl
=max
u/ba∇dbl1
|Ai|!/summationdisplay
ρi∈Giρ−1
i(f(C)CE(ρiu)−g(C)CE(ρiu))/ba∇dbl
≤max
u1
|Ai|!/summationdisplay
ρi∈Gi/ba∇dblρ−1
i(f(C)CE(ρiu)−g(C)CE(ρiu))/ba∇dbl
≤1
|Ai|!/summationdisplay
ρi∈Gimax
u/ba∇dblρ−1
i(f(C)CE(ρiu)−g(C)CE(ρiu))/ba∇dbl
=1
|Ai|!/summationdisplay
ρi∈Gimax
u/ba∇dblf(C)CE(ρiu)−g(C)CE(ρiu)/ba∇dbl
=1
|Ai|!/summationdisplay
ρi∈Gimax
u/ba∇dblf(C)CE(u)−g(C)CE(u)/ba∇dbl
=l∞(f(C)CE,g(C)CE)
SinceQ=Q1◦···◦Q n, we have
ℓ∞(Qf(C)CE,Qg(C)CE)≤ℓ∞(f(C)CE,g(C)CE). (10)
Combing Lemma B.6 ,Equation (8) ,Equation (9) andEquation (10) , we ﬁnish the proof.
B.4 Proof of Theorem 4.6
Theorem 4.6. When the payoﬀ distribution Dis invariant under the permutation of payoﬀs, the Q-projected
(C)CE approximator has a smaller expected equilibrium appr oximation. Formally, for all f(C)CE∈F(C)CEand
permutation-invariant distribution D, we have
Eu∼D[E(Qf(C)CE(u),u)]≤Eu∼D[E(f(C)CE(u),u)],
We ﬁrst prove a lemma about the property of Ei(π,u) andECE
i(π,u).
Lemma B.7.Ei(π,u)andECE
i(π,u)are convex on π, i.e.
pE(CE)
i(π1,u)+(1−p)E(CE)
i(π2,u)≥E(CE)
i(pπ1+(1−p)π2,u),∀p∈[0,1]
Proof.We recall the deﬁnition Ei(π,u) = max ai∈Aiui(ai,π−i)−ui(π) for CCE approximator and ECE
i(π,u) =
maxφi∈Ai→Ai/summationtext
aπ(a)ui(φi(ai),a−i)−ui(π)forCEapproximator. ui(ai,π−i)islinearon π. Givenφ,/summationtext
aπ(a)ui(φi(ai),a−i)
is also linear on π. Moreover, the maximum operator on a set of linear functions will ind uce a convex function.
Proof of Theorem 4.6 .Forf∈F(C)CEand∀i,j∈[n],
Eu∼D[E(CE)
i(Qjf(u),u)] =Eu∼D[E(CE)
i(1
|Aj|!/summationdisplay
ρj∈Gjρ−1
jf(ρju),u)],by deﬁnition
≤1
|Aj|!/summationdisplay
ρj∈GjEu∼D[E(CE)
i(ρ−1
jf(ρju),u)],by convexity
=1
|Aj|!/summationdisplay
ρj∈GjEv∼D[E(CE)
i(ρ−1
jf(v),ρ−1
jv)],letv=ρju
=1
|Aj|!/summationdisplay
ρj∈GjEv∼D[E(CE)
i(f(v),v)] ,invariance ofE(CE)
i(π,u) underρ−1
j∈Gj
=Eu∼D[E(CE)
i(f(u),u)]
22SinceQ=◦iQiandE= max iEi, we have
Eu∼D[E(Qf(u),u)]≤Eu∼D[E(f(u),u)]
B.5 Proof of Theorem 4.7
Theorem 4.7. For bimatrix constant-sum games, when the payoﬀ distributi onDis invariant under the permutation
of payoﬀs, then the X-projected (X∈{O,P}) NE approximator has a smaller expected exploitability. Fo rmally, for
allfNE∈FNEand permutation-invariant distribution Dfor bimatrix constant-sum games, we have
Eu∼D[/summationdisplay
iEi((XfNE)(u),u)]≤Eu∼D[/summationdisplay
iEi(fNE(u),u)]
Proof.We only prove for the P-projected case; the proof for O-projected case is similar and therefore omitted.
Recall
Ei(σ,u) = max
ai∈Aiui(ai,σ−i)−ui(σ)
Denoteu1(σ)+u2(σ)≡c, then
/summationdisplay
iEi(σ,u) = max
a1∈A1,a2∈A2u1(a1,σ2)+u2(a2,σ1)−c
Then we have
Eu∼D[/summationdisplay
iEi((PfNE)(u),u)] =Eu∼D[max
a1,a2u1(a1,(PfNE)(u)2)+u2(a2,(PfNE)(u)1)−c]
=Eu∼D[max
a1u1(a1,(PfNE)(u)2)]+Eu∼D[max
a2u2(a2,(PfNE)(u)1)]−c
For the ﬁrst term,
Eu∼D[max
a1u1(a1,(PfNE)(u)2)] =Eu∼D[max
a1u1(a1,1
|A2|!/summationdisplay
ρ2∈G2ρ−1
2fNE(ρ2u)2)]
≤1
|A2|!/summationdisplay
ρ2∈G2Eu∼D[max
a1u1(a1,ρ−1
2fNE(ρ2u)2)]
=1
|A2|!/summationdisplay
ρ2∈G2Ev∼D[max
a1(ρ−1
2v)1(a1,ρ−1
2fNE(v)2)]
=1
|A2|!/summationdisplay
ρ2∈G2Ev∼D[max
a1v1(a1,fNE(v)2)]
=Eu∼D[max
a1u1(a1,fNE(u)2)]
Similarly, for the second term,
Eu∼D[max
a2u2(a2,(PfNE)(u)1)]≤Eu∼D[max
a2u2(a2,fNE(u)1)]
Above all,
Eu∼D[/summationdisplay
iEi((PfNE)(u),u)] =Eu∼D[max
a1u1(a1,(PfNE)(u)2)]+Eu∼D[max
a2u2(a2,(PfNE)(u)1)]−c
≤Eu∼D[max
a1u1(a1,fNE(u)2)]+Eu∼D[max
a2u2(a2,fNE(u)1)]−c
=Eu∼D[/summationdisplay
iEi(fNE(u),u)]
23B.6 Proof of Theorem 4.8
Theorem 4.8. When the payoﬀ distribution Dis invariant under the permutation of payoﬀs, and fNEsatisﬁes
OPI, then theP-projected NE approximator has a smaller expected NE approx imation. Formally, for all fNE∈FNE
that is OPI and permutation-invariant distribution D, we have
Eu∼D[E((PfNE)(u),u)]≤Eu∼D[E(fNE(u),u)].
We ﬁrst introduce a useful lemma. It is about the property of Ei(σ,u)
Lemma B.8.Ei(σ,u)is
1. Linear on σi, i.e.
pEi((σ1
i,σ−i),u)+(1−p)Ei((σ2
i,σ−i),u) =Ei((pσ1
i+(1−p)σ2
i,σ−i),u),∀p∈[0,1]
2. Convex on σj, i.e.
pEi((σ1
j,σ−j),u)+(1−p)Ei((σ2
j,σ−j),u)≥Ei((pσ1
j+(1−p)σ2
j,σ−j),u),∀p∈[0,1],j/\e}atio\slash=i
Proof.We recall the deﬁnition Ei(σ,u) = max ai∈Aiui(ai,σ−i)−ui(σ). Notice that ui(σ) is linear on σkfor all
k∈[n], thus both ui(ai,σ−i) andui(σ) are linear on σkfor anyk∈[n]. Moreover, the maximum operator on a set
of linear functions will induce a convex function.
Proof of Theorem 4.8 .We prove the theorem in two steps.
Step 1 First, we show that
Eu∼D[Ei((PifNE)(u),u)] =Eu∼D[Ei(fNE(u),u)],∀fNE∈FNE
By deﬁnition,
Eu∼D[Ei(PifNE(u),u)]
=Eu∼D[Ei((1
|Ai|!/summationdisplay
ρi∈Giρ−1
if(ρiu)i,f(u)−i),u)]
=1
|Ai|!/summationdisplay
ρi∈GiEu∼D[Ei((ρ−1
if(ρiu)i,f(u)−i),u)] ,by linearity ofEi(σ,u) onσi
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ei((ρ−1
if(v)i,f(ρ−1
iv)−i),ρ−1
iv)] ,letv=ρiuand use the invariance of D
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ei((ρ−1
if(v)i,f(v)−i),ρ−1
iv)] ,OPI off
=1
|Ai|!/summationdisplay
ρi∈GiEu∼D[Ei((f(u)i,f(u)−i),u)] ,invariance ofEi(σ,u) underρ−1
i∈Gi
=Eu∼D[Ei(fNE(u),u)]
Step 2 Then we show that
Eu∼D[Ej((PifNE)(u),u)]≤Eu∼D[Ej(fNE(u),u)],∀fNE∈FNE,j/\e}atio\slash=i
24Eu∼D[Ej((PifNE)(u),u)]
=Eu∼D[Ej((1
|Ai|!/summationdisplay
ρi∈Giρ−1
if(ρiu)i,f(u)−i),u)]
≤1
|Ai|!/summationdisplay
ρi∈GiEu∼D[Ej((ρ−1
if(ρiu)i,f(u)−i),u)] ,by convexity ofEj(σ,u) onσi
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ej((ρ−1
if(v)i,f(ρ−1
iv)−i),ρ−1
iv)] ,letv=ρiuand use the invariance of D
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ej((ρ−1
if(v)i,f(v)−i),ρ−1
iv)] ,OPI off
=1
|Ai|!/summationdisplay
ρi∈GiEu∼D[Ej((f(u)i,f(u)−i),u)] ,invariance ofEj(σ,u) underρ−1
i∈Gi
=Eu∼D[Ej(fNE(u),u)]
SinceP=◦iPiandE= max iEi, we have
Eu∼D[E((PfNE)(u),u)]≤Eu∼D[E(fNE(u),u)]
B.7 Proof of Theorem 4.9
Theorem 4.9. For bimatrix games, when the payoﬀ distribution Dis invariant under the permutation of payoﬀs,
andfNEsatisﬁes PPE, then the O-projected NE approximator has a smaller expected NE approx imation. Formally,
for allfNE∈FNEthat is PPE and permutation-invariant distribution Dof bimatrix games, we have
Eu∼D[E((OfNE)(u),u)]≤Eu∼D[E(fNE(u),u)].
Proof.We prove the theorem in two steps, similar to the proof of Theorem 4.8 .
Step 1 First we show that for player i∈{1,2}, let{j}={1,2}\{i},
Eu∼D[Ei((OifNE)(u),u)]≤Eu∼D[Ei(fNE(u),u)]
This is because
Eu∼D[Ei((OifNE)(u),u)] =Eu∼D[Ei((fNE(u)i,1
|Ai|!/summationdisplay
ρi∈GifNE(ρiu)j),u)]
≤1
|Ai|!/summationdisplay
ρi∈GiEu∼D[Ei((fNE(u)i,fNE(ρiu)j),u)] ,by convexity ofEionσj
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ei((fNE(ρ−1
iv)i,fNE(v)j),ρ−1
iv)],letv=ρiu
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ei((ρ−1
ifNE(v)i,fNE(v)j),ρ−1
iv)],by PPE of fNE
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ei((fNE(v)i,fNE(v)j),v)] ,invariance ofEi(σ,u) underρ−1
i∈G
=Eu∼D[Ei((fNE)(u),u)]
25Step 2 Then we show that if j/\e}atio\slash=iand{i,j}={1,2}
Eu∼D[Ej((OifNE)(u),u)] =Eu∼D[Ej(fNE(u),u)]
This is because
Eu∼D[Ej((OifNE)(u),u)] =Eu∼D[Ej((fNE(u)i,1
|Ai|!/summationdisplay
ρi∈GifNE(ρiu)j),u)]
=1
|Ai|!/summationdisplay
ρi∈GiEu∼D[Ej((fNE(u)i,fNE(ρiu)j),u)] ,by linearity ofEjonσj
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ej((fNE(ρ−1
iv)i,fNE(v)j),ρ−1
iv)],letv=ρiu
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ej((ρ−1
ifNE(v)i,fNE(v)j),ρ−1
iv)],by PPE of fNE
=1
|Ai|!/summationdisplay
ρi∈GiEv∼D[Ej((fNE(v)i,fNE(v)j),v)] ,invariance ofEj(σ,u) underρ−1
i∈Gi
=Eu∼D[Ej(fNE(u),u)]
SinceO=◦iOiandE= max iEi, we have
Eu∼D[E(OfNE(u),u)]≤Eu∼D[E(fNE(u),u)]
C Omitted Proofs in Section 5
C.1 Proof of Theorem 5.3
Theorem 5.3. For a permutation- ρ-invariant game Γu. if there is a pure NE a∗= (a∗
i)i∈[n]and at least one player
k∈[n]such that a∗
k∈V(ρk), thena∗will never be found by any NE approximator with both PPE and OP I. Besides,
a∗(as a pure CE or CCE) will also never be found by any CE or CCE app roximator with PE.
Proof.Letfbe a PPE and OPI NE approximator. Denote f(u) = (σi)i∈[n]. For player kthata∗
k∈V(ρk), we get
σk=f(u)k(a)=f(ρu)k(b)=f(ρku)k(c)=ρkf(u)k=ρkσk, (11)
where (a) holds since uis permutable w.r.t. ρ, (b) holds by OPI of f, and (c) holds by PPE of f. Ifa∗can be found
byf, we will have 1 = σk(a∗
k)(d)=ρkσk(a∗
k) =σk(ρ−1
k(a∗
k)), where ( d) holds by Equation (11) . However, such result
leads to a contradiction, because a∗
k/\e}atio\slash=ρ−1
k(ak) butσk(a∗
k) =σ(ρ−1
k(a∗
k)) = 1.
Letfbe a PE (C)CE approximator. Denote f(u) =π, we have
π=f(u)(a)=f(ρu)(b)=ρf(u) =ρπ (12)
where (a) holds since uis permutable w.r.t. ρ, (b) holds by PE of f. Ifa∗can be found by f, we will have
1 =π(a∗)(c)=ρπ(a∗) =π(ρ−1a∗) =π(ρ−1
1a∗
1,···,ρ−1
na∗
n), where ( c) holds by Equation (12) . However, from
a∗
k∈V(ρk) we know ρ−1
k(a∗
k)/\e}atio\slash=a∗
k, thenρ−1a∗/\e}atio\slash=a∗, butπ(a∗) =π(ρ−1a∗) = 1.
C.2 Proof of Theorem 5.6
Theorem 5.6. GivenN,M≥2, letF(C)CE
PEbe the function class (target on games with number of players n≤N
and|Ai|≤M) of all the (C)CE approximators with PE. Denote by F(C)CE
generalthe function class of all the (C)CE
approximators. Then we have
SWRN,M(F(C)CE
PE,F(C)CE
general) = 1.
26Proof.Assume f∈F(C)CE
generalis an (C)CE approximator that always ﬁnds the strategy that maxim izes the social
welfare. Afterward, we construct another f0that satisﬁes PE and always ﬁnds the strategy that maximizes socia l
welfare. f0is constructed by orbit averaging:
f0(u) =Qf(u),
thusf0is PE.
DenoteDas an arbitrary payoﬀ distribution of usuch thatDis invariant under permutation and the cardinality
of its support is ﬁnite. We have
Eu∼DSW(Qif(u),u) =Eu∼DSW(1
|Ai|!/summationdisplay
ρi∈Giρ−1
if(ρiu),u)
=Eu∼Dn/summationdisplay
i=1ui(1
|Ai|!/summationdisplay
ρi∈Giρ−1
if(ρiu))
=1
|Ai|!/summationdisplay
ρi∈GiEu∼Dn/summationdisplay
i=1ui(ρ−1
if(ρiu))
=1
|Ai|!/summationdisplay
ρi∈GiEv∼Dn/summationdisplay
i=1(ρ−1
iv)i(ρ−1
if(v)) ,letv=ρiu
=1
|Ai|!/summationdisplay
ρi∈GiEv∼Dn/summationdisplay
i=1vi(f(v))
=Eu∼Dn/summationdisplay
i=1ui(f(u))
=Eu∼DSW(f(u),u)
Due to thatQ=Q1◦···◦Q n, we have
Eu∼DSW(f0(u),u) =Eu∼DSW(f(u),u)
Due to the arbitrariness of D, we know that f0maximizes the social welfare w.r.t. any u.
From above, we immediately know
SWRN,M(F(C)CE
PE,F(C)CE
general) = 1
C.3 Proof of Theorem 5.7
Theorem 5.7. GivenN,M≥2, letFNE
OPI,FNE
PPEandFNE
bothbe the function classes (target on games with number
of players n≤Nand|Ai|≤M) of all the NE approximators with OPI, PPE and both. Denote th e function class
of all the NE approximators as FNE
general. Then we have
SWRN,M(FNE
OPI,FNE
general) =1
MN−1, (1)
SWRN,M(FNE
PPE,FNE
general)≤1
M, (2)
SWRN,M(FNE
both,FNE
general) =1
MN−1. (3)
Additionally, when M≥3, denote by /tildewideFNE
boththe function class of all the NE oracles (functions that alwa ys output
exact NE solutions of the input games) with both PPE and OPI, a nd by/tildewideFNE
generalthe function class of all the NE
oracles. Then we have
SWRN,M(/tildewideFNE
both,/tildewideFNE
general) = 0. (4)
27C.3.1 Proof of Equation (1) andEquation (3)
We ﬁrst prove the theorem with respect to FNE
OPIandFNE
both
Step 1 On the one part, we prove
SWRN,M(FNE
OPI,FNE
general)
SWRN,M(FNE
both,FNE
general)/bracerightBigg
≤1
MN−1
We prove this by construction.
Consider a game with Nplayer andAi= [M] fori∈[N].∀a∈A,i∈[N], deﬁne the payoﬀ ¯ uas follows:
¯ui(a) =/braceleftBigg
1,ifa1=a2=···=aN
0,otherwise
DeﬁneU={u′|u′=◦iρi¯u,ρi∈Gi}andDas a uniform distribution on U. Easy to certify that Dis a permutation-
invariant distribution.
Let˜f∈˜FNE
generalbe the NE oracle that ˜f(¯u)i= 1 and for any u′=◦iρi¯u∈U,˜f(u′)i=ρi(1). Intuitively, the
oracle will choose the action that will provide all players with revenue 1, leading to a social welfare of N. Since
each player has got her maximum possible utility, we have
max
f∈FNE
generalEu∼DSW(f(u),u) = max
˜f∈/tildewideFNE
generalEu∼DSW(˜f(u),u) =N. (13)
For any j1,j2∈[M] andj1< j2, letρ(j1,j2)
i= (1,...,j 2,...,j 1,...,M) for all player i∈[N] be the swap
permutation that swaps actions j1andj2and keeps other actions still. Then ◦i/ne}ationslash=jρ(j1,j2)
i¯u=ρ(j1,j2)
j¯ufor player j.
Forf∈FNE
OPI, we have f(¯u)j=f(◦i/ne}ationslash=jρ(j1,j2)
i¯u)j=f(ρ(j1,j2)
j¯u)jfor arbitrary swap permutation ρ(j1,j2)
j. Since any
permutation can be achieved by composition of swap permutations, we have∀ρj∈Gj,f(¯u)j=f(ρj¯u)j. Based on
that, and by OPI of f,∀ρ=◦i∈[N]ρiwe have f(¯u)j=f(ρ¯u)j, i.e.fis a constant function on U. Without loss of
generality, we denote f(u)≡σfor allu∈U. Then
Eu∼DSW(f(u),u) =1
|U|/summationdisplay
u′∈USW(σ,u′) =1
(M!)N−1SW(σ,/summationdisplay
u′∈Uu′).
Additionally, we have (/summationtext
u′∈Uu′)(a) = ((M−1)!)N−1for anya∈A. Based on that, we have
max
f∈FNE
OPIEu∼DSW(f(u),u) =1
(M!)N−1·N((M−1)!)N−1=N
MN−1. (14)
Combining Equation (13) andEquation (14) , we have
SWRN,M(FNE
OPI,FNE
general)≤1
MN−1.
Due toFNE
both⊆FNE
OPI, we immediately know
SWRN,M(FNE
both,FNE
general)≤1
MN−1
Step 2 On the other part, we prove
SWRN,M(FNE
OPI,FNE
general)
SWRN,M(FNE
both,FNE
general)/bracerightBigg
≥1/MN−1
Deﬁne the maximum possible utility (MPU) for player iwith respect to utility uiand action aias
MPU(ui,ai):= max
a−i∈A−iui(ai,a−i) (15)
28Deﬁne the set of maximum possible utility best response for player iw.r.t.uias
Bi(ui):={ai∈Ai: MPU(ui,ai) = max
a′
i∈AiMPU(ui,a′
i)}
We ﬁrst conduct some simpliﬁcation to the target.
SWRN,M(FNE
both,FNE
general) = inf
Dmaxf∈FNE
bothEu∼DSW(f(u),u)
maxf∈FNE
generalEu∼DSW(f(u),u)≥inf
Dmaxf∈FNE
bothEu∼DSW(f(u),u)
Eu∼DmaxσSW(σ,u)
Then we constrain uto be a cooperation game. For a normal form game Γ u, we deﬁne ˜ u= (˜ui)i∈[n]in which
˜ui=1
n/summationtextn
i=1ui. Then we have SW( σ,u) = SW( σ,˜u), which means that constraining uto be a cooperation game
will induce the same social welfare. Then
inf
Dmaxf∈FNE
bothEu∼DSW(f(u),u)
Eu∼DmaxσSW(σ,u)= inf
Dmaxf∈FNE
bothEu∼DSW(f(u),˜u)
Eu∼DmaxσSW(σ,˜u)
Denotef0be the approximator that always outputs uniform strategy on Bi(˜ui) for player i. It’s obvious that
f0is both OPI and PPE because the operations from utof0(u) are all permutation-equivariant. Then,
inf
Dmaxf∈FNE
bothEu∼DSW(f(u),˜u)
Eu∼DmaxσSW(σ,˜u)≥inf
DEu∼DSW(f0(u),˜u)
Eu∼DmaxσSW(σ,˜u)
Ignoretheinﬁmumandtheexpectationoperator,considerSW(f0(u),˜u)
maxσSW(σ,˜u)forarbitrary ˜ u, denotebbethemaximum
element appeared in ˜ u, then the denominator equals Nb. But for the numerator, for player i, no matter what action
ai∈Bi(˜ui) she chooses, she always has probability at least/producttext
j/ne}ationslash=i1
|Bj|≥1
MN−1to achieve revenue b, therefore
inducing SW( f0(u),˜u)≥Nb
MN−1.
Then,SW(f0(u),˜u)
maxσSW(σ,˜u)≥1
MN−1, so as inf DEu∼DSW(f0(u),˜u)
Eu∼DmaxσSW(σ,˜u), SWR N,M(FNE
both) and SWR N,M(FNE
OPI).
Above all,
SWRN,M(FNE
OPI,FNE
general)
SWRN,M(FNE
both,FNE
general)/bracerightBigg
=1
MN−1
C.3.2 Proof of Equation (2)
We next prove the theorem with respect to FNE
PPEthat
SWRN,M(FNE
PPE,FNE
general)≤1
M
Consider a bimatrix game and Ai= [M] fori∈[2].∀a∈A,i∈[2], deﬁne the payoﬀ ¯ uas follows:
¯ui(a) =/braceleftBigg
1,ifa1=a2
0,otherwise
DeﬁneU:={u′|u′=ρ1ρ2¯u,ρi∈Gi}andDas a uniform distribution on U. Easy to certify that U={u′|u′=
ρ1¯u,ρ1∈G1}={u′|u′=ρ2¯u,ρ2∈G2}andDis a permutation-invariant distribution.
Let˜f∈˜FNE
generalbe the NE oracle that ˜f(¯u)i= 1 and for any u′=◦iρi¯u∈U,˜f(u′)i=ρi(1). Intuitively, the
oracle will choose the action that will provide all players with revenue of 1, leading to a social welfare of 2.
For a permutation ̺on [M], letP̺∈{0,1}M×Mbe the corresponding permutation matrix. Denote Pas
the set of all permutation matrice. As a result, ∀u∈U,∀ρ1∈ G1,ρ1u= (Pρ1u1,Pρ1u2) =:Pρ1uand∀ρ2∈
G2,ρ2u= (u1PT
ρ2,u2PT
ρ2) =:uPT
ρ2. Specially, we have P̺¯uPT
̺= ¯u. Forf∈FNE
PPE, Denote f(¯u) =σ= (σ1,σ2). For
permutation ̺in [M] and payoﬀ u′=P̺¯u= ¯u(PT
̺)−1, by PPE of f, we have f(u′)1=f(P̺¯u)1=P̺σ1=̺σ1,and
f(u′)2=f(¯u(PT
̺)−1)2= (P̺)−1σ2=̺−1σ2.Then we have
SW(f(u′),u′) =/summationdisplay
i(P̺¯u)i(̺σ1,̺−1σ2) =/summationdisplay
i¯ui(σ1,̺−1σ2) =/summationdisplay
i(¯uPT
̺)i(σ1,σ2) = SW(f(¯u),¯uPT
̺)
29Therefore
Eu∼DSW(f(u),u) =1
|U|/summationdisplay
u′∈USW(f(u′),u′)
=1
|U|/summationdisplay
P̺∈PSW(f(¯u),¯uPT
̺)
=1
|U|/summationdisplay
u=¯u(PT̺)∈USW(f(¯u),u)
=1
|U|SW(σ,/summationdisplay
u′∈Uu′).
Since|U|=1
M!and/summationtext
u′∈Uu′is a tensor with all elements equal to ( M−1)!. Thus Eu∼DSW(f(u),u) =2
Mand
SWRN,M(FNE
PPE,FNE
general)≤1
M
C.3.3 Proof of Equation (4)
Consider a 3×3 game as follows, where ǫ∈(0,1
2):
u=
1,10,0 0,1
2+ε
0,01,10,1
2+ε
1
2+ε,01
2+ε,0ε,ε

It is obvious that max σ∗⊆NE(Γ u)SW(σ∗,u) = 2, and the corresponding strategy has been bolded. However, for
NE oracles with both PPE and OPI, it can only output a unique NE with a p ure strategy that induces utility ( ε,ε).
Letρ1=ρ2= (2,1,3), we have ρ1ρ2u=u. From the analysis above we know if fNE∈/tildewideFNE
bothandfNE(u) =
(σ1,σ2), thenσ1(1) =σ1(2),σ2(1) =σ2(2). We integrate the ﬁrst two actions of player 1 and player 2 into a new
action that will choose randomly between the ﬁrst two actions, the n we form the utility matrix below:
u=/bracketleftbigg1
2,1
20,1
2+ε
1
2+ε,0ε,ε/bracketrightbigg
There is a unique NE in this Prisoner’s Dilemma, which has been bolded. Th e gameuis the same with the game
uunder the assumption that σ1(1) =σ1(2) andσ2(1) =σ2(2) inu. Then maxf∈/tildewideFNE
bothSW(f(u),u) = 2ε. Sinceε
can be arbitrarily small, we have SWR 2,3(/tildewideFNE
both,/tildewideFNE
general) = 0. As a result, we have SWR N,M(/tildewideFNE
both,/tildewideFNE
general) = 0
for allN≥2 andM≥3.
30