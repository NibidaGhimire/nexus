A Web of Things Architecture for Digital Twin
Creation and Model-Based Reinforcement Control
Luca Bedogni
Department of Physics, Mathematics and Informatics
University of Modena and Reggio Emilia
Via G. Campi 213/B, 41125 Modena, Italy
luca.bedogni@unimore.itFederico Chiariotti
Department of Information Engineering
University of Padova
Via G. Gradenigo 6/B, 35131 Padova, Italy
chiariot@dei.unipd.it
Abstract ‚ÄîInternet of Things (IoT) devices are available in a
multitude of scenarios, and provide constant, contextual data
which can be leveraged to automatically reconÔ¨Ågure and optimize
smart environments. To realize this vision, ArtiÔ¨Åcial Intelligence
(AI) and deep learning techniques are usually employed, however
they need large quantity of data which is often not feasible in
IoT scenarios. Digital Twins (DTs) have recently emerged as an
effective way to replicate physical entities in the digital domain, to
allow for simulation and testing of models and services. In this
paper, we present a novel architecture based on the emerging
Web of Things (WoT) standard, which provides a DT of a smart
environment and applies Deep Reinforcement Learning (DRL)
techniques on real time data. We implement our system in a real
deployment, and test it along with a legacy system. Our Ô¨Åndings
show that the beneÔ¨Åts of having a digital twin, speciÔ¨Åcally for
DRL models, allow for faster convergence and Ô¨Åner tuning.
Index Terms ‚ÄîWeb of Things, Digital Twin, Reinforcement
Learning, Model-based Learning
I. I NTRODUCTION
The Internet of Things (IoT) has transformed many aspects
of daily lives, as modern, intelligent devices are now available
in a plethora of different use cases. This vast amount of
information, together with the capability of IoT devices to
interact with the real world, opens up several possibilities for
the automatic reconÔ¨Åguration and control of systems based
on ArtiÔ¨Åcial Intelligence (AI) and deep learning techniques,
without human intervention in the loop [1].
The Deep Reinforcement Learning (DRL) revolution, which
started in 2015 with the Ô¨Årst functioning Deep Q-Network
(DQN) model [2], has led to a Ô¨Çourishing of proposed
solutions in various Ô¨Åelds, including the IoT. In the years
since, DRL models have proven their value in a number of
applications, from complex board games to cellular network
optimization. However, some inherent challenges still limit
their use in practical scenarios [3]: while these models can
often overcome traditional approaches after convergence, the
training phase requires some exploration of the state and action
spaces, leading to suboptimal outcomes during training.
This training phase might be extremely long in some appli-
cations, as training even a modestly sized DQN often requires
This work was partly supported by the Italian Ministry of University
and Research under the PNRR ‚ÄúSoE Young Researchers‚Äù grant for project
REDIAL.millions of steps. Moreover, several models may be a viable
solution for the same task, hence understanding the one which
Ô¨Åts the best is not trivial. This can effectively prevent the use
of DRL in many domotic and industrial IoT applications [4],
whose control loops are relatively slow, as the training would
require weeks or even months of highly suboptimal control. On
the other hand, purely passive training (i.e., training the model
on experience samples that were obtained while using a policy
not controlled by the learning agent) has known convergence
issues [5]: if a traditional control policy is used during training,
avoiding highly damaging outcomes, the DQN might never
learn a good policy.
A solution to this issue is given by model-based DRL [6]:
in this paradigm, domain knowledge about the task of the
learning agent is used to create a model of the environment,
which can be used as a simulation environment for ofÔ¨Çine
virtual training. The agent is then free to make mistakes and
learn from experience, without actually causing any damage
in the real world, and the model can be sped up to make
the duration of the training process independent from the
timing of the task, as the only limit to the virtual environment
is the available processing speed. Naturally, the model itself
is not reality, and the agent might need an online training
phase in the real world after reaching convergence in the
simulated environment, but the difÔ¨Åculty of transfer learning
from the virtual environment to the real one is much lower
than learning the policy from scratch, and more importantly,
extremely damaging actions are already associated to very low
values, and a directed exploration policy will never select
them, achieving a much smaller optimality gap during the
online training phase.
Enabling simulation of counterfactual scenarios and differ-
ent control policies is one of the key aims of another paradigm
that is rapidly attracting interest in the engineering community,
theDigital Twin (DT) [7]. DTs include sensory, simulation,
and management aspects of objects and processes, effectively
bridging the physical and digital worlds [8]: a twin can be used
to monitor the state of its physical counterpart, by connecting
it directly to sensors, or to simulate the effects of choices
and actions, using it as a dynamic model disconnected from
physical reality. In fact, a DT can provide the virtual environ-
ment for model-based DRL, acting as a sandbox to train thearXiv:2301.12761v1  [cs.NI]  30 Jan 2023learning agent in without affecting the real environment. At
the same time, the twin can be reÔ¨Åned with updated data from
the real sensors, more accurately matching the physical system
it represents; the DRL agent can then be easily retrained on
the improved sensor data, incorporating the new model with
the experience on the actual physical system.
In this work, we describe a general-purpose Web of Things
(WoT) architecture that can seamlessly support the creation
and twinning of DTs and the model-based training of DRL
agents in a virtual environment composed by one or more
DTs. The architecture can interface directly with sensors and
actuators through WoT standard protocols, as well as providing
a middleware layer that can integrate legacy systems and
proprietary IoT frameworks, such as most domotic systems,
into the architecture. We also provide a Smart Home use
case, in which DRL is used to control the heating system
of a room, considering the number of people occupying it
as well as associated energy efÔ¨Åciency. The DT and DRL
agent are connected to a real deployment, using data from the
installed sensors and providing notiÔ¨Åcations to the user with
advice on thermostat settings. As more data becomes available
over time, the DT for each room is automatically updated,
exploiting more complex models when their training becomes
possible, and the deployed DRL agent is retrained using the
new models, providing a self-improving temperature control
with a seamless occupant experience.
The rest of this paper is organized as follows: Ô¨Årst, we
provide a review of the relevant literature in Sec. II. We then
describe our architecture and the model-based DRL training
procedure in Sec. III, and we present our domotic use case
in Sec. IV. Finally, Sec. V concludes the paper and presents
some possible avenues of future work.
II. R ELATED WORK
The IoT has revolutionized many aspects of daily lives,
becoming pervasive and present in several different scenar-
ios. To fully leverage the IoT‚Äôs potential, Deep Learning or
Machine Learning techniques are often employed to extract
patterns from the vast amount of data that sensors and other
IoT devices produce. The IoT presents many challenges, one
of which is certainly the possibility for them to cooperate
even if made by different vendors and adopting different
communication protocols [1]. This is one of the reasons behind
the increasing interest over the WoT standard, which provides
a homogeneous representation of devices and of operations
which can be performed on them [9], [10], while also offering
Quality of Service (QoS) levels depending on the application
requirements, and security mechanisms to protect the data and
the devices.
However, even being able to seamlessly interconnect devices
does not fully unleash the possibilities offered by the IoT.
What is still missing is the possibility for devices to automat-
ically understand what services can spark from the devices
and the data which is available in their networks. In other
words, what is missing is the interpretation of the data, which
goes beyond pure classiÔ¨Åcation [11], to go towards automaticservice composition built by the device from the data itself
[12]. There have been many contributions in this domain, most
of which focused on a speciÔ¨Åc aspect of the issue, such as how
to discover devices using the same protocol and exchanging
similar information [13], and how to leverage ontologies to
provide semantics to the data, hence to be able to provide a
shared sense to it [14] [15].
There are fewer works which address the autonomous
creation of services from the devices, and more generally
how different devices can form novel services without human
intervention [16]. Apart from understanding the nature of the
data, devices also need to be able to perform different actions
on the network to change its behavior and recognize what
actions may bring the system toward a better state [17]. In
other words, the network has to automatically understand what
is the causality of actions on speciÔ¨Åc variables of interest,
which requires to perform several experiments and possibly
have a long history of data, which is not always feasible
in an IoT scenario. There have been some advances in this
domain [18], but there are still no organic solutions that
can also deal with all the challenges that deployment in real
scenarios brings [16], [19].
During the past decade, another revolution in computing and
AI took place, in parallel to the development of IoT systems:
reinforcement learning, whose practical applications had been
mostly limited to board games and parametric optimization
of hand-designed algorithms, was successfully integrated with
deep neural networks [2], taking a signiÔ¨Åcant leap in terms
of performance and capabilities. Over the past few years,
DRL has become a staple solution in many Ô¨Åelds, including
robotics, automated factory management, and communication
networks. However, DRL solutions for real systems have two
signiÔ¨Åcant problems, which stem from the model-free nature
of the learning procedure: Ô¨Årstly, they require a long training
process, which must include samples of highly suboptimal
actions in order to fully explore the solution space, and
secondly, counterfactuals and alternative options are hard to
evaluate [20].
Domotics, and the control of slow processes such as build-
ing heating and climate control, are an application Ô¨Åeld that
is particularly vulnerable to long training times [4]: while
training a DRL agent in faster environments, with sub-second
time steps, can take a few hours, training time for agents on
processes whose actions are separated by minutes or hours
can be measured in months. Furthermore, the robustness of
the learned solution needs to be veriÔ¨Åed thoroughly, as the
consequences of failures may range from annoying to dire.
In these cases, virtual pre-training before deployment is
a common technique [21]. Complex simulators or simple
black-box models can provide a safe environment fro the
DRL agent to make mistakes [22], transferring the acquired
knowledge to the real world. However, most works using
virtual environments for training designed them ad hoc [23],
requiring extensive human intervention, and the quality of
the simulation environment is crucial for effective transfer
learning [24].Thing Description
DirectoryEngineWoT sensors
and actuatorsMiddleware Legacy systemsDT 0 DT 1 DT 2 DRL 0 DRL 1 DRL 2
Fig. 1: Web of Things architecture.
Causal reasoning and the creation of models of the envi-
ronment, either in advance or during training and operation,
are ways to mitigate these issues: modeling the effects of
actions, and whether the available actions affect control per-
formance [25], can signiÔ¨Åcantly improve DRL training. The
concept of a DT, which uses real data from the environment
to build a model, can be extremely useful: by gradually im-
proving the environment model, and retraining the DRL agent
if needed, the DT paradigm can aid the virtual environment
design process [26], providing the basic building blocks for
automated data-driven training [27].
This work combines ideas from the DT and WoT literature
in a single architecture, which can easily integrate multiple
DTs from different types of sensors and automatically retrain
a DRL agent for complex control task in a virtual environment
generated by a selection of DTs.
III. A RCHITECTURE
In this section, we describe the building blocks of our
framework. We leverage the WoT model, which enables a
standardized interface and communication process for smart
devices belonging to a network. We focus our efforts on
a Smart Home scenario, but our architecture can be easily
extended and adapted to other environments as well.
In Sec. III-A, we describe our framework, which is based
on a WoT network through which devices are able to commu-
nicate. The training procedure for model-based DRL agents
is then described in Sec. III-B, including the twinning of
the digital twin model to the selected sensory set and the
training of the agent in the virtual environment provided by the
twin. We then describe the inclusion of legacy components in
Sec. III-C, to extend our vision and encompass other devices
which may be available through other networks or systems.
A. WoT Network
Our architecture extends the simpler version presented in
[12] and mainly builds on the WoT paradigm, which enables
devices to provide their representation in terms of a Thing
Description (TD), which advertises their own capabilities to
other devices willing to interact with them. Fig. 1 highlights
the different components which build our architecture and their
interactions.
On the lower left cprner of the Ô¨Ågure, we see sensors and
actuators, which are built following the WoT standard: theyprovide a minimalist HTTP server which serves the requests,
and also provide their TD whenever their root URL is visited.
Upon joining the network, devices are also requested to
join a Thing Description Directory (TDD), which stores all the
TDs from devices in the network. Using a TDD has numerous
advantages, such as the possibility to perform queries on it
to retrieve speciÔ¨Åc groups of devices, or to gather all devices
providing the same kind of data and so on. Moreover, it allows
to keep track of all the devices which are available in our
system. The TDD also provides an interface to notify other
software components of devices joining the network. To this
interface we subscribe two other components, which are the
Middleware and the Engine.
The Middleware is in charge of recognizing devices joining
the network and instantiate an appropriate model for interpret-
ing the values by performing two main operations:
It monitors the TDD for devices which have joined the
network, analyzes their TD and instantiates an appropriate
model which can handle such type of data. Moreover, it
can also instantiate an appropriate DT which can handle
that device.
It connects to legacy systems which may be already
present in the smart environment, and provides a WoT
interface to them. These newly created WoT devices will
then be registered to the TDD, so that other devices in the
system can leverage their data and can possibly interact
with them.
Overall, the Middleware is in charge of maintaining updated
the modules and DT components according to the devices
active in the system. Therefore, it also periodically checks
whether registered devices are still alive and removes the
models or DTs whenever such devices left the network.
Finally, the Engine is in charge of optimizing the system by
considering at all the Things registered in the TDD and their
capabilities, as well as monitoring the agent rewards.
B. Model-Based Training
The proposed architecture includes a two-step procedure for
training model-based DRL agents, as shown in Fig. 2: the basic
concept is to exploit the Digital Twin paradigm to provide a
virtual environment that is safe for an agent to explore, without
any impacts on the actual physical system.
1)Digital Twin training and data gathering: Firstly, the
system needs to gather sensory observation and create
a digital twin of the observed environment. In our
architecture, the TDD may include different sensors and
actuators, belonging to different logical and physical
environments and tasks: for example, two temperature
sensors might be in different parts of a building or
plant, and two sensors in the same room might have
different logical functions, which make them function-
ally independent (e.g., light and temperature sensors).
The system administrator then needs to select the set
of sensors to gather in a single DT by querying the
TDD. Once the model is instantiated, the Middleware1. Digital Twin training and data gathering
Aggregator
Manual policy
Sensor 1
Sensor 2
Sensor 3
Actuator 1Actuator 2
Digital Twin
Sensor data
Commands2. Virtual model-based DRL training
Twin 1
Twin 2
Twin 3Virtual environment
DRL agent
CommandsState, rewards
Fig. 2: Diagram of the two stages of a model-based DRL training in the WoT platform.
layer automatically subscribes it to the sensors Ô¨Åtting
the speciÔ¨Åed TD, starting the twinning process. The
platform is entirely agnostic to the training method
used for the model, which could use either statistical or
learning-based methods: since the architecture operates
over standardized queries and responses, the DTs can
be implemented as a black box, even using different
programming languages. During this stage, the actuators
in the environment must be operated using a legacy
policy, which may be deÔ¨Åned by an algorithm or directly
by the user, as shown in Fig. 2.
2)Virtual model-based DRL training: After the DTs have
reached the required accuracy, a DRL agent can be
trained in a virtual environment without affecting the
real system. The twins can be used in a predictive
fashion, applying the results of the agent‚Äôs choices and
simulating the real system. The virtual environment
may include more than one twin, representing different
subsystems that are affected by the agent‚Äôs actions and
concur in determining the system reward. Naturally, the
choice of which DTs must be included, as well as which
actuators can be controlled by the DRL agent, is made
by the system administrator.
From a DRL point of view, the model-based training is
robust, as long as the DTs represent the effects of actions
accurately. In any case, a short online training phase in the
real environment is still possible, with a small cost in terms
of system efÔ¨Åciency. During this phase, a directed exploration
algorithm will still avoid any catastrophic failure modes, as
the Agent will have learned that they lead to signiÔ¨Åcant
penalties in the virtual environment. On the other hand, from
the WoT perspective, the Middleware just needs to connect the
Agent to the proper components: during the virtual training,
observations of the environment are provided by the DTs,
while during real operation, observations from the sensors are
pushed directly to the Agent. In the same way, its commandsare only directed to the DTs during the virtual training, but
are routed to the real Actuators in the operational phase.
C. Legacy Components
In order to integrate our architecture into an existing system,
the Middleware also includes the possibility to interact with
legacy systems already available in the smart environment.
To accomplish this, it is also possible to design speciÔ¨Åc
components directly in the Middleware to interconnect to other
systems exposing data or services.
A note on this is that those speciÔ¨Åc components, developed
to connect to legacy systems, have to be also deployed as a
Thing themselves. This allows for them to be registered in
the TDD upon connecting to the legacy system, so that other
devices can leverage the data exposed by them, or connect
to the offered services. Clearly, the precise deÔ¨Ånition of one
or more legacy components has to be made according to the
scenario and to already existing deployments, as it enables our
system to integrate and cooperate with the others.
IV. U SECASE: HEATING SYSTEM DRL C ONTROL
In this section we describe a prototype implementation
of our system, and we detail how we implemented all the
software components described in Section III. We consider a
smart home scenario, in which different sensors can provide
environmental data in order to schedule activities and optimize
the planning. We focus our efforts on deploying our system
aside a legacy system which manages different sensors and
actuators around a house, which are not compliant to the
WoT standard. This scenario can be found in many modern
homes, which leverage data from off-the-shelf devices which
also provide proprietary software solutions to manage them.
We base our analysis on the climate operations of the house,
which means that our aim is to optimize the temperature of
different rooms accounting for the room occupancy and for
the inside and outside temperature. SpeciÔ¨Åcally, we deploy our
system in a real house deployment with 2 Ô¨Çoors, three roomsNov. 8Nov. 15 Nov. 22 Nov. 29Dec. 65101520Average daily temperature (C)
Living room
Bathroom
Bedroom
External temperature
Fig. 3: Temperature dataset used for our experiments.
and four inhabitants which provide heterogeneity in objectives
and room occupancy, which are:
A living room open space, which also includes the
kitchen;
A bathroom;
A master bedroom shared by two people.
The living room is generally occupied during the morning, at
lunchtime and during the evenings, but it may also get sporadic
occupation throughout the entire day in case people work from
home or at lunch and dinner times. A bathroom typically has a
Ô¨Çat occupancy during the day, meaning that people can use it
at different times without a speciÔ¨Åc routine. On the other hand,
the bedroom instead has zero occupancy during the day, and
full occupancy during the night, although bedtime and wake
ups can have relatively small variations on different days of
the week. We will detail the parameters we took into account
below.
In Figure 3, we also show the temperature dataset we have
collected over 40 days. This data has been obtained through
the legacy system sensors which were already deployed in the
house in November-December 2022. The black line represents
the outdoor temperature and shows a signiÔ¨Åcant drop, from
roughly 12Cto around 5C. The other three lines show the
temperature inside the three rooms considered as part of this
study. While the absolute values differ, their trends are similar
and also reÔ¨Çect the current conÔ¨Åguration of the already existing
heating system. The heating in the three rooms is controlled by
a classical thermostat system, in which the inhabitants set the
desired temperature in different part of the day and the heating
system is switched on to match that temperature. The schedule
varies greatly for the three rooms: the master bedroom is only
heated at night, the living room only during the day, while
the bathroom is heated in the morning and in the evening.
Nevertheless, they show a similar trend as heat spreads through
the whole house.
A. Implementation
The house was already running a legacy system provided
by Home Assistant1, which is a personal smart home hub
providing integration with several off-the-shelf devices. Home
1https://www.home-assistant.io/Assistant allows users to interconnect components offered by
different vendors, and provides a standardized domain and
naming schemes to obtain data and perform actions. We
describe at Ô¨Årst the implementation of the Legacy component
connecting the middleware to Home Assistant, and we then
highlight how it is integrated into our architecture.
Home Assistant provides data with different APIs. For
our purpose, we performed the integration through the data
exposed in InÔ¨ÇuxDB, a popular time-series database which is
integrated within Home Assistant and which stores data from
the devices managed by Home Assistant. Since our aim is
to be WoT compliant to make devices able to register in the
TDD, we developed a software component as a Thing, which
performs the following:
It connects to InÔ¨ÇuxDB and reads all the devices pro-
viding a variable of interest. In our case, since we are
focusing on optimizing the temperature, the correspond-
ing domain is climate , which is the group of devices in
Home Assistant which can provide temperature, humidity
and also perform operations such as switching on a
cooling or heating system.
For each discovered device, it provides a speciÔ¨Åc GET
operation advertised in its thing description. For each of
these methods in the TD, whenever they are requested it
will translate the query to an appropriate InÔ¨ÇuxDB query
to retrieve the data and eventually provide it to the client
requesting it.
This software component is then registered in the TDD, so
that other devices have seamless access to its data without
requiring speciÔ¨Åc knowledge on the technical details of the
connection.
The developed software component also provides bidirec-
tional communication, meaning that it can also feedback
to Home Assistant. This is realized by exposing a PUT
method in the TD of the software, which allows clients to
send data to the Thing, which then routes it back to Home
Assistant. To realize this part, without loss of generality we
have adopted the MQTT2protocol, by publishing data on the
Home Assistant MQTT broker. Clearly, other protocols can
be adopted depending on the scenario and on the speciÔ¨Åc
needs. We show an example of this behavior in Fig. 4: Fig. 4a
shows the MQTT subscribe (RL-TEMP) node inside Node-
RED running in Home Assistant. Figure 4b shows instead
the Home Assistant dashboard, with the three heating systems
managed by our models and with their current statuses.
B. Digital Twin Evaluation
In this section, we show the performance evaluation of our
model in a real deployment, leveraging a Thermal Twin (TT)
which twins the temperature dynamics accounting for outside
and inside temperatures, and a Room Twin (RT) which instead
accounts for the rooms occupancy.
The TT was adapted from grey box RC-equivalent models
in the relevant literature [28]. We considered two measured
2MQTT originally stood for Message Queuing Telemetry Transport, but the
acronym has ofÔ¨Åcially not stood for anything since 2013.(a) Node-RED integration into Home Assistant
(b) Dashboard visualization and state.
Fig. 4: Integration of the legacy component output inside the
Home Assistant instance.
7 14 21 2800:511:5
Days of trainingMSE (C2)Bathroom
Bedroom
Living room
Fig. 5: MSE for the three considered rooms as a function of
the amount of available data.
parameters, the room‚Äôs internal temperature Tiand the ambient
temperature outside the building Ta, which are available from
the smart home sensors, along with a set of hidden variables
which can help in tracking the thermal evolution of a room.
TheTiTemodel includes the temperature of the envelope (i.e.,
the building‚Äôs external walls), and the TiThmodel includes the
temperature of the heating system. The TiTeThmodel includes
both, and the TiTeThRiaincludes a variable thermal resistance
between the air in the room and the exterior of the building.
Naturally, more complex models are more accurate, but require
more data to avoid overÔ¨Åtting.
Fig. 5 shows the Mean Square Error (MSE) of the internal
temperature predicted by the TT for the three rooms we have7 14 21 28Bathroom Bedroom Living room
TiTeThTiThTiTeThTiTeThRia
Ti TiThTiThTiTh
TiTeThTiTeThTiThTiTeTh
Days of trainingRoom
Fig. 6: Best models for the TT as a function of the amount of
training data.
considered in our study. As expected, the MSE generally
decreases as the available data become richer, but the three
rooms all have different trends: while 7 days of data are
enough to train the DT of the living room, the master bedroom
needs signiÔ¨Åcantly more data and has a far higher error. This
can be explained by the fact that the master bedroom is the
room which is least occupied during the day: the model is
then able to learn about the heating system dynamics only
during the night, and the nighttime temperature is kept at a low
value of 16C. This does not allow it to obtain a sufÔ¨Åciently
wide variety of samples to learn from; therefore, the model
does not learn as fast as in the other rooms, which show
a similar behavior. Nevertheless, all three rooms show large
improvements when trained with a longer dataset: the average
error for the master bedroom decreases from more than 1.8C
with only 1 day of training to less than 1.2Cwhen trained
with a month of data, with roughly a 30% reduction in the
total error. A similar comment can be also made for the other
two rooms, where at the beginning of the training the error is
around 1.2Cand it is reduced to roughly 0.7C. We can also
consider which model performs best in each room for different
amounts of training data: the best model for each considered
case is shown in Fig. 6, which conÔ¨Årms our analysis. More
complex models are usually better with more data, and the
evolution of the best model selection follows this trend, with
some oscillations.
There are three main takeaways from this analysis: (i),
personalized models have to be created and trained for each
room, even in the same house, as the heating and temperature
dynamics vary greatly between rooms, depending on their
size, isolation, exposure to sunlight, and heating units; (ii), the
larger the training set, the better the results, as it is possible to
observe a wider variety of samples; (iii), a DT can effectively
help to understand when a model is ready to be deployed, since
it can be trained on a portion of data and tested with fresh
data, and then used whenever the error falls under a scenario-036912151821SunSatFriThuWedTueMon
HourDay
(a) Living room.036912151821SunSatFriThuWedTueMon
HourDay
(b) Bathroom.036912151821SunSatFriThuWedTueMon
HourDay
00:20:40:60:81
(c) Master bedroom.
Fig. 7: Heatmap of the probability for each room to be occupied over time in the synthetic RT model.
speciÔ¨Åc threshold, and iv, the DT can include different types
of models with different levels of complexity, which can be
hot-swapped as new data become available and richer models
can be trained effectively. For a wider review of RC-equivalent
model parameters and training, we refer the reader to [28].
On the other hand, the smart home deployment did not
provide room-level occupant data, so we assumed a simple
Markovian model. For each room, we considered a maximum
number of occupants Nmax, each of whom behaves inde-
pendently. The occupant is in state 1 if they are inside the
room, and the evolution of their state is driven by the matrix
P(d;h), which is a function of the day of the week and of
the current time. Fig. 7 shows the average probability of each
room being occupied over the whole week: the bedroom is
occupied during the night, with a slightly shifted timeframe in
the weekend, while the bathroom and living room are usually
occupied during the day and evening, with different transition
probabilities. Naturally, the actual probability of a room being
occupied at a given time will depend on the state, but we can
see that the bedroom is more predictable, while, as expected,
the bathroom has a much more random pattern throughout
the day. The assumed probability for someone to be in the
bathroom is low but constant throughout the day, while the
occupants remain in the living room for longer times during
the morning and evening, resulting in a higher probability of
the room being occupied. The three rooms also have different
numbers of occupants: we assume that the whole family of 4
may be present in the living room, while only 2 people use
the master bedroom, and only one person at a time can be in
the bathroom (in this case, one person leaving the bathroom
and another entering would count as one person remaining in
the room).
The use of an artiÔ¨Åcial model for the room occupancy re-
duces its accuracy in the real deployment, but the architecture
can easily accommodate new sensors and DTs, automatically
retraining the DRL agent after updates to the underlying
models. In fact, since a time-dependent RT would need to be
trained over multiple weeks to Ô¨Ågure out the family‚Äôs habits,
starting from a relatively accurate prior would signiÔ¨Åcantly
reduce the DT training time.C. Agent evaluation
In this section, we evaluate the performance of the agent,
which leverages the RT and TT to forecast actions which
improve the overall status of the system.
The temperature control Markov Decision Process (MDP) is
relatively simple: the state of the system is represented by the
states of the two twins, which include information about the
thermal state of the room and heating system, as well as the
date and current occupancy of the room. The possible actions
available to the agent are Alevels of heating system power,
which are normalized from 0 (the heating system is off) to 1,
corresponding to the maximum available heating power. The
reward function for taking action awhen there are opeople
in the room, resulting in an internal temperature Ti, is deÔ¨Åned
as follows:
R(Ti;o;a ) = 1(Ti T0) 1(o 1) a
  1(jTi T0j Tlim) (1 2 1(a))sign(Ti T0);(1)
whereT0andTlimare user-deÔ¨Åned threshold temperatures,
1()is the stepwise function, equal to 1 if the argument is
greater than 0 and 0 otherwise, and andare penalty
parameters representing the cost of energy and a generic
penalty for . In other words, the agent receives a reward 1
if the temperature is above the comfort level T0, but only if
there are people in the room; on the other hand, using energy
to heat always has a cost. The second penalty term is used to
avoid excessive temperature swings: if the room is too warm
or too cold by more than Tlimdegrees, actions that do not
lead the system back toward equilibrium (i.e., not turning on
the heat if it is too cold, or turning it on when the room is
already too hot) are penalized. The values we selected for the
parameters are listed in Table I.
TABLE I: MDP parameters.
Parameter Symbol Value
Episode duration (steps) L 1000
Step duration (minutes) t 15
Comfort temperature T0 18C
Threshold temperature Tlim 3C
Energy cost  0:25
Swing penalty  0:2
Discount factor  0.957 14 21 28 0:500:5Reward
Agent (bathroom) Ideal (bathroom)
Agent (bedroom) Ideal (bedroom)
Agent (living room) Ideal (living room)
Fig. 8: DRL agent performance at convergence as a function
of the number of days of available data.
We then implement a DQN model, with an architecture
that depends on the DT models, and is deÔ¨Åned in Table II,
along with the main training parameters: the RT only needs
NRT= 3 parameters (the day of the week, time, and number
of current occupants of the room), while the TT can have a
variableNTTdepending on the RC-equivalent model. The Ti
model only has a single parameter, while the more complex
TiTeThRiamodel has 4. The Ô¨Ånal input value is the ambient
temperature Taoutside the room. The neural network uses
a Softmax exploration policy, whose temperature decreases
logarithmically from 1 to 10 6, and the Adam optimizer, with
ReLU activation.
Fig. 8 shows the performance of the DRL agent, as mea-
sured by the reward, when it is trained using a TT with a
limited amount of data. In all cases, the RT is assumed to be
perfect. We can immediately note that the bedroom needs at
least 3 weeks of data to control the room temperature properly,
while the bathroom can achieve good performance with a
single week. This is consistent with the MSE performance of
the models: while the bathroom dynamics are easy to learn,
and the model predictions are similar to reality, the signiÔ¨Åcant
errors in the bedroom mean that the virtual training leads
the DRL agent to the wrong policy, with catastrophic results.
As the TT improves, so does the performance of the agents,
gradually reaching the optimum. We also highlight that the
optimal performance for a room depends on how much it is
occupied: the dashed lines in the Ô¨Ågure show the performance
of an ideal system with perfect room occupancy prediction
and free energy, i.e., the upper bound to real performance for
each room. The upper bound is extremely hard to reach in
TABLE II: DQN architecture and training parameters.
Layer Symbol Parameters
Input L0NTT+NRT+ 1
Input L1 10L0
Input L2 5L1
Output O A
Hyperparameter Symbol Value
Activation function a ReLU
Learning rate ` 10 5
Dropout probability pd 0.1
Epochs E 25
Episodes per epoch e 20012345678910111213141516171819202122231818:51919:5
Time (h)Ti
00.250.5
Action
Fig. 9: Temperature evolution and control actions in the master
bedroom over the course of a day.
the bigger rooms, as the real agent has to deal with winter
temperatures outside, and consequently frequently uses energy
to power the heating system. On the other hand, the bathroom
does not directly face any walls, and maintaining a comfortable
temperature is relatively easy, so it is possible to achieve an
almost perfect performance.
To provide a better understanding of how the model works,
we also show an example sequence of actions performed
by the DRL agent over the course of a day for the master
bedroom in Fig. 9. The room is only occupied at night,
although during the day has a non-zero probability of having
occupants. During the night (i.e., leftmost part of the plot),
the agent heats up the room beyond the minimum desired
temperature of 18C, with the heating system at half power.
In the morning, the probability of having at least one occupant
signiÔ¨Åcantly drops, hence the DRL agent stops to heat the
room. Sporadic adjustments are made through the day, since
the DRL agent wants to keep the room above the minimum
temperature to avoid any reward losses. In the evening the
DRL agent starts to heat the room again some time before the
usual bedtime, raising the temperature above the minimum
threshold in time for them to go to bed. Obviously, having a
perfect representation of the occupancy would avoid the DRL
agent to sporadically heat the room when the probability of
having occupants is low. Still, Fig. 9 conÔ¨Årms that the system
actually learns a non-obvious policy from the two digital twins,
and improves the system overall.
D. Automatic Agent DeÔ¨Ånition
In the use case we deÔ¨Åned, the DRL agent is still partially
hand-designed: the selection of the DT models was performed
manually, as well as the architecture and hyperparameter
search for the training. However, the WoT architecture we
deÔ¨Åne provides signiÔ¨Åcant room for expansion and for automa-
tion, as agents, models, sensors, and actuators are all controlled
by the same platform.
We can foresee a use case in which multiple twins (e.g.,
including multiple rooms, sensors, and activities) can be
automatically mixed and matched to tasks (as represented by
actuators). The full potential of the proposed architecture isto automate DQN training as well, trying different state deÔ¨Å-
nitions and combinations of actions, as well as automatically
optimizing hyperparameters for each agent. In this case, the
only hand-designed part of the system would be the reward
function, which would be ideal in a domotic scenario: the
inhabitants only need to give the smart home a simple list
of priorities, which they can adjust at any time, and the
architecture will Ô¨Ågure out which agents it will need to deploy,
which twins they will need to accomplish their tasks, and
which tasks are functionally independent and can be separated
to simplify the action space.
V. C ONCLUSIONS
In this paper, we have a presented a novel framework to
realize digital twins based on the Web of Thing standard.
Our architecture allows to build customized digital twins
for complex scenarios, leveraging data from custom made
sensors or legacy ones, by integrating it with already existing
platforms. Our results indicate the beneÔ¨Åts of using DTs
for model-based reinforcement control, as they allow to test
different models and deploy the one which Ô¨Åts the scenario the
best. This has been conÔ¨Årmed by deploying our platform on a
real scenario on which a legacy system was already running,
and integrating our platform with the existing one.
Future work on this topic will be focused on the extension
of our framework to other sensor types in a smart home, a
more in-depth analysis of the room twin, and extended results
on other house types and rooms, which will better generalize
our Ô¨Åndings. Furthermore, the proposed automation of agent
selection and MDP design is an interesting future direction.
REFERENCES
[1] F. Montori, L. Bedogni, M. Di Felice, and L. Bononi, ‚ÄúMachine-
to-machine wireless communication technologies for the Internet of
Things: Taxonomy, comparison and open issues,‚Äù Pervasive and Mobile
Computing , vol. 50, pp. 56‚Äì81, Oct. 2018.
[2] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
et al. , ‚ÄúHuman-level control through deep reinforcement learning,‚Äù
Nature , vol. 518, no. 7540, pp. 529‚Äì533, Feb. 2015.
[3] G. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru,
S. Gowal, and T. Hester, ‚ÄúChallenges of real-world reinforcement
learning: deÔ¨Ånitions, benchmarks and analysis,‚Äù Machine Learning , vol.
110, no. 9, pp. 2419‚Äì2468, Sep. 2021.
[4] Z. Wang and T. Hong, ‚ÄúReinforcement learning for building controls:
The opportunities and challenges,‚Äù Applied Energy , vol. 269, p. 115036,
Jul. 2020.
[5] G. Ostrovski, P. S. Castro, and W. Dabney, ‚ÄúThe difÔ¨Åculty of passive
learning in deep reinforcement learning,‚Äù in 35th Conference on Neural
Information Processing Systems (NeurIPS) , vol. 34, Dec. 2021, pp.
23 283‚Äì23 295.
[6] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims, ‚ÄúMorel:
Model-based ofÔ¨Çine reinforcement learning,‚Äù in 34th Conference on
Neural Information Processing Systems (NeurIPS) , vol. 33, 2020, pp.
21 810‚Äì21 823.
[7] S. Boschert and R. Rosen, ‚ÄúDigital twin‚Äîthe simulation aspect,‚Äù in
Mechatronic Futures . New York City, USA: Springer Cham, Jun. 2016,
pp. 59‚Äì74.
[8] 5G Alliance for Connected Industries and Automation (5G-
ACIA). (2021) Using digital twins to integrate 5G into
production networks. Accessed on Dec. 5, 2022. [Online].
Available: https://5g-acia.org/wp-content/uploads/2021/05/5G-ACIA
Using Digital Twins toIntegrate 5Ginto Production Networks-.pdf[9] D. Raggett, ‚ÄúThe Web of Things: Challenges and opportunities,‚Äù Com-
puter , vol. 48, no. 5, pp. 26‚Äì32, May 2015.
[10] D. Zeng, S. Guo, and Z. Cheng, ‚ÄúThe Web of Things: A survey,‚Äù Journal
of Communications , vol. 6, no. 6, pp. 424‚Äì438, Sep. 2011.
[11] F. Montori, K. Liao, P. P. Jayaraman, L. Bononi, T. Sellis, and D. Geor-
gakopoulos, ‚ÄúClassiÔ¨Åcation and annotation of open internet of things
datastreams,‚Äù in 19th International Conference on Web Information
Systems Engineering (WISE) . Springer, Nov. 2018, pp. 209‚Äì224.
[12] L. Bedogni and F. Poggi, ‚ÄúA Web Of Things context-aware IoT system
leveraging Q-Learning,‚Äù in 19th Annual Consumer Communications &
Networking Conference (CCNC) . IEEE, Jan. 2022, p. 405‚Äì410.
[13] D. Georgakopoulos, P. P. Jayaraman, M. Zhang, and R. Ranjan,
‚ÄúDiscovery-driven service oriented IoT architecture,‚Äù in Conference on
Collaboration and Internet Computing (CIC) . IEEE, Oct. 2015, pp.
142‚Äì149.
[14] R. Agarwal, D. G. Fernandez, T. Elsaleh, A. Gyrard, J. Lanza,
L. Sanchez, N. Georgantas, and V . Issarny, ‚ÄúUniÔ¨Åed IoT ontology to
enable interoperability and federation of testbeds,‚Äù in 3rd World Forum
on Internet of Things (WF-IoT) . IEEE, Dec. 2016, pp. 70‚Äì75.
[15] N. Seydoux, K. Drira, N. Hernandez, and T. Monteil, ‚ÄúIoT-O, a core-
domain IoT ontology to represent connected devices networks,‚Äù in
European Knowledge Acquisition Workshop (EKAW) . Springer, Nov.
2016, pp. 561‚Äì576.
[16] D. M√∏nster, R. Fusaroli, K. Tyl ¬¥en, A. Roepstorff, and J. F. Sherson,
‚ÄúCausal inference from noisy time-series data‚Äîtesting the convergent
cross-mapping algorithm in the presence of noise and external inÔ¨Çu-
ence,‚Äù Future Generation Computer Systems , vol. 73, pp. 52‚Äì62, Aug.
2017.
[17] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman,
‚ÄúBuilding machines that learn and think like people,‚Äù Behavioral and
Brain Sciences , vol. 40, p. e253, Jan. 2017.
[18] M. Lippi, S. Mariani, and F. Zambonelli, ‚ÄúDeveloping a ‚Äúsense of
agency‚Äù in IoT systems: Preliminary experiments in a smart home
scenario,‚Äù in International Conference on Pervasive Computing and
Communications Workshops (PerCom Workshops) . IEEE, Mar. 2021,
pp. 44‚Äì49.
[19] I. Agadakos, G. F. Ciocarlie, B. Copos, T. L. Lepoint, U. Lindqvist,
and M. Locasto, ‚ÄúButterÔ¨Çy effect: Causality from chaos in the IoT,‚Äù in
1st International Workshop on Security and Privacy for the Internet-of-
Things (IoT S&P) . ACM, Apr. 2018, pp. 26‚Äì30.
[20] P. Madumal, T. Miller, L. Sonenberg, and F. Vetere, ‚ÄúExplainable
reinforcement learning through a causal lens,‚Äù in Conference on ArtiÔ¨Åcial
Intelligence , vol. 34, no. 3. AAAI, Apr. 2020, pp. 2493‚Äì2500.
[21] Y . Lei, S. Zhan, E. Ono, Y . Peng, Z. Zhang, T. Hasama, and A. Chong,
‚ÄúA practical deep reinforcement learning framework for multivariate
occupant-centric control in buildings,‚Äù Applied Energy , vol. 324, p.
119742, Oct. 2022.
[22] L. Di Natale, B. Svetozarevic, P. Heer, and C. Jones, ‚ÄúDeep reinforce-
ment learning for room temperature control: a black-box pipeline from
data to policies,‚Äù vol. 2042, no. 1, p. 012004, Nov. 2021.
[23] G. Pinto, Z. Wang, A. Roy, T. Hong, and A. Capozzoli, ‚ÄúTransfer learn-
ing for smart buildings: A critical review of algorithms, applications,
and future perspectives,‚Äù Advances in Applied Energy , p. 100084, Jan.
2022.
[24] T. Schreiber, C. Netsch, M. Baranski, and D. M ¬®uller, ‚ÄúMonitoring data-
driven reinforcement learning controller training: A comparative study
of different training strategies for a real-world energy system,‚Äù Energy
and Buildings , vol. 239, p. 110856, May 2021.
[25] M. Seitzer, B. Sch ¬®olkopf, and G. Martius, ‚ÄúCausal inÔ¨Çuence detection
for improving efÔ¨Åciency in reinforcement learning,‚Äù 35th Conference on
Neural Information Processing Systems (NeurIPS) , vol. 34, pp. 22 905‚Äì
22 918, Dec. 2021.
[26] M. Matulis and C. Harvey, ‚ÄúA robot arm digital twin utilising rein-
forcement learning,‚Äù Computers & Graphics , vol. 95, pp. 106‚Äì114, Apr.
2021.
[27] K. Xia, C. Sacco, M. Kirkpatrick, C. Saidy, L. Nguyen, A. Kircaliali, and
R. Harik, ‚ÄúA digital twin to train deep reinforcement learning agent for
smart manufacturing plants: Environment, interfaces and intelligence,‚Äù
Journal of Manufacturing Systems , vol. 58, pp. 210‚Äì230, Jan. 2021.
[28] J. Leprince, H. Madsen, C. Miller, J. P. Real, R. van der Vlist, K. Basu,
and W. Zeiler, ‚ÄúFifty shades of grey: Automated stochastic model
identiÔ¨Åcation of building heat dynamics,‚Äù Energy and Buildings , vol.
266, p. 112095, Jul. 2022.