Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR
Aneeshan Sain1,2Ayan Kumar Bhunia1Subhadeep Koley1,2Pinaki Nath Chowdhury1,2
Soumitri Chattopadhyay*Tao Xiang1,2Yi-Zhe Song1,2
1SketchX, CVSSP, University of Surrey, United Kingdom.
2iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.
{a.sain, a.bhunia, p.chowdhury, t.xiang, y.song }@surrey.ac.uk
Abstract
This paper advances the fine-grained sketch-based im-
age retrieval (FG-SBIR) literature by putting forward a
strong baseline that overshoots prior state-of-the-arts by
≈11%. This is not via complicated design though, but by
addressing two critical issues facing the community (i) the
gold standard triplet loss does not enforce holistic latent
space geometry, and (ii) there are never enough sketches to
train a high accuracy model. For the former, we propose a
simple modification to the standard triplet loss, that explic-
itly enforces separation amongst photos/sketch instances.
For the latter, we put forward a novel knowledge distilla-
tion module can leverage photo data for model training.
Both modules are then plugged into a novel plug-n-playable
training paradigm that allows for more stable training.
More specifically, for (i) we employ an intra-modal triplet
loss amongst sketches to bring sketches of the same in-
stance closer from others, and one more amongst photos to
push away different photo instances while bringing closer a
structurally augmented version of the same photo (offering
a gain of ≈4-6%). To tackle (ii), we first pre-train a teacher
on the large set of unlabelled photos over the aforemen-
tioned intra-modal photo triplet loss. Then we distill the
contextual similarity present amongst the instances in the
teacher’s embedding space to that in the student’s embed-
ding space, by matching the distribution over inter-feature
distances of respective samples in both embedding spaces
(delivering a further gain of ≈4-5%). Apart from outper-
forming prior arts significantly, our model also yields satis-
factory results on generalising to new classes. Project page:
https://aneeshan95.github.io/Sketch PVT/
1. Introduction
Sketch [7, 18, 42] has long established itself as a worthy
query modality that is complementary to text [17, 25, 28].
Conceived as a category-level retrieval task [10,20], sketch-
based image retrieval (SBIR) has recently taken a turn to a
“fine-grained” setting (i.e., FG-SBIR), where the emphasis
*Interned with SketchX
Intra-modal  
photo
EMATransformer  
EncoderProposed Strong
Transformer FG-
SBIR Baseline
Intra-modal  
sketch  Cross-modal  
InputTeacher
Student
Sketch-photo  
pairsUnlabelled  
photos
Latent spaceunlabelled photosLearned  
Latent space
Knowledge
Distillation
TrainingPre-trained on 
Training from unlabelled data
Figure 1. (left) A strong FG-SBIR baseline with strong PVT [80]
backbone trained on intra-modal triplet loss, stabilised by EMA.
(right) It additionally leverages unlabelled photos to enrich its la-
tent space by distilling instance-wise discriminative knowledge of
a teacher pre-trained on unlabelled photos.
is on fully utilising the faithful nature of sketches to conduct
instance-level retrieval [75, 87].
Despite great strides made in the field, without excep-
tion, all existing FG-SBIR models [65, 88] work around a
cross-modal triplet objective [87] to learn a discriminative
embedding to conduct retrieval. The general intuition has
always been to make a sketch sit closer to its paired photo
while pushing away non-matching ones (Fig. 1). However,
a conventional triplet setup does not enforce sufficient sepa-
ration amongst different photos or sketch instances – largely
because the conventional objective fails to retain the holistic
latent space geometry, being overly restrictive on learning
within-triplet feature separation.
For that and as our first contribution, we utilise an intra-
modal triplet objective in both modalities, in addition to
the regular cross-modal [87] triplet objective. For sketch, a
query-sketch (anchor), is brought closer to another sketch of
the same target-photo (positive) while distancing it from a
sketch of any non-matching photo (negative). For the photo
modality, as there is only one uniquely paired photo (an-
chor), we treat a morphologically augmented version of the
photo as positive, and a non-matching one as negative. Im-
portantly, we show that the best morphological operations
are those that operate on visual attributes bearing relevance
with information exclusive to sketches (e.g., shape) Fig. 1.
The other, perhaps more pressing problem, facing the
1arXiv:2303.13779v1  [cs.CV]  24 Mar 2023community is that of sketch data scarcity – there are just
not enough sketches for the extra performance gain [5, 6].
Instead of going for the more explicit route of photo to
pseudo sketch synthesis [4], we vote for the simple and
more implicit route of leveraging unlabelled photos. For
that, we adapt a knowledge distillation setup – we first train
a model on unlabelled photos only via an intra-modal triplet
loss, then distill its instance-wise discriminative knowledge
to a FG-SBIR model. This apparently simple aim is how-
ever non-trivial to train owing to our cross-modal setup
as naively using standard knowledge distillation paradigms
involving logit distillation [36] from teacher’s embedding
space to the student’s would be infeasible. For a cross-
modal problem like ours, we need to preserve the instance-
wise separation amongst photos, as well as corresponding
sketches. For that, we propose to preserve the contextual
similarity between instances and their nearest neighbours,
modelled as a distribution over pairwise distances, from the
pre-trained teacher (photo model), to that of the student
(FG-SBIR). Inspired from recent literature, we introduce
a novel distillation token into our PVT-backboned student,
that is dedicated towards distilling contextual similarity.
However fitting this token into the existing PVT-
architecture is non-trivial. Unlike other vision transformers
like ViT [27], PVT employs a reshape operation [80], to re-
shape the resultant individual patch tokens at one level back
to the feature map, for input to the next level. It follows
that adding a token here naively would break this operation.
We thus use our distillation token only during input to the
transformer layer at each level [80], and set the modified
token aside before the reshaping operation. A residual con-
nection [75] thereafter connects this modified token as input
to the transformer layer at the next level. Engaging the dis-
tillation token at every level like ours, helps it imbibe the
inductive bias modelled by the pyramidal structure of PVT,
thus facilitating better distillation.
Finally, on the back of a pilot study (Sec. 3), we uncover
a widespread problem with standard triplet loss training –
that training is highly unstable, reflected in the highly os-
cillating evaluation accuracies noted at every 100thtraining
iteration. For that, we take inspiration from literature on
stabilising GAN training [84] on Exponential Moving Av-
erage [84] – a strategy that employs a moving average on
model parameters iteratively, with a higher priority (mathe-
matically exponential) to recent iterations over earlier ones.
To sum up: (i) We propose a strong baseline for FG-
SBIR, that overshoots prior arts by ≈10% (ii) We achieve
this by putting forward two simple designs each tackling a
key problem facing the community: inadequate latent space
separation, and sketch data scarcity. (iii) We introduce a
simple modification to the standard triplet loss to explic-
itly enforces separation amongst photos/sketch instances.
(iv) We devise a knowledge distillation token in PVT [80]that facilitates better knowledge distillation in training from
unlabelled data. Finally, apart from surpassing prior arts
significantly, our model also shows encouraging results on
generalising to new classes, without any paired sketches.
2. Related Works
Fine-grained SBIR (FG-SBIR): FG-SBIR aims at re-
trieving one particular photo from a gallery of specific-
category corresponding to a query-sketch. Initially pro-
posed as a deep triplet-ranking based siamese network [87],
FG-SBIR has progressively improved via attention-based
modules with a higher order retrieval loss [75], textual
tags [17,73], self-supervised pre-training [54], hierarchical
co-attention [66], cross-category generalisation [53], and
reinforcement learning [9]. Overall, they aim to learn a
joint embedding space so as to reduce the cross-modal gap
[32, 66], typically using a triplet ranking loss [87] that aims
to bring a sketch closer to its paired photo while distancing
it from others. Some have optimised target-photo rank [9],
fused classification loss with ranking [37], used multi-task
attribute loss [74], or other loss modules in self-supervised
pretext tasks [5,54] for efficiency. Others addressed sketch-
specific traits like style-diversity [67], data-scarcity [4] and
redundancy of sketch-strokes [6] in favor of better retrieval.
Arguing against such complex frameworks, we aim for a
simpler yet stronger FG-SBIR pipeline, that can generalise
well across categories even in low-data regime.
Transformers in Computer Vision: Transformers [78]
are end-to-end neural networks that leverage self-attention
mechanism [15] for modelling sequential data. Vision
transformers [26, 47, 80] win over traditional CNNs [71] in
their ability to model long-range dependencies in sequential
data (here, visual patches) thus learning a stronger feature
representation than CNNs [26]. Since the seminal work of
ViT [26] that introduced image patches as input to trans-
former layers, it has been improved further via convolu-
tion components [82], attention-guided knowledge distilla-
tion for data-efficiency [77] and a feature pyramid [80, 81]
modeling inductive bias. Common applications include ob-
ject detection [12], image super-resolution [83], image syn-
thesis [39], etc. Despite their recent usage in fine-grained
image recognition [38, 46], only few have employed trans-
formers in encoding sketches [24, 68] Such transformers
however have not been used for FG-SBIR. In this paper, we
adapt transformers for the first time in FG-SBIR to propose
a strong baseline that would outperform existing state-of-
the-arts and we hope research would progress further con-
sidering our baseline as a standard baseline.
Training from Unlabelled Data: Mainly two frontiers
of research have emerged in taking advantage of unlabelled
data – self-supervised and semi-supervised learning. The
former spans over a large literature [40], including gener-
ative models [29] like V AEs [41], contrastive learning ap-
2Figure 2. Left: Training stability of a baseline vs ours. Study on variable data-size (center) and cross-category generalisation (right).
proaches [16, 33], clustering [13], etc. Furthermore, several
pre-text self-supervised tasks have been explored like im-
age colorization [91], in-painting [59], etc. Recent appli-
cations in sketch include a pre-text task of solving jigsaw
puzzles [54], and learning from the dual representative na-
ture of sketches [4,5]. Semi-supervised learning aims at ex-
ploiting large unlabeled data together with sparsely labeled
data to improve model performance. Common approaches
include entropy minimization [30], pseudo-labeling [72], or
consistency regularization [51]. While pseudo-labeling em-
ploys confident prediction [72] from trained classifiers to
create artificial labels [43] for unlabelled data, consistency
regularisation learns a classifier by promoting consistency
in predictions between different views of unlabeled data, ei-
ther via soft [52] or hard [72] pseudo-labels. Recently, data
scarcity in SBIR was handled, by generating more sketches
for unlabelled photos [4] in a semi-supervised setup.
We focus on one of its related paradigm of knowledge
distillation [2] that aims at transferring knowledge of a pre-
trained teacher network to a student . While some lever-
age output logits [2] others focus on hidden layers [64] or
attention-maps [90] of pre-trained teachers for the same.
Improvising further, self-distillation [3] employed the same
network for both student and teacher models, whereas a
multi-exit strategy [60] optimised compute via multiple-
exits at different depths for adaptive inference. Common
applications include object detection [23], semantic seg-
mentation [35], depth-estimation [61], etc. Contrarily, FG-
SBIR demands: (a) instance level discrimination (b) cross-
modal one-to-one correspondence. As collecting sketch-
photo labelled pairs is costly, we train a teacher from abun-
dant unlabelled photos. The instance-discrimination thus
learned in photo space, is distilled to a student FG-SBIR
model to make it more discriminative.
3. Pilot Study: Delving deeper into FG-SBIR
Training stability: Being an instance-level cross-modal
retrieval problem, training an FG-SBIR [66] model is often
found to be unstable, as evaluation accuracy oscillates sig-
nificantly during training. Hence, it is evaluated frequently
to capture the best accuracy. Being the first to consider this,
we first analyse by plotting (Fig. 2) test-set evaluation at
every 100 training-steps of an existing baseline FG-SBIR
model with VGG-16 backbone, trained over standard tripletloss [87]. Towards reducing such instability, we design a
stronger baseline model, which we hope will be a standard
baseline for future research.
Variable dataset size: Existing literature shows FG-
SBIR model performance to suffer from scarcity of training
sketch-data [4]. This leads us to wonder if unlabelled photos
can be leveraged to train an FG-SBIR model to reduce the
data annotation bottleneck on sketches for training. Con-
sequently, we conduct a study to analyse how an FG-SBIR
model performs when training data-size is varied. Accord-
ingly we compute and plot the performances of existing and
ourstronger baselines (Sec. 4) on varying dataset-size in
Fig. 2. As both baselines, perform poorly on decreasing
labelled training data, we explore if unlabelled photos can
boost performance in such a scenario. We thus design a dis-
tillation strategy from unlabelled photos, following which,
our method at ≈50% training data, matches the accuracy
(32.03%) of an existing baseline at 100% training data (red
square in Fig. 2), thereby justifying our paradigm.
Classes without any sketch-photo pairs: Besides train-
ing data scarcity, generalisation to unseen classes for multi-
category FG-SBIR [8], is also one of the challenging issues
in FG-SBIR. Considering a realistic scenario, sketch-photo
pairs might not always be available for all classes, how-
ever photos pertaining to such classes can be curated fairly
easily. Therefore, given a model trained on classes having
sketch-photo pairs, we aim to discover how well can it per-
form on classes lacking paired sketches. We thus conduct
a small pilot study by taking the first 25 classes (alphabet-
ically) from Sketchy [69] – 15 seen training (sketch-photo
pairs available) and 10 testing (only photos available), in an
FG-SBIR setting. Fig. 2 shows baseline model to perform
significantly better on classes with access to sketch-photo
pairs but fails poorly on classes lacking paired sketches.
The main challenge therefore is to leverage the knowledge
of classes having unlabelled photos, to preserve the model’s
accuracy. In doing so, our method maintains a stable per-
formance via its knowledge distillation paradigm.
4. A Stronger FG-SBIR baseline
Overview: Unlike CNN-based existing FG-SBIR baselines
[75, 87] that learn a strong sketch-embedding function over
a cross-modal triplet loss, we enhance the paradigm with
three distinct modifications: (i) Employ vision transformer,
3a+
-
(a)Softmax
KK
Pre-trained  
Teacher  
PVT
Student PVT
 
Modified
PVT blockModified PVT Block
+ + =Softmax
Discriminative featureDistillation feature
InputNext Block
Linear + Norm + Reshape
Patch LayerReshape
Equal
PatchesTransformer Layer
P.E.Distillation
Token
From last blockFigure 3. Using PVT as a backbone, a teacher pre-trained on unlabelled photos, distills the contextual similarity among features in its latent
space to a student, which also learns a discriminative latent via cross- and intra-modal triplet losses. Distillation occurs via a learnable
distillation token (shown as inset) introduced in the student’s PVT backbone, in three ways: (i) from unlabelled photos, (ii) labelled photos
and (iii) by aligning sketches to their paired photos in student-space by distilling contextual similarity of labelled photos in teacher-space.
particularly PVT [80] as our backbone after thorough anal-
ysis (Sec. 6), as unlike CNNs having local receptive fields,
vision transformers ensure a global receptive field, mod-
elling better feature representations. (ii) Besides cross-
modal triplet loss [87], we formulate an intra-modal triplet
loss that helps in discriminative learning. (iii) Towards in-
creasing training stability we follow GAN-literature where
training GANs are often unstable [84], in employing Expo-
nential Moving Average (EMA) to our paradigm [84].
Architecture: In a nutshell, PVT [80] generates fea-
ture maps at different scales by operating on an image
I∈RH×W×3(sketch/photo) over multiple ( m) levels.
Given input Il∈RHl−1×Wl−1×Cl−1at level l∈[1, m]:
(a)a patch-embedding layer extracts patch-wise features
(pi∈RHl−1
pl×Wl−1
pl×Cl−1;i∈[1,Hl−1Wl−1
p2
l]) of patch-size
pl×pl.(b)They are passed via a transformer layer to ob-
tain patch-tokens, which (c)are reshaped to a down-scaled
feature map Fl∈RHl−1
pl×Wl−1
pl×Clas input for next level
(Il+1). Following [80] we use m=4levels, keeping pl=4
per level, to obtain the global average pooled final feature
fI∈Rdfor retrieval.
Cross-modal (CM) Triplet Loss: Taking independent em-
bedding of a sketch ( fs) as anchor, traditional [87] cross-
modal triplet loss LCM
Triaims to minimise its distance from
its paired photo embedding ( fp) while maximising that from
a non-matching one ( fn) in a joint embedding space. Using
mCMas the margin hyperparameter and δ(·,·)as a distance
function where δ(a, b) =||a−b||2, we have,
LCM
Tri= max {0, mCM+δ(fs, fp)−δ(fs, fn)} (1)
Intra-modal (IM) Triplet Loss: Despite separating
sketches from non-matching photos, LCM
Trioften pushes vi-
sually similar photos closer, resulting in sub-optimal la-
tent space. Consequently, we focus on intra-modal feature-separation via two sets of intra-modal triplet losses. Hav-
ing multiple sketches per photo, we take features of query-
sketch as the anchor ( fs), another sketch of the same tar-
get photo ( p) as its positive ( fs+), and that of a random
non-matching photo as the negative ( fs−). Given an FG-
SBIR paradigm, as no separately paired positive photo
exists for any photo ( p), we curate a structural augmen-
tation of pasfpt=Perspective (Rotate (p)) where
Rotate (·) randomly rotates an image (-45°to +45°), and
Perspective (·) introduces random perspective transfor-
mation on it. Bearing semblance with a sketch, this only af-
fects the structure of an image, instead of its colour/texture.
Taking pas anchor, pTas its positive and a random negative
(n) we compute intra-modal losses, with mp
IMandms
IMas
respective margin values for photo and sketch modalities.
LIMp
Tri= max {0, mp
IM+δ(fp, fpt)−δ(fp, fn)},
LIMs
Tri= max {0, ms
IM+δ(fs, fs+)−δ(fs, fs−)}(2)
Exponential Moving Average: Following literature on in-
stability of GAN-training [84], or teacher-training in semi-
supervised contrastive learning [11], we employ EMA [11]
to increase training stability. While uniform average of pa-
rameters stabilises training [84], it gets limited by memory.
A moving average (MA), with exponential priority (EMA)
to recent iterations over earlier ones is thus preferred as,
θt
EMA=βθt−1
EMA+ (1−β)θt(3)
where θ0
EMA=θ0, and βis a hyperparameter deciding the
rate at which early iterations fade. With λ1,2as weighting
hyperparameters, we have our overall training loss:
LTrn=LCM
Tri+λ1LIMp
Tri+λ2LIMs
Tri(4)
5. Training with Unlabelled Photos via KD
Once we have a stronger baseline, we aim to harness the
learning potential of unlabeled photos, towards training a
4strong FG-SBIR model [4]. We thus employ a knowledge
distillation paradigm [36], that transfers the photo instance
discrimination potential from a pre-trained ‘teacher’ ( Ω)
trained on unlabelled photos to our ‘student’ ( ϕ) FG-SBIR
model for cross-modal retrieval. Given a limited amount of
labelled sketch-photo pairs DL={(pi
L;si
L)}NL
ifor our stu-
dent FG-SBIR model, and a larger set of unlabelled photos
DU={pi
U}NU
i, for pre-training our teacher ( NU≫NL),
our goal is to improve retrieval accuracy of our student FG-
SBIR model, using both DLandDU(no paired sketches).
However applying this KD paradigm is non-trivial to our
setting. While conventional KD usually transfers knowl-
edge via logit-distillation [36] for classification problem,
ours is a cross-modal retrieval setup where the output is
a continuous d-dimensional feature in a joint-embedding
space. Moreover, if one naively regresses between features
from a teacher and student for sketch and photo branches,
it might suffer from incompatibility as the two embedding
spaces are different. Also, if teacher [2] and student embed-
ding dimensions are different, an additional feature transfor-
mation layer [64] is needed to match them. While distilling
through pair-wise distance [4,70] might be an option, it fails
to transfer the structural knowledge of the teacher’s entire
latent space to the student, as it focuses on only one pair
at a time. We thus aim to design a distillation framework
that considers the contextual similarity amongst samples in
the teacher’s embedding space and preserves its structural
knowledge while distilling to the student.
Modified PVT for Distillation: Concretely, our network
involves two models – a teacher Ω(·)pre-trained large-
scale photos, and a learnable FG-SBIR student ϕ(·), both of
which uses PVT [80] as backbone feature extractor. While
the teacher’s backbone PVT remains unchanged, we fol-
low recent transformer literature [77] to introduce a learn-
able distillation token ‘ ∆’∈Rd, that allows our model to
learn from the output of the teacher, while remaining com-
plementary to the feature extracted from PVT [80]. How-
ever, naively concatenating a token is infeasible, as unlike
other vision transformers [77], PVT involves reshaping (for
down-scaling) a fixed number of individual patch tokens to
the subsequent feature map, which would be imbalanced
on adding one extra token. Therefore, we focus on the
second step of PVT-block [80] involving transformer layer
(Sec. 4), which can accommodate a variable number of to-
kens. Accordingly, at every level, ‘ ∆’ is fed to the trans-
former layer concatenated with rest of the Nimage patch
tokens ( {pi}N
i=1∈RN×d), to obtain the resultant set of
tokens as {pi}N+1
i=1∈R(N+1)×d. Before reshaping, ∆is
excluded to prevent dimensional mismatch, and fed again
similarly to the transformer layer at the next level via a
residual connection (Fig. 3). Being processed at every level,
‘∆’ not only accommodates the output knowledge of the
teacher network but also, imbibes the inductive bias [80]contributed by the pyramidal structure of PVT. From the fi-
nal layer, the student outputs two features: a discriminative
(f∈Rd), and a distillation feature ( µ∈Rd).
Pre-training Teacher: The teacher Ω(·), is trained on pho-
tos of both labelled and unlabelled sets G=DU∪Dp
L, over
an intra-modal triplet loss following Eqn. 2.
Training FG-SBIR Student: The student’s discriminative
feature is used to train over a combination ( LL
Trn) of cross-
modal and intra-modal triplet losses on labelled data ( DL)
following Eqn. 4. Intra-modal triplet loss over unlabelled
photos is leveraged to harness the potential of unlabelled
photos DUwith a weighting hyperparameter λ3as,
Lϕ
Disc=LL
Trn+λ3LU
Tri (5)
In order to transfer [64] the instance-discriminative
knowledge of photo domain from the pre-trained teacher
Ω(·)to improve the cross-modal instance level retrieval of
student FG-SBIR model ϕ(·), we leverage both unlabelled
photos and sketch/photo pairs from labelled data during
knowledge distillation. First, we pre-compute the features
of all the unlabelled photos DUusing the frozen teacher
model as FΩ
U={fΩ
pi}NU
i=1where fΩ
pi= Ω(pi
U)∈Rd.
Now a very naive way of distillation [36] would be
to pass pi
Uthrough student model ϕ(·)to obtain the dis-
tillation feature µϕ
pi=ϕ(pi
U), and directly regress it
against fΩ
piconsidering it as ground-truth. Alternatively,
one may regress the feature-wise distance between two pho-
tos (pi
U, pj
U) in a similar fashion from teacher to student
model [36]. However, we focus on preserving the struc-
ture of the embedding space of the teacher while distilla-
tion to the student. We thus calculate the pairwise distance
offΩ
pifrom its Knearest neighbours {fΩ
pr1,···, fΩ
prK},
asDΩ
pi={δ(fΩ
pi, fΩ
prj)}K
j=1. Equivalently, we pass pi
U
and its K-nearest neighbours {pr1,···, prK}via the stu-
dent model to obtain corresponding distillation features µϕ
pi
and{µϕ
pr1,···, µϕ
prK}respectively, thus calculating Dϕ
pi=
{δ(µϕ
pi, µϕ
prj)}K
j=1similarly. Although one may calculate a
regression loss between [55] DΩ
piandDϕ
pi, for better stabil-
ity we model them as probability distribution of pairwise-
similarity amongst K-nearest neighbours in the teacher’s
embedding space. As pairwise similarity is negative of pair-
wise distances, we calculate the temperature ( τ) normalised
softmax ( S) [66] probability as Sτ(−DΩ
pi)where,
Sτ(−DΩ
pi)rj=exp(−δ(fΩ
pi, fΩ
prj)/τ)
PrK
k=1exp(−δ(fΩ
pi, fΩ
prk)/τ)(6)
Similarly obtained Sτ(−Dϕ
pi), andSτ(−DΩ
pi)represent the
structural knowledge of embedding spaces of student ϕ(·)
and teacher Ω(·)respectively. The consistency constraint
can therefore be defined as the Kullback-Leibler (KL) di-
vergence [50] between them as,
LpU
KL=KL(Sτ(−DΩ
pi)|| Sτ(−Dϕ
pi)) (7)
5On the other hand, labelled dataset comes with sketch and
photo, where, using the photos we can calculate LpL
KLin a
similar fashion. Given the cross-modal nature of student’s
embedding space [87], we also need to align the sketches
with their paired photos while preserving the existent con-
textual similarity of those photos with their neighbours.
Being trained on photos alone, extracting sketch features
via the teacher Ω(·)to extract sketch-features, would be
a flawed design choice. Considering a sketch-photo pair
(si, pi), we first obtain DΩ
piusing pi. However this time we
usesito calculate Dϕ
si={δ(µϕ
si, µϕ
prj)}K
j=1. Now, calcu-
latingLsL
KLbetween these two resultant probabilities implies
maintaining the same contextual similarity for sketches as
that for their paired photos, as guided by the teacher model.
With λ4,5as respective weighting hyperparameters, the to-
tal distillation loss becomes,
Lϕ
Dist=LpL
KL+λ4LsL
KL+λ5LpU
KL (8)
Summing up, our student model is trained from a weighted
(hyperparameter λ6) combination of two losses as:
Lϕ
trn=Lϕ
Disc+λ6Lϕ
Dist (9)
6. Experiments
Datasets: We use two publicly available datasets, QMUL-
Chair-V2 and QMUL-Shoe-V2 [66,87]. They contain 2000
(400) and 6730 (2000) sketches (photos) respectively with
fine-grained sketch-photo associations. We keep 1275 (300)
and 6051 (1800) sketches (photos) from QMUL-Chair-V2
and QMUL-Shoe-V2 respectively for training while the rest
is used for testing. We also use Sketchy [69] which con-
tains 125 categories with 100 photos each, having at least 5
sketches per photo with fine-grained associations. While,
training uses a standard (90:10) train-test split [69], dur-
ing inference we construct a challenging gallery using pho-
tos across one category for retrieval. Besides such labelled
training data, we use all 50,025 photos of UT-Zap50K [86]
and 7,800 photos [54] collected from shopping websites,
including IKEA, Amazon and Taobao, as unlabelled photos
for shoe and chair retrieval, respectively. For Sketchy we
use its extended version with 60,502 additional photos [45]
introduced later for training from unlabelled data.
Implementation Details: ImageNet [22] pre-trained PVT-
Large [80] model extracts features from 224×224resized
images, keeping patch-size (pl) of 4 at each level and 1, 2,
5, and 8 spatial-reduction-attention heads [80] in 4 succes-
sive levels, with the final feature ( µandf) having size 512.
Implemented via PyTorch [57], our model is trained using
Adam-W optimiser [49] with momentum of 0.9 and weight
decay of 5e-2, batch size of 16, for 200 epochs, on a 11 GB
Nvidia RTX 2080-Ti GPU. Initial learning rate is set to 1e-
3 and decreased as per cosine scheduling [48]. Determined
empirically, mCM,ms
IM,mp
IMandτare set to 0.5, 0.2, 0.3
and 0.01, while λ1→6to 0.8, 0.2, 0.4, 0.4, 0.7 and 0.5 re-
spectively. Following [66] we use Acc.@q, i.e. percentage
of sketches having true matched photo in the top-q list.6.1. Competitors
We compare against: (i) State-of-the-arts (SOTA ):
Triplet-SN [87] trains a Siamese network on cross-modal
triplet loss to learn a discriminative joint sketch-photo em-
bedding space. While HOLEF-SN [75] uses a spatial at-
tention module over Sketch-a-Net [89] backbone, Jigsaw-
SN[54] employs jigsaw-solving pre-training over mixed
patches of photos and edge-maps followed by triplet-based
fine-tuning for better retrieval. Triplet-RL [9] leverages
triplet-loss based pre-training, followed by RL based fine-
tuning for on-the-fly retrieval. We report its results only on
completed sketches as early retrieval is not our goal. Styl-
eVAE [67] meta-learns a V AE-based disentanglement mod-
ule for a style-agnostic retrieval. Following [4] Semi-sup-
SNtrains a sequential photo-to-sketch generation model
that outputs pseudo sketches as labels for unlabelled pho-
tos, to semi-supervise retrieval better.
Table 1. Quantitative comparison of pipelines.
MethodsChair-V2 (%) Shoe-V2 (%)
Top-1 Top-10 Top-1 Top-10SOTATriplet-SN [87] 47.45 84.32 28.71 71.56
HOLEF-SN [75] 50.41 86.31 31.24 74.61
Jigsaw-SN [54] 53.41 87.56 33.51 76.86
OnTheFly [9] 54.54 88.61 34.10 78.82
StyleMeUp [67] 59.86 89.64 36.47 81.83
Semi-sup-SN [4] 60.20 90.81 39.12 85.21Stronger Baseline
SOTA++Triplet-SN-ours 53.48 87.91 33.78 76.84
HOLEF-SN-ours 55.23 88.61 35.41 78.85
Jigsaw-SN-ours 58.51 88.78 37.64 79.78
OnTheFly-ours 59.18 89.35 38.62 81.97
StyleMeUp-ours 65.85 90.84 40.42 82.94
Semi-sup-SN-ours 66.86 91.12 44.35 86.83Backbone VariantsB-ResNet-18 48.42 85.62 26.61 70.31
B-ResNet-50 47.78 82.34 28.12 70.84
B-InceptionV3 55.41 88.21 34.24 78.56
B-VGG-16 58.23 88.78 35.85 80.92
B-VGG-19 61.46 89.16 37.28 81.01
B-ViT 38.71 72.65 16.28 53.42
B-DeIT 56.25 87.72 35.62 79.05
B-SWIN 66.34 91.03 40.71 82.57
B-CvT 68.42 91.21 41.58 83.14
B-CoAtNet 69.68 91.78 42.63 83.20
Ours-Strong 71.22 92.18 44.18 84.68UnlabelledB-Edge-Pretrain 71.58 90.78 44.62 84.85
B-Edge2Sketch 72.16 91.01 45.18 84.92
B-Regress 72.65 91.32 45.45 85.01
B-RKD 73.02 91.78 46.18 85.12
B-PKT 73.45 91.89 46.66 85.47
Ours-Full 74.68 92.79 48.35 85.62
(ii) SOTA-Augmentation (SOTA++ :) To judge how
generic our paradigm is compared to existing frameworks,
we augment the mentioned state-of-the-arts by introducing
our intra-modal triplet objective (Sec. 4) with Exponential
Moving Average in their respective training paradigms.
(iii) Architectural Variants: Using popular CNN architec-
tures like InceptionV3 [76], ResNet-18,50 [34] and VGG-
16,19 [71] as backbone feature extractors, we explore their
potential in FG-SBIR against our strong baseline. Similarly
we also explore a few existing vision transformers as back-
bone feature extractors for our training paradigm, namely,
6B-ViT [26] (ViT-B16 variant), B-DeiT [77] (DEiT-B vari-
ant), B-SWIN [47] (SWIN-B variant), B-CvT [82] (CvT21
variant), and B-CoAtNet [21](CoAtNet3 with 384-dim).
(iv) On training from unlabelled data: Following [4] we
curate a few more baselines that could be used to leverage
unlabelled photos for training. Keeping rest of the paradigm
same, B-Edge-Pretrain [62] naively uses edge-maps of un-
labelled photos to pre-train the retrieval model. Although
edge-maps bear little resemblance to sketches, we similarly
design a baseline B-Edge2Sketch that follows [63] in con-
verting edge-maps of photos to pseudo-sketches as labels
to harness potential of unlabelled photos. As there are no
previous works employing KD for FG-SBIR, we curate a
few baselines offering alternatives on the paradigm of KD.
These methods although repurposed for a cross-modal re-
trieval setting, use our network architecture ( Ω,ϕ) during
training. B-Regress - directly regresses between features
computed by teacher and student for both sketches and im-
ages, after matching corresponding feature dimensions via
additional feature transformation layer, over a l2regression
loss. B-RKD follows [55] to distill the knowledge of rel-
ative pairwise feature-distances of unlabelled photos from
teacher’s embedding space to that of the student, over a
distance-wise distillation loss. Following [56] off-the-
shelf, for unlabelled photos, B-PKT computes the condi-
tional probability density of any two points in teacher’s em-
bedding space [56], which models the probability of any
two samples being close together. Taking ‘N’ such samples,
it obtains a probability distribution over pairwise interac-
tions in that space. Obtaining a similar distribution over the
same samples in student’s embedding space, it minimises
their divergence over a KL-divergence loss.
6.2. Performance Analysis
FG-SBIR pipelines: Tables 1 and 2 compare our meth-
ods against state-of-the-arts and curated baselines. Triplet-
SN[87] and HOLEF-SN [75] score low due to weaker
backbones of Sketch-A-Net [89]. Jigsaw-SN [53] on the
other hand improves performance, owing to self-supervised
Mixed-modal jigsaw solving strategy learning structural in-
formation better. Enhanced by its RL-optimised reward
function OnTheFly [9] surpasses them but fails to ex-
ceed StyleMeUp [67] (2.37% ↑top1 on ShoeV2), thanks
to its complex meta-learned disentanglement module ad-
dressing style-diversity. Unlike others that are restricted to
paired training data, Semi-sup-SN [4] harnesses knowledge
of unlabelled data with its additionally generated pseudo-
sketch labels in achieving comparatively higher perfor-
mance. However, being dependent on the usually unreli-
able quality of generated sketch and the instability of re-
inforcement learning involved, it lags behind our relatively
simpler yet robust knowledge distillation paradigm, boosted
with our better transformer-based feature extractor. Impor-
tantly, when augmenting our ‘strong-baseline’ paradigm toexisting SOTAs, we observe a relative rise in top-1 perfor-
mance of all methods by ≈4−6%overall in SOTA++ sec-
tion (Table 1). Despite costing a minimal memory overhead
(training only), these objectives reward a considerably high
accuracy boost which verifies our method to be an easy-fit
and quite generic to serve as a strong FG-SBIR baseline.
Table 2. Quantitative comparison of pipelines on Sketchy [69].
MethodsSketchy (%)MethodsSketchy (%)
Top-1 Top-5 Top-1 Top-5
Triplet-SN [87] 15.32 34.15 B-InceptionV3 28.71 71.56
HOLEF-SN [75] 16.71 35.92 B-VGG-16 18.84 38.63
Jigsaw-SN [54] 16.74 36.37 B-ViT 7.63 11.23
OnTheFly [9] 04.76 07.81 B-SWIN 32.14 57.68
StyleMeUp [67] 19.62 39.72 B-CoAtNet 33.63 59.31
Triplet-SN-ours 19.48 37.91 B-Edge-Pretrain 34.98 61.32
HOLEF-SN-ours 20.23 38.61 B-Edge2Sketch 35.81 61.74
Jigsaw-SN-ours 21.45 39.56 B-Regress 36.33 62.31
OnTheFly-ours 07.28 12.14 B-RKD 37.02 63.02
StyleMeUp-ours 22.95 45.84 B-PKT 38.62 63.94
Ours-Strong 34.72 65.10 Ours-Full 38.54 71.52
Backbone architectures: Comparing efficiency of CNNs
as backbone feature extractors ( B-CNNs ) for FG-SBIR , we
find B-VGG19 to perform best, being slightly better than
B-VGG16 (by 3.23%) at an extra memory cost (21mb), and
much better than B-ResNet18 (by 10.67%) mainly due to the
latter’s thinner conv-layers in Top-1 accuracy on ShoeV2.
Among transformers, B-CoAtNet, B-CvT andB-SWIN per-
form much better than B-ViT orB-DeIT thanks to their
heirarchical design and mechanisms like shifting window or
convolutional tokens capturing fine-grained sparse details
of a sketch via local patch-level interaction, thus represent-
ing sketches better unlike other two. Surpassing all others,
with its unique pyramidal structure imbibing inductive bias,
PVT [80] ( Ours-Strong,Full ) fits optimally to our method.
Training from Unlabelled Data: While B-Edge-Pretrain
offers little gain over our stronger baseline ( Ours-Strong ),
augmenting edge-maps with B-Edge2Sketch by a selected
subset of strokes, to imbibe the abstractness of sketch, in-
creases accuracy reasonably. Lower scores of B-Regress is
largely due to its metric-based regression across two differ-
ent embedding spaces causing misalignment, whereas at-
tending to one pair at a time, B-RKD [55] fails to preserve
the structural knowledge of the embedding space, thus scor-
ing below B-PKT (by 0.48 top1 on ShoeV2). Although
B-PKT [56] is slightly similar to our paradigm of preserv-
ing the latent space, lacking cross-modal discrimination ob-
jective [87] and therefore being trained on unlabelled data
alone, performs lower than our method which additionally
leverages the potential of labelled sketch-photo pairs.
6.3. Ablation Study
Importance of loss objectives: To justify each loss in our
network of a stronger baseline, we evaluate them in a strip-
down fashion (Table 3). Performance against cross-modal
objective alone (Type-I), increase on aid from intra-modal
objectives (Type-II), as the latter encourages distancing of
7multiple photos visually close to the sketch, based on finer
intra-modal relative discrimination. Adding EMA (Ours)
optimisation [84] ensures smoother convergence of above
objectives with a slight increase (by 2.96%) in accuracy. In
similar spirit we perform experiments on losses of distilla-
tion paradigm (Eqn. 8). Performance using unlabelled data
only (Type-III) rises with added knowledge from labelled
photos (Type-IV). However, without aligning sketches us-
ingLsL
KLit lags behind Ours-Full by 1.14%.
Table 3. Ablative study on QMUL-ShoeV2
Type LCM
TriLIM
TriEMA LpU
KLLpL
KLLsL
KLTop-1 (%)
I ✓ - - ✓ ✓ ✓ 43.28
II ✓ ✓ -✓ ✓ ✓ 45.39
III ✓ ✓ ✓ ✓ - - 46.50
IV ✓ ✓ ✓ ✓ ✓ - 47.21
Ours-Full ✓ ✓ ✓ ✓ ✓ ✓ 48.35
Augmentation Strategy: We design a few experiments to
explore other photo augmentation techniques [1] for LIMp
Tri
(Eqn. 2) like, colour distortion, partial blurring and random
shift in sharpness. Outperforming their respective scores of
46.14%, 47.32% and 47.54% (Acc@1-ShoeV2) our meth-
ods confirms our intuition that morphological augmenta-
tions would better direct the loss objective, as it instills dis-
crimination based on information exclusive to a sketch.
Influence of Distillation Token ( ∆):To justify its impor-
tance we explore two more designs. (A) Without any ex-
plicit distillation token we perform knowledge distillation
using the same feature used for discriminative learning. (B)
Following [77] we append a learnable token to the input
of the last stage, which after processing acts as the distilla-
tion feature. Although case-B ( 45.31%) surpasses case-A
(44.71%) by1.6%in Acc@1 on QMUL-ShoeV2, confirm-
ing the need of a dedicated distillation token, it lags behind
by3.04% to ours. We argue that our design of engaging
the distillation token at every level via residual connection
instills the inductive bias modelled by the pyramidal down-
scaling, into the distillation token via transformer layers,
thus creating a better representation.
Further Analysis: (i) Although ours is a larger model
than the earlier SOTA of Triplet-SN [87], it offers ≈20-25%
gain while taking similar inference time (0.37ms/ShoeV2).
(ii) Our method takes 44.1 ms per training step compared
56.83 ms of SOTA StyleMeUP [67] , proving itself as a
strong and efficient method. (iii) Although our method
takes more hyperparameters than the simpler Triplet-SN
[87] SOTA, they are quick to tune, and justifies itself with a
boost of ≈25% Acc@1 on Chair-V2 [87].
6.4. Multi-category FG-SBIR via Unlabelled Photos
Our pilot study (Sec. 3) reveals FG-SBIR models to per-
form poorly on classes lacking sketch-photo pairs. This
has been explored in a few recent FG-SBIR works. Apart
from Jigsaw-SN [54] (Sec. 6.1), ( CC-Gen ) [53] takes across-category (CC) domain-generalisation approach, mod-
elling a universal manifold of prototypical visual sketch
traits that dynamically embeds sketch and photo, to gen-
eralise on unseen categories. Recently, [8] uses a dedi-
cated meta-learning framework that adapts a trained model
to new classes using a few corresponding sketch-photo pairs
as support. However, both methods have access to sufficient
[54] or few [8] sketch-photo pairs of novel classes, unlike
our setup of no paired sketches . This goes beyond the de-
pendency on paired sketches offering a realistic mid-ground
between standard [87] and zero-shot [25] inference setup,
where not all classes have paired sketches, but their photos
can be collected easily . Consequently, following [85] we
split Sketchy [69] as 21 unseen-test classes, and 104 train-
ing classes with a 70:30 training:validation split in the lat-
ter. Unlike existing methods, our student additionally trains
via distillation from photos of 21 unseen classes for better
retrieval accuracy. For retrieval a separate gallery is main-
tained per category, and average top-q accuracy across 21
classes is reported (Table 4). We compare with a few meth-
ods that leverage data of novel classes, besides labelled data
for training: Jigsaw-SN (extending [54]) trains via an aux-
iliary jigsaw-solving task on unlabelled photos without any
fine-tuning (our setup). Adaptive-SN [8] trains on a few
sketch-photo pairs of new classes. B-EdgePretrain andB-
Edge2Sketch trains on edge-maps and synthetic sketches
via [63] from edge-maps of unlabelled photos respectively.
CC-Gen [53] evaluates in zero-shot [25] setup with no data
from novel classes. Consequently CC-gen scores lower (Ta-
ble 4), compared to Jigsaw-SN with its auxiliary jigsaw-
solving task, whereas baselines using edge-maps of unla-
belled images as pseudo -sketches score relatively better.
Even without using real sketch-photo pairs of test-set for
quick-adaptation like Adaptive-SN [8], we achieve a com-
petitive retrieval accuracy (2.47% Acc@1).
Table 4. Cross-category FG-SBIR on Sketchy [69].
MethodsSketchy (%)MethodsSketchy (%)
Top-1 Top-5 Top-1 Top-5
Jigsaw-SN [54] 23.16 44.63 B-Edge-Pretrain 24.81 46.24
Adaptive-SN [8] 32.71 53.42 B-Edge2Sketch 25.74 48.36
CC-Gen [53] 22.73 42.32 Ours-Full 30.24 51.65
7. Conclusion
In this paper we put forth a strong baseline for FG-SBIR
with PVT [80]-backbone, and offer a novel paradigm that at
its core aims at learning from unlabelled data in FG-SBIR
by distilling knowledge from unlabelled photos. While our
proposed intra-modal triplet loss increases feature separa-
tion in model’s latent space, an EMA paradigm stabilises its
training. Importantly, we for the first time introduce a dis-
tillation token in PVT architecture that explicitly caters to
knowledge distillation. Extensive experiments against ex-
isting frameworks and various baselines show our method
to outperform them, thus proving its significance.
8References
[1] Olusola Oluwakemi Abayomi-Alli, Robertas Dama ˇseviˇcius,
Sanjay Misra, and Rytis Maskeli ¯unas. Cassava disease
recognition from low-quality images using enhanced data
augmentation model and deep learning. Expert Systems ,
2021. 8
[2] Jimmy Ba and Rich Caruana. Do deep nets really need to be
deep? NeurIPS , 2014. 3, 5
[3] Hessam Bagherinezhad, Maxwell Horton, Mohammad
Rastegari, and Ali Farhadi. Label refinery: Improving ima-
genet classification through label progression. arXiv preprint
arXiv:1805.02641 , 2018. 3
[4] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan
Sain, Yongxin Yang, Tao Xiang, and Yi-Zhe Song. More
photos are all you need: Semi-supervised learning for fine-
grained sketch based image retrieval. In CVPR , 2021. 2, 3,
5, 6, 7, 12
[5] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin
Yang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song.
Vectorization and rasterization: Self-supervised learning for
sketch and handwriting. In CVPR , 2021. 2, 3
[6] Ayan Kumar Bhunia, Subhadeep Koley, Abdullah Faiz
Ur Rahman Khilji, Aneeshan Sain, Pinaki Nath Chowdhury,
Tao Xiang, and Yi-Zhe Song. Sketching without worrying:
Noise-tolerant sketch-based image retrieval. In CVPR , 2022.
2
[7] Ayan Kumar Bhunia, Subhadeep Koley, Amandeep Kumar,
Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-
Zhe Song. Sketch2Saliency: Learning to Detect Salient Ob-
jects from Human Drawings. In CVPR , 2023. 1
[8] Ayan Kumar Bhunia, Aneeshan Sain, Parth Shah, Animesh
Gupta, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe
Song. Adaptive fine-grained sketch-based image retrieval.
InECCV , 2022. 3, 8
[9] Ayan Kumar Bhunia, Yongxin Yang, Timothy M
Hospedales, Tao Xiang, and Yi-Zhe Song. Sketch less
for more: On-the-fly fine-grained sketch-based image
retrieval. In CVPR , 2020. 2, 6, 7, 12
[10] Tu Bui, Leonardo Ribeiro, Moacir Ponti, and John Collo-
mosse. Deep manifold alignment for mid-grain sketch based
image retrieval. In ACCV , 2018. 1, 12
[11] Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Char-
less Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential
moving average normalization for self-supervised and semi-
supervised learning. In CVPR , 2021. 4
[12] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In ECCV , 2020. 2
[13] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In ECCV , 2018. 3
[14] Caroline Chan, Fr ´edo Durand, and Phillip Isola. Learning to
generate line drawings that convey geometry and semantics.
InCVPR , 2022. 12
[15] Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Ro-
han Ramanath. An attentive survey of attention models.
ACM TIST , 2019. 2, 12[16] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In ICML , 2020. 3
[17] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan
Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song.
SceneTrilogy: On Human Scene-Sketch and its Complemen-
tarity with Photo and Text. In CVPR , 2023. 1, 2
[18] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan
Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. What
Can Human Sketches Do for Object Detection? In CVPR ,
2023. 1
[19] John Collomosse, Tu Bui, and Hailin Jin. Livesketch: Query
perturbations for guided sketch-based visual search. In
CVPR , 2019. 12
[20] John P. Collomosse, Tu Bui, and Hailin Jin. Livesketch:
Query perturbations for guided sketch-based visual search.
InCVPR , 2019. 1
[21] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.
Coatnet: Marrying convolution and attention for all data
sizes. NeurIPS , 2021. 7
[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 6
[23] Jiajun Deng, Yingwei Pan, Ting Yao, Zhou Wengang, Li
Houqiang, and Tao Mei. Relation distillation networks for
video object detection. In ICCV , 2019. 3
[24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. In NAACL , 2018. 2
[25] Sounak Dey, Pau Riba, Anjan Dutta, Josep Llados, and Yi-
Zhe Song. Doodle to search: Practical zero-shot sketch-
based image retrieval. In CVPR , 2019. 1, 8, 12
[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2021. 2,
7
[27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021. 2
[28] Anjan Dutta and Zeynep Akata. Semantically tied paired
cycle consistency for zero-shot sketch-based image retrieval.
InCVPR , 2019. 1, 12
[29] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NeurIPS ,
2014. 2
[30] Yves Grandvalet and Yoshua Bengio. Semi-supervised
learning by entropy minimization. NeurIPS , 2004. 3
[31] Rick Groenendijk, Sezer Karaoglu, Theo Gevers, and
Thomas Mensink. Multi-loss weighting with coefficient of
variations. In WACV , 2021. 12
[32] David Ha and Douglas Eck. A neural representation of
sketch drawings. In ICLR , 2018. 2
9[33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In CVPR , 2020. 3
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 6
[35] Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming
Sun, and Youliang Yan. Knowledge adaptation for efficient
semantic segmentation. In CVPR , 2019. 3
[36] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the
knowledge in a neural network. In NeurIPS Deep Learning
Workshop , 2014. 2, 5
[37] Fei Huang, Yong Cheng, Cheng Jin, Yuejie Zhang, and Tao
Zhang. Deep multimodal embedding model for fine-grained
sketch-based image retrieval. In ACM SIGIR , 2017. 2
[38] Ruyi Ji, Longyin Wen, Libo Zhang, Dawei Du, Yanjun Wu,
Chen Zhao, Xianglong Liu, and Feiyue Huang. Attention
convolutional binary neural tree for fine-grained visual cate-
gorization. In CVPR , 2020. 2
[39] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:
Two pure transformers can make one strong gan, and that can
scale up. In NeurIPS , 2021. 2
[40] L. Jing and Y . Tian. Self-supervised visual feature learning
with deep neural networks: A survey. IEEE TPAMI , 2020. 2
[41] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. In ICLR , 2014. 2
[42] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain,
Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Pic-
ture that Sketch: Photorealistic Image Generation from Ab-
stract Sketches. In CVPR , 2023. 1
[43] Dong-Hyun Lee. Pseudo-label: The simple and efficient
semi-supervised learning method for deep neural networks.
InICML , 2013. 3
[44] Hangyu Lin, Yanwei Fu, Peng Lu, Shaogang Gong, Xi-
angyang Xue, and Yu-Gang Jiang. Tc-net for isbir: Triplet
classification network for instance-level sketch based image
retrieval. In ACM MM , 2019. 13
[45] Li Liu, Fumin Shen, Yuming Shen, Xianglong Liu, and Ling
Shao. Deep sketch hashing: Fast free-hand sketch-based im-
age retrieval. In CVPR , 2017. 6, 12
[46] Xinda Liu, Lili Wang, and Xiaoguang Han. Transformer
with peak suppression and knowledge guidance for fine-
grained image recognition. Neurocomputing , 2022. 2
[47] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 2, 7
[48] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. In ICLR , 2017. 6
[49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR , 2019. 6
[50] Alexander G de G Matthews, James Hensman, Richard
Turner, and Zoubin Ghahramani. On sparse variational meth-
ods and the kullback-leibler divergence between stochastic
processes. In AISTATS , 2016. 5
[51] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and
Shin Ishii. Virtual adversarial training: a regularizationmethod for supervised and semi-supervised learning. IEEE
TPAMI , 2018. 3
[52] Rafael M ¨uller, Simon Kornblith, and Geoffrey E Hinton.
When does label smoothing help? In NeurIPS , 2019. 3
[53] Kaiyue Pang, Ke Li, Yongxin Yang, Honggang Zhang, Tim-
othy M Hospedales, Tao Xiang, and Yi-Zhe Song. Gener-
alising fine-grained sketch-based image retrieval. In CVPR ,
2019. 2, 7, 8
[54] Kaiyue Pang, Yongxin Yang, Timothy M Hospedales, Tao
Xiang, and Yi-Zhe Song. Solving mixed-modal jigsaw puz-
zle for fine-grained sketch-based image retrieval. In CVPR ,
2020. 2, 3, 6, 7, 8, 12
[55] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Rela-
tional knowledge distillation. In CVPR , 2019. 5, 7
[56] Nikolaos Passalis and Anastasios Tefas. Learning deep rep-
resentations with probabilistic knowledge transfer. In ECCV ,
2018. 7
[57] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in PyTorch. In NeurIPS , 2017. 6
[58] Yash Patel, Giorgos Tolias, and Ji ˇr´ı Matas. Recall@ k surro-
gate loss with large batches and similarity mixup. In CVPR ,
2022. 13
[59] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR , 2016. 3
[60] Mary Phuong and Christoph H Lampert. Distillation-based
training for multi-exit architectures. In ICCV , 2019. 3
[61] Andrea Pilzer, St ´ephane Lathuili `ere, Nicu Sebe, and Ricci
Elisa. Refine and distill: Exploiting cycle-inconsistency and
knowledge distillation for unsupervised monocular depth es-
timation. In CVPR , 2019. 3
[62] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Deep
shape matching. In ECCV , 2018. 7
[63] Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao
Xiang, and Timothy M Hospedales. Learning deep sketch
abstraction. In CVPR , 2018. 7, 8
[64] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. ICLR , 2015. 3, 5
[65] Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowd-
hury, Aneeshan Sain, Subhadeep Koley, Tao Xiang, and Yi-
Zhe Song. CLIP for All Things Zero-Shot Sketch-Based Im-
age Retrieval, Fine-Grained or Not. In CVPR , 2023. 1
[66] Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xi-
ang, and Yi-Zhe Song. Cross-modal hierarchical modelling
for fine-grained sketch based image retrieval. In BMVC ,
2020. 2, 3, 5, 6, 12
[67] Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xi-
ang, and Yi-Zhe Song. Stylemeup: Towards style-agnostic
sketch-based image retrieval. In CVPR , 2021. 2, 6, 7, 8
[68] Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and
Moacir Ponti. Sketchformer: Transformer-based representa-
tion for sketched structure. In CVPR , 2020. 2
[69] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James
Hays. The sketchy database: learning to retrieve badly drawn
bunnies. ACM TOG , 2016. 3, 6, 7, 8
10[70] Muhamad Risqi U Saputra, Pedro PB De Gusmao, Yasin Al-
malioglu, Andrew Markham, and Niki Trigoni. Distilling
knowledge from a deep pose regressor network. In ICCV ,
2019. 5
[71] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR ,
2015. 2, 6
[72] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao
Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin,
Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-
supervised learning with consistency and confidence. In
NeurIPS , 2020. 3
[73] Jifei Song, Yi-Zhe Song, Tony Xiang, and Timothy M
Hospedales. Fine-grained image retrieval: the text/sketch
input dilemma. In BMVC , 2017. 2
[74] Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales,
and Xiang Ruan. Deep multi-task attribute-driven ranking
for fine-grained sketch-based image retrieval. In BMVC ,
2016. 2
[75] Jifei Song, Qian Yu, Yi-Zhe Song, Tao Xiang, and Timo-
thy M Hospedales. Deep spatial-semantic attention for fine-
grained sketch-based image retrieval. In ICCV , 2017. 1, 2,
3, 6, 7, 12, 13
[76] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision. In CVPR , 2016.
6
[77] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In ICML , 2021. 2, 5, 7, 8
[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 2
[79] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-
man Christian Bachmann, Amit Haim Bermano, Daniel
Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso:
Semantically-aware object sketching. ACM Transactions on
Graphics (TOG) , 2022. 12
[80] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-
mid vision transformer: A versatile backbone for dense pre-
diction without convolutions. In ICCV , 2021. 1, 2, 4, 5, 6, 7,
8, 13
[81] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pvtv2: Improved baselines with pyramid vision transformer.
Computational Visual Media , 2022. 2
[82] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-
volutions to vision transformers. In ICCV , 2021. 2, 7
[83] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-
ing Guo. Learning texture transformer network for image
super-resolution. In CVPR , 2020. 2
[84] Yasin Yaz, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap,
Georgios Piliouras, Vijay Chandrasekhar, et al. The unusual
effectiveness of averaging in gan training. In ICLR , 2019. 2,
4, 8[85] Sasi Kiran Yelamarthi, Shiva Krishna Reddy, Ashish Mishra,
and Anurag Mittal. A zero-shot framework for sketch based
image retrieval. In ECCV , 2018. 8
[86] Aron Yu and Kristen Grauman. Fine-grained visual compar-
isons with local learning. In CVPR , 2014. 6, 12
[87] Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M
Hospedales, and Chen-Change Loy. Sketch me that shoe. In
CVPR , 2016. 1, 2, 3, 4, 6, 7, 8, 12, 13
[88] Qian Yu, Jifei Song, Yi-Zhe Song, Tao Xiang, and Timo-
thy M Hospedales. Fine-grained instance-level sketch-based
image retrieval. IJCV , 2021. 1
[89] Qian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang,
and Timothy M Hospedales. Sketch-a-net: A deep neural
network that beats humans. IJCV , 2017. 6, 7
[90] Sergey Zagoruyko and Nikos Komodakis. Paying more at-
tention to attention: Improving the performance of convolu-
tional neural networks via attention transfer. In ICLR , 2017.
3
[91] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In ECCV , 2016. 3
11Supplementary material for
Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR
Aneeshan Sain1,2Ayan Kumar Bhunia1Subhadeep Koley1,2Pinaki Nath Chowdhury1,2
Soumitri Chattopadhyay *Tao Xiang1,2Yi-Zhe Song1,2
1SketchX, CVSSP, University of Surrey, United Kingdom.
2iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.
{a.sain, a.bhunia, p.chowdhury, t.xiang, y.song }@surrey.ac.uk
A. Alternative to Triplet Loss
Factually, for photos, intra-modal triplet loss is a self-supervised objective. For a cross-modal problem such as ours
however, triplet loss can offer some leeway in better conditioning the joint photo-sketch embedding [9, 75, 87]. Empirically,
when compared with contrastive loss [15] as a self-supervised objective (while keeping everything else the same), triplet loss
performs better (45.68% on ShoeV2) further justifying our case.
B. On the need for Knowledge Distillation
During student-training, three objectives are learnt – cross-modal separation, intra-modal separation between photos and
that between sketches. As learning everything together is difficult, we decouple the process by first training a teacher com-
pletely with intra-modal triplet loss on photos, and then use the trained teacher’s photo discrimination knowledge to better
guide the student (FG-SBIR) during its training.
C. On using additional datasets
TU-Berlin and QuickDraw are sketch-only datasets designed towards sketch classification. Some [19, 25] did augment
them for category-level SBIR [10,28], by sourcing unpaired photos. These however do not work for our instance-level setting
- we need instance-level sketch-photo correspondences. The idea of abstraction-influence is very interesting, which shall be
considered as a future work.
D. Dealing with scarcity of sketch-data
Distilling from unlabelled photos is beneficial as they are abundantly available, unlike sketches that require time and
human effort to collect [4]. On distilling from only sketches there is minimal increment from teacher supervision (44.51%
vs. 44.18 % on ShoeV2) as compared to that from photos (48.35% vs. 44.18% on ShoeV2). Faithful sketch-generation in
photo-to-sketch generation tasks is challenging; it’s difficult to quantify its generation-quality, and they hardly generalise to
human-sketches [4]. Using CLIPasso [79] for sketch-generation instead of teacher-supervision, hence delivers a poor result
of 38.57% on ShoeV2. Although works have explored augmenting sketches via stroke-dropping/deformations [87], or as
line-drawings [14], resulting sketches mostly follow edge-maps thus being less reliable. On using [14] instead of teacher-
supervision, we obtained a poorer result of 39.23% compared to our 48.35% on ShoeV2, thus proving our method to be
simpler and more efficient.
E. Clarity on training teacher
The teacher comprises an ImageNet pre-trained PVT backbone trained on 60,502 additional photos from Sketchy (ext) [45]
for Sketchy; 50,025 photos of UT-Zap50k [86] for ShoeV2 [87]; and 7,800 photos from websites like IKEA, etc [54] for
ChairV2 [66].
F. Optimisation for multi-task objectives
We used the available toolbox of WandB Sweeps for quick tuning of hyper-parameters, which provided 48.35% Acc@1
on ShoeV2. Even on using complex loss balancing approach of [31], we obtain a close 47.94%. Furthermore, changing the
*Interned with SketchX
12hyper-parameter values by ±10%, causes a mere ±0.5%change in Acc@1 on ShoeV2 [87]. This proves that our method
despite needing five loss objectives, is quick to tune, thus being easily reproducible.
G. Clarity on training stability
•Learning rate decay: We used exponential rate decay with initial learning rate of 0.001 and decay factor of 0.2.
•Large batch-size: We used 256 batch-size via gradient accumulation [58] on 4 V100-GPU machines.
•Reducing augmentations: We used augmentation (random horizontal flipping only) on just 30% of training data.
Plots below show the above methods’ implementation on top of CNN-Baseline. Least gittering in Ours shows our EMA
approach to be superior.
Evaluation accuracy at every 100thtraining-step [Best if zoomed].
H. Further clarity on experimental results
As we intended to show PVT [80] is a better backbone than the earlier CNN-based ones, we compared prior state-of-arts
to our method using different backbones. Furthermore, for the methods having code available, we replaced their backbones
with PVT, only to obtain inferior results (32.68% for [87] and 34.12% for [75]), thus proving ours as better. Furthermore, our
method surpasses by 8.33% on ShoeV2, against a contemporary method of TC-Net [44], despite having a lesser complexity
of training and simpler loss objectives than the latter.
13