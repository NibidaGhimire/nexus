LineFormer: Rethinking Line Chart Data
Extraction as Instance Segmentation
Jay Lalâˆ—, Aditya Mitkariâ€ , Mahesh Bhosaleâ€ , and David Doermann
Articial Intelligence Innovation Lab (A2IL),
Department of Computer Science and Engineering,
University at Bualo, Bualo, NY, USA
fjayashok, amitkari, mbhosale, doermann g@buffalo.edu
Abstract. Data extraction from line-chart images is an essential com-
ponent of the automated document understanding process, as line charts
are a ubiquitous data visualization format. However, the amount of visual
and structural variations in multi-line graphs makes them particularly
challenging for automated parsing. Existing works, however, are not ro-
bust to all these variations, either taking an all-chart unied approach or
relying on auxiliary information such as legends for line data extraction.
In this work, we propose LineFormer, a robust approach to line data ex-
traction using instance segmentation. We achieve state-of-the-art perfor-
mance on several benchmark synthetic and real chart datasets. Our im-
plementation is available at https://github.com/TheJaeLal/LineFormer.
Keywords: Line Charts, Chart Data Extraction, Chart OCR, Instance
Segmentation
1 Introduction
Automated parsing of chart images has been of interest to the research com-
munity for some time now. Some of the downstream applications include the
generation of new visualizations [20], textual summarization [10], retrieval [26]
and visual question answering [19]. Although the related text and image recogni-
tion elds have matured, research on understanding gures, such as charts, is still
developing. Most charts, such as bar, pie, lines, scatter, etc., typically contain
textual elements (title, axis labels, legend labels) and graphical elements (axis,
tick marks, graph/plot). Like OCR is often a precursor to any document under-
standing task, data extraction from these chart images is a critical component
of the gure analysis process.
For this study, we have chosen to focus on the analysis of line charts. Despite
their widespread use, line charts are among the most dicult to parse accurately
(see Figs. 4,5). A chart data extraction pipeline typically involves many auxiliary
tasks, such as classifying the chart type and detecting text, axes, and legend
âˆ—Corresponding Author
â€ Joint Second AuthorshiparXiv:2305.01837v1  [cs.CV]  3 May 20232 J. Lal et al.
components before data extraction (see Fig. 1). Here, we concentrate on the
nal task of data series extraction, that is, given a chart image, we output an
ordered set of points x,y (image coordinates), representing the line data points.
Our solution can be easily extended to recover the tabular data originally used
to generate the chart gure by mapping the extracted series to given axis units.
Fig. 1: Data Extraction
Recent works[17,18] show that box and point detectors have achieved reason-
able accuracy in extracting data from bar, scatter, and pie charts, as they often
have fewer structural or visual variations. However, line charts pose additional
challenges because of the following.
{Plethora of visual variations in terms of line styles, markers, colors, thickness,
and presence of background grid lines.
{Structural complexities, such as crossings, occlusions, and crowding. (Fig.
[2])
{Absence of semantic information, such as legends, to dierentiate lines with
a similar style or color.
Although some recent works propose specialized solutions for lines, they still
fail to address some of the above problems (see Fig. 4, 5). In this work, we
propose a robust method for extracting data from line charts by formulating it as
an instance segmentation problem. Our main contributions can be summarized
as follows.
{We highlight the unique challenges in extracting data from multi-line charts.
{We propose LineFormer, a simple, yet robust approach that addresses the
challenges above by reformulating the problem as instance segmentation.LineFormer 3
{We demonstrate the eectiveness of our system by evaluating benchmark
datasets. We show that it achieves state-of-the-art results, especially on real
chart datasets, and signicantly improves previous work.
We also open-source our implementation for reproducibility.
2 Related Work
2.1 Chart Analysis
Analyzing data from chart images has been approached from multiple perspec-
tives. Chart-to-text [14] tried to generate natural language summaries of charts
using transformer models. Direct Visual Question Answering (VQA) over charts
has also been another popular area of research [19]. Many high-level understand-
ing tasks can be performed more accurately if the corresponding tabular data
from the chart have been extracted, making the extraction of chart data an
essential component for chart analysis.
2.2 Line Data Extraction
Traditional Approaches Earlier work on data extraction from line charts used
image-processing-based techniques and heuristics. These include color-based seg-
mentation [23], [27], or tracing using connecting line segments [13] or pixels [22].
Patch-based methods capable of recognizing dashed lines have also been used [4].
These methods work well for simple cases; however, their performance deterio-
rates as the number of lines increases, adding more visual and structural varia-
tions.
Keypoint Based More recently, deep learning-based methods for extracting
chart data have gained popularity. Most of these approaches attempt to de-
tect lines through a set of predicted keypoints and then group those belonging
to the same line to dierentiate for multi-line graphs. ChartOCR[17] used a
CornerNet-based keypoint prediction model and push-pull loss to train keypoint
embeddings for grouping. Lenovo[18] used an FPN-based segmentation network
to detect line keypoints. Recently, LineEX[25] extended ChartOCR by replac-
ing CornerNet with Vision Transformer and forming lines by matching keypoint
image patches with legend patches. A common problem with these keypoint-
oriented approaches is that predicted points do not align with the ground-truth
line precisely. Furthermore, robust keypoint association requires aggregating con-
textual information that is dicult to obtain with only low-level features or push-
pull-based embeddings. Some segmentation-based tracking approaches have also
been proposed, treating the line as a graph with line pixels as nodes and con-
nections as edges [15,28]. Lines are traced by establishing the presence of these
edges using low-level features such as color, style, etc., or by matching with the
legend to calculate the connection cost. The cost is minimized using Dynamic4 J. Lal et al.
Programming[28] or Linear Programming[15] to obtain the nal line tracking
results.
We refer the reader to [6,7,8] for a more detailed chart comprehension and
analysis survey.
2.3 Instance Segmentation
The goal of instance segmentation is to output a pixel-wise mask for each class in-
stance. Earlier approaches, for instance segmentation such as MaskRCNN [11],
were derived from top-down object detection techniques. However, when seg-
menting objects with complex shapes and overlapping instances, bottom-up ap-
proaches have performed better [9]. Bottom-up approaches, for instance segmen-
tation, extend semantic segmentation networks by using a discriminative loss [9]
to cluster pixels belonging to the same instance. More recently, transformer-
based architectures have achieved state-of-the-art results on instance segmenta-
tion tasks [29,3]. These architectures are typically trained using a set prediction
objective and output a xed set of instance masks by processing object queries.
3 Motivation
3.1 Line Extraction Challenges
As mentioned, line charts, especially multiline graphs, suer from several struc-
tural complexities. We identify the three most common structural patterns:
crossings, occlusions, and crowding. Fig. 2 shows their examples.
In each of these cases, the visual attributes of the line segments, such as
color, style, and markers, are either obscured or blended with adjacent lines.
Thus, approaches that primarily rely on low-level image features, such as color,
gradients, texture, etc., often fall short. Furthermore, most keypoint-based line
extractors [17,25] tend to suer from two common issues: a.) The inability to
predict distinct keypoint for each line at crossings and occlusions. b.) The sub-
sequent keypoint grouping step suers as it attempts to extract features from
an already occluded local image patch. (See Figs. 4,5).
Recent work [15] attempts to address occlusion by modeling an explicit
optimization constraint. However, the tracking remains dependent on low-level
features and proximity heuristics, which could limit its robustness. It is worth
noting that, in such cases, humans can gather contextual information from sur-
rounding areas and ll the gaps.
3.2 Line As Pixel Mask
Due to the shortcomings in the keypoint-based method (Section3.1), we adopt
a dierent approach inspired by the task of lane detection in autonomous driv-
ing systems. In this context, instance segmentation techniques have eectively
detected dierent lanes from road images [24]. Similarly to lanes, we considerLineFormer 5
Fig. 2: Line - Structural Complexity Patterns
every line in the chart as a distinct instance of the class `line' and predict a
binary pixel mask for each such instance. Thus, rather than treating a line as
a set of arbitrary keypoints, we treat it as a group of pixels and demonstrate
that per-pixel line instance segmentation is a more robust learning strategy than
keypoint detection for line charts. Moreover, this approach does not require an
explicit keypoint-grouping mechanism since all pixels (or points) segmented in
a mask belong to the same line.
4 Approach
To address the problem of occlusion and crowding, we adopt an instance seg-
mentation model based on the encoder-decoder transformer [3], which uses a
masked-attention pixel decoder capable of providing the visual context neces-
sary to detect occluded lines. Our proposed LineFormer system (Fig.3) is simple
and consists of two modules, Mask Prediction and Data Series Extraction.
4.1 Line Mask Prediction
We adopt a state-of-the-art universal segmentation architecture to predict line
instances from charts [3]. The input line chart image IHWis processed by a
transformer model, an encoder-decoder network. The encoder extracts multi-
level features ffrom the input image I, followed by a pixel decoder that up-
samples the extracted features fand outputs a 2D per pixel embedding PCHW.6 J. Lal et al.
Fig. 3: LineFormer System
A transformer decoder attends to the intermediate layer outputs of the pixel
decoder and predicts a mask embedding vector MCN, whereNis equal to the
number of line queries. Finally, the dot product of PandMgives an independent
per-pixel mask EHWN.
4.2 Data Series Extraction
Once the predicted line masks Efor an input image are obtained, we nd the
start and end x-values of the line xrange = [xstart,xend]. Withinxrange , we sam-
ple foreground points ( xi,yi) at a regular interval x. The smaller the interval,
x, the more precisely the line can be reconstructed. We used linear interpolation
on the initial sampled points to address any gaps or breaks in the predicted line
masks. The extracted ( x,y) points can be scaled to obtain the corresponding
data values if the information on the ground truth axis is available.
5 Implementation Details
We adopt the architecture and hyperparameter settings from [3]. For the back-
bone, we use SwinTransformer-tiny [16] to balance accuracy and inference speed.
To ensure a manageable number of line predictions, we set the number of line
queries to 100. As the transformer decoder is trained end-to-end on a set pre-
diction objective, the loss is a linear combination of classication loss and maskLineFormer 7
prediction loss, with weights 1 and 5, respectively. The latter is the sum of
dice loss [21] and cross-entropy loss. All experiments are performed using the
MMDetection [2] framework based on PyTorch.
5.1 Generating Ground Truth Line Masks
Training LineFormer requires instance masks as labels. Thus, for a chart image,
we generate a separate Boolean ground-truth mask for each line instance. As
mentioned earlier, these line masks may have overlapping pixels that reect the
natural structure in the corresponding chart images. Since all datasets for line
data extraction provide annotations in the form of x,y points, we need to convert
these points into pixel line masks. For this, we use the Bresenham algorithm [1]
from computer graphics to generate continuous line masks similar to how the
lines would have been rendered in the corresponding plot image. By doing this,
we can maintain a 1-1 pixel correspondence between the input image and the
output pixel, making the prediction task easier and stabilizing the training.
We x the thickness of lines in ground-truth masks to three pixels, which is
empirically found to work best for all the datasets.
6 Experiments
6.1 Baselines
To test our hypothesis, we performed multiple experiments on several real and
synthetic chart datasets. We compare LineFormer with keypoint-based approaches
as baselines, including line-specic LineEX [25] and unied approaches Char-
tOCR [17] and Lenovo [18]. We take their reported results for the latter, as they
report on the same CHART-Info challenge[7] metrics that we use. For Char-
tOCR [17] and LineEX [25], we use their publicly available pre-trained models
to evaluate on the same competition metrics. We cannot compare with the Lin-
ear Programming method [15] due to the inaccessibility of its implementation
and because its reported results are based on a dierent metric.
6.2 Datasets
Training data-extraction models require a large amount of data. Furthermore,
no single dataset has a sucient number of annotated samples in terms of quan-
tity and diversity to train these models. Thus, following suit with recent works
[15,18,25], we combine multiple datasets for model training.
AdobeSynth19 : This dataset was released from the ICDAR-19 Competition
on Harvesting Raw Tables from Infographics 2019 (CHART-Info-19)[5]. It con-
tains samples of dierent chart types and has nearly 38,000 synthetic line images
generated using matplotlib and annotations for data extraction and other aux-
iliary tasks. We use the competition-provided train test split.8 J. Lal et al.
UB-PMC22 : This is the extended version of the UB-PMC-2020 dataset pub-
lished by CHART-Info-2020 [8]. It consists of real chart samples obtained from
PubMed Central that were manually annotated for several tasks, including data
extraction. Again, we use the provided train test split, which contains about
1500 line chart samples with data extraction annotation in the training set and
158 in the testing set. It is one of the most varied and challenging real-world
chart datasets publicly available for data extraction.
LineEX : This is another synthetic dataset released with LineEX[25] work,
which contains more variations in structural characteristics - line shape, cross-
ings, etc., and visual characteristics - line style and color. It was generated using
matplotlib and is the largest synthetic dataset available for line chart analysis.
We only use a subset, 40K samples from their train set and 10K samples from
the test set for evaluation.
7 Evaluation
The particular choice of metric to evaluate line data extraction performance
varies amongst existing works, yet they all share a common theme. The idea is
to calculate the dierence between the y values for the predicted and ground-
truth line series and to aggregate that dierence to generate a match score.
ChartOCR [17] averages these normalized y value dierences across all ground
truth x,y points, while Figureseer [28] and Linear Programming [15] binarize the
point-wise dierence by taking an error threshold of 2% to consider a point as
correctly predicted. Here, we use the former as it is more precise in capturing the
more minor deviations in line point prediction. Furthermore, the CHART-Info
challenge formalized the same, proposing the Visual Element Detection Score
and the Data Extraction Score, also referred to as task-6a and task-6b metrics
[7].
7.1 Pairwise Line Similarity
Formally, to compare a predicted and ground truth data series, a similarity score
is calculated by aggregating the absolute dierence between the predicted value
yiand the ground truth value yi, for each ground truth data point ( xi;yi). The
predicted value  yiis linearly interpolated if absent.
7.2 Factoring Multi-Line Predictions
Most charts contain multiple lines. Thus, a complete evaluation requires a map-
ping between ground-truth and predicted lines. The assignment is done by com-
puting the pairwise similarity scores (as shown in 7.1) between each predicted
and ground truth line and then performing a bipartite assignment that maxi-
mizes the average pairwise score. This is similar to the mean average precision
(mAP) calculation used to evaluate Object Detection performance.LineFormer 9
LetNpandNgbe the number of lines in the predicted and ground-truth
data series.
We dene K as the number of columns in the pairwise similarity matrix Sij
and the bipartite assignment matrix Xij, whereiranges [1;Np] andjranges
[1;Ng]
K=(
Ng for Visual Element Detection Score(task 6a)
max(Ng;Np) for Data Extraction Score (task 6b)
Sij=(
0 if j >N g
Similarity (Pi;Gj) otherwise
Xij=(
1 if Pimatched with Gj
0 otherwise
The nal score is calculated by optimizing the average pairwise similarity
score under the one-to-one bipartite assignment constraint.
score =1
Kmax
XX
iX
jSijXijs:tX
iXj= 1andX
jXi= 1
The dierence between task-6a and task-6b scores is that the former only
measures line extraction recall and hence doesn't consider the false positives
(extraneous line predictions). In task-6b, however, a score of `0' is assigned for
each predicted data series that hasn't been matched with a corresponding ground
truth, along with an increase in value of denominator Kfor normalizing the score.
8 Results
8.1 Quantitative Analysis
The performance of various systems for line data extraction on visual element
detection and data extraction is shown in Table 1. It can be observed that
the UB-PMC data set proves to be the most challenging of all, as it is diverse
and composed of real charts from scientic journals. LineFormer demonstrates
state-of-the-art results on most real and synthetic datasets. Furthermore, our re-
sults are consistent across the two evaluation metrics: Visual Element Detection
(task-6a) and Data Extraction (task-6b). This stability is not reected in exist-
ing work, as they exhibit substantial drops in performance. This discrepancy can
be attributed to the fact that Task-6b penalizes false positives, indicating that
LineFormer has a considerably higher precision rate. A key highlight of Line-
Former is that it is robust across datasets and shows signicant improvement on
UB-PMC real-world chart data.10 J. Lal et al.
Table 1: LineFormer Quantitative Evaluation
Dataset AdobeSynth19 UB-PMC22 LineEX
Work nMetricVisual
Element
DetectionaData
ExtractionbVisual
Element
DetectionData
ExtractionVisual
Element
DetectionData
Extraction
ChartOCR [17] 84.67 55 83.89 72.9 86.47 78.25
Lenovoc[18] 99.29 98.81 84.03 67.01 - -
LineEX [25] 82.52 81.97 50.23d47.03 71.13 71.08
Lineformer (Ours) 97.51 97.02 93.1 88.25 99.20 97.57
atask-6a from CHART-Info challenge [7]
btask-6b data score from CHART-Info challenge
cImplementation not public, hence only reported numbers on AdobeSynth and UB PMC
dIgnoring samples without legend, as LineEX doesn't support them
8.2 Qualitative Analysis
The qualitative comparison of LineFormer with existing models is carried out by
examining line predictions on selected samples from the dataset. This analysis
focuses on scenarios that involve crossings, occlusions, and crowding to assess the
ability of models to handle such complex situations. The results are presented
in Fig [4,5], which clearly illustrate the diculties faced by existing approaches
as the number of lines and the complexity of the chart increases. LineFormer
performs signicantly better, keeping track of the line even in occlusions and
crowding.
8.3 Ablation Study
Table 2: Performance of dierent backbones on UB-PMC data
Backbone PMC - 6a PMC - 6b
Swin-T [16] 93.1 88.25
ResNet50 [12] 93.17 87.21
Swin-S [16] 93.19 88.51
ResNet101 [12] 93.1 90.08
We conducted an ablation study to understand the impact of dierent back-
bones on line extraction performance. For simplicity, we stick to the UB-PMC
dataset, which has the most diverse set of real-world line charts. Table 2 shows
the results of the study. We observe that performance remains mostly stable
across most backbones, with a general trend towards an increase in 6b scores
with an increase in the number of parameters. Furthermore, it should be noted
that even the smallest backbone, SwinTransformer-Tiny [16] (Swin-T), surpassesLineFormer 11
the current keypoint-based state of the art (see Table 1) on the UB-PMC dataset
by a large margin.
The ablation study demonstrates that the choice of backbone has a relatively
minor impact on the overall performance of line extraction. This suggests that
LineFormer is robust to dierent backbone architectures, and the main advan-
tage is derived from the overall framework and the instance segmentation ap-
proach.
(a) Original Image
 (b) LineFormer
(c) ChartOCR [17]
 (d) LineEx [25]
Fig. 4: Qualitative analysis: PMC samples12 J. Lal et al.
(a) Original Image
 (b) LineFormer
 (c) ChartOCR [17]
 (d) LineEx [25]
Fig. 5: Qualitative analysis: LineEx samples
9 Discussion
After extracting the line data series, each line must be associated with its cor-
responding label in the legend. To accomplish this task, a Siamese network can
match the line embeddings with those of the patches in the provided legend.
LineFormer is designed to allow multiple labels for the same pixels to address
overlapping lines. This enables LineFormer to predict dierent masks for lines
that have overlapping sections. However, occasionally this increases the likeli-
hood that LineFormer produces duplicate masks for the same line. Fig [6] shows
a plot with four lines where LineFormer has predicted 5, a duplicate mask for
line 1.
This issue can be mitigated by selecting the top-n predictions based on legend
information and using Intersection Over Union (IoU) based suppression methods
in its absence.
10 Conclusion
In this work, we have addressed the unique challenges that line charts pose in
data extraction, which arise due to their structural complexities. Our proposed
approach using Instance Segmentation has eectively addressed these challenges.
Experimental results demonstrate that our method, LineFormer, achieves top-
performing results across synthetic and real chart datasets, indicating its poten-
tial for real-world applications.LineFormer 13
(a) annotated image
 (b) mask line 1
 (c) Duplicate mask line 1
(d) mask line 2
 (e) mask line 3
 (f) mask line 4
Fig. 6: Illustration of an occasional issue - repeated line mask for line 1
References
1. Bresenham, J.E.: Algorithm for computer control of a digital plotter. IBM Systems
journal 4(1), 25{30 (1965)
2. Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z.,
Xu, J., Zhang, Z., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R.,
Wu, Y., Dai, J., Wang, J., Shi, J., Ouyang, W., Loy, C.C., Lin, D.: MMDetection:
Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155
(2019)
3. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention
mask transformer for universal image segmentation. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1290{
1299 (2022)
4. Chester, D., Elzer, S.: Getting computers to see information graphics so users do
not have to. In: Foundations of Intelligent Systems: 15th International Symposium,
ISMIS 2005, Saratoga Springs, NY, USA, May 25-28, 2005. Proceedings 15. pp.
660{668. Springer (2005)
5. Davila, K., Kota, B.U., Setlur, S., Govindaraju, V., Tensmeyer, C., Shekhar, S.,
Chaudhry, R.: ICDAR 2019 Competition on Harvesting Raw Tables from Info-
graphics (CHART-Infographics). In: 2019 International Conference on Document
Analysis and Recognition (ICDAR). pp. 1594{1599 (Sep 2019), iSSN: 2379-2140
6. Davila, K., Setlur, S., Doermann, D., Kota, B.U., Govindaraju, V.: Chart Mining:
A Survey of Methods for Automated Chart Analysis. IEEE Transactions on Pat-14 J. Lal et al.
tern Analysis and Machine Intelligence 43(11), 3799{3819 (Nov 2021), conference
Name: IEEE Transactions on Pattern Analysis and Machine Intelligence
7. Davila, K., Tensmeyer, C., Shekhar, S., Singh, H., Setlur, S., Govindaraju, V.: Icpr
2020-competition on harvesting raw tables from infographics. In: Pattern Recogni-
tion. ICPR International Workshops and Challenges: Virtual Event, January 10-15,
2021, Proceedings, Part VIII. pp. 361{380. Springer (2021)
8. Davila, K., Xu, F., Ahmed, S., Mendoza, D.A., Setlur, S., Govindaraju, V.:
ICPR 2022: Challenge on Harvesting Raw Tables from Infographics (CHART-
Infographics). In: 2022 26th International Conference on Pattern Recognition
(ICPR). pp. 4995{5001 (Aug 2022), iSSN: 2831-7475
9. De Brabandere, B., Neven, D., Van Gool, L.: Semantic instance segmentation with
a discriminative loss function. arXiv preprint arXiv:1708.02551 (2017)
10. Demir, S., Carberry, S., McCoy, K.F.: Summarizing information graphics textually.
Computational Linguistics 38(3), 527{574 (2012)
11. He, K., Gkioxari, G., Doll ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the
IEEE international conference on computer vision. pp. 2961{2969 (2017)
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770{778 (2016)
13. Hoque, E., Kavehzadeh, P., Masry, A.: Chart question answering: State of the art
and future directions. In: Computer Graphics Forum. vol. 41, pp. 555{572. Wiley
Online Library (2022)
14. Kanthara, S., Leong, R.T.K., Lin, X., Masry, A., Thakkar, M., Hoque, E., Joty, S.:
Chart-to-text: A large-scale benchmark for chart summarization. arXiv preprint
arXiv:2203.06486 (2022)
15. Kato, H., Nakazawa, M., Yang, H.K., Chen, M., Stenger, B.: Parsing Line Chart
Images Using Linear Programming. pp. 2109{2118 (2022)
16. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer: Hierarchical vision transformer using shifted windows. In: Proceedings
of the IEEE/CVF international conference on computer vision. pp. 10012{10022
(2021)
17. Luo, J., Li, Z., Wang, J., Lin, C.Y.: ChartOCR: Data Extraction From Charts
Images via a Deep Hybrid Framework. pp. 1917{1925 (2021)
18. Ma, W., Zhang, H., Yan, S., Yao, G., Huang, Y., Li, H., Wu, Y., Jin, L.: Towards
an Ecient Framework for Data Extraction from Chart Images. In: Llad os, J., Lo-
presti, D., Uchida, S. (eds.) Document Analysis and Recognition { ICDAR 2021.
pp. 583{597. Lecture Notes in Computer Science, Springer International Publish-
ing, Cham (2021)
19. Masry, A., Long, D.X., Tan, J.Q., Joty, S., Hoque, E.: ChartQA: A Benchmark for
Question Answering about Charts with Visual and Logical Reasoning (Mar 2022),
arXiv:2203.10244 [cs]
20. Mei, H., Ma, Y., Wei, Y., Chen, W.: The design space of construction tools for
information visualization: A survey. Journal of Visual Languages & Computing
44, 120{132 (2018)
21. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks
for volumetric medical image segmentation. In: 2016 fourth international confer-
ence on 3D vision (3DV). pp. 565{571. Ieee (2016)
22. Molla, M.K.I., Talukder, K.H., Hossain, M.A.: Line chart recognition and data
extraction technique. In: Intelligent Data Engineering and Automated Learning:
4th International Conference, IDEAL 2003, Hong Kong, China, March 21-23, 2003.
Revised Papers 4. pp. 865{870. Springer (2003)LineFormer 15
23. Nair, R.R., Sankaran, N., Nwogu, I., Govindaraju, V.: Automated analysis of line
plots in documents. In: 2015 13th international conference on document analysis
and recognition (icdar). pp. 796{800. IEEE (2015)
24. Neven, D., De Brabandere, B., Georgoulis, S., Proesmans, M., Van Gool, L.: To-
wards end-to-end lane detection: an instance segmentation approach. In: 2018 IEEE
intelligent vehicles symposium (IV). pp. 286{291. IEEE (2018)
25. P., S.V., Hassan, M.Y., Singh, M.: LineEX: Data Extraction From Scientic Line
Charts. pp. 6213{6221 (2023)
26. Ray Choudhury, S., Giles, C.L.: An architecture for information extraction from
gures in digital libraries. In: Proceedings of the 24th International Conference on
World Wide Web. pp. 667{672 (2015)
27. Ray Choudhury, S., Wang, S., Giles, C.L.: Curve separation for line graphs in schol-
arly documents. In: Proceedings of the 16th ACM/IEEE-CS on Joint Conference
on Digital Libraries. pp. 277{278 (2016)
28. Siegel, N., Horvitz, Z., Levin, R., Divvala, S., Farhadi, A.: FigureSeer: Parsing
Result-Figures in Research Papers. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) Computer Vision { ECCV 2016. pp. 664{680. Lecture Notes in Computer
Science, Springer International Publishing, Cham (2016)
29. Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end
video instance segmentation with transformers. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 8741{8750 (2021)