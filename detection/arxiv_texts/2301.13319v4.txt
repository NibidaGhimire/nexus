ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning
Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
Karol Gotkowskia,b, Shuvam Guptac, Jose R. A. Godinhoc, Camila G. S. Tochtropc, Klaus
H. Maier-Heinb,dand Fabian Isenseea,b,∗
aHelmholtz Imaging, German Cancer Research Center, Im Neuenheimer Feld 280, Heidelberg, 69120, Baden-Wuerttemberg, Germany
bDivision of Medical Image Computing, German Cancer Research Center, Im Neuenheimer Feld 280, Heidelberg, 69120, Baden-Wuerttemberg, Germany
cHelmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Chemnitzer Straße
40, Freiberg, 09599, Sachsen, Germany
dPattern Analysis and Learning Group, Department of Radiation Oncology, Heidelberg University Hospital, Im Neuenheimer Feld
672, Heidelberg, 69120, Baden-Wuerttemberg, Germany
ARTICLE INFO
Keywords :
individual particle characterization
3D
instance segmentation
deep learning
recycling
mineral processingABSTRACT
Minerals, metals, and plastics are indispensable for a functioning modern society. Yet, their supply
is limited causing a need for optimizing ore extraction and recuperation from recyclable materials.
Typically, those processes must be meticulously adapted to the precise properties of the processed
materials. Advancing our understanding of these materials is thus vital and can be achieved by
crushing them into particles of micrometer size followed by their characterization. Current imaging
approaches perform this analysis based on segmentation and characterization of particles imaged
with computed tomography (CT), and rely on rudimentary postprocessing techniques to separate
touching particles. However, their inability to reliably perform this separation as well as the need
to retrain methods for each new image, these approaches leave untapped potential to be leveraged.
Here,weproposeParticleSeg3D,aninstancesegmentationmethodabletoextractindividualparticles
from large CT images of particle samples containing different materials. Our approach is based
on the powerful nnU-Net framework, introduces a particle size normalization, uses a border-core
representationtoenableinstancesegmentation,andistrainedwithalargedatasetcontainingparticles
of numerous different sizes, shapes, and compositions of various materials. We demonstrate that
ParticleSeg3D can be applied out-of-the-box to a large variety of particle types, including materials
and appearances that have not been part of the training set. Thus, no further manual annotations and
retraining are required when applying the method to new particle samples, enabling substantially
higher scalability of experiments than existing methods. Our code and dataset are made publicly
available.
1. Introduction
Materialresourcessuchasminerals,metals,andplastics
are ubiquitous in modern society, yet, they constitute a
finiteresourcethatneedstobeacquiredcost-effectivelyand
usedresponsibly.Moreover,withtheever-risingdemandfor
specific materials, these processing and recycling processes
are becoming increasingly automated. Characterization of
samples containing crushed and pulverized particles of mi-
crometer size from these materials plays a crucial role in
this process by providing a detailed understanding of the
material composition alongside other factors, contributing
to the development of effective processes to both mine and
recycle specific materials.
So far, sample-level analysis of the materials present in a
sample can be conducted through a multitude of methods
such as X-ray diffraction, inductively coupled plasma mass
∗Corresponding author
karol.gotkowski@dkfz.de (K. Gotkowski); s.gupta@hzdr.de (S.
Gupta); j.godinho@hzdr.de (J.R.A. Godinho);
c.guimaraes-da-silva-tochtrop@hzdr.de (C.G.S. Tochtrop);
klaus.maier-hein@dkfz-heidelberg.de (K.H. Maier-Hein);
f.isensee@dkfz-heidelberg.de (F. Isensee)
ORCID(s):spectrometry (ICP-MS), or computed tomography (CT). In
thelatter,theparticlesareembeddedinanepoxymatrixand
imaged via CT in order to create a semantic segmentation
that segments all particles, distinguishing between different
particle material types but not between particle instances.
The segmentation is either performed through a variant of
intensity thresholding Hassan et al. (2012); Becker et al.
(2016);Dominyetal.(2011);Godinhoetal.(2019)ordeep
learningapproachesWangetal.(2021);Filippoetal.(2021);
Xiao et al. (2020); Latif et al. (2022); Nie et al. (2022);
Liu et al. (2021); Tung et al. (2022). The individual particle
properties that determine the behavior of each particle are
not accounted for in sample-level analyses, limiting our un-
derstandingoftheusedmaterialsandourabilitytooptimize
downstream processes.
A better optimization can be achieved through particle-
levelanalysiswiththecharacteristicsofindividualparticles
Pereiraetal.(2021).Naturally,thisapproachisonlypossible
with high-resolution imaging techniques such as CT. Here,
every particle instance is segmented and assigned a unique
identifier, resulting in a so-called instance segmentation.
Thishastheadvantagethateachparticlecanbefurtherchar-
acterized individually according to its specific shape, size,
Karol Gotkowski et al.: Page 1 of 15arXiv:2301.13319v4  [cs.CV]  14 Dec 2023ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
andparticlehistogramGodinhoetal.(2023).Suchattempts
tocharacterizeindividualparticleshavebeenmostlylimited
to imaging techniques that analyze only a single 2D image
Liu et al. (2020); Baraian et al. (2022); Sun et al. (2022),
leadingtoaninherentstereologicalbiasthatmakestheshape
andsizecharacterizationofparticlesunreliableBlanninetal.
(2021).
Extendingindividualparticlecharacterizationto3Dischal-
lenging, given that upwards of ten thousand particles are
typicallypresentinasingleimage.Evenmore,thepresence
of imaging artifacts as well as highly diverse materials
and filler materials make this a difficult problem where
establishedmethodsthatrelyonintensitythresholdingZhou
et al. (2021); Jiang et al. (2021); Wang et al. (2016, 2015);
Guntoroetal.(2021)mayfailduetotheirlackofrobustness.
To solve this problem for 3D instance segmentation more
sophisticated deep learning methods are a necessity Furat
et al. (2023); Tang et al. (2023) as they use a multitude
of learned features, making them inherently more robust to
these challenges. However, a limitation of these approaches
is constituted through the use of semantic segmentation
models to generate predictions that are converted in a sep-
arate postprocessing step into instance segmentations, lead-
ing to touching particles often not recognized as separate
instances and thus hindering individual particle analysis.
Moreover, a common drawback of all current deep learning
approaches used for particle segmentation is the narrow
scope on specific types of materials, resulting in a lack of
generalization. Consequently, these methods do not work
out-of-the-box when applied to a particle image containing
new materials as part of the image needs to be annotated
manually and the model retrained in order to function prop-
erly.
To truly pave the way for process optimization based on
individual3Dparticlecharacterization,weproposeParticle-
Seg3D,anautomatedout-of-the-box3Dinstancesegmenta-
tion method with high robustness and generalizability that
can handle diverse sample and particle types across mag-
nitudes of different particle sizes without the need to adapt
themethodorannotatenewdataandthusdoesnotconstitute
majorbottlenecks.Ourapproachemploysahighlyoptimized
novel training and inference pipeline based on a 3D U-Net
andistrainedonalargedatasetofCTparticlesampleswitha
multitudeofdifferentmaterialssuchasvariousnaturalores,
slags with complex microstructures, batteries, and other
electronicswithplasticsandlargevariationsofcomponents.
Thus our approach enables a wide range of analyses based
on the intrinsic particle properties measurable based on the
inferred instance segmentations and can be employed even
by non-machine learning experts without prior knowledge.
Our framework and all data are open source and available
under https://github.com/MIC-DKFZ/ParticleSeg3D.2. Materials
2.1. Sample preparation
The samples used in our approach consist of various
materials crushed and pulverized into particles of microm-
eter sizes. They were prepared following a standardized
procedure described in Godinho et al. (2021) which uses
sugar particles as spacers in the ratio of 7 g of sugar to
1 g of particles. The resin used is Paladur (Kulzer, Mit-
sui Chemical Group), a fast-curing acrylic polymer. The
material particles together with the sugar were mixed with
methylmethacrylate-copolymer powder in a mass ratio of
1:1. The methylmethacrylate liquid resin was added to the
solid mixture in a ratio of 3 mL to 10 g. The final paste is
lefttodryinatubewithadiameteradequateforthedesired
voxel spacing of the scan.
Samples were imaged with a CT scanner (CoreTom from
XRE – Tescan; Ghent, Belgium), while the “XRE – re-
con” software (v1.1.0.14, XRE – Tescan, Ghent, Belgium)
was used to reconstruct the 3D images in 16-bit. Scanning
conditions differed on a per-sample basis as to use optimal
reconstruction parameters based on material compositions.
Consequently, voxel intensities and particle sizes vary be-
tween the obtained images.
2.2. Datasets
A diverse set of samples is required in order to train a
model that predicts high-accuracy instance segmentations
and generalizes well even to materials not present in the
trainingdistribution.Thesamplespreparedforourapproach
cover a wide range of materials, e.g. various natural ores,
slags with complex microstructures, batteries, and other
electronicswithplasticsandlargevariationsofcomponents
to fulfill this criterion. The mean image size of a sample
is (997 ±287, 1256 ±218, 1256 ±218) voxels and the mean
particle size of different samples ranges from 55 to 721
micrometers as determined through the equivalent particle
diameter but can also be substantially smaller and larger
due to strong variance in particle sizes within samples. For
the development and training of our method, 19 samples
were prepared in total from which we used 41 patches with
either a size of 128x400x400 or 200x400x400 voxels that
had been extracted and annotated following the procedure
outlined in section 3.1. An example of such a prepared and
annotated sample is shown in Figure 1. Further, we depict
inFigure2theparticleintensitydistributionsofallsamples,
withtheintensitypeakscorrespondingtodifferentmaterials
exhibiting the diverse nature of our samples.
Fortestingtheperformanceofourmodel,wedesignedthree
test sets reflecting different use cases. The first is an in-
distribution set of in total 8 patches taken from 8 unseen
samples, i.e. samples that are not part of the train set, with
the purpose to evaluate the performance in terms of its
prediction accuracy on samples that consist of materials
already seen by the framework during training. The second
is an out-of-distribution set with 5 patches taken from 5
samples that are not part of the train set and that consist
ofmaterialsentirelyorintheircompositionunknowntothe
Karol Gotkowski et al.: Page 2 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
(a) Sample
 (b) Sample patch
 (c) Patch reference segmentation
Figure 1: Overview of the image Synthetic1 and its reference segmentation.
Table 1
A listing of all particle samples, their primary material compo-
sition, and physical particle size range that have been used for
the train set, in-distribution (ID) test set, out-of-distribution
(OOD) test set, and artifacts (A) test set.
Sample Primary Materials Particle Size ( 𝜇m) Set
Ore1-Comp1-Feed1Sn-skarn 122±41
TrainOre1-Comp2-Concentrate1Sn-skarn 113±40
Ore1-Comp3-tailings1Sn-skarn 177±42
Recycling1 Li-Co-Mn-Ni batteries 156±49
Ore2-PS300-VS10 Chromite ore 249±154
Ore2-PS850-VS26 Chromite ore 721±304
Ore2-PS75-VS55 Chromite ore 85±32
Synthetic1 Various ores, slags, batteries 117±67
WEEE slag PCBs 136±89
Slag1 Cu-Fe rich 61±65
Ore3-Comp32Scheelite ore 141±48
Ore7 Sulphide-based ore 344±209
Slag2-PS500 Iron-rich 471±134
Synthetic2 Quartz, calcite, fluorite 126±98
Synthetic3 Calcite 62±40
Synthetic4 Calcite 55±40
Synthetic5 Lepidolite 116±77
Synthetic6 Glass beads, calcite and fluorite 218±92
Synthetic7 Quartz, lepidolite 189±100
Ore1-Comp3-Concentrate1Sn-skarn 133±45
ID TestSlag3 Chromium-rich 402±150
Ore1-Comp4-Feed1Sn-skarn 123±36
Ore2-PS850-VS10 Chromite ore 592±252
Slag2-PS300 Iron-rich 188±85
Ore3-Comp12Scheelite ore 140±52
Ore3-Comp22Scheelite ore 49±32
Slag4-PS3003WEEE-recycling 149±111
Slag5 Zinc-rich 435±134
OOD TestOre5 Cu/Au-ore 147±56
Ore4-PS-Low4Parisite 85±55
Ore4-PS-High4Parisite 59±51
Ore65Sulphide concentrate 393±141
Ore8 Gold ore 127±34 A Test
modeltoevaluatethegeneralizationabilitiesofourmethod.
The third is a set consisting of a single sample that is not
part of the train set with most particles strongly affected by
streak artifacts as a result of beam hardening. No reference
segmentation can be reliably created with the procedure
in section 3.1 and thus this sample is only used for the
qualitativeevaluationinsection5.2totesttherobustnessof
ourmethodagainstartifacts.Alistingofallsamplesisgiven
in Table 1.
Figure 2: Kernel Density Estimation of the different particle
intensity distributions of the train set (blue) and the test
sets (green). Densities have been normalized to [0,1] for each
sample.
3. Methodology
Instancesegmentationof3Dvoxelimagesisstillaprob-
lem mostly solved with conventional methods. 3D instance
segmentation models based on deep learning Kopelowitz
and Engelhard (2019); Fan et al. (2020); Zhao et al. (2018,
2021); Wang et al. (2019); Jeong et al. (2020) are so far
tailored to specific, mostly medical, tasks and have not
been extensively used and evaluated for generalization and
robustnessoutsidetheirscope.Onthecontrary,3Dsemantic
segmentationisawell-studiedtaskwithavarietyofsuitable
modelsprovensuccessfulsuchasU-NetsandTransformers.
1Kern et al. (2019)
2Kern et al. (2022)
3Buchmann et al. (2020)
4Godinho et al. (2023)
5Winardhi et al. (2022)
Karol Gotkowski et al.: Page 3 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
Figure 3: Training and inference pipelines of ParticleSeg3D that employ intensity and particle size normalization to increase
robustness, a border-core representation to enable usage of the nnU-Net, and a chunk sampler to infer predictions for very large
images.
EspeciallynnU-Net,astate-of-the-art3Dsemanticsegmen-
tation model, has proven its performance, generalization
abilities,androbustnessonmanymedicalchallengesIsensee
et al. (2021) and even non-medical applications Grabowski
et al. (2022); Li et al. (2022). However, converting the
semantic segmentations predicted by such models into in-
stance segmentations is challenging as particles cannot be
separated easily due to their inconsistent shapes and ten-
dency to touch and intertwine with each other. In order to
handlesuchcases,wereformulatetheinstancesegmentation
problem as a semantic segmentation problem by converting
the instances to a border-core representation, elaborated
in 3.2.2, enabling the usage of nnU-Net for 3D instance
segmentation.
With this technique at its core, we propose the new deep
learning based method ParticleSeg3D as depicted in Figure
3 that enables a fully automated generation of instance
segmentationsforindividualparticlecharacterizationin3D
CT images. The optimized annotation pipeline developed
for this task and proposed in section 3.1 is used to create
trainingdataforourmethodduetoalackofopenlyavailable
reference instance segmentation data. Subsequently, we use
thedatainourtrainingpipeline,asdescribedinsection3.2,
to train a robust model that generalizes well even to unseen
types of materials. As a result, no additional training data
or finetuning is required for the inference process of new
samples.
3.1. Reference annotation
3D images of particle samples typically contain thou-
sandsofindividualparticles.Annotatingentireimages,evenwith the support of our proposed annotation pipeline, is too
labor-intensive and would yield mostly redundant informa-
tion for our method. Thus, utilizing the heterogeneity of
the images, we take one or several representative smaller
patchesfromeachtrainingsampleforannotation.Toenable
fastandpreciseannotationstheinteractive3DviewerNapari
Sofroniewetal.(2022)isusedwithmultiplepluginstoguide
theannotationprocess.Aguidefortheannotationprocessis
available at https://github.com/MIC-DKFZ/ParticleSeg3D.
For each patch, a semantic segmentation of the particles
is performed through the use of a Random Forest voxel
classifier that is trained with scribble annotations. Random
Forestclassifiersoperateonabroadsetofhandcraftedimage
features such as intensity gradients, image intensities, and
gradient orientations making them more robust against CT
imaging artifacts. The segmentation process is iteratively
repeatedwithmorerefinedscribblesuntiltheresultissatis-
factory.Oncecompleted,theremainingerrorsarecorrected
fully manually. An example of this random forest segmen-
tation is shown in Figure 4. The semantic segmentation is
then converted into an instance segmentation by assigning
all particlesa unique identifier.However, particles touching
each other cannot be separated in this step, and are incor-
rectlygivenonlyasingleidentifier,whichmustsubsequently
becorrected.Toachievethis,weproposeacustomparticle-
splitting tool. A marker is placed on each particle and a
borderisdrawnbetweentheparticles.Theparticle-splitting
toolthencomputesthegeodesicdistanceWangetal.(2018)
of each voxel in both particles to the defined 2D border and
separates them by applying a watershed algorithm on the
geodesicdistanceswiththeparticlemarkersasseedsforthe
Karol Gotkowski et al.: Page 4 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
(a) Patch
 (b) Scribbles
 (c) Segmentation
Figure 4: Annotation process of creating a semantic seg-
mentation with Random Forest through iterative refinement
with scribbles. (a) A patch is extracted from the image; (b)
Scribbles are iteratively refined in the patch; (c) A semantic
segmentation of the particles is successfully created.
(a)TouchingParticles
 (b) Splitting Process
 (c) Split Particles
Figure 5: Annotationprocessofseparatingtouchingparticlesin
3D to create an instance segmentation. (a) Touching particles
are manually identified; (b) A marker is placed on each particle
and a 2D border is drawn with our particle splitting tool;
(c) The particles are successfully separated in 3D and each
is assigned a unique identifier.
algorithm. The result is a well-defined 3D border between
thetwoparticlesinmostcasesrequiringonlyafewseconds
of annotation time per split. The process of this particle
separation is depicted in Figure 5.
3.2. Training pipeline
Thetrainingpipeline,depictedinFigure3.A,startswith
a preprocessing stage to normalize the pixel intensities and
homogenize the particle sizes to cope with the size changes
of multiple orders of magnitude (Section 3.2.1). Next, a
border-corerepresentation,showntoperformwellforlarge-
scalecelltrackingIsenseeetal.(2021)andthussuitablefor
identifyingparticlesinthetenthousands,isusedtomapthe
referenceinstancesegmentationstosemanticsegmentations
(Section 3.2.2). The border-core representations are then
used for training nnU-Net, which has been proven highly
successfulonamultitudeofdiversedatasets(Section3.2.3).
3.2.1. Preprocessing
Voxel intensities 𝐼of all images are normalized via
z-scoring. To this end, the global mean intensity 𝜇and
standard deviation of intensities 𝜎are computed over all
training images. Then, each voxel intensity is normalized
𝐼𝑛𝑜𝑟𝑚with the following Equation 1.
𝐼𝑛𝑜𝑟𝑚=𝐼−𝜎
𝜇(1)Particle sizes in our datasets vary substantially from
55 to 721 micrometers as determined through the equiv-
alent particle diameter. At the same time, CT images are
acquired at varying voxel spacings. Consequently, the size
of particles in voxels varies substantially between images.
Such a broad size distribution can hamper the training of
the segmentation model, causing reduced performance and
reduced robustness. To counteract this problem, we strive
to homogenize the particle size distribution in our training
data. To this end, the reference particle size of a sample
is determined by measuring the diameter of a particle that
is about average size within the sample. No sophisticated
methods are required for this as the particle size only needs
to be approximate. All training patches are then resized to a
common average particle size.
3.2.2. Border-core representation
A border-core representation enables any semantic seg-
mentation model to be used for instance segmentation. In
contrast to ad-hoc conversion methods such as watershed
that often fail to correctly separate instances, a border-core
representation is more advantageous due to the intrinsic
propertiesoftherepresentationthataresubsequentlylearned
by the model during training. An example of this represen-
tation is shown in Figure 6.
Conversion to the border-core representation is achieved by
eroding every instance with a fixed width in voxels (see
section4.2)referredtoasborderthicknesstogenerateacore,
whiletheerodedarearepresentstheborderofaparticle.As
aresult,theintrinsicpropertiesofthisrepresentationarethat
everyparticlehasthesameborderthicknessandthatnocore
of an instance is connected to a core of another instance.
Training a model with such a representation enables the
model to learn these properties and replicate them during
inference. Such predicted border-core representation can
then be easily converted back into instance segmentations
by assigning each core a unique identifier and dilating the
cores back to the original particle shape as defined by the
border.
It is important to note that in some cases, a border-core
prediction may produce multiple small cores for a single
particle if the particle is thinner than the border thickness.
Thus,thesmallextracoresarefilteredawayinanadditional
post-processingsteptopreventmiss-predictions.Thissmall
(a) Patch
 (b) Instances
 (c) Border-Core
Figure 6: The conversion process of a patch (a) from its
instance segmentation (b) into its corresponding border-core
representation (c).
Karol Gotkowski et al.: Page 5 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
coreremoval filtercomputes theEuclidean distanceof each
corevoxeltoitsborderandsubsequentlyremovesanycoreif
thepercentageofdistancessmallerthanaminimumdistance
is larger than a given threshold (see section 4.2).
3.2.3. Model training
We use nnU-Net Isensee et al. (2021) as the semantic
segmentation model in our training and inference pipeline.
nnU-Net is a self-adapting 3D U-Net that creates a finger-
print of relevant properties from a dataset and automat-
ically adapts important training hyperparameters accord-
ingly. Originally developed for 3D biomedical CT and MRI
data, the nnU-Net became a state-of-the-art model on mul-
tiple benchmark datasets and has also shown its efficacy
in other domains Grabowski et al. (2022); Li et al. (2022);
Spronck et al. (2023); Budelmann et al. (2023); Altini et al.
(2022). Its consistent performance across multiple domains
makes it an ideal fit for the task of particle segmentation
in high-resolution CT images. No modifications have been
made to the nnU-Net training, except for stronger data aug-
mentations, detailed in section 4.2, to better cope with the
varying voxel intensity distributions of different materials,
and a touching particle augmentation, described in the fol-
lowing.
3.2.4. Touching particle augmentation
ToalargedegreennU-Netisabletolearnonitsownthe
intrinsicpropertiesofaborder-corerepresentationbasedon
the already existing touching particles in the training data.
However, the number of touching particles in the training
data used in our method is too small for nnU-Net to fully
prevent the miss-prediction of touching particles during in-
ference.Thisleadstosometouchingparticlesbeingmerged
intoasingleparticle.Toreducethiserrorandtofurtherim-
provethennU-Nettraining,weintroduceatouchingparticle
augmentation.Thisaugmentationisappliedoneveryimage
patch in a batch during a training iteration with a certain
probability. It selects a random particle within the current
patch(particleP)andarandomparticletakenfromtheentire
training dataset (particle D) and copies particle D next to
particlePsuchthattheirparticleedgestoucheachother.This
way,nnU-Netlearnsovertimeabetterunderstandingofhow
tocorrectlyidentifytouchingparticlesasseparateinstances
andconsequentlypredictabetterborder-corerepresentation.
3.3. Inference pipeline
Inferenceonanewimageisperformedbyfirstnormaliz-
ingitsvoxelintensities(Section3.2.1).Thisisfollowedbya
slidingwindowpredictioninwhichtheimageisdividedinto
patchesastheentireimageistoolargetostoreinGPUmem-
ory.Foreachpatch,theparticlesizenormalization(Section
3.2.1) is performed on-the-fly and the patch is subsequently
passedtothemodelforpredictingtheborder-coresegmenta-
tion(Section3.2.2).Onceapatchispredicted,itslocalpatch
prediction is inserted into its original position in the global
prediction of the sample, aggregating all patches during the
inferenceprocessintoafinalprediction.Thispredictionisat
last converted into an instance segmentation and resampledback to its original particle size, concluding the inference
process. However, not all images can be processed in this
fashion as the memory required for the predicted patches
can be immense. Therefore, instead of the naive sliding
window approach, we employ a novel chunk-patch-based
sliding window approach proposed by us to solve this issue
as described in Section 3.3.1. A depiction of the complete
inference pipeline is shown in Figure 3.B.
3.3.1. Chunk sampling and aggregation
A sliding window approach with a patch sampler as
depicted in Figure 7a is used during inference as the entire
sample is too large to store in GPU memory. In order to
still achieve the best possible accuracy, it has proven suc-
cessful to use a patch overlap when using a sliding window
approach.Thismeansthatexceptfortheimageedges,every
voxelinthesampleispredictedmultipletimesbythemodel,
and the resulting predictions for a voxel are averaged. The
model predicts class probabilities for every voxel in a patch
and after the sliding window has reached the end of the
sample, the predicted class probabilities of the sample are
converted into the border-core segmentation. However, this
approach reaches technical limitations for large images as
in our case as the averaged patch predictions of every patch
depend on all surrounding patches. Consequently, all pre-
dictedpatchesarerequiredtobestoredinmemory,resulting
in intractable memory consumption of often more than 100
GB.
To solve this issue, we developed a chunk-patch sampling
strategy as depicted in Figure 7b. The sample is subdi-
videdintomultiplechunks.Eachchunkhasanoverlapwith
the neighboring chunks of exactly one patch, ensuring that
voxels at the edges of a chunk are also predicted multiple
times through the patch overlap such that the prediction
accuracydoesnotdecreaseatthechunkedges.Aprediction
isinferredforeachchunkwiththeslidingwindowapproach
and subsequently saved to disk. This enables a memory-
efficient inference of large images requiring not more than
5 GB of memory as only a single chunk is kept in memory
at all times without compromising segmentation accuracy
while adding only a minimal inference time overhead.
(a) Patch Sampler
 (b) Chunk Sampler
Figure 7: Sampling process of a patch sampler compared to
our developed chunk sampler that overcomes the limitations
of the patch sampler.
Karol Gotkowski et al.: Page 6 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
4. Experimental Setup
4.1. Evaluation metrics
The accuracy of the inferred instance segmentations is
quantitatively measured with multiple metrics and can be
dividedintothecategoriesofsegmentationaccuracymetrics
(Section 4.1.1) and separation accuracy metrics (Section
4.1.2) which are introduced in the following sections.
4.1.1. Segmentation accuracy metrics
We use in total three metrics to measure the accuracy
of the inferred predictions. The 𝐹1score (Equation 2), also
known as𝐷𝑖𝑐𝑒score, is a commonly used metric in image
segmentation tasks to measure the ability of a method to
correctly assign pixels to either foreground or background.
However, it does not consider different particles and thus
does not measure whether pixels are assigned to the correct
particle. The 𝐹1𝑚𝑎𝑡𝑐ℎscore (Equation 3) measures how
well particles were recognized and separated, regardless of
how accurately they are delineated. The 𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒score
(Equation 4) considers both segmentation accuracy as well
asthecorrectidentificationofparticlesandisthusthemetric
most indicative of the performance of our method.
Therangesofallthreemetricsarewithin [0,1]withahigher
scoreindicatingabettersegmentationaccuracy.Allmetrics
arebasedonaconfusionmatrix CMwithtruepositives,false
positives, true negatives, and false negatives being denoted
asTP,FP,TN, and FN, respectively. Further, the confusion
matrix CMis computed in three different variations:
•LetCM𝑣be the confusion matrix that is computed
over all voxels 𝑣in a prediction and its respective
reference.
•LetCM𝑚be the confusion matrix that is computed
over the count of all matched and mismatched par-
ticles𝑚with a match being defined as two particles
having a minimum 0.1 𝐹1overlap of its particle
voxels.
•LetCM𝑖be the confusion matrix that is computed
over the voxels 𝑖of a single particle instance in a
predictionanditsrespectivereferencewith 𝐼denoting
all particle instances. Note that completely falsely
predicted particles without ground truth and ground
truthparticleswithoutanypredictionarealsoincluded
in the computation.
𝐹1 =2TP𝑣
2TP𝑣+FP𝑣+FN𝑣(2)
𝐹1𝑚𝑎𝑡𝑐ℎ =2TP𝑚
2TP𝑚+FP𝑚𝑠+FN𝑚(3)
𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒 =∑𝐼
𝑖2TP𝑖
2TP𝑖+FP𝑖+FN𝑖∑𝐼
𝑖1(4)4.1.2. Separation accuracy metrics
When inferring an instance segmentation, a common
challenge is an incorrect identification of touching particles
as a single merged entity, referred to as Merger, and con-
versely,thesplittingofasingleparticleintomultipleentities,
referred to as Splitter. The occurrence of such misiden-
tifications should be minimized, as they can significantly
impactdownstreamanalysis.Toassesstherobustnessofour
approach to these errors, we introduce the MergerRatio and
SplitterRatio metrics,whichrepresentthepercentageoftotal
particles in a sample that are misidentified as Mergersor
Splitters, respectively.
To define these metrics, we consider a set of all particles 𝑃
andaset𝑀oftouchingparticlesthatareincorrectlymerged
intoasingleparticle.Wealsodefineaset GMofalltouching
particle sets 𝑀that are misidentified. The MergerRatio is
then calculated as shown in Equation 5. It is important to
note that𝑀andGMare defined based on the predicted
segmentation, while 𝑃is defined based on the reference
segmentation.Thisenablesfaircomparisonsbetweendiffer-
ent methods, as the number of predicted particles can vary
across methods.
MergerRatio =(∑GM
𝑀𝑖|𝑀𝑖|) −|GM|
|𝑃|(5)
TheSplitterRatio is defined similarly. We consider a set
of all particles 𝑃and a set𝑆of incorrectly predicted split
particles, all part of the same reference particle. We also
define a set GSof all splitter sets 𝑆. The SplitterRatio is
thenobtainedusingtheformulapresentedinEquation6.As
withthe MergerRatio ,thedefinitionsof 𝑆andGSarebased
on the predicted segmentation, while 𝑃is defined based on
the reference segmentation. By using this approach, we can
compare the performance of different methods even if they
predictdifferentnumbersofparticles.Itisworthnotingthat
boththe MergerRatio andthe SplitterRatio cantheoretically
range from 0 to infinity, but in practice, they are usually
smaller than 1. A lower ratio indicates a lower number of
MergersandSplittersand thus better performance.
SplitterRatio =(∑GS
𝑆𝑖|𝑆𝑖|) −|GS|
|𝑃|(6)
4.2. Training configuration
The training of the nnU-Net used in ParticleSeg3D is
done in PyTorch with SGD optimizer, a learning rate of 1e-
2, a weight decay of 3e-5, a momentum of 0.99, and 1000
epochs of training time. A target particle size of 60 voxels
is used in the particle normalization stage. In the border-
core to instance conversion stage, the small core removal
filter uses a minimum distance of 1 with a threshold of
0.95.TheparametersofthennU-Netaugmentations Bright-
nessTransform ,ContrastAugmentationTransform ,Gamma-
Transform ,BrightnessGradientAdditiveTransform andLo-
calGammaTransform have been modified and are provided
in our source code in detail.
Karol Gotkowski et al.: Page 7 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
4.3. Baselines
We intend to compare our method against other fre-
quently used methods quantitatively and qualitatively in
section 5. Consequently, we reviewed common segmenta-
tion and postprocessing strategies used to create instance
segmentations Hassan et al. (2012); Becker et al. (2016);
Dominyetal.(2011);Godinhoetal.(2019)inordertoderive
the standardized instance segmentation method ThreshWa-
ter, which is described in the following section 4.3.1. Fur-
ther, we also compare against the predictions inferred from
theDeep Learning Tool ofthecommercialimageprocessing
software Dragonfly, which is described in more detail in
section 4.3.2.
4.3.1. ThreshWater
Individualparticlecharacterizationviainstancesegmen-
tationismostlyperformedthroughthresholdingfollowedby
a postprocessing step designed to label individual particles
and separate the ones that are touching each other. We
facilitate the same method and use it for comparison to our
method. Considering that particle intensities differ greatly
betweensamplesasshowninFigure2,thethresholdisman-
ually tuned for each image. To combat poor performance in
low-contrastimages,wemorphologicallyopentheresulting
noisy particle mask to suppress an overwhelming number
ofsmallfalsepositiveparticledetectionswhileretainingthe
integrityofthetruepositivelargerparticles.Toconvertthis
semanticsegmentationintoaninstancesegmentationwhere
theparticlesareseparatedandcanbeprocessedindividually,
weagainfollowbestpracticesandmakeuseofawatershed-
based Van der Walt et al. (2014) splitting procedure. Seed
pointsaregeneratedbyerodingtheparticlemask.Theseeds
together with the masks are then used by the watershed
algorithmtoseparatetheparticles.Designchoicesincluding
theamountofmorphologicalopeningforthenoisereduction
and the amount of erosion for the core generation were
systematically evaluated and optimized using the 41 train
patches.
4.3.2. Dragonfly
Dragonfly(v2022.2.0.12227,ObjectsResearchSystems,
Montreal,Quebec,Canada)providessemanticsegmentation
modelssuchas2DU-Netsinits Deep Learning Tool .3DU-
Nets are also available but at the time of writing cannot be
trained due to an internally fixed patch overlap of NxNx1
voxels resulting in an infeasible large number of patches
being sampled. Training of a 2D U-Net with default con-
figuration as provided by Dragonfly was conducted on all
slices of the 41 train patches. During inference, a semantic
segmentation is predicted by Dragonfly, which we convert
intoaninstancesegmentationwiththesameprocedureasin
ThreshWater by eroding the particle mask to generate seed
points for a subsequent watershed instance separation.
5. Results & Discussion
Evaluation of ParticleSeg3D was conducted quantita-
tively (section 5.1), qualitatively (section 5.2), and withrespect to inference time (section 5.3) on an in-distribution
andout-of-distributiontestsetwhichareelaboratedinmore
detail in section 5.1. Reference segmentations for both test
sets were created as described in section 3.1 and were used
to evaluate our predictions. Further, we also evaluated the
method ThreshWater (section 4.3.1) and Dragonfly (section
4.3.2) in the same fashion as our method and used both as
baselines. Training of our method, ThreshWater and Drag-
onfly was exclusively carried out on the 41 train set patches
as detailed in section 2.2.
5.1. Quantitative evaluation
Quantitative method performance was measured us-
ing the𝐹1score, the𝐹1𝑚𝑎𝑡𝑐ℎscore, and the 𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒
score. In addition, we performed an in-depth analysis of
the method’s ability to correctly split touching particles
usingthe MergerRatio andthe SplitterRatio .Foracomplete
description of the metrics used, see section 4.1.
5.1.1. In-Distribution evaluation
Thein-distribution(ID)testsetconsistsofsampleswith
materials similar to the ones used during training and eval-
uation of such samples provides an estimate of the model’s
performanceontypesofsamplesthemodelwillencounterin
real-worldapplications.Intotal,thesetcontains8manually
annotated patches from 8 samples as described in section
2.2. We quantitatively evaluate the predictions of Particle-
Seg3DaswellasthoseofThreshWaterandDragonflyusing
the metrics introduced in section 4.1. However, we discuss
theresultsforDragonflyseparatelyattheendofthissection
as Dragonfly performs significantly worse than the other
methods. For ParticleSeg3D, ThreshWater and Dragonfly,
the𝐹1, the𝐹1𝑚𝑎𝑡𝑐ℎ, and𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒results are shown in
Figure 8 and in Table 2. On average, a 𝐹1of 94.84% is
achieved with ParticleSeg3D over all samples. Even on the
individual samples, the 𝐹1is above 93%, indicating a high
segmentation accuracyfor every sample.Our average 𝐹1is
1.01% higher than that of ThreshWater, which is a substan-
tialimprovementatthislevelofprecision.Thisisespecially
notable given the fact that ThreshWater uses a manually
tuned threshold for each sample whereas our method is
applied out-of-the-box. For the 𝐹1𝑚𝑎𝑡𝑐ℎscore we achieve
a score of 95.16% on average over all samples, implying
a high correct identification rate of individual particles as
instances, which is amongst the most important properties
of an instance segmentation. By comparison, ThreshWater
onlyachievesan 𝐹1𝑚𝑎𝑡𝑐ℎscoreof82.58%,whichis12.58%
less than that of our method and indicates only a moderate
identification rate of individual particles. In terms of the
𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒metric,ParticleSeg3Dachievesascoreof82.13%
on average while ThreshWater only achieves a score of
69.29%andisthus12.84%worsecomparedtoourmethod.It
thusshowsthatThreshWaterperformspoorlyincomparison
to our method in terms of individual particle segmentation
and particle identification as measured by the 𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒
metric.
Following up on the segmentation accuracy evaluation, we
Karol Gotkowski et al.: Page 8 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
Table 2
Segmentation quantification results on the in-distribution test
set of ParticleSeg3D compared to ThreshWater and Dragonfly.
Higher is better.
Name 𝐹1𝐹1𝑚𝑎𝑡𝑐ℎ𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒
Dragonfly 0.6126 0.5587 0.4645
ThreshWater 0.9383 0.8258 0.6929
ParticleSeg3D 0.9484 0.9516 0.8213
Figure 8: Segmentation quantification results on the in-
distribution test set for each sample. Higher is better.
continuewiththediscussionontheseparationaccuracyeval-
uationwiththe MergerRatio andthe SplitterRatio .Here,itis
desirabletoproduceasfew MergersandSplittersaspossible,
which is often difficult as optimizing a method to produce
fewer Mergerscan lead to more Splittersand vice versa. A
balancedtradeoffisthereforecrucial.TheresultsofParticle-
Seg3D,ThreshWaterandDragonflyaredepictedinFigure9
and Table 3. The reference segmentations contain a total of
3020 particles. Of those 3020 particles only 198 particles
aretouching,whichamountsto7%oftheparticles.Thiscan
be attributed to our sample preparation technique, in which
a spacer is used to separate individual particles as much as
possible from each other. Of those 198 touching particles,
64Mergershave been falsely created by ParticleSeg3D and
88Mergersby ThreshWater in the ID set, which results
in a mean MergerRatio of 1.31% and 2.67% over the ID
set, respectively. This means that on average ParticleSeg3D
merges 1.31% of particles with other particles compared to
ThreshWaters 2.67% in an ID sample. In terms of Splitters,
only 11 Splittersin total are created by ParticleSeg3D in
comparison to 329 Splitterscreated by ThreshWater in the
ID set, which results in a mean SplitterRatio of 0.58% and
9.02% over the ID set, respectively. This shows that the
problem of Splittersis successfully mitigated in Particle-
Seg3D, while Mergers, on the contrary, are still present to
some extent, but reduced by more than half. ThreshWater
performed similarly in terms of Mergers but is prone to
create a high number of Splitters, which can have a severe
impact on further downstream analysis tasks. The reasons
for this frequent Splittergeneration are explored in section
5.2.4.Table 3
Separation quantification results on the in-distribution test set
of ParticleSeg3D compared to ThreshWater and Dragonfly.
Lower is better.
Name Merger Ratio Mergers Splitter Ratio Splitters
Dragonfly 0.0062 17 0.1463 84
ThreshWater 0.0267 88 0.0902 329
ParticleSeg3D 0.0131 64 0.0058 11
Figure 9: Separation quantification results on the in-
distribution test set for each sample. Lower is better.
To return to the results of Dragonfly, Dragonfly achieves
a𝐹1of 61.26%, a 𝐹1𝑚𝑎𝑡𝑐ℎof 55.87% and a 𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒of
46.45% on average, which is for all metrics significantly
worse than both our method and that of ThreshWater. Re-
garding the MergerRatio andSplitterRatio of 0.62% and
14.63%, respectively, it seems that Dragonfly performs bet-
ter in terms of Mergers than ParticleSeg3D and Thresh-
Water. However, this is misleading as both the Mergers
andSplitterscannot reliably be estimated on low accuracy
predictions. Dragonfly’s bad results indicates a lack of gen-
eralization already present on in-distribution data and can
thus be expected to worsen on out-of-distribution data.
5.1.2. Out-Of-Distribution evaluation
Incontrasttothein-distribution(ID)testset,theout-of-
distribution(OOD)testsetcontainssampleswithmaterials,
particle sizes, and particle shapes that are not present in the
training dataset. Thus, this evaluation provides valuable in-
formationonhowwellourmethodperformsonsampleswith
different characteristics not seen before and consequently
highlights how it can be expected to generalize to a wider
scopeofuse-cases.Ourout-of-distributiontestsetcontains5
annotatedpatchesfrom5differentsamples.Thequantitative
analysis is analogous to the previous section. The 𝐹1, the
𝐹1𝑚𝑎𝑡𝑐ℎ, and𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒results of our method and Thresh-
Water are shown in Figure 10 and Table 4. On average
ParticleSeg3D reaches on the OOD set a 𝐹1of 93.27%, a
𝐹1𝑚𝑎𝑡𝑐ℎof 89.18%, and a 𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒of 72.77%. The OOD
𝐹1andthusthegeneralforegroundpredictionaccuracyhas
onlyaslightreductionof1.57%whencomparedagainstour
ID𝐹1score.Bycontrast,the 𝐹1𝑚𝑎𝑡𝑐ℎscorehasareduction
Karol Gotkowski et al.: Page 9 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
Table 4
Segmentation quantification results on the out-of-distribution
test set of ParticleSeg3D compared to ThreshWater and
Dragonfly. Higher is better.
Name 𝐹1𝐹1𝑚𝑎𝑡𝑐ℎ𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒
Dragonfly 0.6316 0.4686 0.2813
ThreshWater 0.9347 0.6311 0.4488
ParticleSeg3D 0.9327 0.8918 0.7277
Figure 10: Segmentation quantification results on the out-of-
distribution test set for each sample. Higher is better.
of5.98%ontheOODset.ThisdropoffbetweenOODandID
isevenmoreconsiderableforthe 𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒withareduction
of 9.36%.
The reduction for both metrics can be traced to the sam-
ple Ore4-PS-High, which includes a high number of very
small particles (<30 𝜇m) with only a diameter of 1-2 voxels
that have very low contrast. Thus even for humans, these
particles are almost indistinguishable from the background,
making them hardly detectable for our model. We highlight
the challenges associated with this sample as part of our
qualitativeevaluationinsection5.2.6.WhenOre4-PS-High
is excluded from the evaluation a 𝐹1𝑚𝑎𝑡𝑐ℎof 96.94% and
an𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒of 84.52% are achieved, which are on par
withtheresultsfromthein-distributionset.Thisshowsthat
ParticleSeg3D generalizes well to other materials, particle
sizes, and particle shapes enabling the usage of our method
on a wide scope of use-cases under the assumption that the
particlediametershouldbelargerthan2voxelsonverylow-
contrast images.
The comparison of the OOD ThreshWater results to its ID
results reveals that ThreshWater also achieves worse scores
on the𝐹1𝑚𝑎𝑡𝑐ℎand𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒metrics with a reduction of
19.47% and 24.41%, respectively. However, this reduction
originates not from the Ore4-PS-High sample but is the
result of generally worse and highly varying performance
on all samples in the OOD set. This originates mostly from
badly segmented low-contrast particles and particles that
havenotbeensegmentedatallbyThreshWateraswediscuss
in section 5.2.5 and 5.2.6, demonstrating the limitations of
classical approaches with more complex samples.
ReturningagaintotheresultsofDragonfly,a 𝐹1of63.16%,Table 5
Separation quantification results on the out-of-distribution test
set of ParticleSeg3D compared to ThreshWater and Dragonfly.
Lower is better.
Name Merger Ratio Mergers Splitter Ratio Splitters
Dragonfly 0.0172 23 0.3362 170
ThreshWater 0.0091 10 0.554 188
ParticleSeg3D 0.0097 14 0.0048 2
Figure 11: Separation quantification results on the out-of-
distribution test set for each sample. Lower is better.
a𝐹1𝑚𝑎𝑡𝑐ℎof 46.86% and a 𝐹1𝑖𝑛𝑠𝑡𝑎𝑛𝑐𝑒of 28.13% is achieved
on average, which is as expected worse than on the ID set
and shows Dragonfly’s lack of generalization abilities in its
training pipeline implementation. We continue the out-of-
distributionevaluationwiththeseparationaccuracymetrics
MergerRatio andthe SplitterRatio withtheresultsshownin
Figure 11 and Table 5. The reference segmentations consist
of 1045 particles with only 29 particles (2.55%) touching
each other as a result of our sample preparation strategy. Of
those29touchingparticles14 Mergersarefalselycreatedby
ParticleSeg3DintheOODsetwithThreshWaterperforming
minimallybetterwithonly10 Mergers,whichresultsinsim-
ilar mean MergerRatios of 0.97% and 0.91% over the OOD
set, respectively. However, when considering the number
ofSplitters, ParticleSeg3D only produces 2 Splittersin the
OOD set, while ThreshWater produces 188, which is equal
to a mean SplitterRatio of 0.48% and 55.4% over the OOD
set,respectively.Itcanthusbeconcludedthatourmethodis
able to reliably identify individual particles as can be seen
by its low MergerRatio andSplitterRatio even in samples
with materials, particle sizes, and particle shapes unknown
toourmethod.Ontheotherhand,ThreshWateralsoachieves
alow MergerRatio ,butdemonstratesthesameshortcomings
asinthein-distributionevaluationwithmorethanhalfofthe
detected particles being Splitters.
5.2. Qualitative evaluation
Weconductedaqualitativeevaluationofthein-distribution
(ID), out-of-distribution (OOD), and artifacts test samples
with an overview of some samples and our inferred pre-
dictions given in Figure 12. It can be seen already in the
Karol Gotkowski et al.: Page 10 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
overview thatParticleSeg3D is ableto segment allparticles
with high accuracy and is robust against varying particle
sizes, shapes, and imaging artifacts.
Ore1-Comp3-Concentrate
Ore6
Ore8
Figure 12: Overview of samples with different appearances,
varying particle sizes, and CT imaging artifacts. Left: 2D view
of the entire image; Middle: Zoomed crop of the image; Right:
ParticleSeg3D prediction of the crop
Besides the general sample overview, several selected
particleswereinferredwithParticleSeg3DandThreshWater
forfurtherdiscussion.Thepredictionofeachofthoseparti-
clescanbecategorizedintooneofsixcategoriesdepending
onthepredictionaccuracyandpotentialfailurecases,which
are discussed in the following subsections. The selected
particles are depicted in Figure 13 sorted per category with
each particle prediction further marked as Correct (Green) ,
Borderline (Orange) orFailure (Red) asdeterminedthrough
visual inspection. The low accuracy of Dragonfly’s predic-
tions is discussed separately in section 5.2.7.
5.2.1. High accuracy
ParticleSeg3D predicts most particles with high accu-
racy, as confirmed by our quantitative evaluation in section
5.1,whileamajorityarealsoaccuratelypredictedbyThresh-
Water.AsubsetofparticlesfromboththeIDandOODsets
has been chosen for visualization in Figure 13.A, demon-
strating our method’s generalization abilities to accurately
delineate particle boundaries on diverse materials with a
widerangeofintensities,appearances,andevenmulti-phase
particles.5.2.2. Artifacts
CT imaging artifacts such as streak artifacts caused by
beam-hardening can significantly modify the particle’s ap-
pearanceandalsoaffectneighboringparticles,asillustrated
inFigure13.B.Theseartifactsposeaparticularchallengefor
classical segmentation methods. ThreshWater, for example,
tends to segment entire streaks extending from particles,
often leading to the merging of nearby particles. In con-
trast, our approach provides more realistic delineations of
particle borders, even when multiple particles affected by
artifacts are in close proximity. This highlights the robust-
ness against such artifacts of deep learning techniques that
leverage learned image features, as opposed to classical
approaches that primarily rely on image intensities.
5.2.3. Mergers
In section 4.1, a Mergeris defined as a group of two or
more touching particles that are incorrectly identified as a
single instance. Figure 13.C provides examples of merged
particles and correctly separated particles. While Particle-
Seg3D is successful in separating many touching particles,
it sometimes fails when the particles are too intertwined or
when one of the two touching particles is too thin. This is
due to our small core filter as explained in section 3.2.2,
which removes the core of particles with a diameter of only
1-3 voxels, making it difficult to identify them as separate
instances when they touch another particle. However, this
situation is infrequent in practice, and our method produces
a low number of Mergers, as indicated in our quantitative
evaluation in section 5.1. ThreshWater, on the other hand,
produces noticeably more Mergers, particularly when our
method correctly identifies them as separate instances.
5.2.4. Splitters
In section 4.1, a Splitteris defined as a particle that
is incorrectly identified as multiple particles. Figure 13.D
presents examples of Splitterparticles, as well as correctly
identified particles. ParticleSeg3D produces a negligible
number of Splitters, and no direct cause for these few cases
could be identified. In contrast, ThreshWater produces a
significantlyhighernumberof Splitters,particularlyforlow-
contrast samples. This is because poor predictions on such
samplescanresultindisconnectedregionswithinaparticle,
leadingtoa Splitter.Ourmethod,ontheotherhand,isrobust
against these failure cases, as shown by our comparison
results.
5.2.5. Low contrast
Low-contrast samples can pose a challenge in distin-
guishing between the background and particles, making it
difficult to accurately predict particle borders. Figure 13.E
provides examples of low-contrast particles. ThreshWater
is highly affected by low-contrast particles and produces
inconsistentparticlebordersinalmostallcases.Ontheother
hand, ParticleSeg3D is robust against such challenges and
accurately predicts particle borders with consistency, even
under these difficult low-contrast conditions.
Karol Gotkowski et al.: Page 11 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
Figure 13: Qualitative comparison of particles predicted by ParticleSeg3D and ThreshWater.
5.2.6. Missing
In some cases, the inference method may fail to detect a
particle,resultinginnopredictedparticlemask.Figure13.F
shows examples of such missed particles. ParticleSeg3D
mostly misses particles with a diameter below 3 voxels in
low-contrast,high-noiseimages,indicatingthat3voxelsare
the detection limit of our model. While smaller particles
canbedetected,theirreliabilityiscompromised.Hence,we
propose using this detection limit as a guideline for future
work. In contrast, ThreshWater has a worse detection limit
and also fails on larger particles, in which the particle mask
is filtered away due to the noise reduction step. The noise
reduction step is necessary for ThreshWater, but leads to
smaller particles being missed in the process.
5.2.7. Dragonfly qualitative evaluation
We conducted a qualitative evaluation of the samples
fromthedifferenttestswithfoursamplepredictionoverviews
giveninFigure14.ThesamplesinFigure14aandFigure14b
are part of the in-distribution (ID) set, while the samples in
Figure14candFigure14darepartoftheout-of-distribution
(OOD)set.OuranalysisrevealsthatDragonflyiscapableof
generatingaccuratepredictionsontheIDsetasevidencedin
Figure 14a, but can also fail by almost not recognizing anyparticlesasinFigure14b,suggestingitspoorgeneralization
alreadyonIDdata.ThislackofgeneralizationfromDragon-
flyisfurtherexacerbatedontheOODset,byconsistentlynot
recognizingthemajorityofparticles,exhibitingitsinability
to capture the underlying patterns of particle shapes, sizes,
and intensity characteristics necessary to generalize to new,
unseen images.
5.3. Inference time evaluation
We conduct an evaluation on the impact of different
particle and image sizes on the inference time and deduce
advantages and limitations of ParticleSeg3D. Both the in-
distribution and out-of-distribution test sets are used for the
analysis. Given some fixed hardware, here a Nvidia A100
40GMPCIeGPU(Nvidia,SantaClara,USA),theinference
time only depends on the number of patches extracted and
passed to the nnU-Net for a given image (Section 3.3). The
numberofpatches,inturn,dependsontheimagesizeandthe
𝑃𝑎𝑟𝑡𝑖𝑐𝑙𝑒𝑆𝑖𝑧𝑒 invoxels,withthelatterdeterminedmanually
(see Section 3.2.1) and computed with Equation 7.
ParticleSize voxel =ParticleSize mm
VoxelSpacing mm(7)
Karol Gotkowski et al.: Page 12 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
(a) Ore1-Comp3-Concentrate
 (b) Ore3-Comp2
(c) Ore5
 (d) Ore1-Comp4-Feed
Figure 14: Qualitative examples of Dragonfly predictions. The
accuracy of Dragonfly predictions can be reasonably good
for some samples, as in (a), but is very limited for most
other samples, as in (b), (c) and (d). Most particles are not
identified as particles, and those that are, are predicted with
significantly worse accuracy when compared to ParticleSeg3D
or ThreshWater.
Figure15showstheimpactofthedifferentparticlesizeson
the inference time, with different image sizes being repre-
sented as circles with varying diameters. On average, our
datasethasaparticlesizeof60 ±32voxelsandanimagesize
of (997 ±287, 1256 ±218, 1256 ±218) voxels. The inference
time for one image is in the median 4.41 hours. When
inspecting Figure 5.3.D, we see that with a particle size of
∼60 the inference duration is ∼2 hours and with a particle
size of ∼120 this duration is reduced to only ∼20 minutes.
Conversely, with a particle size of ∼40, the inference time
quickly increases to ∼10 hours. This exponential decline
and rise in inference times originates from the particle size
normalization(Section3.2.1)asparticlesneedtoberescaled
resulting in much larger or smaller effective image sizes.
While having a manageable impact down to a particle size
of∼50 with ∼5 hours inference time, our method becomes
significantly slower for smaller particle sizes.
ParticleSize mm =ParticleSize voxel ∗VoxelSpacing mm
(8)
Thus ParticleSeg3D provides fast inference times with ap-
propriate voxel particles sizes, while still being manageable
downtoavoxelparticlesizeof50.Comparedtothecurrent
alternative deep learning methods that require a multi-day
retrainingoneachsampleduetotheirlackofgeneralization,
ParticleSeg3D provides a faster and more accurate indi-
vidual particle characterization. Further, with the goal of
Figure 15: Impact of the voxel particle size on the inference
time. Evaluated on in-distribution and out-of-distribution test
sets with the marker size being proportional to the respective
image size. Expected inference time per voxel particle size is
represented as a dashed interpolated function.
using the model for a large number of samples, we focused
on striking a good balance between segmentation accuracy
and inference times. For narrower application windows, a
specificallyoptimizedtargetparticlesizefortheparticlesize
normalizationalongwitharetrainingofourmodelcouldbe
conceivable to improve inference times and thus reduce the
timeforanindividualparticlecharacterizationevenfurther.
6. Conclusion
Minerals, metals, and plastics are finite and must be
acquired cost-effectively and responsibly through advances
in processing and recycling. Hereby, the characterization of
individualparticlesplaysacrucialroleinenablingandopti-
mizingautomationinthefaceofrisingdemand.Inthewake
ofincreasinglysophisticatedparticleprocessingtechniques,
there is an ever-growing need for characterizing individual
particles over bulk material characterization. Throughput
andtheabilitytoquicklyanalyzenew,uniquesamplesiskey
tocatalyzefutureprogressinparticleprocessingtechnology.
Onekeysteptowardscharacterizingindividualparticlesisto
isolate them from 3D images containing up to tens of thou-
sandsofsuchparticlesthroughaprocesscalledinstanceseg-
mentation. In this manuscript, we presented ParticleSeg3D,
anewdeeplearningframeworkcapableofdoingjustthat.By
leveraging the power of the nnU-Net framework, as well as
the border-core representation to perform instance segmen-
tation,ParticleSeg3Disabletosubstantiallyoutperformthe
currently used approaches in the field with respect to seg-
mentationaccuracy,itsabilitytoseparatetouchingparticles
as well as its robustness to varying particle sizes, imaging
artifacts, and unseen materials. Our extensive evaluation of
the out-of-distribution test set underlines how our method
can be applied even to previously unseen materials without
requiringretraining.Contrarytotraditionalmethods,where
Karol Gotkowski et al.: Page 13 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
new training data needs to be collected and the algorithm
needstoberetrainedforeachimage,oursolutionconstitutes
asubstantialworkflowimprovementandunlockspreviously
inconceivable levels of scalability. For the vast majority of
samples, it can simply be used to make predictions with no
further human input required.
ParticleSeg3D is thus well positioned to greatly enhance
scalability, precision, and automation of individual particle
3D characterization in a standardized way, and will thus act
asacatalystforacceleratingthedevelopmentofprocessing,
and recycling strategies. All code and datasets are made
publicly available along with instructions on how to use
them.
Acknowledgement
PartofthisworkwasfundedbyHelmholtzImaging(HI),
a platform of the Helmholtz Incubator on Information and
Data Science. This work has received financing from the
DeutscheForschungsgemeinschaftwithinSPP2315,project
470202518.
Conflict of Interest
Theauthorsdeclarethattheyhavenoknowncompeting
financial interests or personal relationships that could have
appeared to influence the work reported in this paper.
References
Altini,N.,Brunetti,A.,Napoletano,V.P.,Girardi,F.,Allegretti,E.,Hussain,
S.M.,Brunetti,G.,Triggiani,V.,Bevilacqua,V.,Buongiorno,D.,2022.
A fusion biopsy framework for prostate cancer based on deformable
superellipses and nnu-net. Bioengineering 9, 343.
Baraian, A., Kellokumpu, V., Paaso, J., Koresaar, L., Kaartinen, J.,
2022. Computing particle size distribution of mineral rocks using
deep learning-based instance segmentation, in: 2022 10th European
Workshop on Visual Information Processing (EUVIP), IEEE. pp. 1–6.
Becker, M., Jardine, M., Miller, J., Harris, M., 2016. X-ray computed
tomography: A geometallurgical tool for 3d textural analysis of drill
core. See AusIMM (2016) , 231–240.
Blannin, R., Frenzel, M., Tuşa, L., Birtel, S., Ivăşcanu, P., Baker, T.,
Gutzmer, J., 2021. Uncertainties in quantitative mineralogical studies
using scanning electron microscope-based image analysis. Minerals
Engineering 167, 106836.
Buchmann, M., Borowski, N., Leißner, T., Heinig, T., Reuter, M.A.,
Friedrich,B.,Peuker, U.A.,2020. Evaluationof recyclabilityofaweee
slagbymeansofintegrativex-raycomputertomographyandsem-based
image analysis. Minerals 10, 309.
Budelmann, D., Qing, C., Laue, H.O., Albadry, M., Dahmen, U., Schwen,
L.O., 2023. Segmentation of lipid droplets in histological images, in:
Medical Imaging with Deep Learning, short paper track.
Dominy,S.,Platten,I.,Howard,L.,Elangovan,P.,Armstrong,R.,Minnitt,
R., Abel, R., 2011. Characterisation of gold ores by x-ray computed
tomography–part 2: Applications to the determination of gold particle
sizeanddistribution,in:ProceedingsoftheFirstAusIMMInternational
Geometallurgy Conference, Brisbane, Australia, pp. 5–7.
Fan, M., Zheng, H., Zheng, S., You, C., Gu, Y., Gao, X., Peng, W., Li, L.,
2020. Mass detection and segmentation in digital breast tomosynthesis
using 3d-mask region-based convolutional neural network: a compara-
tive analysis. Frontiers in molecular biosciences 7, 599333.Filippo, M.P., Gomes, O.d.F.M., da Costa, G.A.O.P., Mota, G.L.A., 2021.
Deep learning semantic segmentation of opaque and non-opaque min-
erals from epoxy resin in reflected light microscopy images. Minerals
Engineering 170, 107007.
Furat, O., Kirstein, T., Leißner, T., Bachmann, K., Gutzmer, J., Peuker,
U.A., Schmidt, V., 2023. Multidimensional characterization of particle
morphology and mineralogical composition using ct data and r-vine
copulas. arXiv preprint arXiv:2301.07587 .
Godinho,J.,Kern,M.,Renno,A.,Gutzmer,J.,2019. Volumequantification
in interphase voxels of ore minerals using 3d imaging. Minerals
Engineering 144, 106016.
Godinho,J.R.,Grilo,B.L.,Hellmuth,F.,Siddique,A.,2021. Mountedsin-
gle particle characterization for 3d mineralogical analysis—mspacman.
Minerals 11, 947.
Godinho,J.R.A.,Hassanzadeh,A.,Heinig,T.,2023.3dquantitativemineral
characterizationofparticlesusingx-raycomputedtomography. Natural
Resources Research 32, 479–499.
Grabowski,B.,Ziaja,M.,Kawulok,M.,Longépé,N.,Saux,B.L.,Nalepa,J.,
2022. Self-configuringnnu-netsdetectcloudsinsatelliteimages. arXiv
preprint arXiv:2210.13659 .
Guntoro, P.I., Ghorbani, Y., Parian, M., Butcher, A.R., Kuva, J.,
Rosenkranz, J., 2021. Development and experimental validation of a
texture-based 3d liberation model. Minerals Engineering 164, 106828.
Hassan, N.A., Airey, G.D., Khan, R., Collop, A.C., 2012. Nondestructive
characterisationoftheeffectofasphaltmixturecompactiononaggregate
orientationandsegregationusingx-raycomputedtomography. Interna-
tional Journal of Pavement Research and Technology 5, 84.
Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H., 2021.
nnu-net: a self-configuring method for deep learning-based biomedical
image segmentation. Nature methods 18, 203–211.
Jeong,J.,Lei,Y.,Kahn,S.,Liu,T.,Curran,W.J.,Shu,H.K.,Mao,H.,Yang,
X., 2020. Brain tumor segmentation using 3d mask r-cnn for dynamic
susceptibilitycontrastenhancedperfusionimaging.PhysicsinMedicine
& Biology 65, 185009.
Jiang,S.,Shen,L.,Guillard,F.,Einav,I.,2021. Characterisationoffracture
evolutionofasinglecementedbrittlegrainusingin-situx-raycomputed
tomography. International Journal of Rock Mechanics and Mining
Sciences 145, 104835.
Kern,M.,Akushika,J.N.,Godinho,J.R.,Schmiedel,T.,Gutzmer,J.,2022.
Integrationofx-rayradiographyandautomatedmineralogydataforthe
optimizationoforesortingroutines. MineralsEngineering186,107739.
Kern, M., Kästner, J., Tolosana-Delgado, R., Jeske, T., Gutzmer, J., 2019.
The inherent link between ore formation and geometallurgy as docu-
mentedbycomplextinmineralizationatthehämmerleindeposit(erzge-
birge, germany). Mineralium Deposita 54, 683–698.
Kopelowitz,E.,Engelhard,G.,2019. Lungnodulesdetectionandsegmen-
tation using 3d mask-rcnn. arXiv preprint arXiv:1907.07676 .
Latif, G., Bouchard, K., Maitre, J., Back, A., Bédard, L.P., 2022. Deep-
learning-based automatic mineral grain segmentation and recognition.
Minerals 12, 455.
Li, X., Yang, S., Liu, H., 2022. An unsupervised concrete crack detection
method based on nnu-net, in: The International Conference on Image,
Vision and Intelligent Systems (ICIVIS 2021), Springer. pp. 609–623.
Liu, X., Zhang, Y., Jing, H., Wang, L., Zhao, S., 2020. Ore image
segmentation method using u-net and res_unet convolutional networks.
RSC advances 10, 9396–9406.
Liu, Y., Zhang, Z., Liu, X., Wang, L., Xia, X., 2021. Efficient image
segmentation based on deep learning for mineral image classification.
Advanced Powder Technology 32, 3885–3903.
Nie, X., Zhang, C., Cao, Q., 2022. Image segmentation method on quartz
particle-size detection by deep learning networks. Minerals 12, 1479.
Pereira, L., Frenzel, M., Khodadadzadeh, M., Tolosana-Delgado, R.,
Gutzmer,J.,2021. Aself-adaptiveparticle-trackingmethodforminerals
processing. Journal of Cleaner Production 279, 123711.
Sofroniew, N., Lambert, T., Evans, K., Nunez-Iglesias, J., Bokota, G.,
Winston, P., Peña-Castellanos, G., Yamauchi, K., Bussonnier, M., Pop,
D.D.,etal.,2022. napari:amulti-dimensionalimageviewerforpython.
Zenodo .
Karol Gotkowski et al.: Page 14 of 15ParticleSeg3D: A Scalable Out-of-the-Box Deep Learning Segmentation Solution for Individual Particle Characterization from
Micro CT Images in Mineral Processing and Recycling
Spronck, J., Gelton, T., van Eekelen, L., Bogaerts, J., Tessier, L., van
Rijthoven, M., van der Woude, L., van den Heuvel, M., Theelen, W.,
van der Laak, J., et al., 2023. nnunet meets pathology: bridging the gap
forapplicationtowhole-slideimagesandcomputationalbiomarkers,in:
Medical Imaging with Deep Learning.
Sun,G.,Huang,D.,Cheng,L.,Jia,J.,Xiong,C.,Zhang,Y.,2022. Efficient
and lightweight framework for real-time ore image segmentation based
on deep learning. Minerals 12, 526.
Tang,K.,DaWang,Y.,Niu,Y.,Honeyands,T.A.,O’Dea,D.,Mostaghimi,
P.,Armstrong,R.T.,Knackstedt,M.,2023. Particleclassificationofiron
ore sinter green bed mixtures by 3d x-ray microcomputed tomography
and machine learning. Powder Technology 415, 118151.
Tung,P.K.M.,Halim,A.Y.,Wang,H.,Rich,A.,Marjo,C.,Regenauer-Lieb,
K., 2022. Deep-xfct: Deep learning 3d-mineral liberation analysis with
micro-x-rayfluorescenceandcomputedtomography. Energies15,5326.
Van der Walt, S., Schönberger, J.L., Nunez-Iglesias, J., Boulogne, F.,
Warner,J.D.,Yager,N.,Gouillart,E.,Yu,T.,2014. scikit-image:image
processing in python. PeerJ 2, e453.
Wang, G., Zuluaga, M.A., Li, W., Pratt, R., Patel, P.A., Aertsen, M., Doel,
T.,David,A.L.,Deprest,J.,Ourselin,S.,etal.,2018. Deepigeos:adeep
interactive geodesic framework for medical image segmentation. IEEE
transactions on pattern analysis and machine intelligence 41, 1559–
1572.
Wang, W., Feng, R., Chen, J., Lu, Y., Chen, T., Yu, H., Chen, D.Z., Wu,
J., 2019. Nodule-plus r-cnn and deep self-paced active learning for 3d
instance segmentation of pulmonary nodules. Ieee Access 7, 128796–
128805.
Wang, W., Li, Q., Xiao, C., Zhang, D., Miao, L., Wang, L., 2021. An
improved boundary-aware u-net for ore image semantic segmentation.
Sensors 21, 2615.
Wang,Y.,Lin,C.,Miller,J.,2015. Improved3dimagesegmentationforx-
raytomographicanalysisofpackedparticlebeds. MineralsEngineering
83, 185–191.
Wang, Y., Lin, C., Miller, J., 2016. 3d image segmentation for analysis of
multisize particles in a packed particle bed. Powder Technology 301,
160–168.
Winardhi, C.W., da Assuncao Godinho, J.R., Rachmawati, C., Achin, I.D.,
Iturbe,A.U.,Frisch,G.,Gutzmer,J.,2022. Aparticle-basedapproachto
predict the success and selectivity of leaching processes using ethaline-
comparison of simulated and experimental results. Hydrometallurgy
211, 105869.
Xiao,D.,Liu,X.,Le,B.T.,Ji,Z.,Sun,X.,2020.Anoreimagesegmentation
method based on rdu-net model. Sensors 20, 4979.
Zhao, M., Liu, Q., Jha, A., Deng, R., Yao, T., Mahadevan-Jansen, A.,
Tyska, M.J., Millis, B.A., Huo, Y., 2021. Voxelembed: 3d instance
segmentation and tracking with voxel embedding based deep learning,
in:MachineLearninginMedicalImaging:12thInternationalWorkshop,
MLMI 2021, Held in Conjunction with MICCAI 2021, Strasbourg,
France, September 27, 2021, Proceedings 12, Springer. pp. 437–446.
Zhao, Z., Yang, L., Zheng, H., Guldner, I.H., Zhang, S., Chen, D.Z., 2018.
Deep learning based instance segmentation in 3d biomedical images
using weak annotation, in: Medical Image Computing and Computer
Assisted Intervention–MICCAI 2018: 21st International Conference,
Granada, Spain, September 16-20, 2018, Proceedings, Part IV 11,
Springer. pp. 352–360.
Zhou, X., Dai, N., Cheng, X., Thompson, A., Leach, R., 2021. Three-
dimensional characterization of powder particles using x-ray computed
tomography. Additive Manufacturing 40, 101913.
Karol Gotkowski et al.: Page 15 of 15