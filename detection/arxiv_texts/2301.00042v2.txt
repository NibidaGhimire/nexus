Quantifying the Expressive Capacity of Quantum Systems: Fundamental Limits and Eigentasks
Fangjun Hu,1,Gerasimos Angelatos,1, 2,Saeed A. Khan,1Marti Vives,1, 3Esin
T¬®ureci,4Leon Bello,1Graham E. Rowlands,2Guilhem J. Ribeill,2and Hakan E. T ¬®ureci1
1Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA
2Raytheon BBN, Cambridge, MA 02138, USA
3Q-CTRL, Santa Monica, CA 90401, USA
4Department of Computer Science, Princeton University, Princeton, NJ 08544, USA
(Dated: April 20, 2023)
The expressive capacity of quantum systems for machine learning is limited by quantum sampling noise
incurred during measurement. Although it is generally believed that noise limits the resolvable capacity of
quantum systems, the precise impact of noise on learning is not yet fully understood. We present a mathematical
framework for evaluating the available expressive capacity of general quantum systems from a Ô¨Ånite number of
measurements, and provide a methodology for extracting the extrema of this capacity, its eigentasks. Eigentasks
are a native set of functions that a given quantum system can approximate with minimal error. We show that
extracting low-noise eigentasks leads to improved performance for machine learning tasks such as classiÔ¨Åcation,
displaying robustness to overÔ¨Åtting. We obtain a tight bound on the expressive capacity, and present analyses
suggesting that correlations in the measured quantum system enhance learning capacity by reducing noise in
eigentasks. These results are supported by experiments on superconducting quantum processors. Our Ô¨Åndings
have broad implications for quantum machine learning and sensing applications.
I. INTRODUCTION
Learning with quantum systems is a promising application
of near-term quantum processors, with several recent demon-
strations in both quantum machine learning (QML) [1‚Äì6] and
quantum sensing [7‚Äì9]. A broad class of such data-driven ap-
plications proceed by embedding data into the evolution of
a quantum system, where the embedding, dynamics, and ex-
tracted outputs via measurement are all governed by a set of
general parameters [10‚Äì12]. Depending on the learning
scheme, different components of this general framework
may be trained for optimal performance of a given task. In
all cases the fundamental role of the quantum system is that
of a high-dimensional feature generator: given inputs u, a set
of frequencies for the occurrence of different measurement
outcomes act as a parameterized feature vector implementing
a functionf(u)that minimizes a chosen loss function (see
Fig. 1). The relationship between the physical structure of
the model and the function classes that can be expressed with
high accuracy is a fundamental question of basic importance
to the success of quantum models. Recent results have begun
to shed light on this important question and provide guidance
on the choice of parameterized quantum models [13‚Äì21]. Yet
when it comes to experimental implementations, the presence
of noise is found to substantially curtail theoretical expecta-
tions for performance [1‚Äì3].
Given an input uto a general dynamical system, we de-
Ô¨Åne its Expressive Capacity (EC) as a measure of the accu-
racy with which Klinearly independent functions ff(u)gof
the input can be constructed from Kmeasured features. This
These two authors contributed equallyis a suitable generalization of the Information Processing Ca-
pacity introduced in Ref. [22] to noisy systems, and a direct
quantiÔ¨Åcation of the information content of these measured
features. A central challenge in determining the EC for quan-
tumsystems is the fundamentally stochastic nature of mea-
surement outcomes. Even when technical noise due to system
parameter Ô¨Çuctuations is minimized as in an error-corrected
quantum computer, there is a fundamental level of noise, the
quantum sampling noise (QSN), which cannot be eliminated
in learning with quantum systems. Quantum sampling noise
therefore sets a fundamental limit to the EC of any physi-
cal system. Although QSN is well-understood theoretically,
a formulation of its impact on learning is a challenging task
as it is strongly determined by the quantum state of the sys-
tem relative to the measurement basis, and is highly correlated
when quantum coupling is present. Consequently, the impact
of QSN is often ignored [10‚Äì12, 23, 24] (with a few excep-
tions [13, 25‚Äì27]), even though it can place strong constraints
on practical optimization [25] and performance [26].
In this article, we develop a mathematical framework to
quantify the EC that exactly accounts for the structure of QSN,
providing a tight bound for a quantum system with Kmea-
surement outcomes under Ssamples, and illustrate how a
mathematical framework for its quantiÔ¨Åcation can guide ex-
perimental design for QML applications. While the strength
of the EC lies in its generality, we provide numerical ex-
amples and experimental results conÔ¨Årming that higher EC
is typically indicative of improved performance on speciÔ¨Åc
tasks. As such, the EC provides a metric to guide ans ¬®atze-
design for improved learning performance in a task-agnostic
and parameter-independent manner. SpeciÔ¨Åcally, our work
identiÔ¨Åes enhancement in measurable quantum correlations as
a general principle to increase the EC of quantum systems un-
der Ô¨Ånite sampling.arXiv:2301.00042v2  [quant-ph]  18 Apr 20232
Input       dimensional 
 input domain
Output           under finite sampling Feature generator(a)
(b)
Individual function 
capacity:Function approximation
e.g. frequency of 
occurence of bitstrings in 
computational basis Distributions for distinct
depend onQuantum system
Quantum 
annealerse.g.Parameterized 
Quantum CircuitsTarget:
Learned Estimate:
Learned linear weights e.g.    -Qubit system
Measurement
Increased
sampling
stochastic features
FIG. 1. (a) Representation of the learning framework considered in
this work: inputs uare transformed to a set of outputs via a parame-
terized feature generator, here implemented using a Ô¨Ånitely-sampled
quantum system as shown in (b). Inputs are encoded in the state
of a quantum system via a general quantum channel U, and infor-
mation is then extracted through a positive operator-valued measure.
This framework describes a wide range of practical quantum sys-
tems, from quantum circuits used in supervised or generative quan-
tum machine learning, to quantum annealers exhibiting continuous
evolution, and beyond, all deÔ¨Åned by a general quantum channel with
parameters. Extracted information takes the form of Kstochastic
features Xobtained under Ô¨Ånite shots S. The geometric structure
of distributions of these measured features is fundamentally deter-
mined by quantum sampling noise, which depends on the quantum
state ^(u;), and hence on the nature of the mapping from input u
to this quantum state. We show four obtained distributions differ-
ing only in the values of inputs uto highlight this dependence. As
shown in (a), learned estimates for desired functions are then con-
structed via a linear combination wofX, with a resolution limited
byS. CapacityC[f]then quantiÔ¨Åes the error in the approximation
of a target function fvia this scheme.
Our work goes beyond simply deÔ¨Åning the EC as a Ô¨Åg-
ure of merit for parameterized quantum systems, however. In
particular, we offer a reliable methodology to identify the na-
tive function set that is most accurately resolvable by a given
encoding under Ô¨Ånite sampling parameterization under QSN.
Equivalently, we show that this deÔ¨Ånes a construction of mea-
sured features spanning the accessible information which is
optimally robust to noise in readout, thereby furnishing a criti-
cal tool employable in any QML framework to improve learn-
ing in experimental settings. This strategy for deÔ¨Åning the
noise-constrained EC naturally focuses on accessible noisy
output features under a speciÔ¨Åed measurement scheme, as op-
posed to unmeasured degrees of freedom. This makes the EC
an efÔ¨Åciently-computable quantity in practice, as we demon-
strate using both numerical simulations and experiments on
IBM Quantum‚Äôs superconducting multi-qubit processors [28].
II. THEORETICAL ANALYSISA. Quantum Sampling Noise and Learning
The most general approach to learning from data using a
generic quantum system is depicted schematically in Fig. 1.
A table with symbols and abbreviations used in the text can
be found in Appendix A. Any quantum learning scheme be-
gins with embedding the data uthrough a quantum channel
parameterized by acting on a known initial state,
^(u;) =U(u;)^0: (1)
This channel includes all quantum operations applied to the
input data; to obtain the computational output or perform
further classical processing, one must extract information
from the quantum system via a set of measurements de-
scribed most generally as a positive operator-valued measure
(POVM). SpeciÔ¨Åcally, we deÔ¨Åne a set of KPOVM elements
f^Mkg, each associated with a distinct measurement outcome
indexedk, and constrained only by the normalization condi-
tionPK 1
k=0^Mk=I(and hence not necessarily commuting).
A single measurement or ‚Äúshot‚Äù then yields a discrete in-
dexk(s)(u)specifying the observed outcome: for input u, if
outcomekis observed in shot sthenk(s)(u) k. Measured
features are then constructed by ensemble-averaging over S
repeated shots:
Xk(u) =1
SX
s(k(s)(u);k) (2)
Hence Xk(u)in this case is the empirical frequency of oc-
currence of the outcome kinSrepetitions of the experiment
with the same input u. These measured features are formally
random variables that are unbiased estimators of the expected
value of the corresponding element ^Mkas computed from
^(u). Explicitly
limS!1Xk(u) =xk(u)Trf^Mk^(u;)g; (3)
so thatxkis the probability of occurrence of the kth out-
come as speciÔ¨Åed by the quantum state. These probability
amplitudes encompass the accessible information in ^(u;):
any observable under this set can be written as a linear com-
bination of POVM elements ^OW=P
kWk^Mk, such that
h^OWi=WTx.
In QML theory, it is standard to consider the limit S!1 ,
and to thus use expected features fxk(u)gfor learning. In any
actual implementation however, measured features fXk(u)g
must be constructed under Ô¨Ånite S, in which case their funda-
mentally quantum-stochastic nature can no longer be ignored.
More precisely, Xare samples from a multinomial distribu-
tion withStrials andKcategories, which can be decomposed
into their expected value ‚Äì the quantum-mechanical event
probabilitiesx‚Äì and a zero-mean, input-dependent noise term
(u):
X(u) =x(u) +1p
S(u); (4)3
Hereencodes the multinomial statistics of QSN; it has non-
zero cumulants of all orders, of which the covariances take the
particularS-independent form,
Cov[j;k](u)jk(u) =jkxk(u) xj(u)xk(u)(5)
or more concisely = diag(x) xxT. We note that the
above expressions are exact; the factor of 1=p
Sis merely
extracted for convenience of the analysis to follow, and in
particular is notmeant to suggest an expansion for large S;
cumulants ofbeyond second-order inherit a complicated S-
dependence [29].
Before developing our capacity analysis, we note that when
viewed in isolation Eq. (4) deÔ¨Ånes an extremely general map
between inputs uand outputs X(u)assembled from Smea-
surements. The readout features it describes could therefore
have been extracted from anydynamical system with stochas-
tic outputs, including systems that are entirely classical. This
is no restriction ‚Äì Eq. (4) also applies to a very broad class
of quantum systems: ultimately, measurement outcomes from
quantum systems are also recorded by an observer as classi-
cal stochastic variables. Where, then, is the quantum nature
of measured features apparent? This is encoded in the fact
that for quantum systems all statistical properties of stochastic
readout features X(u)‚Äì namely Ô¨Årst-order cumulants x(u),
second-order cumulants (u), and all higher-order cumulants
‚Äì are determined explicitly by the quantum state ^(u), which
itself may be a distribution that is hard to generate classically.
The framework we develop here allows characterization of
the function-learning capacity of any noisy dynamical system
satisfying Eq. (4), and provides a practical, experimentally ap-
plicable methodology to optimize learning by avoiding over-
Ô¨Åtting to noise in ML tasks. When applied to classical sys-
tems, it can be viewed as a means of statistical inference based
on data containing classical noise [30]. However, our primary
interest is the study of quantum systems, where the funda-
mental model for the noise process depends nontrivially on
the quantum state, a dependence we account for exactly. This
enables us to extract the limits on function-learning capacity
set by QSN and its dependence on the encoding. In this paper,
using both theoretical studies and experiments on real quan-
tum devices, we analyze how this capacity depends on quan-
tum properties such as the degree of measured correlations,
and how our framework can be applied for optimal learning in
practical QML tasks such as classiÔ¨Åcation.
B. Expressive Capacity
Returning to the situation depicted in Fig. 1, QML and
quantum sensing can generically be cast as encoding data in a
parameterized quantum system, and then using measurement
outcomes to approximate a desired function f(u)(here as-
sumed to be square-integrable Eu[jf(u)j2]<1). The in-
put data is deÔ¨Åned with respect to a distribution p(u)which
can be continuous or discrete: Eu[f]R
dup(u)f(u)'1
NP
nf(u(n))for i.i.d. sampling obeying u(n)p(u)for
alln2[N]. Recalling that features X(u)are estimators of
POVM expectation values ‚Äì the linear combination of which
can be used to construct all accessible observables ‚Äì f(u)is
approximated for Ô¨Ånite SasfW(u) =WTX(u). To quan-
tify the Ô¨Ådelity of this approximation, we introduce the func-
tional capacity [13, 22, 24], which is simply the normalized
mean-squared accuracy of the estimate fW
C[f] = 1 min
W2RKEu[jf(u) fW(u)j2]
Eu[jf(u)j2]: (6)
Minimizing error in the approximation of f(u)byfW(u)
over the input domain to determine capacity thus requires
Ô¨Åndingw= argminWEu[jf WTXj2], which can be al-
ways be expressed analytically via a pseudoinverse operation
(see Appendix C). This function capacity is constructed such
that0C[f]1.
The choice of a linear estimator and a mean squared er-
ror loss function may appear restrictive at Ô¨Årst glance, but the
generality of our formalism averts such limitations. The use
of a linear estimator applied directly to readout features ap-
pears to preclude classical nonlinear post-processing of mea-
surements; however, this is simply to ensure the calculated
functional capacity is a measure of the parameterized quan-
tum system itself, and not of a classical nonlinear layer. Fur-
thermore, the mean squared loss effectively describes the Ô¨Årst
term in a Taylor expansion of a wide range of arbitrary non-
linear post-processing and non-quadratic loss functions (see
Appendix C 5).
To extend the notion of capacity to a task-independent met-
ric representing how much classical information about an in-
put can be extracted from a system in the presence of noise, we
sum the function capacity over a basis of functions ff`g`2N
which are complete and orthonormal with respect to the input
distribution, i.e. equipped with the inner product hf`;f`0ip=R
f`(u)f`0(u)p(u)du=``0. The total Expressive Capacity
(EC) is then CTP1
`=0C[f`];which effectively quantiÔ¨Åes
how many linearly-independent functions can be expressed
from a linear combination of fXk(u)g. Our main result ‚Äì
proven in detail in Appendix C 4 ‚Äì is that given any S2N+,
the EC for a physical system whose measured features are
stochastic variables of the form of Eq. (4) is given by
CT() = Tr 
G+1
SV 1
G!
=K 1X
k=01
1 +2
k()=S:(7)
The Ô¨Årst equality, arrived at through straight-forward alge-
braic manipulation, is written in terms of the expected fea-
ture Gram and covariance matrices GEu[xxT]andV
Eu[]respectively. We later demonstrate that these expected
quantities can be accurately estimated in experiment and con-
sequently under Ô¨Ånite S(see Appendix D 1 and Eq. (D1)). The
second equality remarkably provides a closed-form expres-
sion forCTat anyS, which is independent of the generally-
inÔ¨Ånite setff`g`2N(and thus not subject to numerical chal-
lenges associated with its evaluation over such a set [22]). In-
stead the EC is entirely captured by the function capacity of4
Kdistinct functions, and for a given physical system is fully
characterized by the spectrum of eigenvalues f2
kgk2[K]sat-
isfying the generalized eigenvalue problem
Vr(k)=2
kGr(k): (8)
In the above, all quantities depend on and thus the speciÔ¨Åc
physical system and input embedding via the Gram ( G) and
covariance ( V) matrices. Associated with each 2
kis an eigen-
vectorr(k)living in the space of measured features and thus
deÔ¨Åning a set of Korthogonal functions via the linear trans-
formation
y(k)(u) =X
jr(k)
jxj(u) (9)
We refer tofy(k)gaseigentasks , as they form the minimal set
of orthonormal functions ( Eu[y(j)y(k)] =jk) which com-
pletely accounts for the EC of a physical system and thus the
accessible information content present in its measured fea-
tures. SpeciÔ¨Åcally, the capacity to approximate a given y(k)
withSshots isC[y(k)] = 1=(1 +2
k=S): the EC in Eq. (7) is
simply a sum of eigentask capacities. This further highlights
that a given parameterized system can only approximate a tar-
get function to the degree that it can be written as a linear
combination offy(k)g. The eigentasks thus serve as a power-
ful basis for learning, as shall be explored in Sec. III.
DeÔ¨Åning measured eigentasks y(k)(u) =P
jr(k)
jXj(u),
we Ô¨Ånd (see Appendix C 3) that fr(k)gspecify a unique linear
transformation that simultaneously orthogonalizes not only
the signal, but also the associated noise: Eu[y(j)y(k)] =
jk(1 +2
k=S). The term 2
k=Sis thus the mean squared
error, or noise power, associated with the approximation of
eigentasky(k); equivalently, y(k)has a signal-to-noise ratio of
S=2
k. This leads to a natural interpretation of f2
kgas noise-
to-signal (NSR) eigenvalues. The eigentasks, ordered in in-
creasing noise strength 02
02
12
K 1<1,
are the orthogonal set of functions maximally robust to noise.
Having developed our framework for EC in the most gen-
eral context, in the remainder of this paper we will use it to
analyze quantum systems in particular. The same quantitative
metrics ‚Äì EC, eigentasks, and NSR eigenvalues ‚Äì now carry
the signiÔ¨Åcance of being determined by an arbitrary parame-
terized quantum state ^(u;), whose data-dependence ideally
can be hard to model classically. Our formulation of EC hence
encompasses general quantum states, to the best of our knowl-
edge the Ô¨Årst of its kind, going beyond characterizations of
noise-constrained capacity that have been attempted for linear
classical systems [31] and Gaussian quantum systems [26].
The eigentasks then reveal the set of orthogonal functions best
approximated by the quantum system, and hence are sensitive
to properties such as the degree of quantum correlations. Fi-
nally, the Ô¨Ådelity of approximation of these native functions
‚Äì determined by NSR eigenvalues ‚Äì is constrained fundamen-
tally by QSN.
From Eq. (7) we have limS!1CT= RankfGg, where
RankfGg=K, the number of measured features, providedno special symmetries exist (see Appendix C 6). This impor-
tant result reveals that in the absence of noise all dynamical
systems ‚Äì independent of pararameterization ‚Äì have a capac-
ity which is simply the number of independent accessible de-
grees of freedom [22, 31] The generic exponential scaling
of measured degrees of freedom with quantum system-size
(e.g.K= 2LforL-qubit systems subject to a computational
basis measurement) is often-cited as a motivator for perform-
ing ML with quantum systems [13, 24, 32]. However, as will
be demonstrated shortly, the EC of quantum systems can be
signiÔ¨Åcantly reduced from this limit for Ô¨Ånite Sin a way that
strongly depends on the encoding. By evaluating the ability of
quantum systems to accurately express functions in the pres-
ence of QSN, the capacity analysis above provides an impor-
tant metric to asses the utility of quantum platforms for learn-
ing in practice.
C. Expressive Capacity of Quantum 2-designs
We Ô¨Årst consider the EC of quantum 2-designs: sys-
tems with Ô¨Åxed that map inputs to a unitrary ensemble
fp(u)du;^U(u;)gwhose Ô¨Årst and second moments agree
with those from a uniform (Haar) distribution of unitaries.
Quantum 2-designs are important to recent QML studies
[11, 21] due to their role in deÔ¨Åning ‚Äúexpressibility‚Äù [16, 17]:
a metric quantifying how close a parameterized quantum sys-
tem is to such a 2-design. The capacity eigenproblem Eq. (8)
for any quantum 2-design over K-dimensions can be solved
analytically (see Appendix G), yielding a Ô¨Çat spectrum of
NSR eigenvalues 2
k=K(1 k0). This results in an EC
CT=K¬∑S+ 1
S+K; (10)
which at Ô¨Ånite Scan be signiÔ¨Åcantly lower than K. For qubit-
based systems with K= 2L, allk6= 0 eigentasks have a
noise strength 2L=S, requiringSto grow exponentially with
qubit-number Lin order to extract useful features.
A quantum 2-design is thought of as having maximal ‚Äúex-
pressibility‚Äù, however we see that its EC always vanishes ex-
ponentially with system size for a Ô¨Åxed Ô¨Ånite S. It is exactly
such systems that have been shown to lead to barren plateaus
which preclude learning [21]. To emphasize the distinction
with ‚Äúexpressibility‚Äù, we note that EC reÔ¨Çects how much clas-
sical information can be extracted from the entire ‚Äúquantum
computational stack‚Äù in practice: from an abstract algorithm,
to the quantum hardware on which its implemented, and the
classical electronics used for control and readout. EC re-
quires only noisy computational outputs fXk(u)gand is thus
efÔ¨Åciently-computable in experiment ‚Äì unlike more abstract
metrics [7, 16, 17] ‚Äì yielding a directly relevant metric for
learning with quantum hardware.
III. EXPERIMENTAL RESULTS
To demonstrate the practical utility of our framework, we
now show how the spectrum f2
kg, the EC, and eigentasks5
can all be computed for real quantum devices in the presence
of parameter Ô¨Çuctuations and device noise. We reiterate at
the outset that our approach for quantifying the EC of a quan-
tum system is very general, and can be applied to a variety of
quantum system models. For practical reasons, we perform
experiments on L-qubit IBM Quantum (IBMQ) processors,
whose dynamics is described by a parameterized quantum cir-
cuit containing single and two-qubit gates. However, as an
example of the broad applicability of our approach, in Ap-
pendix E we compute the EC for L-qubit quantum annealers
via numerical simulations, governed by the markedly different
model of continuous-time Hamiltonian dynamics.
On IBMQ devices, resource limitations restrict our com-
putation of EC to 1D inputs uthat are uniformly distributed,
p(u) = Unif[ 1;1], see Fig. 2(a). SpeciÔ¨Åcally, we are limited
toN= 300 distinct inputs; a 1D distribution then ensures fea-
turesfXk(u)gare sufÔ¨Åciently densely sampled to approach
the continuum limit, and are also easy to visualize. We em-
phasize that this analysis can be straightforwardly extended
to multi-dimensional and arbitrarily-distributed inputs given
suitable hardware resources, without modifying the form of
the Gram and covariance matrices.
We are only now required to specify the model of the quan-
tum system, and choose an ansatz tailored to be natively im-
plementable on IBMQ processors (see Appendix B). We Ô¨Åx
^0=j0ih0j
L; note, however, that any other initial state may
be implemented via an additional unitary and absorbed into
the ‚Äúencoding‚Äù, i.e. the quantum channel U(u;)of Eq. (1).
In this way, the dependence of EC on initial states could be
explored in future studies.
The circuit we choose consists of 2Nrepetitions of
the same input-dependent circuit block depicted in Fig. 2(a).
The block itself is of the form Rx(x=2)W(J)Rz(z+
Iu)Rx(x=2), whereRx=z are Pauli-rotations applied
qubit-wise, e.g.Rz=Q
lRz(z
l+I
lu). A two-qubit cou-
pling gate acts between physically connected qubits in the de-
vice and can be written as W(J) =Q
hl;l0iexpf iJ
2^z
l^z
l0g.
Within the structure of this ansatz, we will choose all single-
qubit rotation parameters randomly: x=z
lUnif[0;2]and
I
lUnif[0;10], generally representing a circuit trained for
a particular unspeciÔ¨Åed task. Each instance of random param-
eters, along with associated dissipative processes, speciÔ¨Åes the
quantum channelU(u;)which we refer to as an ‚Äúencoding‚Äù.
We will study the performance of an overall ansatz by looking
at the behavior averaged across encodings as hyperparameters
such asJare varied. In this work we also choose = 3,
which limits circuit depth and associated prevalence of gate
errors, while still generating a complex state with correlation
generally distributed throughout all qubits.
Finally, we consider feature extraction via a computational
basis measurement as is standard in quantum information pro-
cessing: the POVM elements are the K= 2Lprojectors
^Mk=jbkihbkj, wherebkis theL-bit binary representation
of the integer k. However, as with state preparation, measure-
ments in any other basis can be (and in practice, are) real-
ized using an additional unitary prior to computational basisreadout, whose effect can similarly be analyzed as part of the
general encodingU(u;).
Note that for this ansatz, the choice J= 0 ( mod)yields
eitherW=^Ior^z
^z, both of which ensure ^(u)is a
product state and measured features are simply products of
uncorrelated individual qubit observables ‚Äì equivalent to a
noisy classical system. Starting from this product system (PS),
tuning the coupling J6= 0 ( mod)provides a controllable
parameter to realize a quantum correlated system (CS), for
which the 2L-dimensional multinomial distribution x(u)can-
not be represented as a tensor product of Lmarginal binomial
distributions on each qubit. In general, such non-product sys-
tems intuitively result in u-dependent quantum states which
exhibit entanglement and can potentially be more difÔ¨Åcult to
describe classically. This control enables us to address a natu-
ral question regarding EC of quantum systems under Ô¨Ånite S:
what is the dependence of EC and realizable eigentasks on J,
and hence on quantum correlations?
A. Expressive Capacity of Quantum Circuits
To perform the capacity analysis, one must extract mea-
sured features from the quantum system as the input uis
varied, as exempliÔ¨Åed in Fig. 2(a) for the IBMQ ibmq perth
device. For comparison, we also show ideal-device simula-
tions (unitary evolution, no device noise), where slight devia-
tions are observed. The agreement with experimental results
is improved when the effects of gate errors, readout errors,
and qubit relaxation are included, hereafter referred to as ‚Äúde-
vice noise‚Äù simulations, highlighting both the non-negligible
role of device nonidealities, and that our analysis incorporates
them.
The measured features under Ô¨Ånite Sare used to estimate
the Gram and covariance matrices (see detailed techniques in
Appendix D), and to therefore solve the eigenproblem Eq. (8)
for NSR eigenvalues f2
kg. Typical NSR spectra computed
for a random encoding (i.e. set of rotation parameters) on the
device are shown in Fig. 2(b), for J= 0 (PS) andJ==2
(CS), together with corresponding spectra from device noise
simulations, with which they agree well. We note that at lower
k, the device NSR eigenvalues are larger than those from ideal
simulations, and at larger kdeviate from the direct exponential
increase (with order) seen in ideal simulations. Both these
effects are captured by device noise simulations as well and
can therefore be attributed to device errors and dissipation.
The NSR spectra therefore can serve as an effective diagnostic
tool for quantum processors and encoding schemes.
The NSR spectra can be used to directly compute the EC
of the corresponding quantum device for Ô¨Ånite S, via Eq. (7).
Practically, at a given Sonly NSR eigenvalues 2
k.Scon-
tribute substantially to the EC. An NSR spectrum with a Ô¨Çat-
ter slope therefore has more NSR eigenvalues below S, which
gives rise to a higher capacity. Fig. 2(b) shows that the CS6
+ ++C-NOT+ +
+ ++ ++ +Input
CS
PS
Order(a)
(b) (c)
Shots CouplingIBM Perth
Output
Experiment Experiment Simulations SimulationsEstimate
Calculate
Device 
encodingDevice 
encodingDevice noise
Device noise IdealIdeal
Device noise IdealMean over 8 
random encodings: 
Ideal sim.Device noise
sim.Expt.
FIG. 2. (a) A representation of the EC analysis, featuring the
IBMQ Perth device and a schematic of the quantum circuit consid-
ered in this section. On the right, the speciÔ¨Åc feature plotted is X1(u)
(b1= 000001 ) withS= 214shots. (b) Left panel: Device noise-
to-signal spectrum 2
kfor a speciÔ¨Åc encoding as a correlated sys-
tem (CS),J==2(blue crosses) and product system (PS), J= 0
(brown diamonds). Ideal (solid) and device noise (dashed) simula-
tions are also shown. Note the agreement between device and simu-
lation, along with distortion from more direct exponential growth in
2
kwithkin the ideal case, due to device errors. Right panel: CT
vs.Scalculated from the left panel. At a given S, theCTcan be
approximated by performing the indicated sum over all 2
k<S. (c)
Expressive capacity CT(top panel) and expected total correlation
T(lower panel) for the chosen encoding under S= 214from the
IBM device, and device noise simulations (dashed peach). Average
metrics over 8 random encodings for device noise (solid peach) and
ideal (solid gray) simulations are also shown. The S!1 expres-
sive capacity of these encodings always attains the maxfCTg= 64 ,
indicated in dashed red.
generally exhibits an NSR spectrum with a Ô¨Çatter slope than
the PS, yielding a larger capacity for function approximation
across all sampled S.
To more precisely quantify the role of quantum correlations
in EC, we introduce the expected total correlation (ETC) of
the measured state over the input domain of u[33, 34],
T=Eu"LX
l=1S(^M
l(u)) S(^M(u))#
; (11)
where ^M(u)P
k^kk(u)jbkihbkjis the post-measured
state, S(¬∑)is the von Neumann entropy (see Appendix H),
and^l= Tr [L]nflgf^gis the reduced density matrix. There-
fore, non-zero ETC signals the generation of quantum states
over the input domain uthat on average have nontrivial corre-
lations amongst their constituents, including for example pure
many-body states that are entangled. We now compute EC
and ETC using S= 214in Fig. 2(c) as a function of J, for thesame random encoding considered above on the device. We
note that the experimental results show excellent agreement in
both cases with the corresponding device noise simulation; we
also show average EC at S= 214and ETC across 8 random
encodings in both ideal and device noise simulations. The in-
Ô¨Çuence of individual encodings, i.e., random rotation param-
eters, is seen to manifest as small deviations from the overall
EC trend governed by global hyperparameters, such as Jhere
(orLin Appendix Fig. 9). This justiÔ¨Åes our choice of ran-
dom circuits to evaluate the overall capacity of an ansatz for
learning.
We note that product states by deÔ¨Ånition have T= 0[35];
this is seen in ideal simulations for J= 0 ( mod). How-
ever, the actual device retains a small amount of correlation
at this operating point, which is reproduced by device noise
simulations. This can be attributed to gate or measurement
errors as well as cross-talk, the latter being especially rele-
vant for the transmon-based IBMQ platform with a parasitic
always-on ZZ coupling [36]. With increasing J,Tincreases
and peaks around J=2 (mod); interestingly, CTalso
peaks for the same coupling range. From the analogous plot
of EC, we clearly see that at Ô¨Ånite S, increased ETC appears
directly correlated with higher EC. We have observed very
similar behaviour using completely different quantum system
models (see Appendix Fig. 6 [37, 38]). This indicates the util-
ity of enhancing quantum correlations as a means of improv-
ing the general expressive capability of quantum systems.
We caution that this connection between measurement cor-
relations and EC is an observed trend, rather than a law de-
rived from Ô¨Årst principles. One can come up with contrived
situations where increasing correlation has no effect on EC:
for example, appending a layer of CNOT gates directly prior
to measurement will generally increase the ideal ETC of any
ansatz. For measured features however this amounts to a sim-
ple shufÔ¨Çing of labels xk(u)$xk0(u), thus yielding the
same NSR spectrum and EC. The input, quantum-state, and
feature mapping ultimately governs EC: only increases in cor-
relation that also increase the complexity of the measured
features‚Äôu-dependence (as achieved via the intermediate W
gates here) are beneÔ¨Åcial from the perspective of information
processing.
As a Ô¨Ånal important point, note that at Ô¨Ånite S, even with
increased quantum correlations, the maximum EC is still sub-
stantially lower than the upper bound of K= 64 . This re-
mains true even for ideal simulations, and over several ran-
dom encodings, so the underperformance cannot be attributed
to device noise or poor ansatz choice respectively. It is worth
emphasizing that the impact of device noise is captured in the
small EC gap between the ideal and noise simulation curves,
with the remainder of the reduction from K= 64 attributable
to QSN alone. These results clearly indicate that the resulting
sampling noise at Ô¨Ånite Sis the fundamental limitation for
QML applications on this particular IBM device, rather than
other types of noise sources and errors.7
Input
TargetDistinguish inputs from 
Class 1  vs. Class 2
CSCS
PSPS
InputEigentasks           , index(a) (b)
(c)
Learning 
with
          
eigentasksClass 1
Train
TestClass 2
Equiv. to learning 
likelihood function
, ,
FIG. 3. (a) Device eigentasks for correlated system (CS, left)
and product system (PS, right), constructed from noisy features at
S= 210andS= 214. (b) ClassiÔ¨Åcation demonstration on IBMQ
Perth. Binary distributions to be classiÔ¨Åed over the input domain are
shown. (c) The classiÔ¨Åcation task can be cast as learning the likeli-
hood function separating the two distributions; this target function is
shown in the upper panel. Lower panels show the learned estimate
of this target based on the Ntrain = 150 points shown in (b), using
onlyKc(S)eigentasks for S= 214; this cutoff is indicated by the
dashed red lines. For the correlated system Kc(S) = 40 , while for
the product system Kc(S) = 29 .
B. A Robust Approach to Learning
While we have demonstrated the EC as an efÔ¨Åciently-
computable metric of general expressive capability of a noisy
quantum system, some important practical questions arise.
First, does the general EC metric have implications for practi-
cal performance on speciÔ¨Åc QML tasks? Secondly, given the
limiting ‚Äì and unavoidable ‚Äì nature of correlated sampling
noise, does the EC provide any insights on optimal learning
using a particular noisy quantum system and the associated
encoding?
Our formulation addresses both these important questions
naturally, as we now discuss. Recall that beyond being a sim-
ple Ô¨Ågure of merit, the EC is precisely the sum of capacities
to approximate a particular set of orthogonal functions na-
tive to the given noisy quantum system: the eigentasks. Fur-
thermore, these eigentasks y(k)(u)can be directly estimated
from a noisy quantum system via the generalized eigenvec-
torsfr(k)g, and are ordered by their associated NSR f2
kg.
In Fig. 3(a) show a selection of estimated eigentasks from the
device for the CS (J==2)and PS (J= 0) encodings of
Fig. 2(b). For both systems, the increase in noise with eigen-
task order is apparent when comparing two sampling values,
S= 210andS= 214. Furthermore, for any order k, eigen-
tasks for the PS are visibly noisier than the CS; this is consis-
tent with NSR eigenvalues for PS being larger than those for
CS (Fig. 2(b)). The higher expressive capacity of the CS can
be interpreted the ability to accurately resolve more eigentasks
at Ô¨ÅxedS.
The resolvable eigentasks of a Ô¨Ånitely-sampled quantum
system are intimately related to its performance at speciÔ¨Åc
QML applications. To demonstrate this result, we considera concrete application: a binary classiÔ¨Åcation task that is
not linearly-separable. The domain u2[ 1;1]over which
EC was evaluated is separated into two classes, as depicted
in Fig. 3(b). A selection of Ntrain = 150 total samples ‚Äì
with equal numbers from each class ‚Äì are input to the IBMQ
device, and eigentasks fy(k)(u(n))gKLare estimated using
S= 214shots. A linear estimator applied to this set of eigen-
tasks is then trained using logistic regression to learn the class
label associated with each input. Finally, the trained IBMQ
device is used to predict class labels of Ntest= 150 distinct
input samples for testing. Note that we use the random cir-
cuits of the previous section to draw more direct comparisons
between EC and task performance. By training only external
weights instead of internal parameters we are employing
the framework of Reservoir Computing [22, 23], which al-
lows one to avoid the computational overhead and difÔ¨Åculty
associated with training quantum systems while still achiev-
ing comparable performance [4, 13, 26, 32].
This task can equivalently be cast as one of learning the
likelihood function that discriminates the two input distribu-
tions, shown in Fig. 3(c), with minimum error. The set of up
toKLeigentasks y(k)(u), whereKLK, serves as the na-
tive orthonormal basis of readout features used to approximate
anytarget function using the quantum system. Importantly,
the basis is ordered , with eigentasks at higher kcontribut-
ing more noise, as dictated by the NSR eigenvalues 2
k. In
particular, at any level of sampling S, there exists an eigen-
task orderKc(S)after which the NSR 2
k=SÔ¨Årst drops be-
low unity:Kc(S) = maxkf2
k< Sg. Heuristically, includ-
ing eigentasks k > Kc(S)should contribute more ‚Äònoise‚Äô to
the function approximation task than ‚Äòsignal‚Äô. In Fig. 3(c),
we plot the learned estimates of the likelihood function us-
ingKL=Kc(S)eigentasks for both the CS and PS. First,
we note that Kcis lower for the PS than the CS; the former
has fewer resolvable eigentasks at a given S. This limitation
on resolvable features limits function approximation capac-
ity: the learned estimate of the likelihood function using Kc
eigentasks is visibly worse for the PS than the CS.
In this way, higher EC allows noisy quantum systems to
better approximate more functions, which translates to im-
proved learning performance ‚Äì this result is explored system-
ically in Fig. 4(b). Of course, it is natural to ask whether us-
ingKc(S)Keigentasks is optimal: exactly this question
is investigated in Fig. 4(a), where we plot the training and test
accuracy of both device encodings as a function of the number
of measured eigentasks KL. The performance on the speciÔ¨Åc
training and test set shown in Fig. 3(b) is indicated with mark-
ers, and solid lines indicate the average performance over 10
distinct divisions of the data into training and test sets. This
permutation of the learning task is a standard technique to op-
timize hyperparameters in ML, and is done here to eliminate
the sensitivity of these results to the choice of training set.
First note that in all cases, using all eigentasks ( KL=K) ‚Äì
or equivalently all measured features fXg‚Äì leads to far lower
test accuracy than is found in training. The observed deviation
is a distinct signature of overÔ¨Åtting: the optimized estimator
learns noise in the training set (comprised of noisy eigentask8
Classification accuracy
 Classification accuracy
No. of eigentasks used for learning,(a) (b)CS
PSCS
PS
CouplingExperiment Simulations
Device encoding Device encoding Mean over 8 randomencodings:
Ideal Device noiseTest Train Test Train
OverfittingMax. CAMax. CA
Overfitting
Single set
Ensemble
FNN
FIG. 4. (a) Training (light) and testing (dark) accuracy for the de-
vice encodings of Fig. 4(a), as a function of the number of eigentasks
used to approximate the target function. Markers indicate perfor-
mance on the dataset shown in Fig. 4(b), and solid lines are the aver-
age over 10random selections of training and test sets. The shaded
region denotes the maximum and minimum test accuracy observed.
The optimal test set performance is found near the noise-to-signal
cutoffKc(S= 214)(dash-dotted lines) informed by the quantum
system‚Äôs noise-to-signal spectra. (b) Testing set classiÔ¨Åcation ac-
curacy as a function of Jfor our optimal learning method. In all
cases, the average performance over the 10task permutations is re-
ported, using Kc(S= 214). Markers indicate device results for the
chosen encoding, and the corresponding simulation is shown in solid
peach. Dashed peach shows the average of these results over the 8
device noise simulation encodings, and dashed grey the ideal simu-
lation performance in the S!1 limit, where all K= 64 features
are used. The horizontal line denotes the performance of a software
neural network with KL= 64 nodes (and 1153Kctrained pa-
rameters) for comparison.
estimates y(k)(u(n))), and thus loses generalizability to un-
seen samples in testing.
Improvements in model training performance with added
features are only meaningful insofar as they also lead to bet-
ter performance on new data: in both encodings we see test
set classiÔ¨Åcation accuracy peaks near Kc(S). This is partic-
ularly clear for the averaged results, but even for individual
datasets the test accuracy at Kc(S)is within2%of its max-
imum, thus conÔ¨Årming our heuristic reasoning that eigentasks
beyond this order, with an NSR <1, hinder learning. The
eigentask-learning approach naturally allows one to decom-
pose the outputs from quantum measurements into a com-
pressed basis with known noise properties, and then select the
set of these which exactly captures the resolvable information
at a givenS. This robust approach to learning enabled by the
capacity analysis maximizes the ability of a noisy quantum
system to approximate functions without overÔ¨Åtting to noise,
in this case fundamental QSN.
Finally, Fig. 4(b) shows the classiÔ¨Åcation accuracy for this
device encoding as Jis varied, where following the above ap-proach, the optimal Kc(S)set of eigentasks are used for each
encoding. We also show the performance of a similar-scale
(KL= 64 node) software neural network and ideal simula-
tions in the S!1 limit (Kc(1) = 64 ) for comparison.
Note that only these inÔ¨Ånite-shot results approach the classical
neural network, with QSN imposing a signiÔ¨Åcant performance
penalty even for J=2 (mod). We highlight the strik-
ing similarity with Fig. 2(c): encodings with larger quantum
correlations and thus higher expressive capacity will perform
generically better on learning tasks in the presence of noise,
because they generate a larger set of eigentasks that can be re-
solved at a given sampling S. Expressive Capacity is a priori
unaware of the speciÔ¨Åc problem considered here; this exam-
ple thus emphasizes its power as a general metric predictive
of performance on arbitrary tasks.
IV . DISCUSSION
We have developed a straightforward approach to quantify
the expressive capacity of any quantum system in the presence
of fundamental sampling noise. Our analysis is built upon an
underlying framework that determines the native function set
that can be most robustly realized by a Ô¨Ånitely-sampled quan-
tum system: its eigentasks. We use this framework to intro-
duce a methodology for optimal learning using noisy quan-
tum systems, which centers around identifying the minimal
number of eigentasks required for a given learning task. The
resulting learning methodology is resource-efÔ¨Åcient and ro-
bust to overÔ¨Åtting. We demonstrate that eigentasks can be
efÔ¨Åciently estimated from experiments on real devices using
a limited number of training points and Ô¨Ånite shots. We also
demonstrate across two distinct qubit-based ans ¬®atze that the
presence of measured quantum correlations enhances expres-
sive capacity. Our work has direct application to the design
of circuits for learning with qubit-based systems. In particu-
lar, we propose the optimization of expressive capacity as a
meaningful goal for the design of quantum circuits with Ô¨Ånite
measurement resources.
ACKNOWLEDGEMENT
We would like to thank Ronen Eldan, Daniel Gauthier,
Michael Hatridge, Benjamin Lienhard, Peter McMachon,
Sridhar Prabhu, Shyam Shankar, Francesco Tacchino, Lo-
gan Wright for stimulating discussions about the work that
went into this manuscript. This research was developed
with funding from the DARPA contract HR00112190072,
AFOSR award FA9550-20-1-0177, and AFOSR MURI award
FA9550-22-1-0203. The views, opinions, and Ô¨Åndings ex-
pressed are solely the authors and not the U.S. government.
[1] E. Grant, M. Benedetti, S. Cao, A. Hallam, J. Lockhart, V . Sto-
jevic, A. G. Green, and S. Severini, Hierarchical quantum clas-
siÔ¨Åers, npj Quantum Information 4, 1 (2018).[2] V . Havl ¬¥ƒ±Àácek, A. D. C ¬¥orcoles, K. Temme, A. W. Harrow, A. Kan-
dala, J. M. Chow, and J. M. Gambetta, Supervised learning with
quantum-enhanced feature spaces, Nature 567, 209 (2019).9
[3] F. Tacchino, P. Barkoutsos, C. Macchiavello, I. Tavernelli,
D. Gerace, and D. Bajoni, Quantum implementation of an arti-
Ô¨Åcial feed-forward neural network, Quantum Science and Tech-
nology 5, 044010 (2020).
[4] J. Chen, H. I. Nurdin, and N. Yamamoto, Temporal Informa-
tion Processing on Noisy Quantum Computers, Physical Re-
view Applied 14, 024065 (2020).
[5] Y . Suzuki, Q. Gao, K. C. Pradel, K. Yasuoka, and N. Yamamoto,
Natural quantum reservoir computing for temporal information
processing, ScientiÔ¨Åc Reports 12, 1353 (2022).
[6] M. S. Rudolph, N. B. Toussaint, A. Katabarwa, S. Johri, B. Per-
opadre, and A. Perdomo-Ortiz, Generation of High-Resolution
Handwritten Digits with an Ion-Trap Quantum Computer, Phys-
ical Review X 12, 031010 (2022), publisher: American Physi-
cal Society.
[7] J. J. Meyer, Fisher Information in Noisy Intermediate-Scale
Quantum Applications, Quantum 5, 539 (2021).
[8] Z. Ma, P. Gokhale, T.-X. Zheng, S. Zhou, X. Yu, L. Jiang,
P. Maurer, and F. T. Chong, Adaptive Circuit Learning for
Quantum Metrology, arXiv:2010.08702 [quant-ph] (2021).
[9] C. D. Marciniak, T. Feldker, I. Pogorelov, R. Kaubruegger,
D. V . Vasilyev, R. van Bijnen, P. Schindler, P. Zoller, R. Blatt,
and T. Monz, Optimal metrology with programmable quantum
sensors, Nature 603, 604‚Äì609 (2022).
[10] M. Benedetti, E. Lloyd, S. Sack, and M. Fiorentini, Parame-
terized quantum circuits as machine learning models, Quantum
Science and Technology 4, 043001 (2019).
[11] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo,
K. Fujii, J. R. McClean, K. Mitarai, X. Yuan, L. Cincio, and
P. J. Coles, Variational quantum algorithms, Nature Reviews
Physics 3, 625 (2021).
[12] M. Schuld and F. Petruccione, Machine Learning with Quan-
tum Computers , Quantum Science and Technology (Springer
International Publishing, 2021).
[13] L. G. Wright and P. L. McMahon, The Capacity of Quantum
Neural Networks, arXiv:1908.01364 [quant-ph] (2019).
[14] Y . Du, M.-H. Hsieh, T. Liu, and D. Tao, Expressive power of
parametrized quantum circuits, Physical Review Research 2,
033125 (2020).
[15] Y . Du, Z. Tu, X. Yuan, and D. Tao, EfÔ¨Åcient Measure for the
Expressivity of Variational Quantum Algorithms, Physical Re-
view Letters 128, 080506 (2022), publisher: American Physical
Society.
[16] S. Sim, P. D. Johnson, and A. Aspuru-Guzik, Expressibility
and Entangling Capability of Parameterized Quantum Circuits
for Hybrid Quantum-Classical Algorithms, Advanced Quantum
Technologies 2, 1900070 (2019).
[17] Y . Wu, J. Yao, P. Zhang, and H. Zhai, Expressivity of quantum
neural networks, Phys. Rev. Research 3, L032049 (2021).
[18] R. LaRose and B. Coyle, Robust data encodings for quantum
classiÔ¨Åers, Phys. Rev. A 102, 032420 (2020).
[19] M. Schuld, R. Sweke, and J. J. Meyer, Effect of data encod-
ing on the expressive power of variational quantum-machine-
learning models, Physical Review A 103, 032430 (2021).[20] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and
S. Woerner, The power of quantum neural networks, Nature
Computational Science 1, 403‚Äì409 (2021).
[21] Z. Holmes, K. Sharma, M. Cerezo, and P. J. Coles, Connect-
ing ansatz expressibility to gradient magnitudes and barren
plateaus, PRX Quantum 3, 010313 (2022).
[22] J. Dambre, D. Verstraeten, B. Schrauwen, and S. Massar, Infor-
mation Processing Capacity of Dynamical Systems, ScientiÔ¨Åc
Reports 2, 514 (2012).
[23] K. Fujii and K. Nakajima, Harnessing Disordered-Ensemble
Quantum Dynamics for Machine Learning, Physical Review
Applied 8, 024030 (2017).
[24] R. Mart ¬¥ƒ±nez-Pe Àúna, J. Nokkala, G. L. Giorgi, R. Zambrini, and
M. C. Soriano, Information processing capacity of spin-based
quantum reservoir computing systems, Cognitive Computation
(2020).
[25] A. Arrasmith, M. Cerezo, P. Czarnik, L. Cincio, and P. J. Coles,
Effect of barren plateaus on gradient-free optimization, Quan-
tum5, 558 (2021).
[26] J. Garc ¬¥ƒ±a-Beni, G. L. Giorgi, M. C. Soriano, and R. Zam-
brini, Scalable photonic platform for real-time quantum reser-
voir computing, arXiv:2207.14031 [quant-ph] (2022).
[27] A. Anshu, S. Arunachalam, T. Kuwahara, and M. Soleimani-
far, Sample-efÔ¨Åcient learning of interacting quantum systems,
Nature Physics 17, 931 (2021).
[28] IBM Quantum, https://quantum-computing.ibm.com (2022).
[29] J. Wishart, Cumulants of multivariate multinomial distribu-
tions, Biometrika 36, 47 (1949).
[30] D. R. Cox, Principles of Statistical Inference (Cambridge Uni-
versity Press, Cambridge, 2006).
[31] M. Hermans and B. Schrauwen, Memory in linear recurrent
neural networks in continuous time, Neural Networks 23, 341
(2010).
[32] W. D. Kalfus, G. J. Ribeill, G. E. Rowlands, H. K. Krovi, T. A.
Ohki, and L. C. G. Govia, Hilbert space as a computational
resource in reservoir computing, Physical Review Research 4,
033007 (2022).
[33] V . Vedral, The role of relative entropy in quantum information
theory, Reviews of Modern Physics 74, 197 (2002).
[34] K. Modi, T. Paterek, W. Son, V . Vedral, and M. Williamson,
UniÔ¨Åed view of quantum and classical correlations, Physical
Review Letters 104, 080501 (2010).
[35] M. A. Nielsen and I. Chuang, Quantum computation and quan-
tum information (Cambridge University Press, 2010).
[36] S. Sheldon, E. Magesan, J. M. Chow, and J. M. Gambetta,
Procedure for systematically tuning up cross-talk in the cross-
resonance gate, Physical Review A 93, 060302 (2016).
[37] V . Giovannetti, S. Lloyd, and L. Maccone, Quantum metrology,
Physical Review Letters 96, 010401 (2006).
[38] R. Mart ¬¥ƒ±nez-Pe Àúna, G. L. Giorgi, J. Nokkala, M. C. Soriano, and
R. Zambrini, Dynamical phase transitions in quantum reservoir
computing, Phys. Rev. Lett. 127, 100502 (2021).
[39] Z. Pucha≈Ça and J. Miszczak, Symbolic integration with respect
to the haar measure on the unitary groups, Bulletin of the Polish
Academy of Sciences Technical Sciences 65, 21 (2017).10
Appendix A: Table of Symbols and Abbreviations
Abbreviations
NISQ Noisy Intermediate Scale Quantum
(Q)ML (Quantum) Machine Learning
QSN Quantum Sampling Noise
VQC Variational Quantum Circuits
PS Product System
CS Correlated System
EC Total Expressive Capacity, CT
NSR Noise-to-signal ratio
ETC Expected Total Correlation, T
Symbols and notation
S Number of shots
N Number of inputs
L Number of qubits
K Number of measured features; K= 2Lfor computational-basis projective measurement
u Input
 Quantum system parameters
^ Generated quantum state
^Mk POVM elements,jbkihbkjfor computational-basis projective measurement
W General output weights
w Learned Optimal output weights for Ô¨Ånite- SfeaturesfXkg
L Loss function
bk Computational basis eigenstate label
k(s)Measurement outcome for shot s
xk Expected features, Trf^Mk^g
Xk Empirical observed feature, (1=S)P
s(k(s);k)
k Noise component of Xk
G Gram matrix of expected features fxkg
V Expected covariance matrix of random variable X(s)
k(u)over input distribution
2
k Eigen-NSR associated with eigentask k
y(k)Eigentask,P
k0r(k)
k0xk0
r(k)linear combination of expected features fxk0gformingy(k)
y(k)Finite-Sestimate of eigentask,P
k0r(k)
k0Xk0
^Mdiagonal post-measurement state,P
k^kk(u)jbkihbkj
Kc(S) Cutoff index where 2
kapproachesS,max kf2
k<Sg
TABLE I. Table of notations used in main text
Appendix B: Feature maps using quantum systems
In the main text, we introduce the idea of encoding inputs into the state of a quantum system via a parameterized quantum
channel, reproduced below:
^(u;) =U(u;)^0; (B1)
one then measures this state to approximate desired functions of the input. Figure 5 gives a simple example of this mapping from
classical inputs u(here in a 2D compact domain) to a quantum state generated by a u-dependent encoding, and Ô¨Ånally to the
measured features in a 2-qubit system undergoing commuting local measurements in the computational basis. The measurement
outcomes are therefore bitstrings, of which there are K= 2L= 4, namely:bk2f00;01;10;11g. A given shot will yield one
of these possible bitstrings.
On the right we plot samples of features Xkconstructed with different numbers of shots S. As expressed in Eq. (4), the noise
and thus variance in the distribution of samples scales with S. AsS!1 this distribution thus collapses to a single deterministic
point, the corresponding quantum probability x(u). It is also evident from this plot that the shape and orientation of these clusters
depends on the underlying quantum state ^(u;)and associated probabilities x(u)via Eq. (5). In the remainder of this section,
we will consider more complex quantum models, such that they generate mappings which can be useful for learning.11
FIG. 5. Schematic of a simple L= 2qubit circuit, comprised of a CNOT gate sanwiched by input-dependent local x-rotation gatesfRi(u)g.
Different 2D inputs shown on the left are mapped to the Ô¨Ånite- Sfeature space on the right via this circuit. SpeciÔ¨Åcally, a 2D slice ( X00and
X11) of the 4D feature space is shown. Each point represents an individual sample or experiment, i.e. an output constructed with S <1shots
via Eq. (2). Distinct values of S= 102;103;104are shown in different colors (blue, red, green). For each input uand shotsS, the simulation
is conducted for 100repetition.
To describe these models, we begin by Ô¨Årst limiting to 1-D inputs uas analyzed in the main text; generalizations to multi-
dimensional inputs uare straightforward. Then, we write Eq. (B1) in the form
^(u;) =^U(u;)^0^Uy(u;) (B2)
In the main text, we have considered a model for dynamics of an L-qubit quantum system that is natively implementable on
modern quantum computing platforms: namely an ansatz of quantum circuits with single and two-qubit gates. We refer to this
encoding as the circuit ansatz (orC-ansatz for short) for which the operator ^U(u;)takes the precise form
^U(u;) =
Rxx
2
W(J)Rz 
z+Iu
Rxx
2
(C-ansatz) (B3)
For completeness, we recall that Rx=zare Pauli-rotations applied qubit-wise, e.g. Rz=Q
lRz(z
l+I
lu), while the coupling
gate acts between physically connected qubits in the device and can be written as W(J) =Q
hl;l0iexpf iJ
2^z
l^z
l0g. We empha-
size here again that 2N+is an integer, representing the number of repeated blocks in the C-ansatz encoding. We note that the
actual operations implemented on IBMQ processors also include dynamics due to noise, gate, and measurement errors, and thus
must be represented as a general quantum channel as in Eq. (B1). As discussed in the main text, the EC of a quantum system
can be computed in the presence of these more general dynamics, and is sensitive to the limitations introduced by them.
An alternative ansatz which we analyze in this SI, is where the operator ^U(u;)describes continuous Hamiltonian dynamics.
This ansatz is relevant to computation with general quantum devices, such as quantum annealers and more generally quantum
simulators. In this case, which we refer to as the Hamiltonian ansatz (orH-ansatz for short),
^U(u;) = expf i^H(u)tg;^H(u) =^H0+u¬∑^H1 (H-ansatz) (B4)
Heretis a continuous parameter deÔ¨Åning the evolution time; and ^H0=PL
l;l0Jhl;l0i^z
l^z
l0+PL
l=1hx
l^x
l+PL
l=1hz
l^z
land
^H1=PL
l=1hI
l^z
l. The transverse x-Ô¨Åeld strength hx
l=hx+"x
land longitudinal z-drive strength hz;I
l=hz;I+"z;I
lare all
randomly chosen and held Ô¨Åxed for a given realization of the quantum system,
"x;z;I
lhx;z;I
rmsN(0;1); (B5)
whereN(0;1)deÔ¨Ånes the standard normal distribution with zero mean and unit variance. We consider nearest-neighbor inter-
actionsJl;l0, which can be constant Jl;l0J, or drawn from Jl;l0Unif[0;Jmax], where Unif[a;b]is a uniform distribution
with non-zero density within [a;b].
As an aside, we note that the C-ansatz quantum channel described by Eq. (B3) can be considered a Trotterization-inspired
implementation of the H-ansatz in Eq. (B4). In particular, if we set x=z=I=hx=z=I¬∑, wheret=  ¬∑, and consider the
limit !0while keeping tÔ¨Åxed, Eq. (B3) corresponds to a Trotterized implementation of Eq. (B4). This correspondence is
chosen for practical reasons, but is not necessary in our analysis.12
Algorithm 1: Measured features in the probability representation
Input :u2[ 1;+1]
Output : X(u), which approximates xk(u) := Trf^(u)jbkihbkjg
Fors 1toS
Initialize overall state ^0 j0ih0j
L;
Evolve under quantum channel U(u):^(u) U(u)^0;
Measure allLqubits:b(s)(u) bk= (bk;1;bk;2;bk;L)2f0;1gL;
EndFor
Fork 0toK 1
Take the ensemble averages as readout features:
Xk(u) 1
SPS
s=1(bk;b(s)(u)); /*Noticexk(u) := Trf^(u)jbkihbkjg= lim S!1Xk(u)*/
EndFor
Algorithm 2: Training of output weights
Input :fu(1);;u(N)g2[ 1;+1]N
Output :ewN, such thaty=ewN¬∑X(u)can approximate f(u)
Forn 1toN
Generate features X(u(n))through Algorithm 1
EndFor
Collect the features into a regression matrix eFN2RNK;
Compute empirical Gram matrix G 1
NeFT
NeFN; /*For finite S,limN!1G=~G:=G+1
SV*/
Compute target vector Y 
f(u(1));;f(u(N))T
;
ewN (eFT
NeFN) 1eFT
NY; /*For finite S,limN!1ewN=ew:=Eq. (C13) */
Appendix C: Information capacity with quantum sampling noise
1. DeÔ¨Ånition of capacity for quantum systems with sampling noise
A universal function approximation theorem (which will be formally stated in Appendix J), as a basic requirement of most
neural network models, can be made concrete by deÔ¨Åning a metric to quantify how well a given quantum system (or any
dynamical system) approximates general functions. Suppose an arbitrary probability distribution p(u)for a random (scalar)
variableudeÔ¨Åned inDR. This naturally deÔ¨Ånes a function space L2
p(D)containing all functions f:D!RwithR
f2(u)p(u)du<1. The space is equipped with the inner product structure hf1;f2ip=R
f1(u)f2(u)p(u)du. A standard way
to check the ability of Ô¨Åtting nonlinear functions by a physical system is the information processing capacity [22],
C[f`] = 1 min
W`2RKRPK 1
k=0W`kxk(u) f`(u)2
p(u)du
R
f`(u)2p(u)du; (C1)
where functions f`(u)are orthogonal target functions hf`;f`0ip=R
f`(u)f`0(u)p(u)du= 0 for`6=`0. The total expressive
capacity is deÔ¨Åned as CTP1
`=0C[f`], capturing the ability of what type of function the linear combination of physical system
readout features can produce. Dambre et. al. ‚Äôs argument claims that the total capacity must be upper bounded by the number of
featuresCTK.
While Dambre et. al. ‚Äôs result [22] is quite general, it neglects the limitations due to noise in readout features, a fact that is
unavoidable when using quantum systems in the presence of Ô¨Ånite computational and measurement resources. It is generally
accepted that the capacity is reduced in the presence of additive noise, but there are no general results on how to quantify that
reduction. This is our goal here, to arrive at an exact result for capacity reduction under well-deÔ¨Åned conditions.
In this section, we will focus on the impact of fundamental quantum readout noise, or quantum sampling noise (QSN), on
this upper bound under Ô¨Ånite sampling S. GivenuandS, the quantum readout features Xk(u) =1
SPS
s=1(k(s)(u);k)are13
stochastic variables. The expectation vector and covariance matrix of X(u)can be expressed in terms of ^(u)
E[X(u)]x(u) = Trf^Ek^(u)g; (C2)
Cov[ X(u)]1
S(u) =1
S 
diag(x) xxT
: (C3)
To determine the optimal capacity to compute an arbitrary normalized function f(u) =P1
j=0(Y)jujusing the noisy readout
features X(u)extracted from the quantum system, we need to Ô¨Ånd an optimal Wsuch that
C[f] = 1 minWRPK 1
k=0WkXk(u) f(u)2
p(u)du
R
f2(u)p(u)du(C4)
By expanding the numerator of the right-hand side for a given, Ô¨Ånite number of shots S, we Ô¨Ånd
Z
f2(u)p(u)du Z K 1X
k=0WkXk(u) f(u)!2
p(u)du
= K 1X
k1=0K 1X
k2=0Wk1Wk2Z
Xk1(u)Xk2(u)p(u)du+ 2K 1X
k=0WkZ
Xk(u)f(u)p(u)du
  1
NK 1X
k1=0K 1X
k2=0Wk1Wk2NX
n=1Xk1(u(n))Xk2(u(n)) +2
NK 1X
k=0WkNX
n=1Xk(u(n))f(u(n)): (C5)
where we have approximated the integral over the input domain by a Ô¨Ånite sum in the limit of a large number of inputs N.
Next, note that if n6=n0, thenXk1(u(n))andXk2(u(n0))are independent random variables (though not necessarily identically
distributed). The sums over Non the right hand side are therefore sums of bounded independent random variables. In the
limit of large N1, the deviation between stochastic realizations of these sums and their expectation values is exponentially
suppressed, as determined by the Hoeffding inequality. Then, with large probability, the sums over Nmay be replaced by their
expectation values,
Z
f2(u)p(u)du Z K 1X
k=0WkXk(u) f(u)!2
p(u)du
  1
NK 1X
k1=0K 1X
k2=0Wk1Wk2NX
n=1E[Xk1(u(n))Xk2(u(n))] +2
NK 1X
k=0WkNX
n=1E[Xk(u(n))f(u(n))]
= 1
NK 1X
k1=0K 1X
k2=0Wk1Wk2NX
n=1
xk1(u(n))xk2(u(n)) +1
S(u(n))k1k2
+2
NK 1X
k=0WkNX
n=1xk(u(n))f(u(n))
  K 1X
k1=0K 1X
k2=0Wk1Wk2Z
xk1(u)xk2(u) +1
S(u)k1k2
p(u)du+ 2K 1X
k=0WkZ
xk(u)f(u)p(u)du: (C6)
The Ô¨Årst approximation above comes from the Hoeffding inequality, where terms that are dropped are proportional to 1=p
N.
In going from the second to the third line, we have used Eq. (C3). The Ô¨Ånal expression is obtained by rewriting sums over uas
integrals, with an error proportional to 1=p
Nonce more. Thus we can say the original integral in Eq. (C4) is approximately
equal to Eq. (C6) to O(1=p
N). In the limit of a large number of input samples, N!1 , we conclude that all approximations
can be replaced by exact equalities.
The goal of the remaining part of this section is deducing a more compact generalized Rayleigh quotient form of functional
capacity. The dependence of readout features xk(u)on the input ucan always be written in the form of a Taylor expansion,
xk(u) =1X
j=0(T)kjuj(C7)
where we deÔ¨Åne the transfer matrix T()T2RK1that depends on the density matrix ^(u), and in particular on
parameterscharacterizing the quantum system. The Ô¨Årst term in Eq. (C6) does not depend explicitly on the function f(u)14
being constructed, and introduces quantities that are determined entirely by the response of the quantum system of interest to
inputs over the entire domain of u. In particular, we introduce the Gram matrix G2RKKas
(G)k1k2=Z
xk1(u)xk2(u)p(u)du=1X
j1=01X
j2=0(T)k1j1Z
uj1+j2p(u)du
(T)k2j2(TTT)k1k2 (C8)
where in the second line we have also introduced the generalized Hilbert matrix 2R11as
()j1j2=Z
uj1+j2p(u)du: (C9)
Secondly, we introduce the noise matrix V2RKK,
(V)k1k2=Z
(u)k1k2p(u)du=Z
(k1k2xk1(u) xk1(u)xk2(u))p(u)du(D)k1k2 (G)k1k2 (C10)
Here we have also introduced the second-order-moment matrix D2RKKsuch that (D)k1k2=k1k2R
xk1(u)p(u)du. Then,
the noise matrix simply deÔ¨Ånes the covariance of readout features, and is therefore given by V=D G. The second term in
Eq. (C6) depends on f(u)and can be simpliÔ¨Åed using the matrix as well,
Z
xk(u)f(u)p(u)du=1X
j1=01X
j2=0(T)kj1Z
uj1+j2p(u)du
(Y)j2= (TY )k: (C11)
With these deÔ¨Ånitions, Eq. (C4) can be compactly written in matrix form as a Tikhonov regularization problem:
C[f] = 1 min
W0
B@1
2TTW 1
2Y2
+1
SWTVW
YTY1
CA: (C12)
The least-squares form ensures that the optimal value (argmin) wofWhas closed form
w=
TTT+1
SV 1
TY: (C13)
Substitutingwinto the expression for C, we obtain the optimal capacity with which a function fcan be constructed, which
takes the form of a generalized Rayleigh quotient
C[f] =YTTT 
G+1
SV 1TY
YTY: (C14)
2. Eigentasks
Eq. (C14) deÔ¨Ånes the optimal capacity of approximating an arbitrary function f(u) =P1
j=0(Y)juj. We can therefore
naturally ask which functions fmaximise this optimal capacity. To this end, we Ô¨Årst note that the denominator of Eq. (C14) is
simply a normalization factor that can be absorbed into the deÔ¨Ånition of the function f(u)being approximated, without loss of
generality. More precisely, we consider:
hf;fip= 1 =
1
2YT
1
2Y
=YTY: (C15)
Then, we can rewrite the optimal capacity from Eq. (C14) as
C[f] =YT1
2Q1
2Y: (C16)
Here we have deÔ¨Åned the matrix Q2R11as
Q=B
I+1
SR 1
BT; (C17)
B=1
2TTG 1
2; (C18)
R=G 1
2VG 1
2 (C19)15
by introducing the matrix square root of G1
22RKK, andRthenoise-to-signal matrix. The decomposition in Eq. (C17) may
be veriÔ¨Åed by direct substitution into Eq. (C16). The ability to calculate matrix powers and in particular the inverse of Grequires
constraints on its rank, which we show are satisÔ¨Åed in Appendix C 6.
We now consider the measure-independent part of the eigenvectors of Q, indexed Y(k), satisfying the standard eigenvalue
problem:
Q1
2Y(k)=Ck1
2Y(k): (C20)
wherek= 0;;K 1. From Eq. (C16), it is clear that these eigenvectors have a particular meaning. Consider the function
y(k)(u)deÔ¨Åned by the eigenvector Y(k), namely
y(k)(u) =1X
j=0Y(k)
juj; (C21)
which we will refer to from now on as eigentasks . Suppose we wish to construct the function y(k)(u)using outputs obtained
from the physical system deÔ¨Åned by Qin theS!1 limit (namely, with deterministic outputs). At a Ô¨Årst glance, before
we dive into solving the eigenproblem Eq.(C20), we do not know any relationship between y(k)andx(u).The rest part of this
subsection is aiming to prove that y(k)must be a speciÔ¨Åc linear combination of features x(u). Then, the physical system‚Äôs
capacity for this construction is simply given by the corresponding eigenvalue Ck, as may be seen by substituting Eq. (C20)
into Eq. (C16). Formally, the y(k)(u)serves as the critical point (orstationary point ) of the generalized Rayleigh quotient in
Eq. (C14). Consequently, the function that is constructed with largest capacity then corresponds to the nontrivial eigenvector
with largest eigenvalue.
To obtain these eigentasks, we must solve the eigenproblem deÔ¨Åned by Eq. (C20). Here, the representation of Qin Eq. (C17)
becomes useful, as we will see that the eigensystem of Qis related closely to that of the noise-to-signal matrix R. In particular,
we Ô¨Årst deÔ¨Åne the eigenproblem of R,
RG1
2r(k)=2
kG1
2r(k)(C22)
with NSR eigenvalues 2
kand corresponding eigenvectors r(k), which satisfy the orthogonality relation r(k0)TGr(k)=k;k0.
Here ther(k)is equivalent to be deÔ¨Åned as the solution to generalized eigen-problem:
Vr(k)=2
kGr(k): (C23)
This is because Vr(k)=G1
2RG1
2r(k)=2
kG1
2G1
2r(k)=2
kGr(k). The prefactor G1
2is introduced for later convenience.
Eq. (C22) then allows us to deÔ¨Åne the related eigenproblem

I+1
SR 1
G1
2r(k)=
1 +2
k
S 1
G1
2r(k)(C24)
Next, we note that Qis related to the matrix in brackets above via a generalized similarity transformation deÔ¨Åned by B,
Eq. (C17). In particular, BTB=G 1
2GG 1
2=I2RKK, while we remark that BBT6=Isince it is in R11. This
connection allow us to show that
QBG1
2r(k)=B
I+1
SR 1
BTBG1
2r(k)=1
1 +2
k=SBG1
2r(k): (C25)
Comparing with Eq. (C20), we can now simply read off both the eigenvalues and eigenvectors of Q,
Ck=1
1+2
k=S
1
2Y(k)=BG1
2r(k))
=)Y(k)=TTr(k)(C26)
where we have used the deÔ¨Ånition of Bfrom Eq. (C18). The functions deÔ¨Åned by the eigenvectors Y(k)are automatically
orthonormalized:
D
y(k1);y(k2)E
p=
1
2Y(k1)T
1
2Y(k2)
=r(k1)TG1
2BTBG1
2r(k2)=r(k1)TGr(k2)=k1k2: (C27)16
3. Noisy eigentasks from readout features
We can now also discuss the interpretation of f2
kgfor a physical system - in this case a quantum circuit - for which fr(k)gare
known. Consider a single run of the quantum system under Ô¨Ånite shots S, which yields a single instance of the readout features
X(u). We can simply read off that an noisy version of the kth eigentask, y(k)(u)can be constructed as
y(k)(u) =K 1X
k0=0r(k)
k0Xk0(u) (C28)
which is equivalent to requiring the output weights W=r(k).The corresponding set of noisy function is also orthogonal, this
is because Vr(k)=2
kGr(k)impliesr(k)TVr(k0)=2
kk;k0and hence
D
y(k1);y(k2)E
p=r(k1)T
G+1
SV
r(k2)=
1 +2
k
S
k1k2 (C29)
This equation can be further decomposed into two parts. Let the linear transformation of noise (u)by deÔ¨Åning (k)(u) =
1p
SPK 1
k=0r(k)
k0k0(u)
Eu[y(k1)y(k2)] =D
y(k1);y(k2)E
p=r(k1)TGr(k2)=k1k2; (C30)
Eu[(k1)(k2)] =D
(k1);(k2)E
p=1
Sr(k1)TVr(k2)=2
k1
Sk1k2: (C31)
It means that the combination fr(k)2RKgk2[K]not only produces orthogonal eigentasks fy(k)(u)gfor signal, but also induces
a set of orthogonal noise functions f(k)(u)g.
If the quantum circuit can be run multiple times for a given S, multiple instances of X(u)can be obtained, from each of
which an estimate of the kth eigentask y(k)(u)can be constructed. The expectation value of these estimates then simply yields
E[y(k)(u)] =K 1X
k0=0r(k)
k0E[Xk0(u)] =K 1X
k0=0r(k)
k0xk0(u) =y(k)(u) (C32)
If we have access to only a single instance of X(u), however, and thus only one estimate y(k)(u)(asy(k)(u)andy(k)(u)
depicted in Fig. 8), it is useful to know the expected error in this estimate. This error can be extracted from Eq. (C12). In
particular, requiring Y(k)=TTr(k), we have
1
2TTr(k) 1
2Y(k)2
+1
Sr(k)TVr(k)
Y(k)TY(k)=1
Sr(k)TVr(k)=2
k
S: (C33)
This mean squared error in using y(k)(u)to estimatey(k)(u)over the domain of udecreases to zero for S!1 as expected,
since the noise in Xdecreases with S. However, 2
kdeÔ¨Ånes theS-independent contribution to the error. In particular, this
indicates that at a given S, certain functions with lowers NSR eigenvalues 2
kmay be better approximated using this physical
system than others. We present in Fig. 8 the measured features X, the eigentasks yand theirS-Ô¨Ånite version yin a 6-qubit
Hamiltonian based system. The associated eigen-NSR spectrum, expressive capacity, and total correlations are also depicted for
both CSJ6= 0and PSJ= 0.
4. Expressive capacity
Given an arbitrary set of complete orthonormal basis functions f`(u) =P1
j=0(Y`)juj,
hf`;f`0ip=
1
2Y`T
1
2Y`0
=``0: (C34)17
The total capacity is independent of the basis choice
CT(S) =1X
`=0C[f`] =1X
`=0YT
`1
2 
1
2TT
TTT+1
SV 1
T1
2!
1
2Y`
= Tr 
1
2TT
TTT+1
SV 1
T1
2!
= Tr 
G+1
SV 1
G!
=K 1X
k=01
1 +2
k
S: (C35)
5. Estimation in case of nonlinear functions after linear output layer
Usually, instead of taking the linear transformation W¬∑X, the training process can involve some complicated nonlinear
activation functions or classical kernel, which may also be fed into a non-quadratic nonlinear loss function afterwards. These
two processes can be uniÔ¨Åed to be NL(X(u))with any smooth function NL. In this subsection, we show how to translate our
result obtaining from quadratic nonlinear function Eq. (C4) into a more general loss function with form of
L=Eu[NL(X)] (C36)
Now let us Ô¨Årst transform all noisy measured features fXkginto the naturally orthogonal basis of signal fy(k)gand noisef(k)g.
Xk0(u)K 1X
k=0 k0k(y(k)(u) +(k)(u)); (C37)
such transformation of  2RKKmust uniquely exist, this is because all Koffr(k)gare linearly independent. Recall
Eq. (C31) claims that Eu[(k)] = 0 andEu[(k)(k0)] =2
kkk0=S, we can deal with the nonlinearity by taking the cumulant
expansion up to the quadratic term, and we get
L=Eu[NL(X)] =Eu[NL( y)] =Eu"
NL X
k 0;k(y(k)+(k));;X
k K 1;k(y(k)+(k))!#
=Eu[NL( y)] +K 1X
k=0Eu@NL
@y(k)(k)
+1
2K 1X
k1=0K 1X
k2=0Eu@2NL
@y(k1)@y(k2)(k1)(k2)
+O1
S2
Eu[NL( y)] +1
2K 1X
k1=0K 1X
k2=0Eu@2NL
@y(k1)@y(k2)r(k1)Tr(k2)
; (C38)
where the Ô¨Årst order terms vanish due to Hoeffding inequality again. We then make a further approximation of Eq. (C38) by
replacing the (k1)(k2)with itsu-average Eu[(k1)(k2)] =k1k22
k1=S:
LEu[NL( y)] +K 1X
k=02
k
S¬∑Eu@2NL
(@y(k))2
: (C39)
In fact, any of the second terms can be further simpliÔ¨Åed by chain rule: LEu[NL( y)] +P
k2
k
S¬∑Eu[( Tr2
xNL )kk].
The approximation in Eq. (C39) is rough, but it still gives us a sufÔ¨Åcient reason to do the following manipulation: for optimized
L, the dependence on y(k)with2
k=S > 1will be strongly suppressed in large- Nlimit, hence we can pre-exclude the eigentasks
whose2
k=S > 1.
Let us use one typical example, the widely used logistic regression in classiÔ¨Åcation, to illustrate our argument here. As what
we will introduce in Appendix J, the target function is the conditional probability distribution f(u) := Pr[u2C1ju]in such
classiÔ¨Åcation model (see Eq. (J4)), and then there is one more layer of softmax and cross-entropy function acting on linear
mapL=Eu[H(f(u);(W¬∑X(u)))]whereis sigmoid function ( e.g. softmax function (z) = 1=(1 + exp( z))), and
H(p;q) = plnq (1 p) ln(1 q)is the cross-entropy. Especially, any linear combination of fXkgcan be translated into
linear combination
W¬∑X(u)K 1X
k=0
k¬∑(y(k)(u) +(k)(u)); (C40)18
Again, such vector 
= TWmust also uniquely exist. For any NL=g(W¬∑x), one always have  Tr2
xNL =
g00(
¬∑y)
T
:
LEu[H(f;(
¬∑y))] + K 1X
k=02
k
S
2
k!
¬∑Eu[(
¬∑y)(1 (
¬∑y))]: (C41)
It helps us read from the prefactor 2
k=Sinduces a natural regularization on 
kin loss function, in addition to the S-inÔ¨Ånity
term limS!1L=Eu[H(f;(
¬∑y))]. We will leave the detailed discussion of this important application in Appendix I and
Appendix J.
6. Proof that the Gram matrix Gis full rank
Recall that before we analytically Ô¨Ånd the eigenvectors of Q, we Ô¨Årst show that the matrix Gis invertible. It comes from that
allKreadout featuresfxk(u)gk2[K]being linear independent is entirely equivalent to the full-rankness of the corresponding
Gram matrix Rank( G) =K. Thanks to the linearity of readout, we can show such linear independence by contradiction.
Suppose on the contrary there exists coefÔ¨Åcients fckgk2[K]such that
K 1X
k=0ckxk(u) = Tr( K 1X
k=0ck^Ek!
U(u)^0)
= 0: (C42)
However, this means that the quantum observablePK 1
k=0ck^Ekis a zero-expectation readout-qubit quantity for any state U(u)^0
under arbitrary input u, which is impossible. This shows the linear independence. Furthermore, we then argue that it ensures G
has no non-trivial null space. This is because that any fckgk2[K]will satisfy
KX
k1;k2=1ck1ck2(G)k1;k2=Z KX
k1=1ck1xk1(u)! KX
k2=1ck2xk2(u)!
p(u)du=*K 1X
k=0ckxk;K 1X
k=0ckxk+
p: (C43)
where the RHS is the norm of functionPK 1
k=0ckxk(u). The summationPK
k1;k2=1ck1ck2(G)k1;k2= 0 vanishes if and only
if functionPK 1
k=0ckxk(u)is a zero function. That is why the linear independence of features fckgk2[K]is equivalent to that
symmetric matrix Ghas no zero eigenvalues, namely Rank( G) =K.
7. Simplifying the noise-to-signal matrix and its eigenproblem
We have shown that the problem of obtaining the eigentasks for a generic quantum system, and deducing its expressive
capacity under Ô¨Ånite measurement resources, can be reduced simply to solving the eigenproblem of its noise-to-signal matrix
R, Eq. (C22). Note that constructing R=G 1
2VG 1
2requires computing the inverse of G. However, Gcan have small
(although always nonzero) eigenvalues, especially for larger systems, rendering it ill-conditioned and making the computation of
Rnumerically unstable. Fortunately, certain simpliÔ¨Åcations can be made to derive an equivalent eigenproblem that is much easier
to solve. Practically, the probability representation is native to measurement schemes in contemporary quantum processors, and
therefore minimizes the required post-processing of readout features obtained from a real device. More importantly, the strength
of the probability representation lies in the fact that it renders the second-order moment matrix Ddiagonal. In particular,
(D)k1k2=PK 1
k=0(G)kk1;ifk1=k2
0; ifk16=k2(in probability representation of readout features) (C44)
Using V=D G, we can rewrite the eigenproblem for R,
R
G1
2r(k)
=2
kG1
2r(k)
=)G 1
2(D G)G 1
2
G1
2r(k)
=2
kG1
2r(k)
=)G 1Dr(k)= (1 +2
k)r(k)(C45)19
Finally, considering the inverse of the matrix on the left hand side, we obtain the simpliÔ¨Åed eigenproblem for the matrix D 1G,
D 1Gr(k)= (1 +2
k) 1r(k)kr(k); (C46)
which shares eigenvectors with R, and whose eigenvalues are a simple transformation of the NSR eigenvalues 2
k. Impor-
tantly, constructing D 1Gno longer requires calculating any powers of G, and when further choosing readout features in the
probability representation, it relies only on the inversion of a simple diagonal matrix D.
The matrix D 1Ghas signiÔ¨Åcance in spectral graph theory, when interpreting the Gram matrix Gas the adjacency matrix of
a weighted graph. This connection is elaborated upon in Appendix C 8.
8. Connections to spectral graph theory
Let us have a small digression to the graphic theoretic meaning of GandD 1G. Now we consider a weighted graph with
adjacency matrix G. In spectral graph theory, the matrix D 1Gis exactly the random walk matrix associated with graph G, and
then the second order matrix Dhappens to be the degree matrix of this graph since (D)kk=PK 1
k0=0(G)kk0. Then the eigentask
combination coefÔ¨Åcient r(k)is precisely the right eigenvector of random walk matrix. Another concept associated with a graph
isI D 1
2GD 1
2, the normalized Laplacian matrix ofG, while the matrix D 1
2GD 1
2is always referred to be normalized
adjacency matrix in many literatures. The eigenproblem of normalized adjacency matrix can also be solved easily, because
D 1
2GD 1
2
D1
2r(k)
=D1
2D 1Gr(k)=k
D1
2r(k)
: (C47)
From perspective of spectral graph theory, roughly speaking, a reservoir with stronger ability to resist noise are those who has
more ‚Äúbottlenecks‚Äù in graph G‚Äôs connectivity. The extreme case is supposing that k= 1(or1 k= 0) for allk. According
the basic conclusion in spectral graph theory, the normalized Laplacian matrix has Kzero eigenvalues iff the graph Gis fully
disconnected. This gives us the condition when noisy information capacity obtain its upper bound K: there exists a partition
fDomkgk2[K]of domain Dom = [ 1;1]such that ^kk(u) = 1 iffu2Domk.
Appendix D: Spectral analysis based on Ô¨Ånite statistics
While Eq. (C46) is a numerically simpler eigenproblem to solve than Eq. (C22), it still requires the approximation of G(recall
thatDcan be obtained from G) from readout features X(u)under Ô¨Ånite sampling, due to the Ô¨Åniteness of shots S, the number
of input points N, and also the number of realizations of readout features for a given S. To be more precise, in experiment one
only has access to measured features sampled at Ô¨Ånite- SX(indeed, this distinction is the underlying premise of this article).
However, in Eq. (8) GandVare deÔ¨Åned with respect to the ideal x. LeteGEu[XXT]andeVEu[diag( X) XXT].
The objective of Appendix D 1 is showing that the eigen-analysis f2
k;r(k)gcan be accurately substituted with
2
k=S¬∑~2
k
(S 1) ~2
k; (D1)
andr(k)=~r(k)from solving generalized eigenvalue problem eV~r(k)=~2
keG~r(k). In what follows, we show how an ap-
proximationeGNofGcan be constructed from Ô¨Ånitely-sampled readout features, as relevant for practical quantum devices.
Secondly, we also describe an approach to obtain the eigentasks y(k)(u)and corresponding NSR eigenvalues 2
kthat avoids
explicit construction of the Gram matrix, and is thus even more numerically robust.
1. Approximating eigentasks and NSR eigenvalues under Ô¨Ånite SandN
For practical computations, readout features X(u)from the quantum system for Ô¨Ånite Scan be computed for a discrete set
ofu(n)2[ 1;1]forn= 1;:::;N . Labelling the corresponding readout features X(u(n)), we can deÔ¨Åne the regression matrix
constructed from these readout features,
eFN(X(u(1));X(u(2));;X(u(N)))T=0
B@X0(u(1)) XK 1(u(1))
......
X0(u(N))XK 1(u(N))1
CA: (D2)20
Here,eFN2RNK, with subscript Nindicating its construction from a Ô¨Ånite set of Ninputs, is a random matrix due to the
stochasticity of readout features; in particular it can be written as:
eFN=FN+1p
SZ(FN) (D3)
where (FN)nk=E[Xk(u(n))] =xk(u(n)), andZis the centered multinomial stochastic process, so that E[eFN] =FN.
Using this regression matrix eFN, we can obtain an estimation of the Gram matrix and second order moment matrix, which
we denoteeGNandeDN, and whose matrix elements are deÔ¨Åned via
(eGN)k1k21
NNX
n=1Xk1(u(n))Xk2(u(n)) =1
N(eFT
NeFN)k1k2Z
Xk1(u)Xk2(u)p(u)du; (D4)
(eDN)k1k2k1;k21
NNX
n=1Xk1(u(n))k1;k2Z
Xk1(u)p(u)du: (D5)
While the quantities eGNandeDNare computed from stochastic readout features, their stochastic contributions are suppressed
in the large Nlimit by the Hoeffding inequality for sums of bounded stochastic variables. In particular, we can deÔ¨Åne their
deterministic limit for N!1 , according to Eq. (C6), as
eGlim
N!11
N(eFT
NeFN)k1k2=G+1
SV=G+1
S(D G); (D6)
eDlim
N!1eDN=D: (D7)
Inverting the above expressions allow us to express the Gram matrix Gand second-order moment matrix Din terms of the
estimateseGandeDcomputed using a Ô¨Ånite number of shots S,
G=S
S 1eG 1
S 1eD; (D8)
D=eD: (D9)
We see that to lowest order in1
S,GeGandDeD, which is what one might expect naively. However, we clearly see that
the estimation of Gcan be improved by including a higher-order correction in1
S. This contribution arises due to the highly-
correlated nature of noise and signal for quantum systems: we are able to estimate the noise matrix eGandeDusing knowledge
of the readout features, and correct for the contribution to eGandeDthat arises from this noise matrix. We will see that this
contribution will be important in more accurately approximating quantities of interest derived from G,D.
To this end, we recall that our ultimate aim is not just to estimate GandD, but to solve the eigenproblem of Eq. (C46). Using
the above relation, we can then establish eD 1eG=S 1
SD 1G+1
SI, and write Eq. (C46) in a form entirely in terms of eGand
eD,
D 1Gr(k)= (1 +2
k) 1r(k);
=)eD 1eGr(k)=S 1
S(1 +2
k) 1+1
S
r(k): (D10)
Note that the Ô¨Ånal form is conveniently another eigenproblem, now for the Ô¨Ånite- SmatrixeD 1eG:
eD 1eG~r(k)= (1 + ~2
k) 1~r(k)~k~r(k); (D11)
whose eigenvalues and eigenvectors can be easily related to the desired eigenvalues 2
kand eigenvectors r(k)of Eq. (C46).
Following some algebra, we Ô¨Ånd:
2
k=S
(S 1) ~2
k¬∑~2
k=~2
k+1X
j=1~2
k
1 +~2
kj1
Sj
; (D12)
r(k)=~r(k): (D13)21
5 10 15 20 25 30
Orderk100101102103104105Eigen-NSRs Œ≤2
k
Œ≤2
k,S‚Üí‚àû
ÀúŒ≤2
N,k,S= 102
ÀúŒ≤2
k,S= 102
S¬∑ÀúŒ≤2
N,k
(S‚àí1)‚àíÀúŒ≤2
N,k,S= 102
S= 102
FIG. 6. Eigen-analysis in L= 5 H-ansatz system by taking S= 102shots on each of N= 104samples, with true eigen-noise-to-signal
ratios2
k(black),S-Ô¨Ånite sampled ~2
N;k(blue) and corrected (S¬∑~2
N;k)=((S 1) ~2
N;k)(purple). ~2
k, the largeNlimit of ~2
N;kis also
plotted in red for comparison. The data correction is necessary since all ~2
N;kare below the S= 102, and the corrected data show much better
performance even if 2
kS. The estimated line (in purple) are cutoff at k= 25 since all sampled ~2
N;kafter that are larger the S 1so that
they are not correctable.
From Eq. (D12), we see that to lowest order in1
S,2
k~2
k. However, this expression also supplies corrections to higher orders
in1
S, which are non-negligible even for 2
k<S, as we see in example of Fig. 6. In contrast, the estimated eigenvectors ~r(k)to
anyorder in1
Sequal the desired eigenvectors r(k)without any corrections.
Of course, in practice we do not have access to the matrices eGandeD, as these are only deÔ¨Åned precisely in the limit
whereN!1 . However, for large enough N, we can approximate these matrices to lowest order by their Ô¨Ånite Nvalues,
eG=eGN+O 1
N
andeD=eDN+O 1
N
. Then, the eigenproblem in Eq. (D11) can be expressed in the Ô¨Ånal form,
eD 1
NeGN~r(k)
N= (1 + ~2
N;k) 1~r(k)
N~N;k~r(k)
N; (D14)
where the eigenvalues ~2
N;k;~N;kand eigenvectors ~r(k)
Nin the largeNlimit must satisfy
lim
N!1~2
N;k=~2
k;lim
N!1~N;k= ~k;lim
N!1~r(k)
N=~r(k)r(k): (D15)
Here the invertibility of the empirically-computed matrix eDNrequired for Eq. (D14) is numerically checked, based on which
we can establish a better numerical method in Appendix D 2.
Eq. (D14) represents the eigenproblem whose eigenvalues ~2
N;kand eigenvectors ~r(k)
Nwe actually calculate. For large enough
Nand under Ô¨Ånite S, we can use these as valid approximations to the eigenvalues and eigenvectors of Eq. (D11). This Ô¨Ånally
enables us to directly estimate the N;S!1 quantities2
kandr(k)using Eqs. (D12), (D13):
2
kS¬∑~2
N;k
(S 1) ~2
N;k=1 ~N;k
~N;k 1
S; (D16)
r(k)~r(k)
N: (D17)
It is clear that the approximation of 2
kto lowest order will be an underestimate, as the contribution of order1
Sis positive. In
Fig. 7, we plot the estimated eigenvectors ~r(k)
Ncomputed under Ô¨Ånite statistics ( N= 300;S= 1000 , where these two numbers
are relevant for IBM quantum processors) in H-encoding, together with the N;S!1 eigenvectorsr(k), and the estimated
eigenvalues.22
0 5 10 15‚àí101CoeÔ¨Écientr(k)r(1):Œ≤2
1= 0.0v.s. Àúr(1)
N:1‚àíÀúŒ±N,1
ÀúŒ±N,1‚àí1/S= 0.0
Eigenvector of D‚àí1G
Eigenvector of ÀúD‚àí1
NÀúGN
0 5 10 15‚àí0.250.000.25r(2):Œ≤2
2= 4.656v.s. Àúr(2)
N:1‚àíÀúŒ±N,2
ÀúŒ±N,2‚àí1/S= 4.663
0 5 10 15‚àí0.50.0r(3):Œ≤2
3= 5.898v.s. Àúr(3)
N:1‚àíÀúŒ±N,3
ÀúŒ±N,3‚àí1/S= 6.03
0 5 10 150.00.5r(4):Œ≤2
4= 12.661v.s. Àúr(4)
N:1‚àíÀúŒ±N,4
ÀúŒ±N,4‚àí1/S= 12.824
0 5 10 150.00.5CoeÔ¨Écientr(k)r(5):Œ≤2
5= 17.548v.s. Àúr(5)
N:1‚àíÀúŒ±N,5
ÀúŒ±N,5‚àí1/S= 17.571
0 5 10 15‚àí0.50.00.5r(6):Œ≤2
6= 21.166v.s. Àúr(6)
N:1‚àíÀúŒ±N,6
ÀúŒ±N,6‚àí1/S= 22.382
0 5 10 15‚àí0.50.0r(7):Œ≤2
7= 29.809v.s. Àúr(7)
N:1‚àíÀúŒ±N,7
ÀúŒ±N,7‚àí1/S= 30.513
0 5 10 15‚àí0.50.0r(8):Œ≤2
8= 51.107v.s. Àúr(8)
N:1‚àíÀúŒ±N,8
ÀúŒ±N,8‚àí1/S= 51.635
0 5 10 15‚àí0.50.00.5CoeÔ¨Écientr(k)r(9):Œ≤2
9= 69.874v.s. Àúr(9)
N:1‚àíÀúŒ±N,9
ÀúŒ±N,9‚àí1/S= 71.21
0 5 10 15‚àí0.50.00.5r(10):Œ≤2
10= 111.001v.s. Àúr(10)
N:1‚àíÀúŒ±N,10
ÀúŒ±N,10‚àí1/S= 109.021
0 5 10 150.00.5r(11):Œ≤2
11= 151.254v.s. Àúr(11)
N:1‚àíÀúŒ±N,11
ÀúŒ±N,11‚àí1/S= 144.208
0 5 10 15‚àí0.50.00.5r(12):Œ≤2
12= 248.423v.s. Àúr(12)
N:1‚àíÀúŒ±N,12
ÀúŒ±N,12‚àí1/S= 233.445
0 5 10 15
Indexk/primeofr(k)
k/prime0.00.5CoeÔ¨Écientr(k)r(13):Œ≤2
13= 333.471v.s. Àúr(13)
N:1‚àíÀúŒ±N,13
ÀúŒ±N,13‚àí1/S= 348.828
0 5 10 15
Indexk/primeofr(k)
k/prime‚àí0.50.0r(14):Œ≤2
14= 416.321v.s. Àúr(14)
N:1‚àíÀúŒ±N,14
ÀúŒ±N,14‚àí1/S= 409.548
0 5 10 15
Indexk/primeofr(k)
k/prime‚àí0.50.00.5r(15):Œ≤2
15= 655.346v.s. Àúr(15)
N:1‚àíÀúŒ±N,15
ÀúŒ±N,15‚àí1/S= 743.085
0 5 10 15
Indexk/primeofr(k)
k/prime‚àí0.50.00.5r(16):Œ≤2
16= 2191.863v.s. Àúr(16)
N:1‚àíÀúŒ±N,16
ÀúŒ±N,16‚àí1/S= 1945.381
FIG. 7. Estimating noise-to-signal ratio eigenvalues and corresponding eigentask coefÔ¨Åcients under Ô¨Ånite statistics ( N= 300;S= 1000 ) in a
4-qubit H-encoding system, and comparison with theoretical value for N!1;S!1 .
2. Gram matrix-free construction to approximate eigentasks and NSR eigenvalues
If we consider Eq. (D14) and multiply through by D 1
2
N, the resulting equation can be written as an equivalent eigenproblem,
1
NeD 1
2
NeFT
NeFNeD 1
2
N
eD1
2
N~r(k)
N
= ~N;k
eD 1
2
N~r(k)
N
(D18)
where we have also written eGN=1
NeFT
NeFNas in the previous section. Note that as written above, the eigenproblem is
entirely equivalent to obtaining the singular value decomposition of the matrix1p
NeD 1
2
NeFT
N. This particular normalization factor
1p
NeD 1
2
Nis different from the standard z-score of principal components analysis. To obtain the combination coefÔ¨Åcients r(k),
lett(k)2RKbe the left singular vector of1p
NeD 1
2
NeFT
N(which is also the eigenvector of1
NeD 1
2
NeFT
NeFNeD 1
2
ND 1
2eGD 1
2
in the largeNlimit). Thenr(k)=eD 1
2
Nt(k)2RKcan be treated as the combination prefactor of ^Mk, to obtain the observables
which correspond to the eigentasks. The merit of SVD analysis of1p
NeD 1
2
NeFT
Nis that we only need to work with a K-by-N
matrix of features eFN, which is numerically cheaper than further constructing a Gram matrix1
NeFT
NeFN. We will explore more
about the usage of our technique in sense of PCA in Appendix I.
Appendix E: H-ansatz quantum systems: NSR spectra, expressive capacity, and eigentasks
In this section, we evaluate the EC for quantum systems described by the H-ansatz introduced in Appendix B, as an example
of how EC can be efÔ¨Åciently computed for a variety of general quantum systems, and is not just restricted to parameterized
quantum circuits. The results of the analysis are compiled in Fig. 8, and discussed below.
Fig. 8(a) presents the set of features fXk(u)gfor typicalL= 6 qubit CS and PS at S= 1000 with randomly chosen
parameters (referred to as encodings, see caption). The resultant noisy eigentasks fy(k)(u)gand NSR spectraf2
kgextracted
via the eigenvalue analysis are shown in Figs. 8(b) and 8(c) respectively. In the side-by-side comparison in Fig. 8(b), we clearly
see theJ= 0 ansatz transitioning to a regime with more noise at much lower kthan theJ6= 0 ansatz. This is reÔ¨Çected in23
FIG. 8. Eigen analysis in a 6-qubit H-ansatz system (with N= 5000 andS= 1000 ) forming a 1D ring. The Hamiltonian parameters
are chosen randomly with zero-mean and variance (hx
rms;hz
rms;hI
rms) = (20;5;5), andt= 5 (See Appendix B for details). Coupling
strength is uniformly J6= 0 (correlated system) or J= 0 (product system). (a) All 2L= 64 noisy features Xk(u)and (b) noisy eigentasks
y(k)(u) =r(k)¬∑X(u)for selectedkfrom the features in (a), as well as their expected values y(k)(u) = lim S!1y(k)(u) =r(k)¬∑x(u)
(black). (c) Noise-to-signal ratio spectrum 2
kand (d)CTvs shotsSfor both correlated system and product system encodings. (e) CTat
S= 105and (f) ETC, T(^M)in representative random 6-qubit H-ansatz, as a function of coupling strength J. The peaks of capacity and
correlation coincide, around Jhx
rms.
Fig. 8(c), the 2
kspectrum, having a much Ô¨Çatter slope for larger k(note the plot is semilog). Finally, Fig. 8(d) shows the EC of
both systems as a function of S. EC rapidly rises for small Sfor both systems, but the rise of the J= 0system is steeper. After
a certain threshold in S, however, the CS grows more rapidly, approaching the upper bound 26= 64 withS108; in contrast,
the PS has a signiÔ¨Åcantly lower CT.
ForJ!1 we also have T= 0 because ^0=j0ih0j
Lis an eigenstate of the encoding ( ^(u) = ^0). This implies there
must be a peak at some intermediate J, which for both EC and ETC occurs when the coupling is proportional to the transverse
Ô¨ÅeldJhx.
Our results elucidate the same kind of improvement, as can be observed when we consider how the EC Cchanges with J, and
compare it to the total correlation ETC T, as shown in Fig. 8(f). For J!0we have a PS with T= 0, whereas in the J!1
we also have T= 0because ^0=j0ih0j
Lis an eigenstate of the encoding ( ^(u) = ^0). This implies there must be a peak at
some intermediate J, which for both EC and ETC occurs when the coupling is proportional to the transverse Ô¨Åeld Jhx. At
Ô¨ÅniteS, increased ETC is directly related to a higher EC.
Another interesting aspect is the clear trend seen in the maximization of EC around Jhx
rmsfor varioushx
rms, possibly hinting
at the role of increased correlation around the MBL phase transition in random spin systems [38]. This trend is consistent with
results in quantum metrology ‚Äì in general, the SNR obtained from averaging Luncorrelated probes scales as 1=p
L. This scaling
can become favorable in the presence of quantum correlation and other non-classical correlations, in which case the scaling of
the SNR can show up to a quadratic improvement 1=L[37]. For even larger J, we Ô¨Ånd that ^(u)!^0=j0ih0j
L, which
clearly reduces T, but alsoCTas the quantum system state becomes u-independent.
Appendix F: Scaling with quantum system size
An important question in quantum machine learning applications is the possible advantage of using larger quantum systems
for information processing. In this section, we present preliminary results of scaling with quantum system size. The left panel of
Fig. 9 shows EC vs Lat selectSvalues for H-ansatz, while the right panel shows two encodings in the C-ansatz device, as well
as their noisy simulations. In both plots, the dashed line indicates the S!1 resultCT= 2L. We see that the EC increases
when adding more qubits into the Ising chain for the H-ansatz, or when increasing the number of circuit qubits Lfor the C-
ansatz. Note, however, that at any Ô¨Ånite Sthe noise-constrained EC falls off the exponential bound for S!1 . The dropoff
is particularly severe for the IBMQ device, where we are limited to just S104, which signiÔ¨Åcantly suppresses the EC even24
forL= 7 qubits. Note, however, that even if one is well below CT= 2Ldue to this Ô¨Ånite sampling constraint, increasing the
dimension of the quantum system is always an effective way to increase the EC, particularly when compared to the logarithmic
growth with Sof Fig. 2 of Main Text.
3 4 5 6 7
Qubit numbers L0326496128 ECCT
S= 101
S= 102
S= 103
S= 104S= 105
S= 106
S= 107
S‚Üí‚àû
3 4 5 6 7
Qubit numbers L02040 ECCT
S=10
S=27
S=210
S=214
S‚Üí‚àû
FIG. 9. (a) H-ansatz and (b) C-ansatz at Ô¨Ånite Sas a function of qubit number L. Various colours indicate different Svalues, with the S!1
bound in dashed black. Individual noisy simulations are indicated in small and transparent dots, with their average as a thick line, and the
expressive capacity of the C-ansatz device for encoding 1 and 2 are indicated with ‚Äò ‚Äô and ‚Äò +‚Äô respectively.
Appendix G: Analytic solution to the quantum 2-design expressive capacity
The scaling of expressive capacity with system size in general is hard to quantify, and is likely best approached with a
numerical or experimental study for a speciÔ¨Åc quantum model, as done in Fig. 9. However, we can analytically solve for the EC
of a class of quantum models: 2-design parametric quantum circuits fp(u)du;^U(;u)g. We clarify that we are referring here
to systems with speciÔ¨Åc parameters which result in 2-designs with respect to the input distribution p(u); the ensemble average
is taken with respect to inputs u. Quantum literature [21] often refers to general ans ¬®atze which form 2-designs with respect to
parametersinstead, which is not what we are considering here.
To be more speciÔ¨Åc, an ensemble fp(u)du;^U(;u)gis a2-design if the following two quantum channels, deÔ¨Åned on any
2L-qubit state, ^are equal
C(^) =Z
^U(;u)
2^0(^U(;u)y)
2p(u)du=Z
^U
2^0(^Uy)
2dH(^U): (G1)
whereHis the uniform (Haar) measure. We can verify that all information in the Gram matrix is explicitly contained in the
elements ofC(^0
^0). To be more speciÔ¨Åc,
hbk1;bk2jC(^0
^0)jbk1;bk2i
=hbk1;bk2jZ
(^U(;u)
^U(;u))jb0;b0ihb0;b0j(^U(;u)y
^U(;u)y)p(u)du
jbk1;bk2i
=Zhbk1j^U(;u)jb0i2
¬∑hbk2j^U(;u)jb0i2
p(u)du
=Z
xk1(u)xk2(u)p(u)du= (G)k1k2: (G2)
However,C(0) =R
U
2(^0
^0)(Uy)
2dH(U)implies that we can compute the Gram matrix by instead integrating over
the Haar measure [39]:
(G)k1k2=Z
jU0;k1j2jU0;k2j2dH(U) =(2
K(K+1);ifk1=k2;
1
K(K+1);ifk16=k2:(G3)
Then the corresponding second-order matrix Dis given by
(D)kk=2
K(K+ 1)+ (K 1)1
K(K+ 1)=1
K: (G4)25
It is self-consistent that the matrix D= diag 1
K;1
K;;1
K
obeys the normalization condition Tr(D) =K¬∑1
K= 1. Then we
can solve the eigenvalues fkgk2[K]of random walk matrix (D 1G)k1k2=1
K+1(1 +k1k2). It gives
k=1
K+ 1(1 +Kk0): (G5)
Furthermore, we use k=1
1+2
kor2
k=1
k 1to compute the eigen-NSR:
(2
0;2
1;2
2;;2
K 2;2
K 1) = (0;K;K;;K;K ): (G6)
Then the expressive capacity of any 2-design system is given by
CT= 1 +K 1
1 +K
S=K1 +1
S
1 +K
S= 2LS+ 1
S+ 2L: (G7)
Appendix H: Quantum correlation metrics
There is no one standard metric to quantify correlation in a many-body state. The metric we introduce here, the quantum total
correlation , is a quantity inspired by the classical total correlation of Lrandom variables (b1;;bL), that isPL
l=1H(bl) 
H(b1;;bL). Using chain rule of Shannon entropy H(b1;b2;;bL) = H(b1) + H(b2jb1) ++ H(bLjb1;b2;;bL 1)
LX
l=2H(bl) H(b1;b2;;bL) =LX
l=1H(bl) LX
l=1H(bljb1;b2;;bl 1) =LX
l=2I(b1;;bl 1;bl)2[0;L 1]; (H1)
we can see that the classical total correlation tells us how a set of random variables reveals information of each other. Similarly,
quantum total correlation can be deÔ¨Åned as [33, 34]
T(^) =LX
l=1S(^l) S(^) (H2)
where Sis von Neumann entropy and ^l:= Tr [L]nflgf^gis the subsystem state at qubit l. Due to the subadditivity of von-
Neumann entropyPL
l=1S(^l)S(^), we conclude that the quantum total correlation is non-negative, and is zero iff the state
^=NL
l=1^lis a product state.
In this paper‚Äôs measurement scheme, the speciÔ¨Åc readout POVMs are the projectors onto the computational states
fjbkihbkjgk2[K]. Thus, we are in particular interested in analyzing the post-measurement state ^M(u) =P
kkk(u)jbkihbkj
whose subsystems are correspondingly in states ^M
l(u) = Tr [L]nflg
^M(u)	
. We compute the average or expected quantum
total correlation over the input domain uwith respect to the input probability distribution p(u):
T 
^M
=Eu"LX
l=1S(^M
l(u)) S(^M(u))#
=Eu"LX
l=1H(bl(u)) H(b1(u);;bL(u))#
(H3)
where the second equality comes from the diagonal nature of post-measurement state which reduces the quantum total correlation
to a normal classical total correlation.
The post-measurement quantum total correlation always reaches its maximum L 1when the diagonal terms of the state
is a GHZ-type state. Also as a comparison, for a W-statejWi=1p
L(j100i+j010i++j001i), then post-
measurement quantum total correlation T(jWi)is
L
 1
L
log21
L
 L 1
L
log2L 1
L
 L
 1
L
log21
L
= (L 1) log2L
L 1
: (H4)
which is upper bounded by limL!1T(jWi) =1
ln(2)1:443.26
Appendix I: Guidance from EC theory: principal component analysis with respect to quantum noise
Our proposed capacity spectrum analysis has another signiÔ¨Åcant beneÔ¨Åt: it provides a natural truncation scale for eigentasks.
In machine learning theory, the technique of projection of a high-dimensional data to a subspae of reduced dimensionality is
called principal component analysis . Within the computing architecture we are discussing, we are interested in carrying out a
PCA in the function space. More speciÔ¨Åcally, consider a given function f(u), we hope to Ô¨Ånd K0functionsfG(k)(u)gk2[K0]
whereG(k)(u) =PK 1
k0=0g(k)
k0xk0(u)lies in the space spanned by measured features G(k)(u)2Spanfxg, such that the relative
mean square error
min
WEuf PK0
k=1WkPK 1
k0=0g(k)
k0Xk02
Eu[jfj2](I1)
is much smaller as possible. According to Appendix C, the solution to fg(k)gk2[K0]is exactlyg(k)=r(k). Fig. 10 supplies a
concrete example of Ô¨Åtting linear function f(u) =u, by settingK0= 40 in a6-qubit system (and thus K= 64 ).
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu‚àí1.5‚àí1.0‚àí0.50.00.51.01.56-qubit, 40 principal xk(u), with retrain
Combination of ¬ØXk(u)
Combination of xk(u)
Target function f(u) =u
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu‚àí1.5‚àí1.0‚àí0.50.00.51.01.56-qubit, 40 principal y(k)(u), no retrain
Combination of ¬ØXk(u)
Combination of xk(u)
Target function f(u) =u
FIG. 10. Projection onto 40-dimensional space spanned by 40principalxk(u)vs. spanned by Ô¨Årst 40eigentasksy(k), in a 6-qubit H-encoding
system. The number of shots is Ô¨Åxed as S= 5000 .
Fig. 10(a) shows the projection onto the space spanned by the dominant 40readout features. Here, by ‚Äúdominant‚Äù we mean
one can Ô¨Årst train by least square regression to get an output weight w2RK, and then select corresponding wkwith the leading
K0largestw2
k¬∑Eu[jxkj2]. Then we need to use these K0features to retrain and obtain a new output weight w02RK0. In such
particular example, g(k)are some one-hot vectors where the index of 1are chosen by the sorting K0largestw2
k¬∑Eu[jxkj2]as we
described before. We can compare the relative mean square error with the case of g(k)=r(k), the eigentasks. As illustrated in
Fig. 10(b) the latter is able to achieve an approximation of the desired function (here a linear function) with a decidedly smaller
relative mean square error.
One important question is: what would be an appropriate choice of K0in practice? In Appendix D we claim that it is
determined by the set of eigentasks for which 2
k=S < 1, those for which the signal is larger than the noise. Namely we should
deÔ¨Åne the cut-off Kc(S)such that
Kc(S) = max
2
k<Sk: (I2)
Based on this observation, we can further explore the trend of Kc(S)when qubit number Lis scaled. As we show in the main
text, the eigen-NSR spectra grow much slower when Lincreases. Then the quantum system is able to provide much more
eigentasks with more signal than noise. Fig. 11(a) shows spectrum in H-encoding quantum system with sizes ranging from
L= 3toL= 8with Ô¨Åxed hyperparameters. Notice that number of shots S= 5000 here is not a large number, which means that
we cannot sample enough shots so that features converge to their mean values in the 28= 256 dimensional Hilbert space. But
applying eigentasks analysis in this example still shows a fast decay of relative error minWEu[jf PKc(S)
k=0Wky(k)j2]=Eu[jfj2]
until the Ô¨Åtting accuracy saturates at L= 8.27
FIG. 11. PCA for different CS H-encoding system size L= 3;4;5;6;7;8with Ô¨Åxed hyperparameters and S= 5000 . (a) Eigen-noise-to-
signal ratios spectrum of different sized system. (b) Relative error minWEu[jf PKc
k=1Wky(k)j2]=Eu[jfj2]for Ô¨Åttingf(u) =u, whereKc
can be read out from (a). (c) Combination of KceigentasksPKc(S)
k=0wky(k)(u)and noisy eigentasksPKc(S)
k=0wky(k)(u)inL= 5;6;7;8
qubits system.
Appendix J: Quantum-noise-PCA in classiÔ¨Åcation problem
The highly nonlinear readout feature xk(u)should have Taylor expansion xk(u) =P1
j(T)kjuj. Such complicated functions
will span a certain functional space. One fundamental question is what the limit of approximation ability is based on the
architecture we proposed. Hereby we Ô¨Årst show that this architecture under inÔ¨Ånite sampling is capable of approximating any
continuous function on the domain [ 1;1]to arbitrary precision. Furthermore, the linearity of quantum moment readout and
complexity of quantum evolution will help us to understand why such a quantum system has capability to approximate a highly
nonlinear function, under Ô¨Ånite and bounded computational resources. Exploring the capacity for function approximation under
Ô¨Ånite measurement resources, as is done in the main text and Appendix C, highlights the fundamental limitations placed by
quantum noise on computation using the reservoir computing approach.
1. Universal Function Approximation
A very general question is that what type of functions such a single-step quantum evolution can approximate. One conclusion
which can be drawn is the universal function approximation property. That is, give any continuous function from space of
continuous functions on domain [ 1;1],i.e.2C([ 1;1];R), for any given error " > 0, there always exists a function
'(u) =w¬∑x(u)such that
j'(u) (u)j" (J1)
for any input u2[ 1;1]. The proof employs the well-known Stone-Weierstrass theorem. For our particular architecture,
D= [ 1;1]is obviously a compact space, while point-separation can also be trivially fulÔ¨Ålled by a single qubit system ( L= 1).
The subalgebra structure of the function family generated by quantum systems is automatically satisÔ¨Åed in family of all product
systems, as long as we use a Walsh-Hadamard transformation to convert from the quantum probability x(u)to the many-body
Pauli-zproductsfhQ
i^z
l:l2Bi^gfor allB[L].28
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu‚àí101Combination of featuresmmax= 1
mmax= 2
mmax= 3
mmax= 4
mmax= 5
Sinusoid
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu‚àí101
mmax= 1
mmax= 2
mmax= 3
mmax= 4
mmax= 5
Steep tanh
FIG. 12. Function approximation by using y=PK 1
k=0wkxk(u)(solid red lines) to approximate sine function and steep tanh function
(dashed purple lines) in a 5-qubit quantum annealing system, where Ke=Pmmax
m=0 L
m
depends on different quantum moment thresholds
mmax= 1;2;3;4;5. The hyperparameters are (Jmax;hx;hx
rms;hI;hI
rms) = (1; 3;1; 5;2)in unit 1=tand nohzÔ¨Åeld. This simulation shows
that for some simple functions, it is sufÔ¨Åcient to merely use lower order moments, e.g.,mmax= 2 in sine function and mmax= 3 in steep
tanh function.
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu012ClassC0distribution
ClassC1distribution
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu0100200300
ClassC0sample histogram
ClassC1sample histogram
FIG. 13. (Left) Distribution p0(u)andp1(u)for classesC0andC1, respectively. (Right) The histogram of C0andC1. Each class contains
5000 samples.
2. 1D classiÔ¨Åcation as function approximation for noiseless measured features
In this section, we will show how the universal function approximation property of the architecture described in Appendix J 1
enables it to perform ‚Äì among others ‚Äì paradigmatic machine learning tasks such as classiÔ¨Åcation.
Suppose two classes C0andC1of samples, each of which is generated from distributions p0(u)andp1(u)respectively. The
probability of occurrence of C0andC1are both 50%, and we simply let each class equally contain 5000 samples and thus
N= 10000 samples in total. Both distribution are artiÔ¨Åcially deÔ¨Åned by summing several Gaussian distributions with different
amplitudes and widths together. Domain of both distributions are restricted in [ 1;1]and both distributions are also normalized.
Due to the overlap of two distributions, there is some theoretical maximal classical accuracy to distribution whether a given u
belongs to either C0orC1.
During the training, we feed each sample u(n)(belonging to class Cc(n)) into a 5-qubit quantum system. The quantum system
will be read out with Ke=Pmmax
m=0 L
m
different featuresfxk(u(n))gk2[Keff]. Then features of Nsample forms the regressor
matrix. According to the standard supervised learning procedure, we simply train based on (x(u(n));c(n))by logistics regression
where one should minimize the cross-entropy loss
L(W) =1
NNX
n=1
 c(n)log
(W¬∑x(u(n)))
 
1 c(n)
log
1 (W¬∑x(u(n)))
(J2)29
1 2 3 4 5
Output feature threshold order mmax82848688 Testing Accuracy (%)
Theoretical maximal accuracy
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu0.000.250.500.751.00Probabilitymmax= 1
mmax= 2
mmax= 3
mmax= 4
Pr[u‚ààC1|u]
FIG. 14. 1D classiÔ¨Åcation as function approximation in a 5-qubit quantum system with full connectivity. The hyperparameters are
(Jmax;hx;hx
rms;hI;hI
rms) = (1; 3;1; 8;5)in unit 1=tand nohzÔ¨Åeld. (Left) Testing accuracy as a function highest order mmax of mo-
ment feature. (Right) Conditional distribution Pr[u2C1ju](purple dashed line) vs. readout features (w¬∑x(u))withmmax= 1;2;3;4
(red solid line). mmax= 4saturates the approximation accuracy.
whereis the sigmoid function (y) =1
1+e y. A smallL2penaltykWk2(where= 10 6) is added to Eq. (J2) for
preventing overÔ¨Åtting. The optimal Wis then simply the set of weights that minimizes this cost function,
w= argminWfL(W)g (J3)
We test the Ô¨Ådelity of learning the classiÔ¨Åcation task by determining the accuracy of classiÔ¨Åcation on a testing set formed
by drawingN= 10000 new samples (independent of the training set) as a function of the order of output moments extracted,
mmax= 1;2;3;4;5, corresponding to reading out Ke= 6;16;26;31;32features respectively. The resulting testing accuracy
is plotted in the left panel of Fig. 14). We see that the testing accuracy converges to the theoretical maximal accuracy (dashed
green) with increase in readout features.
Importantly, one can show that this improvement in learning performance coincides with training of optimal weights wsuch
that the QRC is able to approximate the conditional distribution Pr[u2C1ju]of the two classes with increasing accuracy (lower
error). To verify this, we Ô¨Årst numerically compute all K= 32 readout feature functions x(u)of the system, by sweeping 500
equidistant values of u2[ 1;1]. Effectively learning the conditional distribution means that (w¬∑x(u))Pr[u2C1ju]. It is
equivalent to use w¬∑x(u)to approximate the following function:
w¬∑x(u) 1(Pr[u2C1ju]): (J4)
We therefore see that the function approximation universality property of the architecture discussed in Appendix J 1 enables its
use as a generic classiÔ¨Åer.
3. Solving classiÔ¨Åcation problem by quantum-noise-PCA
Now we can solve the classiÔ¨Åcation task above by using the quantum-noise princilpal component analysis we learn from
capacity analysis. Suppose a physical system with L= 5 qubits and ring connectivity, we choose the hyperparameter to be
J= 2,hx
rms=hz
rms=hI
rms= 5andt= 3. In this H-encoding scheme, we can obtain K= 32 measured features on each of
N= 105samplesfu(n)g(5000 in classC0and5000 in classC1). We emphasize here that the underlying marginal distribution
p(u)is no longer uniform here, and it will make both f2
kgandfr(k)gvery different.
Given the number of shots S2[101;105], we can still compute the empirical ~r(k)
Nand estimating 2
kby using the correction
techniques we used in Appendix D. By comparing the estimated (1 ~N;k)=(~N;k 1
S)andS, we can Ô¨Ågure out the cutoff
orderKc(S)and combination coefÔ¨Åcients ~r(k)
N, based on which we can deÔ¨Åne a set of observables
^Ok=K 1X
k0=0~r(k)
N;k0^Mk0k= 0;1;;Kc(S): (J5)30
‚àí1.0‚àí0.5 0.0 0.5 1.0
Inputu0.000.250.500.751.00ProbabilityTraining
Testing
Pr[u‚ààC1|u]
101102103104105
ShotsS60708090 Accuracy (%)
Training
Testing
Theoretical maximal accuracy
FIG. 15. (Left) The linear combination with sigmoid activation, that is the stochastic function PKc(S)
k0=1wk0;Train(~r(k)
N¬∑XTrain)k0
(blue
line) andPKc(S)
k0=1wk0;Train(~r(k)
N¬∑XTest)k0
(red line), compared with the true conditional probability Pr[u2C1ju](black line). (Right)
Training accuracy and testing accuracy. They saturate the theoretical maximal accuracy as Sreaches 104105. Their agreement shows the
quantum measurement noise serves well as a regularizer.
It is equivalent to say, by measuring ^Ok, we can effectively obtain eigentasks ~r(k)
N¬∑XTrain . Then we can apply standard logistics
regression on those eigentasks as we did in Eq. J2. The only difference is we no longer need any regularization term as penalty
likekWk2. The training procedure eventual yield wTrain2RKc(S), together with ~r(k)
NandKc(S).
Now we generate a totally new and independent set of u‚Äôs for testing purpose. By measuring ^Ok, one get eigentasks
~r(k)
N¬∑XTest. By plugging wTrain2RKc(S)+1, together with ~r(k)
NandKc(S)in training, we can achieve the testing accu-
racy. The agreement between training and testing accuracy show that the quantum measurement noise effectively works as a
regularizer, and do a pretty good job (see Fig. 15).
Appendix K: Finite sampling bound and uncertainty propagation
We conclude that the principle advantage brought about by correlation in this sections. There we observe that for certain inputs
u(that depend on the input encoding) the measurement of an CS when mapped into the moment space can generate distributions
that can be highly anisotropic at Ô¨Ånite S. While for PS these distributions are generally isotropic unless they are close to the
boundaries of the output domain (when the encoding produces outputs that are eigenstates of the measurement basis). We
observe that this trend is also present in the experimental system despite non-idealities. The origin of higher expressive capacity
at largeSprovided by ESs can be traced back to this basic feature. To be more speciÔ¨Åc, let ^Nk= ^z
l1^z
l2^z
lm, and Xk(u)be
empirical mean based on Ssampling of Pauli- zproducts. Notice that the variance of Xknow has an alternative fomr of
Var[ Xk] =1
S 
h(^z
l1^z
l2^z
lm)2i h^z
l1^z
l2^z
lmi2
=1
S(1 x2
k(u)): (K1)
Thus,
Xk(u) =xk(u) +k(u) =xk(u) +1p
Sk(u); (K2)
where random sampling noise k(u)p
1 x2
k(u)andis a random Ô¨Çuctuation with variance 1. For quantum moment
readout, the amplitude of relative error is
k(u)
xk(u)s
1 x2
k(u)
x2
k(u)1p
S/1p
S: (K3)31
12345678910
Highest order m10‚àí11001011/SNR (Log)
CS, Ô¨Åt
PS, Ô¨Åt
FIG. 16. Noise-to-signal ratio of correlated system vs product system in a 10-qubit quantum annealing system with shot number S= 1000 by
feedingu= 1=2. The hyperparameters are chosen to be (hx;hx
rms;hz;hz
1;rms) = (8;2; 3;2)in unit 1=t. The purple and red colors correspond
to coupling being switched on and off, respectively; and the coupling hyperparameter in CS is Jmax= 2=t. For eachm, theN= 30 dots
are relative error x(r)
k(u)=xk(u) 1of 30 repetitions r= 1;2;;30. The standard deviation of those relative errors (namely NSR) are
also plotted. The correlated system NSR (purple stars) is well Ô¨Åtted by O(1=p
S)(purple dashed line) while the product system NSR (red
stars) scales exponentially as O(2m=p
S)(purple dashed line). We take y-axis being log-scale, and one may Ô¨Ånd in these regime correlated
system 1/SNR grows exponentially faster than product system NSR (red stars) and hence product system readout scheme will be less powerful
in sense of quantum sampling noise resistant.
For classical polynomial readout the amplitude of relative error is obtained by rule of uncertainty propagation
(xl1(u) +l1)(xlm(u) +lm) xl1(u)xlm(u)
xl1(u)xlm(u)l1
xl1(u)++lm
xlm(u)
 s
1 x2
l1(u)
x2
l1(u)++s
1 x2
lm(u)
x2
lm(u)!
1p
S/m1p
S: (K4)
If there is no correlation in quantum system, then the readout features for both quantum moment readout and classical polynomial
readout are the same h^z
l1^z
l2^z
lmi=h^z
l1ih^z
l2ih ^z
lmi. However, even if the expectations under inÔ¨Ånite sampling limit
S!1 are the same, the measurement noise under Ô¨Ånite sampling are still different. For classical polynomial readout, the scal-
ing of still follows the simple additivity relation of uncertainty propagation in Eq. ( ??). But now the noise of xl1(u)xlm(u)
in quantum moment readout will be very strong, this is because xl1(u)xlm(u)is now close to zero, thus
k
xk(u)1
xk(u)1p
S=1
xl1(u)xlm(u)1p
S/2m1p
S: (K5)