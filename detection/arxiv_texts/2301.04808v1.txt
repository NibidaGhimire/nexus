arXiv:2301.04808v1  [math.CO]  12 Jan 2023Redundancy of Codes with Graph Constraints∗
Ghurumuruhan Ganesan†
IISER, Bhopal-462066, gganesan82@gmail.com
Abstract. In this paper, we study the redundancy of linear codes with
graph constraints. First we consider linear parity check co des based on
bipartite graphs with diversity and with generalized graph constraints.
We describe suﬃcient conditions on the constraint probabil ities and use
the probabilistic method to obtain linear codes that achiev e the Gilbert-
Varshamov redundancy bound in addition to satisfying the co nstraints
and the diversity index. In the second part we consider a gene ralization
of graph capacity which we call as the fractional graph capac ity and use
the probabilistic method to determine bounds on the fractio nal capacity
for arbitrary graphs. Speciﬁcally, we establish an upper bo und in terms
of the full graph capacity and a lower bound in terms of the ave rage and
maximum vertex degree of the graph.
Key words: Linear codes, bipartite graphs, fractional graph capacity .
1 Introduction
Codes based on graphs arise often in both theory and applications a nd it is im-
portanttounderstandredundanciesofsuchcodes.Typically,re dundancybounds
like Gilbert-Varshamov, Hamming and Singleton are obtained under th e Ham-
ming distance measure with no restrictions on the codes themselves . In many
applications, the code itself might have additional graph constraint s.
In this paper we study redundancy of codes with graph constraint s as de-
scribed in the following two subsections.
Linear Parity Check Codes
In the ﬁrst part of the paper, we study redundancy of linear parit y check codes
based on bipartite graphs with diversiy and with general constraint s. Linear
codes with graph based constraints arise often in applications. For example, low
∗An abridged version of this paper was presented in 23rd−Thailand-
Japan Conference on Discrete and Computational Geometry, G raphs, and
Games (TJCDCGGG, 2021) and is available in the book of abstra cts at
https://www.math.science.cmu.ac.th/tjcdcggg/
†Corresponding Author2 G. Ganesan
density parity check (LDPC) codes [15] that are used extensively in commu-
nication systems have the constraint that the left and right verte x degrees in
the bipartite graph representation are small compared to the tot al number of
vertices. Another example is expander codes [14] that requires th at the bipar-
tite graph representation to satisfy an expansion property with respect to the
left nodes. A linear time encoding and decoding algorithm for expande r codes
is also presented in [14]. Recently, [7] has proposed a local-decoding a lgorithm
for expander codes capable of correcting a constant fraction of errors using a
(relatively) small number of symbols from corrupted codeword.
InSection2, weconsiderlinearcodeswith diversityand generalizedg raphical
constraints and use random bipartite graphs to show that if the co nstraints are
not severe with regards to their probability of occurrence, then t here are codes
with diversity that attain the Gilbert-Varshamov redundancy boun d in addition
to satisfying the said constraints.
Fractional Graph Capacity
The capacity of a graph was introduced in [13] to determine the limits o f
error-free communication across channels with an underlying conf usion graph.
Lov´ asz [11] obtained the expression for capacity of the cycle on 5 vertices using
“umbrella” projection methods and since then, many bounds as well as variants
ofthe capacityhavebeen studied. For example, [6] studied analog uesofShannon
capacity and its connections with the ultimate chromatic number. Ma rton [12]
obtained expressions for graph capacities for a sequence of grap hs based on typ-
ical sequences and more recently, [1] investigated the problem of a pproximating
graph capacity by ﬁnite graph products.
In this paper, we introduce the notion of fractional capacity of a g raph and
obtain bounds in terms of the graph degree parameters. Physically , fractional
capacity corresponds to the communication limits of channels that c orrupt at
most afraction of symbols in a long codeword. We ﬁrst use the probabilistic
method to obtain upper bound for the fractional capacity in terms of the full
graph capacity and a lower bound in terms of the average and maximu m vertex
degrees. We then consider a special class of graphs called square g raphs and
determine an upper bound for the fractional capacity in terms of t he minimum
degree of its root. In particular for regular square graphs, this o btains upper and
lower bounds for the fractional capacity in terms of the common ve rtex degree.
The paper is organized as follows: In Section 2 we study linear parity c heck
codes with graphical constraints and use random bipartite graphs to determine
achievability of the Gilbert-Varshamov bound. Next in Section 3, we d eﬁne frac-
tional capacity of a graph and state and prove our main result involv ing upper
and lower bounds for the fractional capacity.
2 Linear Parity Check Codes with Graph Constraints
We begin with some general deﬁnitions. For a ﬁnite set Xand integer n≥1,
ann−length word (based on X) is an element of Xn.Ann−length code Cis aRedundancy of Codes 3
subset of Xn.IfXis a ﬁnite ﬁeld, then we have the concept of linear codes: we
say that Cislinearif for any two words c,d∈ Cand any two scalars α,β∈ X,
the word α·c+β·d∈ Cas well.
For two words c= (c1,...,c n) andd= (d1,...,d n) inXn,we deﬁne the
Hamming distance between canddto be
d(c,d) =n/summationdisplay
i=11 1(ci/\e}atio\slash=di), (2.1)
where 1 1(.) refers to the indicator function. In this section, all distances ar e Ham-
ming. We deﬁne the minimum distance dH(C) of the code Cto be the minimum
distance between any two codewords of Cand set the relative distance of Cto
be
δH(C) :=dH(C)−1
n. (2.2)
The rate and redundancy of Care respectively deﬁned as
R(C) :=log(#C)
nlog(#X)(2.3)
and
ξ(C) := 1−R(C) = 1−log(#C)
nlog(#X), (2.4)
where logarithms are to the base two throughout and # Cdenotes the size of C.
In this section we consider binary linear codes with X={0,1}and begin
with a description of the random graph construction of linear parity check codes.
Consider a random bipartite graph with left vertex set X={a1,a2,...,a n}and
right vertex set Y={b1,b2,...,b m}obtained as follows. Let {Xi,j}1≤i≤n,1≤j≤m
be independent and identically distributed binary random variables wit h
P(X1,1= 1) =p= 1−P(X1,1= 0)
where 0< p <1
2is a constant. Throughout, constants do not depend on n.An
edge is present between vertices aiandbjif and only if Xi,j= 1.Letm=nǫfor
some constant ǫ >0 and let G=Gnbe the resulting random graph deﬁned on
the probability space ( Ωn,Fn,Pn).For simplicity we drop the subscript from Pn
henceforth.
For 1≤j≤mletRjbe the set of neighbours of the right vertex bjand
deﬁne the (random) code Cas follows. A word c= (c1,...,c n)∈ Cif and only if
⊕i∈Rjci= 0 for all 1 ≤j≤m. (2.5)
By construction (see [14]), the code Cis linear with rate at least 1 −m
n= 1−ǫ.
We now introduce two restrictions on C.The ﬁrst restriction which we call
as diversity is an expansion-type property with regards to the righ t nodes of the
bipartite graph. The other restriction which we call as constraints are general
events pertaining to the random graphas constructed in the prev iousparagraph.4 G. Ganesan
For 0< γ <1 we say that Chas adiversity index of at least γif
#(Rx\Ry)≥γ#Rxfor anybx,by∈Y. (2.6)
Thus any two parity nodes have at least a fraction γof diﬀerent neighbours.
We could think of condition (2.6) as a mild form of the expansion proper ty
with respect to the parity nodes . We remark that in the usual construction
via expander graphs, the condition for expansion is with regards to the (left)
codeword index nodes of the bipartite graphs (see [14]).
Wenowdeﬁneconstraintsonlinearcodesasfollows.A n−lengthconstraint En
is an event in Fn.For example, the event Hnthat for each 2 ≤i≤√nthe right
verticesbi−1andbi+1both have the left vertex aias a neighbour is an example
of a constraint. We say that the random graph Gsatisﬁes the constraint En
ifG∈ En.Given a collection of constraints Enand real numbers 0 < δ,γ < 1,we
would like to know if there is a linear code with relative distance δand diversity
indexγthat also satisﬁes the constraints. If so, what would be the redun dancy
of such a code?
If there were no constraints or diversity, then the Gilbert-Varsh amov bound
(Theorem 4 .2.1,[5]) implies that there exists an n−length code with redundancy
at mostH(δ)+o(1),whereo(1)−→0 asn→ ∞and
H(x) :=−x·logx−(1−x)·log(1−x) (2.7)
is the (binary) entropy function. Throughout logarithms are to th e base 2.Does
imposing diversity and constraints increase the redundancy of a line ar code?
The following result says that if the constraints are not too strict, then we can
still get linear parity check codes satisfying the Gilbert-Varshamov redundancy
bound and with a given diversity index. Constants mentioned throug hout do not
depend on n.
Theorem 1. Let0< δ <1
2and0< γ <1be any two constants and let {En}
be a collection of constraints such that the probability pn=P(En)satisﬁes
log/parenleftBig
1
pn/parenrightBig
n−→0 (2.8)
asn→ ∞.
There exists a deterministic n−length linear parity check code Dnwith rela-
tive distance at least δ,diversity index at least γ,redundancy H(δ) +o(1)and
satisfying the constraint En.
The condition (2.8) is satisﬁed, for example, if
P(En)≥e−f(n)
for some function fsuch thatf(n)
n−→0 asn→ ∞.For allnlarge, the code Dn
then attains the Gilbert-Varshamov bound in addition to satisfying t he con-
straintEn.Redundancy of Codes 5
For example, the event Hndescribed prior to the statement of Theorem 1
occurs with probability at least p2√nand so satisﬁes (2.8). Consequently, for
allnlarge, there exists a linear code with relative distance at least δ,diversity
index at least γ,redundancy H(δ)+o(1) and satisfying the constraint Hn.
Below, we obtain the desired code in Theorem 1 by the probabilistic met hod.
Proof of Theorem 1
The proof of Theorem 1 consists of three steps. In the ﬁrst step , we choose the
edge probability pto be an appropriate constant so that the diversity condition
is ensured. For such a choice of p,we show in the second step that the minimum
distance of the code Cas obtained in (2.5) is at least δn+1 with high probability
i.e., with probability converging to one as n→ ∞.Finally, in the third step, we
incorporate the constraints into C.Throughout we let ǫ > H(δ) be a constant
and letm=nǫbe the number of parity (right) nodes in the graph Gso that
the size of Cis at least 2n(1−ǫ).
Step 1 (Ensuring diversity) : LetCbe the linear code as obtained in (2.5). To
ensure that Csatisﬁes the diversity property, we argue as follows. Let bxandby
be any tworight vertex nodes. We havethat a left vertex aiis present in Rxwith
probability pandispresentin Rx∩Rywithprobability p2.Thereforebystandard
deviation estimates (Corollary A.1.14,pp. 312,[2]), we have for 0 < θ <1
4that
P(|#Rx−np| ≥npθ)≤exp/parenleftbigg
−θ2
4np/parenrightbigg
(2.9)
and that
P/parenleftbig/vextendsingle/vextendsingle#(Rx∩Ry)−np2/vextendsingle/vextendsingle≥np2θ/parenrightbig
≤exp/parenleftbigg
−θ2
4np2/parenrightbigg
. (2.10)
Letting
Rtot:=/intersectiondisplay
x{|#Rx−np| ≥npθ}
we then get that
P(Rtot)≥1−2m2e−θ2
4np2. (2.11)
Similarly, using np2< np,we get from (2.9) and (2.10) that the event
Fx,y:=/braceleftbig
#(Rx\Ry)≥np(1−θ)−np2(1+θ)/bracerightbig
occurs with probability at least 1 −2e−θ2
4np2and so letting Ftot:=/intersectiontext
x,yFx,y,
we then get that
P(Ftot)≥1−2m2e−θ2
4np2. (2.12)
From (2.11), (2.12) and the union bound, we therefore we get that the event
Ediv:=Rtot∩Ftotoccurs with probability
P(Ediv)≥1−4m2e−θ2
4np2. (2.13)6 G. Ganesan
IfEdivoccurs, then for any right nodes bx,bywe have
#(Rx\Ry)
#Rx≥np(1−θ)−np2(1+θ)
np(1+θ)
=1−θ
1+θ−p
which is at least γprovided θ,pare suﬃciently small constants. We henceforth
ﬁx such a p.
Step 2 (Estimating the minimum distance) : For a set S ⊆ {1,2,...,n}we
deﬁne the word v(S) = (v1,...,v n) satisfying
vi=/braceleftbigg
1 ifi∈ S
0 otherwise .(2.14)
LettingS={al1,...,a lg}be any set of left vertices we upper bound the
probability that Swould cause no parity check violations; i.e. we estimate the
probability that the word v(S) (see (2.14)) belong to the code C.We consider
two cases depending on whether # S ≤tor not, for some integer constant t≥1
to be determined later.
Case I ( #S=g≤t):For 1≤i≤gletNibe the set of neighbours of
the left vertex ali.LetMi:=Ni\/parenleftbigg/uniontext
1≤l≤t
l/negationslash=iNl/parenrightbigg
be the set of (unique) neigh-
bours of vertex alinot adjacent to any of the remaining vertices in S \ {ali}.
Deﬁning Yi,j:= 1 1(bj∈ Mi) we then have that {Yi,j}1≤j≤mare independent and
identically distributed for any 1 ≤i≤g,with
P(Yi,j= 1) =p(1−p)g−1= 1−P(Yi,j= 0).
ThusP(Mi=∅) =/parenleftbig
1−p(1−p)g−1/parenrightbigm≤e−mp(1−p)g−1and consequently
P
/uniondisplay
1≤i≤g{Mi=∅}
≤ge−mp(1−p)g−1. (2.15)
If the event E(S) :=/intersectiontext
1≤i≤g{Mi/\e}atio\slash=∅}occurs, then the word v(S)/∈ C.
Therefore if the event
Elow:=/intersectiondisplay
SE(S) (2.16)
occurs where the intersection is with respect to all subsets S ⊂ {1,2,...,n}
of sizeg≤t,then the minimum distance of any word in Cfrom the all zeros
codewordisatleast t+1.SinceCislinearthisimplies that theminimum distance
ofCis at least t+1.
We now see that Elowoccurs with high probability, i.e., with probability
converging to one as n→ ∞.IfEc
lowdenotes the complement of the set Elow,
then from (2.15) we have
P(Ec
low)≤t/summationdisplay
g=1g/parenleftbiggn
g/parenrightbigg
e−mp(1−p)g−1≤t2/parenleftbiggn
t/parenrightbigg
e−mp(1−p)t−1Redundancy of Codes 7
provided t <n
2.Using/parenleftbign
t/parenrightbig
≤/parenleftbigne
t/parenrightbigtwe further get that
P(Ec
low)≤e−∆0(2.17)
where
∆0:=mp(1−p)t−1−tlog/parenleftBigne
t/parenrightBig
−2logt.
Sincem=ǫnandp >0 is a constant, we have that
∆0≥m
2p(1−p)t−1≥4C·n (2.18)
for allnlarge and some constant C >0.
Case II ( t+1≤#S ≤δn):For a right vertex bj,we recall that Rjis the
random set of (left) neighbours of the vertex bj.Deﬁne the event
Fj(S) :={#(Rj∩S) is odd}.
IfFj(S) occurs, then the word v(S) would cause a parity check violation at
the right vertex bj.Therefore if/uniontext
1≤j≤mFj(S) occurs, then v(S)/∈ C.Deﬁne
the event
Eup:=/intersectiondisplay
S
/uniondisplay
1≤j≤mFj(S)
 (2.19)
where the intersection is with respect to all sets Swhose cardinality lies be-
tweent+ 1 and δn.Extending the above argument we see that if Eupoccurs,
then there is no word in Cwhose distance from the all zeros codeword lies be-
tweent+1andδn.Combiningwith theevent Elowdeﬁnedin(2.16),wehavethat
ifElow∩Eupoccurs, then the minimum distance of the code is at least δn+1.
We estimate the probability that Eupoccurs. For any right vertex bj,the
number of left neighbours # Rjis Binomially distributed with parameters n
andp.Therefore for a deterministic set Swith #S=g,the cardinality of
the random set #( Rj∩S) is Binomially distributed with parameters gandp.
Therefore
P(Fc
j(S)) =/summationdisplay
0≤k≤g
keven/parenleftbiggg
k/parenrightbigg
pk(1−p)g−k
=1
2((1−p+p)g+((1−p)−p)g)
=1
2+1
2(1−2p)g. (2.20)
Let 0< η <1
2be a small constant. Using g≥t+1 and choosing tsuﬃciently
large, we then get from (2.20) that
P/parenleftbig
Fc
j(S)/parenrightbig
≤1
21−η8 G. Ganesan
and so
P
m/intersectiondisplay
j=1Fc
j(S)
≤/parenleftbigg1
21−η/parenrightbiggm
=1
2(1−η)ǫn.
There are/parenleftbign
g/parenrightbig
sets of cardinality gand so from (2.19), we therefore have that
P/parenleftbig
Ec
up/parenrightbig
≤/parenleftBiggδn/summationdisplay
g=t+1/parenleftbiggn
g/parenrightbigg/parenrightBigg
·1
2(1−η)ǫn≤1
2βn, (2.21)
whereβ:= (1−η)ǫ−H(δ)andtheﬁnalinequalityin(2.21)followsfromstandard
Hamming ball estimates (Proposition 3 .3.1 [5]). Since ǫ > H(δ) strictly, we
chooseη >0 small enough so that βis strictly positive. Fixing such an η,we
deﬁneEdist:=Elow∩Eupand have from (2.21), (2.17) and (2.18) that
P(Edist)≥1−e−4Cn−1
2βn≥1−e−3Cn(2.22)
for allnlarge, where the constant C >0 is as in (2.18).
Combining (2.13) and (2.22) and using the fact that m=ǫn,we get from a
union bound that
P(Ediv∩Edist)≥1−4m2e−θ2
4np2−e−3Cn≥1−e−2Dn(2.23)
for allnlarge and some constant D >0.
Step 3 (Incorporating constraints) : From (2.8), we have that Enoccurs with
probability at least e−Dnfor allnlarge, where D >0 is as in (2.23). Therefore
from (2.23) we have
P(En∩Ediv∩Edist)≥e−Dn−e−2Dn>0
and this implies that there exists an n−length linear code with relative distance
at leastδ,diversity index at least γ,redundancy at most ǫ,and satisfying the
constraint En.
3 Fractional Graph Capacity
LetG= (V,E) be any connected graph containing # V=n≥3 vertices and
for a vertex u,letNG[u] be the set of all neighbours of u,including u.The
graph distance between any two nodes uandvis the number of edges in the
shortest path between uandv.A setF ⊂Vof vertices is said to be stable if
no two vertices in Fare adjacent to each other in G.We denote α(G) to be the
independence number, i.e. the maximum size of a stable set in G.
For integer r≥1 letG(r) be the rthstrong graph product of Gobtained
as follows: The graph G(r) has vertex set Vrand two vertices u= (u1,...,u r)Redundancy of Codes 9
andv= (v1,...,v r) are adjacent if and only if ui∈ NG[vi] for each 1 ≤i≤r.
For an integer 1 ≤k≤r,we now deﬁne the subgraph G(r,k)⊆G(r) as follows.
Two vertices v,u∈Vrare adjacent in G(r,k) if and only if uandvare adjacent
inG(r) and diﬀer in at most kentries i.e.,
r/summationdisplay
i=11 1(ui/\e}atio\slash=vi)≤k,
where 1 1(.) refers to the indicator function. We have the following deﬁnition.
Deﬁnition 1. For a real number 0< γ≤1we deﬁne the γ−fractional capacity
ofGto be
Θγ(G) := sup
r≥1
γ(α(G(r,γr)))1
r. (3.1)
Forγ= 1,the term Θ1(G) =:Θ(G) is the graph capacity as deﬁned in [13]. For
diﬀerentiation, we refer to Θ(G) as the fullgraph capacity and Θγ(G) as the
fractional graph capacity.
In the context of codes, each vertex of G(r) is a codeword of length rand
the term γrepresents the maximum fraction of symbols that undergo corrup tion
when passed through a channel with confusion graph G.The quantity ( Θγ(G))r
is then the maximum size of a code from G(r) that allows for error free commu-
nication.
For 0≤x≤1 we let H(x) be the entropy function as in (2.7) and have the
following result.
Theorem 2. LetGbe a connected graph on nvertices and let davand∆be the
average and maximum vertex degree of G,respectively. For any 0< γ≤1we
have that
n·max(f(γ,dav),f(γ,∆),f(γ,n−1))≤Θγ(G)≤n·/parenleftbiggΘ(G)
n/parenrightbiggγ
(3.2)
where
f(γ,x) :=

/parenleftbig
2H(γ)·xγ/parenrightbig−1for0< γ <x
x+1
(x+1)−1forx
x+1≤γ≤1.(3.3)
We have the following remarks:
Remark 1 : From (3.2) and the fact that dav≥1 (recall that Gis connected) we
see that for 0 < γ≤1
2,the fractional graph capacity grows at least asn
2H(γ)·dγ
av.
For example the graph G=C5,the cycle on 5 vertices, has dav=∆= 2
and it is well-known [11] that the full graph capacity Θ(G) =√
5. Therefore
settingγ=1
2,we get from (3.2) that the half graph capacity of C5satisﬁes
5
2√
2≤Θ1
2(C5)≤5
4√
5.10 G. Ganesan
Remark 2 : In general, we see from (3.2) that as γ→0,the fractional graph
capacity Θγ(G)→n,the maximumpossible value.Onthe otherend, setting γ=
1 in the lower bound (3.3), we get that the full graph capacity
Θ(G)≥n
dav+1
which is also obtained via the Tur´ an’s bound [16].
Remark 3 : From (3.1), we see that the upper bound for the fractional grap h
capacityisintermsofthefullgraphcapacity,whilethelowerboundis intermsof
vertexdegrees.In the next section, we consider a particularclas sofgraphscalled
square graphs and obtain upper andlower bounds for the fractional capacity in
terms of the vertex degrees.
Proof of Theorem 2
We provethe lowerbound in (3.2) using the probabilisticmethod and a m aximal
stable set argument (the Gilbert-Varshamov argument [8]) and pr ove the upper
bound in (3.2) using a recursion estimate similar to the Singleton argum ent [8].
Proof of the lower boundin (3.2) :Forauniformlyrandomvector v= (v1,...,v r)∈
Vr,letBγ(v) be the set of all vertices adjacent to vin the graph G(r,γr).The
vertices{vj}1≤j≤naremutually independent and uniformly distributed in Vand
so the expected degree of vjisdav.Consequently, the expected size of Bγ(v) is
E#Bγ(v) =γr/summationdisplay
k=0/parenleftbiggr
k/parenrightbigg
dk
av. (3.4)
For 0< γ <1−1
dav+1,we use the Hamming ball estimate (see Proposi-
tion 3.3.1 [5]) to get that
E#Bγ(v) =γr/summationdisplay
k=0/parenleftbiggr
k/parenrightbigg
dk
av≤nθr(3.5)
whereθ=H(γ)+γlogdav
lognsatisﬁes 0 < θ <1.For 0< ǫ=ǫ(r)<1 to be
determined later, we let
A(ǫ) :={u∈Vr: #Bγ(u)≤nr(θ+ǫ)} (3.6)
and obtain from Markov inequality and (3.5) that
#A(ǫ)≥nr/parenleftbig
1−n−rǫ/parenrightbig
. (3.7)
LetD:={w1,...,wM} ⊆ A(ǫ) be a stable set of maximum size in G(r,γr).
By the maximality, we must have that the union/uniontextM
i=1Bγ(wi) =A(ǫ) and so
from (3.7) and (3.6), we have
nr/parenleftbig
1−n−rǫ/parenrightbig
≤#A(ǫ)≤M/summationdisplay
i=1#Bγ(wi)≤M·nr(θ+ǫ)(3.8)Redundancy of Codes 11
ThusM≥n(1−θ−ǫ)r(1−n−rǫ) and choosing ǫ=1√r,takingrthroots and
allowing r→ ∞,we get that
Θγ(G)≥f(γ,dav) (3.9)
for 0< γ <1−1
dav+1.
For 1−1
dav+1≤γ≤1,we use the fact that the expected ball size in (3.4) is
bounded above by
E#Bγ(v)≤r/summationdisplay
k=0/parenleftbiggr
k/parenrightbigg
dk
av= (dav+1)r. (3.10)
As before we then use the maximality argument to get that (3.9) hold s for 1−
1
dav+1≤γ≤1.
To see that Θγ(G)≥f(γ,∆),we use a similar argument as above along with
the estimate that for any deterministic vector u∈Vrthe ball size
#Bγ(u)≤γr/summationdisplay
k=0/parenleftbiggr
k/parenrightbigg
∆r. (3.11)
Finally, a similar argument also shows that Θγ(G)≥f(γ,L−1) and this com-
pletes the proof of the lower bound in (3.2).
Proof of the upper bound in (3.2) : Letd=γrand letC ⊂G(n) be a stable
set of maximum size in G(r,d) and set A(r,d) := #C.We ﬁrst derive a recursive
estimate involving A(r,d).For a vertex v∈GletC(v) be the set of all vertices
inCwhose last entry is v.By the pigeonhole principle, there exists a v0∈Gsuch
that #C(v0)≥A(r,d)
n.We remove the last entry in each vertex of C(v0) and call
the resulting set as D(v0)⊂G(r−1).The setD(v0) is also stable in G(r−1,d)
and this implies that
A(r,d)≤n·A(r−1,d).
Continuing iteratively we get
A(r,d)≤nr−d·A(d,d). (3.12)
Takingrthroots and using the fact that
sup
r≥1
γ(A(d,d))1
r= sup
d≥1(A(d,d))γd= (Θ(G))γ,
we get the upper bound in (3.2).
Acknowledgements
I thankProfessorsV. Guruswami,C.R. Subramanianandthe refe reesforcrucial
comments that led to an improvement of the paper. I also thank IMS c for my
fellowships.12 G. Ganesan
References
1. N. Alon and E. Lubetzky. (2006). The Shannon capacity of a g raph and the
independence numbers of its powers. IEEE Transactions on Information Theory ,
52, pp. 2172–2176.
2. N. Alon and J. Spencer. (2008). The probabilistic method . Wiley Interscience.
3. S. T. Dougherty. (2017). Algebraic coding theory over ﬁnite commutative rings .
Springer briefs in Mathematics.
4. M. Greferath and S. E. Schmidt. (1999). Linear Codes and Ri ngs of Matrices.
Proceedings of AAECC 13 Hawaii, Springer LNCS 1719 , pp. 160–169.
5. V. Guruswami, A. Rudra and M. Sudan. (2019). Essential Coding The-
ory. Link: https://cse.buﬀalo.edu/faculty/atri/courses/c oding-theory/book/web-
coding-book.pdf.
6. P. Hell and F. S. Roberts. (1982). Analogues of the Shannon Capacity of a Graph.
North-Holland Mathematics Studies ,60, pp. 155–168.
7. B. Hemenway, R. Ostrovsky and M. Wootters. (2015). Local c orrectability of
expander codes. Information and Computation ,243, pp. 178–190.
8. W. C. Huﬀman and V. Pless. (2003). Fundamentals of Error Correcting Codes .
Cambridge University Press.
9. Irwansyah and D. Suprijanto. (2018). Structure of linear codes over the ring Bk.
Journal of Applied Mathematics and Computing ,58, pp. 755–775.
10. Z. Liu and J. Wang. (2019). Linear complementary dual cod es over rings. Designs,
Codes and Cryptography ,87, pp. 3077–3086.
11. L. Lov´ asz. (1979). On the Shannon Capacity of a Graph. IEEE Transactions on
Information Theory ,25, pp. 1–7.
12. K. Marton. (1993). On the Shannon capacity of Probabilis tic Graphs. Journal of
Combinatorial Theory, Series B ,57, pp. 183–195.
13. C. E. Shannon. (1956). The zero-error capacity of a noisy channel. IRE Transac-
tions on Information Theory ,22, pp. 8–19.
14. M. Sipser and D. Spielman. (1996). Expander codes. IEEE Transactions on In-
formation Theory ,42, pp. 1710–1722.
15. R. Urbanke and T. Richardson. (2008). Modern Coding Theory . Cambridge Uni-
versity Press.
16. D. B. West. (2001). Introduction to Graph Theory . Pearson.