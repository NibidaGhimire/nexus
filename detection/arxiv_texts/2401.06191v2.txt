TriNeRFLet: A Wavelet Based Triplane NeRF
Representation
Rajaei Khatib and Raja Giryes
Tel Aviv University
rajaeikhatib@mail.tau.ac.il and raja@tauex.tau.ac.il
Abstract. In recent years, the neural radiance field (NeRF) model has
gained popularity due to its ability to recover complex 3D scenes. Fol-
lowing its success, many approaches proposed different NeRF represen-
tations in order to further improve both runtime and performance. One
such example is Triplane, in which NeRF is represented using three 2D
feature planes. This enables easily using existing 2D neural networks in
this framework, e.g., to generate the three planes. Despite its advan-
tage, the triplane representation lagged behind in 3D recovery quality
compared to NeRF solutions. In this work, we propose the TriNeRFLet
framework, where we learn the wavelet representation of the triplane and
regularize it. This approach has multiple advantages: (i) it allows infor-
mation sharing across scales and regularization of high frequencies; (ii)
it facilitates performing learning in a multi-scale fashion; and (iii) it pro-
vides a ‘natural’ framework for performing NeRF super-resolution (SR),
such that the low-resolution wavelet coefficients are computed from the
provided low-resolution multi-view images and the high frequencies are
acquired under the guidance of a pre-trained 2D diffusion model. We
show the SR approach’s advantage on both Blender and LLFF datasets.
Keywords: Neural Radiance Fields (NeRF) ·Wavelet ·Multiscale rep-
resentation ·3D Super-Resolution ·Diffusion Models
1 Introduction
3D scene reconstruction from multiple 2D views is a challenging task that has
been widely studied, and many methods have been proposed to solve it. Neural
radiance field (NeRF) [28] is prominent among these methods as it has demon-
strated a high level of generalizability in generating novel views with high quality
and consistent lighting. NeRF utilizes an implicit representation of the 3D scene
in the form of a multi-layer perceptron (MLP). This enables it to capture com-
plex 3D geometry and lighting.
At a high level, NeRF relies on the rendering equation that approximates
each pixel in the image using points sampled along the ray that passes through
it [28]. The MLP in NeRF takes as input the frequency encoding of the Eu-
clidean coordinates and view direction of the point of interest, and outputs the
radiance and density at this point. The MLP weights are learned via end-to-endarXiv:2401.06191v2  [cs.CV]  17 Jul 20242 Khatib et al.
Fig. 1:Our approach improves the quality of NeRF reconstruction (zoom-in). From
left to right: TriNeRFLet improvement over Triplane, TriNeRFLet compared to INGP,
TriNeRFLet SR improvement, TriNeRFLet SR compared to NeRF-SR.
optimization, by comparing the values between the rendered pixel and its value
in the corresponding 2D image. After the training process is over, the MLP can
be used to render novel views.
Due to its success in representing 3D scenes, many methods proposed im-
provements to NeRF [5,6,29]. They sought to enhance NeRF 3D reconstruction
capabilities, as well as other drawbacks such as high runtime and aliasing arti-
facts that it suffered from. One such approach uses three axis-aligned 2D feature
planes, denoted as Triplane, to represent the NeRF [7]. In the rendering process,
each point is sampled by projecting it onto each of the three planes and then
concatenating the features that correspond to the three projections. This forms
a single feature vector for the point that is then passed to a small MLP that
outputs the density and color values of this point.
A significant advantage of the Triplane representation is that it can be used
with many already existing 2D methods. In the original work [7], the authors
used an existing 2D Generating Adversarial Network (GAN) architecture to
generate its planes. Follow-up works employed the 2D property of the Triplane
to perform NeRF super-resolution [4] and 3D generation [22,38].
While being useful due to its special 2D structure, the reconstruction qual-
ity achieved by Triplane lagged behind other efficient multiview reconstruction
methods such as instant NGP (INGP) [29], which is an improvement of NeRF,
and 3D Gaussian splatting [21]. Due to its structure, only Triplane entries that
are included in the training views rays are learned. Thus, there might be entries
at the planes that will still have their initial random values also after training
finishes. These random features are used by novel views that rely on them and
therefore they deteriorate the quality of their generation and create artifacts in
these novel views.
To tackle these drawbacks, we present a novel Triplane representation that
relies on a multiscale 2D wavelet structure. Instead of learning the feature planes
directly, we learn the wavelet features of several resolutions, while regularizing
the wavelet representation to be sparse. Thus, regions that are covered by the
training views are learned in a similar way to the case of a regular Triplane,
and regions that are not covered by the training views are updated by a lower
resolution estimate according to how much they are affected by their neighboring
regions. Figure 1(left) shows the improvement in reconstruction quality attainedTriNeRFLet 3
(a)The multiscale wavelet representation trans-
fers information between points on the triplane
planes:Althoughthereddotontheplanewasnot
covered by the training images views, it will get a
coarser estimate from a lower resolution wavelet
layer that corresponds also to the blue dot, which
was updated in training.
(b)Example of the multiscale property of the
wavelet representation. On the left, we present
the wavelet representation. On the right, we show
the feature plane of the wavelet representation
on the left at different resolutions. Note that the
wavelet representation of a higher resolution in-
cludes the wavelet representations of the lower-
resolution planes (the colors correspond to the
matching between each plane and its wavelet rep-
resentation).
Fig. 2:Illustartion of the multiscale property of the wavelet representation.
by this representation. Figure 2a illustrates how regions on the plane may affect
each other due to the wavelet representation.
In our optimization, we use L1regularization on the wavelet coefficients. The
goal is to sparsify the coefficients that are not trained by the given views such
that only the coefficients that were trained will impact the reconstruction also
from other views. Using L1regularization is inspired by the wavelet literature,
which indicates that wavelet features can represent data accurately while being
sparse [11].
To further improve the training, we perform it in a multi-scale fashion, start-
ing with a lower resolution version that updates only the coarse coefficients of
the wavelet. Then, after some iterations, we increase the resolution and add
additional wavelet layers to the representation.
We apply the proposed TriNeRFLet framework to two tasks. First, for 3D
reconstruction, where it closes the performance gap of Triplane against other
multiview 3D reconstruction methods and makes it competitive with current
state-of-the-art (SOTA) methods. Second, we combine it with a pre-trained
2D super-resolution (SR) diffusion models [34,35] to perform NeRF SR. Fig-
ure 1(right) demonstrates the improvement this approach attains. We show in
various experiments the advantage of our TriNeRFLet in novel view reconstruc-
tion compared to INGP and Triplane and the superiority of our SR solution
compared to competing approaches that do not require dedicated training for
multiview or 3D data.
2 Related Works and Background
NeRF[28] is a framework for reconstructing a 3D scene from a set of multiview
images. Its main component is an implicit neural representation of the 3D scene
via a mapping neural network Fθ: (x,d)→(c, σ), where xis the Cartesian
coordinate of the point of interest, dis the direction vector from it to the camera
origin, cis the RGB color at this point and σis the density.4 Khatib et al.
(a)Triplane rendering. The feature vector of each
pointissampledbygatheringtheprojectedfeatures
on the Triplane planes. This vector is passed to an
MLP to obtain color and density at this point. The
final value of a pixel is calculated by using the color
and density of the points that reside on the ray that
passes through it.
(b) 3D reconstruction training scheme. First,
wavelet features are transformed into the Triplane
domain. Next, pixels are rendered using these fea-
tures in order to fit them to their ground-truth val-
ues. The high frequencies channels of LH, HL and
HH from all wavelet levels get regularized by L1
loss.
(c)In the wavelet domain, the low-
frequency coefficients LL Nbaseare observed
at the top left corner (the bright spot).
The rest are the high frequencies channels
LHNbase, HL Nbaseand HH Nbase. All the
other LH, HL and HH channels can be ob-
served accordingly. All these planes belong
to the same 3D object.
Fig. 3: The TriNeRFLet reconstruction framework learns features in wavelet domain,
which are transformed into feature domain to render 3D objects.
The network Fis trained using the pixels of the multiview images using the
rendering equation. For a given pixel, it relates its value to Npoints sampled
along the ray that passes through this pixel, where the ray direction is derived
from the image view. Denoting by t1< t 2< ... < t Nthe depth of the points,
δi=ti+1−tithe distance between adjacent samples, and σi,cithe density and
color of each point respectively, then the pixel value ˆCcan be estimated from
these points via:
ˆC=NX
i=1Ti(1−exp(−σiδi))ci,where Ti= exp
−i−1X
j=1σjδj
.(1)
This equation is a discrete approximation of the rendering equation. As we have
the true pixel value Cof the image, the network parameters θare trained simply
by minimizing the L2 lossC−ˆC
2.TriNeRFLet 5
IntheclassicalNeRFscheme[28],themappingfunction Fθisdividedintotwo
main parts. The first part is a frequency encoding function that encodes xandd
usingFourierfeatures[39].Thesevectorsarethenconcatenatedtoformafeature
vector that is fed to the second part, which is an MLP function that maps the
featurevectorintoitscorrespondingcoloranddensityvalues.Someextensionsto
NeRF consider optimization using unconstrained photo collections [26], few view
points [44], non-rigidly deforming scenes [30], and imperfect camera poses [23].
INGP [29] is an important development, which managed to decrease NeRF
runtime significantly while getting even better reconstruction performance. It
achieved that by using a multiscale hash grid structure alongside a small shallow
MLP that represents the scene instead of the big deep one used in previous
works. Furthermore, this method introduced new rendering techniques and low-
level optimizations in order to get a faster runtime. The multiscale concept of
INGP will also be used in our proposed TriNeRFLet framework.
3D Gaussian Splatting [21] is a state-of-the-art 3D reconstruction approach
that takes a different strategy than NeRF. Instead of using an implicit neural
function, it represents the scene as a group of 3D Gaussians, and learns their
parameters.ItintroducesanewmethodforefficientlyrenderingtheseGaussians,
thus, managing to train rapidly and achieving 3D rendering in real-time. Note
that one needs to distinguish between training and rendering time. While its
training time is similar to INGP, its rendering is faster than NeRF solutions.
Triplane [4,7] is a NeRF variant, in which the frequency encoding part is re-
placed by three perpendicular 2D feature planes of dimension N×Nwith C
channels. To get the feature vector of a point x, the point is projected onto each
of these planes, then the sampled feature values of the projected points are gath-
ered together to form a feature vector. The final feature vector is obtained by
concatenating this feature vector with the frequency encoding of the direction
vector d. These feature planes are learned alongside the MLP weights with a
similar approach to what was explained earlier. Fig. 3a illustrates this process.
The advantage of Triplane compared to other alternatives is its use of 2D
planes, which eases the use of standard 2D networks with it. This has been
used in various applications. Bahat et al. [4] utilized it to train a NeRF super-
resolution (SR) network from multiview training data. By exploiting the triplane
structure, a 2D SR neural network was trained to perform SR on low resolution
planes and in this way increase the overall NeRF resolution. Another example
is [38], in which a diffusion model was trained to generate a Triplane. It utilizes
the generalization power of diffusion models and brings them to 3D generation.
Similarly, Triplane is predicted by a transformer-based neural network in [22] in
order to have fast text to 3D generation.
Wavelets were widely used in signal and image processing applications [25].
With the advent of deep learning, they were employed in various architectures
to improve performance and efficiency. The filter-bank approach of wavelet was
utilized to represent each kernel in a neural network as a combination of filters
fromagivenbasis,e.g.,awaveletbasis[32].Theconceptofusingmulti-resolution
was used to improve positional encoding of implicit representations [15,24,37].6 Khatib et al.
Wavelets were used to improve efficiency and generation of StyleGAN [10], dif-
fusion models [13,18,31] and normalizing flows [45].
Rhoet al.[33] implemented a multiscale 3D grid using a grid decomposition
method that transforms 2D planes alongside 1D vectors (unrelated to the 2D
planes) into a 3D grid, where they used a 2D multiscale wavelet representation
to represent the 2D planes. Despite the use of 2D multiscale representation, this
method is different in essence from TriNeRFLet, which deploys the 2D wavelet
multiscale representation on Triplane planes directly, thus being freed from 3D
grid limitations.
DiffusionModels [9,16]havebecomeaverypopulartoolinrecentyearsforim-
age manipulation and reconstruction tasks [19,20,34,36,42], where a denoising
network is trained to learn the prior distribution of the data. Diffusing mod-
els can be used also beyond generation, where given a degraded image, some
conditioning mechanism is combined with the learned prior to solve different
tasks [2,3,8]. For example, in [1,36,42] diffusion models were utilized for the
problems of deblurring and SR. In this work, we use the Stable Diffusion (SD)
upscaler [34] to perform SR on 2D projections of the low-resolution NeRF in
order to create a high-resolution version of the NeRF. While the SD upscaler is
conditioned both on the low-resolution image and a given text prompt, we use
it without text, as explained later in the paper.
The authors of [43] proposed to fine-tune a diffusion model on the views
of a given scene to regularize unseen views. In a similar spirit, we use images
generated by a diffusion model to fit the scene and thus improve NeRF SR.
3 TriNeRFLet Framework
To alleviate the Triplane drawbacks mentioned above, we present TriNeRFLet.
Instead of using the 2D feature planes directly, we optimize them in the wavelet
domain. At a high level, wavelet transforms a 2D plane to a multiscale represen-
tation that contains both low and high frequencies at different resolutions. The
inverse wavelet transform, convert this representation back to the original 2D
plane. Note that we apply the wavelet separably on each channel. We use the
following notation for the wavelet representation. At resolution iof the wavelet
representation, we denote by LL ithe low frequencies of this resolution, and by
LHi, HL iand HH ithe high frequencies. Note that LL iis used to create the co-
efficients of the next resolution of the wavelet representation. At a certain step,
we stop and remain with LL.
In rendering, TriNeRFLet is similar to regular Triplane, in the sense that
given a point it calculate its features by projecting it to the Triplane and then
feed the features to a MLP that outputs the color ciand density σiof the point
that are then used by the rendering equation (see Figure 3a). Yet, instead of
holding the Triplane in the feature domain, TriNeRFLet holds it in the Wavelet
domain and optimizes the wavelet coefficients. Note that in the regular Triplane
scheme,eachplaneentryimpactsonlyonelocationineachplane.InTriNeRFLet,
it impacts different resolutions in the wavelent representation of the plane (seeTriNeRFLet 7
Figure 2a). Figure 3c shows some learned planes for the ship object and their
corresponding Wavelet coefficients.
The learned parameters of TriNeRFLet, alongside the MLP weights, are the
waveletcoefficients,andtheyarelearnedinanend-to-endfashion.Themultiscale
structure of the wavelet, allows the method to learn fine details whenever it
needs, and to have a lower resolution estimate from lower resolution wavelet
layers that are affected by nearby values when the area is not covered by the
training pixels. The question remains, what should be done with the coefficients
of the higher frequencies that are not updated.
Wavelet regularization. From wavelet theory [11], we expect the LH, HL
and HH channels to be sparse. Thus, we regularize these channels to be sparse
over all resolutions by adding an L1regularization for these coefficients to the
training loss. This allows us to increase the resolution of the planes without
having the problem of having features in it that remain random or redundant.
In TriNeRFLet, also these features get updated.
To further comprehend the significance of the multiscale wavelet structure
and the regularization we use for it, we compare the behavior to rendering novel
views with a regular Triplane. In a regular Triplane, some features will still have
their initial random values after training ends. This introduces artifacts when
rendering novel views, as demonstrated in the ship example in Figure 1. Note
that these artifacts are absent in TriNeRFLet because all the features in the
planes are updated either by direct training or using a coarser estimate from
lower resolution wavelet coefficients (see Figure 2a).
Multiscale training. A key advantage of TriNeRFLet is that the multiscale
property of the wavelet representation enables increasing the resolution of the
TriNeRFLet during training seamlessly. It is possible to continue to train the
higher-resolution TriNeRFLet while using the wavelet coefficients of the lower-
resolution TriNeRFLet. The wavelet of the lower-resolution plane is a subset of
the wavelet of the higher resolution plane (see Figure 2b). We use this flexibility
to improve the training by learning the wavelet planes in a coarse to fine manner.
As we explain later, we leverage the multiscale structure of the wavelet both for
NeRF reconstruction and SR.
The multiscale wavelet structure is not ideal in terms of training runtime due
to the fact that the feature planes need to be reconstructed in each training step.
Yet, its rendering runtime is quite fast since the feature planes are reconstructed
just once and then the runtime is competitive with other well-known fast NeRF
schemes like INGP [29].
4 TriNeRFLet Applications
4.1 TriNeRFLet Reconstruction
The TriNeRFLet framework is straightforwardly applied to 3D scene reconstruc-
tion task. Given the training pixels, wavelet features are optimized end-to-end8 Khatib et al.
on the pixel reconstruction loss in addition to the wavelet high frequencies reg-
ularization (see Figure 3b). The overall loss is:
L=X
i∈TrainingPixelsCi−ˆCi2
2+γX
l∈resolutions(∥LHl∥1+∥HLl∥1+∥HHl∥1),
where γis the L1regularization factor.
As mentioned before, the flexible multiscale structure of TriNeRFLet enables
the training to be in a coarse-to-fine manner. This mitigates the runtime over-
head of the wavelet inverse transform. At the beginning of the training, the
method usually learns high-level features, and then when it starts to converge it
learns the low-level details, which is coherent with the coarse-to-fine approach.
4.2 TriNeRFLet Super-Resolution (SR)
We turn to present a novel NeRF SR approach that relies on the proposed
multiscale structure. This method employs a 2D pre-trained stable diffusion
(SD) upscaler [34] for SR “guidance”. Our approach does not require any kind
of low-high resolution 3D scene pairs for supervision as done in other works [4].
Utilizing the robustness of the SD upscaler, our approach has the potential to
handle different types of scenes and a range of resolutions.
Given a set of low-resolution (LR) multiview images of a given scene, the
goal is to generate high-resolution (HR) views of this scene. We first apply
TriNeRFLet using the LR images, providing a LR 3D reconstruction. In this
reconstruction, we learn only the low frequencies of the wavelet representation.
Denote the size of the LR planes by NLRand the number of wavelet levels by
LLR. For the HR planes, denote the size by Nand the number of levels by L.
Due to the multiscale nature wavelet representations, in the HR TriNeRFLet we
just need to learn the high frequencies of the wavelet, i.e., the first LLRlevels of
the HR wavelet planes are shared with the LR TriNeRFLet reconstruction.
Figure 4a illustrates our TriNeRFLetSR strategy. First, we start by recon-
structng the LR TriNeRFLet using the provided low-resolution multiview im-
ages. After training for sLRsteps, the super-resolution process starts, and a key
component of it is the stable diffusion upscaler refinement (SD refine) step pre-
sented in Figure 4b. Given a low-resolution ground-truth image xgt
LR, its high-
resolution version xest
HRis rendered using all wavelet levels L. Initially, the result
will not be of high-resolution. Thus, to improve it we use a diffusion step with
time trandomly selected from the range [Tmin, Tmax]. Then, similar to [14], a
noise with variance that depends on tis added to xest
HR. Then, we plug the noisy
version of xest
HRinto the SD upscaler to perform the diffusion process from step t
until the end, while being conditioned on the LRimage xgt
LR. This results in the
enhanced version of xest
HR, denoted as xenhanced
HR. We denote this enhancement
process by SD refine.
We use the SD upscaler with a small modification compared to the original
work [34]. In their work, the denoising step is
xt−1=ϵ(xt,z,∅) +α(ϵ(xt,z,y)−ϵ(xt,z,∅)),TriNeRFLet 9
Algorithm 1 TriNeRFLet Super-Resolution (SR)
Input:Ground truth low resolution (LR) images
xigt
LR	k
i=1, LR only steps sLR, To-
tal steps s, Wavelet LR levels LLR, Refresh steps srefresh, Diffusion step limits
Tmin, Tmax.
Output: High-Resolution (HR) TriNeRFLet
1: HR-set ← {} ▷empty dictionary
2:foritr= 0···s−1do
3: Choose randomly a frame i - xigt
LR.
4: Render xiest
LRusing only first LLRwavelet levels.
5: loss←xiest
LR−xigt
LR2
2
6: ifitr≥sLRthen
7: Render xiest
HRusing all wavelet levels.
8:
9: ifitr%srefresh == 0 then
10: HR-set ← {} ▷empty dictionary
11: end if
12:
13: ifi∈HR-set then
14: xigt
HR←HR-set [i]
15: else
16: t∼Rand (Tmin, Tmax)
17: xigt
HR←SDrefine(xiest
HR,xigt
LR, t)
18: HR-set [i]←xigt
HR
19: end if
20: loss←loss+xiest
HR−xigt
HR
1+λLPIPS (xiest
HR,xigt
LR)
21: end if
22: loss←loss+γP
l(∥LHl∥1+∥HLl∥1+∥HHl∥1)
23: Tmax←scheduler (Tmax, i) ▷decrease Tmax
24:end for
where ϵ,α,zandyare the U-net denoiser, a guidance factor, the low-resolution
image and the guidance text respectively. Instead of doing the guidance in the
text direction, we do it in the low-resolution image direction and without text,
i.e.,xt−1=ϵ(xt,∅,∅) +α(ϵ(xt,z,∅)−ϵ(xt,∅,∅)).
Having the refined HR images, we want to use them to update the TriNeR-
FLet. We add these images to the set denoted HR-set. This set is initialized
to be empty and gets updated sequentially. It is refreshed (i.e. emptied) every
srefreshsteps as illustrated in Algorithm 1. The images in HR-set are used as
“ground-truth” images for training the HR TriNeRFLet to update all its Llevels.
To tie all threads together, the method starts by fitting TriNeRFLet with
low-resolution features to the low-resolution ground-truth images. Then SD refine
is used to improve the resolution of the TriNeRFLet generated views and the im-
ages are stored in HR-set. Next, a TriNeRFLet with high-resolution features (us-
ing all wavelet levels) is fitted to HR-set and simultaneously the low-resolution
images are used to train the low levels of the wavelet representation of the
same TriNeRFLet. As in regular TriNeRFLet training, a L1regularization of10 Khatib et al.
(a)TriNeRFLet based super-resolution. Low-
resolution TriNeRFLet renders LR images using
the LR version of the wavelet features, and fits
them to the given low-resolution images. High-
resolution TriNeRFLet renders HR images using
all wavelet levels, and fits them to HR images
from HR-set.
(b)SDrefine process. First, a HR image is ren-
dered. To improve its quality, noise is added to it
and then it is plugged into Stable Diffusion up-
scaler with its LR version for conditioning. The
result, which is a refined HR image is added to
the HR-set.
Fig. 4:TriNeRFLet Super-Resolution (SR) components.
the wavelet coefficients is added to the loss. In addition, we use also a LPIPS
loss between the rendered high resolution and the ground truth low resolution.
As mentioned earlier, HR-set gets refreshed every srefreshtraining steps. Fur-
thermore, the diffusion step upper limit Tmaxis scheduled to linearly decrease
during training in order to force SD refineto introduce fewer changes as training
converges. This is described in Algorithm 1.
The TriNeRFLet multiscale wavelet structure plays an important role in
the SR scheme. First, its multiscale structure establishes a connection between
high-resolution and low-resolution renderings by forcing them to share informa-
tion (using the same first LLRwavelet levels). Second, wavelet regularization,
especially in the layers that only the HR TriNeRFLet uses, constrains the op-
timization process and forces the HR TriNeRFLet to learn only useful wavelet
coefficients that contribute to HR details. Thus, the combination of multiscale
wavelet and diffusion models allows the method to learn the high-resolution
details it exactly needs without redundancy.
The diffusion SR model that we use supports SR from 128to512. Yet,
we want to support other resolutions, such as 100to400or200to800. For
lower resolutions, e.g., going from 100to400, we pad xgt
LRandxest
HRwith zeros
in order to achieve the desired resolution before plugging them into the SD
upscaler. When it finishes, the result is cropped to the original resolution. For
higher resolutions, e.g., going from 200to800, we randomly crop 128and512
resolution crops from xgt
LRandxest
HR, respectively, that correspond to the same
relative location. We plug the images into the upscaler, and the result is actually
an enhanced version of the crop from xest
HR. Then, we only fit the HR render toTriNeRFLet 11
Method MicChairShipMaterials LegoDrumsFicusHotdogAvg. Train
Time↓Render
FPS↑
NeRF 32.9133.0028.6529.62 32.5425.0130.1336.18 31.005≥
1day≤0.2
INGP (paper) 36.2235.0031.1029.18 36.3926.0233.5137.40 33.176 5mins 1
INGP (rerun) 36.2234.8131.129.68 36.0325.1532.4137 32.8 5mins 1
INGP (rerun)* 36.335.1831.2229.8 36.0624.9633.2237.1 32.98 6hours 1
3D Gaussian
Splatting35.3635.8330.8030.00 35.7826.1534.87 37.72 33.32 10mins 135
Triplane 33.8532.8329.5828.15 34.7024.8630.3535.80 31.26 2hours 1.5
Triplane* 34.433.529.9728.1 35.424.930.6836.16 31.64 6hours 1.5
Ours: TriNeR-
FLet Small35.1833.7630.3329.14 35.3225.6633.3436.24 32.37 15mins 1.5
Ours: TriNeR-
FLet Base Light35.7735.0031.1029.35 36.4425.9833.9636.93 33.07 1.5
hours1.5
Ours: TriNeR-
FLet Base36.7235.5431.7729.72 36.6626 33.737.31 33.43 6hours 1.5
Ours: TriNeR-
FLet Large37.22 36 32.7 30.34 37.32 26.2 34.4737.89 34.017 ∼1day1.5
Table 1: Blender Dataset PSNR ( ↑) results with train time and render FPS for each
method. Boldis best, underline is second. For fair comparison, we also show INGP
(rerun)* and Triplane*, which are trained for 6hours as our TriNeRFLet base.
this crop instead of the whole image. When the HR-set is refreshed, a different
random crop will be chosen, thus covering more areas as the process continues.
5 Experiments
We turn to present the results of our method. More visual results and ablations
appear in the supplementary material. Code is available in the github repo.
5.1 3D Scene Reconstruction
For evaluating TriNeRFLet in the 3D recovery task, we define four versions of
TriNeRFLet - small, base light, base and large. The parameters of TriNeRFLet
are wavelet LL component resolution - NLL, base resolution - Nbase, wavelet
layers - L, final resolution - Nfinal, number of channels - C, wavelet regulariza-
tionγ, MLP dim (neurons) - W, MLP hidden layers - DdensityandDcolorand
training steps. These parameters are provided for each model in the Supplemen-
tary Materials. In the small, base light and base versions, TriNeRFLet uses the
exact same MLPs as in INGP. TriNerfLet is trained using the Adam optimizer
with a learning rate of 0.01and an exponential decay scheduler. The wavelet low
frequencies in LL (of size NLL×NLL) are randomly initialized, and the other
frequencies in LH, HL and HH at all levels are initialized with zeros. We use
the Biorthogonal 6.8(Bior 6.8) wavelet type. Ablation for other wavelet types
is found in the sup. mat. The other training details are similar to INGP. Our
implementation uses [40], which is a Pytorch-based implementation of INGP.
Since the wavelet reconstruction component affects the runtime of TriNeR-
FLet, we start the training with lower resolutions of the planes, training coarse12 Khatib et al.
to fine. The initial plane resolution is set to Nbaseand it is increased throughout
the training till it reaches the size Nfinalthat has Lwavelet levels. As described
above, due to the wavelet multiscale structure, when we move to the next res-
olution in training we simply add another level (of higher frequencies) to the
wavelet representation and add it to the training, where the lower levels are kept
as is (but continue to train). As mentioned in Sec. 3, the relatively high training
runtime is one of TriNeRFLet framework disadvantages, and this is due to the
inverse wavelet transform overhead during optimization. Thus, doing the opti-
mization in this coarse to fine manner mitigates this drawback and accelerates
the training (inverse wavelet on smaller planes is faster).
To evaluate the performance of our method, we use the Blender dataset
[28], which contains 8synthetic 3D scenes with complex geometry and lighting.
We compare TriNeRFLet with the vanilla NeRF [28], regular Triplane (with
base config), INGP [29] and 3D Gaussian Splatting [21]. Results are reported
in Table 1. For INGP we report 3 versions: INGP (paper) denotes the officially
published results by [29], and INGP (rerun) and INGP (rerun)* denote the
results that we obtained by running INGP (using [29] software) for 5mins and
6hours respectively. Also, Triplane and Triplane* denote Triplane results when
trained for 5mins and 6hours respectively. We ran all TriNeRFLet versions on
a single A6000 GPU. The reported results indeed demonstrate the improvement
in performance that the multiscale wavelet brings, as it outperforms regular
Triplane by a significant margin, and improves over the current SOTA, even in
case they were trained for a longer time.
Regarding rendering time, the TriNeRFLet (all versions) frames per second
(FPS) are approximately 1.5, while INGP and 3D Gaussian Splatting are ap-
proximately 1and 135respectively. These FPS are tested on a single A6000
GPU. Note that TriNeRFLet rendering FPS is compatible and even slightly
better than INGP, which is known to be fast within the NeRF approaches. 3D
Gaussian Splatting, which is not a NeRF method, is faster in its rendering time
since it uses a different internal structure than NeRF. When compared only to
NeRF-based solutions, our method is very fast with competitive reconstruction
performance. While our rendering time is faster, the training time of TriNeR-
FLet is higher than INGP. This is due to the wavelet reconstruction, which needs
to be done at each training step but only once during rendering time.
5.2 3D Scene Super-Resolution
Blender. We turn to present our results for TriNeRFLet SR. The objective
here is to generate novel views with ×4resolution than the input images. To
this end, we tested two different settings on the Blender dataset [28], 100to400
and200to800. For the first one, we use NLR= 256andN= 1024, while in
the second setting NLR= 512andN= 2048. Further technical details appear
in the Supplementary Materials.
We compare our method with NeRF*, NeRF-Bi, NeRF-Liif, NeRF-Swin,
NeRF-SR (SS) (these methods do not require 3D supervision and are described
in [41]) and NVSR (results of 200→800were provided to us by the authors) [4].TriNeRFLet 13
TriNeRFLetSR TriNeRFLetSR
w/o multiscaleTriNeRFLetSR
w/o LPIPSOur SR scheme
with INGP
PSNR ↑29.49 28.49 29 .27 28 .25
LPIPS ↓0.051 0.057 0 .06 0 .06
SSIM↑0.930 0.919 0 .903 0 .915
Table 2: Ablation: Impact of wavelet multiscale and LPIPS on SR. To show
the significance of having the multi-scale structure of the wavelet, we apply our SR
scheme without using the multiscale rendering or with INGP instead of TriNeRFLet
rendering. We also show the significance of adding the LPIPS loss.
NeRF*isregularNeRFtrainedonLRviewsandthenrenderedinHRresolution,
NeRF-Bi, NeRF-Liif and NeRF-Swin are trained on LR views, then rendered in
LR resolution and upscaled using different 2D SR methods [41].
Results are reported in Table 3. Note that NVSR requires 3D supervision
(paired LR and HR multiview images) and therefore it is considered only as a
reference. When compared only to methods that use 2D supervised methods, we
perform the best in most of the cases. Also, our method’s results are very close
to NVSR, and even slightly outperforms it in some cases, which demonstrates
the superiority of our method despite the fact that it was not trained using 3D
data. Qualitative examples of our approach appear in Figure 5 and sup. mat.
Impact of multiscale on SR. To demonstrate the impact of TriNeRFLet
multiscale on the SR scheme, we modify it to neutralize the multiscale element.
This is done by replacing the low-resolution rendering that uses low-resolution
wavelet features (first LLRlayers) with a downscaled HR rendering that uses
all wavelet layers (i.e. without multiscale render). In the same manner, INGP is
also tested as a representation. Results in Table 2 demonstrate the important
contribution of the TriNeRFLet multiscale structure to the SR scheme, utilizing
its full potential, and that without it the SR scheme performs worse.
Impact of LPIPS loss. To demonstrate the LPIPS loss impact on SR, we
trained the scheme without the LPIPS loss. Results are reported in Table 2, and
as noticed, the LPIPS component indeed improves the SR scheme performance.
LLFF.We turn to demonstrate the TriNeRFLet-based SR scheme on the LLFF
dataset [27], which contains 8 real-world captured scenes. As in the Blender SR
case, we compare our method with the same methods but for input resolution
378×504and×4upscaling. Following [41], we trained TriNeRFLet using the
low-resolution version of all images and then tested the SR results also on all
the images. Table 3 reports the results. Our method is almost as good as [41]
in PSNR and outperforms it in the LPIPS and SSIM metrics by a margin. Our
LPIPS and SSIM performance indicates that our method manages to create a
scene with better visual details and that is perceptually more similar to the
ground-truth high-resolution scene.
6 Conclusions
This paper introduced a new NeRF structure that relies on a multiscale wavelet
representation. This structure improved the performance of Triplane, which14 Khatib et al.
LR image NVSR TriNeRFLet SR HR Ground Truth
Fig. 5: TriNeRFLet SR qualitative results . Note the visual improvement that is
achieved by our approach to NeRF SR compared to LR reconstruction. Moreover,
the qualitative results of our method compete with NVSR, which unlike our strategy
is a 3D supervised method that we consider as a reference. In the sup. mat. we provide
addtional visual examples with more comparisons to other methods.
Blender Blender LLFF
100→400 200→800 378×504→1512×2016
Method PSNR ↑LPIPS ↓SSIM↑PSNR ↑LPIPS ↓SSIM↑PSNR ↑LPIPS ↓SSIM↑
NeRF* 25.56 0 .170 0 .881 27.47 0 .128 0 .900 24.47 0 .388 0 .701
NeRF-Bi 24.74 0 .244 0 .868 26.67 0 .175 0 .900 23.90 0 .481 0 .676
NeRF-Liif 25.36 0 .125 0 .885 27.34 0 .096 0 .912 24.76 0 .292 0 .723
NeRF-Swin 24.85 0 .108 0 .881 26.78 0 .086 0 .906 23.26 0 .247 0 .685
NeRF-SR (SS) 28.07 0.071 0.921 28.46 0 .076 0 .921 25.13 0.244 0.730
TriNeRFLetSR 28.55 0.061 0.913 29.490.051 0.930 25.000.203 0.771
NVSR (3D su-
pervised)∗∗ 29.53 0.054 0.931 ∗∗
Table 3: Super Resolution ×4Results. NVSR is trained with multiview (3D) supervi-
sion and therefore we consider it as reference. Boldis best, underline is second. **For
100→400NVSR reports results of only 4 shapes in the Blender dataset. In this case,
we outperform it with PSNR of 29.18dB for our method compared to 28.5dB of NVSR.
For LLFF, NVSR did not report any results on this resolution or LLFF average.
lagged behind NeRF state-of-the-art methods, achieving competitive results.
Having the multiscale structure enabled us to implement a new diffusion-guided
SR for NeRF. We believe that the concept of learning the features in the mul-
tiscale wavelet domain instead of the original one has the potential to improve
other vision-related applications.
Despite the advantages of our approach, it also has some limitations, mainly
due to the runtime overhead it adds to training time. This can be partially
mitigated by using its light-weight versions and the coarse to fine training, which
reduce training time. We defer further run-time improvements to future work.TriNeRFLet 15
Acknowledgments This work was supported in part by KLA grant.
References
1. Abu-Hussein, S., Tirer, T., Giryes, R.: Adir: Adaptive diffusion for image recon-
struction. arXiv preprint arXiv:2212.03221 (2022)
2. Avrahami, O., Fried, O., Lischinski, D.: Blended latent diffusion. arXiv preprint
arXiv:2206.02779 (2022)
3. Avrahami, O., Lischinski, D., Fried, O.: Blended diffusion for text-driven editing of
natural images. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 18208–18218 (2022)
4. Bahat, Y., Zhang, Y., Sommerhoff, H., Kolb, A., Heide, F.: Neural volume super-
resolution. arXiv preprint arXiv:2212.04666 (2022)
5. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance
fields. In: Proceedings of the IEEE/CVF International Conference on Computer
Vision. pp. 5855–5864 (2021)
6. Barron, J.T.,Mildenhall,B.,Verbin,D.,Srinivasan,P.P.,Hedman,P.:Mip-nerf360:
Unbounded anti-aliased neural radiance fields. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 5470–5479 (2022)
7. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O.,
Guibas,L.J.,Tremblay,J.,Khamis,S.,etal.:Efficientgeometry-aware3dgenerative
adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 16123–16133 (2022)
8. Chung, H., Lee, E.S., Ye, J.C.: Mr image denoising and super-resolution using reg-
ularized reverse diffusion. IEEE Transactions on Medical Imaging 42(4), 922–934
(2023)
9. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances
in neural information processing systems 34, 8780–8794 (2021)
10. Gal, R., Hochberg, D.C., Bermano, A., Cohen-Or, D.: Swagan: A style-based
wavelet-driven generative model. ACM Trans. Graph. 40(4) (2021)
11. Guo, T., Zhang, T., Lim, E., Lopez-Benitez, M., Ma, F., Yu, L.: A review of
wavelet analysis and its applications: Challenges and opportunities. IEEE Access
10, 58869–58903 (2022)
12. Guo, Y.C., Liu, Y.T., Shao, R., Laforte, C., Voleti, V., Luo, G., Chen, C.H., Zou,
Z.X.,Wang,C.,Cao,Y.P.,Zhang,S.H.:threestudio:Aunifiedframeworkfor3dcon-
tent generation. https://github.com/threestudio-project/threestudio (2023)
13. Guth, F., Coste, S., De Bortoli, V., Mallat, S.: Wavelet score-based generative
modeling. In: Advances in Neural Information Processing Systems. vol. 35, pp. 478–
491 (2022)
14. Haque, A., Tancik, M., Efros, A., Holynski, A., Kanazawa, A.: Instruct-nerf2nerf:
Editing 3d scenes with instructions. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision (2023)
15. Hertz, A., Perel, O., Giryes, R., Sorkine-Hornung, O., Cohen-Or, D.: Sape:
Spatially-adaptive progressive encoding for neural optimization. Advances in Neural
Information Processing Systems 34, 8820–8832 (2021)
16. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020)16 Khatib et al.
17. Hong, F., Tang, J., Cao, Z., Shi, M., Wu, T., Chen, Z., Wang, T., Pan, L., Lin,
D., Liu, Z.: 3dtopia: Large text-to-3d generation model with hybrid diffusion priors.
arXiv preprint arXiv:2403.02234 (2024)
18. Kadkhodaie, Z., Guth, F., Mallat, S., Simoncelli, E.P.: Learning multi-scale local
conditional probability models of images. In: The Eleventh International Conference
on Learning Representations (2023), https://openreview.net/forum?id=VZX2I_
VVJKH
19. Kawar, B., Elad, M., Ermon, S., Song, J.: Denoising diffusion restoration models.
In: Advances in Neural Information Processing Systems (2022)
20. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.: Imagic: Text-based real image editing with diffusion models. arXiv preprint
arXiv:2210.09276 (2022)
21. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics (ToG) 42(4),
1–14 (2023)
22. Li, M., Zhou, P., Liu, J.W., Keppo, J., Lin, M., Yan, S., Xu, X.: Instant3d: Instant
text-to-3d generation (2023)
23. Lin, C.H., Ma, W.C., Torralba, A., Lucey, S.: Barf: Bundle-adjusting neural radi-
ance fields. arXiv preprint arXiv:2104.06405 (2021)
24. Lindell, D.B., Van Veen, D., Park, J.J., Wetzstein, G.: Bacon: Band-limited coor-
dinate networks for multiscale scene representation. In: CVPR (2022)
25. Mallat, S.: A Wavelet Tour of Signal Processing, Third Edition: The Sparse Way.
Academic Press, Inc. (2008)
26. Martin-Brualla, R., Radwan, N., Sajjadi, M.S.M., Barron, J.T., Dosovitskiy, A.,
Duckworth, D.: NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo
Collections. In: CVPR (2021)
27. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi,
R.,Ng,R.,Kar,A.:Locallightfieldfusion:Practicalviewsynthesiswithprescriptive
sampling guidelines. ACM Transactions on Graphics (TOG) 38(4), 1–14 (2019)
28. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Commu-
nications of the ACM 65(1), 99–106 (2021)
29. Müller,T.,Evans,A.,Schied,C.,Keller,A.:Instantneuralgraphicsprimitiveswith
a multiresolution hash encoding. ACM Transactions on Graphics (ToG) 41(4), 1–15
(2022)
30. Park, K., Sinha, U., Barron, J.T., Bouaziz, S., Goldman, D.B., Seitz, S.M., Martin-
Brualla, R.: Deformable neural radiance fields. arXiv preprint arXiv:2011.12948
(2020)
31. Phung, H., Dao, Q., Tran, A.: Wavelet diffusion models are fast and scalable image
generators. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 10199–10208 (2023)
32. Qiu, Q., Cheng, X., Sapiro, G., et al.: Dcfnet: Deep neural network with decom-
posed convolutional filters. In: International Conference on Machine Learning. pp.
4198–4207 (2018)
33. Rho, D., Lee, B., Nam, S., Lee, J.C., Ko, J.H., Park, E.: Masked wavelet rep-
resentation for compact neural radiance fields. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 20680–20690 (2023)
34. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 10684–10695 (2022)TriNeRFLet 17
35. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. Advances in Neural
Information Processing Systems 35, 36479–36494 (2022)
36. Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image super-
resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (2022)
37. Saragadam, V., LeJeune, D., Tan, J., Balakrishnan, G., Veeraraghavan, A., Bara-
niuk, R.G.: Wire: Wavelet implicit neural representations. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18507–
18516 (2023)
38. Shue, J.R., Chan, E.R., Po, R., Ankner, Z., Wu, J., Wetzstein, G.: 3d neural field
generation using triplane diffusion. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 20875–20886 (2023)
39. Tancik,M.,Srinivasan,P.P.,Mildenhall,B.,Fridovich-Keil,S.,Raghavan,N.,Sing-
hal, U., Ramamoorthi, R., Barron, J.T., Ng, R.: Fourier features let networks learn
high frequency functions in low dimensional domains. NeurIPS (2020)
40. Tang, J.: Torch-ngp: a pytorch implementation of instant-ngp (2022),
https://github.com/ashawkey/torch-ngp
41. Wang, C., Wu, X., Guo, Y.C., Zhang, S.H., Tai, Y.W., Hu, S.M.: Nerf-sr: High
quality neural radiance fields using supersampling. In: Proceedings of the 30th ACM
International Conference on Multimedia. pp. 6445–6454 (2022)
42. Whang, J., Delbracio, M., Talebi, H., Saharia, C., Dimakis, A.G., Milanfar, P.:
Deblurring via stochastic refinement. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 16293–16303 (2022)
43. Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan,
P.P., Verbin, D., Barron, J.T., Poole, B., et al.: Reconfusion: 3d reconstruction with
diffusion priors. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 21551–21561 (2024)
44. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelNeRF: Neural radiance fields from
one or few images. In: CVPR (2021)
45. Yu, J.J., Derpanis, K.G., Brubaker, M.A.: Wavelet Flow: Fast training of high
resolution normalizing flows. In: NeurIPS (2020)TriNeRFLet: A Wavelet Based Triplane NeRF
Representation
Supplementary Materials
A Ablation
Wavelet type . For all the experiments we conducted in the paper, we applied
the Biorthogonal 6.8 (Bior6.8) wavelet. We present here an ablation experiment
that checks the impact of other wavelet filters. We compare the reconstruction
performance of 4 scenes from the Blender dataset. The setups we tested are
vanilla Triplane (no wavelet), Haar, Biorthogonal 2.2 (Bior2.2), Biorthogonal 2.6
(Bior2.6), Biorthogonal 4.4 (Bior4.4) and Biorthogonal 6.8 (Bior6.8). The results
are presented in Figure 1. We use the TriNeRFLet Base Light setting. Vanilla
Triplane has the same size and structure as this setting. Note that all wavelets
achieve a significant performance advantage over vanilla Triplane (as shown also
in the paper). Generally, higher order wavelets provide a better representation
of smooth functions [25]. Thus, it is not surprising that Bior6.8, Bior4.4 and
Bior 2.6 achieve better performance than Bior 2.2 and Haar. Note though that
in terms of training time, training with Haar wavelet is faster by up to 30%
compared to the other wavelets as Haar complexity is O(N)and the complexity
of applying the other wavelets is O(Nlog(N)).
Coarse To Fine . As mentioned in the paper, coarse to fine (c2f) accelerates
TriNeRFLet framework and is presented as a mitigation for the wavelet inverse
transform overhead. To further check the additional contributions of c2f, we
conduct the Blender reconstruction experiment (Base Light version) but with-
out c2f. Results are presented in Table 2, and as noticed, c2f indeed brings a
contribution in the reconstruction quality, alongside the contribution in training
time.
Wavelet Regularization . To check the importance of the wavelet regulariza-
tion loss, we extend the c2f experiment above to be without wavelet regular-
ization. The results in Table 2 illustrate the important contribution of wavelet
regularization and that it is an essential component of TriNeRFLet performance.
B Technical Details
3D Reconstruction . Table 1 contains the parameters for the reconstruction
versions.
Blender Super-Resolution. In both Blender settings ( 100→400 & 200 →
800), we use C= 16(channels), sLR= 6000(LR only steps), s= 16000 (total
steps), srefresh = 500(refresh steps), Tmin= 0.02andTmaxis initialized with
0.98and decreases linearly to reach 0.25at the end. We use [12] as a base forTriNeRFLet Supplementary Materials 19
Fig. 1:Performance comparison between different wavelet filters and no wavelet at all
(Triplane).
implementing the super-resolution scheme, as it is more suitable for this task.
We run the SR method on a single A 6000RTX GPU, and it runs for almost 7
hours.
LLFF Super-Resolution. We use similar settings to Blender 200→800ex-
periment except for, C= 32(channels), s= 20000 (total steps), srefresh = 250
(refresh steps). Like [28], we also operate in NDC space coordinates, as it fits
this type of scenes better. Similar to the Blender experiment, we ran LLFF SR
method on a single A6000 RTX GPU, where each scene runs almost for 12hours.
NLLL N baseNfinal C γ W D dens Dcoltrain
stepstrainable
parameters
Small 64 4 512 1024 16 0 .2 64 1 2 6 k17M
Base
Light64 5 512 2048 32 0 .4 64 1 2 10 k134M
Base 64 5 512 2048 32 0 .4 64 1 2 43 k134M
Large 64 5 512 2048 48 0 .6 128 1 2 83 k201M
Table 1: The configurations used in the different TriNeRFLet versions20
Mic Chair Ship Materials Lego Drums Ficus Hotdog Avg.Train
Time
Base Light 35.77 35 .00 31 .10 29 .35 36 .44 25 .98 33 .96 36 .93 33.071.5
hours
Base Light
w/o c2f35.52 35 .08 31 .11 29 .14 36 .48 25 .76 33 .46 36 .88 32.932
hours
Base Light
w/o c2f
&wavelet
reg.34.14 33 .33 30 28 .17 35 .02 25 .01 30 .76 35 .95 31.552
hours
Triplane 33.85 32 .83 29 .58 28 .15 34 .70 24 .86 30 .35 35 .80 31.262
hours
Table 2: Coarse to fine (c2f) and wavelet regularization importance ablation. We
compare Base Light version with the versions without c2f and without c2f and wavelet
regularization. The results indicate the important contribution of the c2f and wavelet
regularization.
C Detailed Results
Tables 3,4 and 5 contain the detailed results of the SR experiments described in
the paper.
Mic Chair Ship Materials Lego Drums Ficus Hotdog Avg.
PSNR 31.54 29 .4 28 .11 27 .9 27 .65 24 .18 27 .7 31 .92 28.55
LPIPS 0.05 0 .051 0 .118 0 .048 0 .073 0 .067 0 .044 0 .039 0.061
SSIM 0.914 0 .946 0 .844 0 .921 0 .914 0 .896 0 .936 0 .939 0.913
Table 3: Blender 100→400detailed results.
Mic Chair Ship Materials Lego Drums Ficus Hotdog Avg.
PSNR 31.11 29 .86 28 .14 28 .02 30 .46 24 .17 30 .6 33 .58 29.49
LPIPS 0.032 0 .059 0 .114 0 .046 0 .032 0 .071 0 .021 0 .034 0.051
SSIM 0.966 0 .944 0 .844 0 .93 0 .922 0 .913 0 .96 0 .96 0.93
Table 4: Blender 200→800detailed results.
D Qualitative Results
Figures 2 and 3 provide qualitative results for reconstruction and LLFF SR
experimentsrespectively.Forallresults,thereaderisreferredtotheprojectweb-TriNeRFLet Supplementary Materials 21
Flower Fern Leaves Fortress Horns Orchids Room Trex Avg.
PSNR 26.6 24 20 .01 28 .88 24 .93 21 .38 28 .56 25 .57 25.00
LPIPS 0.214 0 .205 0 .239 0 .175 0 .26 0 .244 0 .125 0 .163 0.203
SSIM 0.808 0 .766 0 .639 0 .832 0 .737 0 .668 0 .901 0 .819 0.771
Table 5: LLFF 378×504→1512×2016detailed results.
page https://rajaeekh.github.io/trinerflet-web .Furthermore,TriNeRFLet-
SR can be incorporated into a generative model. To show that, we use the diffu-
sion process in [17] that generates a triplane and then apply SR to it using our
scheme. Fig. 4 presents examples of triplanes rendered with resolution 128that
are upscaled to 512by TriNeRFLet SR.
Triplane TriNeRFLet INGP Ground-Truth
Fig. 2: NeRF reconstruction qualitative results . Notice the improvement in reconstruc-
tion quality of TriNeRFLet compared to Triplane. More visual results appear in the
project page.22
LR NeRF-SR TriNeRFLet HR
Fig. 3: NeRF LLFF super-resolution qualitative results .
Fig. 4:SR of low-res objects generated by triplane diffusion with text “Astronaut suit
and helmet" and “Colorful electric scooter”. First row is low-res and second row is
TriNerFLet-SR.