Learning Quadruped Locomotion using Bio-Inspired Neural Networks
with Intrinsic Rhythmicity
Chuanyu Yang1, Can Pu1y, Tianqi Wei2, Cong Wang3, Zhibin Li4
Abstract ‚Äî Biological studies reveal that neural circuits lo-
cated at the spinal cord called central pattern generator
(CPG) oscillates and generates rhythmic signals, which are
the underlying mechanism responsible for rhythmic locomotion
behaviors of animals. Inspired by CPG‚Äôs capability to naturally
generate rhythmic patterns, researchers have attempted to
create mathematical models of CPG and utilize them for the
locomotion of legged robots. In this paper, we propose a network
architecture that incorporates CPGs for rhythmic pattern
generation and a multi-layer perceptron (MLP) network for
sensory feedback. We also proposed a method that reformulates
CPGs into a fully-differentiable stateless network, allowing
CPGs and MLP to be jointly trained with gradient-based
learning. The results show that our proposed method learned
agile and dynamic locomotion policies which are capable of
blind traversal over uneven terrain and resist external pushes.
Simulation results also show that the learned policies are
capable of self-modulating step frequency and step length to
adapt to the locomotion velocity.
I. INTRODUCTION
Animals are able to adapt their locomotion gait pattern to
suit the locomotion velocity and ground condition. Efforts
have been put into discovering the underlying mechanism of
animal locomotion, and have obtained evidence that legged
locomotion is rhythmic in nature. Findings have revealed
evidence of special neurons called central pattern generators
in the animal spinal cord. CPGs are a series of coupled
oscillatory neurons capable of producing rhythmic signals in-
ternally without external sensory input. The rhythmic signals
produced by CPGs are necessary for activities that involve
periodic movement, such as breathing, walking, running,
swimming, Ô¨Çying, etc. [1], [2].
In recent years, there has been signiÔ¨Åcant progress in
using deep reinforcement learning (DRL) to train artiÔ¨Åcial
neural networks for legged locomotion. Multi-expert neural
networks can be designed to perform various different loco-
motion tasks [3]. Robust control policy has also been trained
This research is supported by Shenzhen Amigaga Technology Co. Ltd.,
by the Human Resources and Social Security Administration of Shenzhen
Municipality under Overseas High-Caliber Personnel project (Grant NO.
202102222X, Grant NO. 202107124X) and by Human Resources Bureau
of Shenzhen Baoan District under High-Level Talents in Shenzhen Baoan
project (Grant No. 20210400X, Grant No. 20210402X).
* These authors contributed equally to this work.
yCorresponding author. Email: can.pu@amigaga.com
1Chuanyu Yang and Can Pu are with Shenzhen Amigaga Technology Co
Ltd.fchuanyu.yang, can.pu g@amigaga.com
2Tianqi Wei is with the School of ArtiÔ¨Åcial Intelligence, Sun Yat-sen
University, CN.
3Cong Wang is with Shenyang Institute of Automation, Chinese Academy
of Sciences, CN.
4Zhibin Li is with the Department of Computer Science, University
College London, UK.
Fig. 1. Depiction of the agile and dynamic locomotion learned by the
MLP-CPG network architecture.
to successfully traverse over extreme terrain in the wild [4].
The neural networks from both works rely on an external
periodic phase input to produce rhythmic motor patterns.
Apart from a single global phase, multiple local phases can
be provided, e.g. providing four phases as input to match the
four individual legs of the quadruped [5].
Instead of providing external periodic phase input, there
have been attempts in designing network structures capa-
ble of self-generating phases. One approach is to add an
additional phase increment output to the policy. The phase
increment is added to the current phase and is used as the
phase for the next timestep in a bootstrap manner [6], [7],
[4]. To reproduce the agile locomotion behaviors of animals
in legged robots, researchers have looked into animals for
inspiration. Inspired by Ô¨Åndings of CPGs within the animal
nervous system, there have also been attempts in utilizing the
intrinsic rhythmicity of CPGs to generate rhythmic motor
patterns for locomotion controllers in robotics, eliminating
the need for external phase input [8], [9], [10].
This work aims to utilize the natural rhythmicity of CPGs
to generate agile and dynamic locomotion behaviors for
quadruped robots. While CPGs by themselves are capable
of generating open-loop rhythmic signals, sensory feedback
has to be integrated to enable the CPGs to react to external
environment. We propose a framework that combines MLP
networks and CPGs, where the CPGs are responsible for
generating rhythmic patterns, and the MLP receives sensory
information and provides feedback to modulate the CPGs‚Äô
behavior. Our proposed framework reformulates CPGs by
exposing the internal states as inputs and outputs of the
network, essentially turning CPGs into a fully-differentiable
stateless network. The parameters of the MLP and CPGs are
optimized in conjunction via DRL.
The contributions of this paper are:
We proposed a network architecture that combines
CPGs and MLP network to generate rhythmic motions
intrinsically without relying on external phase input. We
name the network architecture as MLP-CPG.
We proposed an approach that reformulates CPGs intoarXiv:2305.07300v1  [cs.RO]  12 May 2023fully-differentiable networks. The CPG parameters and
MLP parameters within the MLP-CPG network archi-
tecture can be jointly optimized with DRL.
We implemented the network architecture on a simu-
lated quadruped robot and demonstrated the policy‚Äôs
capability to track user targets, and the robustness to
environmental uncertainties.
The proposed network architecture is interpretable. Lo-
comotion characteristics such as step frequency, step
length, and gait pattern can be inferred from the CPG
parameters.
II. RELATED WORK
A. CPG-Based Locomotion for Legged Robots
CPGs have gained interest within the robotics Ô¨Åeld due to
their oscillatory and rhythmic nature. To study the rhythmic
mechanism of biological CPGs, researchers have proposed
different CPG models with varying levels of complexity,
from detailed biophysical models, connectionist models, to
abstract models based on mathematical oscillators [2], [11].
Researchers have attempted to leverage the natural rhyth-
mic behaviors of CPGs by incorporating CPGs into the
controller to achieve animal-like locomotion for all kinds of
legged robots, including bipeds, quadrupeds, and hexapods.
Various abstract models of mathematical oscillators such as
SO(2) oscillator, Matsuoka oscillator, and Hopf oscillator
have been employed within the locomotion controller for
different robots [12], [13], [14].
B. Learning CPG-Based Controllers with DRL
DRL has proven to be effective in learning robust and
complex locomotion skills for legged robots in simulation
and real world. There are many works that have combined
CPGs together with MLP networks and trained using the
DRL paradigm. However, due to the intrinsic recurrent nature
of CPG networks, back-propagation through time (BPTT)
is required to obtain the parameters of CPGs while trained
with gradient-based learning, which is computationally more
expensive [10], [15].
One approach of dealing with the recurrent nature of CPGs
is to treat the CPG controller as part of the environment
dynamics. Such approach is referred to as CPG-Actor-Critic,
where the CPG controller, the robot, and the environment
are treated as a single dynamic system called CPG-coupled
system. The action space of the policy is the CPG param-
eters. The policy controls the robot by adjusting the CPG
parameters of the CPG-coupled system. [8], [9], [10].
Another approach of dealing with the recurrent nature
of CPGs is to optimize the parameters of CPGs and MLP
network separately. The parameters of the neural network
can be optimized with DRL, while the parameters of CPG
are optimized using non-gradient based approaches such as
evolutionary strategy (ES) [16], genetic algorithm (GA) [17],
or a biologically plausible learning rule [18], [19]. Wang
et al. proposed a hierarchical control structure with a high-
level neural network and a low-level CPG controller. The
high-level network generates latent variables that are fed intoTABLE I
ROBOT SPECIFICATION .
Joint Range Peak Torque Peak Velocity
Hip roll (-0.523, 0.523) rad 75Nm 19rad=s
Hip pitch (-2.792, 0.349) rad 75Nm 19rad=s
Knee pitch (0.698, 2.792) rad 130Nm 19rad=s
Body mass 42kg
Body size 0.85mx 0.30mx 0.30m(length x width x height)
Leg length 0.33m, 0.34m(upper, lower)
A
B
]0,0[]1,5.0[
ÔÄΩÔÄΩ
ÔÅ∏ÔÅ´ ]5.0,1[ÔÄΩÔÅ´ ]1,5.0[ÔÄΩÔÅ´ ]5.0,1[ÔÄΩÔÅ´ ]1,5.0[ÔÄΩÔÅ´]1,75.0[]0,0[
ÔÄΩÔÄΩ
ÔÅ´ÔÅ∏ ]6,6[ÔÄ≠ÔÄΩÔÅ∏ ]0,0[ÔÄΩÔÅ∏ ]6,6[ÔÄ≠ÔÄΩÔÅ∏ ]0,0[ÔÄΩÔÅ∏
Fig. 2. An example consisting of 2 coupled CPG oscillators. The values
withinij,, anddtare set to 6.0, 5.0, and 0.01. The values within
;;o; are set to 0. (A) and (B) illustrate how the phase and amplitude
of the CPG can be adjusted by feedback component and.
the CPGs to regulate their behaviors. The CPG controller
is optimized using GA, while the neural network is trained
using DRL [17]. Shi et al. used ES to train a CPG-based
foot trajectory generator. A separate neural network is trained
using DRL to generate residual joint angles that correct the
generated foot trajectory [16].
Campanaro et al. proposed an approach that dealt with the
recurrent properties of CPGs by reformulating the CPG into a
fully-differentiable stateless network, and thus allowing end-
to-end training of CPGs alongside MLP. Campanaro et. al.
have only validated the approach on a simple example of a
single two degrees-of-freedom (DoF) hopping leg [15]. We
extended upon the idea of reformulating CPG network into
a stateless network and proposed our MLP-CPG framework.
Furthermore, we have successfully implemented on a 12-
DOF simulated quadruped robot.
III. METHOD
A. Robot Platform and Simulation setup
The Jueying quadruped robot is used as the robot platform
(Table I). Pybullet is chosen as the physics engine for the
simulation environment.
B. CPG Controller
The CPGs within this work are modelled using Hopf
oscillators [14].
_t
i= 2ft+X
jijsin(t 1
j t 1
i ij) +t
i
_rt
i=rt 1
i 
(i+t
i)2 (rt 1
i)2
 t
i=rt
icos(t
i) +t
i+oi;(1)Unfold
h th1ÔÄ´thnthÔÄ´
tx1ÔÄ´tx ntxÔÄ´ xxhWhyW
hhWy ty1ÔÄ´tyntyÔÄ´
hhWhhWhyWhyWhyW
xhWxhWxhWA
Unfold
),,,(ÔÅ´ÔÅ∏ÔÅ£fy ty1ÔÄ´tyntyÔÄ´
dtr rrdt
t t tt t t
ÔÄ¶ÔÄ¶
ÔÄ´ÔÄΩÔÄ´ÔÄΩ
ÔÄ≠ÔÄ≠
11ÔÅ±ÔÅ±ÔÅ±ttr,ÔÅ±1 1,ÔÄ≠ÔÄ≠ t trÔÅ±1 1,ÔÄ´ÔÄ´ t trÔÅ±ntntrÔÄ´ÔÄ´,ÔÅ±
t f ),,,(ÔÅ´ÔÅ∏ÔÅ£1),,,(ÔÄ´t fÔÅ´ÔÅ∏ÔÅ£nt fÔÄ´),,,(ÔÅ´ÔÅ∏ÔÅ£
Unfold
),,,(ÔÅ´ÔÅ∏ÔÅ£fy
ty1ÔÄ´tyntyÔÄ´
dtr rrdt
t t tt t t
ÔÄ¶ÔÄ¶
ÔÄ´ÔÄΩÔÄ´ÔÄΩ
ÔÄ≠ÔÄ≠
11ÔÅ±ÔÅ±ÔÅ±
ttr,ÔÅ±
1 1,ÔÄ≠ÔÄ≠ t trÔÅ±1 1,ÔÄ≠ÔÄ´ÔÄ≠ÔÄ´ nt nt rÔÅ±ntntrÔÄ´ÔÄ´,ÔÅ±
t f ),,,(ÔÅ´ÔÅ∏ÔÅ£1),,,(ÔÄ´t fÔÅ´ÔÅ∏ÔÅ£nt fÔÄ´),,,(ÔÅ´ÔÅ∏ÔÅ£ttr,ÔÅ±1 1,ÔÄ´ÔÄ´ t trÔÅ±B
CFig. 3. Illustration of RNN and CPGs. (A) Unfolded RNN. (B) Unfolded
CPGs. (C) Our reformulation approach transforms CPGs into stateless
networks by exposing the hidden states as network inputs and outputs.
wheret
iandrt
irepresent the phase and amplitude of the i-
th oscillator at timestep t,_t
iand_rt
iare their corresponding
derivative,ijandijare the coupling weight and phase bias
between i-th and j-th oscillator, iis the desired amplitude, oi
is a constant offset of the oscillation setpoint, is a constant
that determines the rising time for r, t
irepresents the output
signal. Finally, t
i,t
i,t
i,ftare the feedback components,
wheret
iadjusts the amplitude, t
iadjusts the phase, t
i
adjusts the oscillation setpoint, ftadjusts the frequency of
the oscillator at timestep t.
The phase and amplitude t
iandrt
iare obtained from their
derivative values_t
iand_rt
iusing the following equation.
t
i=t 1
i+_t
idt; rt
i=rt 1
i+ _rt
idt; (2)
wheredtis the duration of the timestep.
C. Reformulating Hidden States in CPGs
Within Recurrent Neural Networks (RNN), the hidden
states of the previous timestep has to be passed to the current
timestep and combined with the input of the current timestep
to compute the current output. RNN requires BPTT to be
trained. CPGs can be viewed as a type of RNN, as they
require hidden states to propagate information through time.
The transition between consecutive hidden states of a CPG
is determined by eq (1) and eq (2).
We dealt with the recurrent nature of CPGs by proposing
a reformulation approach that exposes the hidden states
through network inputs and outputs. The previous hidden
states will be passed as network inputs, and the current
hidden states will be passed as network outputs. With
our proposed reformulation approach, the CPG network is
essentially transformed into a fully-differentiable stateless
feedforward network (Fig. 3C).
In contrast to previous work where the parameters of
the CPG controllers and MLP network are optimized with
Replay Buffer Update Actor and CriticRobot State
25Hz
Low-Pass
FilterLow-Pass
Filter
RobotPD Controller
Joint 
torques
Joint position
measurements1000Hz
Target Joint
Position
Store Update
SampleActor Network
),,,(1ÔÄ´ttt t srasGoal
State
Internal
StateRobot
State
ReLU256MLP Network
ReLU256ÔÅ≥ÔÅ´Front Right
Hip Roll
Front Right
Hip PitchFront Right
Knee Pitch
Rear Right
Hip RollRear Right
Hip Pitch
Rear Right
Knee PitchFront Left
Hip PitchFront Left
Knee Pitch
Front Left
Hip Roll
Rear Left
Hip RollRear Left
Hip Pitch
Rear Left
Knee PitchÔÅ∏
ÔÅ£fCPG Network
CPG Internal 
stateUser Goal 
stateFig. 4. Control framework overview. The MLP-CPG network generates
target joint positions at 25Hz. The PD controller functions at 1000Hz and
converts the joint angles to joint torques.
TABLE II
CPG PARAMETERS .
Parameter Value
dt 0.04
 12
 Learned via DRL
 Learned via DRL
o [m;m;m;m],m= [0;0:28; 0:1]
 [n;n;n;n],n= [0;0:8;0:8]
separate algorithms, our reformulated CPG network is dif-
ferentiable and can be optimized jointly with MLP network
within the same DRL paradigm without BPTT [17], [16].
D. Network Architecture
The MLP-CPG network consists of two separate modules,
one rhythmic module responsible for generating rhythmic
signals, and another non-linear feedback module responsible
for processing sensory feedback (Fig. 4).
1) Rhythmic module: The rhythmic module contains 12
CPG neurons, each neuron represents a joint. The values of
the CPG parameters are shown in Table II.
2) Feedback module: The feedback module is an MLP
network with 2 hidden layers of size 256 with tanh activation
function. The MLP receives state inputs and regulates the
CPG network via feedback components f,,,. The MLP
also outputs the covariance of the stochastic policy .
The output of the MLP-CPG network  is bounded within
( 1;1)using tanh , and re-scaled to the corresponding joint
limits to be used as the target joint angles.
E. Training Setup
1) Soft Actor Critic: We select Soft Actor Critic (SAC) as
the DRL algorithm for the training of our policy. SAC learnsFig. 5. Red dashed curve is obtained from animal data [21]. Green solid
curve is obtained by Ô¨Åtting a new curve that passes through the origin.
TABLE III
SAC TRAINING HYPERPARAMETERS .
Hyperparameter Value
Discount factor 0.95
Target network smoothing 0.999
Learning rate 3e-4
Weight decay 1e-6
Batch size 128
Gradient update steps 4
Temporal regularization 1e-3
Spatial regularization 1e-3
frequency loss 1e-2
a policy by maximizing the expected return and the entropy.
The optimization objective can be expressed as follows:
JSAC() =TX
t=0E(st;at)[r(st;at) +H((jst))] (3)
whereis the policy, is the sample distribution, ris the
reward,standatare the state and action at time step twithin
the sample distribution, is the temperature parameter, and
H()is the entropy. The temperature parameter determines
the stochasticity of the policy and is tuned automatically
during training to balance exploration and exploitation. The
hyperparameters can be seen in Table III
2) Generating smooth action: We introduce temporal
and spatial regularization terms to encourage the learning
of smoother actions [20]. We only regularize the setpoint
feedback component of the network output.
LT=k(st) (st+1)k2
2;LS=k(st) (^st)k2
2(4)
The spatial regularization loss minimizes the difference
between the action generated under observed state stand
perturbed state ^st N (st;), whereis the standard
deviation. The temporal regularization loss minimizes the
difference between the action under current state stand the
next statest+1.
3) Step frequency: Multiple researches have shown that
animals adjust their step frequency for different locomotion
velocities [21], [22]. Prior knowledge of the step frequency
can be embedded into the controller. Lee et al. manually
designed a state machine that switches between different base
frequencies depending on user velocity command [4].
We provided prior knowledge of step frequency using the
curve function fref=a+blnvcalculated from real animal
data, where frefis the step frequency, vis the locomotion
velocity,a= 1:314 andb= 0:762 are the parameters [21].TABLE IV
THE BASIC DEFINITIONS OF THE MATHEMATICAL NOTATION USED IN
THE EQUATIONS FOR THE REWARD TERMS
Notation DeÔ¨Ånition
'b The projection of the gravity vector in the robot base frame
hb The robot base height (z)in the world frame
vx The forward velocity of the robot base along robot heading
vy The lateral velocity of the robot base along robot heading
vz The vertical linear velocity of the robot base
!roll The roll angular velocity of the robot base
!pitch The pitch angular velocity of the robot base
!yaw The yaw angular velocity of the robot base
 Torque of joints
q Angle of joints
_q Velocity of joints
(^) The desired quantity of placeholder property ()
pf;n The n-th foot horizontal coordinates in the world frame
pb The base horizontal coordinates in the world frame
ph;n The n-th hip horizontal coordinates in the world frame
hw
f;nThe height of n-th foot in world frame
vw
f;nThe velocity of n-th foot in world frame
TABLE V
DETAILED DESCRIPTION OF TASK REWARD TERMS .
Reward term Symbol
Forward velocity8
31K(vx;^vx; 4:6)
Lateral velocity4
31K(vy;^vy; 4:6)
Vertical velocity1
31K(vz;0; 4:6)
Roll velocity1
31K(!roll;0; 1:87)
Pitch velocity1
31K(!pitch;0; 1:87)
Yaw velocity4
31K(!yaw;^!yaw; 1:87)
Base orientation2
31K('b;[0;0; 1]; 2:35)
Base height2
31K(hb;^hb; 51:17)
Joint torque1
31K(;0; 0:001)
Joint velocity1
31K(_q;0; 0:026)
Ground contact1
31(
0;upper body contact with ground
1;:
Self collision1
31(
0;self collision
1;:
Swing & stance2
311
4Pn=1
4K((hw
f;n ^hw
f;n)vw
f;n;0; 460)
Body placement1
31K(1
4Pn=1
4(pw
f;n);pw
b; 51:17)
Foot placement1
311
4Pn=1
4K(pf;n;ph;n; 51:17)
We Ô¨Åtted a new curve that passes through the origin, as we
want zero frequency at zero velocity (Fig. 5):
fref=a+bln (v+c);a= 1:066;b= 0:876;c= 0:289
v= (^v2
x+ ^v2
y)0:5
(5)
where ^vxand^vyare the target forward and lateral ve-
locity, respectively. The step frequency fis bounded within
[0;3]. We introduce loss function Lfto encourage the MLP
network to track the reference frequency fref.
Lf=kf(st) frefk2
2(6)
The frequency loss Lf, the temporal regularization loss
LT, and spatial regularization loss LSare added to the loss
function of the SAC LSAC = JSAC.
L=LSAC+TLT+SLS+fLf: (7)F . Reward Design
Radial basis function (RBF) kernels are used in the reward
design. The output range of RBF is bounded within (0;1).
The formulation of the RBF kernel is shown as follows:
K(x;bx;c) = exp 
c(^x x)2
(8)
wherexis the physical quantity for the evaluation, ^xis
the desired value, and cis the parameter that controls the
width of the RBF.
The reward is the weighted sum of individual reward
terms, each governing a different physical aspect (Table IV
and Table V). The Swing and stance foot reward penalizes
the movement of the stance foot close to the ground while
encouraging the movement of the swing foot far from the
ground. The body placement reward encourages the Center of
Mass (COM) of the robot to be placed close to the center of
the support polygon. The foot placement reward encourages
the feet to be placed close beneath the hip joint.
G. State Space and Action Space
The state space consists of the observations from the
environment SO, the internal states of the CPGs SI, and
the goal states provided by the user SG.
The robot observation states consist of base linear velocity
in robot heading frame vxyz, base angular velocity !rpy,
orientation vector 'b, joint position q, and joint velocity _q.
The goal states consist of the target forward velocity ^vx,
target lateral velocity ^vy, and the target yaw turning rate ^!z.
The internal states consist of the amplitude rand phaseof
the 12 CPG oscillators. A mod operation is used to bound
the phase within the range [0;2]. The observation states SO
are Ô¨Åltered by a low-pass Butterworth Ô¨Ålter with a cut-off
frequency of 10 Hz.
The action space atcontains the target joint angles ^q. A
low-pass Butterworth Ô¨Ålter with a cut-off frequency of 5 Hz
is applied to the target joint angles to encourage smooth
actions. PD controllers are used to calculate joint torques
from the target joint angles ^q, measured joint angles q,
and measured joint velocities _qusing the following equation
=Kp(^q q) +Kd(0 _q), whereKp= 300 ,kd= 10 .
H. Exploration Setup
1) Initialization: The initial forward velocity, lateral ve-
locity, and yaw rate are sampled from U( 1;5),U( 1;1),
andU( ;)respectively. The initial joints angles are
sampled fromN(~q;
4), where ~qis the joint angle during
nominal standing posture. The CPG amplitude and phase
are initialized from U(0;
4)andU(0;2), respectively. The
target forward velocity, target lateral velocity, and target turn-
ing rate are initialized from U( 1;5),U( 1;1),U(
2;
2),
respectively.
2) Early termination: We terminate and restart the train-
ing episode when the robot reaches undesirable fail states.
We deÔ¨Åne the fail state as the state where the body of the
robot comes into contact with the ground or when the robot
has a large body tilt. We also set a time limit of 10 sto
terminate the episode early.IV. RESULTS
A. Effect of Step Frequency
Step frequency modulation behaviors have been observed
within animals, where the step frequency increases alongside
the locomotion velocity. We conduct a comparison study
to investigate the effect of step frequency modulation on
locomotion performance by comparing the following con-
Ô¨Ågurations:
1)Fixed 1.5Hz : The intrinsic frequency of the 12 CPGs
are Ô¨Åxed to 1.5Hz.
2)Fixed 3.0Hz : The intrinsic frequency of the 12 CPGs
are Ô¨Åxed to 3Hz.
2)Adaptive Curve : Prior knowledge of frequency is
provided to encourage the policy to track the reference
frequency calculated from eq (5).
3)Adaptive Free : No prior knowledge of frequency is
provided.
1) Locomotion velocity: Policies from all four conÔ¨Åg-
urations are able to perform successful locomotion while
tracking user commanded velocity (Fig. 6A). However, the
policy from Fixed 1.5Hz conÔ¨Åguration fails to track 4m/s
target velocity, which might be due to the relatively low step
frequency (Fig. 6D1).
Figure 6D2 shows the frequency modulation signal f
provided by the MLP network. Both policies from Adaptive
Curve andAdaptive Free conÔ¨Ågurations learned to increase
their step frequency to reach higher locomotion velocity
(Fig. 6C). While frequency modulation behavior naturally
emerges without providing prior knowledge, providing prior
knowledge of step frequency through a loss function leads
to a smoother frequency modulation behavior.
2) Energy efÔ¨Åciency: We utilize Cost of Transport (CoT)
to measure the energy efÔ¨Åciency of the policies under dif-
ferent locomotion velocities. After comparing the policies
from four conÔ¨Ågurations, we observed that policies with
Ô¨Åxed frequency exhibit signiÔ¨Åcantly higher CoT within the
low-velocity region compared to policies capable of self-
modulating frequency (Fig. 6B). This is due to unnecessarily
high-frequency stepping.
3) Step length: Animals adjust their step length to reg-
ulate their locomotion velocity [22]. We observed similar
emergent behavior of step length modulation among all four
policies. All policies have learned to increase the step length
to reach higher locomotion velocities (Fig. 7A). Policies
modulate the step length via the adjustments of the amplitude
of the joint CPGs. The amplitude of the hip and knee CPG
oscillators within all policies changes accordingly with loco-
motion velocity (Fig. 6D3 & D4). The change in amplitude
will have a direct impact on the step length as can be seen in
Fig. 7B and Fig. 7C, where larger CPG oscillation amplitude
results in a larger step length. The step length for policy
Fixed 1.5Hz is higher compared to the other three policies
to compensate for the relatively lower step frequency.
B. Interpretability of MLP-CPG Architecture
A side beneÔ¨Åt of our MLP-CPG network architecture
is the additional interpretability compared to vanilla neuralA
B
CD1
D2
D3
D4
D5
D6Fig. 6. (A) The sum of rewards of Fixed 3.0Hz ,Adaptive Curve , and Adaptive Free converged to similar values, while the sum of reward of Fixed 1.5Hz
converged to a lower value. (B) Cost of Transport. (C) Correlation between locomotion velocity and step frequency. (D1-D6) The policies are tasked to
move forward while tracking a target command velocity. (D1) Target velocity and measured velocity. (D2) Step frequency. Translucent thick lines are the
modulated frequency feedback fprovided by MLP. Solid thin lines are the real frequency of the gait calculated using time-frequency analysis. (D3 & D4)
Averaged CPG amplitude of the knee and hip joints. (D5 & D6) Joint torques and velocities from a single leg of policy Adaptive Curve .
A B C
Fig. 7. (A) Correlation between locomotion velocity and step length. Step length increases accordingly as the velocity increases. (B) and (C) Correlation
between CPG oscillation amplitude of the hip and knee. Step length increases as the CPG oscillation amplitude increases
A1m/s        1.1Hz
 B C D2m/s        2.1Hz 3m/s        2.8Hz 4m/s        2.8Hz
Fig. 8. Joint phase, step frequency, and foot contact pattern of the Adaptive Curve policy under different locomotion velocities. (A) 1.1 Hzstep frequency
under 1m=s . (B) 2.1Hzstep frequency under 2 m=s . (C) 2.8Hzstep frequency under 3 m=s . (D) 2.8Hzstep frequency under 4 m=s . FL: front left, FR:
front right, RL: rear left, RR: rear right.
networks. The frequency, amplitude, and phase of the CPG
neurons are parameters that can be directly retrieved from theMLP-CPG network. As can be seen from Fig. 7 and Fig. 8,
the frequency, amplitude, and phase of the CPGs correlateA1 A2 A3 A4
B1 B2
B3 B4Fig. 9. (A1-A4) Trajectory following performance of policy Adaptive Curve . (A1) Square Wave. (A2) Cosine Wave. (A3) Eight Curve. (A4) Four-leaved
Clover Curve. (B1-B4) The velocity tracking performance while following trajectory A1-A4.
with step frequency, step length, and foot contact pattern,
which are three important aspects of legged locomotion. By
analyzing the values of the frequency, amplitude, and phase
of the CPG neurons, we can easily interpret the locomotion
characteristics of the robot.
C. Performance Benchmark
1) Target following: We design 4 sets of trajectories with
varying difÔ¨Åculties for the robot to follow (Fig. 9). The robot
is tasked to follow the trajectory by tracking a moving target.
The relative position of the moving target is translated into
velocity command for the robot. The policy from Adaptive
Curve is evaluated.
Given the target position and robot base position, we
calculate the velocity commands as follows:
^vx= xbase
target;^vy= ybase
target;^!yaw= arctan(ybase
target
xbase
target);
(9)
wherexbase
target is the x coordinate of the target in robot
base frame, and ybase
target is the y coordinate of the target in
robot base frame. The velocity targets are bounded within
[ 1;4]m=s,[ 0:75;0:75]m=s,[ 1:0;1:0]rad=s for for-
ward velocity, lateral velocity, and yaw angular rate. The
learned Adaptive Curve policy is able to follow all four
trajectories.
D. Robustness Tests
We design four sets of test scenarios to evaluate the
robustness of the learned policies (Fig. 10). 1) Blind traversal
over Uneven terrain. The uneven terrain consists of small
bumps with height ranging from 0 mto 0.1m. 2) Persistent
perturbation. External perturbation forces are applied to the
robot every 4s through the impact of small cubes with mass
A
B
C
DFig. 10. Overlaid snapshots of the Adaptive Curve policy. (A) Uneven
terrain. (B) External disturbance. (C) Stairs. (D) External load.
of 5kg. 3) Blind traversal over stairs. The stair consists of
steps with height of 0.05m. 4) Carrying external load. A load
with mass of 20 kg(50% of the robot‚Äôs own mass) is placed
on the robot.
Policies from all four training conÔ¨Ågurations are able
to traverse over uneven terrain and stairs, carry external
load, and resist external perturbations (See accompanying
video). The policies exhibit highly dynamic motions while
reacting to external uncertainties in the environment. Notethat the policies are trained on Ô¨Çat ground and encountered
no perturbations during the training phase.
V. D ISCUSSION AND FUTURE WORK
While our work demonstrated robust locomotion in simu-
lation, it has not yet been implemented on a real robot. For
future work, we plan to transfer the policy to a real robot
system. Additionally, high-speed locomotion on real robot
systems is challenging. Even though our policy is capable of
achieving velocities up to 4m/s in simulation while satisfying
motor constraints, having the joint torques and velocities
constantly reaching the motor limits will inevitably cause a
lot of stress and also overheat the motors. Mechanical stress
and motor overheating need to be considered during sim-to-
real transfer.
This paper has only focused on achieving rhythmic motor
task of locomotion for quadruped robots. For future work,
we will investigate the feasibility of using our proposed
MLP-CPG network to achieve multi-tasking of both rhythmic
motor tasks such as locomotion and non-rhythmic motor
tasks such as fall recovery.
VI. CONCLUSION
In this paper, we proposed a bio-inspired network architec-
ture called MLP-CPG that utilizes the intrinsic rhythmicity
of CPGs to generate rhythmic motor patterns for locomotion
without relying on external phase input. We proposed a
reformulation approach that transforms CPGs into stateless
networks by exposing the internal states. With our approach,
the parameters of CPGs can be optimized jointly with the
MLP network using DRL.
Our MLP-CPG has the advantage of interpretability. Lo-
comotion characteristics such as step frequency, step length,
gait pattern of the robot can be inferred by analyzing the
values of the frequency, amplitude, and phase of the CPGs,
Our MLP-CPG network demonstrates emergent behaviors
of step frequency and step length modulation. We observed
that the learned policies increase the step frequency and
step length accordingly as the locomotion velocity increases,
demonstrating that the policy can actively alter locomotion
patterns. The learned policy is not only capable of success-
fully performing the task of target following, but also capable
of robustly traversing over uneven terrain and stairs, carrying
external load, and resisting external perturbations.
REFERENCES
[1] J. Yu, M. Tan, J. Chen, and J. Zhang, ‚ÄúA survey on cpg-inspired
control models and system implementation,‚Äù IEEE transactions on
neural networks and learning systems , vol. 25, no. 3, pp. 441‚Äì456,
2013.
[2] A. J. Ijspeert, ‚ÄúCentral pattern generators for locomotion control in
animals and robots: a review,‚Äù Neural networks , vol. 21, no. 4, pp.
642‚Äì653, 2008.
[3] C. Yang, K. Yuan, Q. Zhu, W. Yu, and Z. Li, ‚ÄúMulti-expert learning
of adaptive legged locomotion,‚Äù Science Robotics , vol. 5, no. 49, p.
eabb2174, 2020.
[4] J. Lee, J. Hwangbo, L. Wellhausen, V . Koltun, and M. Hutter,
‚ÄúLearning quadrupedal locomotion over challenging terrain,‚Äù Science
robotics , vol. 5, no. 47, p. eabc5986, 2020.[5] Y . Shao, Y . Jin, X. Liu, W. He, H. Wang, and W. Yang, ‚ÄúLearning
free gait transition for quadruped robots via phase-guided controller,‚Äù
IEEE Robotics and Automation Letters , 2021.
[6] J. Sheng, Y . Chen, X. Fang, W. Zhang, R. Song, Y . Zheng, and
Y . Li, ‚ÄúBio-inspired rhythmic locomotion for quadruped robots,‚Äù IEEE
Robotics and Automation Letters , 2022.
[7] Y . Yang, T. Zhang, E. Coumans, J. Tan, and B. Boots, ‚ÄúFast and
efÔ¨Åcient locomotion via learned gait transitions,‚Äù in Conference on
Robot Learning . PMLR, 2022, pp. 773‚Äì783.
[8] Y . Cho, S. Manzoor, and Y . Choi, ‚ÄúAdaptation to environmental change
using reinforcement learning for robotic salamander,‚Äù Intelligent Ser-
vice Robotics , vol. 12, no. 3, pp. 209‚Äì218, 2019.
[9] Y . Nakamura, T. Mori, M.-a. Sato, and S. Ishii, ‚ÄúReinforcement
learning for a biped robot based on a cpg-actor-critic method,‚Äù Neural
networks , vol. 20, no. 6, pp. 723‚Äì735, 2007.
[10] S. Fukunaga, Y . Nakamura, K. Aso, and S. Ishii, ‚ÄúReinforcement learn-
ing for a snake-like robot controlled by a central pattern generator,‚Äù in
IEEE Conference on Robotics, Automation and Mechatronics, 2004. ,
vol. 2. IEEE, 2004, pp. 909‚Äì914.
[11] S. Aoi, P. Manoonpong, Y . Ambe, F. Matsuno, and F. W ¬®org¬®otter,
‚ÄúAdaptive control strategies for interlimb coordination in legged
robots: a review,‚Äù Frontiers in neurorobotics , vol. 11, p. 39, 2017.
[12] M. Thor and P. Manoonpong, ‚ÄúA fast online frequency adaptation
mechanism for cpg-based robot motion control,‚Äù IEEE Robotics and
Automation Letters , vol. 4, no. 4, pp. 3324‚Äì3331, 2019.
[13] G. Endo, J. Nakanishi, J. Morimoto, and G. Cheng, ‚ÄúExperimental
studies of a neural oscillator for biped locomotion with qrio,‚Äù in
Proceedings of the 2005 IEEE international conference on robotics
and automation . IEEE, 2005, pp. 596‚Äì602.
[14] S. Gay, J. Santos-Victor, and A. Ijspeert, ‚ÄúLearning robot gait stability
using neural networks as sensory feedback function for central pattern
generators,‚Äù in 2013 IEEE/RSJ international conference on intelligent
robots and systems . Ieee, 2013, pp. 194‚Äì201.
[15] L. Campanaro, S. Gangapurwala, D. D. Martini, W. Merkt, and
I. Havoutis, ‚ÄúCpg-actor: Reinforcement learning for central pattern
generators,‚Äù in Annual Conference Towards Autonomous Robotic Sys-
tems. Springer, 2021, pp. 25‚Äì35.
[16] H. Shi, B. Zhou, H. Zeng, F. Wang, Y . Dong, J. Li, K. Wang, H. Tian,
and M. Q.-H. Meng, ‚ÄúReinforcement learning with evolutionary tra-
jectory generator: A general approach for quadrupedal locomotion,‚Äù
IEEE Robotics and Automation Letters , vol. 7, no. 2, pp. 3085‚Äì3092,
2022.
[17] J. Wang, C. Hu, and Y . Zhu, ‚ÄúCpg-based hierarchical locomotion
control for modular quadrupedal robots using deep reinforcement
learning,‚Äù IEEE Robotics and Automation Letters , vol. 6, no. 4, pp.
7193‚Äì7200, 2021.
[18] T. Wei and B. Webb, ‚ÄúA model of operant learning based on chaotically
varying synaptic strength,‚Äù Neural Networks , vol. 108, pp. 114‚Äì127,
2018.
[19] ‚Äî‚Äî, ‚ÄúA bio-inspired reinforcement learning rule to optimise dynami-
cal neural networks for robot control,‚Äù in 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) . IEEE, 2018,
pp. 556‚Äì561.
[20] A. Duburcq, F. Schramm, G. Bo ¬¥eris, N. Bredeche, and Y . Chevaleyre,
‚ÄúReactive stepping for humanoid robots using reinforcement learning:
Application to standing push recovery on the exoskeleton atalante,‚Äù
arXiv preprint arXiv:2203.01148 , 2022.
[21] L. D. Maes, M. Herbin, R. Hackert, V . L. Bels, and A. Abourachid,
‚ÄúSteady locomotion in dogs: temporal and associated spatial coordina-
tion patterns and the effect of speed,‚Äù Journal of Experimental Biology ,
vol. 211, no. 1, pp. 138‚Äì149, 2008.
[22] M. C. Granatosky and E. J. McElroy, ‚ÄúStride frequency or length? a
phylogenetic approach to understand how animals regulate locomotor
speed,‚Äù Journal of Experimental Biology , vol. 225, no. Suppl 1, p.
jeb243231, 2022.