T2M-GPT: Generating Human Motion from Textual Descriptions with
Discrete Representations
Jianrong Zhang1,3∗, Yangsong Zhang2,3∗, Xiaodong Cun3, Shaoli Huang3, Yong Zhang3
Hongwei Zhao1, Hongtao Lu2, Xi Shen3,†
∗Equal contribution†Corresponding author
1Jilin University2Shanghai Jiao Tong University3Tencent AI Lab
Abstract
In this work, we investigate a simple and must-known con-
ditional generative framework based on Vector Quantised-
Variational AutoEncoder (VQ-VAE) and Generative Pre-
trained Transformer (GPT) for human motion generation
from textural descriptions. We show that a simple CNN-
based VQ-VAE with commonly used training recipes (EMA
and Code Reset) allows us to obtain high-quality discrete rep-
resentations. For GPT, we incorporate a simple corruption
strategy during the training to alleviate training-testing dis-
crepancy. Despite its simplicity, our T2M-GPT shows better
performance than competitive approaches, including recent
diffusion-based approaches. For example, on HumanML3D,
which is currently the largest dataset, we achieve compara-
ble performance on the consistency between text and gener-
ated motion (R-Precision), but with FID 0.116 largely outper-
forming MotionDiffuse of 0.630. Additionally, we conduct
analyses on HumanML3D and observe that the dataset size
is a limitation of our approach. Our work suggests that VQ-
VAE still remains a competitive approach for human motion
generation. Our implementation is available on the project
page: https://mael-zys.github.io/T2M-GPT/ .
1. Introduction
Generating motion from textual descriptions can be used
in numerous applications in the game industry, film-making,
and animating robots. For example, a typical way to access
new motion in the game industry is to perform motion cap-
ture, which is expensive. Therefore automatically generating
motion from textual descriptions, which allows producing
meaningful motion data, could save time and be more eco-
nomical.
Motion generation conditioned on natural language is
challenging, as motion and text are from different modali-
ties. The model is expected to learn precise mapping from
Figure 1. Visual results on HumanML3D [22]. Our approach is
able to generate precise and high-quality human motion consistent
with challenging text descriptions. More visual results are on the
project page.
the language space to the motion space. To this end, many
works propose to learn a joint embedding for language and
motion using auto-encoders [3, 21, 65] and V AEs [52, 53].
MotionClip [65] aligns the motion space to CLIP [55] space.
ACTOR [52] and TEMOES [53] propose transformer-based
V AEs for action-to-motion and text-to-motion respectively.
These works show promising performances with simple de-
scriptions and are limited to producing high-quality motion
when textual descriptions become long and complicated.
Guo et al. [22] and TM2T [23] aim to generate motionarXiv:2301.06052v4  [cs.CV]  24 Sep 2023sequences with more challenging textual descriptions. How-
ever, both approaches are not straightforward, involve three
stages for text-to-motion generation, and sometimes fail to
generate high-quality motion consistent with the text (See
Figure 4 and more visual results on the project page). Re-
cently, diffusion-based models [32] have shown impressive
results on image generation [60], which are then introduced
to motion generation by MDM [66] and MotionDiffuse [74]
and dominates text-to-motion generation task. However,
we find that compared to classic approaches, such as VQ-
V AE [68], the performance gain of the diffusion-based ap-
proaches [66, 74] might not be that significant. In this work,
we are inspired by recent advances from learning the discrete
representation for generation [5,15,16,19,45,58,68,70] and
investigate a simple and classic framework based on Vector
Quantized Variational Autoencoders (VQ-V AE) [68] and
Generative Pre-trained Transformer (GPT) [56, 69] for text-
to-motion generation.
Precisely, we propose a two-stage method for motion
generation from textual descriptions. In stage 1, we use a
standard 1D convolutional network to map motion sequences
to discrete code indices. In stage 2, a standard GPT-like
model [56, 69] is learned to generate sequences of code in-
dices from pre-trained text embedding. We find that the naive
training of VQ-V AE [68] suffers from code collapse. One
effective solution is to leverage two standard recipes during
the training: EMA andCode Reset . We provide a full anal-
ysis of different quantization strategies. For GPT, the next
token prediction brings inconsistency between the training
and inference. We observe that simply corrupting sequences
during the training alleviates this discrepancy. Moreover,
throughout the evolution of image generation, the size of
the dataset has played an important role. We further explore
the impact of dataset size on the performance of our model.
The empirical analysis suggests that the performance of our
model can potentially be improved with larger datasets.
Despite its simplicity, our approach can generate high-
quality motion sequences that are consistent with challeng-
ing text descriptions (Figure 1 and more on the project
page). Empirically, we achieve comparable or even better
performances than concurrent diffusion-based approaches
MDM [66] and HumanDiffuse [74] on two widely used
datasets: HumanML3D [22] and KIT-ML [54]. For example,
on HumanML3D, which is currently the largest dataset, we
achieve comparable performance on the consistency between
text and generated motion (R-Precision), but with FID 0.116
largely outperforming MotionDiffuse of 0.630. We conduct
comprehensive experiments to explore this area, and hope
that these experiments and conclusions will contribute to
future developments.
In summary, our contributions include:
•We present a simple yet effective approach for mo-
tion generation from textual descriptions. Our ap-proach achieves state-of-the-art performance on Hu-
manML3D [22] and KIT-ML [54] datasets.
•We show that GPT-like models incorporating discrete
representations still remain a very competitive approach
for motion generation.
•We provide a detailed analysis of the impact of quanti-
zation strategies and dataset size. We show that a larger
dataset might still offer a promising prospect to the
community.
Our implementation is available on the project page.
2. Related work
VQ-VAE. Vector Quantized Variational Autoencoders
(VQ-V AE), which is a variant of V AE [36], is initially pro-
posed in [68]. VQ-V AE is composed of an AutoEncoder
architecture, which aims at learning reconstruction with dis-
crete representations. Recently, VQ-V AE achieves promis-
ing performance on generative tasks across different modali-
ties, which includes: image synthesis [19, 70], text-to-image
generation [58], speech gesture generation [5], music gen-
eration [15, 16] etc. The success of VQ-V AE for generation
might be attributed to its decoupling of learning the discrete
representation and the prior. A naive training of VQ-V AE suf-
fers from the codebook collapse, i.e., only a number of codes
are activated, which importantly limited the performances
of the reconstruction as well as generation. To alleviate the
problem, a number of techniques can be used during train-
ing, including stop-gradient along with some losses [68] to
optimize the codebook, exponential moving average (EMA)
for codebook update [70], reset inactivated codes during the
training (Code Reset [70]), etc.
Human motion synthesis. Research on human motion
synthesis has a long history [8]. One of the most active
research fields is human motion prediction, which aims at
predicting the future motion sequence based on past ob-
served motion. Approaches mainly focus on efficiently and
effectively fusing spatial and temporal information to gen-
erate deterministic future motion through different models:
RNN [12,20,50,51], GAN [9,30], GCN [49], Attention [48]
or even simply MLP [11, 17, 25]. Some approaches aim
at generating diverse motion through V AE [4, 26, 73]. In
addition to synthesizing motion conditioning on past motion,
another related topic is generating motion “in-betweening”
that takes both past and future poses and fills motion between
them [18,27,28,35,64]. [51] considers the generation of loco-
motion sequences from a given trajectory for simple actions
such as: walking and running. Motion can also be generated
with music to produce 3D dance motion [6,13,37,38,40,41].
For unconstrained generations, [72] generates a long se-
quence altogether by transforming from a sequence of latent(a) Motion VQ-V AE
 (b) T2M-GPT
Figure 2. Overview of our framework for text-driven motion generation. It includes two modules: Motion VQ-V AE (Figure 2a) and
T2M-GPT (Figure 2b). In T2M-GPT, an additional learnable End token is inserted to indicate the stop of the generation. During the
inference, we first generate code indexes in an auto-regressive fashion and then obtain the motion using the decoder in Motion VQ-V AE.
vectors sampled from a Gaussian process. In graphics lit-
erature, many works focus on animator control. Holden
et al. [34] learn a convolutional autoencoder to reconstruct
motion, the learned latent representation can be used to syn-
thesize and edit motion. [33] proposes phase functioned
neural network to perform the control task. [63] uses a deep
auto-regressive framework to scene interaction behaviors.
Starke et al. [62] proposes to reconstruct motion through
periodic features, the learned periodic embedding improves
motion synthesis. Recently, inspired by SinGAN [61] for
image synthesis, Li et al. [39] propose a generative model
approach for motion synthesis from a single sequence.
Text-driven human motion generation. Text-driven hu-
man motion generation aims at generating 3D human mo-
tion from textual descriptions. Text2Action [2] trains an
RNN-based model to generate motion conditioned on a short
text. Language2Pose [3] employs a curriculum learning
approach to learn a joint embedding space for both text
and pose. The decoder can thus take text embedding to
generate motion sequences. Ghost et al. [21] learn two man-
ifold representations for the upper body and the lower body
movements, which shows improved performance compared
to Language2Pose [3]. Similarly, MotionCLIP [65] also
tends to align text and motion embedding but proposes to
utilize CLIP [55] as the text encoder and employ rendered
images as extra supervision. It shows the ability to generate
out-of-distribution motion and enable latent code editing.
However, the generated motion sequences are not in high-
quality and are without global translation. ACTOR [52]
proposes a transformer-based V AE to generate motion in a
non-autoregressive fashion from a pre-defined action class.
TEMOS [53] extends the architecture of ACTOR [52] by
introducing an additional text encoder and producing di-
verse motion sequences given text descriptions. TEMOS
demonstrates its effect on KIT Motion-Language [54] withmainly short sentences and suffers from out-of-distribution
descriptions [53]. TEACH [7] further extends TEMOS to
generate temporal motion compositions from a series of nat-
ural language descriptions. Recently, a large-scale dataset
HumanML3D is proposed in [22]. Guo et al. [22] also pro-
pose to incorporate motion length prediction from text to
produce motion with reasonable length. TM2T [23] consid-
ers text-to-motion and motion-to-text tasks. It also shows
additional improvement can be obtained through jointly train-
ing both tasks. As concurrent works, diffusion-based models
are introduced for text-to-motion generation by MDM [66]
and MotionDiffuse [74]. In this work, we show that without
any sophisticated designs, the classic VQ-V AE framework
could achieve competitive or even better performance with a
classical framework and some standard training recipes.
3. Method
Our goal is to generate high-quality motion that is consis-
tent with text descriptions. The overall framework consists
of two modules: Motion VQ-V AE and T2M-GPT, which
is illustrated in Figure 2. The former learns a mapping be-
tween motion data and discrete code sequences, the latter
generates code indices conditioned on the text description.
With the decoder in Motion VQ-V AE, we are able to re-
cover the motion from the code indices. In Section 3.1, we
present the VQ-V AE module. The T2M-GPT is introduced
in Section 3.2.
3.1. Motion VQ-V AE
VQ-V AE, proposed in [68], enables the model to learn
discrete representations for generative models. Given a mo-
tion sequence X= [x1, x2, . . . , x T]withxt∈Rd, where
Tis the number of frames and dis the dimension of the
motion, we aim to recover the motion sequence through an
autoencoder and a learnable codebook containing Kcodes
C={ck}K
k=1withck∈Rdc, where dcis the dimension ofFigure 3. Architecture of the motion VQ-V AE. We use a standard
CNN-based architecture with 1D convolution ( Conv1D ), residual
block ( ResBlock ) and ReLU activation. ‘ L’ denotes the number
of residual blocks. We use convolution with stride 2 and nearest
interpolation for temporal downsampling and upsampling.
codes. The overview of VQ-V AE is presented in Figure 2a.
With encoder and decoder of the autoencoder denoted by E
andD, the latent feature Zcan be computed as Z=E(X)
withZ= [z1, z2, ..., z T/l]andzi∈Rdc, where lrepresents
the temporal downsampling rate of the encoder E. For i-th
latent feature zi, the quantization through Cis to find the
most similar element in C, which can be properly written as:
ˆzi= arg
ck∈Cmin∥zi−ck∥2 (1)
Optimization goal. To optimize VQ-V AE, the standard
optimization goal [68] Lvqcontains three components: a
reconstruction loss Lre, the embedding loss Lembed and the
commitment loss Lcommit .
Lvq=Lre+||sg[Z]−ˆZ||2|{z}
Lembed+β||Z−sg[ˆZ]||2|{z}
Lcommit(2)
where βis a hyper-parameter for the commitment loss and
sgis the stop-gradient operator. For the reconstruction,
we find that L1 smooth loss Lsmooth
1 performs best and
an additional regularization on the velocity enhances the
generation quality. Let Xrebe the reconstructed motion of
X, i.e., Xre=D(ˆZ), V(X) be the velocity of Xwhere
V= [v1, v2, . . . , v T−1]withvi=xi+1−xi. Therefore, the
objective of the reconstruction is as follows:
Lre=Lsmooth
1 (X, X re) +αLsmooth
1 (V(X), V(Xre))
(3)
where αis a hyper-parameter to balance the two losses. We
provide an ablation study on αas well as different recon-
struction losses ( L1,Lsmooth
1 andL2) in Section B of the
appendix.Quantization strategy. A naive training of VQ-V AE
suffers from codebook collapse [59, 68]. Two training
recipes [59] are commonly used to improve the codebook
utilization: exponential moving average (EMA) and code-
book reset (Code Reset) .EMA makes the codebook Cevolve
smoothly: Ct←λCt−1+ (1−λ)Ct, where Ctis the code-
book at iteration tandλis the exponential moving constant.
Code Reset finds inactivate codes during the training and re-
assigns them according to input data. We provide an ablation
study on the quantization strategy in Section 4.3.
Architecture. We use a simple convolutional architecture
composed of 1D convolution, residual block [29], and ReLU.
Our VQ-V AE architecture is illustrated in Figure 3. The
architecture is inspired by [19, 41]. We use convolution with
stride 2 and nearest interpolation for temporal downsampling
and upsampling respectively. The downsampling rate is thus
l= 2L, where Ldenotes the number of residual blocks. We
provide an ablation study on the architecture in Section 4.3.
The detail of the architecture is provided in Section F of the
appendix.
3.2. T2M-GPT
With a learned motion VQ-V AE, a motion sequence X=
[x1, x2, . . . , x T]can be mapped to a sequence of indices S=
[s1, s2, . . . , s T/l,End]withsi∈[1,2, . . . , s T/l], which are
indices from the learned codebook. Note that a special End
token is added to indicate the stop of the motion, which is
different from [22] that leverages an extra module to predict
motion length. By projecting Sback to their corresponding
codebook entries, we obtain ˆZ= [ˆz1,ˆz2, . . . , ˆzT/l]with
ˆzi=csi, which can be decoded to a motion Xrethrough
the decoder D. Therefore, text-to-motion generation can
be formulated as an autoregressive next-index prediction:
given previous i−1indices, i.e., S<i, and text condition c,
we aim to predict the distribution of possible next indices
p(Si|c, S<i), which can be addressed with transformer [69].
The overview of our transformer is shown in Figure 2b.
Optimization goal. Denoting the likelihood of the full
sequence as p(S|c) =Q|S|
i=1p(Si|c, S<i), we directly maxi-
mize the log-likelihood of the data distribution:
Ltrans =ES∼p(S)[−logp(S|c)] (4)
We leverage CLIP [55] to extract text embedding c, which
has shown its effectiveness in relevant tasks [14, 57, 65].
Causal Self-attention. We apply the causal self-
attention [56] in T2M-GPT. Precisely, the output of the
causal self-attention is calculated as follows:
Attention =SoftmaxQKT×mask√dk
(5)MethodsR-Precision ↑FID↓ MM-Dist ↓Diversity ↑MModality ↑Top-1 Top-2 Top-3
Real motion 0.511±.0030.703±.0030.797±.0020.002±.0002.974±.0089.503±.065-
Our VQ-V AE (Recons.) 0.501±.0020.692±.0020.785±.0020.070±.0013.072±.0099.593±.079-
Seq2Seq [42] 0.180±.0020.300±.0020.396±.00211.75±.0355.529±.0076.223±.061-
Language2Pose [3] 0.246±.0020.387±.0020.486±.00211.02±.0465.296±.0087.676±.058-
Text2Gesture [10] 0.165±.0010.267±.0020.345±.0025.012±.0306.030±.0086.409±.071-
Hier [21] 0.301±.0020.425±.0020.552±.0046.532±.0245.012±.0188.332±.042-
MoCoGAN [67] 0.037±.0000.072±.0010.106±.00194.41±.0219.643±.0060.462±.0080.019±.000
Dance2Music [37] 0.033±.0000.065±.0010.097±.00166.98±.0168.116±.0060.725±.0110.043±.001
TEMOS§[53] 0.424±.0020.612±.0020.722±.0023.734±.0283.703±.0088.973±.0710.368±.018
TM2T [23] 0.424±.0030.618±.0030.729±.0021.501±.0173.467±.0118.589±.0762.424±.093
Guo et al. [22] 0.455±.0030.636±.0030.736±.0021.087±.0213.347±.0089.175±.0832.219±.074
MLD§[71] 0.481±.0030.673±.0030.772±.0020.473±.0133.196±.0109.724±.0822.413±.079
MDM§[66] - - 0.611±.0070.544±.0445.566±.0279.559±.0862.799±.072
MotionDiffuse§[74] 0.491±.0010.681±.0010.782±.0010.630±.0013.113±.0019.410±.0491.553±.042
Our GPT (τ= 0) 0.417±.0030.589±.0020.685±.0030.140±.0063.730±.0099.844±.0953.285±.070
Our GPT (τ= 0.5) 0.491±.0030.680±.0030.775±.0020.116±.0043.118±.0119.761±.0811.856±.011
Our GPT (τ∈ U[0,1])0.492±.0030.679±.0020.775±.0020.141±.0053.121±.0099.722±.0821.831±.048
Table 1. Comparison with the state-of-the-art methods on HumanML3D [22] test set. We compute standard metrics following Guo et
al.[22]. For each metric, we repeat the evaluation 20 times and report the average with 95% confidence interval. Red and Blue indicate the
best and the second best result.§reports results using ground-truth motion length.
where Q∈RT×dkandK∈RT×dkare query and key
respectively, while mask is the causal mask with mask i,j=
−∞ × 1(i < j ) +1(i≥j), where 1(·)is the indicator
function. This causal mask ensures that future information is
not allowed to attend the calculation of current tokens. For
inference, we start from the text embedding and generate
indices in an autoregressive fashion, the generation process
will be stopped if the model predicts the End token. Note
that we are able to generate diverse motions by sampling
from the predicted distributions given by the transformer.
Corrupted sequences for the training-testing discrepancy.
There is a discrepancy between training and testing. For
training, i−1correct indices are used to predict the next
index. While for inference, there is no guarantee that indices
serving as conditions are correct. To address this problem,
we adopt a simple data augmentation strategy: we replace
τ×100% ground-truth code indices with random ones during
training. τcan be a hyper-parameter or randomly sampled
from τ∈ U[0,1]. We provide an ablation study on this
strategy in Section C of the appendix.
4. Experiment
In this section, we present our experimental results. In
Section 4.1, we introduce standard datasets as well as eval-
uation metrics. We compare our results to competitive ap-
proaches in Section 4.2. Finally, we provide analysis anddiscussion in Section 4.3.
4.1. Datasets and evaluation metric
We conduct experiments on two standard datasets for
text-driven motion generations: HumanML3D [22] and KIT
Motion-Language (KIT-ML) [54]. Both datasets are com-
monly used in the community. We follow the evaluation
protocol proposed in [22].
KIT Motion-Language (KIT-ML). KIT-ML [54] contains
3,911 human motion sequences and 6,278 textual annota-
tions. The total vocabulary size, that is the number of unique
words disregarding capitalization and punctuation, is 1,623.
Motion sequences are selected from KIT [47] and CMU [1]
datasets but downsampled into 12.5 frame-per-second (FPS).
Each motion sequence is described by from 1 to 4 sentences.
The average length of descriptions is approximately 8. Fol-
lowing [22, 23], the dataset is split into training, validation,
and test sets with proportions of 80%, 5%, and 15%, respec-
tively. We select the model that achieves the best FID on the
validation set and report its performance on the test set.
HumanML3D. HumanML3D [22] is currently the largest
3D human motion dataset with textual descriptions. The
dataset contains 14,616 human motions and 44,970 text de-
scriptions. The entire textual descriptions are composed of
5,371 distinct words. The motion sequences are originally
from AMASS [46] and HumanAct12 [24] but with specific
pre-processing: motion is scaled to 20 FPS; those that are
longer than 10 seconds are randomly cropped to 10-secondMethodsR-Precision ↑FID↓ MM-Dist ↓ Diversity ↑ MModality ↑Top-1 Top-2 Top-3
Real motion 0.424±.0050.649±.0060.779±.0060.031±.0042.788±.01211.08±.097-
Our VQ-V AE (Recons.) 0.399±.0050.614±.0050.740±.0060.472±.0112.986±.02710.994±.120-
Seq2Seq [42] 0.103±.0030.178±.0050.241±.00624.86±.3487.960±.0316.744±.106-
Language2Pose [3] 0.221±.0050.373±.0040.483±.0056.545±.0725.147±.0309.073±.100-
Text2Gesture [10] 0.156±.0040.255±.0040.338±.00512.12±.1836.964±.0299.334±.079-
Hier [21] 0.255±.0060.432±.0070.531±.0075.203±.1074.986±.0279.563±.072-
MoCoGAN [67] 0.022±.0020.042±.0030.063±.00382.69±.24210.47±.0123.091±.0430.250±.009
Dance2Music [37] 0.031±.0020.058±.0020.086±.003115.4±.24010.40±.0160.241±.0040.062±.002
TEMOS§[53, 71] 0.353±.0020.561±.0020.687±.0023.717±.0283.417±.00810.84±.0710.532±.018
TM2T [23] 0.280±.0060.463±.0070.587±.0053.599±.0514.591±.0199.473±.1003.292±.034
Guo et al. [22] 0.361±.0060.559±.0070.681±.0073.022±.1073.488±.02810.72±.1452.052±.107
MLD§[71] 0.390±.0080.609±.0080.734±.0070.404±.0273.204±.02710.80±.1172.192±.071
MDM§[66] - - 0.396±.0040.497±.0219.191±.02210.847±.1091.907±.214
MotionDiffuse§[74] 0.417±.0040.621±.0040.739±.0041.954±.0622.958±.00511.10±.1430.730±.013
Our GPT ( τ= 0) 0.392±.0070.600±.0070.716±.0060.737±.0493.237±.02711.198±.0862.309±.055
Our GPT ( τ= 0.5) 0.402±.0060.619±.0050.737±.0060.717±.0413.053±.02610.862±.0941.912±.036
Our GPT ( τ∈ U[0,1])0.416±.0060.627±.0060.745±.0060.514±.0293.007±.02310.921±.1081.570±.039
Table 2. Comparison with the state-of-the-art methods on KIT-ML [54] test set. We compute standard metrics following Guo et al. [22].
For each metric, we repeat the evaluation 20 times and report the average with 95% confidence interval. Red and Blue indicate the best and
the second best result.§reports results using ground-truth motion length.
ones; they are then re-targeted to a default human skeletal
template and properly rotated to face Z+ direction initially.
Each motion is paired with at least 3 precise textual descrip-
tions. The average length of descriptions is approximately
12. According to [22], the dataset is split into training, vali-
dation, and test sets with proportions of 80%, 5%, and 15%,
respectively. We select the best FID model on the validation
set and report its performance on the test set.
Implementation details. For Motion VQ-V AE, the code-
book size is set to 512×512. The downsampling rate lis
4. We provide an ablation on the number of codes in Sec-
tion D of the appendix. For both HumanML3D [22] and
KIT-ML [54] datasets, the motion sequences are cropped to
T= 64 for training. We use AdamW [44] optimizer with
[β1, β2] = [0 .9,0.99], batch size of 256, and exponential
moving constant λ= 0.99. We train the first 200K itera-
tions with a learning rate of 2e-4, and 100K with a learning
rate of 1e-5. βandαinLvqandLreare set to 0.02 and
0.5, respectively. Following [22], the dataset KIT-ML and
HumanML3D are extracted into motion features with dimen-
sions 251 and 263 respectively, which correspond to local
joints position, velocity, and rotations in root space as well
as global translation and rotations. These features are com-
puted from 21 and 22 joints of SMPL [43]. More details
about the motion representations are provided in Section E
of the appendix.
For the T2M-GPT, we employ 18 transformer [69] lay-ers with a dimension of 1,024 and 16 heads. The ablation
for different scales of the transformer is provided in Sec-
tion A of the appendix. Following Guo et al . [22], the
maximum length of Motion is 196 on both datasets, and
the minimum lengths are 40 and 24 for HumanML3D [22]
and KIT-ML [54] respectively. The maximum length of the
code index sequence is T′= 50 . We train an extra End
token as a signal to stop index generation. The transformer
is optimized using AdamW [44] with [β1, β2] = [0 .5,0.99]
and batch size 128. The initialized learning rate is set to 1e-4
for 150K iterations and decayed to 5e-6 for another 150K
iterations. Training Motion VQ-V AE and T2M-GPT take
about 14 hours and 78 hours respectively on a single Tesla
V100-32G GPU.
Evaluation metric. Following [22], global representations
of motion and text descriptions are first extracted with the
pre-trained network in [22], and then measured by the fol-
lowing five metrics:
•R-Precision. Given one motion sequence and 32 text
descriptions (1 ground-truth and 31 randomly selected
mismatched descriptions), we rank the Euclidean dis-
tances between the motion and text embeddings. Top-1,
Top-2, and Top-3 accuracy of motion-to-text retrieval
are reported.
•Frechet Inception Distance (FID). We calculate the
distribution distance between the generated and realFigure 4. Visual results on HumanML3D [22] dataset. We compare our generation with Guo et al. [22], MotionDiffuse [74], and
MDM [66]. Distorted motions (red) and sliding (yellow) are highlighted. More visual results can be found on the project page.
motion using FID [31] on the extracted motion features.
•Multimodal Distance (MM-Dist). The average Eu-
clidean distances between each text feature and the
generated motion feature from this text.
•Diversity. From a set of motions, we randomly sample
300 pairs of motion. We extract motion features and
compute the average Euclidean distances of the pairs to
measure motion diversity in the set.
•Multimodality (MModality). For one text description,
we generate 20 motion sequences forming 10 pairs of
motion. We extract motion features and compute the
average Euclidean distances of the pairs. We finally
report the average over all the text descriptions.
Note that more details about the evaluation metrics are
provided in Section E of the appendix.4.2. Comparison to state-of-the-art approaches
Quantitative results. We show the comparison results in
Table 1 and Table 2 on HumanML3D [22] test set and KIT-
ML [54] test set. On both datasets, our reconstruction with
VQ-V AE reaches close performances to real motion, which
suggests high-quality discrete representations learned by our
VQ-V AE. For the generation, our approach achieves compa-
rable performance on text-motion consistency (R-Precision
and MM-Dist) compared to the state-of-the-art method Mo-
tionDiffuse [74], while significantly outperforms MotionDif-
fuse with FID metric. KIT-ML [54] and HumanML3D [22]
are in different scales, which demonstrates the robustness
of the proposed approach. Manually corrupting sequences
during the training of GPT brings consistent improvement
(τ= 0.5v.s.τ= 0). A more detailed analysis is provided
in Section C of the appendix. Unlike Guo et al. [22] involv-Quantizer Reconstruction Generation
Code Reset EMA FID ↓ Top-1↑ FID↓ Top-1↑
0.492±.0040.436±.00342.797±.1560.048±.001
✓ 0.097±.0010.499±.0020.176±.0080.490±.002
✓ 0.102±.0010.494±.0030.248±.0090.461±.002
✓ ✓ 0.070±.0010.501±.0020.116±.0040.491±.003
Table 3. Analysis of VQ-V AE quantizers on HumanML3D [22]
test set. For all the quantizers, we set τ= 0.5and use the same
architectures (VQ-V AE and GPT) described in Section 4.1. We
report FID and Top-1 for both reconstruction and generation. For
each metric, we repeat the evaluation 20 times and report the aver-
age with 95% confidence interval.
ing an extra module to predict motion length, we implicitly
learn the motion length through an additional End token,
which is more straightforward and shown to be more effec-
tive. Note that MDM [66] and MotionDiffuse [74] evaluate
their models with the ground-truth motion length, which is
not practical for real applications.
Qualitative comparison. Figure 4 shows visual results on
HumanML3D [22]. We compare our generations with the
current state-of-the-art models: Guo et al. [22], MDM [66]
and MotionDiffuse [74]. From the example in Figure 4, one
can figure out that our model generates human motion with
better quality than the others, and we highlight in red for
unrealistic motion generated by Guo et al. [22] and Motion-
Diffuse [74]. Moreover, the generated motion of MDM [66]
is not related to the text description. Note that more visual
results and the failure case are provided on the project page.
4.3. Discussion
Quantization strategies. We first investigate the impact
of different quantization strategies presented in Section 3.1.
The results are illustrated in Table 3 for both reconstruction
and generation. We notice that naive VQ-V AE training is not
able to reconstruct nor generate high-quality motion. How-
ever, training with EMA orCode Reset can importantly boost
the performances for both reconstruction and generation.
Impact of dataset size. We further analyze the impact of
dataset size. To understand whether the largest dataset Hu-
manML3D [22] contains enough data for motion generation,
we train our motion VQ-V AE and T2M-GPT on different
subsets of the training data, which consists of 10%, 20%,
50%, 80% and 100% of the training data respectively. The
trained models are evaluated on the entire test set. The results
are illustrated in Figure 5. We evaluate reconstruction for
our motion VQ-V AE and generation for our T2M-GPT using
four metrics: FID, MM-Dist, Top-1, and Top-3 accuracies.
Several insights can be figured out: i)metric for motion qual-
ity (FID) and metric for motion-text consistency (MM-Dist,
Top-1, and Top-3) should be considered at the same time.
(a) FID
 (b) MM-Dist
(c) Top-1 accuracy
 (d) Top-3 accuracy
Figure 5. Impact of dataset size on HumanML3D [22]. We train
our motion VQ-V AE ( Reconstruction ) and T2M-GPT ( Generation )
on the subsets of HumanML3D [22] composed of 10%, 20%, 50%,
80%, and 100% training set respectively. All the models are eval-
uated on the entire test set. We report FID, MM-Dist, Top-1, and
Top-3 accuracy for all the models. Results suggest that our model
might benefit from more training data.
With only 10% data, the motion might be of good quality,
however, the model is not able to generate a correct motion
that corresponds to the text description; ii)the performances
become better with more training data. This trend suggests
that additional training data could bring non-negligible im-
provement to both reconstruction and generation.
5. Conclusion
In this work, we investigated a classic framework based
on VQ-V AE and GPT to synthesize human motion from
textual descriptions. Our method achieved comparable or
even better performances than concurrent diffusion-based
approaches, suggesting that this classic framework remains
a very competitive approach for motion generation. We
explored in detail the effect of various quantization strategies
on motion reconstruction and generation. Moreover, we
provided an analysis of the dataset size. Our finding suggests
that a larger dataset could still bring additional improvement
to our approach.
Acknowledgement We thank Mathis Petrovich, Yuming
Du, Yingyi Chen, Dexiong Chen, and Xuelin Chen for in-
spiring discussions and valuable feedback. This paper is sup-
ported by NSF of China (No. 62176155) and Jilin Province
(20200201037JC), etc. More funding information is pro-
vided in Section H of the appendix.References
[1]Cmu graphics lab motion capture database. http://
mocap.cs.cmu.edu/ . Accessed: 2022-11-11. 5
[2]Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and
Songhwai Oh. Text2action: Generative adversarial synthesis
from language to action. In International Conference on
Robotics and Automation (ICRA) , 2018. 3
[3]Chaitanya Ahuja and Louis-Philippe Morency. Lan-
guage2pose: Natural language grounded pose forecasting.
InInternational Conference on 3D Vision (3DV) , 2019. 1, 3,
5, 6
[4]Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann,
Lars Petersson, and Stephen Gould. A stochastic conditioning
scheme for diverse human motion prediction. In Proceedings
of the Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2020. 2
[5]Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, and
Libin Liu. Rhythmic gesticulator: Rhythm-aware co-speech
gesture synthesis with hierarchical neural embeddings. In
SIGGRAPH Asia , 2022. 2
[6]Andreas Aristidou, Anastasios Yiannakidis, Kfir Aberman,
Daniel Cohen-Or, Ariel Shamir, and Yiorgos Chrysanthou.
Rhythm is a dancer: Music-driven motion synthesis with
global structure. arXiv , 2021. 2
[7]Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and
G¨ul Varol. TEACH: Temporal Action Compositions for 3D
Humans. In International Conference on 3D Vision (3DV) ,
2022. 3
[8]Norman I Badler, Cary B Phillips, and Bonnie Lynn Web-
ber.Simulating humans: computer graphics animation and
control . Oxford University Press, 1993. 2
[9]Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan:
Probabilistic 3d human motion prediction via gan. In Pro-
ceedings of the Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW) , 2018. 2
[10] Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek Baner-
jee, Pooja Guhan, Aniket Bera, and Dinesh Manocha.
Text2gestures: A transformer-based network for generating
emotive body gestures for virtual agents. In Virtual Reality
and 3D User Interfaces (VR) , 2021. 5, 6
[11] Arij Bouazizi, Adrian Holzbock, Ulrich Kressel, Klaus Diet-
mayer, and Vasileios Belagiannis. Motionmixer: Mlp-based
3d human body pose forecasting. In International Joint Con-
ference on Artificial Intelligence (IJCAI) , 2022. 2
[12] Judith Butepage, Michael J Black, Danica Kragic, and Hedvig
Kjellstrom. Deep representation learning for human motion
prediction and classification. In Proceedings of the Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2017. 2
[13] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-
Chen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster:
choreography-oriented music-driven dance synthesis. ACM
Transactions on Graphics (TOG) , 2021. 2
[14] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell
Stander, Eric Hallahan, Louis Castricato, and Edward Raff.
Vqgan-clip: Open domain image generation and editing withnatural language guidance. In Proceedings of the European
Conference on Computer Vision (ECCV) , 2022. 4
[15] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook
Kim, Alec Radford, and Ilya Sutskever. Jukebox: A genera-
tive model for music. arXiv , 2020. 2
[16] Sander Dieleman, Aaron van den Oord, and Karen Simonyan.
The challenge of realistic music generation: modelling raw
audio at scale. In Advances in Neural Information Processing
Systems (NeurIPS) , 2018. 2
[17] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke,
Ali Thabet, and Artsiom Sanakoyeu. Avatars grow legs: Gen-
erating smooth human motion from sparse tracking inputs
with diffusion model. In Proceedings of the Conference on
Computer Vision and Pattern Recognition (CVPR) , 2023. 2
[18] Yinglin Duan, Tianyang Shi, Zhengxia Zou, Yenan Lin, Zhe-
hui Qian, Bohan Zhang, and Yi Yuan. Single-shot motion
completion with transformer. arXiv , 2021. 2
[19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In Pro-
ceedings of the Conference on Computer Vision and Pattern
Recognition (CVPR) , 2021. 2, 4
[20] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jiten-
dra Malik. Recurrent network models for human dynamics.
InProceedings of the International Conference on Computer
Vision (ICCV) , 2015. 2
[21] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian
Theobalt, and Philipp Slusallek. Synthesis of compositional
animations from textual descriptions. In Proceedings of the
International Conference on Computer Vision (ICCV) , 2021.
1, 3, 5, 6
[22] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the Conference
on Computer Vision and Pattern Recognition (CVPR) , 2022.
1, 2, 3, 4, 5, 6, 7, 8, 12, 13
[23] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t:
Stochastic and tokenized modeling for the reciprocal gener-
ation of 3d human motions and texts. In Proceedings of the
European Conference on Computer Vision (ECCV) , 2022. 1,
3, 5, 6
[24] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tion2motion: Conditioned generation of 3d human motions.
InProceedings of the ACM International Conference on Mul-
timedia (ACMMM) , 2020. 5
[25] Wen Guo, Yuming Du, Xi Shen, Vincent Lepetit, Xavier
Alameda-Pineda, and Francesc Moreno-Noguer. Back to
mlp: A simple baseline for human motion prediction. In
Proceedings of the Winter Conference on Applications of
Computer Vision (WACV) , 2022. 2
[26] Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, Joe
Yearsley, and Taku Komura. A recurrent variational autoen-
coder for human motion synthesis. In Proceedings of the
British Machine Vision Conference (BMVC) , 2017. 2
[27] F´elix G Harvey and Christopher Pal. Recurrent transition
networks for character locomotion. In SIGGRAPH Asia 2018
Technical Briefs , 2018. 2[28] F´elix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and
Christopher Pal. Robust motion in-betweening. ACM Trans-
actions on Graphics (TOG) , 2020. 2
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceedings
of the Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2016. 4
[30] Alejandro Hernandez, Jurgen Gall, and Francesc Moreno-
Noguer. Human motion prediction via spatio-temporal in-
painting. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR) , 2019. 2
[31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. Gans trained by a two time-
scale update rule converge to a local nash equilibrium. In Ad-
vances in Neural Information Processing Systems (NeurIPS) ,
2017. 7
[32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In Advances in Neural Information
Processing Systems (NeurIPS) , 2020. 2
[33] Daniel Holden, Taku Komura, and Jun Saito. Phase-
functioned neural networks for character control. ACM Trans-
actions on Graphics (TOG) , 2017. 3
[34] Daniel Holden, Jun Saito, and Taku Komura. A deep learning
framework for character motion synthesis and editing. ACM
Transactions on Graphics (TOG) , 2016. 3
[35] Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece,
Remo Ziegler, and Otmar Hilliges. Convolutional autoen-
coders for human motion infilling. In International Confer-
ence on 3D Vision (3DV) , 2020. 2
[36] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. In International Conference on Learning Repre-
sentations (ICLR) , 2014. 2
[37] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun
Wang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz. Danc-
ing to music. In Advances in Neural Information Processing
Systems (NeurIPS) , 2019. 2, 5, 6
[38] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang,
Sanja Fidler, and Hao Li. Learning to generate diverse dance
motions with transformer. arXiv , 2020. 2
[39] Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and
Olga Sorkine-Hornung. Ganimator: Neural motion synthesis
from a single sequence. ACM Transactions on Graphics
(TOG) , 2022. 3
[40] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa.
Ai choreographer: Music conditioned 3d dance generation
with aist++. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR) , 2021. 2
[41] Siyao Li, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,
Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando:
3d dance generation by actor-critic gpt with choreographic
memory. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR) , 2022. 2, 4
[42] Angela S Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qix-
ing Huang, and Raymond J Mooney. Generating animated
videos of human activities from natural language descrip-
tions. In Advances in Neural Information Processing Systems
(NeurIPS) , 2018. 5, 6[43] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. ACM transactions on graphics (TOG) ,
2015. 6
[44] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations (ICLR) , 2019. 6
[45] Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and
Gr´egory Rogez. Posegpt: Quantization-based 3d human
motion generation and forecasting. In Proceedings of the
European Conference on Computer Vision (ECCV) , 2022. 2
[46] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black. Amass: Archive of
motion capture as surface shapes. In Proceedings of the In-
ternational Conference on Computer Vision (ICCV) , 2019.
5
[47] Christian Mandery, ¨Omer Terlemez, Martin Do, Nikolaus
Vahrenkamp, and Tamim Asfour. The kit whole-body human
motion database. In International Conference on Robotics
and Automation (ICRA) , 2015. 5
[48] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. History
repeats itself: Human motion prediction via motion attention.
InProceedings of the European Conference on Computer
Vision (ECCV) , 2020. 2
[49] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong
Li. Learning trajectory dependencies for human motion pre-
diction. In Proceedings of the International Conference on
Computer Vision (ICCV) , 2019. 2
[50] Julieta Martinez, Michael J Black, and Javier Romero. On
human motion prediction using recurrent neural networks.
InProceedings of the Conference on Computer Vision and
Pattern Recognition (CVPR) , 2017. 2
[51] Dario Pavllo, David Grangier, and Michael Auli. Quater-
net: A quaternion-based recurrent model for human motion.
InProceedings of the British Machine Vision Conference
(BMVC) , 2018. 2
[52] Mathis Petrovich, Michael J. Black, and G ¨ul Varol. Action-
conditioned 3D human motion synthesis with transformer
V AE. In Proceedings of the International Conference on
Computer Vision (ICCV) , 2021. 1, 3
[53] Mathis Petrovich, Michael J. Black, and G ¨ul Varol. TEMOS:
Generating diverse human motions from textual descriptions.
InProceedings of the European Conference on Computer
Vision (ECCV) , 2022. 1, 3, 5, 6
[54] Matthias Plappert, Christian Mandery, and Tamim Asfour.
The kit motion-language dataset. Big data , 2016. 2, 3, 5, 6, 7,
13
[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
InInternational Conference on Machine Learning (ICML) ,
2021. 1, 3, 4
[56] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. In Advances in Neural Information Pro-
cessing Systems (NeurIPS) , 2018. 2, 4[57] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image genera-
tion with clip latents. arXiv , 2022. 4
[58] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning (ICML) , 2021. 2
[59] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generat-
ing diverse high-fidelity images with vq-vae-2. In Advances
in Neural Information Processing Systems (NeurIPS) , 2019.
4
[60] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the Conference on Computer Vision and Pattern Recognition
(CVPR) , 2022. 2
[61] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Singan:
Learning a generative model from a single natural image.
InProceedings of the Conference on Computer Vision and
Pattern Recognition (CVPR) , 2019. 3
[62] Sebastian Starke, Ian Mason, and Taku Komura. Deepphase:
periodic autoencoders for learning motion phase manifolds.
ACM Transactions on Graphics (TOG) , 2022. 3
[63] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito.
Neural state machine for character-scene interactions. ACM
Transactions on Graphics (TOG) , 2019. 3
[64] Xiangjun Tang, He Wang, Bo Hu, Xu Gong, Ruifan Yi, Qi-
long Kou, and Xiaogang Jin. Real-time controllable motion
transition for characters. ACM Transactions on Graphics
(TOG) , 2022. 2
[65] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,
and Daniel Cohen-Or. Motionclip: Exposing human motion
generation to clip space. In Proceedings of the European
Conference on Computer Vision (ECCV) , 2022. 1, 3, 4
[66] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H
Bermano, and Daniel Cohen-Or. Human motion diffusion
model. arXiv , 2022. 2, 3, 5, 6, 7, 8
[67] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz. Mocogan: Decomposing motion and content for video
generation. In Proceedings of the Conference on Computer
Vision and Pattern Recognition (CVPR) , 2018. 5, 6
[68] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. In Advances in Neural Information
Processing Systems (NeurIPS) , 2017. 2, 3, 4
[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural
Information Processing Systems (NeurIPS) , 2017. 2, 4, 6
[70] Will Williams, Sam Ringer, Tom Ash, David MacLeod, Jamie
Dougherty, and John Hughes. Hierarchical quantized au-
toencoders. In Advances in Neural Information Processing
Systems (NeurIPS) , 2020. 2
[71] Chen Xin, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, Jingyi Yu, and Gang Yu. Executing your commands
via motion diffusion in latent space. arXiv , 2022. 5, 6
[72] Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, and
Dahua Lin. Convolutional sequence generation for skeleton-based action synthesis. In Proceedings of the International
Conference on Computer Vision (ICCV) , 2019. 2
[73] Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan
Sunkavalli, Eli Shechtman, Sunil Hadap, Ersin Yumer, and
Honglak Lee. Mt-vae: Learning motion transformations to
generate multimodal human dynamics. In Proceedings of the
European Conference on Computer Vision (ECCV) , 2018. 2
[74] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong,
Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-
driven human motion generation with diffusion model. arXiv ,
2022. 2, 3, 5, 6, 7, 8Num. layers Num. dim Num. heads FID ↓ Top-1↑ Training time (hours).
4 512 8 0.469±.0140.469±.00217
8 512 8 0.339±.0100.481±.00223
8 768 8 0.338±.0090.490±.00330
8 768 12 0.296±.0090.484±.00231
12 768 12 0.273±.0070.487±.00240
12 1024 16 0.149±.0070.489±.00255
16 768 12 0.145±.0060.486±.00347
16 1024 16 0.143±.0070.490±.00459
18 768 12 0.130±.0060.483±.00351
18 1024 16 0.141±.0050.492±.00378
Table 4. Ablation study of T2M-GPT architecture on HumanML3D [22] test set. For all the architectures, we use the same motion
VQ-V AE. The T2M-GPT is trained with τ∈ U[0,1]. The training time is evaluated on a single Tesla V100-32G GPU.
Appendix
In this appendix, we present:
• Section A: ablation study of T2M-GPT architecture.
•Section B: ablation study of the reconstruction loss ( Lre
in Equation [3]) for motion VQ-V AE.
•Section C: ablation study of τfor the corruption strategy
in T2M-GPT training.
•Section D: ablation study of the number of codes in
VQ-V AE.
•Section E: more details on the evaluation metrics and
the motion representations.
•Section F: the detail of the Motion VQ-V AE architec-
ture.
• Section G: limitations of our proposed approach.
• Section H: more funding information.
A. Ablation study of T2M-GPT architecture
In this section, we present results with different trans-
former architectures for T2M-GPT. The results are provided
in Table 4. We notice that better performance can be obtained
with a larger architecture. We finally leverage an 18-layer
transformer with 16 heads and 1,024 dimensions.
B. Impact of the reconstruction loss in motion
VQ-V AE
In this section, we study the effect of the reconstruction
loss (Lrein Equation [3]) and the hyper-parameter α(Equa-
tion [3]). The results are presented in Table 5. We find that
L1 Smooth achieves the best performance on reconstruction,Lcons αReconstruction
FID↓ Top-1 (%)
L1 0 0.095±.0010.493±.002
L1 0.5 0.144±.0010.495±.003
L1 1 0.160±.0010.496±.003
L1Smooth 0 0.112±.0010.496±.003
L1Smooth 0.5 0.070±.0010.501±.002
L1Smooth 1 0.128±.0010.499±.003
L2 0 0.321±.0020.478±.003
L2 0.5 0.292±.0020.483±.002
L2 1 0.213±.0020.490±.003
Table 5. Ablation of losses for VQ-VAE on HumanML3D [22]
test set. We report FID and Top1 metric for the models trained
300K iterations.
and the performance of L1 loss is close to L1 Smooth loss.
For the hyper-parameter α, we find that α= 0.5leads to the
best performance.
C. Impact of τfor the corruption strategy in
T2M-GPT training
τ FID↓ Top-1↑ MM-Dist ↓
0.0 0 .140±.0060.417±.0033.730±.009
0.1 0 .131±.0050.453±.0023.357±.007
0.3 0 .147±.0060.485±.0023.157±.007
0.5 0.116±.0040.491±.0033.118±.011
0.7 0 .155±.0060.480±.0043.183±.011
U[0,1] 0 .141±.0050.492±.0033.121±.009
Table 6. Analysis of τon HumanML3D [22] test set.
In this section, we study τ, which is used for corrupt-
ing sequences during the training of T2M-GPT. The results
are provided in Table 6. We can see that the training with
corrupted sequences τ= 0.5significantly improves over
Top-1 accuracy and FID compared to τ= 0. ComparedNum. codeReconstruction
FID↓ Top-1 (%)
256 0.145±.0010.497±.002
512 0.070±.0010.501±.002
1024 0.090±.0010.498±.003
Table 7. Study on the number of code in codebook on Hu-
manML3D [22] test set.
toτ∈ U[0,1],τ= 0.5is probably preferable for Hu-
manML3D [22], as it achieves comparable Top-1 accuracy
compared to τ∈ U[0,1]but with much better FID.
D. Ablation study of the number of codes in
VQ-V AE
We investigate the number of codes in the codebook in
Table 7. We find that the performance of 512 codes is slightly
better than 1,024 codes. The results show that 256 codes are
not sufficient for reconstruction.
E. More details on the evaluation metrics and
the motion representations.
E.1. Evaluation metrics
We detail the calculation of several evaluation metrics,
which are proposed in [22]. We denote ground-truth motion
features, generated motion features, and text features as fgt,
fpred, and ftext. Note that these features are extracted with
pretrained networks in [22].
FID. FID is widely used to evaluate the overall quality of
the generation. We obtain FID by
FID=∥µgt−µpred∥2−Tr(Σgt+ Σpred−2(ΣgtΣpred)1
2)
(6)
where µgtandµpred are mean of fgtandfpred.Σis the
covariance matrix and Tr denotes the trace of a matrix.
MM-Dist. MM-Dist measures the distance between the
text embedding and the generated motion feature. Given
N randomly generated samples, the MM-Dist measures the
feature-level distance between the motion and the text. Pre-
cisely, it computes the average Euclidean distances between
each text feature and the generated motion feature as follows:
MM-Dist =1
NNX
i=1∥fpred,i−ftext,i∥ (7)
where fpred,i andftext,i are the features of the i-th text-
motion pair.Dilation rateReconstruction
FID↓ Top-1 (%)
1, 1, 1 0.145±.0010.500±.003
4, 2, 1 0.138±.0010.502±.002
9, 3, 1 0.070±.0010.501±.002
16, 4, 1 57.016±.0840.032±.001
Table 8. Ablation study of different dilation rate in VQ-V AE on
HumanML3D [22] test set.
Diversity. Diversity measures the variance of the whole
motion sequences across the dataset. We randomly sample
Sdispairs of motion and each pair of motion features is de-
noted by fpred,i andf′
pred,i . The diversity can be calculated
by
Diversity =1
SdisSdisX
i=1||fpred,i−f′
pred,i|| (8)
In our experiments, we set Sdisto 300 as [22].
MModality. MModality measures the diversity of human
motion generated from the same text description. Precisely,
for the i-th text description, we generate motion 30 times
and then sample two subsets containing 10 motion. We
denote features of the j-th pair of the i-th text description by
(fpred,i,j ,f′
pred,i,j ). The MModality is defined as follows:
MModality =1
10NNX
i=110X
j=1∥fpred,i,j −f′
pred,i,j ∥(9)
E.2. Motion representations
We use the same motion representations as [22]. Each
pose is represented by ( ˙ra,˙rx,˙rz, ry, jp, jv, jr, cf), where
˙ra∈Ris the global root angular velocity; ˙rx∈R,˙rz∈R
are the global root velocity in the X-Z plan; jp∈R3j, jv∈
R3j, jr∈R6jare the local pose positions, velocity and
rotation with j the number of joints; cf∈R4is the foot
contact features calculated by the heel and toe joint velocity.
F. VQ-V AE Architecture
We illustrate the detailed architecture of VQ-V AE in Ta-
ble 9. The dimensions of the HumanML3D [22] and KIT-
ML [54] datasets feature are 263 and 259 respectively.
Dilation rate. We investigate the impact of different dila-
tion rates of the convolution layers used in VQ-V AE, and
the results are presented in Table 8 for reconstruction. We
notice that setting the dilation rate as (9, 3, 1) gives the most
effective and stable performance.G. Limitations
Our approach has two limitations: i)for excessively long
texts, the generated motion might miss some details of the
textual description. Note that this typical failure case exists
for all competitive approaches. ii)some generated motion
sequences slightly jitter on the legs and hands movement, this
can be seen from the visual results provided in the appendix.
We think the problem comes from the VQ-V AE architecture,
with a better-designed architecture, the problem might be
alleviated. For a real application, the jittering problem could
be addressed using a temporal smoothing filter as a post-
processing step.
H. Funding Support
This work is supported by:
• Natural Science Foundation of China (No. 62176155).
•Natural Science Foundation of Jilin Province
(20200201037JC).
•Provincial Science and Technology Innovation Special
Fund Project of Jilin Province (20190302026GX)
•Shanghai Municipal Science and Technology Major
Project (2021SHZDZX0102)Components Architecture
VQ-V AE Encoder (0): Conv1D( Din, 512, kernel size=(3,), stride=(1,), padding=(1,))
(1): ReLU()
(2): 2×Sequential(
(0): Conv1D(512, 512, kernel size=(4,), stride=(2,), padding=(1,))
(1): Resnet1D(
(0): ResConv1DBlock(
(activation1): ReLU()
(conv1): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(9,), dilation=(9,))
(activation2): ReLU()
(conv2): Conv1D(512, 512, kernel size=(1,), stride=(1,)))
(1): ResConv1DBlock(
(activation1): ReLU()
(conv1): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(3,), dilation=(3,))
(activation2): ReLU()
(conv2): Conv1D(512, 512, kernel size=(1,), stride=(1,)))
(2): ResConv1DBlock(
(activation1): ReLU()
(conv1): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(1,))
(activation2): ReLU()
(conv2): Conv1D(512, 512, kernel size=(1,), stride=(1,)))))
Codebook nn.Parameter((512, 512), requires grad=False)
VQ-V AE Decoder (0): 2 ×Sequential(
(0): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(1,))
(1): Resnet1D(
(0): ResConv1DBlock(
(activation1): ReLU()
(conv1): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(9,), dilation=(9,))
(activation2): ReLU()
(conv2): Conv1D(512, 512, kernel size=(1,), stride=(1,)))
(1): ResConv1DBlock(
(activation1): ReLU()
(conv1): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(3,), dilation=(3,))
(activation2): ReLU()
(conv2): Conv1D(512, 512, kernel size=(1,), stride=(1,)))
(2): ResConv1DBlock(
(activation1): ReLU()
(conv1): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(1,))
(activation2): ReLU()
(conv2): Conv1D(512, 512, kernel size=(1,), stride=(1,)))))
(2): Upsample(scale factor=2.0, mode=nearest)
(3): Conv1D(512, 512, kernel size=(3,), stride=(1,), padding=(1,))
(1): ReLU()
(2): Conv1D(512, Din, kernel size=(3,), stride=(1,), padding=(1,))
Table 9. Architecture of our Motion VQ-V AE.