Efﬁciency 360: Efﬁcient Vision Transformers
Badri N. Patro1,Vijay Srinivas Agneeswaran2
12Microsoft
fbadripatro, vagneeswaran g@microsoft.com
Abstract
Transformers are widely used for solving tasks
in natural language processing, computer vision,
speech, and music domains. In this paper, we
talk about the efﬁciency of transformers in terms
of memory (the number of parameters), compu-
tation cost (number of ﬂoating points operations),
and performance of models, including accuracy, ro-
bustness of the model, and fair & bias-free features.
We mainly discuss the vision transformer for the
image classiﬁcation task. Our contribution is to in-
troduce an efﬁcient 360 framework, which includes
various aspects of the vision transformer, to make
it more efﬁcient for industrial applications. By
considering those applications, we categorize them
into multiple dimensions such as privacy, robust-
ness, transparency, fairness, inclusiveness, contin-
ual learning, probabilistic models, approximation,
computational complexity, and spectral complex-
ity. We compare various vision transformer models
based on their performance, the number of param-
eters, and the number of ﬂoating point operations
(FLOPs) on multiple datasets.
1 Introduction
Transformers such as attention all you need [Vaswani et al. ,
2017 ]and Bidirectional Encoder Representations from Trans-
formers (BERT) [Kenton and Toutanova, 2019 ]have recently
become popular in the Machine Learning world for Natural
Language Processing (NLP) tasks such as machine transla-
tion, text summarization, question answering, protein fold
prediction, and even image processing tasks. ChatGPT [gpt,
2022 ]based transformer model and other large language
models have garnered public attention in the last few weeks.
They are used to assist humans in various ways, including
answering questions, generating articles, and even as a cod-
ing assistant, which is helpful for both industrial applications
and academia. This work focuses on Vision transformers
and their application in various industrial dimensions. A few
dimensions that are important from an industry perspective
of these advanced models include robustness, privacy, trans-
parency, inclusiveness, and continual and distributed learn-
ing.
Figure 1: Efﬁcient Vision Transformers: Venn diagram of the
efﬁcient transformer models (Efﬁciency-360). This includes the
robustness of a model, privacy of the model, bias and fairness,
transparency, inclusiveness, efﬁcient learning, probabilistic models,
model approximations, the computational complexity of a model,
and spectral complexity techniques
In literature, Tay et al. [Tayet al. , 2020c ]have discussed
efﬁcient transformers primarily by considering natural lan-
guage processing-based transformers. In this article, we build
on a survey of efﬁcient transformers in the vision domain,
which has a different characterization compared to the exist-
ing surveys. We include more recent work on advanced trans-
formers (especially those published in 2021 and 2022) in our
current survey. Interesting research directions open up as a
result, which we discuss in later sections of this article. As
we shall be discussing, the survey opens up research in in-
clusiveness and privacy. It also suggests that the advanced
transformer models work on high-resolution data, opening
up research in climate modeling and oceanography (wave-
breaking kind of applications). The other contribution of this
paper is the efﬁcient 360 framework, which helps provide a
holistic view of transformers across these dimensions.arXiv:2302.08374v3  [cs.CV]  23 Feb 2023Model Position
EmbeddingAttention
TypeNetwork
ArchitectureAttention
NetworkStructure Extra
Label
ViT[Dosovitskiy et al. , 2020 ] 3 Global MSA Linear Isotropic 7
DeiT [Touvron et al. , 2021a ] 3 Global MSA Linear Isotropic 7
TNT [Han et al. , 2021 ] 3 Global MSA Linear Isotropic 7
T2T [Yuan et al. , 2021 ] 3 Global MSA Linear Isotropic 7
Cross-ViT [Chen et al. , 2021a ] 3 Global MSA Linear Isotropic 7
PVT [Wang et al. , 2021 ] 3 Global MSA Linear Pyramid 7
Swin [Liuet al. , 2021b ] 3 Local MSA Linear Pyramid 7
Twin [Chu et al. , 2021 ] 3 LG MSA Linear Pyramid 7
CiT[Renet al. , 2022 ] 3 Global MSA Linear Isotropic 7
CSwin [Dong et al. , 2022 ] 3 LG MSA Linear Pyramid 7
LV-ViT [Jiang et al. , 2021 ] 3 Global MSA Linear Isotropic 3
WaveViT [Yaoet al. , 2022 ] 3 Global MSA Linear Isotropic 3
CMT [Guo et al. , 2022b ] 3 LG MSA Linear Isotropic 7
RegionViT [Chen et al. , 2022a ] 3 LR MSA Linear Pyramid 7
CoaT [Xuet al. , 2021a ] 3 Local MSA Linear Pyramid 7
CvT [Wuet al. , 2021 ] 7 LG MSA Convolution Pyramid 7
UniFormer [Liet al. , 2022a ] 3 LG MHRA Linear Pyramid 7
MLP-Mixer [Tolstikhin et al. , 2021 ]3 Global* Mixer 7 Isotropic 7
AS-MLP [Lian et al. , 2022 ] 3 Local* Mixer 7 Isotropic 7
CycleMLP [Chen et al. , 2022b ] 3 Global* Mixer 7 Isotropic 7
PoolFormer [Yuet al. , 2022b ] 3 Global* Pooler 7 Isotropic 7
GFNet [Raoet al. , 2021 ] 3 Global* SGN 7 Isotropic 7
GFNet-H [Raoet al. , 2021 ] 3 Global* SGN 7 Pyramid 7
AFNO [Guibas et al. , 2022 ] 3 Global* SGN 7 Isotropic 7
Table 1: We compare transformer models based on position Embedding, token embedding, network Architecture, attention network, Hierar-
chical Type, and extra labels. to comparison. SGN stands to be a Spectral Gating Network, and MSA stands for a Multi-headed Self Attention
network. MHRA stands for Multi-Head Relation Aggregator. LG stands for Local + Global. LR stands for Local + Regional. ”*” indicate
that it’s not an attention-type network.
Figure 2: Inference on efﬁcient transformer model (DeiT).
In this work, we start our survey by considering a clas-
siﬁcation task. The transformer model needs to classify a
given image into 1000 pre-deﬁned classes. To benchmark
the model performance, the research community chooses the
ImageNet-1K [Deng et al. , 2009 ]dataset. We provide sample
examples of the given image of a leopard and classify it into
its classes as shown in ﬁgure-2. We select DeiT-base [Tou-
vron et al. , 2021a ]as a pre-train transformer model and ob-
tain its prediction index & probability scores. We visual-
ize the Grad-CAM-based explanation for the prediction. We
start with a discussion of how efﬁcient DeiT [Touvron et al. ,
2021a ]is over ViT [Dosovitskiy et al. , 2020 ]. Here we de-
ﬁne efﬁciency in terms of model parameters, computational
cost ( i.e., training, inference time,) and performance (Top-1accuracy). Along with this, we discuss various dimensions
for efﬁcient models like bias-free models, robust features,
privacy in the model, transparency in model, efﬁcient way
of learning, easily deploy-able (inclusiveness), and efﬁcient
architecture(self-Attention, MLP-Mixer, and Spectral mod-
els).
In the ﬁgure-1, we have included the major categories
of efﬁcient transformers: computational complexity, spec-
tral complexity, robustness, privacy, approximation, efﬁcient
learning, transparency, fairness, and inclusiveness. We re-
view each in turn in the subsequent sections. Due to the page
constraint, our high-resolution ﬁgures, plots, and details com-
parison of state-of-the-art models are available on our GitHub
page.1
We compare transformer models based on the position Em-
bedding, token embedding, network Architecture, attention
network, Hierarchical Type, and extra labels in table-1. We
check whether position embedding is used in the transformer
or not and the type of token embedding in the transformer
model, like overlapping or non-overlapping. We consider var-
ious types of the core architecture of the transformer models
like Multi-Headed Self Attention, MLP-mixer-based archi-
tecture, and Spectral Gating Networks( using learnable ﬁlter
weights). Also, we check with types of attention networks
like linear layer or Convolution Neural Networks for QKV .
1https://github.com/badripatro/efﬁcient360Figure 3: This ﬁgure shows a comparison of various transformer model architectures. In general, the transformer architecture divides two
parts one token mixing and another one channel mixing. We show various token-mixing model architectures. It is an extended version of the
diagram with spectral mixing shown in PoolFormer [Yuet al. , 2022b ].
Hierarchical architecture is used in the transformer model.
We also compare with models that used extra data for train-
ing like Wave-ViT [Yao et al. , 2022 ]and LV-ViT [Jiang et
al., 2021 ]. SGN tends to Spectral Gating Network, and MSA
tends to a Multi-headed Self Attention network.
Figure-3 shows the general architecture of transformer
models in the vision domain. It contains two main parts To-
ken mixing and channel mixing. The ﬁgure shows various ef-
ﬁcient token-mixing techniques, Such as attention-type token
mixing, MLP-mixture-based token mixing, pooling-based to-
ken mixing, and spectral mixing techniques. The channel
mixing techniques are also similar for all types of transform-
ers.
2 Efﬁcient 360 Framework
The growing size of the neural networks results in improved
model performance. As model size increases, which results
in an increase in memory consumption and computational re-
quirements (for storing weights, activations, and gradients)
while training, those models also increase. Now the chal-
lenge is, how do we design efﬁcient transformer models for a
visual domain?
2.1 Computational complexity
These transformers address the O(N2)computational com-
plexity of transformers in various ways. One of the critical
issues in a transformer is its quadratic complexity concerning
the input sequence length—along dimensions relating to both
computation and memory. The implication is that one has tocompute the NNattention matrix for every layer and at-
tention head. Various approaches have been tried to reduce
thisO(N2)complexity, including the use of caching archi-
tectures. The sparse transformer is one of the popular meth-
ods to address this complexity. Each output position com-
putes weights from a subset of input positions. If the subset
isp
(N), then the complexity of the transformer reduces to
O(Np
(N)), allowing it to handle long-range dependen-
cies.
We start with Vision Transformer (ViT) [Dosovitskiy et al. ,
2020 ]model considers the image a 16 x16 word and is used
to classify the image into predeﬁned categories. In the ViT
model, each image is split into a sequence of tokens of ﬁxed
length and then applied to multiple transformer layers to cap-
ture the global relationship across the token for the classiﬁ-
cation task. However, the performance of ViT is lower than
CNNs on the ImageNet dataset when we train from scratch.
Yuan et al. [Yuan et al. , 2021 ]ﬁnd the reason for the perfor-
mance degradation of the ViT model as 1. The importance
of local structures, such as lines and edges among the neigh-
boring pixels, is not captured by the simple tokenization of
the input image. This leads to low training sample efﬁciency.
2. The redundancy of the attention module of the ViT leads
to limited feature richness and limited training samples for
a ﬁxed computation budget. To overcome these issues of
the ViT model, Yuan et al. [Yuan et al. , 2021 ]proposed a
Tokens-to-token ViT method for training vision transformers
from scratch on ImageNet. The proposed method includes
layer-wise tokens to the token transformation that structures
the image to the token by recursively aggregating neighbor-Figure 4: This ﬁgure shows the performance of Various models
across different model architectures like Tiny (T), Small (S), Base
(B), and Large (L). This plot shows the variation of top-1 accuracy
based on the various architectures like T, S, B, and L. We plot a
number of parameters(M) for each architecture vs. its top-1 perfor-
mance.
ing tokens to one token (Tokens-to-token). This captures the
local structure represented by the surrounding tokens, and the
token’s length is also reduced. The method also includes a
deep narrow structure as an efﬁcient backbone for the vision
transformer. Transformer iN Transformer (TNT) [Han et al. ,
2021 ]ﬁnds an issue in the local patch to capture color infor-
mation and complex object details in the vision transformer.
The author has proposed the TNT method, which considers
16x16 words as a visual sentence and further divided into
small patches of 4x4 as the visual words. This makes boosts
transformer performance by 1.7% compared to the state-of-
the-art method.
Touvron et al. [Touvron et al. , 2021a ]proposed an
efﬁcient transformer model based on distillation technique
(DeiT). It uses a teacher-student strategy that relies on a dis-
tillation token to ensure that a student learns from a teacher
through attention. Bao et al. [Bao et al. , 2021 ]have pro-
posed a masked image model task for a pretrained vision
transformer. The author proposes a self-supervision–based
vision representation model, Bidirectional Encoder represen-
tation from Image Transformers (BEiT), which follows the
BERT [Kenton and Toutanova, 2019 ]method developed for
the Natural Language Processing area. In this method, each
image is considered as two views: One of the image patches
of size 16 x 16 pixels and the other of discrete visual tokens.
The original image is tokenized into visual tokens, with some
image patches randomly masked and then fed to the backbone
pre-trained transformer. After training BEiT, the model can
be ﬁne-tuned for the downstream tasks.
Touvron et al. [Touvron et al. , 2021b ]have studied archi-
tectural optimization in the image transformer model. The
author builds a Class attention image Transformer(CaiT) and
optimizes a deeper transformer network for the image classi-
ﬁcation task. The CaiT method has two main contributions;
one is LayerScale, which is the multiplication of a diagonal
matrix with one output of each residual block, and the second
one is Class attention, which is a set of layers that compile a
collection of patch embedding into class embedding and fed
to the linear classiﬁer.Chen et al. have proposed a transformer model Cross-
ViT [Chen et al. , 2021a ]to combine image patches, i.e.,
tokens in transformer, of different sizes to produce strong
feature representation using dual branch transformers. One
transformer branch handles small patch tokens, and the other
transformer handles large patch tokens with two separate
branches of different computational complexity. Single to-
ken for each branch as a query to exchange information with
other branches using effective token fusing module on cross
attention. It is efﬁcient in terms of a number of FLOPS and
model parameters.
Wang et al. [Wang et al. , 2021 ]have proposed a Pyramid
vision Transformer (PVT) for dense prediction without con-
volutions. Vision-based transformers encounter difﬁculties
while porting these transformers to dense prediction tasks.
The PVT overcomes this issue. PVT is helpful for pixel-level
dense predictions without convolution and non-maximal sup-
pression, such as object detection methods. It is easy to port
transformers using progressive shrinking pyramids and spa-
tial reduction attention. Finally, PVT is evaluated on vari-
ous tasks such as image classiﬁcation, object detection, in-
stance, and semantic segmentation tasks. Liu et al. [Liuet
al., 2021b ]have discussed the issue of adapting transform-
ers from the language domain to the visual domain in ways
that encompass a large variance of visual entities and high-
resolution pixels of images compared to words in the text.
To address this issue, the author proposed Swin Transformer
[Liuet al. , 2021b ], a hierarchical transformer method whose
representation is computed using shifted windows. This tech-
nique overcomes the issue of non-overlapping local windows
of self-attention more efﬁciently.
Transformer models provide high representation power
compare to CNN-based models. But these models are not
good at dense prediction due to excessive memory, compu-
tational cost, and feature, which can inﬂuence the irrever-
ent part of the image. To avoid these issues, the sparse at-
tention method proposed in PVT [Wang et al. , 2021 ]and
Swin [Liuet al. , 2021b ]transformers are data agnostic and
limit the long-range relationship. To solve this issue de-
formable self-attention method is proposed, where the posi-
tion of the value and key pair is selected in a data-dependent
way. The technique is called Deformable Attention Trans-
former (DAT) [Xiaet al. , 2022 ].
Chu et al. [Chu et al. , 2021 ]have discussed the im-
portance of spatial attention for success in the transformer’s
performance on various tasks. The authors proposed two
simple and efﬁcient architectures such as Twins-PCPVT and
Twins-SVT. This paper uses a separable depth-wise convo-
lution attention mechanism known as spatial-separable self-
attention (SSSA). SSSA uses two types of attention oper-
ations: Locally grouped self-attention (LSA) and globally
sub-sampled attention (GSA). LSA deals with ﬁne-grained
and short-distance information, while GSA deals with long-
distance sequences and global information. The second pro-
posed method, Twins-SVT, uses LSA and GSA with matrix
multiplication. The author compares Twins-PCPVT with the
similar architecture PVT [Wang et al. , 2021 ], and Twins-
SVT with similar architecture Swin [Liuet al. , 2021b ]trans-
former. This makes it more efﬁcient in terms of performance.Figure 5: This ﬁgure shows the performance of various state-of-the-art vision transformer models across a number of parameters. We plot
the number of parameters(M) of different transformer models vs. their top-1 performance on the ImageNet-1K dataset. From this ﬁgure, we
conclude that those models which are towards top-left are the best efﬁcient models. For example WaveViT-L ˜citeyao2022wave is the best
efﬁcient model.
CvT [Wuet al. , 2021 ]is an efﬁcient transformer model
which introduces convolution token embedding and convo-
lution transformer block in ViT architecture. The Convolu-
tion Neural Networks are good at shift, scale, and distortion
invariant to ViT architecture, which maintains the merits of
transformer architecture by using dynamic attention, global
context, and better generalization. CvT gets all the bene-
ﬁts from CNNs, like a local receptive ﬁeld, shared weights,
and spatial sub-sampling, along with keeping all the advan-
tages of transformer models. It improves performance com-
pared to CNNs-based models ResNet [Heet al. , 2016 ]and
transformer-based models ViT [Dosovitskiy et al. , 2020 ]and
DeiT [Touvron et al. , 2021a ]. CvT is also more efﬁcient re-
garding the number of FLOPS and parameters. We have an-
alyzed model architecture-wise comparison for CvT, DeiT,
PVT, TNT, and T2T transformer models as shown in the
ﬁgure-4.
Xu et al. have proposed a transformer model CoaT [Xuet
al., 2021a ]for image classiﬁcation tasks trained with co-scale
and conv-attention mechanisms. The co-scale mechanism al-
lows learning representation at different scales to communi-
cate effectively with each other. The conv-attention mecha-
nism realizes relative position embedding with convolutions
in the factorized attention module, which is computationally
efﬁcient. Gua et al. have proposed a CMT [Guo et al. , 2022b ]
transformer based on a hybrid network, which has the advan-
tage of the transformer capturing long-range dependency and
convolution neural network to extract local information. It is
efﬁcient in the number of FLOPS and the network’s perfor-
mance.
MLP-mixer [Tolstikhin et al. , 2021 ]model use multi-layerperceptions (MLPs) to mix input token without using CNNs
or self-attention network. It uses two types of layers one
MLP layer to combine the location feature of image patches
and the other one for mixing spatial information. The MLP-
mixer models achieve competitive scores on image classiﬁ-
cation benchmarks. Lian et al. proposed axial shifted MLP
(AS-MLP) [Lian et al. , 2022 ]network for vision tasks. AS-
MLP pays more attention to local feature interaction by us-
ing axial shifted channels of the feature maps. It captures
local dependency by capturing the ﬂow of the information
in axial directions, as in CNNs. MLP-mixer [Tolstikhin et
al., 2021 ]and ResMLP [Touvron et al. , 2022 ]ﬂattened the
input image patches and fed them into the transformer en-
coder to mix the input tokens linearly using MLP networks.
It is hard to capture the spatial information in the image. To
avoid this issue, Guo et al. have proposed a Hire-MLP [Guo
et al. , 2022c ]network which contains MLP architecture via
Hierarchical rearrangements in two levels. The inner region
rearrangement captures local information inside the spatial
regions, and cross-region rearrangement enables the commu-
nication between two regions. It captures global context by
circularly shifting all tokens along the spatial direction. MLP-
mixer [Tolstikhin et al. , 2021 ], ResMLP [Touvron et al. ,
2022 ]and gMLP [Liuet al. , 2021a ]architecture highly de-
pend on the image size. These MLP models are in quadratic
O(N2) computational complexity in nature, which are infea-
sible for object detection and semantic segmentation task. To
solve this issue, Chen et al. [Chen et al. , 2022b ]has pro-
posed a CycleMLP network, which mainly it can cope with
variable image size and provides linear complexity with im-
age size using local windows. Recently MLP based architec-Image
SizeMethod Network
Type#Param
(M)FLOPS
(G)ImageNet
top-1 (%)Real top-
1 (%)Imagenet-
v2
ResNet-50 [Heet al. , 2016 ] C 25 4.1 76.2 82.5 63.3
2242ResNet-101 [Heet al. , 2016 ] C 45 7.9 77.4 83.7 65.7
ResNet-152 [Heet al. , 2016 ] C 60 11 78.3 84.1 67.0
RedNet-50 [Liet al. , 2021b ] I 15.5 - 78.4 - -
2242RedNet-101 [Liet al. , 2021b ] I 25.6 - 79.1 - -
RedNet-152 [Liet al. , 2021b ] I 34 - 79.3 -
DeiT-S [Touvron et al. , 2021a ] T 22 4.6 79.8 85.7 68.5
DeiT-B [Touvron et al. , 2021a ] T 86 17.6 81.8 86.7 71.5
PVT-Small [Wang et al. , 2021 ] T 25 3.8 79.8 – –
PVT-Medium [Wang et al. , 2021 ] T 44 6.7 81.2 – –
PVT-Large [Wang et al. , 2021 ] T 61 9.8 81.7 – –
Cross-ViT-S [Chen et al. , 2021a ] T 26.7 5.6 81.0 - -
Cross-ViT-B [Chen et al. , 2021a ] T 104.7 21.2 82.2 - -
T2T-ViTt-14 [Yuan et al. , 2021 ] T 22 6.1 80.7 – –
T2T-ViTt-19 [Yuan et al. , 2021 ] T 39 9.8 81.4 – –
T2T-ViTt-24 [Yuan et al. , 2021 ] T 64 15.0 82.2 – –
2242TNT-S [Han et al. , 2021 ] T 24 5.2 81.3 – –
TNT-B [Han et al. , 2021 ] T 66 14.1 82.8 – –
CiT-Ti [Renet al. , 2022 ] T 6 - 75.3 - -
CiT-S [Renet al. , 2022 ] T 22 - 82.0 - -
Visformer [Chen et al. , 2021b ] T 40.2 4.9 82.3 - -
Swin-S [Liuet al. , 2021b ] T 50 8.7 83.2 - -
Swin-B [Liuet al. , 2021b ] T 88 15.4 83.5 - -
LIT-Ti [Panet al. , 2022b ] T 19 3.6 81.1 86.6 70.4
LIT-S [Panet al. , 2022b ] T 27 4.1 81.5 86.4 70.4
LIT-M [Panet al. , 2022b ] T 48 8.6 83.0 87.3 72.0
LIT-B [Panet al. , 2022b ] T 86 15.0 83.4 87.6 72.8
LITv2-S [Panet al. , 2022a ] T 28 3.7 82.0 - -
LITv2-M [Panet al. , 2022a ] T 49 7.5 83.3 - -
LITv2-B [Panet al. , 2022a ] T 87 13.2 83.6 - -
Twins-PCPVT-B [Chu et al. , 2021 ] T 43.8 6.7 82.7 - -
Twins-SVT-B [Chu et al. , 2021 ] T 56 8.6 83.2 - -
Twins-PCPVT-L [Chu et al. , 2021 ] T 60.9 9.8 83.1 - -
Twins-SVT-L [Chu et al. , 2021 ] T 99.2 15.1 83.7 - -
ViL-Small [Zhang et al. , 2021b ] T 24.6 4.9 82.4 - -
ViL-Medium [Zhang et al. , 2021b ] T 39.7 8.7 83.5 - -
ViL-Base [Zhang et al. , 2021b ] T 55.7 13.4 83.7 - -
RegionViT-Ti+ [Chen et al. , 2022a ] T 14.3 2.7 81.5 - -
RegionViT-M+ [Chen et al. , 2022a ] T 42.0 7.9 83.4 - -
RegionViT-B [Chen et al. , 2022a ] T 72.7 13.0 83.2 - -
RegionViT-B+ [Chen et al. , 2022a ] T 73.8 13.6 83.8 - -
DAT-T [Xiaet al. , 2022 ] T 29 4.6 82.0 - -
DAT-S [Xiaet al. , 2022 ] T 50 9.0 83.7 - -
DAT-B [Xiaet al. , 2022 ] T 88 15.8 84.0 - -
SE-CoTNetD-50 [Liet al. , 2022c ] T 23.1 4.1 81.6 - -
SE-CoTNetD-101 [Liet al. , 2022c ] T 40.9 8.5 83.2 - -
SE-CoTNetD-152 [Liet al. , 2022c ] T 55.8 17.0 84.0 - -
ViL-LS-Medium [Zhuet al. , 2021 ] T 39.8 8.7 83.8 - -
ViL-LS-Base [Zhuet al. , 2021 ] T 55.8 13.4 84.1 - -
UniNet-B1 [Liuet al. , 2022 ] CT 11.5 1.1 80.8 - -
CoaT-Lite-Mini [Xuet al. , 2021a ] CT 10 6.8 80.8 - -
CoaT-Lite-Small [Xuet al. , 2021a ] CT 20 4.0 81.9 - -
CvT-13 [Wuet al. , 2021 ] CT 20 4.5 81.6 86.7 70.4
CvT-21 [Wuet al. , 2021 ] CT 32 7.1 82.5 87.2 71.3
LV-ViT- S[Jiang et al. , 2021 ] CT 26 6.6B 83.3 88.1 -
LV-ViT- M[Jiang et al. , 2021 ] CT 56 16.0 84.1 88.4 -
Table 2: Performance of the various models on ImageNet1K [Deng et al. , 2009 ], ImageNet Real [Beyer et al. , 2020 ]and ImageNet V2
matched frequency [Recht et al. , 2019 ]with image size 2242. We report these numbers from CvT [Wuet al. , 2021 ]paper. C stands for
Convolution, I stands for Involution, T tends for Transformer, and CT stands for Convolution Transformers. means extra data used while
training the model.Image
SizeMethod Network
Type#Param
(M)FLOPS
(G)ImageNet
top-1 (%)Real top-
1 (%)Imagenet-
v2
CSwin-T [Dong et al. , 2022 ] T 23 4.3 82.7 - -
CSwin-S [Dong et al. , 2022 ] T 35 6.9 83.6 - -
CSwin-B [Dong et al. , 2022 ] T 78 15.0 84.2 - -
DaViT-Ti [Ding et al. , 2022 ] T 28.3 4.5 82.8 - -
DaViT-S [Ding et al. , 2022 ] T 49.7 8.8 84.2 - -
DaViT-B [Ding et al. , 2022 ] T 87.9 15.5 84.6 - -
CaiT-M36 [Touvron et al. , 2021b ] T 271 53.7 85.1 89.3 -
MViTv2-T [Liet al. , 2022b ] T 24 4.7 82.3 - -
MViTv2-S [Liet al. , 2022b ] T 35 7 83.6 - -
MViTv2-B [Liet al. , 2022b ] T 52 10.2 84.4 - -
MViTv2-L [Liet al. , 2022b ] T 218 42.1 85.3 - -
Wave-ViT- S[Yaoet al. , 2022 ] T 22.7 4.7 83.9 - -
Wave-ViT- B[Yaoet al. , 2022 ] T 33.5 7.2 84.8 - -
Wave-ViT- L[Yaoet al. , 2022 ] T 57.5 14.8 85.5 - -
CMT-Ti [Guo et al. , 2022b ] CT 9.5 0.6 79.1 - -
CMT-XS [Guo et al. , 2022b ] CT 15.2 1.5 81.8 - -
CMT-S [Guo et al. , 2022b ] CT 25.1 4 83.5 - -
CMT-B [Guo et al. , 2022b ] CT 45.7 9.3 84.5 - -
CMT-L [Guo et al. , 2022b ] CT 74.7 19.5 84.8 - -
MaxViT-T [Tuet al. , 2022 ] CT 31 5.6 83.62 - -
MaxViT-S [Tuet al. , 2022 ] CT 69 11.7 84.45 - -
MaxViT-B [Tuet al. , 2022 ] CT 120 23.4 84.95 - -
MaxViT-L [Tuet al. , 2022 ] CT 212 43.9 85.17 - -
UniFormer-S [Liet al. , 2022a ] CT 22 3.6 82.9 - -
UniFormer-B [Liet al. , 2022a ] CT 50 8.3 83.9 - -
UniFormer-L [Liet al. , 2022a ] CT 100 12.6 85.6 - -
Table 3: This is an extension of table- 2, which shows performance of the various models on ImageNet1K [Deng et al. , 2009 ], ImageNet Real
[Beyer et al. , 2020 ]and ImageNet V2 matched frequency [Recht et al. , 2019 ]with image size 2242. We report these numbers from CvT [Wu
et al. , 2021 ]paper. C stands for Convolution, I stands for Involution, T tends for Transformer, and CT stands for Convolution Transformers.
means extra data used while training the model.
ture based on fully connected layers of archives competing for
performance like CNN and transformers. We split images as
multiple tokens(patches) and directly aggregated them with
ﬁxed weights by neglecting varying semantic information of
tokens from different images. To handle this issue, Tang et
al. have proposed Wave-MLP [Tang et al. , 2022 ]method in
which each token has two parts, amplitude, and phase part.
The amplitude is the original feature, and the phase terms are
a complex value chaining according to the image’s semantic
content by modulating the relation between tokens and ﬁxed
weights.
Recently the attention-based module of the transformer
is replaced by the spatial MLPs, as shown in the ﬁgure-
3. These MLP-based models are performing well com-
pared to attention-based models. Generally, a transformer
architecture-speciﬁc token mixing module is necessary for
the model’s performance. Yu et al. have proposed simple
methods to replace the attention module of the transformer
with a simple spatial pooling operator to conduct only basic
token mixing, and this method is known as PoolFormer.
TopFormer: As we know, the computational cost of a vi-
sion transformer is very high for dense prediction tasks like
semantic segmentation tasks on mobile networks. Zhang et
al.[Zhang et al. , 2022 ]have proposed a mobile-friendly ar-
chitecture such as a Token Pyramid vision transformer (Top-Former) for the dense prediction task. The method consists of
a Token pyramid module semantic extractor, injection mod-
ule, and segmentation head on it. The token pyramid module
takes an image as an image put and produces a token pyramid,
which is fed to the semantic extractor to produce scale-aware
semantics, which is injected into a token of the corresponding
scale for augmenting representation using the injection mod-
ule. Finally, The augmented token pyramid to perform the
segmentation task using a segmentation head.
Dong et al. have proposed a Cross-Shaped Window trans-
former model CSWin [Dong et al. , 2022 ]to compute self-
attention in horizontal and vertical strips in parallel to form
cross-shape windows. The main difference from vanilla vi-
sion transformers are 1.) CSWin transformer replaces multi-
headed self-attention with Cross Shaped Window Self Atten-
tion. 2.) it introduces local inductive bias using Locally-
enhanced Positional Encoding (LePE), which is added par-
allel to the self-attention module. The CSWin transformer is
efﬁcient in terms of accuracy and good representation feature
for downstream tasks.
RegionViT [Chen et al. , 2022a ]has proposed a new archi-
tecture that adapts a pyramid structure to capture regional-
to-local attention rather than global attention for the vision
transformer. The method ﬁrst generates regional tokens and
local tokens from images of different patch sizes. RegionalImage
SizeMethod Network
Type#Param
(M)FLOPS
(G)ImageNet
top-1 (%)Real top-
1 (%)Imagenet-
v2
Mixer-B/16 [Tolstikhin et al. , 2021 ] M 59 12.7 76.4 - -
gMLP-S [Liuet al. , 2021a ] M 20 4.5 79.6 - -
gMLP-B [Liuet al. , 2021a ] M 73 15.8 81.6 - -
ResMLP-S12 [Touvron et al. , 2022 ] M 15 3.0 76.6 - -
ResMLP-S24 [Touvron et al. , 2022 ] M 30 6.0 79.4 - -
ResMLP-B24 [Touvron et al. , 2022 ] M 116 23.0 81.0 - -
S2-MLP-wide [Yuet al. , 2022a ] M 71 14.0 80.0 - -
S2-MLP-deep [Yuet al. , 2022a ] M 51 10.5 80.7 - -
ViP-Small/7 [Hou et al. , 2022 ] M 25 6.9 81.5 - -
ViP-Medium/7 [Hou et al. , 2022 ] M 55 16.3 82.7 - -
ViP-Large/7 [Hou et al. , 2022 ] M 88 24.4 83.2 - -
CycleMLP-B1 [Chen et al. , 2022b ] M 15 2.1 78.9 - -
CycleMLP-B2 [Chen et al. , 2022b ] M 27 3.9 81.6 - -
CycleMLP-B3 [Chen et al. , 2022b ] M 38 6.9 82.4 - -
CycleMLP-B4 [Chen et al. , 2022b ] M 52 10.1 83.0 - -
CycleMLP-B5 [Chen et al. , 2022b ] M 76 12.3 83.2 - -
AS-MLP-T [Lian et al. , 2022 ] M 28 4.4 81.3 - -
AS-MLP-S [Lian et al. , 2022 ] M 50 8.5 83.1 - -
AS-MLP-B [Lian et al. , 2022 ] M 88 15.2 83.3 - -
Wave-MLP-T [Tang et al. , 2022 ] M 1 2.4 80.6 - -
2242Wave-MLP-S [Tang et al. , 2022 ] M 30 4.5 82.6 - -
Wave-MLP-M [Tang et al. , 2022 ] M 44 7.9 83.4 - -
Wave-MLP-B [Tang et al. , 2022 ] M 63 10.2 83.6 - -
Hire-MLP-Tiny [Guo et al. , 2022c ] M 18 2.1 79.7 - -
Hire-MLP-Small [Guo et al. , 2022c ] M 33 4.2 82.1 - -
Hire-MLP-Base [Guo et al. , 2022c ] M 58 8.1 83.2 - -
Hire-MLP-Large [Guo et al. , 2022c ] M 96 13.4 83.8 - -
DynaMixer-S [Wang et al. , 2022c ] M 26 7.3 82.7 - -
DynaMixer-M [Wang et al. , 2022c ] M 57 17.0 83.7 - -
DynaMixer-L [Wang et al. , 2022c ] M 97 27.4 84.3 - -
PoolFormer-S12 [Yuet al. , 2022b ] P 12 1.9 77.2 - -
PoolFormer-S24 [Yuet al. , 2022b ] P 21 3.5 80.3 - -
PoolFormer-S36 [Yuet al. , 2022b ] P 31 5.1 81.4 - -
PoolFormer-M36 [Yuet al. , 2022b ] P 56 9.0 82.1 - -
PoolFormer-M48 [Yuet al. , 2022b ] P 73 11.8 82.5 - -
Table 4: This table shows the performance of the various MLP-like Transformer models on ImageNet1K [Deng et al. , 2009 ], ImageNet Real
[Beyer et al. , 2020 ]and ImageNet V2 matched frequency [Recht et al. , 2019 ]with image size 2242. We report these numbers from CvT
[Wuet al. , 2021 ]paper. M stands for MLP-Mixer, and P stands for Pooling Network. means extra data used while training the model.
self-attention captures global information among all regional
tokens, and local self-attention exchanges the information
among one regional token and associates with the local to-
ken via self-attention.
UniFormer [Liet al. , 2022a ]model is a more generaliz-
ing model considering both Convolution and Self-attention
for the Visual Recognition task. As CNNs based models
are more efﬁcient for decreasing local redundancy by con-
volution within a small neighborhood, the limited receptive
ﬁeld makes it hard to capture global dependency. Whereas
transformer-based models can effectively capture long-range
dependency via self-attention while ﬁnding similarity com-
parisons among all the tokens lead to high redundancy.
The UniFormer model uses Multi-Head Relation Aggrega-
tor(MHRA) blocks, which are modeled with local and global
tokens afﬁnity in the shallow and deep layer, respectively, to
reduce redundancy and provide an efﬁcient representing fea-
ture. The main difference in the UniFormer model comparewith the transformer is it uses MHRA, while the transformer
uses MSA (Multi-headed Self Attention).
The scalability of self-attention is restricted to the image
size. That is, the computation complexity increase as im-
age size increases. To avoid this issue, Tu et al. proposed
the MaxVit [Tuet al. , 2022 ]method to efﬁciently scale self-
attention. The technique contains two concepts, one block
local and dilated global attention. The model provides local-
global spatial interaction on various input image resolutions
with linear complexity. MViTv2 [Liet al. , 2022b ]has im-
proved the performance of Multi-scale Vision Transformers
for Classiﬁcation and Detection tasks. The MViTv2 method
decomposed location distance to inject the position informa-
tion to the transformer blocks and residual pooling connec-
tions to composite the effect of pooling in the attention com-
putation.
Pan et al. have proposed an efﬁcient method (LIT) [Pan
et al. , 2022b ], which pays less attention to the self-attentionImage
SizeMethod Network
Type#Param
(M)FLOPS
(G)ImageNet
top-1 (%)Real top-
1 (%)Imagenet-
v2
6002EfﬁcientNet-B7 [Tan and Le, 2019 ] C 43 19 84.3 - -
2562BoTNet–S1-128 [Srinivas et al. , 2021 ] T 79.1 19.3 84.2 - -
UniNet-B2 [Liuet al. , 2022 ] CT 16.2 2.2 82.5 - -
CMT-B [Guo et al. , 2022b ] CT 45.7 9.3 84.5 - -
CMT-L [Guo et al. , 2022b ] CT 74.7 19.5 84.8 - -
UniNet-B3 [Liuet al. , 2022 ] CT 24 4.3 83.5 - -
2882LV-ViT- L[Jiang et al. , 2021 ] CT 150 59.0 85.3 89.3 -
ViT-B/16 [Dosovitskiy et al. , 2020 ] T 86 55.5 77.9 83.6 –
ViT-L/16 [Dosovitskiy et al. , 2020 ] T 307 191.1 76.5 82.2 –
DeiT-B [Touvron et al. , 2021a ] T 86 55.5 83.1 - -
Swin-B [Liuet al. , 2021b ] T 88 47.1 84.5 - -
ViL-LS-Medium [Zhuet al. , 2021 ] T 39.9 28.7 84.4 - -
LITv2-B [Panet al. , 2022a ] T 87 39.7 84.7 - -
DAT-B [Xiaet al. , 2022 ] T 88 49.8 84.8 - -
3842BoTNet-S1-128 [Srinivas et al. , 2021 ] T 79.1 45.8 84.7 - -
CaiT-S36 [Touvron et al. , 2021b ] T 68 48.0 85.4 89.8 -
CSwin-T [Dong et al. , 2022 ] T 23 14.0 84.3 - -
CSwin-S [Dong et al. , 2022 ] T 35 22.0 85.0 - -
CSwin-B [Dong et al. , 2022 ] T 78 47.0 85.4 - -
CvT-13 [Wuet al. , 2021 ] CT 20 16.3 83.0 87.9 71.9
CvT-21 [Wuet al. , 2021 ] CT 32 24.9 83.3 87.7 71.9
UniNet-B5 [Liuet al. , 2022 ] CT 72.9 20.4 84.9 - -
LV-ViT- S[Jiang et al. , 2021 ] CT 26 22.2 84.4 88.9 -
LV-ViT- M[Jiang et al. , 2021 ] CT 56 42.2 85.4 89.5 -
UniFormer-S [Liet al. , 2022a ] CT 22 11.9 84.6 - -
UniFormer-B [Liet al. , 2022a ] CT 50 27.2 86.0 - -
UniFormer-L [Liet al. , 2022a ] CT 100 39.2 86.3 - -
MaxViT-S [Tuet al. , 2022 ] CT 69 36.1 85.74 - -
MaxViT-B [Tuet al. , 2022 ] CT 120 74.2 86.34 - -
MaxViT-L [Tuet al. , 2022 ] CT 212 133.1 86.40 - -
4482CaiT-M36 [Touvron et al. , 2021b ] T 271 247.8 86.3 90.2 -
UniNet-B6 [Liuet al. , 2022 ] CT 117 51 85.6 - -
LV-ViT- L[Jiang et al. , 2021 ] CT 150 157.2 85.9 89.7 -
LV-ViT- L[Jiang et al. , 2021 ] CT 151 214.8 86.4 90.1 -
MaxViT-T [Tuet al. , 2022 ] CT 31 33.7 85.72 - -
5122MaxViT-S [Tuet al. , 2022 ] CT 69 67.6 86.19 - -
MaxViT-B [Tuet al. , 2022 ] CT 120 138.5 86.66 - -
MaxViT-L [Tuet al. , 2022 ] CT 212 245.4 86.70 - -
Table 5: Performance of the various models on ImageNet1K [Deng et al. , 2009 ], ImageNet Real [Beyer et al. , 2020 ]and ImageNet V2
matched frequency [Recht et al. , 2019 ]for various image sizes. We report these numbers from CvT [Wuet al. , 2021 ]paper. C stands for
Convolution, I stands for Involution, T stands for Transformer, and CT stands for Convolution Transformers. means extra data used while
training the model.
module in the vision Transformer. The early self-attention
layers focus on local patterns and provide minor beneﬁts
in hierarchal vision transformers. Pan et al. further im-
prove the LIT model with hilo attention [Panet al. , 2022a ]
method to make a fast vision transformer. Venkataramanan
et al. [Venkataramanan et al. , 2023 ]have proposed a skip-
attention method to improve vision transformers by paying
less attention. Similarly, GC-ViT [Hatamizadeh et al. , 2022 ]
and Beit [Bao et al. , 2022 ]proposed various attention mech-
anisms to improve the performance of the vision transformer
on ImageNet-1K [Deng et al. , 2009 ]dataset.
We compare top-1 accuracy (%) of various transformermodels for image size 2242on the ImageNet dataset over
model parameter in millions(M) as shown in the ﬁgure-5.
2.2 Spectral complexity
Efﬁcient transformers can be designed to speed up trans-
former encoder architecture by replacing the self-attention
network with linear transformations that mix input tokens.
The self-attention layer of the transformer is replaced by a pa-
rameterized Fourier transformation (Fnet) [Lee-Thorp et al. ,
2021 ], which is then followed by a non-linearity and feed-
forward network. Compared to BERT, this network is 80 per-
cent faster and can archive 92 to 97 percent of the transformerImage
SizeMethod Network
Type#Param
(M)FLOPS
(G)ImageNet
top-1 (%)Real top-
1 (%)Imagenet-
v2
4802BiT-M [Kolesnikov et al. , 2020 ] T 928 837 85.4 – –
ViT-B/16 [Dosovitskiy et al. , 2020 ] T 86 55.5 84.0 88.4 –
ViT-L/16 [Dosovitskiy et al. , 2020 ] T 307 191.1 85.2 88.4 –
3842ViT-H/16 [Dosovitskiy et al. , 2020 ] T 632 – 85.1 88.7 –
CvT-13 [Wuet al. , 2021 ] CT 20 16 83.3 88.7 72.9
CvT-21 [Wuet al. , 2021 ] CT 32 25 84.9 89.8 75.6
CvT-W24 [Wuet al. , 2021 ] CT 277 193.2 87.7 90.6 78
Table 6: Performance of the various transformer models pre-trained on ImageNet21K [Deng et al. , 2009 ]and ﬁne-tuned on ImageNet21K
[Deng et al. , 2009 ]ImageNet Real [Beyer et al. , 2020 ]and ImageNet V2 matched frequency [Recht et al. , 2019 ]for image size 384384,
except BiT-M [Kolesnikov et al. , 2020 ], which is ﬁne-tuned on image size 480480. We report these numbers from CvT [Wuet al. , 2021 ]
paper. T tends for Transformer, CT tends for Convolution Transformers.
Image
SizeMethod #Param
(M)FLOPS
(G)ImageNet
top-1 (%)Attention
NetworkExtra
Label
Fnet [Lee-Thorp et al. , 2021 ] 15 2.9 71.2 7 7
GFNet-Ti [Raoet al. , 2021 ] 7 1.3 74.6 7 7
2242GFNet-XS [Raoet al. , 2021 ] 16 2.9 78.6 7 7
GFNet-S [Raoet al. , 2021 ] 25 4.5 80.0 7 7
GFNet-B [Raoet al. , 2021 ] 43 7.9 80.7 7 7
AFNO-S/4 [Guibas et al. , 2022 ]16 15.3 80.9 7 7
Wave-ViT-S [Yaoet al. , 2022 ] 19.8 4.3 82.7 3 7
Wave-ViT-S[Yaoet al. , 2022 ] 22.7 4.7 83.9 3 3
Wave-ViT-B[Yaoet al. , 2022 ]33.5 7.2 84.8 3 3
Wave-ViT-L[Yaoet al. , 2022 ]57.5 14.8 85.5 3 3
GFNet-XS [Raoet al. , 2021 ] 18 8.4 80.6 7 7
3842GFNet-S [Raoet al. , 2021 ] 28 13.2 81.7 7 7
GFNet-B [Raoet al. , 2021 ] 47 23.3 82.1 7 7
Table 7: This table shows the performance analysis of spectral transformer models trained on ImageNet1K [Deng et al. , 2009 ]for image
size224224and384384. It compares the number of parameters, number of FLOPs, and Top-1 accuracy of various spectral vision
transformer models. The red tick mark in the attention column indicates that the attention network is not used in the transformer model. 
indicates that WaveViT [Yaoet al. , 2022 ]uses extra training data while training.
performance. The Global Frequency network (GFnet) [Rao
et al. , 2021 ]proposes a depth-wise global convolution for to-
ken mixing. GFnet involves three steps: Spatial token mix-
ing via Fast Fourier Transform (FFT), frequency gating, and
inverse FFT for token demixing. GFnet is not involved in
channel mixing, is expensive for higher solution images as
sequence length increases, and is not adaptive. Guibias et al.
[Guibas et al. , 2022 ]formulated the token mixing task as an
operator-learning task that learns mapping among continuous
functions in inﬁnite dimensional space. Li et al. [Liet al. ,
2020 ]discuss solving Partial Differential Equations (PDE)
using a Fourier Neural Operator (FNO). FNO works well in
continuous domains. Adapting FNO for a vision domain with
high-resolution image inputs requires modiﬁcation in the de-
sign architecture of FNO from PDE. This is because high im-
ages have discontinuities due to edges and other structures.
Additionally, the channel mixing FNO depends on the chan-
nel size, which has quadratic complexity. The block-diagonal
structure is used on channel mixing weight to handle this
channel mixing issue. The author shared weights across the
tokens of MLP layers for parameter efﬁciency and introduced
sparsity in the frequency domain using soft thresholding for
generalization. These solutions combine, known as Adap-tive Fourier neural Operator (AFNO). In Wave-ViT [Yao et
al., 2022 ], the author has discussed the quadratic complexity
of the self-attention network of the transformer model with
input patch numbers. People have used downsampling oper-
ations using global average polling (GAP) over key/values
to solve this issue in the past. It is observed that down-
sampling operations such as (GAP) are non-invertible, which
causes losing high frequency, such as texture details of the ob-
jects. The author has proposed a wavelet vision transformer
to perform lossless downsampling using wavelet transform
over keys and values. The model performs state-of-the-art
results on image recognition, object detection, and instance
segmentation tasks. The model is efﬁcient in terms of the
number of FLOPS and accuracy. Compared to GFNet [Rao
et al. , 2021 ]and AFNO [Guibas et al. , 2022 ], WaveVit [Yao
et al. , 2022 ]uses an attention network with extra labels for
training. We compare all sorts of spectral networks in table-
7. Another recent work in the spectral domain is Fourier
Image transformer (FIT) [Buchholz and Jug, 2022 ], which
uses Fourier domain encoding for image completion tasks,
predicting high-resolution output given low-resolution input.
This method is demonstrated in computer tomography (CT)
image reconstruction tasks.Method #FLOPS
(G)#Param
(M)CIFAR
10CIFAR
100Pet Flower Cars
ViT-B/16 [Dosovitskiy et al. , 2020 ] 55.4 86 98.1 87.1 - 89.5 -
ViT-L/16 [Dosovitskiy et al. , 2020 ] 190.7 307 97.9 86.4 - 89.7 -
DeiT-B [Touvron et al. , 2021a ] 17.6 85.8 99.1 90.8 - 98.4 92.1
TNT-S ↑384[Han et al. , 2021 ] 17.3 23.8 98.7 90.1 94.7 98.8 -
CaiT-S ↑384[Touvron et al. , 2021b ] 12.9 24.2 99.1 90.8 94.9 98.6 94.1
GFNet-XS [Raoet al. , 2021 ] 2.9 16 98.6 89.1 - 98.1 92.8
GFNet-H-B [Raoet al. , 2021 ] 8.6 54 99.0 90.3 - 98.8 93.2
CMT-S [Guo et al. , 2022b ] 4.04 25.1 99.2 91.7 95.2 98.7 94.4
RegionViT-S [Chen et al. , 2022a ] - - 98.9 90.0 95.3 - 92.8
RegionViT-M [Chen et al. , 2022a ] - - 99.0 90.8 95.5 - 91.9
BiT-M [Kolesnikov et al. , 2020 ] - 928 98.91 92.17 94.46 99.30 -
ViT-B/16 [Dosovitskiy et al. , 2020 ] - 86 98.95 91.67 94.43 99.38 -
ViT-L/16 [Dosovitskiy et al. , 2020 ] - 307 99.16 93.44 94.73 99.61 -
ViT-H/16 [Dosovitskiy et al. , 2020 ] - 632 99.27 93.82 94.82 99.51 -
CvT-13 [Wuet al. , 2021 ] - 20 98.83 91.11 93.25 99.50 -
CvT-21 [Wuet al. , 2021 ] - 32 99.16 92.88 94.03 99.62 -
CvT-W24 [Wuet al. , 2021 ] - 277 99.39 94.09 94.73 99.72
Table 8: Transfer Learning performance on CIFAR10 [Krizhevsky and others, 2009 ], CIFAR100 [Krizhevsky and others, 2009 ], Pet [Parkhi
et al. , 2012 ], Flower [Nilsback and Zisserman, 2008 ]and Cars [Krause et al. , 2013 ]dataset. We reported top-1 accuracy, Number of FLOPS,
and parameters for various transformer models on these datasets. The top block of the table indicates, models are pre-trained on ImageNet 1k
[Deng et al. , 2009 ]and the bottom block indicates, the models are pre-trained on ImageNet 22k [Deng et al. , 2009 ].
2.3 Bias and Fairness Features
This section discusses inductive bias and how efﬁciently we
can train our transformer models. We start with Inductive
Bias (IB) and introduce ViTAE [Xuet al. , 2021b ]work and
its variants. ViTAEv2 [Zhang et al. , 2023 ]lack intrinsic in-
ductive bias (IB) in modeling local visual structure. The
model requires massive training data and time to learn in-
ductive bias implicitly. The author has proposed ViTAEv2
method based on the ViT transformer model to explore Induc-
tive intrinsic bias from convolutions. In this method, the au-
thor uses spatial reduction modules to downsample and feed
the input images into tokens using multiple convolutions with
different dilation rates. This model is based on ViTAE [Xuet
al., 2021b ]. Edaelman et al. [Edelman et al. , 2022 ]has dis-
cussed the theoretical analysis of the Inductive biases of the
self-attention module. The author shows that the bounded-
norm transformer model creates sparse variables, and the sin-
gle multi-head attention can represent the sparse function of
the input sequence. Ren et al. [Ren et al. , 2022 ]have dis-
cussed on induction bias of the vision transformer, which is
not performing well with insufﬁcient data. The author has
introduced a knowledge distillation (KD) based method to
help the training of the transformer. The existing methods
in KD use heavy convolution neural network-based teacher
modules, but in this work, the author uses lightweight mod-
ules with different architectural inductive biases as teacher
modules (such as CNN-based teacher module and Involution-
based teacher module) to co-advise the student transformer
model. The author claims that Co-advise based transformer
model is more efﬁcient in training and provides better perfor-
mances compared to existing methods.
2.4 Transparency
Transparency in the Transformer model indicates a clear un-
derstanding of the transformer model. Basically, we need toknow the details of this model like, what is the transformer
model?, How do we train a transformer model? How do
we make inferences from those models? How can we ex-
plain the inference of the model? We would also like to
know more details about the model, such as multi-headed
self-attention and positional encoding do? In order to under-
stand the transformer model, we need to know the details.
Finally, this will help us to design a more efﬁcient transpar-
ent transformer model. Few recent research has been car-
ried out in the ﬁeld of transparency in the model architecture
and training processing. Park et al. [Park and Kim, 2021 ]
have analyzed the vision transformer model and explained
”How Do Vision Transformers Work?” with certain exam-
ples. The author has analyzed the loss surface for the vi-
sion transformers’ multi-headed self-attention (MSA). They
have also analyzed the behavior of convolution neural net-
work (CNN) and MSA, which seems to be opposite to each
other, for example, MSAs are low pass ﬁlter and CNNs are
high pass ﬁlters. This provides transparency while model-
ing multi-headed self-attention. Raghu et al. [Raghu et al. ,
2021 ]have analyzed ”how are vision transformers solving
image classiﬁcation like convolution networks?” They study
the structure of transformers and CNN and found the ViT
are uniform representations across all the layers. The role
of self-attention enables early aggregation of global informa-
tion, and the role of residual connection in ViT propagates
features from lower to higher layers. We also consider efﬁ-
ciency in terms of training time and convergence of the model
in terms of loss surface. LV-ViT [Jiang et al. , 2021 ]has pro-
posed a different training objective for the vision transform-
ers that compute classiﬁcation loss with additional trainable
class tokens. Location-speciﬁc, each patch token is generated
by a machine using NFNet [Brock et al. , 2021 ]image recog-
nition model. The method reformulates the image classiﬁ-
cation problem into a multi-token-level recognition problemModel mCE(%) mFR(%) mT5D(%)cAcc(%) Top-1
(%)Top-1
(%)Top-1
(%)
ResNet-50 76.70 58.00 82.00 22.30 25.25 2.21 16.76
BiT m-r101x3 58.27 49.99 76.71 03.78 27.25 6.41 28.19
ViT L-16 45.45 33.06 50.15 20.02 40.58 28.10 73.73
Table 9: [Paul and Chen, 2022 ]mCEs of different models and methods on ImageNet-C (lower is better). MFRs and mT5Ds on ImageNet-P
dataset (lower is better). cAcc tends to challenge accuracy. The cAcc column shows performance on detecting vulnerable image foregrounds
from the ImageNet-9 dataset. Columns 6,7, and 8 show top-1 accuracy scores (as percentages) on ImageNet-R, A, and O datasets respectively.
Method ImageNet ImageNet-C mCE ( ↓)
ViT-B/16 81.43 58.85 51.98
ViT-L/16 82.89 64.11 45.46
Mixer-B/16 76.47 47.00 67.35
Mixer-L/16 71.77 40.47 75.84
RN18 69.76 32.92 84.67
RN50 76.13 39.17 76.70
Table 10: This table reports top-1 scores and mCE score of various
models on ImageNet [Deng et al. , 2009 ], ImageNet-C [Hendrycks
and Dietterich, 2018 ]dataset.
by assigning each patch token with machine-generated su-
pervised location-speciﬁc tokens. The method considers all
image patch tokens to compute training loss in a dense man-
ner using machine-supervised token labeling. DeiT [Touvron
et al. , 2021a ]and AugReg [Steiner et al. , 2022 ]have dis-
cussed data augmentation methods and regularization meth-
ods to train Vision Transformers more efﬁciently.
2.5 Robustness
Robustness in the transformer is studied in terms of pertur-
bation, common corruption, distributional shift, and natural
adversarial examples. Shao et al. [Shao et al. , 2021 ]ana-
lyzed the robustness of the transformer model using adver-
sarial perturbation. The authors experimented with a white
box and a transformer attack setting. They observe that ViT
has better adversarial robustness compared to Convolutional
Neural Networks (CNNs). They ﬁnd that ViT features con-
tain low-level information that provides superior robustness
against adversarial attacks. They note the combination of
CNNs and transformers leads to better robustness compared
to pure transformer models with increasing size or added lay-
ers. Additionally, they ﬁnd that pretraining larger datasets
do not improve robustness. For a robust model, the opposite
is applicable. Bhojanapalli et al. [Bhojanapalli et al. , 2021 ]
investigated various measures of the robustness of ViT mod-
els and resnet models against adversarial examples, natural
examples, and common corruptions. The authors have inves-
tigated robustness to perturbation both to the input and to the
model. It is observed that transformers are robust to remove
any single layer from either the input or the model.
Paul et al. [Paul and Chen, 2022 ]studied various aspects
of robust learning methods of ViT [Dosovitskiy et al. , 2020 ],
CNNs, and Big transformer [Kolesnikov et al. , 2020 ]meth-
ods. Paul et al. [Paul and Chen, 2022 ]benchmarked the
robustness of ViTs on a wide range of ImageNet datasets.
Their results are in table-9. Through six experiments, the au-
thors veriﬁed that ViT has improved in robustness comparedto CNN and BIG transformers. The results of those experi-
ments include Experiment 1: attention is crucial for improved
robustness; Experiment 2: the important role of pretraining;
Experiment 3: ViT has better robustness to image masking;
Experiment 4: Fourier spectrum analysis reveals low sensitiv-
ity for ViT; Experiment 5: adversarial perturbation has spread
wider in the energy spectrum; and Experiment 6: ViT has
smoother Loss Landscape to input perturbations. Hendrycks
et al. [Hendrycks and Dietterich, 2018 ]has been introduced
for benchmarking neural network robustness against com-
mon corruptions like natural occurring and corruptions using
ImageNet-C [Hendrycks and Dietterich, 2018 ]dataset. The
ImageNet-C dataset contains perturb version of the original
ImageNet dataset. There are 1000 classes and each has 50
images. The performance of various models is shown in the
table-10.
ViT [Dosovitskiy et al. , 2020 ]models are less effective at
capturing the high-frequency component of images as com-
pared to CNNs, as investigated by Park et al. [Park and
Kim, 2021 ]. HAT [Baiet al. , 2022 ]was the result of a fur-
ther investigation into the effect of an existing transformer
model from a frequency perspective. HAT perturbs the high-
frequency component of the input image with noise using the
RandAugment method. Wu et al. [Wuet al. , 2022 ]inves-
tigated the issue of transformer models vulnerable to adver-
sarial examples like CNNs. This issue is handled in CNNs
with the help of adversarial training, which is the most ef-
fective way to accomplish it in CNNs. But in transform-
ers, the adversarial training has a heavy computational cost
due to the quadratic complexity of the self-attention compu-
tation. The AGAT method uses an efﬁcient attention-guided
adversarial mechanism with removing certainty patch embed-
ding on each layer with an attention-guided dropping strat-
egy during adversarial training. Bai et al. [Baiet al. , 2022 ]
have proposed the HAT method (named for High-frequency
components via Adversarial Training), which perturbs high-
frequency components during the training stage. The HAT
method alters the high-frequency components of the training
image by adding adversarial perturbation and then trains the
Vision Transformer (ViT) [Baiet al. , 2022 ]model with the
altered image to improve the model performance and makes
the model more robust.
2.6 Privacy
Today, pre-trained transformer models are deployed on cloud
systems. One of the main issues in cloud-based model de-
ployment pertains to privacy issues in the data. The major
privacy issues are the exposure of user data such as search
history, medical records, and bank accounts. The currentresearch focuses on preserving privacy in the inference of
transformer models. The paper [Huang et al. , 2020 ]intro-
duced TextHide, a federated learning technique to preserve
privacy, but this method is for sentence-based like machine
translation, sentiment analysis, and paraphrase generation
tasks), rather than for token-based tasks (such as name en-
tity recognition and semantic role labeling). Similarly, the
DP-ﬁnetune [Kerrigan et al. , 2020 ]Differential Privacy (DP)
method allows us to quantify the degree to which we can pro-
tect the sensitivity of data. But, training a DP algorithm de-
grades the quality of the model, which can be tuned using a
public base model on a private dataset.
The paper [Chen et al. , 2022c ]proposed THE-X as a
method by series of approximations on the HE [Boemer et
al., 2020 ]based solution in a transformer. THE-X method
replaces non-polynomial operations with a series of approxi-
mations with the help of these layers such as the SoftMax and
the GELU layer, drop the pooler layer, add Layer normaliza-
tion, use knowledge distillation techniques, and then use HE-
supported operations with HE transformer. THE-X method
is evaluated using BERT-Tiny Model on GLUE [Wang et
al., 2018 ]and benchmarked for a CONLL2003 [Sang and
De Meulder, 2003 ]task.
2.7 Approximation
In this section, we consider various kinds of differential equa-
tions and their approximation methods to make them more ef-
ﬁcient in terms of computational cost and number of param-
eters. The paper [Ruthotto and Haber, 2020 ]was one of the
ﬁrst to provide a theoretical foundation based on Partial Dif-
ferential Equations (PDEs) for deep neural networks such as
ResNets. More speciﬁcally, the author showed that residual
CNNs could be interpreted as a discretization of a space-time
differential equation. Based on the theoretical characteriza-
tion, Ruthotto also proposes new models such as hyperbolic
and parabolic CNNs with special properties. Residual net-
works have also been interpreted as Euler discretizations of
Ordinary Differential Equations (ODEs). However, the Euler
method of solving is not precise and has truncation errors as
it is a ﬁrst-order method. The authors of ODE Transformers
[Liet al. , 2021a ]used a classical higher-order method (Runge
Kutta) to build a transformer block. They evaluated the ODE
transformer on three sequence-generation tasks. These tasks
proved the transformer’s effectiveness, including abstractive
summarization, machine translation, and grammar error cor-
rection. Another effort in this direction is TransEvolve [Dutta
et al. , 2021 ], which provides a Transformer architecture such
as ODE transformer but is modeled on multi-particle dynamic
systems.
Transformers have been shown to be equivalent to uni-
versal computation engines [Luet al. , 2021 ]. The authors
have proposed an architecture known as the Frozen Pretrained
Transformer (FPT), which can be trained on a single modal-
ity (such as text data for language modeling), and identify
abstractions (such as feature representations) that are useful
across modalities. They have taken a GPT, pre-trained it on
only natural language data, and ﬁne-tuned its input and out-
put layers along with the layer normalization parameters and
positional embeddings. This has resulted in the FPT perform-ing comparably with transformers trained completely from
scratch for a variety of tasks such as protein fold prediction,
numerical computation, and even image classiﬁcation.
2.8 Probabilistic methods
The bayesian-based probabilistic model plays an important
role in estimating uncertainty in data and model while clas-
sifying an image. It brings efﬁciency in terms of less num-
ber of parameters (by sampling models using dropout) and
cab able to model efﬁciently using a small amount of data.
Guo et al. [Guo et al. , 2022a ]have proposed Uncertainty-
Guided Probabilistic Transformer (UGPT) for Complex Ac-
tion Recognition. Multi-head self-attention is used to cap-
ture the complex and long-term dynamics of complex actions.
The author has extended the deterministic transformer mech-
anism to the probabilistic transformer mechanism to quantify
the prediction’s uncertainty. The author introduces a novel
training strategy using majority and minority models for esti-
mating epistemic (model) uncertainty. Yang et al. [Yang et
al., 2021 ]have discussed the difﬁculty in camouﬂaged object
detection due to indistinguishable textures leading to inherent
uncertainty on it. The author proposed an uncertainty-guided
transformer reasoning (UGTR) to learn a conditional distribu-
tion over the model output to estimate uncertainty and make
reasoning over it.
2.9 Efﬁcient learning: Continual/ Incremental/
Lifelong/ and Federated learning
Another important aspect is how we will adapt the trained
model with a few new categories of data and new tasks. In
recent years much study has been done in the ﬁeld of con-
tinual learning/incremental learning/ lifelong learning to take
care of new classes and new tasks etc. our scope is not to
deﬁne these terms as per deﬁnition. Here we focus on how
the transformer model helps to improve these learning pro-
cesses. Dytox [Douillard et al. , 2022 ]has discussed trans-
former models for continual learning with dynamic token ex-
pansion. The author has discussed the issue of the existing
deep method’s struggle to continually learn new tasks without
forgetting the previous ones. The transformer’s encoder and
decoder modules are shared among all tasks in order to scale
it for a large number of tokens. The author has introduced a
lifelong vision transformer (LVT) [Wang et al. , 2022a ]to get
rid of catastrophic forgetting in Continual Learning tasks. In
order to achieve this, the author has introduced an attention-
based mechanism to get better stability for continual learn-
ing. Wang et al. [Wang et al. , 2022b ]have introduced a
contrastive vision transformer to mitigate catastrophic forget-
ting. The method uses a contrastive learning strategy using
a transformer to get a better stability-plasticity trade-off for
continual online learning.
2.10 Inclusiveness
The Inclusiveness domain focus on the The transformer
model-based research focuses on empowering everyone and
engaging people with visual, hearing, and other impairments.
We deﬁne this as Inclusiveness. The main challenge is how
we will deploy the transformer model in embedding sys-
tems(Like Microcontroller and Microprocessor) more efﬁ-Tasks Corpus Length Class Metrics Description
Long LISTOPS ListOps [Nangia and
Bowman, 2018 ]2K 10 Accuracy To check the ability to reason hierarchically
while handling long contexts
Byte-level Text
ClassiﬁcationIMDB Reviews [Maas
et al. , 2011 ]4K 2 Accuracy Is the model the classify the text associated with
the real-world applications?
Byte-level Docu-
ment RetrievalAAN [Radev et al. ,
2013 ]8K 2 Accuracy Does the model able to retrieve the matched se-
quence document 1 ?
Image Classiﬁca-
tionCIFAR10 [Krizhevsky
and others, 2009 ]N210 Accuracy Does the model able classify Image of long se-
quence length N2pixels?
Pathﬁnder Synthetic [Linsley et
al., 2018 ]1K 2 Accuracy Is there a connected path exist between point-1
and point-2 of image size 32 X 32?
Path-X Synthetic [Linsley et
al., 2018 ]16K 2 Accuracy Is there a connected path exist between point-1
and point-2 of image size 128 X 128?
Table 11: This table reports a description of various tasks in the Long Range Arena (LRA) benchmark. Along with the description, it provides
corpus details, length of the sequence, number of classes, and evaluation metrics for all the tasks in the LRA benchmark.
ciently for general proposed applications and also an appli-
cation for impaired persons. Very little research has been
carried out in this ﬁeld like ViT Cane [Kumar, 2021 ], Fly-
ing guide dog [Tanet al. , 2021 ], TransDARC [Peng et al. ,
2022 ], and Trans4Trans [Zhang et al. , 2021a ]. ViT Cane [Ku-
mar, 2021 ]provide visual assistants like ﬁnding the short-
est path to a destination and detecting obstacle from a dis-
tance for visually impaired persons. ViT can model uses
a PI camera module to capture the picture and detect the
obstacle using the ViT transformer model. The complete
work is done using a Raspberry Pi microcontroller. The au-
thor has compared the method with CNN-based TOLO ar-
chitecture and shows better results. Flying guide dog [Tan
et al. , 2021 ]helps to discover the walkable path for a visu-
ally impaired person. They use drones to capture real-time
street views and a transformer model to extract the walk-
able area from the segmentation predictions. Finally, the
drone adjusts the movement automatically and guides the per-
son to walk in the walkable areas. Trans4Trans [Zhang et
al., 2021a ]uses an efﬁcient transformer for transparent ob-
ject detection and semantic scene segmentation in real-world
navigation assistance. TransDARC [Peng et al. , 2022 ]uses
a transformer-based model for Driver ActiViTy Recognition
using latent space feature calibration. The author observes the
slowness in understanding the driver’s behavior and response
time of the existing model compared to the transformer-based
model. We account for efﬁciency in terms of comparing per-
formance transformer models with existing CNN-based mod-
els in terms of speed in ViT-Cane and other parameters in the
above papers. We also call it the efﬁcient way of deploying
transformer models on embedded systems.
3 Dataset, and Evaluation
In this section, we have discussed various data sets for the Im-
age classiﬁcation task and evaluated the models’ performance
on those datasets. We have included three important proﬁling
areas for efﬁcient transformers such as 1. State of the Art
comparison on the ImageNet dataset, 2. Transfer learning on
new datasets, and 3. Long Rage Arena (LRA) performance.3.1 Dataset
We compare various transformer results for image classiﬁ-
cation tasks on ImageNet-1K [Deng et al. , 2009 ]dataset and
ImageNet-21K [Deng et al. , 2009 ]. The ImageNet-1K dataset
contains 1.2 million images in the training set and 50K im-
ages in the test set over the 1000 (1K) category of classes.
ImageNet-21K is a large-scale dataset containing 14 million
images in the training set over the 21K category of classes.
We show the transfer learning performance from ImageNet-
1K to other datasets like CIFAR10 [Krizhevsky and others,
2009 ], CIFAR100 [Krizhevsky and others, 2009 ], Oxford-
IIIT-Pet [Parkhi et al. , 2012 ], Oxford-IIIT-Flower [Nils-
back and Zisserman, 2008 ]and Standford Cars [Krause et
al., 2013 ]. We show transfer learning performance from
ImageNet-21K to the above datasets in table-8. We report
the results transformer model on ImageNet-C [Hendrycks and
Dietterich, 2018 ]dataset. We have discussed another bench-
mark dataset for transformers models such as Long Rage
Arena [Tayet al. , 2020b ](LRA), which consists of six chal-
lenging corpora focused on long-range sequences.
3.2 State of the art comparison on ImageNet
Dataset
We analyzed and compared the model performance of var-
ious efﬁcient vision transformers on ImageNet-1k [Deng et
al., 2009 ]dataset as shown in the table- 2. The comparison
is based on the number of parameters in millions (M), num-
ber of ﬂoating point operations (FLOPS), image size, type of
networks, and top-1 accuracy. In table- 2, table- 3, and table-
4 we compare various transformer’s performance for input
size- 224224pixels, whereas in table-5, we compare with
different image sizes like 2562,2882,3842,4482,5122, and
6002. In table-2, we start comparing with Convolution Neu-
ral Net (CNN) architectures like ResNets [Heet al. , 2016 ]
and RegNet [Radosavovic et al. , 2020 ]provide good perfor-
mance on ImageNet [Deng et al. , 2009 ]dataset, ImageNet
Real Dataset [Beyer et al. , 2020 ]and ImageNet-v2 [Recht
et al. , 2019 ]dataset. Similarly, Involution Neural Network
(INN) [Liet al. , 2021b ]provides good performance on Ima-
geNet data compared to ResNet models. We start comparing
transformer base architecture DeiT [Touvron et al. , 2021a ]Model ListOps Text Retrieval Image PathﬁnderPath-X Avg
Chance 10.00 50.00 50.00 10.00 50.00 50.00 44.00
Transformer 36.37 64.27 57.46 42.44 71.40 FAIL 54.39
Local Attention 15.82 52.98 53.39 41.46 66.63 FAIL 46.06
Sparse Trans. 17.07 63.58 59.59 44.24 71.71 FAIL 51.24
Longformer 35.63 62.85 56.89 42.22 69.71 FAIL 53.46
Linformer 35.70 53.94 52.27 38.56 76.34 FAIL 51.36
Reformer 37.27 56.10 53.40 38.07 68.50 FAIL 50.67
Sinkhorn Trans. 33.67 61.20 53.83 41.23 67.45 FAIL 51.39
Synthesizer 36.99 61.68 54.67 41.61 69.45 FAIL 52.88
BigBird 36.05 64.02 59.29 40.83 74.87 FAIL 55.01
Linear Trans. 16.13 65.90 53.09 42.34 75.30 FAIL 50.55
Performer 18.01 65.40 53.82 42.77 77.05 FAIL 51.41
Nystr ¨omformer* 37.34 65.75 81.29 - - - 61.46
Transformer-LS* 38.36 68.40 81.95 - - - 62.90
Table 12: Long Range Aerna results for 6 different tasks for various efﬁcient transformer models. * tends for evaluation as per the proposed
transformer paper
with both CNN and INN-based architecture for image clas-
siﬁcation tasks and observe that transformer architecture per-
forms better compared to ResNet and RegNet architecture.
In comparison with all efﬁcient transformers, Wave-
ViT[Yaoet al. , 2022 ]transformer model is more efﬁcient as
compared to the number of parameters and its top-1 accuracy.
WaveViT- Sperforms better with small parameters(22.7M)
and has a comparative performance(83.9 top-1 accuracy) with
other models on the ImageNet-1k dataset and WaveViT- L
provides the best performance in terms of top-1 accuracy
(85.5% with 57.5M of parameters) among all the transformer-
based models. But the WaveViT model uses supervised ex-
tra training data to achieve this performance. CvT [Wuet
al., 2021 ]model has been evaluated on ImageNet, ImageNet
real [Beyer et al. , 2020 ]dataset, and ImageNet-v2 [Recht
et al. , 2019 ]dataset and Performs comparative results on all
three datasets with a comparatively small number of param-
eters (32M) for the image of size 224x224 pixels. We ana-
lyzed top-1 accuracy for images of size 384x384. We found
that CMT [Guo et al. , 2022b ]with a small number of pa-
rameters(25.1M) and less number of FLOPS(4G) provides an
equivalent top-1 score(83.3) on ImageNet-1k dataset and also
evaluated on ImageNet real and ImageNetV2 [Recht et al. ,
2019 ]dataset as well. But Uniformer [Liet al. , 2022a ]per-
forms best among all the models for image size 2242with
more parameters (100M)and FLOPs(12.6) as shown in table-
3. Similarly, we provide comparison of various MLP-like
transformer models as in table- 4. we report that Hire-MLP-
Large [Guo et al. , 2022c ]model provides the best perfor-
mance in terms of top-1 accuracy (83.8% with 96M of pa-
rameters) among all the MLP-Mixer type transformer mod-
els. Similarly, in the pooler type network, we report that
PoolFormer-M48 [Yuet al. , 2022b ]model provides the best
performance in terms of top-1 accuracy (82.5% with 73M of
parameters) among all pooler type transformer models.
Similarly, table-5 reports the performance of various trans-
former models in various image sizes. For image size 2562,
CMT-B [Guo et al. , 2022b ]performs better, whereas, for im-
age size 2882, LV-ViT [Jiang et al. , 2021 ]performs best,
but it uses extra labeled data to train the model. Similarly,for image size 3842, CSwin-B [Dong et al. , 2022 ]performs
best(85.4% with 78M of parameters and 47G number of
FLOPs) for transformer type networks and Uniformer [Liet
al., 2022a ]performs good (86.3% with 100M of parameters
and 39.2G number of FLOPs), whereas MaxViT [Tuet al. ,
2022 ]performs best (86.4% with 212M of parameters and
133.1G number of FLOPs) for convolution transformer type
networks. For image size 4482, CaiT [Touvron et al. , 2021b ]
performs best(86.3% with 271M of parameters and 247.8G
number of FLOPs) for transformer type networks. For image
size 5122, LV-ViT [Jiang et al. , 2021 ]performs well (86.4%
with 151M of parameters and 214.8G number of FLOPs),
whereas MaxViT [Tuet al. , 2022 ]performs best (86.7% with
212M of parameters and 245.4G number of FLOPs), for con-
volution transformer-type networks with extra labeled trained
data.
Table-7 shows the performance analysis of spectral trans-
former models trained on ImageNet1K [Deng et al. , 2009 ]for
image size 224224and384384. It compares the num-
ber of parameters, number of FLOPs, and Top-1 accuracy of
various spectral vision transformers. The green tick mark in
the WaveViT [Yao et al. , 2022 ]model indicates attention is
used in the transformer model. The green tick mark in the
extra level column indicates that WaveViT [Yaoet al. , 2022 ]
uses extra training data while training. Fnet [Lee-Thorp et al. ,
2021 ], GFNet [Rao et al. , 2021 ]and AFNO [Guibas et al. ,
2022 ]do not use self-attention network compared to Wave-
ViT [Yao et al. , 2022 ]and achieve good performance with
less number of parameters and FLOPs compare to Wave-
ViT [Yao et al. , 2022 ]. Only WaveViT [Yao et al. , 2022 ]
uses extra training data during the training of the transformer
models. It is observed that the extra data helps to improve the
performance in the ImageNet-1K dataset.
Similarly, we report the performance of various transform-
ers on the ImageNet-21K dataset as shown in the table- 6.
Here we compare types of transformer networks like a trans-
former and Convolution transformer, Number of Parameters,
Number of FLOPs, image sizes, and top-1 accuracy.3.3 Transfer learning on New datasets for Image
Classiﬁcation task
Table- 8, shows the transfer learning capability of the pre-
trained transformer models on CIFAR10 [Krizhevsky and
others, 2009 ], CIFAR100 [Krizhevsky and others, 2009 ],
Pets [Parkhi et al. , 2012 ], Flowers [Nilsback and Zisser-
man, 2008 ], Cars [Krause et al. , 2013 ]datasets. We show
the comparison of the number of parameters, number of
FLOPs, and top-1 accuracy of various models on the above
datasets. The table 8 contains two blocks, the top blocks
show a comparison of the transformer models trained on the
ImageNet-1k [Deng et al. , 2009 ](1000 target categories)
dataset, whereas the bottom block shows the comparison
of the transformer models on ImageNet-22K [Deng et al. ,
2009 ](22000 target categories). In the top block, we ob-
served that CMT-S [Guo et al. , 2022b ]performs better in CI-
FAR10 [Krizhevsky and others, 2009 ]dataset i.e., accuracy
is around 99.2%, RegionViT-M [Chen et al. , 2022a ], DeiT-
B[Touvron et al. , 2021a ],CaiT-S [Touvron et al. , 2021b ]
performs better in CIFAR100 [Krizhevsky and others, 2009 ]
dataset i.e., accuracy is around 90.8%. In Oxford-IIIT-
Pet [Parkhi et al. , 2012 ]dataset, RegionViT-M [Chen et al. ,
2022a ]performs better and its accuracy is around 95.5%.
GFNet-H-B [Rao et al. , 2021 ]and TNT-S [Han et al. , 2021 ]
performs better in Oxford-IIIT-Flowers [Nilsback and Zisser-
man, 2008 ]dataset i.e., accuracy is around 98.8%. In Stand-
ford Cars [Krause et al. , 2013 ]dataset,CMT-S [Guo et al. ,
2022b ]performs better and its accuracy is 94.4%.
In bottom block, we observe that CvT-W24 [Wu et
al., 2021 ]performs better in CIFAR10 [Krizhevsky and
others, 2009 ], CIFAR100 [Krizhevsky and others, 2009 ],
and Oxford-IIIT-Flowers [Nilsback and Zisserman, 2008 ]
datasets, whereas ViT-H [Dosovitskiy et al. , 2020 ]performs
well on the Oxford-IIIT-Pet [Parkhi et al. , 2012 ]dataset. We
also analyzed model architecture across layers for ViT [Doso-
vitskiy et al. , 2020 ]and CvT [Wuet al. , 2021 ]models. We
observe that ViT-H with 16 patch size performs better than
ViT-Base and ViT-Large. A similar case with CvT-24 per-
forms better compared to CvT-13 and CvT-21. So we can
conclude that the larger model size performs better.
It is very difﬁcult to claim that pre-training models on
ImageNet-22K [Deng et al. , 2009 ]dataset gives better repre-
sentation features, which helps to perform better in Transfer
learning as compared to pre-training models on ImageNet-
1K[Deng et al. , 2009 ]because the models are different. It
is not clear if it is due to the model, or it is due to the large
dataset with more classes.
3.4 Long Range arena (LRA) Benchmark
Transformers are largely not performing very well on long se-
quence lengths due to quadratic complexity in self-attention.
Long Range Arena (LRA) [Tayet al. , 2020b ]is another eval-
uation benchmark method focused on evaluating model qual-
ity on long-range- context scenarios. LRA benchmark is
applicable to sequence lengths ranging from 1K to 16K to-
kens. It focuses on a wide range of data types and modalities,
such as mathematical reasoning requiring similarity, struc-
tural, text, natural, synthetic images, and visual-spatial rea-
soning. LRA benchmark is basically evaluated the efﬁciencyof transformer models on a list of tasks focus on long-range
data contexts such as Long ListOps task, Byte level Text Clas-
siﬁcation, Byte Level Document Retrieval task, Image classi-
ﬁcation on the sequence of pixels, Pathﬁnder and Pathﬁnder-
X task as shown in table- 11. LRA benchmark is created
based on the Generality of the model(the efﬁcient transformer
model should apply to a variety of the task), Simplicity(the
task should have simple to setup), long inputs (the input se-
quence length should have reasonably long to capture long-
range dependency of the model), Challenging(the task should
have difﬁcult enough for improvement in the model perfor-
mance and to encourage future research direction), Probing
diverse aspects ( the set tasks should access the different capa-
bilities of the model like hierarchical/spatial structures, gen-
eralizations capabilities, etc.) and Non-resource intensive(the
model should be designed to be the lightweight model which
can be accessible to researcher).
The LRA benchmark is evaluated on recent transformer
models such as Performer [Choromanski et al. , 2020 ], Re-
formers [Kitaev et al. , 2019 ], Linformers [Wang et al. , 2020 ],
Linear transformers [Katharopoulos et al. , 2020 ], Synthe-
sizers [Tayet al. , 2021 ], Sinkhorn transformer [Tayet al. ,
2020a ], Sparse transformers [Child et al. , 2019 ], Nystrom-
former [Xiong et al. , 2021 ], Transformer-LS [Zhu et al. ,
2021 ], Longformers [Beltagy et al. , 2020 ]and Big bird [Za-
heer et al. , 2020 ]. None of the latest vision transformer(like
Swin [Liuet al. , 2021b ], Twin [Chu et al. , 2021 ], CvT [Wu
et al. , 2021 ], CSwin [Dong et al. , 2022 ], RegionViT [Chen et
al., 2022a ], WaveViT [Yaoet al. , 2022 ]) models are not eval-
uated on this benchmark. It is more challenging and interest-
ing to see results on this benchmark. Quantitative results for
various transformer models are reported in table-12. From the
table, we observe that it is a very challenging benchmark. The
performance in visual domains is relatively low compared to
language tasks. The image classiﬁcation scores low for long
sequences, and Most of the time, the model fails in the Path-X
task.
4 Conclusion
We have discussed all ten dimensions of the efﬁcient 360
frameworks in different subsections. The survey has opened
up new research areas and directions for transformers. For
instance, we are ourselves coming up with a spectral neural
operator-based transformer which we believe is likely to out-
perform state-of-art transformers with respect to robustness,
explainability, and efﬁciency (have a signiﬁcantly lesser num-
ber of parameters).
We also notice from the diagram (Figure 1), there is room
for research in dimensions such as privacy, transparency, fair-
ness, efﬁcient learning, and most importantly inclusiveness.
Due to the applicability of some of the advanced transformer
models on other modalities of data, a lot of work on audio,
speech, and video has opened up. Further, AI for sciences is
another open area of research including the applicability of
the advanced transformer models on high-resolution data like
weather forecasting and oceanography (wave modeling).References
[Baiet al. , 2022 ]Jiawang Bai, Li Yuan, Shu-Tao Xia,
Shuicheng Yan, Zhifeng Li, and Wei Liu. Improving
vision transformers by revisiting high-frequency compo-
nents. arXiv preprint arXiv:2204.00993 , 2022.
[Baoet al. , 2021 ]Hangbo Bao, Li Dong, Songhao Piao, and
Furu Wei. Beit: Bert pre-training of image transformers.
InInternational Conference on Learning Representations ,
2021.
[Baoet al. , 2022 ]Hangbo Bao, Li Dong, Songhao Piao, and
Furu Wei. Beit: Bert pre-training of image transformers.
InInternational Conference on Learning Representations ,
2022.
[Beltagy et al. , 2020 ]Iz Beltagy, Matthew E Peters, and Ar-
man Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150 , 2020.
[Beyer et al. , 2020 ]Lucas Beyer, Olivier J H ´enaff, Alexan-
der Kolesnikov, Xiaohua Zhai, and A ¨aron van den
Oord. Are we done with imagenet? arXiv preprint
arXiv:2006.07159 , 2020.
[Bhojanapalli et al. , 2021 ]Srinadh Bhojanapalli, Ayan
Chakrabarti, Daniel Glasner, Daliang Li, Thomas Un-
terthiner, and Andreas Veit. Understanding robustness
of transformers for image classiﬁcation. In Proceedings
of the IEEE/CVF International Conference on Computer
Vision , pages 10231–10241, 2021.
[Boemer et al. , 2020 ]Fabian Boemer, Rosario Cammarota,
Daniel Demmler, Thomas Schneider, and Hossein Yalame.
Mp2ml: A mixed-protocol machine learning framework
for private inference. In Proceedings of the 15th Interna-
tional Conference on Availability, Reliability and Security ,
pages 1–10, 2020.
[Brock et al. , 2021 ]Andy Brock, Soham De, Samuel L
Smith, and Karen Simonyan. High-performance large-
scale image recognition without normalization. In Inter-
national Conference on Machine Learning , pages 1059–
1071. PMLR, 2021.
[Buchholz and Jug, 2022 ]Tim-Oliver Buchholz and Florian
Jug. Fourier image transformer. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1846–1854, 2022.
[Chen et al. , 2021a ]Chun-Fu Richard Chen, Quanfu Fan,
and Rameswar Panda. Crossvit: Cross-attention multi-
scale vision transformer for image classiﬁcation. In Pro-
ceedings of the IEEE/CVF international conference on
computer vision , pages 357–366, 2021.
[Chen et al. , 2021b ]Zhengsu Chen, Lingxi Xie, Jianwei
Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer:
The vision-friendly transformer. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 589–598, 2021.
[Chen et al. , 2022a ]Chun-Fu Chen, Rameswar Panda, and
Quanfu Fan. Regionvit: Regional-to-local attention for vi-
sion transformers. In International Conference on Learn-
ing Representations , 2022.[Chen et al. , 2022b ]Shoufa Chen, Enze Xie, GE Chongjian,
Runjian Chen, Ding Liang, and Ping Luo. Cyclemlp: A
mlp-like architecture for dense prediction. In International
Conference on Learning Representations , 2022.
[Chen et al. , 2022c ]Tianyu Chen, Hangbo Bao, Shaohan
Huang, Li Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou,
Jianxin Li, and Furu Wei. The-x: Privacy-preserving
transformer inference with homomorphic encryption. In
Findings of the Association for Computational Linguistics:
ACL 2022 , pages 3510–3520, 2022.
[Child et al. , 2019 ]Rewon Child, Scott Gray, Alec Radford,
and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
[Choromanski et al. , 2020 ]Krzysztof Marcin Choromanski,
Valerii Likhosherstov, David Dohan, Xingyou Song, An-
dreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy
Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking
attention with performers. In International Conference on
Learning Representations , 2020.
[Chu et al. , 2021 ]Xiangxiang Chu, Zhi Tian, Yuqing Wang,
Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and
Chunhua Shen. Twins: Revisiting the design of spatial
attention in vision transformers. Advances in Neural In-
formation Processing Systems , 34:9355–9366, 2021.
[Deng et al. , 2009 ]Jia Deng, Wei Dong, Richard Socher, Li-
Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on
computer vision and pattern recognition , pages 248–255.
Ieee, 2009.
[Ding et al. , 2022 ]Mingyu Ding, Bin Xiao, Noel Codella,
Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual at-
tention vision transformers. In Computer Vision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, Octo-
ber 23–27, 2022, Proceedings, Part XXIV , pages 74–92.
Springer, 2022.
[Dong et al. , 2022 ]Xiaoyi Dong, Jianmin Bao, Dongdong
Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen,
and Baining Guo. Cswin transformer: A general vision
transformer backbone with cross-shaped windows. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 12124–12134, 2022.
[Dosovitskiy et al. , 2020 ]Alexey Dosovitskiy, Lucas Beyer,
Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Min-
derer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition
at scale. In International Conference on Learning Repre-
sentations , 2020.
[Douillard et al. , 2022 ]Arthur Douillard, Alexandre Ram ´e,
Guillaume Couairon, and Matthieu Cord. Dytox: Trans-
formers for continual learning with dynamic token expan-
sion. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 9285–
9295, 2022.
[Dutta et al. , 2021 ]Subhabrata Dutta, Tanya Gautam,
Soumen Chakrabarti, and Tanmoy Chakraborty. Re-designing the transformer architecture with insights from
multi-particle dynamical systems. Advances in Neural
Information Processing Systems , 34:5531–5544, 2021.
[Edelman et al. , 2022 ]Benjamin L Edelman, Surbhi Goel,
Sham Kakade, and Cyril Zhang. Inductive biases and
variable creation in self-attention mechanisms. In Inter-
national Conference on Machine Learning , pages 5793–
5831. PMLR, 2022.
[gpt, 2022 ]https://openai.com/blog/chatgpt/, 2022.
[Guibas et al. , 2022 ]John Guibas, Morteza Mardani, Zongyi
Li, Andrew Tao, Anima Anandkumar, and Bryan Catan-
zaro. Efﬁcient token mixing for transformers via adaptive
fourier neural operators. In International Conference on
Learning Representations , 2022.
[Guo et al. , 2022a ]Hongji Guo, Hanjing Wang, and Qiang
Ji. Uncertainty-guided probabilistic transformer for com-
plex action recognition. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 20052–20061, 2022.
[Guo et al. , 2022b ]Jianyuan Guo, Kai Han, Han Wu, Yehui
Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt:
Convolutional neural networks meet vision transform-
ers. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 12175–
12185, 2022.
[Guo et al. , 2022c ]Jianyuan Guo, Yehui Tang, Kai Han,
Xinghao Chen, Han Wu, Chao Xu, Chang Xu, and Yunhe
Wang. Hire-mlp: Vision mlp via hierarchical rearrange-
ment. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
826–836, June 2022.
[Han et al. , 2021 ]Kai Han, An Xiao, Enhua Wu, Jianyuan
Guo, Chunjing Xu, and Yunhe Wang. Transformer in
transformer. Advances in Neural Information Processing
Systems , 34:15908–15919, 2021.
[Hatamizadeh et al. , 2022 ]Ali Hatamizadeh, Hongxu Yin,
Jan Kautz, and Pavlo Molchanov. Global context vision
transformers. arXiv preprint arXiv:2206.09959 , 2022.
[Heet al. , 2016 ]Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 770–778, 2016.
[Hendrycks and Dietterich, 2018 ]Dan Hendrycks and
Thomas Dietterich. Benchmarking neural network
robustness to common corruptions and perturbations. In
International Conference on Learning Representations ,
2018.
[Hou et al. , 2022 ]Qibin Hou, Zihang Jiang, Li Yuan, Ming-
Ming Cheng, Shuicheng Yan, and Jiashi Feng. Vision
permutator: A permutable mlp-like architecture for visual
recognition. IEEE Transactions on Pattern Analysis &
Machine Intelligence , (01):1–1, 2022.
[Huang et al. , 2020 ]Yangsibo Huang, Zhao Song, Danqi
Chen, Kai Li, and Sanjeev Arora. Texthide: Tackling data
privacy in language understanding tasks. In Findings of theAssociation for Computational Linguistics: EMNLP 2020 ,
pages 1368–1382, 2020.
[Jiang et al. , 2021 ]Zi-Hang Jiang, Qibin Hou, Li Yuan,
Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and
Jiashi Feng. All tokens matter: Token labeling for training
better vision transformers. Advances in Neural Informa-
tion Processing Systems , 34:18590–18602, 2021.
[Katharopoulos et al. , 2020 ]Angelos Katharopoulos,
Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret.
Transformers are rnns: Fast autoregressive transformers
with linear attention. In International Conference on
Machine Learning , pages 5156–5165. PMLR, 2020.
[Kenton and Toutanova, 2019 ]Jacob Devlin Ming-
Wei Chang Kenton and Lee Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In Proceedings of NAACL-HLT ,
pages 4171–4186, 2019.
[Kerrigan et al. , 2020 ]Gavin Kerrigan, Dylan Slack, and
Jens Tuyls. Differentially private language models ben-
eﬁt from public pre-training. In Proceedings of the Second
Workshop on Privacy in NLP , pages 39–45, 2020.
[Kitaev et al. , 2019 ]Nikita Kitaev, Lukasz Kaiser, and
Anselm Levskaya. Reformer: The efﬁcient transformer.
InInternational Conference on Learning Representations ,
2019.
[Kolesnikov et al. , 2020 ]Alexander Kolesnikov, Lucas
Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
Sylvain Gelly, and Neil Houlsby. Big transfer (bit):
General visual representation learning. In European
conference on computer vision , pages 491–507. Springer,
2020.
[Krause et al. , 2013 ]Jonathan Krause, Michael Stark, Jia
Deng, and Li Fei-Fei. 3d object representations for ﬁne-
grained categorization. In Proceedings of the IEEE inter-
national conference on computer vision workshops , pages
554–561, 2013.
[Krizhevsky and others, 2009 ]Alex Krizhevsky et al. Learn-
ing multiple layers of features from tiny images. 2009.
[Kumar, 2021 ]Bhavesh Kumar. Vit cane: Visual as-
sistant for the visually impaired. arXiv preprint
arXiv:2109.13857 , 2021.
[Lee-Thorp et al. , 2021 ]James Lee-Thorp, Joshua Ainslie,
Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens
with fourier transforms. arXiv preprint arXiv:2105.03824 ,
2021.
[Liet al. , 2020 ]Zongyi Li, Nikola Borislavov Kovachki,
Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew
Stuart, Anima Anandkumar, et al. Fourier neural operator
for parametric partial differential equations. In Interna-
tional Conference on Learning Representations , 2020.
[Liet al. , 2021a ]Bei Li, Quan Du, Tao Zhou, Shuhan Zhou,
Xin Zeng, Tong Xiao, and Jingbo Zhu. Ode transformer:
An ordinary differential equation-inspired model for neu-
ral machine translation. arXiv preprint arXiv:2104.02308 ,
2021.[Liet al. , 2021b ]Duo Li, Jie Hu, Changhu Wang, Xiang-
tai Li, Qi She, Lei Zhu, Tong Zhang, and Qifeng Chen.
Involution: Inverting the inherence of convolution for vi-
sual recognition. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages
12321–12330, 2021.
[Liet al. , 2022a ]Kunchang Li, Yali Wang, Junhao Zhang,
Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li,
and Yu Qiao. Uniformer: Unifying convolution and
self-attention for visual recognition. arXiv preprint
arXiv:2201.09450 , 2022.
[Liet al. , 2022b ]Yanghao Li, Chao-Yuan Wu, Haoqi Fan,
Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and
Christoph Feichtenhofer. Mvitv2: Improved multiscale vi-
sion transformers for classiﬁcation and detection. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 4804–4814, 2022.
[Liet al. , 2022c ]Yehao Li, Ting Yao, Yingwei Pan, and Tao
Mei. Contextual transformer networks for visual recogni-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2022.
[Lian et al. , 2022 ]Dongze Lian, Zehao Yu, Xing Sun, and
Shenghua Gao. As-mlp: An axial shifted mlp architec-
ture for vision. In International Conference on Learning
Representations , 2022.
[Linsley et al. , 2018 ]Drew Linsley, Junkyung Kim, Vijay
Veerabadran, Charles Windolf, and Thomas Serre. Learn-
ing long-range spatial dependencies with horizontal gated
recurrent units. Advances in neural information process-
ing systems , 31, 2018.
[Liuet al. , 2021a ]Hanxiao Liu, Zihang Dai, David So, and
Quoc V Le. Pay attention to mlps. Advances in Neural
Information Processing Systems , 34:9204–9215, 2021.
[Liuet al. , 2021b ]Ze Liu, Yutong Lin, Yue Cao, Han Hu,
Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using
shifted windows. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 10012–
10022, 2021.
[Liuet al. , 2022 ]Jihao Liu, Xin Huang, Guanglu Song,
Hongsheng Li, and Yu Liu. Uninet: Uniﬁed architecture
search with convolution, transformer, and mlp. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI ,
pages 33–49. Springer, 2022.
[Luet al. , 2021 ]Kevin Lu, Aditya Grover, Pieter Abbeel,
and Igor Mordatch. Pretrained transformers as universal
computation engines. arXiv preprint arXiv:2103.05247 ,
2021.
[Maas et al. , 2011 ]Andrew Maas, Raymond E Daly, Peter T
Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
Learning word vectors for sentiment analysis. In Pro-
ceedings of the 49th annual meeting of the association for
computational linguistics: Human language technologies ,
pages 142–150, 2011.[Nangia and Bowman, 2018 ]Nikita Nangia and Samuel
Bowman. Listops: A diagnostic dataset for latent tree
learning. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Compu-
tational Linguistics: Student Research Workshop , pages
92–99, 2018.
[Nilsback and Zisserman, 2008 ]Maria-Elena Nilsback and
Andrew Zisserman. Automated ﬂower classiﬁcation over
a large number of classes. In 2008 Sixth Indian Confer-
ence on Computer Vision, Graphics & Image Processing ,
pages 722–729. IEEE, 2008.
[Panet al. , 2022a ]Zizheng Pan, Jianfei Cai, and Bohan
Zhuang. Fast vision transformers with hilo attention.
InAdvances in Neural Information Processing Systems ,
2022.
[Panet al. , 2022b ]Zizheng Pan, Bohan Zhuang, Haoyu He,
Jing Liu, and Jianfei Cai. Less is more: Pay less attention
in vision transformers. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , volume 36, pages 2035–
2043, 2022.
[Park and Kim, 2021 ]Namuk Park and Songkuk Kim. How
do vision transformers work? In International Conference
on Learning Representations , 2021.
[Parkhi et al. , 2012 ]Omkar M Parkhi, Andrea Vedaldi, An-
drew Zisserman, and CV Jawahar. Cats and dogs. In 2012
IEEE conference on computer vision and pattern recogni-
tion, pages 3498–3505. IEEE, 2012.
[Paul and Chen, 2022 ]Sayak Paul and Pin-Yu Chen. Vision
transformers are robust learners. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 36,
pages 2071–2081, 2022.
[Peng et al. , 2022 ]Kunyu Peng, Alina Roitberg, Kailun
Yang, Jiaming Zhang, and Rainer Stiefelhagen. Trans-
darc: Transformer-based driver activity recognition with
latent space feature calibration. In 2022 IEEE/RSJ In-
ternational Conference on Intelligent Robots and Systems
(IROS) , pages 278–285. IEEE, 2022.
[Radev et al. , 2013 ]Dragomir R Radev, Pradeep Muthukr-
ishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl
anthology network corpus. Language Resources and Eval-
uation , 47(4):919–944, 2013.
[Radosavovic et al. , 2020 ]Ilija Radosavovic, Raj Prateek
Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar.
Designing network design spaces. In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition , pages 10428–10436, 2020.
[Raghu et al. , 2021 ]Maithra Raghu, Thomas Unterthiner,
Simon Kornblith, Chiyuan Zhang, and Alexey Dosovit-
skiy. Do vision transformers see like convolutional neu-
ral networks? Advances in Neural Information Processing
Systems , 34:12116–12128, 2021.
[Raoet al. , 2021 ]Yongming Rao, Wenliang Zhao, Zheng
Zhu, Jiwen Lu, and Jie Zhou. Global ﬁlter networks for
image classiﬁcation. Advances in Neural Information Pro-
cessing Systems , 34:980–993, 2021.[Recht et al. , 2019 ]Benjamin Recht, Rebecca Roelofs, Lud-
wig Schmidt, and Vaishaal Shankar. Do imagenet classi-
ﬁers generalize to imagenet? In International Conference
on Machine Learning , pages 5389–5400. PMLR, 2019.
[Renet al. , 2022 ]Sucheng Ren, Zhengqi Gao, Tianyu Hua,
Zihui Xue, Yonglong Tian, Shengfeng He, and Hang Zhao.
Co-advise: Cross inductive bias distillation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 16773–16782, 2022.
[Ruthotto and Haber, 2020 ]Lars Ruthotto and Eldad Haber.
Deep neural networks motivated by partial differential
equations. Journal of Mathematical Imaging and Vision ,
62(3):352–364, 2020.
[Sang and De Meulder, 2003 ]Erik Tjong Kim Sang and
Fien De Meulder. Introduction to the conll-2003 shared
task: Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Natural Lan-
guage Learning at HLT-NAACL 2003 , pages 142–147,
2003.
[Shao et al. , 2021 ]Rulin Shao, Zhouxing Shi, Jinfeng Yi,
Pin-Yu Chen, and Cho-Jui Hsieh. On the adversar-
ial robustness of vision transformers. arXiv preprint
arXiv:2103.15670 , 2021.
[Srinivas et al. , 2021 ]Aravind Srinivas, Tsung-Yi Lin, Niki
Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish
Vaswani. Bottleneck transformers for visual recognition.
InProceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 16519–16529, 2021.
[Steiner et al. , 2022 ]Andreas Peter Steiner, Alexander
Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszko-
reit, and Lucas Beyer. How to train your vit? data,
augmentation, and regularization in vision transformers.
Transactions on Machine Learning Research , 2022.
[Tan and Le, 2019 ]Mingxing Tan and Quoc Le. Efﬁcient-
net: Rethinking model scaling for convolutional neural
networks. In International conference on machine learn-
ing, pages 6105–6114. PMLR, 2019.
[Tanet al. , 2021 ]Haobin Tan, Chang Chen, Xinyu Luo,
Jiaming Zhang, Constantin Seibold, Kailun Yang, and
Rainer Stiefelhagen. Flying guide dog: Walkable path
discovery for the visually impaired utilizing drones and
transformer-based semantic segmentation. In 2021 IEEE
International Conference on Robotics and Biomimetics
(ROBIO) , pages 1123–1128. IEEE, 2021.
[Tang et al. , 2022 ]Yehui Tang, Kai Han, Jianyuan Guo,
Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang. An im-
age patch is a wave: Phase-aware vision mlp. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10935–10944, 2022.
[Tayet al. , 2020a ]Yi Tay, Dara Bahri, Liu Yang, Donald
Metzler, and Da-Cheng Juan. Sparse sinkhorn attention.
InInternational Conference on Machine Learning , pages
9438–9447. PMLR, 2020.
[Tayet al. , 2020b ]Yi Tay, Mostafa Dehghani, Samira Ab-
nar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,Liu Yang, Sebastian Ruder, and Donald Metzler. Long
range arena: A benchmark for efﬁcient transformers. In
International Conference on Learning Representations ,
2020.
[Tayet al. , 2020c ]Yi Tay, Mostafa Dehghani, Dara Bahri,
and Donald Metzler. Efﬁcient transformers: A survey.
ACM Computing Surveys (CSUR) , 2020.
[Tayet al. , 2021 ]Yi Tay, Dara Bahri, Donald Metzler, Da-
Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Re-
thinking self-attention for transformer models. In Inter-
national conference on machine learning , pages 10183–
10192. PMLR, 2021.
[Tolstikhin et al. , 2021 ]Ilya O Tolstikhin, Neil Houlsby,
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
Thomas Unterthiner, Jessica Yung, Andreas Steiner,
Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An
all-mlp architecture for vision. Advances in Neural Infor-
mation Processing Systems , 34:24261–24272, 2021.
[Touvron et al. , 2021a ]Hugo Touvron, Matthieu Cord,
Matthijs Douze, Francisco Massa, Alexandre Sablay-
rolles, and Herv ´e J´egou. Training data-efﬁcient image
transformers & distillation through attention. In In-
ternational Conference on Machine Learning , pages
10347–10357. PMLR, 2021.
[Touvron et al. , 2021b ]Hugo Touvron, Matthieu Cord,
Alexandre Sablayrolles, Gabriel Synnaeve, and Herv ´e
J´egou. Going deeper with image transformers. In
Proceedings of the IEEE/CVF International Conference
on Computer Vision , pages 32–42, 2021.
[Touvron et al. , 2022 ]Hugo Touvron, Piotr Bojanowski,
Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby,
Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel
Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward net-
works for image classiﬁcation with data-efﬁcient training.
IEEE Transactions on Pattern Analysis and Machine In-
telligence , 2022.
[Tuet al. , 2022 ]Zhengzhong Tu, Hossein Talebi, Han
Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and
Yinxiao Li. Maxvit: Multi-axis vision transformer. In
Computer Vision–ECCV 2022: 17th European Confer-
ence, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
Part XXIV , pages 459–479. Springer, 2022.
[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. Advances in neural information processing systems ,
30, 2017.
[Venkataramanan et al. , 2023 ]Shashanka Venkataramanan,
Amir Ghodrati, Yuki M Asano, Fatih Porikli, and
Amirhossein Habibian. Skip-attention: Improving vi-
sion transformers by paying less attention. arXiv preprint
arXiv:2301.02240 , 2023.
[Wang et al. , 2018 ]Alex Wang, Amanpreet Singh, Julian
Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform fornatural language understanding. In International Confer-
ence on Learning Representations , 2018.
[Wang et al. , 2020 ]Sinong Wang, Belinda Z Li, Madian
Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768 ,
2020.
[Wang et al. , 2021 ]Wenhai Wang, Enze Xie, Xiang Li,
Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
Luo, and Ling Shao. Pyramid vision transformer: A ver-
satile backbone for dense prediction without convolutions.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 568–578, 2021.
[Wang et al. , 2022a ]Zhen Wang, Liu Liu, Yiqun Duan, Ya-
jing Kong, and Dacheng Tao. Continual learning with life-
long vision transformer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 171–181, June 2022.
[Wang et al. , 2022b ]Zhen Wang, Liu Liu, Yajing Kong, Ji-
axian Guo, and Dacheng Tao. Online continual learning
with contrastive vision transformer. In Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XX , pages 631–
650. Springer, 2022.
[Wang et al. , 2022c ]Ziyu Wang, Wenhao Jiang, Yiming M
Zhu, Li Yuan, Yibing Song, and Wei Liu. Dynamixer: a
vision mlp architecture with dynamic mixing. In Inter-
national Conference on Machine Learning , pages 22691–
22701. PMLR, 2022.
[Wuet al. , 2021 ]Haiping Wu, Bin Xiao, Noel Codella,
Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:
Introducing convolutions to vision transformers. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision , pages 22–31, 2021.
[Wuet al. , 2022 ]Boxi Wu, Jindong Gu, Zhifeng Li, Deng
Cai, Xiaofei He, and Wei Liu. Towards efﬁcient ad-
versarial training on vision transformers. arXiv preprint
arXiv:2207.10498 , 2022.
[Xiaet al. , 2022 ]Zhuofan Xia, Xuran Pan, Shiji Song, Li Er-
ran Li, and Gao Huang. Vision transformer with de-
formable attention. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
4794–4803, 2022.
[Xiong et al. , 2021 ]Yunyang Xiong, Zhanpeng Zeng,
Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin
Li, and Vikas Singh. Nystr ¨omformer: A nystr ¨om-based
algorithm for approximating self-attention. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence ,
volume 35, pages 14138–14148, 2021.
[Xuet al. , 2021a ]Weijian Xu, Yifan Xu, Tyler Chang, and
Zhuowen Tu. Co-scale conv-attentional image transform-
ers. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 9981–9990, 2021.
[Xuet al. , 2021b ]Yufei Xu, Qiming Zhang, Jing Zhang, and
Dacheng Tao. Vitae: Vision transformer advanced by ex-
ploring intrinsic inductive bias. Advances in Neural Infor-
mation Processing Systems , 34:28522–28535, 2021.[Yang et al. , 2021 ]Fan Yang, Qiang Zhai, Xin Li, Rui
Huang, Ao Luo, Hong Cheng, and Deng-Ping Fan.
Uncertainty-guided transformer reasoning for camou-
ﬂaged object detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages
4146–4155, 2021.
[Yaoet al. , 2022 ]Ting Yao, Yingwei Pan, Yehao Li, Chong-
Wah Ngo, and Tao Mei. Wave-vit: Unifying wavelet
and transformers for visual representation learning. arXiv
preprint arXiv:2207.04978 , 2022.
[Yuet al. , 2022a ]Tan Yu, Xu Li, Yunfeng Cai, Mingming
Sun, and Ping Li. S2-mlp: Spatial-shift mlp architecture
for vision. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision , pages 297–
306, 2022.
[Yuet al. , 2022b ]Weihao Yu, Mi Luo, Pan Zhou, Chenyang
Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and
Shuicheng Yan. Metaformer is actually what you need
for vision. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 10819–
10829, 2022.
[Yuan et al. , 2021 ]Li Yuan, Yunpeng Chen, Tao Wang, Wei-
hao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi
Feng, and Shuicheng Yan. Tokens-to-token vit: Training
vision transformers from scratch on imagenet. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 558–567, 2021.
[Zaheer et al. , 2020 ]Manzil Zaheer, Guru Guruganesh, Ku-
mar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-
ago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. Big bird: Transformers for longer se-
quences. Advances in Neural Information Processing Sys-
tems, 33:17283–17297, 2020.
[Zhang et al. , 2021a ]Jiaming Zhang, Kailun Yang, Angela
Constantinescu, Kunyu Peng, Karin M ¨uller, and Rainer
Stiefelhagen. Trans4trans: Efﬁcient transformer for trans-
parent object segmentation to help visually impaired peo-
ple navigate in the real world. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 1760–1770, 2021.
[Zhang et al. , 2021b ]Pengchuan Zhang, Xiyang Dai, Jian-
wei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng
Gao. Multi-scale vision longformer: A new vision trans-
former for high-resolution image encoding. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 2998–3008, 2021.
[Zhang et al. , 2022 ]Wenqiang Zhang, Zilong Huang,
Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu
Liu, Gang Yu, and Chunhua Shen. Topformer: Token
pyramid transformer for mobile semantic segmentation. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12083–12093,
2022.
[Zhang et al. , 2023 ]Qiming Zhang, Yufei Xu, Jing Zhang,
and Dacheng Tao. Vitaev2: Vision transformer advancedby exploring inductive bias for image recognition and be-
yond. International Journal of Computer Vision , pages
1–22, 2023.
[Zhuet al. , 2021 ]Chen Zhu, Wei Ping, Chaowei Xiao, Mo-
hammad Shoeybi, Tom Goldstein, Anima Anandkumar,
and Bryan Catanzaro. Long-short transformer: Efﬁcient
transformers for language and vision. Advances in Neural
Information Processing Systems , 34:17723–17736, 2021.