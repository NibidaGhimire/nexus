A Minimax Approach Against Multi-Armed
Adversarial Attacks Detection
Federica Granese?
Lix, Inria-Saclay
Institute Polytechnique de Paris
Sapienza University of Rome
Palaiseau, France
federica.granese@inria.frMarco Romanelli?
New York University
New York, NY , USA
mr6852@nyu.eduSiddharth Garg
New York University
New York, NY , USA
sg175@nyu.edu
Pablo Piantanida
International Laboratory on Learning Systems (ILLS)
CNRS - CentraleSup ¬¥elec
Montr ¬¥eal, Canada
pablo.piantanida@centralesupelec.fr
Abstract ‚ÄîMulti-armed adversarial attacks, in which multiple
algorithms and objective loss functions are simultaneously used
at evaluation time, have been shown to be highly successful
in fooling state-of-the-art adversarial examples detectors while
requiring no speciÔ¨Åc side information about the detection mech-
anism. By formalizing the problem at hand, we can propose a
solution that aggregates the soft-probability outputs of multiple
pre-trained detectors according to a minimax approach. The
proposed framework is mathematically sound, easy to implement,
and modular, allowing for integrating existing or future detectors.
Through extensive evaluation on popular datasets (e.g., CIFAR10
and SVHN), we show that our aggregation consistently outper-
forms individual state-of-the-art detectors against multi-armed
adversarial attacks, making it an effective solution to improve
the resilience of available methods.
Index Terms ‚ÄîTrustworthy AI, Minimax approach, Adversar-
ial Examples Detection
I. I NTRODUCTION
In recent years, the need for deep learning models that are
both reliable and accurate has sparked signiÔ¨Åcant interest in
the Ô¨Åeld of trustworthy AI across multiple research domains.
Efforts that aim to provide a deeper understanding of the limi-
tations and capabilities of deep learning models and to develop
methods that can improve their reliability and robustness in
real-world applications have been focused on several key areas.
Detection of misclassiÔ¨Åed samples [1]‚Äì[3], identiÔ¨Åcation of
out-of-distribution patterns [4]‚Äì[11], enhancement of model
robustness against adversarial attacks [12]‚Äì[17], and detection
of adversarial attacks [18]‚Äì[24] are the most relevant research
directions in the Ô¨Åeld.
In particular, we consider the problem of adversarial exam-
ples. These examples are crafted patterns speciÔ¨Åcally designed
starting from ‚Äònatural‚Äô or ‚Äòclean‚Äô samples to fool a model into
?equal contribution.
The work of Federica Granese was supported by the European Research
Council (ERC) project HYPATIA under the European Union‚Äôs Horizon 2020
research and innovation program. Grant agreement N. 835294.making incorrect predictions. To combat this issue, there are
two main strategies: robust training and adversarial detection.
Robust training (e.g., [12], [15], [25]‚Äì[27]) aims to make a
model more resistant to adversarial examples, while adversar-
ial detection (e.g., [18], [28], [29]) attempts to identify and
reject such examples. Our focus is on the second defense
strategy, adversarial detection. Recent Ô¨Åndings have shown
that attackers with little or no information about the speciÔ¨Åc
defense can still cause signiÔ¨Åcant damage, highlighting the
importance of ongoing research to develop robust and effective
adversarial detection methods.
Traditional detection methods are often evaluated using a
single attack strategy, which does not accurately reÔ¨Çect real-
world threats. More recent papers such as [30] highlight
the importance of testing proposed defenses against adaptive
attacks. These are attacks that are speciÔ¨Åcally designed to
target a speciÔ¨Åc defense method and take advantage of a large
amount of prior knowledge about the defense mechanism, such
as the loss function optimized by the defense. This worst-case
scenario evaluation is crucial when proposing new detection
methods to assess their robustness. However, it appears that
no defense is completely invulnerable when so much side
information is provided. Crucially, [31] has shown that even
attackers with less side information can easily fool a detector
using a combination of well-known attacks, without any prior
knowledge of the detector itself. In a multi-armed attack
scenario, a given pattern is perturbed using multiple strategies
and loss functions simultaneously1and detection is considered
successful only if all attacks are correctly identiÔ¨Åed.
Although [31] highlights the problem of simultaneous ad-
versarial attack detection, it does not provide a solution. In
this paper, we aim to address this issue by proposing a
simple yet effective method for aggregating multiple detection
1Henceforth, the terms ‚Äúmulti-armed‚Äù and ‚Äúsimultaneous‚Äù will be used
interchangeably.arXiv:2302.02216v1  [cs.CV]  4 Feb 2023methods to create a ‚Äúteam of experts‚Äù using a minimax
approach. Our proposed framework is highly Ô¨Çexible, allowing
for the combination of any existing or future supervised or
unsupervised method as long as its output can be interpreted as
a probability distribution over two categories. Additionally, our
modular aggregator allows pre-trained detectors to be reused
without additional training or data, and can be easily extended
to new detection methods.
A. Summary of contributions
Our contributions are threefold:
To the best of our knowledge, our proposed aggregation
framework is the Ô¨Årst to combine the expertise of different
adversarial examples detectors and address the problem
of simultaneous attack detection as highlighted in [31].
This method can aggregate pre-trained detectors without
the need for additional training.
From a theoretical perspective, we revisit the simultane-
ous attack detection problem as formulated in [31] and
formalize it as a minimax cross-entropy risk. Based on
this formulation, we derive a surrogate loss function and
use it to characterize our optimal soft-detector in Eq. (10),
leading to our proposed solution.
Empirical evaluations of our proposed solution on popular
datasets, such as CIFAR10 and SVHN, show that it leads
to higher and more consistent performance compared to
the state-of-the-art (SOTA) in the simultaneous attack
setup, even when using simple detectors that individually
perform worse than SOTA detectors, as demonstrated
in Sec. V.
B. Related works
a) Detection mechanisms:
Methods to defend deep models against adversarial attacks
can be grouped into two main families: methods that are
designed to increase the targeted model‚Äôs robustness by re-
training it [12], [15], [32]‚Äì[34], and methods engineered
to detect adversarial examples at evaluation time [19]‚Äì[24].
The work in [18] provides a recent and thorough survey
about the state-of-the-art detection methods, which fall under
two main categories: supervised andunsupervised . Detectors
within the former category extract features either directly from
the targeted network‚Äôs layer [19], [24] or by using statistical
tools [20], [21]. To do so, both natural and adversarial ex-
amples are necessary. Generally, the adversarial samples are
created according to a single Ô¨Åxed algorithm and a given loss
function, which are then also used to create the examples
at evaluation time. Methods falling under the unsupervised
category only rely on the features of natural samples that can
be extracted using different techniques (e.g., feature squeez-
ing[23]) or can be based on autoencoders training procedures
with the scope of minimizing the reconstruction error [22].
b) Attack algorithms:
Since [35] Ô¨Årst shed light on the problem, several machine
learning models, including state-of-the-art neural networks,
have been found to be vulnerable to adversarial examples.Over the years, a plethora of algorithms to generate adversarial
samples has been proposed and overall, we can group them
into two main categories: white-box andblack-box attacks. We
talk about white-box attacks when the adversary knows every-
thing about the target model (its architecture and weights).
Gradient-based attacks belong to this category. They rely on
Ô¨Ånding the perturbation direction, i.e., the sign of gradient at
each pixel of the input, that maximizes the attacker‚Äôs objective
value.
Examples of gradient-based attacks are the Fast Gradi-
ent Sign Method (FGSM) [32], the Basic Iterative Method
(BIM) [36] and the Projected Gradient Descent method
(PGD) [12]. BIM and PGD can be seen as iterative versions
of FGSM (one-step perturbation). Unlike BIM, PGD attacks
start from a random perturbation in L p-ball around the in-
put sample. Another powerful attack is the Carlini-Wagner
attack (CW) [37], which directly minimizes the additive noise
constrained by a function which assure the misclassiÔ¨Åcation
of the perturbed sample. We conclude the list of white-box
attacks by mentioning the DeepFool attack (DF) [38], which
is an iterative method based on a local linearization of the
targeted classiÔ¨Åer, and the resolution of the resulting simpliÔ¨Åed
adversarial problem. In the case of black-box attacks, the
adversary has no access to the internals of the target model,
hence it creates attacks by querying the model and monitoring
outputs of the model to attack. Examples of black-box attacks
are the Square Attack (SA) [39], which iteratively searches
for a random perturbation, and checks if it increases the
attacker‚Äôs objective at each step; the Hop Skip Jump attack
(HOP) [40] which estimates the gradient direction to perturb,
and the Spatial Transformation Attack (STA) [17] which
transforms the original samples by applying small translations
and rotations to them. It is worth to mention that there also
exists gray-box attacks, i.e. when the adversary knows the
training data but not the internals of the model. These attacks
rely on the transferability property of the adversarial examples:
to create attacks these methods build a substitute model that
performs the same task as the target model. A special class
of attacks are the so-called adaptive attacks [30], [37], [41],
[42] where attacks are speciÔ¨Åcally designed to target a given
defence. In this scenario, the attacker is supposed to have full
knowledge of both the targeted classiÔ¨Åer and the underlying
defence.
We refer to the survey in [18] and references therein for a
comprehensive discussion of these topics.
II. M AINDEFINITIONS AND PRELIMINARIES
Adversarial examples are carefully crafted input patterns
designed to deceive a target classiÔ¨Åer into making an incorrect
decision, while remaining as similar as possible to the original
sample. This section will provide a brief overview of the key
concepts related to this topic.
A. Target classiÔ¨Åer
LetXRdbe the input space and let Y=f1;:::;Cgbe
the label space related to a classiÔ¨Åcation task. We denote byPXYthe unknown data distribution over XY . Throughout
the paper, we refer to the classiÔ¨Åer withpbYjX(yjx;), i.e.
the parametric soft-probability model, where y2 Y ,bYis
random variable representing the classiÔ¨Åer‚Äôs inference, and
2represents the learned parameters. The function
h:X!RjYjoutputs the logits vector of the classiÔ¨Åer given
an input sample. The induced hard decision of the classiÔ¨Åer is
deÔ¨Åned asg:X!Y s.t.g(x) =arg maxy2YpbYjX(yjx;).
B. Adversarial problem
Let us consider a natural sample, denoted by x2X, along
with its true label, y2Y. An attacker aims to deceive the
modelgby crafting an adversarial example, x0
`2I Rd,
whereIis a held-out set of images that is distributed according
toPXYbut that was not used during training. The symbol `
denotes the objective loss function `(x;x0
`;)optimized by the
attacker;"is perturbation magnitude, and L p,p2f1;2;1gis
the norm constraint. The goal of the attack is to obtain an x0
`
such thatg(x0
`)6=g(x), in order to force the target model
to make a prediction error. As thoroughly investigated in [35],
the adversarial generation problem is difÔ¨Åcult to tackle and it
is commonly relaxed as follows
x`0x`0(x) = arg max
x`02Rd:kx`0 xkp<"`(x;x`0;); (1)
where x0
`is updated iteration by iteration starting from an
initial given value. The objective function `traditionally used
is the Adversarial Cross-Entropy (ACE) [12], [35]:
`ACE(x;x`0;) =EYjx
 logpbYjX(Yjx0
`;)
; (2)
where the expectation is understood to be over the ground true
conditional distribution of Ygivenx. Recent developments in
the Ô¨Åelds of robustness and misclassiÔ¨Åcation detection [1],
[13], [15] have inspired the work on multi-armed attacks
in [31], which incorporates novel objective functions for
generating diverse adversarial examples. These functions are
brieÔ¨Çy summarized below.
The Kullback-Leibler divergence (KL):
`KL(x;x`0;) =EbYjx"
log 
pbYjX(bYjx;)
pbYjX(bYjx0
`;)!#
:(3)
The Fisher-Rao objective (FR) [15]:
`FR(x;x`0;) = 2 arccos (E); (4)
whereE=P
y2Yq
pbYjX(yjx;)pbYjX(yjx0
`;).
The Gini Impurity score (Gini) [1]:
`Gini(;x`0;) = 1 sX
y2Yp2
bYjX(yjx0
`;): (5)
III. M ULTI -ARMED ADVERSARIAL ATTACK DETECTION
AND MEAD
Finding a framework to assess the robustness of adversarial
attack detection is crucial in establishing trust in this defense.
Except for defenses that are formally certiÔ¨Åed to be robustwithin a certain radius [43] and whose practical usability is still
under investigation and appears to be effective mainly against
black-box attacks [44], the majority of defenses presented
in the literature require extensive empirical evaluation. The
authors of [30] suggest that for each defense, adaptive attacks
should be handcrafted by providing side information to the
attacker on the internal mechanism of the defense mechanism.
For instance, revealing the loss function optimized by the
defense is often enough to craft powerful attacks by reversing
the gradient descent on natural samples.
While adaptive attacks require disclosing much information
about the defense mechanism, even more alarmingly, [31] has
exposed that much less information is required to mount multi-
arm attacks that drastically affect the performance of SOTA
adversarial detection mechanisms. In particular, according to
the latter framework, the target classiÔ¨Åer is attacked simultane-
ously with multiple attack strategies without extra information
on the speciÔ¨Åc detector. To create a set of simultaneous attacks,
multiple perturbed versions of the same natural input sample
are created according to the set of attack strategies, discarding
those that are unable to fool the target classiÔ¨Åer, perturbation
magnitude,", and the norm,Lp. The detector is then evaluated
on all the crafted adversarial examples, and only if all the
attacks are correctly detected is the detection successful.
Interestingly enough, [31] provides empirical evidence for the
‚Äúno-free-lunch-theorem‚Äù in [30], which states that for each
possible attack, a defense can be deceived that provides no
guarantees of robustness against any other attack. The multi-
armed attack scenario, in particular, suggests that there may
exist attacks that are just as damaging as adaptive attacks but
require much less information about the speciÔ¨Åc detector being
used, making this a more realistic and likely scenario to occur.
In this paper, we aim to investigate possible defenses against
the multi-armed attack scenario. To do this, we formalize the
problem and propose a solution incorporating an information-
theoretic minimax approach. An analysis of the adaptive attack
within our proposed framework can be found in Appendix D.
Finally, it is worth noting that recent work has started to look
for a connection between adversarial training and adversarial
examples detection [45]
IV. F ORMALIZATION OF THE PROBLEM OF DETECTING
MULTI -ARMED ADVERSARIAL ATTACKS
In this section, we begin by formalizing the problem of
multi-armed attacks as proposed in [31]. We then delve deeper
into the topic of optimal detectors, and demonstrate how to
apply our proposed solution to practical use-cases.
A. Statistical model
LetKbe the countable set of indexes corresponding to each
possible attack, e.g., based on various attack algorithms and
loss functions, as described in Sec. II-B. Let M=
P(k)
XZ:
k2K	
be the set of joint probability distributions on XZ
which are indexed with k;8k2K, whereXis the input
(feature) space and Z=f0;1gindicates a binary space label
for the adversarial example detection task. At the evaluationtime, the attacker selects an arbitrary strategy k2 K and
then samples an input according to p(k)
XjZ(xjz= 1) which
corresponds to the probability density function induced by
the chosen attack kwherep(k)
XjZ(xjz= 0) =pX(x)almost
surely corresponds to the probability distribution of the natural
samples. The learner is given a set of soft-detectors models:
Q=n
q(k)
bZju:U7! [0;1]2o
k2K;
which have possibly been trained to detect attacks according
to each strategy k2 K , e.g.,q(k)
bZjupbZjU(zju; k)with
parameters kandu2U =fh(x)jx2Rdgdenotes the
space of logits. The set of possible detectors Qis available
to the defender. However, the speciÔ¨Åc attack chosen by the
attacker at the test time is unknown. In the remainder of this
section, we formally devise an optimal detector that exploits
full knowledge of the set Q.
B. A novel objective for detection under simultaneous attacks
Consider a Ô¨Åxed input sample x0and let u0=h(x0).
Clearly, the problem at hand consists in Ô¨Ånding an optimal
soft-detector q?
bZju0that performs well simultaneously over all
possible attacks in K. This can be formalized as the solution
to the following minimax problem:
L(Q;x0) = min
qbZju0max
k2KEq(k)
bZju0h
 logqbZju0i
; (6)
which requires to solve equation 6 for Qand for each given
input sample x0. It is important to note that the minimization
is performed over all (detectors) distributions qbZju0, including
elements that are not part of the set Q.
That being said, the objective in Eq. (6) is not tractable
computationally. To overcome this issue, we derive a surrogate
(an upper bound) that can be computationally optimized. For
any arbitrary choice of qbZju0, we have
max
k2KEq(k)
bZju0h
 logqbZju0i

max
k2KEq(k)
bZju0h
 logq(k)
bZju0i
|{z}
=constant term+
+ max
k2KEq(k)
bZju02
4log0
@q(k)
bZju0
qbZju01
A3
5: (7)
Proof of Eq. (7).
max
k2KEq(k)
bZju0h
 logqbZju0i
=
= max
k2K2
4Eq(k)
bZju0h
 logq(k)
bZju0i
+Eq(k)
bZju02
4log0
@q(k)
bZju0
qbZju01
A3
53
5
max
k2KEq(k)
bZju0h
 logq(k)
bZju0i
+
+ max
k2KEq(k)
bZju02
4log0
@q(k)
bZju0
qbZju01
A3
5:Observe that the Ô¨Årst term in equation 7 of the upper bound
is constant w.r.t. the choice of qbZju0and the second term
is well-known as being equivalent to the average worst-case
regret [46]. This upper bound provides a surrogate to our
intractable objective in equation 6 that can be minimized over
allqbZju0. We can formally state our problem as follows:
~L(Q;x0) = min
qbZju0max
k2KEq(k)
bZju02
4log0
@q(k)
bZju0
qbZju01
A3
5=
= min
qbZju0max
P
E
h
DKL
q(
)
bZju0qbZju0i
;(8)
where the min is taken over all the possible distributions
qbZju0; and 
is a discrete random variable with P
de-
noting a generic probability distribution whose probabilities
are(!1;:::;!jKj), i.e.,P
(k) =!k; andDKL(k)is the
Kullback‚ÄìLeibler divergence, representing the expected value
of regret of qbZjUw.r.t. the worst-case distribution in Q.
Proof of Eq. (8).The equality hold by noticing that
max
P
E
h
DKL
q(
)
bZju0qbZju0i
max
k2KEq(k)
bZju02
4log0
@q(k)
bZju0
qbZju01
A3
5;
and moreover,
max
k2KEq(k)
bZju02
4log0
@q(k)
bZju0
qbZju01
A3
5=E
h
DKL
q(
)
bZju0qbZju0i
;
by choosing the random variable 
 with uni-
form probability over the set of maximizers
K= arg maxk2KEq(k)
bZju0"
log 
q(k)
bZju0
qbZju0!#
, zero otherwise.
The convexity of the KL-divergence allows us to
rewrite Eq. (8) as follows:
min
qbZju0max
P
E
h
DKL
q(
)
bZju0qbZju0i
=
= max
P
min
bqbZju0E
h
DKL
q(
)
bZju0qbZju0i
: (9)
Proof of Eq. (9).We consider a zero-sum game with a
concave-convex mapping deÔ¨Åned on a product of con-
vex sets. The sets of all probability distributions qbZju0andP
are two nonempty convex sets, bounded and Ô¨Å-
nite dimensional. On the other hand, 
P
;qbZju0
!
E
h
DKL
q(
)
bZju0qbZju0i
is a concave-convex mapping, i.e.,
P
!E
h
DKL
q(
)
bZju0qbZju0i
is concave and qbZju0!
E
h
DKL
q(
)
bZju0qbZju0i
is convex for every 
P
;qbZju0
.
Then, by classical min-max theorem [47] we have that Eq. (9)
holds.The solution to Eq. (9) provides the optimal distribution
P?

, i.e. the collection of weights fw?
kg, which leads to our
soft-detector [46]:
bq?
bZju0=X
k2Kw?
kq(k)
bZju0;withP?

= arg max
f!kgIu0(
;bZ);
(10)
whereIu0(;)denotes the Shannon mutual information be-
tween the random variable 
, distributed according to f!kg,
and the binary soft-prediction variable bZ, distributed according
toq(k)
bZju0and conditioned on the particular test example u0.
Proof of Eq. (10).It is enough to show that
min
bqbZju0E
h
DKL
q(
)
bZju0qbZju0i
=Iu0(
;bZ); (11)
for every random variable 
distributed according to an arbi-
trary probability distribution P
and each distribution q(
)
bZju0.
We begin by showing that
E
h
DKL
q(
)
bZju0qbZju0i
Iu0(
;bZ);
for any arbitrary distributions P
andq(
)
bZju0. To this end, we
use the following identities:
E
h
DKL
q(
)
bZju0qbZju0i
=E
Eq(
)
bZju00
@logq(
)
bZju0
qbZju01
A=
=E
Eq(
)
bZju00
@logq(
)
bZju0
PbZ1
A+DKL
PbZkqbZju0
=
=Iu0(
;bZ) +DKL
PbZkqbZju0
Iu0(
;bZ); (12)
wherePbZdenotes the marginal distribution of q(
)
bZju0w.r.t.
P
and the last inequality follows since the KL divergence
is positive. Finally, it is easy to check that by selecting
qbZju0=PbZthe lower bound in equation 12 is achieved which
proves the identity in expression equation 11. By taking the
maximum overall probability distributions P
at both sides of
expression equation 11 the claim follows.
From theory to our practical detector. According to our
derivation in Eq. (10), the optimal detector turns out to be
given by a mixture of the jKj detectors belonging to the
classQ, with weights carefully optimized to maximize the
mutual information between 
and the predicted variable bZ
for each detector in the class Q. Using this key ingredient, it
is straightforward to devise our optimal detector.
DeÔ¨Ånition 1. For any 01and a given x02X, let us
deÔ¨Åne the following detector D:Rd!f0;1g:
D(x0) =1h
q?
bZju0(^z= 1jh(x0))>i
; (13)
where 1[]is the indicator function.
Fig. 1: The shallow detectors are named after the loss function
used to craft the attacks they are trained to detect. Overall,
the SOTA method NSS clearly outperforms all the individual
shallow detectors. The aggregation we propose allows to use
the shallow models to attain a detector whose performance are
consistently comparable and in many cases better than SOTA.
V. E XPERIMENTAL RESULTS
We test our proposed solution by deploying it against the
multi-armed adversarial attacks framework introduced in [31],
and by evaluating its detection performance. The source code
to reproduce our results can be found in the Supplementary
Material.
In our empirical evaluation, we assume that a third party
provides us with four simple supervised detectors. Each of
them is trained to detect a single speciÔ¨Åc kind of attack. This
is a reasonable assumption, as many methods in the literature
are able to successfully detect at least one type of attack and
fail at detecting others. In addition, to emphasize the role
played by the proposed method, these detectors are merely
shallow networks (3 fully-connected layers with 256 nodes
each), which are only allowed to observe the logits of the
target classiÔ¨Åer to distinguish between natural and adversarial
samples. Due to their speciÔ¨Åcs, these individual shallow detec-
tors are bound to perform very poorly, i.e. much worse than
SOTA detectors, against attacks they have not been trained
on, as shown in Fig. 1. This aspect enhances the value of our
solution, which attains favorable performance by aggregating
detectors that individually exhibit subpar performance w.r.t.
SOTA adversarial examples detection methods.
A. Evaluation framework
Evaluation setup: M EAD. We consider all the attack
algorithms mentioned in M EAD [31], and we group them by
the corresponding norm and the perturbation magnitude. For
each natural sample and each gradient-based attack algorithm
(i.e., FGSM, PGD or BIM), we create four adversarial
examples, each corresponding to one of the loss functions
described in Sec. II-B. Table I reports all the attacks in the
multi-armed setting. Each cell corresponds to a group of(a)
 (b)
(c)
Fig. 2: Performance of the various detectors grouped by L p-norm and perturbation magnitude "on CIFAR10. Each shallow
detector is named after the loss function used to craft the attacks they it is trained to detect. The plot shows how our method
consistently attains better performance the the single one on all the different adversarial attacks, supporting the claim of
optimality in Sec. IV.
attacks crafted according to the algorithm (reported in the
cell), the associated norm (indicated by the column label)
and perturbation magnitude (indicated by the row label) and
one of the considered four loss functions. Thus, for example,
when we consider L 1norm and"= 0:125, the detector is
evaluated on 4 + 4 + 4 + 1 = 13 simultaneous adversarial
attacks. Note that we discard the perturbed examples that
do not fool the classiÔ¨Åer as, by deÔ¨Ånition, they are neither
natural nor adversarial.
Evaluation metrics . Following the evaluation setup
described above, for each sample and for each group
of attacks corresponding to each cell in Tab. I we consider
a detection successful, i.e. a true positive, if and only
if all the adversarial attacks are detected. Otherwise, we
report a false negative. We use the classical deÔ¨Ånitions of
true negative and false positive for the natural samples
detection. This means that a true negative is a natural sampledetected as natural, and a false positive is a natural sample
detected as adversarial. We measure the performance of
the detectors in terms of i)AUROC"%[48] (the Area
Under the Receiver Operating Characteristic curve ) which
represents the ability of the detector to discriminate between
adversarial and natural examples (higher is better); ii)
FPR at 95 % TPR (FPR #95%%), i.e., the percentage of
natural examples detected as adversarial when 95 % of the
adversarial examples are detected (lower is better).
Datasets and pre-trained classiÔ¨Åers . We run our experiments
on CIFAR10 [49] and SVHN [50] image datasets. For both,
the pre-trained target classiÔ¨Åer is a ResNet-18 models that
has been trained for 100 epochs, using SGD optimizer with
a learning rate equal to 0:1, weight decay equal to 10 5,
and momentum equal to 0:9. The accuracy achieved by the
classiÔ¨Åers on the original clean data is 99% for CIFAR10 and
100% for SVHN over the train split; 93.3% for CIFAR10 andTABLE I: M EAD. Each cell corresponds to attacks simulta-
neously executed on the targeted classiÔ¨Åer. Attacks created
using all the losses in Sec. II-B are marked with?. Attacks
such as SA and DF are not dependent on the choice for
the loss, but are equally considered as part of the multi-
armed framework. Empty cells correspond to combinations of
perturbation magnitude and norm constraint that are usually
not considered in the literature.
L1 L2 L1 No norm
"= 0:01 - CW2 - -
"= 0:03125 - - PGDi?,FGSM?,BIM?-
"= 0:0625 - - PGDi?,FGSM?,BIM?-
"= 0:1 - HOP - -
"= 0:125 - PGD2?PGDi?,FGSM?,BIM?,SA -
"= 0:25 - PGD2?PGDi?,FGSM?,BIM?-
"= 0:3125 - PGD2?PGDi?,FGSM?,BIM?,CWi -
"= 0:5 - PGD2?PGDi?,FGSM?,BIM?-
"= 1 - PGD2?- -
"= 1:5 - PGD2?- -
"= 2 - PGD2?- -
"= 5 PGD1?- - -
"= 10 PGD1?- - -
"= 15 PGD1?- - -
"= 20 PGD1?- - -
"= 25 PGD1?- - -
"= 30 PGD1?- - -
"= 40 PGD1?- - -
No" - DF - -
max. rotation = 30
max. translation = 8- - - STA
95.5% for SVHN over the test split.
Detectors . The proposed method aggregates four simple
pre-trained detectors. The detectors are four fully-connected
neural networks, composed of 3 layers of 256 nodes each. All
the detectors are trained for 100 epochs, using SGD optimizer
with learning rate of 0.01 and weight decay 0.0005. They
are trained to distinguish between natural and adversarial
examples created according to the PGD algorithm, under L 1
norm constraint and perturbation magnitude "= 0:125 for
CIFAR10 and "= 0:25for SVHN. Each detector is trained
on natural and adversarial examples generated using one of
the loss functions mentioned in Sec. II-B (i.e., ACE Eq. (2),
KL Eq. (3), FR Eq. (4), or Gini Eq. (5)) to craft its adversarial
training samples. We want to point out that the purpose of
this paper is not creating a new supervised detector, but rather
to show a method to aggregate a set of pre-trained detectors.
Moreover, it is important to notice that either supervised and
unsupervised methods can be added to or pool of experts
(cf. Appendix C1), provided that they output a conÔ¨Ådence on
the input sample being or not an adversarial example. Wefurther expand on the selection of the "parameter of the
adversarial examples used at training time in Appendix C3
(cf. Tabs. VII and IX).
NSS [19] . We compare the proposed method with NSS,
which is the best among the supervised SOTA methods against
multi-armed adversarial attacks (cf. [31]). NSS characterizes
the adversarial perturbations through the use of natural scene
statistics , i.e., statistical properties that can be altered by
the presence of adversarial perturbations. NSS is trained by
using PGD algorithm, L 1norm constraint and perturbation
magnitude"= 0:03125 for CIFAR10 and "= 0:0625 for
SVHN. We further expand on the selection of the "parameter
of the adversarial examples used at training time in Tabs. VI
and VIII and Appendix C3.
On the optimization of Eq. (10) . For the optimization
of Eq. (10), we rely on the SciPy [51] library, the
optimize package, and the minimize function which
uses the Sequential Least Squares Programming (SLSQP)
algorithm to Ô¨Ånd the optimum. Further details can be found
in Appendix A.
B. Discussion
We now present the main experimental results to show
the effectiveness of the proposed aggregation method for
adversarial attack detection. Further discussion on these
results, as well as additional experiments can be found
in Appendix B.
1) The shallow detectors
Figs. 1 to 3 provides a graphical interpretation of the
detection performance when ResNet18, trained on CIFAR10,
is the target classiÔ¨Åer. The single detectors are named after
the loss function used to craft the adversarial examples on
which each detector is trained along with the natural samples.
The main takeaway from Fig. 1 is the observation that,
when considered individually, the shallow detectors are clearly
subpar w.r.t. state of the art adversarial attacks detection
mechanism. On the contrary, the aggregation provided by our
method results in detection performance that are comparable
to SOTA performance and, in some cases, outperform well
established detection mechanisms.
Figure 2 sheds light on the fact that the mixture of experts
attained by our proposed method can consistently improve
the detection of adversarial examples over several multi-
armed attacks mounted using different norms and perturbation
magnitudes.
One main takeaway of this paper is that, if we are
provided with generally non-robust detectors whose
performance is good only against a limited amount of
attacks (as it is conÔ¨Årmed by Figs. 1 and 2), we can
successfully aggregate them through the proposed method
to obtain a consistently better detection.(a) Attacks crafted with PGD algorithm, the FR loss,
"= 40 , and norm constraint L 1
(b) Attacks crafted with FGSM algorithm, the FR loss,
"= 40 , and norm constraint L 1
(c) Ours against attacks crafted with PGD algorithm,
the FR loss, "= 40 , and norm constraint L 1
(d) NSS against attacks crafted with PGD algorithm,
the FR loss, "= 40 , and norm constraint L 1
Fig. 3: Discrimination performances. In Fig. 3a and Fig. 3b, the accuracies of the detectors on natural and adversarial examples;
in Fig. 3c and Fig. 3d we show how the proposed method and NSS split the data samples. We report the results for the detection
of adversarial examples in pink, and the results for the detection of natural examples in blue.
In Fig. 3 we consider attacks crafted according to the
PGD algorithm, the FR loss, "= 40 , and norm constraint L 1
(cf. Figs. 3a, 3c and 3d), and attacks crafted according to the
FGSM algorithm, FR loss, "= 0:5, and L1norm in Fig. 3b.
We also report the performance of the considered detectors in
terms of detection accuracy over the natural examples in blue
and the adversarial examples in pink. As we can observe, the
individual detectors, which are named after the loss functions
ACE, FR, KL, and Gini, exhibit different behaviors for the
speciÔ¨Åc attack. In Fig. 3a, the Gini detector drastically fails
at detecting the attack as its accuracy plummets to 0% on
the adversarial examples. In the same way, the FR and KL
detectors but mostly the ACE detector, perform poorly against
FGSM (cf. Fig. 3b). On the contrary, our method, beneÔ¨Åting
from the aggregation, obtains favorable results in both cases,
conÔ¨Årming what we had previously observed.
The histograms in Figs. 3c and 3d show how the method
we propose and NSS separate natural (blue) and adversarial
examples (pink), respectively. The values along the horizontal
axis represent the probability of being classiÔ¨Åed as adversarial,
and the vertical axis represents the frequency of the sampleswithin the bins. The detection error is proportional to the area
of overlap between the blue and the pink histograms. Fig. 3c
and Fig. 3d suggest that the proposed method achieves lower
detection error on the considered attack, as it is conÔ¨Årmed
in Tab. II where our proposed method attains 92.1 AUROC "%,
while NSS only achieves 76.1 AUROC "% and. Additional
plots are provided in Appendix F.
In particular, the performance attained by the proposed
method is consistent across the larges part of the considered
multi-armed adversarial attacks, as conÔ¨Årmed in Tab. II
and Fig. 1.
2) Evaluation of the proposed aggregator in MEAD
On CIFAR10 , our aggregator achieves maximum AUROC
improvement w.r.t. NSS is 79.5 percentage points and hap-
pens for attacks under L1-norm constraint, "= 0:125 and
PGD?, FGSM?, BIM?, SA, i.e. when as many as 13 different
simultaneous adversarial attacks are mounted. Similarly, for
our proposed method the maximum attained FPR at 95%
TPR improvement w.r.t. NSS is 90.3 percentage points and
happens for attacks under L1-norm constraint, "= 0:5and
PGD?, FGSM?, BIM?, i.e., when as many as 12 differentTABLE II: Comparison between the proposed method and NSS on CIFAR10 and SVHN. The?symbol means the perturbation
mechanism is executed in parallel four times starting from the same original clean sample, each time using one of the objective
losses between ACE Eq. (2), KL Eq. (3), FR Eq. (4), Gini Eq. (5).
CIFAR10 SVHN
NSS Ours NSS Ours
AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1?
"= 5 48.5 94.2 62.1 87.1 40.2 91.3 76.9 79.0
"= 10 54.0 90.3 56.8 90.6 36.9 91.3 73.0 82.5
"= 15 58.8 86.8 69.3 84.4 35.6 91.3 78.9 72.5
"= 20 63.5 82.3 78.7 73.1 36.1 91.3 83.6 60.7
"= 25 67.7 77.2 87.1 50.8 37.8 91.3 87.0 48.6
"= 30 71.4 73.4 90.3 35.4 39.8 91.3 89.3 37.2
"= 40 76.1 67.3 92.1 26.4 43.1 91.3 92.6 20.0
Norm L 2
PGD2?
"= 0:125 48.3 94.3 63.9 85.4 40.8 91.3 80.2 74.5
"= 0:25 53.2 91.2 57.1 90.5 37.2 91.3 74.0 81.7
"= 0:3125 55.8 89.2 61.0 88.9 36.1 91.3 75.2 79.4
"= 0:5 63.3 82.6 79.4 73.2 35.9 91.3 82.5 64.4
"= 1 76.4 67.5 91.4 26.4 42.5 91.3 92.3 24.7
"= 1:5 81.0 63.0 91.9 24.2 46.3 91.3 94.1 7.5
"= 2 82.6 62.3 91.9 24.1 49.8 91.3 94.9 5.3
DeepFool
No" 57.0 91.7 81.9 54.8 41.3 91.3 94.9 12.0
CW2
"= 0:01 56.4 90.8 53.4 92.2 41.0 91.3 54.2 92.0
HOP
"= 0:1 66.1 87.0 86.1 49.1 67.6 84.2 96.0 10.2
Norm L 1
PGDi?, FGSM?, BIM?
"= 0:03125 83.0 55.3 82.3 59.7 86.3 46.9 81.4 64.9
"= 0:0625 96.0 17.2 92.0 29.6 88.9 0.7 89.1 33.3
"= 0:25 97.3 0.6 95.9 8.8 51.6 88.9 92.3 16.4
"= 0:5 82.5 100.0 94.6 9.7 46.7 86.7 92.9 14.4
PGDi?, FGSM?, BIM?, SA
"= 0:125 9.4 99.9 88.9 40.8 32.9 91.3 89.2 29.1
PGDi?, FGSM?, BIM?, CWi
"= 0:3125 63.2 99.1 80.0 61.1 41.3 91.3 88.2 33.1
No norm
STA
No" 88.5 38.8 82.7 52.4 91.2 0.2 90.2 23.2
simultaneous adversarial attacks are mounted. Our aggregator
outperforms NSS in the case of the attacks with L 1and L 2
norm, regardless of the algorithm or the perturbation magni-
tude, and in the case of L 1norm with large perturbations.
However, for the attacks with L 1norm and small ", although
the proposed method‚Äôs performance is comparable to that
of NSS, we notice a slight degradation. To shed light on
this, we remind that individual detectors aggregated are based
on the classiÔ¨Åer‚Äôs logits; NSS, on the other hand, extracts
natural scene statistics from the inputs. This more sophisticated
technique makes NSS perform well when tested on attacks
with similar "and the same norm as the ones seen at training
time. Similar conclusions can be drawn for the results onSVHN (cf. Tab. II).
Tab. III shows the modularity of the proposed method when
SOTA detection methods, NSS (a) and FS (b), are plugged in
as a Ô¨Åfth detector. We test Ours+NSS on the attacks on which
our aggregator was outperformed by the competitors. In all
the cases, Ours+NSS outperforms ‚ÄúOurs‚Äù either in terms of
AUROC and FPR. In most of the cases, Ours+NSS is also
better than the individual competitor. In Appendix C1 we
provide further insights on this by showing that the same
behavior is observed when we plug a SOTA unsupervised
method as Ô¨Åfth detector in our pool.TABLE III: Comparison between Ours and Ours+NSS on
CIFAR10. The?symbol means the perturbation mechanism is
executed in parallel four times starting from the same original
clean sample, each time using one of the objective losses
between ACE Eq. (2), KL Eq. (3), FR Eq. (4), Gini Eq. (5).
We focus only in the cases in which the proposed method is
outperformed from the corresponding competitors.
CIFAR10
Ours Ours+NSS
AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 2
CW2
"= 0:01 53.4 92.2 54.1 91.3
Norm L 1
PGDi?, FGSM?, BIM?
"= 0:03125 82.3 59.7 89.9 34.4
"= 0:0625 92.0 29.6 96.4 9.0
"= 0:25 95.9 8.8 96.7 3.5
No norm
STA
No" 82.7 52.4 87.3 35.4
3) Evaluation of the proposed aggregator in the non-
simultaneous setting
In these experiments, we move from the simultaneous
adversarial attack scenario to one where the different detectors
are aggregated to detect one single attack at a time, as usually
done in the literature. We report the complete results Tab. IV.
Crucially, these experiments show that ensemble detectors
can also improve the performance for speciÔ¨Åc attacks. In
particular, we would like to draw attention to the fact that we
outperform NSS in the vast majority of the cases. Moreover,
we achieve a maximum gain of 82.8 percentage points in
terms of AUROC "% (cf. SA attack) and 97.6 percentage
points in terms of FPR #95%% (cf. FGSM with "= 0:5
attack). On the other side, the competitor outperforms our
proposed method only in a few cases, achieving a maximum
gain of 5.9 percentage points in terms of AUROC "% and
27.4 percentage points in terms of FPR #95%% (cf. FGSM
with"=0.03125 attack in both the cases), and these gains are
much lower than those obtained by the proposed method.
VI. F INAL REMARKS
We introduced a new method to tackle the multi-armed
adversarial attacks introduced in M EAD [31]. We formalized
the multi-armed attack detection problem as a minimax cross-
entropy risk and derived a surrogate loss function. Based on
this, we characterized our optimal soft-detector which results
in a mixture of experts as the solution to a minimax problem.
Our empirical results show that aggregating simple detectors
using our method results in consistently improved detection
performance. The achieved performance is comparable and in
large set of cases better than the best state-of-the-art (SOTA)
method in the multi-armed attack scenarios. Our method
has two key beneÔ¨Åts: it is modular, allowing existing and
future methods to be integrated, and it is general, able to
recognize adversarial examples from various attack algorithmsand loss functions. Additionally, our aggregator can potentially
be extended to aggregate both supervised and unsupervised
SOTA adversarial detection methods.
As future work, it would be interesting to apply our detector
aggregator to topics beyond simultaneous adversarial attack
detection. As long as the detector outputs can be interpreted
as a probability distribution across two categories, any existing
or future supervised or unsupervised method can be combined
using our proposed approach, making the aggregator a new
ensemble technique. An example of this extension is intrusion
detection, where an improved detection framework is highly
desired, particularly with the use of ensemble learners [52].
Limitations of the proposed method come from the fact it
relies on a collection of detectors whose expertise is combined
to obtain a more robust adversarial detection. Such models
could be potentially poisoned by a malicious actor, drastically
reducing the aggregator‚Äôs reliability. We think this could have
a potentially severe societal impact if the proposed method
happened to be deployed with no additional checks on the
quality of the available detectors.TABLE IV: The proposed method and NSS in the non-simultaneous setting. The column names ACE, KL, FR, and Gini denote
the loss function used to craft the attacks. HOP, DeepFool, CW2, and STA attacks have already been considered individually
in Tab. II.
CIFAR10
Ours AUROC"% (FPR#95%%) ‚Äì NSS AUROC "% (FPR#95%%)
ACE KL FR Gini
PGD1
"=5 66.2 (83.6) ‚Äì 49.9 (93.5) 64.2 (85.7) ‚Äì 49.6 (93.0) 63.0 (87.1) ‚Äì 49.9 (93.3) 80.7 (58.4) ‚Äì 50.3 (93.2)
"=10 62.6 (87.5) ‚Äì 56.9 (88.4) 62.3 (88.2) ‚Äì 56.6 (88.3) 63.1 (86.5) ‚Äì 57.0 (88.1) 86.9 (46.0) ‚Äì 57.1 (88.8)
"=15 74.2 (81.4) ‚Äì 63.1 (83.0) 75.2 (80.6) ‚Äì 62.8 (83.1) 75.3 (79.4) ‚Äì 63.2 (82.5) 90.0 (31.1) ‚Äì 63.5 (84.0)
"=20 86.8 (65.3) ‚Äì 68.5 (77.1) 87.5 (63.1) ‚Äì 68.1 (77.3) 86.9 (63.3) ‚Äì 68.7 (76.4) 91.7 (31.2) ‚Äì 69.9 (77.6)
"=25 93.9 (38.4) ‚Äì 73.1 (71.1) 94.3 (36.2) ‚Äì 72.7 (71.8) 93.7 (41.1) ‚Äì 73.4 (70.9) 92.3 (28.9) ‚Äì 75.0 (71.4)
"=30 97.1 (12.3) ‚Äì 77.1 (64.5) 97.2 (12.6) ‚Äì 76.8 (65.1) 96.8 (15.9) ‚Äì 77.4 (65.2) 92.6 (27.9) ‚Äì 78.6 (67.3)
"=40 98.9 (1.0) ‚Äì 83.5 (52.7) 99.0 (1.0) ‚Äì 83.3 (53.5) 98.8 (1.0) ‚Äì 83.6 (52.7) 92.7 (27.4) ‚Äì 80.1 (64.9)
PGD2
"=.125 67.9 (81.1) ‚Äì 49.5 (93.8) 65.4 (84.3 ) ‚Äì 49.1 (93.5) 63.9 (86.6) ‚Äì 49.6 (93.5) 80.6 (58.4) ‚Äì 49.5 (94.3)
"=.25 62.3 (87.5) ‚Äì 55.9 (89.1) 62.1 (88.0) ‚Äì 55.6 (89.2) 62.6 (87.6) ‚Äì 55.8 (89.4) 86.7 (46.5) ‚Äì 55.9 (89.8)
"=.3125 66.5 (86.1) ‚Äì 59.4 (86.5) 67.0 (85.9) ‚Äì 59.0 (86.6) 67.8 (84.8) ‚Äì 59.3 (86.6) 88.4 (42.2) ‚Äì 59.3 (87.7)
"=.5 86.4 (67.1) ‚Äì 68.3 (77.4) 87.2 (64.5) ‚Äì 68.0 (77.4) 86.7 (64.0) ‚Äì 68.4 (77.2) 91.4 (31.4) ‚Äì 69.0 (78.7)
"=1 98.9 (0.9) ‚Äì 84.4 (50.6) 98.9 (0.9) ‚Äì 84.3 (50.5) 98.8 (0.9) ‚Äì 84.7 (50.7) 92.5 (27.2) ‚Äì 79.3 (66.8)
"=1.5 99.2 (0.9) ‚Äì 92.8 (28.7) 99.3 (0.9) ‚Äì 92.7 (28.9) 99.3 (0.7) ‚Äì 93.0 (27.3) 92.5 (27.2) ‚Äì 79.5 (66.5)
"=2 99.3 (0.8) ‚Äì 96.8 (13.9) 99.3 (0.8) ‚Äì 96.9 (13.1) 99.3 (0.9) ‚Äì 95.9 (17.2) 92.5 (27.2) ‚Äì 79.5 (66.5)
PGDi
"=.03125 99.1 (0.9) ‚Äì 92.3 (31.0) 99.1 (0.9) ‚Äì 92.1 (31.9) 99.0 (0.9) ‚Äì 92.2 (30.7) 94.8 (21.5) ‚Äì 89.0 (44.0)
"=.0625 99.3 (0.8) ‚Äì 99.1 (3.3) 99.3 (0.8) ‚Äì 99.1 (3.3) 99.3 (0.8) ‚Äì 99.1 (3.6) 97.4 (8.0) ‚Äì98.1 (8.1)
"=.125 99.3 (0.7) ‚Äì 99.7 (0.6) 99.3 (0.9) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.6 (0.6) 97.3 (7.3) ‚Äì 99.6 (0.6)
"=.25 99.3 (0.7) ‚Äì 99.7 (0.6) 99.3 (0.9) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 97.1 (7.3) ‚Äì 99.6 (0.6)
"=.3125 99.3 (0.9) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 97.1 (7.4) ‚Äì 99.7 (0.6)
"=.5 99.3 (0.8) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 97.1 (7.3) ‚Äì 99.6 (0.6)
FGSM
"=.03125 89.2 (47.5) ‚Äì 94.1 (26.7) 91.3 (40.6) ‚Äì 94.0 (27.0) 92.6 (34.1) ‚Äì 96.8 (15.0) 90.7 (42.7) ‚Äì 96.6 (15.3)
"=.0625 96.4 (18.5) ‚Äì 99.4 (1.3) 96.2 (18.7) ‚Äì 99.4 (1.4) 97.6 (10.3) ‚Äì 99.6 (0.6) 97.4 (11.9) ‚Äì 99.6 (0.6)
"=.125 99.3 (3.4) ‚Äì 99.7 (0.6) 99.1 (4.3) ‚Äì 99.7 (0.6) 99.3 (2.5) ‚Äì 99.5 (0.6) 99.3 (2.4) ‚Äì 99.5 (0.6)
"=.25 99.8 (0.6) ‚Äì 99.7 (0.6) 99.7 (0.8) ‚Äì 99.7 (0.6) 99.6 (1.1) ‚Äì 97.9 (0.6) 99.6 (1.1) ‚Äì 97.7 (0.6)
"=.3125 99.7 (0.9) ‚Äì 99.7 (0.6) 99.7 (0.9) ‚Äì 99.7 (0.6) 99.5 (1.5) ‚Äì 95.8 (0.6) 99.5 (1.5) ‚Äì 95.6 (0.6)
"=.5 99.0 (4.9) ‚Äì 99.7 (0.6) 99.2 (2.7) ‚Äì 99.7 (0.6) 99.2 (2.4) ‚Äì 84.9 (100.0) 99.2 (2.4) ‚Äì 84.8 (100.0)
BIM
"=.03125 98.3 (4.6) ‚Äì 90.3 (37.7) 98.3 (4.4) ‚Äì 90.2 (38.1) 97.8 (7.2) ‚Äì 90.5 (37.0) 92.2 (32.6) ‚Äì 88.2 (45.1)
"=.0625 99.4 (0.8) ‚Äì 98.2 (7.5) 99.4 (0.9) ‚Äì 98.2 (7.5) 99.4 (0.8) ‚Äì 98.3 (7.3) 96.6 (13.1) ‚Äì 97.3 (12.9)
"=.125 99.3 (0.9) ‚Äì 99.6 (0.7) 99.3 (0.9) ‚Äì 99.7 (0.7) 99.3 (0.8) ‚Äì 99.6 (0.7) 97.8 (6.9) ‚Äì 99.3 (1.9)
"=.25 99.3 (0.8) ‚Äì 99.7 (0.6) 99.3 (0.9) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 97.4 (7.2) ‚Äì 99.6 (0.6)
"=.3125 99.3 (0.9) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 99.3 (0.9) ‚Äì 99.7 (0.6) 97.1 (7.4) ‚Äì 99.7 (0.6)
"=.5 99.3 (0.8) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 99.3 (0.8) ‚Äì 99.7 (0.6) 96.3 (7.3) ‚Äì 99.7 (0.6)
SA
"=.125 91.2 (39.6) ‚Äì 9.4 (99.9) 91.2 (39.6) ‚Äì 9.4 (99.9) 91.2 (39.6) ‚Äì 9.4 (99.9) 91.2 (39.6) ‚Äì 9.4 (99.9)
CWi
"=.3125 80.7 (60.8) ‚Äì 64.6 (89.8) 80.7 (60.8) ‚Äì 64.6 (89.8) 80.7 (60.8) ‚Äì 64.6 (89.8) 80.7 (60.8) ‚Äì 64.6 (89.8)REFERENCES
[1] F. Granese, M. Romanelli, D. Gorla, C. Palamidessi, and P. Piantanida,
‚ÄúDOCTOR: A simple method for detecting misclassiÔ¨Åcation errors,‚Äù
inAdvances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS
2021, December 6-14, 2021, virtual (M. Ranzato, A. Beygelzimer, Y . N.
Dauphin, P. Liang, and J. W. Vaughan, eds.), pp. 5669‚Äì5681, 2021.
[2] Y . Geifman and R. El-Yaniv, ‚ÄúSelectivenet: A deep neural network with
an integrated reject option,‚Äù in Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA (K. Chaudhuri and R. Salakhutdinov, eds.),
vol. 97 of Proceedings of Machine Learning Research , pp. 2151‚Äì2159,
PMLR, 2019.
[3] A. Gangrade, A. Kag, and V . Saligrama, ‚ÄúSelective classiÔ¨Åcation via
one-sided prediction,‚Äù in The 24th International Conference on ArtiÔ¨Åcial
Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual
Event (A. Banerjee and K. Fukumizu, eds.), vol. 130 of Proceedings of
Machine Learning Research , pp. 2179‚Äì2187, PMLR, 2021.
[4] E. D. C. Gomes, F. Alberge, P. Duhamel, and P. Piantanida, ‚ÄúIgeood: An
information geometry approach to out-of-distribution detection,‚Äù CoRR ,
vol. abs/2203.07798, 2022.
[5] A. Vyas, N. Jammalamadaka, X. Zhu, D. Das, B. Kaul, and T. L. Willke,
‚ÄúOut-of-distribution detection using an ensemble of self supervised
leave-out classiÔ¨Åers,‚Äù in ECCV (8) , pp. 560‚Äì574, 2018.
[6] C. S. Sastry and S. Oore, ‚ÄúDetecting out-of-distribution examples with
Gram matrices,‚Äù in Proceedings of the 37th International Conference on
Machine Learning (H. D. III and A. Singh, eds.), vol. 119 of Proceedings
of Machine Learning Research , pp. 8491‚Äì8501, PMLR, 13‚Äì18 Jul 2020.
[7] Y . Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. Dillon,
B. Lakshminarayanan, and J. Snoek, ‚ÄúCan you trust your model's uncer-
tainty? evaluating predictive uncertainty under dataset shift,‚Äù in Advances
in Neural Information Processing Systems (H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch ¬¥e-Buc, E. Fox, and R. Garnett, eds.), vol. 32,
Curran Associates, Inc., 2019.
[8] W. Liu, X. Wang, J. Owens, and Y . Li, ‚ÄúEnergy-based out-of-distribution
detection,‚Äù Advances in Neural Information Processing Systems , 2020.
[9] D. Hendrycks and K. Gimpel, ‚ÄúA baseline for detecting misclassiÔ¨Åed
and out-of-distribution examples in neural networks,‚Äù in International
Conference on Learning Representations , 2017.
[10] L. H. Zhang, M. Goldstein, and R. Ranganath, ‚ÄúUnderstanding failures in
out-of-distribution detection with deep generative models,‚Äù in Proceed-
ings of the 38th International Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang, eds.),
vol. 139 of Proceedings of Machine Learning Research , pp. 12427‚Äì
12436, PMLR, 2021.
[11] Z. Lin, S. D. Roy, and Y . Li, ‚ÄúMOOD: multi-level out-of-distribution
detection,‚Äù in IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2021, virtual, June 19-25, 2021 , pp. 15313‚Äì15323, Com-
puter Vision Foundation / IEEE, 2021.
[12] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, ‚ÄúTowards
deep learning models resistant to adversarial attacks,‚Äù in 6th Interna-
tional Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings ,
2018.
[13] H. Zhang, Y . Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan,
‚ÄúTheoretically principled trade-off between robustness and accuracy,‚Äù in
International Conference on Machine Learning , pp. 1‚Äì11, 2019.
[14] J.-B. Alayrac, J. Uesato, P.-S. Huang, A. Fawzi, R. Stanforth, and
P. Kohli, ‚ÄúAre labels required for improving adversarial robustness?,‚Äù in
Advances in Neural Information Processing Systems , pp. 12214‚Äì12223,
2019.
[15] M. Picot, F. Messina, M. Boudiaf, F. Labeau, I. B. Ayed, and P. Pi-
antanida, ‚ÄúAdversarial robustness via Ô¨Åsher-rao regularization,‚Äù IEEE
Transactions on Pattern Analysis & Machine Intelligence , 2022.
[16] A. Robey, L. Chamon, G. J. Pappas, H. Hassani, and A. Ribeiro, ‚ÄúAdver-
sarial robustness with semi-inÔ¨Ånite constrained learning,‚Äù Advances in
Neural Information Processing Systems , vol. 34, pp. 6198‚Äì6215, 2021.
[17] L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry, ‚ÄúExploring
the landscape of spatial robustness,‚Äù in Proceedings of the 36th Interna-
tional Conference on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA (K. Chaudhuri and R. Salakhutdinov, eds.),
vol. 97 of Proceedings of Machine Learning Research , pp. 1802‚Äì1811,
PMLR, 2019.[18] A. Aldahdooh, W. Hamidouche, S. A. Fezza, and O. Deforges, ‚ÄúAd-
versarial example detection for dnn models: A review and experimental
comparison,‚Äù ArtiÔ¨Åcial Intelligence Review , 2022.
[19] A. Kherchouche, S. A. Fezza, W. Hamidouche, and O. D ¬¥eforges,
‚ÄúDetection of adversarial examples in deep neural networks with natural
scene statistics,‚Äù in 2020 International Joint Conference on Neural
Networks, IJCNN 2020, Glasgow, United Kingdom, July 19-24, 2020 ,
pp. 1‚Äì7, IEEE, 2020.
[20] X. Ma, B. Li, Y . Wang, S. M. Erfani, S. N. R. Wijewickrema,
G. Schoenebeck, D. Song, M. E. Houle, and J. Bailey, ‚ÄúCharacter-
izing adversarial subspaces using local intrinsic dimensionality,‚Äù in
6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings , 2018.
[21] K. Lee, K. Lee, H. Lee, and J. Shin, ‚ÄúA simple uniÔ¨Åed framework for de-
tecting out-of-distribution samples and adversarial attacks,‚Äù in Advances
in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montr ¬¥eal, Canada (S. Bengio, H. M. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds.), pp. 7167‚Äì7177,
2018.
[22] D. Meng and H. Chen, ‚ÄúMagnet: A two-pronged defense against adver-
sarial examples,‚Äù in Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security, CCS 2017, Dallas, TX,
USA, October 30 - November 03, 2017 (B. M. Thuraisingham, D. Evans,
T. Malkin, and D. Xu, eds.), pp. 135‚Äì147, ACM, 2017.
[23] W. Xu, D. Evans, and Y . Qi, ‚ÄúFeature squeezing: Detecting adversarial
examples in deep neural networks,‚Äù in 25th Annual Network and Dis-
tributed System Security Symposium, NDSS 2018, San Diego, California,
USA, February 18-21, 2018 , The Internet Society, 2018.
[24] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner, ‚ÄúDetecting
adversarial samples from artifacts,‚Äù CoRR , vol. abs/1703.00410, 2017.
[25] R. S. Zimmermann, W. Brendel, F. Tram `er, and N. Carlini, ‚ÄúIn-
creasing conÔ¨Ådence in adversarial robustness evaluations,‚Äù CoRR ,
vol. abs/2206.13991, 2022.
[26] D. Zhou, N. Wang, X. Gao, B. Han, X. Wang, Y . Zhan, and T. Liu,
‚ÄúImproving adversarial robustness via mutual information estimation,‚Äù in
International Conference on Machine Learning, ICML 2022, 17-23 July
2022, Baltimore, Maryland, USA (K. Chaudhuri, S. Jegelka, L. Song,
C. Szepesv ¬¥ari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of
Machine Learning Research , pp. 27338‚Äì27352, PMLR, 2022.
[27] S. RebufÔ¨Å, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. A.
Mann, ‚ÄúFixing data augmentation to improve adversarial robustness,‚Äù
CoRR , vol. abs/2103.01946, 2021.
[28] T. Pang, H. Zhang, D. He, Y . Dong, H. Su, W. Chen, J. Zhu, and T. Liu,
‚ÄúTwo coupled rejection metrics can tell adversarial examples apart,‚Äù in
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 , pp. 15202‚Äì
15212, IEEE, 2022.
[29] J. Raghuram, V . Chandrasekaran, S. Jha, and S. Banerjee, ‚ÄúA general
framework for detecting anomalous inputs to DNN classiÔ¨Åers,‚Äù in
Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang,
eds.), vol. 139 of Proceedings of Machine Learning Research , pp. 8764‚Äì
8775, PMLR, 2021.
[30] F. Tram `er, N. Carlini, W. Brendel, and A. Madry, ‚ÄúOn adaptive attacks
to adversarial example defenses,‚Äù in Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual
(H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.),
2020.
[31] F. Granese, M. Picot, M. Romanelli, F. Messina, and P. Piantanida,
‚ÄúMEAD: A multi-armed approach for evaluation of adversarial examples
detectors,‚Äù in European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML PKDD
2022), Grenoble, France, September 23, 2022 , 2022.
[32] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and harnessing
adversarial examples,‚Äù in 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings (Y . Bengio and Y . LeCun, eds.), 2015.
[33] C. Xie, M. Tan, B. Gong, A. L. Yuille, and Q. V . Le, ‚ÄúSmooth adversarial
training,‚Äù CoRR , vol. abs/2006.14536, 2020.
[34] F. Tram `er, A. Kurakin, N. Papernot, I. J. Goodfellow, D. Boneh, and P. D.
McDaniel, ‚ÄúEnsemble adversarial training: Attacks and defenses,‚Äù in6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings , 2018.
[35] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J.
Goodfellow, and R. Fergus, ‚ÄúIntriguing properties of neural networks,‚Äù in
2nd International Conference on Learning Representations, ICLR 2014,
Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings
(Y . Bengio and Y . LeCun, eds.), 2014.
[36] A. Kurakin, I. J. Goodfellow, and S. Bengio, ‚ÄúAdversarial examples in
the physical world,‚Äù in 5th International Conference on Learning Rep-
resentations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop
Track Proceedings .
[37] N. Carlini and D. A. Wagner, ‚ÄúTowards evaluating the robustness of
neural networks,‚Äù in 2017 IEEE Symposium on Security and Privacy, SP
2017, San Jose, CA, USA, May 22-26, 2017 , pp. 39‚Äì57, IEEE Computer
Society, 2017.
[38] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, ‚ÄúDeepfool: A simple
and accurate method to fool deep neural networks,‚Äù in 2016 IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV , USA, June 27-30, 2016 , pp. 2574‚Äì2582, IEEE Computer
Society, 2016.
[39] M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein, ‚ÄúSquare at-
tack: A query-efÔ¨Åcient black-box adversarial attack via random search,‚Äù
inComputer Vision - ECCV 2020 - 16th European Conference, Glas-
gow, UK, August 23-28, 2020, Proceedings, Part XXIII (A. Vedaldi,
H. Bischof, T. Brox, and J. Frahm, eds.), vol. 12368 of Lecture Notes
in Computer Science , pp. 484‚Äì501, Springer, 2020.
[40] J. Chen, M. I. Jordan, and M. J. Wainwright, ‚ÄúHopskipjumpattack:
A query-efÔ¨Åcient decision-based attack,‚Äù in 2020 IEEE Symposium on
Security and Privacy, SP 2020, San Francisco, CA, USA, May 18-21,
2020 , pp. 1277‚Äì1294, IEEE, 2020.
[41] A. Athalye, N. Carlini, and D. Wagner, ‚ÄúObfuscated gradients give a
false sense of security: Circumventing defenses to adversarial examples,‚Äù
inInternational conference on machine learning , pp. 274‚Äì283, PMLR,
2018.
[42] C. Yao, P. Bielik, P. Tsankov, and M. Vechev, ‚ÄúAutomated discovery of
adaptive attacks on adversarial defenses,‚Äù Advances in Neural Informa-
tion Processing Systems , vol. 34, pp. 26858‚Äì26870, 2021.
[43] J. M. Cohen, E. Rosenfeld, and J. Z. Kolter, ‚ÄúCertiÔ¨Åed adversarial
robustness via randomized smoothing,‚Äù in Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA (K. Chaudhuri and R. Salakhutdinov,
eds.), vol. 97 of Proceedings of Machine Learning Research , pp. 1310‚Äì
1320, PMLR, 2019.
[44] T. Maho, T. Furon, and E. L. Merrer, ‚ÄúRandomized smoothing un-
der attack: How good is it in pratice?,‚Äù IEEE International Con-
ference on Acoustics, Speech and Signal Processing, ICASSP 2022 ,
vol. abs/2204.14187, 2022.
[45] F. Tram `er, ‚ÄúDetecting adversarial examples is (nearly) as hard as
classifying them,‚Äù CoRR , vol. abs/2107.11630, 2021.
[46] A. R. Barron, J. Rissanen, and B. Yu, ‚ÄúThe minimum description length
principle in coding and modeling,‚Äù IEEE Trans. Inf. Theory , vol. 44,
no. 6, pp. 2743‚Äì2760, 1998.
[47] J. von Neumann, ‚ÄúZur theorie der gesellschaftsspiele,‚Äù Mathematische
Annalen , vol. 100, pp. 295‚Äì320, 1928.
[48] J. Davis and M. Goadrich, ‚ÄúThe relationship between precision-recall
and roc curves,‚Äù in Proceedings of the 23rd international conference on
Machine learning , pp. 233‚Äì240, 2006.
[49] A. Krizhevsky, ‚ÄúLearning multiple layers of features from tiny images,‚Äù
tech. rep., 2009.
[50] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng,
‚ÄúReading digits in natural images with unsupervised feature learning,‚Äù in
NIPS Workshop on Deep Learning and Unsupervised Feature Learning
2011 , 2011.
[51] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,
D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J.
van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J.
Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ÀôI. Polat, Y . Feng, E. W.
Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henrik-
sen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro,
F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors, ‚ÄúSciPy 1.0:
Fundamental Algorithms for ScientiÔ¨Åc Computing in Python,‚Äù Nature
Methods , vol. 17, pp. 261‚Äì272, 2020.[52] B. A. Tama and S. H. Lim, ‚ÄúEnsemble learning for intrusion detection
systems: A systematic mapping study and cross-benchmark evaluation,‚Äù
Comput. Sci. Rev. , vol. 39, p. 100357, 2021.
[53] O. Bryniarski, N. Hingun, P. Pachuca, V . Wang, and N. Carlini, ‚ÄúEvad-
ing adversarial example detection defenses with orthogonal projected
gradient descent,‚Äù CoRR , vol. abs/2106.15023, 2021.
[54] N. Carlini and D. A. Wagner, ‚ÄúAdversarial examples are not easily
detected: Bypassing ten detection methods,‚Äù in Proceedings of the 10th
ACM Workshop on ArtiÔ¨Åcial Intelligence and Security, AISec@CCS
2017, Dallas, TX, USA, November 3, 2017 (B. Thuraisingham, B. Big-
gio, D. M. Freeman, B. Miller, and A. Sinha, eds.), pp. 3‚Äì14, ACM,
2017.
[55] N. Carlini and D. A. Wagner, ‚ÄúMagnet and ‚ÄùefÔ¨Åcient defenses against
adversarial attacks‚Äù are not robust to adversarial examples,‚Äù CoRR ,
vol. abs/1711.08478, 2017.
[56] F. Croce and M. Hein, ‚ÄúReliable evaluation of adversarial robustness
with an ensemble of diverse parameter-free attacks,‚Äù in Proceedings of
the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event , vol. 119 of Proceedings of Machine
Learning Research , pp. 2206‚Äì2216, PMLR, 2020.APPENDIX
A. On the optimization of Eq. (10)
The maximization problem in Eq. (10) is well-posed given
that the mutual information is a concave function of !2

. Although from the theoretical point of view, Eq. (10)
guarantees the optimal solution for the average regret min-
imization problem, in practice, we have to deal with some
technical limitations. For the optimization of Eq. (10), we
rely on the SciPy [51] library, package optimize , func-
tionminimize2which uses the Sequential Least Squares
Programming (SLSQP) algorithm to Ô¨Ånd the optimum. This
algorithm relies on local optimization and is particularly
straightforward when dealing with non-linear equations and
equality and inequality constraints, as in our case. Overall,
we obtained the satisfactory results provided in the paper by
assigning default values to all the parameters and by setting
a uniform distribution [!1;!2;!3;!4] = [:25;:25;:25;:25]as
the initial point in the solutions space.
Although these results are satisfactory and conÔ¨Årm the value
of the sound theoretical framework we propose in Sec. IV. We
are well aware that, in some cases, as in Fig. 3a, the proposed
aggregation slightly underperforms in terms of accuracy w.r.t.
the best detector in the set of allowed detectors. In this regard,
we would like to raise a couple of points that are interesting
for practitioners and possible future research:
1) For each input sample, we solve one different opti-
mization problem: although the algorithm above always
reaches the end with a success state, given the Ô¨Ånite
amount of iterations and the tolerance which decides the
stopping criterion, further sample-by-sample parameter
optimization may be required. At this time, we have not
delved into the problem, and we leave this for future
research.
2) The hard decisions made by the single detectors only
depend on the arg max of their soft-probabilities. On
the contrary, the optimization in Eq. (10) considers the
complete soft-probability distributions output by every
single detector. Indeed, although the hard decision on
two randomly considered samples can be right for both,
often, the conÔ¨Ådence in these decisions can be very
different (i.e., two correctly classiÔ¨Åed samples may have
utterly different associated soft probabilities). Further
research on how differently accurate detectors inÔ¨Çuence
the optimization in Eq. (10) is left for future work.
B. Supplementary Results of Section V
In the following, we provide further discussions on the
experiments in Sec. V that have not been included in the
main paper.
1) Experimental environment
We run each experiment on a machine equipped with an
Intel(R) Xeon(R) Gold 6226 CPU, 2.70GHz clock frequency,
and a Tesla V100-SXM2-32GB GPU.
2Therefore we invert the sign of the objective function.2) Time measurements
Training 1 single detector in our method 1h45m10s
Evaluating the optimization in our method 1m35s (for one attack)
Training NSS 3m30s
Evaluating NSS 20s (for one attack)
On the largest set of simultaneous attacks (13 attacks):
Ours 1m35s *1321m
NSS 20s *134m
C. On the MEAD framework
1) State-of-the-art (SOTA) detectors
[31] suggests NSS [19] and FS [23] as the most ro-
bust methods in the simultaneous attacks detection scheme
(i.e., M EAD). We remind that NSS is a supervised method
that extracts the natural scene statistics of the natural and
adversarial examples to train a SVM. On the contrary, FS
is an unsupervised method that uses feature squeezing (i.e.,
reducing the color depth of images and using smoothing to
reduce the variation among the pixels) to compare the model‚Äôs
predictions.
In particular, we choose NSS as a method to compare for
multiple reasons:
1) NSS achieves the best overall score in terms of
AUROC"% and FPR#95%% among the SOTA against
simultaneous attacks (cf. Tab. 3 [31]).
2) NSS achieves the best score in terms of AUROC "% and
FPR#95%% under the L1norm where the biggest group
of simultaneous attacks are evaluated (see Tab. I). This is
stressed in the plots in Fig. 4. Moreover, FS reaches better
performance w.r.t. the proposed method only with PGD1
and PGD2 when the perturbation magnitude is small and
in CW2.
3) The case study for our aggregator in the experimental
section is based on supervised detectors as a consequence
the comparison with a supervised detector was a natural
choice.
For the sake of completeness, the performances of NSS
and FS under M EAD are given in Fig. 4. As shown before
for Ours+NSS, in Tab. V we propose an analysis of the
performance of our method before and after adding the FS
unsupervised detection mechanism to the pull of available
detectors, showing a stark improvement in the latter case.
2) Attacks
We want to emphasize that, differently from the literature,
we are the Ô¨Årst to consider a defense mechanism against the
simultaneous attack setting in which we detect attacks based
on four different losses. More speciÔ¨Åcally, for each ‚Äôclean
dataset‚Äô (in our case CIFAR10 and SVHN):
No. of adversarial examples generated with:
‚ÄìL1norm: 7 (no. of ") * 1 (PGD algorithm) * 4 (no. of
losses) = 28 (‚Äôadversarial datasets‚Äô)
‚ÄìL2norm: 7 (no. of ") * 1 (PGD algorithm) * 4 (no. of
losses) + 3 (CW2, HOP, DeepFool) = 31 (‚Äôadversarial
datasets‚Äô)Fig. 4: The shallow detectors are named after the loss function
used to craft the attacks they are trained to detect. Overall,
the SOTA methods NSS and FS outperform all the individual
shallow detectors. The aggregation we propose allows using
the shallow models to attain a detector whose performance is
consistently comparable and, in many cases, better than SOTA.
‚ÄìL1norm: 6 (no. of ") * 3 (PGD, FGSM, BIM
algorithms) * 4 (no. of losses) + 2 = 74 (‚Äôadversarial
datasets‚Äô)
‚ÄìNo norm: 1 (‚Äôadversarial dataset‚Äô)
=>For a total of 28 + 31 + 74 + 1 = 134 ‚Äôadversar-
ial datasets‚Äô for each ‚Äôclean dataset‚Äô.
Moreover, it is interesting to notice that the experiments
on CIFAR10 and SVHN represent a satisfying choice to
show that state-of-the-art detection mechanisms struggle to
maintain good performance when they are faced with the
framework of simultaneous attacks. That said, we leave the
evaluation of larger datasets as future work.
3) Simulations adversarial attack according to different "
As discussed in Sec. V, both NSS and the shallow
detectors aggregated via the proposed method are trained
on natural and adversarial examples created with PGD al-
gorithm and L 1norm constraint. We show in Tabs. VI
to IX the results of the two methods according to "2
f:03125;:0625;:125;:25;:3125;:5g.
D. The proposed aggregator against the adaptive-attacks in
theMEAD scenario
We present a new experimental setting to address the case
in which also the detectors are attacked at the same time as
the target classiÔ¨Åer, taking the cue from [30], [53]‚Äì[55]. It is
important to note that, in the spirit of the M EAD framework,
we are not simply considering a scenario in which a single
adaptive attack is perpetrated on the classiÔ¨Åer and detectors,
but rather multiple adaptive attacks are concurrently occurring.
This scenario has not yet been considered in [31], so we are the
Ô¨Årst to deal with such a setting. We extend the framework to
include two main cases: (i)for attacks on the classiÔ¨Åer and theTABLE V: Comparison between Ours and Ours+FS on CI-
FAR10. The?symbol means the perturbation mechanism is
executed in parallel four times starting from the same original
clean sample, each time using one of the objective losses
between ACE Eq. (2), KL Eq. (3), FR Eq. (4), Gini Eq. (5).
We focus only in the cases in which the proposed method is
outperformed from the corresponding competitors.
CIFAR10
Ours Ours+FS
AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1?
"= 5 62.1 87.1 69.4 74.5
"= 10 56.8 90.6 76.8 64.5
"= 15 69.3 84.4 77.6 60.3
Norm L 2
PGD2?
"= 0:125 63.9 85.4 67.9 76.4
"= 0:25 57.1 90.5 76.0 64.7
"= 0:3125 61.0 88.9 77.2 62.9
CW2
"= 0:01 53.4 92.2 86.4 46.8
(a) Analysis AUROC "%
(b) Analysis FPR #95%%
Fig. 5: Our method against the adaptive-attacks under M EAD.
We consider the worst case scenario in Tab. X, i.e., when
= 0:1.
single detectors individually; (ii)for attacks on the classiÔ¨Åer
and all the detectors simultaneously.
The tables with the complete results are Tabs. X and XI,
whereis the coefÔ¨Åcient that controls the gradient‚Äôs speed
of the attack against the detectors. We try many different
values=f:1;1;5;10g. The case where is equal to 0 is
added for completeness, and it corresponds to the case where
only the target classiÔ¨Åer is attacked. We report in Fig. 5 the
comparison of the results between case (i)and case (ii)on
CIFAR10 and = 0:1, as this corresponds to the case with the
worst performances. As can be seen, the performances of our
aggregator improve when the detectors are attacked singularly.
This is particularly interesting for the setting we are dealing
with. Indeed, our method is not a new supervised adversarialTABLE VI: Simultaneous attacks detection: NSS on CIFAR10. We train NSS on natural and adversarial examples created with
PGD algorithm and L 1norm constraint. The perturbation magnitude "is shown in the columns. We indicate in bold the best
result.
NSS
0.03125 0.0625 0.125 0.25 0.3125 0.5
AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1
"= 5 48.5 94.2 47.7 94.7 46.6 95.6 46.8 95.5 47.0 95.4 46.5 95.6
"= 10 54.0 90.3 53.4 90.8 51.6 94.3 50.4 94.9 50.4 94.9 50.9 94.7
"= 15 58.8 86.8 58.1 87.4 55.8 92.8 53.8 94.2 53.2 94.4 54.5 93.7
"= 20 63.5 82.3 62.7 82.7 60.1 90.7 57.4 93.2 56.7 93.6 58.2 92.3
"= 25 67.7 77.2 66.8 78.4 64.0 87.8 61.0 92.0 60.1 92.6 61.9 90.6
"= 30 71.4 73.4 70.5 73.5 67.6 83.7 64.4 90.4 63.4 91.4 65.4 88.2
"= 40 76.1 67.3 75.3 68.0 72.6 75.4 69.4 87.2 68.5 88.9 70.4 83.4
Norm L 2
PGD2
"= 0.125 48.3 94.3 47.5 94.8 46.6 95.6 46.7 95.5 47.1 95.4 46.5 95.6
"= 0.25 53.2 91.2 52.6 91.6 50.9 94.6 50.0 95.0 50.0 95.0 50.3 94.8
"= 0.3125 55.8 89.2 55.2 89.9 53.3 93.7 51.7 94.6 51.5 94.7 52.3 94.3
"= 0.5 63.3 82.6 62.6 83.0 60.0 90.7 57.4 93.2 56.7 93.5 58.2 92.4
"= 1 76.4 67.5 75.7 67.8 73.1 75.0 70.1 86.7 69.2 88.5 71.0 83.0
"= 1.5 81.0 63.0 80.5 62.7 78.5 63.5 76.2 80.7 75.6 83.2 76.9 74.4
"= 2 82.6 62.3 82.1 61.6 80.6 62.5 78.6 78.5 78.1 81.2 79.1 72.1
DeepFool
No" 57.0 91.7 56.7 91.7 55.6 93.6 54.6 94.1 54.2 94.3 54.7 94.0
CW2
"= 0.01 56.4 90.8 55.9 90.9 54.5 93.7 53.4 94.3 53.0 94.5 53.6 94.1
HOP
"= 0.1 66.1 87.0 65.1 88.2 63.0 91.3 61.2 92.6 60.8 92.9 61.6 92.1
Norm L 1
PGDi, FGSM, BIM
"= 0.03125 83.0 55.3 82.1 55.2 80.3 57.8 77.4 77.0 76.8 81.3 78.3 65.4
"= 0.0625 96.0 17.2 94.6 17.4 94.9 19.2 94.3 21.6 94.4 21.1 94.4 21.1
"= 0.25 97.3 0.6 94.7 5.9 96.5 2.5 96.9 1.7 97.2 1.1 96.7 2.1
"= 0.5 82.5 100.0 80.4 100.0 81.9 100.0 82.2 100.0 82.4 100.0 82.0 100.0
PGDi, FGSM, BIM, SA
"= 0.125 9.4 99.9 10.4 100.0 26.2 99.9 30.9 100.0 33.8 100.0 27.3 100.0
PGDi, FGSM, BIM, CWi
"= 0.3125 63.2 99.1 62.7 99.0 61.9 99.3 60.9 99.5 60.5 99.5 61.2 99.4
No norm
STA
No" 88.5 38.8 92.0 25.1 92.1 22.4 93.3 18.3 92.7 19.6 92.7 19.7
TABLE VII: Simultaneous attacks detection: the proposed method on CIFAR10. We train NSS on natural and adversarial
examples created with PGD algorithm and L 1norm constraint. The perturbation magnitude "is shown in the columns. We
indicate in bold the best result.
Ours
0.03125 0.0625 0.125 0.25 0.3125 0.5
AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1
"= 5 69.7 82.5 65.5 81.5 62.1 87.1 56.3 93.8 53.2 94.8 48.5 95.5
"= 10 62.3 83.3 62.7 86.3 56.8 90.6 52.1 94.7 52.9 94.6 50.9 95.0
"= 15 66.6 72.7 73.9 77.9 69.3 84.4 65.5 89.0 64.3 91.0 60.4 93.1
"= 20 72.8 58.0 83.7 59.3 78.7 73.1 73.8 82.5 73.5 85.4 69.2 90.3
"= 25 76.8 42.4 89.4 35.9 87.1 50.8 81.3 68.6 79.3 78.0 74.8 87.2
"= 30 79.1 31.1 91.7 21.4 90.3 35.4 84.3 61.2 81.9 73.5 77.5 85.3
"= 40 80.8 22.2 93.0 15.0 92.1 26.4 85.9 56.8 83.1 71.4 78.8 84.5
Norm L 2
PGD2
"= 0.125 71.3 80.8 67.0 80.2 63.9 85.4 56.2 93.8 53.8 94.7 48.6 95.5
"= 0.25 63.1 83.4 62.8 86.7 57.1 90.5 52.3 94.6 52.6 94.7 49.9 95.2
"= 0.3125 64.1 79.3 67.3 83.1 61.0 88.9 58.0 92.8 57.7 93.3 54.5 94.4
"= 0.5 72.9 58.9 83.7 60.7 79.4 73.2 74.6 81.4 73.4 85.4 68.8 90.5
"= 1 81.0 21.7 92.9 15.5 91.4 26.4 85.5 57.2 82.9 72.2 78.7 84.7
"= 1.5 81.5 19.2 93.2 14.2 91.9 24.2 85.9 56.3 83.2 71.9 79.2 84.4
"= 2 81.6 19.0 93.2 14.1 91.9 24.1 85.9 56.3 83.3 71.8 79.2 84.4
DeepFool
No" 91.1 22.0 87.4 33.9 81.9 54.8 70.0 84.4 64.2 91.5 56.3 94.4
CW2
"= 0.01 52.9 90.5 50.7 90.6 53.4 92.2 53.1 94.4 52.0 94.8 50.9 95.0
HOP
"= 0.1 91.3 20.9 89.0 31.0 86.1 49.1 77.0 80.7 72.4 88.1 64.3 92.8
Norm L 1
PGDi, FGSM, BIM
"= 0.03125 67.2 77.3 77.8 65.2 82.3 59.7 78.0 72.1 73.7 83.8 64.1 92.2
"= 0.0625 69.0 83.6 85.3 47.4 92.0 29.6 90.7 35.7 88.0 45.6 81.3 78.3
"= 0.25 72.0 67.4 91.8 23.2 95.9 8.8 94.1 15.4 92.6 19.5 91.6 26.5
"= 0.5 58.3 84.8 84.2 44.1 94.6 9.7 91.2 16.5 90.5 18.8 91.3 22.3
PGDi, FGSM, BIM, SA
"= 0.125 69.0 79.1 84.1 41.9 88.9 40.8 86.6 52.3 85.4 60.4 80.7 79.0
PGDi, FGSM, BIM, CWi
"= 0.3125 66.6 75.0 80.6 51.5 80.0 61.1 72.0 84.0 67.2 90.0 60.0 93.6
No norm
STA
No" 84.8 33.8 85.0 41.5 82.7 52.4 72.9 77.7 70.2 81.7 63.1 92.1TABLE VIII: Simultaneous attacks detection: NSS on SVHN. We train NSS on natural and adversarial examples created with
PGD algorithm and L 1norm constraint. The perturbation magnitude "is shown in the columns. We indicate in bold the best
result.
NSS
0.03125 0.0625 0.125 0.25 0.3125 0.5
AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1
"= 5 37.9 89.3 40.2 91.3 37.2 89.2 4.9 35.5 0.3 8.5 0.0 3.1
"= 10 33.7 89.3 36.9 91.3 34.6 89.2 6.0 35.5 0.4 8.5 0.0 3.1
"= 15 31.9 89.3 35.6 91.3 34.4 89.2 7.6 35.5 0.5 8.5 0.1 3.1
"= 20 31.5 89.3 36.1 91.3 35.7 89.2 9.5 35.5 0.6 8.5 0.1 3.1
"= 25 32.8 89.3 37.8 91.3 38.2 89.2 11.7 35.5 0.9 8.5 0.1 3.1
"= 30 34.5 89.3 39.8 91.3 40.6 89.2 14.1 35.5 1.2 8.5 0.1 3.1
"= 40 37.9 89.3 43.1 91.3 43.4 89.0 16.4 35.5 2.2 8.5 0.3 3.1
Norm L 2
PGD2
"= 0.125 38.7 89.3 40.8 91.3 37.6 89.2 4.7 35.5 0.3 8.5 0.0 3.1
"= 0.25 34.0 89.3 37.2 91.3 34.6 89.2 5.4 35.5 0.3 8.5 0.0 3.1
"= 0.3125 32.6 89.3 36.1 91.3 34.1 89.2 6.1 35.5 0.4 8.5 0.0 3.1
"= 0.5 31.4 89.3 35.9 91.3 35.4 89.2 8.9 35.5 0.5 8.5 0.1 3.1
"= 1 37.4 89.3 42.5 91.3 42.9 89.2 16.0 35.5 2.1 8.5 0.3 3.1
"= 1.5 40.0 89.3 46.3 91.3 46.5 88.4 17.2 35.5 2.8 8.5 0.6 3.1
"= 2 42.1 89.3 49.8 91.3 50.5 88.0 18.7 35.5 3.2 8.5 0.8 3.1
DeepFool
No" 38.1 89.3 41.3 91.3 39.7 89.2 9.2 35.5 0.8 8.5 0.1 3.1
CW2
"= 0.01 37.9 89.3 41.0 91.3 39.5 89.2 9.3 35.5 0.8 8.5 0.1 3.1
HOP
"= 0.1 66.8 82.3 67.6 84.2 60.3 84.6 16.4 35.5 2.7 8.5 0.7 3.1
Norm L 1
PGDi, FGSM, BIM
"= 0.03125 84.1 49.7 86.3 46.9 77.5 72.1 22.2 33.2 4.3 8.5 1.2 3.1
"= 0.0625 87.4 0.2 88.9 0.7 87.5 0.6 33.7 16.8 7.4 6.8 2.5 2.7
"= 0.25 16.7 89.3 51.6 88.9 52.0 85.1 35.4 0.1 8.4 0.1 3.0 0.1
"= 0.5 4.1 89.3 46.7 86.7 46.0 84.6 35.4 0.1 8.4 0.1 3.0 0.1
PGDi, FGSM, BIM, SA
"= 0.125 22.8 89.3 32.9 91.3 43.6 89.2 30.3 32.7 7.1 8.5 2.5 3.1
PGDi, FGSM, BIM, CWi
"= 0.3125 4.7 89.3 41.3 91.3 40.8 89.2 12.7 35.5 1.7 8.5 0.4 3.1
No norm
STA
No" 89.3 0.0 91.2 0.2 85.9 23.4 19.9 33.5 4.2 8.3 1.4 3.1
TABLE IX: Simultaneous attacks detection: the proposed method on SVHN. We train NSS on natural and adversarial examples
created with PGD algorithm and L 1norm constraint. The perturbation magnitude "is shown in the columns. We indicate in
bold the best result.
Ours
0.03125 0.0625 0.125 0.25 0.3125 0.5
AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1
"= 5 79.3 65.2 77.4 73.4 76.9 78.9 76.9 79.0 76.7 79.5 74.0 84.4
"= 10 74.4 65.1 72.8 73.1 71.9 81.6 73.0 82.5 71.9 84.2 66.9 89.4
"= 15 76.0 57.0 75.7 64.6 75.8 73.1 78.9 72.5 77.3 74.7 71.9 84.9
"= 20 77.3 48.1 77.9 54.9 79.2 61.9 83.6 60.7 82.2 64.3 77.4 76.9
"= 25 78.2 40.9 79.4 44.4 81.4 49.4 87.0 48.6 85.7 52.5 81.4 66.7
"= 30 78.8 34.4 80.4 35.3 83.0 36.6 89.3 37.2 88.1 41.6 84.4 53.8
"= 40 79.7 23.4 81.6 22.4 84.7 20.2 92.6 20.0 91.1 23.0 87.8 30.5
Norm L 2
PGD2
"= 0.125 82.2 61.7 80.6 68.4 80.3 72.4 80.2 74.5 80.1 73.5 79.7 75.5
"= 0.25 75.7 63.6 74.0 71.7 73.3 80.3 74.0 81.7 72.6 82.8 67.8 89.0
"= 0.3125 75.5 61.6 74.3 70.1 73.9 78.4 75.2 79.4 73.9 81.7 70.6 86.7
"= 0.5 77.2 50.6 77.6 57.4 78.6 64.1 82.5 64.4 81.2 67.1 76.3 79.5
"= 1 79.5 25.8 81.3 24.8 84.3 24.1 92.3 24.7 90.7 27.7 87.1 36.4
"= 1.5 80.2 19.5 82.2 17.6 85.6 14.3 94.1 7.5 92.9 8.6 89.9 11.8
"= 2 80.5 19.4 82.5 17.5 85.9 14.1 94.9 5.3 94.5 6.8 90.7 9.5
DeepFool
No" 96.3 8.6 95.9 10.5 95.0 12.9 94.9 12.0 95.3 12.1 95.5 12.6
CW2
"= 0.01 59.7 76.3 57.2 80.1 53.4 89.9 54.2 92.0 51.1 93.5 44.3 96.1
HOP
"= 0.1 96.1 7.9 95.6 9.8 95.9 11.7 96.0 10.2 95.9 9.9 96.1 10.0
Norm L 1
PGDi, FGSM, BIM
"= 0.03125 74.3 60.0 75.8 60.3 77.8 62.6 81.4 64.9 80.1 67.1 76.7 75.5
"= 0.0625 78.4 36.0 80.3 34.1 83.2 33.8 89.1 3 3.3 87.9 34.4 85.7 37.4
"= 0.25 80.1 19.4 82.1 17.5 85.2 15.8 92.3 16.4 92.1 16.8 89.6 17.0
"= 0.5 80.3 19.4 82.3 17.5 85.5 14.1 92.9 14.4 91.7 15.2 90.1 14.8
PGDi, FGSM, BIM, SA
"= 0.125 78.9 29.0 80.8 28.1 83.8 28.7 89.2 29.1 88.4 28.9 86.8 28.4
PGDi, FGSM, BIM, CWi
"= 0.3125 78.7 33.4 80.5 31.9 83.1 34.0 88.2 33.1 88.1 31.7 86.7 31.2
No norm
STA
No" 94.7 14.5 93.3 16.8 89.9 23.1 90.2 23.2 91.0 22.4 91.1 22.4TABLE X: The proposed method against the adaptive-attacks under M EAD. In the following setting, we attack each detector
and the classiÔ¨Åer once at a time. is the parameter to control the losses.
CIFAR10
= 0 =:1 = 1 = 5 = 10
AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1?
"= 5 62.1 87.1 61.3 88.6 61.2 89.3 63.1 89.2 62.6 91.3
"= 10 56.8 90.6 53.1 94.5 54.4 93.9 60.0 91.0 60.6 91.9
"= 15 69.3 84.4 51.5 96.5 54.7 94.6 64.1 88.1 65.7 87.7
"= 20 78.7 73.1 53.4 96.8 55.9 94.9 66.7 84.1 69.4 82.7
"= 25 87.1 50.8 54.0 97.2 56.7 94.6 67.8 82.7 71.1 79.0
"= 30 90.3 35.4 54.5 97.1 56.6 94.4 68.9 81.1 71.9 78.4
"= 40 92.1 22.7 54.4 97.0 57.7 93.6 69.4 79.7 72.9 74.2
Norm L 2
PGD2?
"= 0:125 63.9 85.4 61.4 88.0 62.4 88.8 63.7 88.5 63.9 89.9
"= 0:25 57.1 90.5 52.9 94.2 55.0 93.6 60.6 89.7 61.5 90.3
"= 0:3125 61.0 88.9 51.6 95.7 54.1 94.7 62.2 87.8 63.7 87.9
"= 0:5 79.4 73.2 52.8 96.8 55.3 94.3 66.2 84.6 68.8 81.5
"= 1 91.4 26.4 52.7 96.8 57.3 93.4 69.0 78.3 72.1 74.4
"= 1:5 91.9 24.2 53.9 96.1 57.9 91.4 70.5 73.7 74.1 68.1
"= 2 91.9 24.1 54.6 94.6 59.3 88.5 72.3 67.8 75.6 62.7
Norm L 1
PGDi?, FGSM?, BIM?
"= 0:03125 82.3 59.7 45.3 96.2 46.0 96.4 54.5 91.4 57.4 89.3
"= 0:0625 92.0 29.6 44.3 96.2 49.8 93.8 59.7 82.4 64.3 76.4
"= 0:5 94.6 9.7 62.1 81.3 54.9 81.9 66.1 60.8 68.9 57.9
PGDi?, FGSM?, BIM?, SA
"= 0:125 88.9 40.8 48.6 90.7 54.9 85.0 61.9 73.1 66.3 67.5
PGDi?, FGSM?, BIM?, CWi
"= 0:3125 80.0 61.1 56.6 82.0 56.3 79.6 66.1 66.1 69.2 64.4
TABLE XI: The proposed method against the adaptive-attacks under M EAD. In the following setting, we attack all the detectors
and the classiÔ¨Åer together at the time. is the parameter to control the losses.
CIFAR10
= 0 =:1 = 1 = 5 = 10
AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%% AUROC"% FPR#95%%
Norm L 1
PGD1?
"= 5 62.1 87.1 61.2 90.4 63.6 86.8 65.8 83.9 66.3 83.2
"= 10 56.8 90.6 50.5 96.4 55.9 91.6 60.1 88.1 61.1 87.2
"= 15 69.3 84.4 47.3 97.6 53.8 92.3 62.0 84.9 63.7 83.7
"= 20 78.7 73.1 47.1 97.9 54.2 92.5 64.2 82.8 66.8 79.1
"= 25 87.1 50.8 47.8 98.0 55.0 92.1 66.5 79.5 68.8 77.2
"= 30 90.3 35.4 48.8 98.0 55.8 91.3 67.4 78.5 70.4 75.0
"= 40 92.1 22.7 49.1 98.0 56.8 90.5 68.6 77.4 72.5 71.6
Norm L 2
PGD2?
"= 0:125 63.9 85.4 62.4 88.5 65.0 86.2 66.9 82.9 67.2 81.1
"= 0:25 57.1 90.5 51.2 96.0 56.3 91.7 60.6 87.2 61.6 86.8
"= 0:3125 61.0 88.9 56.0 94.6 57.9 93.6 65.3 86.4 66.7 86.6
"= 0:5 79.4 73.2 46.8 97.8 54.6 91.3 64.5 82.4 66.8 79.5
"= 1 91.4 26.4 47.2 98.0 57.8 89.4 69.9 73.8 73.1 71.7
"= 1:5 91.9 24.2 47.5 97.6 59.9 86.9 73.2 68.7 76.5 63.1
"= 2 91.9 24.1 49.0 97.0 62.8 83.3 75.6 63.7 79.5 56.6
Norm L 1
PGDi?, FGSM?, BIM?
"= 0:03125 82.3 59.7 40.2 98.0 47.6 95.5 60.6 86.2 65.0 81.8
"= 0:0625 92.0 29.6 37.9 98.0 47.0 95.9 61.9 82.1 65.8 77.1
"= 0:25 95.9 8.8 36.5 96.4 47.4 97.7 62.5 92.6 65.4 90.8
"= 0:5 94.6 9.7 36.7 96.2 46.0 97.7 61.6 96.1 66.0 94.8
PGDi?, FGSM?, BIM?, SA
"= 0:125 88.9 40.8 38.5 95.9 46.8 95.4 60.1 85.0 61.9 83.2
PGDi?, FGSM?, BIM?, CWi
"= 0:3125 80.0 61.1 37.2 95.3 46.7 97.4 60.9 92.4 64.1 90.1detection method but a framework to aggregate detectors, in
this case, applied to the adversarial detection problem. Hence,
it does not propose solving the problem of Ô¨Ånding a new robust
method for adaptive attacks but rather creating a mixture of
experts based on the proposed sound mathematical framework.
Thus, an attacker to successfully fool our method needs to have
thecomplete access to all the underlying detectors and also
an up-to-the-date knowledge of the detectors employed as the
defender can always include a new detection mechanism to the
pool of the detectors. To give more insights on the proposed
TABLE XII: Comparison between the proposed method and
the single detectors ( stronger version) against the adaptive-
attacks. Norm L 1and"= 0:25(i.e., attacks PGDi?, FGSM?,
BIM?).
CIFAR10 Ours ACE KL FR Gini
AUROC"% 54.6 35.7 30.6 26.3 36.2
FPR#95%% 73.0 96.5 97.0 97.4 99.6
aggregator under this setting, we train a stronger version of the
four shallow detectors where the detectors at training time have
seen the corresponding adaptive attacks generated through the
PGD algorithm. We report the results in Tab. XII where we
focus on the group of simultaneous attacks with L 1norm
and"= 0:25as this represents the worst result of our method
in Tab. XI. If our method was only good as the best among
the detectors, we should expect similar results in Tab. XII.
In this case, the only solution would be to train a better
detector. However, the strength of the aggregator is not
just mimicking the performance of its parts but rather
creating a mixture of experts based on the proposed sound
mathematical framework. Therefore, we should expect better
performances. Indeed, this consistently holds as the method
performs much better than the best detector.
E. AutoAttack
We present an application of AutoAttack [56], a state-of-the-
art evaluation tool for robustness, redesigned for adversarial
detection evaluation and adapted to our simultaneous attacks
framework. In its original version, AutoAttack evaluates the
accuracy of robust classiÔ¨Åers. In so doing, [56] proposes
a multiple attacks framework to ensure that at least one
attack succeeds in producing an adversarial example for each
natural one. In their context, it does not matter which attack
will succeed since any successful attack would undermine
the accuracy of the target classiÔ¨Åer in the same way. In
our case, the number of different successful attacks for each
natural sample will affect the detection quality since a detector
is successful only if it can detect all of them. Because of
the above mentioned differences, it is impossible to deploy
it directly in our framework without any modiÔ¨Åcations. A
modiÔ¨Åed version of AutoAttack, adapted to the evaluation
of our proposed method, has been implemented, and theTABLE XIII: The proposed method on AutoAttack (M EAD
setting). The attacks are APGD-CE, APGD-DLR, FAB, SA.
CIFAR10
Ours
AUROC"% FPR#95%%
Norm L 1
"= 5 57.1 88.4
"= 10 67.1 75.7
"= 15 72.2 66.7
"= 20 72.7 65.2
"= 25 72.8 65.6
"= 30 73.4 64.0
"= 40 73.6 64.0
Norm L 2
"= 0:125 67.4 81.0
"= 0:25 58.0 89.0
"= 0:3125 58.1 88.8
"= 0:5 69.4 74.7
"= 1 75.1 61.6
"= 1:5 76.1 60.7
"= 2 76.1 60.5
Norm L 1
"= 0:03125 75.7 61.0
"= 0:0625 76.0 60.7
"= 0:125 76.8 60.3
"= 0:25 76.8 60.0
"= 0:3125 78.6 57.6
"= 0:5 76.1 60.3
results are presented below. While AutoAttack suggests using
different attack strategies, in our case, we combine different
attack strategies matched with different losses to make the pool
of attacks more strong and more diversiÔ¨Åed.
F . Additional plots
The speciÔ¨Åc shape in the histograms depends on the set of
considered detectors. To shed light on this fact, we include the
plots in Fig. 6 in which we consider a subset of the available
detectors (ACE, KL, FR). These plots should be compared
with the ones in Fig. 3.(a) PGD-L 1-40-ACE
 (b) PGD-L 1-40-KL
(c) PGD-L 1-40-FR
 (d) PGD-L 1-40-Gini
Fig. 6: In pink the results for the adversarial examples and in
blue the ones for the naturals. In this simulation, we consider a
subset of the available detectors (ACE, KL, FR). Under each
plot, we indicate the tested attack conÔ¨Åguration parameters:
algorithm-L p-"-loss.