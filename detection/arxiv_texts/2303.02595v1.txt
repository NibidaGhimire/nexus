PyramidFlow: High-Resolution Defect Contrastive Localization
using Pyramid Normalizing Flow
Jiarui Lei1,2Xiaobo Hu1,2Yue Wang1,2Dong Liu1,2,3,4*
1State Key Laboratory of Modern Optical Instrumentation, Zhejiang University.
2ZJU-Hangzhou Global Scientiﬁc and Technological Innovation Center, China
3Jiaxing Key Laboratory of Photonic Sensing & Intelligent Imaging, China
4Intelligent Optics & Photonics Research Center, Jiaxing Research Institute Zhejiang University
{karrilett, huxiaobo, 426195, liudongopt }@zju.edu.cn
Abstract
During industrial processing, unforeseen defects may
arise in products due to uncontrollable factors. Although
unsupervised methods have been successful in defect local-
ization, the usual use of pre-trained models results in low-
resolution outputs, which damages visual performance. To
address this issue, we propose PyramidFlow, the ﬁrst fully
normalizing ﬂow method without pre-trained models that
enables high-resolution defect localization. Speciﬁcally,
we propose a latent template-based defect contrastive lo-
calization paradigm to reduce intra-class variance, as the
pre-trained models do. In addition, PyramidFlow utilizes
pyramid-like normalizing ﬂows for multi-scale fusing and
volume normalization to help generalization. Our com-
prehensive studies on MVTecAD demonstrate the proposed
method outperforms the comparable algorithms that do not
use external priors, even achieving state-of-the-art perfor-
mance in more challenging BTAD scenarios.
1. Introduction
Due to the uncontrollable factors in the complex in-
dustrial manufacturing process, unforeseen defects will be
brought to products inevitably. As the human visual system
has the inherent ability to perceive anomalies [25], quality
control relies on manual inspection for a long time.
However, large-scale images and tiny defects are chal-
lenging for manual inspection, so increasing research is fo-
cused on automated machine vision inspection. Among all
the methods, supervised deep learning has achieved great
success. It relies on annotated datasets to learn discrimina-
tive features, effectively overcoming the hand-crafted short-
comings. However, because of insufﬁcient negative sam-
ples, the high demand for labels, and the absence of prior
knowledge, those approaches based on supervised learning
I
(a)reconstruction-based methodNF
(b)anomaly-based methodBackbone
(c) PyramidFlow
I
NF
dzdx
zI I IFigure 1. Illustration of various anomaly localization methods.
(a)Reconstruction-based method. (b)Anomaly-based method,
where NFdenotes normalizing ﬂow. (c)Our PyramidFlow, which
combines latent templates and normalizing ﬂow, enables high-
resolution localization.
may suffer in identifying unseen defects in practices,
Recently, unsupervised methods have been applied to
defect detection, as shown in Fig. 1(a,b). Reconstruction-
based methods [4, 15, 23, 29] are the most famous, which
take reconstructed images as templates and then apply ex-
plicit contrast in image space to achieve high-resolution lo-
calization. However, reconstructing using decoders is an
ill-posed inverse problem, it is hard to reconstruct com-
plex details. To overcome the above limitations, anomaly-
based methods [6, 7] utilizing texture-aware pre-trained
models achieves high image-level performance, which also
damages pixel-level visual performance. One of the most
promising methods is convolutional normalizing ﬂows [10,
22, 27], which models the probability distribution further
from pre-trained features, earning higher performance.
In this paper, a Pyramid Normalizing Flow (Pyramid-
Flow) is proposed. It develops the idea of templates from
image space into latent space by normalizing ﬂow, then
performing contrast ∆zdfor high-resolution anomaly lo-
calization, as shown in Fig. 1(c). Speciﬁcally, we propose
the multi-scale Pyramid Coupling Block, which includes
1arXiv:2303.02595v1  [cs.CV]  5 Mar 2023invertible pyramid and volume normalization, as the crit-
ical module to construct volume-preserving PyramidFlow.
To the best of our knowledge, PyramidFlow is the ﬁrst
UNet-like fully normalizing ﬂow speciﬁcally designed for
anomaly localization, analogous to UNet [19] for biomed-
ical image segmentation. Our main contributions can be
summarized as follows.
• We propose a latent template-based defect contrastive
localization paradigm. Similar to the reconstruction-
based methods, we perform contrast localization in la-
tent space, which avoids the ill-posedness and reduces
intra-classes variance efﬁciently.
• We propose PyramidFlow, which includes invertible
pyramids and pyramid coupling blocks for multi-scale
fusing and mapping, enabling high-resolution defect
localization. Additionally, we propose volume normal-
ization for improving generalization.
• We conduct comprehensive experiments to demon-
strate that our advanced method outperforms compa-
rable algorithms that do not use external priors, and
even achieves state-of-the-art performance in complex
scenarios.
2. Related Work
2.1. Deep learning-based Defect Localization
With the rise of deep learning, numerous works apply
generalized computer vision methods for defect detection.
Some works are based on object detection [13,14,28,30,31],
which relies on annotated rectangular boxes, enabling locat-
ing and classifying defects end-to-end. The other is apply-
ing semantic segmentation [5, 18, 24], which enables pixel-
level localization, suit for complex scenarios with difﬁcult-
to-locate boundaries. However, these works still rely on su-
pervised learning, they attempt to collect sufﬁcient defec-
tive samples to learn well-deﬁned representations.
Recently, some promising work has considered the
scarcity of defects in real-world scenarios, where defect-
free samples are only obtained. These methods can be
classiﬁed as reconstruction-based and anomaly-based. The
reconstruction-based method relies on generative models
such as V AE or GAN, which encode a defective image
and reconstruct it to a defect-free image, then localize
the defect with the contrast of these two images. The
reconstruction-based method performs well on single tex-
tural images, but they cannot generalize to non-textural im-
ages for ill-posedness and degeneracy [25]. The anomaly-
based method treats defects as anomalous, applying neu-
ral networks to discriminate between normality and anoma-
lous. These methods extract pre-trained features, then esti-
mate their probability density using Mahalanobis distancesor K-NearestNeighbor, while the lower probability indi-
cates where the image patches are abnormal. Although
anomaly-based methods had achieved great success in de-
fect detection, it locates defects with low pixel-level resolu-
tion compared with reconstruction-based methods, usually
1/16th or even lower, which greatly limits practical indus-
trial applications.
To overcome existing shortcomings, we propose a latent
template-based defect contrastive localization paradigm,
which breaks the limitation of low-frequency texture-
aware-only models, enabling more accurate results.
2.2. Normalizing Flow
Normalizing ﬂow is a kind of invertible neural network
with bijective mappings and traceable Jacobi determinants.
It was ﬁrst proposed for nonlinear independent component
estimation [8] and applied to anomaly detection [21] re-
cently for its invertibility helps prevent mode collapse. The
normalizing ﬂow comprises coupling blocks, these basic
modules for realizing nonlinear mappings and calculating
Jacobi determinants. Originally, NICE [8] proposed the
additive coupling layer with unitary Jacobi determinants,
while RealNVP [9] further proposed the afﬁne coupling
layer that enables the generation of non-volume-preserving
mappings. However, redundant volume degrees of freedom
can lead to increased optimization complexity, creating a
domain gap between maximum likelihood estimation and
anomaly metrics, which may potentially compromise the
generalization performance in anomaly detection.
Previous works [10,21,27] on anomaly localization usu-
ally follow the methods proposed in RealNVP, but some
challenges remain. Some studies [22] have found that con-
volutional normalizing ﬂow focuses on local rather than se-
mantic correlations, which are usually addressed by image
embeddings [12]. Hence, earlier studies [21] adopted pre-
trained backbones, while recent trends used pre-trained en-
coders to extract image patches [10, 22, 27]. However, pre-
trained-based methods rely on task-irrelevant external pri-
ors, which limit generalization in unforeseen scenarios.
To address the above challenges, we propose a pyramid-
like normalizing ﬂow called PyramidFlow, which utilizes
volume normalization to preserve volume mappings that
include task-relevant implicit priors. Additionally, our
method offers the option of using pre-trained models, and
we have observed that external priors from pre-trained mod-
els can improve generalization performance. We will dis-
cuss these contributions in Sec. 4.3 and Sec. 4.4.
3. Methodology
Our algorithm consists of two processes, training and
evaluation, as shown in Fig. 2. The training process is sim-
ilar to siamese networks, the model is optimized by mini-
mizing the Frequency differences /bardblF(∆zd)/bardblwithin the im-
2()Ii
()Ij
(a) Training 
Minimize
} {,0 , 1() , 1 , k Ik B  
k
Image Template1f
Forward f
dz
(b) Evaluation
Idz
com
1x1
Conv
NF
NF
dxdz2
2() ( )dd d zi z zj
dec
dec
dec
dec
NF
NF
Anomaly Localization) (dz
comFigure 2. Schematic of training and evaluation for PyramidFlow. (a)Given any normality pair, minimize the distance of latent variables.
(b)The means of the latent variables are contrasted to the examples, then apply pyramid composition to obtain an anomaly localization
map.
age pair. For the evaluation process, latent templates are
obtained through inference at the total training dataset, then
latent contrast and pyramid composition are applied to ob-
tain an anomaly localization map. The details are shown in
the following sections.
3.1. Invertible Pyramid
Defect images contain various frequency components.
Usually, the low-frequency components represent the slow
gradient background, while the high-frequency components
correspond to details or defects. To decouple the frequency
components and identify each frequency component inde-
pendently, we propose invertible pyramids, which enable
multi-scale decomposition and composition for a single fea-
ture. To facilitate feature learning, previous work applies
pre-trained encoders to extract features. Although pre-
trained methods with external priors help performance im-
provement, to fully explore the advantages of our approach
in our primary study, let’s consider a baseline without any
pre-trained model.
For a three-channel image I, apply orthogonally initial-
ized1×1convolution W∈RC×3to the image for obtain
featuresx=WI. Given a feature xand a positive integer
L, the pyramid decomposition is a mapping from features
to feature setsLdec:x→{xd|d∈ZL−1}, wherexdis the
d−level pyramid can be calculated as
xd=Dd(x)−U(Dd+1(x)) (1)
whereD(·)andU(·)are arbitrary linear upsampling and
downsampling operators, while Dd(·)represents repeated
downsampling dtimes. If Eq. (1) is further satisﬁed
D0(x) =x,DL(x) = 0 , then the inverse operation Lcom:
{xd|d∈ZL−1}of the pyramid decomposition can be de-
scribed as
x=L−1/summationdisplay
d=0Ud(xd) (2)
Differing from Gaussian pyramid, Eq. (2) indicates thatthere is always an inverse operation for pyramid decompo-
sition, which is called pyramid composition. The method
based on Eqs. (1) and (2), enabling perform multi-scale fea-
ture decomposition and composition, is a critical invertible
module for PyramidFlow.
3.2. Pyramid Coupling Block
Invertible Modules. Invertible modules are the essential
elements to implementing invertible neural networks. The
invertible modules introduced in this paper are invertible
convolution, invertible pyramid, and afﬁne coupling block.
The afﬁne coupling block is the basic module that consti-
tutes the normalizing ﬂow. It is based on feature splitting
for invertible nonlinear mappings with easily traceable Ja-
cobian determinants and inverse operations.
As shown in Fig. 3(a), the conventional afﬁne coupling
block splits a single feature along the channel dimension,
where one sub-feature keeps its identity while another is
performed afﬁne transformation controlled by it. Denote
the splitted features are x0,x1and its outputs are y0,y1,
then the corresponding transformation can be described as
y0=x0
y1= exp (s(x0))⊙x1+t(x0)(3)
wheres(·),t(·)are afﬁne parameters, can be estimated by
zero-initialized convolutional neural networks. For for-
mula(3), there is an explicit inverse transformation:
x0=y0
x1= exp (−s(y0))⊙(y1−t(y0))(4)
Denote the element at position i,jofs(·)assi,j(·). As
the Jacobian matrix of transformation(3) is a triangular ma-
trix, its logarithmic determinant can be estimated as
log/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂(y0,y1)
∂(x0,x1)/vextendsingle/vextendsingle/vextendsingle/vextendsingle=/summationdisplay
i,jsi,j(x0) (5)
Eqs. (3) to (5) are the basis of all afﬁne coupling blocks.
However, the coupling block shown in Fig. 3(a) remains
3(a) (b)
Merge
Channels
Split
Channels
Split
ChannelsMerge
Channels
(c)
Decompose
Pyramid
x
0x
1x0y
1yz
1×1
Conv
Compose
Pyramid
0z
1z
Decompose
Pyramid
Compose
Pyramid
(d)2x1x0x x z
1×1
Conv
0z
1z
2z0y
1y
2yx
0x
1x0y
1yx
0x
1x0y
1y0z
1z
dec
comPyramidFlow
Ix0x
1x
2x
3x0z
1z
2z
3z
z
1D k
1L d (e)Figure 3. The proposed pyramid coupling block and PyramidFlow, where the solid line symbolizes the transformation while the dotted
line refers to identity. (a)Channel-splitting afﬁne coupling block. (b)The reverse cascade of (a)-architecture. (c)The proposed scale-wise
pyramid coupling block. (d)The reverse parallel and reparameterized of (c)-architecture. (e)The proposed PyramidFlow, is a stacking of
(c,d)-architecture both in depths and layers, where 1×1convolution is neglected to represent.
identical for one part. Therefore the reverse cascade ar-
chitecture is proposed in NICE [8] such that both parts are
transformed, as shown in Fig. 3(b). The previous works
construct the holistic invertible normalizing ﬂow by itera-
tive applying the structure shown in Fig. 3(b).
Implementation. Our method decomposes a single feature
along the scale and realizes multi-scale feature fusion based
on Eqs. (3) to (5). In our implementation, the multi-scale
afﬁne parameters s(·),t(·)are estimated using a convolu-
tional neural network with two linear layers, where bilinear
interpolation is applied to match the target shape.
In addition, we employ invertible 1x1 convolution [11]
for feature fusion within features. Speciﬁcally, denoting the
full rank matrix corresponding to the invertible 1×1con-
volution as A, which can be decomposed by PLU as
A=PL(U+diag(exp(si))) (6)
wherePis a frozen permutation matrix, Lis a lower tri-
angular matrix with unit diagonal elements, Uis an upper
triangular matrix with zero diagonal elements, and exp(si)
is thei-th eigenvalue of the matrix A, which always holds
nonnegativity. The matrix Ais always invertible during op-
timization, then its logarithmic Jacobian determinant can be
estimated as
log|A|=/summationdisplay
isi (7)
In summary, Eqs. (3) to (7) describe proposed pyramidal
coupling block mathematically, as shown in Fig. 3(c). First,
multi-scale feature fusion(3-5) is performed, and then ap-
ply linear fusion(6-7) for shufﬂe channels. Furthermore, we
propose a dual coupling block as shown in Fig. 3(d), which
is equivalent to the reverse parallel of the coupling block
in Fig. 3(c). The dual coupling block is reparameterized in
our implementation, and its afﬁne parameters s(·),t(·)are
estimated from concatenated features.
Volume Normalization. Suppose that the invertible trans-
formationf:x→zmaps the variable xto the latent vari-
ablez. Previous works have assumed that the latent variable
H, W
N C
H, W
H, W
N C(a) Batch Norm (b) CVN (c) SVN
C NFigure 4. Illustration of volume normalization. (a)Batch Normal-
ization. (b)The proposed Channel V olume Normalization (CVN).
(c)The proposed Spatial V olume Normalization (SVN).
follows basic probability distribution ( e.g. Gaussian distri-
bution), then estimates sample probability density based on
the following equation:
P(x) =P(z)/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂f(x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle(8)
However, this approach relies on the basic distribution
assumption and ignores the effect of the implicit prior in the
probability density transform on generalization. When such
approaches are applied to anomaly detection, the inconsis-
tency between the training objectives and the anomaly eval-
uation results in domain gaps.
Similar to batch normalization or instance normalization
in deep learning, the proposed volume normalization will be
employed for volume-preserving mappings, as illustrated in
Fig. 4. Particularly, for the afﬁne coupling block, the param-
eters(·)is subtracted from its mean value before perform-
ing Eq. (3); for the invertible convolution, the parameter
siis subtracted from its mean value before calculating the
matrixAbased on Eq. (6). Depending on the statistical di-
mension, we propose Spatial V olume Normalization (SVN)
and Channel V olume Normalization (CVN). SVN performs
mean statistics along the spatial dimension, while CVN is
along the channel dimension. Various volume normaliza-
tion methods contain different priors, then we will explore
their impact in Sec. 4.2.
3.3. Pyramid Normalizing Flow
Architecture. Our PyramidFlow can be obtained by stack-
ing the pyramid coupled blocks of Fig. 3(c,d) along the
depthD−1times and along the layer L−1times, as
4shown in Fig. 3(e). Speciﬁcally, PyramidFlow boosts the
imageIto featurexusing matrix W, then performs the
pyramid decomposition based on Eq. (1). The pyramid cou-
pling blocks described in Eqs. (3) to (7) are calculated in the
order described in Fig. 3(e) to obtain potential pyramid fea-
tureszd,d= 0,1,···,L−1, which are ﬁnally composed
into latent variables according to Eq. (2).
Loss Function. In the cases with volume normalization,
the loss function excludes the probability density coefﬁ-
cients. Moreover, the logarithmic Jacobian determinant of
the semi-orthogonal matrix Wis sample-independent, so
its effect could be ignored during training.
Suppose a training batch with 2 normal samples, its la-
tent variables are zd(i),zd(j). Previous studies train neural
networks using spatial difference ∆zd=/bardblzd(i)−zd(j)/bardbl2.
However, it ignored the impact of high-frequency defects.
To address the above shortcoming, we propose the follow-
ing Fourier loss function.
Lloss=/bardblF(Lcom({∆zd|d∈ZL−1}))/bardbl (9)
whereFis the fast Fourier transform of the image. Train-
ing the normalizing ﬂow using Eq. (9) enables the model to
focus on the high-frequency, allowing faster convergence.
We will discuss this trick in Sec. 4.3.
Defect Localization. Previous studies [22, 27] usually lo-
calize defects with obvious differences based on category-
independent zero templates. In our method, the defects are
modeled as anomalous deviations with respect to the tem-
plate. Then, the anomaly of the latent pyramid zdis deﬁned
asσ(zd) =/bardblzd−¯zd/bardbl, where ¯zdis mean of the latent pyra-
mid. Finally, the total anomaly can be estimated as
σ(z) =Lcom({σ(zd)|d∈ZL−1}) (10)
The Eq. (10) shows that the total anomaly is a composi-
tion of anomalies at various scales, which is consistent with
the empirical method proposed by Rudolph, et al. [22].
Image Template Estimation. The image template is a pro-
totype of normal samples, a visualization of the latent tem-
plate. Our fully normalizing ﬂow is based on 1×1convolu-
tion instead of pre-trained encoders, maintaining end-to-end
and near-invertibility, thus the ﬂow’s input xtemp can be re-
trieved using Eq. (2) and Eq. (4) from latent mean ¯zd, then
solve the least square problem WItemp =xtemp for image
templateItemp.
4. Experiment and Discussion
In this chapter, we perform unsupervised anomaly local-
ization experiments on MVTec Anomaly Detection Dataset
[2] (MVTecAD) and BeanTech Anomaly Detection Dataset
[16] (BTAD). MVTecAD contains 15 categories of indus-
trial defect images, ﬁve of which are textural images and
the other ten are object images. The object images contain
70
60
50
40
30
20
10
0ResNet18ResNet50EfficientNet-B6Wide-ResNet50
D2 D4 D8 D16 D32 D6424681012Training Memory Usage  (GB)Model Parameters (M)Model Complexity Analysis
0Figure 5. Analysis of model complexity for various depths. The
bars correspond to the Training Memory Usage (GB) in the left
vertical coordinate, while the line graph and horizontal lines relate
to the Model Parameters (M) on the other side. For each depth D,
the left bar presents the normalizing ﬂow implemented based on
autoFlow framework with memory-saving tricks, while the right
is implemented by PyTorch with auto-differentiation.
three classes (grid, metal nut, screw) without rough registra-
tion and one class (hazelnut) without ﬁne registration, and
we will discuss these cases in Sec. 4.5. The BTAD contains
three types of real-world and industry-oriented textural im-
ages, which is more challenging for pixel-level localization.
All experiments take Area Under the Receiver Operat-
ing characteristic Curve (AUROC) and Area Under the Per
Region Overlap (AUPRO) as evaluation metrics. AUROC is
the most widely used anomaly evaluation metric, and higher
values indicate that various thresholds have less impact on
performance. However, AUROC prefers larger anomalies
and may fail in small proportions of anomalies. Thus, we
further evaluate AUPRO for localization metric, similar to
Intersection Over Union (IoU) commonly used in semantic
segmentation. Detailed deﬁnitions can be found in [2].
4.1. Complexity Analysis
The normalizing ﬂow based on Eqs. (3) and (4) is com-
putationally invertible, which indicates that only one copy
of the variables is necessary for all stages. This feature de-
creases the memory footprint during backpropagation from
linear to constant complexity. We have analyzed the above
characteristics based on a ﬁxed number of pyramid layers
L= 8, image resolution with 256×256, and channels
C= 24 , then changed the number of stacked layers Dto
explore the trends of memory usage and model parameters.
The forward and memory-saving backpropagation is im-
plemented based on the self-developed PyTorch-based [17]
framework autoFlow . All indicators are recorded during
steady-state training, then plotted as bar and line graphs, as
shown in Fig. 5.
5The memory usage based on auto-differentiation in-
creases linearly with depth D, while the implementation
based on normalizing ﬂow achieves approximate depth-
independent memory usage. The memory superiority
enables the proposed method to be trained in memory-
constrained devices below 4G without powerful hardware.
The line graph shows the exponential trends between model
parameters and depth, while the horizontal line represents
the parameters of the usual pre-trained model. We mainly
adopt methods with D < 8or even shallower, where
the number of parameters is far smaller than popular pre-
training-based methods. In summary, in scenarios of mem-
ory constraint, the proposed PyramidFlow enables dealing
with larger images and requires fewer parameters than oth-
ers.
4.2. Study on Volume Normalization
In this subsection, based on MVTecAD, we investigate
the impact of volume normalization on generalization. The
experiment without data augmentation, ﬁxed pyramid lay-
ersL= 4, channelsC= 16 , and linear interpolated im-
age to 256×256. During the training, the volume nor-
malization applies sample mean normalization and updates
the running mean with 0.1 momenta, while in the testing,
the volume normalization is based on the running mean.
We sufﬁciently explored the volume normalization meth-
ods proposed in Sec. 3.2. Some representative categories
are shown as Tab. 1.
Table 1. Quantitative results of CVN and SVN on different
categories. For each case in the table, the ﬁrst column is
Pixel-AUROC% and the second is AUPRO% , while the values
within parentheses represent the relative improvement.
ClassesCVN SVN
AUROC AUPRO AUROC AUPRO
capsule 96.1 (+2.6) 93.1 (+5.1) 93.5 (+0.0) 88.0 (+0.0)
pill 96.2 (+1.8) 96.3 (+1.4) 94.4 (+0.0) 94.9 (+0.0)
toothbrush 98.9 (+2.5) 97.9 (+4.3) 96.4 (+0.0) 93.6 (+0.0)
carpet 88.9 (+0.0) 88.3 (+0.0) 90.8 (+1.9) 91.0 (+2.7)
grid 86.2 (+0.0) 84.5 (+0.0) 94.2 (+8.0) 92.7 (+8.2)
zipper 92.2 (+0.0) 91.9 (+0.0) 95.4 (+3.2) 95.1 (+3.2)
The result in Tab. 1 shows the performance differences
between the various volume normalization methods: CVN
outperforms SVN for the ﬁrst three classes, while the latter
behaves the opposite. We further visualize these defect dis-
tributions in Fig. 6, which shows that SVN-superior classes
are commonly textural images with a larger range of de-
fects, while CVN-superior classes are object images.
SVN with larger receptive ﬁelds achieves non-local lo-
calization by aggregating an extensive range of texture fea-
tures, while CVN realizes accurate localization by shufﬂing
channels. In a word, different volume normalization tech-
niques implicitly embody distinct task-speciﬁc priors. Fur-
thermore, our ablation study in Sec. 4.3 shows that volume
capsule pill toothbrush
carpet grid zipperCVN SVNFigure 6. Defects in Tab. 1 are visualized as heat maps. The top
row displays CVN-superior class object images, while the bottom
row displays SVN-superior class texture images.
normalization does help to improve average performance.
4.3. Ablation Study
This subsection discusses the impact of some proposed
methods on performance. The study is conducted on full
MVTecAD, and other settings are the same as Sec. 4.2. We
ablate four methods from baseline individually. In partic-
ular, experiment I is based on latent Gaussian assumption
and Eq. (8) without volume normalization. For experiment
II , the category-independent zero template ¯zd= 0 is ap-
plied. Then, Experiment III does not adopt the method of
Eq. (10) but composes the pyramid ﬁrst and localizes its
difference later. Finally, experiment IV adopts a spatial
version of the loss function instead of Eq. (9). The result of
the above ablation experiments is shown in Tab. 2.
Table 2. The ablation study on full MVTecAD. For each cell
in the table, the ﬁrst row is Pixel-AUROC% and the second is
AUPRO% . The number within parentheses means the change
relative to baseline, the larger absolute value with larger impor-
tance.
MethodClasses
MEANTexture Object
Ours (baseline)95.2 (+0.0) 95.7 (+0.0) 95.5 (+0.0)
95.1 (+0.0) 93.5 (+0.0) 94.0 (+0.0)
I. w/o V olume
Normalization89.4 (-5.8) 85.2 (-10.5) 86.6 (-8.9)
87.5 (-7.6) 83.6 (-9.9) 84.9 (-9.1)
II. w/o Latent
Template93.1 (-2.1) 90.7 (-5.0) 91.5 (-4.0)
91.9 (-3.2) 84.7 (-8.8) 87.1 (-6.9)
III. w/o Pyramid
Difference87.8 (-7.4) 93.1 (-2.6) 91.3 (-4.2)
87.8 (-7.3) 89.4 (-4.1) 88.9 (-5.1)
IV. w/o Fourier
Loss92.0 (-3.2) 93.3 (-2.4) 92.9 (-2.6)
92.8 (-2.3) 91.9 (-1.6) 92.2 (-1.8)
Tab. 2 demonstrates that experiments I -IV present vari-
ous performance degradation. Experiment I has the largest
average degradation, with the object classes being more af-
fected. Although the non-volume-preserving enables larger
outputs and higher Image-AUROC performance, the im-
plicit prior in volume normalization discussed in Sec. 4.2 is
6Table 3. Quantitative results of various challenging methods on MVTecAD. In the table, the fully normalized ﬂow method is labeled
as FNF, while the abbreviations Res18, WRes50, EfﬁB5, and DTD are denoted as ResNet18, Wide-ResNet50-2, EfﬁcientNet-B5, and
Describable Textures Dataset, respectively. For each case in the table, the ﬁrst row is Pixel-AUROC% and the second is AUPRO% ,
where the best results are marked in bold.
External
PriorMethods carpet leather tile wood bottle cable capsule hazelnut pill toothbrush transistor zipper MEAN
×AnoGAN [23]54.2 64.1 49.7 62.1 85.8 78.0 84.1 87.1 86.8 90.0 79.9 78.1 75.0
20.4 37.8 17.7 38.6 62.0 38.3 30.6 69.8 77.6 74.9 54.9 46.7 47.4
Vanilla V AE [15]62.0 83.5 52.0 69.9 89.4 81.6 90.7 95.1 87.9 95.3 85.1 77.5 80.8
61.9 64.9 24.2 57.8 70.5 77.9 77.9 77.0 79.3 85.4 61.0 60.8 66.6
AE-SSIM [4]87.0 78.0 59.0 73.0 93.0 82.0 94.0 97.0 91.0 92.0 80.0 88.0 84.5
64.7 56.1 17.5 60.5 83.4 47.8 86.0 91.6 83.0 78.4 72.4 66.5 67.3
Ours 90.8 99.6 97.9 93.8 95.9 92.1 96.1 98.0 96.2 98.9 97.4 95.4 96.0
(FNF) 91.0 99.7 95.8 96.2 94.0 86.4 93.1 97.3 96.3 97.7 91.4 95.1 94.5
Res18 S-T [3]93.5 97.8 92.5 92.1 97.8 91.9 96.8 98.2 96.5 97.9 73.7 95.6 93.7
87.9 94.5 94.6 91.1 93.1 81.8 96.8 96.5 96.1 93.3 66.6 95.1 90.6
WRes50 SPADE [6]97.5 97.6 87.4 88.5 98.4 97.2 99.0 99.1 96.5 97.9 94.1 96.5 95.8
94.7 97.2 75.9 87.4 95.5 90.9 93.7 95.4 94.6 93.5 97.4 92.6 92.4
WRes50 PaDiM [7]99.1 99.2 94.1 94.9 98.3 96.7 98.5 98.2 95.7 98.8 97.5 98.5 97.5
96.2 97.8 86.0 91.1 94.8 88.8 93.5 92.6 92.7 93.1 84.5 95.9 92.3
EfﬁB5 CS-Flow [22]98.0 98.4 93.9 88.6 90.9 95.3 97.9 96.3 95.7 96.3 95.5 96.4 95.3
98.0 98.5 94.5 92.9 88.7 94.0 96.1 95.1 91.1 89.9 96.9 95.4 94.2
DTD DRÆM [29]94.9 96.6 99.6 97.3 97.6 95.4 94.0 99.2 95.0 98.1 90.0 94.4 96.0
96.1 97.9 99.7 97.9 97.2 90.4 96.5 98.7 93.7 97.1 92.9 94.7 96.1
Res18 Ours97.4 98.7 97.1 97.0 97.8 91.8 98.6 98.1 96.1 98.5 96.9 96.6 97.1
97.2 99.2 97.2 97.9 95.5 90.3 98.3 98.1 96.1 97.9 94.7 95.4 96.5
more helpful for generalization. For experiment II , it shows
that the latent template beneﬁts the performance, and object
classes are improved greatly. It is because the category-
speciﬁc latent template reduces intra-class variance, helping
convergence during training. Then, experiment III suggests
that multi-scale differences had a more pronounced impact
on textural classes, as higher-level operators with larger re-
ceptive ﬁelds correspond to large defects. Finally, Experi-
ment IV reveals that Fourier loss (9) is the icing on the cake
that helps performance improvement. To summarize, meth-
ods I -III are critical for the proposed model, while tricks
IV help further improvements.
4.4. Anomaly Localization
MVTecAD. We performed defect localization for 12 regis-
tered classes in MVTecAD. In our comparisons, the method
based on pre-trained models or using external datasets is
viewed as requiring external prior, corresponding to the ﬁrst
column of Tab. 3. In our implementation, we augment tex-
tural classes with ﬂips and rotations, each with a probability
of 0.5, while object categories do not undergo any augmen-
tation operation. It is worth noting that those base on com-
plex augmentation or weak supervision is not considered in
our comparisons, as our approach is capable of incorporat-
ing these techniques to improve performance. The detailed
results are shown in Tab. 3.
First, we take three methods based on image contrast,
AnoGAN [23], Vanilla V AE [15], and AE-SSIM [4]. They
are not dependent on external datasets, so it is fair to com-pare them with our FNF model. Furthermore, we also com-
pared our method to those that utilize external priors, such
as S-T, SPADE, etc. All methods were reproduced based
on the ofﬁcial implementation or AnomaLib [1]. For fair
comparisons, we adapted the 1×1convolution Wto the
pre-trained encoder, where the pre-trained encoder is the
ﬁrst two layers of ResNet18 for extracting the image into
features of original 1/4 size and with 64 channels.
As shown in Tab. 3, our FNF method greatly outperforms
the comparable methods without external priors, even ex-
ceeding S-T, SPADE, and CS-Flow that using external pri-
ors. Most of the reconstruction-based methods in Tab. 3
suffer from ill-posedness in complex scenarios ( e.g., tile and
wood.), while our method achieves the best AUPRO score
owing to high-resolution contrast in latent space. How-
ever, a larger resolution implies larger intra-class variance,
which degrades the overall AUROC performance for hard-
to-determine anomaly boundaries. Furthermore, Fig. 7(a)
visualizes representative examples of MVTecAD anomaly
localization, which shows that our method achieves precise
localization with reasonable scale.
BTAD. To fully illustrate our superiority, we experi-
mented on the more challenging BTAD dataset without
any data augmentation, and other settings are the same
as MVTecAD. The detailed result in Tab. 4 shows that
our method also achieves state-of-the-art performance, and
Fig. 7(b) visualizes representative examples of BTAD
anomaly localization.
7Image Template Ours GTtile wood leather toothbrush cable capsule grid metal nut screw hazelnut 01 02
(a) MVTecAD (b) BTAD (c) w/o registrationFigure 7. Visualization of our results on MVTecAD and BTAD. From top to bottom are original images, estimated image templates, our
localization results, and ground truths. (a)The six challenging results on MVTecAD. (b)Two representative results on BTAD. (c)Results
for the four unregistered categories on MVTecAD.
Table 4. Quantitative results of various challenging meth-
ods on BTAD. For each case in the table, the ﬁrst row is
Image-AUROC% and the second is Pixel-AUROC% , where
the best results are marked in bold.
MethodsClasses
MEAM01 02 03
VT-ADL [16]97.6 71.0 82.6 83.7
99.0 94.0 77.0 90.0
P-SVDD [26]95.7 72.1 82.1 83.3
91.6 93.6 91.0 92.1
SPADE [6]91.4 71.4 99.9 87.6
97.3 94.4 99.1 96.9
PatchCore [20]90.9 79.3 99.8 90.0
95.5 94.7 99.3 96.5
PaDiM [7]99.8 82.0 99.4 93.7
97.0 96.0 98.8 97.3
Ours (Res18)100.0 88.2 99.3 95.8
97.4 97.6 98.1 97.7
4.5. Study on unregistered categories
In principle, template-based methods require pixel-level
registration between images and templates, which is typ-
ically satisﬁed in most real-world scenarios, but may fail
in some cases. This subsection explores the performance
of the proposed method on unregistered ( e.g. rotation,
shift) categories ( e.g. grid, metal nut, screw, and hazel-
nut), as shown in Tab. 5. The reconstruction-based AE-
SSIM heavily relies on pixel-level contrast, which decreases
the average localization accuracy (AUPRO%). In contrast,
the anomaly-based SPADE avoids registration issues and
achieves better performance. Fortunately, our ResNet18-
based method, which utilizes normalizing ﬂow to reduce
patch variance, remains competitive in unregistered scenes,
although it falls short of state-of-the-art performance. Wevisualize these categories in Fig. 7(c).
Table 5. Quantitative results on unregistered categories without
Rough Registration (RR) or Fine Registration (FR). For each case
in the table, the ﬁrst row is Pixel-AUROC% and the second is
AUPRO% , where the best results are marked in bold.
MethodsClasses
MEANw/o RR w/o FR
grid metal nut screw hazelnut
AE-SSIM [4]94.0 89.0 96.0 97.0 94.0
84.9 60.3 88.7 91.6 81.4
SPADE [6]93.7 98.1 98.9 99.1 97.5
86.7 94.4 96.0 95.4 93.1
Ours (Res18)95.7 97.2 94.6 98.1 96.4
94.3 91.4 94.7 98.1 94.6
5. Conclusion
In this paper, we propose PyramidFlow, the ﬁrst fully
normalizing ﬂow method based on the latent template-based
contrastive paradigm, utilizing pyramid-like normalizing
ﬂows and volume normalization, enabling high-resolution
defect contrastive localization. Our method can be trained
end-to-end from scratch, similar to UNet, and our compre-
hensive experiments demonstrate that it outperforms com-
parable algorithms that do not use external priors, even
achieving state-of-the-art performance in complex scenar-
ios. While experiments on unregistered categories show
that our method falls short of state-of-the-art, it still exhibits
competitive performance. Future research will focus on im-
proving performance in such scenarios.
Acknowledgment. We would like to extend sincere appre-
ciation to Jiabao Lei for his valuable guidance and insight-
ful suggestions, which greatly contributed to the success of
this work.
8References
[1] S. Akcay, D. Ameln, A. Vaidya, B. Lakshmanan, N. Ahuja,
and U. Genc. Anomalib: A deep learning library for anomaly
detection. In 2022 IEEE International Conference on Image
Processing (ICIP) , pages 1706–1710.
[2] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Mvtec ad–a comprehensive real-world
dataset for unsupervised anomaly detection. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 9592–9600.
[3] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Uninformed students: Student-teacher
anomaly detection with discriminative latent embeddings. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 4183–4192.
[4] Paul Bergmann, Sindy L ¨owe, Michael Fauser, David Sattleg-
ger, and Carsten Steger. Improving unsupervised defect seg-
mentation by applying structural similarity to autoencoders.
InVISIGRAPP (5: VISAPP) , 2019.
[5] Jakob Bo ˇziˇc, Domen Tabernik, and Danijel Sko ˇcaj. Mixed
supervision for surface-defect detection: From weakly
to fully supervised learning. Computers in Industry ,
129:103459, 2021.
[6] Niv Cohen and Yedid Hoshen. Sub-image anomaly
detection with deep pyramid correspondences. CoRR ,
abs/2005.02357, 2020.
[7] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and
Romaric Audigier. Padim: a patch distribution modeling
framework for anomaly detection and localization. In Inter-
national Conference on Pattern Recognition , pages 475–489.
Springer.
[8] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE:
non-linear independent components estimation. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Confer-
ence on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Workshop Track Proceedings ,
2015.
[9] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
Density estimation using real NVP. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Pro-
ceedings . OpenReview.net, 2017.
[10] Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka.
Cﬂow-ad: Real-time unsupervised anomaly detection with
localization via conditional normalizing ﬂows. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision , pages 98–107.
[11] Durk P Kingma and Prafulla Dhariwal. Glow: Generative
ﬂow with invertible 1x1 convolutions. Advances in neural
information processing systems , 31, 2018.
[12] Polina Kirichenko, Pavel Izmailov, and Andrew G Wil-
son. Why normalizing ﬂows fail to detect out-of-distribution
data. Advances in neural information processing systems ,
33:20578–20589, 2020.
[13] Feng Li, Feng Li, and QingGang Xi. Defectnet: Toward
fast and effective defect detection. IEEE Transactions on
Instrumentation and Measurement , 70:1–9, 2021.[14] J. Luo, Z. Yang, S. Li, and Y . Wu. Fpcb surface defect
detection: A decoupled two-stage object detection frame-
work. IEEE Transactions on Instrumentation and Measure-
ment , 70:1–11, 2021.
[15] Takashi Matsubara, Kazuki Sato, Kenta Hama, Ryosuke
Tachibana, and Kuniaki Uehara. Deep generative model us-
ing unregularized score for anomaly detection with heteroge-
neous complexity. IEEE Transactions on Cybernetics , 2020.
[16] P. Mishra, R. Verk, D. Fornasier, C. Piciarelli, and G. L.
Foresti. Vt-adl: A vision transformer network for image
anomaly detection and localization. In 2021 IEEE 30th Inter-
national Symposium on Industrial Electronics (ISIE) , pages
01–06.
[17] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in pytorch. 2017.
[18] R. Ren, T. Hung, and K. C. Tan. A generic deep-learning-
based approach for automated surface inspection. IEEE
Transactions on Cybernetics , 48(3):929–940, 2018.
[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234–241. Springer, 2015.
[20] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard
Sch¨olkopf, Thomas Brox, and Peter Gehler. Towards to-
tal recall in industrial anomaly detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14318–14328.
[21] Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same
same but differnet: Semi-supervised defect detection with
normalizing ﬂows. In Proceedings of the IEEE/CVF winter
conference on applications of computer vision , pages 1907–
1916.
[22] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bas-
tian Wandt. Fully convolutional cross-scale-ﬂows for image-
based defect detection. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision ,
pages 1088–1097.
[23] Thomas Schlegl, Philipp Seeb ¨ock, Sebastian M Waldstein,
Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised
anomaly detection with generative adversarial networks to
guide marker discovery. In International conference on in-
formation processing in medical imaging , pages 146–157.
Springer.
[24] Domen Tabernik, Samo ˇSela, Jure Skvar ˇc, and Danijel
Skoˇcaj. Segmentation-based deep-learning approach for
surface-defect detection. Journal of Intelligent Manufactur-
ing, 31(3):759–776, 2020.
[25] Xian Tao, Xinyi Gong, Xin Zhang, Shaohua Yan, and Chan-
dranath Adak. Deep learning for unsupervised anomaly lo-
calization in industrial images: A survey. IEEE Transactions
on Instrumentation and Measurement , 2022.
[26] Jihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd
for anomaly detection and segmentation. In Proceedings of
the Asian Conference on Computer Vision .
9[27] Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu,
Rui Zhao, and Liwei Wu. Fastﬂow: Unsupervised anomaly
detection and localization via 2d normalizing ﬂows. CoRR ,
abs/2111.07677, 2021.
[28] Xuyi Yu, Wentao Lyu, Di Zhou, Chengqun Wang, and
Weiqiang Xu. Es-net: Efﬁcient scale-aware network for tiny
defect detection. IEEE Transactions on Instrumentation and
Measurement , 71:1–14, 2022.
[29] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko ˇcaj. Draem-
a discriminatively trained reconstruction embedding for sur-
face anomaly detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 8330–
8339, 2021.
[30] N. Zeng, P. Wu, Z. Wang, H. Li, W. Liu, and X. Liu. A small-
sized object detection oriented multi-scale feature fusion ap-
proach with application to defect detection. IEEE Transac-
tions on Instrumentation and Measurement , 71:1–14, 2022.
[31] Jiabin Zhang, Hu Su, Wei Zou, Xinyi Gong, Zhengtao
Zhang, and Fei Shen. Cadn: A weakly supervised learning-
based category-aware object detection network for surface
defect detection. Pattern Recognition , 109:107571, 2021.
10Supplementary Material for
PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid
Normalizing Flow
1. Implementation Details
1.1. Experimental Settings
Hardware. We implemented our models in Python3.8 and Pytorch1.10. Experiments are run on NVIDIA GTX3060 GPUs.
Baseline method. We train our baseline model on 256×256image. During all experiments, the training batch size is ﬁxed
to 2. Model parameters are updated using Adam optimizer with a constant learning rate of 2×10−4, epsilon of 1×10−4,
weight decay of 1×10−5, and beta parameters of (0.5,0.9). In addition, we apply gradient clipping with a maximum gradient
of1.0for training stability.
Pre-trained method. For the pre-trained version of PyramidFlow, we used ImageNet-pretrained ResNet18 from torchvision.
The pre-trained encoder is the ﬁrst two layers of ResNet18 for extracting the features from 1024 ×1024 image to 256×256
features with 64 channels.
1.2. Model Architecture
In this subsection, we provide the detailed architecture of the proposed PyramidFlow, including invertible pyramids,
pyramid coupling blocks, and volume normalization.
Invertible Pyramid. The invertible pyramid is inspired by the Laplacian pyramid, which is commonly used in image pro-
cessing. In invertible pyramids, the pyramid decomposition and composition are performed on the per-channel features. The
linear downsampling operator D(·)ﬁrst applies a Gaussian ﬁlter with kernel size 5×5, then downsamples using nearest-
neighbor interpolation. In contrast, upsampling U(·)performs nearest-neighbor interpolation before applying Gaussian ﬁl-
tering.
Pyramid Coupling Block. For the example of dual coupling blocks, denoting the feature notations as shown in Fig. 3(d), the
corresponding pseudocode is described in Algorithm 1. It is mainly composed of three custom functions - AfﬁneParamBlock,
VolumeNorm2d, andInvConv .
Volume Normalization. The proposed volume normalization is similar to some normalization techniques such as Batch
Normalization, but without normalizing the standard deviation. Taking Channel V olume Normalization (CVN) as an example,
it can be described by the Algorithm 2.
2. More Experiment Results
2.1. Detailed Ablation Results
We present the detailed ablation results of Sec 4.3, as shown in Tables S1 and S2.
Textural Image. As shown in Table S1. For most textural categories, occurring performance degradation when the proposed
methods are ablated. However, the results on the carpet show abnormal performance improvement. This means that inductive
bias brings positive or negative effects on various categories.
Object Image. As shown in Table S2. Due to the image patch in object categories with larger variances, the inﬂuence of
volume normalization and the latent template is also larger. The performance of the object categories is less inﬂuenced by
pyramid difference, indicating that multi-scale is not a critical factor for object defect detection.arXiv:2303.02595v1  [cs.CV]  5 Mar 2023Algorithm 1 Dual Coupling Block. (Python-like Pseudocode)
Input:x0,x1,x2
Output:z0,z1,z2
xcat0= Interpolate( x0,x1.shape)
xcat2= Interpolate( x2,x1.shape)
xcat= Concat( xcat0,xcat2)
s1,t1= AffineParamBlock( xcat)
y1=exp (s1)⊙x1+t1
z0,z1,z2=x0, InvConv( y1),x2
def AffineParamBlock(x, clamp=2):
params = CNN2d(x) % only two convolutional layers and one
activation layer
s0,t= Chunk2d(params)
s= VolumeNorm2d(clamp *0.636 *atan(s0/clamp)) % as shown in
Algorithm 2. Where 0.636 is an approximation of 2/π.
returns,t
def InvConv(y):
˜si=si- mean(si)
kernel = PL(U+diag (exp (˜si))
z = Conv2d(y, kernel)
return z
Algorithm 2 V olume Normalization. (Pytorch-like Pseudocode)
Input: inputx, momentum β
Output: outputy
def VolumeNorm2d( x,β= 0.1):
iftraining:
¯x= mean(x, dim=1) % CVN: zero-mean normalization along
channel dimensions
y=x-¯x
¯xrunning = (1−β)×¯xrunning +β×¯x% update running mean
else:
y=x-¯xrunning
returny
Table S1. The ablation study on textural images in MVTecAD. For each cell in the table, the ﬁrst row is Pixel-AUROC% and the second
isAUPRO% .
MethodTexture
Meancarpet grid leather tile wood
Ours (baseline)90.8 94.2 99.6 97.9 93.8 95.2
91.0 92.7 99.7 95.8 96.2 95.1
I. w/o V olume
Normalization93.5 88.5 99.5 74.4 91.3 89.4
93.7 88.1 95.5 65.7 94.2 87.5
II. w/o Latent
Template91.8 86.8 99.4 94.8 93.0 93.1
91.3 88.0 97.7 89.9 92.7 91.9
III. w/o Pyramid
Difference75.9 78.0 99.3 96.0 89.7 87.8
76.1 76.1 99.4 94.4 93.0 87.8
IV. w/o Fourier
Loss90.5 84.3 99.4 96.2 89.7 92.0
91.4 86.2 99.6 92.6 94.0 92.8Table S2. The ablation study on object images in MVTecAD. For each cell in the table, the ﬁrst row is Pixel-AUROC% and the second
isAUPRO% .
MethodObject
Meanbottle cable capsule hazelnut metalnut pill screw toothbrush transistor zipper
Ours (baseline)95.9 92.1 96.1 98.0 92.8 96.2 94.0 98.9 97.4 95.4 95.7
94.0 86.4 93.1 97.3 89.5 96.3 94.1 97.9 91.4 95.1 93.5
I. w/o V olume
Normalization76.5 84.7 82.9 97.9 87.9 94.8 94.1 56.4 82.2 95.0 85.2
77.8 75.1 81.3 95.4 81.5 81.5 94.0 74.2 82.7 92.6 83.6
II. w/o Latent
Template83.2 87.8 90.0 97.9 87.6 94.6 93.0 84.7 94.8 93.7 90.7
82.4 76.6 87.3 83.9 74.2 89.3 92.7 90.7 77.4 92.8 84.7
III. w/o Pyramid
Difference92.8 91.4 96.0 97.5 86.4 95.3 92.7 98.0 95.4 85.2 93.1
83.4 84.1 94.0 97.6 81.2 95.4 93.1 97.1 90.7 77.2 89.4
IV. w/o Fourier
Loss88.0 88.6 95.1 97.3 88.9 96.2 94.2 98.3 95.1 90.9 93.3
88.0 81.2 94.0 98.3 89.0 96.9 94.4 97.9 88.0 90.8 91.9
carpet leather tile wood bottle cable capsule hazelnut pillImage Ours GT
 PaDiM
 SPADE
 AE-SSIM
Figure S1. Visualization of competitive results on MVTecAD. From top to bottom are original images, AE-SSIM results, SPADE results,
PaDiM results, our results, and ground truths. The red box indicates the localization is ambiguous and non-unique, while the green indicates
successful results.
2.2. More Visualization Results
In this subsection, we present more visualization results of Sec 4.4. Since many categories, we separated results into two
charts for visualization, as shown in Figs. S1 and S2.
MVTecAD. As Figs. S1 and S2 shows, AE-SSIM performs better for simple categories, such as the bottle and zipper.
However, it does not work in complex scenarios, e.g., it cannot localize carpet defects with ﬁxed patterns or pill defects with
high-frequency noises. It is worth noticing that AE-SSIM is a template-based method, which maintains the resolution during
processing, enabling preserve the details in defect localization.
SPADE and PaDiM are pre-trained-based methods. They achieve better results in almost all categories but still maintaintoothbrush transistor zipper grid metal nut screw 01 02 03 Image Ours GT PaDiM SPADE
 AE-SSIMFigure S2. Visualization of competitive results on MVTecAD and BTAD. From top to bottom are original images, AE-SSIM results,
SPADE results, PaDiM results, our results, and ground truths. The last three columns are the results of BTAD. The red box indicates the
localization is ambiguous and non-unique, while the green indicates successful results.
some shortcomings. On the one hand, their localization results are blurry and larger than ground truths. On the other hand,
they cannot localize tiny defects, such as cracks in the wood.
Our proposed PyramidFlow is based on latent templates, which allows for preserving details effectively, with the ability
to detect tiny defects and show their scale. In all categories in MVTecAD, our method achieves the best visual performance.
BTAD. BTAD is more challenging than MVTecAD, as shown in the last three columns of Fig. S2. The AE-SSIM method
almost failed in BTAD without beneﬁcial results. For categories 01 and 02, the localization areas of SPADE and PaDiM are
obviously larger than ground truths. For the most challenging category 03, their results are incredibly varied from GT.
Our method provides more accurate results for BTAD defect localization. For the 01 categories, the localization results
preserve the original details. Categories 02 and 03 also mostly reﬂect the essential shape of the defect.