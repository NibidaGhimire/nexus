AI-Augmented Surveys: Leveraging Large Language
Models and Surveys for Opinion Prediction*
Junsol Kim
Department of Sociology
University of ChicagoByungkyu Lee†
Department of Sociology
New York University
Abstract
Large language models (LLMs) that produce human-like responses have begun to revo-
lutionize research practices in the social sciences. We develop a novel methodological
framework that fine-tunes LLMs with repeated cross-sectional surveys to incorporate the
meaning of survey questions, individual beliefs, and temporal contexts for opinion predic-
tion. We introduce two new emerging applications of the AI-augmented survey: retrodic-
tion (i.e., predict year-level missing responses) and unasked opinion prediction (i.e., predict
entirely missing responses). Among 3,110 binarized opinions from 68,846 Americans in
the General Social Survey from 1972 to 2021, our models based on Alpaca-7b excel in
retrodiction (AUC = 0.86 for personal opinion prediction, ρ= 0.98 for public opinion pre-
diction). These remarkable prediction capabilities allow us to fill in missing trends with
high confidence and pinpoint when public attitudes changed, such as the rising support
for same-sex marriage. On the other hand, our fine-tuned Alpaca-7b models show mod-
est success in unasked opinion prediction (AUC = 0.73, ρ= 0.67). We discuss practical
constraints and ethical concerns regarding individual autonomy and privacy when using
LLMs for opinion prediction. Our study demonstrates that LLMs and surveys can mutually
enhance each other’s capabilities: LLMs can broaden survey potential, while surveys can
improve the alignment of LLMs.
*This work was supported by the National Science Foundation (#2116936) and in part by Lilly Endowment,
Inc., through its support for the Indiana University Pervasive Technology Institute. This work was completed
in part with resources provided by the University of Chicago’s Research Computing Center. We thank Peter
Bearman, Delia Baldassarri, Jonathan Bach, Bart Bonikowski, Philipp Brandt, Siwei Cheng, Sarah Cowan, Dalton
Conley, Yuting Chen, Ryan Dai, Paul DiMaggio, James Evans, Gil Eyal, Ryan Hagen, Mark Hoffman, Mike Hout,
Theodora Hurley, Robert Max Jackson, Wontak Joo, Donghyun Kang, Hyunku Kwon, Keunbok Lee, So Yoon
Lee, Seungwon Lee, John Levi Martin, Kinga Makovi, Lina Moe, Ravaris Moore, Austin Kozlowski, Sebastian
Ortega, Barum Park, Bernice Pescosolido, Brian Powell, Alix Rule, Diana Sandoval Siman, David Stark, Daniel
Tadmon, Josh Whitford, Tytus Wilam, Yoosik Youm, Linda Zhao, Simone Zhang, the members of Knowledge Lab
at the University of Chicago, and the members of Networks in Context Lab for their helpful comments. This work
was presented at the 9th IC2S2 conference, the 2023 American Sociological Association Annual Meeting, Korea
Inequality Research Network Symposium, the CODES seminar at Columbia University, the Inequality Workshop
at NYU and the Generative Articial Intelligence and Sociology Workshop at Yale University. Counter-factual
public opinion trends predicted by our approach are available from: https://augmented-surveys-retrodict.hf.space.
All data and code necessary for replicating our analyses will be made available upon the paper’s publication.
†Corresponding Author: Byungkyu Lee, bklee@nyu.eduarXiv:2305.09620v3  [cs.CL]  7 Apr 2024Introduction
Predicting opinion trends on a range of social issues, from climate change to gay marriage,
is crucial for making informed decisions, tracking social changes, and understanding the dy-
namics of opinion formation (Brooks and Manza, 2006; Burstein, 2003). Recently, numerous
breakthroughs have been made to infer and predict people’s opinions and preferences from
their written records, such as books in the past (e.g., Google Ngram), internet search patterns
(e.g., Google Trend), and public sentiments in social media (e.g., Twitter, Facebook, YouTube)
(Beauchamp, 2017; Grimmer et al., 2022; Moore et al., 2019; O’Connor et al., 2010; Stephens-
Davidowitz, 2017). However, using digital trace data for predicting public opinion presents a
substantial challenge, as these “proxy” measures cannot be deemed reliable without validating
them against other “ground truth” benchmarks, like surveys (Beauchamp, 2017; Ferraro and
Farmer, 1999). Even if digital trace data can closely track public opinion trends, its unobtrusive
and anonymous nature prompts questions about its ability to truly represent the diverse voices
of the population, particularly considering the skewed representation of demographic groups in
digital traces (Cesare et al., 2018).
Surveys have been an essential tool to measure and predict opinions in academic and market
research for a long time. Among others, the General Social Survey (GSS) – a nationally repre-
sentative survey with exceptional quality – has been widely used to track Americans’ opinions
on various social issues and their cultural preferences since the 1970s over a half-century (Mars-
den et al., 2020). However, one of the practical challenges in survey research is that the number
of questions that can be asked in a single survey is limited due to respondent fatigue and cost
(Berinsky, 2017; Couper, 2017). This is especially the case for repeated cross-sectional, na-
tionally representative surveys like the GSS, where most survey items are collected only once
or twice, which makes it hard to monitor all public opinion trends. New questions about ma-
jor social shifts could only be added to these surveys after they attracted considerable public
interest, leading to a lag that limits the surveys’ effectiveness in identifying turning points and
continuities in historical social changes. While surveys provide a precise and representative
measure of public opinion, their range in capturing the full spectrum of public views over time
is limited.
How can we predict a broad spectrum of public opinion in the dynamic social world without
compromising on accuracy and representativeness while simultaneously addressing the limita-
tions of both digital trace and survey data? This paper investigates whether we can address these
challenges by fine-tuning large language models (LLMs) to predict survey responses in nation-
ally representative surveys. Recent studies have already suggested the possibility of using LLMs
trained on massive amounts of digital traces and text data to predict public opinion by leveraging
the remarkable capability of LLMs in mimicking human responses (Aher et al., 2023; Argyle
et al., 2023; Chu et al., 2023; Dillion et al., 2023; H ¨am¨al¨ainen et al., 2023; Horton, 2023; Jiang
et al., 2022; Mei et al., 2024; Meta Fundamental AI Research Diplomacy Team (FAIR) et al.,
2022; Schramowski et al., 2022). However, the assumption behind these attempts is that LLMs
can directly simulate the population-level average responses to a survey question, presuming
1the high levels of alignment of LLMs with the specific population. This assumption has re-
cently been challenged (Santurkar et al., 2023), which is not surprising given well-documented
biases and the skewed representation of demographic groups in the digital trace data used to
train LLMs (Cesare et al., 2018). Instead of building a one-size-fits-all LLM predicting the typ-
ical opinion, we propose a novel method to personalize andcontextualize existing LLMs to be
aligned to individual-specific, heterogeneous beliefs based on their responses to other questions
in specific periods. Our study demonstrates that fine-tuning LLMs with surveys can enhance the
capacity of surveys to predict public opinion by accurately predicting each survey participant’s
answers to new questions over long periods of time in nationally representative surveys.
Our research also critically engages with recent debates about whether LLMs could “re-
place” human subjects (Dillion et al., 2023; Grossmann et al., 2023). Dillion et al (2023)
claimed that LLMs may be able to replace human participants in several psychological ex-
periments on moral judgments. However, this claim is not generalizable to many fields of the
social sciences, especially survey research. While LLMs are trained on a massive amount of
text data generated by humans that could potentially mirror the opinions and behaviors of mil-
lions, social surveys can be useful for steering them to reflect a particular individual’s opinions.
We also need human verification at the end to ensure that its prediction is aligned with highly
dynamic and diverse human voices. In contrast to the LLMs used by Dillion et al. (2023) that
did not intend to capture the dynamic nature of human attitudes across a diverse group of in-
dividuals, survey researchers are especially interested in examining person-to-person variations
as well as longitudinal opinion changes. In this paper, we will demonstrate that LLMs do not
render social surveys obsolete, as they can be used for the alignment of LLMs and validation of
LLMs’ outputs at the minimum. Crucially, we illustrate how LLMs can tackle the challenges of
missing data in social surveys and present new opportunities for social scientists through their
remarkable capability to predict the next token in a question-and-answer sequence.
To anticipate the main arguments, we first introduce the opportunities and challenges of
addressing the issue of unmeasured public opinion in repeated cross-sectional, nationally rep-
resentative surveys, like the GSS. Next, we propose a new methodological framework to incor-
porate the three most important features for opinion prediction – survey questions, individual
beliefs, and survey periods. After introducing data and methods, we evaluate how much LLMs
can improve the predictability of personal and public opinion across three different types of
missing data challenges in the GSS data. Then, we present examples of a novel task – “retro-
diction” (i.e., predicting public opinion on an issue at times when a survey question was not
asked) – based on our models’ remarkable capability to predict year-level missing opinions.
Then, we investigate the performance of our models across different individuals and opinions.
Finally, we discuss privacy concerns, individual autonomy, and the risks of potential misuse that
accompany the creation of a precise opinion prediction tool powered by LLMs (Brayne, 2020;
Floridi et al., 2018).
2Challenges in predicting survey responses
We here introduce three types of challenges in predicting survey responses while focusing on
the opportunities arising from addressing each challenge. First, Figure 1, Panel A illustrates a
common situation in surveys where some respondents fail to answer or skip specific questions
due to refusal and attrition. Although this task has been thoroughly investigated by traditional
missing imputation models (Honaker et al., 2011; Rubin, 1976; van Buuren and Groothuis-
Oudshoorn, 2011), popular multiple imputation techniques, including Amelia and MICE, do
not perform well in cases of imputing categorical responses in sparse data sets (Sengupta et al.,
2023). Increasingly sparse survey data due to high attrition (e.g., online surveys) or complex
designs (e.g., split-ballot design) can be tackled more effectively with an accurate prediction of
missing responses. We use the term “missing data imputation” to refer to predicting response-
level missing data1.
Z Y X ID Year
? Disagree Agree 1 2016
Agree Agree ? 2 2016
? Disagree Agree 3 2016
Agree Agree ? 4 2018
? Agree Agree 5 2018
Disagree ? Agree 6 2018A.Missing Data Imputation B. Retrodiction C. Unasked Opinion Predic tion
Z Y X ID Year
? Disagree Agree 1 2016
? Agree Disagree2 2016
? Disagree Agree 3 2016
Agree Agree ? 4 2018
Disagree Agree ? 5 2018
Disagree Agree ? 6 2018Z Y X ID Year
? Disagree Agree 1 2016
? Agree Disagree2 2016
? Disagree Agree 3 2016
? Agree Disagree4 2018
? Agree Agree 5 2018
? Agree Agree 6 2018
Figure 1: Three types of missing problems in survey research. Panels A-C illustrate three
typical missing data challenges in survey research. Each row indicates an individual subject in
a social survey across different periods, and each column (i.e., X, Y , and Z) indicates public
opinion variables that we aim to measure. The machine learning task in each situation is to
predict the unobserved values [?] in the black cells using the observed values in the white cells.
Panel B presents a scenario that arises in repeated cross-sectional surveys, where certain
questions were not asked in some periods, resulting in year-level missing data. By predicting
responses for the missing years, we can “retrodict” trends and patterns that would have emerged
if the data had been collected consistently every year. For example, the question of whether
same-sex couples have the right to marry one another has been asked since 2008 in the General
1Note that the goal of machine learning prediction is different from that of traditional “missing data imputation”
approaches even though both aim to recover unknown values and quantities of a variable. While missing imputation
concerns the bias and errors of estimators arising from missing values, machine learning prediction focuses on
accurately recovering data points while avoiding over-fitting. We optimize our model using metrics such as AUC,
accuracy, or the F1 score suited for machine learning prediction since our goal is to “predict” missing opinions.
However, we use the term missing data imputation because this term has been widely used to refer to the task of
recovering partially missing responses for some individuals in a survey
3Social Survey. How do we know Americans’ public opinion on same-sex marriage in the 1970s?
Can we precisely pinpoint when public attitudes toward same-sex marriage started to shift?
Developing a device to retrodict missing responses can open an entirely new opportunity for
understanding historical changes, given that survey questions addressing specific issues tend to
be introduced only after societies become aware of social changes concerning those issues (Behr
and Iyengar, 1985; Downs, 1972; Hilgartner and Bosk, 1988). Additionally, survey designers
can utilize this device for question selection since it enables them to focus on less predictable
questions or conversely those expected to shift. We use the term “retrodiction” to refer to
predicting year-level missing data.
Panel C presents a scenario where the goal is to predict individuals’ responses to a ques-
tion that has never been asked in the existing survey data. This unasked opinion prediction
task with LLMs has been proposed by recent studies, motivated by their abilities to understand
questions and generate human-like responses (Argyle et al., 2023; Chu et al., 2023; Jiang et al.,
2022; Santurkar et al., 2023). Considering the limited number of questions that can be prac-
tically included in a survey, developing a device that predicts unasked personal opinions will
offer unprecedented opportunities for social science communities, businesses, organizations,
and policy-makers. For example, researchers could use this device to predict people’s prefer-
ences over beverages that have never been measured in the existing survey data (Brand et al.,
2023; DellaPosta et al., 2015). Or, it could allow researchers to study people’s opinions on
sensitive issues without directly asking them, particularly when asking sensitive questions may
affect response quality and non-response error (Yan, 2021). Thus, achieving high accuracy in
this task suggests the potential to infinitely expand the number of variables we can predict, un-
locking unparalleled opportunities. We use the term “unasked opinion prediction” to refer to
predicting responses to a question without any prior survey responses about the question in the
training data.
Promises and Challenges of Machine Learning in Opinion Pre-
diction
How can we predict individuals’ opinions on various issues? Research shows that the formation
of opinion depends on personal characteristics like race, gender, age, education, income, and
particularly partisanship (Baldassarri and Goldberg, 2014; Boutyline and Vaisey, 2017; Lazars-
feld et al., 1948). Since individuals sharing the same demographic attributes tend to have similar
opinions, interacting more frequently and reinforcing each others’ opinions, we can use demo-
graphic attributes as predictive features of an individual’s opinions (DellaPosta et al., 2015;
Friedkin and Johnsen, 2011; Joo and Fletcher, 2020). This might be particularly true in the
U.S. in recent periods given the stronger partisan sorting and the collapse of cross-cutting align-
ments (Baldassarri and Gelman, 2008; DellaPosta, 2020; Kozlowski and Murphy, 2021), which
implies that simply knowing one’s partisanship – whether they are Republican or Democrat –
may enable us to predict one’s attitudes towards social issues, such as abortion. Nevertheless,
4despite the strong influence of socio-demographic characteristics and especially partisanship on
opinion formation, it is crucial to recognize that individual belief systems extend beyond group
thinking, and the diversity within cultural belief systems cannot be adequately explained solely
by partisanship or other socio-demographic characteristics (Baldassarri and Goldberg, 2014;
DellaPosta, 2020; DiMaggio et al., 2018).
Cultural network analysis has tackled these issues more directly by focusing on the role
of cultural belief systems. Within this paradigm, knowing an individual’s positions on related
issues enables us to predict their opinions on other issues, given the strong correlations between
beliefs across different topics (Goldberg, 2011). It is because individuals’ beliefs are not just
random; instead, they are correlated to each other, inducing a relational belief structure that
provides the foundation for opinion formation (Baldassarri and Goldberg, 2014; Boutyline and
Vaisey, 2017; Converse, P., 1964). In this network of opinions, opinions are clustered into
larger categories of closely correlated opinions. For instance, views on same-sex marriage are
often associated with beliefs about gender roles, religious convictions, and notions of family
and societal norms. Thus, it is possible to predict one’s opinions about same-sex marriage once
we know their opinions on other relevant issues. Furthermore, the concept of cultural schemas –
“socially shared representations deployable in automatic cognition“ – within a society indicates
that predictions about one’s opinion can be made by relying on these widely held belief systems
(Boutyline and Soter, 2021).
Social science research has recently developed and employed machine learning techniques
to predict individual attitudes and behaviors by capturing these underlying correlations among
beliefs and opinions in survey data (Lall and Robinson, 2022; Molina and Garip, 2019; Salganik
et al., 2020; Sengupta et al., 2023). However, these techniques depend on a critical assumption
for accurately predicting public opinion. That is, survey data contain all essential variables
necessary for predicting missing responses to a particular question. However, due to the limited
number of questions that can practically be included in a survey, it is not always feasible to ask
every question that is needed to recover missing responses on a specific question. For example,
the GSS started to ask a question about respondents’ LGBT status after 2008, which could be
highly predictive of support for same-sex marriage. Without such key features, models might
struggle to predict support for same-sex marriage accurately. Even with a comprehensive set of
variables, existing methods could fail to predict responses that have been entirely missing for a
year or those that have not been asked before (see Panels B and C in Figure 1).
These scientific efforts, unfortunately, have achieved only modest success. A large-scale
collaborative project in 2020 involving 160 teams of social scientists has used and developed a
variety of imputation methods and machine learning techniques to predict individual attitudes
and behaviors in the Fragile Families and Child Wellbeing Study (Salganik et al., 2020). Even
with a rich dataset and the use of state-of-the-art machine learning methods, however, predictive
accuracy for six life outcomes was modest at best, marginally surpassing the performance of
basic benchmark models (e.g., OLS models). Furthermore, the models’ prediction errors were
strongly correlated with the family being predicted more than the specific methods being used.
These results suggest that the existing survey data may miss crucial “variables” that are essential
5for predicting outcomes for some families (Garip, 2020). In other words, enhancing the quantity
and quality of data could be a more effective strategy for overcoming these challenges than
adopting more sophisticated methods. In this paper, we will demonstrate that the text of survey
questions can be a crucial piece of information overlooked by current approaches.
Fine-tuning Large Language Models with Nationally Repre-
sentative Surveys
Recent studies propose that large language models (LLMs) could be a next-generation solution
for addressing some of these challenges faced by survey research based on their remarkable
capabilities in imitating what humans would generate in the next word sequence (Argyle et al.,
2023; Jansen et al., 2023; Santurkar et al., 2023; Schramowski et al., 2022). Given that LLMs
are trained by a wide array of text data, including data with Q&A (questions & answers) for-
mats , it makes sense to use LLMs to predict the next token (i.e., answers) to the prompt (i.e.,
questions). For example, LLMs can choose the answer to the question, “Do you agree with
legalizing same-sex marriage?” in a way that their responses would be well aligned with human
responses. Moreover, LLMs can encode a diverse set of information from extensive corpora
generated by a diverse set of people across different periods, which enable them to learn the
changing meaning of words and culture over time (Kozlowski et al., 2019). It is not surprising
given that LLMs are trained not only on books and newspapers from 50 years ago but also on
social media posts after the legalization of same-sex marriage in the 2010s.
Currently, a dominant approach to use LLMs for predicting opinions in survey data is a
prompt-based approach, proposed by Argyle et al. (2023). In this approach, researchers use
prompts containing an individual’ demographic information, such as age, race, gender, political
attitude, party affiliation, income, and education level, to simulate a persona before asking
survey questions. It is akin to making LLMs “role-play” as a survey participant with a specific
set of demographic attributes and respond to a survey question (Wang et al., 2023). Emerging
research documents the wide applicability of prompt-based LLMs. For example, Argyle et al.
(2023) show that LLMs prompted in this way could simulate aggregate-level responses to close-
ended survey questions, including voting behaviors in the American National Election Studies
(ANES). T ¨ornberg et al. (2023) create realistic personas using a similar strategy and data from
the ANES to populate simulated social media platforms and examine the impact of different
news feed algorithms on the quality of social media content.
However, recent works suggest that prompt-based LLMs may fail to accurately simulate
personal and public opinion. First, even after prompting with demographic information, studies
found that vanilla LLMs have limitations in accurately and equally representing subpopulations
(Abid et al., 2021; Santurkar et al., 2023). Using conditioning prompts generally enhanced
overall performance, but the accuracy varied across different demographic groups (Simmons
and Savinov, 2024). Second, LLMs can generate a typical opinion in a particular demographic
group, though it is difficult to predict a specific individual’s heterogenous opinion, consequently
6narrowing the distribution of responses within groups (Argyle et al., 2023; Gordon et al., 2022;
Kirk et al., 2023). Third, there have been concerns about the composition of demographic
groups that contribute to training data used in pre-training LLMs. Since the majority of the
training data of LLMs are from the internet, it is possible that LLMs would learn biased stereo-
types against a particular demographic group (Simmons and Savinov, 2024). Also, as more
recent text data are part of the pre-training data, LLMs’ predictions could be less accurate for
the past opinions (Gonz ´alez-Gallardo et al., 2023; Longpre et al., 2023). Fourth, LLMs’ re-
sponses could be affected by ordering and labeling biases, which may hamper the reliability
of LLMs in consistently predicting the opinion (Dominguez-Olmedo et al., 2023). In sum, al-
though the data used to train LLMs come from diverse individuals and periods, which gives
them the potential to simulate public opinions from diverse populations, their response pat-
terns do not consistently represent any specific individual from any particular period in the data,
which hampers their practical significance (Santurkar et al., 2023).
Against this background, we propose a new methodological framework by fine-tuning LLMs
to predict individuals’ survey responses using the General Social Survey (GSS), a nationally
representative survey of Americans’ opinions since 1972 (Davern et al., 2021; Marsden et al.,
2020). Fine-tuning is the process of partially updating the parameters of LLMs using a relatively
small set of data, enabling these models to perform specific tasks more accurately. Specifically
in the context of survey data, fine-tuning enables the alignment of LLMs with an individual
with specific beliefs or values, bypassing the need to train the model with a large amount of text
information from scratch. Here, we exploit the overlooked fact that surveys collect data through
a series of texts with the same Q&A format that can be used during fine-tuning processes2. By
incorporating texts of survey questions as part of training data in addition to patterns of unique
individual survey responses, our fine-tuned LLMs enable personalized prediction of individual
responses to various questions. Specifically, our models can capture the textual nuances of sur-
vey questions reflected in the training corpus and infer how individuals interpret the meaning
of questions differently across time based on their response patterns. Since fine-tuning LLMs
allows us to assume that the meaning of survey questions, and the correlations among variables
for these questions, can vary among individuals and change over time, our approach can address
methodological challenges associated with previously non-imputable missing data and unasked
opinions, as shown in Panels B and C of Figure 1, more effectively.
We present an overview of our approach to personalizing LLMs to predict public opinion
in Figure 2. Our approach first predicts individuals’ opinions and then aggregates them at the
population level using survey weights to account for sample selection bias (Panel A). Assuming
accurate prediction of opinions across individuals and effective accounting for sample selection
bias through survey weighting, the predictions generated by our method can be deemed rep-
resentative of the population. However, the standard architecture of LLMs does not account
for individual variability in responses, making it challenging to personalize predictions that are
2Questions and responses, such as “What is 1+1?” and “The answer is 2”, are utilized in fine-tuning LLMs
and developing chatbots. Similarly, surveys, comprising questions like “Do you agree with legalizing same-sex
marriage?” and possible answers “Yes” or “No,” can also serve as data for fine-tuning LLMs.
7suited to specific individuals’ distinctive beliefs and opinions in a specific period. Therefore,
we need to customize the architecture of LLMs to make personalized predictions over time.
In doing so, we incorporate the three most important neural embeddings3for predicting opin-
ions – survey question semantic embedding, individual belief embedding, and temporal context
embedding – that capture latent characteristics of survey questions, individual heterogeneities,
and survey periods, respectively (Panel B). Similar to word embeddings that position similar
words close together, these neural embeddings represent similarities in the meanings of survey
questions, individual beliefs, and temporal contexts in high-dimensional vector spaces. Finally,
our models use these latent features to predict the most plausible answer to a specific question
for each individual at a given moment. This novel architecture enables our models to recognize
that survey responses to the same question can vary among individuals and across different time
periods.
Initially, we use sentence-level embeddings from LLMs pre-trained on vast text corpora
to encode the meaning of survey questions, such as “Do you agree or disagree that homosex-
ual couples have the right to marry one another,” which are later mapped into a latent vector
space (Jurafsky and Martin, 2023). Table A1 demonstrates that the pre-trained LLMs accu-
rately understand the semantic meaning of survey questions and generate human-like answers
even before fine-tuning. We then fine-tune them using actual survey responses to better reflect
survey contexts. During this process, the embedding layers’ weights are updated to make an
accurate prediction of a binary response to questions (0 or 1), resulting in questions with similar
response patterns being closely located in the embedding space. Consequently, seemingly un-
related questions can be positioned more closely if it enhances prediction accuracy due to their
similar response patterns (see Table A2). To determine the optimal model for generating the
best predictions, we conduct extensive experiments with three LLMs with varying architectures
and parameters (Alpaca-7b, GPT-J-6b, RoBERTa-large) (Liu et al., 2019; Taori, Rohan et al.,
2023; Wang and Komatsuzaki, Aran, 2021).
A breakthrough we made for personalizing LLMs is to incorporate individual belief embed-
dings to account for individuals’ heterogeneous responses to survey questions based on hetero-
geneous belief systems (Baldassarri and Goldberg, 2014; Milbauer et al., 2021). To generate
individual belief embeddings, we initially assign random latent features for each individual and
optimize them during the fine-tuning process such that two individuals closely located in this
embedding space have a similar set of beliefs (Gordon et al., 2022). Finally, we use period
3In machine learning, an “embedding” is a method of converting complex data, like words, images, or sounds,
into a numerical format that a computer can understand and process. Specifically, in language models, embeddings
transform words or sentences into a list of numbers (i.e., a numeric vector), capturing their meanings, usage, and
relationships with other words. For instance, words like “cat” and “dog” are encoded with similarities due to
their status as animals but differ in numbers denoting their species. Similarly, a sentence like “I love my cat”
gets an embedding reflecting the sentiment of love, the subject ’I’, and the object ’cat.’ The process of learning
embeddings in a neural network can be somewhat analogous to estimating regression coefficients, as both involve
adjusting numerical values to fit the data best. The embedding vectors estimated from language models can be
used to predict “the next token” in a sentence completion task, and as an extension, answers to the question prompt
among many others.
8“Do you agree or disagree that homosexual couples should have the right to marry one another.”ModelSurvey question
…AgreeDisagreeIndividual-level predictionsPopulation-level aggregation
AgreeDisagree
Agree
Survey questionIndividual IDSemantic embeddingIndividual belief embeddingSurvey yearPeriod embedding
Cross layers
Agree
DisagreeorHigher-order interactionsIndividual-level predictionLatent featuresAlpaca-7b
Dense layersModel architectureABFigure 2: An overview of our methodological framework. In Panel A, we use survey weights
when aggregating individual-level prediction into population-level estimates to account for po-
tential sampling bias. In Panel B, individual belief and period embeddings are initially randomly
assigned but optimized during the fine-tuning process using dense and cross layers. Semantic
embedding, initially estimated by pre-trained LLMs (e.g., Alpaca-7b), is also optimized during
the fine-tuning stage.
embeddings to consider temporal changes in the meaning of questions and individuals’ belief
systems (Joo and Fletcher, 2020; Rule et al., 2015). To generate period embeddings, we initially
assign random latent features for each period, which are optimized such that two adjacent peri-
ods characterized by similar response patterns are located close to each other in the latent space
during the fine-tuning process. Figure A1 shows a two-dimensional projection of three embed-
ding spaces. Finally, we consider the higher-order interactions between three embeddings by
utilizing a deep learning architecture called “Deep Cross Network” (DCN) with a classification
layer that predicts binary survey responses (Gordon et al., 2022; Wang et al., 2021). For detailed
information, please refer to the Appendix A.
Data and Method
Data
Our model framework allows us to predict how a given individual will respond in a given time
period to an existing question for (A) missing data imputation, (B) retrodiction and (C) unasked
9opinion prediction, using their answers to other questions in a survey. We fine-tune pre-trained
LLMs on the General Social Survey (GSS), a nationally representative survey in the United
States. The GSS dataset provides comprehensive information about the demographic character-
istics, political and ideological beliefs, cultural tastes, personal morality, and diverse attitudes
of people in the United States. We use 68,846 individuals’ responses to 3,110 questions col-
lected for 33 repeated cross-sectional data between 1972 and 2021 for fine-tuning the LLMs.
The use of the publicly available GSS data does not constitute research with human subjects be-
cause there is no direct interaction with any individual, and no identifiable private information
is used4. We retrieve the text content of GSS survey questions from GSS data explorer5.
To provide more straightforward interpretations of the different response options, we trans-
form them into a binary response by assigning a value of 1 to positive responses (e.g., agree, yes,
true, likely) and 0 to negative responses (e.g., disagree, no, false, unlikely) through a combina-
tion of manual coding and machine-learning models (see Table A3 for top 50 response options).
For instance, positive responses such as “strongly agree” and “agree” were coded as 1, while
negative responses such as “strongly disagree” and “disagree” were coded as 0. To do so, we
combine machine learning techniques and manual human coding6. We find that our models
demonstrate higher or comparable predictive accuracy for the binarized variables relative to the
original binary variables (see the result section, individual-level and opinion-level heterogeneity
of model accuracy). Among all 7,136 questions, we omit questions that rely on answers to other
questions (for instance, if answering question B required selecting a specific response in ques-
tion A), questions with a continuous response scale, and questions with an excessive number of
response categories (see Figure A2 for the variable selection process), which finally leads to the
final analytic sample of 3,110 questions of which responses could be binarized.
Fine-tuning Models
Figure 2 presents our end-to-end model architecture. Figure A3 provides additional informa-
tion regarding the input and output dimensions of each layer. Our approach to personalizing
LLMs is model-agnostic, meaning we can generate personalized responses using any LLM.
For example, we can employ either decoder-only transformer models with billions of parame-
ters (e.g., ChatGPT, GPT-4, Alpaca, GPT-J) or encoder-only transformer models (e.g., BERT,
RoBERTa). Given the limited availability and reproducibility issues of private LLM models
(e.g., ChatGPT and GPT-4) despite their impressive performance (Aiyappa et al., 2023), we
opt for three widely-used open-source alternatives that demonstrate competitive performance in
4National Opinion Research Center has obtained explicit consent for the sharing of individual-level data, and
additional details can be found in their documentation (Davern et al., 2021).
5https://gssdataexplorer.norc.org/variables/vfilter
6We first utilize the SentenceBERT (all-MiniLM-L6-v2) model to identify whether the meaning of each survey
response option is closer to positive or negative responses (Reimers and Gurevych, 2019). And then, two coders
cross-check these classification results and binarize the response options manually.
10previous natural language processing benchmark tests: Alpaca-7b7, GPT-J-6b, and RoBERTa-
large (Liu et al., 2019; Taori, Rohan et al., 2023; Wang and Komatsuzaki, Aran, 2021).
Our models are designed to process three inputs – survey questions, individual IDs, and the
survey year – and generate the predicted probability of each response option as an output. The
models encode survey questions into a semantic embedding, while individual ID and survey
year are respectively encoded into individual belief and period embeddings. These embeddings
are then concatenated and used as inputs to the DCN, which then captures the higher-order
interactions between them and generates the predictions. During the fine-tuning process, all
embeddings and the DCN are jointly trained to generate predicted probabilities for response
options. In doing so, we utilize Huggingface’s API for incorporating LLMs (Alpaca-7b, GPT-
J-6b, RoBERTa-large) and TensorFlow Recommenders (TFRS) for deploying the DCN. During
the fine-tuning process with the GSS data, model components are jointly trained with the DCN
(see Appendix B for more details about model training). We find that using demographic fea-
tures during the fine-tuning process does not significantly affect the models’ predictability, as
shown in Figure A4. Based on these results, we exclude socio-demographic variables in the
training data but include measures of a 7-point scale political ideology and party affiliation
(i.e., Democrat, Republican, Independent, Others) that are known to be the important factors of
opinion formation.
Our model architecture shares a similar design principle with other NLP models that aim to
predict how different individuals label texts differently, such as the jury learning model (Gordon
et al., 2022). Simultaneously, our model architecture shares a similar goal with a group of mod-
els that use latent features to pinpoint key dimensions of beliefs underlying various opinions,
such as principal component analysis (PCA) and the NOMINATE algorithm (Joo and Fletcher,
2020). It is important to highlight that our model architecture does not impose any specific
missing data mechanisms, such as Missing at Random (MAR) or Missing Completely at Ran-
dom (MCAR). Instead, our model operates on the assumption that these latent factors influence
responses, a concept similar to other machine learning models employed for deducing missing
survey responses, like matrix factorization (Sengupta et al., 2023). A key difference is that
our models assume multiple latent features, including sentence embeddings, interact together
to shape responses, while the matrix factorization model does not.
Model Evaluation
We evaluate the model’s performance in predicting opinions at the individual and population
levels in the GSS data by conducting 10-fold cross-validation to measure their accuracy in
restoring missing data (see Figure A5). Specifically, for predicting response-level missing opin-
ions in the missing data imputation task, we randomly remove 10% of responses and attempt to
7Alpaca is a language model developed through supervised learning from a LLaMA-7B base model, using
52,000 instruction-following demonstrations sourced from OpenAI’s text-davinci-003 (Taori, Rohan et al., 2023).
Similar to ChatGPT, it is trained specifically to answer questions, making it well-suited for use in survey response
prediction.
11predict them using the model. For the year-level missing opinions in the retrodiction task, we
randomly remove 10% of survey questions per survey year and predict the responses to them.
For the entirely missing opinions in the unasked opinion prediction task, we randomly select
10% of survey questions and completely remove the responses to them for all survey years. For
each task, we repeat this procedure ten times to guarantee accurate predictions for all questions,
ensuring that the test data is not included in the training data.
Throughout the analysis, we will present AUC (Area Under the receiver operating charac-
teristic Curve; i.e., the extent to which models accurately predict positive responses over their
prediction of negative responses on a scale ranging from 0 to 1, where 1 represents the perfect
prediction and 0.5 is equivalent to a random guess) because it does not require an arbitrary
threshold to binarize the predicted values8. We take the survey-weighted average of the pre-
dicted probability to measure the proportion of positive responses at the population level and
measure correlations between observed responses and predicted responses across all available
years to evaluate the validity of our public opinion prediction. Finally, we investigate the per-
formance of our models against missing data arising from three different assumptions: Missing
Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random
(MNAR) (see Appendix C and D for more details on the model evaluation and simulation of
missing data).
As we formulate predicting opinions in a survey as imputing missing responses, we con-
sider matrix factorization as a benchmark. Despite the extensive development of missing data
imputation models using statistical relationships between variables, existing methods struggle
to recover missing data when survey responses are sparse (Sengupta et al., 2023). Recent stud-
ies show that machine learning techniques, such as matrix factorization, can fill in missing
responses better than traditional missing imputation models when the existing data are sparse
(Blumenstock et al., 2015; Sengupta et al., 2023), making it the ideal benchmark for our tasks
requiring the prediction of extremely sparse responses, such as “retrodiction”9.Given that the
matrix factorization models do not consider the textual information of survey questions using
this as a benchmark allows us to examine how considering the meaning of survey wording and
phrases and their interactions with latent factors improve the performance of opinion prediction
(see Appendix E for more details on the matrix factorization model).
8We also measure Accuracy, and F1-score of our predictions, and obtain similar results. See Table A4.
9The matrix factorization model has been widely used within the realm of recommender systems for predicting
missing values in a matrix where the rows represent users, the columns denote products, and the elements are the
ratings. The same algorithm has proven effective in substituting missing responses within survey data, wherein
the rows represent individuals, the columns denote opinions, and the elements are their responses (Sengupta et al.,
2023).
12Results
Model Performance for Personal Opinion Prediction
Table A4 shows that Alpaca-7b provides the best results overall across all three tasks among the
three LLMs and matrix factorization model. It confirms that LLMs with a larger number of pa-
rameters show better performance. Interestingly, while the predictive performance gaps across
four models are small for missing data imputation, the gaps become larger for retrodiction as
fewer human responses are used during the fine-tuning process, with the largest gaps observed
for unasked opinion prediction.
Figure 3, Panel A displays the performance of the best model (i.e., Alpaca-7b) for individual-
level predictions across three tasks. Our top-performing model succeeds in the missing impu-
tation task (AUC = 0.866), though the matrix factorization model also shows a similar level of
performance (AUC = 0.852). Given that the matrix factorization model presumes that data are
missing completely at random (MCAR) — a stronger assumption than the missing at random
(MAR) principle that underlies standard multiple imputation frameworks — it is essential to
examine how our models operate across various missing data mechanisms. Using the simulated
GSS data based on three different mechanisms (MCAR, MAR, and MNAR: Missing Not At
Random), Figure A6 shows that our model performs better under MCAR, MAR, and MNAR
compared to matrix factorization. These findings indicate that our model performs better in
inferring answers for not only randomly skipped responses, as seen in split-ballot designs, but
also for non-random systematic refusal.
Our model also succeeds in the retrodiction task (AUC = 0.860), where it needs to predict
the entirely missing responses in a certain year, significantly outperforming the matrix factor-
ization model (AUC = 0.798). We further conduct a posthoc analysis to figure out how our
model can generate highly accurate predictions based on sparse survey data by assessing the
relative importance of features used in our model (Wang et al., 2021)10. Table A5 presents the
importance of various features in our model based on Alpaca-7b. Specifically, we estimate F
using the weights of the first cross layer from the DCN. We find that the semantic embedding
capturing the meaning of survey questions makes the largest contribution to predictions (0.243),
which is followed by the interactions between semantic and period embeddings (0.192). These
results suggest that the higher performance of our models compared to the matrix factorization
model may arise from the advanced capabilities of LLMs in processing the meaning of survey
questions and from its consideration of complex interactions between period embeddings and
10We estimate the importance of features and feature interactions in the model by calculating the root squared
sum of weights, known as the Frobenius norm (Wang et al., 2021). This allows us to estimate the importance of
interactions between different sets of features in the model, such as atobandctod, in predicting responses.
F=vuutbX
i=adX
j=c|Wij|2
.
13Figure 3: Model performance for predicting three types of missing responses at individual
and aggregate levels. Panel A displays the Receiver Operating Characteristic (ROC) curve,
indicating how well a model can predict missing responses at an individual level. We also denote
the AUC (Area Under Curve) values, i.e., the probability of the model ranking a randomly
selected positive response over a randomly selected negative response. Panels B-D depict the
relationship between the observed proportion of those who agree in a survey each year and
the predicted proportion of the agreement for the same opinion. The percentage of correct
predictions within a margin of error of 3% is indicated as ”% Correct ± 3%,” which implies that
the difference between the actual and predicted opinions is 3% or less. We color the predictions
that fall within this range.
14two other embeddings that aid in contextualizing model predictions across different periods.
In the task of unasked opinion prediction, we did not have a benchmark from existing ma-
chine learning models. However, LLMs with billions of parameters indicated that even entirely
missing opinions could be predicted, although their performance was lower compared to the two
aforementioned tasks. For instance, Alpaca-7b, with seven billion parameters, achieved an AUC
of 0.733, and GPT-J-6b, with six billion parameters, achieved an AUC of 0.693. Our model’s
lower performance in the unasked opinion prediction task implies that it is harder to predict an
individual’s opinion on a question that has never been asked in the survey than on a question that
has been asked and answered by other individuals at least once in any other periods. However,
bigger LLMs still outperform smaller models like RoBERTa-large (AUC = 0.571), which is
consistent with the finding that upscaled LLMs significantly enhance task-agnostic, zero-shot,
or few-shot performance without fine-tuning (Brown et al., 2020). To understand the nature of
the large gap in AUCs between the retrodiction and unasked opinion prediction models (0.860
vs 0.733), we evaluate the predictive performance of models fine-tuned with varying amounts of
missing data from 10% missing to 90% missing. Figure A7 shows that performances of missing
data imputation and retrodiction models decrease as a smaller amount of training data is used
during the fine-tuning process, though it is not always the case for unasked opinion prediction
models. In sum, these results demonstrate that personalized LLMs better predict opinions when
trained with more human survey responses.
Model Performance for Public Opinion Prediction
Predicting public opinion is a separate challenge from predicting personal opinion, as it requires
accounting for varying probabilities of sample selection when we aggregate personal opinions.
For instance, if the sampling weights for Black respondents are higher than those for White re-
spondents, biases in our estimates—when weighted by sampling weights—will become greater
when the predictive accuracy is lower among Black individuals. Yet, Panels B-D reveal that
performances of public opinion prediction largely mirror individual-level results. Missing data
imputation and retrodiction models that rely on existing human responses show very high cor-
relations between the observed proportions of positive responses and the predicted proportions
for opinions measured in each survey year ( ρ > 0.98), indicating that our predictions can be
reliably used for trend estimation and correlational analysis. For predicting the population pro-
portion, our models with the conventional 3% margin of error can predict approximately 50%
or more of true survey responses in these two scenarios.
Our model shows a relatively lower correlation ( ρ= 0.68) in unasked opinion prediction.
This result suggests that researchers should be cautious when using LLMs to completely replace
trend estimation and correlational analysis in social science research or high-stake decision-
making. For predicting the population proportion, our model with the 3% margin of error can
predict only about 12% of true survey responses in the unasked opinion prediction task. Even
with the 5% margin of error, our model can predict only 18.7%, which is much lower than
77.4% and 72.8% from both missing data imputation and retrodiction models, respectively.
15The lower performance of unasked opinion prediction models for public opinion prediction
highlights how difficult it is to make a precise prediction at the aggregate levels, even with a
decent size of correlations and relatively high individual-level accuracy.
Retrodicting Counter-factual Trends
How can we utilize the near-perfect correlations between the predicted and observed responses
from our retrodiction model? Figure 4, Panels A1-A4, present counter-factual trends of selected
opinions from our models that retrodict what would likely be observed had these opinions been
asked throughout the entire period. We selected them because they showcase the representative
case scenarios in four typical applications of retrodiction based on the average AUCs and the
patterns of observed years11. We compare the retrodicted trends from the best model against
those from the matrix factorization model (Panels B1-B4), which serves as a null model because
it is a simpler model assuming that individuals’ attitudes on an opinion can be predicted by their
attitudes on other opinions that best predict it when it is asked.
There are several core questions that the GSS continues to ask for a long time to effectively
track changes in public opinion and ensure their validity and reliability. One of them is about
attitudes toward homosexual relationships. Our fine-tuned models, based on Alpaca-7b, can
predict it within a 3% margin of error for most years (Panel A1), but predictions from matrix
factorization models are mostly incorrect (Panel B1). Interestingly, both models can predict
that fewer Americans think that homosexual relationships are wrong in more recent periods.
Although the GSS survey has asked this question nearly every year, our predictions might still
be useful for filling in missing responses in years when it was not asked.
As societal changes occur, the GSS likely modifies its questions to reflect them. The GSS
board might have begun incorporating new questions when they recognize issues as being
salient, such as gay rights in 2008, following events like Massachusetts’ legalization of same-
sex marriage in 2004, or they might have discontinued old questions as issues’ prominence and
relevance in social and political conversations diminished over time, for example, a decline in
the use of busing as a racial desegregation tool in the 1990s. Panels A2 and A3 demonstrate
how our model predictions can be useful for filling in missing responses to questions asked
only during specific periods. Panel A2 displays the counterfactual trends for the proportion of
Americans agreeing that homosexuals have the right to marry one another before 2008. While
matrix factorization models can also predict the general upward trend of support for gay mar-
riage (Panel B2), they make incorrect predictions for most periods and underestimate them in
the 1970s and 1980s.
One unique aspect of this same-sex marriage question is that the GSS asked a slightly differ-
ent version of the same question as part of ISSP’s Family and Gender module, which includes
the word “should” in the sentence in 1988, 2004, and 2021. Figure A8 shows that both results
are quite similar, but the question including the word “should” predicts a slightly lower rate
11Those interested in retrodicted trends of other opinions can find them at: https://augmented-surveys-
retrodict.hf.space/.
16Figure 4: Illustration of the potential application of our models and matrix factorization
models for predicting counter-factual trends in the GSS 1972-2021. The generalized ad-
ditive model was used to estimate the counterfactual trends. We define the correct prediction
when the prediction interval within 3% margin of error includes the observed estimate. The
variable name, response option, and wording of questions for each panel are followed: Panels
A1, B1. “What about sexual relations between two adults of the same sex--do you think it is
always wrong (=1), almost always wrong(=1), wrong only sometimes (=1), or not wrong at all
(=0)?” (homosex). Panels A2, B2. “Do you agree or disagree with the following statement?
Homosexual couples have the right to marry one another. Strongly agree (=1), agree (=1), nei-
ther agree nor disagree (=0), disagree (=0), strongly disagree (=0)” (marhomo1). Panels A3,
B3. “In general, do you favor or oppose the busing of (Negro/Black/African-American) and
white school children from one school district to another? Favor (=1), Oppose (=0)” (busing).
Panels A4, B4. “And how often do you refuse to eat meat for moral or environmental reasons?
Always or Often (=1), Sometimes or Never (=0)” (nomeat).
17of agreement. This result demonstrates the reliability of our predictions across different vari-
ables with similar meanings while capturing the nuanced role of question-wording differences.
Panel A3 presents the counterfactual trends for the proportion of Americans who would favor
desegregation busing policy (i.e., whether they support busing Black and white school children
together from one school district to another) even after the GSS stopped asking this question.
It suggests that public opinion on racial busing would have remained largely unchanged since
1996.
Moreover, our models can retrodict counterfactual trends, even for questions asked only
once or twice over five decades. For instance, the vegetarian population in the United States has
been growing, but official statistics are scarce, with the GSS data only capturing this information
in 1994 and 1996. Panel A4 illustrates the counterfactual trends of the proportion of Americans
who consistently or frequently abstain from eating meat due to moral or environmental reasons.
Although this opinion has been measured only twice, our model could estimate the trend we
would observe if we had measured this opinion repeatedly over time. Our model predicts that
roughly 25% of Americans always or often would refuse to eat meat for moral or environmental
reasons in 2018, with the proportion steadily increasing since 1996 (Panel A4). In contrast, the
trend estimations from the matrix factorization model do not seem to adequately capture the
societal shifts toward a more progressive society (in Panels B3 and B4). At this point, one
may wonder how we can trust counterfactual trends based on only a few years of observations.
Figure A9 shows that the predictive performance of our models stays the same regardless of
how many years each opinion is used as part of training data during the fine-tuning process.
Individual-level and Opinion-level Heterogeneity of Model Accuracy
Despite the impressive capability of our models in making a fairly accurate prediction of per-
sonal and public opinion, it is important to ensure that individuals and opinions are equally
predictable. Figure 5 shows the performance of our Alpaca-7b based model across various
subgroups, including sex, age, period, race, region, education, income, and political ideology,
across three scenarios. We estimate OLS regression models to assess between-group gaps in
individual-level AUCs with robust standard errors.
First, opinions of individuals with higher socioeconomic status (SES), as measured by their
levels of education and income, are more predictable than those with lower SES. Namely, indi-
viduals with a master’s degree or higher are more predictable than those without a high school
degree, as indicated by a 0.037 higher AUC, whereas those in the top 20% income bracket are
more predictable than those in the bottom 20%, with a 0.023 higher AUC. Second, our model’s
prediction is less accurate for racial minorities than Whites by a 0.015 lower AUC. Third, our
models can predict strong partisans more accurately than Independents and “something else”
groups. Lastly, we observe a higher AUC in more recent periods. This suggests that, despite
our implementation of period embedding, our models still produce less accurate predictions for
past opinions, with an AUC decrease of up to 0.025 for the 1970s.
Figure 6 shows which factors are associated with opinion-level AUCs. Our best models con-
18Figure 5: Coefficient plots from OLS regression models predicting individual-level AUC
across three different types of missing response prediction. A higher AUC value indicates
greater model accuracy for individuals. Here, each dot represents the expected difference of
AUC (i.e., average marginal effects) against the reference group within each subgroup with the
95% confidence intervals. Red bars indicate that the AUC for a particular group is below the
AUC of the reference group, and blue bars indicate that the AUC for a particular group is above
the AUC of the reference group. Here, a filled dot refers to a statistically significant difference,
and an X refers to a statistically insignificant difference based on robust standard errors (p <
0.05).
firm the recency bias using a linear survey period indicator (i.e., higher accuracy in predicting
opinions in more recent surveys). One might question whether our models exhibited a differ-
ent performance in 2021 when the survey response rate was 17%, the lowest in the history of
GSS (the average response rate in the GSS was 72%), and the survey mode was altered due to
the COVID-19 pandemic. However, we find no significant evidence to support this conjecture.
This also reduces our concerns about data leakage. Since all LLMs have been trained with text
data collected until September 2021 or earlier, there is no chance that people’s survey responses
from 2021 have been directly leaked to the LLMs. The even higher accuracy of LLMs in 2021
19suggests that their performance is not driven by data leakage. The larger sample size (i.e., the
number of respondents who answer a specific survey question in the training data) is obviously
associated with the larger AUC, but this sample size effect is negligible in unasked opinion pre-
diction. Opinions with higher response rates show higher AUCs though they are not significant
for unasked opinion prediction. Surveys that use split ballot designs show better performance,
encouraging more active use of split ballot designs in surveys. One might question whether
predicting opinions with multiple response options, given our binarization method, is harder
than those with just two options. However, we find minor effects, only in the case of unasked
opinion prediction models. The proportion of non-responses, such as refusals or inapplicable
responses, is associated with the lower AUCs, highlighting the challenge of predicting opinions
with fewer responses due to systematic non-responses.
Figure 6: Coefficient plots from OLS regression models predicting opinion-level AUC
across three different types of missing response prediction. A higher AUC value indicates
greater model accuracy for opinions each year. Here, each dot represents the average marginal
effects of each variable. To compare the effect size across different continuous covariates, we
standardize them in regression analysis. Red bars indicate that the variable is negatively corre-
lated with opinion-level AUC, and blue bars indicate that the variable is positively correlated
with opinion-level AUC. Here, a filled dot refers to a statistically significant difference, and an
X refers to a statistically insignificant difference based on robust standard errors (p <0.05).
The strongest factor that affects opinion-level AUCs is the extent to which opinions are cor-
related with seven-scale political ideology. It should not be surprising given the strong evidence
of ideological sorting in public opinion (Baldassarri and Gelman, 2008; Boutyline and Vaisey,
202017). For instance, the following opinions are highly correlated with a measure of political
ideology with a correlation coefficient of 0.53: whether women with low incomes or unmarried
women should have the right to undergo an abortion legally. Our models effectively retrodicted
these opinions, achieving impressive AUCs of 0.94 and 0.95, respectively. In contrast, the as-
sociation between individuals’ dissatisfaction with work situations and their political ideology
was extremely weak, with a correlation coefficient of less than 0.01. The model’s accuracy
in retrodicting this belief was also very low, yielding an AUC of 0.57. On the other hand, an
AUC is smaller when the survey response shows a larger variance, indicating that it is harder
to predict a controversial opinion. Additionally, the models can better predict opinions closely
located with other opinions in the embedding space measured by the average cosine similarity
with all other opinions, but its magnitude is less than 0.01. This means that the model is capable
of accurately predicting opinions not only that are semantically similar, as illustrated in Table
A2, but also that deviate from other opinions in the sentence embedding space.
Discussion
Artificial Intelligence (AI) agents have been with us for a long time, taking various forms such
as recommender algorithms that learn and predict individual preferences and, more recently,
conversational AIs that serve diverse needs (e.g., ChatGPT) (Rahwan et al., 2019). With the
recent advancements in LLMs demonstrating remarkable performance in mirroring human be-
haviors and responses, the possibility of replacing human participants with AI agents in social
science research has been suggested (Bail, 2023; Dillion et al., 2023). Some studies in bio-
logical sciences have already employed in-silico AIs to replace costly in-vivo experiments and
advance scientific discovery (Jumper et al., 2021), and recent scholarship has started exploring
this possibility in social sciences as well (Argyle et al., 2023; Chu et al., 2023; Dillion et al.,
2023; Ziems et al., 2023). In fact, survey researchers rank highest among the professions whose
tasks can be significantly impacted by LLMs (Eloundou et al., 2023). Now is the time to ask
this question: How will LLMs influence social surveys?
Our answer is that LLMs are more useful in augmenting, rather than completely replacing,
human responses in surveys. In social sciences, accurately predicting public opinion remains
a challenge because it is a collective representation of diverse personal opinions, and most
individuals do not always hold consistent and coherent beliefs towards various issues (Bal-
dassarri and Goldberg, 2014; Converse, P., 1964; Zaller and Feldman, 1992). While public
opinion is generally stable over time with individuals holding firmly to their beliefs (Kiley
and Vaisey, 2020), some attitudes (e.g., same-sex marriage) undergo dramatic shifts (Baunach,
2012). Against this background, recent studies employing LLMs show limited successes in pre-
dicting public opinion accurately and raise questions of demographic representativeness (San-
turkar et al., 2023). It is presumably because the existing LLMs lack the capability to address
individual heterogeneities and temporal dynamics. With a flexible methodological framework
to tackle these challenges, we show that personalized LLMs can augment survey-based ap-
21plications with human inputs – missing data imputation and retrodiction. At the same time,
personalized LLMs show modest capacity when human-generated responses are not readily
available, such as unasked opinion prediction, challenging the idea of replacing human subjects
with AI agents.
How can social scientists and decision-makers benefit from our AI-augmented survey method-
ology? We demonstrate that fine-tuning LLMs with nationally representative surveys enables
us to effectively predict a wide range of public opinion while reducing the loss in accuracy or
representativeness, tackling the challenges inherent in both digital trace and survey data. The
practical applicability of our model for missing data imputation arises from its consistently high
accuracy, irrespective of the extent of missing data and different missing mechanisms. Figure
A7 demonstrates that more training data always performs better for missing data imputation
and retrodiction. However, in the absolute sense, our missing data imputation models with
10% training data show a higher than 0.8 AUC. To elaborate on this finding, we examine how
many questions are needed to develop models with reasonably high AUCs. Figure A10 shows
that even only asking 10 questions achieved a good performance (AUC = 0.77), and the per-
formance gain peaked at asking 100 questions (AUC = 0.83). This capability can be useful
when survey designs are expected to be impacted by significant attrition in the current era of
declining response rates (Sengupta et al., 2023). This approach can also be advantageous in
designing opinion-tracking polls to maximize the number of questions posed to a given number
of respondents. For instance, rather than asking the same ten questions to a thousand partici-
pants, pollsters can disseminate twenty questions among the same thousand participants, each
answering ten questions, and employ the model to infer individual responses to the remaining
ten unasked questions. On the other hand, given our model’s remarkable ability to mimic hu-
man responses, even including biases, researchers can use it to refine their survey questions by
systematically examining characteristics of questions that cannot be accurately predicted (e.g.,
poor question wording).
Our model with high accuracy in the retrodiction tasks can help us identify a turning point
by looking into past trends – when they started to shift12. Some may question whether our mod-
els can predict unmeasured opinion trends, even during periods of exogenous shocks such as
COVID-19. As demonstrated in Figure 6, there was no significant variation in the performance
of our model when predicting unmeasured opinions in 2021 compared to other periods. We
speculate that this high level of accuracy arises because the vanilla LLMs, such as Alpaca-7b,
were pre-trained on rich digital traces during the COVID-19 period, and the period embedding
captures temporal dynamics precisely. Our model’s ability to capture public opinion that has
changed dramatically is supported by its accurate prediction of the sudden change in attitudes
supporting same-sex marriage and opposition to eating meat. Furthermore, our models can aid
survey designers in formulating survey questions. For example, they could leverage the model’s
12Alternatively, given the nature of embedding models that group similar questions and periods together, it is
likely that a single opinion or a single period would rarely deviate from other opinions and periods. By scrutinizing
what transpires with other opinions or periods similarly positioned within the embedding spaces, future studies may
gain insights into why we observe such deviations and understand the nature of exogenous shocks.
22future predictions to prioritize questions that are anticipated to uncover unexpected trends.
Finally, despite its modest performance in unasked opinion prediction without any human
response to entirely new questions compared to other tasks, our best model using Alpaca-7b
still shows an AUC of 0.729 in predicting personal opinions. This performance is comparable
to or higher than the performance of recent LLMs without fine-tuning in zero-shot prediction
tasks in social science, such as sentiment analysis and ideology detection (Ziems et al., 2023).
We may benefit from this capability to inspire scientific discovery, such as assisting in the selec-
tion of relevant survey questions and discovering meaningful hypotheses that involve currently
unobserved data (Holm, 2019). For example, researchers may pre-register their analysis plans
by generating advanced predictions, or they may even conduct a preliminary analysis of sim-
ulated data, based on which they can propose new research hypotheses and confirm them by
conducting an actual survey with human participants. Moreover, the model can help pinpoint
which demographic groups or individuals might benefit from oversampling to ensure improved
survey quality and representation, given the relatively small accuracy gaps in unasked opinion
prediction across different demographic groups shown in Figure 5. Nevertheless, at the current
level of LLMs’ performance, their predictions should not be employed directly for high-stake
decisions that pose tangible risks to individuals. The ethical implications and potential for un-
intended consequences are too significant to ignore.
What can we learn from these results with regard to the nature of personal and public opin-
ion? The remarkable performance of our predictive models suggests that the notion of personal
opinion may not be as personal as it seems. The predictability of personal opinions highlights
the inherently social nature of human beings, suggesting that our opinions are embedded in
the social contexts that we belong to. This may not be surprising given that LLMs, trained on
vast amounts of human-generated text, encode a wide spectrum of human attitudes, as humans
utilize technological devices to voice their opinions into the socio-technical reservoir, which
in turn, through algorithmic confounding, constrains and shapes what they perceive, observe,
and generate (Latour, 2007). Some people might be surprised by the LLMs’ ability to extract
relevant information from this extensive record of human history. Simultaneously, others might
not be surprised since it may reaffirm the old idea that most citizens’ question-and-answer pro-
cess in a survey merely involves recalling a blend of partially consistent ideas, including an
over-representation of ideas made salient by question prompts, much like the retrieval process
of LLMs, which are then used to respond to the question (Zaller and Feldman, 1992).
The higher predictability of individuals with higher SES and stronger partisanship is largely
aligned with a theory of political belief systems; namely, they tend to hold coherent belief
systems in which opinions are highly correlated to each other (Converse, P., 1964; Zaller and
Feldman, 1992). The lower predictability of racial minorities should remind us of the recent
finding that the meaning of the terms “liberal” and “conservative” is unfamiliar to many black
Americans (Jefferson, 2020). However, these patterns may also indicate potential biases in
LLMs pre-trained based on a large text corpus (e.g., CommonCrawl) that arise from digital
divides by SES (DiMaggio et al., 2004; Nadeem et al., 2020). To ascertain the origins of ac-
curacy gaps across varying demographic groups, we compare the regression outcomes between
23LLMs and matrix factorization models. Given that matrix factorization models do not use any
textual information, biases from matrix factorization models may indicate the extent of biases
attributable to group differences in belief systems, as opposed to a biased pretraining corpus.
Figure A11 displays similar patterns between the two models, suggesting that these gaps are
likely a result of different levels of belief organization across different demographic groups.
Our study brings several ethical concerns into focus concerning the use of LLMs to predict
personal and public opinion. A major concern lies in the realm of privacy and surveillance. Our
models have demonstrated the ability to accurately estimate personal opinions that respondents
might not be willing to share or may have chosen not to answer. The implications of this could
be far-reaching, especially when such tools could be misused by organizations, for example,
to screen job applicants based on information that individuals have not explicitly agreed to
disclose. This risk escalates as the models grow more precise in predicting opinions and as
more people trust and use these models. Therefore, it is crucial to engage in discussions about
how we can maintain respondent privacy and data protection while concurrently enhancing the
accountability and responsibility of generative AIs (Shevlane et al., 2023).
Parallel to concerns of privacy and surveillance, ethical considerations regarding individ-
ual autonomy and demographic representation are of equal importance. Despite its accuracy,
predicting a person’s opinion without their consent can be seen as a potential infringement on
their autonomy. This is especially apparent when an answer is presumed on their behalf when
they either refuse to respond or express uncertainty. This concern widens when viewed from a
societal perspective, particularly in contexts of democracy since autonomous opinion formation
is a fundamental component of democratic processes (Burstein, 2003; Shapiro, 2011). Sur-
veys are traditionally seen by participants as platforms to voice their opinions and influence
democratic outcomes (Igo, 2008). The potential shift from surveys filled out by individuals to
those generated by AI could significantly disrupt the formation of democratic consensus. Our
model’s lower accuracy for individuals with low socioeconomic status, racial minorities, and
non-partisan affiliations can exacerbate the demographic representation issue. For instance, if
decision-makers use these kinds of models to guide policy implementation, the less predictable
voices of minority groups could be marginalized. This could further undermine the already
fragile trust in surveys.
There are some limitations to the proposed AI-augmented survey approach worth noting.
First, our current models dichotomize response options into positive and negative ones for the
sake of an intuitive understanding of opinions. However, researchers often use a five-point
scale ranging from “strongly disagree” to “strongly agree” to gauge the extent of agreement
or disagreement. Future studies might improve this issue by incorporating multi-class clas-
sification layers or decoders, enabling them to predict opinions with more than two response
options. Second, our internal cross-validation procedure yields high levels of accuracy and
strong correlations between observed and predicted responses, which enhance our confidence
in retrodicting counterfactual trends. However, some may cast doubt on the external validity
of our model’s ability to retrodict. To address this potential limitation, future research could
employ cross-survey validation. This would involve examining whether the models, fine-tuned
24via the General Social Survey, can accurately predict missing opinions in other nationally rep-
resentative surveys (e.g., the American National Election Survey) and vice versa.
Third, while our method is designed to be model-agnostic, indicating its compatibility with
various large language models (LLMs) for interpreting survey questions, the effectiveness of
fine-tuning strategies remains uncertain when applying non-English LLMs or extending our
approach to different national contexts, such as in the case of World Value Surveys. Recently,
Atari et al. (2023) show that vanilla LLMs’ answers to questions in the World Value Surveys
closely align with responses typically observed in Western, Educated, Industrialized, Rich, and
Democratic (WEIRD) countries with the highest correlations with the United States, and the
efforts to align LLMs with human values could unintentionally increase the WEIRD bias (Ryan
et al., 2024). Future studies should examine whether fine-tuning LLMs with cross-cultural
surveys, such as the World Value Survey, could address this bias, and whether LLMs could be
applied for cross-cultural opinion prediction. Fourth, a more thorough set of benchmark tests is
needed to probe the potential of this methodology when applied to local and online surveys that
lack national representativeness. For example, researchers can ask the same set of questions
utilized by the GSS concurrently in their own survey and use the fine-tuned model based on the
GSS data to predict responses to other questions that were not asked. At present, it is uncertain
how useful our approach can be for predicting opinions in non-representative surveys conducted
only once or twice with a small number of respondents.
Finally, most LLMs are pre-trained on recent text data, which may lead to contextual dis-
crepancies, considering that the meaning of words can shift across different historical contexts
(Gonz ´alez-Gallardo et al., 2023; Kozlowski et al., 2019; Longpre et al., 2023). Take the word
“artificial” as an example; it was previously used to denote something skillfully designed, akin
to “artistic.” Now, it is primarily used to indicate something that is not natural such as artificial
flavors and artificial intelligence. Although we have tried to address this issue by integrating
interactions between period and sentence embeddings in our models, we find that predictability
in the 1970s is still lower than in the 2010s13.
Against this background, we anticipate that these limitations may soon be addressed as the
scale of LLMs expands and more scholarly focus is directed toward enhancing the integration
of LLMs with social surveys for opinion prediction. We believe that our research marks a
foundational step and has shown promising potential for the future of social science research
using the LLMs. With the rapid advancement in LLM-related applications, more and more
survey researchers may consider using the AI-augmented survey approach or similar kinds. We
make our code and data available at [suppressed for peer review] to facilitate the replication and
extension of our novel approach (also see the model training and evaluation codes in Appendix
F).
13Alternatively, the model’s stronger performance in recent periods could reflect rising correlations between par-
tisanship and other issues (Baldassarri and Gelman, 2008; Park, 2018) and the collapse of cross-cutting alignments
across different issues (DellaPosta, 2020).
25References
Abid, Abubakar, Maheen Farooqi, and James Zou. 2021. “Persistent Anti-Muslim Bias in Large
Language Models.” In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and
Society , pp. 298–306. Association for Computing Machinery.
Aher, Gati, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. “Using Large Language Mod-
els to Simulate Multiple Humans and Replicate Human Subject Studies. arXiv:2208.10264
[cs.CL].”
Aiyappa, Rachith, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. 2023. “Can We Trust the
Evaluation on ChatGPT? arXiv:2303.12767 [cs.CL].”
Ansolabehere, Stephen, Jonathan Rodden, and James M. Snyder. 2008. “The Strength of Issues:
Using Multiple Measures to Gauge Preference Stability, Ideological Constraint, and Issue
V oting.” The American Political Science Review 102:215–232.
Argyle, Lisa P., Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and
David Wingate. 2023. “Out of One, Many: Using Language Models to Simulate Human
Samples.” Political Analysis https://doi.org/10.1017/pan.2023.2.
Atari, Mohammad, Mona J. Xue, Peter S. Park, Dami ´an Blasi, and Joseph Henrich. 2023.
“Which Humans?”
Bail, Christopher A. 2023. “Can Generative AI Improve Social Science? SocArXiv.”
https://doi.org/10.31235/osf.io/rwtzs.
Baldassarri, Delia and Andrew Gelman. 2008. “Partisans without Constraint: Political Polariza-
tion and Trends in American Public Opinion.” American Journal of Sociology 114:408–446.
Baldassarri, Delia and Amir Goldberg. 2014. “Neither Ideologues nor Agnostics: Alternative
V oters’ Belief System in an Age of Partisan Politics.” American Journal of Sociology 120:45–
95.
Baldassarri, Delia and Barum Park. 2020. “Was There a Culture War? Partisan Polarization and
Secular Trends in US Public Opinion.” The Journal of Politics 82:809–827.
Baunach, Dawn Michelle. 2012. “Changing Same-Sex Marriage Attitudes in America from
1988 Through 2010.” Public Opinion Quarterly 76:364–378.
Beauchamp, Nicholas. 2017. “Predicting and Interpolating State-Level Polls Using Twitter
Textual Data.” American Journal of Political Science 61:490–503.
Behr, Roy L. and Shanto Iyengar. 1985. “Television News, Real-World Cues, and Changes in
the Public Agenda.” Public Opinion Quarterly 49:38.
26Berinsky, Adam J. 2017. “Measuring Public Opinion with Surveys.” Annual Review of Political
Science 20:309–329.
Blumenstock, Joshua, Gabriel Cadamuro, and Robert On. 2015. “Predicting Poverty and Wealth
from Mobile Phone Metadata.” Science 350:1073–1076.
Boutyline, Andrei and Laura K Soter. 2021. “Cultural schemas: What they are, how to find
them, and what to do once you’ve caught one.” American Sociological Review 86:728–758.
Boutyline, Andrei and Stephen Vaisey. 2017. “Belief Network Analysis: A Relational Approach
to Understanding the Structure of Attitudes.” American journal of sociology 122:1371–1447.
Brand, James, Ayelet Israeli, and Donald Ngwe. 2023. “Using gpt for market research.” Avail-
able at SSRN 4395751 .
Brayne, Sarah. 2020. Predict and Surveil: Data, Discretion, and the Future of Policing . Oxford
University Press.
Brooks, Clem and Jeff Manza. 2006. “Social Policy Responsiveness in Developed Democra-
cies.” American Sociological Review 71:474–494.
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda Askell. 2020. “Lan-
guage Models Are Few-Shot Learners.” Advances in neural information processing systems
33:1877–1901.
Burstein, Paul. 2003. “The Impact of Public Opinion on Public Policy: A Review and an
Agenda.” Political Research Quarterly 56:29–40.
Cesare, Nina, Hedwig Lee, Tyler McCormick, Emma Spiro, and Emilio Zagheni. 2018.
“Promises and Pitfalls of Using Digital Traces for Demographic Research.” Demography
55:1979–1999.
Chu, Eric, Jacob Andreas, Stephen Ansolabehere, and Deb Roy. 2023. “Language Models
Trained on Media Diets Can Predict Public Opinion. arXiv:2303.16779 [cs.CL].”
Converse, P. 1964. “The Nature of Belief Systems in Mass Publics.” In Ideology and Discontent ,
edited by Apter, D. E., pp. 206–261. The Free Press.
Couper, Mick P. 2017. “New Developments in Survey Data Collection.” Annual Review of
Sociology 43:121–145.
Davern, Michael, Rene Bautista, Jeremy Freese, Stephen L. Morgan, and Tom W. Smith.
2021. “General Social Surveys, 1972-2021 Cross-section [machine-readable data file, 68,846
cases]. Principal Investigator, Michael Davern; Co-Principal Investigators, Rene Bautista,
27Jeremy Freese, Stephen L. Morgan, and Tom W. Smith; Sponsored by National Science
Foundation. – NORC ed. – Chicago: NORC, 2021: NORC at the University of Chicago
[producer and distributor]. Data accessed from the GSS Data Explorer website at gssdataex-
plorer.norc.org.”
DellaPosta, Daniel. 2020. “Pluralistic Collapse: The “Oil Spill” Model of Mass Opinion Polar-
ization.” American Sociological Review 85:507–536. Publisher: SAGE Publications Inc.
DellaPosta, Daniel, Yongren Shi, and Michael Macy. 2015. “Why Do Liberals Drink Lattes?”
American Journal of Sociology 120:1473–1511.
Dillion, Danica, Niket Tandon, Yuling Gu, and Kurt Gray. 2023. “Can AI Lan-
guage Models Replace Human Participants?” Trends in Cognitive Sciences
https://doi.org/10.1016/j.tics.2023.04.008.
DiMaggio, Paul, Eszter Hargittai, Coral Celeste, and Steven Shafer. 2004. “Digital Inequality:
From Unequal Access to Differentiated Use.” In Social Inequality , edited by Kathryn M.
Neckerman, pp. 355–400. Russell Sage Foundation.
DiMaggio, Paul, Ramina Sotoudeh, Amir Goldberg, and Hana Shepherd. 2018. “Culture out of
attitudes: Relationality, population heterogeneity and attitudes toward science and religion in
the US.” Poetics 68:31–51.
Dominguez-Olmedo, Ricardo, Moritz Hardt, and Celestine Mendler-D ¨unner. 2023. “Question-
ing the survey responses of large language models.” arXiv preprint arXiv:2306.07951 .
Downs, Anthony. 1972. “Up and down with Ecology: The Issue-Attention Cycle.” The public
28:38–50.
Eloundou, Tyna, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. “GPTs Are
GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.
arXiv:2303.10130 [econ.GN].”
Ferraro, Kenneth F. and Melissa M. Farmer. 1999. “Utility of Health Data from Social Surveys:
Is There a Gold Standard for Measuring Morbidity?” American Sociological Review 64:303–
315.
Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Vir-
ginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard
Schafer, Peggy Valcke, and Effy Vayena. 2018. “AI4People—An Ethical Framework for
a Good AI Society: Opportunities, Risks, Principles, and Recommendations.” Minds and
Machines 28:689–707.
Friedkin, Noah E and Eugene C Johnsen. 2011. Social influence network theory: A sociological
examination of small group dynamics , volume 33. Cambridge University Press.
28Garip, Filiz. 2020. “What failure to predict life outcomes can teach us.” Proceedings of
the National Academy of Sciences 117:8234–8235. Publisher: Proceedings of the National
Academy of Sciences.
Goldberg, Amir. 2011. “Mapping Shared Understandings Using Relational Class Analysis:
The Case of the Cultural Omnivore Reexamined.” American Journal of Sociology 116:1397–
1436.
Gonz ´alez-Gallardo, Carlos-Emiliano, Emanuela Boros, Nancy Girdhar, Ahmed Hamdi, Jose G.
Moreno, and Antoine Doucet. 2023. “Yes but.. Can ChatGPT Identify Entities in Historical
Documents? arXiv:2303.17322 [cs.DL].”
Gordon, Mitchell L., Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori
Hashimoto, and Michael S. Bernstein. 2022. “Jury Learning: Integrating Dissenting V oices
into Machine Learning Models.” In Proceedings of the 2022 CHI Conference on Human
Factors in Computing Systems , pp. 1–19. Association for Computing Machinery.
Grimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. Text as Data: A New
Framework for Machine Learning and the Social Sciences . Princeton University Press.
Grossmann, Igor, Matthew Feinberg, Dawn C Parker, Nicholas A Christakis, Philip E Tetlock,
and William A Cunningham. 2023. “AI and the transformation of social science research.”
Science 380:1108–1109.
H¨am¨al¨ainen, Perttu, Mikke Tavast, and Anton Kunnari. 2023. “Evaluating Large Language
Models in Generating Synthetic HCI Research Data: A Case Study.” In Proceedings of the
2023 CHI Conference on Human Factors in Computing Systems , pp. 1–19. Association for
Computing Machinery.
Hastie, Trevor J. 1992. “Generalized Additive Models.” In Statistical Models in S . Routledge.
Hilgartner, Stephen and Charles L Bosk. 1988. “The Rise and Fall of Social Problems: A Public
Arenas Model.” American journal of Sociology 94:53–78.
Holm, Elizabeth A. 2019. “In Defense of the Black Box.” Science 364:26–27.
Honaker, James, Gary King, and Matthew Blackwell. 2011. “Amelia II: A Program for Missing
Data.” Journal of Statistical Software 45:1–47.
Horton, John J. 2023. “Large Language Models as Simulated Economic Agents: What Can We
Learn from Homo Silicus? arXiv:2301.07543 [econ.GN].”
Igo, Sarah E. 2008. The Averaged American: Surveys, Citizens, and the Making of a Mass
Public . Harvard University Press.
29Jansen, Bernard J., Soon-gyo Jung, and Joni Salminen. 2023. “Employing large language mod-
els in survey research.” Natural Language Processing Journal 4:100020.
Jefferson, Hakeem. 2020. “The Curious Case of Black Conservatives: Con-
struct Validity and the 7-Point Liberal-Conservative Scale.” Available at SSRN:
https://ssrn.com/abstract=3602209 or http://dx.doi.org/10.2139/ssrn.3602209.
Jiang, Hang, Doug Beeferman, Brandon Roy, and Deb Roy. 2022. “CommunityLM: Prob-
ing Partisan Worldviews from Language Models.” In Proceedings of the 29th International
Conference on Computational Linguistics , pp. 6818–6826. International Committee on Com-
putational Linguistics.
Joo, Won-Tak and Jason Fletcher. 2020. “Out of Sync, out of Society: Political Beliefs and
Social Networks.” Network Science 8:445–468.
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, Alex
Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie,
Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig
Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pa-
cholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W.
Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. 2021. “Highly Accurate
Protein Structure Prediction with AlphaFold.” Nature 596:583–589.
Jurafsky, Daniel and James Martin. 2023. Speech and Language Processing, 3rd Edition Draft .
Kiley, Kevin and Stephen Vaisey. 2020. “Measuring Stability and Change in Personal Culture
Using Panel Data.” American Sociological Review 85:477–506.
Kirk, Hannah Rose, Bertie Vidgen, Paul R ¨ottger, and Scott A. Hale. 2023. “Personalisation
within Bounds: A Risk Taxonomy and Policy Framework for the Alignment of Large Lan-
guage Models with Personalised Feedback. arXiv:2303.05453 [cs.CL].”
Koren, Yehuda, Robert Bell, and Chris V olinsky. 2009. “Matrix Factorization Techniques for
Recommender Systems.” Computer 42:30–37. Conference Name: Computer.
Kozlowski, Austin C and James P Murphy. 2021. “Issue alignment and partisanship in the
American public: Revisiting the ‘partisans without constraint’thesis.” Social Science Re-
search 94:102498.
Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture:
Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review
84:905–949.
30Lall, Ranjit and Thomas Robinson. 2022. “The MIDAS touch: accurate and scalable missing-
data imputation with deep learning.” Political Analysis 30:179–196.
Latour, Bruno. 2007. Reassembling the Social: An Introduction to Actor-Network-Theory . OUP
Oxford.
Lazarsfeld, Paul F., Bernard R. Berelson, and Hazel Gaudet. 1948. The People’s Choice .
Columbia University Press, 3rd edition.
Lersch, Philipp M. 2023. “Change in Personal Culture over the Life Course.” American Socio-
logical Review 88:220–251.
Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “Roberta: A Robustly Optimized Bert
Pretraining Approach. arXiv:1907.11692 [cs.CL].” .
Longpre, Shayne, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph,
Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. 2023. “A Pretrainer’s Guide
to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity.
arXiv:2305.13169 [cs.CL].” .
Marsden, Peter V ., Tom W. Smith, and Michael Hout. 2020. “Tracking US Social Change Over
a Half-Century: The General Social Survey at Fifty.” Annual Review of Sociology 46:109–
134.
Martin, John Levi. 2010. “Life’s a Beach but You’re an Ant, and Other Unwelcome News for
the Sociology of Culture.” Poetics 38:229–244.
Mei, Qiaozhu, Yutong Xie, Walter Yuan, and Matthew O Jackson. 2024. “A Turing test
of whether AI chatbots are behaviorally similar to humans.” Proceedings of the National
Academy of Sciences 121:e2313925121.
Meta Fundamental AI Research Diplomacy Team (FAIR), Anton Bakhtin, Noam Brown,
Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray,
Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam
Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen
Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and
Markus Zijlstra. 2022. “Human-Level Play in the Game of Diplomacy by Combining Lan-
guage Models with Strategic Reasoning.” Science 378:1067–1074.
Milbauer, Jeremiah, Adarsh Mathew, and James Evans. 2021. “Aligning Multidimensional
Worldviews and Discovering Ideological Differences.” Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing pp. 4832–4845.
31Molina, Mario and Filiz Garip. 2019. “Machine learning for sociology.” Annual Review of
Sociology 45:27–45.
Moore, Frances C., Nick Obradovich, Flavio Lehner, and Patrick Baylis. 2019. “Rapidly De-
clining Remarkability of Temperature Anomalies May Obscure Public Perception of Climate
Change.” Proceedings of the National Academy of Sciences 116:4905–4910.
Nadeem, Moin, Anna Bethke, and Siva Reddy. 2020. “StereoSet: Measuring Stereotypical Bias
in Pretrained Language Models. arXiv:2004.09456 [cs.CL].”
O’Connor, Brendan, Ramnath Balasubramanyan, Bryan Routledge, and Noah Smith. 2010.
“From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series.” In Pro-
ceedings of the Fourth International Conference on Weblogs and Social Media , pp. 122–129.
AAAI Press.
Park, Barum. 2018. “How Are We Apart? Continuity and Change in the Structure of Ideological
Disagreement in the American Public, 1980–2012.” Social Forces 96:1757–1784. 00000.
Park, Chan Young, Julia Mendelsohn, Karthik Radhakrishnan, Kinjal Jain, Tushar Kanakagiri,
David Jurgens, and Yulia Tsvetkov. 2021. “Detecting Community Sensitive Norm Viola-
tions in Online Conversations.” Findings of the Association for Computational Linguistics:
EMNLP 2021 pp. 3386–3397.
Rahwan, Iyad, Manuel Cebrian, Nick Obradovich, Josh Bongard, Jean-Franc ¸ois Bonnefon,
Cynthia Breazeal, Jacob W. Crandall, Nicholas A. Christakis, Iain D. Couzin, Matthew O.
Jackson, Nicholas R. Jennings, Ece Kamar, Isabel M. Kloumann, Hugo Larochelle, David
Lazer, Richard McElreath, Alan Mislove, David C. Parkes, Alex ‘Sandy’ Pentland, Mar-
garet E. Roberts, Azim Shariff, Joshua B. Tenenbaum, and Michael Wellman. 2019. “Ma-
chine Behaviour.” Nature 568:477–486.
Reimers, Nils and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Embeddings Using
Siamese BERT-Networks arXiv:1908.10084 [cs.CL].”
Rubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63:581–592.
Rule, Alix, Jean-Philippe Cointet, and Peter S. Bearman. 2015. “Lexical Shifts, Substantive
Changes, and Continuity in State of the Union Discourse, 1790–2014.” Proceedings of the
National Academy of Sciences 112:10837–10844.
Ryan, Michael J, William Held, and Diyi Yang. 2024. “Unintended Impacts of LLM Alignment
on Global Representation.” arXiv preprint arXiv:2402.15018 .
Salganik, Matthew J., Ian Lundberg, Alexander T. Kindel, Caitlin E. Ahearn, Khaled Al-
Ghoneim, Abdullah Almaatouq, Drew M. Altschul, Jennie E. Brand, Nicole Bohme
32Carnegie, Ryan James Compton, Debanjan Datta, Thomas Davidson, Anna Filippova, Con-
nor Gilroy, Brian J. Goode, Eaman Jahani, Ridhi Kashyap, Antje Kirchner, Stephen McKay,
Allison C. Morgan, Alex Pentland, Kivan Polimis, Louis Raes, Daniel E. Rigobon, Claudia V .
Roberts, Diana M. Stanescu, Yoshihiko Suhara, Adaner Usmani, Erik H. Wang, Muna Adem,
Abdulla Alhajri, Bedoor AlShebli, Redwane Amin, Ryan B. Amos, Lisa P. Argyle, Livia
Baer-Bositis, Moritz B ¨uchi, Bo-Ryehn Chung, William Eggert, Gregory Faletto, Zhilin Fan,
Jeremy Freese, Tejomay Gadgil, Josh Gagn ´e, Yue Gao, Andrew Halpern-Manners, Sonia P.
Hashim, Sonia Hausen, Guanhua He, Kimberly Higuera, Bernie Hogan, Ilana M. Horwitz,
Lisa M. Hummel, Naman Jain, Kun Jin, David Jurgens, Patrick Kaminski, Areg Karapetyan,
E. H. Kim, Ben Leizman, Naijia Liu, Malte M ¨oser, Andrew E. Mack, Mayank Mahajan,
Noah Mandell, Helge Marahrens, Diana Mercado-Garcia, Viola Mocz, Katariina Mueller-
Gastell, Ahmed Musse, Qiankun Niu, William Nowak, Hamidreza Omidvar, Andrew Or,
Karen Ouyang, Katy M. Pinto, Ethan Porter, Kristin E. Porter, Crystal Qian, Tamkinat
Rauf, Anahit Sargsyan, Thomas Schaffner, Landon Schnabel, Bryan Schonfeld, Ben Sender,
Jonathan D. Tang, Emma Tsurkov, Austin van Loon, Onur Varol, Xiafei Wang, Zhi Wang,
Julia Wang, Flora Wang, Samantha Weissman, Kirstie Whitaker, Maria K. Wolters, Wei Lee
Woon, James Wu, Catherine Wu, Kengran Yang, Jingwen Yin, Bingyu Zhao, Chenyun Zhu,
Jeanne Brooks-Gunn, Barbara E. Engelhardt, Moritz Hardt, Dean Knox, Karen Levy, Arvind
Narayanan, Brandon M. Stewart, Duncan J. Watts, and Sara McLanahan. 2020. “Measuring
the Predictability of Life Outcomes with a Scientific Mass Collaboration.” Proceedings of
the National Academy of Sciences 117:8398–8403.
Santurkar, Shibani, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori
Hashimoto. 2023. “Whose Opinions Do Language Models Reflect? arXiv:2303.17548
[cs.CL].”
Schramowski, Patrick, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, and Kristian
Kersting. 2022. “Large Pre-Trained Language Models Contain Human-like Biases of What
Is Right and Wrong to Do.” Nature Machine Intelligence 4:258–268.
Sengupta, Nandana, Madeleine Udell, Nathan Srebro, and James Evans. 2023. “Sparse Data
Reconstruction, Missing Value and Multiple Imputation through Matrix Factorization.” So-
ciological Methodology 53:72–114.
Shapiro, Robert Y . 2011. “Public Opinion and American Democracy.” Public Opinion Quar-
terly 75:982–1017.
Shevlane, Toby, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Le-
ung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya
Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark,
Yoshua Bengio, Paul Christiano, and Allan Dafoe. 2023. “Model evaluation for extreme risks
arXiv:2305.15324 [cs.AI].”
33Simmons, Gabriel and Vladislav Savinov. 2024. “Assessing Generalization for Subpopulation
Representative Modeling via In-Context Learning.” arXiv preprint arXiv:2402.07368 .
Stephens-Davidowitz, Seth. 2017. Everybody Lies: Big Data, New Data, and What the Internet
Can Tell Us About Who We Really Are . Dey Street Books.
Taori, Rohan, Gulrajani, Ishaan, Zhang, Tianyi, Dubois, Yann, Li, Xuechen, Guestrin, Carlos,
Liang, Percy, and Hashimoto, Tatsunori B. 2023. “Alpaca: A Strong, Replicable Instruction-
Following Model.” https://github.com/tatsu-lab/stanford alpaca.
T¨ornberg, Petter, Diliara Valeeva, Justus Uitermark, and Christopher Bail. 2023. “Simulating
social media using large language models to evaluate alternative news feed algorithms.” arXiv
preprint arXiv:2310.05984 .
van Buuren, Stef and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by
Chained Equations in R.” Journal of Statistical Software 45:1–67.
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need arXiv:1706.03762
[cs.CL].”
Wang, Ben and Komatsuzaki, Aran. 2021. “GPT-J-6B: A 6 Billion Parameter Autoregressive
Language Model.” https://github.com/kingoflolz/mesh-transformer-jax/.
Wang, Ruoxi, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi.
2021. “DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale
Learning to Rank Systems.” In Proceedings of the Web Conference 2021 , pp. 1785–1797.
Association for Computing Machinery.
Wang, Zekun Moore, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan
Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. 2023. “Rolellm: Bench-
marking, eliciting, and enhancing role-playing abilities of large language models.” arXiv
preprint arXiv:2310.00746 .
Yan, Ting. 2021. “Consequences of Asking Sensitive Questions in Surveys.” Annual Review of
Statistics and Its Application 8:109–127.
Zaller, John and Stanley Feldman. 1992. “A Simple Theory of the Survey Response: Answering
Questions versus Revealing Preferences.” American Journal of Political Science 36:579–616.
Ziems, Caleb, William Held, Omar Shaikh, Zhehao Zhang, Diyi Yang, and Jiaao Chen. 2023.
“Can Large Language Models Transform Computational Social Science? arXiv:2305.03514
[cs.CL].”
34Appendix for AI-Augmented Surveys: Leveraging Large Lan-
guage Models and Surveys for Opinion Prediction
Appendix A. Introduction to model architecture
Here, we introduce our model architecture designed to address major challenges arising from
data sparsity issues when predicting personal and public opinion in more detail. First, traditional
missing imputation techniques, including the advanced machine-learning models such as matrix
factorization, fall short in predicting responses for new questions beyond the training data,
as they are unable to extrapolate the meaning of survey questions (e.g., Panel C in Figure 1)
(Sengupta et al., 2023). Our proposed model architecture addresses this challenge by encoding
survey questions into sentence embeddings using pre-trained large language models. A large
language model (LLM) refers to a type of neural network that possesses an extensive number
of parameters, typically in the billions or more, trained on vast amounts of unlabeled text data
using self-supervised learning (Vaswani et al., 2017). These pre-trained models encode the
meaning of sentences into neural embeddings, which can be utilized for generating subsequent
tokens or identifying subsequent sentences (e.g., responses to questions).
Specifically, to encode survey questions to neural embeddings, we pass survey questions into
the LLMs and extract the last token’s embedding for decoder-only models (Alpaca and GPT-J)
and a special token embedding (or pooler output) for encoder-only models (RoBERTa-large).
We employ the following prompt, based on Alpaca's instruction data template (Taori, Rohan
et al., 2023), to extract semantic embeddings from pre-trained LLMs: “Below is an instruction
that describes a task. Write a response that appropriately completes the request. ### Instruction:
[SURVEY QUESTION] ### Response: ” To ensure that the semantic embedding accurately
captures the meaning of survey questions, we have the model generate text-based responses
using this embedding and assess whether the responses align with what humans might say. In
most cases, the models generated human-like answers, indicating that the embedding contains
rich semantic information about the survey question (see Table A1). LLMs have different di-
mensions of embeddings. For instance, Alpaca-7b has 4096 dimensions, while RoBERTa-large
has 1024 dimensions. We use a single feed-forward layer to convert the original embedding
to an identical N-dimensional trainable semantic embedding, regardless of the model's origi-
nal dimensionality (See Figure A3). During the fine-tuning process, semantic embeddings are
fine-tuned based on the response patterns in the survey. Specifically, sentence embeddings are
contextualized such that two questions with similar response patterns are more closely mapped
within the embedding space. This enables the fine-tuned models to learn the common socio-
cultural meaning of survey questions and predict appropriate responses. As a result, the fine-
tuned models learn to interpret the meaning of new questions, such as “Do you agree with a law
requiring police permission to purchase a gun?” and locate them in the semantic embeddings
based on the expected patterns in survey responses in the way the predictability of responses
is maximized. Table A2 presents survey questions that are most similar to four questions used
in Figure 4 in the embedding space, which allows us to see how the model captures the simi-
35larity in meanings of questions as well as their response patterns in surveys. We refer to this
embedding vector, s∈Rnas the “semantic embedding” of survey questions.
Second, recent approaches like in-context tuning utilize various prompts to adapt LLMs to
represent specific group opinions (Brown et al., 2020), but they fail to account for unique in-
dividual perspectives because these methods can only predict the commonly held opinions of
a “typical” group (Argyle et al., 2023; Santurkar et al., 2023). To address these challenges,
we add an additional layer of neural embedding in our model architecture – individual belief
embedding. Traditional machine learning models have primarily focused on producing a sin-
gle output for a given input. For instance, when given an input sentence like “the weather is
so good,” models for sentiment analysis are designed to generate a single output, such as “it
conveys a positive sentiment.” However, this method may not generate reliable responses for
controversial issues. For example, when asked about their stance on gun control in the United
States, people’s responses will likely be influenced by their pre-existing beliefs. Recent studies
have developed a methodological framework that accounts for individual heterogeneity, recog-
nizing that people possess diverse opinions (Gordon et al., 2022; Park et al., 2021). Building
on the insights from these models that assign labels on an individual basis, instead of applying
a uniform label to everyone, we assume that each individual’s beliefs are represented as an N-
dimensional embedding vector. We refer to this vector, b∈Rn, as the “belief embedding” of
individuals.
Third, existing approaches overlook temporal heterogeneities in the interpretation of survey
questions and individual belief systems. To address this, we incorporate period embedding as
the third layer of neural embeddings. This accounts for the impact of temporal factors on sur-
vey question responses, such as the gradual shift towards more progressive beliefs over time
(Baldassarri and Park, 2020) and the effects of specific events during a particular period, like
macroeconomic changes, presidential elections, and the COVID-19 pandemic. While some ar-
gue that people’s beliefs are generally stable, attributing fluctuations in opinions to short-term
changes or measurement errors (Ansolabehere et al., 2008; Kiley and Vaisey, 2020), others con-
tend that social transformations and life events can indeed lead to lasting shifts in perspectives
(Lersch, 2023). We represent the historical features of survey periods as an N-dimensional em-
bedding vector, which we refer to as the “period embedding” of surveys, denoted by p∈Rn.
Finally, we incorporate the higher-order interactions between the three embeddings related
to survey question meanings, individual beliefs, and historical contexts by employing a new
deep learning architecture called the “Deep Cross-Network (DCN)” (Gordon et al., 2022; Wang
et al., 2021)14. The DCN comprises multiple cross layers designed to capture feature inter-
actions and feed-forward dense layers, or classifier head, that are responsible for generating
14It is noteworthy that DCN was originally developed in the field of recommendation algorithms (Wang et al.,
2021). The goal of the recommendation algorithm is to predict whether different individuals will prefer particular
objects (e.g., movies, books, or news articles). Similarly, our model predicts whether different individuals will
prefer a particular opinion. Furthermore, our goal of predicting missing elements in a sparse matrix is similar to
the task of recommendation algorithms, which also aim to predict missing elements in sparse matrices.
36the final prediction15. By employing a sigmoid function, the final prediction is generated as a
predicted probability, which falls within the range of 0 to 1. This value represents the likeli-
hood that an individual will indicate a positive response (e.g., agree, yes) to a particular survey
question.
In the following example, we demonstrate how the DCN operates. Let’s assume that s1
captures whether a question is about vaccine hesitancy, b1captures whether an individual holds
conservative ideology, and p1captures the extent to which the COVID-19 pandemic is ongo-
ing16. If an individual’s response to a vaccine mandate is influenced by their conservative ideol-
ogy and the ongoing COVID-19 pandemic, we will need to consider the interaction s1×b1×p1
to predict the individual’s response. To capture these interactions, we define cross layers in the
DCN as follows. We begin by concatenating the semantic embedding of the survey question
(s), the individual’s belief embedding ( b), and the temporal embedding ( p) into a single vector,
denoted as x0. The (l+1)th cross layer can then be defined as follows:
xl+1=x0⊙(Wlxl+bl) +xl
where xl∈R3nandxl+1∈R3nare the input and output of the cross layers, respectively.
Wl∈R3n×3nandbl∈R3nare the learned weights and biases of the cross-layer. Each element
inWlcaptures the relative contribution of the interaction between features in the prediction. In
the DCN with kcross layers, the model includes all feature interactions up to a maximum
polynomial order of k + 1 .
By concatenating multiple cross layers, it is possible to consider more complex interactions.
For instance, the relationship between attitudes toward vaccine mandate and being a liberal
might depend on another latent dimension, such as one’s attitudes toward scientific knowledge.
Complex feature crossings like s1×b1×b2×p1can be captured by employing multiple cross
layers. By considering complex interactions between the meaning of survey questions, indi-
vidual beliefs, and survey periods, we can avoid the assumption that the meaning of opinions
is identical for everyone over time. This assumption has been challenged by previous research
on the heterogeneous perception of cultural meanings across different socio-demographic back-
grounds (Baldassarri and Goldberg, 2014; Goldberg, 2011) and cognitive biases (Martin, 2010).
Figure 2 presents our end-to-end model architecture. Figure A3 provides additional in-
formation regarding the input and output dimensions of each layer. We first encode survey
questions into a semantic embedding using Alpaca-7b, GPT-J-6b, or RoBERTa-large, while
individual ID and survey year are respectively encoded into individual belief and period em-
beddings. These embeddings are then concatenated and used as inputs to the DCN, which
15Although stacking feed-forward dense layers could also account for the interactions between features implic-
itly, cross layers account for them more explicitly, and we can interpret the important feature interactions predictive
of outcomes by analyzing the parameters W.
16Since embeddings, or latent factors, are estimated by the model itself, rather than by researcher, it is difficult
to interpret the meanings of a particular embedding dimension as described here. However, previous research has
shown that embeddings capture the important dimensions used by humans to classify concepts, such as what is
moral or immoral (Schramowski et al., 2022).
37then captures the higher-order interactions between them and generates the predictions. During
the fine-tuning process, all embeddings and the DCN are jointly trained to generate predicted
probabilities.
Appendix B. Model training
To implement our model architecture, we utilize Huggingface’s API for incorporating LLMs
(Alpaca-7b, GPT-J-6b, RoBERTa-large) and TensorFlow Recommenders (TFRS) for deploying
the DCN. During the fine-tuning process with the GSS data, all model components are jointly
trained with the DCN. We freeze the pre-trained parameters of LLMs, except for an adaptable,
feed-forward layer to convert the original contextual embedding to an N-dimensional trainable
semantic embedding, to avoid the known overfitting issues when jointly trained with other em-
beddings (Gordon et al., 2022), such as individual belief and period embeddings, which helps
in keeping computational costs at a manageable level. From our experiments with RoBERTa-
large, we learn that models without freezing the parameters did not meaningfully improve the
performance. We fine-tune our models using the Adam optimizer with a learning rate of 2e-
5, and a binary cross-entropy is used as the loss function. We use a batch size of 128, and
we limit the maximum sequence length to 150 tokens for the RoBERTa-large model. For the
DCN-specific architecture, we experiment with various hyperparameters and choose a fixed
embedding dimension of 50, three cross-layers of size 150, three feed-forward dense layers of
size 150, and a final output layer of size 1 based on the previous literature and a grid search
(Gordon et al., 2022). The number of training epochs is determined based on the performance
in validation data.
Appendix C. Model evaluation
Model evaluation: Personal opinion prediction
With the fine-tuned model, we can predict the probability that a respondent will positively
respond to a specific survey question in a given year, which ranges from 0 (= negative response)
to 1 (= positive response). We evaluate the model’s performance in “missing data imputation”
(Figure 1, Panel A) through the following steps. We assess the model’s capability to predict
missing opinions using 10-fold cross-validation. In practice, we randomly eliminate 10% of the
responses from the training dataset and employ the model to predict these omitted responses
(see also Figure A5, Panel A). For the response-level missing opinion (Panel A, Figure 1), we
randomly distribute the combinations of survey years, survey questions, and individual IDs into
ten groups. During the N-th iteration of cross-validation, the N-th group is excluded from the
training data. The model is then trained on the remaining data for each round, and the excluded
responses are predicted. By repeating this process ten times, we can infer the model’s predictive
performance for all responses.
38To assess the model’s ability in “retrodiction”, that is, predicting year-level missing opin-
ions (Figure 1, Panel B), we employ a modified 10-fold cross-validation technique. In this case,
we evaluate the model’s predictive accuracy when the entire set of responses to 10% of ques-
tions is omitted in a particular year. This scenario differs from the first one, where questions
are “asked” but not answered by participants. By contrast, here we evaluate situations where
questions are “not even asked” during a specific period. To accomplish this, we randomly re-
move around 10% of survey questions in a survey year from the training dataset and train the
model to predict the removed responses (see Figure A5, Panel B). Specifically, we randomly
assign pairs of survey years and questions into ten groups, which are withheld in each round
of cross-validation. During each round, the model predicts the entire set of responses to these
survey questions in the excluded survey years. We repeat this process ten times.
To assess the model’s ability for “unasked opinion prediction,” which refers to the predic-
tion of entirely missing opinions (Figure 1, Panel C), we adopt a distinct 10-fold cross-validation
method. In this scenario, we gauge the model’s predictive accuracy when 10% of existing sur-
vey questions have never been asked by survey designers during the entire survey period. To
achieve this, we randomly eliminate around 10% of survey questions entirely from the training
data and use the model to predict them (see Figure A5, Panel C). Specifically, we randomly al-
locate survey questions into ten groups, which are withheld during each cross-validation round.
During each round, the model predicts the entire set of responses to these survey questions.
By repeating this process ten times, we can infer the model’s predictive performance when the
entire response is missing.
Once we obtain predicted probabilities for each of the three tasks, we assess the Area Under
Curve (AUC) to evaluate the extent to which our models can separate positive responses from
negative responses in their prediction. The AUC measures the probability of the model to
rank a randomly selected positive response over a randomly selected negative response, which
ranges from 0.5 (= random prediction) and 1 (= perfect prediction). We also assess two other
metrics – accuracy and F-1 score – and find similar results, but we do not present them in the
main manuscript because they require an arbitrary threshold for binarizing continuous predicted
probabilities, which could be particularly problematic when the distribution of responses is
imbalanced17. In Table A4, reporting the AUC, accuracy, and F1 scores across different models
and tasks, we use the threshold of 0.5 for accuracy and F1 scores.
17An accuracy is measured by (true positives + true negatives) / (true positives + false positives + true negatives
+ false negatives), where true positives (TP) are the cases where the model correctly predicts the positive class,
false positives (FP) are the cases where the model predicts the positive class but the actual class is negative, and
false negatives (FN) are the cases where the model predicts the negative class but the actual class is positive. An
F1-score is measured by 2 * (precision * recall) / (precision + recall). Precision is calculated by (true positives)
/ (true positive + false positive), capturing how often the model correctly predicts the positive response, whereas
recall is given by (true positives) / (true positives + false negatives). Since these metrics require binary predictions,
we binarize predicted probabilities based on 0.5.
39Model evaluation: Public opinion prediction
We estimate public opinion by aggregating the predicted personal opinion while using survey
weights. Specifically, the fine-tuned model predicts the probability that a particular individual
will give a positive response to a particular survey question in a particular survey year. Then,
we estimate the marginal proportion of individuals giving positive responses to the question
by taking the weighted mean of predicted probabilities using survey weights to account for
sample selection biases. In doing so, we use a linear regression model to rescale the predicted
responses to be close to the observed responses when aggregated, which helps to minimize the
error caused by the difference between the variation in predictions and observations18. To eval-
uate the accuracy of public opinion predictions, we conduct 10-fold cross-validation to produce
predicted probabilities and estimate correlation coefficients between the predicted and observed
proportion of positive responses. High correlations indicate the linear association between the
predicted and observed proportions, which enhances our confidence in using predicted opinions
to capture public opinion trends, including unobserved opinions. Along with correlation coeffi-
cients, we use a conventional margin of error (3%) to evaluate how many of our predictions can
estimate the observed mean for each survey. Specifically, we measure the percent prediction
error by calculating the proportion of opinions that can be estimated by predicted agreement
rates within a 3% margin of error. Finally, we predict counterfactual trends in Figure 4 using
aggregated responses from retrodiction models. To estimate the direction and magnitude of
over-time trends, we employ a Generalized Additive Model (GAM), a flexible, non-parametric
regression technique capable of modeling complex, non-linear relationships between variables
(Hastie, 1992).
Appendix D. MCAR, MAR, and MNAR simulation
Previous research on imputing missing data has explored three different assumptions about how
missing data occurs: Missing Completely at Random (MCAR), Missing at Random (MAR),
and Missing Not at Random (MNAR) (Sengupta et al., 2023). Since our evaluation process
only assumes MCAR, we simulate the missing data based on the other two assumptions, MAR
and MNAR, and re-evaluate the models to examine whether our models show similarly high
accuracy. MCAR supposes that the cause of missing values in a variable is not related to the
observed or unobserved values of any variable. MAR assumes that the reason for missing
values relies only on the values of other observed variables and is not related to the values of
the missing variable. MNAR assumes that the reason for missing values is dependent on the
values of both observed and unobserved variables.
We simulate missing data created under these three assumptions and evaluate whether the
model can anticipate the missing values for these three mechanisms. Let us assume that we
18Note that the simple linear regression model (i.e., y=ax+b) only applies a linear transformation to the predicted
probabilities, without affecting the correlation coefficient. We use the same regression coefficients (a, b) across all
opinions.
40have a dataset Xrepresented by a n×pmatrix, where i= 1, . . . , n denotes individuals and
j= 1, . . . , p denotes observed and unobserved variables. The elements of Xare denoted by
xi,jwhich indicate the survey responses. Additionally, we define a matrix Mthat indicates the
locations of the missing values as a n×pmatrix, where mi,j= 1ifxi,jis missing, and mi,j= 0
ifxi,jis observed. Xobserved andXunobserved represent the observed and unobserved data in X,
respectively. ξis a missingness parameter, which is random and independent of the data X.
First, MCAR assumes that Mis completely unrelated to the data X– unrelated to both
Xobserved andXunobserved . In other words, p(M= 1|X, ξ ) = p(M= 1|ξ). To simulate
missing data based on MCAR, we randomly selected 10% of observed values and removed
them. Second, MAR assumes that Mdepends on the observed values of other variables but
not on unobserved variables that the missing data imputation model cannot consider. There-
fore, p(M= 1|X, ξ ) =p(M= 1|Xobserved , ξ). To simulate missing data based on MAR, we
adopt the method proposed by Sengupta and colleagues (2023) that fits a logistic regression
model predicting mi,jusing the values of other observed variables. Specifically, we used the
observed variables that rarely have missing values (less than 10%) to predict mi,j. Then, we
remove the 10% of values with the highest probability of being missing based on the regres-
sion. Third, MNAR assumes that Mdepends on both observed and unobserved data. Thus,
p(M= 1|X, ξ ) =p(M= 1|Xobserved , X unobserved , ξ). To simulate missing data based on MNAR,
we fit a logistic regression model that predicts mi,jusing demographic variables (i.e., age, co-
hort, gender, race, education, income, and religion). Since demographic variables have not been
used by the missing data imputation model in our main results, we assume that these variables
are unobserved and use these variables to generate MNAR missing values.
Appendix E. Matrix factorization
To establish a benchmark model for comparison, we train a matrix factorization model, which
is a well-established technique for solving matrix completion problems by estimating missing
values based on available elements. This involves decomposing a matrix with known elements
into two lower-dimensional matrices. By optimizing the dot products of these matrices to align
with the known elements of the original matrix, accurate predictions for the missing values
can be obtained. It achieves high accuracy comparable to deep learning models, making it a
popular choice in recommender system development (Koren et al., 2009). Matrix factorization
also outperforms traditional imputation methods, such as Amelia and MICE, in filling missing
data in sparse survey datasets (Sengupta et al., 2023).
First, we create a matrix S∈R68,846×3,110of survey responses. Each row represents an in-
dividual, each column represents a survey question, and each element indicates the individual’s
binarized response (0 = negative and 1 = positive, see Table A1). Using this matrix S, we train
two lower-dimensional matrices: I∈R68,846×50andQ∈R50×3,110. The objective is to opti-
mize IandQsuch that the dot product of the corresponding rows and columns closely matches
the available values in S. Specifically, we minimize the squared difference (Ii·Q·q−Siq)2for
the available values in S. To optimize IandQ, we employ the alternating least squares (ALS)
41method, following the previous study on the GSS 2014 data (Sengupta et al., 2023). We con-
duct ALS for 15 iterations, applying a regularization penalty of λ= 10 to prevent overfitting.
Using the learned latent factors from the matrix factorization model optimized throughout this
process, we predict the missing response of an individual ifor a given variable qby taking the
dot product of the corresponding vectors: Ii·andQ·q.
Appendix F. Source code
The source code provided below outlines the implementation of our model’s architecture and
details the processes of training and evaluating the model. The code assumes that the survey data
has been preprocessed and converted into a dataframe format with five columns ( gss.parquet ):
year,yearid ,variable ,question , and binarized . Each row in this dataframe represents an in-
dividual’s response to a specific question during a particular survey year (refer to Figure A5
for more details). Here, year denotes the survey year, yearid refers to the unique ID assigned
to each survey participant, variable signifies the variable name in the GSS dataset, question
describes the text of the survey question, and binarized indicates the binarized survey response
(with 1 representing a positive response, and 0 a negative response). Given this setup, the
subsequent steps in the code let you fine-tune the model and evaluate its performance.
1import pandas as pd
2from sklearn.preprocessing import LabelEncoder
3import time
4import pickle
5import random
6import os
7import numpy as np
8import pandas as pd
9from tqdm.auto import tqdm
10import torch
11import tensorflow as tf
12import tensorflow_recommenders as tfrs
13from tensorflow.keras import Model
14from tensorflow.keras.callbacks import ModelCheckpoint
15from tensorflow.keras.layers import Input, Dense, Dropout, Embedding,
Reshape
16from transformers import AutoTokenizer, AutoModel, LlamaTokenizer,
LlamaForCausalLM
17from sklearn.model_selection import train_test_split
18from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
19
20# Setting random seed for reproducibility.
21os.environ[’PYTHONHASHSEED’] = str(42)
22random.seed(42)
23np.random.seed(42)
24torch.manual_seed(42)
25tqdm.pandas()
4226
27df = pd.read_parquet(’gss.parquet’)
28
29# Generate ordered IDs for use in models (e.g., convert 20060001, 20060002,
... to 0, 1, 2)
30le = LabelEncoder()
31df[’yearid_id’] = le.fit_transform(df[’yearid’])
32df[’question_id’] = le.fit_transform(df[’variable’])
33df[’year’] = df[’yearid’] // 10000
34df[’year_order’] = le.fit_transform(df[’year’])
35
36# Load the Alpaca 7b model. Replace "Alpaca-7b" with the actual model name
from Hugging Face, such as "Alpaca-7b", "GPT-J-6b", or "RoBERTa-base".
Refer to this link to get the Alpaca-7b model: https://github.com/tatsu-
lab/stanford_alpaca
37model_name = "Alpaca-7b"
38tokenizer = LlamaTokenizer.from_pretrained("Alpaca-7b")
39model = LlamaForCausalLM.from_pretrained("Alpaca-7b")
40
41# Iterate over survey questions, generate an Alpaca prompt template for
each, and retrieve the embeddings of the last token.
42question_list = df[[’question_id’, ’question’]].drop_duplicates().
sort_values(’question_id’)[’question’].tolist()
43embeddings = []
44for question in tqdm(question_list):
45 prompt = "Below is an instruction that describes a task. Write a
response that appropriately completes the request.\n\n"
46 prompt += f"### Instruction:{question}\n\n### Response:"
47 input_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
48 outputs = model(input_ids, output_hidden_states=True)
49 embeddings.append(outputs.hidden_states[-1][:, -1, :].detach().numpy())
#last token embedding
50
51if ’/’ in model_name:
52 model_name = model_name.replace(’/’, ’_’)
53
54pickle.dump(embeddings, open(model_name+’.pkl’, ’wb’))
55
56train_data, val_data = train_test_split(df, test_size=0.1)
57train_data.to_parquet(’train_data.parquet’)
58val_data.to_parquet(’val_data.parquet’)
59
60# Split the dataset into training and validation sets. This example
demonstrates the data splitting process specifically for a missing data
imputation task.
61train_data, val_data = train_test_split(df, test_size=0.1)
62train_data.to_parquet(’train_data.parquet’)
63val_data.to_parquet(’val_data.parquet’)
64
4365def get_model(dim=50):
66 # End-to-end model architecture
67 # Embedding 1: Individual Belief Embedding
68 weights = np.vstack(pickle.load(open(’chavinlo_alpaca-native.pkl’, ’rb’
))) # Load the last token embeddings.
69
70 individual_id = Input(name=’individual_id’, shape=(1, ))
71 x1 = Embedding(df[’yearid_id’].max() + 1, dim, name=’
individual_embedding’)(individual_id)
72 x1 = Reshape((dim, ), name=’reshape1’)(x1)
73 # Embedding 2: Survey Question Semantic Embedding (based on Alpaca)
74 question_id = Input(name=’question_id’, shape=(1, ))
75 question_embedding = Embedding(weights.shape[0], weights.shape[1],
weights=[weights], name=’question_embedding’)(question_id)
76 question_embedding.trainable = False
77 x2 = Dense(50, name=f’question_embedding2’)(question_embedding)
78 x2 = Reshape((50, ), name=’reshape2’)(x2)
79 # Embedding 3: Period Embedding
80 year_id = Input(name=’year_id’, shape=(1, ))
81 x3 = Embedding(df[’year_order’].max() + 1, dim, name=’year_embedding’)(
year_id)
82 x3 = Reshape((dim, ), name=’reshape3’)(x3)
83 # Concatenating three embeddings
84 x = [x1, x2, x3]
85 x = tf.concat(x, axis=1, name=’concat1’)
86 # Passing through cross layers
87 for i in range(3):
88 x = tfrs.layers.dcn.Cross(projection_dim=dim *3, kernel_initializer=
"glorot_uniform", name=f’cross_layer_{i}’)(x)
89 x = Dropout(0.2)(x)
90 # Passing through feed-forward layers
91 for i in range(3):
92 x = Dense(dim *3, activation="relu", name=f’dense_layer_{i}’)(x)
93 x = Dropout(0.2)(x)
94 # Generating final predictions
95 out = Dense(1, activation=’sigmoid’, name="out")(x)
96 inputs = {’individual_id’: individual_id,
97 ’question_id’: question_id,
98 ’year_id’: year_id}
99 model = Model(inputs=inputs, outputs=out)
100 return model
101
102def train(train, val):
103 model = get_model()
104 train_features = {’individual_id’: np.array(train[’yearid_id’]),
105 ’question_id’: np.array(train[’question_id’]),
106 ’year_id’: np.array(train[’year_order’])}
107 train_label = np.array(train[’binarized’])
108 val_features = {’individual_id’: np.array(val[’yearid_id’]),
44109 ’question_id’: np.array(val[’question_id’]),
110 ’year_id’: np.array(val[’year_order’])}
111 val_label = np.array(val[’binarized’])
112
113 # Compiling the model
114 lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
115 2e-05,
116 decay_steps=80000,
117 decay_rate=0.96,
118 staircase=True)
119 model.compile(
120 optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
121 loss=tf.keras.losses.BinaryCrossentropy(),
122 metrics=[tf.keras.metrics.AUC(),
123 tf.keras.metrics.BinaryAccuracy()]
124 )
125 # Fitting the model to the data
126 history = model.fit(train_features, train_label,
127 validation_data=(val_features, val_label),
128 batch_size=8,
129 epochs=10,
130 verbose=1,
131 use_multiprocessing=True,
132 workers=20)
133
134 return model
135
136# Training the model
137train_data = pd.read_parquet(’train_data.parquet’)
138val_data = pd.read_parquet(’val_data.parquet’)
139trained_model = train(train_data, val_data)
140trained_model.save_weights(’missing_imputation.h5’)
141
142# Evaluate the model
143prediction_model = get_model()
144prediction_model.load_weights(’missing_imputation.h5’)
145obs = val_data[’binarized’]
146pred = prediction_model.predict()
147print(roc_auc_score(obs, pred), accuracy_score(obs, pred>0.5), f1_score(obs
, pred>0.5))
45Appendix Tables and Figures
Table A1: Examples of text responses generated by the Alpaca-7b model.
Variables Survey questions Text responses
homosex What about sexual relations be-
tween two adults of the same sex–
do you think it is always wrong,
almost always wrong, wrong only
sometimes, or not wrong at all?I believe that sexual relations be-
tween two adults of the same sex is
not wrong at all.
marhomo1 Do you agree or disagree with the
following statement? Homosexual
couples have the right to marry one
another.I agree.
busing A. In general, do you fa-
vor or oppose the busing of
(Negro/Black/African-American)
and white school children from one
school district to another?I oppose the busing of
(Negro/Black/African-American)
and white school children from one
school district to another.
nomeat C. And how often do you refuse to
eat meat for moral or environmental
reasons?I rarely refuse to eat meat for moral
or environmental reasons.
Note: These responses were generated without reflecting a particular individual’s opinions in
our training data. These responses were generated solely for checking whether the model un-
derstands the semantic meaning of survey questions correctly.
46Table A2: Most similar survey questions.
Survey questions Most similar survey questions Cosine
similarity
(homosex) What about
sexual relations between two
adults of the same sex–do
you think it is always wrong,
almost always wrong, wrong
only sometimes, or not
wrong at all?(homosex1) C. And what about sexual relations
between two adults of the same sex, is it. . .0.949
(teensex) There’s been a lot of discussion about
the way morals and attitudes about sex are
changing in this country. If a man and woman
have sex relations before marriage, do you
think it is always wrong, almost always wrong,
wrong only sometimes, or not wrong at all? A.
What if they are in their early teens, say 14 to
16 years old? In that case, do you think sex
relations before marriage are always wrong,
almost always wrong, wrong only sometimes,
or not wrong at all?0.781
(premars1) A. Do you think it is wrong or not
wrong if a man and a woman have sexual
relations before marriage?0.773
(marhomo1) Do you agree or
disagree with the following
statement? Homosexual
couples have the right to
marry one another.(marhomo) Do you agree or disagree? J.
Homosexual couples should have the right to
marry one another.0.951
(grassy) Some people think the use of
marijuana should be made legal. Other people
think marijuana use should not be made legal.
Which do you favor?0.892
(ssmchild) To what extent do you agree or
disagree with the following statements? B. A
same sex male couple can bring up a child as
well as a male-female couple.0.871
47Table A2: Most similar survey questions (continued).
Survey questions Most similar survey questions Cosine
similarity
(busing) A. In general, do
you favor or oppose the
busing of
(Negro/Black/African-
American) and white school
children from one school
district to another?(busing10) B. Now, thinking about ten years
ago, that is in 1972, did you then favor or
oppose the busing of (Negro/Black) and white
school children from one school district to
another?0.883
(unvote) If an election were held with secret
ballots, would you vote for or against having a
union represent you?0.810
(defund) Favor or oppose reducing funding for
police and instead funding social services0.798
(nomeat) C. And how often
do you refuse to eat meat for
moral or environmental
reasons?(racpromo) Do you think your race or ethnic
background makes your promotion
opportunities better or worse?0.860
(sparts) Listed below are various areas of
government spending. Please indicate whether
you would like to see more or less government
spending in each area. Remember that if you
say ”much more,” it might require a tax
increase to pay for it. H. Culture and the arts.0.839
(hapifwrk) Do you agree or disagree ... D. A
woman and her family will all be happier if she
goes out to work.0.836
Note: For each survey question in Figure 4, we present the most similar survey questions in
the semantic embedding space. The variable names are presented in parenthesis. The similarity
between two survey questions is measured by the cosine similarity between the two semantic
embeddings of the two survey questions.
48Table A3: Binary Transformation of Survey Response Options: Top 50 responses.
Rank Response Options N Binarized
1 yes, no 1010 1, 0
2 strongly agree, agree, neither agree nor disagree, disagree,
strongly disagree210 1, 1, 0, 0, 0
3 strongly agree, agree, disagree, strongly disagree 115 1, 1, 0, 0
4 mentioned, not mentioned 60 1, 0
5 very likely, somewhat likely, not very likely, not at all likely 46 1, 1, 0, 0
6 too little, about right, too much 45 0, 0, 1
7 true, false 31 1, 0
8 strongly agree, agree, not agree/disagree, disagree, strongly
disagree31 1, 1, 0, 0, 0
9 agree, disagree 30 1, 0
10 strongly agree, agree, neither, disagree, strongly disagree 29 1, 1, 0, 0, 0
11 often, sometimes, rarely, never 28 1, 1, 0, 0
12 agree strongly, agree, neither agree nor disagree, disagree,
disagree strongly27 1, 1, 0, 0, 0
13 not at all, 1 or 2 times, 3-5 times, 6 or more times 26 0, 0, 1, 1
14 1 most desirable, 3 most desirable, not mentioned, 3 least
desirable, 1 least desirable26 1, 1, 0, 0, 0
15 strongly favor, favor, neither favor nor oppose, oppose,
strongly oppose23 1, 1, 0, 0, 0
16 never, 1-2 times, 3-5 times, more than 5 times 21 0, 0, 1, 1
17 strongly agree, agree, disagree, or, strongly disagree? 20 1, 1, 0, 0
18 definitely allowed, probably allowed, prob not allowed, def-
initely not allowed20 1, 1, 0, 0
19 very likely, somewhat likely, somewhat unlikely, very un-
likely18 1, 1, 0, 0
20 very true, somewhat true, not too true, not at all true 18 1, 1, 0, 0
21 definitely should, probably should, probably should not,
definitely should not18 1, 1, 0, 0
22 like very much, like it, mixed feelings, dislike it, dislike
very much18 1, 1, 0, 0, 0
23 spend much more, spend more, spend same, spend less,
spend much less18 1, 1, 0, 0, 0
24 very important, important, somewhat important, not at all
important17 1, 1, 0, 0
25 very likely, somewhat likely, mixed, somewhat unlikely,
very unlikely17 1, 1, 0, 0, 0
49Rank Response Options N Binarized
26 essential, very important, fairly important, not very impor-
tant, not important at all17 1, 1, 1, 0, 0
27 did, didn’t 16 1, 0
28 strongly agree, somewhat agree, somewhat disagree,
strongly disagree16 1, 1, 0, 0
29 definitely willing, probably willing, probably unwilling,
definitely unwilling16 1, 1, 0, 0
30 should, should not 15 1, 0
31 strongly agree, agree somewhat, disagree somewhat,
strongly disagree15 1, 1, 0, 0
32 definitely expect, probably expect, probably not expect, def-
initely not expect15 1, 1, 0, 0
33 a great deal of influence, a fair amount, a little influence,
none at all15 1, 1, 0, 0
34 strongly agree, agree, uncertain, disagree, strongly disagree 15 1, 1, 0, 0, 0
35 a reason, not a reason 14 1, 0
36 major reason, minor reason, not a reason 14 1, 0, 0
37 no, yes, respondent, yes, someone respondent knows, yes,
both respondent and someone respondent knows14 0, 1, 1, 1
38 1 not at all effective, 2, 3, 4, 5 extremely effective 14 0, 0, 0, 1, 1
39 remove, not remove 13 1, 0
40 a great deal, only some, hardly any 13 1, 0, 0
41 most important, 2nd most imp., 3rd most imp., not chosen 13 1, 1, 0, 0
42 very likely, somewhat likely, not too likely, not likely at all 13 1, 1, 0, 0
43 definitely true, probably true, probably not true, definitely
not true13 1, 1, 0, 0
44 allowed, not allowed 12 1, 0
45 no, yes 12 0, 1
46 too much, about the right amount, too little 12 1, 0, 0
47 extremely likely, somewhat likely, not too likely, not likely
at all12 1, 1, 0, 0
48 extremely dangerous, very dangerous, somewhat danger-
ous, not very dangerous, not dangerous12 1, 1, 1, 0, 0
49 excellent, very good, good, fair, poor 12 1, 1, 1, 0, 0
50 many times a day, every day, most days, some days, once in
a while, never or almost never12 1, 1, 1, 0, 0, 0
50Table A4: Comparison of prediction performances across four different models across
three scenarios.
Models
Alpaca-7b GPT-J-6b RoBERTa-
largeMatrix
Factorization
Missing data
imputationAUC 0.866 0.864 0.859 0.852
Accuracy 0.782 0.779 0.774 0.784
F1-score 0.765 0.765 0.758 0.770
RetrodictionAUC 0.860 0.859 0.853 0.798
Accuracy 0.775 0.774 0.768 0.740
F1-score 0.755 0.759 0.750 0.687
Unasked opinion
predictionAUC 0.729 0.687 0.566
Accuracy 0.667 0.632 0.546
F1-score 0.640 0.609 0.422
Note: The best-performing models are highlighted in bold. AUC (Area Under the receiver op-
erating characteristic Curve) measures the probability of the model to rank a randomly selected
positive response over a randomly selected negative response. Accuracy is calculated by (true
positives + true negatives) / (true positives + false positives + true negatives + false negatives),
and F1-score is calculated by 2 * (precision * recall) / (precision + recall). Matrix factorization
methods cannot be applied for unasked opinion prediction.
51Table A5: Feature Importance.
Types of embeddings Feature importance
Semantic embeddings 0.243
Individual belief embeddings 0.112
Period embeddings 0.114
Interactions between semantic and individual belief embeddings 0.180
Interactions between semantic and period embeddings 0.192
Interactions between individual belief and period embeddings 0.128
Note: Higher feature importance score measures the relative contribution of specific features to
the overall predictions. Let n be the dimension of each embedding, x ∈R150be the concatena-
tion of semantic, individual belief, and period embeddings, and W∈R150×150is the learned
weights of the cross-layer based on x. The feature importance is estimated using the Frobenius
norm. Specifically, the feature importance of semantic, individual belief, and period embed-
dings is given byqP50
i=1P50
j=1|Wij|2,qP100
i=51P100
j=51|Wij|2,qP150
i=101P150
j=101|Wij|2, re-
spectively. The feature importance of the interactions between semantic and individual belief
embeddings, interactions between semantic and period embeddings, and interactions between
belief and period embeddings are given by (qP50
i=1P100
j=51|Wij|2+qP100
i=51P50
j=1|Wij|2)/2,
(qP50
i=1P150
j=101|Wij|2+qP150
i=101P50
j=1|Wij|2)/2,(qP100
i=51P150
j=101|Wij|2+qP150
i=101P100
j=51|Wij|2)/2,
respectively. To normalize the feature importance values, they have been divided by the sum of
all feature importance values.
52Figure A1: The visualization of semantic embedding, individual belief embedding, and
period embedding. In Panel A, a two-dimensional t-distributed stochastic neighbor embed-
ding (t-SNE) projection of semantic embeddings among survey questions that belong to one of
the top 10 most frequent categories (tags) in the General Social Survey (N=1458, 46.9% of the
entire 3,110 questions) are presented, with points colored by clusters found by hierarchical clus-
tering. The majority of survey questions under the categories of Children, Federal Government,
Job, Religion, Political, Science, United States, and Work are clustered in the green area, while
Computers and Internet questions are clustered in the orange area, Health questions are clus-
tered in the pink area, and the remaining questions are clustered in the blue area. In Panel B, the
t-SNE projection of 68,846 individuals is presented, with points colored by clusters found by
hierarchical clustering. In Panel C, the t-SNE projection of 33-period embeddings (1972-2021)
is presented, with points colored by survey years ranging from earlier periods to more recent
periods.
537,136 variables6,222 variablesStage 1:Select survey questions available in GSS data explorer5,117 variablesStage 2:Keep only categorical variables3,920variablesRemove questions that include any of the following words:'GO', 'REMARKS', 'EACH', 'INTERVIEWER', 'PERSON', 'ABOVE', 'IF', 'CIRCLE', 'PROBE', 'INSTRUCTION', 'CARD', 'RECORD', 'NAME', 'ASK', 'ONLY', 'YES', 'CODE', 'Appendix'3,110variablesStage 4: Remove questions to interviewers or questions asking social network characteristicsRemove questions whose subjects are under: "Date Of Interview", "Interview", "Sample", "Size Of Place Of Interview", "Social Networks", "Surveys"Stage 3:Remove questions of which response options depend on other questions or questions using cards3,739variablesStage 5: Keep questions of which responses can be binarizedFigure A2: Variable selection process. Among all 7,136 variables in the initial GSS re-
peated cross-sectional data, we follow the five steps to finalize the list of 3,110 variables
in our analytic sample. We download variable information from the GSS data explorer,
the official general social survey website that provides the survey questionnaire ( https:
//gssdataexplorer.norc.org/ ).
54Figure A3: Model Architecture. Here, we present input and output dimensions for each
layer: question embedding = survey question semantic embedding, individual embedding =
individual belief embedding, year embedding = period embedding, Cross= cross layers, Dense=
feed-forward dense layers.
550.8600.8630.8600.862
0.8330.8370.8320.835
0.7200.7130.722
0.705Missing Data Imputation Retrodiction Unasked Opinion Prediction
D0P0D0P1D1P0D1P1 D0P0D0P1D1P0D1P1 D0P0D0P1D1P0D1P10.50.60.70.80.9AUCFigure A4: Model performance with or without demographics or partisanship informa-
tion. We use the personalized LLMs based on Alpaca-7b to measure the AUC of models for
missing data imputation, retrodiction, and unasked opinion prediction using one of the ten folds
in the 10-fold cross-validation scheme. The notation D1P1 indicates that demographic infor-
mation (i.e., age, cohort, gender, race, education, income, and religion) and partisanship in-
formation (i.e., political ideology, party affiliation) are used as training data. D1P0 indicates
that demographic information is used, but partisanship information is not. D0P1 indicates that
partisanship information is used, but demographic information is not. Finally, D0P0 indicates
that neither demographic nor partisanship information is used in the model training.
56Random 
GroupResponseID Variable Year
1 Agree 1 X 2016
5 Disagree 2 X 2016
2 Agree 3 X 2016
5 Disagree 4 X 2018
1 Agree 5 X 2018
4 Agree 6 X 2018
6 Disagree 1 Y 2016
3 Agree 2 Y 2016
4 Disagree 3 Y 2016
3 Agree 4 Y 2018
7 Agree 5 Y 2018
6 Disagree 6 Y 2018
8 Disagree 1 Z 2016
7 Agree 2 Z 2016
2 Agree 3 Z 2016
9 Agree 4 Z 2018
8 Disagree 5 Z 2018
9 Disagree 6 X 2018
… … … … …Random 
GroupVariable Year
1 X 2016
3 X 2018
2 Y 2016
6 Y 2018
4 Z 2016
6 Z 2018
… … …A.Missing Data Imputation B.Retrodictio n
Random 
GroupVariable
1 X
6 Y
4 Z
… …C. Unasked Opinion Predic tionFigure A5: Examples of 10-fold cross-validation scheme. In Panel A, when predicting
response-level missing opinions, we randomly allocate the combinations of year, variable, and
individual ID into ten groups, which are held out in each round of cross-validation. In Panel
B, when forecasting year-level missing opinions, we randomly assign pairs of year and variable
into ten groups, which are held out in each round of cross-validation. In Panel C, when pre-
dicting completely missing opinions, we randomly assign variables into ten groups, which are
held out in each round of cross-validation. Data in bold cells are excluded in the first round of
cross-validation. By iterating these processes ten times, we can estimate the prediction we will
get if responses are missing for the entire period, variables, and individuals.
570.867
0.8530.867
0.8530.867
0.853
0.815
0.7610.815
0.7610.815
0.7610.792
0.6890.792
0.6890.792
0.689MCAR MAR MNAR
Alpaca−7b MF Alpaca−7b MF Alpaca−7b MF0.60.70.80.9AUCFigure A6: Performance of Alpaca-7b and matrix factorization models for missing data
imputation by different missing mechanisms. AUC (Area Under Curve) measures the perfor-
mance of the model in predicting data that are MCAR (missing completely at random), MAR
(missing at random), and MNAR (missing not at random) as indicated by each bar. MF: Matrix
factorization.
58Missing Data Imputation Retrodiction Unasked Opinion  Prediction
10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 90 10 20 30 40 50 60 70 80 900.700.750.800.85
Proportion of Missing Data (%)AUC
Alpaca−7b Matrix FactorizationFigure A7: Model performance by the proportion of missing data in training data (10%
to 90%). X-axis indicates the proportion of missing data in the training data. For instance, 10%
indicates that only 10% of the existing data has been used to train the model. Y-axis indicates
the AUC values for missing data imputation, retrodiction, and unasked opinion prediction which
are estimated using one of the ten folds in the 10-fold cross-validation scheme.
59Figure A8: Predicting counter-factual trends in the GSS 1972-2021 for two questions on
support for same-sex marriage. The generalized additive model was used to estimate the
counterfactual trends. We define the correct prediction when the prediction interval within a
3% margin of error includes the observed estimate. The variable name, response option, and
wording of questions for each panel are as followed: Panel A. “Do you agree or disagree with
the following statement? Homosexual couples have the right to marry one another. Strongly
agree (=1), agree (=1), neither agree nor disagree (=0), disagree (=0), strongly disagree (=0)”.
Panel B. “Do you agree or disagree with the following statement? Homosexual couples should
have the right to marry one another. Strongly agree (=1), agree (=1), neither agree nor disagree
(=0), disagree (=0), strongly disagree (=0)”.
600.60.650.70.750.80.850.9
05101520253035AUCNumber of years in training dataFigure A9: Number of years per survey question in training data and AUC in retrodiction.
X-axis indicates the number of years per survey question in training data. For instance, the value
of 1 indicates the case when a survey question is asked only once. Y-axis indicates the AUC
values for year-level missing opinions.
61Figure A10: Model performance by the number of survey questions in the training data.
AUC (Area Under Curve) measures the probability of the model to rank a randomly selected
positive response over a randomly selected negative response. The Y-axis of the graph repre-
sents the AUC score, which is an indicator of how well the model can accurately fill in response-
level missing opinion data. The X-axis displays the number of survey questions in the training
data. We only use survey participants in 2016, 2018, and 2021 for this analysis.
62Figure A11: Comparison of results from OLS regression models predicting individual-
level AUC across two different types of missing data imputation between Alpaca and ma-
trix factorization models. A higher AUC value indicates greater model accuracy for individ-
uals. Here, each dot represents the expected difference of AUC (i.e., average marginal effects)
against the reference group within each subgroup. Here, a filled dot refers to a statistically
significant difference, and an X refers to a statistically insignificant difference based on robust
standard errors (p <0.05).
63