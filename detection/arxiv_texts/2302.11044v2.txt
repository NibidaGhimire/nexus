arXiv:2302.11044v2  [cs.DS]  11 Apr 2023The Target-Charging Technique
for Privacy Analysis across Interactive Computations
Edith Cohen∗Xin Lyu†
Abstract
We propose the Target Charging Technique (TCT), a uniﬁed privacy analysis framework
for interactive settings where a sensitive dataset is acces sed multiple times using diﬀerentially
private algorithms. Unlike traditional composition, wher e privacy guarantees deteriorate quickly
with the number of accesses, TCT allows computations that do n’t hit a speciﬁed target, often the
vast majority, to be essentially free (while incurring inst ead a small overhead on those that do
hit their targets). TCT generalizes tools such as the sparse vector technique and top- kselection
from private candidates and extends their remarkable priva cy enhancement beneﬁts from noisy
Lipschitz functions to general private algorithms.
1 Introduction
In many practical settings of data analysis and optimizatio n, the dataset Dis accessed multiple
times interactively via diﬀerent algorithms (Ai), so thatAidepends on the transcript of prior
responses (Aj(D))j<i. When eachAiis privacy-preserving, we are interested in tight end-to-e nd
privacy analysis. We consider the standard statistical fra mework of diﬀerential privacy introduced
in [DMNS06 ].Composition theorems [ DRV10 ] are a generic way to do that and achieve overall
privacy cost that scales linearly or (via “advanced" compos ition) with square-root dependence in
the number of private computations. We aim for a broad unders tanding of scenarios where the
overall privacy bounds can be lowered signiﬁcantly via the f ollowing paradigm: Each computation
is speciﬁed by a private algorithm Aitogether with a target⊤i, that is a subset of its potential
outputs. The total privacy cost depends only on computation s where the output hits its target, that
isAi(D)∈⊤i. This paradigm is suitable and can be highly beneﬁcial when ( i) the speciﬁed targets
are a good proxy for the actual privacy exposure and (ii) we ex pect the majority of computations
to not hit their target, and thus essentially be “free” in ter ms of privacy cost.
The Sparse Vector Technique (SVT) [ DNR+09,RR10,HR10,Vad17 ] is the quintessential special
case. SVT is focused on speciﬁc type of computations that hav e the form of approximate threshold
tests applied to Lipschitz functions. Concretely, each suc hAboveThreshold test is speciﬁed by
a1-Lipschitz function fand a threshold value tand we wish to test whether f(D)/greaterorsimilart. The
textbook SVT algorithm compares a noisy value with a noisy th reshold (independent Laplace noise
for the values and threshold noise that can be updated only af ter positive responses). Remarkably,
the overall privacy cost depends only on the number of positi ve responses, roughly, composition is
applied to twice the number of positive responses instead of to the total number of computations.
Using our terminology, the target of each test is a positive r esponse.
∗Google Research and Tel Aviv University. edith@cohenwang.com .
†UC Berkeley and Google Research. lyuxin1999@gmail.com .
1SVT privacy analysis beneﬁts when the majority of AboveThreshold test results are negative
(and hence “free”). This makes SVT a key ingredient in a range of methods [ DR14 ]: private
multiplicative weights [ HR10], Propose-Test-Release [ DL09], ﬁne privacy analysis via distance-to-
stability [ TS13], model-agnostic private learning [ BTGT18 ],and designing streaming algorithms that
are robust to adaptive inputs [ HKM+20,CLN+22a].1
We aim to extend such SVT-like privacy analysis beneﬁts to in teractive applications of general
private algorithms (that is, algorithms that provide priva cy guarantees but have no other assump-
tions): private tests, where we would hope to incur privacy c ost only for positive responses, and
private algorithms that return more complex outputs, e.g., vector average, cluster centers, a sani-
tized dataset, or a trained ML model, where the goal is to incu r privacy cost only when the output
satisﬁes some criteria. The textbook SVT, however, seems le ss amenable to such extensions: First,
SVT departs from the natural paradigm of applying private al gorithms to the dataset and reporting
the output. A natural implementation of private AboveThreshold tests would add Laplace noise
to the value and compare with the threshold. Instead, SVT tak es as input the Lipschitz output
of the non-private algorithms with threshold value and the p rivacy treatment is integrated (added
noise both to values and threshold). The overall utility and privacy of the complete interaction
are analyzed with respect to the non-private values, which i s not suitable when the algorithms are
already private. Furthermore, the technique of using a hidd en shared threshold noise across mul-
tiple AboveThreshold tests2is speciﬁc for Lipschitz functions, introduces dependenci es between
responses (that are biased the same way and can be undesirabl e for downstream applications), and
more critically, implies additional privacy cost for repor ting noisy values. Analytics tasks often re-
quire a value to be reported with an above-threshold test res ult, which incurs an additional separate
privacy charge with SVT [ LSL17 ].3
Private tests, mentioned above, are perhaps the most basic e xtension for which we seek SVT-like
beneﬁts. The natural approach would be to apply each test onc e, report the result, and hope to
incur privacy charge only on positive responses. Private te sting was considered in prior works [ LT19,
CLN+22b] but in ways that signiﬁcantly departed from this natural pa radigm: Instead, the approach
of [LT19] processed the private tests so that a positive answer is ret urned only when the probability
pof a positive response by the private test is very close to 1.4This seems unsatisfactory: If the
design goal of the private testing algorithm was to report on ly very high probabilities, then this could
have been integrated into the design (possibly while avoidi ng the factor-2 privacy overhead), and
if otherwise, then we miss out on acceptable positive respon ses with moderately high probabilities
(e.g. 95%).
Top-kselection is another setting where careful privacy analysis is hugely beneﬁcial. Top- kis
a basic subroutine in data analysis, where input algorithms (Ai)i∈[m](aka candidates) that return
results with quality scores are provided in a batch (i.e., non interactively). The selection returns the
kcandidates with highest quality scores on our dataset. The r espective private construct, where the
data is sensitive and the algorithms are private, had been in tensely studied [ MM09 ,FS10,SU17]. We
1Robustness was linked to privacy so that use of SVT allowed de pendence on changes to the output rather than
on the typically much larger number of updates to the input.
2We mention that[ HR10] did not use noisy thresholds but nearly all followup works d id
3Reporting the noisy value that was compared with the noisy th reshold discloses information on the shared
threshold noise.
4Assuming a percentile oracle , that provides the probability pof a1response, they applied AboveThreshold to
log(p/(1−p)(that is2ε-Lipschitz when the test is ε-private). Note that by adding Lap(1/ε)noise to the log ratio,
we eﬀectively need to use a threshold that applies only for pthat is extremely close to 1, that is, 1−p≈2−1/ε.
Therefore positive responses are reported with a diﬀerent ( and much lower) probability than the original test. For the
case where a percentile oracle is not available, [ LT19] proposed an approximate DP computation with a somewhat
less eﬃcient bound.
2might hope for privacy cost that is close to a composition ove rkprivate computations, instead of over
m≫k. The natural approach for top- k(and what we would do on non-sensitive data) is one-shot
(Algorithm 3), where each algorithm is applied once and the responses wit h top-kscores are reported.
Prior works on private selection that achieve this analysis goal include those [ DR19 ,QSZ21 ] that
use the natural one-shot selection but are tailored to Lipsc hitz functions (apply the Exponential
Mechanism [ MT07 ] or the Report-Noise-Max paradigm [ DR14 ]) and works [ LT19,PS22,CLN+22b]
that do apply with general private algorithms but signiﬁcan tly depart from the natural one-shot
approach: They make a randomized number of computations tha t is generally much larger than
m, with eachAiinvoked multiple times or none. The interpretation of the se lection deviates from
top-1and does not naturally extend to top- k.5We seek privacy analysis that applies to one-shot
top-kselection with candidates that are general private algorit hms.
The departures made in prior works from the natural interact ive paradigm and one-shot selec-
tion were essentially compromises: Simple arguments (that apply with both top- 1one-shot private
selection [ LT19] and AboveThreshold tests) show that SVT-like beneﬁts are not possible: If we
perform mcomputations that are ε-DP (that is, mcandidates or mtests), the privacy parameter
value for a pure DP bound is Ω(m)εand the parameter values for an approximate DP bound are
(Ω(εlog(1/δ)),δ). This is a daunting overhead – the privacy charge is of O(log(1/δ))instead of
O(1)invocations. The departure allowed for the appealing beneﬁ ts of pure-DP and remarkably,
for low privacy overhead (factor of 2 or 3 increase in the εparameter) even with a single “above”
response or a single selection.
We revisit the natural paradigms for interactive accesses a nd one-shot selection, for their simplic-
ity, interpretability, and generality, with a fresh approa ch. Considering the mentioned limitations,
we take approximate DP to be a reasonable compromise (that is anyhow necessary with advanced
composition and other divergences). Additionally, we aim f or the regime where many private compu-
tations are performed on the same dataset and out of these man y computations we expect multiple,
sayΩ(log(1/δ)), “target hits” (e.g. positive tests and sum of the k-values of selections). With these
particular relaxations in mind, can we obtain SVT-like bene ﬁts (e.g. privacy charge that corresponds
toO(1)calls per “target hit”) with the natural paradigm? Moreover , can we integrate private top- k
selections in a uniﬁed target-charging analysis, so that ea ch top-kselection we perform amounts
toO(k)additional target hits? Such uniﬁcation would facilitate t ighter analysis with advanced
composition (performed over all target hits) and amortize o verheads.
2 Overview of Contributions
We introduce the Target-Charging Technique (TCT) for privacy analysis over interactive private
computations (see Algorithm 1). Each computation performed on the sensitive dataset Dis speciﬁed
by a private algorithm Aiandtarget pairs⊤i. The interaction is halted after a pre-speciﬁed number
τof computations that satisfy Ai(D)∈⊤i. We deﬁne targets as follows:
Deﬁnition 2.1 (q-Target) .LetM:Xn→Ybe a randomized algorithm. For q∈(0,1]andε >0,
we say that a subset ⊤⊆Y of all possible outcomes is a q-Target ofMif the following holds: For
any pair D0andD1of neighboring data sets, there exist p∈[0,1], and three distributions C,B0
andB1such that
5[LT19] proposed two algorithms for selecting a top candidate from ε-DP candidates. Their design uses a ran-
domized overall number of applications the candidates (tha t is, the algorithms {Ai}). Each call is made with Ai
wherei∼[m]is selected uniformly at random. The ﬁrst algorithm has over all privacy parameter that is ≈2εand
the output has quantile guarantees. The second algorithm ha s privacy parameter 3εand returns the top score over
all calls. Due to the randomized invocations, a logarithmic factor increase in the number of calls is needed in order
to make sure each algorithm is called at least once.
31. The distributions M(D0)andM(D1)can be written as the following mixtures:
M(D0)≡p·C+(1−p)·B0,
M(D1)≡p·C+(1−p)·B1.
2.B0,B1are(ε,0)-indistinguishable,
3.min(Pr[B0∈⊤],Pr[B1∈⊤])≥q.
The eﬀectiveness of a target as a proxy of the actual privacy c ost is measured by its q-value
whereq∈(0,1]. We interpret 1/qas the overhead factor of the actual privacy exposure per target
hit, that is, the number of private accesses that correspond to a single target hit. Note that an
algorithm with a q-target for ε >0must be (ε,0)-DP and that any (ε,0)-DP algorithm has a
1-target, as the set of all outcomes ⊤=Yis a1-target (and hence also a q-target for any q≤1).
The helpful targets are “smaller” (so that we are less likely to be charged) with larger q(so that the
overhead per charge is smaller). We establish the following privacy bounds.
Lemma 2.2 (simpliﬁed meta privacy cost of target-charging) .The privacy parameters of Algo-
rithm 1(applied with ε-DP algorithmsAiandq-targets⊤iuntil targets are hit τtimes) is (ε′,δ)
whereε′≈τ
qεandδ=e−O(τ).
Alternatively, we obtain parameter values (ε′,δ′) = (fε(r,ε),fδ(r,ε) +e−O(τ))wherer≈τ/q
and(fε(r,ε),fδ(r,ε))are privacy parameter values for advanced composition [ DRV10 ] ofr ε-DP
computations.
The proof is provided in Section Bfor a precise and more general statement that applies with
approximate DP algorithms (in which case the δprivacy parameter values of all calls add up). The
proof idea is simple but surprisingly powerful: We compare t he execution of Algorithm 4on two
neighboring data sets D0,D1. Given a request (A,⊤), letp,C,B,B0,B1be the decomposition of
Aw.r.t.D0,D1given by Deﬁnition 2.1. Then, running AonD0,D1can be implemented in the
following equivalent way: we ﬁrst ﬂip a p-biased coin. With probability p, the algorithm samples
fromCand returns the result. Note that in this case, we do notneed to access D0,D1at all!
Otherwise, the algorithm needs to sample from B0orB1, depending on whether the private data is
D0orD1. However, by Property 3 in Deﬁnition 2.1, there is a decent chance (e.g., with probability
at leastq) that Algorithm 1will “notice” the privacy-leaking computation by observin g a result in
the target set⊤. If this indeed happens, the algorithm increments the count er. On average, each
counter increment corresponds to1
qmany accesses to the private data. Finally, when τis moderately
large we apply a concentration inequality to bound the proba bility that the actual number of calls
much exceeds its expectation of τ/q.
The TCT analysis uses the number of target hits (multiplied b y1/q) as a proxy for the actual
privacy leak, with tail bounds applied to obtain high conﬁde nce bounds on the error. The multi-
plicative error decreases when the number τof target hits is larger. In the regime τ >ln(1/δ), we
amortize the mentioned O(log(1/δ))overhead of the natural paradigm and achieve SVT-like bound s
where each target hit results in privacy cost equivalent to O(1/q)calls. In the regime of very few
target hits (e.g., few private tests or private selections) , we still have to eﬀectively “pay” for the
largerτ= Ω(ln(1 /δ)), but TCT still has some advantages over alternative approac hes, due to its
use of the natural paradigm and its applicability with gener al private algorithms.
TCT can be extended to the case where algorithms have varied p rivacy parameter and target
overhead values that may be adaptively chosen. A simple anal ysis can work with the smallest values
encountered. With a tighter analysis, we can expect/summationtext
iεi/qito roughly replaces τε/q, but this
4requires calculation of tighter tail and composition bound s is more complex [ KOV15 ,RRUV16 ] and
does not have simple forms. These are useful (but technical) extensions that we leave for follow up
work.
Algorithm 1: Target Charging
Input: DatasetD={x1,...,x n}∈Xn. Integer τ≥1(Upper limit on the number of target hits).
Fraction q∈[0,1].
C←0 // Initialize target hit counter
whileC < τ do // Main loop
Receive (A,⊤)whereAis anε-DP mechanism, and ⊤is aq-target forA
r←A(D)
Publish r
ifr∈⊤thenC←C+1 // outcome is a target hit
Despite its simplicity, TCT turns out to be surprisingly pow erful due to the existence of natural
targets with low overhead. We present an expansive toolkit t hat is built on top of TCT and describe
application scenarios.
2.1 NotPrior targets
ANotPrior target of an ε-DP algorithm is speciﬁed by any outcome of our choice (the “p rior") that
we denote by⊥. The NotPrior target is the set of all outcomes except ⊥. Surprisingly perhaps,
this is an eﬀective target (See Section Cfor the proof that applies also with approximate-DP):
Lemma 2.3 (Property of a NotPrior target) .LetA:X→Y∪{⊥} , where⊥/ne}a⊔ionslash∈Y , be anε-DP
algorithm. Then the set of outcomes Yconstitutes an1
eε+1-target forA.
Note that for small ε, we have qapproaching 1/2and thus the overhead factor is close to 2.
The TCT privacy analysis is beneﬁcial over plain compositio n when the majority of all outcomes in
our interaction match their prior ⊥. We describe application scenarios for NotPrior targets. For
most of these scenarios, TCT is the only method we are aware of that provides the stated privacy
guarantees in the general context.
Private testing A private test is a private algorithm with a Boolean output. By s pecifying
our prior to be a negative outcome, we obtain an overhead of 2(for small ε) for positive responses,
which matches the overhead of SVT. TCT is the only method we ar e aware of that provides SVT-like
guarantees with general private tests.
Pay-only-for-change When we have a prior on the result of each computation and expe ct the
results of most computations to agree with their respective prior, we set⊥to be our prior. We report
all results but pay only for those that disagree with the prio r. We describe some use cases where
paying only for change can be very beneﬁcial (i) the priors ar e results of the same computations on
an older dataset, so they are likely to remain the same (ii) In streaming or dynamic graph algorithms,
the input is a sequence of updates where typically the number of changes to the output is much
smaller than the number of updates. Diﬀerential privacy was used to obtain algorithms that are
robust to adaptive inputs [ HKM+20,BKM+21] by private aggregation of non-robust copies. The
pay-only-for-change allows for number of changes to output (instead of the much larger number of
updates) that is quadratic in the number of copies. Our resul t enables such gain with any private
aggregation algorithm (that is not necessarily in the form o fAboveThreshold tests).
52.2 Conditional Release
We have a private algorithm A:X→Ybut are interested in the output A(D)only when a certain
condition holds (i.e., when the output is in ⊤⊆Y ). The condition may depend on the interaction
transcript thus far (depend on prior computations and outpu ts). We expect most computations
not to meet their release conditions and want to be “charged” only for the ones that do. Recall
that with diﬀerential privacy, not reporting a result also l eaks information on the dataset, so this
is not straightforward. We deﬁne A⊤:=ConditionalRelease (A,⊤)as the operation that inputs
a dataset D, computes y←A(D). Ify∈⊤, then publish yand otherwise publish ⊥. We show
that this operation can be analysed in TCT as a call with the al gorithm and NotPrior target pair
(A⊤,⊤), that is, a target hit occurs if and only if y∈⊤:
Lemma 2.4 (ConditionalRelease privacy analysis) .A⊤satisﬁes the privacy parameters of A
and⊤is aNotPrior target ofA⊤.
Proof.A⊤processes the output of the private algorithm Aand thus from post processing property
is also private with the same privacy parameter values. Now n ote that⊤is aNotPrior target of
A, with respect to prior ⊥.
We describe some example use-cases:
(i) Private learning of models from the data (clustering, re gression, average, ML model) but we are
interested in the result only when its quality is suﬃcient, s ay above a speciﬁed threshold, or when
some other conditions hold.
(ii) Greedy coverage or representative selection type appl ications, where we incur privacy cost only
for selected items. To do so, we condition the release on the “ coverage” of past responses. For
example, when greedily selecting a subset of features that a re most relevant or a subset of centers
that bring most value.
(iii) Approximate AboveThreshold tests on Lipschitz functions, with release of above-thresh old
noisy values: As mentioned, SVT incurs additional privacy c ost for the reporting whereas TCT
(using ConditionalRelease ) does not, so TCT beneﬁts in the regime of suﬃciently many tar get
hits.
(iv)AboveThreshold tests with sketch-based approximate distinct counts: Dist inct counting sketches [ FM85 ,
FFGM07 ,Coh17 ] meet the privacy requirement by the built-in sketch random ness [SST20 ]. We ap-
plyConditionalRelease and set⊤to be above threshold values. In comparison, despite the
function (distinct count) being 1-Lipschitz, the use of SVT for this task incurs higher overheads in
utility (approximation quality) and privacy: Even for the g oal of just testing, a direct use of SVT
treats the approximate value as the non-private input, whic h reduces accuracy due to the additional
added noise. Treating the reported value as a noisy Lipschit z still incurs accuracy loss due to the
threshold noise, threshold noise introduces bias, and anal ysis is complicated by the response not
following a particular noise distribution. For releasing v alues, SVT as a separate distinct-count
sketch is needed to obtain an independent noisy value [ LSL17 ], which increases both storage and
privacy costs.
2.3 Conditional Release with Revisions
We present an extension of Conditional Release that allows f or followup revisions of the target. The
initial ConditionalRelease and the followup ReviseCR calls are described in Algorithm 2. The
ConditionalRelease call speciﬁes a computation identiﬁer hfor later reference, an algorithm and
6a target pair (A,⊤). It draws rh∼A(D)and internally stores rhand a current target ⊤h←⊤.
Whenrh∈⊤thenrhis published and a charge is made. Otherwise, ⊥is published. Each (followup)
ReviseCR call speciﬁes an identiﬁer hand a disjoint extension ⊤′to its current target ⊤h. Ifrh∈⊤′,
thenrhis published and a charge is made. Otherwise, ⊥is published. The stored current target for
computation his augmented to include ⊤′. Note that a target hit occurs at most once in a sequence
of (initial and followup revise) calls and if and only if the r esult of the initial computation rhis in
the ﬁnal target⊤h.
Algorithm 2: Conditional Release and Revise Calls
// Initial Conditional Release call: Analysed in TCT as a (ε,δ)-DP algorithm A⊤and NotPrior target
⊤
Function ConditionalRelease( h,A,⊤): // unique identifier h, an(ε,δ)-DP algorithm A → Y ,
⊤ ⊂ Y
⊤h←⊤ // Current target for computation h
TCT Charge forδ // Ifδ >0, see Section B
rh←A(D) // Result for computation h
ifrh∈⊤hthen // publish and charge only if outcome is in ⊤h
Publish rh
TCT Charge for a NotPrior target hit of an ε-DP algorithm
else
Publish⊥
// Revise call: Analysed in TCT as a 2ε-DP Algorithm (A | ¬⊤ h)⊤′and NotPrior target ⊤′
Function ReviseCR( h,⊤′): // Revise target to include ⊤′
Input: An identiﬁer hof a prior ConditionalRelease call, target extension ⊤′where
⊤′∩⊤h=∅
ifrh∈⊤′then // Result is in current target, publish and charge
Publish rh
TCT Charge for a NotPrior target hit of an 2ε-DP algorithm
else
Publish⊥
⊤h←⊤h∪⊤′// Update the target to include extension
We show the following (Proof provided in Section D):
Lemma 2.5 (Privacy analysis for Algorithm 2).Each ReviseCR call can be analysed in TCT as a
call to a 2ε-DP algorithm with a NotPrior target⊤′.
Thus, the privacy cost of conditional release followed by a s equence of revise calls is within a
factor of 2 (due to the doubled privacy parameter on revise ca lls) of a single ConditionalRelease
call made with the ﬁnal target.
The revisions extension of conditional release facilitate s our results for private selection, which
are highlighted next.
2.4 Private Top- kSelection
One-shot top- kselection is described in Algorithm 3: We call each algorithm once and report the k
responses with highest quality scores. We establish the fol lowing:
Lemma 2.6 (Privacy of One-Shot Top- kSelection) .Consider one-shot top- kselection (Algorithm 3)
on a dataset Dwhere{Ai}are(ε,δi)-DP. This selection can be simulated exactly in TCT by a
sequence of calls to (2ε,δ)-DP algorithms with NotPrior targets that has ktarget hits.
7As a corollary, assuming ε <1, Algorithm 3is(O(ε/radicalbig
klog(1/δ)),2−Ω(k)+δ+/summationtext
iδi)-DP for
everyδ >0.
To the best of our knowledge, our result is the ﬁrst such bound for one-shot selection from
general private candidates. For the case when the only compu tation performed on Dis a single
top-1selection, we match the “bad example” in [ LT19] (see Theorem I.1). In the regime where
k >log(1/δ)our bounds generalize those speciﬁc to Lipschitz functions in [DR19 ,QSZ21 ] (see
Section I). Importantly, Lemma 2.6allows for a uniﬁed privacy analysis of interactive computa tions
that are interleaved with one-shot selections. We obtain O(1)overhead per target hit when there
areΩ(log(1/δ))hits in total.
Algorithm 3: One-Shot Top- kSelection
Input: A dataset D. Candidate algorithms A1,...,Am. Parameter k≤m.
S←∅
fori= 1,...,m do
(yi,si)←Ai(D)
S←S∪{(i,yi,si)}
returnL←the top-ktriplets from S, by decreasing si
The proofs of Lemma 2.6and implications to selection tasks are provided in Section I. The proof
utilizes Conditional Release with revisions (Section 2.3).
2.4.1 Selection using Conditional Release
We analyse private selection tasks using conditional relea se (see Section Ifor details). First note that
ConditionalRelease calls (without revising) suﬃce for one-shot above-threshold selection (release
all results with quality score that exceeds a pre-speciﬁed t hreshold t), with target hits only on
what was released: We simply specify the release condition t o besi> t. What is missing in order
to implement one-shot top- kselection is an ability to ﬁnd the “right” threshold (a value tso that
exactlykcandidates have quality scores above t), while incurring only ktarget hits. The revise calls
provide the functionality of lowering the threshold of prev ious conditional release calls (lowering the
threshold amounts to augmenting the target). This function ality allows us to simulate a sweep of
themresults of the batch in the order of decreasing quality score s. We can stop the sweep when
a certain condition is met (the condition must be based on the preﬁx of the ordered sequence that
we viewed so far) and we incur target hits only for the preﬁx. T o simulate a sweep, we run a
high threshold tconditional release of all mcandidates and then incrementally lower the threshold
t←t−dtusing sets of mrevise calls (one call per candidate). The released results are in decreasing
order of quality scores. The one-shot top- kselection (Algorithm 3) is simulated exactly by such a
sweep that stops after kscores are released. Hence, the same privacy analysis holds and Lemma 2.6
follows. We emphasize that the sweeping simulation is only f or analysis. The implementation is
described in Algorithm 3.
As mentioned, with this approach we can apply any stopping condition that depends on the preﬁx .
This allows us to use data-dependent selection criteria. On e natural such criteria (instead of using
a rigid value of k) is to choose kwhen there is a large gap in the quality scores, that the (k+1)st
quality score is much lower than the kth score [ ZW22 ]. This criterion can be implemented using
a one-shot algorithm and analyzed in the same way using an equ ivalent sweep. Data-dependent
criteria are also commonly used in applications such as clus tering (choose “the right” number of
clusters according to gap in clustering cost) and greedy sel ection of representatives.
82.5 Best of multiple targets
Multi-target charging, described in Algorithm 8, is a simple but useful extension of Algorithm 1
(that is “single target”). With k-TCT, queries have the form/parenleftbig
A,(⊤i)i∈[k]/parenrightbig
where⊤ifori∈[k]
areq-targets (we allow targets to overlap). The algorithm maint ainskcounters (Ci)i∈[k]. For each
query, for each i, we increment Ciifr∈⊤i. We halt when miniCi=τ.
The multi-target extension allows us to ﬂexibly reduce the t otal privacy cost to that of the “best”
amongktarget indices in retrospect (the one that is hit the least number of times). Interestingl y,
this extension is almost free in terms of privacy cost: The nu mber of targets konly multiplies the
δprivacy parameter (see Section B.1for the proof):
Lemma 2.7 (Privacy of multi-TCT) .Algorithm 8satisﬁes(ε′,kδ′)-approximate DP bounds, where
(ε′,δ′)are privacy bounds for single-target charging (Algorithm 1).
Speciﬁcally, when we expect that one (index) of multiple out comes⊥1,...,⊥kwill dominate
our interaction but can not specify which one it is in advance , we can use k-TCT with NotPrior
targets with priors ⊥1,...,⊥k. From Lemma 2.7, the overall privacy cost depends on the number of
times that the reported output is diﬀerent than the most domi nant outcome. More speciﬁcally, for
private testing, when we expect that one type of outcome woul d dominate the sequence but we do
not know if it is 0or1, we can apply 2-TCT. The total number of target hits corresponds to the less
dominant outcome. The total number of privacy charges (on av erage) is at most (approximately for
smallε) double that, and therefore is always comparable or better t o composition (can be vastly
lower when there is a dominant outcome).
2.6 BetweenThresholds in TCT
TheBetweenThresholds classiﬁer is a reﬁnement of the AboveThreshold test. BetweenThresholds
reports if the noisy Lipschitz value is below, between, or ab ove two thresholds tl< tr.BetweenThresholds
was analysed in [ BSU17 ] in the SVT framework (using noisy thresholds) and it was sho wn that the
overall privacy costs may only depend on the “between" outco mes. Their analysis required that
tr−tl≥(12/ε)(log(10/ε) +log(1 /δ) +1) . We consider the “natural” private BetweenThresholds
classiﬁer that compares the value with added Lap(1/ε)noise to the thresholds. We show (see
Section G) that the “between” outcome is a target with q≥(1−e−(tr−tl)ε)·1
eε+1. Note that the
q-value is smaller by a factor of (1−e−(tr−tl)ε)compared with NotPrior targets. Therefore, there
is smooth degradation in the eﬀectiveness of the between out come as the target as the gap tr−tl
decreases, and matching AboveThreshold when the gap is large. Also note that we require much
smaller gaps tr−tlcompared with [ BSU17 ], also asymptotically ( O(log(1/ε))factor improvement).
Our result brings the use of BetweenThresholds into the practical regime.
Taking a step back, we compare an AboveThreshold test with a threshold twith a BetweenThresholds
classiﬁer with tl=t−1/εandtr=t+1/ε. Counter-intuitively perhaps, despite BetweenThresholds
being more informative than AboveThreshold , as it provides more granular information on the
value, its privacy cost is lower for queries where values are either well above or well below t he
thresholds (since target hits are unlikely also when querie s are well above the threshold). Somehow,
the addition of a third outcome to the test allowed for ﬁner pr ivacy analysis! A natural question
that arises is whether we can extend this beneﬁt more general ly – inject a “boundary outcome”
when our private algorithm does not have one, to tighten the p rivacy analysis. We introduce next
a method that achieves this goal.
92.7 The Boundary Wrapper method
When the algorithm is a tester or a classiﬁer, the result is mo st meaningful when one outcome
dominates the distribution A(D). Moreover, when performing a sequence of tests or classiﬁca tion
tasks we might expect most queries to have high conﬁdence lab els (e.g., [ PSM+18,BTGT18 ]).
Our hope then is to incur privacy cost that depends only on the “uncertainty,” captured by the
probability of non-dominant outcomes.
Recall that when we have for each computation a good prior on w hich outcome is most likely,
this goal can be achieved using NotPrior targets (Section 2.1). When we expect the whole sequence
to be dominated by one type of outcome, even when we don’t know which one it is, this goal can
be achieved via NotPrior with multiple targets (Section 2.5). But these approaches do not apply
when a dominant outcome exists in most computations, but we h ave no handle on it and it can
change arbitrarily between computations in the same sequen ce.
For a private test A, can we somehow choose a moving target per computation to be the value
with the smaller probability argmin b∈{0,1}Pr[A(D) =b]? More generally, with a private classiﬁer,
can we somehow choose the target to be all outcomes except for the most likely one?
Our proposed boundary wrapper , described in Algorithm 4, is a mechanism that achieves that
goal. The privacy wrapper Wtakes any private algorithm A, such as a tester or a classiﬁer, and
wraps it to obtain algorithm W(A). The wrapped algorithm has its outcome set augmented to
include one boundary outcome⊤that is designed to be a q-target. The wrapper returns ⊤with
some probability that depends on the distribution of A(D)and otherwise returns a sample from
A(D)(that is, the output we would get when directly applying AtoD). We then analyse the
wrapped algorithm in TCT.
Note that the probability of the wrapper Areturning⊤is at most 1/3and is roughly proportional
to the probability of sampling an outcome other than the most likely fromA(D). When there is
no dominant outcome the ⊤probability tops at 1/3. Also note that a dominant outcome (has
probability p∈[1/2,1]inA(D)) has probability p/(2−p)to be reported. This is at least 1/3when
p= 1/2and is close to 1whenpis close to 1. For the special case of Abeing a private test, there
is always a dominant outcome.
A wrapped AboveThreshold test provides the beneﬁt of BetweenThresholds discussed in Sec-
tion2.6where we do not pay privacy cost for values that are far from th e threshold (on either side).
Note that this is achieved in a mechanical way without having to explicitly introduce two thresholds
around the given one and deﬁning a diﬀerent algorithm.
Algorithm 4: Boundary Wrapper
Input: DatasetD={x1,...,x n}∈Xn, a private algorithm A
r∗←argmax rPr[A(D) =r] // The most likely outcome of A(D)
π(D)←1−Pr[A(D) =r∗] // Probability that Adoes not return the most likely outcome
c∼Ber(min/braceleftBig
1
3,π
1+π/bracerightBig
) // Coin toss for boundary
ifc= 1then Return⊤else ReturnA(D) // return boundary or value
We establish the following (proofs provided in Section E). The wrapped algorithm is nearly as
private as the original algorithm:
Lemma 2.8 (Privacy of a wrapped algorithm) .IfAisε-DP then Algorithm 4applied toAis
t(ε)-DP where t(ε)≤4
3ε.
Theqvalue of the boundary target of a wrapped algorithm is as foll ows:
10Lemma 2.9 (q-value of the boundary target) .The outcome⊤of a boundary wrapper (Algorithm 4)
of anε-DP algorithm is aet(ε)−1
2(eε+t(ε)−1)-target.
For small εwe obtain q≈t(ε)/(2(ε+t(ε)). Substituting t(ε) =4
3εwe obtain q≈2
7. Since
the target⊤has probability at most 1/3, this is a small loss of eﬃciency ( 1/6factor overhead)
compared with composition in the worst case when there are no dominant outcomes.
The Boundary wrapper method can be viewed as a light-weight wa y to do privacy analysis that
pays only for the “uncertainty” of the response distributio nA(D). There are more elaborate (and
often more complex computationally) methods based on smoot h sensitivity (the stability of A(D)
to changes in D) [NRS07 ,DL09,TS13].
Probability oracle vs. Blackbox access The boundary-wrapper method assumes that the
probability of the most dominant outcome in the distributio nA(D), when it is large enough, is
available to the wrapper. For some algorithms, these values are readily available, for example, the
Exponential Mechanism [ MT07 ] or when applying known noise distributions for AboveThreshold ,
BetweenThresholds , and Report-Noise-Max [ DR19 ]. In principle, the probability can always be
computed (without incurring privacy cost) but sometimes th is can be ineﬃcient. We propose in
Section Fa boundary-wrapping method that only uses blackbox samplin g access to the distribution
A(D).
At a very high level, we show that one can run an (ε,0)-DP algorithmAtwice and observe both
outcomes. Then, denote by Ythe range of the algorithm A. We can show that E={(y,y′) :y/ne}a⊔ionslash=
y′}⊆Y×Y is anΩ(1)-target of this procedure. That is, if the analyst observes t he same outcome
twice, she learns the outcome “for free”. If the two outcomes are diﬀerent, the analyst pays O(ε)of
privacy budget, but she will be able to access both outcomes, which is potentially more informative
than a single execution of the algorithm.
2.7.1 Applications to Private Learning using Non-privacy- preserving Models
Promising recent approaches to achieve scalable private le arning through training non-private mod-
els include Private Aggregation of Teacher Ensembles (PATE ) [PAE+17,PSM+18] and Model-
Agnostic private learning [ BTGT18 ].
The private dataset Dis partitioned into kpartsD=D1⊔···⊔Dkand a model is trained (non-
privately) on each part. For multi-class classiﬁcation wit hclabels, the trained models can be viewed
as functions{fi:X→[c]}i∈[k]. Note that changing one sample in Dcan only change the training set
of one of the models. To privately label an example xdrawn from a public distribution, we compute
the predictions of all the models {fi(x)}i∈[k]and consider the counts nj=/summationtext
i∈[k]1{fi(x) =j}(the
number of models that gave label jto example x) forj∈[c]. We then privately aggregate to obtain
a privacy-preserving label, for example using the Exponent ial Mechanism [ MT07 ] or Report-Noisy-
Max [ DR19 ,QSZ21 ].
This setup is used to process queries (label examples) until the privacy budget is exceeded. In
PATE, the new privately-labeled examples are used to train a newstudent model (and{fi}are
called teacher models). In these applications, tight privacy analysis is c ritical. Composition over
all queries is too lossy – for O(1)privacy, only allows for O(k2)queries. For tighter analysis, we
seek to replace this with O(k2)“target hits.” These works used a combination of methods inc luding
SVT, smooth sensitivity, distance-to-instability, and pr opose-test-release [ DL09,TS13]. We show
that the TCT toolkit provides streamlined tighter analysis :
(i) The works of [ BTGT18 ,PSM+18] pointed out that if the teacher models are suﬃciently accu-
rate, we expect high agreement nj≫k/2for the ground truth label jon most queries. These
11high-agreement examples are also the more useful ones for tr aining the student model. Moreover,
agreement implies stability and the ﬁne-grained privacy co st (when accounted through the men-
tioned methods) is lower. We propose the following method th at exploits the stability of queries
with agreements: Apply the boundary wrapper (Algorithm 4) on top of the Exponential Mecha-
nism. Then use⊤as our target. Agreement queries, where maxjnj≫k/2(or more ﬁnely, when
h= argmax jnjandnh≫maxj∈[k]\{h}nj) are very unlikely to result in target hits.
(ii) If we expect most queries to be either high agreement maxjnj≫k/2or low agreement
maxjnj≪k/2and would like to avoid privacy charges also with very low agr eement, we can
apply AboveThreshold test tomaxjnj. If above, we apply the exponential mechanism. Otherwise,
we report “Low.” The wrapper applied to the combined algorit hm returns a label in [c], “Low,” or
⊤. Note that “Low” is a dominant outcome with no-agreement que ries (where the actual label is
not useful anyway) and a class label in [c]is a dominant outcome with high agreement. We only
pay privacy for weak agreements.
(iii) [PSM+18] proposed the use of example selection with PATE, suggestin g that examples where
the current student model agrees with teachers are less help ful (and thus should not be selected
for training to avoid privacy cost). Our proposed use of the w rapper reduces privacy cost in case
of any teacher agreement (whether or not the student agrees) . We can enhance that: When we
are at a stage in the training where most students prediction s agree with the teacher, we can use
the student prediction as a prior (using NotPrior targets) to avoid privacy charges when there is
agreement (and even still use the training example if we wish ).
2.8 SVT with individual privacy charging
As a direct application of TCT privacy analysis, we obtain an improved sparse vector technique
that supports ﬁne-grained privacy charging for each item in the dataset.
SVT with individual privacy charging was introduced by Kapl an et al [ KMS21 ]. The input is
a dataset D∈Xnand an online sequence of linear queries that are speciﬁed by predicate and
threshold value pairs (fi,Ti). For each query, the algorithms reports noisy AboveThreshold test
results/summationtext
x∈Dfi(x)/greaterorsimilarT. Compared with the standard SVT, which halts after reportin gτpositive
responses, SVT with ﬁne-grained charging maintains a separ ate budget counter Cxfor each item
x. For each query with a positive response, the algorithm only charges items that contribute to
this query (namely, all the x’s such that fi(x) = 1). Once an item xcontributes to τmeaningful
queries (that is, Cx=τ), it is removed from the data set. This ﬁne-grained privacy c harging allows
one to obtain better utility with the same privacy budget, as demonstrated by several recent works
[KMS21 ,CLN+22a].
Our improved SVT with individual charging is described in Al gorithm 5. We establish the
following privacy guarantee (see Section Hfor details):
Theorem 2.10 (Privacy of Algorithm 5).Assumeε <1. Algorithm 5is(O(/radicalbig
τlog(1/δ)ε,2−Ω(τ)+
δ)-DP for every δ∈(0,1).
Compared with the prior work [ KMS21 ]: Our algorithm uses the “natural” approach of adding
Laplace noise and comparing, i.e., computing ˆfi=/parenleftbig/summationtext
x∈Dfi(x)/parenrightbig
+Lap(1/ε)and testing whether
ˆfi≥T, whereas [ KMS21 ] adds two independent Laplace noises. We support publishin g the ap-
proximate sum ˆfifor “Above-Threshold” without incurring additional priva cy costs. Moreover, our
analysis is signiﬁcantly simpler (few lines instead of seve ral pages) and for the same privacy budget,
we improve the utility (i.e., the additive error) by a log(1/ε)/radicalbig
log(1/δ)factor. Importantly, our
improvement aligns the bounds of SVT with individual privac y charging with those of standard
SVT, bringing the former into the practical regime.
12Algorithm 5: SVT with Individual Privacy Charging
Input: Private data set D∈Xn; privacy budget τ >0; Privacy parameter ε >0.
foreach x∈Ddo
Cx←0 // Initialize a counter for item x
fori= 1,2,...,do // Receive queries
Receive a predicate fi:X→[0,1]and threshold Ti∈R
ˆfi←/parenleftbig/summationtext
x∈Dfi(x)/parenrightbig
+Lap(1/ε) // Add Laplace noise to count
ifˆfi≥Tithen // Compare with threshold
Publish ˆfi
foreach x∈Dsuch that f(x)>0do
Cx←Cx+1
ifCx=τthen
Remove xfromD
else
Publish⊥
Conclusion
We introduced the Target Charging Technique (TCT), a versat ile uniﬁed privacy analysis framework
that is particularly suitable when a sensitive dataset is ac cessed multiple times via diﬀerentially
private algorithms. We provide a toolkit that is suitable fo r multiple natural scenarios, demonstrate
signiﬁcant improvement over prior work for basic tasks such as private testing and one-shot selection,
describe use cases, and list challenges for followup works. TCT is simple with low overhead and we
hope will be adopted in practice.
13References
[BKM+21] Amos Beimel, Haim Kaplan, Yishay Mansour, Kobbi Nissim, T hatchaphol Saranurak,
and Uri Stemmer. Dynamic algorithms against an adaptive adv ersary: Generic con-
structions and lower bounds. CoRR , abs/2111.03980, 2021.
[BSU17] Mark Bun, Thomas Steinke, and Jonathan Ullman. Make Up Your Mind: The Price
of Online Queries in Diﬀerential Privacy , pages 1306–1325. 2017.
[BTGT18] Raef Bassily, Om Thakkar, and Abhradeep Guha Thakurt a. Model-agnostic private
learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman , N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems , volume 31.
Curran Associates, Inc., 2018.
[Che52] H. Chernoﬀ. A measure of the asymptotic eﬃciency for test of a hypothesis based on
the sum of observations. Annals of Math. Statistics , 23:493–509, 1952.
[CLN+22a] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, Mosh e Shechner, and Uri Stem-
mer. On the robustness of countsketch to adaptive inputs. In Proceedings of the 39th
International Conference on Machine Learning (ICML) , 2022.
[CLN+22b] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and U ri Stemmer. Generalized
private selection and testing with high conﬁdence, 2022.
[CLN+22c] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and U ri Stemmer. Õptimal diﬀer-
entially private learning of thresholds and quasi-concave optimization, 2022.
[Coh17] E. Cohen. Hyperloglog hyper extended: Sketches for concave sublinear frequency statis-
tics. In KDD. ACM, 2017. full version: https://arxiv.org/abs/1607.06517 .
[DL09] Cynthia Dwork and Jing Lei. Diﬀerential privacy and r obust statistics. STOC ’09, New
York, NY, USA, 2009. Association for Computing Machinery.
[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and A dam Smith. Calibrating noise
to sensitivity in private data analysis. In TCC, 2006.
[DNR+09] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum , and Salil Vadhan. On
the complexity of diﬀerentially private data release: Eﬃci ent algorithms and hardness
results. In Proceedings of the Forty-First Annual ACM Symposium on Theo ry of Com-
puting , STOC ’09, page 381–390, New York, NY, USA, 2009. Associatio n for Computing
Machinery.
[DR14] Cynthia Dwork and Aaron Roth. The algorithmic founda tions of diﬀerential privacy.
Found. Trends Theor. Comput. Sci. , 9(3–4):211–407, aug 2014.
[DR19] David Durfee and Ryan M. Rogers. Practical diﬀerenti ally private top-k selection with
pay-what-you-get composition. In Hanna M. Wallach, Hugo La rochelle, Alina Beygelz-
imer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, e ditors, Advances in
Neural Information Processing Systems 32: Annual Conferen ce on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada , pages 3527–3537, 2019.
14[DRV10] Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and diﬀerential
privacy. In 51th Annual IEEE Symposium on Foundations of Computer Scien ce, FOCS
2010, October 23-26, 2010, Las Vegas, Nevada, USA , pages 51–60. IEEE Computer
Society, 2010.
[FFGM07] P. Flajolet, E. Fusy, O. Gandouet, and F. Meunier. H yperloglog: The analysis of a near-
optimal cardinality estimation algorithm. In Analysis of Algorithms (AofA) . DMTCS,
2007.
[FM85] P. Flajolet and G. N. Martin. Probabilistic counting algorithms for data base applica-
tions. Journal of Computer and System Sciences , 31:182–209, 1985.
[FS10] Arik Friedman and Assaf Schuster. Data mining with di ﬀerential privacy. In Proceedings
of the 16th ACM SIGKDD International Conference on Knowledg e Discovery and Data
Mining , KDD ’10, page 493–502, New York, NY, USA, 2010. Association for Computing
Machinery.
[HKM+20] Avinatan Hassidim, Haim Kaplan, Yishay Mansour, Yossi M atias, and Uri Stemmer.
Adversarially robust streaming algorithms via diﬀerentia l privacy. In Annual Conference
on Advances in Neural Information Processing Systems (Neur IPS), 2020.
[HR10] Moritz Hardt and Guy N. Rothblum. A multiplicative we ights mechanism for privacy-
preserving data analysis. In 51th Annual IEEE Symposium on Foundations of Computer
Science, FOCS 2010, October 23-26, 2010, Las Vegas, Nevada, USA, pages 61–70. IEEE
Computer Society, 2010.
[KMS21] Haim Kaplan, Yishay Mansour, and Uri Stemmer. The sp arse vector technique, revis-
ited. In Mikhail Belkin and Samory Kpotufe, editors, Conference on Learning Theory,
COLT 2021, 15-19 August 2021, Boulder, Colorado, USA , volume 134 of Proceedings
of Machine Learning Research , pages 2747–2776. PMLR, 2021.
[KOV15] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. Th e composition theorem for dif-
ferential privacy. In Proceedings of the 32nd International Conference on Intern ational
Conference on Machine Learning - Volume 37 , ICML’15, page 1376–1385. JMLR.org,
2015.
[LSL17] Min Lyu, Dong Su, and Ninghui Li. Understanding the s parse vector technique for
diﬀerential privacy. Proc. VLDB Endow. , 10(6):637–648, 2017.
[LT19] Jingcheng Liu and Kunal Talwar. Private selection fr om private candidates. In Moses
Charikar and Edith Cohen, editors, Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, US A, June 23-26,
2019, pages 298–309. ACM, 2019.
[MM09] Frank McSherry and Ilya Mironov. Diﬀerentially priv ate recommender systems: Build-
ing privacy into the netﬂix prize contenders. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and Data M ining, KDD ’09, page
627–636, New York, NY, USA, 2009. Association for Computing Machinery.
[MT07] Frank McSherry and Kunal Talwar. Mechanism design vi a diﬀerential privacy. In 48th
Annual IEEE Symposium on Foundations of Computer Science (F OCS 2007), October
20-23, 2007, Providence, RI, USA, Proceedings , pages 94–103. IEEE Computer Society,
2007.
15[NRS07] Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling
in private data analysis. In Proceedings of the Thirty-Ninth Annual ACM Symposium on
Theory of Computing , STOC ’07, page 75–84, New York, NY, USA, 2007. Association
for Computing Machinery.
[PAE+17] Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian J . Goodfellow, and Kunal Tal-
war. Semi-supervised knowledge transfer for deep learning from private training data. In
5th International Conference on Learning Representations , ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017.
[PS22] Nicolas Papernot and Thomas Steinke. Hyperparamete r tuning with Rényi diﬀerential
privacy. In The Tenth International Conference on Learning Representa tions, ICLR
2022, Virtual Event, April 25-29, 2022 . OpenReview.net, 2022.
[PSM+18] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Rag hunathan, Kunal Talwar,
and Úlfar Erlingsson. Scalable private learning with PATE. In6th International Con-
ference on Learning Representations, ICLR 2018, Vancouver , BC, Canada, April 30 -
May 3, 2018, Conference Track Proceedings . OpenReview.net, 2018.
[QSZ21] Gang Qiao, Weijie J. Su, and Li Zhang. Oneshot diﬀere ntially private top-k selection. In
Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference
on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Eve nt, volume 139 of
Proceedings of Machine Learning Research , pages 8672–8681. PMLR, 2021.
[RR10] Aaron Roth and Tim Roughgarden. Interactive privacy via the median mechanism.
In Leonard J. Schulman, editor, Proceedings of the 42nd ACM Symposium on Theory
of Computing, STOC 2010, Cambridge, Massachusetts, USA, 5- 8 June 2010 , pages
765–774. ACM, 2010.
[RRUV16] Ryan Rogers, Aaron Roth, Jonathan Ullman, and Sali l Vadhan. Privacy odometers and
ﬁlters: Pay-as-you-go composition. In Proceedings of the 30th International Conference
on Neural Information Processing Systems , NIPS’16, Red Hook, NY, USA, 2016. Cur-
ran Associates Inc.
[SST20] Adam Smith, Shuang Song, and Abhradeep Thakurta. Th e ﬂajolet-martin sketch itself
preserves diﬀerential privacy: Private counting with mini mal space. In Proceedings of
the 34th International Conference on Neural Information Pr ocessing Systems , NIPS’20,
2020.
[SU17] Thomas Steinke and Jonathan R. Ullman. Tight lower bo unds for diﬀerentially private
selection. In Chris Umans, editor, 58th IEEE Annual Symposium on Foundations of
Computer Science, FOCS 2017, Berkeley, CA, USA, October 15- 17, 2017 , pages 552–
563. IEEE Computer Society, 2017.
[TS13] Abhradeep Guha Thakurta and Adam Smith. Diﬀerential ly private feature selection
via stability arguments, and the robustness of the lasso. In Shai Shalev-Shwartz and
Ingo Steinwart, editors, Proceedings of the 26th Annual Conference on Learning Theor y,
volume 30 of Proceedings of Machine Learning Research , pages 819–850, Princeton, NJ,
USA, 12–14 Jun 2013. PMLR.
16[Vad17] Salil P. Vadhan. The complexity of diﬀerential priv acy. In Yehuda Lindell, editor,
Tutorials on the Foundations of Cryptography , pages 347–450. Springer International
Publishing, 2017.
[ZW22] Yuqing Zhu and Yu-Xiang Wang. Adaptive private-k-se lection with adaptive k and
application to multi-label pate. In Gustau Camps-Valls, Fr ancisco J. R. Ruiz, and
Isabel Valera, editors, Proceedings of The 25th International Conference on Artiﬁc ial
Intelligence and Statistics , volume 151 of Proceedings of Machine Learning Research ,
pages 5622–5635. PMLR, 28–30 Mar 2022.
17A Preliminaries
Notation. We say that a function fover datasets is t-Lipschitz if for any two neighboring datasest
D0,D1, it holds that|f(D1)−f(D0)|≤t. For two reals a,b≥0andε >0, we write a≈εbif
e−εb≤a≤eεb.
For two random variables X0,X1, we say that they are ε-indistinguishable, denoted X0≈εX1,
if their max-divergence and symmetric counterpart are both at most ε. That is, for b∈{0,1},
maxS⊆supp(Xb)ln/bracketleftBig
Pr[Xb∈S]
Pr[X1−b∈S]/bracketrightBig
≤ε.
We similarly say that for δ >0, the random variables are (ε,δ)-indistinguishable, denoted
X0≈ε,δX1, if forb∈{0,1}
max
S⊆supp(Xb)ln/bracketleftbiggPr[Xb∈S]−δ
Pr[X1−b∈S]/bracketrightbigg
≤ε.
For two probability distributions, B0,B1We extend the same notation and write B0≈εB1and
B0≈ε,δB1when this holds for random variables drawn from the respecti ve distributions.
The following relates (ε,0)and(ε,δ)-indistinguishability with δ= 0andδ >0.
Lemma A.1. LetB0,B1be two distributions. Then B0≈ε,δB1if and only if we can express them
as mixtures
Bb≡(1−δ)·Nb+δ·Eb,
whereN0≈εN1.
We treat random variables interchangeably as distribution s, and in particular, for a randomized
algorithmsAand input Dwe useA(D)to denote both the random variable and the distribution.
We say an algorithm Aisε-DP (pure diﬀerential privacy), if for any two neighboring datasets D
andD′,A(D)≈εA(D′). Similarly, we say Ais(ε,δ)-DP (approximate diﬀerential privacy) if for
any two neighboring datasets D,D′, it holds thatA(D)≈ε,δA(D′)[DMNS06 ]. We refer to ε,δas
theprivacy parameters .
Aprivate test is a diﬀerentially private algorithm with Boolean output (sa y in{0,1}).
Remark A.2. The literature in diﬀerential privacy uses diﬀerent deﬁnit ions of neighboring datasets
but in this work the deﬁnition and properties are used in a bla ck-box fashion. TCT, and properties
in these preliminaries, apply with an abstraction.
The following is immediate from Lemma A.1:
Corollary A.3 (Decomposition of an approximate DP Algorithm) .An algorithmAis(ε,δ)-DP if
and only if for any two neighboring datasets D0andD1we can represent each distribution A(Db)
(b∈{0,1}) as a mixture
A(Db)≡(1−δ)·Nb+δ·Eb,
whereN0≈εN1.
Diﬀerential privacy satisﬁes the post-processing propert y (post-processing of the output of a
private algorithm remains private with the same parameter v alues) and also has nice composition
theorems:
Lemma A.4 (DP composition [ DMNS06 ,DRV10 ]).An interactive sequence of rexecutions of
ε-DP algorithms satisﬁes (ε′,δ)-DP for
18•ε′=rεandδ= 0by basic composition [ DMNS06 ], or
•for anyδ >0,
ε′=1
2rε2+ε/radicalbig
2rlog(1/δ).
by advanced composition [ DRV10 ].
A.1 Simulation-based privacy analysis
Privacy analysis of an algorithm Avia simulations is performed by simulating the original alg orithm
Aon two neighboring datasets D0,D1. The simulator does not know which of the datasets is the
actual input (but knows everything about the datasets). Ano ther entity called the "data holder" has
the 1-bit information b∈{0,1}on which dataset it is. We perform privacy analysis with resp ect to
what the holder discloses to the simulator regarding the pri vate bitb(taking the maximum over all
choices of D0,D1). The privacy analysis is worst case over the choices of two n eighboring datasets.
This is equivalent to performing privacy analysis for A.
Lemma A.5 (Simulation-based privacy analysis) .[CLN+22c] LetAbe an algorithm whose input is
a dataset. If there exist a pair of interactive algorithms SandHsatisfying the following 2 properties,
then algorithmAis(ε,δ)-DP.
1. For every two neighboring datasets D0,D1and for every bit b∈{0,1}it holds that
/parenleftbig
S(D0,D1)↔H(D0,D1,b)/parenrightbig
≡A(Db).
Here/parenleftbig
S(D0,D1)↔H(D0,D1,b)/parenrightbig
denotes the outcome of Safter interacting with H.
2. Algorithm His(ε,δ)-DP w.r.t. the input bit b.
A.2 Privacy Analysis with Failure Events
Privacy analysis of a randomized algorithm Ausing designated failure events is as follows:
1. Designate some runs of the algorithm as failure events .
2. Compute an upper bound on the maximum probability, over da tasetsD, of a transcript with
a failure designation.
3. Analyse the privacy of the interaction transcript condit ioned on no failure designation.
Note that the failure designation is only used for the purpos e of analysis. The output on failure
runs is not restricted (e.g., could be the dataset D)
Lemma A.6 (Privacy analysis with privacy failure events) .Consider privacy analysis of Awith
failure events. If the probability of a failure event is boun ded byδ∗∈[0,1]and the transcript
conditioned on non-failure is (ε′,δ′)-DP then the algorithm Ais(ε,δ+δ∗)-DP.
Proof. LetD0andD1be neighboring datasets. From our assumptions, for b∈{0,1}, we can
representA(Db)as the mixtureA(Db)≡(1−δb)·Zb+δb·Fb, whereZ0≈ε′,δ′Z1, andδ(b)≤δ∗.
From Lemma A.1, we have Zb≡(1−δ′)·Nb+δ′·Eb, whereN0≈ε′N1.
19Then
A(Db) = (1−δ(b))·Zb+δ(b)·F(b)
= (1−δ∗)·Zb+(δ∗−δ(b))·Zb+δ(b)·Fb
= (1−δ∗)·Zb+δ∗·/parenleftBig
(1−δ(b)/δ∗)·Zb+δ(b)·Fb/parenrightBig
= (1−δ∗)(1−δ′)·Nb+(1−δ∗)δ′·Eb+δ∗·/parenleftBig
(1−δ(b)/δ∗)·Zb+δ(b)·Fb/parenrightBig
= (1−δ∗−δ′)·Nb+δ′δ∗·N+(1−δ∗)δ′·Eb+δ∗·/parenleftBig
(1−δ(b)/δ∗)·Zb+δ(b)·Fb/parenrightBig
The claim follows from Corollary A.3.
Using simulation-based privacy analysis we can treat an int eractive sequence of approximate-
DP algorithms (optionally with designated failure events) as a respective interactive sequence of
pure-DP algorithms where the δparameters are anlaysed through failure events. This simpl iﬁes
analysis:
We can relate the privacy of a composition of approximate-DP algorithms to that of a compo-
sition of corresponding pure-DP algorithms:
Corollary A.7 (Composition of approximate-DP algorithms) .An interactive sequence of (εi,δi)-
DP algorithms ( i∈[k]) has privacy parameter values (ε′,δ′+/summationtextk
i=1δi), where(ε′,δ′)are privacy
parameter values of a composition of pure (εi,0)-DP algorithms i∈[k].
Proof. We perform simulation-based analysis. Fix two neighboring datasets D0,D1. For an (εi,δi)-
DP algorithm, we can consider the mixtures as in Corollary A.3. We draw c∼Ber(δi)and ifc= 1
designate the output as failure and return r∼E(b). Otherwise, we return r∼N(b). The overall
failure probability is bounded by 1−/producttext
i(1−δi)≤/summationtext
iδi. The output conditioned on non-failure is
a composition of (εi,0)-DP algorithms ( i∈[k]). The claim follows using Lemma A.6.
B The Target-Charging Technique
We extend the deﬁnition of q-targets (Deﬁnition 2.1) so that it applies with approximate DP algo-
rithms:
Deﬁnition B.1 (q-target with (ε,δ)of a pair of distributions) .LetA → Y be a randomized
algorithm. Let Z0andZ1be two distributions with support Y. We say that⊤⊆Y is aq-target of
(Z0,Z1)with(ε,δ), whereε >0andδ∈[0,1), if there exist p∈[0,1]and ﬁve distributions C,Bb,
andEb(forb∈{0,1}) such that Z0andZ1can be written as the mixtures
Z0≡(1−δ)·(p·C+(1−p)·B0)+δ·E0
Z1≡(1−δ)·(p·C+(1−p)·B1)+δ·E1
whereB0≈εB1, andmin(Pr[B0∈⊤],Pr[B1∈⊤])≥q.
Deﬁnition B.2 (q-target with (ε,δ)of a randomized algorithm) .LetA→Y be a randomized
algorithm. We say that ⊤⊆Y is aq-target ofAwith(ε,δ), whereε >0andδ∈[0,1), if for any
pairD0,D1of neighboring datasets, ⊤is aq-target with (ε,δ)ofA(D0)andA(D1).
We can relate privacy of an algorithms or indistinguishabil ity of two distributions to existence
ofq-targets:
20Lemma B.3. (i) If(Z0,Z1)have aq-target with (ε,δ)thenZ0≈ε,δZ1. Conversely, if Z0≈ε,δZ1
then(Z0,Z1)have a1-target with (ε,δ)(the full support is a 1-target).
(ii) If an algorithm Ahas aq-target with (ε,δ)thenAis(ε,δ)-DP. Conversely, if an algorithm
Ais(ε,δ)-DP then it has a 1-target (the set Y) with(ε,δ).
Proof. If two distributions B0,B1have aq-target with (ε,δ)than from Deﬁnition B.1they can be
represented as mixtures. Now observe the if B0≈εB1then the mixtures also satisfy p·C+(1−
p)·B0≈εp·C+(1−p)·B0. Using Lemma A.1, we getZ0≈ε,δZ1.
For (ii) considerAand two neighboring datasets D0andD1. Using Deﬁnition B.2and applying
the argument above we obtain A(D0)≈ε,δA(D1). The claim follows using Corollary A.3.
Now for the converse. If Z0≈ε,δZ1then consider the decomposition as in Lemma A.1. Now we
setp= 0andBb←Nbto obtain the claim with q= 1and the target being the full support.
For (ii), ifA→Y is(ε,δ)-DP then consider neighboring {D0,D1}. We haveA(D0)≈ε,δA(D1).
We proceed as with the distributions.
Algorithm 6is an extension of Algorithm 1that permits calls to approximate DP algorithms.
The extension also inputs a bound τon the number of target hits and a bound τδon the cummulative
δparameter values of the algorithms that were called. We appl y adaptively a sequence of (ε,δ)-DP
algorithms with speciﬁed q-targets to the input data set Dand publish the results. We halt when
the ﬁrst of the following happens (1) the respective target s ets are hit for a speciﬁed τnumber of
times (2) the accumulated δ-values exceed the speciﬁed limit τδ.
Algorithm 6: Target Charging with Approximate DP
Input: DatasetD={x1,...,x n}∈Xn. Integer τ≥1(Upper limit on the number of target hits).
τδ≥0(upper limit on cumulative δparameter). Fraction q∈[0,1].
C←0,Cδ←0 // Initialize target hit and failure counters
fori= 1,...do // Main loop
Receive (Ai,⊤i)whereAiis an(ε,δi)-DP mechanism, and ⊤iis aq-target with (ε,δi)forA
r←Ai(D)
ifCδ+δi> τδthen Halt
Cδ←Cδ+δ // TCT charge for δi
Publish r
ifr∈⊤then // TCT Charge for a q-target hit with ε
C←C+1
ifC=τthen Halt
The privacy cost of Target-Charging is as follows (This is a p recise and more general statement
of Lemma 2.2):
Theorem B.4 (Privacy of Target-Charging) .Algorithm 6satisﬁes the following approximate DP
privacy bounds:
/parenleftbigg
(1+α)τ
qε,Cδ+δ∗(τ,α)/parenrightbigg
, for anyα >0;
/parenleftbigg1
2(1+α)τ
qε2+ε/radicalbigg
(1+α)τ
qlog(1/δ),δ+Cδ+δ∗(τ,α)/parenrightbigg
, for anyδ >0,α >0.
whereδ∗(τ,α)≤e−α2
2(1+α)τandCδ≤τδis as computed by the algorithm.
21Algorithm 7: Simulation of Target Charging
Input: Two neighboring datasets D0,D1, private b∈{0,1},τ∈N,τδ∈R≥0,q∈[0,1],α >0.
C←0,Cδ←0,h←0 // Initialize; his a counter on the number of non-fail calls to data holder
fori= 1,...do // Main loop
Receive (Ai,⊤i)whereAiis an(ε,δi)-DP mechanism, and ⊤iis aq-target with (ε,δi)forA
ifCδ+δi> τδthen Halt
Cδ←Cδ+δ
Letp∈[0,1],C,B0≈εB1, andEb(forb∈{0,1}) such that
A(Db)≡(1−δ)·(p·C+(1−p)·Bb)+δ·Eb// By Definition B.2
ifBer(δ)≡1then // Non-private Data Holder call with Failure
Fail
Publish r∼Eb
else
ifBer(p)≡1then
Publish r∼C // No access to data holder
else
Publish r∼Bb//ε-DP Data Holder Call
h←h+1 // counter of ε-private data holder calls
ifh >(1+α)τ/qthen // Number of Holder calls exceeded limit
Fail
ifr∈⊤then // outcome is a target hit
C←C+1
ifC=τthen Halt
Proof. We apply the simulation-based privacy analysis in Lemma A.5and use privacy analysis with
failure events (Lemma A.6).
The simulation is described in Algorithm 7. Fix two neighboring data sets D0andD1. The
simulator initializes the target hit counter C←0and the cumulative δ-values tracker Cδ←0. For
i≥1it proceeds as follows. It receives (Ai,⊤i)whereAiis(ε,δi)-DP. IfCδ+δi> τδit halts. Since
⊤iis aq-target forAi, there are p,C,B0,B1,E0andE1as in Deﬁnition B.2. The simulator ﬂips
a biased coin c′∼Ber(δ). Ifc′= 1it outputs r∼Eband the execution is designated as Fail. In
this case there is an interaction with the data holder but als o a failure designation. The simulator
ﬂips a biased coin c∼Ber(p). Ifc= 1, then the simulator publishes a sample r∼C(this does not
require an interaction with the data holder). Otherwise, th e data holder is called. The data holder
publishes r∼Bb. We track the number hof calls to the data holder. If hexceeds(1+α)τ/q, we
designate the execution as Fail. Ifr∈⊤ithenCis incremented. If C=τ, the algorithm halts.
The correctness of the simulation (faithfully simulating A lgorithm 1on the dataset Db) is
straightforward. We analyse the privacy cost. We will show t hat
(i) the simulation designated a failure with probability at mostCδ+δ∗(τ,α).
(ii) Conditioned on no failure designation, the simulation performed at most r= (1+α)τ
qadaptive
calls to(ε,0)-DP algorithms
Observe that (ii) is immediate from the simulation declarin g failure when h > r. We will establish
(i) below.
The statement of the Theorem follows from Lemma A.6and when applying the DP composition
bounds (Lemma A.4). The ﬁrst bounds follow using basic composition and the sec ond follow using
advanced composition [ DRV10 ].
22This analysis yields the claimed privacy bounds with respec t to the private bit b. From
Lemma A.5this is the privacy cost of the algorithm.
It remains to show bound the failure probability. There are t wo ways in which a failure can occur.
The ﬁrst is on each call, with probability δi. This probability is bounded by 1−/producttext
iδi≤/summationtext
iδi≤Cδ.
The second is when the number hof private accesses to the data holder exceeds the limit. We s how
that the probability that the algorithm halts with failure d ue to that is at most δ∗.
We consider a process that continues until τcharges are made. The privacy cost of the simulation
(with respect to the private bit b) depends on the number of times that the data holder is called .
LetXbe the random variable that is the number of calls to the data h older. Each call is ε-DP with
respect to the private b. In each call, there is probability at least qfor a “charge” (increment of C).
A failure is the event that the number of calls to data holder e xceeds(1+α)τ/qbeforeτcharges
are made. We show that this occurs with probability at most δ∗(τ,α):
Pr/bracketleftbigg
X >(1+α)τ
q/bracketrightbigg
≤δ∗(τ,α). (1)
To establish ( 1), we ﬁrst observe that the distribution of the random variab leXis dominated by
a random variable X′that corresponds to a process of drawing i.i.d. Ber(q)until we get τsuccesses
(Domination means that for all m,Pr[X′> m]≥Pr[X > m]). Therefore, it suﬃces to establish
that
Pr/bracketleftbigg
X′>(1+α)τ
q/bracketrightbigg
≤δ∗(τ,α).
LetYbe the random variable that is a sum of m= 1+/floorleftBig
(1+α)τ
q/floorrightBig
i.i.d.Ber(q)random variables.
Note that
Pr/bracketleftbigg
X′>(1+α)τ
q/bracketrightbigg
= Pr[Y < τ].
We bound Pr[Y < τ]using multiplicative Chernoﬀ bounds [ Che52 ]6. The expectation is µ=mq
and we bound the probability that the sum of Bernoulli random v ariables is below1
1+αµ= (1−
α
1+α)µ. Using the simpler form of the bounds we get using µ=mq≥(1+α)τ
Pr[Y < τ] = Pr[Y <(1−α
1+α)µ]≤e−α2
2(1+α)2µ≤e−α2
2(1+α)τ.
Remark B.5 (Number of target hits) .The TCT privacy analysis has a tradeoﬀ between the ﬁnal
“ε” and “δ” privacy parameters. There is multiplicative factor of (1 +α)(√1+αwith advanced
composition) on the “ ε” privacy parameter. But when we use a smaller αwe need a larger value of
τto keep the “ δ” privacy parameter small. For a given α,δ∗>0, we can calculate a bound on the
smallest value of τthat works. We get
τ≥21+α
α2·ln(1/δ∗) (simpliﬁed Chernoﬀ)
τ≥1
(1+α)ln/parenleftbig
eα/(1+α)(1+α)−1/(1+α)/parenrightbig·ln(1/δ∗) (raw Chernoﬀ)
Forα= 0.5we getτ >10.6·ln(1/δ∗). Forα= 1we getτ >3.26·ln(1/δ∗). Forα= 5we get
τ >0.31·ln(1/δ∗).
6Bound can be tightened when using precise tail probability v alues.
23Remark B.6 (Mix-and-match TCT) .TCT analysis can be extended to the case where we use
algorithms with varied privacy guarantees εiand varied qivalues.7In this case the privacy cost
depends on/summationtext
i|Ai(D)∈⊤iεi
qi. The analysis relies on tail bounds on the sum of random varia bles, is
more complex. Varied εvalues means the random variables have diﬀerent size suppor ts. A simple
coarse bound is according to the largest support, which allo ws us to use a simple counter for target
hits, but may be lossy with respect to precise bounds. The dis cussion concerns the (analytical or
numerical) derivation of tail bounds is non-speciﬁc to TCT a nd is tangential to our contribution.
B.1 Multi-Target TCT
Algorithm 8: Multi-Target Charging
Input: DatasetD={x1,...,x n}∈Xn. Integer τ≥1(charging limit). Fraction q∈[0,1],
k≥1(number of targets).
fori∈[k]doCi←0 // Initialize charge counters
whilemini∈[k]Ci< τdo // Main loop
Receive (A,(⊤i)i∈[k])whereAis anε-DP mechanism, and ⊤iis aq-target forA
r←A(D)
Publish r
fori∈[k]do
ifr∈⊤ithenCi←Ci+1 // outcome is in q-target ⊤i
Proof of Lemma 2.7(Privacy of multi-Target TCT).8Let(ε,δ)be the privacy bounds for Mithat
is single-target TCT with (Ai,⊤i). LetMbe thek-target algorithm. Let ⊤j
ibe theith target in
stepj.
We say that an outcome sequence R= (rj)h
j=1∈Ris valid for i∈[k]if and only ifMiwould
halt with this output sequence, that is,/summationtexth
j=11{rj∈⊤j
i}=τandrh∈⊤h
i. We deﬁne G(R)⊂[k]
to be all i∈[k]for which Ris valid.
Consider a set of sequences H. Partition Hintok+1setsHiso thatH0={R∈H|G(R) =∅}
andHimay only include R∈Hfor which i∈G(R). That is, H0contains all sequences that are
not valid for any iandHimay contain only sequences that are valid for i.
Pr[M(D)∈H] =k/summationdisplay
i=1Pr[M(D)∈Hi] =k/summationdisplay
i=1Pr[Mi(D)∈Hi]
≤k/summationdisplay
i=1/parenleftbig
eε·Pr[Mi(D′)∈Hi]+δ/parenrightbig
=eε·k/summationdisplay
i=1Pr[Mi(D′)∈Hi]+k·δ
=eεPr[M(D′)∈H]+k·δ.
7One of our applications of revise calls to conditional relea se (see Section Dapplies TCT with both ε-DP and
2ε-DP algorithms even for base ε-DP algorithm)
8We note that the claim generally holds for online privacy ana lysis with the best of multiple methods. We provide
a proof speciﬁc to multi-target charging below.
24C Properties of NotPrior targets
Recall that a NotPrior target of an (ε,δ)-DP algorithm is speciﬁed by any potential outcome (of
our choice) that we denote by ⊥. The NotPrior target is the set of all outcomes except ⊥. In this
Section we prove (a more general statement of) Lemma 2.3:
Lemma C.1 (Property of a NotPrior target) .LetM:X→Y∪{⊥} , where⊥/ne}a⊔ionslash∈Y , be an
(ε,δ)-DP algorithm. Then the set of outcomes Yconstitutes an1
eε+1-target with (ε,δ)forM.
We will use the following lemma:
Lemma C.2. If two distributions Z0,Z1with supportY∪{⊥} satisfyZ0≈εZ1thenYconstitutes
an1
eε+1-target with (ε,0)for(Z0,Z1).
Proof of Lemma 2.3.From Deﬁnition B.2, it suﬃces to show that for any two neighboring datasets,
D0andD1, the setYis an1
eε+1-target with (ε,δ)for(M(D0),M(D1))(as in Deﬁnition B.1).
Consider two neighboring datasets. We have M(D0)≈ε,δM(D1). Using Lemma A.1, for
b∈{0,1}we can have
M(Db) = (1−δ)·Nb+δ·Eb, (2)
whereN0≈εN1. From Lemma C.2,Yis a1
eε+1-target with (ε,0)for(N0,N1). From Deﬁnition B.1
and (2), this means that Yis a1
eε+1-target with (ε,δ)for(M(D0),M(D1)).
C.1 Proof of Lemma C.2
We ﬁrst prove Lemma C.2for the special case of private testing (when the support is {0,1}):
Lemma C.3 (target for private testing) .LetZ0andZ1with support{0,1}satisfyZ0≈εZ1Then
⊤={1}(or⊤={0}) is an1
eε+1-target with (ε,0)for(Z0,Z1).
Proof. We show that Deﬁnition B.1is satisﬁed with⊤={1},q=1
eε+1and(ε,0), andZ0,Z1.
π= Pr[Z0∈⊤]
π′= Pr[Z1∈⊤]
be the probabilities of ⊤outcome in Z0andZ1respectively. Assume without loss of generality
(otherwise we switch the roles of Z0andZ1) thatπ′≥π. Ifπ≥1
eε+1, the choice of p= 0and
Bb=Zb(and any C) trivially satisﬁes the conditions of Deﬁnition 2.1. Generally, (also for all
π <1
eε+1):
• Let
p= 1−π′eε−π
eε−1.
Note that since Z0≈εZ1it follows that π′≈επand(1−π′)≈ε(1−π)and therefore p∈[0,1]
for any applicable 0≤π≤π′≤1.
• LetCbe the distribution with point mass on ⊥={0}.
• LetB0=Ber(1−π′−π
π′−e−επ) =Ber(π−πe−ε
π′−e−επ)
• LetB1=Ber(1−π′−π
eεπ′−π) =Ber(eεπ′−π′
eεπ′−π)
25We show that this choice satisﬁes Deﬁnition 2.1withq=1
eε+1.
• We show that for both b∈{0,1}.Zb≡p·C+ (1−p)·Bb: It suﬃces to show that the
probability of⊥is the same for the distributions on both sides. For b= 0, the probability of
⊥in the right hand side distribution is
p+(1−p)·π′−π
π′−e−επ= 1−π′eε−π
eε−1+π′eε−π
eε−1·π′−π
π′−e−επ= 1−π .
Forb= 1, the probability is
p+(1−p)·π′−π
eεπ′−π= 1−π′eε−π
eε−1+π′eε−π
eε−1·π′−π
eεπ′−π
= 1−π′eε−π
eε−1/parenleftbigg
1−π′−π
eεπ′−π/parenrightbigg
= 1−π′eε−π
eε−1·eεπ′−π−π′+π
eεπ′−π= 1−π′.
• We show that for b∈{0,1},Pr[Bb∈⊤]≥1
eε+1.
Pr[B0∈⊤] =π−e−επ
π′−e−επ
≥π−e−επ
eεπ−e−επ=eε−1
e2ε−1=1
eε+1.
Pr[B1∈⊤] =π′(eε−1)
π′eε−π
≥π(eε−1)
πe2ε−π=eε−1
e2ε−1=1
eε+1
Note that the inequalities are tight when π′=π(and are tighter when π′is closer to π). This
means that our selected qis the largest possible that satisﬁes the conditions for the target
being⊤.
• We show that B0andB1areε-indistinguishable, that is
Ber(1−π′−π
π′−e−επ)≈εBer(1−π′−π
eεπ′−π).
Recall that Ber(a)≈εBer(b)if and only if a≈εband(1−a)≈ε(1−b). First note that
e−ε·π′−π
π′−e−επ=π′−π
eεπ′−π
Hence
π′−π
π′−e−επ≈επ′−π
eεπ′−π.
It also holds that
1≤π−e−επ
π′−e−επ
π′(1−e−ε)
π′−e−επ=π′
π≤eε.
26Proof of Lemma C.2.The proof is very similar to that of Lemma C.3, with a few additional details
since⊤=Ycan have more than one element (recall that ⊥is a single element).
Assume (otherwise we switch roles) that Pr[Z0=⊥]≥Pr[Z1=⊥]. Let
π= Pr[Z0∈Y]
π′= Pr[Z1∈Y].
Note that π′≥π.
We choose p,C,B0,B1as follows. Note that when π≥1
eε+1, then the choice of p= 0and
Bb=Zbsatisﬁes the conditions. Generally,
• Let
p= 1−π′eε−π
eε−1.
• LetCbe the distribution with point mass on ⊥.
• LetB0be⊥with probabilityπ′−π
π′−e−επand otherwise (with probabilityπ−πe−ε
π′−e−επ) beZ0condi-
tioned on the outcome being in Y.
• LetB1be⊥with probabilityπ′−π
eεπ′−πand otherwise (with probabilityeεπ′−π′
eεπ′−π) beZ1conditioned
on the outcome being in Y.
It remains to show that these choices satisfy Deﬁnition 2.1:
The argument for Pr[Bb∈Y]≥eε−1
e2ε−1is identical to Lemma C.3(withY=⊤).
We next verify that for b∈{0,1}:Zb≡p·C+(1−p)·Bb. The argument for the probability
of⊥is identical to Lemma C.3. The argument for y∈Yfollows from the probability of being in Y
being the same and that proportions are maintained.
Forb= 0, the probability of y∈Yin the right hand side distribution is
(1−p)·π−πe−ε
π′−e−επ·Pr[Z0=y]
Pr[Z0∈Y]=π·Pr[Z0=y]
Pr[Z0∈Y]= Pr[Z0=y].
Forb= 1, the probability of y∈Yin the right hand side distribution is
(1−p)·eεπ′−π′
eεπ′−π·Pr[Z1=y]
Pr[Z1∈Y]=π′·Pr[Z1=y]
Pr[Z1∈Y]
= Pr[Z1=y].
Finally, we verify that B0andB1areε-indistinguishable. Let W⊂Y. We have
Pr[B0∈W] =π(1−e−ε)
π′−e−επ·Pr[Z0∈W]
π=eε−1
π′eε−πPr[Z0∈W]
Pr[B1∈W] =π′(eε−1)
eεπ′−π·Pr[Z1∈W]
π′=eε−1
π′eε−πPr[Z1∈W].
Therefore
Pr[B0∈W]
Pr[B1∈W]=Pr[Z0∈W]
Pr[Z1∈W]
and we use Z0≈εZ1. The case of W=⊥is identical to the proof of Lemma C.3. The case⊥∈W
follows.
27D Conditional Release with Revisions
In this section we analyze an extension to conditional relea se that allows for revision calls to be made
with respect to previous computations. This extension was presented in Section 2.3and described
in Algorithm 2. A conditional release applies a private algorithm A→Y with respect to a subset
of outcomes⊤⊂Y . It draws y∼A(D)and returns yify∈⊤and⊥otherwise. Each revise calls
eﬀectively expands the target to ⊤h∪⊤′, when⊤his the prior target and ⊤′a disjoint extension.
If the (previously) computed result hits the expanded targe t (y∈⊤′), the value yis reported and
charged. Otherwise, additional revise calls can be perform ed. The revise calls can be interleaved
with other TCT computations at any point in the interaction.
D.1 Preliminaries
For a distribution Zwith supportYandW⊂Ywe denote by ZWthe distribution with support
W∪{⊥} where outcomes not in Ware “replaced” by⊥. That is, for y∈W,Pr[ZW=y] := Pr[Z=y]
andPr[ZW=⊥] := Pr[Z/ne}a⊔ionslash∈W].
For a distribution Zwith supportYandW⊂Ywe denote by Z|Wtheconditional distribution
ofZonW. That is, for y∈W,Pr[(Z|W) =y] := Pr[Z=y]/Pr[Z∈W].
Lemma D.1. IfB0≈ε,δB1thenB0
W≈ε,δB1
W.
Lemma D.2. LetB0,B1be probability distributions with support Ysuch that B0≈εB1. Let
W⊂Y. ThenB0|W≈2εB1|W.
We extend these deﬁnitions to a randomized algorithm A, whereAW(D)has distributionA(D)W
and(A|W)(D)has distributionA(D)|W. The claims in Lemma D.1and Lemma D.2then transfer
to privacy of the algorithms.
D.2 Analysis
To establish correctness, it remains to show that each ConditionalRelease call with an (ε,δ)-DP
algorithmAcan be casted in TCT as a call to an (ε,δ)-DP algorithm with a NotPrior target and
each ReviseCR call cap be casted as a call to an 2ε-DP algorithm with a NotPrior target.
Proof of Lemma 2.5.The claim for ConditionalRelease was established in Lemma 2.4: Condi-
tional release ConditionalRelease (A,⊤)calls the algorithm A⊤with target⊤. From Lemma D.1,
A⊤is(ε,δ)-DP whenAis(ε,δ)-DP.⊤constitutes a NotPrior target forA⊤with respect to prior
⊥.
We next consider revision calls as described in Algorithm 2. We ﬁrst consider the case of a
pure-DPA(δ= 0).
When ConditionalRelease publishes⊥, the internally stored value rhconditioned on published
⊥is a sample from the conditional distribution A(D)|¬⊤.
We will show by induction that this remains true after ReviseCR calls, that is the distribution
ofrhconditioned on⊥being returned in all previous calls is A(D)|¬⊤hwhere⊤his the current
expanded target.
AnReviseCR call with respect to current target ⊤hand extension⊤′can be equivalently framed
as drawing r∼A(D)|¬⊤h. From Lemma D.2, ifAisε-DP thenA|¬⊤ his2ε-DP. Ifr∈⊤′we
publish it and otherwise we publish ⊥. This is a conditional release computation with respect to
the2ε-DP algorithmA|¬⊤ hand the target⊤′. Equivalently, it is a call to the 2ε-DP algorithm
(A|¬⊤ h)⊤′with a NotPrior target⊤′.
28Following the ReviseCR call, the conditional distribution of rhconditioned on⊥returned in the
previous calls isA(D)|¬(⊤h∪⊤′)as claimed. We then update ⊤h←⊤h∪⊤′.
It remains to handle the case δ >0. We consider ReviseCR calls for the case where Ais(ε,δ)-
DP (approximate DP). In this case, we want to show that we char ge for the δvalue once, only
on the original ConditionalRelease call. We apply the simulation-based analysis in the proof of
Theorem B.4with two ﬁxed neighboring datasets. Note that this can be vie wed as each call being
with a pair of distributions with an appropriate q-target (that in our case is always a NotPrior
target).
The ﬁrst ConditionalRelease call uses the distributions A(D0)andA(D1). From Lemma A.1
they can be expressed as respective mixtures of pure N0≈εN1part (with probability 1−δ) and
non-private parts. The non-private draw is designated fail ure with probability δ. Eﬀectively, the
call in the simulation is then applied to the pair (N0
⊤,N1
⊤)with target⊤.
A followup ReviseCR call is with respect to the previous target ⊤hand target extension ⊤′.
The call is with the distributions (Nb|¬⊤h)⊤′that using Lemma D.1and Lemma D.2satisfy
(N0|¬⊤h)⊤′≈2ε(N1|¬⊤h)⊤′.
E Boundary Wrapper Analysis
In this section we provide details for the boundary wrapper m ethod including proofs of Lemma 2.8
and Lemma 2.9. For instructive reasons, we ﬁrst consider the special case of private testing and
then outline the extensions to private classiﬁcation.
Algorithm 4when specialized for tests ﬁrst computes π(D) = min{Pr[A(D) = 0],1−Pr[A(D) =
0]}, returns⊤with probability π/(1+π)and otherwise (with probability 1/(1+π)) returnA(D).
Overall, we return the less likely outcome with probability π/(1+π), and the more likely one with
probability (1−π)/(1+π).
Lemma E.1 (Privacy of wrapped test) .If the test is ε-DP then the wrapper test is t(ε)-DP where
t(ε)≤4
3ε.
Proof. Working directly with the deﬁnitions, t(ε)is the maximum of
max
π∈(0,1/2)/vextendsingle/vextendsingle/vextendsingle/vextendsingleln/parenleftbigg1−e−επ
1+e−επ·1+π
1−π/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤4
3ε (3)
max
π∈(0,1/2)/vextendsingle/vextendsingle/vextendsingle/vextendsingleln/parenleftbigge−επ
1+e−επ·1+π
π/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ε (4)
max
π∈(e−ε
2,1
1+eε)/vextendsingle/vextendsingle/vextendsingle/vextendsingleln/parenleftbiggπ
1+π·2−eεπ
eεπ/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ε (5)
max
π∈(e−ε
2,1
1+eε)/vextendsingle/vextendsingle/vextendsingle/vextendsingleln/parenleftbigg1−π
1+π·2−eεπ
1−eεπ/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤4
3ε (6)
max
π∈(e−ε
2,1
1+eε)/vextendsingle/vextendsingle/vextendsingle/vextendsingleln/parenleftbiggπ
1+π·2−eεπ
1−eεπ/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤ε (7)
Inequality ( 3) bounds the ratio change in the probably of the larger probab ility outcome when it
remains the same and ( 4) the ratio change in the probability of the smaller probabil ity outcome
when it remains the same between the neighboring datasets. W hen the less probable outcome
changes between the neighboring datasets it suﬃces to consi der the case where the probability of
the initially less likely outcome changes to eεπ >1/2so thateεπ <1−π, that is the change is
29fromπtoeεπwhereπ∈(e−ε
2,1
1+eε). Inequalities 5and6correspond to this case. The wrapped
probabilities of the ⊤outcome are the same as the less probably outcome in the case t hat it is the
same in the two databases. Inequality 7corresponds to the case when there is change.
We now show that ⊤is a target for the wrapped test.
Lemma E.2 (q-value of the boundary target) .The outcome⊤of a boundary wrapper of an ε-DP
test is aet(ε)−1
2(eε+t(ε)−1)-target.
Proof. Consider two neighboring datasets where the same outcome is less likely for both and π≤π′.
Suppose without loss of generality that 0is the less likely outcome.
The common distribution (C)has point mass on 1.
The distribution B0is a scaled part of M(D0)that includes all 0and⊤outcomes (probability
π/(1+π)each) and probability of ∆et(ε)
et(ε)−1of the1outcomes, where ∆ =2π′
1+π′−2π
1+π.
The distribution B1is a scaled part of M(D1)that includes all 0and⊤outcomes (probability
π′/(1+π′)each) and probability of ∆1
et(ε)−1of the1outcomes.
It is easy to verify that B0≈t(ε)B1and that
1−p=2π′
1+π′+∆1
et(ε)−1=2π
1+π+∆et(ε)
et(ε)−1
=2π′
1+π′et(ε)
et(ε)−1−2π
1+π1
et(ε)−1
=2
et(ε)−1(et(ε)π′
1+π′−π
1+π)
Usingπ
1+π≤π′
1+π′andπ′
1+π′
π
1+π≤eεwe obtain
q≥π
1+π
1−p
=et(ε)−1
2/parenleftBigg
1
et(ε)·π′
1+π′·1+π
π−1/parenrightBigg
≥et(ε)−1
21
et(ε)+ε−1.
Extension to Private Classiﬁcation To extension from Lemma E.1to Lemma 2.8follows by
noting that the same arguments also hold respectively for se ts of outcomes and also cover the case
when there is no dominant outcome and when there is a transiti on between neighboring datasets
from no dominant outcome to a dominant outcome. The extensio n from Lemma E.1to Lemma 2.8
is also straightforward by also noting the cases above (that only make the respective ∆smaller),
and allowing Cto be empty when there is no dominant outcome.
30F Boundary wrapping without a probability oracle
We present a boundary-wrapping method that does not assume a probability oracle. This method
accesses the distribution A(D)in a blackbox fashion.
Lemma F.1. SupposeA:X∗→Yis an(ε,0)-DP algorithm where |Y|<∞. Denote byA◦A
the following algorithm: on input D, independently run Atwice and publish both outcomes. Deﬁne
E:={(y,y′) :y/ne}a⊔ionslash=y′}⊆Y×Y . Then,A◦A is a(2ε,0)-DP algorithm, and Eis af(ε)-target for
A◦A, where
f(ε) = 1−/radicalbig
e2ε/(1+e2ε).
The algorithm behind Lemma F.1is a natural one: we run the given mechanism Atwice, and
pay the privacy cost only when the two outcomes are diﬀerent. Intuitively, if there is a dominant
outcome i∗such that Pr[A(D) =i∗]≈1, then both executions would output i∗with high probability,
in which case there will be no privacy cost.
Proof.A◦A is(2ε,0)-DP by the basic composition theorem. Next, we verify the sec ond claim.
Identify elements of Yas1,2,...,m=|Y|. LetD,D′be two adjacent data sets. For each
i∈[m], let
pi= Pr[A(D) =i], p′
i= Pr[A(D′) =i].
We deﬁne a distribution C. For each i∈[m], deﬁneqito be the largest real such that
p2
i−qi∈[e−2ε(p′
i2−qi),e2ε(p′
i2−qi)].
Then, we deﬁne Cto be a distribution over {(i,i) :i∈[m]}wherePr[C= (i,i)] =qi/summationtext
jqj.
We can then write (A◦A)(D) =α·C+(1−α)·N0and(A◦A)(D′) =α·C+(1−α)·N1,
whereα=/summationtext
iqi, andN0andN1are2ε-indistinguishable.
Next, we consider lower-bounding Pr[N0= (y,y′) :y/ne}a⊔ionslash=y′]. The lower bound of Pr[N0=
(y,y′) :y/ne}a⊔ionslash=y′]will follow from the same argument.
Indeed, we have
Pr[N0= (y,y′) :y/ne}a⊔ionslash=y′]
Pr[N0= (y,y)]=/summationtext
ipi(1−pi)/summationtext
ip2
i−qi.
We claim that
p2
i−qi≤1−p′
i2.
The inequality is trivially true if p2
i≤1−p′
i2. Otherwise, we can observe that for q:=pi2+p′
i2−1>0,
we have pi2−q= 1−p′
i2andp′
i2−q= 1−pi2. Since1−p′
i2∈[e−2ε(1−p2
i),e2ε(1−p2
i)], this
implies that qican only be larger than q.
Since we also trivially have that p2
i−qi≤pi2, we conclude that
Pr[N0= (y,y′) :y/ne}a⊔ionslash=y′]
Pr[N0= (y,y)]≥/summationtext
ipi(1−pi)/summationtext
imin(p2
i,1−p′
i2)≥/summationtext
ipi(1−pi)/summationtext
imin(p2
i,e2ε(1−p2
i)).
Next, it is straightforward to show that, for every p∈[0,1], one has
p(1−p)
min(p2,e2ε(1−p2))= min/parenleftbigg1−p
p,p
e2ε(1+p)/parenrightbigg
≥1−/radicalbig
e2ε/(1+e2ε)/radicalbig
e2ε/(1+e2ε).
Consequently,
Pr[N0= (y,y′) :y/ne}a⊔ionslash=y′] =Pr[N0= (y,y′) :y/ne}a⊔ionslash=y′]
Pr[N0= (y,y′) :y/ne}a⊔ionslash=y′]+Pr[N0= (y,y)]≥1−/radicalbig
e2ε/(1+e2ε),
as desired.
31Remark F.2. For a typical use case where ǫ= 0.1, we have f(ε)≈0.258. Then, by applying
Theorem B.4, on average we pay ≈8εprivacy cost for each target hit. Improving the constant of 8
is a natural question for future research. We also note that w hile the overhead is more signiﬁcant
compared to the boundary wrapper of Algorithm 4, the output is more informative as it includes two
independent responses of the core algorithm whereas Algori thm4returns one or none (when ⊤is
returned). We expect that it is possible to design less-info rmative boundary wrappers for the case of
blackbox access (no probability oracle) that have a lower ov erhead. We leave this as an interestion
question for followup work.
Gqvalue for BetweenThresholds
We provide details for the BetweenThresholds classiﬁer (see Section 2.6). The BetweenThresholds
classiﬁer is a reﬁnement of AboveThreshold . It is speciﬁed by a 1-Lipschitz function f, two thresh-
oldstℓ< tr, and a privacy parameter ε. We compute ˜f(D) =f(D)+Lap(1/ε), whereLapis the
Laplace distribution. If ˜f(D)< tℓwe return L. If˜f(D)> trwe return H. Otherwise, we return ⊤.
Lemma G.1 (Eﬀectiveness of the “between” target) .The⊤outcome is an (1−e−(tr−tl)ε)·eε−1
e2ε−1-
target for BetweenThresholds .
Proof. Without loss of generality we assume that tℓ= 0andtr=t/ε.
Consider two neighboring data sets D0andD1and the respective f(D0)andf(D1). Sincefis
1-Lipschitz, we can assume without loss of generality (other wise we switch the roles of the two data
sets) that f(D0)≤f(D1)≤f(D0) + 1. Consider the case f(D1)≤0. The case f(D0)≥t/εis
symmetric and the cases where one or both of f(Db)are in(0,t/ε)make⊥a more eﬀective target.
πb
L:= Pr[f(Db)+Lap(1/ε)< tℓ= 0] = 1−1
2e−|f(Db)|ε
πb
H:= Pr[f(Db)+Lap(1/ε)> tr=t/ε] =1
2e−(|f(Db)|ε−t
πb
⊤:= Pr[f(Db)+Lap(1/ε)∈(0,t/ε)] =1
2/parenleftBig
e−|f(Db)|ε−e−(|f(Db)|ε−t/parenrightBig
=1
2e−|f(Db)|ε(1−e−t)
Note that π0
L≈επ1
Landπ1
H≈επ0
H,π0
L≥π1
Landπ1
H≥π0
H
We set
p= (π1
L−1
eε−1(π0
L−π1
L))+(π0
H−1
eε−1(π1
H−π0
H))
and the distribution Cto be Lwith probability (π1
L−1
eε−1(π0
L−π1
L))/pandHotherwise.
We specify pand the distributions BbandCas we did for NotPrior (Lemma 2.3) with respect
to “prior” L. (We can do that and cover also the case where f(D0)> t/ε where the symmetric prior
would be Hbecause the target does not depend on the values being below o r above the threshold).
The only diﬀerence is that our target is smaller, and include s only⊤rather than⊤andH.
Because of that, the calculated qvalue is reduced by a factor of
πb
⊤
πb
⊤+πb
H=1
2e−|f(Db)|ε(1−e−t)
1
2e−|f(Db)|ε= (1−e−t).
32H Analysis of SVT with individual privacy charging
We provide the privacy analysis for SVT with individual priv acy charging (see Section 2.8).
Proof of Theorem 2.10.We apply simulation-based privacy analysis (see Section A.5). Consider two
neighboring datasets DandD′=D∪{x}. The only queries where potentially f(D)/ne}a⊔ionslash=f(D′)and we
may need to call the data holder are those with f(x)/ne}a⊔ionslash= 0. Note that for every x′∈D, the counter
Cx′is the same during the execution of Algorithm 5on either DorD′. This is because the update of
Cx′depends only on the published results and fi(x′), both of which are public information. Hence,
we can think of the processing of Cx′as a post-processing when we analyze the privacy property
between DandD′.
Afterxis removed, the response on DandD′is the same, and the data holder does not need to
be called. Before xis removed from D′, we need to consider the queries such that f(x)/ne}a⊔ionslash= 0while
Cx< τ. Note that this is equivalent to a sequence of AboveThreshold tests to linear queries, we
apply TCT analysis with ConditionalRelease applied with above threshold responses. The claim
follows from Theorem B.4.
We also add that Algorithm 5can be implemented with BetweenThresholds test (see Sec-
tion2.6), the extension is straightforward with the respective pri vacy bounds following from Lemma G.1
(qvalue for target hit).
I Private Selection
In this section we provide proofs and additional details for private selection in TCT (Sections 2.4
and2.4.1). LetA1,...,Ambe ofmprivate algorithms that return results with quality scores . The
private selection task asks us to select the best algorithm f rom the mcandidates. The one-shot
selection described in Algorithm 3(withk= 1) runs each algorithm once and returns the response
with highest quality.
It is shown in [ LT19] that if eachAiis(ε,0)-DP then the one-shot selection algorithm degrades
the privacy bound to (mε,0)-DP. However, if we relax the requirement to approximate DP, we can
show that one-shot selection is (O(log(1/δ)ε),δ)-DP, which is independent of m(the number of
candidates). Moreover, in light of a lower-bound example by [LT19], Theorem I.1is tight up to
constant factors.
Formally, our theorem can be stated as
Theorem I.1. Suppose ε <1. LetA1,...,Am:Xn→ Y× Rbe a list of (ε,δi)-DP algo-
rithms, where the output of Aiconsists of a solution y∈ Yand a score s∈R. Denote by
Best(A1,...,Am)the following algorithm (Algorithm 3withk= 1): run eachA1,...,Amonce,
getmresults(y1,s1),...,(ym,sm), and output (yi∗,si∗)wherei∗= argmax isi.
Then, for every δ∈(0,1),Best(A1,...,Am)satisﬁes(ε′,δ′)-DP where ε′=O(εlog(1/δ)),δ′=
δ+/summationtext
iδi.
Proof. Discrete scores. We start by considering the case that the output scores from A1,...,Am
always lie in a ﬁnite setX⊆R. The case with continuous scores can be analyzed by a discret ization
argument.
FixD0,D1to be a pair of adjacent data sets. We consider the following i mplementation of the
vanilla private selection.
Assuming the score of Ai(D)always lies in the set X, it is easy to see that Algorithm 9simulates
the top-1 one-shot selection algorithm (Algorithm 3withk= 1) perfectly. Namely, Algorithm 9
33Algorithm 9: Private Selection: A Simulation
Input: Private data set D. The set Xdeﬁned above.
fori= 1,...,m do
(yi,si)←Ai(D)
forˆs∈Xin the decreasing order do
fori= 1,...,m do
ifsi≥ˆsthen
return(yi,si)
ﬁrst runs eachAi(D)once and collects mresults. Then, the algorithm searches for the lowestˆs∈X
such that there is a pair (yi,si)with a score of at least si≥ˆs. The algorithm then publishes this
score.
On the other hand, we note that Algorithm 9can be implemented by the conditional release
with revisions framework (cf. Algorithm 2). Namely, Algorithm 9ﬁrst runs each private algorithm
once and stores all the outcomes. Then the algorithm gradual ly extends the target set (namely,
when the algorithm is searching for the threshold ˆs, the target set is {(y,s) :s≥ˆs}), and tries
to ﬁnd an outcome in the target. Therefore, it follows from Le mma 2.5and Theorem B.4that
Algorithm 9is(O(εlog(1/δ)),δ+/summationtext
iδi)-DP.
Continuous scores. We then consider the case that the distributions of the score s ofA1(D),...,AK(D)
arecontinuous overR. We additionally assume that the distribution has no “point mass”. This is
to say, for every i∈[m]andˆs∈R, it holds that
lim
∆→0Pr
(yi,si)∼Ai(D)[ˆs−∆≤s≤ˆs+∆] = 0 .
This assumption is without loss of generality because we can always add a tiny perturbation to the
original output score of Ai(D).
FixD,D′as two neighboring data sets. We show that the vanilla select ion algorithm preserves
diﬀerential privacy between DandD′.
Letη >0be an arbitrarily small real number. Set M=10·m4
η. For each ℓ∈[1,M], letqℓ∈R
be the unique real such that
Pr
i∼[m],(yi,si)∼Ai(D)[si≥qℓ] =ℓ
M+1.
Similarly we deﬁne q′
ℓwith respect toAi(D′). LetX={qℓ,q′
ℓ}.
Now, consider running Algorithm 9with the set Xand candidate algorithms A1,...,AKonD
orD′. Sort elements of Xin the increasing order, which we denote as X={ˆq1≤···≤ˆqm}. After
samplingAi(D)for each i∈[m], Algorithm 9fails to return the best outcome only if one of the
following events happens.
• The best outcome (y∗,s∗)satisﬁes that s∗<ˆq1.
• There are two outcomes (yi,si)and(yj,sj)such that si,sj∈[ˆqℓ,ˆqℓ+1)for some ℓ∈[n].
If Item 1 happens, Algorithm 9does not output anything. If Item 2 happens, then it might be
possible that i < j,s i> sj, but Algorithm 9outputssi.
34It is easy to see that Event 1 happens with probability at mostm2
M≤ηby the construction of
X. Event 2 happens with probability at most M·m4
M2≤η. Therefore, the output distribution of
Algorithm 9diﬀers from the true best outcome by at most O(η)in the statistical distance. Taking
the limit η→0completes the proof.
Remark I.2. Theorem I.1shows that there is a factor of log(1/δ)overhead when we run top-1 one-
shot private selection (Algorithm 9) only once. Nevertheless, we observe that if we compose top- 1
one-shot selection with other algorithms under the TCT fram ework (e.g., compose multiple top-1
one-shot selections, generalized private testing, or any o ther applications mentioned in this paper)),
thenon-average we only pay 4εprivacy cost (one NotPrior target hit with a 2ε-DP algorithm) per
top-1 selection (assuming εis suﬃciently small so that eε≈1). In particular, adaptively performing
cexecutions of top-1 selection is (ε′,δ)-DP where ε′=ε·(4/radicalbig
clog(1/δ) +o(√c)).
Liu and Talwar [ LT19] established a lower bound of 2εon the privacy of a more relaxed top-1
selection task. Hence, there is a factor of 2 gap between this lower bound and our privacy analysis.
Note that for the simpler task one-shot above threshold scor e (discussed in Section 2.4.1), where the
goal is to return a response that is above the threshold if the re is one, can be implemented using a
single target hit on Conditional Release call (without revi se) and this matches the lower bound of
2ε. We therefore suspect that it might be possible to tighten th e privacy analysis of top-1 one-shot
selection. We leave it as an interesting question for follow up work.
I.1 One-Shot Top- kSelection
In this section, we prove our results for top- kselection.
We consider the natural one-shot algorithm for top- kselection described in Algorithm 3, which
(as mentioned in the introduction) generalizes the results presented in [ DR19 ,QSZ21 ], which were
tailored for selecting from 1-Lipschitz functions, using the Exponential Mechanism or t he Report-
Noise-Max paradigm.
We prove the following privacy theorem for Algorithm 3.
Theorem I.3. Suppose ε <1. Assume that each Aiis(ε,0)-DP. Then, for every δ∈(0,1),
Algorithm 3is(ε·O(/radicalBig
klog(1
δ)+log(1
δ)),δ)-DP.
Remark I.4. The constant hidden in the big-Oh depends on ε. For the setting that εis close to zero
so thateε≈1andδ≥2o(k), the privacy bound is roughly (ε′,δ)-DP where ε′=ε·(4/radicalbig
klog(1/δ)+
o(√
k)).
Remark I.5. We can takeAias the Laplace mechanism applied to a 1-Lipschisz quality function fi
(namely,Ai(D)outputs a pair (i,fi(D)+Lap(1/ε)), whereidenotes the ID of the i-th candidate,
andfi(D)+Lap(1/ε)is the noisy quality score of Candidate iwith respect to the data D). In this
way, Theoerem I.3recovers the main result of [ QSZ21 ] easily.
Moreover, Theorem I.3improves over [ QSZ21 ] from three aspects: Firstly, Theorem I.3allows
us to report the noisy quality scores of selected candidates for free, while [ QSZ21 ] needs to run one
additional round of Laplace mechanism to publish the qualit y scores. Second, our privacy bound
has no dependence on m, while the bound in the prior work [ QSZ21 ] was(O(ε/radicalbig
klog(m/δ)),δ)-
DP. Lastly, Theorem I.3applies more generally to anyprivate-preserving algorithms, instead of the
classic Laplace mechanism.
Proof. The proof is similar to that of Theorem I.1. Namely, we run each Ai(D)once and store
all results. Then we maintain a threshold T, which starts with T=∞. We gradually decrease
35T, and use Algorithm 2(Conditional Release with Revised Calls) to ﬁnd outcomes wi th a quality
score larger than T. We keep this process until we identify klargest outcomes. The claimed privacy
bound now follows from Lemma 2.5and Theorem B.4.
36