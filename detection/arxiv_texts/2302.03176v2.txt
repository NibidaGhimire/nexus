Astronomy &Astrophysics manuscript no. main ©ESO 2023
June 1, 2023
TDCOSMO XIV: Practical Techniques for Estimating External
Convergence of Strong Gravitational Lens Systems and
Applications to the SDSS J0924+0219 System
Patrick Wells1, Christopher D. Fassnacht1, and C. E. Rusu2
1Department of Physics and Astronomy, University of California, Davis, CA 95616, USA
email: pwells@ucdavis.edu
2National Astronomical Observatory of Japan, Tokyo, 181-8588, Japan
Received Feb. 6th, 2023; accepted – –, —-
ABSTRACT
Context. Time-delay cosmography uses strong gravitational lensing of a time-variable source to infer the Hubble Constant. The
measurement is independent from both traditional distance ladder and CMB measurements. An accurate measurement with this
technique requires the consideration of the e ffects of objects along the line of sight outside the primary lens, which is quantified by
the external convergence ( κext). In absence of such corrections, H0will be biased towards higher values in overdense fields and lower
values in underdense fields.
Aims. We discuss the current state of the methods used to account for environment e ffects. We present a new software package built
for this kind of analysis and others that can leverage large astronomical survey datasets. We apply these techniques to the SDSS
J0924 +0219 strong lens field.
Methods. We infer the relative density of the SDSS J0924 +0219 field by computing weighted number counts for all galaxies in
the field, and comparing to weighted number counts computed for a large number of fields in a reference survey. We then compute
weighted number counts in the Millennium Simulation and compare these results to infer the external convergence of the lens field.
Results. Our results show the SDSS J0924 +0219 field is a fairly typical line of sight, with median κext=−0.012 and standard
deviationσκ=0.028.
Key words. methods: data analysis, gravitational lensing: strong, gravitational lensing: weak, cosmology: cosmological parameters,
cosmology: distance scale
1. Introduction
One of the most important problems in modern cosmology is the
so-called Hubble Tension: the name given to an apparent dis-
crepancy between the value of the Hubble Constant ( H0) inferred
from the Cosmic Microwave Background assuming the standard
Λ-CDM cosmological model (e.g. Aghanim et al. 2020), and the
value inferred from Cepheid-calibrated Type Ia supernovae. (e.g.
Riess et al. 2021). The solution to this discrepancy may involve
unknown systematics, new physics, or some combination of the
two. Work on the resolution is ongoing, but having independent
methods for inferring the value of the Hubble constant is criti-
cal for solving the problem. Such methods include using the tip
of the red giant branch to calibrate supernovae,(Freedman et al.
2019) and BAO +BBN (Cuceu et al. 2019), among others. Here
we consider Time Delay Cosmography, which uses strong grav-
itational lensing of time-variable sources (usually quasars) to in-
fer the value of H0.
TDCOSMO is an international collaboration which aims to
use strong gravitational lensing to infer the value of H0with sub-
percent precision. A typical lens in the TDCOSMO sample in-
volves a source quasar being strongly lensed by a foreground
galaxy, producing four images. When the quasar’s luminosity
varies, this variation appears in each of th eimages at di fferent
times. The "time delay" between two images depends on the
mass distribution of the lens, and the time-delay distance :D∆t=(1+zd)DdDs
Dds∝1
H0(1)
where zdis the redshift of the lensing galaxy (or "deflector"),
andDd,Ds, and Ddsare the angular diameter distances to the de-
flector, source, and from the deflector to the source, respectively.
Given a model of the lens and a measurement of the time delays,
it is possible to infer the time-delay distance and therefore H0.
When analysing a strong lens, there are a number of sources
of uncertainty that propagate to the final inferred value of H0
(see Millon et al. 2020, for a more complete overview). Among
these are so-called environmental e ffects: gravitational bodies
besides the primary lens that a ffect the lensing observables and,
by proxy, the inferred value of H0. Accounting for these ef-
fects is crucial in improving the precision of the inferred mea-
surement. Broadly speaking we treat weak perturbers di fferently
from strong ones, with the di fference being determined with the
"flexion shift" formalism (see for example Buckley-Geer et al.
2020; Sluse et al. 2019; McCully et al. 2017).
In this paper, we focus on the techniques for inferring the
cumulative e ffect of all weak perturbers along the line of sight
to the source quasar. In principle, an "ideal" analysis would in-
clude these perturbers in a complete mass model of the system,
rendering the analysis we do here unnecessary. While this may
be possible for a few systems where high-quality spectroscopic
Article number, page 1 of 12arXiv:2302.03176v2  [astro-ph.CO]  31 May 2023A&A proofs: manuscript no. main
data of the perturbers is available, it does not scale well to the
large number of strong lensing systems we aim to analyze in the
future. Instead of building a complete model, we compare the
field of interest to some suitably large reference field, with the
goal of estimating the relative density of the field as compared to
the universe at large. This is a purely statistical analysis. In this
paper, we will discuss the current state of this technique, apply
it to the field around the SDSS J0924 +0219 lens system (here-
after J0924, see section 4.1), and discuss how it might be iterated
upon in the future. The cumulative e ffect of all weak perturbers
is parameterized by the external convergence , denotedκext.
While controlling uncertainties on individual lens systems is
crucial, the precision of the H0measurement can be improved
by including many lenses in the final inference. Of course, this
is much easier said than done as the amount of work required
to fully analyze a single lens is significant. However in the
modern era of data-driven astronomy, the amount of available
data is increasing by orders of magnitude and developing tools
for efficiently analyzing these datasets is a top priority. To that
end, we introduce lenskappa , a new package designed specif-
ically for the types of environment analysis discussed in this
paper, but with broader applications. lenskappa is built on
top of heinlein , a data management library designed for use
with large astronomical survey datasets. At present heinlein
andlenskappa support Subaru Hyper Suprime Cam Strategic
Survey Program (HSC-SSP; Aihara et al. 2019), the Dark En-
ergy Survey (Collaboration 2005), and the CFHT Legacy Survey
(Gwyn 2012).
This paper is organized as follows: In Section 2, we outline
the fundamentals of Time Delay Cosmography and the basic
process used to compute a value of κextfor a generic lens sys-
tem. In Section 3 we discuss the datasets used in our analysis,
and introduce lenskappa . In Section 4, we report the results of
our analysis of the J0924 field obtained using lenskappa . In
Section 5 we discuss our results and look forward to later work.
2. Summary of the technique
At a high level, gravitational lensing is the result of the under-
lying mass distributions of the universe, with greater concentra-
tions of mass resulting in more significant lensing. In general, it
is difficult to assess the impact of any one mass structure on the
image of some background object. Strong lenses are naturally an
exception, which occurs when a single, high-mass object falls
on (or nearly on) the axis drawn between an observer and some
background source. However we are seeking to understand the
total lensing e ffect of many additional objects, each producing
a tiny e ffect on the lensing observables. As light passes through
the universe, the amount of lensing it undergoes will be deter-
mined by the relative concentration of mass along its full path
of travel. Our analysis therefore seeks to infer the e ffects of the
mass distributions in our line of sight by comparing it to many
lines of sight in the universe. By determining how close this line
of sight is to the average line of sight in the universe, we can
place constraints on its impact on our primary lens observables.
In this section, we introduce the basics of time delay cosmog-
raphy and describe the technique we use to estimate κextfor a
generic lens system. For a more complete overview, we refer the
reader to Treu & Marshall (2016) and Birrer et al. (2022).2.1. Fundamentals of Time Delay Cosmography, and Strong
vs. Weak Lensing
Time delay cosmography focuses on strong lensing of time-
variable sources, usually a quasar. As the luminosity of the
source varies, this variation appears in each of the several images
of the source, but not at the same time. The time delay between
any two images can be written as follows:
∆tab=D∆t
c(− →θa−− →β)2
2−(− →θb−− →β)2
2−ψ(− →θa)+ψ(− →θb)
, (2)
where− →θrepresents the angular position of an image on the
sky,− →βrepresents the actual (unobservable) angular position of
the source on the sky, and ψis the scaled lensing potential. The
first two terms are a result of the di fferent distances traveled by
the light for the two images, while the later two terms are the
difference in the Shapiro time delay. The goal of a complete cos-
mographic analysis is to infer the time-delay distance (see eq. 1).
Using measurements of the time delay combined with a robust
model of the lensing galaxy it is possible to measure the time
delay distance and, therefore, the Hubble constant.
The multiple images observed in such a system are en exam-
ple of strong gravitational lensing . Qualitatively, strong lensing
is any lensing which produces multiple images of some back-
ground object. Quantitatively, strong lensing occurs whenever
the local density of the lens is greater than the lensing critical
density:
Σcr=c2Ds
4πGD dsDd(3)
For a given perturber, the convergence at a given location is
defined as the local density in units of the critical density:
κ(− →θ)=Σ(− →θ)
Σcr(4)
Forκ<1, strong lensing does not occur. Instead, mass distri-
butions with κ <1 result in magnification or demagnification of
the image of the background object. This is the situation for the
perturbers along the line of sight in our system that are not in-
cluded in the primary lens model, though we note that some per-
turbers that meet this criterion areincluded in the mass model
based on their flexion shift (see section 2.2), and therefore ex-
cluded from our statistical analysis. However, the e ffect of the
pertubers we doinclude in the estimate discussed here is not
directly observable because the actual angular size of the back-
ground object is not known.
We quantify the cumulative e ffect of all perturbers (not in-
cluding those incorporated into the primary lens model) and
voids along the line of sight with the external convergence, κext.
Conceptually, the value of κextis the convergence of a mass sheet
which, if placed coplanar to the primary lens, would produce the
same magnification or demagnification as the perturbers do col-
lectively. The external convergence is defined relative to that of a
line of sight where the mass distribution is smoothly distributed
with a density equal to the global mass density of the Universe.
Therefore positive values of κextrepresent lines of sight that are
overdense with respect to the overall density of the Universe,
while underdense lines of sight have negative values.
Article number, page 2 of 12Patrick Wells et al.: TDCOSMO XIV: Practical Techniques for Estimating External Convergence of Strong Gravitational Lens Systems and
Applications to the SDSS J0924 +0219 System
These perturbers can also produce shear, denoted γext, which
results in stretching and distorting of the image of the source.
However, this e ffect can be estimated in the primary lens model,
as it a ffects the location of the the images and the shape of the
Einstein ring (if present). We use the γextconstraint from the lens
model in our analysis as an additional constraint (see section 2.6)
Assumingκextcan be measured, it serves as a correction fac-
tor to the computed value of the time delay distance:
D∆t=D′
∆t
1−κext, (5)
where D′
∆trepresents the uncorrected value of the time delay
distance. This propagates directly to the inferred value of the
Hubble constant by
H0=(1−κext)H′
0 (6)
where H′
0is the value of the Hubble Constant inferred before
correcting for the environment. We see therefore that in the ab-
sence of the appropriate corrections, the inferred value of Hubble
constant would be biased towards higher values in an overdense
field, and lower values in an underdense field.
We now review the techniques we use to compute the value
ofκextfor a generic lens system.
2.2. Relevant Perturbers in the Line of Sight
To start, we identify objects along the line of sight that contribute
toκext. We separate objects into strong andweak perturbers, us-
ing an operational definition discussed below. Strong perturbers
are generally close to the center of the field or agalaxy group
(see for example Sluse et al. 2019; Fassnacht et al. 2006). Strong
perturbers are included explicitly in the mass model of the lens,
while weak perturbers are treated statistically. To separate these,
we use the flexion shift formalism, first proposed in McCully
et al. (2017). The flexion shift of an object is given by
∆3x=f(β)×(θEθE,p)2
θ3(7)
whereθEandθE,pare the Einstein radii of the main lens
and perturber respectively, and θis their angular separation. The
quantity f(β) is given by
f(β)=(
(1−β)2,ifz>zd
1, ifz<zd)
(8)
where
β=DdpDs
DpDds(9)
and where Ddp,Ds,Dp, and Ddsare the angular diameter dis-
tances from the deflector to the perturber, to the source, to the
perturber, and from the deflector to the source. The flexion shift
roughly measures the perturbations to the images of the source
due to 3rd order terms from the perturber. Ultimately, what con-
stitutes a "weak" or "strong" perturber is somewhat arbitrary, but
there this is a clear trade-o ffbetween the improvement from in-
cluding a particular perturber in the mass model and the amount
of work required to do so. McCully et al. (2017) recommends
using ∆3x=10−4arcsec as the cuto ffbetween strong and weak
perturbers to ensure a <1% bias on H0.2.3. Comparison Datasets
The first step in determining the value of κextis comparing the
field of interest to a large number of lines of sight from some
large reference field. This comparison gives us an empirical esti-
mate of the relative matter density of the lens field as compared
to all lines of sight in the universe. The reference field should
be large enough to avoid sampling bias. For a small comparison
field (on the order of a few deg2) statistical overdensities or un-
derdensities may occur (see for example Fassnacht et al. 2010).
However modern survey datasets are available which cover hun-
dreds to thousands of square degrees, allowing us to use a suf-
ficiently large comparison field to avoid sampling bias (see sec-
tion 4.2 for a discussion of our choices for this analysis). Ideally
the data for the reference field and lens field are taken by the
same instrument and processed by the same analysis pipeline.
This turns out to be the case for the analysis of J0924 discussed
later in this paper, but will not be true in general. At a very mini-
mum we seek data with at least one band in common, with deep
enough observations to produce meaningful results. When com-
paring the survey dataset to our line of sight, we set a magnitude
cut and remove any objects from both datasets fainter than this
cut. Previous work (see Collett et al. 2013) has suggested that
i<24 represents a good limit, as setting a fainter limit does not
appear to meaningfully impact the results of this style of analy-
sis. This limit is also bright enough to be well above the detection
limits of modern sky surveys, ensuring reliable photometry.
Once the appropriate data are in hand, it is important to con-
sider which objects should be used in the comparison. It is im-
portant to set a magnitude cut that will remove objects too close
to the detection limit of the instrument for photometry to be re-
liable, while still leaving enough data to make robust estimates
of the relative density. However setting too bright of a limit will
result in having too few objects to compare to.
Additionally, we cut out all objects with a redshift greater
than the redshift of the source quasar, as these objects will not
affect the path of the light as it travels from the quasar to our
telescopes.
2.4. Weighted Number Counts of Lens Field
Because the value of κextcannot be directly measured, we first
define tracer quantities that can be computed directly from the
available data. By comparing the value of these quantities in the
lens field to the value of the identical quantity computed for a
large number of reference fields, we obtain an empirical esti-
mate of the relative density of the line of sight of interest. As
a first approximation, we expect the greatest contribution to κext
from massive objects close to the center of the line of sight. The
primary mass contribution in any line of sight will be dark mat-
ter halos, but these are not directly observable. To quantify the
contribution from these weak perturbers, we compute weighted
number counts of the visible structures (i.e. galaxies) in the lens
field as compared to a large number of reference fields. This
technique has been used extensively in previous work (Fassnacht
et al. 2006; Suyu et al. 2010; Greene et al. 2013; Buckley-Geer
et al. 2020) . To do this, we select a region of interest around the
lens and compare it to a large number of identically-shaped fields
selected at random from the reference survey. At each step, we
compute the ratio of the weighted number counts for the galaxies
in the lens field to the identical statistic computed in the given
reference field. For the given step, the value of the weight is
therefore:
Article number, page 3 of 12A&A proofs: manuscript no. main
Table 1. Weighted Number Count Definitions
Name Value Symbol
Number Count wj=1 wn
Inverse Distance wj=1/rj w1/r
Potential wj=mj/rj wp
Redshift wj=zs·zj−z2
jwz
z/r wj=wz,j/rj wz/r
Wi=P
jwj,lensP
jwj,i(10)
Where iindexes the reference fields, jindexes the galaxies
in a given field, and wis the value of the weighted statistic for
the given galaxy.
Following Rusu et al. (2017) we also consider a second style
of weighting that improves our results. Instead of summing the
value of weights for all objects in the field, we instead compute
the weight for a given field as wi,meds=niwjwhere niis the num-
ber of galaxies in the given reference field and wjis the median
value of the weight for all galaxies in the reference field. Do-
ing this helps avoid situations where single objects dominate the
sum in a particular line of sight. This is especially important for
weights involving stellar mass and the inverse seperation. In this
scheme, the value of the weight for the given reference field is
therefore
Wi,meds≡nlenswj,lens
niwj,i(11)
There has been discussion in the literature about which are
the best weights to consider for the most robust determination of
κext(Greene et al. 2013; Rusu et al. 2017, 2019). In this work,
we only consider a subset of the weights considered in previous
works (see Table 1).
2.5. Weighted Number Counts in Simulated Data
In order to determine the posterior distribution of κextfor the
given system, it is necessary to compare the weighted num-
ber counts to similar counts obtained from a reference field for
whichκextis known. We use a simulated dataset for this purpose.
The simulation must contain several components in order to be
suitable for this analysis. First, it must contain catalogs of galax-
ies with known luminosity and redshift. Second, values of the
external convergence must be measured at a suitably large num-
ber of points to be representative of the universe at large. Rusu
et al. (2017) examined possible biases from inferring κextusing
the number counts method in the Millennium Simulation, and
found these methods produced a good estimate of κext. We dis-
cuss our choice of simulation further in section 4.3. Because this
technique involves ratios, much of the dependence on the simu-
lation’s underlying cosmological parameters should cancel out.
However, ensuring this would require a second simulation with
the attributes described. An exciting development in this space
are the initial results from the MillenniumTNG (Hernández-
Aguayo et al. 2022). Full weak-lensing convergence maps are
planned, but not yet available.2.6. From Weighted Number Counts to κext
We now have weighted number counts for the lens field itself and
for a large number of fields in a simulated dataset, each of which
is associated with a value of κext. We seek to compute p(κext|d):
the probability distribution of κextgiven the data. We can replace
this with the joint probability distribution of κextandthe data as
follows:
p(κext|d)=p(κext,d)
p(d)=Z
dW qp(κext,Wq,d)
p(d)(12)
After some work it can be shown (see Rusu et al. 2017)
p(κext|d)=Z
psim(κext|W)p(W|d)Y
idW i (13)
Where p(W|d) is the probability distribution of given weight-
ing scheme given the data, and psim(κext|W) is the probability dis-
tribution of κextin the simulated dataset, given a particular value
of the weight. Here, it is assumed that the simulated dataset is
the correct prior for the observable universe. In other words, that
it correctly relates values of weights to values of κext
Additionally, a constraint on κextbased onγextcan be in-
cluded, which accounts for the expected correlation between
these two values. In general, γis a two-dimensional vector on
the plane of the sky, but we use only the overall magnitude in
our analysis. γextis a parameter that can be fitted in the primary
mass model of the lens. We use γextjust as we would a weighted
number count distribution. For any range of values of γext, we
can construct a histogram of the value of κextfor all lines of sight
with values of γextthat fall in this range. We then weight the con-
tribution from these lines of sight by p(γ|d), which is the poste-
rior onγinferred from the mass model.This provides a prior on
κextthat can meaningfully a ffect the final result. In section 5, we
present results both with and without using γextas a constraint.
In previous work (e.g. Rusu et al. 2017), this full probabil-
ity distribution for each weight p(W|d) was replaced by a nor-
mal distribution centered on the median of the full weight dis-
tribution, with a width determined by examining measurement
uncertainties. In practice, this width was much smaller than the
width of the actual distribution. This is cheaper computationally,
but ignores covariance between the various types of the weights.
While the increase in computation time is significant, the major-
ity of the important decisions in the analysis are made when we
compute weighted number count ratios, and using this method
does not meaningfully increase our time-to-result
Since all the individual weights are measured at the same
sequence of randomly-drawn fields, we can construct a full m-
dimensional probability distribution p(Ws|d)=p(Wn,W1/r,...|d)
where mis the number of weights being considered. We then
explore this probability distribution when implementing the for-
malism described above. Formally, the posterior on κextbe-
comes:
p(κext|d)=Z
psim(κext|Ws)p(Ws|d)dmWsp (14)
When computing this quantity, we split the m-dimensional
probability distribution into 200mm-dimensional bins. We have
also tested this procedure 100mbins and see consistent results
for the J0924 field. With significantly more bins, the computa-
tional time balloons and the number of fields in each bin drops
significantly, even near the center of the distribution. The value
Article number, page 4 of 12Patrick Wells et al.: TDCOSMO XIV: Practical Techniques for Estimating External Convergence of Strong Gravitational Lens Systems and
Applications to the SDSS J0924 +0219 System
ofp(Ws|d) is simply the number of lines of sight in the reference
survey that fall in this bin divided by the total number of lines of
sight being considered. Given the large numbers of lines of sight
we consider, the distributions are smooth and it may be possi-
ble to model them explicitly and explore the distribution with an
MCMC, but we save this for a future analysis. The exception to
this is the pure unweighted number counts in a 45′′aperture, due
to the fact that the "weight" is integer valued and the number of
galaxies in the aperture is relatively modest.
Forpsim(κext|Ws) we construct a histogram of the measured
values ofκextfor all lines of sight from the simulated datasets that
fall within the given bin, normalized by the number of lines of
sight in the bin. Without this normalization, κextvalues near the
mean of the simulation will always be weighted more heavily
because there are comparatively more of them. Our choice of
simulation is discussed in section 4.3
The inputs to our analysis code include the full weight dis-
tributions for the lens field, the weights computed from the sim-
ulated dataset, and the κextmaps for the simulated dataset for the
source redshift. We iterate over the probability distribution dis-
cussed above, computing a histogram of the values of κextin each
m-dimensional bin. The overall histogram is the sum of these
histograms, weighted by the value of the weight distribution in
that bin.
2.7. Comparison to Other Methods
There are other methods for estimating κextwhich have been ex-
plored as discussed below. Ultimately many of these techniques
involve a trade-o ffbetween speed of analysis and precision of
the final result. Our goal is explicitly to design and implement
techniques that allow us to analyze hundreds or even thousands
of lens systems in a reasonable amount of time, with the goal of
combining full results (including lens modeling, and time delay
measurements) from many lenses to make some final statement
about the value of H0. Because of this, a technique with slightly
less constraining power for a single lens is tolerable if it can
be performed and iterated on rapidly. A few other techniques
for estimating κextare discussed below. While all show promise,
and are interesting for their own sake, none show a significant
enough improvement to justify the increased time and complex-
ity of analysis, at least in the context of our stated large-scale
goals.
2.7.1. Weak Lensing Analysis
While the value of κat any given point on the sky is not observ-
able,γcanbe measured by looking a distortions in the shapes
of galaxies in the line of sight. Assuming γcan be measured,
techniques such as those presented in Kaiser et al. (1995) can be
employed to reconstruct the underlying mass distribution. How-
ever this analysis requires extremely high quality data about the
morphology of galaxies in the line of sight, as γis measured
from the extremely small distortions that are present in the im-
ages of the galaxies. Obtaining such high quality data, as well
as the overhead of analyzing it, represents a significant bottle-
neck. This analysis was performed in Tihhonova et al. (2018)
on the same line of sight analyzed in Rusu et al. (2017). Results
between the two methods are consistent, with only modest im-
provement to precision for the weak lensing analysis.2.7.2. Explicit Modeling
Building an explicit mass model of all perturbers (or at least,
some larger sample) is attractive from a pure astrophysics per-
spective. This approach was explored in McCully et al. (2017)
and demonstrated some success. However, the analysis assumed
lines of sight with extensive spectroscopic coverage. While this
may be true for some lines of sight, spectroscopy is expensive in
time and resources and it is not obvious that the improvement is
significant enough to justify this. Furthermore, explicitly mod-
eling a line of sight requires making assumptions about the host
halos of the galaxies, a potential source of additional bias.
2.7.3. Machine Learning Methods
An additional interesting approach using Bayesian graph neural
networks (BGNNs) is presented in Park et al. (2022). This ap-
proach was compared to a toy version of the analysis discussed
here, using only a single summary statistic instead of a combina-
tion of several. The BGNN technique demonstrated greater pre-
cision and accuracy over using a single summary statistic on sim-
ulated data, however we estimate the di fference would be much
less significant if the comparison was done against the full line of
sight analysis discussed in this work. That said, a more detailed
comparison between these techniques would be welcome in the
future. Park et al. (2022) also demonstrated a meaningful bias for
both techniques in more extreme fields ( κ<−0.05 andκ>0.06)
due to a lack of similarly extreme fields in simulated datasets to
compare to. This is worth examining further, but does not imme-
diately suggest significant improvements from using the BGNN
techniques, especially for extreme fields.
3. Datasets, automation, and lenskappa
As with many areas of astronomy, time-delay cosmography is
grappling with datasets that are growing at unprecedented rates.
There are dozens of known quad lenses which may be suitable
for cosmographic analysis, but a full analysis has only been com-
pleted on a small fraction of them. Techniques for increasing the
rate of analysis are therefore crucial to the continued success of
the technique.
While there are many open science questions in Time Delay
Cosmography, much of the solution to the rate problem lies in
software engineering rather than astronomy. When building for
this kind of of analysis, we keep three key questions in mind:
–What is the minimum amount of data required to complete a
particular analysis step with su fficient precision?
–How much of the analysis can realistically be automated?
–Do we expect the analysis techniques to change significantly
in the future?
The answer to these questions may not be independent. For
example, a pipeline that uses less data at the expense of increas-
ing uncertainties on individual systems may be tolerable if it sig-
nificantly increases the rate at which these systems can be ana-
lyzed. The third question is also important. Writing flexible soft-
ware packages that can easily be updated as analysis techniques
evolve usually increase the time to first result, but substantially
decrease average time to result in the long run.
Among the various steps required to fully analyze a lens
system, the number counts technique discussed in this paper is
likely the most straightforward to automate. It is mostly sta-
tistical, and the most di fficult computational challenge is e ffi-
ciently filtering a large dataset by location. Furthermore the sur-
vey datasets used in this analysis are quite robust, and many of
Article number, page 5 of 12A&A proofs: manuscript no. main
the lens systems fall within one or more survey footprints. This
makes the first question a non-issue, at least for a significant frac-
tion of the lenses.
To that end, we introduce lenskappa1andheinlein2. The
goal of lenskappa is to build a tool capable of automating en-
vironment analysis to the greatest extent possible, while still
providing su fficient flexibility to allow us to iterate on our cur-
rent methods. lenskappa is in turn built on top of heinlein ,
which serves as a high-level interface to locally stored astro-
nomical datasets. When computing weighted number counts in
lenskappa , the core weighting loop consists of:
1. Select a region of interest from a large survey.
2. Retrieve object catalog and auxiliary data for the region.
3. Compute interesting quantities, using the data retrieved for
the region.
A single iteration of this loop is represented in Figure 1
heinlein handles the middle step of this loop. It provides
high-level routines for storage, retrieval, and filtering of large
survey datasets, as well as intuitive interfaces for interaction be-
tween data types (for example, applying a bright star mask to a
catalog). heinlein can perform a 120" cone search in the HSC
dataset in around 1.5 seconds, without requiring the data be pre-
loaded into memory. For later queries of nearby locations, the
speed is improved by more than an order of magnitude through
caching. This makes heinlein suitable both for interactive use
and for the kind of analysis done in lenskappa .
With data retrieval optimized, lenskappa focuses on al-
lowing users to design and implement analyses that operate on
large swathes of the sky. The techniques described in this pa-
per, for example, could easily be adapted to build mass maps
of the universe as seen in these surveys. However the goal is
to work towards a tool which allows for much more flexibility,
enabling anyanalysis that involves calculations done on many
small regions within a large astronomical survey. Other applica-
tions could include lens finding, though we note that image data
is not yet supported in heinlein.
lenskappa includes several features to facilitate this includ-
ing:
–High-level API for defining analyses
–Plugin architecture for adding new capabilities without mod-
ifying the core code.
–Automatic support for any dataset supported by heinlein
3.1. Analyzing modern astronomical survey datasets at scale
One of the big challenges in doing analyses on these kinds of
datasets is the need for the computing environments to be close
to the data whenever possible. Querying over the internet is use-
ful for assembling datasets, but is not a particularly good solu-
tion when analyzing a dataset at scale. Many researchers will not
have access to su fficient storage to store these datsets, and it is
impractical to expect individual survey teams to provide comput-
ing resource for general use. The size of these datasets will en-
able next-generation analyses, but only with the development of
next-generation tools running at scale, which will require com-
puting infrastructure that may not be readily available to many
researcher.
We support using cloud computing services to fill this gap.
Commercial providers have expanded and matured by leaps and
1https: //github.com /PatrickRWells /lenskappa
2https: //github.com /PatrickRWells /heinleinbounds over the last decade, and routinely handle storage and
analysis tasks on datasets orders of magnitude larger than the
ones being discussed here. Additionally, cloud computing tech-
nologies are significantly more accessible then on-premise tech-
nology: they require far lower startup costs and can be quickly
scaled (to accommodate more users, or bigger jobs) without the
bottlenecks that slow down the expansion of on-premise infras-
tructure.
It is also possible to use cloud solutions as a supplement
to already-existing on premise solutions. Holzman et al. (2017)
demonstrated this by analyzing data from the Compact Muon
Solenoid experiment at massive scale.
In the future, we plan to develop lenskappa andheinlein
tools that could be easily deployed onto services like these, pro-
viding quick and easy access to large survey datasets in addi-
tion to techniques for processing and analyzing that data. The
datasets would be stored in the cloud, allowing users to de-
ploy their analyses without worrying about the connection to
the underlying dataset. Such an approach has been demonstrated
by Kusnierz et al. (2022), which enabled serverless access to
ROOT for quick analysis tasks on high energy datasets with-
out the user having to manually retrieve the data. With some
work, heinlein will enable this type of analysis by serving
as a bridge between computing infrastructure (which could be
managed by individual researchers) and the underlying data lake
(which could be managed by the survey team). Abstracting away
data retrieval will allow researchers to focus on what matters
most: designing the analyses they wish to perform and interpret-
ing results.
4. Analysis of SDSS J0924+0219
In this section, we discuss the analysis of the J0924 system, as
performed by lenskappa .
4.1. The Lens Field
SDSS J0924 +0219 is a quadruply lensed quasar first reported in
Inada et al. (2003). The quasar itself is at redshift z=1.523,
while the lensing galaxy is located at z=0.384 (Eigenbrod, A.
et al. 2006). Quadruply lensed quasars are particularly valuable
for cosmographic analysis because it is in principle possible to
measure 12 di fferent time delays, though only three of these are
independent. An image of the field can be seen in figure 2. This
system was modeled in Chen et al. (2022)
One particularly important feature of the lens field is the
bright star located in the lower left. The star covers a reasonably
large fraction of the overall area in the 120" aperture, undoubt-
edly covering several background objects and making photome-
try for objects very near it unreliable. We apply techniques that
have previously been used in this kind of analysis to correct for
this. This technique is discussed in more detail in section 4.2.
The lens field falls within the Subaru Hyper Surpime-Cam
Strategic Survey Program footprint, and full color information is
available for all objects in the relevant catalogs. We use these cat-
alogs, including photometric redshifts, for all objects inside the
field. Additionally we use the bright star masks and photometric
redshift PDFs provided by the HSC team.
Our analysis follows the same outline discussed in section 2.
We discuss the details particular to this lens field below.
Article number, page 6 of 12Patrick Wells et al.: TDCOSMO XIV: Practical Techniques for Estimating External Convergence of Strong Gravitational Lens Systems and
Applications to the SDSS J0924 +0219 System
Fig. 1. Flowchart representing the process of computing a weighted count ratio given a location in a reference survey. Work done in heinlein is
contained within the dotted box.
4.2. The HSC Survey and Weighted Number Count Ratios
The Hyper Suprime-Cam Subaru Strategic Survey is a large sur-
vey program, aiming to cover roughly 1400 deg2of sky in five
photometric bands ( grizy ) down to i∼26, with deeper coverage
expected in smaller regions of the sky (Aihara et al. 2017). We
base our analysis on the roughly 400 deg2that had coverage in
all five bands as of the second data release (Aihara et al. 2019).
Data release 3 was made available while this paper was in prepa-
ration, but we do not consider it here.
The HSC Survey is a natural choice for a comparison dataset
for this system because the J0924 field itself falls within the sur-
vey footprint. We therefore have both robust and comparable
photometry. When computing weighted number count ratios, the
basic approach is identical to the one outlined in the section 2.4,
with some specific adjustments:
We use objects with r_extendness_value =1.0, which
selects galaxies. Bosch et al. (2017) demonstrates that their al-
gorithm for computing this quantity does a reasonably good job
of selecting galaxies, though it may incorrectly classify some
galaxies as stars, and vice-versa. However we note that the HSC-
wide survey was intentionally selected to enable cosmological
analyses by being in regions that are away from the galactic
plane and low on dust extinction (Aihara et al. 2017). This,
combined with robust bright-star masks and our large sample
size ensures unmasked stars do not significantly impact the final
weighted number count ratios.The full photometric redshift PDFs for all objects in the data
release have been made available by the HSC team (Nishizawa
et al. 2020). They use two separate fitting algorithms, and results
from both algorithms are included in the catalog. We compute
weighted number count ratios using both sets of redshifts, and
do not find a meaningful di fference between the resultant distri-
butions. As such, we use the "DEmP" redshift and stellar masses
for our analysis (Hsieh & Yee 2014; Tanaka et al. 2017).
When computing weighted number counts, we remove all
objects closer than 5 arcsec from the center of the field following
Rusu et al. (2017). Objects this close to the center of the field are
typically explicitly included in the mass model of the lens, and so
we also remove them from the comparison fields to avoid biasing
results.
The HSC survey team makes available masks that represent
areas of the sky where photometry may be unreliable or lacking
due to the presence of bright stars (Coupon et al. 2017). This
is particularly important in our field due to the presence of the
bright star that can be seen in Figure 2 When iterating over the
reference survey, we retrieve the bright star masks for each re-
gion being considered. We apply both these masks and the masks
for the lens field itself to both catalogs at each weighting step.
Doing this ensures that results are not biased if a given field in
the reference survey has significantly more or less of its area cov-
ered by bright star masks. This procedure was first used in Rusu
et al. (2017)
Article number, page 7 of 12A&A proofs: manuscript no. main
Fig. 2. Field around SDSS J0924, shown in HSC i-band. The red rings represent 5", 45" and 120" apertures respectively. Bottom right: 10” cutout
of the lens system from HST imaging.
The 400 deg2of sky we consider here is separated into seven
disconnected regions. Initially, we computed weighted num-
ber count ratios in five of these regions. This produces nearly-
identical distributions, with the di fference between the lowest
and highest median being 0 .1σthe standard deviation of the dis-
tribution, suggesting our fields are large enough to to avoid sam-
pling bias. Based on this, we restrict our subsequent analysis to
135 deg2of sky located in the region 332◦<RA<359◦and
−1.5◦<Dec<6◦. For each combination of aperture and lim-
iting magnitude, we compute weighted count ratios at 100,000
randomly selected fields.
4.3. Millennium Simulation
The Millennium Simulation (Springel et al. 2005) is a dark mat-
ter only simulation split into 64 4 ×4 deg2fields. After the orig-
inal run was completed, synthetic galaxy catalogs were painted
into the resultant halos by several teams. Following (Rusu et al.2017) we use the semi-analytic catalogues of De Lucia & Blaizot
(2007). Additionally, Hilbert et al. (2009) split each 4 ×4 deg2
field into a grid of 4096x4096 points and used ray tracing to
compute convergence and shear at each of these points in 63
redshift planes. These, combined with its large size, makes it an
excellent choice for our analysis. In our analysis, we use redshift
plane 36 with z=1.504.
First, we compute the weighted number counts at a large
number (order 106) of equally spaced grid points in the Mil-
lennium simulation. For the 45” aperture, we place the fields
90” apart (snapped to the nearest grid point), while for the 120”
aperture we place fields 60” apart. Both cases result in over 1.5
million lines of sight across the simulation, each of which has an
associated value of κextandγextThis di ffers from the same cal-
culation for the lens field itself in a key way: the values reported
are the total value of the weights at every point considered, rather
than a ratio of values. To normalize, we divide weighted num-
ber count in each field by the median value for all lines of sight
Article number, page 8 of 12Patrick Wells et al.: TDCOSMO XIV: Practical Techniques for Estimating External Convergence of Strong Gravitational Lens Systems and
Applications to the SDSS J0924 +0219 System
Fig. 3. Location and relative weights of all galaxies with z<zs.The
black circles represent the 45" and 120" apertures respectively. Blue
objects are those with i <23, while red objects are those with 23 <i<
24. Relative object weights of the object are represented by the size of
the dot.
in our sample. The median value of the resultant distribution is
therefore unity.
A key di fference between the synthetic catalogs and the real
data from the HSC survey is that in the Millennium Simulation
catalog redshifts are exact. In previous work (eg. Rusu et al.
2017) this di fference was accounted for by computing photomet-
ric redshifts for all objects in the Millennium Simulation, using
the same pipeline that was used to compute the photometric red-
shifts in the comparison dataset. We are unable to use this tech-
nique in this case because the HSC survey photo- zpipelines are
not publicly available as of the preparation of this paper. Instead,
we download the full catalog of training data used by the HSC.
These are galaxies for which spectroscopic redshifts are avail-
able. We divide the objects in the test dataset into redshift and
magnitude bins. For each galaxy in a bin, we compute the o ffset
in the central value of the redshift ( zphot−zspec), and take the
median of these as our estimate of the redshift bias in that bin.
Additionally we take the median value of σz(as reported by the
HSC photometric redshift pipeline) for all photometric redshifts
in that bin. We take the the median value of σzas our estimate of
photometric redshift uncertainty. For each object in the Millen-
nium Simulation catalog, we construct an artificial photemetric
redshift PDF. The center of the distribution is the "actual" red-
shift of the object, o ffset by the the amount computed previously
for the appropriate bin. The width of the distribution is the value
ofσzcomputed for the same bin.
At each weighing step in the Millennium Simulation, we
then sample from these "photometric redshift" PDFs when com-
puting weighted number counts. For each line of sight, we sam-
ple from each of these PDFs 50 times. This produces 50 sepa-
rate catalogs (all with slightly di fferent values for object "red-
shifts"), and we compute weighted number counts for each one.
We find that this process produces no meaningful change to our
final inference for κext, even when a weight that depends on the
redshift is taken into account. However this process massively
increases the amount of data output by our code, and therefore
significantly increases the amount of time required to do the finalTable 2. Medians of Weighted Number Counts for J0924
Weight i<23, 120" i<24, 120" i<23, 45" i<24, 45"
wn 1.04 1.04 1.42 1.35
w1/r 1.17 1.09 1.57 1.35
wp 1.19 1.10 1.56 1.36
wz 1.03 0.99 1.56 1.37
wz/r 1.21 1.07 1.74 1.37
w1/r,meds 1.08 1.01 1.59 1.45
wp,meds 1.12 1.01 1.50 1.47
wz,meds 1.03 1.00 1.50 1.36
wz/r,meds 1.10 1.04 1.63 1.40
inference on κext. This does suggest that photometric redshift un-
certainties do not have a significant impact on the inferred value
ofκext, but we plan to explore this more completely in the future.
5. Results and Discussion
Our weighted number counts for the lens field include a total of
five weights ( wn,w1/r,wp,wz,wz/r, see Table 1), two apertures
(45" and 120"), two limiting magnitudes (23 and 24), and two
summing techniques (pure sum and medians). For brevity, we
report only the medians of these distributions in Table 2. The
full distributions, along with the analysis code used to produce
the posterior distributions for κextare available on github.3. A
visual catalog of the objects in the field and their relative weights
can be seen in Figure 3.
The weighted number count ratios suggest the SDSS J0924
field is mildly overdense as compared to the universe as a whole.
This overdensity is significantly more obvious when consider-
ing the 45" aperture. This is quite reasonable; as the size of the
aperture increases, the density of field will approach the density
of the universe as a whole. See Figure 4 for an overview of the
weights considered in this work.
It is however less obvious why the value of the weights seem
to depend on the limiting magnitude, with the median values for
i<23 being significantly higher than for i<24. This may
suggest that the quality of the lens field catalog is poor below
magnitude 23, We remind the reader that the 5" region around the
lens itself is masked when computing weighted number counts.
Based on the above, we would expect our final value of κext
to be somewhat positive. However the strength of the external
shear of the field, reported in (Chen et al. 2022) is 0 .017+0.001
−0.003.
This places it significantly below the median (and, in fact, mean)
for all lines of sight in the Millennium Simulation. For each com-
bination of limiting magnitude and aperture we compute κextfor
a number of di fferent combinations of weights:
–Pure (unweighted) number counts, combined with each of
the remaining weights individually.
–Pure number counts and inverse distance weights, combined
with each of the remaining weights individually.
–For each of the above cases, we run the kappa inference both
with and without the constraint from γ.
The number count ratios used for this analysis are listed in
Table 2. We do not mix distributions obtained using our two dif-
ferent weighting techniques.
We also repeat this for each while using the median weight-
ing scheme rather than the sum scheme. All together, this leaves
us with 112 individual histograms for the value of kappa.
3https: //github.com /PatrickRWells /J0924-analysis
Article number, page 9 of 12A&A proofs: manuscript no. main
Fig. 4. 2D histograms of for each possible pair of the weight number count ratios considered in this work, computed with a 45” aperture and
limiting magnitude i<24. The inner and outer contours represent 68 and 95% confidence intervals, respectively.
We find that the choice of specific weights is less important
than the number of weights being considered. In all cases, infer-
ringκextwith three weights instead of two tightens the resultant
distribution, but does not significantly a ffect the central value.
Rusu et al. (2017) found best results when combining wnandwr
with one additional weight and the constraint from γext. We find
the same here. Specifically, the central value of the distribution
does not change meaningfully based on the choice of the third
weight, but we find slightly tighter constraints when using wp.
We also find that results are consistent between apertures,
but find that a brighter value of the limiting magnitude results
in a noisier posterior on κext. Because so many objects fall be-
tween magnitude 23 and 24, removing these objects results in
significantly noisier weighted number counts which translate to
the final inference on κ.Considering only γas a constraint leads
to a median value on κextof -0.015. As a result, including γextas
a constraint significantly lowers the central value of our distribu-
tions, though it also tightens the distribution. However we notethis shift is fairly modest as compared to the width of the dis-
tribution. This is similar to the result seen in Rusu et al. (2017),
though in that case the inferred value of the shear was signif-
icantly closer to the median in the Milennium Simulation of
0.028, and the shift of the central value was not as significant.
We find our tightest constraints on κextthrough a combination of
wn,w1/randwpcombined with constraints from γext. This leads
us to a final value of κextof -0.012 with a width σκ=0.028.
Without the constraint from γext, we obtain a median value of
0.012 with σκ=0.053.Full posteriors for these combinations
can be found in Figure 5. We use a 45” aperture and limiting
magnitude of i<24 for these results.
6. Conclusions and Future Work
In this paper, we have discussed the current state of the line
of sight number counts technique for environment analyses in
time delay cosmography. We have introduced two main improve-
Article number, page 10 of 12Patrick Wells et al.: TDCOSMO XIV: Practical Techniques for Estimating External Convergence of Strong Gravitational Lens Systems and
Applications to the SDSS J0924 +0219 System
Fig. 5. Comparison of smoothed posterior on κextforwn+wr+wpwithout
γext,wn+wr+wpwithγext, andγextalone.
ments to previous iterations of the analysis. First, our packages
lenskappa andheinlein make designing and running these
analyses much quicker than before, in addition to making it
much simpler to add additional survey datasets. This will ac-
celerate the pace of future analyses, and enable population-level
analyses of lens environments. Additionally, we have made use
of the entire distributions of weighted number counts, which ac-
counts for covariance between weights and is generally more ro-
bust than just exploring a small region around the medians. We
have applied these techniques to the J0924 field, and found that
this field is a fairly typical line of sight, with a slightly negative
median value of κext.
6.1. Future Development of lenskappa
Our primary goal for this project has been to build a software tool
that can quickly and reliably analyze weak perturbers along lines
of sight to strong gravitational lenses. We have accomplished
this goal, but we plan to extend the capabilities of lenskappa to
include tools for analyzing strong perturbers and coherent struc-
tures (such as galaxy groups). These objects must be handled in-
dividually, and require very di fferent analysis tools. However we
see a significant advantages to being able to do full environment
analyses within a single software package.
6.2. Future Analyses
Leveraging the capabilities of lenskappa , we hope to better un-
derstand lines of sight to gravitational lenses on a population
level. Fassnacht et al. (2010) and Wong et al. (2018) have shown
that lenses seem to fall in preferentially overdense lines of sight,but that this overdensity seems to be confined to the immediate
surroundings of the lens itself. Lenskappa gives us the tools to
perform these population-level analyses quickly, with the free-
dom to adjust and re-run the analysis as needed. As a first step,
we hope to complete an analysis with 4-5 new lens systems, as
well as 2-3 systems that have previously been analyzed to check
our results.
As an additional check, we would like to analyze a large
number of non-lens lines of sight. This would ensure there are
no biases introduced by comparing distributions based on real
galaxy catalogs to those obtained from the synthetic catalogs in
the Millennium Simulation.
Recently, Park et al. (2022) demonstrated the use of Bayesian
Graph Neural Network to estimate the value of κextin a simulated
dataset. Their method out-performs a simplified version of the
analysis performed here that uses only a single weight. Further
work may demonstrate the ability of the technique to match or
even outperform the weighted number counts technique, but a
more complete comparision will need to be performed to assess
this.
Acknowledgements. P.W. and C.D.F. acknowledge support for this work
from the National Science Foundation under Grant No. AST-1907396.
P.W. thanks Liz Buckley-Geer, Anowar Shajib, and Simon Birrer for useful dis-
cussions throughout the preparation of this manuscript.
References
Aghanim, P. C. N., Akrami, Y ., Ashdown, M., et al. 2020, Astronomy & Astro-
physics, 641, A6
Aihara, H., AlSayyad, Y ., Ando, M., et al. 2019, Publications of the Astronomi-
cal Society of Japan, 71
Aihara, H., Arimoto, N., Armstrong, R., et al. 2017, Pub-
lications of the Astronomical Society of Japan, 70
[https://academic.oup.com/pasj/article-pdf/70/SP1/S4/23692189/psx066.pdf ],
s4
Birrer, S., Millon, M., Sluse, D., et al. 2022
Bosch, J., Armstrong, R., Bickerton, S., et al. 2017, Publications of the Astro-
nomical Society of Japan, 70
Buckley-Geer, E. J., Lin, H., Rusu, C. E., et al. 2020, Monthly Notices of the
Royal Astronomical Society, 498, 3241–3274
Chen, G. C.-F., Fassnacht, C. D., Suyu, S. H., et al. 2022, Monthly Notices of
the Royal Astronomical Society, 513, 2349
Collaboration, T. D. E. S. 2005 [ arXiv:astro-ph/0510346 ]
Collett, T. E., Marshall, P. J., Auger, M. W., et al. 2013, Monthly Notices of the
Royal Astronomical Society, 432, 679
Coupon, J., Czakon, N., Bosch, J., et al. 2017, Publications of the Astronomical
Society of Japan, 70
Cuceu, A., Farr, J., Lemos, P., & Font-Ribera, A. 2019, Journal of Cosmology
and Astroparticle Physics, 2019, 044
De Lucia, G. & Blaizot, J. 2007, Monthly Notices of the Royal Astronomical
Society, 375, 2
Eigenbrod, A., Courbin, F., Dye, S., et al. 2006, A&A, 451, 747
Fassnacht, C. D., Gal, R. R., Lubin, L. M., et al. 2006, The Astrophysical Journal,
642, 30
Fassnacht, C. D., Koopmans, L. V . E., & Wong, K. C. 2010, Monthly Notices of
the Royal Astronomical Society, 410, 2167
Freedman, W. L., Madore, B. F., Hatt, D., et al. 2019, The Astrophysical Journal,
882, 34
Greene, Z. S., Suyu, S. H., Treu, T., et al. 2013, The Astrophysical Journal, 768,
39
Gwyn, S. D. J. 2012, The Astronomical Journal, 143, 38
Hernández-Aguayo, C., Springel, V ., Pakmor, R., et al. 2022
Hilbert, S., Hartlap, J., White, S. D. M., & Schneider, P. 2009, Astronomy &
Astrophysics, 499, 31
Holzman, B., Bauerdick, L. A. T., Bockelman, B., et al. 2017, Computing and
Software for Big Science, 1
Hsieh, B. C. & Yee, H. K. C. 2014, The Astrophysical Journal, 792, 102
Inada, N., Becker, R. H., Burles, S., et al. 2003, The Astronomical Journal, 126,
666
Kaiser, N., Squires, G., & Broadhurst, T. 1995, The Astrophysical Journal, 449,
460
Article number, page 11 of 12A&A proofs: manuscript no. main
Kusnierz, J., Padulano, V . E., Malawski, M., et al. 2022, in 2022 22nd IEEE In-
ternational Symposium on Cluster, Cloud and Internet Computing (CCGrid)
(IEEE)
McCully, C., Keeton, C. R., Wong, K. C., & Zabludo ff, A. I. 2017, The Astro-
physical Journal, 836, 141
Millon, M., Galan, A., Courbin, F., et al. 2020, Astronomy & Astrophysics, 639,
A101
Nishizawa, A. J., Hsieh, B.-C., Tanaka, M., & Takata, T. 2020
Park, J. W., Birrer, S., Ueland, M., et al. 2022
Riess, A. G., Casertano, S., Yuan, W., et al. 2021, The Astrophysical Journal
Letters, 908, L6
Rusu, C. E., Fassnacht, C. D., Sluse, D., et al. 2017, Monthly Notices of the
Royal Astronomical Society, 467, 4220
Rusu, C. E., Wong, K. C., Bonvin, V ., et al. 2019, Monthly Notices of the Royal
Astronomical Society, 498, 1440
Sluse, D., Rusu, C. E., Fassnacht, C. D., et al. 2019, Monthly Notices of the
Royal Astronomical Society, 490, 613
Springel, V ., White, S. D. M., Jenkins, A., et al. 2005, Nature, 435, 629
Suyu, S. H., Marshall, P. J., Auger, M. W., et al. 2010, The Astrophysical Journal,
711, 201
Tanaka, M., Coupon, J., Hsieh, B.-C., et al. 2017, Pub-
lications of the Astronomical Society of Japan, 70
[https://academic.oup.com/pasj/article-pdf/70/SP1/S9/23692265/psx077.pdf ],
s9
Tihhonova, O., Courbin, F., Harvey, D., et al. 2018, Monthly Notices of the Royal
Astronomical Society, 477, 5657
Treu, T. & Marshall, P. J. 2016, The Astronomy and Astrophysics Review, 24
Wong, K. C., Sonnenfeld, A., Chan, J. H. H., et al. 2018, The Astrophysical
Journal, 867, 107
Article number, page 12 of 12