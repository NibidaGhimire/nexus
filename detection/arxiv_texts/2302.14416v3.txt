DREAM: Efficient Dataset Distillation by Representative Matching
Yanqing Liu1,2*Jianyang Gu1,2*Kai Wang1†Zheng Zhu3Wei Jiang2Yang You1‡
1National University of Singapore2Zhejiang University3Tsinghua University
{yanqing liu, gu jianyang, jiangwei zju }@zju.edu.cn
{kai.wang, youy }@comp.nus.edu.sg zhengzhu@ieee.org
Code: https://github.com/lyq312318224/DREAM
Abstract
Dataset distillation aims to synthesize small datasets
with little information loss from original large-scale ones
for reducing storage and training costs. Recent state-of-
the-art methods mainly constrain the sample synthesis pro-
cess by matching synthetic images and the original ones
regarding gradients, embedding distributions, or training
trajectories. Although there are various matching objec-
tives, currently the strategy for selecting original images
is limited to naive random sampling. We argue that ran-
dom sampling overlooks the evenness of the selected sam-
ple distribution, which may result in noisy or biased match-
ing targets. Besides, the sample diversity is also not con-
strained by random sampling. These factors together lead
to optimization instability in the distilling process and de-
grade the training efficiency. Accordingly, we propose a
novel matching strategy named as Dataset distillation by
REpresent AtiveMatching (DREAM), where only represen-
tative original images are selected for matching. DREAM
is able to be easily plugged into popular dataset distilla-
tion frameworks and reduce the distilling iterations by more
than 8 times without performance drop. Given sufficient
training time, DREAM further provides significant improve-
ments and achieves state-of-the-art performances.
1. Introduction
Deep learning has made remarkable achievements in the
computer vision society [21, 12, 39, 30, 46, 17, 8, 59], and
the success is closely related to a large amount of efforts in
data collection and annotation. But along with the progress
of these efforts, the huge amount of data, in turn, becomes a
barrier to both storage and training [56, 20]. Many methods
are introduced to reduce the scale of datasets [51, 43, 37,
*Equal contribution.
†Project lead.
‡Corresponding author.
0 10 20 30 40
/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000b/uni0000002f/uni00000015/uni0000000c050100150200250/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni00000019/uni00000011/uni00000017/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni0000001a/uni00000011/uni00000014
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni00000014/uni00000013/uni00000011/uni00000013/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni0000001b/uni00000011/uni0000001b/uni00000024/uni0000004f/uni0000004f/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056
/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056(a) The gradient norm distribution of the plane class in CIFAR10.
Random 
Sampling
DREAM
Matched original images Synthetic images
(b) The migration of synthetic samples during training.
Figure 1: Samples on the decision boundaries usually pro-
vide larger gradients, which biases the gradient matching
optimization. Random sampling (left) overlooks the even-
ness of of the selected sample distribution, resulting in un-
stable optimization process of the synthesized samples. By
only matching with proper gradients from representative
original samples, our proposed DREAM (right) greatly im-
proves the training efficiency of dataset distillation tasks.
Best viewed in color.
9]. Among these, dataset distillation, aiming at condensing
large-scale datasets into smaller ones with little information
loss, has become a hot topic to tackle the problem of data
burden [4, 50, 25, 6, 13].
Dataset distillation methods are roughly divided into two
categories: coreset-based and optimization-based. Coreset-
based method employ certain metrics to heuristically select
samples for representing the original dataset [27, 44]. How-arXiv:2302.14416v3  [cs.CV]  30 Aug 2023ever, it is difficult to rely on a small proportion of original
samples to contain the information of the whole dataset, re-
sulting in low compression rate. Optimization-based meth-
ods alleviate the defect by incorporating image synthesis to
introduce more information into single images [51]. Specif-
ically, these methods initialize a small amount of learnable
image tensors and update them through matching the train-
ing gradients [56, 25], embedding distributions [55, 50] or
training trajectories [4, 13] with the original images.
Although the optimization-based methods achieve con-
siderable performance as well as compression ratio, the dis-
tillation process itself still requires a large amount of time.
We analyze the problem from the strategy of selecting orig-
inal images for matching, which is mostly set as random
sampling in previous works [56]. We argue that random
sampling overlooks the evenness of the selected sample dis-
tribution. On the one hand, the matching optimization may
be overly prone to certain samples with dominant matching
targets, such as boundary samples with larger training gra-
dients [50]. On the other hand, the sample diversity inside
a mini-batch is also not constrained, leading to potential in-
formation insufficiency. These factors together result in op-
timization instability of the dataset distillation process, and
degrade the training efficiency.
Accordingly, we propose a novel matching strategy
named as Dataset distillation by REpresent AtiveMatching
(DREAM) to address the aforementioned training efficiency
issue. Specifically, a clustering process inside each class is
conducted at intervals to generate sub-clusters reflecting the
sample distribution. The sub-cluster centers, which not only
are representative for surrounding samples, but also evenly
cover the whole class distribution, are selected for match-
ing. As shown in Fig. 1a, the gradient distribution of the
selected samples contains less variation. By only match-
ing with representative samples, DREAM largely reduces
the instability during training, and provides a smoother and
more robust distillation process. For the synthetic image
initialization, we adopt a similar clustering-based strategy,
where the center sample is selected from each sub-cluster,
which further accelerates the training process.
DREAM can be easily plugged into popular dataset dis-
tillation frameworks. Compared with commonly adopted
random matching, DREAM significantly improves the
training efficiency in the distilling process. We conduct ex-
tensive experiments to validate that it only takes less than
one eighth of the iterations for DREAM to obtain compa-
rable performance with the baseline methods. In addition,
given sufficient training iterations, DREAM further boosts
the performance to surpass other state-of-the-art methods.
Our main contributions are summarized as:
• We analyze the training efficiency of optimization-
based dataset distillation from the strategy of selecting
original samples for matching.• We propose a Dataset distillation by REpresentAtive
Matching (DREAM) strategy. By only matching rep-
resentative images, DREAM accelerates the training
process by more than 8 ×without performance drop.
• DREAM is able to be easily plugged into a variety of
dataset distillation frameworks. Extensive experiments
prove that DREAM consistently improves the perfor-
mance of the distilled dataset.
2. Related Works
2.1. Dataset Distillation
Dataset distillation can be roughly divided into 2 cate-
gories: coreset-based and optimization-based.
Coreset-based methods select a certain proportion of
data based on certain metrics [18, 5]. Lapedriza et al. mea-
sure the importance of the sample by the benefits obtained
from training the model on the sample [27]. Toneva et al.
find that samples have different forgetting characteristics
and the easily forgotten samples have larger information
amount [44]. Coresets are also utilized to solve continual
learning [38, 1, 52] and active learning tasks [41]. Besides,
Shleifer et al. accelerate the search of neural network ar-
chitecture by selecting a group of “easier” samples [42].
Although coreset-based methods are practical to apply, it
is hard to obtain rich information from a small amount of
original samples. Therefore, coreset-based methods are re-
stricted from further reducing the compression ratio.
Optimization-based methods implement dataset distil-
lation by synthesizing image samples constrained by vari-
ous optimization targets. Wang et al. raise the dataset dis-
tillation concept from the optimization aspect, and update
the synthetic images in a meta-learning style [51]. Multiple
works are then proposed to constrain the image generation
by matching training gradients [56, 54, 23], embedding dis-
tributions [55, 50] and training trajectories [4] with original
images. IDC injects more information into synthetic sam-
ples under the limit of fixed storage size [25]. Nguyen et
al. build up a distributed meta-learning framework and in-
corporate the kernel approximation methods [35]. RFAD
speeds up the computation by introducing a random fea-
ture approximation [31]. HaBa employs data hallucination
networks to construct base images and improves the repre-
sentation capability of distilled datasets [29]. FRePo intro-
duces an efficient meta-gradient computation method and
a “model pool” to alleviate the overfitting [60]. DiM [49]
transfers knowledge by distilling datasets into generative
models. Optimization-based methods largely improve the
compression ratio via fusing more information into syn-
thetic images. However, recent state-of-the-art methods re-
quire a large number of iterations to obtain desired vali-
dation accuracy, indicating low training efficiency. In thiswork, we focus on designing a novel matching strategy for
more efficient dataset distillation training.
2.2. Clustering
Clustering divides samples into groups in an unsuper-
vised manner [40]. K-means [15, 2] specifies the number of
target clusters, and optimizes the partition to obtain clusters
with similar sizes [19]. DBSCAN, based on density, does
not require the number of target clusters in advance. The
clusters are formed by gradually adding data points within
the tolerance range. [14]. It is applicable to dataset of any
shape, yet the size of the generated clusters is unstable, out-
liers are excluded from clusters, and close clusters may be
merged. Hierarchical clustering methods include Agglom-
erative and Divisive. The former fuses multiple clusters un-
til a certain condition is met, and the latter divides a cluster
through segmentation [11].
3. Method
Aiming at addressing the training efficiency problem
for dataset distillation tasks, we propose a novel Dataset
distillation by REpresentAtive Matching (DREAM) strat-
egy. By only matching the representative original images,
DREAM reduces the optimization instability, and achieves
a smoother and more robust training process. In this section,
we orderly introduce the basic training schemes of dataset
distillation, our observations on the training efficiency and
the detailed design of DREAM.
3.1. Preliminaries
Given a large-scale dataset T={(xi
t, yi
t)}|T |
i=1, the tar-
get of dataset distillation is generating a small surrogate
dataset S={(xi
s, yi
s)}|S|
i=1with as little information loss
as possible, where |S| ≪ |T | . The information loss is usu-
ally measured by the performance drop between training a
model with the original images Tand the surrogate set S.
The commonly adopted optimization-based methods follow
a synthetic pipeline. The surrogate set Sis first initialized
with random original images from T. Under the constraints
of matching objectives ϕ(·), the synthetic images are up-
dated to mimicking the distribution of the original images,
which is formulated as:
S∗= arg min
SD(ϕ(S), ϕ(T)), (1)
whereDis the matching metric. Typically, we select the
training gradients as the matching objective ϕ(·). Given a
random model Mθwith training parameters θ,Sis sup-
posed to give similar gradients to Tthroughout the training
process of Mθ, that is:
S∗= arg min
SD(∇θL(Mθ(A(S))),∇θL(Mθ(A(T)))),
(2)where L(·,·)is the training loss, and Ais the differen-
tiable augmentation [24, 57, 45, 58]. Practically, the match-
ing objectives are calculated on the synthetic images and a
mini-batch of original images {(xi
t, yi
t)}N
i=1sampled from
Twith the same class labels. The objective matching and
Mθtraining is conducted alternatively, such that gradients
at different training stages are matched, which forms the
inner optimization loop. The inner loop is iterated with dif-
ferent random Mθfor more varied matching gradients.
Recent literature offers various matching objectives and
achieves significant testing accuracy via training in the
small synthetic dataset [50, 4, 25]. However, the distillation
process itself still requires a large amount of training time,
indicating low training efficiency. We analyze the relation-
ship between the training efficiency and the sampled origi-
nal images for matching, and accordingly propose a novel
matching strategy.
3.2. Observations on Training Efficiency
In the dataset distillation process, the knowledge is dis-
tilled from sampled original images by matching certain ob-
jectives. The selection of original images thereby has large
influences on the training efficiency. Recent literature usu-
ally adopts random sampling for selecting the original im-
ages [56, 25]. We set gradient matching as an example and
carefully illustrate that random sampling disturbs efficient
training of the dataset distillation.
Firstly, we analyze the matching effects of samples in
different regions. Among all the samples in a class, those
near the distribution center have higher prediction accuracy,
indicating smaller backward gradients, while those on the
decision boundaries have the contrary condition. For gradi-
ent matching, the central samples provide less effective su-
pervision, while the gradients of the boundary ones largely
dominate the optimization direction. We show the training
accuracy curve of matching the synthetic images with only
the central or boundary samples in Fig. 2b. The small gradi-
ents provided by central samples soon fail to provide effec-
tive supervision. On the other hand, although the boundary
samples are essential for building decision boundaries, only
matching with them brings chaotic matching targets, which
degrades the distillation performance.
Secondly, we demonstrate that random sampling cannot
guarantee an evenly distributed mini-batch along the train-
ing process. We record the Maximum Mean Discrepancy
(MMD) between the selected mini-batch and the whole
class distribution during training in Fig. 2c. It can be ob-
served that the MMD is kept at a relatively high level, with
large fluctuations during the training process. For the gradi-
ent matching, as the mini-batch cannot effectively and con-
sistently cover the original class sample distribution, the
gradient difference of different samples are not balanced.
The matching target of a mini-batch may be biased byOriginal Images
Synthetic Images𝑝𝑡
𝑝𝑠𝑔𝑡
𝑔𝑠ℳ𝜃 ∇𝜃ℒ(⋅)forward
backward
Clustering
Match(a) The training pipeline of the proposed DREAM strategy.
44
40
0100 200 300 400 500 600 700 800 9001000
Iteration48525668
64
60AccuracyAccuracy -Iteration
Ours
Random
Central 
Boundary
(b) The accuracy curve with different strate-
gies for selecting original images.
010 20 30 40 50 60 70 80 90
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni000000511234/uni00000030/uni00000030/uni00000027/uni00000030/uni00000030/uni00000027/uni00000010/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000032/uni00000058/uni00000055/uni00000056
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050(c) The MMD curve between the sampled
mini-batch and the corresponding class data.
40
 20
 0 20 40
/uni00000046/uni00000052/uni00000050/uni00000053/uni00000010/uni0000001440
20
02040/uni00000046/uni00000052/uni00000050/uni00000053/uni00000010/uni00000015
/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni00000010/uni00000036/uni00000031/uni00000028(d) Example clustering and sub-cluster cen-
ter results of DREAM.
Figure 2: The original images obtained by random sampling have uneven distributions, which may result in noisy or biased
matching targets. Besides, the coverage of random sampling on the whole sample space is low and has large fluctuations
during training. Comparatively, the centers selected by DREAM (stars) are representative for corresponding sub-clusters,
and are evenly distributed over the whole class feature space. Experiments for (b) and (c) are conducted under 10 images-
per-class setting on CIFAR-10. Best viewed in color.
boundary samples with larger training gradients, which re-
sults in unstable supervision.
Besides, an unevenly distributed mini-batch also indi-
cates relatively poor sample diversity. Information redun-
dancy at dense regions and information lack at sparse re-
gions make the mini-batch insufficient to represent the orig-
inal data. The above factors result in optimization instabil-
ity of the distillation process, and hence degrade the training
efficiency. Since randomly sampling original images dis-
turbs the training efficiency for dataset distillation training,
we propose to design a novel strategy to construct mini-
batches with even and diverse distribution for matching.
3.3. Representative Matching
Based on the purpose of achieving stable and fast opti-
mization, only representative original images are selected
for gradient matching. The selection of representative im-
ages are supposed to obey the following two principles. On
the one hand, the selected images should be evenly dis-
tributed to avoid biased matching targets. On the other
hand, while ensuring diversity, the selected samples should
reflect the overall sample distribution of the class as accu-
rately as possible.
Therefore, we employ a clustering process for selecting
representative original images. Out of the considerations
of uniform sub-cluster sizes and distribution, without lossof generality, we adopt K-Means [15, 2, 36] for dividing
sub-clusters. As shown in Fig. 2d, the clustering is con-
ducted inside each class to generate Nsub-clusters that re-
flect the sample density. Nis a pre-defined hyper-parameter
for the mini-batch size of real images. The sub-cluster cen-
ters evenly cover the sample space of the whole class, and
simultaneously provide sufficient diversity, which perfectly
meets the above principles.
The complete training pipeline is illustrated in Fig. 2a.
The clustering-selected original mini-batch and the syn-
thetic images with the same class label are passed through
the random model Mθto obtain prediction scores ptand
ps. Subsequently calculate the classification losses and their
corresponding gradients. The gradient differences are back-
warded to update synthetic images according to Eq. 2. Con-
sidering the brought extra time cost, the clustering process
is conducted every Iintiterations.
Additionally, at the beginning of the training process, we
cluster the data of each class into sub-clusters correspond-
ing to the pre-defined images-per-class number. We select
the center samples of each sub-cluster as the initialization
of the synthetic images. A more balanced clustering-based
initialization better reflects the data distribution, and accel-
erates the convergence from the very beginning of the train-
ing process.4. Experiments
4.1. Datasets and Implementation Details
We verify the effectiveness of our method on multi-
ple popular dataset distillation benchmarks, including CI-
FAR10 [26], CIFAR100 [26], SVHN [33], MNIST [28],
FashionMNIST [53] and TinyImageNet [10]. For evalua-
tion, we train a model on the distilled synthetic images and
test it on the original testing images. Top-1 accuracy is re-
ported to show the performance.
Without specific designation, the experiment is con-
ducted on 3-layer convolutional networks (ConvNet-3) [16]
with 128 filters and instance normalization [47]. The match-
ing mini-batch size for original images is set as 128. By
default we set IDC [25] as the baseline method. The gra-
dient matching metric Din Eq. 2 is empirically set as the
mean squared error for CIFAR-10, CIFAR-100, TinyIma-
geNet and SVHN. For MNIST and FashionMNIST, Dis set
as the mean absolute error [25]. We conduct 1,200 matching
iterations in total, inside each of which 100 inner loops are
conducted. SGD is set as the optimizer, with a learning rate
of 0.005. For clustering, we employ the matching model
for feature extraction. The clustering interval Iintis set as
10 iterations, whose sensitiveness is analyzed in Sec. 4.3.
We also analyze the influence of different sampling strategy
from the sub-clusters in Sec. 4.3. For evaluation, we train
a network for 1,000 epochs on the distilled images with a
learning rate of 0.01. We perform 5 experiments and report
the mean and standard deviation of the results.
4.2. Comparison with State-of-the-art Methods
We compare the distilled synthetic dataset performance
of DREAM and other state-of-the-art (SOTA) coreset-based
and optimization-based methods on multiple datasets with
different images-per-class (IPC) settings in Tab. 1. Besides,
on TinyImageNet, we compare DREAM with DM [55] and
MTT [4] in Tab. 2. Under all experiment circumstances,
the proposed DREAM consistently surpasses other SOTA
methods. With a small IPC setting especially, under the
guidance of proper gradients, DREAM is more robust than
other methods, which proves the effectiveness of the rep-
resentative matching strategy. Further narrowing the per-
formance gap between small-scale distilled datasets and the
original ones indicates that the information loss of dataset
distillation is reduced. More detailed comparisons are in-
cluded in the supplementary material.
4.3. Ablation Study and Analysis
Extended experiments are designed to verify the effec-
tiveness of our proposed DREAM strategy. Without spe-
cific designation, the experiment is conducted under the 10
IPC setting on CIFAR-10 dataset.Component Combination Evaluation. Firstly, we ver-
ify the isolated effects of each component in our proposed
DREAM strategy in Tab. 3. Under the same initialization,
our proposed representative matching strategy largely im-
proves the final dataset performance. Comparatively, the
clustering-based initialization offers a large performance
lead before the training begins, yet eventually brings limited
improvements. Nevertheless, it still provides stable boosts
and accelerates the training convergence added on the rep-
resentative matching to form the whole DREAM method.
Combining all the components, the full DREAM method
cuts the required iterations to achieve the baseline perfor-
mance by more than 8 times.
Additionally, in Fig. 2 we further illustrate the effec-
tiveness of DREAM. Fig. 2b shows that by simply assign-
ing samples from the sub-clusters as initialization of syn-
thetic images, the validation performance surpasses random
initialization by a large margin. Under the joint effect of
representative matching and clustering-based initialization,
DREAM achieves the final performance of random sam-
pling with less than eighth of training iterations, demon-
strating a significant training efficiency improvement. Con-
tinuing increasing the training iterations, DREAM further
improves the dataset performance by applying proper gra-
dient as supervision.
From the sample distribution perspective, Fig. 2c demon-
strates that the original images selected by DREAM con-
sistently show lower MMD scores with the original distri-
bution with less fluctuations, compared with random sam-
pling. The smaller fluctuations validates that sub-cluster
centers effectively and stably cover the feature distribution,
and reduces the noise at the sample level during the training
process. With sufficient sample diversity, distribution even-
ness and appropriate gradient supervision, DREAM ensures
a smoother and more robust optimization process for dataset
distillation training.
For better illustration of the universality of DREAM,
we apply the representative matching and clustering-based
initialization to some other baseline methods and receive
similar effects in Tab. 3. The accuracy curve comparisons
are presented in the supplementary material. It proves that
DREAM is able to be easily plugged into dataset distillation
frameworks and help improve the training efficiency.
Cross Architecture Generalization Analysis. It has
been a problem for previous optimization-based dataset dis-
tillation works to generalize across architectures as the syn-
thetic images would over-fit to the model utilized for gradi-
ent matching [56, 25]. In Tab. 4 we demonstrate the cross
architecture performance of our proposed DREAM strategy.
We distill the dataset with ConvNet-3 and ResNet-10 [21],
and validate the performance on ConvNet-3, ResNet-10 and
DenseNet-121 [22].
DREAM surpasses the compared methods on both theTable 1: Top-1 accuracy of test models trained on distilled synthetic images on multiple datasets. The distillation training is
conducted with ConvNet-3.†denotes the reported error range is reproduced by us.
IPC Ratio %Coreset Selection Training Set Synthesis Whole
Random Herding DC [56] DSA [54] DM [55] CAFE [50] MTT [4] IDC [25] DREAM Dataset
MNIST1 0.017 64.9±3.589.2±1.691.7±0.588.7±0.689.7±0.693.1±0.3 - 94.2 ±0.2†95.7±0.3
10 0.17 95.1±0.993.7±0.397.4±0.297.8±0.197.5±0.197.2±0.3 - 98.4 ±0.1†98.6±0.199.6±0.0
50 0.83 97.9±0.294.8±0.298.8±0.299.2±0.198.6±0.198.6±0.2 - 99.1 ±0.1†99.2±0.1
FashionMNIST1 0.017 51.4±3.867.0±1.970.5±0.670.6±0.6 - 77.1 ±0.9 - 81.0 ±0.2†81.3±0.2
10 0.17 73.8±0.771.1±0.782.3±0.484.6±0.3 - 83.0 ±0.4 - 86.0 ±0.3†86.4±0.393.5±0.1
50 0.83 82.5±0.771.9±0.883.6±0.488.7±0.2 - 84.8 ±0.4 - 86.2 ±0.2†86.8±0.3
SVHN1 0.014 14.6±1.620.9±1.331.2±1.427.5±1.4 - 42.6 ±3.3 - 68.5 ±0.9†69.8±0.8
10 0.14 35.1±4.150.5±3.376.1±0.679.2±0.5 - 75.9 ±0.6 - 87.5 ±0.3†87.9±0.495.4±0.1
50 0.7 70.9±0.972.6±0.882.3±0.384.4±0.4 - 81.3 ±0.3 - 90.1 ±0.1†90.5±0.1
CIFAR101 0.02 14.4±2.021.5±1.228.3±0.528.8±0.726.0±0.830.3±1.146.3±0.850.6±0.4†51.1±0.3
10 0.2 26.0±1.231.6±0.744.9±0.552.1±0.548.9±0.646.3±0.665.3±0.767.5±0.569.4±0.484.8±0.1
50 1.0 43.4±1.040.4±0.653.9±0.560.6±0.563.0±0.455.5±0.671.6±0.274.5±0.174.8±0.1
CIFAR1001 0.2 4.2±0.3 8.4±0.312.8±0.313.9±0.311.4±0.312.9±0.324.3±0.3 - 29.5±0.3
10 2 14.6±0.517.3±0.325.2±0.332.3±0.329.7±0.327.8±0.340.1±0.445.1±0.4†46.8±0.756.2±0.3
50 10 30.0±0.433.7±0.5 - 42.8 ±0.443.6±0.437.9±0.347.7±0.2 - 52.6±0.4
Table 2: Top-1 accuracy of test models trained on distilled
synthetic images on TinyImageNet. The distillation training
is conducted with ConvNet-3.
IPC Ratio % DM [55] MTT [4] DREAM Whole
1 0.017 3.9±0.28.8±0.310.0±0.437.6±0.450 0.83 24.1±0.328.0±0.329.5±0.3
Table 3: Ablation study on the components of the proposed
DREAM. RM indicates Representative Matching, and Init
stands for clustering-based initialization. “Iter” stands for
the required iterations to achieve the baseline performance.
CompTop-1 IterCompTop-1RM Init RM Init
IDC- - 67.5 ±0.51000DC- - 44.9 ±0.5
✓ - 68.9 ±0.5350 ✓ ✓ 45.9±0.3
-✓68.1±0.3750DSA- - 52.1 ±0.5
✓ ✓ 69.4±0.4150 ✓ ✓ 53.1±0.4
absolute performance and the performance drop when ap-
plying the distilled dataset on an unseen architecture. The
strong cross architecture generalization capability verifies
that DREAM helps build a more reasonable distilled dataset
compared to random sampling.
Sampling Strategy Analysis. Representative matching
conducts clustering for each class and samples original im-
ages from the sub-clusters to form a mini-batch. We analyze
the influence of different sampling strategy on the train-
ing results in Tab. 5 and Fig. 3. Among each sub-cluster,
the top- nsamples closest to the center are selected. ByTable 4: Ablation study on cross architecture distilled
dataset performance of the proposed DREAM strategy. The
dataset is first distilled on a model D and then validated on
another model T. † denotes the result is reproduced by us.
D\T Conv-3 Res-10 Dense-121
MTT [4]Conv-3 64.3 ±0.7 34.5±0.6†41.5±0.5†
Res-10 44.2 ±0.3†20.4±0.9†24.2±1.3†
IDC [25]Conv-3 67.5 ±0.5 63.5±0.1 61.6±0.6
Res-10 53.6 ±0.6†50.6±0.9†51.7±0.6†
DREAMConv-3 69.4±0.4 66.3±0.8 65.9±0.5
Res-10 53.7±0.6 51.0±0.9 52.8±0.6
Table 5: Ablation study on different sampling strategy to
form a mini-batch from sub-clusters.
Sub-cluster number N
32 64 128 256
167.2±0.368.5±0.169.4±0.468.9±0.2
Samples per 2 67.7±0.368.6±0.369.2±0.7 -
sub-cluster n467.7±0.468.7±0.4 - -
867.5±0.3 - - -
grouping different sub-cluster number and samples per sub-
cluster, we are able to obtain original image mini-batches
different in scale and diversity. As observed in the results,
by representative matching the dataset performance is gen-
erally stable, and receives improvements to certain extent
over the baseline (67.5).
Compared in more detail, with a small sub-cluster num-0 5 10 15 20 25 30 35 40020406080100120140160/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni00000017/uni00000011/uni00000017/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni00000018/uni00000011/uni00000014
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni00000014/uni00000013/uni00000011/uni00000013/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni0000001b/uni00000011/uni0000001b/uni00000036/uni00000058/uni00000045/uni00000010/uni00000046/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000031/uni00000003/uni00000020/uni00000003/uni00000016/uni00000015
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056
/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056
0 5 10 15 20 25 30 35 40020406080100120/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni00000019/uni00000011/uni00000017/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni0000001a/uni00000011/uni00000014
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni00000014/uni00000013/uni00000011/uni00000013/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni0000001b/uni00000011/uni0000001b/uni00000036/uni00000058/uni00000045/uni00000010/uni00000046/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000031/uni00000003/uni00000020/uni00000003/uni00000014/uni00000015/uni0000001b
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056
/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056
0 5 10 15 20 25 30 35 40
/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000b/uni0000002f/uni00000015/uni0000000c020406080/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni0000001b/uni00000011/uni00000015/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni0000001b/uni00000011/uni00000015
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051/uni0000001d/uni00000003/uni00000014/uni00000013/uni00000011/uni00000013/uni0000000f/uni00000003/uni00000036/uni00000057/uni00000047/uni0000001d/uni00000003/uni0000001b/uni00000011/uni0000001b/uni00000036/uni00000058/uni00000045/uni00000010/uni00000046/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000031/uni00000003/uni00000020/uni00000003/uni00000015/uni00000018/uni00000019
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056
/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030/uni00000003/uni00000056/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056Figure 3: The gradient distribution comparison between
random sampling and our proposed DREAM strategy un-
der different sub-cluster sample number N. Best viewed in
color.
berN= 32 , the sub-cluster centers are more likely to be
distributed in areas with smaller gradients, as shown in the
first row of Fig. 3. As the random model Mθis trained,
these samples gradually fail to provide effective gradients
for supervision, resulting in a sub-optimal performance.
Oppositely, a larger sub-cluster number N= 256 involves
a distribution closer to random sampling, which brings a
small performance drop, as shown in the last row of Fig. 3.
Due to memory limitations, it is not applicable to further
increase N, but it is conceivable that the extreme condition
should yield similar results to random sampling. On the
other hand, the sample number per sub-cluster nhas only
a slight effect on the results. The group of 1 center sample
per sub-cluster and 128 sub-clusters in total is proved to ob-
tain the optimal gradient supervision as in the second row
of Fig. 3, and is chosen for mini-batch composition.
Training Stability Analysis. In order to more intu-
itively demonstrate the effects of the proposed DREAM
strategy on the training process, we visualize the feature mi-
gration of DREAM and random sampling in Fig. 4a. Specif-
ically, we randomly select a synthetic image as initialization
and record its updated version every 10 iterations. We em-
ploy a random network to extract the features of all images,
and calculate the Euclidean distance between adjacent ver-
sions of images. For DREAM, at the beginning of the train-
ing process, under the direction of proper gradients, the syn-
thetic image goes through a larger migration. Within 100
iterations, the synthetic image has reached a relatively op-timal position, and makes subsequent fine-tuning. On the
contrary, there are still large fluctuations for the synthetic
image matched with randomly sampled original images in
the late training period, partly due to the noisy matching
targets generated by uneven mini-batches.
Clustering Interval Sensitivity Analysis. We evaluate
the influence of different clustering interval Iinton the final
dataset performance in Fig. 4b. Conducting clustering at
every iteration leads to the best performance, while adding
the clustering interval until 10 brings mild influences. As
there is an obvious top-1 accuracy degradation when the in-
terval is further increased to 20, we select an interval of 10
to balance the distilled dataset performance and the extra
calculation cost. More analysis on the computational cost
of clustering is included in the supplementary material.
Experimental results on ImageNet-1K. We com-
pare DREAM with the current state-of-the-art method
TESLA [7] on ImageNet-1K in Tab. 6. The experimental
setting is the same as in TESLA. DREAM shows excellent
performance on large datasets.
Table 6: Results on ImageNet.
Method TESLA DREAM
Acc (IPC=10) 17.8 18.5
Differences from DC-BENCH[6]. We provide a de-
tailed comparison between DREAM and DC-BENCH to
clarify their distinctions. DC-BENCH solely concentrates
on a better initialization and lacks specific designs for the
subsequent matching-based optimization, while DREAM
selects representative samples for matching and enables the
realization of a fully efficient training process for distil-
lation. Furthermore, DREAM conducts extensive exper-
iments to analyze the impact of cluster number, sample
number per cluster and clustering interval. DC-BENCH
achieves comparable performance using 30 %of iterations,
whereas DREAM achieves similar results with only 10-
20%iterations. Additionally, given sufficient training time,
DREAM further achieves up to 3.7 %and 5.8 %accu-
racy improvements for gradient matching and distribution
matching respectively which surpass DC-BENCH’s 1.3 %.
4.4. Visualizations
Gradient Difference Curve. As the dataset distillation
training is constrained by matching the training gradients, a
smaller gradient difference also indicates a better matching
effect. Therefore, we also visualize the gradient difference
curve of the dataset distillation in Fig. 4c, which is calcu-
lated by the training loss Eq. 2. We add the DREAM strat-
egy to DC, DSA and IDC methods. Throughout the training
process, DREAM holds a smaller gradient difference com-
pared with the baseline methods. On the one hand, it veri-
fies the effectiveness of DREAM on improving the training0200 400 600 8001000
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni000000510.51.01.52.02.53.03.54.0/uni0000002f/uni00000052/uni0000004a/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050/uni0000004c/uni00000046/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000003/uni00000030/uni0000004c/uni0000004a/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000032/uni00000058/uni00000055/uni00000056
/uni00000035/uni00000044/uni00000051/uni00000047/uni00000052/uni00000050(a)
1 2 4 6 810 20
/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000059/uni00000044/uni0000004f60626466687072/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000008/uni0000000c
9092949698100
/uni00000037/uni00000052/uni00000053/uni00000010/uni00000018/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni0000000b/uni00000008/uni0000000c
/uni00000037/uni00000052/uni00000053/uni00000010/uni00000014 /uni00000037/uni00000052/uni00000053/uni00000010/uni00000018 (b)
0 200 400 600 800 1000
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni0000005180100120140160/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000003/uni00000027/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni0000000b/uni00000027/uni00000026/uni00000012/uni00000027/uni00000036/uni00000024/uni0000000c
4681012
/uni0000002a/uni00000055/uni00000044/uni00000047/uni0000004c/uni00000048/uni00000051/uni00000057/uni00000003/uni00000027/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni0000000b/uni0000002c/uni00000027/uni00000026/uni0000000c
/uni00000027/uni00000026
/uni00000027/uni00000026/uni0000000e/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030
/uni00000027/uni00000036/uni00000024
/uni00000027/uni00000036/uni00000024/uni0000000e/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030
/uni0000002c/uni00000027/uni00000026
/uni0000002c/uni00000027/uni00000026/uni0000000e/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030 (c)
20 40 60 80 100
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000048/uni0000005620406080/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000027/uni00000035/uni00000028/uni00000024/uni00000030
/uni0000002c/uni00000027/uni00000026
/uni0000002c/uni00000027/uni00000026/uni00000010
/uni0000002b/uni00000048/uni00000055/uni00000047/uni0000004c/uni00000051/uni0000004a
/uni00000027/uni00000036/uni00000024 (d)
Figure 4: (a): The feature migration during the training process. (b): Ablation study on different clustering interval. (c): The
training loss curve during the training process. (d): The continual learning accuracy curve.
Ours Random
Figure 5: The sample distribution comparison on the final
distilled images (marked as red stars) between our proposed
DREAM (left) and random sampling (right).
efficiency to reduce the gradient difference in limited itera-
tions. On the other hand, the large fluctuations of the base-
line methods also validate the existence of noisy gradients
generated by random sampling.
Sample Distribution Visualization. In order to more
intuitively demonstrate the effectiveness of our proposed
DREAM on generating synthetic sets well covering the
original sample distribution, we visualize the t-SNE graphs
of the synthetic images for both random sampling and
DREAM. As shown in Fig. 5, the final distribution con-
strained by DREAM strategy evenly cover the whole class,
while random sampling generates biased optimization re-
sults. Furthermore, a large percentage of samples are pulled
to the distribution edge in the random sampling results,
which also validates that the matching is biased by bound-
ary samples with larger gradients. By consistently provid-
ing proper gradient supervision, DREAM achieves a more
diverse and robust distillation result.
Synthetic Image Visualization. In order to more intu-
itively demonstrate the effects of DREAM on the distilled
images, we compare the distillation results of adding the
proposed DREAM strategy or not in Fig. 6. DREAM im-
proves the quality of the distilled datasets from two perspec-
tives. Firstly, the images optimized by DREAM show more
Figure 6: The distilled dataset comparison between DC
(Upper row) and DC with DREAM strategy (Bottom row)
on CIFAR-10 (plane, car, dog, cat classes). DREAM intro-
duces more obvious categorical characteristics and variety
to the distilled image. Best viewed in color. More visual-
ization is provided in supplementary material.
obvious categorical characteristics. Secondly, DREAM in-
troduces more variety to the distilled images. With these
two improvements, DREAM helps the distilled datasets to
obtain better validation performance.
4.5. Application on Continual Learning
Dataset distillation generates compact datasets that are
able to represent the original ones, which can thus be ap-
plied to continual learning problems [38, 1, 52, 25]. We
further validate the effectiveness of the proposed DREAM
strategy on the continual learning scenarios in Fig. 4d. Fol-
lowing the settings in [56, 25], we conduct a 5-step class-incremental experiment on CIFAR-100, each step with 20
classes. For better demonstrating the generalization capa-
bility of DREAM, the distillation synthesis is conducted on
ConvNet-3, and the validation on ResNet-10.
DREAM consistently maintains performance advantages
over other approaches throughout the training process, and
the performance gap is further enlarged as the learnt class
number is gradually increased. It proves that better dis-
tillation quality helps the model construct clearer decision
boundaries and memorize the discriminative information.
5. Conclusion
In this paper, we propose a novel Dataset distillation
by REpresentAtive Matching (DREAM) strategy to ad-
dress the training efficiency problem for dataset distillation.
By only matching with the representative original images,
DREAM reduces the optimization instability, and reaches a
smoother and more robust training process. It is able to be
easily plugged into popular dataset distillation frameworks
to reduce the training iterations by more than 8 times with-
out performance drop. The stable optimization also pro-
vides higher final performance and generalization capabil-
ity. The more efficient matching allows future works to de-
sign more complicated matching metrics.
6. Limitations and Future Works
Although the proposed DREAM strategy significantly
improves the training efficiency of optimization-based
dataset distillation methods, the calculation burden is still
large when the image size and the class number increases.
It is difficult for these methods to handle ultra large-scale
datasets like ImageNet [10], even if the training efficiency
has been improved by DREAM. We will explore more
resource-friendly ways to conduct dataset distillation in fu-
ture works.
Acknowledgement
This research is supported by the National Research
Foundation, Singapore under its AI Singapore Programme
(AISG Award No: AISG2-PhD-2021-08- 008). Yang You’s
research group is being sponsored by NUS startup grant
(Presidential Young Professorship), Singapore MOE Tier-1
grant, ByteDance grant, ARCTIC grant, SMI grant and Al-
ibaba grant. The research is also supported by the National
Natural Science Foundation of China (No. 62173302).
References
[1] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. In NeurIPS , pages 11817–11826, 2019. 2, 8[2] David Arthur and Sergei Vassilvitskii. k-means++: The
advantages of careful seeding. Technical report, Stanford,
2006. 3, 4
[3] Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales.
Flexible dataset distillation: Learn labels instead of images.
arXiv preprint arXiv:2006.08572 , 2020. 13
[4] George Cazenavette, Tongzhou Wang, Antonio Torralba,
Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by
matching training trajectories. In CVPR , pages 4750–4759,
2022. 1, 2, 3, 5, 6, 13
[5] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baha-
ran Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec,
and Matei Zaharia. Selection via proxy: Efficient data se-
lection for deep learning. arXiv preprint arXiv:1906.11829 ,
2019. 2
[6] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Dc-
bench: Dataset condensation benchmark. In NeurIPS , 2022.
1, 7
[7] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling
up dataset distillation to imagenet-1k with constant memory.
InInternational Conference on Machine Learning , pages
6565–6590. PMLR, 2023. 7
[8] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and
Michael Felsberg. Eco: Efficient convolution operators for
tracking. In CVPR , pages 6638–6646, 2017. 1
[9] Zhou Daquan, Kai Wang, Jianyang Gu, Xiangyu Peng,
Dongze Lian, Yifan Zhang, Yang You, and Jiashi Feng.
Dataset quantization. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , 2023. 1
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , pages 248–255. Ieee, 2009. 5, 9
[11] Chris Ding and Xiaofeng He. Cluster merging and splitting
in hierarchical clustering algorithms. In ICDM. , pages 139–
146. IEEE, 2002. 3
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1
[13] Jiawei Du, Yidi Jiang, Vincent Y . F. Tan, Joey Tianyi Zhou,
and Haizhou Li. Minimizing the accumulated trajectory error
to improve dataset distillation. In CVPR , pages 3749–3758,
2023. 1, 2
[14] Martin Ester, Hans-Peter Kriegel, J ¨org Sander, Xiaowei Xu,
et al. A density-based algorithm for discovering clusters in
large spatial databases with noise. In KDD , pages 226–231,
1996. 3
[15] Edward W Forgy. Cluster analysis of multivariate data: effi-
ciency versus interpretability of classifications. Biometrics ,
21:768–769, 1965. 3, 4
[16] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot
visual learning without forgetting. In CVPR , pages 4367–
4375, 2018. 5
[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, andYoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139–144, 2020. 1
[18] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A
comprehensive library for coreset selection in deep learning.
arXiv preprint arXiv:2204.08499 , 2022. 2
[19] Greg Hamerly and Charles Elkan. Alternatives to the k-
means algorithm that find better clusterings. In CIKM , pages
600–607, 2002. 3
[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In CVPR , pages 16000–16009, 2022. 1
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
pages 770–778, 2016. 1, 5
[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In CVPR , pages 4700–4708, 2017. 5
[23] Zixuan Jiang, Jiaqi Gu, Mingjie Liu, and David Z Pan. Delv-
ing into effective gradient matching for dataset condensation.
arXiv preprint arXiv:2208.00311 , 2022. 2, 12
[24] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,
Jaakko Lehtinen, and Timo Aila. Training generative ad-
versarial networks with limited data. NeurIPS , 33:12104–
12114, 2020. 3
[25] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo
Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and
Hyun Oh Song. Dataset condensation via efficient synthetic-
data parameterization. arXiv preprint arXiv:2205.14959 ,
2022. 1, 2, 3, 5, 6, 8, 12, 13
[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5, 12
[27] Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, and
Antonio Torralba. Are all training examples equally valu-
able? arXiv preprint arXiv:1311.6510 , 2013. 1, 2
[28] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
5, 12
[29] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and
Xinchao Wang. Dataset distillation via factorization. In
NeurIPS , 2022. 2, 12, 13
[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , pages 10012–10022, 2021. 1
[31] Noel Loo, Ramin Hasani, Alexander Amini, and Daniela
Rus. Efficient dataset distillation using random feature ap-
proximation. In NeurIPS , 2022. 2, 12, 13
[32] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Opti-
mizing millions of hyperparameters by implicit differentia-
tion. In International Conference on Artificial Intelligence
and Statistics , pages 1540–1552. PMLR, 2020. 12
[33] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. 2011. 5, 12
[34] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset
meta-learning from kernel ridge-regression. In ICLR , 2020.
13[35] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon
Lee. Dataset distillation with infinitely wide convolutional
networks. NeurIPS , 34:5186–5198, 2021. 2, 13
[36] Sehban Omer. fast-pytorch-kmeans, 9 2020. 4
[37] Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xi-
angyu Peng, Daquan Zhou, and Yang You. Infobatch: Loss-
less training speed up by unbiased dynamic data pruning.
arXiv preprint arXiv:2303.04947 , 2023. 1
[38] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classi-
fier and representation learning. In CVPR , pages 2001–2010,
2017. 2, 8
[39] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
Farhadi. You only look once: Unified, real-time object de-
tection. In CVPR , pages 779–788, 2016. 1
[40] Hajar Rehioui, Abdellah Idrissi, Manar Abourezq, and
Faouzia Zegrari. Denclue-im: A new approach for big data
clustering. Procedia Computer Science , 83:560–567, 2016.
3
[41] Ozan Sener and Silvio Savarese. Active learning for convolu-
tional neural networks: A core-set approach. arXiv preprint
arXiv:1708.00489 , 2017. 2
[42] Sam Shleifer and Eric Prokop. Using small proxy
datasets to accelerate hyperparameter search. arXiv preprint
arXiv:1906.04887 , 2019. 2
[43] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
Ganguli, and Ari Morcos. Beyond neural scaling laws: beat-
ing power law scaling via data pruning. Advances in Neural
Information Processing Systems , 35:19523–19536, 2022. 1
[44] Mariya Toneva, Alessandro Sordoni, Remi Tachet des
Combes, Adam Trischler, Yoshua Bengio, and Geof-
frey J Gordon. An empirical study of example forget-
ting during deep neural network learning. arXiv preprint
arXiv:1812.05159 , 2018. 1, 2
[45] Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen,
Trung-Kien Nguyen, and Ngai-Man Cheung. Towards good
practices for data augmentation in gan training. arXiv
preprint arXiv:2006.05338 , 2:3, 2020. 3
[46] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki-
hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker.
Learning to adapt structured output space for semantic seg-
mentation. In CVPR , pages 7472–7481, 2018. 1
[47] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-
stance normalization: The missing ingredient for fast styliza-
tion. arXiv preprint arXiv:1607.08022 , 2016. 5
[48] Paul Vicol, Jonathan P Lorraine, Fabian Pedregosa, David
Duvenaud, and Roger B Grosse. On implicit bias in over-
parameterized bilevel optimization. In International Con-
ference on Machine Learning , pages 22234–22259. PMLR,
2022. 12
[49] Kai Wang, Jianyang Gu, Daquan Zhou, Zheng Zhu, Wei
Jiang, and Yang You. Dim: Distilling dataset into genera-
tive model. arXiv preprint arXiv:2303.04707 , 2023. 2
[50] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,
Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and
Yang You. Cafe: Learning to condense dataset by aligning
features. In CVPR , pages 12196–12205, 2022. 1, 2, 3, 6, 13[51] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and
Alexei A Efros. Dataset distillation. arXiv preprint
arXiv:1811.10959 , 2018. 1, 2, 13
[52] Felix Wiewel and Bin Yang. Condensed composite memory
continual learning. In IJCNN , pages 1–8. IEEE, 2021. 2, 8
[53] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-
mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
5
[54] Bo Zhao and Hakan Bilen. Dataset condensation with dif-
ferentiable siamese augmentation. In ICML , pages 12674–
12685. PMLR, 2021. 2, 6, 12, 13
[55] Bo Zhao and Hakan Bilen. Dataset condensation with distri-
bution matching. arXiv preprint arXiv:2110.04181 , 2021. 2,
5, 6, 13
[56] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset
condensation with gradient matching. In ICLR , 2020. 1, 2,
3, 5, 6, 8, 12, 13
[57] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song
Han. Differentiable augmentation for data-efficient gan
training. NeurIPS , 33:7559–7570, 2020. 3
[58] Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and
Han Zhang. Image augmentations for gan training. arXiv
preprint arXiv:2006.02595 , 2020. 3
[59] Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xi-
angyu Yue, and Yang You. Preventing zero-shot transfer
degradation in continual learning of vision-language models.
arXiv preprint arXiv:2303.06628 , 2023. 1
[60] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset
distillation using neural feature regression. In NeurIPS ,
2022. 2, 137. Comparisons with More Methods
We compare the distilled dataset performance between
DREAM and more methods in Tab. 7. The experiments are
conducted under 1, 10, and 50 images-per-class (IPC) set-
tings on MNIST [28], SVHN [33], CIFAR-10, and CIFAR-
100 [26] datasets. DREAM achieves SOTA results on most
cases. Especially when IPC is small, DREAM has gained
significant performance gap over other methods, which also
validates the effectiveness of matching representative sam-
ples. RFAD [31] employs ConvNet with 1024 convolu-
tional channels, while our results are reported based on 128-
channel ConvNet. Except for IPC=1 CIFAR-10, DREAM
distills better synthetic images than RFAD. HaBa [29] in-
volves a data hallucination process, which generates more
samples from base images. It holds higher performance on
IPC=10 CIFAR-10 and IPC=1 CIFAR-100, while in other
circumstances, DREAM has superior performances.
8. Accuracy Curve Visualization
We apply the DREAM strategy to more dataset distilla-
tion methods, such as DC [56], DSA [54], etc. In addition
to stable performance improvements, we visualize the ac-
curacy curve during training in Fig. 7. It can be observed
that compared with the original methods, DREAM only re-
quires one fifth and one tenth of the iteration number on
the DC and DSA to achieve the original performance, re-
spectively. Further increasing the training time brings con-
tinuous performance improvement, which also proves that
our method is universal and is able to be easily plugged for
popular dataset distillation frameworks. The above experi-
ments are all based on the setting of 10 images-per-class on
CIFAR10.
9. DREAM on Distribution Matching
In addition to gradient matching, we also explore the ap-
plicability of our method in embedding distribution match-
ing. For distribution matching, the optimization is con-
strained by:
S∗= arg min
SD(ξ(Mθ(A(S))), ξ(Mθ(A(T)))),(3)
where ξrepresents averaging the features in the channel di-
mension. For gradient matching, the boundary samples gen-
erate large gradients which bias the optimization, while for
distribution matching, the boundary samples shift the aver-
age feature. Random sampling introduces random factors to
the shifts and degrades the matching efficiency. DREAM,
on the contrary, ensures the evenness and diversity of the
original images for matching. It largely reduces the feature
shifts and consistently provide appropriate supervision for
the optimization. We conduct the experiments on IDC [25]with distribution matching. Under the setting of 10 images-
per-class on the CIFAR10 dataset, the original IDC per-
forms much poorer than gradient matching, which is also
stated in [25]. Applying DREAM completely reversed this
situation by improving the dataset performance by a large
margin. Besides, it only takes less than one tenth of the orig-
inal iteration number to reach the performance, as shown in
Fig. 7c.
10. Distilled Dataset Visualization
In order to more intuitively demonstrate the effects on
the distilled images, we compare the distillation results of
adding the proposed DREAM strategy or not in Fig. 8.
DREAM improves the quality of the distilled datasets from
two perspectives. Firstly, the images optimized by DREAM
show more obvious categorical characteristics. Secondly,
DREAM introduces more variety to the distilled images.
With these two improvements, dream brings better perfor-
mance to the distilled datasets.
We provide some extra visualizations of the distilled im-
ages on MNIST, FashionMNIST and SVHN in Fig. 9.
11. Differences from Related Works
There are some recent works focusing on improving the
efficiency of dataset distillation. RFAD reduces the calcu-
lation of neural tangent kernel matrix in Kernel Inducing
Points (KIP) from O(|S2|)toO(|S|)by using random fea-
ture approximation [31]. It takes into account the similar-
ity between Neural Tangent Kernel (NTK) and Neural Net-
work Gaussian Process (NNGP) kernels. RFAD focuses
on reducing the calculation complexity in KIP, while our
proposed DREAM is aiming at improving the matching ef-
ficiency through selecting representative original images,
which has no contradicts.
Jiang et al. analyze the shortcomings of gradient match-
ing method and propose the idea of matching multi-level
gradients from the angle perspective [23]. There are also
many other methods [32, 48], which analyze the shortcom-
ings of existing methods from the perspective of two-level
optimization and improve the efficiency. Comparatively,
DREAM addresses the matching efficiency problem from
the sampling perspective for both gradient matching and
embedding distribution matching. DREAM is able to be
easily plugged into other dataset distillation methods to sig-
nificantly reduce the required training iterations.
12. Clustering Analysis
We further analyze the extra time cost caused by the clus-
tering process in Tab. 8. For CIFAR-10, in each inner loop,
the matching process and image updating cost 0.2s. Every
10 inner loops, a clustering process is conducted. The clus-
tering process takes 1s, so by average the clustering time forTable 7: Top-1 accuracy of test models trained on distilled synthetic images on multiple datasets. The distillation training is
conducted with ConvNet-3.
DatasetMNIST SVHN CIFAR10 CIFAR100
1 10 50 1 10 50 1 10 50 1 10 50
DD [51] - 79.5 - - - - - 36.8 - - - -
LD [3] 60.9 87.3 93.3 - - - 25.7 38.3 42.5 11.5 - -
DC [56] 91.7 97.4 98.8 31.2 76.1 82.3 28.3 44.9 53.9 12.8 25.2 -
DSA [54] 88.7 97.8 99.2 27.5 79.2 84.4 28.8 52.1 60.6 13.9 32.3 42.8
DM [55] 89.7 97.5 98.6 - - - 26.0 48.9 63.0 11.4 29.7 43.6
CAFE [50] 93.1 97.2 98.6 42.6 75.9 81.3 30.3 46.3 55.5 12.9 27.8 37.9
MTT [4] - - - - - - 46.3 65.3 71.6 24.3 40.1 47.7
IDC [25] 94.2 98.4 99.1 68.5 87.5 90.1 50.6 67.5 74.5 - 45.1 -
KIP [34, 35] 90.1 97.5 98.3 57.3 75.0 80.5 49.9 62.7 68.6 15.7 28.3 -
RFAD [31] 94.4 98.5 98.8 52.2 74.9 80.9 53.6 66.3 71.1 26.3 33.0 -
HaBa [29] 92.4 97.4 98.1 69.8 83.2 88.3 48.3 69.9 74.0 33.4 40.2 47.0
FRePo [60] 93.0 98.6 99.2 - - - 46.8 65.5 71.7 28.7 42.5 44.3
DREAM 95.7 98.6 99.2 69.8 87.9 90.5 51.1 69.4 74.8 29.5 46.8 52.6
0100 200 300 400 500 600 700 800 900 1000
Iteration0.250.300.400.50AccuracyAccuracy -Iteration
DC+DREAM 
DC
(a) The accuracy curve of adding DREAM
strategy to DC.
0100 200 300 400 500 600 700 800 900 1000
Iteration0.30
0.250.400.500.60AccuracyAccuracy -Iteration
DSA+DREAM 
DSA(b) The accuracy curve of adding DREAM
strategy to DSA.
0100 200 300 400 500 600 700 800 900 1000
Iteration506070AccuracyAccuracy -Iteration
IDC(feat)+DREAM
IDC(feat)(c) The accuracy curve of adding DREAM
strategy to distribution matching.
Figure 7: Applying the DREAM strategy brings stable performance and efficiency improvements.
Table 8: Time cost of adding DREAM strategy (s).
Datasets Methods ClusteringUpdate Inner
Images Loop
CIFAR-10IDC [25] - 0.2 0.2
DREAM 0.1 0.2 0.3
CIFAR-100IDC [25] - 2.0 2.0
DREAM 0.1 2.0 2.1
each inner loop is 0.1s. The total average inner loop time
is 0.3s, compared to the original 0.2s. Considering that we
only need one tenth to one fifth of the iterations to obtain the
original performance, we save more than 70 %of the time.
For CIFAR-100 with more classes, the extra clustering time
is one twentieth of the original image updating time, which
is negligible. DREAM significantly improves the matchingefficiency and reduces the required training time for dataset
distillation.(a) DC
 (b) DC+DREAM
(c) DSA
 (d) DSA+DREAM
Figure 8: Applying DREAM improves the image quality and sample diversity.
(a) MNIST
 (b) FashionMNIST
 (c) SVHN
Figure 9: Example visualizations of the distilled images on MNIST, FashionMNIST and SVHN.