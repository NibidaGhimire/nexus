Explainability and Robustness of Deep
Visual Classication Models
Jindong Gu
Munich 2022arXiv:2301.01343v1  [cs.CV]  3 Jan 2023Abstract
Deep learning has revolutionized AI and deep neural networks, in particular, have been
hugely successful in a wide range of applications. Deep neural network architectures with
dierent inductive biases have been proposed in dierent communities. In the computer vi-
sion community, Convolutional Neural Networks (CNNs), rst proposed in the 1980's, have
become the standard visual classication model. Recently, as alternatives to CNNs, Cap-
sule Networks (CapsNets) and Vision Transformers (ViTs) have been proposed. CapsNets,
which were inspired by the information processing of the human brain, are considered to
have more inductive bias than CNNs, whereas ViTs are considered to have less inductive
bias than CNNs. All three classication models have received great attention since they
can serve as backbones for various downstream tasks, e.g. object detection and semantic
segmentation. However, these models are far from being perfect.
As pointed out by the community, there are two weaknesses in standard Deep Neural
Networks (DNNs). One of the limitations of DNNs is the lack of explainability. Even
though they can achieve or surpass human expert performance in the image classication
task, the DNN-based decisions are dicult to understand. In many real-world applica-
tions, however, individual decisions need to be explained. The other limitation of DNNs
is adversarial vulnerability. Concretely, the small and imperceptible perturbations of in-
puts can mislead DNNs. The vulnerability of deep neural networks poses challenges to
current visual classication models. The potential threats thereof can lead to unaccept-
able consequences. Besides, studying model adversarial vulnerability can lead to a better
understanding of the underlying models.
Our research aims to address the two limitations of DNNs. Specically, we focus on
deep visual classication models, especially the core building parts of each classication
model, e.g. dynamic routing in CapsNets and self-attention module in ViTs.
We argue that both the lack of explainability and adversarial vulnerability can be
attributed to the dierence in the visual features used by visual recognition models and
the human visual system to recognize objects. Namely, the visual clues used by standard
CNNs are dierent from the ones used by our visual system. The dierences make the
interpretation of classications dicult. Similarly, the dierences also leave attackers the
chance to manipulate decisions with quasi-imperceptible input perturbations.
We have analyzed if the brain-inspired Capsule Network (CapsNet) performs more ro-
bustly than the CNNs. Our investigation on CapsNet shows CapsNets with more inductiveAbstract 3
bias do not perform better than CNNs. The dynamic routing therein can even harm the
robustness, in contrast to the common belief. Compared to CNNs and CapsNets, Vision
Transformers (ViTs) are considered to have less inductive bias in their architecture. Given
the patch-wise input image representation of ViT, we dissect ViT with adversarial patch
attack methods. We nd that vision transformers are more robust to naturally corrupted
patches than CNNs, whereas they are more vulnerable to adversarial patches. Specically,
the attention module can eectively ignore naturally corrupted patches. However, when
attacked by an adversary, it can be easily fooled.
Overall, our work provides a detailed analysis of CNNs, CapsNet, and ViTs in terms of
explainability and robustness. The contribution of this thesis will facilitate the application
of existing popular deep visual classication models and inspires the development of more
intelligent classiers in the future.Chapter 1
Introduction
1.1 Motivation
Articial intelligence changes our daily lives in many perspectives. The recent advances of
articial intelligence are mainly powered by Deep Learning method [1]. As a revolutionary
technique, Deep Learning methods are also embraced by other disciplines, e.g.bioscience
and astronomy. As a representative model in the framework of deep learning, deep neural
networks (DNNs) dominate the community due to their powerful expressiveness. However,
two limitations of deep neural networks prevent their wide application in safety-critical
domains, e.g.the medical domain and autonomous driving system.
One of the limitations of deep neural networks is their lack of explainability. Even
though the DNN-based intelligent system can achieve or surpass human expert perfor-
mance on some tasks, it is not clear how the system reaches its decisions. For exam-
ple, Deep convolutional neural networks (DCNNs) achieve start-of-the-art performance on
many tasks, such as visual object recognition [2, 3, 4, 5]. However, since they lack trans-
parency, they are considered as "black box" solutions. In real-world applications, however,
individual decisions need to be explained to gain the trust of the users. e.g., autonomous
driving systems should reassure passengers by giving explanations when braking the car
abruptly [6, 7]. Decisions made by deep models are also required to be veried in the med-
ical domain. Mistakes of unveried models could have an unexpected impact on humans
or lead to unfair decisions [8, 9, 10]. Besides, AI applications must comply with related
legislation, e.g., the right to explanation in GDPR of the European Union [11].
The other limitation of deep neural networks is limited generalization robustness. When
deep neural networks are deployed in real-world applications, the input can deviate from2 1. Introduction
Figure 1.1: The overview of deep visual classication model architectures. This gure is
based on the gures in [17, 3, 12]
the training data distribution. The inference on the input with overlapped patterns [12],
ane-transformed pattern [12, 13], and natural corruption [14] can result in unexpected
results. Besides the robustness to out-of-distribution data, the low robustness to articial
perturbation also raises great concern in the community. Concretely, the small and im-
perceptible articial perturbations of inputs can mislead DNN-based intelligent systems.
For example, given an image correctly classied by a deep convolutional neural network,
a hardly human-perceptible articial perturbation can cause the convolutional neural net-
work to misclassify the image when added to it. The vulnerability of Deep Learning poses
challenges to current intelligent systems. The adversarial images on CNNs can pose po-
tential threats to security-sensitive CNN-based applications, e.g., face verication [15] and
autonomous driving [16]. The potential threats thereof can lead to unacceptable conse-
quences. Besides, the existence of adversarial images demonstrates that the object recogni-
tion process in CNNs is dramatically dierent from that in human brains. Hence, the study
of adversarial examples on deep neural networks can also lead to a better understanding
of the underlying object recognition models.
Since [18] proposed the AlexNet, deep neural networks have revolutionized the computer
vision community. In the image classication task, the classication model consists of two
parts, i.e., feature extractor and classier. The modules that extract features from input
images are also adopted as feature extractor (dubbed backbone ) in downstream tasks,
e.g., object detection [19, 20] and semantic segmentation [21, 22, 23]. The improvement
of the classication models often also benets the downstream tasks due to the improved
backbone. In this thesis, we focus on deep visual classication models from the perspectives
of explainability and robustness.1.1 Motivation 3
As one of the representatives of deep visual classication models, convolutional neural
networks have dominated the computer vision community in the last decade [18]. How-
ever, CNNs suer from many limitations, e.g., only local information aggregation at lower
layers and the broken equivariance. Recently, the community has been attempting to
propose new models to overcome the limitations. Two among them have received great
attention from the community. The one is Capsule Networks (CapsNet) which is inspired
by the information processing in the human brain [12]. Compared to CNNs, CapsNet is
more inductively-biased where the partial information processing in the human brain is
integrated into the model, e.g., the transformation process. The other is Vision Trans-
former(ViT) [17]. Given the success of Transformer in natural language processing (NLP),
the work [17] generalizes Transformer architectures to image classication task by rep-
resenting the input image as a sequence of image patches. Compared to CNNs, ViTs
are less inductive-biased where information aggregation is also possible at lower layers.
Convolutional Neural Networks, Capsule Networks, and Vision Transformers raise great
attention in the community. Hence, in this work, we mainly focus on the three deep visual
classication models.
In the rest of this chapter, we rst introduce background knowledge about CNNs, Cap-
sNets, and ViTs in Section 1.2. Then, in Section 1.3, we present a summary of the explain-
ability of deep visual classications and describe our contributions to the explainability of
deep visual classication models. Last, in Section 1.4, we show the categorization of the
robustness of deep visual classications and describe our contributions to the robustness
of deep visual classication models.
Contributions. In this dissertation, our contributions can be summarized from two
perspectives. From the perspective of explainability, we rst present a novel method, called
CLRP, to explain CNN-based image classications in Chapter 2. Then, in Chapter 3, we
present our interpretable capsule networks whose predictions can be explained with built-in
modules. Last, we show our understanding of ViT-based image classications in Chapter 7.
From the perspective of robustness, our contributions mainly focus on the role the model
architecture plays in terms of both natural robustness and adversarial robustness. We
present our ndings and improvements of Capsule Networks' natural robustness to non-
additive perturbation in Chapters 4 and 5, and further propose our adversary Vote Attack
method to show the vulnerability of CapsNets in Chapter 6. Besides, we introduce our
understanding of the robustness of ViT-based classications to patch-wise perturbations
in Chapter 7.4 1. Introduction
1.2 Background Knowledge
1.2.1 Convolutional Neural Networks
To recognize the patterns of the images, many operations have been proposed, e.g., Scale-
Invariant Feature Transform (SIFT) [24], Histogram of Oriented Gradients(HOG) [25], and
Convolution. Especially, the convolutional operation dominates the community in the last
decade as an image feature extraction operation.
Formally, convolution is a mathematical operation on two functions that produces a
third function that expresses how the shape of one is modied by the other. In the domain
of computer vision, the discrete variant of convolution is adopted since the images are
saved as discrete signals. Concretely, given an image X2R(CHW)and a convolution
kernel k2R(CPQ), the feature map H2R(H0W0)extracted by the convolution kernel
is computed as
H(i; j)=CX
c=1PX
p=1QX
q=1X(c; i+p 1; j+q 1)k(c; p; q ); (1.1)
where (i;j) is the index of elements in the feature map H,Cis the number of channels of
input images and ( P;Q) are the size of the feature map. A single kernel corresponds to a
single feature map. Multiple kernels are often applied to extract multiple feature maps.
Besides, the pooling (subsampling) operation is applied to the feature maps extracted
by convolution operation to aggregate the visual information. In the pooling operation,
the mean operation or the max operation is often applied. The pooling operation with size
(s;s) can be expressed as
H0
(i; j)=Pmax
p=1H(i; j): (1.2)
Convolution can be further applied to the pooled feature maps. The convolutional and
pooling operations are applied alternatively on the image to obtain the nal feature maps.
The features HL
(i; j)extracted by a list of convolutional operations and pooling opera-
tions are taken as the nal image representation. A single or multiple fully connected layers
(i.e. a MLP module) is used as classier that maps the features into the ground-truth class.
Z=MLP (HL
(i; j)) (1.3)
The output probabilities can be obtained by applying softmax function on the logits Z.
The predicted class is dened as argmax (Zi).1.2 Background Knowledge 5
Figure 1.2: The overview of LeNet-5 architecture [26].
The work [26] proposes Convolution Neural Network (CNN) in the end-to-end learning
framework to recognize hand-written digits. Therein, LeNet-5 is the classic instance of
convolution neural networks, which is visualized in Fig. 1.2. The proposed LeNet-5 starts
with two convolutional layers, and each is followed by a pooling layer. Then, a three-layer
MLP module maps the feature to the logits.
Figure 1.3: The overview of AlexNet architecture [18].
Given the limited computational resource, the architecture and the corresponding train-
ing strategy proposed in [26] does not scale well to the large-scale dataset. With the advance
of the computational power, the work [18] proposes AlexNet, which achieves impressive
accuracy on ImageNet-1k dataset. AlexNet consists of ve convolutional layers, some of
which are followed by max-pooling layers, and three fully-connected layers with a nal 1000-
way softmax. In terms of model architecture, AlexNet is deeper and wider than LeNet-5.
From the perspective training strategy, to make AlexNet work well, the work [18] proposes
non-saturating neurons, i.e., Rectied Linear Units (ReLUs) to activate the neurons and6 1. Introduction
Figure 1.4: The overview of Residual block with skip connection [3].
Figure 1.5: The overview of ResNet architecture [3].
employs dropout method to regularize the training process. Especially, they propose a
GPU-specic implementation of GPU operation to make the training process feasible.
One intuitive way to improve AlexNet is to build deeper layers. However, the AlexNet
with deeper layers does not converge well during training due to the gradient vanishing
problem. Namely, the gradients become zeros or close to zeros when propagating from
the output layer to low layers. Due to the gradient vanishing problem, the parameter
update of low layers is challenging. To overcome the challenges, the work [3] proposes
skip-connection, which can propagate the gradients from deep layers to low layers directly
by skipping some intermediate layers.
The block with such a skip connection is called residual block. A popular residual block
is shown in Fig. 1.4. As an instance, the work [3] proposes ResNet which consists of a list
of residual blocks. When equipped with skip connections, ResNets with even more than
100 layers can converge well. ResNets still dominate the computer vision community. We
show the ResNet18 in Fig. 1.5 as an example where 18 layers are built into the ResNet to
extract features.1.2 Background Knowledge 7
Figure 1.6: The overview of CapsNet architectures. The CapsNet architecture consists of
four components, such as primary capsule extraction, voting, routing, and class-conditional
reconstruction. The primary capsule extraction module rst maps the raw input features
to low-level capsules. The voting process transforms low-level capsules to make votes with
a transformation matrix. Then, the routing module identies the weight of each vote and
computes the nal high-level capsules. In the last part, the reconstruction subnetwork
reconstructs input images from capsules to regularize the learning process.
Convolutional Network Follow-Ups: The CNN-based deep visual classier has al-
ready surpassed human-level performance in the image classication task [27]. In the last
years, the architectures of convolutional neural networks have still been improved from
dierent perspectives. On the one hand, the more advanced architectures have been pro-
posed to further push the state-of-the-art performance [4, 5, 28]. On the other hand, the
eciency of architecture has received great attention since real-world CNN-based appli-
cations often require less memory consumption and computational cost. The eciency of
architecture has been addressed from dierent perspective, e.g., light-weight architecture
design [29, 30], architecture pruning [31, 32, 33, 34], and distilling knowledge from large
architectures to small architectures [35, 36, 37, 38]. More recently, many researchers focus
on neural architecture search where the architectures are searched automatically from a
predened search space [39, 40, 41]. The found architecture can surpass the manually
designed ones.
1.2.2 Capsule Networks
Inspired by the information process in the human brain, Hinton proposes Capsule Networks
(CapsNet) [12]. Dierent from CNNs, CapsNets represent a visual entity with a vector
instead of a single scale value, called Capsule. CapsNets [12] encode visual entities with8 1. Introduction
capsules. Each capsule is represented by an activity vector (e.g., the activation of a group
of neurons), and elements of each vector encode the properties of the corresponding entity.
The length of the activation vector indicates the condence of the entity's existence. The
output classes are represented as high-level capsules.
The most popular version of Capsule Networks is Dynamic Routing Capsule Networks
(DR-CaosNet). We introduce the architecture details of DR-CapsNet as follows. As shown
in Fig. 1.6, CapsNet starts with one (or more) convolutional layer(s) that convert the
raw pixel intensities Xinto low-level visual entities ui. Concretely, CapsNet extracts
feature maps of shape ( C0;H0;W0) from input image X2R(CHW)with two standard
convolutional layers where C0,H0,W0are the number of channels, the height, and the width
of the feature maps, respectively. The extracted feature maps are reformulated as primary
capsules (C0=Din;H0;W0;Din) whereDinis the dimensions of the primary capsules. There
areN=C0=DinH0W0primary capsules all together. Each capsule ui, aDin-dimensional
vector, consists of Dinunits across Dinfeature maps at the same location. For example,
the red bar marked with uiin Fig. 1.6 is a low-level capsule.
In the voting process, each primary capsule is transformed to make a vote with a
transformation matrix Wij2R(DinNDout)in, whereNis the number of output classes
andDoutis the dimensions of output capsules. The vote from the i-th low-level capsules
to thej-th high-level capsules is
^ujji=Wijui: (1.4)
Then, a routing module is applied to identify weight for each vote. Given all Nvotes
^ujjiof theL-th layer with Ncapsules,Mhigh-level capsule sjof the (L+ 1)-th layer with
Mcapsules, the routing process is
sj=NX
icij^ujji (1.5)
wherecijis a coupling coecient that models the degree with which ^ujjiis able to predict
sj. The capsule sjis shrunk to a length in [0, 1) by a non-linear squashing function g(),
which is dened as
vj=g(sj) =ksjk2
1 +ksjk2sj
ksjk: (1.6)
By doing the squashing operation, the length of the vector is mapped to [0, 1) that rep-
resents the condence of the high-level entity's existence. In DR-CapsNet, the high-level
capsules correspond to output classes, and its length means the output probability.1.2 Background Knowledge 9
Note that the coupling coecients fcijgin Equation 1.5 are computed by an iterative
routing procedure. They are updated so that high agreement ( aij=vT
j^ujji) corresponds
to a high value of cij.
cij=exp(bij)P
kexp(bik)(1.7)
where initial logits bikare the log prior probabilities and updated with bik=bik+aijin
each routing iteration. The coupling coecients between a i-th capsule of the L-th layer
and all capsules of the ( L+ 1)-th layer sum to 1, i.e.,PM
j=1cij= 1. The steps in Equations
1.9, 1.5, 1.6, and 1.7 are repeated Ktimes in the routing process, where sjandcijdepend
on each other.
The length of the nal output capsule vjcorresponds to the output probability of
thej-th class. Dierent from CNNs where cross-entropy loss is often applied to compute
classication loss. In DR-CapsNet, the margin loss function is applied to compute the
classication loss
Lk=Tkmax(0;m+ kvkk)2
+(1 Tk) max(0;kvkk m )2(1.8)
whereTk= 1 if the object of the k-th class is present in the input. As in [12], the
hyper-parameters are often empirically set as m+= 0:9,m = 0:1 and= 0:5.
A reconstruction sub-network reconstructs the input image from all Noutput capsules
with a masking mechanism. The ones corresponding to the non-ground-truth classes are
masked with zeros before being transferred to the reconstruction sub-network. Due to
the masking mechanism, only the capsule of the ground-truth class is visible for the re-
construction. Hence, the reconstruction process is called class-conditional reconstruction.
The reconstruction loss is computed as a regularization term in the loss function.
Capsule Network Follow-Ups: Many routing mechanisms have been proposed to im-
prove the performance of CapsNet, such as Expectation-Maximization Routing [42], Self-
Routing [43], Variational Bayes Routing [44], Straight-Through Attentive Routing [45], and
Inverted Dot-Product Attention routing [46]. An alternative to the routing mechanism to
aggregate information is proposed in work [47] where they replace the dynamic routing
with a multi-head attention-based graph pooling approach. To reduce the parameters of
CapsNet, a matrix or a tensor is used to represent an entity instead of a vector [42, 48]. The
size of the learnable transformation matrix can also be reduced by the matrix/tensor repre-
sentations. Besides, the work [13] proposes to share a transformation matrix to reduce the10 1. Introduction
Figure 1.7: The overview of Vision Transformer Architectures. The gure is taken
from [17].
network parameters. Another way to improve CapsNet is to integrate advanced modules
of ConvNet into CapsNet, e.g., skip connections [3, 48] and dense connections [5, 49].
1.2.3 Vision Transformers
Transformers with self-attention-based architectures have become the model of choice in
natural language processing (NLP) [50]. Inspired by the success of Transformers in NLP
community, the work [17] proposes Vision Transformer(ViT) where they replace the convo-
lutions entirely with self-attention layers and achieve remarkable performance in the image
classication task. As a promising alternative to CNNs, Vision Transformer raises the
great attention of our community.
Dierent from CNNs, ViT represents an input image as a sequence of image patches.
Then, the list of self-attention modules are applied to the sequence of image patches se-
quentially. We now introduce the details of the primary Vision Transformer architecture
in [17]. As shown in Fig. 1.7, the input image X2R(CHW)is split into image patches
fxi2RPPCji2(1;2;3;:::;H=PW=P )gwherePis the patch size. The embedding
of each patch is extracted from the raw image patch with linear projection parameters
W02R(HW=P2Dp). Before the application of self-attention module, the position informa-1.2 Background Knowledge 11
Figure 1.8: The overview of Transformer Encoder.
tion of image patches is also integrated into the patch embedding. The embedding of the
patch xiis described as
E0
i=xiW0+PE i; (1.9)
where PE iis the position embedding of the image patch fxi, which encodes the patch
position information in the input images. The position embedding PE icould be manually
designed or learnable. In ViT, the learnable version is adopted.
A learnable class-token embedding E0
0is added into the list of patch embeddings. The
class embedding in the last layer is taken as the image embedding for classication. We
now introduce the transformer encoder where the list of blocks is applied to transform
the input embeddings. As shown in Fig. 1.8, each block consists of two main modules,
namely, a multi-head self-attention module to model the inter-patch relationship and an
MLP module to project each patch respectively.
When the self-attention module with a single head in l+ 1-th layer is applied to input
patchesfEl
i2RDpji2(0;1;2;3;:::;H=PW=P )gin thel-th layer, the output embedding
of the patch El
iis
Kl+1
i=Wl+1
kEl
i;
Ql+1
i=Wl+1
qEl
i;
Vl+1
i=Wl+1
vEl
i;
Al+1
i=Softmax (Ql+1
iKl+1
0;Ql+1
iKl+1
1; :::; Ql+1
iKl+1
H=PW=P +1;);
El+1
i=H=PW=P +1X
j=1Al+1
ijVj:(1.10)12 1. Introduction
In this equation, the key, query, and value of patch embedding is computed rst. The
attention of El+1
ito all patches in l-th layer is obtained with the query of i-th patch and
all keys. The output embedding El+1
iis the weighted sum of all values of patches. The
output embeddings of dierent heads are concatenated as the nal embedding. Then, an
MLP module with two MLP layers is applied to project the nal embedding of each patch
into a new feature space. The nal embedding of the class-token patch is taken as the
image representation to classify the image. A linear classier maps the features to output
space.
Vision Transformer Follow-Ups: Since the ViT was proposed, many new vision trans-
former architectures have been proposed [51, 52, 53]. A hybrid architecture that consists
of both convolutional layers and self-attention blocks has also been explored [54, 55]. Be-
sides, the pure patch-based architecture without attention mechanism has also been pro-
posed [56]. By the time this thesis is written, the arm-race between ResNet and Vision
Transformers is still going on [57]. Recently, many researchers employ the Transformer
architecture as a uniform architecture that model both images and texts [58, 59].1.3 Explanability of Deep Visual Classications 13
Approach Description
Saliency MapsIdentifying the relevance of each input pixel to the out-
put class [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
72, 73, 74].
Counterfactual ExplanationIdenties how the given input could change such that the
classier would output a dierent specied class [75, 76].
Explanatory SentencesGenerating natural language sentences that describe the
class-discriminative pixels [77, 78].
Supporting Training ImagesIdentifying training images most responsible for a given
prediction [79].
Built-in ExplanationGenerating Explanations with built-in modules (in ex-
plainable classier) for a given prediction [79].
Disentangled RepresentationsIdentifying the human-interpretable properties of the
recognized object in the input image [80, 12, 81, 82].
Table 1.1: Summarization of dierent approaches for explaining image classications.
1.3 Explanability of Deep Visual Classications
1.3.1 Introduction
Deep Neural Networks (DNNs) have shown impressive performance in high-dimensional
input data. Especially, the performance of DNNs can even surpass human-level perfor-
mance in the image classication task. The traditional machine learning methods classify
images with hand-crafted images, while DNNs make predictions based on the features
learned automatically from data with an optimization algorithm. Hence, it is challenging
to understand the classication decisions made by DNNs. In recent years, many directions
have been explored to explain individual image classications. We summarize and roughly
categorize them in Table 1.1. We introduce each approach as follows.
Saliency Maps, as intuitive explanations, have received great attention in the commu-
nity. The saliency map is a heat map, each element of which indicates the importance of
the pixel in the corresponding position. The saliency map is expected to have recognizable
patterns like the objects in the input image. The primary work [60] takes the vanilla gra-
dient of the loss with respect to the input as the saliency map. However, the gradients are
noisy and the pattern therein is barely recognizable. To improve the saliency map, many14 1. Introduction
methods have been proposed [60, 83, 61, 62, 63, 64, 65, 66, 67, 84, 85]. The primary method
and the improved variants are model-aware, which leverage the parameters and the gradi-
ents of neural networks to compute saliency maps. Besides the model-aware methods, the
model-agnostic saliency methods are also preferred in many scenarios. For example, they
are able to explain any classiers; the explanations produced from two or more dierent
types of models are comparable; an ensemble model can be explained without requiring
knowledge of model components. There are two types of model-agnostic saliency methods.
The one is to build an explanation generation model, e.g. a neural network with U-net
architecture [86, 68, 69]. The other is to approximate the local decision boundary of the
underlying model with an explainable model, e.g., linear classier [70]. The explanation
generated from the explainable surrogate model can be used to explain individual decisions.
Counterfactual Explanation describes what changes to the situation would have resulted
in arriving at the alternative decision. In the case of image classication, Counterfactual
Explanation is the counterfactual image, which indicates that the output will become the
target class if the input image is replaced with the counterfactual image. The work [75]
creates a counterfactual image with a conditional generative model, which generates part
of the pre-dened image region conditional on the rest of the image. The desired property
of the generated image is to most change the classier's decision. Another work [76]
formulates the generation of the counterfactual image as an image editing problem. Their
method performs well even in the ne-grained classications.
Natural language, as a natural interface, has also been explored to explain the visual
classications. The works [77, 78] build modules to generate natural language sentences to
explain the decisions where the sentences describe the class-discriminative features. The
explanatory sentences are dierent from the caption/description generated by multi-model
models. The contemporary vision-language models describe image content but fail to tell
class-discriminative features which justify visual predictions.
Another way to explain visual classications is to identify the training points most
responsible for a given prediction. To trace a model's prediction back to its training data,
the work [79] leverages inuence functions, i.e., a classic technique from robust statistics.
Given a classication, they can be the most responsible training image that supports the
predictions. The created explanation can tell where the local decision boundary of the
model came from at a specic data point.
The approaches introduced above are post-hoc. Namely, the explanations are created
for o-shelf models without intervening in their training process. An alternative to post-1.3 Explanability of Deep Visual Classications 15
hoc explanation methods is to integrate dedicated modules into the model to be trained,
e.g. attention mechanism [47], explanation module [68] and prototype module [87]. In
the inference stage, the modules can be used to create explanations directly. The created
explanations are dubbed built-in explanations, which are more ecient and easy to create.
The image representations learned by DNNs are often distributed, which makes the
classication decision less explanation. It is dicult to interpretable the decision process
inside the model. One way to mitigate this problem is to constrain the model to learn
disentangled representations where each element of representation corresponds to a human-
understandable concept [80, 12, 81, 88, 82].
In this subsection, we have introduced the popular methods applied to explain individ-
ual classication decisions. In the rest of this section, we present our contributions towards
understanding the classications. Specically, we briey introduce our works on the topic
of explaining classication decisions made by Convolutional Networks, Capsule Networks,
and Vision Transformers.
1.3.2 Explainability of Convolutional Neural Network-based Clas-
sication
A large number of saliency methods have been proposed to better understand individ-
ual decisions of deep convolutional neural networks. As one of the representatives, the
Layer-wise Relevance Propagation (LRP) approach is able to create pixel-wise explana-
tory saliency maps. LRP method has also been widely applied to many tasks in dierent
domains, e.g., in medical domain [89] and in NLP [90].
The explanations generated by LRP are known to be pixel-wise and instance-specic.
However, the discriminativeness of the explanations has not been evaluated yet. Ideally, the
visualized objects in the explanation should correspond to the class that the class-specic
neuron represents. Namely, the explanations should be class-discriminative.
Our work [66] evaluates the discriminativeness of the explanations generated by LRP.
Concretely, we evaluate the explanations generated by LRP on the o-the-shelf models,
e.g., VGG16 [2] pre-trained on the ImageNet dataset [91]. The results are shown in Fig.
1.9. For each test image, we create four saliency maps as explanations. The rst three ex-
planation maps are generated for top-3 predictions, respectively. The fourth one is created
for randomly chosen 10 classes from the top-100 predicted classes (which ensure that the
score to be propagated is positive). The white text in each explanation map indicates the16 1. Introduction
Figure 1.9: The explanations generated by LRP on VGG16 Network. The images from
validation datasets of ImageNet are classied using the o-the-shelf models pre-trained on
the ImageNet. The classications of the images are explained by the LRP approach. For
each image, we generate four explanations that correspond to the top-3 predicted classes
and a randomly chosen multiple-classes. The explanations are not class-discriminative.
class the output neuron represents and the corresponding classication probability. The
generated explanations are instance-specic, but not class-discriminative. In other words,
they are independent of class information. The explanations for dierent target classes,
even randomly chosen classes, are almost identical.
Based on LRP, our work [66] proposes Contrastive Layer-wise Relevance Propagation
(CLRP), which is capable of producing instance-specic, class-discriminative, pixel-wise
explanations. Before introducing our CLRP, we rst discuss the conservative property
in the LRP. In a DNN, given the input X=fx1;x2;x3;;xng, the output Y=
fy1;y2;y3;;ymg, the scoreSyj(activation value) of the neuron yjbefore softmax layer,
the LRP generate an explanation for the class yjby redistributing the score Syjlayer-
wise back to the input space. The assigned relevance values of the input neurons are
R=fr1;r2;r3;;rng. The conservative property is dened as follows: The generated
saliency map is conservative if the sum of assigned relevance values of the input is equal
to the score of the class-specic neuron,Pn
i=1ri=Syj.
The overview of the CLRP are shown in Fig. 1.10. We rst describe the LRP as follows.
Thej-th class-specic neuron yjis connected to input variables by the weights Wof layers
between them. The neuron yjmodels a visual concept O. For an input example X, the
LRP maps the score Syjof the neuron back into the input space to get relevance vector
R=fLRP(X;W;Syj). In our contrastive LRP, we construct a dual virtual concept O,
which models the opposite visual concept to the concept O. For instance, the concept O1.3 Explanability of Deep Visual Classications 17
Figure 1.10: The gure shows an overview of our CLRP. For each predicted class, the
approach generates a class-discriminative explanation by comparing two signals. The blue
line means the signal that the predicted class represents. The red line models a dual
concept opposite to the predicted class. The nal explanation is the dierence between the
two saliency maps that the two signal generate.
models the zebra , and the constructed dual concept Omodels the non-zebra . One way
to model the Ois to select all classes except for the target class representing O, i.e. the
dashed red lines in Fig. 1.10 are connected to all classes except for the target class zebra .
Next, the score Syjof target class is uniformly redistributted to other classes. Given the
same input example X, the LRP generates an explanation Rdual=fLRP(X;W;Syj) for
the dual concept. The Contrastive LRP is dened as follows:
RCLRP = max( 0;(R Rdual)) (1.11)
where the function max( 0;X) means replacing the negative elements of Xwith zeros.
The dierence between the two saliency maps cancels the common parts. Without the
dominant common parts, the non-zero elements in RCLRP are the most relevant pixels.
Besides the qualitative evaluation, we also evaluate the explanations quantitatively with
a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show
that the CLRP generates better explanations than the LRP.
1.3.3 Explainability of Capsule Network-based Classication
Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed
to recognize objects from images. The current literature demonstrates many advantages
of CapsNets over CNNs. However, how to create explanations for individual classications
of CapsNets has not been well explored.18 1. Introduction
Figure 1.11: The illustration of GraCapsNets: The extracted primary capsules are trans-
formed and modeled as multiple graphs. The pooling result on each graph (head) corre-
sponds to one vote. The votes on multiple graphs (heads) are averaged to generate the
nal prediction.
The widely used saliency methods are mainly proposed for explaining CNN-based clas-
sications; they create saliency map explanations by combining activation values and the
corresponding gradients, e.g., Grad-CAM. They combine activation values and the received
gradients in specic layers, e.g., deep convolutional layers. In CapsNets, instead of deep
convolutional layers, an iterative routing mechanism is applied to extract high-level visual
concepts. Hence, these saliency methods cannot be trivially applied to CapsNets. Besides,
the routing mechanism makes it more challenging to identify interpretable input features
relevant to a classication.
To overcome the lack of interpretability, we can either propose new post-hoc interpre-
tation methods for CapsNets or modify the model to have build-in explanations. In our
published work [47], we explore the latter. Specically, we propose interpretable Graph
Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head
attention-based Graph Pooling approach. Our GraCapsNet includes an attention-based
pooling module, with which individual classication explanations can be created eectively
and eciently.
As introduced in Background Section, CapsNets start with convolutional layers that
convert the input pixel intensities Xinto primary capsules ui(i.e., low-level visual entities).
Each uiis transformed to vote for high-level capsules ^ ujjiwith learned transformation
matrices. Then, a routing process is used to identify the coupling coecients cij, which
describe how to weight votes from primary capsules. Finally, a squashing function is
applied to the identied high-level capsules sjso that the lengths of them correspond to
the condence of the class's existence.1.3 Explanability of Deep Visual Classications 19
Dierent routing mechanisms dier only in how to identify cij. Routing processes de-
scribe one way to aggregate information from primary capsules into high-level ones. In
our GraCapsNets, we implement the information aggregation by multi-head graph pooling
processes. In CapsNets, the primary capsules represent object parts, e.g., the eyes and
nose of a cat. In our GraCapsNets, we explicitly model the relationship between the pri-
mary capsules (i.e., part-part relationship) with graphs. Then, the followed graph pooling
operations pool relevant object parts from the graphs to make a classication vote. Since
the graph pooling operation reveals which input features are pooled as relevant ones, we
can easily create explanations to explain the classication decisions.
The overview of our GraCapsNets is illustrated in Fig. 1.11. In GraCapsNet, the
primary capsules uiare transformed into a feature space. All transformed capsules u0
i
are modeled as multiple graphs. Each graph corresponds to one head, the pooling result
on which corresponds to one vote. The votes on multiple heads are averaged as the nal
prediction.
The transformed capsules u0
ican be modeled as multiple graphs. A graph consists
of a set of nodes and a set of edges. As shown in Fig. 1.11, the primary capsules are
reshaped from Lgroups of feature maps. Each group consists of Cfeature maps of the
sizeKK. Correspondingly, the transformed capsules u0
iwherei2f1;2;:::K2gform a
single graph with K2nodes. Each node corresponds to one transformed capsule u0
i, and
the activation vector of u0
iis taken as features of the corresponding node. The graph edge
can be represented by an adjacency matrix, where dierent priors can be modeled. The
spatial relationship between primary capsules is modeled in our work.
Given node features Xl2R(K2Dout)and adjacency matrix A2R(K2K2)in thel-th
head of GraCapsNet. We rst compute the attention of the head as Attl= softmax( AXlW)
where W2RDoutMare learnable parameters. Doutis the dimension of the node features
andMis the number of output classes. The output is of the shape ( K2M). In our
GraCapsNet for object recognition, Attlcorresponds to the visual attention of the heads.
The graph pooling output Sl2R(MDout)of the head is computed as Sl= (Attl)TXl.
The nal predictions of GraCapsNets are based on all Lheads with outputs Slwhere
l2f1;2;:::;Lg. The output capsules are V= squash(1
LPL
l=1Sl).
In our GraCapsNet, we can use visual attention as built-in explanation to explain the
predictions of GraCapsNets. The averaged attenion over lheads is
E=1
LLX
l=1Attl(1.12)20 1. Introduction
Figure 1.12: Adversarial Patch Attack or Natural Patch Corruption on Vision Transformer.
where Attlcorresponds to the attention of the l-th head. The created explanations Eare
of the shape ( K2M). Given the predicted class, the KKattention map indicates
which pixels of the input image support the prediction.
The explanations for individual classications of GraCapsNets can be created in an
eective and ecient way. Surprisingly, without a routing mechanism, our GraCapsNets
can achieve better classication performance and better adversarial robustness, and still
keep other advantages of CapsNets, namely, disentangled representations and ane trans-
formation robustness.
1.3.4 Explainability of Vision Transformer-based Classication
The recent advances in Vision Transformer (ViT) have demonstrated its impressive perfor-
mance in image classication [17, 51], which makes it a promising alternative to Convolu-
tional Neural Network (CNN). Unlike CNNs, ViT represents an input image as a sequence
of image patches. Then, a self-attention mechanism is applied to aggregate information
from all patches. The attention can be used to create saliency maps to explain ViT-based
classication decisions, e.g. with Rollout Attention method [92]. The patch-wise input
image representation in ViT makes the following question interesting: How does the at-
tention of ViT change when individual input image patches are perturbed with natural
corruptions or adversarial perturbations? For example, Fig. 1.12 illustrates the case where
a single patch of the input is perturbed or attacked.1.3 Explanability of Deep Visual Classications 21
(a) Clean Image
 (b) with Naturally Corrupted Patch
 (c) with Adversarial Patch
Figure 1.13: Images with patch-wise perturbations (top) and their corresponding atten-
tion maps (bottom). The attention mechanism in ViT can eectively ignore the naturally
corrupted patches to maintain a correct prediction, whereas it is forced to focus on the
adversarial patches to make a mistake. The images with corrupted patches are all cor-
rectly classied. The images with adversary patches in subgure 1.13c are misclassied as
dragony ,axolotl , and lampshade , respectively.
In our work [93], we study the robustness of vision transformers to patch-wise per-
turbations. Surprisingly, we nd that vision transformers are more robust to naturally
corrupted patches than CNNs, whereas they are more vulnerable to adversarial patches.
Furthermore, we conduct extensive qualitative and quantitative experiments to understand
the classication under patch perturbations.
We have revealed that ViT's stronger robustness to natural corrupted patches and
higher vulnerability against adversarial patches are both caused by the attention mecha-
nism. Specically, the attention model can help improve the robustness of vision transform-
ers by eectively ignoring natural corrupted patches. However, when vision transformers
are attacked by an adversary, the attention mechanism can be easily fooled to focus more
on the adversarially perturbed patches and cause a mistake.
Digging down further, we nd the reason behind this is that the self-attention mech-
anism of ViT can eectively ignore the natural patch corruption, while it's also easy to
manipulate the self-attention mechanism to focus on an adversarial patch. This is well
supported by rollout attention visualization [92] on ViT. As shown in Fig. 1.13 (a), ViT
successfully attends to the class-relevant features on the clean image, i.e., the head of the
dog. When one or more patches are perturbed with natural corruptions, shown in Fig. 1.13
(b), ViT can eectively ignore the corrupted patches and still focus on the main foreground
to make a correct prediction. In Fig. 1.13 (b), the attention weights on the positions of22 1. Introduction
naturally corrupted patches are much smaller even when the patches appear in the fore-
ground. In contrast, when the patches are perturbed with adversarial perturbations by an
adversary, shown in Fig. 1.13 (c), ViT is successfully fooled to make a wrong prediction
because the attention of ViT is misled to focus on the adversarial patches instead.
In our work [93], we provide our understanding of the attention changes of ViT when
individual input image patches are perturbed with natural corruptions or adversarial per-
turbations.1.4 Robustness of Deep Visual Classication Models 23
Natural
RobustnessAdditive Natural CorruptionRobustness to the noisy images that are added
with various noise [94, 14], such as, white noise,
blur, weather, and digital categories.
Non-Additive Ane TransformationRobustness to the images that are ane-
transformed from standard ones [95, 12, 13].
AdditiveDense AttackRobustness to the images where all pixels can
be changed under a certain constraint [96, 97].
Adversarial
RobustnessSparse AttackRobustness to the images where only a few pix-
els of each image can be manipulated [98].
Patch AttackRobustness to the perturbed images where only
a single patch (a specic region) of each image
can be manipulated [99, 100].
Non-AdditiveTransformation
-Based AttackRobustness to adversarial images that is cre-
ated by delicated ane transformations [101].
Sementic AttackRobustness to semantic adversarial images that
is created by image synthesis [102].
Table 1.2: Categorization of Robustness in Image Classication Task.
1.4 Robustness of Deep Visual Classication Models
1.4.1 Introduction
In this thesis, we mainly consider two types of robustness, namely, natural robustness and
adversarial robustness. When an image is captured, dierent corruption can happen, e.g.,
the existence of white noise, the eect of weather, the compression in the digitalization
process, and random ane transformation. The robustness to these images with natural
corruption is denoted as natural robust. Adversarial robustness describes the robustness
of models to adversarial images, which is created by an adversary. Both natural robustness
and adversarial robustness are critical in some safety-critical domains. We summarize and
categorize the robustness in Tab. 1.2.
Besides the type of attacks in Tab. 1.2, adversarial attacks can be categorized into
targeted and untargeted ones. The goal of targeted attacks is to mislead the model to a
specic target class, while the goal of untargeted ones is to fool the model to make wrong
predictions.
In terms of the availability of the target models, adversarial attacks can also be cat-
egorized into white-box and black-box attacks. The white-box attacks assume that the24 1. Introduction
adversary has all access to target models including model parameters, model architectures,
and even defense methods. In contrast, in the setting of black-box attacks, the adversary
can only obtain the output of the target model. The black-box attacks have also received
great attention since it is realistic in real-world applications.
The implementation of white-box attacks is relatively cheap where they create adversar-
ial examples with the gradients of the self-dened objective function with respect to inputs.
However, the implementation of black-box attacks can be computationally expensive given
the limited available information. One way to created adversarial examples in a black-box
fashion is to leverage their transferability [103, 104, 105, 106, 107, 108, 109, 110, 111, 112,
113], namely, the adversarial examples created on one model can also fool another. The
adversary rst trains a surrogate model on the same training data as the one used for the
target model and creates adversarial examples on the surrogate model to fool the target
model, which is called transfer-based black-box attack. However, the transfer-based black-
box attacks require access to the training data of the target model. To overcome the limi-
tation, the query-based black-box attacks have been proposed where the attacks are based
on the outputs obtained by querying the target models directly [114, 115, 116, 115, 117].
In addition, based on the constraints on the adversarial images, the generated adver-
sarial perturbations can be quasi-imperceptible or unbounded. The popular metric of to
measure the distance between clean images and adversarial image is `pnorm [98], such
as,`1,`2and`1. However, the metric is not perfectly aligned with human perception.
The more advanced metric has also been explored in the community, e.g., Wasserstein
distance [118].
Given the potential threats posed by adversarial attacks, many defense strategies have
been proposed to build adversarially robust models. One of the most eective defense
methods is adversarial training, which creates adversarial examples and adds them to the
training dataset in each training iteration. Besides, the pre-processing methods have been
explored to purify adversarial examples [119, 120, 121, 122, 123, 124, 125, 126, 127, 128,
129, 130, 131, 132, 133]. However, some of the defense strategies have broken again in later
publications [134]. Some defense methods provide certied robustness to break arm-race
between adversary and defense [135, 136, 137, 138, 139, 140, 141, 142, 143, 144]. Even
many methods have been published to address, the accuracy of the model under attacks is
still much lower than the accuracy on clean images, especially on the large dataset [145].
In addition to building robust model, another way to address the threats is to detect
adversarial examples rst [146, 147, 148, 149, 150, 151, 152].1.4 Robustness of Deep Visual Classication Models 25
In this subsection, we categorize the robustness of image classications. Our contri-
butions of this thesis mainly focus on the role the model architecture plays in terms of
both natural robustness and adversarial robustness. In the rest of this section, we present
our contributions towards the robustness of image classication models, such as Capsule
Networks and Vision Transformers.
1.4.2 Robustness of Capsule Network-based Classication
Human visual recognition is quite insensitive to ane transformations. For example, enti-
ties in an image, and a rotated version of the entities in the image, can both be recognized
by the human visual system, as long as the rotation is not too large. Convolutional Neural
Networks (CNNs), the currently leading approach to image analysis, achieve ane ro-
bustness by training on a large amount of data that contain dierent transformations of
target objects. Given limited training data, a common issue in many real-world tasks, the
robustness of CNNs to novel ane transformations is limited [12].
With the goal of learning image features that are more aligned with human percep-
tion, Capsule Networks (CapsNets) have recently been proposed [12]. Our work [13] rst
investigates the eectiveness of components that make CapsNets robust to input ane
transformations, with a focus on the routing algorithm. However, recent work [153] shows
that all routing algorithms proposed so far perform even worse than a uniform/random
routing procedure.
From both numerical analysis and empirical experiments, our investigation reveals that
the dynamic routing procedure contributes neither to the generalization ability nor to the
ane robustness of CapsNets. Therefore, it is infeasible to improve the ane robustness by
modifying the routing procedure. Instead, we investigate the limitations of the CapsNet
architectures and propose a simple solution. Namely, we propose to apply an identical
transformation function for all primary capsules and replace the routing with a simple
averaging procedure.
Besides the high ane transformation robustness, CapsNets also demonstrate other ad-
vantages, such as the ability to recognize overlapping digits and the semantic representation
compactness. In recent years, It has been suggested that CapsNets have the potential to
surpass the dominant convolutional networks in these aspects [12, 42, 48, 154]. However,
there lack of comprehensive comparisons to support this assumption, and even for some
reported improvements, there are no solid ablation studies to gure out which ones of the
components in CapsNets are, in fact, eective.26 1. Introduction
In our work [155], we rst carefully examine the major dierences in design between the
capsule networks and the common convolutional networks adopted for image classication.
The dierence can be summarized as a non-shared transformation module, a dynamic
routing layer to automatically group input capsules to produce output capsules, a squashing
function, a marginal classication loss, and a class-conditional reconstruction sub-network
with a reconstruction loss .
Unlike previous studies [12, 42] which usually take CapsNet as a whole to test its
robustness, our work [155] instead tries to study the eects of each of the above components
in their eectiveness on robustness. We consider the three dierent aspects, such as the
robustness to ane transformations, the ability to recognize overlapping digits, and the
semantic representation compactness.
Our investigations reveal that some widely believed benets of Capsule networks could
be wrong:
1. The dynamic routing actually may harm the robustness to input ane transforma-
tion, in contrast to the common belief;
2. The high performance of CapsNets to recognize overlapping digits can be mainly
attributed to the extra modeling capacity brought by the transformation matrices.
3. Some components of CapsNets are indeed benecial for learning semantic represen-
tations, e.g., the conditional reconstruction and the squashing function, but they are
mainly auxiliary components and can be applied beyond CapsNets.
In addition to these ndings, we also enhance common ConvNets by the useful compo-
nents of CapsNet, and achieve greater robustness. Our investigation shows that Capsule
Network is not more robust than Convolutional Network.
1.4.3 Robustness of Vision Transformer-based Classication
CapsNets with brain-inspired architectures have more inductive bias than CNNs. Dierent
from CapsNet, Vision Transformer (ViT) [17] has less architecture bias than CNNs. ViT
processes the input image as a sequence of image patches. Then, a self-attention mechanism
is applied to aggregate information from all patches. Existing works have shown that
ViTs are more robust than CNNs when the whole input image is perturbed with natural
corruptions or adversarial perturbations [156, 157, 158]. Given the patch-based architecture
of ViT, our work studies the robustness of ViT to patch-based perturbation.1.4 Robustness of Deep Visual Classication Models 27
Two typical types of perturbations are considered to compare the robustness between
ViTs and CNN (e.g., ResNets [3]). One is natural corruptions [14], which is to test models'
robustness under distributional shift. The other is adversarial perturbations [159, 119],
which are created by an adversary to specically fool a model to make a wrong prediction.
We reveal that ViT does not always perform more robustly than ResNet. When indi-
vidual image patches are naturally corrupted, ViT performs more robustly than ResNet.
However, when input image patch(s) are adversarially attacked, ViT shows a higher vul-
nerability. Digging down further, we nd the reason behind this is that the self-attention
mechanism of ViT can eectively ignore the natural patch corruption, while it's also easy
to manipulate the self-attention mechanism to focus on an adversarial patch.
Based on the patch-based architectural structure of vision transformers, we further
investigate the sensitivity of ViT against patch positions and patch alignment of adversarial
patches. First, we discover that ViT is insensitive to dierent patch positions, while ResNet
shows high vulnerability on the central area of input images and much less on corners. We
attribute this to the architecture bias of ResNet where pixels in the center can aect more
neurons than the ones in corners. In contrast, each patch within ViT can equally interact
with other patches regardless of its position. Further, we nd that for ViT, the adversarial
perturbation designed to attack one particular position can successfully transfer to other
positions of the same image as long as they are aligned with input patches. In contrast,
the ones on ResNet hardly do.
To summarise, in our work [93], we compare ViT and CNNs in terms of the robustness
to natural patch corruptions or adversarial patch attacks.28 1. IntroductionBibliography
[1] Yann LeCun, Yoshua Bengio, and Georey Hinton. Deep learning. In nature , 2015.
[2] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-
scale image recognition. In ICLR , 2014.
[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In Proceedings of the IEEE International Conference on Computer
Vision , pages 770{778, 2016.
[4] Christian Szegedy, Vincent Vanhoucke, Sergey Ioe, Jon Shlens, and Zbigniew Wo-
jna. Rethinking the inception architecture for computer vision. In Proceedings of the
IEEE International Conference on Computer Vision , pages 2818{2826, 2016.
[5] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
Densely connected convolutional networks. In Proceedings of the IEEE International
Conference on Computer Vision , pages 4700{4708, 2017.
[6] Jinkyu Kim and John F Canny. Interpretable learning for self-driving cars by visu-
alizing causal attention. In International Conference on Computer Vision (ICCV) ,
pages 2961{2969, 2017.
[7] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, Zeynep Akata, et al.
Textual explanations for self-driving vehicles. In ECCV , pages 577{593. Springer,
2018.
[8] Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed
impact of fair machine learning. In ICML , 2018.
[9] Tatsunori B Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.
Fairness without demographics in repeated loss minimization. In ICML , 2018.30 BIBLIOGRAPHY
[10] Jindong Gu and Daniela Oelke. Understanding bias in machine learning. In arXiv
preprint arXiv:1909.01866 , 2019.
[11] Andrew Selbst and Julia Powles. \meaningful information" and the right to expla-
nation. In Conference on Fairness, Accountability and Transparency , pages 48{48.
PMLR, 2018.
[12] Sara Sabour, Nicholas Frosst, and Georey E Hinton. Dynamic routing between
capsules. In Advances in neural information processing systems (NeurIPS) , pages
3856{3866, 2017.
[13] Jindong Gu and Volker Tresp. Improving the robustness of capsule networks to image
ane transformations. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 7285{7293, 2020.
[14] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness
to common corruptions and perturbations. In International Conference on Learning
Representations (ICLR) , 2019.
[15] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize
to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceed-
ings of the 2016 acm sigsac conference on computer and communications security ,
pages 1528{1540, 2016.
[16] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei
Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world at-
tacks on deep learning visual classication. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 1625{1634, 2018.
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recog-
nition at scale. In arXiv:2010.11929 , 2020.
[18] Alex Krizhevsky, Ilya Sutskever, and Georey E Hinton. Imagenet classication with
deep convolutional neural networks. In Advances in neural information processing
systems , 2012.BIBLIOGRAPHY 31
[19] Ross Girshick, Je Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hier-
archies for accurate object detection and semantic segmentation. In CVPR , pages
580{587, 2014.
[20] Kaiming He, Georgia Gkioxari, Piotr Doll ar, and Ross Girshick. Mask r-cnn. In
International Conference on Computer Vision (ICCV) , pages 2961{2969, 2017.
[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks
for semantic segmentation. In Proceedings of the IEEE International Conference on
Computer Vision , pages 3431{3440, 2015.
[22] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyra-
mid scene parsing network. In CVPR , 2017.
[23] Liang-Chieh Chen, George Papandreou, Florian Schro, and Hartwig Adam. Re-
thinking atrous convolution for semantic image segmentation. In arXiv:1706.05587 ,
2017.
[24] David G Lowe. Object recognition from local scale-invariant features. In Proceedings
of the seventh IEEE international conference on computer vision , volume 2, pages
1150{1157. Ieee, 1999.
[25] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection.
In2005 IEEE computer society conference on computer vision and pattern recognition
(CVPR'05) , volume 1, pages 886{893. Ieee, 2005.
[26] Yann LeCun, L eon Bottou, Yoshua Bengio, and Patrick Haner. Gradient-based
learning applied to document recognition. In Proceedings of the IEEE , 1998.
[27] Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi. A sur-
vey of the recent architectures of deep convolutional neural networks. In Articial
intelligence review , 2020.
[28] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen
Wei. Deformable convolutional networks. In Proceedings of the IEEE International
Conference on Computer Vision , pages 764{773, 2017.
[29] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Ecient32 BIBLIOGRAPHY
convolutional neural networks for mobile vision applications. In arXiv preprint
arXiv:1704.04861 , 2017.
[30] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shuenet: An extremely
ecient convolutional neural network for mobile devices. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 6848{6856, 2018.
[31] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In NIPS ,
pages 598{605, 1990.
[32] Babak Hassibi and David G Stork. Second order derivatives for network pruning:
Optimal brain surgeon. In NeurIPS , 1993.
[33] Song Han, Je Pool, John Tran, and William Dally. Learning both weights and
connections for ecient neural network. In NeurIPS , pages 1135{1143, 2015.
[34] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning
convolutional neural networks for resource ecient inference. In ICLR , 2017.
[35] Georey Hinton, Oriol Vinyals, and Je Dean. Distilling the knowledge in a neural
network. In stat, volume 1050, page 9, 2015.
[36] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo
Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR , 2015.
[37] Jindong Gu and Volker Tresp. Search for better students to learn distilled knowledge.
InarXiv preprint arXiv:2001.11612 , 2020.
[38] Jindong Gu, Wei Liu, and Yonglong Tian. Simple distillation baselines for improving
small self-supervised models. In arXiv preprint arXiv:2106.11304 , 2021.
[39] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning.
InICLR , 2017.
[40] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray
Kavukcuoglu. Hierarchical representations for ecient architecture search. In ICLR ,
2018.
[41] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Dierentiable architecture
search. In ICLR , 2019.BIBLIOGRAPHY 33
[42] Georey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em
routing. In International conference on learning representations (ICLR) , 2018.
[43] Taeyoung Hahn, Myeongjang Pyeon, and Gunhee Kim. Self-routing capsule net-
works. In Advances in Neural Information Processing Systems (NeurIPS) , pages
7658{7667, 2019.
[44] Fabio De Sousa Ribeiro, Georgios Leontidis, and Stefanos D Kollias. Capsule routing
via variational bayes. In AAAI , pages 3749{3756, 2019.
[45] Karim Ahmed and Lorenzo Torresani. Star-caps: Capsule networks with straight-
through attentive routing. In Advances in Neural Information Processing Systems ,
2019.
[46] Yao-Hung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov.
Capsules with inverted dot-product attention routing. In International Conference
on Learning Representations (ICLR) , 2020.
[47] Jindong Gu. Interpretable graph capsule networks for object recognition. In Pro-
ceedings of the AAAI Conference on Articial Intelligence (AAAI) , 2020.
[48] Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima
Jayasekara, Suranga Seneviratne, and Ranga Rodrigo. Deepcaps: Going deeper with
capsule networks. In The IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 10725{10733, 2019.
[49] Sai Samarth R Phaye, Apoorva Sikka, Abhinav Dhall, and Deepti R Bathula. Multi-
level dense capsule networks. In Asian Conference on Computer Vision , pages 577{
592. Springer, 2018.
[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances
in neural information processing systems , pages 5998{6008, 2017.
[51] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-
rolles, and Herv e J egou. Training data-ecient image transformers & distillation
through attention. In International Conference on Machine Learning , pages 10347{
10357. PMLR, 2021.34 BIBLIOGRAPHY
[52] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
Transformer in transformer. In arXiv:2103.00112 , 2021.
[53] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted
windows. In arXiv:2103.14030 , 2021.
[54] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin,
Herv e J egou, and Matthijs Douze. Levit: a vision transformer in convnet's clothing
for faster inference. In arXiv:2104.01136 , 2021.
[55] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll ar, and Ross Gir-
shick. Early convolutions help transformers see better. In arXiv:2106.14881 , 2021.
[56] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszko-
reit, et al. Mlp-mixer: An all-mlp architecture for vision. In Advances in Neural
Information Processing Systems , volume 34, 2021.
[57] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
and Saining Xie. A convnet for the 2020s. In arXiv preprint arXiv:2201.03545 , 2022.
[58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-
hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
Learning transferable visual models from natural language supervision. In Interna-
tional Conference on Machine Learning , pages 8748{8763. PMLR, 2021.
[59] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unied vision-language
pre-training with mixture-of-modality-experts. In arXiv preprint arXiv:2111.02358 ,
2021.
[60] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional
networks: Visualising image classication models and saliency maps. In ICLR , 2013.
[61] Sebastian Bach, Alexander Binder, Gr egoire Montavon, Frederick Klauschen, Klaus-
Robert M uller, and Wojciech Samek. On pixel-wise explanations for non-linear clas-
sier decisions by layer-wise relevance propagation. In PloS one , 2015.BIBLIOGRAPHY 35
[62] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, Dhruv Batra, et al. Grad-cam: Visual explanations from deep
networks via gradient-based localization. In International Conference on Computer
Vision (ICCV) , pages 618{626, 2017.
[63] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important
features through propagating activation dierences. In ICML-Volume 70 , pages 3145{
3153. JMLR. org, 2017.
[64] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep
networks. In ICML-Volume 70 , pages 3319{3328. JMLR. org, 2017.
[65] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi egas, and Martin Wattenberg.
Smoothgrad: removing noise by adding noise. In arXiv preprint arXiv:1706.03825 ,
2017.
[66] Jindong Gu, Yinchong Yang, and Volker Tresp. Understanding individual decisions
of cnns via contrastive backpropagation. In Asian Conference on Computer Vision ,
2018.
[67] Suraj Srinivas and Fran cois Fleuret. Full-gradient representation for neural network
visualization. In Advances in Neural Information Processing Systems , pages 4126{
4135, 2019.
[68] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classiers.
InAdvances in Neural Information Processing Systems , pages 6967{6976, 2017.
[69] Patrick Schwab and Walter Karlen. Cxplain: Causal explanations for model inter-
pretation under uncertainty. In Advances in Neural Information Processing Systems ,
pages 10220{10230, 2019.
[70] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?:
Explaining the predictions of any classier. In Proceedings of the 22nd ACM SIGKDD
international conference on knowledge discovery and data mining , pages 1135{1144.
ACM, 2016.
[71] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep
neural network decisions: Prediction dierence analysis. In ICLR , 2017.36 BIBLIOGRAPHY
[72] Jindong Gu. Verication of classication decisions in convolutional neural networks,
2022. US Patent App. 17/294,746.
[73] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by
meaningful perturbation. In Proceedings of the IEEE International Conference on
Computer Vision , pages 3429{3437, 2017.
[74] Jindong Gu and Volker Tresp. Contextual prediction dierence analysis for explaining
individual image classications. In arXiv preprint arXiv:1910.09086 , 2019.
[75] Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explain-
ing image classiers by counterfactual generation. In ICLR , 2019.
[76] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Coun-
terfactual visual explanations. In ICML , 2019.
[77] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Je Donahue, Bernt Schiele,
and Trevor Darrell. Generating visual explanations. In European Conference on
Computer Vision (ECCV) , pages 3{19. Springer, 2016.
[78] Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. Grounding
visual explanations. In European Conference on Computer Vision (ECCV) , pages
269{286. Springer, 2018.
[79] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inuence
functions. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70 , pages 1885{1894. JMLR. org, 2017.
[80] Mhd Hasan Sarhan, Abouzar Eslami, Nassir Navab, and Shadi Albarqouni. Learning
interpretable disentangled representations using adversarial vaes. In Domain Adap-
tation and Representation Transfer and Medical Image Learning with Less Labels and
Imperfect Data , pages 37{44. Springer, 2019.
[81] Dahuin Jung, Jonghyun Lee, Jihun Yi, and Sungroh Yoon. icaps: An interpretable
classier via disentangled capsule networks. In arXiv preprint arXiv:2008.08756 ,
2020.
[82] Xinqi Zhu, Chang Xu, and Dacheng Tao. Where and what? examining interpretable
disentangled representations. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 5861{5870, 2021.BIBLIOGRAPHY 37
[83] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller.
Striving for simplicity: The all convolutional net. In ICLR , 2014.
[84] Jindong Gu and Volker Tresp. Saliency methods for explaining adversarial attacks.
InarXiv preprint arXiv:1908.08413 , 2019.
[85] Jindong Gu and Volker Tresp. Semantics for global and local interpretation of deep
neural networks. In arXiv preprint arXiv:1910.09085 , 2019.
[86] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional net-
works for biomedical image segmentation. In International Conference on Medical
image computing and computer-assisted intervention , pages 234{241. Springer, 2015.
[87] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K
Su. This looks like that: deep learning for interpretable image recognition. In
Advances in neural information processing systems , volume 32, 2019.
[88] Jindong Gu and Volker Tresp. Neural network memorization dissection. In arXiv
preprint arXiv:1911.09537 , 2019.
[89] Yinchong Yang, Volker Tresp, Marius Wunderle, and Peter A Fasching. Explaining
therapy predictions with layer-wise relevance propagation in neural networks. In
2018 IEEE International Conference on Healthcare Informatics (ICHI) , pages 152{
162. IEEE, 2018.
[90] Leila Arras, Gr egoire Montavon, Klaus-Robert M uller, and Wojciech Samek. Ex-
plaining recurrent neural network predictions in sentiment analysis. In arXiv preprint
arXiv:1706.07206 , 2017.
[91] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A
large-scale hierarchical image database. In 2009 IEEE conference on computer vision
and pattern recognition , pages 248{255. Ieee, 2009.
[92] Samira Abnar and Willem Zuidema. Quantifying attention ow in transformers. In
Annual Meeting of the Association for Computational Linguistics (ACL) , 2020.
[93] Jindong Gu, Volker Tresp, and Yao Qin. Are vision transformers robust to patch
perturbations? In arXiv preprint arXiv:2111.10659 , 2021.38 BIBLIOGRAPHY
[94] Jungkyu Lee, Taeryun Won, Tae Kwan Lee, Hyemin Lee, Geonmo Gu, and Kiho
Hong. Compounding the performance improvements of assembled techniques in a
convolutional neural network. In arXiv preprint arXiv:2001.06268 , 2020.
[95] Simyung Chang, John Yang, SeongUk Park, and Nojun Kwak. Broadcasting con-
volutional network for visual relational reasoning. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 754{769, 2018.
[96] IJ Goodfellow, J Shlens, and C Szegedy. Explaining and harnessing adversarial
examples. In ICLR , 2014.
[97] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In
arXiv preprint arXiv:1706.06083 , 2017.
[98] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. The limitations of deep learning in adversarial settings. In
2016 IEEE European symposium on security and privacy (EuroS&P) , pages 372{387.
IEEE, 2016.
[99] Tom B Brown, Dandelion Man e, Aurko Roy, Mart n Abadi, and Justin Gilmer.
Adversarial patch. In arXiv preprint arXiv:1712.09665 , 2017.
[100] Danny Karmon, Daniel Zoran, and Yoav Goldberg. Lavan: Localized and visible
adversarial noise. In International Conference on Machine Learning , pages 2507{
2515. PMLR, 2018.
[101] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song.
Spatially transformed adversarial examples. In arXiv preprint arXiv:1801.02612 ,
2018.
[102] Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition Work-
shops , pages 1614{1619, 2018.
[103] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable
adversarial examples and black-box attacks. In arXiv preprint arXiv:1611.02770 ,
2016.BIBLIOGRAPHY 39
[104] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and
Alan L Yuille. Improving transferability of adversarial examples with input diversity.
InCVPR , 2019.
[105] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable
adversarial examples by translation-invariant attacks. In CVPR , 2019.
[106] Junhua Zou, Zhisong Pan, Junyang Qiu, Xin Liu, Ting Rui, and Wei Li. Improv-
ing the transferability of adversarial examples with resized-diverse-inputs, diversity-
ensemble and region tting. In ECCV , 2020.
[107] Yiwen Guo, Qizhang Li, and Hao Chen. Backpropagating linearly improves trans-
ferability of adversarial examples. In NeurIPS , 2020.
[108] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip
connections matter: On the transferability of adversarial examples generated with
resnets. In ICLR , 2020.
[109] Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim.
Enhancing adversarial example transferability with an intermediate level attack. In
International Conference on Computer Vision (ICCV) , 2019.
[110] Nathan Inkawhich, Kevin J Liang, Binghui Wang, Matthew Inkawhich, Lawrence
Carin, and Yiran Chen. Perturbing across the feature hierarchy to improve standard
and strict blackbox attack transferability. In NeurIPS , 2020.
[111] Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, and Alan Yuille.
Learning transferable adversarial examples via ghost networks. In AAAI , 2020.
[112] Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, and Quanshi Zhang.
A unied approach to interpreting and boosting adversarial transferability. In ICLR ,
2021.
[113] Jindong Gu, Hengshuang Zhao, Volker Tresp, and Philip Torr. Adversarial examples
on segmentation models can be easy to transfer. In arXiv preprint arXiv:2111.11368 ,
2021.
[114] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo:
Zeroth order optimization based black-box attacks to deep neural networks without40 BIBLIOGRAPHY
training substitute models. In Proceedings of the 10th ACM workshop on articial
intelligence and security , 2017.
[115] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui
Hsieh. Query-ecient hard-label black-box attack: An optimization-based approach.
InarXiv preprint arXiv:1807.04457 , 2018.
[116] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Practical black-box attacks
on deep neural networks using ecient query mechanisms. In ECCV , 2018.
[117] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
Square attack: a query-ecient black-box adversarial attack via random search. In
ECCV , 2020.
[118] Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via
projected sinkhorn iterations. In International Conference on Machine Learning ,
pages 6808{6817. PMLR, 2019.
[119] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harness-
ing adversarial examples. In arXiv preprint arXiv:1412.6572 , 2014.
[120] Florian Tram er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and
Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In arXiv
preprint arXiv:1705.07204 , 2017.
[121] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural
networks. In 2017 ieee symposium on security and privacy (sp) , pages 39{57. IEEE,
2017.
[122] Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for
free! In arXiv preprint arXiv:1904.12843 , 2019.
[123] Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You
only propagate once: Accelerating adversarial training via maximal principle. In
NeurIPS , 2019.
[124] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and
Michael Jordan. Theoretically principled trade-o between robustness and accuracy.
InInternational Conference on Machine Learning , pages 7472{7482. PMLR, 2019.BIBLIOGRAPHY 41
[125] Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving
fast adversarial training. In arXiv preprint arXiv:2007.02617 , 2020.
[126] Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overtting in
single-step adversarial training. In arXiv preprint arXiv:2010.01799 , 2020.
[127] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting
adversarial training. In arXiv preprint arXiv:2001.03994 , 2020.
[128] Leslie Rice, Eric Wong, and Zico Kolter. Overtting in adversarially robust deep
learning. In International Conference on Machine Learning , pages 8093{8104.
PMLR, 2020.
[129] BS Vivek and R Venkatesh Babu. Single-step adversarial training with dropout
scheduling. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR) , pages 947{956. IEEE, 2020.
[130] Saehyung Lee, Hyungyu Lee, and Sungroh Yoon. Adversarial vertex mixup: To-
ward better adversarially robust generalization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 272{281, 2020.
[131] Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan
Gu. On the convergence and robustness of adversarial training. In arXiv preprint
arXiv:2112.08304 , 2021.
[132] Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, et al. Towards ecient and
eective adversarial training. In Advances in Neural Information Processing Systems ,
volume 34, 2021.
[133] Boxi Wu, Heng Pan, Li Shen, Jindong Gu, Shuai Zhao, Zhifeng Li, Deng Cai, Xiaofei
He, and Wei Liu. Attacking adversarial attacks as a defense. In arXiv preprint
arXiv:2106.04938 , 2021.
[134] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false
sense of security: Circumventing defenses to adversarial examples. In International
conference on machine learning , pages 274{283. PMLR, 2018.
[135] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certied defenses against
adversarial examples. In arXiv preprint arXiv:1801.09344 , 2018.42 BIBLIOGRAPHY
[136] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. Certied robust-
ness for top-k predictions against adversarial perturbations via randomized smooth-
ing. In arXiv preprint arXiv:1912.09899 , 2019.
[137] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certied adversarial robustness
via randomized smoothing. In International Conference on Machine Learning , pages
1310{1320. PMLR, 2019.
[138] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certied adversarial
robustness with additive noise. In Advances in neural information processing systems ,
volume 32, 2019.
[139] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A
convex relaxation barrier to tight robustness verication of neural networks. In
Advances in Neural Information Processing Systems , volume 32, 2019.
[140] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien
Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained
smoothed classiers. In Advances in Neural Information Processing Systems , vol-
ume 32, 2019.
[141] Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca
Daniel. Higher-order certication for randomized smoothing. In Advances in Neural
Information Processing Systems , volume 33, pages 4501{4511, 2020.
[142] Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised
smoothing: A provable defense for pretrained classiers. In Advances in Neural
Information Processing Systems , volume 33, pages 21945{21957, 2020.
[143] Jindong Gu, Hengshuang Zhao, Volker Tresp, and Philip HS Torr. Segpgd: An
eective and ecient adversarial attack for evaluating and boosting segmentation
robustness. In European Conference on Computer Vision , pages 308{325. Springer,
2022.
[144] Boxi Wu, Jindong Gu, Zhifeng Li, Deng Cai, Xiaofei He, and Wei Liu. Towards
ecient adversarial training on vision transformers. In European Conference on
Computer Vision , pages 307{325. Springer, 2022.BIBLIOGRAPHY 43
[145] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He. Fea-
ture denoising for improving adversarial robustness. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 501{509, 2019.
[146] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial
examples in deep neural networks. In arXiv preprint arXiv:1704.01155 , 2017.
[147] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. Detect-
ing adversarial samples from artifacts. In arXiv preprint arXiv:1703.00410 , 2017.
[148] Tianyu Pang, Chao Du, Yinpeng Dong, and Jun Zhu. Towards robust detection
of adversarial examples. In Advances in Neural Information Processing Systems ,
volume 31, 2018.
[149] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unied framework
for detecting out-of-distribution samples and adversarial attacks. In Advances in
neural information processing systems , volume 31, 2018.
[150] Zhihao Zheng and Pengyu Hong. Robust detection of adversarial attacks by modeling
the intrinsic properties of deep neural networks. In Advances in Neural Information
Processing Systems , volume 31, 2018.
[151] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical
test for detecting adversarial examples. In International Conference on Machine
Learning , pages 5498{5507. PMLR, 2019.
[152] Gilad Cohen, Guillermo Sapiro, and Raja Giryes. Detecting adversarial samples
using inuence functions and nearest neighbors. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages 14453{14462, 2020.
[153] Inyoung Paik, Taeyeong Kwak, and Injung Kim. Capsule networks need an improved
routing algorithm. In Asian Conference on Machine Learning , pages 489{502. PMLR,
2019.
[154] Jindong Gu, Baoyuan Wu, and Volker Tresp. Eective and ecient vote attack on
capsule networks. In International Conference on Learning Representations (ICLR) ,
2021.44 BIBLIOGRAPHY
[155] Jindong Gu, Volker Tresp, and Han Hu. Capsule network is not more robust than
convolutional network. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14309{14317, 2021.
[156] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Un-
terthiner, and Andreas Veit. Understanding robustness of transformers for image
classication. In arXiv:2103.14586 , 2021.
[157] Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the
adversarial robustness of visual transformers. In arXiv:2103.15670 , 2021.
[158] Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. In
arXiv:2105.07581 , 2021.
[159] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR ,
2013.