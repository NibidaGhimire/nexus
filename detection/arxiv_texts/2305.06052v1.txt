Post-training Model Quantization Using GANs for Synthetic Data Generation
Athanasios Masouris
Delft University of Technology
Delft, The Netherlands
a.masouris@student.tudelft.nlMansi Sharma
Intel
Washington, USA
mansi.sharma@intel.comAdrian Boguszewski
Intel
Swindon, UK
adrian.boguszewski@intel.com
Alexander Kozlov
Intel
Dubai, UAE
alexander.kozlov@intel.comZhuo Wu
Intel
Shanghai, China
zhuo.wu@intel.comRaymond Lo
Intel
Santa Clara, USA
raymond.lo@intel.com
Abstract
Quantization is a widely adopted technique for deep neu-
ral networks to reduce the memory and computational re-
sources required. However, when quantized, most models
would need a suitable calibration process to keep their per-
formance intact, which requires data from the target do-
main, such as a fraction of the dataset used in model train-
ing and model validation ( i.e. calibration dataset).
In this study, we investigate the use of synthetic data
as a substitute for the calibration with real data for the
quantization method. We propose a data generation method
based on Generative Adversarial Networks that are trained
prior to the model quantization step. We compare the
performance of models quantized using data generated
by StyleGAN2-ADA and our pre-trained DiStyleGAN, with
quantization using real data and an alternative data gen-
eration method based on fractal images. Overall, the re-
sults of our experiments demonstrate the potential of lever-
aging synthetic data for calibration during the quantiza-
tion process. In our experiments, the percentage of ac-
curacy degradation of the selected models was less than
0:6%, with our best performance achieved on MobileNetV2
(0:05%). The code is available at: https://github.
com/ThanosM97/gsoc2022-openvino
1. Introduction
Model optimization is essential in deep learning since
it demonstrates efﬁcacy in using computational resources,
scalability, and even better performance. Techniques such
as model pruning [14,18,35], weight compression [15], and
knowledge distillation [1, 20] have been proven to reduce
the model complexity without sacriﬁcing its accuracy.
Another approach that stands out is model quantiza-tion. Model quantization is a method used in deep learn-
ing to reduce a model’s memory footprint and computa-
tional complexity and enable inference on hardware with
limited resources ( e.g. smartphones, IoT devices). Addi-
tionally, quantization facilitates faster inference by reducing
the computations required during forward propagation, and
thus less power consumption.
Quantization is achieved by reducing the precision
of model parameters and activations from their original
ﬂoating-point precision to a lower-precision representation
(e.g. 8-bit ﬁxed-point). There are two types of quantiza-
tion, weight and activation. The goal of the weight quanti-
zation process is to ﬁnd the nearest low-precision weights
to the original ﬂoating-point ones. Activation quantization
reduces the precision of the activations produced by the
model. It can be achieved by either setting a precision based
on the range of the activations during runtime ( i.e. dynamic
ﬁxed-point quantization) or by setting the same precision
for all activations ( i.e. uniform ﬁxed-point quantization).
While a model can be quantized and trained from scratch
(i.e.quantization-aware training ), a more practical ap-
proach is post-training quantization , which optimizes the
performance of existing pre-trained models. However, post-
training quantization can have a signiﬁcant impact on model
performance. The goal is to minimize the accuracy loss
caused by quantization errors, while maximizing the ef-
ﬁciency of the end model. Speciﬁc methods, such as
accuracy-aware quantization, can also be applied. In keep-
ing the model’s accuracy intact, these methods require a cal-
ibration dataset ( i.e. a small set of samples representative of
the overall dataset the model was trained on). Neverthe-
less, due to privacy concerns, private use, or the scale of the
dataset, a calibration subset may not always be available. In
these cases, synthetic-generated data can be leveraged.
In this study, we investigate the use of synthetic data gen-arXiv:2305.06052v1  [cs.CV]  10 May 2023erated by Generative Adversarial Networks (GANs) [12] as
a substitute for the calibration with real data by the quanti-
zation method. GANs are known for their ability to gener-
ate new, realistic, and diverse data samples that are similar
to the original data distribution. While the publication of a
dataset might be hindered by privacy concerns or the sheer
size of the data, a GAN model can become publicly avail-
able and thus be used to produce synthetic samples accord-
ing to the users’ needs or the requirements of a process ( i.e.
quantization).
2. Related Work
Prompted by the ever-increasing size of deep neural net-
works, techniques for reducing their computational costs
have been thoroughly studied for efﬁcient learning. Early
attempts focused on reducing the number of network pa-
rameters by grouping the weights [6], replacing costly op-
erations such as fully connected layers [36], or by pruning
connections between layers [16, 26, 27]. Network quanti-
zation has also been studied, with early work representing
weights and activations using only a single bit, introducing
the Binarized Neural Networks (BNNs) [9, 21]. While this
representation achieved a substantial reduction in compu-
tational costs, it also led to accuracy degradation on more
complex models and datasets. In Gupta et al. [13], the au-
thors demonstrated that using 16-bit ﬁxed-point represen-
tation with stochastic rounding when training a CNN leads
to negligible degradation in the classiﬁcation accuracy. In
Banner et al. [2], the precision was further reduced to an
8-bit representation while they quantized both weights and
activations in all layers. Their proposed 8-bit training did
not affect the models’ accuracy when trained on a large-
scale dataset. In Zhang et al. [39], the authors proposed a
quantization method by training quantizers from data. In
Han et al . [14], they attempted to compress deep neural
networks by combining pruning, trained quantization, and
Huffman coding. Although their method could signiﬁcantly
compress the size of deep neural networks, the pruning pro-
cess can be time-consuming and difﬁcult to optimize.
Post-training quantization has also been the focus of re-
search. In Lin et al. [28], the authors proposed an SQNR-
based optimization approach to convert a pre-trained ﬂoat-
ing point deep convolutional model to its ﬁxed-point equiv-
alent. In Banner et al. [3], 4-bit quantization was proposed
using the ACIQ method to optimize the clipping value,
while in Choukroun et al. [7], they minimized the quanti-
zation MSE for both weights and activations. More recent
studies suggested quantization by splitting outlier channels
[40], using adaptive weight rounding [30], or bias correc-
tion [11]. Additionally, given that post-training quantiza-
tion requires a small calibration dataset, which may not be
readily available, studies have addressed this issue by either
employing data-free quantization techniques [4,31] or usingsynthetic data [25].
3. Methodology
In this study, we examine the effect of the quantization
process on the performance of the models in terms of classi-
ﬁcation accuracy. To do so, we utilized OpenVINO’s Post-
training Optimization Tool (POT) [8], which supports uni-
form integer quantization. Using this tool, the weights and
activations of the classiﬁcation models were converted from
ﬂoating-point to integer precision (8-bit representation).
POT provides two quantization methods. First, the De-
fault Quantization process performs the quantization using
a non-labeled calibration dataset to estimate the range of ac-
tivation values. Second, the Accuracy-aware Quantization
provides control over the deﬁned accuracy metric, allowing
the tool to quantize speciﬁc layers of the model while main-
taining the accuracy within a predeﬁned range. In contrast
with the default quantization, the accuracy-aware quantiza-
tion requires a labeled calibration dataset.
Given that a calibration dataset may not always be avail-
able, in this study, we also investigate the use of syn-
thetic data for the calibration process. We generated syn-
thetic data using Generative Adversarial Networks (GANs)
[12], StyleGAN2-ADA [22] and our own DiStyleGAN
(Sec. 3.2), pre-trained to approximate the distribution of the
real data the classiﬁcation models were trained on.
Subsequently, experiments were conducted using the
Default Quantization and the Accuracy-aware quantization
methods provided by POT. The quantized models were pre-
trained on the classiﬁcation task on the CIFAR-10 dataset
[24], and their performances upon quantization were com-
pared on the CIFAR-10 test set. Furthermore, both quan-
tization techniques were applied using multiple calibration
datasets (real, synthetic, and fractal).
3.1. Models
Five models were selected to be quantized during the
experiments. All of them were pre-trained on the CIFAR-
10 dataset for classiﬁcation in PyTorch and were obtained
from PyTorch Hub1. The models, along with their corre-
sponding versions, that were quantized were the follow-
ing: ResNet20 (resnet20) [17], VGG16 (vgg16 bn)[34],
MobileNetV2 (mobilenetv2 x14)[33], ShufﬂeNetV2 (shuf-
ﬂenetv2 x20)[29], and RepVGG (repvgg a2)[10].
3.2. GAN Training
The current state-of-the-art model in class-conditional
image generation on CIFAR-10 is StyleGAN2 [23]. While
this model can generate synthetic images that look similar
to the ones from the CIFAR-10 dataset, generating those
images requires substantial computational resources due to
1https://github.com/chenyaofo/pytorch-cifar-modelsFigure 1. Overview of DiStyleGAN’s architecture
its more than 20 million parameters. In an attempt to lower
this requirement, we used the knowledge distillation frame-
work described in Chang and Lu [5]. In particular, we used
StyleGAN2 as a teacher network, to train a student network,
DiStyleGAN (Fig. 1), to produce synthetic images that ap-
proximate the CIFAR-10 distribution while requiring only
about a tenth of the former’s parameters.
3.2.1 Black-Box Distillation
Similar to Chang et al. [5], the teacher network is applied as
a black box, requiring only limited access to its input-output
pairs. In particular, 50;000synthetic images generated by
StyleGAN2 were collecte, equally distributed among the 10
classes of CIFAR-10, along with the input noise vectors and
the corresponding class labels. This collection, or dataset,
is then used to train DiStyleGAN in a supervised way. Fol-
lowing this approach, no knowledge of the internal or inter-
mediate features of the teacher model is required and can be
discarded afterward.
3.2.2 Objectives
For the training of DiStyleGAN, we leveraged the same ob-
jectives as in [5].
LG=LKDfeat+1LKDpix+2LKDS+3LGAN S(1)
whereLKDfeatis the feature-level distillation loss calcu-
lated from feature maps extracted using the Discriminator’s
network for the images generated by both the student andthe teacher networks, LKDpixis the pixel-level distillation
loss calculated as the pixel L1distance between the student
and teacher images, LKDSis the adversarial distillation loss
used to train the student generator to approximate the distri-
bution of the teacher’s, and LGAN Sis the adversarial GAN
loss used to train the student generator to approximate the
distribution of the real data.
For the discriminator, the following objective was used:
LD=LKDD+4LGAN D (2)
whereLKDDis the adversarial distillation loss used to en-
courage the discriminator to distinguish between generated
images by the student and teacher networks and LGAN Dis
the adversarial GAN loss used to encourage discrimination
between student and real images.
3.2.3 Network Architectures
Generator Initially, the Gaussian random noise vector is
projected to 128 dimensions using a Fully Connected layer.
Subsequently, the condition embedding and a projected
noise vector are concatenated and passed through another
Fully Connected layer, followed by three consecutive Up-
sampling blocks. Each upsampling block consists of an up-
sample layer (scale factor=2, mode=’nearest’), a 3x3 con-
volution with padding, a Batch Normalization layer, and a
Gated Linear Unit (GLU). Finally, there is a convolutional
block consisting of a 3x3 convolution with padding and a
hyperbolic tangent activation function (tanh), which pro-
duces the fake image.Calibration DatasetModel
ResNet20 VGG16 MobileNetV2 ShufﬂeNetV2 RepVGG
CIFAR-10 0.28% 0.05% 0.07% 1.26% 41.94%
StyleGAN2-ADA 0.43% 0.16% 0.05% 7.48% 41.37%
DiStyleGAN 0.46% 0.07% 0.16% 1.49% 42.85%
Fractal 1.10% 0.69% 0.54% 87.21% 41.73%
(a) Default Quantization algorithm. Even if the best results are achieved with the original test set,
the accuracy degradation is minimal (less than 1.5%) for most cases using synthetic data.Calibration DatasetModel
ShufﬂeNetV2 RepVGG
CIFAR-10 1.26% 0.45%
StyleGAN2-ADA 0.59% 0.48%
DiStyleGAN 0.11% 0.55%
Fractal 87.21% 42.16%
(b) Accuracy-aware Quantization algorithm. The re-
sults from synthetic data (except fractal images) are
comparable to the real dataset.
Table 1. Accuracy drop for each model and calibration dataset, as measured in the classiﬁcation task on the CIFAR-10 test set.
Discriminator DiStyleGAN’s discriminator consists
of 4 consecutive Downsampling blocks (4x4 strided-
convolution, Spectral Normalization, and a LeakyReLU),
with each of them reducing the spatial size of the input im-
age by a factor of 2. These four blocks also produce the
feature maps that calculate the Feature Loss. Subsequently,
the logit is ﬂattened, projected to 128 dimensions, and con-
catenated with the class condition embedding before being
passed through a ﬁnal fully connected layer to produce the
class-conditional discriminator loss.
3.3. Calibration Datasets
The data used for the calibration of the quantization pro-
cess came from four different distributions. First, we con-
ducted quantization using a subset of the real CIFAR-10
data as a frame of reference. Second, we opted for syn-
thetic images produced by the StyleGAN2-ADA model, the
state-of-the-art model in class-conditional image generation
on CIFAR-10. Third, we produced synthetic images using
DiStyleGAN. Finally, fractal image data generated by Da-
tumaro [38] were also used. While the former two synthetic
datasets approximate the CIFAR-10 distribution, and thus
could be considered representative, the fractal images do
not constitute a representative dataset for the deep learn-
ing models pre-trained on CIFAR-10. However, we include
them in our experiments since Lazarevich et al. [25] demon-
strated that it is possible to perform post-training quantiza-
tion even with a non-representative dataset.
Samples from each of the four data distributions for the
classes ”Airplane“ and”Horse“ are illustrated in Figure 2,
while samples for the rest of the classes can be found in Ap-
pendix A. Additionally, Section 3.3 presents the Inception
Score (IS) [32] calculated for each data distribution. The
inception score is a metric that evaluates the quality of syn-
thetic generated images. The calculated score is based on
the ability of a pre-trained InceptionV3 [37] model to clas-
sify the images of a synthetic dataset produced by a gener-
ative model. In our case, we calculated the inception scores
using datasets of 50;000synthetic samples from each dis-
tribution.
For our quantization experiments, 5000 images wereModel Inception Score
StyleGAN2-ADA 10.34
DiStyleGAN 6.78
Fractal 3.31
Table 2. Inception score calculated for each synthetic dataset used
in the calibration process (higher is better).
used from each of the aforementioned datasets ( 500 im-
ages per class of the CIFAR-10 dataset). Then, the ofﬁ-
cial CIFAR-10 test set was utilized to evaluate the results
of the quantized models for all combinations of calibration
datasets, PyTorch models, and quantization techniques.
4. Experiments
4.1. Results
Tables 1a and 1b showcase the accuracy degradation per-
centage of the quantized models with respect to the perfor-
mance of the original PyTorch models on the classiﬁcation
task on the CIFAR-10 test set. These results were obtained
using the two different quantization algorithms, default and
accuracy-aware, and the four calibration datasets described
in Section 3.3. In addition, Section 3.3 presents the in-
ception scores of the synthetic calibration datasets, a quan-
titative metric that indicates the difference in the quality
of the generated images between the four aforementioned
datasets.
4.2. Discussion
Based on the results obtained by the experiments, it is
noticeable that in most cases, the Default Quantization algo-
rithm can successfully quantize the models while showing
only a negligible degradation in accuracy. This especially
holds true for the ResNet20, VGG, and MobileNetV2 net-
works. Even with a non-representative calibration dataset
(i.e. Fractal), the accuracy decrease is limited to a maxi-
mum of 1.1%, as showcased in Table 1a, which is in par
with the ﬁndings of Lazarevich et al. [25].
When the default quantization algorithm fails, theFigure 2. Samples from each of the four calibration datasets for the CIFAR-10 classes ”Airplane” and ”Horse”.
accuracy-aware algorithm can be employed. The RepVGG
model suffers the most by the default algorithm, with more
than a 40% accuracy decrease across all calibration datasets.
This could be potentially attributed to the high variance of
RepVGG’s activation weights, constituting the weight dis-
tribution quite different across different channels. However,
further experimentation is required to validate this claim.
On the other hand, the results of the accuracy-aware quan-
tization shown in Table 1b prove that the same model can
be quantized using synthetic data with minimal accuracy
degradation (0.5%). The same, but less signiﬁcant, ef-
fect can be observed for the ShufﬂeNetV2 model. It should
also be noted that the accuracy-aware quantization requires
a representative calibration dataset; thus the corresponding
results for the fractal images were expected.
Finally, it is essential to notice that the two synthetic
datasets, StyleGAN2-ADA and DiStyleGAN, lead to com-
parable results with the ofﬁcial CIFAR-10 dataset in the
case of quantization. Surprisingly enough, although the
DiStyleGAN model was trained through knowledge distil-
lation, with the StyleGAN2-ADA as the teacher network,
there are cases where the quantization process using the
dataset generated by the former leads to better results com-
pared to when the latter is used. This ﬁnding is also in con-
trast with the quantitative results for the synthetic generated
images (Sec. 3.3) which clearly showcase the superiority of
the StyleGAN2-ADA model compared to our DiStyleGAN,
in terms of the quality of their synthetic generated images.However, the inception score metric does not take into con-
sideration how synthetic images compare to real images.
Other metrics ( e.g.Fr´echet inception distance (FID) [19])
that evaluate the synthetic images while taking into account
the distribution of the real data, should also be included in
further experimentation.
5. Conclusion
Based on our experiments, post-training quantization
leads to minimal accuracy degradation for three of the se-
lected models (ResNet20, VGG16, MobileNetV2), regard-
less of the used calibration dataset. For the remaining
models (ShufﬂeNetV2, RepVGG), while an accuracy-aware
quantization is required, the results indicate similar perfor-
mance when the real data are used for calibration compared
to the synthetic. Insigniﬁcant differences were also ob-
served between the quantized models, when synthetic data
produced by the complex StyleGAN model were used, com-
pared to when they were produced by DiStyleGAN. This
ﬁnding suggests that in the case of quantization when the
synthetic data of the calibration dataset approximate the dis-
tribution of the real data, even a simple generator might be
enough. However, further experiments with synthetic cali-
bration datasets of varied quality are required to corroborate
this ﬁnding.References
[1] Jimmy Ba and Rich Caruana. Do deep nets really need to be
deep? Advances in neural information processing systems ,
27, 2014. 1
[2] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
Scalable methods for 8-bit training of neural networks. Ad-
vances in neural information processing systems , 31, 2018.
2
[3] Ron Banner, Yury Nahshan, and Daniel Soudry. Post train-
ing 4-bit quantization of convolutional networks for rapid-
deployment. Advances in Neural Information Processing
Systems , 32, 2019. 2
[4] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,
Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel
zero shot quantization framework. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13169–13178, 2020. 2
[5] Ting-Yun Chang and Chi-Jen Lu. Tinygan: Distilling big-
gan for conditional image generation. In Proceedings of the
Asian Conference on Computer Vision , 2020. 3
[6] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Wein-
berger, and Yixin Chen. Compressing neural networks with
the hashing trick. In International conference on machine
learning , pages 2285–2294. PMLR, 2015. 2
[7] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.
Low-bit quantization of neural networks for efﬁcient infer-
ence. In 2019 IEEE/CVF International Conference on Com-
puter Vision Workshop (ICCVW) , pages 3009–3018. IEEE,
2019. 2
[8] Intel Corporation. Post-training quantization with pot.
https : / / docs . openvino . ai / latest / pot _
introduction.html , 2018. Accessed: 18-01-2023. 2
[9] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre
David. Binaryconnect: Training deep neural networks with
binary weights during propagations. Advances in neural in-
formation processing systems , 28, 2015. 2
[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,
Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style
convnets great again. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
13733–13742, 2021. 2
[11] Alexander Finkelstein, Uri Almog, and Mark Grobman.
Fighting quantization bias with bias. arXiv preprint
arXiv:1906.03193 , 2019. 2
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Commu-
nications of the ACM , 63(11):139–144, 2020. 2
[13] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and
Pritish Narayanan. Deep learning with limited numerical
precision. In International conference on machine learning ,
pages 1737–1746. PMLR, 2015. 2
[14] Song Han, Huizi Mao, and William J. Dally. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding, 2015. 1, 2
[15] Song Han, Jeff Pool, John Tran, and William Dally. Learn-
ing both weights and connections for efﬁcient neural net-work. Advances in neural information processing systems ,
28, 2015. 1
[16] Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal
brain surgeon and general network pruning. In IEEE interna-
tional conference on neural networks , pages 293–299. IEEE,
1993. 2
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 2
[18] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning
for accelerating very deep neural networks. In Proceedings
of the IEEE international conference on computer vision ,
pages 1389–1397, 2017. 1
[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 5
[20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2(7), 2015. 1
[21] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-
Yaniv, and Yoshua Bengio. Binarized neural networks. Ad-
vances in neural information processing systems , 29, 2016.
2
[22] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,
Jaakko Lehtinen, and Timo Aila. Training generative adver-
sarial networks with limited data. Advances in neural infor-
mation processing systems , 33:12104–12114, 2020. 2
[23] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In Proc. CVPR , 2020. 2
[24] Alex Krizhevsky et al. Learning multiple layers of features
from tiny images. Technical report , 2009. 2
[25] Ivan Lazarevich, Alexander Kozlov, and Nikita Malinin.
Post-training deep neural network pruning via layer-wise
calibration. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 798–805, 2021. 2, 4
[26] Yann LeCun, John Denker, and Sara Solla. Optimal brain
damage. Advances in neural information processing systems ,
2, 1989. 2
[27] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv
preprint arXiv:1608.08710 , 2016. 2
[28] Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.
Fixed point quantization of deep convolutional networks. In
International conference on machine learning , pages 2849–
2858. PMLR, 2016. 2
[29] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet v2: Practical guidelines for efﬁcient cnn architec-
ture design. In Proceedings of the European conference on
computer vision (ECCV) , pages 116–131, 2018. 2
[30] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Chris-
tos Louizos, and Tijmen Blankevoort. Up or down? adap-
tive rounding for post-training quantization. In International
Conference on Machine Learning , pages 7197–7206. PMLR,
2020. 2[31] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and
Max Welling. Data-free quantization through weight equal-
ization and bias correction. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 1325–
1334, 2019. 2
[32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in neural information processing
systems , 29, 2016. 4
[33] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4510–4520, 2018. 2
[34] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 , 2014. 2
[35] Suraj Srinivas and R Venkatesh Babu. Data-free param-
eter pruning for deep neural networks. arXiv preprint
arXiv:1507.06149 , 2015. 1
[36] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1–9, 2015.
2
[37] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the in-
ception architecture for computer vision, 2015. 4
[38] OpenVINO toolkit. Dataset management frame-
work (datumaro), 2020. https://github.com/
openvinotoolkit/datumaro ; Accessed: 12/02/2023.
4
[39] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang
Hua. Lq-nets: Learned quantization for highly accurate and
compact deep neural networks. In Proceedings of the Eu-
ropean conference on computer vision (ECCV) , pages 365–
382, 2018. 2
[40] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and
Zhiru Zhang. Improving neural network quantization with-
out retraining using outlier channel splitting. In International
conference on machine learning , pages 7543–7552. PMLR,
2019. 2A. Calibration Datasets Samples
Figure A.1. Samples from each of the four calibration datasets for the CIFAR-10 classes: ”Automobile”, ”Bird”, ”Cat”, and ”Deer“.Figure A.2. Samples from each of the four calibration datasets for the CIFAR-10 classes: ”Dog”, ”Frog”, ”Ship”, and ”Truck”.