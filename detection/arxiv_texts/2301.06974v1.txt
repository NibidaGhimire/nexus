Towards Improving the Explainability of
Text-based Information Retrieval with Knowledge Graphs
Boqi Chenâˆ—
McGill University
Montreal, Quebec, Canada
boqi.chen@mail.mcgill.caKua Chenâˆ—
McGill University
Montreal, Quebec, Canada
kua.chen@mail.mcgill.caYujing Yangâˆ—
McGill University
Montreal, Quebec, Canada
yujing.yang2@mail.mcgill.ca
Afshin Amini
Aggregate Intellect
Toronto, Ontario, Canada
afshin.emd@gmail.comBharat Saxena
BMC Software Inc
Pune, MH, India
bharat_saxena@bmc.comCecilia ChÃ¡vez-GarcÃ­a
Aggregate Intellect
London, Ontario, Canada
cecilia.uku@gmail.com
Majid Babaei
McGill University
Montreal, Quebec, Canada
majid.babaei@mcgill.caAmir Feizpour
Aggregate Intellect
Montreal, Quebec, Canada
amir.fzpr@gmail.comDÃ¡niel VarrÃ³
McGill University
Montreal, Quebec, Canada
daniel.varro@mcgil.ca
ABSTRACT
Thanks to recent advancements in machine learning, vector-based
methods have been adopted in many modern information retrieval
(IR) systems. While showing promising retrieval performance, these
approaches typically fail to explain why a particular document is re-
trieved as a query result to address explainable information retrieval
(XIR). Knowledge graphs record structured information about en-
tities and inherently explainable relationships. Most of existing
XIR approaches focus exclusively on the retrieval model with little
consideration on using existing knowledge graphs for providing an
explanation. In this paper, we propose a general architecture to in-
corporate knowledge graphs for XIR in various steps of the retrieval
process. Furthermore, we create two instances of the architecture
for different types of explanation. We evaluate our approaches on
well-known IR benchmarks using standard metrics and compare
them with vector-based methods as baselines.
CCS CONCEPTS
â€¢Information systems â†’Information retrieval ;â€¢Comput-
ing methodologies â†’Knowledge representation and reason-
ing.
KEYWORDS
explainable information retrieval, knowledge graphs, entity linking,
natural language processing
âˆ—All three authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
TrustLOG-CIKM â€™22, October 21, 2022, Atlanta, GA, USA
Â©2022 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnnACM Reference Format:
Boqi Chen, Kua Chen, Yujing Yang, Afshin Amini, Bharat Saxena, Cecilia
ChÃ¡vez-GarcÃ­a, Majid Babaei, Amir Feizpour, and DÃ¡niel VarrÃ³. 2022. To-
wards Improving the Explainability of Text-based Information Retrieval
with Knowledge Graphs. In The First Workshop on Trustworthy Learn-
ing on Graphs in Conjunction with the 31st ACM International Confer-
ence on Information and Knowledge Management (TrustLOG-CIKM â€™22),
October 21, 2022, Atlanta, GA, USA. ACM, New York, NY, USA, 7 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Motivation. Given a query in natural language as input, text-
based information retrieval (IR) is a task of accessing the most
relevant documents. Modern IR systems usually exploit text embed-
dings, which map text into a vector such that the distance between
the vectors represents the similarity of the text.
In real-world IR use cases, a user can make better decisions
if additional explanation is provided to the retrieved results. For
example, when a user queries what contributes to heart disease, an
explanation stating the reason help the user decide the relevance of
a result. This is local explanation which addresses why a document
ğ‘‘is relevant to the user query ğ‘. In general, an local explanation
should be (1) interpretable i.e. it should be understandable by a
human and (2) accurate i.e. it should accurately answer the question.
Common explanations may highlight the most relevant parts
of a document [ 5] or provide feature importance score [ 25]. Re-
cent approaches also use explainable features to re-rank candidates
generated by vector-based IR systems [ 23]. However, few existing
approaches explicitly incorporate the structured information from
the text, which may serve as an important source for explanations.
Problem statement. In this paper, we focus on the task of ex-
plainable information retrieval (XIR), which aims to increase the
explainability of retrieval algorithms, i.e., make the result more
understandable and reflect the internal behavior of the algorithm.
We aim to investigate how knowledge graphs as external sources
with explicit semantic relationships can be utilized to improve XIR.arXiv:2301.06974v1  [cs.IR]  17 Jan 2023TrustLOG-CIKM â€™22, October 21, 2022, Atlanta, GA, USA B. Chen, K. Chen, Y. Yang, A. Amini, B. Saxena, C. ChÃ¡vez-GarcÃ­a, M. Babaei, A. Feizpour and D. VarrÃ³
User QueryDocuments
(1) Query  
PreprocessingKnowledge
GraphRanked  
DocumentsDocument
Index
Explanations Matching
(4)Explanation
Generation(0) Indexing
(2) Candidate  
Generation(3) Re-rankingLegend
used for explainability
can be improved for explainability
Figure 1: General architecture for explanation with KG
A knowledge graph (KG) is a directed labeled graph where nodes
represent entities, and edges capture relationships between nodes.
KGs have been shown to be effective for improving the performance
of IR systems [ 27] and generating explanation for other machine
learning algorithms [ 35]. However, the use of KG for explainable
IR is still restricted to domain-specific use cases [1, 47].
KGs provide explicit semantic relations between entities, which
can serve as explanation, or help clarify the intent of the query.
Normally, such semantic relations also appear in the text. For exam-
ple, the sentence "obesity contributes to heart disease" reflects two
entities obesity andheart disease related by the contribute relation.
Linking entities and relationships, i.e. identifying and connecting
the matching parts of a KG and a piece of text, is a well-known
challenge [ 8,21]. While existing entity linking tools can be used as
a starting point, improving the explainability of IR by using KGs
also necessitates (1) a general integrated architecture, and (2) a
detailed recipe on how to use KG information to improve specific
types of explanations.
Objective and contribution. Given a set of natural language doc-
uments and a query, our paper proposes a general architecture to
integrate KG into vector-based IR systems to generate local expla-
nation for the retrieved results. We use various entity/relationship
linking methods to match the most relevant parts of the KG to the
text and then use this information to improve the explainability of
the IR system. In summary, the specific contributions are:
â€¢We propose a general architecture that divides IR into dif-
ferent phases and highlights how KG can be exploited for
better explanation in each phase.
â€¢We instantiate the general architecture in two ways for ex-
plainable IR by integrating an open knowledge graph Wiki-
data and various entity linking approaches.
â€¢We conduct initial experimental evaluations on the two spe-
cific explanation approaches on two datasets: WIKIQA [ 48]
and Robust04 [ 19]. Assuming an ideal entity matcher, our
technique improves the performance of identifying the most
important sentence by 6.96% and the base retrieval perfor-
mance by 0.45%.
2 APPROACH
2.1 General Architecture
Figure 1 shows the general architecture of our explainable IR system
enhanced with KGs. In a preprocessing phase, (0) documents are
stored and arranged into the document index (assumed to store
documents in a vectorized form). Then (1) our framework takes auser query in natural language as input, and preprocesses the query
(e.g. by query expansion [ 3]). Next (2) candidate generation retrieves
a list of ranked candidate documents which are most relevant to
the processed query. (3) This list of candidate documents may be
rearranged by re-ranking . Finally, (4) our framework outputs ranked
documents together with the output from explanation generation .
Our architecture incorporates how extra knowledge from KG
can be exploited using entity/relationship matching in each step
(1)-(4) of IR to gradually improve explainability.
â€¢Entity/relation matching: Bridging text with correspond-
ing entities in a knowledge base is carried out by entity/re-
lation matching [ 31]. This step first uses entity recognition
to detect keywords and entities in the query, such as proper
names [ 18]. These entities are matched to nodes in the KG.
Then relation extraction is applied to identify the relations
between instances and concepts of corpus data, which trans-
forms identified entities into structured data [15].
â€¢Query preprocessing for explainability: The user query
can be expanded with information that is retrieved from
the KG. A KG normally contains both entity information
such as description, alias and properties, as well as relation
information such as connected entities and relation types.
Such extra knowledge can be added to the query to improve
query understanding.
â€¢Explainable candidate generation: Explainable candi-
date generation may use a KG constructed directly from
documents themselves. This KG is built from entities and
relations identified from the documents where each entity
may be related to one or more documents. Then, the candi-
date generation becomes a KG retrieval problem and it can
be performed using exact or fuzzy entity matching.
â€¢Explainable re-ranking: The KG can also be used to cal-
culate explainable features for re-ranking. For example, both
the query and candidate document can be first translated into
a set of entities. Then pair-to-pair relatedness of the entities
can be calculated using their distance and connectivity on
the KG. Such score can be aggregated into query-document
relatedness to be used for re-ranking.
â€¢Explanation generation: Information from steps (1)-(3)
can be used to generate enhanced explanations to the user.
The expanded query from (1) can be used to find the most rel-
evant part of a retrieved document and explain why a query
is related to a document. KG used in (2) can provide the
number of matching entities between the query and docu-
ments. Finally, features calculated in (3) provide intrinsically
explainable ranking of the documents.
2.2 KG for Most Important Sentence
The most important sentence (MIS) is the most relevant part of a
passage and explains how the passage is related to the query. The
MIS attracts the userâ€™s attention and emphasizes the main content
that relates to the query so that the user can understand why the
passage is retrieved without reading the whole passage. Formally,
letğ‘be the query and ğ‘†be a set of sentences from the document,
andğ‘ ğ‘–ğ‘š(Â·,Â·)be some similarity function between two pieces of text.Towards Improving the Explainability of Text-based Information Retrieval with Knowledge Graphs TrustLOG-CIKM â€™22, October 21, 2022, Atlanta, GA, USA
Figure 2: Example of MIS
The most important sentence is defined as
ğ‘€ğ¼ğ‘†=arg max
ğ‘ âˆˆğ‘†ğ‘ ğ‘–ğ‘š(ğ‘,ğ‘ ) (1)
A method widely used to measure similarity between two pieces
of text is embedding. We adapt this method in our approach by
embedding the text into a vector. Then a standard similarity metric
between two vectors such as cosine similarity or dot product can
be used to approximate the similarity function ğ‘ ğ‘–ğ‘šin Equation 1.
We chose cosine similarity in our approach.
Example 1. Figure 2 shows an example of highlighting the MIS
in a paragraph. The paragraph is about a tablespoon, and the MIS
contains the definition and capacity of a tablespoon.
Naively, one can use a sentence retriever that combine the query
embedding and the embedding of each sentence in the passage
to identify the MIS with cosine distance [ 33]. Yet, this approach
normally misses some semantic information about the query entity.
For example, the word â€™bankâ€™ may mean a financial institute as well
as construction on the river. To avoid semantic ambiguity in the
query, we expand the query using entity linking and relationship
extraction with a reference KG.
We extend query preprocessing to better identify MIS while keep-
ing other parts of the architecture unchanged. Specifically, we use
extra information of the linked entity from the KG to perform query
expansion in three different ways.
(A)If the query contains an entity with a direct relationship, the
linked entity is considered as potential information, and it is
appended to the query (see Figure 3).
(B)If the query contains only one entity, then the description of
the entity is used for query expansion.
(C)If the query contains more than one entity, but it does not
contain any relationship, then only entities are used for query
expansion.
In case (A), one can navigate the KG to find relevant information
on the KG for the query. However, many queries do not contain
any relations for KG navigation. To still benefit from KG in such
cases, we use entities information on the KG for case (B) and (C).
The expanded query is then (1) used in candidate generation
to rank the candidates and (2) used in explanation generation to
highlight the MIS.
Has contributing factor  
P1479Heart disease
Q190805Obesity  
Q12174  
Atherosclerosis  
Q12252357Figure 3: Example of entities and relations
Example 2. Figure 3 exemplifies entity linking and relationship
extraction used to identify the MIS. The query is cause of heart dis-
ease, and the extracted entity and relationship are heart disease and
hascontributing factor . So the linked (unique) entities are obesity
and atherosclerosis which are thus used for query expansion.
2.3 KG for Explainable Re-ranking
User studies has shown that re-ranking based on explainable features
can improve the interpretability of the ranking result [ 23] since
users can get insights into the passage by observing only the feature
values. Examples of such explainable features include term statistics,
contextual words, and citation-based popularity [23].
Our framework incorporates the idea of re-ranking documents
along explainable features to better explain the ranking of a docu-
ment. We propose a new explainable feature used for re-ranking
candidate documents based on the KG with entity matching.
Entity matching finds how two text passages are related to each
other by (1) extracting entities from two passages [ 37], and (2) calcu-
lating the pair-to-pair relatedness between two types of entities [ 8].
The authors of [ 8] present a measure of relatedness ğ‘…based on the
overlap of the incoming links between two entities on graphs such
as Wikipedia KG. Given a pair of phrase, the algorithm will find
the corresponding entities on the KG ğ‘ƒğ‘andğ‘ƒğ‘and then compute
the relatedness between ğ‘ƒğ‘andğ‘ƒğ‘as suggested in [8] as follow:
ğ‘…(ğ‘ƒğ‘,ğ‘ƒğ‘)=ğ‘™ğ‘œğ‘”(ğ‘šğ‘ğ‘¥(|ğ‘–ğ‘›(ğ‘ƒğ‘),ğ‘–ğ‘›(ğ‘ƒğ‘)|))âˆ’ğ‘™ğ‘œğ‘”(|ğ‘–ğ‘›(ğ‘ƒğ‘)âˆ©ğ‘–ğ‘›(ğ‘ƒğ‘)|)
ğ‘™ğ‘œğ‘”(ğ‘Š)âˆ’ğ‘™ğ‘œğ‘”(ğ‘šğ‘–ğ‘›(|ğ‘–ğ‘›(ğ‘ƒğ‘),ğ‘–ğ‘›(ğ‘ƒğ‘)|))
(2)
whereğ‘–ğ‘›(ğ‘ƒğ‘)is the set of incoming edges to entity ğ‘ƒğ‘and W is the
total number of nodes in the KG.
To get an entity relatedness score between the query and a
document, we average the relatedness values along document enti-
ties and then sum up along query entities. Specifically, the query-
document relatedness score is defined as follows:
ğ‘„ğ·ğ‘… =ğ‘–=ğ‘›âˆ‘ï¸
ğ‘–=0Ãğ‘—=ğ‘š
ğ‘—=0ğ‘…(ğ¸ğ‘ğ‘–,ğ¸ğ‘‘ğ‘—)
ğ‘š(3)
whereğ‘…is the relatedness score function [ 8] between two entities,
ğ¸ğ‘is a set ofğ‘›entities from the query, and ğ¸ğ‘‘is a set ofğ‘šentities
from the document. This value could be used as an explainable
feature to re-rank documents proposed by candidate generation .
Though this approach ignores structure from the sentence such
as the order of entities. the computation can be done efficiently
in parallel for each individual entity. We leave exploring structure
information of the sentence as future work.
3 EVALUATION
In this section, we investigate the following research questions:TrustLOG-CIKM â€™22, October 21, 2022, Atlanta, GA, USA B. Chen, K. Chen, Y. Yang, A. Amini, B. Saxena, C. ChÃ¡vez-GarcÃ­a, M. Babaei, A. Feizpour and D. VarrÃ³
â€¢RQ1: How can a KG influence the performance of highlight-
ing the most important sentence?
â€¢RQ2: How can a KG affect the performance of retrieval by
re-ranking with explainable features?
3.1 Experiment Setup
Reference KG. Wikidata [ 40] is a open knowledge base containing
structured data with 99,959,412 data items from multiple domains.
We select Wikidata as the reference KG as it is often used for tasks
such as link prediction [ 10], knowledge graph retrieval [ 41] and
recommendation [ 2]. In the experiment, we first use the entity
linker to identify entities and relations from the query and then
navigate the KG from the matched entities and relations (using the
qwikidata package [ 34]). At the same time, we also used Wikipedia
page graph [42] to identify the relatedness of two entities.
Evaluation datasets. To evaluate the performance of identifying
the MIS , we select WIKIQA [ 48], which is a popular multi-domain
question-answering dataset and benchmark used by the IR com-
munity. Unlike most other IR datasets, sentences in this dataset
are labeled as answers to their related questions. We choose this
dataset for two main reasons: (1) high quality answers are selected
from the Wikipedia page summary, and (2) the sentences that an-
swer the question are labeled for evaluating the performance of
sentence-level retrieval.
To evaluate the performance of re-ranking documents with ex-
plainable metrics , we use the test set from TREC 2004 Robust (Ro-
bust04) [ 19], which is popular benchmark to evaluate the perfor-
mance of text-based IR. Robust04 contains 249 queries and 528,000
corpora from the news. Each query is associated with multiple
relevant documents for document ranking evaluation. Moreover,
this dataset assigns various levels of document relevance to a query,
which makes it suitable to assess the ranking performance.
Compared approaches. To demonstrate the benefit of extra knowl-
edge from KG, we rely on a pre-trained embedding retriever with
a pre-trained sentence transformer model [ 9,26] for all compared
approaches. We use Haystack [ 22] to build the IR pipeline for all
compared approaches and use FAISS [ 14] to perform quick vector
ranking based on cosine similarity.
In the case of identifying MIS , we first split the retrieved docu-
ments into individual sentences. A retriever with the pre-trained
embedding model will rank the sentences using cosine similarity
with the query. Finally, the sentence with the highest similarity
score will be treated as the MIS (see Equation 1), which will be com-
pared with the ground-truth sentence to check the performance.
Unlike the baseline (with no query expansion), our approach ex-
pands the query with information from the KG.
In case of explainable re-ranking , the baseline approach also apply
the embedding retrievers to rank documents with cosine similarity.
The baseline directly uses the ranking from the embedding retriever
and do not re-rank the documents. In our approach, we use the
same retriever to generate the candidates but re-rank them using
query-document relatedness from Section 2.3.
Evaluation metrics. To evaluate the performance of the ap-
proaches, we use several metrics widely adapted by the IR commu-
nity. Specifically, Accuracy is used to evaluate the performance ofPassage Retriever Sentence Retriever
baseline 97.53 66.67
ours 1 97.12 62.71
ours 2 97.94 71.31
Table 1: Accuracy for sentence retrieval (in %)
identifying MIS. To assess the performance of re-ranking, we use
Precision ( P) and Recall to measure the quality of retrieved docu-
ments. However, since these metrics do not consider the ranking,
we use Mean Average Precision at k ( MAP@k ) and Normalized Dis-
counted Cumulative Gain at k ( NDCG@k ) to evaluate the quality
of ranking for the top kdocuments.
3.2 RQ1: MIS as Explanation
Rationale. In this RQ, we aim to evaluate the influence of ex-
tra knowledge incorporated into the query from the KG on the
performance of highlighting the MIS.
Setup. We use the query from the WIKIQA dataset and the Wiki-
data knowledge base for query expansion. In the baseline approach,
we use the original query to retrieve the most relevant passage and
then retrieve the MIS within the passage. Our approach performs
the same retrieval process using the expanded query. For the evalu-
ation, we calculate the accuracy of retrieving the top MIS for both
approaches. We measure the performance for identifying MIS using
the test set with 243 queries and 1081 documents.
We measure the performance for two variations of our approach
in query expansion: (1) using an existing entity matching tool ( ours
1) and (2) using an ideal entity matching approach ( ours 2 ). We use
the same reference KG in both cases. In ours 1 , we used WAT [ 21]
to identify entities within the query and link to Wikidata items.
However, this automated process can be erroneous and may not
reflect the benefit of extra information from the KG. To illustrate
the potential benefit of KGs, in ours 2 , we manually reviewed the
results of WAT, identified entities with relationships, and linked
them to the correct entities. In this approach, we assume an ideal
performance of the entity matcher for query expansion and test
the performance of sentence retrieval.
Discussion. In Table 1, the passage retriever column illustrates
the accuracy of the original IR task of finding the most relevant
document to the query. The sentence retriever shows the accuracy
of identifying MIS.
The accuracy of the passage retriever is high for all three ap-
proaches (more than 98.00%). The accuracy of ours 1 is slightly
lower than the baseline approach by 0.77%, and the accuracy of
ours 2 is higher than the baseline approach by 0.24%. The perfor-
mance for the sentence retriever shows a similar trend. While the
accuracy of ours 1 is lower than the baseline by 1.79%, the result
ofours 1 outperforms the baseline by 7.29%.
After an in-depth investigation of query expansion, we found
that the external entity matching tool used in ours 1 was not accu-
rate for some queries. In this case, the query is expanded with mis-
leading information and does not select the best MIS, which causes
performance decrease. However, with an ideal entity matcher ( ours
2), the performance of identifying MIS is improved significantly.Towards Improving the Explainability of Text-based Information Retrieval with Knowledge Graphs TrustLOG-CIKM â€™22, October 21, 2022, Atlanta, GA, USA
P Recall MAP@20 NDCG@20
baseline 31.09 15.71 10.38 20.87
ours 31.09 15.71 8.13 17.89
Table 2: Evaluation result for re-ranking (in %)
RQ1:We find that the performance gain for highlighting the MIS
is dependent on the performance of the entity matching and the
relevance of the KG. Using an ideal entity matcher, the additional
information from the KG significantly improves the accuracy of
sentence retrieval without modifying the retrieval model.
3.3 RQ2: Explainable Re-ranking
Rationale. Entity matching and query-document relatedness are
used as explainable features to re-rank documents, and the re-
ranking result is then compared with the baseline approach. While
these features can improve the explainabilty of the ranking, we aim
to evaluate their influence on the ranking performance.
Setup. The Robust04 dataset was used in this part of the eval-
uation. In the baseline approach, an embedding retriever is used
to retrieve and rank documents based on cosine similarity with
original queries and documents from the dataset. Our approach
(ours ) retrieves documents with the same embedding retriever, but
the documents are ranked based on the query-document related-
ness score (see Equation 3). Specifically, it uses REL [ 37] for entity
linking and TagMe [ 8] for calculating entity relatedness scores and
combines these two techniques to compute query-document entity
relatedness scores for ranking documents. The quality of the ranked
documents is measured by the TERC evaluation tool [36].
Discussion. As shown in the Table 2, both methods have the
same PandRecall because re-ranking does not exclude existing
candidates from candidate generation. Moreover, these two metrics
do not measure the ranking performance. While our approach
improves explainability of results (thanks to the metrics derived
from the KG), it achieves 8.13% MAP@20 and 17.89% NDCG@20
which are slightly lower than the ones of the baseline approach (by
2.25% and 2.98%).
To further investigate this drop in performance, we compared
the result ranking of the two approaches and found that only 17.95%
queries have better ranking results with the new approach while
ranking stays the same for the majority of the queries. After in-
vestigating entity linking for queries, we noticed similarly limited
performance of entity linking as in RQ1. This finding suggests that a
better entity linking may also potentially improve the performance
of the re-ranking performance.
RQ2:We find that while explainable features from the KG increase
the explainability of the ranking, they may decrease the retrieval
performance. We suspect this decrease is rooted in imperfections of
the entity linker. Additionally, our finding is in line with others [ 23]
that explainable re-ranking decreases the ranking performance.
3.4 Threats to Validity
Internal validity. The training of embedding models is normally
non-deterministic. In order to minimize the effect of randomness,we used a pre-trained embedding model [ 9,26] and kept it the same
throughout the experiment.
External validity. We used Wikidata as our reference KG through-
out the experiments. Obviously, one may get different improve-
ments by using KG from a different domain. In the experiment of
RQ1, we measured performance using an existing entity matchers
[21,37] and assuming an ideal entity matcher. The real-world per-
formance may be in between the two. Similarly, in the experiment
ofRQ2, we re-ranked the documents with entity relatedness using
the KG, and one may get a different result by incorporating different
explainable features using the KG.
Construct validity. We measured the performance of compared
approaches using popular IR benchmarks on classic IR metrics,
which follows the best practices in IR evaluations.
4 RELATED WORK
4.1 Explainable Artificial Intelligence
Generally, XAI approaches can be divided into two categories: ante-
hocmethods and post-hoc methods (by using an external explainer
on an already trained model) [ 11,39]. The latter can be further
divided into two categories, model-specific andmodel-agnostic .
Ante-hoc methods create transparent models so that the explana-
tion can be generated directly from the models, such as tree-based
models [ 13,29] or extending the architecture of neural networks
for explanation [ 6,28,30]. There are also many surveys on post-hoc
methods for explanation [11, 38, 50].
Model-specific methods are confined to particular models by
getting explanations using the internal model representation and
learning process, and can generate explanation directly from model
output. However, most of the methods only work with simple inter-
pretable models such as linear models, nearest neighbours and tree-
based models [ 11]. Instead, model-agnostic methods only analyze
input and output pairs to improve explainability without looking
into the model internals. One popular model-agnostic methods,
LIME [49] was proposed to explain the predictions of any classifier
in an interpretable and faithful manner. Another popular method,
SHAP [17] is built based on the Shapley regression values, which is
an approach to compute feature importance and allow fast approxi-
mations in situations where training models on all feature subsets
would be intractable. However, these methods suffer from problems
such as poor global explanation and correlated features [12].
In this paper, we proposed a general framework to generate
model-specific post-hoc explanation for embedding-based informa-
tion retrieval models.
4.2 Explainable Information Retrieval
Explainability in information retrieval can be categorized by the
form of the outcome: highlighting-based, feature importance based,
rules-based, and mixed approaches [11].
One famous example for highlighting-based explanation is
Google Search, where content snippets and keywords are shown
along with the search result. Similarly Chios and Verberne [ 5]
proposed highlighting most important snippet for deep IR mod-
els.Feature importance based explanation methods may calculate
scores from queries and documents [ 5,23,25]. Some approachesTrustLOG-CIKM â€™22, October 21, 2022, Atlanta, GA, USA B. Chen, K. Chen, Y. Yang, A. Amini, B. Saxena, C. ChÃ¡vez-GarcÃ­a, M. Babaei, A. Feizpour and D. VarrÃ³
operate on term importance from the query, while others focus on
re-ranking the documents with explainable features [ 23].Rule-based
explanation aims to answer specific properties about the internal
mechanism of the IR model [ 32] by providing a simplified model to
explain decisions. Finally, mixed explanation combines all methods
to provide comprehensive explainability [5, 23].
While explainable IR is mostly post-hoc, there also exist ap-
proaches using explainable embedding to rank documents [ 20,24]
such that the distance in the vector space explains the relatedness
of the query and document.
Compared with these approaches, our general framework gener-
ates mixed forms of explanations for IR systems with an external
knowledge graph. Furthermore, we propose a concrete instantiat-
ing of the framework to highlight the most important sentence and
explainable re-ranking.
4.3 Knowledge Graph for Information
Retrieval
Entities from a knowledge graph can be used within an IR system
in order to help understand of a userâ€™s intent, queries, and docu-
ments beyond what can be achieved through word tokens on their
own [ 27]. Reinanda et al. [ 27] summarised approaches on how to
leverage entity-oriented information in KG: expansion-based ,latent
factor modeling ,language modeling , and deep learning approaches .
At the same time, KG is also being used in some domain-specific
applications for explanation of IR systems.
Expansion-based approaches enrich entity-oriented information
in the retrieval process by expanding queries and/or documents [ 4],
for example, expanding the query with synonyms.
Latent factor modeling attempts to find concepts in queries and
documents. The authors of [ 44] presented a new technique for
improving ranking using external data and knowledge bases. This
technique treats the external objects as latent layer between query
and documents to learn judging document relevance.
Language modeling approaches consider semantic information
when building language models of queries and documents. For ex-
ample, Ensan et al. [ 7] proposed a document retriever which uses
semantic linking systems for forming a graph representation of
documents and queries, where nodes represent concepts from docu-
ments and edges represent semantic relatedness between concepts.
Deep learning approaches uses knowledge graph embeddings
in neural ranking systems. Examples are embedding queries and
documents in the entity space, [ 46] and constructing an interaction
matrix between queries and entity representations. [16, 45]
KG for XIR Hasan et al [ 1] proposed a framework for generating
explanation in domain-specific IR applications by incorporating
domain knowledge graphs. The author of [ 47] applies KG to explain
the IR model by building knowledge-aware paths with the help of
attention scores. Similarly, the author of [ 43] designs an explainable
recommendation system which contains explicit reasoning with
KG for decision making to make recommendations explainable.
In this work, we proposed a general framework for utilizing
knowledge graphs to improve the explainability in IR systems.5 CONCLUSIONS AND FUTURE WORK
In this paper, we investigate the task of XIR, targeting to explain
why a document is relevant to a query. While existing approaches
use highlighting and feature importance, less focus has been put
on using an external knowledge base to generate explanations.
We propose a general architecture that uses semantic informa-
tion from KGs to improve the explainability of IR systems. We take
advantage of existing entity and relationship matching methods to
identify the most relevant part of the KG to a passage and navigate
the KG to help explain the result of retrieval from each process in
IR. We demonstrate the effectiveness of KGs with two examples
IR: highlighting the most important sentence and re-ranking with
explainable features.
We carried out an initial experimental evaluation of our approach
using multiple metrics with the key finding that the performance
of our approach largely depends on the quality of entity matching.
With an erroneous entity linker, the performance can decrease
compared to the baselines. However, with an ideal entity matcher,
our technique improved performance of identifying MIS by 6.96%
and the base retrieval performance by 0.45%.
We believe this general architecture opens many directions for
using KG to improve the explainability of IR. In the future, we
aim to apply the general architecture to (1) combine the architec-
ture with a more advanced entity/relation matching methods, (2)
adapt various knowledge graph retrieval methods to improve the
explainability of candidate generation , (3) integrate KG with the
embedding model to create explainable text embeddings, (4) capture
structure information from documents and queries and (5) evaluate
how our approaches influence the explainability of IR system in
humanâ€™s perspective with user study. The code and artifacts for the
experiments can be found in1
6 ACKNOWLEDGEMENT
This paper is partially supported by the NSERC RGPIN-2022-04357
project and the FRQNT-B2X project (file number: 319955). This
work is done as part of the McGill Summer Undergraduate Research
in Engineering 2022 project (ECSE-025).
REFERENCES
[1]Hasan Abu-Rasheed, Christian Weber, Johannes Zenkert, Mareike DornhÃ¶fer,
and Madjid Fathi. 2022. Transferrable Framework Based on Knowledge Graphs
for Generating Explainable Results in Domain-Specific, Intelligent Information
Retrieval. In Informatics , Vol. 9. MDPI, 6.
[2]Kholoud AlGhamdi, Miaojing Shi, and Elena Simperl. 2021. Learning to rec-
ommend items to wikidata editors. In International Semantic Web Conference .
Springer, 163â€“181.
[3]Hiteshwar Kumar Azad and Akshay Deepak. 2019. Query expansion techniques
for information retrieval: a survey. Information Processing & Management 56, 5
(2019), 1698â€“1735.
[4]Saeid Balaneshinkordan and Alexander Kotov. 2016. An empirical comparison
of term association and knowledge graphs for query expansion. In European
conference on information retrieval . Springer, 761â€“767.
[5] Ioannis Chios and Suzan Verberne. 2021. Helping results assessment by adding
explainable elements to the deep relevance matching model. arXiv preprint
arXiv:2106.05147 (2021).
[6]Orsolya CsiszÃ¡r, GÃ¡bor CsiszÃ¡r, and JÃ³zsef Dombi. 2020. Interpretable neural
networks based on continuous-valued logic and multicriteria decision operators.
Knowledge-Based Systems 199 (2020), 105972.
[7]Faezeh Ensan and Ebrahim Bagheri. 2017. Document retrieval model through
semantic linking. In Proceedings of the tenth ACM international conference on web
search and data mining . 181â€“190.
1https://github.com/Aggregate-Intellect/xirTowards Improving the Explainability of Text-based Information Retrieval with Knowledge Graphs TrustLOG-CIKM â€™22, October 21, 2022, Atlanta, GA, USA
[8]Paolo Ferragina and Ugo Scaiella. 2012. Fast and Accurate Annotation of Short
Texts with Wikipedia Pages. IEEE Software 29, 1 (2012), 70â€“75.
[9]flax-sentence embeddings. 2021. all_datasets_v3_mpnet-base. https://
huggingface.co/flax-sentence-embeddings/all_datasets_v3_mpnet-base
[10] Mikhail Galkin, Max Berrendorf, and Charles Tapley Hoyt. 2022. An Open Chal-
lenge for Inductive Link Prediction on Knowledge Graphs. ArXiv abs/2203.01520
(2022).
[11] Mir Riyanul Islam, Mobyen Uddin Ahmed, Shaibal Barua, and Shahina Begum.
2022. A systematic review of explainable artificial intelligence in terms of different
application domains and tasks. Applied Sciences 12, 3 (2022), 1353.
[12] Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor, and Mohiuddin
Ahmed. 2021. Explainable artificial intelligence approaches: A survey. arXiv
preprint arXiv:2101.09429 (2021).
[13] Sarah Itani, Fabian Lecron, and Philippe Fortemps. 2020. A one-class classification
decision tree based on kernel density estimation. Applied Soft Computing 91
(2020), 106250.
[14] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-scale similarity
search with GPUs. IEEE Transactions on Big Data 7, 3 (2019), 535â€“547.
[15] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning
entity and relation embeddings for knowledge graph completion. In Twenty-ninth
AAAI Conference on Artificial Intelligence .
[16] Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2018. Entity-duet
neural ranking: Understanding the role of knowledge graph semantics in neural
information retrieval. arXiv preprint arXiv:1805.07591 (2018).
[17] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in neural information processing systems 30 (2017).
[18] Alireza Mansouri, Lilly Suriani Affendey, and Ali Mamat. 2008. Named entity
recognition approaches. International Journal of Computer Science and Network
Security 8, 2 (2008), 339â€“344.
[19] NIST Information Technology Laboratoryâ€™s (ITL) Retrieval Group of the In-
formation Access Division (IAD). 2022. TREC 2004 Robust Track Guidelines.
https://trec.nist.gov/data/robust/04.guidelines.html
[20] Abhishek Panigrahi, Harsha Vardhan Simhadri, and Chiranjib Bhattacharyya.
2019. Word2Sense: sparse interpretable word embeddings. In Proceedings of the
57th annual meeting of the Association for Computational Linguistics . 5692â€“5705.
[21] Francesco Piccinno and Paolo Ferragina. 2014. From TagME to WAT: A New
Entity Annotator. In Proceedings of the First International Workshop on Entity
Recognition and Disambiguation (Gold Coast, Queensland, Australia) (ERD â€™14) .
Association for Computing Machinery, New York, NY, USA, 55â€“62.
[22] Tanay; etc Pietsch, Malte; Soni. 2022. Haystack. https://github.com/deepset-
ai/haystack/
[23] Sayantan Polley, Atin Janki, Marcus Thiel, Juliane Hoebel-Mueller, and Andreas
Nuernberger. 2021. Exdocs: Evidence based explainable document search. In
ACM SIGIR Workshop on Causality in Search and Recommendation .
[24] M Atif Qureshi and Derek Greene. 2019. Eve: explainable vector based embedding
technique using wikipedia. Journal of Intelligent Information Systems 53, 1 (2019),
137â€“165.
[25] Jerome Ramos and Carsten Eickhoff. 2019. Explainability in Transparent Infor-
mation Retrieval Systems.
[26] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing . Association for Computational
Linguistics.
[27] Ridho Reinanda, Edgar Meij, Maarten de Rijke, et al .2020. Knowledge graphs:
An information retrieval perspective. Foundations and Trends Â®in Information
Retrieval 14, 4 (2020), 289â€“444.
[28] Isabel Rio-Torto, Kelwin Fernandes, and LuÃ­s F Teixeira. 2020. Understanding
the decisions of CNNs: An in-model approach. Pattern Recognition Letters 133
(2020), 373â€“380.
[29] Omer Sagi and Lior Rokach. 2020. Explainable decision forest: Transforming a
decision forest into an interpretable tree. Information Fusion 61 (2020), 124â€“138.
[30] Anirban Sarkar, Deepak Vijaykeerthy, Anindya Sarkar, and Vineeth N Balasub-
ramanian. 2022. A Framework for Learning Ante-hoc Explainable Models via
Concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition . 10286â€“10295.
[31] Wei Shen, Jianyong Wang, and Jiawei Han. 2014. Entity linking with a knowledge
base: Issues, techniques, and solutions. IEEE Transactions on Knowledge and Data
Engineering 27, 2 (2014), 443â€“460.
[32] Jaspreet Singh and Avishek Anand. 2019. Exs: Explainable search using local
model agnostic interpretability. In Proceedings of the Twelfth ACM International
Conference on Web Search and Data Mining . 770â€“773.
[33] Amit Singhal et al .2001. Modern information retrieval: A brief overview. IEEE
Data Eng. Bull. 24, 4 (2001), 35â€“43.
[34] Kensho Technologies. 2019. qwikidata. https://github.com/kensho-technologies/
qwikidata.
[35] Ilaria Tiddi and Stefan Schlobach. 2022. Knowledge graphs as tools for explainable
machine learning: A survey. Artificial Intelligence 302 (2022), 103627.[36] Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely
Fast Python Interface to trec_eval. In SIGIR . ACM.
[37] Johannes M. van Hulst, Faegheh Hasibi, Koen Dercksen, Krisztian Balog, and
Arjen P. de Vries. 2020. REL: An Entity Linker Standing on the Shoulders of
Giants. In Proceedings of the 43rd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR â€™20) . ACM.
[38] Sahil Verma, John Dickerson, and Keegan Hines. 2020. Counterfactual explana-
tions for machine learning: A review. arXiv preprint arXiv:2010.10596 (2020).
[39] Giulia Vilone and Luca Longo. 2020. Explainable Artificial Intelligence: a Sys-
tematic Review. https://arxiv.org/abs/2006.00093 (2020).
[40] Denny VrandeÄiÄ‡ and Markus KrÃ¶tzsch. 2022. Wikidata: Main Page. https:
//www.wikidata.org/wiki/Wikidata:Main_Page
[41] Andra Waagmeester, Gregory Stupp, Sebastian Burgstaller-Muehlbacher, Ben-
jamin M Good, Malachi Griffith, Obi L Griffith, Kristina Hanspers, Henning
Hermjakob, Toby S Hudson, Kevin Hybiske, et al .2020. Science Forum: Wikidata
as a knowledge graph for the life sciences. Elife 9 (2020), e52614.
[42] Wikipedia contributors. 2004. Plagiarism â€” Wikipedia, The Free Encyclopedia.
https://en.wikipedia.org/w/index.php?title=Plagiarism&oldid=5139350 [Online;
accessed 22-July-2004].
[43] Yikun Xian, Zuohui Fu, Shan Muthukrishnan, Gerard De Melo, and Yongfeng
Zhang. 2019. Reinforcement knowledge graph reasoning for explainable rec-
ommendation. In Proceedings of the 42nd international ACM SIGIR conference on
research and development in information retrieval . 285â€“294.
[44] Chenyan Xiong and Jamie Callan. 2015. Esdrank: Connecting query and doc-
uments through external semi-structured data. In Proceedings of the 24th ACM
international on conference on information and knowledge management . 951â€“960.
[45] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017. Word-entity duet represen-
tations for document ranking. In Proceedings of the 40th International ACM SIGIR
conference on research and development in information retrieval . 763â€“772.
[46] Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking
for academic search via knowledge graph embedding. In Proceedings of the 26th
international conference on world wide web . 1271â€“1279.
[47] Zuoxi Yang. 2020. Biomedical information retrieval incorporating knowledge
graph for explainable precision medicine. In Proceedings of the 43rd International
ACM SIGIR Conference on Research and Development in Information Retrieval .
2486â€“2486.
[48] Yang Yi, Yih Wen-tau, and Christopher Meek. 2015. WikiQA: A Challenge Dataset
for Open-Domain Question Answering. Association for Computational Linguistics ,
2013â€“2018.
[49] Yujia Zhang, Kuangyan Song, Yiming Sun, Sarah Tan, and Madeleine Udell. 2019.
" Why Should You Trust My Explanation?" Understanding Uncertainty in LIME
Explanations. arXiv preprint arXiv:1904.12991 (2019).
[50] Jianlong Zhou, Amir H Gandomi, Fang Chen, and Andreas Holzinger. 2021.
Evaluating the quality of machine learning explanations: A survey on methods
and metrics. Electronics 10, 5 (2021), 593.