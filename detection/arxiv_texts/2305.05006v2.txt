Synthesis of Annotated Colon Cancer Tissue Images from
Gland Layout
Srijay Deshpande, Fayyaz Minhas, and Nasir Rajpoot
Tissue Image Analytics Centre, University Of Warwick, Coventry, United Kingdom
1. ABSTRACT
Generating realistic tissue images with annotations is a challenging task that is important in many computational
histopathology applications. Synthetically generated images and annotations are valuable for training and
evaluating algorithms in this domain. To address this, we propose an interactive framework generating pairs
of realistic colorectal cancer histology images with corresponding glandular masks from glandular structure
layouts. The framework accurately captures vital features like stroma, goblet cells, and glandular lumen. Users
can control gland appearance by adjusting parameters such as the number of glands, their locations, and sizes.
The generated images exhibit good Frechet Inception Distance (FID) scores compared to the state-of-the-art
image-to-image translation model. Additionally, we demonstrate the utility of our synthetic annotations for
evaluating gland segmentation algorithms. Furthermore, we present a methodology for constructing glandular
masks using advanced deep generative models, such as latent diffusion models. These masks enable tissue image
generation through a residual encoder-decoder network.
Keywords: Computational Pathology, Generative Adversarial Networks, Diffusion Models, Deep Learning
2. INTRODUCTION
Deep learning algorithms necessitate large amounts of training data, which can be challenging and expensive to
obtain, and may require involvement from highly skilled pathologists. Numerous techniques have been proposed
for generating synthetic tissue images of high quality.1–4Synthetic histology images are useful for developing
predictive models for downstream tasks,3,5,6as well as for educational purposes7and clinical quality assurance.8
Most of the aforementioned use cases of synthetic histology image generation require image synthesis from custom
glandular layouts or user-defined tissue parameters, such as the number of glands and disease grade.
In pathology image analysis, due to tissue heterogeneity and variations in acquired tissue images in laboratories,
the data annotation phase must often be repeated for different tissue types, such as glands, fat tissues, and blood
vessels, to achieve optimal performance. For tasks like gland segmentation, manually generating component
masks that highlight glandular portions can be a laborious and time-consuming task. This presents a significant
obstacle to generating a large amount of annotated data for segmentation algorithms. Some researchers have
investigated generating synthetic pathology images from tissue component masks.5,9,10These methods either
assume that input component masks are already present or require the explicit construction of component masks
by generating random shapes of respective tissue components like nuclei, which can be erroneous and unrealistic.
Furthermore, crafting component masks for larger multi-cellular structures, such as glands, can be challenging.
Therefore, generating synthetic images along with component masks simultaneously is desirable, as it potentially
reduces the cost of annotations and also produces realistic annotated pairs.
In this work, we propose a user-interactive framework capable of generating colorectal tissue images along
with corresponding tissue component masks simultaneously, using the input gland layout. To the best of our
knowledge, it is the first framework that can generate annotated colorectal tissue images controlled from the
gland layout. This layout enables users to specify the locations and sizes of the glands in colorectal tissue images.
We harness the properties of Generative Adversarial Networks (GANs) within an adversarial setting to create
realistic tissue images. Additionally, we present a methodology based on latent diffusion models11for synthesizing
Further author information: (Send correspondence to Srijay Deshpande)
Srijay Deshpande: E-mail: deshpandesrijay@gmail.comarXiv:2305.05006v2  [eess.IV]  4 Apr 2024Figure 1. Block diagram of the proposed framework: (a) shows an input gland layout where glands are arranged on 2-d
spatial locations. Each gland is characterized by a gland specific vector which undergoes affine transformation to form
latent vectors (b). Each latent vector is consumed by the individual mask generator which outputs binary individual
glandular masks (c). The generated individual masks along with input bounding boxes act as an input to the bilinear
interpolation algorithm, which wraps generated masks inside bounding boxes creating the intermediate tensor (d). The
mask generator network consumes the intermediate tensor and generates the glandular mask (e), which is then passed
through the encoder-decoder generator network generating the final tissue image (f). There are three discriminators
employed for generated mask, image and glandular parts inside the image.
glandular masks without relying on specific glandular layouts or similar inputs. Cutting-edge approaches, such
as taming transformers,12OpenAI’s DALL-E,13and Stable Diffusion,11have also employed quantization-based
architectures to produce lifelike images. Key highlights of our work are:
1.We propose a framework that can generate realistic colorectal tissue images and corresponding tissue
component/gland masks simultaneously.
2.The proposed framework allows user to change the appearance of glands by their locations and sizes. To
the best of our knowledge, it is the first framework that can generate annotated colorectal tissue images
controlled from the gland layout.
3.We demonstrate the efficacy of the annotated pairs generated using the proposed framework for evaluation
of the gland segmentation algorithm.
4.We provide a vector quantized variational autoencoder (VQ-VAE) based methodology, supplemented by the
integration of the Diffusion model. This combination enables the generation of realistic glandular masks,
subsequently producing tissue images using the residual encoder decoder network.
3. METHOD
Our aim is to develop the framework to construct the colorectal cancer tissue image and its corresponding tissue
component mask highlighting glandular regions, from the input gland layout. The gland layout can be describedas a first quadrant Cartesian plane where users can arrange glands on its 2-d spatial locations. The gland layout
is consumed by the framework fthat constructs the tissue component mask first, which later assists to generate
the complete tissue image of size N×Npixels. The overview is given in Figure 1. Below we describe the main
components of the proposed framework:
The input gland layout can be assumed as a set of nglands, Glayout ={gk≡(− →lk,− →sk)|k= 1,2...n}, where− →lk
and− →skdenote the location and size vectors respectively of the gland gk. The gland gkis characterized by a gland
specific vector− →zksampled from the Gaussian noise z∼ N(0,1). The Gaussian noise is used to ensure the variable
appearance of glandular objects in the final image. The gland specific vector− →zkis passed through the affine
transformation A, generating nlatent embeddings {ak|k= 1,2...n}of dimensionality D, i.e., ak=A(− →zk;θA),
where θArepresents the function’s trainable weights.
3.1 Generation of Glandular Mask
Generated latent embeddings are then consumed by the individual mask generator network I, generating the
corresponding individual gland binary masks {mk|k= 1,2...n}, each of the size B×Bpixels i.e., mk=M(ak;θI),
where θIdenotes the trainable parameters of the network. The individual mask generator network is comprised
of series of blocks having transpose convolution layer followed by the ReLU activation.
Next step is to align generated binary masks on the appropriate locations and construct the tissue component
mask of same size as that of the final image, i.e. N×Npixels. For this purpose, we utilize the input bounding
boxes for each object, {bk|k= 1,2...n}. These bounding boxes are either obtained from the datasets (procedure
given in section 4) or realized from the input location and size parameters (section 5.6). Each gland specific vector− →zkis multiplied element-wise with the individual glandular mask mk, and wrapped to the positions of bounding
boxbkusing the fixed bilinear interpolation function14F, to give the intermediate tensor of dimensionality
D×B×B, i.e., C=F({− →zk, mk, bk|k= 1,2...n}). The intermediate tensor has Dchannels which are then
reduced to 1 channel using the mask generator network M, to give the glandular mask or tissue component mask,
i.e.T=M(C, θM), where θMdenotes its trainable parameters.
3.2 Tissue Image Generation
After generating the tissue component mask, we feed it to the encoder-decoder generator network E. The
image-to-image translation encoder-decoder network is used as an image generator, to construct the final tissue
image Z=E(T, θE).
The encoder consists of a series of (convolution + ReLU) blocks that generate a fixed size encoding of the
input image. The decoder constructs the final image using a series of (Transpose convolution + ReLU) blocks.
Taking inspiration from Pix2Pix15, the network Ealso adopted skip-connections between the layers with the
same sized feature maps so that the first downsampling layer is connected with the last upsampling layer, the
second downsampling layer is connected with the second last upsampling layer, and so on. These skip-connections
give image generator the flexibility to bypass the encoding part to subsequent layers and enable consideration of
low level features from earlier encoding blocks in the generator.
3.3 Discriminators
We employ 3 discriminator neural networks in an adversarial training setting: mask discriminator DM(M, θ MD)
for the generated glandular mask (M),image discriminator DZ(Z, θZD)for the generated tissue image Z, and the
gland discriminator DG(Zgi, θGD)for glandular portions {Zgi|i= 1,2..n}inside the tissue image, where nis the
number of glands; θMD,θZDandθGDdenotes the respective trainable parameters of those discriminators. The
first two discriminators employ PatchGAN15discriminator which predicts the realism of the different portions
from the generated component mask and the tissue image, respectively. The adversarial losses based on these
discriminators ensure tissue component masks and tissue images are realistic.
The architecture of the gland discriminator is comprised of a series of convolution operations and predicts a
single score of realism for the generated glandular portions cropped out from the final tissue image based on
input bounding boxes, and resized to a fixed size using bilinear interpolation.14It ensures the generated glands,
one by one, appear real with their micro components like goblet cells and lumen.We employ an adversarial loss function16for all discriminators. A discriminator Dt(X, θtD)attempts to
maximize the loss by classifying the input image Xgenerated by the generator function G(X, θG)which tries
to minimize it, where θtDandθGdenotes the set of trainable parameters of the respective networks. Dtis the
discriminator type (of DM,DZorDG). The adversarial min-max loss function is given by,
min
θGmax
θtDLt
GAN(θG, θtD) =EX∼pdata (X)[logD t(X, θtD)] +EX∼pX(X)[log(1−Dt(G(X, θG), θDt)](1)
The exact architectures of all of the above networks can be found in Appendix 5.
3.4 Loss Components
The training loss of the proposed framework is made up of several terms as it involves multiple networks. The
different loss components used in the framework are described below:
The complete framework with trainable parameters {θA, θI, θM, θE, θMD, θZD, θGD}is trained by minimizing
a loss function with the following components:
Individual Binary Glandular Masks Reconstruction Loss: This component penalize the difference between
ground truth {ˆmk|k= 1,2...n}and generated individual binary glandular masks {mk|k= 1,2...n}using the
mean square error (MSE) as follows,
LGlandMaskRec (θA, θI) =nX
k=1MSE ( ˆmk, mk) (2)
where ˆmkis the ground truth, mkis the generated individual glandular mask and nis the number of glands in
the tissue image. As we saw in section 3, mkis dependent on the trainable parameters θAandθI. Similarly, we
can define the other loss components.
Mask Reconstruction Loss: This loss is employed to penalize the difference between ground truth ˆTand
generated tissue component mask Twith mean square error (MSE) as below,
LTissueMaskRec (θA, θI, θM) =MSE (ˆT, T) (3)
where ˆTis ground truth and Tis the generated tissue component mask.
Image Reconstruction Loss: This component captures the reconstruction error between ground truth ˆZand
generated tissue image Zusing the L1difference,
LImageRec (θA, θI, θM, θE) =∥ˆZ−Z∥1 (4)
where ˆZis ground truth and Zis the generated tissue image.
Adversarial Loss Components : As we discussed in section 3.3, we employ 3 adversarial loss components:
LM
GAN,LZ
GANandLG
GANfor tissue component mask, tissue image and the glandular portions cropped out from
the tissue image, respectively. Their expressions can be realized by putting t=M,t=Zandt=Gin (1).
Thus, the overall learning problem can be cast as a the following adversarial optimization problem based on
the linear combination of adversarial and reconstruction losses,
(5)min
θA,θI,θM,θEmax
θMD,θZD,θGDλ1LImageRec (θA, θI, θM, θE) +λ2LTissueMaskRec (θA, θI, θM)
+λ3LGlandMaskRec (θA, θI) +λ4LM
GAN(θG={θA, θI, θM}, θtD=θMD)
+λ5LZ
GAN(θG={θA, θI, θM, θE}, θtD=θZD) +λ6LG
GAN(θG={θA, θI, θM, θE}, θtD=θGD)
where λ1, λ2..λ6denote the weights of corresponding loss components.3.5 Synthesis of Glandular Masks using Latent Diffusion Model
In this section, we discuss the latent diffusion model11based framework used for synthesis of glandular masks. The
process is done in two steps: (i) Training VQ-VAE17on glandular masks to learn the discrete latent representations,
and (ii) sampling new latent representations using the Diffusion model18conditioned on the cancer type (benign
or malignant). Below we describe both steps in detail.
3.5.1 Generation of Discrete Latent Representations using VQ-VAE
The VQ-VAE17model first passes the input glandular mask through an encoder neural network, which generates
an encoded latent representation of the mask. The encoded latent representation is then passed through a
quantization layer. This layer maps the continuous values of the latent representation to discrete values using a
predefined codebook of vectors. Each vector in the codebook represents a possible value of the latent representation.
The quantization layer assigns a vector from the codebook to each entity based on the Euclidean distance between
the entity and the vectors in the codebook. The vectors obtained after the quantization layer are called quantized
embeddings.
Finally, the quantized embeddings are passed through a decoder network, which reconstructs the glandular
mask. In order to sample out-of-dataset masks, we acquire knowledge about the underlying latent variables. By
learning a prior over these latents, we can significantly reduce the memory and computational resources required.
This enables us to generate high-resolution masks simply by adjusting the size of the latent space.
3.5.2 Sampling Glandular Masks using Diffusion Model
To generate masks from a learnt distribution of latent variables or compressed masks, we establish a prior
distribution over them. We achieve this using the Diffusion model.18Diffusion models work in two phases. In the
first phase known as forward diffusion, they use diffusion processes to transform real images into noisy images.
These noisy images generally follow a Gaussian distribution. The second phase, reverse diffusion process, involves
iteratively applying a denoising function or a deep neural network to the initial noise sampled from the Gaussian
distribution, which removes noise and updates the image. This process is repeated multiple times to generate
samples from the target distribution. Diffusion models use a learned denoising function, typically implemented as
U-Net,19to denoise the image at each step of the reverse diffusion process.
We apply the Diffusion model on latent vectors learnt using VQ-VAE17model, conditioned on the cancer
type (benign or malignant). We enable this conditioning mechanism using the cross-attention11mechanism.
The sampled latent vectors are passed through the VQ-VAE decoder model trained in the last step to generate
realistic glandular masks. In this experiment, instead of binary glandular masks, we use tissue component masks
highlighting glands (in green color), stromal region (red color) and the background (blue color) as shown in
Figure 5. The idea is to model dependencies between maximum number of components inside the tissue image for
crafting realistic glandular masks. The binary glandular masks can be obtained later from the generated tissue
component masks by extracting glands and keeping others as the background. To generate the synthetic tissue
images, we employ the residual encoder decoder network.20
4. EXPERIMENTAL RESULTS
4.1 Data Acquisition
We use the DigestPath21colonoscopy tissue segment dataset to assess the performance of our algorithm. The
dataset is collected from the DigestPath2019 challenge∗. It contains 660 very large tissue images with an average
size of 5000×5000pixels. Each image is associated with pixel-level annotation for glandular regions. This
dataset originally contained annotations for malignant lesions (250 images) only. In order to obtain a tissue
segmentation mask for benign glands, we used a semi-automatic approach. For this purpose, we first trained a
gland segmentation model named Mild-Net22on the GlaS dataset,23,24and obtained gland segmentation masks
for images with normal grades, in the DigestPath dataset which were manually refined. From these image, we
extracted 1733 patches of size 512×512that were later resized to 256×256. Out of these, we kept around 1300
patches for training (train set) and the rest for testing purpose (test set).
∗https://digestpath2019.grand-challenge.org/Figure 2. Visual results of generated colorectal tissue images along with their gland segmentation masks from input gland
layouts (a). (b) shows original gland segmentation masks while (c) shows the ground truth tissue images.
The obtained masks can be binary (separating glandular region and rest) which is utilized in the proposed
framework or can be ternary (glandular regions, stromal regions and background) which are used for generating
synthetic masks as described in Section 3.5. The stromal regions in ternary masks are obtained by performing
pixel-thresholding on H&E tissue images. The procedure to acquire bounding boxes from the gland masks collected
from the digestpath dataset, and also to construct them from the input location− →land size− →sparameters (gland
layout) is given in Appendix 5.6.
4.2 Model Training
To train the whole framework, we set the target tissue image size N= 256, input noise dimensionality for the
gland specific embeddings, dim(z) = 6, latent vector size D= 32and generated per gland size B= 64. For the
loss function (shown in equation 5), we set λ1=λ2=λ3= 100 .0andλ4=λ5=λ6= 1.0after cross-validation
tuning.
We train all models using Adam optimizer25with learning rate 10−4and batch size 1 for approx 300K
iterations. For each iteration, we first update the generator weights {θA, θI, θM, θE}, then update discriminators
{DT, DZ, DG}. The framework is implemented in Pytorch on an Nvidia Titan X and took almost 2 days for
training.
4.3 Visual Results
The visual results of generated tissue images (from the test set), can be seen in Figure 2. We can observe that
glandular shape are preserved, tissue components like goblet cells, stromal regions are constructed with fidelity
with moderate deformities in the glandular lumen. The generated tissue component masks also appear close to
actual masks. The slight deformities and variations in shapes of those masks is a result of using Gaussian noise in
the representation embedding of glands, which also make them realistic in nature. We also investigate the change
in appearance of glands after altering size− →sand location− →l. Figure 3 shows the results of tissue images after
changing their locations and sizes. The bounding boxes get modified after altering sizes and locations, which
effectively change the size and orientation of glands.
4.4 Quantitative Analysis
We evaluate the quality of generated images (from the test set) using the Frechet Inception Distance (FID),26a
standard metric used to assess the quality of images by the generative model. It computes the distance between
convolution feature maps calculated for real and generated images. For our experiments, to collect convolution
features, we use the pretrained InceptionV327network trained on the ImageNet dataset.28The lower the FID
score is the better is the image quality. As the metric depends on the image size, to get a sense of its scale, we
also compute the FID score between ground truth images and random noise of the same size. As a baseline, weFigure 3. The leftmost image (a) shows the generated sample out from the proposed framework. Images on the right to it
shows the change in appearance of the yellow bordered gland, after altering location− →l= (lx, ly)and size− →s= (sx, sy).
(b) and (c) shows the shift of that gland to left side (lowering lx) and upwards (increasing ly), respectively. For the same
gland, (d) shows the contraction horizontally and (e) shows expansion vertically after reducing ( sx) and increasing ( sy),
respectively.
adapt the image-to-image Pix2Pix network15to generate tissue images from existing tissue component masks,
and compute the FID for tissue images generated by it. The comparative results are shown in Table 1. Table
1 shows the proposed model achieves better results compared to the random noise and little inferior to that
of that of Pix2Pix. The reason can be that as, we are aiming to construct gland segmentation masks as well
along with the final tissue images, while Pix2Pix assumes ground truth masks already present and constructs the
tissue image from it. Thus, the construction error in generation of masks can influence the performance of our
framework, and may slightly lower the quality of generated images. However, looking at the scale of FID values,
the difference between both frameworks is not significant.
Model FID
Random 485±5.7
Pix2Pix 120±2.7
Proposed Framework 134±2.4
Table 1. Frechet Inception Distance (FID) score comparison
4.5 Assessment through gland segmentation
We also assess the quality of annotated pairs generated by our framework using the U-net based gland segmentation
algorithm.19We train U-net on patches of size 256×256from the train set and compute segmentation masks of
both real and synthetic images from the test set. We use the Dice score29between the real component masks and
masks computed by U-net on real images, and also between the generated component masks and masks computed
by U-net on synthetic images. Sample results are shown in Figure 4.
We obtained an average Dice index of 0.9022 (with standard deviation 0.006) and 0.9001 (with standard
deviation 0.012) for both respective cases. This highly similar obtained score validates the applicability of both
generated tissue images along with their tissue component masks, for the evaluation of gland segmentation
algorithms.
4.6 Image Synthesis from Masks Generated Using Latent Diffusion Model
The tissue component masks generated through the latent diffusion model based framework (described in Section
3.5), along with the corresponding tissue images created using the residual encoder-decoder network,20are
displayed in Figure 5. The resulting FID between genuine images and images produced through this method isFigure 4. Samples of both real (above) and constructed (below) annotated pairs of tissue images and corresponding gland
segmentation masks. The masks shown on the right side are generated from the U-net based segmentation algorithm when
applied on original (above) and synthetic (below) images.
Figure 5. Annotated pairs synthesized using the latent diffusion model. The first two pairs (from right) depict annotated
colon tissue images of the benign grade, while the last two show images of the malignant type.
130.5 ( ±2.8). Comparing these FID values with those obtained previously, it’s evident that the synthetic images
closely resemble real images.
5. CONCLUSION AND FUTURE DIRECTIONS
In this work, we presented an interactive framework to generate annotated pairs of colorectal tissue images along
with their tissue component masks. We performed experiments on DigestPath dataset and demonstrate the
framework’s ability to generate realistic images preserving morphological features including stroma, goblet cells
and glandular lumen. The generated images maintain good FID scores when compared with the state-of-the-art
image-to-image translation model. We showed variability in glandular appearance after altering sizes and locations
of glands. Additionally, we also demonstrated the applicability of synthetic annotated pairs for the evaluation of
gland segmentation algorithms.
One visible limitation in the proposed framework is that it requires glandular layout to construct images
which still need some efforts. However, we have given an alternate methodology to construct the glandular masks
and thereby generating tissue images.
The idea can be extended in future to generate annotated tissue image pairs for various tasks in computational
histopathology such as nuclei segmentation, cancer grading etc. The generated pairs can potentially replace thereal-world training data, pass the legal and security barriers while using them, assist the training and validation
of digital pathology algorithms, and reduce the cost and efforts of acquiring data.REFERENCES
[1]Quiros, A. C., Murray-Smith, R., and Yuan, K., “Pathology gan: learning deep representations of cancer
tissue,”arXiv preprint arXiv:1907.02644 (2019).
[2]Mahmood, F., Borders, D., Chen, R. J., Mckay, G. N., Salimian, K. J., Baras, A., and Durr, N. J., “Deep
adversarial training for multi-organ nuclei segmentation in histopathology images,” IEEE Transactions on
Medical Imaging 39(11), 3257–3267 (2020).
[3]Deshpande, S., Minhas, F., Graham, S., and Rajpoot, N., “Safron: Stitching across the frontier network for
generating colorectal cancer histology images,” Medical Image Analysis 77, 102337 (2022).
[4]Deshpande, S., Dawood, M., Minhas, F., and Rajpoot, N., “Synclay: Interactive synthesis of histology images
from bespoke cellular layouts,” Medical Image Analysis 91, 102995 (Jan. 2024).
[5]Hou, L., Agarwal, A., Samaras, D., Kurc, T., Gupta, R., and Saltz, J., “Robust histopathology image analysis:
To label or to synthesize?,” 8525–8534 (06 2019).
[6]Levine, A. B., Peng, J., Farnell, D., Nursey, M., Wang, Y., Naso, J., Ren, H., Farahani, H., Chen, C., Chiu,
D., Talhouk, A., Sheffield, B., Riazy, M., Ip, P., Parra-Herran, C., Mills, A., Singh, N., Tessier-Cloutier, B.,
Salisbury, T. D., Lee, J., Salcudean, T., Jones, S. J. M., Huntsman, D., Gilks, C., Yip, S., and Bashashati, A.,
“Synthesis of diagnostic quality cancer pathology images by generative adversarial networks,” The Journal of
Pathology 252(2020).
[7]McGaghie, W. C., “Medical education research as translational science,” Science Translational Medicine 2(19),
19cm8–19cm8 (2010).
[8]He, J., Baxter, S., Xu, J., Xu, J., Zhou, X., and Zhang, K., “The practical implementation of artificial
intelligence technologies in medicine,” Nature Medicine 25(01 2019).
[9]Senaras, C., Niazi, M. K. K., Sahiner, B., Pennell, M. P., Tozbikian, G., Lozanski, G., and Gurcan, M. N.,
“Optimized generation of high-resolution phantom images using cgan: Application to quantification of ki67
breast cancer images,” PloS one13(5), e0196846 (2018).
[10]Deshpande, S., Minhas, F., and Rajpoot, N., “Train small, generate big: Synthesis of colorectal cancer
histology images,” in [ Simulation and Synthesis in Medical Imaging ], Burgos, N., Svoboda, D., Wolterink,
J. M., and Zhao, C., eds., 164–173, Springer International Publishing, Cham (2020).
[11]Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B., “High-resolution image synthesis
with latent diffusion models,” in [ Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) ], (2022).
[12]Esser, P., Rombach, R., and Ommer, B., “Taming transformers for high-resolution image synthesis,” in
[Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ], 12873–
12883 (June 2021).
[13]Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I., “Zero-shot
text-to-image generation,” in [ Proceedings of the 38th International Conference on Machine Learning ], Meila,
M. and Zhang, T., eds., Proceedings of Machine Learning Research 139, 8821–8831, PMLR (18–24 Jul 2021).
[14]Jaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K., “Spatial transformer networks,” in
[NIPS], (2015).
[15]Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A., “Image-to-image translation with conditional adversarial
networks,” in [ 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ], 5967–5976
(2017).
[16]Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,
Y., “Generative adversarial nets,” in [ Advances in neural information processing systems ], 2672–2680 (2014).
[17]van den Oord, A., Vinyals, O., and Kavukcuoglu, K., “Neural discrete representation learning,” in [ Proceedings
of the 31st International Conference on Neural Information Processing Systems ],NIPS’17, 6309–6318, Curran
Associates Inc., Red Hook, NY, USA (2017).
[18]Ho, J., Jain, A., and Abbeel, P., “Denoising diffusion probabilistic models,” in [ Advances in Neural Information
Processing Systems ], Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., eds., 33, 6840–6851,
Curran Associates, Inc. (2020).[19]Ronneberger, O., Fischer, P., and Brox, T., “U-net: Convolutional networks for biomedical image segmenta-
tion,” in [ International Conference on Medical image computing and computer-assisted intervention ], 234–241,
Springer (2015).
[20]Ashual, O. and Wolf, L., “Specifying object attributes and relations in interactive scene generation,” 2019
IEEE/CVF International Conference on Computer Vision (ICCV) , 4560–4568 (2019).
[21]Li, J., Yang, S., Huang, X., Da, Q., Yang, X., Hu, Z., Duan, Q., Wang, C., and Li, H., [ Signet Ring Cell
Detection with a Semi-supervised Learning Framework ], 842–854 (05 2019).
[22]Graham, S., Chen, H., Gamper, J., Dou, Q., Heng, P.-A., Snead, D., Tsang, Y. W., and Rajpoot, N.,
“Mild-net: minimal information loss dilated network for gland instance segmentation in colon histology
images,” Medical image analysis 52, 199–211 (2019).
[23]Sirinukunwattana, K., Pluim, J. P., Chen, H., Qi, X., Heng, P.-A., Guo, Y. B., Wang, L. Y., Matuszewski,
B. J., Bruni, E., Sanchez, U., Böhm, A., Ronneberger, O., Cheikh, B. B., Racoceanu, D., Kainz, P., Pfeiffer,
M., Urschler, M., Snead, D. R., and Rajpoot, N. M., “Gland segmentation in colon histology images: The
glas challenge contest,” Medical Image Analysis 35, 489–502 (jan 2017).
[24]Sirinukunwattana, K., Snead, D. R. J., and Rajpoot, N. M., “A stochastic polygons model for glandular
structures in colon histology images,” IEEE Transactions on Medical Imaging 34(11), 2366–2378 (2015).
[25]Kingma, D. and Ba, J., “Adam: A method for stochastic optimization,” International Conference on Learning
Representations (12 2014).
[26]Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S., “Gans trained by a two time-scale
update rule converge to a local nash equilibrium,” in [ Advances in neural information processing systems ],
6626–6637 (2017).
[27]Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z., “Rethinking the inception architecture
for computer vision,” in [ Proceedings of the IEEE conference on computer vision and pattern recognition ],
2818–2826 (2016).
[28]Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L., “Imagenet: A large-scale hierarchical image
database,” in [ 2009 IEEE conference on computer vision and pattern recognition ], 248–255, Ieee (2009).
[29]Zou, K. H., Warfield, S. K., Bharatha, A., Tempany, C. M., Kaus, M. R., Haker, S. J., Wells III, W. M.,
Jolesz, F. A., and Kikinis, R., “Statistical validation of image segmentation quality based on a spatial overlap
index1: scientific reports,” Academic radiology 11(2), 178–189 (2004).
[30] Bradski, G., “The OpenCV Library,” Dr. Dobb’s Journal of Software Tools (2000).Appendix
Here we describe the neural network architectures for each component within the proposed framework
5.1 Individual Mask Generator Network
We generate the individual mask for each of the glands using the individual mask generator network . The input is
the individual gland latent vectors obtained after affine transformation on the original gland embeddings, and
output is the 64×64pixels glandular mask with all elements ranged between 0 and 1. The mask regression
network composed of series of individual mask generator blocks, where each block consist of interpolation +
convolution + batch normalization + ReLU activation operations. The exact architecture is shown in the table 2,
while architecture of the individual mask generator block is shown in Table 3.
IndexInputs Operation Output Shape
(1) - Gland Latent Vector 32
(2) (1) Reshape 32 x 1 x 1
(3) (2)Individual Mask Generator Block 32 x 2 x 2
(4) (3)Individual Mask Generator Block 32 x 4 x 4
(5) (4)Individual Mask Generator Block 32 x 8 x 8
(6) (5)Individual Mask Generator Block 32 x 16 x 16
(7) (6)Individual Mask Generator Block 32 x 32 x 32
(8) (7)Individual Mask Generator Block 32 x 64 x 64
(9) (8) Conv2d (K=1, 32 →1) 1 x 64 x 64
(10) (9) Sigmoid 1 x 64 x 64
Table 2. Architecture of the individual mask generator network. The function implements function Ifrom the main text.
The notation Conv2d(K , Cin→Cout) is a convolution with K×Kkernels, Cininput channels and Coutoutput channels;
all convolutions with stride 1 with zero padding that ensures input and output have the same spatial size.
Operation Output Shape
Interpolation 32 x 2S x 2S
Conv2d (K=3, 32 →32)32 x 2S x 2S
Batch Normalization 32 x 2S x 2S
ReLU 32 x 2S x 2S
Table 3. Architecture of the individual mask generator block. The input is the feature map of shape C×S×S, where C is
the number of channels from the feature map of the last layer, and S×Sis the dimension of height and width.
5.2 Mask Generator Network
The generated intermediate tensor (explained in 3.1) has 32 channels, which got reduced to 1 using the mask
generator network , forming the glandular mask or tissue component mask. The network comprised of a series of
convolution + Relu operations. The exact architecture is shown in Table 4.
5.3 Encoder-Decoder Network
The final tissue image is generated from the generated glandular mask with the help of encoder decoder network .
The encoder consist of a series of “Encode" blocks (shown in Table 6) and generates the lower sized encoding of
the input mask, while decoder comprised of a series of “Decode" blocks (shown in Table 7) and generates the final
tissue image from the encoding. The exact architecture of the encoder-decoder network is shown in Table 5.
5.4 Mask and Image Discriminators
The discriminator we employed has the similar architecture for both glandular masks ( DM) and tissue images
(DZ), takes the real or fake image of shape C×256×256as an input ( C= 1for component mask and C= 3for
tissue image), and classifies an overlapping grid of size 7×7image patches from the input image as real or fake.
The exact architecture of the discriminator is shown in Table 8IndexInputs Operation Output Shape
(1) -Generate Cumulative Mask 32 x 256 x 256
(2) (1) Conv2d (K=3, 32 →16)16 x 256 x 256
(3) (2) LeakyReLU 16 x 256 x 256
(4) (3) Conv2d (K=3, 16 →8)8 x 256 x 256
(5) (4) LeakyReLU 8 x 256 x 256
(6) (5) Conv2d (K=3, 8 →4) 4 x 256 x 256
(7) (6) LeakyReLU 4 x 256 x 256
(8) (7) Conv2d (K=3, 8 →4) 1 x 256 x 256
(9) (8) LeakyReLU 1 x 256 x 256
Table 4. Architecture of the mask generator network. The network implements function Mfrom the main text. LeakyReLU
uses a negative slope coefficient of 0.2
IndexInputs Operation Output Shape
(1) -Generate Component Mask 1 x 256 x 256
(2) (1) Encode(1,64) 64 x 128 x 128
(3) (2) Encode(64,128) 128 x 64 x 64
(4) (3) Encode(128,256) 256 x 32 x 32
(5) (4) Encode(256,512) 512 x 16 x 16
(6) (5) Encode(512,512) 512 x 8 x 8
(7) (6) Encode(512,512) 512 x 4 x 4
(8) (7) Encode(512,512) 512 x 2 x 2
(9) (8) Encode(512,512) 512 x 1 x 1
(10) (9,8) Decode(512,512) 1024 x 2 x 2
(11) (10,7) Decode(1024,512) 1024 x 4 x 4
(12) (11,6) Decode(1024,512) 1024 x 8 x 8
(13) (12,5) Decode(1024,512) 1024 x 16 x 16
(14) (12,4) Decode(1024,256) 512 x 32 x 32
(15) (14,3) Decode(512,128) 256 x 64 x 64
(16) (15,2) Decode(256,64) 128 x 128 x 128
(17) (16) Upsample 128 x 256 x 256
(18) (17) Conv2d (K=4, 128 →3)3 x 256 x 256
(19) (18) Tanh 3 x 256 x 256
Table 5. Architecture of the encoder-decoder network. The network implements the function Efrom the main text.
Operation Output Shape
Conv2d (K=4, Cin→Cout) Coutx S x S
Instance Normalization (if normalize=True) Coutx S x S
LeakyReLU Coutx S x S
Dropout (if dropout=True) Coutx S x S
Table 6. Architecture of the “Encode" block. LeakyReLU uses a negative slope coefficient of 0.2. The input is image of size
Cin×S×S
Operation Output Shape
ConvTranspose2d(K=4, Cin→Cout) Coutx S x S
Instance Normalization Coutx S x S
ReLU Coutx S x S
Dropout (if dropout=True) Coutx S x S
Table 7. Architecture of the “Decode" block. The input is image of size Cin×S×S
5.5 Gland Discriminator
Our gland discriminator DGconsumes image pixels corresponding to glandular areas from the real or fake tissue
images, and classifies them as real or fake. The glandular areas are cropped out using their bounding boxIndexInputs Operation Output Shape
(1) - Generate the Image C x 256 x 256
(2) (1) Conv2d (K=4, C →16, S=2) 16 x 128 x 128
(3) (2) LeakyReLU 16 x 128 x 128
(4) (3) Conv2d (K=4, 16 →32, S=2) 32 x 64 x 64
(5) (4) LeakyReLU 32 x 64 x 64
(6) (5) Instance Normalization 32 x 64 x 64
(7) (6) Conv2d (K=4, 32 →64, S=2) 64 x 32 x 32
(8) (7) LeakyReLU 64 x 32 x 32
(9) (8) Instance Normalization 64 x 32 x 32
(10) (9)Conv2d (K=4, 64 →128, S=2) 128 x 16 x 16
(11) (10) LeakyReLU 128 x 16 x 16
(12) (11) Instance Normalization 128 x 16 x 16
(13) (12)Conv2d (K=4, 128 →256, S=2) 256 x 8 x 8
(14) (13) LeakyReLU 256 x 8 x 8
(15) (14) Instance Normalization 256 x 8 x 8
(16) (15) Conv2d (K=4, 256 →1 , S=1) 1 x 7 x 7
Table 8. Architecture of the Discriminator network. C=1 when input is the tissue component mask and C=3 for the tissue
image. All but the last Conv2d operation has stride 2. LeakyReLU uses a negative slope coefficient of 0.2
IndexInputs Operation Output Shape
(1) -Crop glandular portions from the generated image 3 x 64 x 64
(2) (1) Conv2d (K=5, 3 →16, S=2) 16 x 30 x 30
(3) (2) Batch Normalization 16 x 30 x 30
(4) (3) LeakyReLU 16 x 30 x 30
(5) (4) Conv2d (K=5, 16 →32, S=2) 32 x 13 x 13
(6) (5) Batch Normalization 32 x 13 x 13
(7) (6) LeakyReLU 32 x 13 x 13
(8) (7) Conv2d (K=5, 32 →64, S=2) 64 x 5 x 5
(9) (8) Global Average Pooling 64
(10) (9) Affine Transformation 1024
(11) (10) Affine Transformation 1
Table 9. Architecture of the gland discriminator, DG. LeakyReLU uses a negative slope coefficient of 0.2
coordinates, and resized to 64×64pixels using the bilinear interpolation method. The exact architecture of the
gland discriminator is shown in Table 9
5.6 Acquisition of Bounding Boxes
After we collect the tissue images and their annotated gland masks from the digestpath dataset as described
in section 4.1, we used the OpenCV (Open Source Computer Vision Library) python library30to extract the
location of glands i.e., centroids of white blob objects from the black and white tissue component mask (as shown
in mask in figure 1). Later we also collected the bounding boxes of those identified glandular objects using the
built-in function boundingRect() function of the same library.
During inference, apart from using the ground truth bounding boxes obtained by the procedure described
above, we also construct bounding box for the gland gkusing the input size− →skand location− →lkattributes, taken
from the gland layout. Given the input size− →lk= (skx, sky), where sxandsyare the horizontal and vertical
spanning lengths of glands, and the centroid location− →lk= (lkx, lky), the bounding box coordinates for gkare
computed as,
bk= (lkx−(skx/2),(lky−(sky/2),(lkx+ (skx/2),(lky+ (sky/2)). (6)Overall, input to the proposed framework is the set of glandular locations, their sizes, their bounding boxes
acquired from the dataset or constructed using the above procedure, and output is the pair of the tissue image
and its tissue component mask.