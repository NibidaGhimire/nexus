Tight Guarantees for Interactive Decision Making
with the Decision-Estimation Coeﬃcient
Dylan J. Foster
dylanfoster@microsoft.comNoah Golowich∗
nzg@mit.eduYanjun Han
yjhan@mit.edu
January 19, 2023
Abstract
A foundational problem in reinforcement learning and interactive decision making is to understand what
modeling assumptions lead to sample-eﬃcient learning guarantees, and what algorithm design principles
achieve optimal sample complexity. Recently, Foster et al. (2021) introduced the Decision-Estimation
Coeﬃcient (DEC), a measure of statistical complexity which leads to upper and lower bounds on the
optimal sample complexity for a general class of problems encompassing bandits and reinforcement learning
with function approximation. In this paper, we introduce a new variant of the DEC, the Constrained
Decision-Estimation Coeﬃcient , and use it to derive new lower bounds that improve upon prior work on
three fronts:
•they hold in expectation, with no restrictions on the class of algorithms under consideration.
•they hold globally, and do not rely on the notion of localization used by Foster et al. (2021).
•most interestingly, they allow the reference model with respect to which the DEC is deﬁned to be
improper , establishing that improper reference models play a fundamental role.
We provide upper bounds on regret that scale with the same quantity, thereby closing all but one of
the gaps between upper and lower bounds in Foster et al. (2021). Our results apply to both the regret
framework and PAC framework, and make use of several new analysis and algorithm design techniques
that we anticipate will ﬁnd broader use.
Contents
1 Introduction 2
1.1 Interactive Decision Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Background: Decision-Estimation Coeﬃcient . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Constrained Decision-Estimation Coeﬃcient and Overview of Results . . . . . . . . . . . . . . 5
1.4 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.5 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2 Lower Bounds 7
2.1 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Proof of PAC Lower Bound (Theorem 2.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3 Upper Bounds 13
3.1 Online Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.2 Algorithm and Upper Bound for PAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.3 Proof of PAC Upper Bound (Theorem 3.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.4 Algorithm and Upper Bound for Regret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4 Decision-Estimation Coeﬃcient: Structural Properties 22
∗Supported by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Fellowship.
1arXiv:2301.08215v1  [cs.LG]  19 Jan 20234.1 Relationship Between Constrained and Oﬀset DEC . . . . . . . . . . . . . . . . . . . . . . . . 22
4.2 Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
4.3 Reference Models: Role of Suboptimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.4 Reference Models: Role of Convexity and Randomization . . . . . . . . . . . . . . . . . . . . 26
5 Improvement over Prior Work 27
5.1 Additional Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
6 Additional Examples 29
A Preliminaries 33
A.1 Minimax Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
B Omitted Proofs from Section 2 33
B.1 Proof of Regret Lower Bound (Theorem 2.2) . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
C Omitted Proofs from Section 3 39
C.1 Proof of Regret Upper Bound (Theorem 3.2) . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
D Proofs and Additional Results from Section 4 45
D.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
D.2 Additional Properties of the Decision-Estimation Coeﬃcient . . . . . . . . . . . . . . . . . . . 46
D.3 Omitted Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
E Omitted Proofs from Section 5 58
1 Introduction
Sample eﬃciency in reinforcement learning and decision making is a fundamental challenge. State-of-the-art
algorithms (Lillicrap et al., 2015; Mnih et al., 2015; Silver et al., 2016) often require millions of rounds of
interaction to achieve human-level performance, which is prohibitive in practice and constitutes a barrier to
reliable deployment. For continued progress on challenging real-world domains where the agent must navigate
high-dimensional state and observation spaces, it is critical to design algorithms that can take advantage of
users’ domain knowledge (via modeling and function approximation) to enable generalization and improved
sample eﬃciency. As such, a foundational question is to understand what modeling assumptions lead to
sample-eﬃcient learning guarantees, and what algorithms achieve optimal sample complexity.
The non-asymptotic theory of reinforcement learning is rich with suﬃcient conditions under which sample-
eﬃcient learning is possible (Dean et al., 2020; Yang and Wang, 2019; Jin et al., 2020; Modi et al., 2020; Ayoub
et al., 2020; Krishnamurthy et al., 2016; Du et al., 2019; Li, 2009; Dong et al., 2019; Zhou et al., 2021), as
well as structural properties that aim to unify these conditions (Russo and Van Roy, 2013; Jiang et al., 2017;
Sun et al., 2019; Wang et al., 2020; Du et al., 2021; Jin et al., 2021), but conditions that are necessary have
generally been elusive. Recently though, Foster et al. (2021) introduced the Decision-Estimation Coeﬃcient
(DEC), a uniﬁed notion of statistical complexity that leads to both upper and lowerbounds on the optimal
sample complexity in a general decision making framework. The results of Foster et al. (2021) show that the
DEC plays a role for interactive decision making analogous to that of the VC dimension and its relatives in
statistical learning, but leave room for tighter quantitative guarantees. In this paper, we introduce a new
variant of the DEC, the Constrained Decision-Estimation Coeﬃcient , and use it to close several gaps between
the upper and lower bounds in Foster et al. (2021).
1.1 Interactive Decision Making
We consider the Decision Making with Structured Observations (DMSO) framework of Foster et al. (2021), a
general setting for interactive decision making that encompasses bandits problems (including structured and
contextual bandits) and reinforcement learning with function approximation.
2The protocol consists of Trounds. For each round t= 1;:::;T:
1. The learner selects a decisiont2, where is thedecision space .
2.The learner receives a reward rt2R Rand observation ot2Osampled via (rt;ot)M?(t), where
M?: !(RO )is the underlying model.
We refer toRas thereward space andOas theobservation space . The model M?, which we formalize
as a conditional distribution, plays the role of the underlying environment, and is unknown to the learner.
However, the learner is assumed to have access to a model classM (!(RO ))that contains M?.
Assumption 1.1 (Realizability) .The learner has access to model class Mthat contains the true model M?.
The model classMrepresents the learner’s prior knowledge of the underlying environment. For structured
bandit problems, where models correspond to reward distributions, it encodes structure in the reward
landscape (smoothness, linearity, convexity, etc.), and for reinforcement learning problems, where models
correspond to Markov decision processes (MDPs), it typically encodes structure in the transition probabilities
or value functions. In more detail:
•Bandits. In bandit problems, Mis a reward distribution, tis referred to as an actionorarmand
is referred to as the action space ; there are no observations beyond rewards ( O=f?g). By choosingM
so that the mean reward function fMhas appropriate structure, one can capture bandit problems with
continuous or inﬁnite action spaces and structured rewards, including linear bandits (Dani et al., 2007;
Abernethy et al., 2008; Bubeck et al., 2012), bandit convex optimization (Kleinberg, 2004; Flaxman
et al., 2005; Bubeck et al., 2017; Lattimore, 2020), and nonparametric bandits (Kleinberg, 2004; Bubeck
et al., 2011; Magureanu et al., 2014).
•Reinforcement learning. In episodic reinforcement learning, each model M2Mspeciﬁes a non-
stationary horizon- HMarkov decision process M=
fShgH
h=1;A;fPM
hgH
h=1;fRM
hgH
h=1;d1	
, whereShis
the state space for layer h,Ais the action space, PM
h:ShA! (Sh+1)is the probability transition
kernel for layer h,RM
h:ShA! (R)is the reward distribution for layer h, andd12(S1)is the
initial state distribution. Decisions =fh:Sh!(A)gH
h=1arepolicies(mappings from states to
actions). Given a policy , an episode proceeds as follows (beginning from s1d1). Forh= 1;:::;H:
–ahh(sh).
–rhRM
h(sh;ah)andsh+1PM
h(jsh;ah).
This process leads to (r;o)M(), wherer=PH
h=1rhis the cumulative reward in the episode, and
the observation o= (s1;a1;r1);:::(sH;aH;rH)is the episode’s trajectory (sequence of observed states,
actions, and rewards). By choosing Mappropriately, one can encompass standard classes of MDPs
(e.g., tabular MDPs or linear systems) Dean et al. (2020); Yang and Wang (2019); Jin et al. (2020);
Modi et al. (2020); Ayoub et al. (2020); Krishnamurthy et al. (2016); Du et al. (2019); Li (2009); Dong
et al. (2019), as well as more general structural conditions (Jiang et al., 2017; Sun et al., 2019; Wang
et al., 2020; Du et al., 2021; Jin et al., 2021).
For a model M2M,EM;[]denotes expectation under the process (r;o)M(),fM():=EM;[r]denotes
the mean reward function, and M:= arg max2fM()denotes the optimal decision.
We consider two types of guarantees for interactive decision making: regret guarantees and PAC (Probably
Approximately Correct) guarantees. For regret guarantees, we are concerned with the cumulative suboptimality
given by
RegDM(T):=TX
t=1Etpt
fM?(M?) fM?(t)
; (1)
whereptis the learner’s randomization distribution (conditional distribution over t) for round t. For PAC
guarantees, we are only concerned with ﬁnal suboptimality . After rounds t= 1;:::;Tcomplete, the learner
can use all of the data collected to select a ﬁnal decision b(which may be randomized according to a
distribution p2()), and we measure performance via
Risk DM(T):=Ebp
fM?(M?) fM?(b)
: (2)
3We refer to Foster et al. (2021, 2022b) for further background and measure-theoretic details, as well as
additional examples and discussion.
1.2 Background: Decision-Estimation Coeﬃcient
To motivate our results, let us ﬁrst recall the Decision-Estimation Coeﬃcient of Foster et al. (2021); for this
discussion, we restrict our attention to regret guarantees. Deﬁne the squared Hellinger distance for probability
measures PandQwith a common dominating measure by
D2
H(P;Q) =Zr
dP
d r
dQ
d2
d: (3)
For a model class M, reference model M: !(RO ), and scale parameter  >0, the Decision-Estimation
Coeﬃcient is given by
r-deco
(M;M) = inf
p2()sup
M2MEp
fM(M) fM() D2
H 
M();M()
: (4)
Here, we depart slightly from the notation in Foster et al. (2021) and append the preﬁx r-(indicating “regret”)
and the superscript “o” (indicating “oﬀset”) to distinguish from other variants that will be introduced shortly.
Foster et al. (2021) (see also Foster et al. (2022b)) use the DEC to provide upper and lower bounds on the
optimal regret for interactive decision making:
•On the upper bound side, there exists an algorithm ( Estimation-to-Decisions ) that obtains
E[RegDM(T)].inf
max
sup
M2co(M)r-deco
(M;M)T+Est H(T)
: (5)
Here, the term Est H(T)represents the sample complexity required to perform statistical estimation
with the class, and has Est H(T)logjMjfor the special case of ﬁnite classes.
•On the lower bound side, any algorithm must have
E[RegDM(T)]&inf
max
sup
M2Mr-deco
(M0;M)T+
; (6)
whereM0is a certain “localized” subclass of M(roughly speaking, models for which kfM fMk1.
T).
While the results of Foster et al. (2021) show that the DEC characterizes learnability for various classes of
models (for instance, convex classes with low “estimation complexity”), the quantitative rates leave room for
improvement. The aim of this paper is to address the following gaps:
•To provide lower bounds that hold in expectation (as opposed to low probability) the lower bound (6)
restricts to a localized subclass of M0M, which may have lower complexity than the original class.
This yields reasonable results for many special cases, but can be arbitrarily loose in general.
•The lower bound (6) restricts to reference models M2M, yet the upper bound (5) maximizes over
reference models M2co(M), potentially leading to larger regret. For example, Proposition 3.1 of
Foster et al. (2021) gives a model class Mfor which this distinction leads to an arbitrarily large gap
between the upper and lower bounds.
A third gap, which we do not address, is the presence of the estimation complexity Est H(T)in Eq. (5), which
is not present in the lower bound. See Foster et al. (2021, 2022a) for further discussion around this issue.
41.3 Constrained Decision-Estimation Coeﬃcient and Overview of Results
To address the issues in the prequel, we introduce a new complexity measure, the Constrained Decision-
Estimation Coeﬃcient . For a reference model M: !(RO ), deﬁne , deﬁned via
r-decc
"(M;M) = inf
p2()sup
M2M
Ep[fM(M) fM()]jEp
D2
H 
M();M()
"2	
;(7)
with the convention that the value above is zero whenever the set
Hp;"(M):=
M2Mj Ep
D2
H 
M();M()
"2	
(8)
is empty; the superscript “c” indicates “constrained”, and distinguishes from the oﬀset counterpart. Our main
quantity of interest is
r-decc
"(M) = sup
M2co(M)r-decc
"(M[fMg;M); (9)
where co(M)denotes the convex hull of the class M.
The constrained and oﬀset DEC diﬀer only in how the quantity Ep
D2
H 
M();M()
, representing
information gain, is incorporated. The constrained DEC places a hard constraint on the information gain,
while the oﬀset DEC treats the information gain as a penalty for the max-player, amounting to a soft
constraint. At ﬁrst glance, one might expect these quantities to be equivalent via Lagrangian duality. It is
indeed the case that the oﬀset DEC upper bounds the constrained DEC:
r-decc
"(M;M)inf
fr-deco
(M;M)_0 +"2g;8M;
but strong duality fails, and the complexity measures are not equivalent in general; detailed discussion is
given in Section 4.
Main results for regret framework. The constrained DEC possesses a number of useful properties not
shared by the oﬀset DEC, including implicitly enforcing a form of localization in an adaptive fashion (cf.
Section 4). By leveraging these properties, we provide improved lower and upper bounds that close all but
one of the gaps between the bounds in Foster et al. (2021). Our main result is as follows.
Theorem (informal). For any model class M:
•Lower bound: For a worst-case model in M, any algorithm must have
E[RegDM(T)]e
(1)r-decc
"(T)(M)T
for"(T) =e p
1=T
,
•Upper bound: There exists an algorithm (Estimation-to-Decisions+) that achieves
E[RegDM(T)]eO(1)r-decc
"(T)(M)T
for"(T) =e p
Est H(T)=T
e p
logjMj=T
.
This lower and upper bound are always tighter than the respective bounds in prior work, and the lower bound
in particular improves upon Foster et al. (2021) on two fronts:
•It holds globally, and removes the notion of localization used by Foster et al. (2021). In addition, it
holds in expectation, with no restriction on the class of algorithms under consideration.
•More interestingly, it scales with r-decc
"(M) =supM2co(M)r-decc
"(M[fMg;M)as opposed to, say,
supM2Mr-decc
"(M;M), showing (in tandem with the upper bound) that improper reference models
M2co(M)play a fundamental role, and are not simply an artifact of the upper bounds in Foster et al.
(2021).
5Together, our upper and lower bounds form an important step toward building a simple, user-friendly, and
uniﬁed theory for interactive decision making based on the Decision-Estimation Coeﬃcient. The only gap our
results leave open is the role of the estimation error Est H(T), a deep issue that necessitates future research.
On the technical side, our lower bounds make use of several new analysis techniques and structural results
that depart sharply from previous approaches (Foster et al., 2021, 2022b). Our upper bounds build upon
the Estimation-to-Decisions paradigm introduced in Foster et al. (2021), but the design and analysis are
substantially more sophisticated, as certain properties of the constrained DEC that aid in proving lower
bounds lead to non-trivial challenges in deriving upper bounds. We anticipate that our techniques will ﬁnd
broader use, and to this end we provide user-friendly tools for working with the constrained DEC in Section 4.
Remark 1.1. At this point, the reader may wonder why the deﬁnition (9) incorporates the set M[fMg,
whereMmay lie outside the class M. We show in Section 4 that this modiﬁcation, despite not being required
for the oﬀset DEC, plays a central role for the constrained DEC.
Main results for PAC framework. Moving from the regret framework to PAC, we work with the
following PAC counterparts to the oﬀset and constrained DEC:
p-deco
(M;M) = inf
p;q2()sup
M2M
Ep[fM(M) fM()] Eq
D2
H 
M();M()	
(10)
and
p-decc
"(M;M) = inf
p;q2()sup
M2M
Ep[fM(M) fM()]jEq
D2
H 
M();M()
"2	
;(11)
with the convention that the value in Eq. (11) is zero when Hq;"(M) =?. These deﬁnitions parallel (4)
and (7), but allow the min-player to select a separate exploration distribution qunder which the information
gain (Hellinger distance) is evaluated, and exploitation distribution punder which regret is evaluated.
Since one can always choose p=q, it is immediate that p-deco
(M;M)r-deco
(M;M), and likewise
p-decc
"(M;M)r-decc
"(M;M). Deﬁning1
p-decc
"(M) = sup
M2co(M)p-decc
"(M;M); (12)
we show that the PAC DEC leads to the following lower and upper bounds on PAC sample complexity.
Theorem (informal). For any model class M:
•Lower bound: For a worst-case model in M, any PAC algorithm with Trounds of interaction must have
E[Risk DM(T)]e
(1)p-decc
"(T)(M)
for"(T) =e p
1=T
.
•Upper bound: There exists an algorithm (Estimation-to-Decisions+) that achieves
E[Risk DM(T)]eO(1)p-decc
"(T)(M)
for"(T) =e p
Est H(T)=T
e p
logjMj=T
.
1.4 Preliminaries
Interactive decision making. We assume throughout the paper that R=[0;1]unless otherwise stated.
We letM+=fM: !([0;1]O)gdenote the space of all possible models with rewards in [0;1]. We
adopt the shorthand gM() =fM(M) fM().
1Compared the deﬁnition of the constrained DEC for regret, the deﬁnition p-decc
"(M) =supM2co(M)p-decc
"(M;M)does not
apply the DEC to the class M[fMg. See Section 4.3 for a detailed explanation.
6We adopt the same formalism for probability spaces as in Foster et al. (2021, 2022b). Decisions are associated
with a measurable space (;P), rewards are associated with the space (R;R), and observations are associated
with the space (O;O). The history up to time tis denoted by Ht= (1;r1;o1);:::; (t;rt;ot). We deﬁne

t=tY
i=1(RO );andFt=tO
i=1(P
R
O)
so that Htis associated with the space (
t;Ft). For a classM, we deﬁne
V(M):= sup
M;M02Msup
2sup
A2R
OM(Aj)
M0(Aj)	
_e;
ﬁniteness of this quantity is not necessary for our any of our results, but improves our lower bounds
(Theorems 2.1 and 2.2) by a log(T)factor.
Divergences. For probability distributions PandQover a measurable space (
;F)with a common
dominating measure, we deﬁne the total variation distance as
DTV(P;Q) = sup
A2FjP(A) Q(A)j=1
2Z

jdP dQj:
1.5 Organization
Section 2 presents our main lower bounds for the regret and PAC frameworks, and Section 3 presents
complementary upper bounds. Section 4 establishes structural results concerning the constrained DEC and
oﬀset DEC, and Section 5 provides a detailed comparison to bounds from prior work. We close with additional
examples (Section 6). Unless otherwise stated, proofs are deferred to the appendix.
Additional notation. For an integer n2N, we let [n]denote the setf1;:::;ng. For a setZ, we let (Z)
denote the set of all probability distributions over Z, and letZcdenote the complement. We adopt standard
big-oh notation, and write f=eO(g)to denote that f=O(gmaxf1;polylog(g)g). We use .only in informal
statements to emphasize the most notable elements of an inequality.
2 Lower Bounds
In this section, we provide lower bounds based on the constrained Decision-Estimation Coeﬃcient for the
PAC and regret frameworks, then prove the lower bound for PAC, highlighting the most salient ideas behind
the result.
2.1 Main Results
We provide minimax lower bounds for interactive decision making, which show for any model class Mand
horizonT2N, the worst-case regret (resp. PAC sample complexity) for any algorithm is lower bounded by
the constrained DEC for an appropriate choice of the radius parameter ">0. Our lower bounds for PAC
and regret take a nearly identical form, and diﬀer only in the use of p-decc
"(M)versus r-decc
"(M). To state
the results, deﬁne C(T):= log(T^V(M)).
Theorem 2.1 (Main Lower Bound: PAC) .Let"(T):=c1p
TC(T), wherec >0is a suﬃciently small
numerical constant and C:= 48p
2. For allT2Nsuch that the condition
p-decc
"(T)(M)C"(T) (13)
is satisﬁed, it holds that for any PAC algorithm, there exists a model in Msuch that
E[Risk DM(T)]
(1)sup
M2M+p-decc
"(T)(M[fMg;M)
(1)p-decc
"(T)(M): (14)
7Theorem 2.2 (Main Lower Bound: Regret) .There exist universal constants C;C0>0andc;c0>0such
that the following holds. Let "(T):=c1p
TC(T). For allT2Nsuch that the condition
r-decc
"(T)(M)C"(T) (15)
is satisﬁed, it holds that for any regret minimization algorithm, there exists a model in Msuch that
E[RegDM(T)]c0sup
M2M+r-decc
"(T)(M[fMg;M)T
logT C0p
T
log(T)(16)
c0r-decc
"(T)(M)T
logT C0p
T
log(T):
Theorems 2.1 and 2.2 show that the constrained DEC is a fundamental limit for interactive decision making.
Importantly, these lower bounds remove the notion of localization required by prior work, and show that the
DEC remains a lower bound even if one allows for improper reference models M2co(M); as a result, they
are always tighter than those found in Foster et al. (2021, 2022b). We will show in a moment (Section 3) that
our lower bounds can achieved algorithmically, up to a diﬀerence in radius that depends on the estimation
capacity forM(p
logjMj=Tversus 1=p
Tfor the case of ﬁnite classes). We defer a detailed comparison to
prior work to Section 5, and take this time to build intuition as to the behavior of the lower bounds and give
an overview of our proof techniques.
Remark 2.1. WheneverV(M) =O(1), we have"(T)/1=p
Tin Theorems 2.1 and 2.2. In the general
case where V(M)is not bounded, we have "(T)/1=p
Tlog(T), and the lower bounds lose a logarithmic
factor. For most standard model classes, one has V(M) =1, but there exists a subclass M0Mwith
V(M0) =O(1)and p-decc
"(M0)&p-decc
"(M)(resp. r-decc
"(M0)&r-decc
"(M)). In this case, one can derive a
tighter lower bound with "(T)/1=p
Tby applying Theorem 2.1 (resp. Theorem 2.2) to M0. See Foster
et al. (2021) for examples.
Remark 2.2. Theorem 2.1 (resp. Theorem 2.2) scales with the quantity supM2M+p-decc
"(M[fMg;M)
p-decc
"(M)(resp. supM2M+r-decc
"(M[fMg;M)r-decc
"(M)), which allows for arbitrary reference models
M =2co(M). We show in Section 4.4 that maximizing over reference models M2M+does not increase
the value of the DEC beyond what is attained by M2co(M), so this result does not contradict our upper
bounds. We state the lower bounds in this form because 1) our proof works with M2M+directly, and does
not use the structure of co(M), and 2) allowing for M2M+often simpliﬁes calculations.
Understanding the lower bounds. Let us give a sense for how the lower bounds behave for standard
model classes. We focus on regret, since this is the main object of interest for prior work.
•p
T-rates.Forthemostwell-studiedclassesfound throughouttheliteratureon banditsandreinforcement
learning, we have
r-decc
"(M)/"p
Cprob;
whereCprob>0is a problem-dependent constant that reﬂects some notion of intrinsic complexity. In
this case, the condition (15) is satisﬁed whenever Cprobis larger than some numerical constant, and
Theorem 2.2 gives2
E[RegDM(T)]e
 p
CprobT
:
Examples (cf. Section 6) include multi-armed bandits with Aactions, where CprobA(leading to
E[RegDM(T)]e
(p
AT)), linear bandits in dimension d, whereCprobd(leading to E[RegDM(T)]
e
(p
dT)), and tabular reinforcement learning with Sstates,Aactions, and horizon H, whereCprob
HSA(leading to E[RegDM(T)]e
(p
HSAT )).
2This requires V(M) =O(1)so that"(T)/1=p
T; see Remark 2.1.
8•Nonparametric rates. For nonparametric model classes, where the optimal regret is of larger order thanp
T, one typically has
r-decc
"(M)/"1 
for some2(0;1). In this case, the condition in Eq. (15) is satisﬁed whenever Tis a suﬃciently large
constant, and Theorem 2.2 gives
E[RegDM(T)]e
(T1+
2):
A standard example is Lipschitz bandits over [0;1]d, where we have r-decc
"(M)/"1 d
d+2, leading to
E[RegDM(T)]e
 
Td+1
d+2
.
•Fast rates. For problems with low noise, such as noiseless bandits, the DEC typically exhibits threshold
behavior, with
r-decc
"(M)/I
"1=p
Cprob	
;
whereCprobis a problem-dependent parameter. For example, if Mconsists of multi-armed bandit
instances with  =f1;:::;Agand noiseless, binary rewards, one can take Cprob/A. For such settings,
the condition (15) is satisﬁed whenever T=eO(Cprob), and Theorem 2.2 gives
E[RegDM(T)]e
 
minfCprob;Tg
:
For PAC guarantees, the situation is similar to regret, but the lower bounds are scaled by a 1=Tfactor due
to normalization. Brieﬂy:
•Whenever p-decc
"(M)/"p
Cprob, Theorem 2.1 gives E[Risk DM(T)]e
q
Cprob
T
, which implies
thate

Cprob
"2
samples are required to learn an "-optimal policy.
•Whenever p-decc
"(M)/"1 for2(0;1), Theorem 2.1 gives E[Risk DM(T)]e
(T (1 )
2), which
implies thate
(" 2
1 )samples are required to learn a "-optimal policy.
•Whenever p-decc
"(M)/If"1=p
Cprobg, whereCprobis a problem-dependent parameter, Theorem 2.1
givesE[Risk DM(T)]
(1)untilT=e
(Cprob); that is, at least e
(Cprob)samples are required to learn
beyond constant suboptimality.
We refer to Section 6 for further examples and details.
Proof techniques: PAC. We now highlight some of the key ideas behind the proof of Theorem 2.1. The
high-level structure of the proof is as follows. For any algorithm, one can construct a “hard” pair of models
M1;M22Msuch that:
1.The joint laws of the decisions tand observations (rt;ot)induced by the algorithm under M1andM2
are close in total variation (i.e., DTV(PM1;PM2)1=4, where PMdenotes the law of HTunderM)
2.Any algorithm with risk much smaller than the DEC must query substantially diﬀerent decisions in 
depending on whether the underlying model is M1orM2.
SincePM1arePM2close enough (in total variation) that the algorithm will fail to distinguish the models with
constant probability, and since the optimal decisions for the models are (approximately) exclusive, the lower
bound follows.
We select the pair of models (M1;M2)in anadversarial fashion based on the algorithm under consideration,
in a way that generalizes the approach taken in Foster et al. (2021, 2022b). We ﬁx an arbitrary model
M12M, then, letting qM1:=EM1h
1
TPT
t=1qt(jHt 1)i
andpM1:=EM1[p(jHT)]denote the learner’s
average play under this model, choose M2as the model that attains the maximum in Eq. (11) with
(pM1;qM1)plugged in. This approach suﬃces to prove lower bounds that scale with supM2Mp-decc
"(M;M),
but is not suﬃcient to incorporate improper reference models and prove a lower bound that scales with
9p-decc
"(M) =supM2co(M)p-decc
"(M[fMg;M). Indeed, unless the class Mis convex, there is no reason
why an algorithm with low risk for models in Mshould have low risk for improper mixturesM2co(M).
Thus, naively choosing M1=M2co(M)is problematic, as we have no way to relate the algorithm’s risk
underM1to that for models in the class.
To circumvent this issue, we iterate the process above: Given an arbitrary, potentially improper reference
modelM2M+, we ﬁrst obtain M1by ﬁnding the model that attains the maximum in Eq. (11) with (pM;qM)
plugged in. With this model in hand, we obtain M2in a similar fashion, but condition on the event the
learner behaves near-optimally for M1. That is, we ﬁnd the maximizer for Eq. (11) with the distribution
(pM(jE1);qM)plugged in, where E1is the set of near-optimal decisions for M1. This argument leads to lower
bounds that scale with supM2M+p-decc
"(M;M)p-decc
"(M)because the reference model Macts only as a
midpoint between M1andM2(whose existence allows us to control the total variation between the models),
and as a result is not required to live in M.
Proof techniques: Regret. The proof of our lower bound for regret (Theorem 2.2) follows a similar
approach to Theorem 2.1. However, non-trivial diﬃculties arise in applying the iterative conditioning scheme
in the preceding discussion because there are no longer separate distributions for exploration ( qM) and
exploitation ( pM), causing the analysis of regret and information to be coupled. To address this, we adopt a
somewhat diﬀerent two-part scheme.
1.First, weprovealowerboundthatissimilartoTheorem2.2, butqualitativelyweakerinthesensethatthe
quantity r-decc
"(T)(M) =supM2M+r-decc
"(T)(M;M)inEq. (16)isreplacedby supM2Mr-decc
"(T)(M;M);
that is, the lower bound restricts to proper reference models. This is proven using a similar approach to
our lower bound for PAC.
2.Then, we upgrade this weaker result to the full claim of Theorem 2.2, using the following algorithmic
result: for any model class Mand anyM2M+(not necessarily in M) satisfying mild technical
conditions, if there exists an algorithm that achieves expected regret at most Rwith respect to the
classM, then there exists an algorithm that achieves regret at most O(RlogT)with respect to the
enlarged classM[fMg. By then choosing Mappropriately and applying the proper lower bound from
Part 1 to a slightly modiﬁed version of the class M[fMg, we are able to establish Theorem 2.2.
See Appendix B for the proof.
Beneﬁts of the constrained DEC. The main technical advantage gained by working with the constrained
DEC over the oﬀset DEC is that, by placing a hard constraint on the Hellinger distance between models
under consideration, we can appeal to stronger change-of-measure arguments than those considered in prior
work; this is key to deriving in-expectation (as opposed to low probability) lower bounds. In particular, the
radius"(T)1=p
Tis the largest possible choice such that for any algorithm, one can ﬁnd a worst-case pair
of models for which the total variation distance DTV(PM1;PM2)is asmall constant (say, 1=4). Whenever
the total variation distance between the induced laws is constant, the algorithm must fail to distinguish M1
andM2with constant probability, which entails large regret if the optimal decisions for M1andM2are
signiﬁcantly diﬀerent.
Foster et al. (2021) emphasize that the Decision-Estimation Coeﬃcient can be thought of as interactive
counterpart to the modulus of continuity in statistical estimation (Donoho and Liu, 1987, 1991a,b). We ﬁnd
the constrained DEC to be a more direct analogue than the oﬀset DEC: The modulus of continuity places a
hard constraint on Hellinger distance for similar technical reasons, and lower bounds based on the modulus
make use of the same 1=p
Tradius.
2.2 Proof of PAC Lower Bound (Theorem 2.1)
In what follows, we prove the PAC lower bound (Theorem 2.1). The proof for regret (Theorem 2.2), which is
similar but carries some important technical diﬀerences, is deferred to Appendix B.
10Preliminaries. Formally, for T2N, analgorithm for the PAC framework is a collection of mappings
(p;q) = 
fqt(j)gT
t=1;p(j)
that (adaptively) draws decisions tqt(jHt 1)(fort2[T]), and then
outputs the ﬁnal decision bp(jHT)conditioned on the history HT. We deﬁne PM;(p;q)as the law of HT
when the underlying model is Mand the algorithm is (p;q). Throughout the proof, we will use the elementary
propertyDTV(P;Q)DH(P;Q).
Proof of Theorem 2.1. For later reference, we deﬁne a constant
c0= 1=16;
and deﬁne C(T) = 28log(T^V(M)). FixT2Nand an algorithm (p;q) =fqt(j);p(j)gT
t=1. For each
modelM2M+, we use the abbreviation PMPM;(p;q), and write EMfor the corresponding expectation. In
addition, we deﬁne
pM=EM[p(jHT)];andqM=EM"
1
TTX
t=1qt(jHt 1)#
:
Choosing a hard pair of models. Fix an arbitrary reference model M2M+and deﬁne":=1
10p
C(T)T
and"(T) :="=p
2. We will prove a lower bound in terms of p-decc
"(T)(M;M). We abbreviate :=
p-decc
"(T)(M;M), so that the assumption (13) gives 48".
To begin, choose any model M12Msatisfying:
M12
M2M :EqM
D2
H 
M();M()
"2^EpM
D2
H 
M();M()
"2	
:(17)
We will make use of the fact that, by Lemma D.3, we have that for all p2(),
dec"=p
2(M;M)
sup
M2M
Ep[gM()]jEqM
D2
H 
M();M()
"2^Ep
D2
H 
M();M()
"2	
;(18)
which implies (along with Eq. (13)) that the set in Eq. (17) is non-empty. For any model M2M+, deﬁne
EM:=
2 :gM()c0	
;
and deﬁneA1:=EM1. Letp0:=pM(jAc
1), and set
M2:= arg max
M2M
Ep0[gM()]jEqM
D2
H 
M();M()
"2^Ep0
D2
H 
M();M()
"2	
;
(19)
as withM1, Lemma D.3 implies that this set is non-empty. Finally, deﬁne A2:=EM2\Ac
1.
Lower bounding the algorithm’s risk. We now recall Lemma A.13 from Foster et al. (2021), which
states that for all models M,
D2
H 
PM;PM
C(T)TEqM
D2
H 
M();M()
: (20)
By the data processing inequality, this further implies that
D2
TV(pM;pM)C(T)TEqM
D2
H 
M();M()
: (21)
SinceEqM
D2
H 
Mi();M()
"2fori2f1;2g, our choice "1
10p
TC(T)implies that
DTV 
pMi;pM
1
10;fori2f1;2g:
11As a result, for each i2f1;2g, we have
EpMi[gMi()]c0pMi(2EMi) (22)
c0 
pM(2EMi) DTV 
pM;pMi
c0(pM(2EMi) 1=10): (23)
Thus, to prove the theorem, it suﬃces to lower bound pM(2EMi)by1=4for at least one of i2f1;2g, which
will show that the quantity in Eq. (23) is at least3c0
20(in fact, any constant lower bound greater than 1=10
suﬃces).
We assume henceforth that pM(Ac
1)1=2, as otherwise we have pM(EM1) =pM(A1)1=2, in which case the
result immediately follows from Eq. (23). Before continuing, we note that since EpM
D2
H 
M1();M()

"2and
EpM
1f2Ac
1gD2
H 
M2();M()
Ep0
D2
H 
M2();M()
"2;
the triangle inequality for Hellinger distance implies that EpM
1f2Ac
1gD2
H(M1();M2())
4"2:
Hence, by Eq. (D.1) and Jensen’s inequality, we have
EpM[ 1f2Ac
1gjfM1() fM2()j]q
EpM[ 1f2Ac
1gD2
H(M1();M2())]2":(24)
Lower bounding the gap. To proceed, we will ﬁrst establish that
fM2(M2)fM1(M1) +
2(1 4c0) 2": (25)
To do this, we note that from the deﬁnition of M2,
EpM[ 1f2Ac
1ggM2()]pM(Ac
1)Ep0[gM2()]1
2p-decc
"=p
2(M;M) =
2;
where we have used the assumption that pM(Ac
1)1
2. Thus,
EpM[ 1f2A 2ggM2()] =EpM[ 1f2Ac
1ggM2()] EpM[ 1f2Ac
1\(EM2)cggM2()]
EpM[ 1f2Ac
1ggM2()] c0
2(1 2c0); (26)
where the ﬁrst inequality follows because gM2()<c0for2(EM2)c.
Next, we compute
c0EpM[ 1f2A 2g(fM1(M1) fM1())]
=EpM[ 1f2A 2g(fM2(M2) fM1())] +pM(A2)(fM1(M1) fM2(M2))
EpM[ 1f2A 2g(fM2(M2) fM2())] 2"+pM(A2)(fM1(M1) fM2(M2))

2(1 2c0) 2"+pM(A2)(fM1(M1) fM2(M2));
where the ﬁrst inequality follows because gM1()< c0for2A 2Ac
1= (EM1)c, the second-to-last
inequality follows from Eq. (24) and the fact that A2Ac
1, and the ﬁnal inequality follows by Eq. (26).
Rearranging and using that pM(A2)2[0;1], we obtain
fM2(M2) fM1(M1)pM(A2)(fM2(M2) fM1(M1))

2(1 4c0) 2":
12Lower bounding the failure probability and concluding. To ﬁnish the proof, we use the inequality
(25) to bound the probability pM((EM2)c\Ac
1)as follows:
pM((EM2)c\Ac
1)
2(1 4c0) 2"
EpM[ 1f(EM2)c\Ac
1g(fM2(M2) fM1(M1))]
EpM[ 1f(EM2)c\Ac
1g((fM2(M2) fM2()) (fM1(M1) fM1()))] + 2
EpM[ 1f(EM2)c\Ac
1ggM2()] + 2"
c0+ 2";
where the ﬁrst inequality uses Eq. (25), the second inequality uses Eq. (24) and the fact that (EM2)c\Ac
1Ac
1,
the third inequality uses that gM10, and the ﬁnal inequality uses the deﬁnition of EM2. Since we have
assumed that "
48andc01=16, it follows that pM((EM2)c\Ac
1)=8
=4= 1=2. Thus, we have established
that
pM(A1) +pM(EM2)pM(A1[EM2)1=2;
which implies that either pM(EM1) =pM(A1)1=4orpM(EM2)1=4. As a consequence, by Eq. (23), we
have that for some i2f1;2g,EpMi[gMi()]3c0
20, thus establishing the desired lower bound.
To wrap up, we note that the lower bound we have established holds for an arbitrary reference model
M2M+, so we are free to choose M2M+to maximize p-decc
"=p
2(M;M).
Remark 2.3. The structure of the proof Theorem 2.1 bears some superﬁcial similarities to that of the
classical two-point method (Donoho and Liu, 1987, 1991a,b; Yu, 1997; Tsybakov, 2008) in statistics and
information theory, but has a number of fundamental diﬀerences.
1.First, in our lower bound, the pair of models (M1;M2)is chosen in an adversarial fashion based on the
algorithm under consideration, while the classical approach selects a pair of models obliviously. When
considering only two models, choosing the models adversarially is critical to capture the complexity of
classesMthat require distinguishing between many distinct decisions. For example, even in the simple
special case of multi-armed bandits, this is necessary to make the number of actions Aappear in the
lower bound.
2.Second, and perhaps more importantly, the classical two-point argument cannot be directly applied
because the function gM()does not enjoy the metric structure required by this approach. In particular,
the classical “separation condition”, which takes the form gM1() +gM2()8when applied to our
setting, does not hold. Instead, the crux of the proof is to show that, as a consequence of our choice for
M1andM2, we have
EpM1[gM1()] +EpM2[gM2()]&:
To establish this inequality, we take advantage of the fact that 1) p-decc
"(T)(M)&"(T), by assumption,
and 2) rewards rare observed and lie in the range [0;1]; the latter implies that jfM() fM()j
DH 
M();M()
for allM;M2M+.
3 Upper Bounds
We now give algorithms that obtain upper bounds on sample complexity that complement the lower bounds
in Section 2. First, in Section 3.2 we give an algorithm and upper bound for the PAC framework. We
then prove this result in Section 3.3, highlighting the key algorithm design principles and analysis ideas.
Then, in Section 3.4, we give an algorithm and upper bound for regret. Both of our algorithms use the
Estimation-to-Decisions paradigm of Foster et al. (2021). Our algorithm for regret builds upon our algorithm
for PAC, but is more technically involved.
133.1 Online Estimation
Our algorithms and regret bounds use the primitive of an online estimation oracle , denoted by AlgEst, which is
an algorithm that, given knowledge of some class Mcontaining the true model M?, estimates the underlying
modelM?from data on the ﬂy. At each round t, given the data Ht 1= (1;r1;o1);:::; (t 1;rt 1;ot 1)
observed so far an estimation oracle builds an estimate
cMt=AlgEst
f(i;ri;oi)gt 1
i=1
for the true model M?. We measure the oracle’s estimation performance in terms of cumulative Hellinger
error, which we assume is bounded as follows.
Assumption 3.1 (Estimation oracle for M).At each time t2[T], an online estimation oracle AlgEstfor
Mreturns, given
Ht 1= (1;r1;o1);:::; (t 1;rt 1;ot 1)
with (ri;oi)M?(i)andipi, an estimator cMt2co(M)such that whenever M?2M,
Est H(T):=TX
t=1Etpth
D2
H
M?(t);cMt(t)i
Est H(T;); (27)
with probability at least 1 , where Est H(T;)is a known upper bound.
Algorithms satisfying Assumption 3.1 can be obtained via online conditional density estimation (that is,
online learning with the logarithmic loss). Typically, the best possible estimation rate Est H(T;)will reﬂect
the statistical capacity of the class M. Standard examples include ﬁnite classes, where the exponential
weights algorithm (also known as Vovk’s aggregating algorithm) achieves Est H(T;)O(log(jMj=)), and
parametric classes in Rd, where one can achieve Est H(T;)eO(d). See Section 4 of Foster et al. (2021) for
further background.
3.2 Algorithm and Upper Bound for PAC
Algorithm 1 displays our main algorithm for the PAC framework, E2D+. The algorithm is built upon the
Estimation-to-Decisions paradigm of Foster et al. (2021), which uses the following scheme for each round t:
•Obtain an estimator cMt2co(M)forM?from the online estimation oracle AlgEst.
•Sampletfrom a decision distribution obtained by solving the min-max optimization problem that
deﬁnes the DEC, with cMtplugged in as the reference model.
E2D+follows this template, but incorporates non-trivial changes that are tailored to i) the constrained (as
opposed to oﬀset) DEC and ii) PAC guarantees (as opposed to regret). Brieﬂy, the algorithm consists of two
phases, an exploration phase and anexploitation phase , which we outline below.
Exploration phase. In theexploration phase , which consists of rounds t= 1;:::;J, whereJ=e
(T),
Algorithm 1 repeatedly obtains an estimator cMtby querying the estimation oracle AlgEstwith the current
dataset Ht 1= (1;r1;o1);:::; (t 1;rt 1;ot 1)(Line 5), then computes the pair of distributions (pt;qt)that
solve the min-max problem that deﬁnes the PAC DEC (Eq. (11)) with cMtplugged in as the reference model
(Line 6):
(pt;qt) := arg min
p;q2()sup
M2Hq;"(T)(cMt)Ep[fM(M) fM()]: (28)
By deﬁnition, the value above is always bounded by p-decc
"(T)(M;cMt). Recall that ptmay be though of
as anexploitation distribution , and that qtmay be thought of as an exploration distribution . With these
distributions in hand, Algorithm 1 samples tqtfrom the exploration distribution qt. The distribution pt
is not used in this phase, but is retained for the exploitation phase that follows.
14Algorithm 1 Estimation-to-Decisions ( E2D+) for PAC
1:parameters :
Number of rounds T2N.
Failure probability >0.
Online estimation oracle AlgEst.
2:DeﬁneL:=dlog 2=e,J:=T
L+1, and Est H:=Est H
2T
dlog 2=e;
4dlog 2=e
.
3:Set"(T):= 8q
dlog 2=e
TEst H.
/* Exploration phase */
4:fort= 1;2;;Jdo
5:Obtain estimate cMt=AlgEst
f(i;ri;oi)gt 1
i=1
.
6:Compute
(pt;qt) := arg min
p;q2()sup
M2Hq;"(T)(cMt)Ep[fM(M) fM()];
with the convention that the value is zero if Hq;"(T)(cMt) =?.
7:Sample decision tqtand update estimation oracle AlgEstwith (t;rt;ot).
/* Exploitation phase */
8:SampleLindicest1;:::;tLUnif([J])independently.
9:For each`2[L], drawJindependent samples 1
`;:::;J
`qt`, and observe (j
`;rj
`;oj
`)for eachj2[J].
10:For each`2[L]andj2[J], compute
fMj
`:=AlgEst
f(i
`;ri
`;oi
`)gj 1
i=1
;
and letfM`:=1
JPJ
j=1fMj
`. //fM`is a high-quality estimate for M?underqt`.
11:output: Setbp:=ptb`and outputbbp, whereb`:= arg min`2[L]Eqt`
D2
H cMt`();fM`()
.
Exploitation phase. In theexploitation phase (or, post-processing phase), we aim to identify an exploita-
tion distribution pt2fp1;:::;pJgfrom the collection computed during the exploration phase that is “good”
in the sense that it has suﬃciently low suboptimality under M?. The motivation behind this stage is as
follows. From the expression (28) and the deﬁnition of the constrained DEC, we are guaranteed that pthas
Ept
fM?(M?) fM?(t)
p-decc
"(T)(M;cMt)
for any round t2[J]whereM?2Hqt;"(T)(cMt). The challenge here is that the estimation oracle ensures only
that the cumulative estimation error for the estimators cM1;:::;cMJis low. There is no guarantee that the
per-round estimation error Etqth
D2
H
M?(t);cMt(t)i
will decrease with time, and for any ﬁxed round t
of interest, this quantity might be trivially large. This can lead to a problem we term false exclusion , where
M?=2Hqt;"(T)(cMt). False exclusion is problematic because we have no control over the suboptimality under
M?for rounds t2[J]where it occurs. The good news is that while the online estimation oracle may lead
to false exclusion for some rounds, Markov’s inequality implies that (for our choice of "(T), which depends
onEst H(T;)) the true model M?will be included in Hqt;"(T)(cMt)forat least half of the rounds t2[J].
Hence, the exploitation phase proceeds by sampling (on Line 8) a small number of rounds t1;:::;tL2[J]—a
logarithmic number suﬃces to ensure that at least one is good with high probability—and performing a test
to identify a good distribution ptb`within the setfpt1;:::;ptLg, which is then returned by the algorithm.
In more detail, the exploitation phase (Line 8 through Line 11) proceeds by gathering many (namely,
15(T=logT)) samples from qt`for each of the Lroundsft`g`2[L]and then, for each `2[L], using the
estimation oracle AlgEstto produce an estimated model fM`2Mbased on these samples. Since many
samples are used to produce the estimate fM`, it is guaranteed to be close to the true model M?underqt`.
This means that by choosing a round tb`that minimizes the Hellinger distance between cMtb`andfMb`(Line 11),
we have that with high probability, M?2Hqtb`;"(cMtb`), thus solving the false exclusion problem, and ensuring
that the exploitation distribution ptb`has low risk.
Main result. We show that E2D+enjoys the following guarantee for PAC.
Theorem 3.1 (Main Upper Bound: PAC) .Fix2 
0;1
10
andT2N. Suppose that Assumptions 1.1 and 3.1
hold, and let Est H:=Est H
2T
dlog 2=e;
4dlog 2=e
. With"(T):= 8q
dlog 2=e
TEst H, Algorithm 1 guarantees
that with probability at least 1 ,
Risk DM(T)p-decc
"(T)(M):
Thus, the expected risk achieved by Algorithm 1 is bounded by
E[Risk DM(T)]p-decc
"(T)(M) +: (29)
This result matches the lower bound in Theorem 2.1, with the only gap being the choice of radius ">0for the
constrained DEC: The lower bound (Theorem 2.1) has "(T)/q
1
T, while the upper bound (Theorem 3.1) has
"(T)/q
log(2=)Est H
T. As discussed in Foster et al. (2021), understanding when the estimation complexity
Est Hcan be removed or weakened is subtle issue, as there are some classes Mfor which this term is necessary,
and others for which it is superﬂuous. This is the main question left open by our research.
Let us instantiate Theorem 3.1 for some standard examples, focusing on the special case where Mis ﬁnite
andEst H(T;)log(jMj=)for simplicity.
•Whenever p-decc
"(M)/"p
Cprob, Theorem 3.1 gives E[Risk DM(T)]eOq
CproblogjMj
T
, which
translates into eO
CproblogjMj
"2
samples to learn an "-optimal policy.
•Whenever p-decc
"(M)/"1 for2(0;1), Theorem 3.1 gives E[Risk DM(T)]eO((logjMj=T)(1 )
2),
which translates into eO(logjMj" 2
1 )samples to learn a "-optimal policy.
•Whenever p-decc
"(M)/If"1=p
Cprobg, whereCprobis a problem-dependent parameter, Theorem 3.1
givesRisk DM(T) = 0with high probability whenever Te
(CproblogjMj).
3.3 Proof of PAC Upper Bound (Theorem 3.1)
We now prove Theorem 3.1. Before proceeding with the proof, we give some brief background on the notion
of online-to-batch conversion, which is used in the exploitation phase and its analysis.
Background: Online-to-batch conversion. As discussed in the prequel, we assume access to an online
oracle AlgEst. We use the onlineguarantee the oracle the algorithm provides—namely, that it ensures that
the cumulative estimation error is bounded for an adaptively chosen sequence of decisions—in a non-trivial
fashion during the exploration phase, but our analysis additionally makes use the fact that online oracles can
be used to provide guarantees for oﬄineestimation.
For oﬄine estimation, we consider a setting in which there is some p2()so thatpt=pfor allt, and the
algorithm must output a single model estimate cMsuch that Eph
D2
H
cM();M?()i
"2. We can obtain
such a guarantee using an online estimation oracle via the following online-to-batch conversion process:
16•For eacht= 1;:::;T, obtaincMtby running AlgEston samplesf(i;ri;oi)gt 1
t=1, wheretpand
(rt;ot)M?(t).
•LetcM:=1
TPT
t=1cMt2co(M).
It is evident from Assumption 3.1 and convexity of the squared Hellinger distance that with probability at
least 1 , the estimator cMconstructed above satisﬁes
Eph
D2
H
M?();cM()i
=Ep"
D2
H 
M?();1
TTX
t=1cMt()!#
1
TTX
t=1Eph
D2
H
M?();cMt()i
Est H(T;)
T: (30)
This is the strategy employed in Line 10 of Algorithm 1. Standard oﬄine estimation algorithms (e.g., MLE)
can be used to derive similar guarantees, but we make use of online-to-batch in order to keep notation light,
since Algorithm 1 already requires the online estimation algorithm for the exploration phase.
Proof of Theorem 3.1. We begin by analyzing the exploitation phase. e Recall that we set J:=
T
dlog 2=e+1T
2L. By Assumption 3.1, we have that with probability at least 1 
4L,
JX
t=1Etqth
D2
H
M?(t);cMt(t)i
Est H
J;
4L
Est H:
We denote this event by E0, and condition on it going forward. Since we have "(T)232
JEst Hby deﬁnition,
it follows from Markov’s inequality that if s2[J]is chosen uniformly at random, then with probability at
least 1=2,
Esqsh
D2
H
M?(s);cMs(s)i
"(T)2
16: (31)
Going forward, our aim is to show that the exploitation phase identiﬁes such an index s2[J]. Indeed, for
anys2[J]such that the inequality (31) holds, we have M?2Hqs;"(T)(cMs), and hence
Eps
fM?(M?) fM?()
p-decc
"(T)(M;cMs)p-decc
"(T)(M)
from the expression (28) and the deﬁnition of the constrained DEC.
To proceed, ﬁrst observe that for the uniformly sampled indices t1;:::;tL2[J], a standard conﬁdence
boosting argument implies that with probability at least 1 2 L1 
2, there is some `2[L]so that Eq.
(31) is satisﬁed with s=t`. We denote this event by F.
Next, recall the deﬁnition fM`=1
JPJ
j=1fMj
`in Eq. (10) of Algorithm 1. Using Assumption 3.1 together with
Eq. (30), we have that for each `2[L], there is an event that occurs with probability at least 1 
4L, denoted
byE`, such that under E`we have
Eqt`h
D2
H
M?();fM`()i
Est H(J;=4L)
JEst H
J"(T)2
32;
where the second inequality uses our choice for "(T). Deﬁne E:=TL
`=0E`, so that Eoccurs with probability
at least 1 (L+1)
4L1 
2. We deﬁne E=F\TL
`=0E`, so that Eoccurs with probability at least
1 (L+1)
4L 
21 .
We now show that the exploitation phase succeeds whenever the event Eholds. By the triangle inequality for
Hellinger distance, letting `2[L]be any index such that Eq. (31) is satisﬁed with s=t`, we have
Eqt`h
D2
H
fM`();cMt`()i
2
Eqt`h
D2
H
M?();fM`()
+D2
H
M?();cMt`()i
"(T)2
4:
17From the deﬁnition on Line 11, the index b`2[L]satisﬁes
Eqtb`h
D2
H
fMb`();cMtb`()i
Eqt`h
D2
H
fM`();cMt`()i
"(T)2
4:
Using the triangle inequality for Hellinger distance once more, we obtain that under the event E,
Eqtb`h
D2
H
cMtb`();M?()i
2"(T)2
4+"(T)2
32
<"(T)2;
which means that M?2Hqtb`;"(T)(cMtb`). It follows that whenever Eholds,
Risk DM(T) =Eptb`
fM?(M?) fM?()
 sup
M2H
qtb`;"(T)(cMtb`)Eptb`[fM(M) fM()]
= inf
p;q2()sup
M2Hq;"(T)(cMtb`)Ep[fM(M) fM()]
=p-decc
"(M;cMtb`)
 sup
M2co(M)p-decc
"(T)(M;M) =p-decc
"(T)(M);
where the ﬁrst equality follows from the choice of ptb`;qtb`in Line 6. The conclusion in Eq. (29) of the theorem
statement follows from the observation that Risk DM(T)is bounded above by 1, as we assume that rewards
lie in [0;1].
3.4 Algorithm and Upper Bound for Regret
We now present a variant of E2D+for the regret framework, Algorithm 2, which attains a regret bound that
scales with r-decc
"(T)(M)Tfor an appropriate radius "(T)>0.
Onlineestimation. Algorithm2makesuseofanonlineestimationoracleinthesamefashionasAlgorithm1,
but we require a slightly stronger oracle capable of incorporating constraints on the model class, speciﬁed via
a subsetM0M.
Assumption 3.2 (Constrained estimation oracle for M).A constrained estimation oracle for Mtakes as
input a constraint set M0M. At each time t2[T], the online estimation oracle AlgEstreturns, given
Ht 1= (1;r1;o1);:::; (t 1;rt 1;ot 1)
with (ri;oi)M?(i)andipi, an estimator cMt2co(M0)such that whenever M?2M0,
Est H(T):=TX
t=1Etpth
D2
H
M?(t);cMt(t)i
Est H(T;); (32)
with probability at least 1 , where Est H(T;)is a known upper bound.
This assumption is identical to Assumption 3.1, except that i) the oracle takes a constraint setM0Mas
an input before the learning process begins, and ii) the oracle is required to produce cMt2co(M0); that is,
the estimator is required to lie in the convex hull of the constraint set M0. All estimation algorithms that we
are aware of can achieve this guarantee with minor or no modiﬁcations, including the exponential weights
algorithm, which satisﬁes Assumption 3.2 with Est H(T;)O(log(jMj=)).
3Algorithm 2 will continue to work if we modify it to break out of the loop over s2Siwhen this if statement is reached,
which is somewhat more natural. We use the version presented here because it slightly simpliﬁes the proof.
18Algorithm 2 Estimation-to-Decisions ( E2D+) for Regret
1:parameters :
Number of rounds T2Nand error probability 2(0;1).
Failure probability >0.
Online estimation oracle AlgEst.
ConstantsC1>1.// Specified in Appendix C.
2:DeﬁneN:=dlogTeandL:=dlog 1=e.
3:Set"N:=q
C1Est H(T;)L
Teach"i:=p
2N i"Nfori2[N].
4:Split [T]into2Ncontiguous blocks E1[R 1[[EN[RN, wherejRij;jEij2h
2i
4;2i
2i
for alli2[N].
5:SetM1:=M.
6:fori2[N]do
/* Exploration for epoch i*/
7:Initialize an instance of AlgEst, with time horizon jEij, failure probability , and model class Mi.
8:fort2Eido
9: Obtain estimate cMt:=AlgEst(f(s;rs;os)gs2Ei;s<t), wherecMt2co(Mi).
10: Compute
pt:= arg min
p2()sup
M2Hp;"i(cMt)[fcMtgEp[fM(M) fM()]:
11: Sample decision tptand update estimation oracle AlgEstwith (t;rt;ot).
/* Refinement for epoch i*/
12:Sample a subsetSiEiof sizeLuniformly at random (with replacement).
13:Initializestmp
i=?.// With high probability, stmp
iwill be updated in the for loop.
14:fors2Sido
15: Initialize an instance of AlgEstwith horizon Ji:=jRij=L, failure probability , and model class Mi.
16: for1jJido
17: Sample a decision j
spsand update AlgEstwith (j
s;rj
s;oj
s).
18: Obtain estimate fMj
s:=AlgEst
f(k
s;rk
s;ok
s)j 1
k=1g
, wherefMj
s2co(Mi).
19: ifPj
k=1Epsh
D2
H
cMs();fMk
s()i
>Ji"2
i
4then
20: DeﬁneJi;s:=j, andbreakout of the loop over j(i.e., proceed to the next value in Si).
21: ifj=Jithen
22: Setstmp
i=sandJi;s=Ji.3
23:Setsi=stmp
iifstmp
i6=?, else choose si2Siarbitrarily. Set cMi:=1
Ji;sPJi;s
j=1fMj
siandbpi=psi.
24:forany remaining rounds tinRido
25: Playtpsi(the rewards and observations can be ignored).
26:Set
Mi+1:=
M2MjEbpih
D2
H
M();cMi()i
Est H(Ji;)
Ji
:
19Overview of algorithm. Algorithm 2 employs the Estimation-to-Decisions principle of Foster et al. (2021)
but, like Algorithm 1, incorporates substantial modiﬁcations tailored to the constrained (as opposed to oﬀset)
DEC. The core of the algorithm is Line 10, which—at each round t—obtains an estimator cMt2co(M), then
computes an exploratory distribution ptby solving the min-max problem that deﬁnes the regret DEC (Eq.
(7)) withcMtplugged in:
pt:= arg min
p2()sup
M2Hp;"(cMt)[fcMtgEp[fM(M) fM()]; (33)
for an appropriate choice of ">0that depends on t.
As with Algorithm 1, the main challenge Algorithm 2 needs to overcome is false exclusion : Whenever
M?2Hpt;"(cMt), it is immediate from the deﬁnition above that
Etpt
fM?(M?) fM?(t)
r-decc
"(M[fcMtg;cMt);
but what happens if M?=2Hpt;"(cMt)? For PAC, the solution employed by Algorithm 1 is simple: The online
estimation guarantee for AlgEstensures that M?is correctly included for at least half the rounds, and we
only need to identify a single round where it is included. For regret, this reasoning no longer suﬃces: We
cannot simply ignore the rounds in which M?is excluded, as the regret for these rounds must be controlled.
Epoch scheme. To address the issue of false exclusion, Algorithm 2 breaks the rounds 1;:::;Tinto epochs
1;:::;Nof doubling length, with each epoch iconsisting of a contiguous set of exploration rounds Ei[T]and
reﬁnement rounds Ri[T]. Each epoch proceeds in a similar fashion to Algorithm 1, but takes advantage of
the data collected in previous epochs to explore in a fashion that is robust to false exclusions:
•Each exploration phase gathers data using a sequence of exploratory distributions fptgt2Eicomputed
by solving the DEC with the estimated models fcMtgt2Ei, following Eq. (33). However, the estimated
modelscMtare restricted to lie in co(Mi), whereMiis a conﬁdence set computed using data from the
previous epoch.
•The purpose of the reﬁnement phase in epoch iis to use the distributions generated in the exploration
phase to compute a conﬁdence set Mi+1for thenext epoch that satisﬁes a certain invariant that allows
us to translate low regret with respect to models in the conﬁdence set to low regret under M?.
To describe the phases for an epoch i2[N]in greater detail, we deﬁne
i:=C0r-decc
"i(M) + 64"i;
for a constant C0>1whose value is speciﬁed in the full proof (Appendix C).
Exploration phase. For each step twithin the exploration phase Ei, we obtain an estimator cMt2co(Mi)
from the estimation oracle and solve
pt:= arg min
p2()sup
M2Hp;"i(cMt)[fcMtgEp[fM(M) fM()]: (34)
whereMiMis the conﬁdence set produced by the reﬁnement phase in epoch i 1. The setMiis
constructed so that M?2Miwith high probability, but in addition, the following localization property can
be shown (using that cMt2co(Mi)for allt2Ei; see the proof of Lemma C.3):
fM?(M?)fcMt(cMt) +i 1
28t2Ei: (35)
The condition (35) implies that we are always in a favorable position with respect to regret:
•IfM?2Hpt;"i(cMt)—thatis, thetruemodelisnotfalselyexcluded—wehave Ept[fM?(M?) fM?()]
r-decc
"i(M)iby deﬁnition.
20•Even ifM?is falsely excluded, Eq. (35) implies that up to certain nuisance terms, we have (as shown
in Lemma C.4):
Ept
fM?(M?) fM?()
.Ept
fcMt(cMt) fcMt()
.i 1;
where the latter inequality uses the deﬁnition (34), which implies that
Ept
fcMt(cMt) fcMt()
r-decc
"i(M[fcMtg;cMt)i:
In both situations, we incur no more than O(i)regret per round. Due to the doubling epoch schedule, the
total contribution to regret for all exploration rounds is no more than eO 
r-decc
"(T)(M)T
.
Reﬁnement phase. For the analysis of the exploration phase in epoch i+ 1to succeed, the reﬁnement
phase at epoch imust construct a conﬁdence set Mi+1so that the localization property (35) is satisﬁed with
scalei. To achieve this, we use a similar approach to the exploitation phase in Algorithm 1. For all the
roundst2Eifor whichM?is not falsely excluded, we are guaranteed that i) M?2Hpt;"i(cMt), and ii) all
M2Hpt;"i(cMt)satisfy
Ept
fM(M) fM()
r-decc
"i(M)i
4;
which can be shown to imply the localization property in Eq. (35) holds at epoch i(the proof of this fact
uses thatcMt2co(Mi+1)for allt2Ei+1). In addition, the estimation guarantee for the oracle (Assumption
3.2) implies that M?is included inHpt;"i(cMt)for at least half of the rounds t2Ei. Hence, to construct
Mi+1, we sample a small number of rounds t1;:::;tL2Ei(Line 12; a logarithmic number Lsuﬃces) and
perform a test based on Hellinger distance to identify a “good” distribution pt`from the setfpt1;:::;ptLg
with the property that M?2Hpt`;"i(cMt`). We then setMi+1=Hpt`;"i(cMt`), ensuring that M?2Mi+1
and the localization property is satisﬁed.
Main result. We now present the main regret guarantee for E2D+(Algorithm 2). To state the result in
the simplest form possible, we assume that the regret DEC satisﬁes a mild growth condition.
Assumption 3.3 (Regularity of DEC) .The classMsatisﬁes the following: for some constant Creg>1and
all"2(0;2), we have
r-decc
"(M)C2
regr-decc
"=Creg(M):
This condition asserts that the DEC does not shrink too quickly as a function of the parameter ". It is
automatically satisﬁed whenever r-decc
"(M)/"for2, with1corresponding to the “p
T-regret or
greater” regime; regret bounds that hold under more general assumptions are given in Appendix C.
Theorem 3.2 (Main Upper Bound: Regret) .FixT2Nand2(0;1). Suppose that Assumptions 1.1, 3.2
and 3.3 hold, and let Est H:=Est H(T;2). Then Algorithm 2, with C1>0chosen appropriately, ensures
that with probability at least 1 ,
RegDM(T)r-decc
"(T)(M)O(Tlog(T)) +Oq
Tlog(1=)Est H
;
where"(T):=Cq
Est Hlog(1=)
Tfor a numerical constant C > 0.
The proof of this result is deferred to Appendix C. As in the PAC setting, this upper bound matches
the corresponding lower bound (Theorem 2.2) up to the radius for the constrained DEC ( "(T)q
1
Tfor
Theorem 2.2 versus "(T)q
Est H
Tfor Theorem 3.2), and cannot be improved beyond logarithmic factors
without further assumptions.
21Remark 3.1 (Relaxing the regularity condition) .It follows immediately from the proof of Theorem 3.2 that
the following holds. Suppose that in place of Assumption 3.3, we assume that there is some function r(")so
that, for all "2(0;2), we have: (1) r-decc
"(M)r(")and (2) r(")C2
regr("=Creg). Then the regret bound
of Theorem 3.2 holds with r-decc
"(T)(M)replaced by r("(T)).
Examples of Theorem 3.2, under the assumption that Mis ﬁnite and Est H(T;)O 
log(jMj=)
, include:
•Whenever r-decc
"(M)/"p
Cprob, Theorem 3.2 gives E[RegDM(T)]eO p
CprobTlogjMj
.
•Whenever r-decc
"(M)/"1 for2(0;1), Theorem 3.2 gives E[RegDM(T)]eO 
T(1+)
2log1 
2jMj+
T1
2log1
2jMj
.
4 Decision-Estimation Coeﬃcient: Structural Properties
The lower and upper bounds in Sections 2 and 3, which are stated in terms of the constrained Decision-
Estimation Coeﬃcient, are tight up to dependence on the model estimation error Est H(T;)(recall that the
lower bounds use scale "(T) =e
 p
1=T
, while the upper bounds use scale "(T) =eO p
Est H(T;)=T
). It
is natural to ask how these results are related to the lower and upper bounds in Foster et al. (2021, 2022b),
which are stated in terms of the oﬀset DEC, and at ﬁrst glance are not obviously comparable. Toward
developing such an understanding, this section establishes a number of structural properties for the DEC.
•In Section 4.1, we show that the constrained and oﬀset DEC are nearly equivalent for PAC. For regret,
we show that it is always possible to bound the constrained DEC by the oﬀset DEC in a tight fashion,
but the converse is not true in general.
•In Section 4.2, we show that the constrained DEC implicitly enforces a form of localization, and uncover
a tighter relationship between the constrained and oﬀset variants of the regret DEC for localized classes.
Sections 4.3 and 4.4 investigate the role of the reference model M2M+.
•First, in Section 4.3, we show that the deﬁnition r-decc
"(M) =supM2co(M)r-decc
"(M[fMg;M), which
incorporates suboptimality under the reference model M, is critical to obtain tight upper and lower
bounds. This is a fundamental diﬀerence from PAC, where we show that it suﬃces to use the deﬁnition
p-decc
"(M) = supM2co(M)p-decc
"(M;M), which does not incorporate suboptimality under M.
•Then, in Section 4.4, we show that allowing for arbitrary, potentially improper reference models M2M+
never increases the value of the DEC beyond what is achieved by reference models M2co(M). This
illustrates the fundamental role of convexity. In addition, we show that a similar equivalence holds for
variants of the DEC that incorporate randomized mixture estimators.
4.1 Relationship Between Constrained and Oﬀset DEC
It is immediate that one can bound the constrained DEC by the oﬀset DEC in a tight fashion. Focusing on
regret for concreteness, we can use the method of Lagrange multipliers to show that for any M2Mand
">0,
r-decc
"(M;M) = inf
p2()sup
M2M
Ep[gM()]jEp
D2
H 
M();M()
"2	
= inf
p2()sup
M2Minf
>0
Ep[gM()]  
Ep
D2
H 
M();M()
 "2	
inf
>0inf
p2()sup
M2M
Ep[gM()]  
Ep
D2
H 
M();M()
 "2	
= inf
>0
r-deco
(M;M) +"2	
: (36)
22The same approach yields an analogous inequality for PAC. For general models M =2M, the following slightly
looser version of Eq. (36) holds.4
Proposition 4.1. For allM2M+and">0, we have
r-decc
"(M[fMg;M)8inf
>0
r-deco
(M;M)_0 +"2	
+ 7": (37)
Examples include:
•Whenever r-deco
(M)/Cprob
, Proposition 4.1 yields r-decc
"(M)."p
Cprob. In this case, both our
results and the bounds in Foster et al. (2021) lead to E[RegDM(T)]eO p
CprobTEst H(T;)
.
•Moregenerally, whenever r-deco
(M)/
Cprob

forsome<1, thenProposition4.1yields r-decc
"(M).
C
1+
prob"2
1+.
In what follows we investigate when and to what extent the constrained and oﬀset variants of the DEC can
be related in the opposite direction to Eq. (36)—both for regret and PAC.
4.1.1 PAC DEC: Constrained Versus Oﬀset
For the PAC setting, the following result shows that the constrained and oﬀset DEC are equivalent up to
logarithmic factors in most parameter regimes.
Proposition 4.2. For all">0andM2M+, we have
p-decc
"(M;M)inf
0
p-deco
(M;M)_0 +"2	
: (38)
On the other hand, for all 1andM2M+, lettingL:= 2dlog 2e, we have
p-deco
(4L+1)(M;M)2
+ sup
">0
p-decc
"(M;M) "2
4
: (39)
Whenever p-decc
"(M)/"for1, one loses only logarithmic factors by passing to the oﬀset DEC using
Eq. (38) and using Eq. (39) to pass back to the constrained DEC. Yet, in the case where the constrained
DEC has “fast” behavior of the form p-decc
"(M)/Cprob"2orp-decc
"(M)/If"1=p
Cprobg, this process
is lossy (due to the1
term in Eq. (39)), and spoils the prospect of a faster-than-p
Trate. This is why we
present our results for PAC in terms of the constrained DEC, and why we use a dedicated algorithm tailored
to the constrained DEC (Algorithm 1) as opposed to a direct adaptation of the algorithm based on the oﬀset
DEC in Foster et al. (2021); taking the latter approach and combining it with Eq. (39) would not lead to fast
rates.
4.1.2 Regret DEC: Constrained Versus Oﬀset
In light of the near-equivalence between constrained and oﬀset DEC for the PAC setting, one might hope
that a similar equivalence would hold for regret. However, a naive adaptation of the techniques used to prove
Proposition 4.2 only leads to the following, quantitatively weaker converse to Proposition 4.1.
Proposition 4.3. For all >0andM2M+,
r-deco
(M;M)r-decc
 1=2(M;M): (40)
The bound on the oﬀset DEC in Eq. (40) is unsatisfying due to the scale "= 1=2on the right-hand
side. For example, in the case of the multi-armed bandit with Aactions, we have r-decc
"(M)/"p
Aand
4An inequality that replaces the right-hand side of Eq. (37) with r-deco
(M[fMg;M)follows immediately by applying Eq.
(36) to the classM[fMg. The inequality (37) is stronger, and the proof is more involved.
23r-deco
(M)/A
, yet Eq. (40) only yields the inequality r-decc
"(M).q
A
. This leads to a suboptimal
A1=3T2=3-type regret bound when plugged into the upper bounds from Foster et al. (2021) (cf. Eq. (59)).
Unfortunately, the following result shows that Proposition 4.3 is tight in general, even when M2M.
Proposition 4.4. For all1, there exists a model class MM+such that for all "2(0;1),
r-decc
"(M) = sup
M2co(M)r-decc
"(M[fMg;M)O
"21=2
;
so in particular r-decc
"(M)O(")for" 1=2. Yet, there exists M2Msuch that
r-deco
(M;M)

 1=2
:
This rules out the possibility of an inequality tighter than Eq. (40), and shows that the constrained and oﬀset
DEC have fundamentally diﬀerent behavior for regret.
4.2 Localization
For a scale parameter >0and reference model M2M+, deﬁne the following localized subclass ofM:
M(M):=fM2MjfM(M)fM(M) +g: (41)
The tightest upper and lower bounds on regret in Foster et al. (2021) are stated in terms of the oﬀset DEC
for the localized class (41), for an appropriate choice of >0that depends on T(see Section 5 for precise
statements). Our bounds based on the constrained DEC avoid the explicit use of localization, but in what
follows, we show that the constrained DEC implicitly enforces a form of localization.
Proposition 4.5 (Localization for PAC DEC) .For all" > 0andM2 M+, letting("):=p
3"+
p-deccp
6"(M;M), we have
p-decc
"(M;M)p-deccp
3"(M(")(M);M):
A similar result holds for regret, but we require that the DEC satisﬁes a slightly stronger version of the
regularity condition Assumption 3.3.
Deﬁnition 4.1 (Strong regularity of DEC) .ForM2M+, the constrained DEC is said to satisfy the strong
regularity condition relative to Mif there exist constants Cregp
2andcreg<C regsuch that for all ">0,
r-decc
Creg"(M;M)c2
regr-decc
"(M;M): (42)
The constrained DEC is said to satisfy strong regularity relative to a class M0M+if for all">0,
sup
M2M0r-decc
Creg"(M[fMg;M)c2
regsup
M2M0r-decc
"(M[fMg;M): (43)
This condition is satisﬁed with Creg= 2andcreg= 2=2whenever r-decc
"(M;M)/"for<2.
Proposition 4.6 (Localization for regret DEC) .LetM2M+be given, and assume that the strong regularity
condition (42) is satisﬁed relative to M. Then, for all ">0, letting("):=Creg"+r-decc
Creg"(M;M)
C2
reg 
"+r-decc
"(M;M)
, we have
r-decc
"(M;M)Clocr-decc
Creg"(M(")(M);M);
whereCloc:=
1
c2reg 1
C2reg 1
.
Note that in the case where r-decc
"(M;M)/"for a constant <2, choosingCreg= 2andcreg= 2=2gives
Cloc=O(1). These results show that the constrained DEC—both for PAC and regret—is equivalent (up to
constants) to the constrained DEC for the localized subclass M(M), for a radius that depends on the
value of the DEC itself. In contrast, the oﬀset DEC does not automatically enforce any form of localization,
which explains why it was necessary to explicitly restrict to a localized subclass in prior work.
244.2.1 Constrained versus Oﬀset DEC: Tighter Equivalence for Localized Classes
Building on the insights in the prequel, we now show that for localized classes, it is possible to bound the
oﬀset DEC for regret by the constrained DEC in a tighter fashion that improves upon Proposition 4.3.
Proposition 4.7. Let; > 0andM2M+be given. For all ">0, we have
r-deco
(M(M)[fMg;M)r-decc
"(M[fMg;M) + max
0; +1
2 "2
2
; (44)
which in particular yields
r-deco
(M(M)[fMg;M)r-deccp
2=(M[fMg;M) +1
2: (45)
The bound in Eq. (45) replaces the term r-deccp
1=(M)in Proposition 4.3 with r-deccp
=(M), leading to
improvement when 1. Notably, the bound is strong enough that, by combining it with Proposition 4.6,
it is possible to upper bound the constrained DEC by the localized oﬀset DEC, and then pass back to
the constrained DEC in a fashion that loses only constant factors—at least whenever r-decc
"(M)&". The
following result uses this approach to derive a near-equivalence for the constrained DEC and localized oﬀset
DEC; we also use this approach within the proof of Proposition 4.11.
Proposition 4.8. Whenever the strong regularity condition (42) in Deﬁnition 4.1 is satisﬁed for M2M+,
it holds that for all ">0, letting(";):="2,
c1sup
>c3" 1r-deco
(Mc2(";)(M);M)r-decc
"(M[fMg;M)c0
1sup
>c0
3" 1r-deco
(Mc0
2(";)(M);M) +c0
4";
(46)
wherec1;c2;c3>0are numerical constants and c0
1;c0
2;c0
3;c0
4>0are constants that depend only on Cregand
creg.
In light of this result, our upper bounds (Theorem 3.2) can be thought of as improving prior work by achieving
the tightest possible localization radius (roughly, =O("2)instead of=O("2+r-deco
(M))). Section 5
gives examples for which this leads to quantitative improvement in rate.
4.3 Reference Models: Role of Suboptimality
We now turn our attention to understanding the role of the reference model Mwith respect to which the
Decision-Estimation Coeﬃcient is deﬁned. Recall that for regret, our upper and lower bounds scale with
r-decc
"(M) = sup
M2co(M)r-decc
"(M[fMg;M) (47)
= sup
M2co(M)inf
p2()sup
M2M[fMg
Ep[fM(M) fM()]jEp
D2
H 
M();M()
"2	
:
By maximizing over M2M[fMg, this deﬁnition forces the min-player to choose p2()such that
the suboptimality Ep
fM(M) fM()
underMis small. This is somewhat counterintuitive, since
M2co(M)does not necessarily lie in the class M, yet our results show that r-decc
"(M)characterizes the
minimax regret for M. A-priori, one might expect that the quantity supM2co(M)r-decc
"(M;M), which does
not incorporate suboptimality under M, would be a more natural complexity measure. In what follows,
we show that this quantity has fundamentally diﬀerent behavior from Eq. (47), and that incorporating
suboptimality under Mis essential to characterize minimax regret.
Proposition 4.9. For any">0suﬃciently small, there exists a model class Msuch that
sup
M2co(M)r-decc
"(M;M)c"; (48)
25yet
r-decc
"(M) = sup
M2co(M)r-decc
"(M[fMg;M)c0"2=3; (49)
wherec;c0>0are numerical constants.
It is straightforward to show that for the choice "="(T)/1=p
T, the optimal regret for the class in
Proposition 4.9 is E[RegDM(T)]=e(T2=3). This result is recovered by Theorem 2.2, which scales with the
quantity in Eq. (49). However, the quantity in Eq. (48) incorrectly suggests ap
T-type rate, which is not
achievable.
For the oﬀset DEC, the role of suboptimality under Mis more subtle. It is possible to show that in general,
r-deco
(M[fMg;M)r-deco
(M;M), analogous to Proposition 4.9, but Proposition 4.1 shows that the
latter quantity suﬃces to upper bound bound r-decc
"(M[fMg;M).
While the preceding discussion shows that incorporating suboptimality under M =2Mis necessary to obtain
tight guarantees for regret, the following result shows that this distinction is largely inconsequential for PAC,
and motivates the deﬁnition p-decc
"(M) = supM2co(M)p-decc
"(M;M).
Proposition 4.10. For allM2M+and">0,
p-decc
"(M[fMg;M)p-deccp
3"(M;M) + 4": (50)
4.4 Reference Models: Role of Convexity and Randomization
We now focus on understanding the role of improper reference models M =2M. Focusing on regret, our upper
bound (Theorem 3.2) scales with
r-decc
"(T)(M) = sup
M2co(M)r-decc
"(T)(M[fMg;M); (51)
which maximizes over all possible reference models in the convex hull co(M). On the other hand, our lower
bound (Theorem 2.2) scales with
sup
M2M+r-decc
"(T)(M[fMg;M)r-decc
"(T)(M): (52)
Both results allow for improper models M =2M, but the quantity (52) allows the reference model to be
unconstrained, and could be larger than the quantity (51) a-priori. Why is there no contradiction here? In
what follows, we show that for both the constrained and oﬀset DEC, allowing for arbitrary, unconstrained
reference models as in Eq. (52) can only increase the value beyond that achieved by M2co(M)by constant
factors.
Before stating our results, let us mention a secondary, related goal, which is to understand the role of
randomized reference models . Foster et al. (2021) introduce a variant of the Decision-Estimation Coeﬃcient
tailored to randomized (or, mixture) reference models, in which Mis drawn from a distribution 2(M).
We deﬁne constrained and oﬀset variants of this complexity measure for 2(M)as follows:
r-decc;rnd
"(M;) = inf
p2()sup
M2M
Ep[fM(M) fM()]jEMEp
D2
H 
M();M()
"2	
;(53)
r-deco;rnd
(M;) = inf
p2()sup
M2MEp
fM(M) fM() EM
D2
H 
M();M()
: (54)
Recent work of Chen et al. (2022) extends the results of Foster et al. (2021) to provide regret bounds that
scale with sup2(M)r-deco
(M;), which one might hope to be smaller than supM2co(M)r-deco
(M;M)(it
is never larger due to Jensen’s inequality). We show that this is not the case: For both constrained and oﬀset,
the randomized DEC is sandwiched between the DEC with M2M+and the DEC with M2co(M).
26Proposition 4.11. Suppose that Assumption A.1 is satisﬁed. For all  >0, we have
sup
M2M+r-deco
(M;M)sup
2(M)r-deco;rnd
=4(M;) sup
M2co(M)r-deco
=4(M;M): (55)
In addition, suppose that the strong regularity condition (Deﬁnition 4.1, Eq. (43)) is satisﬁed relative to M+.
Then for all ">0, we have
sup
M2M+r-decc
"(M[fMg;M)c1sup
2(M)r-decc;rnd
c2"(M[fMg;) +c3" (56)
c1sup
M2co(M)r-decc
c2"(M[fMg;M) +c3"; (57)
whereM:=EM0[M0]andc1;c2;c3>0are constants that depend only on Creg;Cloc>0.
A similar equivalence holds for PAC; see Appendix D.2.2. The main consequences of this result are as follows.
•Since allowing for arbitrary reference models M2M+never increases the value over reference models
M2co(M), one can freely work with whichever version is more convenient, either for upper or lower
bounds.
•From a statistical perspective, it is not possible to further tighten our results by working with the
DEC with randomized estimators, since this complexity measure is never smaller than the variant with
M2co(M)by more than constant factors.
We mention in passing that the proof of the equivalence (55) for the oﬀset DEC is a simple consequence of
the minimax theorem and convexity of squared Hellinger distance, but the proof of the equivalence (56) is
quite involved, and uses the tools developed in Section 4.2 to pass back and forth between the constrained
and oﬀset DEC. We are curious as to whether there is a simpler proof.
5 Improvement over Prior Work
In this section, we use the tools developed in Section 4 to show that the regret bounds in Theorems 2.2
and 3.2 always improve upon those in prior work (Foster et al., 2021, 2022b). We then highlight some concrete
model classes for which our bounds provide meaningful improvement, and discuss additional related work.
Regret bounds from prior work. Recall that for a model class Mand reference model M2M, we
deﬁne the localized subclass around Mvia
M(M) =
M2M :fM(M)fM(M) 	
: (58)
where>0is the radius. Focusing on ﬁnite classes for simplicity, the best upper bounds from prior work
are those of Foster et al. (2021), which take the form
E[RegDM(T)]eO(1)min
>0max
sup
M2co(M)r-deco
(M(;T)(M);M)T; logjMj
;(59)
for(;T) =eO 
r-deco
(M) +
TlogjMj+ 1
. The best lower bounds from prior work are those of Foster
et al. (2022b, Theorem D.1), which apply to all algorithms with “sub-Chebychev” tail behavior,5and scale as
E[RegDM(T)]
(1)max
>p
C(T)Tsup
M2Mr-deco
(M(;T)(M);M)T; (60)
whereC(T):=O(log(T^V(M)))and(;T):=C(T) 1
T.
5Sub-Chebychev algorithms are those for which the root-mean-squared regret is of the same order as the expected regret.
Foster et al. (2021) provide lower bounds that do not require the assumption of sub-Chebychev tail behavior, but these results
depend on the DEC for a smaller subclass of the form M1
(M) =n
M2M :jgM() gM()j82o
, and can be loose
compared to Eq. (60).
27Our improvement. The following result, which follows immediately from Proposition 4.8, implies that
the upper and lower bounds in Theorem 3.2 and Theorem 2.2, are always tighter than the guarantees in Eq.
(59) and Eq. (60), respectively, under an appropriate regularity condition.
Corollary 5.1. Whenever the strong regularity condition (Deﬁnition 4.1) is satisﬁed for M2M+with
Cloc;Creg=O(1), we have that for all ">0and >0,
r-decc
"(M[fMg;M)O 
r-deco
(M(";)(M);M)_0 +"2+"
; (61)
where(";) =O 
r-deco
(M;M)_0 +"2+ 1
. In addition, for all ">0,
(" 1), andM2M+,
r-decc
"(M[fMg;M)r-deco
(M(";)(M);M); (62)
where(";) = 
 
"2
.
By applying Eq. (61) with "(T) =eOq
logjMj
T
, we conclude that the upper bound in Theorem 3.2 is always
bounded above by the quantity in Eq. (59) up to logarithmic factors in Tand1=. Similarly, by applying
Eq. (62) with "(T) =e
q
1
T
we see that the lower bound in Theorem 2.2 is always bounded below by the
quantity in Eq. (60) up to log(T)factors and an additive O(p
T)term. Beyond simply scaling with a larger
complexity measure, Theorem 2.2 1) holds for arbitrary algorithms, removing the sub-Chebychev assumption
used by Foster et al. (2022b), and 2) allows for improper reference models M =2M.
We now provide concrete model classes for which our results lead to quantitative improvements in rates. Our
ﬁrst example is a model class for which our main upper bound (Theorem 3.2) improves over Foster et al.
(2021) by (implicitly) achieving a tighter localization radius than Eq. (59).
Example 5.1 (Improvement from upper bound) .Consider a model class M;parameterized by 2(0;1=2],
2(0;1), andA2N.
1. = [A][fg, whereis a “revealing” decision.
2.O= [A][f?g, where?is a null symbol.
3.We haveM=fM;igi2[A][ffMg. For each i2[A], the model M;i2M;has rewards and
observations deﬁned as follows:
(a)For2[A],fM;i() =1
2+ 1f=ig, andfM;i() = 0. All2haver=fM;i()almost
surely under rM;i().
(b)For2[A], we receive the observation o=?. Selecting gives the observation o=i2[A]with
probability ando=?with probability 1 .
4. The model fMis deﬁned as follows:
(a)We haveffM() =1
2for all2[A]andffM() = 0, withr=ffM()almost surely under rfM()
for all2.
(b)All2[A]haveo=?almost surely. For , we observe o=?with probability 1 and
oUnif([A])with probability .
LetM:=M1;[M2;, with1= 1=2,2/T 1=4,/T 1=2, andA/T2. Then:
•The E2D+algorithm, via Theorem 3.2, achieves E[RegDM(T)]eO(p
T).
•The regret bound in Eq. (59) scales with e
(T5=8).
/
The next example is a model class for which our main lower bound (Theorem 2.2) improves over Foster et al.
(2021), as a consequence of allowing for improper reference models M =2M.
28Example 5.2 (Improvement from lower bound) .LetA2Nand =f1;:::;Ag. Consider the multi-armed
bandit model class M=fM1;:::;MAgconsisting models of the form
Mi() = Ber(fi());
wherefi():=1
2+ If=ig. Foster et al. (2021) show that regardless of how >0is chosen,
r-deco
(M;M)1
forall >0andM2M, sothelowerbound(60)canatmostgive E[RegDM(T)]
(p
T).
On the other hand, by choosing M() =Ber(1
2), which has M =2M, it is straightforward to see that whenever
/"p
A, we have r-decc
"(M)r-decc
"(M;M)
("p
A). Setting /"(T)p
A, Theorem 2.2 gives
E[RegDM(T)]e
(p
AT);
which is optimal. This shows that in general, allowing for improper reference models M =2Mis necessary to
obtain tight lower bounds. /
5.1 Additional Related Work
Concurrent work of Chen et al. (2022) independently discovered the oﬀset variant of the PAC Decision-
Estimation Coeﬃcient, and used it to give upper and lower bounds for PAC sample complexity by adapting
the techniques of Foster et al. (2021). Our guarantees for both regret and PAC are always tighter than these
results, analogous to the improvement we obtain over Foster et al. (2021) (see also Section 4.4), but our
techniques are otherwise complementary.
6 Additional Examples
We close with some brief examples that showcase the behavior of the constrained Decision-Estimation
Coeﬃcient, as well as our upper and lower bounds, for standard model classes of interest. For regret, Foster
et al. (2021) provide lower bounds on the (localized) oﬀset DEC for a number of canonical models in bandits
and reinforcement learning. It is straightforward to derive lower bounds on the constrained DEC by combining
these results with Corollary 5.1. Likewise, Foster et al. (2021) give global upper bounds on the oﬀset DEC
for the same examples, which immediately lead to upper bounds on the constrained DEC via Proposition 4.1.
This approach leads to lower and upper bounds on the constrained Decision-Estimation Coeﬃcient for all
of the examples considered in Foster et al. (2021). We summarize these results, as well the implied lower
bounds on regret, in Table 1; upper bounds on regret are similar, but depend additionally on Est H(T;). See
Foster et al. (2021) for further background.
Setting r-decc
"(M) Lower Bound (Theorem 2.2)
Multi-Armed Bandit "p
Ap
AT
Multi-Armed Bandit w/ gap If">=p
Ag A=
Linear Bandit "p
dp
dT
Lipschitz Bandit "1 d
d+2 Td+1
d+2
ReLU Bandit If">2 
(d)g 2
(d)
Tabular RL "p
HSAp
HSAT
Linear MDP "p
dp
dT
RL w/ linear Q?If"2 
(d)_2 
(H)g 2
(d)^2
(H)
Deterministic RL w/ linear Q?If"1=p
dg d
Table 1: Lower bounds for bandits and reinforcement learning recovered by the constrained Decision-
Estimation Coeﬃcient, where A=#actions , =gap,d=feature dim. ,H=episode horizon , and
S=#states. Numerical constants and log(T)factors are suppressed.
Example: Multi-armed bandit. We now sketch the approach to lower bounds outlined above in greater
detail, focusing on multi-armed bandits for concreteness. Foster et al. (2021) show that for when Mis the
29class of all multi-armed bandit instances with  =f1;:::;Agand Bernoulli rewards, there exists M2M
such that for all c1A,
sup
M2Mr-deco
(M(M);M)c2A
;
where:=c3A
, andc1;c2;c3>0are numerical constants. Corollary 5.1 implies that for all ">0and
M2M+,
r-decc
"(M)sup
>0r-deco
(M(";)(M);M);
where(";) =c"2for a suﬃciently small numerical constant c. For any given ">0, if we set=c0A1=2="
for a suﬃciently large constant c0, we haveM(M)M(";)(M), and we conclude that
r-decc
"(M)

"p
A
for all"c00A 1=2, wherec00is a suﬃciently small constant. Plugging this lower bound on the DEC into
Theorem 2.2 yields a lower bound on regret of the form E[RegDM(T)]e
(p
AT).
Acknowledgements
We thank Jian Qian, Sasha Rakhlin, Rob Schapire, and Andrew Wagenmaker for helpful comments and
discussions.
30References
Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An eﬃcient algorithm for
bandit linear optimization. In Proc. of the 21st Annual Conference on Learning Theory (COLT) , 2008.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning
with value-targeted regression. In International Conference on Machine Learning , pages 463–474. PMLR,
2020.
Sébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. X-armed bandits. Journal of Machine
Learning Research , 12(5), 2011.
Sébastien Bubeck, Nicolo Cesa-Bianchi, and Sham M Kakade. Towards minimax policies for online linear
optimization with bandit feedback. In Conference on Learning Theory , pages 41–1. JMLR Workshop and
Conference Proceedings, 2012.
Sébastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex optimization. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing , pages 72–85, 2017.
Fan Chen, Song Mei, and Yu Bai. Uniﬁed algorithms for rl with decision-estimation coeﬃcients: No-regret,
pac, and reward-free learning. arXiv preprint arXiv:2209.11745 , 2022.
Varsha Dani, Thomas P Hayes, and Sham Kakade. The price of bandit information for online optimization.
2007.
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity of
the linear quadratic regulator. Foundations of Computational Mathematics , 20(4):633–679, 2020.
Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Provably eﬃcient reinforcement learning with aggregated
states.arXiv preprint arXiv:1912.06366 , 2019.
David L Donoho and Richard C Liu. Geometrizing rates of convergence. Annals of Statistics , 1987.
David L Donoho and Richard C Liu. Geometrizing rates of convergence, II. The Annals of Statistics , pages
633–667, 1991a.
David L Donoho and Richard C Liu. Geometrizing rates of convergence, III. The Annals of Statistics , pages
668–701, 1991b.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably
eﬃcient RL with rich observations via latent state decoding. In International Conference on Machine
Learning , pages 1665–1674. PMLR, 2019.
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
Bilinear classes: A structural framework for provable generalization in RL. International Conference on
Machine Learning , 2021.
Richard Durrett. Probability: theory and examples . Duxbury Press, Belmont, CA, ﬁfth edition, 2019. ISBN
0-534-24318-5.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the
bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM
symposium on Discrete algorithms , pages 385–394, 2005.
Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive
decision making. arXiv preprint arXiv:2112.13487 , 2021.
Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari. A note on model-free
reinforcement learning with the decision-estimation coeﬃcient. arXiv preprint arXiv:2211.14250 , 2022a.
Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the complexity of adversarial
decision making. arXiv preprint arXiv:2206.13063 , 2022b.
31Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual
decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine
Learning , pages 1704–1713, 2017.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably eﬃcient reinforcement learning with
linear function approximation. In Conference on Learning Theory , pages 2137–2143, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryooseﬁ. Bellman eluder dimension: New rich classes of RL problems,
and sample-eﬃcient algorithms. Neural Information Processing Systems , 2021.
Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural
Information Processing Systems , 17:697–704, 2004.
AkshayKrishnamurthy, AlekhAgarwal, andJohnLangford. PACreinforcementlearningwithrichobservations.
InAdvances in Neural Information Processing Systems , pages 1840–1848, 2016.
Tor Lattimore. Improved regret for zeroth-order adversarial bandit convex optimisation. Mathematical
Statistics and Learning , 2(3):311–334, 2020.
Lihong Li. A unifying framework for computational reinforcement learning theory . Rutgers, The State
University of New Jersey—New Brunswick, 2009.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 ,
2015.
Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bound and
optimal algorithms. In Conference on Learning Theory , pages 975–999. PMLR, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529, 2015.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning
using linearly combined model ensembles. In International Conference on Artiﬁcial Intelligence and
Statistics , pages 2010–2020. PMLR, 2020.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration.
InAdvances in Neural Information Processing Systems , pages 2256–2264, 2013.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. nature, 529(7587):484, 2016.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL in
contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In
Conference on learning theory , pages 2898–2933. PMLR, 2019.
Alexandre B Tsybakov. Introduction to Nonparametric Estimation . Springer Publishing Company, Incorpo-
rated, 2008.
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function
approximation: Provably eﬃcient approach via bounded eluder dimension. Advances in Neural Information
Processing Systems , 33, 2020.
Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features. In
International Conference on Machine Learning , pages 6995–7004. PMLR, 2019.
Bin Yu. Assouad, fano, and le cam. In Festschrift for Lucien Le Cam , pages 423–435. Springer, 1997.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for
linear mixture markov decision processes. In Conference on Learning Theory , pages 4532–4576. PMLR,
2021.
32A Preliminaries
A.1 Minimax Theorem
For certain structural results, we require that the oﬀset Decision-Estimation Coeﬃcient (either the regret or
PAC variant) is equal to its Bayesian counterpart. This is a consequence of the minimax theorem whenever
mild topological conditions are satisﬁed; note that our objective can always be made convex-concave by
writing
r-deco
(M;M) = inf
p2()sup
2(M)Ep;M
fM(M) fM() D2
H 
M();M()
;
so all that is required to invoke the minimax theorem is compactness. We state this as an assumption to
avoid committing to a particular set of technical conditions.
Assumption A.1 (Minimax swap) .For the regret DEC, we have
r-deco
(M;M) =r-deco
(M;M):= sup
2(M)inf
p2()Ep;M
fM(M) fM() D2
H 
M();M()
:
(63)
For the PAC DEC, we have
p-deco
(M;M) =p-deco
(M;M):= sup
2(M)inf
p;q2()EM
Ep[fM(M) fM()] Eq
D2
H 
M();M()
:
(64)
As the simplest possible example, Assumption A.1 is satisﬁed whenever Ris bounded and is ﬁnite (cf.
Proposition 4.2 in Foster et al. (2021)), but assumption can be shown to hold under substantially more
general conditions.
B Omitted Proofs from Section 2
B.1 Proof of Regret Lower Bound (Theorem 2.2)
In this section, we prove Theorem 2.2. The proof proceeds in two parts:
•In Appendix B.1.1, we state and prove Lemma B.1, a lower bound which is similar to Theorem 2.2, but
restricts to proper reference models (speciﬁcally, the lower bound scales with supM2Mr-decc
"(T)(M;M)).
isM.
•In Appendix B.1.2, we prove the following algorithmic result (Lemma B.2): For any class Mand any
M2M+(not necessarily in M), if there is an algorithm that achieves regret of at most Rwith respect
to the model class M, then there is an algorithm that achieves regret at most O(RlogT)with respect
to the model class M[fMg. We then prove Theorem 2.2 by combining this result with Lemma B.1.
We mention in passing that the two-part approach in this section can also be applied to derive lower bounds
for the PAC framework, but we adopt the alternative approach in Section 2.2 because it leads to a result
with fewer logarithmic factors.
B.1.1 Lower Bound for Proper Reference Models ( M2M)
In this section we prove Lemma B.1, a weaker lower bound analogous to the one stated in Theorem 2.2, but
with the DEC replaced by a smaller quantity constrained to have M2M. This weaker version is shown
below.
Lemma B.1. Lete"(T):=c11p
TC(T), wherec1>0is a suﬃciently small numerical constant. For all
T2Nsuch that the condition
sup
M2Mr-decc
e"(T)(M;M)8e"(T) (65)
33is satisﬁed, we have that for any regret minimization algorithm, there exists a model in Msuch that, under
this model,
E[RegDM(T)]
(T)sup
M2Mr-decc
e"(T)(M;M): (66)
We remark that the condition (65) can be relaxed by replacing the constant 8 on the right-hand side with
any constant strictly greater than 1.
Proof of Lemma B.1. Let the algorithm under consideration be ﬁxed, and let PMdenote the induced
law of HTwhenMis the underlying model. Let EMdenote the corresponding expectation, and let pM:=
EMh
1
TPT
t=1pti
. Deﬁne":=e"(T) =c1p
TC(T), where the constant c1>0will be speciﬁed below. Let M2M
be chosen to maximize r-decc
"(M;M), and deﬁne :=r-decc
"(M;M).
Restricting to models performing poorly on M.IfEpM[gM()]=10, then, by the deﬁnition of
pM, we have EM[RegDM(T)]T=10, completing the proof of the lemma. Hence, we may assume going
forward that EpM[gM()]<=10.
Choosing an alternative model. Deﬁne
M= arg max
M2M
EpM[fM(M) fM()]jEpM
D2
H 
M();M()
"2	
; (67)
so that
EpM[gM()]r-decc
"(M;M) =: (68)
Letc22(0;1)be ﬁxed and deﬁne E=f:gM()c2g. Recall that by Lemma A.13 of Foster et al.
(2021), we have
D2
H 
PM;PM
C(T)TEpM
D2
H 
M();M()
C(T)T"2;
where we remind the reader that C(T) =O(log(T^V(M))). We choose the constant c1>0in the deﬁnition
of"=e"(T)to be suﬃciently small so that D2
H 
PM;PM
1=100and thusDTV 
PM;PM
1
10.
Observe that from the deﬁnition of E, we have
EpM[fM(M) fM()]c2pM(E)c2(pM(E) DTV 
PM;PM
)
c2(pM(E) 1=10): (69)
Therefore, it suﬃces to lower bound pM(E)by1=2.
Lower bounding the gap. We now compute
fM(M) fM(M)EpM[gM() gM()] EpM[jfM() fM()j]
EpM[gM()] 
10 "
9
10 "; (70)
where the second inequality uses the assumption that EpM[gM]< = 10and Lemma D.1, and the ﬁnal
inequality uses Eq. (68).
34Finishing up. We conclude by noting that
pM(Ec)9
10 "
EpM[ 1fEcg(fM(M) fM(M))]
EpM[ 1fEcg(gM() gM())] +"
c2+";
where the ﬁrst inequality uses Eq. (70), the second inequality uses Lemma D.1, and the third inequality uses
thatgM()0for all2, as well as the fact that gM()<c2for2Ec. Rearranging, we conclude that
pM(Ec
1)1=2as long asc21=8and"=8. Our choice of M, together with the growth condition (65),
ensures that we indeed have "=8, thus establishing via Eq. (69), that EpM[gM()]
()as desired.
B.1.2 Reducing from Improper ( M2M+) to Proper (M2M )
In this section, we work with several choices for the model class and regret minimization algorithm. To avoid
ambiguity, let us introduce some additional notation. Recall that an algorithm for the T-timestep interactive
decision making problem (in the regret framework) is speciﬁed by a sequence p= (p1;:::;pT), where for each
t2[T],ptis a probability kernel from (
t 1;Ft 1)to(;P). Given an algorithm p, we let PM;p[]denote
the law it induces on HTwhenM2M+is the underlying model, and let EM;p[]denote the corresponding
expectation. With this notation, the algorithm’s expected regret when the underlying model is M2M+is
EM;p[RegDM(T)].
The following lemma is the main technical result of this section. It shows that any model M2Mwith
bounded optimal value can be added to a model class Mwithout substantially increasing the minimax regret.
Lemma B.2. Let the time T2Nand model classMbe ﬁxed. Let M2M+be any model such that for
allM2M,fM(M)fM(M) +for some >0. The minimax regret for the model class M[fMgis
bounded above as follows:
inf
(p0)1;:::;(p0)Tsup
M?2M[fMgEM?;p0[RegDM(T)]ClogTinf
p1;:::;pTsup
M?2MEM?;p[RegDM(T)] +C(p
T+T):(71)
whereC > 0denotes a universal constant.
Before proving Lemma B.2, we show how it implies Theorem 2.2.
Proof of Theorem 2.2. FixT2N, and write "="(T) =c1p
2TC(T), where the constant c1>0is chosen as
in Lemma B.1 (in particular, note thatp
2"=e"(T), wheree"(T)is as deﬁned in Lemma B.1). Let M2M+
be chosen to maximize r-decc
"(M[fMg;M), so that r-decc
"(M) =r-decc
"(M[fMg;M). Deﬁne
fM:=fM2M[fMgjfM(M)fM(M) p
2"g;
so thatM2fM. Then by Lemma D.2, we have
r-decc
"(M) =r-decc
"(M[fMg;M)r-deccp
2"(fM;M) +p
2"; (72)
Note that for all M2fM, we have that fM(M)fM(M) +p
2"fM(M) +p
2". Thus, by applying
Lemma B.2 with =p
2"to the classfMnfMg, we see that
inf
(p0)1;:::;(p0)Tsup
M?2fMEM?;p0[RegDM(T)]ClogTinf
p1;:::;pTsup
M?2fMnfMgEM?;p[RegDM(T)] +C(p
T+p
2"T)
ClogTinf
p1;:::;pTsup
M?2MEM?;p[RegDM(T)] +C(p
T+p
2"T);(73)
where the second inequality follows since fMnfMgM.
35To proceed, we will apply Lemma B.1 to the class fM. To verify that the condition Eq. (65) is satisﬁed for
this class, we note that, as remarked above, e"(T) =p
2", so that
sup
M02fMr-decc
e"(T)(fM;M0) = sup
M02fMr-deccp
2"(fM;M0)r-deccp
2"(fM;M)r-decc
"(M) p
2"8";
where the second-to-last inequality uses Eq. (72) and the ﬁnal inequality uses the assumption from Eq. (15)
that r-decc
"(M)10". Lemma B.1 gives that, for some universal constant c2>0,
inf
(p0)1;:::;(p0)Tsup
M?2fMEM?;p0[RegDM(T)]c2Tsup
M02fMr-deccp
2"(fM;M0)
c2Tr-deccp
2"(fM;M)c2T(r-decc
"(M) p
2");(74)
where the ﬁnal inequality uses Eq. (72). Combining Eq. (74) and Eq. (73) gives that
inf
p1;:::;pTsup
M?2MEM?;p[RegDM(T)]1
ClogT
c2T(r-decc
"(M) p
2") C(p
T+p
2"T)
;
which implies that for some constants C0;C00;c0
2>0, we have
inf
p1;:::;pTsup
M?2MEM?;p[RegDM(T)]1
logT
c0
2T(r-decc
"(M) C0") C00p
T
:
As long as1
2r-decc
"(M)>C0", it follows that
inf
p1;:::;pTsup
M?2MEM?;p[RegDM(T)]c0
2T
2 logTr-decc
"(M) C00p
T
logT;
as desired.
Finally, we prove Lemma B.2.
Proof of Lemma B.2. Fix any algorithm p= (p1;:::;pT). We deﬁne a modiﬁed algorithm p0=
((p0)1;:::; (p0)T)in Algorithm 3. Roughly speaking, p0runspmultiple times, re-initializing pwhenever
the average reward for the current run falls too far below fM(M). If the algorithm p0ﬁnds that it has
re-initialized pmore than log(T)times, it will switch to playing Mfor all remaining rounds. The crux of the
proof will be to show that the worst-case regret of p0for models inM[fMgis not much larger than the
worst-case regret of pfor models inM.
As per our convention, in the context of Algorithm 3, we let Ftdenote the sigma-algebra generated
byf(s;rs;os)g1st. We bound the regret of the algorithm p0by considering the following cases for
M?2M[fMg.
Case 1:M?=M.LetT02[T+ 1]be deﬁned to be the smallest value of tfor which the decision tis
chosen using the rule at Line 10, or T+ 1if there is no such step. By construction, we have that
T0 1X
t=1(fM(M) rt) =TX
t=11ft<T 0g(fM(M) rt)(R+ 1)dlogTe: (75)
36Algorithm 3 Algorithmp0used in proof of Lemma B.2
1:parameters :
Number of rounds T2N.
Algorithmp= (p1;:::;pT).
2:InitializeI= 1,T1= 1, andR= 4supM?2MEM?;p[RegDM(T)] +T+ 8p
T.
3:for1tTdo
4:Deﬁne (p0)t()to be the distribution pt TI+1(jf(s;rs;os)gt 1
s=TI).
5:Drawt(p0)t, and observe (t;rt;ot).
6:ifPt
s=TI(fM(M) rs)Rthen
7: SetTI+1:=t+ 1and then increment I. // This has the effect of re-initializing p.
8:ifI >dlogTethen
9:breakout of loop.
10:For remaining time steps t(if any): play t:=M(i.e., set (p0)t=IM).
Note that, for each t2[T], the variable 1ft<T 0gis measurable with respect to Ft 1. As a result, we have
EM;p0[RegDM(T)] =EM;p0"TX
t=1 
fM(M) Et(p0)t[fM(t)]#
=EM;p0"TX
t=11ft<T 0g 
fM(M) Et(p0)t[fM(t)]#
=EM;p0"TX
t=11ft<T 0g(fM(M) rt)#
+EM;p0"TX
t=11ft<T 0g 
rt Et(p0)t[fM(t)]#
(R+ 1)logT;
where the ﬁnal inequality uses Eq. (75) and the fact that for each t, we have
E
1ft<T 0g 
rt Et(p0)t[fM(t)]
jFt 1
= 1ft<T 0gE[rt Et(p0)t
fM(t)]jFt 1
= 0:
Thus, in the case M?=M, we have veriﬁed that the claimed upper bound in Eq. (71) on the regret of p0
holds.
Case 2:M?2M.We ﬁrst state and prove two technical lemmas.
Lemma B.3. For the algorithm p, any model M?2M+, and random variable (potentially dependent on
HT) taking values in [T], it holds that
EM?;p"X
t=1Etpt[fM?(M?) fM?(t)]#
EM?;p[RegDM(T)]:
Proof of Lemma B.3. The result follows by noting that
EM?;p[RegDM(T)] EM?;p"X
t=1Etpt[fM?(M?) fM?(t)]#
=EM?;p"TX
t=11f <tgEtpt[fM?(M?) fM?(t)]#
0;
37where we have used that the random variable Etpt[fM?(M?) fM?(t)]is non-negative a.s.
The next lemma concerns the probability that a single run of the algorithm pviolates the condition in Line 6
of Algorithm 3.
Lemma B.4. For any algorithm p= (p1;:::;pT)and modelM?2M, it holds that
PM?;p 
9tT:tX
s=1(fM(M) rs)>R!
1
2;
whereR= 4supM?2MEM?;p[RegDM(T)] +T+ 8p
T.
Proof of Lemma B.4. LetFtbe the sigma-algebra generated by f(s;rs;os)gt
s=1. FixM?2 M
and deﬁne R0:=EM?;p[RegDM(T)]. By Markov’s inequality and the fact that the random variables
Etpt[fM?(M?) fM?(t)]are all non-negative, it holds that
PM?;p 
sup
tTtX
s=1Esps[fM?(M?) fM?(s)]>4R0!
=PM?;p TX
t=1Etpt[fM?(M?) fM?(t)]>4R0!
1
4: (76)
Now, deﬁne X0= 0andXt=Pt
s=1(Esps[fM?(s)] rs)fort2[T]. Note that (Xt)t0is a martingale
with respect to the ﬁltration Ft. Therefore, by Theorem 4.5.1 of Durrett (2019), it holds that
EM?;p
sup
tTjXtj2
4EM?;p"TX
t=1E
(Etpt[fM?(t)] rt)2jFt 1#
4T;
where the ﬁnal inequality uses that jEtpt[fM?(t)] rtj1for allt. By Jensen’s inequality and Markov’s
inequality, it follows that for any >0,
PM?;p
sup
tTjXtj>2p
T
1
;
and by choosing = 4, we see that
PM?;p 
sup
tTtX
s=1 
Esps[fM?(s)] rs
>8p
T!
1
4: (77)
Combining Eq. (76) and Eq. (77), we have
PM?;p 
9tT:tX
s=1Esps[fM?(M?) rs]>4R0+ 8p
T!
1
2:
SinceM?2M, and sofM?(M?)fM(M) , it follows that
PM?;p 
9tT:tX
s=1Esps[fM(M) rs]>4R0+ 8p
T+T!
1
2;
as desired.
We now continue with the proof of Lemma B.2. Write L=dlogTe, and denote the ﬁnal value of Iin
Algorithm 3 by I0L+ 1. IfI0L, then setTI0+1==TL+1=T+ 1. Note that for each `2[L+ 1],
T` 1is a stopping time (since the event fT` 1 =tgis measurable with respect to Ftfor eacht2[T]),
and thusT`is a stopping time as well.
38Lemma B.4 together with the deﬁnition of (p0)tin Line 4 establishes that for each `2[L],
PM?;p0(T`+1TjT`T) =PM?;p0 
9ts.t.T 1tT`andtX
s=T`(fM(M) rs)>RjT`T!
1
2:
Therefore, since the event fT`Tgis equal to the event fT`0T8`0`g,
PM?;p0(TL+1T) =PM?;p0(8`L+ 1; T`T)
=LY
`=1PM?;p0(T`+1TjT`0T8`0`)
=LY
`=1PM?;p0(T`+1TjT`T)
(1=2)L1=T:
Furthermore, for each `2[L], we have
EM?;p02
4T`+1 1X
t=T`Et(p0)t[fM?(M?) fM?(t)]3
5=EM?;p0"TX
t=11fT`t<T`+1gEt(p0)t[fM?(M?) fM?(t)]#
EM?;p[RegDM(T)];
where the inequality uses Lemma B.3 and the deﬁnition of p0in Line 4 for steps T`t<T`+1. It follows that
EM?;p0"TX
t=1Et(p0)t[fM?(M?) fM?(t)]#
TPM?;p0(TL+1T) +LX
`=1EM?;p02
4T`+1 1X
t=T`Et(p0)t[fM?(M?) fM?(t)]3
5
1 +LEM?;p[RegDM(T)];
which veriﬁes the claimed upper bound on regret in Eq. (71).
C Omitted Proofs from Section 3
In this section we prove Theorem 3.2, which shows that Algorithm 2 attains a regret bound based on the
constrained DEC
C.1 Proof of Regret Upper Bound (Theorem 3.2)
Toward proving Theorem 3.2, we introduce a few success events that will be used throughout the analysis:
1. For each i2[N],Aidenotes the event that all of the following inequalities hold:
8s2Si;Ji;sX
j=1Epsh
D2
H
M?();fMj
s()i
Est H(Ji;); (78)
Epsih
D2
H
cMi();M?()i
Est H(Ji;)
Ji; (79)
Epsih
D2
H
cMsi();M?()i
"2
i; (80)
where we recall that siis the index deﬁned on Line 23.
392. For each i2[N],Bidenotes the event that
X
t2EiEpth
D2
H
M?();cMt()i
Est H(jEij;): (81)
3. For each i2[N],Cidenotes the event that M?2Mi.
4. For each i2[N],Didenotes the event that there is some s2Siso that
Epsh
D2
H
M?();cMs()i
"2
i
16:
In addition, we deﬁne
A=\
i2[N]Ai;B=\
i2[N]Bi;C=\
i2[N]Ci;andD=\
i2[N]Di:
We also recall the following notation, which will be used throughout the proof.
•Fori2[N]we set
i:=C0r-decc
"i(M) + 64"i; (82)
with the convention that 0= 1. The constant C0>0in Eq. (82), as well as the constant C1>0
speciﬁed in Algorithm 2, will need to be taken suﬃciently large; in what follows, we show that C020
andC1128will suﬃce.
C.1.1 Technical Lemmas
Before proving Theorem 3.2, we state and prove several technical lemmas concerning the performance of
Algorithm 2. The following lemma shows that the event A\B\Coccurs with high probability.
Lemma C.1. Suppose that C1128. The event A\B\C\Doccurs with probability at least 1 3LN.
Proof of Lemma C.1. We show that PT
i0iAi0\Bi0\Ci0\Di0
1 3Lifor eachi2[N]using
induction on i. Fixi2[N]and let us condition onT
i0<i(Ai0\Bi0\Ci0\Di0).
Establishing that Ciholds. The fact that Ai 1holds implies that Ebpi 1h
D2
H
M?();cMi 1()i

Est H(Ji 1;)
Ji 1, which implies (by deﬁnition) that M?2Mi, i.e.,Ciholds.
Establishing that Biholds. Conditioned on Ci, it follows from Assumption 3.2 that Biholds with
probability at least 1 .
Establishing that Diholds. Note that as a consequence of our parameter settings,
"2
i2 iT"2
N= 2 iC1Est H(T;)Lmax
32Est H(Ji;)
Ji;32Est H(jEij;)
jEij
;(83)
where we have used that C1128and that Est H(T;)maxfEst H(jEij;);Est H(Ji;)g. Then since Bi(i.e.,
(81)) holds, at least jEij=2roundst2EisatisfyM?2H"i=4;pt(cMt). SincejSij=Llog1=, it follows that
with probability at least 1 , there is some s2Si, which we denote by s?
i, for which M?2H"i=4;ps?
i(cMs?
i).
In particular, conditioned on Bi, the event Diholds with probability at least 1 .
40Establishing that Aiholds. Next, from Assumption 3.2 and the fact that Hellinger distance is always
non-negative, we have that with probability at least 1 L, for alls2Si,
Ji;sX
j=1Epsh
D2
H
M?();fMj
s()i
Est H(Ji;); (84)
which veriﬁes that (78) holds. Next, let us condition on the event that Diholds. Then applying (84) to
s=s?
iand using the deﬁnition of s?
i(recall that s?
iis deﬁned in the prequel so that M?2H"i=4;ps?
i(cMs?
i)),
we see that
Ji;s?
iX
j=1Eps?
ih
D2
H
cMs?
i();fMj
s?
i()i
Ji;s?
iX
j=12Eps?
ih
D2
H
cMs?
i();M?()i
+ 2Eps?
ih
D2
H
cM?();fMj
s?
i()i
2Ji;s?
i"2
i
16+ 2Ji;s?
iX
j=1Eps?
ih
D2
H
M?();fMj
s?
i()i
Ji"2
i
8+ 2Est H(Ji;)3Ji"2
i
16;
where the ﬁnal inequality uses Eq. (83). By the deﬁnition of Ji;s?
ion Lines 20 and 22, it must be the case
thatJi;s?
i=Ji, and thusstmp
iis assigned at least once on Line 22. Therefore, the value of siset on Line 23
satisﬁes
Epsih
D2
H
cMi();M?()i
1
JiJiX
j=1Epsih
D2
H
fMj
si();M?()i
Est H(Ji;)
Ji;
where the ﬁrst inequality uses convexity of the squared Hellinger distance and the second inequality uses (84)
together with the fact that Ji;si=Ji. The above display veriﬁes (79); to verify (80), we note that, since stmp
i
is assigned at least once,
JiX
j=1Epsih
D2
H
cMsi();fMj
si()i
Ji"2
i
4: (85)
Thus, we may compute
Epsih
D2
H
cMsi();M?()i
2Epsih
D2
H
cMi();cMsi()i
+ 2Epsih
D2
H
cMi();M?()i
2
JiJiX
j=1Epsih
D2
H
cMsi();fMj
si()i
+2
JiJiX
j=1Epsih
D2
H
M?();fMj
si()i
"2
i
2+2Est H(Ji;)
Ji"2
i;
where the second inequality uses the convexity of squared Hellinger distance, the third inequality uses (84)
fors=siand (85), and the ﬁnal inequality uses (83). As the above display veriﬁes (80), we conclude that
conditioned on Diholding, Aiholds with probability at least 1 L.
Wrapping up. Summarizing, conditioned onT
i0<iAi0\Bi0\Ci0\Di0, we have shown that Ai\Bi\Ci\Di
holds with probability 1 2 L1 3L. Thus, the inductive hypothesis that P T
i0<iAi0\Bi0\Ci0

1 3L(i 1)implies that PT
i0iAi0\Bi0\Ci0\Di0
(1 3L(i 1))(1 3L)1 3Li.
Summarizing, we get that P(A\B\C\D)1 3LN.
Lemma C.2 shows that the distributions bpicomputed in Algorithm 2 enjoy low suboptimality with respect to
M?.
41Lemma C.2 (Accuracy of reﬁned policies) .Suppose that C04. Then for each epoch i2[N], under the
eventAi, the distribution bpisatisﬁes
Ebpi
fM?(M?) fM?()
i
4: (86)
Proof of Lemma C.2. Conditioning on the event Aigives that (80) holds, which can in particular be
written asM?2H"i;psi(cMsi). Therefore, by the choice of bpi=psiin Line 23 and the deﬁnition in Line 10,
underAi,
Epsi
fM?(M?) fM?()
 sup
M2H"i;psi(cMsi)[fcMsigEpsi[fM(M) fM()]
=r-decc
"i(M[fcMsig;cMsi) sup
M2co(M)r-decc
"i(M[fMg;M)i
4;
where the ﬁnal inequality follows as long as C04.
Lemma C.3 relates the suboptimality under M?for any distribution p2()to that of any model
M2co(Mi+1), in terms of the distance between MandM?. We ultimately apply the lemma with M=cMt
for eacht2Ei+1to derive the following lemma, Lemma C.4.
Lemma C.3 (Comparison with models in reﬁned class) .Fixi2[N]. Then for all M2co(Mi+1)and all
p2(), under the event Ai,
Ep
fM?(M?) fM?()
Ep[fM(M) fM()] +i
2+q
Ep[D2
H(M();M?())]:(87)
Proof of Lemma C.3. We ﬁrst upper bound the optimal value under M?by the optimal value under any
M2co(Mi+1). To do so, ﬁrst note that, by Lemma C.2 (in particular, the fact that Eq. (86) holds at epoch
i), we have that Ebpi[fM?(M?) fM?()]i
4underAi. Then, for any M2Mi+1, we have that, under
the event Ai,
Ebpi
fM?(M?) fM()
Ebpi
fM?(M?) fM?()
+q
Ebpi[D2
H(M();M?())]
i
4+r
2Ebpih
D2
H
M();cMi()i
+ 2Ebpih
D2
H
cMi();M?()i
i
4+ 2s
Est H(Ji;)
Jii
2; (88)
where the second-to-last inequality holds since M2Mi+1and by assumption of the event Ai(in particular,
using (79)), and the ﬁnal inequality holds since 2q
Est H(Ji;)
Jii
4by our choice of i64"iand Eq. (83).
Now ﬁx any M2co(Mi+1), and note that we can write M=EM0M[M0]for someM2(Mi+1). Then
for all2,fM() =EM0M[fM0()], and it follows from Eq. (88) that (again under Ai)
fM?(M?) fM(M)Ebpi
fM?(M?) fM()
=EM0MEbpi
fM?(M?) fM0()
i
2:
Given anyM2co(Mi+1), we have now that under Ai,
Ep
fM?(M?) fM?()
i
2+Ep[fM(M) fM()] +Ep
jfM() fM?()j
i
2+Ep[fM(M) fM()] +q
Ep[D2
H(M();M?())];
as desired.
42Our ﬁnal technical lemma, Lemma C.4, bounds the sub-optimality for all policies played in each epoch Ei.
The need to establish a result of this type is a crucial diﬀerence between the regret and PAC frameworks,
and motivates many of the algorithm design choices behind Algorithm 2.
Lemma C.4 (“Backup” regret guarantee) .Fix anyi2[N]. Then for all t2Ei, we have that under the
eventAi 1,
Ept
fM?(M?) fM?()
r-decc
"i(M[fcMtg;cMt) +i 1
2+r
Epth
D2
H
cMt();M?()i
:
Proof of Lemma C.4. Fix anyt2Ei. The choice of ptin Line 10 ensures that
Ept[fcMt(cMt) fcMt()]r-decc
"i(M[fcMtg;cMt): (89)
Next, under the event Ai 1, we have
Ept
fM?(M?) fM?()
Epth
fcMt(cMt) fcMt()i
+i 1
2+r
Epth
D2
H
cMt();M?()i
r-decc
"i(M[fcMtg;cMt) +i 1
2+r
Epth
D2
H
cMt();M?()i
;
where the ﬁrst inequality uses Lemma C.3 at epoch i 1withp=ptandM=cMt, together with the fact
thatcMt2co(Mi)by construction, and the second inequality uses Eq. (89).
C.1.2 Proof of Theorem 3.2
Proof of Theorem 3.2. Let us condition on the event A\B\C\D, which, by Lemma C.1, holds with
probability 1 3L= 1 3dlog 1=e. Fixi2[N]. We analyze the regret in each epoch ias follows.
•We ﬁrst analyze the rounds in t2Ei. By Lemma C.4, under the event Ai 1, we have
X
t2EiEpt
fM?(M?) fM?()
jEij 
sup
M2co(M)r-decc
"i(M[fMg;M) +i 1!
+X
t2Eir
Epth
D2
H
cMt();M?()i
jEij(i+i 1) +s
jEijX
t2EiEpth
D2
H
cMt();M?()i
2jEiji 1+p
jEijEst H(jEij;); (90)
where the second-to-last inequality follows by our choice of iand the ﬁnal inequality follows from the
fact that Biholds andii 1.
•We next analyze the rounds in Ri. We ﬁrst analyze those rounds in which a decision j
spswas
sampled on Line 17. To do so, ﬁx any s2Si. We ﬁrst note that, by deﬁnition of Ji;s,
Ji;sX
j=1r
Epsh
D2
H
fMjs();cMs()i
vuutJi;sJi;sX
j=1Epsh
D2
H
fMjs();cMs()i
s
Ji;sJi"2
i
4+ 2
p
2Ji+Ji"i=2:
43Furthermore, since the event Aiholds (in particular, using (78)), we have
Ji;sX
j=1r
Epsh
D2
H
fMjs();M?()i
vuutJi;sJi;sX
j=1Epsh
D2
H
fMjs();M?()i
q
Ji;sEst H(Ji;)q
J2
i"2
i=32Ji"i;
where the second-to-last inequality uses (83). Using the above displays, we have
Ji;sX
j=1Ej
sps
fM?(M?) fM?(j
s)
Ji;s 
r-decc
"i(M) +i 1+r
Epsh
D2
H
cMs();M?()i!
2Ji;si 1+Ji;sX
j=1r
2Epsh
D2
H
cMs();fMjs()i
+r
2Epsh
D2
H
M?();fMjs()i
2Jii 1+3Ji"i
2+p
2Ji:
Next we analyze the rounds t2Riwheretpsi=bpion Line 25. Since Aiholds, we have from
Lemma C.2 that Etbpi[fM?(M?) fM?()]i=4, meaning that the total contribution to the regret
from such rounds t2Riis at mostjRiji=4. Thus, the overall contribution to regret from rounds in
Riis bounded above as follows:
X
t2RiEpt[fM?(M?) fM?()]jRiji
4+L
2Jii+3Ji"i
2+p
2Ji
4jRiji+p
2LjRij;
where in the second inequality we have used that jRij=JiLand3"i=2i.
Summarizing, under the event A\B\C\D, the total regret is bounded above by
TX
t=1Ept
fM?(M?) fM?()
NX
i=1
4i 1(jRij+jEij) +p
jEijEst H(jEij;) +p
2LjRij
:(91)
We now simplify the expression in Eq. (91). Recall that Assumption 3.3 gives that for all ">0,
r-decc
"(M)C2
regr-decc
"=Creg(M):
Applying this inequality a total ofl
log("i="N)
log(Creg)m
times for each i2[N]gives that
r-decc
"i(M)C2
reg"i
"N2
r-decc
"N(M)
=C2
reg2N ir-decc
"N(M):
44Then by the choice i=C0r-decc
"i(M) + 64"ifor eachi2[N], we have
NX
i=1i 12i64NX
i=1"i 12i+C0NX
i=12ir-decc
"i 1(M)
64NX
i=1"Np
2N+1+i+O NX
i=12Nr-decc
"N(M)!
128p
C1Est H(T;)LNX
i=1p
2i+1+O 
NTr-decc
"N(M)
:
Therefore, we may upper bound the expression in Eq. (91) as follows (using that Est H(jEij;)Est H(T;)
for eachi):
NX
i=1
4i 1(jRij+jEij) +p
jEijEst H(jEij;) +p
2LjRij
4NX
i=1i 12i+p
Est H(T;)NX
i=1p
2i+p
2LNX
i=1p
2i
O 
p
Est H(T;)LNX
i=1p
2i+p
LNX
i=1p
2i+NTr-decc
"N(M)!
Op
Tlog(1=)Est H(T;) +Tlog(T)r-decc
"N(M)
;
where we have used that L=O(log1=)in the ﬁnal inequality. The proof is completed by rescaling from 
to2and noting that, by construction, we have "NCq
Est H(T;1=)log 1=
Tfor a universal constant C > 0.
D Proofs and Additional Results from Section 4
D.1 Technical Lemmas
Lemma D.1. LetMandMhaveR [0;1]. Then for all "0andp2(), ifM2Hp;"(M), then
Ep
jfM() fM()j
": (92)
Proof of Lemma D.1. Since rewards are in [0;1], we have
Ep
jfM() fM()j
Ep
DTV 
M();M()
q
Ep
D2
H 
M();M()
":
Lemma D.2. Fix a model class M. LetM2M+and">0be given, and set
M0=fM2MjfM(M)fM(M) +"g:
Then we have
r-decc
"=p
2(M;M)r-decc
"(M0;M) +":
Proof of Lemma D.2. Letp2()achieve the value of r-decc
"(M0;M), and setp0=1
2p+1
2IM. Let
M2Hp0;"=p
2(M)Hp;"(M)\HIM;"(M). We claim that M2M0. Indeed,
fM(M) fM(M)DH 
M(M);M(M)
"
45by Lemma D.1. It follows that
sup
M2Hp0;"=p
2(M)Ep[fM(M) fM()] sup
M2Hp;"(M)\M0Ep[fM(M) fM()]r-decc
"(M0;M);(93)
so that
sup
M2Hp0;"=p
2(M)Ep0[fM(M) fM()]1
2r-decc
"(M0;M) +1
2sup
M2Hp0;"=p
2(M)[fM(M) fM(M)]:
To bound the ﬁnal term above, we have
sup
M2Hp0;"=p
2(M)[fM(M) fM(M)] sup
M2Hp0;"=p
2(M)
fM(M) fM(M)
+"
 sup
M2Hp0;"=p
2(M)Ep
fM(M) fM()
+"
 sup
M2Hp0;"=p
2(M)Ep[fM(M) fM()] + 2"
r-decc
"(M0;M) + 2";
where the ﬁrst and third inequalities use Lemma D.1, and the last inequality applies Eq. (93).
Lemma D.3. For a model class Mand reference model M2M+, deﬁne
]p-decc
"(M;M) = inf
p;q2()sup
M2Hp;"(M)\Hq;"(M)Ep[fM(M) fM()]; (94)
with the convention that the value above is zero when Hp;"(M)\Hq;"(M) =?. For allM2M+and">0,
we have
]p-decc
"(M;M)p-decc
"(M;M)]p-deccp
2"(M;M): (95)
Proof of Lemma D.3. The ﬁrst inequality is immediate. For the second, we have
p-decc
"(M;M) inf
p;q2()sup
M2H1
2p+1
2q;"(M)Ep[fM(M) fM()];
by observing that for any minimizer qforp-decc
", we can arrive at an upper bound by substituting q0=1
2p+1
2q.
The result now follows because H1
2p+1
2q;"(M)Hp;p
2"(M)\Hq;p
2"(M).
D.2 Additional Properties of the Decision-Estimation Coeﬃcient
D.2.1 Localization
The following result is an extension of Proposition 4.7 which accommodates randomized estimators.
Proposition D.1. Let; > 0and2(M)be given. Let M=EM0[M0]. For all">0, we have
r-deco;rnd
(M(M)[fMg;)r-decc;rnd
"(M[fMg;) + max
0; +1
2 "2
2
:(96)
which in particular yields
r-deco;rnd
(M(M)[fMg;)r-decc;rndp
2=(M[fMg;) +1
2: (97)
46Proof of Proposition D.1. Fix2(M)and">0, and letp2()be a minimizer for r-decc;rnd
"(M[
fMg;). Fix anyM2M(M)[fMg. We bound the regret under pby considering two cases.
Case 1.IfEMEp
D2
H 
M();M()
"2, it follows from the deﬁnition r-decc;rnd
"(M[fMg;)of that
Ep[fM(M) fM()]r-decc;rnd
"(M[fMg;).
Case 2.For the second case, suppose that EMEp
D2
H 
M();M()
>"2. We now compute
Ep[fM(M) fM()]+Ep
fM(M) fM()
+Ep
fM(M) fM()
+1
2+
2Ep
(fM() fM())2
+r-decc;rnd
"(M[fMg;) +1
2+
2Ep
D2
H 
M();M()
;
+r-decc;rnd
"(M[fMg;) +1
2+
2EMEp
D2
H 
M();M()
;
where the second inequality uses Young’s inequality and the ﬁnal inequality uses convexity of the squared
Hellinger distance. Rearranging, we obtain
Ep
fM(M) fM() EM
D2
H 
M();M()
+r-decc;rnd
"(M[fMg;) +1
2 
2EMEp
D2
H 
M();M()
+r-decc;rnd
"(M[fMg;M) +1
2 "2
2:
Recalling that Mcan be any model in M(M)[fMg, we obtain
r-deco;rnd
(M(M)[fMg;)max
r-decc;rnd
"(M[fMg;); +1
2+r-decc;rnd
"(M[fMg;) "2
2
=r-decc;rnd
"(M[fMg;) + max
0; +1
2 "2
2
:
D.2.2 Role of Convexity for PAC DEC
For2(M), we deﬁne “randomized” variants of the PAC DEC, analogous to those introduced in Section 4,
as follows:
p-decc;rnd
"(M;) = inf
p;q2()sup
M2M
Ep[fM(M) fM()]jEMEq
D2
H 
M();M()
"2	
;(98)
p-deco;rnd
(M;) = inf
p;q2()sup
M2M
Ep[fM(M) fM()] EMEq
D2
H 
M();M()	
:(99)
The following result provides a PAC counterpart to Eq. (55) of Proposition 4.11.
Proposition D.2. Suppose that Assumption A.1 holds. For all  >0, we have
sup
M2M+p-deco
(M;M)sup
2(M)p-deco;rnd
=4(M;) sup
M2co(M)p-deco
=4(M;M): (100)
A PAC analogue of Eq. (56) can be proven by adapting the proof of Proposition 4.11; we do not include this
result.
47Proof of Proposition D.2. LetM2M+and >0be given. We ﬁrst prove the inequality (100). By
Assumption A.1, we have
p-deco
(M;M) = sup
2(M)inf
p;q2()EM
Ep[fM(M) fM()] Eq
D2
H 
M();M()
:
Since Hellinger distance satisﬁes the triangle inequality, we have that for all 2,
EM;M0
D2
H(M();M0())
2EM
D2
H 
M();M()
+ 2EM0
D2
H 
M0();M()
= 4EM
D2
H 
M();M()
:
It follows that
p-deco
(M;M)sup
2(M)inf
p;q2()EMh
Ep[fM(M) fM()] 
4EM0Eq
D2
H(M();M0())i
sup
2(M)sup
2(M)inf
p;q2()EMh
Ep[fM(M) fM()] 
4EM0Eq
D2
H(M();M0())i
sup
2(M)inf
p;q2()sup
M2Mn
Ep[fM(M) fM()] 
4EM0Eq
D2
H(M();M0())o
= sup
2(M)p-deco;rnd
=4(M;):
Jensen’s inequality further implies that sup2(M)p-deco;rnd
=4(M;)supM2co(M)p-deco
=4(M;M).
D.2.3 PAC DEC with Greedy Decisions
For a model class Mand reference model M2M+, deﬁne
p-decc;greedy
" (M;M) = inf
q2()sup
M2M
fM(M) fM(M)jEq
D2
H 
M();M()
"2	
;
with the convention that the value above is zero when Hq;"(M) =?.
Proposition D.3. For all">0andM2M+, we have
p-decc
"(M;M)p-decc;greedy
" (M;M)p-deccp
3"(M;M) + 4": (101)
Proof of Proposition D.3. It is immediate that p-decc;greedy
" (M;M)p-decc
"(M;M), so let us prove
the second inequality. Let M2Mand" >0be given, and let (p;q)be minimizers for ]p-decc
"(M;M).
Deﬁneq0=1
3q+1
3p+1
3IM. Note thatHq0;"=p
3(M)Hq;"(M)\Hp;"(M)\HIM;"(M). As a result, for all
M2Hq0;"=p
3(M), we have
fM(M) fM(M) =fM(M) fM(M) + (fM(M) fM(M))
fM(M) fM(M) +"
Ep
fM(M) fM()
+"
Ep[fM(M) fM()] + 2"
]p-decc
"(M;M) + 2"
p-decc
"(M;M) + 2";
where the ﬁrst and third inequalities use Lemma D.1 and the ﬁnal inequality uses Lemma D.3.
48D.3 Omitted Proofs
Proof of Proposition 4.1. LetM2M+be given. We ﬁrst prove a more general result under the
assumption that for some >0,fM(M)fM(M) +for allM2M:
r-decc
"(M[fMg;M)+ inf
>0
r-deco
(M;M)_0 + 4"2+ (4) 1	
: (102)
Then, at the end of the proof, we show that it is possible to take =O(")without loss of generality.
Let >0be given and let p0be the minimizer for r-deco
(M;M), so that
sup
M2MEp0
fM(M) fM() D2
H 
M();M()
r-deco
(M;M):
LetM?:= arg minM2MEp0
D2
H 
M();M()
, and let 2:=Ep0
D2
H 
M?();M()
.
We will bound the constrained DEC, r-decc
"(M[fMg;M), by playing the distribution
p:= (1 q)IM+qp0;
where
q:=2"2
2^1:
Before proceeding, we state a basic technical lemma.
Lemma D.4. The distribution p0satisﬁes
Ep0
fM(M) fM()
+r-deco
(M;M) + (4) 1+ 22: (103)
We bound the value of the constrained DEC for pby considering two cases.
Case 1:q= 1.Ifq= 1, then 22"2, andp=p0. For models M2Hp;"(M), the deﬁnition of p0implies
that
Ep[fM(M) fM()]r-deco
(M;M) +Ep
D2
H 
M();M()
r-deco
(M;M) +"2:
For the model M, Lemma D.4 implies that
Ep
fM(M) fM()
+r-deco
(M;M) + (4) 1+ 4"2:
Case 2:q<1.Ifq<1, then for all M2M, we have
Ep
D2
H 
M();M()
qEp0
D2
H 
M();M()
qEp0
D2
H 
M?();M()
= 2"2>"2;
where the second inequality uses that M?minimizes Ep0
D2
H 
M();M()
, and the last inequality uses
thatq=2"2
2wheneverq<1. It follows thatHp;"(M)[fMg=fMg, so we only need to bound the regret of
the distribution punderM. To do so, we observe that
Ep
gM()
=qEp0
gM()
q 
+r-deco
(M;M) + (4) 1+ 22
+r-deco
(M;M)_0 + (4) 1+q22
=+r-deco
(M;M)_0 + (4) 1+ 4"2;
where the ﬁrst inequality uses Lemma D.4, and the ﬁnal equality uses that q=2"2
2.
49Finishing up. We have established that
r-decc
"(M[fMg;M)+ inf
>0
r-deco
(M;M)_0 + 4"2+ (4) 1	
wheneverfM(M)fM(M) +for allM2M. To conclude, we appeal to Lemma D.2 applied to the class
M[fMg, which implies that
r-decc
"(M[fMg;M)r-deccp
2"(M0[fMg;M) +p
2"; (104)
whereM0=
M2MjfM(M)fM(M) +p
2"	
. Applying (102) to the quantity r-deccp
2"(M0[fMg;M)
and combining with (104) yields
r-decc
"(M[fMg;M)2p
2"+ inf
>0
r-deco
(M;M)_0 + 8"2+ (4) 1	
:
To simplify this result slightly, we consider two cases. If r-deco
(M;M)(4) 1, then choosing = (4") 1
gives inf>0
r-deco
(M;M) + 4"2+ (4) 1	
4". Otherwise,wehave inf>0
r-deco
(M;M) + 4"2+ (4) 1	

inf>0
2r-deco
(M;M) + 8"2	
.
Proof of Lemma D.4. Observe that
Ep0
fM(M) fM()
+Ep0
fM?(M?) fM()
+Ep0
fM?(M?) fM?()
+ ;
where the ﬁrst inequality uses the fact that fM(M)fM(M) +for allM2 M, and the second
inequality uses Lemma D.1. In addition, the deﬁnition of p0implies that Ep0[fM?(M?) fM?()]
r-deco
(M;M) +2, so we have
Ep0
fM(M) fM()
+r-deco
(M;M) +2+ 
+r-deco
(M;M) + 22+ (4) 1:
Proof of Proposition 4.2. We ﬁrst prove the inequality (38). Let ">0andM2M+be ﬁxed. Using
the method of Lagrange multipliers, we have
p-decc
"(M;M) = inf
p;q2()sup
M2M
Ep[gM()]jEq
D2
H 
M();M()
"2	
= inf
p;q2()sup
M2Mmax
inf
0
Ep[gM()]  
Eq
D2
H 
M();M()
 "2	
;0
inf
0inf
p;q2()sup
M2Mmax
Ep[gM()]  
Eq
D2
H 
M();M()
 "2
;0	
inf
0
p-deco
(M;M)_0 +"2	
:
We now prove the inequality (39). Let 1andM2M+be ﬁxed. For integers i0, deﬁne"i= 2 i=2.
For eachi0, let(pi;qi)denote a minimizer to the following expression:
inf
pi;qi2()sup
M2Hqi;"i(M)Epi[gM()] = p-decc
"i(M;M):
Recalling that L= 2dlog 2e, set
q=1
2IM+q0++qL 1
4L+p0++pL 1
4L;andp=IM:
50Consider any M2M. We will upper bound the value
Ep[fM(M) fM()] 4LEq
D2
H 
M();M()
:
Choosej2f0;:::;L 1gas large as possible so that
Eq
D2
H 
M();M()
"2
j
4L: (105)
If such an index jdoes not exist, we must have Eq
D2
H 
M();M()
>1=(4L). In this case, we have
Ep[fM(M) fM()] 4LEq
D2
H 
M();M()
1 0p-decc
0(M;M):
Suppose going forward that 0jL 1satisfying (105) exists. If j < L 1, since we chose the
largest possible value of j, we have Eq
D2
H 
M();M()
"2
j
8L. In addition, regardless of the value of
j2f0;1;:::;L 1g, by the deﬁnition of q, we have
Eqj
D2
H 
M();M()
4LEq
D2
H 
M();M()
"2
j;
and
Epj
D2
H 
M();M()
4LEq
D2
H 
M();M()
"2
j;
that is,M2Hpj;"j(M)\Hqj;"j(M). It follows that
fM(M) fM(M)Epj
fM(M) fM()
Epj[fM(M) fM()] +"j
p-decc
"j(M;M) +"j;
where the second inequality uses that M2Hpj;"j(M)and the ﬁnal inequality uses that M2Hqj;"j(M). As
a result, we can compute
Ep[fM(M) fM()] 4LEq
D2
H 
M();M()
fM(M) fM(M) 4L
8L"2
j 1fj <L 1g
fM(M) fM(M) +p-decc
"j(M;M) +"j 
2"2
j 1fj <L 1g
1
2+
2 
fM(M) fM(M)2+p-decc
"j(M;M) +"j 
2"2
j 1fj <L 1g
1
2+Eq
D2
H 
M();M()
+p-decc
"j(M;M) +"j 
2"2
j 1fj <L 1g
1
+Eq
D2
H 
M();M()
+p-decc
"j(M;M) +"j 
2"2
j;
where the ﬁnal inequality uses that "2
L 11=2sinceL2 log(2). Rearranging, we obtain
Ep[fM(M) fM()] (4L+ 1)Eq[D2
H 
M();M()
] (106)
1
+p-decc
"j(M;M) +"j 
2"2
j
2
+p-decc
"j(M;M) 
4"2
j;
as desired.
51Proof of Proposition 4.3. Let >0andM2M+be given. Fix ">0to be chosen later, and let pbe
the minimizer for r-decc
"(M;M). Consider the value of the oﬀset DEC for M2M:
Ep
fM(M) fM() D2
H 
M();M()
:
We consider two cases. First, if M2Hp;"(M), it is immediate that
Ep
fM(M) fM() D2
H 
M();M()
Ep[fM(M) fM()]r-decc
"(M;M):
On the other hand if M =2Hp;"(M), we have Ep
D2
H 
M();M()
"2, and since gM1, we have
Ep
fM(M) fM() D2
H 
M();M()
1 "20
by choosing "= 1=2. We conclude that
r-deco
(M;M)r-decc
 1=2(M;M):
Proof of Proposition 4.4. Recall the model classes M;deﬁned in Example 5.1, parametrized by
2(0;1=2];2(0;1);A2N. Consider any choice for ,, andA; we will specify concrete values below.
Lemma E.2 gives that for all ">0,
r-decc
"(M;) = sup
M2co(M;)r-decc
"(M;[fMg;M)O"2

:
On the other hand, Lemma E.1 gives that for the choice of M=fM2M;, we have, for all  >0,
r-deco
(M;;fM)
2 + 8 4=A:
Given >0, let us choose = 1=2; = 1=p, andA= 2562=. Then the resulting model class M=M;
satisﬁes r-decc
"(M)O("21=2)for all">0, yet
sup
M2Mr-deco
(M;M)1
4 + 16 4
A1
321
1=2^1
 4
A1
641
1=2^1

( 1=2):
Proof of Proposition 4.5. Recall the deﬁnition of ]p-decc
"(M;M)in (94). Let " >0andM2M+be
given, and let (p0;q0)be minimizers for ]p-decc
"(M;M). Then for all M2Hp0;"(M)\Hq0;"(M), we have
]p-decc
"(M;M)Ep0[fM(M) fM()]
Ep0
fM(M) fM()
 "
fM(M) fM(M) ";
where the second inequality uses Lemma D.1. That is, if we deﬁne ="+]p-decc
"(M;M), we have
Hp0;"(M)\Hq0;"(M) M(M). Now, let (p;q)be minimizers for p-decc
"(M(M);M), and set q=
1
3q+1
3q0+1
3p0. We have
p-decc
"=p
3(M;M) sup
M2Hq;"=p
3(M)Ep[fM(M) fM()]
 sup
M2Hq;"(M)\Hp0;"(M)\Hq0;"(M)Ep[fM(M) fM()]
 sup
M2Hq;"(M)\M(M)Ep[fM(M) fM()]
=p-decc
"(M(M);M):
52Finally, using Lemma D.3, we have "+p-deccp
2"(M;M).
Proof of Proposition 4.6. We ﬁrst prove the following result, which does not requite any regularity
condition.
Lemma D.5. Fix anyM2M+and" > 0, and let0:="+r-decc
"(M;M). Then for any constant
Cregp
2, it holds that
r-decc
"=Creg(M;M)1
C2regr-decc
"(M;M) +r-decc
"(M0(M);M):
Proof of Lemma D.5. Given" > 0andM, letpbe the distribution that achieves the value for
r-decc
"(M;M). Then for all M2Hp;"(M), we have
r-decc
"(M;M)Ep[fM(M) fM()]
Ep
fM(M) fM()
 "
fM(M) fM(M) ";
where the second inequality uses Lemma D.1. Hence, for 0:="+r-decc
"(M;M), we haveHp;"(M)M0(M).
Now, letp0be the minimizer for r-decc
"(M0(M);M). Set p=1
C2regp+
1 1
C2reg
p0. Using that
1
C2reg
1 1
C2reg 1
1
wheneverCregp
2, we have
r-decc
"=Creg(M;M) sup
M2Hp;"=C reg(M)Ep[fM(M) fM()]
 sup
M2Hp;"(M)\Hp0;"(M)Ep[fM(M) fM()]
1
C2regsup
M2Hp;"(M)Ep[fM(M) fM()] + sup
M2Hp;"(M)\Hp0;"(M)Ep0[fM(M) fM()]
=1
C2regr-decc
"(M;M) + sup
M2Hp;"(M)\Hp0;"(M)Ep0[fM(M) fM()]
1
C2regr-decc
"(M;M) + sup
M2M0(M)\Hp0;"(M)Ep0[fM(M) fM()]
=1
C2regr-decc
"(M;M) +r-decc
"(M0(M);M):
WenowcompletetheproofofProposition4.6. Undertheassumedgrowthcondition,wehave r-decc
"=Creg(M;M)
1
c2regr-decc
"(M;M), so rearranging the result of Lemma D.5 (with 0=(")) yields
r-decc
"=Creg(M;M)1
c2reg 1
C2reg 1
r-decc
"(M(")(M);M):
The result in the proposition follows by rescaling "to"Creg.
53For use later on, we also prove the following variant of Proposition 4.6, which concerns the DEC for the class
M[fMg.
Proposition D.4 (Localization for regret DEC; global version) .Consider any set M0M+, and assume
that the strong regularity condition (43) is satisﬁed relative to M0. Then, for all " >0, letting("):=
Creg"+ supM2M0r-decc
Creg"(M[fMg;M)C2
reg 
"+ supM2M0r-decc
"(M[fMg;M)
, we have
sup
M2M0r-decc
"(M[fMg;M)Clocsup
M2M0r-decc
Creg"(M(")(M)[fMg;M);
whereCloc:=
1
c2reg 1
C2reg 1
.
Proof of Proposition D.4. Deﬁne:="+supM2M0r-decc
"(M[fMg;M). Applying Lemma D.5 to the
classM[fMgfor each choice of M2M0, we obtain that
sup
M2M0r-decc
"=Creg(M[fMg;M)1
C2regsup
M2M0r-decc
"(M[fMg;M)+ sup
M2M0r-decc
"(M(M)[fMg;M):(107)
The growth condition (43) gives that
sup
M2M0r-decc
"=Creg(M[fMg;M)1
c2regsup
M2M0r-decc
"(M[fMg;M):
Then rearranging (107) yields
sup
M2M0r-decc
"=Creg(M[fMg;M)1
c2reg 1
C2reg 1
sup
M2M0r-decc
"(M(M)[fMg;M):(108)
The result in the proposition statement follows by replacing "with"Creg.
Proof of Proposition 4.7. FixM2M+and" >0, and letp2()be a minimizer for r-decc
"(M[
fMg;M).
Fix anyM2M(M)[fMg. We bound the regret under pby considering two cases.
Case 1.IfEp
D2
H 
M();M()
"2, thenM2Hp;"(M), and it follows that Ep[fM(M) fM()]
r-decc
"(M[fMg;M).
Case 2.For the second case, suppose that Ep
D2
H 
M();M()
>"2. We now compute
Ep[fM(M) fM()]+Ep
fM(M) fM()
+Ep
fM(M) fM()
+1
2+
2Ep
(fM() fM())2
+r-decc
"(M[fMg;M) +1
2+
2Ep
D2
H 
M();M()
;
where the second inequality uses Young’s inequality. Rearranging, we obtain
Ep
fM(M) fM() D2
H 
M();M()
+r-decc
"(M[fMg;M) +1
2 
2Ep
D2
H 
M();M()
+r-decc
"(M[fMg;M) +1
2 "2
2: (109)
Recalling that Mcan be any model in M(M)[fMg, we obtain
r-deco
(M(M)[fMg;M)max
r-decc
"(M[fMg;M); +1
2+r-decc
"(M[fMg;M) "2
2
=r-decc
"(M[fMg;M) + max
0; +1
2 "2
2
:
54Proof of Proposition 4.8. We begin with the upper bound on the constrained DEC. Let ">0be ﬁxed.
Using Proposition 4.6, we have
r-decc
"(M[fMg;M)Clocr-decc
Creg"(M(M)[fMg;M);
where=Creg"+r-decc
Creg"(M[fMg;M)Creg"+c2
regr-decc
"(M[fMg;M)C2
reg 
"+r-decc
"(M;M)
(see Deﬁnition 4.1). Next, for all  >0, using Proposition 4.1, we have
r-decc
Creg"(M(M)[fMg;M)8 
r-deco
(M(M);M)_0 +C2
reg"2
+ 7Creg";
so that
r-decc
"(M[fMg;M)8Cloc 
r-deco
(M(M);M)_0 +C2
reg"2
+ 7ClocCreg":
We now set
?= (16C2
regCloc) 1"+r-decc
"(M[fMg;M)
"2;
which satisﬁes ?1
16C2regCloc"and gives
r-decc
"(M[fMg;M)8Clocr-deco
?(M(M);M)_0 +1
2r-decc
"(M[M;M) + (7ClocCreg+ 1=2)";
or after rearranging,
r-decc
"(M[fMg;M)16Clocr-deco
?(M(M);M)_0 + 2(7ClocCreg+ 1=2)":
In addition, we have
C2
reg(16C2
regCloc)?"2=C2
reg(16C2
regCloc)(";?):
To conclude, we over-bound by maximizing over ?1
16C2regCloc".
For the lower bound on the constrained DEC, it is an immediate consequence of Proposition 4.7 that for all
">0and >p
2" 1, letting="2
4,
r-deco
(M(M);M)r-decc
"(M[fMg;M) + max
+1
2 "2
2;0
=r-decc
"(M[fMg;M);
for allM2M+. Since we are free to maximize over p
2" 1, this establishes the result.
Proof of Proposition 4.9. Consider the following model class M, parametrized by 2(0;1=2):
1. =N[fg.
2.We haveM=fMaga2N. For eacha2N, the model Ma2Mhas rewards and observations deﬁned as
follows:
(a) For2N,fMa() =1
2+ 1f=ag, whilefMa() = 0.
(b) For all 2, we haver=fMa()almost surely under rMa().
(c) For2N, we receive the observation o=?.
(d)Selectinggives the observation o2f0;1gN, where for each i2N,oiBer(1=2 + 1fa=ig)
is drawn independently (thus, we have O=f0;1gN[f?g).
55Upper bound. We will show that there are constants c;C > 0so that, for ">0,
sup
M2co(M)r-decc
"(M;M) 1
"pc	
C":
Since supM2co(M)r-decc
"(M;M)for all"0(as the choice of p=I1satisﬁes Ep[fM(M) fM()]
for allM2M), it suﬃces to show that for "<pcand for any M2co(M), we have r-decc
"(M;M) = 0.
GivenM2co(M)and"1=2, we can write M() =EM0[M0()]for some2(M). We deﬁne a
distribution p2()according to the following cases:
•Ifputs mass at most 2=3on each model M2M, we deﬁne p=I.
•Otherwise, there is a unique choice for a?2[A]so that(Ma?)2=3, and in this case, we deﬁne
p=Ia?.
Now consider any model Ma2M. Consider the ﬁrst case above, and write oMa()andoM().
Note thatoaBer(1=2 +), whileoaBer(1=2 +)for some2
3. It follows that
Ep
D2
H 
Ma();M()
=D2
H 
Ma();M()
D2
H(Ber(1=2 +);Ber(1=2 +))c2;
for a numerical constant c >0. Sincec2> "2, it follows that Ma62Hp;"(M); since the choice of Mais
arbitrary, we conclude that Hp;"(M) =?in this case.
Now, consider the second case above. For a=a?, we have that Ep[fMa(Ma) fMa()]= 0:Fora6=a?,
we have that PrMa(a?)(r6= 1=2) = 0, while PrM(a?)(r6= 1=2)2=3. As a result,
Ep
D2
H 
Ma();M()
=D2
H 
Ma(a?);M(a?)
D2
H(Ber(0);Ber(2=3))4=9>"2;(110)
meaning that Ma62Hp;"(M).
Lower bound. PickA2, and letM=Unif(fMaga2[A]). Givenp2(), leta=arg mina2[A]p(a), so that
p(a)1=A. We observe that
Ep[fMa(Ma) fMa()](1 1=A)=2
and
Ep
D2
H 
Ma();M()
p()D2
H 
Ma();M()
+ 2p(a) +X
i2[A];i6=ap(i)D2
H 
Ma(i);M(i)
:
Foralli6=a,wehaveD2
H 
Ma(i);M(i)
D2
H(Ber(0);Ber(1=A))2=A,sothatP
i2[A];i6=ap(i)D2
H 
Ma(i);M(i)

2=A. As long as is a suﬃciently small numerical constant, we also have, using the tensorization property of
the squared Hellinger distance,
D2
H 
Ma();M()
D2
H(Ber(1=2 +);Ber(1=2 +=A)) + (A 1)D2
H(Ber(1=2);Ber(1=2 +=A))
c
2+ (A 1)2
A2
C2;
whereC;c> 0are numerical constants. Altogether, this gives
Ep
D2
H 
Ma();M()
p()C2+ 4=A:
We choose Alarge enough such that 4=A"2=2. There are now two cases to consider.
•Ifp()"2
2C2, thenMa2Hp;"(M), and
Ep[fMa(Ma) fMa()]
2:
56•If this is not the case, we have
Ep
fM(M) fM()
1
2p()"2
4C2^1:
By combining these cases, we conclude that there are numerical constants C;c> 0such that
r-decc
"(M[fMg;M)CI
">c3=2	
:
In particular, choosing /"2=3gives r-decc
"(M[fMg;M)
("2=3), while r-decc
"(M;M)O(").
Proof of Proposition 4.10. By Proposition D.3, we have that for all ">0andM2M+,
p-decc
"(M[fMg;M)p-decc;greedy
" (M[fMg;M) =p-decc;greedy
" (M;M)p-deccp
3"(M;M) + 4":
Proof of Proposition 4.11. By Assumption A.1, we have
r-deco
(M;M) = sup
2(M)inf
p2()Ep;M
fM(M) fM() D2
H 
M();M()
:
Observe that since Hellinger distance satisﬁes the triangle inequality, we have that for all 2,
EM;M0
D2
H(M();M0())
2EM
D2
H 
M();M()
+ 2EM0
D2
H 
M0();M()
= 4EM
D2
H 
M();M()
:
As a result, we have
r-deco
(M;M)sup
2(M)inf
p2()Ep;Mh
fM(M) fM() 
4EM0D2
H(M();M0())i
sup
2(M)sup
2(M)inf
p2()Ep;Mh
fM(M) fM() 
4EM0D2
H(M();M0())i
sup
2(M)inf
p2()sup
M2MEph
fM(M) fM() 
4EM0D2
H(M();M0())i
= sup
2(M)r-deco;rnd
=4(M;):
For the second inequality in (55), it follows immediately from convexity of squared Hellinger distance that
sup2(M)r-deco;rnd
(M;)supM2co(M)r-deco
(M;M).
We now prove (56). Let ">0be given. Since the strong regularity condition is satisﬁed relative to M+,
Proposition D.4 with M0=M+implies that
sup
M2M+r-decc
"(M[fMg;M)Clocsup
M2M+r-decc
Creg"(M(M)[fMg;M);
where:=C2
reg 
"+ supM2M+r-decc
"(M[fMg;M)
. Now, let
fM(M) =
M2MjfM(M)fM(M) +;fM(M)fM(M) +	
:
Using Lemma D.2, along with the fact that C2
reg"p
2Creg"sinceCregp
2, we have that
sup
M2M+r-decc
Creg"(M(M)[fMg;M)sup
M2M+r-deccp
2Creg"(fM(M)[fMg;M) +p
2Creg":
57LetfMbe the model inM+that attains the maximum in the right-hand side above, and set M0=fM(fM).
Let >0be ﬁxed. Using Proposition 4.1, we have
r-deccp
2Creg"(M0[ffMg;fM)8 sup
M2M+r-deco
(M0;M)_0 + 16C2
reg"2+ 7p
2Creg":
By Eq. (55), we have
sup
M2M+r-deco
(M0;M) sup
2(M0)r-deco;rnd
=4(M0;):
Consider any 2(M0)and letM:=EM0[M0]2co(M0)co(M). Observe that if M2M0=fM(fM),
then
fM(M)ffM(fM) +EM0
fM0(fM)
+ 2max
2EM0
fM0()
+ 2=fM(M) + 2:
Hence,M0M 2(M), and we have the upper bound
sup
2(M0)r-deco;rnd
=4(M0;)sup
2(M)r-deco;rnd
=4(M2(M);):
For any2(M), Proposition D.1 implies that
r-deco;rnd
=4(M2(M);)r-decc;rnd
4p
=(M[fMg;) +2
:
Putting everything together, this establishes that for all  >0,
sup
M2M+r-decc
"(M[fMg;M)Cloc 
8 sup
2(M)r-decc;rnd
4p
=(M[fMg;) + 8p
2Creg"+ 16C2
reg"2+16
!
Cloc 
8 sup
2(M)r-decc;rnd
4p
=(M[fMg;) + 24C2
reg"2+24
!
:
We choose =1
24ClocC4reg
"2. Since"2=", this gives
sup
M2M+r-decc
"(M[fMg;M)c1sup
2(M)r-decc;rnd
c2"(M[fMg;) +1
2C2reg+c4";
c1sup
2(M)r-decc;rnd
c2"(M[fMg;) +1
2sup
M2M+r-decc
"(M[fMg;M) +c3";
wherec1;c2;c3;c4>0are constants that depend only on CregandCloc. Rearranging yields the ﬁrst inequality
in Eq. (56); the second inequality now follows from Jensen’s inequality.
E Omitted Proofs from Section 5
Proof of Corollary 5.1. The lower bound is an immediate corollary of Proposition 4.8, so let us prove the
upper bound. Let ">0be ﬁxed. Using Proposition 4.6, we have
r-decc
"(M;M)Clocr-decc
Creg"(M(M);M);
where=C2
reg 
"+r-decc
"(M;M)
. Recall that we assume Creg;Cloc=O(1). Hence, for all  >0be ﬁxed,
using Proposition 4.1, we have
r-decc
Creg"(M(M);M)O 
r-deco
(M(M);M)_0 +"2+"
:
58Proposition 4.1 also gives
=O("+r-decc
"(M;M))O("+r-deco
(M;M)_0 +"2)O(r-deco
(M;M)_0 +"2+ 1) =(";);
where the second inequality is AM-GM. This establishes the result.
Proof for Example 5.1.
We ﬁrst lower bound the quantity in Eq. (59), then prove an upper bound on the regret of E2D+.
Lower bound on oﬀset DEC and regret bound from Eq. (59). We start with a basic lower bound on the oﬀset
DEC for the class M;.
Lemma E.1. Let2(0;1=4),2(0;1)andA2be given. For all  >0,
r-deco
(M;;fM) =r-deco
(M;
(fM);fM)
2 + 8 4=A:
We now prove a lower bound on the quantity
R:= min
>0max
sup
M2co(M)r-deco
(M(;T)(M);M)T; logjMj
appearing in Eq. (59). We begin by lower bounding the localization radius
(;T) = 

T+r-deco
(M)
:
We choose 1= 1=2andA=T2, so that whenever Tis a suﬃciently large constant, Lemma E.1 gives
(;T)

T+r-deco
(M1;;fM)


T+1
1 + 4=A


T+1
1 +

r1
T^1
:
It follows that as long as 1=T, if we set
2=cr1
T;
wherecis a suﬃciently small numerical constant, then regardless of how is chosen,
M2;(fM)M(;T)(fM);
and
Rmin
>0max
r-deco
(M2;(fM);fM)T; 
:
Applying Lemma E.1 once more, we have
R

min
>02T
1 ++

 
2T^s
2T
!


 1=2T1=2^ 3=4T1=4
:
We set=T 1=2, which gives
R
(T5=8);
as desired.
Upper bound on constrained DEC and regret of E2D+.We now bound the regret of E2D+via Theorem 3.2.
We ﬁrst bound the constrained DEC.
59Lemma E.2. Let2(0;1)be given, and letMall:=[2(0;1=2]M;. Then for all ">0,
r-decc
"(M)r-decc
"(Mall)O"2

:
Let/T 1=2andA/T2as in the prequel. Plugging the bound from Lemma E.2 into Theorem 3.2 (cf. Eq.
(91)) gives
E[RegDM(T)]eO"(T)2
T+p
T
=eOp
T
;
since, with the usual choice of estimation oracle, we can take "(T)eOq
logjMj
T
andlogjMjO(log(A))
O(log(T)).
Proof of Lemma E.1. We ﬁrst remark that M;=M;
(fM), sinceffM() = 1=2for all2[A], and all
M2M;havefM(M)1=2 +.
We now lower bound the value of the oﬀset DEC. Consider any distribution p2(), and leti:=
arg mini2[A]p(i), so thatp(i)1=A. We have
Ep
fM;i(M;i) fM;i()
(1 1=A p()) +1
4p()
2;
since1=4andA2. We now bound the Hellinger distance via
Eph
D2
H
M;i();fM()i
p()D2
H
M;i();fM()
+ 2p(i):
Observe that D2
H
M;i();fM()
2andp(i)1=A, so that
Eph
D2
H
M;i();fM()i
2p() + 2=A:
Combining the calculations so far gives
Eph
fM;i(M;i) fM;i() D2
H
M;i();fM()i

2 2p() 4=A:
On the other hand, by choosing M=fM, we have
Eph
ffM(fM) ffM() D2
H
fM();fM()i
=1
2p();
so that
r-deco
(M;;fM)min
p2()max
2 2p();1
2p()
 4=A;

2 + 8 4=A:
Proof of Lemma E.2. LetM2co(Mall)and"1=10be given. Assume that 25"2
1=2, as the result
is trivial otherwise.
60Leti=arg maxi2[A]PoM()(o=i)and setp= (1 25"2
)q+ 25"2
I, whereq2([A])is another
distribution whose value will be chosen shortly. We ﬁrst observe that if M;j2M;Mallfor some>0
andj6=i, then since PoM()(o=j)=2, we have
D2
H 
M;j();M()
q
PoM;j()(o=j) q
PoM()(o=j)2
(p
 p
=2)2
20;
where we have used that PoM;j()(o6=?) =PoM()(o6=?) =, sinceM2co(Mall). It follows that
regardless of how q2([A])is chosen,M;j=2Hp;"(M), since
Ep
D2
H 
M;j();M()
25
20"2>"2:
Hence, if we deﬁne M0
i=fM;i2Mallj2(0;1=2]g, we haveHp;"(M)[fMgM0
i[ffMg[fMg, and
it remains to choose qsuch that the regret on all of these models is small. We note that Ep
gfM
25"2

regardless of how qis chosen, so we restrict our attention to MandM0
igoing forward.
LetM?= arg minM2M0
iD2
H 
M(i);M(i)
. We will show that
fM(M) fM(i)D2
H 
M?(i);M(i)
: (111)
To establish this fact, ﬁrst note that if M=i, then (111) is immediate. Otherwise, let M2(Mall)be
such thatM() =EM0M[M0()]for all2, and then
fM(M) fM(i) = max
2[A]EM0M
fM0() fM0(i)
1
2PM0M(M062M0
i)1
2PrM(i)(r= 1=2);
(112)
where the ﬁnal inequality follows since all models M02MallnM0
isatisfyr= 1=2a.s. whenrM0(i). Recall
the elementary fact that for all events Aand distributions PandQ.
(P(A) Q(A))2
P(A) +Q(A)2D2
H(P;Q): (113)
SinceM?2M0
i, we have PrM?(i)(r= 1=2) = 0, and so, using (113), it follows that
PrM(i)(r= 1=2)2D2
H 
M(i);M?(i)
;
and combining with (112) establishes (111).
To proceed, we choose q2([A])by settingq(i) =4"2
D2
H(M?(i);M(i))^1, andq(M) = 1 q(i). We consider
two cases
•Ifq(i) = 1, it is immediate that for all M2M0
i,Ep[gM()]25"2
. In addition,
4"2
D2
H 
M?(i);M(i)1;
so (111) implies that
fM(M) fM(i)4"2:
It follows that Ep
gM()
25"2
+ (fM(M) fM(i))25"2
+ 4"2.
•Ifq(i)<1, then for all M2M0
i,
Ep
D2
H 
M();M()
1
2q(i)D2
H 
M(i);M(i)
1
2q(i)D2
H 
M?(i);M(i)
= 2"2;
61soM =2Hp;"(M). It follows thatHp;"(M)\M0
i=?. All that remains is to bound the regret under M,
which we do as follows:
Ep[gM()]q(i)(fM(M) fM()) + 25"2
q(i)D2
H 
M?(i);M(i)
+ 25"2

= 4"2+ 25"2
:
Putting the cases above together, we conclude that
sup
M2co(Mall)r-decc
"(Mall[fMg;M)O"2

:
62