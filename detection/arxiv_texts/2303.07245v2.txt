arXiv:2303.07245v2  [cs.IT]  30 Oct 20231
Concentration without Independence
via Information Measures
Amedeo Roberto Esposito, Marco Mondelli
Institute of Science and Technology Austria
{amedeoroberto.esposito, marco.mondelli }@ist.ac.at
Abstract
We propose a novel approach to concentration for non-indepe ndent random variables. The main idea is to “pretend”
that the random variables are independent and pay a multipli cative price measuring how far they are from actually being
independent. This price is encapsulated in the Hellinger in tegral between the joint and the product of the marginals,
which is then upper bounded leveraging tensorisation prope rties. Our bounds represent a natural generalisation of
concentration inequalities in the presence of dependence: we recover exactly the classical bounds (McDiarmid’s
inequality) when the random variables are independent. Fur thermore, in a “large deviations” regime, we obtain
the same decay in the probability as for the independent case , even when the random variables display non-trivial
dependencies. To show this, we consider a number of applicat ions of interest. First, we provide a bound for Markov
chains with ﬁnite state space. Then, we consider the Simple S ymmetric Random Walk, which is a non-contracting
Markov chain, and a non-Markovian setting in which the stoch astic process depends on its entire past. To conclude,
we propose an application to Markov Chain Monte Carlo method s, where our approach leads to an improved lower
bound on the minimum burn-in period required to reach a certa in accuracy. In all of these settings, we provide a
regime of parameters in which our bound fares better than wha t the state of the art can provide.
Index Terms
Concentration, dependent random variables, large deviati ons, information measures, Hellinger integral, Markov
chains, McDiarmid’s inequality, hypercontractivity
I. I NTRODUCTION
It is well-known that, given a sequence Xn= (X1,...,X n)of independent, but not necessarily identically
distributed, random variables with joint measure PXn, one can prove that for every function fsatisfying proper
Lipschitz assumptions:
PXn(|f−PXn(f)| ≥t)≤2exp/parenleftigg
−t2
k/⌊ard⌊lf/⌊ard⌊l2
Lip/parenrightigg
. (1)
Here,/⌊ard⌊lf/⌊ard⌊l2
Lipdepends on the metric structure of the measure space, and kis a constant depending on the approach
used to prove the inequality, e.g., transportation-cost inequalities, log-Sobolev inequal ities, martingale method, see
the survey [1]. One notable example is McDiarmid’s inequali ty for functions with “bounded jumps”: i.e., if for
everyxn,ˆxand every 1≤i≤none has that
|f(x1,...,x i,...,x n)−f(x1,...,ˆx,...,x n)| ≤ci, (2)
October 31, 2023 DRAFT2
then the following holds [2]:
PXn(|f−PXn(f)| ≥t)≤2exp/parenleftbigg
−2t2
/summationtextn
i=1c2
i/parenrightbigg
. (3)
This represents the “golden standard” of concentration. In terestingly, as underlined above, McDiarmid’s inequality
does not require the Xi’s to be identically distributed; it does, however, require the random variables to be
independent . Most of the methods in the literature that tried to relax the latter assumption required the development
of novel techniques. However, existing results generally d o not recover the rate of decay provided in the independent
setting.
In this paper, we present a novel approach that outperforms t he state of the art in various settings and regimes.
Speciﬁcally, we show improved bounds for ﬁnite-state space Markov chains (Section IV-A), the Simple Symmetric
Random Walk (SSRW, Section IV-B), a non-Markovian process ( Section IV-C), and Monte Carlo Markov Chain
(MCMC, Section IV-D). In the case of the SSRW, our improvemen ts are the most dramatic: in sharp contrast with
existing techniques, we are able to capture the correct scal ing between the distance from the average t, the number
of variables n, and the decay probability in the concentration bound. We re mark that our new method – based on
a change of measure argument – is rather ﬂexible and can be emp loyed in most settings. In fact, it only requires
the absolute continuity between the joint and the product of the marginals, while existing approaches generally
have more restrictive assumptions ( e.g., Markovianity with stationary distribution [3] or contrac tivity [4, 5]). The
key idea is to shift the focus from proving concentration to b ounding an information measure ( i.e., the Hellinger
integral, see Deﬁnition 2) between the joint distribution a nd the product of the marginals. Crucially, the Hellinger
integral satisﬁes tensorisation properties that allow us t o easily upper bound it, even in high-dimensional settings
(see Appendix D). We highlight that our approach provides a natural generalisation of the existing concentration of
measure results to dependent random variables, in the sense that we recover exactly McDiarmid’s inequality when
the random variables are independent. Furthermore, for suf ﬁciently large t, namely, in a “large deviations” regime,
we approach the decay rate (3) for the independent case, even when the random variables are actually dependent.
The rest of the paper is organized as follows. In Section I-A, we discuss related work in the area. in Section II
we cover the preliminaries, namely, information measures ( Section II-A), Markov kernels (Section II-B), and strong
data-processing inequalities (SDPIs, Section II-C). We th en provide the main result of this work in Section III,
which is then applied in Section IV to four different setting s: ﬁnite-state space Markov chains (Section IV-A), the
SSRW (Section IV-B) a non-Markovian stochastic process (Se ction IV-C), and MCMC methods (Section IV-D).
Concluding remarks are provided in Section V. Part of the pro ofs and additional discussions are deferred to the
appendices.
A. Related Work
The problem of concentration for dependent random variable s has been addressed in multiple ways. The ﬁrst
results in the area are due to Marton [4–7] who heavily relied on transportation-cost inequalities (Pinsker-like
inequalities) and an elegant mixture of information-theor etic and geometric approaches. Another important contri-
bution, building upon Marton’s work, was given by Samson in [ 8] where some of Marton’s results were extended
October 31, 2023 DRAFT3
to include Φ-mixing processes. More recent advances, complementing an d generalising the work by Samson and
Marton, were provided in [9], where the Martingale method wa s employed to prove concentration for dependent (but
deﬁned on a countable space) processes and in [10, 11], where the idea of couplings was exploited. In particular,
the results derived in [10] are equivalent to the ones advanc ed in [9] but obtained through couplings rather than
linear programming. Moreover, [11] leverages Marton’s cou pling. For an extensive treatise on Marton’s coupling
please refer to [12, Chapter 8]. All of these approaches meas ure the degree of dependence by looking at distances
between conditional distributions (organised in matrices whose norms are then computed, see Equation (77)) or by
constructing “minimal couplings” between conditional dis tributions (see Equation (78)). The resulting quantities,
which are necessary in order to analyse the corresponding pr obabilities, can be difﬁcult to compute, especially
in non-Markovian settings. Another approach similar in spi rit to ours is given in [3], where a generalisation of
Hoeffding’s inequality for stationary Markov chains is pro vided. Other related work can be found in [13–17]. These
results aim to establish Hoeffding-like inequalities for M arkov chains by relating it to a different Markov chain
whose cumulant generating function can be bounded under dif ferent assumptions: [13–16] are restricted to discrete
and ergodic Markov chains, while [17] extends to general sta te-space but requires geometric ergodicity. Yet another
approach in providing exponential concentration for geome trically ergodic Markov chains can be found in [18, 19].
All these generalisations of Hoeffding’s inequality do not , however, allow for arbitrary functions of a sequence of
random variables, but they are restricted to (sums of) bound ed functions applied to each individual sample and
they all require the existence of a stationary distribution . Given that the approach presented in [3] is more general
than the one proposed in [13–17], our results will be compare d directly with [3]. Another (less related) approach
can be found in [20], where the strength of the dependence is m easured in a different way with respect to both
this work and the related work mentioned above. Moreover, th e approach in [20] is mostly restricted to empirical
averages of bounded random variables and includes an additi onal additive factor that grows with the number of
samples. Exponential bounds for stochastic chains of unbou nded memory on countable alphabets are instead given
in [21]. Additionally, an approach that leverages the Kullb ack-Leibler divergence can be found in [22, 23]. Finally,
we remark that [24] exploits a technique similar to what is pu rsued in this work, in order to extend McDiarmid’s
inequality to the case where the function fdepends on the random variables themselves (while the rando m variables
remain, in fact, independent). Said result was then applied to a learning setting.
II. P RELIMINARIES
In this section, we will deﬁne the main objects utilised thro ughout the document and deﬁne the relevant notation.
We will adopt a measure-theoretic framework. Given a measur able space (X,F)and two measures µ,ν which
render it a measure space, if νis absolutely continuous with respect to µ(denoted with ν≪µ), then we will
represent withdν
dµthe Radon-Nikodym derivative of νwith respect to µ. Given a (measurable) function f:X →R
and a measure µ, we denote with µ(f) =/integraltext
fdµthe Lebesgue integral of fwith respect to the measure µ. The
Radon-Nikodym derivatives represent the main building blo ck of the following fundamental objects.
October 31, 2023 DRAFT4
A. Hellinger integral, α-norm and R ´enyi’sα-divergence
An important ingredient of this work is information measure s. In particular, we will focus on Hellinger integrals
which can be seen as a transformation of the Lα-norms and of R´ enyi’s α-divergences. Let us introduce them and
then show the relationships with other well-known objects i n the literature. Hellinger integrals can be seen as a
ϕ-Divergence with a speciﬁc parametrised choice of ϕ[25].
Deﬁnition 1 (ϕ-divergences) .Let(Ω,F,P),(Ω,F,Q)be two probability spaces. Let ϕ:R+→Rbe a convex
function such that ϕ(1) = 0 . Consider a measure µsuch that P ≪µandQ ≪µ. Denoting with p,qthe densities
of the measures with respect to µ, theϕ-divergence of PfromQis deﬁned as
Dϕ(P/⌊ard⌊lQ) :=/integraldisplay
qϕ/parenleftbiggp
q/parenrightbigg
dµ. (4)
Particularly relevant to us will be the family of parametris ed divergences that stems from ϕα(x) =xαforα >1.
The function ϕα(x)is convex on the positive axis for every α >1. However, it does not satisfy the property that
ϕ(1) = 0 . Said requirement can be lifted with the consequence of losi ng the property that Dϕ(ν/⌊ard⌊lµ) = 0 if and only
ifν=µ. We will call the family of divergences stemming from such fu nctions the Hellinger integrals of orderα.
Deﬁnition 2 (Hellinger integrals) .Let(Ω,F,ν),(Ω,F,µ)be two probability spaces, and let ϕα:R+→Rbe
deﬁned as ϕα(x) =xα. Letµandνbe two probability measures such that ν≪µ, then the Hellinger integral of
orderαis given by
Hα(ν/⌊ard⌊lµ) :=Dϕα(ν/⌊ard⌊lµ) =/integraldisplay/parenleftbiggdν
dµ/parenrightbiggα
dµ. (5)
Let us highlight that we are not considering Hellinger diver gences of order α(including the so-called χ2-
divergence) which consist of divergences stemming fromxα−1
(α−1), but rather a transformation of said family. In fact,
the Hellinger divergences are equal to 0if and only if the measures coincide. In contrast, the Hellin ger integral is
equal to1if the two measures coincide.
Remark 1 (ϕ-Divergences) .Despite the fact that Deﬁnition 1 uses a reference measure µand the densities with
respect to this measure, ϕ-divergences can be shown to be independent from the dominat ing measure. In fact, when
absolute continuity between P,Qholds, i.e.,P ≪ Q ,1we obtain [25]
Dϕ(P/⌊ard⌊lQ) =/integraldisplay
ϕ/parenleftbiggdP
dQ/parenrightbigg
dQ. (6)
Moreover, ϕ-Divergences can be seen as a generalisation of well-known o bjects like the Kullback-Leibler Diver-
gence. Indeed, the KL-divergence is retrieved by setting ϕ(t) =tlog(t). Other common examples are the Total
Variation distance ( ϕ(t) =1
2|t−1|), the Hellinger distance ( ϕ(t) = (√
t−1)2), and Pearson χ2-divergence
(ϕ(t) =t2−1). We remark that ϕ-divergences do not include the family of R´ enyi’s α-divergences.
1We will make this assumption throughout the paper.
October 31, 2023 DRAFT5
Hellinger integrals, other than belonging to the family of ϕ-Divergences can also be related to other notable
objects, namely: R´ enyi Divergences of order αandLα-norms [26, 27]:
Deﬁnition 3 (R´ enyi divergences) .Let(Ω,F,P),(Ω,F,Q)be two probability spaces. Let α >0be a positive real
number different from 1. Consider a measure µsuch that P ≪µandQ ≪µ(such a measure always exists, e.g.,
µ= (P+Q)/2)) and denote with p,qthe densities of P,Qwith respect to µ. Then, the α-divergence of Pfrom
Qis deﬁned as
Dα(P/⌊ard⌊lQ) :=1
α−1log/integraldisplay
pαq1−αdµ. (7)
Remark 2.Deﬁnition 3 is independent of the chosen measure µ. In fact,/integraltext
pαq1−αdµ=/integraltext/parenleftig
q
p/parenrightig1−α
dPand,
whenever P ≪ Q or0< α <1, we have/integraltext
pαq1−αdµ=/integraltext/parenleftig
p
q/parenrightigα
dQ, see [27]. Furthermore, it can be shown that,
ifα >1andP /ne}ationslash≪ Q , thenDα(P/⌊ard⌊lQ) =∞. The behavior of the measure for α∈ {0,1,∞} can be deﬁned by
continuity. These objects can also be seen as a generalisati on of the Kullback-Leibler Divergence. Indeed, one has
thatD1(P/⌊ard⌊lQ) =D(P/⌊ard⌊lQ)which denotes the KL-divergence between PandQ; furthermore, if D(P/⌊ard⌊lQ) =∞or
there exists β >1such that Dβ(P/⌊ard⌊lQ)<∞, thenlimα↓1Dα(P/⌊ard⌊lQ) =D(P/⌊ard⌊lQ)[27, Theorem 5]. For an extensive
treatment of α-divergences and their properties, we refer the reader to [2 7].
Going back to the Hellinger integral, the following relatio nship holds:
Hα(ν/⌊ard⌊lµ) =/vextenddouble/vextenddouble/vextenddouble/vextenddoubledν
dµ/vextenddouble/vextenddouble/vextenddouble/vextenddoubleα
Lα(µ)= exp((α−1)Dα(ν/⌊ard⌊lµ)), (8)
where/vextenddouble/vextenddouble/vextenddoubledν
dµ/vextenddouble/vextenddouble/vextenddouble
Lα(µ)denotes the Lα-norm of the Radon-Nikodym derivative with respect to the me asureµ.
B. Markov kernels
Most of the comparisons with the state of the art will be drawn in Markovian settings. In this section, we will
deﬁne the main objects necessary in order to carry out said co nfrontation.
Deﬁnition 4 (Markov kernel) .Let(Ω,F)be a measurable space. A Markov kernel Kis a mapping K:F ×Ω→
[0,1]such that:
1) for every x∈Ω, the mapping E∈ F →K(E|x)is a probability measure on (Ω,F);
2) for every E∈ F the mapping x∈Ω→K(E|x)is anF-measurable real-valued function.
A Markov kernel can be seen as acting on measures “from the rig ht”,i.e., given a measure µon(Ω,F),
µK(E) =µ(K(E|·)) =/integraldisplay
dµ(x)K(E|x), (9)
and on functions “from the left”, i.e., given a function f: Ω→R,
Kf(x) =/integraldisplay
dK(y|x)f(y). (10)
Given a sequence of random variables (Xn)n∈N, one says that it represents a Markov chain if, given i≥1, there
exists a Markov kernel Kisuch that for every measurable event E:
P(Xi∈E|X1,...,X i−1) =P(Xi∈E|Xi−1) =Ki(E|Xi−1) almost surely . (11)
October 31, 2023 DRAFT6
If for every i≥1,Ki=Kfor some Markov kernel K, then the Markov chain is said to be time-homogeneous.
Whenever the index is suppressed from K, we will be referring to a time-homogeneous Markov chain. Th e kernel
Kof a Markov chain describes the probability of getting from xtoEin one step, i.e., for every i≥1,K(E|x) =
P(Xi∈E|Xi−1=x). One can then deﬁne (inductively) the κ-step kernel Kκas follows:
Kκ(E|x) =/integraldisplay
Kκ−1(E|y)dK(y|x). (12)
Note that Kκis also a Markov kernel, and it represents the probability of getting from xtoEinκsteps:
Kκ(E|x) =P(Xκ+1∈E|X1=x). If(Xn)n∈Nis the Markov chain associated to the kernel KandX0∼µ, then
µKmdenotes the measure of Xm+1at everym∈N. Furthermore, a probability measure πis a stationary measure
forKifπK(E) =π(E)for every measurable event E. We also note that, if the state space is discrete, then K
can be represented using a stochastic matrix.
Given this dual perspective on Markov operators (acting on m easures or functions), one can then study their
contractive properties. In particular, let us deﬁne
/⌊ard⌊lK/⌊ard⌊lα→α:= sup
f/ne}ationslash=0/⌊ard⌊lKf/⌊ard⌊lα
/⌊ard⌊lf/⌊ard⌊lα. (13)
Then, Markov kernels are generally contractive [28], meani ng that/⌊ard⌊lK/⌊ard⌊lα→α≤1and, consequently, /⌊ard⌊lKf/⌊ard⌊lα≤ /⌊ard⌊lf/⌊ard⌊lα
for every f. Similarly, given γ≤α, one can deﬁne the following quantity
/⌊ard⌊lK/⌊ard⌊lα→γ:= sup
f/ne}ationslash=0/⌊ard⌊lKf/⌊ard⌊lα
/⌊ard⌊lf/⌊ard⌊lγ. (14)
It has been proven that many Markovian operators are hyper-c ontractive [28–30], meaning that /⌊ard⌊lK/⌊ard⌊lα→γ≤1for
someγ < α . Given a kernel Kandα >1, we denote by γ⋆
K(α)the smallest γsuch that Kis hyper-contractive,
i.e., such that /⌊ard⌊lK/⌊ard⌊lα→γ≤1. Said coefﬁcient has been characterised for some Markov ope rators [28, 30]. In case the
Markov kernel is not time-homogeneous, in order to simplify the notation, instead of denoting the corresponding
coefﬁcient with γ⋆
Ki(α), we will simply denote it with γ⋆
i(α).
Given a Markov kernel Kand a measure µ, one can also deﬁne the adjoint/dual operator (or backward c hannel)
K←as the operator such that /an}⌊ra⌋ketle{tg,Kf/an}⌊ra⌋ketri}ht=/an}⌊ra⌋ketle{tK←g,f/an}⌊ra⌋ketri}htfor allgandf[31, Eq. (1.1)]. While one can deﬁne dual
Markovian operators more generally, here we will focus on di screte settings where they can be explicitly speciﬁed
viaKandµ[31, Eq. (1.2)]:
K←
µ(y|x) =K(y|x)µ(x)
µK(y). (15)
C. Strong Data-Processing Inequalities
An important property shared by divergences is the Data-Pro cessing Inequality (DPI): given two measures µ,ν
and a Markov kernel K, one has that, for every convex ϕ,
Dϕ(νK/⌊ard⌊lµK)≤Dϕ(ν/⌊ard⌊lµ). (16)
This property holds as well for R´ enyi’s α-divergences, despite them not being a ϕ-divergence [27, Theorem 9].
DPIs represent a widely used tool and a line of work has focuse d on tightening them. In particular, in many settings
of interest, given a reference measure µ, one can show that Dϕ(νK/⌊ard⌊lµK)is strictly smaller than Dϕ(ν/⌊ard⌊lµ)unless
October 31, 2023 DRAFT7
ν=µ. Furthermore, the characterization of the ratio Dϕ(νK/⌊ard⌊lµK)/Dϕ(ν/⌊ard⌊lµ)has lead to the study of “strong
Data-Processing Inequalities” [31, Deﬁnition 3.1].
Deﬁnition 5 (Strong Data-Processing Inequalities) .Given a probability measure µ, a Markov kernel Kand a
convex function ϕ, we say that Ksatisﬁes a ϕ-type Strong Data-Processing Inequality (SDPI) atµwith constant
c∈[0,1)if
Dϕ(νK/⌊ard⌊lµK)≤c·Dϕ(ν/⌊ard⌊lµ), (17)
for allν≪µ. The tightest such constant cis denoted by
ηϕ(µ,K) = sup
ν/ne}ationslash=µDϕ(νK/⌊ard⌊lµK)
Dϕ(ν/⌊ard⌊lµ),
ηϕ(K) = sup
µηϕ(µ,K).
Example 1 (SDPI for the KL and the BSC) .Letµ=Ber(1/2),ǫ <1
2andK=BSC(ǫ),i.e.,K(y|x) =ǫif
x=yandK(y|x) = 1−ǫotherwise. Then, one has that ηxlogx(µ,K) = (1−2ǫ)2[28], which implies that
ηxlogx(µ,K)<1for allǫ >0.
Whileηϕcan be a difﬁcult object to compute even for simple channels, some universal upper and lower bounds
are known [31, Theorems 3.1, 3.3]:
ηϕ(K)≤sup
x,ˆx/⌊ard⌊lK(·|x)−K(·|ˆx)/⌊ard⌊lTV=η|x−1|(K) =ηTV(K), (18)
ηϕ(µ,K)≥η(x−1)2(µ,K) =ηχ2(µ,K). (19)
We remark that these bounds hold for functions ϕsuch that ϕ(1) = 0 or, equivalently, when the divergence Dϕ(ν/⌊ard⌊lµ)
is deﬁned to be µ/parenleftig
ϕ/parenleftig
dν
dµ/parenrightig/parenrightig
−ϕ(1). For general convex functions ϕ, as well as for R´ enyi’s divergences, the DPI
holds and SDPI constants are still deﬁned analogously, howe ver one cannot use common techniques to bound said
quantities. The following counter-example highlights the issue.
Example 2 (Counter-example for Hellinger integrals and R´ enyi’s di vergences) .Letν= (1/3,2/3)andK1=
BSC(1/3). Then, the stationary distribution πis given by (1/2,1/2)andνK1= (5/9,4/9).A direct calculation
gives that H2(νK1/⌊ard⌊lπK1) =82
81andH2(ν/⌊ard⌊lπ) =10
9. Moreover, if K=BSC(λ), one has that ηTV(K) =|1−2λ|
(see [31, Remark 3.1] and Equation (18)). Thus,
H2(νK1/⌊ard⌊lπK1)
H2(ν/⌊ard⌊lπ)=41
45> ηTV(K1) =1
3, (20)
which means that the inequality (18) is violated. This is due to the fact that ϕ2(x) =x2is not equal to 0atx= 1.
In fact, renormalising H2leads to the χ2-divergence, which satisﬁes
H2(νK1/⌊ard⌊lπK1)−1
H2(ν/⌊ard⌊lπ)−1=χ2(νK1/⌊ard⌊lπK1)
χ2(ν/⌊ard⌊lπ)=1
9< ηTV(K1) =1
3. (21)
Similarly, let K2=BSC(1/5), which gives that ηTV(K2) = 3/5. Consider now Dα(K2(·|0)/⌊ard⌊lπ) =Dα(δ0K2/⌊ard⌊lπK) =
1
α−1log(21−α(0.2α+(0.8)α)). Moreover, Dα(δ0/⌊ard⌊lπ) = log(2) . Thus, by setting α= 6, one has that
ηDα(K2)>Dα(δ0K2/⌊ard⌊lπK2)
Dα(δ0/⌊ard⌊lπ)= 0.6138> ηTV(K2) = 0.6, (22)
October 31, 2023 DRAFT8
which violates again the inequality (18).
III. M AIN RESULT
Theorem 1. LetPXnbe the joint distribution of (X1,...,X n),PXithe marginal corresponding to Xi, and
P/circlemultiplytextn
i=1Xithe joint measure induced by the product of the marginals. If PXn≪ P/circlemultiplytextn
i=1Xi, for any function f
satisfying Equation (2), anyt >0andα >1, one has
PXn/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftbigg−2t2
β/summationtextn
i=1c2
i/parenrightbigg
H1
αα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi). (23)
Moreover, one can further upper-bound Equation (23) as follows for a general measure PXn
PXn/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftbigg−2t2
β/summationtextn
i=1c2
i/parenrightbigg
·n/productdisplay
i=2max
xi−1H1
αα(PXi|Xi−1=xi−1/⌊ard⌊lPXi), (24)
while, if (X1,...,X n)are Markovian under PXn, i.e.,PXi|Xi−1=PXi|Xi−1almost surely, then the following
holds:
PXn/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftbigg−2t2
β/summationtextn
i=1c2
i/parenrightbigg/parenleftiggn/productdisplay
i=2Hα
i/parenrightigg1
α
, (25)
withβ=α/(α−1),Hα
i=P1
βi−1
Xi−1/parenleftbigg
Hβi−1
αiααi(PXi|Xi−1/⌊ard⌊lPXi)/parenrightbigg
,αi>1fori≥0,β0= 1,αn= 1 , and
βi=αi/(αi−1).
The proof of Theorem 1 is in Appendix A. If the function fsatisﬁes Equation (2) with ci=1
n, like in the case
of the empirical mean, one obtains
PXn/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftbigg
−n/parenleftbigg2t2
β−1
nαlogHα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi)/parenrightbigg/parenrightbigg
. (26)
This means that if
t >/radicalbigg
β
2nαlnHα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi), (27)
then Theorem 1 guarantees an exponential decay. If the sign o f the inequality (27) is reversed, then the bound actually
becomes trivial, for nlarge enough. The threshold behavior just described charac terises the main difference of this
bound with respect to existing approaches: while there are n o restrictive assumptions required (other than absolute
continuity of the measures at play), the bound can be trivial if the joint distribution is “too far” from the product of
the marginals. In contrast, other approaches, like the one d escribed in [9], do not generally exhibit such a behaviour.
Next, we will characterize the key quantity Hα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi)as a function of nin the concrete examples of
Section IV. Before doing that, a few additional remarks are i n order.
Remark 3 (Simpliﬁcation of the bound) .The expression on the RHS of Equation (25) can be complicated to
compute, especially due to the presence of {αi}∞
i=2. Making a speciﬁc choice, which meaningfully reduces the
number of parameters ( i.e., takingαi→1for every i≥2), Equation (25) boils down to the following, simpler,
expression:
PXn/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftbigg−2t2
β/summationtextn
i=1c2
i/parenrightbigg
·n/productdisplay
i=2max
xi−1H1
αα(PXi|Xi−1=xi−1/⌊ard⌊lPXi). (28)
October 31, 2023 DRAFT9
Moreover, Equation (28) can be re-written as follows:
PXn/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftigg
1
β/parenleftigg
−2t2
/summationtextn
i=1c2
i+n/summationdisplay
i=2max
xi−1Dα(PXi|Xi−1=xi−1/⌊ard⌊lPXi)/parenrightigg/parenrightigg
. (29)
Equation (29) allows to exploit the SDPI coefﬁcient for Dα(see Remark 7), which in some settings improves
upon leveraging Equation (28) along with hypercontractivi ty, see Appendix E-E.
Remark 4 (Concentration without independence for a general event E).Theorem 1 can be proved in more generality.
Indeed, for any measurable event E, one can say that, for every α >1,
PXn(E)≤ P1
β/circlemultiplytextn
i=1Xi(E)·Hα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi). (30)
Thus, our framework is notrestricted to a McDiarmid-like setting, but it can be used to generalise anyconcentration
of measure approach to dependent random variables. The idea is that concentration holds when random variables
are independent, namely, P/circlemultiplytextn
i=1Xi(E)decays exponentially in nunder suitable assumptions. Then, Equation (30)
shows that a similar exponential decay holds also in the pres ence of dependence, as long as the measure of the joint
is not “too far” from the product of the marginals. The “dista nce” between joint and product of the marginals is
captured by the Hellinger integral Hα. In particular, if the joint measure corresponds to the product of the marginals,
thenH1
αα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi) = 1 for every α. Thus, taking the limit of α→ ∞ , one recovers
P/circlemultiplytextXi(E) =PXn(E)≤ P/circlemultiplytextXi(E). (31)
Remark 5 (Choice of α).On the RHS of both Equations (23) and (28), the probability te rm is raised to the power
α−1
αand multiplied by the α-norm of the Radon-Nikodym derivative. On the one hand, as αgrows, the α-norm
grows as well, which increases the Hellinger integral; on th e other hand, as αgrows,α−1
αtends to1, which reduces
the probability. This introduces a trade-off between the tw o quantities that renders the optimisation over αnon-
trivial. We highlight that considering the limit of α→ ∞ provides the fastest exponential decay and it recovers the
probability for independent random variables. This has the cost of rendering the multiplicative constant larger, and
we will discuss in detail the choice of αin the various examples of Section IV.
Remark 6 (Tensorisation and McDiarmid’s) .Note that Equation (23) requires only absolute continuity a s an
assumption. Equations (24) and (28) instead leverage tenso risation properties of Hα. These tensorisation properties
are particularly suited for Markovian settings as Equation (28) shows. However, in the general case, one can still
reduceHα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi)(a divergence between n-dimensional measures) to none-dimensional objects, see
Appendix D for details about the tensorisation of both the He llinger integral Hαand R´ enyi’s α-divergence Dαand
thus retrieve Equation (24). Note that Equation (23) gives a natural generalisation of concentration inequalities to
the case of arbitrarily dependent random variables (just li ke Equation (25) generalises them to Markovian settings).
Indeed, if PXn=P/circlemultiplytext
iXi, then taking the limit of β→1in both Equation (23) and Equation (25), one recovers
the classical concentration bound for independent random v ariables (see the discussion in Remark 5 recalling that
β=α/(α−1)).
October 31, 2023 DRAFT10
IV. A PPLICATIONS
Let us now apply Theorem 1 to four settings:
1) In Section IV-A, we consider a discrete-time Markovian setting . Here, we specialise Theorem 1 leveraging
the (hyper-)contraction properties of the Markov kernel al ong with the discrete structure of the problem, thus
showing that in certain parameter regimes our bound fares be tter than what the state of the art can provide;
2) In Section IV-B, we consider a non-contracting Markovian setting that does notadmit a stationary distribution.
Both these properties do not allow the application of most of the existing work in the literature. In contrast,
not only our approach can be applied, but it provides exponen tially decaying probability bounds, while [9,
Theorem 1.2] can only provide an upper bound that does not van ish asngrows;
3) In Section IV-C, we consider a non-Markovian setting where the entire past of the process inﬂuences each
step. Here, to the best of our knowledge, we provide the ﬁrst b ound that exponentially decays in nand has
a closed-form expression, while existing approaches eithe r cannot be employed or require the computation of
complicated quantities ( e.g., Equations (77) and (78));
4) Finally, in Section IV-D, we apply Theorem 1 to provide err or bounds on Markov Chain Monte Carlo methods.
Similarly to the other settings, we propose a regime of param eters in which our results fare better and,
consequently, provide an improved lower bound on the minimu m burn-in period necessary to achieve a certain
accuracy in MCMC.
We will hereafter assume, for simplicity of exposition, tha tci= 1/nin Equation (2) like in the case of the empirical
mean. All the results hold for general ci’s, but the expressions and comparisons would become more cu mbersome.
A. Discrete-Time Markov chains
Consider a discrete-time setting and a Markov chain (Xn)n∈Ndetermined by a sequence of transition ma-
trices(Kn)n∈N. Assume that X1∼P1and letXidenote the random variable whose distribution is given by
P1K1...Ki−1.2
Theorem 2. Fori≥1, suppose Kiis a discrete-valued Markov kernel, and let γ⋆
i(α)be the smallest parameter
making it hyper-contractive, see Section II-B. Then, for ev ery function fsatisfying Equation (2)withci=1
nand
everyα >1,
PXn(/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t)≤21
βexp/parenleftigg
−2nt2
β+n−1/summationdisplay
i=1/parenleftbigg
log/⌊ard⌊lK←
i/⌊ard⌊lα→γ⋆
i(α)−1
¯γ⋆
i(α)min
j∈supp(Pi)logPi(j)/parenrightbigg/parenrightigg
.
(32)
2One can also see Xias the outcome of Xi−1after being passed through the channel Ki−1.
October 31, 2023 DRAFT11
Moreover, if the Markov kernel is time-homogeneous, i.e., Ki=Kfor every i≥1, then
PXn(/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t)
≤21
βexp/parenleftigg
−2nt2
β+(n−1)log/⌊ard⌊lK←/⌊ard⌊lα→γ⋆
K(α)−1
¯γ⋆
K(α)n−1/summationdisplay
i=1/parenleftbigg
min
j∈supp(Pi)logPi(j)/parenrightbigg/parenrightigg
(33)
≤21
βexp/parenleftbigg
−2nt2
β+(n−1)log/⌊ard⌊lK←/⌊ard⌊lα→γ⋆
K(α)−n−1
¯γ⋆
K(α)/parenleftbigg
min
i=1,...,(n−1)min
j∈supp(Pi)logPi(j)/parenrightbigg/parenrightbigg
. (34)
In the above equations ¯γ⋆
i(α)and¯γ⋆
K(α)denote the H ¨older conjugates of, respectively, γ⋆
i(α)andγ⋆
K(α).
Proof. The main object one has to bound, according to Theorem 1, is th e following:
max
xi−1Hα(PXi|Xi−1=xi−1/⌊ard⌊lPXi),withi≥2. (35)
From the properties of the Markov kernel, one has that PXi=PXi−1Ki−1. Furthermore, PXi|Xi−1=xi−1can be
seen asδxi−1Ki−1whereδxi−1is a Dirac-delta measure centered at xi−1. Thus, recalling the deﬁnition of γ⋆
i−1(α)
from Section II-B,
H1
αα(PXi|Xi−1=xi−1/⌊ard⌊lPXi) =H1
αα(δxi−1Ki−1/⌊ard⌊lPXi−1Ki−1) (36)
=/vextenddouble/vextenddouble/vextenddouble/vextenddoubledδxi−1Ki−1
dPXi−1Ki−1/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Lα(PXi−1Ki−1)(37)
≤/vextenddouble/vextenddoubleK←
i−1/vextenddouble/vextenddouble
α→γ⋆
i−1(α)H1
γ⋆
i−1(α)
γ⋆
i−1(α)(δxi−1/⌊ard⌊lPXi−1), (38)
where Equation (38) follows from [31, Lemma A.1]. Moreover, for every κ >1,
H1
κκ(δxi−1/⌊ard⌊lPXi−1) =PXi−1({xi−1})1−κ
κ=PXi−1({xi−1})−1
¯κ, (39)
where¯κdenotes the H¨ older’s conjugate of κandPXi−1({xi−1})the measure that PXi−1assigns to the point xi−1.
Thus, the following sequence of steps, along with Equation ( 28), concludes the argument:
n/productdisplay
i=2max
xi−1H1
αα(PXi|Xi−1=xi−1/⌊ard⌊lPXi)≤n/productdisplay
i=2max
xi−1/vextenddouble/vextenddoubleK←
i−1/vextenddouble/vextenddouble
α→γ⋆
i−1(α)H1
γ⋆
i−1(α)
γ⋆
i−1(α)(δxi−1/⌊ard⌊lPXi−1) (40)
=/parenleftiggn/productdisplay
i=2/vextenddouble/vextenddoubleK←
i−1/vextenddouble/vextenddouble
α→γ⋆
i−1(α)/parenrightigg/parenleftiggn/productdisplay
i=2max
xi−1/parenleftbigg
PXi−1({xi−1})−1
¯γ⋆
i−1(α)/parenrightbigg/parenrightigg
(41)
=/parenleftiggn−1/productdisplay
i=1/⌊ard⌊lK←
i/⌊ard⌊lα→γ⋆
i(α)/parenrightigg/parenleftiggn−1/productdisplay
i=1/parenleftbigg
min
xiPXi({xi})/parenrightbigg−1
¯γ⋆
i(α)/parenrightigg
. (42)
Moreover, given the discrete setting, one can replace the me asurePXiwith the corresponding pmf which is denoted
byPi.
If the Markov kernel Kiis only contractive (and not hyper-contractive), then γ⋆
i(α) =αand¯γ⋆
i(α) =β, which
allows to simplify Equations (32) to (34). In this case, if
t2≥β
2n−1
nlog/⌊ard⌊lK←/⌊ard⌊lα→α−n−1
2n/parenleftbigg
min
imin
jlogPi(j)/parenrightbigg
(43)
= (1+on(1))/parenleftbiggβ
2log/⌊ard⌊lK←/⌊ard⌊lβ
α→α−1
2/parenleftbigg
min
imin
jlogPi(j)/parenrightbigg/parenrightbigg
, (44)
October 31, 2023 DRAFT12
then Theorem 2 gives exponential (in n) concentration even in the case of dependence.
Remark 7 (Hypercontractivity, SDPI and R´ enyi’s divergences) .Another perspective naturally stems from Equa-
tion (29). Indeed, similarly to Theorem 2, one has
PXn(/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t)≤21
βexp/parenleftigg
−1
β/parenleftigg
2nt2−n/summationdisplay
i=2max
xi−1Dα(PXi|Xi−1=xi−1/⌊ard⌊lPXi)/parenrightigg/parenrightigg
(45)
≤21
βexp/parenleftigg
−1
β/parenleftigg
2nt2−ηα(K)n/summationdisplay
i=2max
xi−1Dα(δxi−1/⌊ard⌊lPXi−1)/parenrightigg/parenrightigg
(46)
= 21
βexp/parenleftigg
−1
β/parenleftigg
2nt2+ηα(K)n/summationdisplay
i=2min
xi−1logPi−1(xi−1)/parenrightigg/parenrightigg
. (47)
We remark that Dαcan also be hyper-contractive with respect to some Markovia n operators, meaning that in Equa-
tion (46) for instance, one could consider Dγ(δx−1/⌊ard⌊lPXi−1)withγ < α (this is equivalent to hyper-contractivity of
Markov operators, [32, Section IV]). One such example is the Ornstein–Uhlenbeck channel with noise parameter t,
cf. [1, Eq. 3.2.37], for which one can prove hyper-contracti vity with respect to Dα[1, Theorem 3.2.3]. Moreover,
in some settings, leveraging SDPIs for Dαcan provide an improvement over Theorem 2, see Appendix E-E f or a
detailed comparison.
We now compare the concentration bound provided by Theorem 2 with existing bounds in the literature. In this
section, the comparison concerns a general Markov kernel, a nd the explicit calculations for a binary kernel are
deferred to Appendix E.
1) Comparison with [9, Theorem 1.2]: Let us consider the same setting as in Theorem 2. Then, [9] giv es
P(|f−PXn(f)| ≥t)≤2exp/parenleftbigg
−nt2
2M2n/parenrightbigg
, (48)
whereMn= max 1≤i≤n−1/parenleftig
1+/summationtextn−1
j=i/producttextj
k=iηKL(Kk)/parenrightig
and we recall that ηKL(Ki) = supx,ˆxTV(Ki(·|x),Ki(·|ˆx))
is the contraction coefﬁcient of the Markov kernel Ki. First, note that, if the random variables are independent a nd
thusPXn=P/circlemultiplytextXi, then Equation (48) reduces to
PXn(E)≤2exp(−nt2/2),
while Theorem 2 with γ⋆
K(α) =α→ ∞ and¯γ⋆
K(α) =β→1recovers McDiarmid’s inequality with the correct
constant in front of n,i.e.,
PXn(E)≤2exp(−2nt2).
Assume now that the Markov kernel is time-homogeneous and ha s a contraction coefﬁcient ηTV(K)<1. Then,
Mn= max 1≤i≤n−11−ηTV(K)n+1−i
1−ηTV(K)=1−ηTV(K)n
1−ηTV(K). For compactness, deﬁne Pi⋆(j⋆) := min iminjPi(j). Making
a direct comparison, one has that if
t2>n−1
n/parenleftbigg2M2
n
4M2n−β/parenrightbigg
log/⌊ard⌊lK←/⌊ard⌊lβ
α→α
Pi⋆(j⋆)(49)
= (1+on(1))/parenleftbigg2
4−β(1−ηTV(K))2/parenrightbigg
log/⌊ard⌊lK←/⌊ard⌊lβ
α→α
Pi⋆(j⋆), (50)
October 31, 2023 DRAFT13
then Equation (34) (with γ⋆
K(α) =αand¯γ⋆
K(α) =β) provides a faster exponential decay than Equation (48). Th e
explicit calculations for the special case of a binary kerne l are provided in Appendix E-A.
One can then consider the limit of α→ ∞ which renders β→1. On the one hand, this implies a larger
multiplicative coefﬁcient (as Hαgrows with α, see Remark 5) and, consequently, it increases the minimum v alue
oftone can consider in Equation (50). On the other hand, it guara ntees a faster exponential decay in Equations (33)
to (34). In fact, as β→1, the RHS of (34) scales as exp(−2nt2(1+on(1))) for large enough t, which matches the
behavior of Equation (3). Let us highlight that, to the best o f our knowledge, our approach is the ﬁrst to recover
thesame exponential decay rate obtained in the independent case, even in the presence of cor relation among the
random variables.
We remark that the convergence results in Theorem 2 and Equat ion (48) are with respect to different constants:
Theorem 2 considers the concentration of faroundP/circlemultiplytextn
i=1Xi(f), while Equation (48) around PXn(f). However,
given the faster rate of convergence guaranteed by our frame work – for this example and, even more impressively
so, for the one in Section IV-B – the mean of funder the product of the marginals might be regarded as a natu ral
object to consider when proving concentration results for t hese processes. This hypothesis is corroborated by the
approach presented in [3], where concentration for station ary Markov chains is provided around the mean with
respect to the stationary measure π. Indeed, when X1∼π, the product of the marginals reduces to the tensor
product of the stationary measure π⊗n. To make a direct comparison with [9], one can leverage [33, P roposition
1.8] (reproduced in Appendix B) and either reduce both resul ts to concentration bounds around the median, or
transform Theorem 1 in a result on concentration around the m ean with respect to PXn. This would introduce
additional terms, rendering the comparison cumbersome and outside the scope of this work. We however perform
said comparison explicitly in a different setting (see Sect ion IV-B).
2) Comparison with [3, Theorem 1]: [3] considers a more restricted setting in which fis the sum of nbounded
functions that separately act on each of the nrandom variables, i.e.,f=/summationtextn
i=1fi(Xi)withfi∈[ai,bi].3By
assuming further that ai= 0andbi=1
nfor every i(e.g., empirical mean), we are in a setting in which Theorem 2
holds as it is. Moreover, the setting in [3] requires the Mark ov chain to be time-homogeneous and admit a stationary
distribution π(notice that none of these assumptions are necessary for The orems 1 and 2 to hold). In this case [3,
Theorem 1] gives:
PXn/parenleftbig/vextendsingle/vextendsinglef−π⊗n(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤2exp/parenleftbigg
−1−λ
1+λ2nt2/parenrightbigg
, (51)
where(1−λ)denotes the absolute spectral gap of the Markov chain, see [3 , Deﬁnition 19], which characterizes
the speed of convergence to the stationary distribution. Co mparing with Equation (34) with γ⋆
K(α) =α→ ∞ and
¯γ⋆
K(α) =β→1,4one has that if
t2>(1+on(1))1+λ
4λlog/⌊ard⌊lK←/⌊ard⌊l∞→∞
Pi⋆(j⋆), (52)
3This choice of fin Theorem 1 transforms the result in a generalisation of Hoe ffding’s inequality to dependent settings. It is easy to see that
iffi∈[ai,bi]then (2) holds with ci= (bi−ai)and the statement follows.
4As mentioned earlier, this optimizes the rate of convergenc e, at the expense of the value of tfrom which we obtain an improvement.
October 31, 2023 DRAFT14
then Theorem 2 provides a faster decay than Equation (51). A m ore explicit comparison in which the absolute
spectral gap is computed for a binary Markov kernel can be fou nd in Appendix E-B.
3) Comparison with [5]: Let us now derive the corresponding result of concentration around the median in order
to compare with [5]. Leveraging [33, Proposition 1.8] (see a lso Appendix B) along with Equation (34) (again, with
γ⋆
K(α) =α→ ∞ and¯γ⋆
K(α) =β→1), we have
PXn(|f−mf| ≥t)≤2exp
−2n/parenleftigg
t−/radicalbigg
ln4+Cn
2n/parenrightigg2
+Cn
, (53)
whereCn= (n−1)log(/⌊ard⌊lK←/⌊ard⌊l∞→∞/Pi⋆(j⋆)). This also implies (see, e.g., [33, Proposition 1.3] or [1, Theorem
3.4.1]) that, if t >/radicalig
log4+Cn
2nand given any event Esuch that PXn(E)≥1
2, then
PXn(Ec
t)≤2exp
−2n/parenleftigg
t−/radicalbigg
log4+Cn
2n/parenrightigg2
+Cn

=1
2exp/parenleftig
−2nt2+2t/radicalbig
2n(log4+Cn)/parenrightig
,(54)
whereEt={y∈ Xn:d(x,y)≤tfor some x∈E}andddenotes the normalised Hamming metric.
Let us denote a:= 1−maximaxx,ˆxTV(PXi|Xi−1=x,PXi|Xi−1=ˆx).Hence, assuming t >1
a/radicalbig
log(1/PXn(E))/n,
[5, Proposition 1 & Proposition 4] give
PXn(Ec
t)≤exp
−2n/parenleftigg
at−/radicalbigg
log(1/PXn(E))
2n/parenrightigg2
 (55)
≤exp/parenleftig
−2nt2a2+2ta/radicalbig
2nlog2/parenrightig
. (56)
Ignoring multiplicative constants and comparing the expon ents of (54) and (56), one can see that, whenever
t≥(1+on(1))/radicalbigg
2log/parenleftig
/⌊ard⌊lK←/⌊ard⌊l∞→∞
Pi⋆(j⋆)/parenrightig
(1−a2), (57)
then our approach improves upon [5].
The Dobrushin coefﬁcient of the kernel, captured by the quan tity(1−a), measures the degree of dependence
of the stochastic model. The smaller 1−ais, the less “dependent” the model is. If PXnreduces to a product
distribution then a= 1. In this case, Equation (54) boils down to McDiarmid’s inequ ality. In contrast, the larger
1−ais, the worse the behavior of Equation (54). If a= 0, then the Markov chain is not contracting and violates
the assumption of [4–7]. Our approach, instead, can still pr ovide meaningful results, as we will see in Section IV-B.
A more explicit comparison for the case of a binary kernel can be found in Appendix E-C.
B. A non-contracting Markov chain
In order to provide concentration for Markov chains, existi ng work requires either contractivity of the Markov
chain [4], stationarity [3, 5] or some form of mixing [5, 8]. A well-known Markov chain that evades most of these
concepts is the SSRW. Suppose to have a sequence of i.i.d. Rad emacher random variables, i.e.,P(Xi=−1) =
P(Xi= +1) = 1 /2, fori≥1; the initial condition is X0= 0 w.p.1. Then, a SSRW is the Markov chain (Si)i∈N
October 31, 2023 DRAFT15
deﬁned as Si=Si−1+Xi. This Markov chain does not admit a stationary distribution , it is not contracting, but
it is expanding (in this case, both towards the positive and t he negative axes of the real line). Let us denote with
Kithe kernel at step i. Then, at each step i >2, one can always ﬁnd two different realisations of Si−1, let us
call them s1,s2, such that supp (Ki(·|s1))∩supp(Ki(·|s2)) =∅,i.e., the support of Siis constantly growing. This
implies that the approaches in [3, 4] cannot be employed, whi le [9, Theorem 1] yields:
P(|f−PXn(f)| ≥t)≤2exp/parenleftbigg
−t2
2n/parenrightbigg
. (58)
A meaningful regime (given also the expanding nature of the M arkov chain along the integers)5arises when
considering t/greaterorsimilar√n. Let us now compute the Hellinger integral in this speciﬁc se tting. This is done in the lemma
below, proved in Appendix F.
Lemma 1. Leti≥1,x∈supp(Si−1),0≤j≤i, andα≥1. Then,
21
β/parenleftBig
−1+i(1−h2(i+1
2i)+1
2log2/parenleftBig
π
2/parenleftBig
i2−1
i/parenrightBig/parenrightBig/parenrightBig
≤H1
αα(PSi|Si−1=x/⌊ard⌊lPSn)≤2i1
β−1+1
α, (59)
whereh2(x) =−xlog2(x)−(1−x)log2(1−x)denotes the binary entropy. Thus one has that:
n−2
4β≤log2H1
αα(PSn/⌊ard⌊lP/circlemultiplytextn
j=1Sj)≤n(n−1)
2β. (60)
Combining Lemma 1 and Theorem 1, one has that
P/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Si(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftbigg−2nt2
β+n(n−1)
2βln2/parenrightbigg
. (61)
It is easy to see that, whenever t >√
(n−1)ln(2)
2, Equation (61) gives an exponential decay. For instance, ch oosing
t=√n, one retrieves
P/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Si(f)/vextendsingle/vextendsingle≥√n/parenrightbig
≤21
βexp/parenleftbigg
−n2
β/parenleftbigg
2−ln2
2+ln2
2n/parenrightbigg/parenrightbigg
. (62)
In contrast, the same choice in Equation (58) gives
P(|f−PSn(f)| ≥√n)≤2exp(−1/2). (63)
More generally, selecting tof order√nsufﬁces to achieve an exponential decay in Equation (61), wh ile to obtain
a similar speed of decay in Equation (58) tneeds to be at least of order n. The approach advanced in this work
can, thus, not only be employed in settings where most of the o ther approaches fail ( e.g., [3–5]), but it also brings a
signiﬁcant improvement over the rate of decay that one can pr ovide. It is worth noticing that Equation (61) provides
concentration around P⊗n
i=1Si(f)while Equation (58) provides concentration around PSn(f). In order to provide a
more fair comparison between the two approaches, we will now provide a bound on concentration around PSn(f)
via Theorem 1. In particular, one can prove the following (se e Appendix C for the proof).
Lemma 2. Letα >1. Denote with tα=/radicalig
βln(Hα(PXn/⌊ard⌊lP⊗Xi))
2αn. Then, the following holds true:
/vextendsingle/vextendsinglePXn(f)−P⊗n
i=1Xi(f)/vextendsingle/vextendsingle≤tα+√β
21
α/radicalig
2n
αln(Hα(PXn/⌊ard⌊lP⊗n
i=1Xi)). (64)
5The standard deviation of Snis√n, hence it is expected that Snis±O(√n).
October 31, 2023 DRAFT16
Moreover, one can leverage Lemma 1 to prove that√β
21
α/radicalBig
2n
αln(Hα(PXn/⌊ard⌊lP⊗n
i=1Xi))≤21
ββ
n√
ln(2)(2−4
n)=on(1).
Consequently, one has that
|f−PSn(f)|=/vextendsingle/vextendsinglef−PSn(f)−P⊗n
i=1Si(f)+P⊗n
i=1Si(f)/vextendsingle/vextendsingle (65)
≤/vextendsingle/vextendsinglef−P⊗n
i=1Si(f)/vextendsingle/vextendsingle+/vextendsingle/vextendsinglePXn(f)−P⊗n
i=1Si(f)/vextendsingle/vextendsingle (66)
≤/vextendsingle/vextendsinglef−P⊗n
i=1Si(f)/vextendsingle/vextendsingle+tα+on(1). (67)
Equation (67) together with Lemma 1 and Theorem 1 imply that:
P(|f−PSn(f)| ≥t+tα+on(1))≤P/parenleftbig/vextendsingle/vextendsinglef−P⊗n
i=1Si(f)/vextendsingle/vextendsingle≥t/parenrightbig
(68)
≤21
βH1
αα(PSn/⌊ard⌊lP⊗n
i=1Si)exp/parenleftbigg
−2nt2
β/parenrightbigg
. (69)
Hence, selecting ˜t= 2√n, Theorem 1 and Lemma 1 lead to
P/parenleftbig
|f−PSn(f)| ≥˜t/parenrightbig
≤21
βexp/parenleftbigg
−n2
β/parenleftbigg
2−ln2
2+ln2
2n/parenrightbigg/parenrightbigg
, (70)
while, similarly to before, [9, Theorem 1] (see Equation (58 )) leads to:
P/parenleftbig
|f−PSn(f)| ≥˜t/parenrightbig
≤2exp(−2). (71)
C. A non-Markovian Process
Next, we consider a non-Markovian setting in which each step of the stochastic process depends on its entire
past:
Xn=

+1,with probability/summationtextn−1
i=0piXi,
−1,with probability 1−/summationtextn−1
i=0piXi,(72)
forn≥1,pi>0andX0= +1 with probability 1. The choice of the parameters piis arbitrary (given the
constraint that/summationtextn−1
i=0pi<1) but for concreteness we will set pi= 2−i−1for every i≥0. Then,PX1(1|x0) =1
2=
PX1(−1|x0)and for each n≥1,
PXn(1|xn−1
0) =1
2+n−1/summationdisplay
i=1pixi=n−1/summationdisplay
i=0xi2−i−1= 1−PXn(−1|xn−1
0),
withx0= 1. Consequently, following the calculations detailed in App endix G, we have
Hα(PXn(·|xn−1
0)/⌊ard⌊l(1/2,1/2))<2⌊α
2⌋/summationdisplay
j=0/parenleftbigg⌊α⌋
2j/parenrightbigg/parenleftigg
2n−1/summationdisplay
i=1pixi/parenrightigg2j
, (73)
which, as pi= 2−i−1, gives
max
xn−1
1Hα(PXn(·|xn−1
0)/⌊ard⌊l(1/2,1/2))<2α. (74)
Thus,H1
αα(PXn/⌊ard⌊l(1/2,1/2)⊗n)<2n−1and an application of Theorem 1, as stated in Equation (28), y ields:
P/parenleftbig/braceleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/bracerightbig/parenrightbig
≤inf
β>121
βexp/parenleftbigg
−2n
β/parenleftbigg
t2−n−1
nβln2
2/parenrightbigg/parenrightbigg
, (75)
with exponential decay whenever
t2>(1+on(1))βln2
2. (76)
October 31, 2023 DRAFT17
Like before, choosing a larger βslows down the exponential decay, but it reduces the multipl icative coefﬁcient
introduced via Hα. Fornandtlarge enough, one can pick β→1and retrieve a McDiarmid-like exponential
decay. Given that this setting does not characterise a Marko vian dependence (at each step the stochastic process
depends on its entire past), one cannot employ the technique described in [3] or in [4]. One can, however, employ
the technique described in [8] and [9]. Both these approache s require the computation of {¯θij}1≤i<j≤nwith
¯θij:= sup
xi−1,w,ˆw/vextenddouble/vextenddoubleL(Xn
j|Xi= (xi−1,w))−L(Xn
j|Xi= (xi−1,ˆw))/vextenddouble/vextenddouble
TV, (77)
whereL(Xn
j|Xi= (xi−1,w))denotes the conditional distribution of Xn
jgivenXi= (xi−1,w). The¯θ’s are then
organised in n×nupper-triangular matrices whose norms are computed in orde r to provide an upper bound on the
probability of interest. In particular, [8] requires the ℓ2-norm, while [9] requires the operator norm of the matrix.
Similarly, to employ the technique provided in [7] one would need to compute the following quantity
¯C= max
1≤i≤nsup
xi−1∈Xi−1
w,ˆw∈Xinf
π∈Π(L(Xn|xi,w),L(Xn|xi,ˆw))π(d). (78)
Here,ddenotes the normalised Hamming metric and Π(L(Xn|xi,w),L(Xn|xi,ˆw))represents the set of all the
couplings deﬁned on Xn×Xnsuch that the corresponding marginals are L(Xn|xi,w)andL(Xn|xi,ˆw). However,
computing any of these objects in practice can be complicate d even in simple settings. In contrast, with the
framework proposed here and thanks to the tensorisation pro perties of the Hellinger integral, one can easily bound
the information measure and provide an exponentially decay ing probability (whenever the probability of the same
event under independence decays exponentially and for oppo rtune choices of the parameters).
D. Markov Chain Monte Carlo
An intriguing application of the method proposed in [3] cons ists in providing error bounds for Markov Chain
Monte Carlo methods. For instance, assume that one is trying to estimate the mean π(f)for some function fand
some measure πwhich cannot be directly sampled. A common approach consist s in considering a Markov chain
{Xi}i≥1whose stationary distribution is πand estimating π(f)via empirical averages of samples {Xi}n0+n
n0+1, where
n0characterises the so-called “burn-in period”. This period ensures that enough time has passed and the Markov
chain is sufﬁciently close to the stationary distribution πbefore sampling from it. [3, Theorem 12] gives that
P/parenleftigg
1
nn/summationdisplay
i=1f(Xn0+i)−π(f)> t/parenrightigg
≤C(ν,n0,α)exp/parenleftbigg
−1
β·1−max{λr,0}
1+max{λr,0}·2nt2
(b−a)2/parenrightbigg
, (79)
wheref:X →[a,b]is uniformly bounded, λrrepresents the right-spectral gap (see [3, Deﬁnition 20]), α∈
(1,+∞),βdenotes its H¨ older’s conjugate and Cis a constant depending on the burn-in period n0, the Radon-
Nikodym derivative between the starting measure νand the stationary measure π, andα. Using the tools provided
in this work, we obtain:
P/parenleftigg
1
nn/summationdisplay
i=1f(Xn0+i)−π(f)> t/parenrightigg
≤exp/parenleftbigg
−2nt2
β(b−a)2/parenrightbigg
H1
αα(νKn0/⌊ard⌊lπ)n/productdisplay
i=2max
xn0+i−1H1
αα(K(·|xn0+i−1)/⌊ard⌊lπ)
≤C(ν,n0,α)exp/parenleftbigg
−2nt2
β(b−a)2/parenrightbigg
max
xπ({x})−n−1
β, (80)
October 31, 2023 DRAFT18
where Equation (80) follows from the fact that H1
αα(νKn0/⌊ard⌊lπ)represents the Lα(π)-norm of the Radon-Nikodym
derivative and can thus be bounded like in [3, Theorem 12]. Th e idea behind the result is as follows. Given that
one is trying to estimate the mean of funderπusing empirical averages, if one had samples taken in an i.i. d.
fashion from π, the exponential convergence would be guaranteed. However , the issue is that one does nothave
access to samples of π. Thus, changing the measure to an n-fold tensor product of π, one can still guarantee an
exponential decay at the cost of a multiplicative price depe nding on how far the samples are from the stationary
distribution. By making a direct comparison, assuming λr>0, one can see that if
t2≥n−1
n(b−a)2
21+λr
2λrlog/parenleftbigg1
minxπ({x})/parenrightbigg
(81)
= (1+on(1))(b−a)2
21+λr
2λrlog/parenleftbigg1
minxπ({x})/parenrightbigg
,
then the RHS of Equation (80) decays faster than the RHS of Equ ation (79). A comparison for the binary symmetric
channel, with the computations of all the parameters, can be found in Appendix E-D.
Similarly to [3], one can also show that an exponential decay is guaranteed in Equation (80) if n0= Ω(logn).
Furthermore, as Equation (80) improves the exponential dec ay fortsatisfying Equation (81), in the same regime
the induced lower bound over n0will be improved as well. Finally, we highlight that Theorem 1 applies to a much
larger family of functions fthan what can be handled by [3].
V. C ONCLUSIONS
We introduced a novel approach to the concentration of measu re for dependent random variables. The generality
of our framework allows to consider arbitrary kernels witho ut requiring either stationarity (as opposed to [3]) or
contractivity (as opposed to [4, 5]). Moreover, our techniq ue applies to any family of functions which is known
to concentrate when the random variables are actually indep endent. Said technique is employed and compared
to the state of the art in four different settings: ﬁnite-sta te Markov chains, a non-contractive one (the SSRW), a
non-Markovian process, and Monte Carlo Markov Chain method s. In each of these settings, we provide a regime of
parameters in which we guarantee a McDiarmid-like decay and improve over existing results. The improvement is the
most striking in the case of the SSRW, where the only (closed- form) alternative approach gives a constant probability
of deviation from the average, as opposed to the exponentially decaying probability guaranteed by our framework.
The bounds provided display a threshold phenomenon dependi ng on the accuracy t,i.e., one can show concentration
only for values of tlarger than a threshold depending on the Hellinger integral (and its scaling with respect to the
number of variables n). Consider for instance Equation (75): if t2>β
2nlnH1
αα≈(1 +on(1))(βln(2)/2), then
the exponent is negative and one has exponential concentrat ion, otherwise the exponent becomes positive and the
bound trivialises to something larger than 1. We believe this to be an artifact of the analysis and not an in trinsic
property of the concentration of measure phenomenon.
ACKNOWLEDGMENTS
The authors are partially supported by the 2019 Lopez-Loret a Prize. They would also like to thank Professor Jan
Maas for providing valuable suggestions and comments on an e arly version of the work.
October 31, 2023 DRAFT19
REFERENCES
[1] M. Raginsky and I. Sason, “Concentration of measure ineq ualities in information theory, communications, and
coding,” Foundations and Trends in Communications and Information T heory , vol. 10, no. 1-2, pp. 1–246,
2013.
[2] C. McDiarmid, On the method of bounded differences , ser. London Mathematical Society Lecture Note Series.
Cambridge University Press, 1989, p. 148–188.
[3] J. Fan, B. Jiang, and Q. Sun, “Hoeffding’s inequality for general markov chains and its applications to statistical
learning,” Journal of Machine Learning Research , vol. 22, no. 139, pp. 1–35, 2021.
[4] K. Marton, “A measure concentration inequality for cont racting Markov chains,” Geometric and functional
analysis , vol. 6, no. 3, pp. 556–571, 1996.
[5] ——, “Bounding ¯d-distance by informational divergence: a method to prove me asure concentration,” The
Annals of Probability , vol. 24, pp. 857–866, 1996.
[6] ——, “Measure concentration for a class of random process es,”Probability Theory and Related Fields , vol.
110, p. 427–439, 1998.
[7] ——, “Measure concentration and strong mixing.” Sci. Math. Hungarica Studia , vol. 40, pp. 95–113, 2003.
[8] P.-M. Samson, “Concentration of measure inequalities f or Markov chains and Φ-mixing processes,” The Annals
of Probability , vol. 28, no. 1, pp. 416 – 461, 2000.
[9] L. A. Kontorovich and K. Ramanan, “Concentration inequa lities for dependent random variables via the
martingale method,” The Annals of Probability , vol. 36, no. 6, pp. 2126 – 2158, 2008.
[10] J.-R. Chazottes, P. Collet, C. K¨ ulske, and F. Redig, “C oncentration inequalities for random ﬁelds via coupling,”
Probability Theory and Related Fields , vol. 137, pp. 201–225, 2005.
[11] D. Paulin, “Concentration inequalities for Markov cha ins by Marton couplings and spectral methods,”
Electronic Journal of Probability , vol. 20, pp. 1 – 32, 2015.
[12] S. Boucheron, G. Lugosi, and P. Massart, Concentration Inequalities: A Nonasymptotic Theory of Ind epen-
dence . Oxford University Press, 2013.
[13] D. W. Gillman, “Hidden Markov chains: convergence rate s and the complexity of inference,” Ph.D. dissertation,
Massachusetts Institute of Technology, Boston, US, 1993.
[14] I. H. Dinwoodie, “A probability inequality for the occu pation measure of a reversible Markov chain,” The
Annals of Applied Probability , vol. 5, no. 1, pp. 37–43, 1995.
[15] C. A. Le´ on and F. Perron, “Optimal Hoeffding bounds for discrete reversible Markov chains,” The Annals of
Applied Probability , vol. 14, no. 2, pp. 958–970, 2004.
[16] K.-M. Chung, H. Lam, Z. Liu, and M. Mitzenmacher, “Chern off-hoeffding bounds for Markov chains:
Generalized and simpliﬁed,” in Symposium on Theoretical Aspects of Computer Science , 2012.
[17] B. Miasojedow, “Hoeffding’s inequalities for geometr ically ergodic Markov chains on general state space,”
Statistics & Probability Letters , vol. 87, pp. 115–120, 2014.
[18] J. Dedecker and S. Gou¨ ezel, “Subgaussian concentrati on inequalities for geometrically ergodic Markov
October 31, 2023 DRAFT20
chains,” Electronic Communications in Probability , vol. 20, no. none, pp. 1 – 12, 2015. [Online]. Available:
https://doi.org/10.1214/ECP.v20-3966
[19] A. Havet, M. Lerasle, E. Moulines, and E. Vernet, “A quan titative McDiarmid’s inequality for geometrically
ergodic Markov chains,” Electronic Communications in Probability , vol. 25, no. none, pp. 1 – 11, 2020.
[Online]. Available: https://doi.org/10.1214/20-ECP28 6
[20] C. H. Lampert, L. Ralaivola, and A. Zimin, “Dependency- dependent bounds for sums of dependent random
variables,” arXiv preprint arXiv:1811.01404 , 2018.
[21] J. R. Chazottes, S. Gallo, and D. Takahashi, “Optimal ga ussian concentration bounds for stochastic chains of
unbounded memory,” 2020.
[22] I. Kontoyiannis, L. Lastras-Montano, and S. Meyn, “Rel ative entropy and exponential deviation bounds for
general markov chains,” in Proceedings. International Symposium on Information Theo ry, 2005. ISIT 2005. ,
2005, pp. 1563–1567.
[23] I. Kontoyiannis, L. A. Lastras-Monta˜ no, and S. P. Meyn , “Exponential bounds and stopping rules for mcmc
and general markov chains,” in Proceedings of the 1st International Conference on Perform ance Evaluation
Methodolgies and Tools , ser. valuetools ’06. New York, NY , USA: Association for Com puting Machinery,
2006, p. 45–es. [Online]. Available: https://doi.org/10. 1145/1190095.1190152
[24] A. R. Esposito, M. Gastpar, and I. Issa, “Generalizatio n error bounds via R´ enyi-, f-divergences and maximal
leakage,” IEEE Transactions on Information Theory , vol. 67, no. 8, pp. 4986–5004, 2021.
[25] F. Liese and I. Vajda, “On divergences and informations in statistics and information theory,” IEEE Transactions
on Information Theory , vol. 52, no. 10, pp. 4394–4412, 2006.
[26] A. R´ enyi, “On measures of entropy and information,” Proceedings of the 4th Berkeley Symposium on
Mathematics, Statistics and Probability , vol. 1, pp. 547–561, 1960.
[27] T. van Erven and P. Harremo¨ es, “R´ enyi divergence and K ullback-Keibler divergence,” IEEE Transactions on
Information Theory , vol. 60, no. 7, pp. 3797–3820, 2014.
[28] R. Ahlswede and P. Gacs, “Spreading of sets in product sp aces and hypercontraction of the Markov operator,”
The Annals of Probability , vol. 4, no. 6, pp. 925 – 939, 1976.
[29] E. Mossel, R. O’Donnell, O. Regev, J. E. Steif, and B. Sud akov, “Non-interactive correlation distillation,
inhomogeneous Markov chains, and the reverse Bonami-Beckn er inequality,” Israel Journal of Mathematics ,
vol. 154, no. 1, pp. 299–336, 2006.
[30] S. Kamath and V . Anantharam, “Non-interactive simulat ion of joint distributions: The Hirschfeld-Gebelein-
R´ enyi maximal correlation and the hypercontractivity rib bon,” in 50th Annual Allerton Conference on
Communication, Control, and Computing , 2012, pp. 1057–1064.
[31] M. Raginsky, “Strong data processing inequalities and φ-sobolev inequalities for discrete channels,” IEEE
Transactions on Information Theory , vol. 62, no. 6, pp. 3355–3389, 2016.
[32] ——, “Logarithmic sobolev inequalities and strong data processing theorems for discrete channels,” in IEEE
International Symposium on Information Theory , 2013, pp. 419–423.
[33] M. Ledoux, The Concentration of Measure Phenomenon , ser. Mathematical surveys and monographs.
October 31, 2023 DRAFT21
American Mathematical Society, 2001.
[34] L. Ambrosio, N. Gigli, and G. Savar´ e, Gradient Flows in Metric Spaces and in the Space of Probabili ty
Measures , 2nd ed. Springer Science & Business Media, 2008.
[35] F. G¨ otze, H. Sambale, and A. Sinulis, “Higher order con centration for functions of weakly dependent random
variables,” Electronic Journal of Probability , vol. 24, pp. 1 – 19, 2019.
[36] A. Wyner, “The common information of two dependent rand om variables,” IEEE Transactions on Information
Theory , vol. 21, no. 2, pp. 163–179, 1975.
[37] R. G. Gallager, Information Theory and Reliable Communication . USA: John Wiley & Sons, Inc., 1968.
APPENDIX A
PROOF OF THEOREM 1
Proof. Assume that E={|f−P/circlemultiplytextn
i=1Xi(f)| ≥t}. Then, one has that
PXn(E) =/integraldisplay/BDEdPXn (82)
=/integraldisplay/BDEdPXn
dP/circlemultiplytextn
i=1XidP/circlemultiplytextn
i=1Xi (83)
≤/parenleftbigg/integraldisplay/BDEdP/circlemultiplytextn
i=1Xi/parenrightbiggα−1
α/parenleftigg/integraldisplay/parenleftigg
dPXn
dP/circlemultiplytextn
i=1Xi/parenrightiggα
dP/circlemultiplytextn
i=1Xi/parenrightigg1
α
(84)
=P1
β/circlemultiplytextn
i=1Xi(E)H1
αα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi), (85)
where the inequality in the third line follows from H¨ older’ s inequality. Moreover,
Hα(PXn/⌊ard⌊lP/circlemultiplytextn
i=1Xi) =PX1/parenleftigg
P/circlemultiplytextn
i=2Xi/parenleftigg
dPXn
2|X1
dP/circlemultiplytextn
i=2Xi/parenrightiggα/parenrightigg
(86)
=PX1/parenleftigg
PX2/parenleftigg/parenleftbiggdPX2|X1
dPX2/parenrightbiggα/parenleftigg
P/circlemultiplytextn
i=3Xi/parenleftigg
dPXn
3|X2
dP/circlemultiplytextn
i=3Xi/parenrightiggα/parenrightigg/parenrightigg/parenrightigg
(87)
≤ PX1
PX2/parenleftbigg/parenleftbiggdPX2|X1
dPX2/parenrightbiggαα2/parenrightbigg1
α2
PX2
/parenleftigg
P/circlemultiplytextn
i=3Xi/parenleftigg
dPXn
3|X2
dP/circlemultiplytextn
i=3Xi/parenrightiggα/parenrightiggβ2
1
β2
 (88)
=H2
α·PX2
/parenleftigg
P/circlemultiplytextn
i=3Xi/parenleftigg
dPXn
3|X2
dP/circlemultiplytextn
i=3Xi/parenrightiggα/parenrightiggβ2
1
β2
, (89)
where the inequality follows from H¨ older’s inequality, th e fact that the expectation is taken with respect to the produ ct
measureP/circlemultiplytext
iXias well as the Markovianity of PXn.H2
α=P1
β1
X1/parenleftbigg
Hβ1
α2αα2(PX2|X1/⌊ard⌊lPX2)/parenrightbigg
withβ1= 1 will now
be the only term depending on X1. Repeating the same sequence of steps (an application of the Disintegration
Theorem [34, Theorem 5.3.1] to a Markovian setting, like in [ 35, Proposition 2.1], followed by H¨ older’s inequality)
(n−2)more times leads to the product of Hi
αas deﬁned in the statement of the theorem. The result then fol lows
by noticing that
P1
β/circlemultiplytext
iXi(E)≤21
βexp/parenleftbigg
−2nt2
β/parenrightbigg
,
by McDiarmid’s inequality.
October 31, 2023 DRAFT22
APPENDIX B
CONCENTRATION AROUND MEAN AND MEDIAN
The following result is a useful tool that allows us to compar e concentration around a constant, concentration
around the mean and concentration around the median. It is us ed in order to compare our results with the ones
proposed in [4, 5]. A proof can be found in [33, Proposition 1. 8] and the statement is provided here for ease of
reference.
Proposition 1. Letfbe a measurable function on a probability space (X,Ω,µ). Assume that, for some af∈R
and a non-negative function h:R+→R+such that limr→∞h(r) = 0 ,
µ({|f−af| ≥r})≤h(r) (90)
for allr >0, then
µ({|f−mf| ≥r+r0})≤h(r), (91)
wheremfis the median of fandr0is such that h(r0)<1
2. Moreover, if ¯h=/integraltext∞
0h(x)dx <∞, thenfis
µ-integrable, |af−µ(f)| ≤¯hand, for every r >0,
µ(/braceleftbig
|f−µ(f)| ≥r+¯h/bracerightbig
)≤h(r). (92)
In particular, if h(r)≤Cexp(−crp)with0< p <∞, then
µ({|f−M| ≥r})≤C′exp(−κpcrp), (93)
whereC′depends only on Candp,κpdepends only on p, andMis either the mean or the median.
APPENDIX C
PROOF OF LEMMA 2
Proof. From Theorem 1 (see Equation (27)) we know that if t > tαone has an exponential decay in the probability,
while ift≤tαthen the exponent is positive and the bound is typically larg er than1and, thus, trivial. From this,
October 31, 2023 DRAFT23
we can prove that
|PXn(f)−P⊗n
i=1Xi(f)|=|PXn(f−c)|
≤ PXn(|f−c|)
=/integraldisplay∞
0PXn(|f−c| ≥t)dt
≤/integraldisplaytα
01dt+/integraldisplay∞
tαPXn(|f−c| ≥t)dt
≤tα+21
βH1
αα(PXn/⌊ard⌊lP⊗n
i=1Xi)/integraldisplay∞
tαexp/parenleftbigg
−2nt2
β/parenrightbigg
dt
≤tα+21
βH1
αα(PXn/⌊ard⌊lP⊗n
i=1Xi)β
2ntα/integraldisplay∞
tα2nt
βexp/parenleftbigg
−2nt2
β/parenrightbigg
dt
=tα+21
βH1
αα(PXn/⌊ard⌊lP⊗n
i=1Xi)β
2ntα/parenleftbigg
−1
2exp/parenleftbigg
−2nt2
β/parenrightbigg/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle∞
tα
=tα+21
βH1
αα(PXn/⌊ard⌊lP⊗n
i=1Xi)β
4ntαexp/parenleftbigg
−2nt2
α
β/parenrightbigg
=tα+21
ββ
4ntα
=tα+21
ββ
4n√
2n/radicalig
β
αln(Hα(PXn/⌊ard⌊lP⊗n
i=1Xi))
=tα+√β
21
α/radicalig
2n
αln(Hα(PXn/⌊ard⌊lP⊗n
i=1Xi)),
which gives the desired result.
APPENDIX D
TENSORISATION
Many information measures satisfy tensorisation properti es, meaning that, if νandµare measures acting on an
n-dimensional space (typically an n-fold Cartesian product of one-dimensional spaces), it is p ossible to compute
the divergence between νandµusing “one-dimensional projections”. This is particularl y useful when the second
measure is a product-measure. Indeed, if PandQare two probability measures on the space XnandPis a
product-measure, denoting with ¯Xithe(n−1)-tuple(X1,...,X i−1,Xi+1,...,X n), then [1, Proposition 3.1.2]
gives that
D(Q/⌊ard⌊lP)≤n/summationdisplay
i=1/integraldisplay
dQ¯XiD(QXi|¯Xi/⌊ard⌊lPXi), (94)
whereD(Q/⌊ard⌊lP)denotes the KL-divergence between QandP. Hence, having access to a bound on D(QXi|¯Xi/⌊ard⌊lPXi)
for every 1≤i≤nimplies a bound on the KL-divergence between the two n-dimensional measures QandP.
This property is pivotal in proving concentration results i n a variety of ways [1, Section 3.1.3]. Since of independent
interest, we will now state the tensorisation properties of Hαas explicit results, as well as the corresponding R´ enyi’s
Dαanalogues. In particular, whenever the second measure is a p roduct measure while the ﬁrst one is Markovian,
it is possible to prove the following.
October 31, 2023 DRAFT24
Theorem 3. LetQandPbe two probability measures on the space Xnsuch that Q ≪ P , and assume that Pis
a product measure (i.e., P=/circlemultiplytextn
i=1Pi). Assume also that Qis a Markov measure induced by Q1and the kernels
Qi(·|·)with1≤i≤n, i.e.,Q(xn) =Q1(x1)/producttextn
i=2Qi(xi|xi−1). Moreover, given a constant c, letX0=c(almost
surely) be an auxiliary random variable, then,
Hα(Q/⌊ard⌊lP)≤n/productdisplay
i=1P1
βi−1
Xi−1/parenleftbigg
Hβi−1
αiααi(Qi(·|Xi−1)/⌊ard⌊lPXi)/parenrightbigg
, (95)
whereαi≥1fori≥0,β0= 1,αn= 1, andβi=αi/(αi−1). Moreover, selecting αi→1+which implies
βi→ ∞ for every i≥1, one recovers
Hα(Q/⌊ard⌊lP)≤Hα(Q1/⌊ard⌊lP1)·n/productdisplay
i=2max
xi−1∈XHα(Qi(·|xi−1)/⌊ard⌊lPi). (96)
Proof. The proof follows from the steps undertaken in Equations (86 ) to (89) along with the discussion immediately
after, but replacing PXnwithQ.
Theorem 3, which involves products of Hellinger integrals, can be re-written as a sum by considering R´ enyi’s
divergences, due to the relationship connecting the two qua ntities (see Equation (8)).
Corollary 1. Under the same assumptions as in Theorem 3, one has that
Dα(Q/⌊ard⌊lP)≤1
α−1n/summationdisplay
i=11
βi−1logPXi−1/parenleftbigg
exp/parenleftbigg(ααi−1)βi−1
αi(Dααi(Qi(·|Xi−1)/⌊ard⌊lPXi)/parenrightbigg/parenrightbigg
, (97)
whereαi≥1fori≥0,β0= 1,αn= 1 andβi=αi/(αi−1). Moreover, selecting αi→1+which implies
βi→ ∞ for every i≥1, one recovers
Dα(Q/⌊ard⌊lP)≤Dα(Q1/⌊ard⌊lP1)+n/summationdisplay
i=2max
xi−1Dα(Qi(·|xi−1)/⌊ard⌊lPi). (98)
This result allows us to control from above the Hellinger int egral of two n-dimensional objects using the Hellinger
integral of simpler 1-dimensional objects. For instance, if the ﬁrst measure rep resents the distribution induced by
a time-homogeneous Markov chain, then all the kernels Qicoincide and the expression becomes even easier to
compute, as one can see in Section IV.
A similar approach can also be employed to provide a result in cases where Qis an arbitrary measure.
Theorem 4. LetQandPbe two probability measures on the space Xnsuch that Q ≪ P , and assume that Pis
a product-measure (i.e., P=/circlemultiplytextn
i=1Pi). Then,
Hα(Q/⌊ard⌊lP)≤Hα(Q1/⌊ard⌊lP1)·n/productdisplay
i=2max
xi−1
1=x1...xi−1∈Xi−1Hα(Qi(·|xi−1
1)/⌊ard⌊lPi). (99)
Similarly, one has that
Dα(Q/⌊ard⌊lP)≤Dα(Q1/⌊ard⌊lP1)+n/summationdisplay
i=2max
xi−1
1=x1...xi−1∈Xi−1Dα(Qi(·|xi−1
1)/⌊ard⌊lPi). (100)
The proof follows from the same argument of Theorem 3. The key difference is that, without making any
additional assumptions on Q, the entire “past” of the process needs to be considered. Thi s is why writing an
October 31, 2023 DRAFT25
expression similar to Equation (95) without Markovianity w ould be rather cumbersome, and we restrict ourselves
to the setting where the additional parameters are all consi dered to be such that βj→ ∞ .
Finally, we show some lower bounds on Hellinger integrals an d R´ enyi’s divergences.
Theorem 5. LetQandPbe two probability measures on the space Xnsuch that Q ≪ P , and assume that Pis
a product-measure (i.e., P=/circlemultiplytextn
i=1Pi). Assume also that Qis a Markov measure induced by Q1and the kernels
Qi(·|·)with1≤i≤n, i.e.,Q(xn) =Q1(x1)/producttextn
i=2Qi(xi|xi−1). Moreover, given a constant c, letX0=c(almost
surely) be an auxiliary random variable, then,
Hα(Q/⌊ard⌊lP)≥n/productdisplay
i=1P1
βi−1
Xi−1/parenleftbigg
Hβi−1
αiααi(Qi(·|Xi−1)/⌊ard⌊lPXi)/parenrightbigg
, (101)
and
Dα(Q/⌊ard⌊lP)≥1
α−1n/summationdisplay
i=11
βi−1logPXi−1/parenleftbigg
exp/parenleftbigg
sign(αi)(ααi−1)βi−1
αi(Dααi(Qi(·|Xi−1)/⌊ard⌊lPXi)/parenrightbigg/parenrightbigg
,(102)
whereαi≤1fori≥1,β0= 1,αn= 1 andβi=αi/(αi−1). Moreover, selecting α1→1−which implies
βi→ −∞ for every i≥1, one recovers
Hα(Q/⌊ard⌊lP)≥Hα(Q1/⌊ard⌊lP1)·n/productdisplay
i=2min
xi−1∈XHα(Qi(·|xi−1)/⌊ard⌊lPi), (103)
which, in the case of R ´enyi’s divergences, specialises to
Dα(Q/⌊ard⌊lP)≥Dα(Q1/⌊ard⌊lP1)+n/summationdisplay
i=2min
xi−1∈XDα(Qi(·|xi−1)/⌊ard⌊lPi). (104)
Proof. The proof follows from similar arguments as the proof of Theo rem 3. The sole difference is that, instead
of using H¨ older’s inequality at each step, one uses reverse H¨ older’s inequality.
APPENDIX E
EXPLICIT COMPARISON FOR A BINARY KERNEL
The setting is the following: let Kbe a time-homogeneous Markov chain characterised by a doubl y-stochastic
transition matrix characterised by the vector of parameter s¯λ= (λ1,...,λ m)(i.e., the rows and columns of Kare
permutations of ¯λwith the constraints/summationtext
iKi,j=/summationtext
jKi,j= 1). In this case, the Markov chain admits a stationary
distribution πwhich corresponds to the uniform distribution over the samp le space. Hence, if one is considering an
m-dimensional space, then π({x}) =1
mforx∈ {1,...,m}. In this case, if P1∼π, thenPi∼πfor every i≥1.
Moreover, one can prove that K=K←and the bound of Theorem 1 reduces to
PXn/parenleftbig
{|f−P/circlemultiplytext
nXn(f)| ≥t/parenrightbig
≤21
βexp/parenleftbigg
−1
β/parenleftig
2nt2−(n−1)log/parenleftig
m/vextenddouble/vextenddouble¯λ/vextenddouble/vextenddoubleβ
ℓα/parenrightig/parenrightig/parenrightbigg
, (105)
Henceforth, we will consider m= 2 for simplicity and, thus,/vextenddouble/vextenddouble¯λ/vextenddouble/vextenddouble
ℓα:= (/summationtextm
i=1λα
i)1
αcan be expressed as (λα+
(1−λ)α)1
α.Specialising Equation (105) to this setting, one obtains th e following result.
Corollary 2. Letn >1, and let X1,...,X nbe a sequence of random variables such that X1∼π. For every
α >1and every function fsuch that Equation (2)holds true, one has that
P/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤21
βexp/parenleftbigg
−2nt2
β+n−1
βln/parenleftig
2((1−λ)α+λα)1
α−1/parenrightig/parenrightbigg
. (106)
October 31, 2023 DRAFT26
A. Comparison with [9, Theorem 1.2]
The bound obtained via the techniques of [9] is
P(|f−PXn(f)| ≥t)≤2exp/parenleftigg
−2λ2nt2
((1−2λ)n−1)2/parenrightigg
. (107)
Let us denote κα:= ((1−λ)α+λα))1
α−1<1. Then, by direct comparison, it is possible to see that, when ever
t2>((1−2λ)n−1)2(1−1/n)ln(2κα)
2((1−2λ)n−1)2−βλ2):=¯t2, (108)
then the RHS of (106) decays faster than the RHS of (107). In pa rticular, for a given λ <1
2and ifα >4
3, then
¯t2= (1+on(1))ln(2κα)
2(1−βλ2)<(1+on(1))2ln2
4−β. (109)
Here, one can explicitly see the trade-off between the proba bility term and the Hellinger integral, described
in Remark 5 and mediated by the choice of α. Taking the limit α→ ∞ in Equation (106) leads to the following
upper bound
P/parenleftbig/vextendsingle/vextendsinglef−P/circlemultiplytextn
i=1Xi(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤2exp/parenleftbig
−2nt2+(n−1)ln(2(1 −λ))/parenrightbig
. (110)
In this setting, in order to improve over Equation (107), one needst2>¯t2, with
¯t2= (1+on(1))ln(2(1−λ))
2(1−λ2)<(1+on(1))2ln2
3. (111)
Clearly,1−λ > καfor every α >1. Hence, on the one hand, Equation (110) introduces a worse mu ltiplicative
constant (a larger αimplies a larger Hellinger integral, and we are considering the limit of α→ ∞ ) and increases
the minimum value of tone can consider for a given λ <1/2(Equation (109) is increasing in α). On the other hand,
it provides a faster exponential decay with n. In fact, as β→1, the RHS of (110) scales as exp(−2nt2(1+on(1)))
for large enough t, which matches the behavior of Equation (3).
B. Comparison with [3]
In the setting considered above, [3, Theorem 1] gives
P/parenleftbig/vextendsingle/vextendsinglef−π⊗n(f)/vextendsingle/vextendsingle≥t/parenrightbig
≤2exp/parenleftbigg
−2λ
1−λnt2/parenrightbigg
. (112)
This means that, considering the decay provided by Equation (110) (which optimises the speed of decay for large
enought) whenever
t2>1−λ
2−4λln(2(1−λ))(1+on(1)), (113)
then Equation (110) leads to a faster decay than Equation (11 2).
October 31, 2023 DRAFT27
C. Comparison with [5]
For the kernel considered in this appendix, Equation (53) ho lds with Cn= (n−1)log(2(1 −λ)). Further-
more, for every iandx,ˆx, we have that TV(PXi|Xi−1=x,PXi|Xi−1=ˆx) = (1−2λ).Consequently, assuming
t >1/(2λ)/radicalbig
ln(1/PXn(E))/n, [5, Proposition 1 & Proposition 4] give
PXn(Ec
t)≤exp
−2n/parenleftigg
t(2λ)−/radicalbigg
ln(1/PXn(E))
2n/parenrightigg2
 (114)
≤exp/parenleftigg
−8nt2λ2+8tλ/radicalbigg
nln2
2/parenrightigg
. (115)
Comparing Equation (54) with Cn= (n−1)log(2(1 −λ))with Equation (115), one can see that, whenever
t≥/radicalbig
2ln(2(1−λ))
(1−4λ2)(1+on(1)), (116)
then Equation (54) improves over Equation (115).
A similar comparison can be drawn with respect to the tools in [4] (in which the degree of dependence is
measured differently), but it would lead to a worse bound tha n that expressed in Equation (115).
D. MCMC
Considering the same setting detailed in the previous subse ctions, one can more explicitly characterise the
parameters determining Equation (81). In particular, one h as thatπ= (1/2,1/2)and the spectral gap is equal
to1−2λ. Consequently, if n0= 0 and one considers α→ ∞ , Equation (79) becomes:
P/parenleftigg
1
nn/summationdisplay
i=1f(Xn0+i)−π(f)> t/parenrightigg
≤2exp/parenleftbigg
−λ
1−λ·2nt2
(b−a)2/parenrightbigg
·max{ν({0}),ν({1})}, (117)
while Equation (80) boils down to the following:
P/parenleftigg
1
nn/summationdisplay
i=1f(Xn0+i)−π(f)> t/parenrightigg
≤2exp/parenleftbigg
−2nt2
(b−a)2/parenrightbigg
(2−2λ)n−1·max{ν({0}),ν({1})}. (118)
Making a direct comparison one can see that if
t2≥/parenleftbigg/parenleftbigg
1−1
n/parenrightbigg
ln(2−2λ)/parenrightbigg(b−a)2
21−λ
1−2λ, (119)
then Equation (118) provides a faster decay than Equation (1 17).
E. Comparison between SDPI for Dαand hypercontractivity
Ifµ= (1/2,1/2)andK=BSC(λ), thenµK=µand one has, following Wyner’s notation [36, Eq. 1.17], the
so-called Doubly-Symmetric Binary Source “DSBS (λ)”. In this setting, the hyper-contractivity constant is giv en by
[29, 30]
γ⋆(α) = (1−2λ)2(α−1)−1. (120)
Moreover, one can analytically see that, for every µ,
Dα(K/⌊ard⌊lµK)
Dα(δ0/⌊ard⌊lµ)<(1−2λ)(1+1
α)
(1−λ)(α−1)
α. (121)
October 31, 2023 DRAFT28
Asα→1+, the LHS of Equation (121) approaches a ratio between KL-div ergences, while the RHS approaches
(1−2λ)2, which equals ηKL(K)[31]. Furthermore, taking the limit of α→ ∞ (which optimises the exponential rate
of decay), the expression in Equation (121) provides an impr ovement over simply using DPI, while Equation (120)
trivialises to γ⋆(+∞) = +∞. Hence, denoting with E={|f−P/circlemultiplytextXi(f)| ≥t}, one has that, for every α >1,
PXn(E)≤21
βexp/parenleftbigg
−2nt2
β/parenrightbigg
·

exp/parenleftig
(1−2λ)2(α−1)−2
(1−2λ)2(α−1)−1(n−1)log2/parenrightig
via Equation (34) & Equation (120) ,
exp/parenleftbigg
1
β(1−2λ)(1+1
α)
(1−λ)(α−1)
α(n−1)log2/parenrightbigg
via Equation (47) & Equation (121) .
(122)
One can see that, for αlarge enough, Equation (47) improves upon Equation (34) for everyλ. In fact, taking
α→ ∞ gives
PXn(E)≤2exp/parenleftbig
−2nt2/parenrightbig
·

exp((n−1)log2) via Equation (34) & Equation (120) ,
exp/parenleftig
(1−2λ)
(1−λ)(n−1)log2/parenrightig
via Equation (47) & Equation (121) .(123)
APPENDIX F
PROOF OF LEMMA 1
Proof. For every n≥0, we have
supp(Sn) =n/uniondisplay
j=0{n−2j}.
Moreover, given 0≤j≤n,
P(Sn=n−2j) =/parenleftbiggn
n−j/parenrightbigg
2−n.
Furthermore, given x∈supp(Sn−1)and0≤j≤n,
PSn|Sn−1=x(n−2j) =1
2
/BD{n−2j−x=1}+1
2
/BD{n−2j−x=−1}.
October 31, 2023 DRAFT29
Therefore, the following chain of inequalities holds:
Hα(PSn|Sn−1=x/⌊ard⌊lPSn) =n/summationdisplay
j=0Pα
Sn|Sn−1=x(n−2j)·P1−α
Sn(n−2j)
= 2−α/parenleftbig
P1−α
Sn(x+1)+P1−α
Sn(x−1)/parenrightbig
= 2−α/parenleftigg/parenleftbigg/parenleftbiggn
n+x+1
2/parenrightbigg
2−n/parenrightbigg1−α
+/parenleftbigg/parenleftbiggn
n+x−1
2/parenrightbigg
2−n/parenrightbigg1−α/parenrightigg
= 2−α+n(α−1)/parenleftigg/parenleftbiggn
n+x+1
2/parenrightbigg1−α
+/parenleftbiggn
n+x−1
2/parenrightbigg1−α/parenrightigg
≤2−α+n(α−1)/parenleftigg/parenleftbiggn
n+x+1
2/parenrightbiggn+x+1
2(1−α)
+/parenleftbiggn
n+x−1
2/parenrightbiggn+x−1
2(1−α)/parenrightigg
(124)
= 2−α+n(α−1)/parenleftigg/parenleftbiggn+x+1
2n/parenrightbiggn+x+1
2(α−1)
+/parenleftbiggn+x−1
2n/parenrightbiggn+x−1
2(α−1)/parenrightigg
(125)
≤2−α+n(α−1)/parenleftigg/parenleftbiggn+x+1
2n/parenrightbiggn+x+1
2(α−1)
+/parenleftbiggn+x+1
2n/parenrightbiggn+x−1
2(α−1)/parenrightigg
(126)
= 2−α+n(α−1)/parenleftbiggn+x+1
2n/parenrightbiggn+x−1
2(α−1)
·/parenleftigg
1+/parenleftbiggn+x+1
2n/parenrightbigg(α−1)/parenrightigg
(127)
≤2−α+n(α−1)+1.
Here, the inequality (124) follows from the fact that/parenleftbign
k/parenrightbig
≥/parenleftbign
k/parenrightbigkalong with 1−α≤0; the inequality (126)
follows from the fact thatn+x+1
2n>0. Moreover, it is easy to see thatn+x+1
2=n+x−1
2+ 1 and thus the
factorisation in Equation (127) follows. To conclude, it su fﬁce to notice thatn+x+1
2n≤1. Denoting with1
β=α−1
α,
the computations just above imply that
H1
αα(PSn/⌊ard⌊lP/circlemultiplytextn
j=1Sj)≤n/productdisplay
i=2max
x∈suppSi−1H1
αα(PSi|Si−1=x/⌊ard⌊lPSi)
≤n/productdisplay
i=221
α−1+i
β=n/productdisplay
i=22−1
β+i
β= 21
β/summationtextn
i=2(i−1)= 21
β/summationtextn−1
j=1j= 2n(n−1)
2β.
For the lower bound we need the following observations. Let u s denote with
h(k) =/parenleftbiggn
n+k+1
2/parenrightbigg1−α
+/parenleftbiggn
n+k−1
2/parenrightbigg1−α
, (128)
one can easily prove that:
•h(k) =h(−k)i.e., it is even;
•h(k)is decreasing if k∈ {−n+1,...,0}and increasing if k∈ {0,...,n−1}.
October 31, 2023 DRAFT30
Hence,argmink∈{−n+1,...,n−1}h(k) = 0 andargmaxk∈{−n+1,...,n−1}h(k) ={n−1,−n+1}. One thus has that:
Hα(PSn|Sn−1=x/⌊ard⌊lPSn) = 2−α+n(α−1)/parenleftigg/parenleftbiggn
n+x+1
2/parenrightbigg1−α
+/parenleftbiggn
n+x−1
2/parenrightbigg1−α/parenrightigg
(129)
≥2−α+n(α−1)2/parenleftbiggn
n+1
2/parenrightbigg1−α
(130)
= 2−α+n(α−1)+1/parenleftigg
n!/parenleftbign+1
2/parenrightbig
!/parenleftbign−1
2/parenrightbig
!/parenrightigg1−α
(131)
≥2−α+n(α−1)+1/parenleftigg/radicalbiggn
2π/parenleftbign+1
2n−1
2/parenrightbig2nh2(n+1
2n)/parenrightigg1−α
(132)
= 2−α+n(α−1)+1−(α−1)nh2(n+1
2n)/parenleftigg/radicaligg
2n
π(n2−1)/parenrightigg1−α
(133)
= 2−α+n(α−1)+1−(α−1)nh2(n+1
2n)+(α−1)
2log/parenleftBig
π
2n2−1
n/parenrightBig
, (134)
where Equation (130) follows from selecting x= 0, Equation (132) follows from the fact that
/parenleftbiggn
k/parenrightbigg
≤/radicalbiggn
2πk(n−k)2nh2(k/n),
whereh2(x) =−xlog2(x)−(1−x)log2(1−x)denotes the binary entropy (see [37, Problem 5.8]). Thus,
log2H1
αα(PSn/⌊ard⌊lP/circlemultiplytextn
j=1Sj)≥log2n/productdisplay
i=2min
x∈suppSi−1H1
αα(PSi|Si−1=x/⌊ard⌊lPSi) (135)
=1
α/parenleftiggn/summationdisplay
i=2(1−α)+i(α−1)−(α−1)ih2/parenleftbiggi+1
2i/parenrightbigg
+(α−1)
2log2/parenleftbiggπ
2/parenleftbiggi2−1
i/parenrightbigg/parenrightbigg/parenrightigg
(136)
=n/summationdisplay
i=2−1
β+1
βi/parenleftbigg
1−h2/parenleftbiggi+1
2i/parenrightbigg/parenrightbigg
+1
2βlog2/parenleftbiggπ
2/parenleftbiggi2−1
i/parenrightbigg/parenrightbigg
(137)
=1
β/parenleftiggn/summationdisplay
i=2−1+i/parenleftbigg
1−h2/parenleftbiggi+1
2i/parenrightbigg/parenrightbigg
+1
2log2/parenleftbiggπ
2/parenleftbiggi2−1
i/parenrightbigg/parenrightbigg/parenrightigg
(138)
≥1
β/parenleftigg
1
4+n/summationdisplay
i=4−1+i/parenleftbigg
1−h2/parenleftbiggi+1
2i/parenrightbigg/parenrightbigg
+1
2log2/parenleftbiggπ
2/parenleftbiggi2−1
i/parenrightbigg/parenrightbigg/parenrightigg
(139)
≥1
β/parenleftigg
1
4+n/summationdisplay
i=4−1+1
2log2/parenleftbiggπ
2/parenleftbiggi2−1
i/parenrightbigg/parenrightbigg/parenrightigg
(140)
≥1
β/parenleftbigg1
4−(n−3)+(n−3)
2log2/parenleftbigg15π
8/parenrightbigg/parenrightbigg
(141)
=1
β/parenleftbigg1
4+(n−3)/parenleftbigg
−1+1
2log2/parenleftbigg15π
8/parenrightbigg/parenrightbigg/parenrightbigg
(142)
≥1
β/parenleftbigg1
4+(n−3)
4/parenrightbigg
=1
4β(n−2), (143)
where Equation (139) follows as the ﬁrst two terms in the summ ation add up to ≈0.27, Equation (140) follows
from the fact that 0≤h2(k)≤1with equality if and only if k=1
2and thus 1−h2((i+ 1)/2i)≥0(fori
large enough the difference approaches 0). Moreover, since g(x) = (x2−1)/xis an increasing function and thus
g(x)≥g(4)forx≥4, one also has that Equation (141) holds. To conclude, one can see that1
2log2/parenleftbig15π
8/parenrightbig
−1>1
4.
October 31, 2023 DRAFT31
APPENDIX G
PROOF OF THE INEQUALITIES IN (73) AND (74)
Given the setting, denoting with yn=/summationtextn−1
i=1pixi, one has that
Hα(PXn|Xn−1=xn−1
1/⌊ard⌊l(1/2,1/2))=/parenleftbigg1
2/parenrightbigg1−α/parenleftigg/parenleftigg
1
2+n−1/summationdisplay
i=1pixi/parenrightiggα
+/parenleftigg
1
2−n−1/summationdisplay
i=1pixi/parenrightiggα/parenrightigg
≤/parenleftbigg1
2/parenrightbigg1−α
/parenleftigg
1
2+n−1/summationdisplay
i=1pixi/parenrightigg⌊α⌋
+/parenleftigg
1
2−n−1/summationdisplay
i=1pixi/parenrightigg⌊α⌋

=/parenleftbigg1
2/parenrightbigg1−α/parenleftigg⌊α⌋/summationdisplay
k=0/parenleftbigg⌊α⌋
k/parenrightbigg/parenleftbigg1
2/parenrightbigg⌊α⌋−k
yk
n+⌊α⌋/summationdisplay
k=0/parenleftbigg⌊α⌋
k/parenrightbigg/parenleftbigg1
2/parenrightbigg⌊α⌋−k
(−yn)k/parenrightigg
=/parenleftbigg1
2/parenrightbigg1−α
⌊α⌋/summationdisplay
k=0/parenleftbigg⌊α⌋
k/parenrightbigg/parenleftbigg1
2/parenrightbigg⌊α⌋−k/parenleftbig
yk
n+(−yn)k/parenrightbig

=/parenleftbigg1
2/parenrightbigg1−α
⌊α/2⌋/summationdisplay
j=0/parenleftbigg⌊α⌋
2j/parenrightbigg/parenleftbigg1
2/parenrightbigg⌊α⌋−2j
2y2j
n

≤2⌊α/2⌋/summationdisplay
j=0/parenleftbigg⌊α⌋
2j/parenrightbigg
(2yn)2j.
Moreover, setting pi= 2−i−1gives
max
xn−1
1Hα(PXn|Xn−1=xn−1
1/⌊ard⌊l(1/2,1/2))≤2max
xn−1
1⌊α/2⌋/summationdisplay
j=0/parenleftbigg⌊α⌋
2j/parenrightbigg/parenleftigg
2n−1/summationdisplay
i=1pixi/parenrightigg2j
= 2⌊α/2⌋/summationdisplay
j=0/parenleftbigg⌊α⌋
2j/parenrightbigg/parenleftiggn−1/summationdisplay
i=12−i/parenrightigg2j
= 2⌊α/2⌋/summationdisplay
j=0/parenleftbigg⌊α⌋
2j/parenrightbigg/parenleftbig
1−2−n+1/parenrightbig2j
≤2⌊α/2⌋/summationdisplay
j=0/parenleftbigg⌊α⌋
2j/parenrightbigg
= 2⌊α⌋≤2α.
October 31, 2023 DRAFT