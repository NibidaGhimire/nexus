SepicNet: Sharp Edges Recovery by Parametric Inference of Curves in 3D
Shapes
Kseniya Cherenkova⋆†
kseniya.cherenkova@uni.luElona Dupont⋆
elona.dupont@uni.luAnis Kacem⋆
anis.kacem@uni.lu
Ilya Arzhannikov†
iarzhannikov@artec3d.comGleb Gusev†
gleb@artec3d.comDjamila Auoada⋆
djamila.aouada@uni.lu
⋆SnT, University of Luxembourg†Artec 3D
Abstract
3D scanning as a technique to digitize objects in real-
ity and create their 3D models, is used in many fields and
areas. Though the quality of 3D scans depends on the tech-
nical characteristics of the 3D scanner, the common draw-
back is the smoothing of fine details, or the edges of an ob-
ject. We introduce SepicNet, a novel deep network for the
detection and parametrization of sharp edges in 3D shapes
as primitive curves. To make the network end-to-end train-
able, we formulate the curve fitting in a differentiable man-
ner. We develop an adaptive point cloud sampling tech-
nique that captures the sharp features better than uniform
sampling. The experiments were conducted on a newly in-
troduced large-scale dataset of 50k 3D scans, where the
sharp edge annotations were extracted from their paramet-
ric CAD models, and demonstrate significant improvement
over state-of-the-art methods.
1. Introduction
Today’s high-precision 3D scanning technologies help to
obtain extremely realistic 3D models of real objects, which
can be used in various applications, e.g., in 3D design and
modeling [4, 8, 20, 21], for reverse engineering purposes in
the context of Computer-Aided Design (CAD) [3,6]. One of
the major issues encountered in the process of scanning the
object, is the sharpness degradation effect. For instance, 3D
scanning tends to smooth the high-level geometrical details
of real objects such as sharp edges. Depending on the qual-
ity of a 3D scanner, this effect is less or more prominent.
In this work, we propose to recover sharp features from
a 3D reconstructed model in a data-driven manner by para-
metric inference of sharp edges of a 3D model. We de-
Figure 1. CAD model vs. scanned 3D model vs. sharp recovered
3D model and the detected sharp edges marked in red color.
velop an end-to-end trainable network architecture called
SepicNet (Sharp Edges recovery by Parametric Inference of
Curves). SepicNet consists of two main modules. The first
module takes as input a sampled 3D point cloud, detects
sharp edge points and groups them into different segments
with predicted primitive types per segment among line, cir-
cle, and b-spline. The second module estimates the corre-
sponding parameters of the identified primitives throughout
a differentiable fitting. As the output of our network we
have a set of parametrized sharp edges, labeled on a point
cloud, that are further used to recover the sharp edges of
a corresponding 3D scan by reprojecting nearby mesh ver-
tices on parametrized edges. Considering the feature in-
tense nature of 3D models, we also propose a novel adap-
tive point sampling scheme designed to approximate their
geometry better than the standard uniform sampling. The
design of SepicNet is substantially novel in its effort to in-
corporate the priors learnt from a vast collection of corre-
sponding pairs of CAD models and their 3D scanned coun-
terparts.
Our contributions . We develop a modular SepicNet ar-arXiv:2304.06531v1  [cs.CV]  13 Apr 2023chitecture for parametric inference of sharp edges from 3D
scans of objects. We propose an edge points consolidation
approach based on a distance field to closest sharp edge es-
timation. To account for topological complexity of an ob-
ject we introduce an adaptive sampling technique for high-
resolution details based on principal curvatures of model’s
surface. We outsource a large-scale CC3D-PSE dataset with
pairs of CAD models and their virtually scanned counter-
parts annotated with sharp edge parametrizations as basic
curves: lines, circles and b-splines.
2. Related Work
Scanned 3D point sets are irregular and non-uniform,
and need to be consolidated to enhance the surface recon-
struction quality. One possible solution is to introduce edge-
awareness in the consolidation of point sets in a data-driven
manner. The EC-NET [25] network processes patches of
points and learns to consolidate points using an edge-aware
joint loss function when learning from the data. The perfor-
mance of the model was demonstrated on a very limited set
of 12 manually labelled scans. Another way is to introduce
the a-priors with the objective to preserve sharp features for
surface reconstruction methods as done in DeepPrior [24].
In both cases, the presence of high-frequency features and
noise in the input scanned data, makes it extremely chal-
lenging to recover the sharpness of the scans.
The inference of edges as parametric curves is an alterna-
tive that arises directly from CAD surface parametrization
as boundary-representation (b-rep). Following this direc-
tion, the PC2WF model [12] infers a wireframe of linear
edges from a point cloud based on a vertex localization and
an edge detector that identifies the pairs of vertices con-
nected with an edge. The work [14] proposes a paramet-
ric approach to extract a wireframe based on an estimated
scalar distance field DEF [15] that represents the proximity
to the nearest sharp feature curve. PIE-Net [23] proposes
to jointly detect edge and corner points, after which a curve
proposal module generates an over-complete collection of
curves that are further ranked.
The performance of all the above learning-based men-
tioned methods is heavily dependent on the variance of 3D
data used for training. Several related datasets have been
collected by the community. For instance, EC-Net [25] used
a subset of data from ShapeNet dataset [1], which totals to
3M shapes with semantic and category annotations. Despite
its large size, ShapeNet dataset does not offer any informa-
tion about the edges of CAD models. The ABC dataset [9]
used in PIE-Net [23], is a collection of 1M parametric CAD
models, where the edge sharpness information can be ex-
tracted from surface patches smoothness labels. The avail-
ability of CAD models in this dataset makes it appealing for
parametric sharp edge inference, but the scanned 3D coun-
terparts are missing. More recently, the CC3D dataset [2]has been proposed to bring the ongoing research on to a
real-world scenario. It consists of more than 50k pairs of
scanned 3D shapes and CAD models unrestricted to any cat-
egory with varying complexity. In our work, we exploited
the information from the CAD models in CC3D dataset to
extract and parametrize ground truth sharp edges of each
scan.
3. Method
The proposed approach aims to recover sharp edges in
a 3D shape with smooth edges as depicted in Figure 1, ap-
proximating the ones of the CAD model. These sharp edges
are learned from the pairing of 3D scans and CAD models.
A 3D scan χprovided as a triangular mesh is approx-
imated by an intermediate representation as a point cloud
X={xj} ∈RN×3, where Ndenotes the number of
points. The edge points on the scans are assigned based
on the distance threshold τto the closest edge in the CAD
model. Ground truth sharp edges are extracted from CAD
models by computing the deviations of normals between
two neighboring surface patches and considering edges
above the selected threshold to be sharp. The resulting set
of sharp edges can be then represented by the corresponding
parametric set of curves Eextracted from the CAD models.
Given a set of Mpairs of 3D scans and sharp edge anno-
tations {Xi, Ei}M
i=1, our objective is to learn a non-linear
mapping, Ψ, such that
Ψ :RN×3→ E,
∀1≤i≤N, Ψ(Xi) =ˆEi≈Ei,
where Eis the set of all parametric edges, Eiis the set
of ground-truth parametric sharp edges and ˆEiis the set of
predicted ones from the 3D scan Xi.
We address the learning of the mapping Ψthrough ap-
proximating it by a neural network we called SepicNet ,
which stands for Sharp Edges recovery by Parametric In-
ference of Curves.
As illustrated in Figure 3, the proposed approach consists
of three main steps: (1) dataset preparation and sampling
point clouds on 3D scans; (2) detecting sharp edge points
on the sampled point clouds and decomposing the result-
ing points into different segments; (3) and fitting parametric
curves on them. In next subsections, we explain further the
aforementioned steps in detail.
3.1. Parametric Sharp Edges Dataset
The proposed approach is a supervised method that re-
quires parametric sharp edge annotations of 3D scans. Con-
sequently, we build on top of a publicly available dataset
CC3D [2], which contains more than 50k CAD models
in STEP format, unrestricted to any category, with vary-
ing complexity from simple to highly detailed designs. InFigure 2. Sample models from CC3D-PSE, from left to right, the CAD model, the 3D scan and the parametric sharp edges.
Sharp Edge typeNumber of sharp edges per modelTotal number of sharp edges Number of modelsMinimum Lower quartile Median Upper Quartile Maximum
All ModelsLine 0 4 24 102 14278 7320145
50164Circular 0 0 0 15 8200 2079922
Spline 0 4 12 38 13764 2258783
All 0 16 52 186 23064 11658850
Models with only line edges Line 1 12 24 76 12480 - 3505
Models with only circular edges Circular 1 3 4 8 4096 - 7835
Models with only spline edges Spline 1 4 8 24 1681 - 265
Models with no sharp edges - - - - - - - 1210
Table 1. Distribution of the number of sharp edges in the CC3D-PSE dataset.
this dataset, CAD models are virtually scanned and recon-
structed using a proprietary 3D scanning pipeline resulting
in pairs of 3D models in triangular mesh format and CAD
models as b-reps. Unlike other alternatives such as ABC
dataset, where the noise is usually synthetically added to
the sampled point cloud, the 3D shapes in CC3D dataset
have geometrical details such as edges smoothed due to the
limitations of a scanning technology.
Accordingly, we propose a new dataset called CC3D-
PSE1consisting of 3D models annotated with parametric
sharp edges. Some examples are shown in Figure 2. We ex-
tracted sharp edge annotations from CAD models and trans-
ferred them to the corresponding 3D models. OpenCascade
API was used to extract the parametric sharp edges directly
from b-rep models of CC3D dataset. Three types of para-
metric curves are considered consisting of lines, circles and
splines, that are common to describe the boundary curves in
b-rep formats.
From the CC3D-PSE dataset statistics reported in Ta-
ble 1, one can conclude that lines are the most common
primitive curves with more than 7million lines represent-
ing more than 62% of the edges in the whole dataset. The
second most common primitive is spline with more than 2
million spline segments. However, models with only spline
edges only represent 0.5%of the dataset which suggests
that spline segments are usually combined with other types
of primitives to construct sharp edges of a model. Circu-
lar segments are present in the dataset in a similar number
to splines recording more than 2million circular segments.
Nevertheless, circular segments were enough to construct
the sharp edges of 7835 models which consists of more
than15% of the dataset. The CC3D-PSE has been used
in SHARP challenges2.
1The CC3D-PSE dataset is publicly available on https://cvi2.
uni.lu/cc3d-pse/ .
2https://cvi2.uni.lu/sharp2022/challenge2/ .3.2. Decomposition Module
Given an input point cloud Xsampled on a 3D scan, the
decomposition module detects sharp edge points, consoli-
dates them along the edge and groups them into different
segments with primitive types identified.
Detection : The first part of the decomposition module
predicts the sharp edge points by assigning a binary label
to each point of the input point cloud indicating its belong-
ing to a sharp edge. Rather than use standard binary cross-
entropy loss, we use the focal loss [11] for its robustness
to the class imbalance in the data, as the number of sharp
edge points is usually smaller than the number of non-edge
points. Therefore, the edge loss is defined by
Le=Lfocal =−NX
j=1(1− P⋆
j)ηlogP⋆
j, (1)
where ηis a modulating factor reducing the loss contribu-
tion from easy examples, P⋆
j=Pjif the point xjis a an
edge point and P⋆
j= 1− Pjotherwise, with Pjdenoting
the model’s estimated probability for the point xjbeing an
edge point.
Consolidation : The consolidation of the points along
the edge is done by predicting per point displacement vec-
tors (offsets) ˆvj∈R3by the network. After estimating the
offsets, or in other words, the vector distance field to the
closest edge, we apply them to predicted sharp edge points
by a simple addition x⋆
j=xj+ˆvj. The loss function for
edge distance field estimation consists of a regression of the
offsets ˆvjusing L2-norm with respect to the ground-truth
ones as follows
Lo=NX
j=1||ˆvj−vj||2, (2)
where the ground-truth offsets vjare computed in advance
based on point-to-edge distances between the point cloudsampled on the scan Xand the ground-truth edges E.
Primitive Type Classification : Our SepicNet predicts
the type of the primitive of each segment among three pos-
sible types, namely, line, circle, and spline segments. The
standard categorical cross-entropy loss Ltis used to learn
the primitive types. The primitive type of the segment is
defined based on the major voting of its points.
Clustering on Embeddings : The number of different
segments in different models vary greatly in CC3D-PSE
dataset, thus we find it limiting to have a fixed maximum
number of edge segments. In our case the clustering is done
without knowing the number of segments a priori to dis-
cover groups of them belonging to the same segments. This
is achieved by learning a point-wise embedding of the de-
tected sharp edge points Φ :R3→Rd, and applying the
standard density based clustering algorithm on the embed-
ded edge points to obtain a membership matrix ˆW, such
thatˆwk=ˆW:,k∈[0,1]Kiindicates the points belong-
ing to k-th segment of a model Xiwith a total number of
edges segments Ki. The embedding Φis learned using a
triplet mining strategy. Given an anchor sharp edge point
x⋆
j, triplets of edge points (x⋆
j,x⋆+
j,x⋆−
j)are formed by
considering an edge point from the same segment x⋆+
jand
another one from a different segment x⋆−
j. The triplets are
selected randomly during the training. Point pairs from the
same segment are embedded closer to each other to form
a cluster, while points belonging to different segments are
pushed away using the following triplet loss
Lemb=nTX
j=1max(∥Φ(x⋆
j)−Φ(x⋆+
j)∥2− ∥Φ(x⋆
j)−Φ(x⋆−
j)∥2+θ,0),
(3)
where nTis the number of triplets generated from the edge
points and θis a margin between positive and negative pairs.
In our experiments, θ= 0.5. During training, a differen-
tiable version of the mean-shift clustering algorithm is used
analogous to the one in [22]. At the inference time, we ex-
changed it with a GPU-accelerated version of hdbscan from
cuML [19], as it improved the results in our experiments.
Hdbscan is known to excel when the data has arbitrarily
shaped clusters of different sizes and densities, and when a
certain amount of noise is involved.
The final decomposition loss is a weighted combination
of the all the above mentioned losses
Ldcmp =αeLe+αoLo+αtLt+αembLemb.(4)
3.3. Fitting Module
The curve parameter estimation is an analytic function
of segmented and offsetted points x⋆
j, i.e. the predicted
membership matrix ˆW. An edge segment is represented
as one of the following types including a linear segment
Figure 3. Overview of the SepicNet pipeline. The detection and
segmentation modules take a 3D point cloud (with optional nor-
mals and curvatures) and decompose it into segments labeled by
primitive type. The fitting module predicts parameters of a prim-
itive that best approximates each segment. The three modules are
jointly trained in an end-to-end manner.
Hl= (xs,xe), where xs,xe, are the start and end points re-
spectively; a circular arc Hc= (xc,xs,xe, r, ρ, n), where
xc,xs,xeare center, start and end points, ris the radius,
andρspecifies the rotation direction with respect to the ref-
erence plane normal n; and b-spline Hbis defined as b-
spline with control points {xcp
d, d∈(3, D−1)}, where D
is its degree.
Parametric curves fitting : These parameters of the
curves are estimated via least-squares fitting based on dif-
ferentiable SVD on the set of segmented edge points. Here
we describe the estimation of the parameters {Hk}for
three types of curves (lines, circles, b-splines) from a set
of 3D points X={xj}and their predicted membership
ˆw= ˆW:,k. For simplicity, we will assume a fixed k
and omit it in the formulas. Alongside, we also give the
parametric definitions of curve segments introducing a sin-
gle parameter tthat is used for sampling points from a
parametrized segment and to correctly define the bound-
ary values on the sampling parameter tfor each segment
later. A line in parametric form is Ll(t) =xs+tu, where
u= (xe−xs)/||(xe−xs)||is a unit line direction vector,
ts≤t≤te. A circular arc is Lc(t) =xc+ucost+vsint,
andu=xs−xcandv=u×n. A spline is parametrized as
Lb(t) =Pk
i=0xcp
iβK
i(t), where βK
i(t)isithb-spline basis
function of order K.
The distance from a point xto a line Hlcan be calculated
by the following formula:
D2
line(x, Hl) =||(x−(xs+tu))||2. (5)
Defining ˆHas the minimizer to the weighted sum of
squared distances, we have to solve
Eline(Hl, X, w ) =NX
j=1wj(Xj,:−(xs+tu))2,(6)which becomes minimizing a linear least-square problem.
Solving∂E
∂xs= 0 we obtain ˜xsas the mean point ˜xs=
1
nP
ixj. The parameter ˆuon the set of mean-centered
points X= (x0−˜xs, ..,xn−1−˜xs)Tis a solution to
E(u, X, w ) =||diag(w)1
2Xu||2. (7)
The solution
˜u= arg min
u∈R3,||u||=1||Au||2(8)
is given by the right singular vector corresponding to the
largest singular value of matrix A=diag(w)1
2X. As shown
in [7], the gradient with respect to Vcan be backpropagated
through the SVD computation.
To estimate the parameters of a circle from a set of 3D
points we perform the following steps, first fit the plane onto
a set of mean-centered points X; then project the mean-
centered points onto a fitted plane to get 2D coordinates;
use least-squares to fit circle in 2D coordinates to get the
estimations of center ˜xcand radius ˜r; and finally, transform
the circle center back to 3D coordinates.
The find the estimation of the normal ˜nof a plane we
follow [10], which leads to a homogeneous least square
problem same as Equation 8, and can be solved in the same
way.
The distance from a point xto a circle Hcis defined as
D2
circle(x, Hc) = (||x−xc|| −r)2. (9)
To project 3D points onto the fitting plane the following
Rodrigues rotation formula is used
Xr=Xcosθ+(k×X) sinθ+k(k·X)(1−cos(θ)).(10)
where k= ˜nnT
z, nz= (0,0,1), and θ= arccos(˜ n, nz).
The implicit equation of a circle with a radius rand a
center (xc, yc)in 2D is (x−xc)2+ (y−yc)2=r2, or
(2xc)x+ (2yc)y+ (r2−x2
c−y2
c) =x2+y2, introduc-
ingc= (c0, c1, c2)as a vector for unknown coefficients.
To solve Arc=bwe consider fitting using differences of
squared lengths and squared radius, or the following notion
of the distance:
Ecircle(Hc, X, w ) =NX
j=1wj(||c−Xj,:||2−r2)2.(11)
Setting∂Ecircle
∂r2 = 0 , we obtain the radius ˜r=
(1
nP
j(||xj−˜c||2))1
2. Plugging ˜rback in Equation 11
we end up with
Ecircle(c, X, w, r ) =||diag(w)1
2Xc−y||2, (12)
where Xi,:= 2(−Xi,:+PN
j=1wjXj,:PN
j=1wj)andyi=XT
i,:Xi,:−
PN
j=1wjXT
j,:Xj,:PN
j=1wjThis least-squares problem is solved in dif-
ferentiable manner [16] via Cholesky decomposition.The fitting of b-spline curves is done via approximation
using least-squares method with fixed number of control
points. To fit b-splines we implement an algorithm A9.1
described in the book [17]. For this approximation to work,
we need to specify the order of points in which the b-spline
uses them in the formulation. This ordering we perform on
the basis of the minimal spanning tree on the input embed-
dings returned by hdbscan. For each spline segment in this
tree we find the longest path through breadth-first search,
which gives the ordering of the corresponding points.
The fitting residual loss is a sum of segment distances
between a predicted parametrized segment ˆHkand a set of
points ˜xsampled uniformly U(Hk)from ground truth seg-
ments Hkof type tkdefined as
Lfit=Lres=X
kEp∼U(Hk)Dtk(˜x,ˆHk). (13)
For basic primitives (i.e., lines and circles), the distances
Dtkbetween a point and a parametrized curve are com-
puted analytically, while for b-splines, we approximate it
with Chamfer distance [5]. The end-to-end training is per-
formed with a sum of decomposition and fitting losses
Ltotal=αLdcmp +βLfit. (14)
3.4. Adaptive Sampling
The complexity of the models in CC3D-PSE dataset, as
well as real 3D shapes can significantly vary from basic
shapes to highly detailed designs with small elements as
shown in examples in Figure 2. While uniform or Pois-
son sampling are typically used for point cloud generation
from a 3D surface, these methods are insensitive to the high
resolution details. To be able to recover sharp features, we
develop a new adaptive sampling algorithm that produces
dense point sets around regions with high-resolution ge-
ometrical details. The principal curvatures there undergo
large changes compared to the low resolution regions. The
weighted sample elimination technique, described in [26],
is adapted to the weights computed on the principal curva-
tures κ1andκ2at a given point on a 3D models. For each
point xj∈R3in the point cloud of a 3D shape Xi, we
assign the weight according to the equation below:
ω(xj) = exp ( |κ1(xj)|+|κ2(xj)|)γ, (15)
where γis a controllable intensity factor.
Additionaly, these principal curvatures are exploited fur-
ther to enrich the point features that propagate through
the network with Gaussian kj=κj
1κj
2, and mean hj=
1
2(κj
1+κj
2)curvatures. In Figure 4, we demonstrate an
example of a point cloud uniformly sampled compared to
our proposed adaptive scheme. In contrast to the standard
uniform sampling, the proposed adaptive sampling captures
sharp features better. In Figure 5, we also present a couple
of examples that showcase the difference between the uni-Figure 4. Adaptive curvature-based vs. uniform sampling, left-to-
right: a uniformly sampled point cloud, original mesh, adaptively
sampled point clouds with intensity factor γ= 1.0The size of all
point clouds is fixed to 10k points in the examples.
Figure 5. Adaptive curvature-based sampling, left-to-right: orig-
inal mesh, calculated mean and gaussian curvatures on the mesh,
adaptively sampled point clouds with intensity factor γ= 1.0and
γ= 2.0, a uniformly sampled point cloud. The size of all point
clouds is fixed to 10k points.
form sampling and our adaptive curvature-based sampling
algorithm with respect to varying intensity factor γ.
4. Experiments
4.1. Setup
An input 3D shape is represented by a point cloud X∈
RN×3, where N= 10kis the number of points in our ex-
periments. The SepicNet is trained on CC3D-PSE dataset
which is randomly split into three non-intersecting folds:
80% for training, 10% for testing and 10% for validation,
which is approximately 35k, 7,5k, 7,5k models. Ground-
truth point clouds are generated by adaptively sampling 10k
points on the 3D scan surfaces, while the ground-truth edges
parametrization is extracted from the corresponding CAD
model. The data is normalized to a unit sphere. The decom-
position model with the embedding size set to 64 is pre-
trained to speed up the convergence for 100 epochs with
Adam optimizer and Cosine Annealing learning rate sched-
ule in the range of (10−3,10−4). After that, SepicNet is ini-
tialized with pre-trained decomposition weights, and it con-
tinues fine-tuning together with fitting for 50 more epochs
in end-to-end manner. The best model is selected based on
the test set overall performance.Metrics The multi-class classification of primitive type
prediction is evaluated via Precision/Recall, and mean
intersection-over-union metric IOU . The matching seg-
ments intersection-over-union sIoU is calculated for pre-
dicted edge segments that have been matched to the ground
truth ones, where we use Hungarian matching to find cor-
respondences between segments. The Chamfer distance be-
tween edges ECD is used to measure the quality of the final
fitting results.
Computation and Implementation Details All the ex-
periment are performed on a single Nvidia RTX 3090 GPU
for all the methods and variations mentioned in this work.
To demonstrate the versatility of the method to different ar-
chitectures, we implemented and trained the SepicNet with
two different backbones: PointNet++ [18], as a well estab-
lished standard backbone for 3D point cloud processing,
and a Point-V oxel CNN [13] as an efficient alternative for
learning on 3D point clouds. The summary of the SepicNet
performance is given in Table 2. The metrics achieved are
similar in both cases, which reflects the generizability of the
method. The average end-to-end inference time for Sepic-
Net is around 3 sec per model. It is worth noticing that the
inference time increases with the number of identified curve
segments. The timings favor the PVCNN backbone specifi-
cally at training stage, so we report the results in the rest of
our experiments for a PVCNN version of SepicNet.
BackboneTraining time
daysInference time
sec per modelPrecision ↑Recall ↑IoU↑ECD ↓
PointNet++ [18] 7 3.2 0.455 0.491 0.585 0.036
PVCNN [13] 5.5 2.9 0.457 0.488 0.586 0.037
Table 2. Performance of SepicNet with different backbones.
Our method has several parameters which values were
selected based on a grid search results on a subset of the
data. One of the most important choices which effects the
convergence of the training significantly is a focal loss mod-
ulating factor η, the best edge detection performance was
reached with η= 1.5. The combination of losses weights
αe= 1, αo= 10 , αt= 2, αemb= 1, β= 1 in Equa-
tion 14 results in a stable training in all our experiments.
The distance threshold to the closest edge τis set to 0.005
portion of the diagonal length of the bounding box of the
model. This value is also well aligned with the maximum
resolution of 128 of the voxel grid in PVCNN backbone.
We ended up to set the adaptive sampling intensity factor
γ= 1.5as it does not produce very dense point clusters
around the edges for models with only a few edges com-
pared to large γvalues, and outlines the edges for a selected
size of point clouds (N=10k) for a major part of the dataset.
The selection of control points of a b-spline is done via a
refinement procedure. In our case, we iteratively upsample
the control point by a factor of 2 until a fitting tolerance
(0.01 in our experiments), measured as a Chamfer distanceDataset Model Precision ↑Recall ↑IoU↑ECD ↓
CC3D-PSEEC-Net [25] 0.333 0.345 0.371 0.172
PIE-Net [23] 0.321 0.316 0.39 0.153
SepicNet (ours) 0.457 0.488 0.586 0.037
ABC⋆ PIE-Net [23] 0.521 0.516 0.590 0.083
SepicNet (ours) 0.539 0.501 0.655 0.017
Table 3. Comparison with state-of-the-art methods.
between the points of a segment and fitted b-spline sampled
points, is achieved.
4.2. Comparison with state-of-the-art
In this section, the results of the SepicNet model are
compared with other state-of-the-art methods, PIE-Net [23]
and EC-NET [25] on our CC3D-PSE and a subset of
ABC [9] dataset.
The CC3D-PSE dataset was converted to PIE-Net and
EC-Net input formats with uniformly sampled points per
point cloud, and both were retrained from scratch with pro-
vided default parameters. The estimated metrics of the
trained models on our CC3D-PSE dataset are provided in
the Table 3. SepicNet significantly outperforms state-of-
the-art on CC3D-PSE data. Since the authors of PIE-
Net [23] did not share the overall data they used from ABC,
we followed the protocol for data preparation they shared
and collected a subset of around 20k models from ABC. We
have retrained PIE-Net and our SepicNet on ABC⋆dataset
and reported the metrics in Table 3. SepicNet demonstrates
superior performance to other methods on both datasets,
and the performance gain is more obvious on 3D scanned
data of CC3D-PSE dataset.
4.3. Ablation Studies
We extend the set of input features with the local fea-
tures extracted from the sampled points, namely, point loca-
tionxj, point normal nj, Gaussian curvature kj, and mean
curvature hj, calculated in this point. The results of the
ablation experiments are summarized in Table 4. Given a
different set of input features and samplings, we re-train the
SepicNet in equal settings otherwise for each case.
As we can see, the uniformly sampled version
of SepicNetxnkh
u loses to other variations of adaptive
SepicNet a. The curvature-weighted adaptive sampling cer-
tainly captures the sharp features better than uniform and
additionally boosts performance of SepicNet. According
to the metrics, we have continuously improved the perfor-
mance extending the set of input features (x,n,k,h), the
points, normals and curvatures. This suggests that, indeed,
curvatures bring additional discriminative information to
SepicNet training. The qualitative results of our best per-
forming SepicNetxnkh
a model on the test part of CC3D-PSE
data are presented in Figure 6. The ECD reaches 0.037in
Figure 6. Results of our SepicNet. From left to right: the original
3D scan, the sampled point cloud, the sharp segments detected,
primitives fitted and ground truth edges. Challenging examples in
the bottom three rows such as dense threads and shallow printed
elements
our experiments.
Model Precision ↑Recall ↑IoU↑sIoU↑ECD ↓
SepicNetxnkh
u 0.343 0.375 0.401 0.250 0.113
SepicNetx
a 0.353 0.380 0.520 0.252 0.103
SepicNetxn
a 0.416 0.438 0.530 0.353 0.082
SepicNetxnkh
a 0.457 0.488 0.586 0.441 0.037
Table 4. SepicNet ablation performance with different sets of
input features (x, n, k, h), where x- point coordinates, n - point
normals, k - Gaussian and h - Mean curvatures of a point, u is short
for uniform sampling, a stands for adaptive sampling.
Noticeable, the adaptive sampling performs as desig-
nated, the high resolution details are well reflected in the
sampled point clouds. Nevertheless, comparing the perfor-
mance of uniform SepicNet uwith methods in Table 3, we
confirm that our approach in case of uniformaly sampled
point cloud preforms better than both EC-Net and PIE-Net.
5. More visual results and discussion
More examples of the output of our SepicNet models at
different steps of the pipeline can be seen in Figure 7. AFigure 7. More examples of SepicNet results.
visual comparison of a recovery of sharp edges from the
parametric curves predicted by our SepicNet model are pre-
sented in Figure 8. The recovered edges are constructed by
projecting nearby triangle vertices on the closest predicted
sharp edge. According to the three main steps in our Sepic-
Net pipeline for parametric inference of sharp edges, the
major sources for the errors in the resulting parametriza-
tions come from:
• the incorrect primitive types classification for edge
points, resulting in the fitting of a wrong primitive as
demonstrated in the bottom right example in Figure 7;
• the incorrect segmentation into curve segments, result-
ing into edges found at the detection stage, being omit-
ted in the final set of parametric edges. We exclude
such edges based on the threshold value of an error
between the fitted primitive and the point set being ap-
proximated. An example of such case with a spline
(blue) segments in third row right example in Figure 7;
• the shallow engravings and the printings on the surface
of the model (as the one in Figure 6 fifth row) are com-
mon reconstruction failures due to a fixed resolution
and a fixed number of points used during sampling;
• some segments are disconnected in the areas around
corner points which were missed in the prediction. We
believe this situation can be improved with a more
dense sampling of points in the corner areas within our
adaptive sampling strategy.
The results of SepicNet are more contained in the chal-
lenging examples which present in CC3D dataset with small
holes and threads such as in three bottom examples in Fig-
ure 6. The degradation of the quality of the parametric in-
ference can be explained by insufficient resolution of the
point clouds, and a huge amount of segments in the model.
(a) Input 3D shape
 (b) SepicNet
Figure 8. Example of a 3D shape recovered sharp edges with our
SepicNet
6. Conclusions
We presented a method to infer the parametric formula-
tion of sharp edges of an object from its 3D shape in or-
der to mitigate the smoothing of sharp edges that is spe-
cific to 3D scanning. We also present a new large-scale
dataset of aligned pairs of CAD models and scanned 3D
models annotated with parametric sharp edges. Our end-to-
end trainable network performance is further improved by a
proposed adaptive sampling of point sets.
The major limitation of our method is the case when the
model fails to predict and mark some parts of the edge seg-
ments as sharp, resulting in topological artifacts. We sug-
gest that an additional module for the closed loops supervi-
sion can help to alleviate topological artifacts. In compari-
son with similar existing datasets, the CC3D-PSE contains
many models with large numbers of edge segments, mak-
ing the task particularly challenging. The proposed dataset
has a broad variation in the complexity of the models, which
makes it appealing for the community in the future research.7. Acknowledgements
The present project is supported by the
National Research Fund, Luxembourg under
the BRIDGES2021/IS/16849599/FREE-3D and
IF/17052459/CASCADES projects, and by Artec 3D.
References
[1] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,
Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,
Manolis Savva, Shuran Song, Hao Su, et al. Shapenet:
An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 , 2015. 2
[2] Kseniya Cherenkova, Djamila Aouada, and Gleb Gusev.
Pvdeconv: Point-voxel deconvolution for autoencoding cad
construction in 3d. In 2020 IEEE International Conference
on Image Processing (ICIP) , pages 2741–2745, 2020. 2
[3] Elona Dupont, Kseniya Cherenkova, Anis Kacem, Sk Aziz
Ali, Ilya Aryhannikov, Gleb Gusev, and Djamila Aouada.
Cadops-net: Jointly learning cad operation types and steps
from boundary-representations. In 2022 International Con-
ference on 3D Vision (3DV) , 2022. 1
[4] Mostafa Ebrahim. 3d laser scanners: History, applications,
and future. 10 2014. 1
[5] Haoqiang Fan, Hao Su, and Leonidas Guibas. A point set
generation network for 3d object reconstruction from a single
image. In CVPR , pages 2463–2471, 07 2017. 5
[6] Abid Haleem, Mohd Javaid, Ravi Pratap Singh, Shanay Rab,
Rajiv Suman, Lalit Kumar, and Ibrahim Haleem Khan. Ex-
ploring the potential of 3d scanning in industry 4.0: An
overview. International Journal of Cognitive Computing in
Engineering , 3:161–171, 2022. 1
[7] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchis-
escu. Training deep networks with structured layers by ma-
trix backpropagation, 2016. 5
[8] Ahmet Serdar Karadeniz, Sk Aziz Ali, Anis Kacem, Elona
Dupont, and Djamila Aouada. Tscom-net: Coarse-to-fine 3d
textured shape completion network. In Computer Vision–
ECCV 2022 Workshops: Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part V , pages 289–306. Springer, 2023.
1
[9] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis
Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa,
Denis Zorin, and Daniele Panozzo. ABC: A big CAD model
dataset for geometric deep learning. CoRR , abs/1812.06216,
2018. 2, 7
[10] Lingxiao Li, Minhyuk Sung, Anastasia Dubrovina, Li Yi,
and Leonidas J. Guibas. Supervised fitting of geometric
primitives to 3d point clouds. CoRR , abs/1811.08988, 2018.
5
[11] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar. Focal
loss for dense object detection. In 2017 IEEE International
Conference on Computer Vision (ICCV) , pages 2999–3007,
Los Alamitos, CA, USA, oct 2017. IEEE Computer Society.
3[12] Yujia Liu, Stefano D’Aronco, Konrad Schindler, and
Jan Dirk Wegner. PC2WF: 3d wireframe reconstruction from
raw point clouds. CoRR , abs/2103.02766, 2021. 2
[13] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-
voxel cnn for efficient 3d deep learning. In Advances in Neu-
ral Information Processing Systems , 2019. 6
[14] Albert Matveev, Alexey Artemov, Denis Zorin, and Evgeny
Burnaev. 3d parametric wireframe extraction based on dis-
tance fields. CoRR , abs/2107.06165, 2021. 2
[15] Albert Matveev, Ruslan Rakhimov, Alexey Artemov, Gleb
Bobrovskikh, Emil Bogomolov, Daniele Panozzo, Denis
Zorin, and Evgeny Burnaev. Def: Deep estimation of
sharp geometric features in 3d shapes. arXiv preprint
arXiv:2011.15081 , 2020. 2
[16] Iain Murray. Differentiation of the cholesky decomposition,
2016. 5
[17] Les Piegl and Wayne Tiller. The NURBS Book (2nd Ed.) .
Springer-Verlag, Berlin, Heidelberg, 1997. 5
[18] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J.
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. CoRR , abs/1706.02413, 2017. 6
[19] Sebastian Raschka, Joshua Patterson, and Corey Nolet. Ma-
chine learning in python: Main developments and technol-
ogy trends in data science, machine learning, and artificial
intelligence. arXiv preprint arXiv:2002.04803 , 2020. 4
[20] Alexandre Saint, Anis Kacem, Kseniya Cherenkova, and
Djamila Aouada. 3dbooster: 3d body shape and texture re-
covery. In Computer Vision–ECCV 2020 Workshops: Glas-
gow, UK, August 23–28, 2020, Proceedings, Part II 16 , pages
726–740. Springer, 2020. 1
[21] Alexandre Saint, Anis Kacem, Kseniya Cherenkova, Kon-
stantinos Papadopoulos, Julian Chibane, Gerard Pons-Moll,
Gleb Gusev, David Fofi, Djamila Aouada, and Bj ¨orn Otter-
sten. Sharp 2020: The 1st shape recovery from partial tex-
tured 3d scans challenge results. In Computer Vision–ECCV
2020 Workshops: Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part II 16 , pages 741–755. Springer, 2020. 1
[22] Gopal Sharma, Difan Liu, E. Kalogerakis, Subhransu Maji,
S. Chaudhuri, and Radomir Mvech. Parsenet: A parametric
surface fitting network for 3d point clouds. In ECCV , 2020.
4
[23] Xiaogang Wang, Yuelang Xu, Kai Xu, Andrea Tagliasac-
chi, Bin Zhou, Ali Mahdavi-Amiri, and Hao Zhang. Pie-net:
Parametric inference of point cloud edges, 2020. 2, 7
[24] Francis Williams, Teseo Schneider, Cl ´audio T. Silva, Denis
Zorin, Joan Bruna, and Daniele Panozzo. Deep geomet-
ric prior for surface reconstruction. CoRR , abs/1811.10943,
2018. 2
[25] Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and
Pheng-Ann Heng. Ec-net: an edge-aware point set consoli-
dation network. CoRR , abs/1807.06010, 2018. 2, 7
[26] Cem Yuksel. Sample elimination for generating poisson disk
sample sets. Computer Graphics Forum (Proceedings of EU-
ROGRAPHICS 2015) , 34(2):25–32, 2015. 5