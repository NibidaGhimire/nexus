Learning Dynamics on Invariant Measures using PDE-Constrained Optimization
Learning Dynamics on Invariant Measures
Using PDE-Constrained Optimization
Jonah Botvinick-Greenhouse,1,a)Robert Martin,2and Yunan Yang3
1)Center for Applied Mathematics, Cornell University, Ithaca, NY 14850
2)DEVCOM Army Research Laboratory, Research Triangle Park, Durham, NC 27709
3)Institute for Theoretical Studies, ETH Zürich, Zürich, Switzerland 8092
(Dated: 6 July 2023)
We extend the methodology in [Yang et al., 2023] to learn autonomous continuous-time dynamical systems from
invariant measures. The highlight of our approach is to reformulate the inverse problem of learning ODEs or SDEs
from data as a PDE-constrained optimization problem. This shift in perspective allows us to learn from slowly sampled
inference trajectories and perform uncertainty quantification for the forecasted dynamics. Our approach also yields a
forward model with better stability than direct trajectory simulation in certain situations. We present numerical results
for the Van der Pol oscillator and the Lorenz-63 system, together with real-world applications to Hall-effect thruster
dynamics and temperature prediction, to demonstrate the effectiveness of the proposed approach.
Data-driven models have proven to be instrumental across
numerous scientific disciplines for their ability to predict
and control the behavior of complex physical systems.1
Popular approaches for modeling dynamical trajectories
typically adopt a Lagrangian perspective and seek a point-
wise matching with either the observed data or its approxi-
mate state derivatives. When the observed data has a poor
temporal resolution and the state derivatives are difficult
to approximate, these approaches may struggle. Such dif-
ficulties are further exaggerated when measurements are
contaminated with noise, and the system in question ex-
hibits sensitive dependence on initial conditions. In this
paper, we propose an alternative approach that can cir-
cumvent some of these challenges by treating global statis-
tics of the observed dynamics as the inference data.
I. INTRODUCTION
Differential equations are typically used to model trajectory
data originating from physical systems. Common techniques
for fitting differential equations to trajectory data include
the shooting methods,6,7neural differential equations,5,8,9and
SINDy.2,10Methods based on the Kalman filter are effective
in data assimilation and in estimating unknown states and pa-
rameters of a system as it evolves in time.11–16When used
to identify model parameters, these approaches fall under the
broad category of system identification.
These approaches all adopt a Lagrangian perspective and
directly fit the modeled trajectories or their state derivatives
to the observed measurements. While these techniques have
seen great success in modeling complex dynamics, their ap-
plication is generally limited to inference trajectories sampled
at a relatively high frequency. When the inference trajectory
is sampled slowly, or in the worst-case scenario when mea-
surement times are unknown, these approaches may not be
a)Corresponding author: jrb482@cornell.eduapplicable. For example, see Figure 1 in which we investi-
gate the use of SINDy2and a Neural ODE5for modeling the
dynamics of a slowly sampled limit cycle.
There are at least three sources of instabilities when directly
using the trajectory data to perform velocity reconstruction.
First, for certain chaotic dynamical systems, a small pertur-
bation in the initial condition can lead to a large deviation in
the trajectory at a later time, which cannot be differentiated
from inaccurate dynamics by looking at the data alone. Sec-
ond, the estimation of the particle velocity suffers from slowly
sampled trajectory data, which directly affects the reconstruc-
tion of the dynamics, as shown in Figure 1. Third, the mea-
surement (extrinsic) noise and the model intrinsic noise both
change the state location. The small noise pollution can be
amplified more in the velocity estimation using the divided
difference with a small time step. All three factors share the
nature that a small perturbation in the trajectory data leads to
a large deviation in the estimated velocity/learned dynamics.
In contrast with the Lagrangian approach to modeling dy-
namics, our method builds on an Eulerian perspective17,18
in which velocity models are constructed to yield the same
asymptotic statistics as the observed measurements. This ap-
proach converts what is traditionally regarded as an ordinary
differential equation (ODE) or stochastic differential equation
(SDE) modeling problem into a partial differential equation
(PDE)-constrained optimization problem. The motivation of
our method is that, in certain situations, the PDE forward
model yields better stability in solving the inverse problem
than direct trajectory forward simulation based upon an ODE
or SDE. Importantly, our method does not rely on prior knowl-
edge of sampling times and can thus be used to learn the dy-
namics from slowly sampled trajectories.
There are two important differences between the line of
work using Kalman filters and our proposed method. First,
a Kalman filter is a particular case of the Bayes filter using the
Bayes theorem, while our reconstruction follows a determinis-
tic inverse problem (PDE-constrained optimization). Second,
time is a crucial element in designing a Kalman filter, while
in our approach, we use the invariant measure and a time-
independent PDE surrogate model. Once the flow has been in-
ferred, we can also perform uncertainty quantification for thearXiv:2301.05193v3  [math.DS]  4 Jul 2023Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 2
FIG. 1. Comparison with the SINDy2–4and the Neural ODE5frameworks for learning slowly sampled dynamics. The left panel shows the
original dynamics (in grey) and the first eight points of a slowly sampled trajectory (in red). While the SINDy and Neural ODE frameworks
can learn the quickly sampled dynamics (model output in grey), both struggle to learn from the slowly sampled trajectory (model output in
red). On the other hand, our framework can learn from both quickly and slowly sampled dynamics (the red and grey model outputs coincide).
Additional experiment details are provided in Section V A.
forecasted dynamics, building towards extending grid-based
Bayesian estimation of nonlinear low-dimensional systems19
to slowly sampled unknown systems with nontrivial invariant
measures.
More specifically, instead of directly treating the noisy ob-
servations {˜x(ti)}n
i=1from one single trajectory of an au-
tonomous flow ˙x=v∗(x)as inference data, we consider the
occupation measure ρ∗generated by a single trajectory, where
for each measurable set B,
ρ∗(B):=1
nn
∑
i=1χB(˜x(ti)),χB(x) =(
1,x∈B,
0,x̸∈B.(1)
When the occupation measures generated by a nontrivial
(see Section II A) set of initial conditions all weakly con-
verge to the same invariant measure, the limiting measure
is said to be physical.20In this work, we consider the class
of autonomous systems for which the occupation measure of
Lebesgue-almost all initial conditions converges to a unique
physical measure. Notably, this encompasses chaotic attrac-
tors such as the Lorenz-63 system.21,22This assumption guar-
antees the uniqueness of the invariant measure for the dynam-
ical system under study. If we relax it and allow the existence
of multiple invariant measures, further treatment of the PDE
forward model is needed. For instance, the fact that differ-
ent invariant measures are mutually singular as well as infor-
mation on the initial condition, among other considerations,
is necessary to guarantee that the steady-state solution picked
up by the PDE model matches the observed invariant measure.
We remark that the definition of a physical measure demon-
strates its robustness to perturbations with respect to initial
conditions.
Going forward, we write v=v(θ) =v(x;θ)to denote the
dependence of the reconstructed velocity fields on a set of pa-
rameters θ∈Θwhere Θ⊂Rmis the admissible set of all
parameter values. The concrete form of θdepends on the hy-
pothesis space of v, which will be discussed in Section IV B.
The task is now to find the best-parameterized model v(x;θ)approximating the true velocity v∗through the optimization
inf
θ∈ΘJ(θ), J(θ):=D(ρε(v(θ)),ρ∗). (2)
The formulation (2) represents an inverse data-matching prob-
lem, in which Ddenotes a metric or divergence on the space
of probability measures and ρε(v(θ))is a regularized approxi-
mation to the physical measure of the dynamical system, given
some regularization parameter ε>0 and the current velocity
v(θ). That is, v(θ)7→ρε(v(θ))is our new forward model.
Although one could approximate ρ(v(θ))by numerically
integrating a trajectory and binning the observed states to a
histogram,17this approach does not permit simple differen-
tiation of the resulting measure with respect to the parame-
tersθ. When the size of θ, i.e., m, is large, it is practical to
use gradient-based optimization methods for solving the op-
timization problem (2), and one has to compute the essen-
tial gradient ∂θJ. In Ref. 18, this was handled by view-
ingρε(v(θ))as the dominant eigenvector of a regularized
Markov matrix originating from an upwind finite volume dis-
cretization of the continuity equation. The derivative ∂θJwas
then seamlessly computed via the adjoint-state method.18The
computation time of the adjoint-state method is independent
of the size of θ, making the framework presented in Ref. 18
well-suited for large-scale computational inverse problems.
In this work, we build upon the framework proposed in
Ref. 18 and study invariant measure-based velocity learning
with a large-scale parameter space applied to real data. There
are three essential new contributions:
1. We consider the Fokker–Planck equation as the par-
tial differential equation (PDE) forward model for
ρε(v(θ)), rather than the continuity equation. This
is motivated by the Fokker–Planck equation’s greater
modeling capacity. Indeed, the Fokker–Planck equation
reduces to the continuity equation when its diffusion
term is zero, and it can fit intrinsic noise present in tra-
jectories which reduces over-fitting the parameterized
velocity v(θ). Moreover, the Fokker–Planck equation
can be seen as an alternative to the teleportation reg-
ularization used for the continuity equation in Ref. 18Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 3
in order to guarantee the uniqueness of the computed
stationary solution ρε(v(θ)).
2. In contrast to only learning three coefficients as done in
Ref. 18, we parameterize the velocity v(θ)using piece-
wise polynomial, global polynomial, and neural net-
work discretizations, which can all yield large param-
eter spaces with thousands of dimensions. We compare
the reconstructed velocity in each case and further dis-
cuss how the choice of parameterization affects the in-
verse problem’s well-posedness and the reconstructed
velocity’s regularity. We also consider various met-
rics/divergences as the choice of the objective function.
3. We investigate velocity learning in time-delay coordi-
nates, which can characterize the full dynamics from
partial state measurements alone.23After performing
the optimization (2), we evolve the learned Fokker–
Planck equation forward in time to quantify the uncer-
tainty in predictions of future dynamics. Based on this
framework, we demonstrate that forecasts incur larger
uncertainties when the embedding dimension is not suf-
ficiently high. It is worth noting that there is no analytic
form for the velocity in time-delay coordinates, even
for well-studied dynamical systems. We also stress that
our proposed approach permits larger-scale modeling of
time-delayed dynamics than the approach considered in
Ref. 17, due to the use of the adjoint-state method when
solving the PDE-constrained optimization.
The rest of the paper is organized as follows. In Section II,
we review essential background on dynamical systems, in-
variant measures, the Fokker–Planck equation, and time-delay
coordinates. In Section III, we introduce the forward surro-
gate model ρε(v(θ))and analyze its modeling errors. In Sec-
tion IV, we present an efficient gradient calculation for the
objective function J(θ)by treating (2) as a PDE-constrained
optimization problem and utilizing the adjoint-state method.
We then adapt the gradient calculation to various velocity pa-
rameterizations, including neural network discretizations in
which the gradient is computed along with the backpropaga-
tion technique.24
Finally, in Section V, we present velocity reconstructions
for the Van der Pol oscillator and the Lorenz-63 system. We
also model dynamics in time-delay coordinates based on real-
world data from a Hall-effect thruster and actual temperature
recordings. We perform uncertainty quantification on the last
two real-data examples. Conclusions follow in Section VI.
II. BACKGROUND
This section reviews the essential background on invariant
measures, stochastic dynamics, the Fokker–Planck equation,
and time-delay coordinates. We also review the Eulerian ap-
proach for parameter identification proposed in Refs. 17 and
18, as well as past work on the discrete inverse Frobenius–
Perron problem.25A. Physical Measures
Physical measures characterize the long-term statistical be-
havior of a significant collection of dynamical trajectories.
When a dynamical system is chaotic and exhibits sensitive
dependence on initial conditions, the existence of a physical
measure unifies the statistical properties of trajectories that are
pointwise dissimilar. While ergodic measures also describe
the long-term statistical behavior of dynamical trajectories,
they may have very small support or even be singular. On the
other hand, when a dynamical system admits a physical mea-
sure, it holds that the trajectories corresponding to a positive
Lebesgue measure subset of initial conditions will all share
the same statistical behavior. We will now formalize these
ideas in the language of ergodic theory. For a more thorough
treatment of the topic, we refer to Refs. 20, 26–28.
While we will review the theory of physical measures in the
context of discrete-time dynamical systems, our applications
will consider dynamics given by a time- ∆tflow map for some
∆t>0. Following Ref. 20, we assume that Mis a compact
Riemannian manifold and that T:M→Mis a diffeomor-
phism. A probability measure µis said to be invariant with
respect to the map Tifµ(T−1(B)) = µ(B)for all B∈B,
whereBdenotes the Borel σ-algebra (see Ref. 29, Definition
2.1). Hereafter, we will assume that µis an invariant measure.
A point x∈Mis said to be generic (see Ref. 20, Section 2.2)
if for all g∈C(M), it holds that
lim
N→∞1
NN−1
∑
k=0g(Tk(x)) =Z
Mgdµ. (3)
The left-hand side of (3) is known as the time-average of
a function g∈C(M)whereas the right-hand side of (3) is
known as the space average. It follows Birkhoff’s pointwise
ergodic theorem (see Ref. 29, Theorem 2.30) that the time-
average of any g∈C(M)necessarily exists on a set of full
µ-measure. To formally discuss the statistical properties of
dynamical trajectories, we now define the N-step occupation
measure given the initial condition x∈Mas
µx,N(B):=1
NN−1
∑
k=0χB(Tk(x)), ∀B∈B. (4)
The condition that a point x∈Mis generic is equivalent to
the condition
lim
N→∞µx,N=µ, (5)
where convergence takes place in the weak-* topology (see
Ref. 29, Definition 4.19). Since the quantity µx,N(B)ap-
proximates the average amount of time for which the or-
bit{Tk(x)}∞
k=0initiated at x∈Mresides in a measurable
setB∈B, this convergence indicates that the collection of
generic points all share the same asymptotic statistical behav-
ior. When the measure µis ergodic (see Ref. 29, Definition
4.19), it holds that µ-almost every x∈Mis a generic point
(see Ref. 29, Corollary 4.20). However, if µis an ergodicLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 4
measure that is singular with respect to the Lebesgue mea-
sure, the resulting collection of generic points may be phys-
ically insignificant and difficult to observe computationally.
Motivated by this perspective, an invariant measure µis said
to be physical if there exists a collection of generic points with
positive Lebesgue measure (see Ref. 20, Definition 2.3).
We will next discuss the ways in which a physical invariant
measure µcan be computationally approximated. If one col-
lects the measurements {Tk(x)}N
k=1, the weak-* convergence
in (5) suggests that the physical measure µwill describe the
statistics of our measurements provided that Nis sufficiently
large. Motivated by this perspective, we can discretize the
domain Mand directly compute the occupation measure (4)
for each cell in the discretization to approximate the physical
measure. This procedure has been previously used to approx-
imate physical measures.17,18,30Other approaches have been
proposed to compute the invariant measure as the stationary
vector of the finite-dimensional approximation of the continu-
ous Frobenius–Perron operator,31including Ulam’s method,27
and Galerkin-type methods.32,33More precisely, these dis-
cretizations are used to construct a Markov matrix that rep-
resents a random dynamical system approximating the deter-
ministic map T:M→M. An invariant measure for the dis-
crete approximation is then recovered as a stationary vector
of the resulting Markov matrix. As the discretization is re-
fined, certain assumptions guarantee that the desired physical
measure will be recovered in the weak-* limit (see Ref. 32,
Theorem 4.14).
B. Stochastic Dynamics and the Fokker–Planck Equation
Consider an Itô stochastic differential equation (SDE) of
the form
dXt=v(Xt)dt+σ(Xt)dWt, X0=x. (6)
Above, Wtis a Brownian motion, vis the velocity, and σde-
termines the diffusion matrix Σ(x) =1
2σ(x)σ(x)⊤.For sim-
plicity, we will consider the case of a constant diffusion. Sim-
ilar to the deterministic setting, there are analogous notions
of invariant measures, ergodicity, and physical measures in
the stochastic setting.34,35One may use the Euler–Maruyama
method to obtain the numerical solution to (6) on the time in-
terval [0,T], which assigns
Xj+1=Xj+v(Xj)∆t+σ(Xj)ξj√
∆t,
where {ξj}are independently and identically distributed
(i.i.d.) from N(0,I), the standard normal distribution on Rd,
∆t:=T/N,andj∈ {0,..., N−1}.
The Fokker–Planck equation provides a PDE description of
the probability density ρ(x,t)of the random variable Xt. The
density evolves as (see Ref. 36, Page 88)
∂ρ(x,t)
∂t=−∇·(ρ(x,t)v(x))+∇·
∇·(Σ(x)ρ(x,t))
.(7)
By assuming a constant diffusion, we may write Σ(x) =DI,
where Idenotes the identity and D>0 is a constant repre-
senting the scale of the diffusion. Equation (7) can then besimplified to read
∂ρ(x,t)
∂t=−∇·(ρ(x,t)v(x))+D∇2ρ(x,t). (8)
We leave the study of a non-constant or anisotropic diffusion
for later work. We remark that if D=0, (8) reduces to the
so-called continuity equation, which instead models the prob-
ability flow of the ODE given by ˙x=v(x).Under certain
conditions,37the steady-state solution ρ(x)of (8) exists and
satisfies
∇·(ρ(x)v(x)) = D∇2ρ(x). (9)
Since (9) describes a limiting distribution lim t→∞ρ(x,t),it
has been previously used to provide approximations of invari-
ant measures for stochastically-forced dynamical systems.30
In Ref. 38, an SDE learning problem was studied using (7) as
the modelling equation with different data assumptions.
C. Delay Coordinates and Takens’ Theorem
The technique of time-delay embedding is a popular ap-
proach for reconstructing chaotic dynamical systems from
limited observations.17,39–41The procedure involves embed-
ding time series measurements ψ(t) =ψ(x(t))of the state
x(t)into d-dimensional Euclidean space by considering the
vector of time-lagged observations
Ψd,τ(t) = (ψ(t),ψ(t−τ),...,ψ(t−(d−1)τ)),
for some τ>0. Takens’ theorem23provides suitable assump-
tions under which Ψd,τ(t)andx(t)are related via a diffeo-
morphism, implying that the time-lagged vector of partial ob-
servations Ψd,τ(t)is sufficient for reconstructing the full state
x(t).Notably, the embedding dimension provided in Ref. 23
isd=2m+1 where mis the dimension of a compact mani-
foldMon which the flow map ftfor the original dynamics
is defined. In cases when trajectories are attracted to a com-
pact subset Awith box-counting dimension (see Ref. 42, Page
586) dAstrictly less than m, it turns out that lower-dimensional
embeddings can be obtained.
When a time-series projection ψ(t)of an unknown system
˙x=v(x)is observed, one can try to numerically determine
a suitable embedding dimension dand time delay τ; see for
example, Refs. 43–46. Choosing a proper embedding dimen-
sion and time delay is important for obtaining a reliable sur-
rogate model of the original dynamics in time-delayed coor-
dinates. Notably, in Section V B, we demonstrate that mod-
els for the velocity in time-delayed coordinates can incur ex-
cess uncertainties when the embedding dimension is not suf-
ficiently large.
D. Prior Work on Learning Dynamics from Invariant
Measures
For chaotic systems, trajectories are sensitive to initial con-
ditions and estimation parameters. Sometimes, the approxi-
mate reference velocity field {ˆv(x(ti))}cannot be accuratelyLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 5
estimated from a trajectory {x(ti)}due to the lack of observa-
tional data, slow sampling, discontinuous or inconsistent time
trajectories, and noisy measurements. To tackle such diffi-
culties, instead of working with the Lagrangian trajectories,
Refs. 17 and 18 propose an Eulerian approach by treating the
occupation measure (4) as the data. When enough samples are
available, the occupation measure can be treated as an approx-
imation to the invariant measure; see Section II A. Finding the
optimal parameter θis then translated into the optimization
problem (2). The reference measure ρ∗is the occupation mea-
sure converted from the observed trajectories {ˆx(ti)}; see (4).
In Ref. 17, the approximated synthetic ρε(v(θ))is generated
by first simulating the synthetic trajectories {x(ti;θ)}based
on the dynamical system and then computing its histogram
following (4). Since this approach requires lengthy trajectory
simulation, each evaluation of ρε(v(θ))for a given θis rela-
tively costly. Moreover, it is difficult to compute the derivative
ofρε(v(θ))with respect to θdue to the histogram approxima-
tion of nonlinear trajectories. As an improvement to the origi-
nal idea in Ref. 17, Ref. 18 proposes a surrogate model to ap-
proximate ρε(v(θ))that is differentiable in θand sometimes
faster to compute. The key idea is to solve for ρε(v(θ))as the
distributional steady-state solution to the continuity equation
(i.e., (9) with D=0) using a finite volume upwind scheme
together with the teleportation regularization. The gradient of
the objective function Jin (2) with respect to the parameter θ
can be efficiently computed based on the adjoint-state method
(see Ref. 18, Sec. 5). The problem of learning an SDE from an
invariant measure is also studied in Ref. 47, which uses a deep
learning framework to invert the drift and diffusion terms.
The task of learning a dynamical system from an invari-
ant measure has also been studied in the discrete-time set-
ting under the inverse Frobenius–Perron problem.25,48–50The
Frobenius–Perron operator, also known as the transfer opera-
tor, characterizes the time evolution of an initial measure µ0
according to some prespecified dynamical system. Given a
probability measure µ, the inverse Frobenius–Perron prob-
lem seeks to construct a dynamical system for which µis
a fixed point of the associated transfer operator. The most
widely studied case involves recovering an ergodic map T
on[0,1]for which a prescribed absolutely continuous mea-
sure is the unique fixed point of the discrete transfer opera-
tor. In this particular setting, various approaches such as topo-
logical conjugation51and matrix methods52have been intro-
duced to solve the inverse problem. The multivariate inverse
Frobenius–Perron problem was also studied in Ref. 53, where
ergodic maps were constructed to adhere to the statistics of
two-dimensional densities. Moreover, due to inherent non-
uniqueness in the inverse problem, recent approaches further
restrict the solution space of the discrete ergodic maps to those
with a prescribed power spectrum.54To the best of our knowl-
edge, Refs. 17, 18, and 47, and our contributions here are
the first works that numerically solve the inverse Frobenius–
Perron problem in the continuous-time setting. Notably, we
do not assume that µis absolutely continuous, as we use
a finite-volume discretization to approximate the Frobenius–
Perron operator.III. THE FORWARD MODEL AND MODELING ERRORS
A central contribution of this work is to consider a different
regularized forward model than the one in Ref. 18, especially
for trajectory measurements containing intrinsic noise, which
can be interpreted as sample paths of stochastic dynamical
systems (6). In those cases, the Fokker–Planck equation (7)
is a better candidate as the PDE surrogate model, as it con-
tains a diffusion term that can fit noise present in the data.
Based on the relationship between (6) and (7), one can learn
both the velocity field v(x)and the diffusion tensor Σ(x)in
the optimization framework (2). For simplicity, we only con-
sider a fixed diffusion constant and leave the investigation of
multi-parameter inversion to future work.
We will use (9) as the forward model to fit invariant mea-
sures generated by trajectories with intrinsic noise. While the
diffusion term allows the model to fit the intrinsic noise and
prevent over-fitting the noise into the target velocity compo-
nent, it also controls the scaling of the reconstructed veloc-
ityv(x;θ). Indeed, when D=0 and ˜ v(x) =av(x), we have
∇·(ρ(x)˜v(x)) = 0 as long as ∇·(ρ(x)v(x)) = 0, for any a>0.
However, for most cases, evandvwill not solve the stationary
Fokker-Planck equation (9) for D>0.
A. Finite Volume Discretization
We assume that our system evolves on the d-dimensional
rectangular state space
Ω= [a1,b1]×···× [ad,bd]⊂Rd,
with a spatially dependent velocity v:Ω→Rd. We define ni∈
Z+, 1≤i≤d, to be the number of equally-spaced points along
thei-th spatial dimension at which we wish to approximate the
solution of (8), as well as the mesh spacing
∆xi:=bi−ai
ni−1.
We are thus interested in obtaining a solution to the forward
problem at points of the form
xk1,...,kd:= (a1+k1∆x1,..., ad+kd∆xd),
where ki∈ {1,..., ni}. We will index our coordinates using
column-major order and write xk1,...,kd=xjwhere
j=k1+d
∑
i=2(ki−1)Si,Si:=i−1
∏
j=1nj. (10)
We will regard xjas the center of the cell Cjwhere
Cj=d
∏
i=1
ai+
ki−1
2
∆xi,ai+
ki+1
2
∆xi
.
Following the approach in Ref. 19, we implement a first-order
upwind finite volume discretization of the continuity equation,
adding a diffusion term using the central difference schemeLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 6
and enforcing a zero-flux boundary condition.55This allows
us to obtain an explicit time-evolution of the probability vec-
torρ=ρ1ρ2...ρN⊤∈RN, where N=∏d
i=1ni. While ρ
is a discrete probability measure over the cells Cj, it also cor-
responds to a piecewise-constant probability density function
onΩ.
With an abuse of notation, we will refer to both the
piecewise-constant density and the discrete probability mea-sure as ρ. We discretize the time domain with a step size
∆t. Based on (8), the probability vector at the l-th time step
evolves as
ρ(l+1)=ρ(l)+Kρ(l), K=d
∑
i=1∆t
∆xKi,
where each Kiis a tridiagonal matrix of the form
Ki:=...
−vi,−
j−1+D
∆xi
......
vi,−
j−1−wi,+
j−1−2D
∆xi−vi,−
j+D
∆xi
.........
wi,+
j−1+D
∆xivi,−
j−wi,+
j−2D
∆xi−vi,−
j+1+D
∆xi
.........
wi,+
j+D
∆xivi,−
j+1−wi,+
j+1−2D
∆xi
......
wi,+
j+1+D
∆xi
...

Si
∈RN×N.
Above, we have defined for each j∈ {1,..., N}the upwind
velocities
vi,−
j:=min
0,vi
j	
, vi,+
j:=max
0,vi
j	
,
wi,−
j:=min
0,wi
j	
, wi,+
j:=max
0,wi
j	
,
where vi
j:=v 
xj−ei∆xi/2
·eiandwi
j:=v 
xj+ei∆xi/2
·ei
denote the i-th components of the velocity field at the center
of cell faces, and {ei}is the standard basis in Rd. We remark
that if xjis away from ∂Ω, then wi,±
j=vi,±
j+1.To enforce the
zero-flux boundary condition, we set both the velocity vand
diffusion Dto be zero on ∂Ω. As a result, the columns of K
each sum to zero, and the total probability
ρ(l)·1=1, 1:=1...1⊤∈RN,
is conserved under time evolution. Since numerical artifacts
cause the flux accumulation along the boundary, we also en-
force ρ=0 on ∂Ω. When the boundary ∂Ωis sufficiently
far from the trajectory data, this artifact is insignificant. Here-
after, we assume the uniform spatial discretization ∆xi=∆x
for all i=1,..., d.Here, we used an explicit time stepping
scheme. The Courant–Friedrichs–Lewy (CFL) stability con-
dition enforces ∆t=O(∆x2)to ensure the stability of thescheme. To be more specific, we choose
∆t<1
2d∆x2
D+∆x∥v∥∞,
where ∥v∥∞=max i∥v(x)·ei∥∞. In this way, we can en-
force that all entries of I+Kare non-negative with columns
summed to one, which implies that I+Kis a Markov matrix.
For a complete description of the finite volume scheme, we
refer to Ref. 55. We remark that there are many higher-order
structure-preserving schemes to solve (8) which also yield a
Markov matrix; see Ref. 56 for example. A more accurate
numerical scheme can further reduce the forward modeling
error, which is left for future work.
B. Teleportation and Diffusion Regularization
We use the finite volume discretization of the Fokker–
Planck equation in Section III A to approximate its steady-
state solution. After discretization, finding such stationary
distributions to (9) is equivalent to solving the linear system:
(I+K)ρ=ρ.Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 7
(a) The computed steady state solution to (9) for decreasing values of ∆x.
(b) The approximate physical measure obtained by binning a time trajectory based on the SDE (6) for decreasing values of ∆x.
FIG. 2. As the mesh size of the forward model discretization is refined, we visually observe the convergence of the computed steady-state
solution (a) to the approximate physical measure (b). The Van der Pol oscillator (19) with c=1 and D=0.001 is used in this example, and
the histograms indicate mass-per cell.
Since the columns of Ksum to zero, we have that M:=I+K
is a column-stochastic Markov matrix. When D̸=0,Mis
a transition matrix for an ergodic Markov chain, which has a
unique equilibrium. When D=0, to guarantee the uniqueness
of the equilibrium, Ref. 18 applies the so-called teleportation
regularization57and considers
Mε:= (1−ε)M+εU,U=N−111⊤∈RN×N.
There is now a unique solution to the linear system
Mερ=ρ,ρ·1=1,ρ>0. (11)
From a computational aspect, it is useful to take advantage of
the fact that M−Iis sparse where I∈RN×Nis the identity
matrix and to instead solve
(1−ε)(M−I)ρ=−N−1ε1,
where we have simply rearranged terms in (11) and used the
fact that ρ·1=1.
Since Uis also a column stochastic Markov matrix with the
uniform probability of visiting any point of the mesh, using
Mεamounts to stopping the dynamics based on Mat a random
time and restarting it from a uniformly randomly chosen initial
point. The size of εrepresents the restarting frequency–the
smaller ε, the rarer we restart.18
On the other hand, adding the diffusion component Dto the
tridiagonal matrix Kcan be seen as another way of regulariz-
ing the noise-free Markov matrix by adding a scaled Brownian
motion after each discrete evolution of the deterministic dy-
namics. For deterministic dynamics with D=0, the solution
to (9) might not be unique if there is more than one attrac-
tor. The use of teleportation connects all attractors through
the “random restart”, and the solution ρεto the linear sys-
tem (11) has support that connects all the disjoint attractors.Similarly, when D̸=0, the Brownian motion connects all dis-
joint attractors of the deterministic dynamics, giving a unique
steady-state solution. In this scenario, the use of teleportation
for the diffusive case is simply a numerical treatment to im-
prove the conditioning of matrix Mrather than to guarantee
the uniqueness of ρ.
It is worth noting that both the teleportation regulariza-
tion and an incorrect diffusion coefficient could be sources
of modeling error when we perform parameter identifica-
tion. Although these regularizations enable faster evaluation
ofρε(v(θ))and better posedness of the forward problem, they
may reduce the accuracy of the inverse problem solution.
C. Numerical Diffusion
In Figure 2, we illustrate the ρεcomputed as the steady-
state solution to the Fokker–Planck equation in the top row
and the approximation to physical invariant measures of the
corresponding SDE in the bottom row. From Figure 2, we see
that on a coarse mesh, the first-order finite volume scheme in-
curs significant numerical error, which gives a computed solu-
tion with an artificial diffusion effect and thus is often referred
to as the numerical diffusion.19The amount of numerical dif-
fusion is reduced as the mesh is refined since it is incurred
by the first-order scheme. In particular, it is expected to de-
cay as O(max i∆xi)in the L∞norm as we refine the mesh.55
Besides the teleportation and the modeling diffusion D, the
presence of numerical diffusion is another modeling error in-
curred from solving the forward problem.Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 8
IV. GRADIENT CALCULATION & VELOCITY
PARAMETERIZATION
Another main contribution of this paper is to reconstruct the
velocity field v(x)using large-scale parameterizations v(x;θ),
which turns an infinite-dimensional problem of searching for
v(x)in a function space to a finite-dimensional optimization
problem of finding θ∈Θ⊂Rm. Here, we introduce pa-
rameterizations based on piecewise-constant, neural network,
and global polynomial functions. We also investigate various
data-fitting objective functions Jthat compare the mismatch
between the observed and simulated invariant measures, ρ∗
andρε(v(θ)). We compute the gradient of such functions
with respect to the coefficients θin the parameterized velocity
model v(x;θ)based on the adjoint-state method for the PDE-
constrained part and the backpropagation technique24for the
neural network part. Thanks to these techniques, we can then
efficiently evaluate the gradients of Jwith respect to θand
thus conveniently use gradient-based optimization algorithms
to iteratively update θ, e.g., steepest descent, L-BFGS, con-
jugate gradient descent methods as well as stochastic meth-
ods such as Adam.58For notational simplicity, we will write
ρ(v(θ)) =ρε(v(θ))throughout this section.
A. Gradient Calculation Through the Adjoint-State Method
Recall the finite volume scheme in Section III A for solv-
ing (9). The forward model yields a discrete measure
ρ(v(θ)) =ρ(θ) = [ρ1(θ)...ρj(θ)...ρN(θ)]⊤over the cells
{Cj}, which converges to the solution to (9) in the weak sense
as we refine the discretization parameters. For the explicit
form of ρ(v(θ)), we refer to Ref. 18, Eqn. (5.1). Note that we
have highlighted the dependence of our approximate steady-
state distributional solution to the Fokker–Planck equation (9)
on the velocity v(x;θ). Our goal is to solve the optimization
problem (2):
inf
θ∈ΘJ(ρ(v(θ)),ρ∗)
by using gradient-based methods, where Jis the cost func-
tion, and ρ∗represents our inference data. The adjoint-state
method is an efficient technique by which we can evaluate
the derivative ∂θJ, as the computation time is largely inde-
pendent of the size of θ. One can derive the adjoint-state
method for gradient computations by differentiating the dis-
crete constraint,59which in our case is the eigenvector prob-
lem:
g(ρ(θ),θ) =Mε(θ)ρ(θ)−ρ(θ) =0,
where ρ(θ)·1=1.Specifically, we compute ∂θJ=λ⊤∂θg
where λsolves 
∂ρg⊤λ=− 
∂ρJ⊤.In our case, this lin-
ear system is the adjoint equation (see Ref. 18, Eqn. (5.8))
(M⊤
ε−I)λ=− 
∂ρJ⊤+ 
∂ρJ⊤ρ1, (12)
and the derivative
∂θJ=λ⊤ 
∂θMε
ρ. (13)As a result, we only need to compute the derivatives ∂ρJand
∂θMεto determine the gradient ∇θJ= (∂θJ)⊤. The former
depends on the choice of the objective function, while the lat-
ter is based on a specific parameterization of the velocity field
v(x;θ)determined by its hypothesis space.
1. The Computation of ∂ρJ
For the objective function J, we consider the quadratic
Wasserstein distance, the squared L2norm, the Kullback–
Leibler (KL) Divergence, and the Jensen–Shannon (JS) Di-
vergence.
Quadratic Wasserstein Distance: For probability mea-
sures ρandρ∗onΩ, with finite second-order moments, the
squared quadratic Wasserstein distance is defined by
W2
2(ρ,ρ∗):=inf
Tρ,ρ∗∈TZ
Ω|x−Tρ,ρ∗(x)|2dρ(x),
where
T:={T:Ω→Ω:ρ(T−1(B)) =ρ∗(B),B∈B}
is the set of maps that push ρforward into ρ∗.60With an abuse
of notation, we also use ρ(x)andρ∗(x)to denote the densi-
ties of ρandρ∗respectively. For efficient computation of the
W2distance, we utilize the back-and-forth method,61which
instead uses the dual Kantorovich formulation60
W2
2(ρ,ρ∗) =sup
φ1,φ2Z
Ωφ1(x)ρ∗(x)dx+Z
Ωφ2(x)ρ(x)dx
,
where φ1∈L1
ρ∗(Ω)andφ2∈L1
ρ(Ω)are required to satisfy
φ1(x)+φ2(y)≤ |x−y|2. In this case, the Fréchet derivative of
J=W2
2(ρ,ρ∗)with respect to ρis given by
∂J
∂ρ=φ2.
Squared L2Norm: The squared L2distance as the objec-
tive function and its Fréchet derivative are given by
J=1
2Z
Ω|ρ(x)−ρ∗(x)|2dx,
∂J
∂ρ=ρ−ρ∗.
KL-Divergence: The KL-divergence and its Fréchet deriva-
tive are given by
J=DKL(ρ,ρ∗):=Z
Ωρ∗(x)logρ∗(x)
ρ(x)
dx,
∂DKL
∂ρ=−ρ∗(x)
ρ(x).
We remark that our definition of the KL-divergence differs
from many applications in which it is commonly computed as
J=DKL(ρ∗,ρ).Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 9
JS-Divergence: Defining ρ′:= (ρ+ρ∗)/2, the JS-
divergence and its Fréchet derivative are given by
J=DJS(ρ,ρ∗) =1
2DKL(ρ,ρ′)+1
2DKL(ρ∗,ρ′),
∂DJS
∂ρ=1
2log2ρ
ρ+ρ∗
.
Based on definitions of the KL and JS divergence, it is clear
that we may encounter numerical instability issues if either
ρorρ∗is not supported on the entire domain Ω. Thus, we
remark that for the computation of both the KL and JS diver-
gences, we restrict the domain Ωto regions where both ρand
ρ∗are strictly positive. This is equivalent to the definition of
the KL and JS divergence based upon the so-called Csiszar
divergence (see Ref. 62, Eqn. (1)).
2. The Computation of ∂θJ
We have presented a few cases of ∂ρJfor different choices
ofJ. Next, we show how to obtain ∂θMε, which is the othernecessary component in the adjoint-state method for gradient
calculation; see (12)-(13). To begin with, we consider θ=
{vi
j}for all i=1,..., dandj=1,..., N, which corresponds to
one variant of piecewise-constant velocity parameterization.
Since we are only interested in computing the gradient
away from ∂Ω, we can utilize the property that wi,±
j=vi,±
j+1.
First, observe that
∂Mε
∂vi
j= (1−ε)d
∑
ℓ=1∆t
∆x∂Kℓ
∂vi
j= (1−ε)∆t
∆x∂Ki
∂vi
j,
as well as
∂Ki
∂vi
j=...
0
......
−H(vi
j)−(1−H(vi
j))
.........
H(vi
j) (1−H(vi
j)) 0
.........
0 0
......
0
...

Si
H(x):=(1,x>0
0,x≤0. (14)
In (14), H(·)is the Heaviside function. We remark that ∂vi
jKican only be nonzero in the (j,j),(j,j−Si),(j−Si,j), and
(j−Si,j−Si)-th entries where Siis defined in (10). After solving (12) for λand applying (13), we deduce that
∂J
∂vi
j=λ·∂Mε
∂vi
jρ= (1−ε)∆t
∆x 
λ·∂Ki
∂vi
jρ!
= (1−ε)∆t
∆x 
H(vi
j)ρj−Siλj+(1−H(vi
j))ρjλj−H(vi
j)ρj−Siλj−Si−(1−H(vi
j))ρjλj−Si
= (1−ε)∆t
∆x 
λj−λj−Si 
H(vi
j)ρj−Si+(1−H(vi
j))ρj
. (15)
Equation (15) provides an efficient way for computing
the gradient of the objective function with respect to the
piecewise-constant velocity based on cells {Cj}from our
finite-volume discretization.Alternatively, if the velocity v=v(x;θ)is smoothly param-
eterized by the vector θ= [θ1,...,θk,...,θm]⊤∈Rm, for eachLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 10
θk, we can then evaluate
∂J
∂θk=N
∑
j=1d
∑
i=1∂J
∂vi
j∂vi
j
∂θk(16)
∂vi
j
∂θk=ei·∂v
∂θk
(xj−ei∆xi/2;θ),
to determine the derivative ∂θJ.By using a similar indexing
convention to Section III A, we can collect the terms ∂vi
jJand
∂θkvi
jinto the vectors ∂vJand∂θkv, respectively. Therefore,
the double summation in (16) is achieved by the inner-product
∂vJ·∂θkv. Note that for different θk, we only need to change
∂θkvas∂vJdoes not depend on θk.
B. Velocity parameterization
We now apply Equations (15) and (16) to evaluate the gra-
dients of several parameterized velocity models. Specifically,
we consider piecewise constant, global polynomial, and neu-
ral network parameterizations of the velocity.
1. Piecewise-Constant parameterization
In the case of the piecewise-constant parameterization, we
model the velocity as
v(x;θ) =d
∑
i=1N
∑
i=jvi
jχCj(x)ei, θ={vi
j}. (17)
Here, we again use the column-major ordering from Sec-
tion III A to accumulate vectors of cells Cjwith centers xj,
and velocity components
vi
j=v(xj−ei∆xi/2)·ei
along the i-th direction of the cell face located at xj−ei∆x/2.
The parameter space of the model presented in (17) is given by
{vi
j}, which has size N·d, and the gradient of the parameters
{vi
j}can be directly evaluated by (15).
We remark that (17) is only one variant of piecewise-
constant parameterization since the parameterization mesh
is the same as the discretization mesh in the finite-volume
method; see Section III A. These two meshes do not have to
be coupled together. To reduce the numerical error from the
first-order scheme, it is preferable to reduce the spacing {∆xi},
but we can keep the parameterization mesh fixed so the size
of the optimization problem does not change. In this case, we
need to apply the chain rule (16) to obtain the final gradient
after evaluating (15).
The model defined by (17) can be learned by gradient-
based optimization methods. The regularity of the piecewise-
constant model defined by (17) can be improved to a C0func-
tion by interpolating between the values vi
jusing either piece-
wise linear or higher-order piecewise polynomial functions,
as in Ref. 63.2. Global Polynomial parameterization
Though the regularity of the piecewise-constant model
given by (17) can be improved by interpolation, the inverted
velocity v(x;θ)may still be highly oscillatory if the mesh size
∆xis small. Modeling approaches, such as SINDy,2learn the
velocity fields of dynamical systems from a polynomial basis
together with sparse regression. Here, we show how the gradi-
ent derivation in (16) can be adapted to such polynomial basis
parameterization of the velocity field
v(x;θ) = [ v1(x;θ),..., vd(x;θ)]⊤=d
∑
i=1vi(x;θ)ei.
The i-th component of the velocity field vi(x;θ)parameter-
ized by a linear combination of the monomial basis of degree
at most Kcan be written as
vi(x;θ) =M
∑
ℓ=1ai
ℓ(x⊤e1)1ki
ℓ...(x⊤ed)dki
ℓ (18)
M=d+K
K
,
where the powers are represented by multi-indices
ki
ℓ= (1ki
ℓ,...,dki
ℓ),
with 1 ≤ℓ≤M,|ki
ℓ| ≤K, and θ={ai
ℓ}. The size of θin this
case is d·M. To learn the model parameterized by (18), we
can use (16) to compute the gradient ∂J/∂ai
ℓ. Without loss
of generality, we assume ∆xi=∆x, for all 1 ≤i≤d. The only
term in (16) which explicitly depends on the velocity parame-
terization is
∂vi
j
∂ai
ℓ=
(xj−ei∆x/2)⊤e11ki
ℓ...
(xj−ei∆x/2)⊤eddki
ℓ,
where i,j,ai
ℓand the multi-index ki
ℓare fixed. Note that
∂ai
ℓvi′
j=0 ifi′̸=i. Thus, we can again use gradient-based
methods to infer proper polynomial coefficients {ai
ℓ}.
Although a global polynomial parameterization guarantees
ideal C∞regularity of the parameterized velocity v(x;θ), the
Runge phenomenon could be a potential downside of this ap-
proach. Specifically, as we increase the maximum degree K
of the polynomial basis, we may encounter substantial inter-
polation errors near the boundary ∂Ω.
3. Neural Network parameterization
Motivated by the universal approximation theory of neural
networks,64we may also choose to model each component of
the velocity vi(x;θ)as a feed-forward neural network, where
the tunable parameters θmake up the network’s weights and
biases. We follow Ref. 65 to combine the adjoint-state method
for the PDE constraints and the backpropagation technique to
update the weights and biases of the neural network.Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 11
The term ∂vi
jJin the gradient calculation (16) can be com-
puted by first evaluating the neural network on the mesh of
cell face centers oriented in the direction of eito obtain {vi
j},
which is then plugged into (15) to obtain ∂vi
jJ. The remain-
ing term ∂θvin (16) is then computed via the backpropagation
technique.24
For simplicity, we restrict ourselves to single-layer feed-
forward networks. Moreover, by using a smooth activation
function, such as the hyperbolic tangent or the sigmoid func-
tion, we can guarantee C∞regularity of the reconstructed ve-
locity v(x;θ)on the domain Ω. To enforce the zero-flux
boundary condition, we manually set v=0 on ∂Ω. Conse-
quently, the neural network parameterization may lack regu-
larity near ∂Ω. However, if the domain is sufficiently large,
the support of the physical measure will be very far from ∂Ω,
in which case we will not observe any discontinuities origi-
nating from the boundary condition while simulating the tra-
jectories based on (6). As we increase the number of nodes in
the hidden layer of the neural network, both the approxima-
tion power and the potential difficulty of training the neural
network are expected to increase.
V. NUMERICAL RESULTS
In this section, we present several numerical examples to
demonstrate the utility of the proposed approach for learn-
ing dynamical systems from invariant measures with intrinsic
noise.1In Section V A, we study the inverse problem for the
Van der Pol oscillator with a neural network parameterization
of the velocity. In Section V B, we time-delay embed a signal
sampled from a Hall-effect thruster and proceed to model the
dynamics in delay-coordinates based upon the time-delayed
invariant measure. We then illustrate that a low-dimensional
embedding may increase the uncertainty of the learned model
and that the choice of parameterization largely affects the reg-
ularity of the reconstructed velocity. In Section V C, we study
rolling averages of a temperature data set and perform uncer-
tainty quantification using the learned Fokker–Planck PDE in
time-delayed coordinates. We conclude in Section V D by in-
verting a component of the Lorenz-63 system’s velocity using
a neural network parameterization. All experiments are con-
ducted using an Intel i7-1165G7 CPU.
A. Van der Pol Oscillator
We begin by considering the autonomous Van der Pol
oscillator,66given by
(
˙x=y,
˙y=c(1−x2)y−x.(19)
1We include publicly available code ( link ) which contains an example
demonstrating the velocity inversion for the Van der Pol oscillator (19)
based on a global polynomial parameterization. It can also be used to re-
produce the comparison in Figure 1 and Table II.Our results for learning a dynamical system with prescribed
statistical properties given by the stochastically-forced Van
der Pol oscillator are shown in Figure 3. In the top row,
the first panel features the velocity of (19) for the choice of
c=0.5,the second panel shows the approximate occupation
measure (see (4)) obtained from the simulation of a single
SDE trajectory (see (6)), the third panel shows the SDE trajec-
tory used to approximate the invariant measure, and the fourth
panel shows the dynamics of the oscillator without stochastic
forcing. Throughout, we color the SDE trajectories by their
histogrammed density to illustrate the connection between the
Lagrangian and Eulerian perspectives. We also stress that the
experiment in Figure 3 assumes the diffusion coefficient to be
known a priori, but that Section V B relaxes this assumption.
In the following rows of Figure 3, we use neural network,
piecewise constant, and global polynomial parameterizations
of the velocity to solve the inverse problem using the opti-
mization framework from Sections III A and IV. For the case
of the neural network parameterization, we compare each ob-
jective function studied in Section IV A 1, while we only focus
on the L2objective for the remaining two parameterizations.
Across all tests, the reconstructed velocity is shown to vary
significantly from the true velocity shown in the first row of
Figure 3. This is mainly due to the lack of data away from the
main attracting limit cycle. In regions of the state space with
no available data, we can only expect that the modeled veloc-
ityv(x;θ)will direct trajectories towards the attracting limit
cycle on which the invariant measure is supported. Indeed,
this is what we observe.
Parameterization Objective Wall-Clock Time (s) Error
Piecewise constant L23.13·1012.36·10−1
Global polynomial L21.75·1022.90·10−2
Neural network L24.14·1029.07·10−3
Neural network KL 2.04·1027.11·10−3
Neural network JS 2.16·1029.48·10−3
Neural network W2 2.16·1031.07·10−2
TABLE I. Comparison of the wall-clock computation time and the
error for the experiments shown in Figure 3. The error is quantified
by the squared W2distance between the occupation measure of the
ground truth diffuse trajectory (see the third panel of Figure 3a), and
the occupation measure accumulated from the simulation of a trajec-
tory with diffusion according to the learned velocity (see the fourth
panel of Figure 3d–3c).
Moreover, while the learned PDE model (9) matches the
observed occupation measure (4) across all tests, we find that
the SDE and ODE trajectories generated using the learned ve-
locity vector fields may vary depending on the parameteriza-
tion. Table I provides a comparison of the accuracy of the
learned models, as well as the required computation times.
While the piecewise constant velocity is by construction dis-
continuous and thus does not naturally guarantee the existence
and uniqueness of the corresponding ODE solution, the neu-
ral network parameterization based on the hyperbolic tangentLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 12
(a) Ground truth velocity, occupation measure, diffuse trajectory, and non-diffuse trajectory for the Van der Pol oscillator with c=0.5 and D=0.02.
(b) Piecewise constant parameterization (see Section IV B 1) with the squared L2objective function.
(c) Degree five global polynomial parameterization (see Section IV B 2) with the squared L2objective function.
(d) Neural network parameterization (see Section IV B 3) with the squared L2objective function.
(e) Neural network parameterization with the KL divergence objective function.
(f) Neural network parameterization with the JS divergence objective function.
(g) Neural network parameterization with the squared W2objective function.
FIG. 3. Learning velocity fields to reproduce the statistics of the stochastically-forced Van der Pol oscillator. The ground truth occupation
measure, velocity, and dynamics are shown in (a). The results for inverting the velocity based on the occupation measure from (a) using neural
network, piecewise constant, and global polynomial parameterizations are shown in (b)–(g). The first column shows the objective function;
the second column shows the learned velocity vector field; the third column shows the final PDE forward model evaluation based on the
learned velocity; the fourth column shows the simulation of a diffuse trajectory, and the final column shows the simulation of a trajectory
without diffusion. Specifically, the “diffuse trajectories” are simulated according to the Euler–Maruyama method using the assumed diffusion
coefficient D=0.02, while the “non-diffuse” trajectories assume D=0.The coloring of each diffuse trajectory is given by the occupation
measure it generates; see (4). Across all tests, the objective function is minimized to 0 .25%−0.35% of its initial value. For (b)–(c), the
L-BFGS-B algorithm is used for optimization. In (d)–(g), the neural network architecture consists of a single hidden layer with the hyperbolic
tangent activation function, trained by the Adam optimizer with a learning rate of 10−1.Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 13
activation function yields a C∞velocity. Moreover, while the
global polynomial parameterization is also C∞, it may suffer
from the Runge phenomena and grow rapidly near the bound-
ary of the domain. Thus, we mainly consider neural network
parameterizations of the velocity for the remainder of the nu-
merical tests.
To reduce the computational cost of the inversion in the fi-
nal row of Figure 3, we compute J=W2
2on a coarsened
mesh. Among the four objective functions in Figure 3, it is
worth noting that the W2metric does not compare the two
densities pointwisely and is well-defined for comparing sin-
gular measures. The distance reflects both the local intensity
differences and the global geometry mismatches.67It has also
been shown that the Wasserstein metric is robust to noise.68,69
Thanks to the geometric nature of the optimal transporta-
tion problem, the Wasserstein metric is primarily sensitive to
global changes such as translation and dilation and is robust to
small local perturbations such as noisy measurements of ρ∗.
The better stability also brings a downside as the optimization
landscape can be relatively flat around the ground truth, which
may lead to compromised accuracy in the velocity inversion.
The different velocities shown in the second column of Fig-
ure 3 reveal that there is nonuniqueness if we only use the in-
variant measure as the reference data. The current modeling
assumption yields dynamics reproducing the same invariant
measure but does not necessarily recover the same velocity
field. Depending on the concrete application, one can add reg-
ularization, time information, or focus on velocities in a par-
ticular parameterized subspace to avoid nonuniqueness. The
large error for the reconstructed velocity near the origin is due
to the fact that the method aims to learn the flow on or (in
the case of stochastically-forced dynamics) near the invariant
measure. It is, therefore, unsurprising that the learned velocity
does not match the ground truth where there is no data.
In Figure 4, we show how the inversion accuracy and com-
putation time depend on the chosen value of ∆x. That is, as
∆xdecreases, we can learn velocities that can reproduce the
statistics of the observed occupation measure more accurately,
with the cost of longer computation time.
Next, we provide experimental details on the comparison
of our approach with SINDy2and the Neural ODE5frame-
works in Figure 1. This test uses the Van der Pol oscillator
with c=2. Since the SINDy and Neural ODE methods are
designed for modeling ODEs, the experiments in Figure 1 use
the diffusion coefficient D=0. While we only plot the first 8
points of the slowly sampled trajectory in Figure 1, the full tra-
jectory used for inference contains 2 .5·103observations. The
quickly sampled trajectory also consists of 2 .5·103observa-
tions. The three approaches considered for comparison have
various hyperparameters which can be tuned. For SINDy, we
learn the models from the monomial basis up to degree three
and use the sequentially thresholded least squares optimizer
with threshold 0 .025 to enforce a sparsity condition on the
learned coefficients; see Ref. 2. For the Neural ODE frame-
work, the velocity is parameterized by a single-layer fully
connected neural network with 100 nodes and a hyperbolic
tangent activation function. The Neural ODE is trained us-
ing a multiple shooting approach with the mean-squared er-
FIG. 4. We demonstrate how the computation time and inversion
accuracy depend on the mesh spacing used in the first-order FVM
solver. Here, we use the Van der Pol oscillator with D=0.05 and
learn the velocity using a neural network parameterization. The
Adam optimizer is used with a learning rate of 10−2.In each case,
we reduce the KL divergence objective function below 0 .5% of its
initial value. The error is quantified in terms of the squared W2dis-
crepancy between the simulated occupation measure ˆρaccording to
the learned dynamics and the observed occupation measure ρ∗.
ror objective function. More specifically, rather than treating
the simulation of a single long time-trajectory as the forward
model, we integrate N−1 trajectories initiated at the observed
data points {x(ti)}N−1
i=1for a time of ∆t=ti+1−ti. This ap-
proach results in greater success while modeling slowly sam-
pled dynamics. The Adam optimizer with a learning rate of
10−3is used, and the tolerance for both relative and absolute
error of the ODE solver is set as 10−5.
To ensure a fair comparison with the Neural ODE frame-
work, we consider our approach based on a neural network
parameterization of the velocity using the same architecture,
optimizer, and learning rate. For our approach, we use the KL-
divergence objective function (see Section IV A 1), apply ad-
ditional Gaussian filtering to the occupation measure (see (4))
to simplify the resulting optimization, assume a diffusion co-
efficient of D=10−3during training, and set ∆x=0.1. Thus,
the only differences between the setup for our approach and
the Neural ODE framework are the forward model and objec-
tive function.
Method Sampling Freq. Wall-Clock Time (s) Error
SINDy 10.00 2·10−25.6·10−3
Neural ODE 10.00 5·1025.32·10−3
Ours 10.00 5·1021.14·10−1
SINDy 0.25 10−23.52
Neural ODE 0.25 5·1021.81
Ours 0.25 5·1026.79·10−2
TABLE II. Comparison with the SINDy and Neural ODE frame-
works for learning from trajectories sampled at different frequencies
(Hz). The wall-clock computation time is reported, and the error is
quantified by W2
2(ˆρ,ρ∗), where ˆρis the simulated occupation mea-
sure from the learned velocity field, and ρ∗is the observed occupa-
tion measure.Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 14
As shown in Figure 1, all three frameworks can learn from
the quickly sampled trajectory. However, SINDy and the Neu-
ral ODE frameworks are less robust to changes in the sam-
pling frequency of the inference data than our approach. This
is further demonstrated in Table II, where we quantify the er-
ror in the simulated occupation measure based on the learned
velocity. We report the average error over ten trials with dif-
ferent random training seeds to compare our method and the
Neural ODE framework. When the data is sampled at a suffi-
ciently high frequency, Table II also shows that methods such
as SINDy or the Neural ODE are preferable in terms of both
computational cost and accuracy.
B. Hall-Effect Thruster
We now turn to the more realistic setting of experimentally
sampled time-series data. Specifically, we study the Cathode–
Pearson signal sampled from a Hall-effect thruster (HET) in
its breathing mode. Hall-effect thrusters are in-space propul-
sion devices that exhibit dynamics resembling stable limit cy-
cles while in breathing mode. For details about the experi-
mental setup used to collect the data, the reader is encour-
aged to consult Refs. 70 and 71. In Section V B 1, we utilize
Takens’ theorem23to reformulate the large-scale optimization
framework presented in Sections III and IV to be compatible
with scalar time-series observations, and in Section V B 2 we
demonstrate numerical results based upon this reformulation.
1. Methods
Intrinsic physical fluctuations present in the Cathode–
Pearson signal indicate that the HET’s dynamics may be mod-
eled well by a Fokker–Planck equation. Motivated by this
insight, we first time-delay embed the Cathode–Pearson sig-
nalC(t)ind-dimensions to form the trajectory Cd,τ(t):=
(C(t),C(t−τ)...,C(t−(d−1)τ)). We then use a histogram
approximation to compute the occupation measure ρ∗of
Cd,τ(t); see (4). By viewing each dimension of the coordi-
nate system on which the measure ρ∗is supported as the in-
dependent variables C−kτ(t):=C(t−kτ)where 0 ≤k≤d−1,
we then seek a solution to the optimization problem (2) for a
velocity v=v(Cd,τ;θ). Such a velocity can then provide us
with a model of the asymptotic statistics of the embedded tra-
jectory Cd,τ(t), provided that a suitable diffusion coefficient
can be found.
We note that forming the time-delay coordinates Cd,τ(t)
does require a knowledge of measurements at uniform in-
crements in time. However, the available data may still be
sampled slowly enough such that it is impractical to seek a
direct approximation of the Lagrangian velocity through the
standard approaches described in Section I. This perspective
motivates our use of the approach developed in Sections III
and IV to learn dynamical systems from invariant measures in
time-delay coordinates.There are a few additional considerations that arise when
adapting the modeling framework presented in Sections III
and IV to real-world data. Namely, we do not know the proper
diffusion coefficient a priori (as was the case in Section V A).
Moreover, the invariant measure that the model is based on
does not contain any information about the time scale at which
the system evolves. Towards this, we utilize the following
three-step procedure as a computationally efficient means to
mitigate these difficulties.
1. Bin the trajectory Cd,τ(t)onto a d-dimensional mesh
with spacing ∆xalong each axis to form the occupa-
tion measure ρ∗, assume a constant diffusion coefficient
D>0, and learn the velocity v=v(Cd,τ;θ),using the
framework from Sections III and IV.
2. Bin the trajectory Cd,τ(t)onto another d-dimensional
mesh with spacing ∆ˆx≤∆xto create a new occupation
measure ˆρ∗and adjust the diffusion coefficient by solv-
ing the optimization problem
˜D=argmin
ˆD∈RJ(ρε(v;ˆD),ˆρ∗), (20)
where the term ρε(v;ˆD)in (20) denotes the forward
model evaluation with the diffusion coefficient ˆD.
3. Rescale both the velocity and diffusion by solving the
optimization problem
˜a=argmin
a∈RN
∑
i=1ˆC(ti;a)−Cd,τ(ti)2
2, (21)
where ˆC(ti;a)denotes the time- tisolution of the ODE
initial value problem with velocity av(·;θ)and initial
condition Cd,τ(t0). The final velocity and diffusion are
then given by ˜ av(·;θ)and ˜a˜D, respectively.
The three-step approach makes repeated use of the fact that
ρε(v;D) =ρε(av;aD), for any scalar multiple a>0.Indeed, if
the true diffusion coefficient D∗>0 is unknown a priori, but
we instead seek a solution v(·,θ)with a different diffusion
D>0, it is guaranteed that the velocity v= (D/D∗)v∗will
still provide a solution to the inverse problem. This observa-
tion motivates step one, in which an arbitrary diffusion coef-
ficient is used to find a solution v(·;θ)to the inverse problem.
As the dimensionality dis increased, solving the large-scale
optimization problem in step 1 on a fine mesh becomes infea-
sible. As such, step one is typically performed on a coarse
mesh where additional Gaussian filtering is applied to the in-
ference measure ρ∗to make the large-scale optimization more
feasible.
The diffusion coefficient is then adjusted in step two on a
finer mesh via (20) to mitigate the errors due to the Gaus-
sian filtering, numerical diffusion, and histogram errors in-
curred during step one (see Figure 2). Finally, in step three,
the scale of both the velocity and diffusion are adjusted via
(21) such that the time evolution of simulated trajectories is
consistent with the inference trajectory Cd,τ(t)in delay coor-
dinates. Since diffusion plays a relatively small role over shortLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 15
(a) Inference data
 (b) Learned velocity
 (c) Simulated trajectory
FIG. 5. Learning the velocity from the embedded Cathode-Pearson signal’s invariant measure. We present the time-delay embedded signal
(a), the reconstructed velocity field from the embedded signal’s occupation measure (b), and trajectory simulated with the Euler–Maruyama
method from the learned velocity and diffusion coefficient (c). In (b), blue indicates slow speed and red indicates fast. The velocity was
parameterized by a neural network with 500 nodes in a single hidden layer and learned using the KL divergence loss function. The three-step
procedure in Section V B 1 is used to learn the model, and in step one, additional Gaussian filtering is applied to the occupation measure ρ∗to
simplify the resulting optimization.
time scales for the quasi-periodic HET data, we use the zero-
diffusion trajectory to calibrate a reasonable time-scaling be-
tween our model and the available data. However, as the mag-
nitude of the diffusion increases, the least squares fit in (21)
will become less reliable, and it may be preferable to instead
minimize a transport cost between a collection of model sam-
ples and a collection of data samples at each time-step. While
this final optimization is similar in spirit to various Lagrangian
approaches for learning dynamics (see Section I), we remark
that the parameter space in (21) has only one dimension.
2. Results
The results of the three-step procedure in Section V B 1 for
learning the HET dynamics are shown in Figure 5 for an em-
bedding dimension of d=3 and time-delay of τ=1.4·10−5
seconds, or rather τ=.23 when normalizing the time-scale to
the HET breathing mode frequency (16.6kHz). The modeled
trajectory accurately reconstructs the shape of the embedded
Cathode–Pearson signal but cannot capture the variable diffu-
sion present throughout the time-delayed signal. We do not
expect to capture such details, as we assume a constant dif-
fusion coefficient in our model. Nevertheless, we regard the
reconstruction of the 3D globally attracting limit cycle as a
success and leave extending the model to account for the case
of a non-constant diffusion tensor to future work.
The dimensionality of the original HET dynamics is un-
known, and as such, a sufficient embedding dimension for the
Cathode–Pearson signal is unclear, though likely very high.
Interestingly, we can compare the model learned in Figure 5
with a 2D analog to demonstrate that when the number of
time delays is not sufficiently large, there is more uncertainty
in modeling the time-delayed dynamics. This phenomenon is
most evident when inspecting regions of the delayed Cathode–Pearson signal for which the 2D embedding lacks structure
readily observed in 3D.
Specifically, consider a collection of nearby samples
{C3,τ(ti)}n
i=1in the 3D time-delay coordinate system
(C0,C−τ,C−2τ).The corresponding 2D samples {C2,τ(ti)}n
i=1
will also be nearby one another in the 2D time-delay coordi-
nate system (C0,C−τ).In Figure 6, we initiate uniform distri-
butions centered about these samples in both 2D and 3D time-
delay coordinate systems. We then evolve both the samples
and initial uniform distributions forward in time. The evolu-
tion of the ground truth samples is simply determined by the
time-delayed Cathode Pearson signal Cd,τ(t), and the evolu-
tion of the uniform distributions is given by Fokker–Planck
models constructed from the time-delayed Cathode Pearson
signal’s invariant measure. As the modeled probability den-
sities and ground truth samples evolve in time, we observe in
Figure 6 that the mean of the 3D model matches the true sam-
ple mean more closely than the 2D model and that it has less
uncertainty.
In Figure 7, we study the three parameterizations from
Section IV B for learning the time-delayed Cathode–Pearson
signal’s velocity, now with an embedding dimension of two
to allow for clearer visualizations. It can be seen that the
density associated with each velocity parameterization in-
deed matches the ground truth density in Figure 7, but that
the velocity fields differ significantly from one another. The
piecewise-constant velocity in Figure 7 suffers from poor reg-
ularity with discontinuities on the attracting limit cycle. As a
result, we lose the connection between the Eulerian and La-
grangian dynamics and cannot reconstruct zero-diffusion tra-
jectories that form a stable limit cycle. On the other hand,
the velocities parameterized by the global polynomial and the
neural network are both C∞. The differences among these
three can clearly be seen via the zoomed-in velocity plots in
the second row of Figure 7. The global polynomial and neu-Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 16
(a) Using the 2D model to predict the evolution of the samples C2,τ.
(b) Using the 3D model to predict the evolution of the samples C3,τ.
(c) Uncertainty comparison for the 2D and 3D model predictions.
FIG. 6. Comparing the model accuracy and uncertainty for the embedded Cathode–Pearson signal with 2D and 3D time delays. The time evo-
lution of the 2D and 3D models is compared to a collection {Cd,τ(ti)}n
i=1of samples (plotted in black) from the time-delayed Cathode–Pearson
signal. The plots (a)–(b) feature a qualitative comparison, whereas (c) shows a quantitative comparison of the uncertainties. Throughout, the
time units are normalized to the inverse of a HET breathing mode frequency (16.6kHz). Both the 2D and 3D models utilized a neural network
velocity parameterization with 500 nodes in a single hidden layer and reduced the KL divergence objective function to 0.1% of its initial value
during training. As in Figure 5, the three-step procedure in Section V B 1 is used to learn the models, and in step one, additional Gaussian
filtering is applied to the occupation measure ρ∗to simplify the resulting optimization. The 3D visualization was plotted using Ref. 72.
ral network discretizations are both global parameterizations
of the velocity, and as such, their values near the domain’s
boundary are dictated by the available data in the center of the
domain. This causes the polynomial velocity to rapidly in-
crease near the boundary, and a similar effect can also be seen
for the neural network.
It is worth noting that the initial condition for the optimiza-tion in Figure 7 can play a large role in the reconstructed
velocity, which is related to the optimization landscape of
the nonconvex optimization problem (2) we tackle. In the
case of the piecewise-constant discretization, we initialize all
velocities to be significantly less than the diffusion coeffi-
cient D=0.1. Thus, diffusion initially dominates in the fi-
nite volume solver, and all non-boundary cells will containLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 17
(a) Learned velocity fields for the three parameterizations.
(b) Close-up view of the learned velocity fields.
(c) Forward model evaluations for the learned velocity fields.
FIG. 7. Comparison between the three parameterizations detailed in Section IV B for learning a velocity field from the time-delayed Cathode–
Pearson signal’s invariant measure, using a diffusion coefficient D=0.01. The learned velocities and densities for the piecewise constant (PC),
global polynomial (GP), and neural network (NN) discretizations are shown in the three columns, respectively. We show the learned velocity
field on the full state space (a), a close-up of the velocity field’s direction near the attracting limit cycle (b), and the forward model output
ρε(v(θ))for each parameterization (c). The resulting parameter spaces of these discretizations have a dimensionality of 9800 (PC), 56 (GP),
and 400 (NN). The L2loss is reduced below 0 .1% of its initial cost for the PC and NN discretizations and reduced below 0 .7% of its initial
value for the GP case when we stopped the optimization.
nonzero mass, which allows for accurate gradient updates ev-
erywhere. This phenomenon can also help neural network
training, though it is not always necessary due to the global
nature of parameterization. Moreover, we initialize our poly-
nomial basis to form the velocity
(˙x,˙y) = (−y+x(0.1−x2−y2),x+y(0.1−x2−y2)),
which describes a globally attracting limit cycle. To converge
to the ground truth limit cycle of the time-delayed Cathode–
Pearson signal, this initial velocity only needs to be translated
and deformed.C. Temperature Uncertainty Quantification
We now study 2D time-delay embedded data of weekly
rolling averages of the temperature in Ithaca, NY , between
2006 and 2020.73We view temperature fluctuations over short
time scales as an intrinsic diffusion process and the approx-
imately periodic oscillation of seasonal temperatures driven
by some nonzero velocity. Thus, we model the 2D data in
delay coordinates as a diffuse limit cycle. We again follow
the procedure in Section V B 1 to learn a velocity v(x;θ)and
diffusion coefficient D, which closely matches the occupation
measure.
As in Section V B, we can use the trained model v(x;θ)Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 18
(a) Occupation measure from weekly rolling
averaged temperature measurements.
(b) Learned velocity from the observed occupation
measure.
(c) PDE forward model for the learned velocity
(d) Evolving the learned Fokker–Planck equation in time to quantify the uncertainty in future temperature forecasts.
FIG. 8. Performing prediction and uncertainty quantification for Ithaca, NY’s temperature in 2019. (a): The ground truth occupation measure
accumulated from 13 years of weekly rolling averaged temperature observations, normalized by an affine transformation to [−1,1]; (b): The
learned velocity vector field; (c): The corresponding forward model output. In (d), the PDE model with a uniform initialization in the box
from (c) is evolved in time and used to quantify the uncertainty in the measurements of C0. Observed trajectories of the temperature in delay
coordinates with initial conditions displayed in the top left plot are also shown to demonstrate the effectiveness of the learned model. A time
delay of τ=280 days is used, and the model is trained using a neural network parameterization and the KL-divergence objective function.
to quantify measurement uncertainties through the Fokker–
Planck equation (9), whose solution is a probability density in
the time-delay coordinates (C0,C−τ). Specifically, if we know
some initial probability distribution that captures the current
state of the temperature system well, we can consider the time
evolution of the distribution using our trained model to quan-
tify the uncertainty of future temperature measurements. The
process of evolving both the Fokker–Planck PDE from a uni-
form distribution and the ground truth sample paths from past
temperature measurements is shown in Figure 8. The uncer-
tainty bounds from the model accurately capture fluctuations
in the training data used to form the occupation measure (plot-
ted in black), as well as a testing sample path previously un-
seen by the model (plotted in red).
It is also worth noting that the confidence intervals we con-
struct may be larger than the actual range due to several fac-
tors, including additional extrinsic noise from filtering the
data, modeling errors accumulated from the hypothesis space,
numerical diffusion in the forward model, and a sub-optimal
embedding dimension. Reducing such errors may result in
tighter confidence intervals and considering time delays in
higher dimensions could yield better predictions of the tem-
perature’s transient behaviors.D. Lorenz-63 System
We conclude this section by studying the Lorenz-63
system,21defined by


˙x=c1(y−x)
˙y=x(c2−z)−y
˙z=xy−c3z, (22)
where we consider (c1,c2,c3) = ( 10,28,8/3).For these
choices of parameters, the Lorenz-63 system exhibits chaotic
behavior and admits a unique physical measure.22In Fig-
ure 9, we assume that the quantities ˙ yand ˙zare known, and
we learn a model for the velocity in the x-direction, using
the stochastically-forced Lorenz-63 system’s occupation mea-
sure. We emphasize that the data used to approximate the
Lorenz system’s occupation measure can be sampled slowly
or even randomly in time (see Ref. 18, Figure 7). From the
approximate occupation measure, we are able to successfully
invert the first component ˙ xof the Lorenz-63 system’s velocity
via a neural network parameterization.
We remark that when ˙ x,˙y,and ˙zare all simultaneously in-
verted, the optimization is unsuccessful at reconstructing theLearning Dynamics on Invariant Measures using PDE-Constrained Optimization 19
(a) Learned velocity vector field (left), a simulated trajectory with diffusion (middle), and a simulated trajectory without diffusion (right).
(b) True velocity (left), a ground truth SDE trajectory (middle), and a ground true ODE trajectory (right).
FIG. 9. Neural network parameterization of ˙ xusing the Lorenz system’s stochastically perturbed invariant measure with D=10 and ∆x=2.
(a): The learned velocity and a single trajectory plotted both with and without diffusion; (b) The ground truth velocity and trajectories both
with and without diffusion. For visualization of the occupation measure used to learn the model displayed in the top row, we refer to Ref. 18.
true velocity (22). While we may be able to learn a velocity
that approximately recovers the stationary state of the Lorenz-
63 system in the sense of (9), the physical property (3) does
not hold. Whether the difficulties of inverting all velocity
components of the Lorenz-63 system are due to inherent non-
uniqueness in the inverse problem or simply inconvenient lo-
cal minima during training is worth further investigation in
future work. To demonstrate the applicability of our approach
to non-rational velocities, we also consider the Arctan Lorenz-
63 system,18given by


˙x=50arctan (c1(y−x)/50)
˙y=50arctan (x(c2−z)/50−y/50)
˙z=50arctan (xy/50−c3z/50), (23)
where again (c1,c2,c3) = ( 10,28,8/3).The results for invert-
ing ˙xfrom the occupation measure generated by (23) with ad-
ditional stochastic forcing are shown in Figure 10, assuming
that the quantities ˙ yand ˙zare known.
VI. CONCLUSION
In this paper, we introduced a PDE-constrained optimiza-
tion approach to modeling trajectory data originating fromstochastic dynamical systems. We first adapted the invariant
measure surrogate model in Ref. 18 based upon the continu-
ity equation to the Fokker–Planck equation. This increased
our modeling capacity and prevented overfitting the recon-
structed velocity while modeling intrinsically noisy trajec-
tories. We next extended the three-coefficient learning per-
formed in Ref. 18 to thousands of coefficients by modeling
the velocity via global polynomials, piecewise polynomials,
and fully connected neural networks. The efficient gradient
computation presented in Section IV made these large-scale
parameterizations of the velocity computationally tractable.
We finally studied velocity inversion for invariant measures of
time-delay embedded observables. The method of time-delay
embedding is useful for analyzing real-world data, where in
many cases, only limited observations of complex systems are
available. As such, we proceeded to learn the velocity in time-
delay coordinates for a Hall-effect thruster system and rolling
weekly averages of temperature measurements. Using these
models, we predicted future states of the systems and quanti-
fied uncertainty in forecasts by evolving the learned Fokker–
Planck equation forward in time.Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 20
(a) Learned velocity vector field (left), a simulated trajectory with diffusion (middle), and a simulated trajectory without diffusion (right).
(b) True velocity (left), a ground truth SDE trajectory (middle), and a ground true ODE trajectory (right).
FIG. 10. Neural network parameterization of ˙ xusing the Arctan Lorenz system’s stochastically perturbed invariant measure with D=10 and
∆x=2. The neural network used to learn the velocity contains a single layer of 100 nodes with the sigmoid activation function, and the L2
objective function is used to train the model. In (a), we show results for the learned velocity vector field and the simulation of trajectories
using the learned velocity both with and without diffusion. For comparison, in (b), we also include the ground truth velocity and trajectories
with and without diffusion.
ACKNOWLEDGEMENTS
This paper was supported in part by a fellowship award
under contract FA9550-21-F-0003 through the National De-
fense Science and Engineering Graduate (NDSEG) Fellow-
ship Program, sponsored by the Air Force Research Labora-
tory (AFRL), the Office of Naval Research (ONR) and the
Army Research Office (ARO). R. Martin was partially sup-
ported by AFOSR Grants FA9550-20RQCOR098 (PO: Leve)
and FA9550-20RQCOR100 (PO: Fahroo). This work was
done in part while Y . Yang was visiting the Simons Institute
for the Theory of Computing in Fall 2021. Y . Yang acknowl-
edges support from Dr. Max Rössler, the Walter Haefner
Foundation and the ETH Zürich Foundation. This material
is based upon work supported by the National Science Foun-
dation under Award Number DMS-1913129.
We thank Dr. Chen Li for his helpful suggestions and gen-
erosity in sharing code for the approach of Section IV B 3.
We would like to thank the referees for carefully reading
our manuscript and giving many constructive comments that
helped improve the paper.AUTHOR DECLARATIONS
Conflict of Interest
The authors have no conflicts to disclose.
Author Contributions
Jonah Botvinick-Greenhouse : Formal analysis (lead),
software (lead), writing – original draft preparation (equal),
writing – review and editing (equal). Robert Martin : Con-
ceptualization (equal), writing – review and editing (equal).
Yunan Yang : Conceptualization (equal), formal analysis
(supporting), supervision (lead), writing – original draft
preparation (equal), writing – review and editing (equal).
Data Availability
The data used in Sections V A, V B, and V D is available
from the authors upon reasonable request. The data used in
Section V B was obtained from the EPTEMPEST experimen-Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 21
tal program funded by AFSOR grant FA9550-17QCOR497
(Program Officer: Dr. Brett Pokines). The data used in Sec-
tion V C is openly available via Ref. 73.
REFERENCES
1F. J. Montáns, F. Chinesta, R. Gómez-Bombarelli, and J. N. Kutz, “Data-
driven modeling and learning in science and engineering,” Comptes Rendus
Mécanique 347, 845–855 (2019).
2S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equa-
tions from data by sparse identification of nonlinear dynamical systems,”
Proceedings of the National Academy of Sciences 113, 3932–3937 (2016).
3B. de Silva, K. Champion, M. Quade, J.-C. Loiseau, J. Kutz, and S. Brun-
ton, “PySINDy: A Python package for the sparse identification of nonlinear
dynamical systems from data,” Journal of Open Source Software 5, 2104
(2020).
4A. A. Kaptanoglu, B. M. de Silva, U. Fasel, K. Kaheman, A. J. Gold-
schmidt, J. Callaham, C. B. Delahunt, Z. G. Nicolaou, K. Champion, J.-
C. Loiseau, J. N. Kutz, and S. L. Brunton, “Pysindy: A comprehensive
python package for robust sparse system identification,” Journal of Open
Source Software 7, 3994 (2022).
5R. T. Chen, Y . Rubanova, J. Bettencourt, and D. K. Duvenaud, “Neural or-
dinary differential equations,” Advances in Neural Information Processing
Systems 31(2018).
6E. Baake, M. Baake, H. Bock, and K. Briggs, “Fitting ordinary differential
equations to chaotic data,” Physical Review A 45, 5524 (1992).
7C. Michalik, R. Hannemann, and W. Marquardt, “Incremental single
shooting—a robust method for the estimation of parameters in dynamical
systems,” Computers & Chemical Engineering 33, 1298–1305 (2009).
8J. Jia and A. R. Benson, “Neural jump stochastic differential equations,”
Advances in Neural Information Processing Systems 32(2019).
9E. Negrini, G. Citti, and L. Capogna, “System identification through lips-
chitz regularized deep neural networks,” Journal of Computational Physics
444, 110549 (2021).
10U. Fasel, J. N. Kutz, B. W. Brunton, and S. L. Brunton, “Ensemble-SINDy:
Robust sparse model discovery in the low-data, high-noise limit, with active
learning and control,” Proceedings of the Royal Society A 478, 20210904
(2022).
11R. Van Der Merwe and E. A. Wan, “The square-root unscented Kalman
filter for state and parameter-estimation,” in 2001 IEEE international con-
ference on acoustics, speech, and signal processing. Proceedings (Cat. No.
01CH37221) , V ol. 6 (IEEE, 2001) pp. 3461–3464.
12G. Evensen, “The ensemble kalman filter: Theoretical formulation and
practical implementation,” Ocean dynamics 53, 343–367 (2003).
13D. Simon, Optimal state estimation: Kalman, H infinity, and nonlinear ap-
proaches (John Wiley & Sons, 2006).
14J. Harlim, A. Mahdi, and A. J. Majda, “An ensemble Kalman filter for
statistical estimation of physics constrained nonlinear regression models,”
Journal of Computational Physics 257, 782–812 (2014).
15F. Hamilton, T. Berry, and T. Sauer, “Ensemble Kalman filtering without a
model,” Physical Review X 6, 011021 (2016).
16C. Schillings and A. M. Stuart, “Analysis of the ensemble Kalman filter for
inverse problems,” SIAM Journal on Numerical Analysis 55, 1264–1290
(2017).
17C. Greve, K. Hara, R. Martin, D. Eckhardt, and J. Koo, “A data-driven
approach to model calibration for nonlinear dynamical systems,” Journal of
Applied physics 125, 244901 (2019).
18Y . Yang, L. Nurbekyan, E. Negrini, R. Martin, and M. Pasha, “Opti-
mal transport for parameter identification of chaotic dynamics via invari-
ant measures,” SIAM Journal on Applied Dynamical Systems 22, 269–310
(2023).
19T. R. Bewley and A. S. Sharma, “Efficient grid-based bayesian estimation
of nonlinear low-dimensional systems with sparse non-gaussian pdfs,” Au-
tomatica 48, 1286–1290 (2012).
20L.-S. Young, “What are SRB measures, and which dynamical systems have
them?” Journal of statistical physics 108, 733–754 (2002).21S. Luzzatto, I. Melbourne, and F. Paccaut, “The Lorenz attractor is mixing,”
Communications in Mathematical Physics 260, 393–401 (2005).
22W. Tucker, “The Lorenz attractor exists,” Comptes Rendus de l’Académie
des Sciences - Series I - Mathematics 328, 1197–1202 (1999).
23F. Takens, “Detecting strange attractors in turbulence,” in Dynamical Sys-
tems and Turbulence, Warwick 1980 (Springer, 1981) pp. 366–381.
24Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hub-
bard, and L. D. Jackel, “Backpropagation applied to handwritten zip code
recognition,” Neural Computation 1, 541–551 (1989).
25A. M. McDonald, M. A. van Wyk, and G. Chen, “The inverse Frobenius–
Perron problem: A survey of solutions to the original problem formulation,”
AIMS Mathematics 6, 11200–11232 (2021).
26W. Cowieson and L.-S. Young, “SRB measures as zero-noise limits,” Er-
godic Theory and Dynamical Systems 25, 1115–1138 (2005).
27G. Froyland, Estimating physical invariant measures and space averages
of dynamical systems indicators , Ph.D. thesis, The University of Western
Australia (1996).
28A. Katok and B. Hasselblatt, Introduction to the Modern Theory of Dynami-
cal Systems , Encyclopedia of Mathematics and its Applications (Cambridge
University Press, 1995).
29M. Einsiedler and T. Ward, Ergodic Theory: with a view towards Number
Theory (Springer London, London, 2011).
30A. Allawala and J. B. Marston, “Statistics of the stochastically forced
Lorenz attractor by the Fokker-Planck equation and cumulant expansions,”
Physical Review E 94(2016), 10.1103/physreve.94.052218.
31A. Lasota and M. C. Mackey, Chaos, fractals, and noise: stochastic aspects
of dynamics , V ol. 97 (Springer Science & Business Media, 1998).
32M. Dellnitz, G. Froyland, and O. Junge, “The algorithms behind GAIO —
set oriented numerical methods for dynamical systems,” in Ergodic The-
ory, Analysis, and Efficient Simulation of Dynamical Systems , edited by
B. Fiedler (Springer Berlin Heidelberg, Berlin, Heidelberg, 2001) pp. 145–
174.
33S. Klus, P. Koltai, and C. Schütte, “On the numerical approximation of
the Perron-Frobenius and Koopman operator,” Journal of Computational
Dynamics 3, 51–77 (2016).
34A. Blumenthal and L.-S. Young, “Equivalence of physical and SRB mea-
sures in random dynamical systems,” Nonlinearity 32, 1494–1524 (2019).
35J. Hong and X. Wang, “Invariant measures for stochastic differential equa-
tions,” in Invariant Measures for Stochastic Nonlinear Schrödinger Equa-
tions (Springer, 2019) pp. 31–61.
36G. A. Pavliotis, Stochastic Processes and Applications (Springer New York,
NY , 2014).
37W. Huang, M. Ji, Z. Liu, and Y . Yi, “Steady states of Fokker–Planck equa-
tions: I. existence,” Journal of Dynamics and Differential Equations 27,
721–742 (2015).
38X. Chen, L. Yang, J. Duan, and G. E. Karniadakis, “Solving inverse
stochastic problems from discrete particle observations using the fokker–
planck equation and physics-informed neural networks,” SIAM Journal on
Scientific Computing 43, B811–B830 (2021).
39S. L. Brunton, B. W. Brunton, J. L. Proctor, E. Kaiser, and J. N. Kutz,
“Chaos as an intermittently forced linear system,” Nature communications
8, 19 (2017).
40A. Kirtland, J. Botvinick-Greenhouse, M. DeBrito, M. Osborne, C. John-
son, R. S. Martin, S. J. Araki, and D. Q. Eckhardt, “An unstructured mesh
approach to nonlinear noise reduction for coupled systems,” arXiv preprint
arXiv:2209.05944 (2022).
41G. Sugihara, R. May, H. Ye, C.-h. Hsieh, E. Deyle, M. Fogarty, and
S. Munch, “Detecting causality in complex ecosystems,” science 338, 496–
500 (2012).
42T. Sauer, J. A. Yorke, and M. Casdagli, “Embedology,” Journal of Statisti-
cal Physics 65, 579–616 (1991).
43D. Chelidze, “Reliable Estimation of Minimum Embedding Dimension
Through Statistical Analysis of Nearest Neighbors,” Journal of Computa-
tional and Nonlinear Dynamics 12(2017).
44H. Ma and C. Han, “Selection of embedding dimension and delay time in
phase space reconstruction,” Frontiers of Electrical and Electronic Engi-
neering in China 1, 111–114 (2006).
45A. Maus and J. C. Sprott, “Neural network method for determining embed-
ding dimension of a time series,” Communications in Nonlinear Science
and Numerical Simulation 16(2010), 10.1016/j.cnsns.2010.10.030.Learning Dynamics on Invariant Measures using PDE-Constrained Optimization 22
46S. Wallot and D. Mønster, “Calculation of average mutual information
(AMI) and false-nearest neighbors (FNN) for the estimation of embedding
parameters of multidimensional time series in Matlab,” Frontiers in psy-
chology 9, 1679 (2018).
47X. Chen, H. Wang, and J. Duan, “Detecting stochastic governing laws with
observation on stationary distributions,” Physica D: Nonlinear Phenomena
448, 133691 (2023).
48X. Nie, D. Coca, J. Luo, and M. Birkin, “Solving the inverse Frobenius-
Perron problem using stationary densities of dynamical systems with input
perturbations,” Communications in Nonlinear Science and Numerical Sim-
ulation 90, 105302 (2020).
49D. Pingel, P. Schmelcher, and F. Diakonos, “Theory and examples of
the inverse Frobenius–Perron problem for complete chaotic maps,” Chaos
(Woodbury, N.Y .) 9, 357–366 (1999).
50N. Wei, Solutions of the Inverse Frobenius–Perron Problem , Master’s the-
sis, Concordia University (2015), unpublished.
51S. Grossmann and S. Thomae, “Invariant distributions and stationary corre-
lation functions of one-dimensional discrete processes,” Zeitschrift Natur-
forschung Teil A 32, 1353–1363 (1977).
52X. Nie and D. Coca, “A matrix-based approach to solving the inverse frobe-
nius–perron problem using sequences of density functions of stochastically
perturbed dynamical systems,” Communications in Nonlinear Science and
Numerical Simulation 54, 248–266 (2018).
53C. Fox, L.-J. Hsiao, and J.-E. Lee, “Solutions of the multivariate inverse
Frobenius–Perron problem,” Entropy 23, 838 (2021).
54A. M. McDonald and M. A. van Wyk, “A novel approach to solving the gen-
eralized inverse Frobenius–Perron problem,” in 2020 IEEE International
Symposium on Circuits and Systems (ISCAS) (IEEE, 2020) pp. 1–5.
55R. J. LeVeque et al. ,Finite volume methods for hyperbolic problems ,
V ol. 31 (Cambridge University Press, 2002).
56J. Hu and X. Zhang, “Positivity-preserving and energy-dissipative finite dif-
ference schemes for the Fokker–Planck and Keller–Segel equations,” IMA
Journal of Numerical Analysis (2022).
57D. F. Gleich, “Pagerank beyond the web,” SIAM Review 57, 321–363
(2015).
58D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in
ICLR (Poster) (2015).
59L. Nurbekyan, W. Lei, and Y . Yang, “Efficient Natural Gradient De-
scent Methods for Large-Scale PDE-Based Optimization Problems,” arXiv
preprint arXiv:2202.06236 (2022).60C. Villani, Topics in optimal transportation , V ol. 58 (American Mathemat-
ical Soc., 2021).
61M. Jacobs and F. Léger, “A fast approach to optimal transport: The back-
and-forth method,” Numerische Mathematik 146, 513–544 (2020).
62T. Séjourné, F.-X. Vialard, and G. Peyré, “Faster unbalanced optimal
transport: Translation invariant sinkhorn and 1-d frank-wolfe,” in Interna-
tional Conference on Artificial Intelligence and Statistics (PMLR, 2022)
pp. 4995–5021.
63F. Lu, M. Maggioni, and S. Tang, “Learning interaction kernels in stochas-
tic systems of interacting particles from multiple trajectories,” Foundations
of Computational Mathematics , 1–55 (2021).
64K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward net-
works are universal approximators,” Neural Networks 2, 359–366 (1989).
65C. Li, M. Dunlop, and G. Stadler, “Bayesian neural network priors for
edge-preserving inversion,” Inverse Problems and Imaging 16, 1229–1254
(2022).
66J. Guckenheimer, “Dynamics of the Van der Pol equation,” IEEE Transac-
tions on Circuits and Systems 27, 983–989 (1980).
67B. Engquist and Y . Yang, “Optimal transport based seismic inversion: Be-
yond cycle skipping,” Communications on Pure and Applied Mathematics
(2020).
68M. M. Dunlop and Y . Yang, “Stability of gibbs posteriors from the wasser-
stein loss for bayesian full waveform inversion,” SIAM/ASA Journal on
Uncertainty Quantification 9, 1499–1526 (2021).
69B. Engquist, K. Ren, and Y . Yang, “The quadratic Wasserstein metric for
inverse data matching,” Inverse Problems 36, 055001 (2020).
70D. Eckhardt, J. Koo, R. Martin, M. Holmes, and K. Hara, “Spatiotemporal
data fusion and manifold reconstruction in hall thrusters,” Plasma Sources
Science and Technology 28, 045005 (2019).
71N. A. MacDonald, M. A. Cappelli, and W. A. H. Jr., “Time-synchronized
continuous wave laser-induced fluorescence on an oscillatory xenon dis-
charge,” Review of Scientific Instruments 83, 1–8 (2012).
72P. Ramachandran and G. Varoquaux, “Mayavi: 3D visualization of scien-
tific data,” Computing in Science & Engineering 13, 40–51 (2011).
73H. J. Diamond, T. R. Karl, M. A. Palecki, C. B. Baker, J. E. Bell, R. D.
Leeper, D. R. Easterling, J. H. Lawrimore, T. P. Meyers, M. R. Helfert,
G. Goodge, and P. W. Thorne, “U.S. Climate Reference Network after One
Decade of Operations: Status and Assessment,” Bulletin of the American
Meteorological Society 94, 485 – 498 (2013).