Double Equivariance for Inductive Link Prediction for Both
New Nodes and New Relation Types
Jincheng Zhou
Purdue University
zhou791@purdue.eduYucheng Zhang
Purdue University
zhan4332@purdue.eduJianfei Gao∗
Purdue University
gao462@purdue.eduYangze Zhou∗
Purdue University
zhou950@purdue.edu
Bruno Ribeiro
Purdue University
ribeiro@cs.purdue.edu
Abstract
The task of fully inductive link prediction in knowledge graphs has gained signif-
icant attention, with various graph neural networks being proposed to address it.
This task presents greater challenges than traditional inductive link prediction
tasks with only new nodes, as models must be capable of zero-shot generalization
to both unseen nodes and unseen relation types in the inference graph. Despite
the development of novel models, a unifying theoretical understanding of their
success remains elusive, and the limitations of these methods are not well-studied.
In this work, we introduce the concept of double permutation-equivariant repre-
sentations and demonstrate its necessity for effective performance in this task.
We show that many existing models, despite their diverse architectural designs,
conform to this framework. However, we also identify inherent limitations in
double permutation-equivariant representations, which restrict these models’
ability to learn effectively on datasets with varying characteristics. Our findings
suggest that while double equivariance is necessary for meta-learning across
knowledge graphs from different domains, it is not sufficient. There remains a
fundamental gap between double permutation-equivariant models and the con-
cept of foundation models designed to learn patterns across all domains.
1 Introduction
Knowledge graphs (KGs) are often domain-specific, such as those used in health care [ 1], material
science [ 2,3], e-commerce [ 4], etc.. For robust learning across multiple domains, it is desirable
to pretrain knowledge graph models on multiple domains and then perform zero-shot predictions
on new (unseen) domains (Figure 1). This work lays the foundations for what we term zero-shot
fully inductive link prediction , which aims to predict missing factual triplets in knowledge graphs at
inference time, even when both nodes and relations are unseen during training [5, 6].
Fully inductive link prediction differs from conventional inductive link prediction task [ 7–11], where
only unseen nodes are found during inference, but the vocabulary of relation types remains the same.
But while there are graph neural networks (GNNs) for zero-shot fully inductive link prediction, such
as InGram [ 5] and Ultra [ 6] (sometimes described as knowledge graph foundation models [ 6,12]),
there is still no unified theoretical framework that explains why these approaches should work or any
theory to guide the design of future architectures.
Contributions. This work introduces a general theoretical framework, named double permutation-
equivariant graph representations , which we demonstrate as the underlying principle behind models
like InGram [ 5] and Ultra [ 6]. We demonstrate how this framework provides a general blueprint
for designing future models by constructing a simple model following the theory and improving
∗Equal contribution
Preprint. Preliminary work.arXiv:2302.01313v8  [cs.LG]  14 Jan 2025Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
1-Hop
Relations
Are
V ariablesNodes
Are
V ariablesTest has
New Nodes &
New RelationsT est
racing driver
racing driver
teammate
teammate
team principle
Christian Horner
Max V erstappen  Sergio  PérezRed Bull Racing
occupationoccupationT raining
based in
Formula One Driveremployeeemployee?
co-worker
co-workerCEO
Sundar Pichai
Peter Norvig Jeff DeanGoogle
ﬁeld ofﬁeld of
Computer Science
headquarter in
(a) (Our task) fully inductive link prediction
1-Hop team principle
teammate
teammate
occupationoccupationracing driverracing driver
Relations
Are
ConstantsNodes
Are
V ariablesTest has
New NodesT est
racing driverracing driver
teammate
teammateteam principle
Christian Horner
Max V erstappen  Sergio  PérezRed Bull Racing
occupationoccupationT raining
based in
Formula One Driverracing driverracing driver?
teammate
teammateteam principle
Toto W olff
Lewis Hamiltion Geor ge RussellMercedes-Benz
occupationoccupationbased in
Formula One Driver
 (b) Node-only inductive link prediction
Figure 1: (a)Fully inductive link prediction: Model learns to inductively predict new facts over an inference-
time KG from a different domain, with new nodes and new relation types. (b) Traditional node-only inductive
link prediction: Inference-time KG has new nodes but relation types are all seen in training.
an existing model (InGram). Most importantly, we highlight the limitations of allsuch models in
scenarios where negative transfer [13, 14] happens. Specifically:
1.We formally define the notion of double equivariant structural representations anddistributionally
double equivariant positional embeddings . We demonstrate the generality of this theoretical
framework by proving that Ultra [ 6] produces double equivariant structural representations, while
InGram [5] generates distributionally double equivariant positional embeddings.
2.Based on the these insights, we propose a simple yet effective variant of InGram, named DEq-
InGram , which enhances the robustness and stability of the original InGram model. Additionally,
we introduce a straightforward modeling framework called ISDEA+ , based on the principle of
double equivariant structural representations. This framework can transform anyGNNs designed
for homogeneous graphs into double equivariant models suitable for knowledge graphs.
3.We empirically demonstrate the effectiveness and improvements brought by our framework on two
new knowledge graph datasets: PediaTypes and WikiTopics. Despite the positive results, we show
thatallexisting double equivariant models face significant limitations in their ability to jointly
learn across multiple domains. Specifically, these models suffer from negative transfer when
certain knowledge graphs are combined during training. This finding highlights a key challenge
on the path toward achieving ideal graph foundation models [6, 12].
2 Double Equivariance Enables Fully Inductive Link Prediction
In what follows, we first introduce the fully inductive link prediction task. We then proceed to
theoretically describe the task in a general setting and propose our double equivariant modeling
framework to handle the task using structural representations and positional embeddings.
2.1 Formalizing the Fully Inductive Link Prediction Task
We now introduce notations and definitions used throughout this paper. First, we formally define
our inductive link prediction task for both new nodes and new relation types, i.e., fully inductive
link prediction, over knowledge graphs. We denote [n] :={1, . . . , n }for any n∈N. LetG(train)=
(V(train),R(train),A(train))be the training knowledge graph, where V(train)is the set of N(train)training
nodes, R(train)is the set of R(train)training relation types. We also define two associated bijective
mappings v(train)
· : [N(train)]→ V(train), r(train)
· : [R(train)]→ R(train)that enumerate the nodes and
relation types in training. The tensor A(train)∈ {0,1}N(train)×R(train)×N(train)defines the adjacency of
the training graph such that ∀(i, k, j )∈[N(train)]×[R(train)]×[N(train)],A(train)
i,k,j= 1indicates that
the triplet (v(train)
i, r(train)
k, v(train)
j)is present in the data (we denote (i, k, i)as the k-th attribute of
2Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
node i). To simplify notation, we further refer to the collection of all knowledge graphs of any sizes
asA:=∪∞
N=1∪∞
R=2{0,1}N×R×N.
Definition 2.1 (Fully inductive link prediction task ).The task of fully inductive link prediction
learns a model on a set of Ktraining graphs {G(train)
1, . . . ,G(train)
K}(often in a self-supervised fashion
by masking links) and inductively applies it to predict missing links in an inference graph G(inf)=
(V(inf),R(inf),A(inf))with new nodes and new relation types, i.e., V(inf)̸⊂ ∪K
i=1V(train)
i,R(inf)̸⊂
∪K
i=1R(train)
i , without extra context given to the model.
In what follows, for simplicity we ignore the superscript train andtest. And since there are bijections
v·andr·between indices and nodes and relation types, we represent the triplet (vi, rk, vj)∈ V ×
R × V with indices (i, k, j )∈[N]×[R]×[N], and mainly use Ato denote the knowledge graph.
Figure 1(a) shows a concrete example of a fully inductive link prediction task. Here, the training KG
capturing the topic of F1 racing and the test KG concerning the organizational structure of a company
have non-overlapping set of entities and relation types. This is in contrast to existing node-only
inductive task as depicted in Figure 1(b) as well as some of the relation-inductive settings studied by
existing work [ 15–19]. Notably, the latter generally can handle unseen relation types only when the
test graph is a superset of the training graph . In this case, the existence of an edge with an unseen
relation type could be inferred from its neighborhood that only contains training relation types. These
methods thus cannot be applied to Figure 1(a) - here the neighborhood around the target edge in
the test graph, (Google, employee, Jeff Dean), only consists of other unseen relation types. We also
note that the test graph does not carry additional context such as textual description embeddings for
unseen relations or graph ontology. Section 4 and appendix D have more thorough discussion.
Because of the generality of Definition 2.1, the fully inductive link prediction forces the model to not
rely on potential overlaps or additional contextual information, and must learn to differentiate nodes
and relations based only on their structural relationships in A, rather than their labels in VandR.
This is achieved by what we call the double equivariant representations , which we elaborate next.
2.2 Double Equivariant Representations for Knowledge Graphs
In what follows, we provide definitions and theoretical statements of our proposed double equivariant
knowledge graph representations in the main paper while referring all proofs to Appendix C. The
proposal starts with defining the permutation actions on knowledge graphs as:
Definition 2.2 (Node and relation permutation actions on knowledge graphs ).For any knowledge
graphA∈Awith number of nodes and relations N, R , a node permutation ϕ∈SNis an element of
the symmetric group SN, a relation permutation τ∈SRis an element of the symmetric group SR,
and the operation ϕ◦τ◦Ais the action of ϕandτonA, defined as ∀(i, k, j )∈[N]×[R]×[N],(ϕ◦
τ◦A)ϕ◦i,τ◦k,ϕ◦j=Ai,k,j where ϕ◦i=ϕiandτ◦k=τk. The node and relation permutation
actions on Aare commutative, i.e., ϕ◦τ◦A=τ◦ϕ◦A.
To learn structural representation for both nodes and relations, we first design triplet representations
that are invariant to the two permutation actions on nodes and relations, as shown below.
Definition 2.3 (Double invariant triplet representations ).For any knowledge graph A∈A
with number of nodes and relations N, R , a double invariant triplet representation is a function
Γtriplet:∪∞
N=1∪∞
R=2([N]×[R]×[N])×A→Rd, d≥1, such that ∀(i, k, j )∈[N]×[R]×[N],∀ϕ∈
SN,∀τ∈SR,Γtriplet((i, k, j ),A) = Γ triplet((ϕ◦i, τ◦k, ϕ◦j), ϕ◦τ◦A).
To understand the property of our double invariant triplet representations, we first introduce the notion
of knowledge graph isomorphism and triplet double isomorphism.
Definition 2.4 (Knowledge graph isomorphism and Triplet isomorphism ).We say two knowledge
graphsA(G),A(G′)∈Awith number of nodes and relations N(G), R(G)andN(G′), R(G′)respec-
tively, are isomorphic (denoted as “ A(G)≃graphA(G′)”) if and only if ∃ϕ∈SN(G),∃τ∈SR(G),
such that ϕ◦τ◦A(G)=A(G′). And we say two triplets (i(G), k(G), j(G))∈[N(G)]×[R(G)]×
[N(G)],(i(G′), k(G′), j(G′))∈[N(G′)]×[R(G′)]×[N(G′)]are isomorphic triplets (denoted as
“((i(G), k(G), j(G)),A(G))≃triplet((i(G′), k(G′), j(G′)),A(G′))”) if and only if ∃ϕ∈SN(G),∃τ∈
SR(G), such that ϕ◦τ◦A(G)=A(G′)and(i(G′), k(G′), j(G′)) = (ϕ◦i(G), τ◦k(G), ϕ◦j(G)).
As an example, the training and test graphs in Figure 1(a) are isomorphic KGs. Moreover, the
triplet (Red Bull Racing, racing driver, Sergio Pérez) in the training graph is isomorphic to the triplet
3Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
(Google, employee, Jeff Dean) in the test graph, as they are the counterparts of each other in the
respective isomorphic KGs. It is clear that our double invariant triplet representations are able to
output the same representations for these isomorphic triplets, enabling fully inductive link prediction,
where the model trained to predict the missing (Red Bull Racing, racing driver, Sergio Pérez) in the
training graph is able to predict the missing (Google, employee, Jeff Dean) in test.
Definitions 2.3 and 2.4 generalize the traditional graph isomorphism defined on homogeneous
graphs [ 20,21] to knowledge graphs, by considering not only permutations actions on the nodes,
which is those concerned by traditional graph isomorphisms, but also permutation actions on the
relation types. The connection between Definition 2.3 and logical reasoning can be found in
Appendix B. Next, we define the structure double equivariant representations for the whole knowledge
graphA(akin to how GNNs provide representations for a whole graph).
Definition 2.5 (Double equivariant knowledge graph structural representations ).For any
knowledge graph A∈Awith number of nodes and relations N, R , a function Γgraph:A→
∪∞
N=1∪∞
R=2RN×R×N×d, d≥1is double equivariant w.r.t. arbitrary node ϕ∈SNand relation
τ∈SRpermutations, if Γgraph(ϕ◦τ◦A) =ϕ◦τ◦Γgraph(A). Moreover, valid mappings of Γgraph
must map a domain element to an image element with the same number of nodes and relations.
Finally, we connect Definitions 2.3 and 2.5 by showing how to build double equivariant graph
representations from double invariant triplet representations in Theorem 2.6, and vice-versa.
Theorem 2.6 (From double invariant triplet representations to double equivariant graph rep-
resentations ).For all A∈Awith number of nodes and relations N, R , given a double invari-
ant triplet representation Γtriplet, we can construct a double equivariant graph representation as
(Γgraph(A))i,k,j:= Γ triplet((i, k, j ),A),∀(i, k, j )∈[N]×[R]×[N], and vice-versa.
Next, we consider positional graph embeddings that are equivariant in distribution.
2.3 Distributionally Double Equivariant Positional Graph Embeddings
InGram [ 5] is an existing work capable of performing our fully inductive link prediction task (Defini-
tion 2.1), but it does so with what we now define as distributionally double equivariant positional
embeddings , which are permutation sensitive, as we will show in Section 3.2:
Definition 2.7 (Distributionally double equivariant positional embeddings ).For any knowledge
graphA∈Awith number of nodes and relations N, R , the distributionally double equivariant
positional embeddings of Aare defined as joint samples of random variables Z|A∼p(Z|A), where
the tensor Zis defined as Zi,k,j∈Rd, d≥1,∀(i, k, j )∈[N]×[R]×[N], where we say p(Z|A)
is a double equivariant probability distribution on Adefined as ∀ϕ∈SN,∀τ∈SR, p(Z|A) =
p(ϕ◦τ◦Z|ϕ◦τ◦A).
Prior work on (standard) link prediction tasks has shown the advantages of equivariant representa-
tions over positional embeddings [ 22]. Others have established the equivalence between positional
embeddings and structural representations for simple graphs by proving that representations based
on an expectation of the positional embeddings are equivariant to node permutations [ 21]. In what
follows, we extend this result to the double equivariant setting:
Theorem 2.8 (From distributional double equivariant positional embeddings to double equiv-
ariant representations ).For any knowledge graph A∈A, the average Ep(Z|A)[Z|A]is a double
equivariant knowledge graph representation (Definition 2.5) for any distributional double equivariant
positional embeddings Z|A(Definition 2.7).
Intuitively, a distributionally double equivariant model will produce different embeddings for triplet
(Red Bull Racing, racing driver, Sergio Pérez) and (Google, employee, Jeff Dean) in Figure 1(a).
However, if the model is run multiple times (say each time with a different random seed), then
the averaged embeddings for the two triplets averaged across runs will be very similar. Later
in Section 3.2, we use the result in Theorem 2.8 to introduce DEq-InGram, a double equivariant
representation that builds upon InGram’s distributionally double equivariant positional embeddings
(Definition 2.7) that is shown to significantly outperforms the original InGram in Section 5.
3 Double Equivariance to Guide Model Design
Ultra [ 6] and InGram [ 5] are two recently proposed methods designed to address the fully inductive
link prediction task. In this section, we demonstrate that both methods can be understood within
4Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
the framework of double equivariance: Ultra generates double equivariant structural representations,
while InGram produces distributionally double equivariant positional embeddings. Leveraging insight
from Theorem 2.8, we present a straightforward enhancement to InGram, which we call DEq-InGram.
Next, we describe ISDEA+, a general framework for converting any equivariant GNNs designed for
homogeneous graphs into double equivariant models.
3.1 Ultra Generates Double Equivariant Structural Representations
Ultra was proposed by Galkin et al. [6]to address the fully inductive link prediction task. It builds
upon NBFNet [ 10], a path-based neural model originally designed for link prediction in homogeneous
graphs. The key architectural innovation of Ultra is the construction of an unweighted, directed
relation graph Grel. In this relation graph, nodes represent the relations Rof the original graph G,
and directed edge capture interaction between these relations with four distinct types: head-to-head,
head-to-tail, tail-to-head, and tail-to-tail. Specifically, for two relations r1andr2inR, if there exists
a node vinVthat serves as the head for both r1andr2in the original graph G, then r1andr2are
connected by a head-to-head edge in Grel. The same logic applies to the other three types of edges,
effectively capturing various ways relations interact within the knowledge graph.
During the forward propagation, Ultra first runs an instance of NBFNet on the relation graph Grel.
The resulting node representations from Grelcorrespond to representations of the relations of Rand
are used as edge representations in the original graph G. Subsequently, a second instance of NBFNet
is applied to produce the final node representation in G.
Our main result concerning Ultra is that it operates as a double equivariant graph representation
model(Definition 2.5). The key intuition is that the relation graph Grelis constructed to be invariant
to the permutation of relation identities. Since NBFNet itself is an equivariant GNN, the resulting
relation representations are equivariant to permutations of relations. Consequently, the final node rep-
resentations generated by the second NBFNet instance are invariant under the combined permutations
of nodes and relations. For a detailed proof, please refer to Appendix C.
Lemma 3.1. The triplet representations generated by Ultra [ 6] are double equivariant structural
representations (Definition 2.5).
3.2 InGram Generates Distributionally Double Equivariant Positional Embeddings
InGram, introduced by Lee et al. [5], is another model designed to address the fully inductive link
prediction task. Like Ultra [ 6], InGram constructs a relation graph to capture the interaction between
relations. However, unlike Ultra, InGram’s relation graph is undirected but weighted, with edge
weights computed heuristically to represent the affinity between relations. Intuitively, a high affinity
between two relations indicates that they frequently co-occur in the original graph G.
In the forward propagation, Ingram utilizes a GATv2 model [ 23] on the relation graph to generate
relation embeddings. These relation embeddings are then input into a second GATv2 instance
operating on the original graph to produce node embeddings. Finally, InGram applies a variant of
DistMult [24] to compute triplet scores using the generated node and relation embeddings.
Another key difference from Ultra, as we demonstrate in the following lemma, is that the embeddings
produced by InGram are sensitive to permutations of node and relation identities. This sensitivity
arises because InGram relies on initial node and relation embeddings before the first neural network
layer, which are initialized via Glorot initialization [ 25] and re-initialized at the start of each training
epoch and during inference. Since random initialization is inherently sensitive to permutations
of identities, the resulting embeddings from InGram are not permutation-equivariant. However,
due to the model’s architectural design, we show that InGram’s embeddings can be considered as
distributionally double equivariant positional embeddings (Definition 2.7), meaning they are double
equivariant in expectation . For a detailed proof, please refer to Appendix C.
Lemma 3.2. The triplet representations generated by InGram [ 5] output distributionally double
equivariant positional embeddings (Definition 2.7).
3.3 DEq-InGram: Enhancing Double Equivariance via Monte Carlo Sampling
Building on our insight from Lemma 3.2 that InGram [ 5]’s embeddings are double equivariant in
expectation , we propose a straightforward enhancement, which we name DEq-InGram. As we will
5Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
later demonstrate in Section 5, despite its simplicity, this modification significantly enhances the
model’s performance, showing the strength of our theoretical framework.
Specifically, our approach uses the insight from Theorem 2.8 by employing Monte
Carlo sampling to obtain an estimate of a double equivariant graph representation. Let
ZInGram ((i, k, j ),A(inf),V(0),R(0))denote InGram’s triplet score function, where A(inf)is the in-
ference knowledge graph, V(0)∈RN(inf)×drepresents an initial node embeddings of dimension d,
andR(0)∈RR(inf)×d′represents an initial relation embeddings of dimension d′. Our DEq-InGram
computes its embeddings as follows:
ΓDEq-InGram ((i, k, j ),A(inf)) =1
MMX
m=1ZInGram ((i, k, j ),A(inf),V(0)
m,R(0)
m) (1)
where {V(0)
m}M
m=1and{R(0)
m}M
m=1areMi.i.d. samples drawn from Glorot initialization [ 25].
Notably, this modification is applied at inference time and does not require retraining the InGram
model. Hence, one can use a pre-trained InGram checkpoint directly and achieve improved test
accuracy using our modification.
3.4 Inductive Structural Double Equivariant Architecture Plus (ISDEA+)
Finally, we propose ISDEA+, an alternative approach to Ultra [ 6] and InGram [ 5] which allows us to
convert any GNNs designed for homogeneous graphs to a double equivariant graph representation
model. Different from Ultra and InGram, we do not explicitly construct a relation graph. Rather,
we perform an equivariant set aggregation of node representations over the relations using the DSS
aggregation layer [ 26,27]. This guarantees that the resulting representations are equivariant to
permutations of relation identities.
Specifically, given a knowledge graph Gwith adjacency matrix A, letA(k)be the relation-induced
subgraph of Aconsisting of only edges with relation k∈ R. Then, Gcan be equivalently expressed
as a collection of relation-induced subgraphs, A= 
A(1), . . . , A(R)	 	
. Since the actions of the
two permutation groups SNandSRcommute, the double equivariance of A(Definition 2.4) can be
described as two (single) equivariance: A (graph) equivariance ϕ∈SNover each relation-induced
subgraph A(k), k= 1, . . . , R , and a (set) equivariance τ∈SR(over the set of subgraphs). Our
double equivariant ISDEA+ model then make use of DSS layer [ 26,27] to perform aggregation over
set of relations. Specifically, the ISDEA+ layer L:A→ ∪∞
N=1∪∞
R=2RN×R×N×dis defined as
follows. For each k= 1, ..., R :
(L(A)):,k=GNN 1
A(k)
+GNN 2
X
k′∈[R],k̸=k
A(k′)
, (2)
where GNN 1,GNN 2:A→ ∪∞
N=1RN×N×darearbitrary node-equivariant GNN layers that produce
pairwise representations [ 10,22,28]. Here, we abuse the notationPto denote any valid set
aggregation such as sum, mean, max, DepSets [ 29], etc.. The following lemma shows that ISDEA+
directly produces double equivariant representations. For a detailed proof, please refer to Appendix C.
Lemma 3.3. ΓISDEA+ in Equation (2)is a double invariant triplet representation as per Definition 2.3.
4 Related Work
A more comprehensive discussion of related work can be found in Appendix D.
Inductive link prediction over new nodes (but not new relations). Rule-induction methods [ 24,
30–32] are inherently node-independent which aim to extract First-order Logical Horn clauses from
the attributed multigraph. Recently, with the advancement of GNNs, various works [ 7–11] have
applied the idea of GNN in relational prediction to learn structural node/pairwise representation.
Although all these methods can be used to perform inductive link prediction over solely new nodes ,
they can not handle new relation types in test.
Inductive link prediction over both new nodes and new relations. Existing methods for querying
triplets involving both new nodes and new relations generally assume access to extra context,
such as generating language embedding for textual descriptions of unseen relation types [ 33–36],
6Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Table 1: Relation & Node Hits@10 performance on Fully Inductive Link Prediction over
PediaTypes. We report standard deviations over 5 runs. A higher value means better fully inductive
link prediction performance. Models labeled with DEq are double equivariant models, and those
labeled with d-DEq are distributionally double equivariant ones. The dataset name “ X-Y” means
training on graph Xand testing on graph Y. The best values are shown in bold font, while the
second-best values are underlined. The highest standard deviation within each task is highlighted in
red color. “Rand” row contains unbiased estimations of the performance from a random predictor.
Double equivariant models consistently achieve better results than non-double-equivariant
models with generally smaller standard deviations. N/A*: Not available due to constant crashes.
(a)Relation prediction (i,?, j)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00
GAT 18.58 ±00.52 18.93 ±00.33 19.40 ±00.28 18.87 ±00.19 18.78 ±00.28 18.76 ±00.33 19.78 ±01.39 19.15 ±00.35
GIN 19.34 ±00.32 19.34 ±00.29 18.98 ±00.27 18.88 ±00.47 19.30 ±00.52 18.86 ±00.35 18.69 ±00.75 18.92 ±00.68
GraphConv 19.18 ±00.27 19.02 ±00.64 19.19 ±00.24 18.93 ±00.60 19.46 ±00.38 19.13 ±00.54 19.13 ±01.24 18.89 ±00.57
NBFNet 21.93 ±02.53 22.20 ±02.92 18.98 ±02.75 7.01±01.43 23.51 ±07.06 23.05 ±03.55 31.50 ±04.82 35.17 ±05.13
RMPI 27.91 ±06.48 28.62 ±03.75 27.51 ±06.48 25.59 ±06.48 N/A* 16.76 ±04.03 39.03 ±20.28 11.77 ±07.07
InGram (d-DEq) 78.74 ±07.48 62.11 ±13.60 48.72 ±08.94 65.60 ±14.42 77.75 ±06.60 63.32 ±02.78 67.98 ±25.45 64.98 ±26.69
Ultra (DEq) 99.96 ±00.01 99.85 ±00.04 99.92 ±00.02 99.69 ±00.16 99.54 ±00.12 97.42 ±00.62 95.72 ±01.67 98.15 ±00.10
DEq-InGram (DEq) 87.94 ±05.68 80.47 ±09.90 68.89 ±05.45 80.79 ±10.51 91.47 ±01.53 77.03 ±04.09 77.72 ±21.92 89.30 ±05.53
ISDEA+ (DEq) 99.12 ±00.24 98.84 ±00.06 99.20 ±00.13 98.99 ±00.12 98.56 ±00.12 98.03 ±00.17 88.78 ±03.23 96.45 ±00.24
(b) Node prediction (i, k,?)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00 19.60 ±00.00
GAT 89.77 ±00.41 86.83 ±00.41 66.24 ±02.81 69.08 ±00.66 31.08 ±01.07 77.05 ±00.36 53.51 ±00.29 64.13 ±00.31
GIN 90.10 ±00.61 85.32 ±01.18 73.32 ±03.35 75.66 ±04.85 34.87 ±09.12 78.67 ±02.46 56.87 ±00.44 65.27 ±01.14
GraphConv 92.97 ±00.11 90.56 ±00.04 83.58 ±00.68 82.64 ±00.65 40.59 ±01.72 79.28 ±01.29 68.91 ±00.51 76.50 ±00.14
NBFNet 87.64 ±01.81 89.77 ±00.80 85.56 ±02.07 59.78 ±03.73 63.23 ±03.65 78.24 ±00.90 49.97 ±01.44 66.36 ±02.64
RMPI 89.59 ±06.61 81.79 ±02.17 82.93 ±03.56 81.38 ±06.19 N/A* 65.76 ±07.45 55.67 ±06.61 71.03 ±02.12
InGram (d-DEq) 92.32 ±01.00 83.71 ±03.53 90.82 ±01.84 92.15 ±00.90 61.44 ±09.84 87.60 ±01.21 54.79 ±08.81 67.84 ±06.38
Ultra (DEq) 98.67 ±00.05 98.22 ±00.10 98.34 ±00.07 97.00 ±00.12 92.00 ±00.22 95.92 ±00.13 77.22 ±01.97 85.48 ±01.15
DEq-InGram (DEq) 94.47 ±00.60 88.90 ±02.06 93.85 ±00.36 94.02 ±00.74 71.94 ±07.37 91.47 ±00.62 71.53 ±04.78 80.53 ±07.96
ISDEA+ (DEq) 95.39 ±00.30 81.57 ±03.17 97.66 ±00.19 95.03 ±00.44 86.60 ±00.59 90.93 ±00.24 69.62 ±01.10 73.16 ±00.82
a shared background graph connecting seen and unseen relations (e.g., test graph has training
relations [ 17,18,37]), or access to graph ontology [ 15]. Hence, these methods cannot be directly
applied to test graphs that neither contain meaningful descriptive information of the unseen relation
types (e.g., url links) nor connection with nodes and relation types seen in training.
To the best of our knowledge, InGram [ 5] and Ultra [ 6] are the only existing methods capable of
performing fully inductive link prediction without additional context data (just the test graph structure
is available during inference). InGram and Ultra introduce relation graphs to capture relational
representations based on their interactions. The connection between these methods and our work has
been described in Sections 2.3 and 3.2.
5 Experiments on How Simple Double Equivariant Models Can Perform
Fully Inductive Link Prediction
In this section, we aim to answer one question: Can the general blueprint proposed in this work
guide graph models to perform fully inductive link prediction over knowledge graphs accurately?
Specifically, we want to know Q1: Can the proposed ISDEA+ framework successfully transform
GNNs designed for homogeneous graphs into double equivariant models to perform fully inductive
link prediction? Q2: Can the proposed DEq-InGram outperforms InGram on fully inductive link
prediction in terms of both accuracy and robustness?
Dataset. To fully test the model’s capability for fully inductive link prediction, we create a new dataset
PediaTypes (details in Appendix E.1.2) by sampling from the OpenEA library [ 38]. The OpenEA
library [ 38] contains multiple knowledge graphs of relational databases from different domains
on similar topics, such as DBPedia [ 39] in different languages (English, French and German),
YAGO [ 40] and Wikidata [ 41]. PediaTypes includes pairs of knowledge graphs such as English-to-
French DBPedia (denoted as EN-FR), DBPedia-to-YAGO (denoted as DB-YG), etc.. In each graph,
7Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
triplets are randomly divided into 80% training, 10% validation, and 10% test. We then train and
validate the model on one of the graphs (e.g., EN) and directly apply it to another graph (e.g., DE),
which has completely new nodes and new relation types.
Baselines. To the best of our knowledge, InGram [ 5] and Ultra [ 6] are the only two works capable
of performing fully inductive link prediction without needing significant modification to the model.
We also run RMPI [ 15], which is capable of performing fully inductive link prediction but requires
extra context at test time. In addition, we consider a variant of NBFNet [ 10] in which we modify
its architecture following the approach in [ 5] to enable it on fully inductive link prediction. We also
compare our models with message-passing GNNs, including GAT [ 42], GIN [ 20], GraphConv [ 43],
which treats the graph as a homogeneous graph by ignoring the relation types. For fair comparisons,
we add distance features as in Equation (4) to increase the expressiveness of these GNNs. Additional
baseline details are in Appendix E.
Evaluation. We report the Hits@10 performances over 5 runs of different random seeds for all
models on both the relation prediction task of (i,?, j)and the more traditional node prediction task
of(i, k,?). For each task, we sample 50 negative triplets for each ground-truth positive target triplet
during test evaluation by corrupting the relation type or the tail node respectively. Further experiment
details on baseline implementations, ablation studies, and other metrics (e.g., MRR, Hits@1) can be
found in Appendix E.
Experiment results. Table 1a shows the results on the relation prediction task, and Table 1b shows
the node prediction task on PediaTypes. In all scenarios across both tasks, our model ISDEA+ obtains
competitive performance matching the best model, Ultra. The success of ISDEA+, despite its simplic-
ity, validates the effectiveness of our general blueprint in transforming arbitrary homogeneous GNNs
into double equivariant models. Furthermore, DEq-InGram, with its straightforward modification
guided by the theory of double equivariance, significantly outperforms the original InGram across
all datasets. This directly corroborates our theoretical predictions in Section 2 that a model directly
producing double equivariant representations is more stable than positional embeddings, which are
only double equivariant in expectation.
6 Experiment on Negative Transfers in Meta-Learning
As we have empirically demonstrated the effectiveness of the double equivariance framework in Sec-
tion 5, we now turn to a crucial question: are existing double equivariant models ready to be
knowledge graph foundation models [ 6,12]?Foundation models, as coined by [ 44], are models pre-
trained on massive-scale data and are capable of adaption to a wide range of downstream applications.
A fundamental requirement of such models is the ability to effectively learn from pre-training data
spanning multiple domains, while overcoming the challenge of negative transfer [ 13,14]. Negative
transfer occurs when the inclusion of data from certain domains in the training set somehow impairs
the model’s transfer learning performance. Another critical requirement is adherence to strong data
scaling law [ 45–47], meaning that the model’s performance should consistently improve as the
training data scale increases. Hence, we investigate the following questions Q1: Are there specific
knowledge graph that consistently cause negative transfer in existing double equivariance models?
Q2: How well do existing double equivariant models conform to the data scaling law?
Dataset and experiment setup. To this end, we introduce another dataset WikiTopics , and design
a meta-learning experiment. WikiTopics is constructed from WikiData-5M [ 48] by grouping the
relations into 11 different topics, or domains, such as infrastructure, science, sport, locations, etc..
Each group of relations induces a knowledge graph containing only the relations from that group,
allowing each knowledge graph to be viewed as representing a specific domain. For the experiment,
we select a subset of these knowledge graphs for training, while holding out others for testing.
Specifically, we train multiple model instances with an increasing number of knowledge graphs
mixed into the training process to investigate whether the models adhere to data scaling laws.
Additionally, we experiment with different combinations of training knowledge graphs to determine if
a certain knowledge graph would consistently trigger negative transfer. Appendix E contains further
dataset and experiment details.
Experiment results. Table 2 presents the main findings of our experiment. Here we trained the
models on a progressively increasing combination of training KGs from the domains of Infrastructure,
8Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Table 2: Average training and zero-shot Hits@1 performance (in %) of Ultra, DEq-InGram, and
ISDEA+ on a meta-learning scenario in WikiTopics. (1) Locis one KG that we identified to consis-
tently trigger negative transfer effects among all models. Mixing Locin training data consistently
diminishes both the training and test performance across all double equivariant models. We
show in parenthesis the relative performance difference comparing the training combination with Loc
and a similar combination without Loc. (2) All models exhibit limited performance improvement
as the number of training KG domains increases.
(a)Relation prediction (i,?, j)
Meta-Learning Training KGs Neg. Transfer?Training Hits@1 Average Zero-shot Hits@1
Ultra DEq-InGram ISDEA+ Ultra DEq-InGram ISDEA+
Infra 91.82 73.94 75.15 77.22 62.07 80.70
Loc ✓ 28.31 (−69.17%) 18.53 (−74.94%) 81.10 (+7.92%) 13.06 (−83.09%) 19.64 (−68.36%) 85.16 (+5.53%)
Infra + Sci 92.79 58.44 83.98 80.07 60.19 85.87
Infra + Loc ✓ 82.34 (−11.26%) 59.64 (+2.05%) 73.17 (−12.87%) 48.69 (−39.19%) 47.36 (−21.32%) 74.27 (−13.42%)
Infra + Sci + Sport 92.03 87.03 86.62 79.28 67.48 80.88
Infra + Sci + Loc ✓ 83.65 (−9.11%) 36.56 (−57.99%) 84.21 (−2.78%) 22.42 (−71.72%) 12.28 (−81.80%) 75.47 (−6.69%)
Infra + Sci + Sport + Tax 91.50 57.98 95.63 76.67 30.25 80.70
Infra + Sci + Sport + Loc ✓ 91.84 (+0.37%) 31.81 (−45.14%) 91.72 (−4.08%) 69.89 (−8.84%) 20.25 (−33.06%) 79.53 (−1.44%
(b)Node prediction (i, r,?)
Meta-Learning Training KGs Neg. Transfer?Training Hits@1 Average Zero-shot Hits@1
Ultra DEq-InGram ISDEA+ Ultra DEq-InGram ISDEA+
Infra 89.93 82.65 73.53 67.19 47.22 39.79
Loc ✓ 36.92 (−58.95%) 84.93 (+2.76%) 42.31 (−42.46%) 43.04 (−35.94%) 38.47 (−18.53%) 27.15 (−31.77%)
Infra + Sci 87.44 79.80 81.73 67.49 47.67 41.92
Infra + Loc ✓ 64.19 (−26.59%) 88.35 (+10.71%) 59.60 (−27.08%) 62.98 (−36.23%) 42.18 (−11.52%) 38.50 (−8.16%)
Infra + Sci + Sport 68.40 66.82 66.23 66.97 53.92 41.34
Infra + Sci + Loc ✓ 64.09 (−6.30%) 87.14 (+30.41%) 66.32 (+0.14%) 63.08 (−5.81%) 41.20 (−23.59%) 37.63 (−8.97%)
Infra + Sci + Sport + Tax 70.26 43.87 71.51 66.90 34.16 42.45
Infra + Sci + Sport + Loc ✓ 68.71 (−2.21%) 74.28 (+69.32%) 64.55 (−9.73%) 65.88 (−1.52%) 44.33 (+29.77%) 41.34 (−2.61%)
Science, Sport, Taxonomies, and Locations (abbreviated as Infra, Sci, Sport, Tax, & Loc respectively).
The models were then evaluated on four held-out test domains: art, award, education, and health care,
with results averaged across these test domains. Each model was trained for 3 different random seeds
for each setting, and we report the Hits@1 performance for both node prediction (i, k,?)and relation
prediction (i,?, j)tasks, using 50 negative samples. Additionally, we report the models’ accuracy on
both the training graphs and the inference graphs.
We first note that the knowledge graph from the domain Loc is indeed one KG that consistently
triggers negative transfer across all models. Specifically, whenever Loc is included in the training
data mix (e.g. Infra + Sci + Loc), the models demonstrate worse zero-shot test performance compared
to training with other combinations of KGs with the same number of domains (e.g. Infra + Sci +
Sport). Additionally, in most cases, the models also exhibit a drop in training accuracy when Loc is
included. This suggests that the Loc KG may contain patterns that conflict with those in other KGs,
causing models with limited expressivity to struggle with jointly learning these conflicting patterns.
Therefore, in response to Q1, all evaluated models are indeed susceptible to negative transfer, with
Loc serving as a concrete example.
Our second observation concerns the data scaling law. We find that no model consistently exhibits
improved zero-shot test performance as the number of training KG domains increases, whether the
Loc KG, which causes negative transfer, is included in the training mix or not. Notably, DEq-InGram
unexpectedly shows a sharp decline in performance when the number of training domains increases
from 3 to 4, while Ultra and ISDEA+ generally display saturated accuracy. Therefore, in response to
Q2, our experiments indicate that all existing double equivariant models have limited data scaling
capabilities. Addressing this limitation remains an important direction for future research.
7 Conclusion
This work formally introduced the concept of double equivariant structural representations and
distributionally double equivariant positional embedding as a unifying theoretical framework for fully
inductive link prediction in knowledge graphs. We demonstrated that this framework underpins the
9Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
effectiveness of existing models capable of such a task, and provided a solid blueprint for designing
future models. Our empirical studies showed that the proposed framework significantly enhances
model performance, as seen with DEq-InGram, and offers a systematic approach to constructing
double equivariant models, such as with ISDEA+.
Despite these advancements, our experiments also highlighted that the current double equivariant
models are hindered by negative transfer effects and poor data scaling behavior when jointly learning
from data over multiple domains in a meta-learning setting. Our proposed benchmark and the
experiment results thus presents opportunities for future work to address these gaps to advance toward
realizing true knowledge graph foundation models.
References
[1]Payal Chandak, Kexin Huang, and Marinka Zitnik. Building a knowledge graph to enable
precision medicine. Scientific Data , 10(1):67, 2023. 1
[2]Vineeth Venugopal, Sumit Pai, and Elsa Olivetti. The largest knowledge graph in materials
science-entities, relations, and link prediction through graph representation learning. In AI for
Accelerated Materials Design NeurIPS 2022 Workshop , 2022. 1
[3]Michael J Statt, Brian A Rohr, Dan Guevarra, Santosh K Suram, John M Gregoire, et al. The
materials experiment knowledge graph. Digital Discovery , 2(4):909–914, 2023. 1
[4]Xin Luna Dong. Challenges and innovations in building a product knowledge graph. In
Proceedings of the 24th ACM SIGKDD International conference on knowledge discovery &
data mining , pages 2869–2869, 2018. 1
[5]Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang. InGram: Inductive knowledge
graph embedding via relation graphs. In Proceedings of the 40th International Conference on
Machine Learning , pages 18796–18809, 2023. 1, 2, 4, 5, 6, 7, 8, 21, 24, 26, 33, 34
[6]Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. Towards
foundation models for knowledge graph reasoning. arXiv preprint arXiv:2310.04562 , 2023. 1,
2, 4, 5, 6, 7, 8, 20, 24, 29, 32
[7]Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and
Max Welling. Modeling relational data with graph convolutional networks. In European
semantic web conference , pages 593–607. Springer, 2018. 1, 6, 17, 23
[8]Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph
reasoning. In International Conference on Machine Learning , pages 9448–9457. PMLR, 2020.
17, 18, 23, 25, 26
[9]Mikhail Galkin, Etienne Denis, Jiapeng Wu, and William L Hamilton. Nodepiece: Compo-
sitional and parameter-efficient representations of large knowledge graphs. In International
Conference on Learning Representations , 2021. 17, 23
[10] Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford
networks: A general graph neural network framework for link prediction. Advances in Neural
Information Processing Systems , 34:29476–29490, 2021. 5, 6, 8, 17, 20, 23
[11] Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, and
Sebastian Riedel. Refactor gnns: Revisiting factorisation-based models from a message-
passing perspective. In Advances in Neural Information Processing Systems , 2022. 1, 6,
23
[12] Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah,
Michael Galkin, and Jiliang Tang. Graph foundation models. arXiv preprint arXiv:2402.02216 ,
2024. 1, 2, 8
[13] Wen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu. A survey on negative transfer.
IEEE/CAA Journal of Automatica Sinica , 10(2):305–329, 2022. 2, 8
[14] Zirui Wang. Mitigating negative transfer for better generalization and efficiency in transfer
learning . PhD thesis, Carnegie Mellon University, 2023. 2, 8
[15] Yuxia Geng, Jiaoyan Chen, Jeff Z Pan, Mingyang Chen, Song Jiang, Wen Zhang, and Huajun
Chen. Relational message passing for fully inductive knowledge graph completion. In 2023
10Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
IEEE 39th International Conference on Data Engineering (ICDE) , pages 1221–1233. IEEE,
2023. 3, 7, 8, 23, 24, 26
[16] Ming Zhao, Weijia Jia, and Yusheng Huang. Attention-based aggregation graph networks for
knowledge graph information transfer. In Advances in Knowledge Discovery and Data Mining:
24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part
II 24 , pages 542–554. Springer, 2020. 23, 24
[17] Mingyang Chen, Wen Zhang, Zhen Yao, Xiangnan Chen, Mengxiao Ding, Fei Huang, and
Huajun Chen. Meta-learning based knowledge extrapolation for knowledge graphs in the
federated setting. In Lud De Raedt, editor, Proceedings of the Thirty-First International
Joint Conference on Artificial Intelligence, IJCAI-22 , pages 1966–1972. International Joint
Conferences on Artificial Intelligence Organization, 7 2022. Main Track. 7, 24
[18] Qian Huang, Hongyu Ren, and Jure Leskovec. Few-shot relational reasoning via connection
subgraph pretraining. In Neural Information Processing Systems , 2022. 7, 23, 24
[19] Jiarui Jin, Yangkun Wang, Kounianhua Du, Weinan Zhang, Zheng Zhang, David Wipf, Yong
Yu, and Quan Gan. Inductive relation prediction using analogy subgraph embeddings. In
International Conference on Learning Representations , 2022. 3, 24
[20] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations , 2019. 4, 8, 17, 18, 23
[21] Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional
node embeddings and structural graph representations. In Eighth International Conference on
Learning Representations , 2020. 4, 17, 23, 28
[22] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory
of using graph neural networks for multi-node representation learning. Advances in Neural
Information Processing Systems , 34:9061–9073, 2021. 4, 6, 23
[23] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv
preprint arXiv:2105.14491 , 2021. 5, 21
[24] Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding
entities and relations for learning and inference in knowledge bases. In Proceedings of the
International Conference on Learning Representations (ICLR) 2015 , 2015. 5, 6, 17, 21, 23
[25] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward
neural networks. In Proceedings of the thirteenth international conference on artificial
intelligence and statistics , pages 249–256. JMLR Workshop and Conference Proceedings,
2010. 5, 6, 21
[26] Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric
elements. In International Conference on Machine Learning , pages 6734–6744. PMLR, 2020.
6
[27] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. Signature
verification using a" siamese" time delay neural network. Advances in neural information
processing systems , 6, 1993. 6
[28] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in
neural information processing systems , 31, 2018. 6, 23
[29] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in neural information processing systems ,
pages 3391–3401, 2017. 6
[30] Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for
knowledge base reasoning. Advances in neural information processing systems , 30, 2017. 6,
18, 23
[31] Christian Meilicke, Manuel Fink, Yanjie Wang, Daniel Ruffinelli, Rainer Gemulla, and Heiner
Stuckenschmidt. Fine-grained evaluation of rule-and embedding-based systems for knowledge
graph completion. In International semantic web conference , pages 3–20. Springer, 2018.
[32] Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum:
End-to-end differentiable rule mining on knowledge graphs. Advances in Neural Information
Processing Systems , 32, 2019. 6, 18, 23
11Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
[33] Pengda Qin, Xin Wang, Wenhu Chen, Chunyun Zhang, Weiran Xu, and William Yang Wang.
Generative adversarial zero-shot relational learning for knowledge graphs. In Proceedings of
the AAAI Conference on Artificial Intelligence , volume 34, pages 8673–8680, 2020. 6, 23, 24
[34] Yuxia Geng, Jiaoyan Chen, Zhuo Chen, Jeff Z. Pan, Zhiquan Ye, Zonggang Yuan, Yantao Jia,
and Huajun Chen. Ontozsl: Ontology-enhanced zero-shot learning. In Jure Leskovec, Marko
Grobelnik, Marc Najork, Jie Tang, and Leila Zia, editors, WWW ’21: The Web Conference
2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021 , pages 3325–3336. ACM / IW3C2,
2021. doi: 10.1145/3442381.3450042. 23, 24
[35] Hanwen Zha, Zhiyu Chen, and Xifeng Yan. Inductive relation prediction by bert. Proceedings
of the AAAI Conference on Artificial Intelligence , 36:5923–5931, Jun. 2022. doi: 10.1609/aaai.
v36i5.20537.
[36] Hongwei Wang, Hongyu Ren, and Jure Leskovec. Relational message passing for knowledge
graph completion. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining , pages 1697–1707, 2021. 6, 23
[37] Jiajun Chen, Huarui He, Feng Wu, and Jie Wang. Topology-aware correlations between
relations for inductive link prediction in knowledge graphs. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 35, pages 6271–6278, 2021. 7, 24
[38] Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami,
and Chengkai Li. A benchmarking study of embedding-based entity alignment for knowledge
graphs. Proceedings of the VLDB Endowment , 13(11):2326–2340, 2020. 7, 26
[39] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, et al. Dbpedia–a large-
scale, multilingual knowledge base extracted from wikipedia. Semantic web , 6(2):167–195,
2015. 7
[40] Thomas Rebele, Fabian Suchanek, Johannes Hoffart, Joanna Biega, Erdal Kuzey, and Gerhard
Weikum. Yago: A multilingual knowledge base from wikipedia, wordnet, and geonames. In
The Semantic Web–ISWC 2016: 15th International Semantic Web Conference, Kobe, Japan,
October 17–21, 2016, Proceedings, Part II 15 , pages 177–185. Springer, 2016. 7
[41] Denny Vrande ˇci´c and Markus Krötzsch. Wikidata: a free collaborative knowledgebase.
Communications of the ACM , 57(10):78–85, 2014. 7
[42] Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and
Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903 , 2017. 8, 17, 18,
21, 23
[43] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen,
Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural
networks. Proceedings of the AAAI Conference on Artificial Intelligence , 33(01):4602–4609,
Jul. 2019. 8, 17, 18
[44] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von
Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the
opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021. 8
[45] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural
language models. arXiv preprint arXiv:2001.08361 , 2020. 8
[46] Dingshuo Chen, Yanqiao Zhu, Jieyu Zhang, Yuanqi Du, Zhixun Li, Qiang Liu, Shu Wu, and
Liang Wang. Uncovering neural scaling laws in molecular representation learning. Advances
in Neural Information Processing Systems , 36, 2024.
[47] Qian Huang, Hongyu Ren, Peng Chen, Gregor Kržmanc, Daniel Zeng, Percy S Liang, and Jure
Leskovec. Prodigy: Enabling in-context learning over graphs. Advances in Neural Information
Processing Systems , 36, 2024. 8
[48] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and
Jian Tang. Kepler: A unified model for knowledge embedding and pre-trained language
representation. Transactions of the Association for Computational Linguistics , 9:176–194,
2021. 8, 28
12Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
[49] Michael Norman, Vince Kellen, Shava Smallen, Brian DeMeulle, Shawn Strande, Ed La-
zowska, Naomi Alterman, Rob Fatland, Sarah Stone, Amanda Tan, Katherine Yelick, Eric
Van Dusen, and James Mitchell. Cloudbank: Managed services to simplify cloud ac-
cess for computer science research and education. In Practice and Experience in Ad-
vanced Research Computing , PEARC ’21, New York, NY , USA, 2021. Association for
Computing Machinery. ISBN 9781450382922. doi: 10.1145/3437359.3465586. URL
https://doi.org/10.1145/3437359.3465586 .
[50] Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In
International conference on machine learning , pages 7134–7143. PMLR, 2019. 17
[51] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In International Conference on Learning Representations , 2017. 17, 23, 24, 25
[52] Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Benjamin Chamberlain, Michael Bronstein,
and Federico Monti. Sign: Scalable inception graph neural networks. In ICML 2020 Workshop
on Graph Representation Learning and Beyond , 2020. 17
[53] Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. Neural-symbolic models
for logical queries on knowledge graphs. In International Conference on Machine Learning ,
pages 27454–27478. PMLR, 2022. 18
[54] Haiquan Qiu, Yongqi Zhang, Yong Li, and Quanming Yao. Logical expressiveness of graph
neural network for knowledge graph reasoning. arXiv preprint arXiv:2303.12306 , 2023. 18,
23
[55] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
Neural message passing for quantum chemistry. In Proceedings of the 34th International
Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research ,
pages 1263–1272. PMLR, 2017. 21
[56] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The sur-
prising power of graph neural networks with random node initialization. arXiv preprint
arXiv:2010.01179 , 2020. 21
[57] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural
networks. In Proceedings of the 2021 SIAM international conference on data mining (SDM) ,
pages 333–341. SIAM, 2021.
[58] Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational
pooling for graph representations. In International Conference on Machine Learning , pages
4663–4673. PMLR, 2019. 21, 23
[59] Ilya Sutskever, Joshua Tenenbaum, and Russ R Salakhutdinov. Modelling relational data using
bayesian clustered tensor factorization. Advances in neural information processing systems ,
22, 2009. 23
[60] Maximilian Nickel, V olker Tresp, and Hans-Peter Kriegel. A three-way model for collective
learning on multi-relational data. In Icml, 2011.
[61] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana
Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural
information processing systems , 26, 2013.
[62] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In Proceedings of the AAAI conference on artificial intelligence ,
volume 28, 2014.
[63] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard.
Complex embeddings for simple link prediction. In International conference on machine
learning , pages 2071–2080. PMLR, 2016.
[64] Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of
knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 30,
2016.
[65] T Trouillon, CR Dance, E Gaussier, J Welbl, S Riedel, and G Bouchard. Knowledge graph
completion via complex tensor factorization. Journal of Machine Learning Research , 18(130):
1–38, 2017.
13Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
[66] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d
knowledge graph embeddings. In Proceedings of the AAAI conference on artificial intelligence ,
volume 32, 2018.
[67] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph
embedding by relational rotation in complex space. In International Conference on Learning
Representations , 2019. 23
[68] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. Advances in neural information processing
systems , 29, 2016. 23, 24
[69] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems , 30, 2017.
[70] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine , 34
(4):18–42, 2017. 23
[71] Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha Talukdar. Composition-based
multi-relational graph convolutional networks. In International Conference on Learning
Representations , 2019. 23
[72] Daniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla. You can teach an old dog new
tricks! on training knowledge graph embeddings. In International Conference on Learning
Representations , 2020. 23
[73] Pablo Barcelo, Mikhail Galkin, Christopher Morris, and Miguel Romero Orth. Weisfeiler and
leman go relational. arXiv preprint arXiv:2211.17113 , 2022. 23
[74] Pablo Barceló, Egor Kostylev, Mikael Monet, Jorge Pérez, Juan Reutter, and Juan-Pablo Silva.
The logical expressiveness of graph neural networks. In 8th International Conference on
Learning Representations (ICLR 2020) , 2020. 23
[75] Ni Lao and William W Cohen. Relational retrieval using a combination of path-constrained
random walks. Machine learning , 81(1):53–67, 2010. 23
[76] Luis Antonio Galárraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek. Amie: associ-
ation rule mining under incomplete evidence in ontological knowledge bases. In Proceedings
of the 22nd international conference on World Wide Web , pages 413–422, 2013. 23
[77] Kewei Cheng, Jiahao Liu, Wei Wang, and Yizhou Sun. Rlogic: Recursive logical rule learning
from knowledge graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining , pages 179–189, 2022. 23
[78] Mikhail Galkin, Zhaocheng Zhu, Hongyu Ren, and Jian Tang. Inductive logical query
answering in knowledge graphs. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information Processing Systems , 2022. 23
[79] Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. One-shot
relational learning for knowledge graphs. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing , pages 1980–1990, 2018. 23, 24
[80] Xin Lv, Yuxian Gu, Xu Han, Lei Hou, Juanzi Li, and Zhiyuan Liu. Adapting meta knowledge
graph information for multi-hop reasoning over few-shot relations. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3376–3381,
2019. 24
[81] Mingyang Chen, Wen Zhang, Yuxia Geng, Zezhong Xu, Jeff Z Pan, and Huajun Chen.
Generalizing to unseen elements: A survey on knowledge extrapolation for knowledge graphs.
arXiv preprint arXiv:2302.01859 , 2023. 23
[82] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. In International Conference on Learning Representations (ICLR) , 2015.
24
[83] Mingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen, and Huajun Chen. Meta relational
learning for few-shot link prediction in knowledge graphs. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
14Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 4217–4226,
2019. 24
[84] Chuxu Zhang, Huaxiu Yao, Chao Huang, Meng Jiang, Zhenhui Li, and Nitesh V Chawla.
Few-shot knowledge graph completion. In Proceedings of the AAAI conference on artificial
intelligence , volume 34, pages 3041–3048, 2020. 24
[85] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast
adaptation of deep networks. In International conference on machine learning , pages 1126–
1135. PMLR, 2017. 24
[86] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and
locally connected networks on graphs. CoRR , abs/1312.6203, 2013. URL https://api.
semanticscholar.org/CorpusID:17682909 . 24
[87] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole V on Lilienfeld.
Quantum chemistry structures and properties of 134 kilo molecules. Scientific data , 1(1):1–7,
2014. 25
[88] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural
relational inference for interacting systems. In International conference on machine learning ,
pages 2688–2697. PMLR, 2018.
[89] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung.
Revisiting point cloud classification: A new benchmark dataset and classification model on
real-world data. In Proceedings of the IEEE/CVF international conference on computer vision ,
pages 1588–1597, 2019.
[90] Jiaqi Han, Yu Rong, Tingyang Xu, and Wenbing Huang. Geometrically equivariant
graph neural networks: A survey. ArXiv , abs/2202.07230, 2022. URL https://api.
semanticscholar.org/CorpusID:246863599 .
[91] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural
networks. In International conference on machine learning , pages 9323–9332. PMLR, 2021.
25
[92] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-
training molecular graph representation with 3d geometry. arXiv preprint arXiv:2110.07728 ,
2021. 25
[93] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei,
Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learn-
ing framework. In International Conference on Learning Representations , 2023. URL
https://api.semanticscholar.org/CorpusID:259298651 . 25
[94] Nathaniel Thomas, Tess E. Smidt, Steven M. Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and
Patrick F. Riley. Tensor field networks: Rotation- and translation-equivariant neural networks
for 3d point clouds. ArXiv , abs/1802.08219, 2018. URL https://api.semanticscholar.
org/CorpusID:3457605 . 25
[95] Fabian Fuchs, Daniel Worrall, V olker Fischer, and Max Welling. Se (3)-transformers: 3d
roto-translation equivariant attention networks. Advances in neural information processing
systems , 33:1970–1981, 2020.
[96] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing
convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In
International Conference on Machine Learning , pages 3165–3176. PMLR, 2020.
[97] Jonas Köhler, Leon Klein, and Frank Noé. Equivariant flows: exact likelihood generative
learning for symmetric densities. In International conference on machine learning , pages
5361–5370. PMLR, 2020. 25
[98] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations
for graph classification extrapolations. In International Conference on Machine Learning ,
pages 837–851. PMLR, 2021. 25
[99] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang
Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-
distribution generalization on graphs. Advances in Neural Information Processing Systems , 35:
22131–22148, 2022. 25
15Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
[100] Tong Zhao, Gang Liu, Daheng Wang, Wenhao Yu, and Meng Jiang. Learning from counter-
factual links for link prediction. In International Conference on Machine Learning , pages
26911–26926. PMLR, 2022. 25
[101] Judea Pearl. Causality . Cambridge university press, 2009. 25
[102] Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD
’06, page 631–636, New York, NY , USA, 2006. Association for Computing Machinery. ISBN
1595933395. doi: 10.1145/1150402.1150479. 29
[103] Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. Little Ball of Fur: A Python Library for
Graph Sampling. In Proceedings of the 29th ACM International Conference on Information
and Knowledge Management (CIKM ’20) , page 3133–3140. ACM, 2020. 29
[104] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and
text inference. In Proceedings of the 3rd workshop on continuous vector space models and
their compositionality , pages 57–66, 2015. 29, 31
[105] Wenhan Xiong, Thi-Lan-Giao Hoang, and William Yang Wang. Deeppath: A reinforcement
learning method for knowledge graph reasoning. In Conference on Empirical Methods in Nat-
ural Language Processing , 2017. URL https://api.semanticscholar.org/CorpusID:
20667722 . 29, 31
[106] Farzaneh Mahdisoltani, Joanna Asia Biega, and Fabian M. Suchanek. Yago3: A knowledge
base from multilingual wikipedias. In Conference on Innovative Data Systems Research , 2015.
URL https://api.semanticscholar.org/CorpusID:6611164 . 29, 31
16Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
A Detailed Model Design for ISDEA+
Here we provide further details on the architectural design of ISDEA+.
A.1 Implementation Details of ISDEA+
We use GNN layers for constructing L1, L2. Since most-expressive pairwise representations are
computationally expensive, we trade-off expressivity in the implementation of Equation (2) for speed
and memory by using node representation GNN layers [ 20,42,43]. Specifically, for a knowledge
graphAwith number of nodes and relations N, R , at each iteration t= 1, ..., T , for each relation type
k∈[R], all nodes i∈[N]are associated with two learned vectors h(t)
i,k∈Rdt, h(t)
i,¬k∈Rdt, dt≥1.
If there are no node attributes, we initialize ∀k∈[R], h(0)
i,k=h(0)
i,¬k= 1. Then we recursively
compute the update, ∀i∈[N],∀k∈[R],
h(t+1)
i,k=GNN(t)
1
h(t)
i,k, 
h(t)
j,kj∈Nk(i)	 	
, h(t+1)
i,¬k=GNN(t)
2
h(t)
i,¬k, 
h(t)
j,¬kj∈S
k′̸=kNk(i)	 	
, ift=0,
h(t+1)
i,k=GNN(t)
1
h(t)
i,k, 
h(t)
j,kj∈S
k′∈[R(∗)]Nk′(i)	 	
,h(t+1)
i,¬k=GNN(t)
2
h(t)
i,¬k, 
h(t)
j,¬kj∈S
k′∈[R(∗)]Nk′(i)	 	
,ift>0,
where GNN(t)
1andGNN(t)
2denote two GNN layers and Nk(i):={j|Aj,k,i= 1}denotes the
neighborhood set of node iwith relation kin the unattributed graph A(k). To get the final repre-
sentation Xi,kfor the node iwith respect to relation k. We define hi,k=h(0)
i,kh(1)
i,k···h(T)
i,k,
hi,¬k=h(0)
i,¬kh(1)
i,¬k···h(T)
i,¬k, and combine the two embeddings as illustrated in Equation (2),
Xi,k=MLP 1(hi,k) +MLP 2(hi,¬k),∀i∈[N],∀k∈[R], (3)
where MLP 1,MLP 2are two multi-layer perceptrons, ∥as the concatenation operation.
As shown by previous studies, structural node representations are not most expressive for link
prediction in unattributed graphs [ 21,50]. Hence, we concatenate iandj(double equivariant)
node representations with the shortest distance between iandjin the observed graph as our triplet
representations (appending distances is also adopted in the representations of prior work [ 8,9]).
Finally, we obtain the triplet representation,
ΓISDEA+ ((i, k, j ),A(∗)) =
Xi,kXj,kd(i, j)d(j, i)
,∀(i, k, j )∈[N]×[R]×[N],(4)
where we denote d(i, j)as the length of shortest path from itojwithout considering (i, k, j ). Since
our graph is directed, we concatenate them in both directions.
As in [ 7,10,24], we use negative sampling in our training with the difference that we account for
both predicting missing nodes and relation types (Definition 2.1). Specifically, for each positive
training triplet (i, k, j )such that A(train)
i,k,j= 1, we first randomly corrupt either the head or the tail
nndtimes to generate the negative (node) examples (i, k, j′). Additionally, we also want our model
to learn the correct relation type (i,?, j)between a pair of nodes. Thus, we corrupt relation nrl
times to generate negative (relation) examples (i, k′, j). In our training, nnd=nrl= 2; while in
evaluation, nnd= 50, nrl= 0 for node evaluation, and nnd= 0, nrl= 50 for relation evaluation.
Following [ 7], we use cross-entropy loss to encourage the model to score positive examples higher
than corresponding negative examples:
L=−X
(i,k,j)∈S
log
Γtriplet((i, k, j ),A(train))
+1
nnd+nrlnnd+nrlX
p=1log
1−Γtriplet 
i′
p, k′
p, j′
p
,A(train)!
, (5)
where S=n
(i, k, j )A(train)
i,k,j= 1o
, and 
i′
p, k′
p, j′
p
are the p-th negative node or relation example
corresponding to (i, k, j ).
We choose GCN [ 51] as our GNN kernel. In the implementation, we also follow SIGN [ 52] to not
have activation function between GNN layers to make the method faster and more scalable, and only
17Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
have activation functions in the final MLPs in Equation (3). In the message passing scheme, we only
use DSS-GNN in the first layer, while using the whole graph adjacency matrix in following layer
updates. This procedure guarantees the double equivariant property of ISDEA+ and increases the
expressiveness of ISDEA+ to capture more diverse relation paths. In training, we carefully design the
training batch, so that the each gradient step considers only one relation vs all others. Another way to
improve the computation complexity is via parallelization.
Time Complexity. For each layer of ISDEA+, it can be treated as running 2 unattributed GNN times
on the knowledge graph, thus time cost is roughly 2times of adopted GNN. In our experiment, we
use node representation GNNs (e.g., GIN [ 20], GAT [ 42], GraphConv [ 43]) as our GNN architecture,
thus the complexity is O(L(|S|d+|V|d2))where Lis the number of layers, dis the maximum size
of hidden layers, |V|is number of nodes and |S|is number of fact triplets (number of edges) in the
knowledge graph. Besides, for both positive and negative samples (i, k, j ), our method requires the
shortest distance between any two nodes without considering (i, k, j ), which can be achieved from
the Dijkstra or Floyd algorithm.
B Connection to Double Equivariant Logical Reasoning
In what follows, we follow the literature and connect link prediction in knowledge graph to logical
induction [ 8,53,54]. Existing logical induction requires all involved relations to be observed at
least once, thus, such logical reasoning can not generalize to new relation types. We propose the
Universally Quantified Entity and Relation (UQER) Horn clause, a double equivariant extension of
conventional logical reasoning, which is capable of generalizing to new relation types, and show that
the double invariant triplet representation in Definition 2.4 is capable of encoding such set of UQER
Horn Clauses.
Definition B.1 (Universally Quantified Entity and Relation (UQER) Horn clause) .An UQER Horn
clause involving Mnodes and Krelations is defined by an indicator tensor B∈ {0,1}M×K×M:
∀E1∈ V,(∀Eu∈ V \ { E1, . . . , E u−1})M
u=2,∀C1∈ R,(∀Cc∈ R \ { C1, . . . , C c−1})K
c=2,
^
u,u′=1,...,M,c =1,...,K,
Bu,c,u′=1(Eu, Cc, Eu′) =⇒(E1, C1, Eh), (6)
for any node set Vand relation set Rwith number of nodes and relations N, R s.t.N≥M, R≥K,
h∈ {1,2}(where h= 1indicates a self-loop relation or a relational node attribute), where if M > h ,
∀u∈ {h+ 1, . . . , M },PM
u′=1PK
c=1Bu,c,u′+Bu′,c,u≥1, and if K≥2,∀c∈ {2, . . . , K },PM
u=1PM
u′=1Bu,c,u′+Bu′,c,u≥1(every variable should appear at least once in the formula).
Note that our definition of UQER Horn clauses (Definition B.1) is a generalization of the First Order
Logic (FOL) clauses in [ 8,30–32] such that the relations in the Horn clauses are also universally
quantified rather than predefined constants. UQER can be used to predict new relations in the test
knowledge graph with pattern matching , i.e., if the left-hand-side (condition) of a UQER can be
satisfied in the test knowledge graph, then the right-hand-side (implication) triplet should be present.
In Figure 2, we illustrate two examples using UQER to predict new relations at test time.
We now connect our double equivariant representations (Definition 2.3) with the UQER Horn clauses.
Theorem B.2. For any UQER Horn clause defined by B∈ {0,1}M×K×M(Definition B.1), there
exists a double invariant triplet predictor Γtriplet:∪∞
N=1∪∞
R=2([N]×[R]×[N])×A→ {0,1}
(Definition 2.3), such that for any set of truth statements S ⊆ V × R × V and their equivalent
tensor representation A∈A(whereAi,k,j= 1iff(vi, rk, vj∈ S), it satisfies Γtriplet((i, k, j ),A) =
1iff(i, k, j )∈ S′, where S′=
(i, k, j )∀(i, k, j ),such that (E1, C1, E2) = ( vi, rk, vj)∈ V ×
R × V ,∃M−2E3, ..., E M∈ V \ { E1, E2},∃K−1C2, ..., C K∈ R \ { C1},where∀(u, c, u′)∈
[M]×[K]×[M],Bu,c,u′= 1⇒(Eu, Cc, Eu′)∈ S	
is the set of true statements induced by modus
ponens by the truth statements Sand the UQER Horn clause, where the existential quantifier ∃k
means exists at least kdistinct values.
The full proof is in Appendix C, showing how the universal quantification in Definition B.1 is a
double invariant predictor.
18Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
UQER:Couple_OfLives_In
Lives_InT raining
Colleague_OfWorks_InT est
Classmate_OfStudies_In
Studies_In?Colleague_Of
(a) A Simple UQER Application
UQER:Father_OfBrother_Of
Uncle_Of
Father_OfBrother_Of
Uncle_OfT raining
Mother_OfSister_OfT est
Aunt_Of
Mother_OfSister_Of
Aunt_Of? (b) A Complex UQER Application
Figure 2: (a) The UQER (bottom) learned from training can be used to predict missing new relation
“Studies_In” in red since an assignment of left-hand-side of the UQER (E1,Classmate_Of , E3)∧
(E3,Studies_In , E2)is satisfied in test. (b) UQER can contain disconnected components, giving
more freedom to its application. For example, the UQER (bottom) can be learned from training to
repeat arbitrary logical chain, which makes it possible to deal with new female relations at test time
and will predict “Aunt_Of” in test just as “Uncle_Of” (red) in training.
C Proofs
Theorem 2.6 (From double invariant triplet representations to double equivariant graph rep-
resentations ).For all A∈Awith number of nodes and relations N, R , given a double invari-
ant triplet representation Γtriplet, we can construct a double equivariant graph representation as
(Γgraph(A))i,k,j:= Γ triplet((i, k, j ),A),∀(i, k, j )∈[N]×[R]×[N], and vice-versa.
Proof. (⇒) For any knowledge graph A∈Awith number of nodes and relations N, R ,Γtriplet:
∪∞
N=1∪∞
R=2([N]×[R]×[N])×A→Rd, d≥1is a double invariant triplet representation
as in Definition 2.3. Using the double invariant triplet representation, we can define a function
Γgraph:A→ ∪∞
N=1∪∞
R=2RN×R×N×dsuch that ∀(i, k, j )∈[N]×[R]×[N],(Γgraph(A))i,k,j, :=
Γtriplet((i, k, j ),A). Then ∀ϕ∈SN,∀τ∈SR,(Γgraph(ϕ◦τ◦A))ϕ◦i,τ◦k,ϕ◦j,:= Γ triplet((ϕ◦i, τ◦
k, ϕ◦j), ϕ◦τ◦A). We know Γtriplet((i, k, j ),A) = Γ triplet((ϕ◦i, τ◦k, ϕ◦j), ϕ◦τ◦A). Thus
we conclude, ∀ϕ∈SN,∀τ∈SR,∀(i, k, j )∈[N]×[R]×[N],(ϕ◦τ◦Γgraph(A))ϕ◦i,τ◦k,ϕ◦j,:=
(Γgraph(A))i,k,j, := Γ triplet((i, k, j ),A) = Γ triplet((ϕ◦i, τ◦k, ϕ◦j), ϕ◦τ◦A) = (Γ graph(ϕ◦τ◦
A))ϕ◦i,τ◦k,ϕ◦j,:. In conclusion, we show that ϕ◦τ◦Γgraph(A) = Γ graph(ϕ◦τ◦A), which proves
the constructed Γgraph is a double equivariant representation as in Definition 2.5.
(⇐) For any knowledge graph A∈Awith number of nodes and relations N, R , assume Γgraph:
A→ ∪∞
N=1∪∞
R=2RN×R×N×dis a double equivariant representation as Definition 2.5. Since
Γgraph(ϕ◦τ◦A) =ϕ◦τ◦Γgraph(A), then∀(i, k, j )∈[N]×[R]×[N],(Γgraph(ϕ◦τ◦A))ϕ◦i,τ◦k,ϕ◦j=
(ϕ◦τ◦Γgraph(A))ϕ◦i,τ◦k,ϕ◦j= (Γ graph(A))i,k,j. Then we can define Γtriplet:∪∞
N=1∪∞
R=2([N]×[R]×
[N])×A→Rd, d≥1, such that ∀(i, k, j )∈[N]×[R]×[N],Γtriplet((i, k, j ),A) = (Γ graph(A))i,k,j.
It is clear that Γtriplet((i, k, j ),A) = (Γ graph(A))i,k,j= (Γ graph(ϕ◦τ◦A))ϕ◦i,τ◦k,ϕ◦j= Γ triplet((ϕ◦
i, τ◦k, ϕ◦j), ϕ◦τ◦A). Thus, we show Γtriplet is a double invariant triplet representation as in
Definition 2.3.
Theorem 2.8 (From distributional double equivariant positional embeddings to double equiv-
ariant representations ).For any knowledge graph A∈A, the average Ep(Z|A)[Z|A]is a double
equivariant knowledge graph representation (Definition 2.5) for any distributional double equivariant
positional embeddings Z|A(Definition 2.7).
Proof. Based on Definition 2.7, for any knowledge graph A∈Awith number of nodes and relations
N, R , the distributionally double equivariant positional embeddings of Aare defined as joint samples
of random variables Z|A∼p(Z|A), where the tensor Zis defined as Zi,k,j∈Rd, d≥1,∀(i, k, j )∈
[N]×[R]×[N], where we say p(Z|A)is a double equivariant probability distribution on Adefined
as∀ϕ∈SN,∀τ∈SR, p(Z|A) =p(ϕ◦τ◦Z|ϕ◦τ◦A).
19Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
The tensor Zis defined as Zi,k,j∈Rd,∀(i, k, j )∈[N]×[R]×[N], thusZ∈RN×R×N×d. So we
can consider Ep(Z|A)[Z|A]as a function on A, and output a representation in RN×R×N×d. Since
∀ϕ∈SN,∀τ∈SR, p(Z|A) =p(ϕ◦τ◦Z|ϕ◦τ◦A), it is clear to have ∀ϕ∈SN,∀τ∈SR, ϕ◦τ◦
Ep(Z|A)[Z|A] =ϕ◦τ◦R
zp(Z=z|A)dz=R
ϕ◦τ◦zp(Z=z|A)dz=R
ϕ◦τ◦zp(ϕ◦τ◦Z=
ϕ◦τ◦z|ϕ◦τ◦A)d(ϕ◦τ◦z) =Ep(ϕ◦τ◦Z|ϕ◦τ◦A)[ϕ◦τ◦Z|ϕ◦τ◦A]. Since the permutation
ϕ, τonly changes the ordering of the output representation element-wise, we can interchange the
permutations with the integral.
Finally, for any knowledge graph A∈Awith number of nodes and relations N, R , we can define
Γgraph(A) :A→ ∪∞
N=1∪∞
R=2RN×R×N×d, d≥1such that Γgraph(A) :=Ep(Z|A)[Z|A]. And we can
derive ϕ◦τ◦Γgraph(A) =ϕ◦τ◦Ep(Z|A)[Z|A] =Ep(ϕ◦τ◦Z|ϕ◦τ◦A)[ϕ◦τ◦Z|ϕ◦τ◦A] = Γ graph(ϕ◦τ◦A).
Thus, Γgraph(A) :=Ep(Z|A)[Z|A]is a double equivariant knowledge graph representation as per
Definition 2.5.
Lemma 3.1. The triplet representations generated by Ultra [ 6] are double equivariant structural
representations (Definition 2.5).
Proof. Consider an arbitrary knowledge graph G= (V,R,V)with the adjacency matrix A, and
its isomorphic graph with adjacency matrix A′induced by some arbitrary entity and relation per-
mutations ϕ∈SNandτ∈SR, i.e.,A′=ϕ◦τ◦A. Fix a triplet (h, r, t )∈ V × R × V , and let
(h′, r′, t′) = (ϕ◦h, τ◦r, ϕ◦t).
1.Given any graph A∗, an NBFNet produces pairwise structural node representations equivariant
to node permutations if it initializes the node features by setting all-ones vector to the head node
h∗of the query triplet (h∗, r∗,?), and zero vectors to every other node [ 10]. That is, for any
query (h∗, r∗,?)and any valid node permutation ϕ∗∈SN, we have
Γ(triplet)
NBFNet ((h∗, r∗, t∗),A∗) = Γ(triplet)
NBFNet ((ϕ∗◦h∗, r∗, ϕ∗◦t∗), ϕ∗◦A∗),∀t∗∈ V∗.
2.The relation graph Grelconstructed by ULTRA is invariant to the node permutation ϕ. This
is because changing entity labels does not affect the t2h,h2h,h2t,t2tinteractions between
relations.
3.ULTRA invokes the first instance of NBFNet on the relation graph Grel. Hence, following
1., we can conclude that the output triplet representation of this NBFNet instance on Grelis
invariant to the relation permutation τ, because each node in Grelcorresponds to a relation in
G, and so the relation permutation τcorresponds to some node permutation on Grel. Namely,
∀ˆr∈ R, k∈ {t2h,h2h,h2t,t2t},
Γ(triplet)
NBFNet ((r, k,ˆr),Arel) = Γ(triplet)
NBFNet ((τ◦r, k, τ◦ˆr), τ◦Arel).
Consequently, the output relation embedding Rris equivariant to the relation permutation:
Rr[ˆr] =Rτ◦r[τ◦ˆr],∀ˆr∈ R.
Together with 2., we know that Rrisinvariant to node permutations, i.e., ϕ◦Rr=Rr. Hence,
Rris double equivariant: ϕ◦τ◦Rr[ˆr] =Rτ◦r[τ◦ˆr].
4.ULTRA then invokes the second instance of NBFNet on the original graph G, by setting the
initial node and edge features from Rr. Again, following 1., we know that the output triplet
representations of ULTRA are invariant to the node permutations:
Γ(triplet)
ULTRA ((h, r, t ),A) = Γ(triplet)
ULTRA ((ϕ◦h, r, ϕ ◦t), ϕ◦A),∀t∈ V.
5.Then, since the relation representations Rrare double equivariant, and they are used as edge
features in the original graph, this means that:
(a)The initial node embeddings set for the head node is the same: Rr[r] =Rτ◦r[τ◦r] =
Rr′[r′].
(b)The edge type features are equivariant to τ:Rr[ˆr] =Rτ◦r[τ◦ˆr] =Rr′[τ◦ˆr]for any
other relation/edge type ˆr∈ R.
20Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
6. Hence, the final triplet representations produced by ULTRA is double invariant:
Γ(triplet)
ULTRA ((h, r, t ),A)
= Γ(triplet)
ULTRA ((ϕ◦h, r, ϕ ◦t), ϕ◦A)
= Γ(triplet)
ULTRA ((ϕ◦h, τ◦r, ϕ◦t), τ◦ϕ◦A)
= Γ(triplet)
ULTRA ((h′, r′, t′),A′).
7. As a result, the graph representations produced by ULTRA is double equivariant. Namely,
ϕ◦τ◦ΓULTRA (A) = Γ ULTRA (A′) = Γ ULTRA (ϕ◦τ◦A).
Lemma 3.2. The triplet representations generated by InGram [ 5] output distributionally double
equivariant positional embeddings (Definition 2.7).
Proof. To solve fully inductive link prediction, InGram [ 5] first constructs a relation graph , in which
the relation types are treated as nodes, and the edges between them are weighted by the affinity scores,
a measure of co-occurrence between relation types in the original knowledge graph. It then employs
a variant of the GATv2 [ 23,42] on the relation graph to propagate and generate embeddings for the
relation types. These relation embeddings, together with another GATv2, are applied to the original
knowledge graph to generate embeddings for the nodes. Finally, a variant of DistMult [ 24] is used to
compute the scores for individual triplets from the embeddings of the head and tail nodes and the
embedding of the relation.
If the input node and relation embeddings to the InGram model were to be the same across all nodes
and across all relation types respectively (such as vectors of all ones), then InGram would have
produced double structural representations for the triplets (definition 2.3). Simply put, this is because
the relation graphs of InGram [ 5] encode only the structural features of the relation types (their
mutual structural affinity), which is double equivariant to the permutation of relation type and node
indices. Since the same initial embeddings for all nodes and relations are naively double equivariant,
and the GATv2 [ 23,42] is a message-passing neural network [ 55] that also produces equivariant
representations, the final relation embeddings would be double equivariant. Same analysis will also
show the final node embeddings are double equivariant.
However, to improve the expressivity of the model, InGram [ 5] randomly re-initializes the input
embeddings for all node and relation types using Glorot initialization [ 25]for each epoch during
training , a technique inspired by recent studies on the expressive power of GNNs [ 56–58]. Unfortu-
nately, random initial features break the double equivariance of the generated representations, making
them sensitive to the permutation of node and relation type indices. However, since the initial node
V(0)and relation embeddings R(0)are randomly initialized, and by design of InGram architec-
ture, we have ∀(i, k, j )∈[N]×[R]×[N],∀ϕ∈SN, τ∈SR,ZInGram ((i, k, j ),A,V(0),R(0)) =
ZInGram ((ϕ◦i, τ◦k, ϕ◦j), ϕ◦A,V(0),R(0))for any random samples of node and relation embed-
dings v(0), r(0). We define ZInGram|A= [ZInGram ((i, k, j ),A,V(0),R(0)))](i,k,j)∈[N]×[R]×[N], and
ϕ◦τ◦ZInGram|ϕ◦τ◦A= [ZInGram ((ϕ◦i, τ◦k, ϕ◦j), ϕ◦τ◦A,V(0),R(0)))](ϕ◦i,τ◦k,ϕ◦j)∈[N]×[R]×[N].
Since V(0),R(0)random variables that do not change with permutations, we can easily derive
p(ϕ◦τ◦ZInGram|ϕ◦τ◦A) =p(ZInGram|A). Thus, InGram is a distributionally double equivariant
positional graph embedding of Aas per Definition 2.7.
Lemma 3.3. ΓISDEA+ in Equation (2)is a double invariant triplet representation as per Definition 2.3.
Proof. From the ISDEA+ model architecture (Equation (4)), ΓISDEA+ ((i, k, j ),A) = (Xi,k∥Xj,k∥
d(i, j)∥d(j, i)). Using DSS layers, we can guarantee the node representations Xi,kwe learn are
double invariant under the node and relation permutations, where Xi,kinAis equal to Xϕ◦i,τ◦kin
ϕ◦τ◦A. It is also clear that the distance function is invariant to node and relation permutations,
i.e.∀i, j∈[N],d(i, j)inAis the same as d(ϕ◦i, ϕ◦j)inϕ◦τ◦A. Thus ΓISDEA+ ((i, k, j ),A) =
ΓISDEA+ ((ϕ◦i, τ◦k, ϕ◦j), ϕ◦τ◦A)is a double invariant triplet representation as in Definition 2.3.
21Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Theorem B.2. For any UQER Horn clause defined by B∈ {0,1}M×K×M(Definition B.1), there
exists a double invariant triplet predictor Γtriplet:∪∞
N=1∪∞
R=2([N]×[R]×[N])×A→ {0,1}
(Definition 2.3), such that for any set of truth statements S ⊆ V × R × V and their equivalent
tensor representation A∈A(whereAi,k,j= 1iff(vi, rk, vj∈ S), it satisfies Γtriplet((i, k, j ),A) =
1iff(i, k, j )∈ S′, where S′=
(i, k, j )∀(i, k, j ),such that (E1, C1, E2) = ( vi, rk, vj)∈ V ×
R × V ,∃M−2E3, ..., E M∈ V \ { E1, E2},∃K−1C2, ..., C K∈ R \ { C1},where∀(u, c, u′)∈
[M]×[K]×[M],Bu,c,u′= 1⇒(Eu, Cc, Eu′)∈ S	
is the set of true statements induced by modus
ponens by the truth statements Sand the UQER Horn clause, where the existential quantifier ∃k
means exists at least kdistinct values.
Proof. Recall that we have two different cases h= 1andh= 2for Equation (6) in Definition B.1 of
UQER. For the ease of proof, we will focus on the case where h= 2in the following content, and
for the case h= 1, the proof will be the same.
Given h= 2, any UQER is defined by B∈ {0,1}M×K×Mas
∀E1∈ V,(∀Eu∈ V \ { E1, . . . , E u−1})M
u=2,∀C1∈ R,(∀Cc∈ R \ { C1, . . . , C c−1})K
c=2,
^
u,u′=1,...,M,c =1,...,K,
Bu,c,u′=1(Eu, Cc, Eu′) =⇒(E1, C1, Eh), (7)
for any node set Vand relation set Rwith number of nodes and relations N, R s.t.N≥M, R≥K,
where if M > 2,∀u∈ {3, . . . , M },PM
u′=1PK
c=1Bu,c,u′+Bu′,c,u≥1, and if K≥2,∀c∈
{2, . . . , K },PM
u=1PM
u′=1Bu,c,u′+Bu′,c,u≥1(every variable should appear at least once in the
formula).
For all sets of truth statements ∀S ⊆ ∪∞
N=1∪∞
R=2V×R×V , it has an equivalent tensor representation
A∈ {0,1}N×R×Nsuch that Ai,k,j= 1 ⇐⇒ (vi, rk, vj∈ S. We can then define a triplet
representation Γtriplet based on the given UQER as, ∀(i, k, j )∈[N]×[R]×[N],
Γtriplet((i, k, j ),A) =1if(i, k, j )∈ S′
0otherwise ,(8)
where we define S′=
(i, k, j )∀(i, k, j )∈[N]×[R]×[N],such that (E1, C1, E2) =
(vi, rk, vj)∈ V × R × V ,∃M−2E3, ..., E M∈ V \ { E1, E2},∃K−1C2, ..., C K∈ R \
{C1},where ∀(u, c, u′)∈[M]×[K]×[M],Bu,c,u′= 1⇒(Eu, Cc, Eu′)∈ S	
is the set of
true statements induced by modus ponens from the truth statements Sand the UQER Horn Clause,
where the existential quantifier ∃kmeans exists at least kdistinct values.
All we need to show is that Equation (8) is a double invariant triplet representation. For any
node permutation ϕ∈SNand relation permutation τ∈SRofA, we define ϕ◦τ◦ S=
{(vϕ◦i, rτ◦k, vϕ◦i|(vi, rk, vj∈ S} which corresponds to their equivalent tensor representation
ϕ◦τ◦A, where (ϕ◦τ◦A)ϕ◦i,τ◦k,ϕ◦j= 1 ⇐⇒ (vi, rk, vj)∈ S otherwise 0. Similarly,
we have ϕ◦τ◦ S′=
(ϕ◦i, τ◦k, ϕ◦j)∀(i, k, j )∈[N]×[R]×[N],such that (E1, C1, E2) =
(vϕ◦i, rτ◦k, vϕ◦j)∈ V × R × V ,∃M−2E3, ..., E M∈ V \ { E1, E2},∃K−1C2, ..., C K∈ R \
{C1},where ∀(u, c, u′)∈[M]×[K]×[M],Bu,c,u′= 1⇒(ϕ◦Eu, τ◦Cc, ϕ◦Eu′)∈ϕ◦τ◦S	
.
By definition, we have that for any (i, k, j )∈ S′,
Γtriplet((ϕ◦i, τ◦k, ϕ◦j), ϕ◦τ◦A) =1if(ϕ◦i, τ◦k, ϕ◦j)∈ϕ◦τ◦ S′
0otherwise ,.
Now we show that (i, k, j )∈ S′if and only if (ϕ◦i, τ◦k, ϕ◦j)∈ϕ◦τ◦ S′. If(i, k, j )∈ S′,
thenE1=vi, E2=vj, C1=rk,∃M−2E3, ..., E M∈ V \ { E1, E2},∃K−1C2, ..., C K∈ R \
{C1},such that Bu,c,u′= 1 = ⇒(Eu, Cc, Eu′)∈ S . Since (Eu, Cc, Eu′)∈ S if and only if
(ϕ◦Eu, τ◦Cc, ϕ◦Eu′)∈ϕ◦τ◦Sby definition, we have (ϕ◦i, τ◦k, ϕ◦j)∈ϕ◦τ◦S′. Similarly
we can prove if (ϕ◦i, τ◦k, ϕ◦j)∈ϕ◦τ◦ S′, then (i, k, j )∈ S′with the same reasoning.
In conclusion, for any A∈Awith number of nodes and relations N, R , since (i, k, j )∈ S′if and
only if (ϕ◦i, τ◦k, ϕ◦j)∈ϕ◦τ◦ S′, then by definition Γtriplet((ϕ◦i, τ◦k, ϕ◦j), ϕ◦τ◦A) =
22Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Γtriplet((i, k, j ),A)holds∀(i, k, j )∈[N]×[R]×[N], which proves Γtriplet is a double invariant
triplet representation (Definition 2.3).
D Additional Related Work
Link prediction in knowledge graphs, which are commonly used to represent relational data in a
structured way by indicating different types of relations between pairs of nodes in the graph, involves
predicting not only the existence of missing edges but also the associated relation types.
Transductive link prediction. In transductive link prediction, missing links are predicted over a
fixed set of nodes and relation types as in training. Traditionally, factorization-based methods [ 24,59–
67] have been proposed to obtain latent embedding of nodes and relation types to capture their relative
information in the graph. These models try to score all combinations of nodes and relations with
embeddings as factors, similar to tensor factorization. Although excellence in transductive tasks, these
positional embeddings [ 21] (a.k.a. permutation-sensitive embeddings) require extensive retraining
to perform inductive tasks over new nodes or relations [ 8]. However, in real-world applications,
relational data is often evolving, requiring link prediction over new nodes and new relation types, or
even entirely new graphs.
Inductive link prediction over new nodes (but not new relations) with GNN-based model. In
recent years, with the advancement of graph neural networks (GNNs) [ 42,51,58,68–70], in graph
machine learning fields, various works has applied the idea of GNN in relational prediction to
ensure the inductive capability of the model, including RGCN [ 7], CompGCN [ 71], GraIL [ 8],
NodePiece [ 9], NBFNet [ 10], ReFactorGNNs [ 11] etc.. Specifically, RGCN [ 7] and CompGCN [ 71]
were initially designed for transductive link prediction tasks. As GNNs are node permutation
equivariant [ 20,21], these models learn structural node/pairwise representation, which can be used to
perform inductive link prediction over solely new nodes , while most of the GNN performance are
worse than FM-based methods [ 11,72]. Specifically, GraiL [ 8] extends the idea from [ 28] to use
local subgraph representations for knowledge graph link prediction. Refactor GNNs [ 11] aims to
build the connection between FM and GNNs, where they propose an architecture to cast FMs as
GNNs. NodePiece [ 9] uses anchor-nodes for parameter-efficient architecture for knowledge graph
completion. NBFNet [ 10] extends the Bellman-Ford algorithm, which learns pairwise representations
by all the path representations between nodes. [ 73] analyzes knowledge graph-GNNs expressiveness
by connecting it with the Weisfeiler-Leman test in knowledge graph.
Inductive link prediction over new nodes (but not new relations) with logical induction. The
relation prediction problem in relational data represented by knowledge graph can also be considered
as the problem of learning first-order logical Horn clauses [ 8,24,30,32] from the relational data,
where one aims to extract logical rules on binary predicates. These methods are inherently node-
independent and are only able to perform inductive link prediction over new nodes . [74] discusses
the connection between the expressiveness of GNNs and first-order logical induction, but only on
node GNN representation and logical node classifier. [ 54] further analyzes the logical expressiveness
of GNNs for knowledge graph by showing GNNs are able to capture logical rules from graded
modal logic and provides a logical explanation of why pairwise GNNs [ 10,22] can achieve SOTA
results. In our paper, we try to build the connection between triplet representation and logical Horn
clauses. Traditionally, logical rules are learned through statistically enumerating patterns observed
in knowledge graph [ 75,76]. Neural LP [ 30] and DRUM [ 32] learn logical rules in an end-to-
end differentiable manner using the set of logic paths between two nodes with sequence models.
RLogic [ 77] follows a similar manner, which breaks a big sequential model into small atomic models
in a recursive way. [ 78] aims to inductively extract logical rules by devising NodePiece [ 9] and
NBFNet [10]. However, all these methods are not able to deal with new relation types in test.
Inductive link prediction over both new nodes and new relations (with extra context). Few-shot
and zero-shot relational reasoning [ 15,16,18,33,34,36,79–81] aim to query triplets involving
unseen relation types with access to few or zero support triplets of these unseen relation types at test
time. Recent methods [ 15,16,18,33] can even query over unseen nodes. Yet, they often need extra
context in the test graph, such as textual descriptions and/or ontological information of the unseen
23Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
relation types or a shared background graph between the training and test graph, i.e., the test nodes
and relation types are connected to the training ones. For instance, zero-shot link prediction methods
such as [ 33] employ a generative adversarial network [ 82] to utilize the additional textual information
to bridge the semantic gap between seen and unseen relations. Later, OntoZSL [ 34] presented
an ontology-enhanced zero-shot learning approach that incorporates both ontology structural and
textural information. Similarly, TACT [ 37] aims to model the topological correlations between the
target relations and their adjacent relations (assumes there are relations that are seen in train) using a
relational correlation network to learn more expressive representations of the target relations. A recent
work is RMPI [ 15] that extracts enclosing subgraphs around the target triplet, which are assumed to
contain triplets of some relation types seen in training and uses graph ontology to bridge the unseen
relation types to the seen ones. Another one [ 16] uses attention-based GNNs and convolutional
transition for link prediction over new nodes and new relations assuming a shared background graph
between training and test (i.e., new relations in test are connected with existing nodes and relations in
training). MaKEr [ 17] also uses the local graph structure to handle new nodes and new relation types
using a meta-learning framework, assuming the test graph has overlapping relations and entities with
the training graph. On the other hand, few-shot relational reasoning methods learn representations
of the unseen relation types from the few support triplets, which are generally assumed to connect
to existing nodes and relations seen in training [ 79,83,84]. For example, [ 79] was the first to
solve the one-shot task by proposing to compute matching scores between the new relation types
observed in the support set to those training relation types. Later, FSRL [ 84] extends [ 79] by using an
attention-based aggregation to take advantage of information from all support triplets. Another line
of work is called subgraph reasoning, i.e., predict (i, r?, j)by extracting subgraphs surrounding iand
jand subgraphs surrounding another triplet (i′, r, j′)with the same target relation type rbut different
entities i′andj′. As long as these two set of subgraphs are similar, one can proclaim that the target
triplet (i, r, j)should exist. GraphANGEL [ 19] connects this subgraph reasoning idea to first-order
logical (FOL) reasoning to tackle unseen relation types at test time, by extracting subgraphs whose
structural patterns conform to a set of pre-defined FOL rules. Concurrently, CSR [ 18] employs a
similar method but grounds the subgraph reasoning procedure in the language of statistical hypothesis
testing. In comparison to our work, all existing subgraph reasoning methods require that the target
triplet (i, r, j)with the unseen relation type rmust be surrounded by subgraphs consisting solely
of triplets with relation types seen in training. In other words, (i, r, j)must be connected to the
training graph. Hence, they are not applicable when the test KG comes from another domain that is
disconnected to the training KG, which is the most general inductive scenario studied by this work.
Finally, one other line of research is to solve few-shot relational reasoning via meta-learning. For
instance, MetaR [ 83] updates a meta representation over the relation types, and MetaKGR [ 80] adopts
MAML [ 85] to learn meta parameters for frequently occurring relations, which can then be adapted
to few-shot relations. Similar to subgraph reasoning methods, all of these few-shot learning methods
require that the few-shot triplets are connected to a background graph observed during training in
order to learn about the relationship between new relation types and existing ones. Hence, all these
methods cannot be directly applied to test graphs that neither contain textual descriptions of the
unseen relation types nor triplets involving those relation types seen in training.
Inductive link prediction over both new nodes and new relations (no extra context). In this
paper, we focus on the most general task, i.e., inductive link prediction over both new nodes and
new relations on entirely new test graphs without textual descriptions, which we call fully inductive
link prediction . To the best of our knowledge, InGram [ 5] and ULTRA [ 6] are the only existing
methods capable of performing this task. InGram and ULTRA introduce relation graphs to capture
relational representations based on their interactions. In contrast to InGram and ULTRA that designed
a specific architecture, our work proposes a general theoretical framework for designing an entire
class of models capable of solving the fully inductive link prediction task, which encompasses them
as a specific instantiation. Modeling details of InGram and ULTRA have been substantially discussed
in the main paper.
Equivariance of graph neural networks. A substantial body of literature has focused on restricting
neural networks to a class of functions relevant to the task by exploiting the symmetry of problems and
enforcing equivariance with respect to a certain symmetry group of transformations. Earlier examples
such as translation equivariance in Convolutional Neural Networks have shown the effectiveness
of such methodology for computer vision tasks [ 68,86]. In the field of graph machine learning,
Graph Neural Networks possess the equivariance to permutation to node identities [ 51]. Our work
24Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
on double equivariance belongs to this category of research and is a generalization of traditional
GNN equivariance by extending it to the Cartesian product of permutation groups of both node and
relation identity permutations. One parallel line of research is to exploit the geometric symmetry in
the input node features on tasks such as point cloud data, molecular structure representation, and
particle simulation [ 87–93]. In particular, several works have studied how to achieve equivariance
to rotations and translations for 3D vector input features, i.e., E(3) equivariance, as well as SE(3)
equivariance when reflections are also considered [ 94–97]. Satorras et al. [91] then proposes an E( n)
equivariance architecture that can efficiently generalize to translation, rotation, and reflection in larger
than 3 dimensional spaces. Follow-up work further applies these geometry equivariance to the task
of learning molecular representations [ 92,93]. In comparison to our work, these studies focus on
symmetry in the input features, whereas the theory of double equivariance is concerned with the
structure of knowledge graphs. Hence, they are orthogonal to our contribution.
Out-of-Distribution (OOD) generalization of graph neural networks. Many different types
of OOD scenarios on graphs have been studied by the literature and each type of OOD scenario
potentially requires a different kind of invariance assumption, among which causality has been found
to be one of the best framework to model the invariances[ 98–100]. For example, Bevilacqua et al.
[98] first proposed to tackle OOD graph classification tasks, where test graphs have larger size
than those seen in training, by understanding the problem from a causal perspective and capture
the invariance via the structural causal models (SCMs) [ 101]. Chen et al. [99] then extended this
methodology to a wider class of OOD and distribution shift scenarios on homogeneous graphs. Zhao
et al. [100] similarly proposed to understand the problem of link prediction from a causal perspective
and achieve superior performance on a wide range of link prediction tasks, although they did not
directly the OOD problems. In connection to our work, the causality and invariance assumptions
studied by all these works are largely orthogonal to our invariance assumption of double equivariance.
In particular, we posit that double equivariance is a necessary condition for effective solution to
the zero-shot fully inductive task on knowledge graphs. Without guaranteeing double equivariance,
applying the methods from [ 98–100] would not solve our task. Nevertheless, the interesting question
remains as to whether a double equivariant link prediction model can benefit from these causality
assumptions, and whether doing so can better alleviate the negative transfer problem that we observe
in the meta-learning setting (Section 6). We leave this interesting and important investigation to
future work.
E Experiments
E.1 Fully inductive link prediction task over both new nodes and new relation types
In this section, we provide more detailed experiment results and analysis for our method on inductively
fully inductive link prediction on both new nodes and new relation types.
E.1.1 Experiment Setup
Evaluation Metrics. We sample 50negative triplets for each test positive triplet during test
evaluation by corrupting either nodes or relation types (Equation (5)), and use Nodes Hits@ kand
Relation Hits@ kseparately which counts the ratio of positive triplets ranked at or above the k-th
place against the 50negative samples as evaluation metric over 5runs. Specifically, for Node
prediction evaluation, we sample without replacement 50negative tail (or head) nodes, and for
Relation prediction evaluation, we sample with replacement 50negative relation types (can also
handle cases where the number of test relations is less than 50). We also report other widely used
metrics such as MRR.
Hyperparameters and Implementation Details. For homogeneous GNN methods, NBFNet and
ISDEA+, We follow the same configuration as [ 8] such that the hidden layers have 32 neurons. We
use Adam optimizer with grid search over learning rate α∈ {0.01,0.001,0.0001}, and over weight
decay β∈ {0.0005,0}. For all datasets, we train these models for 10 epochs with a mini-batch size
of 16. For the GNN kernel we choose GCN [ 51] of ISDEA+. For these models, the number of hops
and number of layers are 3 on all datasets to ensure fair comparison.
25Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Since NBFNet is designed to only perform inductive link prediction with solely new nodes and
utilizes trained relation embeddings, we use randomly initialized embeddings for the unseen relation
types at test time to enable it for performing fully inductive link prediction.
To run InGram [ 5] on PediaTypes and WikiTopics, we conduct hyperparameter search over the
configurations of ranking loss margin γ∈ {1.0,2.0}, learning rate α∈ {0.0005,0.001}, number of
entity layers L∈ {2,3,4}, and number of entity layers ˆL∈ {2,3,4}. For other hyperparameters, we
use the suggested values from InGram [ 5] and their codebase, such as the number of bins B= 10
and the number of attention heads K= 8. We then use the overall best-performing hyperparameters
on PediaTypes and the best-performing hyperparameters on WikiTopics to run InGram on all tasks in
PediaTypes and all tasks in WikiTopics respectively.
To run DEq-InGram, we use the same trained checkpoints of InGram. The difference is at inference
time, where instead of a single forward pass with one sample of randomly initialized entity and
relation embeddings for InGram, we draw 10 samples of initial entity and relation embeddings and
run 10 forward passes. This yields 10 Monte Carlo samples of the triplet scores, which we then use
to compute the DEq-InGram triplet scores according to Equation (1).
For RMPI [ 15], we use the provided hyperparameters from the codebase and run the RMPI-NE
version of the model with a concatenation-based fusion function, which generally has the best
performance reported in RMPI [ 15]. We note that, since our knowledge graph does not contain
ontological information over the unseen relation types of the test graphs, we instead provide the
model with randomly initialized embeddings for the unseen relation types to perform fully inductive
link prediction.
Training was performed on NVidia A100s, L4s, GeForce RTX 2080 Ti, and TITAN V GPUs.
E.1.2 Fully inductive link prediction over PediaTypes
As discussed in Section 5, we create our own fully inductive link prediction benchmark dataset
PediaTypes. Each graph in PediaTypes is sampled from a graph in the OpenEA library [ 38] (under
GPL-3.0 license). OpenEA [ 38] library provides multiple pairs of knowledge graph, each pair of
which is a database containing similar topics. Each node of a graph corresponds to the Universal
Resource Identifier (URI) of an entity in the database, e.g., “ http://dbpedia.org/resource/E399772 ”
from English DBPedia. Each relation type of a graph corresponds to the URI of a relation in
the database, e.g., “ http://dbpedia.org/ontology/award ” from English DBPedia. Moreover, since
each pair of graphs describes similar topics, most entities and relations are highly related, e.g.,
“http://dbpedia.org/resource/E678522 ” from English and “ http://fr.dbpedia.org/resource/E415873 ”
from French are indeed the same thing, except that the labeling is different. These multilingual KGs
predominantly use English for relation labels, which causes an overlap in relations. However, in our
experimental setup, we treat relations as if they were in different languages and do not leverage this
overlapping information during model training. Thus, we would expect a powerful model that is
insensitive to node and relation type labelings to be able to learn on one graph of the pair and perform
well on the other graph of the same pair.
To control the size under a feasible limitation, we use the same subgraph sampling algorithm as
GraIL [ 8], which proposes link prediction benchmarks over solely new nodes. Details are provided in
Algorithm 1. For each pair of graphs from the OpenEA library, e.g., English-to-French DBPedia, we
first apply the sampling algorithm as in Algorithm 1 on each graph to reduce the size of each graph.
Then we randomly split querying triplets given by the Algorithm 1 into 80% training, 10% validation,
and 10% test for each graph. Finally, to construct the task where we learn on English DBPedia
but test on French DBPedia (denoted as EN-FR), we pick training and validation triplets from the
English graph for model tuning, and only use test triplets from the French graph for model evaluation;
Similarly, for task from French to English (FR-EN), we pick training and validation triplets from
French graph for model tuning, and only use test triplets from English graph for model evaluation.
Thedataset statistics for PediaTypes are summarized in Figure 3.
Additional Results. We present the Node & Relation Hits@10 performance in the main paper. We
provide more results including MRR, Hits@1, Hits@5 in Tables 3 to 5. We can see that our proposed
ISDEA+ and DEq-InGram achieve competent performance with the best baseline in the much harder
relation prediction task, showing their power to generalize to both new nodes and new relations. The
structural double equivariant model ISDEA+ performs worse on node prediction over some datasets,
26Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
DBPedia Wikidata
#Nodes 4906 4948
#Relations 144 102
#Triplets (Obv.) 17593 23888
#Triplets (Qry.) 1666 2456
#Avg. Deg. 7.85 10.65
0 4 8 12 16 20 24
Degree0%2%4%6%8%10%12%DensityDBPedia
Wikidata
(a)DB←→ WDDBPedia YAGO
#Nodes 4795 4751
#Relations 64 17
#Triplets (Obv.) 13248 11327
#Triplets (Qry.) 1177 973
#Avg. Deg. 6.02 5.18
0 4 8 12 16 20 24
Degree0%2%4%6%8%10%12%DensityDBPedia
YAGO
(b)DB←→ YGEnglish French
#Nodes 4962 4933
#Relations 122 101
#Triplets (Obv.) 30876 24165
#Triplets (Qry.) 3326 2485
#Avg. Deg. 13.79 10.81
0 4 8 12 16 20 24
Degree0%2%4%6%8%10%12%DensityEnglish
French
(c)EN←→ FREnglish German
#Nodes 4890 4915
#Relations 121 67
#Triplets (Obv.) 25177 29011
#Triplets (Qry.) 2626 3100
#Avg. Deg. 11.37 13.07
0 4 8 12 16 20 24
Degree0%2%4%6%8%10%12%DensityEnglish
German
(d)EN←→ DE
Figure 3: Statistics of PediaTypes: We report graph statistics including the number of nodes,
number of relations, observed (obv.) triplets, querying (qry.) triplets, and average degree for each
graph pair, e.g., (a) corresponds to DBPedia-and-Wikidata pair, and will be used to construct DB2WD
and WD2DB tasks. We also report (in & out) degree distribution on each graph at the bottom. We
omit tail distribution larger than 25 since they are too small and almost flat.
Algorithm 1 Sampling Algorithm for PediaTypes. This is a subgraph sampling code for a single
graph (either training or test). It will reduce the large original graph into a connected graph of the
required size.
Require: Raw graph triplets Sraw, Raw graph node set Vraw, Raw graph relation set Rraw, Maximum
number of nodes N, Maximum number of edges M, Maximum node degree D.
Ensure: Subgraph triplets Ssub
1:Ssub← ∅
2:Vsub← ∅
3:Rsub← ∅
4:Create an empty queue Q.
5:Get the node v0with the highest degree in the raw graph.
6:Q.add(v0)
7:Vsub← Vsub∪ {v0}
8:while|Q|>0do
9: u←Q.pop()
10: if|Vsub| ≥Nor|Vsub| ≥Mthen
11: continue
12: end if
13: B={(v, r, u )|(r, v)∈ Rraw× Vraw} ∪ {(u, r, v )|(r, v)∈ Rraw× Vraw}
14: if|B|> D then
15: Uniformly select Dtriplets from BasB′
16: else
17: B′← B
18: end if
19: for(i, r, j)∈ B′do
20: ifi=uthen
21: Q.add(j)
22: Vsub← Vsub∪ {j}
23: else
24: Q.add(i)
25: Vsub← Vsub∪ {i}
26: end if
27: Ssub← Ssub∪ {(i, r, j)}
28: end for
29:end while
27Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Table 3: Relation & Node MRR performance on Fully Inductive Link Prediction over Pedi-
aTypes. We report standard deviations over 5 runs. A higher value means better fully inductive
link prediction performance. Models labeled with DEq are double equivariant models, and those
labeled with d-DEq are distributionally double equivariant ones. “Rand” column contains unbiased
estimations of the performance from a random predictor. Double equivariant models consistently
achieve better results than non-double-equivariant models with generally smaller standard
deviations. N/A*: Not available due to constant crashes.
(a)Relation prediction (i,?, j)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 8.86 ±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00
GAT 8.04 ±00.25 7.93±00.04 8.17±00.08 8.12±00.09 8.06±00.15 7.90±00.12 8.12±00.21 8.17±00.16
GIN 8.07 ±00.09 8.09±00.05 8.07±00.13 8.07±00.11 8.03±00.20 7.97±00.30 7.82±00.27 7.84±00.14
GraphConv 7.92 ±00.16 7.97±00.12 8.07±00.15 8.03±00.05 8.14±00.04 7.98±00.18 8.04±00.24 7.84±00.13
NBFNet 10.25 ±01.24 9.53±00.85 8.15±01.21 4.32±00.26 10.33 ±02.45 8.97±01.24 9.29±01.38 14.54 ±04.76
RMPI 12.45 ±01.90 12.10 ±02.71 11.69 ±04.37 10.28 ±01.28 N/A* 8.54 ±02.70 17.89 ±12.22 6.53±02.16
InGram (d-DEq) 50.03 ±05.32 26.31 ±08.27 21.32 ±07.84 29.81 ±14.21 48.70 ±10.06 38.81 ±03.10 29.94 ±13.28 32.26 ±13.97
Ultra (DEq) 82.50 ±02.44 72.99 ±07.41 76.16 ±07.00 91.05 ±01.53 88.52 ±03.82 75.30 ±04.87 47.42 ±12.54 80.38 ±01.85
DEq-InGram (DEq) 73.38 ±05.77 41.61 ±10.12 46.86 ±09.11 40.56 ±14.80 80.74 ±04.47 66.06 ±02.91 39.51 ±16.76 49.10 ±05.43
ISDEA+ (DEq) 72.96 ±00.77 65.73 ±00.58 59.95 ±03.91 84.71 ±01.11 71.47 ±00.31 71.47 ±00.69 66.48 ±06.75 67.36 ±00.43
(b) Node prediction (i, k,?)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 8.86 ±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00 8.86±00.00
GAT 51.43 ±00.25 49.48 ±01.51 26.22 ±00.44 25.45 ±01.23 16.87 ±00.59 34.66 ±00.33 37.22 ±00.29 45.96 ±00.29
GIN 53.72 ±03.45 52.03 ±03.38 34.60 ±07.43 37.27 ±09.42 20.75 ±07.22 40.37 ±08.20 35.80 ±01.36 44.77 ±00.92
GraphConv 63.72 ±01.76 57.77 ±01.09 48.18 ±00.96 45.18 ±00.15 22.49 ±00.76 50.30 ±02.80 38.71 ±00.55 50.54 ±00.42
NBFNet 69.22 ±02.44 74.01 ±01.41 63.49 ±02.44 38.86 ±02.55 41.26 ±02.58 64.02 ±01.25 38.13 ±01.11 52.30 ±02.09
RMPI 63.02 ±02.94 43.72 ±05.65 44.82 ±02.93 46.84 ±05.36 N/A* 46.33 ±08.76 43.00 ±03.70 53.72 ±01.84
InGram (d-DEq) 71.23 ±01.73 55.67 ±05.65 55.94 ±02.76 61.15 ±01.42 34.50 ±08.47 57.05 ±03.73 26.36 ±04.73 56.23 ±01.56
Ultra (DEq) 84.97 ±00.09 88.20 ±00.25 81.03 ±00.10 72.80 ±00.26 69.04 ±00.18 81.43 ±00.16 49.19 ±01.35 60.24 ±00.74
DEq-InGram (DEq) 78.45 ±00.89 68.59 ±04.30 66.13 ±01.48 70.32 ±01.58 44.71 ±08.98 69.23 ±02.53 35.67 ±03.92 48.07 ±08.76
ISDEA+ (DEq) 74.95 ±01.56 57.17 ±01.70 74.38 ±00.66 62.62 ±00.22 63.21 ±00.69 70.58 ±00.42 34.79 ±00.49 40.71 ±01.75
which might be due to the node GNN implementation of ISDEA+. These tasks do not care much
about the actual relation type as we can see from the superior performance of homogeneous GNNs on
node prediction. So the additional equivariance over relations and the training loss over both negative
nodes and negative relations might cause the model to focus more on the relation prediction task,
while the double equivariant structural representation might hurt the performance of missing node
prediction [21].
But it is important to note that the structural double equivariant models excel on relation prediction
and achieves much better results on Hits@1 and Hits@5 as shown in Tables 4 and 5. We also
note that in the Hits@1 and Hits@5 Tables 4 and 5, there are cases where DEq-InGram has higher
variances than the original InGram while achieving much better average performance. This is because
due to the random initialization, InGram performs poorly on the much harder Hits@1 and Hits@5
performance compared to Hits@10. In some seeds of the runs, DEq-InGram successfully improves
the performance of InGram, but there are still seeds of runs that DEq-InGram still performs similar to
InGram. Thus, it results in DEq-InGram having much better average results while also with higher
standard deviations.
E.1.3 WikiTopics: Testing meta-learning and zero-shot transfer capabilities
The WikiTopics dataset is created from the WikiData-5M [ 48] (under CC0 1.0 license). Each node
in the graphs of this dataset represents an entity described by an existing Wikipedia page, and
each relation type corresponds to a particular relation between the entities, such as “director of” or
“designed by”. The node and relation type indices are codenames that start with the prefix “Q” and
“P” respectively, which are devoid of semantic meaning. Nevertheless, WikiData-5M [ 48] provides
aliases for all nodes and relation types that map their indices to textual descriptions, and we use these
textual descriptions to group the relation types into 11 different topic groups, or domains (we do not
however provide these textual descriptions to the models per the specification of the fully inductive
link prediction task). In total, WikiData-5M [ 48] contains 822 relation types. We create WikiTopics
28Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Table 4: Relation & Node Hits@1 performance on Fully Inductive Link Prediction over
PediaTypes. We report standard deviations over 5 runs. A higher value means better fully inductive
link prediction performance. Models labeled with DEq are double equivariant models, and those
labeled with d-DEq are distributionally double equivariant ones. “Rand” column contains unbiased
estimations of the performance from a random predictor. Double equivariant models consistently
achieve better results than non-double-equivariant models with generally smaller standard
deviations. N/A*: Not available due to constant crashes.
(a)Relation prediction (i,?, j)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 1.96 ±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00
GAT 1.07 ±00.14 1.01±00.01 1.03±00.03 1.11±00.09 1.07±00.14 0.99±00.21 0.96±00.16 1.09±00.25
GIN 1.01 ±00.03 0.95±00.08 1.03±00.06 1.10±00.06 0.96±00.15 1.00±00.15 0.92±00.15 0.83±00.17
GraphConv 0.91 ±00.03 0.97±00.06 1.05±00.14 1.01±00.03 1.09±00.07 0.91±00.04 0.94±00.22 0.88±00.20
NBFNet 4.43 ±01.24 3.62±01.01 2.49±01.23 0.51±00.18 4.18±02.17 2.80±00.83 1.63±00.89 7.30±05.01
RMPI 3.92 ±02.08 4.04±01.83 3.37±02.20 2.13±00.79 N/A* 2.39 ±02.35 7.36±09.03 0.91±00.92
InGram (d-DEq) 35.19 ±07.73 12.40 ±07.55 8.45±06.57 16.46 ±16.33 33.66 ±12.09 25.69 ±03.88 14.24 ±12.00 15.83 ±12.59
Ultra (DEq) 78.64 ±03.16 66.58 ±09.50 70.53 ±08.99 89.25 ±01.90 86.07 ±04.78 70.47 ±05.95 35.73 ±15.06 76.42 ±02.37
DEq-InGram (DEq) 65.26 ±10.23 26.90 ±12.97 36.80 ±11.16 25.34 ±18.48 75.00 ±06.42 60.35 ±02.56 24.28 ±14.29 30.82 ±10.43
ISDEA+ (DEq) 58.43 ±01.29 48.68 ±00.96 37.29 ±05.11 75.08 ±01.99 57.05 ±00.92 56.00 ±01.17 59.36 ±07.96 49.41 ±00.85
(b) Node prediction (i, k,?)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 1.96 ±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00 1.96±00.00
GAT 31.80 ±00.64 30.19 ±02.30 10.23 ±00.96 8.68±01.69 7.98±00.89 16.26 ±00.34 26.09 ±00.47 33.06 ±00.29
GIN 34.59 ±04.64 34.57 ±05.26 17.69 ±07.91 20.74 ±10.01 12.42 ±06.59 23.10 ±09.67 23.72 ±01.62 32.26 ±01.89
GraphConv 47.48 ±02.60 40.37 ±01.52 31.96 ±01.02 28.46 ±00.13 12.53 ±00.34 35.82 ±03.54 24.12 ±00.80 37.05 ±00.51
NBFNet 64.17 ±02.68 69.68 ±01.63 57.50 ±02.66 32.26 ±02.81 34.56 ±02.54 59.70 ±01.38 33.32 ±01.11 47.47 ±02.08
RMPI 48.27 ±03.74 26.92 ±04.87 27.38 ±03.09 29.60 ±04.77 N/A* 34.81 ±08.97 33.29 ±03.20 42.14 ±02.87
InGram (d-DEq) 60.00 ±02.06 41.59 ±06.37 39.05 ±02.99 45.44 ±01.69 22.06 ±08.10 42.54 ±04.50 13.47 ±03.50 20.09 ±04.96
Ultra (DEq) 82.21 ±00.12 86.39 ±00.30 77.61 ±00.13 67.74 ±00.32 63.87 ±00.19 78.43 ±00.20 42.13 ±01.31 54.25 ±00.70
DEq-InGram (DEq) 69.46 ±01.12 57.65 ±05.54 51.93 ±01.88 57.06 ±01.96 32.12 ±09.51 57.84 ±03.28 20.49 ±03.35 33.01 ±08.87
ISDEA+ (DEq) 62.17 ±02.38 44.67 ±01.92 63.36 ±00.78 47.78 ±00.48 51.76 ±00.86 60.15 ±00.69 20.74 ±00.28 26.24 ±02.03
datasets from all 822 relation types, which comprise graphs with as many as 66 relation types. Each
graph has a disjoint set of relation types from all other graphs. Table 6 shows the 11 topics/domains
of the WikiTopics dataset, each corresponding to a distinct KG with distinct relation types.
To control the overall size of the graphs in WikiTopics, we downsample 10,000nodes for each
domain from the subgraph consisting of only the triplets with the relation types belonging to that
domain. We adopt the Forest Fire sampling procedure with burning probability p= 0.8[102]
implemented in the Little Ball of Fur Python package [ 103]. We then split the downsampled domain
KG into 90% observable triplets and 10% querying triplets to be predicted by the models. When
splitting, we ensure that the set of nodes in the querying triplets is a subset of those in the observable
triplets. This way, the model is not tasked with the impossible task of predicting relation types
between orphaned nodes previously unseen in the observable part of the graph. This is implemented
via an iterative procedure, where we first sample a batch of missing triplets from the downsampled
domain graph, then discard those that contain unseen nodes in the rest of the triplets, and repeat this
process until the number of sampled triplets reaches 10% of total triplets. Figure 4 shows the data
statistics of WikiTopics dataset.
Meta-learning on WikiTopics and zero-shot transfer to large-scale KGs. The experiment
described in Section 6 is conducted by training double equivariance models (Ultra [ 6], DEq-Ingram,
ISDEA+) on a mixture of WikiTopics KGs with an increasing number of training domains and
test the models’ average zero-shot performance on the remaining held-out domains. The results as
shown in Table 2 indicate that current double equivariant models suffer from negative transfer (their
performance drops when certain “bad” KG is included in the training mixture) and they have poor
data scaling law (their performance saturates after 3 training domains). To investigate whether this
phenomenon is consistent on other KGs that are larger in scale than WikiTopics, we conduct an
additional experiment where we test the models’ zero-shot performance on 3 commonly used large-
scale KGs in the literature: FB15K237 [ 104], NELL995 [ 105], and YAGO310 [ 106]. Table 7 shows
the sizes of the 3 KGs respectively. In this experiment, we use the same training model checkpoints
29Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Table 5: Relation & Node Hits@5 performance on Fully Inductive Link Prediction over
PediaTypes. We report standard deviations over 5 runs. A higher value means better fully inductive
link prediction performance. Models labeled with DEq are double equivariant models, and those
labeled with d-DEq are distributionally double equivariant ones. “Rand” column contains unbiased
estimations of the performance from a random predictor. Double equivariant models consistently
achieve better results than non-double-equivariant models with generally smaller standard
deviations. N/A*: Not available due to constant crashes.
(a)Relation prediction (i,?, j)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 9.80 ±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00
GAT 9.08 ±00.39 8.63±00.25 9.47±00.18 9.20±00.24 8.95±00.36 8.63±00.29 9.58±00.50 9.16±00.23
GIN 9.09 ±00.16 9.31±00.15 9.18±00.28 9.23±00.34 9.12±00.12 8.85±00.56 8.53±00.66 8.61±00.34
GraphConv 8.97 ±00.66 8.74±00.26 9.23±00.11 8.82±00.10 9.17±00.29 9.11±00.50 9.01±00.72 8.73±00.15
NBFNet 12.94 ±01.77 12.46 ±01.40 8.56±01.67 2.68±00.72 13.44 ±04.02 11.74 ±03.02 11.95 ±03.78 20.37 ±05.90
RMPI 16.39 ±04.15 15.76 ±04.58 15.86 ±08.05 12.56 ±02.70 N/A* 8.91 ±03.51 24.25 ±19.24 4.98±03.08
InGram (d-DEq) 67.15 ±05.04 37.86 ±14.41 30.99 ±11.82 40.00 ±13.02 65.80 ±09.59 51.66 ±03.57 43.27 ±19.30 51.54 ±26.09
Ultra (DEq) 98.32 ±00.07 97.01 ±01.24 98.78 ±00.35 98.41 ±00.50 97.83 ±00.61 92.22 ±01.77 75.95 ±07.70 95.85 ±00.03
DEq-InGram (DEq) 83.23 ±05.64 59.83 ±11.57 54.30 ±08.25 57.65 ±15.74 87.08 ±02.55 70.79 ±03.80 51.45 ±29.14 75.85 ±07.26
ISDEA+ (DEq) 93.79 ±00.20 91.21 ±00.25 94.04 ±01.21 96.64 ±00.13 90.83 ±02.29 93.97 ±00.45 74.27 ±06.29 92.75 ±00.40
(b) Node prediction (i, k,?)performance in %. Higher ↑is better.
Models EN-FR FR-EN EN-DE DE-EN DB-WD WD-DB DB-YG YG-DB
Rand 9.80 ±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00 9.80±00.00
GAT 78.49 ±00.44 74.70 ±00.68 42.17 ±00.91 42.39 ±00.52 20.96 ±00.65 57.26 ±00.89 46.92 ±00.37 59.20 ±00.41
GIN 79.96 ±01.88 74.33 ±01.16 53.97 ±07.61 55.89 ±10.06 25.05 ±09.23 61.94 ±06.71 46.56 ±01.37 57.48 ±00.35
GraphConv 85.21 ±00.63 80.67 ±00.30 67.76 ±01.19 64.97 ±00.43 28.37 ±01.41 67.36 ±02.37 53.79 ±00.72 64.13 ±00.23
NBFNet 81.48 ±02.24 85.15 ±01.06 77.62 ±02.41 48.73 ±02.59 51.52 ±03.21 72.18 ±00.90 44.01 ±01.40 60.34 ±02.28
RMPI 82.47 ±02.25 64.88 ±07.62 67.24 ±04.38 69.47 ±06.60 N/A* 60.11 ±08.77 51.57 ±05.03 66.67 ±01.28
InGram (d-DEq) 85.15 ±01.74 72.32 ±05.31 78.84 ±02.86 81.01 ±00.97 45.96 ±11.09 74.88 ±03.09 37.49 ±06.84 50.66 ±06.76
Ultra (DEq) 96.61 ±00.04 96.34 ±00.08 95.07 ±00.09 91.92 ±00.13 83.82 ±00.32 91.41 ±00.08 66.92 ±01.61 76.66 ±01.38
DEq-InGram (DEq) 89.62 ±00.63 81.54 ±02.82 84.57 ±00.95 87.16 ±01.04 57.44 ±09.14 83.14 ±01.64 51.77 ±05.14 65.33 ±09.57
ISDEA+ (DEq) 92.45 ±00.73 71.24 ±02.13 89.98 ±00.96 82.65 ±00.79 76.12 ±00.87 83.33 ±00.46 48.04 ±01.78 57.92 ±01.47
Table 6: The 11 different topics/domains of the WikiTopics dataset.
Domain KG index Abbreviation Description
T1 Art Art and Media Representation
T2 Award Award Nomination and Achievement
T3 Edu Education and Academia
T4 Health Health, Medicine, and Genetics
T5 Infra Infrastructure and Transportation
T6 Loc Location and Administrative Entity
T7 Org Organization and Membership
T8 People People and Social Relationship
T9 Science Science, Technology, and Language
T10 Sport Sport, and Game Competition
T11 Tax Taxonomy and Biology
obtained for Table 2 and evaluate their zero-shot test performance on FB15K237, NELL995, and
YAGO310. We report the zero-shot test Hits@1 accuracy for both relation prediction and node
prediction tasks averaged across all 3 KGs. Note that this is still the fully inductive setting, since all
of the 3 KGs are used for inference purpose only. The set of entities and relation types are distinct
from that in the training WikiTOpics KGs.
Table 8 shows the result of this experiment. These new results on large-scale KGs are consistent with
what we learned from Table 2 in Section 6. To emphasize, our observation is two-fold: First, same
as what we observed in previous experiments, the domain Loc is one example of a KG that elicits
negative transfer effects from all existing double equivariant models. In general, whenever Loc is
included in the training data mix (E.g. Infra+Sci+Sport vs Infra+Sci+Loc), the model’s zero-shot test
performance tends to deteriorate (E.g. a drop of -21.77% for Ultra). This trend is more prominent
on the relation prediction task. Second, all existing models demonstrate relatively poor data scaling
law, especially when the number of training data domains increases from 3 (Infra + Sci + Sport) to 4
30Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
#Nodes # Relations #Triplets (Obv.) #Triplets (Qry.) Avg. Deg.
Art 10000 45 28023 3113 6.23
Award 10000 10 25056 2783 5.57
Edu 10000 15 14193 1575 3.15
Health 10000 20 15337 1703 3.41
Infra 10000 27 21646 2405 4.81
Loc 10000 35 80269 8918 17.84
Org 10000 18 30214 3357 6.71
People 10000 25 58530 6503 13.01
Sci 10000 42 12516 1388 2.78
Sport 10000 20 46717 5190 10.38
Tax 10000 31 19416 2157 4.32
0 510 15 20 25 30 35
Degree0.0%0.4%0.8%1.2%1.6%2.0%2.4%DensityArt
Award
Education
Health Care
Infrastructure
Location
Organization
People
Science
Sport
T axonomy
Figure 4: Statistics of WikiTopics: We report graph statistics including the number of nodes,
number of relations, observed (obv.) triplets, querying (qry.) triplets, and average degree for each
graph. We also report (in & out) degree distribution on each graph at the bottom. We omit tail
distribution larger than 35 since they are fairly small and almost flat.
(Infra + Sci + Sport + Tax). All models show saturated performance, with DEq-InGram in particular
having a significant drop in the relation prediction task.
These new results on large-scale KGs, together with our previous results from Table 2, highlight the
gap of existing double equivariant models towards achieving a true “graph foundation model (GFM).”
AsLocappears to be a KG that has different patterns from other domain-specific KGs, an ideal GFM
should be able to jointly learn these different patterns and enhance its zero-shot test performance
on new domains, instead of being interfered with and forgetting what it has learned, as the current
models do. Our experiments here therefore pinpoint a specific scenario in our benchmark in which
future work that proposes new GFM architectures can attempt to address and evaluate their methods.
Table 7: Sizes of FB15K237 [ 104], NELL995 [ 105], and YAGO310 [ 106] used as inference graphs
in the large-scale WikiTopics Meta-learning experiment. Observable triplets are the number of edges
in the input KG to the model. Test triplets are the number of target ground-truth edges to evaluate.
The observable triplets are taken from the training split and the test triplets from the test split of the
original datasets.
Dataset Entities Relations Observable Triplets Test Triplets
FB15K237 14541 237 272115 20466
NELL995 74536 200 149678 2818
YAGO310 123182 37 1079040 5000
Cross-domain one-to-one zero-shot transfer over WikiTopics KGs. In addition to the meta-
learning experiments we reported in Table 2 and Table 8, we also tested double equivariant model’s
31Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
Table 8: Average zero-shot Hits@1 performance (in %) of Ultra, DEq-InGram, and ISDEA+ by
training on increasingly larger mixtures of WikiTopics KGs and test on FB15K237, NELL995, and
YAGO310. (1) Locis one WikiTopics KG that we identified to consistently trigger negative transfer
effects among all models. Mixing Locin training data consistently diminishes both the training
and test performance across all double equivariant models. We show in parenthesis the relative
performance difference comparing the training combination with Locand a similar combination
without Loc. (2) All models exhibit limited performance improvement as the number of training
KG domains increases.
(a)Average zero-shot Hits@1 of Relation prediction (i,?, j)task
Meta-Learning Training KGs Neg. Transfer? Ultra DEq-InGram ISDEA+
Infra 59.64 10.30 65.73
Loc ✓ 14.90 (−75.01%) 10.79 (+4.76%) 46.02 (−29.99%)
Infra + Sci 69.08 16.06 62.47
Infra + Loc ✓ 54.93 (−20.48%) 13.99 (−12.89%) 58.21 (−6.82%)
Infra + Sci + Sport 74.55 54.17 60.82
Infra + Sci + Loc ✓ 58.32 (−21.77%) 19.85 (−63.36%) 56.59 (−6.95%)
Infra + Sci + Sport + Tax 65.99 25.70 61.23
Infra + Sci + Sport + Loc ✓ 49.26 (−25.35%) 19.91 (−22.53%) 60.27 (−1.57%)
(b)Average zero-shot Hits@1 of Node prediction (i, r,?)task
Meta-Learning Training KGs Neg. Transfer? Ultra DEq-InGram ISDEA+
Infra 79.31 22.12 51.84
Loc ✓ 58.45 (−26.30%) 22.64 (+2.35%) 57.24 (+10.42%)
Infra + Sci 78.08 32.24 65.52
Infra + Loc ✓ 72.26 (−7.45%) 36.24 (+12.41%) 58.50 (−10.71%)
Infra + Sci + Sport 77.70 39.24 65.53
Infra + Sci + Loc ✓ 51.91 (−33.19%) 38.61 (−1.61%) 61.88 (−5.57%)
Infra + Sci + Sport + Tax 77.90 39.41 60.04
Infra + Sci + Sport + Loc ✓ 80.22 (+2.98%) 40.26 (+2.16%) 62.53 (+4.15%)
(Ultra [ 6], DEq-Ingram, ISDEA+) zero-shot performance from each WikiTopic domain to another
one domain. In this setting, we train the models on each of the 11graphs for 5 random seeds,
and for each trained model checkpoint, we cross-test it on all the other 10 graphs, resulting in a
total of 550statistics. We report the mean results across random seeds in heatmaps. We present
a detailed results (heatmaps with values) of Node and Relation Hits@10, Hits@1, and MRR for
WikiTopics in Figures 5 and 6. Due to the large number of runs ( 11×10 = 110 different train-test
scenarios, each with 5 random seeds, resulting in a total of 550 runs) and the time constraints to run
all baseline models, we perform the evaluation over only the three models (ISDEA+, DEq-InGram,
and InGram) that are designed for our fully inductive link prediction task. Figure 5 shows that for
the task of predicting missing relation types (i,?, j), ISDEA+ and DEq-InGram are consistently
better than InGram across all different metrics. Especially, the structural double equivariant ISDEA+
model exhibits more consistent results across different train-test scenarios than both DEq-InGram and
InGram, and achieves significantly better results in Hits@1 and MRR, showcasing its ability for fully
inductive link prediction in a much harder evaluation scenario. For the task of prediction missing
nodes (i, k,?)as shown in Figure 6, ISDEA+, DEq-InGram, and InGram showcase comparable
performance, whereas ISDEA+ exhibits more consistent results across different train-test scenarios
than both DEq-InGram and InGram. We also note that similar to the relation prediction task, ISDEA+
also exhibits the best performance in the Hits@1 metric for the node prediction task.
32Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
(a)ISDEA+ Hits@10
 (b)DEq-InGram Hits@10
 (c)Original InGram Hits@10
(d)ISDEA+ Hits@1
 (e)DEq-InGram Hits@1
 (f)Original InGram Hits@1
(g)ISDEA+ MRR
 (h)DEq-InGram MRR
 (i)Original InGram MRR
Figure 5: Relation prediction (i,?, j)performance over WikiTopics for ISDEA+, DEq-InGram,
and InGram [ 5]. Each row within each heatmap corresponds to a training graph, and each column
within each heatmap corresponds to a test graph. A darker color means better performance. Both
ISDEA, DEq-InGram perform significantly better than InGram, especially for Hits@1 and
MRR, whereas ISDEA+ exhibits more consistent results across different train-test scenarios
than both DEq-InGram and InGram.
33Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types
(a)ISDEA+ Hits@10
 (b)DEq-InGram Hits@10
 (c)Original InGram Hits@10
(d)ISDEA+ Hits@1
 (e)DEq-InGram Hits@1
 (f)Original InGram Hits@1
(g)ISDEA+ MRR
 (h)DEq-InGram MRR
 (i)Original InGram MRR
Figure 6: Node prediction (i, k,?)performance over WikiTopics for ISDEA+, DEq-InGram,
and InGram [ 5]. Each row within each heatmap corresponds to a training graph, and each column
within each heatmap corresponds to a test graph. A darker color means better performance. ISDEA+,
DEq-InGram, and InGram showcase comparable performance in general, and ISDEA+ exhibits
the best performance on Hits@1 in particular.
34