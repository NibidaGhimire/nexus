MIXCE: Training Autoregressive Language Models
by Mixing Forward and Reverse Cross-Entropies
Shiyue Zhang♠∗Shijie Wu♡Ozan ˙Irsoy♡
Steven Lu♡Mohit Bansal♠Mark Dredze♡♣David Rosenberg♡
♡Bloomberg♠UNC Chapel Hill♣Johns Hopkins University
Abstract
Autoregressive language models are trained by
minimizing the cross-entropy of the model dis-
tribution Qθrelative to the data distribution P–
that is, minimizing the forward cross-entropy ,
which is equivalent to maximum likelihood es-
timation (MLE). We have observed that models
trained in this way may “over-generalize”, in
the sense that they produce non-human-like
text. Moreover, we believe that reverse cross-
entropy , i.e., the cross-entropy of Prelative to
Qθ, is a better reflection of how a human would
evaluate text generated by a model. Hence,
we propose learning with MIXCE, an objec-
tive that mixes the forward and reverse cross-
entropies. We evaluate models trained with this
objective on synthetic data settings (where Pis
known) and real data, and show that the result-
ing models yield better generated text without
complex decoding strategies.
https://github.com/bloomberg/
mixce-acl2023
1 Introduction
Rapid advances in pre-trained large-scale autore-
gressive language models (LMs) have dramati-
cally improved the performance of a variety of
tasks (Radford et al., 2019; Brown et al., 2020;
Zhang et al., 2022; Chowdhery et al., 2022). How-
ever, these systems still struggle in many open-
ended generation settings, where they are asked
to produce a long text following a short prompt.
In these cases, we seek systems that generate sen-
sical, coherent, fluent, and engaging, or in short,
human-like text (Pillutla et al., 2022).
Different decoding strategies to generate such
text from pretrained LMs suffer from different de-
generation problems. Unbiased sampling1usually
∗Work done during an internship at Bloomberg.
1Unbiased sampling is vanilla random sampling, i.e., sam-
pling with temperature=1.0. It is also called ancestral sam-
pling (Eikema and Aziz, 2020) or pure sampling (Holtzman
PPQQ-logP(x)   -logQ(x)   Reverse Cross-EntropyForward Cross-Entropy
x ~ Qx ~ P
Figure 1: MIXCEcombines two complementary driving
forces: reverse CE helps narrow the model distribution
Qθdown when it is broader than data distribution P,
while forward CE helps broaden Qθout when it is nar-
rower than P.2
results in incoherent and nonsensical text, while
greedy and beam searches often get stuck in rep-
etition loops (Holtzman et al., 2020) (see exam-
ples in Figure 12). These observations suggest
that the learned LM distribution Qθstill differs
substantially from the human LM distribution P.
A possible reason is that the autoregressive mod-
eling of Qθgives a non-zero probability to ev-
ery possible sequence of tokens, while many se-
quences are impossible under P. Nevertheless, we
still hope that Qθ(x)is as small as possible when
P(x) = 0 . To this end, maximum likelihood esti-
mation (MLE), i.e., minimizing the cross-entropy
(CE)−Ex∼P[logQθ(x)], is the most widely used
objective to train Qθ(x)using sequences sampled
from P. In an idealized setting, with unlimited
training data and model capacity, as well as a per-
fect optimizer, fitting Qθwith MLE will learn a
distribution as close to Pas we like. However, in
practice, we only have finite and noisy data.
We argue that the MLE objective only weakly
et al., 2020). We call it unbiased sampling because it allows
unbiased exploration of the model distribution.
2Note that logP(x)is infinite when P(x) = 0 . But in
practice, we use logP(x) =P
tlog(P(xt|x<t)+ϵ)to avoid
log 0 andϵ= 1e−30.arXiv:2305.16958v2  [cs.CL]  26 May 2024penalizes generations xfrom Qθthat are “bad”,
in the sense that P(x) = 0 . When Qθputs a
small amount of probability mass onto P(x) = 0
space, MLE cannot sufficiently discourage this
behavior (see Figure 3 in Appendix E). More-
over, minimizing forward CE, −Ex∼P[logQθ(x)],
is equivalent to minimizing the forward KL di-
vergence between PandQθ, i.e., KL (P||Qθ)=
Ex∼P[logP(x)/Qθ(x)]. Forward KL has a zero-
avoiding property – avoiding Qθ(x) = 0 when
P(x)̸= 0(Murphy, 2012). Therefore, if there is
noise in the data, Qθwill try to cover the noise as
well, which leads the model to over generalize , in
the sense of putting non-trivial probability mass
overP(x) = 0 generations (Huszár, 2015; Theis
et al., 2016; Ott et al., 2018; Kang and Hashimoto,
2020). As a result, we observe samples from the
model deviating from human-like text. A common
strategy is to modify the decoding method, e.g.,
top-k, top-p, typical, contrastive (Fan et al., 2018;
Holtzman et al., 2020; Meister et al., 2022; Li et al.,
2022) samplings, to tailor the model distribution
Qθin a post-hoc manner to avoid unwanted gener-
ations. In contrast, our approach differs: how can
we obtain a better Qθto obviate the need for these
sampling strategies?
We propose a novel training objective for autore-
gressive LMs – MIXCEthatMixes the forward and
reverse Cross- Entropies: −η·Ex∼P[logQθ(x)]−
(1−η)·Ex∼Qθ[logP(x)].MIXCEcan be under-
stood in two ways. First, we want model genera-
tions to be high-quality as well as diverse. Reverse
cross-entropy reflects how we conduct human eval-
uations, sampling from the model Qθand evaluat-
ing it by the human P, where the focus is text qual-
ity. Forward cross-entropy emphasizes the diver-
sityof model generations (Hashimoto et al., 2019).
Second, MIXCEworks similarly to a mixture of the
forward and reverse KL divergences. The reverse
KL divergence (KL (Qθ||P)) iszero-forcing – forc-
ingQθ(x) = 0 when P(x) = 0 – and thus more
strongly penalizes generating non-human-like sam-
ples compared to MLE. Overall, MIXCEcombines
two complementary driving forces to better fit Qθ
toP(Figure 1). We elaborate on these interpreta-
tions in § 3.1.
Unfortunately, optimizing reverse cross-entropy
is intractable because we do not know P. Hence,
we propose an approximation of the reverse cross-
entropy (see § 3.2), which ends up being a self-
reinforced loss function that encourages the modelto produce generations in which it is already con-
fident. This loss function has the same computa-
tional complexity as forward cross-entropy, making
MIXCE easy to implement and as fast as MLE.
We demonstrate the effectiveness of MIXCEin
both a synthetic setting, where the “human” distri-
bution Pis known, as well as a real setting. For the
synthetic case, we evaluate six learning objectives:
MIXCE,MIXCE∗(MIXCEwithout approxima-
tion), forward KL (=MLE), reverse KL, the mix-
ture of two KL divergences, and Jensen–Shannon
(JS) divergence. We show that MIXCE∗works
slightly worse than the mixture of KLs while out-
performing other objectives, and MIXCEworks
worse than MIXCE∗but generally outperforms
MLE. In real settings, we finetune GPT-2 (Rad-
ford et al., 2019) of different sizes on three English
text domains using MIXCEor MLE. Our results
show that, compared to MLE, unbiased sampling
from MIXCE-finetuned models produces text that
has diversity (Meister et al., 2022) closer to that
of human text, has higher Coherence (Su et al.,
2022), has higher Mauve (Pillutla et al., 2021),
and is preferred by humans. When using top- p
sampling (Holtzman et al., 2020) and carefully tun-
ingp, generations from MLE-finetuned models are
similar to those generated from MIXCE-finetuned
models. Nonetheless, MIXCEmodels have tuned
pvalues closer to 1, implying a less noisy model
distribution. In addition, we modify the original
Mauve to make it more robust to spurious features
(e.g., text length), under which MIXCEstill im-
proves over MLE when using unbiased sampling.
2 Background and Related Work
2.1 Autoregressive Language Modeling
Language generation is mostly based on the au-
toregressive language modeling methodology. The
generation of one word is conditioned on previ-
ously generated words, Qθ(xt|x<t), and the final
probability of the sequence xis the product of prob-
abilities of each step, Qθ(x) =Q
tQθ(xt|x<t).
Early works build n-gram neural LMs (Bengio
et al., 2000) and then RNN-based LMs (Mikolov
et al., 2010), and now Transformers (Vaswani et al.,
2017) have become the dominant architecture. Lan-
guage generation models have either a decoder-
only (Mikolov et al., 2010) or an encoder-decoder
architecture (Sutskever et al., 2014; Bahdanau et al.,
2015). In this work, we focus on decoder-only
LMs. In recent years, many large-scale pre-traineddecoder-only LMs have been introduced (Radford
et al., 2019; Brown et al., 2020; Zhang et al., 2022;
Chowdhery et al., 2022). They can be finetuned for
downstream tasks and even perform surprisingly
well in a zero-shot or few-shot manner. Despite the
impressive performance, language degeneration is
one of the key issues that remain to be solved.
2.2 Language Degeneration
According to Holtzman et al. (2020), language de-
generation refers to output text that is bland, in-
coherent, or gets stuck in repetitive loops . It is
widely observed in open-ended generations from
pretrained LMs. Please refer to some GPT2-large
examples in Figure 12. Two commonly observed
patterns of degeneration are the incoherent text
from unbiased sampling and the repetitive text from
greedy or beam search. Degeneration also appears
in sequence-to-sequence generation tasks but in a
slightly different form (Stahlberg and Byrne, 2019).
There is no agreement on what causes degen-
eration. Ott et al. (2018) attribute it to data noise
and the smooth class of model functions. It is
inherent in the model’s structure to have support
everywhere, in particular, because all probabilities
are produced by softmax, which is strictly posi-
tive. Therefore, Hewitt et al. (2022) assume that an
LM distribution is the true data distribution plus a
uniform-like smoothing distribution. Based on the
observation that human-like text has a large but not
too large likelihood under the learned LM distribu-
tion (Zhang et al., 2021), a lot of works propose em-
pirically useful decoding methods beyond unbiased
sampling and greedy/beam search (Fan et al., 2018;
Holtzman et al., 2020; Eikema and Aziz, 2020;
Basu et al., 2021; Meister et al., 2022; Li et al.,
2022; Hewitt et al., 2022; Su et al., 2022; Krishna
et al., 2022). One of these approaches is the canoni-
cal top- p(or nucleus) sampling method (Holtzman
et al., 2020), which samples from top tokens that
take up pproportion (e.g., 95%) of the probability
mass at each decoding step. Even though these
decoding methods work impressively well, they
are post-hoc fixes rather than learning the LM ac-
curately in the first place. Therefore, some other
works criticize the MLE training objective and pro-
pose alternative loss functions.
2.3 Objectives Beyond MLE
Unlikelihood training (Welleck et al., 2020; Li
et al., 2020) was proposed to penalize repetition
(or any undesirable phenomenon) explicitly duringtraining. The idea is to minimize the likelihood
of a set of negative tokens at each generation step
during training. The selection of negative tokens is
pre-defined, e.g., tokens that appear often in the pre-
vious context. MIXCEshares the same goal with
unlikelihood training – matching the human LM
distribution, but provides a more general approach
without targeting any specific problem.
Similar to our motivation, Kang and Hashimoto
(2020) think that the zero-avoiding property of
MLE makes the model sensitive to dataset noise.
To cover these noisy examples, the model has to put
non-trivial probability mass on the P(x) = 0 area.
To combat this problem, they propose a loss trunca-
tion method that drops high-loss (low-likelihood)
examples during training time.
Pang and He (2021) want to address the mis-
match of learning objective and human evaluation
(likelihood vs. quality) and introduce the GOLD al-
gorithm to approximate reverse cross-entropy. Our
approximation is similar to theirs but has a different
derivation process (see § 3.2). Moreover, GOLD
is evaluated on controlled generation tasks (e.g.,
summarization and translation) in which the goal
is to generate one high-quality text for each input,
and diversity is not so important. In contrast, if we
train the LM only with reverse CE till convergence,
the model will deterministically produce the most
likely text for each prompt, which is undesirable
for an LM. Therefore, mixing forward and reverse
CEs is necessary.
The idea of MIXCE is also relevant to
GANs (Goodfellow et al., 2014). GANs opti-
mize the Jensen–Shannon (JS) divergence between
model and data distributions. Essentially, JS diver-
gence is also for balancing the two driving forces
of forward and reverse KL divergences (Huszár,
2015), and it has been successfully used for evaluat-
ing LM-generated text (Pillutla et al., 2021). How-
ever, probably due to the discrete nature of text,
GANs have not been well applied to LM training.
Caccia et al. (2020) show that previous language
GANs often give up diversity for quality.
Another related work is Popov and Kudinov
(2018), which finetunes LMs with the sum of the
forward cross-entropy loss and reverse KL diver-
gence. They train a discriminator to estimate re-
verse KL, similar to a GAN. On the other hand, we
directly approximate reverse cross-entropy in our
objective function, without training an additional
discriminator.Concurrently, with the same motivation as ours,
Ji et al. (2023) propose to replace MLE with min-
imization of the total variation distance (TVD)
(Van Handel, 2014) between data and model distri-
butions. Notably, their final approximation of TVD,
which they call TaiLr, is equivalent to forward
cross-entropy when the hyperparameter γ= 0
and equals our approximated reverse cross-entropy
when γ= 1.
3 Methodology
3.1 M IXCE
Our MIXCElearning objective for training LMs
is the combination of forward and reverse cross-
entropies, written as
−η·Ex∼P[logQθ(x)]−(1−η)·Ex∼Qθ[logP(x)]
(1)
where ηis the mixing ratio. When η= 1, it be-
comes the normal MLE objective; and when η= 0,
it is the reverse cross-entropy only.
TheMIXCEloss can be understood in two ways.
First, reverse and forward cross-entropy (CE) em-
phasize quality anddiversity respectively. The re-
verse CE, −Ex∼Qθ[logP(x)], focuses on quality
because it resembles how we conduct human eval-
uations – sampling from the model Qθand evaluat-
ing it by the human P. In human evaluations, the
focus is more on the quality of the model-generated
text. So, it is possible that a model always gener-
ates the same few high-quality texts, but still gets
high human evaluation scores. This is similar to
themode collapse problem of GANs. The forward
CE,−Ex∼P[logQθ(x)], instead focuses more on
diversity because it needs any sample from Pto
have a non-trivial probability under Qθ(Hashimoto
et al., 2019). Note that it does not mean forward
CE has zero effect on quality, rather, the model
likelihood Qθ(x)only loosely correlates with the
human-perceived quality of x(Zhang et al., 2021).
Second, we hypothesize that MIXCEworks sim-
ilarly to a mixture of forward and reverse KL di-
vergences, which we will show empirically in our
synthetic experiments (§ 4.1). On the one hand,
minimizing forward KL is equivalent to optimizing
forward CE. On the other hand, reverse KL diver-
gence, Ex∼Qθ[logQθ(x)
P(x)], has two parts: reverse
CE and negative entropy of Qθ,Ex∼Qθ[logQθ(x)].
Reverse CE is minimized when the model deter-
ministically outputs the most likely example, i.e.,
Qθ(x) =δ(the most likely xunder P). Instead,minimizing the negative entropy (maximizing the
entropy) of the model encourages it to be as un-
certain as possible, i.e., having a large support and
uniform distribution. This entropy term counter-
acts the narrowing-down effect of reverse CE. As
discussed above, forward CE pushes the Qdistri-
bution to fully cover the support of P. In this case,
forward CE can also help counteract the narrowing-
down effect of reverse CE, i.e., the maximizing en-
tropy term becomes less important when forward
CE is present. Hence, we think it is reasonable to
drop it from reverse KL.
Overall, M IXCE combines two complementary
training signals, as shown in Figure 1. Reverse CE
prevents the model distribution from being broader
than the data distribution, while forward CE is more
helpful for preventing the model distribution from
being narrower than the data distribution. Although
forward CE also has non-zero loss when the model
distribution is too wide, its loss magnitude is much
smaller than what reverse CE provides (see Ap-
pendix E for more discussion). When data is clean,
two CEs work jointly to help learn the data distribu-
tion better. When data is noisy, the mixing ratio η
allows us to trade-off between emphasizing a good
coverage of the data and putting more weight on
the actually high-quality sequences.
3.2 Optimization of Reverse CE
Optimizing MIXCEis non-trivial. The obstacle
is to minimize the reverse CE, −Ex∼Qθ[logP(x)]
with respect to θ. To this end, we need to know
Pand to have a differentiable sampling operation
from Qθ. In our synthetic experiments (§ 4.1), we
use a distribution Pof our own construction and
use Gumbel-Softmax (Jang et al., 2017; Maddi-
son et al., 2017) to make the sampling operation
differentiable.
However, in a real setting, we do not know P.
To deal with this, we take the following steps to
derive an approximation of the gradient of the re-
verse cross-entropy (we omit the negative sign for
simplicity):
∇θEx∼Qθ[logP(x)] (2)
≈∇θEx∼Qθ[P(x)] (3)
=X
x∇θQθ(x)P(x) (4)
=X
xQθ(x)∇θlogQθ(x)P(x) (5)
=X
xP(x)Qθ(x)∇θlogQθ(x) (6)=Ex∼P[Qθ(x)∇θlogQθ(x)] (7)
=Ex∼P[TY
t=1Qθ(xt|x<t)TX
t=1∇θlogQθ(xt|x<t)]
(8)
≈Ex∼P[TX
t=1Qθ(xt|x<t)∇θlogQθ(xt|x<t)](9)
First, from (2) to (3), we substitute expected
log-likelihood by expected accuracy .˙Irsoy (2019)
shows that expected accuracy is a comparable or
better alternative loss function to cross-entropy for
classification tasks. Then, following the Policy Gra-
dient theorem (Williams, 1992; Sutton et al., 1999),
we get (4) and (5), where we view model Qθas the
policy and P(x)as the reward we want to optimize
for the whole sequence. Next, we switch from the
expectation of Qθto the expectation of P(from
(5) to (6) and (7)), so that we can use the offline
samples from P(data samples in the training set)
instead of online sampling from Qθ. We unfold
Qθ(x), which results in (8). Until this point, theo-
retically, we are already able to optimize the model
using Equation (8) without knowing P. However,
the product of Qθ(xt|x<t)has a very high vari-
ance, and in practice, it underflows when Tis large.
Therefore, we apply a final rough approximation
that leads to (9).
Equations (8) and (9) are apparently not equiva-
lent to each other. Nonetheless, they have similar
effects. Intuitively, in (8), we weigh the gradients of
each sequence differently based on their sequence-
level probabilities, Qθ(x); in other words, it pro-
motes high-likelihood sequences. Similarly, (9)
weighs gradients at each step by Qθ(xt|x<t), i.e.,
promoting high-likelihood tokens at each step. So
essentially, they both encourage the model to pro-
duce generations in which it is already confident .
We call it a self-reinforced objective. To further
illustrate why self-reinforcement makes sense, we
conduct an analysis using GPT-2 (Radford et al.,
2019). Please refer to Appendix D for a detailed
discussion. In short, we show that MLE-pretrained
GPT-2 on average assigns a higher probability to
human text than to text sampled from the model.
Therefore, when we promote high-probability se-
quences or tokens, it is like “pushing” the model
distribution toward the human distribution. But,
we need to avoid overly “pushing” it to the ex-
tremely high-probability region where repetitive
greedy search outputs locate.Note that our approximation of reverse cross-
entropy is relevant to the method proposed by Pang
and He (2021), though we have a different deriva-
tion process from theirs. Please see Appendix A
for a detailed comparison.
Finally, combining forward CE and Equation (9),
our approximated MIXCEobjective is to maximize
Ex∼P[TX
t=1(η+(1−η)·Qθ(xt|·))∇θlogQθ(xt|·)],
(10)
where Qθ(xt|·)is short for Qθ(xt|x<t). This loss
function has the same computational complexity
as forward CE (MLE). Since Qθ(xt|x<t)is strictly
lower than 1 (it is around 0.017 to 0.13 when using
GPT-2), the gradient from approximated reverse
CE is smaller than that from forward CE. Therefore,
it is important to tune ηto balance the effects of
two CEs.
After the camera-ready version of this paper was
published on ACL 2023 (Zhang et al., 2023), we
developed a new derivation of reverse CE that pro-
vides a more interpretable derivation of token-level
self-reinforcement. Please refer to Appendix B for
the details. In Appendix C, we also discuss the re-
lation between MIXCEand the leaky loss function
proposed in ˙Irsoy (2019).
4 Experiments
4.1 Synthetic Experiments
We first conduct experiments in a synthetic ideal
setting, where we know P, to show the effective-
ness of mixing two cross-entropies with or without
approximation. Moreover, during evaluation, we
can directly compare the learned model parameters
against the ground truth parameters of P.
Define the “human” LM P.We start by defin-
ingPas a bi-gram LM. Bi-gram means that the
prediction of the next token only depends on the im-
mediately previous token, i.e., P(xt|xt−1). There-
fore,Pis determined by a transition matrix among
words M∈RV×V(V=vocabulary size) and a
start token probability distribution π∈RV, i.e.,
stochastic finite-state automata. The last token in
the vocabulary is the end-of-sequence (EOS) token.
For simplicity, we initialize πas a uniform distri-
bution. To initialize M, we use two methods. The
first is random initialization . We sample categori-
cal distributions from a Dirichlet ( α=0.5) prior to
initialize each row of M. However, one remain-
ing problem is that Phas support everywhere. TohaveP= 0areas, we randomly assign 0s to a cer-
tain percent of values in each row of Mand then
re-normalize to sum to 1.3We test 3 percentages:
10%, 50%, and 90%. The second is initialization
using real data . We sample 5000 pieces of text
from WebText (Radford et al., 2019), count the oc-
currence of bigrams, and then use the occurrence to
initialize M. In this case, there are naturally 0s in
M, and the larger the vocabulary size is, the sparser
Mis. No matter which initialization is used, we
reserve the last row of Mfor EOS and it has all
0s, i.e., will not transit to any token. We set the
vocabulary size V=20, 50, 100, 500, or 1000.4
Learn an LM Qθ.We implement model Qθas
a simple neural bigram LM. Given the word em-
bedding ei−1of the previous token xi−1, the next
token is predicted via a simple neural network f:
hi−1=Dropout (ReLU (W1ei−1+b1)),
Q(xi|xi−1) =Softmax (W2hi−1+b2),
where W1∈Rd×d(dis the hidden dimension
size),b1∈Rd,W2∈Rd×V, andb2∈RVare
model parameters. After training this model, the
learned transition matrix can be obtained by M′=
f(E),Eis the word embedding matrix.
Synthetic data. We sample sequences from P.
We set the max sequence length as 500. We sample
50K and 5K sequences as the training and valida-
tion set, respectively. There is no test set because
we directly compare the learned transition matrix
M′to the gold Mduring evaluation.
Metrics. (1)avg. js : we compute the JS diver-
gence between each row (except the last row) of M′
and the corresponding row in M, and then average
across rows. This metric evaluates the overall diver-
gence of M′fromM, and equals 0 iff M′=M;
(2)avg. 0s : we get the probabilities from M′from
positions where the corresponding gold probabili-
ties are 0 in M, and take their average. If M′=M,
avg. 0s = 0, but vice versa is not true.
Objectives. (1)Forward KL , KL (P||Qθ)=
Ex∼P[logP(x)/Qθ(x)], which is equivalent
to MLE; (2) Reverse KL , KL (Qθ||P)=
3When we assign 0s, we make sure every token has non-
zero transition probability to EOS.
4Our defined bi-gram LMs are always tight, i.e., do not
“leak” probability mass onto infinite sequences because we
make sure that all accessible tokens also have non-zero paths
to other tokens. Please refer to Du et al. (2022) for the proof.Random (50%) WebText
V ocab Objective avg. js avg. 0s avg. js avg. 0s
Gold 0.0 0.0 0.0 0.0
20 For. KL 7.40e-4 1.44e-4 9.93e-4 1.79e-4
Rev. KL 1.36e-1 7.42e-6 3.93e-3 1.95e-6
Mix KLs 4.89e-4 5.15e-5 9.91e-4 1.11e-5
JS 2.14e-1 4.88e-5 1.12e-2 5.84e-6
MIXCE* 8.12e-4 1.05e-4 1.36e-3 1.19e-4
MIXCE 7.02e-4 1.25e-4 1.00e-3 1.79-4
50 For. KL 6.47e-3 5.65e-4 4.30e-3 4.77e-4
Rev. KL 4.29e-1 1.53e-3 3.48e-2 5.30e-5
Mix KLs 4.45e-3 2.80e-4 3.91e-3 2.83e-4
JS 4.74e-1 1.40e-3 9.23e-3 2.48e-5
MIXCE* 4.49e-3 3.72e-4 3.94e-3 2.75e-4
MIXCE 6.47e-3 5.64e-4 4.29e-3 4.77e-4
100 For. KL 3.56e-2 1.44e-3 9.70e-3 3.10e-4
Rev. KL 5.57e-1 3.62e-4 1.00e-1 4.04e-5
Mix KLs 2.74e-2 2.10e-4 9.19e-3 1.84e-4
JS 5.53e-1 9.69e-4 1.73e-1 5.56e-4
MIXCE* 2.85e-2 9.16e-4 9.61e-3 1.87e-4
MIXCE 3.56e-2 1.41e-3 9.69e-3 3.16e-6
500 For. KL 2.39e-1 1.49e-3 4.60e-2 1.78e-4
Rev. KL 6.78e-1 2.76e-6 3.05e-1 1.68e-5
Mix KLs 2.32e-1 8.60e-4 4.27e-2 1.33e-4
JS 5.34e-1 7.19e-4 2.78e-1 3.84e-5
MIXCE* 2.34e-1 1.38e-3 4.23e-2 1.29e-4
MIXCE 2.35e-1 1.46e-3 4.53e-2 1.64e-4
1000 For. KL 2.93e-1 8.80e-4 8.10e-2 1.50e-4
Rev. KL 6.85e-1 1.21e-6 3.30e-1 6.26e-6
Mix KLs 2.91e-1 8.57e-4 7.50e-2 1.17e-4
JS 4.59e-1 5.97e-4 3.02e-1 1.93e-5
MIXCE* 2.92e-1 8.58e-4 7.44e-2 1.14e-4
MIXCE 2.92e-1 8.76e-4 7.94e-2 1.42e-4
Table 1: Synthetic experimental results. Random (50%)
randomly initializes Mand sets 50% of the probabilities
to 0. WebText means initializing Mby the bigram
occurrence in the WebText data. Gold refers to the
results when M′=M.avg. js is our main metric, which
represents the average JS divergence between Mand
M′(please see the definition of avg. 0s in text). Each
number is a 5-seed average, and Table 7 shows the 95%
confidence intervals of some experiments.
Ex∼Qθ(x)[logQθ(x)/P(x)]; (3) Mixture of two
KLs,η·KL(P||Qθ) + (1 - η)·KL(Qθ||P);
(4)JS, we use a general definition of JS diver-
gence (Huszár, 2015), η·KL(P||M) + (1 - η)
·KL(Qθ||M), where M=η·P + (1 - η)·Qθ;5
(5)Oracle mixture of cross-entropies (MIXCE∗),
where we use the known P. (6) Approximated
mixture of cross-entropies (MIXCE), where we
assume Pis unknown. Except for Forward KL
and MIXCE, the other four objectives all need
to sample from Qθand require gradients to pass
through this sampling operation. To this end, we
use Gumbel-Softmax (Jang et al., 2017; Maddison
et al., 2017) to make sampling differentiable.
5When η= 0.5, it is the same as the objective of
GAN (Goodfellow et al., 2014). But instead of using GAN’s
min-max loss, we directly optimize JS because we know P.WikiText WebText WritingPrompts
Model Size Objective ppl div mauve coh ppl div mauve coh ppl div mauve coh
Human - 0.89 1.0 0.628 - 0.84 1.0 0.633 - 0.85 1.0 0.473
SmallMLE 26.98 0.91 0.67 0.556 21.45 0.87 0.90 0.555 28.45 0.87 0.85 0.397
MIXCE 35.04 0.87 0.93 0.567 21.69 0.85 0.92 0.565 28.79 0.86 0.89 0.403
MediumMLE 20.43 0.90 0.73 0.573 15.92 0.87 0.88 0.560 22.72 0.88 0.89 0.414
MIXCE 25.92 0.88 0.95 0.584 16.51 0.83 0.93 0.585 23.04 0.86 0.91 0.419
LargeMLE 18.24 0.90 0.75 0.567 14.13 0.87 0.81 0.570 21.95 0.87 0.87 0.425
MIXCE 23.44 0.88 0.95 0.578 14.66 0.82 0.94 0.592 21.04 0.86 0.94 0.429
Table 2: Unbiased sampling results of models finetuned by MLE or MIXCEon three datasets. For all metrics, the
closer to the human scores the better. Bold numbers are the ones that are closer to human scores in each setting.
Each number is a 3-run average.
WikiText WebText WritingPrompts
Model Size Objective bestp div mauve coh bestp div mauve coh bestp div mauve coh
Human - 0.89 1.0 0.628 - 0.84 1.0 0.633 - 0.85 1.0 0.473
SmallMLE 0.85 0.89 0.93 0.584 0.93 0.84 0.94 0.580 0.97 0.86 0.90 0.410
MIXCE 0.99 0.87 0.95 0.568 0.99 0.84 0.93 0.571 0.99 0.85 0.90 0.407
MediumMLE 0.85 0.88 0.95 0.602 0.93 0.85 0.95 0.592 0.97 0.86 0.92 0.428
MIXCE 0.99 0.87 0.96 0.590 0.99 0.81 0.93 0.594 0.99 0.85 0.92 0.427
LargeMLE 0.87 0.89 0.96 0.594 0.95 0.84 0.87 0.593 0.99 0.86 0.89 0.430
MIXCE 0.99 0.87 0.97 0.580 0.99 0.81 0.94 0.601 0.99 0.86 0.94 0.435
Table 3: Top- psampling results of the same models as Table 2. Since changing the decoding method will not affect
perplexity, we report the selected best pinstead.
Model selection. During training, we check the
validation loss (the value of the objective function)
after every epoch and only save the best checkpoint
that has the lowest validation loss. For objectives
withη, we choose the best ηbased on the avg. js
result on the validation set. We report a 5-seed
average for each experiment. The search space of
ηis [0.99, 0.9, 0.5, 0.1, 0.01]. Selected best ηs are
reported in Table 11 in the Appendix.
Results. Table 1 (and Table 6 in the Appendix)
shows the results of our synthetic experiments.
Across 4 kinds of initialization of Mand 5 vo-
cabulary sizes, we observe some common patterns.
First, the mixture of two KLs often gets the best
avg. js compared to other objectives, and MIXCE∗
usually comes second. This supports our expec-
tation that the mixture of two cross-entropies ap-
proximates the mixture of two KLs (§ 3.1), as
well as demonstrates that combining two KLs or
CEs can help learn the data distribution more ac-
curately compared to MLE. Second, the approxi-
mated MIXCEusually under-performs MIXCE∗
but outperforms forward KL (MLE). Third, reverse
KL generally works best for the avg. 0s metric, due
to its property of zero-forcing – forcing Qθ(x) = 0
when P(x) = 0 . Lastly, JS divergence oftentimes
works similarly to reverse KL, which is consistentwith the observation made by Caccia et al. (2020)
– language GANs trade off diversity for quality.
4.2 GPT-2 Experiments
Next, we test MIXCEin a real setting where we
do not know P, but we have finite samples from
P. We use GPT-2 (Radford et al., 2019) as the
LMQθ. Though GPT-2 models are already pre-
trained by MLE, for simplicity, we use different
objectives to finetune it. We test GPT-2 in 3 sizes:
small (24M), medium (355M), and large (774M).
See more implementation details in Appendix I.
Real data. We use English text data from 3 do-
mains: (1) WikiText (Merity et al., 2017): text from
Wikipedia; (2) WebText (Radford et al., 2019): text
from the Web. It was used for pretraining GPT-2;
and (3) WritingPrompts (Fan et al., 2018): text
from the writing prompts forum of Reddit. We
sample from each of these 3 datasets to form our
training, development, and test sets. By default, our
training/development/test set contains 50K/5K/5K
examples. Please find more details about these
datasets in Appendix I.
Metrics. (1)Perplexity (ppl) is defined as
e−1
N∗TP
NP
TlogeQθ(xt|x<t), where Nis the num-
ber of examples and Tis the sequence length. Per-
plexity is not necessarily correlated with humanperceived quality (Zhang et al., 2021). (2) Diver-
sity (div) : following Meister et al. (2022), we de-
finen-gram diversity as the average fraction of
unique vs. total n-grams for n∈{1, 2, 3, 4}
in each piece of text. (3) Mauve (Pillutla et al.,
2021) compares model-generated text against hu-
man text via a KL divergence curve and is the state-
of-the-art metric for open-ended text generation.
We use Mauve as our primary metric. (4) Coher-
ence (coh) (Su et al., 2022) computes the cosine
similarity between the embedding of prompt and
the embedding of continuation, and embeddings
are from SimCSE (Gao et al., 2021). All metrics
arethe closer to human scores the better .
Objectives. Since we have no access to P, we
can only implement two out of the six objectives
we test in the synthetic setting: (1) MLE , which is
equal to forward CE or forward KL; (2) MIXCE,
the approximated mixture of cross-entropies.
Decoding. We use unbiased sampling (see foot-
note 1) as our primary decoding method as it allows
us to explore the learned distribution in an unbi-
ased way (Eikema and Aziz, 2020). Additionally,
we test top-psampling (Holtzman et al., 2020) to
check if MIXCEis complementary to advanced
decoding methods, and we carefully tune pon the
development set. For each text, we take the first 50
tokens (by GPT-2 tokenizer) as the prompt and set
the max generation length as 512.
Model selection. We finetune the model for 5
epochs on the training set and save the best check-
point with the lowest dev loss. We select the best
mixing ratio ηand the best pbased on the Mauve
score on the dev set. The search space of ηis
[0.99, 0.9, 0.7, 0.5, 0.3, 0.1, 0.01, 0.0] and that of
pis [0.85, 0.87, 0.89, 0.91, 0.93, 0.95, 0.97, 0.99].
Selected best ηs are reported in Table 12 in the
Appendix. Best ps are reported in Table 3. Metric
scores are reported on the test set and are 3-run
averages because sampling is stochastic.
Results. Table 2 shows unbiased sampling results
of models in different sizes and finetuned with dif-
ferent objectives on three datasets. As you can
see,MIXCE-finetuned models usually get worse
perplexity but consistently better diversity, mauve,
and coherence, compared to MLE-finetuned mod-
els. Table 3 shows top- psampling results from
the same models as Table 2. Since perplexity will
not change as the decoding method changes, weWhich is better?
Dataset MIXCE MLE Same
WikiText 135* 85 95
WebText 139* 79 97
WritingPrompts 111 119 85
Table 4: Human evaluation results. The star (*) means
significantly6better ( p <0.01).
instead report the selected best pin this table. It
can be seen that after carefully applying top- psam-
pling, MIXCE-finetuned models work on par with
MLE-finetuned models for diversity, mauve, and
coherence. Nonetheless, the best pforMIXCE
models is always 0.99, while MLE models have
smaller and more diverse ps. This indicates that
MIXCE leads to a less noisy model distribution.
Human evaluation. Besides automatic metrics,
we also conduct a human evaluation. Following Kr-
ishna et al. (2022), we conduct blind A/B test-
ing. We randomly sample 105 examples from each
dataset. For each example, we ask humans to read
two generations from MLE and MIXCE-finetuned
GPT-2 large models, respectively, and the order of
showing these two generations is random. We use
unbiased sampling to get the generations. Then,
we ask them to judge which one is better (or they
are the same) and justify their preference, based on
fluency, coherence, informativeness, and whether it
is sensical. We conduct this evaluation on Amazon
Mechanical Turk and collect 3 responses for each
example. Please refer to Appendix H for more de-
tails and examples. The final results are shown in
Table 4. As you can observe, MIXCE-finetuned
models significantly outperform MLE-finetuned
models on both WikiText and WebText domains,
while the two methods perform similarly on Writ-
ingPrompts. It is also worth noting that, compared
to the results shown in Table 2, none of the 4 au-
tomatic metrics share the same trend with human
evaluation.
4.3 Robustness & Analysis
Varying training data sizes. We test 3 other
training data sizes: 10K, 25K, and 100K using
GPT-2 small. Table 5 in the Appendix contains the
results, and it shares the same story trend as Table 2:
MIXCE-finetuned models get worse perplexity but
in general work better than MLE-finetuned models
for diversity, mauve, and coherence.
6The significance test is conducted following the bootstrap
test setup (Efron and Tibshirani, 1994).Varying ηand max generation length. To exam-
ine how the mixing ratio ηand the max generation
length affect the performance, we show the mauve
score curves on the dev set in Figure 4. The x-axis
is the mixing ratio ηfrom 0 to 1 ( MIXCE=MLE
when η= 1), and the y-axis is the mauve score
with different max generation lengths (128, 320,
and 512). First, reasonable performances are usu-
ally observed when η≥0.1, and only training the
models with approximated reverse CE (i.e., η= 0)
leads to degeneration. Second, the advantage of
MIXCEis more prominent when the max genera-
tion length is longer.
Controlled Mauve. The max generation length
is not the actual text length because when sampling
from the model, EOS can be generated at any step.
We find that the actual text length can affect the
mauve computation. Even if we truncate all texts
to the same length, the incompleteness caused by
truncation can be another confounding factor. Both
text length and text completeness are irrelevant to
text quality but can be used by mauve to distinguish
model generations from human texts. Therefore, to
eliminate the influence of these confounding fac-
tors, we propose a controlled mauve (orc-mauve )
computation approach. Concretely, for human texts
and model generations, we randomly sample 10K
L-length text fragments from each of these two
sets.Lis the number of tokens. Then, we compute
the mauve between these two sets of fragments.
Table 8 shows the results. As you can see, c-mauve
scores are in general very high ( ≥0.90), which
may indicate that, after controlling the confounding
factors, the ability of mauve to distinguish model
text from human text has been weakened. MIXCE
still gets better performance than MLE in most
cases. Besides, we also compute controlled coher-
ence in the same fashion, and MIXCEretains its
advantage. Please refer to Appendix F.4 for more
details about controlled Mauve and Coherence.
5 Conclusion
We propose a novel training objective, MIXCE, for
autoregressive language modeling. MIXCEcom-
bines forward and reverse cross-entropies, which
can be viewed as combining two complementary
driving forces for better fitting the model distribu-
tion to the data distribution. We demonstrate the
superiority of MIXCEover MLE in both synthetic
and real settings via both automatic and human
evaluations. In the future, MIXCEcan be poten-tially used for pretraining language models.
Acknowledgments
We thank anonymous reviewers for their valuable
comments. We thank Xiang Zhou for the help-
ful discussions. This work was supported by a
Bloomberg Data Science Ph.D. Fellowship.
Limitations
One apparent disadvantage of MIXCEis the mix-
ing ratio η. As shown in Table 12 and Figure 4, the
bestηchanges as the experimental setting changes.
It may be because we use mauve as the model se-
lection criteria or because different datasets have
different noise levels. In general, we do not have a
good answer to which ηshould be used. The ideal
solution is to select ηbased on the performance of
the development set like what we did. However, in
pretraining settings, it is too expensive to search
over multiple ηs. Therefore, how to find a univer-
salηor how to determine ηautomatically is an
important problem to resolve before MIXCEcan
be reliably used for pretraining.
As we mentioned in § 1, language degenera-
tion of open-ended generation shows two distinct
patterns: the nonsensical text from unbiased sam-
pling and the repetition loops from greedy search.
Though MIXCEhelps improve the performance of
sampling, we still see repetition loops when using
greedy search.
Ethical Considerations
As the OpenAI team pointed out, GPT-2 does not
distinguish fact from fiction, so it can not support
use cases that require the generated text to be true.
Additionally, GPT-2 reflect the biases inherent to
the systems they were trained on, so it can not be
deployed into systems that interact with humans
unless the deployers first carry out a study of bi-
ases relevant to the intended use case. Though
ourMIXCE-finetuned GPT-2 gets improved per-
formance with respect to the metrics we used, the
above statement still holds. At this point, we are
not sure whether MIXCEcan help improve fac-
tuality or lead to less biased generations, but we
are sure that the generations still have non-factual
content and biases.References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd International
Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference
Track Proceedings .
Sourya Basu, Govardana Sachitanandam Ramachan-
dran, Nitish Shirish Keskar, and Lav R. Varshney.
2021. {MIROSTAT}: A {neural} {text} {decoding}
{algorithm} {that} {directly} {controls} {perplexity}.
InInternational Conference on Learning Representa-
tions .
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Advances in Neural Information Processing Systems ,
volume 13. MIT Press.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Proceedings of the 34th International Conference on
Neural Information Processing Systems , NIPS’20,
Red Hook, NY , USA. Curran Associates Inc.
Massimo Caccia, Lucas Caccia, William Fedus, Hugo
Larochelle, Joelle Pineau, and Laurent Charlin. 2020.
Language gans falling short. In International Confer-
ence on Learning Representations .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .
Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara
Meister, Jason Eisner, and Ryan Cotterell. 2022. A
measure-theoretic characterization of tight language
models. arXiv preprint arXiv:2212.10502 .
Bradley Efron and Robert J Tibshirani. 1994. An intro-
duction to the bootstrap . CRC press.
Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding
all you need? the inadequacy of the mode in neural
machine translation. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics ,
pages 4506–4520, Barcelona, Spain (Online). Inter-
national Committee on Computational Linguistics.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers) ,
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C
Courville, and Yoshua Bengio. 2014. Generative
adversarial nets. In NIPS .
Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang.
2019. Unifying human and statistical evaluation for
natural language generation. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers) , pages 1689–1701, Minneapolis, Min-
nesota. Association for Computational Linguistics.
John Hewitt, Christopher D. Manning, and Percy Liang.
2022. Truncation sampling as language model
desmoothing. In Findings of the Conference on
Empirical Methods in Natural Language Processing
(Findings of EMNLP) .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text de-
generation. In International Conference on Learning
Representations .
Ferenc Huszár. 2015. How (not) to train your generative
model: Scheduled sampling, likelihood, adversary?
arXiv preprint arXiv:1511.05101 .
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categori-
cal reparameterization with gumbel-softmax. In 5th
International Conference on Learning Representa-
tions, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings . OpenReview.net.
Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang,
and Minlie Huang. 2023. Tailoring language gener-
ation models under total variation distance. In The
Eleventh International Conference on Learning Rep-
resentations .
Daniel Kang and Tatsunori B. Hashimoto. 2020. Im-
proved natural language generation via loss trunca-
tion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
718–731, Online. Association for Computational Lin-
guistics.
Kalpesh Krishna, Yapei Chang, John Wieting, and Mo-
hit Iyyer. 2022. Rankgen: Improving text generation
with large ranking models. In Empirical Methods in
Natural Language Processing .Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck,
Y-Lan Boureau, Kyunghyun Cho, and Jason Weston.
2020. Don’t say that! making inconsistent dialogue
unlikely with unlikelihood training. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 4715–4728, Online.
Association for Computational Linguistics.
Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,
Jason Eisner, Tatsunori Hashimoto, Luke Zettle-
moyer, and Mike Lewis. 2022. Contrastive decoding:
Open-ended text generation as optimization.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The concrete distribution: A continuous re-
laxation of discrete random variables. In 5th Inter-
national Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings . OpenReview.net.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan
Cotterell. 2022. Locally typical sampling. Transac-
tions of the Association for Computational Linguis-
tics, abs/2202.00666.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In International Conference on Learning Repre-
sentations .
Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-
nock `y, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Interspeech ,
volume 2, pages 1045–1048. Makuhari.
Kevin P Murphy. 2012. Machine learning: a probabilis-
tic perspective . MIT press.
Myle Ott, Michael Auli, David Grangier, and
Marc’Aurelio Ranzato. 2018. Analyzing uncertainty
in neural machine translation. In Proceedings of the
35th International Conference on Machine Learn-
ing, volume 80 of Proceedings of Machine Learning
Research , pages 3956–3965. PMLR.
Richard Yuanzhe Pang and He He. 2021. Text genera-
tion by learning from demonstrations. In ICLR .
Krishna Pillutla, Lang Liu, John Thickstun, Sean
Welleck, Swabha Swayamdipta, Rowan Zellers, Se-
woong Oh, Yejin Choi, and Zaid Harchaoui. 2022.
Mauve scores for generative models: Theory and
practice. arXiv preprint arXiv:2212.14578 .
Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,
John Thickstun, Sean Welleck, Yejin Choi, and Zaid
Harchaoui. 2021. Mauve: Measuring the gap be-
tween neural text and human text using divergence
frontiers. Advances in Neural Information Process-
ing Systems , 34:4816–4828.
Vadim Popov and Mikhail Kudinov. 2018. Fine-
tuning of language models with discriminator. arXiv
preprint arXiv:1811.04623 .Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog.
Felix Stahlberg and Bill Byrne. 2019. On NMT search
errors and model errors: Cat got your tongue? In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3356–
3362, Hong Kong, China. Association for Computa-
tional Linguistics.
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-
peng Kong, and Nigel Collier. 2022. A contrastive
framework for neural text generation. In Advances
in Neural Information Processing Systems .
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
Advances in neural information processing systems ,
27.
Richard S Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 1999. Policy gradient methods
for reinforcement learning with function approxima-
tion. Advances in neural information processing
systems , 12.
L Theis, A van den Oord, and M Bethge. 2016. A
note on the evaluation of generative models. In In-
ternational Conference on Learning Representations
(ICLR 2016) , pages 1–10.
Ramon Van Handel. 2014. Probability in high dimen-
sion. Technical report, PRINCETON UNIV NJ.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2020. Neu-
ral text generation with unlikelihood training. In
International Conference on Learning Representa-
tions .
Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine learning , 8(3):229–256.
Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and
Arvind Neelakantan. 2021. Trading off diversity and
quality in natural language generation. In Proceed-
ings of the Workshop on Human Evaluation of NLP
Systems (HumEval) , pages 25–33, Online. Associa-
tion for Computational Linguistics.
Shiyue Zhang, Shijie Wu, Ozan Irsoy, Steven Lu, Mohit
Bansal, Mark Dredze, and David Rosenberg. 2023.
MixCE: Training autoregressive language models
by mixing forward and reverse cross-entropies. InProceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 9027–9050, Toronto, Canada.
Association for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open pre-
trained transformer language models.
Ozan ˙Irsoy. 2019. On expected accuracy. arXiv preprint
arXiv:1905.00448 .
Appendix
A Connection to Pang and He (2021)
In Section 3.2, we introduce an approximation of
the reverse cross-entropy (CE) objective. Similarly,
Pang and He (2021) also propose to approximate
reverse CE, and the resulting GOLD algorithm is
similar to our Equation 9. Here, we would like to
clarify the difference and connection.
The following equation is the start policy gradi-
ent equation used by Pang and He (2021).
Eτ∼πθ[X
t∇θlogπθ(at|st)ˆQ(st, at)]
They used different notations from ours. πθis the
same as our Qθ, i.e., πθ(at|st)is the same as our
Qθ(xt|x<t).ˆQis the accumulated future reward
from timestamp t,PT
t′=tγt′−trt′,γis the decay
factor and rt′is the reward for each step. We will
discuss ˆQin detail later.
Then, they apply importance sampling to sample
from a different behavioral policy πb. Since they
also use examples from the training set, their πbis
the same as our human (or data) distribution P.
Eτ∼πb[X
twt∇θlogπθ(at|st)ˆQ(st, at)]
wtis the importance weight. They use a per-action
approximation: wt≈πθ(at|st)
πb(at|st), which is similar to
how we get Equation 9 from Equation 8.
Since πbis unknown, they assume a uniform
distribution: πb≈1/N(Nis the number of train-
ing examples). Hence, their final approximated
gradient is:
Eτ∼πb[X
tπθ(at|st)∇θlogπθ(at|st)ˆQ(st, at)]
They define rt′andˆQin three ways. The first is
called δ-reward, i.e., ˆQ= 1. In this case, theirfinal gradient is exactly the same as our Equation 9.
However, as you can see, we take a different path
of derivation. Instead of using this δ-reward, our
ˆQis the sequence-level reward P(x). The reward
P(x)nicely helps us to switch from the expectation
ofQθto the expectation of P(from Equation 5
to Equation 7). Therefore, without assuming a
uniform distribution of πb, ourπbis just P.
When using the other two rewards, they also
need to know P. To address this, they use an MLE-
pretrained model as a proxy of P.
Overall, we introduce a different derivation ap-
proach for approximating reverse CE. Moreover, as
we mentioned in § 2.3, Pang and He (2021) focused
on improving controlled generation tasks where the
focus is on the quality of the text, while we focus
on open-ended generations where quality and diver-
sity are both important. Therefore, we mix reverse
CE with forward CE to form our MIXCElearning
objective.
B Alternative Derivation of Reverse CE
Here we propose another derivation of approximate
reverse cross-entropy (CE) (Eq. 2, which also re-
sults in Eq. 9). Different from the derivation in
Section 3.2, here we provide a more intuitive in-
terpretation of how we end up with token-level
self-reinforcement (Eq. 9).
Ex∼Qθ[logP(x)] (11)
=Ex∼Qθ"TX
t=1logP(xt|x<t)#
(12)
=TX
t=1Ex∼Qθ[logP(xt|x<t)] (13)
=TX
t=1Ex<t∼QθExt∼Qθ(·|x<t)[logP(xt|x<t)]
(14)
≈TX
t=1Ex<t∼QθExt∼Qθ(·|x<t)[P(xt|x<t)](15)
≈TX
t=1Ex<t∼PExt∼Qθ(xt|x<t)P(xt|x<t)(16)
Then, we calculate the gradient as follows:
TX
t=1Ex<t∼P∇θExt∼Qθ(xt|x<t)P(xt|x<t)
(17)500 1000 1500 2000 2500
sequence-level neg. log-likelihood010002000300040005000number of exampleswikitext
human
sample
greedy
0 1000 2000 3000
sequence-level neg. log-likelihood010002000300040005000webtext
human
sample
greedy
0 1000 2000 3000
sequence-level neg. log-likelihood010002000300040005000writingPrompts
human
sample
greedy
1 2 3 4 5
token-level neg. log-likelihood010002000300040005000number of exampleswikitext
human
sample
greedy
0 2 4 6
token-level neg. log-likelihood010002000300040005000webtext
human
sample
greedy
0 2 4 6
token-level neg. log-likelihood010002000300040005000writingPrompts
human
sample
greedyFigure 2: The histograms of sequence-level and token-level negative log-likelihoods of human texts and model
generations from GPT-2 large.
=TX
t=1Ex<t∼PExt∼P(xt|x<t)[Qθ(·)∇θlogQθ(·)]7
(18)
=TX
t=1Ex∼P[Qθ(xt|x<t)∇θlogQθ(xt|x<t)]
(19)
=Ex∼P[TX
t=1Qθ(xt|x<t)∇θlogQθ(xt|x<t)]
(20)
In this derivation, we make two approximation
steps. Following the same idea as Eq. 3, we get
Eq. 15 by substituting expected log-likelihood with
expected accuracy ( ˙Irsoy, 2019); but differently,
here, they are log-likelihood or accuracy of tokens
at step t. Then, we propose to approximate the
model prefix ( x<t∼Qθ) by human prefix ( x<t∼
P), which results in Eq. 16. The final Eq. 20 is
exactly the same as Eq. 9. It is also worth noting
that, compared to the derivation in Section 3.2, in
this alternative derivation, we approximate the loss
7Qθ(·) =Qθ(xt|x<t)function instead of the gradient, which is more
intuitive in the sense that we know what we are
exactly optimizing.
C Relation to ˙Irsoy (2019)
In our derivation described in Section 3.2, we ap-
proximate Eq. 2 by Eq. 3. However, after this step,
it is no longer reverse CE anymore; instead, it is
optimizing the expected accuracy of model Qθbe-
causeEx∼Qθ[P(x)] =Ex∼P[Qθ(x)]. Optimizing
the expected accuracy is the exact proposal of ˙Irsoy
(2019) for conducting classification tasks. Quot-
ing their original words, they think the benefit of
expected accuracy over expected log-likelihood is
“instead of prioritizing correction of those instances
that we perform very poorly on, by weighing prob-
ability errors equally, we might just be able to push
more instances to the other side of the decision
boundary.” This is the same motivation of our
work: we hope the reverse CE loss (with or with-
out approximation) can focus on “quality” rather
than “diversity”. Meanwhile, since expected ac-
curacy has close-to-zero derivatives for examples
that we are the most incorrect on, ˙Irsoy (2019) also
proposes to mix expected accuracy with expectedWikiText WebText WritingPrompts
Data Size Objective ppl div mauve coh ppl div mauve coh ppl div mauve coh
Human - 0.89 1.0 0.628 - 0.84 1.0 0.633 - 0.85 1.0 0.473
10KMLE 29.23 0.91 0.60 0.537 22.03 0.88 0.82 0.542 30.40 0.88 0.74 0.385
MIXCE 36.70 0.88 0.93 0.546 22.79 0.83 0.86 0.562 30.65 0.87 0.81 0.395
25KMLE 27.90 0.91 0.68 0.545 21.75 0.88 0.86 0.547 29.37 0.88 0.79 0.394
MIXCE 35.73 0.88 0.94 0.562 21.97 0.85 0.88 0.561 29.67 0.86 0.86 0.401
100KMLE 25.93 0.90 0.69 0.559 21.31 0.87 0.90 0.556 27.63 0.87 0.88 0.401
MIXCE 34.13 0.87 0.93 0.575 21.58 0.85 0.92 0.566 28.01 0.85 0.90 0.409
Table 5: Unbiased sampling results of GPT-2 small models finetuned by MLE or MIXCEon three datasets of
different training data sizes. All metrics are the closer to the human scores the better. Bold numbers are the ones
that are closer to human scores in each setting.
log-likelihood, which is similar to our MIXCEob-
jective.
But different from their work, we focus on text
generation instead of classification. We need an-
other critical approximation step (either Eq. 8 to
Eq. 9 in Section 3.2 or Eq. 16 in Appendix B) to
arrive at the final token-level self-reinforcement
loss function.
D Intuition behind the Self-reinforced
Objective
To further illustrate why this self-reinforced objec-
tive (Equation (8) or (9)) makes sense and their
shortcomings, we conduct an analysis using GPT-2
large (Radford et al., 2019). We first sample 5000
pieces of text from WikiText, WebText, and Writ-
ingPrompts, respectively, and we call them human
texts. Then, using the first 50 tokens of each hu-
man text as a prompt, we get 5000 sampling and
greedy search generations from pretrained GPT-2
large (max generation length = 512). Next, we use
the same model to score human texts and model
generations and get the sequence-level and token-
level negative log-likelihoods. Figure 2 shows the
histograms of these negative log-likelihoods.
In Figure 2, we take the human text histogram
(in blue) as a proxy of human distribution and
the sampling text histogram (in red) as a proxy of
model distribution . As you can see, the support of
model distribution usually contains the support of
human distribution. It supports our previous claim
that MLE-trained models tend to over-generalize.
Meanwhile, at both the sequence and the token
levels, the model on average assigns a higher prob-
ability to human text than to text sampled from
the model. Therefore, when we promote high-
probability sequences or tokens, it is equivalent
to pushing the model distribution toward the hu-
Reverse Cross-EntropyForward Cross-EntropyPQ
x ~ P-logQ(x)   PQ-logP(x)   x ~ QtmpbyryuerDecember 20221 IntroductionMixCEs= ⌘⇤Ex⇠P[logQ✓(x)] (1 ⌘)⇤Ex⇠Q✓[logP(x)] Ex⇠P[logQ✓(x)] Ex⇠Q✓[logP(x)]
1tmpbyryuerDecember 20221 IntroductionMixCEs= ⌘⇤Ex⇠P[logQ✓(x)] (1 ⌘)⇤Ex⇠Q✓[logP(x)] Ex⇠P[logQ✓(x)] Ex⇠Q✓[logP(x)]
1Figure 3: Forward CE only weakly penalizes the model
Qθwhen it puts a small amount of probability mass
ontoP(x) = 0 space. And the loss magnitude is much
smaller than what we will get from reverse CE.
man distribution. However, we need to avoid overly
pushing it to the extremely high-probability region
where greedy search outputs locate (in yellow) be-
cause they are known to be poor-quality and repeti-
tive. Also, as shown in the figure, when promoting
high-probability sequences , even if we overdo it, it
will still be within the support of human distribu-
tion. In contrast, when promoting high-probability
tokens , it can go outside the support of the human
distribution, which is the drawback of Equation (9)
compared to Equation (8).
Lastly, if we train the model only with the self-
reinforced objective till convergence, it is inevitable
to end up with a model that can only output greedy
search generations. Hence, we need to combine it
with the forward cross-entropy.
E Loss Magnitude
As shown in Figure 1, we use reverse cross-entropy
(CE) to provide a driving force for narrowing the
model distribution down when it is broader than the
data distribution. And forward CE is to broaden
the model distribution up. However, it does not
mean forward CE does not have the opposite drive
force because forward CE is minimized if and only
ifQθ(x) =P(x). However, as shown in Figure 3,
the loss magnitude is greatly smaller than the lossmixing ratio 
0.20.40.60.81.0mauve(max length=128)
wikitext
small
medium
large
mixing ratio 
0.20.40.60.81.0
webtext
small
medium
large
mixing ratio 
0.00.20.40.60.81.0
writingPrompts
small
medium
large
mixing ratio 
0.00.20.40.60.81.0mauve(max length=320)
small
medium
large
mixing ratio 
0.00.20.40.60.8
small
medium
large
mixing ratio 
0.00.20.40.60.81.0
small
medium
large
0.0 0.2 0.4 0.6 0.8 1.0
mixing ratio 
0.00.20.40.60.81.0mauve(max length=512)
small
medium
large
0.0 0.2 0.4 0.6 0.8 1.0
mixing ratio 
0.00.20.40.60.8
small
medium
large
0.0 0.2 0.4 0.6 0.8 1.0
mixing ratio 
0.00.20.40.60.8
small
medium
largeFigure 4: The mauve scores obtained by MIXCE-finetuned GPT-2 models on development sets with different max
generation lengths and different η. Note that when η= 1,MIXCEis equivalent to MLE. The x-axis is the mixing
ratioη, and the y-axis refers to mauve scores with different max generation lengths. The 3 lines in each subplot show
the results of GPT-2 models in different sizes. The 3 subplots in each row are the results of 3 datasets respectively.
Unbiased sampling is used as the decoding method. Each dot is the average of 3 runs of sampling and the error bar
shows the standard deviation of 3 runs.
magnitude we get from reverse CE.
F Additional Results
F.1 Additional synthethic experiments
Table 6 shows the results of additional synthetic ex-
periments besides Table 1 in the main paper. Here,
the goal transition matrix Mis randomly initialized
with 10% and 90% zero probabilities.
As the magnitudes of both avg. js and avg. 0s
are fairly small, we examine the 95% confidence
intervals under one synthetic experimental setting –
initializing the transition matrix Mby the bigram
occurrence in the WebText data and setting vocab-
ulary size as 1000. Table 7 contains the results.We can see that 95% confidence intervals are small
enough to maintain the trend of the results.
F.2 Varying training data sizes
Table 5 shows the results of using different training
data sizes in the real-data setting.
F.3 Varying ηand max generation length
Figure 4 illustrates the curves of mauve scores on
the development sets.
F.4 Controlled Mauve and Coherence
We find that the actual length of the text is a con-
founding factor of mauve computation. For ex-
ample, when we compute mauve between a set ofRandom (10%) Random (90%)
V ocab Objective avg. js avg. 0s avg. js avg. 0s
Gold 0.0 0.0 0.0 0.0
20 For. KL 3.65e-4 1.80e-4 7.56e-4 9.10e-5
Rev. KL 3.41e-3 5.56e-6 1.87e-1 1.54e-6
Mix KLs 3.11e-4 7.11e-5 4.01e-4 2.67e-5
JS 5.68e-3 1.17e-5 2.14e-1 5.24e-4
MIXCE* 4.92e-4 1.59e-4 4.87e-4 2.95e-5
MIXCE 3.31e-4 1.57e-4 7.08e-4 8.49e-5
50 For. KL 6.01e-3 1.21e-3 2.18e-3 8.90e-5
Rev. KL 2.03e-2 2.01e-5 4.11e-1 4.55e-6
Mix KLs 4.65e-3 1.29e-4 1.54e-3 3.41e-5
JS 1.03e-1 9.03-5 4.24e-1 1.25e-5
MIXCE* 5.20e-3 6.84e-4 1.48e-3 2.70e-5
MIXCE 5.96e-3 1.20e-3 2.03e-3 7.70e-5
100 For. KL 3.34e-2 2.49e-3 6.98e-3 1.49e-4
Rev. KL 2.30e-1 1.79e-3 5.30e-1 6.25e-6
Mix KLs 2.98e-2 4.66e-4 5.04e-3 6.34e-5
JS 2.38e-1 1.06e-3 5.18e-1 1.32e-3
MIXCE* 3.10e-2 1.73e-3 5.12e-3 6.00e-5
MIXCE 3.29e-2 2.44e-3 7.01e-3 1.50e-5
500 For. KL 1.56e-1 1.57e-3 1.93e-1 8.45e-4
Rev. KL 2.94e-1 9.91e-4 6.49e-1 2.33e-6
Mix KLs 1.55e-1 1.45e-3 1.70e-1 6.83e-4
JS 2.95e-1 9.78e-4 5.75e-1 1.35e-3
MIXCE* 1.55e-1 1.45e-3 1.69e-1 6.71e-4
MIXCE 1.55e-1 1.56e-3 1.88e-1 6.28e-4
1000 For. KL 1.83e-1 8.95e-4 3.65e-1 7.31e-4
Rev. KL 2.86e-1 6.12e-4 6.68e-1 3.88e-6
Mix KLs 1.80e-1 8.64e-4 3.50e-1 6.86e-4
JS 2.88e-1 6.11e-4 5.80e-1 7.73e-4
MIXCE* 1.83e-1 8.64e-4 3.50e-1 6.84e-4
MIXCE 1.83e-1 8.92e-4 3.48e-1 6.71e-4
Table 6: The results of the other two synthetic exper-
iments. Random (10%) and Random (90%) both use
random initialization for M, and 10% and 90% prob-
abilities in Mare 0 respectively. Gold refers to the
results when M′=M. Each value is a 5-seed average.
texts and the same set with an extra new line to-
ken after each text (or the same set with the last
k tokens being truncated), the score will be lower
than 0.01. Though you may think truncating all
texts to the same length can resolve this problem,
we find that the incompleteness caused by trunca-
tion can also be a confounding factor. For instance,
keeping human texts intact, we truncate texts gen-
erated by two systems by their shorter lengths (i.e.,
for each example, we truncate text1 and text2 by
min_length(text1, text2)). Then, the system whose
texts get truncated less will get a greatly larger
mauve score than the other system. Therefore, to
eliminate the influence of these two confounding
factors, we propose a controlled mauve computa-
tion approach. Concretely, for the set of human
textsThand the set of model-generated texts Tm,
we randomly sample 10K L-length text fragments
from each of these two sets. Lis the number of
tokens in each text fragment. After that, we com-WebText
V ocab Objective avg. js avg. 0s
1000 For. KL 8.10e-2 ±2.45e-4 1.50e-4 ±5.58e-7
MIXCE* 7.44e-2 ±2.46e-4 1.14e-4 ±6.15e-7
MIXCE 7.94e-2 ±2.15e-4 1.42e-4 ±5.05e-7
Table 7: Synthetic experimental results with 95% confi-
dence intervals. WebText means initializing Mby the
bigram occurrence in the WebText data.
pute the mauve between these two sets of 10K text
fragments. We denote this controlled mauve as
c-mauve L.
Fh,L={fi
h,L}10K
i=1, fi
h,L∼Th
Fm,L={fi
m,L}10K
i=1, fi
m,L∼Tm
c-mauve L=mauve (Fh,L,Fm,L)
To sample each fragment, we first randomly sample
a text tifrom the set, and then randomly select a
start token s(as long as there are more than L
tokens from sto the end of ti), then the fragment is
ti[s:s+L]. Finally, Table 8 shows the results. We
setL=100, 200, and 300, except that we could
not get 10K 200-token fragments from WikiText
because its texts are shorter.
The Coherence score (Su et al., 2022) computes
the cosine similarity between the prompt and the
continuation. We suspect that the length of the
continuation may affect the score. Therefore, fol-
lowing the same idea of controlled mauve, we also
sample 10K fragments of the same length from
the set of texts for evaluation and compute coher-
ence on the fragments. And for each fragment,
we take the first 50 tokens as the prompt and the
rest as the continuation. Table 9 shows the results.
As you can observe, under this controlled setting,
MIXCE-finetuned models generally achieve better
coherence over MLE-finetuned models.
F.5 Text length of model generations
Though by default we set the max generation length
as 512, the actual text length can vary as the EOS
token can be sampled at any time step. Therefore,
we list the average text length of the human text
and GPT2-large generations in Table 10. We ob-
serve that model generations are always shorter
than human text. Compared to MLE, our MIXCE-
finetuend model produces shorter text on Wiki-
Text while producing longer text on the other two
datasets. We suspect that the shorter length of
MIXCEon WikiText is due to the small mixingWikiText WebText WritingPrompts
Model Size Objective c-mauve 100 c-mauve 100 c-mauve 200 c-mauve 300 c-mauve 100 c-mauve 200 c-mauve 300
Human 0.97 0.96 0.96 0.96 0.96 0.96 0.96
SmallMLE 0.92 0.93 0.92 0.90 0.94 0.94 0.92
MIXCE 0.92 0.94 0.94 0.93 0.95 0.94 0.94
mediumMLE 0.94 0.93 0.91 0.90 0.94 0.94 0.93
MIXCE 0.93 0.95 0.94 0.94 0.95 0.94 0.94
LargeMLE 0.93 0.93 0.93 0.91 0.94 0.94 0.93
MIXCE 0.93 0.94 0.94 0.93 0.95 0.95 0.95
Table 8: Controlled mauve results. Unbiased sampling is used as the decoding method, i.e., using the same model
generations as Table 2. Human scores are not 1 because sampling 10K fragments twice result in two different sets.
Each number is a 3-run average.
WikiText WebText WritingPrompts
Model Size c-coh 100 c-coh 100 c-coh 200 c-coh 300 c-coh 100 c-coh 200 c-coh 300
Human 0.570 0.521 0.583 0.600 0.412 0.470 0.481
SmallMLE 0.504 0.444 0.515 0.535 0.350 0.412 0.429
MIXCE 0.508 0.458 0.524 0.545 0.363 0.422 0.437
MediumMLE 0.518 0.446 0.515 0.535 0.355 0.415 0.432
MIXCE 0.527 0.484 0.546 0.565 0.362 0.425 0.437
LargeMLE 0.521 0.449 0.515 0.536 0.372 0.431 0.447
MIXCE 0.522 0.469 0.531 0.569 0.369 0.434 0.450
Table 9: Controlled coherence results. Unbiased sampling is used as the decoding method, i.e., using the same
model generations as Table 2. Each number is a 3-run average.
WikiText WebText WritingPrompts
Model Size Objective avg. len avg. len avg. len
Human 124.5 304.5 332.5
LargeMLE 114.8 284.2 325.8
MIXCE 89.0 298.9 326.4
Table 10: Unbiased sampling text lengths of models
finetuned by MLE or MIXCEon three datasets. Length
is computed by simply splitting text by whitespaces.
ratio (0.1) chosen based on mauve (see Table 12).
However, we do not think shorter text length leaves
to better mauve, as shown by the other two datasets
and discussed in F.4.
G Best ηs
Table 11 has the best ηs for synthetic experiments.
Table 12 contains the best ηs selected for GPT-2
experiments.
H Human Evaluation Details
We conduct A/B testing (or pairwise comparison)
to compare generations from two models. As
shown in Figure 5, in each job, we give the eval-
uator two text paragraphs (in random order) that
share the same beginning part (the prompt) but
have different continuations. Then, they need to
choose which one they think is better (or non-distinguishable). To avoid random selections, they
are also asked to provide a justification for their
choice. We find this justification not only gives
us additional explanations of their choices but also
helps us easily identify bad workers, because bad
workers tend to use one single justification or sev-
eral repeated justifications.
We instruct them by defining a good text para-
graph as being:
•Fluent : Should have no obviously ungram-
matical sentences, missing components, etc.
that make the text difficult to read.
•Coherent : Should stay on topic with the
prompt and build from sentence to sentence
to a coherent body of information.
•Informative : Should have diverse and inter-
esting content.
•Sensical: Should generally make sense.
Since short text has little information and long
text is difficult to read, we only use paragraphs with
5 to 8 sentences for evaluation. If a paragraph has
more than 8 sentences, we truncate it to 8 sentences.
And we remove paragraphs with less than 400 or
more than 2000 characters. Besides, to eliminateModel section is based on avg. js
Random (50%) WebText Random (10%) Random (90%)
V ocab Objective bestη bestη bestη bestη
20 Mix KLs 0.99 0.9 0.99 0.99
JS 0.9 0.9 0.9 0.9
MIXCE* 0.99 0.99 0.99 0.99
MIXCE 0.9 0.99 0.99 0.99
50 Mix KLs 0.99 0.99 0.9 0.99
JS 0.01 0.99 0.9 0.9
MIXCE* 0.99 0.99 0.99 0.99
MIXCE 0.99 0.99 0.99 0.9
100 Mix KLs 0.9 0.99 0.9 0.99
JS 0.01 0.99 0.99 0.01
MIXCE* 0.99 0.99 0.99 0.99
MIXCE 0.5 0.9 0.5 0.99
500 Mix KLs 0.9 0.99 0.99 0.99
JS 0.99 0.99 0.99 0.99
MIXCE* 0.99 0.99 0.99 0.99
MIXCE 0.1 0.5 0.1 0.1
1000 Mix KLs 0.99 0.99 0.99 0.99
JS 0.99 0.99 0.99 0.99
MIXCE* 0.99 0.99 0.99 0.99
MIXCE 0.1 0.5 0.1 0.1
Table 11: The selected best ηof synthetic experiments reported in Table 1 and Table 6. The model section is based
on avg. js.
Model section is based on mauve (max length=512) on dev set
WikiText WebText WritingPrompts
Model Size Objective bestη bestη bestη
Small M IXCE 0.1 0.5 0.5
Medium M IXCE 0.1 0.3 0.5
Large M IXCE 0.1 0.3 0.7
Table 12: The selected best ηof GPT-2 experiments reported in Table 2. The model section is based on mauve (max
length=512) on the dev set.
the influence of length difference, we do not select
examples whose length difference between two
paragraphs is more than 1 sentence or more than
200 characters.
We conduct this evaluation on Amazon Mechan-
ical Turk. We only allow workers, who are located
in the US, have a Masters Qualification,8have an
approval rate larger than 97%, and have more than
10000 HITs approved, to do our tasks. In addition,
we first ran a testing batch, then manually checked
the results, and selected 44 qualified workers to
continue doing the rest of our tasks.
For each of the 3 datasets, we sampled 105 ex-
amples and collected 3 responses per example. In
total, we received 945 human evaluations. We pay
workers $1 per response, and it takes around 5 min-
8https://www.mturk.com/worker/helputes to finish one response, i.e., the hourly rate is
around $12.
Table 13 shows that inter-annotator agreements.
Figure 6-11 are 6 randomly sampled examples from
human evaluation results, 2 examples per dataset.
I Reproducibility
In our GPT-2 experiments, we use English text data
from 3 domains: (1) WikiText (Merity et al., 2017):
text from Wikipedia, and we use wikitext-103-raw-
v1 from Hugging Face.9Its license is Creative
Commons Attribution-ShareAlike License (CC BY-
SA 4.0). (2) WebText (Radford et al., 2019): text
from the Web. It was used for pretraining GPT-2.
The full WebText is not available but they released
9https://huggingface.co/datasets/
wikitextDataset all agree 2 agree no agreement
WikiText 24% 59% 17%
WebText 24% 52% 24%
WritingPrompts 20% 70% 10%
Table 13: Inter-annotator agreement. The numbers are the portions of examples that have a 3-annotator agreement
(all agree), a 2-annotator agreement (2 agree), or no agreement. E.g., 24% of examples used in human evaluation
for WikiText have an agreement among 3 annotators.
a subset on Github10. The GitHub repository con-
tains an MIT license, and they did not specify the li-
cense of the data. But they indicated in the readme:
“We look forward to the research produced using
this data!” (3) WritingPrompts (Fan et al., 2018)11:
text from the writing prompts forum of Reddit. Its
GitHub repository also contains an MIT license
without specification of the data license. However,
WritingPrompts has been used by many other re-
search works, e.g., Pillutla et al. (2021). We use
their official dev and test sets as much as possible.
If they have fewer than 5K examples, we sample
from their official training set to make up the rest.
All of our experiments were conducted on
NVIDIA Tesla V100 32G GPUs. We use a sin-
gle GPU to run each experiment and change the
batch size to fit models of different sizes. When
fine-tuning GPT-2 small using a single GPU with
MLE or MIXCE, it took less than 1 hour to finish 5
epochs on 50K WikiText training data and took less
than 2 hours to finish 5 epochs on 50K WebText or
WringPrompts training data.
We implemented our GPT-2 based models based
on the GPT-2 modeling code from Hugging Face
Transformers12. For training and evaluation, we
modified the example script of causal language
model training13. We used the default optimizer,
learning rate, scheduler, etc. in that script. But
we set the maximum training epochs as 5 and
changed the batch size and gradient accumulation
steps based on the model size to fit it in one 32G-
memory GPU.
10https://github.com/openai/
gpt-2-output-dataset
11https://github.com/facebookresearch/
fairseq/tree/main/examples/stories
12https://github.com/huggingface/
transformers/blob/main/src/transformers/
models/gpt2/modeling_gpt2.py
13https://github.com/huggingface/
transformers/blob/main/examples/pytorch/
language-modeling/run_clm_no_trainer.pyFigure 5: Human evaluation interface and a random example from our collected human annotations.Paragraph1 (MLE) :
Within minutes of issuing the signal and turning his flagship HMS Queen Charlotte, Howe’s plan began to falter.
Many of the British captains had either misunderstood or ignored the signal and were hanging back in the original line.
Other ships were still struggling with damage to their ships and other equipment caused by the storm, so that Australia’s
war effort was threatened. In response to the storm-ravaged Australian distress call on 12 March, Howe ordered his ship
HMS Sun Babies as flagship of rowing party V HMNZS Platypus. He assigned elevensacks from the ship to the crew for a
rescue, and placed much of the more aged of his crew and stockmen on sick leave on the advice of Admiral Rear-Admiral
John Forbes.
Paragraph2 (M IXCE):
Within minutes of issuing the signal and turning his flagship HMS Queen Charlotte, Howe ´s plan began to falter. Many
of the British captains had either misunderstood or ignored the signal and were hanging back in the original line.
Other ships were still struggling with damage to their yards and were standing still. Another officer, Lieutenant Colonel
Strother, privately stated that the British ship captains had "obsessed [ed] the whole thing over with the consciousness of the
impossibility of victory". Commander Howe was eager to proclaim victory, but it was not until the Chief of the General Staff,
Sir Charles Ogilvy, finally appeared on the scene that he decided to make a decision.
Choice : paragraph2
Justification : Paragraph 1 has details that do not flow with each other as if Howe was involved with two ships at the same
time. The beginning of the paragraph mentions Howe’s flagship HMS Queen Charlotte and later on his flagship HMS
Sun Babies. It is not clear about the role of Australians whether there were Australians in the crew or they were helping
Australian ships. In paragraph 2, there is no mention of another nation other than just British. It has details of different ranks
of commands. Moreover the paragraph focus on failure and uncertainty that flow to the rest of the paragraph.
Choice : paragraph2
Justification : Paragraph 2 seems to make more sense and is more coherent than Paragraph 1. #2 has a narrative flow that can
be read from start to finish, giving the reader a sense of what is happening. The quote is the weak point, but as a quote, can be
taken as it is given here. On the other hand, Paragraph 1 can also be read from start to finish, but makes less sense, and seems
less coherent.
Choice : paragraph2
Justification : Paragraph 2 is more coherent, and it wins hands down in terms of making sense. Paragraph 1 has Howe sending
men on leave in the middle of a battle and also has him assigning "elevensacks" to the crew, which isn’t even a real word.
Figure 6: WikiText example 1.
Paragraph1 (MLE) :
In imitation of the Greeks, Nero built a number of gymnasiums and theatres. Enormous gladiatorial shows were
also held. Nero also established the quinquennial Neronia. The festival included games, poetry, and theater. For
the recesses of the Calendrical Mysterium, the city quinquenored, with the city many times superseded.[ndx] Nero also
Festina-Illuminatae in honour of the doomed Emperor Caeser and his son Severus, the founders of the Flavian dynasty, who
were slain in that period. In Britain, Nero erected the Attic theatre.
Paragraph2 (M IXCE):
In imitation of the Greeks, Nero built a number of gymnasiums and theatres. Enormous gladiatorial shows were also
held. Nero also established the quinquennial Neronia. The festival included games, poetry, and theater. In order to
combat the influence of the Venetian party, the local aristocracy discouraged the shooting of arrows at the official spectators.
Nero’s penchant for shooting arrows became a local insult.
Choice : paragraph2
Justification : The first one is hard to understand. What does " the city quinquenored, with the city many times super-
seded.[ndx]" mean.? It does not make sense. The second one is easy to understand.
Choice : paragraph2
Justification : I had to read Paragraph 1 a few times before it made sense, but Paragraph 2 flowed nicely and logically without
effort.
Choice : paragraph2
Justification : 2 takes a bit of a stretch but it can at least follow some plausible logical order. 1 just abruptly starts talking
about geopolitics which doesn’t make sense in that context.
Figure 7: WikiText example 2.Paragraph1 (MLE) :
Two years ago, Sandrine Jourdan walked up a friend’s Sunshine Coast driveway and simply disappeared. The
Caboolture woman has not accessed her bank accounts, used her phone or wished any of her three children a happy
birthday since then. She insists she hasn’t moved house, childcare is still open and she can speak to her family now. "I owe
them no money." Jourdan said. Jourdan was last heard from around February this year and her family does not know where
she is. One of her family members told me Jourdan was hers but to leave Jourdan alone would be like asking the sick to go to
the doctor. However, the woman, who is in her 50s, said Jourdan was extremely important to her family and they didn’t want
to send her any money. They saw her daily and had almost no tensions of any kind to handle
Paragraph2 (M IXCE):
Two years ago, Sandrine Jourdan walked up a friend’s Sunshine Coast driveway and simply disappeared. The
Caboolture woman has not accessed her bank accounts, used her phone or wished any of her three children a happy
birthday since then. But she has no one to blame but herself. The welfare office at Taree hostel was "terminating her
accommodation on a whim" last Spring, she says. Discarded money The hostel is not Charter and cannot employ staff.
Desperate for a fresh start, Sandrine tried to access her bank accounts and managed to access her daughter’s allowance, the
bequests of her parents, a benefit card and money in savings. She has not seen her family since. To compound her plight, she
has lost her job on the condominium project near the town of Machalung where she was staying
Choice : non-distinguishable
Justification : Both are bad and neither follow the prompt. Paragraph 1 isn’t coherent. She is on good terms with her family,
then not, then back again. Paragraph 2 also has contradictions, and the sentence "Discarded money The hostel is not Charter
and cannot employ staff. " seems randomly inserted.
Choice : non-distinguishable
Justification : Neither follows the prompt in the least. It is about a woman who disappears, and both go on to have quotes and
information about the missing woman as if she were still interacting with others. There are incomplete sentences in 2 which
are not present in 1, but I still can’t say 1 is better because it’s so off prompt.
Choice : paragraph1
Justification : Paragraph 2 has some punctuation and grammar errors, for example "Discarded money The hostel is not
Charter and cannot employ staff."
Figure 8: WebText example 1.
Paragraph1 (MLE) :
Another Phantom is leaving the building. The musical, which is the longest running show in Broadway history,
announced Monday that the Phantom, James Barbour, will leave the production on Dec. 23, 2017, after a close to
three-year run. A premature announcement about the closure came shortly after the show was first announced at the 2016
Tony Awards. A representative for JBL had told The Hollywood Reporter’s Hayden Dingman last year that the Broadway
appearance would likely wrap but that there were still plans to continue performing on the stage. The event marked the end of
the run of ’The Phantom’ which reached a two-decade high in the Broadway season. The show had run a whopping 129
shows on Broadway since its inception in 1995. After 11 seasons, Ice Cube’s ’Once’, which had been announced 14 months
prior, was a box office success
Paragraph2 (M IXCE):
Another Phantom is leaving the building. The musical, which is the longest running show in Broadway history,
announced Monday that the Phantom, James Barbour, will leave the production on Dec. 23, 2017, after a close to
three-year run. "Moving on to a new chapter with a new partner is one of the hardest things we’ve ever done as a show,"
said producer Scott Ploog. "Joining forces with our fearless leader James was a huge statement to us." After singing and
dancing its way through the Fort Worth Convention Center in 2011, the show was working on a script for its sixth season.
The Phantom surprised the show’s production staff with a call from James, who was the project’s conductor for the years it
was running. "After hearing the love he had for the show, I knew whether I wanted to take a job as indispensable leader of the
cast or what, I had to decide," said Expedition leader Cary Joji. "He was just engaging, embracing and completely out of his
ways, but he was relentless
Choice : paragraph2
Justification : Paragraph 2 feels more personal with more quotes from the people that worked on the show.
Choice : paragraph2
Justification : Both do well on fluency. For coherence, I don’t think either one forms a totally coherent body of information,
but Paragraph 2 sticks to the topic a touch better. I think they’re roughly equal in terms of diverse and interesting content. As
usual, making sense is the metric each is worst at. I believe they are indistinguishably terrible in that regard. So Paragraph 2
wins, just barely, via being slightly more coherent.
Choice : paragraph2
Justification : The first paragraph seems to be a mix of unrelated items, at first discussing a performer leaving the cast of
Phantom and then talking about the end of the production and then concluding with "After 11 seasons, Ice Cube’s ’Once’,
which had been announced 14 months prior, was a box office success", which seems unrelated to Phantom. The second
paragraph seemed little off also, but had somewhat more coherence. The first paragraph also lacked a couple comma, by rule,
but I didn’t factor that in very much.
Figure 9: WebText example 2.Paragraph1 (MLE) :
[WP] There is no such thing as “dating ” anymore. Just like in Cinderella, As soon as you touch the hand of“ the one”
you have an insatiable urge to kiss. Sam sighed. Despite her best efforts to hide it, the bruises and cuts beneath her blouse
were pretty apparent from her sitting down. Her heart pierced so many ribs that buckles broke, and chips torn the flesh of her
ribs, arm and leg. And they weren’t just sitting there - it was pouring out down the side of the castle. When they arrived back
at the villa after a short- but exhausting trip from the train station, they were determined to hug and kiss, just as well as they
could in dark alleys. Unfortunately, the darkness blinded them from view, so the light was flowing with the pouring rain,
sending sparks from their inexpensive outfits
Paragraph2 (M IXCE):
[WP] There is no such thing as “dating ” anymore. Just like in Cinderella, As soon as you touch the hand of“ the
one” you have an insatiable urge to kiss. Sam sighed. Despite her best efforts to reverse the trend, her hand seemed to
instinctively take the hand of the lone participant. He smiled, as Sam closed her eyes. "Today’s the day you know, Yattar,"
he insisted confidentially. "Today’s the day you know the thrill of kissing!" Yattar nodded, with conviction. "Anyone the
purveyors of-" "Remind me to spice everything up," she dismissed, this time making eye contact with the man her mouth had
caught them
Choice : paragraph2
Justification : Assuming that "Yattar" is a pet name/different name for "Sam", 2 is much more cohesive and fluent than 1. It
is easy to follow the story and the story makes more sense than 1. The "pouring out down the side of the castle" in 1 makes
no sense, and other content is pretty incoherent as well.
Choice : paragraph1
Justification : Paragraph 1 is a more concrete story of love, while 2 feels more disjointed.
Choice : paragraph2
Justification : Paragraph 2 is more cohesive, sticking to the concept introduced in the first sentence and describing the initial
meeting of a couple. Para 1 rather jarringly adds a description of injuries and a castle setting.
Figure 10: WritingPrompts example 1.
Paragraph1 (MLE) :
[CC] I wrote these stories recently here on WP, and would like to share them with my friends; however, I feel like they
could be tuned up a bit. Any thoughts on how to make improvements is greatly appreciated. First off, there are a lot of
inconsistencies in their sentences, it’s good to have a prompt where you can color all the immersion steps individually so you
can judge meaningful sequences when looking at them in context. 2nd, while I did my best to adhere to game mechanics in
the story, they were probably made largely to draw parallels between the reader and protagonist. Seth Vargo Hughes is an
unfortunately short person himself (7 weeks a year usa is somewhere between 8 to 10 years long. I amn’t entirely sure what
his killer skills are now, or about what he can do), so he may get caught with some pretty nasty tools and weapons, and at the
end of the story, he’s not gonna give the guy who rescued him the satisfaction of just putting his fist through his friend’s chest
to make his statement true. But that happens. 2nd, 2 other things that PWP failed to properly convert into told story
Paragraph2 (M IXCE):
[CC] I wrote these stories recently here on WP, and would like to share them with my friends; however, I feel like they
could be tuned up a bit. Any thoughts on how to make improvements is greatly appreciated. First off, there’s a ton of
image and word overlap on this story, which leads to a lot of writing and visibility issues. There definitely should have been
done more body alignment, with the characters looking like more anthropomorphic animal animals more often. Additionally,
this idea if poorly worded, as the phrase "spider" was spelled multiple times throughout the story and is such a terribly named
word. It should be the appropriate sound, and become "f*cking spider"! It also doesn’t help that you are writing in third
person, which means that you could have brought in third person pronouns, such as "She" or "I" (though you may want to
change some of the wording, such as "She"). The last thing I would change about this story is the different lengthy, regular
sentences that end so often. One of the most common techniques used for lengthening stories is italicizing, wherein you
substitute a word with an italicized sound.
Choice : paragraph2
Justification : Paragraph 1 has several grammatical flaws. It also begins to ramble in places. Paragraph 2 includes pertinent
details and completes the main subject.
Choice : paragraph1
Justification : Both are truly terrible on every metric. Paragraph 2 is so problematic on fluency that Paragraph 1 wins despite
using the non-word "amn ´t." As far as coherence and information goes, they are equally dreadful, and neither makes any sense
whatsoever.
Choice : paragraph2
Justification : 1 deviates halfway through the prompt and starts talking about a different subject matter almost seemlessly. It
almost makes sense if you don’t read it very closely.
Figure 11: WritingPrompts example 2.Repetitive generation from greedy search:
Aliens start abducting humans and turning them into "space zombies" in the year 2077. The film is set in the year 2077,
and follows a group of survivors who are forced to live in a space station. The film is directed by Ridley Scott, who also
wrote the script. The film is set to be released in the UK on November 22. The film is the first in a planned trilogy of films
based on the novel by Robert Aickman. The novel is set in the year 2077, and follows a group of survivors who are forced to
live in a space station. The film is set to be released in the UK on November 22. The film is the first in a planned trilogy of
films based on the novel by Robert Aickman. The novel is set in the year 2077, and follows a group of survivors who are
forced to live in a space station.
The scientists have discovered something terrible . .. The scientists have discovered something terrible... The scientists
have discovered something terrible... The scientists have discovered something terrible... The scientists have discovered
something terrible... The scientists have discovered something terrible... The scientists have discovered something terrible...
The scientists have discovered something terrible... The scientists have discovered something terrible... The scientists have
discovered something terrible... The scientists have discovered something terrible... The scientists have discovered something
terrible... The scientists have discovered something terrible... The scientists have discovered something terrible... The
scientists have discovered something terrible... The scientists have discovered something terrible... The scientists have
discovered something terrible... The scientists have discovered something terrible... The scientists
The silence before the storm comes from the people who are most affected by the storm. "The people who are most affected
by the storm are the people who are most vulnerable to the storm," said Dr. David R. Jones, a professor of meteorology at
the University of Alabama at Birmingham. "They are the people who are most likely to be hit by the storm." The storm is
expected to bring heavy rain and strong winds to the Southeast, with the potential for flooding and mudslides. The National
Weather Service said the storm is expected to bring up to a foot of rain to the Southeast, with the potential for up to a foot of
rain in some areas. The storm is expected to bring up to a foot of rain to the Southeast, with the potential for up to a foot of
rain in some areas. The storm is expected to bring up to a foot of rain to the Southeast, with the potential for up to a foot of
Incoherent and nonsensical generations from unbiased sampling:
Aliens start abducting humans but what begins as friendly relations turns sour as a vicious extraterrestrial force emerges
with deadly power, and an army of killer cyborgs, sent back in time by the mysterious alien intelligence known as the
Chronatrix. The first wave of an enemy race, that is coming for humanity. And only the best and bravest of them will survive
the war. Written by James Hake
The scientists have discovered something terrible . this is not a big deal. But the scientists are still in the stage to figure out
what it is. It ´s not just a single thing. We ´re dealing with the biggest problem that mankind has facing it." So, what exactly does
"this is not a big deal" mean? Well, there ´s no way to predict precisely what this "thing" will be. However, the physicists
hope that the discovery will lead to some better method of analyzing supernovas, which could in turn lead to some advances
in astronomy and technology. The discovery was made when an optical telescope belonging to the European Southern
Observatory in Garching, Germany, was studying the aftermath of an exploding star. The supernova that caused the flare was
so powerful that several billion people were nearby, according to the Associated Press, so the astronomers used a method
previously used to observe supernovas from afar in telescopes orbiting Earth. "What we need now’
The silence before the storm comes out of knots, flows easily, and, when it interferes, changes direction with an ebb and
flow of its own accord. In place of ideal styles, this has come to be known as "Easy Song Coloration." Creative Music,
and Learn to Run With it, presents Joshua Kelley Matheny, author of a new book inspired by classic cocktails. Only from
CreativityAsGeek.com, drop in for some free advice on How to Make Bloody Marys, Cranberry Fish Tacos, Mud Pie
Doughnuts, PSL s ´mores (all for less money than a bottle of Wild Turkey), and more. The first hundred requests can be
monitored along with this week ´s theme between midnight and 12PM EST. Then a 100 laker may set sail for one of Prominent
Icons at the Loop and Fillmore for a free party at the green fabric threshold. Author Joshua Kelley Matheny ´s’
Figure 12: Examples of degeneration. Text is generated by GPT2-large (Radford et al., 2019).