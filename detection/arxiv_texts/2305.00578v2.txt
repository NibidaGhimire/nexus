A New Clustering Framework
By Hao Chen and Xiancheng Lin
Department of Statistics, University of California, Davis,
One Shields Avenue, Davis, California, 95616, U.S.A.
hxchen@ucdavis.edu xclin@ucdavis.edu
Summary
Detecting clusters is a critical task in various fields, including statistics, engineering and
bioinformatics. Our focus is primarily on the modern high-dimensional scenario, where traditional
methods often fail due to the curse of dimensionality. In this study, we introduce a non-parametric
framework for clustering that is applicable to any number of dimensions. Simulation results
demonstrate that this new framework surpasses existing methods across a wide range of settings.
We illustrate the proposed method with real data applications in distinguishing cancer tissues
from normal tissues through gene expression data.
Some key words : Curse of dimensionality, Multivariate analysis, High-dimensional data, Gaussian mixture, Gene
microarray.
1. Introduction
Cluster analysis, or clustering, is a fundamental problem in statistics. Broadly speaking, the
task is to group a set of objects into clusters in such a way that the objects in one cluster are
more “similar” to each other than to those in other clusters. The definition for “similar” can be
somewhat arbitrary. In the following, we discuss this through a few toy examples and establish
the criterion we will use for the paper.
1.1. What is “similar”?
Here are some toy examples in two dimensions. Figure 1 shows the data colored according
to their generating schemes. We applied two classical clustering algorithms to these data: the
(a) (b) (c) (d) (e)
Fig. 1: Examples of different shapes of two-dimensional data, with different colors representing
different data generating schemes: (a) three samples generated from three bivariate Gaussian
distributions; (b) one sample from a bivariate Gaussian distribution and another from a parabola
with perturbations; (c) two parabolas with perturbations; (d) two rings with perturbations; and
(e) two logarithmic spirals with perturbations.
1arXiv:2305.00578v2  [stat.ME]  15 Aug 20242
(a) (b) (c) (d) (e)
Fig. 2: Clustering results based on the standard 𝑘-means approach (top row) and the spectral
clustering (bottom row) for the five datasets in Figure 1.
standard𝑘-means clustering algorithm ( kmeans function in R) and spectral clustering ( specc
function in the Rpackage kernlab ). The clustering results are shown in Figure 2.
The clustering results based on spectral clustering are much more satisfying. Hence, even
though the definition of “similar” can be somewhat arbitrary, one likely wants to separate obser-
vations from different data generating schemes into distinct clusters. Generally speaking, cluster
analysis is useful when the dataset consists of a mixture of observations from different underlying
data-generating schemes or models, and we aim to partition the observations into clusters such
that each cluster can be modeled simply , for instance, using a function or distribution. This allows
us to summarize the dataset concisely and implement different treatments for different clusters
according to their properties. In the examples above, the 𝑘-means approach minimizes the within-
cluster sum of squares (group ‘similar’ points to some extent); however, it does not facilitate the
follow-up task of understanding the observations in each cluster, making it less useful.
In the following, we refer to points that can be represented by a relatively simple distribution
or model as one cluster, and our goal is to group observations that are similar in underlying
distribution or model .
1.2. Difficulties in clustering high-dimensional data
In Section 1.1, spectral clustering performs well across all examples. However, this may not
be the case when the dimensionality of the data is high. Consider a scenario where the data is
a mixture of observations from two distributions, N𝑑(0,Σ)andN𝑑(𝑎1,𝑏Σ), with𝑑=800 and
Σ𝑖𝑗=0.1|𝑖−𝑗|, whereΣ𝑖𝑗is the(𝑖,𝑗)th element of Σ. Suppose there are 50 observations in each
cluster, and the goal is to separate them. We examine two simulation settings:
•Setting 1 (only mean differs): 𝑎=0.25,𝑏=1.
•Setting 2 (only variance differs): 𝑎=0,𝑏=1.2.
For spectral clustering, we use an advanced choice, kpar = ‘local’ , in the specc function
to avoid unexpected errors encountered with high-dimensional data (details in Supplement S1).
Spectral clustering encompasses various methods, each differing in their approach to constructing
similarity graphs and handling data complexity. Therefore, in addition to specc , we include the
method described in Chapter 14.5.3 of Hastie et al. (2009) (Spectral), which involves constructing3
Table 1: Mis-clustering rates for various clustering methods under two settings. The smallest
mis-clustering rate under each setting is in bold.
Spectral Specc RSpec(A) RSpec(L) IF-PCA 𝑡-SNE
Setting 1 0.157 0.006 0.279 0.167 0.254 0.098
Setting 2 0.467 0.446 0.418 0.371 0.451 0.462
a 10-NN similarity graph and applying conventional spectral clustering. We also include the
Regularized Spectral Clustering method proposed by Rohe et al. (2011) ( reg.SP in the Rpackage
randnet ) to the same similarity graph for both the adjacency matrix (RSpecA) and laplacian
matrix (RSpecL). Additionally, we apply some recently popular clustering methods. IF-PCA,
proposed by Jin & Wang (2016), has shown successes in clustering gene microarray data. 𝑡-SNE
(𝑡-distributed stochastic neighbor embedding) (Maaten & Hinton, 2008) was initially proposed
for data visualization by projecting observations into a two-dimensional space, effectively groups
“similar” observations in this space. Mwangi et al. (2014) proposed to use the 𝑘-means clustering
algorithm on the two-dimensional projected data from 𝑡-SNE. We apply these methods to the
data generated from the two settings above, and the mis-clustering rates from a typical simulation
run are shown in Table 1.
Setting 1 is a relatively straightforward case where the two clusters differ only in mean. In this
scenario, we see that specc achieves the best performance. Setting 2 is more complex, with the
two clusters differing in scale. All methods struggle in this setting. For spectral clustering, it is
closely related to the Mincut problem when observations are represented through a similarity
graph. When the dimension is high and the two clusters differ in scale, the curse of dimensionality
causes the observations in the cluster with larger variance to be more sparsely scattered than those
in the cluster with smaller variance. As a result, for any observation in the cluster with larger
variance, its nearest points tend to be from the other cluster, causing the spectral clustering to
fail. We explore this phenomenon further in Section 2.
In this paper, we propose a new clustering criterion based on theoretical results (Section 2). The
new method is compared with existing methods under various settings in Section 3 and applied
to real data applications in Section 4. The paper concludes with discussions in Section 5.
2. Proposed method
2.1. Graph partition methods review
First, we review the graph clustering problem since our method will be based on similarity
graphs constructed from the observations. The primary graph clustering formulations focus
on graph cut and partitioning problems (Chan et al., 1993; Shi & Malik, 2000). MinCut is an
example of a traditional graph partitioning method. The MinCut method focuses on identifying the
partition of a graph into two disjoint sets by minimizing the number of cut edges. The objective is
to separate the graph into clusters with the fewest possible inter-cluster connections. However, Wu
& Leahy (1993) observed that in many tested graphs, the solutions of this problem are partitions
with isolated nodes in clusters. The Ratio Cut method (Leighton & Rao, 1988) improves upon
MinCut by considering the number of nodes in each clusters, avoiding isolated nodes. Later, the
Normalized Cut (Ncut) method (Shi & Malik, 2000) seeks to minimize the cut relative to the
number of edges in a cluster instead of the number of nodes. It has been shown that relaxing
Ncut leads to normalized spectral clustering, while relaxing RatioCut results in unnormalized4
spectral clustering on the graph laplacian matrix (Von Luxburg, 2007). Spectral clustering has
since become more popular. However, when we investigate the similarity graphs constructed on
the observations and take into account the patterns caused by the curse of dimensionality, some
powerful statistics can be derived.
2.2. Notations
We use𝐺𝑘to denote the set of edges in the 𝑘-nearest neighbor ( 𝑘-NN) graph, and(𝑖,𝑗)∈𝐺𝑘
if node𝑖points to node 𝑗(each node points to its first 𝑘nearest neighbors). Let 𝑥be an𝑁-length
vector of 1’s and 2’s, and define
𝑅1,𝑘(𝑥)=∑︁
(𝑖,𝑗)∈𝐺𝑘𝐼𝑥𝑖=𝑥𝑗=1, 𝑅 2,𝑘(𝑥)=∑︁
(𝑖,𝑗)∈𝐺𝑘𝐼𝑥𝑖=𝑥𝑗=2.
Then,𝑥divides the observations into two clusters: 1’s (Cluster 1) and 2’s (Cluster 2). Let 𝑚be the
number of 1’s in 𝑥and𝑛be the number of 2’s in 𝑥. We define𝑝11,𝑘(𝑥)as one𝑘th of the probability
that an observation in cluster 1 finds another observation in cluster 1 among its first 𝑘nearest
neighbors, i.e., 𝑝11,𝑘(𝑥)=𝑅1,𝑘(𝑥)/(𝑘𝑚). Similarly, we have 𝑝12,𝑘(𝑥)=(𝑘𝑚𝑥−𝑅1,𝑘(𝑥))/(𝑘𝑚),
𝑝21,𝑘(𝑥)=𝑅2,𝑘(𝑥)/(𝑘𝑛),𝑝22,𝑘(𝑥)=(𝑘𝑛−𝑅2,𝑘(𝑥))/(𝑘𝑛). It is clear that
𝑝11,𝑘(𝑥)+𝑝12,𝑘(𝑥)=1, 𝑝 21,𝑘(𝑥)+𝑝22,𝑘(𝑥)=1.
Let𝑥∗represent the true clustering, with 𝑚∗1’s and𝑛∗2’s in𝑥∗(𝑚∗+𝑛∗=𝑁). We define
𝑝(0)
11and𝑝(0)
22as the expected values of 𝑝11,𝑘and𝑝22,𝑘when the two clusters are from the same
distribution:
𝑝(0)
11=𝑚∗−1
𝑁−1, 𝑝(0)
22=𝑛∗−1
𝑁−1.
2.3. Patterns in high dimensions
We explore how the curse of dimensionality manifests. Figure 3 plots the average probabilities
of observations in sample 1 and sample 2 finding 𝑘nearest neighbors within the sample. If a
𝑘-NN graph on the pooled observations is constructed such that each observation points to its
first𝑘nearest neighbors, then 𝑝11is defined as the number of edges in the graph pointing from
an observation in sample 1 to another in sample 1 divided by the total number of edges pointing
from an observation in sample 1. Similarly, 𝑝22is defined for sample 2. Here, sample 1 contains 𝑚
observations randomly generated from N𝑑(0,Σ), and sample 2 contains 𝑛observations randomly
generated fromN𝑑(𝑎1,𝑏Σ), where𝑑=200 andΣ𝑖𝑗=0.1|𝑖−𝑗|. We consider three settings:
•Setting 0 (same distribution): 𝑎=0,𝑏=1.
•Setting 1 (mean differs): 𝑎=0.3,𝑏=1.
•Setting 2 (mean and variance differ): 𝑎=0.3,𝑏=1.3.
When the two samples are from the same distribution, the probability that one observation
finds another observation among its first 𝑘nearest neighbors is always 𝑘/(𝑁−1). Let𝑝(0)
11and
𝑝(0)
22be the expectations of 𝑝11and𝑝22when the two samples are from the same distribution.
Then𝑝(0)
11=(𝑚−1)/(𝑁−1)and𝑝(0)
22=(𝑛−1)/(𝑁−1). Under setting 0, we see that 𝑝11and
𝑝22are close to𝑝(0)
11and𝑝(0)
22, respectively, across all 𝑘values.
When the two samples are from different distributions with a mean difference, 𝑝11and𝑝22are
larger than𝑝(0)
11and𝑝(0)
22, respectively, until 𝑘=𝑁−1, where each observation finds every other
observations as its first 𝑁−1 nearest neighbors.5
Fig. 3: Plots of 𝑝11(black) and𝑝22(blue) considering 𝑘nearest neighbors under different settings.
The dashed lines represent the baseline when the two clusters are from the same distribution
𝑝(0)
11=(𝑚−1)/(𝑁−1)(black dashed line) and 𝑝(0)
22=(𝑛−1)/(𝑁−1)(blue dashed line). The
two dashed lines overlap when 𝑚=𝑛.
When the two distributions differ in scale, 𝑝11is larger than 𝑝(0)
11, but𝑝22is smaller than
𝑝(0)
22across all𝑘values. This illustrate the curse of dimensionality. Here, 𝑑=200 and sample 2
is from a distribution with a larger variance than sample 1. As the volume of the space grows
exponentially with dimension, observations in sample 2 tend to find observations in sample 1 to
be closer than other observations in sample 2, resulting in 𝑝22being smaller than 𝑝(0)
22.
2.4. New clustering criteria
If the two clusters are indeed from two different distributions, we would have the following
three possible scenarios for some 𝑘’s according to Section 2.3:
(1)𝑝11,𝑘(𝑥∗)> 𝑝(0)
11,𝑝22,𝑘(𝑥∗)> 𝑝(0)
22,
(2)𝑝11,𝑘(𝑥∗)> 𝑝(0)
11,𝑝22,𝑘(𝑥∗)< 𝑝(0)
22,
(3)𝑝11,𝑘(𝑥∗)< 𝑝(0)
11,𝑝22,𝑘(𝑥∗)> 𝑝(0)
22.
From Figure 3, we see that 𝑝11,𝑘(𝑥∗)and𝑝22,𝑘(𝑥∗)depend on𝑘. Here, we first fix 𝑘and discuss
the clustering criteria with the aim of finding 𝑥∗. We will then discuss the choice of 𝑘in Section
2.5. For notation simplicity, we use 𝑅1(𝑥)and𝑅2(𝑥)instead of𝑅1,𝑘(𝑥)and𝑅2,𝑘(𝑥)in the rest of
this section.6
We borrow strengths from graph-based two-sample tests (Chen & Friedman, 2017) and propose
to use the following two quantities:
𝑍𝑤(𝑥)=𝑅𝑤(𝑥)−𝜇𝑤(𝑥)
𝜎𝑤(𝑥), 𝑅𝑤(𝑥)=𝑛−1
𝑁−2𝑅1(𝑥)+𝑚−1
𝑁−2𝑅2(𝑥),
𝑍𝑑(𝑥)=𝑅𝑑(𝑥)−𝜇𝑑(𝑥)
𝜎𝑑(𝑥), 𝑅𝑑(𝑥)=𝑅1(𝑥)−𝑅2(𝑥),
where𝜇𝑤(𝑥),𝜎𝑤(𝑥),𝜇𝑑(𝑥),𝜎𝑑(𝑥)are normalizing terms so that 𝑍𝑤(𝑥)and𝑍𝑑(𝑥)are properly
standardized to have mean 0 and variance 1 if there is only one cluster. Their expressions can be
obtained through similar treatments under the two-sample testing setting (Schilling, 1986; Henze,
1988) and the change-point detection setting (Liu & Chen, 2022). Let X𝑚be the set of all 𝑥’s
with𝑚1’s and(𝑁−𝑚)=𝑛0’s. Then, for 𝑥∈X𝑚, we have,
𝜇𝑤(𝑥)=(𝑚−1)(𝑛−1)
(𝑁−1)(𝑁−2)𝑘𝑁,
𝜎𝑤(𝑥)=𝑚𝑛(𝑚−1)(𝑛−1)
𝑁(𝑁−1)(𝑁−2)(𝑁−3)
𝑘𝑁+𝑞1,𝑘−1
𝑁−2(𝑞2,𝑘+𝑘𝑁−𝑘2𝑁)−2
𝑁−1𝑘2𝑁
,
𝜇𝑑(𝑥)=𝑘(𝑚−𝑛),
𝜎𝑑(𝑥)=𝑚𝑛
𝑁(𝑁−1)(𝑞2,𝑘+𝑘𝑁−𝑘2𝑁).
where𝑞1,𝑘=Í
(𝑖,𝑗)∈𝐺𝑘𝐼(𝑗,𝑖)∈𝐺𝑘and𝑞2,𝑘=Í𝑁
𝑖=1Í
𝑗≠𝑙𝐼(𝑗,𝑖),(𝑙,𝑖)∈𝐺𝑘.
Define E2(·)as the expectation under the assumption that there are two clusters.
Theorem 1. For any fixed 𝑘, if𝑝11,𝑘(𝑥∗)and𝑝22,𝑘(𝑥∗)fall into any of the three scenarios, we
have:
•Under scenario (1), E2(𝑍𝑤(𝑥))is maximized at 𝑥=𝑥∗or𝑥=1−𝑥∗.
•Under scenario (2), E2(𝑍𝑑(𝑥))is maximized at 𝑥=𝑥∗.
•Under scenario (3), E2(𝑍𝑑(𝑥))is maximized at 𝑥=1−𝑥∗.
Based on the above theorem, we could use max 𝑥𝑍𝑤(𝑥)to cluster in scenario (1), and use
max𝑥𝑍𝑑(𝑥)to cluster in scenarios (2) and (3). However, in practice, we won’t know which
scenario it is, so we propose to use
𝑀𝜅(𝑥)=max(𝑍𝑤(𝑥),𝜅𝑍𝑑(𝑥))
as the criterion. The choice of 𝜅is discussed in Section 2.6 with 𝜅=1.55 as the default choice.
Table 2 adds the mis-clustering rate of the new method to Table 1. We see that the mis-clustering
rate of the new method under setting 1 is on par with the best remaining methods, and the
mis-clustering rate of the new method under setting 2 is much lower than all other methods.
Table 2: Mis-clustering rate under the same settings as in Table 1.
Spectral Specc RSpec(A) RSpec(L) IF-PCA 𝑡-SNE New
Setting 1 0.157 0.006 0.279 0.167 0.254 0.098 0.010
Setting 2 0.467 0.446 0.418 0.371 0.451 0.462 0.041
Remark 1. It can also be shown that, under scenario (1), E2(𝑅𝑤(𝑥))−𝜇𝑤(𝑥)is maximized
at𝑥=𝑥∗or𝑥=1−𝑥∗; under scenario (2), E2(𝑅𝑑(𝑥))−𝜇𝑑(𝑥)is maximized at 𝑥=𝑥∗; under
scenario (3), E2(𝑅𝑑(𝑥))−𝜇𝑑(𝑥)is maximized at 𝑥=1−𝑥∗. However, we do not recommend
using𝑅𝑤(𝑥)−𝜇𝑤(𝑥)and𝑅𝑑(𝑥)−𝜇𝑑(𝑥)as the criteria because they are not well normalized
and are difficult to combine if one does not know which scenario the actual data falls in.7
Proof. We first compute E2(𝑅𝑤(𝑥))andE2(𝑅𝑑(𝑥)). For𝑥, suppose there are Δ1∈[0,𝑚∗]
observations in cluster 1 placed into cluster 2, and Δ2∈[0,𝑛∗]observations in cluster 2 placed
into cluster 1. So 𝑥∈X𝑚with𝑚=𝑚∗−Δ1+Δ2. Since𝑘is fixed here, for simplicity, we denote
𝑝11,𝑘(𝑥∗),𝑝12,𝑘(𝑥∗),𝑝21,𝑘(𝑥∗),𝑝22,𝑘(𝑥∗)by𝑝∗
11,𝑝∗
12,𝑝∗
21,𝑝∗
22, respectively. Then, we have,
E2(𝑅1(𝑥))=𝑝∗
11𝑘𝑚∗(𝑚∗−Δ1)(𝑚∗−Δ1−1)
𝑚∗(𝑚∗−1)+𝑝∗
12𝑘𝑚∗(𝑚∗−Δ1)Δ2
𝑚∗𝑛∗+𝑝∗
21𝑘𝑛∗(𝑚∗−Δ1)Δ2
𝑚∗𝑛∗+𝑝22𝑘𝑛∗Δ2(Δ2−1)
𝑛∗(𝑛∗−1),
E2(𝑅2(𝑥))=𝑝∗
22𝑘𝑛∗(𝑛∗−Δ2)(𝑛∗−Δ2−1)
𝑛∗(𝑛∗−1)+𝑝∗
21𝑘𝑛∗(𝑛∗−Δ2)Δ1
𝑛∗𝑚∗+𝑝∗
12𝑘𝑚∗(𝑛∗−Δ2)Δ1
𝑛∗𝑚∗+𝑝∗
11𝑘𝑚∗Δ1(Δ1−1)
𝑚∗(𝑚∗−1),
E2(𝑅𝑤(𝑥))=𝑛∗+Δ1−Δ2−1
𝑁−2E2(𝑅1(𝑥))+𝑚∗−Δ1+Δ2−1
𝑁−2E2(𝑅2(𝑥))
=𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22
𝑁−2+𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−𝑁(𝑚∗−1)
𝑚∗(𝑚∗−1)(𝑁−2)Δ2
1
+𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−𝑁(𝑛∗−1)
𝑛∗(𝑛∗−1)(𝑁−2)Δ2
2+2𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22+𝑁
𝑚∗𝑛∗(𝑁−2)Δ1Δ2
−𝑘Δ1(2𝑚∗−1)(𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)∗𝑝∗
22)−𝑁(𝑚∗−1)2
𝑚∗(𝑚∗−1)(𝑁−2)
−𝑘Δ2(2𝑛∗−1)(𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)∗𝑝∗
22)−𝑁(𝑛∗−1)2
𝑛∗(𝑛∗−1)(𝑁−2),
E2(𝑅𝑑(𝑥))=E2(𝑅1(𝑥))−E2(𝑅2(𝑥))
=𝑝∗
11𝑘𝑚∗−𝑝∗
22𝑘𝑛∗−𝑘Δ1
𝑝∗
11−𝑛∗
𝑚∗𝑝∗
22+𝑁
𝑚∗
+𝑘Δ2
𝑝∗
22−𝑚∗
𝑛∗𝑝∗
11+𝑁
𝑛∗
.
We start with E2(𝑍𝑑(𝑥))as it is relatively simpler, and some of the intermediate results can be
used for E2(𝑍𝑤(𝑥)). Notice that
E2(𝑅𝑑(𝑥))−𝜇𝑑(𝑥)
=𝑝∗
11𝑘𝑚∗−𝑝∗
22𝑘𝑛∗−𝑘(𝑚−𝑛)−𝑘Δ1
𝑝∗
11−𝑛∗
𝑚∗𝑝∗
22+𝑁
𝑚∗
+𝑘Δ2
𝑝∗
22−𝑚∗
𝑛∗𝑝∗
11+𝑁
𝑛∗
=𝑘(𝑝∗
11𝑚∗−𝑝∗
22𝑛∗−(𝑚∗−𝑛∗))−𝑘Δ1
𝑝∗
11−𝑛∗
𝑚∗𝑝∗
22+𝑁
𝑚∗−2
+𝑘Δ2
𝑝∗
22−𝑚∗
𝑛∗𝑝∗
11+𝑁
𝑛∗−2
=(𝑝∗
11𝑚∗−𝑝∗
22𝑛∗−(𝑚∗−𝑛∗))𝑘
1−Δ1
𝑚∗−Δ2
𝑛∗
.
Then
E2(𝑍𝑑(𝑥))=E2(𝑅𝑑(𝑥))−𝜇𝑑(𝑥)
𝜎𝑑(𝑥)=(𝑝∗
11𝑚∗−𝑝∗
22𝑛∗−(𝑚∗−𝑛∗))𝑘𝑓𝑑(Δ1,Δ2),
where
𝑓𝑑(Δ1,Δ2)=1−Δ1
𝑚∗−Δ2
𝑛∗√
(𝑚∗−Δ1+Δ2)(𝑛∗+Δ1−Δ2).
Note that
𝜕𝑓𝑑
𝜕Δ1=−𝑁
2𝑚∗𝑛∗(𝑛∗−Δ2)(𝑚∗−Δ1+2Δ2)+Δ1Δ2
(𝑚∗−Δ1+Δ2)1.5(𝑛∗+Δ1−Δ2)1.5<0,
𝜕𝑓𝑑
𝜕Δ2=−𝑁
2𝑚∗𝑛∗(𝑚∗−Δ1)(𝑛∗−Δ2+2Δ1)+Δ1Δ2
(𝑚∗−Δ1+Δ2)1.5(𝑛∗+Δ1−Δ2)1.5<0.
So𝑓𝑑(Δ1,Δ2)achieves its maximum when Δ1=Δ 2=0 with the maximum being1√
𝑚∗𝑛∗, and
achieve its minimum when Δ1=𝑚∗,Δ2=𝑛∗with the minimum being −1√
𝑚∗𝑛∗. Also note that,
under scenario (2), we have
𝑝∗
11𝑚∗−𝑝∗
22𝑛∗−(𝑚∗−𝑛∗)>𝑚∗−1
𝑁−1𝑚∗−𝑛∗−1
𝑁−1−(𝑚∗−𝑛∗)=0,8
and under scenario (3), we have
𝑝∗
11𝑚∗−𝑝∗
22𝑛∗−(𝑚∗−𝑛∗)<0.
Therefore, under scenario (2), E2(𝑍𝑑(𝑥))is maximized when Δ1=Δ 2=0, i.e., at𝑥=𝑥∗; and
under scenario (3), E2(𝑍𝑑(𝑥))is maximized when Δ1=𝑚∗,Δ2=𝑛∗, i.e., at𝑥=1−𝑥∗.
Now, we move onto E2(𝑍𝑤(𝑥)). We first compute E2(𝑅𝑤(𝑥))−𝜇𝑤(𝑥):
E2(𝑅𝑤(𝑥))−𝜇𝑤(𝑥)
=𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑁−2+𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑚∗(𝑚∗−1)(𝑁−2)Δ2
1
+𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑛∗(𝑛∗−1)(𝑁−2)Δ2
2+2𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑚∗𝑛∗(𝑁−2)Δ1Δ2
−𝑘Δ1(2𝑚∗−1)
𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)∗𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑚∗(𝑚∗−1)(𝑁−2)
−𝑘Δ2(2𝑛∗−1)
𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)∗𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑛∗(𝑛∗−1)(𝑁−2)
=𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑁−2
×
1+Δ2
1
𝑚∗(𝑚∗−1)+Δ2
2
𝑛∗(𝑛∗−1)+2Δ1Δ2
𝑚∗𝑛∗−(2𝑚∗−1)Δ1
𝑚∗(𝑚∗−1)−(2𝑛∗−1)Δ2
𝑛∗(𝑛∗−1)
.
Then
E2(𝑍𝑤(𝑥))=𝑘𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
𝑁−2𝑓𝑤(Δ1,Δ2),
where
𝑓𝑤(Δ1,Δ2)=1+Δ2
1
𝑚∗(𝑚∗−1)+Δ2
2
𝑛∗(𝑛∗−1)+2Δ1Δ2
𝑚∗𝑛∗−(2𝑚∗−1)Δ1
𝑚∗(𝑚∗−1)−(2𝑛∗−1)Δ2
𝑛∗(𝑛∗−1)√
(𝑚∗−Δ1+Δ2)(𝑛∗+Δ1−Δ2)(𝑚∗−Δ1+Δ2−1)(𝑛∗+Δ1−Δ2−1).
It’s complicated to work on 𝑓𝑤directly. Here, we first consider
˜𝑓𝑑(Δ1,Δ2)=1−Δ1
𝑚∗−1−Δ2
𝑛∗−1√
(𝑚∗−Δ1+Δ2−1)(𝑛∗+Δ1−Δ2−1).
With similar arguments as for 𝑓𝑑(Δ1,Δ2), we know that, for Δ1∈[0,𝑚∗−1],Δ2∈[0,𝑛∗−1],
˜𝑓𝑑(Δ1,Δ2)achieves its maximum when Δ1=Δ 2=0 with the maximum being1√
(𝑚∗−1)(𝑛∗−1), and
achieve its minimum when Δ1=𝑚∗−1,Δ2=𝑛∗−1 with the minimum being −1√
(𝑚∗−1)(𝑛∗−1).
For the three corner cases (Δ1=𝑚∗−1,Δ2=𝑛∗),(Δ1=𝑚∗,Δ2=𝑛∗−1),(Δ1=𝑚∗,Δ2=𝑛∗),
we discuss them slightly later.
Hence, forΔ1∈[0,𝑚∗−1],Δ2∈[0,𝑛∗−1], we have𝑓𝑑(Δ1,Δ2)˜𝑓𝑑(Δ1,Δ2)achieves its maxi-
mum atΔ1=Δ 2=0 andΔ1=𝑚∗−1,Δ2=𝑛∗−1 with the maximum1√
𝑚∗𝑛∗(𝑚∗−1)(𝑛∗−1). Notice
that
𝑓𝑤(Δ1,Δ2)=𝑓𝑑(Δ1,Δ2)˜𝑓𝑑(Δ1,Δ2)−ℎ(Δ1,Δ2),
where
ℎ(Δ1,Δ2)=𝑁−2
𝑚∗𝑛∗(𝑚∗−1)(𝑛∗−1)Δ1Δ2√
(𝑚∗−Δ1+Δ2)(𝑛∗+Δ1−Δ2)(𝑚∗−Δ1+Δ2−1)(𝑛∗+Δ1−Δ2−1).
Here,ℎ(Δ1,Δ2)>0 whenΔ1Δ2>0, so𝑓𝑤(0,0)> 𝑓𝑤(𝑚∗−1,𝑛∗−1). For the three left out
corner cases, we have9
•𝑓𝑤(𝑚∗,𝑛∗)=1√
𝑛∗𝑚∗(𝑛∗−1)(𝑚∗−1)=𝑓𝑤(0,0).
•𝑓𝑤(𝑚∗,𝑛∗−1)=1−2
𝑛∗√
(𝑛∗−1)(𝑚∗+1)(𝑛∗−2)𝑚∗=√︃
(1−2
𝑛∗)(1−2
𝑚∗+1)√
𝑛∗𝑚∗(𝑛∗−1)(𝑚∗−1)< 𝑓𝑤(0,0).
•𝑓𝑤(𝑚∗−1,𝑛∗)=1−2
𝑚∗√
(𝑛∗+1)(𝑚∗−1)𝑛∗(𝑚∗−2)=√︃
(1−2
𝑚∗)(1−2
𝑛∗+1)√
𝑛∗𝑚∗(𝑛∗−1)(𝑚∗−1)< 𝑓𝑤(0,0). □
Hence,𝑓𝑤(Δ1,Δ2)achieves its maximum when Δ1=Δ 2=0 orΔ1=𝑚∗,Δ2=𝑛∗. Under scenario
(1),
𝑚∗(𝑛∗−1)𝑝∗
11+𝑛∗(𝑚∗−1)𝑝∗
22−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1
>𝑚∗(𝑛∗−1)𝑚∗−1
𝑁−1+𝑛∗(𝑚∗−1)𝑛∗−1
𝑁−1−(𝑚∗−1)(𝑛∗−1)𝑁
𝑁−1=0.
SoE2(𝑍𝑑(𝑥))achieves its maximum when Δ1=Δ 2=0 orΔ1=𝑚∗,Δ2=𝑛∗, i.e., when𝑥=𝑥∗
or𝑥=1−𝑥∗.
Given the results from Theorem 1, the next step is to find an efficient way to solve the
optimization problem. Generally speaking, the optimization of 𝑍𝑤(𝑥)or𝑍𝑑(𝑥)is an NP-hard
problem. Instead of searching through the entire label assignment space, we address this task
with a greedy algorithm. Suppose we are given an initial division vector xof the clusters into two
groups (this vector could be randomly generated). We then examine all possible single changes
tox, defined as changing one element in xfrom𝑠to 1−𝑠, and choose the change that will result
in the largest increase in the corresponding 𝑍𝑤(or𝑍𝑑). We continue making such moves until
no further increase is possible. We start with many initial values for the partition xand pick the
result that yields the maximum value of 𝑍𝑤(or𝑍𝑑). Notice that at each step, we only change one
element of x. Therefore, it is unnecessary to loop over the entire adjacency matrix to calculate
𝑅1,𝑅2,𝜎𝑑, and𝜎𝑤to update the values of 𝑍𝑤(𝑥)and𝑍𝑑(𝑥). Instead, we can only check the
edges related to the label switch to calculate the increase or decrease in 𝑅1and𝑅2. For𝜎𝑑and
𝜎𝑤, the only terms related to the optimization process are 𝑚,𝑛, which can be easily obtained in
each step.
2.5. Choice of 𝑘
The first step of our clustering procedure is to construct 𝐺𝑘(the𝑘-NN graph). Here, 𝑘can be
any integer between 1 and 𝑁−1. We can create all the possible 𝐺𝑘and perform the clustering
process. Typically, different values of 𝑘lead to different clustering outcomes. Since the underlying
true partition of the clusters is usually not available, a natural question arises: how to choose 𝑘
such that the clustering criteria have the optimal power or give us the most reasonable partition.
In this section, we intend to empirically discuss the choice of 𝑘.
We want to choose 𝑘such that𝑝11,𝑘(𝑥∗)and𝑝22,𝑘(𝑥∗)are far from 𝑝(0)
11and𝑝(0)
22, while
ensuring there are enough edges in the graph to contribute to the evidence. Heuristically, when
the value of 𝑘is small, the graph is sparse and some useful similarity information among the
individuals might not be fully captured by 𝑍𝑤and𝑍𝑑. On the other hand, when 𝑘is too large, it
includes some irrelevant information. If groundtruth is available for a given dataset, there always
exists an optimal (or a few optimal) choice of 𝑘that will result in the partition with the largest
overlap with the underlying true group assignments.
The choice of the hyper-parameter 𝑘under the two-sample testing and change-point detection
regime for the statistic 𝑍𝑤and𝑍𝑑was discussed in Zhu & Chen (2021), Zhang & Chen (2021)
and Zhou & Chen (2022). Specifically, they consider the power of the methods for 𝑘=[𝑁𝜆]when10
Fig. 4: Values of 𝑍𝑤,𝑍𝑑, and𝑀𝜅(top panel) and mis-clustering rate (bottom panel) over 𝑘.
𝑚=𝑛=50.
varying𝜆from 0 to 1 in different settings. In two-sample testing, Zhu & Chen (2021) suggested
using𝑘=[𝑁0.5]with the𝑘-MST, while Zhou & Chen (2022) suggested 𝜆=0.65 for the𝑘-NN
graph.
Here, we explore the relation between 𝑘and mis-clustering rate. In particular, we aim to find
out whether the 𝑘that leads to the maximum value of 𝑀𝜅results in the minimum clustering error.
In each simulation setting, we generate the 𝑘-NN graphs for 𝑘from 1 to𝑁−3, increased by 2
at each time. We consider the Gaussian mixture example where we have two groups of samples
fromN𝑑(0,Σ)andN𝑑(𝑎1,𝑏Σ), withΣ𝑖𝑗=0.1|𝑖−𝑗|.
•Setting 1 (only mean differs): 𝑎 >0,𝑏=1.
•Setting 2 (only variance differs): 𝑎=0,𝑏 > 1.
•Setting 3 (both mean and variance differ): 𝑎 >0,𝑏 > 1.
We first examine the values of 𝑍𝑤and𝑍𝑑for different settings, along with the value of 𝑀𝜅with
𝜅=1.55, i.e.,𝑀𝜅(𝑥)=max(𝑍𝑤(𝑥),1.55𝑍𝑑(𝑥)). The choice of 𝜅is discussed in Section 2.6.
Here, our primarily concern is the value of 𝑘that leads to the maximum 𝑀𝜅, and the value of
𝜅does not affect this choice of 𝑘as long as𝜅is within a certain range. We set the dimension
to be𝑑=800. For the cluster sizes, we examine both balanced ( 𝑚=𝑛=50) and unbalanced
(𝑚=30,𝑛=70) cases.
Figure 4 plots the values of 𝑍𝑤,𝑍𝑑, and𝑀𝜅over𝑘under all three settings for the balanced case
(top panel), as well as the mis-clustering rate (bottom panel). We see that the mis-clustering rate
reaches its minimum when 𝑀𝜅is at its maximum in all settings. The same pattern also appears
for the unbalanced case (Figure 5). This is exciting, as 𝑀𝜅over𝑘is fully observable given the
data, allowing us to select the 𝑘that maximizes 𝑀𝜅in practice.11
Fig. 5: Values of 𝑍𝑤,𝑍𝑑, and𝑀𝜅(top panel) and mis-clustering rate (bottom panel) over 𝑘.
𝑚=30,𝑛=70.
2.6. Choice of 𝜅
From Section 2.4, max 𝑥𝑍𝑤(𝑥)can be used to cluster scenario (1), and max 𝑥𝑍𝑑(𝑥)is suitable
for scenarios (2) and (3). Since we don’t know which scenario we are dealing with in reality,
𝑀𝜅(𝑥)=max(𝑍𝑤(𝑥),𝜅𝑍𝑑(𝑥))was proposed for these three scenarios. Ideally, when the scenario
is (1), we want 𝑍𝑤to dominate in 𝑀𝜅; while when the scenario is (2) or (3), we want 𝑍𝑑to
dominate in 𝑀𝜅. However, this dominance does not need to hold for all 𝑘’s. Instead, as long as
𝑀𝜅picks the correct quantity ( 𝑍𝑤or𝑍𝑑) when the correct quantity reaches its maximum, the
goal is achieved given how we choose 𝑘. This can be seen clearly from Figure 4 and 5. In Figure
4 and 5, the leftmost pictures show two clusters generated from N𝑑(0,Σ)andN𝑑(𝑎1,Σ). This
belongs to scenario (1), and 𝑍𝑤should dominate 𝑀𝜅based on the statements above. We see that
𝑀𝜅is identical to 𝑍𝑤when𝑍𝑤reach its maximum but differs from 𝑍𝑤at the boundaries. That
will not affect the mis-clustering rate using 𝑀𝜅given how we choose 𝑘.
With these observations, we use the following simulation study to choose 𝜅. All other parameters
remain the same as in Figure 4 setting 1 (only mean differs). We gradually increase the location
difference parameter 𝑎to determine the upper bound of 𝜅. All other parameters remain the same as
in Figure 4 setting 2 (only variance differs), we gradually increase the scale difference parameter
𝑏to determine the lower bound of 𝜅. For each setting, we run 50 times.
In Figure 6, left panel, the mis-clustering rate significantly decreases at around 𝑎=0.2. When
the signal is less than this threshold, the difference between the two clusters is not substantial
enough for the algorithm to distinguish them effectively, resulting in a mis-clustering rate close
to 0.5. After this phase transition, the mis-clustering rate is much smaller. Therefore, we want 𝑍𝑤
to dominate in 𝑀𝜅when𝑎≥0.2, which means choosing 𝜅smaller than the ratio when 𝑎≥0.2.
This establishes an upper bound for 𝜅, which is the minimum value of the boxplot at 𝑎=0.2.
Similarly, in the right panel, we want 𝑍𝑑to dominate in 𝑀𝜅when𝑏≥1.15. This establishes a
lower bound for 𝜅, which is the maximum value of the boxplot at 𝑏=1.15. Thus, a proper choice12
Fig. 6: Plots of ratio ( 𝑟=𝑍𝑤/𝑍𝑑) versus signal strength. Left panel: location difference with
𝑏=1, and𝑎increasing from 0.05 to 0.3. The blue dashed line at 𝑟=1.8 represents the minimum
value of the boxplot when 𝑎=0.2. The “mis rate” indicates the mean mis-clustering rate when
using𝑍𝑤as the criterion, with green representing higher errors and orange indicating lower errors.
Right panel: scale difference with 𝑎=0, and𝑏increasing from 1.05 to 1.3. The red dashed line
at𝑟=1.3 represents the maximum value of the boxplot when 𝑏=1.15. The “mis rate” indicates
the mean mis-clustering rate when using 𝑍𝑑as the criterion, with green representing higher errors
and orange indicating lower error.
of𝜅would be between the upper bound 𝑟=1.8 and the lower bound 𝑟=1.3. For simplicity, we
choose the middle point 𝜅=1.55 as our default choice. It is important to note that if there is
a prior probability indicating which of the three scenarios a particular problem belongs to, the
choice of𝜅can be adjusted to favor more on 𝑍𝑤or𝑍𝑑accordingly.
2.7. A ternary search algorithm
In this Section, we introduce a search algorithm for selecting 𝑘when computational resources
are limited. Specifically, we focus on the algorithm of choosing 𝑘in Section 2.5. Notice that
max
𝑘𝑀𝜅=max
𝑘max
𝑥(𝑍𝑤(𝑥),𝜅𝑍𝑑(𝑥)).
To find the maximum value of 𝑀𝜅, we need to determine the maximum values of 𝑍𝑤and𝑍𝑑
separately. From Theorem 1 and Figures 4 and 5, we observe that 𝑍𝑑or𝑍𝑤is a unimodal function
over𝑘when it is the proper criterion. Therefore, we can use a ternary search algorithm to find
the maximum value of 𝑀𝜅(Bajwa et al., 2015). We summarize the algorithm below. For ease of
notation, we denote the maximum value of 𝑍𝑤(𝑥)(or𝑍𝑑(𝑥)) on the𝑘-NN graph𝐺𝑘as𝑓𝑘(𝑥).
Based on Theorem 1, we note that for a given dataset, only be one of 𝑍𝑤or𝑍𝑑may be a unimodal
function. While ternary search is guaranteed to find the optimal value for unimodal functions, it
suffices for our purposes since the maximum value of 𝑀𝜅corresponds to the maximum value of
the unimodal function when it is the correct clustering criterion.
3. Numerical studies
In this section, we compare the performance of the new method with other existing methods
on synthetic data. Specifically, we compare the following methods:13
Algorithm 1 . Ternary Search
Input:𝑓(𝑘)for a series of 𝑘-NN graphs𝐺𝑘.
Output: The maximum value of 𝑓(𝑘).
Algorithm:
1. Set left to 1 and right toN - 1 .
2. While right -left is greater than 1, do the following:
a. Compute left third as⌊left+(right−left)/3⌋.
b. Compute right third as⌊right−(right−left)/3⌋.
c. If𝑓(left third)is less than𝑓(right third), setleft toleft third ;
d. Otherwise, set right toright third .
3. Return𝑓j
left+right
2k
.
•Spectral: Spectral clustering as described in Hastie et al. (2009).
•Specc: The specc function in the kernlab package with ‘kpar=local’. (A discussion of the
choice of “kpar” can be found in Supplement S1.)
•RSpecA and RSpecL: Regularized Spectral Clustering ( reg.SP in the randnet package)
proposed by Rohe et al. (2011), applied to the 10-NN adjacency matrix and the laplacian
matrix, respectively.
•IF-PCA: Influential Feature PCA proposed by Jin & Wang (2016).
•𝑡-SNE: Using 𝑡-SNE to project the data into two dimensional space and then applying the
k-means clustering algorithm on the projected data (Mwangi et al., 2014).
We consider a few different settings below:
•Gaussian mixture: Cluster 1: X1∼N𝑑(0,Σ)and Cluster 2: X2∼N𝑑(𝑎1,𝑏Σ),Σ𝑖𝑗=0.1|𝑖−𝑗|.
The location parameter 𝑎∈{0,0.1,0.5}and the scale parameter 𝑏∈{0.8,1,1.4}.
•t-distribution mixture: Cluster 1: X1∼𝑡20(0,Σ)and Cluster 2: X2∼𝑡20(𝑎1,𝑏Σ),Σ𝑖𝑗=0.1|𝑖−𝑗|.
The location parameter 𝑎∈{0,0.25,0.3}and the scale parameter 𝑏∈{0.8,1,3,3.5}.
For each setting, the mis-clustering rate is estimated through 50 simulation runs. Figure 7 is
arranged in three columns representing location difference, scale difference, and location-scale
difference, respectively. The first column displays the results for the traditional location difference
Gaussian Mixture with a fixed covariance matrix. The top panel shows a small difference between
clusters with 𝑎=0.1, while the bottom panel shows a larger difference with 𝑎=0.5. The second
column displays the results for scale-only difference, considering both the variance parameter
𝑏 >1 and𝑏 <1. The third column combines the parameters from the first and second columns.
We observe that, except for the top left panel where all methods are inefficient due to the small
signal, the new method performs well for all other settings, either being the best or among
the best methods. In particular, under scale difference (middle panel), our method dramatically
outperforms the others.
Figure 8 shows mis-clustering rates for different methods under the mixture of two multivariate
𝑡-distributions. In the multivariate 𝑡-distribution location-difference mixture scenario, 𝑡-SNE
performs very well when the difference is small, which is expected given how 𝑡-SNE was derived.
However, when the signal increases (lower left panel), our new method performs even better
than𝑡-SNE. In the scale difference scenario, Specc shows surprisingly high power. It performs14
Fig. 7: Mis-clustering rates for Gaussian mixtures N𝑑(0,Σ)andN𝑑(𝑎1,𝑏Σ).
slightly better than our new method, while other methods completely lose power. However, for
the location-scale mixture case with moderate to high dimensions, our new method remains the
most effective, followed by Specc, 𝑡-SNE, Spectral, and other methods.
Fig. 8: Mis-clustering rates for multivariate 𝑡-distribution mixtures 𝑡20(0,Σ)and𝑡20(𝑎1,𝑏Σ).15
To sum up, for the 𝑡20distribution, our method remains the overall best among as the dimension
𝑑increases, with Specc slightly outperforming it in the scale difference scenario. While 𝑡-SNE
performs significantly better compared to the Gaussian case, this is entirely expected because it
was developed for heavy distribution distributions. All other methods are not as powerful as our
new method in the 𝑡20setting.
4. Real-world applications
Here, we apply the proposed method to seven gene expression datasets, each with two classes
and more features than the sample size: Alon et al. (1999); Golub et al. (1999); Wang et al.
(2005); Gordon et al. (2002); Bhattacharjee et al. (2001); Singh et al. (2002); Su et al. (2001).
The datasets are summarized in Table 3. Datasets 1, 2, and 6 were analyzed in Dettling (2004),
while datasets 3, 5, and 7 were analyzed and grouped into two classes in Yousefi et al. (2010).
Dataset 4 is from Gordon et al. (2002).
We summarize the datasets used in our analysis in Table 3. In all these datasets, the true labels
are provided and are considered as the “ground truth”. These labels are only used to evaluate
the error rate of various clustering methods. Table 4 displays the mis-clustering rates of different
methods on these seven datasets.
For the Colon Cancer dataset, our new method achieves a mis-clustering rate of 0.112, while
all other methods have a mis-clustering rate above 0.2. In a recent study (Singh & Verma, 2022),
the best performance of various methods on the Colon Cancer dataset in terms of the Random
Index (RI) was 51.77%. In comparison, our clustering result has an RI value of 79.64%, which
is a significant improvement. It is noteworthy that the Colon Cancer dataset is known to be
challenging for clustering, even for classification tasks where the labels of the training samples
are available (Donoho & Jin, 2008). Therefore, achieving an 11.2% mis-clustering error is a
breakthrough for the new method.
For the Leukemia dataset, all methods appear to be effective. Our method and Specc have
the lowest mis-clustering rate at 4.2%. For the Lung Cancer (1) dataset, IF-PCA performs best
with a mis-clustering rate of 3.3%. Our new method and Specc have a mis-clustering rate around
10%. Traditional Spectral clustering achieves a 5.5% misclustering rate. Other methods are not
effective. For the Lung Cancer (2) dataset, several methods, including Specc, IF-PCA and our
new methods, achieve equivalent best performance.
For the Breast Cancer, SuCancer, and Prostate Cancer datasets, none of the methods achieve a
mis-clustering rate lower than 30%. New methods are needed for these three datasets. It is worth
Table 3: Dataset information
Data name Abbreviation Source 𝑛 𝑝
Colon Cancer Cln Alon et al. (99) 62 2000
Leukemia Leuk Golub et al. (99) 72 3571
Lung Cancer(1) Lung1 Gordon et al. (02) 181 12,533
Lung Cancer(2) Lung2 Bhattacharjee et al. (01) 203 12,600
Breast Cancer Brst Wang et al. (05) 276 22,215
Prostate Cancer Prst Singh et al. (02) 102 6033
SuCancer Sur Su et al. (01) 174 790916
Table 4: Mis-clustering rates on seven gene expression datasets.
Data Spectral Specc RSpec(A) RSpec(L) IF-PCA 𝑡-SNE New
Cln 0.467 0.451 0.209 0.258 0.403 0.415 0.112
Leuk 0.166 0.042 0.152 0.069 0.069 0.074 0.042
Lung1 0.055 0.093 0.254 0.403 0.033 0.466 0.116
Lung2 0.246 0.217 0.467 0.231 0.217 0.293 0.217
Brst 0.467 0.431 0.355 0.344 0.406 0.438 0.409
Sur 0.390 0.333 0.385 0.379 0.500 0.405 0.494
Prst 0.431 0.421 0.470 0.441 0.382 0.441 0.431
noting that the mis-clustering rate we report for IF-PCA on the SuCancer dataset differs from the
number in Jin & Wang (2016) because we apply IF-PCA to the dataset without data modification.
In summary, the Breast Cancer, SuCancer, and Prostate Cancer datasets are extremely challeng-
ing for the existing methods. One possible reason might be that the two clusters in these datasets
are not homogeneous. Aside from these extremely challenging datasets, our new method performs
well for the remaining datasets and makes significant advancements for the Colon Cancer dataset.
5. Conclusion and discussion
We proposed a new clustering framework that takes into account a useful pattern manifested by
the curse of dimensionality in the 𝑘-NN graph, improving upon existing methods for clustering
high-dimensional data, especially when there are two Gaussian clusters differ in scale. Two
important parameters in our method are 𝑘in the𝑘-NN graph and 𝜅in𝑀𝜅=max{𝑍𝑤,𝜅𝑍𝑑}.
In this study, we focused on the traditional clustering task, which is an unsupervised learning
task, and suggested using 𝜅=1.55 and the value of 𝑘that results in the largest value of 𝑀𝜅. In
semi-supervised clustering, where we have partial labels of the data as well as a larger set of
unlabeled data, the labeled data can be used to train the parameters 𝑘and𝜅.
In this study, we focused on two clusters, but many real-world datasets contain more than
two clusters. When the number of clusters is fixed and known, we can use top-down divisive
algorithms to extend this framework to multiple clusters. This involves recursively dividing a
larger cluster into two smaller clusters until we reach the desired number of clusters. At each
division step, a criterion such as the maximum value of 𝑀𝜅can be used to determine which
cluster to divide. Further research is needed to evaluate the performance of different criteria in
this setting. When the number of clusters is unknown, the task becomes more challenging. In
this case, a new penalized criterion that combines 𝑀𝜅and BIC may be considered, but extensive
investigation is required.
Acknowledgement
Hao Chen and Xiancheng Lin were supported in part by the NSF awards DMS-1848579 and
DMS-2311399.
References
Alon, U. ,Barkai, N. ,Notterman, D. A. ,Gish, K. ,Ybarra, S. ,Mack, D. &Levine, A. J. (1999). Broad patterns
of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide17
arrays. Proceedings of the National Academy of Sciences 96, 6745–6750.
Bajwa, M. S. ,Agarwal, A. P. &Manchanda, S. (2015). Ternary search algorithm: Improvement of binary search.
In2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom) . IEEE.
Bhattacharjee, A. ,Richards, W. G. ,Staunton, J. ,Li, C. ,Monti, S. ,Vasa, P. ,Ladd, C. ,Beheshti, J. ,Bueno, R. ,
Gillette, M. et al. (2001). Classification of human lung carcinomas by mrna expression profiling reveals distinct
adenocarcinoma subclasses. Proceedings of the National Academy of Sciences 98, 13790–13795.
Chan, P. K. ,Schlag, M. D. &Zien, J. Y. (1993). Spectral k-way ratio-cut partitioning and clustering. In Proceedings
of the 30th international Design Automation Conference .
Chen, H. &Friedman, J. H. (2017). A new graph-based two-sample test for multivariate and object data. Journal of
the American statistical association 112, 397–409.
Dettling, M. (2004). Bagboosting for tumor classification with gene expression data. Bioinformatics 20, 3583–3593.
Donoho, D. &Jin, J. (2008). Higher criticism thresholding: Optimal feature selection when useful features are rare
and weak. Proceedings of the National Academy of Sciences 105, 14790–14795.
Golub, T. R. ,Slonim, D. K. ,Tamayo, P. ,Huard, C. ,Gaasenbeek, M. ,Mesirov, J. P. ,Coller, H. ,Loh, M. L. ,
Downing, J. R. ,Caligiuri, M. A. et al. (1999). Molecular classification of cancer: class discovery and class
prediction by gene expression monitoring. science 286, 531–537.
Gordon, G. J. ,Jensen, R. V. ,Hsiao, L.-L. ,Gullans, S. R. ,Blumenstock, J. E. ,Ramaswamy, S. ,Richards, W. G. ,
Sugarbaker, D. J. &Bueno, R. (2002). Translation of microarray data into clinically relevant cancer diagnostic
tests using gene expression ratios in lung cancer and mesothelioma. Cancer research 62, 4963–4967.
Hastie, T. ,Tibshirani, R. ,Friedman, J. H. &Friedman, J. H. (2009). The elements of statistical learning: data
mining, inference, and prediction , vol. 2. Springer.
Henze, N. (1988). A multivariate two-sample test based on the number of nearest neighbor type coincidences. The
Annals of Statistics 16, 772–783.
Jin, J. &Wang, W. (2016). Influential features pca for high dimensional clustering. The Annals of Statistics 44,
2323–2359.
Leighton, F. T. &Rao, S. (1988). An approximate max-flow min-cut theorem for uniform multicommodity flow
problems with applications to approximation algorithms. In FOCS , vol. 88.
Liu, Y.-W. &Chen, H. (2022). A fast and efficient change-point detection framework based on approximate 𝑘-nearest
neighbor graphs. IEEE Transactions on Signal Processing 70, 1976–1986.
Maaten, L. v. d. &Hinton, G. (2008). Visualizing data using t-sne. Journal of Machine Learning Research 9,
2579–2605.
Mwangi, B. ,Soares, J. C. &Hasan, K. M. (2014). Visualization and unsupervised predictive clustering of high-
dimensional multimodal neuroimaging data. Journal of neuroscience methods 236, 19–25.
Rohe, K. ,Chatterjee, S. &Yu, B. (2011). Spectral clustering and the high-dimensional stochastic blockmodel .
Schilling, M. F. (1986). Multivariate two-sample tests based on nearest neighbors. Journal of the American Statistical
Association 81, 799–806.
Shi, J. &Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and
machine intelligence 22, 888–905.
Singh, D. ,Febbo, P. G. ,Ross, K. ,Jackson, D. G. ,Manola, J. ,Ladd, C. ,Tamayo, P. ,Renshaw, A. A. ,D’Amico,
A. V. ,Richie, J. P. et al. (2002). Gene expression correlates of clinical prostate cancer behavior. Cancer cell 1,
203–209.
Singh, V. &Verma, N. K. (2022). Gene expression data analysis using feature weighted robust fuzzy-means clustering.
IEEE Transactions on NanoBioscience 22, 99–105.
Su, A. I. ,Welsh, J. B. ,Sapinoso, L. M. ,Kern, S. G. ,Dimitrov, P. ,Lapp, H. ,Schultz, P. G. ,Powell, S. M. ,
Moskaluk, C. A. ,Frierson Jr, H. F. et al. (2001). Molecular classification of human carcinomas by use of gene
expression signatures. Cancer research 61, 7388–7393.
Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and computing 17, 395–416.
Wang, Y. ,Klijn, J. G. ,Zhang, Y. ,Sieuwerts, A. M. ,Look, M. P. ,Yang, F. ,Talantov, D. ,Timmermans, M. ,
Meijer-van Gelder, M. E. ,Yu, J. et al. (2005). Gene-expression profiles to predict distant metastasis of lymph-
node-negative primary breast cancer. The Lancet 365, 671–679.
Wu, Z. &Leahy, R. (1993). An optimal graph theoretic approach to data clustering: Theory and its application to
image segmentation. IEEE transactions on pattern analysis and machine intelligence 15, 1101–1113.
Yousefi, M. R. ,Hua, J. ,Sima, C. &Dougherty, E. R. (2010). Reporting bias when using real data sets to analyze
classification performance. Bioinformatics 26, 68–76.
Zhang, Y. &Chen, H. (2021). Graph-based multiple change-point detection. arXiv preprint arXiv:2110.01170 .
Zhou, D. &Chen, H. (2022). Ring-cpd: Asymptotic distribution-free change-point detection for multivariate and
non-euclidean data. arXiv preprint arXiv:2206.03038 .
Zhu, Y. &Chen, H. (2021). Limiting distributions of graph-based test statistics. arXiv preprint arXiv:2108.07446 .