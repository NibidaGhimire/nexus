A New Clustering Framework
By Hao Chen and Xiancheng Lin
Department of Statistics, University of California, Davis,
One Shields Avenue, Davis, California, 95616, U.S.A.
hxchen@ucdavis.edu xclin@ucdavis.edu
Summary
Detecting clusters is a critical task in various fields, including statistics, engineering and
bioinformatics. Our focus is primarily on the modern high-dimensional scenario, where traditional
methods often fail due to the curse of dimensionality. In this study, we introduce a non-parametric
framework for clustering that is applicable to any number of dimensions. Simulation results
demonstrate that this new framework surpasses existing methods across a wide range of settings.
We illustrate the proposed method with real data applications in distinguishing cancer tissues
from normal tissues through gene expression data.
Some key words : Curse of dimensionality, Multivariate analysis, High-dimensional data, Gaussian mixture, Gene
microarray.
1. Introduction
Cluster analysis, or clustering, is a fundamental problem in statistics. Broadly speaking, the
task is to group a set of objects into clusters in such a way that the objects in one cluster are
more â€œsimilarâ€ to each other than to those in other clusters. The definition for â€œsimilarâ€ can be
somewhat arbitrary. In the following, we discuss this through a few toy examples and establish
the criterion we will use for the paper.
1.1. What is â€œsimilarâ€?
Here are some toy examples in two dimensions. Figure 1 shows the data colored according
to their generating schemes. We applied two classical clustering algorithms to these data: the
(a) (b) (c) (d) (e)
Fig. 1: Examples of different shapes of two-dimensional data, with different colors representing
different data generating schemes: (a) three samples generated from three bivariate Gaussian
distributions; (b) one sample from a bivariate Gaussian distribution and another from a parabola
with perturbations; (c) two parabolas with perturbations; (d) two rings with perturbations; and
(e) two logarithmic spirals with perturbations.
1arXiv:2305.00578v2  [stat.ME]  15 Aug 20242
(a) (b) (c) (d) (e)
Fig. 2: Clustering results based on the standard ğ‘˜-means approach (top row) and the spectral
clustering (bottom row) for the five datasets in Figure 1.
standardğ‘˜-means clustering algorithm ( kmeans function in R) and spectral clustering ( specc
function in the Rpackage kernlab ). The clustering results are shown in Figure 2.
The clustering results based on spectral clustering are much more satisfying. Hence, even
though the definition of â€œsimilarâ€ can be somewhat arbitrary, one likely wants to separate obser-
vations from different data generating schemes into distinct clusters. Generally speaking, cluster
analysis is useful when the dataset consists of a mixture of observations from different underlying
data-generating schemes or models, and we aim to partition the observations into clusters such
that each cluster can be modeled simply , for instance, using a function or distribution. This allows
us to summarize the dataset concisely and implement different treatments for different clusters
according to their properties. In the examples above, the ğ‘˜-means approach minimizes the within-
cluster sum of squares (group â€˜similarâ€™ points to some extent); however, it does not facilitate the
follow-up task of understanding the observations in each cluster, making it less useful.
In the following, we refer to points that can be represented by a relatively simple distribution
or model as one cluster, and our goal is to group observations that are similar in underlying
distribution or model .
1.2. Difficulties in clustering high-dimensional data
In Section 1.1, spectral clustering performs well across all examples. However, this may not
be the case when the dimensionality of the data is high. Consider a scenario where the data is
a mixture of observations from two distributions, Nğ‘‘(0,Î£)andNğ‘‘(ğ‘1,ğ‘Î£), withğ‘‘=800 and
Î£ğ‘–ğ‘—=0.1|ğ‘–âˆ’ğ‘—|, whereÎ£ğ‘–ğ‘—is the(ğ‘–,ğ‘—)th element of Î£. Suppose there are 50 observations in each
cluster, and the goal is to separate them. We examine two simulation settings:
â€¢Setting 1 (only mean differs): ğ‘=0.25,ğ‘=1.
â€¢Setting 2 (only variance differs): ğ‘=0,ğ‘=1.2.
For spectral clustering, we use an advanced choice, kpar = â€˜localâ€™ , in the specc function
to avoid unexpected errors encountered with high-dimensional data (details in Supplement S1).
Spectral clustering encompasses various methods, each differing in their approach to constructing
similarity graphs and handling data complexity. Therefore, in addition to specc , we include the
method described in Chapter 14.5.3 of Hastie et al. (2009) (Spectral), which involves constructing3
Table 1: Mis-clustering rates for various clustering methods under two settings. The smallest
mis-clustering rate under each setting is in bold.
Spectral Specc RSpec(A) RSpec(L) IF-PCA ğ‘¡-SNE
Setting 1 0.157 0.006 0.279 0.167 0.254 0.098
Setting 2 0.467 0.446 0.418 0.371 0.451 0.462
a 10-NN similarity graph and applying conventional spectral clustering. We also include the
Regularized Spectral Clustering method proposed by Rohe et al. (2011) ( reg.SP in the Rpackage
randnet ) to the same similarity graph for both the adjacency matrix (RSpecA) and laplacian
matrix (RSpecL). Additionally, we apply some recently popular clustering methods. IF-PCA,
proposed by Jin & Wang (2016), has shown successes in clustering gene microarray data. ğ‘¡-SNE
(ğ‘¡-distributed stochastic neighbor embedding) (Maaten & Hinton, 2008) was initially proposed
for data visualization by projecting observations into a two-dimensional space, effectively groups
â€œsimilarâ€ observations in this space. Mwangi et al. (2014) proposed to use the ğ‘˜-means clustering
algorithm on the two-dimensional projected data from ğ‘¡-SNE. We apply these methods to the
data generated from the two settings above, and the mis-clustering rates from a typical simulation
run are shown in Table 1.
Setting 1 is a relatively straightforward case where the two clusters differ only in mean. In this
scenario, we see that specc achieves the best performance. Setting 2 is more complex, with the
two clusters differing in scale. All methods struggle in this setting. For spectral clustering, it is
closely related to the Mincut problem when observations are represented through a similarity
graph. When the dimension is high and the two clusters differ in scale, the curse of dimensionality
causes the observations in the cluster with larger variance to be more sparsely scattered than those
in the cluster with smaller variance. As a result, for any observation in the cluster with larger
variance, its nearest points tend to be from the other cluster, causing the spectral clustering to
fail. We explore this phenomenon further in Section 2.
In this paper, we propose a new clustering criterion based on theoretical results (Section 2). The
new method is compared with existing methods under various settings in Section 3 and applied
to real data applications in Section 4. The paper concludes with discussions in Section 5.
2. Proposed method
2.1. Graph partition methods review
First, we review the graph clustering problem since our method will be based on similarity
graphs constructed from the observations. The primary graph clustering formulations focus
on graph cut and partitioning problems (Chan et al., 1993; Shi & Malik, 2000). MinCut is an
example of a traditional graph partitioning method. The MinCut method focuses on identifying the
partition of a graph into two disjoint sets by minimizing the number of cut edges. The objective is
to separate the graph into clusters with the fewest possible inter-cluster connections. However, Wu
& Leahy (1993) observed that in many tested graphs, the solutions of this problem are partitions
with isolated nodes in clusters. The Ratio Cut method (Leighton & Rao, 1988) improves upon
MinCut by considering the number of nodes in each clusters, avoiding isolated nodes. Later, the
Normalized Cut (Ncut) method (Shi & Malik, 2000) seeks to minimize the cut relative to the
number of edges in a cluster instead of the number of nodes. It has been shown that relaxing
Ncut leads to normalized spectral clustering, while relaxing RatioCut results in unnormalized4
spectral clustering on the graph laplacian matrix (Von Luxburg, 2007). Spectral clustering has
since become more popular. However, when we investigate the similarity graphs constructed on
the observations and take into account the patterns caused by the curse of dimensionality, some
powerful statistics can be derived.
2.2. Notations
We useğºğ‘˜to denote the set of edges in the ğ‘˜-nearest neighbor ( ğ‘˜-NN) graph, and(ğ‘–,ğ‘—)âˆˆğºğ‘˜
if nodeğ‘–points to node ğ‘—(each node points to its first ğ‘˜nearest neighbors). Let ğ‘¥be anğ‘-length
vector of 1â€™s and 2â€™s, and define
ğ‘…1,ğ‘˜(ğ‘¥)=âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆğºğ‘˜ğ¼ğ‘¥ğ‘–=ğ‘¥ğ‘—=1, ğ‘… 2,ğ‘˜(ğ‘¥)=âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆğºğ‘˜ğ¼ğ‘¥ğ‘–=ğ‘¥ğ‘—=2.
Then,ğ‘¥divides the observations into two clusters: 1â€™s (Cluster 1) and 2â€™s (Cluster 2). Let ğ‘šbe the
number of 1â€™s in ğ‘¥andğ‘›be the number of 2â€™s in ğ‘¥. We defineğ‘11,ğ‘˜(ğ‘¥)as oneğ‘˜th of the probability
that an observation in cluster 1 finds another observation in cluster 1 among its first ğ‘˜nearest
neighbors, i.e., ğ‘11,ğ‘˜(ğ‘¥)=ğ‘…1,ğ‘˜(ğ‘¥)/(ğ‘˜ğ‘š). Similarly, we have ğ‘12,ğ‘˜(ğ‘¥)=(ğ‘˜ğ‘šğ‘¥âˆ’ğ‘…1,ğ‘˜(ğ‘¥))/(ğ‘˜ğ‘š),
ğ‘21,ğ‘˜(ğ‘¥)=ğ‘…2,ğ‘˜(ğ‘¥)/(ğ‘˜ğ‘›),ğ‘22,ğ‘˜(ğ‘¥)=(ğ‘˜ğ‘›âˆ’ğ‘…2,ğ‘˜(ğ‘¥))/(ğ‘˜ğ‘›). It is clear that
ğ‘11,ğ‘˜(ğ‘¥)+ğ‘12,ğ‘˜(ğ‘¥)=1, ğ‘ 21,ğ‘˜(ğ‘¥)+ğ‘22,ğ‘˜(ğ‘¥)=1.
Letğ‘¥âˆ—represent the true clustering, with ğ‘šâˆ—1â€™s andğ‘›âˆ—2â€™s inğ‘¥âˆ—(ğ‘šâˆ—+ğ‘›âˆ—=ğ‘). We define
ğ‘(0)
11andğ‘(0)
22as the expected values of ğ‘11,ğ‘˜andğ‘22,ğ‘˜when the two clusters are from the same
distribution:
ğ‘(0)
11=ğ‘šâˆ—âˆ’1
ğ‘âˆ’1, ğ‘(0)
22=ğ‘›âˆ—âˆ’1
ğ‘âˆ’1.
2.3. Patterns in high dimensions
We explore how the curse of dimensionality manifests. Figure 3 plots the average probabilities
of observations in sample 1 and sample 2 finding ğ‘˜nearest neighbors within the sample. If a
ğ‘˜-NN graph on the pooled observations is constructed such that each observation points to its
firstğ‘˜nearest neighbors, then ğ‘11is defined as the number of edges in the graph pointing from
an observation in sample 1 to another in sample 1 divided by the total number of edges pointing
from an observation in sample 1. Similarly, ğ‘22is defined for sample 2. Here, sample 1 contains ğ‘š
observations randomly generated from Nğ‘‘(0,Î£), and sample 2 contains ğ‘›observations randomly
generated fromNğ‘‘(ğ‘1,ğ‘Î£), whereğ‘‘=200 andÎ£ğ‘–ğ‘—=0.1|ğ‘–âˆ’ğ‘—|. We consider three settings:
â€¢Setting 0 (same distribution): ğ‘=0,ğ‘=1.
â€¢Setting 1 (mean differs): ğ‘=0.3,ğ‘=1.
â€¢Setting 2 (mean and variance differ): ğ‘=0.3,ğ‘=1.3.
When the two samples are from the same distribution, the probability that one observation
finds another observation among its first ğ‘˜nearest neighbors is always ğ‘˜/(ğ‘âˆ’1). Letğ‘(0)
11and
ğ‘(0)
22be the expectations of ğ‘11andğ‘22when the two samples are from the same distribution.
Thenğ‘(0)
11=(ğ‘šâˆ’1)/(ğ‘âˆ’1)andğ‘(0)
22=(ğ‘›âˆ’1)/(ğ‘âˆ’1). Under setting 0, we see that ğ‘11and
ğ‘22are close toğ‘(0)
11andğ‘(0)
22, respectively, across all ğ‘˜values.
When the two samples are from different distributions with a mean difference, ğ‘11andğ‘22are
larger thanğ‘(0)
11andğ‘(0)
22, respectively, until ğ‘˜=ğ‘âˆ’1, where each observation finds every other
observations as its first ğ‘âˆ’1 nearest neighbors.5
Fig. 3: Plots of ğ‘11(black) andğ‘22(blue) considering ğ‘˜nearest neighbors under different settings.
The dashed lines represent the baseline when the two clusters are from the same distribution
ğ‘(0)
11=(ğ‘šâˆ’1)/(ğ‘âˆ’1)(black dashed line) and ğ‘(0)
22=(ğ‘›âˆ’1)/(ğ‘âˆ’1)(blue dashed line). The
two dashed lines overlap when ğ‘š=ğ‘›.
When the two distributions differ in scale, ğ‘11is larger than ğ‘(0)
11, butğ‘22is smaller than
ğ‘(0)
22across allğ‘˜values. This illustrate the curse of dimensionality. Here, ğ‘‘=200 and sample 2
is from a distribution with a larger variance than sample 1. As the volume of the space grows
exponentially with dimension, observations in sample 2 tend to find observations in sample 1 to
be closer than other observations in sample 2, resulting in ğ‘22being smaller than ğ‘(0)
22.
2.4. New clustering criteria
If the two clusters are indeed from two different distributions, we would have the following
three possible scenarios for some ğ‘˜â€™s according to Section 2.3:
(1)ğ‘11,ğ‘˜(ğ‘¥âˆ—)> ğ‘(0)
11,ğ‘22,ğ‘˜(ğ‘¥âˆ—)> ğ‘(0)
22,
(2)ğ‘11,ğ‘˜(ğ‘¥âˆ—)> ğ‘(0)
11,ğ‘22,ğ‘˜(ğ‘¥âˆ—)< ğ‘(0)
22,
(3)ğ‘11,ğ‘˜(ğ‘¥âˆ—)< ğ‘(0)
11,ğ‘22,ğ‘˜(ğ‘¥âˆ—)> ğ‘(0)
22.
From Figure 3, we see that ğ‘11,ğ‘˜(ğ‘¥âˆ—)andğ‘22,ğ‘˜(ğ‘¥âˆ—)depend onğ‘˜. Here, we first fix ğ‘˜and discuss
the clustering criteria with the aim of finding ğ‘¥âˆ—. We will then discuss the choice of ğ‘˜in Section
2.5. For notation simplicity, we use ğ‘…1(ğ‘¥)andğ‘…2(ğ‘¥)instead ofğ‘…1,ğ‘˜(ğ‘¥)andğ‘…2,ğ‘˜(ğ‘¥)in the rest of
this section.6
We borrow strengths from graph-based two-sample tests (Chen & Friedman, 2017) and propose
to use the following two quantities:
ğ‘ğ‘¤(ğ‘¥)=ğ‘…ğ‘¤(ğ‘¥)âˆ’ğœ‡ğ‘¤(ğ‘¥)
ğœğ‘¤(ğ‘¥), ğ‘…ğ‘¤(ğ‘¥)=ğ‘›âˆ’1
ğ‘âˆ’2ğ‘…1(ğ‘¥)+ğ‘šâˆ’1
ğ‘âˆ’2ğ‘…2(ğ‘¥),
ğ‘ğ‘‘(ğ‘¥)=ğ‘…ğ‘‘(ğ‘¥)âˆ’ğœ‡ğ‘‘(ğ‘¥)
ğœğ‘‘(ğ‘¥), ğ‘…ğ‘‘(ğ‘¥)=ğ‘…1(ğ‘¥)âˆ’ğ‘…2(ğ‘¥),
whereğœ‡ğ‘¤(ğ‘¥),ğœğ‘¤(ğ‘¥),ğœ‡ğ‘‘(ğ‘¥),ğœğ‘‘(ğ‘¥)are normalizing terms so that ğ‘ğ‘¤(ğ‘¥)andğ‘ğ‘‘(ğ‘¥)are properly
standardized to have mean 0 and variance 1 if there is only one cluster. Their expressions can be
obtained through similar treatments under the two-sample testing setting (Schilling, 1986; Henze,
1988) and the change-point detection setting (Liu & Chen, 2022). Let Xğ‘šbe the set of all ğ‘¥â€™s
withğ‘š1â€™s and(ğ‘âˆ’ğ‘š)=ğ‘›0â€™s. Then, for ğ‘¥âˆˆXğ‘š, we have,
ğœ‡ğ‘¤(ğ‘¥)=(ğ‘šâˆ’1)(ğ‘›âˆ’1)
(ğ‘âˆ’1)(ğ‘âˆ’2)ğ‘˜ğ‘,
ğœğ‘¤(ğ‘¥)=ğ‘šğ‘›(ğ‘šâˆ’1)(ğ‘›âˆ’1)
ğ‘(ğ‘âˆ’1)(ğ‘âˆ’2)(ğ‘âˆ’3)
ğ‘˜ğ‘+ğ‘1,ğ‘˜âˆ’1
ğ‘âˆ’2(ğ‘2,ğ‘˜+ğ‘˜ğ‘âˆ’ğ‘˜2ğ‘)âˆ’2
ğ‘âˆ’1ğ‘˜2ğ‘
,
ğœ‡ğ‘‘(ğ‘¥)=ğ‘˜(ğ‘šâˆ’ğ‘›),
ğœğ‘‘(ğ‘¥)=ğ‘šğ‘›
ğ‘(ğ‘âˆ’1)(ğ‘2,ğ‘˜+ğ‘˜ğ‘âˆ’ğ‘˜2ğ‘).
whereğ‘1,ğ‘˜=Ã
(ğ‘–,ğ‘—)âˆˆğºğ‘˜ğ¼(ğ‘—,ğ‘–)âˆˆğºğ‘˜andğ‘2,ğ‘˜=Ãğ‘
ğ‘–=1Ã
ğ‘—â‰ ğ‘™ğ¼(ğ‘—,ğ‘–),(ğ‘™,ğ‘–)âˆˆğºğ‘˜.
Define E2(Â·)as the expectation under the assumption that there are two clusters.
Theorem 1. For any fixed ğ‘˜, ifğ‘11,ğ‘˜(ğ‘¥âˆ—)andğ‘22,ğ‘˜(ğ‘¥âˆ—)fall into any of the three scenarios, we
have:
â€¢Under scenario (1), E2(ğ‘ğ‘¤(ğ‘¥))is maximized at ğ‘¥=ğ‘¥âˆ—orğ‘¥=1âˆ’ğ‘¥âˆ—.
â€¢Under scenario (2), E2(ğ‘ğ‘‘(ğ‘¥))is maximized at ğ‘¥=ğ‘¥âˆ—.
â€¢Under scenario (3), E2(ğ‘ğ‘‘(ğ‘¥))is maximized at ğ‘¥=1âˆ’ğ‘¥âˆ—.
Based on the above theorem, we could use max ğ‘¥ğ‘ğ‘¤(ğ‘¥)to cluster in scenario (1), and use
maxğ‘¥ğ‘ğ‘‘(ğ‘¥)to cluster in scenarios (2) and (3). However, in practice, we wonâ€™t know which
scenario it is, so we propose to use
ğ‘€ğœ…(ğ‘¥)=max(ğ‘ğ‘¤(ğ‘¥),ğœ…ğ‘ğ‘‘(ğ‘¥))
as the criterion. The choice of ğœ…is discussed in Section 2.6 with ğœ…=1.55 as the default choice.
Table 2 adds the mis-clustering rate of the new method to Table 1. We see that the mis-clustering
rate of the new method under setting 1 is on par with the best remaining methods, and the
mis-clustering rate of the new method under setting 2 is much lower than all other methods.
Table 2: Mis-clustering rate under the same settings as in Table 1.
Spectral Specc RSpec(A) RSpec(L) IF-PCA ğ‘¡-SNE New
Setting 1 0.157 0.006 0.279 0.167 0.254 0.098 0.010
Setting 2 0.467 0.446 0.418 0.371 0.451 0.462 0.041
Remark 1. It can also be shown that, under scenario (1), E2(ğ‘…ğ‘¤(ğ‘¥))âˆ’ğœ‡ğ‘¤(ğ‘¥)is maximized
atğ‘¥=ğ‘¥âˆ—orğ‘¥=1âˆ’ğ‘¥âˆ—; under scenario (2), E2(ğ‘…ğ‘‘(ğ‘¥))âˆ’ğœ‡ğ‘‘(ğ‘¥)is maximized at ğ‘¥=ğ‘¥âˆ—; under
scenario (3), E2(ğ‘…ğ‘‘(ğ‘¥))âˆ’ğœ‡ğ‘‘(ğ‘¥)is maximized at ğ‘¥=1âˆ’ğ‘¥âˆ—. However, we do not recommend
usingğ‘…ğ‘¤(ğ‘¥)âˆ’ğœ‡ğ‘¤(ğ‘¥)andğ‘…ğ‘‘(ğ‘¥)âˆ’ğœ‡ğ‘‘(ğ‘¥)as the criteria because they are not well normalized
and are difficult to combine if one does not know which scenario the actual data falls in.7
Proof. We first compute E2(ğ‘…ğ‘¤(ğ‘¥))andE2(ğ‘…ğ‘‘(ğ‘¥)). Forğ‘¥, suppose there are Î”1âˆˆ[0,ğ‘šâˆ—]
observations in cluster 1 placed into cluster 2, and Î”2âˆˆ[0,ğ‘›âˆ—]observations in cluster 2 placed
into cluster 1. So ğ‘¥âˆˆXğ‘šwithğ‘š=ğ‘šâˆ—âˆ’Î”1+Î”2. Sinceğ‘˜is fixed here, for simplicity, we denote
ğ‘11,ğ‘˜(ğ‘¥âˆ—),ğ‘12,ğ‘˜(ğ‘¥âˆ—),ğ‘21,ğ‘˜(ğ‘¥âˆ—),ğ‘22,ğ‘˜(ğ‘¥âˆ—)byğ‘âˆ—
11,ğ‘âˆ—
12,ğ‘âˆ—
21,ğ‘âˆ—
22, respectively. Then, we have,
E2(ğ‘…1(ğ‘¥))=ğ‘âˆ—
11ğ‘˜ğ‘šâˆ—(ğ‘šâˆ—âˆ’Î”1)(ğ‘šâˆ—âˆ’Î”1âˆ’1)
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)+ğ‘âˆ—
12ğ‘˜ğ‘šâˆ—(ğ‘šâˆ—âˆ’Î”1)Î”2
ğ‘šâˆ—ğ‘›âˆ—+ğ‘âˆ—
21ğ‘˜ğ‘›âˆ—(ğ‘šâˆ—âˆ’Î”1)Î”2
ğ‘šâˆ—ğ‘›âˆ—+ğ‘22ğ‘˜ğ‘›âˆ—Î”2(Î”2âˆ’1)
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1),
E2(ğ‘…2(ğ‘¥))=ğ‘âˆ—
22ğ‘˜ğ‘›âˆ—(ğ‘›âˆ—âˆ’Î”2)(ğ‘›âˆ—âˆ’Î”2âˆ’1)
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)+ğ‘âˆ—
21ğ‘˜ğ‘›âˆ—(ğ‘›âˆ—âˆ’Î”2)Î”1
ğ‘›âˆ—ğ‘šâˆ—+ğ‘âˆ—
12ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’Î”2)Î”1
ğ‘›âˆ—ğ‘šâˆ—+ğ‘âˆ—
11ğ‘˜ğ‘šâˆ—Î”1(Î”1âˆ’1)
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1),
E2(ğ‘…ğ‘¤(ğ‘¥))=ğ‘›âˆ—+Î”1âˆ’Î”2âˆ’1
ğ‘âˆ’2E2(ğ‘…1(ğ‘¥))+ğ‘šâˆ—âˆ’Î”1+Î”2âˆ’1
ğ‘âˆ’2E2(ğ‘…2(ğ‘¥))
=ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22
ğ‘âˆ’2+ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’ğ‘(ğ‘šâˆ—âˆ’1)
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)(ğ‘âˆ’2)Î”2
1
+ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’ğ‘(ğ‘›âˆ—âˆ’1)
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)(ğ‘âˆ’2)Î”2
2+2ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22+ğ‘
ğ‘šâˆ—ğ‘›âˆ—(ğ‘âˆ’2)Î”1Î”2
âˆ’ğ‘˜Î”1(2ğ‘šâˆ—âˆ’1)(ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)âˆ—ğ‘âˆ—
22)âˆ’ğ‘(ğ‘šâˆ—âˆ’1)2
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)(ğ‘âˆ’2)
âˆ’ğ‘˜Î”2(2ğ‘›âˆ—âˆ’1)(ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)âˆ—ğ‘âˆ—
22)âˆ’ğ‘(ğ‘›âˆ—âˆ’1)2
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)(ğ‘âˆ’2),
E2(ğ‘…ğ‘‘(ğ‘¥))=E2(ğ‘…1(ğ‘¥))âˆ’E2(ğ‘…2(ğ‘¥))
=ğ‘âˆ—
11ğ‘˜ğ‘šâˆ—âˆ’ğ‘âˆ—
22ğ‘˜ğ‘›âˆ—âˆ’ğ‘˜Î”1
ğ‘âˆ—
11âˆ’ğ‘›âˆ—
ğ‘šâˆ—ğ‘âˆ—
22+ğ‘
ğ‘šâˆ—
+ğ‘˜Î”2
ğ‘âˆ—
22âˆ’ğ‘šâˆ—
ğ‘›âˆ—ğ‘âˆ—
11+ğ‘
ğ‘›âˆ—
.
We start with E2(ğ‘ğ‘‘(ğ‘¥))as it is relatively simpler, and some of the intermediate results can be
used for E2(ğ‘ğ‘¤(ğ‘¥)). Notice that
E2(ğ‘…ğ‘‘(ğ‘¥))âˆ’ğœ‡ğ‘‘(ğ‘¥)
=ğ‘âˆ—
11ğ‘˜ğ‘šâˆ—âˆ’ğ‘âˆ—
22ğ‘˜ğ‘›âˆ—âˆ’ğ‘˜(ğ‘šâˆ’ğ‘›)âˆ’ğ‘˜Î”1
ğ‘âˆ—
11âˆ’ğ‘›âˆ—
ğ‘šâˆ—ğ‘âˆ—
22+ğ‘
ğ‘šâˆ—
+ğ‘˜Î”2
ğ‘âˆ—
22âˆ’ğ‘šâˆ—
ğ‘›âˆ—ğ‘âˆ—
11+ğ‘
ğ‘›âˆ—
=ğ‘˜(ğ‘âˆ—
11ğ‘šâˆ—âˆ’ğ‘âˆ—
22ğ‘›âˆ—âˆ’(ğ‘šâˆ—âˆ’ğ‘›âˆ—))âˆ’ğ‘˜Î”1
ğ‘âˆ—
11âˆ’ğ‘›âˆ—
ğ‘šâˆ—ğ‘âˆ—
22+ğ‘
ğ‘šâˆ—âˆ’2
+ğ‘˜Î”2
ğ‘âˆ—
22âˆ’ğ‘šâˆ—
ğ‘›âˆ—ğ‘âˆ—
11+ğ‘
ğ‘›âˆ—âˆ’2
=(ğ‘âˆ—
11ğ‘šâˆ—âˆ’ğ‘âˆ—
22ğ‘›âˆ—âˆ’(ğ‘šâˆ—âˆ’ğ‘›âˆ—))ğ‘˜
1âˆ’Î”1
ğ‘šâˆ—âˆ’Î”2
ğ‘›âˆ—
.
Then
E2(ğ‘ğ‘‘(ğ‘¥))=E2(ğ‘…ğ‘‘(ğ‘¥))âˆ’ğœ‡ğ‘‘(ğ‘¥)
ğœğ‘‘(ğ‘¥)=(ğ‘âˆ—
11ğ‘šâˆ—âˆ’ğ‘âˆ—
22ğ‘›âˆ—âˆ’(ğ‘šâˆ—âˆ’ğ‘›âˆ—))ğ‘˜ğ‘“ğ‘‘(Î”1,Î”2),
where
ğ‘“ğ‘‘(Î”1,Î”2)=1âˆ’Î”1
ğ‘šâˆ—âˆ’Î”2
ğ‘›âˆ—âˆš
(ğ‘šâˆ—âˆ’Î”1+Î”2)(ğ‘›âˆ—+Î”1âˆ’Î”2).
Note that
ğœ•ğ‘“ğ‘‘
ğœ•Î”1=âˆ’ğ‘
2ğ‘šâˆ—ğ‘›âˆ—(ğ‘›âˆ—âˆ’Î”2)(ğ‘šâˆ—âˆ’Î”1+2Î”2)+Î”1Î”2
(ğ‘šâˆ—âˆ’Î”1+Î”2)1.5(ğ‘›âˆ—+Î”1âˆ’Î”2)1.5<0,
ğœ•ğ‘“ğ‘‘
ğœ•Î”2=âˆ’ğ‘
2ğ‘šâˆ—ğ‘›âˆ—(ğ‘šâˆ—âˆ’Î”1)(ğ‘›âˆ—âˆ’Î”2+2Î”1)+Î”1Î”2
(ğ‘šâˆ—âˆ’Î”1+Î”2)1.5(ğ‘›âˆ—+Î”1âˆ’Î”2)1.5<0.
Soğ‘“ğ‘‘(Î”1,Î”2)achieves its maximum when Î”1=Î” 2=0 with the maximum being1âˆš
ğ‘šâˆ—ğ‘›âˆ—, and
achieve its minimum when Î”1=ğ‘šâˆ—,Î”2=ğ‘›âˆ—with the minimum being âˆ’1âˆš
ğ‘šâˆ—ğ‘›âˆ—. Also note that,
under scenario (2), we have
ğ‘âˆ—
11ğ‘šâˆ—âˆ’ğ‘âˆ—
22ğ‘›âˆ—âˆ’(ğ‘šâˆ—âˆ’ğ‘›âˆ—)>ğ‘šâˆ—âˆ’1
ğ‘âˆ’1ğ‘šâˆ—âˆ’ğ‘›âˆ—âˆ’1
ğ‘âˆ’1âˆ’(ğ‘šâˆ—âˆ’ğ‘›âˆ—)=0,8
and under scenario (3), we have
ğ‘âˆ—
11ğ‘šâˆ—âˆ’ğ‘âˆ—
22ğ‘›âˆ—âˆ’(ğ‘šâˆ—âˆ’ğ‘›âˆ—)<0.
Therefore, under scenario (2), E2(ğ‘ğ‘‘(ğ‘¥))is maximized when Î”1=Î” 2=0, i.e., atğ‘¥=ğ‘¥âˆ—; and
under scenario (3), E2(ğ‘ğ‘‘(ğ‘¥))is maximized when Î”1=ğ‘šâˆ—,Î”2=ğ‘›âˆ—, i.e., atğ‘¥=1âˆ’ğ‘¥âˆ—.
Now, we move onto E2(ğ‘ğ‘¤(ğ‘¥)). We first compute E2(ğ‘…ğ‘¤(ğ‘¥))âˆ’ğœ‡ğ‘¤(ğ‘¥):
E2(ğ‘…ğ‘¤(ğ‘¥))âˆ’ğœ‡ğ‘¤(ğ‘¥)
=ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘âˆ’2+ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)(ğ‘âˆ’2)Î”2
1
+ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)(ğ‘âˆ’2)Î”2
2+2ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘šâˆ—ğ‘›âˆ—(ğ‘âˆ’2)Î”1Î”2
âˆ’ğ‘˜Î”1(2ğ‘šâˆ—âˆ’1)
ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)âˆ—ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)(ğ‘âˆ’2)
âˆ’ğ‘˜Î”2(2ğ‘›âˆ—âˆ’1)
ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)âˆ—ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)(ğ‘âˆ’2)
=ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘âˆ’2
Ã—
1+Î”2
1
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)+Î”2
2
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)+2Î”1Î”2
ğ‘šâˆ—ğ‘›âˆ—âˆ’(2ğ‘šâˆ—âˆ’1)Î”1
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)âˆ’(2ğ‘›âˆ—âˆ’1)Î”2
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)
.
Then
E2(ğ‘ğ‘¤(ğ‘¥))=ğ‘˜ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
ğ‘âˆ’2ğ‘“ğ‘¤(Î”1,Î”2),
where
ğ‘“ğ‘¤(Î”1,Î”2)=1+Î”2
1
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)+Î”2
2
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)+2Î”1Î”2
ğ‘šâˆ—ğ‘›âˆ—âˆ’(2ğ‘šâˆ—âˆ’1)Î”1
ğ‘šâˆ—(ğ‘šâˆ—âˆ’1)âˆ’(2ğ‘›âˆ—âˆ’1)Î”2
ğ‘›âˆ—(ğ‘›âˆ—âˆ’1)âˆš
(ğ‘šâˆ—âˆ’Î”1+Î”2)(ğ‘›âˆ—+Î”1âˆ’Î”2)(ğ‘šâˆ—âˆ’Î”1+Î”2âˆ’1)(ğ‘›âˆ—+Î”1âˆ’Î”2âˆ’1).
Itâ€™s complicated to work on ğ‘“ğ‘¤directly. Here, we first consider
Ëœğ‘“ğ‘‘(Î”1,Î”2)=1âˆ’Î”1
ğ‘šâˆ—âˆ’1âˆ’Î”2
ğ‘›âˆ—âˆ’1âˆš
(ğ‘šâˆ—âˆ’Î”1+Î”2âˆ’1)(ğ‘›âˆ—+Î”1âˆ’Î”2âˆ’1).
With similar arguments as for ğ‘“ğ‘‘(Î”1,Î”2), we know that, for Î”1âˆˆ[0,ğ‘šâˆ—âˆ’1],Î”2âˆˆ[0,ğ‘›âˆ—âˆ’1],
Ëœğ‘“ğ‘‘(Î”1,Î”2)achieves its maximum when Î”1=Î” 2=0 with the maximum being1âˆš
(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1), and
achieve its minimum when Î”1=ğ‘šâˆ—âˆ’1,Î”2=ğ‘›âˆ—âˆ’1 with the minimum being âˆ’1âˆš
(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1).
For the three corner cases (Î”1=ğ‘šâˆ—âˆ’1,Î”2=ğ‘›âˆ—),(Î”1=ğ‘šâˆ—,Î”2=ğ‘›âˆ—âˆ’1),(Î”1=ğ‘šâˆ—,Î”2=ğ‘›âˆ—),
we discuss them slightly later.
Hence, forÎ”1âˆˆ[0,ğ‘šâˆ—âˆ’1],Î”2âˆˆ[0,ğ‘›âˆ—âˆ’1], we haveğ‘“ğ‘‘(Î”1,Î”2)Ëœğ‘“ğ‘‘(Î”1,Î”2)achieves its maxi-
mum atÎ”1=Î” 2=0 andÎ”1=ğ‘šâˆ—âˆ’1,Î”2=ğ‘›âˆ—âˆ’1 with the maximum1âˆš
ğ‘šâˆ—ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1). Notice
that
ğ‘“ğ‘¤(Î”1,Î”2)=ğ‘“ğ‘‘(Î”1,Î”2)Ëœğ‘“ğ‘‘(Î”1,Î”2)âˆ’â„(Î”1,Î”2),
where
â„(Î”1,Î”2)=ğ‘âˆ’2
ğ‘šâˆ—ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)Î”1Î”2âˆš
(ğ‘šâˆ—âˆ’Î”1+Î”2)(ğ‘›âˆ—+Î”1âˆ’Î”2)(ğ‘šâˆ—âˆ’Î”1+Î”2âˆ’1)(ğ‘›âˆ—+Î”1âˆ’Î”2âˆ’1).
Here,â„(Î”1,Î”2)>0 whenÎ”1Î”2>0, soğ‘“ğ‘¤(0,0)> ğ‘“ğ‘¤(ğ‘šâˆ—âˆ’1,ğ‘›âˆ—âˆ’1). For the three left out
corner cases, we have9
â€¢ğ‘“ğ‘¤(ğ‘šâˆ—,ğ‘›âˆ—)=1âˆš
ğ‘›âˆ—ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)(ğ‘šâˆ—âˆ’1)=ğ‘“ğ‘¤(0,0).
â€¢ğ‘“ğ‘¤(ğ‘šâˆ—,ğ‘›âˆ—âˆ’1)=1âˆ’2
ğ‘›âˆ—âˆš
(ğ‘›âˆ—âˆ’1)(ğ‘šâˆ—+1)(ğ‘›âˆ—âˆ’2)ğ‘šâˆ—=âˆšï¸ƒ
(1âˆ’2
ğ‘›âˆ—)(1âˆ’2
ğ‘šâˆ—+1)âˆš
ğ‘›âˆ—ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)(ğ‘šâˆ—âˆ’1)< ğ‘“ğ‘¤(0,0).
â€¢ğ‘“ğ‘¤(ğ‘šâˆ—âˆ’1,ğ‘›âˆ—)=1âˆ’2
ğ‘šâˆ—âˆš
(ğ‘›âˆ—+1)(ğ‘šâˆ—âˆ’1)ğ‘›âˆ—(ğ‘šâˆ—âˆ’2)=âˆšï¸ƒ
(1âˆ’2
ğ‘šâˆ—)(1âˆ’2
ğ‘›âˆ—+1)âˆš
ğ‘›âˆ—ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)(ğ‘šâˆ—âˆ’1)< ğ‘“ğ‘¤(0,0). â–¡
Hence,ğ‘“ğ‘¤(Î”1,Î”2)achieves its maximum when Î”1=Î” 2=0 orÎ”1=ğ‘šâˆ—,Î”2=ğ‘›âˆ—. Under scenario
(1),
ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘âˆ—
11+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘âˆ—
22âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1
>ğ‘šâˆ—(ğ‘›âˆ—âˆ’1)ğ‘šâˆ—âˆ’1
ğ‘âˆ’1+ğ‘›âˆ—(ğ‘šâˆ—âˆ’1)ğ‘›âˆ—âˆ’1
ğ‘âˆ’1âˆ’(ğ‘šâˆ—âˆ’1)(ğ‘›âˆ—âˆ’1)ğ‘
ğ‘âˆ’1=0.
SoE2(ğ‘ğ‘‘(ğ‘¥))achieves its maximum when Î”1=Î” 2=0 orÎ”1=ğ‘šâˆ—,Î”2=ğ‘›âˆ—, i.e., whenğ‘¥=ğ‘¥âˆ—
orğ‘¥=1âˆ’ğ‘¥âˆ—.
Given the results from Theorem 1, the next step is to find an efficient way to solve the
optimization problem. Generally speaking, the optimization of ğ‘ğ‘¤(ğ‘¥)orğ‘ğ‘‘(ğ‘¥)is an NP-hard
problem. Instead of searching through the entire label assignment space, we address this task
with a greedy algorithm. Suppose we are given an initial division vector xof the clusters into two
groups (this vector could be randomly generated). We then examine all possible single changes
tox, defined as changing one element in xfromğ‘ to 1âˆ’ğ‘ , and choose the change that will result
in the largest increase in the corresponding ğ‘ğ‘¤(orğ‘ğ‘‘). We continue making such moves until
no further increase is possible. We start with many initial values for the partition xand pick the
result that yields the maximum value of ğ‘ğ‘¤(orğ‘ğ‘‘). Notice that at each step, we only change one
element of x. Therefore, it is unnecessary to loop over the entire adjacency matrix to calculate
ğ‘…1,ğ‘…2,ğœğ‘‘, andğœğ‘¤to update the values of ğ‘ğ‘¤(ğ‘¥)andğ‘ğ‘‘(ğ‘¥). Instead, we can only check the
edges related to the label switch to calculate the increase or decrease in ğ‘…1andğ‘…2. Forğœğ‘‘and
ğœğ‘¤, the only terms related to the optimization process are ğ‘š,ğ‘›, which can be easily obtained in
each step.
2.5. Choice of ğ‘˜
The first step of our clustering procedure is to construct ğºğ‘˜(theğ‘˜-NN graph). Here, ğ‘˜can be
any integer between 1 and ğ‘âˆ’1. We can create all the possible ğºğ‘˜and perform the clustering
process. Typically, different values of ğ‘˜lead to different clustering outcomes. Since the underlying
true partition of the clusters is usually not available, a natural question arises: how to choose ğ‘˜
such that the clustering criteria have the optimal power or give us the most reasonable partition.
In this section, we intend to empirically discuss the choice of ğ‘˜.
We want to choose ğ‘˜such thatğ‘11,ğ‘˜(ğ‘¥âˆ—)andğ‘22,ğ‘˜(ğ‘¥âˆ—)are far from ğ‘(0)
11andğ‘(0)
22, while
ensuring there are enough edges in the graph to contribute to the evidence. Heuristically, when
the value of ğ‘˜is small, the graph is sparse and some useful similarity information among the
individuals might not be fully captured by ğ‘ğ‘¤andğ‘ğ‘‘. On the other hand, when ğ‘˜is too large, it
includes some irrelevant information. If groundtruth is available for a given dataset, there always
exists an optimal (or a few optimal) choice of ğ‘˜that will result in the partition with the largest
overlap with the underlying true group assignments.
The choice of the hyper-parameter ğ‘˜under the two-sample testing and change-point detection
regime for the statistic ğ‘ğ‘¤andğ‘ğ‘‘was discussed in Zhu & Chen (2021), Zhang & Chen (2021)
and Zhou & Chen (2022). Specifically, they consider the power of the methods for ğ‘˜=[ğ‘ğœ†]when10
Fig. 4: Values of ğ‘ğ‘¤,ğ‘ğ‘‘, andğ‘€ğœ…(top panel) and mis-clustering rate (bottom panel) over ğ‘˜.
ğ‘š=ğ‘›=50.
varyingğœ†from 0 to 1 in different settings. In two-sample testing, Zhu & Chen (2021) suggested
usingğ‘˜=[ğ‘0.5]with theğ‘˜-MST, while Zhou & Chen (2022) suggested ğœ†=0.65 for theğ‘˜-NN
graph.
Here, we explore the relation between ğ‘˜and mis-clustering rate. In particular, we aim to find
out whether the ğ‘˜that leads to the maximum value of ğ‘€ğœ…results in the minimum clustering error.
In each simulation setting, we generate the ğ‘˜-NN graphs for ğ‘˜from 1 toğ‘âˆ’3, increased by 2
at each time. We consider the Gaussian mixture example where we have two groups of samples
fromNğ‘‘(0,Î£)andNğ‘‘(ğ‘1,ğ‘Î£), withÎ£ğ‘–ğ‘—=0.1|ğ‘–âˆ’ğ‘—|.
â€¢Setting 1 (only mean differs): ğ‘ >0,ğ‘=1.
â€¢Setting 2 (only variance differs): ğ‘=0,ğ‘ > 1.
â€¢Setting 3 (both mean and variance differ): ğ‘ >0,ğ‘ > 1.
We first examine the values of ğ‘ğ‘¤andğ‘ğ‘‘for different settings, along with the value of ğ‘€ğœ…with
ğœ…=1.55, i.e.,ğ‘€ğœ…(ğ‘¥)=max(ğ‘ğ‘¤(ğ‘¥),1.55ğ‘ğ‘‘(ğ‘¥)). The choice of ğœ…is discussed in Section 2.6.
Here, our primarily concern is the value of ğ‘˜that leads to the maximum ğ‘€ğœ…, and the value of
ğœ…does not affect this choice of ğ‘˜as long asğœ…is within a certain range. We set the dimension
to beğ‘‘=800. For the cluster sizes, we examine both balanced ( ğ‘š=ğ‘›=50) and unbalanced
(ğ‘š=30,ğ‘›=70) cases.
Figure 4 plots the values of ğ‘ğ‘¤,ğ‘ğ‘‘, andğ‘€ğœ…overğ‘˜under all three settings for the balanced case
(top panel), as well as the mis-clustering rate (bottom panel). We see that the mis-clustering rate
reaches its minimum when ğ‘€ğœ…is at its maximum in all settings. The same pattern also appears
for the unbalanced case (Figure 5). This is exciting, as ğ‘€ğœ…overğ‘˜is fully observable given the
data, allowing us to select the ğ‘˜that maximizes ğ‘€ğœ…in practice.11
Fig. 5: Values of ğ‘ğ‘¤,ğ‘ğ‘‘, andğ‘€ğœ…(top panel) and mis-clustering rate (bottom panel) over ğ‘˜.
ğ‘š=30,ğ‘›=70.
2.6. Choice of ğœ…
From Section 2.4, max ğ‘¥ğ‘ğ‘¤(ğ‘¥)can be used to cluster scenario (1), and max ğ‘¥ğ‘ğ‘‘(ğ‘¥)is suitable
for scenarios (2) and (3). Since we donâ€™t know which scenario we are dealing with in reality,
ğ‘€ğœ…(ğ‘¥)=max(ğ‘ğ‘¤(ğ‘¥),ğœ…ğ‘ğ‘‘(ğ‘¥))was proposed for these three scenarios. Ideally, when the scenario
is (1), we want ğ‘ğ‘¤to dominate in ğ‘€ğœ…; while when the scenario is (2) or (3), we want ğ‘ğ‘‘to
dominate in ğ‘€ğœ…. However, this dominance does not need to hold for all ğ‘˜â€™s. Instead, as long as
ğ‘€ğœ…picks the correct quantity ( ğ‘ğ‘¤orğ‘ğ‘‘) when the correct quantity reaches its maximum, the
goal is achieved given how we choose ğ‘˜. This can be seen clearly from Figure 4 and 5. In Figure
4 and 5, the leftmost pictures show two clusters generated from Nğ‘‘(0,Î£)andNğ‘‘(ğ‘1,Î£). This
belongs to scenario (1), and ğ‘ğ‘¤should dominate ğ‘€ğœ…based on the statements above. We see that
ğ‘€ğœ…is identical to ğ‘ğ‘¤whenğ‘ğ‘¤reach its maximum but differs from ğ‘ğ‘¤at the boundaries. That
will not affect the mis-clustering rate using ğ‘€ğœ…given how we choose ğ‘˜.
With these observations, we use the following simulation study to choose ğœ…. All other parameters
remain the same as in Figure 4 setting 1 (only mean differs). We gradually increase the location
difference parameter ğ‘to determine the upper bound of ğœ…. All other parameters remain the same as
in Figure 4 setting 2 (only variance differs), we gradually increase the scale difference parameter
ğ‘to determine the lower bound of ğœ…. For each setting, we run 50 times.
In Figure 6, left panel, the mis-clustering rate significantly decreases at around ğ‘=0.2. When
the signal is less than this threshold, the difference between the two clusters is not substantial
enough for the algorithm to distinguish them effectively, resulting in a mis-clustering rate close
to 0.5. After this phase transition, the mis-clustering rate is much smaller. Therefore, we want ğ‘ğ‘¤
to dominate in ğ‘€ğœ…whenğ‘â‰¥0.2, which means choosing ğœ…smaller than the ratio when ğ‘â‰¥0.2.
This establishes an upper bound for ğœ…, which is the minimum value of the boxplot at ğ‘=0.2.
Similarly, in the right panel, we want ğ‘ğ‘‘to dominate in ğ‘€ğœ…whenğ‘â‰¥1.15. This establishes a
lower bound for ğœ…, which is the maximum value of the boxplot at ğ‘=1.15. Thus, a proper choice12
Fig. 6: Plots of ratio ( ğ‘Ÿ=ğ‘ğ‘¤/ğ‘ğ‘‘) versus signal strength. Left panel: location difference with
ğ‘=1, andğ‘increasing from 0.05 to 0.3. The blue dashed line at ğ‘Ÿ=1.8 represents the minimum
value of the boxplot when ğ‘=0.2. The â€œmis rateâ€ indicates the mean mis-clustering rate when
usingğ‘ğ‘¤as the criterion, with green representing higher errors and orange indicating lower errors.
Right panel: scale difference with ğ‘=0, andğ‘increasing from 1.05 to 1.3. The red dashed line
atğ‘Ÿ=1.3 represents the maximum value of the boxplot when ğ‘=1.15. The â€œmis rateâ€ indicates
the mean mis-clustering rate when using ğ‘ğ‘‘as the criterion, with green representing higher errors
and orange indicating lower error.
ofğœ…would be between the upper bound ğ‘Ÿ=1.8 and the lower bound ğ‘Ÿ=1.3. For simplicity, we
choose the middle point ğœ…=1.55 as our default choice. It is important to note that if there is
a prior probability indicating which of the three scenarios a particular problem belongs to, the
choice ofğœ…can be adjusted to favor more on ğ‘ğ‘¤orğ‘ğ‘‘accordingly.
2.7. A ternary search algorithm
In this Section, we introduce a search algorithm for selecting ğ‘˜when computational resources
are limited. Specifically, we focus on the algorithm of choosing ğ‘˜in Section 2.5. Notice that
max
ğ‘˜ğ‘€ğœ…=max
ğ‘˜max
ğ‘¥(ğ‘ğ‘¤(ğ‘¥),ğœ…ğ‘ğ‘‘(ğ‘¥)).
To find the maximum value of ğ‘€ğœ…, we need to determine the maximum values of ğ‘ğ‘¤andğ‘ğ‘‘
separately. From Theorem 1 and Figures 4 and 5, we observe that ğ‘ğ‘‘orğ‘ğ‘¤is a unimodal function
overğ‘˜when it is the proper criterion. Therefore, we can use a ternary search algorithm to find
the maximum value of ğ‘€ğœ…(Bajwa et al., 2015). We summarize the algorithm below. For ease of
notation, we denote the maximum value of ğ‘ğ‘¤(ğ‘¥)(orğ‘ğ‘‘(ğ‘¥)) on theğ‘˜-NN graphğºğ‘˜asğ‘“ğ‘˜(ğ‘¥).
Based on Theorem 1, we note that for a given dataset, only be one of ğ‘ğ‘¤orğ‘ğ‘‘may be a unimodal
function. While ternary search is guaranteed to find the optimal value for unimodal functions, it
suffices for our purposes since the maximum value of ğ‘€ğœ…corresponds to the maximum value of
the unimodal function when it is the correct clustering criterion.
3. Numerical studies
In this section, we compare the performance of the new method with other existing methods
on synthetic data. Specifically, we compare the following methods:13
Algorithm 1 . Ternary Search
Input:ğ‘“(ğ‘˜)for a series of ğ‘˜-NN graphsğºğ‘˜.
Output: The maximum value of ğ‘“(ğ‘˜).
Algorithm:
1. Set left to 1 and right toN - 1 .
2. While right -left is greater than 1, do the following:
a. Compute left third asâŒŠleft+(rightâˆ’left)/3âŒ‹.
b. Compute right third asâŒŠrightâˆ’(rightâˆ’left)/3âŒ‹.
c. Ifğ‘“(left third)is less thanğ‘“(right third), setleft toleft third ;
d. Otherwise, set right toright third .
3. Returnğ‘“j
left+right
2k
.
â€¢Spectral: Spectral clustering as described in Hastie et al. (2009).
â€¢Specc: The specc function in the kernlab package with â€˜kpar=localâ€™. (A discussion of the
choice of â€œkparâ€ can be found in Supplement S1.)
â€¢RSpecA and RSpecL: Regularized Spectral Clustering ( reg.SP in the randnet package)
proposed by Rohe et al. (2011), applied to the 10-NN adjacency matrix and the laplacian
matrix, respectively.
â€¢IF-PCA: Influential Feature PCA proposed by Jin & Wang (2016).
â€¢ğ‘¡-SNE: Using ğ‘¡-SNE to project the data into two dimensional space and then applying the
k-means clustering algorithm on the projected data (Mwangi et al., 2014).
We consider a few different settings below:
â€¢Gaussian mixture: Cluster 1: X1âˆ¼Nğ‘‘(0,Î£)and Cluster 2: X2âˆ¼Nğ‘‘(ğ‘1,ğ‘Î£),Î£ğ‘–ğ‘—=0.1|ğ‘–âˆ’ğ‘—|.
The location parameter ğ‘âˆˆ{0,0.1,0.5}and the scale parameter ğ‘âˆˆ{0.8,1,1.4}.
â€¢t-distribution mixture: Cluster 1: X1âˆ¼ğ‘¡20(0,Î£)and Cluster 2: X2âˆ¼ğ‘¡20(ğ‘1,ğ‘Î£),Î£ğ‘–ğ‘—=0.1|ğ‘–âˆ’ğ‘—|.
The location parameter ğ‘âˆˆ{0,0.25,0.3}and the scale parameter ğ‘âˆˆ{0.8,1,3,3.5}.
For each setting, the mis-clustering rate is estimated through 50 simulation runs. Figure 7 is
arranged in three columns representing location difference, scale difference, and location-scale
difference, respectively. The first column displays the results for the traditional location difference
Gaussian Mixture with a fixed covariance matrix. The top panel shows a small difference between
clusters with ğ‘=0.1, while the bottom panel shows a larger difference with ğ‘=0.5. The second
column displays the results for scale-only difference, considering both the variance parameter
ğ‘ >1 andğ‘ <1. The third column combines the parameters from the first and second columns.
We observe that, except for the top left panel where all methods are inefficient due to the small
signal, the new method performs well for all other settings, either being the best or among
the best methods. In particular, under scale difference (middle panel), our method dramatically
outperforms the others.
Figure 8 shows mis-clustering rates for different methods under the mixture of two multivariate
ğ‘¡-distributions. In the multivariate ğ‘¡-distribution location-difference mixture scenario, ğ‘¡-SNE
performs very well when the difference is small, which is expected given how ğ‘¡-SNE was derived.
However, when the signal increases (lower left panel), our new method performs even better
thanğ‘¡-SNE. In the scale difference scenario, Specc shows surprisingly high power. It performs14
Fig. 7: Mis-clustering rates for Gaussian mixtures Nğ‘‘(0,Î£)andNğ‘‘(ğ‘1,ğ‘Î£).
slightly better than our new method, while other methods completely lose power. However, for
the location-scale mixture case with moderate to high dimensions, our new method remains the
most effective, followed by Specc, ğ‘¡-SNE, Spectral, and other methods.
Fig. 8: Mis-clustering rates for multivariate ğ‘¡-distribution mixtures ğ‘¡20(0,Î£)andğ‘¡20(ğ‘1,ğ‘Î£).15
To sum up, for the ğ‘¡20distribution, our method remains the overall best among as the dimension
ğ‘‘increases, with Specc slightly outperforming it in the scale difference scenario. While ğ‘¡-SNE
performs significantly better compared to the Gaussian case, this is entirely expected because it
was developed for heavy distribution distributions. All other methods are not as powerful as our
new method in the ğ‘¡20setting.
4. Real-world applications
Here, we apply the proposed method to seven gene expression datasets, each with two classes
and more features than the sample size: Alon et al. (1999); Golub et al. (1999); Wang et al.
(2005); Gordon et al. (2002); Bhattacharjee et al. (2001); Singh et al. (2002); Su et al. (2001).
The datasets are summarized in Table 3. Datasets 1, 2, and 6 were analyzed in Dettling (2004),
while datasets 3, 5, and 7 were analyzed and grouped into two classes in Yousefi et al. (2010).
Dataset 4 is from Gordon et al. (2002).
We summarize the datasets used in our analysis in Table 3. In all these datasets, the true labels
are provided and are considered as the â€œground truthâ€. These labels are only used to evaluate
the error rate of various clustering methods. Table 4 displays the mis-clustering rates of different
methods on these seven datasets.
For the Colon Cancer dataset, our new method achieves a mis-clustering rate of 0.112, while
all other methods have a mis-clustering rate above 0.2. In a recent study (Singh & Verma, 2022),
the best performance of various methods on the Colon Cancer dataset in terms of the Random
Index (RI) was 51.77%. In comparison, our clustering result has an RI value of 79.64%, which
is a significant improvement. It is noteworthy that the Colon Cancer dataset is known to be
challenging for clustering, even for classification tasks where the labels of the training samples
are available (Donoho & Jin, 2008). Therefore, achieving an 11.2% mis-clustering error is a
breakthrough for the new method.
For the Leukemia dataset, all methods appear to be effective. Our method and Specc have
the lowest mis-clustering rate at 4.2%. For the Lung Cancer (1) dataset, IF-PCA performs best
with a mis-clustering rate of 3.3%. Our new method and Specc have a mis-clustering rate around
10%. Traditional Spectral clustering achieves a 5.5% misclustering rate. Other methods are not
effective. For the Lung Cancer (2) dataset, several methods, including Specc, IF-PCA and our
new methods, achieve equivalent best performance.
For the Breast Cancer, SuCancer, and Prostate Cancer datasets, none of the methods achieve a
mis-clustering rate lower than 30%. New methods are needed for these three datasets. It is worth
Table 3: Dataset information
Data name Abbreviation Source ğ‘› ğ‘
Colon Cancer Cln Alon et al. (99) 62 2000
Leukemia Leuk Golub et al. (99) 72 3571
Lung Cancer(1) Lung1 Gordon et al. (02) 181 12,533
Lung Cancer(2) Lung2 Bhattacharjee et al. (01) 203 12,600
Breast Cancer Brst Wang et al. (05) 276 22,215
Prostate Cancer Prst Singh et al. (02) 102 6033
SuCancer Sur Su et al. (01) 174 790916
Table 4: Mis-clustering rates on seven gene expression datasets.
Data Spectral Specc RSpec(A) RSpec(L) IF-PCA ğ‘¡-SNE New
Cln 0.467 0.451 0.209 0.258 0.403 0.415 0.112
Leuk 0.166 0.042 0.152 0.069 0.069 0.074 0.042
Lung1 0.055 0.093 0.254 0.403 0.033 0.466 0.116
Lung2 0.246 0.217 0.467 0.231 0.217 0.293 0.217
Brst 0.467 0.431 0.355 0.344 0.406 0.438 0.409
Sur 0.390 0.333 0.385 0.379 0.500 0.405 0.494
Prst 0.431 0.421 0.470 0.441 0.382 0.441 0.431
noting that the mis-clustering rate we report for IF-PCA on the SuCancer dataset differs from the
number in Jin & Wang (2016) because we apply IF-PCA to the dataset without data modification.
In summary, the Breast Cancer, SuCancer, and Prostate Cancer datasets are extremely challeng-
ing for the existing methods. One possible reason might be that the two clusters in these datasets
are not homogeneous. Aside from these extremely challenging datasets, our new method performs
well for the remaining datasets and makes significant advancements for the Colon Cancer dataset.
5. Conclusion and discussion
We proposed a new clustering framework that takes into account a useful pattern manifested by
the curse of dimensionality in the ğ‘˜-NN graph, improving upon existing methods for clustering
high-dimensional data, especially when there are two Gaussian clusters differ in scale. Two
important parameters in our method are ğ‘˜in theğ‘˜-NN graph and ğœ…inğ‘€ğœ…=max{ğ‘ğ‘¤,ğœ…ğ‘ğ‘‘}.
In this study, we focused on the traditional clustering task, which is an unsupervised learning
task, and suggested using ğœ…=1.55 and the value of ğ‘˜that results in the largest value of ğ‘€ğœ…. In
semi-supervised clustering, where we have partial labels of the data as well as a larger set of
unlabeled data, the labeled data can be used to train the parameters ğ‘˜andğœ….
In this study, we focused on two clusters, but many real-world datasets contain more than
two clusters. When the number of clusters is fixed and known, we can use top-down divisive
algorithms to extend this framework to multiple clusters. This involves recursively dividing a
larger cluster into two smaller clusters until we reach the desired number of clusters. At each
division step, a criterion such as the maximum value of ğ‘€ğœ…can be used to determine which
cluster to divide. Further research is needed to evaluate the performance of different criteria in
this setting. When the number of clusters is unknown, the task becomes more challenging. In
this case, a new penalized criterion that combines ğ‘€ğœ…and BIC may be considered, but extensive
investigation is required.
Acknowledgement
Hao Chen and Xiancheng Lin were supported in part by the NSF awards DMS-1848579 and
DMS-2311399.
References
Alon, U. ,Barkai, N. ,Notterman, D. A. ,Gish, K. ,Ybarra, S. ,Mack, D. &Levine, A. J. (1999). Broad patterns
of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide17
arrays. Proceedings of the National Academy of Sciences 96, 6745â€“6750.
Bajwa, M. S. ,Agarwal, A. P. &Manchanda, S. (2015). Ternary search algorithm: Improvement of binary search.
In2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom) . IEEE.
Bhattacharjee, A. ,Richards, W. G. ,Staunton, J. ,Li, C. ,Monti, S. ,Vasa, P. ,Ladd, C. ,Beheshti, J. ,Bueno, R. ,
Gillette, M. et al. (2001). Classification of human lung carcinomas by mrna expression profiling reveals distinct
adenocarcinoma subclasses. Proceedings of the National Academy of Sciences 98, 13790â€“13795.
Chan, P. K. ,Schlag, M. D. &Zien, J. Y. (1993). Spectral k-way ratio-cut partitioning and clustering. In Proceedings
of the 30th international Design Automation Conference .
Chen, H. &Friedman, J. H. (2017). A new graph-based two-sample test for multivariate and object data. Journal of
the American statistical association 112, 397â€“409.
Dettling, M. (2004). Bagboosting for tumor classification with gene expression data. Bioinformatics 20, 3583â€“3593.
Donoho, D. &Jin, J. (2008). Higher criticism thresholding: Optimal feature selection when useful features are rare
and weak. Proceedings of the National Academy of Sciences 105, 14790â€“14795.
Golub, T. R. ,Slonim, D. K. ,Tamayo, P. ,Huard, C. ,Gaasenbeek, M. ,Mesirov, J. P. ,Coller, H. ,Loh, M. L. ,
Downing, J. R. ,Caligiuri, M. A. et al. (1999). Molecular classification of cancer: class discovery and class
prediction by gene expression monitoring. science 286, 531â€“537.
Gordon, G. J. ,Jensen, R. V. ,Hsiao, L.-L. ,Gullans, S. R. ,Blumenstock, J. E. ,Ramaswamy, S. ,Richards, W. G. ,
Sugarbaker, D. J. &Bueno, R. (2002). Translation of microarray data into clinically relevant cancer diagnostic
tests using gene expression ratios in lung cancer and mesothelioma. Cancer research 62, 4963â€“4967.
Hastie, T. ,Tibshirani, R. ,Friedman, J. H. &Friedman, J. H. (2009). The elements of statistical learning: data
mining, inference, and prediction , vol. 2. Springer.
Henze, N. (1988). A multivariate two-sample test based on the number of nearest neighbor type coincidences. The
Annals of Statistics 16, 772â€“783.
Jin, J. &Wang, W. (2016). Influential features pca for high dimensional clustering. The Annals of Statistics 44,
2323â€“2359.
Leighton, F. T. &Rao, S. (1988). An approximate max-flow min-cut theorem for uniform multicommodity flow
problems with applications to approximation algorithms. In FOCS , vol. 88.
Liu, Y.-W. &Chen, H. (2022). A fast and efficient change-point detection framework based on approximate ğ‘˜-nearest
neighbor graphs. IEEE Transactions on Signal Processing 70, 1976â€“1986.
Maaten, L. v. d. &Hinton, G. (2008). Visualizing data using t-sne. Journal of Machine Learning Research 9,
2579â€“2605.
Mwangi, B. ,Soares, J. C. &Hasan, K. M. (2014). Visualization and unsupervised predictive clustering of high-
dimensional multimodal neuroimaging data. Journal of neuroscience methods 236, 19â€“25.
Rohe, K. ,Chatterjee, S. &Yu, B. (2011). Spectral clustering and the high-dimensional stochastic blockmodel .
Schilling, M. F. (1986). Multivariate two-sample tests based on nearest neighbors. Journal of the American Statistical
Association 81, 799â€“806.
Shi, J. &Malik, J. (2000). Normalized cuts and image segmentation. IEEE Transactions on pattern analysis and
machine intelligence 22, 888â€“905.
Singh, D. ,Febbo, P. G. ,Ross, K. ,Jackson, D. G. ,Manola, J. ,Ladd, C. ,Tamayo, P. ,Renshaw, A. A. ,Dâ€™Amico,
A. V. ,Richie, J. P. et al. (2002). Gene expression correlates of clinical prostate cancer behavior. Cancer cell 1,
203â€“209.
Singh, V. &Verma, N. K. (2022). Gene expression data analysis using feature weighted robust fuzzy-means clustering.
IEEE Transactions on NanoBioscience 22, 99â€“105.
Su, A. I. ,Welsh, J. B. ,Sapinoso, L. M. ,Kern, S. G. ,Dimitrov, P. ,Lapp, H. ,Schultz, P. G. ,Powell, S. M. ,
Moskaluk, C. A. ,Frierson Jr, H. F. et al. (2001). Molecular classification of human carcinomas by use of gene
expression signatures. Cancer research 61, 7388â€“7393.
Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and computing 17, 395â€“416.
Wang, Y. ,Klijn, J. G. ,Zhang, Y. ,Sieuwerts, A. M. ,Look, M. P. ,Yang, F. ,Talantov, D. ,Timmermans, M. ,
Meijer-van Gelder, M. E. ,Yu, J. et al. (2005). Gene-expression profiles to predict distant metastasis of lymph-
node-negative primary breast cancer. The Lancet 365, 671â€“679.
Wu, Z. &Leahy, R. (1993). An optimal graph theoretic approach to data clustering: Theory and its application to
image segmentation. IEEE transactions on pattern analysis and machine intelligence 15, 1101â€“1113.
Yousefi, M. R. ,Hua, J. ,Sima, C. &Dougherty, E. R. (2010). Reporting bias when using real data sets to analyze
classification performance. Bioinformatics 26, 68â€“76.
Zhang, Y. &Chen, H. (2021). Graph-based multiple change-point detection. arXiv preprint arXiv:2110.01170 .
Zhou, D. &Chen, H. (2022). Ring-cpd: Asymptotic distribution-free change-point detection for multivariate and
non-euclidean data. arXiv preprint arXiv:2206.03038 .
Zhu, Y. &Chen, H. (2021). Limiting distributions of graph-based test statistics. arXiv preprint arXiv:2108.07446 .