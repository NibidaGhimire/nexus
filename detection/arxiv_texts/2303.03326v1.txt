Keep It Simple: CNN Model Complexity Studies
for Interference ClassiÔ¨Åcation Tasks
Taiwo Oyedare?, Vijay K. Shahy, Daniel J. Jakubisin?z, Jeffrey H. Reed?
?Bradley Dept. of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA
yDept. of Cybersecurity Engineering, George Mason University, Fairfax, USA
zVirginia Tech National Security Institute, Blacksburg, V A, USA
Email: toyedare@vt.edu, vshah22@gmu.edu, djj@vt.edu, reedjh@vt.edu
Abstract ‚ÄîThe growing number of devices using the wireless
spectrum makes it important to Ô¨Ånd ways to minimize interfer-
ence and optimize the use of the spectrum. Deep learning models,
such as convolutional neural networks (CNNs), have been widely
utilized to identify, classify, or mitigate interference due to their
ability to learn from the data directly. However, there have been
limited research on the complexity of such deep learning models.
The major focus of deep learning-based wireless classiÔ¨Åcation
literature has been on improving classiÔ¨Åcation accuracy, often at
the expense of model complexity. This may not be practical for
many wireless devices, such as, internet of things (IoT) devices,
which usually have very limited computational resources and
cannot handle very complex models. Thus, it becomes important
to account for model complexity when designing deep learning
based models for interference classiÔ¨Åcation. To address this,
we conduct an analysis of CNN based wireless classiÔ¨Åcation
that explores the trade-off amongst dataset size, CNN model
complexity, and classiÔ¨Åcation accuracy under various levels of
classiÔ¨Åcation difÔ¨Åculty: namely, interference classiÔ¨Åcation, hetero-
geneous transmitter classiÔ¨Åcation, and homogeneous transmitter
classiÔ¨Åcation. Our study, based on three wireless datasets, shows
that a simpler CNN model with fewer parameters can perform
just as well as a more complex model, providing important
insights into the use of CNNs in computationally constrained
applications.
Index Terms ‚ÄîInterference ClassiÔ¨Åcation, Convolutional Neu-
ral Networks, Model Complexity.
I. I NTRODUCTION
The proliferation of internet of things (IoT), 5G devices,
and other wireless technologies has led to an increase in the
number of wireless devices that interfere with each other,
either intentionally or unintentionally. Classifying interference
is essential for ensuring good communication quality and
adhering to spectrum sharing policies. The concept of wireless
interference, where a signal of interest is disrupted by another
signal sharing the same channel, has been studied extensively
in the Ô¨Åeld of wireless communication [1]‚Äì[3]. This type of
interference can signiÔ¨Åcantly degrade the signal-to-noise-plus-
interference ratio (SINR) and disrupt communication between
a transmitter and receiver.
Traditionally, many interference classiÔ¨Åcation techniques
have relied on rule-based approaches that are not effective
when multiple coexisting technologies are in use. As a result,
alternative classiÔ¨Åcation algorithms that use feature detection
or extraction techniques, such as cyclostationary feature de-
tection [4], have been explored. However, these techniquesrequire domain expertise and can result in a complicated solu-
tion that is oftentimes not scalable. In recent years, researchers
have turned to deep learning techniques to reduce the need
for domain expertise [3], [5], [6]. Interference suppression
applications have also widely used deep learning [7].
Convolutional neural networks (CNNs), a model-free deep
learning approach, have been shown to be effective in various
domains such as image classiÔ¨Åcation and natural language
processing. CNNs have been used for various classiÔ¨Åcation
tasks, including protocol/interference classiÔ¨Åcation [3], [5],
[6], [8]‚Äì[10], transmitter classiÔ¨Åcation [11], and modulation
classiÔ¨Åcation [12]. In using CNNs or other deep learning
models for classiÔ¨Åcation tasks in wireless communication
applications, researchers need to ensure that they have access
to high quality datasets and efÔ¨Åcient models. While earlier
works [5], [6], [9] have focused on improving the classiÔ¨Åcation
performance of different deep learning models, the relationship
between the size of a dataset, the complexity of the CNN
models used, and the difÔ¨Åculty of classiÔ¨Åcation is often over-
looked or assumed. Even though many researchers would tune
hyper-parameters of their CNN during the training process,
an insight into other factors (such as Ô¨Ålter size, number of
nodes in the hidden layer, etc) that affect model complexity
has not been typically investigated. In resource-constrained
applications like the IoT, CNN models that are too complex
may not be feasible for classiÔ¨Åcation in real-world situations.
This is because IoT is marked by its limited processing power
and storage capabilities, which can lead to challenges in terms
of performance, security, privacy, and reliability [13], [14]. It is
essential to understand the processes that lead to the selection
of hyper-parameters in relation to model complexity, the size
of the dataset and the difÔ¨Åculty of classiÔ¨Åcation.
A typical CNN architecture consists of a series of feed-
forward layers that apply convolutional Ô¨Ålters and pooling
operations, followed by fully-connected layers that convert the
2D feature maps produced by the earlier layers into 1D vectors
for classiÔ¨Åcation [15]. While CNNs do not require a separate
feature extraction step before being used, they can be time-
consuming and difÔ¨Åcult to train from scratch because they
require a large labeled dataset for building and training the
model [16]. The complexity of deep learning models can be
inÔ¨Çuenced by various factors, such as the number of layers,
number of Ô¨Ålters, size of the Ô¨Ålters, and number of nodes in thearXiv:2303.03326v1  [eess.SP]  6 Mar 2023Homogeneous 
Classification
Specific Emitter ( e.g.
multiple USRP 2921 
devices)Heterogeneous 
Classification
Different transmitter 
category ( e.g.USRP 
2921, B200 Pluto SDR)Protocol or 
interference 
classification
Different interference 
sources ( e.g.CWI, 
MCWI, CI )Fig. 1: Overview of Interference ClassiÔ¨Åcation
hidden layer. Researchers in the Ô¨Åeld of deep learning often
aim to improve the performance of their models by hyper-
parameter tuning and other optimization techniques. In the
literature, there has been a focus on improving classiÔ¨Åcation
performance through these methods. Although it is important
to optimize deep learning models for performance, there has
been limited attention in the wireless communication literature
on thoroughly analyzing the factors that inÔ¨Çuence model
complexity. This paper aims to Ô¨Åll this gap by studying
the relationship between model complexity, dataset size, and
classiÔ¨Åcation difÔ¨Åculty in a thorough and empirical manner.
To the best of our knowledge, this is one of the Ô¨Årst studies
to examine this relationship.
Our contributions are as follows:
We thoroughly analyze the complexity of three different
CNN architectures (simple, medium, and complex) in
relation to dataset size and classiÔ¨Åcation difÔ¨Åculty.
We show, empirically, that the performance of a simple
CNN model with fewer parameters is comparable to
that of a more complex CNN model. This is important
because resource-constrained devices, which have limited
processing power and storage capabilities, can beneÔ¨Åt
from using simpler CNN models.
II. O VERVIEW OF CLASSIFICATION TASKS
In this paper, our interference classiÔ¨Åcation task is per-
formed at levels of difÔ¨Åculty as shown in Fig. 1. At the
interference or protocol level, a CNN can be used to classify
different protocols or interference sources. At the heteroge-
neous level, a CNN can be used to classify different transmitter
categories. Finally, at the homogeneous level, a CNN can be
used to classify speciÔ¨Åc emitters (homogeneous) categories,
such as transmitters from the same model or manufacturer. The
outermost layer is the easiest classiÔ¨Åcation while the innermost
layer is the most difÔ¨Åcult.
A. Transmitter Categorization
Most transmitters have features that are peculiar to each
of them. For instance, when run at high power, the power
ampliÔ¨Åers used in many wireless devices sometimes display
non-linearities [17] [18]. These non-linearities can be used togroup the transmitters into different categories. We brieÔ¨Çy dis-
cuss the features of the transmitters used for our classiÔ¨Åcation
tasks.
1) Category A (USRP 2921): These transmitters utilize
more reliable linear power ampliÔ¨Åers and Ô¨Åner Ô¨Ålters than
other transmitter categories.
2) Category B (USRP B200): In comparison to those in
category A, the components in the transmitters in this category
are less reliable. They were designed with low-cost experimen-
tation in mind. They employ a single chain of the AD9364 ,
which is frequently utilized to decrease hardware and software
complexity.
3) Category C (Adalm Pluto SDR): This category‚Äôs trans-
mitters are far less capable than the other two categories‚Äô.
Considering their small size and low cost, the Pluto SDR is
capable of a wide range of useful SDR applications.
B. Levels of ClassiÔ¨Åcation
In this section we discuss the three levels of classiÔ¨Åcation
experiments carried out for the model complexity study.
1) Protocol or Interference ClassiÔ¨Åcation: The protocol or
interference level of classiÔ¨Åcation is easier than the other two
discussed in this section. This is because, there are enough
distinguishing features at this level. For instance, in the radio
frequency interference dataset used in this paper, there is
a marked difference between the three types of jammers
described . For instance, the MCWI, which combines the SoI
and a two-tone CW is structurally different from the CI [19].
2) Heterogeneous ClassiÔ¨Åcation: One transmitter from
each of the three categories listed above in Section II-A is
used in this study. While two of the three transmitters (USRP
2921 and USRP B200) were produced by the same company
(National Instruments), all three are of distinct models. In level
of difÔ¨Åculty, this classiÔ¨Åcation is easier than homogeneous
classiÔ¨Åcation (discussed in Section II-B3) but harder than
the protocol or interference classiÔ¨Åcation discussed in Section
II-B1
3) Homogeneous ClassiÔ¨Åcation: In this classiÔ¨Åcation task,
we seek to distinguish transmitters within categories A, B,
or C. The transmitters are identical (same manufacturer and
model), making this the most challenging classiÔ¨Åcation prob-
lem of all three levels. This is because the classiÔ¨Åcation algo-
rithm must identify slight variations in transmitters which have
the same architecture and hardware components. The same
OFDM waveform is sent by all of the devices, signiÔ¨Åcantly
complicating categorization.
III. I MPLEMENTATION DETAILS OF THE CNN
A. CNN Parameters for Model Complexity
The CNN algorithm utilized was a modiÔ¨Åed Tensor-
Ô¨Çow CNN model that was used to categorize handwritten
digits from the MNIST dataset. We created three levels of
complexity for our CNN models by varying the number of
nodes in the hidden layer, the number of Ô¨Ålters, and the size
of the training dataset. These factors all contribute to the
number of parameters in the model, and as a general rule, theTABLE I: CNN Parameters for Model Complexity
Parameters Simple Medium Complex
Nodes in the hidden
layer0.26k 1.04k 8k
Total number of
parameters for the
whole network6.1k 48k 276k
Filter size (No. of Fil-
ters)3*3(16) 3*3(32)
Batch size 16
Number of layers 3
Strides 2
Number of classes4 (for homogeneous devices)
3 (for heterogeneous devices)
Learning rate 0.0002
Max pooling size 2 * 2
Dropout Probability 50%
Input matrix size 38*100
Activation function ReLU
Optimizer Adam optimizer with cross entropy loss
Loss Cross entropy loss
Training sizes 16,64,256,1024,4096,8192,16384
Test sizes 2k
complexity of a CNN algorithm increases with the number of
parameters. As a CNN architecture becomes more complex,
it is generally expected that the performance of the CNN
algorithm will improve. However, one potential downside is
that the algorithm may begin to overÔ¨Åt to the training data,
resulting in a higher training accuracy compared to the test
accuracy.
Our network contains three convolutional layers, max pool-
ing layers and a fully connected layer. A 33Ô¨Ålter is
applied to the input matrix by the convolutional layers. Con-
volution operations are carried out in the resulting sub-region
to produce a single value in the respective output feature map.
To integrate non-linearities into the model, the scale-invariant
rectiÔ¨Åed linear unit (ReLU) activation function is applied to
the feature map values.
The data collected by the convolutional layer is down-
sampled using the pooling layer. We utilized the maximum
pooling function of 22. This indicates that the most
important features of the signals are kept while others are
deleted [20], which facilitates transmitter classiÔ¨Åcation. Table
I lists speciÔ¨Åcs of the parameters for all three designs.
TABLE II: CNN Parameters for Interference ClassiÔ¨Åcation
Parameters RESNet
Batch size 64
Number of layers 18
Learning rate 0.0001
Maximum pooling dimension 22
Activation function ReLU
Training/test size 80%/20%
Dropout Probability 80%
B. CNN Architecture for Interference ClassiÔ¨Åcation
In this section, we describe the architecture of the CNN
model and the training parameters used for the interference
classiÔ¨Åcation task. The CNN model is a pre-trained ResNet18
model described in [21]. The architecture is summarized in
Table II. The model consists of many convolutional layers,two fully-connected layers, and one output layer, in that
order. Leaky ReLU (Leaky ReLU (x) = maxfx;xg, where
2(0;1)is a preset parameter). All convolutional layers and
fully linked layers are subjected to an activation function with
= 0:2. The resulting (output) layer then has the softmax
function applied to it. All convolutional layers are subject to
batch normalization [22], however the output layer and fully-
connected layers are excluded. Additionally, we use stride = 2
in the convolutional layers rather than 22pooling layers for
down-sampling. Such changes enhance performance and lower
the variance of the results across various training epochs. x is
normalized as x0=x=xmax, wherexmax is the largest input
value allowed in x. We utilize the Adam optimizer with the
suggested default values in [23]. The learning rate is 110 4,
and the batch size is 64.
IV. E XPERIMENTAL SETUP
In this section, we describe the datasets used for the
investigation of dataset size, model complexity and level of
classiÔ¨Åcation. It is worth noting that we only used publicly
available datasets for interference classiÔ¨Åcation, while we used
our own testbed to generate datasets for the model complexity
studies presented in this paper. This is because there are some
limitation with using public dataset, for instance, we were not
able to control the types of transmitters used to generate the
interference. Also, the channel used for transmission cannot
be changed since we are using the dataset as is.
A. Dataset Generated For Model Complexity
The dataset generation process used for the assessment of
model complexity is similar to the one used in [11]. The only
difference is that more training data was added when compared
to the work in [11]. We deÔ¨Åne the process of creating a base-
band waveform, transmitting it over a channel, and receiving
it as the transmitter-receiver chain. It is important to note that
the details of the transmitter-receiver chain can be found in
[11].
1) Baseband Waveform Generation: The hardware devices
transmit OFDM packets created using GNU Radio Companion
signal processing blocks. This is done by generating a stream
of bits (0s and 1s) from a random source. We used an OFDM
waveform that receives 10,000 data bits from a random source,
which are mapped to the OFDM waveform using a QPSK
modulation scheme with an FFT length of 512 and occupying
the center 200 subcarriers with a cyclic preÔ¨Åx of 128.
2) RF Transmission: During transmission, GNU Radio and
the transmitter hardware are connected through the USRP
hardware driver (UHD) hardware support package. All USRP
devices are managed and communicated with using a library
called UHD. This is done using a GNU Radio block that takes
as inputs the sampling rate, RF bandwidth, buffer size, center
frequency, physical address of the device, and attenuation.
The OFDM waveform is then up-converted to 2.45 GHz as
the transmission center frequency and transmitted through
the hardware‚Äôs antenna. The sets of transmitters include four
USRP 2921s, four USRP B200s, and four Adalm Pluto soft-
ware deÔ¨Åned radios (SDRs).TABLE III: Comparison of Training Accuracy and Testing Accuracy for all Model Complexity and Device Category
Homogeneous ClassiÔ¨Åcation (USRP 2921) Homogeneous ClassiÔ¨Åcation (USRP B200)
Simple CNN Medium CNN Complex CNN Simple CNN Medium CNN Complex CNN
Dataset
SizeTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
Acc
16 1.00 0.407 1.00 0.406 1.00 0.408 1.00 0.502 1.00 0.492 1.00 0.544
64 0.996 0.545 1.00 0.563 1.00 0.568 0.988 0.567 1.00 0.608 1.00 0.591
256 1.00 0.660 1.00 0.688 1.00 0.694 0.996 0.623 1.00 0.658 1.00 0.652
1024 0.971 0.744 1.00 0.784 1.00 0.788 0.955 0.663 1.00 0.709 1.00 0.722
4096 0.947 0.807 0.99 0.836 1.00 0.829 0.900 0.806 0.995 0.821 1.00 0.826
8192 0.899 0.838 0.986 0.856 0.99 0.842 0.864 0.830 0.984 0.837 0.99 0.858
16384 0.88 0.860 0.975 0.862 0.99 0.876 0.856 0.843 0.971 0.860 0.97 0.870
Homogeneous ClassiÔ¨Åcation (Adalm Pluto SDR) Heterogeneous ClassiÔ¨Åcation
Simple CNN Medium CNN Complex CNN Simple CNN Medium CNN Complex CNN
Dataset
SizeTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
AccTraining
AccTest
Acc
16 0.859 0.369 1.00 0.437 1.00 0.407 0.96 0.762 1.00 0.842 1.00 0.816
64 0.988 0.512 1.00 0.548 1.00 0.450 1.00 0.836 1.00 0.844 1.00 0.841
256 0.939 0.641 1.00 0.633 1.00 0.579 0.997 0.883 1.00 0.885 1.00 0.895
1024 0.862 0.700 0.993 0.689 1.00 0.673 0.99 0.913 1.00 0.920 1.00 0.919
4096 0.820 0.718 0.959 0.699 0.99 0.706 0.97 0.937 0.99 0.954 1.00 0.952
8192 0.775 0.736 0.901 0.738 0.96 0.718 0.98 0.955 0.986 0.958 1.00 0.968
16384 0.772 0.747 0.865 0.753 0.85 0.754 0.98 0.97 0.975 0.970 1.00 0.972
3) RF Reception: On the receiver side, the signal is re-
ceived when the antenna and the transmitter are on the
same channel. We assume that the receiver knows the center
frequency and bandwidth of the transmitter and corrects for
frequency offset at the receiver. The transmitted signal‚Äôs center
frequency and sample rate are stored in the UHD USRP
Source block, which is used by a computer running GNU
Radio Companion to down-convert the signal to baseband
frequency. After being delivered to a low noise ampliÔ¨Åer, the
signal is separated into in-phase and quadrature components
at baseband. It is then low-pass Ô¨Åltered and transferred to an
analog-to-digital converter (ADC). When the ADC process is
completed, the digital samples are clocked into an FPGA. Af-
ter being digitally down-converted using precision frequency
tuning, a series of Ô¨Ålters are used to decimate the FPGA image.
The raw samples are then transmitted to a host computer
through the host interface (using an Ethernet gigabit link, in
this case) following decimation, made possible by the UHD.
The complex samples are collected by the host computer using
GNU Radio, and the IQ data is saved to a Ô¨Åle and transferred
to MATLAB for further processing.
4) Continuous Wavelet Transform (CWT) Signal Prepro-
cessing: The features in the received signal that can be
employed in the classiÔ¨Åcation process are highlighted by
the CWT. For CWT, a MNmatrix of coefÔ¨Åcients is gener-
ated fromNsamples for a N-length signal, where Mrepre-
sents the number of scales [24]. The scales are determined
automatically using the wavelength‚Äôs energy distribution in
frequency and time [24].
The CWT MATLAB function is used to pre-process the
signals in order to enhance key transmitter properties in the
received signal. After the signal has been transformed, the
resultant 2D matrix of size MNfor each sample is stacked
together and sent to the CNN classiÔ¨Åer as a three dimensional
matrix. The dimension of the output of the CWT process for
this project is a 38100 matrix of coefÔ¨Åcient whereas the
input to the CWT is 2128.B. Datasets for Interference ClassiÔ¨Åcation
1) Radio Frequency Interference Dataset: In this work, we
use publicly available wireless interference datasets to evaluate
our approach. The RFI dataset used in this study was created
by the authors of [19]. They created the dataset by combin-
ing a signal of interest (SOI) with three types of jammers
(continuous-wave interference (CWI), multi-continuous-wave
interference (MCWI), and chirp interference (CI)) at different
signal-to-noise ratios (SNRs).
2) CRAWDAD Dataset: The second dataset used in this
paper was obtained from the Community Resource for Archiv-
ing Wireless Data at Dartmouth (CRAWDAD) website. This
dataset, provided by Schmidt et al. [3], includes traces of IEEE
802.11b/g, IEEE 802.15.4, and Bluetooth packet transmissions
with different SNRs in the baseband [25], as well as frequency
offsets in the baseband [9]. There are a total of 15 distinct
classes, 10 of which are IEEE 802.15.1 devices, 3 of which are
IEEE 802.11 devices, and the remaining 2 are IEEE 802.15.4
devices.
V. R ESULTS
In this section we discuss the results from the model
complexity and interference classiÔ¨Åcation studies. Model com-
plexity studies are important since they help to understand
the relationship between classiÔ¨Åcation performance and the
number of parameters used by a deep learning algorithm (CNN
used in this paper). It is important to note that generating
our own dataset to study model complexity helps us to vary
different conditions in the data generating process which can
not be done with publicly available datasets.
A. Model Complexity
This section examines the performance of the various CNN
models across different device categories. When the test
accuracy is signiÔ¨Åcantly lower than the training accuracy, it
suggests that the algorithm performs well on the training set
but poorly on the test set, indicating overÔ¨Åtting. In order for aTrue Label
Predicted Label(a) Confusion matrix for the RFI Dataset.( ClassiÔ¨Åcation Accuracy:
97:8%)
True Label
Predicted Label(b) Confusion Matrix for the CRAWDAD Dataset ( ClassiÔ¨Åcation Accu-
racy: 80% )
Fig. 2: Confusion Matrix for CNN classiÔ¨Åcation of the RFI and CRAWDAD Datasets.
deep learning algorithm to perform well on new, unseen data,
it is important for the training and test accuracies to be similar.
This indicates that the algorithm has learned to generalize well.
Table III compares the training accuracy to the test accuracy
for all the classiÔ¨Åcation tasks. At smaller dataset sizes, all the
models tend to overÔ¨Åt signiÔ¨Åcantly. However, as the dataset
size increases, the algorithms tend to overcome overÔ¨Åtting
issues. For most of the classiÔ¨Åcation categories, the overÔ¨Åtting
problem is signiÔ¨Åcantly reduced, with the difference between
the training and test accuracy being within 5%. OverÔ¨Åtting
often occurs when the CNN is complex, with multiple layers
and many nodes in the hidden layer. To prevent overÔ¨Åtting,
regularization techniques such as dropout [26] and early stop-
ping [27] can be used.
Figure 3 demonstrates that, for most transmitters, the simple
CNN performs similarly to the medium and complex CNNs,
despite having signiÔ¨Åcantly fewer parameters. In fact, Fig-
ure 3d shows that the performance of the simple CNN is
comparable to that of the complex CNNs for heterogeneous
transmitters after a dataset size of 100. While more complex
or sophisticated CNN models may offer some beneÔ¨Åts, there
must be a balance between the network‚Äôs generalization ability
and its complexity. These Ô¨Åndings suggest that, it is important
for the deep learning model to be both simple and robust, this
is especially true for resource-constrained applications .
B. Interference ClassiÔ¨Åcation
Accurate classiÔ¨Åcation of interference sources is crucial for
interference suppression or mitigation. Using a pre-trained
ResNet18 model, we achieved a 97:8%accuracy on the RFI
dataset, as shown in Table II. Figure 2a shows the confusion
matrix for the interference classes, indicating excellent perfor-
mance on this comparatively easy classiÔ¨Åcation task. For theCRAWDAD dataset, the classiÔ¨Åcation performance was about
80%, as shown in the confusion matrix in Figure 2b, after 25
epochs.
As discussed in Section V-A, while more complex CNN
models tend to perform better in classiÔ¨Åcation tasks, they
are typically prone to overÔ¨Åtting. OverÔ¨Åtting occurs when a
deep learning model memorizes the training dataset, leading
to high training accuracy but low test or validation accuracy.
This can be seen in the results for the Pluto SDR in both
Table III and Figure 3c. These results further emphasize the
importance of our Ô¨Åndings from the model complexity study
discussed in Section V-A. As previously mentioned, a less
complex model can often perform as well as a more complex
one without the risk of overÔ¨Åtting. Therefore, it is important
to use models that are neither too complex nor too simple.
The right level of complexity can be determined by using
just enough 2D convolution layers and Ô¨Ålters to achieve good
performance, starting with a simpler model and gradually
increasing complexity as needed, this helps to prevent both
under-Ô¨Åtting and overÔ¨Åtting issues.
VI. C ONCLUSION
In conclusion, this paper has thoroughly examined the use
of CNN for interference classiÔ¨Åcation. Our results demonstrate
that the CNN model is capable of accurately classifying
different interference sources, as shown in the two datasets we
used. Our study found that, while medium and complex CNN
classiÔ¨Åers performed slightly better than the simple classiÔ¨Åer,
the difference in performance was not signiÔ¨Åcant. This is an
important Ô¨Ånding since resource-constrained devices can easily
leverage the simpler models. When designing deep learning
models, the goal is to create models that can be applied to
new data. Simple models are more likely to be able to do1021031040.30.40.50.60.70.80.91
Dataset SizeTest AccuracySimple
Medium
Complex(a) USRP 2921
1021031040.30.40.50.60.70.80.91
Dataset SizeTest AccuracySimple
Medium
Complex (b) USRP B200
1021031040.30.40.50.60.70.80.91
Dataset SizeTest AccuracySimple
Medium
Complex (c) Pluto SDR
1021031040.30.40.50.60.70.80.91
Dataset SizeTest AccuracySimple
Medium
Complex (d) Heterogeneous
Fig. 3: Comparison of Test Accuracy for Homogeneous and Heterogeneous Transmitter Classes
this because they are less prone to overÔ¨Åtting, which is a
common issue with complex models. Different classiÔ¨Åcation
tasks, such as homogeneous and heterogeneous classiÔ¨Åcation,
may require different amounts of data and different levels
of model complexity. In our study, the training dataset size,
number of nodes in the hidden layer has had the greatest
impact on CNN model performance. While the size of the
dataset is important, we also note that the quality of the
training dataset can also signiÔ¨Åcantly impact the performance
of CNN classiÔ¨Åers.
ACKNOWLEDGMENT
This work was supported in part by the U.S. Air Force
Research Laboratory (AFRL) under Grant FA8750-20-2-0504,
in part by the Lockheed Martin Corporation under Grant M16-
005-RPP010, and in part by the National Science Foundation
under Grant CNS-1564148.
REFERENCES
[1] L. Qiu, Y . Zhang, F. Wang, M. K. Han, and R. Mahajan, ‚ÄúA general
model of wireless interference,‚Äù in Proceedings of the 13th annual ACM
international conference on Mobile computing and networking , pp. 171‚Äì
182, 2007.
[2] J. Bruno, ‚ÄúInterference reduction in wireless networks,‚Äù Computing
Research Topics, Computing Sciences Department, Villanova University ,
2007.
[3] M. Schmidt, D. Block, and U. Meier, ‚ÄúWireless interference iden-
tiÔ¨Åcation with convolutional neural networks,‚Äù in 2017 IEEE 15th
International Conference on Industrial Informatics (INDIN) , pp. 180‚Äì
185, IEEE, 2017.
[4] K. Kim, I. A. Akbar, K. K. Bae, J.-S. Um, C. M. Spooner, and J. H.
Reed, ‚ÄúCyclostationary approaches to signal detection and classiÔ¨Åcation
in cognitive radio,‚Äù in 2007 2nd ieee international symposium on new
frontiers in dynamic spectrum access networks , pp. 212‚Äì215, IEEE,
2007.
[5] J. Yu, M. Alhassoun, and R. M. Buehrer, ‚ÄúInterference classiÔ¨Åcation
using deep neural networks,‚Äù arXiv preprint arXiv:2002.00533 , 2020.
[6] J. Kim, S. Lee, Y .-H. Kim, and S.-C. Kim, ‚ÄúClassiÔ¨Åcation of interference
signal for automotive radar systems with convolutional neural network,‚Äù
IEEE Access , vol. 8, pp. 176717‚Äì176727, 2020.
[7] T. Oyedare, V . K. Shah, D. J. Jakubisin, and J. H. Reed, ‚ÄúInterference
suppression using deep learning: Current approaches and open chal-
lenges,‚Äù IEEE Access , 2022.
[8] Y . Zhao, W.-C. Wong, H. K. Garg, T. Feng, Z. Zhang, and L. Tang,
‚ÄúIndoor position recognition and interference classiÔ¨Åcation with a nested
lstm network,‚Äù in 2019 IEEE Global Communications Conference
(GLOBECOM) , pp. 1‚Äì6, IEEE, 2019.
[9] S. Grunau, D. Block, and U. Meier, ‚ÄúMulti-label wireless interfer-
ence identiÔ¨Åcation with convolutional neural networks,‚Äù arXiv preprint
arXiv:1804.04395 , 2018.[10] K. Sankhe, M. Belgiovine, F. Zhou, S. Riyaz, S. Ioannidis, and
K. Chowdhury, ‚ÄúOracle: Optimized radio classiÔ¨Åcation through convo-
lutional neural networks,‚Äù in IEEE INFOCOM 2019-IEEE Conference
on Computer Communications , pp. 370‚Äì378, IEEE, 2019.
[11] T. Oyedare and J.-M. J. Park, ‚ÄúEstimating the required training dataset
size for transmitter classiÔ¨Åcation using deep learning,‚Äù in 2019 IEEE
International Symposium on Dynamic Spectrum Access Networks (DyS-
PAN) , pp. 1‚Äì10, IEEE, 2019.
[12] T. J. O‚ÄôShea, J. Corgan, and T. C. Clancy, ‚ÄúConvolutional radio modula-
tion recognition networks,‚Äù in International conference on engineering
applications of neural networks , pp. 213‚Äì226, Springer, 2016.
[13] H. F. Atlam, R. J. Walters, and G. B. Wills, ‚ÄúFog computing and the
internet of things: A review,‚Äù big data and cognitive computing , vol. 2,
no. 2, p. 10, 2018.
[14] F. Pereira, R. Correia, P. Pinho, S. I. Lopes, and N. B. Carvalho, ‚ÄúChal-
lenges in resource-constrained iot devices: Energy and communication
as critical success factors for future iot deployment,‚Äù Sensors , vol. 20,
no. 22, p. 6420, 2020.
[15] D. Rav `ƒ±, C. Wong, F. Deligianni, M. Berthelot, J. Andreu-Perez, B. Lo,
and G.-Z. Yang, ‚ÄúDeep learning for health informatics,‚Äù IEEE journal
of biomedical and health informatics , vol. 21, no. 1, pp. 4‚Äì21, 2016.
[16] H. Mohsen, E.-S. A. El-Dahshan, E.-S. M. El-Horbaty, and A.-B. M.
Salem, ‚ÄúClassiÔ¨Åcation using deep learning neural networks for brain
tumors,‚Äù Future Computing and Informatics Journal , vol. 3, no. 1,
pp. 68‚Äì71, 2018.
[17] F. Edalat, Effect of power ampliÔ¨Åer nonlinearity on system performance
metric, bit-error-rate (BER) . PhD thesis, Massachusetts Institute of
Technology, 2003.
[18] A. C. Polak, S. Dolatshahi, and D. L. Goeckel, ‚ÄúIdentifying wireless
users via transmitter imperfections,‚Äù IEEE Journal on selected areas in
communications , vol. 29, no. 7, pp. 1469‚Äì1479, 2011.
[19] S. Ujan, N. Navidi, and R. Jr Landry, ‚ÄúAn efÔ¨Åcient radio frequency inter-
ference (rÔ¨Å) recognition and characterization using end-to-end transfer
learning,‚Äù Applied Sciences , vol. 10, no. 19, p. 6885, 2020.
[20] W. Wang, Y . Ning, H. Rangwala, and N. Ramakrishnan, ‚ÄúA multiple
instance learning framework for identifying key sentences and detecting
events,‚Äù in Proceedings of the 25th ACM International on Conference
on Information and Knowledge Management , pp. 509‚Äì518, 2016.
[21] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE conference on computer vision
and pattern recognition , pp. 770‚Äì778, 2016.
[22] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep
network training by reducing internal covariate shift,‚Äù arXiv preprint
arXiv:1502.03167 , 2015.
[23] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980 , 2014.
[24] ‚ÄúCWT-based time-frequency analysis.‚Äù https://www.mathworks.com/
help/wavelet/examples/cwt-based-time-frequency-analysis.html.
[25] M. Schmidt, D. Block, and U. Meier, ‚ÄúCRAWDAD dataset
OWL/interference (v. 2019-02-12).‚Äù Downloaded from https://crawdad.
org/owl/interference/20190212, Feb. 2019.
[26] L. Xie, J. Wang, Z. Wei, M. Wang, and Q. Tian, ‚ÄúDisturblabel:
Regularizing CNN on the loss layer,‚Äù in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , pp. 4753‚Äì
4762, 2016.
[27] X. Ying, ‚ÄúAn overview of overÔ¨Åtting and its solutions,‚Äù in Journal of
physics: Conference series , vol. 1168, p. 022022, IOP Publishing, 2019.