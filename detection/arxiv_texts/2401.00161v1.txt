DiffHybrid-UQ: Uncertainty Quantification for Differentiable
Hybrid Neural Modeling
Deepak Akharea, Tengfei Luoa,b,c, Jian-Xun Wanga,b,d,∗
aDepartment of Aerospace and Mechanical Engineering, University of Notre Dame, Notre Dame, IN, USA
bCenter for Sustainable Energy (ND Energy), University of Notre Dame, Notre Dame, IN, USA
cDepartment of Chemical and Biomolecular Engineering, University of Notre Dame, Notre Dame, IN, USA
dLucy Family Institute for Data & Society, University of Notre Dame, Notre Dame, IN, USA
Abstract
The hybrid neural differentiable models mark a significant advancement in the field of scien-
tific machine learning. These models, integrating numerical representations of known physics
into deep neural networks, offer enhanced predictive capabilities and show great potential for
data-driven modeling of complex physical systems. However, a critical and yet unaddressed
challenge lies in the quantification of inherent uncertainties stemming from multiple sources.
Addressing this gap, we introduce a novel method, DiffHybrid-UQ, for effective and efficient
uncertainty propagation and estimation in hybrid neural differentiable models, leveraging the
strengths of deep ensemble Bayesian learning and nonlinear transformations. Specifically,
our approach effectively discerns and quantifies both aleatoric uncertainties, arising from
data noise, and epistemic uncertainties, resulting from model-form discrepancies and data
sparsity. This is achieved within a Bayesian model averaging framework, where aleatoric
uncertainties are modeled through hybrid neural models. The unscented transformation
plays a pivotal role in enabling the flow of these uncertainties through the nonlinear func-
tions within the hybrid model. In contrast, epistemic uncertainties are estimated using an
ensemble of stochastic gradient descent (SGD) trajectories. This approach offers a practical
approximation to the posterior distribution of both the network parameters and the physical
parameters. Notably, the DiffHybrid-UQ framework is designed for simplicity in implemen-
∗Corresponding author. Tel: +1 540 3156512
Email address: jwang33@nd.edu (Jian-Xun Wang)
Preprint submitted to Elsevier January 2, 2024arXiv:2401.00161v1  [cs.LG]  30 Dec 2023tation and high scalability, making it suitable for parallel computing environments. The
merits of the proposed method have been demonstrated through problems governed by both
ordinary and partial differentiable equations.
Keywords: Differentiable Programming, Scientific Machine Learning, Physics-integrated
Neural Networks, Uncertainty Quantification, Scalable Bayesian Learning
1. Introduction
The evolving dynamics of computational science are driven by the advent of advanced
numerical algorithms, enhanced computational infrastructures, and the proliferation of ex-
tensive datasets. This evolution has catalyzed the development of innovative methodologies
to address multifaceted challenges associated with scientific modeling and predictive analyt-
ics. Within this transformative framework, Scientific Machine Learning (SciML) emerges as
a salient discipline, intricately blending the principles of traditional scientific modeling with
the state-of-the-art machine learning techniques, especially in the context of handling volu-
minous data and advanced GPU computing. At its core, SciML represents a paradigm that
seamlessly integrates the rigorous foundations of scientific principles with the adaptability
and efficiency of machine learning methods. Such an integration not only amplifies the com-
putational efficiency but often enhances the accuracy of predictions for complex scientific
problems. Illustrative of the advancements within the SciML domain are methodologies like
Physics-Informed Neural Networks (PINN) [1–3], neural operators [4–6], equation discovery
techniques [7–9], and hybrid neural models [10–13].
In the expanding landscape of SciML, hybrid neural modeling emerges as a distinctive
frontier that marries domain-specific knowledge with cutting-edge data-driven methodolo-
gies, striking a balance between deep-rooted scientific principles and the adaptability of
modern machine learning (ML) techniques. While traditional scientific models, rooted in
established physical laws and equations, are invaluable tools for understanding various phys-
ical phenomena, they often struggle when faced with complex systems whose underlying
physics has not been fully understood or when managing extensive datasets. In contrast,
deep neural networks (DNNs) are adept at deciphering intricate patterns from large-scale
2data, but their black-box nature can pose interpretability challenges and sometimes limits
their generalization in out-of-sample scenarios. Combining the strengths of traditional scien-
tific models with ML, hybrid techniques endeavor to address these challenges, leveraging the
advantages of both worlds. Past effort in hybrid learning model tend to weakly integrate ML
with physics-based numerical models [14, 15]. Typically, an ML model is trained offline, and
later incorporated into a standard numerical solver. Such strategies gained traction, espe-
cially in data-driven closure modeling for turbulence [16–18] or atmospheric flows [19, 20]. A
significant challenge, however, arises from the necessity for labeled data during intermediate
phases — a requirement that is frequently unmet. Furthermore, when ML models and nu-
merical solvers operate somewhat independently, the resulting hybrid system can sometimes
exhibit instability, leading to unreliable and often inaccurate a posteriori predictions [21].
Recognizing these challenges, the prevailing consensus highlights the advantages of a more
integrated approach – creating a unified hybrid model that promises not just to improve com-
putational efficiency, but also yield more robust and accurate solutions. The paradigm of
differentiable programming (DP) offers a pathway to this goal, enabling joint optimization
of both ML and numerical components within a unified training environment. Recently,
there has been a growing interest in developing differentiable solvers and hybrid neural
models, which have demonstrated significant potential across various scientific fields [10–
13, 22–25]. For example, Kochkov et al. [10] integrated convolutional neural networks into
a differentiable computational fluid dynamic (CFD) solver, facilitating more effective coarse
graining. Similarly, Huang et al. [24] incorporated DNNs into a differentiable finite element
solver, enabling the derivation of constitutive relations for nonlinear materials from indirect
measurements. Wang and co-workers have pioneered differentiable hybrid neural models to
efficiently emulate spatiotemporal physics of fluid [11], fluid-structure interaction [12], and
composite materials manufacturing processes [13]. Their designs are deeply rooted in the
profound relationship between neural network architectural components (e.g., convolution
layers, residual connections) and the numerical representations of PDEs - a subject cur-
rently gaining traction in the ML community [26–30]. Building on these connections, they
3pointed out that traditional numerical solvers can be interpreted as unique neural network
instances, where the convolutional kernels, recurrent structures, and residual connections
are pre-determined entirely by the known physics and associated numerical schemes, rather
than being learned from data [11]. Therefore, the differentiable hybrid neural models can
be viewed as physics-integrated neural networks, where the known physics, represented by
discretized PDE operators, are woven into the neural architecture, guided by the connection
between numerical PDEs and neural architecture components [12].
Despite the promise of hybrid neural modeling in advancing computational science and
engineering, addressing the intrinsic uncertainties in these models remains a significant con-
cern. These models, while effectively integrating domain-specific knowledge with machine
learning methodologies, are not immune to uncertainties from multiple sources. For exam-
ple, aleatoric uncertainty can emerge from noisy training datasets, while the vast parameter
landscape of neural networks and potential model-form discrepancy of physics-based model
can induce epistemic uncertainty. The growing prominence of these issues has brought un-
certainty quantification (UQ) in SciML into the spotlight [31]. As we know, UQ for DNNs is
a long-standing challenge within the ML community, largely due to the intricate, nonlinear
nature of DL models. The extremely high-dimensional parameter space of DL models make
traditional Bayesian sampling techniques infeasible [32]. This limitation has directed re-
searchers toward approximate Bayesian learning strategies such as variational inference [33],
stochastic gradient MCMC [34, 35], and Bayes by back-propagation [36]. In parallel, en-
semble learning methods such as deep ensemble [37], snapshot ensemble [38], SWAG [39],
and SeBayS [40] have emerged as effective tools for estimating uncertainty in DNNs. The
posterior landscape, however, becomes even more intricate for hybrid neural models that
blend physics-based numerical solvers with DNNs. The fusion of mathematical representa-
tions of physical phenomena with the inherent complexities of neural networks introduces
multifaceted sources of uncertainty. How to quantify and propagate uncertainty for these
differentiable hybrid neural models remains uncharted territory, indicating a significant gap
in the current literature.
4In this study, we present DiffHybrid-UQ, a novel probabilistic differentiable hybrid neural
modeling framework designed for predicting spatiotemporal physics. This framework stands
out due to its capability of quantifying associated uncertainty propagation in a scalable
manner. At the core of our proposed method is the integration of the deep-ensemble based
stochastic weight averaging Gaussian (SWAG) method with the unscented nonlinear trans-
formation, devised to effectively estimate and propagate multi-source uncertainties inherent
to hybrid neural models, offering both robust predictions and comprehensive uncertainty
assessments. Specifically, our method discerns both aleatoric and epistemic uncertainties
stemming from diverse sources, such as measurement noise and model form discrepancies,
within a Bayesian model averaging framework. Aleatoric uncertainties, grounded on the
Gaussian assumption, are modeled using DNNs, where the associated hyperparameters are
learned from data. The unscented transformation is adopted to facilitate the flow of uncer-
tainty through the nonlinear functions within the hybrid framework. On the other hand,
epistemic uncertainties are estimated using an ensemble of stochastic gradient decent (SGD)
trajectories, providing an approximation to the posterior distribution of the network and
physical parameters. A salient advantage of the proposed DiffHybrid-UQ is its straightfor-
ward implementation and amenability to parallelism. Moreover, the design sidesteps exten-
sive hyperparameter tuning, minimizing chances of biased outcomes. This endeavor aims
to fill the research gap concerning uncertainty propagation and quantification in physics-
integrated differentiable hybrid neural models, bolstering the reliability and robustness of
their predictions. The rest of the paper is organized as follows: Section 2 detailed the pro-
posed methodology, including problem formulation from a Bayesian perspective and model-
ing of both aleatoric and epistemic uncertainties. The merits of the proposed model have
been demonstrated in a variety of numerical experiments in Section 3 Finally, Section 4
concludes the paper.
52. Methodology
2.1. Problem formulation
In various scientific and engineering disciplines, Partial Differential Equations (PDEs) are
ubiquitously employed as a mathematical formalism to describe the behavior of continua,
including but not limited to fluid dynamics, solid mechanics, heat and mass transfer, elec-
tromagnetic fields, and quantum mechanical systems. However, for many complex systems,
the analytical form of the governing PDEs is often partially known, which can be generally
represented as,
∂v
∂t=F
K(v(¯x);λK,λU),U(v(¯x);λK,λU)
,¯x∈Ωp,t, (1a)
BC(v(¯x);λK,λU) = 0 ,¯x∈∂Ωp,t, (1b)
IC(v(¯x);λK,λU) = 0 ,¯x∈Ωp,t=0, (1c)
where ¯x={x, t}represents the spatiotemporal coordinates, with xspecifically denoting
spatial coordinates and trepresenting time. The spatiotemporal domain Ω p,tis defined as
the Cartesian product Ω p×[0, Tr], where Ω prepresents the spatial extent of the physical
domain and Trdenotes the time range under consideration; ∂Ωp,t≜∂Ωp×[0, Tr] with ∂Ωp
representing the boundary of the physical domain. The nonlinear functions K(·) and U(·)
characterize the known and unknown segments of the PDEs, respectively, which are coupled
by a nonlinear functional F(·). These functions are intrinsically reliant on certain physical
parameters, which may sometimes remain elusive or uncertain. In this formulation, λKand
λUdenote the known and unknown physical parameters, respectively. Furthermore, the
initial and boundary conditions can be abstractly represented by the differential operators
ICandBC, respectively, which may also be contingent upon specific parameters.
Direct physics-based modeling faces substantial challenges due to the incompleteness of
the governing equation. Addressing this intricate issue, differential hybrid (DiffHybrid) mod-
els such as PiNDiff model [13] have emerged to integrate neural networks with the known
part of the PDEs using differentiable programming (DP) to learn the physics from data.
Within this paradigm, unknown functions/operators such as F(·) and U(·), are modeled
6as DNNs – FnnandUnn– with trainable parameters θnn. Simultaneously, the elusive
physical parameters λUare rendered trainable. A salient feature of the DiffHybrid models
lies in their construction using DP, allowing the joint optimization of both DNNs and un-
known physical parameters λUwithin a unified training environment through gradient back-
propagation. The training of DiffHybrid model can be formulated as a PDE-constrained
optimization problem, where the parameters to be optimized are collectively represented as
θ= [θnn,λU]T∈Rd. Given data D={¯xi,vi}N
i=1, these trainable parameters are optimized
by minimizing the objective function, J(vθ(¯x),D), defined, for example, by MSE or negative
log-likelihood. The optimization procedure is expressed as follows,
min
θJ(vθ(¯x),D;θ)
s.t.∂vθ(¯x)
∂t=Fnn
K(vθ(¯x);λK,λU),Unn(vθ(¯x);λK,λU,θnn);θnn
,¯x∈Ωp,t
BC(vθ(¯x);λK,λU) = 0 ,¯x∈∂Ωp,t
IC(vθ(¯x);λK,λU) = 0 ,¯x∈Ωp,t=0(2)
As a result of the end-to-end training process, the DiffHybrid model ensures that the so-
lution vector vθ(¯x) = DiffHybrid( ¯x;θ) consistently satisfies the governing PDEs for the
given set of parameters θ. While DiffHybrid models, as described above, exhibit significant
advantages over conventional black-box DNNs in predictive performance [13], they are not
without vulnerabilities, especially when applied to out-of-training scenarios. This limitation
raises questions about the reliability and robustness of the model’s predictions, making the
estimation and propagation of uncertainty through hybrid neural solvers a crucial yet chal-
lenging endeavor. To this end, we propose the DiffHybrid-UQ framework, which employs an
ensemble-based Stochastic Weight Averaging Gaussian (En-SWAG) method to estimate the
epistemic uncertainty and utilizes the unscented transformation technique for the propaga-
tion of Gaussian-based aleatoric uncertainty through the nonlinear functions. This approach
aims to substantially improve the reliability and robustness of DiffHybrid models by offering
not just accurate but also uncertainty-quantified predictions.
72.2. Overview of DiffHybrid-UQ framework
To effectively quantify the inherent uncertainties associated with the predicted state vari-
ablevθ(˜x), it is modeled as a random variable, denoted by V(˜x) conditioned on modeling
parameters θ, where ˜xrepresents the inputs of the model including spatiotemporal coordi-
nates ¯xand other nontrainable input parameters in the governing PDEs and IC/BCs. In this
framework, the hybrid neural model is designed to yield the probability distribution p(V|˜x,θ)
for random variable Vcorresponding to a specified input ˜xand associated model parame-
tersθ. Given a dataset D, the model can be trained following Bayesian principles, which
is to determine the posterior distribution of the modeling parameters as p(θ|D). Namely,
the output of the trained DiffHybrid-UQ model is the probability distribution p(V|˜x,D) of
the predicted states V, conditional on the training dataset D, which can be derived using
Bayesian Model Averaging (BMA),
p(V|˜x,D) =Z
p(V|˜x,θ)p(θ|D)dθ, (3)
where posterior distribution p(θ|D) of trainable parameters is computed using Bayes’ theo-
rem,
p(θ|D) =p(D|θ)p(θ)R
p(D|θ)p(θ)dθ, (4)
where p(θ) is our prior beliefs about θandp(D|θ) is the joint likelihood of the datasest given
the model prediction Vwith specific ˜xandθ. As the measurement noise of the dataset
D={˜xi,vD,i}N
i=1is usually independent and identically distributed, the joint likelihood can
be expressed as p(D|θ) =QN
i=1p(vD,i|˜xi,θ).
The integral in Eq. 3 can be estimated via Monte Carlo method in practice,
p(V|˜x,D)≈1
MMX
j=1p(V|˜x,θ(j)),θ(j)∼p(θ|D), (5)
where {θ(j)}M
j=1areMMonte Carlo samples obtained from the posterior distribution p(θ|D).
Through this approximation, the probabilistic prediction of ( V|˜x,D) essentially becomes a
mixture of density functions conditioned on θsamples. Typically, one of the primary metrics
8to express uncertainty is the variance (often used to define the credible interval), which can
be decomposed as follows using the law of total variance:
Var(V|˜x,D) =Eθ|D[Var(V|˜x,θ)]| {z }
Aleatoric+Varθ|D(E[V|˜x,θ])| {z }
Epistemic, (6)
where the aleatoric uncertainty captures inherent system variabilities like measurement noise,
while the epistemic uncertainty reflects the inadequacy of our model, stemming from in-
complete physical understanding and the intricacies of neural network predictions. Up to
now, two primary challenges still remain unresolved: (1) effectively modeling the likelihood
function to capture the aleatoric uncertainty; and (2) addressing the practical difficulties
in sampling the posterior p(θ|D), given the extremely high dimensionality of the parame-
ter space inherent to the hybrid neural model. Subsequent subsections will delve into our
solutions for these challenges within the proposed DiffHybrid-UQ framework.
2.3. Modeling likelihood via DiffHybrid model for aleatoric UQ
To account for aleatoric uncertainty, the measurement noise ϵof the state variables is
assumed to be an additive Gaussian noise with zero mean,
V|˜x,θ=¯vθ(˜x) +ϵθ. (7)
Namely, the conditional probability of the state ( V|˜x,θ) is Gaussian,
p(V|˜x,θ) =N
¯vθ(˜x), diag (ΣV,θ)(˜x)
, (8)
where the mean ¯vθ(˜x) and diagonal covariance matrix diag(ΣV,θ)(˜x) are approximated by
the differentiable hybrid neural model with trainable parameters θ. Alternatively put, the
output of the DiffHybrid solver is the probability distribution of the predicted state variable,
characterized by it’s first and second moments,
¯vθ(˜x), diag (ΣV,θ)(˜x) = DiffHybrid( ˜x;θ). (9)
Given the dataset D={˜xi,vD,i}N
i=1, the joint likelihood is computed as,
p(D|θ) =NY
i=11p
(2π)ddet (Σ V,θ)exp
−1
2 
vD,i−¯vθ(˜xi)TΣ−1
V,θ(˜xi) 
vD,i−¯vθ(˜xi)
,(10)
9where dis the dimension of the spatiotemporal state. When disregarding the uncertainty as-
sociated with θ, the training of the DiffHybrid model can be formulated as the minimization
of the negative log-likelihood,
θ∗= min
θJ 
D,(V|˜x,θ)
= min
θ−logp(D|θ). (11)
This process allows the model to quantify the uncertainty arising from the noise present
in the dataset D. However, a crucial point to bear in mind is that this training merely
yields a single realization, i.e., a point estimate θ∗, out of the full posterior distribution
p(θ|D). Consequently, this model is unable to capture epistemic uncertainty that originates
from DiffHybrid model parameterization. To truly gauge this, it becomes imperative to also
approximate the parameter posterior and assess the total uncertainty as described in Eq. 5.
Using Monte Carlo based BMA, the probability distribution of the prediction p(V|˜x,D)
given data becomes a Gaussian mixture, with its mean and variance (i.e., diagonal part of
its covariance matrix) as,
E(V|˜x,D)≈1
MMX
j=1¯vθ(˜x;θ(j)), (12a)
Var(V|˜x,D)≈1
MMX
j=1diag 
ΣV,θ(˜x;θ(j))
| {z }
Aleatoric+1
MMX
j=1 ¯vθ(˜x;θ(j))−E(V|˜x,D)2
| {z }
Epistemic(12b)
Section 2.5 will delve into the practical methods for approximating the posterior p(θ|D).
2.4. Aleatoric uncertainty propagation within DiffHybrid framework
In this subsection, we elaborate on the design of the DiffHybrid framework for likelihood
modeling. As indicated in Eq. 9, the output of the DiffHybrid model represents the prob-
ability distribution of the predicted state variable, which is primarily characterized by its
mean and variance. This necessitates the formulation of a model adept at processing and
predicting these statistical attributes (i.e., first- and second-order moments). As discussed
in Section 2.1, the DiffHybrid framework is a implicit neural network defined by the partially
10known governing PDEs. Namely, the stochastic state Vθ(¯x) is modeled implicitly as,
∂Vθ(¯x)
∂t=Fnn
K(Vθ(¯x);λK,λU),Unn(Vθ(¯x);λK,λU,θnn);θnn
,¯x∈Ωp,t, (13a)
BC(Vθ(¯x);λK,λU) = 0 ,¯x∈∂Ωp,t, (13b)
IC(Vθ(¯x);λK,λU,Σ2
0) = 0 ,¯x∈Ωp,t=0,(13c)
When formulating auto-regressive structure for temporal evolution, the architecture of the
DiffHybrid neural for uncertainty propagation is depicted in Fig. 1. In contrast to determin-
Figure 1: The overview of the auto-regressive DiffHybrid architecture for uncertainty propagation.
istic DiffHybrid models [12, 13], the operators K,Unn, andFnnhere are designed to handle
probabilistic distribution rather than deterministic values. To this end, all neural networks
within this architecture are configured to process the mean and covariance at each grid point
as inputs, producing dual outputs that represent the estimated mean and covariance of the
intermediate functions. This design enables the DiffHybrid model to learn the spatiotem-
poral dynamics via the prediction mean ¯vθ(¯x), and also to quantify the associated aleatoric
uncertainty via the diagonal covariance estimation, diag 
Σ2
V,θ(¯x)
, which are spatiotemporal
fields as well. The initial diagnoal covariance values are set as diag 
Σ2
V,θ(x, t= 0)
=θΣ2
0,
11which are trainable. These values will be propagated over time following the autoregressive
temporal evolution, as illustrated in Fig. 1. In the current model design, the diag(Σ2) value
at the boundary is assumed to be zero. The trainable parameters of the DiffHybrid-UQ
model include DNN parameters, unknown physical coefficient, initial diagonal covariance
matrix, collectively denoted as θ= (θλU,θNN,θΣ2
0). Detailed DNNs architecture used for
this study can be found in Appendix D.2.
The PDE operators within the known segment of the right-hand side of the governing
equation (Eq.13) require discretization to formulate the predefined convolution layers, en-
coding the known physics. Detailed information on the discretization process is available in
Appendix C. After discretization, these discretized known operators need to manage the
propagation of uncertainty on the grid level. We consider two types of operators in this con-
text: linear and nonlinear known PDE operators. For linear operators Kl, such as discretized
diffusion terms ∇2, the propagation of mean and covariance matrix of a random variable Vis
straightforward. This is because the affine transformation of a Gaussian distribution result
in another Gaussian distribution. Specifically, we have
Kl 
N(¯vθ,Σ2
V,θ)
=N(K¯vθ+b, KΣ2
V,θKT), (14)
where the linear PDE operator is defined as Kl(v) =Kv+b.
When it comes to noninear PDE operators, Kn, the propagation of mean and variance
is more challenging, as it typically requires either linearization of the operator, which often
compromises accuracy, or the use of Monte Carlo sampling methods, which can be com-
putationally intensive if not infeasible. To address these challenges, our work utilizes the
unscented transform method [41]. The unscented transform (UT) is a mathematical tech-
nique used for filtering and smoothing in nonlinear dynamical systems. It aims to accurately
propgate the first and second moments of a probability distribution through nonlinear map-
pings. The method involves generating a small, deterministically selected set of sigma points
that effectively capture the mean and covariance of the initial probability distribution. These
sigma points are then transformed by applying the target nonlinear function to each point,
a process that distinguishes the UT from Monte Carlo methods which rely on random sam-
12pling. The number of sigma points used is small, greatly enhancing efficiency over Monte
Carlo methods. After undergoing the nonlinear transformation, the mean and covariance
of the new distribution are recalculated from these transformed sigma points through a
weighted summation, with weights as specified in [41]. This method is especially advanta-
geous as it avoids the complexities of Jacobian matrix calculations required in linearization
techniques like the Extended Kalman Filter. Moreover, it yields higher accuracy in scenar-
ios with significant nonlinearities or when precision in the distribution’s tail is crucial. A
illustrative plot of the UT vs. MC based uncertainty propagation is shown in Fig. 2. More
implementation details can be found in Appendix D.1.
Figure 2: Illustrative plot of unscented transformation (UT) versus Monte Carlo (MC) methods for uncer-
tainty propagation.
2.5. Scalable posterior sampling for epistemic UQ
Direct computation of the posterior p(θ|D) using traditional Bayesian sampling tech-
niques, e.g., MCMC, is impractical for deep learning models due to their high dimensional-
ity, which slows convergence and requires extensive computation. Additionally, DNNs often
present complex posterior landscapes that these methods struggle to navigate, leading to
memory constraints and convergence issues. To overcome these limitations, ensemble-based
techniques have emerged as a promising alternative, offering scalable and efficient approaches
to capture epistemic uncertainty. In this study, we employ the Stochastic Weight Averaging
13Gaussian (SWAG) [39], in combination with a DeepEnsemble framework [37], to approxi-
mate the posterior p(θ|D). This approach is grounded in the hypothesis by Mandt et al. [42],
suggesting that stochastic gradient descent (SGD) iterations can empirically approximate a
Markov chain of samples with a stationary distribution. Here, an SGD trajectory is inter-
preted as an approximate Bayesian MC sampler, effectively estimating the local geometry of
the posterior distribution of the trainable parameters θ. To enhance scalability and simplify
the process, the local posterior geometry is characterized primarily by its first and second
moments. This involves collecting only the running time mean and variance of θalong the
SGD trajectory to form a Gaussian approximation of the local posterior, a method known
as SWAG. Given the complex and multimodal nature of DNN posterior landscapes, SWAG
is ideally integrated within the DeepEnsemble framework. As illustrated in Fig. 3, this ap-
proach entails independently obtaining an ensemble of SGD trajectories in parallel, with
each trajectory potentially approximating different peaks of the posterior. This method not
only simplifies the representation of the posterior but also ensures a comprehensive explo-
ration of the model’s parameter space, significantly enhancing the accuracy and reliability
of epistemic uncertainty quantification in hybrid neural models.
Figure 3: (a) A schematic of the estimation of the posterior distribution using the ensemble-based SWAG
training (b) Prediction/Inference using the posterior distribution
Specifically, the training involves multiple ( Nm) instances of the DiffHybrid model, each
14initialized with a distinct set of initial parameters θ(j)
0, j= 1,···, Nm. These models are in-
dependently updated using the SWAG algorithm. For each SGD trajectory θ(j)
traj=
θ(j)
i	T
i=1,
the Gaussian statistics
θ(j)
SWA,Σ(j)
diag,ˆD(j)	
are computed on the fly. Here θ(j)
SWAis the mean,
Σ(j)
diagis diagonal covariance matrix, and ˆD(j)=θ(j)
traj−θ(j)
SWAis the deviation matrix for given
θ(j)
traj, respectively. To limit the rank and reduce the memory usage, ˆD(j)is often estimated
by retaining only the last rvectors of ˆD(j)corresponding to the last rsamples obtained
during training, thereby the deviation matrix results in ˆD(j)= [d(j)
T−r+1, ...,d(j)
T] with column
vector d(j)
k=θ(j)
k−θ(j)
SWA.Trepresents the total number of samples in each SGD trajectory.
Notably, the sampling can be conducted intermittently, every few epochs, with the interval
being adjustable. Consequently, the trajectory length Tcan be less than the total number of
training epochs. Additionally, to maintain memory efficiency, only the key statistics, rather
than the entire SGD trajectory θ(j)
traj, are stored during training. These treatments ensure the
scalability and computational feasibility for large DL and hybrid neural models. Upon the
completion of SWAG training, the local geometry of the posterior p(θ|D) is approximated
by the Gaussian density defined by each set of
θ(j)
SWA,Σ(j)
diag,ˆD(j)	
. The local posterior can
be sampled using the following equation,
˜θ(j)=θ(j)
SWA+1√
2·Σ(j)
diagz1+1p
2(r−1)ˆD(j)z2, z 1∼ N(0, Id), z2∼ N(0, Ir), (15)
where drepresents the number of training parameters in θ(j), and IdandIrare two identity
matrices of ranks dandr, respectively. Using the idea of DeepEnsemble method, the full pos-
terior distribution can be approximated by aggregating the Gaussian samples from multiple
independent SWAG local posterior approximations,
θ(j)
SWA,Σ(j)
diag,ˆD(j)	Nm
j=1. SWAG effec-
tively captures the posterior geometry around local minima, while the ensemble approach
addresses the multimodal nature of the posterior, thereby facilitating the efficient estima-
tion of epistemic/model uncertainty. Consequently, our approach involves training multi-
ple DiffHybrid model instances with SWAG simultaneously to capture different Gaussian
posterior distributions characterized by low-rank covariance matrices. The overall training
algorithm of the proposed DiffHybrid-UQ model is detailed in Algorithm 1 and the SWAG
implementation details are provided in Appendix B.
15Algorithm 1 Training algorithm for DiffHybrid-UQ model
Data: Read experimental data D={xi,vi}N
j=1
Initialize: Initialize trainable parameters {θ(j)
0}Nm
j=1={θ(1)
0,θ(2)
0, . . . , θ(Nm)
0}
fori = 1 to N epoch do
▷Training multiple models with different θ(j)
forθ(j)
i={θ(j)
i}Nm
j=1do
(vθ,Σ2
V,θ)←DIffHybrid-UQ 
Vinit= (v0,θ(j)
i,Σ2
0)),θ(j)
i= (θ(j)
i,λU,θ(j)
i,NN,θ(j)
i,Σ2
0)
(v(p)
θ,Σ2
V,θ(p))←Map toObservable( vθ,Σ2
V,θ) ▷Exp observables
L ←log(Σ2
V,θ(p))
2+(Vi−v(p)
θ)2
2Σ2
V,θ(p) ▷Loss function
θ(j)
i+1←θ(j)
i−λ∇θL(θ(j)
i) ▷Update θ(j)parameters
ifMOD(i, interval) = 0 then
n←i/interval
¯θ(j)←n¯θ(j)+θ(j)
i+1
n+1▷Update movements
¯θ2(j)←n¯θ2(j)+θ2
i+1(j)
n+1
ˆD(j)←[d(j)
i−r, ...,d(j)
i+1] ▷Store deviation
end
end
end
3. Results
This section presents extensive numerical experiments conducted to assess the perfor-
mance of the proposed DiffHybrid-UQ framework in modeling various dynamical systems
governed by ODEs and PDEs, with a focus on uncertainty quantification.
3.1. Modeling dynamical systems governed by ODEs
We first study the proposed DiffHybrid-UQ model on the Hamiltonian systems governed
by the simple ODEs described as follows,
∂x1
∂t=f1(x1, x2) =x2, (16a)
∂x2
∂t=f2(x1, x2) =−sin(x1), (16b)
16where v= [x1, x2]T∈R2is a two-dimensional state variable, and the functions f1, f2:
R2⇒R1can be linear or nonlinear, defining the right-hand side of the governing equations.
Synthetic data are generated by solving Eq. 16, and Gaussian random noise is added to
simulate measurement uncertainties typical of real-world experimental data. To illustrate
the DiffHybrid-UQ model’s capabilities, the governing physics is assumed to be only partially
known. Specifically, while the function f2is known, f1is considered unknown. Therefore,
we construct an auto-regressive hybrid neural model based on Eq. 16, where a trainable
Bayesian neural network (BNN), Unn:R4⇒R2represents the right-hand side of Eq. 16a,
processing the mean and variance of vto predict the Gaussian statistics of f1. The known
function f2is encapsulated within a UT function, resulting a probabilistic function, K=
UT(f2) :R4⇒R2. Both UnnandKfunction as probabilistic entities, handling random
variables to capture the inherent uncertainties in the training data.
During the training phase, the DiffHybrid-UQ model is trained with data spanning a
single trajectory over the time interval [0 : ∆ t:T′], where T′< T. The model’s predictive
performance is assessed in the forecast/testing range ( T′: ∆t:T]. To reflect realistic exper-
imental conditions, different measurement errors are introduced for different variables, ac-
knowledging that measurement uncertainty can differ across variables. To intensively assess
the model’s capacity for uncertainty prediction, we conduct three test cases: These scenarios
Case Data D Description
1 ( x1, x2∈R1×[0 : 0.1 : 20 s]) Complete data for both variables are provided
2 ( x1, x2∈R1×[5 : 0.1 : 20 s]) Partial data for both variables are provided
3 ( x2∈R1×[0 : 0.1 : 20 s]) Partial data for only one variable is provided
serve to test the model’s efficacy in accurately estimating uncertainties under varying data
availability conditions. In-depth analyses and results of these test cases will be discussed in
subsequent sections.
3.1.1. Case 1: Complete data for both state variables
The DiffHybrid-UQ model is trained using data for both the x1andx2variables over
20-second interval D= (x1, x2∈R1×[0 : 0 .1 : 20 s]). The training, spanning 10,000
17epochs, yields results depicted in Fig. 4. To verify the UQ performance of the proposed
DiffHybrid-UQ model, we also perform exact Bayesian learning using Hamiltonian Monte
Carlo (HMC) method, often considered the gold standard for Bayesian UQ. For HMC,
150,000 samples are obtained, initialized at the local minima obtained from gradient descent
training, thereby bypassing the burn-in phase to enhance computational efficiency. A side-
by-side comparison of the DiffHybrid-UQ and HMC predictions is shown in Fig. 4, with
color-shaded regions representing 3-STD confidence intervals. The prediction mean of the
Figure 4: Compares the DiffHybrid-UQ model’s prediction and UQ against HMC method. Training data
spans D= (x1, x2∈R1×[0 : 0.1 : 20 s]) with testing in the ( x1, x2∈R1×(20 : 0 .1 : 30 s]) region.
DiffHybrid model aligns very well with the ground truth for both x1andx2, even in the
forecasting region (20 : 0 .1 : 30 s], demonstrating model’s forecasting capability. Overall, the
total prediction uncertainty of the DiffHbrid-UQ model matches that of the HMC method.
Notably, the shaded areas in the first column, indicative of aleatoric uncertainty, differ for
variables x1andx2, corresponding to the varying noise levels in the data, as shown in the
third column. The aleatoric uncertainty estimated by the DiffHybrid-UQ model closely
18agree with that of the HMC method, reflecting its effectiveness in capturing inherent data
uncertainty. Regarding epistemic uncertainty, represented by shaded areas in the second
column, these remain small but increase in the forecast region where data are unavailable.
This rise in epistemic uncertainty is expected and reasonable due to the lack of data in this
region. However, a noticeable difference in epistemic uncertainty predictions between the
DiffHybrid-UQ and HMC methods exists, likely due to the initialization of all HMC samples
at a single local minimum, in contrast to the DeepEnsemble strategy of the DiffHybrid-UQ
model, which more effectively explores the multimodal posterior.
To evaluate the generalization capability of the DiffHybrid learning framework, the
trained model is tested on various trajectories generated under different initial conditions.
Figure 5 shows how the model predictions compared with the ground truth across these sce-
narios. As predictions deviate from the training trajectory, initialized at x= [0,15], there is
Figure 5: DiffHybrid-UQ prediction with quantified uncertainties compared against the ground truth for
different testing initial conditions. Trained initial condition: x= [0,15].
a notable decrease in mean prediction accuracy, accompanied by increased uncertainty. This
trend, evident in the parametric extrapolation region, indicates the DiffHybrid-UQ model
is able to reasonably estimate the confidence of its prediction, which diminish as it moves
beyond the range of the training scenarios.
Overall, the DiffHybrid-UQ model exhibit promising performance, producing uncertainty
19estimates that are in line with those obtained from the HMC samping. Its ability to quan-
tify both aleatoric and epistemic uncertainties offers valuable insights into the reliability of
its predictions, effectively addressing the inherent uncertainties stemming from both data
variability and model formulation.
3.1.2. Case 2: Sparse data for both variables
In this scenario, the DiffHybrid-UQ model is trained with partial data for both the x1
andx2variables, specifically within the time range of [5 : 0 .1 : 20 s], denoted as D=
(x1, x2∈R1×[5 : 0 .1 : 20 s]). Notably, data for the initial 5 seconds is not included. The
model undergoes a training period of 10,000 epochs, with results presented in Fig. 6. The
Figure 6: Comparison of the DiffHybrid-UQ model’s prediction with UQ against the ground truth. Training
data is D= (x1, x2∈R1×[5 : 0.1 : 20 s]), with testing in ( x1, x2∈R1×[0 : 0.1 : 5s)∪(20 : 0 .1 : 30 s]).
confidence intervals of the model predictions effectively reflect the data uncertainty for both
x1andx2. While the prediction for x1shows a notable deviation from the ground truth,
the model aptly indicates lower confidence in this region. On the other hand, for x2, the
model’s uncertainty remains relatively low in the same interval, suggesting a good alignment
20between the DiffHybrid-UQ predictions and the ground truth. The model’s performance in
unseen data regions is reasonable, with an increasing level of uncertainty observed towards
the forecasting region (20 : 0 .1 : 30 s]. Thus, the DiffHybrid-UQ model showcases its ability to
estimate uncertainties effectively, even with partially available data. By precisely capturing
data uncertainties and indicating reduced confidence in data-sparse regions, the model offers
valuable insights into the reliability of its predictions.
3.1.3. Case 3: Incomplete data for only one state variable
A key strength of hybrid neural models lies in their ability to be trained with incomplete
data, even when labels for certain state variables are absent. To demonstrate this capability,
we trained the DiffHybrid-UQ model using only the data for x2, designated as D= (x2∈
R1×[0 : 0.1 : 20 s]). This training, encompassing 10,000 epochs, produced results illustrated
in Fig. 7. The results show that the DiffHybrid-UQ model effectively captures the dynamics
Figure 7: Comparison of the DiffHybrid-UQ model’s prediction with UQ against the ground truth. Training
data is D= (x2∈R1×[0 : 0.1 : 20 s]), with testing in ( x1∈R1×[0 : 0.1 : 30 s], x2∈R1×(20 : 0 .1 : 30 s]).
21ofx2, while the x1prediction mean notably deviates from the ground truth. This is due to
that the information for x1is learned indirectly from the labeled data of x2. For x1, the model
understandably exhibits much higher uncertainty as not directly labels are provided. In
contrast, for x2, the model displays relatively low uncertainty. In line with previous findings,
confidence interval increases in the forecast region, reflecting the increased model uncertainty
beyond the training data. Its ability to reflect data scarcity and increase uncertainty in
unobserved regions highlights its robustness in limited data scenarios.
3.2. Modeling spatiotemporal dynamics governed by PDEs
In this section, the DiffHybrid-UQ is utilized to model spatiotemporal dynamics with
quantified uncertainty, which are also spatiotemporal fields. Specifically, its efficacy is as-
sessed on a reaction-diffusion system governed by the following PDEs,
˙v1=D1∇2v1+s1(v1, v2) (17a)
˙v2=D2∇2v2+s2(v1, v2) (17b)
where v1, v2∈Rnx×nyare the state variables over a discretized 2-D domain, characterized
bynx×nygrid points in the x, ydirections. The diffusion coefficients are set as D1=
2.8×10−4andD2= 5.0×10−2. The source terms s1(v1, v2) =v1−v3
1−v2−0.005 and
s2(v1, v2) = 10( v1−v2) define the reaction dynamics. Neumann boundary conditions are
applied in these simulations. Similar to ODE cases above, synthetic datasets for these PDEs
are generated and perturbed with Gaussian random noises. We assume that the model forms
for the reaction dynamics is partially known: s1is given, while s2(v1, v2) remains unknown.
Therefore, in our DiffHybrid-UQ framework, the known s1(v1, v2) is encapsulated within an
UT function, resulting in a known random function K(V) =UT(s1)(V) :R4⇒R2, which is
seamlessly integrated with Bayesian neural networks Ubnn(V) :R4⇒R2to learn the missing
reaction physics. The diffusion operator is encoded as known convolution operations using
finite difference method (FDM),
∇2v(i, j) =v(i+ 1, j) +v(i−1, j)−2v(i, j)
∆x2+v(i, j+ 1) + v(i, j−1)−2v
∆y2. (18)
22where v(i+ 1, j), v(i−1, j), v(i, j+ 1), v(I, j−1) are neighbouring values of v(i, j) post-
discretization. Now, the discretized diffusion term as a linear combination of adjacent spatial
points leads to the output mean and variance as,
µ∇2V(i, j) =V(i+ 1, j) +V(i−1, j)−2V(i, j)
∆x2+V(i, j+ 1) + V(i, j−1)−2V
∆y2(19a)
Σ2
∇2V(i, j) =Σ2
V(i+1,j)+ Σ2
V(i−1,j)+ 4Σ2
V(i,j)
∆x4+Σ2
V(i,j+1)+ Σ2
V(i,j−1)+ 4Σ2
V(i,j)
∆y4(19b)
Furthermore, diffusion coefficients Diare considered as unknown variables and kept trainable
throughout all test cases, unless specifically mentioned otherwise. The Hybrid models are
typically employed in scenarios with limited, indirect data. To replicate such conditions,
unless otherwise stated, we provide training data only for the v2variable and exclude any
labels for v1. Training is conducted on a specific trajectory within the timeframe [0 : ∆ t:T′],
thusD= (v2∈Rnx×ny×[0 : ∆ t:T′]), where T′< T. The model’s predictive performance
is then evaluated in the test region ( v1∈Rnx×ny×[0 : ∆ t:T], v2∈Rnx×ny×(T′: ∆t:T]).
3.2.1. Dynamical forecasting with spatiotemporal uncertainty estimation
To investigate the spatiotemporal behavior of predicted uncertainty, we trained the
DiffHybrid-UQ model using the data of a single trajectory for up to 4 seconds on a 20 ×20
grid, i.e., D= (v2∈R20×20×[0 : 0.01 : 4 s]) and tested it in the forecast region (4 : 0 .01 : 8 s].
For this analysis, the diffusion coefficients Diwere assumed to be known. To clearly observe
the spatial uncertainty prediction, the initial condition is defined by a Gaussian random
field. The model was trained for 100 epochs, with results displayed in Fig. 8 and Fig. 9.
As we can see from Fig. 8, the prediction mean of both v1andv2agree with the ground
truth reasonably well, while the prediction errors grow temporally. There is a strong correla-
tion between the spatial prediction error (third row) and the estimated uncertainty (fourth
row). Notably, no training data was provided for v1, highlighting the model’s capability to
learn the dynamics and estimate corresponding uncertainty in the absence of direct labels.
Fig. 9 illustrates temporal uncertainty through a shaded region representing three times
the standard deviation, signifying the confidence interval. This interval effectively captures
the data uncertainty, which increases over time, consistent with the auto-regressive nature
23(a)v1
(b)v2
Figure 8: Comparison of the DiffHybrid-UQ model’s prediction (2nd row) with UQ (4th row) against the
ground truth (1st row) for variable v1(a) and v2(b) over time. Training data: D= (v2∈R20×20×[0 : 0.01 :
4s]), with testing in ( v1∈R20×20×[0 : 0.01 : 8 s], v2∈R20×20×(4 : 0.01 : 8 s]).
24(a)v1
 (b)v2
Figure 9: Comparison of the DiffHybrid-UQ model’s prediction with UQ against the ground truth for variable
v1(a) and v2(b) at three spatial locations over time. Training data: D= (v2∈R20×20×[0 : 0 .01 : 4 s]),
with testing in ( v1∈R20×20×[0 : 0.01 : 8 s], v2∈R20×20×(4 : 0.01 : 8 s]).
of the model where confidence diminishes with each subsequent time-step prediction. The
aleatoric uncertainty for v1remains relatively constant throughout the forecast period since
not direct labels are involved for v1. In contrast, the data uncertainty for v2escalates over
time, a consequence of the larger diffusion coefficient D2. Since data for v2was provided
during training, the epistemic uncertainty for v2is comparatively lower than that for v1.
These results affirm that the DiffHybrid-UQ model successfully captures the spatiotemporal
behavior of predicted uncertainty.
3.2.2. Inference and generalization capability of DiffHybrid-UQ model
The inference capability of the DiffHybrid-UQ model is evaluated through an experimen-
tal study where the diffusion coefficients Diare assumed to be unknown and trainable. The
model is trained on a single trajectory for up to 3.5 seconds ( D= (v2∈R20×20×[0 : 0.01 :
3.5s])) and then tested in the forecast region (3 .5 : 0.01 : 5 s]. After 300 epochs of training,
the outcomes are presented in Fig. 10 and Fig. 11. The results indicate strong prediction
25(a)v1
(b)v2
Figure 10: Comparison of the DiffHybrid-UQ model’s prediction (2nd row) with UQ (4th row) against the
ground truth (1st row) for v1(a) and v2(b) over time. Training data: D= (v2∈R20×20×[0 : 0.01 : 3 .5s]),
with testing in ( v1∈R20×20×[0 : 0.01 : 3 .5s], v2∈R20×20×(3.5 : 0.01 : 5 s]).
26(a)v1
 (b)v2
Figure 11: Comparison of the DiffHybrid-UQ model’s prediction with UQ against the ground truth for v1
(a) and v2(b) at three spatial locations over time. Training data: D= (v2∈R20×20×[0 : 0.01 : 3 .5s]), with
testing in ( v1∈R20×20×[0 : 0.01 : 3 .5s], v2∈R20×20×(3.5 : 0.01 : 5 s]).
performance by the model, as the prediction mean agree with the ground truth well for both
variables. As depicted in Fig. 10, the predicted spatial uncertainty aligns well with expecta-
tions, which correlate with the prediction errors. The confidence interval shown in Fig. 11
effectively encapsulates the data scattering. Moreover, in line with prior observations, there
is a notable increase in model-form uncertainty over time, further validating the model’s
effectiveness in dynamic modeling with UQ.
In this case, the model accurately predicts the dynamics of the unobserved state v1and
also successfully infers the unknown diffusion coefficients D1andD2during the training pro-
cess, as shown in Fig. 12. Hybrid models are often susceptible to aliasing errors, particularly
when trained with limited data. This issue can lead to inaccuracies in both the learned surro-
gate functions and the inferred coefficients, with the magnitude of errors typically increasing
as the size of training data diminishes. In response to this challenge, the DiffHybrid-UQ
model is designed to estimate errors in inferred parameters, thereby providing a more robust
analysis. Figure 12 exemplifies the model’s capability in quantifying uncertainty associated
27with the diffusion coefficients throughout the training process. This uncertainty, indicative
of the aliasing effect, is observed to decrease as training advances. While it is not possible
to guarantee absolute accuracy in the inferred diffusion coefficients due to the inverse and
ill-posed nature of the problem, the DiffHybrid-UQ model enables a reasonably confident
estimation. The fidelity of these inferences is contingent upon the dominant physics cap-
tured in the data, along with other influential factors, which will be elaborated upon in the
Discussion section.
Figure 12: DiffHybrid-UQ inferred diffusion coefficients ( D1andD2) with quantified uncertainty over training
epochs. DE represents ensembles of inferred diffusion coefficients.
To evaluate the DiffHybrid-UQ model’s generalizability, we performed tests on various
grid sizes and time steps, using a randomly generated initial conditions, which is distinct from
the one used in the training phase. Figure 13 illustrates the test results on a 10 ×10 grid with
a time step of 0.02 seconds, while Fig. 14 presents the test results for a 40 ×40 grid with a time
step of 0.001 seconds. Across these tests, the model’s predictions agree with the references
reasonably well, demonstrating great generalizability. Notably, regions with higher deviation
from the ground truth correspond to increased uncertainty in the model’s predictions. This
observation underscores the model’s capability of quantifying uncertainty, even under varying
testing conditions. It is crucial to highlight that, despite potential inaccuracies in the inferred
diffusion coefficients, the trained surrogate model consistently demonstrates its ability to
provide reliable predictions with quantified uncertainty.
28Figure 13: DiffHybrid-UQ testing results on an unseen randomly generated initial field, on a 10 ×10 grid
with time-step of 0.02 sec.
Figure 14: DiffHybrid-UQ testing results on an unseen randomly generated initial field, on a 40 ×40 grid
with time-step of 0.001 sec.
293.2.3. Correcting discretization errors and speedup
In hybrid neural solvers, employing coarse spatio-temporal resolution is a common prac-
tice to enhance inference efficiency. However, this approach can introduce significant dis-
cretization errors in the PDE-integrated components, potentially affecting the accuracy of
solutions. Within our DiffHybrid-UQ framework, this issue can be addressed by trainable
neural network blocks, which are designed not only to learn and model unknown physi-
cal phenomena but also to concurrently correct for the discretization errors inherent in the
coarser resolution. This dual capability allows the DiffHybrid-UQ model to maintain so-
lution accuracy while benefiting from the computational efficiency of a coarser grid and
larger timesteps. In this context, we explored the efficacy of the DiffHybrid-UQ model op-
erating on coarse spatiotemporal resolutions. To this end, no structural modifications are
needed, and the model’s Unncomponent serves a dual role: it acts as a surrogate for un-
known functions and corrects solutions on coarser grids. In our evaluation, the model was
trained on a coarse 20 ×20 grid with a time step of 0.01 seconds, using data sampled from
a high-resolution solution on a 200 ×200 grid with a finer time step of 0.0005 seconds. This
approach allowed us to investigate the model’s ability to replicate fine-grid precision on a
coarser grid. The training data covered a single trajectory up to 3.7 seconds, denoted as
D= (v1, v2∈R20×20×[0 : 0 .01 : 3 .5s]∼R200×200×[0 : 0 .0005 : 3 .5s]), and the model’s
predictions were subsequently assessed in the forecast period of 3.7 s to 5 s.
Post-training for 500 epochs, the DiffHybrid-UQ model exhibited commendable perfor-
mance within the training range. The results, presented in Fig.15, showed reasonable predic-
tion accuracy. However, a deviation from the ground truth was observed in the extrapolation
region, where the model predicted a notably higher level of uncertainty, reflecting its recog-
nition of increased error potential. This ability to predict with heightened uncertainty in
less certain regions demonstrates the model’s robustness, offering fine-grid level accuracy on
coarser grids and thus reducing computational load. The efficiency of the DiffHybrid-UQ
model, in terms of inference time and memory usage for both fine and coarse simulations,
is detailed in Table.1, highlighting its capability to produce high-fidelity results with lower
30Figure 15: Compares the DiffHybrid-UQ model’s predictions and UQ on a 20 ×20 grid with a time step of
0.01 sec against the ground truth generated on a 200 ×200 grid with a time step of 0.0005 sec. Training
data spans D= (v1, v2∈R20×20×[0 : 0 .01 : 3 .5s]∼R200×200×[0 : 0 .0005 : 3 .5s]) with testing in the
(v1, v2∈R20×20×(3.5 : 0.01 : 5 s]∼R200×200×(3.5 : 0.0005 : 5 s]) region.
computational demands and memory requirements.
Inference CPU time GPU time Memory
R2×200×200×[0 : 0.0005 : 5 s] 0.67 sec 0.66 sec 33 GB
R2×20×20×[0 : 0.01 : 5 s] 8.8 sec 1.15 sec 12 MB
Table 1: Inference time and memory consumption.
3.3. Identifiability of unknown physical parameters
The identifiability of physical parameters (e.g., diffusion coefficients) within the DiffHybrid-
UQ framework, presents intriguing insights into the model’s capability and limitations. As
31shown in cases presented above, a notable issue is the relatively low accuracy of inferred mean
diffusion coefficients and associated high prediction uncertainty. This phenomenon can be
attributed to the training data’s dynamics being primarily driven by reaction physics, re-
ducing the influence of the diffusion term on the overall dynamics. Therefore, the diffusion
coefficients are less sensitive and difficult to be inferred accurately. We hypothesized that
when the dynamics are more substantially influenced by the diffusion process, the model’s
ability to accurately infer diffusion coefficients would improve. To test this hypothesis, a
Figure 16: DiffHybrid-UQ test predictions with UQ, where diffusion coefficients are inferred simultaneously.
simulation with enhanced diffusion prominence was conducted, setting D1= 2.8×10−3and
D2= 5.0×10−3. This modification aimed to shift the focus towards the diffusion process.
After 300 epochs of training, the results, as depicted in Fig. 16, indicated a notable improve-
ment in parameter inversion accuracy. The model’s estimated confidence intervals closely
captured the data/aleatoric uncertainty, and the inferred diffusion coefficients were more
32aligned with the ground truth. Furthermore, a decrease in model uncertainty was observed
with each training epoch. These observations substantiate our hypothesis, emphasizing that
the model’s inferential accuracy is heavily dependent on the representation of governing
physical processes in the training data. In scenarios where diffusion processes dominate, the
model has enhanced capability in accurately inferring diffusion coefficients.
4. Conclusion
In this study, we have introduced DiffHybrid-UQ, a novel approximate Bayesian learn-
ing framework for quantifying uncertainty in physics-integrated hybrid neural differentiable
models. This methodology, blending ensemble Bayesian learning with nonlinear transfor-
mations, has demonstrated substantial potential in effectively capturing and quantifying
uncertainties associated with data-driven modeling of complex physical systems. Through
rigorous evaluation on both ordinary and partial differential equations, the DiffHybrid-UQ
model has demonstrated its proficiency in diverse aspects of UQ. Notably, it has accurately
provided lower confidence levels for predictions in regions with sparse training data, effec-
tively capturing the growth of uncertainty over time due to error accumulation. In scenarios
involving parametric extrapolation, the model adeptly indicated increased uncertainty, par-
ticularly when initial conditions diverged from those in the training set. For spatiotemporal
problems governed by PDEs, the DiffHybrid-UQ model can be trained with partial obser-
vational data, delivering reasonable predictions of spatiotemporal uncertainty. Its capacity
to simultaneously infer physical parameters with quantified uncertainty, especially in the
presence of aliasing errors, was particularly noteworthy. Furthermore, we also showcased the
DiffHybrid-UQ model’s ability to provide accurate predictions on coarse grids with reduced
computational demands, underscoring its potential in practical applications where compu-
tational efficiency is critical. In sum, this work marks a crucial stride forward in the field of
UQ for hybrid neural differentiable models that integrate physics-based numerical compo-
nents with deep learning models using differentiable programming. This advancement holds
great promise for refining physics-informed, data-driven decision-making processes across
33a wide array of scientific and engineering disciplines, fostering more accurate, robust, and
trustworthy models for complex systems analysis.
Overall, this research represents a significant advancement in the field of uncertainty
quantification for physics-integrated hybrid deep neural differentiable models. The successful
integration of ensemble learning and nonlinear transformation provides a robust framework
for enhancing prediction reliability and interpretability. The proposed method holds immense
potential for improving hybrid simulation-based decision-making processes across various
scientific and engineering domains.
Acknowledgment
The authors would like to acknowledge the funds from the Air Force Office of Scientific
Research (AFOSR), United States of America under award number FA9550-22-1-0065. JXW
would also like to acknowledge the funding support from the Office of Naval Research under
award number N00014-23-1-2071 and the National Science Foundation under award number
OAC-2047127 in supporting this study.
Compliance with Ethical Standards
Conflict of Interest: The authors declare that they have no conflict of interest.
Appendix A. Nomenclature
Nomenclature
v dynamic state variable
Vθdynamic state variable distribution with mean vθ, and covariance diag(Σ2
V,θ) approx-
imated by the probabilistic DiffHybrid-UQ model for a given θ
vθ dynamic state variable approximated by the deterministic DiffHybrid model for a
given θ
34µV mean of state vapproximated by the probabilistic DiffHybrid-UQ model considering
allθ
diag(Σ2
V,θ) diagonal covariance matrix of state vapproximated by the probabilistic DiffHybrid-
UQ model for a given θ
diag(Σ2
V) diagonal covariance matrix of state vapproximated by the probabilistic DiffHybrid-
UQ modell considering all θ
Appendix B. Stochastic Weight Averaging Gaussian (SWAG)
The detailed SWAG [39] algorithm is described in this section. For a given SGD training
trajectory θtraj={θi}T
i=1={θ1,θ2, ...,θT}, the SWAG algorithm formulates an approxi-
mate Gaussian distribution N(θSWA,1
2(Σdiag+Σ low-rank )) for posterior, where the mean θSWA
is calculated as
θSWA=¯θ=1
TTX
i=1θi, (B.1)
and the diagonal covariance approximation Σ diagof the posterior distribution is calculated
as
¯θ2=1
TTX
i=1θ2
i (B.2a)
Σdiag=diag(¯θ2−θ2
SWA). (B.2b)
The sample covariance matrix of rank Tfor a given SGD training trajectory θtrajcan be
calculated using Eq. B.3a. However, storing the whole trajectory θtrajfor a model with
a large number of parameters will be an infeasible task. Therefore, the SWAG method
approximates the sample covariance with a low rank rmatrix as given in Eq. B.3b and store
deviation matrix ˆDwith columns di= (θi−¯θi).
Σ =1
T−1TX
i=1(θi−θSWA)(θi−θSWA)T(B.3a)
Σlow-rank ≈1
r−1TX
i=T−r+1(θi−¯θi)(θi−¯θi)T=1
r−1TX
i=T−r+1ˆDˆDT(B.3b)
35Namely, instead of storing the whole trajectory, the posterior distribution statistics are
updated while training according to the following algo [39]
¯θ←n¯θ+θi
n+ 1(B.4a)
¯θ2←n¯θ2+θ2
i
n+ 1(B.4b)
ˆD←concatenate( ˆD[:,2 :],θi−¯θ) (B.4c)
and the following equation is utilized to obtain samples from N(θSWA,1
2(Σdiag+ Σ low-rank ))
for prediction.
˜θ=θSWA+1√
2·Σ1/2
diagz1+1p
2(r−1)ˆDz2, z 1∼ N(0, Id), z2∼ N(0, Ir) (B.5)
Appendix C. Discretization of Governing PDE for the stochastic state V(¯ x)
Considering the probability distribution of all the random variables are diabolized Gaus-
sian distribution, the mean and variance of the random variable, which is a linear combination
of other random variables Y=P
iciXiis calculated as
µY=X
iciµXi, (C.1a)
Σ2
Y=X
ic2
iΣ2
Xi, (C.1b)
Appendix C.1. Discretization in time
Lets rewrite Eq. 13a
∂Vθ(¯x)
∂t=Fnn
K(Vθ(¯x);λK,λU),Unn(Vθ(¯x);λK,λU,θnn);θnn
as
∂
∂t(V(x, t)|θ) =Fnn(t,V(x, t)|θ). (C.2)
Now to discretize Eq. C.2 in time, we approximate the spatiotemporal random state
variable V(x, t) as a spatial random state variable at various times {0, ..., t, ..., T r}.
V(x, t)≈ {V(t)(x)}Tr
t=0
36Equation. C.2 can be discretized in time using the Euler method as
(V(t+1)(x)|θ) = (V(t)(x)|θ) + ∆ tFnn(t,V(t)(x)|θ), (C.3)
where the mean and the variance of the Gaussian random variable Vwill be updated as
(µK1,Σ2
K1) = (µFnn,Σ2
Fnn) =Fnn 
t,(µVt,Σ2
Vt)|θ
, (C.4a)
(µVt+1|µVt,θ) =µVt+ ∆tµK1 (C.4b)
(Σ2
Vt+1|Σ2
Vt,θ) = Σ2
Vt+ ∆t2Σ2
K1., (C.4c)
Similarly, eq. C.2 can be discretized in time using the Runge-Kutta 4th-order method as
(µK1,Σ2
K1) =Fnn 
t,(µVt,Σ2
Vt)|θ
, (C.5a)
(µK2,Σ2
K2) =Fnn 
t+ 0.5∆t,(µVt+ 0.5∆tµK1,Σ2
Vt+ 0.25∆t2Σ2
K1)|θ
, (C.5b)
(µK3,Σ2
K3) =Fnn 
t+ 0.5∆t,(µVt+ 0.5∆tµK2,Σ2
Vt+ 0.25∆t2Σ2
K2)|θ
, (C.5c)
(µK4,Σ2
K4) =Fnn 
t+ ∆t,(µVt+ ∆tµK3,Σ2
Vt+ ∆t2Σ2
K3)|θ
, (C.5d)
(µVt+1|µVt,θ) =µVt+∆t
6 
µK1+ 2µK2+ 2µK3+µK4
(C.5e)
(Σ2
Vt+1|Σ2
Vt,θ) = Σ2
Vt+∆t2
36 
Σ2
K1+ 4Σ2
K2+ 4Σ2
K3+ Σ2
K4
(C.5f)
where Kiare Gaussian random variables Ki(x|θ)∼ N(µKi(x, t|θ),diag(Σ2
Ki(x, t|θ))).
Appendix C.2. Discretization in space
Now to discretize the right-hand side of Eq. C.2 in space, we approximate the spatial
random state variable V(t)(x) as a random state variable at a grid point x.
V(t)(x)≈ {V(t)
(x)}ngrid
x=0.
Any method can be used to discretize the above equation in space. Here in this study, FDM
is used. The discretization results in Gaussian random vector V(x, t)≈ {V(t)(x)}Tr
t=0≈
{{V(t)
(x)}ngrid
x=0}Tr
t=0. And for 2D, V∼ N (V|µV,diag(Σ2
V)), where µV∈Rnt×nv×(nx×ny)and
diag(Σ2
V)∈Rnt×nv×(nx×ny)are the mean and co-variance vector. Here nt,nv,nx, and nyare
37the number of time steps, the number of variables, and the number of grid points in the x
and y directions, respectively.
For instance, for given V(t)∼ N (V(t)|µ(t),diag(Σ2(t))), where µ(t)∈Rnv×(nx×ny)and
diag(Σ2(t))∈Rnv×(nx×ny)are the mean and co-variance vector, the discretization of the
Laplace term results in
∇2V(t)
(i,j)=V(t)
(i+1,j)+V(t)
(i−1,j)−2V(t)
(i,j)
∆x2+V(t)
(i,j+1)+V(t)
(i,j−1)−2V(t)
(i,j)
∆y2
And as the Laplace is a linear operator, we can write
∇2µ(t)
(i,j)=µ(t)
(i+1,j)+µ(t)
(i−1,j)−2µ(t)
(i,j)
∆x2
+µ(t)
(i,j+1)+µ(t)
(i,j−1)−2µ(t)
(i,j)
∆y2(C.6)
∇2diag(Σ2(t)
(i,j)) =diag(Σ2(t)
(i+1,j)) + diag(Σ2(t)
(i−1,j)) + 4diag(Σ2(t)
(i,j))
∆x4
+diag(Σ2(t)
(i,j+1)) + diag(Σ2(t)
(i,j−1)) + 4diag(Σ2(t)
(i,j))
∆y4. (C.7)
where i, jare x , y indices.
Appendix D. Uncertainty Propagation through functions
Appendix D.1. Uncertainty Propagation through known functions using Unscented-Transform
method
As discussed in the methodology section, the known non-linear operators Khave been
designed to handle probability distribution with the help of the UT method. This is achieved
by wrapping the deterministic operator Kdwith the UT function. Detailed procedure is given
below.
The UT function takes the mean and variance of the input dynamic variable Vand
calculates sigma points Xusing the following formulation[41].
38X0 =µV
Xi =µV+p
(L+λ)Σ2
V
ii= 1, . . . , L
Xi =µV−p
(L+λ)Σ2
V
i−Li=L+ 1, . . . , 2L
W(m)
0 =λ/(L+λ)
W(c)
0 =λ/(L+λ) + (1 −α2+β)
W(m)
i =W(c)
i= 1/{2(L+λ)}i= 1, . . . , 2L
where λ=α2(L+κ)−Lis a scaling parameter. Lis the dimension of the dynamic
random variable V,αdetermines the spread of the sigma points around µV,κis a secondary
parameter, and βis used to incorporate prior knowledge of of Vdistribution. Here for current
study we used α= 1, κ= 0, and β= 2. These sigma vectors are thereafter propagated
through the nonlinear function Kd,
Yi=Kd(Xi)i= 0, . . . , 2L
and the mean and covariance for yare approximated using a weighted sample mean and
covariance of the posterior sigma points,
µy≈2LX
i=0W(m)
iYi
Σ2
y≈2LX
i=0W(c)
i{Yi−µy}{Y i−µy}T.
Detailed algorithm for operator Kis given in algo. 2.
Appendix D.2. Uncertainty Propagation through Neural-Network
Figure D.17 illustrates the Neural Network architectures utilized for learning the un-
known functions in both ODE and PDE systems. For ODE-NN, each hidden layer has
8 neurons, while in the case of PDE-NN, 32 neurons were employed. The PDE-NN net-
work follows a point-to-point approach, taking independent distribution statistics (first- and
second-order moments) inputs at every point of the grid and generating the corresponding
output distribution statistics at that specific grid point.
39Algorithm 2 An algorithm for uncertainty propagation through nonlinear function
Function K(Kd,V= (µV,Σ2
V), α, β, κ ):
L←dim(µV)
λ←α2(L+κ)−L
W(m)
0←λ/(L+λ) ▷Calculate weights
W(c)
0←λ/(L+λ) + (1 −α2+β)
X0←µV ▷Calculate Sigma points
fori = 1 to Ldo
W(m)
i←1/2(L+λ) ▷Calculate weights
W(c)
i←1/2(L+λ)
Xi←µV+ (p
(L+λ)Σ2
V)i ▷Calculate Sigma points
Xi+L←µV−(p
(L+λ)Σ2
V)i
Yi← K d(Xi) ▷Propagate Xthrough Kn
Yi+L← K d(Xi+L)
end
µy←P2L
i=0Wi(m)Yi ▷Calculate mean and variance of o/p
Σ2
y←P2L
i=0Wi(c){Yi−µy}{Y i−µy}T
return µy,Σ2
y ▷Output random variable
Appendix E. Algorithms
The detailed algorithms used to construct a DiffHybrid-UQ solver for Hamiltonian ODEs
and Reaction-Diffusin PDEs case are shown in Alog. 3 and Alog. 4, respectively.
Appendix F. Additional Computational Results
Appendix F.1. Hamiltonian
In addition to the test cases shown in the Result section, one more case was conducted on
the Hamiltonian ODEs system, where the DiffHybrid-UQ model is trained by only providing
the data for the x2variables in 5 to 20 sec, specifically D= (x2∈R1×[5 : 0 .1 : 20 s]) =
40Figure D.17: Neural network architecture used in DiffHybrid-UQ model.
Algorithm 3 An algorithm for DiffHybrid-UQ solver for Hamiltonian ODEs system
Function DiffHybrid-UQ( xinit= (µ0
x,Σ0
x),θ= (θλU,θNN,θΣ2
0)):
t←0
while t < t maxdo
(µs1,Σ2
s1)← NNr
U((µx1,Σ2
x1)t,θ) ▷Compute source with UT
(µs2,Σ2
s2)← Kr
n((µx2,Σx2)t,θ) ▷Compute source with NN
▷Euler time stepping, (can use RK4)
(µxi,Σ2
xi)t+1←(µxi,Σ2
xi)t+ (∆ tµsi,∆t2Σ2
si)
end
return ({µxi}tmax
t=0,{Σ2
xi}tmax
t=0) ▷Predicted time series
R1×[0 : 0 .1 : 20 s]. The training process involved 10,000 epochs, and the results obtained
are presented in Fig. F.18.
Due to the limited amount of provided data, the predicted uncertainty is significantly
large, except in the region where the data is available. Despite these challenging conditions,
41Algorithm 4 An algorithm for DiffHybrid-UQ solver for reaction-diffusion PDEs system
Function DiffHybrid-UQ( Vinit= (µ0
V,Σ0
V),θ= (θλU,θNN,θΣ2
0)):
t←0
while t < t maxdo
(µ∇2Vi,Σ2
∇2Vi)←Laplace(( µt
Vi,Σt
Vi),θ) ▷Construct Laplace
(µs1,Σ2
s1)← Kr
n((µV1,ΣV1)t,θ) ▷Compute source with UT
(µs2,Σ2
s2)← NNr
U((µV2,ΣV2)t,θ) ▷Compute source with NN
▷Euler time stepping, (can use RK4)
(µVi,Σ2
Vi)t+1←(µVi,Σ2
Vi)t+ (∆ t(θDiµ∇2Vi+µsi),∆t2(θ2
DiΣ2
∇2Vi+ Σ2
si))
(µVi,Σ2
Vi)t+1←Boundary Condition(( µVi,Σ2
Vi)t+1)
end
return ({µVi}tmax
t=0,{Σ2
Vi}tmax
t=0) ▷Predicted time series
Figure F.18: Comparison of the DiffHybrid-UQ model’s prediction with UQ against the ground truth.
Training data is D= (x2∈R1×[5 : 0.1 : 20 s]), with testing in ( x1∈R1×[0 : 0.1 : 30 s], x2∈R1×[0 : 0.1 :
5s)∪(20 : 0 .1 : 30 s]) region.
the predicted model uncertainty exhibits reasonable behavior. For instance, in the 0 to
5-second region, the model uncertainty for x2remains high, while for x1, it increases rapidly.
42Appendix F.2. Reaction-Diffusion
Figure F.19: Compares the DiffHybrid-UQ model’s predictions and UQ on a 20 ×20 grid with a time step of
0.01 sec against the ground truth. Training data spans D= (v1, v2∈R20×20×[0 : 0.01 : 3 .5s]) with testing
in the ( v1, v2∈R20×20×(3.5 : 0.01 : 5 s]) region.
Furthermore, in addition to the test cases discussed in the Results section, we present two
additional test cases involving the Reaction-Diffusion PDEs system. Figure F.19 displays
the results obtained after training the model for 300 epochs on a 20 ×20 grid with a time-
step of 0.01 sec. In this case, data for v1andv2is provided up to 3.5 sec, represented as
D= (v1, v2∈R20×20×[0 : 0.01 : 3 .5s]) =R2×20×20×[0 : 0.01 : 3 .5s]. Comparing this case to
a scenario where only data for v2is provided (Fig. 10 and Fig. 11), we observe a reduction
in error, improved accuracy in inferring the diffusion coefficients, and accordingly reduction
in confidence interval.
Similarly to the test case described in the Corrector Network section, we present the
43Figure F.20: Compares the DiffHybrid-UQ model’s predictions and UQ on a 20 ×20 grid with a time step
of 0.01 sec against the ground truth generated on a 200 ×200 grid with a time step of 0.0005 sec. Training
data spans D= (v2∈R20×20×[0 : 0 .01 : 3 .5s]∼R200×200×[0 : 0 .0005 : 3 .5s]) with testing in the
(v1∈R20×20×[0 : 0.01 : 5 s]∼R200×200×[0 : 0.0005 : 5 s], v2∈R20×20×(3.5 : 0.01 : 5 s]∼R200×200×(3.5 :
0.0005 : 5 s]) region.
results in Figure F.20, where the training data is provided solely for v2. The model is
trained for 500 epochs. In the training region, which extends up to 3.5 sec, the results
exhibit reasonable accuracy. However, as we move into the extrapolation region for v1,
the predictions deviate from the ground truth. Notably, the predicted uncertainty in the
extrapolation region is also high, indicating the lack of reliability in the Hybrid model’s
prediction.
44References
[1] M. Raissi, P. Perdikaris, G. Karniadakis, Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial
differential equations, Journal of Computational Physics 378 (2019) 686–707.
[2] L. Sun, H. Gao, S. Pan, J.-X. Wang, Surrogate modeling for fluid flows based on physics-
constrained deep learning without simulation data, Computer Methods in Applied Me-
chanics and Engineering 361 (2020) 112732.
[3] L. Sun, J.-X. Wang, Physics-constrained bayesian neural network for fluid flow recon-
struction with sparse and noisy data, Theoretical and Applied Mechanics Letters 10 (3)
(2020) 161–169.
[4] Z. Li, N. B. Kovachki, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, A. Anandku-
mar, et al., Fourier neural operator for parametric partial differential equations, in:
International Conference on Learning Representations, 2021.
[5] L. Lu, P. Jin, G. Pang, Z. Zhang, G. E. Karniadakis, Learning nonlinear operators via
deeponet based on the universal approximation theorem of operators, Nature Machine
Intelligence 3 (3) (2021) 218–229.
[6] S. Wang, H. Wang, P. Perdikaris, Learning the solution operator of parametric partial
differential equations with physics-informed deeponets, Science advances 7 (40) (2021)
eabi8605.
[7] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equations from data
by sparse identification of nonlinear dynamical systems, Proceedings of the national
academy of sciences 113 (15) (2016) 3932–3937.
[8] Z. Chen, Y. Liu, H. Sun, Physics-informed learning of governing equations from scarce
data, Nature communications 12 (1) (2021) 1–13.
45[9] L. Sun, D. Z. Huang, H. Sun, J.-X. Wang, Bayesian spline learning for equation discovery
of nonlinear dynamics with quantified uncertainty, in: NeurIPS, PMLR, 2022.
[10] D. Kochkov, J. A. Smith, A. Alieva, Q. Wang, M. P. Brenner, S. Hoyer, Ma-
chine learning–accelerated computational fluid dynamics, Proceedings of the National
Academy of Sciences 118 (21) (2021) e2101784118.
[11] X.-Y. Liu, L. Lu, H. Sun, J.-X. Wang, Predicting parametric spatiotemporal dy-
namics by multi-resolution pde structure-preserved deep learning, arXiv preprint
arXiv:2205.03990 (Accepted by Communication Physics) (2022).
[12] X. Fan, J.-X. Wang, Differentiable hybrid neural modeling for fluid-structure interaction,
Journal of Computational Physics 496 (2024) 112584.
[13] D. Akhare, T. Luo, J.-X. Wang, Physics-integrated neural differentiable (PiNDiff) model
for composites manufacturing, Computer Methods in Applied Mechanics and Engineer-
ing 406 (2023) 115902.
[14] J. Tompson, K. Schlachter, P. Sprechmann, K. Perlin, Accelerating eulerian fluid simu-
lation with convolutional networks, in: International Conference on Machine Learning,
PMLR, 2017, pp. 3424–3433.
[15] R. Vinuesa, S. L. Brunton, Enhancing computational fluid dynamics with machine
learning, Nature Computational Science 2 (6) (2022) 358–366.
[16] J. Wang, J. Wu, H. Xiao, A physics-informed machine learning approach of improving
rans predicted reynolds stresses, in: 55th AIAA aerospace sciences meeting, 2017, p.
1712.
[17] K. Duraisamy, G. Iaccarino, H. Xiao, Turbulence modeling in the age of data, Annual
Review of Fluid Mechanics 51 (2019) 357–377.
46[18] J.-X. Wang, J. Huang, L. Duan, H. Xiao, Prediction of reynolds stresses in high-mach-
number turbulent boundary layers using physics-informed machine learning, Theoretical
and Computational Fluid Dynamics 33 (1) (2019) 1–19.
[19] L. Zanna, T. Bolton, Data-driven equation discovery of ocean mesoscale closures, Geo-
physical Research Letters 47 (17) (2020) e2020GL088376.
[20] M. A. Mendez, A. Ianiro, B. R. Noack, S. L. Brunton, Data-Driven Fluid Mechanics:
Combining First Principles and Machine Learning, Cambridge University Press, 2023.
[21] J. Wu, H. Xiao, R. Sun, Q. Wang, Reynolds-averaged navier–stokes equations with
explicit data-driven reynolds stress closure can be ill-conditioned, Journal of Fluid Me-
chanics 869 (2019) 553–586.
[22] M. Innes, A. Edelman, K. Fischer, C. Rackauckas, E. Saba, V. B. Shah, W. Tebbutt, A
differentiable programming system to bridge machine learning and scientific computing,
arXiv preprint arXiv:1907.07587 (2019).
[23] F. D. A. Belbute-Peres, T. Economon, Z. Kolter, Combining differentiable pde solvers
and graph neural networks for fluid flow prediction, in: international conference on
machine learning, PMLR, 2020, pp. 2402–2411.
[24] D. Z. Huang, K. Xu, C. Farhat, E. Darve, Learning constitutive relations from indirect
observations using deep neural networks, Journal of Computational Physics 416 (2020)
109491.
[25] B. List, L.-W. Chen, N. Thuerey, Learned turbulence modelling with differentiable
fluid solvers: physics-based loss functions and optimisation horizons, Journal of Fluid
Mechanics 949 (2022) A25.
[26] R. T. Chen, Y. Rubanova, J. Bettencourt, D. K. Duvenaud, Neural ordinary differential
equations, Advances in neural information processing systems 31 (2018).
47[27] Z. Long, Y. Lu, X. Ma, B. Dong, Pde-net: Learning pdes from data, in: International
conference on machine learning, PMLR, 2018, pp. 3208–3216.
[28] J. Zhang, X. Tao, L. Yang, R. Wu, P. Sun, C. Wang, Z. Zheng, Forward imaging neural
network with correction of positional misalignment for fourier ptychographic microscopy,
Optics Express 28 (16) (2020) 23164–23175.
[29] M. Eliasof, E. Treister, Diffgcn: Graph convolutional networks via differential operators
and algebraic multigrid pooling, Advances in neural information processing systems 33
(2020) 18016–18027.
[30] M. Thorpe, T. M. Nguyen, H. Xia, T. Strohmer, A. Bertozzi, S. Osher, B. Wang,
Grand++: Graph neural diffusion with a source term, in: International Conference on
Learning Representations, 2021.
[31] A. F. Psaros, X. Meng, Z. Zou, L. Guo, G. E. Karniadakis, Uncertainty quantification
in scientific machine learning: Methods, metrics, and comparisons, Journal of Compu-
tational Physics 477 (2023) 111902.
[32] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh,
P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya, et al., A review of uncertainty quan-
tification in deep learning: Techniques, applications and challenges, Information Fusion
76 (2021) 243–297.
[33] D. M. Blei, A. Kucukelbir, J. D. McAuliffe, Variational inference: A review for statis-
ticians, Journal of the American statistical Association 112 (518) (2017) 859–877.
[34] M. Welling, Y. W. Teh, Bayesian learning via stochastic gradient langevin dynamics,
in: Proceedings of the 28th international conference on machine learning (ICML-11),
2011, pp. 681–688.
[35] R. Zhang, A. F. Cooper, C. De Sa, Amagold: Amortized metropolis adjustment for
efficient stochastic gradient mcmc, in: International Conference on Artificial Intelligence
and Statistics, PMLR, 2020, pp. 2142–2152.
48[36] C. Blundell, J. Cornebise, K. Kavukcuoglu, D. Wierstra, Weight uncertainty in neural
network, in: International conference on machine learning, PMLR, 2015, pp. 1613–1622.
[37] B. Lakshminarayanan, A. Pritzel, C. Blundell, Simple and scalable predictive uncer-
tainty estimation using deep ensembles, Advances in neural information processing sys-
tems 30 (2017).
[38] G. Huang, Y. Li, G. Pleiss, Z. Liu, J. E. Hopcroft, K. Q. Weinberger, Snapshot ensem-
bles: Train 1, get m for free, arXiv preprint arXiv:1704.00109 (2017).
[39] W. J. Maddox, P. Izmailov, T. Garipov, D. P. Vetrov, A. G. Wilson, A simple baseline
for bayesian uncertainty in deep learning, Advances in Neural Information Processing
Systems 32 (2019).
[40] S. Jantre, S. Madireddy, S. Bhattacharya, T. Maiti, P. Balaprakash, Sequential bayesian
neural subnetwork ensembles, arXiv preprint arXiv:2206.00794 (2022).
[41] E. A. Wan, R. Van Der Merwe, The unscented kalman filter for nonlinear estimation, in:
Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications,
and Control Symposium (Cat. No. 00EX373), Ieee, 2000, pp. 153–158.
[42] S. Mandt, M. D. Hoffman, D. M. Blei, Stochastic gradient descent as approximate
bayesian inference, arXiv preprint arXiv:1704.04289 (2017).
49