Generative Model-based Simulation of Driver
Behavior when Using Control Input Interface for
Teleoperated Driving in Unstructured Canyon
Terrains?
Hyeonggeun Yun1, Younggeol Cho2, Jinwon Lee1, Arim Ha1, and Jihyeok Yun1
1Agency for Defense Development, Daejeon, Republic of Korea
fyhg8423,leejw509,arimida86,jihyeok.yun g@gmail.com
2Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea
rangewing@kaist.ac.kr
Abstract. Unmanned ground vehicles (UGVs) in unstructured environ-
ments mostly operate through teleoperation. To enable stable teleoper-
ated driving in unstructured environments, some research has suggested
driver assistance and evaluation methods that involve user studies, which
can be costly and require lots of time and eort. A simulation model-
based approach has been proposed to complement the user study; how-
ever, the models on teleoperated driving do not account for unstructured
environments. Our proposed solution involves simulation models of tele-
operated driving for drivers that utilize a deep generative model. Ini-
tially, we build a teleoperated driving simulator to imitate unstructured
environments based on previous research and collect driving data from
drivers. Then, we design and implement the simulation models based on
a conditional variational autoencoder (CVAE). Our evaluation results
demonstrate that the proposed teleoperated driving model can gener-
ate data by simulating the driver appropriately in unstructured canyon
terrains.
Keywords: Teleoperated driving Â·Simulation model Â·Generative model
Â·Driver behavior Â·Unstructured environment.
1 Introduction
Teleoperated driving of unmanned ground vehicles (UGVs) is a prominent exam-
ple of teleoperation in unstructured environments. One of the diculties in tele-
operated driving is stability, which is driving while sustaining consistent states
in response to abrupt changes in the local environment [12]. In unstructured
environments, ensuring stable driving is essential to reduce collision risks and
increase vehicle safety. However, teleoperation is more complex than commer-
cial driving, as drivers lack physical feedback from the vehicle regarding speed,
?This work was supported by Agency for Defense Development Grant funded by the
Korean Government (2023).arXiv:2305.09874v1  [cs.RO]  17 May 20232 H. Yun et al.
acceleration, and posture [13]. Furthermore, the diculty of teleoperated driv-
ing increases in unstructured environments since roads are not standardized,
and there are diverse elements, such as canyons, obstacles, and potholes. Con-
sequently, teleoperated driving in unstructured environments requires extensive
practice and is dicult for drivers.
To support teleoperated driving, previous research proposed driver assis-
tance methods for collision avoidance and stable driving [1,16]. User studies
and human-in-the-loop experiments have been used to evaluate driver assistance
methods. However, conducting studies with drivers requires a huge eort and
time, making repeated or iterative experiments challenging. Human-in-the-loop
experiments and teleoperated driving with drivers, in particular, are high-cost.
Thus, an evaluation approach based on a simulation model of users is proposed
to complement the user study [8].
Previously, modeling and simulating of drivers have been studied in terms
of driving behavior. Wang et al. classied and clustered driving behaviors us-
ing a machine learning-based approach for personalized compensation for speed
tracking errors [17]. Li et al. proposed a human driving model that predicts the
steering behavior of drivers in the teleoperation of UGV with dierent speeds
[5]. They used ACT-R cognitive architecture, a two-point steering model, and a
far-point control model to simulate human steering behavior. Schnelle et al. also
suggested a driver steering model which can adapt a driving behavior of an indi-
vidual driver [14]. Existing research has primarily centered on the development
of simulation models for driving behaviors on urban roads, such as lane changing
and vehicle following, using rule-based approaches. Unfortunately, these models
have a limited capacity to reect the driving behavior of multiple drivers, and
their applicability to mimicking teleoperated driving of drivers in unstructured
environments is also restricted.
In this paper, we solve this problem using a self-supervised generative model.
Based on previous research [9], we propose a simulation model of teleoperated
driving commands based on a conditional variational autoencoder (CVAE) in
unstructured environments, particularly unstructured canyon terrains. First, we
collected driving data from drivers using a teleoperated driving simulator, includ-
ing control inputs, dynamic state information of a platform, and environmental
information. The data is then used to train and validate our simulation models.
Second, we design a forward simulation model and an inverse simulation model
for teleoperated driving based on CVAE. CVAE is a deep conditional generative
model that produces stochastic inferences based on the inputs and generates
realistic output predictions [15]. We rst train the forward simulation model to
generate environmental information and dynamic state information, then train
the inverse simulation model to generate control inputs for drivers.
2 Teleoperated Driving Dataset Collection
Designing a teleoperated driving model requires driving data from drivers in
unstructured environments. Thus, we employed an o-road teleoperated drivingGenerative Model-based Simulation of Teleoperated Driver Behavior 3
Control 
Input 
Interface Controller 
Interface Data 
Collection Control 
Inputs (modified)
Control 
Inputs 
Control 
Inputs 
Environmental 
Information 
Dynamic 
States Control Inputs Environmental 
Information Dynamic States Game Engine-based Simulator 
Teleoperated driving 
Interface Teleoperated driving 
Platform 
Fig. 1. Game engine-based simulator conguration with input interface.
simulator that could reect real-world physical eects from previous work to
simulate teleoperated driving in the unstructured environment [1] (See Figure
1). The simulator is implemented using the Unity Pro game engine and Vehicle
Physics Pro automotive simulation engine. The simulator provided a truck-like
wheeled platform with a LiDAR sensor that replicates Velodyne 16-channel 3D
LiDAR and driving interfaces including a speedometer and a tachometer. Drivers
used a Logitech G29 racing wheel as a control input interface.
To simulate the teleoperated driving, we collected the following simulator
data: control inputs, dynamic state information, and environmental information.
The control inputs consisted of steering data and pedal data from the controller.
The dynamic state information consisted of the current speed, orientation (yaw,
roll, pitch), and current position of the platform in the simulator. The environ-
mental information in this study refers to LiDAR Point Cloud data captured by
the LiDAR sensor. The simulator stores all data every 100 milliseconds because
we contemplated a generation model that utilizes environmental data.
To collect driving data, we utilized ve terrains that imitated canyons and
rough terrains (See Figure 2). As we concentrated on unstructured environments
and canyons, the terrains' pathways were unpaved, laneless, and surrounded by
steep mountains.
We collected driving data from 5 drivers (experienced drivers) with experi-
ence using the simulator and 14 drivers (inexperienced drivers) with no experi-
ence using the simulator. The experienced drivers continuously collected driving
data from three terrains (Figure 2.a, b, c). The inexperienced drivers operated
the wheeled platform on the two terrains (Figure 2.d, e) to collect driving data.
We used a driving dataset of experienced drivers for the forward simulation
model and a dataset of inexperienced drivers for the inverse simulation model.
3 TELEOPERATED DRIVING MODEL
We implemented a teleoperated driving model based on the driving data of
drivers. Our objective is to simulate the teleoperated driving of drivers so that we
can readily measure and predict the teleoperated driving performance without4 H. Yun et al.
(d) Terrain 4 (b) Terrain 2 (a) Terrain 1 (c) Terrain 3 
(e) Terrain 5 
Fig. 2. Five terrains (maps) include both curved and linear pathways for data collection
and model evaluation. Blue circle denotes the starting point of each terrain.
drivers participating in teleoperated driving experiments directly. To achieve
the goal, we considered a simulation model based on a deep generative model
adopting a conditional variational autoencoder (CVAE). A previous study has
demonstrated that a CVAE is useful at simulating physical movements based
on sensor inputs and generating various outputs from given conditions [9]. We
considered that a CVAE-based model could generate control inputs for multiple
drivers by utilizing the relationship between current and past data.
Therefore, we designed a teleoperated driving simulation model based on a
CVAE that can mimic the teleoperated driving behavior of drivers. Since we
used the training and inference framework of previous research [9], we designed
a forward simulation model and an inverse simulation model with a CVAE.
The forward model uses the previous data from the nine-time steps and the
control inputs from the current time step to generate the environmental data
and dynamic state data for the current time step, and the inverse model uses
the previous data from the nine-time steps and the environmental data and
dynamic state data from the current time step to generate the control inputs for
the current time step.
3.1 Preprocessing
We preprocessed the collected dataset in accordance with prior research in or-
der to enhance the eectiveness of training and quality of data [1,2,6,10]. InGenerative Model-based Simulation of Teleoperated Driver Behavior 5
preprocessing, the dataset was divided into control inputs consisting of steering
input and acceleration/brake pedal input, dynamic state information consisting
of orientation data and speed data, and environmental information consisting of
LiDAR Point Cloud data. For each ithtime step, we created a Control Vector
ci, a State Vector si, and an Environment Vector eithrough preprocessing.
First, by normalizing the steering input data, acceleration pedal input data,
and brake pedal input data, we generate a Control Vector cifor the control inputs
[10]. The steering input data was normalized to a range of 0-1. The acceleration
pedal input data was scaled to a range of 0-1, while the brake pedal input data
was scaled to -1-0. They were then combined as the pedal input data, which was
subsequently normalized from -1-1 to 0-1.
We create a State Vector sifor the dynamic state information of the platform
by normalizing the current speed and orientation data. The orientation data
was normalized to a range of 0-1 from 0-360 degrees, and the speed data was
normalized to a range of 0-1 from 0-30 m/s, because the current speed of data
did not exceed 30 m/s.
In the environmental information, an Environment Vector eiwas constructed
using LiDAR Point Cloud data. The LiDAR Point Cloud data was converted to
cylindrical coordinates and obstacle points were identied by detecting points
with a slope greater than the threshold (which was set to 45 degrees for this pa-
per) along the same azimuth [2,6]. The obstacle point data was then transformed
into a vector depicting the distance between the obstacle and the platform along
the forward 180 degrees. The distance values were then normalized to a range
of 0-1 on the assumption that the utmost distance was 50 meters.
3.2 Training and Inference Approach with Two Simulation Models
As we mentioned above, we trained two simulation models: a forward simulation
model and an inverse simulation model. The forward model generates observed
data derived from user interactions, while the inverse model generates user inter-
actions derived from observed data [7,9]. Generally, the forward model is derived
from principles and theories based on psychology and physics, and the inverse
model is derived from a machine learning-based model based on users' behavior
data. However, in our proposed model, we also used a machine learning-based
model for the forward model since we thought that the observed sensor data
could be derived from the control inputs and sensor data of previous time steps.
Thus, we designed two CVAE-based simulation models. The training and
inference approach with two simulation models consisted of three steps.
(1) First, we create a Perception Vector by combining an Environment Vector
and State. We utilize the Perception Vector and Control Vector of the pre-
vious nine-time steps and the Control Vector of the current time step as a
condition vector for the CVAE, and use a Noise Vector of the same size as
the Perception Vector of the current time step as input. The forward CVAE
model generates the Perception vector corresponding to the current time6 H. Yun et al.
step. To generate accurate perception vectors, we use a dataset of 102,073
time steps generated by the experienced drivers for training (Figure 3.a).
(2) Second, we use the Perception Vector generated by the forward CVAE model
used in step (1) and the Perception Vector and Control Vector from the
previous nine-time steps as a condition vector, and a Noise Vector of the
same size as the Control Vector at the current time step as input. The
Control Vector at the current time step is generated by the inverse CVAE
model. To generate diverse drivers' behavior, we use a dataset of 88,955 time
steps generated from inexperienced drivers for training (Figure 3.b).
(3) Third, we utilize the inverse CVAE model trained in step (2) to generate a
Control Vector for the current time step from the Perception Vectors and
Control Vectors of the previous nine-time steps and the Perception Vector
for the current time step. The inverse CVAE model simulates the control
inputs of drivers represented by the generated control vector (Figure 3.c).
Perception 
Vector 
(9 x 454) CVAE 
(Forward Model) 
Environment 
Vector 
(10 x 450) State Vector 
(10 x 4) Control Vector 
(10 x 2) 
CVAE 
(Forward Model) Environment 
Vector 
(9 x 450) State Vector 
(9 x 4) Control 
Vector 
(9 x 2)
Generated 
Perception Vector 
(1 x 454) Environment 
Vector 
(1 x 450) State Vector 
(1 x 4) Generated Perception Vector 
Noise 
Vector 
(1 x 2) CVAE 
(Inverse Model) Data from 
previous 9 time steps 
Generated 
Control 
Vector 
(1 x 2)Generated Control Vector 
CVAE 
(Inverse Model) Generated 
Control 
Vector 
(1 x 2)Generated Control Vector 
Environment 
Vector 
(10 x 450) State Vector 
(10 x 4) Control 
Vector 
(9 x 2)
Noise 
Vector 
(1 x 2) Compare for training 
at current time step (a) 
(b) 
(c) Noise Vector 
(1 x 454) Control Vector 
(10 x 2) 
Fig. 3. Training and inference approach with a forward simulation model and an inverse
simulation model.
We used time-series data from ten consecutive time steps, including the cur-
rent time step and the nine-time steps preceding it. The condition vector was
constructed by combining the State Vector, Environment Vector, and Control
Vector. We also used a random vector as a noise vector ~ n, and the sample vector
zwas set to the same size as the noise vector. The dataset was divided into a
training dataset for updating the model's parameters and a validation datasetGenerative Model-based Simulation of Teleoperated Driver Behavior 7
for verifying the model's performance. The model was trained for 1,000 epochs
using a batch size of 2,048. The initial learning rate was set to 1 10 3, and it
was decreased to a tenth every 300 epochs using a step decay scheduler in or-
der to enhance training performance. Mean Squared Error (MSE) function and
Kullback{Leibler (KL) divergence were used for loss functions, whereas VAE and
CVAE typically use Binary Cross Entropy (BCE) function and KL divergence
for loss functions. In addition, since it is crucial to restoring the vector at the
current time step based on previous information, we only compared a loss be-
tween generated vector at the current time step cgen;t for training. The training
was conducted on a GPU server equipped with the NVIDIA Tesla A100.
3.3 Model Architecture
Linear (GELU) LSTM (Tanh)Linear 
(GELU) Linear 
(GELU)
Linear (GELU) Î¼
ÏzLSTM (Tanh)Linear (Sigmoid) 
Linear (GELU) 
Linear (GELU) 
Linear (GELU) 
Condition 
Vector z (1 x N) Encoder 
Decoder 
Condition 
Vector noise vector 
(1 x N) Generated 
Vector 
(1 x N)
Fig. 4. CVAE-based teledriving model architecture for a forward simulation model and
an inverse simulation model. Nis the size of the generated vector to simulate.
The architecture of the CVAE-based teleoperated driving model is illustrated
in Figure 4. Our model comprises an encoder with four Linear layers and an
LSTM layer, as well as a decoder with four Linear layers and an LSTM layer.
In the encoder, we rst generate a Condition Vector q= (qt 9;qt 8;:::;q t) and
a Noise Vector ~ n2RNwheretrepresents the current time step and Nrepresents
the size of the generated vector to simulate. Then, the combination of Noise Vec-
tor ~nand Condition Vector qgenerates an input vector x= (qt 9;qt 8;:::;q t~n)
whererepresents concatenation. We rst extract time-series features xencfrom
xwith two Linear layers and an LSTM layer.
xenc=Linear enc;2(LSTM enc(Linear enc;1(x))) (1)
Then, two Linear layers generate a mean vector and a variance vector 
that are typically employed in variational autoencoder (VAE).8 H. Yun et al.
=Linear enc;3(xenc) (2)
=Linear enc;4(xenc) (3)
The model then generates a sample vector z= (zt 9;zt 8;:::;z t) from the
mean vector and variance vector .
z=+2 (4)
whereN(0;12) andis an element-wise product.
In the decoder, we only need zt, so theztvector and the Condition Vector q
are combined to generate the vector x0= (qt 9;qt 8;:::;q tzt).
Then, sequential features rdec;1are extracted from x0by an LSTM layer of
the decoder.
rdec;1=LSTM dec(x0) (5)
Finally, four Linear layers of the decoder then generate a Vector ggen=
(ggen;t 9;ggen;t 8;:::;g gen;t)2RN. We useggen;t vector to simulate state and
environmental information or control inputs corresponding to the current time
step.
rdec;2=Linear dec;3(Linear dec;2(Linear dec;1(rdec;1))) (6)
ggen=Linear dec;4(rdec;2) (7)
The activation function for Linear layers, excluding the last Linear layer of
the decoder, was the Gaussian error linear unit (GELU) function, while the
activation function for LSTM layers was the Tanh function. The last Linear
layer of the decoder utilized the sigmoid function as an activation function to
generate a vector in the range 0-1.
4 Model Evaluation
4.1 Evaluation Design
To evaluate the capacity of our CVAE-based teledriving model to simulate the
teleoperated driving behavior of drivers, we compared the driving performance
of the simulated model to that that of real drivers. We collected teleoperated
driving data of the simulation model from the canyon terrain (Figure 2.e) where
drivers drove. As a dataset of drivers' teleoperated driving consists of a total of
28 driving data operated by 14 drivers twice, the simulation model also drove
28 times in the same terrain. After arriving at the destination, driving data was
added to the dataset for purposes of comparison.Generative Model-based Simulation of Teleoperated Driver Behavior 9
To compare the driving performance of drivers to that of the simulation
model, we employed four metrics based on previous research [1,3,4,11]: the stan-
dard deviation of lateral position (SDLP), the standard deviation of speed (SDS),
the average speed (AvgSpeed), and the driving completion time (DCT). For all
metrics, lower scores mean superior driving performance.
4.2 Evaluation Result
â â¡â¢
â£
â¤
â¥â¦â§â¨
Fig. 5. Terrain divisions for drivers and simulation model performance comparison
Figures 6 and 7 illustrate comparison results of the driving performance of
the drivers and the simulation model, respectively. To establish a relationship
between the driving performance of drivers and the simulation model, the ex-
periment terrain was segmented into nine sections (See Figure 5). We examined
whether there was a correlation between the average driving performance of
drivers and the average driving performance of the simulation model for each of
the nine sections. Thus, we conducted a linear regression on the driving perfor-
mance data for the nine sections and measured the Pearson correlation coecient
(r). Figure 6 shows the relationship between drivers and the simulation model.
Figure 6.a shows that our simulation model is correlated with the drivers in
terms of SDLP ( r= 0:84). Figure 6.b shows that our simulation model is also
correlated with the drivers in terms of SDS ( r= 0:81). Figure 6.c and Figure
6.d demonstrate that our simulation model correlates with drivers in terms of
AvgSpeed and DCT ( r= 0:84 andr= 0:92). Based on the results, our simula-
tion model adequately reproduces the teleoperated driving of drivers (Pearson
correlation coecient r>= 0:8 for all metrics).
Figure 7 illustrates the mean and standard deviation of drivers' SDLP, SDS,
AvgSpeed, and DCT, along with the simulation model. We conducted a Welch's10 H. Yun et al.
(a) (b)
(c) (d)y = 0.80x + 4.68 
r = 0.92 y = 0.81x + 1.33 
r = 0.84 y = 0.75x + 0.41 
r = 0.81 y = 1.20x - 0.27 
r = 0.84 
Fig. 6. Nine divisions analysis of the relationship between drivers and simulation model
performance. A blue dot represents two driving performances in each division: the
drivers' and the simulation model's.
T-Test to determine whether a statistically signicant dierence existed. Figure
7.a, Figure 7.b, and Figure 7.c show that there was no signicant dierence be-
tween the drivers and the simulation model in SDLP ( p= 0:64), SDS (p= 0:06),
and AvgSpeed ( p= 0:10). In contrast, Figure 7.d demonstrates that DCT of
drivers was signicantly less than DCT of the simulation model ( p= 0:02).
Although the simulation model performed worse than drivers in DCT, the dif-
ference in DCT was 11 seconds. Therefore, we are able to argue that our model
adequately simulates drivers' teleoperated driving.
5 Conclusion
In this paper, we proposed a CVAE-based teleoperated driving model for un-
structured canyon terrains. We implemented a teleoperated driving simulator
for unstructured environments, enabling us to mimic teleoperated driving sce-
narios and collect driving data from drivers. Then, we designed a teleoperated
driving model based on a CVAE-based architecture. From time-series data, theGenerative Model-based Simulation of Teleoperated Driver Behavior 11
(a) (b) 
(c) (d) 
Fig. 7. Comparison of average performance of drivers and our simulation model. *:
p <0:05, **: p <0:005, ***: p <0:0005
forward simulation model and the inverse simulation model generates a percep-
tion vector and a control vector at the current time step. The evaluation results
demonstrated that our simulation model could reproduce drivers' teleoperated
driving. This study could be expanded in the future to include a variety of o-
road environments with low-friction roads, craters, and obstacles. If these studies
mature, we can anticipate that our proposed simulation model, which mimics
human behavior despite using a limited number of user studies, will bring about
in a novel approach for dataset generation and user study.
References
1. Cho, Y., Yun, H., Lee, J., Ha, A., Yun, J.: Goondae: Denoising-based driver as-
sistance for o-road teleoperation. IEEE Robotics and Automation Letters 8(4),
2405{2412 (2023). https://doi.org/10.1109/LRA.2023.3250008
2. Choe, T.S., Park, J.B., Joo, S.H., Park, Y.W.: Obstacle detection for unmanned
ground vehicle on uneven and dusty environment. In: Signal Processing, Sen-12 H. Yun et al.
sor/Information Fusion, and Target Recognition XXIV. vol. 9474, pp. 433{440.
SPIE (2015)
3. Knappe, G., Keinath, A., Bengler, K., Meinecke, C.: Driving simulator as an eval-
uation tool{assessment of the inuence of eld of view and secondary tasks on
lane keeping and steering performance. In: 20th International Technical Confer-
ence on the Enhanced Safety of Vehicles (ESV) National Highway Trac Safety
Administration. No. 07-0262 (2007)
4. Knapper, A., Christoph, M., Hagenzieker, M., Brookhuis, K.: Comparing a driving
simulator to the real road regarding distracted driving speed. European journal of
transport and infrastructure research 15(2) (2015)
5. Li, C., Tang, Y., Zheng, Y., Jayakumar, P., Ersal, T.: Modeling human steering
behavior in teleoperation of unmanned ground vehicles with varying speed. Human
factors 64(3), 589{600 (2022)
6. Li, N., Yu, X., Liu, X., Lu, C., Liang, Z., Su, B.: 3d-lidar based negative obstacle
detection in unstructured environment. In: Proceedings of the 2020 4th Interna-
tional Conference on Vision, Image and Signal Processing. pp. 1{6 (2020)
7. Moon, H.S., Oulasvirta, A., Lee, B.: Amortized inference with user simulations. In:
Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems.
pp. 1{20 (2023)
8. Murray-Smith, R., Oulasvirta, A., Howes, A., M uller, J., Ikkala, A., Bachinski,
M., Fleig, A., Fischer, F., Klar, M.: What simulation can do for hci research.
Interactions 29(6), 48{53 (2022)
9. Murray-Smith, R., Williamson, J.H., Ramsay, A., Tonolini, F., Rogers, S., Loriette,
A.: Forward and inverse models in hci: Physical simulation and deep learning for
inferring 3d nger pose. arXiv preprint arXiv:2109.03366 (2021)
10. Naranjo, J.E., Gonz alez, C., Garc a, R., De Pedro, T.: Cooperative throttle and
brake fuzzy control for acc + stop&go maneuvers. IEEE Transactions on Vehicular
Technology 56(4), 1623{1630 (2007)
11. Neumeier, S., Wintersberger, P., Frison, A.K., Becher, A., Facchi, C., Riener, A.:
Teleoperation: The holy grail to solve problems of automated driving? sure, but la-
tency matters. In: Proceedings of the 11th International Conference on Automotive
User Interfaces and Interactive Vehicular Applications. pp. 186{197 (2019)
12. Opiyo, S., Zhou, J., Mwangi, E., Kai, W., Sunusi, I.: A review on teleoperation of
mobile ground robots: Architecture and situation awareness. International Journal
of Control, Automation and Systems 19(3), 1384{1407 (2021)
13. Park, K., Youn, E., Kim, S., Kim, Y., Lee, G.: Design of acceleration feedback of
ugv using a stewart platform. In: 2021 21st International Conference on Control,
Automation and Systems (ICCAS). pp. 79{82. IEEE (2021)
14. Schnelle, S., Wang, J., Su, H., Jagacinski, R.: A driver steering model with per-
sonalized desired path generation. IEEE Transactions on Systems, Man, and Cy-
bernetics: Systems 47(1), 111{120 (2016)
15. Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep
conditional generative models. Advances in neural information processing systems
28(2015)
16. Storms, J., Chen, K., Tilbury, D.: A shared control method for obstacle avoidance
with mobile robots and its interaction with communication delay. The International
Journal of Robotics Research 36(5-7), 820{839 (2017)
17. Wang, Z., Liao, X., Wang, C., Oswald, D., Wu, G., Boriboonsomsin, K., Barth,
M.J., Han, K., Kim, B., Tiwari, P.: Driver behavior modeling using game engine
and real vehicle: A learning-based approach. IEEE Transactions on Intelligent Ve-
hicles 5(4), 738{749 (2020)