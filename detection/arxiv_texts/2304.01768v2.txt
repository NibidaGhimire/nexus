arXiv:2304.01768v2  [math.OC]  26 May 2023Convergence of alternating minimisation algorithms for
dictionary learning
Simon Ruetz simon.ruetz@uibk.ac.at
Karin Schnass karin.schnass@uibk.ac.at
Universit¨ at Innsbruck
Technikerstraße 13
6020 Innsbruck, Austria
Abstract
In this paper we derive suﬃcient conditions for the convergence of two popular
alternating minimisation algorithms for dictionary learning - the Metho d of Optimal
Directions (MOD) and Online Dictionary Learning (ODL), which can also be thought
of as approximative K-SVD. We show that given a well-behaved initialisa tion that is
either within distance at most 1 /log(K) to the generating dictionary or has a special
structure, ensuring that each element of the initialisation only point s to one generating
element, both algorithms will converge with geometric convergence rate to the gener-
ating dictionary. This is done even for data models with non-uniform d istributions on
the supports of the sparse coeﬃcients. These allow the appearan ce frequency of the
dictionary elements to vary heavily and thus model real data more c losely.
Keywords: dictionary learning; sparse matrix factorisation; sparse coding; M ethod of
Optimal Directions; MOD; Online Dictionary Learning; ODL; K-Singular Value Decompo-
sition; K-SVD; convergence; non-uniform support distribution; r ejective sampling
1. Introduction
The goal of dictionary learning is to ﬁnd compact data repres entations by factorising a
data matrix Y∈Rd×Ninto the product of a dictionary matrix Φ ∈Rd×Kwith normalised
columns and a sparse coeﬃcient matrix X∈RK×N
Y≈ΦXandXsparse. (1)
The sparsifying dictionary or sparse components of a data cl ass can be used to iden-
tify further structure in the data, [13], or simply be exploi ted for many data processing
tasks, such as signal restoration or compressed sensing, [2 0, 10, 7]. Following the seminal
Bayesian approach, [13], there exist several strategies an d algorithms to tackle the above
problem [13, 3, 12, 35, 17, 18, 19, 37, 33] and a growing number of theoretical results to
accompany them [14, 38, 4, 34, 15, 6, 5, 1, 2, 39, 40, 8, 36, 26, 2 3]. These fall mainly into
two categories - optimisation based approaches and graph cl ustering algorithms. While
graph clustering algorithms have stronger theoretical suc cess guarantees, especially in the
overcomplete case K > d, [2, 4], optimisation based approaches, in particular alte rnating
optimisation algorithms, are extremely successful and pop ular in practice. The common
starting point of three golden classics among these alterna ting optimisation algorithms,
1the Method of Optimal Directions (MOD), [12], the K-Singula r Value Decomposition (K-
SVD), [3] and the algorithm for Online Dictionary Learning ( ODL), [19], is the following
programme. Given the data matrix Y= (y1,...,yN) and a dictionary size Kﬁnd a dictio-
nary Ψ = (ψ1,...,ψ K) and sparse coeﬃcients X= (x1,...xN) that optimise
argminΨ,X/ba∇dblY−ΨX/ba∇dbl2
Fs.t.X∈Sand/ba∇dblψk/ba∇dbl2= 1 for all k. (2)
The setSenforces sparsity on the coeﬃcients matrix, for instance by allowing only Snon-
zero coeﬃcients per column, that is /ba∇dblxn/ba∇dbl0≤S. The norm constraint on the dictionary
elements, also called atoms, removes the scaling ambiguity between the dictionary and the
coeﬃcients. Note that there remains a sign ambiguity, so for any solution to the problem
above, we get 2Kequivalent solutions by ﬂipping the signs of the atoms and ad justingX
accordingly. Since the problem is not convex there might exi st even more local and/or
global minima and to ﬁnd a local minimum one cannot use gradie nt descent as the gradient
with respect to (Ψ ,X) does not have a closed form solution. This is why the three cl assic al-
gorithms from above employ alternating optimisation and al ternate between optimising the
coeﬃcients while keeping the dictionary ﬁxed and optimisin g the dictionary while keeping
the coeﬃcients ﬁxed. To update the coeﬃcients, one aims for i nstance to ﬁnd
ˆX= argminX/ba∇dblY−ΨX/ba∇dbl2
F= argminX/summationdisplay
n/ba∇dblyn−Ψxn/ba∇dbl2
2s.t./ba∇dblxn/ba∇dbl0≤S,(3)
which corresponds to Nsparse approximation problems. While solving these proble ms
exactly is NP-complete in general, [22, 41], and thus unfeas ible for large S, there exist
many eﬃcient routines, which have success guarantees under additional conditions and
perform well in practice, such as OMP, [25], used for MOD and K -SVD. LARS, [11], used
for ODL solves a related problem. In this paper we consider si mple thresholding, which is
on par with computationally more involved algorithms like O MP in a dictionary learning
context, [24], and still relatively easy to analyse theoret ically.
Conversely, to update the dictionary for ﬁxed coeﬃcients ˆXone aims to ﬁnd
argminΨ/ba∇dblY−ΨˆX/ba∇dbl2
F=:f(Ψ) s.t./ba∇dblψk/ba∇dbl2= 1. (4)
The approach leading to the MOD update, [12], is to give up the unit norm constraint
on the dictionary elements in (4). The modiﬁed problem then h as a closed form solution,
argminΨf(Ψ) =YˆX†, so one can simply update the dictionary as YˆX†D, whereDis a
diagonal matrix ensuring that each atom has unit norm.
Another idea is to solve (4) approximately with one step of pr ojected block coordinate
descent. So we ﬁrst calculate the gradient of fwith respect to Ψ resulting in
∇Ψf(Ψ) =−/summationdisplay
(yn−Ψˆxn)ˆx∗
n=−YˆX∗+ΨˆXˆX∗.
We then choose adapted step sizes for each dictionary elemen t stored in the diagonal matrix
Λ and – with Dagain a diagonal matrix ensuring normalisation – update the dictionary as
/bracketleftig
Ψ+(YˆX∗−ΨˆXˆX∗)·Λ/bracketrightig
·D=/bracketleftig
ΨΛ−1−YˆX∗+ΨˆXˆX∗/bracketrightig
·ΛD. (5)
2Algorithm 1.1: MOD and ODL (aK-SVD) - one iteration
Input : Ψ,Y,S,κ(withκ>1)
SetˆX= (ˆx1,...,ˆxN) = 0
foreachndo
ˆIn= argmaxI:|I|=S/ba∇dblΨ∗
Iyn/ba∇dbl1 // thresholding
ˆxn(ˆIn)←Ψ†
ˆInyn // coefficient estimation
if/ba∇dblˆxn/ba∇dbl2≥κ/ba∇dblyn/ba∇dbl2then
ˆxn←0 // set pathological estimates to zero
end
end
Ψ←/braceleftigg
YˆX†
YˆX∗−ΨˆXˆX∗+Ψdiag( ˆXˆX∗)// MOD update
// ODL (aK-SVD) update
Ψ←(ψ1//ba∇dblψ1/ba∇dbl2,...,ψ K//ba∇dblψK/ba∇dbl2) // atom normalisation
Output: Ψ
The particular choice Λ−1= diag(ˆXˆX∗) leads to the dictionary update in ODL, [20].
Finally, for K-SVD the update of the k-th atom and corresponding coeﬃcients can be de-
rived from (4) or rather (2) by ﬁxingΨ ,ˆXexcept for the k-th atomψkandits corresponding
locations (but not values) of non-zero entries in the k-th row of ˆXand optimising for both
using a singular value decomposition. For more details on th e update we refer to [3]. How-
ever, we want to point out that K-SVD and ODL can be seen as clos ely related. Indeed
if we take an approximate version of K-SVD, known as aK-SVD, [ 27], where the SVD is
replaced with a 1 step power iteration, and additionally ski p the coeﬃcient update during
the dictionary update, we arrive again at ODL, see [29] for mo re details. To keep the close
link in mind we will refer to the dictionary update as in (5) as ODL (aK-SVD) update.
As already mentioned the disadvantage of the practically eﬃ cient, alternating optimisation
algorithms for dictionary learning is that they are less sup ported by theory. In particular,
when assuming that the data follows a random sparse model wit h an overcomplete gener-
ating dictionary Φ, there are no global recovery guarantees , in the sense that one of the
algorithms above or a variant will recover Φ with high probab ility.
Our Contribution In this paper we will cement the theoretical foundations of d ictionary
learningviaalternatingminimisation by characterising t heconvergence basinforbothMOD
and ODL (aK-SVD) in combination with thresholding as sparse approximation algorithm
as summarised in Algorithm 1. In particular we will show that under mild conditions on the
generating dictionary, any initialisation within atom-wi seℓ2-norm and scaled operator norm
distanceoforder1 /logKwillconvergeexponentiallyclosetothegeneratingdictio narygiven
enough training signals. Moreover, we provide further cond itions on an initialisation that
ensure convergence even for distances close to the maximal d istance√
2.
In case of ODL (aK-SVD) this is the ﬁrst result of this kind. Fo r MOD a convergence
radius of order 1 /S2was derived in [1], however, our results show that even under milder
assumptions the basin of convergence is several orders of ma gnitude larger. Finally, our
most important contribution is that we provide the ﬁrst theo retical results which are valid
3for data generating models where the non-zero entries of the generating coeﬃcients are not
essentially uniformly distributed. This means that some di ctionary elements can be more
likely to appear in the data representation than others. Suc h a non-homogeneous use of
dictionary elements can usually be observed in dictionarie s learned on image or audio data.
Our results, proving the stability and convergence of alter nating minimisation methods for
non-uniform sparse support models, might be an explanation for their practical advantages
over graph clustering or element-wise dictionary learning methods.
Organisation The remainder of the paper is organised as follows. After int roducing the
necessary notation in Section 2 we will deﬁne our random sign al model and give some
intuition why MOD and ODL should converge in Section 3. We the n provide our main
result together with some explanations and connections to o ther work and the proof of the
main theorem in Section 4. However, to make the proof more acc essible, the four lemmas
it relies on are deferred to Appendix A and further technical prerequisites to Appendix B.
We conclude with a discussion of our results and an outline of future work in Section 5.
2. Notation and setting
The notation and setting follows [32, 29] very closely. Let A∈Rd×KandB∈RK,m.
LetAkandAkdenote the k-th column and k-th row ofArespectively and A∗denote the
transpose of the matrix A. For 1≤p,q,r≤∞we set/ba∇dblA/ba∇dblp,q:= max /bardblx/bardblq=1/ba∇dblAx/ba∇dblp. Recall
that/ba∇dblAB/ba∇dblp,q≤ /ba∇dblA/ba∇dblq,r/ba∇dblB/ba∇dblr,pand/ba∇dblAx/ba∇dblq≤ /ba∇dblA/ba∇dblq,p/ba∇dblx/ba∇dblp. Often encountered quantities
are/ba∇dblA/ba∇dbl2,1= max k∈{1,...,K}/ba∇dblAk/ba∇dbl2and/ba∇dblA/ba∇dbl∞,2= max k∈{1,...,d}/ba∇dblAk/ba∇dbl2,which denote the
maximalℓ2norm of a column resp. row of A. For ease of notation we sometimes write
/ba∇dblA/ba∇dbl=/ba∇dblA/ba∇dbl2,2for the largest absolute singular value of A. For a vector v∈Rd, we denote
byv:= min i|vi|the smallest absolute value of vand/ba∇dblv/ba∇dbl∞the maximal entry of v. For
a subsetI⊆K:={1,...,K}, called the support, we denote by AIthe submatrix with
columns indexed by IandAI,Ithe submatrix with rows and columns indexed by I. We
further set RI:= (II)∗∈R|I|×K, allowing us to write AI=AR∗
I. This also allows us to
embed a matrix AI∈Rd×SintoRd×Kby zero-padding via AIRI∈Rd×K. We use the
convention that subscripts take precedent over transposin g, e.g.A∗
I= (AI)∗. We denote by
1I∈RKthe vector, whose entries indexed by Iare 1 and zero else. Further, for any vector
vwe denote by Dvresp. diag(v) the diagonal matrix with von the diagonal and abbreviate
Dv·w:=Dv·Dw. Finally we write ⊙for the Hadamard Product (or pointwise product) of
two matrices/vectors of the same dimension.
Throughout this paper we will denote by Φ ∈Rd×Kthe generating dictionary, i.e. the
ground truth we want to recover, and by Ψ ∈Rd×Kthe current guess. Wlog we will assume
that the columns of Ψ are signed in way that the vector α∈RKdeﬁned via αi:=/an}b∇acketle{tφi,ψi/an}b∇acket∇i}ht
has only positive entries. We will denote the ℓ2-distance between dictionary elements by
ε(Ψ,Φ) :=/ba∇dblΨ−Φ/ba∇dbl2,1= max
i/ba∇dblψi−φi/ba∇dbl2⇔ε(Ψ,Φ)2= 2−2α. (6)
If it is clear from context, we will sometimes write εinstead ofε(Ψ,Φ). An important
variable which will be used frequently throughout this pape r isZ:= Ψ−Φ — the diﬀerence
matrix between the generating dictionary Φ and a guess Ψ. We d eﬁne the distance between
4Φ and Ψ as
δ(Ψ,Φ) := max/braceleftig
/ba∇dbl(Ψ−Φ)D√π/ba∇dbl,/ba∇dblΨ−Φ/ba∇dbl2,1/bracerightig
= max/braceleftig
/ba∇dblZD√π/ba∇dbl,/ba∇dblZ/ba∇dbl2,1/bracerightig
.(7)
This might not seem intuitive at ﬁrst glance, but to show conv ergence we have to control
the weighted operator norm of the diﬀerence matrix as well as t heℓ2-distance. Again, if it
is clear from context we will simply write δ.
3. Probabilistic model
As was noted in the introduction we want the locations of the n on-zero coeﬃcients to follow
a non-uniform distribution, allowing some dictionary elem ents to be picked more frequently
than others.
Deﬁnition 1 (Poisson and rejective sampling) Letδkdenote a sequence of Kinde-
pendent Bernoulli 0-1random variables with expectations 0≤pk≤1such that/summationtextK
k=1pk=S
and denote by PBthe probability measure of the corresponding Poisson sampl ing model. We
say the support Ifollows the Poisson sampling model, if I:={k|δk= 1}and each support
I⊆Kis chosen with probability
PB(I) =/productdisplay
i∈Ipi/productdisplay
j/∈I(1−pj). (8)
We say our support Ifollows the rejective sampling model, if each support I⊆Kis chosen
with probability
PS(I) :=PB(I||I|=S). (9)
If it is clear from the context, we write P(I)instead of PS(I).
The Poisson sampling model would be quite convenient since t he probability of one atom
appearing in the support is independent of the others. Unfor tunately, we need a model with
exactly S-sparse supports, rather than supports that are S- sparse on average. The rejective
model satisﬁes this second condition and still yields almos t independent atoms, in the sense
that we can often reduce estimates for rejective sampling to estimates for Poisson sampling,
[32]. Note also that unless we arein the uniformcase, where pi=S/Kfor alli, theinclusion
probabilities πi:=P(i∈I) are diﬀerent from the parameters pi. They can be related using
[32][Lemma 1], which we restate for convenience in Appendix B.1. We frequently use the
square diagonal matrix D√π:= diag((√πk)k). Based on the rejective sampling model, we
deﬁne the following model for our signals.
Deﬁnition 2 (Signal model) Given a generating dictionary Φ∈Rd×Kconsisting of K
normalized atoms, we model our signals as
y= ΦIxI=/summationdisplay
i∈Iφixi, xi=ciσi, (10)
where the support I={i1,...iS}⊆Kis chosen according to the rejective sampling model (9)
with parameters p1,...,pKsuch that/summationtextK
i=1pi=Sand0<pk≤1/6, the coeﬃcient sequence
5c= (ci)i∈RKconsists of i.i.d. bounded random variables ciwith0≤cmin≤ci≤cmax≤1
and the sign sequence σ∈{−1,1}Kis a Rademacher sequence, i.e. its entries σiare i.i.d
withP(σi=±1) = 1/2. Supports, coeﬃcients and signs are modeled as independent and we
can writex=1I⊙c⊙σ.
The assumption pi≤1/6 ensures that the piand the corresponding inclusion probabilities
of the rejective sampling model πiare not too diﬀerent (see Theorem 15). We further
introduce the vector β∈RKviaβi:=E[c2
i] and denote by Dβthe corresponding diagonal
matrix. To see how this signal model allows us to prove conver gence of the algorithms, note
that before normalisation, by using Y= ΦX, we can write the two dictionary update steps
concisely as
MOD:YˆX∗(ˆXˆX∗)−1= ΦXˆX∗(ˆXˆX∗)−1,
ODL:1
N/bracketleftig
YˆX∗−ΨˆXˆX∗+Ψdiag( ˆXˆX∗)/bracketrightig
=1
N/bracketleftig
ΦXˆX∗−ΨˆXˆX∗+Ψdiag( ˆXˆX∗)/bracketrightig
,
where we scaled the update step of the ODL algorithm by a facto r of 1/N. The key is to
show that both these update steps concentrate around Φ. To th at end we deﬁne the two
averages of random matrices
A:=1
NXˆX∗=1
NN/summationdisplay
n=1xnˆx∗
nandB:=1
NˆXˆX∗=1
NN/summationdisplay
n=1ˆxnˆx∗
n. (11)
With these we can write the update step of MOD as Φ AB−1and that of ODL as Φ A−
Ψ[B−diag(B)]. We ﬁrst take a closer look at the terms within the sums abov e, where for
simplicity we drop the index n. Assuming that thresholding ﬁnds the correct support I, we
can writexˆx∗using the zero-padding operator R∗
Ias
xˆx∗=x(R∗
IΨ†
Iy)∗=xx∗Φ∗(Ψ†
I)∗RI,
Further assuming that Ψ Iis well conditioned, meaning Ψ∗
IΨI≈I, we can approximate
Ψ†
I≈Ψ∗
I, leading to
xˆx∗≈xx∗Φ∗ΨIRI=xx∗Φ∗ΨR∗
IRI=xx∗Φ∗Ψdiag(1I).
As we modelled the generating coeﬃcients as x=c⊙σ⊙11, using the independence of
c,σ,Iwe get that in expectation over c,σ,
Ec,σ[xx∗] =Ec[cc∗]⊙Eσ[σσ∗]⊙(1I1∗
I) =Dβdiag(1I) = diag( 1I)Dβ.
So the empirical estimator A=1
N/summationtextN
n=1xnˆx∗
n≈E[xˆx] will be well approximated by
E[xˆx∗]≈Ediag(1I)DβΦ∗Ψdiag(1I) = (DβΦ∗Ψ)⊙E[1I1∗
I].
The matrix E[1I1∗
I] simply stores as ij-th entry how often {i,j}⊆I, meaning the diagonal
entries are far larger than the oﬀ-diagonal ones, and we can ap proximate E[1I1∗
I]≈Dπ+
ππ∗≈Dπ. Finally using that Dα= diag(Φ∗Ψ) a similar analysis for Byields
A≈E[xˆx∗]≈(DβΦ∗Ψ)⊙Dπ=Dπ·α·βandB≈E[ˆxˆx∗]≈Dπ·α2·β.
6So before normalisation the updates via MOD and ODL should be approximately
ΦAB−1≈ΦD−1
αand ΦA−Ψ[B−diag(B)]≈ΦDπ·α·β. (12)
This means that the output of both dictionary update steps af ter normalisation should be
very close to the ground truth, and the proof boils down to qua ntifying the error in the
approximation steps outlined above.
4. Main result
Concretely, we will prove the following theorem.
Theorem 3 Assume our signals follow the signal model in 2. Deﬁne
α:= min
k|/an}b∇acketle{tψk,φk/an}b∇acket∇i}ht|= 1−ε2/2, γ:=cmin
cmaxandρ:= 2κ2S2γ−2α−2π−3/2,
withκ2≥2and letC,nbe two universal constants, where Cis no larger than 42andnno
larger than 130. Denote by δ⋆the desired recovery accuracy and assume δ⋆log(nKρ/δ ⋆)≤
γ/C. We abbreviate ν= 1//radicalbig
log(nKρ/δ ⋆)<1. If the atom-wise distance ε=ε(Ψ,Φ)of
the current guess Ψto the generating dictionary Φsatisﬁes
max/braceleftbig
ν/ba∇dblΦD√π/ba∇dbl,µ(Φ)/bracerightbig
≤/parenleftbigg
1−ε2
2/parenrightbigg
·γ
4Clog(nKρ/δ ⋆)(13)
and the current guess Ψadditionally satisﬁes either
max/braceleftbig
ν/ba∇dblΨD√π/ba∇dbl,µ(Ψ),µ(Ψ,Φ)/bracerightbig
≤/parenleftbigg
1−ε2
2/parenrightbigg
·γ
4Clog(nKρ/δ ⋆)(14)
or δ(Ψ,Φ)≤γ
Clog(nKρ/δ ⋆)=:δ◦, (15)
then the updated and normalised dictionary ˆΨ, which is output by ODL or MOD, satisﬁes
δ(ˆΨ,Φ)≤1
2·/parenleftbig
δ⋆/2+min{δ◦,δ(Ψ,Φ)}/parenrightbig
=:1
2·∆, (16)
except with probability
60Kexp/parenleftbigg
−N(∆/16)2
2ρ2+ρ∆/16/parenrightbigg
. (17)
To make the theorem more accessible, we now provide a detaile d discussion of the implica-
tions and assumptions.
Convergence First note that a repeated application of the theorem proves convergence
of MOD and ODL (aK-SVD) up to accuracy δ⋆, assuming a generating dictionary and an
initialisation that satisﬁes the conditions above and give n a new batch of training signal
in each iteration of size which scales at worst as N≈logKρ2/δ2
⋆. To see this, note that
the theorem ensures that the distance between the generatin g dictionary and the output
7dictionary decreases fast enough for condition (15) to be sa tisﬁed after one step. Once an
input dictionary satisﬁes δ⋆≤δ(Ψ,Φ)≤δ◦each iteration will w.h.p. shrink the distance
by a factor η≤3/4, since we have
δ(ˆΨ,Φ)≤1
4·δ⋆+1
2·δ(Ψ,Φ)≤3
4·δ(Ψ,Φ).
As the conditions on the generating dictionary get easier to satisfy in each step, we can
iterate this argument to get convergence.
Finally, if an input dictionary already satisﬁes δ(Ψ,Φ)≤δ⋆, that is, we reached the desired
precision, the contraction property (16) guarantees that t he output dictionary still satisﬁes
δ(ˆΨ,Φ)≤3
4δ⋆≤δ⋆. This stability is due to the assumption that the target reco very
accuracy satisﬁes δ⋆log(nKρ/δ ⋆)≤γ/Cmeaningδ⋆≤δ◦. Going through the proof shows
that without this assumption, for instance because there ar e not enough training signals for
a higher accuracy available, the theorem above is still true and the target distance will be
achieved after one step. In this case however, since the outp ut dictionary might no longer
satisfy (14), yet not be close enough to satisfy (15), it is no t guaranteed that a further
iteration will preserve this distance.
Generating dictionary The ﬁrst condition in (13) contains as implicit conditions o n the
generating dictionary that
/ba∇dblΦD√π/ba∇dbl/lessorsimilar1√logKandµ(Φ)/lessorsimilar1
logK, (18)
whichtogether ensurethat Φis asensibledictionary. Itgua rantees that most randomsparse
supports are well-conditioned, that is /ba∇dblΦ∗
IΦI− /C1/ba∇dbl≤ϑ<1. This further means that most
signals generated by the model have a stable representation ,y= ΦIxIwith/ba∇dbly/ba∇dbl2≈/ba∇dblxI/ba∇dbl2,
andcouldbeidentiﬁedbyasparseapproximationalgorithmt hathasaccesstoΦ. Toseethat
the condition is quite mild, note that for uniformly distrib uted supports, pi=πi=S/K,
and Φ a unit norm tight frame, /ba∇dblΦ/ba∇dbl=K/d, we can rewrite it as S≤d/logK, which is for
instance a common requirement in compressed sensing.
Initialisation (input dictionary): Theconditionin(13)furtherlimitsthemaximalatom-
wisedistanceofaninitialisation. Assumingagainuniform lydistributedsupportsandatight
generating dictionary it corresponds to
ε(Ψ,Φ) = max
k/ba∇dblψk−φk/ba∇dbl/lessorsimilar/parenleftigg
2−2/radicalbigg
SlogK
d/parenrightigg1/2
Consideringthatthemaximaldistancebetweentwounitnorm vectors is√
2, thismeansthat
convergence is possible even from far away initialisations . However, to actually converge
we have additional requirements. The initialisation needs to be well behaved, which is
ensured by (14) via /ba∇dblΨD√π/ba∇dbl/lessorsimilar1√logKandµ(Ψ)/lessorsimilar1
logK, similar to (18) for the generating
dictionary. The more stringent and interesting requiremen t in (14) is the condition on the
cross-coherence µ(Ψ,Φ) or rather the cross-Gram matrix Ψ∗Φ. As the minimal entry on its
diagonal is α= (1−ε2/2) it translates to
max
i/\e}atio\slash=j|/an}b∇acketle{tψi,φj/an}b∇acket∇i}ht|·logK/lessorsimilarmin
k|/an}b∇acketle{tψk,φk/an}b∇acket∇i}ht|,
8or Ψ∗Φ being diagonally dominant. Intuitively, (13) and (14) mea n that the admissible
distance can be very close to√
2, as long as the initialisation is a well-behaved dictionar y
and no two estimated atoms point to the same generating atom, meaning it is clear to the
sparse approximation algorithm which estimated atom belon gs to which generating atom.
While it might be possible to relax this separation conditio n, it is unlikely that it can be
removed in theovercomplete case. Indeed, following thegui delines in [23], onecan construct
well-behaved incoherent initialisations for which both MO D and ODL converge to a local
minimum that is not equivalent to the generating dictionary .
Finally, if the initialisation is within distance 1 /logKto the generating dictionary, meaning
condition (15) is satisﬁed, it is automatically well-behav ed and has a diagonally dominant
cross-Gram matrix, because it inherits these properties fr om the generating dictionary,
which by (13) is well behaved and incoherent.
Number of signals: Fromtheprobability boundin(17)weseethat inorderforthe failure
probability in each step to be small, the number of the fresh s ignals per iteration has to be
approximately
N≈ρ2
δ2⋆·logK≈1
π3δ2⋆·logK.
This ensures that even the most rarely appearing atoms are se en often enough to learn
them properly. The relation above reﬂects the general depen dencies, meaning we need
more training signals for higher accuracy and more imbalanc ed atom distributions, but is a
little too pessimistic. We expect that the scaling can be red uced toN≈logKδ−1
⋆π−3/2and
evenN≈KlogK/δ⋆in the uniform case, but leave the endeavour to those still mo tivated
after reading the current proof1.
Attainable accuracy: Wehavealreadyseenthatthetargetaccuracyshouldnotbech osen
toolargeinordertohavestableconvergenceandthatthenum beroftrainingsignalsrequired
in each iteration should grow with the desired accuracy. Ano ther interesting observation
is that even given an arbitrarily close initialisation and a rbitrarily many training signals,
the best attainable accuracy is limited by the coherence and conditioning of the generating
dictionary via (13) — even in the noiseless case considered h ere. The main reason is
that (13) is a very light condition which does not exclude the existence and rare selection
of sparse supports, leading to ill-conditioned or even rank -deﬁcient matrices Ψ I. Such
supports cannot be recovered by thresholding or any other sp arse approximation algorithm.
This failure probability stops the algorithm from attainin g arbitrarily small precision. If
we exchange (13) by the more restrictive assumption 2 Sµ(Φ)<1 then all supports of size
Sare well enough conditioned to be identiﬁed by a sparse algor ithm like OMP using Φ (or
some very small perturbation of it). This means that Φ is a ﬁxe d point of the algorithm
and that for a close enough initialisation we have convergen ce to Φ in expectation.
Comparison to existing results for MOD: Finally, we can compare our result with
that in [1] for MOD with an ℓ1-minimisation based sparse approximation routine. As-
suming a signal model with uniformly selected random suppor ts and a non-zero coeﬃcient
distribution, which is bounded from above, it is shown that f or an approximately tight dic-
tionary with coherence µ(Φ)/lessorsimilar1/√
dand a sparsity level S/lessorsimilard1/6MOD will converge from
1. Hints how to proceed can be found after the proof Lemma 10
9any initialisation satisfying ε(Ψ,Φ)/lessorsimilar1/S2, given enough training signals. In this special
case our assumptions on the generating dictionary — µ(Φ)/lessorsimilar1/logKandS/lessorsimilard/log(K)
— are more relaxed. As already seen, this comes at the price of a theoretical limit on the
achievable accuracy, which in practice, however, is determ ined by the number of available
signals. Further, we also have lighter conditions on the dis tance between the initialisation
and the generating dictionary. In particular, for a tight di ctionary our conditions can be
written as
ε(Ψ,Φ)/lessorsimilar1
logKand/ba∇dblΨ−Φ/ba∇dbl/lessorsimilar/radicaligg
K
Slog2K,
whereas the assumption in [1] that ε(Ψ,Φ)/lessorsimilar1/S2implies/ba∇dblΨ−Φ/ba∇dbl/lessorsimilar√
K/S2by using
/ba∇dblA/ba∇dbl≤/ba∇dblA/ba∇dblF≤√
K/ba∇dblA/ba∇dbl2,1. Hence the assumptions in [1] are more restrictive.
These restrictions seem due to using ℓ1-minimisation for sparse approximation, which on
the other hand has the advantage that it does not require the d istribution of the non-zero
coeﬃcients tobeboundedaway fromzero. However, suchanass umptionisusedinthegraph
clustering algorithm suggested in [1] to get an initialisat ion, that satisﬁes the convergence
conditions. This indicates that such a condition is necessa ry to get a larger convergence
area.
Proof[Proof of Theorem 3] We ﬁrst collect the results of Lemmas 7-1 0. Writing
T:= (D√π·α)−1B(D√π·α·β)−1andIℓc:=I−eℓe∗
ℓ (19)
for convenience, we get that except with failure probabilit y as in (17) we have
/ba∇dblΦA(D√π·α·β)−1−ΦD√π/ba∇dbl2,2≤α∆/8 and/ba∇dblT− /C1/ba∇dbl2,2≤∆/4,
as well as for all ℓ∈{1,···,K}
/ba∇dblΦA(Dπ·α·β)−1eℓ−φℓ/ba∇dbl2≤∆
8and max/braceleftigγαν
4C,/ba∇dblΨD√π/ba∇dbl/bracerightig
·/ba∇dblIℓcTeℓπ−1/2
ℓ/ba∇dbl2≤∆
8.
Using these 4 inequalities we show that for both algorithms a properly scaled version of
the updated dictionary, which we denote by ¯Ψ, contracts towards the generating dictionary.
Concretely, we show that for some constants sℓclose to 1
/ba∇dbl(¯Ψ−Φ)D√π/ba∇dbl≤∆/4 and max
ℓ/ba∇dbl¯ψℓ−sℓφℓ/ba∇dbl≤∆/3. (20)
Together these bounds guarantee that the normalised versio n of the updated dictionary ˆΨ
satisﬁesδ(ˆΨ,Φ)≤∆/2, meaning we have contraction towards the generating dicti onary in
the weighted operator norm and the maximum column norm simul taneously. We start with
the proof of ODL which is a bit simpler.
ODL: Motivated by 12, we deﬁne a scaled version of the updated dict ionary¯Ψ :=
[ΦA−ΨB+Ψdiag(B)](Dπ·α·β)−1,which ensures that on average ¯Ψ concentrates around
10Φ. The scaling does not change the underlying algorithm, sin ce we have a normalisation
step at the end of each iteration, which we will analyse after wards. We decompose ¯Ψ as
¯Ψ = ΦA(Dπ·α·β)−1−Ψ[B−diag(B)](Dπ·α·β)−1
= ΦA(Dπ·α·β)−1−ΨD√π·α/bracketleftbig
(D√π·α)−1B(Dπ·α·β)−1−D−1√π/bracketrightbig
+ΨD√π·α/bracketleftbig
(D√π·α)−1diag(B)(Dπ·α·β)−1−D−1√π/bracketrightbig
= ΦA(Dπ·α·β)−1−ΨD√π·α(T− /C1)D−1√π+ΨD√π·α(diag(T)− /C1)D−1√π.(21)
We ﬁrst show contraction in the weighted operator norm. Note that in both regimes we
have/ba∇dblΨD√π/ba∇dbl≤2/C, either by direct assumption or based on the bound
/ba∇dblΨD√π/ba∇dbl≤/ba∇dblΦD√π/ba∇dbl+/ba∇dbl(Ψ−Φ)D√π/ba∇dbl≤/ba∇dblΦD√π/ba∇dbl+δ≤/ba∇dblΦD√π/ba∇dbl+δ◦.
With the expression for the updated dictionary ¯Ψ in (21) and using the fact that /ba∇dblDα/ba∇dbl≤1
we can bound the operator norm of the diﬀerence ( ¯Ψ−Φ)D√πas
/ba∇dbl(¯Ψ−Φ)D√π/ba∇dbl≤/ba∇dblΦA(D√π·α·β)−1−ΦD√π/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤α∆/8 (7)+2/ba∇dblΨD√π·α/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤4/C·/ba∇dblT− /C1/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
·∆/4 (8)≤∆
4.
Next we show that for each atom of the scaled dictionary the ℓ2-distance also decreases
with each iteration. We access the ℓ-th dictionary atom ¯ψℓsimply by multiplying ¯Ψ with
the standard basis vector eℓ. This yields
¯ψℓ=¯Ψeℓ= ΦA(Dπ·α·β)−1eℓ−ΨD√π·α[T−diag(T)]D−1√πeℓ
= ΦA(Dπ·α·β)−1eℓ+ΨD√π·αIℓcTeℓπ−1/2
ℓ. (22)
Using this decomposition together with our second set of ine qualities we get
/ba∇dbl¯ψℓ−φℓ/ba∇dbl≤/ba∇dblΦA(Dπ·α·β)−1eℓ−φℓ/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤∆/8 (9)+/ba∇dblΨD√π·α/ba∇dbl·/ba∇dblIℓcTeℓπ−1/2
ℓ/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤∆/8 (10)≤∆
4,
which shows the second part in (20) for sℓ= 1. To ﬁnishthe proof we still need to show that
(20) guarantees contraction in the weighted operator norm a nd in the maximum column
norm after normalisation. However, we postpone the analysi s of the normalising step to
after the analysis of the MOD algorithm, since it is the same f or both algorithms.
MOD: Turning to the MOD algorithm we recall that if the estimated c oeﬃcient matrix
ˆXhas full row rank Kor equivalently ˆXˆX∗has full rank, which is guaranteed by the
second of our 4 inequalities, we can write the dictionary upd ate step before normalisation
as ΦXˆX∗/parenleftbigˆXˆX∗/parenrightbig−1= ΦAB−1. This dictionary update step — though conceptually very
easy — is harder to analyse theoretically due to the inverse o f the matrix B. Again we
will do the analysis for a scaled version of the updated dicti onary¯Ψ := ΦAB−1Dα≈Φ.
As for ODL we start by showing that the weighted operator norm of the diﬀerence ¯Ψ−Φ
contracts. We split ¯ΨD√πas follows
¯ΨD√π= ΦAB−1D√π·α
= ΦA(D√π·α·β)−1+ΦA(D√π·α·β)−1/parenleftig/bracketleftbig
(D√π·α)−1B(D√π·α·β)−1/bracketrightbig−1− /C1/parenrightig
= ΦA(D√π·α·β)−1+ΦA(D√π·α·β)−1/parenleftbig
T−1− /C1/parenrightbig
. (23)
11Since by Lemma 8 the matrix Tis close to the identity, /ba∇dblT− /C1/ba∇dbl≤∆/4, the Neumann series
for its inverse converges and we have T−1= [ /C1−( /C1−T)]−1=/summationtext
k≥0( /C1−T)k. Using the
geometric series formula we get for the operator norms of T−1,T−1− /C1
/ba∇dblT−1/ba∇dbl=/ba∇dbl/summationdisplay
k≥0( /C1−T)k/ba∇dbl≤/summationdisplay
k≥0/ba∇dbl /C1−T/ba∇dblk≤1
1−∆/4, (24)
/ba∇dblT−1− /C1/ba∇dbl=/ba∇dbl/summationdisplay
k≥1( /C1−T)k/ba∇dbl≤/summationdisplay
k≥1/ba∇dblT− /C1/ba∇dblk≤∆
4−∆≤∆
3, (25)
where for the last inequality we have used that ∆ ≤3
2·δ◦≤1/4≤1 sinceδ◦=γν2/Cand
ν≤1/3. Note also that by Lemma 7 and (13), we get
/ba∇dblΦA(D√π·α·β)−1/ba∇dbl≤/ba∇dblΦA(D√π·α·β)−1−ΦD√π+ΦD√π/ba∇dbl≤α∆
8+/ba∇dblΦD√π/ba∇dbl≤5
4·αγν
4C.(26)
Combining these observations and using the triangle inequa lity repeatedly yields
/ba∇dbl(¯Ψ−Φ)D√π/ba∇dbl≤/ba∇dblΦA(D√π·α·β)−1−ΦD√π/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤α∆/8 (7)+/ba∇dblΦA(D√π·α·β)−1/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤αγν/C(26)·/ba∇dblT−1− /C1/ba∇dbl/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤∆/3 (25)≤∆
4.
This shows that under the assumptions of the theorem, the wei ghted operator norm of
the distance between the generating dictionary and the scal ed update decreases in each
iteration.
Now tothecontraction of theatomwise ℓ2-norm. First weagain splitourupdateddictionary
atom¯ψℓinto two parts using that I=eℓe∗
ℓ+Iℓc
¯ψℓ=¯Ψeℓ= ΦA(D√π·α·β)−1T−1eℓπ−1
2
ℓ
= ΦA(Dπ·α·β)−1eℓ·e∗
ℓT−1eℓ+ΦA(D√π·α·β)−1IℓcT−1eℓπ−1
2
ℓ.(27)
Using the shorthand sℓ=e∗
ℓT−1eℓand substracting sℓφℓwe get
/ba∇dbl¯ψℓ−sℓφℓ/ba∇dbl≤|sℓ|·/ba∇dblΦA(Dπ·α·β)−1eℓ−φℓ/ba∇dbl+/ba∇dblΦA(D√π·α·β)−1/ba∇dbl·/ba∇dblIℓcT−1eℓπ−1
2
ℓ/ba∇dbl.
The ﬁrst term is well-behaved and makes no problems. Indeed, since|sℓ|≤/ba∇dblT−1/ba∇dbl, using
(24) and Lemma 9 yields
|sℓ|·/ba∇dblΦA(Dπ·α·β)−1eℓ−φℓ/ba∇dbl≤(1−∆/4)−1·∆/8. (28)
To bound/ba∇dblIℓceℓT−1eℓ/ba∇dblwe useT−1=I+T−1(I−T) andIℓceℓ= 0 to get
/ba∇dblIℓcT−1eℓ/ba∇dbl=/ba∇dblIℓcT−1(I−T)eℓ/ba∇dbl=/ba∇dblIℓcT−1(eℓe∗
ℓ+Iℓc)(I−T)eℓ/ba∇dbl
≤/ba∇dblIℓcT−1eℓ/ba∇dbl·/ba∇dblI−T/ba∇dbl+/ba∇dblT−1/ba∇dbl·/ba∇dblIℓcTeℓ/ba∇dbl.
Rearranging the above and using (24) as well as Lemma 8 yields
/ba∇dblIℓcT−1eℓ/ba∇dbl≤/ba∇dblT−1/ba∇dbl
1−/ba∇dblI−T/ba∇dbl·/ba∇dblIℓcTeℓ/ba∇dbl≤1
(1−∆/4)2·/ba∇dblIℓcTeℓ/ba∇dbl.
Combining these observations with the bound from (26) and Le mma 10 yields
/ba∇dbl¯ψℓ−sℓφℓ/ba∇dbl≤1
1−∆/4·∆
8+1
(1−∆/4)2·5
4·αγν
4C·/ba∇dblIℓcTeℓπ−1
2
ℓ/ba∇dbl
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
≤∆/8 (10)≤∆
3.(29)
12Normalisation Combining the above results shows that with high probabilit y, the dic-
tionary update step of both algorithms before normalisatio n satisﬁes
/ba∇dbl(¯Ψ−Φ)D√π/ba∇dbl2,2≤∆/4 and max
ℓ/ba∇dbl¯ψℓ−sℓφℓ/ba∇dbl≤∆/3, (30)
wheresℓ= 1 in case of ODL and |sℓ−1|≤/ba∇dblT−1−1/ba∇dbl≤∆/3 in case of MOD. So what
is left to show is that the normalisation step at the end of eac h iteration does not interfere
with convergence. Let F:= diag(/ba∇dbl¯ψi/ba∇dbl2)−1be the square diagonal normalization matrix and
denote by ˆΨ :=¯ΨFthe normalized dictionary of the current update step. Since /ba∇dblφℓ/ba∇dbl= 1
we have
|/ba∇dbl¯ψℓ/ba∇dbl−1|≤/ba∇dbl¯ψℓ−φℓ/ba∇dbl≤/ba∇dbl¯ψℓ−sℓφℓ/ba∇dbl+/ba∇dbl(sℓ−1)φℓ/ba∇dbl≤∆/3+∆/3≤∆,
which further means that /ba∇dblF/ba∇dbl2,2≤(1−∆)−1and/ba∇dbl /C1−F/ba∇dbl≤∆·(1−∆)−1. Hence, using
that by assumption /ba∇dblΦD√π/ba∇dbl≤1/(4C) and ∆≤2/Cthe weighted operator norm of the
diﬀerence of the generating dictionary Φ and the normalised u pdateˆΨ can be bounded as
/ba∇dbl(ˆΨ−Φ)D√π/ba∇dbl≤/ba∇dbl(¯Ψ−Φ)D√π/ba∇dbl/ba∇dblF/ba∇dbl+/ba∇dblΦD√π/ba∇dbl/ba∇dbl( /C1−F)/ba∇dbl
≤∆
4·1
1−∆+1
4C·∆
1−∆≤∆
2. (31)
To bound the ℓ2-norm we simply use Lemma B.10 from [35], which says that if /ba∇dblφ/ba∇dbl= 1 and
/ba∇dblψ−sφ/ba∇dbl≤t, thenˆψ=ψ//ba∇dblψ/ba∇dblsatisﬁes/ba∇dblˆψ−φ/ba∇dbl2≤2−2/radicalbig
1−t2/s2. Combining this with
the bound√1−t≥1−t
2−tfort∈(0,1) and using t= ∆/3 and|1−sℓ|≤∆/3, meaning
sℓ≥1−∆/3, yields
/ba∇dblˆψℓ−φℓ/ba∇dbl≤t·/parenleftbigg
s2
ℓ−t2
2/parenrightbigg−1/2
≤∆
3·/parenleftbigg/parenleftig
1−∆
3/parenrightig2
−∆2
18/parenrightbigg−1/2
≤∆
2.
This shows that not only /ba∇dbl(ˆΨ−Φ)D√π/ba∇dblbut also/ba∇dblˆΨ−Φ/ba∇dbl2,1and thusδ(ˆΨ,Φ) is bounded
by ∆/2 and thus concludes the proof of Theorem 3.
5. Discussion
In this paper we have shown that two widely used alternating m inimisation algorithms,
MOD and ODL (aK-SVD) both combined with thresholding, conve rge to a well behaved
data-generating dictionary Φ from any initialisation Ψ tha t either lies within distance
O(1/logK) or is itself well-behaved and has a diagonally dominant cro ss-coherence ma-
trix Ψ∗Φ. For ODL this constitutes the ﬁrst convergence result of th is kind, while for MOD
it extends the convergent areas, derived in [1], by orders of magnitude and under weaker
assumptions on the generating dictionary.
We want to emphasize that — to the best of our knowledge — our co nvergence theorem is
the ﬁrst result in theoretical dictionary learning, which i s valid for signal generating models
that do not assume a (quasi)-uniform distribution of sparse supports. Instead it can handle
more realistic distributions, where each atom is used with a diﬀerent probability. This sta-
bility might alsobeanexplanation forthepopularityof alt ernating minimisation algorithms
13in practical applications such as image processing. Natura l images, for instance, contain
more low than high-frequencies and the success of e.g. wavel ets in classical signal processing
can be attributed to the fact that they can be divided into fre quency bands with diﬀerent
appearance probabilities. Since also more recent schemes s uch as compressed sensing can
by improved by exploiting information about the inclusion/ appearance probabilities of each
wavelet, [32, 28], we expect that our study of dictionary lea rning for non-uniform support
distributions will encourage the development of exciting n ew signal processingmethods that
leverage this additional information about the frequency o f appearance (and thus in some
sense the importance) of diﬀerent atoms.
Note that the techniques we used to prove convergence for MOD and ODL here can also be
used to turn the contraction results for ITKrM, [23], into co nvergence results, [30]. As part
of our future work we plan to further increase the realism of o ur data models by including
diﬀerent coeﬃcient distributions for diﬀerent atoms and nois e. We also plan to analyse
partial convergence of the dictionary, meaning convergenc e for most but not all atoms. The
main motivation for this is that a randomly initialised dict ionary is still rather unlikely to
satisfy even the relaxed diagonal dominance requirement. H owever, we expect that after
resorting, a large, left upper part of Ψ∗Φ will be diagonally dominant and that all asso-
ciated atoms still converge quite closely. A result of this k ind would put the replacement
and adaptive dictionary learning strategies, developed in [23], on a ﬁrmer theoretical basis.
Lastly, we are interested in increasing the realism of our si gnal model even further by in-
cluding also second order inclusion probabilities into our generating model. This is again
inspired by the behaviour of wavelets in natural images, whe re it is know that spatially
close wavelets are likely to appear together.
Acknowledgments
This work was supported by the Austrian Science Fund (FWF) un der Grant no. Y760.
14Appendix A. Proofs of the 4 inequalities
A major hurdle in analysing the above dictionary learning al gorithms is that each update
of the sparse coeﬃcients involves projecting onto submatri ces of the current guess Ψ. For
the remainder of this chapter we will set ϑ:= 1/4 and write
FΦ:={I:/ba∇dblΦ∗
IΦI− /C1/ba∇dbl≤ϑ}andFΨ:={I:/ba∇dblΨ∗
IΨI− /C1/ba∇dbl≤ϑ} (32)
for the set of index sets where the random variables Φ Iresp. Ψ Iare well conditioned. We
further write
FZ:=/braceleftig
I:/ba∇dblZI/ba∇dbl≤δ·/radicalbig
2log(nKρ/δ ⋆)/bracerightig
(33)
for the set of index sets, where the norm of the random variabl eZIis comparable to δ.
Finally, set
G:=FΦ∪FΨ∪FZ. (34)
We also need to control the sparse approximation step in each iteration. Recall that thresh-
olding amounts to ﬁnding the largest Sentries in magnitude of Ψ∗y, collecting them in the
index set ˆIand calculating the corresponding optimal coeﬃcients as ˆ xˆI= Ψ†
ˆIy. For the
remainder of this chapter, we write
H:=/braceleftig
(I,σ,c)|ˆI=I/bracerightig
(35)
for the set of index, sign and coeﬃcient triplets, where thre sholding is guaranteed to recover
the correct support. With all the necessary notation in plac e, we can show that under the
assumptions of Theorem 3, the failure probability of thresh olding and the probability that
our submatrices are ill-conditioned can be bound by approxi matelyδ⋆/ρ. This will be used
repeatedly by the lemmas afterwards.
Lemma 4 Under the assumptions of Theorem 3 we have
[2P(Hc)+P(Gc)]·ρ≤δ⋆/32. (36)
ProofWe begin with bounding the failure probability of threshold ing,P(Hc). SetN:=
Ψ∗Φ−diag(Ψ∗Φ). By deﬁnition of the algorithm, thresholding recovers th e full support of
a signaly= ΦIxI, if
/ba∇dblΨ∗
Icy/ba∇dbl∞</ba∇dblΨ∗
Iy/ba∇dblmin.
Notethatthesignalshavetwosourcesofrandomness, σandI. Recallthat α= mini|/an}b∇acketle{tψi,φi/an}b∇acket∇i}ht|.
Plugging in the deﬁnition of y, we bound the failure probability as
Py(/ba∇dblΨ∗
Iy/ba∇dblmin</ba∇dblΨ∗
Icy/ba∇dbl∞) =Py(/ba∇dblΨ∗
IΦIxI/ba∇dblmin</ba∇dblΨ∗
IcΦIxI/ba∇dbl∞)
≤Py(cmin/ba∇dbldiag(Ψ∗
IΦI)/ba∇dblmin−/ba∇dblNI,IxI/ba∇dbl∞</ba∇dblΨ∗
IcΦIxI/ba∇dbl∞)
≤Py(cmin·α<2/ba∇dblNIxI/ba∇dbl∞)
≤Py/parenleftbig
2/ba∇dblNIxI/ba∇dbl∞≥cmin·α/vextendsingle/vextendsingle/ba∇dblNI/ba∇dbl∞,2<η/parenrightbig
+PS(/ba∇dblNI/ba∇dbl∞,2≥η).(37)
15To bound the ﬁrst term, we use that for k∈I, we havexk=σkck, whereσ∈RSis an
independent Rademacher sequence. As the signs σare independent of the support I, we
can apply Hoeﬀding’s inequality to each entry of NIxI. The second term is a little more
involved. By the Poissonisation trick [31, Lemma 3.5] we hav e
P(/ba∇dblNI/ba∇dbl∞,2≥η)≤2PB(/ba∇dblNI/ba∇dbl∞,2≥η),
wherePBis the Poisson sampling model corresponding to the p1,···,pK. Now a simple ap-
plication of the matrix Chernoﬀ inequality, [42], restated in 11, together with an application
of Hoeﬀding’s inequality to the ﬁrst term in (37) yields
Py(/ba∇dblΨ∗
Iy/ba∇dblmin</ba∇dblΨ∗
Icy/ba∇dbl∞)≤2Kexp/parenleftbigg
−c2
min
8c2maxη2·α2/parenrightbigg
+2K/parenleftigg
e/ba∇dblND√p/ba∇dbl2
∞,2
η2/parenrightiggη2
µ(Ψ,Φ)2
≤2Kexp/parenleftbigg
−γ2
8η2·α2/parenrightbigg
+2K/parenleftigg
2e/ba∇dblND√π/ba∇dbl2
∞,2
η2/parenrightiggη2
µ(Ψ,Φ)2
,(38)
where we used that (1 −/ba∇dblp/ba∇dbl∞)·pi≤πi, from [32] resp. Theorem 15(a), which implies
pi≤6
5πi<2πias well as/ba∇dblND√p/ba∇dbl2
∞,2≤2/ba∇dblND√π/ba∇dbl2
∞,2.
Before going on we recall the abbreviations
ν=1/radicalbig
log(nKρ/δ ⋆)≤1
3andδ◦=γ
Clog(nKρ/δ ⋆)=γν2
C,
where the bound on νholds true since nρδ−1
⋆≥130·2·42. Setting η:=γαν/4 the ﬁrst
term in 38 becomes 2 Kδ2
⋆/(nρK)2. To bound the second term observe that
/ba∇dblND√π/ba∇dbl2
∞,2=/ba∇dbl[Ψ∗Φ−diag(Ψ∗Φ)]D√π/ba∇dbl2
∞,2≤/ba∇dblΦD√π/ba∇dbl2, (39)
so our condition on the generating dictionary in (13), /ba∇dblΦD√π/ba∇dbl≤γαν/(4C), ensures that
2e/ba∇dblND√π/ba∇dbl2
∞,2η−2≤e−2. To lower bound the exponent of the second term we will have to
look at both regimes described in Theorem 3, separately. In t he ﬁrst regime, δ > δ◦, by
(15) we simply have µ(Ψ,Φ)≤αγν2/(4C). In the second regime, δ≤δ◦, we can employ
the following bound
µ(Ψ,Φ) = max
i/\e}atio\slash=j|/an}b∇acketle{tφi,ψj/an}b∇acket∇i}ht|≤max
i/\e}atio\slash=j|/an}b∇acketle{tφi,φj/an}b∇acket∇i}ht|+max
j/ba∇dblψj−φj/ba∇dbl≤µ(Φ)+δ,
together with the observation that whenever δ≤δ◦≤1/C, we haveα= 1−ε2/2≥
1−δ2/2≥(C−1)/Cand therefore also
δ≤αγ
αClog(nKρ/δ ⋆)≤αγ
(C−1)log(nKρ/δ ⋆)=αγν2
(C−1). (40)
This means that in both regimes we have µ(Ψ,Φ)≤2αγν2/(C−1). Substituting these
bounds into 38, we get
P(Hc)≤2K/parenleftbiggδ⋆
nρK/parenrightbigg2
+2K/parenleftbiggδ⋆
nρK/parenrightbigg(C−1)2/32
≤4K/parenleftbiggδ⋆
nρK/parenrightbigg2
. (41)
16Now we turn to boundingthe quantity P(Fc
Z). Again by the Poissonisation trick, the matrix
Chernoﬀ inequality and the bound pi≤2πiwe have
P/parenleftbig
/ba∇dblZIZ∗
I/ba∇dbl>t/parenrightbig
≤2PB/parenleftbig
/ba∇dblZIZ∗
I/ba∇dbl>t/parenrightbig
≤2K/parenleftbigge/ba∇dblZDpZ∗/ba∇dbl
t/parenrightbiggt/ε2
≤2K/parenleftbigg2e/ba∇dblZDπZ∗/ba∇dbl
t/parenrightbiggt/ε2
≤2K/parenleftbigg2eδ2
t/parenrightbiggt/δ2
.
Settingt= 2δ2max{e2,log(nKρ/δ ⋆)}= 2δ2log(nKρ/δ ⋆) we get
P(Fc
Z) =P/parenleftig
/ba∇dblZI/ba∇dbl≥δ·/radicalbig
2log(nKρ/δ ⋆)/parenrightig
≤2K/parenleftbiggδ⋆
nKρ/parenrightbigg2
. (42)
Next we use Theorem 13, [31], and pi≤6
5πito bound P(Fc
Φ∪Fc
Ψ) as
P(Fc
Φ∪Fc
Ψ)≤512Kexp/parenleftbigg
−min/braceleftbigg5ϑ2
24e2/ba∇dblΦD√π/ba∇dbl2,ϑ
2µ(Φ),5ϑ2
24e2/ba∇dblΨD√π/ba∇dbl2,ϑ
2µ(Ψ)/bracerightbigg/parenrightbigg
.
In the ﬁrst regime we have by assumption that max {ν/ba∇dblΦD√π/ba∇dbl,µ(Φ)}≤γαν2/(4C) and
max{ν/ba∇dblΨD√π/ba∇dbl,µ(Ψ)} ≤γαν2/(4C), while in the second regime we only have the ﬁrst
inequality as well as δ≤γαν2/(C−1). However, we can use the bound
/ba∇dblΨD√π/ba∇dbl≤/ba∇dblΦD√π/ba∇dbl+/ba∇dbl(Ψ−Φ)D√π/ba∇dbl≤/ba∇dblΦD√π/ba∇dbl+δ
≤γαν·(1
4C+ν
C−1)≤γαν/C, (43)
and the fact that for δ≤δ◦=γν2/C≤ν2/Cwe have
µ(Ψ) = max
i/\e}atio\slash=j|/an}b∇acketle{tψi,ψj/an}b∇acket∇i}ht|≤max
i/\e}atio\slash=j|/an}b∇acketle{tφi,φj/an}b∇acket∇i}ht+/an}b∇acketle{tφi,ψj−φj/an}b∇acket∇i}ht+/an}b∇acketle{tψi−φi,ψj/an}b∇acket∇i}ht|
≤µ(Φ)+2ε≤µ(Φ)+2δ≤9
4·ν2/C. (44)
All together this means that for ϑ= 1/4 we can lower boundthe expression in the minimum
in both regimes by 2log( nKρ/δ ⋆), leading to P(Fc
Φ∪Fc
Ψ)≤512K(δ⋆/(nρK))2.Collecting
all the bounds we ﬁnally get for n≥130 that
P(Hc)·2ρ+P(Gc)·ρ≤522Kρ/parenleftbiggδ⋆
nρK/parenrightbigg2
≤δ⋆
32.
Another ingredient we needto prove the4inequalities usedi n theproofof our main theorem
is the following lemma to estimate expectations of products of random matrices. A detailed
proof, based on [9, 16], can be found in Appendix B.1 .
Lemma 5 LetA(I)∈Rd1×d2,B(I)∈Rd2×d3,C(I)∈Rd3×d4be random matrices, where
Iis a discrete random variable taking values in IandG⊆I. If for all I∈Gwe have
/ba∇dblB(I)/ba∇dbl≤Γthen
/ba∇dblE[A(I)·B(I)·C(I)· /BDG(I)]/ba∇dbl≤/ba∇dblE[A(I)A(I)∗]/ba∇dbl1/2·Γ·/ba∇dblE[C(I)∗C(I)]/ba∇dbl1/2.
17Finally, we also need the following corollary of results fro m [32], which is again proved in
Appendix B.2.
Corollary 6 Denote by Ethe expectation according to the rejective sampling probab ility
with levelSand byπ∈RKthe ﬁrst order inclusion probabilities of level S. LetIbe a
K×Kmatrix with zero diagonal, W= (w1...,wK)andV= (v1,...,vK)a pair ofd×K
matrices andGa subset of all supports of size S, meaningG⊆{I:|I|=S}. If/ba∇dblπ/ba∇dbl∞≤1/3,
we have
/ba∇dblE[D−1√πR∗
III,IRID−1√π]/ba∇dbl≤3·/ba∇dblD√πID√π/ba∇dbl, (a)
/ba∇dblE[D−1√πR∗
III,II∗
I,IRID−1√π]/ba∇dbl≤9
2·/ba∇dblD√πID√π/ba∇dbl2+3
2·max
k/ba∇dble∗
kID√π/ba∇dbl2,(b)
/ba∇dblE[WR∗
IRIV∗· /BDI(ℓ) /BDG(I)]/ba∇dbl≤πℓ·(/ba∇dblWD√π/ba∇dbl·/ba∇dblVD√π/ba∇dbl+/ba∇dblwℓ/ba∇dbl·/ba∇dblvℓ/ba∇dbl), (c)
as well as
/ba∇dblE[D−1√πIℓcR∗
III,II∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl
≤3
2·πℓ·/parenleftbig
3·/ba∇dblD√πIeℓ/ba∇dbl2+max kI2
kℓ+9
2·/ba∇dblD√πID√π/ba∇dbl2+3
2·maxk/ba∇dble∗
kID√π/ba∇dbl2/parenrightbig
.(d)
With the last three results in place we are ﬁnally able to prov e Lemmas 7-10, which provide
the 4 inequalities our proof is based on.
Lemma 7 Under the assumptions of Theorem 3 we have
P/parenleftbigg
/ba∇dblΦA(D√π·α·β)−1−ΦD√π/ba∇dbl>α∆/8/parenrightbigg
≤(d+K)exp/parenleftbigg
−N(∆/16)2
2ρ2+ρ∆/16/parenrightbigg
.(45)
ProofThe idea is to writeΦ A(D√π·α·β)−1−ΦD√πas a sum of independentrandom matrices
and apply the matrix Bernstein inequality to show that we hav e concentration. Since
we assumed in the algorithm that the estimated coeﬃcients ca n never have larger norm
than the signal times κwe ﬁrst deﬁne for v∈Rdthe set of possible stable supports as
B(v) :={I:/ba∇dblΨ†
Iv/ba∇dbl≤κ/ba∇dblv/ba∇dbl}. Based on this deﬁnition we deﬁne the following random
matrices for n∈[N]
ˆYn: =yny∗
n(Ψ†
ˆIn)∗RˆIn(D√π·α·β)−1· /BDB(yn)(ˆIn)−ΦD√π,
where as always, ˆIndenotes the set found by the thresholding algorithm. As each matrix
ˆYnonly depends on the signal ynthey are independent and we have
N−1/summationtext
nˆYn= ΦA(D√π·α·β)−1−ΦD√π,
so we can use the matrix Bernstein inequality 12 from [42] to b ound the left hand side of
(45). For that we have to ﬁnd an upper bound for the operator no rm. By the assumptions
on the generating dictionary and since we ensured in the algo rithm that the estimated
coeﬃcients, which are too large are set to zero, meaning /ba∇dblˆxn/ba∇dbl≤κ/ba∇dblyn/ba∇dbland/ba∇dblyn/ba∇dbl≤Scmax,
we get forρ= 2κ2S2γ−2α−2π−3/2andπ<1/3,
/ba∇dblˆYn/ba∇dbl≤κS2c2
max/ba∇dblD−1
β/ba∇dbl/ba∇dblD−1
α/ba∇dbl/ba∇dblD−1√π/ba∇dbl+/ba∇dblΦD√π/ba∇dbl≤ραπ+1≤ρα/2 =:r.(46)
18Bounding/ba∇dblE[ˆYn]/ba∇dblis a little more involved. Recall that His the set of signals y, meaning
support,signandcoeﬃcient triplets( I,σ,c), wherethresholdingrecoversthecorrectsupport
from the corresponding signal. Further Gis the set of supports IwhereϑIis small - i.e. the
corresponding subdictionary Ψ Iis well-conditioned. For each nwe deﬁne a new random
matrixYn, for which the estimated support ˆInis replaced with the correct support Inand
ΦD√πis replaced by Φdiag( 1In)D−1√π
Yn: =yny∗
n(Ψ†
In)∗RIn(D√π·α·β)−1· /BDB(yn)(In)−Φdiag(1In)D−1√π.
Note that/ba∇dblΦdiag(1In)D−1√π/ba∇dbl≤Sπ−1/2so the same bound as for /ba∇dblˆYn/ba∇dblholds. Concretely,
with the same argument as above Ynis bounded by r. Further, by deﬁnition of Hthe
ﬁrst terms of the two random matrices YnandˆYncoincide onH, while the second terms
coincide in expectation, meaning E[Φdiag(1In)D−1√π] = ΦD√π. So dropping the index nfor
convenience, as each signal has the same distribution, e.g. , writingIforIn, we get using
Lemma 4,
/ba∇dblE[ˆY]/ba∇dbl≤/ba∇dblE[ˆY−Y]/ba∇dbl+/ba∇dblE[Y]/ba∇dbl≤P(Hc)·2r+/ba∇dblE[ /BDGc(I)Y]/ba∇dbl+/ba∇dblE[ /BDG(I)Y]/ba∇dbl
≤P(Hc)·2r+P(Gc)·r+/ba∇dblE[ /BDG(I)Y]/ba∇dbl.(47)
Next note that whenever I∈Gwe have for any sign and coeﬃcient pair ( σ,c) that the
corresponding signal ysatisﬁes/ba∇dblΨ†
Iy/ba∇dbl≤(1−ϑ)−1
2·/ba∇dbly/ba∇dbl≤κ/ba∇dbly/ba∇dbl, so we haveG⊆B(y),
meaning /BDB(y)
/BDG= /BDG. Remembering that y= ΦIxI= ΦI(σI⊙cI), we can take the
expectation over ( σ,c), which, using the shorthand EG[f(I)] :=EI[ /BDG(I)f(I)], yields
/ba∇dblE[ /BDG(I)Y]/ba∇dbl=/ba∇dblEI[ /BDG(I)·Eσ,c[ΦIxIx∗
IΦ∗
I(Ψ†
I)∗RI(D√π·α·β)−1−Φdiag(1I)D−1√π]]/ba∇dbl
=/ba∇dblEG[ΦIΦ∗
I(Ψ†
I)∗RI(D√π·α)−1−ΦI(Dα)I,IRI(D√π·α)−1]/ba∇dbl. (48)
We next have a closer look at the term Φ∗
I(Ψ†
I). SetH=I−Ψ∗Ψ. ForI∈Gwe have
/ba∇dblHI,I/ba∇dbl=/ba∇dbl /C1S−Ψ∗
IΨI/ba∇dbl≤ϑ, meaning Ψ Ihas full rank and Ψ†
I= (Ψ∗
IΨI)−1Ψ∗
I.We can
further use the Neumann series to get the useful identity
(Ψ∗
IΨI)−1= ( /C1S−HI,I)−1=/summationtext
k≥0Hk
I,I= /C1S+(Ψ∗
IΨI)−1HI,I (49)
= /C1S+HI,I(Ψ∗
IΨI)−1, (50)
and the norm estimate /ba∇dbl(Ψ∗
IΨI)−1/ba∇dbl≤(1−ϑ)−1. By deﬁnition of Z= Ψ−Φ, we have
E:= diag(Z∗Ψ) = /C1−Dα. We also deﬁne the zero diagonal matrix
I:= (ΨE−Z)∗Ψ = (ΦE−ZDα)∗Ψ = (Φ−ΨDα)∗Ψ, (51)
which lets us express Φ∗
I(Ψ†
I)∗as
Φ∗
I(Ψ†
I)∗= (Ψ∗
I−Z∗
I)(Ψ†
I)∗=IS−Z∗
I(Ψ†
I)∗
= (Dα)I,I+EI,IΨ∗
IΨI(Ψ∗
IΨI)−1−Z∗
IΨI(Ψ∗
IΨI)−1
= (Dα)I,I+II,I(Ψ∗
IΨI)−1= (Dα)I,I+II,I+II,I(Ψ∗
IΨI)−1HI,I.(52)
19Note that for any Iwe have/ba∇dblII,I/ba∇dbl≤/ba∇dbl(ΨE−Z)I/ba∇dbl·/ba∇dblΨI/ba∇dbl≤ε√
S·√
S <2Sand therefore
/ba∇dblΦIII,IRI(D√π·α)−1/ba∇dbl≤ρα/2 =r. So substituting the expression for Φ∗
I(Ψ†
I)∗above into
(48) resp. (47) and rewriting Φ I= ΦD√πD−1√πR∗
Iwe get
/ba∇dblE[ˆY]/ba∇dbl≤[2P(Hc)+P(Gc)]·r+/ba∇dblEG[ΦI(II,I+II,I(Ψ∗
IΨI)−1HI,I)RI(D√π·α)−1]/ba∇dbl
≤[2P(Hc)+P(Gc)]·r+P(Gc)·r+/ba∇dblE[ΦIII,IRI(D√π·α)−1]/ba∇dbl
+/ba∇dblEG[ΦIII,I(Ψ∗
IΨI)−1HI,IRI(D√π·α)−1]/ba∇dbl
≤[P(Hc)+P(Gc)]·ρα+/ba∇dblΦD√π/ba∇dbl·/ba∇dblE[D−1√πR∗
III,IRID−1√π]/ba∇dbl·/ba∇dblD−1
α/ba∇dbl
+/ba∇dblΦD√π/ba∇dbl·/ba∇dblEG[D−1√πR∗
III,I(Ψ∗
IΨI)−1HI,IRID−1√π]/ba∇dbl·/ba∇dblD−1
α/ba∇dbl.(53)
Using Corollary 6(a) and ε≤min{δ,√
2}, we bound the ﬁrst expectation as
/ba∇dblE[D−1√πR∗
III,IRID−1√π]/ba∇dbl≤3/ba∇dblD√πID√π/ba∇dbl≤3·(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl)·/ba∇dblΨD√π/ba∇dbl
≤3·δ·/ba∇dblΨD√π/ba∇dbl·(/ba∇dblΦD√π/ba∇dbl+1). (54)
Before we estimate the second expectation, note that applyi ng Corollary 6(b) to H,I,I∗
and using that max {/ba∇dblΨD√π/ba∇dbl,/ba∇dblΦD√π/ba∇dbl} ≤ν/C≤1/8 yields the following three bounds,
whose derivation can be found in Appendix B.2,
/ba∇dblE[D−1√πR∗
IHI,IH∗
I,IRID−1√π]/ba∇dbl≤2·/ba∇dblΨD√π/ba∇dbl2, (55)
/ba∇dblE[D−1√πR∗
III,II∗
I,IRID−1√π]/ba∇dbl≤1/2·(3/ba∇dblZD√π/ba∇dbl+3ε)2·/ba∇dblΨD√π/ba∇dbl2, (56)
/ba∇dblE[D−1√πR∗
II∗
I,III,IRID−1√π]/ba∇dbl≤2·(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl)2. (57)
ApplyingTheorem5tothesecondexpectationin(53),usingt hatonGwehave/ba∇dbl(Ψ∗
IΨI)−1/ba∇dbl≤
(1−ϑ)−1≤4/3, and the ﬁrst two inequalities above yields
/ba∇dblEG[D−1√πR∗
III,I·(Ψ∗
IΨI)−1·HI,IRID−1√π]/ba∇dbl
≤/ba∇dblE[D−1√πR∗
III,II∗
I,IR∗
ID√π/ba∇dbl1/2·4/3·/ba∇dblE[D−1√πRIH∗
I,IHI,IRID−1√π]/ba∇dbl1/2
≤/parenleftbig
4/ba∇dblZD√π/ba∇dbl+4ε/parenrightbig
·/ba∇dblΨD√π/ba∇dbl2≤8·δ·/ba∇dblΨD√π/ba∇dbl2. (58)
Substituting (54), (58) and the probability bound from Lemm a 4 into (53) leads to
/ba∇dblE[ˆY]/ba∇dbl≤αδ⋆/32+δ/α·/ba∇dblΦD√π/ba∇dbl·/ba∇dblΨD√π/ba∇dbl·(3+3/ba∇dblΦD√π/ba∇dbl+8/ba∇dblΨD√π/ba∇dbl).(59)
BytheassumptionsofTheorem3wehaveinbothregimes /ba∇dblΦD√π/ba∇dbl≤γαν/(4C)and/ba∇dblΨD√π/ba∇dbl·
δ≤γαν/(2C), whiledueto(43)wehave /ba∇dblΨD√π/ba∇dbl≤γαν/Cagaininbothregimes. Bounding
the quantity in (59) in two ways we get
/ba∇dblE[ˆY]/ba∇dbl≤αδ⋆
32+αγ
C·min/braceleftbiggγν2
C,2γν2
C·δ/bracerightbigg
≤αδ⋆
32+αγ
C·min{δ◦,δ}≤α·∆
16.
Finally, an application of the matrix Bernstein inequality 12 witht=m=α∆/16 and
r=αρ/2 and some simpliﬁcations yield the desired bound.
The next lemma shows that the matrix B=/summationtextN
n=1ˆxnˆx∗
nessentially behaves like a diagonal
matrix. In particular, after rescaling we have ( D√π·α)−1B(D√π·α·β)−1≈ /C1.
20Lemma 8 Under the assumptions of Theorem 3 we have
P/parenleftbig
/ba∇dbl(D√π·α)−1B(D√π·α·β)−1− /C1/ba∇dbl>∆/4/parenrightbig
≤2Kexp/parenleftbigg
−N(∆/16)2
2ρ2+ρ∆/16/parenrightbigg
.
ProofWe will follow the approach in last proof very closely, that i s, we write the matrix
(D√π·α)−1B(D√π·α·β)−1− /C1as a scaled sum of independent random matrices ˆYnand apply
the matrix Bernstein inequality. Recalling that ˆIndenotes the set found by thresholding
and thatB(v) :={I:/ba∇dblΨ†
Iv/ba∇dbl≤κ/ba∇dblv/ba∇dbl}denotes the set of possible stable supports for v, we
deﬁne forn∈[N] the matrices ˆYnas well as their auxiliary counterparts Ynas
ˆYn: = (D√π·α)−1R∗
ˆInΨ†
ˆInyny∗
n(Ψ†
ˆIn)∗RˆIn(D√π·α·β)−1/BDB(yn)(ˆIn)− /C1
andYn: = (D√π·α)−1R∗
InΨ†
Inyny∗
n(Ψ†
ˆIn)∗RIn(D√π·α·β)−1/BDB(yn)(In)−diag(1In)D−1
π.
Recall that ρ= 2κ2S2γ−2α−2π−3/2, so both matrices can be bounded as
max{/ba∇dblˆYn/ba∇dbl,/ba∇dblYn/ba∇dbl}≤κ2S2c2
max/ba∇dblD−1
β/ba∇dbl/ba∇dblD−2
α/ba∇dbl/ba∇dblD−1
π/ba∇dbl+/ba∇dblD−1
π/ba∇dbl≤3ρ/4 =:r.(60)
OnH, meaning whenever thresholding succeeds, the ﬁrst terms of ˆYnandYnagain coincide
while the second terms are the same in expectation, that is E[diag(1In)D−1
π] = /C1. So with
the same argument as in (47) and as usual dropping the index nfor convenience, we get
/ba∇dblE[ˆY]/ba∇dbl≤2ρ·P(Hc)+ρ·P(Gc)+/ba∇dblE[ /BDG(I)Y]/ba∇dbl. (61)
Similarly as in (48) we next usethat all well conditioned sup portsare stable for any signal y,
meaningG⊆B(y). Taking the expectation over ( σ,c) yields
/ba∇dblE[ /BDG(I)Y]/ba∇dbl=/ba∇dblEGEσ,c[(D√π·α)−1R∗
IΨ†
IΦIxIx∗
IΦ∗
I(Ψ†
I)∗RI(D√π·α·β)−1−diag(1I)D−1
π]/ba∇dbl
=/vextenddouble/vextenddoubleEG/bracketleftbig
(D√π·α)−1R∗
I/parenleftbig
Ψ†
IΦ∗
IΦ∗
I(Ψ†
I)∗−(Dα)2
I,I/parenrightbig
RI(D√π·α)−1/bracketrightbig/vextenddouble/vextenddouble. (62)
Using the expression for Φ∗
I(Ψ†
I)∗from (52) we get
Ψ†
IΦIΦ∗
I(Ψ†
I)∗−(D2
α)I,I= (Dα)I,III,I(Ψ∗
IΨI)−1+(Ψ∗
IΨI)−1I∗
I,I(Dα)I,I
+(Ψ∗
IΨI)−1I∗
I,III,I(Ψ∗
IΨI)−1.(63)
The ﬁrst two terms on the right hand side are each other’s tran spose, so substituting the
above into (62) yields
/ba∇dblE[ /BDG(I)Y]/ba∇dbl≤2·/vextenddouble/vextenddoubleEG/bracketleftbig
D−1√πR∗
III,I(Ψ∗
IΨI)−1RID−1√π/bracketrightbig/vextenddouble/vextenddouble·/ba∇dblD−1
α/ba∇dbl
+/ba∇dblD−1
α/ba∇dbl·/vextenddouble/vextenddoubleEG/bracketleftbig
D−1√πR∗
I(Ψ∗
IΨI)−1I∗
I,III,I(Ψ∗
IΨI)−1RID−1√π/bracketrightbig/vextenddouble/vextenddouble·/ba∇dblD−1
α/ba∇dbl.(64)
To estimate the ﬁrst term we repeat the steps in (53), noting t hat for all Iwe have
/ba∇dblD−1√πR∗
III,IRID−1√π/ba∇dbl≤ρα2/2. Using (54) as well as (58) we get
/ba∇dblEGD−1√πR∗
III,I(Ψ∗
IΨI)−1RID−1√π]/ba∇dbl≤P(Gc)·ρα2/2+/ba∇dblE[D−1√πR∗
III,IRID−1√π]/ba∇dbl
+/ba∇dblEG[D−1√πR∗
III,I(Ψ∗
IΨI)−1HI,IRID−1√π]/ba∇dbl
≤P(Gc)·ρα2/2+3·(/ba∇dblΦD√π/ba∇dbl·δ2/2+/ba∇dblZD√π/ba∇dbl)·/ba∇dblΨD√π/ba∇dbl
+4·/parenleftbig
/ba∇dblZD√π/ba∇dbl+δ/parenrightbig
·/ba∇dblΨD√π/ba∇dbl2. (65)
21Beforeweestimatethesecondtermnotethatfor I∈Gwecanuse(51)tobound /ba∇dblII,I(Ψ∗
IΨI)−1/ba∇dbl
in two diﬀerent ways, either as
/ba∇dblII,I(Ψ∗
IΨI)−1/ba∇dbl=/ba∇dbl(Φ−ΨDα)∗
IΨI(Ψ∗
IΨI)−1/ba∇dbl
=/ba∇dblΦI(Ψ†
I)∗+(Dα)I,I/ba∇dbl≤/radicalig
1+ϑ
1−ϑ+1≤/radicalbig
5/3+1≤7/3,(66)
or recalling that on Gwe have/ba∇dblZI/ba∇dbl2=/ba∇dblZ∗
IZI/ba∇dbl≤2δ2log(nKρ/δ ⋆) = 2δ2/ν2as
/ba∇dblII,I(Ψ∗
IΨI)−1/ba∇dbl=/ba∇dbl(ΨE−Z)∗
IΨI(Ψ∗
IΨI)−1/ba∇dbl
=/ba∇dblEI,I−Z∗
I(Ψ†
I)∗/ba∇dbl≤ε2/2+δ/ν·√
2≤√
2δ/ν·(ν/2+1)≤2δ/ν.(67)
We next split the second term using (49), apply Theorem 5 with the abbreviation Γ =
min{7/3,2δ/ν}and use Corollary 6(b) or rather (57) and (55) to get
/vextenddouble/vextenddoubleEG/bracketleftbig
D−1√πR∗
I(Ψ∗
IΨI)−1I∗
I,III,I(Ψ∗
IΨI)−1RID−1√π/bracketrightbig/vextenddouble/vextenddouble
≤/vextenddouble/vextenddoubleE/bracketleftbig
D−1√πR∗
II∗
I,III,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble
+2/vextenddouble/vextenddoubleEG/bracketleftbig
D−1√πR∗
II∗
I,I·II,I(Ψ∗
IΨI)−1·HI,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble
+/vextenddouble/vextenddoubleEG/bracketleftbig
D−1√πR∗
IHI,I·(Ψ∗
IΨI)−1I∗
I,III,I(Ψ∗
IΨI)−1·HI,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble
≤/vextenddouble/vextenddoubleE/bracketleftbig
D−1√πR∗
II∗
I,III,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble
+2/vextenddouble/vextenddoubleE/bracketleftbig
D−1√πR∗
II∗
I,III,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble1/2·Γ·/vextenddouble/vextenddoubleE/bracketleftbig
D−1√πR∗
IHI,IHI,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble1/2
+/vextenddouble/vextenddoubleE/bracketleftbig
D−1√πR∗
IHI,IHI,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble·Γ2
=/parenleftig/vextenddouble/vextenddoubleE/bracketleftbig
D−1√πR∗
II∗
I,III,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble1/2+Γ/vextenddouble/vextenddoubleE/bracketleftbig
D−1√πR∗
IHI,IHI,IRID−1√π/bracketrightbig/vextenddouble/vextenddouble1/2/parenrightig2
≤2/parenleftbig
/ba∇dblΦD√π/ba∇dbl·δ2/2+/ba∇dblZD√π/ba∇dbl+min{7/3,2δ/ν}·/ba∇dblΨD√π/ba∇dbl/parenrightbig2. (68)
Substituting (65) and (68) into (64) and this in turn into (61 ) yields
/ba∇dblE[ˆY]/ba∇dbl≤2ρ·[P(Hc)+P(Gc)]+6/α·/parenleftbig
/ba∇dblΦD√π/ba∇dbl·δ2/2+/ba∇dblZD√π/ba∇dbl/parenrightbig
·/ba∇dblΨD√π/ba∇dbl
+8/α·/parenleftbig
/ba∇dblZD√π/ba∇dbl+δ/parenrightbig
·/ba∇dblΨD√π/ba∇dbl2
+2/α2·/parenleftbig
/ba∇dblΦD√π/ba∇dbl·δ2/2+/ba∇dblZD√π/ba∇dbl+min{7/3,2δ/ν}·/ba∇dblΨD√π/ba∇dbl/parenrightbig2.(69)
We next show that the expression above is bounded by ∆ /6. In both regimes we have by
Lemma (4) that 2 ρ·[P(Hc) +P(Gc)]≤δ⋆/16. In the ﬁrst regime, if δ > δ◦=γν2/Cand
therefore min{δ◦,δ}=δ◦, we use that δ≤√
2, max{/ba∇dblΨD√π/ba∇dbl,/ba∇dblΦD√π/ba∇dbl}≤αγν/(4C) and
thus/ba∇dblZD√π/ba∇dbl≤αγν/(2C) to get
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆
16+γν2
C·γ
16C/bracketleftbig
18α+12α+2·(16/3)2/bracketrightbig
≤δ⋆
16+δ◦·87γ
16C≤∆
6.
Conversely in the second regime, when δ≤δ◦and therefore min {δ◦,δ}=δ, we use that
/ba∇dblZD√π/ba∇dbl≤δ≤δ◦=γν2/Cand/ba∇dblΨD√π/ba∇dbl≤αγν/Cas well asν≤1/2 andα≥1−δ2
◦/2≥
17/18 to get
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆
16+δ·γ
2C/bracketleftbigg
6+6αγ2
64C2+8αγ
4C+/parenleftigγ2
64C2+1
α+2γ
C/parenrightig2/bracketrightbigg
≤δ⋆
16+δ·4γ
C≤∆
6.
22As before we arrive at the desired bound after applying the ma trix Bernstein inequality 12
fort= ∆/16 withr=3
4ρandm= ∆/6 and some simpliﬁcations.
Now we turn to bounding individual columns of the random matr ices treated in the last
two lemmas.
Lemma 9 Under the conditions of Theorem 3 we have
P/parenleftig
/ba∇dblΦA(Dπ·α·β)−1eℓ−φℓ/ba∇dbl>∆/8/parenrightig
≤28exp/parenleftbigg
−N(∆/16)2
2ρ2+ρ∆/16/parenrightbigg
.
Proof As in the matrix case the idea is to write the vector whose norm we want to
estimate as sum of independent random vectors based on the si gnalsynand use Bernstein’s
inequality. To this end we deﬁne for a ﬁxed index ℓthe random vectors
ˆYn:=/bracketleftbig
yny∗
n(Ψ†
ˆIn)∗RˆIn(Dπ·α·β)−1· /BDB(yn)(ˆIn)−Φ/bracketrightbig
eℓ,
Yn:=/bracketleftbig
yny∗
n(Ψ†
In)∗RIn(Dπ·α·β)−1· /BDB(yn)(In)−Φdiag(1In)D−1
π/bracketrightbig
eℓ.
Notethatwecanobtain ˆYn,YnbymultiplyingtheanaloguematricesintheproofofLemma7
from the right by D−1√πeℓ. Following the proof strategy of Lemma 7 with the necessary
changes, we ﬁrst bound the ℓ2-norm of the random vectors ˆYn,Ynas
max{/ba∇dblˆYn/ba∇dbl,/ba∇dblYn/ba∇dbl}≤κS2c2
max/ba∇dbl/ba∇dblD−1
π/ba∇dbl/ba∇dblD−1
β/ba∇dbl/ba∇dblD−1
α/ba∇dbl+S/ba∇dblD−1
π/ba∇dbl≤3ρ/4 =:r.
Repeating the procedures in (47) we next get
/ba∇dblE[ˆY]/ba∇dbl≤[2P(Hc)+P(Gc)]·ρ+/ba∇dblE[ /BDG(I)Y]/ba∇dbl≤δ⋆/32+/ba∇dblE[ /BDG(I)Y]/ba∇dbl,(70)
while repeating the procedures in (48)/(61), and using the e xpression from (52) Φ∗
I(Ψ†
I)∗=
(Dα)I,I+II,I(Ψ∗
IΨI)−1= (Dα)I,I+II,I+II,I(Ψ∗
IΨI)−1HI,Iyields
/ba∇dblE[ /BDG(I)Y]/ba∇dbl=/vextenddouble/vextenddoubleEG/bracketleftbig
ΦR∗
I/parenleftbig
Φ∗
IΨ†∗
I−(Dα)I,I/parenrightbig
RIeℓ/bracketrightbig/vextenddouble/vextenddouble/(αℓπℓ)
≤/vextenddouble/vextenddoubleEG/bracketleftbig
ΦR∗
III,I(Ψ∗
IΨI)−1RIeℓ/bracketrightbig/vextenddouble/vextenddouble/(απℓ)
≤/parenleftbig/vextenddouble/vextenddoubleEG/bracketleftbig
ΦR∗
III,IRIeℓ/bracketrightbig/vextenddouble/vextenddouble+/vextenddouble/vextenddoubleEG/bracketleftbig
ΦR∗
III,I(Ψ∗
IΨI)−1HI,IRIeℓ/bracketrightbig/vextenddouble/vextenddouble/parenrightbig
/(απℓ).(71)
We next use the decomposition I=Iℓc+eℓe∗
ℓ. Note that for any diagonal matrix Dwe
haveIℓcDeℓ= 0 and that for any matrix Vℓℓ=eℓVeℓ. SinceII,Ihas a zero diagonal and
we haveII,I=RI(ΨE−Z)∗ΨR∗
I=RI(ΦE−ZDα)∗ΨR∗
I, Corollary 6(c) yields for the ﬁrst
term
/ba∇dblΦEG/bracketleftbig
R∗
III,IRIeℓ/bracketrightbig/vextenddouble/vextenddouble=/ba∇dblEG/bracketleftbig
ΦIℓcR∗
IRI(ΦE−ZDα)∗/BDI(ℓ)/bracketrightbig
·ψℓ/vextenddouble/vextenddouble
≤πℓ·/ba∇dblΦIℓcD√π/ba∇dbl·/ba∇dbl(ΦE−ZDα)D√π/ba∇dbl·/ba∇dblψℓ/vextenddouble/vextenddouble
≤πℓ·/ba∇dblΦD√π/ba∇dbl·(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl). (72)
Before we estimate the second term with the same split, note t hat applying Corollary 6(d)
toH,I,I∗and using that max {/ba∇dblΨD√π/ba∇dbl,/ba∇dblΦD√π/ba∇dbl}≤ν/C≤1/8 yields the following three
23bounds, derived in detail in Appendix B.2,
/ba∇dblE[D−1√πIℓcR∗
IHI,IH∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl≤9·πℓ·max{µ(Ψ),/ba∇dblΨD√π/ba∇dbl}2,(73)
/ba∇dblE[D−1√πIℓcR∗
III,II∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl≤9·πℓ·δ2, (74)
/ba∇dblE[D−1√πIℓcR∗
II∗
I,III,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl≤9·πℓ·δ2. (75)
Further, recall that HI,ℓ=RIHeℓandIℓ,I=e∗
ℓIR∗
I. So applying Theorem 5 and using
(74) as well as twice Corollary 6(c), we get
/vextenddouble/vextenddoubleEG/bracketleftbig
ΦR∗
III,I(Ψ∗
IΨI)−1HI,IRIeℓ/bracketrightbig/vextenddouble/vextenddouble
≤/ba∇dblφℓ/ba∇dbl·/vextenddouble/vextenddoubleEG/bracketleftbig
Iℓ,I·(Ψ∗
IΨI)−1·HI,ℓ
/BDI(ℓ)/bracketrightbig/vextenddouble/vextenddouble
+/ba∇dblΦD√π/ba∇dbl·/vextenddouble/vextenddoubleEG/bracketleftbig
D−1√πIℓcII,I·(Ψ∗
IΨI)−1·HI,ℓ
/BDI(ℓ)/bracketrightbig/vextenddouble/vextenddouble
≤/parenleftig
/ba∇dblE[Iℓ,II∗
ℓ,I
/BDI(ℓ)]/ba∇dbl1/2+/ba∇dblΦD√π/ba∇dbl·/ba∇dblE[D−1√πIℓcII,II∗
I,IIℓcD−1√π
/BDI(ℓ)]/ba∇dbl1/2/parenrightig
·4/3·/ba∇dblE[H∗
I,ℓHI,ℓ
/BDI(ℓ)]/ba∇dbl1/2
≤(√πℓ·/ba∇dble∗
ℓID√π/ba∇dbl+/ba∇dblΦD√π/ba∇dbl·√πℓ·3δ)·4/3·√πℓ·/ba∇dble∗
ℓHD√π/ba∇dbl
≤4/3·πℓ·δ·(/ba∇dblΨD√π/ba∇dbl+3/ba∇dblΦD√π/ba∇dbl)·/ba∇dblΨD√π/ba∇dbl. (76)
Plugging the last two bounds back into (71) and (70) yields
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆/32+1/α·/ba∇dblΦD√π/ba∇dbl·(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl)
+4/3·δ/α·/ba∇dblΨD√π/ba∇dbl·(/ba∇dblΨD√π/ba∇dbl+3/ba∇dblΦD√π/ba∇dbl).
We next proceed as in the last proof. We always have /ba∇dblΦD√π/ba∇dbl≤αγν/(4C). Ifδ > δ◦,
thus min{δ,δ◦}=δ◦, we use that additionally we have /ba∇dblΨD√π/ba∇dbl≤αγν/(4C) and/ba∇dblZD√π/ba∇dbl≤
αγν/(2C) to get
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆
32+γν2
C·αγ
16C·(1+2+4/3·√
2·4)≤δ⋆
32+δ◦·11αγ
16C≤∆
16.
Conversely, if δ≤δ◦, thus min{δ,δ◦}=δ, we use that/ba∇dblZD√π/ba∇dbl≤δtogether with/ba∇dblΨD√π/ba∇dbl≤
αγν/Candν≤1/3 to get
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆
32+δ·γν
4C·/parenleftbiggαγν
4C·δ/2+1+4/3·7αγν
C/parenrightbigg
≤δ⋆
32+δ·2γν
4C≤∆
16.
Finally an application of the vector Bernstein inequality 1 2 fort=m= ∆/16 andr= 3ρ/4
and some simpliﬁcations yield the desired bound.
Now to the grand ﬁnal, showing that also the fourth inequalit y used in the proof of the
main theorem is satisﬁed with high probability.
Lemma 10 Under the conditions of Theorem 3 for Λ = max/braceleftbig
/ba∇dblΨD√π/ba∇dbl,αγν
4C/bracerightbig
P/parenleftbigg
Λ·/ba∇dblIℓc(D√π·α)−1B(Dπ·α·β)−1eℓ/ba∇dbl>3∆
16/parenrightbigg
≤28exp/parenleftbigg
−N(∆/16)2
2ρ2+ρ∆/16/parenrightbigg
.
24ProofAs usual we rewrite the vector to bound as sum of random vector s based on the
signalsynand use Bernstein’s inequality. Thus we deﬁne
ˆYn: = Λ·Iℓc(D√π·α)−1R∗
ˆInΨ†
ˆInyny∗
nΨ†∗
ˆInRˆIn(Dπ·α·β)−1/BDB(yn)(ˆIn)eℓ
and its counterpart Ynby simply replacing in the above ˆInbyIn. Since for any diagonal
matrixDwe have IℓcDeℓ= 0 we can again obtain ˆYn,Ynfrom the corresponding matrices
in the proof of Lemma 8, this time by multiplying from the left byIℓcand from the right
byD−1√πeℓ. Following the usual proof strategy we ﬁrst bound the ℓ2-norm of the random
vectorsˆYn,Ynas
max{/ba∇dblˆYn/ba∇dbl,/ba∇dblYn/ba∇dbl}≤κ2/ba∇dbly/ba∇dbl2/ba∇dblD−2
α/ba∇dbl/ba∇dblD−1
β/ba∇dbl/ba∇dblD−3/2
π/ba∇dbl≤3ρ/4 =:r,
while for the expectation we get similar to (48) and (62)
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆/32+Λ·π−1
ℓ·/ba∇dblD−2
α/ba∇dbl·/vextenddouble/vextenddoubleEG/bracketleftbig
D−1√πIℓcR∗
I/parenleftbig
Ψ†
IΦIΦ∗
IΨ†∗
I/parenrightbig
RIeℓ/bracketrightbig/vextenddouble/vextenddouble.(77)
Using a decomposition as in (63) we get
IℓcR∗
I/parenleftbig
Ψ†
IΦIΦ∗
IΨ†∗
I/parenrightbig
RIeℓ=−IℓcR∗
III,I(Ψ∗
IΨI)−1RIeℓ−IℓcR∗
I(Ψ∗
IΨI)−1I∗
I,IRIeℓ
+IℓcR∗
I(Ψ∗
IΨI)−1I∗
I,III,I(Ψ∗
IΨI)−1RIeℓ.(78)
The expectation correspondingto the ﬁrstterm can beobtain ed by replacing Φ with D−1√πIℓc
in (71). Going through (72) and (76) with the same change yiel ds
/ba∇dblEG[D−1√πIℓcR∗
III,I(Ψ∗
IΨI)−1RIeℓ]/ba∇dbl
≤πℓ·(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl+4δ·/ba∇dblΨD√π/ba∇dbl). (79)
To estimate the second term note that I∗
I,IRIeℓ=RIIℓcI∗eℓ
/BDI(ℓ) sinceIhas a zero
diagonal. So using the identity (50), Theorem 5, Corollary 6 (c) and (d) or rather (73) we
get for the second term
/ba∇dblEG[D−1√πIℓcR∗
I(Ψ∗
IΨI)−1I∗
I,IRIeℓ]/ba∇dbl
≤/ba∇dblEG[D−1√πIℓcR∗
I(Ψ∗
IΨI)−1RIIℓcD−1√π
/BDI(ℓ)]/ba∇dbl·/ba∇dblD√πI∗eℓ/ba∇dbl
≤ε·/ba∇dblΨD√π/ba∇dbl·/parenleftbig
/ba∇dblEG[D−1√πIℓcR∗
IRIIℓcD−1√π
/BDI(ℓ)]/ba∇dbl
+/ba∇dblEG[D−1√πIℓcR∗
IHI,I·(Ψ∗
IΨI)−1·IℓcRID−1√π
/BDI(ℓ)]/ba∇dbl/parenrightbig
≤δ·/ba∇dblΨD√π/ba∇dbl·/parenleftbig
πℓ+/ba∇dblE[D−1√πIℓcR∗
IHI,IH∗
I,IRIIℓcD−1√π
/BDI(ℓ)]/ba∇dbl1/2·4/3·√πℓ/parenrightbig
≤πℓ·δ·/ba∇dblΨD√π/ba∇dbl·/parenleftbig
1+4·max{µ(Ψ),/ba∇dblΨD√π/ba∇dbl}). (80)
As probably feared the third term in (78) requires further de composition. Again we split
the inverse (Ψ∗
IΨI)−1into (Ψ∗
IΨI)−1=I+ (Ψ∗
IΨI)−1HI,I=I+HI,I(Ψ∗
IΨI)−1. Recalling
that forI∈Gwe have/ba∇dblII,I(Ψ∗
IΨI)−1/ba∇dbl≤min{7/3,2δ/ν}= Γ and applying Theorem 5 to
25all four resulting terms yields
/ba∇dblEG[D−1√πIℓcR∗
II∗
I,I·II,IRIeℓ]/ba∇dbl
≤/ba∇dblE[D−1√πIℓcR∗
II∗
I,III,IRIIℓcD−1√π
/BDI(ℓ)]/ba∇dbl1/2·/ba∇dblE[I∗
I,ℓII,ℓ
/BDI(ℓ)]/ba∇dbl1/2
/ba∇dblEG[D−1√πIℓcR∗
IHI,I·(Ψ∗
IΨI)−1I∗
I,I·II,IRIeℓ]/ba∇dbl
≤/ba∇dblE[D−1√πIℓcR∗
IHI,IH∗
I,IRIIℓcD−1√π
/BDI(ℓ)]/ba∇dbl1/2·Γ·/ba∇dblE[I∗
I,ℓII,ℓ
/BDI(ℓ)]/ba∇dbl1/2
/ba∇dblEG[D−1√πIℓcR∗
II∗
I,I·II,I(Ψ∗
IΨI)−1·HI,IRIeℓ]/ba∇dbl
≤/ba∇dblE[D−1√πIℓcR∗
II∗
I,III,IRIIℓcD−1√π
/BDI(ℓ)]/ba∇dbl1/2·Γ·/ba∇dblE[H∗
I,ℓHI,ℓ
/BDI(ℓ)]/ba∇dbl1/2
/ba∇dblEG[D−1√πIℓcR∗
IHI,I·(Ψ∗
IΨI)−1I∗
I,III,I(Ψ∗
IΨI)−1·HI,IRIeℓ]/ba∇dbl
≤/ba∇dblE[D−1√πIℓcR∗
IHI,IH∗
I,IRIIℓcD−1√π
/BDI(ℓ)]/ba∇dbl1/2·Γ2·/ba∇dblE[H∗
I,ℓHI,ℓ
/BDI(ℓ)]/ba∇dbl1/2
Bounding the terms on the left hand side via Corollary 6(d) or rather (73) and (75) and
the terms on the right hand side via Corollary 6(c) we get
/ba∇dblEG[D−1√πIℓcR∗
I(Ψ∗
IΨI)−1I∗
I,III,I(Ψ∗
IΨI)−1RIeℓ]/ba∇dbl
≤πℓ·/parenleftbig
3δ+3Γmax{µ(Ψ),/ba∇dblΨD√π/ba∇dbl}/parenrightbig/parenleftbig
/ba∇dble∗
ℓID√π/ba∇dbl+Γ/ba∇dble∗
ℓHD√π/ba∇dbl/parenrightbig
≤πℓ·/parenleftbig
3δ+3Γmax{µ(Ψ),/ba∇dblΨD√π/ba∇dbl}/parenrightbig/parenleftbig
/ba∇dblΦD√π/ba∇dblε2/2+/ba∇dblZD√π/ba∇dbl+Γ/ba∇dblΨD√π/ba∇dbl/parenrightbig
.(81)
Finally substituting (79)-(81) into (77) yields
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆
32+Λ
α2/parenleftig
/ba∇dblΦD√π/ba∇dblε2
2+/ba∇dblZD√π/ba∇dbl+δ/ba∇dblΨD√π/ba∇dbl/parenleftbig
5+4max{µ(Ψ),/ba∇dblΨD√π/ba∇dbl}/parenrightbig/parenrightig
+3Λ
α2/parenleftig
δ+Γmax{µ(Ψ),/ba∇dblΨD√π/ba∇dbl}/parenrightig/parenleftig
/ba∇dblΦD√π/ba∇dblε2
2+/ba∇dblZD√π/ba∇dbl+Γ/ba∇dblΨD√π/ba∇dbl/parenrightig
.
As before we distinguish between δ > δ◦, where min{δ,δ◦}=δ◦=γν2/Cand we can use
/ba∇dblΨD√π/ba∇dbl≤αγν/(4C) = Λ,µ(Ψ)≤αγν2/(4C) as well as/ba∇dblZD√π/ba∇dbl≤αγν/(2C) and Γ≤7/3,
to get
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆
32+γ2ν2
16C2/bracketleftig
3+√
2/parenleftig
5+αγν
C/parenrightig
+16/parenleftig√
2+7αγν
12C/parenrightig/bracketrightig
≤δ⋆
32+δ◦36γ
16C≤∆
16,
andδ≤δ◦, where min{δ,δ◦}=δand we can use/ba∇dblΨD√π/ba∇dbl≤αγν/C= Λ andµ(Ψ)≤
9ν2/(4C), meaning max{µ(Ψ),/ba∇dblΨD√π/ba∇dbl}≤ν≤1 as well as/ba∇dblZD√π/ba∇dbl≤δand Γ≤2δ/ν, to
again get
/ba∇dblE[ˆY]/ba∇dbl≤δ⋆
32+δγν
αC/bracketleftig
1+9δ+9αγ
8C/parenleftbig
8ν+δν+δ2ν+16δ/parenrightbig/bracketrightig
≤δ⋆
32+δ2γν
αC≤∆
16.
As before an application of the vector Bernstein inequality 12 fort=m= ∆/16 and
r= 3ρ/4 and some simpliﬁcations yield the desired bound.
Note that for the probability bounds in the last four lemmas, we have used the simpliﬁed
version of the vector/matrix Bernstein inequality from [42 ], stated in 12, which bounds
the variance appearing in the denominator via the quantity r. Using the same approach as
above one could estimate terms of the form /ba∇dbl˜Yn˜Y∗
n/ba∇dbland/ba∇dbl˜Y∗
n˜Yn/ba∇dbl, where˜Ynis the centered
version ofYn, to improve the bounds on the variance and thus lower the numb er of training
signals required in the main theorem, [21, 42].
26Appendix B. Technical results
For convenience we ﬁrst restate several bounds for random ma trices used in the proofs of
Lemma 4 as well as Lemma 7-10. We then provide the proofs of Lem ma 5 and Corollary 6
together with the detailed derivation of (55-57) and (73-75 ).
B.1 Random matrix bounds
We ﬁrst recall two bounds for sums of random matrices and vect ors.
Theorem 11 (Matrix Chernoﬀ inequality [42]) LetX1,...,XNbe independent random
positive semi-deﬁnite matrices taking values in Rd×d. Assume that for all n∈{1,...,N},
/ba∇dblXn/ba∇dbl≤ηa.s. and/ba∇dbl/summationtextN
n=1E[Xn]/ba∇dbl≤µmax. Then, for all t≥eµmax,
P/parenleftigg
/ba∇dblN/summationdisplay
n=1Xn/ba∇dbl≥t/parenrightigg
≤K/parenleftigeµmax
t/parenrightigt
η.
Theorem 12 (Matrix resp. vector Bernstein inequality [42, 21]) Consider a sequence
Y1,...,YNof independent, random matrices (resp. vectors) with dimen siond×K(resp.d).
Assume that each random matrix (resp. vector) satisﬁes
/ba∇dblYn/ba∇dbl≤ra.s. and/ba∇dblE[Yn]/ba∇dbl≤m.
Then, for all t>0,
P/parenleftigg
/ba∇dbl1
NN/summationdisplay
n=1Yn/ba∇dbl≥m+t/parenrightigg
≤κexp/parenleftbigg−Nt2
2r2+(r+m)t/parenrightbigg
, (82)
whereκ=d+Kfor the matrix Bernstein inequality and κ= 28for the vector Bernstein
inequality.
Next we recall Theorem 3.1 from [31] which allows us to contro l the operator norm of a
submatrix with high probability.
Theorem 13 (Operator norm of a random submatrix [31]) LetΨbe a dictionary
and assume I⊆Kis chosen according to the rejective sampling model with pro babilities
p1,...,pKsuch that/summationtextK
i=1pi=S. Further let Dpdenote the diagonal matrix with the
vectorpon its diagonal. Then
P(/ba∇dblΨ∗
IΨI− /C1/ba∇dbl>ϑ)≤216Kexp/parenleftbigg
−min/braceleftbiggϑ2
4e2/ba∇dblΨDpΨ∗/ba∇dbl,ϑ
2µ(Ψ)/bracerightbigg/parenrightbigg
.
To prove Theorem 5 we ﬁrst state and prove the following lemma to bound sums of
products of matrices.
Lemma 14 (Sums of products of matrices [9], [16]) LetAn∈Rd1×d2,Bn∈Rd2×d3,
Cn∈Rd3×d4. Then
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay
n=1AnBnCn/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay
n=1AnA∗
n/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1/2
max
n/ba∇dblBn/ba∇dbl/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleN/summationdisplay
n=1C∗
nCn/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1/2
.
27ProofWrite
N/summationdisplay
n=1AnBnCn=
A1A2A3.
. .
. .
. .

B1. . .
. B2
. B 3
. .

C1. . .
C2.
C3.
. .
.
Now the result immediately follows by applying the followin g properties of the operator
norm/ba∇dblABC/ba∇dbl≤/ba∇dblA/ba∇dbl/ba∇dblB/ba∇dbl/ba∇dblC/ba∇dbl,/ba∇dblA/ba∇dbl=/ba∇dblAA∗/ba∇dbl1/2and/ba∇dblC/ba∇dbl=/ba∇dblC∗C/ba∇dbl1/2.
Lemma 5 is a straightforward consequence of the result above .
Proof[of Lemma 5] We want to show that for random matrices A(I)∈Rd1×d2,B(I)∈
Rd2×d3,C(I)∈Rd3×d4, whereIis a discrete random variable taking values in IandG⊆I
with max I∈G/ba∇dblB(I)/ba∇dbl≤Γ we have
/ba∇dblE[A(I)·B(I)·C(I)· /BDG(I)]/ba∇dbl≤/ba∇dblE[A(I)A(I)∗]/ba∇dbl1/2·Γ·/ba∇dblE[C(I)∗C(I)]/ba∇dbl1/2.
Rewriting the expectation as a sum and applying the lemma abo ve yields
/ba∇dblE[A(I)B(I)C(I) /BDG(I)]/ba∇dbl=/ba∇dbl/summationdisplay
I∈GP[I]1/2A(I)B(I)C(I)P[I]1/2/ba∇dbl
≤/ba∇dbl/summationdisplay
I∈GP[I]A(I)A(I)∗/ba∇dbl1/2·Γ·/ba∇dbl/summationdisplay
I∈GP[I]C(I)∗C(I)/ba∇dbl1/2
≤/ba∇dblE[A(I)A(I)∗]/ba∇dbl1/2·Γ·/ba∇dblE[C(I)∗C(I)]/ba∇dbl1/2,
where in the last inequality we have used that the matrices A(I)A(I)∗andC(I)∗C(I) are
positive semideﬁnite and that P[I]≥0.
B.2 Proof of Corollary 6
Finally, we will turn to the proof of Corollary 6. Note that in the uniformly distributed
support model, which is equivalent to rejective sampling wi th uniform weights pi=S/K,
most estimates become trivial, since we have πi=P(i∈I) =S/K=piandP({i,j}⊆I) =
S(S−1)
K(K−1). This means that for a zero diagonal matrix Iwe have
E[R∗
III,IRI] =E[I⊙(1I1∗
I)] =I⊙E[1I1∗
I] =I·S(S−1)
K(K−1),
so we simply get
/ba∇dblE[D−1√πR∗
III,IRID−1√π]/ba∇dbl=/ba∇dblI/ba∇dbl·S−1
K−1≤/ba∇dblI/ba∇dbl·S
K=/ba∇dblD√πID√π/ba∇dbl.
Unfortunately, in the rejective sampling model with non-un iform weights pi, these estimates
become much more involved. In particular, we will heavily re ly on the following theorem,
collecting results from [32].
28Theorem 15 LetPBbe the probability measure corresponding to the Poisson sam pling
model with weights pi<1andPSbe the probability measure corresponding to the associated
rejective sampling model with parameter S,PS(I) =PB(I||I|=S), as in Deﬁnition 1.
Further denote by ESthe expectation with respect to PSand byπSthe vector of ﬁrst order
inclusion probabilities of level S, meaningπS(i) =PS(i∈I)or equivalently πS=ES(1I).
We have
(1−/ba∇dblp/ba∇dbl∞)·pi≤πS(i)≤2·pi,if/summationtext
kpk=S, (a)
πS−1(i)≤πS(i), (b)
PS({i,j}⊆I)≤πS(i)·πS(j),ifi/ne}ationslash=j. (c)
Further, deﬁning for L⊆[K]with|L|<Sthe setL={I⊆[K] :L⊆I}, we have
ES/bracketleftbig
1I\L1∗
I\L· /BDL(I)/bracketrightbig
·/productdisplay
ℓ∈L[1−πS(ℓ)]/√∇ecedesequalES−|L|[1I1∗
I]·/productdisplay
ℓ∈LπS(ℓ). (d)
Finally, ifπ:=πSsatisﬁes/ba∇dblπ/ba∇dbl∞<1, then for any K×KmatrixAwe have
/ba∇dblA⊙E[1I1∗
I]/ba∇dbl≤1+/ba∇dblπ/ba∇dbl∞
(1−/ba∇dblπ/ba∇dbl∞)2·/ba∇dblDπ[A−diag(A)]Dπ/ba∇dbl+/ba∇dbldiag(A)Dπ/ba∇dbl.(e)
With these results in hand we can ﬁnally prove Corollary 6.
Proof[of Corollary 6 including (55-57) and (73-75)]
(a)We want to show that for a matrix Iwith zero diagonal we have
/ba∇dblE[D−1√πR∗
III,IRID−1√π]/ba∇dbl≤3·/ba∇dblD√πID√π/ba∇dbl.
Using the identities AI,I=RIAR∗
IandR∗
IRI= diag(1I), we can rewrite for a general
matrixAand a diagonal matrix D
DR∗
IAI,IRID=DR∗
IRIAR∗
IRID=Ddiag(1I)Adiag(1I)D
= diag(1I)DADdiag(1I) = (DAD)⊙(1I1∗
I).
Using Theorem 15(e) and /ba∇dblπ/ba∇dbl∞≤1/3 we therefore get for Iwith zero-diagonal
/ba∇dblE[D−1√πR∗
III,IRID−1√π]/ba∇dbl=/ba∇dblE[(D−1√πID−1√π)⊙(1I1∗
I)]/ba∇dbl
=/ba∇dbl(D−1√πID−1√π)⊙E[1I1∗
I]/ba∇dbl
≤3/ba∇dblDπD−1√πID−1√πDπ/ba∇dbl= 3/ba∇dblD√πID√π/ba∇dbl,
which proves (a). (a)/check
(b)We want to show that for a matrix Iwith zero diagonal we have
/ba∇dblE[D−1√πR∗
III,II∗
I,IRID−1√π]/ba∇dbl≤9
2·/ba∇dblD√πID√π/ba∇dbl2+3
2·max
k/ba∇dble∗
kID√π/ba∇dbl2.
We again rewrite the expression, whose expectation we need t o estimate, as
R∗
III,II∗
I,IRI=R∗
IRI·I·R∗
IRI·I∗·R∗
IRI
= diag(1I)·I·diag(1I)·I∗·diag(1I)
= [I·diag(1I)·I∗]⊙(1I1∗
I)
=/parenleftbig/summationdisplay
k∈IIkI∗
k/parenrightbig
⊙(1I1∗
I) =/summationdisplay
k∈I(IkI∗
k)⊙(1I1∗
I).
29Since thek-th entry of Ikand therefore both the k-th row and k-th column of IkI∗
kare
zero, we have ( IkI∗
k)⊙(1I1∗
I) = (IkI∗
k)⊙(1I\{k}1∗
I\{k}), yielding
R∗
III,II∗
I,IRI=/summationdisplay
k∈I(IkI∗
k)⊙(1I\{k}1∗
I\{k})
=/summationdisplay
k( /BDI(k)·IkI∗
k)⊙(1I\{k}1∗
I\{k})
=/summationdisplay
k(IkI∗
k)⊙(1I\{k}1∗
I\{k}· /BDI(k)). (83)
UsingtheSchurProductTheorem, whichsaysthatforp.s.dma tricesA,P,¯P, withPij,¯Pij≥
0 andP/√∇ecedesequal¯Pwe haveA⊙P/√∇ecedesequalA⊙¯P, together with Theorem 15(d) further leads to
ES/bracketleftbig
R∗
III,II∗
I,IRI/bracketrightbig
=/summationdisplay
k(IkI∗
k)⊙ES/bracketleftbig
1I\{k}1∗
I\{k}· /BDI(k)/bracketrightbig
/√∇ecedesequal/summationdisplay
k(IkπS(k)
1−πS(k)I∗
k)⊙ES−1[1I1∗
I]
=/parenleftbig/summationdisplay
kIkπS(k)
1−πS(k)I∗
k/parenrightbig
⊙ES−1[1I1∗
I]
= (Idiag(πS
1−πS)I∗)⊙ES−1[1I1∗
I].
Abbreviating M:=Idiag(πS
1−πS)I∗, and using Theorem 15 (e) and (b) we get
/ba∇dblES/bracketleftbig
D−1√πR∗
III,II∗
I,IRID−1√π/bracketrightbig
/ba∇dbl=/ba∇dblD−1√πES/bracketleftbig
R∗
III,II∗
I,IRI/bracketrightbig
D−1√π/ba∇dbl
≤/ba∇dbl(D−1√πMD−1√π)⊙ES−1[1I1∗
I]/ba∇dbl
≤3/ba∇dblDπS−1[D−1√πMD−1√π−diag(D−1√πMD−1√π)]DπS−1/ba∇dbl
+/ba∇dbldiag(D−1√πMD−1√π)DπS−1/ba∇dbl
≤3/ba∇dblD√πMD√π−diag(D√πMD√π)/ba∇dbl+/ba∇dbldiag(M)/ba∇dbl
≤3/ba∇dblD√πMD√π/ba∇dbl+/ba∇dbldiag(M)/ba∇dbl,
where in last inequality we have used that D√πMD√πis positive semideﬁnite. Combining
the inequality above with the bounds
/ba∇dblD√πMD√π/ba∇dbl=/ba∇dblD√πID√πdiag(1
1−π)D√πI∗D√π/ba∇dbl≤(1−/ba∇dblπ/ba∇dbl∞)−1/ba∇dblD√πID√π/ba∇dbl2,
/ba∇dbldiag(M)/ba∇dbl= max
ke∗
kID√πdiag(1
1−π)D√πI∗ek≤(1−/ba∇dblπ/ba∇dbl∞)−1max
k/ba∇dble∗
kID√π/ba∇dbl2,
and our assumption that /ba∇dblπ/ba∇dbl∞≤1/3 leads to (b). (b)/check
(55-57) We next specialise the general inequality from (b) to the thr ee concrete matrices
needed in the proofs of Lemma 7-10. For the case H=I−Ψ∗Ψ, note that since D√πΨ∗ΨD√π
is a positive semideﬁnite matrix we have
/ba∇dblD√πHD√π/ba∇dbl=/ba∇dblD√πΨ∗ΨD√π−diag(D√πΨ∗ΨD√π)/ba∇dbl≤/ba∇dblD√πΨ∗ΨD√π/ba∇dbl=/ba∇dblΨD√π/ba∇dbl2.
30Further note that the k-th entry of e∗
kHis zero, meaning
/ba∇dble∗
kHD√π/ba∇dbl=/ba∇dbl(e∗
k−ψ∗
kΨ)D√π/ba∇dbl≤/ba∇dblψ∗
kΨD√π/ba∇dbl≤/ba∇dblΨD√π/ba∇dbl.
So, as long as/ba∇dblΨD√π/ba∇dbl≤1/3, which holds in both regimes, we get
/ba∇dblE[D−1√πR∗
IHI,IH∗
I,IRID−1√π]/ba∇dbl≤9/2·/ba∇dblΨD√π/ba∇dbl4+3/2·/ba∇dblΨD√π/ba∇dbl2
≤(9/2·1/32+3/2)·/ba∇dblΨD√π/ba∇dbl2= 2/ba∇dblΨD√π/ba∇dbl2.
In caseI= (ΨE−Z)∗Ψ = (ΦE−ZDα)∗Ψ, we use
/ba∇dblD√πID√π/ba∇dbl≤/ba∇dbl(ΦE−ZDα)D√π/ba∇dbl·/ba∇dblΨD√π/ba∇dbl= (/ba∇dblΦD√πE+ZD√πDα/ba∇dbl)·/ba∇dblΨD√π/ba∇dbl
≤(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl)·/ba∇dblΨD√π/ba∇dbl.
Further sinceEkk=/an}b∇acketle{tψk,zk/an}b∇acket∇i}htandI−ψkψ∗
kis an orthogonal projection we have
/ba∇dble∗
kID√π/ba∇dbl≤/ba∇dblψk/an}b∇acketle{tψk,zk/an}b∇acket∇i}ht−zk/ba∇dbl·/ba∇dblΨD√π/ba∇dbl
=/ba∇dbl(ψkψ∗
k−I)zk/ba∇dbl·/ba∇dblΨD√π/ba∇dbl≤/ba∇dblzk/ba∇dbl·/ba∇dblΨD√π/ba∇dbl≤ε·/ba∇dblΨD√π/ba∇dbl.
So, using that ε≤√
2, as long as/ba∇dblΦD√π/ba∇dbl≤1, we get
2·/ba∇dblE[D−1√πR∗
III,II∗
I,IRID−1√π]/ba∇dbl
≤(9/ba∇dblΦD√π/ba∇dbl2·ε4/4+9ε2/ba∇dblZD√π/ba∇dbl+9/ba∇dblZD√π/ba∇dbl2+3ε2)·/ba∇dblΨD√π/ba∇dbl2.
≤(9·ε2/2+18ε/ba∇dblZD√π/ba∇dbl+9/ba∇dblZD√π/ba∇dbl2+3ε2)·/ba∇dblΨD√π/ba∇dbl2
≤(3/ba∇dblZD√π/ba∇dbl+3ε)2·/ba∇dblΨD√π/ba∇dbl2.
Finally note that /ba∇dblD√πID√π/ba∇dbl=/ba∇dblD√πI∗D√π/ba∇dbl. Combining this with the bound
/ba∇dble∗
kI∗D√π/ba∇dbl≤/ba∇dblψk/ba∇dbl·/ba∇dbl(ΦE−ZDα)D√π/ba∇dbl≤/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl. (84)
we get that again as long as /ba∇dblΨD√π/ba∇dbl≤1/3
/ba∇dblE[D−1√πR∗
II∗
I,III,IRID−1√π]/ba∇dbl≤(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl)2·(9/2·/ba∇dblΨD√π/ba∇dbl2+3/2)
≤(/ba∇dblΦD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl)2·2,
which proves the third inequality. (55-57)/check
(c)We want to show that for a subset Gof all supports of size Sand a pair of d×K
matricesW= (w1...,wK) andV= (v1,...,vK) we have
/ba∇dblE[WR∗
IRIV∗· /BDI(ℓ) /BDG(I)]/ba∇dbl≤πℓ·(/ba∇dblWD√π/ba∇dbl·/ba∇dblVD√π/ba∇dbl+/ba∇dblwℓ/ba∇dbl·/ba∇dblvℓ/ba∇dbl).
Forℓ∈Iwe can rewrite WR∗
IRIV∗=Wdiag(1I)V∗=Wdiag(1I\{ℓ})V∗+wℓv∗
ℓ. Using
this split and that by Theorem 15(c) we have P({ℓ,k}⊆I)≤πℓπkyields
/ba∇dblE[WR∗
IRIV∗· /BDI(ℓ) /BDG(I)]/ba∇dbl
≤/ba∇dblWE/bracketleftbig
diag(1I\{ℓ})· /BDI(ℓ) /BDG(I)/bracketrightbig
V∗/ba∇dbl+/ba∇dblwℓv∗
ℓ/ba∇dbl·E[ /BDI(ℓ) /BDG(I)]
≤/ba∇dblWD√π/ba∇dbl·/ba∇dblD−1√πE/bracketleftbig
diag(1I\{ℓ})· /BDI(ℓ) /BDG(I)/bracketrightbig
D−1√π/ba∇dbl·/ba∇dblD√πV∗/ba∇dbl+/ba∇dblwℓv∗
ℓ/ba∇dbl·πℓ
=/ba∇dblWD√π/ba∇dbl·/ba∇dblVD√π/ba∇dbl·max
k/\e}atio\slash=ℓπ−1
k·P({ℓ,k}⊆I∩I∈G)+/ba∇dblwℓ/ba∇dbl·/ba∇dblvℓ/ba∇dbl·πℓ
≤πℓ·(/ba∇dblWD√π/ba∇dbl·/ba∇dblVD√π/ba∇dbl+/ba∇dblwℓ/ba∇dbl·/ba∇dblvℓ/ba∇dbl)
31which completes the proof of (c). (c)/check
(d)We prove that for a general matrix Iwith zero diagonal we have
/ba∇dblE[D−1√πIℓcR∗
III,II∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl
≤πℓ
1−πℓ/parenleftbigg
3/ba∇dblD√πIeℓ/ba∇dbl2+max
kI2
kℓ+9
2/ba∇dblD√πID√π/ba∇dbl2+3
2max
k/ba∇dble∗
kID√π/ba∇dbl2/parenrightbigg
.(85)
Using (83) and the identity Iℓc= diag(1[K]\{ℓ}), we ﬁrst rewrite
IℓcR∗
III,II∗
I,IRIIℓc· /BDI(ℓ) = (R∗
III,II∗
I,IRI)⊙(1[K]\{ℓ}1∗
[K]\{ℓ})· /BDI(ℓ)
=/summationdisplay
k(IkI∗
k)⊙/parenleftbig
1I\{k,ℓ}1∗
I\{k,ℓ}· /BDI(ℓ) /BDI(k)/parenrightbig
.
As before an application of the Schur Product Theorem and The orem 15(d) leads to
ES[IℓcR∗
III,II∗
I,IRIIℓc· /BDI(ℓ)] =/summationdisplay
k(IkI∗
k)⊙ES/bracketleftbig
1I\{k,ℓ}1∗
I\{k,ℓ}· /BDI(ℓ) /BDI(k)/bracketrightbig
/√∇ecedesequalπS(ℓ)
1−πS(ℓ)(IℓI∗
ℓ)⊙ES−1[1I1∗
I]
+/summationdisplay
k/\e}atio\slash=ℓπS(ℓ)
1−πS(ℓ)/parenleftbigg
IkπS(k)
1−πS(k)I∗
k/parenrightbigg
⊙ES−2[1I1∗
I]
/√∇ecedesequalπS(ℓ)
1−πS(ℓ)/parenleftig
(IℓI∗
ℓ)⊙ES−1[1I1∗
I]+M⊙ES−2[1I1∗
I]/parenrightig
,
where again M=Idiag(πS
1−πS)I∗. Finally, applying again Theorem 15 (e) and (b) and
similar simpliﬁcations as in the proof of (b) yield
/ba∇dblES[D−1√πIℓcR∗
III,II∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl
≤πℓ
1−πℓ/parenleftbig
/ba∇dbl(D−1√πIℓI∗
ℓD−1√π)⊙ES−1[1I1∗
I]/ba∇dbl+/ba∇dbl(D−1√πMD−1√π)⊙ES−2[1I1∗
I]/ba∇dbl/parenrightbig
≤πℓ
1−πℓ(3/ba∇dblD√πIℓI∗
ℓD√π/ba∇dbl+/ba∇dbldiag(IℓI∗
ℓ)/ba∇dbl+3/ba∇dblD√πMD√π/ba∇dbl+/ba∇dbldiag(M)/ba∇dbl)
≤πℓ
1−πℓ/parenleftbig
3/ba∇dblD√πIeℓ/ba∇dbl2+max kI2
kℓ+9
2/ba∇dblD√πID√π/ba∇dbl2+3
2maxk/ba∇dble∗
kID√π/ba∇dbl2/parenrightbig
,
which together with our assumption that /ba∇dblπ/ba∇dbl∞≤1/3 leads to (d). (d)/check
(73-75) Finally, we specialise the general inequality from (d) to th e three concrete cases
needed in the proofs of Lemma 7-10. Reusing the bounds for Hfrom the corresponding
special case of (b) as well as H2
kℓ=|/an}b∇acketle{tψk,ψℓ/an}b∇acket∇i}ht|2≤µ(Ψ)2and the assumption /ba∇dblΨD√π/ba∇dbl≤1/3,
yields
/ba∇dblE[D−1√πIℓcR∗
IHI,IH∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl
≤3/2·πℓ·/parenleftbig
3·/ba∇dblΨD√π/ba∇dbl2+µ(Ψ)2+9/2·1/9·/ba∇dblΨD√π/ba∇dbl2+3/2·/ba∇dblΨD√π/ba∇dbl2/parenrightbig
≤3/2·πℓ·max{µ(Ψ)2,/ba∇dblΨD√π/ba∇dbl2}·6≤9·πℓ·max{µ(Ψ)2,/ba∇dblΨD√π/ba∇dbl2}.
32To prove the other two inequalities note that for I=W∗Vwe get
/ba∇dblE[D−1√πIℓcR∗
III,II∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl·1−πℓ
πℓ
≤3/ba∇dblD√πW∗vℓ/ba∇dbl2+max
k(w∗
kvℓ)2+9/ba∇dblD√πW∗VD√π/ba∇dbl2+3max
k/ba∇dblw∗
kVD√π/ba∇dbl2
≤/parenleftbig
max
k/ba∇dblwk/ba∇dbl2+3/ba∇dblWD√π/ba∇dbl2/parenrightbig
·/parenleftbig
max
ℓ/ba∇dblvℓ/ba∇dbl2+3/ba∇dblVD√π/ba∇dbl2/parenrightbig
. (86)
Applying this to I= (ΨE−Z)∗Ψ and reusing the bounds for the corresponding special
case of (b) with/ba∇dblΨD√π/ba∇dbl≤1/8 yields
/ba∇dblE[D−1√πIℓcR∗
III,II∗
I,IRIIℓcD−1√π· /BDI(ℓ)]/ba∇dbl
≤3/2·πℓ·/bracketleftbig
3·(/ba∇dblΨD√π/ba∇dbl·ε2/2+/ba∇dblZD√π/ba∇dbl)2+ε2/bracketrightbig
·/parenleftbig
1+3·/ba∇dblΨD√π/ba∇dbl2/parenrightbig
≤3/2·πℓ·δ2·/bracketleftbig
3·(/ba∇dblΨD√π/ba∇dbl·ε/2+1)2+1/bracketrightbig
·/parenleftbig
1+3·/ba∇dblΨD√π/ba∇dbl2/parenrightbig
≤9·πℓ·δ2.
Due to the symmetry of (86) this also proves the case of I∗. (73-75)/check
References
[1] A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Le arning sparsely used over-
complete dictionaries via alternating minimization. SIAM Journal on Optimization ,
26(4):2775–2799, 2016.
[2] A. Agarwal, A. Anandkumar, and P. Netrapalli. A clusteri ng approach to learning
sparsely used overcomplete dictionaries. IEEE Transactions on Information Theory ,
63(1):575–592, 2017.
[3] M. Aharon, M. Elad, and A.M. Bruckstein. K-SVD: An algori thm for designing over-
complete dictionaries for sparse representation. IEEE Transactions on Signal Process-
ing., 54(11):4311–4322, November 2006.
[4] S. Arora, R. Ge, and A. Moitra. New algorithms for learnin g incoherent and overcom-
plete dictionaries. In COLT 2014 (arXiv:1308.6273) , 2014.
[5] S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, eﬃcient, and neural algorithms for
sparse coding. In COLT 2015 (arXiv:1503.00778) , 2015.
[6] B. Barak, J.A. Kelner, and D. Steurer. Dictionary learni ng and tensor decomposition
via the sum-of-squares method. In STOC 2015 (arXiv:1407.1543) , 2015.
[7] E. Cand` es, J. Romberg, and T. Tao. Robust uncertainty pr inciples: exact signal
reconstruction from highly incomplete frequency informat ion.IEEE Transactions on
Information Theory , 52(2):489–509, 2006.
[8] N. Chatterji and P. Bartlett. Alternating minimization for dictionary learning with
randominitialization. In Advances in Neural Information Processing Systems 30 (NIPS
2017), 2017.
33[9] W.Dai andY.Ye. Acharacterization onsingularvalueine qualities ofmatrices. Journal
of Function Spaces , 2020:1–4, 02 2020. doi: 10.1155/2020/1657381.
[10] D.L. Donoho. Compressed sensing. IEEE Transactions on Information Theory , 52(4):
1289–1306, 2006.
[11] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Le ast angle regression. The
Annals of Statistics , 32(2):407 – 499, 2004.
[12] K. Engan, S.O. Aase, and J.H. Husoy. Method of optimal di rections for frame design.
InICASSP99 , volume 5, pages 2443 – 2446, 1999.
[13] D.J. Field and B.A. Olshausen. Emergence of simple-cel l receptive ﬁeld properties by
learning a sparse code for natural images. Nature, 381:607–609, 1996.
[14] R. Gribonval and K. Schnass. Dictionary identiﬁabilit y - sparse matrix-factorisation
vial1-minimisation. IEEE Transactions on Information Theory , 56(7):3523–3539, July
2010.
[15] R. Gribonval, R. Jenatton, and F. Bach. Sparse and spuri ous: dictionary learning
with noise and outliers. IEEE Transactions on Information Theory , 61(11):6298–6319,
2015.
[16] O. Hirzallah and F. Kittaneh. Inequalities for sums and direct sums of Hilbert space
operators. Linear Algebra and its Applications , 424(1):71–82, 2007.
[17] K. Kreutz-Delgado, J.F. Murray, B.D. Rao, K. Engan, T. L ee, and T.J. Sejnowski.
Dictionary learning algorithms for sparse representation .Neural Computations , 15(2):
349–396, 2003.
[18] M. S. Lewicki and T. J. Sejnowski. Learning overcomplet e representations. Neural
Computations , 12(2):337–365, 2000.
[19] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learn ing for matrix factorization
and sparse coding. Journal of Machine Learning Research , 11:19–60, 2010.
[20] J. Mairal, F. Bach, and J. Ponce. Task-driven dictionar y learning. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 34(4):791–804, 2012.
[21] S. Minsker. On some extensions of Bernstein’s inequali ty for self-adjoint operators.
Statistics and Probability Letters , 127:111–119, 2017.
[22] B. Natarajan. Sparse approximate solutions to linear s ystems. SIAM J. Computing ,
25(2):227–234, 1995.
[23] M.C.PaliandK.Schnass. Dictionarylearning–fromloc altowardsglobalandadaptive.
Information and Inference: A Journal of the IMA , 12(3), 2023.
[24] M.C. Pali, S. Ruetz, and K. Schnass. Average performanc e of OMP and Thresholding
under dictionary mismatch. IEEE Signal Processing Letters , 29:1077–1081, 2022.
34[25] Y. Pati, R. Rezaiifar, and P. Krishnaprasad. Orthogona l Matching Pursuit: recursive
function approximation with application to wavelet decomp osition. In Asilomar Conf.
on Signals Systems and Comput. , 1993.
[26] Q. Qu, Y. Zhai, X. Li, Y. Zhang, and Z. Zhu. Geometric anal ysis of nonconvex opti-
mization landscapes for overcomplete learning. In International Conference on Learn-
ing Representations , 2020.
[27] R. Rubinstein, M. Zibulevsky, and M. Elad. Eﬃcient impl ementation of the K-SVD
algorithm using batch orthogonal matching pursuit. Techni cal Report 40(8), CS Tech-
nion, 2008.
[28] S. Ruetz. Adapted variable density subsampling for com pressed sensing.
arXiv:2206.13796 , 2022.
[29] S. Ruetz. Compressed Sensing and Dictionary Learning with Non-Unifor m Support
Distribution . PhD thesis, University of Innsbruck, 2022.
[30] S. Ruetz. Convergence of ITKrM for dictionary learning .in preparation , 2023.
[31] S. Ruetz and K. Schnass. Submatrices with non-uniforml y selected random supports
and insights into sparse approximation. SIAM Journal on Matrix Analysis and Appli-
cations, 42(3):1268–1289, 2021.
[32] S.Ruetz andK. Schnass. Non-asymptotic boundsfor incl usion probabilities in rejective
sampling. arXiv:2212.09391 , 2022.
[33] C. RusuandB. Dumitrescu. Stagewise K-SVDto design eﬃc ient dictionaries for sparse
representations. IEEE Signal Processing Letters , 19(10):631–634, 2012.
[34] K. Schnass. On the identiﬁability of overcomplete dict ionaries via the minimisation
principle underlying K-SVD. Applied and Computational Harmonic Analysis , 37(3):
464–491, 2014.
[35] K. Schnass. Local identiﬁcation of overcomplete dicti onaries. Journal of Machine
Learning Research , 16(Jun):1211–1242, 2015.
[36] K. Schnass. Convergence radius and sample complexity o f ITKM algorithms for dic-
tionary learning. Applied and Computational Harmonic Analysis , 45(1):22–58, 2018.
[37] K. Skretting and K. Engan. Recursive least squares dict ionary learning algorithm.
IEEE Transactions on Signal Processing , 58(4):2121–2130, 2010.
[38] D. Spielman, H. Wang, and J. Wright. Exact recovery of sp arsely-used dictionaries. In
COLT 2012 (arXiv:1206.5882) , 2012.
[39] J.Sun, Q.Qu, andJ.Wright. Completedictionary recove ry over thesphereI: Overview
and geometric picture. IEEE Transactions on Information Theory , 63(2):853–884,
2017.
35[40] J.Sun,Q.Qu,andJ.Wright. Completedictionaryrecove ryoverthesphereII:Recovery
by Riemannian trust-region method. IEEE Transactions on Information Theory , 63
(2):885–915, 2017.
[41] A. M. Tillmann. On the computational intractability of exact and approximate dictio-
nary learning. IEEE Signal Processing Letters , 22(1):45–49, 2015.
[42] J. Tropp. User-friendly tail bounds for sums of random m atrices. Foundations of
Computational Mathematics , 12(4):389–434, 2012.
36