MEMORY-AUGMENTED ONLINE VIDEO ANOMALY DETECTION
Leonardo Rossi Vittorio Bernuzzi Tomaso Fontanini Massimo Bertozzi Andrea Prati
University of Parma, Department of Engineering and Architecture
ABSTRACT
The ability to understand the surrounding scene is of paramount
importance for Autonomous Vehicles (A Vs). This paper
presents a system capable to work in an online fashion, giving
an immediate response to the arise of anomalies surround-
ing the A V , exploiting only the videos captured by a dash-
mounted camera. Our architecture, called MOVAD , relies on
two main modules: a Short-Term Memory Module to extract
information related to the ongoing action, implemented by a
Video Swin Transformer (VST), and a Long-Term Memory
Module injected inside the classifier that considers also re-
mote past information and action context thanks to the use of a
Long-Short Term Memory (LSTM) network. The strengths of
MOV AD are not only linked to its excellent performance, but
also to its straightforward and modular architecture, trained
in a end-to-end fashion with only RGB frames with as less
assumptions as possible, which makes it easy to implement
and play with. We evaluated the performance of our method
on Detection of Traffic Anomaly (DoTA) dataset, a challeng-
ing collection of dash-mounted camera videos of accidents.
After an extensive ablation study, MOV AD is able to reach an
AUC score of 82.17%, surpassing the current state-of-the-art
by+2.87AUC.
Index Terms —Video Transformer, LSTM, Video Anomaly
Detection, Online V AD, Memory-Augmented Networks
1. INTRODUCTION AND RELATED WORKS
Autonomous Vehicles (A Vs) are becoming every day a re-
ality thanks to the recent enormous scientific and technical
advances. Nevertheless, safety of A Vs is still a relevant is-
sue which can jeopardize their world-wide diffusion. Increas-
ing safety of A Vs can be reached by providing vehicles with
the ability of detecting anomalous situations in a prompt way.
Their detection provides information to avoid collisions, pro-
tect pedestrians, or re-route the current travel [4]. Among the
different sensors exploited in A Vs, cameras offer rich real-
time information about the scene, but, despite all, anomalous
situation detection in real traffic scenarios is still very chal-
lenging. In particular, we still miss a formal shared model
of what an anomaly should be, because many times the risk-
iness is highly subjective. In addition, there are plenty of
possible accident classes with a very exiguous number of ex-amples to take into account compared to normal traffic situa-
tions. Lastly, the definition of time boundaries of an anomaly
is even more subjective and doubtful. Nevertheless, some at-
tempts have been made to propose a deterministic method to
define an anomaly. Fang et al. [3] labels the anomaly start
from the moment in which half part of the object involved
in the accident appears in the view. Yao et al. [15] proposed
the Detection of Traffic Anomaly (DoTA) dataset, where the
anomaly is starting the instant after which the accident is un-
avoidable. We take this last dataset as the benchmark for our
work because it is the most complete and used dataset for this
type of task in the road traffic safety context.
There have been several previous works addressing the
problem of video anomaly detection. Authors in [6] pro-
posed a Convolutional AutoEncoder (ConvAE) trained only
on normal frames with the objective of frame reconstruction.
In [9, 13], authors used Convolutional LSTM Auto-Encoder
as framework to encode appearance and motion. As noted
by [11], auto-encoder-type reconstruction methods are sensi-
tive to the amount of anomalies that occurs in the scene and
many times they require additional ad-hoc post-processing
techniques. Authors in [7] proposed AnoPred, which uses
a multi-task loss, including image intensity, optical flow,
gradient and adversarial losses for Video Anomaly Detec-
tion (V AD) by predicting a future frame. As stated in [15],
AnoPred was thought in a video surveillance context, while
videos acquired from inside a moving vehicle are more dy-
namic and difficult to predict. In [16], authors of STFE model
make a two-stage detector: a coarse detection to encode tem-
poral features with Histogram of Optical Flow (HOF) [12]
and ordinal features of frames by a CNN, and then, a fine
detection, by encoding the CNN features and spatial rela-
tionships of the objects. In FOL [15], authors try to avoid
the future prediction for the entire frame, focusing instead
on tracking actors’ positions and predicting their future loca-
tions. Conversely to us, the applicability of both last methods
are limited by the presence of actors in the Field of View
(FOV). Moreover, AnoPred [7], STFE [16] and FOL [15] rely
on supplementary input information other than RGB frames,
such as optical flow, bounding boxes and ego-motion infor-
mation of the moving camera. Our model outperforms them
by exclusively processing RGB frames, thereby highlighting
its capability to comprehend anomalies in video traffic scenes
without the need for auxiliary information. In [14], the TRNarXiv:2302.10719v2  [cs.CV]  27 Sep 2023Short-Term Memory Module(Video Swin Transformer - VST)
Reducer
Layer Norm
Linear
Dropout
Long-Term Memory Module(LSTM)
XRNN State(previous)
Layer Norm
LinearRNN State(next)
Dropout
Linear
Head
0/1s[t]
NFs[t]
Time
f[t-3]f[t-1]f[t-2]f[t]
Video Clip Length......
......Fig. 1 : The online frame-level V AD architecture. f[t]is the
frame at time t,xthe output of the Reducer, NFthe number
of frames in input to the VST, s[t]the anomaly classification
score of the frame f[t].
model couples the action detection task with both the tem-
poral dependencies modeled by a RNN and the anticipation
of the future via a temporal decoder. Like them, we exploit
an RNN to obtain temporal information, but not limiting our-
selves to this. We formulated an advanced version of memory,
differentiating between short- and long-memory through two
distinct and trained-at-same-time modules, obtaining a more
informative description of the ongoing situation, without
speculating on the future. Summarizing, the main contribu-
tions of this paper are:
• A straightforward end-to-end architecture for Online
Video Anomaly Detection (V AD) called MOVAD ,
which processes ongoing RGB frames, with as less
assumptions as possible about the ongoing action and
without any extra auxiliary information.
• A plugged-in Transformer [8] model used as backbone
for Short-Term Memory information extraction in an
online scenario. This allows the system to incorporate
the most recent temporal and spatial correlations by re-
lying only on the current and few past frames.
• The injection of a LSTM module inside the classifi-
cation head to model the Long-Term Memory and to
exploit contextual information spread across the entire
history.
• An exhaustive ablation study over the dataset DoTA [15].
Compared w.r.t. the state of the art, MOV AD shows su-
perior performance in terms of AUC.
2. MEMORY-AUGMENTED ONLINE VAD
In our system, recently observed frames and past frames are
taken into account as a source of information related to the
ongoing action and the context, respectively. In order to pro-
cess current and past frames for the Online V AD task, weequipped MOV AD of two main components: a Short-Term
Memory Module (STMM) and a Long-Term Memory Mod-
ule (LTMM) inside the classification head (see Fig. 1).
Short-Term Memory Module. In order to pre-elaborate the
new coming frame f[t]and, in conjunction, to retain near-
past spatio-temporal information f[t−1]..f[t−(NF−1)],
we chosen a transformer architecture over an RNN, to be able
to process them in a parallel fashion. We lean towards trans-
former over 3D convolutional models because of the well
demonstrated abilities to intercept long-distance interactions
in space and time [10]. We selected VST [8] as our back-
bone network for the STMM over models like ViViT [1],
due to its superior performance and more efficient compu-
tation of self-attention mechanism. Despite this, the self-
attention mechanism remains computationally intensive, par-
ticularly for long videos. Therefore, we limited the input to a
small temporal window of NF= 4frames of the video, going
from the current frame at time tto the previous frame at time
t−(NF−1). In this way, we obtain a compact representation
of the short history that will be subsequently forwarded to the
LTMM. VST takes as input a video with size NF×H×W×3,
where NF,H,Wand3correspond to the number of frames,
height, width and RGB channels, respectively. The model
internally splits the frames in non-overlapping 3D patches,
partitioning the video inNF
2×H
4×W
43D tokens, and us-
ing a 3D shifted window mechanism to obtain cross-window
connections and exploit spatio-temporal information.
Long-Term Memory Module. As will be highlighted in ab-
lation in Section 3.1, too much information about the past
(NF>4) tends to mislead the STMM, resulting in a loss
of performance. According to our hypothesis, this occurs be-
cause all frames are placed at the same level, without a possi-
bility of weighing them based on how much they belong to the
past. For this reason, we engineered a different way to inte-
grate and enrich the output latent space from the STMM with
contextual information extrapolated from the distant past. Ev-
ery time a new frame is available, we transfer the compact
representation obtained from the STMM to our LTMM, in
order to update its representation of the past. In this way,
we can maintain the focus on what is happening now, and at
same time accumulating a richer understanding of the scene,
enabling more precise classifications. In details, the output
of VST goes through Adaptive Average Pool 3D layer (Re-
ducer in Fig. 1) and, finally, enters inside the classification
head. As shown in Fig. 1, the head is composed by a series
of normalization layers, linear layers and dropout, alternat-
ing. A stacked three-cell LSTM module is inserted after the
last normalization layer to model the long-term memory. The
state, composed by three hidden h[t]and cell c[t]states, is up-
dated whenever a new frame is available. The LSTM receives
in input a features block of [B,1024] , where Bis the batch
size, and returns a block of same size together with the state
of the cells. Since the state is relatively small, the module is
very efficient and leads to a fixed and limited additional com-Fig. 2 : Performance comparison, changing the number of
frames ( NF) in input to the STMM (from 1 to 6).
Fig. 3 : Performance comparison, changing the number of
LSTM cells (from 0 to 4). “LSTM (2 cells)” corresponds to
“NF= 3” in Fig. 2.
putational cost. For each frame f[t], the classification head
outputs the anomaly classification score s[t]∈[0,1], where
0means no anomaly and 1means the frame is anomalous. A
weighted cross-entropy loss was chosen to train the model,
giving higher weight to the anomaly class reflecting the dis-
tribution of the data. We used the formula wi=e/ei, where
wiis the weight associated to the class, ethe total amount of
examples in the dataset and eithe amount of examples for the
classi.
3. EXPERIMENTAL RESULTS
Dataset . We perform our training and test on Task 1, the
frame-level V AD, of DoTA dataset [15], using only the
anomaly class and its temporal boundaries, strictly in the
online scenario.
Evaluation Metrics . We use the well-known Area Under the
Curve (AUC) metric at frame-level, to evaluate how well the
model is able to temporally locate the anomaly in the videos.
Training details. We perform the training on a single ma-chine with an A100 GPU, with Stochastic Gradient Descent
(SGD) optimization algorithm, learning rate of 0.0001, mo-
mentum of 0.9. We use SGD instead of Adam because in our
experiments the latter led the training to be more unstable, re-
sulting in model diverging after few epochs. Unless otherwise
specified, batch size is 8, input video size is 320×240, Video
Clip Length (VCL), which is the number of frames inside the
batch for each video, is 8, LSTM cell number is 2, NF is
3, linear weights are initialized using a uniform distribution,
LSTM cells with a (semi-)orthogonal matrix, bias parame-
ters are set to zero and the VST is initialized with a model
pretrained on Something-Something v2 [5]. We train using
a weighted Cross-Entropy loss to address the issue of imbal-
anced data within the DoTA dataset, assigning wn= 0.3and
wa= 0.7to the normal and anomaly class, respectively fol-
lowing equation reported at the end of Section 2.
Fig. 4 : Performance comparison, changing the VCL. The “8
frames” configuration corresponds to “ NF= 3” in Fig. 2.
#Short-term Long-term AUC
1 66.53
2 ✓ 74.46
3 ✓ 68.76
4 ✓ ✓ 79.21
Table 1 : Performance comparison with and w/out short and
long-term memory. Short-term: with ( NF= 3) and w/out
(NF= 1). Long-term: with (2 cells) and w/out (0 cells).
3.1. Ablation study
In this section different configurations will be individually ex-
plored and, finally, an optimal setup of MOV AD will be pre-
sented in Section 3.2.
Memory modules effectiveness. As shown in Table 1, we
first tested the effect of memories. STMM and LTMM both
contribute to enhance the general performance, obtaining the
best AUC when both are active, highlighting their importance.
Short-Term Memory Module. We decided to design our
STMM based on a Video Swin-B architecture, characterized#Method Input AUC
1ConvAE [6] (*) Gray 64.3
2ConvAE [6] (*) Flow 66.3
3ConvLSTMAE [2] (*) Gray 53.8
4ConvLSTMAE [2] (*) Flow 62.5
5AnoPred [7] (*) RGB 67.5
6AnoPred [7] (*) Masked RGB 64.8
7FOL-Ensemble [15] (*) RGB + Box + Flow + Ego 73.0
8STFE [16] RGB + Flow 79.3
9Our (MOVAD) RGB ( 320×240) 80.09
10Our (MOVAD) RGB ( 640×480) 82.17
Table 2 : Benchmarks of V AD methods on the DoTA dataset.
Both MOV AD models are trained with the best configuration
of: VCL of 8, 3 LSTM cells and NF= 4. (Note: Results
with (*) are taken from DoTA [7] paper.
Model ST AH LA OC TC V P V O OO UK
AnoPred [7] 69.9 73.6 75.2 69.7 73.5 66.3 N/A N/A N/A
AnoPred [7] + Mask 66.3 72.2 64.2 65.4 65.6 66.6 N/A N/A N/A
FOL-STD [15] 67.3 77.4 71.1 68.6 69.2 65.1 N/A N/A N/A
FOL-Ensemble [15] 73.3 81.2 74.0 73.4 75.1 70.1 N/A N/A N/A
STFE [16] 75.2 84.5 72.1 77.3 72.8 71.9 N/A N/A N/A
Our (MOVAD) 85.6 85.1 83.9 82.2 85.3 86.2 79.3 86.7 77.1
Our (MOVAD) † 86.6 86.3 84.9 83.7 85.5 81.6 77.4 87.9 73.8
Model ST∗AH∗LA∗OC∗TC∗V P∗V O∗OO∗UK∗
AnoPred [7] 70.9 62.6 60.1 65.6 65.4 64.9 64.2 57.8 N/A
AnoPred [7] + Mask 72.9 63.7 60.6 66.9 65.7 64.0 58.8 59.9 N/A
FOL-STD [15] 75.1 66.2 66.8 74.1 72.0 69.7 63.8 69.2 N/A
FOL-Ensemble [15] 77.5 69.8 68.1 76.7 73.9 71.2 65.2 69.6 N/A
STFE [16] 80.6 65.6 69.9 76.5 74.2 N/A 75.6 70.5 N/A
Our (MOVAD) 72.1 71.6 72.3 76.5 75.7 74.1 77.9 71.7 69.1
Our (MOVAD) † 72.2 74.0 74.8 80.2 79.6 76.8 82.2 78.3 72.9
Table 3 : Detection accuracy (AUC) for individual accident
categories. “*” non-ego anomaly categories. “†” if input res-
olution is 640×480instead of 320×240. N/A=Not Available.
Bold and red values are the best and second-best results.
by an embedding dimension of C= 128 after the linear pro-
jection of the patches. In Fig. 2, results when varying the
number of frames NFprocessed by the STMM at each step
are displayed. As expected, taking into account only the cur-
rent frame is the worst situation, loosing any temporal infor-
mation and making the training unstable. This is reasonable:
by not having any knowledge of the near past, the STMM
leads the LTMM to overfitting since consecutive frames are
very similar. On the contrary, increasing the number of frames
generally increase the performance. This is true until process-
ing 5 or more frames, where the effect becomes counterpro-
ductive. As mentioned in Section 2, according to our hypothe-
ses, overloading the transformer becomes harmful, as it does
not have a mechanism to weight the remote and recent past
differently. Overall, highest AUC is obtained with 4 frames.
Long-Term Memory Module. In Fig. 3 the LTMM capabil-
ities are evaluated, varying the number of cells from zero (no
LSTM at all) to four. Indeed, having no cells makes train-
ing slower in saturating performance and the lowest AUC is
reached. In general, by increasing the number of cells, the
maximum AUC is reached slower but it is higher than w/outLSTM. With 1 cell the performance saturates very quickly,
with a slow degradation during the rest of the epochs. On
the other side, 4 cells lead to a very slow saturation w/out
reaching the best performance. The global maximum AUC
is reached with 2 cells, even if with just + 0.02compared to
3 cells. Despite this, we prefer the latter configuration, be-
cause we think its ability to increase the quality of training in a
slower and more continuous way has a better general benefit.
We verified this hypothesis and, in conjunction with NF= 4
(best configuration in the previous experiment), it permits to
obtain higher AUC than with 2 cells. We speculate this hap-
pens because both (3 cells in Fig. 3 and NF= 4 in Fig. 2)
reach the best AUC at same time (around 400 epochs).
Video clip length (VCL). In Fig. 4, different values of VCL
are explored. The worst and most unstable training is ob-
tained with 4 frames, probably because they are too few to
exploit the long-term memory effect of LSTM cells. Increas-
ing VCL permits to saturate performance quicker, but a value
too high (like 12 or 16) tends to produce a lower AUC overall.
The highest AUC is obtained with 8 frames as VCL, which
represents a good trade-off between enlarging clip size and
exploiting LSTM cells, and reducing it to avoid overfitting,
due to consecutive frames being too similar to each other.
3.2. Comparison with the state of the art
Finally, in Table 2 we compare MOV AD (with 320×240
and640×480input videos size) with state-of-the-art (SOTA)
models. The training of MOV AD followed the best configura-
tion found using information obtained through ablation stud-
ies: VCL of 8, 3 LSTM cells and NF= 4. Both MOV AD
configurations surpass the SOTA models. Our best MOV AD
surpasses SOTA by + 2.87AUC, reaching the highest value of
82.17% AUC. Table 3 shows results per class, grouped by ego
and non-ego anomaly categories (see [15] for explanation of
classes). It is worth to note that, unlike previous SOTA mod-
els which incorporate e.g. object detectors, our model also
processes without any problem videos in which traffic partic-
ipants are not present or visible. For this reason, the table
shows also the ego-involved VO (vehicle-obstacle collision)
and OO (oncoming out-of-control) classes.
4. CONCLUSIONS
In this paper, we propose MOV AD, a new architecture for the
frame-level V AD task, capable to work in an online fashion,
handling the most restrictive V AD scenario with an end-to-
end training, requiring only RGB frames. It is composed by
STMM, which extracts information related to the ongoing ac-
tion, implemented by a VST, and LTMM that considers re-
mote past, thanks to LSTM injected inside the classifier. We
evaluated its performance on DoTA dataset, a collection of
dash-mounted camera videos of accidents, reaching 82.17%
AUC, surpassing SOTA by + 2.87AUC.5. REFERENCES
[1] Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lu ˇci´c,
M., Schmid, C.: Vivit: A video vision transformer. In:
Procs. of the IEEE/CVF Intl. Conf. on Computer Vision
(ICCV). pp. 6836–6846 (October 2021) 2
[2] Chong, Y .S., Tay, Y .H.: Abnormal event detection in
videos using spatiotemporal autoencoder. In: Advances
in Neural Networks-ISNN 2017: 14th Intl. Sympo-
sium, ISNN 2017, Sapporo, Hakodate, and Muroran,
Hokkaido, Japan, June 21–26, 2017, Procs., Part II 14.
pp. 189–196. Springer (2017) 4
[3] Fang, J., Yan, D., Qiao, J., Xue, J.: Dada: A large-scale
benchmark and model for driver attention prediction in
accidental scenarios (2019) 1
[4] Gandhi, T., Trivedi, M.M.: Pedestrian protection sys-
tems: Issues, survey, and challenges. IEEE Tran. on In-
telligent Transportation Systems 8(3), 413–430 (2007)
1
[5] Goyal, R., Ebrahimi Kahou, S., Michalski, V .,
Materzynska, J., Westphal, S., Kim, H., Haenel, V ., Fru-
end, I., Yianilos, P., Mueller-Freitag, M., et al.: The”
something something” video database for learning and
evaluating visual common sense. In: Procs. of the IEEE
international Conf. on computer vision. pp. 5842–5850
(2017) 3
[6] Hasan, M., Choi, J., Neumann, J., Roy-Chowdhury,
A.K., Davis, L.S.: Learning temporal regularity in video
sequences. In: Procs. of the IEEE Conf. on computer vi-
sion and pattern recognition. pp. 733–742 (2016) 1, 4
[7] Liu, W., Luo, W., Lian, D., Gao, S.: Future frame pre-
diction for anomaly detection–a new baseline. In: Proc.
of Conf. on Computer Vision and Pattern Recognition.
pp. 6536–6545 (2018) 1, 4
[8] Liu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., Hu,
H.: Video Swin Transformer. In: 2022 IEEE/CVF Conf.
on Computer Vision and Pattern Recognition (CVPR).
pp. 3192–3201. IEEE, New Orleans, LA, USA (Jun
2022) 2
[9] Luo, W., Liu, W., Gao, S.: Remembering history with
convolutional lstm for anomaly detection. In: 2017
IEEE Intl. Conf. on Multimedia and Expo. pp. 439–444
(2017) 1
[10] Moutik, O., Sekkat, H., Tigani, S., Chehri, A., Saadane,
R., Tchakoucht, T.A., Paul, A.: Convolutional neural
networks or vision transformers: Who will win the race
for action recognitions in visual data? Sensors 23(2),
734 (2023) 2[11] Ramachandra, B., Jones, M.J., Vatsavai, R.R.: A survey
of single-scene video anomaly detection. IEEE transac-
tions on pattern analysis and machine intelligence 44(5),
2293–2312 (2020) 1
[12] Wang, H., Schmid, C.: Action recognition with im-
proved trajectories. In: Procs. of the IEEE international
Conf. on computer vision. pp. 3551–3558 (2013) 1
[13] Wang, L., Zhou, F., Li, Z., Zuo, W., Tan, H.: Abnormal
event detection in videos using hybrid spatio-temporal
autoencoder. In: 2018 25th IEEE Intl. Conf. on Image
Processing (ICIP). pp. 2276–2280. IEEE (2018) 1
[14] Xu, M., Gao, M., Chen, Y .T., Davis, L.S., Crandall, D.J.:
Temporal recurrent networks for online action detection.
In: Procs. of the IEEE/CVF Intl. Conf. on Computer
Vision. pp. 5532–5541 (2019) 1
[15] Yao, Y ., Wang, X., Xu, M., Pu, Z., Wang, Y ., Atkins, E.,
Crandall, D.: Dota: Unsupervised detection of traffic
anomaly in driving videos. IEEE Tran. on Pattern Anal-
ysis and Machine Intelligence pp. 1–1 (2022) 1, 2, 3,
4
[16] Zhou, Z., Dong, X., Li, Z., Yu, K., Ding, C., Yang, Y .:
Spatio-Temporal Feature Encoding for Traffic Accident
Detection in V ANET Environment. IEEE Tran. on In-
telligent Transportation Systems 23(10), 19772–19781
(Oct 2022) 1, 4