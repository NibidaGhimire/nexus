Physics-informed neural networks for predicting gas
flow dynamics and unknown parameters in diesel
engines
Kamaljyoti Natha, 1, Xuhui Mengb, 1, Daniel J Smithc,
George Em Karniadakisa,d,âˆ—
aDivision of Applied Mathematics, Brown University, United States of America
bInstitute of Interdisciplinary Research for Mathematics and Applied Science, School of
Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China
cCummins Inc., United States of America
dSchool of Engineering, Brown University, United States of America
Abstract
This paper presents a physics-informed neural network (PINN) approach for monitoring
the health of diesel engines. The aim is to evaluate the engine dynamics, identify unknown
parameters in a â€mean valueâ€ model, and anticipate maintenance requirements. The PINN
model is applied to diesel engines with a variable-geometry turbocharger and exhaust gas
recirculation, using measurement data of selected state variables. The results demonstrate
the ability of the PINN model to predict simultaneously both unknown parameters and
dynamics accurately with both clean and noisy data, and the importance of the self-adaptive
weight in the loss function for faster convergence. The input data for these simulations are
derived from actual engine running conditions, while the outputs are simulated data, making
this a practical case study of PINNâ€™s ability to predict real-world dynamical systems. The
mean value model of the diesel engine incorporates empirical formulae to represent certain
states, but these formulae may not be generalizable to other engines. To address this, the
study considers the use of deep neural networks (DNNs) in addition to the PINN model.
The DNNs are trained using laboratory test data and are used to model the engine-specific
empirical formulae in the mean value model, allowing for a more flexible and adaptive
representation of the engineâ€™s states. In other words, the mean value model uses both the
PINN model and the DNNs to represent the engineâ€™s states, with the PINN providing a
physics-based understanding of the engineâ€™s overall dynamics and the DNNs offering a
more engine-specific and adaptive representation of the empirical formulae. By combining
these two approaches, the study aims to offer a comprehensive and versatile approach to
monitoring the health and performance of diesel engines.
Keywords: Diesel engine, Gas flow dynamics, Parameters estimation, Physics Informed Neural
Networks, Digital Twins.
âˆ—Corresponding author: G. E. Karniadakis (george karniadakis@brown.edu)
1These authors contributed equally (Kamaljyoti Nath, Xuhui Meng),
E-mail addresses: kamaljyoti nath@brown.edu (Kamaljyoti Nath), xuhui meng@hust.edu.cn (Xuhui Meng),
daniel.j.smith@cummins.com (Daniel J Smith), george karniadakis@brown.edu (George Em Karniadakis)
August 8, 2023arXiv:2304.13799v2  [cs.LG]  5 Aug 20231 Introduction
Powertrains of the future must meet increasingly stringent requirements for emissions, per-
formance, reliability, onboard monitoring, and serviceability. Capable system models for
estimating states and adapting to an individual systemâ€™s behaviour are critical elements to meet
control and health monitoring needs. Leveraging purely data-driven models to meet these
requirements provides simplicity in modelling and captures dynamics difficult to formulate ana-
lytically. However, large data needs, poor physical interpretability, challenges with systems with
long memory effects and sparse sensing, as well as inability to extrapolate beyond the training
datasets present onerous burdens to practical implementation. Relying on purely theory-based
models allows for directly interpretable results with higher confidence and fewer data for cal-
ibration but often causes a tradeoff of modelling relevant dynamics versus model complexity,
challenges in systems with high uncertainties, poor modelling where dynamics are not well
understood, and slow solution of higher-order models. Modelling solutions that leverage the
strengths of theory-guided as well as data-driven models have the potential to reduce data needs,
increase robustness, and effectively use theoretical and practical knowledge of the system.
To investigate model architectures, balancing the strengths of both theory-based models and
data-driven models, this work explores the application of physics-informed neural networks
(PINNs) to a diesel internal combustion engine model for the purposes of simultaneous param-
eter and state estimation. The physical portion is based on the mean value model of a diesel
engine with a variable geometry turbocharger (VGT), and exhaust gas recirculation (EGR)
proposed by Wahlstr Â¨om and Eriksson [24].
Physics-informed Neural Networks (PINNs) [21] is a new method of training neural net-
works, which takes into account the physics of a problem while evaluating the parameters of
the neural network. The method is suitable for both evaluation of the solution of PDF (forward
problem) and the data-driven identification of parameters of PDF (inverse problem). It takes
advantage of automatic differentiation [1] in formulating a physical loss in the loss function
along with data loss. Jagtap et al. [12] proposed conservative PINNs (cPINNs) for conservation
laws, which employs domain decomposition with a PINN formulation in each domain. Further,
Jagtap and Karniadakis [10] introduced domain decomposition for general PDEs using the so-
called extended PINN (XPINN). hp-VPINN is a variational formulation of PINN with domain
decomposition proposed by Kharazmi et al. [14]. Meng et al. [20] proposed the Parareal PINN
(PPINN) approach for long-time integration of time-dependent partial differential equations.
The authors of [5] proposed â€œseparableâ€ PINN, which can reduce the computational time and
increase accuracy for high dimensional PDEs. In PINN, there are multiple loss functions, and
the total loss function is given by the weighted sum of individual losses. McClenny and Braga-
Neto [19] proposed a self-adaptive weight technique, which is capable of tuning the weights
automatically. PINN and its variants were also considered in various inverse problems like
supersonic flows [13], nano-optics and metamaterials [4], unsaturated groundwater flow [7].
Detailed reviews of PINN can be found in [3, 6, 17].
Modelling of diesel engines using neural networks has been considered in the past. Biao et al.
[2] considered Nonlinear Auto-Regressive Moving Average with eXogenous inputs (NARMAX)
method for system identification of locomotive diesel engines. The model has three inputs to the
network, i.e. the fuel injected, the load of the main generator, and the feedback rotation speed
(from the output); the outputs are rotation speed and diesel power. The authors considered
Levenberg-Marquardt (LM) algorithm to train the network. Finesso and Spessa [8] developed
a three-zone thermodynamic model to predict nitrogen oxide and in-cylinder temperature heat
release rate for direct injection diesel engines under steady state and transient conditions. The
2model is zero-dimensional, and the equations can be solved analytically. Thus, it required
a very short computational time. Tosun et al. [23] predicted torque, carbon monoxide, and
oxides of nitrogen using neural networks (3 independent networks) for diesel engines fueled
with biodiesel-alcohol mixtures. The authors considered three fuel properties (density, cetane
number, lower heating value) and engine speed as input parameters and the networks are
optimized using the Levenberg-Marquardt method. The authors observed that neural network
results are better than the least square method. Gonz Â´alez et al. [9] integrated a data-driven
model with a physics-based (equation-based) model for the gas exchange process of a diesel
engine. The authors modelled the steady-state turbocharger using a neural network. Further,
the authors integrated the data-driven model with an equation-based model. Recently, Kumar
et al. [16] considered DeepONet [18] to predict the state variable of the same mean value
engine model [24] we considered in this study. The authors consider dynamic data to train
the model. However, the model can predict only the state variable at the particular (trained)
ambient temperature and pressure, as variations of ambient temperature and pressure are not
considered in the training of DeepONet. The model also does not predict the parameters of the
engine model. While the model was trained using dynamic data, the physics of the problem
was not considered while training the network. The model (DeepONet) is capable of predicting
dynamic responses.
In the present study, we formulate a PINN model for the data-driven identification of
parameters and prediction of dynamics of system variables of a diesel engine. In PINN, the
physics of the system is directly included in the form of physics loss along with data loss. While
data-driven models require large amount over the entire operational range in training, PINN can
be trained with a smaller amount of data as it is trained online. The dynamics characteristic
of the state variables is automatically incorporated. PINN may be used for the solution of
differential equations or for the identification of parameters and prediction of state variables.
In the present study, we are specifically interested in estimating unknown parameters and states
when we know a few state variables from field data. The dynamics of the state variables of the
mean value engine [24] are described by first-order differential equations. We will utilize these
equations in the formulation of the physics-informed loss function. The unknown parameters
are considered trainable and updated in the training process along with the neural network
parameters.
The engine model also considers a few empirical formulae in its formulation. These
equations are engine-specific, and the coefficients of these equations need to be evaluated
from experimental data. These equations are static in nature, and thus may be trained with
smaller data compared to dynamic equations. We know that deep neural networks (DNNs)
are universal approximators of any continuous function, thus, DNNs may be considered more
general approximators of these empirical formulae. One of the advantages of considering DNNs
over empirical formulae is that we do not need to assume the type of non-linearity between
the input out variables. The neural network learns the non-linearity if trained with sufficient
data. We approximate the empirical formulae using DNNs and train them using laboratory test
data. Once these networks are trained using laboratory test data, these are considered in the
PINNs model in places of the empirical formulae. During the training of the PINNs model, the
parameters of these networks are remain constant.
The training data for the inverse problem and laboratory data are generated using the
Simulink file [22] accompanied in [24]. The input to Simulink is taken from actual field data.
By doing this, we are trying to generate data as realistic as field data. Furthermore, we also
consider noise to the field data generated. We observed that the proposed PINNs model can
predict the dynamics of the states and unknown parameters. We summarize below a few of the
3salient features of the present study:
1. We formulated PINNs-based parameter identification for real-world dynamical systems,
in the present case, a diesel engine. This is significant as it started a new paradigm for
future research for onboard systems for the health monitoring of engines.
2. We showed how PINNs could be implemented in predicting important unknown parame-
ters of diesel engines from field data. From these predicted parameters, one can infer the
health and serviceability requirements of the engine.
3. We showed the importance of self-adaptive weights (given the fast transient dynamics) in
the accuracy and faster convergence of results for PINNs for the present study.
4. The engine model generally considers empirical formulae to evaluate a few of its quan-
tities. These empirical formulae are engine-specific and require lab test data for the
evaluation of the coefficients. We have shown how neural networks can be considered
to model the empirical formulae. We have shown how we can train these networks from
lab-test data. This is important as it may provide a better relationship for the empirical
formulae.
5. The field data for the inverse problem are generated considering input recorded from
actual engine running conditions. Further, we consider appropriate noise in the simulated
data, mimicking near real-world field data.
We organize the rest of the article as follows: in section 1.1, we discuss the detailed problem
statement and different cases considered for simulation studies. In section 2, first, we discuss
PINNs for the inverse problems for the diesel engine and the surrogates for the empirical
formula. We discuss a detailed flow chart for the inverse problem for the PINN engine model
in section 2.2. In section 2.3, we discuss the laboratory data required and their generation for
the training of surrogates for the empirical formulae. We also discuss the field data generation
for the inverse problem. We present the results and discussion in section 3. The conclusions of
the present study are discussed in the section 4.
1.1 Problem setup
In this section, we first introduce the mean value model for the gas flow dynamics [24] in the
diesel engine, and then we will formulate the inverse problems that we are interested in.
As shown in Fig. 1, the engine model considered in the present study mainly comprises
six parts: the intake and exhaust manifold, the cylinder, the exhaust gas recirculation (EGR)
valve system, the compressor and the turbine. More details on each engine part can be seen in
Appendix B. We note that the engine considered here is the same as in [24].
4uegr
EGR valve
Intake
manifoldExhaust
manifold
Cylinder
CompressorTurbineuvgtuÎ´
pim
XOimpem
XOemWegr
Wei Weo
Ï‰t
WcWt
1Fig. 1: Schematic diagram of the diesel engine: A schematic diagram of the mean value
diesel engine with a variable-geometry turbocharger (VGT) and exhaust gas recirculation
(EGR) [24]. The main components of the engine are the intake manifold, the exhaust
manifold, the cylinder, the EGR valve system, the compressor, and the turbine. The
control input vector is u={ğ‘¢ğ›¿,ğ‘¢ğ‘’ğ‘”ğ‘Ÿ,ğ‘¢ğ‘£ğ‘”ğ‘¡}, and engine speed is ğ‘›ğ‘’. (Source: Figure is
adopted from [24])
To describe the gas flow dynamics in the engine illustrated in Fig. 1, e.g., the dynamics
in the manifold pressures, turbocharger, EGR and VGT actuators, a mean value model of the
diesel engine with variable geometric turbocharger and exhaust gas recirculation was proposed
in [24]. We will also utilize the same model as the governing equations to describe the gas flow
dynamics considered in the current study. Specifically, the model proposed in [24] has eight
states expressed as follows:
x={ğ‘ğ‘–ğ‘š, ğ‘ğ‘’ğ‘š, ğ‘‹ğ‘‚ğ‘–ğ‘š, ğ‘‹ğ‘‚ğ‘’ğ‘š, ğœ”ğ‘¡,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2,Ëœğ‘¢ğ‘£ğ‘”ğ‘¡}, (1)
whereğ‘ğ‘–ğ‘šandğ‘ğ‘’ğ‘šare the intake and exhaust manifold pressure, respectively, ğ‘‹ğ‘‚ğ‘–ğ‘šandğ‘‹ğ‘‚ğ‘’ğ‘š
are the oxygen mass fractions in the intake and exhaust manifold, respectively, ğœ”ğ‘¡is the turbo
speed; Ëœğ‘¢ğ‘£ğ‘”ğ‘¡represents the VGT actuator dynamics. A second-order system with an overshoot
and a time delay is used to represent the dynamics of the EGR-valve actuator. The model is
represented by subtraction of two first-order models, Ëœ ğ‘¢ğ‘’ğ‘”ğ‘Ÿ1and Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2, with different gains and
time constants. Further, the control inputs for the engine are u={ğ‘¢ğ›¿, ğ‘¢ğ‘’ğ‘”ğ‘Ÿ, ğ‘¢ğ‘£ğ‘”ğ‘¡}and the
engine speed is ğ‘›ğ‘’, in whichğ‘¢ğ›¿is the mass of injected fuel, ğ‘¢ğ‘’ğ‘”ğ‘Ÿandğ‘¢ğ‘£ğ‘”ğ‘¡are the EGR and
VGT valve positions, respectively. Furthermore, the position of the valves, i.e., ğ‘¢ğ‘’ğ‘”ğ‘Ÿandğ‘¢ğ‘£ğ‘”ğ‘¡,
may vary from 0% to 100%, which indicates the complete close and opening of the valves,
respectively. The mean value engine model is then expressed as
Â¤x=ğ‘“(x,u,ğ‘›ğ‘’). (2)
5In addition, the states describing the oxygen mass fraction of the intake and exhaust manifold,
i.e.,ğ‘‹ğ‘‚ğ‘–ğ‘šandğ‘‹ğ‘‚ğ‘’ğ‘š, are not considered in the present study as the rest of the states do not depend
on these two states. Also, the parameters of the oxygen mass fractions are assumed to be constant
and known. The governing equations for the remaining six states are as follows:
ğ‘‘
ğ‘‘ğ‘¡ğ‘ğ‘–ğ‘š=ğ‘…ğ‘ğ‘‡ğ‘–ğ‘š
ğ‘‰ğ‘–ğ‘š(ğ‘Šğ‘+ğ‘Šğ‘’ğ‘”ğ‘Ÿâˆ’ğ‘Šğ‘’ğ‘–), (3)
ğ‘‘
ğ‘‘ğ‘¡ğ‘ğ‘’ğ‘š=ğ‘…ğ‘’ğ‘‡ğ‘’ğ‘š
ğ‘‰ğ‘’ğ‘š(ğ‘Šğ‘’ğ‘œâˆ’ğ‘Šğ‘¡âˆ’ğ‘Šğ‘’ğ‘”ğ‘Ÿ), (4)
ğ‘‘
ğ‘‘ğ‘¡ğœ”ğ‘¡=ğ‘ƒğ‘¡ğœ‚ğ‘šâˆ’ğ‘ƒğ‘
ğ½ğ‘¡ğœ”ğ‘¡, (5)
ğ‘‘Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1
ğ‘‘ğ‘¡=1
ğœğ‘’ğ‘”ğ‘Ÿ1
ğ‘¢ğ‘’ğ‘”ğ‘Ÿ(ğ‘¡âˆ’ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿ)âˆ’Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1
, (6)
ğ‘‘Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2
ğ‘‘ğ‘¡=1
ğœğ‘’ğ‘”ğ‘Ÿ2
ğ‘¢ğ‘’ğ‘”ğ‘Ÿ(ğ‘¡âˆ’ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿ)âˆ’Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2
, (7)
ğ‘‘Ëœğ‘¢ğ‘£ğ‘”ğ‘¡
ğ‘‘ğ‘¡=1
ğœğ‘£ğ‘”ğ‘¡
ğ‘¢ğ‘£ğ‘”ğ‘¡(ğ‘¡âˆ’ğœğ‘‘ğ‘£ğ‘”ğ‘¡)âˆ’Ëœğ‘¢ğ‘£ğ‘”ğ‘¡
. (8)
Two additional equations used for the computation of ğ‘‡ğ‘’ğ‘šin Eq. (4) read as:
ğ‘‡1=ğ‘¥ğ‘Ÿğ‘‡ğ‘’+(1âˆ’ğ‘¥ğ‘Ÿ)ğ‘‡ğ‘–ğ‘š, (9)
ğ‘¥ğ‘Ÿ=Î 1/ğ›¾ğ‘ğ‘’ğ‘¥âˆ’1/ğ›¾ğ‘ğ‘
ğ‘Ÿğ‘ğ‘¥ğ‘£, (10)
whereğ‘‡1is the temperature when the inlet valve closes after the intake stroke and mixing, and
ğ‘¥ğ‘Ÿis the residual gas fraction. A brief discussion on the governing equations of the engine
model is presented in Appendix B. Interested readers can also refer to [24] for more details.
In the present study, we have field measurements on a certain number of variables, i.e., ğ‘ğ‘–ğ‘š,
ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, andğ‘Šğ‘’ğ‘”ğ‘Ÿas well as the inputs, i.e., uandğ‘›ğ‘’, at discrete times. Further, some of the
parameters in the system, e.g., ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ , which are difficult to measure
directly, are unknown. ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ is the maximum effective area of the EGR valve, ğœ‚ğ‘ ğ‘is the
compensation factor for non-ideal cycles, â„ğ‘¡ğ‘œğ‘¡is the total heat transfer coefficient of the exhaust
pipes andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ is the maximum area in the turbine that the gas flows through. From the field
prediction of these parameters, we can infer the health of the engine; a higher deviation from
their design value may indicate a fault in the system. We are interested in (1) predicting the
dynamics of all the variables in Eqs. (3)-(10), and (2) identifying the unknown parameters in
the system, given field measurements on ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, andğ‘Šğ‘’ğ‘”ğ‘Ÿas well as Eqs. (3)-(10). We
refer to the above problem as the inverse problem in this study. Specifically, the following cases
are considered for a detailed study:
Case 1 Prediction of dynamics of the system and identification of 3 unknown parameters
ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘andâ„ğ‘¡ğ‘œğ‘¡with clean data of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, andğ‘Šğ‘’ğ‘”ğ‘Ÿ.
Case 2 Prediction of dynamics of the system and identification of 3 unknown parameters
ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘andâ„ğ‘¡ğ‘œğ‘¡with noisy data of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, andğ‘Šğ‘’ğ‘”ğ‘Ÿ.
Case 3 Prediction of dynamics of the system and identification of 4 unknown parameters
ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ with clean data of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, andğ‘Šğ‘’ğ‘”ğ‘Ÿ.
6Case 4 Prediction of dynamics of the system and identification of 4 unknown parameters
ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ with noisy data of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, andğ‘Šğ‘’ğ‘”ğ‘Ÿ.
In the present study, we consider self-adaptive weights [19] (discussed in section 2 and Appendix
A) in our loss function. We study the above four cases using self-adaptive weight. In order to
understand the effect and importance of self-adaptive weights in the convergence and accuracy
of results, we consider one more case without self-adaptive weight,
Case 5 Prediction of dynamics of the system and identification of 4 unknown parameters
ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ with clean data of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, andğ‘Šğ‘’ğ‘”ğ‘Ÿwithout
self-adaptive weights.
The results of Case 1 andCase 2 are presented in Appendix G. First, we study the results
ofCase 3 andCase 5 to understand the accuracy and convergence of PINN method and the
importance of self-adaptive weights. Then, we study the results of Case 4 . The results for Case
3,Case 4 andCase 5 are discussed in section 3.
2 Methodology
We consider to employ the deep learning algorithm, particularly, the physics-informed neural
networks (PINNs), to solve the inverse problem discussed in section 1.1. To begin with, we first
briefly review the basic principle of PINNs, and then we discuss how to employ PINNs for the
present inverse problem.
2.1 PINNs for inverse problems in the diesel engine
We first briefly review the PINNs [10, 12, 14, 20, 21] for solving inverse problems, and then we
introduce how to employ the PINNs to solve the specific problem that we are of interest for the
diesel engine.
7Ë†yÏƒ
Ïƒ
ÏƒÏƒ
Ïƒ
Ïƒg(Ë†y)
tÏƒ
Ïƒ
Ïƒ
ÏƒÏƒ
Ïƒ
Ïƒ
Ïƒr=f/parenleftbigg
h(Ë†y), g(Ë†y),dË†y
dt,d2Ë†y
dt2, . . . ,Î›, C, q(t)/parenrightbiggh(Ë†y)
dË†y
dt
d2Ë†y
dt2
...Ë†y
Neural network ( {W,b})Physics (ODE)
L(Î¸)< /epsilon1Back
propagateloss,L(Î¸) = Î»1L1(Ë†yâˆ’y) +Î»2L2(r)
Î¸={W,b,Î›}
PredictNoupdate Î¸
YesPretrained Neural
network ( Î¸p={Wp,bp})
1Fig. 2: Schematic of physics-informed neural networks (PINNs) for inverse problems: The
left part of the figure, enclosed in the red dashed line, shows a DNN whose input is
time. The DNN is to approximate the solution ( ğ‘¦) to a differential equation. The top
left part of the figure enclosed in the black dashed line shows an DNN whose input
is the output ğ‘¦(maybe with other input, e.g. ambient condition). The output of this
network is a function ğ‘”(ğ‘¦). This network is pre-trained with laboratory data of ğ‘¦and
ğ‘”(ğ‘¦). The right part of the figure, enclosed in the blue dashed line, denotes the physics
loss/residue. The DNN (enclosed in a red dashed line) approximates the solution to any
differential equation, and the equation is encoded using automatic differentiation. The
total lossL(Î¸)includes the loss of equation as well as the data. The ğœ†1andğœ†2are two
weights to the data loss and physics loss, which may be fixed or adaptive depending
upon the problem and solution method. Î¸={W,b,Î›}represents the parameters in
DNN, Wandbare the weights and biases of DNN, respectively and Î›are the unknown
parameters of the ODE; ğœis the activation function, ğ‘(ğ‘¡)is the right-hand side (RHS)
of the differential equation (source term), â„is the function of the predicted variable,
andğ‘Ÿis the residual for the equation. Î¸ğ‘ƒ={Wğ‘ƒ,bğ‘ƒ}represents the parameters in
pre-trained neural network, Wğ‘ƒandbğ‘ƒare the weights and biases of the pre-trained
neural network.
As illustrated in Fig. 2, the PINN is composed of two parts, i.e., a fully-connected neural
network which is to approximate the solution to a particular differential equation and the physics-
informed part in which the automatic differentiation [1] is employed to encode the corresponding
differential equation. Further, ğœ¦represents the unknowns in the equation, which can be either a
constant or a field. In particular, ğœ¦are trainable variables as the unknowns are constant, but they
could also be approximated by a DNN if the unknown is a field. The loss function for solving
the inverse problems consists of two parts, i.e., the data loss and the equation loss, which reads
8as:
L(Î¸;ğœ¦)=1
ğ‘€ğ‘€âˆ‘ï¸
ğ‘–=1|Ë†ğ‘¦(ğ‘¡ğ‘–;Î¸)âˆ’ğ‘¦(ğ‘¡ğ‘–)|2
|                        {z                        }
data loss+1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1|ğ‘Ÿ(ğ‘¡ğ‘–;Î¸;ğœ¦)|2
|                 {z                 }
equation loss(11)
where Î¸denotes the parameters in the DNN; ğ‘€andğ‘represent the number of measurements
and the residual points, respectively; Ë† ğ‘¦(ğ‘¡ğ‘–;Î¸)denotes the prediction of DNN at the time ğ‘¡ğ‘–;ğ‘¦(ğ‘¡ğ‘–)
is the measurement at ğ‘¡ğ‘–, andğ‘Ÿ(ğ‘¡ğ‘–;Î¸;ğœ¦)represents the residual of the corresponding differential
equation, which should be zero in the entire domain. By minimizing the loss in Eq. (11), we
can obtain the optimal parameters, i.e., Î¸, of the DNN as well as the unknowns, i.e., ğœ¦, in the
system. In the present study, we have a few empirical equations that we approximate using
DNNs. These DNNs are trained first using data and considered in place of these empirical
formulae. We fixed the parameters of these networks when we minimized the loss function for
the PINN model. Furthermore, note that here we employ the system described by one equation
as the example to demonstrate how to use PINNs for solving inverse problems. For the system
with more than one equation, we can either utilize an DNN with multiple outputs or multiple
DNNs as the surrogates for the solutions to differential equations. In addition, a similar idea
can also be employed for systems with multiple unknown fields. The loss function can then be
rewritten as
L(Î¸;ğœ¦)=ğ¾âˆ‘ï¸
ğ‘˜=1"
1
ğ‘€ğ‘˜ğ‘€ğ‘˜âˆ‘ï¸
ğ‘–=1|Ë†ğ‘¦ğ‘˜(ğ‘¡ğ‘–;Î¸)âˆ’ğ‘¦ğ‘˜(ğ‘¡ğ‘–)|2#
|                                     {z                                     }
data loss+ğ¿âˆ‘ï¸
ğ‘™=1"
1
ğ‘ğ‘™ğ‘ğ‘™âˆ‘ï¸
ğ‘–=1|ğ‘Ÿğ‘™(ğ‘¡ğ‘–;Î¸;ğœ¦)|2#
|                           {z                           }
equation loss(12)
whereğ¾andğ¿denote the number of variables that can be measured as well as the equations,
respectively; ğ‘€ğ‘˜andğ‘ğ‘™are the number of measurements for the ğ‘˜thvariable and the number of
residual points for the ğ‘™thequation, respectively; and ğœ¦collects all the unknowns in the system.
Variables ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘šğ‘¥ğ‘Ÿğ‘‡1 Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2ğœ”ğ‘¡ Ëœğ‘¢ğ‘£ğ‘”ğ‘¡
SurrogateN1(ğ‘¡;Î¸1)N2(ğ‘¡;Î¸2)N3(ğ‘¡;Î¸3)N4(ğ‘¡;Î¸4)N5(ğ‘¡;Î¸5)N6(ğ‘¡;Î¸6)
Equation 3 and 4 10 9 6 and 7 5 8
Table 1: Neural network surrogates employed PINNs for solving the inverse problems.
Nğ‘–(ğ‘¡;Î¸ğ‘–),ğ‘–=1,...,6 denotes the surrogate for the ğ‘–ğ‘¡â„DNN parameterized by Î¸ğ‘–with
the inputğ‘¡. In particular,N1(ğ‘¡;Î¸1)andN4(ğ‘¡;Î¸4)have two outputs, which are used
to approximate{ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š}and{Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2}, respectively; the remaining DNNs have
only one output.
For the inverse problem presented in section 1.1, we are interested in (1) learning the dynam-
ics of the six states (2) inferring the unknown parameters in the system, given measurements on
{ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,ğ‘Šğ‘’ğ‘”ğ‘Ÿ}as well as Eqs. (3)-(10), using PINNs. Specifically, we utilize six DNNs
as the surrogates for the solutions to different equations, and the corresponding equations are
encoded using the automatic differentiation, as illustrated in Table 1. In addition, the loss for
9training the PINNs for Case 1 to Case 4 is expressed as follows:
L(Î¸,ğœ¦,Î»ğ‘ğ‘–ğ‘š,Î»ğ‘ğ‘’ğ‘š,Î»ğœ”ğ‘¡,Î»ğ‘Šğ‘’ğ‘”ğ‘Ÿ,ğœ†ğ‘‡1)=Lğ‘ğ‘–ğ‘š+Lğ‘ğ‘’ğ‘š+Lğœ”ğ‘¡+Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘¢ğ‘£ğ‘”ğ‘¡+10Ã—Lğ‘¥ğ‘Ÿ+ğœ†ğ‘‡1Ã—Lğ‘‡1+
Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘–ğ‘š+Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘’ğ‘š+Lğ‘–ğ‘›ğ‘–
ğœ”ğ‘¡+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘£ğ‘”ğ‘¡+Lğ‘–ğ‘›ğ‘–
ğ‘¥ğ‘Ÿ+100Ã—Lğ‘–ğ‘›ğ‘–
ğ‘‡1+
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘–ğ‘š(Î»ğ‘ğ‘–ğ‘š)+Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘’ğ‘š(Î»ğ‘ğ‘’ğ‘š)+
Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ”ğ‘¡(Î»ğœ”ğ‘¡)+Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘Šğ‘’ğ‘”ğ‘Ÿ(Î»ğ‘Šğ‘’ğ‘”ğ‘Ÿ),(13)
where Î¸=(Î¸1,...,Î¸6)are the parameters of all NNs in PINNs, ğœ¦are the unknown parameters,
which will be inferred from the given measurements, Lğœ™,ğœ™=(ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2,Ëœğ‘¢ğ‘£ğ‘”ğ‘¡,ğ‘¥ğ‘Ÿ,ğ‘‡1)
are the losses for the corresponding equations, and Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ“,ğœ“=(ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,ğ‘Šğ‘’ğ‘”ğ‘Ÿ)are the losses
for the corresponding measurements, and Lğ‘–ğ‘›ğ‘–
ğœ™,ğœ™=(ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2,Ëœğ‘¢ğ‘£ğ‘”ğ‘¡,ğ‘¥ğ‘Ÿ,ğ‘‡1)are the
losses for the initial conditions, Î»ğ‘ğ‘–ğ‘š,Î»ğ‘ğ‘’ğ‘š,Î»ğœ”ğ‘¡, andÎ»ğ‘Šğ‘’ğ‘”ğ‘Ÿare the weights for different loss
terms which are used to balance each term in the loss function. In particular, the self-adaptive
weight technique proposed in [19], which is capable of tuning the weights automatically, is
utilized here to obtain the optimal Î»ğ‘ğ‘–ğ‘š,Î»ğ‘ğ‘’ğ‘š,Î»ğœ”ğ‘¡, andÎ»ğ‘Šğ‘’ğ‘”ğ‘Ÿ. More details for self-adaptive
weights in PINN can be found in Appendix A.
In the Case 5 where we have not considered self-adaptive weights, so the loss function is
given as
L(Î¸,ğœ¦)=Lğ‘ğ‘–ğ‘š+Lğ‘ğ‘’ğ‘š+Lğœ”ğ‘¡+Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘¢ğ‘£ğ‘”ğ‘¡+10Ã—Lğ‘¥ğ‘Ÿ+103Ã—Lğ‘‡1+
Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘–ğ‘š+Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘’ğ‘š+Lğ‘–ğ‘›ğ‘–
ğœ”ğ‘¡+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘£ğ‘”ğ‘¡+Lğ‘–ğ‘›ğ‘–
ğ‘¥ğ‘Ÿ+100Ã—Lğ‘–ğ‘›ğ‘–
ğ‘‡1+
103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘–ğ‘š+103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘’ğ‘š+
103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ”ğ‘¡+103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘Šğ‘’ğ‘”ğ‘Ÿ,(14)
As for training the PINNs in the present study, we first employ the first-order optimizer,
i.e., Adam [15], to train the parameters in the NNs, unknowns in the systems as well as the
self-adaptive weights for a certain number of steps. We then fix the self-adaptive weight and
employed Adam to train the parameters in the NNs, and unknowns in the systems for another
certain number of steps. We then switch to the second-order accuracy optimizer, i.e., LBFGS-B,
to further optimize the parameters in the NNs and the unknowns in the systems. Note that the
self-adaptive weights are optimized at the first training stage of Adam only, and they are fixed
during the second training stage of Adam and LBFGS-B training with the values at the end of
the first stage of Adam optimization.
2.1.1 Neural network surrogates for empirical formulae
In the mean value engine model proposed in [24], empirical formulae, e.g., polynomial functions,
are employed for the volumetric efficiency ( ğœ‚ğ‘£ğ‘œğ‘™), effective area ratio function for EGR valve
(ğ‘“ğ‘’ğ‘”ğ‘Ÿ), turbine mechanical efficiency ( ğœ‚ğ‘¡ğ‘š), effective area ratio function for VGT ( ğ‘“ğ‘£ğ‘”ğ‘¡), choking
function (for VGT) ( ğ‘“Î ğ‘¡), compressor efficiency ( ğœ‚ğ‘), and volumetric flow coefficient (for the
compressor) ( Î¦ğ‘). Note that these empirical formulae are engine-specific and may not be
appropriate for the diesel engines considered in the present study. Deep neural networks
10(DNNs), which are known to be universal approximators of any continuous function, are thus
utilized as more general surrogates for the empirical formulae here. Particularly, we employ six
DNNs for the aforementioned variables, and the inputs for each DNN are presented in Table 2.
Variable ğœ‚ğ‘£ğ‘œğ‘™ ğ‘“ğ‘’ğ‘”ğ‘Ÿ ğ¹ğ‘£ğ‘”ğ‘¡,Î ğ‘¡ğœ‚ğ‘¡ğ‘š ğœ‚ğ‘ Î¦ğ‘
SurrogateN(ğ‘ƒ)
1(x;Î¸ğ‘ƒ
1)N(ğ‘ƒ)
2(x;Î¸ğ‘ƒ
2)N(ğ‘ƒ)
3(x;Î¸ğ‘ƒ
3)N(ğ‘ƒ)
4(x;Î¸ğ‘ƒ
4)N(ğ‘ƒ)
5(x;Î¸ğ‘ƒ
5)N(ğ‘ƒ)
6(x;Î¸ğ‘ƒ
6)
Input ( x){ğ‘ğ‘–ğ‘š,ğ‘›ğ‘’} Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ{Ëœğ‘¢ğ‘£ğ‘”ğ‘¡,Î ğ‘¡}{ğœ”ğ‘¡,ğ‘‡ğ‘’ğ‘š,Î ğ‘¡}{ğ‘Šğ‘,Î ğ‘}{ğ‘‡ğ‘ğ‘šğ‘,Î ğ‘,ğœ”ğ‘¡}
Equationâ€ B.8 B.24 D.4 B.33 B.40 B.50
â€ The empirical equations are discussed in Appendix
Table 2: Neural network surrogates for empirical formulae N(ğ‘ƒ)
ğ‘–(x;Î¸ğ‘ƒ
ğ‘–),ğ‘–=1,..., 6 denotes
the surrogate for the ğ‘–ğ‘¡â„DNN parameterized by ğœƒğ‘ƒ
ğ‘–with the input x. All the neural
networks have one output each.
We now discuss the training of the DNNs illustrated in Table 2. In laboratory experiments,
measurements on all variables are available. We can then train the neural network surrogates in
Table 2 using the data collected in the laboratory. The loss function considered for the training
of these networks is
Lğ‘–(Î¸ğ‘ƒ
ğ‘–)=1
ğ‘›ğ‘–ğ‘›ğ‘–âˆ‘ï¸
ğ‘—=1h
ğ‘¦(ğ‘—)
ğ‘–âˆ’Ë†ğ‘¦(ğ‘—)
ğ‘–i2
=1
ğ‘›ğ‘–ğ‘›ğ‘–âˆ‘ï¸
ğ‘—=1h
ğ‘¦(ğ‘—)
ğ‘–âˆ’N(ğ‘ƒ)
ğ‘–(xğ‘–;Î¸ğ‘ƒ
ğ‘–)(ğ‘—)i2
, ğ‘–=1,2,..., 6 (15)
whereğ‘–=1,2,..., 6 are the different neural networks for the approximation of the empirical
formulae, xğ‘–are the input corresponds to the ğ‘–thnetwork, Ë†ğ‘¦ğ‘–andğ‘¦ğ‘–are the output of the ğ‘–th
network and the corresponding labelled values respectively, ğ‘›ğ‘–is the number of labelled dataset
corresponds to the ğ‘–thneural network. The laboratory data required for calculating labelled data
for training each of these networks are shown in Table 3 (in Â§2.3). We discuss the calculation of
labelled data from the laboratory data in Appendix D. We train these networks using the Adam
optimizer. Upon the training of these DNNs, we will plug them in the PINNs to replace the
empirical models, which are represented by the pretrained neural network with the output ğ‘”(ğ‘¦)
in Fig. 2.
2.2 Flowchart for PINN model for diesel engine
In section 1.1, we discussed the problem setup, and in subsequent sections, we discussed the
approximation of different variables using neural networks as well as the basics of the PINN
method and the implementation of PINN in the present problem. In Fig. 2, we have shown a
schematic diagram along with a pre-trained network for a general ordinary differential equation.
In this section, we show a complete flowchart for the calculation of physics loss functions for
the engine problem. In Fig. 3, we show the flow chart for calculating the physics-informed
loss for the present problem. Note that we have not shown the data loss and the self-adaptive
weights in the flow chart.
11N1(t;Î¸1) tN2(t;Î¸2) t
N3(t;Î¸3) t
N(P)
1(.;Î¸P
1)Cylinder
ï¬‚owÎ·vol
ne
uÎ´Cylinder Tem-
peratureWei
WfÎ·sc
htotT1xr
pem
pim
N4(t;Î¸4) t
EGR Flow AegrmaxTem
N(P)
2(.;Î¸P
2)EGR
dynamics
Ëœuegr
fegrËœuegr1
Ëœuegr2
N6(t;Î¸6) t
Turbine Flow Avgtmaxpamb
pem
N(P)
3(.;Î¸P
3)fvgtÃ—fÎ t Ëœuvgt
Turbine PowerWtN5(t;Î¸5) tpim
pamb
N(P)
6(.;Î¸P
6)Tamb
Compressor ï¬‚owÏ‰t
Î¦c
N(P)
5(.;Î¸P
5)
Compressor PowerWc Î·cr(pim) =dpim
dtâˆ’RaTim
Vim(Wc+Wegrâˆ’Wei)
r(pem) =dpem
dtâˆ’ReTem
Vem(Weoâˆ’Wtâˆ’Wegr)
WegrWeo
r(Ï‰t) =dÏ‰t
dtâˆ’PtÎ·mâˆ’Pc
JtÏ‰t
PcPtÎ·mr(Ëœuegr1) =dËœuegr1
dtâˆ’1
Ï„egr1(uegr(tâˆ’Ï„degr)âˆ’Ëœuegr1)
r(Ëœuegr2) =dËœuegr2
dtâˆ’1
Ï„egr2(uegr(tâˆ’Ï„degr)âˆ’Ëœuegr2)uegr
r(Ëœuvgt) =dËœuvgt
dtâˆ’1
Ï„vgt(uvgt(tâˆ’Ï„dvgt)âˆ’Ëœuvgt)uvgtr(xr) =xrâˆ’Î (1/Î³a)
ex(âˆ’1/Î³a)
p
rcxv
r(T1) =T1âˆ’(xrTe+ (1âˆ’xr)Tim)Î e=pem
pim
Texpxv
N(P)
4(.;Î¸P
4)Î·tmNi(t;Î¸i)Neural Network
N(P)
i(.;Î¸P
i)Pre-trained
Neural NetworkUnknown
ParameterLxr(Î¸) =1
nn/summationdisplay
i=1r(xr)2
LT1(Î¸) =1
nn/summationdisplay
i=1r(T1)2
LËœuegr1(Î¸) =1
nn/summationdisplay
i=1r(Ëœuegr1)2
LËœuegr2(Î¸) =1
nn/summationdisplay
i=1r(Ëœuegr2)2
LËœuvgt(Î¸) =1
nn/summationdisplay
i=1r(Ëœuvgt)2Lpim(Î¸) =1
nn/summationdisplay
i=1r(pim)2
Lpem(Î¸) =1
nn/summationdisplay
i=1r(pem)2
LÏ‰t(Î¸) =1
nn/summationdisplay
i=1r(Ï‰t)2
1
Fig. 3: Flow chart for the proposed PINNs model: (Detailed caption is added on the other
page).
12Detailed Caption for Fig. 3
Fig.3: Flow chart for proposed PINNs model: Flow chart for the proposed PINN model
for the inverse problem for the engine for prediction of dynamics of the system variables and
estimation of unknown parameters. The inputs are input control vector {ğ‘¢ğ›¿,ğ‘¢ğ‘’ğ‘”ğ‘Ÿ,ğ‘¢ğ‘£ğ‘”ğ‘¡}and
engine speed ğ‘›ğ‘’.
â€¢Six neural network Nğ‘–(ğ‘¡;Î¸),ğ‘–=1,2,..., 6 indicated in dashed rectangular oval takes time ğ‘¡
as input and predict ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğ‘¥ğ‘Ÿandğ‘‡1Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2,ğœ”ğ‘¡and Ëœğ‘¢ğ‘£ğ‘”ğ‘¡as shown in Table 1.
â€¢Four unknown parameters indicated in hexagon are ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡,ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ .
â€¢Six pre-trained neural networks N(ğ‘ƒ)
ğ‘–(.;Î¸),ğ‘–=1,2,..., 6 indicated in dashed-dotted rect-
angular oval takes appropriate input and predict the empirical formulae as shown in Table
2. The parameters (weights and biases) of these pre-trained DNNs are kept fixed to predict
the empirical formulae.
There are eight main blocks calculating different variables. The equations for the calculation
of each of the quantities are shown in Appendix B.
â€¢Cylinder flow: calculatesğ‘Šğ‘’ğ‘–,ğ‘Šğ‘“andğ‘Šğ‘’ğ‘œusing Eqs. B.5, B.7 and B.6 respectively.
â€¢Cylinder Temperature: calculatesğ‘¥ğ‘£,ğ‘¥ğ‘,ğ‘‡ğ‘’andğ‘‡ğ‘’ğ‘šusing Eqs. B.14, B.13, B.9 and B.16
respectively. â„ğ‘¡ğ‘œğ‘¡andğœ‚ğ‘ ğ‘are considered as learnable parameters in the calculation of ğ‘‡ğ‘’ğ‘š
andğ‘‡ğ‘’respectively.
â€¢EGR dynamics: calculated Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿusing Eq. B.19
â€¢EGR Flow: calculates EGR mass flow ğ‘Šğ‘’ğ‘”ğ‘Ÿusing Eq. B.20. ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ is considered as
learnable parameter.
â€¢Compressor flow: calculates compressor mass flow ğ‘Šğ‘using Eq. B.49
â€¢Compressor Power: calculates compressor power ğ‘ƒğ‘using Eq. B.39
â€¢Turbine Flow: calculates turbine mass flow ğ‘Šğ‘¡using Eq. B.27. ğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ is considered as
trainable parameter.
â€¢Turbine Power: calculates effective turbine power ğ‘ƒğ‘¡ğœ‚ğ‘šusing Eq. B.32
There are five blocks, which calculate the residual of the equation. The first block calculates
the residual for state equations for ğ‘ğ‘–ğ‘šandğ‘ğ‘’ğ‘š; the second one calculates the residual for the
equations of ğ‘¥ğ‘Ÿandğ‘‡1; the third block calculates the residuals for state equations for Ëœ ğ‘¢ğ‘’ğ‘”ğ‘Ÿ1
and Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2; the fourth block calculates the residual for the state equation for Ëœ ğ‘¢ğ‘£ğ‘”ğ‘¡, and the fifth
block calculates the residual for the state equation for ğœ”ğ‘¡. There are another two blocks, which
calculate the physics loss. The first one calculates the physics loss corresponding to state
variableğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘šandğœ”ğ‘¡. The second block calculates state physics loss corresponding to
state variables Ëœ ğ‘¢ğ‘’ğ‘”ğ‘Ÿ1, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2, Ëœğ‘¢ğ‘£ğ‘”ğ‘¡and physics loss corresponding to ğ‘¥ğ‘Ÿandğ‘‡1. The data losses
can be calculated from the variables calculated from the appropriate blocks.
2.3 Data generation
We now discuss the generation of data for training the NNs utilized in this study. Specifically,
we have mainly two different types of data here: (1) the data collected from the laboratory that
are used to train the DNN surrogates to replace the empirical formulae used in [24]; and (2)
field datağ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿ.
In laboratory experiments, we have measurements on state variables, some of which can be
employed for training the neural network surrogates for the empirical formulae. The laboratory
13data required to calculate the labelled data for each of the surrogates are shown in Table 3. The
calculation of labelled data from laboratory data is discussed in Appendix D. After training, we
consider these pre-trained surrogates in field experiments in place of the empirical formulae.
The parameters of these networks are kept constant in the PINNs model for the field experiment.
In field experiments, we have records for the four inputs, i.e., u={ğ‘¢ğ›¿, ğ‘¢ğ‘’ğ‘”ğ‘Ÿ, ğ‘¢ğ‘£ğ‘”ğ‘¡}andğ‘›ğ‘’.
In addition, we only have measurements on four variables in field experiments, i.e., the intake
manifold pressure ( ğ‘ğ‘–ğ‘š), exhaust manifold pressure ( ğ‘ğ‘’ğ‘š), turbine speed ( ğœ”ğ‘¡) and EGR mass
flow (ğ‘Šğ‘’ğ‘”ğ‘Ÿ).
In both the laboratory and field experiments, we have the records for the inputs (i.e.,
u={ğ‘¢ğ›¿, ğ‘¢ğ‘’ğ‘”ğ‘Ÿ, ğ‘¢ğ‘£ğ‘”ğ‘¡}andğ‘›ğ‘’) from an actual engine running conditions. Considering that
we only have a certain number of records for the variables in the running engine, which
cannot be used to verify our PINN model since our objective is to use it to predict the whole
gas flow dynamics in the engine. We, therefore, take the records for the real inputs (i.e.,
u={ğ‘¢ğ›¿, ğ‘¢ğ‘’ğ‘”ğ‘Ÿ, ğ‘¢ğ‘£ğ‘”ğ‘¡}andğ‘›ğ‘’) and employ them as the inputs for the governing equations Eqs.
3-8. We then solve these equations using Simulink to obtain the dynamics for all variables.
We use the data from Simulink to mimic the real-world measurements, which are employed as
the training data for PINNs and the DNN for the pre-trained networks. The remaining data are
used as the validation data to test the accuracy of PINN for reconstructing the gas dynamics in
a running engine, given partial observations. Given that the real measurements are generally
noisy, we add 3%, 3%, 1% and 10% Gaussian noise in ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿ, respectively.
These different signals have different noise values because they are different measurements with
different noise characteristics.
In the present study, we consider two sets of input data in the training and testing of the
surrogate neural networks for the empirical formulae. The first set of data (Set-I) is two (2)
hours of data collected at a sampling rate of 1 sec. This control input vector {ğ‘¢ğ›¿,ğ‘¢ğ‘’ğ‘”ğ‘Ÿ,ğ‘¢ğ‘£ğ‘”ğ‘¡}
andğ‘›ğ‘’are considered to generate simulated data with different ambient conditions, which are
shown in Table 4. The second set of data (Set-II) is twenty-minute (20 minutes) data collected
at a sampling rate of 0.2 sec. This control input vector {ğ‘¢ğ›¿,ğ‘¢ğ‘’ğ‘”ğ‘Ÿ,ğ‘¢ğ‘£ğ‘”ğ‘¡}andğ‘›ğ‘’are considered to
generate simulated data with Case-V ambient conditions.
The labelled data for the training of surrogate neural network for ğœ‚ğ‘£ğ‘œğ‘™,ğ¹ğ‘£ğ‘”ğ‘¡,Î ğ‘¡,ğœ‚ğ‘andÎ¦ğ‘
are generated for Case-I to Case-IV with a ğ‘‘ğ‘¡=0.2 sec. The testing data are generated for
Case-V with the same ğ‘‘ğ‘¡. We observed from the engine model that the EGR valve actuator
is independent of the other system of the engine and depends only on the EGR control signal
(ğ‘¢ğ‘’ğ‘”ğ‘Ÿ). Thus, for the training of surrogate neural network for ğ‘“ğ‘’ğ‘”ğ‘Ÿ(N(ğ‘ƒ)
2(:,Î¸)), we consider
the training data set corresponding to Case-I only and the testing data set corresponding to
Case-V. The labelled data for ğœ‚ğ‘¡ğ‘šare calculated from Eq. (B.25) ( Â§B.4), which is a differential
equation, thus requires a finer ğ‘‘ğ‘¡. The simulated data for the calculation of labelled ğœ‚ğ‘¡ğ‘šare
generated with ğ‘‘ğ‘¡=0.025 sec in all the Cases. We assume that the Set-I data for the input control
vector includes a good operating range for training surrogate neural networks for the empirical
formulae. The field data ( ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿ) for the inverse problem are considered from
Case-V.
14Empirical quantitiesâ€ â€ ,â€ â€ â€ Symbol Laboratory test data requiredâ€ â€ â€ â€ 
Volumetric efficiency ( Â§B.2)ğœ‚ğ‘£ğ‘œğ‘™â€¢Intake manifold pressure ( ğ‘ğ‘–ğ‘š)
â€¢Engine speed ( ğ‘›ğ‘’)
â€¢Total mass flow from the intake
manifold into the cylinders ( ğ‘Šğ‘’ğ‘–)
â€¢Intake manifold temperature ( ğ‘‡ğ‘–ğ‘š)
Effective area ratio function
for EGR ( Â§B.3)ğ‘“ğ‘’ğ‘”ğ‘Ÿâ€¢EGR position ( Ëœ ğ‘¢ğ‘’ğ‘”ğ‘Ÿ)
â€¢EGR mass flow ( ğ‘Šğ‘’ğ‘”ğ‘Ÿ)
â€¢Exhaust manifold pressure ( ğ‘ğ‘’ğ‘š)
â€¢Intake manifold pressure ( ğ‘ğ‘–ğ‘š)
â€¢Exhaust manifold temperature ( ğ‘‡ğ‘’ğ‘š)
Effective area ratio function
for VGT (ğ‘“ğ‘£ğ‘”ğ‘¡) and chocking
function (ğ‘“Î ğ‘¡) (Â§B.4)ğ‘“ğ‘£ğ‘”ğ‘¡Ã—ğ‘“Î ğ‘¡â€¢VGT position ( Ëœ ğ‘¢ğ‘£ğ‘”ğ‘¡)
â€¢Exhaust manifold pressure ( ğ‘ğ‘’ğ‘š)
â€¢Ambient pressure ( ğ‘ğ‘ğ‘šğ‘)
â€¢Turbine mass flow ( ğ‘Šğ‘¡)
â€¢Exhaust manifold pressure ( ğ‘ğ‘’ğ‘š)
â€¢Exhaust manifold temperature ( ğ‘‡ğ‘’ğ‘š)
Turbine mechanical
efficiencyâ€ â€ â€ â€ â€ (Â§B.4)ğœ‚ğ‘¡ğ‘šâ€¢Turbine speed ( ğœ”ğ‘¡)
â€¢Exhaust manifold temperature ( ğ‘‡ğ‘’ğ‘š)
â€¢Exhaust manifold pressure ( ğ‘ğ‘’ğ‘š)
â€¢Ambient pressure ( ğ‘ğ‘ğ‘šğ‘)
â€¢Compressor mass flow ( ğ‘Šğ‘)
â€¢Compressor temperature ( ğ‘‡ğ‘)
â€¢Ambient temperature ( ğ‘‡ğ‘ğ‘šğ‘)
â€¢Turbine mass flow ( ğ‘Šğ‘¡)
Compressor efficiency ( Â§B.5)ğœ‚ğ‘â€¢Intake manifold pressure ( ğ‘ğ‘–ğ‘š)
â€¢Compressor mass flow ( ğ‘Šğ‘)
â€¢Temperature after the compressor ( ğ‘‡ğ‘)
â€¢Ambient temperature ( ğ‘‡ğ‘ğ‘šğ‘)
â€¢Ambient pressure ( ğ‘ğ‘ğ‘šğ‘)
Volumetric flow coefficient
for compressor ( Â§B.5)Î¦ğ‘â€¢Turbine speed ( ğœ”ğ‘¡)
â€¢Compressor mass flow ( ğ‘Šğ‘)
â€¢Intake manifold pressure ( ğ‘ğ‘–ğ‘š)
â€¢Ambient temperature ( ğ‘‡ğ‘ğ‘šğ‘)
â€¢Ambient pressure ( ğ‘ğ‘ğ‘šğ‘)
â€ It is assumed that the parameters/constant are known, however not the coefficients for
the empirical formulae.
â€ â€ The definition of the quantities are discussed in relevant sections in Appendix B.
â€ â€ â€ The calculations of the empirical quantify from the laboratory data are included
in Appendix D.
â€ â€ â€ â€ A brief discussion on instrumentation and test procedure is included in Appendix C.
â€ â€ â€ â€ â€ For calculation of ğœ‚ğ‘¡ğ‘š, dynamic data are required (discussed in Appendix D and
section H).
Table 3: List of empirical formulae represented using a pre-trained neural network and lab test
data required for their trainingâ€ .
15Caseğ‘‡ğ‘ğ‘šğ‘ ğ‘ğ‘ğ‘šğ‘Ã—105(Pa)InputSamplingPurpose(kelvin) (Approx. elevation) ğ‘‘ğ‘¡
Case-I 233.15 (âˆ’40â—¦C) 0.7000 (at 3000 m) Set-I 1 sec Training
Case-II 233.15 (âˆ’40â—¦C) 1.0111 (at 17.9 m) Set-I 1 sec Training
Case-III 270.15 (âˆ’3â—¦C) 0.7000 (at 3000 m) Set-I 1 sec Training
Case-IV 313.15 (40â—¦C) 1.0111 (at 17.9 m) Set-I 1 sec Training
Case-V 298.15 (25â—¦C) 0.8000 (at 1837 m) Set-II 0.2 sec Testing
Table 4: Ambient conditions for training and testing of neural networks: The different
ambient conditions are considered for generating training and testing data. The input
data Set-I is two hours of input control vector {ğ‘¢ğ›¿,ğ‘¢ğ‘’ğ‘”ğ‘Ÿ,ğ‘¢ğ‘£ğ‘”ğ‘¡}andğ‘›ğ‘’collected from
an actual engine running condition. Similarly, Set-II is twenty-minute of input con-
trol vector{ğ‘¢ğ›¿,ğ‘¢ğ‘’ğ‘”ğ‘Ÿ,ğ‘¢ğ‘£ğ‘”ğ‘¡}andğ‘›ğ‘’collected from an actual engine running condition.
Case-I to Case-IV are considered for the training of the surrogate neural network for
the empirical formulae ( N(ğ‘ƒ)
ğ‘–(:,Î¸ğ‘–)), while Case-V is considered for testing of these
networks. The data for the field data are also considered from Case-V.
3 Results and discussions
In this section, we demonstrate the applicability of proposed PINNs for solving the inverse
problems discussed in section 1.1. Case 1 and Case 2 have three unknown parameters, while
Case 3 to Case 5 have four unknown parameters. The predicted values of the unknowns for all
five cases are shown in Table 5. In this section, we will discuss the results of Case 3 to Case 5.
The results of Case 1 and Case 2 are presented in Appendix G.
First, we study the results of Case 3 and Case 5 to understand the applicability of PINN and
the importance of self-adaptive weight in accuracy and convergence. Then, we study the results
of Case 4, which is similar to Case 3; however, with added noise in the field data considered.
We also discuss the results for the surrogate for the empirical formulae in Appendix H. Note
that the results for all variables are presented in a normalized scale from zero to one using the
following equation,
ğ‘¥ğ‘ ğ‘ğ‘ğ‘™ğ‘’=ğ‘¥âˆ’ğ‘¥ğ‘šğ‘–ğ‘›
ğ‘¥ğ‘šğ‘ğ‘¥âˆ’ğ‘¥ğ‘šğ‘–ğ‘›, (16)
whereğ‘¥andğ‘¥ğ‘ ğ‘ğ‘ğ‘™ğ‘’ are the data before and after scaling, respectively, ğ‘¥ğ‘šğ‘–ğ‘›is the minimum value
of true data of ğ‘¥within the time span considered, ğ‘¥ğ‘šğ‘ğ‘¥is the maximum value of true data of ğ‘¥
within the time span considered.
We are considering the input control vector {ğ‘¢ğ›¿,ğ‘¢ğ‘’ğ‘”ğ‘Ÿ,ğ‘¢ğ‘£ğ‘”ğ‘¡}and engine speed ğ‘›ğ‘’from
an actual field record. It is assumed that these data have inherent noise in their records.
Detailed studies are carried out considering a 1-minute duration. The number of resid-
ual points considered in the physics-informed loss and data loss is 301 at equal ğ‘‘ğ‘¡=0.2
sec. The initial conditions considered for {ğ‘ğ‘–ğ‘š, ğ‘ğ‘’ğ‘š, ğ‘¥ğ‘Ÿ, ğ‘‡ 1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2, ğœ”ğ‘¡,Ëœğ‘¢ğ‘£ğ‘”ğ‘¡}are
{8.0239Ã—104,8.1220Ã—104,0.0505,305.3786,18.2518,18.1813,1.5827Ã—103,90.0317}
respectively. The measured field data are also considered for 1 min with equal ğ‘‘ğ‘¡=0.2 sec.
Thus, each of the measured field quantities has 301 records.
The details of the neural networks considered for the PINN problem are shown in Table 6.
We consider ğœ(.)=tanh(.)activation function for hidden layers for all the neural networks. We
would also like to emphasize that the scaling of output is one of the important considerations for
faster and an accurate convergence of the neural network. Furthermore, output transformation
16ğ´ğ‘’ğ‘”ğ‘Ÿğ‘ğ‘šğ‘¥ ğœ‚ğ‘ ğ‘â„ğ‘¡ğ‘œğ‘¡ğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥Known
variablesPredicted
variables
True 4Ã—10âˆ’41.102 96.28 8.456Ã—10âˆ’4
Case 1 3.93Ã—10âˆ’41.12 110 NAClear data of
ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,
ğ‘Šğ‘’ğ‘”ğ‘ŸThe neural
networks
predict:ğ‘ğ‘–ğ‘š,
ğ‘ğ‘’ğ‘š, Ëœğ‘¢ğ‘£ğ‘”ğ‘¡, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2,ğ‘‡1,ğ‘¥ğ‘Ÿ.
The pretrained
neural networks
predict:ğœ‚ğ‘£ğ‘œğ‘™,
ğœ‚ğ‘¡ğ‘š,ğœ‚ğ‘,Î¦ğ‘,
ğ¹ğ‘£ğ‘”ğ‘¡,Î ğ‘¡,ğ‘“ğ‘’ğ‘”ğ‘Ÿ.
Other variables
are derived
from these
predicted
quantities.Case 2 3.93Ã—10âˆ’41.12 109 NANoisy data of
ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,
ğ‘Šğ‘’ğ‘”ğ‘Ÿ
Case 3 3.61Ã—10âˆ’40.962 113 7.86Ã—10âˆ’4Clear data of
ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,
ğ‘Šğ‘’ğ‘”ğ‘Ÿ
Case 4 3.51Ã—10âˆ’40.834 134 7.27Ã—10âˆ’4Noisy data of
ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,
ğ‘Šğ‘’ğ‘”ğ‘Ÿ
Case 5 2.28Ã—10âˆ’40.829 140 7.27Ã—10âˆ’4Case 3
without
self-adaptive
weights
Mask and scale considered
Mask ExponentialSoft-
plusExpo-
nentialExponentialFor faster convergence and to have
positive value
ScaleÃ—10âˆ’4Ã—1Ã—10Ã—10âˆ’4 Scale to obtain the parameters in
physical domain
Table 5: Predicted unknowns: Predicted unknown parameters for different cases considered.
is another important consideration. The outputs are physical quantity and always positive. The
governing equations are valid only for positive quantities (e.g. in Eq. B.20, a negative ğ‘ğ‘–ğ‘šwill
result in negative ğ‘Šğ‘’ğ‘”ğ‘Ÿ). The output transformation will ensure that the predicted quantities
are always positive in each epoch. Similarly, as shown in Table 5, the mask for the unknown
parameters will ensure a positive value. We also observed that the unknown parameters are of
different scales. The scale considered for the unknown parameters will ensure the optimization
of these parameters is on the same scale. The parameters of the neural network are optimized
first using Adam optimized in Tensorflow-1 with 200 Ã—103epoch and further with LBFGS-B
optimized. It is also important to note that we have considered self-adaptive weights in the
proposed method; thus, we considered different optimizers for each set of self-adaptive weights.
Further, self-adaptive weights are optimized only during the process of Adam optimization
up to 100Ã—103epoch. After 100Ã—103epoch and during the process of optimization using
LBFGS-B, the self-adaptive weights are considered constants with the values at 100 Ã—103epoch
of Adam optimization. The sizes of self-adaptive weight are 301 Ã—1 forÎ»ğ‘ğ‘–ğ‘š,Î»ğ‘ğ‘’ğ‘š,Î»ğœ”ğ‘¡and
Î»ğ‘Šğ‘’ğ‘”ğ‘Ÿ. The size of self adaptive weight of Î»ğ‘‡1is 1Ã—1. Softplus masks are considered for all
the self-adaptive weights.
17Neural
networkNetwork size OutputOutputs transformation
â€¡â€¡ Scaling
N1(ğ‘¡;Î¸1) [ 1,10,10,10,2]ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘šğ‘†ğ‘ƒ(ğ‘ğ‘–ğ‘š)+0.5,ğ‘†ğ‘ƒ(ğ‘ğ‘’ğ‘š)Ã—105
N2(ğ‘¡;Î¸2) [ 1 10,10,1]ğ‘¥ğ‘Ÿ ğ‘†ğ‘ƒ(ğ‘¥ğ‘Ÿ) Ã— 0.03
N3(ğ‘¡;Î¸3) [ 1,15,15,15,1]ğ‘‡1ğ‘†ğ‘ƒ(ğ‘‡1)+230/300Ã—300
N4(ğ‘¡;Î¸4) [ 1,10,10,10,2] Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2ğ‘†(Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2) Ã— 100
N5(ğ‘¡;Î¸5) [ 1,10,10,1]ğœ”ğ‘¡ ğ‘†ğ‘ƒ(ğœ”ğ‘¡)Ã—5Ã—103
N6(ğ‘¡;Î¸6) [ 1,10,10,1] Ëœğ‘¢ğ‘£ğ‘”ğ‘¡ ğ‘†(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡) Ã— 100
â€¡â€¡ğ‘†ğ‘(.)âˆ’â†’ softplus function. ğ‘†(.)âˆ’â†’ sigmoid function
Table 6: Details of neural network for PINNs: Details of neural networks considered to
approximate the state variables and ğ‘‡1andğ‘¥ğ‘Ÿ. The input to the neural networks is time
ğ‘¡and the activation functions for the hidden layers are ğœ(.)=tanh(.). The outputs
for each network are shown in the â€œOutputâ€ column. The â€œOutput transformationâ€
column shows whether the output from the neural network is passed through any other
function. The last column, â€Scalingâ€, shows the scaling factor to be multiplied by the
final output to obtain the variable in physical space. The input to the networks is time
0 to 60 sec and scaled between [âˆ’1,1].
3.1 PINN for the inverse problem with four unknown parameters
Results for Case 3 and Case 5
We first consider Case 3 and Case 5 in which we have four unknown parameters ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡
andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ . The dynamics of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿcan be obtained from the corresponding
sensor measurements. We then employ the PINN to predict the dynamics for the variables and
infer the four unknowns in the system. The difference between the two cases is that in Case 3,
we have considered self-adaptive weights, while in Case 5, we have not considered self-adaptive
weights. We consider these two cases to study the applicability of PINN and the importance of
self-adaptive weights in the present problem.
The predicted output from the neural networks, i.e., the states and ğ‘‡1andğ‘¥ğ‘Ÿare shown in
Fig. 4. The predicted values of the unknown parameters are shown in Table 5. We observe that
the predicted states are in good approximation with the true value in both cases. However, the
predictedğ‘‡1andğ‘¥ğ‘Ÿare not in good agreement with the true value. We study the effect of ğ‘‡1and
ğ‘¥ğ‘Ÿon the other variables by comparing the predicted dynamics of ğ‘‡ğ‘’andğ‘‡ğ‘’ğ‘š(ref Eqs. (B.9)
and (B.16)). We also note that ğ‘‡ğ‘’depends on the unknown ğœ‚ğ‘ ğ‘andğ‘‡ğ‘’ğ‘šdepends on unknowns
ğ‘‡ğ‘’andâ„ğ‘¡ğ‘œğ‘¡. The predicted dynamics of ğ‘‡ğ‘’andğ‘‡ğ‘’ğ‘šare shown in Fig. 5.b and 5.c, respectively.
We observe that both ğ‘‡ğ‘’andğ‘‡ğ‘’ğ‘šshow somewhat good agreement even ğ‘‡1andğ‘¥ğ‘Ÿdo not match
with the true value. The accuracy is more in Case 3 compared to Case 5. We believe that the
difference in the true value and the predicted value is due to the error in the predicted value of
unknown parameters. We also study the dependent variables ğ´ğ‘’ğ‘”ğ‘Ÿandğ‘Šğ‘¡of unknown ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥
andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ , and are shown in Fig. 5.a and 5.d, respectively. We observe that in Case 3, the
predicted dynamics for both variables show good agreement with true value. However, in Case
5, theğ´ğ‘’ğ‘”ğ‘Ÿdoes not show good agreement with true value. This is because the predicted value
ofğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ has more error than Case 3.
180 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Ëœuegr1
(c)uegr1True
Case 5
Case 3
0 20 40 60
t(sec)0.00.51.0Ëœuegr2
(d)uegr2True
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Ëœuvgt
(e) ËœuvgtTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Ï‰t
(f)Ï‰tTrue
Case 5
Case 3
0 20 40 60
t(sec)0.00.51.0T1
(g)T1True
Case 5
Case 3
0 20 40 60
t(sec)0.00.51.0xr
(h)xrTrue
Case 5
Case 3Fig. 4: Predicted states and ğ‘‡1andğ‘¥ğ‘Ÿfor Case 3 and Case 5: Predicted dynamics of the
state variables of the engine and ğ‘‡ğ‘Ÿandğ‘¥1for Case 3 (PINN with self-adaptive weights)
and Case 5 (standard PINN without self-adaptive weights). The variables are scaled
using Eq. 16. It can be observed that the predicted dynamics of the states are in good
agreement with the true values. However, ğ‘‡1andğ‘¥ğ‘Ÿdo not match with the true value.
We study the dependent variables of these two variables, and are shown in Fig. 5.
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(a)AegrTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Te
(b)TeTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(c)TemTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(c)WtTrue
Case 5
Case 3
Fig. 5: Predicted dynamics of dependent variables for Case 3 and Case 5: Predicted dy-
namics ofğ´ğ‘’ğ‘”ğ‘Ÿ,ğ‘‡ğ‘’,ğ‘‡ğ‘’ğ‘šandğ‘Šğ‘¡for Case 3 and Case 5. These variables depend on the
unknown parameters ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ respectively. We also note that ğ‘‡ğ‘’
depends onğ‘‡1andğ‘¥ğ‘Ÿ.
In order to study the importance of self-adaptive weights, we study the convergence of the
unknown parameters for both cases with self-adaptive weight (Case 3) and without self-adaptive
weight (Case 5). The convergences of the unknown parameters with epoch for both cases are
shown in Fig. 6. In Case 3 (with self-adaptive weights), we can observe that the unknown
parameters converge faster and are more accurate. Furthermore, we also study the effect of
19different initialization of network parameters for PINN and self-adaptive weights. We run the
PINN model for Case 3 and Case 5 with different initialization of parameters of PINN (DNN
and unknown parameters) and self-adaptive weight keeping other hyperparameters (number of
epoch considered, learning rate scheduler etc.) the same. The results for both cases are shown in
Fig. 7. It is observed that for unknowns, ğœ‚ğ‘ ğ‘andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ for both cases show similar accuracy.
However, for unknowns, ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ andâ„ğ‘¡ğ‘œğ‘¡, Case 3, which is with self-adaptive weights, shows
better accuracy than Case 5 (without self-adaptive weights) for all the runs. In Fig. 8, we
show the self-adaptive weights for ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿafter 100Ã—103epoch (constant value
after 100Ã—103epoch). Thus, we conclude that self-adaptive weights are important for better
accuracy and convergence for the present problem.
0 1 2
epoch Ã—105246Aegrmax Ã—10âˆ’4
(a)AegrmaxTrue
Case 5
Case 3
0 1 2
epoch Ã—1050.81.0Î·sc
(b)Î·scTrue
Case 5
Case 3
0 1 2
epoch Ã—105100200300htot
(c)htotTrue
Case 5
Case 3
0 1 2
epoch Ã—10581012Avgtmax Ã—10âˆ’4
(d)AvgtmaxTrue
Case 5
Case 3
Fig. 6: Convergence of the unknown parameters for Case 3 and Case 5: Convergence of the
unknown parameters with epoch for Case 3 (PINN with self-adaptive weights) and Case
5 (standard PINN without self-adaptive weights). It is observed that Case 3 converges
faster and also shows better accuracy.
1 15 30
#2.53.03.54.0AegrmaxÃ—10âˆ’4
(a)Aegrmax1 15 30
#0.91.01.1Î·sc
(b)Î·sc1 15 30
#1.01.21.4htotÃ—102
(c)htot1 15 30
#7.58.08.5AvgtmaxÃ—10âˆ’4
(d)Avgtmax
Black dashed lineâ†’true value, Blue dots â†’Case 3, Red dotsâ†’Case 5
Fig. 7: Predicted unknown parameters for Case 3 and Case 5: Predicted unknown param-
eters for Case 3 (PINN with self-adaptive weights) and Case 5 (standard PINN) when
prediction is made multiple times with different initialisation of the parameters of PINN,
self-adaptive weights, and the unknown parameters.
200 20 40 60
t(sec)2004006008001000Î»pim
(a)Î»pim0 20 40 60
t(sec)2004006008001000Î»pem
(b)Î»pem0 20 40 60
t(sec)7008009001000Î»Ï‰t
(c)Î»Ï‰t0 20 40 60
t(sec)02505007501000Î»Wegr
(d)Î»WegrFig. 8: Self-adaptive weights for Case 3: Self-adaptive wights for ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿafter
100Ã—103epoch of Adam optimization. The values of the self-adaptive weights after
100Ã—103Adam optimization and LBFGS-B optimization are constant with the values
of self-adaptive weight at 100 Ã—103epoch.
Results for Case 4: Four unknowns with noisy measurement data
In the previous section, we have shown the effectiveness of PINN and the importance of self-
adaptive weights. In this section, we test the robustness of the proposed PINN formulation for
predicting the gas flow dynamics of the diesel engine given noisy data. In particular, we are
considering Case 4 (the same Case 3 but noisy measure data), in which we have four unknown
parametersğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡andğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ , with noise measurement of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,ğ‘Šğ‘’ğ‘”ğ‘Ÿ.
We contaminate the training data ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,ğ‘Šğ‘’ğ‘”ğ‘Ÿconsidered in Case 3 with Gaussian
noise and consider these as synthetic field measurements. We present the predicted dynamics
of the known data in Fig. 9(a)-(d) and unknown parameters in Table 5. We observe that the
dynamics of the predicted ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡,ğ‘Šğ‘’ğ‘”ğ‘Ÿmatches with the reference solution. However,
in the case of ğ‘Šğ‘’ğ‘”ğ‘Ÿ, there is a small discrepancy in the predicted values near 20-25 sec, which
we can attribute to over-fitting caused by the noisy training data. We study the dynamics of
ğ‘‡ğ‘’ğ‘š,ğ‘Šğ‘’ğ‘–ğ´ğ‘’ğ‘”ğ‘Ÿandğ‘Šğ‘¡on which we do not have any measured data, and these are shown in Fig.
9(e)-(h). We observe that ğ‘Šğ‘’ğ‘–matches with the reference results. Most of the dynamics of ğ´ğ‘’ğ‘”ğ‘Ÿ
andğ‘Šğ‘¡match with the reference solution. The mismatch in these two variables may also be
attributed to over-fitting caused by noisy data. The profile of ğ‘‡ğ‘’ğ‘šmatches with the reference
solution, however, it is not an exact match with the reference solution. This is because of the
error in the predicted value of unknown parameter ğœ‚ğ‘ ğ‘andâ„ğ‘¡ğ‘œğ‘¡. We also note that in the present
study, we do not have any temperature measurements of field data. Thus, we expect an error
in the predicted temperature measurement. We also study the convergence of the unknown
parameters with epoch and shown in Fig. 10. We note that, in this case, we consider the same
hyperparameters in the optimization process. In some cases, we see over-feeting due to noisy
data. This may be controlled by changing the hyperparameters, specially the learning rate for
the self-adaptive weights.
210 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Ï‰t
(c)Ï‰tTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Wegr
(d)WegrTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(e)TemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wei
(f)WeiTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(g)AegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(h)WtTrue
PredFig. 9: Predicted dynamics for variables for Case 4: Predicted dynamics of (a)-(d) variables
whose noisy field measurements are known. (e)-(h) dynamics of other important vari-
ables, which are also dependent on the unknown parameters. These results are for Case
4 with 4 unknown parameters and noisy field measurements.
0 1 2
epoch Ã—105234Aegrmax Ã—10âˆ’4
(a)AegrmaxTrue
Pred
0 1 2
epoch Ã—1050.70.80.91.01.1Î·sc
(b)Î·scTrue
Pred
0 1 2
epoch Ã—105100200300htot
(c)htotTrue
Pred
0 1 2
epoch Ã—105681012Avgtmax Ã—10âˆ’4
(d)AvgtmaxTrue
Pred
Fig. 10: Convergence of the unknown parameters for Case 4: Convergence of the unknown
parameters with epoch for Case 4 (PINN with self-adaptive weights and noisy field
data)
We also study the prediction of empirical formulae in this case and shown in Fig. 11.
We observe that ğœ‚ğ‘£ğ‘œğ‘™andğœ‚ğ‘match with the reference solution. These two quantity gives the
volumetric efficiency of the cylinder and the efficiency of the compressor. The other four
quantities (ğ‘“ğ‘’ğ‘”ğ‘Ÿ,ğ‘“ğ‘£ğ‘”ğ‘¡Ã—ğ‘“Î ğ‘¡,ğœ‚ğ‘¡ğ‘š,Î¦ğ‘), also match most of its points. The discrepancy can be
attributed to the noisy measurement of field data.
220 20 40 60
t(sec)0.000.250.500.751.00Î·vol
(a)Î·volTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Pred
0 20 40 60
t(sec)0.00.51.0fvgtÃ—fÎ t
(c)fvgtÃ—fÎ tTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Î·tm
(d)Î·tmTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Î·c
(e)Î·cTrue
Pred
0 20 40 60
t(sec)0.00.51.0Î¦c
(f) Î¦ cTrue
PredFig. 11: Empirical formulae for Case 4: The prediction of empirical formulae for Case 4
(PINN with self-adaptive weights and noisy field data).
4 Summary
In this study, we proposed a PINNs-based method for estimating unknown parameters and
predicting the dynamics of variables of a mean value diesel engine with VGT and EGR, given
the measurement of a few of its variables. Specifically, we know field data of intake manifold
pressure (ğ‘ğ‘–ğ‘š), exhaust manifold pressure ( ğ‘ğ‘’ğ‘š), turbine speed ( ğœ”ğ‘¡) and EGR flow ( ğ‘Šğ‘’ğ‘”ğ‘Ÿ). We
predicted the dynamics of the system variables and unknown parameters ( ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,ğœ‚ğ‘ ğ‘,â„ğ‘¡ğ‘œğ‘¡and
ğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ ). The input data for the study are considered from actual engine running conditions
and show good accuracy in predicted results. We also studied the importance of self-adaptive
weight in the accuracy and convergence of results. Furthermore, we also showed how we could
approximate empirical formulas for different quantities using neural networks and train them.
We believe the proposed method could be considered for an online monitoring system of diesel
engines. The field-measured data are collected using individual sensors. Thus, in the event of
sensor failure or erroneous data, the method may give erroneous results. The method also does
not consider a failure of engine components, e.g. leakage in the EGR valve. We considered
the engine model proposed in [24]. Future research may include modelling of failure of engine
components. Since the proposed PINN consider online training, with change in input data,
field measured data or ambient condition, the PINN networks are required to train again. The
accuracy of the results also depends on the size of neural networks and the optimization strategy
(e.g. optimizer, learning rate scheduler) considered. For example, a large neural network
or higher value in learning rate may result in overfitting of predicted results. The activation
function also plays an important role in the accuracy and computational cost [11]. Further
study may include a neural architecture search for optimal network sizes considering different
operational ranges. Future studies may include a more robust and efficient PINN method for
23the problem that can be used with edge systems, including proper transfer learning strategies
to reduce the computation cost. As there is noise in the measured data, the future study in this
regard may also be towards uncertainty quantification of the predicted dynamics and unknown
parameters.
Input data and data generations
The input data Set-I and Set-II are collected from actual engine running conditions. These data
are considered to generate simulated data with different ambient conditions using the Simulink
file [22] accompany [24].
Author Contributions Statement
Kamaljyoti Nath: Conceptualization, Formal analysis, Investigation, Methodology, Software,
Validation, Visualization, Writing â€“ original draft, Writing â€“ review & editing. Xuhui Meng:
Conceptualization, Formal analysis, Investigation, Methodology, Software, Validation, Visual-
ization, Writing â€“ original draft, Writing â€“ review & editing. Daniel J Smith: Conceptual-
ization, Data curation, Project administration, Supervision, Writing â€“ original draft, Writing â€“
review & editing. George Em Karniadakis: Conceptualization, Funding acquisition, Project
administration, Resources, Supervision, Writing â€“ original draft, Writing â€“ review & editing.
All authors reviewed the manuscript.
Acknowledgement
KN, XM and GEK would like to acknowledge the support by Cummins Inc. USA.
References
[1] Baydin, A. G., Pearlmutter, B. A., Radul, A. A. and Siskind, J. M. [2018], â€˜Automatic
differentiation in machine learning: a surveyâ€™, Journal of Marchine Learning Research
18, 1â€“43.
[2] Biao, L., Qing-chun, L., Zhen-hua, J. and Sheng-fang, N. [2009], System identification
of locomotive diesel engines with autoregressive neural network, inâ€˜2009 4th IEEE
Conference on Industrial Electronics and Applicationsâ€™, pp. 3417â€“3421.
[3] Cai, S., Mao, Z., Wang, Z., Yin, M. and Karniadakis, G. E. [2022], â€˜Physics-informed
neural networks (PINNs) for fluid mechanics: A reviewâ€™, Acta Mechanica Sinica pp. 1â€“12.
[4] Chen, Y., Lu, L., Karniadakis, G. E. and Negro, L. D. [2020], â€˜Physics-informed neural net-
works for inverse problems in nano-optics and metamaterialsâ€™, Opt. Express 28(8), 11618â€“
11633.
URL: https://opg.optica.org/oe/abstract.cfm?URI=oe-28-8-11618
[5] Cho, J., Nam, S., Yang, H., Yun, S.-B., Hong, Y. and Park, E. [2022], â€˜Separable pinn: Mit-
igating the curse of dimensionality in physics-informed neural networksâ€™, arXiv preprint
arXiv:2211.08761 .
24[6] Cuomo, S., Di Cola, V. S., Giampaolo, F., Rozza, G., Raissi, M. and Piccialli, F. [2022],
â€˜Scientific machine learning through physicsâ€“informed neural networks: Where we are
and whatâ€™s nextâ€™, Journal of Scientific Computing 92(3), 88.
[7] Depina, I., Jain, S., Mar Valsson, S. and Gotovac, H. [2022], â€˜Application of physics-
informed neural networks to inverse problems in unsaturated groundwater flowâ€™, Georisk:
Assessment and Management of Risk for Engineered Systems and Geohazards 16(1), 21â€“
36.
[8] Finesso, R. and Spessa, E. [2014], â€˜A real time zero-dimensional diagnostic model for the
calculation of in-cylinder temperatures, hrr and nitrogen oxides in diesel enginesâ€™, Energy
Conversion and Management 79, 498â€“510.
URL: https://www.sciencedirect.com/science/article/pii/S0196890413008212
[9] Gonz Â´alez, J. P., Ankobea-Ansah, K., Peng, Q. and Hall, C. M. [2022], â€˜On the integration
of physics-based and data-driven models for the prediction of gas exchange processes on
a modern diesel engineâ€™, Proceedings of the Institution of Mechanical Engineers, Part D:
Journal of Automobile Engineering 236(5), 857â€“871.
URL: https://doi.org/10.1177/09544070211031401
[10] Jagtap, A. D. and Karniadakis, G. E. [2020], â€˜Extended physics-informed neural net-
works (XPINNs): A generalized space-time domain decomposition based deep learning
framework for nonlinear partial differential equationsâ€™, Communications in Computational
Physics 28, 2002â€“2041.
[11] Jagtap, A. D. and Karniadakis, G. E. [2023], â€˜How important are activation functions in
regression and classification? a survey, performance comparison, and future directionsâ€™,
Journal of Machine Learning for Modeling and Computing 4(1).
[12] Jagtap, A. D., Kharazmi, E. and Karniadakis, G. E. [2020], â€˜Conservative physics-
informed neural networks on discrete domains for conservation laws: Applications to
forward and inverse problemsâ€™, Computer Methods in Applied Mechanics and Engineer-
ing365, 113028.
URL: https://www.sciencedirect.com/science/article/pii/S0045782520302127
[13] Jagtap, A. D., Mao, Z., Adams, N. and Karniadakis, G. E. [2022], â€˜Physics-informed
neural networks for inverse problems in supersonic flowsâ€™, Journal of Computational
Physics 466, 111402.
URL: https://www.sciencedirect.com/science/article/pii/S0021999122004648
[14] Kharazmi, E., Zhang, Z. and Karniadakis, G. E. [2021], â€˜hp-VPINNs variational physics-
informed neural networks with domain decompositionâ€™, Computer Methods in Applied
Mechanics and Engineering 374, 113547.
URL: https://www.sciencedirect.com/science/article/pii/S0045782520307325
[15] Kingma, D. P. and Ba, J. [2014], â€˜Adam: A method for stochastic optimizationâ€™, arXiv
preprint arXiv:1412.6980 .
[16] Kumar, V., Goswami, S., Smith, D. J. and Karniadakis, G. E. [2023], â€˜Real-time prediction
of multiple output states in diesel engines using a deep neural operator frameworkâ€™, arXiv
preprint arXiv:2304.00567 .
25[17] Lawal, Z. K., Yassin, H., Lai, D. T. C. and Che Idris, A. [2022], â€˜Physics-informed neural
network (pinn) evolution and beyond: A systematic literature review and bibliometric
analysisâ€™, Big Data and Cognitive Computing 6(4).
URL: https://www.mdpi.com/2504-2289/6/4/140
[18] Lu, L., Jin, P., Pang, G., Zhang, Z. and Karniadakis, G. E. [2021], â€˜Learning nonlinear
operators via deeponet based on the universal approximation theorem of operatorsâ€™, Nature
machine intelligence 3(3), 218â€“229.
[19] McClenny, L. and Braga-Neto, U. [2020], â€˜Self-adaptive physics-informed neural networks
using a soft attention mechanismâ€™, arXiv preprint arXiv:2009.04544 .
[20] Meng, X., Li, Z., Zhang, D. and Karniadakis, G. E. [2020], â€˜PPINN: Parareal physics-
informed neural network for time-dependent PDEsâ€™, Computer Methods in Applied Me-
chanics and Engineering 370, 113250.
URL: https://www.sciencedirect.com/science/article/pii/S0045782520304357
[21] Raissi, M., Perdikaris, P. and Karniadakis, G. [2019], â€˜Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear
partial differential equationsâ€™, Journal of Computational Physics 378, 686â€“707.
URL: https://www.sciencedirect.com/science/article/pii/S0021999118307125
[22] Software packages from Vehicular Systems [2010], http://www.fs.isy.liu.se/
Software . [Online].
[23] Tosun, E., Aydin, K. and Bilgili, M. [2016], â€˜Comparison of linear regression and artificial
neural network model of a diesel engine fueled with biodiesel-alcohol mixturesâ€™, Alexan-
dria Engineering Journal 55(4), 3081â€“3089.
URL: https://www.sciencedirect.com/science/article/pii/S1110016816302228
[24] Wahlstr Â¨om, J. and Eriksson, L. [2011], â€˜Modelling diesel engines with a variable-geometry
turbocharger and exhaust gas recirculation by optimization of model parameters for captur-
ing non-linear system dynamicsâ€™, Proceedings of the Institution of Mechanical Engineers,
Part D: Journal of Automobile Engineering 225(7), 960â€“986.
URL: https://doi.org/10.1177/0954407011398177
26Appendices
Appendix A : Note on neural network and training of PINN
Appendix B : Engine model
Appendix C : Brief discussion on lab test data
Appendix D : Calculation of labelled data for training of the neural network for the empirical
formulae
Appendix E : Detailed loss function for the PINNs model for the engine
Appendix F : Additional Tables
Appendix G : Additional figures (Results for Case 1 and Case 2)
Appendix H : Neural network surrogates for empirical formulae
Appendix A Note on neural network and training of PINN
In this section, we present more details on neural networks, PINN and optimization for inverse
problems. As shown in Fig. 2, the neural network (FFN/DNN) takes time ğ‘¡as input and
approximates the unknown variable ğ‘¦. For a DNN with ğ‘›âˆ’1 hidden layers, the equation for the
neural network can be written as,
y0=ğ‘¡ Input (A.1a)
yğ‘–=ğœ(Wğ‘–yğ‘–âˆ’1+bğ‘–)ğ‘–âˆ€1â‰¤ğ‘–â‰¤ğ‘›âˆ’1 Hidden layers (A.1b)
Ë†y=yğ‘›=Wğ‘›yğ‘›âˆ’1+bğ‘›âˆ’1 Output layer (A.1c)
where, Wandbare the weights matrices and bias vectors of the network, ğœ(.)is an activation
function, which is considered as hyperbolic tangent function in the present study. The output Ë† ğ‘¦is
a function of input ğ‘¡parameterized by the weights Wand biases b. We can tune the parameters
of the network to predict a large number of snapshots ğ‘¦by minimizing a loss function using an
appropriate optimization technique.
In the case of a data-driven model of neural networks, the loss function is generally consid-
ered as the mean square error (MSE) between the predicted ( Ë† ğ‘¦) and the exact value ( ğ‘¦). On the
other hand, in the case of PINN, the neural network output is made to satisfy the differential
equation and the initial/boundary conditions. The derivatives of the equation are generally eval-
uated using automatic differentiation. As discussed in section 2.1, the loss function is a weighted
sum of physics loss which is MSE of residual and boundary/initial loss, which is MSE between
predicted and exact boundary/initial value. The optimal parameters (weights and biases) of the
network are obtained using an optimization method such as Adam or L-BFGS-B. In the case
of an inverse problem using PINN where the objective is to predict unknown parameters ( Î›)
along with the variable ( ğ‘¦). The unknown parameters are also optimized along with the network
parameters. Thus, the trainable parameters are weights, biases and the unknown parameters
(Î¸={W,b,Î›}). Also, an additional loss function is added, which is data loss between the
predicted and the known value of ğ‘¦. Furthermore, in the present study we have considered self
27adaptive wights [19] for the loss function. The loss function is maximized with respect to the
self self adaptive weights ( ğœ†). Thus, the optimization process may be written as,
min
Î¸max
Î»L(Î¸,Î») (A.2)
Consider the updates of a gradient descent/ascent approach to this problem
Î¸=Î¸âˆ’ğ‘™ğ‘Ÿğœƒâˆ‡ğœƒL(Î¸,Î») (A.3a)
Î»=Î»+ğ‘™ğ‘Ÿğœ†âˆ‡ğœ†L(Î¸,Î») (A.3b)
where,ğ‘™ğ‘Ÿğœƒandğ‘™ğ‘Ÿğœ†are the learning rate associated with Î¸andÎ».
In the present study, the parameters are optimized using Adam and L-BFGS-B optimizer in
Tensorflow-1 (with single precision floating point). Further, self-adaptive weights are optimized
only during the process of Adam optimization up to fixed epoch as discussed in section 3
Appendix B Engine model
As discussed in section 1.1, we consider a mean value engine model proposed by Wahlstr Â¨om
and Eriksson [24] in our present study. The engine has eight states, and we have considered six
in the present study. These are,
x={ğ‘ğ‘–ğ‘š, ğ‘ğ‘’ğ‘š, ğœ”ğ‘¡,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2,Ëœğ‘¢ğ‘£ğ‘”ğ‘¡} (B.1)
whereğ‘ğ‘–ğ‘šandğ‘ğ‘’ğ‘šare the intake and exhaust manifold pressure, respectively, ğœ”ğ‘¡is the turbo
speed. Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1and Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2are the two states for the EGR actuator dynamics, and Ëœ ğ‘¢ğ‘£ğ‘”ğ‘¡represents
the VGT actuator dynamics. The control inputs for the engine are u={ğ‘¢ğ›¿, ğ‘¢ğ‘’ğ‘”ğ‘Ÿ, ğ‘¢ğ‘£ğ‘”ğ‘¡}and the
engine speed is ğ‘›ğ‘’. Whereğ‘¢ğ›¿is the mass of injected fuel, ğ‘¢ğ‘’ğ‘”ğ‘Ÿandğ‘¢ğ‘£ğ‘”ğ‘¡are the EGR and VGT
valve positions, respectively. The mean value engine model is then expressed as
Â¤x=ğ‘“(x,u,ğ‘›ğ‘’). (B.2)
The engine model consists of 6 parts intake and exhaust manifold, the cylinder, the turbine,
EGR valve system, and the compressor system. A schematic diagram of the engine is shown
in Fig. 1. In this section, we briefly discuss the equation required for the present study and
these are taken from [24]. For detail of the engine model, the interested reader may refer to
Wahlstr Â¨om and Eriksson [24].
B.1 Manifold pressures
The pressure at the intake manifold ( ğ‘ğ‘–ğ‘š) is modelled using a first-order differential equation as,
ğ‘‘
ğ‘‘ğ‘¡ğ‘ğ‘–ğ‘š=ğ‘…ğ‘ğ‘‡ğ‘–ğ‘š
ğ‘‰ğ‘–ğ‘š ğ‘Šğ‘+ğ‘Šğ‘’ğ‘”ğ‘Ÿâˆ’ğ‘Šğ‘’ğ‘–(B.3)
whereğ‘‡ğ‘–ğ‘šandğ‘‰ğ‘–ğ‘šare the temperature and volume of the intake manifold, respectively, and
both are assumed to be constant, ğ‘Šğ‘,ğ‘Šğ‘’ğ‘”ğ‘Ÿandğ‘Šğ‘’ğ‘–are the compressor mass flow, EGR mass
flow and total mass flow, respectively. The ideal gas constant and specific heat capacity of the
air areğ‘…ğ‘andğ›¾ğ‘, respectively.
Similarly, the exhaust manifold pressure ğ‘ğ‘’ğ‘šis modelled as,
ğ‘‘
ğ‘‘ğ‘¡ğ‘ğ‘’ğ‘š=ğ‘…ğ‘’ğ‘‡ğ‘’ğ‘š
ğ‘‰ğ‘’ğ‘š ğ‘Šğ‘’ğ‘œâˆ’ğ‘Šğ‘¡âˆ’ğ‘Šğ‘’ğ‘”ğ‘Ÿ(B.4)
28whereğ‘…ğ‘’is the ideal gas constant of the exhaust gas with specific heat capacity ğ›¾ğ‘’,ğ‘‡ğ‘’ğ‘šand
ğ‘‰ğ‘’ğ‘šare the exhaust manifold temperature, and volume, ğ‘Šğ‘’ğ‘œ,ğ‘Šğ‘¡are the mass flow out from the
cylinder and turbine mass flow, respectively.
B.2 Cylinder
The total mass flow from the intake manifold to the cylinder ğ‘Šğ‘’ğ‘–, and the total mass flow out of
the cylinderğ‘Šğ‘’ğ‘œare modelled as,
ğ‘Šğ‘’ğ‘–=ğœ‚ğ‘£ğ‘œğ‘™ğ‘ğ‘–ğ‘šğ‘›ğ‘’ğ‘‰ğ‘‘
120ğ‘…ğ‘ğ‘‡ğ‘–ğ‘š(B.5)
ğ‘Šğ‘’ğ‘œ=ğ‘Šğ‘“+ğ‘Šğ‘’ğ‘– (B.6)
whereğ‘Šğ‘“is the fuel mass flow into the cylinder is given by,
ğ‘Šğ‘“=10âˆ’6
120ğ‘¢ğ›¿ğ‘›ğ‘’ğ‘›ğ‘ğ‘¦ğ‘™, (B.7)
ğ‘‰ğ‘‘,ğ‘›ğ‘’andğ‘›ğ‘ğ‘¦ğ‘™are the displaced volume, engine speed and the number of cylinders, respectively.
The volumetric efficiency, ğœ‚ğ‘£ğ‘œğ‘™of the cylinder may be modelled as
ğœ‚ğ‘£ğ‘œğ‘™=ğ‘ğ‘£ğ‘œğ‘™1âˆšğ‘ğ‘–ğ‘š+ğ‘ğ‘£ğ‘œğ‘™2âˆšğ‘›ğ‘’+ğ‘ğ‘£ğ‘œğ‘™3 (B.8)
whereğ‘ğ‘£ğ‘œğ‘™1,ğ‘ğ‘£ğ‘œğ‘™2andğ‘ğ‘£ğ‘œğ‘™3are constant.
The temperature at cylinder out based upon ideal-gas Seliger cycle (or limited pressure
cycle) and given as,
ğ‘‡ğ‘’=ğœ‚ğ‘ ğ‘Î 1âˆ’1/ğ›¾ğ‘ğ‘’ğ‘Ÿ1âˆ’ğ›¾ğ‘ğ‘ğ‘¥1/ğ›¾ğ‘âˆ’1
ğ‘
ğ‘ğ‘–ğ‘›1âˆ’ğ‘¥ğ‘ğ‘£
ğ‘ğ‘ğ‘+ğ‘¥ğ‘ğ‘£
ğ‘ğ‘‰ğ‘
+ğ‘‡1ğ‘Ÿğ›¾ğ‘âˆ’1
ğ‘
(B.9)
where the pressure ratio ( Î ğ‘’) over the cylinder is ratio of pressure at exhaust ( ğ‘ğ‘’ğ‘š) and intake
(ğ‘ğ‘–ğ‘š),
Î ğ‘’=ğ‘ğ‘’ğ‘š
ğ‘ğ‘–ğ‘š, (B.10)
the fuel consumed during constant-volume combustion is ğ‘¥ğ‘ğ‘£and fuel consumed during constant
pressure combustion is 1 âˆ’ğ‘¥ğ‘ğ‘£,ğœ‚ğ‘ ğ‘andğ‘Ÿğ‘are compensation factor for non-ideal cycles and
compression ratio. The temperature, ğ‘‡1when the inlet valve closes and after the intake stroke
and mixing is given by,
ğ‘‡1=ğ‘¥ğ‘Ÿğ‘‡ğ‘’+(1âˆ’ğ‘¥ğ‘Ÿ)ğ‘‡ğ‘–ğ‘š (B.11)
The residual gas fraction ( ğ‘¥ğ‘Ÿ) is model as
ğ‘¥ğ‘Ÿ=Î 1/ğ›¾ğ‘ğ‘’ğ‘¥âˆ’1/ğ›¾ğ‘ğ‘
ğ‘Ÿğ‘ğ‘¥ğ‘£(B.12)
The pressure ratio ( ğ‘¥ğ‘) and the volume ratio ( ğ‘¥ğ‘£) in the Seliger cycle between point 3 (after
combustion) and point 2 (before combustion) are modelled as,
ğ‘¥ğ‘=ğ‘3
ğ‘2=1+ğ‘ğ‘–ğ‘›ğ‘¥ğ‘ğ‘£
ğ‘ğ‘‰ğ‘ğ‘‡1ğ‘Ÿğ›¾ğ‘âˆ’1
ğ‘(B.13)
ğ‘¥ğ‘£=ğ‘£3
ğ‘£2=1+ğ‘ğ‘–ğ‘›(1âˆ’ğ‘¥ğ‘ğ‘£)
ğ‘ğ‘ğ‘h
(ğ‘ğ‘–ğ‘›ğ‘¥ğ‘ğ‘£/ğ‘ğ‘‰ğ‘)+ğ‘‡1ğ‘Ÿğ›¾ğ‘âˆ’1
ğ‘i (B.14)
29where the specific energy constant of the charge is modelled as
ğ‘ğ‘–ğ‘›=ğ‘Šğ‘“ğ‘ğ»ğ‘‰
ğ‘Šğ‘’ğ‘–+ğ‘Šğ‘“(1âˆ’ğ‘¥ğ‘Ÿ) (B.15)
The temperature at cylinder out modelled in Eq. (B.9) is the temperature at the cylinder exit.
However, it is not the same as the temperature as the exhaust manifold. This is due to the heat
loss in the exhaust pipes between the cylinder and the exhaust manifold. The exhaust manifold
temperature ( ğ‘‡ğ‘’ğ‘š) is given as
ğ‘‡ğ‘’ğ‘š=ğ‘‡ğ‘ğ‘šğ‘+(ğ‘‡ğ‘’âˆ’ğ‘‡ğ‘ğ‘šğ‘)expâˆ’â„ğ‘¡ğ‘œğ‘¡ğœ‹ğ‘‘ğ‘ğ‘–ğ‘ğ‘’ğ‘™ğ‘ğ‘–ğ‘ğ‘’ğ‘›ğ‘ğ‘–ğ‘ğ‘’
ğ‘Šğ‘’ğ‘œğ‘ğ‘ğ‘’
(B.16)
whereğ‘‡ğ‘ğ‘šğ‘is the ambient temperature, ğ‘‘ğ‘ğ‘–ğ‘ğ‘’,ğ‘™ğ‘ğ‘–ğ‘ğ‘’andğ‘›ğ‘ğ‘–ğ‘ğ‘’are the pipe diameter, pipe length
and the number of pipes, respectively.
B.3 EGR valve
The actuator dynamics of the EGR-valve are modelled as,
ğ‘‘
ğ‘‘ğ‘¡Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1=1
ğœğ‘’ğ‘”ğ‘Ÿ1
ğ‘¢ğ‘’ğ‘”ğ‘Ÿ(ğ‘¡âˆ’ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿ)âˆ’Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1
(B.17)
ğ‘‘
ğ‘‘ğ‘¡Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2=1
ğœğ‘’ğ‘”ğ‘Ÿ2
ğ‘¢ğ‘’ğ‘”ğ‘Ÿ(ğ‘¡âˆ’ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿ)âˆ’Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2
(B.18)
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ=ğ¾ğ‘’ğ‘”ğ‘ŸËœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1âˆ’(ğ¾ğ‘’ğ‘”ğ‘Ÿâˆ’1)Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2 (B.19)
whereğœğ‘’ğ‘”ğ‘Ÿ1,ğœğ‘’ğ‘”ğ‘Ÿ2are time constants, ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿis the time delay and ğ¾ğ‘’ğ‘”ğ‘Ÿis a constant that affect
the overshoot.
We model the mass flow through the EGR valve through the restriction ( ğ‘ğ‘’ğ‘š<ğ‘ğ‘–ğ‘š) as,
ğ‘Šğ‘’ğ‘”ğ‘Ÿ=ğ´ğ‘’ğ‘”ğ‘Ÿğ‘ğ‘–ğ‘šÎ¨ğ‘’ğ‘”ğ‘Ÿâˆšğ‘‡ğ‘’ğ‘šğ‘…ğ‘’(B.20)
whereÎ¨ğ‘’ğ‘”ğ‘Ÿis a parabolic function
Î¨ğ‘’ğ‘”ğ‘Ÿ=1âˆ’1âˆ’Î ğ‘’ğ‘”ğ‘Ÿ
1âˆ’Î ğ‘’ğ‘”ğ‘Ÿğ‘œğ‘ğ‘¡âˆ’12
(B.21)
The effective area is modelled as,
ğ´ğ‘’ğ‘”ğ‘Ÿ=ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ğ‘“ğ‘’ğ‘”ğ‘Ÿ(Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ) (B.22)
When the sonic conditions are reached (flow is choked) in the throat and when no backflow can
occur (1<ğ‘ğ‘–ğ‘š/ğ‘ğ‘’ğ‘š), the pressure ratio Î ğ‘’ğ‘”ğ‘Ÿover the valve is limited and modelled as,
Î ğ‘’ğ‘”ğ‘Ÿ=ï£±ï£´ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³Î ğ‘’ğ‘”ğ‘Ÿğ‘œğ‘ğ‘¡ ifğ‘ğ‘–ğ‘š
ğ‘ğ‘’ğ‘š<Î ğ‘’ğ‘”ğ‘Ÿğ‘œğ‘ğ‘¡
ğ‘ğ‘–ğ‘š
ğ‘ğ‘’ğ‘šifÎ ğ‘’ğ‘”ğ‘Ÿğ‘œğ‘ğ‘¡â‰¤ğ‘ğ‘–ğ‘š
ğ‘ğ‘’ğ‘šâ‰¤1
1 if 1 <ğ‘ğ‘–ğ‘š
ğ‘ğ‘’ğ‘š(B.23)
30ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ,Î ğ‘’ğ‘”ğ‘Ÿğ‘œğ‘ğ‘¡ are constant and ğ‘“ğ‘’ğ‘”ğ‘Ÿ(Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ)is modelled as a polynomial function,
ğ‘“ğ‘’ğ‘”ğ‘Ÿ(Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ)=ï£±ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£³ğ‘ğ‘’ğ‘”ğ‘Ÿ1Ëœğ‘¢2
ğ‘’ğ‘”ğ‘Ÿ+ğ‘ğ‘’ğ‘”ğ‘Ÿ2Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ+ğ‘ğ‘’ğ‘”ğ‘Ÿ3if Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿâ‰¤âˆ’ğ‘ğ‘’ğ‘”ğ‘Ÿ2
2ğ‘ğ‘’ğ‘”ğ‘Ÿ1
ğ‘ğ‘’ğ‘”ğ‘Ÿ3âˆ’ğ‘2
ğ‘’ğ‘”ğ‘Ÿ2
4ğ‘ğ‘’ğ‘”ğ‘Ÿ1if Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ>âˆ’ğ‘ğ‘’ğ‘”ğ‘Ÿ2
2ğ‘ğ‘’ğ‘”ğ‘Ÿ1(B.24)
whereğ‘ğ‘’ğ‘”ğ‘Ÿ1,ğ‘ğ‘’ğ‘”ğ‘Ÿ2andğ‘ğ‘’ğ‘”ğ‘Ÿ3are constant.
B.4 Turbocharger
The turbo speed, ğœ”ğ‘¡is modelled as a first-order differential model as,
ğ‘‘
ğ‘‘ğ‘¡ğœ”ğ‘¡=ğ‘ƒğ‘¡ğœ‚ğ‘šâˆ’ğ‘ƒğ‘
ğ½ğ‘¡ğœ”ğ‘¡(B.25)
whereğ½ğ‘¡is the inertia, ğ‘ƒğ‘¡andğ‘ƒğ‘are the power delivered by the turbine and power required to
drive the compressor, respectively, ğœ‚ğ‘šis the mechanical efficiency of the turbocharger.
The VGT actuator system is modelled as a first-order system
ğ‘‘Ëœğ‘¢ğ‘£ğ‘”ğ‘¡
ğ‘‘ğ‘¡=1
ğœğ‘£ğ‘”ğ‘¡
ğ‘¢ğ‘£ğ‘”ğ‘¡(ğ‘¡âˆ’ğœğ‘‘ğ‘£ğ‘”ğ‘¡)âˆ’Ëœğ‘¢ğ‘£ğ‘”ğ‘¡
(B.26)
whereğœğ‘£ğ‘”ğ‘¡andğœğ‘‘ğ‘£ğ‘”ğ‘¡are the time constant and time delay respectively.
The turbine mass flow ( ğ‘Šğ‘¡) is calculated using
ğ‘Šğ‘¡=ğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ğ‘ğ‘’ğ‘šğ‘“Î ğ‘¡(Î ğ‘¡)ğ‘“ğ‘£ğ‘”ğ‘¡(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡))
âˆšğ‘‡ğ‘’ğ‘šğ‘…ğ‘’(B.27)
ğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ is the maximum area in the turbine that the gas flow through.
ğ‘“Î ğ‘¡(Î ğ‘¡)=âˆšï¸ƒ
1âˆ’Î ğ¾ğ‘¡
ğ‘¡ (B.28)
whereğ¾ğ‘¡a constant and Î ğ‘¡=ğ‘ğ‘’ğ‘ /ğ‘ğ‘’ğ‘š.ğ‘ğ‘’ğ‘ >ğ‘ğ‘ğ‘šğ‘if there is a restriction like an after-treatment
system. However, in the model we consider, there is no restriction after the turbine, thus
Î ğ‘¡=ğ‘ğ‘ğ‘šğ‘
ğ‘ğ‘’ğ‘š(B.29)
Further, with the increase in VGT control signal ( ğ‘¢ğ‘£ğ‘”ğ‘¡), the effective area increases and thus
also increases the flow. The effective area of the VGT ğ‘“ğ‘£ğ‘”ğ‘¡(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡)is modelled as an ellipse
ğ‘“ğ‘£ğ‘”ğ‘¡(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡)âˆ’ğ‘ğ‘“2
ğ‘ğ‘“12
+Ëœğ‘¢ğ‘£ğ‘”ğ‘¡âˆ’ğ‘ğ‘£ğ‘”ğ‘¡2
ğ‘ğ‘£ğ‘”ğ‘¡12
=1 (B.30)
which is
ğ‘“ğ‘£ğ‘”ğ‘¡(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡)=ğ‘ğ‘“2+ğ‘ğ‘“1vut
max 
0,1âˆ’Ëœğ‘¢ğ‘£ğ‘”ğ‘¡âˆ’ğ‘ğ‘£ğ‘”ğ‘¡2
ğ‘ğ‘£ğ‘”ğ‘¡12!
(B.31)
The power delivered by the turbine, ğ‘ƒğ‘¡and the mechanical efficiency of the turbocharger ğœ‚ğ‘š
are modelled as,
ğ‘ƒğ‘¡ğœ‚ğ‘š=ğœ‚ğ‘¡ğ‘šğ‘Šğ‘¡ğ‘ğ‘ğ‘’ğ‘‡ğ‘’ğ‘š
1âˆ’Î 1âˆ’1/ğ›¾ğ‘’
ğ‘¡
(B.32)
31ğœ‚ğ‘¡ğ‘š=ğœ‚ğ‘¡ğ‘š,ğ‘šğ‘ğ‘¥âˆ’ğ‘ğ‘š(ğµğ‘†ğ‘…âˆ’ğµğ‘†ğ‘…ğ‘œğ‘ğ‘¡)2(B.33)
where, the blade speed ratio (BSR) is defined as the ratio of the turbine blade tip speed to the
speed which a gas reaches when expanded entropically at the given pressure ratio Î ğ‘¡.
ğµğ‘†ğ‘…=ğ‘…ğ‘¡ğœ”ğ‘¡âˆšï¸ƒ
2ğ‘ğ‘ğ‘’ğ‘‡ğ‘’ğ‘š(1âˆ’Î 1âˆ’1/ğ›¾ğ‘’
ğ‘¡)(B.34)
whereğ‘…ğ‘¡is the turbine blade radius, and
ğ‘ğ‘š=ğ‘ğ‘š1[ğ‘šğ‘ğ‘¥(0,ğœ”ğ‘¡âˆ’ğ‘ğ‘š2]ğ‘ğ‘š3(B.35)
B.5 Compressor
The compressor model consists of two models: the compressor efficiency model and the
compressor mass flow model. The compressor efficiency is defined as the ratio of the power
from the isentropic process ( ğ‘ƒğ‘,ğ‘ ) to the compressor power ( ğ‘ƒğ‘)
ğœ‚ğ‘=ğ‘ƒğ‘,ğ‘ 
ğ‘ƒğ‘=ğ‘‡ğ‘ğ‘šğ‘(Î 1âˆ’1/ğ›¾ğ‘ğ‘âˆ’1)
ğ‘‡ğ‘âˆ’ğ‘‡ğ‘ğ‘šğ‘(B.36)
whereğ‘‡ğ‘is the temperature after the compressor, and the pressure ratio is given by,
Î ğ‘=ğ‘ğ‘–ğ‘š
ğ‘ğ‘ğ‘šğ‘(B.37)
The power from the isentropic process is given as,
ğ‘ƒğ‘,ğ‘ =ğ‘Šğ‘ğ‘ğ‘ğ‘ğ‘‡ğ‘ğ‘šğ‘(Î 1âˆ’1/ğ›¾ğ‘ğ‘âˆ’1) (B.38)
whereğ‘Šğ‘is the compressor mass flow and ğ‘ğ‘ğ‘is a constant. Thus, the compressor power can
be modelled from Eq. B.36 and Eq. B.38 as
ğ‘ƒğ‘=ğ‘ƒğ‘,ğ‘ 
ğœ‚ğ‘=ğ‘Šğ‘ğ‘ğ‘ğ‘ğ‘‡ğ‘ğ‘šğ‘
ğœ‚ğ‘(Î 1âˆ’1/ğ›¾ğ‘âˆ’1
ğ‘) (B.39)
ğœ‚ğ‘is modelled as an ellipses, which depends on the pressure ratio ( Î ğ‘) and compressor mass
flow (ğ‘Šğ‘),
ğœ‚ğ‘=ğœ‚ğ‘ğ‘šğ‘ğ‘¥âˆ’Xğ‘‡Qğ‘X (B.40)
whereXis a vector and given as,
X=ğ‘Šğ‘âˆ’ğ‘Šğ‘ğ‘œğ‘ğ‘¡
ğœ‹ğ‘âˆ’ğœ‹ğ‘ğ‘œğ‘ğ‘¡
(B.41)
whereğ‘Šğ‘ğ‘œğ‘ğ‘¡andğœ‹ğ‘ğ‘œğ‘ğ‘¡are the optimum values of ğ‘Šğ‘andğœ‹ğ‘respectively. ğœ‹ğ‘is a non linear
transformation of Î ğ‘as
ğœ‹ğ‘=(Î ğ‘âˆ’1)ğ‘ğœ‹(B.42)
andQğ‘is a semi-definite matrix
Qğ‘=ğ‘1ğ‘3
ğ‘3ğ‘2
(B.43)
32The model for compressor mass flow, ğ‘Šğ‘is modelled using two non-dimensional variables:
energy transfer coefficient ( Î¨ğ‘) and volumetric flow coefficient ( Î¦ğ‘). The energy transfer
coefficient is defined as,
Î¨ğ‘=2ğ‘ğ‘ğ‘ğ‘‡ğ‘ğ‘šğ‘(Î 1âˆ’1/ğ›¾ğ‘ğ‘âˆ’1)
ğ‘…2ğ‘ğœ”2
ğ‘¡(B.44)
whereğ‘…ğ‘is the compressor blade ratio. The volumetric flow coefficient is defined as,
Î¦ğ‘=ğ‘Šğ‘/ğœŒğ‘ğ‘šğ‘
ğœ‹ğ‘…3ğ‘ğœ”ğ‘¡=ğ‘…ğ‘ğ‘‡ğ‘ğ‘šğ‘
ğ‘ğ‘ğ‘šğ‘ğœ‹ğ‘…3ğ‘ğœ”ğ‘¡ğ‘Šğ‘ (B.45)
The energy transfer coefficient ( Î¨ğ‘) and volumetric flow coefficient ( Î¦ğ‘) can be described by a
part of an ellipse,
ğ‘Î¨1(ğœ”ğ‘¡)(Î¨ğ‘âˆ’ğ‘Î¨2)2+ğ‘Î¦1(ğœ”ğ‘¡)(Î¦ğ‘âˆ’ğ‘Î¦2)2=1 (B.46)
whereğ‘Î¨1andğ‘Î¦1are function of turbine speed ( ğœ”ğ‘¡) and modelled as a second order polynomial
as,
ğ‘Î¨1(ğœ”ğ‘¡)=ğ‘ğœ”Î¨1ğœ”2
ğ‘¡+ğ‘ğœ”Î¨2ğœ”ğ‘¡+ğ‘ğœ”Î¨3 (B.47)
ğ‘Î¦1(ğœ”ğ‘¡)=ğ‘ğœ”Î¦1ğœ”2
ğ‘¡+ğ‘ğœ”Î¦2ğœ”ğ‘¡+ğ‘ğœ”Î¦3 (B.48)
Solving Eq. B.46 for Î¦ğ‘and Eq. B.45 for ğ‘Šğ‘, the compressor mass flow is given as,
ğ‘Šğ‘=ğ‘ğ‘ğ‘šğ‘ğœ‹ğ‘…3
ğ‘ğœ”ğ‘¡
ğ‘…ğ‘ğ‘‡ğ‘ğ‘šğ‘Î¦ğ‘ (B.49)
Î¦ğ‘=vut
max 
0,1âˆ’ğ‘ğœ“1(Î¨ğ‘âˆ’ğ‘Î¨2)2
ğ‘Î¦1!
+ğ‘Î¦2 (B.50)
whereğ‘ğœ”Î¨1,ğ‘ğœ”Î¨2,ğ‘ğœ”Î¨3,ğ‘ğœ”Î¦1,ğ‘ğœ”Î¦2,ğ‘ğœ”Î¦3,ğ‘Î¦2andğ‘Î¨2are constant.
Appendix C Brief discussion on lab test data
The engine model considered in this study uses empirical formulae. These equations are engine
specific and may not be appropriate for the present study. As discussed in section 2.1.1, we
consider surrogate neural networks and uses lab test data to train these model. In this section,
we briefly discuss lab test data.
Practical limitations exist when instrumenting engines for testing. Some physical phenomena
are easily measurable, while others are not. When conducting modelling efforts, one must
consider the necessary measurements for model tuning to ensure the experimental setup is
adequate. The data collection capabilities can also impact loss function weights based on data
trustworthiness, as well as noise values applied in the analysis. There are a few signals that
pose particular challenges in cost-effective and simple measurement in part due to the high
temperature, pressure, dynamics and flow constituents in some areas.
Often as areas closer to the cylinder are considered, measurements become increasingly
difficult. For example, exhaust port flow, ğ‘Šğ‘’ğ‘œis difficult to measure directly, as the gas is very
hot and reactive. In-cylinder measurements are limited by high pressure and temperatures,
requiring specialized equipment. Even measuring charge flows directly can be challenging.
Because of these limitations, care must be taken in the experimental methods and analysis design
to ensure enough data is gathered to be able to observe and identify the system. Sometimes
33steady state characterizations are used to obtain a static characterization. Consider volumetric
efficiency as an example: because measuring flow directly into or out of the cylinder is difficult,
a fresh air flow measurement combined with an EGR flow measurement can be used to estimate
charge flow to enable the calculation of volumetric efficiency. However, any intake, EGR, or
Exhaust leak impacts this measurement, as does the tolerance stack up of both measurements.
Appendix D Calculation of labelled data for training of the
neural network for the empirical formulae
We approximate the empirical formula using surrogate neural networks and are discussed in
section 2.1.1. The lab test data required for calculating each of these quantities are shown in
Table 3. In this section, we discuss the calculation of labelled data from lab-test data. The
functional approximation of the empirical formulae is independent of time; thus, static data
may be considered for the calculation of labelled data. However, in the case of calculation of
labelled forğœ‚ğ‘¡ğ‘š, the differential equation Eq. (B.25) is considered. Thus, we consider dynamic
data for this calculation.
We approximate the volumetric efficiency using a surrogate neural network ( N(ğ‘ƒ)
1(:,Î¸ğ‘ƒ
1).
The inputs to the network are intake manifold pressure ( ğ‘ğ‘–ğ‘š) and engine speed ( ğ‘›ğ‘’) and trained
using labelled data of ğœ‚ğ‘£ğ‘œğ‘™. The labelled ğœ‚ğ‘£ğ‘œğ‘™are calculated from measurement of ğ‘Šğ‘’ğ‘–using Eq.
(B.5),
ğœ‚ğ‘£ğ‘œğ‘™=120ğ‘…ğ‘ğ‘‡ğ‘–ğ‘šğ‘Šğ‘’ğ‘–
ğ‘ğ‘–ğ‘šğ‘›ğ‘’ğ‘‰ğ‘‘(D.1)
The effective area ratio function for EGR valve is approximated using a surrogate neural
network (N(ğ‘ƒ)
2(:,Î¸ğ‘ƒ
2). The input to the network is Ëœ ğ‘¢ğ‘’ğ‘”ğ‘Ÿand trained using labelled data of ğ‘“ğ‘’ğ‘”ğ‘Ÿ.
The labelled ğ‘“ğ‘’ğ‘”ğ‘Ÿare calculated from the measurement of ğ‘Šğ‘’ğ‘”ğ‘Ÿusing Eqs. (B.20) and (B.22),
ğ´ğ‘’ğ‘”ğ‘Ÿ=âˆšğ‘‡ğ‘’ğ‘šğ‘…ğ‘’
ğ‘ğ‘–ğ‘šÎ¨ğ‘’ğ‘”ğ‘Ÿğ‘Šğ‘’ğ‘”ğ‘Ÿ (D.2)
ğ‘“ğ‘’ğ‘”ğ‘Ÿ=ğ´ğ‘’ğ‘”ğ‘Ÿ
ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥(D.3)
It is important to node that the value of Î¨ğ‘’ğ‘”ğ‘Ÿvaries from 0 to 1 (Eq. (B.21)), thus in the
calculation of ğ´ğ‘’ğ‘”ğ‘Ÿ, a situation may occurs where division by 0. This situation occurs when
ğ‘ğ‘’ğ‘š<ğ‘ğ‘–ğ‘š(Eq. (B.23)). In order to avoid this, labelled data are calculated only for Î¨ğ‘’ğ‘”ğ‘Ÿ>10âˆ’15.
The neural network approximating ( N(ğ‘ƒ)
3(:,Î¸ğ‘ƒ
3) forğ¹ğ‘£ğ‘”ğ‘¡,Î =ğ‘“ğ‘£ğ‘”ğ‘¡Ã—ğ‘“Î ğ‘¡is trained using la-
belled data which are calculated from the measurement of turbine mass flow ( ğ‘Šğ‘¡) using Eq.
(B.27),
ğ¹ğ‘£ğ‘”ğ‘¡,Î ğ‘¡(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡,Î ğ‘¡)=ğ‘“ğ‘£ğ‘”ğ‘¡(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡)Ã—ğ‘“Î ğ‘¡(Î ğ‘¡)=ğ‘Šğ‘¡âˆšğ‘‡ğ‘’ğ‘šğ‘…ğ‘’
ğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥ğ‘ğ‘’ğ‘š(D.4)
The training for the neural network ( N(ğ‘ƒ)
4(:,Î¸ğ‘ƒ
4) for the surrogate model of turbine mechan-
ical efficiency ( ğœ‚ğ‘¡ğ‘š) is done using labelled ğœ‚ğ‘¡ğ‘šwhich is calculated from the measurement of ğœ”ğ‘¡
using Eqs. (B.25) and (B.32)
ğ‘ƒğ‘¡ğœ‚ğ‘š=ğ‘ƒğ‘+ğ½ğ‘¡ğœ”ğ‘¡ğ‘‘ğœ”ğ‘¡
ğ‘‘ğ‘¡(D.5)
The compressor power ( ğ‘ƒğ‘) is calculated as,
ğ‘ƒğ‘=ğ‘Šğ‘ğ‘ğ‘ğ‘(ğ‘‡ğ‘âˆ’ğ‘‡ğ‘ğ‘šğ‘) (D.6)
34In the present study, we consider a five-point method to approximate the derivative present in
Eq. (D.5).
ğ‘“(1)(ğ‘¥)â‰ˆâˆ’ğ‘“(ğ‘¥+2â„)+8ğ‘“(ğ‘¥+â„)âˆ’8ğ‘“(ğ‘¥âˆ’â„)+ğ‘“(ğ‘¥âˆ’2â„)
12â„(D.7)
Onceğ‘ƒğ‘¡ğœ‚ğ‘šcalculated from Eq. (D.5), the labelled ğœ‚ğ‘¡ğ‘šare calculated using Eq. (B.32)
ğœ‚ğ‘¡ğ‘š=ğ‘ƒğ‘¡ğœ‚ğ‘š
ğ‘Šğ‘¡ğ‘ğ‘ğ‘’ğ‘‡ğ‘’ğ‘š
1âˆ’Î 1âˆ’1/ğ›¾ğ‘’
ğ‘¡ (D.8)
The values of ğœ‚ğ‘¡ğ‘šare restricted to maximum value ğœ‚ğ‘¡ğ‘š,ğ‘šğ‘ğ‘¥
ğœ‚ğ‘¡ğ‘š=min(ğœ‚ğ‘¡ğ‘š,ğ‘šğ‘ğ‘¥,ğœ‚ğ‘¡ğ‘š), ğœ‚ğ‘¡ğ‘š,ğ‘šğ‘ğ‘¥=0.8180 (D.9)
The labelled data for the training of neural network ( N(ğ‘ƒ)
5(:,Î¸ğ‘ƒ
5) for compressor efficiency
(ğœ‚ğ‘) is calculated using Eqs. (B.39) and (B.36)
ğœ‚ğ‘=ğ‘ƒğ‘,ğ‘ 
ğ‘ƒğ‘(D.10a)
=ğ‘Šğ‘ğ‘ğ‘ğ‘ğ‘‡ğ‘ğ‘šğ‘
Î 1âˆ’1/ğ›¾ğ‘ğ‘âˆ’1
ğ‘Šğ‘ğ‘ğ‘ğ‘(ğ‘‡ğ‘âˆ’ğ‘‡ğ‘ğ‘šğ‘)(D.10b)
=ğ‘‡ğ‘ğ‘šğ‘
Î 1âˆ’1/ğ›¾ğ‘ğ‘âˆ’1
ğ‘‡ğ‘âˆ’ğ‘‡ğ‘ğ‘šğ‘(D.10c)
To avoid any division by 0, the value of ğ‘‡ğ‘âˆ’ğ‘‡ğ‘ğ‘šğ‘less than 10âˆ’6are considered as 10âˆ’6. Further,
the value ofğœ‚ğ‘is clipped between 0.2 and ğœ‚ğ‘ğ‘šğ‘ğ‘¥
ğœ‚ğ‘=max(0.2,ğœ‚ğ‘) (D.11a)
ğœ‚ğ‘=min(ğœ‚ğ‘ğ‘šğ‘ğ‘¥,ğœ‚ğ‘), ğœ‚ğ‘ğ‘šğ‘ğ‘¥=0.7364 (D.11b)
The training for the neural network ( N(ğ‘ƒ)
6(:,Î¸ğ‘ƒ
6) for surrogate model of volumetric flow
coefficient (Î¦ğ‘) is done using labelled data which are calculated from the measurement of
compressor mass flow ( ğ‘Šğ‘) Eq. (B.45)
Î¦ğ‘=ğ‘…ğ‘ğ‘‡ğ‘ğ‘šğ‘
ğ‘ğ‘ğ‘šğ‘ğœ‹ğ‘…3ğ‘ğœ”ğ‘¡ğ‘Šğ‘ (D.12)
Appendix E Detail loss function for the PINNs model for the
engine
We consider the following loss function for Case 1 to Case 4, which have self-adaptive weights
in the loss function,
L(Î¸,ğœ¦,Î»ğ‘ğ‘–ğ‘š,Î»ğ‘ğ‘’ğ‘š,Î»ğœ”ğ‘¡,Î»ğ‘Šğ‘’ğ‘”ğ‘Ÿ,ğœ†ğ‘‡1)=Lğ‘ğ‘–ğ‘š+Lğ‘ğ‘’ğ‘š+Lğœ”ğ‘¡+Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘¢ğ‘£ğ‘”ğ‘¡+10Ã—Lğ‘¥ğ‘Ÿ+ğœ†ğ‘‡1Ã—Lğ‘‡1+
Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘–ğ‘š+Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘’ğ‘š+Lğ‘–ğ‘›ğ‘–
ğœ”ğ‘¡+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘£ğ‘”ğ‘¡+Lğ‘–ğ‘›ğ‘–
ğ‘¥ğ‘Ÿ+100Ã—Lğ‘–ğ‘›ğ‘–
ğ‘‡1+
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘–ğ‘š(Î»ğ‘ğ‘–ğ‘š)+Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘’ğ‘š(Î»ğ‘ğ‘’ğ‘š)+
Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ”ğ‘¡(Î»ğœ”ğ‘¡)+Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘Šğ‘’ğ‘”ğ‘Ÿ(Î»ğ‘Šğ‘’ğ‘”ğ‘Ÿ),(E.1)
35In the Case 5 where we have not considered self-adaptive weights, the loss function is given as
L(Î¸,ğœ¦)=Lğ‘ğ‘–ğ‘š+Lğ‘ğ‘’ğ‘š+Lğœ”ğ‘¡+Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘¢ğ‘£ğ‘”ğ‘¡+10Ã—Lğ‘¥ğ‘Ÿ+103Ã—Lğ‘‡1+
Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘–ğ‘š+Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘’ğ‘š+Lğ‘–ğ‘›ğ‘–
ğœ”ğ‘¡+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1+
Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2+Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘£ğ‘”ğ‘¡+Lğ‘–ğ‘›ğ‘–
ğ‘¥ğ‘Ÿ+100Ã—Lğ‘–ğ‘›ğ‘–
ğ‘‡1+
103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘–ğ‘š+103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘’ğ‘š+
103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ”ğ‘¡+103Ã—Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘Šğ‘’ğ‘”ğ‘Ÿ,(E.2)
where Î¸=(Î¸1,...,Î¸6)are the hyperparameters of all NNs in PINNs, which include both weights
and biases, ğœ¦are the unknown parameters of the equations which need to be found out. Î»ğ‘ğ‘–ğ‘š,
Î»ğ‘ğ‘’ğ‘š,Î»ğœ”ğ‘¡andÎ»ğ‘Šğ‘’ğ‘”ğ‘Ÿare the self-adaptive weight [19] for the data loss in ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿ
respectively. ğœ†ğ‘‡1is the self-adaptive weight for physics loss in ğ‘‡1.Lğ‘ğ‘–ğ‘š(Î¸),Lğ‘ğ‘’ğ‘š(Î¸),Lğœ”ğ‘¡,
LËœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,LËœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2, andLËœğ‘¢ğ‘£ğ‘”ğ‘¡are the physics loss corresponding to the differential equations of the
states of the diesel engine ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2and Ëœğ‘¢ğ‘£ğ‘”ğ‘¡respectively.Lğ‘¥ğ‘ŸandLğ‘‡1are the
physics loss correspond to ğ‘¥ğ‘Ÿandğ‘‡1respectively.
Lğ‘ğ‘–ğ‘š=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(ğ‘ğ‘–ğ‘š)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘‘ğ‘ğ‘–ğ‘š
ğ‘‘ğ‘¡âˆ’ğ‘…ğ‘ğ‘‡ğ‘–ğ‘š
ğ‘‰ğ‘–ğ‘š ğ‘Šğ‘+ğ‘Šğ‘’ğ‘”ğ‘Ÿâˆ’ğ‘Šğ‘’ğ‘–2
(E.3a)
Lğ‘ğ‘’ğ‘š=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(ğ‘ğ‘’ğ‘š)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘‘ğ‘ğ‘’ğ‘š
ğ‘‘ğ‘¡âˆ’ğ‘…ğ‘’ğ‘‡ğ‘’ğ‘š
ğ‘‰ğ‘’ğ‘š ğ‘Šğ‘’ğ‘œâˆ’ğ‘Šğ‘¡âˆ’ğ‘Šğ‘’ğ‘”ğ‘Ÿ2
(E.3b)
Lğœ”ğ‘¡=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(ğœ”ğ‘¡)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘‘ğœ”ğ‘¡
ğ‘‘ğ‘¡âˆ’ğ‘ƒğ‘¡ğœ‚ğ‘šâˆ’ğ‘ƒğ‘
ğ½ğ‘¡ğœ”ğ‘¡2
(E.3c)
LËœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘‘Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1
ğ‘‘ğ‘¡âˆ’1
ğœğ‘’ğ‘”ğ‘Ÿ1
ğ‘¢ğ‘’ğ‘”ğ‘Ÿ(ğ‘¡âˆ’ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿ)âˆ’Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ12
(E.3d)
LËœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘‘Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2
ğ‘‘ğ‘¡âˆ’1
ğœğ‘’ğ‘”ğ‘Ÿ2
ğ‘¢ğ‘’ğ‘”ğ‘Ÿ(ğ‘¡âˆ’ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿ)âˆ’Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ22
(E.3e)
LËœğ‘¢ğ‘£ğ‘”ğ‘¡=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(Ëœğ‘¢ğ‘£ğ‘”ğ‘¡)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘‘Ëœğ‘¢ğ‘£ğ‘”ğ‘¡
ğ‘‘ğ‘¡âˆ’1
ğœğ‘£ğ‘”ğ‘¡
ğ‘¢ğ‘£ğ‘”ğ‘¡(ğ‘¡âˆ’ğœğ‘‘ğ‘£ğ‘”ğ‘¡)âˆ’Ëœğ‘¢ğ‘£ğ‘”ğ‘¡2
(E.3f)
Lğ‘¥ğ‘Ÿ=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(ğ‘¥ğ‘Ÿ)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1 
ğ‘¥ğ‘Ÿâˆ’Î 1/ğ›¾ğ‘ğ‘’ğ‘¥âˆ’1/ğ›¾ğ‘ğ‘
ğ‘Ÿğ‘ğ‘¥ğ‘£!2
(E.3g)
Lğ‘‡1=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘Ÿ(ğ‘‡1)2=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1(ğ‘‡1âˆ’(ğ‘¥ğ‘Ÿğ‘‡ğ‘’+(1âˆ’ğ‘¥ğ‘Ÿ)ğ‘‡ğ‘–ğ‘š))2(E.3h)
whereğ‘›andğ‘Ÿ(.)are the number of residual points and residual, respectively.
In the Case 1 to Case 4, Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘–ğ‘š(Î»ğ‘ğ‘–ğ‘š,Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘’ğ‘š(Î»ğ‘ğ‘’ğ‘š),Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ”ğ‘¡(Î»ğœ”ğ‘¡), andLğ‘‘ğ‘ğ‘¡ğ‘
ğ‘Šğ‘’ğ‘”ğ‘Ÿ(Î»ğ‘Šğ‘’ğ‘”ğ‘Ÿ)are the
36data loss inğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿrespectively and defined as,
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘–ğ‘š(Î»ğ‘ğ‘–ğ‘š)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğ‘(ğ‘—)
ğ‘–ğ‘šğ‘‘ğ‘ğ‘¡ğ‘âˆ’Ë†ğ‘(ğ‘—)
ğ‘–ğ‘šN1
ğœ†(ğ‘—)
ğ‘ğ‘–ğ‘ši2
(E.4a)
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘’ğ‘š(Î»ğ‘ğ‘’ğ‘š)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğ‘(ğ‘—)
ğ‘’ğ‘šğ‘‘ğ‘ğ‘¡ğ‘âˆ’Ë†ğ‘(ğ‘—)
ğ‘’ğ‘šN1
ğœ†(ğ‘—)
ğ‘ğ‘’ğ‘ši2
(E.4b)
Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ”ğ‘¡(Î»ğœ”ğ‘¡)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğœ”(ğ‘—)
ğ‘¡ğ‘‘ğ‘ğ‘¡ğ‘âˆ’Ë†ğœ”(ğ‘—)
ğ‘¡N5
ğœ†(ğ‘—)
ğœ”ğ‘¡i2
(E.4c)
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘Šğ‘’ğ‘”ğ‘Ÿ(Î»ğ‘Šğ‘’ğ‘”ğ‘Ÿ)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğ‘Š(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿğ‘‘ğ‘ğ‘¡ğ‘âˆ’bğ‘Š(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿğ‘ ğ‘
ğœ†(ğ‘—)
ğ‘Šğ‘’ğ‘”ğ‘Ÿi2
(E.4d)
The same in the Case 5 is given as,
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘–ğ‘š=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğ‘(ğ‘—)
ğ‘–ğ‘šğ‘‘ğ‘ğ‘¡ğ‘âˆ’Ë†ğ‘(ğ‘—)
ğ‘–ğ‘šN1i2
(E.5a)
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘’ğ‘š=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğ‘(ğ‘—)
ğ‘’ğ‘šğ‘‘ğ‘ğ‘¡ğ‘âˆ’Ë†ğ‘(ğ‘—)
ğ‘’ğ‘šN1i2
(E.5b)
Lğ‘‘ğ‘ğ‘¡ğ‘
ğœ”ğ‘¡=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğœ”(ğ‘—)
ğ‘¡ğ‘‘ğ‘ğ‘¡ğ‘âˆ’Ë†ğœ”(ğ‘—)
ğ‘¡N5i2
(E.5c)
Lğ‘‘ğ‘ğ‘¡ğ‘
ğ‘Šğ‘’ğ‘”ğ‘Ÿ=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘—=1h
ğ‘Š(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿğ‘‘ğ‘ğ‘¡ğ‘âˆ’bğ‘Š(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿğ‘ ğ‘i2
(E.5d)
whereğ‘ğ‘–ğ‘šğ‘‘ğ‘ğ‘¡ğ‘,ğ‘ğ‘’ğ‘šğ‘‘ğ‘ğ‘¡ğ‘,ğœ”ğ‘¡ğ‘‘ğ‘ğ‘¡ğ‘andğ‘Šğ‘’ğ‘”ğ‘Ÿğ‘‘ğ‘ğ‘¡ğ‘are the measured data of ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡andğ‘Šğ‘’ğ‘”ğ‘Ÿ
respectively. Ë† ğ‘ğ‘–ğ‘šN1, Ë†ğ‘ğ‘’ğ‘šN1and Ë†ğœ”ğ‘¡N5are the predicted values in ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘šandğœ”ğ‘¡respectively
fromN1(ğ‘¡;Î¸1),N1(ğ‘¡;Î¸1)andN5(ğ‘¡;Î¸5)respectively. Similarly, bğ‘Šğ‘’ğ‘”ğ‘Ÿğ‘ ğ‘is predicted value in
ğ‘Šğ‘’ğ‘”ğ‘Ÿfrom NNs output. ğ‘›is the number of measured data points.
Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘–ğ‘š,Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘’ğ‘š,Lğ‘–ğ‘›ğ‘–
ğœ”ğ‘¡,Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1,Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2,Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘£ğ‘”ğ‘¡,Lğ‘–ğ‘›ğ‘–
ğ‘¥ğ‘ŸandLğ‘–ğ‘›ğ‘–
ğ‘‡1are the losses in initial conditions in
37ğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2, Ëœğ‘¢ğ‘£ğ‘”ğ‘¡,ğ‘¥ğ‘Ÿandğ‘‡1respectively.
Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘–ğ‘š=1
11âˆ‘ï¸
ğ‘—=1
ğ‘(ğ‘—)
ğ‘–ğ‘š0âˆ’Ë†ğ‘(ğ‘—)
ğ‘–ğ‘š02
(E.6a)
Lğ‘–ğ‘›ğ‘–
ğ‘ğ‘’ğ‘š=1
11âˆ‘ï¸
ğ‘—=1
ğ‘(ğ‘—)
ğ‘’ğ‘š0âˆ’Ë†ğ‘(ğ‘—)
ğ‘’ğ‘š02
(E.6b)
Lğ‘–ğ‘›ğ‘–
ğœ”ğ‘¡=1
11âˆ‘ï¸
ğ‘—=1
ğœ”(ğ‘—)
ğ‘¡0âˆ’Ë†ğœ”(ğ‘—)
ğ‘¡02
(E.6c)
Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1=1
11âˆ‘ï¸
ğ‘—=1
Ëœğ‘¢(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿ10âˆ’Ë†Ëœğ‘¢(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿ102
(E.6d)
Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2=1
11âˆ‘ï¸
ğ‘—=1
Ëœğ‘¢(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿ20âˆ’Ë†Ëœğ‘¢(ğ‘—)
ğ‘’ğ‘”ğ‘Ÿ202
(E.6e)
Lğ‘–ğ‘›ğ‘–
Ëœğ‘¢ğ‘£ğ‘”ğ‘¡=1
11âˆ‘ï¸
ğ‘—=1
Ëœğ‘¢(ğ‘—)
ğ‘£ğ‘”ğ‘¡0âˆ’Ë†Ëœğ‘¢(ğ‘—)
ğ‘£ğ‘”ğ‘¡02
(E.6f)
Lğ‘–ğ‘›ğ‘–
ğ‘¥ğ‘Ÿ=1
11âˆ‘ï¸
ğ‘—=1
ğ‘¥(ğ‘—)
ğ‘Ÿ0âˆ’Ë†ğ‘¥(ğ‘—)
ğ‘Ÿ02
(E.6g)
Lğ‘–ğ‘›ğ‘–
ğ‘‡1=1
11âˆ‘ï¸
ğ‘—=1
ğ‘‡(ğ‘—)
10âˆ’Ë†ğ‘‡(ğ‘—)
102
(E.6h)
whereğ‘ğ‘–ğ‘š0,ğ‘ğ‘’ğ‘š0,ğœ”ğ‘¡0, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ10, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ20, Ëœğ‘¢ğ‘£ğ‘”ğ‘¡0,ğ‘¥ğ‘Ÿ0andğ‘‡10are the initial conditions and Ë† ğ‘ğ‘–ğ‘š0, Ë†ğ‘ğ‘’ğ‘š0,
Ë†ğœ”ğ‘¡0,Ë†Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ10,Ë†Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ20,Ë†Ëœğ‘¢ğ‘£ğ‘”ğ‘¡0, Ë†ğ‘¥ğ‘Ÿ0and Ë†ğ‘‡10are corresponding output from neural network at time ğ‘¡=0
forğ‘ğ‘–ğ‘š,ğ‘ğ‘’ğ‘š,ğœ”ğ‘¡, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ1, Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ2, Ëœğ‘¢ğ‘£ğ‘”ğ‘¡,ğ‘¥ğ‘Ÿandğ‘‡1respectively.
38Appendix F Additional Tables
Description Symbol Value
1 Ideal gas constant of air ğ‘…ğ‘ 287
2 Intake manifold temperature ğ‘‡ğ‘–ğ‘š 300.6186
3 Intake manifold volume ğ‘‰ğ‘–ğ‘š 0.0220
4 Ideal gas constant of exhaust gas ğ‘…ğ‘’ 286
5 Exhaust manifold volume ğ‘‰ğ‘’ğ‘š 0.0200
6 Displaced volume of the cylinder ğ‘‰ğ‘‘ 0.0127
7 Number of cylinder ğ‘›ğ‘ğ‘¦ğ‘™ 6
8 Specific heat capacity ratio of air ğ›¾ğ‘ 1.3964
9 Specific heat capacity at constant pressure of air ğ‘ğ‘ğ‘ 1011
10 Specific heat capacity at constant volume air ğ‘ğ‘£ğ‘ 724
11 Compression ratio ğ‘Ÿğ‘ 17
12 Fuel consumed during constant-volume combustion ğ‘¥ğ‘ğ‘£ 2.3371Ã—10âˆ’14
13 Heating value of fuel ğ‘ğ»ğ‘‰ 42900000
14 Diameter of exhaust pipe ğ‘‘ğ‘ğ‘–ğ‘ğ‘’ 0.1
15 Length of exhaust pipe ğ‘™ğ‘ğ‘–ğ‘ğ‘’ 1
16 Number of exhaust pipe ğ‘›ğ‘ğ‘–ğ‘ğ‘’ 2
17Specific heat capacity at constant pressure of exhaust
gasğ‘ğ‘ğ‘’ 1332
18 Time constant 1 for EGR ğœğ‘’ğ‘”ğ‘Ÿ1 0.05
19 Time constant 2 for EGR ğœğ‘’ğ‘”ğ‘Ÿ2 0.13
20 Time delay constant for EGR ğœğ‘‘ğ‘’ğ‘”ğ‘Ÿ 0.065
21 Constant for EGR overshoot ğ¾ğ‘’ğ‘”ğ‘Ÿ 1.8
22 Optimal value of pressure ratio of EGR Î ğ‘’ğ‘”ğ‘Ÿğ‘œğ‘ğ‘¡ 0.6500
23 Inertial of turbocharger ğ½ğ‘¡ 2.0Ã—10âˆ’4
24 Time constant for VGT ğœğ‘£ğ‘”ğ‘¡ 0.025
25 Time delay constant for VGT ğœğ‘‘ğ‘£ğ‘”ğ‘¡ 0.04
26 Specific heat capacity at constant pressure of exhaust ğ‘ğ‘ğ‘’ 1332
27 Specific heat capacity ratio of exhaust gas ğ›¾ğ‘’ 1.2734
28 turbine blade radius ğ‘…ğ‘¡ 0.04
29 compressor blade radius ğ‘…ğ‘ 0.0400
Table F1: Values of the constants considered in the present study.
Unknown ğœ‚ğ‘ ğ‘â„ğ‘¡ğ‘œğ‘¡ğ´ğ‘’ğ‘”ğ‘Ÿğ‘šğ‘ğ‘¥ ğ´ğ‘£ğ‘”ğ‘¡ğ‘šğ‘ğ‘¥
Value 1.1015 96.2755 4.0Ã—10âˆ’48.4558Ã—10âˆ’4
Table F2: True value of the unknown parameters.
39Symbol Value Symbol Value Symbol Value
ğ‘ğ‘£ğ‘œğ‘™1âˆ’2.0817Ã—10âˆ’4ğ‘ğ‘’ğ‘”ğ‘Ÿ1âˆ’1.1104Ã—10âˆ’4
ğ‘ğ‘£ğ‘œğ‘™2 -0.0034 ğ‘ğ‘’ğ‘”ğ‘Ÿ2 0.0178
ğ‘ğ‘£ğ‘œğ‘™3 1.1497 ğ‘ğ‘’ğ‘”ğ‘Ÿ3 0
ğ‘ğœ”Î¨1 1.0882Ã—10âˆ’8ğ‘ğœ”Î¦1âˆ’1.4298Ã—10âˆ’8ğ‘Î¨2 0
ğ‘ğœ”Î¨2âˆ’1.7320Ã—10âˆ’4ğ‘ğœ”Î¦2âˆ’0.0015 ğ‘Î¦2 0
ğ‘ğœ”Î¨3 1.0286 ğ‘ğœ”Î¦3 29.6462
ğœ‹ğ‘ğ‘œğ‘ğ‘¡ 1.0455 ğ‘ğ‘š1 1.3563 ğ‘ğ‘£ğ‘”ğ‘¡1 126.8719
ğ‘Šğ‘ğ‘œğ‘ğ‘¡ 0.2753 ğ‘ğ‘š2 2.7692ğ‘’+03ğ‘ğ‘£ğ‘”ğ‘¡2 117.1447
ğ‘1 3.0919 ğ‘ğ‘š3 0.0100 ğ‘ğ‘“1 1.9480
ğ‘2 2.1479 ğµğ‘†ğ‘…ğ‘œğ‘ğ‘¡ 0.9755 ğ‘ğ‘“2âˆ’0.7763
ğ‘3âˆ’2.4823 ğœ‚ğ‘¡ğ‘š,ğ‘šğ‘ğ‘¥ 0.8180 ğ¾ğ‘¡ 2.8902
ğœ‚ğ‘ğ‘šğ‘ğ‘¥ 0.7364
ğ‘ğœ‹ 0.2708
Table F3: Value for the coefficients of the empirical formulae.
Appendix G Additional figures
In this section, we present the results for Case 1 and Case 2.
For Case-1: 3 unknown parameters with clean data
The predicted state variables and ğ‘‡1andğ‘¥ğ‘Ÿfor Case 1 (3 unknown with clear data) are shown
in Fig. G1. The predicted dynamics of the known variables are shown in Fig. G2(a)-(d). In
Fig. G2(e)-(h), we have shown the dynamics of variables which are dependent on the unknown
parameters. The predicted empirical formulae are shown in Fig. G3. We also studied the
convergence of the unknown parameters, which are shown in Fig. G4.
400 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Ëœuegr1
(c)uegr1True
Pred
0 20 40 60
t(sec)0.00.51.01.5Ëœuegr2
(d)uegr2True
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Ëœuvgt
(e) ËœuvgtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Ï‰t
(f)Ï‰tTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00T1
(g)T1True
Pred
0 20 40 60
t(sec)0.00.51.0xr
(h)xrTrue
PredFig. G1: Predicted states and ğ‘‡1andğ‘¥ğ‘Ÿfor Case 1: Predicted dynamics of the state variables
of the engine and ğ‘‡1andğ‘¥ğ‘Ÿfor Case 1 (PINN with self-adaptive weights for 3 unknown
parameters). It can be observed that the predicted dynamics of the states are in good
agreement with the true values. However, similar to 4 unknown parameters ğ‘‡1andğ‘¥ğ‘Ÿ
do not match with the true value.
410 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Ï‰t
(c)Ï‰tTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Wegr
(d)WegrTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(e)TemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wei
(f)WeiTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(g)AegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(h)WtTrue
PredFig. G2: Predicted dynamics of variables for Case 1: (a)-(d) Predicted dynamics of the
variables whose field measurement data are known. (e)-(h) dynamics of important
variables which also depend on the unknown parameters
0 20 40 60
t(sec)0.000.250.500.751.00Î·vol
(a)Î·volTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fvgtÃ—fÎ t
(c)fvgtÃ—fÎ tTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Î·tm
(d)Î·tmTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Î·c
(e)Î·cTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Î¦c
(f) Î¦ cTrue
Pred
Fig. G3: Empirical formulae for Case 1: The predicted values of empirical formulae for Case
1 (3 unknown parameters with clean data).
420.0 0.5 1.0 1.5 2.0
epoch Ã—105234Aegrmax Ã—10âˆ’4
(a)AegrmaxTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch Ã—1050.91.01.1Î·sc
(b)Î·scTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch Ã—105100125150175htot
(c)htotTrue
PredFig. G4: Convergence of the unknown parameters for Case 1: Convergence of the unknown
parameters with epoch for Case 1 (3 unknown parameters with clean data)
For Case-2: 3 unknown parameters with noisy data
The predicted state variables and ğ‘‡1andğ‘¥ğ‘Ÿfor Case 2 (3 unknown with noisy data) are shown
in Fig. G5. The predicted dynamics of the known variables are shown in Fig. G6(a)-(d). In
Fig. G6(e)-(h), we have shown the dynamics of variables which are dependent on the unknown
parameters. The predicted empirical formulae are shown in Fig. G7. We also studied the
convergence of the unknown parameters, which are shown in Fig. G8.
0 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Ëœuegr1
(c)uegr1True
Pred
0 20 40 60
t(sec)0.00.51.0Ëœuegr2
(d)uegr2True
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Ëœuvgt
(e) ËœuvgtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Ï‰t
(f)Ï‰tTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00T1
(g)T1True
Pred
0 20 40 60
t(sec)0.00.51.0xr
(h)xrTrue
Pred
Fig. G5: Predicted states and ğ‘‡1andğ‘¥ğ‘Ÿfor Case 2: Predicted dynamics of the state variables
of the engine and ğ‘‡1andğ‘¥ğ‘Ÿfor Case 2 (PINN with self-adaptive weights for 3 unknown
paramters). It can be observed that the predicted dynamics of the states are in good
agreement with the true values. However, similar to 4 unknown parameters ğ‘‡1andğ‘¥ğ‘Ÿ
do not match with the true value.
430 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Ï‰t
(c)Ï‰tTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Wegr
(d)WegrTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(e)TemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wei
(f)WeiTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(g)AegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(h)WtTrue
PredFig. G6: Predicted dynamics of variables for Case 2: (a)-(d) Predicted dynamics of the
variables whose field measurement data are known. (e)-(h) dynamics of important
variables which also depend on the unknown parameters
0 20 40 60
t(sec)0.000.250.500.751.00Î·vol
(a)Î·volTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fvgtÃ—fÎ t
(c)fvgtÃ—fÎ tTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Î·tm
(d)Î·tmTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Î·c
(e)Î·cTrue
Pred
0 20 40 60
t(sec)âˆ’101Î¦c
(f) Î¦ cTrue
Pred
Fig. G7: Empirical formulae for Case 2: The predicted values of empirical formulae for Case
2 (3 unknown parameters with noisy data).
440.0 0.5 1.0 1.5 2.0
epoch Ã—105246Aegrmax Ã—10âˆ’4
(a)AegrmaxTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch Ã—1050.80.91.01.11.2Î·sc
(b)Î·scTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch Ã—105100125150175200htot
(c)htotTrue
PredFig. G8: Convergence of the unknown parameters for Case 2: Convergence of the unknown
parameters with epoch for Case 2 (3 unknown parameters with noisy data)
Appendix H Neural network surrogates for empirical formu-
lae
The empirical formulae of the engine model are approximated using surrogate neural networks
and are discussed in section 2.1.1. In section 2.3, we discuss the laboratory data required to
train these neural networks. The laboratory data required for training of each neural network are
shown in Table 3 ( Â§2.3). The labelled data for training these neural networks may be calculated
from static data on the entire operational range of each quantity except for turbine mechanical
efficiency (ğœ‚ğ‘¡ğ‘š). The labelled data for the turbine mechanical efficiency is calculated using Eq.
(B.25) ( Â§B.4), which is a differential equation, thus requiring dynamic data with fine ğ‘‘ğ‘¡. The
calculations of the labelled data from the laboratory measurements are discussed in Appendix
D. In the case of training of neural network N(ğ‘ƒ)
3(x;Î¸ğ‘ƒ
3)for the approximation of ğ¹ğ‘£ğ‘”ğ‘¡,Î ğ‘¡,ğ¿2
weight regularizer is considered in the loss function with a coefficient 5 Ã—10âˆ’10.
The predicted values of the empirical formulae for Case-V (Table 4 in Â§2.3) with the true
values for 1-minute duration are shown in Fig. H1. The % relative ğ¿2errors for training and
testing data set are shown in the last two columns of Table H1. We observe that the neural
networks are able to predict the empirical quantity with very good accuracy. The testing error
in the case of the surrogate neural network for ğ‘“ğ‘’ğ‘”ğ‘Ÿis smaller than the training error. This is
because of the nature of the function and the data considered. The input-output relationship is
simple, with only one input and one output. The maximum value of the testing data is smaller
than the maximum value of the training data. Similarly, the minimum value of testing data is
larger than the minimum value of training data. We also observed that the standard deviation of
testing data is smaller than the standard deviation of the training data. Since the EGR system is
independent and ğ‘“ğ‘’ğ‘”ğ‘Ÿdepends only on Ëœ ğ‘¢ğ‘’ğ‘”ğ‘Ÿ, not any other variables (e.g. ambient temperature
and pressure), we assume that most of the testing set of data might be within the training data
set (training data set is 2 hrs while testing data set is 20 minutes). Thus, the testing error is
marginally smaller than the training error. The testing error in the case of the surrogate model
forğœ‚ğ‘¡ğ‘šis smaller than that of the training error. The approximation considered in calculating
the labelled data for ğœ‚ğ‘¡ğ‘šfrom the laboratory data, we have considered a five-point method
to approximate the differentiation present in Eq. (B.25) ( Â§B.4). Thus, a few noisy data are
observed in both training and testing data sets. As the duration of the training data set is larger
than the testing dataset, the amount of noisy data is more in the training data. Thus, the error
in training is slightly higher than the testing error. These neural networks, after training, will
be used in the places of the empirical formulae in the inverse problem. The trained weights and
45biases will be considered fixed in the inverse problem.
Neural InputNetwork size OutputOutput ğ¿2error (%)
network ( x) restrictâ€¡Train Test
N(ğ‘ƒ)
1(x;Î¸ğ‘ƒ
1)ğ‘›ğ‘’,ğ‘ğ‘–ğ‘š[2,4,4,1]ğœ‚ğ‘£ğ‘œğ‘™ 0.01 0.03
N(ğ‘ƒ)
2(x;Î¸ğ‘ƒ
2) Ëœğ‘¢ğ‘’ğ‘”ğ‘Ÿ[1,4,4,1]ğ‘“ğ‘’ğ‘”ğ‘Ÿğ‘†(ğ‘“ğ‘’ğ‘”ğ‘Ÿ) 0.14 0.10
N(ğ‘ƒ)
3(x;Î¸ğ‘ƒ
3)Î ğ‘¡,Ëœğ‘¢ğ‘£ğ‘”ğ‘¡[2,8,8,8,1]ğ¹ğ‘£ğ‘”ğ‘¡,Î ğ‘¡1.1Ã—ğ‘†(ğ¹ğ‘£ğ‘”ğ‘¡,Î ğ‘¡)0.03 0.52
N(ğ‘ƒ)
4(x;Î¸ğ‘ƒ
4)ğœ”ğ‘¡,Î ğ‘¡,ğ‘‡ğ‘’ğ‘š[3,4,4,4,1]ğœ‚ğ‘¡ğ‘š min(0.818,ğœ‚ğ‘¡ğ‘š)1.62 1.32
N(ğ‘ƒ)
5(x;Î¸ğ‘ƒ
5)ğ‘Šğ‘,Î ğ‘[2,4,4,4,1]ğœ‚ğ‘ max(0.2,ğ‘†(ğœ‚ğ‘)) 0.16 0.18
N(ğ‘ƒ)
6(x;Î¸ğ‘ƒ
6)ğœ”ğ‘¡,Î ğ‘,ğ‘‡ğ‘ğ‘šğ‘[3,10,10,10,1]Î¦ğ‘ğ‘†(Î¦ğ‘) 0.76 1.13
â€¡ğ‘†âˆ’â†’sigmoid function
Table H1: Details of neural networks for empirical formulae: Details of the DNNs to ap-
proximate empirical formulae. The first two columns specify the neural network
(Table 2) and their input, respectively. The third column indicates the neural network
size considered. The activation function of the hidden layers is ğœ(.)=tanh(.). The
â€Outputâ€ column specifies the empirical quantity the neural network approximated.
The last two columns give the results for the test data after the completion of the
training. The column â€Errorâ€ specifies the relative % ğ¿2error for the test case (Case
V). Appropriate scaling of input and output are considered in the training of neural
networks.
46015 30 45 60
t(sec)0.000.250.500.751.00Î·vol
(a)Î·volTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00fvgtÃ—fÎ t
(c)fvgtÃ—fÎ tTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00Î·tm
(d)Î·tmTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00Î·c
(e)Î·cTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00Î¦c
(f) Î¦ cTrue
PredictedFig. H1: Prediction of empirical formulae: The predicted and true values of the empirical
formulae for test case (Case-V). The plots are normalized within 20-minute data, and
only a portion (0 to 1 minute) of the results are shown. The predicted empirical
formulae are in good agreement with the true values.
47