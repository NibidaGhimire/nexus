Physics-informed neural networks for predicting gas
flow dynamics and unknown parameters in diesel
engines
Kamaljyoti Natha, 1, Xuhui Mengb, 1, Daniel J Smithc,
George Em Karniadakisa,d,∗
aDivision of Applied Mathematics, Brown University, United States of America
bInstitute of Interdisciplinary Research for Mathematics and Applied Science, School of
Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China
cCummins Inc., United States of America
dSchool of Engineering, Brown University, United States of America
Abstract
This paper presents a physics-informed neural network (PINN) approach for monitoring
the health of diesel engines. The aim is to evaluate the engine dynamics, identify unknown
parameters in a ”mean value” model, and anticipate maintenance requirements. The PINN
model is applied to diesel engines with a variable-geometry turbocharger and exhaust gas
recirculation, using measurement data of selected state variables. The results demonstrate
the ability of the PINN model to predict simultaneously both unknown parameters and
dynamics accurately with both clean and noisy data, and the importance of the self-adaptive
weight in the loss function for faster convergence. The input data for these simulations are
derived from actual engine running conditions, while the outputs are simulated data, making
this a practical case study of PINN’s ability to predict real-world dynamical systems. The
mean value model of the diesel engine incorporates empirical formulae to represent certain
states, but these formulae may not be generalizable to other engines. To address this, the
study considers the use of deep neural networks (DNNs) in addition to the PINN model.
The DNNs are trained using laboratory test data and are used to model the engine-specific
empirical formulae in the mean value model, allowing for a more flexible and adaptive
representation of the engine’s states. In other words, the mean value model uses both the
PINN model and the DNNs to represent the engine’s states, with the PINN providing a
physics-based understanding of the engine’s overall dynamics and the DNNs offering a
more engine-specific and adaptive representation of the empirical formulae. By combining
these two approaches, the study aims to offer a comprehensive and versatile approach to
monitoring the health and performance of diesel engines.
Keywords: Diesel engine, Gas flow dynamics, Parameters estimation, Physics Informed Neural
Networks, Digital Twins.
∗Corresponding author: G. E. Karniadakis (george karniadakis@brown.edu)
1These authors contributed equally (Kamaljyoti Nath, Xuhui Meng),
E-mail addresses: kamaljyoti nath@brown.edu (Kamaljyoti Nath), xuhui meng@hust.edu.cn (Xuhui Meng),
daniel.j.smith@cummins.com (Daniel J Smith), george karniadakis@brown.edu (George Em Karniadakis)
August 8, 2023arXiv:2304.13799v2  [cs.LG]  5 Aug 20231 Introduction
Powertrains of the future must meet increasingly stringent requirements for emissions, per-
formance, reliability, onboard monitoring, and serviceability. Capable system models for
estimating states and adapting to an individual system’s behaviour are critical elements to meet
control and health monitoring needs. Leveraging purely data-driven models to meet these
requirements provides simplicity in modelling and captures dynamics difficult to formulate ana-
lytically. However, large data needs, poor physical interpretability, challenges with systems with
long memory effects and sparse sensing, as well as inability to extrapolate beyond the training
datasets present onerous burdens to practical implementation. Relying on purely theory-based
models allows for directly interpretable results with higher confidence and fewer data for cal-
ibration but often causes a tradeoff of modelling relevant dynamics versus model complexity,
challenges in systems with high uncertainties, poor modelling where dynamics are not well
understood, and slow solution of higher-order models. Modelling solutions that leverage the
strengths of theory-guided as well as data-driven models have the potential to reduce data needs,
increase robustness, and effectively use theoretical and practical knowledge of the system.
To investigate model architectures, balancing the strengths of both theory-based models and
data-driven models, this work explores the application of physics-informed neural networks
(PINNs) to a diesel internal combustion engine model for the purposes of simultaneous param-
eter and state estimation. The physical portion is based on the mean value model of a diesel
engine with a variable geometry turbocharger (VGT), and exhaust gas recirculation (EGR)
proposed by Wahlstr ¨om and Eriksson [24].
Physics-informed Neural Networks (PINNs) [21] is a new method of training neural net-
works, which takes into account the physics of a problem while evaluating the parameters of
the neural network. The method is suitable for both evaluation of the solution of PDF (forward
problem) and the data-driven identification of parameters of PDF (inverse problem). It takes
advantage of automatic differentiation [1] in formulating a physical loss in the loss function
along with data loss. Jagtap et al. [12] proposed conservative PINNs (cPINNs) for conservation
laws, which employs domain decomposition with a PINN formulation in each domain. Further,
Jagtap and Karniadakis [10] introduced domain decomposition for general PDEs using the so-
called extended PINN (XPINN). hp-VPINN is a variational formulation of PINN with domain
decomposition proposed by Kharazmi et al. [14]. Meng et al. [20] proposed the Parareal PINN
(PPINN) approach for long-time integration of time-dependent partial differential equations.
The authors of [5] proposed “separable” PINN, which can reduce the computational time and
increase accuracy for high dimensional PDEs. In PINN, there are multiple loss functions, and
the total loss function is given by the weighted sum of individual losses. McClenny and Braga-
Neto [19] proposed a self-adaptive weight technique, which is capable of tuning the weights
automatically. PINN and its variants were also considered in various inverse problems like
supersonic flows [13], nano-optics and metamaterials [4], unsaturated groundwater flow [7].
Detailed reviews of PINN can be found in [3, 6, 17].
Modelling of diesel engines using neural networks has been considered in the past. Biao et al.
[2] considered Nonlinear Auto-Regressive Moving Average with eXogenous inputs (NARMAX)
method for system identification of locomotive diesel engines. The model has three inputs to the
network, i.e. the fuel injected, the load of the main generator, and the feedback rotation speed
(from the output); the outputs are rotation speed and diesel power. The authors considered
Levenberg-Marquardt (LM) algorithm to train the network. Finesso and Spessa [8] developed
a three-zone thermodynamic model to predict nitrogen oxide and in-cylinder temperature heat
release rate for direct injection diesel engines under steady state and transient conditions. The
2model is zero-dimensional, and the equations can be solved analytically. Thus, it required
a very short computational time. Tosun et al. [23] predicted torque, carbon monoxide, and
oxides of nitrogen using neural networks (3 independent networks) for diesel engines fueled
with biodiesel-alcohol mixtures. The authors considered three fuel properties (density, cetane
number, lower heating value) and engine speed as input parameters and the networks are
optimized using the Levenberg-Marquardt method. The authors observed that neural network
results are better than the least square method. Gonz ´alez et al. [9] integrated a data-driven
model with a physics-based (equation-based) model for the gas exchange process of a diesel
engine. The authors modelled the steady-state turbocharger using a neural network. Further,
the authors integrated the data-driven model with an equation-based model. Recently, Kumar
et al. [16] considered DeepONet [18] to predict the state variable of the same mean value
engine model [24] we considered in this study. The authors consider dynamic data to train
the model. However, the model can predict only the state variable at the particular (trained)
ambient temperature and pressure, as variations of ambient temperature and pressure are not
considered in the training of DeepONet. The model also does not predict the parameters of the
engine model. While the model was trained using dynamic data, the physics of the problem
was not considered while training the network. The model (DeepONet) is capable of predicting
dynamic responses.
In the present study, we formulate a PINN model for the data-driven identification of
parameters and prediction of dynamics of system variables of a diesel engine. In PINN, the
physics of the system is directly included in the form of physics loss along with data loss. While
data-driven models require large amount over the entire operational range in training, PINN can
be trained with a smaller amount of data as it is trained online. The dynamics characteristic
of the state variables is automatically incorporated. PINN may be used for the solution of
differential equations or for the identification of parameters and prediction of state variables.
In the present study, we are specifically interested in estimating unknown parameters and states
when we know a few state variables from field data. The dynamics of the state variables of the
mean value engine [24] are described by first-order differential equations. We will utilize these
equations in the formulation of the physics-informed loss function. The unknown parameters
are considered trainable and updated in the training process along with the neural network
parameters.
The engine model also considers a few empirical formulae in its formulation. These
equations are engine-specific, and the coefficients of these equations need to be evaluated
from experimental data. These equations are static in nature, and thus may be trained with
smaller data compared to dynamic equations. We know that deep neural networks (DNNs)
are universal approximators of any continuous function, thus, DNNs may be considered more
general approximators of these empirical formulae. One of the advantages of considering DNNs
over empirical formulae is that we do not need to assume the type of non-linearity between
the input out variables. The neural network learns the non-linearity if trained with sufficient
data. We approximate the empirical formulae using DNNs and train them using laboratory test
data. Once these networks are trained using laboratory test data, these are considered in the
PINNs model in places of the empirical formulae. During the training of the PINNs model, the
parameters of these networks are remain constant.
The training data for the inverse problem and laboratory data are generated using the
Simulink file [22] accompanied in [24]. The input to Simulink is taken from actual field data.
By doing this, we are trying to generate data as realistic as field data. Furthermore, we also
consider noise to the field data generated. We observed that the proposed PINNs model can
predict the dynamics of the states and unknown parameters. We summarize below a few of the
3salient features of the present study:
1. We formulated PINNs-based parameter identification for real-world dynamical systems,
in the present case, a diesel engine. This is significant as it started a new paradigm for
future research for onboard systems for the health monitoring of engines.
2. We showed how PINNs could be implemented in predicting important unknown parame-
ters of diesel engines from field data. From these predicted parameters, one can infer the
health and serviceability requirements of the engine.
3. We showed the importance of self-adaptive weights (given the fast transient dynamics) in
the accuracy and faster convergence of results for PINNs for the present study.
4. The engine model generally considers empirical formulae to evaluate a few of its quan-
tities. These empirical formulae are engine-specific and require lab test data for the
evaluation of the coefficients. We have shown how neural networks can be considered
to model the empirical formulae. We have shown how we can train these networks from
lab-test data. This is important as it may provide a better relationship for the empirical
formulae.
5. The field data for the inverse problem are generated considering input recorded from
actual engine running conditions. Further, we consider appropriate noise in the simulated
data, mimicking near real-world field data.
We organize the rest of the article as follows: in section 1.1, we discuss the detailed problem
statement and different cases considered for simulation studies. In section 2, first, we discuss
PINNs for the inverse problems for the diesel engine and the surrogates for the empirical
formula. We discuss a detailed flow chart for the inverse problem for the PINN engine model
in section 2.2. In section 2.3, we discuss the laboratory data required and their generation for
the training of surrogates for the empirical formulae. We also discuss the field data generation
for the inverse problem. We present the results and discussion in section 3. The conclusions of
the present study are discussed in the section 4.
1.1 Problem setup
In this section, we first introduce the mean value model for the gas flow dynamics [24] in the
diesel engine, and then we will formulate the inverse problems that we are interested in.
As shown in Fig. 1, the engine model considered in the present study mainly comprises
six parts: the intake and exhaust manifold, the cylinder, the exhaust gas recirculation (EGR)
valve system, the compressor and the turbine. More details on each engine part can be seen in
Appendix B. We note that the engine considered here is the same as in [24].
4uegr
EGR valve
Intake
manifoldExhaust
manifold
Cylinder
CompressorTurbineuvgtuδ
pim
XOimpem
XOemWegr
Wei Weo
ωt
WcWt
1Fig. 1: Schematic diagram of the diesel engine: A schematic diagram of the mean value
diesel engine with a variable-geometry turbocharger (VGT) and exhaust gas recirculation
(EGR) [24]. The main components of the engine are the intake manifold, the exhaust
manifold, the cylinder, the EGR valve system, the compressor, and the turbine. The
control input vector is u={𝑢𝛿,𝑢𝑒𝑔𝑟,𝑢𝑣𝑔𝑡}, and engine speed is 𝑛𝑒. (Source: Figure is
adopted from [24])
To describe the gas flow dynamics in the engine illustrated in Fig. 1, e.g., the dynamics
in the manifold pressures, turbocharger, EGR and VGT actuators, a mean value model of the
diesel engine with variable geometric turbocharger and exhaust gas recirculation was proposed
in [24]. We will also utilize the same model as the governing equations to describe the gas flow
dynamics considered in the current study. Specifically, the model proposed in [24] has eight
states expressed as follows:
x={𝑝𝑖𝑚, 𝑝𝑒𝑚, 𝑋𝑂𝑖𝑚, 𝑋𝑂𝑒𝑚, 𝜔𝑡,˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2,˜𝑢𝑣𝑔𝑡}, (1)
where𝑝𝑖𝑚and𝑝𝑒𝑚are the intake and exhaust manifold pressure, respectively, 𝑋𝑂𝑖𝑚and𝑋𝑂𝑒𝑚
are the oxygen mass fractions in the intake and exhaust manifold, respectively, 𝜔𝑡is the turbo
speed; ˜𝑢𝑣𝑔𝑡represents the VGT actuator dynamics. A second-order system with an overshoot
and a time delay is used to represent the dynamics of the EGR-valve actuator. The model is
represented by subtraction of two first-order models, ˜ 𝑢𝑒𝑔𝑟1and ˜𝑢𝑒𝑔𝑟2, with different gains and
time constants. Further, the control inputs for the engine are u={𝑢𝛿, 𝑢𝑒𝑔𝑟, 𝑢𝑣𝑔𝑡}and the
engine speed is 𝑛𝑒, in which𝑢𝛿is the mass of injected fuel, 𝑢𝑒𝑔𝑟and𝑢𝑣𝑔𝑡are the EGR and
VGT valve positions, respectively. Furthermore, the position of the valves, i.e., 𝑢𝑒𝑔𝑟and𝑢𝑣𝑔𝑡,
may vary from 0% to 100%, which indicates the complete close and opening of the valves,
respectively. The mean value engine model is then expressed as
¤x=𝑓(x,u,𝑛𝑒). (2)
5In addition, the states describing the oxygen mass fraction of the intake and exhaust manifold,
i.e.,𝑋𝑂𝑖𝑚and𝑋𝑂𝑒𝑚, are not considered in the present study as the rest of the states do not depend
on these two states. Also, the parameters of the oxygen mass fractions are assumed to be constant
and known. The governing equations for the remaining six states are as follows:
𝑑
𝑑𝑡𝑝𝑖𝑚=𝑅𝑎𝑇𝑖𝑚
𝑉𝑖𝑚(𝑊𝑐+𝑊𝑒𝑔𝑟−𝑊𝑒𝑖), (3)
𝑑
𝑑𝑡𝑝𝑒𝑚=𝑅𝑒𝑇𝑒𝑚
𝑉𝑒𝑚(𝑊𝑒𝑜−𝑊𝑡−𝑊𝑒𝑔𝑟), (4)
𝑑
𝑑𝑡𝜔𝑡=𝑃𝑡𝜂𝑚−𝑃𝑐
𝐽𝑡𝜔𝑡, (5)
𝑑˜𝑢𝑒𝑔𝑟1
𝑑𝑡=1
𝜏𝑒𝑔𝑟1
𝑢𝑒𝑔𝑟(𝑡−𝜏𝑑𝑒𝑔𝑟)−˜𝑢𝑒𝑔𝑟1
, (6)
𝑑˜𝑢𝑒𝑔𝑟2
𝑑𝑡=1
𝜏𝑒𝑔𝑟2
𝑢𝑒𝑔𝑟(𝑡−𝜏𝑑𝑒𝑔𝑟)−˜𝑢𝑒𝑔𝑟2
, (7)
𝑑˜𝑢𝑣𝑔𝑡
𝑑𝑡=1
𝜏𝑣𝑔𝑡
𝑢𝑣𝑔𝑡(𝑡−𝜏𝑑𝑣𝑔𝑡)−˜𝑢𝑣𝑔𝑡
. (8)
Two additional equations used for the computation of 𝑇𝑒𝑚in Eq. (4) read as:
𝑇1=𝑥𝑟𝑇𝑒+(1−𝑥𝑟)𝑇𝑖𝑚, (9)
𝑥𝑟=Π1/𝛾𝑎𝑒𝑥−1/𝛾𝑎𝑝
𝑟𝑐𝑥𝑣, (10)
where𝑇1is the temperature when the inlet valve closes after the intake stroke and mixing, and
𝑥𝑟is the residual gas fraction. A brief discussion on the governing equations of the engine
model is presented in Appendix B. Interested readers can also refer to [24] for more details.
In the present study, we have field measurements on a certain number of variables, i.e., 𝑝𝑖𝑚,
𝑝𝑒𝑚,𝜔𝑡, and𝑊𝑒𝑔𝑟as well as the inputs, i.e., uand𝑛𝑒, at discrete times. Further, some of the
parameters in the system, e.g., 𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 , which are difficult to measure
directly, are unknown. 𝐴𝑒𝑔𝑟𝑚𝑎𝑥 is the maximum effective area of the EGR valve, 𝜂𝑠𝑐is the
compensation factor for non-ideal cycles, ℎ𝑡𝑜𝑡is the total heat transfer coefficient of the exhaust
pipes and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 is the maximum area in the turbine that the gas flows through. From the field
prediction of these parameters, we can infer the health of the engine; a higher deviation from
their design value may indicate a fault in the system. We are interested in (1) predicting the
dynamics of all the variables in Eqs. (3)-(10), and (2) identifying the unknown parameters in
the system, given field measurements on 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡, and𝑊𝑒𝑔𝑟as well as Eqs. (3)-(10). We
refer to the above problem as the inverse problem in this study. Specifically, the following cases
are considered for a detailed study:
Case 1 Prediction of dynamics of the system and identification of 3 unknown parameters
𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐andℎ𝑡𝑜𝑡with clean data of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡, and𝑊𝑒𝑔𝑟.
Case 2 Prediction of dynamics of the system and identification of 3 unknown parameters
𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐andℎ𝑡𝑜𝑡with noisy data of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡, and𝑊𝑒𝑔𝑟.
Case 3 Prediction of dynamics of the system and identification of 4 unknown parameters
𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 with clean data of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡, and𝑊𝑒𝑔𝑟.
6Case 4 Prediction of dynamics of the system and identification of 4 unknown parameters
𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 with noisy data of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡, and𝑊𝑒𝑔𝑟.
In the present study, we consider self-adaptive weights [19] (discussed in section 2 and Appendix
A) in our loss function. We study the above four cases using self-adaptive weight. In order to
understand the effect and importance of self-adaptive weights in the convergence and accuracy
of results, we consider one more case without self-adaptive weight,
Case 5 Prediction of dynamics of the system and identification of 4 unknown parameters
𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 with clean data of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡, and𝑊𝑒𝑔𝑟without
self-adaptive weights.
The results of Case 1 andCase 2 are presented in Appendix G. First, we study the results
ofCase 3 andCase 5 to understand the accuracy and convergence of PINN method and the
importance of self-adaptive weights. Then, we study the results of Case 4 . The results for Case
3,Case 4 andCase 5 are discussed in section 3.
2 Methodology
We consider to employ the deep learning algorithm, particularly, the physics-informed neural
networks (PINNs), to solve the inverse problem discussed in section 1.1. To begin with, we first
briefly review the basic principle of PINNs, and then we discuss how to employ PINNs for the
present inverse problem.
2.1 PINNs for inverse problems in the diesel engine
We first briefly review the PINNs [10, 12, 14, 20, 21] for solving inverse problems, and then we
introduce how to employ the PINNs to solve the specific problem that we are of interest for the
diesel engine.
7ˆyσ
σ
σσ
σ
σg(ˆy)
tσ
σ
σ
σσ
σ
σ
σr=f/parenleftbigg
h(ˆy), g(ˆy),dˆy
dt,d2ˆy
dt2, . . . ,Λ, C, q(t)/parenrightbiggh(ˆy)
dˆy
dt
d2ˆy
dt2
...ˆy
Neural network ( {W,b})Physics (ODE)
L(θ)< /epsilon1Back
propagateloss,L(θ) = λ1L1(ˆy−y) +λ2L2(r)
θ={W,b,Λ}
PredictNoupdate θ
YesPretrained Neural
network ( θp={Wp,bp})
1Fig. 2: Schematic of physics-informed neural networks (PINNs) for inverse problems: The
left part of the figure, enclosed in the red dashed line, shows a DNN whose input is
time. The DNN is to approximate the solution ( 𝑦) to a differential equation. The top
left part of the figure enclosed in the black dashed line shows an DNN whose input
is the output 𝑦(maybe with other input, e.g. ambient condition). The output of this
network is a function 𝑔(𝑦). This network is pre-trained with laboratory data of 𝑦and
𝑔(𝑦). The right part of the figure, enclosed in the blue dashed line, denotes the physics
loss/residue. The DNN (enclosed in a red dashed line) approximates the solution to any
differential equation, and the equation is encoded using automatic differentiation. The
total lossL(θ)includes the loss of equation as well as the data. The 𝜆1and𝜆2are two
weights to the data loss and physics loss, which may be fixed or adaptive depending
upon the problem and solution method. θ={W,b,Λ}represents the parameters in
DNN, Wandbare the weights and biases of DNN, respectively and Λare the unknown
parameters of the ODE; 𝜎is the activation function, 𝑞(𝑡)is the right-hand side (RHS)
of the differential equation (source term), ℎis the function of the predicted variable,
and𝑟is the residual for the equation. θ𝑃={W𝑃,b𝑃}represents the parameters in
pre-trained neural network, W𝑃andb𝑃are the weights and biases of the pre-trained
neural network.
As illustrated in Fig. 2, the PINN is composed of two parts, i.e., a fully-connected neural
network which is to approximate the solution to a particular differential equation and the physics-
informed part in which the automatic differentiation [1] is employed to encode the corresponding
differential equation. Further, 𝜦represents the unknowns in the equation, which can be either a
constant or a field. In particular, 𝜦are trainable variables as the unknowns are constant, but they
could also be approximated by a DNN if the unknown is a field. The loss function for solving
the inverse problems consists of two parts, i.e., the data loss and the equation loss, which reads
8as:
L(θ;𝜦)=1
𝑀𝑀∑︁
𝑖=1|ˆ𝑦(𝑡𝑖;θ)−𝑦(𝑡𝑖)|2
|                        {z                        }
data loss+1
𝑁𝑁∑︁
𝑖=1|𝑟(𝑡𝑖;θ;𝜦)|2
|                 {z                 }
equation loss(11)
where θdenotes the parameters in the DNN; 𝑀and𝑁represent the number of measurements
and the residual points, respectively; ˆ 𝑦(𝑡𝑖;θ)denotes the prediction of DNN at the time 𝑡𝑖;𝑦(𝑡𝑖)
is the measurement at 𝑡𝑖, and𝑟(𝑡𝑖;θ;𝜦)represents the residual of the corresponding differential
equation, which should be zero in the entire domain. By minimizing the loss in Eq. (11), we
can obtain the optimal parameters, i.e., θ, of the DNN as well as the unknowns, i.e., 𝜦, in the
system. In the present study, we have a few empirical equations that we approximate using
DNNs. These DNNs are trained first using data and considered in place of these empirical
formulae. We fixed the parameters of these networks when we minimized the loss function for
the PINN model. Furthermore, note that here we employ the system described by one equation
as the example to demonstrate how to use PINNs for solving inverse problems. For the system
with more than one equation, we can either utilize an DNN with multiple outputs or multiple
DNNs as the surrogates for the solutions to differential equations. In addition, a similar idea
can also be employed for systems with multiple unknown fields. The loss function can then be
rewritten as
L(θ;𝜦)=𝐾∑︁
𝑘=1"
1
𝑀𝑘𝑀𝑘∑︁
𝑖=1|ˆ𝑦𝑘(𝑡𝑖;θ)−𝑦𝑘(𝑡𝑖)|2#
|                                     {z                                     }
data loss+𝐿∑︁
𝑙=1"
1
𝑁𝑙𝑁𝑙∑︁
𝑖=1|𝑟𝑙(𝑡𝑖;θ;𝜦)|2#
|                           {z                           }
equation loss(12)
where𝐾and𝐿denote the number of variables that can be measured as well as the equations,
respectively; 𝑀𝑘and𝑁𝑙are the number of measurements for the 𝑘thvariable and the number of
residual points for the 𝑙thequation, respectively; and 𝜦collects all the unknowns in the system.
Variables 𝑝𝑖𝑚,𝑝𝑒𝑚𝑥𝑟𝑇1 ˜𝑢𝑒𝑔𝑟1, ˜𝑢𝑒𝑔𝑟2𝜔𝑡 ˜𝑢𝑣𝑔𝑡
SurrogateN1(𝑡;θ1)N2(𝑡;θ2)N3(𝑡;θ3)N4(𝑡;θ4)N5(𝑡;θ5)N6(𝑡;θ6)
Equation 3 and 4 10 9 6 and 7 5 8
Table 1: Neural network surrogates employed PINNs for solving the inverse problems.
N𝑖(𝑡;θ𝑖),𝑖=1,...,6 denotes the surrogate for the 𝑖𝑡ℎDNN parameterized by θ𝑖with
the input𝑡. In particular,N1(𝑡;θ1)andN4(𝑡;θ4)have two outputs, which are used
to approximate{𝑝𝑖𝑚,𝑝𝑒𝑚}and{˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2}, respectively; the remaining DNNs have
only one output.
For the inverse problem presented in section 1.1, we are interested in (1) learning the dynam-
ics of the six states (2) inferring the unknown parameters in the system, given measurements on
{𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,𝑊𝑒𝑔𝑟}as well as Eqs. (3)-(10), using PINNs. Specifically, we utilize six DNNs
as the surrogates for the solutions to different equations, and the corresponding equations are
encoded using the automatic differentiation, as illustrated in Table 1. In addition, the loss for
9training the PINNs for Case 1 to Case 4 is expressed as follows:
L(θ,𝜦,λ𝑝𝑖𝑚,λ𝑝𝑒𝑚,λ𝜔𝑡,λ𝑊𝑒𝑔𝑟,𝜆𝑇1)=L𝑝𝑖𝑚+L𝑝𝑒𝑚+L𝜔𝑡+L𝑢𝑒𝑔𝑟1+
L𝑢𝑒𝑔𝑟2+L𝑢𝑣𝑔𝑡+10×L𝑥𝑟+𝜆𝑇1×L𝑇1+
L𝑖𝑛𝑖
𝑝𝑖𝑚+L𝑖𝑛𝑖
𝑝𝑒𝑚+L𝑖𝑛𝑖
𝜔𝑡+L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟1+
L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟2+L𝑖𝑛𝑖
˜𝑢𝑣𝑔𝑡+L𝑖𝑛𝑖
𝑥𝑟+100×L𝑖𝑛𝑖
𝑇1+
L𝑑𝑎𝑡𝑎
𝑝𝑖𝑚(λ𝑝𝑖𝑚)+L𝑑𝑎𝑡𝑎
𝑝𝑒𝑚(λ𝑝𝑒𝑚)+
L𝑑𝑎𝑡𝑎
𝜔𝑡(λ𝜔𝑡)+L𝑑𝑎𝑡𝑎
𝑊𝑒𝑔𝑟(λ𝑊𝑒𝑔𝑟),(13)
where θ=(θ1,...,θ6)are the parameters of all NNs in PINNs, 𝜦are the unknown parameters,
which will be inferred from the given measurements, L𝜙,𝜙=(𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2,˜𝑢𝑣𝑔𝑡,𝑥𝑟,𝑇1)
are the losses for the corresponding equations, and L𝑑𝑎𝑡𝑎
𝜓,𝜓=(𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,𝑊𝑒𝑔𝑟)are the losses
for the corresponding measurements, and L𝑖𝑛𝑖
𝜙,𝜙=(𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2,˜𝑢𝑣𝑔𝑡,𝑥𝑟,𝑇1)are the
losses for the initial conditions, λ𝑝𝑖𝑚,λ𝑝𝑒𝑚,λ𝜔𝑡, andλ𝑊𝑒𝑔𝑟are the weights for different loss
terms which are used to balance each term in the loss function. In particular, the self-adaptive
weight technique proposed in [19], which is capable of tuning the weights automatically, is
utilized here to obtain the optimal λ𝑝𝑖𝑚,λ𝑝𝑒𝑚,λ𝜔𝑡, andλ𝑊𝑒𝑔𝑟. More details for self-adaptive
weights in PINN can be found in Appendix A.
In the Case 5 where we have not considered self-adaptive weights, so the loss function is
given as
L(θ,𝜦)=L𝑝𝑖𝑚+L𝑝𝑒𝑚+L𝜔𝑡+L𝑢𝑒𝑔𝑟1+
L𝑢𝑒𝑔𝑟2+L𝑢𝑣𝑔𝑡+10×L𝑥𝑟+103×L𝑇1+
L𝑖𝑛𝑖
𝑝𝑖𝑚+L𝑖𝑛𝑖
𝑝𝑒𝑚+L𝑖𝑛𝑖
𝜔𝑡+L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟1+
L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟2+L𝑖𝑛𝑖
˜𝑢𝑣𝑔𝑡+L𝑖𝑛𝑖
𝑥𝑟+100×L𝑖𝑛𝑖
𝑇1+
103×L𝑑𝑎𝑡𝑎
𝑝𝑖𝑚+103×L𝑑𝑎𝑡𝑎
𝑝𝑒𝑚+
103×L𝑑𝑎𝑡𝑎
𝜔𝑡+103×L𝑑𝑎𝑡𝑎
𝑊𝑒𝑔𝑟,(14)
As for training the PINNs in the present study, we first employ the first-order optimizer,
i.e., Adam [15], to train the parameters in the NNs, unknowns in the systems as well as the
self-adaptive weights for a certain number of steps. We then fix the self-adaptive weight and
employed Adam to train the parameters in the NNs, and unknowns in the systems for another
certain number of steps. We then switch to the second-order accuracy optimizer, i.e., LBFGS-B,
to further optimize the parameters in the NNs and the unknowns in the systems. Note that the
self-adaptive weights are optimized at the first training stage of Adam only, and they are fixed
during the second training stage of Adam and LBFGS-B training with the values at the end of
the first stage of Adam optimization.
2.1.1 Neural network surrogates for empirical formulae
In the mean value engine model proposed in [24], empirical formulae, e.g., polynomial functions,
are employed for the volumetric efficiency ( 𝜂𝑣𝑜𝑙), effective area ratio function for EGR valve
(𝑓𝑒𝑔𝑟), turbine mechanical efficiency ( 𝜂𝑡𝑚), effective area ratio function for VGT ( 𝑓𝑣𝑔𝑡), choking
function (for VGT) ( 𝑓Π𝑡), compressor efficiency ( 𝜂𝑐), and volumetric flow coefficient (for the
compressor) ( Φ𝑐). Note that these empirical formulae are engine-specific and may not be
appropriate for the diesel engines considered in the present study. Deep neural networks
10(DNNs), which are known to be universal approximators of any continuous function, are thus
utilized as more general surrogates for the empirical formulae here. Particularly, we employ six
DNNs for the aforementioned variables, and the inputs for each DNN are presented in Table 2.
Variable 𝜂𝑣𝑜𝑙 𝑓𝑒𝑔𝑟 𝐹𝑣𝑔𝑡,Π𝑡𝜂𝑡𝑚 𝜂𝑐 Φ𝑐
SurrogateN(𝑃)
1(x;θ𝑃
1)N(𝑃)
2(x;θ𝑃
2)N(𝑃)
3(x;θ𝑃
3)N(𝑃)
4(x;θ𝑃
4)N(𝑃)
5(x;θ𝑃
5)N(𝑃)
6(x;θ𝑃
6)
Input ( x){𝑝𝑖𝑚,𝑛𝑒} ˜𝑢𝑒𝑔𝑟{˜𝑢𝑣𝑔𝑡,Π𝑡}{𝜔𝑡,𝑇𝑒𝑚,Π𝑡}{𝑊𝑐,Π𝑐}{𝑇𝑎𝑚𝑏,Π𝑐,𝜔𝑡}
Equation†B.8 B.24 D.4 B.33 B.40 B.50
†The empirical equations are discussed in Appendix
Table 2: Neural network surrogates for empirical formulae N(𝑃)
𝑖(x;θ𝑃
𝑖),𝑖=1,..., 6 denotes
the surrogate for the 𝑖𝑡ℎDNN parameterized by 𝜃𝑃
𝑖with the input x. All the neural
networks have one output each.
We now discuss the training of the DNNs illustrated in Table 2. In laboratory experiments,
measurements on all variables are available. We can then train the neural network surrogates in
Table 2 using the data collected in the laboratory. The loss function considered for the training
of these networks is
L𝑖(θ𝑃
𝑖)=1
𝑛𝑖𝑛𝑖∑︁
𝑗=1h
𝑦(𝑗)
𝑖−ˆ𝑦(𝑗)
𝑖i2
=1
𝑛𝑖𝑛𝑖∑︁
𝑗=1h
𝑦(𝑗)
𝑖−N(𝑃)
𝑖(x𝑖;θ𝑃
𝑖)(𝑗)i2
, 𝑖=1,2,..., 6 (15)
where𝑖=1,2,..., 6 are the different neural networks for the approximation of the empirical
formulae, x𝑖are the input corresponds to the 𝑖thnetwork, ˆ𝑦𝑖and𝑦𝑖are the output of the 𝑖th
network and the corresponding labelled values respectively, 𝑛𝑖is the number of labelled dataset
corresponds to the 𝑖thneural network. The laboratory data required for calculating labelled data
for training each of these networks are shown in Table 3 (in §2.3). We discuss the calculation of
labelled data from the laboratory data in Appendix D. We train these networks using the Adam
optimizer. Upon the training of these DNNs, we will plug them in the PINNs to replace the
empirical models, which are represented by the pretrained neural network with the output 𝑔(𝑦)
in Fig. 2.
2.2 Flowchart for PINN model for diesel engine
In section 1.1, we discussed the problem setup, and in subsequent sections, we discussed the
approximation of different variables using neural networks as well as the basics of the PINN
method and the implementation of PINN in the present problem. In Fig. 2, we have shown a
schematic diagram along with a pre-trained network for a general ordinary differential equation.
In this section, we show a complete flowchart for the calculation of physics loss functions for
the engine problem. In Fig. 3, we show the flow chart for calculating the physics-informed
loss for the present problem. Note that we have not shown the data loss and the self-adaptive
weights in the flow chart.
11N1(t;θ1) tN2(t;θ2) t
N3(t;θ3) t
N(P)
1(.;θP
1)Cylinder
ﬂowηvol
ne
uδCylinder Tem-
peratureWei
Wfηsc
htotT1xr
pem
pim
N4(t;θ4) t
EGR Flow AegrmaxTem
N(P)
2(.;θP
2)EGR
dynamics
˜uegr
fegr˜uegr1
˜uegr2
N6(t;θ6) t
Turbine Flow Avgtmaxpamb
pem
N(P)
3(.;θP
3)fvgt×fΠt ˜uvgt
Turbine PowerWtN5(t;θ5) tpim
pamb
N(P)
6(.;θP
6)Tamb
Compressor ﬂowωt
Φc
N(P)
5(.;θP
5)
Compressor PowerWc ηcr(pim) =dpim
dt−RaTim
Vim(Wc+Wegr−Wei)
r(pem) =dpem
dt−ReTem
Vem(Weo−Wt−Wegr)
WegrWeo
r(ωt) =dωt
dt−Ptηm−Pc
Jtωt
PcPtηmr(˜uegr1) =d˜uegr1
dt−1
τegr1(uegr(t−τdegr)−˜uegr1)
r(˜uegr2) =d˜uegr2
dt−1
τegr2(uegr(t−τdegr)−˜uegr2)uegr
r(˜uvgt) =d˜uvgt
dt−1
τvgt(uvgt(t−τdvgt)−˜uvgt)uvgtr(xr) =xr−Π(1/γa)
ex(−1/γa)
p
rcxv
r(T1) =T1−(xrTe+ (1−xr)Tim)Πe=pem
pim
Texpxv
N(P)
4(.;θP
4)ηtmNi(t;θi)Neural Network
N(P)
i(.;θP
i)Pre-trained
Neural NetworkUnknown
ParameterLxr(θ) =1
nn/summationdisplay
i=1r(xr)2
LT1(θ) =1
nn/summationdisplay
i=1r(T1)2
L˜uegr1(θ) =1
nn/summationdisplay
i=1r(˜uegr1)2
L˜uegr2(θ) =1
nn/summationdisplay
i=1r(˜uegr2)2
L˜uvgt(θ) =1
nn/summationdisplay
i=1r(˜uvgt)2Lpim(θ) =1
nn/summationdisplay
i=1r(pim)2
Lpem(θ) =1
nn/summationdisplay
i=1r(pem)2
Lωt(θ) =1
nn/summationdisplay
i=1r(ωt)2
1
Fig. 3: Flow chart for the proposed PINNs model: (Detailed caption is added on the other
page).
12Detailed Caption for Fig. 3
Fig.3: Flow chart for proposed PINNs model: Flow chart for the proposed PINN model
for the inverse problem for the engine for prediction of dynamics of the system variables and
estimation of unknown parameters. The inputs are input control vector {𝑢𝛿,𝑢𝑒𝑔𝑟,𝑢𝑣𝑔𝑡}and
engine speed 𝑛𝑒.
•Six neural network N𝑖(𝑡;θ),𝑖=1,2,..., 6 indicated in dashed rectangular oval takes time 𝑡
as input and predict 𝑝𝑖𝑚,𝑝𝑒𝑚,𝑥𝑟and𝑇1˜𝑢𝑒𝑔𝑟1, ˜𝑢𝑒𝑔𝑟2,𝜔𝑡and ˜𝑢𝑣𝑔𝑡as shown in Table 1.
•Four unknown parameters indicated in hexagon are 𝜂𝑠𝑐,ℎ𝑡𝑜𝑡,𝐴𝑒𝑔𝑟𝑚𝑎𝑥 and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 .
•Six pre-trained neural networks N(𝑃)
𝑖(.;θ),𝑖=1,2,..., 6 indicated in dashed-dotted rect-
angular oval takes appropriate input and predict the empirical formulae as shown in Table
2. The parameters (weights and biases) of these pre-trained DNNs are kept fixed to predict
the empirical formulae.
There are eight main blocks calculating different variables. The equations for the calculation
of each of the quantities are shown in Appendix B.
•Cylinder flow: calculates𝑊𝑒𝑖,𝑊𝑓and𝑊𝑒𝑜using Eqs. B.5, B.7 and B.6 respectively.
•Cylinder Temperature: calculates𝑥𝑣,𝑥𝑝,𝑇𝑒and𝑇𝑒𝑚using Eqs. B.14, B.13, B.9 and B.16
respectively. ℎ𝑡𝑜𝑡and𝜂𝑠𝑐are considered as learnable parameters in the calculation of 𝑇𝑒𝑚
and𝑇𝑒respectively.
•EGR dynamics: calculated ˜𝑢𝑒𝑔𝑟using Eq. B.19
•EGR Flow: calculates EGR mass flow 𝑊𝑒𝑔𝑟using Eq. B.20. 𝐴𝑒𝑔𝑟𝑚𝑎𝑥 is considered as
learnable parameter.
•Compressor flow: calculates compressor mass flow 𝑊𝑐using Eq. B.49
•Compressor Power: calculates compressor power 𝑃𝑐using Eq. B.39
•Turbine Flow: calculates turbine mass flow 𝑊𝑡using Eq. B.27. 𝐴𝑣𝑔𝑡𝑚𝑎𝑥 is considered as
trainable parameter.
•Turbine Power: calculates effective turbine power 𝑃𝑡𝜂𝑚using Eq. B.32
There are five blocks, which calculate the residual of the equation. The first block calculates
the residual for state equations for 𝑝𝑖𝑚and𝑝𝑒𝑚; the second one calculates the residual for the
equations of 𝑥𝑟and𝑇1; the third block calculates the residuals for state equations for ˜ 𝑢𝑒𝑔𝑟1
and ˜𝑢𝑒𝑔𝑟2; the fourth block calculates the residual for the state equation for ˜ 𝑢𝑣𝑔𝑡, and the fifth
block calculates the residual for the state equation for 𝜔𝑡. There are another two blocks, which
calculate the physics loss. The first one calculates the physics loss corresponding to state
variable𝑝𝑖𝑚,𝑝𝑒𝑚and𝜔𝑡. The second block calculates state physics loss corresponding to
state variables ˜ 𝑢𝑒𝑔𝑟1, ˜𝑢𝑒𝑔𝑟2, ˜𝑢𝑣𝑔𝑡and physics loss corresponding to 𝑥𝑟and𝑇1. The data losses
can be calculated from the variables calculated from the appropriate blocks.
2.3 Data generation
We now discuss the generation of data for training the NNs utilized in this study. Specifically,
we have mainly two different types of data here: (1) the data collected from the laboratory that
are used to train the DNN surrogates to replace the empirical formulae used in [24]; and (2)
field data𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟.
In laboratory experiments, we have measurements on state variables, some of which can be
employed for training the neural network surrogates for the empirical formulae. The laboratory
13data required to calculate the labelled data for each of the surrogates are shown in Table 3. The
calculation of labelled data from laboratory data is discussed in Appendix D. After training, we
consider these pre-trained surrogates in field experiments in place of the empirical formulae.
The parameters of these networks are kept constant in the PINNs model for the field experiment.
In field experiments, we have records for the four inputs, i.e., u={𝑢𝛿, 𝑢𝑒𝑔𝑟, 𝑢𝑣𝑔𝑡}and𝑛𝑒.
In addition, we only have measurements on four variables in field experiments, i.e., the intake
manifold pressure ( 𝑝𝑖𝑚), exhaust manifold pressure ( 𝑝𝑒𝑚), turbine speed ( 𝜔𝑡) and EGR mass
flow (𝑊𝑒𝑔𝑟).
In both the laboratory and field experiments, we have the records for the inputs (i.e.,
u={𝑢𝛿, 𝑢𝑒𝑔𝑟, 𝑢𝑣𝑔𝑡}and𝑛𝑒) from an actual engine running conditions. Considering that
we only have a certain number of records for the variables in the running engine, which
cannot be used to verify our PINN model since our objective is to use it to predict the whole
gas flow dynamics in the engine. We, therefore, take the records for the real inputs (i.e.,
u={𝑢𝛿, 𝑢𝑒𝑔𝑟, 𝑢𝑣𝑔𝑡}and𝑛𝑒) and employ them as the inputs for the governing equations Eqs.
3-8. We then solve these equations using Simulink to obtain the dynamics for all variables.
We use the data from Simulink to mimic the real-world measurements, which are employed as
the training data for PINNs and the DNN for the pre-trained networks. The remaining data are
used as the validation data to test the accuracy of PINN for reconstructing the gas dynamics in
a running engine, given partial observations. Given that the real measurements are generally
noisy, we add 3%, 3%, 1% and 10% Gaussian noise in 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟, respectively.
These different signals have different noise values because they are different measurements with
different noise characteristics.
In the present study, we consider two sets of input data in the training and testing of the
surrogate neural networks for the empirical formulae. The first set of data (Set-I) is two (2)
hours of data collected at a sampling rate of 1 sec. This control input vector {𝑢𝛿,𝑢𝑒𝑔𝑟,𝑢𝑣𝑔𝑡}
and𝑛𝑒are considered to generate simulated data with different ambient conditions, which are
shown in Table 4. The second set of data (Set-II) is twenty-minute (20 minutes) data collected
at a sampling rate of 0.2 sec. This control input vector {𝑢𝛿,𝑢𝑒𝑔𝑟,𝑢𝑣𝑔𝑡}and𝑛𝑒are considered to
generate simulated data with Case-V ambient conditions.
The labelled data for the training of surrogate neural network for 𝜂𝑣𝑜𝑙,𝐹𝑣𝑔𝑡,Π𝑡,𝜂𝑐andΦ𝑐
are generated for Case-I to Case-IV with a 𝑑𝑡=0.2 sec. The testing data are generated for
Case-V with the same 𝑑𝑡. We observed from the engine model that the EGR valve actuator
is independent of the other system of the engine and depends only on the EGR control signal
(𝑢𝑒𝑔𝑟). Thus, for the training of surrogate neural network for 𝑓𝑒𝑔𝑟(N(𝑃)
2(:,θ)), we consider
the training data set corresponding to Case-I only and the testing data set corresponding to
Case-V. The labelled data for 𝜂𝑡𝑚are calculated from Eq. (B.25) ( §B.4), which is a differential
equation, thus requires a finer 𝑑𝑡. The simulated data for the calculation of labelled 𝜂𝑡𝑚are
generated with 𝑑𝑡=0.025 sec in all the Cases. We assume that the Set-I data for the input control
vector includes a good operating range for training surrogate neural networks for the empirical
formulae. The field data ( 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟) for the inverse problem are considered from
Case-V.
14Empirical quantities††,†††Symbol Laboratory test data required††††
Volumetric efficiency ( §B.2)𝜂𝑣𝑜𝑙•Intake manifold pressure ( 𝑝𝑖𝑚)
•Engine speed ( 𝑛𝑒)
•Total mass flow from the intake
manifold into the cylinders ( 𝑊𝑒𝑖)
•Intake manifold temperature ( 𝑇𝑖𝑚)
Effective area ratio function
for EGR ( §B.3)𝑓𝑒𝑔𝑟•EGR position ( ˜ 𝑢𝑒𝑔𝑟)
•EGR mass flow ( 𝑊𝑒𝑔𝑟)
•Exhaust manifold pressure ( 𝑝𝑒𝑚)
•Intake manifold pressure ( 𝑝𝑖𝑚)
•Exhaust manifold temperature ( 𝑇𝑒𝑚)
Effective area ratio function
for VGT (𝑓𝑣𝑔𝑡) and chocking
function (𝑓Π𝑡) (§B.4)𝑓𝑣𝑔𝑡×𝑓Π𝑡•VGT position ( ˜ 𝑢𝑣𝑔𝑡)
•Exhaust manifold pressure ( 𝑝𝑒𝑚)
•Ambient pressure ( 𝑝𝑎𝑚𝑏)
•Turbine mass flow ( 𝑊𝑡)
•Exhaust manifold pressure ( 𝑝𝑒𝑚)
•Exhaust manifold temperature ( 𝑇𝑒𝑚)
Turbine mechanical
efficiency†††††(§B.4)𝜂𝑡𝑚•Turbine speed ( 𝜔𝑡)
•Exhaust manifold temperature ( 𝑇𝑒𝑚)
•Exhaust manifold pressure ( 𝑝𝑒𝑚)
•Ambient pressure ( 𝑝𝑎𝑚𝑏)
•Compressor mass flow ( 𝑊𝑐)
•Compressor temperature ( 𝑇𝑐)
•Ambient temperature ( 𝑇𝑎𝑚𝑏)
•Turbine mass flow ( 𝑊𝑡)
Compressor efficiency ( §B.5)𝜂𝑐•Intake manifold pressure ( 𝑝𝑖𝑚)
•Compressor mass flow ( 𝑊𝑐)
•Temperature after the compressor ( 𝑇𝑐)
•Ambient temperature ( 𝑇𝑎𝑚𝑏)
•Ambient pressure ( 𝑝𝑎𝑚𝑏)
Volumetric flow coefficient
for compressor ( §B.5)Φ𝑐•Turbine speed ( 𝜔𝑡)
•Compressor mass flow ( 𝑊𝑐)
•Intake manifold pressure ( 𝑝𝑖𝑚)
•Ambient temperature ( 𝑇𝑎𝑚𝑏)
•Ambient pressure ( 𝑝𝑎𝑚𝑏)
†It is assumed that the parameters/constant are known, however not the coefficients for
the empirical formulae.
††The definition of the quantities are discussed in relevant sections in Appendix B.
†††The calculations of the empirical quantify from the laboratory data are included
in Appendix D.
††††A brief discussion on instrumentation and test procedure is included in Appendix C.
†††††For calculation of 𝜂𝑡𝑚, dynamic data are required (discussed in Appendix D and
section H).
Table 3: List of empirical formulae represented using a pre-trained neural network and lab test
data required for their training†.
15Case𝑇𝑎𝑚𝑏 𝑝𝑎𝑚𝑏×105(Pa)InputSamplingPurpose(kelvin) (Approx. elevation) 𝑑𝑡
Case-I 233.15 (−40◦C) 0.7000 (at 3000 m) Set-I 1 sec Training
Case-II 233.15 (−40◦C) 1.0111 (at 17.9 m) Set-I 1 sec Training
Case-III 270.15 (−3◦C) 0.7000 (at 3000 m) Set-I 1 sec Training
Case-IV 313.15 (40◦C) 1.0111 (at 17.9 m) Set-I 1 sec Training
Case-V 298.15 (25◦C) 0.8000 (at 1837 m) Set-II 0.2 sec Testing
Table 4: Ambient conditions for training and testing of neural networks: The different
ambient conditions are considered for generating training and testing data. The input
data Set-I is two hours of input control vector {𝑢𝛿,𝑢𝑒𝑔𝑟,𝑢𝑣𝑔𝑡}and𝑛𝑒collected from
an actual engine running condition. Similarly, Set-II is twenty-minute of input con-
trol vector{𝑢𝛿,𝑢𝑒𝑔𝑟,𝑢𝑣𝑔𝑡}and𝑛𝑒collected from an actual engine running condition.
Case-I to Case-IV are considered for the training of the surrogate neural network for
the empirical formulae ( N(𝑃)
𝑖(:,θ𝑖)), while Case-V is considered for testing of these
networks. The data for the field data are also considered from Case-V.
3 Results and discussions
In this section, we demonstrate the applicability of proposed PINNs for solving the inverse
problems discussed in section 1.1. Case 1 and Case 2 have three unknown parameters, while
Case 3 to Case 5 have four unknown parameters. The predicted values of the unknowns for all
five cases are shown in Table 5. In this section, we will discuss the results of Case 3 to Case 5.
The results of Case 1 and Case 2 are presented in Appendix G.
First, we study the results of Case 3 and Case 5 to understand the applicability of PINN and
the importance of self-adaptive weight in accuracy and convergence. Then, we study the results
of Case 4, which is similar to Case 3; however, with added noise in the field data considered.
We also discuss the results for the surrogate for the empirical formulae in Appendix H. Note
that the results for all variables are presented in a normalized scale from zero to one using the
following equation,
𝑥𝑠𝑐𝑎𝑙𝑒=𝑥−𝑥𝑚𝑖𝑛
𝑥𝑚𝑎𝑥−𝑥𝑚𝑖𝑛, (16)
where𝑥and𝑥𝑠𝑐𝑎𝑙𝑒 are the data before and after scaling, respectively, 𝑥𝑚𝑖𝑛is the minimum value
of true data of 𝑥within the time span considered, 𝑥𝑚𝑎𝑥is the maximum value of true data of 𝑥
within the time span considered.
We are considering the input control vector {𝑢𝛿,𝑢𝑒𝑔𝑟,𝑢𝑣𝑔𝑡}and engine speed 𝑛𝑒from
an actual field record. It is assumed that these data have inherent noise in their records.
Detailed studies are carried out considering a 1-minute duration. The number of resid-
ual points considered in the physics-informed loss and data loss is 301 at equal 𝑑𝑡=0.2
sec. The initial conditions considered for {𝑝𝑖𝑚, 𝑝𝑒𝑚, 𝑥𝑟, 𝑇 1,˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2, 𝜔𝑡,˜𝑢𝑣𝑔𝑡}are
{8.0239×104,8.1220×104,0.0505,305.3786,18.2518,18.1813,1.5827×103,90.0317}
respectively. The measured field data are also considered for 1 min with equal 𝑑𝑡=0.2 sec.
Thus, each of the measured field quantities has 301 records.
The details of the neural networks considered for the PINN problem are shown in Table 6.
We consider 𝜎(.)=tanh(.)activation function for hidden layers for all the neural networks. We
would also like to emphasize that the scaling of output is one of the important considerations for
faster and an accurate convergence of the neural network. Furthermore, output transformation
16𝐴𝑒𝑔𝑟𝑎𝑚𝑥 𝜂𝑠𝑐ℎ𝑡𝑜𝑡𝐴𝑣𝑔𝑡𝑚𝑎𝑥Known
variablesPredicted
variables
True 4×10−41.102 96.28 8.456×10−4
Case 1 3.93×10−41.12 110 NAClear data of
𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,
𝑊𝑒𝑔𝑟The neural
networks
predict:𝑝𝑖𝑚,
𝑝𝑒𝑚, ˜𝑢𝑣𝑔𝑡, ˜𝑢𝑒𝑔𝑟1,
˜𝑢𝑒𝑔𝑟2,𝑇1,𝑥𝑟.
The pretrained
neural networks
predict:𝜂𝑣𝑜𝑙,
𝜂𝑡𝑚,𝜂𝑐,Φ𝑐,
𝐹𝑣𝑔𝑡,Π𝑡,𝑓𝑒𝑔𝑟.
Other variables
are derived
from these
predicted
quantities.Case 2 3.93×10−41.12 109 NANoisy data of
𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,
𝑊𝑒𝑔𝑟
Case 3 3.61×10−40.962 113 7.86×10−4Clear data of
𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,
𝑊𝑒𝑔𝑟
Case 4 3.51×10−40.834 134 7.27×10−4Noisy data of
𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,
𝑊𝑒𝑔𝑟
Case 5 2.28×10−40.829 140 7.27×10−4Case 3
without
self-adaptive
weights
Mask and scale considered
Mask ExponentialSoft-
plusExpo-
nentialExponentialFor faster convergence and to have
positive value
Scale×10−4×1×10×10−4 Scale to obtain the parameters in
physical domain
Table 5: Predicted unknowns: Predicted unknown parameters for different cases considered.
is another important consideration. The outputs are physical quantity and always positive. The
governing equations are valid only for positive quantities (e.g. in Eq. B.20, a negative 𝑝𝑖𝑚will
result in negative 𝑊𝑒𝑔𝑟). The output transformation will ensure that the predicted quantities
are always positive in each epoch. Similarly, as shown in Table 5, the mask for the unknown
parameters will ensure a positive value. We also observed that the unknown parameters are of
different scales. The scale considered for the unknown parameters will ensure the optimization
of these parameters is on the same scale. The parameters of the neural network are optimized
first using Adam optimized in Tensorflow-1 with 200 ×103epoch and further with LBFGS-B
optimized. It is also important to note that we have considered self-adaptive weights in the
proposed method; thus, we considered different optimizers for each set of self-adaptive weights.
Further, self-adaptive weights are optimized only during the process of Adam optimization
up to 100×103epoch. After 100×103epoch and during the process of optimization using
LBFGS-B, the self-adaptive weights are considered constants with the values at 100 ×103epoch
of Adam optimization. The sizes of self-adaptive weight are 301 ×1 forλ𝑝𝑖𝑚,λ𝑝𝑒𝑚,λ𝜔𝑡and
λ𝑊𝑒𝑔𝑟. The size of self adaptive weight of λ𝑇1is 1×1. Softplus masks are considered for all
the self-adaptive weights.
17Neural
networkNetwork size OutputOutputs transformation
‡‡ Scaling
N1(𝑡;θ1) [ 1,10,10,10,2]𝑝𝑖𝑚,𝑝𝑒𝑚𝑆𝑃(𝑝𝑖𝑚)+0.5,𝑆𝑃(𝑝𝑒𝑚)×105
N2(𝑡;θ2) [ 1 10,10,1]𝑥𝑟 𝑆𝑃(𝑥𝑟) × 0.03
N3(𝑡;θ3) [ 1,15,15,15,1]𝑇1𝑆𝑃(𝑇1)+230/300×300
N4(𝑡;θ4) [ 1,10,10,10,2] ˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2𝑆(˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2) × 100
N5(𝑡;θ5) [ 1,10,10,1]𝜔𝑡 𝑆𝑃(𝜔𝑡)×5×103
N6(𝑡;θ6) [ 1,10,10,1] ˜𝑢𝑣𝑔𝑡 𝑆(˜𝑢𝑣𝑔𝑡) × 100
‡‡𝑆𝑝(.)−→ softplus function. 𝑆(.)−→ sigmoid function
Table 6: Details of neural network for PINNs: Details of neural networks considered to
approximate the state variables and 𝑇1and𝑥𝑟. The input to the neural networks is time
𝑡and the activation functions for the hidden layers are 𝜎(.)=tanh(.). The outputs
for each network are shown in the “Output” column. The “Output transformation”
column shows whether the output from the neural network is passed through any other
function. The last column, ”Scaling”, shows the scaling factor to be multiplied by the
final output to obtain the variable in physical space. The input to the networks is time
0 to 60 sec and scaled between [−1,1].
3.1 PINN for the inverse problem with four unknown parameters
Results for Case 3 and Case 5
We first consider Case 3 and Case 5 in which we have four unknown parameters 𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡
and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 . The dynamics of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟can be obtained from the corresponding
sensor measurements. We then employ the PINN to predict the dynamics for the variables and
infer the four unknowns in the system. The difference between the two cases is that in Case 3,
we have considered self-adaptive weights, while in Case 5, we have not considered self-adaptive
weights. We consider these two cases to study the applicability of PINN and the importance of
self-adaptive weights in the present problem.
The predicted output from the neural networks, i.e., the states and 𝑇1and𝑥𝑟are shown in
Fig. 4. The predicted values of the unknown parameters are shown in Table 5. We observe that
the predicted states are in good approximation with the true value in both cases. However, the
predicted𝑇1and𝑥𝑟are not in good agreement with the true value. We study the effect of 𝑇1and
𝑥𝑟on the other variables by comparing the predicted dynamics of 𝑇𝑒and𝑇𝑒𝑚(ref Eqs. (B.9)
and (B.16)). We also note that 𝑇𝑒depends on the unknown 𝜂𝑠𝑐and𝑇𝑒𝑚depends on unknowns
𝑇𝑒andℎ𝑡𝑜𝑡. The predicted dynamics of 𝑇𝑒and𝑇𝑒𝑚are shown in Fig. 5.b and 5.c, respectively.
We observe that both 𝑇𝑒and𝑇𝑒𝑚show somewhat good agreement even 𝑇1and𝑥𝑟do not match
with the true value. The accuracy is more in Case 3 compared to Case 5. We believe that the
difference in the true value and the predicted value is due to the error in the predicted value of
unknown parameters. We also study the dependent variables 𝐴𝑒𝑔𝑟and𝑊𝑡of unknown 𝐴𝑒𝑔𝑟𝑚𝑎𝑥
and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 , and are shown in Fig. 5.a and 5.d, respectively. We observe that in Case 3, the
predicted dynamics for both variables show good agreement with true value. However, in Case
5, the𝐴𝑒𝑔𝑟does not show good agreement with true value. This is because the predicted value
of𝐴𝑒𝑔𝑟𝑚𝑎𝑥 has more error than Case 3.
180 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00˜uegr1
(c)uegr1True
Case 5
Case 3
0 20 40 60
t(sec)0.00.51.0˜uegr2
(d)uegr2True
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00˜uvgt
(e) ˜uvgtTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00ωt
(f)ωtTrue
Case 5
Case 3
0 20 40 60
t(sec)0.00.51.0T1
(g)T1True
Case 5
Case 3
0 20 40 60
t(sec)0.00.51.0xr
(h)xrTrue
Case 5
Case 3Fig. 4: Predicted states and 𝑇1and𝑥𝑟for Case 3 and Case 5: Predicted dynamics of the
state variables of the engine and 𝑇𝑟and𝑥1for Case 3 (PINN with self-adaptive weights)
and Case 5 (standard PINN without self-adaptive weights). The variables are scaled
using Eq. 16. It can be observed that the predicted dynamics of the states are in good
agreement with the true values. However, 𝑇1and𝑥𝑟do not match with the true value.
We study the dependent variables of these two variables, and are shown in Fig. 5.
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(a)AegrTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Te
(b)TeTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(c)TemTrue
Case 5
Case 3
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(c)WtTrue
Case 5
Case 3
Fig. 5: Predicted dynamics of dependent variables for Case 3 and Case 5: Predicted dy-
namics of𝐴𝑒𝑔𝑟,𝑇𝑒,𝑇𝑒𝑚and𝑊𝑡for Case 3 and Case 5. These variables depend on the
unknown parameters 𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 respectively. We also note that 𝑇𝑒
depends on𝑇1and𝑥𝑟.
In order to study the importance of self-adaptive weights, we study the convergence of the
unknown parameters for both cases with self-adaptive weight (Case 3) and without self-adaptive
weight (Case 5). The convergences of the unknown parameters with epoch for both cases are
shown in Fig. 6. In Case 3 (with self-adaptive weights), we can observe that the unknown
parameters converge faster and are more accurate. Furthermore, we also study the effect of
19different initialization of network parameters for PINN and self-adaptive weights. We run the
PINN model for Case 3 and Case 5 with different initialization of parameters of PINN (DNN
and unknown parameters) and self-adaptive weight keeping other hyperparameters (number of
epoch considered, learning rate scheduler etc.) the same. The results for both cases are shown in
Fig. 7. It is observed that for unknowns, 𝜂𝑠𝑐and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 for both cases show similar accuracy.
However, for unknowns, 𝐴𝑒𝑔𝑟𝑚𝑎𝑥 andℎ𝑡𝑜𝑡, Case 3, which is with self-adaptive weights, shows
better accuracy than Case 5 (without self-adaptive weights) for all the runs. In Fig. 8, we
show the self-adaptive weights for 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟after 100×103epoch (constant value
after 100×103epoch). Thus, we conclude that self-adaptive weights are important for better
accuracy and convergence for the present problem.
0 1 2
epoch ×105246Aegrmax ×10−4
(a)AegrmaxTrue
Case 5
Case 3
0 1 2
epoch ×1050.81.0ηsc
(b)ηscTrue
Case 5
Case 3
0 1 2
epoch ×105100200300htot
(c)htotTrue
Case 5
Case 3
0 1 2
epoch ×10581012Avgtmax ×10−4
(d)AvgtmaxTrue
Case 5
Case 3
Fig. 6: Convergence of the unknown parameters for Case 3 and Case 5: Convergence of the
unknown parameters with epoch for Case 3 (PINN with self-adaptive weights) and Case
5 (standard PINN without self-adaptive weights). It is observed that Case 3 converges
faster and also shows better accuracy.
1 15 30
#2.53.03.54.0Aegrmax×10−4
(a)Aegrmax1 15 30
#0.91.01.1ηsc
(b)ηsc1 15 30
#1.01.21.4htot×102
(c)htot1 15 30
#7.58.08.5Avgtmax×10−4
(d)Avgtmax
Black dashed line→true value, Blue dots →Case 3, Red dots→Case 5
Fig. 7: Predicted unknown parameters for Case 3 and Case 5: Predicted unknown param-
eters for Case 3 (PINN with self-adaptive weights) and Case 5 (standard PINN) when
prediction is made multiple times with different initialisation of the parameters of PINN,
self-adaptive weights, and the unknown parameters.
200 20 40 60
t(sec)2004006008001000λpim
(a)λpim0 20 40 60
t(sec)2004006008001000λpem
(b)λpem0 20 40 60
t(sec)7008009001000λωt
(c)λωt0 20 40 60
t(sec)02505007501000λWegr
(d)λWegrFig. 8: Self-adaptive weights for Case 3: Self-adaptive wights for 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟after
100×103epoch of Adam optimization. The values of the self-adaptive weights after
100×103Adam optimization and LBFGS-B optimization are constant with the values
of self-adaptive weight at 100 ×103epoch.
Results for Case 4: Four unknowns with noisy measurement data
In the previous section, we have shown the effectiveness of PINN and the importance of self-
adaptive weights. In this section, we test the robustness of the proposed PINN formulation for
predicting the gas flow dynamics of the diesel engine given noisy data. In particular, we are
considering Case 4 (the same Case 3 but noisy measure data), in which we have four unknown
parameters𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡and𝐴𝑣𝑔𝑡𝑚𝑎𝑥 , with noise measurement of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,𝑊𝑒𝑔𝑟.
We contaminate the training data 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,𝑊𝑒𝑔𝑟considered in Case 3 with Gaussian
noise and consider these as synthetic field measurements. We present the predicted dynamics
of the known data in Fig. 9(a)-(d) and unknown parameters in Table 5. We observe that the
dynamics of the predicted 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡,𝑊𝑒𝑔𝑟matches with the reference solution. However,
in the case of 𝑊𝑒𝑔𝑟, there is a small discrepancy in the predicted values near 20-25 sec, which
we can attribute to over-fitting caused by the noisy training data. We study the dynamics of
𝑇𝑒𝑚,𝑊𝑒𝑖𝐴𝑒𝑔𝑟and𝑊𝑡on which we do not have any measured data, and these are shown in Fig.
9(e)-(h). We observe that 𝑊𝑒𝑖matches with the reference results. Most of the dynamics of 𝐴𝑒𝑔𝑟
and𝑊𝑡match with the reference solution. The mismatch in these two variables may also be
attributed to over-fitting caused by noisy data. The profile of 𝑇𝑒𝑚matches with the reference
solution, however, it is not an exact match with the reference solution. This is because of the
error in the predicted value of unknown parameter 𝜂𝑠𝑐andℎ𝑡𝑜𝑡. We also note that in the present
study, we do not have any temperature measurements of field data. Thus, we expect an error
in the predicted temperature measurement. We also study the convergence of the unknown
parameters with epoch and shown in Fig. 10. We note that, in this case, we consider the same
hyperparameters in the optimization process. In some cases, we see over-feeting due to noisy
data. This may be controlled by changing the hyperparameters, specially the learning rate for
the self-adaptive weights.
210 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00ωt
(c)ωtTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Wegr
(d)WegrTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(e)TemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wei
(f)WeiTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(g)AegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(h)WtTrue
PredFig. 9: Predicted dynamics for variables for Case 4: Predicted dynamics of (a)-(d) variables
whose noisy field measurements are known. (e)-(h) dynamics of other important vari-
ables, which are also dependent on the unknown parameters. These results are for Case
4 with 4 unknown parameters and noisy field measurements.
0 1 2
epoch ×105234Aegrmax ×10−4
(a)AegrmaxTrue
Pred
0 1 2
epoch ×1050.70.80.91.01.1ηsc
(b)ηscTrue
Pred
0 1 2
epoch ×105100200300htot
(c)htotTrue
Pred
0 1 2
epoch ×105681012Avgtmax ×10−4
(d)AvgtmaxTrue
Pred
Fig. 10: Convergence of the unknown parameters for Case 4: Convergence of the unknown
parameters with epoch for Case 4 (PINN with self-adaptive weights and noisy field
data)
We also study the prediction of empirical formulae in this case and shown in Fig. 11.
We observe that 𝜂𝑣𝑜𝑙and𝜂𝑐match with the reference solution. These two quantity gives the
volumetric efficiency of the cylinder and the efficiency of the compressor. The other four
quantities (𝑓𝑒𝑔𝑟,𝑓𝑣𝑔𝑡×𝑓Π𝑡,𝜂𝑡𝑚,Φ𝑐), also match most of its points. The discrepancy can be
attributed to the noisy measurement of field data.
220 20 40 60
t(sec)0.000.250.500.751.00ηvol
(a)ηvolTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Pred
0 20 40 60
t(sec)0.00.51.0fvgt×fΠt
(c)fvgt×fΠtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ηtm
(d)ηtmTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ηc
(e)ηcTrue
Pred
0 20 40 60
t(sec)0.00.51.0Φc
(f) Φ cTrue
PredFig. 11: Empirical formulae for Case 4: The prediction of empirical formulae for Case 4
(PINN with self-adaptive weights and noisy field data).
4 Summary
In this study, we proposed a PINNs-based method for estimating unknown parameters and
predicting the dynamics of variables of a mean value diesel engine with VGT and EGR, given
the measurement of a few of its variables. Specifically, we know field data of intake manifold
pressure (𝑝𝑖𝑚), exhaust manifold pressure ( 𝑝𝑒𝑚), turbine speed ( 𝜔𝑡) and EGR flow ( 𝑊𝑒𝑔𝑟). We
predicted the dynamics of the system variables and unknown parameters ( 𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,𝜂𝑠𝑐,ℎ𝑡𝑜𝑡and
𝐴𝑣𝑔𝑡𝑚𝑎𝑥 ). The input data for the study are considered from actual engine running conditions
and show good accuracy in predicted results. We also studied the importance of self-adaptive
weight in the accuracy and convergence of results. Furthermore, we also showed how we could
approximate empirical formulas for different quantities using neural networks and train them.
We believe the proposed method could be considered for an online monitoring system of diesel
engines. The field-measured data are collected using individual sensors. Thus, in the event of
sensor failure or erroneous data, the method may give erroneous results. The method also does
not consider a failure of engine components, e.g. leakage in the EGR valve. We considered
the engine model proposed in [24]. Future research may include modelling of failure of engine
components. Since the proposed PINN consider online training, with change in input data,
field measured data or ambient condition, the PINN networks are required to train again. The
accuracy of the results also depends on the size of neural networks and the optimization strategy
(e.g. optimizer, learning rate scheduler) considered. For example, a large neural network
or higher value in learning rate may result in overfitting of predicted results. The activation
function also plays an important role in the accuracy and computational cost [11]. Further
study may include a neural architecture search for optimal network sizes considering different
operational ranges. Future studies may include a more robust and efficient PINN method for
23the problem that can be used with edge systems, including proper transfer learning strategies
to reduce the computation cost. As there is noise in the measured data, the future study in this
regard may also be towards uncertainty quantification of the predicted dynamics and unknown
parameters.
Input data and data generations
The input data Set-I and Set-II are collected from actual engine running conditions. These data
are considered to generate simulated data with different ambient conditions using the Simulink
file [22] accompany [24].
Author Contributions Statement
Kamaljyoti Nath: Conceptualization, Formal analysis, Investigation, Methodology, Software,
Validation, Visualization, Writing – original draft, Writing – review & editing. Xuhui Meng:
Conceptualization, Formal analysis, Investigation, Methodology, Software, Validation, Visual-
ization, Writing – original draft, Writing – review & editing. Daniel J Smith: Conceptual-
ization, Data curation, Project administration, Supervision, Writing – original draft, Writing –
review & editing. George Em Karniadakis: Conceptualization, Funding acquisition, Project
administration, Resources, Supervision, Writing – original draft, Writing – review & editing.
All authors reviewed the manuscript.
Acknowledgement
KN, XM and GEK would like to acknowledge the support by Cummins Inc. USA.
References
[1] Baydin, A. G., Pearlmutter, B. A., Radul, A. A. and Siskind, J. M. [2018], ‘Automatic
differentiation in machine learning: a survey’, Journal of Marchine Learning Research
18, 1–43.
[2] Biao, L., Qing-chun, L., Zhen-hua, J. and Sheng-fang, N. [2009], System identification
of locomotive diesel engines with autoregressive neural network, in‘2009 4th IEEE
Conference on Industrial Electronics and Applications’, pp. 3417–3421.
[3] Cai, S., Mao, Z., Wang, Z., Yin, M. and Karniadakis, G. E. [2022], ‘Physics-informed
neural networks (PINNs) for fluid mechanics: A review’, Acta Mechanica Sinica pp. 1–12.
[4] Chen, Y., Lu, L., Karniadakis, G. E. and Negro, L. D. [2020], ‘Physics-informed neural net-
works for inverse problems in nano-optics and metamaterials’, Opt. Express 28(8), 11618–
11633.
URL: https://opg.optica.org/oe/abstract.cfm?URI=oe-28-8-11618
[5] Cho, J., Nam, S., Yang, H., Yun, S.-B., Hong, Y. and Park, E. [2022], ‘Separable pinn: Mit-
igating the curse of dimensionality in physics-informed neural networks’, arXiv preprint
arXiv:2211.08761 .
24[6] Cuomo, S., Di Cola, V. S., Giampaolo, F., Rozza, G., Raissi, M. and Piccialli, F. [2022],
‘Scientific machine learning through physics–informed neural networks: Where we are
and what’s next’, Journal of Scientific Computing 92(3), 88.
[7] Depina, I., Jain, S., Mar Valsson, S. and Gotovac, H. [2022], ‘Application of physics-
informed neural networks to inverse problems in unsaturated groundwater flow’, Georisk:
Assessment and Management of Risk for Engineered Systems and Geohazards 16(1), 21–
36.
[8] Finesso, R. and Spessa, E. [2014], ‘A real time zero-dimensional diagnostic model for the
calculation of in-cylinder temperatures, hrr and nitrogen oxides in diesel engines’, Energy
Conversion and Management 79, 498–510.
URL: https://www.sciencedirect.com/science/article/pii/S0196890413008212
[9] Gonz ´alez, J. P., Ankobea-Ansah, K., Peng, Q. and Hall, C. M. [2022], ‘On the integration
of physics-based and data-driven models for the prediction of gas exchange processes on
a modern diesel engine’, Proceedings of the Institution of Mechanical Engineers, Part D:
Journal of Automobile Engineering 236(5), 857–871.
URL: https://doi.org/10.1177/09544070211031401
[10] Jagtap, A. D. and Karniadakis, G. E. [2020], ‘Extended physics-informed neural net-
works (XPINNs): A generalized space-time domain decomposition based deep learning
framework for nonlinear partial differential equations’, Communications in Computational
Physics 28, 2002–2041.
[11] Jagtap, A. D. and Karniadakis, G. E. [2023], ‘How important are activation functions in
regression and classification? a survey, performance comparison, and future directions’,
Journal of Machine Learning for Modeling and Computing 4(1).
[12] Jagtap, A. D., Kharazmi, E. and Karniadakis, G. E. [2020], ‘Conservative physics-
informed neural networks on discrete domains for conservation laws: Applications to
forward and inverse problems’, Computer Methods in Applied Mechanics and Engineer-
ing365, 113028.
URL: https://www.sciencedirect.com/science/article/pii/S0045782520302127
[13] Jagtap, A. D., Mao, Z., Adams, N. and Karniadakis, G. E. [2022], ‘Physics-informed
neural networks for inverse problems in supersonic flows’, Journal of Computational
Physics 466, 111402.
URL: https://www.sciencedirect.com/science/article/pii/S0021999122004648
[14] Kharazmi, E., Zhang, Z. and Karniadakis, G. E. [2021], ‘hp-VPINNs variational physics-
informed neural networks with domain decomposition’, Computer Methods in Applied
Mechanics and Engineering 374, 113547.
URL: https://www.sciencedirect.com/science/article/pii/S0045782520307325
[15] Kingma, D. P. and Ba, J. [2014], ‘Adam: A method for stochastic optimization’, arXiv
preprint arXiv:1412.6980 .
[16] Kumar, V., Goswami, S., Smith, D. J. and Karniadakis, G. E. [2023], ‘Real-time prediction
of multiple output states in diesel engines using a deep neural operator framework’, arXiv
preprint arXiv:2304.00567 .
25[17] Lawal, Z. K., Yassin, H., Lai, D. T. C. and Che Idris, A. [2022], ‘Physics-informed neural
network (pinn) evolution and beyond: A systematic literature review and bibliometric
analysis’, Big Data and Cognitive Computing 6(4).
URL: https://www.mdpi.com/2504-2289/6/4/140
[18] Lu, L., Jin, P., Pang, G., Zhang, Z. and Karniadakis, G. E. [2021], ‘Learning nonlinear
operators via deeponet based on the universal approximation theorem of operators’, Nature
machine intelligence 3(3), 218–229.
[19] McClenny, L. and Braga-Neto, U. [2020], ‘Self-adaptive physics-informed neural networks
using a soft attention mechanism’, arXiv preprint arXiv:2009.04544 .
[20] Meng, X., Li, Z., Zhang, D. and Karniadakis, G. E. [2020], ‘PPINN: Parareal physics-
informed neural network for time-dependent PDEs’, Computer Methods in Applied Me-
chanics and Engineering 370, 113250.
URL: https://www.sciencedirect.com/science/article/pii/S0045782520304357
[21] Raissi, M., Perdikaris, P. and Karniadakis, G. [2019], ‘Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear
partial differential equations’, Journal of Computational Physics 378, 686–707.
URL: https://www.sciencedirect.com/science/article/pii/S0021999118307125
[22] Software packages from Vehicular Systems [2010], http://www.fs.isy.liu.se/
Software . [Online].
[23] Tosun, E., Aydin, K. and Bilgili, M. [2016], ‘Comparison of linear regression and artificial
neural network model of a diesel engine fueled with biodiesel-alcohol mixtures’, Alexan-
dria Engineering Journal 55(4), 3081–3089.
URL: https://www.sciencedirect.com/science/article/pii/S1110016816302228
[24] Wahlstr ¨om, J. and Eriksson, L. [2011], ‘Modelling diesel engines with a variable-geometry
turbocharger and exhaust gas recirculation by optimization of model parameters for captur-
ing non-linear system dynamics’, Proceedings of the Institution of Mechanical Engineers,
Part D: Journal of Automobile Engineering 225(7), 960–986.
URL: https://doi.org/10.1177/0954407011398177
26Appendices
Appendix A : Note on neural network and training of PINN
Appendix B : Engine model
Appendix C : Brief discussion on lab test data
Appendix D : Calculation of labelled data for training of the neural network for the empirical
formulae
Appendix E : Detailed loss function for the PINNs model for the engine
Appendix F : Additional Tables
Appendix G : Additional figures (Results for Case 1 and Case 2)
Appendix H : Neural network surrogates for empirical formulae
Appendix A Note on neural network and training of PINN
In this section, we present more details on neural networks, PINN and optimization for inverse
problems. As shown in Fig. 2, the neural network (FFN/DNN) takes time 𝑡as input and
approximates the unknown variable 𝑦. For a DNN with 𝑛−1 hidden layers, the equation for the
neural network can be written as,
y0=𝑡 Input (A.1a)
y𝑖=𝜎(W𝑖y𝑖−1+b𝑖)𝑖∀1≤𝑖≤𝑛−1 Hidden layers (A.1b)
ˆy=y𝑛=W𝑛y𝑛−1+b𝑛−1 Output layer (A.1c)
where, Wandbare the weights matrices and bias vectors of the network, 𝜎(.)is an activation
function, which is considered as hyperbolic tangent function in the present study. The output ˆ 𝑦is
a function of input 𝑡parameterized by the weights Wand biases b. We can tune the parameters
of the network to predict a large number of snapshots 𝑦by minimizing a loss function using an
appropriate optimization technique.
In the case of a data-driven model of neural networks, the loss function is generally consid-
ered as the mean square error (MSE) between the predicted ( ˆ 𝑦) and the exact value ( 𝑦). On the
other hand, in the case of PINN, the neural network output is made to satisfy the differential
equation and the initial/boundary conditions. The derivatives of the equation are generally eval-
uated using automatic differentiation. As discussed in section 2.1, the loss function is a weighted
sum of physics loss which is MSE of residual and boundary/initial loss, which is MSE between
predicted and exact boundary/initial value. The optimal parameters (weights and biases) of the
network are obtained using an optimization method such as Adam or L-BFGS-B. In the case
of an inverse problem using PINN where the objective is to predict unknown parameters ( Λ)
along with the variable ( 𝑦). The unknown parameters are also optimized along with the network
parameters. Thus, the trainable parameters are weights, biases and the unknown parameters
(θ={W,b,Λ}). Also, an additional loss function is added, which is data loss between the
predicted and the known value of 𝑦. Furthermore, in the present study we have considered self
27adaptive wights [19] for the loss function. The loss function is maximized with respect to the
self self adaptive weights ( 𝜆). Thus, the optimization process may be written as,
min
θmax
λL(θ,λ) (A.2)
Consider the updates of a gradient descent/ascent approach to this problem
θ=θ−𝑙𝑟𝜃∇𝜃L(θ,λ) (A.3a)
λ=λ+𝑙𝑟𝜆∇𝜆L(θ,λ) (A.3b)
where,𝑙𝑟𝜃and𝑙𝑟𝜆are the learning rate associated with θandλ.
In the present study, the parameters are optimized using Adam and L-BFGS-B optimizer in
Tensorflow-1 (with single precision floating point). Further, self-adaptive weights are optimized
only during the process of Adam optimization up to fixed epoch as discussed in section 3
Appendix B Engine model
As discussed in section 1.1, we consider a mean value engine model proposed by Wahlstr ¨om
and Eriksson [24] in our present study. The engine has eight states, and we have considered six
in the present study. These are,
x={𝑝𝑖𝑚, 𝑝𝑒𝑚, 𝜔𝑡,˜𝑢𝑒𝑔𝑟1,˜𝑢𝑒𝑔𝑟2,˜𝑢𝑣𝑔𝑡} (B.1)
where𝑝𝑖𝑚and𝑝𝑒𝑚are the intake and exhaust manifold pressure, respectively, 𝜔𝑡is the turbo
speed. ˜𝑢𝑒𝑔𝑟1and ˜𝑢𝑒𝑔𝑟2are the two states for the EGR actuator dynamics, and ˜ 𝑢𝑣𝑔𝑡represents
the VGT actuator dynamics. The control inputs for the engine are u={𝑢𝛿, 𝑢𝑒𝑔𝑟, 𝑢𝑣𝑔𝑡}and the
engine speed is 𝑛𝑒. Where𝑢𝛿is the mass of injected fuel, 𝑢𝑒𝑔𝑟and𝑢𝑣𝑔𝑡are the EGR and VGT
valve positions, respectively. The mean value engine model is then expressed as
¤x=𝑓(x,u,𝑛𝑒). (B.2)
The engine model consists of 6 parts intake and exhaust manifold, the cylinder, the turbine,
EGR valve system, and the compressor system. A schematic diagram of the engine is shown
in Fig. 1. In this section, we briefly discuss the equation required for the present study and
these are taken from [24]. For detail of the engine model, the interested reader may refer to
Wahlstr ¨om and Eriksson [24].
B.1 Manifold pressures
The pressure at the intake manifold ( 𝑝𝑖𝑚) is modelled using a first-order differential equation as,
𝑑
𝑑𝑡𝑝𝑖𝑚=𝑅𝑎𝑇𝑖𝑚
𝑉𝑖𝑚 𝑊𝑐+𝑊𝑒𝑔𝑟−𝑊𝑒𝑖(B.3)
where𝑇𝑖𝑚and𝑉𝑖𝑚are the temperature and volume of the intake manifold, respectively, and
both are assumed to be constant, 𝑊𝑐,𝑊𝑒𝑔𝑟and𝑊𝑒𝑖are the compressor mass flow, EGR mass
flow and total mass flow, respectively. The ideal gas constant and specific heat capacity of the
air are𝑅𝑎and𝛾𝑎, respectively.
Similarly, the exhaust manifold pressure 𝑝𝑒𝑚is modelled as,
𝑑
𝑑𝑡𝑝𝑒𝑚=𝑅𝑒𝑇𝑒𝑚
𝑉𝑒𝑚 𝑊𝑒𝑜−𝑊𝑡−𝑊𝑒𝑔𝑟(B.4)
28where𝑅𝑒is the ideal gas constant of the exhaust gas with specific heat capacity 𝛾𝑒,𝑇𝑒𝑚and
𝑉𝑒𝑚are the exhaust manifold temperature, and volume, 𝑊𝑒𝑜,𝑊𝑡are the mass flow out from the
cylinder and turbine mass flow, respectively.
B.2 Cylinder
The total mass flow from the intake manifold to the cylinder 𝑊𝑒𝑖, and the total mass flow out of
the cylinder𝑊𝑒𝑜are modelled as,
𝑊𝑒𝑖=𝜂𝑣𝑜𝑙𝑝𝑖𝑚𝑛𝑒𝑉𝑑
120𝑅𝑎𝑇𝑖𝑚(B.5)
𝑊𝑒𝑜=𝑊𝑓+𝑊𝑒𝑖 (B.6)
where𝑊𝑓is the fuel mass flow into the cylinder is given by,
𝑊𝑓=10−6
120𝑢𝛿𝑛𝑒𝑛𝑐𝑦𝑙, (B.7)
𝑉𝑑,𝑛𝑒and𝑛𝑐𝑦𝑙are the displaced volume, engine speed and the number of cylinders, respectively.
The volumetric efficiency, 𝜂𝑣𝑜𝑙of the cylinder may be modelled as
𝜂𝑣𝑜𝑙=𝑐𝑣𝑜𝑙1√𝑝𝑖𝑚+𝑐𝑣𝑜𝑙2√𝑛𝑒+𝑐𝑣𝑜𝑙3 (B.8)
where𝑐𝑣𝑜𝑙1,𝑐𝑣𝑜𝑙2and𝑐𝑣𝑜𝑙3are constant.
The temperature at cylinder out based upon ideal-gas Seliger cycle (or limited pressure
cycle) and given as,
𝑇𝑒=𝜂𝑠𝑐Π1−1/𝛾𝑎𝑒𝑟1−𝛾𝑎𝑐𝑥1/𝛾𝑎−1
𝑝
𝑞𝑖𝑛1−𝑥𝑐𝑣
𝑐𝑝𝑎+𝑥𝑐𝑣
𝑐𝑉𝑎
+𝑇1𝑟𝛾𝑎−1
𝑐
(B.9)
where the pressure ratio ( Π𝑒) over the cylinder is ratio of pressure at exhaust ( 𝑝𝑒𝑚) and intake
(𝑝𝑖𝑚),
Π𝑒=𝑝𝑒𝑚
𝑝𝑖𝑚, (B.10)
the fuel consumed during constant-volume combustion is 𝑥𝑐𝑣and fuel consumed during constant
pressure combustion is 1 −𝑥𝑐𝑣,𝜂𝑠𝑐and𝑟𝑐are compensation factor for non-ideal cycles and
compression ratio. The temperature, 𝑇1when the inlet valve closes and after the intake stroke
and mixing is given by,
𝑇1=𝑥𝑟𝑇𝑒+(1−𝑥𝑟)𝑇𝑖𝑚 (B.11)
The residual gas fraction ( 𝑥𝑟) is model as
𝑥𝑟=Π1/𝛾𝑎𝑒𝑥−1/𝛾𝑎𝑝
𝑟𝑐𝑥𝑣(B.12)
The pressure ratio ( 𝑥𝑝) and the volume ratio ( 𝑥𝑣) in the Seliger cycle between point 3 (after
combustion) and point 2 (before combustion) are modelled as,
𝑥𝑝=𝑝3
𝑝2=1+𝑞𝑖𝑛𝑥𝑐𝑣
𝑐𝑉𝑎𝑇1𝑟𝛾𝑎−1
𝑐(B.13)
𝑥𝑣=𝑣3
𝑣2=1+𝑞𝑖𝑛(1−𝑥𝑐𝑣)
𝑐𝑝𝑎h
(𝑞𝑖𝑛𝑥𝑐𝑣/𝑐𝑉𝑎)+𝑇1𝑟𝛾𝑎−1
𝑐i (B.14)
29where the specific energy constant of the charge is modelled as
𝑞𝑖𝑛=𝑊𝑓𝑞𝐻𝑉
𝑊𝑒𝑖+𝑊𝑓(1−𝑥𝑟) (B.15)
The temperature at cylinder out modelled in Eq. (B.9) is the temperature at the cylinder exit.
However, it is not the same as the temperature as the exhaust manifold. This is due to the heat
loss in the exhaust pipes between the cylinder and the exhaust manifold. The exhaust manifold
temperature ( 𝑇𝑒𝑚) is given as
𝑇𝑒𝑚=𝑇𝑎𝑚𝑏+(𝑇𝑒−𝑇𝑎𝑚𝑏)exp−ℎ𝑡𝑜𝑡𝜋𝑑𝑝𝑖𝑝𝑒𝑙𝑝𝑖𝑝𝑒𝑛𝑝𝑖𝑝𝑒
𝑊𝑒𝑜𝑐𝑝𝑒
(B.16)
where𝑇𝑎𝑚𝑏is the ambient temperature, 𝑑𝑝𝑖𝑝𝑒,𝑙𝑝𝑖𝑝𝑒and𝑛𝑝𝑖𝑝𝑒are the pipe diameter, pipe length
and the number of pipes, respectively.
B.3 EGR valve
The actuator dynamics of the EGR-valve are modelled as,
𝑑
𝑑𝑡˜𝑢𝑒𝑔𝑟1=1
𝜏𝑒𝑔𝑟1
𝑢𝑒𝑔𝑟(𝑡−𝜏𝑑𝑒𝑔𝑟)−˜𝑢𝑒𝑔𝑟1
(B.17)
𝑑
𝑑𝑡˜𝑢𝑒𝑔𝑟2=1
𝜏𝑒𝑔𝑟2
𝑢𝑒𝑔𝑟(𝑡−𝜏𝑑𝑒𝑔𝑟)−˜𝑢𝑒𝑔𝑟2
(B.18)
˜𝑢𝑒𝑔𝑟=𝐾𝑒𝑔𝑟˜𝑢𝑒𝑔𝑟1−(𝐾𝑒𝑔𝑟−1)˜𝑢𝑒𝑔𝑟2 (B.19)
where𝜏𝑒𝑔𝑟1,𝜏𝑒𝑔𝑟2are time constants, 𝜏𝑑𝑒𝑔𝑟is the time delay and 𝐾𝑒𝑔𝑟is a constant that affect
the overshoot.
We model the mass flow through the EGR valve through the restriction ( 𝑝𝑒𝑚<𝑝𝑖𝑚) as,
𝑊𝑒𝑔𝑟=𝐴𝑒𝑔𝑟𝑝𝑖𝑚Ψ𝑒𝑔𝑟√𝑇𝑒𝑚𝑅𝑒(B.20)
whereΨ𝑒𝑔𝑟is a parabolic function
Ψ𝑒𝑔𝑟=1−1−Π𝑒𝑔𝑟
1−Π𝑒𝑔𝑟𝑜𝑝𝑡−12
(B.21)
The effective area is modelled as,
𝐴𝑒𝑔𝑟=𝐴𝑒𝑔𝑟𝑚𝑎𝑥𝑓𝑒𝑔𝑟(˜𝑢𝑒𝑔𝑟) (B.22)
When the sonic conditions are reached (flow is choked) in the throat and when no backflow can
occur (1<𝑝𝑖𝑚/𝑝𝑒𝑚), the pressure ratio Π𝑒𝑔𝑟over the valve is limited and modelled as,
Π𝑒𝑔𝑟= 
Π𝑒𝑔𝑟𝑜𝑝𝑡 if𝑝𝑖𝑚
𝑝𝑒𝑚<Π𝑒𝑔𝑟𝑜𝑝𝑡
𝑝𝑖𝑚
𝑝𝑒𝑚ifΠ𝑒𝑔𝑟𝑜𝑝𝑡≤𝑝𝑖𝑚
𝑝𝑒𝑚≤1
1 if 1 <𝑝𝑖𝑚
𝑝𝑒𝑚(B.23)
30𝐴𝑒𝑔𝑟𝑚𝑎𝑥 ,Π𝑒𝑔𝑟𝑜𝑝𝑡 are constant and 𝑓𝑒𝑔𝑟(˜𝑢𝑒𝑔𝑟)is modelled as a polynomial function,
𝑓𝑒𝑔𝑟(˜𝑢𝑒𝑔𝑟)= 
𝑐𝑒𝑔𝑟1˜𝑢2
𝑒𝑔𝑟+𝑐𝑒𝑔𝑟2˜𝑢𝑒𝑔𝑟+𝑐𝑒𝑔𝑟3if ˜𝑢𝑒𝑔𝑟≤−𝑐𝑒𝑔𝑟2
2𝑐𝑒𝑔𝑟1
𝑐𝑒𝑔𝑟3−𝑐2
𝑒𝑔𝑟2
4𝑐𝑒𝑔𝑟1if ˜𝑢𝑒𝑔𝑟>−𝑐𝑒𝑔𝑟2
2𝑐𝑒𝑔𝑟1(B.24)
where𝑐𝑒𝑔𝑟1,𝑐𝑒𝑔𝑟2and𝑐𝑒𝑔𝑟3are constant.
B.4 Turbocharger
The turbo speed, 𝜔𝑡is modelled as a first-order differential model as,
𝑑
𝑑𝑡𝜔𝑡=𝑃𝑡𝜂𝑚−𝑃𝑐
𝐽𝑡𝜔𝑡(B.25)
where𝐽𝑡is the inertia, 𝑃𝑡and𝑃𝑐are the power delivered by the turbine and power required to
drive the compressor, respectively, 𝜂𝑚is the mechanical efficiency of the turbocharger.
The VGT actuator system is modelled as a first-order system
𝑑˜𝑢𝑣𝑔𝑡
𝑑𝑡=1
𝜏𝑣𝑔𝑡
𝑢𝑣𝑔𝑡(𝑡−𝜏𝑑𝑣𝑔𝑡)−˜𝑢𝑣𝑔𝑡
(B.26)
where𝜏𝑣𝑔𝑡and𝜏𝑑𝑣𝑔𝑡are the time constant and time delay respectively.
The turbine mass flow ( 𝑊𝑡) is calculated using
𝑊𝑡=𝐴𝑣𝑔𝑡𝑚𝑎𝑥𝑝𝑒𝑚𝑓Π𝑡(Π𝑡)𝑓𝑣𝑔𝑡(˜𝑢𝑣𝑔𝑡))
√𝑇𝑒𝑚𝑅𝑒(B.27)
𝐴𝑣𝑔𝑡𝑚𝑎𝑥 is the maximum area in the turbine that the gas flow through.
𝑓Π𝑡(Π𝑡)=√︃
1−Π𝐾𝑡
𝑡 (B.28)
where𝐾𝑡a constant and Π𝑡=𝑝𝑒𝑠/𝑝𝑒𝑚.𝑝𝑒𝑠>𝑝𝑎𝑚𝑏if there is a restriction like an after-treatment
system. However, in the model we consider, there is no restriction after the turbine, thus
Π𝑡=𝑝𝑎𝑚𝑏
𝑝𝑒𝑚(B.29)
Further, with the increase in VGT control signal ( 𝑢𝑣𝑔𝑡), the effective area increases and thus
also increases the flow. The effective area of the VGT 𝑓𝑣𝑔𝑡(˜𝑢𝑣𝑔𝑡)is modelled as an ellipse
𝑓𝑣𝑔𝑡(˜𝑢𝑣𝑔𝑡)−𝑐𝑓2
𝑐𝑓12
+˜𝑢𝑣𝑔𝑡−𝑐𝑣𝑔𝑡2
𝑐𝑣𝑔𝑡12
=1 (B.30)
which is
𝑓𝑣𝑔𝑡(˜𝑢𝑣𝑔𝑡)=𝑐𝑓2+𝑐𝑓1vut
max 
0,1−˜𝑢𝑣𝑔𝑡−𝑐𝑣𝑔𝑡2
𝑐𝑣𝑔𝑡12!
(B.31)
The power delivered by the turbine, 𝑃𝑡and the mechanical efficiency of the turbocharger 𝜂𝑚
are modelled as,
𝑃𝑡𝜂𝑚=𝜂𝑡𝑚𝑊𝑡𝑐𝑝𝑒𝑇𝑒𝑚
1−Π1−1/𝛾𝑒
𝑡
(B.32)
31𝜂𝑡𝑚=𝜂𝑡𝑚,𝑚𝑎𝑥−𝑐𝑚(𝐵𝑆𝑅−𝐵𝑆𝑅𝑜𝑝𝑡)2(B.33)
where, the blade speed ratio (BSR) is defined as the ratio of the turbine blade tip speed to the
speed which a gas reaches when expanded entropically at the given pressure ratio Π𝑡.
𝐵𝑆𝑅=𝑅𝑡𝜔𝑡√︃
2𝑐𝑝𝑒𝑇𝑒𝑚(1−Π1−1/𝛾𝑒
𝑡)(B.34)
where𝑅𝑡is the turbine blade radius, and
𝑐𝑚=𝑐𝑚1[𝑚𝑎𝑥(0,𝜔𝑡−𝑐𝑚2]𝑐𝑚3(B.35)
B.5 Compressor
The compressor model consists of two models: the compressor efficiency model and the
compressor mass flow model. The compressor efficiency is defined as the ratio of the power
from the isentropic process ( 𝑃𝑐,𝑠) to the compressor power ( 𝑃𝑐)
𝜂𝑐=𝑃𝑐,𝑠
𝑃𝑐=𝑇𝑎𝑚𝑏(Π1−1/𝛾𝑎𝑐−1)
𝑇𝑐−𝑇𝑎𝑚𝑏(B.36)
where𝑇𝑐is the temperature after the compressor, and the pressure ratio is given by,
Π𝑐=𝑝𝑖𝑚
𝑝𝑎𝑚𝑏(B.37)
The power from the isentropic process is given as,
𝑃𝑐,𝑠=𝑊𝑐𝑐𝑝𝑎𝑇𝑎𝑚𝑏(Π1−1/𝛾𝑎𝑐−1) (B.38)
where𝑊𝑐is the compressor mass flow and 𝑐𝑝𝑎is a constant. Thus, the compressor power can
be modelled from Eq. B.36 and Eq. B.38 as
𝑃𝑐=𝑃𝑐,𝑠
𝜂𝑐=𝑊𝑐𝑐𝑝𝑎𝑇𝑎𝑚𝑏
𝜂𝑐(Π1−1/𝛾𝑎−1
𝑐) (B.39)
𝜂𝑐is modelled as an ellipses, which depends on the pressure ratio ( Π𝑐) and compressor mass
flow (𝑊𝑐),
𝜂𝑐=𝜂𝑐𝑚𝑎𝑥−X𝑇Q𝑐X (B.40)
whereXis a vector and given as,
X=𝑊𝑐−𝑊𝑐𝑜𝑝𝑡
𝜋𝑐−𝜋𝑐𝑜𝑝𝑡
(B.41)
where𝑊𝑐𝑜𝑝𝑡and𝜋𝑐𝑜𝑝𝑡are the optimum values of 𝑊𝑐and𝜋𝑐respectively. 𝜋𝑐is a non linear
transformation of Π𝑐as
𝜋𝑐=(Π𝑐−1)𝑐𝜋(B.42)
andQ𝑐is a semi-definite matrix
Q𝑐=𝑎1𝑎3
𝑎3𝑎2
(B.43)
32The model for compressor mass flow, 𝑊𝑐is modelled using two non-dimensional variables:
energy transfer coefficient ( Ψ𝑐) and volumetric flow coefficient ( Φ𝑐). The energy transfer
coefficient is defined as,
Ψ𝑐=2𝑐𝑝𝑎𝑇𝑎𝑚𝑏(Π1−1/𝛾𝑎𝑐−1)
𝑅2𝑐𝜔2
𝑡(B.44)
where𝑅𝑐is the compressor blade ratio. The volumetric flow coefficient is defined as,
Φ𝑐=𝑊𝑐/𝜌𝑎𝑚𝑏
𝜋𝑅3𝑐𝜔𝑡=𝑅𝑎𝑇𝑎𝑚𝑏
𝑝𝑎𝑚𝑏𝜋𝑅3𝑐𝜔𝑡𝑊𝑐 (B.45)
The energy transfer coefficient ( Ψ𝑐) and volumetric flow coefficient ( Φ𝑐) can be described by a
part of an ellipse,
𝑐Ψ1(𝜔𝑡)(Ψ𝑐−𝑐Ψ2)2+𝑐Φ1(𝜔𝑡)(Φ𝑐−𝑐Φ2)2=1 (B.46)
where𝑐Ψ1and𝑐Φ1are function of turbine speed ( 𝜔𝑡) and modelled as a second order polynomial
as,
𝑐Ψ1(𝜔𝑡)=𝑐𝜔Ψ1𝜔2
𝑡+𝑐𝜔Ψ2𝜔𝑡+𝑐𝜔Ψ3 (B.47)
𝑐Φ1(𝜔𝑡)=𝑐𝜔Φ1𝜔2
𝑡+𝑐𝜔Φ2𝜔𝑡+𝑐𝜔Φ3 (B.48)
Solving Eq. B.46 for Φ𝑐and Eq. B.45 for 𝑊𝑐, the compressor mass flow is given as,
𝑊𝑐=𝑝𝑎𝑚𝑏𝜋𝑅3
𝑐𝜔𝑡
𝑅𝑎𝑇𝑎𝑚𝑏Φ𝑐 (B.49)
Φ𝑐=vut
max 
0,1−𝑐𝜓1(Ψ𝑐−𝑐Ψ2)2
𝑐Φ1!
+𝑐Φ2 (B.50)
where𝑐𝜔Ψ1,𝑐𝜔Ψ2,𝑐𝜔Ψ3,𝑐𝜔Φ1,𝑐𝜔Φ2,𝑐𝜔Φ3,𝑐Φ2and𝑐Ψ2are constant.
Appendix C Brief discussion on lab test data
The engine model considered in this study uses empirical formulae. These equations are engine
specific and may not be appropriate for the present study. As discussed in section 2.1.1, we
consider surrogate neural networks and uses lab test data to train these model. In this section,
we briefly discuss lab test data.
Practical limitations exist when instrumenting engines for testing. Some physical phenomena
are easily measurable, while others are not. When conducting modelling efforts, one must
consider the necessary measurements for model tuning to ensure the experimental setup is
adequate. The data collection capabilities can also impact loss function weights based on data
trustworthiness, as well as noise values applied in the analysis. There are a few signals that
pose particular challenges in cost-effective and simple measurement in part due to the high
temperature, pressure, dynamics and flow constituents in some areas.
Often as areas closer to the cylinder are considered, measurements become increasingly
difficult. For example, exhaust port flow, 𝑊𝑒𝑜is difficult to measure directly, as the gas is very
hot and reactive. In-cylinder measurements are limited by high pressure and temperatures,
requiring specialized equipment. Even measuring charge flows directly can be challenging.
Because of these limitations, care must be taken in the experimental methods and analysis design
to ensure enough data is gathered to be able to observe and identify the system. Sometimes
33steady state characterizations are used to obtain a static characterization. Consider volumetric
efficiency as an example: because measuring flow directly into or out of the cylinder is difficult,
a fresh air flow measurement combined with an EGR flow measurement can be used to estimate
charge flow to enable the calculation of volumetric efficiency. However, any intake, EGR, or
Exhaust leak impacts this measurement, as does the tolerance stack up of both measurements.
Appendix D Calculation of labelled data for training of the
neural network for the empirical formulae
We approximate the empirical formula using surrogate neural networks and are discussed in
section 2.1.1. The lab test data required for calculating each of these quantities are shown in
Table 3. In this section, we discuss the calculation of labelled data from lab-test data. The
functional approximation of the empirical formulae is independent of time; thus, static data
may be considered for the calculation of labelled data. However, in the case of calculation of
labelled for𝜂𝑡𝑚, the differential equation Eq. (B.25) is considered. Thus, we consider dynamic
data for this calculation.
We approximate the volumetric efficiency using a surrogate neural network ( N(𝑃)
1(:,θ𝑃
1).
The inputs to the network are intake manifold pressure ( 𝑝𝑖𝑚) and engine speed ( 𝑛𝑒) and trained
using labelled data of 𝜂𝑣𝑜𝑙. The labelled 𝜂𝑣𝑜𝑙are calculated from measurement of 𝑊𝑒𝑖using Eq.
(B.5),
𝜂𝑣𝑜𝑙=120𝑅𝑎𝑇𝑖𝑚𝑊𝑒𝑖
𝑝𝑖𝑚𝑛𝑒𝑉𝑑(D.1)
The effective area ratio function for EGR valve is approximated using a surrogate neural
network (N(𝑃)
2(:,θ𝑃
2). The input to the network is ˜ 𝑢𝑒𝑔𝑟and trained using labelled data of 𝑓𝑒𝑔𝑟.
The labelled 𝑓𝑒𝑔𝑟are calculated from the measurement of 𝑊𝑒𝑔𝑟using Eqs. (B.20) and (B.22),
𝐴𝑒𝑔𝑟=√𝑇𝑒𝑚𝑅𝑒
𝑝𝑖𝑚Ψ𝑒𝑔𝑟𝑊𝑒𝑔𝑟 (D.2)
𝑓𝑒𝑔𝑟=𝐴𝑒𝑔𝑟
𝐴𝑒𝑔𝑟𝑚𝑎𝑥(D.3)
It is important to node that the value of Ψ𝑒𝑔𝑟varies from 0 to 1 (Eq. (B.21)), thus in the
calculation of 𝐴𝑒𝑔𝑟, a situation may occurs where division by 0. This situation occurs when
𝑝𝑒𝑚<𝑝𝑖𝑚(Eq. (B.23)). In order to avoid this, labelled data are calculated only for Ψ𝑒𝑔𝑟>10−15.
The neural network approximating ( N(𝑃)
3(:,θ𝑃
3) for𝐹𝑣𝑔𝑡,Π=𝑓𝑣𝑔𝑡×𝑓Π𝑡is trained using la-
belled data which are calculated from the measurement of turbine mass flow ( 𝑊𝑡) using Eq.
(B.27),
𝐹𝑣𝑔𝑡,Π𝑡(˜𝑢𝑣𝑔𝑡,Π𝑡)=𝑓𝑣𝑔𝑡(˜𝑢𝑣𝑔𝑡)×𝑓Π𝑡(Π𝑡)=𝑊𝑡√𝑇𝑒𝑚𝑅𝑒
𝐴𝑣𝑔𝑡𝑚𝑎𝑥𝑝𝑒𝑚(D.4)
The training for the neural network ( N(𝑃)
4(:,θ𝑃
4) for the surrogate model of turbine mechan-
ical efficiency ( 𝜂𝑡𝑚) is done using labelled 𝜂𝑡𝑚which is calculated from the measurement of 𝜔𝑡
using Eqs. (B.25) and (B.32)
𝑃𝑡𝜂𝑚=𝑃𝑐+𝐽𝑡𝜔𝑡𝑑𝜔𝑡
𝑑𝑡(D.5)
The compressor power ( 𝑃𝑐) is calculated as,
𝑃𝑐=𝑊𝑐𝑐𝑝𝑎(𝑇𝑐−𝑇𝑎𝑚𝑏) (D.6)
34In the present study, we consider a five-point method to approximate the derivative present in
Eq. (D.5).
𝑓(1)(𝑥)≈−𝑓(𝑥+2ℎ)+8𝑓(𝑥+ℎ)−8𝑓(𝑥−ℎ)+𝑓(𝑥−2ℎ)
12ℎ(D.7)
Once𝑃𝑡𝜂𝑚calculated from Eq. (D.5), the labelled 𝜂𝑡𝑚are calculated using Eq. (B.32)
𝜂𝑡𝑚=𝑃𝑡𝜂𝑚
𝑊𝑡𝑐𝑝𝑒𝑇𝑒𝑚
1−Π1−1/𝛾𝑒
𝑡 (D.8)
The values of 𝜂𝑡𝑚are restricted to maximum value 𝜂𝑡𝑚,𝑚𝑎𝑥
𝜂𝑡𝑚=min(𝜂𝑡𝑚,𝑚𝑎𝑥,𝜂𝑡𝑚), 𝜂𝑡𝑚,𝑚𝑎𝑥=0.8180 (D.9)
The labelled data for the training of neural network ( N(𝑃)
5(:,θ𝑃
5) for compressor efficiency
(𝜂𝑐) is calculated using Eqs. (B.39) and (B.36)
𝜂𝑐=𝑃𝑐,𝑠
𝑃𝑐(D.10a)
=𝑊𝑐𝑐𝑝𝑎𝑇𝑎𝑚𝑏
Π1−1/𝛾𝑎𝑐−1
𝑊𝑐𝑐𝑝𝑎(𝑇𝑐−𝑇𝑎𝑚𝑏)(D.10b)
=𝑇𝑎𝑚𝑏
Π1−1/𝛾𝑎𝑐−1
𝑇𝑐−𝑇𝑎𝑚𝑏(D.10c)
To avoid any division by 0, the value of 𝑇𝑐−𝑇𝑎𝑚𝑏less than 10−6are considered as 10−6. Further,
the value of𝜂𝑐is clipped between 0.2 and 𝜂𝑐𝑚𝑎𝑥
𝜂𝑐=max(0.2,𝜂𝑐) (D.11a)
𝜂𝑐=min(𝜂𝑐𝑚𝑎𝑥,𝜂𝑐), 𝜂𝑐𝑚𝑎𝑥=0.7364 (D.11b)
The training for the neural network ( N(𝑃)
6(:,θ𝑃
6) for surrogate model of volumetric flow
coefficient (Φ𝑐) is done using labelled data which are calculated from the measurement of
compressor mass flow ( 𝑊𝑐) Eq. (B.45)
Φ𝑐=𝑅𝑎𝑇𝑎𝑚𝑏
𝑝𝑎𝑚𝑏𝜋𝑅3𝑐𝜔𝑡𝑊𝑐 (D.12)
Appendix E Detail loss function for the PINNs model for the
engine
We consider the following loss function for Case 1 to Case 4, which have self-adaptive weights
in the loss function,
L(θ,𝜦,λ𝑝𝑖𝑚,λ𝑝𝑒𝑚,λ𝜔𝑡,λ𝑊𝑒𝑔𝑟,𝜆𝑇1)=L𝑝𝑖𝑚+L𝑝𝑒𝑚+L𝜔𝑡+L𝑢𝑒𝑔𝑟1+
L𝑢𝑒𝑔𝑟2+L𝑢𝑣𝑔𝑡+10×L𝑥𝑟+𝜆𝑇1×L𝑇1+
L𝑖𝑛𝑖
𝑝𝑖𝑚+L𝑖𝑛𝑖
𝑝𝑒𝑚+L𝑖𝑛𝑖
𝜔𝑡+L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟1+
L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟2+L𝑖𝑛𝑖
˜𝑢𝑣𝑔𝑡+L𝑖𝑛𝑖
𝑥𝑟+100×L𝑖𝑛𝑖
𝑇1+
L𝑑𝑎𝑡𝑎
𝑝𝑖𝑚(λ𝑝𝑖𝑚)+L𝑑𝑎𝑡𝑎
𝑝𝑒𝑚(λ𝑝𝑒𝑚)+
L𝑑𝑎𝑡𝑎
𝜔𝑡(λ𝜔𝑡)+L𝑑𝑎𝑡𝑎
𝑊𝑒𝑔𝑟(λ𝑊𝑒𝑔𝑟),(E.1)
35In the Case 5 where we have not considered self-adaptive weights, the loss function is given as
L(θ,𝜦)=L𝑝𝑖𝑚+L𝑝𝑒𝑚+L𝜔𝑡+L𝑢𝑒𝑔𝑟1+
L𝑢𝑒𝑔𝑟2+L𝑢𝑣𝑔𝑡+10×L𝑥𝑟+103×L𝑇1+
L𝑖𝑛𝑖
𝑝𝑖𝑚+L𝑖𝑛𝑖
𝑝𝑒𝑚+L𝑖𝑛𝑖
𝜔𝑡+L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟1+
L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟2+L𝑖𝑛𝑖
˜𝑢𝑣𝑔𝑡+L𝑖𝑛𝑖
𝑥𝑟+100×L𝑖𝑛𝑖
𝑇1+
103×L𝑑𝑎𝑡𝑎
𝑝𝑖𝑚+103×L𝑑𝑎𝑡𝑎
𝑝𝑒𝑚+
103×L𝑑𝑎𝑡𝑎
𝜔𝑡+103×L𝑑𝑎𝑡𝑎
𝑊𝑒𝑔𝑟,(E.2)
where θ=(θ1,...,θ6)are the hyperparameters of all NNs in PINNs, which include both weights
and biases, 𝜦are the unknown parameters of the equations which need to be found out. λ𝑝𝑖𝑚,
λ𝑝𝑒𝑚,λ𝜔𝑡andλ𝑊𝑒𝑔𝑟are the self-adaptive weight [19] for the data loss in 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟
respectively. 𝜆𝑇1is the self-adaptive weight for physics loss in 𝑇1.L𝑝𝑖𝑚(θ),L𝑝𝑒𝑚(θ),L𝜔𝑡,
L˜𝑢𝑒𝑔𝑟1,L˜𝑢𝑒𝑔𝑟2, andL˜𝑢𝑣𝑔𝑡are the physics loss corresponding to the differential equations of the
states of the diesel engine 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡˜𝑢𝑒𝑔𝑟1, ˜𝑢𝑒𝑔𝑟2and ˜𝑢𝑣𝑔𝑡respectively.L𝑥𝑟andL𝑇1are the
physics loss correspond to 𝑥𝑟and𝑇1respectively.
L𝑝𝑖𝑚=1
𝑛𝑛∑︁
𝑖=1𝑟(𝑝𝑖𝑚)2=1
𝑛𝑛∑︁
𝑖=1𝑑𝑝𝑖𝑚
𝑑𝑡−𝑅𝑎𝑇𝑖𝑚
𝑉𝑖𝑚 𝑊𝑐+𝑊𝑒𝑔𝑟−𝑊𝑒𝑖2
(E.3a)
L𝑝𝑒𝑚=1
𝑛𝑛∑︁
𝑖=1𝑟(𝑝𝑒𝑚)2=1
𝑛𝑛∑︁
𝑖=1𝑑𝑝𝑒𝑚
𝑑𝑡−𝑅𝑒𝑇𝑒𝑚
𝑉𝑒𝑚 𝑊𝑒𝑜−𝑊𝑡−𝑊𝑒𝑔𝑟2
(E.3b)
L𝜔𝑡=1
𝑛𝑛∑︁
𝑖=1𝑟(𝜔𝑡)2=1
𝑛𝑛∑︁
𝑖=1𝑑𝜔𝑡
𝑑𝑡−𝑃𝑡𝜂𝑚−𝑃𝑐
𝐽𝑡𝜔𝑡2
(E.3c)
L˜𝑢𝑒𝑔𝑟1=1
𝑛𝑛∑︁
𝑖=1𝑟(˜𝑢𝑒𝑔𝑟1)2=1
𝑛𝑛∑︁
𝑖=1𝑑˜𝑢𝑒𝑔𝑟1
𝑑𝑡−1
𝜏𝑒𝑔𝑟1
𝑢𝑒𝑔𝑟(𝑡−𝜏𝑑𝑒𝑔𝑟)−˜𝑢𝑒𝑔𝑟12
(E.3d)
L˜𝑢𝑒𝑔𝑟2=1
𝑛𝑛∑︁
𝑖=1𝑟(˜𝑢𝑒𝑔𝑟2)2=1
𝑛𝑛∑︁
𝑖=1𝑑˜𝑢𝑒𝑔𝑟2
𝑑𝑡−1
𝜏𝑒𝑔𝑟2
𝑢𝑒𝑔𝑟(𝑡−𝜏𝑑𝑒𝑔𝑟)−˜𝑢𝑒𝑔𝑟22
(E.3e)
L˜𝑢𝑣𝑔𝑡=1
𝑛𝑛∑︁
𝑖=1𝑟(˜𝑢𝑣𝑔𝑡)2=1
𝑛𝑛∑︁
𝑖=1𝑑˜𝑢𝑣𝑔𝑡
𝑑𝑡−1
𝜏𝑣𝑔𝑡
𝑢𝑣𝑔𝑡(𝑡−𝜏𝑑𝑣𝑔𝑡)−˜𝑢𝑣𝑔𝑡2
(E.3f)
L𝑥𝑟=1
𝑛𝑛∑︁
𝑖=1𝑟(𝑥𝑟)2=1
𝑛𝑛∑︁
𝑖=1 
𝑥𝑟−Π1/𝛾𝑎𝑒𝑥−1/𝛾𝑎𝑝
𝑟𝑐𝑥𝑣!2
(E.3g)
L𝑇1=1
𝑛𝑛∑︁
𝑖=1𝑟(𝑇1)2=1
𝑛𝑛∑︁
𝑖=1(𝑇1−(𝑥𝑟𝑇𝑒+(1−𝑥𝑟)𝑇𝑖𝑚))2(E.3h)
where𝑛and𝑟(.)are the number of residual points and residual, respectively.
In the Case 1 to Case 4, L𝑑𝑎𝑡𝑎
𝑝𝑖𝑚(λ𝑝𝑖𝑚,L𝑑𝑎𝑡𝑎
𝑝𝑒𝑚(λ𝑝𝑒𝑚),L𝑑𝑎𝑡𝑎
𝜔𝑡(λ𝜔𝑡), andL𝑑𝑎𝑡𝑎
𝑊𝑒𝑔𝑟(λ𝑊𝑒𝑔𝑟)are the
36data loss in𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟respectively and defined as,
L𝑑𝑎𝑡𝑎
𝑝𝑖𝑚(λ𝑝𝑖𝑚)=1
𝑛𝑛∑︁
𝑗=1h
𝑝(𝑗)
𝑖𝑚𝑑𝑎𝑡𝑎−ˆ𝑝(𝑗)
𝑖𝑚N1
𝜆(𝑗)
𝑝𝑖𝑚i2
(E.4a)
L𝑑𝑎𝑡𝑎
𝑝𝑒𝑚(λ𝑝𝑒𝑚)=1
𝑛𝑛∑︁
𝑗=1h
𝑝(𝑗)
𝑒𝑚𝑑𝑎𝑡𝑎−ˆ𝑝(𝑗)
𝑒𝑚N1
𝜆(𝑗)
𝑝𝑒𝑚i2
(E.4b)
L𝑑𝑎𝑡𝑎
𝜔𝑡(λ𝜔𝑡)=1
𝑛𝑛∑︁
𝑗=1h
𝜔(𝑗)
𝑡𝑑𝑎𝑡𝑎−ˆ𝜔(𝑗)
𝑡N5
𝜆(𝑗)
𝜔𝑡i2
(E.4c)
L𝑑𝑎𝑡𝑎
𝑊𝑒𝑔𝑟(λ𝑊𝑒𝑔𝑟)=1
𝑛𝑛∑︁
𝑗=1h
𝑊(𝑗)
𝑒𝑔𝑟𝑑𝑎𝑡𝑎−b𝑊(𝑗)
𝑒𝑔𝑟𝑁 𝑁
𝜆(𝑗)
𝑊𝑒𝑔𝑟i2
(E.4d)
The same in the Case 5 is given as,
L𝑑𝑎𝑡𝑎
𝑝𝑖𝑚=1
𝑛𝑛∑︁
𝑗=1h
𝑝(𝑗)
𝑖𝑚𝑑𝑎𝑡𝑎−ˆ𝑝(𝑗)
𝑖𝑚N1i2
(E.5a)
L𝑑𝑎𝑡𝑎
𝑝𝑒𝑚=1
𝑛𝑛∑︁
𝑗=1h
𝑝(𝑗)
𝑒𝑚𝑑𝑎𝑡𝑎−ˆ𝑝(𝑗)
𝑒𝑚N1i2
(E.5b)
L𝑑𝑎𝑡𝑎
𝜔𝑡=1
𝑛𝑛∑︁
𝑗=1h
𝜔(𝑗)
𝑡𝑑𝑎𝑡𝑎−ˆ𝜔(𝑗)
𝑡N5i2
(E.5c)
L𝑑𝑎𝑡𝑎
𝑊𝑒𝑔𝑟=1
𝑛𝑛∑︁
𝑗=1h
𝑊(𝑗)
𝑒𝑔𝑟𝑑𝑎𝑡𝑎−b𝑊(𝑗)
𝑒𝑔𝑟𝑁 𝑁i2
(E.5d)
where𝑝𝑖𝑚𝑑𝑎𝑡𝑎,𝑝𝑒𝑚𝑑𝑎𝑡𝑎,𝜔𝑡𝑑𝑎𝑡𝑎and𝑊𝑒𝑔𝑟𝑑𝑎𝑡𝑎are the measured data of 𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡and𝑊𝑒𝑔𝑟
respectively. ˆ 𝑝𝑖𝑚N1, ˆ𝑝𝑒𝑚N1and ˆ𝜔𝑡N5are the predicted values in 𝑝𝑖𝑚,𝑝𝑒𝑚and𝜔𝑡respectively
fromN1(𝑡;θ1),N1(𝑡;θ1)andN5(𝑡;θ5)respectively. Similarly, b𝑊𝑒𝑔𝑟𝑁 𝑁is predicted value in
𝑊𝑒𝑔𝑟from NNs output. 𝑛is the number of measured data points.
L𝑖𝑛𝑖
𝑝𝑖𝑚,L𝑖𝑛𝑖
𝑝𝑒𝑚,L𝑖𝑛𝑖
𝜔𝑡,L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟1,L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟2,L𝑖𝑛𝑖
˜𝑢𝑣𝑔𝑡,L𝑖𝑛𝑖
𝑥𝑟andL𝑖𝑛𝑖
𝑇1are the losses in initial conditions in
37𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡˜𝑢𝑒𝑔𝑟1, ˜𝑢𝑒𝑔𝑟2, ˜𝑢𝑣𝑔𝑡,𝑥𝑟and𝑇1respectively.
L𝑖𝑛𝑖
𝑝𝑖𝑚=1
11∑︁
𝑗=1
𝑝(𝑗)
𝑖𝑚0−ˆ𝑝(𝑗)
𝑖𝑚02
(E.6a)
L𝑖𝑛𝑖
𝑝𝑒𝑚=1
11∑︁
𝑗=1
𝑝(𝑗)
𝑒𝑚0−ˆ𝑝(𝑗)
𝑒𝑚02
(E.6b)
L𝑖𝑛𝑖
𝜔𝑡=1
11∑︁
𝑗=1
𝜔(𝑗)
𝑡0−ˆ𝜔(𝑗)
𝑡02
(E.6c)
L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟1=1
11∑︁
𝑗=1
˜𝑢(𝑗)
𝑒𝑔𝑟10−ˆ˜𝑢(𝑗)
𝑒𝑔𝑟102
(E.6d)
L𝑖𝑛𝑖
˜𝑢𝑒𝑔𝑟2=1
11∑︁
𝑗=1
˜𝑢(𝑗)
𝑒𝑔𝑟20−ˆ˜𝑢(𝑗)
𝑒𝑔𝑟202
(E.6e)
L𝑖𝑛𝑖
˜𝑢𝑣𝑔𝑡=1
11∑︁
𝑗=1
˜𝑢(𝑗)
𝑣𝑔𝑡0−ˆ˜𝑢(𝑗)
𝑣𝑔𝑡02
(E.6f)
L𝑖𝑛𝑖
𝑥𝑟=1
11∑︁
𝑗=1
𝑥(𝑗)
𝑟0−ˆ𝑥(𝑗)
𝑟02
(E.6g)
L𝑖𝑛𝑖
𝑇1=1
11∑︁
𝑗=1
𝑇(𝑗)
10−ˆ𝑇(𝑗)
102
(E.6h)
where𝑝𝑖𝑚0,𝑝𝑒𝑚0,𝜔𝑡0, ˜𝑢𝑒𝑔𝑟10, ˜𝑢𝑒𝑔𝑟20, ˜𝑢𝑣𝑔𝑡0,𝑥𝑟0and𝑇10are the initial conditions and ˆ 𝑝𝑖𝑚0, ˆ𝑝𝑒𝑚0,
ˆ𝜔𝑡0,ˆ˜𝑢𝑒𝑔𝑟10,ˆ˜𝑢𝑒𝑔𝑟20,ˆ˜𝑢𝑣𝑔𝑡0, ˆ𝑥𝑟0and ˆ𝑇10are corresponding output from neural network at time 𝑡=0
for𝑝𝑖𝑚,𝑝𝑒𝑚,𝜔𝑡, ˜𝑢𝑒𝑔𝑟1, ˜𝑢𝑒𝑔𝑟2, ˜𝑢𝑣𝑔𝑡,𝑥𝑟and𝑇1respectively.
38Appendix F Additional Tables
Description Symbol Value
1 Ideal gas constant of air 𝑅𝑎 287
2 Intake manifold temperature 𝑇𝑖𝑚 300.6186
3 Intake manifold volume 𝑉𝑖𝑚 0.0220
4 Ideal gas constant of exhaust gas 𝑅𝑒 286
5 Exhaust manifold volume 𝑉𝑒𝑚 0.0200
6 Displaced volume of the cylinder 𝑉𝑑 0.0127
7 Number of cylinder 𝑛𝑐𝑦𝑙 6
8 Specific heat capacity ratio of air 𝛾𝑎 1.3964
9 Specific heat capacity at constant pressure of air 𝑐𝑝𝑎 1011
10 Specific heat capacity at constant volume air 𝑐𝑣𝑎 724
11 Compression ratio 𝑟𝑐 17
12 Fuel consumed during constant-volume combustion 𝑥𝑐𝑣 2.3371×10−14
13 Heating value of fuel 𝑞𝐻𝑉 42900000
14 Diameter of exhaust pipe 𝑑𝑝𝑖𝑝𝑒 0.1
15 Length of exhaust pipe 𝑙𝑝𝑖𝑝𝑒 1
16 Number of exhaust pipe 𝑛𝑝𝑖𝑝𝑒 2
17Specific heat capacity at constant pressure of exhaust
gas𝑐𝑝𝑒 1332
18 Time constant 1 for EGR 𝜏𝑒𝑔𝑟1 0.05
19 Time constant 2 for EGR 𝜏𝑒𝑔𝑟2 0.13
20 Time delay constant for EGR 𝜏𝑑𝑒𝑔𝑟 0.065
21 Constant for EGR overshoot 𝐾𝑒𝑔𝑟 1.8
22 Optimal value of pressure ratio of EGR Π𝑒𝑔𝑟𝑜𝑝𝑡 0.6500
23 Inertial of turbocharger 𝐽𝑡 2.0×10−4
24 Time constant for VGT 𝜏𝑣𝑔𝑡 0.025
25 Time delay constant for VGT 𝜏𝑑𝑣𝑔𝑡 0.04
26 Specific heat capacity at constant pressure of exhaust 𝑐𝑝𝑒 1332
27 Specific heat capacity ratio of exhaust gas 𝛾𝑒 1.2734
28 turbine blade radius 𝑅𝑡 0.04
29 compressor blade radius 𝑅𝑐 0.0400
Table F1: Values of the constants considered in the present study.
Unknown 𝜂𝑠𝑐ℎ𝑡𝑜𝑡𝐴𝑒𝑔𝑟𝑚𝑎𝑥 𝐴𝑣𝑔𝑡𝑚𝑎𝑥
Value 1.1015 96.2755 4.0×10−48.4558×10−4
Table F2: True value of the unknown parameters.
39Symbol Value Symbol Value Symbol Value
𝑐𝑣𝑜𝑙1−2.0817×10−4𝑐𝑒𝑔𝑟1−1.1104×10−4
𝑐𝑣𝑜𝑙2 -0.0034 𝑐𝑒𝑔𝑟2 0.0178
𝑐𝑣𝑜𝑙3 1.1497 𝑐𝑒𝑔𝑟3 0
𝑐𝜔Ψ1 1.0882×10−8𝑐𝜔Φ1−1.4298×10−8𝑐Ψ2 0
𝑐𝜔Ψ2−1.7320×10−4𝑐𝜔Φ2−0.0015 𝑐Φ2 0
𝑐𝜔Ψ3 1.0286 𝑐𝜔Φ3 29.6462
𝜋𝑐𝑜𝑝𝑡 1.0455 𝑐𝑚1 1.3563 𝑐𝑣𝑔𝑡1 126.8719
𝑊𝑐𝑜𝑝𝑡 0.2753 𝑐𝑚2 2.7692𝑒+03𝑐𝑣𝑔𝑡2 117.1447
𝑎1 3.0919 𝑐𝑚3 0.0100 𝑐𝑓1 1.9480
𝑎2 2.1479 𝐵𝑆𝑅𝑜𝑝𝑡 0.9755 𝑐𝑓2−0.7763
𝑎3−2.4823 𝜂𝑡𝑚,𝑚𝑎𝑥 0.8180 𝐾𝑡 2.8902
𝜂𝑐𝑚𝑎𝑥 0.7364
𝑐𝜋 0.2708
Table F3: Value for the coefficients of the empirical formulae.
Appendix G Additional figures
In this section, we present the results for Case 1 and Case 2.
For Case-1: 3 unknown parameters with clean data
The predicted state variables and 𝑇1and𝑥𝑟for Case 1 (3 unknown with clear data) are shown
in Fig. G1. The predicted dynamics of the known variables are shown in Fig. G2(a)-(d). In
Fig. G2(e)-(h), we have shown the dynamics of variables which are dependent on the unknown
parameters. The predicted empirical formulae are shown in Fig. G3. We also studied the
convergence of the unknown parameters, which are shown in Fig. G4.
400 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00˜uegr1
(c)uegr1True
Pred
0 20 40 60
t(sec)0.00.51.01.5˜uegr2
(d)uegr2True
Pred
0 20 40 60
t(sec)0.000.250.500.751.00˜uvgt
(e) ˜uvgtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ωt
(f)ωtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00T1
(g)T1True
Pred
0 20 40 60
t(sec)0.00.51.0xr
(h)xrTrue
PredFig. G1: Predicted states and 𝑇1and𝑥𝑟for Case 1: Predicted dynamics of the state variables
of the engine and 𝑇1and𝑥𝑟for Case 1 (PINN with self-adaptive weights for 3 unknown
parameters). It can be observed that the predicted dynamics of the states are in good
agreement with the true values. However, similar to 4 unknown parameters 𝑇1and𝑥𝑟
do not match with the true value.
410 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00ωt
(c)ωtTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Wegr
(d)WegrTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(e)TemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wei
(f)WeiTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(g)AegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(h)WtTrue
PredFig. G2: Predicted dynamics of variables for Case 1: (a)-(d) Predicted dynamics of the
variables whose field measurement data are known. (e)-(h) dynamics of important
variables which also depend on the unknown parameters
0 20 40 60
t(sec)0.000.250.500.751.00ηvol
(a)ηvolTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fvgt×fΠt
(c)fvgt×fΠtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ηtm
(d)ηtmTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ηc
(e)ηcTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Φc
(f) Φ cTrue
Pred
Fig. G3: Empirical formulae for Case 1: The predicted values of empirical formulae for Case
1 (3 unknown parameters with clean data).
420.0 0.5 1.0 1.5 2.0
epoch ×105234Aegrmax ×10−4
(a)AegrmaxTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch ×1050.91.01.1ηsc
(b)ηscTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch ×105100125150175htot
(c)htotTrue
PredFig. G4: Convergence of the unknown parameters for Case 1: Convergence of the unknown
parameters with epoch for Case 1 (3 unknown parameters with clean data)
For Case-2: 3 unknown parameters with noisy data
The predicted state variables and 𝑇1and𝑥𝑟for Case 2 (3 unknown with noisy data) are shown
in Fig. G5. The predicted dynamics of the known variables are shown in Fig. G6(a)-(d). In
Fig. G6(e)-(h), we have shown the dynamics of variables which are dependent on the unknown
parameters. The predicted empirical formulae are shown in Fig. G7. We also studied the
convergence of the unknown parameters, which are shown in Fig. G8.
0 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00˜uegr1
(c)uegr1True
Pred
0 20 40 60
t(sec)0.00.51.0˜uegr2
(d)uegr2True
Pred
0 20 40 60
t(sec)0.000.250.500.751.00˜uvgt
(e) ˜uvgtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ωt
(f)ωtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00T1
(g)T1True
Pred
0 20 40 60
t(sec)0.00.51.0xr
(h)xrTrue
Pred
Fig. G5: Predicted states and 𝑇1and𝑥𝑟for Case 2: Predicted dynamics of the state variables
of the engine and 𝑇1and𝑥𝑟for Case 2 (PINN with self-adaptive weights for 3 unknown
paramters). It can be observed that the predicted dynamics of the states are in good
agreement with the true values. However, similar to 4 unknown parameters 𝑇1and𝑥𝑟
do not match with the true value.
430 20 40 60
t(sec)0.000.250.500.751.00pim
(a)pimTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00pem
(b)pemTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00ωt
(c)ωtTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Wegr
(d)WegrTrue
Pred
Data
0 20 40 60
t(sec)0.000.250.500.751.00Tem
(e)TemTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wei
(f)WeiTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Aegr
(g)AegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00Wt
(h)WtTrue
PredFig. G6: Predicted dynamics of variables for Case 2: (a)-(d) Predicted dynamics of the
variables whose field measurement data are known. (e)-(h) dynamics of important
variables which also depend on the unknown parameters
0 20 40 60
t(sec)0.000.250.500.751.00ηvol
(a)ηvolTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00fvgt×fΠt
(c)fvgt×fΠtTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ηtm
(d)ηtmTrue
Pred
0 20 40 60
t(sec)0.000.250.500.751.00ηc
(e)ηcTrue
Pred
0 20 40 60
t(sec)−101Φc
(f) Φ cTrue
Pred
Fig. G7: Empirical formulae for Case 2: The predicted values of empirical formulae for Case
2 (3 unknown parameters with noisy data).
440.0 0.5 1.0 1.5 2.0
epoch ×105246Aegrmax ×10−4
(a)AegrmaxTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch ×1050.80.91.01.11.2ηsc
(b)ηscTrue
Pred
0.0 0.5 1.0 1.5 2.0
epoch ×105100125150175200htot
(c)htotTrue
PredFig. G8: Convergence of the unknown parameters for Case 2: Convergence of the unknown
parameters with epoch for Case 2 (3 unknown parameters with noisy data)
Appendix H Neural network surrogates for empirical formu-
lae
The empirical formulae of the engine model are approximated using surrogate neural networks
and are discussed in section 2.1.1. In section 2.3, we discuss the laboratory data required to
train these neural networks. The laboratory data required for training of each neural network are
shown in Table 3 ( §2.3). The labelled data for training these neural networks may be calculated
from static data on the entire operational range of each quantity except for turbine mechanical
efficiency (𝜂𝑡𝑚). The labelled data for the turbine mechanical efficiency is calculated using Eq.
(B.25) ( §B.4), which is a differential equation, thus requiring dynamic data with fine 𝑑𝑡. The
calculations of the labelled data from the laboratory measurements are discussed in Appendix
D. In the case of training of neural network N(𝑃)
3(x;θ𝑃
3)for the approximation of 𝐹𝑣𝑔𝑡,Π𝑡,𝐿2
weight regularizer is considered in the loss function with a coefficient 5 ×10−10.
The predicted values of the empirical formulae for Case-V (Table 4 in §2.3) with the true
values for 1-minute duration are shown in Fig. H1. The % relative 𝐿2errors for training and
testing data set are shown in the last two columns of Table H1. We observe that the neural
networks are able to predict the empirical quantity with very good accuracy. The testing error
in the case of the surrogate neural network for 𝑓𝑒𝑔𝑟is smaller than the training error. This is
because of the nature of the function and the data considered. The input-output relationship is
simple, with only one input and one output. The maximum value of the testing data is smaller
than the maximum value of the training data. Similarly, the minimum value of testing data is
larger than the minimum value of training data. We also observed that the standard deviation of
testing data is smaller than the standard deviation of the training data. Since the EGR system is
independent and 𝑓𝑒𝑔𝑟depends only on ˜ 𝑢𝑒𝑔𝑟, not any other variables (e.g. ambient temperature
and pressure), we assume that most of the testing set of data might be within the training data
set (training data set is 2 hrs while testing data set is 20 minutes). Thus, the testing error is
marginally smaller than the training error. The testing error in the case of the surrogate model
for𝜂𝑡𝑚is smaller than that of the training error. The approximation considered in calculating
the labelled data for 𝜂𝑡𝑚from the laboratory data, we have considered a five-point method
to approximate the differentiation present in Eq. (B.25) ( §B.4). Thus, a few noisy data are
observed in both training and testing data sets. As the duration of the training data set is larger
than the testing dataset, the amount of noisy data is more in the training data. Thus, the error
in training is slightly higher than the testing error. These neural networks, after training, will
be used in the places of the empirical formulae in the inverse problem. The trained weights and
45biases will be considered fixed in the inverse problem.
Neural InputNetwork size OutputOutput 𝐿2error (%)
network ( x) restrict‡Train Test
N(𝑃)
1(x;θ𝑃
1)𝑛𝑒,𝑝𝑖𝑚[2,4,4,1]𝜂𝑣𝑜𝑙 0.01 0.03
N(𝑃)
2(x;θ𝑃
2) ˜𝑢𝑒𝑔𝑟[1,4,4,1]𝑓𝑒𝑔𝑟𝑆(𝑓𝑒𝑔𝑟) 0.14 0.10
N(𝑃)
3(x;θ𝑃
3)Π𝑡,˜𝑢𝑣𝑔𝑡[2,8,8,8,1]𝐹𝑣𝑔𝑡,Π𝑡1.1×𝑆(𝐹𝑣𝑔𝑡,Π𝑡)0.03 0.52
N(𝑃)
4(x;θ𝑃
4)𝜔𝑡,Π𝑡,𝑇𝑒𝑚[3,4,4,4,1]𝜂𝑡𝑚 min(0.818,𝜂𝑡𝑚)1.62 1.32
N(𝑃)
5(x;θ𝑃
5)𝑊𝑐,Π𝑐[2,4,4,4,1]𝜂𝑐 max(0.2,𝑆(𝜂𝑐)) 0.16 0.18
N(𝑃)
6(x;θ𝑃
6)𝜔𝑡,Π𝑐,𝑇𝑎𝑚𝑏[3,10,10,10,1]Φ𝑐𝑆(Φ𝑐) 0.76 1.13
‡𝑆−→sigmoid function
Table H1: Details of neural networks for empirical formulae: Details of the DNNs to ap-
proximate empirical formulae. The first two columns specify the neural network
(Table 2) and their input, respectively. The third column indicates the neural network
size considered. The activation function of the hidden layers is 𝜎(.)=tanh(.). The
”Output” column specifies the empirical quantity the neural network approximated.
The last two columns give the results for the test data after the completion of the
training. The column ”Error” specifies the relative % 𝐿2error for the test case (Case
V). Appropriate scaling of input and output are considered in the training of neural
networks.
46015 30 45 60
t(sec)0.000.250.500.751.00ηvol
(a)ηvolTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00fegr
(b)fegrTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00fvgt×fΠt
(c)fvgt×fΠtTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00ηtm
(d)ηtmTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00ηc
(e)ηcTrue
Predicted
015 30 45 60
t(sec)0.000.250.500.751.00Φc
(f) Φ cTrue
PredictedFig. H1: Prediction of empirical formulae: The predicted and true values of the empirical
formulae for test case (Case-V). The plots are normalized within 20-minute data, and
only a portion (0 to 1 minute) of the results are shown. The predicted empirical
formulae are in good agreement with the true values.
47