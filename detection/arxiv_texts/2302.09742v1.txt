AFFECT -CONDITIONED IMAGE GENERATION
Francisco Ibarrola
The University of Sydney,
NSW, Australia.
francisco.ibarrola@sydney.edu.auRohan Lulham
The University of Sydney,
NSW, Australia.
rohan.lulham@sydney.edu.auKazjon Grace
The University of Sydney,
NSW, Australia.
kazjon.grace@sydney.edu.au
happy (high Valence)
 excited (high Arousal)
 in control (high Dominance)
unhappy (low Valence)
 calm (low Arousal)
 not in control (low Dominance)
Figure 1: Example images of a “ﬂaming landscape” generated using VQGAN+CLIP with our affect conditioning,
maximising (top row) and minimising (bottom row) the three dimensions we utilise: valence (left), arousal (middle),
and dominance (right).
ABSTRACT
In creativity support and computational co-creativity contexts, the task of discovering appropriate
prompts for use with text-to-image generative models remains difﬁcult. In many cases the creator
wishes to evoke a certain impression with the image, but the task of conferring that succinctly in a
text prompt poses a challenge: affective language is nuanced, complex, and model-speciﬁc. In this
work we introduce a method for generating images conditioned on desired affect, quantiﬁed using a
psychometrically validated three-component approach, that can be combined with conditioning on
text descriptions. We ﬁrst train a neural network for estimating the affect content of text and images
from semantic embeddings, and then demonstrate how this can be used to exert control over a variety
of generative models. We show examples of how affect modiﬁes the outputs, provide quantitative and
qualitative analysis of its capabilities, and discuss possible extensions and use cases.arXiv:2302.09742v1  [cs.AI]  20 Feb 20231 Introduction
Recent advances in Generative AI have made it possible to produce high quality images based solely on a textual
description [ 1,2,3]. These techniques are largely based on learning a latent space that encodes the semantic meanings
of words or phrases, which can then be used to condition the generative process. Images typically contain much
more information than short text prompts, and hence generative models need to learn the distribution of the residual
information in order to “ﬁll in the gaps”. As a result, controlling this behaviour through iterative prompt engineering
has become a major component of real-world workﬂows using these technologies to produce images [4].
These difﬁculties are often encountered within an artistic or design setting, where users may seek to produce an image
that evokes a certain impression in its viewers. Choosing the right words to do that is a signiﬁcant problem, and one
that extends beyond the obvious challenge of coming up with the right words to convey a feeling, as complex or highly
speciﬁc prompts can suffer from adverse effects like visual concept leakage. For example, “a man stuck in a trafﬁc jam
is eating” will, in DALLE-2 [ 5], produce images of a man in a car eating jam straight from the jar [6]. Such surprising
images are unlikely to match a user’s intention (even if in this case the scene is borderline plausible). The complexity of
the landscape of semantic latent spaces and the sparseness of the conditioning stimuli can lead to undesirable outputs,
particularly when prompts are grammatically complex or involve words with multiple meanings.
Mismatches between intent and output are particularly relevant when human-AI co-creation is involved [ 7,8], due to
the constructive nature of the creative process itself. Psychological studies of creativity suggest that the tasks to which
creative thinking is applied are under-speciﬁed [ 9], meaning that creativity is as much a matter of problem-framing as
of problem-solving [ 10]. A user occupied with wrestling an obstinate model to capture what they want right now, we
contend, will be less able to reﬁne those intents and ﬁgure out what they actually want. In creativity support applications
such as art (where it may be desirable that an image evoke a particular mood or emotion [ 11]), or design (where diverse
solutions must be explored [ 12]), this signiﬁcantly damages the utility of existing generative models. An ethnographic
study of an online community that sprung up around a popular text-to-image generator supports this: the intent-output
mismatch was found to be a signiﬁcant practical challenge, so much so that the time-intensive process of collaborative
prompt engineering became a major focus for the community [13].
In this paper we propose an additional conditioning stream, intended to be a simple and abstract way to diminish
the need for highly-speciﬁc iteratively-tuned prompts, and in doing so reduce this mismatch between user and model
semantics. We propose to condition images directly on affect: the emotive component of experiencing a stimulus.
Images with very similar semantic content ("a dog in a forest") can evoke very different affect based on details not
present in the prompt: perhaps the dog is snarling, perhaps it’s an injured puppy, or perhaps it’s excitedly retrieving a
thrown stick. All of those details could be added to the prompt, but only if the user already knew exactly what they
wanted, which is often not the case in creativity-support [14].
We develop a method for conditioning image generation in a variety of models, where affect is quantiﬁed using a
psychometrically validated approach [ 15]. This research established that cross-culturally, peoples’ feelings about a
wide range of stimuli (including words, phrases, scenarios, images, and people) varied on the same three affective
dimensions; Valance (AKA evaluation, or how “nice” or “happy” something feels), Arousal (AKA activity, or how
“exciting” or “lively” something feels) and Dominance (AKA power, or how “in control” something makes one feel). In
Affect Control Theory, large data sets are used to model social interaction, experience and uncertainty with impressive
ﬁdelity [ 16,17,18]. Computational models of these dimensions have been used in sentiment analysis tasks (typically
just the valence and arousal dimensions, [ 19]), but not – to the best of our knowledge – to condition generative models.
We show that these three dimensions of affect (which we call V AD scores, for Valence, Arousal & Dominance) can be pre-
dicted from CLIP [ 20] and BERT [ 21] encodings, and that this can be used to guide generative models. We demonstrate
this in both generation-by-optimization models (CLIPDraw [ 2] and VQGAN+CLIP [ 1]) and in conditional generation
(Stable Diffusion [ 22]). We show through a user study that our affect-conditioning allows for generating images with
desired affect content without degrading the quality of images (i.e. ﬁt-to-prompt). We end with a qualitative analysis of
the relationship between semantics and affect-guided generation, as well as a discussion of the potential applications
of this technique. Code to reproduce our experiments is available at https://github.com/ﬁbarrola/affect_conditioned_
generation
2 Affect Prediction
In order to build a model capable of generating images conditioned on affect scores, we begin by training a neural
network to estimate such scores. It is timely to note that our available data for this task consist of a list of 13k+ words,
whose affect scores were derived through surveys [ 17], and a dataset of 1.1k+ images [ 23], also with their respective
V AD scores.
2We begin by recalling that CLIP [ 20] consists of two neural networks fandgthat map images and text (respectively)
to a common latent space Z, where the cosine similarity between two points attest to how semantically close they
are. This means that given a (square) image x2[0;1]3MMand a caption t, their cosine similarity in Z=R512,
hg(t);f(x)ishould be large if and only if the caption corresponds to the image (and vice versa). By transitivity, this
also implies that semantically similar images (or text prompts) should be close in Z.
Since images and text are indistinguishable in Z, and so are the scores in R3, the problem can be reduced to training a
neural network A:R512!R3. Given that there is no evident structural correlation between the coordinates of Z, we
can simply use a Multi-Layer Perceptron (MLP), whose details and training parameters are speciﬁed in Sections 2.1
and 2.2.
2.1 Network Architecture
Our affect prediction model had an input dimension of 512 (to match the CLIP latent space), followed by two rectiﬁed
linear layers of dimension 64 and 32, followed by an output layer of dimension 3 (to match the V AD model of affect).
All layers included bias nodes.
2.2 Data and Training
Training was performed using a dataset of 13913 words and another of 1194 images, both annotated with affect scores
v2[0;8:6]3. We computed their CLIP embeddings using the corresponding networks, and combined them into a
single dataset of 15107 vectors in R512. The dataset was split 70%=30% into training and testing sets, and training was
performed for 1500 epochs, backpropagating using Adam optimization and with 20% dropout between layers.
Previous to training, we scaled the CLIP embeddings as well as the affect scores to [0;1]. We shall hereafter refer to the
affect scores on the [0;1]interval for ease of understanding.
3 Affect-Conditioned Generation
Having developed a neural network Athat can predict affect scores based on CLIP latents, we can use it to condition
image generation models to produce outputs with a desired affect score v02R3.
Let us consider the space of square images of MbyMpixelsX:= [0;1]3MM;and an image generator h:Y!X
that produces an image x2X based on a set of parameters y2Y.
3.1 Generation-by-optimization with affect
Some generative models, like VQGAN+CLIP [ 1] and CLIPDraw [ 2], work by guiding hby optimizing the set of
parametersyfor the produced image to match a given text prompt tin terms of CLIP embeddings. This means, ﬁnding
yminimizing a semantic loss
Ls(g(t);fh(y));
where as before gandfare CLIP’s text and image encoders, respectively.
In VQGAN+CLIP, Yis VQGAN’s [ 24] latent space and Lsis the spherical distance between the CLIP embeddings of
the prompt and generated image. In CLIPDraw, Yis the set of possible Bézier curves parameters, and Lsis the cosine
similarity between the CLIP embeddings of the prompt and generated image.
Given that our affect-predicting neural network Ais a differentiable function, we can guide these kind of generation-by-
optimization methods by deﬁning a new cost function
Lgo(y;t;v0):=Ls(g(t);fh(y)) +kAfh(y) v0k2; (1)
where>0is a regularization parameter that determines the weight we assign to the output matching the affect vector
against the semantic meaning of the prompt.
3.2 Conditional generative models with affect
In other generative models, such as Stable Diffusion [ 22], the generator htakes an embedding of the text prompt
z=b(t)as conditioning input, and the result depends solely on this and the initial conditions or noise .
In the particular case of Stable Diffusion, the embedding space Zconsists of 77 channels of ﬁne-tuned BERT
embeddings. We know that close points in this space correspond to prompts with similar semantic meanings, and so
3a way to steer an output to have a desired affect score v0would be to ﬁnd points close to z(t)2R77768that have a
more suitable score. Namely, minimizing with respect to z
Lsd(z;t;v0):=X
ckbc(t) zck2+kAc(zc) v0k2; (2)
where>0is a weighting parameter and Acis a MLP trained (as before) to predict affect scores from the c-th channel
of the embedding.
The network structure used for this was the same as the one described in Section 2.1, except the ﬁrst layer input is of
size 768 (to match a single BERT channel). Of course, this entails training 77 similar networks, but given the number of
network parameters this is fairly fast. Unlike the CLIP space, BERT encodes only text, so we removed the affect-image
pairs from our training data and just trained these 77 networks on (70% of) the 13,913 affect-word pairs. The process
is otherwise analogous to the previous one, and both code and our pre-trained models are available from the same
repository.
4 Results
Figure 2: Affect values of images matching four prompts. In blue, the ground truth affect values for the images obtained
in surveys from [Lang et al. , 1999]. In red, the affect scores estimated from the images using the model described in
Section 2, in green the affect scores estimated from the prompts by the same model. The licence for using these images
prohibits their publication to preserve their value.
Let us start by assessing the performance of our affect-predicting neural networks, trained from CLIP latents as well as
from BERT embeddings.
Type Mean Error (CLIP) Mean Error (BERT)
Text 0:064 0:0620:007
Image 0:076 N/A
Table 1: Estimation errors over the test data for our affect scores prediction networks, based on CLIP latents (images
and text) or BERT embeddings (77 channels, text).
The obtained training errors are depicted in Table 1. These are computed over affect scores scaled to [0;1], so we can
think of them as percentages. Given that the ground truth data is from a survey, the target affect scores for a given
word (or image) are the means of the empirical distributions of user ratings. In that sense it is worth mentioning that
97:7%of estimations using CLIP fall within a standard deviation of the survey means, as well as 98:4%of BERT-based
predictions.
Note that the models were trained on single words (and images in the case of the CLIP version), but given that we are
basing predictions on latent spaces capable of encoding both words and phrases, it should also work for predicting the
affect scores of text prompts. While we do not have a ground truth for evaluating (or training) on multi-word input,
we can approximately assess the model’s performance by manually generating prompts that describe images in our
training set (for which we do have V AD scores) and then comparing the model’s predicted affect for that phrase to the
ground-truth affect available for the image. This is imprecise as there are many ways of describing any image, but
broad agreement would suggest efﬁcacy of phrase-level prediction. Figure 2 shows brief descriptions of four randomly
selected images (the actual images are under a non-disclosure policy) along with their true affect scores, the scores
estimated from the images using the model described in Section 2, and the affect scores predicted using the image
descriptions as prompts, using the same model. Note that these text prompts were written before any predictions were
made, without iteration or cherry-picking. We would include the images themselves, but the licence for using this
dataset prohibits their publication to preserve their value for psychology experiments. Note that across all four prompts
there are very small differences between ground truth, prediction from the prompt, and prediction from the images.
4Figure 3: Quality (i.e. ﬁt-to-prompt) ratings from a survey of 27 participants for images generated using generation-by-
optimization methods, with (AC) or without (No AC) incorporating Affect Control. With no observable quality effects
between the two we claim that our affect-conditioning approach does not impact subjective image quality.
Figure 4: User choices regarding which images better reﬂect the corresponding affect state.
4.1 Generation-by-optimization with affect
In order to test incorporating affect control into a generation-by-optimization context, we used the approach described
in Section 3.1 with CLIPDraw and VQGAN+CLIP. That is, we chose the corresponding semantic loss Lsin 1 and
different prompts and target affect vectors v.
Figure 1 shows an example of images generated using VQGAN+CLIP with a ﬁxed prompt (“ﬂaming landscape”) and
different target affect vectors. Target affect vectors were constructed to have a 1(or0) in the affect position we wanted
to enhance (or decrease), and 0:5in the remaining components.
A ﬁrst-order qualitative examination of the six images in Fig. 1 shows signiﬁcant differences in tone and content. The
high valence image has small, almost ﬂighty wisps of ﬂame and pleasantly contrasting sky colours. The low valence
image has dead trees, larger ﬂames, and burnt-out areas. The high arousal image has towering ﬂames spitting pillars of
black smoke over a rough landscape, while the low arousal image has a few small, contained ﬂames and a blue sky. The
high dominance image (remember that "dominance" here means the viewer feels dominant or in control, not that the
image dominates the viewer) is likewise calm with a blue sky, while the low-dominance image shows a large ﬁre-front
and a rough sky. All six images are of a “ﬂaming landscape”, but their affective content clearly varies.
In order to quantitatively support these observations, we conducted a survey that consisted of two tasks. First, the 27
participants were presented with a set of images (generated with or without affect control) and asked to rate how they
matched a provided prompt, using a 7-pt Likert scale. This was to test whether our method entailed a loss of quality (i.e.
ﬁt-to-prompt). Secondly, they were given pairs of affect-conditioned images, one generated to have a high score of
either valence, arousal, or dominance, and the other with a low score on the same component. Half the images were
from VQGAN+CLIP [ 1], the other half from CLIPDraw [ 2]. The subjects were then asked to pick which image of the
pair felt happier (if the conditioning component was Valence), or more calm (for Arousal) or made them feel more in
control (for Dominance).
The results of the ﬁrst (image quality) task for each prompt are depicted in Figure 3. No consistent trend is observed
across the prompts, suggesting that affect conditioning does not reduce subjective ratings of ﬁt-to-prompt.
Participant choices for perceived affect are summarized in Figure 4. Respondents could clearly distinguish affect-
conditioned images for Valence and Arousal, but not for Dominance. This suggests that our affect-conditioning
model aligns with viewer expectations across a variety of prompts and two different generators for the ﬁrst two affect
dimensions, but not for the third. Subjectively, participants reported difﬁculty understanding what it meant to feel “in
control” or “not in control” of an image, potentially suggesting confounds in that part of the study. It’s also important to
note that the images generated by VQGAN+CLIP and CLIPDraw are abstract and dream-like, which may have been
less compatible with feelings of dominance than with feelings of happiness/unhappiness or excitement/calm.
54.2 Affect Conditioned Stable Diffusion
Stable Diffusion produces images of higher quality than the synthesis-based methods, so we perform a more in-depth
qualitative analysis on results generated with it. We generated several paired samples of images conditioned to have low
or high values of each affect component. The images are “paired” in the sense that the diffusion process was initiated
from the same noise tensors, meaning that the only differences we should observe are a consequence of the affect
conditioning. We present our analysis of several pairs here, noting that the nature of the subject matter makes that
analysis tend unavoidably towards the poetic. Further examples can be found in the supplementary material.High Valence
 Low Valence
Figure 5: Images of “A forest” generated using affect-conditioned Stable Diffusion with either High or Low Valence.
The inﬂuence of Valence on the prompt “A forest” can be seen in Figure 5. The high valence forests appear to have
more sunlight, and thicker, greener foliage, which are known to be a human preference [ 25]. One low-valence image,
by contrast, is snow-covered. The viewpoint of the images tends to point upwards, emphasising the height of the trees
in a way that we interpret as evoking majesty or reverence. By contrast the low-valence images have a viewpoint that
focuses on the depth of the forest, perhaps evoking a feeling of being lost or endangered.
How Arousal affects the prompt “the sea at sunrise” (Figure 6) is more direct. The lower Arousal images depict a ﬂat
sea with small, even waves, consistent with the notion that “still waters create feelings of calmness and tranquility.”
[26]. In addition to the rougher seas, high arousal also appears to correlate with a stronger and more vibrant dawn light,
with greater contrast accentuating the clouds.
Unlike what we have observed with the methods that generate abstract images, the effect of dominance appears clearly
in the images of "An elephant" in Figure 7. Feeling in control watching something as imposing as an elephant seems
unlikely, but our method produced images of children’s toys or cartoons when high Dominance was enforced. This
qualitative difference shows the capacity of affect-conditioning to push generators into diverse and a-priori unlikely
regions of the subspaces of images associated with a particular prompt.
This suggests that affect-controlled image generation may be useful for inspiring new ways in which we can confer
a desired affect to a design or artwork. Such modiﬁcations could then be integrated into the prompt in a creative
application, furthering that iterative process of discovery. To demonstrate this, we modiﬁed the prompts for the above
images using characteristics we observed in the affect-conditioned results. Table 2 shows our model’s predicted affect
scores for each of those prompt modiﬁcations, showing consistently larger or smaller affect scores in the corresponding
components as those modiﬁers are added or removed. This shows the potential of affect conditioning both as a tool
not just for producing images that provoke a particular affect directly, but also for exploring the space of appropriate
prompts.
6High Arousal
 Low Arousal
Figure 6: Images of “The sea at sunrise” generated using affect-conditioned Stable Diffusion with either High or Low
Arousal.High (viewer) Dominance
 Low (viewer) Dominance
Figure 7: Images generated using affect-conditioned Stable Diffusion using the prompt “An elephant” with either High
or Low Dominance.
5 Discussion and Conclusions
The success of our affect-conditioned generative models shows that CLIP semantic latents effectively represent affective
information, in that it can be predicted from them (at least in the Valence and Arousal dimensions) accurately. This
provides an interesting insight on the continuity of this latent space and its mapping to affect, since 90% of our our
training data consisted of single words, and that the remaining images constitute a relatively small number of data
points compared to the number of learned parameters.
7Prompt V A D
A forest in summer 0.802 0.408 0.647
A forest 0.746 0.407 0.646
A forest in winter 0.695 0.411 0.572
A wavy sea at sunrise 0.680 0.365 0.602
The sea at sunrise 0.770 0.356 0.641
A calm sea at sunrise 0.768 0.331 0.655
A toy elephant 0.610 0.376 0.591
An elephant 0.610 0.370 0.577
A real elephant 0.554 0.400 0.545
Table 2: Affect scores of text prompts, computed from CLIP latents.High Valence
 Low Valence
Figure 8: Images of “A house overlooking the sea” generated using affect-conditioned Stable Diffusion with either
High or Low Valence.
Our survey suggests that affect conditioning can imbue Valence and Arousal scores into CLIPDraw and VQGAN+CLIP
generated images. Our survey did not support a similar conclusion for the third component of affect, Dominance
(although we note that previous research has characterised dominance as the "least strong" of the three [ 23]). A
qualitative analysis of our high- and low-dominance conditioned images (see Figure 1) does show some potential effects,
but they were not human-discriminable in our study. We hypothesize that this has to do with the inherent difﬁculty of
conferring dominance in abstract images like those produced by VQGAN+CLIP and CLIPDraw. This conjecture is
supported by affect-conditioned images from the less-abstract Stable Diffusion: while we did not conduct a second
survey using these, the fact that it produced toys when told to draw an elephant that the viewer felt dominant over (see
Figure 7 shows its power in at least some scenarios.
We contend that affect conditioning is a valuable direction for future generative models, particularly in human-AI
co-creative contexts. Prompt engineering is hard and time consuming, because many creators do not know exactly what
they are looking for [ 9], and even if they did the model may not interpret their words as they intend [ 13]. We propose,
however, that many artists and designers have a sense of the "vibe" they desire in their ﬁnished product, and affect
conditioning gives a way for them to directly target that. To give a concrete example, users may want to increase control
but may have not thought of tricks like drawing the elephant as a toy, or may want to increase pleasantness but not
thought of focusing on the treetops rather than the horizon.
Figure 8 shows another example of how affect-conditioning could guide design. It contains several versions of “A
house overlooking the sea” with either high or low Valence. From these images a designer could infer certain desirable
characteristics: having big windows, wood and glass furnishings, and a pool. While these may seem obvious to any
8architect or real-estate agent, we would emphasise that (since the affect prediction networks here were trained on BERT
embeddings only), this model has never “seen” an image of “a house overlooking the sea” associated with any affect
value. Those desirable visual features were inferred using only the continuity of the BERT latent space, with no images
ever provided as reference.
While these results are promising, there is still much to do in this direction. It should be possible to re-train a diffusion-
based model to directly incorporate affect, using the prediction network proposed in this work as ground truth, leading
to faster (and potentially more accurate) real-time results. User studies of new generative creativity support tools that
incorporate affect conditioning will also be needed to support our hypotheses about our approach’s efﬁcacy in that
context.
Acknowledgments
We would like to acknowledge the support of the Australian Research Council (Grant #DP200101059) in completing
this work.
References
[1]Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and
Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. In
European Conference on Computer Vision , pages 88–105. Springer, 2022.
[2]Kevin Frans, Lisa B Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through language-
image encoders. arXiv preprint arXiv:2106.14843 , 2021.
[3]Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487 , 2022.
[4]Vivian Liu and Lydia B Chilton. Design guidelines for prompt engineering text-to-image generative models. In
CHI Conference on Human Factors in Computing Systems , pages 1–23, 2022.
[5]Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image
generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
[6]Royi Rassin, Shauli Ravfogel, and Yoav Goldberg. Dalle-2 is seeing double: Flaws in word-to-concept mapping
in text2image models. arXiv preprint arXiv:2210.10606 , 2022.
[7]Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kﬁr Aberman. Dreambooth:
Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242 , 2022.
[8]Francisco Ibarrola, Tomas Lawton, and Kazjon Grace. A collaborative, interactive and context-aware drawing
agent for co-creative design. arXiv preprint arXiv:2209.12588 , 2022.
[9] Bec Paton and Kees Dorst. Brieﬁng and reframing: A situated practice. Design studies , 32(6):573–587, 2011.
[10] Mary Lou Maher and Josiah Poon. Modeling design exploration as co-evolution. Computer-Aided Civil and
Infrastructure Engineering , 11(3):195–209, 1996.
[11] Michael Cook, Simon Colton, Alison Pease, and Maria Teresa Llano. Framing in computational creativity-a
survey and taxonomy. In ICCC , pages 156–163, 2019.
[12] Kazjon Grace, Mary Lou Maher, David Wilson, and Nadia Najjar. Personalised speciﬁc curiosity for computational
design systems. In Design Computing and Cognition’16 , pages 593–610. Springer, 2017.
[13] Jonas Oppenlaender. The creativity of text-to-image generation. In Proceedings of the 25th International Academic
Mindtrek Conference , pages 192–202, 2022.
[14] Gaetano Cascini, Yukari Nagai, Georgi V Georgiev, Jader Zelaya, Niccolò Becattini, Jean-François Boujut, Hernan
Casakin, Nathan Crilly, Elies Dekoninck, John Gero, et al. Perspectives on design creativity and innovation
research: 10 years later, 2022.
[15] Charles Egerton Osgood, William H May, Murray Samuel Miron, and Murray S Miron. Cross-cultural universals
of affective meaning , volume 1. University of Illinois Press, 1975.
[16] David R Heise. Expressive order: Conﬁrming sentiments in social actions . Springer Science & Business Media,
2007.
9[17] Amy Beth Warriner, Victor Kuperman, and Marc Brysbaert. Norms of valence, arousal, and dominance for 13,915
english lemmas. Behavior research methods , 45(4):1191–1207, 2013.
[18] Jesse Hoey and Tobias Schröder. Bayesian affect control theory of self. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence , volume 29, 2015.
[19] Jin Wang, Liang-Chih Yu, K Robert Lai, and Xuejie Zhang. Dimensional sentiment analysis using a regional
cnn-lstm model. In Proceedings of the 54th annual meeting of the association for computational linguistics
(volume 2: Short papers) , pages 225–230, 2016.
[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language
supervision. In International Conference on Machine Learning , pages 8748–8763. PMLR, 2021.
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10684–10695, 2022.
[23] Peter J Lang, Margaret M Bradley, Bruce N Cuthbert, et al. International affective picture system (iaps): Instruction
manual and affective ratings. The center for research in psychophysiology, University of Florida , 1999.
[24] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12873–12883, 2021.
[25] Rob Kuper. Preference, complexity, and color information entropy values for visual depictions of plant and
vegetative growth. HortTechnology , 25(5):625–634, 2015.
[26] Cigdem Sakici. Assessing landscape perceptions of urban waterscapes. The Anthropologist , 21(1-2):182–196,
2015.
10