Motion-Zero: A Zero-Shot Trajectory Control Framework of Moving Object
for Diffusion-Based Video Generation
Changgu Chen1Junwei Shu1, Gaoqi He1, Changbo Wang2*, Yang Li1*,
1School of Computer Science and Technology, East China Normal University, Shanghai, China
2School of Data Science and Engineering, East China Normal University, Shanghai, China
{52215901006, 51265902091}@stu.ecnu.edu.cn, {gqhe, cbwang, yli}@cs.ecnu.edu.cn,
Abstract
Recent large-scale pre-trained diffusion models have demon-
strated a powerful generative ability to produce high-quality
videos from detailed text descriptions. However, exerting con-
trol over the motion of objects in videos generated by any
video diffusion model remains a challenging problem. In this
paper, we propose a novel zero-shot moving object trajectory
control framework, Motion-Zero, to enable arbitrary single-
object-trajectory control for the text-to-video diffusion model.
To this end, an initial noise prior module is designed to pro-
vide a position-based prior to improve the stability of the
appearance of the moving object and the accuracy of posi-
tion. In addition, based on the attention map of the U-Net,
spatial constraints are directly applied to the denoising pro-
cess of diffusion models, which further ensures the positional
consistency of moving objects during the inference. Further-
more, temporal consistency is guaranteed with a proposed
shift temporal attention mechanism. Our method can be flexi-
bly applied to various state-of-the-art video diffusion models
without any training process. Extensive experiments demon-
strate our proposed method can control the motion trajectories
of arbitrary objects while preserving the original ability to
generate high-quality videos.
Introduction
In recent years, the generative capabilities of diffusion models
have been widely recognized in both text-to-image (Dhari-
wal and Nichol 2021; Ho, Jain, and Abbeel 2020; Rombach
et al. 2022; Song, Meng, and Ermon 2020) and text-to-video
domains (Guo et al. 2023; Ho et al. 2022). Although these
video models are capable of producing high-definition, high-
resolution, and fluid video animations, the dynamic motion
trajectories of the generated objects are relatively random in
existing text-to-video models (Blattmann et al. 2023; Zhang
et al. 2023). Thus, accurate control of an object’s motion
trajectories in a generated video remains rudimentary.
Various strategies have been proposed to address this prob-
lem. Some methods (Zhang, Rao, and Agrawala 2023; Wu
et al. 2023a) try to control the trajectory of the moving object
by providing a detailed conditional motion sequence, such
as a dancing skeleton. However, the cost of acquiring con-
ditional control sequences is non-negligible and inhibits the
diverse generation of video outputs by the user. To obtain
*Corresponding authors.variety over the generated video with more control capability,
i.e. trajectories of moving objects, several methods (Wang
et al. 2023d,c; Yin et al. 2023) harness a substantial dataset
of motion and trajectory pairs to train the baseline models.
Although the results are very impressive and promising, these
methods are solely applicable to the base models on which
they were trained and cannot be applied to other models di-
rectly. Furthermore, all of these methods require extensive
training and significant computational resources, preventing
ordinary users from using them.
Nevertheless, the fundamental text-to-video diffusion mod-
els (Blattmann et al. 2023; Zhang et al. 2023) have under-
gone an extremely large scale training dataset (Bain et al.
2021; Xue et al. 2022). Theoretically, the pre-trained model
should inherently have learned extensive knowledge about
the dynamics of a variety of object movements. However, the
semantics of the latent space in different modules of a video
diffusion model are not explicitly defined, which makes it dif-
ficult to manually control generated video in desired motion
dynamics. To some extent, the methods mentioned above try
to align and define those latent spaces in the baseline model
by training with additional labeled datasets, and successfully
harnessing the intrinsic knowledge of pre-trained models to
control the motion. In addition, we observe that the initial
noise plays a significant inspirational role in the generation
of the videos. The same initial noise tends to produce videos
with similar content. With these two characteristics, we can
manipulate the trajectory of moving objects.
In this paper, we propose a zero-shot trajectory control
framework, Motion-Zero, to guide the motion of generated
objects in video. Our proposed method can be easily applied
to pre-trained video diffusion model while achieving univer-
sality and plug-and-play capabilities of controlling. To this
end, we first design an Initial Noise Prior Module to pro-
vide noises based on the motion trajectories given by the user.
Then, inspired by the fact that the values of the cross-attention
map within the U-Net largely determine the generation loca-
tion of the subject mentioned in the prompt (Xie et al. 2023;
Epstein et al. 2023), Spatial Constraints with an attention sim-
ilarity loss over these attention maps are proposed to achieve
precise manipulation of object positioning within individ-
ual frames. Furthermore, we observe that merely imposing
spatial constraints on the positions of objects can negatively
impact the overall coherence and video quality. To preservearXiv:2401.10150v4  [cs.CV]  8 Jan 2025Figure 1: Our Motion-Zero framework endows different pre-trained video diffusion models with the capability to manipulate
object trajectories directly, circumventing the need for supplementary training. By designating the target entity in the input
prompts and a sequence of bounding boxes, users can intuitively direct the motion path of the object within the generated video.
the continuity throughout the video sequence, a novel Shift
Temporal Attention Mechanism is proposed to further main-
tain the baseline model focus on the same objects across the
time axis. Through the implementation of these modules, our
proposed framework can effectively generate high-quality
video controlled by bounding-box trajectories without any
training process. Samples generated by Motion-Zero can be
viewed in Fig.1. The main innovations of our work can be
summarized as follows,
•We propose a zero-shot framework Motion-Zero, which
is capable of controlling generated object for arbitrary
trajectory within a pre-trained video generation diffusion
model. Our Motion-Zero is plug-and-play and without
any additional training.
•Our Initial Noise Prior Module is a novel way to acquire
semantic initial noise for high-quality generation. More-
over, Spatial Constraints and Shift Temporal Attention
Mechanism respectively exploit the cross-attention and
temporal-attention dimensions to obtain spatial control
and temporal consistency.
•Extensive experiments demonstrate that our proposed
work can be applied to pre-trained video diffusion mod-
els. Our method enables the original baseline to generate
objects with arbitrary trajectories.
Related Works
With the rapid development of deep learning (Chen et al.
2023b; Zheng et al. 2024b,a; Wang et al. 2023a; Sun et al.
2024b, 2023; Li et al. 2024b,a; Wang et al. 2024c,b; Yang, Li,
and Chen 2024; Cai et al. 2021, 2022, 2023; Chen et al. 2024,
2023a; Song et al. 2023b, 2024, 2023a; Sun, Li, and Wang
2023; Sun et al. 2024a), there are pre-trained methodologies
in place that have the capability to control the motion trajec-
tories of objects within generated videos. VideoComposer
(Wang et al. 2023c) employs a two-stage training strategy to
incrementally incorporate temporal information and control
signals. A motion condition encoder is proposed for train-
ing, designed to enhance the model’s ability to understand
and integrate motion-specific information. DragNUMA (Yin
et al. 2023) also employs a comparable training strategy and
utilizes optical flow as a conditioning mechanism for trajec-
tory modeling. MotionCtrl (Wang et al. 2023d) synthesizes
the approaches of these two works and introduces a camera
control module and a trajectory control module to furtherrefine the management of movement within generated video
content. Boximator (Wang et al. 2024a) proposes a novel
self-tracking technique to simplify the learning of box-object
relationships. MotionBooth (Wu et al. 2024) allows for con-
trolling the movement of a specific object by fine-tuning a
photo of that object. (Yu et al. 2024) enables the movement
of a specified object in a user’s input photo to generate a
video. Motion-I2V (Shi et al. 2024) leverages a two-stage
training process, allowing users to alter the motion in the
generated video by dragging. These models typically rely on
extensive training on large-scale datasets such as WebVid-
10M (Bain et al. 2021) and HD-VILA-100M (Xue et al. 2022)
which lead to expensive training costs. Also, these models are
typically constrained to operate on the specific models they
were trained on, lacking the flexibility to interchange base
video diffusion models. This limitation hinders their adapt-
ability and limits the scope of their application to only those
scenarios for which they were explicitly designed. Our pro-
posed model stands out in that it can be applied to any base
diffusion model without the necessity for further training.
Recently, Trailblazer (Ma, Lewis, and Kleijn 2023) enhances
attention on the box area in cross-attention with a zero-shot
setting, similar to our work. In contrast, our operation on
cross-attention is supervised by losses, providing better in-
terpretability and effectiveness. Peekaboo (Jain et al. 2024)
uses a masked attention module to achieve control without
the need for training. FreeTraj (Qiu et al. 2024) utilizes fre-
quency fusion to generate results whose trajectory is aligned
with the given box. Although these methods exploit zero-shot
settings in controllable video generalization, our proposed
method emphasises on the importance of the initial noise and
the control of spatial and temporal consistency, and achieves
superior performance.
Preliminaries
Video Diffusion Model: Video diffusion models are designed
to produce high-quality and diverse videos, guided by text
prompts. To save the computational costs, (Rombach et al.
2022) utilize a U-Net as a denoising model within a latent
space, significantly reducing the computational load in terms
of both time and space.
In detail, these models employ a Variational Autoencoder
(V AE), which comprises an encoder Eand a decoder D. The
encoder compresses the original video from pixel space into alatent representation, and the decoder reconstructs the video
from this latent space back to pixel space. The 3D U-Net
typically consists of a series of down-sampling blocks, mid-
dle blocks, and up-sampling blocks. Each block is equipped
with convolutional layers, spatial transformers, and temporal
transformers. The optimization of the 3D U-Net (denoted as
ϵθ) is executed through a noise prediction loss function:
L=Ez0,c,ϵ∼N(0,I),t[||ϵ−ϵθ(zt, t,c)||2
2], (1)
where z0is the latent code of the training videos, c is the
text prompt condition, ϵis the Gaussian noise added to the
latent code, and tis the time step. The noised latent code zt
is determined as:
zt=√atz0+√
1−atϵ,at=tY
i=1at, (2)
where atis a hyper-parameter used for controlling the noise
strength based on time t.
Motion Trajectory Control: Based on video diffusion, the
task of motion trajectory control is to precisely control the
motion trajectory of objects in generated videos. The opti-
mization objective can be formulated as:
L=Ez0,c,ϵ∼N(0,I),t[||ϵ−ϵθ(zt, t,c,B)||2
2]. (3)
Specifically, users can input a text condition calong with a
sequence of rectangular boxes B={(xf
1, yf
1),(xf
2, yf
2)}Nf,
where (xf
1, yf
1),(xf
2, yf
2)are the upper left and lower right
points of the box in frame f,Nfis the total frame number.
The boxes Bcorrespond to the position of the moving object.
Methodology
Overview
Existing trajectory control methods require large-scale video
training data to optimize Eq.3, leading to a high compu-
tational cost. Differently, our work falls into the setting of
zero-shot motion trajectory control. To this end, our proposed
Motion-Zero framework operates entirely during the infer-
ence stage, thus eliminating the need for training and making
it compatible with various pre-trained video diffusion models.
The pipeline is shown in Fig.2 (a). In the following section,
we provide a detailed presentation of our proposed Motion-
Zero framework and its components. Firstly, we introduce
a noise prior generation module to produce the initial noise
for diffusion. Then, we describe the designed constraints and
loss of the moving object position and spatial consistency.
Finally, our shift temporal attention mechanism is presented
to further improve the temporal consistency.
Initial Noise Prior Module
According to the theory of DDIM Inversion (Ho, Jain, and
Abbeel 2020), the initial noise has a significant impact on
the final generated outcome. We introduce Initial Noise Prior
Module (INPM) to leverage this property to provide a strong
prior for the position of the moving object. Several steps
are involved to integrate a moving object into a sequence of
frames with a coherent prior as shown in Fig.2 (b). Firstly,
given a prompt cand the boxes in the same location withAlgorithm 1: Initial Noise Prior Module
Input :c,B
Parameter :Nf,λp
Function :Pix(a,b)means the elements get from tensor a
in the range of box b
Output : Initial noise zT
1:z∗∼ N(0,I)▷random sample the first latent code
2:Vmeta←VideoDiffusion (z∗,c,B0)▷Video Dif-
fusion using SC, the first box and cas condition
3:zmeta←Encoder (Vmeta)
4:zI←DDIM _Inverse (zmeta)▷inverse the zmeta
5:zT←z∗
6:forall f=1,2,..., Nfdo
7:Pix(zf
T,Bf)←λp·Pix(zf
I,B0)+
(1-λp)·Pix(zf
T,Bf)▷local mixup operation
8:end for
9:return zT▷initial latent with position prior
{B0}Nf, a meta video Vmeta is sampled, z∗∼ N(0,I)as
latent input, using the baseline video diffusion model and our
proposed spatial constraints (introduced in the next section).
This generated video has the target object staying at the
location with {B0}Nfdue to the spatial constraints. It is
noteworthy that controlling the model to generate standing-
still objects is much easier than controlling the movement
of objects. Then, a video latent zmeta is generated based on
Vmeta from Encoder E. Once zmeta is prepared, we perform
a DDIM Inversion to obtain the corresponding noise latent
representation zI. We crop the latent representation within the
boxB0for each frame, creating a sequence of latent patches
containing the visual target. Subsequently, we use a local
mixup operation (Zhang et al. 2017) to mix the latent patches
and the initial noise z∗in the range of Bfframe by frame. Our
INPM allows us to set a coherent prior in the corresponding
object’s position in the initial noises. It also ensures that
the animated object maintains consistency in appearance
and movement across the video frames, without incurring
additional computational costs during the generation process.
Details are shown in Alg.1.
Spatial Constraints with Attention Map
The INPM alone is insufficient for precise manipulation of
an object’s trajectory. To further improve the capability of
control, we introduce Spatial Constraints (SC) deployed at
each denoising step tto optimize the intermediate latent rep-
resentation zt. This optimization is crucial for enhancing
the accuracy of the moving object position and preserving
spatial consistency. Within the conditional denoising archi-
tecture (Rombach et al. 2022), cross-attention serves as the
pivotal bridge that connects the text prompt with the content
generated. During the denoising steps, conditioned on the
prompt cand the intermediate features zt, the corresponding
cross-attention map can be obtained as A:
A=Softmax (QK⊤/√
d), (4)
Q=WQzt,K=WKc, (5)Figure 2: Overview of our Motion-Zero. The total pipeline is shown in (a). Given the box condition Band the prompt condition,
we generate the prior latents zTby our Initial Noise Prior Module (INPM) as shown on (b). At timestep t,ztis firstly optimized
toz′
tby the Spatial Constraints (SC). Subsequently, z′
tis passed to the UNet with Shift Temporal Attention Module (STAM) as
demonstrated on (c). All the parameters of the video diffusion are frozen. T1represents the number of timesteps during which
SC and STAM are applied, and T2denotes the number of timesteps where the original video diffusion process is utilized.
where Q,Kare the query and key with the ztandc, re-
spectively. WQ,WKare two learnable matrices, which are
frozen in our settings. Assuming a maximum number Npof
prompt tokens {p1, ...,pNp}in condition c, at time step t, it
is feasible to derive Npcross-attention maps {A1, ...,ANp}.
When a user specifies the index kof prompt tokens pk
intended to control the trajectory of an object, along with
the box B, the box-loss can be employed to ensure that our
model controls the object to appear within the input box in
every frame. Specifically, the box-loss is achieved through an
optimization approach that maximizes the values of the Ak
for the corresponding prompt tokens inside the box, while
minimizing the values outside the box. Frame by frame, we
scale down the user-specified coordinate box to the corre-
sponding coordinates in the latent space and construct a mask
Mfwhere 1for areas in the box and 0otherwise. To ensure
the values inside the box for attention are maximized, we
propose an intuitive solution Lias
Lif= 1−1
PX
g(Af
k·Mf, P), (6)
where g(·, P)means the top Phighest value will be selected.
If we employ all the values within the attention maps, it could
disrupt the stability of the denoising process. Conversely,
utilizing too few values could result in a less pronounced
control effect. On the other hand, we aim for the attention
values outside the box to be as minimal as possible which is
formulated as
Lof=1
PX
g(Af
k·(1−Mf), P). (7)
Within these two constraints, we can ensure that the object
generated in each frame is contained within the box. However,
there is no guarantee that the object will be positioned at thecenter of the box. To mitigate this, we propose another center-
lossLcto encourage the centroid of the Af
kto closely align
with the center of the box as
(WAf
k, HAf
k) =1P
w,hAf
k,w,h(P
w,hw·Af
k,w,h,P
w,hh·Af
k,w,h),(8)
Lcf=||(xf
1+xf
2
2,yf
1+yf
2
2)−(WAf
k, HAf
k)||1,(9)
where Af
k,w,h is the value of Af
kat the position of (w, h),
(WAf
k, HAf
k)is the centroid position of Af
kwith the k-th
token concept. After establishing robust control over the
position of the object, we recognize unexpected variations
in the object’s appearance due to the substantial extent of
movement. To ensure appearance consistency across frames,
we strive to maintain the uniformity of the Af
kwithin the box.
To this end, we introduce a similarity loss Lsas
Ls= 1−1
Nf−1PNf−1
f=1Sim(Pix(Af
i,Bf),Pix(Af+1
i,Bf+1)),(10)
where Sim(·,·)means the similarity of two elements and
Pix()is a function getting corresponding elements of the
mapAfwithin a box range Bf. Cosine similarity is adopted
here. At each timestep, the overall spatial constraints Lspare
formulated as follows:
Lsp=X
f(λiLif+λoLof+λcLcf) +λsLs, (11)
where λi, λo, λc, λsare hyper-parameters. By minimizing
and calculating the gradient of the Eq.11, we can optimize
our latent ztin Eq.2 as follows:
z′
t←zt−βt· ∇L sp, (12)
where βtlinearly decays at each timestep t. Specifically,
before denoising with the U-Net at each timestep t, we up-
datezttoz′
tusing Eq.12. Then, we continue the denoisingprocess using z′
t. Under the combined effect of the aforemen-
tioned constraints, the latent variable ztat each timestep will
gradually shift towards generating high-response attention
in the specified position, while ensuring that the appearance
attributes of the object within the box remain unchanged. Con-
sequently, the target object is synthesized within the bounding
box area provided by the user.
Shift Temporal Attention Mechanism
After applying the spatial constraints on attention maps, pre-
trained video diffusion models still encounter difficulties
in generating sequences of continuous actions. Within the
temporal module of the diffusion process, the latent represen-
tation is reshaped into the following configuration:
z′
(b·H·W+h·W+w,f,c )=z(b,f,c,h,w ) (13)
where z′represents the result of applying the rearrange op-
eration to z. Within the temporal transformer, attention is
focused on the same pixel across different frames. This leads
to a scenario where, if the extent of motion is too large, the
same position in different frames could undergo significant
semantic changes, resulting in a lack of coherence in the
generated dynamics.
To overcome this inconsistency, we propose a Shift Tempo-
ral Attention Mechanism (STAM) to improve the dynamics
of the moving object in different frames. Specifically, we
shift the elements of zfinside the Bfrange with the ele-
ments inside the B0range and use the overlapped parts to fill
in the vacated spaces, as shown in Fig.2 (c). Therefore, the
subsequent frames within the box range can be aligned with
the box range of the first frame. The steps are shown:
zf
w=Shift (zf,Bf,B0),
zw′=TemporalAttention (zw),
zf′=Shift (zf
w′,B0,Bf),(14)
where Shift (·, a, b)is the shift operation, zf
wmeans the
shifted latent zfof frame fandzw= [z1
w, ..,zf
w].By ap-
plying STAM to a TemporalAttention in the baseline
video diffusion models, we can achieve coherence in the
motion of moving objects without additional training, and
without incurring extra computational costs during inference.
Note that we do not use STAM in INPM as the motion of
objects in the meta video Vmeta generated within INPM oc-
curs at the same location. Therefore, there is no need to shift
temporal attention to align the positions of moving objects.
Experiments
We evaluate the effectiveness of our method from both quali-
tative and quantitative perspectives. In our experiments, the
default baseline is ZeroScope (Sterling 2023). ModelScope
(Wang et al. 2023b) is also employed to show that our method
can be applied to various video diffusion baselines. Trail-
Blazer (Ma, Lewis, and Kleijn 2023) and Peekaboo (Jain et al.
2024) are involved in experiments to demonstrate the con-
trol ability of our proposed method. Following TrailBlazer,
33 prompts containing different moving objects and motion
patterns are employed as the evaluation dataset. For simplemotions, we employed eight movement trajectories. For ex-
periments involving complex trajectories, we utilized 17 ran-
domly generated motion curves. Please refer to the appendix
for specific prompts and trajectory parameters.
Implementation Details Our algorithm is fully imple-
mented during the inference stage, thus it does not require
any training. The hyper-parameter λi, λoare set to 1, λcis
set to 0.05, λsis set to 0.5, λpis set to 0.8. To balance the
trade-off between the size of GPU memory consumption and
the semantic information retained in the attention map A, we
choose a 48×48size for Awhen the output resolution is
384×384. We use DDIM (Song, Meng, and Ermon 2020) as
our sampling method. In the experiment, T1in Fig.2 is set as
10, which means the SC and the STAM are employed during
the first 10 timesteps; T2is set to 20, thus the total denoising
timestep Tis 30. All of our experiments are conducted on a
single NVIDIA A100 GPU.
Comparisons with SOTA Methods
Qualitative Analysis. The results of the qualitative exper-
iments with simple and complex trajectories are shown in
Fig.3 and Fig.4, respectively. Our proposed Motion-Zero
(+Ours) applied on other baselines (ModelScope and Zero-
Scope) can greatly increase the controllability of the objects’
motion trajectories. In Fig.3, baselines (+Ours) refers to ap-
plying MotionZero to the baseline methods, while original
baselines (+Prompt) have an extra prompt added to indicate
the motion of objects: moving from left to right. As shown
in Fig. 3, Motion-Zero correctly guides the motion of fishes
by following the specific trajectories indicated by the red
boxes. In contrast, both original ModelScope (+Prompt) and
ZeroScope (+Prompt) fail to control the objects following
the expected trajectories. Fig.3 (c) demonstrates the gener-
ated results of TrailBlazer. It shows that the fish generated by
TrailBlazer does not strictly follow the red bounding boxes.
Fig.3 (d) demonstrates the results of Peekaboo. It is observed
that the object detaches from the box in the last few frames,
and the movement direction of the fish on the left does not
align with the direction of the box.
Fig.4 indicates the comparison between ours, TrailBlazer,
and Peekaboo in complex trajectories setting. Our method
effectively controls the motion of objects for any trajectory,
e.g. a flying back rocket. On the contrary, TrailBlazer exhibits
cases where moving objects are coupled with the background
motion, e.g. the penguin is relatively stationary. Furthermore,
the rocket and the rabbit are losing control and the motion
becomes irrelevant with the bounding boxes. The penguins
generated by Peekaboo lack dynamism, maintaining a single
pose while only the scene moves. It fails to properly generate
rockets, which might be due to the unusual nature of rotating
and moving rockets. Regarding the rabbit scene, the content
generated by Peekaboo exhibited frame skipping.
Quantitative Analysis. Following LOVEU-TGVE com-
petition (Wu et al. 2023b), we use the CLIP score (Hessel
et al. 2021) to verify text-video consistency (Text Align)
and inter-frame consistency (Consistency). The PickScore
(Kirstain et al. 2023) is employed to predict user preferences
of our model. To further evaluate the control capability ofFigure 3: Quality comparison results on different methods.
We take one frame from every three frames. The input prompt:
A fish is swimming in the sea. We employed ModelScope (a)
and ZeroScope (b) as our baseline models and compared the
effect of incorporating additional prompts with the integration
of our Motion-Zero. In addition, we conducted a comparative
analysis with TrailBlazer and Peekaboo.
our model, we employ metrics including mIoU, AP50, Cov.,
and CD. mIoU stands for mean Intersection-over-Union of
the detected bounding boxes and the input box on the gener-
ated video. This metric is primarily used to assess whether
the model can effectively control the position and size of
moving objects. We use the OWLViT-large detector (Min-
derer et al. 2022) to detect object boxes in the generated
video. AP50 refers to average precision@50%, which is used
to determine if the overlap between the detected boxes and
the user-provided boxes is greater than 50%. Cov. is used
to assess the probability of the detector detecting moving
objects. CD represents the distance between the center of the
moving object in the generated video and the center of the
user-provided box, reflecting whether the generated center of
the moving object is consistent with the user input.
As shown in Tab.1, the left part indicates the generation
quality, and it shows that our proposed Motion-Zero frame-
work does not compromise the baseline model’s performance
but increases video generation quality instead. This demon-
strates that our method exploits and preserves the model’s
generation capability even with the zero-shot setting. In ad-
dition, the motion subject constrained by the box results in
improved semantic accuracy and temporal consistency. This
indicates that our proposed Motion-Zero can get a better gen-
eration quality score due to the effectiveness of consistency
between the prompt and motion.
For the left in Tab.1, our method outperforms Trailblazer
in terms of control performance for both simple and complex
trajectories and surpasses Peekaboo in every aspect. Trail-
blazer performs better on the Align metric because it manu-
ally modifies cross-attention rather than deriving it through
loss. However, this diminishes its performance on other met-
rics. Through mIoU and AP50 metrics, we can see that the
size and position of the moving objects generated by ourAlign Cons. Pick. mIoU AP50 Cov. CD( ↓)
ZeroScope 20.31 0.88 18.98 - - - -
+Ours 21.96 0.94 19.89 0.54 0.64 0.96 0.07
ModelScope 20.55 0.91 18.70 - - - -
+Ours 23.54 0.90 19.56 0.54 0.64 0.88 0.08
TrailBlazer 21.53 0.92 19.64 0.52 0.64 0.91 0.14
Peekaboo 20.04 0.83 18.70 0.43 0.45 0.89 0.11
TrailBlazer (c.) 23.34 0.92 18.81 0.50 0.61 0.91 0.13
Peekaboo (c.) 20.89 0.84 18.53 0.41 0.42 0.88 0.11
Ours (c.) 22.40 0.95 19.19 0.55 0.67 0.97 0.07
Table 1: Automatic metric on baseline methods and SOTA
methods. The left half indicates the quality of the generation,
while the right half demonstrates the control capability of
the model. All metrics expect CD to be such that higher
values( ↑) indicate better performance. (c.) means the method
is tested in complex trajectories. Align means Text Align,
Cons. means Consistency and Pick. means PickScore.
Align Cons. Pick. mIoU AP50 Cov. CD( ↓)
w/o INPM 20.30 0.93 19.29 0.40 0.41 0.86 0.09
w/o SC 21.27 0.92 19.10 0.18 0.17 0.71 0.22
w/o STAM 20.64 0.93 19.27 0.43 0.47 0.83 0.13
Ours 21.96 0.94 19.89 0.54 0.64 0.96 0.07
Table 2: The table of ablation study on different modules.
method have a closer match for the boxes provided by the
users. This confirms that LoandLiindeed ensure that the
generated objects remain within the boxes given by the users.
Higher Cov. score demonstrates that the objects generated
by our method are clearer. This proves that our INPM and
STAM modules enable smoother motion of objects, while
Lsmaintains the consistency of the objects. At the same
time, CD indicates that the centers of the objects generated
by our method are closer to the centers of the boxes provided.
This reflects that Lccan position the generated objects at the
center of the box.
Ablation Study
Quantitative Analysis. We conduct ablation experiments on
different components of Motion-Zero, as shown in Tab.2. It
indicates that removing individual modules leads to a signifi-
cant decrease in the model’s performance. We observe that
when SC is removed, the scores of Align do not decrease
significantly. However, when STAM or INPM is removed,
the scores of Align decrease noticeably, demonstrating the
significant role of the STAM and INPM modules in improv-
ing the consistency between text and generated content. The
SC has the greatest impact on location control as the scores
drop from 0.54 to 0.18 in mIoU and from 0.64 to 0.17 in
AP50. Without the SC module, there is no guarantee for
objects’ position and spatial consistency between frames. Ad-
ditionally, the absence of STAM and INPM also results in
inferior performance in temporal consistency for moving ob-
jects as the Cov. drops and it becomes difficult for the model
to generate expected positioned objects as mIoU and AP50
decrease. Overall, the three modules are all important for
video generation and the STAM and INPM have more impact
on generation quality.Figure 4: Quality comparison results with complex trajectories. We take one frame from every three frames. The input prompt of
the first row: A penguin standing on an iceberg. The second row: A rocket launching into space from a launchpad. The third row:
A rabbit burrowing downwards into its warren. Zoom in for the best view.
Figure 5: Attention maps with different components. Prompt:
A seal walking on the ice.
Appearance Consistency Control
ZeroScope 2.97 2.80 2.27
ModelScope 2.57 2.55 2.23
Trailblazer 3.32 3.32 3.40
Peekaboo 3.96 3.84 3.55
Ours 4.75 4.57 4.67
Table 3: The table of user study.
Visualization of Attention Maps. We visualize the attention
maps in the cross-attention block, as shown in Fig.5. The
second row displays the cross-attention map at step 0 under
random initial noise conditions. It is observed that the atten-
tion values are random. As shown in the third row, when we
introduce INPM, we can see that the areas with high attention
response at step 0 are already approximately located at the po-
sitions of the user’s box. This reflects that INPM can provide
strong prior information for the position of moving objects.
After optimization by SC, in the fourth and fifth rows, we
can see that the areas with high response in the attention map
are accurately located at the box position and are close to the
position of the generated moving object. This demonstrates
that SC can enforce high attention values inside the box while
keeping the low response values outside the box.
User Study
We randomly pick 3 videos from 10 videos generated by each
model, 15 in total to be used in our user study. Every video
has 3 dimensions of evaluation rating: Appearance, Consis-
tency, and Control, and the scores 1 to 5 indicate appearance,consistency, and control capabilities from low to high. We
randomly choose 60 users from our university through an
incentivized questionnaire to evaluate each dimension of the
15 videos. At the beginning of our questionnaire, we provide
users with a detailed explanation of the task and the scoring
guidelines. Only after confirming that the user fully under-
stands these instructions do they proceed with the question-
naire. The results are shown in Tab.3. For baseline models,
additional prompts are used to control the movements. The
voters prefer our methods from all aspects compared with
baselines, Trailblazer, and Peekaboo. To further validate the
effectiveness of our user study, we use the Cronbach’s αco-
efficient to assess the internal consistency (reliability) of the
questionnaire. The Cronbach’s αcoefficient for our question-
naire is 0.901, indicating a high level of reliability. We use
the Friedman Test to confirm that our results are statistically
significant. The results indicate significant differences among
the five methods (Friedman Test, X2= 1534.29, p< 0.001).
Conclusion
In this paper, we proposed a novel zero-shot framework,
Motion-Zero, for arbitrary object motion trajectory control
that can be applied to various video diffusion models. Un-
like previous methods require extensive training, our method
enabled motion-control video generation without any fine-
tuning of the baseline model. Our Initial Noise Prior Mod-
ule was proposed to acquire prior initial noise for a high-
quality generation. Moreover, Spatial Constraints and Shift
Temporal Attention Mechanism respectively exploited the
cross-attention dimension and temporal-attention dimension
to obtain spatial control and temporal consistency. Extensive
experiments demonstrated the efficacy and generalization of
our proposed approach.
Acknowledgments
This work is supported by the National Natural Science
Foundation of China (62472178, 62376244), Fundamental
Research Funds for the Central Universities, and Shang-
hai Urban Digital Transformation Special Fund Project
(202301027). This work is also sponsored by Natural Sci-
ence Foundation of Chongqing, China (CSTB2022NSCQ-
MSX0552) and the Open Projects Program of State Key
Laboratory of Multimodal Artificial Intelligence Systems
(No.MAIS2024111).References
Bain, M.; Nagrani, A.; Varol, G.; and Zisserman, A. 2021.
Frozen in Time: A Joint Video and Image Encoder for End-
to-End Retrieval. In IEEE International Conference on Com-
puter Vision .
Blattmann, A.; Dockhorn, T.; Kulal, S.; Mendelevitch, D.;
Kilian, M.; Lorenz, D.; Levi, Y .; English, Z.; V oleti, V .;
Letts, A.; et al. 2023. Stable video diffusion: Scaling la-
tent video diffusion models to large datasets. arXiv preprint
arXiv:2311.15127 .
Cai, Y .; Chen, L.; Guan, H.; Lin, S.; Lu, C.; Wang, C.; and
He, G. 2023. Explicit invariant feature induced cross-domain
crowd counting. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 37, 259–267.
Cai, Y .; Chen, L.; Ma, Z.; Lu, C.; Wang, C.; and He, G.
2021. Leveraging intra-domain knowledge to strengthen
cross-domain crowd counting. In 2021 IEEE International
Conference on Multimedia and Expo (ICME) , 1–6. IEEE.
Cai, Y .; Ma, Z.; Lu, C.; Wang, C.; and He, G. 2022. Global
Representation Guided Adaptive Fusion Network for Stable
Video Crowd Counting. IEEE Transactions on Multimedia ,
25: 5222–5233.
Chen, L.; Cai, Y .; Lu, C.; Wang, C.; and He, G. 2023a. Video-
based spatio-temporal scene graph generation with efficient
self-supervision tasks. Multimedia Tools and Applications ,
82(25): 38947–38966.
Chen, L.; Song, Y .; Cai, Y .; Lu, J.; Li, Y .; Xie, Y .; Wang,
C.; and He, G. 2024. Multi-Prototype Space Learning for
Commonsense-Based Scene Graph Generation. In Proceed-
ings of the AAAI Conference on Artificial Intelligence , vol-
ume 38, 1129–1137.
Chen, Z.; Li, B.; Wu, S.; Jiang, K.; Ding, S.; and Zhang, W.
2023b. Content-based Unrestricted Adversarial Attack. In
Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.;
and Levine, S., eds., Advances in Neural Information Process-
ing Systems , volume 36, 51719–51733. Curran Associates,
Inc.
Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat
gans on image synthesis. Advances in neural information
processing systems , 34: 8780–8794.
Epstein, D.; Jabri, A.; Poole, B.; Efros, A. A.; and Holynski,
A. 2023. Diffusion self-guidance for controllable image
generation. arXiv preprint arXiv:2306.00986 .
Guo, Y .; Yang, C.; Rao, A.; Wang, Y .; Qiao, Y .; Lin, D.;
and Dai, B. 2023. Animatediff: Animate your personalized
text-to-image diffusion models without specific tuning. arXiv
preprint arXiv:2307.04725 .
Hessel, J.; Holtzman, A.; Forbes, M.; Bras, R. L.; and Choi,
Y . 2021. Clipscore: A reference-free evaluation metric for
image captioning. arXiv preprint arXiv:2104.08718 .
Ho, J.; Chan, W.; Saharia, C.; Whang, J.; Gao, R.; Gritsenko,
A.; Kingma, D. P.; Poole, B.; Norouzi, M.; Fleet, D. J.; et al.
2022. Imagen video: High definition video generation with
diffusion models. arXiv preprint arXiv:2210.02303 .
Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion
probabilistic models. Advances in neural information pro-
cessing systems , 33: 6840–6851.Jain, Y .; Nasery, A.; Vineet, V .; and Behl, H. 2024. Peekaboo:
Interactive video generation via masked-diffusion. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 8079–8088.
Kirstain, Y .; Polyak, A.; Singer, U.; Matiana, S.; Penna, J.;
and Levy, O. 2023. Pick-a-pic: An open dataset of user
preferences for text-to-image generation. arXiv preprint
arXiv:2305.01569 .
Li, Z.; Wang, W.; Cai, Y .; Qi, X.; Wang, P.; Zhang, D.; Song,
H.; Jiang, B.; Huang, Z.; and Wang, T. 2024a. Unifiedmllm:
Enabling unified representation for multi-modal multi-tasks
with large language model. arXiv preprint arXiv:2408.02503 .
Li, Z.; Xu, Q.; Zhang, D.; Song, H.; Cai, Y .; Qi, Q.; Zhou, R.;
Pan, J.; Li, Z.; Tu, V .; et al. 2024b. Groundinggpt: Language
enhanced multi-modal grounding model. In Proceedings of
the 62nd Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , 6657–6678.
Ma, W.-D. K.; Lewis, J. P.; and Kleijn, W. 2023. TrailBlazer:
Trajectory Control for Diffusion-Based Video Generation.
Minderer, M.; Gritsenko, A.; Stone, A.; Neumann, M.; Weis-
senborn, D.; Dosovitskiy, A.; Mahendran, A.; Arnab, A.;
Dehghani, M.; Shen, Z.; Wang, X.; Zhai, X.; Kipf, T.; and
Houlsby, N. 2022. Simple Open-V ocabulary Object Detection
with Vision Transformers. arXiv preprint arXiv:2205.06230 .
Qiu, H.; Chen, Z.; Wang, Z.; He, Y .; Xia, M.; and Liu, Z.
2024. FreeTraj: Tuning-Free Trajectory Control in Video
Diffusion Models. arXiv:2406.16863.
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-
mer, B. 2022. High-resolution image synthesis with latent dif-
fusion models. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , 10684–10695.
Shi, X.; Huang, Z.; Wang, F.-Y .; Bian, W.; Li, D.; Zhang,
Y .; Zhang, M.; Cheung, K. C.; See, S.; Qin, H.; et al. 2024.
Motion-i2v: Consistent and controllable image-to-video gen-
eration with explicit motion modeling. In ACM SIGGRAPH
2024 Conference Papers , 1–11.
Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion
implicit models. arXiv preprint arXiv:2010.02502 .
Song, S.; Chen, J.; Li, C.; and Wang, C. 2023a. GVQA:
Learning to Answer Questions about Graphs with Visual-
izations via Knowledge Base. In Proceedings of the 2023
CHI Conference on Human Factors in Computing Systems ,
CHI ’23. New York, NY , USA: Association for Computing
Machinery. ISBN 9781450394215.
Song, S.; Li, C.; Li, D.; Chen, J.; and Wang, C. 2024.
GraphDecoder: Recovering Diverse Network Graphs From
Visualization Images via Attention-Aware Learning. IEEE
Transactions on Visualization and Computer Graphics , 30(7):
3074–3088.
Song, S.; Li, C.; Sun, Y .; and Wang, C. 2023b. VividGraph:
Learning to Extract and Redesign Network Graphs From
Visualization Images. IEEE Transactions on Visualization
and Computer Graphics , 29(7): 3169–3181.
Sterling, S. 2023. ZeroScope.
Sun, Y .; Li, Y .; and Wang, C. 2023. Multi-Source Templates
Learning for Real-Time Aerial Tracking. In ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) , 1–5.
Sun, Y .; Yu, F.; Chen, S.; Zhang, Y .; Huang, J.; Li, C.; Li, Y .;
and Wang, C. 2024a. Chattracker: Enhancing visual tracking
performance via chatting with multimodal large language
model. arXiv preprint arXiv:2411.01756 .
Sun, Z.; Chen, S.; Yao, T.; Yi, R.; Ding, S.; and Ma, L. 2024b.
Rethinking Open-World DeepFake Attribution with Multi-
perspective Sensory Learning. International Journal of Com-
puter Vision .
Sun, Z.; Chen, S.; Yao, T.; Yin, B.; Yi, R.; Ding, S.; and
Ma, L. 2023. Contrastive Pseudo Learning for Open-World
DeepFake Attribution. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , 20882–
20892.
Wang, H.; Li, B.; Wu, S.; Shen, S.; Liu, F.; Ding, S.; and
Zhou, A. 2023a. Rethinking the learning paradigm for dy-
namic facial expression recognition. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recog-
nition , 17958–17968.
Wang, J.; Yuan, H.; Chen, D.; Zhang, Y .; Wang, X.; and
Zhang, S. 2023b. Modelscope text-to-video technical report.
arXiv preprint arXiv:2308.06571 .
Wang, J.; Zhang, Y .; Zou, J.; Zeng, Y .; Wei, G.; Yuan,
L.; and Li, H. 2024a. Boximator: Generating rich and
controllable motions for video synthesis. arXiv preprint
arXiv:2402.01566 .
Wang, W.; Li, Z.; Xu, Q.; Cai, Y .; Song, H.; Qi, Q.; Zhou, R.;
Huang, Z.; Wang, T.; and Xiao, L. 2024b. QCRD: Quality-
guided Contrastive Rationale Distillation for Large Language
Models. arXiv preprint arXiv:2405.13014 .
Wang, W.; Li, Z.; Xu, Q.; Li, L.; Cai, Y .; Jiang, B.; Song,
H.; Hu, X.; Wang, P.; and Xiao, L. 2024c. Advancing Fine-
Grained Visual Understanding with Multi-Scale Alignment
in Multi-Modal Models. arXiv preprint arXiv:2411.09691 .
Wang, X.; Yuan, H.; Zhang, S.; Chen, D.; Wang, J.; Zhang,
Y .; Shen, Y .; Zhao, D.; and Zhou, J. 2023c. VideoComposer:
Compositional Video Synthesis with Motion Controllability.
arXiv preprint arXiv:2306.02018 .
Wang, Z.; Yuan, Z.; Wang, X.; Chen, T.; Xia, M.; Luo, P.;
and Shan, Y . 2023d. MotionCtrl: A Unified and Flexible
Motion Controller for Video Generation. arXiv preprint
arXiv:2312.03641 .
Wu, J.; Li, X.; Zeng, Y .; Zhang, J.; Zhou, Q.; Li, Y .; Tong, Y .;
and Chen, K. 2024. Motionbooth: Motion-aware customized
text-to-video generation. arXiv preprint arXiv:2406.17758 .
Wu, J. Z.; Ge, Y .; Wang, X.; Lei, S. W.; Gu, Y .; Shi, Y .; Hsu,
W.; Shan, Y .; Qie, X.; and Shou, M. Z. 2023a. Tune-a-video:
One-shot tuning of image diffusion models for text-to-video
generation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , 7623–7633.
Wu, J. Z.; Li, X.; Gao, D.; Dong, Z.; Bai, J.; Singh, A.;
Xiang, X.; Li, Y .; Huang, Z.; Sun, Y .; He, R.; Hu, F.; Hu,
J.; Huang, H.; Zhu, H.; Cheng, X.; Tang, J.; Shou, M. Z.;
Keutzer, K.; and Iandola, F. 2023b. CVPR 2023 Text Guided
Video Editing Competition. arXiv:2310.16003.Xie, J.; Li, Y .; Huang, Y .; Liu, H.; Zhang, W.; Zheng, Y .; and
Shou, M. Z. 2023. Boxdiff: Text-to-image synthesis with
training-free box-constrained diffusion. In Proceedings of
the IEEE/CVF International Conference on Computer Vision ,
7452–7461.
Xue, H.; Hang, T.; Zeng, Y .; Sun, Y .; Liu, B.; Yang, H.;
Fu, J.; and Guo, B. 2022. Advancing high-resolution video-
language representation with large-scale video transcriptions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 5036–5045.
Yang, L.; Li, Y .; and Chen, L. 2024. ClothPPO: A Proxi-
mal Policy Optimization Enhancing Framework for Robotic
Cloth Manipulation with Observation-Aligned Action Spaces.
In Larson, K., ed., Proceedings of the Thirty-Third Interna-
tional Joint Conference on Artificial Intelligence, IJCAI-24 ,
6895–6903. International Joint Conferences on Artificial In-
telligence Organization. Main Track.
Yin, S.; Wu, C.; Liang, J.; Shi, J.; Li, H.; Ming, G.; and
Duan, N. 2023. Dragnuwa: Fine-grained control in video
generation by integrating text, image, and trajectory. arXiv
preprint arXiv:2308.08089 .
Yu, S.; Fang, J. Z.; Zheng, J.; Sigurdsson, G.; Ordonez, V .;
Piramuthu, R.; and Bansal, M. 2024. Zero-shot controllable
image-to-video animation via motion decomposition. In
Proceedings of the 32nd ACM International Conference on
Multimedia , 3332–3341.
Zhang, D. J.; Wu, J. Z.; Liu, J.-W.; Zhao, R.; Ran, L.; Gu, Y .;
Gao, D.; and Shou, M. Z. 2023. Show-1: Marrying pixel and
latent diffusion models for text-to-video generation. arXiv
preprint arXiv:2309.15818 .
Zhang, H.; Cisse, M.; Dauphin, Y . N.; and Lopez-Paz, D.
2017. mixup: Beyond empirical risk minimization. arXiv
preprint arXiv:1710.09412 .
Zhang, L.; Rao, A.; and Agrawala, M. 2023. Adding Con-
ditional Control to Text-to-Image Diffusion Models. ArXiv ,
abs/2302.05543.
Zheng, T.; Geng, C.; Jiang, P.; Wan, B.; Zhang, H.; Chen,
J.; Wang, J.; and Li, B. 2024a. Non-uniform Timestep Sam-
pling: Towards Faster Diffusion Model Training. In Cai, J.;
Kankanhalli, M. S.; Prabhakaran, B.; Boll, S.; Subramanian,
R.; Zheng, L.; Singh, V . K.; César, P.; Xie, L.; and Xu, D.,
eds., Proceedings of the 32nd ACM International Conference
on Multimedia, MM 2024, Melbourne, VIC, Australia, 28
October 2024 - 1 November 2024 , 7036–7045. ACM.
Zheng, T.; Jiang, P.; Wan, B.; Zhang, H.; Chen, J.; Wang, J.;
and Li, B. 2024b. Beta-Tuned Timestep Diffusion Model. In
Leonardis, A.; Ricci, E.; Roth, S.; Russakovsky, O.; Sattler,
T.; and Varol, G., eds., Computer Vision - ECCV 2024 - 18th
European Conference, Milan, Italy, September 29-October 4,
2024, Proceedings, Part III , volume 15061 of Lecture Notes
in Computer Science , 114–130. Springer.The appendix provides additional details and in-depth anal-
yses of Motion-Zero. The code will be released after accep-
tance. The structure of the appendix is as follows:
• Comparation with SOTA pre-trained methods.
• A more comprehensive ablation study.
• The details of the evaluation dataset.
• Discuss, limitations and future work.
• Border Impacts.
Compare with Pre-trained SOTA methods
To evaluate the effectiveness of our method, we compare
it with the current state-of-the-art pre-trained method for
controlling the trajectories of moving objects, MotionCtrl
(Wang et al. 2023d). As shown in Fig. 6, our results are
comparable to those of the pre-trained model in terms of
controlling the direction of the fish’s movements. In the case
of generating a bear, MotionCtrl only manages to rotate the
bear’s body, whereas Motion-Zero can effectively generate
the bear’s movement. Note that our method additionally al-
lows for the control of their size. Our advantage also lies in
the ability to plug and play with base models without the
need for any training.
Ablation Study
Ablation on Main Components We conduct a series of ab-
lation experiments to verify the effectiveness of our modules
individually.
Impact of INPM. INPM is designed to provide a prior for
the initial position of moving objects, facilitating the model
to more stably control the position of the object’s movement.
As shown in Fig.7, the second row demonstrates the effects
of removing this module. We can see that the lion is not
entirely within the frame, and due to the forceful control by
SC, the lion’s body undergoes significant deformation, while
the lion’s head shows no movement at all. For Motion-Zero,
the lion’s body is clearly generated, and the lion’s head also
moves with the changes in the frame, resulting in the high
overall stability of the lion. This shows that the INPM module
plays a significant role in stably controlling the position of
objects and generating stable videos.
Impact of SC. In this section, we validate the importance
of SC. SC primarily provides auxiliary support for control
over spatial positioning. As shown in the third line in Fig.7,
without spatial constraints, the generated motion of the lion
appears mostly outside the box. The SC is the most important
module for our Motion-Zero controlling method. The reason
is that SC is the only gradient-based modification to the
baseline compared with other modules.
Impact of STAM. The role of the STAM is to ensure the
temporal consistency of the moving object, which is to main-
tain the continuity of its motion as it moves. As illustrated in
Fig.7, we present the effects of omitting the STAM module
in the last row. It can be noted that during the movement, the
lion’s legs undergo deformation and become blurred. In con-
trast, the first row demonstrates that the lion’s leg movements
are well-generated with strong continuity, highlighting the
significance of our STAM. More details are provided in the
supplementary materials.Ablation on Parameter T1T1determines the timesteps at
which the SC and STAM modules act during the diffusion
denoising process. As shown in the fig. 8, when no constraints
are applied, the squirrel essentially shows no displacement.
When T1is set to 10, the squirrel moves with the box while
maintaining its stability. When T1is 20, although the squirrel
moves with the box, its shape does not remain consistent. At
T1of 30, even though the moving object is still within the
box, it is almost unrecognizable as a squirrel. The conclusion
is that a higher T1provides better control but significantly
reduces clarity and object identity maintenance. From the
experiments, the optimal value for T1is 10.
Ablation on Parameter λiandλoIn SC, LiandLoare
designed to increase the values of Acorresponding to the
prompt inside the box and decrease the values of Afor the
prompt outside the box. As depicted in Fig.9, the variations
in the video generation effects with λiandλovalues ranging
from 0.0 to 1.0 were tested. From the first row, it can be seen
that without the use of λiandλo, the box does not effectively
control the outcome. As λiandλoare incremented, it is
evident that the chicken increasingly appears concentrated
within the box. In the final row, it is observed that the box
can perfectly control the chicken’s movement trajectory.
Effectiveness of Parameter λsIn SC, Lsis used to main-
tain the object’s consistency across frames. As shown in the
Fig.10, the effects of the output video were tested with λsat
0.0, 0.5, and 1.0. It is observed that when λsis set to 0.0, the
orientation and some of the rhinoceros’s external features do
not maintain consistency. With λsat 0.5, the appearance of
the rhinoceros is observed to be consistent throughout. How-
ever, when λsis increased to 1.0, it is noted that although
the object within the box maintained consistency, the overall
semantics of the video were disrupted. From the experiment,
the optimal value for λswas determined to be 0.5.
Effectiveness of Parameter λcIn SC, Lcis utilized to
ensure that the center of the object approaches the center
of the given box. As illustrated in Fig.11, evaluations are
conducted to observe the control effects on video generation
when λcwas set to 0.00, 0.05, and 0.10. It is demonstrated
that with λcat 0.00, although the seal remains inside the box,
its center does not align with the center of the box, resulting
in the seal not moving with the box but instead changing
direction. At a λcvalue of 0.05, the seal is observed to move
perfectly with the motion of the box. When λcis increased
to 0.10, the background become blurry, and the seal’s body
appeared distorted. Based on the experimental evidence, a λc
value of 0.05 yielded the best control outcome.
Influence of Parameter λpIn the INPM, λis used to con-
trol the ratio between the DDIM Inversion (Song, Meng, and
Ermon 2020) prior noise zIand the standard normal noise
z∗. As shown in Fig.14, we utilized two sets of prompts and
varied the parameter λpfrom 0.0 to 1.0 in increments of 0.2
to demonstrate the effects of different λpvalues. We observed
that when the value of λpis set to 0.0, the video featuring
sheep on the left side shows multiple sheep appearing. On the
right side, the pig does not appear within the frame. Instead
of moving from left to right, it seems more like the pig isturning, with little change in the position of its head. When
λpwas increased to 0.2, it was noted that the sheep on the left
side were reduced to a single individual, which represented
an improvement. However, the positioning of the sheep did
not effectively correspond with the movements of the frame.
For the pig depicted on the right side, its position appeared to
be relatively static, not following the shifts in the frame, with
only the head showing movement through turning. With λp
set between 0.4 and 1.0, the movement of the sheep on the
left was found to be well-controlled. However, the motion
trajectory of the pig on the right proved challenging to con-
trol at values of λpat 0.4, 0.6, and 1.0. It was only when λp
reached 0.8 that the pig’s movement trajectory was perfectly
managed. It can be concluded that the INPM plays a crucial
role in controlling object positioning. Based on experimental
observations, a λpvalue of 0.8 has emerged as the optimal
empirical setting.
Details of Evaluation Datasets
In this section, we elaborate on the validation dataset de-
scribed in Section 5.2.2 of the main text.
Evaluation Prompts. Similar to Trailblazer (Ma, Lewis, and
Kleijn 2023), we used 33 prompts with various subjects and
motions as the evaluation dataset, as shown below:
• A woodpecker climbing up a tree trunk.
• A squirrel descending a tree after gathering nuts.
• A bird diving towards the water to catch fish.
• A frog leaping up to catch a fly.
• A parrot flying upwards towards the treetops.
• A squirrel jumping from one tree to another.
• A rabbit burrowing downwards into its warren.
• A satellite orbiting Earth in outer space.
• A skateboarder performing tricks at a skate park.
• A leaf falling gently from a tree.
• A paper plane gliding in the air.
• A bear climbing down a tree after spotting a threat.
• A duck diving underwater in search of food.
• A kangaroo is hopping down a gentle slope.
• An owl swooping down on its prey during the night.
• A balloon drifting across a clear sky.
• A bus moving through London streets.
• A plane flying high in the sky.
• A helicopter hovering above a cityscape.
• A streetcar trundling down tracks in a historic district.
• A rocket launching into space from a launchpad.
• A deer standing in a snowy field.
• A horse grazing in a meadow.
• A fox sitting in a forest clearing.
• A swan floating gracefully on a lake.
• A panda munching bamboo in a bamboo forest.
• A penguin standing on an iceberg.
• A lion lying in the savanna grass.• An owl perched silently in a tree at night.
• A dolphin just breaking the ocean surface.
• A camel resting in a desert landscape.
• A kangaroo standing in the Australian outback.
• A colorful hot air balloon tethered to the ground.
Evaluation Trajectories. Following MotionCtrl (Wang et al.
2023d), we use 8 simple trajectories (Fig.12) and 17 complex
trajectories (Fig.13) to test the robustness of our method.
Discussion, Limitations and Future Works
Due to the absence of the need for additional training, our
method can be applied to all pre-trained video diffusion mod-
els. However, on the other hand, its generative performance
is entirely dependent on the base model. Existing video dif-
fusion models sometimes generate relatively unstable ob-
jects, such as deformation in moving objects, holes in the
background, and various issues like the inability to precisely
control the attributes of generated objects with text.
The hyper-parameters presented in our paper are empirical
values that we have found to be applicable in most cases.
If we need to apply our algorithm to a new base model,
appropriate parameter adjustments can yield better results.
Our method can currently effectively control the move-
ment of objects in videos, but the trajectory of the movement
is entirely controlled by the user, lacking semantic interac-
tion with the video. For example, in a forest scene, if a user
provides the first frame as a reference, they can use a prompt
likea little rabbit goes to the stream to drink water to control
a rabbit to navigate and avoid obstacles in the forest automat-
ically. This kind of interaction between object movement and
the background, as well as narrative-like semantic control,
will be the direction of our future work.
Broader Impacts
Our approach is based on video diffusion models, and it does
not require training, so in principle, both potential positive
societal impacts and negative societal impacts of our method
on society are the same as the potential impacts that the video
diffusion model may have on society. Furthermore, our ap-
proach can make the generation of controllable videos easier,
allowing the generated videos to better match user expec-
tations, and potentially improving the efficiency of social
production.    +Ours     +Ours(a) ModelScope (b) ZeroScope
 (c)TrailBlazer
 (d) Peekaboo
 (e) MotionCtrl
Figure 6: Comparisons with SOTA methods. The prompt of the left part is A fish is swimming in the sea. Right: A bear is walking
on the grass. Zoom in for the best view.
Ours W/O  STAM W/O INPM
 W/O SC
Figure 7: Ablation studies on different components. We use
ZeroScope as our baseline model. To demonstrate the co-
herence of the generated object’s motion, we captured every
other frame, resulting in a total of 6 frames. The prompt is A
lion is walking on the field. Zoom in for the best view.
0
10
20
30Figure 8: Ablation studies on T1ranging from 0 to 30. The
prompt is A squirrel descending a tree after gathering nuts.
Zoom in for the best view.0.0
0.2
0.4
0.6
0.8
1.0Figure 9: Ablation studies on different λiandλorange from
0.0 to 1.0. The prompt: A chicken is walking in the farm.
0.0
0.5
1.0
Figure 10: Ablation studies on different λsvalues of 0.0, 0.5,
1.0. The prompt: A rhinoceros is walking on the field.
0.00
0.05
0.10
Figure 11: Ablation studies on different λcvalues of 0.00,
0.05, 0.10. The prompt part: A seal is walking on the ice.
Figure 12: Simple trajectory evaluation dataset, consisting of
eight simple trajectories. The blue points represent the center
of the boxes. The color of the bounding box in the first frame
is the lightest, while the color of the bounding box in the last
frame is the darkest.
Figure 13: Complex trajectory evaluation dataset, consisting
of seventeen complex trajectories. The blue points represent
the center of the boxes. The color of the bounding box in the
first frame is the lightest, while the color of the bounding box
in the last frame is the darkest.0.0
0.2
0.4
0.6
0.8
1.0Figure 14: Ablation studies on different λprange from 0.0 to 1.0. The prompt of the left part: A sheep is walking on the field.
Right part: A pig is walking on the grass.