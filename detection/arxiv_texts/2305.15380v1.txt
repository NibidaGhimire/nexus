arXiv:2305.15380v1  [cs.CL]  24 May 2023Sentiment Analysis Using Aligned Word Embeddings for Urali c
Languages
Khalid Alnajjar
Rootroo Ltd
Helsinki, Finland
name@rootroo.comMika Hämäläinen
Rootroo Ltd
Helsinki, Finland
name@rootroo.comJack Rueter
University of Helsinki
Finland
first.last@helsinki.fi
Abstract
In this paper, we present an approach for
translating word embeddings from a ma-
jority language into 4 minority languages:
Erzya, Moksha, Udmurt and Komi-Zyrian.
Furthermore, we align these word em-
beddings and present a novel neural net-
work model that is trained on English
data to conduct sentiment analysis and
then applied on endangered language data
through the aligned word embeddings. To
test our model, we annotated a small sen-
timent analysis corpus for the 4 endan-
gered languages and Finnish. Our method
reached at least 56% accuracy for each en-
dangered language. The models and the
sentiment corpus will be released together
with this paper. Our research shows that
state-of-the-art neural models can be used
with endangered languages with the only
requirement being a dictionary between
the endangered language and a majority
language.
1 Introduction
Most of the languages spoken in the world are en-
dangered to one degree or another. The fact of be-
ing endangered sets some limitations on how mod-
ern NLP research can be done with such languages
given that many endangered languages do not have
vast textual resources available online, and even
with the resources that are available, there is a
question about the quality of the data resulting
from a variety of factors such as ﬂuency of the
author, soundness of spelling and, on the lowest
level, inconsistencies in character encoding (see
Hämäläinen 2021).
This paper focuses on the following Uralic
languages: Erzya (myv), Moksha (mdf), Komi-
Zyrian (kpv) and Udmurt (udm). Unesco clas-
siﬁes these languages as deﬁnitely endangered(Moseley, 2010). In terms of NLP, these languages
have FSTs (Rueter et al., 2020, 2021), Universal
Dependencies Treebanks (Partanen et al., 2018;
Rueter and Tyers, 2018) (excluding Udmurt) and
constraint grammars available in Giella reposito-
ries (Moshagen et al., 2014). For some of the lan-
guages, there have also been efforts in employ-
ing neural models in disambiguation (Ens et al.,
2019) and morphological tasks (Hämäläinen et al.,
2021). Out of these languages, only Erzya has
several neural based models available such as ma-
chine translation models (Dale, 2022), a wav2vec
model and a Stanza model (Qi et al., 2020).
In this paper, we present a method for trans-
lating word embeddings models from larger lan-
guages into the endangered languages in question.
Furthermore, we ﬁne-tune the models with lan-
guage speciﬁc text data, align them and show re-
sults in a sentiment analysis task where no train-
ing data is provided in any of the endangered lan-
guages. We have made our data and models pub-
licly available on Zenodo1.
2 Related work
Apart from the work described earlier in the con-
text of the endangered languages in question, there
has been a lot of previous work on multilingual
NLP where a model is trained in one language to
sentence classiﬁcation and then applied in the con-
text of other languages. In this section, we will
describe some of those approaches together with
sentiment analysis approaches.
A recent paper demonstrates sentiment analy-
sis on 100 languages (Yilmaz et al., 2021). The
authors use RoBERTa-XLM to extract feature
vectors. These are then used in training a bi-
directional LSTM based classiﬁer model. An-
other line of work (Liu and Chen, 2015) compares
several different multilabel classiﬁcation methods
1https://zenodo.org/record/7866456on the task of sentiment analysis showing that
RAkEL (Tsoumakas et al., 2010) gave the best
performance on raw token input. A recent paper
(Hämäläinen et al., 2022) demonstrated promising
results in French sentiment analysis on a model
that was trained in English, Italian, Spanish and
German. The approach relied on a multilingual
BERT (Devlin et al., 2019). Öhman (2021) sug-
gests that lexicon based approaches, while vi-
able for endangered languages, are not particularly
suitable for sentiment analysis.
In the context of cross-lingual NLP, there is
work on POS tagging. For instance, Kim et al.
2017 propose a new model that does not require
parallel corpora or other resources. The model
uses a common BLSTM for knowledge transfer
and another BLSTM for language-speciﬁc repre-
sentations. It is trained using language-adversarial
training and bidirectional language modeling as
auxiliary objectives to capture both language-
general and language-speciﬁc information.
Another line of work by Xu et al. 2018 fo-
cuses on cross-lingual transfer of word embed-
dings, which aims to create mappings between
words in different languages by learning transfor-
mation functions over corresponding word embed-
ding spaces. The proposed algorithm simultane-
ously optimizes transformation functions in both
directions by using distributional matching and
minimizing back-translation losses. This approach
uses a neural network implementation to calculate
the Sinkhorn distance, a distributional similarity
measure, and optimize objectives through back-
propagation.
For machine translation Chen et al. 2022
demonstrate the importance of both multilingual
pretraining and ﬁne-tuning for effective cross-
lingual transfer in zero-shot translation using a
neural machine translation (NMT) model. The
paper presents SixT+, a many-to-English NMT
model that supports 100 source languages but is
trained on a parallel dataset in only six languages.
SixT+ initializes the decoder embedding and full
encoder with XLM-R large (Conneau et al., 2020)
and trains encoder and decoder layers using a
two-stage training strategy.
3 Data
We use two books, Suomi eilen ja nyt ( Fin-
land yesterday and now ) by Häkkinen (1997) and
Павлик Морозов (Pavlik Morozov) by Gubarev(1948) both of which are available in Finnish,
Erzya, Moksha, Komi-Zyrian and Udmurt. The
sentences of the books have been aligned across
all the languages at the Research Unit for V olgaic
Languages in University of Turku. The size of the
corpus for each language can be seen in Table 1.
tokens sentences
Finnish 43k 3.1k
Erzya 50k 3.6k
Moksha 51k 3.4k
Komi-Zyrian 50k 3.3k
Udmurt 53k 3.6k
Table 1: The corpus size for each language
Out of the entire corpus, we annotate 35 nega-
tive sentences and 33 positive sentences for eval-
uation for Finnish. We use the alignment infor-
mation to project this annotation for the rest of
the languages as well and verify manually that the
sentences express the same sentiment in each lan-
guage. This forms our test corpus for sentiment
analysis that consists of altogether 68 sentiment
annotated sentences.
Furthermore, we lemmatize all the texts us-
ing the FSTs provided in UralicNLP (Hämäläinen,
2019). The corpus is lemmatized because we in-
tend to translate and align a lemmatized word em-
beddings model. This also makes the overall ap-
proach more robust given that covering the en-
tire morphology of a language would require us
to have much larger corpora.
4 Word embeddings
Word embeddings capture the semantic and syn-
tactic links between words by constructing vec-
tor representations of words. These vectors
can be utilized to measure the semantic simi-
larity between words, ﬁnd analogous concepts,
cluster words (Hämäläinen and Alnajjar, 2019;
Stekel et al., 2021) and more. In this work, we
use English and Finnish as the big languages that
facilitate aligning and classifying words and sen-
tences for the endangered languages. English has
an overnumerous amount of linguistic resources,
whether as raw text or labeled data, while the en-
dangered resources that we are working with have
translation dictionaries for Finnish. For this rea-
son, we use Finnish as the intermediate language
that bridges these endangered languages with En-
glish resources.
The English model that we utilize is trained on
the English Wikipedia dump of February 2017 andGigaword 5th edition2(Fares et al., 2017). For
Finnish, we used recent word embeddings trained
by Language Bank of Finland (2022). These em-
beddings have been trained on several Finnish
newspapers. Both of these models have been
trained on lemmatized text.
The English word vectors have a dimension size
of 300, while the Finnish word vectors have a di-
mension size of 100. In order to make the dimen-
sion sizes of the two sets of embeddings compati-
ble, dimensionality reduction is applied to the En-
glish embeddings using principal component anal-
ysis (PCA) (Tipping and Bishop, 1999). This pro-
cess reduces the dimensionality of the English em-
beddings to 100, allowing them to be compared
and analyzed alongside the Finnish embeddings.
4.1 Creation of embeddings
We aim to create word embeddings for endangered
languages, which currently lack pre-existing em-
beddings. We use dictionaries from GiellaLT3,
which we augment using graph-based methods to
predict new translations through the Ve’rdd4plat-
form (Alnajjar et al., 2022, 2021). We present the
number of dictionary translations from each en-
dangered language to Finnish that we obtained
from the base dictionaries and predictions in Ta-
ble 2.
Translations Predictions Total
kpv 10983 14421 25404
mdf 36235 3903 40138
myv 18056 5018 23074
udm 36502 6966 43468
Table 2: Number of translations and predictions
from the source languages to Finnish
To create embeddings for the endangered lan-
guages, we adopt a method of cloning the Finnish
embeddings and substituting the Finnish lemma
with its corresponding translation in the endan-
gered language. Where translations were absent,
we omitted the word vector. The resulting embed-
dings consist of 7,908, 10,338, 7,535, and 9,505
word vectors for kpv, mdf, myv, and udm, respec-
tively. The lower number of word coverage can be
attributed to multi-word expressions present in the
dictionaries but not the embeddings.
In the next step of our study, we ﬁne-tuned the
word embeddings for both Finnish and the endan-
2http://vectors.nlpl.eu/repository/20/17.zip
3https://github.com/giellalt
4https://akusanat.com/verdd/gered languages by using two books as additional
data sources. This involved expanding the vocab-
ulary of each embeddings model whenever a new
word was encountered in the data. We also ad-
justed the embeddings weights based on the co-
occurrences of words in the text, using a window
size of 5 and a minimum count of 5 for a word to
be considered in the vocabulary. After completing
this process, the vocabulary size of the endangered
language embeddings were 10,396, 11,877, 9,030,
and 11,080, in the same order as mentioned above.
4.2 Alignment of embeddings
Our goal here is to align the Finnish word embed-
dings with the English ones, followed by align-
ing the embeddings of endangered languages to
the Finnish embeddings, in a supervised manner.
This was achieved by creating alignment dictio-
naries and aligning the embedding spaces together
similarly to Alnajjar (2021).
To align Finnish embeddings with English, we
used the Fin-Eng dictionary by Ylönen (2022),
which is based on the March 2023 English Wik-
tionary dump. We also used the Finnish-English
dictionaries provided by MUSE (Conneau et al.,
2017). Regarding the endangered languages, we
use the XML dictionaries to align them with
Finnish. We set aside 20% of the Wiktionary and
XML data for testing the alignments.
One thing that we have noticed is the lack of
the words “no” and “not” in the English embed-
dings due to stopword removal. To address this,
we appended a translation from “not” to “nt” in
the Finnish-English alignment data used in the
training stage. Whenever the text contained these
words, they were automatically mapped to “nt” in
the following steps of our research.
We followed the approach described by
MUSE (Conneau et al., 2017) to align all the em-
beddings, with 20 iterations of reﬁnement to align
Finnish with English and 5 iterations to align all
the other languages to Finnish.
5 Sentence embeddings
Word embeddings represent the meaning of a sin-
gle word, whereas sentence embeddings repre-
sent the meaning of an entire sentence or docu-
ment. Sentence embeddings are capable of cap-
turing more the context and excel at tasks that call
for comprehension of the meaning of a whole text,
such as sentiment analysis. Hence, we build sen-Language Label Precision Recall F1-Score Accuracy
engneg 0.77 0.76 0.760.76pos 0.75 0.76 0.76
ﬁnneg 0.77 0.75 0.760.75pos 0.73 0.75 0.74
kpvneg 0.57 0.57 0.570.56pos 0.55 0.55 0.55
mdfneg 0.63 0.65 0.640.63pos 0.64 0.62 0.63
myvneg 0.71 0.69 0.700.69pos 0.67 0.69 0.68
udmneg 0.69 0.63 0.660.63pos 0.58 0.63 0.60
Table 3: Precision, recall, f1-score and accuracy for each l anguage and label
tence embeddings for English that are based on the
English word embeddings.
The procedure for creating sentence embed-
dings was conducted by averaging the word em-
beddings of a given sentence and subsequently
feeding them to two fully-connected feed-forward
layers, thereby constructing a Deep Averaging
Network (DAN). The sentence embeddings are
trained on the STS Benchmark (Cer et al., 2017)
using SBERT, a method for sentence embeddings
that was proposed by (Reimers and Gurevych,
2019).
6 Sentiment analysis
We create a sentiment classiﬁer that takes in
the sentence embeddings and predicts a senti-
ment polarity label. For training the sentiment
analysis model, we use the Stanford Sentiment
Treebank (Socher et al., 2013), Amazon Reviews
Dataset (McAuley and Leskovec, 2013) and Yelp
Dataset5. These datasets are available in English
and we use their sentiment annotations (positive-
negative) to train our model.
The sentiment classiﬁer is constructed as a
three-layer fully-connected network, wherein the
hidden layers are comprised of 300 neurons each.
In order to mitigate overﬁtting, a dropout opera-
tion (Srivastava et al., 2014) is performed prior to
the ﬁnal classiﬁcation layer. The model consists
of 121,202 trainable parameters in total, and is
trained over the course of three epochs.
7 Results
In this section, we show the results of the sen-
timent classiﬁcation model on the in-domain,
English-language train splits of the sentiment cor-
pora we used to train the model. Furthermore,
5https://www.yelp.com/datasetwe show the results of the sentiment classiﬁcation
model when applied on our own annotated data for
the 4 endangered Uralic languages in question and
Finnish. These results can be seen in Table 3.
All in all, our model performs relatively well.
The accuracy for Finnish is almost as high as it
is for English despite not having any Finnish sen-
timent annotated training data. This means that
our approach can achieve rather good results when
there is a lot of translation data available between
the two languages. The results drop for the endan-
gered languages, but we do ﬁnd the 69% accuracy
for Erzya to be quite formidable, however, the re-
sult for Komi-Zyrian of 56% leaves some room for
improvement.
8 Conclusions
In this paper, we outlined a method for translat-
ing word embeddings from a majority language,
Finnish, to four minority languages - Erzya, Mok-
sha, Udmurt, and Komi-Zyrian. The word em-
beddings were aligned and a new neural network
model was introduced. This model was trained
using English data to carry out sentiment analy-
sis and was then applied to data in the endangered
languages using the aligned word embeddings.
We built an aligned sentiment analysis corpus
for the four endangered languages and Finnish and
used it to test our model. The results were promis-
ing and our study demonstrated that even the lat-
est neural models can be utilized with endangered
languages if a dictionary between the endangered
language and a larger language is available.
Acknowledgments
This research is supported by FIN-CLARIN and
Academy of Finland (grant 345610 Kielivarojen
ja kieliteknologian tutkimusinfrastruktuuri ).References
Khalid Alnajjar. 2021. When word embeddings be-
come endangered. In Multilingual Facilitation ,
pages 275–288. Rootroo Ltd.
Khalid Alnajjar, Mika Hämäläinen, Niko Tapio Par-
tanen, and Jack Rueter. 2022. Using graph-based
methods to augment online dictionaries of endan-
gered languages. In Proceedings of the Fifth Work-
shop on the Use of Computational Methods in the
Study of Endangered Languages , pages 139–148.
Khalid Alnajjar, Jack Rueter, Niko Partanen, and Mika
Hämäläinen. 2021. Enhancing the erzya-moksha
dictionary automatically with link prediction. Folia
Uralica Debreceniensia , 28:7–18.
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo
Lopez-Gazpio, and Lucia Specia. 2017.
SemEval-2017 task 1: Semantic textual similarity multilin gual and crosslingual focused evaluation.
InProceedings of the 11th International Workshop
on Semantic Evaluation (SemEval-2017) , pages
1–14, Vancouver, Canada. Association for Compu-
tational Linguistics.
Guanhua Chen, Shuming Ma, Yun Chen, Dongdong
Zhang, Jia Pan, Wenping Wang, and Furu Wei. 2022.
Towards making the most of cross-lingual transfer
for zero-shot neural machine translation. In Pro-
ceedings of the 60th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 142–157, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Édouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451.
Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087 .
David Dale. 2022. The ﬁrst neural machine translation
system for the erzya language. In Proceedings of the
ﬁrst workshop on NLP applications to ﬁeld linguis-
tics, pages 45–53.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics , pages 4171–4186, Min-
neapolis, Minnesota.
Jeff Ens, Mika Hämäläinen, Jack Rueter, and Philippe
Pasquier. 2019. Morphosyntactic disambiguation in
an endangered language setting. In Proceedings ofthe 22nd Nordic Conference on Computational Lin-
guistics , pages 345–349, Turku, Finland. Linköping
University Electronic Press.
Murhaf Fares, Andrey Kutuzov, Stephan
Oepen, and Erik Velldal. 2017.
Word vectors, reuse, and replicability: Towards a communit y repository of large-text resources.
InProceedings of the 21st Nordic Conference on
Computational Linguistics , pages 271–276, Gothen-
burg, Sweden. Association for Computational
Linguistics.
Vitali Gubarev. 1948. Павлик Морозов .Книгам
лыкшы мары изд-во .
Mika Hämäläinen. 2019. Uralicnlp: An nlp library for
uralic languages. Journal of open source software .
Mika Hämäläinen. 2021. Endangered languages are
not low-resourced! Multilingual Facilitation .
Mika Hämäläinen and Khalid Alnajjar. 2019. Let’s
face it. ﬁnnish poetry generation with aesthetics and
framing. In Proceedings of the 12th International
Conference on Natural Language Generation , pages
290–300.
Mika Hämäläinen, Khalid Alnajjar, and Thierry
Poibeau. 2022. Video games as a corpus: Senti-
ment analysis using fallout new vegas dialog. In
Proceedings of the 17th International Conference on
the Foundations of Digital Games , pages 1–4.
Mika Hämäläinen, Niko Partanen, Jack Rueter, and
Khalid Alnajjar. 2021. Neural morphology dataset
and models for multiple languages, from the large to
the endangered. In Proceedings of the 23rd Nordic
Conference on Computational Linguistics (NoDaL-
iDa), pages 166–177.
Kaisa Häkkinen. 1997. Suomi eilen ja tänään. Inde-
pendently published, Helsinki.
Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for POS tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2832–2838, Copenhagen, Denmark. As-
sociation for Computational Linguistics.
Language Bank of Finland. 2022. Word embeddings
trained with word2vec from the ﬁnnish text collec-
tion. In http://urn.ﬁ/urn:nbn:ﬁ:lb-2022041406 .
Shuhua Monica Liu and Jiun-Hung Chen. 2015. A
multi-label classiﬁcation based approach for senti-
ment classiﬁcation. Expert Systems with Applica-
tions , 42(3):1083–1093.
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems , pages
165–172.Christopher Moseley, editor. 2010. Atlas of
the World′s Languages in Danger , 3rd edi-
tion. UNESCO Publishing. Online version:
http://www.unesco.org/languages-atlas/.
Sjur Moshagen, Jack Rueter, Tommi Pirinen, Trond
Trosterud, and Francis M Tyers. 2014. Open-source
infrastructures for collaborative work on under-
resourced languages. Collaboration and Computing
for Under-Resourced Languages in the Linked Open
Data Era , pages 71–77.
Emily Öhman. 2021. The validity of lexicon-based
sentiment analysis in interdisciplinary research. In
Proceedings of the Workshop on Natural Language
Processing for Digital Humanities , pages 7–12.
Niko Partanen, Rogier Blokland, KyungTae Lim,
Thierry Poibeau, and Michael Rießler. 2018. The
ﬁrst Komi-Zyrian universal dependencies treebanks.
InSecond Workshop on Universal Dependencies
(UDW 2018), November 2018, Brussels, Belgium ,
pages 126–132.
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,
and Christopher D Manning. 2020. Stanza: A
python natural language processing toolkit for many
human languages. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations , pages 101–
108.
Nils Reimers and Iryna Gurevych. 2019.
Sentence-bert: Sentence embeddings using siamese bert-ne tworks.
InProceedings of the 2019 Conference on Empir-
ical Methods in Natural Language Processing .
Association for Computational Linguistics.
Jack Rueter, Mika Hämäläinen, and Niko Partanen.
2020. Open-source morphology for endangered
mordvinic languages. In Proceedings of Second
Workshop for NLP Open Source Software (NLP-
OSS) . The Association for Computational Linguis-
tics.
Jack Rueter, Niko Partanen, Mika Hämäläinen, and
Trond Trosterud. 2021. Overview of open-source
morphology development for the komi-zyrian lan-
guage: Past and future. In Proceedings of the Sev-
enth International Workshop on Computational Lin-
guistics of Uralic Languages . The Association for
Computational Linguistics.
Jack Michael Rueter and Francis M Tyers. 2018. To-
wards an open-source universal-dependency tree-
bank for erzya. In International Workshop for Com-
putational Linguistics of Uralic Languages .
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 conference on
empirical methods in natural language processing ,
pages 1631–1642.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks from overﬁt ting.
Journal of Machine Learning Research ,
15(56):1929–1958.
Moshe Stekel, Amos Azaria, and Shai Gordin. 2021.
Word sense induction with attentive context cluster-
ing. In Proceedings of the Workshop on Natural
Language Processing for Digital Humanities , pages
144–151.
Michael E. Tipping and Christopher M. Bishop. 1999.
Mixtures of Probabilistic Principal Component Analyzers.
Neural Computation , 11(2):443–482.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis
Vlahavas. 2010. Random k-labelsets for multilabel
classiﬁcation. IEEE transactions on knowledge and
data engineering , 23(7):1079–1089.
Ruochen Xu, Yiming Yang, Naoki Otani, and Yuexin
Wu. 2018. Unsupervised cross-lingual transfer of
word embedding spaces. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 2465–2474.
Selim F Yilmaz, E Batuhan Kaynak, Aykut Koç, Hamdi
Dibeklio ˘glu, and Suleyman Serdar Kozat. 2021.
Multi-label sentiment analysis on 100 languages
with dynamic weighting for label imbalance. IEEE
Transactions on Neural Networks and Learning Sys-
tems.
Tatu Ylönen. 2022. Wiktextract: Wiktionary as
machine-readable structured data. In Proceedings of
the Thirteenth Language Resources and Evaluation
Conference , pages 1317–1325.