Published in Transactions on Machine Learning Research (05/2023)
Attacking Perceptual Similarity Metrics
Abhijay Ghildyal abhijay@pdx.edu
Department of Computer Science
Portland State University
Feng Liu ﬂiu@pdx.edu
Department of Computer Science
Portland State University
Reviewed on OpenReview: https: // openreview. net/ forum? id= r9vGSpbbRO
Abstract
Perceptual similarity metrics have progressively become more correlated with human judg-
ments on perceptual similarity; however, despite recent advances, the addition of an im-
perceptible distortion can still compromise these metrics. In our study, we systematically
examine the robustness of these metrics to imperceptible adversarial perturbations. Fol-
lowing the two-alternative forced-choice experimental design with two distorted images and
one reference image, we perturb the distorted image closer to the reference via an adversar-
ial attack until the metric ﬂips its judgment. We ﬁrst show that all metrics in our study
are susceptible to perturbations generated via common adversarial attacks such as FGSM,
PGD, and the One-pixel attack. Next, we attack the widely adopted LPIPS metric us-
ing spatial-transformation-based adversarial perturbations (stAdv) in a white-box setting
to craft adversarial examples that can eﬀectively transfer to other similarity metrics in a
black-boxsetting. WealsocombinethespatialattackstAdvwithPGD( /lscript∞-bounded)attack
to increase transferability and use these adversarial examples to benchmark the robustness
of both traditional and recently developed metrics. Our benchmark provides a good start-
ing point for discussion and further research on the robustness of metrics to imperceptible
adversarial perturbations. Code is available at https://tinyurl.com/attackingpsm.
1 Introduction
Comparison of images using a similarity measure is crucial for deﬁning the quality of an image for many
applications in image and video processing. Recently, perceptual similarity metrics have become vital for
optimizing and evaluating deep neural networks used in low-level computer vision tasks (Dosovitskiy & Brox,
2016; Zhu et al., 2016; Johnson et al., 2016; Ledig et al., 2016; Sajjadi et al., 2017; Kettunen et al., 2019a;
Zhang et al., 2020; Son et al., 2020; Niklaus & Liu, 2020; Karras et al., 2020). Learned perceptual image
patch similarity (LPIPS) metric by Zhang et al. (2018b) is one such widely adopted perceptual similarity
metric. Apart from these image enhancement and generation tasks, similarity metrics are also used in
optimizing, constraining, and evaluating adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015;
Carlini & Wagner, 2017; Kurakin et al., 2017; Hosseini & Poovendran, 2018; Dong et al., 2018; Shamsabadi
et al., 2020; Laidlaw & Feizi, 2019). A limitation in early adversarial robustness studies has been the use of
/lscriptpnorms as a distance metric to judge the imperceptibility of synthesized adversarial perturbations. These
attack methods optimized for stronger adversarial perturbations while keeping the perturbations within
imperceptibility levels via an /lscriptpnorm. However, as we now know, /lscriptpdistance metrics are not a good proxy
to human perception, and several learned perceptual similarity metrics have been developed to correlate
better with human judgment. More recently, Laidlaw et al. (2020) proposed neural perceptual threat models
(NPTM)andsubsequentlyadefensemethodthatcouldgeneralizewellagainstunforeseenadversarialattacks,
in which, instead of an /lscriptpnorm, the severity, or perceptibility of the adversarial perturbations, is bounded
by LPIPS, a learned perceptual similarity metric. Hence, they employed LPIPS in their optimization to
1arXiv:2305.08840v1  [cs.CV]  15 May 2023Published in Transactions on Machine Learning Research (05/2023)
𝐻𝑢𝑚𝑎𝑛
𝐼!𝐼"𝐼#$%
0.0590.5950.1440.6970.045üüü𝑆𝑆𝐼𝑀𝐿𝑃𝐼𝑃𝑆0.037ü𝐿&
+=
𝐼"𝐼'()𝛿
𝑓𝐼#$%,𝐼!<𝑓𝐼#$%,𝐼'()𝑓𝐼#$%,𝐼!>𝑓𝐼#$%,𝐼"?
Figure 1:I1is more similar to IrefthanI0according to all perceptual similarity metrics and humans. We
attackI1by adding imperceptible adversarial perturbations ( δ) such that the metric ( f) ﬂips its earlier
assigned rank, i.e., in the above sample, I0becomes more similar to Iref.
generate adversarial examples. However, it remains unanswered whether LPIPS itself is robust towards
imperceptible adversarial perturbations. The question then arises, “How robust are perceptual similarity
metrics against imperceptible adversarial perturbations?” We posit that more accurate and robust perceptual
similarity metrics can lead to stronger defenses against adversarial threats. In a recent study, Mahloujifar
et al. (2020) showed that a better perception model to test the imperceptibility of adversarial perturbations
can lead to stronger robustness guarantees for image classiﬁers.
We begin by examining whether it is possible to ﬁnd imperceptible adversarial perturbations that can
overturn perceptual similarity judgments. It is well known that machine learning models are easy to fool
with adversarial perturbations imperceptible to the human eye (Szegedy et al., 2014). Interestingly, similar
imperceptible perturbations can bring about a sizeable change in the measured distance of a distorted
image from its reference. As shown in Figure 1, we examine this change in measured distances using a
two-alternative forced choice (2AFC) test example, where the participants were asked, “which of the two
distorted images ( I0andI1) is more similar to the reference image ( Iref)”. Then, we apply an imperceptible
perturbation to the distorted image that has the lower perceptual distance (i.e., more similar to Iref)
to see if the similarity judgment for the sample overturns. In such a scenario, human opinion remains
the same, while perceptual similarity metrics often overturn their judgment. Perceptual similarity metrics
measure the similarity between two images and are widely used in many real-world applications. Having
a robust metric is sometimes critical. Copyright protection is one critical use case where automatic image
similarity assessment plays an important role. A malicious user can upload copyright-protected images with
imperceptible perturbations, making the images less detectable on the internet. Interestingly, recent work
began to investigate the perceptual robustness of image quality assessment (IQA) methods via adversarial
perturbations Zhang et al. (2022) and Lu et al. (2022). However, these studies focus on no-reference image
quality assessment methods. The robustness of perceptual similarity metrics, often used as full-reference
image quality assessment methods, has been less studied.
There are two popular approaches to examining the robustness of perceptual similarity metrics: (1) addition
of small amounts of hand-crafted distortions such as translation, rotation, dilation, JPEG compression,
and Gaussian blur, and (2) analysis of more advanced adversarial perturbations. For the former, seminal
contributions have been made (Ma et al., 2018; Ding et al., 2020; Bhardwaj et al., 2020; Gu et al., 2020).
However, in contrast to previous work, we focus on performing the latter as it has not received considerable
attention. In our work, we demonstrate that threats to similarity metrics can be easily created using
common gradient-based iterative white-box attacks, such as fast gradient sign method (FGSM) (Goodfellow
et al., 2015) and projected gradient descent (PGD) (Madry et al., 2018). These attacks do not deform
the structure but rather manipulate pixel values in the image. In recent research, questions regarding the
robustness of perceptual similarity metrics towards geometric distortions are of central interest (as discussed
above). Hence, we also use the spatial adversarial attack stAdv (Xiao et al., 2018), which geometrically
deforms the image. It utilizes optical ﬂow for crafting perturbations in the spatial domain. We use this
attack to generate adversarial samples for comparing the robustness of various metrics.
We also examine whether perceptual metrics can be attacked in black box settings. To this end, we ﬁrst use
the One-pixel attack (Su et al., 2019) that uses diﬀerential evolution (Storn & Price, 1997) to optimize a
single-pixel perturbation on the adversarial image. While compared to white box attacks such as FGSM and
2Published in Transactions on Machine Learning Research (05/2023)
PGD, this One-pixel attack does not need the model parameters of a similarity metric, it needs to access
its output. Therefore, we furthermore explore transferable attacks (Liu et al., 2017; Xie et al., 2018; 2019)
which requires no information about the model. Speciﬁcally, we generate adversarial examples using the
parameters of a source model and use them to attack a target model. In our study, we use LPIPS(AlexNet)
as the source model and attack it via stAdv. We extend the successfully attacked examples onto a target
perceptual similarity metric. It is a black-box setting as it does not require access to the target perceptual
metric’s parameters. In our work, we combine stAdv (spatial attack) with PGD ( /lscript∞-bounded attack) that
strengthens the severity of the adversarial examples.
The main contribution of this paper is the ﬁrst systematical investigation on whether existing perceptual
similarity metrics are susceptible to state-of-the-art adversarial attacks. Our study includes a set of carefully
selected attacking methods and a wide variety of perceptual similarity metrics. Our study shows that all
these similarity metrics, including both traditional quality metrics and recent deep learning-based metrics,
can be successfully attacked by both white-box and black-box attacks.
2 Related Work
Earlier metrics such as SSIM (Wang et al., 2004) and FSIMc (Zhang et al., 2011) were designed to ap-
proximate the human visual systems’ ability to perceive and distinguish images, speciﬁcally using statistical
features of local regions in the images. Whereas, recent metrics (Zhang et al., 2018b; Prashnani et al., 2018;
Maetal.,2018;Kettunenetal.,2019b;Dingetal.,2020;Bhardwajetal.,2020;Ghildyal&Liu,2022)aredeep
neural network based approaches that learn from human judgments on perceptual similarity. LPIPS (Zhang
et al., 2018b) is one such widely used metric. It leverages the activations of a feature extraction network at
each convolutional layer to compute diﬀerences between two images which are then passed on to linear layers
to ﬁnally predict the perceptual similarity score. Prashnani et al. (2018) developed the Perceptual Image
Error Metric (PieAPP) that uses a weight-shared feature extractor on each of the input images, followed
by two fully-connected networks that use the diﬀerence of those features to generate patch-wise errors and
corresponding weights. The weighted average of the errors is the ﬁnal score. Liu et al. (2022) used the Swin
Transformer (Liu et al., 2021) for multi-scale feature extraction in their metric, Swin-IQA. Its ﬁnal score is
the average across all cross-attention operations on the diﬀerence between the features. Swin-IQA performs
better than the CNN-based metrics in accurately ranking, according to human opinion, the distorted images
synthesized by methods from the Challenge on Learned Image Compression (CLIC, 2022).
In recent years, apart from making the perceptual similarity metrics correlate well with human opinion, there
has been growing interest in examining their robustness towards geometric distortions. Wang & Simoncelli
(2005) noted that geometric distortions cause consistent phase changes in the local wavelet coeﬃcients while
structural content stays intact. Accordingly, they developed complex wavelet SSIM (CW-SSIM) that used
phase correlations instead of spatial correlations, making it less sensitive to geometric distortions. Ma et al.
(2018) benchmarked the sensitivity of various metrics against misalignment, scaling artifacts, blurring, and
JPEGcompression. TheythentrainedaCNNwithaugmentedimagestocreatethegeometrictransformation
invariant metric (GTI-CNN). In a similar study, Ding et al. (2020) suggested computing global measures
instead of pixel-wise diﬀerences and then blurred the feature embeddings by replacing the max pooling layers
withl2-pooling layers. It made their metric, deep image structure and texture similarity (DISTS), robust to
local and global distortions. Ding et al. (2021) extend DISTS making it robust for perceptual optimization
of image super-resolution methods. They separate texture from the structure in the extracted multi-scale
feature maps via a dispersion index. Then, to compute feature diﬀerences for the ﬁnal similarity score, they
modify SSIM by adaptively weighting its structure and texture measurements using the dispersion index.
Bhardwaj et al. (2020) developed the perceptual information metric (PIM). PIM has a pyramid architecture
with convolutional layers that generate multi-scale representations, which get processed by dense layers to
predict mean vectors for each spatial location and scale. The ﬁnal score is estimated using symmetrized
KL divergence using Monte Carlo sampling. PIM is well correlated with human opinions and is robust
against small image shifts, even though it is just trained on consecutive frames of a video, without any
human judgments on perceptual similarity. Czolbe et al. (2020) used Watson’s perceptual model (Watson,
1993) and replaced discrete cosine transform with discrete fourier transform (DFT) to develop a perceptual
similarity loss function robust against small shifts. Kettunen et al. (2019b) compute the average LPIPS
3Published in Transactions on Machine Learning Research (05/2023)
score over an ensemble of randomly transformed images. Their self-ensembling metric E-LPIPS is robust
to the Expectations over Transformations attacks (Athalye et al., 2018; Carlini & Wagner, 2017). Our
attack approach is similar to an attack investigated by Kettunen et al. (2019b), where the adversarial images
look similar but have a large LPIPS distance (smaller distance means more similarity). However, they
only investigate the LPIPS metric. Ghildyal & Liu (2022) develop a shift-tolerant perceptual metric that
is robust to imperceptible misalignments between the reference and the distorted image. For it, they test
various neural network elements and modify the architecture of the LPIPS metric rather than training it
on augmented data to handle the misalignment, making it more consistent with human perception. So
far, the majority of prior research has focused on geometric distortions, while no study has systematically
investigated the robustness of various similarity metrics to more advanced adversarial perturbations that
are more perceptually indistinguishable. We seek to address this critical open question, whether perceptual
similarity metrics are robust against imperceptible adversarial perturbations . In our paper, we show that
the metrics often overturn their similarity judgment after the addition of adversarial perturbations, unlike
humans, to whom the perturbations are unnoticeable.
There exists a considerable body of literature on adversarial attacks (Szegedy et al., 2014; Goodfellow
et al., 2015; Liu et al., 2017; Papernot et al., 2016; Carlini & Wagner, 2017; Xie et al., 2018; Hosseini &
Poovendran, 2018; Madry et al., 2018; Xiao et al., 2018; Brendel et al., 2018; Song et al., 2018; Zhang et al.,
2018a; Engstrom et al., 2019; Laidlaw & Feizi, 2019; Su et al., 2019; Wong et al., 2019; Bhattad et al.,
2019; Xie et al., 2019; Zeng et al., 2019; Dolatabadi et al., 2020; Tramèr et al., 2020; Laidlaw et al., 2020;
Croce et al., 2020; Wu & Zhu, 2020), but none of the previous investigations have ever considered attacking
perceptual similarity metrics, except for E-LPIPS (Kettunen et al., 2019b) which only studies the LPIPS
metric. This paper builds upon this line of research and carefully selects a set of representative attacking
algorithms to investigate the adversarial robustness of similarity metrics. We brieﬂy describe these methods
and how we employ them to attack similarity metrics in Section 3. In parallel, Lu et al. (2022) developed
an adversarial attack for neural image assessment (NIMA) (Talebi & Milanfar, 2018) to prevent misuse of
high-quality images on the internet. NIMA is NR-IQA, while we systematically investigate several FR-IQA
methods against various attacks.
Recent work underlines the importance of perceptual distance as a bound for adversarial attacks (Laidlaw
et al., 2020; Wang et al., 2021; Zhang et al., 2022). Laidlaw et al. (2020) developed a neural perceptual
threat model (NPTM) that employs the perceptual similarity metric LPIPS(AlexNet) as a bound for gen-
erating adversarial examples and provided evidence that lp-bounded and spatial attacks are near subsets
of the NPTM. Similarly, Zhang et al. (2022) developed a perceptual threat model to attack no-reference
IQA methods by constraining the perturbations via full-reference IQA, i.e., perceptual similarity metrics
such as SSIM, LPIPS, and DISTS. They posit that the metrics are “approximations to human perception of
just-noticeable diﬀerences” (Zhang et al., 2022), therefore, can keep perturbations imperceptible. Moreover,
Laidlaw et al. (2020) found LPIPS to correlate well with human opinion when evaluating adversarial exam-
ples.However, it has not yet been established whether LPIPS and other perceptual similarity metrics are
adversarially robust. We investigate this in our work, and the ﬁndings in our study indicate that all metrics,
including LPIPS, are not robust to various kinds of adversarial perturbations.
3 Method
Dataset. Our study uses the Berkeley-Adobe perceptual patch similarity (BAPPS) dataset, originally used
to train a perceptual similarity metric (Zhang et al., 2018b). Each sample in this dataset contains a set of 3
images: 2 distorted ( I0andI1) and 1 reference ( Iref). For perceptual similarity assessment, the scores were
generated using a two-alternative forced choice (2AFC) test where the participants were asked, “which of
two distortions is more similar to a reference” (Zhang et al., 2018b). For the validation set, 5 responses per
sample were collected. The ﬁnal human judgment is the average of the responses. The types of distortions in
this dataset are traditional, CNN-based, and distortions by real algorithms such as super resolution, frame
interpolation, deblurring, and colorization. Human opinions could be divided, i.e., all responses in a sample
may not have voted for the same distorted image. In our study, to ensure that the two distorted images in
the sample have enough disparity between them, we only select those samples where humans unanimously
voted for one of the distorted images. In total, there are 12,227 such samples.
4Published in Transactions on Machine Learning Research (05/2023)
It is non-trivial to compare metrics based on a norm-based constraint simply because a change of 10% in
metric A’s score is not equal to a 10% change in metric B’s score. But how does one calculate the fooling
rate that measures the susceptibility of a similarity metric? A straightforward method is to compare all
metrics against human perceptual judgment. The 2AFC test gathers human judgment on which of the two
distorted images is more similar to the reference. Using this knowledge, we can benchmark various metrics
and test whether their accuracy drops or, i.e. if they ﬂip their judgment when attacked. To make it a fair
challenge, we only use samples where human opinion completely prefers one distorted image over the other.
Attack Models. As observed in Figure 1, the addition of adversarial perturbations can lead to a rank ﬂip.
We make use of existing attack methods such as FGSM (Goodfellow et al., 2015), PGD (Madry et al., 2018),
One-pixel attack (Su et al., 2019), and spatial attack stAdv (Xiao et al., 2018) to generate such adversarial
samples. These attack methods were originally devised to fool image classiﬁcation models, therefore, we
introduce minor modiﬁcations in their procedures to attack perceptual similarity metrics. We select one of
the distorted images, I0orI1, that is more similar to Irefto attack. The distorted image being attacked
isIprey, and the other image is Iother; accordingly, for the sample in Figure 1, I1isIpreyandI0isIother.
Considersias the similarity score between IiandIref1. Before the attack, the original rank is sother>sprey,
but after the attack Ipreyturns intoIadv, and when the rank ﬂips, sadv>sother. In image classiﬁcation, a
misclassiﬁcation is used to measure the attack’s success, while for perceptual similarity metrics, an attack is
successful when the rank ﬂips.
𝐿𝑃𝐼𝑃𝑆:0.1439
𝐼!"#
𝜖⋅𝑠𝑖𝑔𝑛(∇$!𝐽(𝜃,𝐼%,𝐼&,𝐼'()))=𝜖=0.024𝐼%+
𝐿𝑃𝐼𝑃𝑆:0.0445
Figure 2: FGSM attack on LPIPS(Alex). In this white-box
attack, we use the LPIPS network parameters to compute
the signed gradient. With increase in /epsilon1, the severity of the
attack increases. In this example, the adversarial perturba-
tions are hardly visible. The RMSE between the prey image
I1and the adversarial image Iadvis 3.53.Fast Gradient Sign Method. FGSM is a
popular white-box attack introduced by Good-
fellow et al. (2015). This attack method
projects the input image Ionto the boundary
of an/epsilon1sized/lscript∞-ball, and therefore, restricts
the perturbations to the locality of I. We fol-
low this method to generate imperceptible per-
turbationsbyconstraining /epsilon1tobesmallforour
experiments. This attack starts by ﬁrst com-
puting the gradient with respect to the loss
function of the image classiﬁer being attacked.
The signed value of this gradient multiplied
by/epsilon1generates the perturbation, and thus,
Iadv:=I+/epsilon1·sign (∇IJ(θ,I,target )), whereθ
are the model parameters. We adopt this
method to attack perceptual similarity met-
rics. We formulate a new loss function for an untargeted attack as:
J(θ,Iprey,Iother,Iref) =/parenleftbiggsother
sother +sprey−1/parenrightbigg2
(1)
We maximize this loss, i.e., move in the opposite direction of the optimization by adding the perturbation to
theimage. Thehumanscoreofallthesamplesinourselecteddatasetiseither0or1, unanimousvote. Hence,
we can easily employ the loss function in Equation 1, because if the metric predicts the rank correctly then
(sother/(sother +sprey))would be≈1. Afterwards, if the attack is successful then (sother/(sother +sadv))
becomes less than 0.5, causing the rank to ﬂip. Algorithm 3 (refer Appendix B) provides the details for the
FGSM attack. First, Ipreyis selected based on the original rank. The model parameters remain constant,
and we compute the gradients with respect to the input image Iprey. To increase perturbations in normalized
images, we increase the /epsilon1in steps of 0.0001 starting from 0.0001. When /epsilon1is large enough, the rank ﬂips. It
would mean that the attack was successful (see example in Figure 2). If the ﬁnal value of /epsilon1is small then
the perturbation is imperceptible, making it hard to discern any diﬀerence between the original image and
its adversarial sample.
Projected Gradient Descent. PGD attack by Madry et al. (2018) takes a similar approach to FGSM,
but instead of a single large step like in FGSM, PGD takes multiple small steps for generating perturbation
1smallersimeansIiis more similar to Iref
5Published in Transactions on Machine Learning Research (05/2023)
𝐼!"#𝛿=𝐼$+
𝐿𝑃𝐼𝑃𝑆:0.1465𝐿𝑃𝐼𝑃𝑆:0.045
Figure 3: PGD attack on LPIPS(Alex). In this white-box attack, we use the LPIPS network parameters to
compute the signed gradient. With increase in the number of attack iterations, the severity of the attack
increases. In this example, perturbations in Iadvare not visible. The RMSE between the prey image I1and
the adversarial image Iadvis 2.10.
δ. Hence, the projection of Istays either inside or on the boundary of the /epsilon1-ball. This multistep attack is
deﬁned as:
It+1
adv=Pc/parenleftbig
It
adv+α·sign (∇It
advJ(θ,It
adv,Iother,Iref)/parenrightbig
(2)
Algorithm 1: PGD attack on Similarity Metrics
Input:I0,I1,Iref, metricf,/epsilon1(perturbation limit 0.03
1),max_iterations (30),α(step size 0.001)
Output:attack_successTrue on rank ﬂip
2s0=f(Iref,I0);s1=f(Iref,I1);rank =int(s0>s1);
3// IfI0is more similar to Irefthenrankis 0 else 1
4ifrank = 1thenIprey=I1;sother=s0;
5elseIprey=I0;sother=s1;
6δ=zeros_like(Iprey) // perturbation
7k= 0
8whilek≤max_iterations do
9Iadv =clip(Iprey +δ,min =−1,max = 1)
10sadv =f(Iref,Iadv)
11 ifsadv>sotherthen return True// Attack successful
12J=/parenleftbig
(sother/(sother +sadv))−1/parenrightbig2// Loss
13signed_grad=sign/parenleftbig
∇IadvJ/parenrightbig
14I/prime
adv=Iadv+α∗signed_grad
15δ=clip(I/prime
adv−Iprey,min =−/epsilon1,max = +/epsilon1)
16k=k+ 1
17return False// Attack unsuccessfulwhereJis the loss deﬁned in Equation 1. The
perturbation on each pixel is bounded to a prede-
ﬁned range using the projection constraint Pc. We
implement Pcusing a clip operation on the ﬁnal
perturbation δ(Line 14 Algorithm 1). As shown
in Algorithm 1, the signed gradient is multiplied
with step size α, and this adversarial perturbation
isaddedto It
adv. Theﬁnalperturbation δisthedif-
ference between It
advandIprey, and in our method,
δis bounded by /lscript∞norm. Hence, the PGD attack
is an /lscript∞-bounded attack .
One-pixel Attack. The previous two approaches
are white-box attacks. We now use a black-box
attack, the One-pixel attack by Su et al. (2019)
that perturbs only a single pixel using diﬀerential
evolution (Storn & Price, 1997).
The objective of the One-pixel attack is deﬁned as:
maximize
e(Iprey )∗f(Iprey +e(Iprey),Iref)
subject to||e(Iprey)||0≤d(3)
wherefis the similarity metric, and the vector e(Iprey)is the additive adversarial perturbation, and dis 1
for the One-pixel attack. This algorithm aims to ﬁnd a mutation to one particular pixel such that a similarity
metricf, such as LPIPS, will consider Ipreyis less similar to Irefthan it is originally, and thus, the rank is
ﬂipped. Note, for LPIPS, a larger score indicates the two images being less similar. Please refer to Su et al.
(2019) for more details of this attack algorithm. For attack example, please refer to Figure 8 in Appendix C.
Spatial Attack (stAdv). The goal of the stAdv attack is to deform the image geometrically by displacing
pixels (Xiao et al., 2018). It generates adversarial perturbations in the spatial domain rather than directly
manipulating pixel intensity values. This attack synthesizes the spatially distorted adversarial image ( Iadv)
via optimizing a ﬂow vector and backward warping with the input image ( Iprey) using diﬀerentiable bilinear
interpolation (Jaderberg et al., 2015). For each sample, we start with a ﬂow initialized with zeros and then
optimize it using L-BFGS (Liu & Nocedal, 1989) for the following loss.
L=αLrank +βLflow (4)
6Published in Transactions on Machine Learning Research (05/2023)
𝐿𝑃𝐼𝑃𝑆:0.1465𝐼!"#𝐹𝑙𝑜𝑤𝐼$
𝐿𝑃𝐼𝑃𝑆:0.0445
Backward warping via Bilinear Interpolation
Figure 4: Spatial attack stAdv on LPIPS(AlexNet). We attack LPIPS(AlexNet) to create adversarial images.
This attack optimizes a ﬂow vector to create perturbations in the spatial domain. In this example, ﬂow
distorts the structure of the horse to generate the adversarial image. The RMSE between the prey image I1
and the adversarial image Iadvis 2.50.
Lflow =pixels/summationdisplay
pneighbors (p)/summationdisplay
q/radicalBig
(up−uq)2+ (vp−vq)2 (5)
where (u,v)is the displacement vector at pixel location pand its 4 neighbors q.
Lrank =/parenleftbiggsother
sother +sadv/parenrightbigg2
(6)
whereαis 50 andβis 0.05.
Algorithm 2: stAdv attack on LPIPS
Input:I0,I1,Iref, LPIPSf,max_iterations (250)
Output:attack_successTrue on rank ﬂip
1Function stAdv_attack( flow,f,Iprey,Iref,sother ):
2Iadv= warp(flow,Iprey) // Backwarp via bilinear interpolation
3sadv =f(Iref,Iadv)
4Lrank,Lperturb =calc_loss (Iref,Iprey,Iadv,sother,f)
5L=Lrank +Lperturb
6gradient =∇flowL
7 ifsadv>sotherthen return 0,gradient ,flow// Attack successful
8 else returnL,gradient ,flow// Attack unsuccessful
1s0=f(Iref,I0);s1=f(Iref,I1);
2rank =int(s0>s1)// IfI0is more similar to Irefthenrankis 0 else 1
3ifrank = 1thenIprey=I1;sother=s0;
4elseIprey=I0;sother=s1;
5// Initialize a ﬂow vector with zeros
6flow=zeros_like(2 *Ipreyheight *Ipreywidth)
7converge ,grad,flow=L-BFGS(func=stAdv_attack, args=(flow,f,Iprey,
Iref,sother), iterations= max_iterations ) // Optimize ﬂow vector
8ifconverge = 0thenattack_success= True
9elseattack_success= False
10returnattack _successAs we minimizeLrank, the pertur-
bations in Iadvwill increase, and
thusrankwillﬂip. Simultaneously,
we also minimize Lflowwhich de-
ﬁnes the amount of perturbations
generated by ﬂow to distort the
image. It enforces the perturba-
tions to be constrained to make as
little change to the attacked im-
ageIpreyas possible. Xiao et al.
(2018) performed a user study to
test the perceptual quality of the
images having perturbations gen-
erated by the stAdv attack and
found them to be indistinguishable
by humans. By visual inspection,
we found the adversarial perturba-
tions on the images imperceptible
in our studies as well.
4 Experiments and Results
We experiment with a wide variety of similarity metrics including both traditional ones, such as L2,
SSIM (Wang et al., 2004), MS-SSIM (Wang et al., 2003), CW-SSIM (Wang & Simoncelli, 2005) and
FSIMc (Zhang et al., 2011), and the recent deep learning based ones, such as WaDIQaM-FR (Bosse
et al., 2018), GTI-CNN (Ma et al., 2018), LPIPS (Zhang et al., 2018b), E-LPIPS (Kettunen et al., 2019b),
DISTS (Ding et al., 2020), Watson-DFT (Czolbe et al., 2020), PIM (Bhardwaj et al., 2020), A-DISTS (Ding
et al., 2021), ST-LPIPS (Ghildyal & Liu, 2022), and Swin-IQA (Liu et al., 2022). We adopt the BAPPS val-
idation dataset (Zhang et al., 2018b) for our experiments. Following Zhang et al. (2018b) we scale the image
7Published in Transactions on Machine Learning Research (05/2023)
Table 1: Accuracy on the subset selected for our experiments
correlates with the 2AFC score computed on the complete
BAPPS validation dataset.
Network2AFC (%) on Accuracy (%) on
complete BAPPS subset of BAPPS
(36344 samples) (12227 samples)
L2 63.2 79.7
SSIM (Wang et al., 2004) 63.1 80.8
WaDIQaM-FR (Bosse et al., 2018) 66.5 83.3
LPIPS(Alex) (Zhang et al., 2018b) 69.8 92.4
LPIPS(VGG) (Zhang et al., 2018b) 68.1 89.8
DISTS (Ding et al., 2020) 68.9 91.3patches from size 256×256to64×64. As
mentioned in Section 3, we believe that the
predictedrankbyametricwillbeeasytoﬂip
on samples close to the decision boundary;
therefore, we take a subset of the samples in
the dataset which have a clear winner, i.e.,
all human responses indicated that one was
distinctly better than the other. Now, in
our dataset, we have 12,227 samples. We
report the accuracy of metrics on the subset
ofselectedsamplesandcompareitwiththeir
Two-alternativeforcedchoice(2AFC)scoresonthecompleteBAPPSvalidationdataset. AsshowninTable1,
all these metrics consistently correlated better with the human opinions on the subset of BAPPS than on
the full dataset, which is expected as we removed the ambiguous cases.
In this section, we ﬁrst show that similarity metrics are susceptible to both white-box and black-box attacks.
Based on this premise, we hypothesize that these similarity metrics are vulnerable to transferable attacks. To
prove this, we attack the widely adopted LPIPS using the spatial attack stAdv to create adversarial examples
and use them to benchmark the adversarial robustness of these similarity metrics. Furthermore, we add a
few iterations of the PGD attack, hence combining our spatial attack with /lscript∞-bounded perturbations, to
enhance transferability to other perceptual similarity metrics.
4.1 Adversarial Attack on Perceptual Similarity Metrics
Through the following study, we test our hypothesis that similarity metrics are susceptible to adversarial
attacks. We ﬁrst determine whether it is possible to create imperceptible adversarial perturbations that
can overturn the perceptual similarity judgment, i.e., ﬂip the rank of the images in the sample. We try to
achieve this by simply attacking with widely used white-box attacks like FGSM, and PGD, and a black-box
attack like the One-pixel attack. As reported in Table 2, all these attacks can successfully ﬂip the rank
assigned by both traditional metrics such as L2, and SSIM (Wang et al., 2004), and learned metrics such
as WaDIQaM-FR (Bosse et al., 2018), LPIPS (Zhang et al., 2018b), and DISTS (Ding et al., 2020), in a
signiﬁcant amount of samples.
For the PGD attack, the maximum /lscript∞-norm perturbation2cannot be more than 0.03 as the step size αis
0.001, and the maximum attack iterations is 30. We chose 30 after visually inspecting for the imperceptibility
of perturbations on the generated adversarial samples. With the same threshold, the FGSM attack would
not be as successful as PGD, which we show in Appendix E. Therefore, to report the results of the FGSM
attack, based on empirical evaluation, we select the maximum /epsilon1as 0.05. We present the results separately
for samples where the originally predicted rank by the metric matches the rank provided by humans. Now,
focusing only on the samples where the metric matches with the ranking by humans, we found L2 and DISTS
to be the most robust against FGSM and PGD with only about 30% of the samples ﬂipped, while LPIPS
and WadIQaM-FR were the least robust, with about 80% of the samples ﬂipped. The same conclusion can
also be reached by observing /epsilon1(or perturbations) required to attack them. Next, despite being a black-box
attack, the One-pixel attack can also successful ﬂip ranks. LPIPS(AlexNet) has the least robustness to the
One-pixel attack with 82% of the samples ﬂipped, and this lack of adversarial robustness is consistent across
all three attacks. SSIM and WadIQaM-FR are more robust to this attack, with only 18% and 31% samples
ﬂipped. It is interesting to note that similar results are achievable by using just the score of the adversarial
image, i.e., sadvas loss for optimization.
Not surprisingly, it is easier to ﬂip rank for the samples where the metric does not match with human
opinion. As reported in Table 2, a much higher number of those samples ﬂip where the rank by metric
and humans did not match. These samples have a lower /epsilon1, which means that lesser perturbations were
required to ﬂip the rank. We attribute the easy rank-ﬂipping for these samples to the fact that the distorted
images in each sample, i.e., IotherandIprey, are much closer to the decision boundary for the rank ﬂip.
2All/epsilon1(or perturbation) in this paper were computed from normalized images in the range [-1,1].
8Published in Transactions on Machine Learning Research (05/2023)
Table 2: FGSM, PGD, and One-pixel attack results. Larger /epsilon1allows more perturbations, and lower RMSE
relates to higher imperceptibility.
NetworkSame Rank
by Human
& MetricTotal
SamplesFGSM (/epsilon1<0.05) PGD One-pixel
#Samples
FlippedMean
/epsilon1RMSE#Samples
Flipped% pixels with /epsilon1RMSE#Samples
Flipped µ σ >0.001 >0.01 >0.03µ σ
L2X 9750 3759/39% 0.023 2.9 1.7 2348/24% 84.4 56.1 0.0 1.9 1.0 4225/43%
7 2477 1550/63% 0.017 2.2 1.6 1202/49% 82.0 42.7 0.0 1.5 1.0 1412/57%
SSIM X 9883 6922/70% 0.018 2.5 1.7 5297/54% 94.6 53.6 0.0 1.8 1.0 1787/18%
(Wang et al., 2004) 7 2344 2013/86% 0.011 1.6 1.3 1843/79% 87.3 32.0 0.0 1.3 0.8 1005/43%
WadIQaM-FR X 10191 8841/87% 0.006 1.0 1.0 10176/100% 69.2 4.3 0.0 0.7 0.3 3130/31%
(Bosse et al., 2018) 7 2036 2012/100% 0.001 0.6 0.3 2035/100% 41.2 0.1 0.0 0.5 0.1 1598/79%
LPIPS(Alex) X 11303 7247/64% 0.018 2.4 1.7 8806/78% 86.8 28.7 0.0 1.3 0.6 9255/82%
(Zhang et al., 2018b) 7 924 912/99% 0.004 0.9 0.7 917/99% 59.5 3.2 0.0 0.8 0.3 921/100%
LPIPS(VGG) X 10976 8434/77% 0.012 1.7 1.5 9689/88% 81.6 15.6 0.0 1.0 0.5 7212/66%
(Zhang et al., 2018b) 7 1251 1244/100% 0.003 0.8 0.5 1246/100% 52.3 1.6 0.0 0.7 0.2 1219/98%
DISTS X 11158 3043/27% 0.025 3.3 1.8 2306/21% 97.0 75.4 0.0 2.6 1.3 7416/67%
(Ding et al., 2020) 7 1069 795/74% 0.016 2.2 1.7 723/68% 91.9 50.0 0.0 2.0 1.3 1033/97%
Table 3: Comparing samples where the
rank by metric was the same as assigned by
humans versus samples where it was not.
NetworkSame Rank by
Human & MetricSimilarity Diﬀ.
abs(s0−s1)
L2 X 0.036
7 0.025
SSIM X 0.114
(Wang et al., 2004) 7 0.054
WadIQaM-FR X 0.231
(Bosse et al., 2018) 7 0.064
LPIPS(Alex) X 0.169
(Zhang et al., 2018b) 7 0.024
LPIPS(VGG) X 0.174
(Zhang et al., 2018b) 7 0.037
DISTS X 0.103
(Ding et al., 2020) 7 0.022To test this, we calculate the absolute diﬀerence between sother
andsprey, i.e., the perceptual distances of IotherandIpreyfrom
Iref. As reported in Table 3, the similarity diﬀerence for these
samples is much lesser than samples where the rank predicted
by metric is the same as the rank assigned by humans. This
result indicates that the samples where rank predicted by met-
ric is not the same as the rank assigned by humans lie closer
to the decision boundary, causing them to ﬂip easier.
Imperceptibility. We discuss the imperceptibility of the ad-
versarial perturbations by comparing the root mean square er-
ror (RMSE3) between the original and the perturbed image.
As expected, the PGD attack is stronger than FGSM as it is
capable of ﬂipping a signiﬁcant number of samples with lesser
adversarial perturbations. In Appendix E, we experiment with
increasing step size αfor the PGD attack, which further in-
creases its severity.Table 4: Comparing PSNR of adversarial images gen-
erated via FGSM vs. PGD. The /epsilon1for the adversarial
images generated via FGSM is <0.05. A higher mean
PSNR of the PGD examples shows that the adversarial
perturbations are less perceptible.
NetworkSame Rank by
Human & MetricFGSM PGD
PSNR PSNR
µ σ µ σ
L2 X 40.81 6.49 44.15 5.49
743.75 7.00 46.08 5.70
SSIM X 42.51 6.55 44.60 5.31
(Wang et al., 2004) 746.39 6.09 47.19 5.16
WadIQaM-FR X 50.81 5.60 52.19 3.47
(Bosse et al., 2018) 753.92 3.25 54.35 2.73
LPIPS(Alex) X 42.80 6.70 46.82 4.09
(Zhang et al., 2018b) 749.98 4.19 50.80 3.14
LPIPS(VGG) X 45.96 6.38 48.68 3.72
(Zhang et al., 2018b) 750.56 3.27 51.09 2.46
DISTS X 39.50 6.22 41.19 5.75
(Ding et al., 2020) 743.64 6.95 44.41 6.39As reported in Table 2, for the PGD attack, a
good portion of the adversarial image ( Iadv) has
/epsilon1< 0.01, while for FGSM, the amount of pixel per-
turbation all over the image is a constant /epsilon1value
which moreover is higher for a successful attack.
Thus, on average, the Iadvgenerated via PGD has
lower RMSE and a higher PSNR (see Table 4) with
the original image Iprey, compared to the Iadvgen-
erated via FGSM. We also perform a visual sanity
check and ﬁnd the perturbations satisfactorily im-
perceptible. Only a single pixel is perturbed for
Iadvgenerated via the One-pixel attack, which we
consider suitably imperceptible.
4.2 Transferable Adversarial Attack
In a real-world scenario, the attacker may not have
access to the metric’s architecture, hyper-parameters, data, or outputs. In such a scenario, a practical
solution for the attacker is to transfer adversarial examples crafted on a source metric to a target perceptual
similaritymetric. Previousstudieshavesuggestedreliableapproachesforcreatingsuchblack-boxtransferable
3Throughout this paper, RMSE was calculated on images with pixel values ranging [0,255].
9Published in Transactions on Machine Learning Research (05/2023)
Table 5: Transferable adversarial attacks on perceptual similarity metrics. The adversarial examples were
generated by attacking LPIPS(AlexNet) via stAdv. In total, there are 2726 samples. Next, we attacked
LPIPS(AlexNet) using PGD(10). Then, we combined stAdv+PGD(10) by perturbing the stAdv generated
images with PGD(10). Accurate samples are the ones for which the predicted rank by metric is equal to the
rank assigned by humans. The transferability increases when the attacks are combined.
Network#Accurate
Samples# Accurate Samples Flipped
PGD(10) PGD(20) stAdvstAdv + stAdv + stAdv + stAdv +
PGD(5) PGD(10) PGD(15) PGD(20)
L2 2099/77% 101/5% 174/8% 77/4% 134/6% 189/9% 200/10% 257/12%
SSIM (Wang et al., 2004) 2093/77% 237/11% 442/21% 78/4% 180/9% 339/16% 370/18% 540/26%
MS-SSIM (Wang et al., 2003) 2022/74% 158/8% 256/13% 76/4% 162/8% 224/11% 234/12% 333/16%
CWSSIM (Wang & Simoncelli, 2005) 1883/69% 101/5% 172/9% 42/2% 60/3% 128/7% 139/7% 193/10%
FSIMc (Zhang et al., 2011) 2025/74% 222/11% 325/16% 202/10% 233/12% 302/15% 310/15% 393/19%
WaDIQaM-FR (Bosse et al., 2018) 2083/76% 95/5% 186/9% 59/3% 85/4% 146/7% 156/7% 238/11%
GTI-CNN (Ma et al., 2018) 1946/71% 448/23% 480/25% 494/25% 488/25% 504/26% 510/26% 543/28%
LPIPS(Squz.) (Zhang et al., 2018b) 2503/92% 298/12% 656/26% 114/5% 221/9% 519/21% 555/22% 886/35%
LPIPS(VGG) (Zhang et al., 2018b) 2317/85% 435/19% 814/35% 131/6% 288/12% 643/28% 685/30% 992/43%
E-LPIPS (Kettunen et al., 2019b) 2442/90% 503/21% 643/26% 517/21% 552/23% 641/26% 655/27% 817/33%
DISTS (Ding et al., 2020) 2413/89% 311/13% 576/24% 146/6% 257/11% 510/21% 546/23% 801/33%
Watson-DFT (Czolbe et al., 2020) 2179/80% 387/18% 614/28% 216/10% 324/15% 532/24% 562/26% 750/34%
PIM-1 (Bhardwaj et al., 2020) 2468/91% 696/28% 814/33% 756/31% 772/31% 826/33% 852/35% 958/39%
PIM-5 (Bhardwaj et al., 2020) 2457/90% 751/31% 844/34% 765/31% 791/32% 864/35% 893/36% 963/39%
A-DISTS (Ding et al., 2021) 2346/86% 339/14% 661/28% 164/7% 276/12% 561/24% 590/25% 850/36%
ST-LPIPS(Alex) (Ghildyal & Liu, 2022) 2470/91% 104/4% 198/8% 96/4% 123/5% 205/8% 212/9% 310/13%
ST-LPIPS(VGG) (Ghildyal & Liu, 2022)2493/91% 210/8% 453/18% 103/4% 153/6% 321/13% 360/14% 576/23%
SwinIQA (Liu et al., 2022) 2310/85% 249/11% 357/15% 262/11% 279/12% 342/15% 375/16% 482/21%
adversarialexamplesforimageclassiﬁers(Tramèretal.,2017;Zhouetal.,2018;Inkawhichetal.,2019;Huang
et al., 2019; Li et al., 2020; Hong et al., 2021). This paper focuses on perceptual similarity metrics and how
they perform against such transferable adversarial examples. Speciﬁcally, we transfer the stAdv attack on
LPIPS(AlexNet) to other metrics. We chose LPIPS(AlexNet) as it is widely adopted in many computer
vision, graphics, and image / video processing applications. Furthermore, we combine the stAdv attack with
PGD to increase the transferability of the adversarial examples to other metrics. In this study, we only
consider samples for which the metrics and the human opinions agree on their rankings.
stAdv. As shown in Figure 4, stAdv has the capability of attacking high-level image features. As a white-
box attack on LPIPS(AlexNet), out of the 11,303 accurate samples from total 12,227 samples, stAdv was
able to ﬂip judgment on 4658 samples with a mean RMSE of 2.37 with standard deviation 1.42. Because
we need high imperceptibility, we remove samples with RMSE >3 and are left with 3327 samples. We then
perform a visual sanity check and remove some more with ambiguity, keeping only strictly imperceptible
samples. In the end, we have 2726 samples, with a mean RMSE of 1.58 with standard deviation 0.63, which
we transfer to other metrics as a black-box attack. As reported in Table 5, all metrics are prone to the attack.
WaDIQaM-FR (Bosse et al., 2018) is most robust, while PIM (Bhardwaj et al., 2020) that was found robust
to small imperceptible shifts is highly susceptible to this attack, although PIM is 15% more accurate than
WaDIQaM-FR. DISTS, ST-LPIPS, and Swin-IQA have similar high accuracy as PIM but better robustness.
Finally, we saw that, on average, learned metrics are more correlated with human opinions, but traditional
metrics exhibit more robustness to the imperceptible transferable stAdv adversarial perturbations.
PGD(10). We now attack the original 2726 selected samples with the PGD attack. As shown in Section 4.1,
perturbations generated via PGD have low perceptibility; hence, we create adversarial samples using PGD.
In stAdv, we stopped the attack when the rank predicted by LPIPS(AlexNet) ﬂipped. While in PGD, for
comparison’ssake, weﬁxthenumberofattackiterationsto10foreachsampletoguaranteethetransferability
of perturbations. We call this transferable attack PGD(10), and the mean RMSE of the adversarial images
generated is 1.28 with a standard deviation of 0.11. The metrics SSIM and WaDIQaM-FR are most robust
to the transferable PGD(10) attack, as reported in Table 5.
Combining stAdv and PGD(10). The attacks stAdv and PGD are orthogonal approaches as PGD
(/lscript∞-bounded attack) manipulates the intensity of individual pixels while stAdv (spatial attack) manipulates
the location of the pixels. We now combine the two by attacking the samples generated via stAdv with
PGD(10). The mean RMSE of the generated adversarial images is 2.19 with a standard deviation of 0.41,
just 0.61 higher than images generated via stAdv. As reported in Table 5, the increase in severity of the
10Published in Transactions on Machine Learning Research (05/2023)
adversarial perturbations in stAdv+PGD(10) leads to increased transferability. This result also is consistent
with previous ﬁndings by Engstrom et al. (2019) where they combined PGD on top of their spatial attack
and found that it leads to an additive increment in the misclassiﬁcation rate.
PGD(10) stAdv stAdv+PGD(10)05001000150020002500# Samples
8%16%
5%13%12%22%Learned metrics
Traditional metrics# Accurate Samples
# Accurate Samples Flipped
Figure 5: Comparing traditional metrics (L2, SSIM,
MS-SSIM, CW-SSIM, and FSIMc) versus learned met-
rics (WaDIQaM-FR, GTI-CNN, LPIPS, DISTS, E-LPIPS,
Watson-DFT, PIM, A-DISTS, ST-LPIPS, and Swin-IQA).Summary. In this paper, we successfully
demonstrate that a wide variety of perceptual
similarity metrics are susceptible to adversar-
ial attacks. We show that adversarial pertur-
bations crafted for LPIPS(AlexNet) generated
via stAdv, can be transferred to other met-
rics. Furthermore, combining stAdv (spatial
attack) with PGD ( /lscript∞-bounded attack) in-
creases their transferability. We showcase a
few examples in Figure 6 and Figure 7. In ad-
dition, the severity of the attack increases with
the increasing number of PGD iterations (see
Table 5). Our investigations also show that
although more accurate, learned metrics may
not be more robust than traditional ones (see
Figure 5). Further tests carried out on two ad-
ditional datasets and higher resolution images,
in Appendix D, corroborate with our previous results. We demonstrate the reverse of our attack in Ap-
pendix F, i.e., we attack the less similar of the two distorted images to make it more similar to the reference
image. In summary, our ﬁndings point towards the need to develop robust perceptual similarity metrics.
5 Broader Impacts Statement
Perceptual similarity metrics have a wide variety of applications. Hence, there are beneﬁts to studying
the robustness of these metrics, and this work presents an opportunity to further improve the alignment of
these metrics with human perception. At the same time, it is important to consider the negative outcomes
of our work. Exposing the vulnerability of these metrics provides more details to malicious actors who
would want to misuse this information to attack applications that make use of these similarity metrics in
their pipeline, such as evading copyright detection. Perceptual similarity metrics can also be misused to
synthesize malware images that could go undetected online. Therefore, we suggest further research on this
topic to include appropriate defenses or more discussion on ways for mitigating such vulnerabilities. To aid
further research on this topic, we shall make our code and data publicly available.
6 Conclusion
In this paper, we studied the robustness of various traditional and learned perceptual similarity metrics to
imperceptible perturbations. We devised a methodology to craft such perturbations via adversarial attacks.
Our ﬁndings suggest that, when comparing two images with respect to a reference, the addition of imper-
ceptible distortions can overturn a metric’s similarity judgment. The results of our study indicate that even
learned perceptual metrics that match with human similarity judgments are susceptible to such impercep-
tible adversarial perturbations. We crafted adversarial examples using the spatial attack, stAdv, that were
transferable to other metrics. We show that when combined with the PGD attack, the transferability of the
adversarial examples can be further increased. Perceptual similarity metrics are designed to simulate the
human visual system, and for this reason, these metrics are increasingly used in the assessment of image
and video quality in real-world scenarios. Since invisible distortions can negatively impact the performance
of similarity metrics, future studies for the design and development of newer metrics should also focus on
validating robustness.
11Published in Transactions on Machine Learning Research (05/2023)
L2
SSIM
FSIMc
WaDIQaM-FR
GTI-CNN
DISTS
LPIPS(Squeeze)
LPIPS(VGG)
E-LPIPS
Watson-DFT
PIM-1
PIM-5
Iref
0.0091
0.8754
0.99069
1.2747
135.61
0.0996
0.0736
0.0916
0.0057
908.63
0.6141
6.2894Iother
0.0127
0.8823
0.99058
1.3567
255.97
0.0729
0.0393
0.0669
0.0041
922.66
0.4485
5.0282Iprey
0.0128
0.8721
0.99061
1.3730
220.48
0.0952
0.0421
0.0802
0.0069
1112.21
1.1852
11.3717Iadv PGD(10)
0.0128
0.8770
0.99061
1.3622
217.10
0.0873
0.0490
0.0783
0.0068
1071.77
1.2937
12.0675Iadv stAdv
0.0128
0.8635
0.99064
1.3572
217.65
0.1152
0.0517
0.1011
0.0075
1136.02
1.2917
12.2006Iadv stAdv+PGD(10)
L2
SSIM
FSIMc
WaDIQaM-FR
GTI-CNN
DISTS
LPIPS(Squeeze)
LPIPS(VGG)
E-LPIPS
Watson-DFT
PIM-1
PIM-5
Iref
0.0361
0.3163
0.98102
1.3614
133.18
0.2772
0.0986
0.2167
0.0115
2433.66
2.9635
33.8370Iother
0.0050
0.5807
0.98274
1.2760
59.11
0.2324
0.0761
0.1601
0.0103
1344.98
2.5469
27.0413Iprey
0.0057
0.5528
0.98079
1.2575
77.51
0.2739
0.1231
0.2451
0.0169
1415.91
3.2072
35.6628Iadv PGD(10)
0.0056
0.5646
0.98016
1.2983
78.95
0.2678
0.1058
0.2028
0.0170
1392.29
3.2161
37.6837Iadv stAdv
0.0063
0.5357
0.97770
1.2943
85.07
0.3021
0.1762
0.3269
0.0178
1410.53
3.5531
39.1791Iadv stAdv+PGD(10)
L2
SSIM
FSIMc
WaDIQaM-FR
GTI-CNN
DISTS
LPIPS(Squeeze)
LPIPS(VGG)
E-LPIPS
Watson-DFT
PIM-1
PIM-5
Iref
0.0010
0.9739
0.99992
1.1214
47.72
0.1180
0.0023
0.0791
0.0139
924.09
0.7539
7.0737Iother
0.0010
0.9779
0.99985
1.1190
11.53
0.0065
0.0013
0.0027
0.0002
541.48
0.0110
0.1121Iprey
0.0012
0.9730
0.99983
1.1177
79.21
0.0200
0.0025
0.0069
0.0045
783.71
1.0787
11.2964Iadv PGD(10)
0.0012
0.9743
0.99983
1.1165
85.79
0.0129
0.0017
0.0038
0.0047
693.21
1.1750
12.0483Iadv stAdv
0.0015
0.9681
0.99980
1.1184
84.42
0.0283
0.0033
0.0103
0.0052
861.64
1.1291
11.7169Iadv stAdv+PGD(10)
Figure 6: Transferable attack on perceptual similarity metrics. In example 1 (Top), the RMSE between
IpreyandIadvimages (left to right) is 1.26, 2.89, and 2.47. In example 2 (Mid.), the RMSE between Iprey
andIadvimages (left to right) is 1.29, 1.02, and 1.91. In example 3 (Bot.), RMSE between IpreyandIadv
images (left to right) is 1.43, 1.2, and 2.15. Please refer to Figure 7 in Appendix A for more examples. Text
in red indicates that the rank has ﬂipped.
12Published in Transactions on Machine Learning Research (05/2023)
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In International Conference on Machine Learning , vol-
ume 80, pp. 274–283, 2018.
Sangnie Bhardwaj, Ian Fischer, Johannes Ballé, and Troy Chinen. An unsupervised information-theoretic
perceptual quality metric. In Advances in Neural Information Processing Systems 33 , 2020.
Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and DA Forsyth. Unrestricted adversarial examples
via semantic manipulation. In International Conference on Learning Representations , 2019.
Sebastian Bosse, Dominique Maniry, Klaus-Robert Müller, Thomas Wiegand, and Wojciech Samek. Deep
neural networks for no-reference and full-reference image quality assessment. IEEE Transactions on Image
Processing , 27(1):206–219, 2018.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks
against black-box machine learning models. In International Conference on Learning Representations ,
2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE
Symposium on Security and Privacy , pp. 39–57, 2017.
CLIC. Workshop and challenge on learned image compression, 2022. URL http://www.compression.cc/ .
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion,
Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness
benchmark. arXiv/2010.09670 , 2020.
Steﬀen Czolbe, Oswin Krause, Ingemar Cox, and Christian Igel. A loss function for generative neural
networks based on watson’s perceptual model. In Advances in Neural Information Processing Systems ,
pp. 2051–2061, 2020.
Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. Image quality assessment: Unifying structure
and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence , pp. 1–1, 2020.
Keyan Ding, Yi Liu, Xueyi Zou, Shiqi Wang, and Kede Ma. Locally adaptive structure and texture similarity
for image quality assessment. In Proceedings of the 29th ACM International Conference on Multimedia ,
pp. 2483–2491, 2021.
Hadi Mohaghegh Dolatabadi, Sarah Erfani, and Christopher Leckie. Advﬂow: Inconspicuous black-box
adversarial attacks using normalizing ﬂows. In Advances in Neural Information Processing Systems , 2020.
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting
adversarial attacks with momentum. In IEEE Conference on Computer Vision and Pattern Recognition ,
pp. 9185–9193, 2018.
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep
networks. In Advances in Neural Information Processing Systems , pp. 658–666, 2016.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Exploring the
landscape of spatial robustness. In International Conference on Machine Learning , pp. 1802–1811, 2019.
Abhijay Ghildyal and Feng Liu. Shift-tolerant perceptual similarity metric. In European Conference on
Computer Vision , 2022.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
InInternational Conference on Learning Representations , 2015.
13Published in Transactions on Machine Learning Research (05/2023)
Jinjin Gu, Haoming Cai, Haoyu Chen, Xiaoxing Ye, Jimmy S. Ren, and Chao Dong. Pipal: A large-scale
image quality assessment dataset for perceptual image restoration. In European Conference on Computer
Vision, volume 12356, pp. 633–651, 2020.
Sanghyun Hong, Yigitcan Kaya, Ionut ,-Vlad Modoranu, and Tudor Dumitras. A panda? no, it’s a sloth:
Slowdown attacks on adaptive multi-exit neural network inference. In International Conference on Learn-
ing Representations , 2021.
Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In IEEE Conference on Computer
Vision and Pattern Recognition Workshops , pp. 1614–1619, 2018.
Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. Enhancing adversarial
example transferability with an intermediate level attack. In IEEE International Conference on Computer
Vision, pp. 4733–4742, 2019.
Nathan Inkawhich, Wei Wen, Hai Helen Li, and Yiran Chen. Feature space perturbations yield more
transferable adversarial examples. In IEEE Conference on Computer Vision and Pattern Recognition , pp.
7066–7074, 2019.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in
neural information processing systems , pp. 2017–2025, 2015.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-
resolution. In European Conference on Computer Vision , volume 9906, pp. 694–711, 2016.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of stylegan. In IEEE Conference on Computer Vision and Pattern
Recognition , pp. 8110–8119, 2020.
Markus Kettunen, Erik Härkönen, and Jaakko Lehtinen. Deep convolutional reconstruction for gradient-
domain rendering. ACM Transactions on Graphics , 38, 2019a.
Markus Kettunen, Erik Härkönen, and Jaakko Lehtinen. E-lpips: Robust perceptual image similarity via
random transformation ensembles. arXiv/1906.03973 , 2019b.
AlexeyKurakin, IanGoodfellow, andSamyBengio. Adversarialexamplesinthephysicalworld. International
Conference on Learning Representations - Workshop , 2017.
Cassidy Laidlaw and Soheil Feizi. Functional adversarial attacks. In Advances in Neural Information Pro-
cessing Systems , 2019.
Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense against unseen
threat models. In International Conference on Learning Representations , 2020.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew P. Aitken, Alykhan Tejani, Johannes
Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative
adversarial network. arXiv/1609.04802 , 2016.
Qizhang Li, Yiwen Guo, and Hao Chen. Yet another intermediate-level attack. In European Conference on
Computer Vision , pp. 241–257, 2020.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathe-
matical programming , 45(1):503–528, 1989.
JianzhaoLiu,XinLi,YandingPeng,TaoYu,andZhiboChen. Swiniqa: Learnedswindistanceforcompressed
image quality assessment. In IEEE Conference on Computer Vision and Pattern Recognition Workshops ,
pp. 1795–1799, 2022.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and
black-box attacks. In International Conference on Learning Representations , 2017.
14Published in Transactions on Machine Learning Research (05/2023)
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In IEEE International Conference on
Computer Vision , 2021.
Ning Lu, Li Dong, Diqun Yan, and Xianliang Jiang. On attacking deep image quality evaluator via spatial
transform. In IEEE International Conference on Systems, Man, and Cybernetics , pp. 2876–2881, 2022.
Kede Ma, Zhengfang Duanmu, and Zhou Wang. Geometric transformation invariant image quality assess-
ment using convolutional neural networks. In 2018 IEEE International Conference on Acoustics, Speech
and Signal Processing , pp. 6732–6736, 2018.
AleksanderMadry, AleksandarMakelov, LudwigSchmidt, DimitrisTsipras, andAdrianVladu. Towardsdeep
learning models resistant to adversarial attacks. In International Conference on Learning Representations ,
2018.
SaeedMahloujifar,ChongXiang,VikashSehwag,SihuiDai,andPrateekMittal. Robustnessfromperception.
InInternational Conference on Learning Representations Workshop on Security and Safety in Machine
Learning Systems , 2020.
Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In IEEE Conference on
Computer Vision and Pattern Recognition , pp. 5437–5446, 2020.
Nicolas Papernot, P. Mcdaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical black-box attacks
against deep learning systems using adversarial examples. arXiv/1602.02697 , 2016.
Ekta Prashnani, Hong Cai, Yasamin Mostoﬁ, and Pradeep Sen. Pieapp: Perceptual image-error assessment
through pairwise preference. In IEEE Conference on Computer Vision and Pattern Recognition , pp.
1808–1817, 2018.
Mehdi SM Sajjadi, Bernhard Scholkopf, and Michael Hirsch. Enhancenet: Single image super-resolution
through automated texture synthesis. In IEEE International Conference on Computer Vision , pp. 4491–
4500, 2017.
Ali Shahin Shamsabadi, Ricardo Sanchez-Matilla, and Andrea Cavallaro. Colorfool: Semantic adversarial
colorization. In IEEE Conference on Computer Vision and Pattern Recognition , pp. 1151–1160, 2020.
SanghyunSon, JaerinLee, SeungjunNah, RaduTimofte, KyoungMuLee, YihaoLiu, LiangbinXie, LiSiyao,
Wenxiu Sun, Yu Qiao, Chao Dong, Woonsung Park, Wonyong Seo, Munchurl Kim, Wenhao Zhang, Pablo
Navarrete Michelini, Kazutoshi Akita, and Norimichi Ukita. AIM 2020 challenge on video temporal super-
resolution. In European Conference on Computer Vision - Workshops , pp. 23–40, 2020.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial examples
with generative models. Advances in Neural Information Processing Systems , 31, 2018.
RainerStornandKennethPrice. Diﬀerentialevolution–asimpleandeﬃcientheuristicforglobaloptimization
over continuous spaces. Journal of global optimization , 11(4):341–359, 1997.
Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural
networks. IEEE Transactions on Evolutionary Computation , 23(5):828–841, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. International Conference on Learning Representations ,
2014.
Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE Transactions on Image
Processing , 27(8):3998–4011, 2018.
Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of trans-
ferable adversarial examples. arXiv/1704.03453 , 2017.
15Published in Transactions on Machine Learning Research (05/2023)
Florian Tramèr, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adver-
sarial example defenses. Advances in Neural Information Processing Systems , 33, 2020.
Yajie Wang, Shangbo Wu, Wenyi Jiang, Shengang Hao, Yu-an Tan, and Quanxin Zhang. Demiguise attack:
Crafting invisible semantic adversarial perturbations with perceptual similarity. In International Joint
Conference on Artiﬁcial Intelligence , 2021.
Zhou Wang and Eero P Simoncelli. Translation insensitive image similarity in complex wavelet domain. In
IEEE International Conference on Acoustics, Speech, and Signal Processing , volume 2, pp. ii–573, 2005.
Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality as-
sessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers , volume 2, pp.
1398–1402. IEEE, 2003.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE transactions on Image Processing , 13(4):600–612, 2004.
Andrew B Watson. DCT quantization matrices visually optimized for individual images. In Human vision,
visual processing, and digital display IV , volume 1913, pp. 202–216. International Society for Optics and
Photonics, 1993.
Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected Sinkhorn
iterations. In International Conference on Machine Learning , volume 97, pp. 6808–6817, 2019.
Lei Wu and Zhanxing Zhu. Towards understanding and improving the transferability of adversarial examples
in deep neural networks. In Asian Conference on Machine Learning , volume 129 of PMLR, pp. 837–850,
18–20 Nov 2020.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed
adversarial examples. In International Conference on Learning Representations , 2018.
CihangXie, JianyuWang, ZhishuaiZhang, ZhouRen, andAlanYuille. Mitigatingadversarialeﬀectsthrough
randomization. In International Conference on Learning Representations , 2018.
Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan Yuille. Improving
transferability of adversarial examples with input diversity. In IEEE Conference on Computer Vision and
Pattern Recognition , 2019.
Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi-Keung Tang, and
Alan L Yuille. Adversarial attacks beyond the image space. In IEEE Conference on Computer Vision and
Pattern Recognition , 2019.
HuanZhang,HonggeChen, ZhaoSong,DuaneBoning,InderjitSDhillon, andCho-JuiHsieh. Thelimitations
ofadversarialtrainingandtheblind-spotattack. In International Conference on Learning Representations ,
2018a.
Kai Zhang, Shuhang Gu, and Radu Timofte. NTIRE 2020 challenge on perceptual extreme super-resolution:
Methods and results. In IEEE Conference on Computer Vision and Pattern Recognition Workshops , pp.
492–493, 2020.
Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. FSIM: a feature similarity index for image quality
assessment. IEEE Transactions on Image Processing , 20(8):2378–2386, 2011.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonableeﬀectiveness
of deep features as a perceptual metric. In IEEE Conference on Computer Vision and Pattern Recognition ,
pp. 586–595, 2018b.
Weixia Zhang, Dingquan Li, Xiongkuo Min, Guangtao Zhai, Guodong Guo, Xiaokang Yang, and Kede Ma.
Perceptual attacks of no-reference image quality models with human-in-the-loop. In Advances in Neural
Information Processing Systems , 2022.
16Published in Transactions on Machine Learning Research (05/2023)
Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang, Xiang Gan, and Yong Yang. Trans-
ferable adversarial perturbations. In European Conference on Computer Vision , pp. 452–467, 2018.
Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A. Efros. Generative visual manipulation
on the natural image manifold. In European Conference on Computer Vision , volume 9909, pp. 597–613,
2016.
17Published in Transactions on Machine Learning Research (05/2023)
A Transferable Attack on Perceptual Similarity Metrics
L2
SSIM
FSIMc
WaDIQaM-FR
GTI-CNN
DISTS
LPIPS(Squeeze)
LPIPS(VGG)
E-LPIPS
Watson-DFT
PIM-1
PIM-5
Iref
0.0079
0.7717
0.99940
1.3602
95.99
0.1303
0.1149
0.1893
0.0115
1501.56
2.1654
21.3579Iother
0.0047
0.8554
0.99937
1.2796
81.11
0.1030
0.0794
0.1188
0.0077
1025.79
1.0225
10.0332Iprey
0.0048
0.8501
0.99926
1.2949
139.53
0.1111
0.0880
0.1346
0.0124
1278.13
2.8536
26.9732Iadv PGD(10)
0.0047
0.8526
0.99922
1.2962
133.61
0.1070
0.0855
0.1244
0.0131
1305.37
3.2559
29.6540Iadv stAdv
0.0048
0.8436
0.99903
1.3113
165.44
0.1139
0.0940
0.1409
0.0140
1422.43
3.1688
29.2036Iadv stAdv+PGD(10)
L2
SSIM
FSIMc
WaDIQaM-FR
GTI-CNN
DISTS
LPIPS(Squeeze)
LPIPS(VGG)
E-LPIPS
Watson-DFT
PIM-1
PIM-5
Iref
0.0121
0.9068
0.99392
1.1942
53.66
0.1341
0.0264
0.0545
0.0039
1097.13
0.2170
3.4366Iother
0.0133
0.9112
0.99181
1.2634
28.88
0.1034
0.0371
0.0462
0.0033
901.26
0.2429
2.9138Iprey
0.0133
0.9006
0.99185
1.2653
62.75
0.1121
0.0395
0.0520
0.0055
1147.84
1.0924
12.0777Iadv PGD(10)
0.0133
0.9103
0.99187
1.2699
61.31
0.1056
0.0375
0.0472
0.0054
1078.05
1.2546
13.0601Iadv stAdv
0.0133
0.8958
0.99183
1.2813
69.90
0.1132
0.0411
0.0571
0.0065
1157.19
1.2119
13.2696Iadv stAdv+PGD(10)
L2
SSIM
FSIMc
WaDIQaM-FR
GTI-CNN
DISTS
LPIPS(Squeeze)
LPIPS(VGG)
E-LPIPS
Watson-DFT
PIM-1
PIM-5
Iref
0.0368
0.6526
0.96532
1.1196
473.92
0.2266
0.0694
0.2057
0.0138
2314.26
1.6024
14.5537Iother
0.0336
0.6797
0.97225
1.0508
325.45
0.2105
0.0467
0.1614
0.0119
2072.06
0.8015
10.2235Iprey
0.0339
0.6693
0.97213
1.0513
379.77
0.2278
0.0516
0.1823
0.0131
2137.63
1.4746
14.8395Iadv PGD(10)
0.0338
0.6778
0.97228
1.0550
382.28
0.2132
0.0483
0.1640
0.0139
2006.34
1.5086
15.0432Iadv stAdv
0.0342
0.6667
0.97208
1.0580
386.48
0.2297
0.0545
0.1897
0.0132
2305.92
1.5491
15.2823Iadv stAdv+PGD(10)
Figure 7: Transferable attack on perceptual similarity metrics. In example 1 (Top), the RMSE between
IpreyandIadvimages (left to right) is 1.35, 1.43, and 2.25. In example 2 (Mid.), the RMSE between Iprey
andIadvimages (left to right) is 1.25, 0.95, and 1.77. In example 3 (Bot.), RMSE between IpreyandIadv
images (left to right) is 1.37, 0.99, and 2.0. Text in red indicates that the rank has ﬂipped.
18Published in Transactions on Machine Learning Research (05/2023)
B FGSM Attack on Similarity Metrics
We explain the FGSM attack on perceptual similarity metrics in Algorithm 3.
Algorithm 3: FGSM attack on Similarity Metrics
Input:I1,I2,Iref, metricf,max_/epsilon1(0.05)
Output: Least/epsilon1value which led to rank ﬂip
1s0=f(Iref,I0);s1=f(Iref,I1);
2rank =int(s0>s1)// IfI0is more similar to Irefthenrankis 0 else 1
3ifrank = 1thenIprey=I1;sother=s0;
4elseIprey=I0;sother=s1;
5sprey=f(Iref,Iprey )
6J=/parenleftbig
(sother/(sother +sprey ))−1/parenrightbig2// Loss
7signed_grad=sign/parenleftbig
∇IpreyJ/parenrightbig
8/epsilon1= 0.0001
9while/epsilon1≤max_/epsilon1do
10Iadv =Iprey +/epsilon1·signed_grad
11Iadv =clip(Iadv,min =−1,max = 1)// range [-1,1]
12sadv =f(Iref,Iadv)
13 ifsadv>sotherthen
14 return True// Attack successful
15/epsilon1=/epsilon1+ 0.0001
16return 1// Largest value of /epsilon1
C One-Pixel Attack on Similarity Metrics
𝐼!"#1𝑝𝑖𝑥𝑒𝑙=𝐼$+
𝐿𝑃𝐼𝑃𝑆:0.1868𝐿𝑃𝐼𝑃𝑆:0.0445
Figure 8: One-pixel attack on LPIPS(Alex). This is a black-box attack as it does not require LPIPS network
parameters to generate the adversarial perturbations. The one-pixel perturbation is hardly visible. The
RMSE between the prey image I1and the adversarial image Iadvis 1.38.
D Results on Additional Datasets and High Resolution Images
Additional datasets. To test the vulnerability of perceptual similarity on higher image resolutions to
adversarial attacks, we use the PieAPP test dataset (Prashnani et al., 2018) and the CLIC validation
dataset (CLIC, 2022). The CLIC dataset contains 5220 triplet samples (reference, distorted image A, and
distorted image B), and it acts as a test dataset for us since none of the metrics have been trained on it.
The PieAPP test set consists of 40 reference images with 15 distorted images per reference image. Out of
these, we only select those triplet samples where the preference for distorted image A over B was >85% and
vice versa. Hence, we end up with 1381 samples for our experiment. The original image size of the CLIC
samples is 768x768 while for PieAPP is 256x256.
White-box PGD attack. We ﬁrst test the white-box attack on metrics via PGD. As shown in the Tables
6 anf 7, the white-box PGD attack is easily ﬂipping rankings on both datasets. The samples on the PieAPP
dataset are harder to ﬂip than the CLIC dataset. We posit that the reason for this lies in the selection
criteria for our samples. Since for the PieAPP dataset, we chose only those samples where human preference
19Published in Transactions on Machine Learning Research (05/2023)
Table 6: Whitebox PGD attack results on the PieAPP dataset.
Network Image ResolutionSame Rank
by Human
& MetricTotal
SamplesPGD
#Samples
Flipped% pixels with /epsilon1 RMSE
>0.001 >0.01 >0.03µ σ
L264x64X 899 126/14.0% 67.5 49.9 0.0 1.7 0.8
7 482 65/13.5% 80.2 48.6 0.0 1.7 0.9
256x256X 963 59/6.1% 87.8 69.8 0.0 2.0 0.9
7 418 46/11.0% 85.9 62.4 0.0 2.1 1.0
SSIM
(Wang et al., 2004)64x64X 910 391/43.0% 97.8 67.0 0.0 2.0 0.9
7 471 120/25.5% 94.3 44.1 0.0 1.7 1.0
256x256X 990 364/36.8% 96.7 68.9 0.0 2.1 0.9
7 391 185/47.3% 95.0 54.2 0.0 1.8 1.0
LPIPS(Alex)
(Zhang et al., 2018b)64x64X 1016 861/84.7% 90.2 30.3 0.0 1.3 0.7
7 365 347/95.1% 89.9 31.7 0.0 1.3 0.6
256x256X 1184 868/73.3% 90.4 39.9 0.0 1.5 0.6
7 197 191/97.0% 84.2 20.3 0.0 1.1 0.6
DISTS
(Ding et al., 2020)64x64X 1041 125/12.0% 97.8 73.5 0.0 2.2 0.9
7 340 70/20.6% 96.4 65.1 0.0 2.1 1.0
256x256X 1286 47/3.7% 97.4 73.8 0.0 2.3 1.0
7 95 25/26.3% 95.4 70.1 0.0 2.1 1.1
ST-LPIPS(Alex)
(Ghildyal & Liu, 2022)64x64X 1005 823/81.9% 89.4 24.4 0.0 1.2 0.7
7 376 370/98.4% 87.2 26.3 0.0 1.2 0.6
256x256X 1239 599/48.3% 93.9 55.1 0.0 1.8 0.7
7 142 138/97.2% 90.4 33.9 0.0 1.4 0.7
Table 7: Whitebox PGD attack results on the CLIC dataset.
Network Image ResolutionSame Rank
by Human
& MetricTotal
SamplesPGD
#Samples
Flipped% pixels with /epsilon1 RMSE
>0.001 >0.01 >0.03µ σ
L2256x256X 3167 3152/99.5% 67.6 26.6 0.0 1.1 0.6
7 2053 2027/98.7% 67.5 19.6 0.0 1.0 0.6
512x512X 3120 2911/93.3% 74.8 37.2 0.0 1.4 0.8
7 2100 1918/91.3% 74.8 30.7 0.0 1.3 0.8
768x768X 2992 2399/80.2% 79.8 45.4 0.0 1.6 0.9
7 2228 1762/79.1% 80.5 48.0 0.0 1.6 0.8
SSIM
(Wang et al., 2004)256x256X 3307 3307/100.0% 84.2 5.7 0.0 0.8 0.4
7 1913 1912/99.9% 76.0 6.2 0.0 0.8 0.4
512x512X 3200 3189/99.7% 89.1 14.6 0.0 1.0 0.5
7 2020 2005/99.3% 85.8 11.4 0.0 0.9 0.5
768x768X 2997 2941/98.1% 89.9 18.6 0.0 1.1 0.6
7 2223 2173/97.8% 87.8 14.9 0.0 1.0 0.6
LPIPS(Alex)
(Zhang et al., 2018b)256x256X 3820 3820/100.0% 54.5 0.0 0.0 0.7 0.0
7 1400 1400/100.0% 39.3 0.0 0.0 0.7 0.0
512x512X 3965 3965/100.0% 64.4 0.3 0.0 0.7 0.1
7 1255 1255/100.0% 50.8 0.1 0.0 0.7 0.1
768x768X 3849 3839/99.7% 73.6 2.4 0.0 0.7 0.2
7 1371 1371/100.0% 67.8 0.7 0.0 0.7 0.1
DISTS
(Ding et al., 2020)256x256X 3822 3327/87.0% 97.4 55.2 0.0 1.7 0.8
7 1398 1308/93.6% 95.9 41.4 0.0 1.5 0.8
512x512X 4004 2626/65.6% 98.6 72.7 0.0 2.1 0.9
7 1216 968/79.6% 98.2 62.4 0.0 1.9 0.9
768x768X 3952 1286/32.5% 98.6 80.0 0.0 2.4 0.9
7 1268 499/39.4% 96.9 69.4 0.0 2.2 0.9
ST-LPIPS(Alex)
(Ghildyal & Liu, 2022)256x256X 3793 3793/100.0% 56.1 0.0 0.0 0.7 0.0
7 1427 1427/100.0% 40.2 0.0 0.0 0.7 0.0
512x512X 4026 4026/100.0% 70.4 0.4 0.0 0.7 0.1
7 1194 1194/100.0% 53.5 0.1 0.0 0.7 0.1
768x768X 4021 4009/99.7% 81.3 5.2 0.0 0.8 0.3
7 1199 1199/100.0% 72.8 1.8 0.0 0.7 0.2
for a distorted image over the other was >85%, it seems that the margin between the classes, namely, “less
similar” and “more similar” to the reference, is larger, than in the CLIC dataset, making it harder to ﬂip
the rank.
20Published in Transactions on Machine Learning Research (05/2023)
White-box stAdv attack. We attack the LPIPS(Alex) metric using stAdv on the PieAPP dataset. For
the images with higher resolution, it was harder to ﬂip rank. However, that could be due to the settings of
our setup. In the loss deﬁned in Equation 4 for the stAdv attack, minimizing Lflowconstrains the amount of
ﬂow used to generate the adversarial perturbations while minimizing Lrankencourages more perturbations.
Hence, if we increase α, i.e., the weight for Lrank, a larger amount of perturbations would be generated as
the ﬂow generating adversarial perturbations will be less constrained. As shown in Table 5, we observe that
increasingαhelps ﬂipping rank for more samples, but the RMSE of the IadvwithIpreyis also higher.
Table 8: Whitebox stAdv attack on LPIPS(Alex) on the PieAPP dataset.
Image Resolution #Accurate Samples αfrom Equation 4 # Accurate Samples Flipped RMSE ( µ/σ)
64x64 101650 899/88.5% 4.3/2.0
200 1000/98.4% 5.8/3.1
1000 1016/100.0% 7.8/4.9
256x256 118450 28/2.4% 0.8/0.3
200 158/13.3% 2.1/1.3
1000 566/47.8% 3.7/1.9
Transferable Adversarial attack. Here we test the transferable PGD(20) attack. In this experiment,
we attack the LPIPS(Alex) metric using the PGD. This experiment is performed on the PieAPP dataset
because we found it harder to ﬂip samples on it. Out of the 1184 accurate samples, the rank ﬂipped for 635
samples with a mean RMSE of 1.92 with a standard deviation of 0.15. We test the transferability of these
635 samples to other perceptual similarity metrics. We found that although the metrics did change their
scores due to the adversarial perturbations, worsening their prediction, it was still harder to ﬂip ranks on
this dataset. Less than 10% of the samples ﬂipped ranks. However, the transferable attack results in Table 9
are consistent with the results on the BAPPS dataset in Table 5 of the main paper. The traditional metrics
are more robust than the learned metrics, while the learned metrics are more accurate. The transformer-
based metric swinIQA has high accuracy and robustness. E-LPIPS and ST-LPIPS(VGG) which are more
robust variants of LPIPS(VGG), showcase more robustness, with ST-LPIPS(VGG) also being more accurate.
Similarly, PIM-1 and DISTS are also accurate, along with being more robust. Surprisingly, WaDIQaM-FR
showcases higher accuracy on the PieAPP dataset than on the BAPPS dataset, along with being robust on
both datasets.
Table 9: Transferable PGD(20) attack on perceptual similarity metrics.
Network #Accurate Samples #Accurate Samples Flipped via PGD(20)
L2 448/71% 2/0.4%
SSIM (Wang et al., 2004) 456/72% 17/3.7%
MS-SSIM (Wang et al., 2003) 460/72% 11/2.4%
CWSSIM (Wang & Simoncelli, 2005) 414/65% 15/3.6%
FSIMc (Zhang et al., 2011) 461/73% 4/0.9%
WaDIQaM-FR (Bosse et al., 2018) 602/95% 13/2.2%
GTI-CNN (Ma et al., 2018) 454/71% 2/0.4%
PieAPP Prashnani et al. (2018) 476/75% 8/1.7%
LPIPS(Squz.) (Zhang et al., 2018b) 611/96% 26/4.3%
LPIPS(VGG) (Zhang et al., 2018b) 554/87% 63/11.4%
E-LPIPS (Kettunen et al., 2019b) 554/87% 8/1.4%
DISTS (Ding et al., 2020) 607/96% 24/4.0%
Watson-DFT (Czolbe et al., 2020) 475/75% 32/6.7%
PIM-1 (Bhardwaj et al., 2020) 558/88% 22/3.9%
PIM-5 (Bhardwaj et al., 2020) 550/87% 33/6.0%
A-DISTS (Ding et al., 2021) 512/81% 36/7.0%
ST-LPIPS(Alex) (Ghildyal & Liu, 2022) 614/97% 14/2.3%
ST-LPIPS(VGG) (Ghildyal & Liu, 2022) 584/92% 25/4.3%
SwinIQA (Liu et al., 2022) 597/94% 17/2.8%
21Published in Transactions on Machine Learning Research (05/2023)
E FGSM versus PGD attack
In our experiments in Table 2, the value chosen for the maximum allowable /lscript∞perturbation for the PGD
attack is lower than that for the FGSM attack. However, if the value is the same, then PGD would be better
at ﬂipping the rankings. As shown in Table 10, the PGD attack is more successful than FGSM. In the case
of traditional metrics, the results for both attacks are similar. However, for learned perceptual similarity
metrics like LPIPS, the number of ﬂips by PGD are greater with a lesser amount of perturbation required.
Table 10: FGSM and PGD attack results when the maximum /lscript∞-norm perturbation is the same for both.
NetworkSame Rank
by Human
& MetricTotal
SamplesFGSM (/epsilon1<0.03) PGD
#Samples
FlippedMean
/epsilon1RMSE#Samples
Flipped% pixels with /epsilon1RMSE
µ σ >0.001 >0.01 >0.03µ σ
L2X 9750 2419/25% 0.014 1.9 1.0 2348/24% 84.4 56.1 0.0 1.9 1.0
7 2477 1220/49% 0.011 1.5 1.0 1202/49% 82.0 42.7 0.0 1.5 1.0
SSIM X 9883 5383/54% 0.012 1.7 1.0 5297/54% 94.6 53.6 0.0 1.8 1.0
(Wang et al., 2004) 7 2344 1851/79% 0.008 1.3 0.8 1843/79% 87.3 32.0 0.0 1.3 0.8
LPIPS(Alex) X 11303 5620/50% 0.012 1.7 1.0 8806/78% 86.8 28.7 0.0 1.3 0.6
(Zhang et al., 2018b) 7 924 897/97% 0.003 0.9 0.4 917/99% 59.5 3.2 0.0 0.8 0.3
LPIPS(VGG) X 10976 7431/68% 0.008 1.3 0.9 9689/88% 81.6 15.6 0.0 1.0 0.5
(Zhang et al., 2018b) 7 1251 1235/99% 0.002 0.8 0.4 1246/100% 52.3 1.6 0.0 0.7 0.2
DISTS X 11158 1827/16% 0.015 2.1 1.6 2306/21% 97.0 75.4 0.0 2.6 1.3
(Ding et al., 2020) 7 1069 643/60% 0.011 1.0 1.0 723/68% 91.9 50.0 0.0 2.0 1.3
In the PGD attack, the step size αis often greater when compared to ours, i.e., 0.001, such that it allows
the perturbations to go beyond the maximum /lscript∞-norm threshold /epsilon1and then can be projected back to the /epsilon1
radius. We test with a larger values of α, and as shown in Table 11, the severity of the attack increases as α
is increased, however, at the expense of more % pixels with perturbation >0.01. The other parameters for
the attack are kept the same.
Table 11: PGD attack results with increasing step size α.
Network αSame Rank
by Human
& MetricTotal
SamplesPGD
#Samples
Flipped% pixels with /epsilon1RMSE
>0.001 >0.01 >0.03µ σ
L20.00100X 9750 2348/24% 84.4 56.1 0.0 1.9 1.0
7 2477 1202/49% 82.0 42.7 0.0 1.5 1.0
0.00375X 9750 2419/25% 87.3 63.8 0.0 1.9 1.0
7 2477 1220/49% 88.2 51.0 0.0 1.6 1.0
0.00600X 9750 2419/25% 87.3 67.9 0.0 2.2 1.1
7 2477 1220/49% 88.2 55.6 0.0 1.8 1.1
SSIM0.00100X 9883 5297/54% 94.6 53.6 0.0 1.8 1.0
7 2344 1843/79% 87.3 32.0 0.0 1.3 0.8
0.00375X 9883 5418/55% 99.2 63.7 0.0 1.8 1.0
7 2344 1858/79% 99.0 40.5 0.0 1.3 0.9
0.00600X 9883 5418/55% 99.1 70.5 0.0 2.1 1.1
7 2344 1858/79% 99.0 46.8 0.0 1.5 1.0
LPIPS(Alex)
(Zhang et al., 2018b)0.00100X 11303 8806/78% 86.8 28.7 0.0 1.3 0.6
7 924 917/99% 59.5 3.2 0.0 0.8 0.3
0.00375X 11303 9926/88% 90.4 45.3 0.0 1.5 0.8
7 924 920/100% 93.4 7.5 0.0 0.8 0.3
0.00600X 11303 9994/88% 88.0 55.2 0.0 1.8 0.8
7 924 920/100% 93.4 15.1 0.0 0.9 0.4
LPIPS(VGG)
(Zhang et al., 2018b)0.00100X 10976 9689/88% 81.6 15.6 0.0 1.0 0.5
7 1251 1246/100% 52.3 1.6 0.0 0.7 0.2
0.00375X 10976 10322/94% 89.9 29.6 0.0 1.2 0.7
7 1251 1248/100% 95.8 3.7 0.0 0.7 0.2
0.00600X 10976 10337/94% 88.4 40.8 0.0 1.4 0.8
7 1251 1248/100% 96.2 7.9 0.0 0.8 0.3
22Published in Transactions on Machine Learning Research (05/2023)
F Reversing the PGD Attack
𝐼!"#𝛿=𝐼$+𝐿𝑃𝐼𝑃𝑆:0.044𝐿𝑃𝐼𝑃𝑆:0.144
Figure 9: PGD attack on LPIPS(Alex). We make the less similar of the two distorted images more similar
to the reference image. The RMSE between the prey image I0and the adversarial image Iadvis 4.20.
In this experiment, we try the reverse of the white-box PGD attack in Section 3. For this attack, we do
the opposite, i.e., we attack the distorted image that is less similar to Iref. Before the attack, the original
rank issother<sprey, but after the attack Ipreyturns intoIadv, and when the rank ﬂips, sadv<sother. We
use the LPIPS network parameters to compute the signed gradient via the loss function in Equation 7. As
shown in Table 12, it is possible to reverse the attack performed in Table 2.
J(θ,Iprey,Iother,Iref) =/parenleftbiggsother
sother +sprey/parenrightbigg2
(7)
Table 12: Reverse PGD attack results. Here we attack the less similar distorted image and make it more
similar to the reference image. Below are the results of the Whitebox PGD attack on the BAPPS dataset.
NetworkSame Rank
by Human
& MetricTotal
SamplesPGD
#Samples
Flipped% pixels with /epsilon1 RMSE
>0.001 >0.01 >0.03µ σ
LPIPS(Alex)
(Zhang et al., 2018b)X 11303 6758/59.9% 87.0 27.4 0.0 1.27 0.59
7 924 858/92.9% 60.3 5.6 0.0 0.82 0.34
23