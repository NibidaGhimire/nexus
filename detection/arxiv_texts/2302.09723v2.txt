Emphasizing Unseen Words: New Vocabulary
Acquisition for End-to-End Speech Recognition
Leyuan Qua,b,, Cornelius Webera, Stefan Wermtera
aKnowledge Technology, Department of Informatics, University of Hamburg, Hamburg,
Germany
bDepartment of Articial Intelligence, Zhejiang Lab, Hangzhou, China
Abstract
Due to the dynamic nature of human language, automatic speech recogni-
tion (ASR) systems need to continuously acquire new vocabulary. Out-Of-
Vocabulary (OOV) words, such as trending words and new named entities, pose
problems to modern ASR systems that require long training times to adapt their
large numbers of parameters. Dierent from most previous research focusing on
language model post-processing, we tackle this problem on an earlier process-
ing level and eliminate the bias in acoustic modeling to recognize OOV words
acoustically. We propose to generate OOV words using text-to-speech systems
and to rescale losses to encourage neural networks to pay more attention to
OOV words. Specically, we enlarge the classication loss used for training
neural networks' parameters of utterances containing OOV words (sentence-
level), or rescale the gradient used for back-propagation for OOV words (word-
level), when ne-tuning a previously trained model on synthetic audio. To over-
come catastrophic forgetting, we also explore the combination of loss rescaling
and model regularization, i.e. L2 regularization and elastic weight consolidation
(EWC). Compared with previous methods that just ne-tune synthetic audio
with EWC, the experimental results on the LibriSpeech benchmark reveal that
Corresponding author
Email addresses: quleyuan9826@gmail.com (Leyuan Qu),
cornelius.weber@uni-hamburg.de (Cornelius Weber), stefan.wermter@uni-hamburg.de
(Stefan Wermter)
URL: http://www.informatik.uni-hamburg.de/WTM/ (Leyuan Qu)
Preprint submitted to Neural Networks February 22, 2023arXiv:2302.09723v2  [cs.CL]  21 Feb 2023our proposed loss rescaling approach can achieve signicant improvement on the
recall rate with only a slight decrease on word error rate. Moreover, word-level
rescaling is more stable than utterance-level rescaling and leads to higher recall
rates and precision on OOV word recognition. Furthermore, our proposed com-
bined loss rescaling and weight consolidation methods can support continual
learning of an ASR system.
Keywords: Automatic speech recognition, continual learning,
out-of-vocabulary word recognition, end-to-end learning, loss rescaling
1. Introduction
Recently, end-to-end ASR models have been receiving a lot of attention and
achieving impressive performance [1, 2, 3]. These models signicantly simplify
the training process to directly map acoustic inputs to characters or words.
Additionally, limited domain-specic knowledge is required, which dramatically
boosts model development and deployment. However, end-to-end models need
a lot of training data and perform poorly on words out-of-vocabulary (OOV)
or rarely existing in the training data, for example, trending words and new
named entities.
Since it takes substantial eorts to collect labeled OOV speech data for ASR
model training, current approaches to tackle the OOV problem mainly involve a
language model (LM) or post-processing, for instance, user-dependent language
models [4, 5], LM rescoring [6] and nite-state transducer lattice extension [7].
However, the post-processing techniques only obtain limited improvement as
they do not tackle the root causes at the acoustic level.
Alternatively, ne-tuning end-to-end ASR models with synthetic audio con-
taining OOV words can eciently improve the recall rate of unseen vocabu-
lary, which usually leverages advanced text-to-speech (TTS) systems to generate
audio-text pairs required for ASR model training. However, the catastrophic
forgetting problem substantially degrades the overall performance of ASR sys-
tems, especially on non-OOV words. Elastic weight consolidation (EWC) [8] is
2adapted to tackle this problem but leads to a limited recall rate improvement for
OOV word recognition. In this paper, we take this method a step further and
propose loss rescaling to encourage models to pay more attention to unknown
words. Instead of just ne-tuning ASR models where all words are treated
equally, enlarging the loss of utterances containing OOV words (sentence-level)
or increasing the gradient of unseen words (word-level) can eciently incline the
model to update the weights related to OOV words. We choose 100 OOV words
appearing in LRS3-TED dataset but not existing in LibriSpeech dataset. Then,
we crawl texts including the new words from the Internet and synthesize audio
with TTS systems. The experimental results of ne-tuning audio-text pairs on
a hybrid CTC1/attention ASR model show a signicant improvement on recall.
When combining EWC with the word-level loss rescaling, we achieve 45.81% of
recall on the ROOV test with only 7.8% and 4.6% of relative WER increase on
the LibriSpeech test-clean and test-other data sets respectively. As a result, we
have improved the recognition of OOV words while maintaining the accuracy
for non-OOV words.
2. Related Work
2.1. The Recognition of OOV Words in End-to-End ASR Models
Since the OOV problem has occurred, a number of approaches have been
proposed for conventional GMM-HMM2models [9, 10] and hybrid DNN-HMM3
models [11, 12]. In this section, we only review methods towards end-to-end ASR
architectures which have been the most promising methods in speech recognition
in recent years.
Aleksic et al. [13] extend class-based LMs [4, 5] by creating a user-dependent
small LM for contact name recognition on voice commands, which is compiled
1CTC is the abbreviation for Connectionist Temporal Classication.
2GMM and HMM are short for Gaussian Mixture Model and Hidden Markov Model re-
spectively.
3DNN is short for Deep Neural Network.
3dynamically based on the contact names on users' devices. Moreover, contact
insertion reward is proposed to avoid excessive bias and to balance the infor-
mation between user-dependent and user-independent cases. Hori et al. [14]
combine word-level with character-level language modeling in end-to-end archi-
tectures. With the word-level LM, the model can achieve better performance
by learning stronger and longer context information, while character-level LM
is used to overcome the OOV issue that the word-level LM suers from. A sim-
ilar idea is investigated by Li et al.[15] on acoustic-to-word mapping employing
character-level modeling units to tackle OOV issues. Williams et al. [16] lever-
age contextual information, for instance, users' locations, users' favorite songs,
and calendar events, to partially rescore the output likelihood from sequence-to-
sequence models during beam search instead of bringing an additional LM in.
Since previous work does not consider errors generated by speech recognition
systems when combining them with external LMs, Guo et al. [6] incorporate
a spelling correction model into the speech recognizer training, that directly
maps speech recognizer outputs to ground-truth texts. The experimental re-
sults suggest that the proposed spelling correction model outperforms n-best
LM rescoring and TTS data ne-tuning.
To enable on-device end-to-end speech recognition models to individually
recognize new named entities, such as the contact names on mobile phones, Sim
et al. [17] compare LM biasing and acoustic model ne-tuning methods. Further-
more, several techniques, such as layer freezing, early stopping, and EWC, are
investigated to suppress model overtting during ne-tuning. Instead of using
a word-level LM in ASR in which a pre-dened lexicon is required, Likhoma-
nenko et al. [18] attempt to decode acoustic models with a character-level LM
which is not constrained by lexicons. The lexicon-free decoder achieves better
results on OOV experiments since the character-level LM is naturally able to
handle unseen words. Dierent from traditional hybrid ASR models, for in-
stance DNN-HMM, end-to-end ASR architectures aim to learn LMs and acous-
tic models into one module, which leads to no clear division between language
and acoustic models in the end-to-end fashion. To combine LMs trained from
4a dierent domain, Variani et al. [19] propose hybrid autoregressive transducer
(HAT) to separately model internal LM used for training and an external LM
from a dierent domain used for inference. Consequently, Meng et al. [20] im-
prove HAT by estimating and subtracting the internal LM scores, and properly
integrate an external LM into end-to-end ASR architectures without any further
training. To further improve the model performance on proper nouns, Zhao et
al. [7] optimize the shallow-fusion method [21] (integrate an external LM into
the inference of a sequence-to-sequence model) by building the LM at subword
level instead of at word level. Additionally, they propose the early contextual
nite state transducer (FST) to avoid the proper noun candidates being pruned
during the Viterbi beam search. Moreover, a common set of prexes is utilized
to avoid the contextual biasing always being active and prevent models from
degrading on cases not containing OOV words.
Dierent from most of the previous work focusing on LM post-processing
which requires candidate units existing in n-best lists or decoding lattices, in this
paper, we tackle the OOV problem on an earlier processing level by eliminating
the bias in acoustic modeling to recognize OOV words acoustically.
2.2. Data Augmentation with Synthetic Audio for ASR
Proper speech data augmentation does not only boost the model perfor-
mance, but can also signicantly improve system robustness and generaliza-
tion [22]. There are many strategies used in ASR training, for example, noise
addition, pitch shifting, speed perturbation, back-translation [23] and room im-
pulse response injection with real or simulated data [24]. More recently, a sim-
ple yet eective approach, SpecAugment [25], has been proposed and achieves
state-of-the-art results on the LibriSpeech benchmark corpus. The basic idea
for SpecAugment is randomly masking or cropping a xed area on spectro-
grams in the time- or frequency-domain, which eectively prevents model over-
tting, especially for noisy conditions. Another well-established method is mix-
ing synthetic audio with real data by leveraging advanced TTS models, like
Tacotron2 [26], DeepVoice3 [27] and FlowTron [28].
5Rossenbach et al. [29] compare commonly used data augmentation strate-
gies with the TTS audio. The results reveal the eectiveness of TTS data in
ASR system training. Laptev et al. [30] investigate the eect of augmenting
data with TTS audio for low-resource speech recognition. The resulting models
outperform other systems with the same setting and semi-supervised learning
methods. Furthermore, other authors explore the inuence of the audio quality
with dierent vocoders, i.e. Grin-Lim and LPCNet [31]. Instead of just mixing
the synthetic audio data during training, Rosenberg et al. [32] exploit the im-
pact of the TTS model's eectiveness and diversity on ASR results. Moreover,
lexical diversity is also investigated on domain adaptation experiments.
Inspired by the benet brought by TTS data, we synthesize audio with text
crawled from the Internet containing OOV words as the training set for new
vocabulary acquisition and model adaptation.
2.3. End-to-End ASR Architectures
End-to-end ASR architectures aim to directly map acoustic observations to
text transcripts, such as characters and words. Dierent from conventional sys-
tems which separately train the acoustic model and LM with dierent criteria
and corpora, end-to-end ASR systems train all modules jointly. Consequently,
the training progress is signicantly simplied and does not require too much
domain-specic knowledge. There are four main categories in end-to-end learn-
ing, i.e. CTC, Recurrent Neural Network Transducer (RNN-T), encoder-decoder
with attention and hybrid CTC/attention architectures. They mainly dier in
their alignment of input acoustic features and output label sequences.
2.3.1. CTC-based End-to-End Models
The basic idea of CTC-based approaches is to bring in a special token,
'blank', which is dynamically lled in the places between modeling units. Con-
sequently, the exact boundary information required by conventional methods
is not needed anymore. Then, a carefully designed dynamic programming al-
gorithm is used to search optimal paths and convert the frame-level token se-
6quences to meaningful utterances by removing blank tokens and merging re-
peated labels.
Graves et al. [1] rstly adopt the ground-breaking CTC approach to over-
come the problems faced in conventional ASR systems, i.e. the requirement of
frame-level segmentation and the mapping from model outputs to ground-truth
labels. Hannun et al. [33] simplify the CTC building process and propose a
modied prex-search decoding algorithm to completely discard the cumber-
some decoding strategies used in HMM-based systems. Amodei et al. [34] con-
duct a comprehensive evaluation on model architecture, system optimization,
and model deployment. Furthermore, parallel training on model level [35] and
data level [36] is utilized to heavily speed up the training process.
Furthermore, substituting RNNs with Convolutional Neural Networks (CNNs)
has been a growing trend since pure CNN-based architectures can dramatically
reduce the training time and inference latency, which is critical for speech recog-
nition tasks running in real time. Jasper [37] achieves state-of-the-art results on
the LibriSpeech dataset by stacking the CNN-only blocks. Inspired by Jasper,
Krimany et al. [38] propose QuartzNet which uses a similar fully convolutional
architecture but with signicantly fewer parameters. QuartzNet enables the
CTC-based end-to-end ASR model to run locally on mobile devices.
2.3.2. RNN-T End-to-End Models
CTC assumes that every output node is conditionally independent of other
outputs, which makes it dicult to model the dependency between adjacent
frames. Consequently, RNN-T [2] was proposed to overcome the unreasonable
conditionally independence assumptions. Dierent from CTC which only fo-
cuses on acoustic sequence modeling, RNN-T uses a separate module, called
prediction network, to model the context information, which can be treated as
an intrinsic language model, followed by a joint network to classify the concate-
nation of encoder and prediction network outputs.
Rao et al. [39] exploit some strategies for RNN-T architecture training and
nd that CTC-based encoder pre-training and language model-based prediction
7network pre-training improve the model performance. Zhang et al. [40] present
a Transformer-based [41] RNN-T model, in which the Transformer modules are
used to learn representations from speech signals and text sequences. Inspired
by Jasper [37] and QuartzNet [38], Han et al. [42] propose ContextNet by in-
troducing a fully convolutional encoder into the RNN-T architecture. Since the
CNN has a weaker receptivity for long context than Long Short-Term Memory
(LSTM) and Transformers, the squeeze-and-excitation (SE) layer is integrated
into ContextNet to enhance long-distance dependence. Gulati et al. [43] propose
Conformer which combines CNNs with self-attention [41] to concurrently learn
local and global features, and show signicant improvement on the LibriSpeech
test and test-other set.
2.3.3. Attention-based Encoder-Decoder Models
Another branch of end-to-end systems is the attention-based encoder-decoder
architecture. Dierent from CTC or RNN-T architectures, the attention mecha-
nism dynamically aligns the encoder and decoder time steps to temporally align
the input and output sequences. Chorowski et al. [44] transfer the attention-
based recurrent networks to speech recognition and achieve comparable results
on the TIMIT benchmark that compare well with conventional methods. Chan
et al. [45] propose LAS (listen, attend and spell) to directly map acoustic in-
puts to transcription. The results show that the attention mechanism prevents
model overtting on the training set. Meanwhile, Bahdanau et al. [46] show that
attention-based models can implicitly learn better context information than
CTC and conventional models. In addition, local monotonic attention [47],
full-sequence attention [48], time-restricted self-attention [49], multi-channel at-
tention [50] and online attention [51] are proposed to reduce the complexity of
attention computation and learn more robust alignments.
2.3.4. Hybrid CTC/Attention Architectures
To fully incorporate the merits of CTC and attention models, Kim et al. [52]
propose to jointly train CTC and attention-based approaches in a multi-task
8learning fashion by sharing one encoder. The evaluation on WSJ and CHiME-4
noisy speech shows the hybrid architecture can eciently speed up convergence
and learn more robust alignment between input frames and output sequences.
Hori et al. [53] extend the hybrid CTC/attention method with a joint decod-
ing algorithm by rescoring or combining the probabilities from both objective
functions. Then, a monotonic chunk-wise attention [54] and transformer-based
encoder [55] are utilized to enable the hybrid CTC/attention model to work
in online streaming tasks. Zhang et al. [56] propose a new two-pass approach
(U2) which unies the streaming and non-streaming ASR models into one ar-
chitecture. The hybrid CTC/attention architecture is becoming more and more
popular. There has been a trend to unify the streaming and non-streaming ar-
chitecture into one model with this architecture. However, limited research has
focused on the recognition of OOV words in end-to-end architectures. In this
paper, we tackle the problem of OOV words with the U2 ASR model4which
will be described in more detail in 4.1.
3. Methodology
In this section, we demonstrate the proposed loss rescaling approaches at
sentence level and word level. Furthermore, we introduce the techniques of L2
regularization and EWC used to overcome catastrophic forgetting problems.
3.1. Loss Rescaling at Sentence Level
During training, the CTC function returns one loss per utterance, and the
mean of all utterance losses in the same mini-batch would be used for back-
propagation. As shown in Figure 1 (a), each bar in the gure means one ut-
terance loss in a randomly selected mini-batch. We observe that the utterance
losses in one mini-batch are evenly distributed. Sometimes, the loss of utter-
ances containing OOV words can be slightly higher or even lower than other
utterances without OOV words. Consequently, the model pays equal attention
4https://github.com/wenet-e2e/wenet
9(a) (b)Utterances without OOV words 
Utterances with OOV wordsLoss  
Loss  
Utterance UtteranceRescaled LossFigure 1: (a) Utterance loss distribution in one mini-batch. (b) Utterance loss distribution
after loss rescaling.
to each utterance or word, which leads to the nal model performance heav-
ily relying on the frequency of words in training sets. Sometimes, the model
attention is even biased by non-OOV words.
To emphasize an utterance containing OOV words, we rescale the utterance
loss, as indicated in Figure 1 (b), by multiplying it with a hyper-parameter 
in Eq. (1), where xare acoustic inputs, yare the target text references, and O
is the set of OOV words.
Lsentence (x;y) =8
><
>:LCTC(x;y);ifonot iny;8o2O;
LCTC(x;y);ifoiny;8o2O(1)
3.2. Loss Rescaling at Word Level
Given an input acoustic vector x= (x0;;xT), and a target label sequence
y= (y0;;yU), whereT >> U , andTandUare the length of the acoustic
vector and target label sequence respectively, the CTC loss aims to maximize
the log probability in Eq. (2), where eyis the extended label sequence of yby
10'News about Brexit'
news
about
bre_
xi_
ttFigure 2: Illustration of CTC decoding lattices for the example sentence of 'News about
Brexit', where the modeling unit is subword and \Brexit" is an OOV word. Black nodes are
label tokens and white nodes are blank tokens.
inserting blank labels at the beginning and the end of yand between every
two label tokens, eY=Y[. When training CTC ASR systems, the last layer of
the neural network outputs one N1 vector for each acoustic frame xtat time
stept, whereNis the number of modeling units. For example, when building
on character level, Nis 28 (a:::z + space + blank).
LCTC = P(eyjx) = X
a2F 1(y)P(ajx) (2)
After processing the last input vector xT, we can get a (2 U+ 1)Tlattice
matrix, for instance the one in Figure 2 for the utterance \News about Brexit"
modeling on subword units is shown. Figure 3 lists some intermediate decoding
results of the CTC function. The nal CTC loss contains the sum of probabilities
of all possible paths that can be converted to be the target labels yby merging
repeat units and removing blank tokens as shown in Eq. (2), where ais a possible
token path andF:ey!yis the function that maps the extended label sequence
eyback to the true label sequence y.
We denote by(t;u) andb(t;u) as the probability of a label and a blank token
at node (t;u) respectively. According to the denition of CTC [57], as shown in
11news
about
bre_
xi_
ttnews
about
bre_
xi_
ttaboutnews
about
bre_
xi_
ttbre_ ...news
about
bre_
xi_
ttnews
news about bre_ xi_ tFigure 3: Possible intermediate decoding sequences of the CTC function.
Forward Backward
Figure 4: Diagram of three cases in forward or backward computation, where black nodes
are label tokens and white nodes are blank tokens. Red and blue arrows are forward and
backward paths respectively.
Figure 4, for any blank tokens, there are only two paths from the previous step
reaching the current blank token since label tokens can not be skipped. When
the current node is a label token and is same as u 2, there is no connection
fromu 2 tou. A blank token has to be generated to separate the adjacent
same tokens. For instance, there is no path from the rst `o' to the second one
when decoding the word `look'. Lastly, three possible paths can reach the label
token when u6=u 2. Three same cases for the backward procedure are shown
12in Figure 4 as well.
According to the three cases shown in Figure 4, the forward variable (t;u)
can be calculated recursively as follows:
(t;u) =8
>>>>>>>>>>>>>>>><
>>>>>>>>>>>>>>>>:by(t 1;u 1)(t 1;u 1) if (t;u) =blank
+b(t 1;u)(t 1;u)
b(t 1;u 1)(t 1;u 1) if (t;u)6=blank
+by(t 1;u)(t 1;u) and (t,u) = (t  1;u 2)
by(t 1;u 2)(t 1;u 2) if (t;u)6=blank
+b(t 1;u 1)(t 1;u 1) and (t;u)6= (t 1;u 2)
+by(t 1;u)(t 1;u)(3)
It is worth noting that when the two adjacent tokens are same, i.e. ( t;u) =
(t 1;u 2), there is no direct transition between the two repeated tokens and
the second one can only be reached through the blank token or the node of
(t 1;u).
Similarly, blue arrows reaching the node ( t;u) and the backward variable
(t;u) can be represented by Eq. (4) in three cases.
(t;u) =8
>>>>>>>>>>>>>>>><
>>>>>>>>>>>>>>>>:b(t;u)(t+ 1;u) if ( t;u) =blank
+by(t;u)(t+ 1;u+ 1)
by(t;u)(t+ 1;u) if ( t;u)6=blank
+b(t;u)(t+ 1;u+ 1) and ( t;u) = (t+ 1;u+ 2)
by(t;u)(t+ 1;u) if ( t;u)6=blank
+b(t;u)(t+ 1;u+ 1) and ( t;u)6= (t+ 1;u+ 2)
+by(t;u)(t+ 1;u+ 2)(4)
P(At;ujx), the probability of any candidate paths passing through node ( u;t)
conditioned on the input sequence x, can be obtained by multiplying forward
(Eq. (3)) and backward probabilities (Eq. (4)) by Eq. (5).
P(At;ujx) =(t;u)(t;u) (5)
Thereby, the gradient of the CTC loss function LCTCw:r:tby(t;u) andb(t;u)
13can be estimated by Eq. (6) and Eq. (7) respectively.
@LCTC
@by(t;u)/8
>>>>>>>>>><
>>>>>>>>>>:(t;u)(t+ 1;u+ 1) if (t;u) =blank
(t;u)(t+ 1;u) if (t;u)6=blank
and (t;u) = (t+ 1;u+ 2)
(t;u)((t+ 1;u) if (t;u)6=blank
+(t+ 1;u+ 2)) and ( t;u)6= (t+ 1;u+ 2)(6)
@LCTC
@b(t;u)/8
><
>:(t;u)(t+ 1;u) if (t;u) =blank
(t;u)(t+ 1;u+ 1) if (t;u)6=blank(7)
The CTC function treats all nodes equally and aims to minimize the global
loss, which makes models hardly focus on local connections in the decoding lat-
tice. To guide models to pay more attention to the OOV words, we emphasize
the OOV words (the nodes in the dotted box in Figure 2) by rescaling the prob-
abilities of OOV nodes in candidate alignments. Thus, the rescaled probability
of all alignments passing through OOV nodes is as follows:
eP(At;ujx) =8
><
>:P(At;ujx);ifu2O
P(At;ujx);otherwise(8)
The regularized loss function at word level is:
Lword = X
AF 1(ey)eP(At;ujx) (9)
We implement our approach by multiplying the gradients of OOV nodes on
the candidate path with , as shown in the following equations:
@Lword
@by(t;u)=8
><
>:@LCTC
@by(t;u);ifu2O
@LCTC
@by(t;u);otherwise(10)
@Lword
@b(t;u)=8
><
>:@LCTC
@b(t;u);ifu2O
@LCTC
@b(t;u);otherwise(11)
whereOis the set of OOV words tokenized into subwords.
143.3. Overcoming Catastrophic Forgetting
Directly ne-tuning models on a dataset obeying a dierent distribution from
the original training set may lead to catastrophic forgetting. The updated model
may overt the new dataset but forget the knowledge learned on the original
one. To overcome models suering from catastrophic forgetting, we adopt two
approaches during ne-tuning. The rst one is mixing partial original audio
from LibriSpeech used for baseline model training with synthetic speech, since
adding data that obeys the same distribution as the training set can eciently
mitigate the forgetting problem. We explore the eect of dierent mixing ratios
and present the results in Section 5. The other approach is constraining model
parameters from updating during ne-tuning with L2 regularization or EWC [8],
and we will introduce the details in the following sections.
3.3.1. L2 Regularization
The L2 regularization loss LL2() is shown in Eq. (12), where L() is the
original CTC loss or rescaled loss in Eq. (1) and Eq. (9). iis theith parameter
of the ASR model to be updated during ne-tuning, and 0
iis theith parameter
in the baseline model which is invariable and saved locally. is the coecient
to balance the scale of two parts.
LL2() =LCTC() +
2X
i(i 0
i)2(12)
L2 loss takes the dierence between the ne-tuned model and the old model
into account to ensure the updated model will not stray away too much from
the baseline.
3.3.2. Elastic Weight Consolidation
Dierent from L2 loss that always refers to a xed standard and treats all
parameters equally, the EWC loss as shown in Eq. (13) uses the diagonal of
the Fisher information matrix Fto dynamically weigh the importance of each
15model parameter for the source task.
LEWC () =LCTC() +
2X
iFi(i 0
i)2(13)
The Fisher information matrix Fcan be estimated by the following equation
with the gradients of the convergent source model.
Fi=1
jDjX
d2D@2LCTC(d;0
i)
@0
i21
jDjX
d2D@LCTC(d;0
i)2
@0
i2(14)
where0andDare the parameters and the dataset used in the source ASR task
respectively. The more important parameters for the source task would have
the larger values in the Fisher information matrix, which constrains the change
of important parameters and avoids knowledge forgetting. The less important
ones would have relatively small values and are encouraged to adapt on the new
task. In this paper, the diagonal of the Fisher information matrix is estimated
on the LibriSpeech 960h training set.
4. Experiments
4.1. ASR Model Architecture
The end-to-end ASR model used in our experiments is the two-pass hybrid
CTC/attention architecture, U2 [56], as shown in Figure 5. The shared encoder
converts acoustic features xinto a latent vector henc, then the CTC decoder
transforms the latent vector into character/word probability P(ytjxt) with the
same length as the input frames. Meanwhile, the attention decoder generates
one character/word probability P(yujyu 1;;y0;x) per time step by condi-
tioning on the attention content vector cuand the decoder output from the last
stepyu 1. During training, the sum of CTC loss and attention loss is used to
do back-propagation, while during inference, the n-best hypotheses produced
by the CTC decoder are rescored by the attention decoder to obtain better
performance. The candidate with the highest score will be the nal output.
16CTC Decoder
EncoderAttention Decoder
AttentionRescoringFigure 5: Two-pass hybrid CTC/attention ASR architecture.
We develop our model based on the U2 model published with the WeNet
toolkit [58] which unies streaming and non-streaming ASR models into one
architecture by proposing the dynamic chunk-based attention. The encoder
consists of 12 conformer blocks, with 4 multi-head attention, 2048 linear units,
swish activation, a positional dropout rate of 0.1 and Conv2D kernel size of 31 for
each block. The attention decoder contains 6 transformer blocks, and the CTC
decoder is composed of 1 linear layer and 1 log softmax function. The U2 model
is pre-trained on the 960h LibriSpeech corpus and achieves 3.18% of WER and
8.72% of WER on the test-clean and the test-other test set respectively when
rescoring with attention decoder. The pre-trained weights are available on the
website5, which will be the baseline model for all our experiments.
4.2. OOV Set with Real Audio
We build a 100-OOV-word dataset from LRS3-TED [59] corpus since there
are no standard OOV corpora published by the community. LRS3-TED is an
audio-visual dataset collected from TED and TEDx talks with spontaneous
5http://mobvoi-speech-public.ule.ucloud.cn/public/wenet/librispeech/20210216
conformer exp.tar.gz
17speech and various speaking styles. It is comprised of over 400 hours of video
by more than 5000 speakers and contains an extensive vocabulary. We lter
the vocabulary existing in LRS3-TED but not present in LibriSpeech and select
100 OOV words from more than 100 speakers used for test, where each OOV
word contains 50 utterances. We random split these utterances into training,
validation and test sets with a ratio of 2:1:2. The duration ratio of training,
validation and test sets is 3h : 1.6h : 2.8h. In the rest of this paper, the three
sets will be referred to as real OOV (ROOV) training, ROOV val and ROOV
test set respectively. We report all experimental results on the ROOV test set.
More details about the 100 OOV words can be found in the Appendix.
4.3. OOV Set with Synthetic Audio
Our goal in this paper is to improve OOV word recognition with synthetic
audio and loss rescaling methods. In this subsection, we introduce the synthetic
dataset used for model training.
Text Crawling : we crawl 100 sentences for each new word with Scrapy6,
where each sentence contains less than 50 words in case of running out of mem-
ory. During crawling, we lter those sentences include OOV words out of the
selected 100-OOV-word set to ensure that the model performance is only inu-
enced by the chosen ones.
Speech Synthesis : we split the 100 sentences for each new word into train-
ing and validation with a ratio of 9:1. The model evaluation will be conducted on
the ROOV test set with real audio. Instead of applying a single multi-speaker
TTS system, we use several commercial Application Programming Interfaces
(APIs), i.e. Baidu TTS API7, Google TTS API8, iFLYTEK TTS API9, Ten-
cent TTS API10and Alibaba TTS API11to synthesize audio and to enable more
6https://scrapy.org/
7https://ai.baidu.com/tech/speech/tts
8https://cloud.google.com/text-to-speech
9https://www.xfyun.cn/doc/tts/online tts/API.html
10https://cloud.tencent.com/document/api
11https://www.alibabacloud.com/help/doc-detail/84435.htm
18voices and more variety in speech. In contrast to open-source multi-speaker TTS
models, the commercial APIs produce higher quality speech, which is crucial for
the following experiments. 8 dierent speaker voices (4 males and 4 females)
are used for training set synthesis and 2 voices (1 male and 2 female) for the
validation set. There is no voice overlap between the 2 parts. The duration ratio
of synthetic audio used for training and validation is 9.0h : 2.3h. In the rest
of this paper, the synthetic data will be referred to as synthetic OOV (SOOV)
training and SOOV val set respectively.
Data Augmentation : to avoid overtting, we perturb speech on speed
with the factors of 0.9, 1.0, and 1.1. Furthermore, clean speech is augmented
with 5 kinds of room impulse responses12. 10 noise sources [60], such as an-
nouncements, appliances, and trac, are added to the reverberated speech with
6 levels of speech-to-noise ratio (0, 4, 8, 12, 16 and 20). Moreover, SpecAug-
ment [25] with 2 frequency masks (maximum width 50) is utilized on the y
during training.
4.4. Evaluation Metrics
We use 3 metrics to evaluate the experimental results of our proposed method:
•WER : word error rate is the ratio of error terms, i.e., substitutions, dele-
tions, and insertions, to the total number of words in the reference.
•Recall : recall is the number of true positives TPover the sum of the
number of true positives and the number of false negatives FN.
Recall =TP
TP+FN(15)
•Precision : precision is the number of true positives TPover the sum of
the number of true positives and the number of false positives FP.
Precision =TP
TP+FP(16)
12http://www.iks.rwth-aachen.de/en/research/tools-downloads/databases/aachen-
impulse-response-database/
194.5. Training Settings
The baseline model is trained on the 960h LibriSpeech dataset with a batch
size of 12, an initial learning rate of 4e-3 and 25000 warm-up steps. When doing
ne-tuning for OOV experiments, we use a bigger batch size of 20 to enable
the model to see more utterances not containing OOV words and avoid loss
explosion. A tiny initial learning rate of 4e-6 is utilized for ne-tuning, which
is annealed with a value of 1.1 after every 3000 steps, since a tiny learning rate
can eciently ensure stable model learning and retain the previously learned
knowledge. In addition, to avoid gradient explosion, we clip all gradients greater
than 2, while the threshold used in baseline training is 5.
The validation set during model training is the mixture of the LibriSpeech
dev and OOV TTS dev set with the ratio of 1:1. The model checkpoint perform-
ing best on the mixture validation set is used for evaluation on test sets with
early stopping. It is noteworthy that the attention mechanism and attention
decoder are always frozen since we found that ne-tuning the entire network
leads to gradient explosion. In addition, it is hard to balance the CTC loss and
attention loss since the CTC loss is rescaled in our methods. We only use the
the attention and the decoder for rescoring.
5. Experimental Results
In this section, we report the experimental results from the following per-
spectives.
5.1. Results of Speech Mixture from Source and Target Domain
To mitigate catastrophic forgetting, we mix original real speech in the Lib-
riSpeech training set (source domain) with the ROOV or the SOOV training
set (target domain). It is still an open question what the best mixing ratio is.
We ne-tune the baseline model with dierent ratios and report results on the
standard LibriSpeech test sets (test-clean and test-other) and the ROOV test
set.
20As shown in Table 1, when ne-tuning only with ROOV training set (real
speech from LRS3-TED), the model shows the inability to retain old knowledge
and performs badly on previous LibriSpeech tasks, which leads to a tremendous
rise in WER on the test-other set from 8.72% to 41.52%. Context information
is crucial for ASR models. In the 0:1 setting, the pre-trained ASR model is de-
stroyed, especially the learned context knowledge. Consequently, the overtted
model is hard to infer a correct context and recognize OOV words.
Table 1: The inuence of the ratio of speech data from LibriSpeech and real speech in ROOV
training set (LRS3-TED) on ASR and OOV word recognition.
Model RatioWER WER WER Recall Precision
test-clean test-other ROOV test ROOV test ROOV test
Baseline - 3.18 8.72 15.33 1.37 100
LibriSpeech
+
ROOV training0:1 30.18 41.52 38.21 19.58 97.28
1:1 23.34 30.41 25.83 24.30 93.18
2:1 13.26 26.77 23.27 32.05 98.41
3:1 7.26 15.25 22.31 30.82 98.02
4:1 4.11 11.05 15.52 26.12 99.04
The forgetting tendency slows down as original data is incorporated into
training. When the ratio of audio from LibriSpeech and LRS3-TED is 2:1, the
model achieves the highest recall of 32.05%. The more data from the source task
(LibriSpeech) is used for training, the more previous knowledge is retained and
the better performance is obtained on the LibriSpeech evaluation sets. However,
the model tends to focus on the previous LibriSpeech tasks as the ratio increases,
which leads to the decrease of recall on the ROOV test.
We can draw the same conclusion when ne-tuning ASR models with syn-
thetic data as shown in Table 2. We prioritize the model performance regarding
recall since the goal of this paper is to enable the ASR model to learn new
vocabulary, and the catastrophic forgetting issue will be tackled in the next
section. Therefore, the 2:1 mixture ratio is used in the following experiments.
21Table 2: The inuence of the ratio of speech data from LibriSpeech and synthetic speech in
SOOV training set on ASR and OOV word recognition.
Model RatioWER WER WER Recall Precision
test-clean test-other ROOV test ROOV test ROOV test
Baseline - 3.18 8.72 15.33 1.37 100
LibriSpeech
+
SOOV training0:1 35.54 53.42 42.27 13.51 99.83
1:1 29.05 39.08 30.22 20.51 91.21
2:1 20.34 28.72 25.31 27.54 97.67
3:1 13.37 19.21 23.22 26.25 98.53
4:1 6.71 16.23 20.31 23.21 98.08
5.2. Results of Loss Rescaling at Sentence Level
In this section, we explore the eect of loss rescaling at sentence level. We
compare the model performance using real speech data with the results using
synthetic audio. As shown in Table 3, using L2 and EWC regularization ef-
ciently reduces catastrophic forgetting and improves the recall rate on OOV
words while the WER increases only in few cases on the test-clean and test-
other test sets. We nd = 5e7 is the best weight to balance the L2/EWC loss
and the ASR losses.
We reproduce the method proposed by Zheng et al. [61], in which a RNN-T
ASR model is ne-tuned with EWC on mixed real and synthetic audio. However,
the dataset is not published. It is noteworthy that the experimental results re-
ported in Table 3 and Table 4 are based on our generated data with the method
proposed by Zheng et al. [61]. In addition, as shown in Table 3, row \Iso-
lated Words", we de-emphasize the non-OOV words by ne-tuning ASR models
with utterances containing only isolated OOV words which are segmented from
real or synthetic continuous speech according to the time alignment informa-
tion obtained from the Montreal Forced Aligner13. Compared with the method
proposed by Zheng et al. [61], ne-tuning ASR models with only isolated OOV
13https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner
22words can eectively improve the recall rate but it leads to much more serious
forgetting on non-OOV recognition.
Table 3: Loss rescaling at sentence level with L2/EWC regularization. andare the loss
weight in Eq. (1) and the weight of L2/EWC in Eq. (12)/Eq. (13) respectively. R is short for
\Real" and represents using ROOV training set with real audio for ne-tuning. S is short for
\Synthetic" and denotes using SOOV training set with synthetic audio for ne-tuning.
  WER# WER# WER# Recall" Precision"
Test sets - - test-clean test-other ROOV test ROOV test ROOV test
Model - - R S R S R S R S R S
Baseline 1 0 3.18 8.72 15.33 1.37 100
L21 5e7 5.21 5.32 10.22 10.91 15.03 18.84 24.13 15.24 98.12 99.02
10 5e7 5.53 5.36 10.93 12.02 15.17 18.69 35.12 24.42 98.51 92.03
100 5e7 6.07 6.38 11.38 11.94 15.01 19.28 50.02 31.26 94.35 95.71
1000 5e7 6.83 6.93 11.83 12.48 15.74 19.74 53.42 39.72 90.04 89.43
10000 5e7 7.22 7.79 13.48 13.59 16.33 19.26 51.78 39.77 83.44 78.25
Zheng et al. [61] 1 5e7 3.18 3.20 8.93 9.02 14.87 18.23 30.57 16.20 98.43 99.12
Isolated Words 1 5e7 5.83 6.32 11.01 12.27 14.65 18.47 35.92 22.48 97.65 98.15
EWC10 5e7 5.21 5.20 9.91 9.83 14.59 18.34 49.22 30.71 98.11 99.04
100 5e7 5.29 5.37 9.82 9.98 14.66 18.04 54.28 41.22 97.21 97.55
1000 5e7 5.55 5.54 10.54 11.37 14.67 18.37 54.06 42.38 88.24 89.41
10000 5e7 6.07 6.12 11.78 11.84 14.52 18.02 55.31 42.58 80.51 84.28
When just ne-tuning the base model with real or synthetic OOV audio,
all words are treated equally, which leads the model to hardly focus on the
OOV words we concern. Therefore, we propose loss rescaling and encourage the
model to pay more attention to OOV words by enlarging the loss of sentences
containing unknown words. For the loss rescaling weight , we examine the
values 1, 10, 100, 1000, and 10,000. As we can see in Table 3, the OOV recall
rapidly increases when rescaling the target sentences by 100 times compared to
only ne-tuning using L2 (50.02% VS 24.13% for real speech and 31.26% VS
24.13% for synthetic audio).
As a bigger is used, the recall further rises, but the WER on the test-
clean and the test-other test sets is getting worse. We hypothesize that directly
rescaling the entire sentence loss may also enhance irrelevant words or noises,
23which leads to gradient explosion during training and accelerates forgetting
previous knowledge. Hence, we have to use a very small learning rate and clip
the gradients over 2.0 to ensure the progress of ne-tuning. In contrast to L2
regularization, EWC can provide more stable and resilient protection of the
weights important for the previous LibriSpeech tasks but still with a relatively
high loss in the ASR performance, as shown in Table 3.
In addition, when ne-tuning only with the synthetic audio, we obtain
competitive recall rate compared to utilizing real speech from the LRS3-TED
dataset, for example, when using EWC and rescaling the loss by 1000 times
larger, we achieve 54.06% VS 42.38% recall rate for real and synthetic speech.
Furthermore, compared with the method proposed by Zheng et al. [61], rescal-
ing the OOV utterance loss can achieve signicant improvement on recall with
only slight decrease on WER and precision.
5.3. Results of Loss Rescaling at Word Level
Instead of enhancing the entire sentence loss, in this section, we report the
results of only rescaling unknown words. As shown in Table 4, the weight,
which is needed to balance the ASR and L2/EWC loss, is smaller (1e7) than
the one used at sentence level (5e7) in Table 3, and we do not observe gradient
explosion during training unless is very large, e.g. 1e4. The results without
loss rescaling ( = 1) is slightly dierent from the one shown in Table 3, which
is caused by dierent values used for L2/EWC normalization. Using 10 times
smallercan obtain a similar or even higher recall at word-level rescaling, for
example, using of 100 gets a 45.81% recall rate at word level compared to
usingof 1000 getting a 42.38% recall rate at sentence level when regularizing
models with EWC and ne-tuning with only synthetic data.
Rescaling loss on OOV words obtains lower WER on the standard Lib-
riSpeech test sets. Moreover, the bigger is used, the higher recall is achieved
for the ROOV test. Furthermore, a worse WER is obtained for the LibriSpeech
benchmark, which is observed at sentence-level loss rescaling as well. To make a
trade-o between WER and Recall, 100 times loss rescaling is performing best
24for our experimental results with only a 7.8%/4.6% relative WER increase on
the test-clean/test-other test set or a 45.81% recall rate on the ROOV test.
In addition, we obtain competitive performance by only using synthetic audio
compared with using real speech data for ne-tuning.
Table 4: Loss rescaling at word level with L2/EWC regularization. andare the loss
weight in Eq. (1) and the weight of L2/EWC in Eq. (12)/Eq. (13) respectively. R is short for
\Real" and represents using ROOV training set with real audio for ne-tuning. S is short for
\Synthetic" and denotes using SOOV training set with synthetic audio for ne-tuning.
  WER# WER# WER# Recall" Precision"
Test sets - - test-clean test-other ROOV test ROOV test ROOV test
Model - - R S R S R S R S R S
Baseline 1 0 3.18 8.72 15.33 1.37 100
L21 1e7 4.03 4.04 9.05 9.23 14.95 15.21 24.01 13.77 98.12 99.02
10 1e7 4.16 4.23 9.31 9.01 15.01 15.03 33.45 24.63 98.51 92.03
100 1e7 4.53 5.03 9.48 14.83 14.95 18.28 48.41 32.49 94.35 95.71
1000 1e7 4.71 4.62 10.73 14.45 15.19 19.74 55.28 43.24 90.04 89.43
10000 1e7 5.33 5.59 12.74 14.15 16.26 19.26 55.83 44.04 83.44 78.25
Zheng et al. [61] 1 1e7 3.19 3.18 8.83 8.79 14.09 15.44 30.83 18.12 97.25 98.44
Isolated Words 1 1e7 4.22 5.47 10.15 11.19 14.31 16.24 38.07 27.62 95.14 94.33
EWC10 1e7 3.22 3.31 8.94 8.83 13.96 15.22 49.10 31.43 94.23 94.57
100 1e7 3.30 3.43 8.97 9.12 13.47 15.17 59.74 45.81 90.45 89.14
1000 1e7 3.55 3.62 9.99 10.45 14.24 15.54 59.76 46.03 80.61 81.09
10000 1e7 4.01 5.37 10.28 11.23 14.15 15.87 62.19 46.71 77.17 69.42
5.4. Discussion
New vocabulary emerges all the time due to the evolution of human language.
Therefore, it is important to enable a trained ASR system to dynamically ac-
quire unseen vocabulary. The combined loss rescaling and weight consolidation
methods proposed in this paper can support continual learning [62] of an ASR
system. The methods neither require any labeled data nor do they require
retraining a new ASR model from scratch.
An interesting nding is that enhancing the gradient of blank tokens within
and after OOV words is important as well, which encourages the decoding pro-
25cedure moving forward, for instance, the rows of u6,u8andu10in Figure 2.
Otherwise, the decoding progress is cut o and models repeatedly produce one
token, such as \news about bre brebre" when only enlarging the gradient of
\bre " in the utterance of \new about bre xit". Sometimes, the ne-tuned
ASR system even repeats one token, for example \bre bre bre bre bre",
whenis very large.
Additionally, we nd that the performance and the speed of convergence are
aected by the batch size, especially for loss rescaling at sentence level. When
the batch size is small, e.g. 5, all utterances in one batch may contain OOV
words, which leads to a bigger rescaled loss. Consequently, the model suers
from gradient explosion, and L2 or EWC regularization can hardly constrain
the model weights diverging.
6. Conclusion
In this paper, we present the use of synthetic speech to boost an ASR model
on the recognition of OOV words. In addition to ne-tuning with audio con-
taining OOV words, we propose to rescale loss at sentence level or word level,
which encourages models to pay more attention to unknown words. Experi-
mental results reveal that ne-tuning the baseline ASR model combined with
loss rescaling and L2/EWC regularization can signicantly improve the OOV
word recall rate and eciently overcome models suering from catastrophic
forgetting. Furthermore, loss rescaling at word level is more stable than at sen-
tence level and results in less ASR performance loss on general non-OOV words
and previous LibriSpeech tasks. The combination of proposed loss rescaling,
which updates the new task-related parameters (OOV word recognition), and
EWC, which retains the old task-learned weights (speech recognition on the
LibriSpeech dataset), can enable continual learning of an ASR system.
The proposed target word loss rescaling method is simple and eective, but
there are still some issues left to be improved. Currently, results are evaluated on
synthetic audio data which is dierent from the spontaneous speech recorded
26in the real world. Future work could focus on real-scenario speech collecting
and model evaluation, which enables us to well understand and compare the
contribution of using synthetic and real speech containing OOV words. Addi-
tionally, the current OOV word set needs to be known, how to automatically
detect and optimize OOV words is a potential direction. The trade-o between
WER on universal test sets (e.g. LibriSpeech test-clean and test-other sets) and
recall rate on the OOV set is another issue. A dynamic L2/EWC weight [63]
can be adopted to replace the xed weight. Later in the ne-tuning, a xed
regularization weight could inuence model updating. Moreover, we are in-
terested in investigating the eectiveness of our proposed method on RNN-T
and attention-based encoder-decoder ASR systems. It is also worthwhile to
explore our loss rescaling method on some general unbalanced label problems,
for example, speaker diarization and voice verication. Since continual learning
in sequence processing is a young research eld [64, 65, 66], our loss rescaling
method may have wider implication for data where novel elements are learnt in
temporal or spatial context with known elements.
Acknowledgments
The authors gratefully acknowledge the support from the China Scholarship
Council (CSC), from the Young Scientists Foundation of Zhejiang Lab, from
the German Research Foundation DFG (projects TRR 169 and LeCAREbot)
and from the BMWK (projects KI-SIGS and SiDiMo).
References
[1] A. Graves, S. Fern andez, F. Gomez, J. Schmidhuber, Connectionist tem-
poral classication: labelling unsegmented sequence data with recurrent
neural networks, in: Proceedings of the 23rd International Conference on
Machine Learning, 2006, pp. 369{376.
[2] A. Graves, Sequence transduction with recurrent neural networks, in:
27Proceedings of the 23rd International Conference on Machine Learning
(ICML), 2012.
[3] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly
learning to align and translate, in: International Conference on Learning
Representations (ICLR), 2015.
[4] P. F. Brown, V. J. Della Pietra, P. V. Desouza, J. C. Lai, R. L. Mercer,
Class-based n-gram models of natural language, Computational Linguistics
18 (4) (1992) 467{480.
[5] S. R. Maskey, M. Bacchiani, B. Roark, R. Sproat, Improved name recog-
nition with meta-data dependent name networks, in: IEEE International
Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vol. 1,
IEEE, 2004, pp. I{789.
[6] J. Guo, T. N. Sainath, R. J. Weiss, A spelling correction model for end-to-
end speech recognition, in: IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), IEEE, 2019, pp. 5651{5655.
[7] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia, B. Li, R. Pang,
Shallow-fusion end-to-end contextual biasing., in: Proc. INTERSPEECH,
2019, pp. 1418{1422.
[8] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A.
Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al., Over-
coming catastrophic forgetting in neural networks, in: Proceedings of the
National Academy of Sciences, Vol. 114, National Acad Sciences, 2017, pp.
3521{3526.
[9] I. Sheikh, D. Fohr, I. Illina, G. Linares, Modelling semantic context of
OOV words in large vocabulary continuous speech recognition, IEEE/ACM
Transactions on Audio, Speech, and Language Processing 25 (3) (2017)
598{610.
28[10] Y. Miao, F. Metze, S. Rawat, Deep maxout networks for low-resource
speech recognition, in: IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU), IEEE, 2013, pp. 398{403.
[11] K. M. Knill, M. J. Gales, S. P. Rath, P. C. Woodland, C. Zhang, S.-
X. Zhang, Investigation of multilingual deep neural networks for spoken
term detection, in: IEEE Workshop on Automatic Speech Recognition and
Understanding (ASRU), IEEE, 2013, pp. 138{143.
[12] K. Hwang, W. Sung, Character-level incremental speech recognition with
recurrent neural networks, in: IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), IEEE, 2016, pp. 5335{5339.
[13] P. Aleksic, C. Allauzen, D. Elson, A. Kracun, D. M. Casado, P. J. Moreno,
Improved recognition of contact names in voice commands, in: IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing (ICASSP),
IEEE, 2015, pp. 5172{5175.
[14] T. Hori, S. Watanabe, J. R. Hershey, Multi-level language modeling and
decoding for open vocabulary end-to-end speech recognition, in: 2017 IEEE
Automatic Speech Recognition and Understanding Workshop (ASRU),
IEEE, 2017, pp. 287{293.
[15] J. Li, G. Ye, R. Zhao, J. Droppo, Y. Gong, Acoustic-to-word model without
OOV, in: 2017 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU), IEEE, 2017, pp. 111{117.
[16] I. Williams, A. Kannan, P. S. Aleksic, D. Rybach, T. N. Sainath, Contex-
tual speech recognition in end-to-end neural network systems using beam
search., in: Proc. INTERSPEECH, 2018, pp. 2227{2231.
[17] K. C. Sim, F. Beaufays, A. Benard, D. Guliani, A. Kabel, N. Khare,
T. Lucassen, P. Zadrazil, H. Zhang, L. Johnson, et al., Personalization
of end-to-end speech recognition on mobile devices for named entities, in:
292019 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU), IEEE, 2019, pp. 23{30.
[18] T. Likhomanenko, G. Synnaeve, R. Collobert, Who needs words? Lexicon-
free speech recognition, in: Proc. INTERSPEECH, 2019, pp. 3915{3919.
[19] E. Variani, D. Rybach, C. Allauzen, M. Riley, Hybrid autoregressive trans-
ducer (HAT), in: IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), IEEE, 2020, pp. 6139{6143.
[20] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen,
R. Zhao, J. Li, Y. Gong, Internal language model estimation for domain-
adaptive end-to-end speech recognition, in: 2021 IEEE Spoken Language
Technology Workshop (SLT), IEEE, 2021, pp. 243{250.
[21] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, R. Prabhavalkar,
An analysis of incorporating an external language model into a sequence-to-
sequence model, in: IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), IEEE, 2018, pp. 1{5828.
[22] T. Ko, V. Peddinti, D. Povey, S. Khudanpur, Audio augmentation for
speech recognition, in: Sixteenth Annual Conference of the International
Speech Communication Association, 2015.
[23] T. Hayashi, S. Watanabe, Y. Zhang, T. Toda, T. Hori, R. Astudillo,
K. Takeda, Back-translation-style data augmentation for end-to-end ASR,
in: IEEE Spoken Language Technology Workshop (SLT), IEEE, 2018, pp.
426{433.
[24] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, S. Khudanpur, A study on
data augmentation of reverberant speech for robust speech recognition, in:
IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, 2017, pp. 5220{5224.
30[25] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
Q. V. Le, SpecAugment: A simple data augmentation method for auto-
matic speech recognition, in: Proc. INTERSPEECH, 2019, pp. 1613{1617.
[26] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen,
Y. Zhang, Y. Wang, R. Skerrv-Ryan, et al., Natural TTS synthesis by con-
ditioning WaveNet on mel spectrogram predictions, in: IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE,
2018, pp. 4779{4783.
[27] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan, S. Narang,
J. Raiman, J. Miller, Deep Voice 3: 2000-speaker neural text-to-speech,
in: International Conference on Learning Representations (ICLR), 2018,
pp. 214{217.
[28] R. Valle, K. Shih, R. Prenger, B. Catanzaro, FlowTron: an autoregres-
sive ow-based generative network for text-to-speech synthesis, in: Inter-
national Conference on Learning Representations (ICLR), 2020.
[29] N. Rossenbach, A. Zeyer, R. Schl uter, H. Ney, Generating synthetic audio
data for attention-based speech recognition systems, in: IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE,
2020, pp. 7069{7073.
[30] A. Laptev, R. Korostik, A. Svischev, A. Andrusenko, I. Medennikov, S. Ry-
bin, You do not need more data: improving end-to-end speech recognition
by text-to-speech data augmentation, in: 2020 13th International Congress
on Image and Signal Processing, BioMedical Engineering and Informatics
(CISP-BMEI), IEEE, 2020, pp. 439{444.
[31] J.-M. Valin, J. Skoglund, LPCNet: Improving neural speech synthesis
through linear prediction, in: IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), IEEE, 2019, pp. 5891{5895.
31[32] A. Rosenberg, Y. Zhang, B. Ramabhadran, Y. Jia, P. Moreno, Y. Wu,
Z. Wu, Speech recognition with augmented synthesized speech, in:
2019 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU), IEEE, 2019, pp. 996{1002.
[33] A. Y. Hannun, A. L. Maas, D. Jurafsky, A. Y. Ng, First-pass large vocab-
ulary continuous speech recognition using bi-directional recurrent DNNs,
in: arXiv preprint arXiv:1408.2873, 2014.
[34] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,
C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen, et al., Deep Speech 2:
End-to-end speech recognition in English and Mandarin, in: International
Conference on Machine Learning, PMLR, 2016, pp. 173{182.
[35] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, N. Andrew, Deep
learning with COTS HPC systems, in: International Conference on Ma-
chine Learning, PMLR, 2013, pp. 1337{1345.
[36] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z.
Mao, M. Ranzato, A. Senior, P. Tucker, et al., Large scale distributed deep
networks, in: Advances in Neural Information Processing Systems, 2012,
pp. 1232{1240.
[37] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen,
H. Nguyen, R. T. Gadde, Jasper: An End-to-End Convolutional Neural
Acoustic Model, in: Proc. INTERSPEECH, 2019, pp. 71{75.
[38] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev, V. Lavrukhin,
R. Leary, J. Li, Y. Zhang, Quartznet: Deep automatic speech recognition
with 1D time-channel separable convolutions, in: IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020,
pp. 6124{6128.
[39] K. Rao, H. Sak, R. Prabhavalkar, Exploring architectures, data and
units for streaming end-to-end speech recognition with RNN-Transducer,
32in: IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU), IEEE, 2017, pp. 193{199.
[40] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, S. Ku-
mar, Transformer transducer: A streamable speech recognition model with
transformer encoders and RNN-T loss, in: IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 7829{
7833.
[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
 L. Kaiser, I. Polosukhin, Attention is all you need, in: Advances in Neural
Information Processing Systems, 2017, pp. 5998{6008.
[42] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang,
Y. Wu, ContextNet: Improving convolutional neural networks for auto-
matic speech recognition with global context, in: Proc. INTERSPEECH,
2020, pp. 3610{3614.
[43] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y. Wu, et al., Conformer: Convolution-augmented
transformer for speech recognition, in: Proc. INTERSPEECH, 2020, pp.
5036{5040.
[44] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, Y. Bengio, Attention-
based models for speech recognition, in: Advances in Neural Information
Processing Systems, Vol. 28, 2015.
[45] W. Chan, N. Jaitly, Q. V. Le, O. Vinyals, Listen, attend and spell, in:
IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2016.
[46] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, Y. Bengio, End-to-
end attention-based large vocabulary speech recognition, in: IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing (ICASSP),
IEEE, 2016, pp. 4945{4949.
33[47] A. Tjandra, S. Sakti, S. Nakamura, Local monotonic attention mechanism
for end-to-end speech and language processing, in: the International Joint
Conference on Natural Language Processing, 2017.
[48] R. Prabhavalkar, T. N. Sainath, B. Li, K. Rao, N. Jaitly, An analysis of
"Attention" in sequence-to-sequence models, in: Proc. INTERSPEECH,
2017, pp. 3702{3706.
[49] D. Povey, H. Hadian, P. Ghahremani, K. Li, S. Khudanpur, A time-
restricted self-attention layer for ASR, in: IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2018, pp.
5874{5878.
[50] S. Braun, D. Neil, J. Anumula, E. Ceolini, S.-C. Liu, Multi-channel atten-
tion for end-to-end speech recognition, in: Proc. INTERSPEECH, 2018,
pp. 17{21.
[51] R. Fan, P. Zhou, W. Chen, J. Jia, G. Liu, An online attention-based model
for speech recognition, in: Proc. INTERSPEECH, 2018, pp. 4390{4394.
[52] S. Kim, T. Hori, S. Watanabe, Joint CTC-attention based end-to-end
speech recognition using multi-task learning, in: IEEE international Con-
ference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2017,
pp. 4835{4839.
[53] T. Hori, S. Watanabe, J. R. Hershey, Joint CTC/attention decoding for
end-to-end speech recognition, in: Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers),
2017, pp. 518{529.
[54] H. Miao, G. Cheng, P. Zhang, T. Li, Y. Yan, Online hybrid CTC/Attention
architecture for end-to-end speech recognition., in: Proc. INTERSPEECH,
2019, pp. 2623{2627.
[55] H. Miao, G. Cheng, C. Gao, P. Zhang, Y. Yan, Transformer-based online
CTC/attention end-to-end speech recognition architecture, in: IEEE Inter-
34national Conference on Acoustics, Speech and Signal Processing (ICASSP),
IEEE, 2020, pp. 6084{6088.
[56] B. Zhang, D. Wu, Z. Yao, X. Wang, F. Yu, C. Yang, L. Guo, Y. Hu, L. Xie,
X. Lei, Unied streaming and non-streaming two-pass end-to-end model for
speech recognition, arXiv preprint arXiv:2012.05481.
[57] A. Graves, Supervised sequence labelling with recurrent neural networks,
in: Studies in Computational Intelligence, 2008.
[58] B. Zhang, D. Wu, C. Yang, X. Chen, Z. Peng, X. Wang, Z. Yao, X. Wang,
F. Yu, L. Xie, et al., WeNet: Production rst and production ready end-
to-end speech recognition toolkit, in: Proc. INTERSPEECH, 2021.
[59] T. Afouras, J. S. Chung, A. Zisserman, Lrs3-ted: a large-scale dataset for
visual speech recognition, arXiv preprint arXiv:1809.00496.
[60] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, J. Gehrke,
A scalable noisy speech dataset and online subjective test framework, in:
Proc. INTERSPEECH, 2019, pp. 1816{1820.
[61] X. Zheng, Y. Liu, D. Gunceler, D. Willett, Using synthetic audio to improve
the recognition of out-of-vocabulary words in end-to-end ASR systems, in:
IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, 2021, pp. 5674{5678.
[62] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, S. Wermter, Continual
lifelong learning with neural networks: A review, Neural Networks 113
(2019) 54{71.
[63] I. Leang, G. Sistu, F. B urger, A. Bursuc, S. Yogamani, Dynamic task
weighting methods for multi-task networks in autonomous driving systems,
in: 2020 IEEE 23rd International Conference on Intelligent Transportation
Systems (ITSC), IEEE, 2020, pp. 1{8.
35[64] A. Cossu, A. Carta, V. Lomonaco, D. Bacciu, Continual learning for recur-
rent neural networks: an empirical evaluation, Neural Networks 143 (2021)
607{627.
[65] B. Ehret, C. Henning, M. R. Cervera, A. Meulemans, J. Von Oswald, B. F.
Grewe, Continual learning in recurrent neural networks, arXiv preprint
arXiv:2006.12109.
[66] K. Ahrens, F. Abawi, S. Wermter, Drill: Dynamic representations for im-
balanced lifelong learning, in: International Conference on Articial Neural
Networks, Springer, 2021, pp. 409{420.
367. Appendix
Table 5: The 100 OOV words selected from LRS3 dataset.
MEANINGFUL QUANTUM RESILIENCE VACCINE MONITORING
RESEARCHER GLOBALLY GLOBALIZATION EMPOWERMENT SPACECRAFT
HORMONE HEALTHCARE PRESCHOOL WORKFORCE ALGORITHM
ISRAELI NOBEL EMPATHY ECOSYSTEM INTERACTING
SCHIZOPHRENIA SOFTWARE INTEGRATE PROGRAMMED YOGA
ALZHEIMER VIETNAM RETHINK PAKISTAN LATVIA
PARKINSON SOCCER RACISM SILICON MARIJUANA
NEUROSCIENCE GENETICALLY MAINSTREAM DEMOGRAPHIC UNEMPLOYMENT
PARADIGM GENOME CREATIVITY INSULIN PROSTHETIC
TRANSGENDER KENYA RACIST STRESSFUL PERSONALIZED
RAPED VIRAL STORYTELLING ENTREPRENEURSHIP ADULTHOOD
MICROSOFT STEREOTYPE EXPERTISE LITERACY RAINFOREST
MINDSET GOOGLE WORKPLACE YOUTUBE SAUDI
FACEBOOK IDEOLOGY INTERACTIVE AUTISM CHEMOTHERAPY
SUSTAINABILITY ACTIVIST ROBOTIC SOCIETAL EBOLA
PRODUCTIVITY TEENAGER DOPAMINE SYNDROME VIABLE
TRANSFORMATIVE CORTEX OXYTOCIN PARENTING JIHAD
RECYCLING DIMENSIONAL TARGETED LAPTOP NIGERIA
SUSTAINABLE PROGRAMMING SMARTPHONE COLLABORATIVE VEGAN
EINSTEIN RESEARCHING VACCINE WIKIPEDIA ANTIMATTER
37