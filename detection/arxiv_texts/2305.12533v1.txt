arXiv:2305.12533v1  [math.NA]  21 May 2023UNIFIED FRAMEWORK FOR FIEDLER-LIKE STRONG
LINEARIZATIONS OF POLYNOMIAL AND RATIONAL MATRICES
RANJAN KUMAR DAS∗ANDHARISH K. PILLAI†
Abstract. Linearization is a widely used method for solving polynomia l eigenvalue problems
(PEPs) and rational eigenvalue problem (REPs) in which the P EP/REP is transformed to a gen-
eralized eigenproblem and then solve this generalized eige nproblem with algorithms available in the
literature. Fiedler-like pencils (Fiedler pencils (FPs), generalized Fiedler pencils (GFPs), Fiedler
pencils with repetition (FPRs) and generalized Fiedler pen cils with repetition (GFPRs)) are well
known classes of strong linearizations of matrix polynomia ls with each of them having some special
properties. GFPs are an intriguing family of linearization s, and GF pencils are the fundamental
building blocks of FPRs and GFPRs. As a result, FPRs and GFPRs have distinctive features and
they provide structure-preserving linearizations for str uctured matrix polynomials. But GFPRs do
not use the full potential of GF pencils. Indeed, not all the G FPs are FPRs or GFPRs, and vice
versa.
The main aim of this paper is two-fold. First, to build a uniﬁe d framework for all the Fiedler-like
pencilsFPs, GFPs, FPRsand GFPRs. To thatend, we construct a newfamilyofstrong linearizations
(named as EGFPs) of a matrix polynomial P(λ) that subsumes all the Fiedler-like linearizations.
A salient feature of the EGFPs family is that it allows the con struction of structured preserving
banded linearizations with low bandwidth forstructured (s ymmetric, Hermitian, palindromic)matrix
polynomial. Low bandwidth structured linearizations may b e useful for numerical computations.
Second, to utilize EGFPs directly to form a family of Rosenbr ock strong linearizations of an n×n
rational matrix G(λ) associated with a realization. We describe the formulas fo r the construction of
low bandwidth linearizations for P(λ) andG(λ). We show that the eigenvectors, minimal bases and
minimal indices of P(λ) andG(λ) can be easily recovered from those of the strong linearizat ions of
P(λ) andG(λ), respectively.
Key words. Matrix polynomial, Rational matrices, System matrix, Fied ler pencil, Lineariza-
tion, Eigenvalue, Eigenvector, Minimal basis, Minimal ind ices.
AMS subject classiﬁcations. 65F15, 15A57, 15A18, 65F35
1. Introduction. Matrix polynomials and rational matrices arise in many ap-
plications, see [1–3,6,8,27,34,39,42,44,47–51,54,55] and th e references therein. Lin-
earization is a widely used method for computing the spectral data ( poles, zeros,
eigenvalues, eigenvectors, minimal bases and minimal indices) of mat rix polynomials
and rational matrices.
LetP(λ) be ann×nmatrix polynomial of degree m≥2 given by
P(λ) =m/summationdisplay
i=0λiAi, A0,...,A m∈Cn×n, Am/\e}atio\slash= 0. (1.1)
Then anmn×mnmatrix pencil L(λ) :=X+λYis said to be a linearization [34,
42] ofP(λ) if there exist mn×mnunimodular matrix polynomials U(λ) andV(λ)
(i.e., det(U(λ)) and det( V(λ)) are nonzero constants independent of λ) such that
U(λ)L(λ)V(λ) = diag(I(m−1)n, P(λ)) forλ∈C.Additionally, if revL(λ) :=Y+λX
is a linearization of revP(λ) thenL(λ) is said to be a strong linearization [42] of P(λ),
whererevP(λ) :=λmP(1/λ).Inspired by the seminal work of Miroslav Fiedler for
∗Corresponding author, Department of Mathematics, Josip Ju raj Strossmayer University of Osi-
jek, Osijek-31000, Croatia ( ranjankdiitg@gmail.com, rkdas@mathos.hr ). This author’s research is
supported by the Croatian Science Foundation, grant no. IP- 2019-04-6774, Vibration Reduction in
Mechanical Systems - VIMS.
†Department of Electrical Engineering, IIT Bombay, Powai, I ndia (hp@ee.iitb.ac.in ).
1scalarpolynomial[32], Fiedler pencils (FPs) andgeneralizedFiedlerpen cils (GFPs) of
a polynomialmatrix P(λ) arestudied in [7,13] which arestronglinearizationsof P(λ).
Matrixpolynomial havingstructure suchas symmetric, skew-symm etric, palindromic,
even, odd, etc., arise in many applications, and the structure ofte n induces spectral
symmetry in the eigenvalues. The spectral symmetry in the eigenva lues has physical
signiﬁcance and hence it is important to consider a linearization that p reserves the
spectral symmetry, see [1,5,8–12,14,16,21,24–26,30,35,3 6,38,40–43,45,46] and
the references therein. To overcome the limitation of FPs and GFPs in order to
construct structure preserving linearizations, FPRs and GFPRs a re contructed in [53]
and [15], respectively. Note that the basic building blocks of FPRs and GFPRs are
a very special type of GFPs and so FPRs and GFPRs do not utilize the p otential
of GFPs in fullest. Indeed, not all the GFPs are FPRs or GFPRs, and v ice versa.
For example, consider the GFP T(λ) =λMP
(−4,−3,−1)−MP
(0,2)and the FPR L(λ) =
(λMP
−4,−3−MP
(0,1,2))MP
1ofP(λ) :=/summationtext4
i=0λiAi(see Section 2.1 for the deﬁnition of
MP
j). ThenT(λ) is not an FPR and L(λ) is not a GFP. Further, consider the pencil
F(λ) := (λMP
(−4,−1)−MP
(2,3,0))MP
2. ThenF(λ) is neither a GFP nor an FPR of P(λ).
This motivates us to present a uniﬁed framework that subsumes all the Fiedler-like
linearizations, i.e., FPs, GFPs, FPRs, and GFPRs of P(λ). We named this family as
Extended generalizedFiedlerpencils (EGFPs) of P(λ). Indeed, EGFPsis a hugeclass
of linearizations that expand the arena in which to look for the struc tured preserving
linearizations for structured P(λ). Also, it allows us to construct linearizations with
special property like banded linearizations with low bandwidth.
We mentionthattheredoesnotexistanyblockpenta-diagonal(re sp., tridiagonal)
symmetric GFPR for symmetric P(λ) having degree m≥8 (resp.,m≥4) [17]. But,
the family of EGFPs has no such shortcoming and enables us to const ruct symmetric
block penta-diagonal EGFPs for symmetric P(λ). Form= 8,the pencil
L(λ) =
−A80λA8 0 0 0 0 0
0 0 −InλIn0 0 0 0
λA8−InλA7−A6λA60 0 0 0
0λInλA6λA5+A4−In0 0 0
0 0 0 −In0λIn 0 0
0 0 0 0 λInλA3+A2A1−X
0 0 0 0 0 A1−λA1+A0λX
0 0 0 0 0 −X λX 0

is a block symmetric and block penta-diagonal EGFP, where X∈Cn×n. Moreover,
ifXis symmetric then L(λ) is symmetric when P(λ) is symmetric. We mention that
L(λ) is not a GFPR of P(λ) [17]. For palindromic P(λ), a framework of structured-
preserving linearizations having speciﬁc block-bandwidth is studied in [22] by using
the EFPRs of P(λ). Further, in [22], it is discussed how the EGFPs overcomethe lim-
itation of the family of GFs and FPRs in order to construct palindromic linearizations
with speciﬁc bandwidth, see [22] for details.
Linearizationofrationalmatrices is a relativelynew concept and has been studied
extensively in past few years by following the state-space model fo r rational matrices
pioneered by Rosenbrock [48], see [3,4,6,17–19,21,28,29,31,4 9] and the reference
therein. Consider a realization of an n×nrational matrix G(λ) of the form G(λ) =
P(λ) +C(λE−A)−1B, whereA−λEis anr×rpencil with Ebeing nonsingular,
P(λ) =/summationtextm
i=0λiAiis ann×nmatrix polynomial of degree m,C∈Cn×randB∈
Cr×n.Fiedler pencils (FPs) and generalized Fiedler pencils (GFPs) and gener alized
2Fiedler pencils with repetitions (GFPRs) have been constructed in [3, 4,21] which
are shown to be Rosenbrock strong linearizations (see Deﬁnition 2.1 ) ofG(λ) in [21].
Note that, neither GFPs of G(λ) is a subset of GFPRs, and vice versa. Further,
the construction of FPs, GFPs, and GFPRs of G(λ) is described by deﬁning Fiedler
matrices for G(λ) separately, that is, the Fiedler matrices of G(λ) are deﬁned by
invokingtheFiedlermatricesof P(λ)inoneblock,followedbypositioningtheconstant
matricesB,C,AandEin the appropriate positions of the other blocks. This force
to state a lot of notations and deﬁnitions for G(λ) parallel to the existing notations
and deﬁnitions for P(λ). We address these issues in this paper. More precisely, the
main contributions of the present paper are the following.
We build an uniﬁed framework (named as EGFPs) that subsumes all th e Fiedler-
like linearizations FPs, GFPs, FPRs and GFRs for matrix polynomials. We show by
some examples that EGFPs is a rich source to look into the structure d preserving
linearizations for structured (symmetric, skew-symmetric, even , odd, palindromic,
etc.,) matrix polynomials. Then we characterize block tridiagonal and block penta-
diagonal EGFPs of P(λ). This characterization also works for FPs, GFPs, FPRs, and
GFPRs ofP(λ). By utilizing the EGFPsof P(λ), weconstruct afamily ofRosenbrock
strong linearizationsof G(λ) directly. This construction is simple and it does not need
to deﬁne Fiedler matrices for G(λ) separately (as it is done in case of FPs, GFPs and
GFPRs in [3,4,21]). Further, we describe how to construct block tr idiagonal and
block penta-diagonal Rosenbrock strong linearization of G(λ). Finally, we describe
easy recovery of eigenvectors, minimal bases and minimal indices of P(λ) andG(λ)
from those of the linearizations.
The rest of the paper is organized as follows. We collect some basic re sults
in Section 2. In Section 3, we introduce EGFPs of P(λ), and give a characteriza-
tion for block tridiagonal and block penta-diagonal EGFPs of P(λ). Construction of
Rosenbrock strong linearizations for rational matrices from thos e of EGFPs of ma-
trix polynomial is described in Section 4. In Section 5, we describe the recovery of
eigenvectors, minimal bases and minimal indices of P(λ) andG(λ) from those of the
linearizations.
Notation. We denote by C[λ] the ring (over C) of scalar polynomials and by
C(λ) the ﬁeld of rational functions of the form p(λ)/q(λ),wherep(λ) andq(λ) are
polynomials in C[λ].We denote by C[λ]m×n(resp.,C(λ)m×n) the vector space of
m×nmatrix polynomials (resp., rational matrices) over C(resp., over C(λ)). The
spacesC[λ]mandC(λ)m,respectively, denote C[λ]m×nandC(λ)m×nwhenn= 1.We
denote the j-th column of the n×nidentity matrix Inbyejand the transpose (resp.,
conjugate transpose) of an m×nmatrixAbyAT(resp.,A∗). The right and left
null spaces of Aare given by Nr(A) :={x∈Cn:Ax= 0}andNl(A) :={x∈Cm:
xTA= 0},respectively. We denote by A⊗Bthe Kronecker product of the matrices
AandB.
2. Preliminaries. LetG(λ)∈C(λ)m×n. The rank of G(λ) over the ﬁeld C(λ)
is called the normal rank ofG(λ) and is denoted by nrank( G).If nrank(G) =n=m
thenG(λ) is said to be regular, otherwise G(λ) is said to be singular. A complex
numberµ∈Cis said to be an eigenvalue ofG(λ) if rank(G(µ))<nrank(G).We
denote the set of eigenvalues of Gbyeig(G).Let
D(λ) := diag/parenleftbiggφ1(λ)
ψ1(λ),...,φk(λ)
ψk(λ),0m−k,n−k/parenrightbigg
(2.1)
3be the Smith-McMillan form [39,48] of G(λ),wherek:= nrank(G) and the scalar
polynomials φi(λ) andψi(λ) are monic and pairwise coprime and that φi(λ) divides
φi+1(λ) andψi+1(λ) dividesψi(λ),fori= 1,2,...,k−1. Note that, if G(λ) is a
matrix polynomial, then ψi(λ) = 1,fori= 1,2,...,k, andD(λ) is called the Smith
normal form of G(λ). SetφG(λ) :=/producttextk
j=1φj(λ) andψG(λ) :=/producttextk
j=1ψj(λ).Then
µ∈Cis a pole of G(λ) ifψG(µ) = 0.A complex number µis said to be a zeroof
G(λ) ifφG(µ) = 0.Thespectrum ofG(λ) is given by Sp( G) :={λ∈C:φG(λ) = 0}
and consists of the ﬁnite zeros of G(λ).Note that eig(G)⊂Sp(G).See [3,6,39] for
more on eigenvalues and zeros of G(λ).
LetG(λ)∈C(λ)n×n. We consider a realization of G(λ) of the form
G(λ) =/summationdisplaym
j=0Ajλj+C(λE−A)−1B=:P(λ)+C(λE−A)−1B,(2.2)
whereλE−Ais anr×rmatrix pencil with Ebeing nonsingular, C∈Cn×rand
B∈Cr×n.The realization (2.2) is said to be minimal if the size of the pencil λE−A
is the smallest among all the realizations of G(λ),see [39]. The matrix polynomial
S(λ) :=/bracketleftbiggP(λ)C
BA−λE/bracketrightbigg
(2.3)
is called the system matrix (or the Rosenbrock system matrix) of G(λ) associated
with the realization (2.2). The system matrix S(λ) is said to be irreducible if the
realization (2.2) is minimal. The system matrix S(λ) is irreducible if and only if
rank/parenleftBig/bracketleftbigB A−λE/bracketrightbig/parenrightBig
=r= rank/parenleftBig/bracketleftbigCT(A−λE)T/bracketrightbigT/parenrightBig
,see [39,48]. Observe
thateig(G)⊂eig(S)andwehave eig(S) = Sp(G)whenS(λ)isirreducible,see[3,48].
Ann×nmatrix polynomial U(λ) is said to be unimodular if det(U(λ)) is a
nonzero constant independent of λ.A rational matrix G(λ) is said to be properif
G(λ)→Dasλ→ ∞,whereDis a matrix. An n×nrational matrix F(λ) is said to
bebiproper ifF(λ) is proper and F(∞) is a nonsingular matrix [51].
Definition 2.1 ( [18]).LetL(λ)be an(mn+r)×(mn+r)irreducible system
matrix of the form
L(λ) :=/bracketleftbiggX −λYC
BH−λK/bracketrightbigg
, (2.4)
whereH−λKis anr×rpencil with Kbeing nonsingular. Then L(λ)is said to be
a Rosenbrock strong linearization of G(λ)if the following conditions hold.
(a) There exist mn×mnunimodular matrix polynomials U(λ)andV(λ), and
r×rnonsingular matrices U0andV0such that
/bracketleftbiggU(λ)0
0U0/bracketrightbigg
L(λ)/bracketleftbiggV(λ)0
0V0/bracketrightbigg
=/bracketleftbiggI(m−1)n0
0S(λ)/bracketrightbigg
.
(b) There exist mn×mnbiproper rational matrices Oℓ(λ)andOr(λ)such that
Oℓ(λ)λ−1G(λ)Or(λ) =/bracketleftbiggI(m−1)n0
0λ−mG(λ)/bracketrightbigg
,
whereG(λ) :=X −λY+C(λK−H)−1Bis the transfer function of L(λ).
4The pencil L(λ)is also referred to as a Rosenbrock strong linearization of S(λ).
We refer to [18] for more on Rosenbrock strong linearizations of G(λ) and the
relation between the structural indices of (ﬁnite and inﬁnite) zero s and poles of G(λ)
andL(λ).Suﬃce it to say that the condition (a) ensures (see, [4, Theorem 3.4 ]) that
U(λ)G(λ)V(λ) = diag(I(m−1)n, G(λ)) which in turn ensures that G(λ) andG(λ)
have the same ﬁnite zeros and poles. The irreducibility of L(λ) guarantees that the
ﬁnite zeros and poles of G(λ) are the same as the ﬁnite eigenvalues of L(λ) and
H−λK,respectively; see [18,39]. On the other hand, the condition (b) ens ures that
the structural indices of zeros and poles of G(λ) at inﬁnity can be recovered from the
structural indices of eigenvalues and poles of L(λ) at inﬁnity (see [18]). See [6] for
an alternative deﬁnition of strong linearization of rational matrices and the follow-
up papers [28,29,31] for more on linearization. Thus the zeros and poles ofG(λ)
including their structural indices can be obtained by solving the eigen value problems
L(λ)v= 0 and (H−λK)u= 0; see [3,4,18,21].
2.1. Index tuples. Index tuples play a very important role in deﬁning and
analyzing Fiedler-like pencils. For a ready reference, we now deﬁne in dex tuples and
matrix assignments.
Fork,ℓ∈Z,we use the notation ( k:ℓ) := (k,k+ 1,...,ℓ) ifk≤ℓ,and
(k:ℓ) :=∅ifk>ℓ.We deﬁne ( ∞:ℓ) :=∅for any integer ℓ.
Definition 2.2 ( Index tuple ).A tupleσ:= (j1,j2,...,j p)∈Zpis said to
be anindex tuple containing indices from Z.We deﬁne −σ:= (−j1,−j2,...,−jp),
rev(σ) := (jp,jp−1,...,j 2,j1)andσ+q:= (j1+q,j2+q,...,j p+q)forq∈Z.
Further, for any index tuples α1:= (i1,...,ip)andα2:= (j1,...,j q), we deﬁne
α1∪α2:= (α1,α2) = (i1,...,ip,j1,...,j q).
Definition 2.3. Letαandβbe index tuples. Then αis said to be a subtuple
ofβifα=βor ifαcan be obtained from βby deleting some indices in β.
Consecutions and inversions of an index tuple are easy to derive and it will be
used frequently in this paper.
Definition 2.4 ( Consecutions and inversions, [20] ).Letαbe an index tuple
containing indices from {0 :m}(resp.,−{0 :m}). Suppose that t∈α. Then
we say that αhaspconsecutions attif(t,t+ 1,...,t+p)is a subtuple of αbut
(t,t+1,...,t+p,t+p+1)is not a subtuple of α. We denote the number of consecutions
ofαattbyct(α). Ift /∈αthen we deﬁne ct(α) :=−1.We say that αhasqinversions
attif(t+q,t+q−1,...,t)is a subtuple of αbut(t+q+ 1,t+q,...,t)is not a
subtuple of α. We denote the number of inversions of αattbyit(α). Ift /∈αthen
we deﬁneit(α) :=−1.
Example 2.5. Letα:= (1,0,2,1,3,2,4,1,3,2,1)be an index tuple containing
indices from {0 : 6}. Thenc0(α) = 3as(0,1,2,3)is a subtuple of αand(0,1,2,3,4)
is not a subtuple of α. Similarly, c3(α) = 1as(3,4)is a subtuple of αand(3,4,5)is
not a subtuple of α. Further,i0(α) = 1as(1,0)is a subtuple of αand(2,1,0)is not
a subtuple of α. Similarly, observe that i1(α) = 3andi3(α) = 1. As5/∈αwe have
c5(α) =−1andi5(α) =−1.
ForX∈Cn×n, we consider the elementary matrices given by [15]
M0(X) :=/bracketleftbiggI(m−1)n
X/bracketrightbigg
, Mi(X) :=
I(m−i−1)n
X In
In0
I(i−1)n
, i= 1 :m−1,
5M−m(X) :=/bracketleftbiggX
I(m−1)n/bracketrightbigg
andM−i(X) :=
I(m−i−1)n
0In
InX
I(i−1)n
i= 1 :m−1.
Fori= 1 :m−1, the matrices Mi(X) andM−i(X) are invertible for any Xand
(Mi(X))−1=M−i(−X).On the other hand, the matrices M0(X) andM−m(X) are
invertible if and only if Xis invertible. Note that Mi(X)Mj(Y) =Mj(Y)Mi(X)
holds for any matrices X,Y∈Cn×nif||i|−|j||>1.
For the rest of the paper, we assume that P(λ) :=/summationtextm
j=0λjAj.Fori∈ {−m:
m−1}, we deﬁne [15]
MP
i:=/braceleftbigg
Mi(−Ai),ifi≥0
Mi(A−i),ifi<0.
ThenMP
iare the Fiedler companion matrices associated with P(λ) and are given
by [7,15,23]
MP
0=/bracketleftbiggI(m−1)n
−A0/bracketrightbigg
,MP
i=
I(m−i−1)n
−AiIn
In0
I(i−1)n
fori= 1 :m−1,
MP
−m=/bracketleftbigg
Am
I(m−1)n/bracketrightbigg
, MP
−i=
I(m−i−1)n
0In
InAi
I(i−1)n
fori= 1 :m−1.
Note thatMP
0(resp.,MP
−m) is invertible if and only if A0(resp.,Am) is invertible.
We deﬁneMP
−0:= (MP
0)−1andMP
m:= (MP
−m)−1.
Definition 2.6 ( Matrixassignments,[15] ).Lett:= (t1,t2,...,tr)be a nonempty
index tuple containing indices from {±0,±1,...,±m}andX:= (X1,X2,...,X r)be
a tuple ofn×nmatrices. We deﬁne Mt(X) :=Mt1(X1)Mt2(X2)···Mtr(Xr)and say
thatXis amatrix assignment fort. Further, we say that the matrix Xjis assigned
to the index tjint. The matrix assignment Xfortis said to be nonsingular if the
matrices assigned by Xto the positions in toccupied by the ±0and±mindices are
nonsingular. Further, we deﬁne rev(X) := (Xr,...,X 1).
For an index tuple t,Mt(X) is invertible if Xis an invertible matrix assignment.
We deﬁneMt(X) :=Imniftis an empty set. Further, if X1,...,Xsare matrix
assignments for the index tuples t1,...,ts, respectively, then we deﬁne [15]
M(t1,...,ts)(X1,...,Xs) :=Mt1(X1)···Mts(Xs).
We deﬁneMP
t:=MP
t1MP
t2···MP
trift:= (t1,t2,...,tr) is a nonempty index tuple.
Definition 2.7. [15] Lett:= (t1,t2,...,tr)be an index tuple containing indices
from{±0,±1,...,±m}. ThenX:= (X1,X2,...,X r)is said to be the trivial matrix
assignment associated with P(λ)ifMtj(Xj) =MP
tjforj= 1 :r.
Thus, ifXis the trivial matrix assignment for tassociated with P(λ), then
Mt(X) =MP
t.
With a view to providing a unique representation of equivalent index tu ples, we
now deﬁne SIP, rsf, and csf of an index tuple, which we will use exten sively.
Definition 2.8. [12,15,53] Let τ:= (j1,j2,...,j q)be an index tuple containing
indices from Zandσ:= (i1,i2,...,it)be an index tuple containing indices from
{0 :h}for some non-negative integer h. Then:
6(a)jpis said to be a simple index ofτifjp/\e}atio\slash=jkfork= 1 :qandk/\e}atio\slash=p.We
say thatτissimpleif each index jpis a simple index for p= 1 :q.
(b)σis said to satisfy the Successor Inﬁx Property (SIP) if for every pair of
indicesia,ib∈σwith1≤a < b≤tsatisfyingia=ib,there exists at least
one indexic=ia+1such thata < c < b. Setα:=−σ. Thenαis said to
satisfy the SIP if α+hsatisﬁes the SIP.
(c)σis said to be in column standard form if
σ= (as:bs,as−1:bs−1,...,a 2:b2,a1:b1),
for some 0≤b1<···<bs−1<bs≤hand0≤aj≤bj,for allj= 1,...,s.
Setβ:=−σ. Thenβis said to be in column standard form if β+his in
column standard form.
(d)σis said to be in row standard form if
σ= (rev(a1:b1),rev(a2:b2),...,rev(as−1:bs−1),rev(as:bs)),
for some 0≤b1<···<bs−1<bs≤hand0≤aj≤bj,for allj= 1,...,s.
Setβ:=−σ. Thenβis said to be in row standard form if β+his in row
standard form.
Letαbe an index containing indices from {0 :m}(resp.,−{0 :m}) such that
αsatisﬁes the SIP. Then the positions of the block entries of Mα(X) do not depend
upon the particular matrix assignment X, that is, the positions of the block entries
ofMα(X) depend only on α, see [15].
Definition 2.9. [15,53] Let αandβbe two index tuples containing indices from
{0 :m}(resp.,−{0 :m}). Thenαis said to be equivalent toβ(written as α∼β) if
MP
α=MP
β.
It is shown in [12,15,53] that an index tuple satisfying the SIP is equiv alent to a
unique index tuple in column/row standard form, see [12,15,53] for more details.
Definition 2.10 ( csf, rsf, [12,53] ).Letσbe an index tuple containing indices
from{0 :m}(resp.,−{0 :m}) such that σsatisﬁes the SIP. Then the unique tuple
in column standard form which is equivalent to σis called the column standard form
ofσand is denoted by csf(σ).Similarly, the unique tuple in row standard form which
is equivalent to σis called the row standard form of σand is denoted by rsf(σ).
The following results will be used frequently in this paper.
Proposition 2.11. [17,20,21] Let αbe an index tuple containing indices from {0 :
m}such thatαsatisﬁes the SIP. Let s∈αandcs(α)be the number of consecutions
ofαats. Thenα∼/parenleftbig
αL,s,s+1,...,s+cs(α),αR/parenrightbig
for some tuples αLandαRsuch
thats /∈αLands+cs(α),s+cs(α) + 1/∈αR. Similarly, let t∈αandit(α)be
the number of inversions of αatt. Thenα∼/parenleftbig
αL,t+it(α),...,t+1,t,αR/parenrightbig
for some
tuplesαLandαRsuch thatt /∈αRandt+it(α),t+it(α)+1/∈αL.
Proposition 2.12. Letαbe an index tuple containing indices from −{0 :m}
such thatαsatisﬁes the SIP. Then we have the following.
(a) If−s∈αandc−s(α) =p, thenα∼/parenleftbig
αL,−s,−(s−1),...,−(s−p),αR/parenrightbig
for
some index tuples αLandαRsuch that −s /∈αLand−(s−p),−(s−p−1)/∈
αR.
(b) if−t∈αandi−t(α) =q, thenα∼/parenleftbig
αL,−(t−q),...,−(t−1),−t,αR/parenrightbig
for
some index tuples αLandαRsuch that −t /∈αRand−(t−q),−(t−q−1)/∈αL.
3. Extended generalized Fiedler pencil of P(λ).Fiedler pencils (FPs), gen-
eralized Fiedler pencils (GFPs), Fiedler pencils with repetition (FPRs) a nd general-
7ized Fiedler pencils with repetition (GFPRs) are well known classes of s trong lin-
earizations of matrix polynomials [13,15,23,53]. In this section, we c onstruct a new
family linearizations (named as EGFPs) of P(λ) that subsumes FPs, GFPs, FPRs
and GFPRs. We show by examples that EGFPs are rich source for con struction
of structure preserving linearizations for structured P(λ). Then in Section 3.1, we
characterize block tridiagonal and block penta-diagonal EGFPs of P(λ).
Definition 3.1 ( EGFP pfP(λ)).Let(σ,ω)be a permutation of {0 :m}. Set
τ:=−ω.Letσ1andσ2be index tuples containing indices from σ\{m−1,m}such
that(σ1,σ,σ2)satisﬁes the SIP. Similarly, let τ1andτ2be index tuples containing
indices from τ\{−1,−0}such that (τ1,τ,τ2)satisﬁes the SIP. Let X1,X2,Y1andY2
be any arbitrary nonsingular matrix assignments for σ1,σ2,τ1, andτ2, respectively.
Then the pencil
L(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2) (3.1)
is said to be an extended generalized Fiedler pencil (EGFP) ofP(λ).
Example 3.2. LetP(λ) :=/summationtext5
i=0λiAi. ThenL(λ) :=/parenleftbig
λMP
(−5,−1)−MP
(3,4,2,0)/parenrightbig
M3(X)
=
λA5+A4−X−In0 0
A3λX+A2λIn−In0
−InλIn0 0 0
0 −In0 0 λIn
0 0 0 λInλA1+A0

is an EGFP of P(λ). Note that L(λ)does not belong to FPs, GFPs, FRs, and GFPRs.
Note that if P(λ)is symmetric (i.e., Ai=AT
i, fori= 0 :m) thenL(λ)is symmetric
forX=−A3. Further, we mention that this symmetric pencil can not be ge nerated
using the framework given in [15] for symmetric linearizati ons./squaresolid
It is easy to observethat the family of EGFPs of P(λ) contains all the Fiedler-like
pencils FPs, GFPs, FPRs and GFPRs of P(λ). Indeed we have the following remark.
Remark 3.3. LetL(λ)be as given in Deﬁnition 3.1. Then by considering special
types ofσ,τ,σjandτj,j= 1,2,we have the following:
(1) ifσis a permutation of {0 :m−1},τ= (−m),σ1=∅=σ2andτ1=∅=τ2
thenL(λ)is an FP of P(λ).
(2) ifσ1=∅=σ2andτ1=∅=τ2theL(λ)is a GFP of P(λ).
(3) ifσis a permutation of {0 :h},0≤h≤m−1,τis a permutation of
{−m:−(h+1)}thenL(λ)is a GFPR of P(λ).
(4) ifσandτare as given in (3), and XiandYi, i= 1,2,are trivial matrix
assignments, then L(λ)is an FPR of P(λ).
The following result shows that, with some generic nonsingularity con ditions, the
EGFPs are strong linearization of P(λ).
Theorem 3.4. If all the matrix assignments XjandYj,j= 1,2,are nonsingular,
then the EGFP L(λ)as given in Deﬁnition 3.1 is a strong linearization of P(λ).
Proof. We have L(λ) =M(τ1,σ1)(Y1,X1)T(λ)M(σ2,τ2)(X2,Y2), whereT(λ) :=
λMP
τ−MP
σis a GF pencil of P(λ). AsXjandYj,j= 1,2,are nonsingular matrix
assignments, M(τ1,σ1)(Y1,X1) andM(σ2,τ2)(X2,Y2) are nonsingular matrices. Since
T(λ) is a strong linearization of P(λ), it follows that L(λ) is a strong linearization of
P(λ).
Assumption: Henceforth, for simplicity and without loss of generality, througho ut
this paper we assume that the matrix assignments are always nonsin gular.
Remark 3.5. It follows from Deﬁnition 3.1 that (τ1,σ1)/\e}atio\slash∼(σ1,τ1)and(σ2,τ2)/\e}atio\slash∼
8(τ2,σ2)in general. So Mτ1(Y1)Mσ1(X1)/\e}atio\slash=Mσ1(X1)Mτ1(Y1)andMσ2(X2)Mτ2(Y2)/\e}atio\slash=
Mτ2(Y2)Mσ2(X2). Thus, for L(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)
andT(λ) :=Mσ1(X1)Mτ1(Y1)(λMP
τ−MP
σ)Mτ2(Y2)Mσ2(X2),we haveL(λ)/\e}atio\slash=T(λ).
By interchanging the positions of Mσj(Xj)withMτj(Yj)inL(λ)we obtain a new
family of pencils for P(λ).
An EGFP L(λ) :=λL1−L0is said to be operation if each block of L1and
L0is either 0,±In, Aj,A−1
j, j= 0 :m,or any one of the matrices in the matrix
assignments Xj,Yj,j= 1,2.The following result shows that EGFPs are operation
free. We prove Proposition 3.6 in Appendix C.
Proposition 3.6. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2) =:
λL1−L0be an EGFP of P(λ)such thatm−1andmsimultaneously do not belong to
σ, and−1and−0simultaneously do not belong to τ. ThenL(λ)is operation free.
Weendthissectionbyconstructingfewexamplesofstructurepre servinglineariza-
tions for structured (symmetric, even, odd, palindromic, skew-s ymmetric)P(λ).
Example 3.7. LetP(λ) :=/summationtext6
i=0λiAibe symmetric. Then the EGFP L(λ)
given byL(λ) :=/parenleftbig
λMP
(−6,−3,−2,−4,−0)−MP
(1,5)/parenrightbig
MP
−3=

λA6+A5−In0 0 0 0
−In0 0 λIn 0 0
0 0 0 −InλIn 0
0λIn−InλA4−A3λA30
0 0 λInλA3λA2+A1−In
0 0 0 0 −In−A−1
0

is a symmetric strong linearization of P(λ)ifA0is nonsingular.
A matrix polynomial P(λ) :=/summationtextm
j=0Ajλjis said to be T-even ifP(−λ)T=P(λ),
i.e.,AT
i= (−1)iAifori= 0 :m. Similarly, P(λ) is said to be T-odd ifP(−λ)T=
−P(λ), i.e.,AT
i= (−1)i+1Aifori= 0 :m.
Example 3.8. LetP(λ) :=/summationtext5
i=0λiAibeT-even. Let L(λ)be the EGFP of
P(λ)as given in Example 3.2, where X=−A3. SetQ= diag(In,−In,In,−In,In).
Then
QL(λ) =
λA5+A4A3−In0 0
−A3λA3−A2−λInIn0
−InλIn 0 0 0
0 In 0 0 −λIn
0 0 0 λInλA1+A0

is aT-even strong linearization of P(λ)but it cannot be generated using the framework
of FPs, GFPs, FPRs and GFPRs.
Example 3.9. LetP(λ) :=/summationtext5
i=0λiAibeT-odd. LetL(λ)be the EGFP of P(λ)
as given in Example 3.2, where X=−A3. SetQ= diag(In,−In,−In,In,−In). Then
QL(λ) =
λA5+A4A3−In0 0
−A3λA3−A2−λInIn 0
In −λIn0 0 0
0 −In0 0 λIn
0 0 0 −λIn−λA1−A0

is aT-odd strong linearization of P(λ)but it cannot be generated using the framework
of FPs, GFPs, FPRs and GFPRs. /squaresolid
9The matrix polynomial P(λ) :=/summationtextm
j=0Ajλjis said to be skew-symmetric if
P(λ)T=−P(λ), i.e.,AT
i=−Aifori= 0,1,...,m.
Example 3.10. LetP(λ) :=/summationtext5
i=0λiAibe skew-symmetric. Let L(λ) :=/parenleftbig
λMP
(−5,−1)−MP
(3,4,2,0)/parenrightbig
MP
3be the EGFP of P(λ)as given in Example 3.2. Set
Q= diag(In,In,−In,−In,In). Then
QL(λ) =
λA5+A4A3−In0 0
A3−λA3+A2λIn−In0
In −λIn0 0 0
0 In 0 0 −λIn
0 0 0 λInλA1+A0

is a skew-symmetric strong linearization of P(λ)but it cannot be generated using the
framework of FPs, GFPs, FPRs and GFPRs.
The matrix polynomial P(λ) :=/summationtextm
j=0Ajλjis said to be T-palindromic (resp.,
T-anti-palindromic)if AT
i=Am−i(resp.,AT
i=−Am−i) fori= 0 :m. A subfamily of
EGFPs that preserve the T-Palindromic and T-anti-palindromic structure of P(λ) is
studied in [22]. The following example is a palindromic linearization of a palind romic
P(λ), see [22] for banded palindromic linearizations with speciﬁc block-ba ndwidth.
LetP(λ) :=/summationtext7
i=0λiAibeT-palindromic. Then the anti-block-penta-diagonal pencil
λ
0 0 0 0 0 −In0
0 0 0 0 0 −A1−A0
0 0 0 InA30 0
0 0 0 0 −In0 0
InA6A50 0 0 0
0A7A60 0 0 0
0 0 A70 0 0 0
+
0 0 0 0 In0 0
0 0 0 0 A1A00
0 0 0 0 A2A1A0
0 0 In0 0 0 0
0 0 A4−In0 0 0
−In−A60 0 0 0 0
0−A70 0 0 0 0

is aT-palindromic strong linearization of P(λ) but it cannot be generated using the
framework of FPs, GFPs, FPRs and GFPRs.
3.1. Low bandwidth banded EGFPs. Low bandwidth banded linearizations
ofP(λ) are important from the point of numerical computations. As EGFP s ofP(λ)
substantially enlarge the arena in which to look for strong linearizatio ns ofP(λ), it is
natural to investigate the possibility to construct low bandwidth ba nded EGFPs. In
particular, we are interested to construct block tridiagonal and b lock penta-diagonal
EGFPs ofP(λ). The following simple observations from the elementary matrices of
P(λ) will be used throughout of the paper.
We make the convention that ej= 0 forj≤0 andj > m. LetZbe ann×n
arbitrary matrix. Then for j= 0 :m−2, we have
(eT
m−i⊗In)Mi+1(Z) = (eT
m−i⊗In)
I(m−i−2)n
Z In
In0
Iin
=eT
m−(i+1)⊗In,
and fori= 0 :m−1 andj /∈ {i,i+1}, we have (eT
m−i⊗In)Mj(Z) =eT
m−i⊗In.This
shows that
(eT
m−i⊗In)Mj(Z) =/braceleftbiggeT
m−(i+1)⊗Inforj=i+1, i= 0 :m−2,
eT
m−i⊗Inforj /∈ {i,i+1}, i= 0 :m−1.(3.2)
10Similarly, we have
Mj(Z)(em−i⊗In) =/braceleftbiggem−(i+1)⊗Inforj=i+1, i= 0 :m−2,
em−i⊗Inforj /∈ {i,i+1}, i= 0 :m−1.(3.3)
Further, for i= 1 :m−1, we have
(eT
m−i⊗In)M−i(Z) = (eT
m−i⊗In)
I(m−i−1)n
0In
InZ
I(i−1)n
=eT
m−(i−1)⊗In,
and fori= 0 :m−1, j /∈ {i,i+1},we have (eT
m−i⊗In)M−j(Z) =eT
m−i⊗In.Thus
(eT
m−i⊗In)M−j(Z) =/braceleftbiggeT
m−(i−1)⊗Inforj=iandi= 1 :m−1,
eT
m−i⊗Inforj /∈ {i,i+1}, j= 0 :m−1,(3.4)
Similarly, we have
M−j(Z)(em−i⊗In) =/braceleftbiggem−(i−1)⊗Inforj=iandi= 1 :m−1,
em−i⊗Inforj /∈ {i,i+1}, i= 0 :m−1.(3.5)
Weneedthefollowingpropositionstocharacterizethebockpenta- diagonalEGFPs
ofP(λ). Since the proofs are lengthy and involved, we provide the proofs of Proposi-
tions 3.11, 3.12, 3.13 and 3.14 in Appendix B to make this paper simpler to read.
Proposition 3.11. Letαbe an index tuple containing indices from {0 :m−1}
such thatαsatisﬁes the SIP. Suppose that ck(α)≤1andik(α)≤1for any index
1≤k≤m−1. LetXbe a matrix assignment for α. ThenMα(X)is a block
penta-diagonal matrix. Equivalently, we have
(eT
m−k⊗In)Mα(X) =2/summationdisplay
j=−2(eT
m−(k−j)⊗Xj)fork= 0 :m−1,(3.6)
whereXj= 0orXjbelongs to the matrix assignment X.We prove Proposition 3.11
in Appendix B.1.
Analogous to Proposition 3.11 we have the following result for index tu ple con-
taining negative indices.
Proposition 3.12. Letαbe an index tuple containing indices from {−m:−1}
such thatαsatisﬁes the SIP. Suppose that c−k(α)≤1andi−k(α)≤1for any index
−(m−1)≤ −k≤ −1(this implies that c−m(α)≤2andi−m(α)≤2). LetXbe a
matrix assignment for α. ThenMα(X)is a block penta-diagonal matrix. Equivalently,
we have
(eT
m−k⊗In)Mα(X) =2/summationdisplay
j=−2(eT
m−(k−j)⊗Xj)fork= 0 :m−1,(3.7)
whereXj= 0orXjbelongs to the matrix assignment X.
Proposition 3.13. Letαbe an index tuple containing indices from {0 :m}such
thatαsatisﬁes the SIP. Suppose that cj(α)≥2orij(α)≥2for some 1≤j≤m−1.
LetXbe any arbitrary nonsingular matrix assignment for α. ThenMα(X)is not a
block penta-diagonal matrix.
11For index tuple containing negative indices we have the following result .
Proposition 3.14. Letβbe an index tuple containing indices from {−m:−0}
such thatβsatisﬁes the SIP. Suppose that c−j(β)≥2ori−j(β)≥2for some 1≤
j≤m−1. LetXbe a matrix assignment for β. ThenMβ(X)would never be a block
penta-diagonal matrix.
To characterize block penta-diagonal EGFPs we need the following d eﬁnition.
Definition 3.15. Letαandβbe sub-permutations of {0 :m−1}and{−m:−1},
respectively. Then k∈α\{0}is said to be an end index of αifk−1/∈αork+1/∈α.
Similarly, −t∈β\{−m}is said to be an end index of βif−(t−1)/∈βor−(t+1)/∈β.
The following theorem characterizes all block penta-diagonal EGFP s ofP(λ).
Theorem 3.16. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)
be an EGFP of P(λ). Suppose that σj(resp.,τj), forj= 1,2, does not contain
the end indices of σ(resp.,τ). ThenL(λ)is block penta-diagonal if and only if
ct(σ1,σ,σ2)≤1,it(σ1,σ,σ2)≤1,c−t(τ1,τ,τ2)≤1andi−t(τ1,τ,τ2)≤1for any
index1≤t≤m−1.
Proof. (=⇒) The forward implication follows from Proposition 3.13 and Proposi-
tion 3.14.
(⇐=)Wehave L(λ) =:λL1−L0, whereL1:=Mτ1(Y1)Mσ1(X1)MP
τMσ2(X2)Mτ2(Y2)
andL0:=Mτ1(Y1)Mσ1(X1)MP
σMσ2(X2)Mτ2(Y2). Note that L(λ) is a block penta-
diagonal pencil if and only if both L0andL1are block penta-diagonal matrices. We
only prove that L0is a block penta-diagonal matrix. Similarly, L1is block penta-
diagonal.
Sinceτ1andτ2do not contain the end indices of τ, it follows that ||j|−|k|| ≥2 for
j∈(τ1,τ2)andk∈(σ1,σ,σ2). Henceτjcommuteswith( σ1,σ,σ2),j= 1,2,andhence
L0=Mτ1(Y1)Mτ2(Y2)Mσ1(X1)MP
σMσ2(X2) =Mσ1(X1)MP
σMσ2(X2)Mτ1(Y1)Mτ2(Y2).
Suppose that m∈σ. Recall that, as σ1andσ2contain indices from {0 :
m−2}, there are no repetition of m−1 andmin the indices of ( σ1,σ,σ2). Hence
(σ1,σ,σ2)∼(β,m) (resp., (σ1,σ,σ2)∼(m,β)) ifm−1∈σandσhas a con-
secution (resp., inversion) at m−1, whereβ:= (σ1,σ,σ2)\ {m}. Further, since
Mm(W) = diag(W−1,I(m−1)n) (for any nonsingular matrix W∈Cn×n) is a block
diagonalmatrix, without loss ofgeneralitywe assume that m /∈σ, i.e.,m /∈(σ1,σ,σ2).
Now we prove that L0is a block penta-diagonal matrix. Note that to prove L0is
a block penta-diagonal matrix it is enough to show that
(eT
m−k⊗In)L0=2/summationdisplay
j=−2(eT
m−(k−j)⊗Zj) fork= 0 :m−1, (3.8)
whereZjisanyoneofthematrices0 ,±In,±A0,...,±Amorthematricesin X1,X2,Y1
andY2. Since (σ,−τ) is a permutation of {0 :m}, we have either k∈σork∈ −τ.
Case-I: Suppose that k∈σ. Then −k /∈τ. This implies that −k,−(k+
1)/∈(τ1,τ2) since (τ1,τ,τ2) satisﬁes the SIP. Hence by (3.4), we have ( eT
m−k⊗
In)Mτ1(Y1)Mτ2(Y2) =eT
m−k⊗In.This implies that ( eT
m−k⊗In)L0= (eT
m−k⊗
In)Mσ1(X1)MP
σMσ2(X2).Now (3.8) follows from Proposition 3.11 by considering
α:= (σ1,σ,σ2) andX:= (X1,P,X2), wherePdenotes the trivial matrix assign-
ment forσ.
Case-II: Suppose that k∈ −τ. Then there are two cases.
(a) Suppose that k+1∈σ(i.e.,−kis an end index of τ). Sinceτj,j= 1,2,does
not contain the end indices of τ, we havek,k+1/∈ −(τ1,τ2). Now by following the
similar arguments as given in Case-I we have (3.8).
12(b) Suppose that −(k+1)∈τ. Then, since( σ,−τ) isapermutationof {0 :m}, we
havek,k+1/∈(σ1,σ,σ2). Henceby(3.2), wehave( eT
m−k⊗In)Mσ1(X1)MP
σMσ2(X2) =
eT
m−k⊗In. Consequently, we have ( eT
m−k⊗In)L0= (eT
m−k⊗In)Mτ1(Y1)Mτ2(Y2).Now
(3.8) follows from Proposition 3.12 by considering β:= (τ1,τ2) andX:= (Y1,Y2).
HenceL0is a block penta-diagonal matrix. This completes the proof.
For constructing block tridiagonal EGFPs of P(λ) we need the following results.
Proposition 3.17. Letαbe an index tuple containing indices from {0 :m}such
thatαsatisﬁes the SIP. Suppose that cj(α)≥1orij(α)≥1for some 1≤j≤m−1.
LetXbe any arbitrary nonsingular matrix assignment for α. ThenMα(X)is not a
block-tridiagonal matrix.
Similarly, let βbe an index tuple containing indices from {−m:−0}such that
βsatisﬁes the SIP. Suppose that c−j(β)≥1ori−j(β)≥1for some 1≤j≤m−1.
LetYbe any arbitrary nonsingular matrix assignment for β. ThenMβ(Y)is not a
block-tridiagonal matrix.
Proof. The proof is similar to those of Proposition 3.13 and Proposition 3.14.
The following theorem characterizes all block tridiagonal EGFPs of P(λ). Note
that, for an index tuple αcontaining nonnegative indices and any index t, we have
ct(α) = 0 andit(α) = 0 imply that t∈αbutt−1,t+1/∈α. Similar thing holds for
index tuple containing negative indices.
Theorem 3.18. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be
an EGFP of P(λ). ThenL(λ)is block tridiagonal if and only if ct(σ1,σ,σ2) = 0,
it(σ1,σ,σ2) = 0,c−t(τ1,τ,τ2) = 0andi−t(τ1,τ,τ2) = 0for any index 1≤t≤m−1.
Proof. (=⇒) The forward implication follows from Proposition 3.17.
(⇐=) Deﬁne A:=Mτ1(Y1)Mσ1(X1) andB:=Mσ2(X2)Mτ2(Y2). Then we have
L(λ) =:λL1−L0, whereL1:=AMP
τBandL0:=AMP
σB. Note that L(λ) is a
block-tridiagonal pencil if and only if both L0andL1are block-tridiagonal matrices.
We only proof that L0is a block tridiagonalmatrix. Similarly, L1is block tridiagonal.
It is given that, for any index 1 ≤t≤m−1, ift∈σthen we have ct(σ1,σ,σ2) = 0
andit(σ1,σ,σ2) = 0. This implies that, for 1 ≤j≤m−2, ifj∈σthenj+1/∈σ.
Moreover, we have σj= (0) orσj=∅,j= 1,2.SinceM0(Z) = diag(I(m−1)n,Z), we
haveMσj(Xj) = diag(I(m−1)n,Xj) orMσj(Xj) =Imnforj= 1,2.Further, giventhat
c−p(τ1,τ,τ2) = 0andi−p(τ1,τ,τ2) = 0foranyindex −(m−1)≤ −p≤ −1and−p∈τ.
This implies that τj= (−m) orτj=∅,j= 1,2.SinceM−m(Z) = diag(Z,I(m−1)n),
we haveMτj(Yj) = diag(Yj,I(m−1)n) orMτj(Yj) =Imn,j= 1,2.Consequently, A
andBare block diagonal matrices. Hence L0is block tridiagonal if and only if MP
σ
is block tridiagonal. Since, |j−k| ≥2 for distinct j,k∈σwith 1≤j,k≤m−1,
it is clear from the Fiedler matrices MP
ℓ,ℓ= 0 :m, thatMP
σis tridiagonal. This
completes the proof.
Remark 3.19. Since EGFPs subsumes FPs, GFPs, FPRs and GFPRs, Theo-
rem 3.16 and Theorem 3.18, respectfully, provides a charact erization for block penta-
diagonal and block tridiagonal FPs, GFPs, FPRs and GFPRs.
4. Rosenbrock strong linearizations of rational matrices. In this section,
we construct a family of Rosenbrock strong linearizations of a ratio nal matrix G(λ)
directly from the EGFPs of P(λ) as given in Section 3. We show that this family
subsumes all the Fiedler-like linearizations constructed in [3,4,21]. T hroughout of
this section, we consider the rational matrix G(λ) =P(λ) +C(λE−A)−1Band its
13associated system matrix S(λ) as given in (2.2) and (2.3), respectfully.
Definition 4.1 ( EGFP ofG(λ)).LetL(λ)be an EGFP of P(λ)as given in
(3.1) such that 0∈σand−m∈τ. Then the pencil
L(λ) :=/bracketleftBigg
L(λ)em−i0(σ1,σ)⊗C
eT
m−c0(σ,σ2)⊗BA−λE/bracketrightBigg
(4.1)
is said to be an EGFP of G(λ). We also refer to L(λ)as an EGFP of S(λ).
Example 4.2. LetG(λ) :=/summationtext5
i=0λiAi+C(λE−A)−1B.LetL(λ) :=/parenleftbig
λMP
(−5,−3)−
MP
(4,1,2,0)/parenrightbig
MP
1be the EGFP of P(λ)associated with σ= (4,1,2,0),τ= (−5,−3),
σ1= (1),σ2=∅, andτ1=∅=τ2. Note that c0(σ,σ2) = 1andi0(σ1,σ) = 1. Then
L(λ) =
λA5+A4−In0 0 0 0
−In0λIn 0 0 0
0λInλA3+A2A1−In0
0 0 A1−λA1+A0λInC
0 0 −InλIn 00
0 0 0 B 0A−λE

is an EGFP of G(λ).
Note that if G(λ) is symmetric (i.e., G(λ)T=G(λ)) then there exists a minimal
symmetricrealizationof G(λ) ofthe form G(λ) =P(λ)+BT(λE−A)−1B, whereP(λ)
is symmetric (i.e., AT
i=Ai, i= 0 :m), andAandEare symmetric matrices with
Ebeing nonsingular [28]. The system matrix S(λ) :=/bracketleftbiggP(λ)BT
BA−λE/bracketrightbigg
is obviously
symmetric and irreducible. Then L(λ) as given in Example 4.2 is a symmetric EGFP
ofG(λ) and it followsfrom Theorem4.6that L(λ) isaRosenbrockstronglinearization
ofG(λ).
From Deﬁnition 4.1, it is clear that the family of EGFPs of G(λ) contains all
the Fiedler-like pencils (FPs, GFPs, FPRs, GFRs) of G(λ) constructed in [3,4,21].
Indeed, if the pencil L(λ) in Deﬁnition 4.1 is an FP (respectively, GFP, FPR and
GFPR) ofP(λ), thenL(λ) is an FP (respectively, GFP, FPR and GFPR) of G(λ).
Next we show that EGFPs of G(λ) are Rosenbrock strong linearizations of G(λ).
To that end, we need the following result which is a corollary of [20, Le mma 3.10].
Lemma 4.3. Let0≤h≤m−1,and letσbe a permutation of {0 :h}. Letσ1
andσ2be index tuples containing indices from {0 :h−1}such that (σ1,σ,σ2)satisﬁes
the SIP. Then (eT
m−c0(σ)⊗In)Mσ2(Y) =eT
m−c0(σ,σ2)⊗InandMσ1(X) (em−i0(σ)⊗
In) =em−i0(σ1,σ)⊗Infor any arbitrary matrix assignments XandYforσ1andσ2,
respectively.
Analogs to Lemma 4.3, we have following result for EGFPs of P(λ) which will be
used in proving that EGFPs of G(λ) are Rosenbrock strong linearizations of G(λ).
Lemma 4.4. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be
an EGFP of P(λ)as given in (3.1). Suppose that 0∈σand−m∈τ. Then
(eT
m−c0(σ)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−c0(σ,σ2)⊗In, (4.2)
andMτ1(Y1)Mσ1(X1)(em−i0(σ)⊗In) =em−i0(σ1,σ)⊗In. (4.3)
Proof. Given that 0 ∈σand−m∈τ(i.e.,m /∈σ). Lethbe the integer such
that 0,1,...,h∈σandh+1/∈σ. Thenc0(σ)≤handi0(σ)≤h. Further, we have
h,h+1/∈σ1∪σ2ash+1/∈σand (σ1,σ,σ2) satisﬁes the SIP.
14Let/hatwideσand/hatwideσj,j= 1,2, respectively, be the subtuples of σandσjwith indices
{0 :h}. Similarly, let /hatwide/hatwideσand/hatwide/hatwideσj,j= 1,2, respectively, be the subtuples of σandσj
with indices {h+2:m}. Then/hatwideσand/hatwide/hatwideσcommutessince foranyindices k∈/hatwideσandℓ∈/hatwide/hatwideσ
we have |k−ℓ|>1. Thusσ∼(/hatwideσ,/hatwide/hatwideσ)∼(/hatwide/hatwideσ,/hatwideσ). Similarly, σj∼(/hatwideσj,/hatwide/hatwideσj)∼(/hatwide/hatwideσj,/hatwideσj) for
j= 1,2.Further, ( /hatwideσ,/hatwide/hatwideσj)∼(/hatwide/hatwideσj,/hatwideσ) forj= 1,2.Sinceh+1/∈σ, we havei0(σ) =i0(/hatwideσ)
andc0(σ) =c0(/hatwideσ). Further, c0(σ,σ2) =c0(/hatwide/hatwideσ,/hatwideσ,/hatwide/hatwideσ2,/hatwideσ2) =c0(/hatwide/hatwideσ,/hatwide/hatwideσ2,/hatwideσ,/hatwideσ2) =c0(/hatwideσ,/hatwideσ2),
where the last equality holds as 0 /∈/hatwide/hatwideσ∪/hatwide/hatwideσ2. Similarly, i0(σ1,σ) =i0(/hatwideσ1,/hatwideσ).Note that
X2andY2are arbitrary matrix assignments. We by denote ( ∗) any arbitrary matrix
assignment. Then we have
(eT
m−c0(/hatwideσ)⊗In)M/hatwide/hatwideσ2(∗)M/hatwideσ2(∗)Mτ2(∗)
= (eT
m−c0(/hatwideσ)⊗In)M/hatwideσ2(∗)Mτ2(∗) by (3.2) since/braceleftbigg
c0(/hatwideσ) =c0(σ)≤hand/hatwide/hatwideσ2
contains indices from {h+2 :m}
= (eT
m−c0(/hatwideσ,/hatwideσ2)⊗In)Mτ2(∗) by Lemma 4 .3
=eT
m−c0(/hatwideσ,/hatwideσ2)⊗Inby (3.4) since/braceleftbigg
c0(/hatwideσ,/hatwideσ2)≤handτ2contains indices
from{−m:−(h+2)}.
Thus (eT
m−c0(σ)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−c0(σ,σ2)⊗Inwhich prove (4.2). Similar
proof holds for (4.3). Indeed, we have
Mτ1(∗)M/hatwideσ1(∗)M/hatwide/hatwideσ1(∗)(em−i0(/hatwideσ)⊗In)
=Mτ1(∗)M/hatwideσ1(∗)(em−i0(/hatwideσ)⊗In) by (3.3) since/braceleftbigg
i0(/hatwideσ) =i0(σ)≤hand/hatwide/hatwideσ1
contains indices from {h+2 :m}
=Mτ1(∗)(em−i0(/hatwideσ1,/hatwideσ)⊗In) by Lemma 4 .3
=em−i0(/hatwideσ1,/hatwideσ)⊗Inby (3.5) since/braceleftbiggi0(/hatwideσ1,/hatwideσ)≤handτ1contains indices
from{−m:−(h+2)}.
ThusMτ1(Y1)Mσ1(X1)(em−i0(σ)⊗In) =em−i0(σ1,σ)⊗In. This completes the proof.
The following proposition is a restatement of [21, Theorem 3.6 and The orem 3.9]
which shows that the Fiedler pencils are Rosenbrock strong lineariza tion ofG(λ).
Proposition 4.5. LetT(λ) :=λMP
−m−MP
αbe the Fiedler pencil of P(λ)associ-
ated witha permutation αof{0 :m−1}. ThenT(λ) :=/bracketleftBigg
T(λ)em−i0(α)⊗C
eT
m−c0(α)⊗BA−λE/bracketrightBigg
is the Fiedler pencil of G(λ)associated with α. Moreover, T(λ)is a Rosenbrock strong
linearization of G(λ). Further, a pencil L(λ)given byL(λ) := diag( X,X0)T(λ)diag(Y,Y0),
whereX,Y ∈Cmn×mnandX0,Y0∈Cr×rare nonsingular matrices, is a Rosenbrock
strong linearization of G(λ).
We are now ready to prove that EGFPs of G(λ) are Rosenbrock strong lineariza-
tions ofG(λ).
Theorem 4.6. LetL(λ)be an EGFP of G(λ)given as in (4.1). Then L(λ)is a
Rosenbrock strong linearization of G(λ).
Proof. Let (σ,ω) be the permutation of {0 :m}where 0∈σandm∈ω. We have
τ= (−β,−m,−γ) for some permutations βandγ. Deﬁneα:= (rev(β),σ,rev(γ)).
Thenαis a permutation of {0 :m−1}andT(λ) :=λMP
−m−MP
αis a Fiedler
linearization of P(λ). This implies that T(λ) :=/bracketleftBigg
T(λ)em−i0(α)⊗C
eT
m−c0(α)⊗BA−λE/bracketrightBigg
is a
15Rosenbrock strong linearization of G(λ) follows from Proposition 4.5. Note that A:=
M(τ1,σ1)(Y1,X1)MP
−βandB:=MP
−γM(σ2,τ2)(X2,Y2) are nonsingular matrices. Hence
by Proposition 4.5, we only need to show that diag( A,Ir)T(λ)diag(B,Ir) =L(λ).
Since diag( A,Ir)T(λ)diag(B,Ir) =
/bracketleftBigg
M(τ1,σ1)(Y1,X1)MP
−β
Ir/bracketrightBigg
T(λ)/bracketleftBigg
MP
−γM(σ2,τ2)(X2,Y2)
Ir/bracketrightBigg
=/bracketleftBigg
T(λ) M(τ1,σ1)(Y1,X1)MP
−β(em−i0(α)⊗C)
(eT
m−c0(α)⊗B)MP
−γM(σ2,τ2)(X2,Y2) A−λE/bracketrightBigg
,
theremainingweneedtoproveisthat( eT
m−c0(α)⊗In)MP
−γM(σ2,τ2)(X2,Y2) =eT
m−c0(σ,σ2)⊗
InandM(τ1,σ1)(Y1,X1)MP
−β(em−i0(α)⊗In) =em−i0(σ1,σ)⊗In. From Lemma 4.4 we
have(eT
m−c0(σ)⊗In)M(σ2,τ2)(X2,Y2) =eT
m−c0(σ,σ2)⊗InandM(τ1,σ1)(Y1,X1)(em−i0(σ)⊗
In) =em−i0(σ1,σ2)⊗In. Hencetheonlythingleft toshowisthat( eT
m−c0(α)⊗In)MP
−γ=
eT
m−c0(σ)⊗InandMP
−β(em−i0(α)⊗In) =em−i0(σ)⊗In. We proceed as follows.
We have 0 /\e}atio\slash=β(since 0 ∈σ). This implies c0(α) =c0(rev(β),σ,rev(γ)) =
c0(σ,rev(γ). Letc0(σ) =k. Now we have two cases. Case-I: suppose that k+ 1∈
rev(γ), and Case-II: suppose that k+1/∈rev(γ).
Case-I: Let k+ 1∈rev(γ) andck+1(rev(γ)) =ℓ. Then by the deﬁnition of
consecutions, we have k+1,k+2,...,k+1+ℓis a subtuple of rev(γ) andk+1,k+
2,...,k+ 1 +ℓ,k+ℓ+ 2 is not a subtuple of rev(γ). This implies that rev(γ)∼
(δ,k+1,k+ 2,...,k+ 1+ℓ) and hence MP
rev(γ)=MP
(δ,k+1,k+2,...,k+1+ℓ), whereδis
the subpermutation of rev(γ) such that k+1,k+2,...,k+1+ℓ /∈δ.Consequently,
we havec0(α) =k+1+ℓ. Now (eT
m−c0(α)⊗In)MP
−γ= (eT
m−(k+1+ℓ)⊗In)MP
−γ=
(eT
m−(k+1+ℓ)⊗In)MP
(−(k+1+ℓ),−(k+ℓ),...,−(k+2),−(k+1))MP
−rev(δ)
= (eT
m−k⊗In)MP
−rev(δ)by applying (3.4) repetatively
=eT
m−k⊗Infollows from (3.4) since −k,−(k+1)/∈ −rev(δ)
=eT
m−c0(σ)⊗In.
Case-II: Suppose that k+ 1/∈rev(γ). Then it follows from the deﬁnition of
consecutionsthat c0(α) =c0(σ) =k. Consequently, −k,−(k+1)/∈ −γsincek∈σ. So
by (3.4) we have ( eT
m−c0(α)⊗In)MP
−γ= (eT
m−k⊗In)MP
−γ=eT
m−k⊗In=eT
m−c0(σ)⊗In.
A similar proof holds for MP
−β(em−i0(α)⊗In) =em−i0(σ)⊗Inby using inversions
instead of consecutions and (3.5). This completes the proof.
WeendthissectionbycharacterizingEGFPsof G(λ) havingblocktridiagonaland
block penta-diagonal form. We deﬁne the block-tridiagonal and blo ck penta-diagonal
system matrices as follows.
Definition 4.7. LetA(λ) :=/bracketleftbiggX −λYC
BH−λK/bracketrightbigg
be an(mn+r)×(mn+
r)matrix pencil, where X= (Xij)andY= (Yij)arem×mblock matrices with
Xij,Yij∈Cn×n,C ∈Cmn×r,B ∈Cr×mnandH,K∈Cr×r. ThenA(λ)is said to be
block-tridiagonal if XandYare block tridiagonal matrices and (eT
k⊗In)C= 0and
B(ek⊗In) = 0fork= 1 :m−1.Similarly, A(λ)is said to be block penta-diagonal if
XandYare block penta-diagonal matrices and (eT
k⊗In)C= 0andB(ek⊗In) = 0
fork= 1 :m−2.
16Observe that the EGFP L(λ) given in Example 4.2 is a block penta-diagonal
linearization of G(λ). The following results which follow immediately from Theo-
rem 3.18 and Theorem 3.16, respectively, characterize bock tridiag onal and block
penta-diagonal EGFPs of G(λ)
Corollary 4.8. LetL(λ)be an EGFP of G(λ)as given in Deﬁnition 4.1.
ThenL(λ)is block tridiagonal if and only if ct(σ1,σ,σ2) = 0,it(σ1,σ,σ2) = 0,
c−t(τ1,τ,τ2) = 0andi−t(τ1,τ,τ2) = 0for any index 1≤t≤m−1.
Corollary 4.9. LetL(λ)be an EGFP of G(λ)as given in Deﬁnition 4.1.
Suppose that σj(resp.,τj), forj= 1,2, does not contain the end indices of σ(resp.,
τ). ThenL(λ)is block penta-diagonal if and only if ct(σ1,σ,σ2)≤1,it(σ1,σ,σ2)≤1,
c−t(τ1,τ,τ2)≤1andi−t(τ1,τ,τ2)≤1for any index 1≤t≤m−1.
5. Recovery of minimal bases and minimal indices. Thissectionisdevoted
in describing the recovery of eigenvectors, minimal bases and minima l indices of P(λ)
andG(λ) from those of the linearizations constructed in Section 3 and Sect ion 4,
respectfully.
WhenG(λ) is singular, the right null space Nr(G) and the left null space Nl(G)
ofG(λ) are given by
Nr(G) :={x(λ)∈C(λ)n:G(λ)x(λ) = 0} ⊂C(λ)n,
Nl(G) :={y(λ)∈C(λ)m:y(λ)TG(λ) = 0} ⊂C(λ)m.
LetB:=/parenleftbig
x1(λ),...,x p(λ)/parenrightbig
be a polynomial basis [33,39] of Nr(G) ordered so that
deg(x1)≤ ··· ≤deg(xp),wherex1(λ),...,x p(λ) are vector polynomials, that is, are
elements of C[λ]n.Then Ord( B) := deg(x1) +···+ deg(xp) is called the orderof
the basis B.A basis Bis said to be a minimal polynomial basis [39] of Nr(G) if
Eis any polynomial basis of Nr(G) then Ord( E)≥Ord(B).A minimal polynomial
basisB:=/parenleftbig
x1(λ),...,x p(λ)/parenrightbig
ofNr(G) with deg( x1)≤ ··· ≤ deg(xp) is called a
right minimal basis ofG(λ) and deg(x1)≤ ··· ≤deg(xp) are called the right minimal
indicesofG(λ).Aleft minimal basis and theleft minimal indices ofG(λ) are deﬁned
similarly. See [33,39] for further details.
We say that a k×pmatrix polynomial Z(λ) is a minimal basis if the columns of
Z(λ) form a minimal basis of the subspace of C(λ)kspanned (over the ﬁeld C(λ)) by
the columns of Z(λ).
We need the following result which is a restatement of [13, Theorem 4.1 and
Theorem 4.2] describes the recovery of minimal bases and minimal ind ices ofP(λ)
from those of the GF pencils.
Theorem 5.1. [13] Let (σ,ω)be a permutation of {0 :m}such that 0∈σand
m∈ω. Setτ:=−ω. LetT(λ) :=λMP
τ−MP
σbe the GF pencil of P(λ)associated
with(σ,τ). Letτbe given by τ:= (τl,−m,τr). Setα:= (−rev(τl),σ,−rev(τr)).
Letc(α)andi(α), respectively, be the total number of consecutions and inve rsions of
the permutation αof{0 :m−1}. Consider FGF(P) =eT
m−c0(σ)⊗InandKGF(P) =
eT
m−i0(σ)⊗In. LetZ(λ)be anmn×pmatrix polynomial. Then we have the following.
IfZ(λ)is a right (resp., left) minimal basis of T(λ)then[FGF(P)Z(λ)](resp.,
[KGF(P)Z(λ)]) is a right (resp., left) minimal basis of P(λ).Further, if ε1≤ ··· ≤εp
are the right (resp., left) minimal indices of T(λ)thenε1−i(α)≤ ··· ≤εp−i(α)
(resp.,ε1−c(α)≤ ··· ≤εp−c(α)) are the right (resp., left) minimal indices of P(λ).
The following result describe the recovery of minimal bases and minima l indices
of a singular P(λ) from those of the EGFPs of P(λ).
Theorem 5.2. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be
an EGFP of P(λ)as given in (3.1). Suppose that P(λ)is singular. Let τbe given by
17τ:= (τl,−m,τr). LetcL:=c(−rev(τl),σ,−rev(τr))andiL:=i(−rev(τl),σ,−rev(τr)),
respectively, be the total number of consecutions and inver sions of the permutation
(−rev(τl),σ,−rev(τr))of{0 :m−1}. LetZ(λ)be anmn×pmatrix polynomial.
Then we have the following.
(a)Right and left minimal bases. IfZ(λ)is a right (resp., left) minimal
basis ofL(λ)then/bracketleftbig
(eT
m−c0(σ,σ2)⊗In)Z(λ)/bracketrightbig
(resp.,/bracketleftbig
(eT
m−i0(σ1,σ)⊗In)Z(λ)/bracketrightbig
)
is a right (resp., left) minimal basis of P(λ).
(b) Ifε1≤ ··· ≤εpare the right (resp., left) minimal indices of L(λ)then
ε1−iL≤ ··· ≤εp−iL(resp.,ε1−cL≤ ··· ≤εp−cL) are the right (resp.,
left) minimal indices of P(λ).
Proof. WehaveL(λ) =AT(λ)B,whereA:=Mτ1(Y1)Mσ1(X1),B:=Mσ2(X2)Mτ2(Y2),
andT(λ) :=λMP
τ−MP
σis a GF pencil of P(λ) associated with the permutation
ω:= (σ,−τ) of{0 :m}such that 0 ∈σand−m∈τ. SinceXjandYj,j= 1,2,
are nonsingular matrix assignments, AandBare nonsingular matrices. Thus, the
mapB:Nr(L)→ Nr(T), x(λ)/mapsto→ Bx(λ),is an isomorphism and maps a minimal
basis of Nr(L) to a minimal basis of Nr(T). On the other hand, by Theorem 5.1,
FGF(P) :Nr(T)→ Nr(P), y(λ)/mapsto→(eT
m−c0(σ)⊗In)y(λ), is an isomorphism and maps
a minimal basis of Nr(T) to a minimal basis of Nr(P). Consequently, we have
FGF(P)B:Nr(L)→ Nr(P), z(λ)/mapsto→(eT
m−c0(σ)⊗In)Bz(λ), is an isomorphism and
maps a minimal basis of Nr(L) to a minimal basis of Nr(P). By Lemma 4.4, we have
(eT
m−c0(σ)⊗In)B= (eT
m−c0(σ)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−c0(σ,σ2)⊗In
which proves the recovery of right minimal bases.
Next, weprovetherecoveryofleftminimalbases. Notethat AT:Nl(L)→ Nl(T),
x(λ)/mapsto→ ATx(λ),is a linear isomorphism and maps a minimal basis of Nl(L) to
a minimal basis of Nl(T). By Theorem 5.1, KGF(P) :Nl(T)→ Nl(P), y(λ)/mapsto→
(eT
m−i0(σ)⊗In)y(λ),is a linear isomorphism and maps a minimal basis of Nl(T) to a
minimal basis of Nl(P). Consequently, we have KGF(P)AT:Nl(L)→ Nl(P), z(λ)/mapsto→
(eT
m−i0(σ)⊗In)ATz(λ), is an isomorphism and maps a minimal basis Nl(L) to a
minimal basis Nl(P). By Lemma 4.4, we have ( eT
m−i0(σ)⊗In)AT= (eT
m−i0(σ)⊗
In)(Mτ1(Y1)Mσ1(X1))T
=(Mτ1(Y1)Mσ1(X1)(em−i0(σ)⊗In))T
=eT
m−i0(σ1,σ)⊗In
which proves the recovery of left minimal bases.
AsL(λ) is strictly equivalent to T(λ), the left (resp., right) minimal indices of
L(λ) andT(λ) are the same. Hence the desired results for minimal indices follow fr om
Theorem 5.1.
5.1. Recovery of eigenvectors. In this section we describe the recovery of
eigenvectors of P(λ) corresponding to an eigenvalue µ∈Cfrom those of the EGFPs
ofP(λ).The next result is a restatement of [13, Theorems 3.2] which describ es the
recoveryofeigenvectorsof P(λ) fromthoseofthe GFpencils. We presentthese results
for ready reference.
Theorem 5.3. [13] Suppose that P(λ)is regular and µ∈Cis an eigenvalue of
P(λ). LetT(λ) :=λMP
τ−MP
σbe a GF pencil of P(λ)associated with a permutation
18(σ,ω)of{0 :m}, whereτ:=−ω.Deﬁne
FGF(P) :=

eT
m−c0(σ)⊗Inif0∈σandc0(σ)<m
A−1
m(eT
1⊗In)if0∈σandc0(σ) =m
eT
m−s⊗Inif/braceleftbigg0∈ω,i0(ω)+1∈σand
s:=i0(ω)+ci0(ω)+1(σ)+1<m
A−1
m(eT
1⊗In)if/braceleftbigg0∈ω,i0(ω)+1∈σand
i0(ω)+ci0(ω)+1(σ)+1 =m
eT
m−i0(ω)⊗Inif0∈ω, i0(ω)<mandi0(ω)+1/∈σ
A−1
0(eT
m⊗In)if0∈ωandi0(ω) =m
and
KGF(P) :=

eT
m−i0(σ)⊗Inif0∈σandi0(σ)<m
A−T
m(eT
1⊗In)if0∈σandi0(σ) =m
eT
m−s⊗Inif/braceleftbigg
0∈ω, c0(ω)+1∈σand
s:=c0(ω)+ic0(ω)+1(σ)+1<m
A−T
m(eT
1⊗In)if/braceleftbigg0∈ω, c0(ω)+1∈σand
c0(ω)+ic0(ω)+1(σ)+1 =m
eT
m−c0(ω)⊗Inif0∈ω, c0(ω)<mandc0(ω)+1/∈σ
A−T
0(eT
m⊗In)if0∈ωandc0(ω) =m.
Deﬁne the maps FGF(P) :Nr(T)→ Nr(P), x/mapsto→FGF(P)x,andKGF(P) :Nl(T)→
Nl(P), y/mapsto→KGF(P)y. Let Z be an mn×pmatrix such that rank(Z) =p. Then we
have the following.
IfZis a basis of Nr(T(µ))(resp.,Nl(T(µ))), then/bracketleftbig
FGF(P)Z/bracketrightbig
(resp.,/bracketleftbig
KGF(P)Z/bracketrightbig
)
is a basis of Nr(P(µ))(resp.,Nl(P(µ))).
LetL(λ) =M(τ1,σ1)(Y1,X1)(λMP
τ−MP
σ)M(σ2,τ2)(X2,Y2) be an EGFP of P(λ).
Observe that the basic building blocks λMP
τ−MP
σofL(λ) is a GF pencil of P(λ). In
order to derive eigenvector recovery of P(λ) from those of L(λ) we need to compute
FGF(P)M(σ2,τ2)(X2,Y2) andM(τ1,σ1)(Y1,X1)(KGF)Tfor the various cases of FGF(P)
andKGF(P) given as in Theorem 5.3. We only state the outcomes of these compu -
tations in the following results for ease reading of the paper as the p roofs are not
important to the development that follows. We refer to Appendix A f or the proofs.
Lemma 5.4. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be an
EGFP ofP(λ)as given in (3.1). Suppose that 0∈σ. Then we have the following.
(a) Ifc0(σ)<mthen(eT
m−c0(σ)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−c0(σ,σ2)⊗In. Sim-
ilarly, ifi0(σ)<mthenMτ1(Y1)Mσ1(X1)(em−i0(σ)⊗In) =em−i0(σ1,σ)⊗In.
(b) Ifc0(σ) =mthen(eT
1⊗In)Mσ2(X2)Mτ2(Y2) =eT
1⊗In,and ifi0(σ) =m
thenMτ1(Y1)Mσ1(X1)(e1⊗In) =e1⊗In.
Lemma 5.5. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be an
EGFP ofP(λ)as given in (3.1). Suppose that 0∈ω(recall that τ=−ω). Then we
have the following.
(a) Ifi0(ω)+1∈σands:=i0(ω)+ci0(ω)+1(σ)+1<mthen
(eT
m−s⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−p⊗In,
wherep:=i0(ω)+ci0(ω)+1(σ,σ2)+1.
(b) Ifs=min part (a), then (eT
1⊗In)Mσ2(X2)Mτ2(Y2) =eT
p⊗In,where
p:=c−(m−1)(τ2)+2.
19(c) Ifi0(ω)<mandi0(ω)+1/∈σthen
(eT
m−i0(ω)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−p⊗In, (5.1)
wherep:=i0(ω)−c−i0(ω)(τ2)−1.
(d) Ifi0(ω) =min part (c), then (eT
m⊗In)Mσ2(X2)Mτ2(Y2) =eT
m⊗In.
The following result is analogous to Lemma 5.5.
Lemma 5.6. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be an
EGFP ofP(λ)as given in (3.1). Suppose that 0∈ω(recall that τ=−ω). Then we
have the following.
(a) Ifc0(ω)+1∈σands:=c0(ω)+ic0(ω)+1(σ)+1<mthen
Mτ1(Y1)Mσ1(X1)(em−s⊗In) =em−p⊗In,
wherep:=c0(ω)+ic0(ω)+1(σ1,σ)+1.
(b) Ifs=min part (a), then Mτ1(Y1)Mσ1(X1)(e1⊗In) =ep⊗In,wherep:=
i−(m−1)(τ1)+2.
(c) Ifc0(ω)<mandc0(ω)+1/∈σthen
Mτ1(Y1)Mσ1(X1)(em−c0(ω)⊗In) =em−p⊗In, (5.2)
wherep:=c0(ω)−i−c0(ω)(τ1)−1.
(d) Ifc0(ω) =min part (c), then Mτ1(Y1)Mσ1(X1)(em⊗In) =em⊗In.
We are now ready to describe the recovery of eigenvectors of a re gularP(λ) from
those of the EGFPs of P(λ).
Theorem 5.7. Suppose that P(λ)is regular and µ∈Cis an eigenvalue of
P(λ). LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be an EGFP of
P(λ)as given in (3.1). (Recall that ω=−τ). Let Z be an mn×pmatrix such that
rank(Z) =p. Deﬁne
FEGFP(P) :=

eT
m−c0(σ,σ2)⊗Inif0∈σandc0(σ)<m
A−1
m(eT
1⊗In)if0∈σandc0(σ) =m
eT
m−p⊗In if

0∈ω,i0(ω)+1∈σand
s:=i0(ω)+ci0(ω)+1(σ)+1<m,
wherep:=i0(ω)+ci0(ω)+1(σ,σ2)+1
A−1
m(eT
p⊗In)if

0∈ω,i0(ω)+1∈σand
i0(ω)+ci0(ω)+1(σ)+1 =m,
wherep:=c−(m−1)(τ2)+2
eT
m−p⊗In if/braceleftbigg
0∈ω, i0(ω)<mandi0(ω)+1/∈σ,
wherep:=i0(ω)−c−i0(ω)(τ2)−1
A−1
0(eT
m⊗In)if0∈ωandi0(ω) =m
20and
KEGFP(P) :=

eT
m−i0(σ1,σ)⊗Inif0∈σandi0(σ)<m
A−T
m(eT
1⊗In)if0∈σandi0(σ) =m
eT
m−p⊗In if

0∈ω, c0(ω)+1∈σand
s:=c0(ω)+ic0(ω)+1(σ)+1<m,
wherep:=c0(ω)+ic0(ω)+1(σ1,σ)+1
A−T
m(eT
p⊗In)if

0∈ω, c0(ω)+1∈σand
c0(ω)+ic0(ω)+1(σ)+1 =m,
wherep:=i−(m−1)(τ1)+2
eT
m−p⊗In if/braceleftbigg0∈ω, c0(ω)<mandc0(ω)+1/∈σ,
wherep:=c0(ω)−i−c0(ω)(τ1)−1
A−T
0(eT
m⊗In)if0∈ωandc0(ω) =m.
Then the maps FEGFP(P) :Nr(L(µ))→ Nr(P(µ)), x/mapsto→FEGFP(P)x,andKEGFP(P) :
Nl(L(µ))→ Nl(P(µ)), y/mapsto→KEGFP(P)y,are linear isomorphisms. Thus, if Zis a
basis ofNr(L(µ))(resp.,Nl(L(µ))), then/bracketleftbig
FEGFP(P)Z/bracketrightbig
(resp.,/bracketleftbig
KEGFP(P)Z/bracketrightbig
) is a basis
ofNr(P(µ))(resp.,Nl(P(µ))).
Proof. We have L(λ) =Mτ1(Y1)Mσ1(X1)T(λ)Mσ2(X2)Mτ2(Y2), whereT(λ) :=
λMP
τ−MP
σis a GF pencil of P(λ) associatedwith the permutation ( σ,−τ) of{0 :m}.
SinceXjandYj,j= 1,2,are nonsingular matrix assignments, M(τ1,σ1)(Y1,X1) and
M(σ2,τ2)(X2,Y2) are nonsingular. Hence the map M(σ2,τ2)(X2,Y2) :Nr(L(µ))→
Nr(T(µ)), x/mapsto→/parenleftbig
M(σ2,τ2)(X2,Y2)/parenrightbig
x,is a linear isomorphism. On the other hand, by
Theorem 5.3, FGF(P) :Nr(T(µ))→ Nr(P(µ)) is a linear isomorphism. Thus the map
Nr(L(µ))→ Nr(P(µ)), x/mapsto→/parenleftbig
FGF(P)M(σ2,τ2)(X2,Y2)/parenrightbig
x,
is a linear isomorphism. Now the desired result for recovery of right e igenvectors
follows from Theorem 5.3 and Lemmas 5.4 and 5.5.
Next we prove the recovery of left eigenvectors. As ( M(σ1,τ1)(Y1,X1))Tis non-
singular, (M(σ1,τ1)(Y1,X1))T:Nl(L(µ))→ Nl(T(µ)), y/mapsto→(M(σ1,τ1)(Y1,X1))Ty,is
a linear isomorphism. Also, by Theorem 5.3, KGF(P) :Nl(T(µ))−→ N l(P(µ)) is a
linear isomorphism. Thus the map
Nl(L(µ))→ Nl(P(µ)), y/mapsto→/parenleftBig
KGF(P)(M(σ1,τ1)(Y1,X1))Ty/parenrightBig
,
is a linear isomorphism. Now the desired result for recoveryof left eig envectorsfollows
from Theorem 5.3 and Lemmas 5.4 and 5.6.
We now illustrate eigenvector recovery rule for P(λ) from those of the EGFPs of
P(λ) by considering a few examples.
Example 5.8. LetP(λ) :=/summationtext6
i=0λiAi. Suppose that P(λ)is regular and
µ∈Cis an eigenvalue of P(λ). Letσ:= (1,2,5),σ1:=∅,σ2:= (1),τ:=
(−6,−3,−4,−0),τ1:= (−4)andτ2:=∅. LetXandYbe any arbitrary matrix assign-
ments forτ1andσ2, respectively. Then the pencil L(λ) =M−4(X)/parenleftbig
λMP
(−3,−4,−6,−0)−
MP
(1,2,5)/parenrightbig
M1(Y) =
21
λA6+A5−In0 0 0 0
0 0 −InλIn 0 0
−In0λIn−X λX 0 0
0λInλA4λA3+A2−Y−In
0 0 0 A1λY−InλIn
0 0 0 −In−λA−1
00

is an EGFP of P(λ).
Letu∈ Nr(L)andv∈ Nl(L). Deﬁneui:= (eT
i⊗In)uandvi:= (eT
i⊗In)v,i=
1 : 6.Note that 0∈ωandi0(ω) = 0. Further,i0(ω)+1 = 1 ∈σandci0(ω)+1(σ) = 1,
which implies that s:=i0(ω)+ci0(ω)+1(σ)+1 = 2<6.Asc1(σ,σ2) = 1, we havep:=
i0(ω)+ci0(ω)+1(σ,σ2)+1 = 2.Thus by Theorem 5.7, (eT
6−2⊗In)u=u4∈ Nr(P(µ)).
To verify the recovery rule, consider L(λ)u= 0.This gives
(λA6+A5)u1−u2= 0 (5.3)
−u3+λu4= 0 (5.4)
−u1+(λIn−X)u3+λXu4= 0 (5.5)
λu2+λA4u3+(λA3+A2)u4−Yu5−u6= 0 (5.6)
A1u4+(λY−In)u5+λu6= 0 (5.7)
−u4−λA−1
0u5= 0 (5.8)
From (5.4) we have u3=λu4. Substituting u3=λu4in (5.5) we have u1=λ2u4.
Addingλtimes (5.6) with (5.7) and then substituting u3=λu4we haveλ2u2+
(λ3A4+λ2A3+λA2+A1)u4−u5= 0which together with (5.8) gives
λ3u2+(λ4A4+λ3A3+λ2A2+λA1+A0)u4= 0. (5.9)
Now substituting the value of λ3u2from (5.9) and u1=λ2u4in (5.3), we have
P(λ)u4= 0.
Next, consider v∈ Nl(L). Deﬁnevi:= (eT
i⊗In)v,i= 1 : 6.Note that 0∈ωand
c0(ω) = 0. Further,c0(ω)+1 = 1 ∈σandi1(σ) = 0which implies that s:=c0(ω)+
ic0(ω)+1(σ)+1 = 1<6.Asi1(σ1,σ) = 0, we havep:=c0(ω)+ic0(ω)+1(σ1,σ)+1 = 1.
Hence by Theorem 5.7, we have (eT
6−1⊗In)v=v5∈ Nl(P(µ))which can be easily
veriﬁed. /squaresolid
Next, we consider an EGFP which is not operation free (as −1 and−0 simulta-
neously belong to τ) but the recovery of eigenvector is operation free.
Example 5.9. LetP(λ) :=/summationtext5
i=0λiAi. Suppose that P(λ)is regular and µ∈C
is an eigenvalue of P(λ). Letσ:= (4,2,3),σ1:=∅,σ2:= (2),τ:= (−5,−1,−0)
andτ1=∅=τ2. LetXbe any arbitrary matrix assignment for σ2. Then the EGFP
L(λ) =/parenleftbig
λMP
(−5,−1,−0)−MP
(4,2,3)/parenrightbig
M2(X)ofP(λ)is given by
L(λ) =
λA5+A4A3−X−In 0
−InλIn0 0 0
0A2λX−InλIn 0
0−In0 0 −λA−1
0
0 0 λIn0−λA1A−1
0−In
.
Letu∈ Nr(L)andv∈ Nl(L). Deﬁneui:= (eT
i⊗In)uandvi:= (eT
i⊗In)v,i=
1 : 5.Note that 0∈ωandi0(ω) = 1. Further,i0(ω)+1 = 2 ∈σandci0(ω)+1(σ) = 1,
22which imply that s:=i0(ω)+ci0(ω)+1(σ)+1 = 3<5.Asc2(σ,σ2) = 1, we havep:=
i0(ω)+ci0(ω)+1(σ,σ2)+1 = 3.Thus by Theorem 5.7, (eT
5−3⊗In)u=u2∈ Nr(P(µ))
which can be easily veriﬁed.
Note that 0∈ωandc0(ω) = 0<5. Further,c0(ω)+1 = 1/∈σ. Asτ1=∅, we
havei−c0(ω)(τ1) =−1. Sop:=c0(ω)−i−c0(ω)(τ1)−1 = 0. Hence by Theorem 5.7,
(eT
5−0⊗In)v=v5∈ Nl(P(µ))which can be easily veriﬁed. /squaresolid
The EGFP in the following example is not operation free (as −1 and−0 simul-
taneously belong to τ), but we can easily recover the eigenvectors of P(λ) from those
of the EGFP.
Example 5.10. LetP(λ) :=/summationtext3
i=0λiAi. Suppose that P(λ)is regular and µ∈C
is an eigenvalue of P(λ). Letσ:= (3),σ1:=∅=σ2,τ:= (−2,−1,−0),τ1=∅and
τ2:= (−2). Then the EGFP L(λ) =/parenleftbig
λMP
(−2,−1,−0)−MP
3/parenrightbig
MP
−2is given by
L(λ) =λ
0 0 −A−1
0
0In−A2A−1
0
InA2−A1A−1
0
−
0A−1
30
InA20
0 0In
.
Letx∈ Nr(L(µ))andy∈ Nl(L(µ)). Deﬁnexi:= (eT
i⊗In)xandyi:= (eT
i⊗In)y
fori= 1 : 3.Note that 0∈ωandi0(ω) = 2. Further, i0(ω) + 1 = 3 ∈σand
ci0(ω)+1(σ) = 0, which implies s:=i0(ω) +ci0(ω)+1(σ) + 1 = 3 , i.e.,s=m. Now,
c−(m−1)(τ2) =c−2(τ2) = 0. Thus by Theorem 5.7, A−1
m(eT
2⊗In)x=A−1
3x2∈
Nr(P(µ))which can be easily veriﬁed.
For left eigenvector, we have c0(ω) = 0<m,c0(ω)+1 = 1/∈σandi−c0(ω)(τ1) =
−1(asτ1=∅). Thusp=c0(ω)−i−c0(ω)(τ1)−1 = 0. Thus by Theorem 5.7,
(eT
m−p⊗In)y=y3∈ Nl(P(µ))which can be easily veriﬁed. /squaresolid
5.2. Eigenvalue at inﬁnity and recovery of eigenvectors. This section
is devoted in describing the recovery of eigenvectors of P(λ) corresponding to an
eigenvalue at ∞from the eigenvectors of EGFPs. Recall that ∞is an eigenvalue of
P(λ) =/summationtextm
i=0λiAiif and only if 0 is an eigenvalue of rev(P(λ)) =/summationtextm
i=0λiAm−i, i.e.,
0 is an eigenvalue of Am. ThusAmis singular, which implies that MP
−mis singular,
i.e.,MP
mdoes not exist as MP
m= (MP
−m)−1. So−malways belongs to τ, that is,
m /∈σ. To derive the recovery formulas, we need the following result which is a
restatement of [13, Theorem 3.4] that describes the recovery of eigenvectors of P(λ)
corresponding to an eigenvalue at ∞from those of the GF pencils of P(λ).
Theorem 5.11. [13] LetP(λ)be a regular matrix polynomial. Let T(λ) :=
λMP
−τ−MP
σbe a GF pencil of P(λ)associated with a permutation (σ,τ)of{0 :m},
wherem∈τ.Suppose that ∞is an eigenvalue of P(λ). LetZbe anmn×kmatrix
such that rank(Z) =k.
Right eigenvectors at ∞.IfZis a basis of the right eigenspace of T(λ)at∞,
then we have the following.
(a) Ifc−m(τ)<mthen/bracketleftbig
(eT
c−m(τ)+1⊗In)Z/bracketrightbig
is a basis of the right eigenspace of
P(λ)at∞.
(b) Ifc−m(τ) =mthen/bracketleftbig
A−1
0(eT
m⊗In)Z/bracketrightbig
is a basis of the right eigenspace of
P(λ)at∞.
Left eigenvectors at ∞.IfZis a basis of the left eigenspace of T(λ)at∞, then
we have the following.
(c) Ifi−m(τ)< mthen/bracketleftbig
(eT
i−m(τ)+1⊗In)Z/bracketrightbig
is a basis of the left eigenspace of
P(λ)at∞.
23(d) Ifi−m(τ) =mthen/bracketleftbig
A−T
0(eT
m⊗In)Z/bracketrightbig
is a basis of the left eigenspace of P(λ)
at∞.
The following result follows from the proof of [20, Theorem 3.21 (see (3.9) and
(3.14))].
Lemma 5.12. Let0≤h≤m−1,and letτbe a permutation of {−m:−(h+1)}.
Letτ1andτ2be index tuples containing indices from τsuch that (τ1,τ,τ2)satisﬁes
the SIP. Then (eT
c−m(τ)+1⊗In)Mτ2(Y) =eT
c−m(τ,τ2)+1⊗InandMτ1(X)(ei−m(τ)+1⊗
In) =ei−m(τ1,τ)+1⊗Infor any arbitrary matrix assignments XandYforτ1andτ2,
respectively.
Similar to Lemma 5.12, we have the following result for EGFPs which will pla y a
crucial role in the recovery of eigenvectors of P(λ) corresponding to an eigenvalue ∞
from those of the EGFPs.
Lemma 5.13. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be an
EGFP ofP(λ)such that −m∈τ.
(a) Suppose that c−m(τ)<mandi−m(τ)<m.Then
(eT
c−m(τ)+1⊗In)Mσ2(X2)Mτ2(Y2) =eT
c−m(τ,τ2)+1⊗In
and
Mτ1(Y1)Mσ1(X1)(ei−m(τ)+1⊗In) =ei−m(τ1,τ)+1⊗In.
(b) Ifc−m(τ) =mthen(eT
m⊗In)Mσ2(X2)Mτ2(Y2) =eT
m⊗In, and ifi−m(τ) =m
thenMτ1(Y1)Mσ1(X1)(em⊗In) =em⊗In.
Proof. (a) For an arbitrary Z∈Cn×n, from (3.4) and (3.5) we have
(eT
j⊗In)M−k(Z) =eT
j⊗Infork /∈ {m−j,m−j+1}, j= 1 :m,(5.10)
M−k(Z)(ej⊗In) =ej⊗Infork /∈ {m−j,m−j+1}, j= 1 :m.(5.11)
Case-I: Suppose that σ/\e}atio\slash=∅. Let 0 ≤h≤m−1 be the integer such that
−m,−(m−1),...,−(m−h)∈τand−(m−h−1)/∈τ. This implies that −(m−
h),−(m−h−1)/∈τ1∪τ2since−(m−h−1)/∈τand (τ1,τ,τ2) satisﬁes the SIP.
Further, we have c−m(τ)≤handi−m(τ)≤h.
Let/hatwideτand/hatwideτj,j= 1,2,respectively, be thesubtuplesof τandτjwithindices {−m:
−(m−h)}. Similarly, let /hatwide/hatwideτand/hatwide/hatwideτj,j= 1,2, respectively, be the subtuples of τand
τjwith indices {−(m−h−2) :−0}, where{−a:−0}:={−a,−(a−1),...,−1,−0}
for any integer a≥0. Then /hatwideτand/hatwide/hatwideτcommutes since for any indices k∈/hatwideτandℓ∈/hatwide/hatwideτ,
we have ||k|−|ℓ||>1. Thusτ∼(/hatwideτ,/hatwide/hatwideτ)∼(/hatwide/hatwideτ,/hatwideτ). Similarly, τj∼(/hatwideτj,/hatwide/hatwideτj)∼(/hatwide/hatwideτj,/hatwideτj) for
j= 1,2,as/hatwideτjand/hatwide/hatwideτjcommutes. Also we have ( /hatwideτ,/hatwide/hatwideτj)∼(/hatwide/hatwideτj,/hatwideτ) forj= 1,2.
Since−(m−h−1)/∈τ,we havei−m(τ) =i−m(/hatwideτ) andc−m(τ) =c−m(/hatwideτ). As
(/hatwideτ,/hatwideτ2) contains indices from {−m:−(m−h)}, we havec−m(/hatwideτ,/hatwideτ2)≤h. This shows
thatc−m(/hatwideτ,/hatwideτ2,/hatwide/hatwideτ2) =c−m(/hatwideτ,/hatwideτ2) as/hatwide/hatwideτ2contains indices from {−(m−h−2) :−0}.
Consequently, we have c−m(τ,τ2) =c−m(/hatwide/hatwideτ,/hatwideτ,/hatwideτ2,/hatwide/hatwideτ2) =c−m(/hatwideτ,/hatwideτ2,/hatwide/hatwideτ2) =c−m(/hatwideτ,/hatwideτ2).
Similarly, we have i−m(τ1,τ) =i−m(/hatwideτ1,/hatwideτ).
Note thatX2andY2are arbitrary matrix assignments. We denote by ( ∗) for
any arbitrary matrix assignment. Now we have ( eT
c−m(τ)+1⊗In)Mσ2(∗)Mτ2(∗) =
(eT
c−m(/hatwideτ)+1⊗In)Mσ2(∗)M/hatwide/hatwideτ2(∗)M/hatwideτ2(∗) =
(eT
c−m(/hatwideτ)+1⊗In)M/hatwide/hatwideτ2(∗)M/hatwideτ2(∗) by (3.2) since/braceleftbiggc−m(/hatwideτ)≤handσ2contains
indices from {0 :m−h−2}
24= (eT
c−m(/hatwideτ)+1⊗In)M/hatwideτ2(∗) by (5.10) since/braceleftbigg
c−m(/hatwideτ)≤hand/hatwide/hatwideτ2contains
indices from {−(m−h−2) :−0}
=eT
c−m(/hatwideτ,/hatwideτ2)+1⊗In=eT
c−m(τ,τ2)+1⊗Inby Lemma 5 .12.
Hence (eT
c−m(τ)+1⊗In)Mσ2(X2)Mτ2(Y2) =eT
c−m(τ,τ2)+1⊗In.
Similarly, we have Mτ1(∗)Mσ1(∗)(ei−m(τ)+1⊗In) =
M/hatwideτ1(∗)M/hatwide/hatwideτ1(∗)Mσ1(∗)(ei−m(/hatwideτ)+1⊗In)
=M/hatwideτ1(∗)M/hatwide/hatwideτ1(∗)(ei−m(/hatwideτ)+1⊗In) by (3.3) since/braceleftbiggi−m(/hatwideτ)≤handσ1contains
indices from {0 :m−h−2}
=M/hatwideτ1(∗)(ei−m(/hatwideτ)+1⊗In) by (5.11) since/braceleftbigg
i−m(/hatwideτ)≤hand/hatwide/hatwideτ1contains
indices from {−(m−h−2) :−0}
=ei−m(/hatwideτ1,/hatwideτ)+1⊗In=ei−m(τ1,τ)+1⊗Inby Lemma 5 .12.
Thus, we have Mτ1(Y1)Mσ1(X1)(ei−m(τ)+1⊗In) =ei−m(τ1,τ)+1⊗In.
Case-II: Suppose that σ=∅, i.e.,τis a permutation of {−m:−0}. This implies
σ1=∅=σ2and henceMσ1(X1) =Imn=Mσ2(X2). Given that c−m(τ)≤m−1
andi−m(τ)≤m−1. By deﬁning /tildewideτ:=τ\ {−0}and/tildewideσ:= (0), and by considering
h=m−1, a verbatim proof of Case-I gives the desired result.
(b) Ifc−m(τ) =mori−m(τ) =m, thenσ=∅. So there are no choices for σ1
andσ2, i.e.,σ1=∅=σ2. HenceMσ1(X1) =Imn=Mσ2(X2). Further, from the
deﬁnition of EGFPs of P(λ) we haveτj,j= 1,2,contains indices from {−m:−2}.
Consequently, from (5.10) and (5.11), we have ( eT
m⊗In)Mτ2(Y2) =eT
m⊗Inand
Mτ1(Y1)(em⊗In) =em⊗Inwhich gives the desired result.
We are now ready to describe the recovery of eigenvectors of P(λ) corresponding
to the eigenvalue at ∞from those of the EGFPs of P(λ).
Theorem 5.14. LetL(λ) :=Mτ1(Y1)Mσ1(X1)(λMP
τ−MP
σ)Mσ2(X2)Mτ2(Y2)be
an EGFP of P(λ). Suppose that P(λ)is regular and ∞is an eigenvalue of P(λ). Let
Zbe anmn×kmatrix such that rank(Z) =k.
Right eigenvectors at ∞.IfZis a basis of the right eigenspace of L(λ)at∞,
then we have the following.
(a) Ifc−m(τ)<m, then[(eT
c−m(τ,τ2)+1⊗In)Z]is a basis of the right eigenspace
ofP(λ)at∞.
(b) Ifc−m(τ) =m, then/bracketleftbig
A−1
0(eT
m⊗In)Z/bracketrightbig
is a basis of the right eigenspace of
P(λ)at∞.
Left eigenvectors at ∞.IfZis a basis of the left eigenspace of L(λ)at∞,
then we have the following.
(c) Ifi−m(τ)<m, then[(eT
i−m(τ1,τ)+1⊗In)Z]is a basis of the left eigenspace of
P(λ)at∞.
(d) Ifi−m(τ) =m, then[A−T
0(eT
m⊗In)Z]is a basis of the left eigenspace of P(λ)
at∞.
Proof. SetL1:=M(τ1,σ1)(Y1,X1)MP
τM(σ2,τ2)(X2,Y2)andL0:=M(τ1,σ1)(Y1,X1)MP
σ
M(σ2,τ2)(X2,Y2). ThenL(λ) =λL1−L0. Recall that ∞is an eigenvalue of L(λ)⇔0
is an eigenvalue of rev(L(λ))⇔0 is an eigenvalue of L1. SinceM(τ1,σ1)(Y1,X1) is
invertible, we have Nr(L1) =Nr(MP
τM(σ2,τ2)(X2,Y2)). Note that the map
Nr(MP
τM(σ2,τ2)(X2,Y2))→ Nr(MP
τ), z/mapsto→(M(σ2,τ2)(X2,Y2))z,
25is an isomorphism. Deﬁne T(λ) :=λMP
τ−MP
σ. ThenNr(MP
τ) =Nr(rev(T(0))).
(a) Now, by Theorem 5.11, the map
Nr(rev(T(0)))−→ N r(rev(P(0))), u/mapsto→(eT
c−m(τ)+1⊗In)u,
is an isomorphism. Hence the map
Nr(rev(L(0)))−→ N r(rev(P(0))), x/mapsto→((eT
c−m(τ)+1⊗In)M(σ2,τ2)(X2,Y2))x,
is an isomorphism. Now by Lemma 5.13, we have ( eT
c−m(τ)+1⊗In)M(σ2,τ2)(X2,Y2) =
eT
c−m(τ,τ2)+1⊗In. Hence the result for the right eigenspace of P(λ) at∞follows.
The proof is similar for part (b).
(c)Next, weprovetheresultsforlefteigenspaceof P(λ)at∞. SinceM(σ2,τ2)(X2,Y2)
is invertible, we have Nl(rev(L(0)))=Nl(L1) =Nl(M(τ1,σ1)(Y1,X1)MP
τ). Hence it
followsthatthemap Nl(M(τ1,σ1)(Y1,X1)MP
τ)−→ N l(MP
τ), z/mapsto→(Mτ1,σ1(Y1,X1))Tz,
is an isomorphism. We have Nl(MP
τ) =Nl(rev(T(0))). Now, by Theorem 5.11, the
map
Nl(rev(T(0)))−→ N l(rev(P(0))), v/mapsto→(eT
i−m(τ)+1⊗In)v,
is an isomorphism. Hence the map
Nl(rev(L(0)))−→ N l(rev(P(0))), y/mapsto→((eT
i−m(τ)+1⊗In)(M(τ1,σ1)(Y1,X1))T)y,
is an isomorphism. Now by Lemma 5.13, we have M(τ1,σ1)(Y1,X1)(ei−m(τ)+1⊗In) =
ei−m(τ1,τ)+1⊗In. Hence the result for the left eigenspaces of P(λ) at∞follows.
The proof is similar for part (d).
The following example illustrates our recovery rule for eigenvectors ofP(λ) cor-
responding to an eigenvalue at ∞from those of the EGFPs of P(λ).
Example 5.15. LetP(λ) :=/summationtext5
i=0λiAi. Suppose that P(λ)is regular and ∞
is an eigenvalue of P(λ). Letσ:= (0,2),σ1:=∅=σ2,τ:= (−4,−5,−3,−1),
τ2= (−4)andτ1=∅. LetXbe any arbitrary matrix assignment for τ2. Then the
EGFPL(λ) =/parenleftbig
λMP
(−4,−5,−3,−1)−MP
(0,2)/parenrightbig
M−4(X) =:λL1−L0ofP(λ)is given by
L(λ) =λ
0 0In0 0
0A5A40 0
InX A 30 0
0 0 0 0 In
0 0 0 InA1
−
0In0 0 0
InX0 0 0
0 0−A2In0
0 0In0 0
0 0 0 0 −A0
.
Letxandy, respectively, be right and left eigenvectors of L(λ)corresponding to
the eigenvalue ∞. Deﬁnexi:= (eT
i⊗In)xandyi:= (eT
i⊗In)y,i= 1 : 5.We have
c−m(τ) =c−5(−4,−5,−3,−1)= 0<5andc−5(τ,τ2) =c−5(−4,−5,−3,−1,−4)= 1.
Hence by Theorem 5.14, (eT
c−m(τ,τ2)+1⊗In)x= (eT
2⊗In)x=x2is a right eigenvector of
P(λ)corresponding tothe eigenvalue ∞. Similarly, i−5(τ1,τ) =i−5(−4,−5,−3,−1)=
1.Hence by Theorem 5.14, (eT
i−m(τ1,τ)+1⊗In)y= (eT
2⊗In)y=y2is a left eigenvector
ofP(λ)corresponding to the eigenvalue ∞.
To verify the recovery rule, consider L1x= 0.This givesA5x2= 0andxi= 0
fori= 3,4,5.Now ifx2= 0thenx1= 0. Thusx2= 0⇒x= 0. Hencex2/\e}atio\slash= 0and
is a right eigenvector of P(λ)corresponding to the eigenvalue ∞.
Similarly,yTL1= 0impliesyT
2A5= 0andyT
i= 0fori= 3,4,5.Now ifyT
2= 0
thenyT
1= 0. ThusyT
2= 0⇒y= 0. Hencey2/\e}atio\slash= 0and is a left eigenvector of P(λ)
corresponding to the eigenvalue ∞./squaresolid
265.3. Recovery of eigenvectors, minimal bases and minimal in dices of
G(λ).We now describe the recovery of eigenvectors, minimal bases and m inimal
indices ofG(λ) from those of the EGFPs of G(λ) construed in Section 4. We proceed
as follows. Throughout of this section, we consider G(λ) =P(λ) +C(λE−A)−1B
andS(λ) as given in (2.2) and (2.3), respectfully.
Theorem 5.16. [18,52] (I) Suppose that G(λ)is singular. Let Z(λ) :=/bracketleftbiggZn(λ)
Zr(λ)/bracketrightbigg
be a matrix polynomial, where Zn(λ)hasnrows andZr(λ)hasrrows. IfZ(λ)is
a right (resp., left) minimal basis of S(λ)thenZn(λ)is a right (resp., left) minimal
basis ofG(λ).Further, the right (resp., left) minimal indices of G(λ)andS(λ)are
the same.
(II) Suppose that G(λ)is regular and µ∈Cis an eigenvalue of G(λ). LetZ:=/bracketleftbiggZn
Zr/bracketrightbigg
be an(n+r)×pmatrix such that rank(Z) =p,whereZnhasnrows and
Zrhasrrows. IfZis a basis of Nr(S(µ))(resp.,Nl(S(µ))) thenZnis a basis of
Nr(G(µ))(resp.,Nl(G(µ))).
Thus, in view of Theorem 5.16, we only need to describe the recovery of eigenvec-
tors, minimal bases and minimal indices of S(λ) from those of the EGFPs of G(λ).
To that end, we need the following result.
Theorem 5.17. [19] Letαbe a permutation of {0 :m−1}andT(λ) :=/bracketleftBigg
T(λ)em−i0(α)⊗C
eT
m−c0(α)⊗BA−λE/bracketrightBigg
be the Fiedler pencil of G(λ)associated with α, where
T(λ) :=λMP
−m−MP
αis the Fiedler pencil of P(λ). Then we have the following:
(I) Minimal bases . Suppose that S(λ)is singular. Then the maps
F:Nr(T)→ Nr(S),/bracketleftbiggu(λ)
v(λ)/bracketrightbigg
/mapsto→/bracketleftbigg(eT
m−c0(α)⊗In)u(λ)
v(λ)/bracketrightbigg
,
H:Nl(T)→ Nl(S),/bracketleftbiggu(λ)
v(λ)/bracketrightbigg
/mapsto→/bracketleftbigg(eT
m−i0(α)⊗In)u(λ)
v(λ)/bracketrightbigg
,
are linear isomorphisms, where u(λ)∈C(λ)mnandv(λ)∈C(λ)r.Further, F(resp.,
H) maps a minimal basis of Nr(T)(resp.,Nl(T)) to a minimal basis of Nr(S)(resp.,
Nl(S)).
Further, if ε1≤ ··· ≤εpare the right (resp., left) minimal indices of T(λ)then
ε1−i(α)≤ ··· ≤εp−i(α)(resp.,ε1−c(α)≤ ··· ≤εp−c(α)) are the right (resp.,
left) minimal indices of S(λ), wherec(α)andi(α)be the total number of consecutions
and inversions of the permutation α, respectively.
(II) Eigenvectors. Suppose that S(λ)is regular and µ∈Cis an eigenvalue of
S(λ). LetZ:=/bracketleftbigg
Zmn
Zr/bracketrightbigg
be an(mn+r)×pmatrix such that rank(Z) =p,whereZmn
hasmnrows andZrhasrrows. IfZis a basis of Nr(T(µ))(resp.,Nl(T(µ))) then/bracketleftBigg
(eT
m−c0(α)⊗In)Zmn
Zr/bracketrightBigg
(resp.,/bracketleftBigg
(eT
m−i0(α)⊗In)Zmn
Zr/bracketrightBigg
) is a basis of Nr(S(µ))(resp.,
Nl(S(µ))).
We now describe the recovery of eigenvectors, minimal bases and m inimal indices
ofS(λ) from those of the EGFPs of S(λ).
Theorem 5.18. LetL(λ) :=/bracketleftBigg
L(λ)em−i0(σ1,σ)⊗C
eT
m−c0(σ,σ2)⊗BA−λE/bracketrightBigg
be an EGFP
27G(λ)as given in (4.1). Let Z(λ) :=/bracketleftbigg
Zmn(λ)
Zr(λ)/bracketrightbigg
be an(mn+r)×pmatrix polynomial,
whereZmn(λ)hasmnrows andZr(λ)hasrrows. Then we have the following.
(a) IfZ(λ)is a right (resp., left) minimal basis of L(λ)then/bracketleftBigg
(eT
m−c0(σ,σ2)⊗In)Zmn(λ)
Zr(λ)/bracketrightBigg
(resp.,/bracketleftBigg
(eT
m−i0(σ1,σ)⊗In)Zmn(λ)
Zr(λ)/bracketrightBigg
) is a right (resp., left) minimal basis of S(λ).
(b) Letτbe given by τ:= (−β,−m,−γ).Setα:= (rev(β),σ,rev(γ)). Letc(α)
andi(α)be the total number of consecutions and inversions of the per mutationα. If
ε1≤ ··· ≤εpare the right (resp., left) minimal indices of L(λ)thenε1−i(α)≤ ··· ≤
εp−i(α)(resp.,ε1−c(α)≤ ··· ≤εp−c(α)) are the right (resp., left) minimal indices
ofS(λ).
Proof. It is given that L(λ) is the EGFP of P(λ) associated with σ,τ,σi, i= 1,2,
andτi, i= 1,2. Letτ= (−β,−m,−γ) for some permutations βandγ. Deﬁne
α:= (rev(β),σ,rev(γ)). Thenαis a permutation of {0 :m−1}andT(λ) :=/bracketleftBigg
T(λ)em−i0(α)⊗C
eT
m−c0(α)⊗BA−λE/bracketrightBigg
is an FP of G(λ), whereT(λ) :=λMP
−m−MP
αis a
FP ofP(λ). From the proof of Theorem 4.6, recall that diag( A,Ir)T(λ)diag(B,Ir) =
L(λ),whereA:=M(τ1,σ1)(Y1,X1)MP
−βandB:=MP
−γM(σ2,τ2)(X2,Y2). Since V:=
diag(B,Ir) is a nonsingular matrix, it is easily seen that the map V:Nr(L)→
Nr(T), z(λ)/mapsto→Vz(λ),is an isomorphism and maps a minimal basis of Nr(L) to
a minimal basis of Nr(T). On the other hand, by Theorem 5.17, F:Nr(T)→
Nr(S),/bracketleftbiggx(λ)
y(λ)/bracketrightbigg
/mapsto→/bracketleftbigg(eT
m−c0(α)⊗In)x(λ)
y(λ)/bracketrightbigg
,is an isomorphism and maps a minimal
basis ofNr(T) to a minimal basis of Nr(S), wherex(λ)∈C(λ)mnandy(λ)∈C(λ)r.
Consequently, FV:Nr(L)→ Nr(S), z(λ)/mapsto→ FVz(λ),is an isomorphism and maps
a minimal basis of Nr(L) to a minimal basis of Nr(S). Note that in the proof of
Theorem 4.6 it is shown that ( eT
m−c0(α)⊗In)B=eT
m−c0(σ,σ2)⊗In. Consequently, we
haveFV=/bracketleftbiggeT
m−c0(σ,σ2)⊗In
Ir/bracketrightbigg
,and hence the desired result for the recovery of
right minimal bases follows.
An analogous proof holds for describing the recovery of left minimal bases.
Finally, let ε1≤ ··· ≤εpbe the right (resp., left) minimal indices of L(λ). Since
the pencil T(λ) is strictly equivalent to L(λ),ε1≤ ··· ≤εpare also the right (resp.,
left) minimal indices of T(λ). Hence by Theorem 5.17, ε1−i(α)≤ ··· ≤εp−i(α)
(resp.,ε1−c(α)≤ ··· ≤εp−c(α)) are the right (resp., left) minimal indices of S(λ).
This completes the proof.
The next result describes the recovery of eigenvectors of S(λ) from those of the
EGFPs of S(λ) whenS(λ) is regular.
Theorem 5.19. LetL(λ) :=/bracketleftBigg
L(λ)em−i0(σ1,σ)⊗C
eT
m−c0(σ,σ2)⊗BA−λE/bracketrightBigg
be an EGFP
S(λ)given in (4.1). Suppose that S(λ)is regular and µ∈Cis an eigenvalue of
S(λ). LetZ:=/bracketleftbiggZmn
Zr/bracketrightbigg
be an(mn+r)×pmatrix such that rank(Z) =p,where
Zmnhasmnrows andZrhasrrows. IfZis a basis of Nr(L(µ))(resp.,Nl(L(µ)))
then/bracketleftBigg
(eT
m−c0(σ,σ2)⊗In)Zmn
Zr/bracketrightBigg
(resp.,/bracketleftBigg
(eT
m−i0(σ1,σ)⊗In)Zmn
Zr/bracketrightBigg
) is a basis of Nr(S(µ))
28(resp.,Nl(S(µ))).
Proof. A verbatim proof of Theorem 5.18 together with part (II) of Theo rem 5.17
yields the desired results.
Appendix A. Proofs of Lemmas 5.4, 5.5 and 5.6. The following result
will be used frequently in this section.
Proposition A.1. Letαbe an index tuple containing indices from {0 :m−1}
such thatαsatisﬁes the SIP. Let Zbe an arbitrary matrix assignment for α. Let
0≤s≤m−1. Suppose that s+1∈α. Then we have the following.
(a) If the subtuple of αwith indices {s,s+ 1}starts with s+ 1, then(eT
m−s⊗
In)Mα(Z) =eT
m−(s+1+cs+1(α))⊗In.
(b) If the subtuple of αwith indices {s,s+1}ends withs+1, thenMα(Z)(em−s⊗
In) =em−(s+1+is+1(α))⊗In.
Proof. (a) Setp:=cs+1(α), i.e.,αhaspconsecutions at s+1. Since αsatisﬁes
the SIP, by Proposition 2.11,
α∼/parenleftbig
αL,s+1,s+2,...,s+p+1,αR/parenrightbig
,
wheres+1/∈αLands+p+1,s+p+2/∈αR. Further, since the subtuple of αwith
indices{s,s+1}starts with s+1, we have s /∈αL. We denote by ( ∗) any arbitrary
matrix assignment. Then we have
(eT
m−s⊗In)MαL(∗)M(s+1,s+2,...,s+p+1)(∗)MαR(∗)
= (eT
m−s⊗In)M(s+1,s+2,...,s+p+1)(∗)MαR(∗) by (3.2) sinces,s+1/∈αL
= (eT
m−(s+p+1)⊗In)MαR(∗) by repeatedly applying (3 .2)
=eT
m−(s+p+1)⊗Inby (3.2) sinces+p+1,s+p+2/∈αR.
Thus, in particular, we have ( eT
m−s⊗In)Mα(Z) =eT
m−(s+p+1)⊗In.Sincep=cs+1(α),
the desired result follows.
(b) Setq:=is+1(α), i.e.,αhasqinversions at s+1. Sinceαsatisﬁes the SIP, by
Proposition 2.11,
α∼/parenleftbig
αL,s+q+1,...,s+2,s+1,αR/parenrightbig
,
wheres+1/∈αRands+q+1,s+q+2/∈αL. Further, since the subtuple of αwith
indices{s,s+1}ends withs+1, we have s /∈αR. Now we have
MαL(∗)M(s+1+q,...,s+2,s+1)(∗)MαR(∗)(em−s⊗In)
=MαL(∗)M(s+1+q,...,s+2,s+1)(∗)(em−s⊗In) by (3.3) sinces,s+1/∈αR
=MαL(∗)(em−(s+q+1)⊗In) by repeatedly applying (3 .3)
=em−(s+q+1)⊗Inby (3.3) ass+q+1,s+q+2/∈αL.
Thus, in particular, we have Mα(Z)(em−s⊗In) =em−(s+q+1)⊗In.Sinceq=is+1(α),
the desired result follows.
Analogous to Proposition A.1, we have the following results for negat ive index
tuples.
Proposition A.2. Letαbe an index tuple containing indices from {−m:−1}
such thatαsatisﬁes the SIP. Let Zbe an arbitrary matrix assignment for α. Let
291≤s≤m−1. Suppose that −s∈α. If the subtuple of αwith indices {−s,−(s+1)}
starts with −s, then(eT
m−s⊗In)Mα(Z) =eT
m−(s−c−s(α)−1)⊗In.
Similarly, if the subtuple of αwith indices {−s,−(s+ 1)}ends with −s, then
Mα(Z)(em−s⊗In) =em−(s−i−s(α)−1)⊗In.
A.1. The proof of Lemma 5.4.
Proof. (a)Firstweprovethat( eT
m−c0(σ)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−c0(σ,σ2)⊗In.
Sinceσhasc0(σ) (≤m−1) consecutions at 0, we have σ∼(σL,0,1,...,c 0(σ)),
where either m∈σLorm∈ −τ. If−m∈τthen the desired result follows from
Lemma 4.4. But there is nothing special about m∈σ. Ifm∈σthen by deﬁning
/tildewideσ:=σ\{m}and/tildewideτ:= (τ,−m), we havec0(σ) =c0(/tildewideσ) andc0(σ,σ2) =c0(/tildewideσ,σ2). Now
by Lemma 4.3, ( eT
m−c0(/tildewideσ)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−c0(/tildewideσ,σ2)⊗In. Consequently,
(eT
m−c0(σ)⊗In)Mσ2(X2)Mτ2(Y2) = (eT
m−c0(/tildewideσ)⊗In)Mσ2(X2)Mτ2(Y2) =eT
m−c0(/tildewideσ,σ2)⊗
In=eT
m−c0(σ,σ2)⊗In.
Further, by similar arguments as above, we have Mτ1(Y1)Mσ1(X1)(em−i0(σ)⊗
In) =em−i0(σ1,σ)⊗In. This completes the proof of (a).
(b) Note that if k /∈ {±(m−1),±m}, then (eT
1⊗In)Mk(Z) =eT
1⊗Infor any
matrixZ∈Cn×n. Hence (eT
1⊗In)Mσ2(X2) =eT
1⊗InandMσ1(X1)(e1⊗In) =e1⊗In
asσj,j= 1,2,contains indices from {0 :m−2}. Further, since τ1=∅=τ2, we
haveMτ1(Y1) =Imn=Mτ2(Y2). Consequently, we have ( eT
1⊗In)Mσ2(X2)Mτ2(Y2) =
eT
1⊗InandMτ1(Y1)Mσ1(X1)(e1⊗In) =e1⊗In. This completes the proof of (b).
A.2. The proof of Lemma 5.5.
Proof. (a) Given that i0(ω) +1∈σands:=i0(ω) +ci0(ω)+1(σ) + 1< m. Set
q:=ci0(ω)+1(σ). Thens=i0(ω) + 1 +q. Note that 0 ,1,...,i0(ω)/∈σ. Sinceσ
is a sub-permutation and has qconsecutions at i0(ω) + 1, we have σ∼(/hatwideσ,i0(ω) +
1,i0(ω)+2,...,i0(ω)+1+q), that is,
σ∼(/hatwideσ,i0(ω)+1,i0(ω)+2,...,s). (A.1)
Suppose that s+ 1/∈σ2. Thens /∈σ2, since otherwise ( σ,σ2) would not satisfy
the SIP which would contradict the condition that ( σ1,σ,σ2) satisﬁes the SIP. Thus
s,s+ 1/∈σ2. Hence by (3.2), we have ( eT
m−s⊗In)Mσ2(X2)Mτ2(Y2) = (eT
m−s⊗
In)Mτ2(Y2) =eT
m−s⊗In, where the last equality follows from (3.4) since s∈σ
implies that −s,−(s+ 1)/∈τ2.Now, since s+ 1/∈σ2, it is clear from (A.1) that
ci0(ω)+1(σ,σ2) =ci0(ω)+1(σ). This shows that p=swhich yields the desired result.
Next, suppose that s+1∈σ2. Setr:=cs+1(σ2), i.e.,σ2hasrconsecutions at
s+1. Then by Proposition 2.11, we have
σ2∼/parenleftbig
σL
2,s+1,s+2,...,s+1+r,σR
2/parenrightbig
(A.2)
for some index tuples σL
2andσR
2, wheres+ 1/∈σL
2ands+ 1+r,s+ 2 +r /∈σR
2.
Since (σ,σ2) satisﬁes the SIP and s+ 1/∈σL
2, it is clear from (A.1) and (A.2) that
s /∈σL
2.So the subtuple of σ2with indices {s,s+ 1}starts with s+ 1. Hence by
Proposition A.1, ( eT
m−s⊗In)Mσ2(X2) =eT
m−(s+1+r)⊗In.Now sinces+1+r∈σ2,
we haves+ 1 +r,s+ 2 +r∈σ. This implies s+ 1 +r,s+ 2 +r /∈τ2. Thus
by (3.4), (eT
m−(s+1+r)⊗In)Mτ2(Y2) =eT
m−(s+1+r)⊗In.Now, by (A.1) and (A.2),
ci0(ω)+1(σ,σ2) =s+r−i0(ω).Consequently, s+r+1 =pwhich proves (a).
(b)Sinceσ2containsindicesfrom {0 :m−2},by(3.2),wehave( eT
1⊗In)Mσ2(X2)Mτ2(Y2)
= (eT
1⊗In)Mτ2(Y2).Asi0(ω)+1∈σands=m, we havem∈σ. Thus−m /∈τand
hence−m /∈τ2.
30Suppose that −(m−1)/∈τ2. As−m,−(m−1)/∈τ2, by (3.4), we have ( eT
1⊗
In)Mτ2(Y2) =eT
1⊗In.Hencethedesiredresultfollowsfromthefactthat −(m−1)/∈τ2
impliesc−(m−1)(τ2) =−1. Next, supposethat −(m−1)∈τ2. Setq:=c−(m−1)(τ2).As
−m /∈τ2, the subtuple of τ2with indices {−m,−(m−1)}starts with −(m−1).Hence
byPropositionA.2, ( eT
1⊗In)Mτ2(Y2) = (eT
m−(m−1)⊗In)Mτ2(Y2) =eT
m−(m−q−2)⊗In=
eT
q+2⊗Inwhich proves (b).
(c) Leti0(ω)<mandi0(ω)+1/∈σ, i.e.,−i0(ω),−(i0(ω)+1)∈τ.This implies
thati0(ω),i0(ω)+1/∈σ2. Thus by (3.2), we have
(eT
m−i0(ω)⊗In)Mσ2(X2)Mτ2(Y2) = (eT
m−i0(ω)⊗In)Mτ2(Y2).
Sinceωisasub-permutationandhas i0(ω)inversionsat0,wehave ω∼(i0(ω),...,1,0,/hatwideω),
wherei0(ω)+1∈/hatwideω. Consequently, we have (recall that τ=−ω)
τ∼/parenleftBig
−i0(ω),...,−1,−0,/hatwideτ/parenrightBig
,where−(i0(ω)+1)∈/hatwideτ. (A.3)
Suppose that −i0(ω)/∈τ2. Then−(i0(ω)+1)/∈τ2, since otherwise ( τ,τ2) would not
satisfy the SIP which would contradict the condition that ( τ1,τ,τ2) satisﬁes the SIP.
As−i0(ω),−(i0(ω)+1)/∈τ2, by(3.4), wehave( eT
m−i0(ω)⊗In)Mτ2(Y2) =eT
m−i0(ω)⊗In.
Again, since −i0(ω)/∈τ2, we havec−i0(ω)(τ2) =−1 which gives (5.1).
Next, suppose that −i0(ω)∈τ2. Setq:=c−i0(ω)(τ2).Then by Proposition 2.12,
τ2∼/parenleftBig
τL
2,−i0(ω),−(i0(ω)−1),...,−(i0(ω)−q),τR
2/parenrightBig
(A.4)
forsomeindex tuples τL
2andτR
2such that −i0(ω)/∈τL
2and−(i0(ω)−q),−(i0(ω)−q−
1)/∈τR
2. As (τ,τ2) satisﬁes the SIP and −i0(ω)/∈τL
2, it is clear from (A.3) and (A.4)
that−(i0(ω)+1)/∈τL
2. Thus the subtuple of τ2with indices from {−i0(ω),−(i0(ω)+
1)}starts with −i0(ω).Hence by Proposition A.2, we have
(eT
m−i0(ω)⊗In)Mτ2(Y2) =eT
m−(i0(ω)−q−1)⊗In,
which gives (5.1) as q=c−i0(ω)(τ2).This completes the proof of (c).
(d) Let 0 ∈ωandi0(ω) =m. Thenσ=∅=σ2. HenceMσ2(X2) =Imn.
Further, since τ2contains indices from {−m:−2}, we have −0,−1/∈τ2. Thus
(eT
m⊗In)Mτ2(Y2) =eT
m⊗Inwhich proves (d).
A.3. The proof of Lemma 5.6.
The proof of Lemma 5.6 is analogous to the proof of Lemma 5.5.
Appendix B. Proofs of Propositions 3.11, 3.12, 3.13 and 3.14 .
B.1. The proof of Proposition 3.11.
Proof. Itiseasilyseenthattheelementarymatrices Mj(W),j= 0 :m−1,satisﬁes
(eT
m−i⊗In)Mj(W) = (eT
m−i⊗W)+(eT
m−(i−1)⊗In) forj=iandi= 1 :m−1,and
(eT
m⊗In)M0(W) =eT
m⊗W.Consequently, from (3.2) and (3.3) we have the following
(eT
m−i⊗In)Mj(W) =

eT
m−(i+1)⊗In forj=i+1 andi= 0 :m−2,
(eT
m−i⊗W)+(eT
m−(i−1)⊗In) forj=iandi= 1 :m−1,
eT
m⊗W forj=i= 0,
eT
m−i⊗In/braceleftbiggotherwise, i.e., when
j /∈ {i,i+1}fori= 0 :m−1.
(B.1)
31It is given that ck(α)≤1 andik(α)≤1 for 1≤k≤m−1. This implies c0(α)≤2
andi0(α)≤2. Recall from the deﬁnition of consecutions and inversions of any in dex
tupleβat any index tthat ift /∈βthenct(β) =−1 =it(β). Consequently, we have
/braceleftbigg−1≤ck(α)≤1 and−1≤ik(α)≤1 fork= 1 :m−1,
−1≤c0(α)≤2 and−1≤i0(α)≤2.(B.2)
Consider 0 ≤k≤m−1.We now prove (3.6). There are two cases.
Case-I: Suppose that the subtuple of αwith indices {k,k+1}starts with k+1.
Then by Proposition A.1, we have ( eT
m−k⊗In)Mα(X) =eT
m−(k+1+ck+1(α))⊗In.Since
−1≤ck+1(α)≤1, we have (3.6).
Case-II: Suppose that the subtuple of αwith indices {k,k+1}starts with k. Let
the row standard form of αbe given by rsf(α) := (β,rev(ak:k),γ) =
/parenleftbig
rev(a0: 0),rev(a1: 1),...,rev(ak:k),rev(ak+1:k+1),...,rev(am−1:m−1)/parenrightbig
.
(B.3)
Then we must have rev(ak:k)/\e}atio\slash=∅, since otherwise it is clear from (B.3) that the
subtuple of αwith indices {k,k+1}would start with k+1 which would contradict
our assumption of Case-II. Since αis a tuple containing nonnegative indices, by using
(B.2) it is clear from (B.3) that aj≥j−1 for allj={0 :m−1}\{2}anda2≥0.
Further, since rev(ak:k)/\e}atio\slash=∅,wehaveak∈ {k,k−1}ifk/\e}atio\slash= 2,andak∈ {k,k−1,k−2}
ifk= 2.
Sinceα∼rsf(α), without loss of generality, we assume that α=rsf(α). That
is,α:= (β,rev(ak:k),γ). LetYandZ, respectively, be the corresponding matrix
assignments for βandγassociated with X. We denote by Xp,p∈ {ak:k}, the
matrices associated with Xand assigned to the index p∈ {ak:k}. That is, X=/parenleftbig
Y,rev(Xak···Xk),Z/parenrightbig
. Now we have
(eT
m−k⊗In)Mα(X) = (eT
m−k⊗In)Mβ(Y)Mrev(ak:k)(rev(Xak···Xk))Mγ(Z)
= (eT
m−k⊗In)Mrev(ak:k)(rev(Xak···Xk))Mγ(Z) by (B.1) ask,k+1/∈β
=

/parenleftBig
(eT
m−(ak−1)⊗In)+/summationtextk
j=ak(eT
m−j⊗Xj)/parenrightBig
Mγ(Z) ifak>0,
/parenleftBig/summationtextk
j=ak(eT
m−j⊗Xj)/parenrightBig
Mγ(Z) if ak= 0(B.4)
by applying ( B.1) repeatedly .
DeﬁneS:={ak−1 :k}ifak>0 andS:={ak:k}ifak= 0. It is clear from (B.4)
that for evaluating ( eT
m−k⊗In)Mα(X) we need to calculate ( eT
m−ℓ⊗In)Mγ(Z) for
allℓ∈S. Sinceγ:=/parenleftbig
rev(ak+1:k+1),...,rev(am−1:m−1)/parenrightbig
, it is clear that, for
ℓ∈S, we have either ℓ /∈γor the subtuple of γwith indices {ℓ,ℓ+ 1}starts with
ℓ+1. By Proposition A.1, we have ( eT
m−ℓ⊗In)Mγ(Z) =eT
m−(ℓ+1+cℓ+1(γ))⊗In.Hence
from (B.4), we have ( eT
m−k⊗In)Mα(X) =
/braceleftBigg
(eT
m−(ak+cak(γ))⊗In)+/summationtextk
j=ak(eT
m−(j+1+cj+1(γ))⊗Xj) ifak>0,
/summationtextk
j=ak(eT
m−(j+1+cj+1(γ))⊗Xj) if ak= 0.(B.5)
Sinceγis a subtuple of α, we havect(γ)≤ct(α) for any index t. Hence −1≤
cℓ+1(γ)≤1 for allℓ∈S. Further, since ak∈ {k,k−1}ifk/\e}atio\slash= 2, andak∈
{k,k−1,k−2}ifk= 2, it is clear from (B.5) that (3.6) holds. Hence Mα(X) is a
block penta-diagonal matrix. This completes the proof.
32B.2. The proof of Proposition 3.12.
Proof. It is easily seen that the elementary matrices M−j(W),j= 0 :m, satisﬁes
(eT
m−(m−1)⊗In)M−m(W) =eT
1⊗Wand
(eT
m−i⊗In)M−j(W) = (eT
m−i⊗W)+(eT
m−(i+1)⊗In) forj=i+1,andi= 0 :m−2.
Consequently, from (3.4) and (3.5) we have the following.
(eT
m−i⊗In)M−j(W) =

(eT
m−i⊗W)+(eT
m−(i+1)⊗In) forj=i+1,andi= 0 :m−2
eT
1⊗W forj=i+1, i=m−1
eT
m−(i−1)⊗In forj=i,andi= 1 :m−1
eT
m−i⊗In/braceleftbiggotherwise, i.e., when
j /∈ {i,i+1}fori= 0 :m−1.
(B.6)
The rest of the proof is similar to Proposition 3.11.
B.3. The proof of Proposition 3.13.
Proof. Suppose that cj(α)≥2 for somej≥1. Let 1≤k≤m−1 be the smallest
integer belongs to αsuch thatck(α)≥cℓ(α) for any index ℓ∈α(i.e.,ck(α)≥2).
Then by Proposition2.11, we have α∼(αL,k,k+1,...,k+ck(α),αR), wherek /∈αL.
Now we have the following cases.
(a) Assume that k >1. Then 1 /∈αL, since otherwise k−1≥1 andck−1(α)>
ck(α) which is a contradiction. This implies that the subtuple of αwith indices
{k−1,k}starts with k. Hence by Proposition A.1, we have ( eT
m−(k−1)⊗In)Mα(X) =
eT
m−(k+ck(α))⊗In. That is,/parenleftbig
m−(k+ck(α))/parenrightbig
-th block entry of ( m−(k−1))-th block
row isIn. Sinceck(α)≥2, we have k+ck(α)>k+1 = (k−1)+2. Hence Mα(X)
is not a block penta-diagonal matrix.
(b) Assume that k= 1. Then ( αL,(1 : 1+c1(α)),αR), wherec1(α)≥2. If 0/∈αL
then the subtuple of αwith indices {0,1}starts with 1. Hence by Proposition A.1, we
have (eT
m⊗In)Mα(X) =eT
m−(1+c1(α))⊗In. Sincec1(α)≥2, we have 1+ c1(α)≥3.
HenceMα(X) is not a block penta-diagonal matrix.
LetX0be the matrix assigned to the index 0 ∈αLassociated with X, and let
X= (XL,XM,XR), where XL,XM,andXRare the matrix assignments associated
withαL,(1 : 1+c1(α)) andαR, respectively. Then ( eT
m⊗In)MαL(XL) =eT
m⊗X0by
(B.1) as 0 ∈αLand 1/∈αL. Now the subtuple of β:= (1 : 1+c1(α),αR) with indices
{0,1}startswith 1. HencebyPropositionA.1, wehave( eT
m⊗)Mβ(∗) =eT
m−(1+c1(α))⊗
In, where ( ∗) denotes any arbitrary matrix assignment. Hence ( eT
m⊗In)Mα(X) =
eT
m−(1+c1(α))⊗X0. Sincec1(α)≥2, we have 1+ c1(α)≥3. This shows that Mα(X)
is not a block penta-diagonal matrix.
Similarly, if ij(α)≥2, for some j≥1, then it can be shown that Mα(X) is not a
block penta-diagonal matrix.
B.4. The proof of the Proposition 3.14.
The proof of Proposition 3.14 is similar to the proof of Proposition 3.13 .
Appendix C. Proof of Proposition 3.6.
First, we present some technical results which will be used to prove that EGFPs
are operation free pencils. Recall from (B.1) and (B.6) that the Fied ler matrices MP
j,
33j∈ {−m:m−1}, satisﬁes the following:
(eT
m−i⊗In)MP
j=

eT
m−(i+1)⊗In ifj=i+1, i= 0 :m−2
(eT
m−i⊗(−Ai))+(eT
m−(i−1)⊗In) ifj=i, i= 1 :m−1
eT
m⊗(−A0) if j=i= 0
eT
m−i⊗Inotherwise, i.e., when
j /∈ {i,i+1}, i= 0 :m−1,
(C.1)
(eT
m−i⊗In)MP
−j=

(eT
m−i⊗Ai+1)+(eT
m−(i+1)⊗In) ifj=i+1, i= 0 :m−2
eT
1⊗Am ifj=i+1, i=m−1
eT
m−(i−1)⊗In ifj=iandi= 1 :m−1
eT
m−i⊗Inotherwise, i.e., when
j /∈ {i,i+1}, i= 0 :m−1.
(C.2)
The followingresult follows fromPropositionA.1. We denote by ( ∗) anyarbitrary
matrix assignment.
Remark C.1. Letαbe an index tuple containing indices from {0 :m−1}such
thatαsatisﬁes the SIP. Let 0≤s≤m−1. Suppose that s,s+1/∈αor the subtuple
ofαwith indices {s,s+ 1}starts with s+ 1. Then it follows from Proposition A.1
that(eT
m−s⊗In)Mα(∗) =eT
m−k⊗In, wherek=s+1+cs+1(α).
The following result follows from Proposition A.2.
Remark C.2. Letβbe an index tuple containing indices from {−m:−1}such
thatβsatisﬁes the SIP. Let 1≤s≤m−1. Suppose that −s,−(s+ 1)/∈βor
the subtuple of βwith indices {−s,−(s+ 1)}starts with −s. Then it follows from
Proposition A.2 that (eT
m−s⊗In)Mα=eT
m−t⊗In, wheret=s−1−c−s(α).
Proposition C.3. Letαbe an index tuple containing indices from {0 :m−1}
such thatαsatisﬁes the SIP. Let 0≤s≤m−1. Suppose that the subtuple of αwith
indices{s+1,s+2}starts with s+2. Letα= (β,s+2,γ), wheres+1,s+2/∈β.
Further, suppose that the subtuple of γwith indices {s,s+1}starts with s+1. Then
(eT
m−(s+1)⊗In)Mα(∗)/\e}atio\slash= (eT
m−s⊗In)Mγ(∗). Moreover, (eT
m−(s+1)⊗In)Mα(∗) =
eT
m−k⊗Inand(eT
m−s⊗In)Mγ(∗) =eT
m−ℓ⊗Inwithk>ℓ, wherek=s+2+cs+2(α)
andℓ=s+1+cs+1(γ).
Proof. Since the subtuple of αwith indices {s+ 1,s+ 2}starts with s+ 2,
by Remark C.1, we have ( eT
m−(s+1)⊗In)Mα(∗) =eT
m−k⊗In, wherek=s+ 2 +
cs+2(α). Similarly, since the subtuple of γwith indices {s,s+1}starts with s+1, by
Remark C.1, we have ( eT
m−s⊗In)Mγ(∗) =eT
m−ℓ⊗In, whereℓ=s+1+cs+1(γ). Now
letcs+1(γ) =p, i.e., (s+1,s+2,...,s+p+1)is a subtuple of γ. Sinceα= (β,s+2,γ)
satisﬁes the SIP, it follows that ( s+2,...,s+p+1,s+p+2) must be a subtuple of
α. Hencecs+2(α)≥pandk=s+2+cs+2(α)≥s+p+2>s+p+1=ℓwhich gives
the desired result.
The following result is analogous to Proposition C.3.
Proposition C.4. Letαbe an index tuple containing indices from {−m:−1}
such thatαsatisﬁes the SIP. Suppose that the subtuple of αwith indices {−s,−(s+
1)}starts with −s. Letα= (β,−s,γ), where−s,−(s+ 1)/∈β. Further, suppose
that the subtuple of γwith indices {−(s+ 1),−(s+ 2)}starts with −(s+ 1). Then
(eT
m−s⊗In)Mα(∗)/\e}atio\slash= (eT
m−(s+1)⊗In)Mγ(∗). Moreover, (eT
m−s⊗In)Mα(∗) =eT
m−k⊗In
and(eT
m−(s+1)⊗In)Mγ(∗) =eT
m−ℓ⊗Inwithk < ℓ, wherek=s−1−c−s(α)and
ℓ=s−c−(s+1)(γ).
34Fora,b,q∈Z, we denote {a:qb}:={a,a+q,a+2q,...,b}. The following facts
will be used to prove that EGFPs are operation free pencils.
Remark C.5. Let1≤h1≤h2< h2+ 1≤h3≤m−1. Letαbe an index
tuple containing indices from {−h1:−1−h2}∪{h2+1 :h3}. Letβ(resp.,γ) be the
subtuple of αwith indices {−h1:−1−h2}(resp.,{h2+1 :h3}). Suppose that βand
γsatisfy the SIP. Then from (C.1) and (C.2) we have the followi ng.
•Leth1−1≤j≤h2−1. If the subtuple of αwith indices {−j,−(j+ 1)}
starts with −jthen the indices h2+1 :h3inαare redundant for evaluating
(eT
m−j⊗In)MP
α, i.e.,(eT
m−j⊗In)MP
α= (eT
m−j⊗In)MP
β.
•If the the subtuple of αwith indices {−h2,h2+ 1}starts with −h2then
similarly as above the indices h2+ 1 :h3inαare redundant for evaluat-
ing(eT
m−h2⊗In)MP
α. On the other hand, if the subtuple of αwith indices
{−h2,h2+1}starts with h2+1then the indices −h1:−1−h2are redundant
for evaluating (eT
m−h2⊗In)MP
α, i.e.,(eT
m−h2⊗In)MP
α= (eT
m−h2⊗In)MP
γ.
•Leth2+1≤j≤h3. If the subtuple of αwith indices {j,j+1}starts withj+1
then the indices −h1:−1−h2are redundant for evaluating (eT
m−j⊗In)MP
α,
i.e,(eT
m−j⊗In)MP
α= (eT
m−j⊗In)MP
γ.
The following result will be useful for proving that EGFPs are operat ion free.
Lemma C.6. Let1≤h1≤h2< h2+1≤h3≤m−1. Letαbe an index tuple
containing indices from {−h1:−1−h2}∪{h2+1 :h3}. Deﬁneβ:=the subtuple of
αwith indices {−h1:−1−h2}andγbe the subtuple of αwith indices {h2+1 :h3}.
Suppose that βandγsatisﬁes the SIP. Then Mα(∗)is operation free.
Proof. We will prove that MP
αis operation free. The proof depends only on the
indices ofαand does not depend on the matrix assignments for α. HenceMα(∗) is
operation free.
For proving MP
αis operation free it is equivalent to show that ( eT
m−j⊗In)MP
α
is operation free for all j= 0 :m−1. It follows from (C.1) and (C.2) that ( eT
m−j⊗
In)MP
α=eT
m−j⊗Infor allj∈ {0 :h1−2}∪{h3+1 :m−1}. Hence it remains to
show that ( eT
m−j⊗In)MP
αis operation free for all j=h1−1 :h3. We proceed as
follows.
(a) Leth1−1≤j≤h2−1. We prove that ( eT
m−j⊗In)MP
αis operation free.
Case-I:Suppose that −j,−(j+1)/∈αor the subtuple of αwith indices {−j,−(j+
1)}starts with −j. Then by Remark C.2 and Remark C.5, we have ( eT
m−j⊗In)MP
α=
eT
m−k⊗In, wherek=j−1−c−j(α). Hence (eT
m−j⊗In)MP
αis operation free.
Case-II: Suppose that the subtuple of αwith indices {−j,−(j+1)}starts with
−(j+1). The following steps show that ( eT
m−j⊗In)MP
αis operation free.
Step-1: Let α= (δj,−(j+1),αj), where −j,−(j+1)/∈δj. Then we have
(eT
m−j⊗In)MP
α= (eT
m−j⊗In)MP
(−(j+1),αj)by (C.1) and (C.2) since −j,−(j+1)/∈δj
=/parenleftBig
(eT
m−j⊗Aj+1)+(eT
m−(j+1)⊗In)/parenrightBig
MP
αjby (C.2)
= (eT
m−j⊗Aj+1)MP
αj+(eT
m−(j+1)⊗In)MP
αj. (C.3)
Now, since α= (δj,−(j+1),αj) satisﬁes the SIP, we have either −j,−(j+1)/∈αjor
the subtuple of αjwith indices {−j,−(j+1)}starts with −j. Then by Remark C.2
and Remark C.5, we have ( eT
m−j⊗In)MP
αj=eT
m−kj⊗In, wherekj:=j−1−c−j(αj).
Hence from (C.3) we have
(eT
m−j⊗In)MP
α= (eT
m−kj⊗Aj+1)+(eT
m−(j+1)⊗In)MP
αj. (C.4)
35The evaluation of ( eT
m−j⊗In)MP
αis completed if any one of the following cases hold.
Otherwise, we move to Step-2.
•Suppose that −(j+ 1),−(j+ 2)/∈αjor the subtuple of αjwith indices
{−(j+1),−(j+2)}startswith −(j+1). ThenbyRemarkC.2andRemarkC.5
we have (eT
m−(j+1)⊗In)MP
αj=eT
m−kj+1⊗In, wherekj+1:=j−c−(j+1)(αj).
It follows by Proposition C.4 that kj< kj+1. Hence it follows from (C.4)
that (eT
m−j⊗In)MP
αis operation free.
•Supposethat j+1 =h2. Ifthe subtuple of αjwith indices {−h2,h2+1}starts
with−h2then by similar argument as above it follows that ( eT
m−j⊗In)MP
α
is operation free. On the other hand, if the subtuple of subtuple of αjwith
indices{−h2,h2+1}starts with h2+1 then by Remark C.1 and Remark C.5,
we have (eT
m−h2⊗In)MP
αj=eT
m−ℓ⊗In, whereℓ:=h2+1+ch2+1(αj). Hence
it follows from (C.4) that ( eT
m−j⊗In)MP
αis operation free.
Step-2: Suppose that the subtuple of αjwith indices {−(j+1),−(j+2)}starts
with−(j+2). Then repeat Step-1 with αreplaced by αjandjreplaced by j+1.
Since{h1−1 :h2−1}containss:=h2−h1+1 number of indices, we must stop
beforesnumbers of steps. This completes the proof of ( eT
m−j⊗In)MP
αis operation
free.
(b) Letj=h2. If the subtuple of αwith indices {−h2,h2+1}starts with h2+1
then by Remark C.1 and Remark C.5, we have ( eT
m−h2⊗In)MP
α=eT
m−ℓ⊗In, where
ℓ:=h2+1+ch2+1(α). Hence (eT
m−h2⊗In)MP
αis operation free. On the other hand if
the the subtuple of αwith indices {−h2,h2+1}starts with −h2then by Remark C.2
and Remark C.5, we have ( eT
m−h2⊗In)MP
α=eT
m−ℓ⊗In, whereℓ:=h2−1−c−h2(α).
Hence (eT
m−h2⊗In)MP
αis operation free.
(c) Leth2+1≤j≤h3. We prove that ( eT
m−j⊗In)MP
αis operation free.
Case-I: Suppose that j,j+ 1/∈αor the subtuple of αwith indices {j,j+ 1}
starts with j+1. Then by Remark C.1 and Remark C.5, we have ( eT
m−j⊗In)Mα=
(eT
m−j⊗In)Mγ=eT
m−k⊗Inwhich is operation free, where k=j+1+cj+1(γ).
Case-II: Suppose that the subtuple of αwith indices {j,j+1}starts with j. Then
the following steps show that ( eT
m−j⊗In)Mαis operation free.
Step-1: Let α= (ξj,j,αj), wherej,j+1/∈ξj. Then we have
(eT
m−j⊗In)MP
α= (eT
m−j⊗In)MP
(j,αj)by (C.1) and (C.2) sincej,j+1/∈ξj
=/parenleftBig
(eT
m−j⊗(−Aj))+(eT
m−(j−1)⊗In)/parenrightBig
MP
αjby (C.1)
= (eT
m−j⊗(−Aj))MP
αj+(eT
m−(j−1)⊗In)MP
αj (C.5)
Sinceα= (ξj,j,αj) satisﬁes the SIP, we have either j,j+1/∈αjor the subtuple of
αjwith indices {j,j+1}starts with j+1. Hence by Remark C.1 and Remark C.5,
we have (eT
m−j⊗In)MP
αj=eT
m−kj⊗In, wherekj:=j+ 1 +cj+1(αj). Hence from
(C.5) we have
(eT
m−j⊗In)MP
α= (eT
m−kj⊗(−Aj))+(eT
m−(j−1)⊗In)MP
αj.(C.6)
The evaluation of ( eT
m−j⊗In)MP
αis completed if any one of the following cases hold.
Otherwise, we move to Step-2.
•Suppose that j,j−1/∈αjor the subtuple of αjwith indices {j,j−1}starts
withj. Then by Remark C.1 and RemarkC.5, we have ( eT
m−(j−1)⊗In)MP
αj=
36eT
m−kj−1⊗In,wherekj−1:=j+cj(αj). ByPropositionC.3wehave kj>kj−1.
Hence it follows from (C.6) that ( eT
m−j⊗In)MP
αis operation free.
•Suppose that j−1 =h2. If the subtuple of αjwith indices {−h2,h2+ 1}
starts with h2+1 then by similar arguments as above it follows that it follows
that (eT
m−j⊗In)MP
αis operation free. On the other hand, if the subtuple of
subtuple of αjwith indices {−h2,h2+1}startswith −h2then by RemarkC.2
and Remark C.5, we have ( eT
m−h2⊗In)MP
αj=eT
m−ℓ⊗In, whereℓ:=h2−1+
c−h2(αj). Hence it follows from (C.6) that ( eT
m−j⊗In)MP
αis operation free.
Step-2: Suppose that the subtuple of αjwith indices {j,j−1}starts with j−1.
Then repeat Step-1 with αreplaced by αjandjreplaced by j−1.
Since{h1+1 :h3}containss:=h3−h2number of indices, we must stop before
snumbers of steps. This prove that ( eT
m−j⊗In)MP
αis operation free.
C.1. Proof of Proposition 3.6.
Proof. Recall that ( σ,ω) is a permutation of {0 :m}andτ=−ω. SinceM−0(Z)
is a block diagonal matrix and −0,−1/∈τ1∪τ2(i.e.,−0 and−1 do not repeat),
without loss of generality we assume that 0 ∈σ. Similarly, since Mm(Z) is a block
diagonal matrix and m−1,m /∈σj,j= 1,2, (i.e.,m−1 andmdo not repeat),
without loss of generality we assume that m∈ −τ.
Now, since ( σ,−τ) is a permutation of {0 :m}with 0∈σandm∈ −τ, there exist
0≤h1<h2<···<hk−1<hk≤m−1 (with odd k) such that σis a permutation
of{0 :h1}∪{h2+1 :h3}∪···∪{hk−1+1 :hk}and−τis a permutation of {h1+1 :
h2}∪{h3+1 :h4}∪···∪{hk+1 :m}. This implies that σ1andσ2containindices from
{0 :h1−1}∪{h2+1 :h3−1}∪···∪{hk−1+1 :hk−1}since (σ1,σ,σ2) satisﬁes the SIP,
and−τ1and−τ2contain indices from {h1+2 :h2}∪{h3+2 :h4}∪···∪{hk+2 :m}
since (τ1,τ,τ2) satisﬁes the SIP.
Setα:= (τ1,σ1,σ,σ2,τ2). We now show that Mα(∗) is operation free (i.e., L0,
given in Proposition 3.6, is operation free). From the above paragra ph, it is clear that
αcontains indices from {0 :h1}∪{−(h1+2) :−1−h2,h2+1 :h3}∪{−(h3+2) :−1
−h4,h4+ 1 :h5} ∪ ··· ∪ {− (hk−2+ 2) : −1−hk−1,hk−1+ 1 :hk} ∪ {hk+ 2 :m}
=:H1∪H3∪H5∪··· ∪Hk∪Hk+1. Note that, for indices i∈Hsandj∈Ht, we
have||i| − |j|| ≥2 ifs/\e}atio\slash=t. HenceMα(∗) is operation free if MαHj(∗) is operation
free for all j= 1,3,...,k+1, where αHjis the subtuple of αwith indices from Hj.
Sinceτ1∪τ2(resp., (σ1,σ,σ2)) does not contains any index of H1(resp.,Hk+1), we
haveMαH1(∗) (resp.,MαHk+1(∗)) is operation free. Further, by Lemma C.6, we have
MαHj(∗) is operation free for all j= 3,5,...,k. HenceMα(∗) is operation free, i.e.,
L0is operation free.
Similar proof for Mβ(∗) is operation free, where β:= (τ1,σ1,τ,σ2,τ2), i.e.,L1,
given in Proposition 3.6, is operation free. Hence the EGFP L(λ) is operation free.
REFERENCES
[1]B. Adhikari and R. Alam, On backward errors of structured polynomial eigenproblems solved
by structure preserving linearizations, Linear Algebra. Appl., 434 (2011), pp.1989-201.
[2]B. Adhikari, R. Alam and D. Kressner, Structured eigenvalue condition numbers and lin-
earizations for matrix polynomials, Linear Algebra Appl., 435 (2011), pp.2193-2221.
[3]R. Alam and N. Behera ,Linearizations for rational matrix functions and Rosenbro ck system
polynomials , SIAM J. Matrix Anal. Appl., 37 (2016), pp.354-380.
[4]R. Alam and N. Behera ,Generalized Fiedler pencils for rational matrix functions ,SIAM J.
Matrix Anal. Appl., 39 (2018), pp.587-610.
37[5]R. Alam, S. Bora, M. Karow, V. Mehrmann and J. Moro, Perturbation theory for Hamil-
tonian matrices and the distance to bounded realness, SIAM Journal Matrix Anal. Appl.,
32 (2011), pp.484-514.
[6]A. Amparan, F. M. Dopico, S. Marcaida, and I. Zaballa ,Strong linearizations of rational
matrices , SIAM J. Matrix Anal. Appl., 39 (2018), pp.1670-1700.
[7]E. N. Antoniou and S. Vologiannidis, A new family of companion forms of polynomial
matrices , Electron. J. Linear Algebra, 11 (2004), pp.78-87.
[8]T. Apel, V. Mehrmann, and D. Watkins, Structured eigenvalue methods for the computation
of corner singularities in 3D anisotropic elastic structur es,Comput. Methods Appl. Mech.
Engrg., 191 (2002), pp.4459-4473.
[9]S. Bora, M. Karow, C. Mehl, and P. Sharma, Structured eigenvalue backward errors of
matrix pencils and polynomials with Hermitian and related s tructures, SIAM J. Matrix
Anal. Appl., 35 (2014), pp.453-475.
[10]S. Bora, M. Karow, C. Mehl, and P. Sharma, Structured eigenvalue backward errors of
matrix pencils and polynomials with palindromic structure s,SIAM J. Matrix Anal. Appl.,
36 (2015), pp.393-416.
[11]M. I. Bueno, K. Curlett and S. Furtado ,Structured strong linearizations from Fiedler
pencils with repetition I, Linear Algebra Appl., 460 (2014), pp.51-80.
[12]M. I. Bueno and F. De Ter ´an,Eigenvectors and minimal bases for some families of Fiedler -
like linearizations, Linear Multilinear Algebra, 62 (2014), pp.39-62.
[13]M. I. Bueno, F. De Ter ´an and, F. M. Dopico ,Recovery of eigenvectors and minimal bases of
matrix polynomials from generalized Fiedler linearizatio ns,SIAM J. Matrix Anal. Appl.,
32 (2011), pp.463-483.
[14]M. I. Bueno, F. M. Dopico and, S. Furtado ,Linearizations of Hermitian matrix polynomials
preserving the sign characteristic, SIAM J. Matrix Anal. Appl., 38 (2017), pp.249-272.
[15]M. I. Bueno, F. M. Dopico, S. Furtado, and M. Rychnovsky ,Large vector spaces of block-
symmetric strong linearizations of matrix polynomials , Linear Algebra Appl., 477 (2015),
pp.165-210.
[16]M. I. Bueno and S. Furtado ,Structured strong linearizations from Fiedler pencils wit h rep-
etition II, Linear Algebra Appl., 463 (2014), pp.282-321.
[17]R. K. Das ,Strong Linearizations of Polynomial and Rational Matrices and Recovery of Spectral
Data, Ph.D. Thesis, Indian Institute of Technology Guwahati, In dia, (2019).
[18]R. K. Das and R. Alam ,Aﬃne spaces of strong linearizations for rational matrices and the
recovery of eigenvectors and minimal bases , Linear Algebra Appl., 569 (2019), pp. 335–368.
[19]R. K. Das and R. Alam ,Recovery of minimal bases and minimal indices of rational ma trices
from Fiedler-like pencils , Linear Algebra Appl., 566 (2019), pp. 34–60.
[20]R. K. Das and R. Alam ,Automatic recovery of eigenvectors and minimal bases of mat rix
polynomials from generalized Fiedler pencils with repetit ion, Linear Algebra Appl., 569
(2019), pp. 78–112.
[21]R. K. Das and R. Alam ,Structured strong linearizations of structured rational m atrices,
Linear and Multilinear Algebra, 70 (2022), pp. 6018-6051.
[22]R. K. Das and R. Alam ,Palindromic Linearizations of Palindromic Matrix Polynom ials of
Odd Degree Obtained from Fiedler-Like Pencils , Vietnam J. Math. 48 (2020), pp. 865-891.
[23]F. De Ter ´an, F. M. Dopico, and D. S. Mackey ,Fiedler companion linearizations and the
recovery of minimal indices, SIAM J. Matrix Anal. Appl., 31 (2010), pp.2181-2204.
[24]F. De Ter ´an, F. M. Dopico, and D. S. Mackey ,Palindromic companion forms for matrix
polynomials of odd degree , J. Comput. Appl. Math., 236 (2011), pp. 1464-1480.
[25]F. De Ter ´an, F. M. Dopico, and D. S. Mackey, Spectral equivalence of matrix polynomials
and the index sum theorem, Linear Algebra Appl., 459 (2014), pp.264-333.
[26]F. De Ter ´an, C. Hernando and J. Perez ,Structured strong ℓ-iﬁcations for structured matrix
polynomials in the monomial basis , Electron. J. Linear Algebra, 37 (2021), pp.35-71.
[27]F. M. Dopico, P. W. Lawrence, J. P ´erez, and P. Van Dooren ,Block Kronecker lin-
earizations of matrix polynomials and their backward error s, Numer. Math., 140 (2018),
pp. 373-426 .
[28]F. M. Dopico, S. Marcaida, and M. C. Quintana ,Strong linearizations of rational matrices
with polynomial part expressed in an orthogonal basis , Linear Algebra Appl., 570 (2019),
pp. 1–45.
[29]F. M. Dopico, S. Marcaida, M. C. Quintana, and P. Van Dooren, Local linearizations
of rational matrices with application to rational approxim ations of nonlinear eigenvalue
problems , Linear Algebra and its Applications, 604 (2020), pp. 441-4 75.
[30]F. M. Dopico, J. Perez, and P. Van Dooren, Structured backward error analysis of linearized
structured polynomial eigenvalue problems, Math. Comp., 88 (2019), pp.1189-1228.
38[31]F. M. Dopico, M. C. Quintana, and P. Van Dooren ,Structural backward stability in rational
eigenvalue problems solved via block Kronecker linearizat ions, Calcolo, 60, 7 (2023).
[32]M. Fiedler, A note on companion matrices, Linear Algebra Appl., 372 (2003), pp.325-331.
[33]G. D. Forney, Jr. Minimal bases of rational vector spaces, with applications to multivariable
linear systems, SIAM J. Control., 13 (1975), pp.493-520.
[34]I. Gohberg, P. Lancaster and L. Rodman, Matrix Polynomials, Academic Press Inc., New
York London, 1982.
[35]D.J. Higham and N.J. Higham, Structured backward error and condition of generalized eig en-
value problems, SIAM J. Matrix Anal. Appl., 20(1998), pp.493-512.
[36]N. J. Higham, D. S. Mackey, N. Mackey and F. Tisseur, Symmetric linearizations for
matrix polynomials , SIAM J. Matrix Analysis Appl., 29 (2006), pp.143-159.
[37]A. Hilliges, C. Mehl, and V. Mehrmann, On the solution of palindromic eigenvalue prob-
lems,In Proceedings of the 4th Europian Congress on Computationa l Methods in Applied
Science and Engineering (ECCOMAS), (2004), Jyv¨ askyl¨ a, F inland.
[38]T. M. Huang, W. W. Ling, and J. Qiang, Structure preserving algorithms for palindromic
quadratic eigenvalue problems arising vibration of fast tr ains,SIAM J. Matrix. Anal.
Appl., 30 (2009), 1566 - 1592.
[39]T. Kailath, Linear Systems, Prentice-Hall Inc., Englewood Cliﬀs, N.J., 1980.
[40]D. Kressner, C. Schr ¨oder, and D. S. Watkins, Implicit QR algorithms for palindromic and
even eigenvalue problems , Numer. Algorithms, 51 (2009), pp.209-238
[41]D.S. Mackey, N. Mackey, C. Mehl, and V. Mehrmann, Structured polynomial eigenvalue
problems: Good vibrations from good linearizations, SIAM J. Matrix Anal. Appl., 28
(2006), pp.1029-1051.
[42]D. S. Mackey, N. Mackey, C. Mehl and V. Mehrmann, Vector spaces of linearizations for
matrix polynomials, SIAM J. Matrix Anal. Appl., 28 (2006), pp.971-1004.
[43]C. Mehl, V. Mehrmann, and P. Sharma, Structured eigenvalue/eigenvector backward errors
of matrix pencils arising in optimal control, Electron. J. Linear Algebra, 34 (2018), pp.526-
560.
[44]V. Mehrmann and H. Voss, Nonlinear eigenvalue problems: a challenge for modern eige n-
value methods, GAMM Mitt. Ges. Angew. Math. Mech., 27 (2004), pp. 121-152.
[45]V. Mehrmann and D. Watkins, Structure-preserving methods for computing eigenpairs of
large sparse skew-Hamiltonian/Hamiltonian pencils, SIAM J. Sci. Comput., 22 (2001),
pp.1905-1925.
[46]V. Mehrmann and D. Watkins, Polynomial eigenvalue problems with Hamiltonian structur e,
Electron. Trans. Numer. Anal., 13 (2002), pp.106-118.
[47]J. Planchard, Eigenfrequencies of a tube bundle placed in a conﬁned ﬂuid, Comput. Methods
Appl. Mech. Engrg., 30 (1982), pp. 75-93.
[48]H. H. Rosenbrock, State-space and Multivariable Theory, John Wiley & Sons, Inc., New
York, 1970.
[49]Y. Su and Z. Bai, Solving rational eigenvalue problems via linearization, SIAM J. Matrix
Anal. Appl., 32 (2011), pp.201-216.
[50]F. Tisseur and K. Meerbergen, The Quadratic Eigenvalue Problem , SIAM Review, 43
(2001), pp.235-286.
[51]A. I. G. Vardulakis, Linear Multivariable Control, John Wiley & Sons Ltd., 1991.
[52]G. Verghese, P. Van Dooren, and T. Kailath ,Properties of the system matrix of a gener-
alized state-space system , Internat. J. Control, 30 (1979), pp.235–243.
[53]S. Vologiannidis and E. N. Antoniou, A permuted factors approach for the linearization of
polynomial matrices, Math. Control Signals Systems, 22 (2011), pp.317-342.
[54]H. Voss, A rational spectral problem in ﬂuid-solid vibration, Electron. Trans. Numer. Anal.,
16 (2003), pp.93-105.
[55]H. Voss, Iterative projection methods for computing relevant energ y states of a quantum dot,
J. Comput. Phys., 217 (2006), pp.824-833.
39