arXiv:2301.10638v1  [cs.LG]  25 Jan 2023MLPGradientFlow:
going with the ﬂow of multilayer perceptrons
(and ﬁnding minima fast and accurately)
Johanni Brea, Flavio Martinelli, Berﬁn S ¸im¸ sek, Wulfram Gerstner
January 26, 2023
Abstract
MLPGradientFlow is a software package to solve numerically the grad ient ﬂow dif-
ferential equation ˙θ=−∇L(θ;D), where θare the parameters of a multi-layer percep-
tron,Dis some data set, and ∇Lis the gradient of a loss function. We show numer-
ically that adaptive ﬁrst- or higher-order integration methods bas ed on Runge-Kutta
schemes have better accuracy and convergence speed than gra dient descent with the
Adam optimizer. However, we ﬁnd Newton’s method and approximatio ns like BFGS
preferable to ﬁnd ﬁxed points (local and global minima of L) eﬃciently and accu-
rately. For small networks and data sets, gradients are usually co mputed faster than
in pytorch and Hessian are computed at least 5 ×faster. Additionally, the package fea-
tures an integrator for a teacher-student setup with bias-free , two-layer networks trained
with standard Gaussian input in the limit of inﬁnite data. The code is acc essible at
https://github.com/jbrea/MLPGradientFlow.jl .
We consider multi-layer perceptrons
fθ:RDin→RDout(1)
x∝mapsto→f(x) =fL
θL◦fL−1
θL−1◦···◦f1
θ1(x) (2)
fi
θi=(Wi,bi):RDi−1→RDi (3)
x∝mapsto→/braceleftbiggσi(Wix+bi) ifχi= true
σi(Wix) otherwise, (4)
with weights Wi∈RDi×Di−1, bias indicator χi∈ {true, false }, biasesbi∈RDiand element-
wise applied activation function σi:R→R,D0=Din,DL=Doutandθ= (θ1,...,θL) =
(W1,b1,...,WL,bL). With data D= ((x1,y1),(x2,y2),...,(xN,yN)) and single-sample loss
ℓ(fθ(xi),yi)∈R, we deﬁne the loss
L(θ;D) =N/summationdisplay
iℓ(fθ(xi),yi)+R(θ) (5)
with barrier function
R(θ) =/braceleftbigg0 if1
2∝ba∇dblθ∝ba∇dbl2
2≤c/parenleftbig1
2∝ba∇dblθ∝ba∇dbl2
2−c/parenrightbig2otherwise(6)
1ﬁnite data constructor Net(layers = ((D1,σ1,χ1),...,(DL,σL,χL)),
input = (x1,...,x N), target = (y1,...,y N))
inﬁnite data constructor NetI(θstudent,θteacher)
loss and gradient loss(net, θ),gradient(net, θ)
Hessian and its spectrum hessian(net, θ),hessian spectrum(net, θ)
solving Equation 7 train(net, θ)
Table 1: Most important functions of MLPGradientFlow.
wherec∈R+is some constant. The barrier function is included to guaran tee ﬁxed points
with ﬁnite norm of the parameter values, unless cis chosen to be inﬁnite. We deﬁne the
gradient ﬂow diﬀerential equation
˙θ(t) =−η∇L(θ(t);D), (7)
whereηis a constant and ∇Lthe gradient of the loss in Euclidean metric. We aim at ﬁnding
solutions
θ(T) =/integraldisplayT
0˙θ(t)dt (8)
of the gradient ﬂow diﬀerential equation (Equation 7) for ini tial conditions θ(0) =θ0.
If the target values are generated by a multi-layer perceptr on, i.e.yi=fθ∗(xi) with param-
etersθ∗, we speak of a (noise-free) teacher-student setup. Note tha t the teacher does not
need to have the same number of parameters as the student.
Results
MLPGradientFlow ( https://github.com/jbrea/MLPGradientFlow.jl )is written in Julia,
butitcanalsobeusedandwastested withaPythoninterface. Themost importantfunctions
are listed in Table 1.
All results where CPU time matters were obtained on a single t hread of an AMD Ryzen 7
PRO 5850U using Julia 1.8.5 on Linux. Diﬀerential equations a re integrated with methods
from the OrdinaryDiﬀEq.jl package [1]. To search ﬁxed points of the diﬀerential equation,
i.e. (local) minima of the loss, we use optimizers deﬁned in O ptim.jl [2] and NLopt.jl [3].
Gradient descent ( Descent) and Adam( Adam) arecopied from Flux.jl [4] anddeﬁneddirectly
in MLPGradientFlow.jl.
Adaptive Ordinary Diﬀerential Equation Solvers are prefer able over Gra-
dient Descent or Adam
To compare diﬀerent integration methods, we solved the gradi ent ﬂow diﬀerential equa-
tion Equation 7 with a reference algorithm at tolerance 10−6and kept the solution as
a reference trajectory ( θ∗(0),θ∗(t1),...,θ∗(T∗)) at 106logarithmically spaced time points
(t0= 0,...,t104=T∗). The reference algorithm was Rodas5 for the network with 33 pa-
rameters and CVODE BDF for the network with 601 parameters. Next, we ran diﬀerent
2name needs Hessian ode solver works with minibatches
Gradient Descent no yes (small step size) yes
Runge-Kutta 4 (RK4) no yes no
CVODE BDF no yes no
KenCarp58 yes yes no
Rodas5 yes yes no
Adam no no yes
(L)-BFGS no no no
SLSQP no no no
Newton(TrustRegion) yes no no
Table 2: Method properties. Diﬀerential equation solvers are listed on the upper half of
the table, ﬁxed point solvers on the lower half. There exist o nline (L)-BFGS methods (see
e.g.[5, 6]) that we did not consider here.
A1 B1
10−1210−510210−3103109
distance to reference dmprogress tm33 parameters
Rodas5
KenCarp58
RK4
CVODE BDF
Descent(0.01)
Descent(0.001)
Adam(0.001)
Adam(0.0001)10−1210−510210−3100103
distance to reference dmprogress tm601 parameters
A2 B2
10−1210−510210−1510−7101
distance to reference dmﬁnal loss
10−1210−510210−810−3102
distance to reference dmﬁnal loss
Figure 1: Eﬃciency and accuracy of diﬀerent approximations to the gradient ﬂow.
For given CPU time, higher-order methods Rodas5 and KenCarp 58 reach high accuracy (low
distance dmto gradient ﬂow) and integrate the diﬀerential equation to a l argetm(A1) and
a low loss (A2) for networks with 33 parameters. For larger ne tworks (601 parameters in
B1 and B2), the gap between ﬁrst order and higher order method s decreases because of the
long durations for computing Hessians, but the diﬀerential e quation solver CVODE BDF
still excels. We used 104samples in 2 input dimensions in these simulations.
integration methods mforaﬁxeddurationonasingleCPUthread andkept theirtraje ctories
(θm(0),θm(t1),...,θm(Tm)) at 103logarithmically spaced time points. For each method m,
we computed the distance to the reference target trajectory
3dm=1
103/summationtext103
i=1minj∝ba∇dblθ∗(tj)−θm(ti)∝ba∇dbl2
2, the progress tm= argmin t∝ba∇dblθ∗(t)−θm(Tm)∝ba∇dbl2
2and the
ﬁnal loss at θm(Tm). This was repeated for two diﬀerent network sizes, and multi ple random
seeds for the dataset and the initial condition. We ﬁnd for sm all networks that higher-order
methods Rodas5 and KenCarp58 are usually preferable over CV ODEBDF, Runge Kutta 4
(RK4), Euler integration (Gradient Descent) or Adam and for larger networks CVODE BDF
works best (Figure 1).
Finding Fixed Points
Finding ﬁxed points is surprisingly diﬃcult even for small n etworks and datasets. If we are
only interested in ﬁnding low loss values within a given time budget, (approximate) second
order optimizers like (L)BFGS, SLSQP or Newton’s method are usually better than dif-
ferential equation solvers, presumably because they do not spend time following accurately
the trajectory of the gradient ﬂow (Figure 2A&B). To ﬁnd ﬁxed points eﬃciently, we there-
fore suggest switching to (approximate) second-order opti mization after using diﬀerential
equation solvers for a given time or iteration limit.
We investigated convergence in a teacher-student setup, wh ere two-layer teachers ( L= 2)
are generated with independently and normally distributed parameters and the students
have at least as many neurons in layer 1 as the teachers. In thi s setting, the students can
theoretically reach zero loss. However, because we use doub le precision (64-bits) for all
ﬂoating point numbers, the round-oﬀ errors for the diﬀerence between target values yiand
predictions of the students fθ(xi) are on the order of 10−16. Hence, the mean squared error
is on the order of (10−16)2= 10−32for simulations that converge to a global minimum.
We observe, ﬁrst, that it is indeed possible to reach loss lev els on the order of 10−32with
KenCarp58, Newton’s method and SLSQP. BFGS usually stalls a t loss levels around 10−14,
unless the gradients are scaled with η≫1. However, even with rescaling BFGS usually does
not ﬁnd global minima as accurately as Newton’s method or SLS QP. Second, we ﬁnd that
it can take very long to converge, even with eﬃcient higher-o rder methods; almost 20% of
the simulations reported in Figure 2 did not fully converge w ithin 104steps of KenCarp58
and 30×104steps of Newton’s method. Third, it is diﬃcult to ﬁnd a reliab le convergence
criterion. Tiny gradient norms and smallest Hessian eigenv alues near zero do not imply
that a ﬁxed point is reached (see “non-converged” point that is probably on the path to a
global minimum in Figure 2C2). And even if we call a point “pro bably converged”, because
no decrease of the loss is found in subsequent epochs of 104steps of Newton optimization,
simulations may not have fully converged to a ﬁxed point (see “converged” point that is
probably on the path to a global minimum in Figure 2C2).
4A B
AdamLBFGSKenCarpNewtonBFGSSLSQP10−1810−1010−2
methodmean squared error33 parameters
AdamLBFGSKenCarpNewtonBFGSSLSQP10−410−2100
methodmean squared error1153 parameters
C1
10−3510−2910−2310−1710−1110−510110−1710−1110−5
mean squared error/ba∇dbl∇L/ba∇dbl∞−10−4< λmin<−10−8
−10−8≤λmin<0
0≤λmin<10−8
10−8≤λmin
not converged
C2
10−3510−2910−2310−1710−1110−510110−1710−1110−5
“non-converged”
global?“converged”
global?local
mean squared error/ba∇dbl∇L/ba∇dbl∞D0=D1∗= 2,D1= 2
D0=D1∗= 2,D1= 4
D0=D1∗= 2,D1= 8
D0=D1∗= 4,D1= 4
D0=D1∗= 4,D1= 8
D0=D1∗= 4,D1= 16
D0=D1∗= 8,D1= 8
D0=D1∗= 8,D1= 16
D0=D1∗= 8,D1= 32
not converged
Figure 2: Finding ﬁxed points eﬃciently. For a ﬁxed time budget, the approximate second
order method SLSQP ﬁnds typically the lowest loss value for small net works (A), whereas BFGS is
preferable for larger networks ( B). InCwe show simulation results, where we know that the smallest
loss is at 0. The starts of the gray lines in C1show the gradient norm and loss value after 104
steps of KenCarp58 and the points show the gradient norm, loss va lue and minimal eigenvalue of the
Hessian (in color code) after at most 30 epochs of 104additional steps of Newton’s method. We call a
simulation “probably converged”, if the loss found by Newton’s meth od stayed the same or increased
relative to the previous epoch. If this never happened within 30 epo chs, we call the simulation “not
converged” (marked with a black cross). In C2the show the same data with symbol- and color-code
indicating the number of input dimensions D0the number of hidden neurons of the teacher D1∗and
the number of hidden neurons of the student D1. Based on inspection of the parameters, we belief
that the points marked “global?” will converge to a global minimum, if g iven more compute time,
whereas the nearby point marked with “local” is probably stuck in a loc al minimum. Therefore, it is
unclear, if the points marked as “probably converged” (no black cr oss) at losses between 10−30and
10−16reached a local minimum or if they did not fully converge to a global minim um, despite our
stringent deﬁnition of convergence.
5Eﬃciency
A B
101102103104105100101102
batch sizespeedup relative to pytorchgradient
101102103104105100101102
batch sizespeedup relative to pytorchhessian
41 parameters 201 parameters 401 parameters
Figure 3: Speedup relative to pytorch. The speedup factor (duration MLPGradient-
Flow/duration pytorch) for diﬀerent network sizes is measur ed on a single CPU thread with
multiple repetitions of gradient and Hessian computations usingthe Python module“timeit”
and calling MLPGradientFlow from Python.
We use a custom implementation of gradient and Hessian compu tations, taking inspiration
fromBishop(1992) [7]andSimpleChains.jl( https://github.com/PumasAI/SimpleChains.jl ).
Relative to pytorch we observe a signiﬁcant speedup, in part icular for the computation of
Hessians (Figure 3).
Finite versus Inﬁnite Data
10310410510−310−210−1100101
∝1/N
number of samples Ndistance to inﬁnite data solution
Figure 4: Inﬁnite, standard Gaussian input data. The accuracy of the ﬁnite-data
solution improves with rate 1 /N. These results were obtained with 9 input dimensions.
6In a teacher-student setting with bias-free 2-layer networ ks and normally distributed input
data, the loss function can be written in terms of one- to thre e-dimensional Gaussian inte-
grals, independently of the input dimension [8]. For linear , relu and erf activation functions
these integrals can be evaluated analytically, otherwise t hey can be computed numerically
using cubature methods (we use Cuba.jl https://github.com/giordano/Cuba.jl ). How-
ever, accurately computing these integrals in every step of a diﬀerential equation solver is
time consuming. Therefore we ﬁtted 3-layer perceptrons wit h 96 neurons in each hidden
layer to datasets with approximately 105samples, such that they approximate the numerical
integrals with high accuracy (typically, loss and gradient s are within 10−5Euclidean dis-
tance from the numerical integrals on test data, and Hessian s are within 10−3distance). We
recommend using these approximations for the diﬀerential eq uation solvers and switching
to numerical integration, if ﬁne-tuning of the ﬁxed points i s desired. In contrast to existing
diﬀerential equation solvers for this setting (e.g. [8]), th is package supports more activation
functions and provides also Hessians, which enables the usa ge of higher-order solvers.
Asanalternativetonumericalintegrationorapproximatio nsthereof, onecansampledatasets
with normally distributed input. But the accuracy of the ﬁni te-data solution improves only
with a rate inversely proportional to the size of the dataset (Figure 4), whereas the com-
putational cost increases linearly with the size of the data set. In particular in high input
dimensions, the numerical integrals and its approximation s are preferable over approxima-
tions with ﬁnite data sets.
Conclusions
We described the software package MLPGradientFlow.jl, a to ol to integrate thegradient ﬂow
diﬀerential equation of the loss function of neural networks . We illustrated the advantages
and drawbacks of diﬀerent methods with toy examples. The pack age is mainly targeted at
small networks with a few thousand parameters, at most. For l arger networks, the time and
space requirements of computing and inverting Hessians bec omes impractical and Hessian-
free methods should be considered (e.g. [9] or integrators b ased on Krylov methods [1]). The
largest setting we considered so far is a network with 79’510 parameters and 70’000 samples
in 784 input dimensions, which we integrated with CVODE BDF (see “scripts/mnist.jl” in
https://github.com/jbrea/MLPGradientFlow.jl ).
References
[1] Christopher Rackauckas and Qing Nie. Diﬀerentialequati ons.jl – a performant and
feature-rich ecosystem for solving diﬀerential equations i n julia. The Journal of Open
Research Software , 5(1), 2017. Exported from https://app.dimensions.ai on 2 019/05/05.
[2] Patrick Kofod Mogensen and Asbjørn Nilsen Riseth. Optim : A mathematical optimiza-
tion package for Julia. Journal of Open Source Software , 3(24):615, 2018.
[3] Steven G. Johnson. The nlopt nonlinear-optimization pa ckage.
7[4] Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandh i, Marco Concetto Rudilosso,
Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. F ashionable modelling
with ﬂux. CoRR, abs/1811.01457, 2018.
[5] Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic
l-bfgs algorithm. In Arthur Gretton and Christian C. Robert , editors, Proceedings of
the 19th International Conference on Artiﬁcial Intelligence a nd Statistics , volume 51 of
Proceedings of Machine Learning Research , pages 249–258, Cadiz, Spain, 09–11 May
2016. PMLR.
[6] L´ eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimi zation methods for large-scale
machine learning. SIAM Review , 60(2):223–311, Jan 2018.
[7] Chris Bishop. Exact calculation of the hessian matrix fo r the multilayer perceptron.
Neural Computation , 4(4):494–501, Jul 1992.
[8] Sebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Kr zakala, and Lenka Zde-
borov´ a. Dynamics of stochastic gradient descent for two-l ayer neural networks in the
teacher-student setup. In H. Wallach, H. Larochelle, A. Bey gelzimer, F. d 'Alch´ e-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems ,
volume 32. Curran Associates, Inc., 2019.
[9] Barak A. Pearlmutter. Fast exact multiplication by the h essian.Neural Computation ,
6(1):147–160, Jan 1994.
8