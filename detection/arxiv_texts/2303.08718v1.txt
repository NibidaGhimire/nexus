arXiv:2303.08718v1  [math.ST]  18 Feb 2023MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV
MODELS: REDUCTION TO DIMENSION 2
SHULAN HU, XINYU WANG, AND LIMING WU
Abstract. In this paper, we introduce the maximum entropy estimator (MEE
in short) ˆθME
nbased on 2-dimensional empirical distribution of the observation
sequence ( y0,y1,···,yn) of a Hidden Markov Model (HMM in short), when the
sample size is big: in that case the maximum likelihood estimator (MLE) is t oo
consuming in time by the classical Baum-Welch EM algorithm. We prove t he
consistency and the asymptotic normality of ˆθME
nin a quite general framework,
where the asymptotic covariance matrix is explicitly estimated in term s of the 2-
dimensional Fisher information. To complement it we use also the 2-dim ensional
relativeentropyto study the hypothesestesting problem. Furth ermorewe propose
the gradient descent of 2-dimensional relative entropy (2RE) algo rithm for ﬁnding
ˆθME
n, which works for very big nand big number mof the hidden states. Some
numerical examples are furnished and commented for illustrating ou r theoratical
results.
1.Introduction
1.1.Background. A hidden Markov model (HMM) is a discrete-time bivariate
stochastic process {Zn= (Xn,Yn)}n≥0:
observed signals: Y0Y1··· Yn···
↑ ↑ ··· ↑ ···
unobserved states : X0→X1→ ··· → Xn···
where
•(Xn)n≥0is a Markov chain valued in M={1,···,m}(m≥2), with the
transition probability matrix P= (pij)i,j∈M(i.e.pij=P(Xn+1=j|Xn=i));
•GivenX[0,n]:= (X0,···,Xn) = (x0,···,xn) =:x[0,n],Y0,···,Ynare condi-
tionally independent, with values in some signal space S, and the conditional
law ofYnis given by
qβi=qβi(y)σ(dy), i=xn
where{qβ;β∈¯O}is a family of probability measures on Swith the pa-
rameterβvarying in the closure ¯Oof some open and bounded subset Oin
Rd,σ(dy) is some reference positive measure (e.g. the counting measure or
the Lebesgue measure according to Sis at most countable or the Euclidean
space).
The transition probabilities pijand the parameters βiin the signal distributions
depend on some unknown parameter θ= (θ1,···,θM), i.e.pij=pij(θ),βi=βi(θ),
whereθvaries in the closure Θ of some bounded and open subset Θ0ofRM. Often
pij,βiare all unknown, in that case θ= ((pij)i/negationslash=j,(βi)) hasM=m2−m+md
12 SHULAN HU, XINYU WANG, AND LIMING WU
unknown parameters. The objective for the statistical inferenc es of HMMs is to
estimate or to determine θthrough the observation sequence Y[0,n]=y[0,n].
The underlying process {Xn}is often referred to as the regime, the signals pro-
cess{Yn}as theobservation sequence . HMMs were introduced by Baum and Petrie
[2](1966). HMMs have been widely studied and used in statistics and info rmation
theory. HMMs are found of useful applications across a range of s cience and engi-
neering domains, including genomics, speech recognition, signal pro cessing, optical
character recognition, machine translation, computer vision, ﬁna nce and economics
etc. See the surveys by Rabiner [ 27](1989) and Ephraim [ 15](2002).
Baum and Petrie [ 2] and Petrie [ 26] studied statistical inference of ﬁnite-state
ﬁnite-signal HMMs by proving the identiﬁability of the HMM and the con sistency,
the asymptotic normality of the maximum likelihood estimator (MLE in sh ort)
θML
nof the unknown parameters θ. Baum, Petrie, Soules and Weiss [ 3](1970) in-
troduced the forward-backward algorithm for calculating the con ditional distribu-
tion ofXkknowing the observation sequence, and developed the so-called Ba um-
Welch EM (expectation-maximization) algorithm for ﬁnding the MLE. T hose two
algorithms, together with the Viterbi algorithm (Viterbi [ 37] (1967)) for ﬁnding
argmaxx[0,n]p(x[0,n]|y[0,n]), the decoding problem in information theory, constitute
the box of the three main computation tools for HMMs.
Generalizations to more general HMMs (with continuous signal or co ntinuous
state, or switching HMMs etc) or studies of new problems are realize d during the
last ﬁfty years: identiﬁability of a HMM ([ 6], [35], [16], [32], [7]), new ergodic the-
orems for relative entropy densities of HMMs ([ 22], [17], [12]), consistency and as-
ymptotic normality of the MLE ([ 4],[22],[28],[29],[5],[8],[12],[14],[1]), algorithms for
estimating the state, parameter, number of states ([ 10],[30],[33],[7],[25],[18],[23]), ex-
ponential forgetting of the predictor ([ 17]), large deviations ([ 20]) and concentration
inequalities ([ 19]) etc.
1.2.Motivation. The main diﬃculty for the statistical studies and applications of
HMMs is: the likelihood function of the observation sequence y[0,n]= (y0,···,yn)
pθ(y[0,n]) =/summationdisplay
x[0,n]∈Sn+1ν(x0)qβx0(y0)n/productdisplay
k=1pxk−1,xkqβxk(yk)
being a sum of mn+1-terms, is very diﬃcult to compute as function of θfor bign
(though,givenaﬁxed θ, thiscanbecalculatedbyBaum-Welchalgorithmin O(m2n)-
steps). We recall that the introduction of the predictor pk(·) :=Pθ,ν(Xk=·|y[0,k−1])
(and the associated recursion formula) has played a crucial role in t he theoretical
probabilistic study of HMMs ([ 22],[17],[20]), through the formula
pθ(y[0,n]) =n/productdisplay
k=0/parenleftBiggm/summationdisplay
i=1pk(i)qβi(yk)/parenrightBigg
.
The Baum-Welch EM algorithm is very consuming in time for large nand big
m: in each iteration, one requires O(m2n)-operations in the expectation step with-
out counting the maximization step (in the mixed Gaussian or Poissonia n signals
cases there is the very useful re-estimation explicit formula of Bau m). But when
nis not big enough, the problem of local minima arises. Even choosing nu merousMAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 3
diﬀerent initial points of θcould help ﬁnding the MLE, that does not work surly
mathematically and that will increase in many times the computation co st.
A ﬁrst concrete application of HMM with big n(n= 1000), up to our knowl-
edge, was carried out by Titsias, Holmes and Yau [ 36](2016): they proposed a new
algorithm based on the k-segment approximation method. Their method works
well when piiare close to 1: in that case the number of changes of states cx[0,n]=/summationtextn
k=11xk−1/negationslash=xkis not big (say ≤k), and
P(Y[0,n]=y[0,n],cX[0,n]≤k)
being close to the likelihood function pθ(y[0,n]), is a sum of at most/summationtextk
j=1Cj
n+1m2-
terms, much more easier to treat for not big k. They developed the algorithms
associated with this k-segment approximation. However their innovative method
losses its pertinence when the probability that cX[0,n]> kis not negligible for rela-
tively big k(i.e. when piiare not close to 1).
Our motivation is: when nis very large or mis big such as in DNA sequencing or
economics or ﬁnances and when the MLE becomes diﬃcult to compute , we should
ﬁnd some substituter of the MLE for the statistical inferences of HMMs.
1.3.Objective. As a substituter of the MLE, we propose the maximal entropy es-
timator (MEE in short, denoted by ˆθME
n) based only on the 2-dimensional empirical
distribution of the observation sequence
Ly,2
n=1
nn/summationdisplay
k=1δ(yk−1,yk),(δ·is the Dirac measure at the point ·) (1.1) Ly2
and an algorithm for computing ˆθME
n. Our method works for large observation
dataset (big m,n) and for several statistical purposes such as parameter estima -
tion or hypotheses testing, by showing the identiﬁability, the consis tency and the
asymptotic normality of ˆθME
nor convergence in law of the 2-dimensional relative
entropy.
Ourstartingpointisaverynaivefeeling: as LY,2
nconvergesinlawtothestationary
2-dimensional distribution QY,2
θof (Y0,Y1), it is stable (varying few randomly) and
robust (depending few on the possible errors in the observations ( yk−1,yk) for some
k), unlike the very random sequence Y[0,n]. Moreover LY,2
nwould be a suﬃcient
statistic if ( Yn) were Markov (though it is NOT). If QY,2
θdetermines uniquely θ,LY,2
n
would become an asymptotically suﬃcient statistic. That will allow us to reduce
the statistical problems of sample size n+ 1 of HMMs to dimension 2. The main
objective of this paper is to rend the above naive intuition rigorous a nd useful for
statistical inferences.
1.4.Organization. Inthenextsection2, weshowtheﬁrstcrucialtheoreticalresult
which says that the 2-dimensional stationary distribution QY,2
θdetermines uniquely
all unknown parameters in θ(the so called identiﬁability), as for stationary Markov
chains (whereas ( Yn) is not Markov). That justiﬁes rigorously our naive intuition
above: as LY,2
n→QY,2
θ,LY,2
ndistinguishes or determines uniquely θifnisbig enough,
i.e. it is an asymptotically suﬃcient statistic. The MEE ˆθME
nandthe associated gra-
dient descent algorithm for HMMs are presented in Section 3. We pro ve the strong4 SHULAN HU, XINYU WANG, AND LIMING WU
consistency and the asymptotic normality of the MEE, and provide t he explicit ex-
pression of the asymptotic covariance matrix based on the 2-dimen sional Fisher’s
information in Section 4. The hypthesis testing results based on the 2-dimensional
relative entropy, including type I error and type II error estimate s, are given in Sec-
tion 5. In Section 6 we discuss HMMs with signals of mixture of exponen tial model,
covering the usual Gaussian, Poisson cases. In Section 7, we furn ish numerical sim-
ulations and statistical analysis of several examples for illustrating the usefulness of
MEE and and the numerical validity of the 2RE algorithm.
2.Assumptions and the identifiability
2.1.Notations. At ﬁrst the signal space Sis either at most countable or the Eu-
clidean space Rlequipped with the discrete metric d(y,y′) = 1y/negationslash=y′or the Euclidean
metricd(y,y′) =|y−y′|, and the associated Borel σ-ﬁeldS. On the space M1(S)
of probability measures on ( S,S), besides the weak convergence topology, we recall
the total variational metric between µ,ν∈ M1(S)
/ba∇dblν−µ/ba∇dbltv= sup
A∈S|ν(A)−µ(A)|.
Its probabilistic meaning is
/ba∇dblν−µ/ba∇dbltv= inf
X,YP(X/ne}ationslash=Y)
where the inﬁmum is taken over all couples of random variables X,Yso that the
law ofX(resp. Y) is µ(resp.ν) (a such couple ( X,Y) is called a coupling of (µ,ν)).
Givenθ∈Θ and an initial distribution νof the hidden Markov chain, we denote
byPθ,νthe probability measure on (Ω ,F) under which ( Zn= (Xn,Yn))n≥0is the
HMM with all parameters given in the Introduction, so that the law of X0isν.
2.2.Assumptions. Throughout the paper we assume that for our HMM,
(H0)For the vector of the unknown parameters θ= (θ1,···,θM),
•θvaries in the closure Θof some bounded and open subset Θ0ofRM.
•For alli∈M,θ∈Θ,βi(θ)∈¯O, the closure of some open and bounded subset
OofRd.
•The mapping θ→((pij(θ))i,j∈M,(βi(θ))i∈¯O)is continuous and injective on
Θ.
•β→qβis a continuous mapping from ΘtoM1(S)equipped with the weak
convergence topology.
Our next assumption is about the ergodicity and the aperiodicity of t he hidden
Markov chain ( Xk)k≥0.
(H1)There are n0∈N∗,κ >0and a probability measure ν0onMcharging all
states of Msuch that for any θ∈Θ, the transition probability matrix Pθ= (pij(θ))
satisﬁes : Pn0
θ(i,j)≥κν0(j)for alli,j∈M.
For the signal distributions qβi(θ), 1≤i≤m, we assume
(H2) (the hidden states are ordered by (βi=βi(θ)))For any re-ordering
τ:M−→M(bijection), if
(βτ(1),···,βτ(m)) = (β1,···,βm),MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 5
thenτ(i) =i,∀i∈M. In other words the hidden states are ordered by
the parameters (β1,···,βm). This implies that βi/ne}ationslash=βjfor diﬀerent hidden
statesi,j.
(H3) (the identiﬁability of the hidden states) Forβi,˜βi∈¯O,i= 1,···,m,
if
m/summationdisplay
i=1ciqβi=m/summationdisplay
i=1˜ciq˜βi
whereci,˜ci≥0and/summationtext
ici=/summationtext
i˜ci= 1, then/summationtext
iciδβi=/summationtext
i˜ciδ˜βi.
Remark 2.1. The reader is referred to the known works [ 6], [35], [16], [32], [7] (and
the references therein) for HMMs satisfying the identiﬁability (H3)of the hidden
states.
Those four assumptions will be assumed throughout the paper.
2.3.2-dimensional contiguous empirical distribution. Under(H1), (Xk) has
a unique invariant probability measure µθonM, i.e.µθPθ=µθwhenµθis identiﬁed
as the line-vector ( µθ(i))1≤i≤m, andµθ(i)≥κν0(i)>0 for each i∈M. Moreover
applying the classic Doeblin’s theorem (which is a quantitative reﬁneme nt of the
famous Perron-Frobenius theorem) to the Markov chain Zn= (Xn,Yn), we have for
any initial distribution νofX0and for every measurable subset AofM×S,
|Pθ,ν((Xn,Yn)∈A)−m/summationdisplay
i=1µθ(i)qβi{s;(i,s)∈A}| ≤(1−κ)[n/n0],∀n≥0.(2.1)a21
Inparticular as ngoestoinﬁnity, the2-dimensional (contiguous) empirical measure s
LY,2
n=1
nn/summationdisplay
k=1δ(Yk−1,Yk) (2.2)
converges Pθ,ν-a.s. intheweakconvergencetopology,tothestationary2-dimen sional
(contiguous) observation distribution QY,2
θdetermined by
QY,2
θ(A0×A1) =/summationdisplay
i,j∈Mµθ(i)pij(θ)qβi(θ)(A0)qβj(θ)(A1), A0,A1∈ S.(2.3)
If we could prove that QY,2
θdetermines uniquely θ,LY,2
nwould become an asymptot-
ically suﬃcient statistic of θ. That is the purpose of the following
prop: iden Proposition 2.1 (Identiﬁability) .Under(H0), (H1), (H2) and (H3) , the un-
known parameter θis identiﬁable via the 2-dimensional distribution QY,2
θ, i.e. for
θ0,θ1∈Θ, ifQY,2
θ0=QY,2
θ1, thenθ0=θ1.
Furthermore if QY,2
θn→QY,2
θ, thenθn→θ.
Proof.At ﬁrst the 1-dimensional stationary distribution
QY,1
θ(dy) =Pθ,µθ(Y0∈dy) =/summationdisplay
i∈Mµθ(i)qβi(θ)(dy)6 SHULAN HU, XINYU WANG, AND LIMING WU
is ﬁnite mixture of {qβ;β∈¯O}. Givenθ0,θ1∈Θ, ifQY,2
θ0=QY,2
θ1, thenQY,1
θ0=QY,1
θ1.
By the identiﬁability of hidden states in (H3),
m/summationdisplay
i=1µθ0(i)δβi(θ0)=m/summationdisplay
i=1µθ1(i)δβi(θ1).
Therefore βi(θ0) =βi(θ1) for alliby(H2)and then µθ0=µθ1. Below we can write
βi=βi(θk) andµθ=µθkfork= 0,1.
We turn now to the identiﬁcation of the transition probabilities pij. Since
QY,2
θ0=/summationdisplay
i,j∈Mµθ(i)pij(θ0)qβi⊗qβj=QY,2
θ1=/summationdisplay
i,j∈Mµθ(i)pij(θ1)qβi⊗qβj
we have any A0∈ S,
m/summationdisplay
j=1/parenleftBiggm/summationdisplay
i=1µθ(i)pij(θ0)qβi(A0)/parenrightBigg
qβj=m/summationdisplay
j=1/parenleftBiggm/summationdisplay
i=1µθ(i)pij(θ1)qβi(A0)/parenrightBigg
qβj
Taking the value of Sof those two measures, we obtain
/summationdisplay
i,jµθ(i)pij(θ0)qβi(A0) =/summationdisplay
i,jµθ(i)pij(θ1)qβi(A0).
Hence whenever this sum is not zero, we obtain by (H2) and (H3)
m/summationdisplay
i=1µθ(i)pij(θ0)qβi(A0) =m/summationdisplay
i=1µθ(i)pij(θ1)qβi(A0),∀j
which still holds true if the sum above is zero. As A0is arbitrary, the above equality
holds in the measure sense. Using once more (H2) and (H3) we obtain
µθ(i)pij(θ0) =µθ(i)pij(θ1),∀i,j
i.e.pij(θ0) =pij(θ1) forµθ(i)>0 for alli.
In summary we have proved βi(θ0) =βi(θ1) andpij(θ0) =pij(θ1) for alli,j∈M.
Thenθ0=θ1by the injectivity in (H0).
Forthelastclaim, itisenoughtoshowthattheinverse mappingΦ−1ofΦ : Θ→F
is continuous where Φ( θ) =QY,2
θ,F={QY,2
θ;θ∈Θ}. As Φ is continuous and
injective (just proved above), Φ send every compact subset of Θ to a compact subset
ofF. But since Θ is compact by our assumptions, every closed subset of Θ is
compact. Then Φ−1:F→Θ is continuous. That ﬁnishes the proof. /square
2.4.Two classical examples.
Example 2.2 (HMM with Gaussian observations) .This is the most used HMM.
The signal space SisR, and for β= (m,1/(2σ2)),
qβ=N(m,σ2) =1√
2πσexp/parenleftbigg
−(y−m)2
2σ2/parenrightbigg
the normal law with mean mand variance σ2>0. We take θ= ((pij)i/negationslash=j,(βi)),
where
βi= (mi,1/(2σ2
i)), i∈M.MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 7
In other words we assume all transition probabilities pijand (mi,σ2
i) in the distri-
bution of signal of state iare unknown. (H0)is obviously satisﬁed.
OnR2consider the total order ( x1,y1)≺(x2,y2) deﬁned by x1< x2or (x1=
x2,y1< y2) (lexicographical order) . This total order allows us to orderer th e hidden
states in M={1,···,m}by
β1≺β2≺ ··· ≺βm.
With this ordering of the hidden states, (H2)is satisﬁed. To verify other as-
sumptions, we must specify the domains Θh,Θswhere our unknown parameters
θh= (pij)i/negationslash=j,θs= (βi)i∈Mvary. We assume
−1
δ≤mi≤1
δ, δ≤σ2
i≤1
δ,|mi−mj|+|σ2
i−σ2
j| ≥δ
for some suﬃciently small δgivena priori(the last condition means that the signals
emitted by diﬀerent hidden states are suﬃciently diﬀerent). Finally Θsis the set of
all (θs
i= (mi,1/(2σ2
i)))i∈Msatisfying the two conditions above.
We assume that for some n0∈N∗,κ >0 andν0∈ M1(M),
Θh={θh= (pij)i/negationslash=j:pij≥0,pii:= 1−/summationdisplay
j:j/negationslash=ipij≥0, Pn0(i,j)≥κν0(i),∀i,j}
whereP(i,j) =pijfori/ne}ationslash=jandP(i,i) = 1−/summationtext
j/negationslash=ipij.
Finally Θ = Θh×Θs. With the choice of Θh, we see that (H1)is satisﬁed.
The identiﬁability of the hidden states in (H3)is well known (e.g. [ 35]).
Example 2.3 (HMM with Poisson observations) .In this example S=Nand
qβ(k) =e−ββk
k!, k∈N.
We order (or name) the states of Mby
0< β1< β2<···< βm
and assume that for some δ >0 suﬃciently small,
δ≤βi≤1
δ, βi+1−βi≥δ
for alli. The domain of θs= (β1,···,βm) is the closed and bounded set satisfying
those two conditions.
Takingθ= (pij)i/negationslash=jand Θhas in the previous example and Θ = Θh×Θs, we see
that assumptions (H0),(H1)and(H2)are satisﬁed, and the identiﬁability of the
hidden states (H3)is also well known ([ 35]) .
See [6], [35], [16], [32], [7] for much more examples for which ( H3) is satisﬁed.
3.MEE and the associated gradient descent algorithm
3.1.Maximum entropy estimator (MEE). We ﬁrstly recall the general relative
entropy (also called Kullback-Leibler divergence or information).8 SHULAN HU, XINYU WANG, AND LIMING WU
Deﬁnition 3.1. Letµandνare two probability measures on the same general
measurable space ( E,E). The relative entropy H(ν|µ) ofνw.r.t.µis deﬁned by
H(ν|µ) =/braceleftbigg/integraltext
logdν
dµdν,ifν≪µ
+∞,otherwise .
(logx= logex). Speciﬁcally, for two discrete distribution ν≪µon the at most
countable set S,
H(ν|µ) =/summationdisplay
y∈S:ν(y)>0ν(y)logν(y)
µ(y).
As the identiﬁability of QY,2
θholds in the quite general framework (Hk)(k=
0,1,2,3), then LY,2
nis an asymptotically suﬃcient statistic of θ. The principle of
maximum entropy in statistical mechanics suggests that the true p robability distri-
butionQY,2
θ0should minimize the relative entropy H(LY,2
n|QY,2
θ), i.e. maximize the
entropy in physics because the relative entropy is un constant minu s the entropy in
physics. This makes sense only if Sat most countable. The relative entropy is a
crucial tool both in probability, statistics and information.
Deﬁnition 3.2 (MEE).WhenSis at most countable and given the observation
sequence Y[0,n]=y[0,n], the (2-dimensional) maximum entropy estimator θME
nofθis
deﬁned as
θME
n= argmin θ∈ΘH(LY,2
n|QY,2
θ) (3.1)
More generally let QY,2
θ(y,y′) be the density of QY,2
θw.r.t.σ(dy)σ(dy′). As
H(LY,2
n|QY,2
θ) =/summationtext
(y,y′)∈S2LY,2
n(y,y′)logLY,2
n(y,y′)−/summationtext
(y,y′)∈S2LY,2
n(y,y′)logQY,2
θ(y,y′)
=/summationtext
(y,y′)∈S2LY,2
n(y,y′)logLY,2
n(y,y′)−1
n/summationtextn
k=1logQY,2
θ(yk−1,yk)
where the ﬁrst term in the last line above does not depend on θ, and the second
term makes sense even in the continuous signal case. That is why th e MEE can be
deﬁned by
Deﬁnition 3.3. In general signal space case, given the observation signal s equence
Y[0,n]=y[0,n], the MEE θME
nis deﬁned as
θME
n= argmin
θ∈Θ−1
nn/summationdisplay
k=1logQY,2
θ(yk−1,yk). (3.2)
Given the observation sequence ( y0,···,yn), we will use ˆθME
nfor estimation of the
unknown parameters. Before doing that we ﬁrst introduce an algo rithm for ﬁnding
the MEE.
3.2.Gradient descent algorithm for MEE. For ﬁnding the minimum of
H(θ) =−1
nn/summationdisplay
k=1logQY,2
θ(yk−1,yk) (3.3) HthetaMAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 9
theidealmathematicalandalsothemostappliedapproachistocons iderthegradient
ﬂow associated to the objective function H(θ)
d
dtθ(t) =−∇θH(θ(t)). (3.4)gf
Gradient descent algorithm is just the Euler method for solving this d iﬀerential
equation, described as follows. Given the observation sequence Y[0,n]=y[0,n]=
(y0,···,yn), gradient descent algorithm for MEE can be reformulated as follow s in
the case where Sis ﬁnite: choose a suitable small step size ǫ >0,
Algorithm 1: Gradient descent algorithm for MEE with ﬁnite Salgo1Input:The observation sequence Y[0,n]=y[0,n]= (y0,···,yn), step size
ǫ >0, an arbitrary initial point θ(0), and some ﬁxed error δ >0
Output: ˆθME
n
1Calculate the 2-dimensional empirical distribution LY,2
n(y,y′), (y,y′)∈S2.
2repeat
3The (k+1)th iteration is
θ(k+1) =θ(k)+ǫ·/summationdisplay
(y,y′)∈S2LY,2
n(y,y′)∇θlogQY,2
θ(y,y′)|θ=θ(k).(3.5)
4untilH(LY,2
n|QY,2
θ(k))is less than δ;
The repeat step works when |S|2is not too big. When Sis continuous, we have
two choice: the ﬁrst one is to discretize the continuous distribution and use the same
way in discrete case, the second is to change the repeat step in the algorithm by
θ(k+1) =θ(k)+ǫ·1
nn/summationdisplay
k=1∇θlogQY,2
θ(yk−1,yk)|θ=θ(k). (3.6)
The calculation of ∇θQY,2
θrequires the derivatives of the invariant measure µθwhich
is only implicitly depending on Pθ= (pij(θ))i,j∈M. Fortunately, it can be calculated
explicitly in full generality.
prop: pa Proposition 3.1. Assume(H1), for any initial distribution νonM,
/ba∇dblνPn
θ−µθ/ba∇dbltv≤(1−κ)[n/n0],n∈N. (3.7)prop: pa1
Moreover if θ→PθisC1-smooth, then for any l= 1,···,M,
∂θlµθ=+∞/summationdisplay
k=0(µθ·∂θlPθ)·Pk
θ (3.8)prop: pa2
Those two formulas are applied for the computation of µθand∂θlµθin the algo-
rithm above.
Proof.The ﬁrst explicit geometric convergence of νPn
θtoµθis a direct consequence
of Doeblin’s theorem. For the second conclusion, taking derivative w .r.t.θlin
µθ=µθPθ, we obtain
∂θlµθ= (∂θlµθ)Pθ+µθ∂θlPθ.10 SHULAN HU, XINYU WANG, AND LIMING WU
If/summationtext
iν(i) = 0, by ( 3.7),
/summationdisplay
j|(νPn)(j)| ≤(1−κ)[n/n0]/summationdisplay
i|ν(i)|. (3.9)
As
/summationdisplay
j(µθ∂θlPθ)(j) =/summationdisplay
iµθ(i)∂θl/parenleftBigg/summationdisplay
jpij(θ)/parenrightBigg
= 0
then
/ba∇dblµθ∂θlPθPk
θ/ba∇dbltv≤(1−κ)[k/n0]/ba∇dblµθ∂θlPθ/ba∇dbltv.
Moreover
/ba∇dbl∂θlPθ/ba∇dbltv=1
2/summationdisplay
j/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
iν(i)∂θlpij(θ)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤1
2max
i/summationdisplay
j|∂θlpij(θ)|.
Therefore
∂θlµθ=∞/summationdisplay
k=0µθ(∂θlPθ)Pk
θ
and the series is geometrically convergent:
/ba∇dblµθ(∂θlPθ)Pk
θ/ba∇dbltv≤1
2max
i/summationdisplay
j|∂θlpij(θ)|(1−κ)[k/n0].
/square
4.The strong consistency and the asymptotic normality of the
MEE
4.1.Strong consistency. At ﬁrst we introduce
(H4)the signal distributions (qβ)β∈¯Osatisfy
•for anyβ,β′∈¯O,qβ,qβ′are equivalent;
•qβ(y)is continuous in β∈¯Oforσ-a.e.y∈S;
•for anyβ∈¯O,
sup
β′/vextendsingle/vextendsingle/vextendsingle/vextendsinglelogqβ(y)
qβ′(y)/vextendsingle/vextendsingle/vextendsingle/vextendsingle∈L1(qβ).
thm: sc Theorem 4.1. (Strong consistency) Assume (H0-H4). For any θ∈Θand any
initial distribution νofX0, we have under Pθ,ν,
ˆθME
n→θ,a.s.
I. Proof in the ﬁnite-signal case without (H4).LetSbe ﬁnite. We have Pθ,ν-a.s.
0≤H(LY,2
n|QY,2
θMEn)≤H(LY,2
n|QY,2
θ)→0
by the law of large number for the geometrically ergodic Markov chain Zn=
(Xn,Yn). By Csiszar-Kullback-Pinsker’s inequality
/ba∇dblν−µ/ba∇dbl2
tv≤1
2H(ν|µ)MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 11
we have
/ba∇dblLY,2
n−QY,2
θMEn/ba∇dbltv→0,/ba∇dblLY,2
n−QY,2
θ/ba∇dbltv→0,Pθ,ν−a.s.
Therefore /ba∇dblQY,2
θMEn−QY,2
θ/ba∇dbltv→0,Pθ,ν−a.s.. By Proposition 2.1,θME
n→θ,Pθ,ν−
a.s.. /square
II. Proof in the general signal case. The proof becomes much more diﬃcult.
For anyε >0, letAn= [|θME
n−θ| ≥ε]. We remark that for H(θ) given in ( 3.3),
0≤H(θ)−H(θME
n) =−1
nn/summationdisplay
k=1logQY,2
θ(Yk−1,Yk)
QY,2
θMEn(Yk−1,Yk)
≤ −1Aninf
θ′:|θ′−θ|≥ε1
nn/summationdisplay
k=1logQY,2
θ(Yk−1,Yk)
QY,2
θ′(Yk−1,Yk)−1Acninf
θ′:|θ′−θ|<ε1
nn/summationdisplay
k=1logQY,2
θ(Yk−1,Yk)
QY,2
θ′(Yk−1,Yk)
(4.1)Tsc1
Let
h(y,y′) :=/parenleftBigg
θ′→h(y,y′)(θ′) = logQY,2
θ(y,y′)
QY,2
θ′(y,y′)/parenrightBigg
θ′∈Θ(4.2)Tsc2
which is valued in the separable Banach space C(Θ) of continuous functions on the
compact Θequipped withsup-norm /ba∇dbl·/ba∇dblΘ. Let usadmit that /ba∇dblh/ba∇dblΘisQY,2
θ-integrable,
whose proof, quite technical, is left in the Appendix. By the Banach s pace valued
version of the ergodic theorem, we have Pθ,ν-a.s.
sup
θ′∈Θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
k=1h(Yk−1,Yk)(θ′)−/integraldisplay
S2h(y,y′)(θ′)dQY,2
θ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle→0.
Therefore Pθ,ν-a.s.
infθ′:|θ′−θ|≥ε1
n/summationtextn
k=1logQY,2
θ(Yk−1,Yk)
QY,2
θ′(Yk−1,Yk)→infθ′:|θ′−θ|≥εH(QY,2
θ|QY,2
θ′) =c(ε)
infθ′:|θ′−θ|<ε1
n/summationtextn
k=1logQY,2
θ(Yk−1,Yk)
QY,2
θ′(Yk−1,Yk)→infθ′:|θ′−θ|<εH(QY,2
θ|QY,2
θ′) = 0
Sinceθ′→H(QY,2
θ|QY,2
θ′) is lower semi-continuous, its inﬁmum over the compact
{θ′∈Θ :|θ′−θ| ≥ε}is attained, so the constant above c(ε) is positive. Taking
liminf n→+∞in (4.1), we obtain
0≤ −limsup
n→∞1An·c(ε),Pθ,ν−a.s.
where it follows that Pθ,ν(An,i.o.) = 0. That is the desired strong consistency. /square
4.2.Central limit theorem for MEE. We require the Fisher information of
(QY,2
θ)θ∈Θ0for the CLT of θME
n. We state at ﬁrst
(H5)The model is C2-regular, more precisely
•θ→pij(θ),θ→βi(θ),i,j∈MareC2-smooth on Θ0;
•β→qβ(y)isC2-smooth for σ-a.e.y∈S;
•supβ′∈K|∇β′logqβ′| ∈L2(qβ)for all compact subset K⊂Oandβ∈O;
•supβ′∈K/ba∇dbl∇2
β′logqβ′/ba∇dbl ∈L1(qβ)for all compact subset K⊂Oandβ∈O.12 SHULAN HU, XINYU WANG, AND LIMING WU
Here/ba∇dblA/ba∇dbl:= sup|z|=1|Az|is the matrix norm.
Deﬁnition 4.1. The Fisher-information matrix I2(θ)of(QY,2
θ)θ∈Θ0is deﬁned by
I2(θ) =/integraldisplay
S2∇θlogQY,2
θ·∇T
θlogQY,2
θdQY,2
θ
which is the covariance matrix/parenleftBig
Cov(∂θilogQY,2
θ,∂θjlogQY,2
θ)/parenrightBig
1≤i,j≤MunderQY,2
θ.
HereQY,2
θis also interpreted as the density w.r.t. σ(dy)σ(dy′).
thm-CLT Theorem 4.2. Assume (H0-H5). Suppose that
I2(θ)is invertible for all θ∈Θ0(4.3)
then for any θ∈Θ0and any initial distribution νofX0, the MEE ˆθME
nis asymp-
totically normal under Pθ,ν:
√n(ˆθME
n−θ)L− →N(0,I2(θ)−1ΓθI2(θ)−1)
whereΓθ= (Γθ(i,j))1≤i,j≤Mis given by
Γθ(i,j) =Cov(∂θilogQY,2
θ(Y0,Y1),∂θjlogQY,2
θ(Y0,Y1))
++∞/summationdisplay
k=1Cov(∂θilogQY,2
θ(Y0,Y1),∂θjlogQY,2
θ(Yk,Yk+1))
++∞/summationdisplay
k=1Cov(∂θjlogQY,2
θ(Y0,Y1),∂θilogQY,2
θ(Yk,Yk+1))(4.4)thm-CLT2
taken under Pθ,µθ.
Moreover the asymptotic covariance matrix I2(θ)−1ΓθI2(θ)−1satisﬁes
I2(θ)−1ΓθI2(θ)−1≤/parenleftbigg
1+2n0
1−√1−κ/parenrightbigg
I2(θ)−1, θ∈Θ0. (4.5)CLT1
in the order of nonnegative deﬁniteness of symmetric matric es, where n0,κare given
in(H1).
Proof.AsˆθME
n→θ∈Θ0,Pθ,ν-a.s. by Theorem 4.1,ˆθME
n∈Θ0for allnbig enough.
Then
∇θHn(θ) =∇θHn(θ)−∇θHn(ˆθME
n) =∇2
θHn(ξn)(θ−ˆθME
n) (4.6) thm-CLT1
for some random vector ξnin the segment from θtoˆθME
n. By the strong consistency
ofˆθME
nagain,ξn→θ,Pθ,ν-a.s.. For any δ >0, as
EPθ,µθsup
θ′:|θ′−θ|<δ/ba∇dbl∇2
θHn(θ′)−∇2
θHn(θ)/ba∇dbl
≤EPθ,µθ1
nn/summationdisplay
k=1sup
θ′:|θ′−θ|<δ/ba∇dbl∇2
θ′logQY,2
θ′(Yk−1,Yk)−∇2
θlogQY,2
θ(Yk−1,Yk)/ba∇dbl
=/integraldisplay
sup
θ′:|θ′−θ|<δ/ba∇dbl∇2
θ′logQY,2
θ′(y,y′)−∇2
θlogQY,2
θ(y,y′)/ba∇dbldQY,2
θMAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 13
which tends to zero as δ→0 by(H5). Then as δ→0, supθ′:|θ′−θ|<δ/ba∇dbl∇2
θHn(θ′)−
∇2
θHn(θ)/ba∇dbl →0uniformlyin n≥1inprobability- Pθ,µθ,andconsequently itconverges
to zero in probability- Pθ,ν,uniformly in n≥1. Therefore
∇2
θHn(ξn)−∇2
θHn(θ)→0
in probability- Pθ,ν. Since by the ergodic theorem,
∇2
θHn(θ) =−1
nn/summationdisplay
k=1∇2
θlogQY,2
θ(Yk−1,Yk)→ −/integraldisplay
∇2
θlogQY,2
θ(y0,y1)dQY,2
θ=I2(θ)
(4.7)thm-CLT5
Pθ,ν-a.s., we obtain
/parenleftbig
∇2
θHn(ξn)/parenrightbig−1→I2(θ)−1,in probability- Pθ,ν (4.8)thm-CLT3
By (4.6), on the event AnthatθME
n∈Θ0and∇2
θHn(ξn) is invertible,
√n(ˆθME
n−θ) = (∇2
θHn(ξn))−1/parenleftbig√n∇θHn(θ)/parenrightbig
By the CLT of the Markov chain Zn= (Xn,Yn) (Meyn-Tweedie [ 24]) and the fact
that|∇θlogQY,2
θ(y,y′)| ∈L2(QY,2
θ) by(H5), and
/integraldisplay
∇θlogQY,2
θ(y,y′)dQY,2
θ(y,y′) = 0
we have
√n∇θHn(θ) =−1√nn/summationdisplay
k=1∇θlogQY,2
θ(Yk−1,Yk)in law− −− → N(0,Γθ)
where Γ θis given by ( 4.4). This, together with Pθ,ν(An)→1 and the convergence
in probability in ( 4.8), implies that
√n(ˆθME
n−θ)in law− −− → N(0,I2(θ)−1ΓθI2(θ)−1)
the desired CLT.
It remains to prove ( 4.5). For any z∈RMwith|z|= 1, letz′:=I2(θ)−1z
/angbracketleftbig
z,I2(θ)−1ΓθI2(θ)−1z/angbracketrightbig
=/an}b∇acketle{tz′,Γθz′/an}b∇acket∇i}ht
= Var/parenleftBig/angbracketleftBig
z′,∇θlogQY,2
θ/angbracketrightBig
(Y0,Y1)/parenrightBig
+2+∞/summationdisplay
k=1Cov/parenleftBig/angbracketleftBig
z′,∇θlogQY,2
θ/angbracketrightBig
(Y0,Y1),/angbracketleftBig
z′,∇θlogQY,2
θ/angbracketrightBig
(Yk,Yk+1)/parenrightBig
≤Var/parenleftBig/angbracketleftBig
z′,∇θlogQY,2
θ/angbracketrightBig
(Y0,Y1)/parenrightBig/parenleftBigg
1+2+∞/summationdisplay
k=1ρ2(Pk
θ)/parenrightBigg
where
ρ2(Pk
θ) := sup
µθ(f2)≤1/ba∇dblPk
θf−µθ(f)/ba∇dblL2(µθ).14 SHULAN HU, XINYU WANG, AND LIMING WU
By Del Moral-Ledoux-Miclo [ 9, Proposition 1.1],
ρ2(Pk
θ)≤/radicalBigg
1
2max
i/summationdisplay
j|Pk
θ(i,j)−µ(j)| ≤(1−κ)[k/n0]/2.
Substituting it into the previous inequality and noting that
Var/parenleftBig/angbracketleftBig
z′,∇θlogQY,2
θ/angbracketrightBig
(Y0,Y1)/parenrightBig
=/angbracketleftbig
z′,I(2)(θ)z′/angbracketrightbig
=/angbracketleftbig
I−1
2(θ)z,z/angbracketrightbig
we obtain
/angbracketleftbig
z,I2(θ)−1ΓθI2(θ)−1z/angbracketrightbig
≤/angbracketleftbig
z,I−1
2(θ)z/angbracketrightbig/parenleftBigg
1+2+∞/summationdisplay
k=1(1−κ)[k/n0]/2/parenrightBigg
where (4.5) follows. /square
5.Hypothesis testing: θ=θ0vsθ=θ1
Of course we can use the asymptotic normality of θME
nfor the hypothesis testing:
H0:θ=θ0vsH1:θ=θ1.
Given a level of conﬁdence α∈(1/2,1), accept H0, if|ˆθME
n−θ0| ≤cα√nwhere
P/parenleftBigg
|I−1/2
2(θ0)η|< cα/radicalBigg
1+2n0
1−√1−κ/parenrightBigg
=α
andηis the standard Gaussian random vector in RMof lawN(0,I).
Its level of conﬁdence is approximatively greater than α, by the CLT and ( 4.5) in
Theorem 4.2. The second type error of this test is very small: if |θ1−θ0|=β/√n
withβmuch bigger than cα,
Pθ1/parenleftbigg
|ˆθME
n−θ0| ≤cα√n/parenrightbigg
≤Pθ1/parenleftbigg
|ˆθME
n−θ1| ≥ |θ1−θ0|−cα√n/parenrightbigg
/√∇ecedesequalPθ1/parenleftBigg
|I−1/2
2(θ1)η| ≥(β−cα)/radicalBigg
1+2n0
1−√1−κ/parenrightBigg
where the law of ηisN(0,I).
However we want to propose another hypothesis test basing on th e relative en-
tropy. The reason is very simple from the algorithm point of view: one can only
approximate the MEE ˆθME
nbyˆθME
n,Nwhen the gradient descent algorithm related
to the relative entropy H(LY,2
n|QY,2
θ) stops at some step N(when the algorithm
stabilizes). So one can take θ0≈ˆθME
n,Nand to test if θ=θ0.
Becauseweusetherelativeentropy H(LY,2
n|QY,2
θ)forthehypothesistesting: θ=θ0
vsθ=θ1, we assume that the signal space Sis ﬁnite in this section.
5.1.Central limit theorem for the relative entropy and χ2-divergence. For
bounding the relative entropy, we recall the χ2-divergence χ2(ν|µ) and the relation-
ships between these metrics. We begin by recalling a known result.MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 15
lem: rela Lemma 5.1. (see [34]) On ﬁnite state space S,χ2-divergence is deﬁned by
χ2(ν|µ) =/summationdisplay
y∈S(ν(y)−µ(y))2
µ(y).
Then we have
2||ν−µ||2
tv≤H(ν|µ)≤log(χ2(ν|µ)+1) (5.1) eq: rela
and
χ2(ν|µ)≤1
minµ(y)||ν−µ||2
tv.
The ﬁrst inequality in ( 5.1) is the famous Csiszar-Kullback-Pinsker inequality.
Due to the central limit theorems of Markov chains (see Meyn and Tw eedie [24]),
we can obtain the CLT for the relative entropy.
thm: clt for H Theorem 5.2. UnderPθ,ν, we have
2nH(LY,2
n|QY,2
θ)in law− −−− →/summationdisplay
y,y′ξ2(y,y′)
QY,2
θ(y,y′)
and
nχ2(LY,2
n|QY,2
θ)in law− −−− →/summationdisplay
y,y′ξ2(y,y′)
QY,2
θ(y,y′),
whereξ= (ξ(y,y′))(y,y′)∈S2is a centered gaussian vector with covariance matrix
Γ((y,y′),(˜y,˜y′)) := Cov( ξ(y,y′),ξ(˜y,˜y′))
= Cov(1 (y,y′)(Y0,Y1),1(˜y,˜y′)(Y0,Y1))++∞/summationdisplay
k=1Cov(1 (y,y′)(Y0,Y1),1(˜y,˜y′)(Yk,Yk+1))
++∞/summationdisplay
k=1Cov(1 (˜y,˜y′)(Y0,Y1),1(y,y′)(Yk,Yk+1))
where the covariances in the last line above are under the sta tionary probability
measure Pθ,µθ.
5.2.Expectation and Concentration for the limit law of the relat ive en-
tropy.For applications of Theorem 5.2, we must control the limit law of |ζ|2where
ζ=
ζ(y,y′) =ξ(y,y′)/radicalBig
QY,2
θ(y,y′)
and|ζ|2=/summationdisplay
(y,y′)ξ2(y,y′)
QY,2
θ(y,y′)
whereξ= (ξ(y,y′))(y,y′)∈S2is given in Theorem 5.2.
prop55 Proposition 5.3.
(a)
EPθ,ν|ζ|2≤/parenleftbigg
1+2n0
1−√1−κ/parenrightbigg
·(|S2|−1)16 SHULAN HU, XINYU WANG, AND LIMING WU
(b) The maximum eigenvalue λmaxof the covariance matrix Γζof the gaussian vector
ζsatisﬁes
λmax≤1+2n0
1−√1−κ.
(c) For any c >1,
P(|ζ|2> cE|ζ|2)≤exp/parenleftbigg
−(√c−1)2
2λmax·E|ζ|2/parenrightbigg
5.3.Hypothesis test by means of the relative entropy. For the hypothesis
testing:H0:θ=θ0vsH1:θ=θ1, given a level of conﬁdence α∈(1/2,1),acceptH0
ifH(LY,2
n|QY,2
θ)≤cα
n(and reject H0otherwise) . The constant cαcan be determined
approximatively by the concentration of |ζ|2. Ifnis big enough, the type I error
Pθ0/parenleftBig
H(LY,2
n|QY,2
θ0)>cα
n/parenrightBig
≈Pθ0/parenleftbig
|ζ|2>2cα/parenrightbig
.
One can determine cαfrom Proposition 5.3so that the last probability is less than
1−α.
For estimating the second type error for big n, by Lemma 5.1,
Pθ1/parenleftBig
H(LY,2
n|QY,2
θ0)≤cα
n/parenrightBig
≤Pθ1/parenleftbigg
||LY,2
n−QY,2
θ0||tv≤/radicalbiggcα
2n/parenrightbigg
≤Pθ1/parenleftbigg
||LY,2
n−QY,2
θ1||tv≥ ||QY,2
θ1−QY,2
θ0||tv−/radicalbiggcα
2n/parenrightbigg
by the triangular inequality. Let c(θ0,θ1) :=||QY,2
θ1−QY,2
θ0||tvwhich is a positive
constant (by Proposition 2.1) and easy to be computed in practice. By the con-
centration inequality of Markov chains in Djellot et al.[11], once if c(θ0,θ1)>
Eθ1||LY,2
n−QY,2
θ1||tv+/radicalbigcα
2n>0 (the latter is an inﬁnitesimal O(1/√n)), we have
Pθ1/parenleftbigg
||LY,2
n−QY,2
θ1||tv≥ ||QY,2
θ1−QY,2
θ0||tv−/radicalbiggcα
2n/parenrightbigg
≤exp/parenleftBigg
−n·2
(1+n0(1−κ)/κ))2/bracketleftbigg
c(θ0,θ1)−Eθ1||LY,2
n−QY,2
θ1||tv−/radicalbiggcα
2n/bracketrightbigg2/parenrightBigg
.
The last term is exponentially small in big n. In other words this relative entropy
hypothesis testing is exponentially powerful in n.
6.Numerical examples
In this section we illustrate the algorithm 2RE described in §3, by means of two
concrete examples with very great sample size, for which the classic algorithms are
too time-consuming for being useful in practice. We show how the alg orithm 2RE
perform in regard to a great size of observation sequences and ho w the estimated
parameters diﬀer from the true parameters. In the two examples there are only two
hiddens states 1 ,2.MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 17
6.1.Example 1: Poisson observation. The data is generated from the HMM
with hidden states {1,2}and Poisson observation sequence on N. We suppose that
the parameters in θ= (p12,p21,β1,β2) are all unknown, where βkis the parameter
of the Poisson distribution of the signal when the hidden state is k(k= 1,2).
We use
Pθ=/bracketleftbigg
0.3 0.7
0.6 0.4/bracketrightbigg
and (β1,β2) = (2.5,0.5)
i.e.θ=θ0= (0.7,0.6;2.5,0.5) to generate a sequence of n= 105signals. In this
case, the Fisher information I2(θ0) ofQY,2
θis

0.85161298 −0.42440013 0 .19349763 −0.43938774
−0.42440013 0 .94910932 −0.17431689 0 .38227075
0.19349763 −0.17431689 2 .13206361 −1.0718548
−0.43938774 0 .38227075 −1.0718548 1 .17119549

Its inverse is

1.75095757 0 .5470283 0 .23391698 0 .6924219
0.5470283 1 .42470472 −0.11810743 −0.36788003
0.23391698 −0.11810743 0 .92911967 0 .97661805
0.6924219 −0.36788003 0 .97661805 2 .12745374

In the algorithm, we set ǫ= 0.001 as the step size in the gradient descent, and
l= 30 as the the number of iterations for calculating the invariant mea sureµθand
its derivative ∂θijµθ(according to the formula ( 3.8)).
Start the 2RE algorithm with the initial value
Pθ(0)=/bracketleftbigg
0.5 0.5
0.5 0.5/bracketrightbigg
and (β1(0),β2(0)) = (3 ,0.1).
We obtain the iteration results in the table 1,
Table 1. Iteration results table2
k Pθ(k) (β1(k),β2(k)) H(k)
5000/bracketleftbigg
0.28691485 0 .71308515
0.59653801 0 .40346199/bracketrightbigg
(2.5073621,0.50915734) 5.9192171556647552 ×10−4
10000/bracketleftbigg
0.28788447 0 .71211553
0.59600346 0 .40399654/bracketrightbigg
(2.50811494 ,0.50833996) 5.923377320959681 ×10−4
whereH(k) =H(LY,2
n|QY,2
θ(k)).
As seen from the above table, the estimated values are very precis e if we have
enough observations and use enough iterations in the 2RE algorithm . The mathe-
matical reason is the Fisher information in the present Poisson signa l model is not
small. All numerical results above are within the prevision of our theo ratical results
in Theorems 4.2and5.2.18 SHULAN HU, XINYU WANG, AND LIMING WU
6.2.Example 2: Gaussian observation. The data is generated from the HMM
with hidden state {1,2}and Gaussian observation sequence on ( −∞,∞) with
qβ(y) =1√
2πexp(−(y−β)2
2).
We generate a signal-data sequence of length n= 5000 of this HMM with the
following parameters:
Pθ=/bracketleftbigg
0.2 0.8
0.7 0.3/bracketrightbigg
and (β1,β2) = (0,3)
i.e.θ0= (0.8,0.7;0,3). In this case, the Fisher information I2(θ0) ofQY,2
θis

2.0939−0.4257 0.2467 0 .3394
−0.4257 2.0731−0.3360−0.2829
0.2467−0.3360 0.7159−0.1299
0.3394−0.2829−0.1299 0.8537

Its inverse is

0.54996962 0 .04677193 −0.21023406 −0.23513801
0.04677193 0 .56800595 0 .28923517 0 .21364196
−0.21023406 0 .28923517 1 .68409397 0 .43568218
−0.23513801 0 .21364196 0 .43568218 1 .40194479

For the continuous signals, we replace the repeat step in the Algorit hm1by
θ(k+1) =θ(k)+ǫ·1
nn/summationdisplay
k=1∇θlogQY,2
θ(yk−1,yk)|θ=θ(k).
we setǫ= 0.1 as the step size, l= 30 as the number of iterations for calculating the
invariant measure µθand its derivative ∂θijµθ. Begin the algorithm 2RE with the
initial value
Pθ(0)=/bracketleftbigg
0.5 0.5
0.5 0.5/bracketrightbigg
and (β1(0),β2(0)) = (0 ,1).
We obtain the iteration results of our 2RE in the table 2,
Table 2. Iteration results table1
k Pθ(k) (β1(k),β2(k))
100/bracketleftbigg
0.19086042 0 .80913958
0.701435 0 .298565/bracketrightbigg
(0.00000604 ,3.00604911)
200/bracketleftbigg
0.19223189 0 .80776811
0.70309968 0 .29690032/bracketrightbigg
(0.00572507 ,3.01038024)
For this example, the number of the observations n= 5000 is smaller than the
previous example n= 106. Thus the estimated values are less precise. On the other
hand,ǫ= 0.1 is bigger than ǫ= 0.00001 in the previous one, we get the asymptotic
estimated values in much less iterations.MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 19
7.Comparison of the MEE with the MLE
The advantages of the MEE w.r.t. the MLE are all in the case where th e sample
sizenis big:
(1) when n≥1000, the EM algorithm for ﬁnding the MLE is much more time
consumming than our 2RE algorithm.
(2) Our algorithm for ﬁnding the MEE works for very big n, such as n= 105
as in the study of the genomes; for which the EM algorithm is no longer
practicable.
(3) Moreover as
∇2
θ/parenleftBigg
1
nn/summationdisplay
k=1logQY,2
θ(Yk−1,Yk)/parenrightBigg
→EPθ,µθ∇2
θlogQY,2
θ(Y0,Y1) =I2(θ),
whentheFisherinformation I2(θ)isnotdegenerate,1
n/summationtextn
k=1logQY,2
θ(yk−1,yk)
will be strictly convex, there is no problem of local minima in our 2RE algo -
rithm.
The disadvantage of the MEE w.r.t. the MLE is: the MLE, once comput able, is
the best choice because it is asymptotically eﬃcient in the sense of Le hman. When
100≤n≤1000, the MLE is computable by the EM algorithm, it is better than
the MEE. Notice also that the Fisher information I(θ) in the CLT of the MLE
([22],[28],[29]) is the optimal one (Cramer-Rao inequality), then always bigger tha n
our two-dimensional Fisher information I2(θ).
8.Appendix
8.1.Integrability of hin the proof of Theorem 4.1.
Proof.In this paragraph we prove that/integraltext
supθ′|h(y,y′)(θ′)|dQY,2
θ<+∞wherehis
given by ( 4.2). At ﬁrst
QY,2
θ(y,y′)sup
θ′∈ΘlogQY,2
θ(y,y′)
QY,2
θ′(y,y′)
≤sup
β,β′∈¯O/parenleftBigg/summationdisplay
i,jµθ(i)pij(θ)qβi(θ)(y)qβj(θ)(y′)/parenrightBigg
log/summationtext
i,jµθ(i)pij(θ)qβi(θ)(y)qβj(θ)(y′)
qβ(y)qβ′(y′)
≤sup
β,β′∈¯O/summationdisplay
i,jµθ(i)pij(θ)qβi(θ)(y)qβj(θ)(y′)logqβi(θ)(y)qβj(θ)(y′)
qβ(y)qβ′(y′)
≤/summationdisplay
i,jµθ(i)pij(θ)qβi(θ)(y)qβj(θ)(y′)/parenleftBigg
sup
β∈¯Ologqβi(θ)(y)
qβ(y)+ sup
β′∈¯Ologqβj(θ)(y′)
qβ′(y′)/parenrightBigg
where the third line inequality follows by the convexity of xlogx. The last term is
σ(dy)σ(dy′)-integrable by (H4), i.e. supθ′∈ΘlogQY,2
θ(y,y′)
QY,2
θ′(y,y′)is bounded from above by
aQY,2
θ-integrable function.20 SHULAN HU, XINYU WANG, AND LIMING WU
For the lower bound,
inf
θ′∈ΘlogQY,2
θ(y,y′)
QY,2
θ′(y,y′)
≥inf
β,β′∈¯Olog/summationtext
i,jµθ(i)pij(θ)qβi(θ)(y)qβj(θ)(y′)
qβ(y)qβ′(y′)
≥inf
β,β′∈¯O/summationdisplay
i,jµθ(i)pij(θ)logqβi(θ)(y)qβj(θ)(y′)
qβ(y)qβ′(y′)
≥/summationdisplay
i,jµθ(i)pij(θ)/parenleftbigg
inf
β∈¯Ologqβi(θ)(y)
qβ(y)+ inf
β′∈¯Ologqβj(θ)(y′)
qβ′(y′)/parenrightbigg
=/summationdisplay
iµθ(i) inf
β∈¯Ologqβi(θ)(y)
qβ(y)+/summationdisplay
jµθ(j) inf
β∈¯Ologqβj(θ)(y′)
qβ′(y′)
and by(H4)
/integraldisplay
S|inf
β∈¯Ologqβi(θ)(y)
qβ(y)|dqβj(θ)
≤/integraldisplay
Ssup
β∈¯O|logqβj(θ)(y)
qβ(y)|dqβj(θ)+/integraldisplay
S|logqβj(θ)(y)
qβi(θ)(y)|dqβj(θ)<+∞
Then inf θ′logQY,2
θ(y,y′)
QY,2
θ′(y,y′)is bounded from below by a QY,2
θ(dy,dy′)-integrable func-
tion. Combining the upper and lower controls above, /ba∇dblh(y,y′)/ba∇dblΘisQY,2
θ(dy,dy′)-
integrable. /square
8.2.The proof of Theorem 5.2.
Proof.LetZ(2)
k= (Zk,Zk+1) = ((Xk,Yk),(Xk+1,Yk+1))whichisalsoaMarkovchain,
by CLT for Markov chains (cf. Meyn-Tweedie [ 24, Theorem 17.0.1]),
√n/parenleftBig
LY,2
n(y,y′)−QY,2
θ(y,y′)/parenrightBig
(y,y′)∈S2in law− −− →ξ= (ξ(y,y′))(y,y′)∈S2(8.1)A71
whereξisthecentered Gaussianvector withthecovariancematrixgiven inT heorem
5.2. We divide the proof into two steps.
Step 1. By Taylor’s formula of order 2, for any x >0, there exists 0 ≤t≤1,
such that
xlogx= (x−1)+1
2·1
t+(1−t)x·(x−1)2.
Since
/summationdisplay
(y,y′)/parenleftBigg
LY,2
n(y,y′)
QY,2
θ(y,y′)−1/parenrightBigg
QY,2
θ(y,y′) = 0,MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 21
we have
H(LY,2
n|QY,2
θ) =/summationdisplay
(y,y′)LY,2
n(y,y′)
QY,2
θ(y,y′)logLY,2
n(y,y′)
QY,2
θ(y,y′)·QY,2
θ(y,y′)
=1
2/summationdisplay
(y,y′)1
tn(ω)+(1−tn(ω))LY,2
n(y,y′)
QY,2
θ(y,y′)/parenleftBigg
LY,2
n(y,y′)
QY,2
θ(y,y′)−1/parenrightBigg2
·QY,2
θ(y,y′).
By the law of large number,
1
tn(ω)+(1−tn(ω))LY,2
n(y,y′)
QY,2
θ(y,y′)a.s.− − →1.
Then for any ε >0,
Pθ/parenleftbigg
(1−ε)1
2χ2(LY,2
n|QY,2
θ)≤H(LY,2
n|QY,2
θ)≤(1+ε)1
2χ2(LY,2
n|QY,2
θ)/parenrightbigg
→1.(8.2)A72
Step 2. By (8.1),
nχ2(LY,2
n|QY,2
θ) =/summationdisplay
(y,y′)[√n(LY,2
n(y,y′)−QY,2
θ(y,y′))]2
QY,2
θ(y,y′)
in law− −− →/summationdisplay
(y,y′)ξ2(y,y′)
QY,2
θ(y,y′)
Thus by ( 8.2), we obtain
nH(LY,2
n|QY,2
θ)in law− −− →1
2/summationdisplay
(y,y′)ξ2(y,y′)
QY,2
θ(y,y′).
The proof of Theorem 5.2is completed. /square
8.3.Proof of Proposition 5.3.
Deﬁnition 8.1. Dobrushin’s ergodic coeﬃcient of the transition probability matrix
Pis deﬁned by
rD(P) = sup
i,j∈M||P(i,·)−P(j,·)||tv22 SHULAN HU, XINYU WANG, AND LIMING WU
Proof of Proposition 5.3.(a) For any g:S2→R,
E(/summationdisplay
(y,y′)∈S2ξ(y,y′)g(y,y′))2
= Var(g(Y0,Y1))+2+∞/summationdisplay
k=1Cov(g(Y0,Y1),g(Yk,Yk+1))
≤Var(g)+2+∞/summationdisplay
k=1ρ2(Pk−1
θ)Var(g)
≤Var(g)+2+∞/summationdisplay
k=1/radicalBig
rD(Pk−1
θ)Var(g).
But by(H1),rD(Pk
θ)≤(1−κ)[k/n0], then
E(/summationdisplay
(y,y′)∈S2ξ(y,y′)g(y,y′))2≤/parenleftbigg
1+2n0
1−√1−κ/parenrightbigg
Var(g). (8.3)A74
Hence
E|ζ|2=/summationdisplay
y,y′Eξ2(y,y′)
QY,2
θ(y,y′)
≤/summationdisplay
(y,y′)∈S2/parenleftbigg
1+2n0
1−√1−κ/parenrightbigg
·Var(1(y,y′))
QY,2
θ(y,y′)
=/summationdisplay
(y,y′)∈S2/parenleftbigg
1+2n0
1−√1−κ/parenrightbigg
·QY,2
θ(y,y′)(1−QY,2
θ(y,y′))
QY,2
θ(y,y′)
=/parenleftbigg
1+2n0
1−√1−κ/parenrightbigg
·(|S2|−1).
(b). Let Γ ζbe the covariance matrix of gaussian vector ζ, for any λ∈RS2, we
haveEe/angbracketleftζ,λ/angbracketright=e1
2/angbracketleftλ,Γζλ/angbracketright. Furthermore, let ( ek)1≤k≤|S|2be the orthonormal basis of
eigenvectors of Γ ζassociated to the eigenvalues ( λk).ζk:=/an}b∇acketle{tζ,ek/an}b∇acket∇i}htare i.i.d. of law
N(0,1). So we have
Eea|ζ|2=|S2|/productdisplay
k=1Eeaλk|ζk|2=/productdisplay
k1√1−2λka.
Notice that if Z∼ N(0,1), for any λ <1/2,EeλZ2=1√1−2λ. So we have
Eea|ζ|2=/productdisplay
k1√1−2λka
where it follows
logEea|ζ|2=−|S2|/summationdisplay
k=11
2log(1−2λka). (8.4)eq: logeMAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 23
Letλmaxbe the maximum eigenvalue of Γ ζ. Since for any z∈RS2with|z|= 1,
/an}b∇acketle{tz,Γζz/an}b∇acket∇i}ht=E
/summationdisplay
(y,y′)∈S2z(y,y′)ξ(y,y′)/radicalbig
QY,2θ(y,y′)
2
≤1+2n0
1−√1−κ
by (8.3). We obtain thus λmax≤1+2n0
1−√1−κ.
By (8.4), we get for any a∈(0,1/(2λmax)),
logEea|ζ|2≤(/summationdisplay
kλk)a+/summationdisplay
k1
2(2λka)2·1
1−2λka
≤/parenleftbigg
a+a2·2λmax
1−2λmaxa/parenrightbigg
E|ζ|2
Forc >1 ﬁxed, we have for any a∈(0,1/(2λmax)),
P(|ζ|2> cE|ζ|2)≤e−a(c−1)E|ζ|2·Eea(|ζ|2−E|ζ|2)
≤exp/parenleftbigg
−/parenleftbigg
a(c−1)−2λmaxa2
1−2λmaxa/parenrightbigg
E|ζ|2/parenrightbigg
By optimization over a∈(0,1
2λmax),
P(|ζ|2> cE|ζ|2)≤exp/parenleftbigg
−(√c−1)2
2λmax·E|ζ|2/parenrightbigg
.
/square
References
AHL2016 [1] Grigory Alexandrovich, Hajo Holzmann, and Anna Leister. Nonpa rametric
identiﬁcation and maximum likelihood estimation for hidden markov mode ls.
Biometrika , 103(2):423–434, 2016.
BP1966 [2] Leonard E Baum and Ted Petrie. Statistical inference for proba bilistic func-
tions of ﬁnite state markov chains. The annals of mathematical statistics ,
37(6):1554–1563, 1966.
BPSW1970 [3] LeonardE Baum, Ted Petrie, GeorgeSoules, and NormanWeiss. A maximiza-
tion technique occurring in the statistical analysis of probabilistic fu nctions of
markov chains. The annals of mathematical statistics , 41(1):164–171, 1970.
BBSM1986 [4] Lalit R Bahl, Peter F Brown, Peter V De Souza, and Robert L Merce r. Max-
imum mutual information estimation of hidden markov model paramet ers for
speech recognition. In proc. icassp , volume 86, pages 49–52, 1986.
BR1996 [5] Peter J Bickel, Ya’Acov Ritov, et al. Inference in hidden markov mo dels i:
Local asymptotic normality in the stationary case. Bernoulli , 2(3):199–228,
1996.
BK1957 [6] David Blackwell, Lambert Koopmans, et al. On the identiﬁability prob lem
for functions of ﬁnite markov chains. The Annals of Mathematical Statistics ,
28(4):1011–1015, 1957.24 SHULAN HU, XINYU WANG, AND LIMING WU
BB1998 [7] I.A. Boguslavski˘ ı and M.Yu Borodovski˘ ı. On identiﬁcation of st ates of a se-
quence generated by a hidden markov model. Journal of Computer and Sys-
tems Sciences International , 37, 07 1998.
BRR1998 [8] Peter J Bickel, Ya’acov Ritov, Tobias Ryden, et al. Asymptotic nor mality
of the maximum-likelihood estimator for general hidden markov mode ls.The
Annals of Statistics , 26(4):1614–1635, 1998.
del2003contraction [9] P Del Moral, M Ledoux, and L Miclo. On contraction properties of m arkov
kernels.Probability theory and related ﬁelds , 126(3):395–420, 2003.
DLR1977 [10] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelih ood
from incomplete data via the em algorithm. Journal of the Royal Statistical
Society: Series B (Methodological) , 39(1):1–22, 1977.
DGW04 [11] H. Djellout, A. Guillin, and L. Wu. Transportation cost-informat ion inequali-
ties and applications to random dynamical systems and diﬀusions. Annals of
Probability , 32(3B):2702–2732, 2004.
DM2001 [12] RandalDouc, CatherineMatias, etal. Asymptoticsofthemaxim umlikelihood
estimator for general hidden markov models. Bernoulli , 7(3):381–420, 2001.
DMR2004 [13] RandalDouc, EricMoulines, TobiasRyd´ en, etal. Asymptoticpr opertiesofthe
maximum likelihood estimator in autoregressive models with markov reg ime.
The Annals of statistics , 32(5):2254–2304, 2004.
DMOH2011 [14] RandalDouc, EricMoulines, JimmyOlsson, RamonVanHandel, eta l. Consis-
tency of the maximum likelihood estimator for general hidden markov models.
the Annals of Statistics , 39(1):474–513, 2011.
Ephraim02 [15] YarivEpharaim. HiddenMarkovProcesses. IEEE transactions on information
theory, vol. 48: 1518–1569, 2002.
FR1992 [16] Donald R Fredkin and John A Rice. Maximum likelihood estimation and
identiﬁcation directly from single-channel recordings. Proceedings of the Royal
Society of London. Series B: Biological Sciences , 249(1325):125–132, 1992.
GM2000 [17] Franc¸ cois Le Gland and Laurent Mevel. Exponential forgettin g and geometric
ergodicity in hidden markov models. Mathematics of Control, Signals and
Systems, 13(1):63–93, 2000.
GWR2007 [18] AmitGruber, YairWeiss, andMichal Rosen-Zvi. Hiddentopicmark ovmodels.
InArtiﬁcial intelligence and statistics , pages 163–170, 2007.
H2011 [19] ShuLan Hu. Transportation inequalities for hidden markov chain s and appli-
cations.Science China Mathematics , 54(5):1027–1042, 2011.
HW2011 [20] Shulan Hu and Liming Wu. Large deviations for random dynamical s ystems
and applications to hidden markov models. Stochastic Processes and their
Applications , 121(1):61–90, 2011.
KR1998 [21] Vikram Krishnamurthy and Tobias Ryden. Consistent estimation of linear
and non-linear autoregressive models with markov regime. Journal of time
series analysis , 19(3):291–307, 1998.
L1992 [22] Brian G Leroux. Maximum-likelihood estimation for hidden markov m odels.
Stochastic processes and their applications , 40(1):127–143, 1992.
MA2013 [23] Donald EK Martin and John AD Aston. Distribution of statistics of hidden
state sequences through the sum-product algorithm. Methodology and Com-
puting in Applied Probability , 15(4):897–918, 2013.MAXIMUM ENTROPY ESTIMATOR FOR HIDDEN MARKOV MODELS: REDUCT ION TO DIMENSION 2 25
meyn2012 [24] Sean P Meyn and Richard L Tweedie. Markov chains and stochastic stability .
Springer Science & Business Media, 2012.
NG2001 [25] Dennis Nilsson and Jacob Goldberger. Sequentially ﬁnding the n-b est list in
hidden markov models. In International joint conference on artiﬁcial intelli-
gence, volume 17, pages 1280–1285. Lawrence Erlbaum Associates LTD, 2001.
Petrie69 [26] Ted Petrie. Probabilistic functions of ﬁnite state Markov chains .Ann. Math.
Statist., 40(1): 97–115, 1969.
Rabiner89 [27] Lawrence Rabiner. A tutorial on hidden Markov models and selec ted applica-
tions in speech recognition. Proc. IEEE , vol. 77: 257–286, 1989.
R1994 [28] Tobias Ryd´ en. Consistent and asymptotically normal paramet er estimates for
hidden markov models. The Annals of Statistics , pages 1884–1895, 1994.
R1995-1 [29] Tobias Ryd´ en. Consistent and asymptotically normal paramet er estimates for
markov modulated poisson processes. Scandinavian journal of statistics , pages
295–303, 1995.
R1995-2 [30] Tobias Ryd´ en. Estimating the order of hidden markov models. Statistics: A
Journal of Theoretical and Applied Statistics , 26(4):345–354, 1995.
R1996-1 [31] Tobias Ryd´ en. An em algorithm for estimation in markov-modulat ed poisson
processes. Computational Statistics & Data Analysis , 21(4):431–447, 1996.
R1996-2 [32] Tobias Ryd´ en. On identiﬁability and order of continuous-time ag gregated
markov chains, markov-modulated poisson processes, and phase -type distri-
butions. Journal of Applied Probability , 33(3):640–653, 1996.
R1997 [33] Tobias Ryd´ en. On recursive estimation for hidden markov mode ls.Stochastic
Processes and their Applications , 66(1):79–96, 1997.
sason2015upper [34] Igal Sason and Sergio Verd´ u. Upper bounds on the relative en tropy and r´ enyi
divergence as a function of total variation distance for ﬁnite alpha bets. In
2015 IEEE Information Theory Workshop-Fall (ITW) , pages 214–218. IEEE,
2015.
Teicher1963 [35] Teicher, Henry. Identiﬁability of Finite Mixtures. The Annals of Mathematical
Statistics , 34():1265–1269, 1963.
THY2016 [36] Michalis K Titsias, Christopher C Holmes, and Christopher Yau. St atistical
inference in hidden markov models using k-segment constraints. Journal of
the American Statistical Association , 111(513):200–215, 2016.
Viterbi67 [37] Viterbi, A.J. Error bounds for convolutional codes and an asym ptotically
optimum decoding algorithm IEEE Transaction on Information Theory , IT-
13: 260-269, 1967.
School of Statistics and Mathematics, Zhongnan University of Economics and
Law
Email address :hushulan@zuel.edu.cn
Wenlan School of Business, Zhongnan University of Economic s and Law
Email address :wangxinyu@zuel.edu.cn
Universit ´e Clermont Auvergne, Laboratoire de math ´ematiques Blaise Pascal
Email address :Li-Ming.Wu@math.univ-bpclermont.fr