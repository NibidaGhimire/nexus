Variable selection in linear regression models:
choosing the best subset is not always the best choice
Moritz Hankeâˆ—â€ 1, Louis Dijkstra1, Ronja Foraita1, and Vanessa Didelez1
1Leibniz Institute for Prevention Research & Epidemiology { BIPS,
Achterstr. 30, 28359 Bremen (Germany)
December 2022
Abstract
Variable selection in linear regression settings is a much discussed problem. Best
subset selection (BSS) is often considered the intuitive 'gold standard', with its use
being restricted only by its NP-hard nature. Alternatives such as the least abso-
lute shrinkage and selection operator (Lasso) or the elastic net (Enet) have become
methods of choice in high-dimensional settings. A recent proposal represents BSS
as a mixed integer optimization problem so that much larger problems have become
feasible in reasonable computation time. We present an extensive neutral compari-
son assessing the variable selection performance, in linear regressions, of BSS com-
pared to forward stepwise selection (FSS), Lasso and Enet. The simulation study
considers a wide range of settings that are challenging with regard to dimensionality
(with respect to the number of observations and variables), signal-to-noise ratios
and correlations between predictors. As main measure of performance, we used the
best possible F1-score for each method to ensure a fair comparison irrespective of
any criterion for choosing the tuning parameters, and results were conrmed by
alternative performance measures. Somewhat surprisingly, it was only in settings
where the signal-to-noise ratio was high and the variables were (nearly) uncorre-
lated that BSS reliably outperformed the other methods, even in low-dimensional
âˆ—Shared rst authorship
â€ Corresponding author. E-mail: hanke@leibniz-bips.de
1arXiv:2302.12034v1  [stat.ME]  23 Feb 2023settings. Further, the FSS's performance was nearly identical to BSS. Our results
shed new light on the usual presumption of BSS being, in principle, the best choice
for variable selection. Especially for correlated variables, alternatives like Enet are
faster and appear to perform better in practical settings.
Keywords: Best subset selection; Lasso; Linear regression; Mixed integer optimization;
Variable selection
1 Introduction
Selecting a subset of variables as direct predictors for an outcome is a much studied
problem in (linear) regression modelling and has received renewed attention in the context
of high-dimensional data where some variable selection is unavoidable. It might appear
self-evident that best subset selection (BSS) [Beale et al., 1967, Hocking and Leslie,
1967, Garside, 1971] should be the gold standard for variable selection: Clearly, if we
assume there are sdirect predictors and consider allcombinations of variables up to
a subset size ksthe true model has to be one of the candidate models, making
BSS the obvious choice for variable selection. The main reason for dismissing BSS is
that a naive implementation quickly becomes computationally infeasible1with larger
numbers of variables [Hastie et al., 2020]. In their path-breaking work, Bertsimas et al.
[2016] have formulated BSS as a mixed integer optimization problem (MIO) pushing
the boundaries for the feasible number of variables pto be in the thousands while still
searching over moderate subset sizes of k. This now allows a better comparison of BSS
with variable selection methods such as the popular Least absolute shrinkage estimator
(Lasso) [Tibshirani, 1996] or variants thereof, e.g. the adaptive Lasso [Zou, 2006] or the
Elastic net (Enet) and its adaptive version [Zou and Hastie, 2005, Zou and Zhang, 2009].
The convex optimization nature of all these methods enables quick computation even for
millions of variables making them the main methods of choice in high-dimensional settings
[Qian et al., 2020]. While these methods' theoretical and empirical performances have
been studied in much detail [Hastie et al., 2009, Chun and Kele s, 2010, B uhlmann and
van de Geer, 2011, Xu et al., 2012, Houwelingen and Sauerbrei, 2013, Sanchez-Pinto et al.,
2018, Yu et al., 2019, Lima et al., 2020, Wang et al., 2020, Lederer, 2022], the question
1For example, with p= 100 and a subset size k= 15 there are over 2 :5331017possible sets.
2arises how they actually compare empirically to BSS in realistic high-dimensional settings.
This is of much interest especially in the elds of genetics and bioinformatics, where high-
dimensional selection problems are common and L1-,L2- orL1-penalisation approaches
are often used as computationally feasible alternatives to BSS [Li et al., 2018, Atyeo et al.,
2021, Overmyer et al., 2021].
A rst extensive comparison of BSS with the Lasso and a simplied version of the
relaxed Lasso [Meinshausen, 2007] regarding their predictive performance has been carried
out by Hastie et al. [2020]. Perhaps surprisingly, the authors nd that neither BSS nor
the Lasso uniformly dominate each other, and, moreover, that forward step-wise selection
is not worse than BSS, while the relaxed Lasso shows the best performance overall.
An explanation may be found in the dierent bias-variance trade-os of the dierent
approaches which enter all the metrics for assessing predictive accuracy considered by
Hastie et al. [2020]. Their results may, therefore, not hold up for variable selection
performance, since dierent sets of selected variables can give very similar predictions,
but only one set of variables is the true set of direct predictors. Another recent simulation
study using the MIO formulation was carried out by Takano and Miyashiro [2020]. The
authors compared BSS with Lasso based on dierent optimization criteria determining
the subset size k. However, in their study, they only considered a low dimensional setting
withn= 100 observations and p= 25 predictors.
In the present paper, we complement the above studies by specically evaluating the
selection performance of BSS compared to other established approaches, gaining further
insights into the properties of all these methods. Selecting the true direct predictors
of an outcome, relative to a large set of available variables, is a distinct problem from
prediction and of much substantial interest in its own right, for instance in genetics. We
take advantage of the MIO formulation, which allows BSS to be used in high-dimensional
settings. While we choose a similar setting for our simulation study as Hastie et al.
[2020], we also extend their approach in several important ways: For wider applicabil-
ity, we consider more complex situations than just the Toeplitz correlation structure; we
also choose dierent positions of the direct predictors within those correlation structures.
For added realism, we supplement the fully-synthetic simulations with semi-synthetic
simulations where we use the actually observed correlation structure from real data,
namely gene expressions provided by The Cancer Genome Atlas Program [The Can-
3cer Genome Atlas Research Network, 2011]. Overall our simulation has 276 dierent
parameter combinations and the results can be compared in an interactive web-app at
https://bestsubset.bips.eu .
All methods considered in the present paper require choosing a tuning parameter or
subset size potentially aecting which and how many variables are selected. To enable
a neutral and fair comparison, we choose for each method its optimal tuning parameter
(or subset size) in terms of its best achievable F1-score. This allows us to assess the
best possible variable selection performance, separating this from the issue of choosing
a tuning parameter. In practice, there appears to be no gold-standard for choosing the
tuning parameter, especially in high-dimensions. Instead, to ensure a fair and practically
feasible comparison of the methods we use an alternative approach by choosing the tuning
parameters to obtain a given number of selected variables, i.e. subset size.
The paper is organized as follows: The methods section describes the selection proce-
dures under investigation and their theoretical properties. Subsequently, we describe the
setup of the simulation study as well as our ndings. Finally, we will draw conclusions
and give an outlook on future work in the last section.
2 Methods
Given a vector of responses y2Rn, a design matrix X2Rnp, a vector of coecients
2Rpand a noise vector 2RnwithiN (0;2) for independent i= 1;:::;n , we
assume the linear model
y=X+ (1)
where xj,j= 1;:::;p , has been standardized such thatPn
i=1xi;j= 0 andn 1Pn
i=1x2
i;j=
1. We further assume to be sparse in the sense that for s=Pp
j=1I(j6= 0) we have
s=O(nc) for 0< c < 1 [Meinshausen and B uhlmann, 2006, Zhao and Yu, 2006]. Let
^denote an estimator of and supp() the support of a vector, i.e. indicating which
elements are non-zero. We say that an estimator or procedure is selection consistent if
supp( ^) converges in probability to supp( ).
While the ordinary least squares (OLS) estimator is the best linear unbiased estimator
forwhenn>p , it is not suited for variable selection where the aim is to discriminate
4zeros from non-zeros in . Forj= 0, it can be shown that the OLS estimate is
^j=Op
n 1logn
[Horowitz, 2015], i.e., it does not select a model for nite nas
estimated coecients will not be exactly zero.
For variable selection, dierent penalized least squares approaches can be formulated
as an optimization problem of the form
^() = arg min
jjy Xjj2
2+jjjjq; (2)
where the penalty term jjjjq:=Pp
j=1jjjq1=q
denotes the Lq-norm with special case
jjjj0:=Pp
j=1I(j6= 0). The tuning parameter 0 controls the strength of the
penalty, where for q <2 a largershrinks ^jmore towards 0 and fewer variables are
selected. Choosing can be based on criteria like AIC, BIC, cross-validation or stabil-
ity procedures [Akaike, 1998, Schwarz, 1978, Picard and Cook, 1984, Meinshausen and
B uhlmann, 2010] each with dierent goals and assumptions [Shao, 1997, Yang, 2005, Ar-
lot and Celisse, 2010]. In the following, we will focus on some of the most prominent
penalization approaches.
2.1 Best subset selection
Using theL0-norm in (2) is known as Best Subset Selection (BSS) and can be formulated
as the following discrete optimization problem
^BSS= arg min
jjy Xjj2
2 s.t.jjjj0k (3)
withk2Ndetermining the maximal number of non-zeros in ^. Zhang and Zhang
[2012] showed that if a uniform signal strength condition for the smallest true predictor
inholds, BSS can achieve selection consistency. Shen et al. [2013] dened a degree of
separation that describes how dicult it is to distinguish the true model from all other
models in terms of the projection of ybased on ^. As a necessary condition for a L0-
norm based penalty approach to be selection consistent, they showed that the degree of
separation has to be larger than a threshold that is a function of p,nand2.
The minimization of (3) is known to be NP-hard [Natarajan, 1995, Barron et al.,
1999] and state-of-the-art algorithms have been capable of solving BSS problems in a
5feasible amount of time only if p<50. Recently Bertsimas et al. [2016] reformulated (3)
as a mixed integer optimization (MIO) problem
arg min
;zjjy Xjj2
2
s.t. (i;1 zi) : SOS-1; i = 1;:::;p
zi2f0;1g; i = 1;:::;p
pX
i=1zik:(4)
where SOS-1 denotes a Specially Ordered Set of Type 1, i.e., at most one element of ( i;1 
zi) can be non-zero. The authors showed that this reformulation guarantees optimality
in the sense of (3). Due to ecient MIO solvers like Gurobi [Gurobi Optimization, 2021],
problem (4) can be solved in minutes even when pis in the 1000s, nin the 100s and a
moderate value kis selected. However, certifying the optimality of the solution can take
much more time. For example, the Gurobi solver uses a lower and upper bound criterion
to nd a solution, where the convergence rate of the lower bound criterion is much faster
[Hastie et al., 2020].
2.2 Forward step-wise selection
While BSS is limited through its NP-hard nature, step-wise selection is a popular alter-
native. It gradually adds (removes) variables to (from) a model based on a model t
criterion. Due to this greedy strategy, these algorithms are computationally less chal-
lenging with complexity O(p2). However, step-wise selection approaches are known to
have numerous drawbacks: they result in unstable nal models that are sensitive to small
changes in the data [James and McCulloch, 1990, Breiman, 1996, Whittingham et al.,
2006], and they are only locally optimal, and often miss direct predictors while selecting
irrelevant variables [Derksen and Keselman, 1992, Smith, 2018]. Moreover, inference is
problematic as these methods usually do not account for multiple testing issues [Whit-
tingham et al., 2006, Mundry and Nunn, 2009]. Despite these drawbacks, we will consider
forward step-wise selection (FSS) [Efron et al., 2004, Hastie et al., 2009] in our simula-
tion study, because it can be interpreted as greedy heuristic version of BSS [Hastie et al.,
2020]. It is dened as an iterative algorithm and starts with an empty active set model
6A0=;and^(k)
A0=0. At each step t= 1;:::;k , the variable jtis selected that maximizes
arg max
jt=2At 1x>
jt(I PAt 1)y
jj(I PAt 1)xjtjj2
wherePAt 1denotes the projection of yonto the column space of XAt 1. Givenjt, the
active set is updated as At=At 1[fjtgand used to estimate
^(t)
At= arg min
jjy XAtjj2
2;
^(t)
nfAtg= 0
wherenfAtgdenotes the set of predictors not selected in step t.
2.3 Lasso and Elastic net
The least absolute shrinkage and selection operator (Lasso) [Tibshirani, 1996] uses an
L1-norm as penalty term in (2) and shrinks all estimated coecients towards zero, where
some estimates will be exactly zero for a suciently large tuning parameter . Hence,
the Lasso can be used for variable selection as only variables with non-zero coecients
are selected. This property will be used in our simulation study. The total number of
zero coecients is controlled by , where larger values will result in sparser models. The
Lasso can be combined with fast algorithms so that problems with pin the 10;000's can
easily be solved; settings for which BSS with MIO can no longer be applied. However,
the Lasso also has several drawbacks. Firstly, it only allows up to nnon-zero regression
coecients, which can be a limiting factor if np, [Hastie et al., 2009]. Secondly, if
irrelevant variables are highly correlated with direct predictors of the outcome y, the
Lasso selects almost arbitrarily of those true and false variables and is known not to be
consistent, not even for the sign of the coecient. Furthermore, if there is a high pairwise
correlation within a set of variables and they all are true direct predictors for y, the Lasso
tends to select only one of these variables [Zou and Hastie, 2005, Xu et al., 2012]. Thus,
it cannot guarantee consistent variable selection [Zhao and Yu, 2006]. To address some
of these drawbacks, dierent modications of the Lasso have been proposed. They rely
on a-priori knowledge about the data generating process or the functional relationship
between variables [Tibshirani et al., 2005, Zou, 2006, Meinshausen, 2007, Friedman et al.,
72010, Ala z et al., 2013, Simon et al., 2013].
An alternative is the Elastic net (Enet) [Zou and Hastie, 2005], which can be formu-
lated as a weighted combination of the Lasso and an additional L2-penalization (Ridge)
term
^Enet= arg min
jjy Xjj2
2+jjjj1+ (1 )jjjj2
2: (5)
The second tuning parameter 0 1 controls the weighting between the L1- and
L2-penalty. Here the L1-penalty induces a Lasso-type variable selection, while the L2-
penalty helps with highly correlated variables by increasing the diagonal entries of the
covariance matrix X>X. The latter guarantees a positive-denite covariance matrix so
that it is possible for all pestimated coecients to be non-zero. More importantly, the
L2-penalty can be interpreted as an articial decorrelation of the variables, making it
easier to jointly select highly correlated variables if they are all direct predictors for y
[Zou and Hastie, 2005].
3 Simulation study
3.1 Simulation design and evaluation for synthetic datasets
To evaluate the performance of BSS, FSS, Lasso and the Enet for variable selection,
we simulated synthetic data from a linear model (1) with X Np(0;) and y
Nn(X;2I). Since real-world applications of variable selection often have a small signal-
to-noise ratio , we followed Hastie et al. [2020] and set 2=>
with 0:056.
We considered both a low-dimensional ( n= 1000 and p= 100), high-dimensional
(n= 100 and p= 1000) and an intermediate ( n= 500 and n= 500) setting. To assess
the eect of correlation between predictors, we used an uncorrelated structure and the
standard Toeplitz structure. The latter was created by setting the pairwise correlation
between two variables x;uandx;vforu;v= 1;:::;p asju vjwith2 f0:35;0:7g.
Although the Toeplitz structure is a popular correlation structure in simulations, it can
be implausible for some applications. For example, in genetic epidemiology, it is often
reasonable to assume that genes within a functional group are correlated with each other,
but that they are nearly independent of genes from other functional groups. Hence, we
8simulated data with correlations following a block structure for which variables were
grouped into blocks of size 10. Pairwise correlations within the blocks were set to 2
f0:35;0:7g; pairwise correlations between variables outside of the blocks were set to = 0.
The number of direct predictors was set to s= 10 in all settings and their position was
either consecutive or equally spaced along the sequence of variables. For the consecutive
positioning, we set the rst ten coecients to be non-zeros, while for the equally spaced
positioning, we set every tenth coecient to be non-zero (see Figure 1). In all scenarios,
a non-zero direct predictor was set to j= 1. Overall, we investigated 270 dierent
scenarios and each one was repeated 100 times. We used the F1-score
Figure 1: Schematic representation of the dierent correlation structures and the position-
ing of the non-zeros coecients (orange). The Toeplitz structure, with all ten non-zeros
in consecutive positioning (a) and equally spaced positioning (b), has a gradually de-
creasing correlation between all variables. The second row shows a block structure where
all coecients of one block are set to non-zero (c) or only one coecient of each block is
set as non-zero for 10 blocks (d). The correlation between variables within one block is
always the same while the correlation between variables of dierent blocks is set to zero.
9F1= 2PR
P+R
as main performance measure, where P=TP
TP+FPis the precision and R=TP
TP+FNis
the recall,TPthe number of true positives, FPthe number of false positives, and FN
the number of false negatives. Low values reect that an increase in recall can only be
achieved by lowering the precision considerably and vice versa. In this sense, high values
reect a good recall without sacricing precision. For imbalanced confusion matrices,
the F1-score can give misleading results compared to other measures like Matthew's
correlation coecient (MCC) [Hand and Christen, 2018, Chicco and Jurman, 2020, Zhu,
2020]. However, this is unlikely for high dimensional variable selection problems as the
assumed sparsity in these settings implies large numbers of (true) negatives and thus very
similar F1-score and MCC values. For the sake of completeness, we still computed the
MCC and the F2-score, the latter placing more weight on recall than the F1-score. We
found very similar results; MCC and F2-score results are provide the in the web-app.
Further, we investigate the methods' performances for a range of given subset sizes k.
3.2 Simulation design and evaluation for semi-synthetic datasets
To assess the performance of the methods in a real-world correlation setting, we gener-
ated semi-synthetic data [B uhlmann and Mandozzi, 2014, Wang et al., 2020] using gene
expression data from ovarian cancer samples from The Cancer Genome Atlas Program
[The Cancer Genome Atlas Research Network, 2011]. The dataset contains n= 594
units andp= 22;277 genes and can be accessed online [Tucker et al., 2014, Wang et al.,
2020]. For the low-dimensional setting, we randomly selected p= 100 genes and used all
n= 594 observations. For a high-dimensional setting, we randomly selected p= 1000
genes andn= 100 observations. The outcome ywas simulated in analogy to the above
fully synthetic settings, where we set the signal-to-noise ratio = 0:42;1:22;3:52 and the
coecient of the 10 direct predictors to j= 1. In determining the direct predictors,
we followed the approach of B uhlmann and Mandozzi [2014], i.e., out of the p= 100 (or
p= 1000) variables, we selected the pair with the highest absolute correlation as the rst
two direct predictors x1andx2. We then chose those eight genes exhibiting the largest
correlation with x1as further true direct predictors. Each of the semi-synthetic scenarios
was repeated 100 times. The empirical mean correlation across pairs of true direct predic-
10tors for the low-dimensional setting was  0:19, and 0:37 for the high-dimensional
setting. The methods' performances were again evaluated on the basis of the F1-score.
3.3 Selection of tuning parameters
All methods under consideration rely either on choosing a tuning parameter or a set size
ka-priori. The Enet further requires two tuning parameters. Obviously, the performance
of each variable selection method depends on the choice of these tuning parameter(s)
or subset size. Since we are interested in the best possible performance of each method
regarding variable selection, we used a grid of tuning parameters andfor Lasso/Enet,
i.e.= 0:1;0:2;:::; 0:9 and 1000 values for were the largest returns an empty model
and the smallest a full model. For BSS and FSS, we used subset sizes of k= 1;:::; 15,
which means that for k10 both methods had the chance to nd the true model. In
the nal step, we only chose those tuning values/set sizes that gave the highest F1-score
(or MCC or F2-score) for that method in the considered setting. We consider this `best
possible F1-score' as an indicator of a methods' potential. In practice, this choice of
tuning parameter is not feasible. Dierent practical approaches for choosing the tuning
parameter(s) have been developed, such as the Akaike / Bayesian information criteria
[Akaike, 1998, Schwarz, 1978], or (re-)sampling based techniques like cross-validation
[Allen, 1974, Stone, 1974] and stability approaches [Meinshausen and B uhlmann, 2010,
Liu et al., 2010]. However, there is no unique, let alone neutral, standard choice, especially
in in high-dimensions and when the aim is to identify the true variables as opposed to
mere prediction of the outcome; moreover, re-sampling is computationally intractable
with BSS. For a more practical and fair assessment, we therefore additionally inspected
the performance for each method by choosing the tuning parameters to achieve a given
number of selected variables, which we varied over the range k= 1;2;:::; 15. This
reects and provides additional insights into the methods' performances when the tuning
parameters are set such that a desired number kof variables (subset size) is selected.
Although MIO allows for more ecient BSS, it can potentially still run for hours,
making an extensive comparison infeasible without a time limit. Following the suggestion
of Bertsimas et al. [2016] and Hastie et al. [2020], we set the time limit for the synthetic
data and the semi-synthetic data to 3 minutes and 10 minutes, respectively, for each value
ofkin each simulation run. These time limits are sucient for the Gurobi solver to nd a
11solution. However, certifying the solution (proving optimality) can take much longer. As
the time limit might disadvantage BSS, we further investigated the number of certied
optimal BSS solutions and the impact of varying the time limit on the performance
measures in selected settings. In addition, we analyzed the impact of varying the subset
sizeskon the BSS performance for the certied solutions.
4 Results
Surprisingly, BSS reliably outperformed the other methods only in settings with a high
signal-to-noise ratio and when the variables were uncorrelated. Even in a low-dimensional
synthetic data setting, with the number of observations ten times the number of variables,
the selection performance of BSS drops dramatically if the true predictors are moderately
correlated. In those cases, BSS is even outperformed by the Lasso, which is known to be
inconsistent for variable selection when the true predictors are highly correlated. Inter-
estingly, the much simpler FSS achieves similar performance as BSS in almost all settings
and, in some, performs even slightly better. The results for the synthetic settings are
corroborated by those for the semi-synthetic data settings with the real-world correla-
tion structures. Again, the BSS and FSS performances are similar and, especially in the
high-dimensional setting, the performance of BSS does not improve much with a high
signal-to-noise ratio. Even in a low-dimensional setting, BSS is clearly inferior to the
Enet and the Lasso when the signal-to-noise ratio is low to moderate. The evaluation of
the selection performance under dierent subset sizes supports our approach to use the
best possible F1-score for showing the potential of the methods under consideration. In
no setting could we observe an unstable F1-score for an increasing subset size; rather, in
correlated settings, BSS and FSS are often incapable of selecting more than a few true
direct predictors even for larger subset sizes.
To give some details on the BSS performance, we address a selection of four synthetic
data settings. These settings dier with regard to (i) dimensionality, (ii) the position of
the non-zero coecients, (iii) correlation structure, and (iv) correlation strength. We will
then report results from the semi-synthetic data settings, and investigate the possible role
of the time limit on certication. The remaining results support our main conclusions
and are shown as supplementary material or can be seen in the interactive web-app at
12https://bestsubset.bips.eu . The supplementary material and the web-app also show
the results for the MCC and the F2-score (both being similar to the results of the F1-
score), and the performance with respect to a range of given subset sizes. Note that
for the Enet, we only show results with 2f0:1;0:5;0:9g, representing a mostly Ridge-
weighted, a balanced and a mostly Lasso-weighted Enet, respectively. All results for the
Enet with all nine dierent = 0:1;0:2;:::0:9 are accessible through our web-app.
4.1 Variable selection performance
For the synthetic datasets, all methods perform, in general, better with a Toeplitz corre-
lations structure than with a block structure. This seems plausible since the correlations
under the Toeplitz structure are weaker than within the blocks. For the high dimensional
block setting with equally spaced non-zeros, = 0:35 and low , all methods have a
relatively low best possible F1-score (see Figure 2). FSS and BSS show nearly identical
results and only outperform the Lasso and the Enet when the signal-to-noise ratios are
high. Lasso and Enet show better corresponding recall values on average except for very
highwhile the variability is high for  <0:25.
Figure 3 shows the results for a high-dimensional Toeplitz structure setting with
consecutive non-zeros. The methods exhibit large dierences: for 2:07 the Ridge-
weighted Enet ( = 0:1) performs very well, reaching a high F1-score close to 1, clearly
beneting from the decorrelation property. In comparison, the other methods cannot
cope with highly correlated direct predictors as seen from the low corresponding recall
values and low F1-scores of the Lasso, and the weak performances of BSS and FSS. Figure
3 shows that the recall of BSS and FSS improves slightly with an increase of .
In the low-dimensional block setting with = 0:7 and equally spaced non-zeros, BSS
and FSS barely outperform Lasso and Enet for most . Figure 4 shows that this is mainly
due to the relatively high precision. However, when the non-zeros are consecutive, the
performance of FSS and BSS decreases drastically. Even in the low-dimensional case, the
signal-to-noise ratio has to be very large ( = 6) to achieve comparable results to Lasso
and Enet (see Figure 5). In these cases, the F1-scores of FSS and BSS are dominated by
their poor recall.
The results of the semi-synthetic data simulations show a similar tendency. Again, the
performances of BSS and FSS are very similar. In the high-dimensional setting (Figure
13Figure 2: High-dimensional synthetic data setting ( p= 1000,n= 100,= 0:35).
Boxplots of best possible F1-scores and corresponding precision and recall values for a
block correlation setting with equally spaced non-zero coecients.
6), Ridge-weighted Enet versions ( = 0:1) achieve an average best possible F1-score
>0:5 for= 0:42, while BSS' average best possible F1-score is <0:5 even for= 3:52.
For the low-dimensional setting (Figure 7), BSS is again clearly inferior to Enet and
Lasso for= 0:42 while for 1:22 all methods perform very well. The results are
similar to the block setting with = 0:35 for the fully synthetic data simulations (see the
supplementary material or the interactive web-app).
Figure 8 illustrates the role of the dimensionality and the amount of correlation be-
tween true direct predictors on the variable selection performance in a block setting. All
methods perform worse in high dimensional settings but they dier strongly in their per-
formance when large correlations are present. While the Enet benets from the higher
14Figure 3: High-dimensional synthetic data setting ( p= 1000,n= 100,= 0:7). Boxplots
of best possible F1-scores and corresponding precision and recall values for a Toeplitz
correlation setting with consecutive non-zero coecients.
correlations due to its grouping eect [Zou and Hastie, 2005, Zhou, 2013] Lasso and es-
pecially BSS and FSS perform worse. Even in the low-dimensional setting with a relative
high signal-to-noise ratio they hardly achieve a perfect F1-score unlike the competitors
when there is a large correlation.
When we choose all tuning parameters to achieve given subset sizes, we nd essentially
the same picture as described above regarding the role of correlated variables and signal-
to-noise ratios. Moreover, no method exhibited any unstable results (see Figure 9 and
the web-app). We conclude that slightly sub-optimal tuning parameters or given subset
sizes do not cause substantial dierences in the performance measures.
15Figure 4: Low-dimensional synthetic data setting ( p= 100,n= 1000,= 0:7). Boxplots
of best possible F1-scores and corresponding precision and recall values for a block-
structured correlation setting with equally spaced non-zero coecients.
4.2 Time limit, certication of solutions and subset sizes of BSS
Although we needed to set a time limit for the Gurobi solver to certify the solutions of
each subset size, we conjecture that the poor performance of BSS cannot be explained by
non-certied runs or the time limit itself. We investigated this by comparing dierent time
limits, up to one hour, for (the same) 100 runs in one setting (see Figure 10; this is the
low-dimensional block setting with consecutive non-zeros, signal-to-noise ratio = 0:42
and correlation = 0:7). Panels 10A and 10B suggest that while increasing the time
limit leads to a higher proportion of certied runs it does not aect the best possible
F1-scores. However, the F1-score also depends on the specied subset size, with larger
subsets taking longer to certify and even one hour possibly being insucient; too short
16Figure 5: Low-dimensional synthetic data setting ( p= 100,n= 1000,= 0:7). Boxplots
of best possible F1-scores and corresponding precision and recall values for a block-
structured correlation setting with consecutive non-zero coecients.
a time limit might reduce the chances of BSS to identify more direct predictors. Since
the number of direct predictors is s= 10, only a specied subset size of k= 10 can
theoretically result in a perfect F1-score of 1. Panels 10C and 10D suggest that neither
the time limit nor the certication of the result makes a dierence to the performance of
BSS. In fact, the BSS methods appear to `hit a ceiling' with selecting (in this particular
setting) at most ve or six true predictors even when k10 and even when certied.
In consequence, the number of false positives necessarily increases to make up the subset
sizek. Note that Enet with = 0:1 achieved for this setting in all runs a perfect F1-score
(see Figure 5).
We see a similar behaviour of BSS for the low-dimensional semi-synthetic data with
17Figure 6: High-dimensional semi-synthetic data setting ( p= 1000,n= 100) with boxplots
of best possible F1-scores and corresponding precision and recall values.
= 0:42 (see Figure 57 supplementary material). As the correlation between the true
predictors is weaker ( 0:19) the performance of BSS is generally better. However,
regardless of time limit or certication, BSS appears to rarely select all 10 true direct
predictors even when k>10.
5 Conclusion and Discussion
We carried out an extensive simulation study to compare the performance of BSS and
its competitors regarding variable selection in a linear regression setting. We investi-
gated a broad range of parameter constellations as well as dierent and, maybe more
realistic correlation structures than previous works have done. Further, we evaluated the
18Figure 7: Low-dimensional semi-synthetic data setting ( p= 100,n= 594) with boxplots
of best possible F1-scores and corresponding precision and recall values.
methods on semi-synthetic data with actual real-world correlation structures in high- and
low-dimensional settings. Our results show that the Enet and the Lasso outperform BSS
and FSS in most scenarios. This was unexpected since BSS must have considered the true
model as a candidate model in every single run (since k= 15> s= 10). Perhaps even
more surprisingly, we found that BSS also performs relatively poorly in low-dimensional
settings if there is a moderate to high correlation between the true predictors. BSS out-
performs the other methods in the synthetic simulations only under nearly uncorrelated
predictors and when the signal-to-noise ratio is very high, which is unrealistic in most
practical situations. We also argue that the poor performance of BSS cannot be explained
by non-certied runs or time limits. Moreover, we detected a weakness of BSS in selecting
some but not all true direct predictors without adding considerably more false positives;
19Figure 8: Eect of correlation strength and dimensionality in a block-structured correla-
tion setting with consecutive non-zero coecients and signal-to-noise ratio = 2:07.
Figure 9: Average variable selection performance based on the subset size kfor a low-
dimensional block-structured correlation setting ( p= 100,n= 1000,= 0:7,= 0:71)
with consecutive non-zero coecients. The dashed vertical line indicated k=s, i.e.
where the number of selected variables equals the number of true direct predictors.
the problem was noticeable in all our simulations settings, not only in the settings inves-
tigated for certication, whenever the signal-to-noise ratio was not high or the true direct
predictors were correlated (see interactive web-app and supplementary material). This
suggests that the objective function (3) and the discrete nature of the optimisation does
not easily allow BSS to distinguish between further true predictors and false ones in such
settings. To the best of our knowledge, our work is the rst to highlight this shortcoming
of BSS.
In summary, the L0-norm penalization seems only appropriate in situations where a
high signal-to-noise ratio and (nearly) uncorrelated true predictors are plausible. Alter-
natively, based on our empirical results, using a Ridge-weighted Enet seems a good choice
for settings with correlated predictors.
20Our complete set of results can be accessed via a web-app at https://bestsubset.bips.eu
and can be found in the supplementary material. We would like to reiterate that our sim-
ulation study was designed to evaluate the variable selection methods, and not to assess
the criteria for selecting the tuning parameters or subset size. In addition, we focused on
scenarios with non-zero predictors of the same positive size. It is reasonable to assume
that dierent sizes and signs will alter the performances. However, there is no reason to
assume that this would specically improve the performance of BSS to the other methods.
In future research, it might be promising to combine BSS or FSS with approaches
like Lasso or Enet so as to preselect or decorrelate covariates. Hence, further insights
on the role of the correlation structure for variable selection performance of BSS are
desirable. This may not only resolve the computational complexity of BSS but also its
poor performance with correlated predictors.
Acknowledgements
We acknowledge nancial support by the Deutsche Forschungsgemeinschaft (DFG) through
project FO 1045/2-1.
Conict of Interest
The authors have declared no conict of interest.
Data Availability
All (semi)-synthetic data of this simulation study can be generated by the R-code under
â€¢https://github.com/bips-hb/bsscomparison ,
â€¢https://github.com/bips-hb/simsham and
â€¢https://github.com/bips-hb/semisynthetic data simulation .
The TCGA data for generating the semi-synthetic data simulation is available from the
authors website
https://bioinformatics.mdanderson.org/Supplements/ResidualDisease/
21All results of the simulation can be accessed under https://bestsubset.bips.eu .
All raw results of the medium- and high-dimensional synthetic settings can be downloaded
from
https://www.bips-institut.de/fileadmin/downloads/BestSubsetResults.zip
Raw data of all the semi-synthetic and low-dimensional synthetic settings are stored in
in the repository https://github.com/bips-hb/bsscomparison .
References
H. Akaike. Information theory and an extension of the maximum likelihood principle. In
E. Parzen, K. Tanabe, and G. Kitagawa, editors, Selected Papers of Hirotugu Akaike ,
volume 1, pages 199{213. Springer New York, New York, NY, 1 edition, 1998.
C. M. Ala z, A. Barbero, and J. R. Dorronsoro. Group fused lasso. In V. Mladenov,
P. Koprinkova-Hristova, G. Palm, A. E. P. Villa, B. Appollini, and N. Kasabov, editors,
Articial Neural Networks and Machine Learning - ICANN 2013 , pages 66{73, Berlin,
Heidelberg, 2013. Springer Berlin Heidelberg.
D. M. Allen. The relationship between variable selection and data agumentation and a
method for prediction. Technometrics , 16(1):125{127, 1974.
S. Arlot and A. Celisse. A survey of cross-validation procedures for model selection.
Statistics Surveys , 4:40{79, 2010.
C. Atyeo, K. M. Pullen, E. A. Bordt, S. Fischinger, J. Burke, A. Michell, M. D. Slein,
C. Loos, L. L. Shook, A. A. Boatin, et al. Compromised SARS-CoV-2-specic placental
antibody transfer. Cell, 184(3):628{642, 2021.
A. Barron, L. Birg e, and P. Massart. Risk bounds for model selection via penalization.
Probability Theory and Related Fields , 113(3):301{413, Feb 1999.
E. M. L. Beale, M. G. Kendall, and D. W. Mann. The discarding of variables in multi-
variate analysis. Biometrika , 54(3/4):357{366, 1967.
D. Bertsimas, A. King, and R. Mazumder. Best subset selection via a modern optimiza-
tion lens. The Annals of Statistics , 44(2):813{852, 2016.
22L. Breiman. Bagging predictors. Machine Learning , 24:123{140, 1996.
P. B uhlmann and J. Mandozzi. High-dimensional variable screening and bias in subse-
quent inference, with an empirical comparison. Computational Statistics , 29(3):407{
430, 2014.
P. B uhlmann and S. van de Geer. Statistics for high-dimensional data: Methods, theory
and applications . Springer, Heidelberg, 2011.
D. Chicco and G. Jurman. The advantages of the Matthews correlation coecient (MCC)
over F1 score and accuracy in binary classication evaluation. BMC Genomics , 21(1):
1{13, 2020.
H. Chun and S. Kele s. Sparse partial least squares regression for simultaneous dimension
reduction and variable selection. Journal of the Royal Statistical Society. Series B
(Statistical Methodology) , 72(1):3{25, 2010.
S. Derksen and H. J. Keselman. Backward, forward and stepwise automated subset
selection algorithms: Frequency of obtaining authentic and noise variables. British
Journal of Mathematical and Statistical Psychology , 45(2):265{282, 1992.
B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of
Statistics , 32(2):407{499, 04 2004.
J. Friedman, T. Hastie, and R. Tibshirani. A note on the Group Lasso and a Sparse-Group
Lasso, 2010.
M. J. Garside. Some computational procedures for the best subset problem. Journal of
the Royal Statistical Society Series C , 20(1):8{15, March 1971.
L. Gurobi Optimization. Gurobi optimizer reference manual, 2021. URL http://www.
gurobi.com .
D. Hand and P. Christen. A note on using the f-measure for evaluating record linkage
algorithms. Statistics and Computing , 28(3):539{547, may 2018.
T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning: data
mining, inference and prediction . Springer, New York, 2 edition, 2009.
23T. Hastie, R. Tibshirani, and R. Tibshirani. Best Subset, Forward Stepwise or Lasso?
Analysis and Recommendations Based on Extensive Comparisons. Statistical Science ,
35(4):579 { 592, 2020.
R. R. Hocking and R. N. Leslie. Selection of the Best Subset in regression analysis.
Technometrics , 9(4):531{540, 1967.
J. L. Horowitz. Variable selection and estimation in high-dimensional models. Canadian
Journal of Economics/Revue canadienne d' economique , 48(2):389{407, 2015.
H. C. Houwelingen and W. Sauerbrei. Cross-validation, shrinkage and variable selection
in linear regression revisited. Open Journal of Statistics , 3:79{102, 2013.
F. C. James and C. E. McCulloch. Multivariate analysis in ecology and systematics:
Panacea or Pandora's box? Annual Review of Ecology, Evolution, and Systematics ,
21:129{166, 1990.
J. Lederer. Fundamentals of High-Dimensional Statistics: With Exercises and R Labs .
Springer, Heidelberg, 2022.
X. Li, H. Wang, J. Long, G. Pan, T. He, O. Anichtchik, R. Belshaw, D. Albani, P. Edison,
E. K. Green, et al. Systematic analysis and biomarker study for Alzheimer's disease.
Scientic reports , 8(1):1{14, 2018.
E. Lima, P. Davies, J. Kaler, F. Lovatt, and M. Green. Variable selection for inferen-
tial models with relatively high-dimensional data: Between method heterogeneity and
covariate stability as adjuncts to robust selection. Scientic Reports , 10, 12 2020.
H. Liu, K. Roeder, and L. A. Wasserman. Stability Approach to Regularization Selec-
tion (StARS) for high-dimensional Graphical Models. Advances in neural information
processing systems , 24 2:1432{1440, 2010.
N. Meinshausen. Relaxed Lasso. Computational Statistics and Data Analysis , 52:374{393,
09 2007.
N. Meinshausen and P. B uhlmann. High-dimensional graphs and variable selection with
the Lasso. The Annals of Statistics , 34(3):1436{1462, 2006.
24N. Meinshausen and P. B uhlmann. Stability Selection. Journal of the Royal Statistical
Society, Series B , 72:417{473, 2010.
R. Mundry and C. Nunn. Stepwise model tting and statistical inference: turning noise
into signal pollution. The American Naturalist , 173(1):119{123, 2009.
B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on
Computing , 24(2):227{234, 1995.
K. A. Overmyer, E. Shishkova, I. J. Miller, J. Balnis, M. N. Bernstein, T. M. Peters-
Clarke, J. G. Meyer, Q. Quan, L. K. Muehlbauer, E. A. Trujillo, et al. Large-scale
multi-omic analysis of COVID-19 severity. Cell systems , 12(1):23{40, 2021.
R. R. Picard and R. D. Cook. Cross-validation of regression models. Journal of the
American Statistical Association , 79(387):575{583, 1984.
J. Qian, Y. Tanigawa, W. Du, M. Aguirre, C. Chang, R. Tibshirani, M. A. Rivas, and
T. Hastie. A fast and scalable framework for large-scale and ultrahigh-dimensional
sparse regression with application to the UK Biobank. PLOS Genetics , 16(10):1{30,
10 2020.
L. Sanchez-Pinto, L. Venable, J. Fahrenbach, and M. Churpek. Comparison of variable
selection methods for clinical predictive modeling. International Journal of Medical
Informatics , 116, 05 2018.
G. Schwarz. Estimating the dimension of a model. The Annals of Statistics , 6:461{464,
1978.
J. Shao. An asymptotic theory for linear model selection. Statistica Sinica , 7(2):221{242,
1997.
X. Shen, W. Pan, Y. Zhu, and H. Zhou. On constrained and regularized high-dimensional
regression. Annals of the Institute of Statistical Mathematics , 65(5):807{832, 10 2013.
N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. A Sparse-Group Lasso. Journal of
Computational and Graphical Statistics , 22(2):231{245, 2013.
G. Smith. Step away from stepwise. Journal of Big Data , 5(1):32, 2018.
25M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of
the Royal Statistical Society: Series B (Methodological) , 36(2):111{133, 1974.
Y. Takano and R. Miyashiro. Best subset selection via cross-validation criterion. TOP ,
28(2):475{488, 2020.
The Cancer Genome Atlas Research Network. Integrated genomic analyses of ovarian
carcinoma. Nature , 474:609{615, 2011.
R. Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the Royal
Statistical Society (Series B) , 58:267{288, 1996.
R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness
via the Fused Lasso. Journal of the Royal Statistical Society, Series B , pages 91{108,
2005.
S. L. Tucker, K. Gharpure, S. M. Herbrich, A. K. Unruh, A. M. Nick, E. K. Crane, R. L.
Coleman, J. Guenthoer, H. J. Dalton, S. Y. Wu, R. Rupaimoole, G. Lopez-Berestein,
B. Ozpolat, C. Ivan, W. Hu, K. A. Baggerly, and A. K. Sood. Molecular biomarkers of
residual disease after surgical debulking of high-grade serous ovarian cancer. Clinical
Cancer Research , 20(12):3280{3288, 2014.
F. Wang, S. Mukherjee, S. Richardson, and S. Hill. High-dimensional regression in prac-
tice: an empirical study of nite-sample prediction, variable selection and ranking.
Statistics and Computing , 30, 05 2020.
M. J. Whittingham, P. A. Stephens, R. B. Bradbury, and R. P. Freckleton. Why do we
still use stepwise modelling in ecology and behaviour? Journal of Animal Ecology , 75
(5):1182{1189, 2006.
H. Xu, C. Caramanis, and S. Mannor. Sparse algorithms are not stable: A no-free-lunch
theorem. IEEE Transactions on Pattern Analysis and Machine Intelligence , 34(1):
187{193, Jan 2012.
Y. Yang. Can the strengths of AIC and BIC be shared? A conict between model
indentication and regression estimation. Biometrika , 92(4):937{950, 2005.
26X. Yu, H. Ge, D. Lu, M. Zhang, Z. Lai, and R. Yao. Comparative study on variable selec-
tion approaches in establishment of remote sensing model for forest biomass estimation.
Remote Sensing , 11(12), 2019.
C.-H. Zhang and T. Zhang. A general theory of concave regularization for high-
dimensional sparse estimation problems. Statistical Science , 27(4):576{593, 11 2012.
P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning
Research , 7:2541{2563, 2006.
D.-X. Zhou. On grouping eect of elastic net. Statistics & Probability Letters , 83(9):
2108{2112, sep 2013.
Q. Zhu. On the performance of matthews correlation coecient (mcc) for imbalanced
dataset. Pattern Recognition Letters , 136:71{80, 2020.
H. Zou. The Adaptive Lasso and its oracle properties. Journal of the American Statistical
Association , 101(476):1418{1429, 2006.
H. Zou and T. Hastie. Regularization and variable selection via the Elastic Net. Journal
of the Royal Statistical Society, Series B , 67:301{320, 2005.
H. Zou and H. H. Zhang. On the Adaptive Elastic-Net with a diverging number of
parameters. Annals of Statistics , 37(4):1733{1751, 08 2009.
27Figure 10: BSS variable selection performance based on dierent time limits by certi-
cation for a block-structured low-dimensional setting with = 0:7, consecutive true
predictors and = 0:42. For all time limits, the same 100 simulated datasets have been
used. Panels A and B show the best possible F1-scores across kwith respect to the time
limit, the latter separately for certied vs. non-certied runs. Panels C and D show the
F1-score and the number of selected true direct predictors with respect to the subset
sizekand the time limit for each subset size. The orange lines in C and D represent
the theoretical best value that could be achieved for the given subset size. The numbers
underneath the boxplots are the corresponding number of runs for each boxplot.
28