ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on
Chat-based Large Language Models
Zhipeng Chen1,3‚àó, Kun Zhou2,3‚àó, Beichen Zhang1,3, Zheng Gong1,3,
Wayne Xin Zhao1,3‚Ä†and Ji-Rong Wen1,2,3
1Gaoling School of Artificial Intelligence, Renmin University of China.
2School of Information, Renmin University of China.
3Beijing Key Laboratory of Big Data Management and Analysis Methods.
{zhipeng_chen,gongzheng0109,jrwen}@ruc.edu.cn,francis_kun_zhou@163.com
{zhangbeichen724,batmanfly}@gmail.com
Abstract
Although large language models (LLMs) have
achieved excellent performance in a variety of
evaluation benchmarks, they still struggle in
complex reasoning tasks which require specific
knowledge and multi-hop reasoning. To im-
prove the reasoning abilities, we propose Chat-
CoT , a tool-augmented chain-of-thought rea-
soning framework for chat-based LLMs ( e.g.,
ChatGPT). In ChatCoT, we model the chain-
of-thought (CoT) reasoning as multi-turn con-
versations, to utilize tools in a more natural
way through chatting. At each turn, LLMs
can either interact with tools or perform the
reasoning. Our approach can effectively lever-
age the multi-turn conversation ability of chat-
based LLMs, and integrate the thought chain
following and tools manipulation in a unified
way. Specially, we initialize the early turns
of the conversation by the knowledge about
tools, tasks, and reasoning format, and pro-
pose an iterative tool-augmented reasoning step
to perform step-by-step tool-augmented rea-
soning. The experiment results on two com-
plex reasoning datasets (MATH and HotpotQA)
have shown the effectiveness of ChatCoT on
complex reasoning tasks, achieving a 7.9%
relative improvement over the state-of-the-art
baseline. Our code and data are available at:
https://github.com/RUCAIBOX/ChatCoT .
1 Introduction
Recently, large language models (LLMs) (Zhao
et al., 2023a) have shown great potential as general-
purpose task solvers in a variety of real-world ap-
plications. With excellent few-shot and zero-shot
ability, LLMs, such as GPT-4 (OpenAI, 2023) and
LLaMA (Touvron et al., 2023), can even outper-
form full-data supervised-tuned models on many
tasks with suitable prompting strategies.
Among these prompting strategies, chain-of-
thought (CoT) prompting (Wei et al., 2022; Ko-
‚àóEqual contributions.
‚Ä†Corresponding author.jima et al., 2022) has been a prominent approach
to eliciting the reasoning abilities of LLMs. It
incorporates the intermediate reasoning steps of
exemplars into the input prompt, to instruct LLMs
to solve a question step by step. Despite the re-
markable improvement by CoT prompting, LLMs
still have difficulties in solving complex reasoning
tasks that involve specific functionalities, such as
arithmetic calculation and information retrieval (Lu
et al., 2022; Qian et al., 2022). To address this is-
sue, external tools ( e.g., calculator, search engine)
have been employed to fulfill the basic functional-
ities (Schick et al., 2023; Paranjape et al., 2023),
easing the burden of LLMs. With proper interfaces,
LLMs can be guided by prompts to manipulate
tools when necessary.
However, as tools are not intrinsically inte-
grated with LLMs, incorporating external tools
would have to interrupt the CoT reasoning pro-
cess of LLMs. Such an issue would become more
intractable on complex reasoning tasks that fre-
quently invoke the use of tools. To address it, exist-
ing work either relies on LLMs to prearrange the
tool use plan for subsequent execution (Zhou et al.,
2022; Jiang et al., 2023b), or needs to design for-
mal actions pertaining to specific tasks (Dua et al.,
2022; Khattab et al., 2022; Jiang et al., 2023a). De-
spite the effectiveness, the two types of methods
still suffer from potential issues: the former one
cannot interact with tools after generating the plan,
even seeing obvious mistakes; while the latter one
has to frequently switch between reasoning with
LLMs and taking actions, hurting the continuity of
the CoT reasoning process.
To overcome these disadvantages, we seek a
more unified way to integrate CoT reasoning and
tool manipulation. As the key idea, we consider
tools manipulation by LLMs as the interaction be-
tween LLMs and tools, in which LLMs send the
use requests and tools respond to support specific
functions. Further, inspired by the recent progressarXiv:2305.14323v3  [cs.CL]  6 Nov 2023of ChatGPT-like LLMs (called chat-based LLMs ),
we model the interaction process between LLMs
and tools as a multi-turn conversation, and leverage
the excellent chatting capacities for manipulating
tools by LLMs. At each turn, the LLM can freely
interact with tools when in need, otherwise perform
the reasoning by itself. The conversation contin-
ues until the final answer is derived by LLMs. In
this process, as chat-based LLMs can well under-
stand the multi-turn context, they can follow the
thought chain in the whole conversation and natu-
rally invoke the tools accordingly, thus keeping the
continuity of the reasoning process.
To this end, in this paper, we propose ChatCoT,
a tool-augmented chain-of-thought reasoning strat-
egy for chat-based LLMs. As the major merit, Chat-
CoT can perform the CoT reasoning across multi-
turn conversation, and freely interact with tools at
immediate steps. Concretely, we first store the use-
ful knowledge at early turns of the conversation,
including tools, tasks, and multi-turn reasoning for-
mat, to help LLMs utilize task-specific knowledge
to perform reasoning or manipulate tools. Then, we
iterate a specially designed tool-augmented reason-
ingstep in which LLMs interact with tools, to per-
form step-by-step tool-augmented reasoning, until
obtaining the final answer.
To evaluate the effectiveness, we implement
ChatCoT on ChatGPT, and conduct experi-
ments on two complex reasoning benchmarks,
i.e., MATH (Hendrycks et al., 2021) and Hot-
potQA (Yang et al., 2018). Experimental results
show that ChatCoT achieves very promising per-
formance on MATH with 7.9% relative improve-
ment in average over the SOTA baselines ( i.e.,
PHP (Zheng et al., 2023)). Besides, our approach
can also be integrated with other strategies, e.g.,
self-consistency, and ChatCoT can achieve better
performance by incorporating these strategies.
2 Related Work
Tool-Augmented Large Language Models. With
the large-scale parameters and pre-training cor-
pus, large language models (LLMs) ( e.g., Flan
T5 (Chung et al., 2022), ChatGPT (OpenAI, 2022)
and LLaMA (Touvron et al., 2023)) have demon-
strated strong zero-shot and few-shot ability in
NLP tasks ( e.g., language generation, reasoning).
However, LLMs have still struggled with com-
plex reasoning tasks requiring task-specific knowl-
edge and multi-step reasoning ( e.g., mathemati-cal problem solving). Previous work (Zhao et al.,
2022, 2023b; Luo et al., 2023) has constructed task-
specific corpus and utilized continue pre-training
and instruction tuning to inject relative knowledge
into LLMs and enhance the complex reasoning abil-
ity of LLMs. In order to further reduce the mistakes
made by LLMs, existing methods have explored to
augment LLMs with external tools. They can be
roughly categorized into the following two types.
The first type of methods (Gao et al., 2023; Parisi
et al., 2022; Qiao et al., 2023) train the model pa-
rameters to support the utilization of the external
tools, where they collect or synthesize the tool-
augmented examples to tune the model parame-
ters (Schick et al., 2023; Patil et al., 2023; Hao
et al., 2023). Another type of methods (Gao et al.,
2022; Yao et al., 2022; Zhang et al., 2023) uti-
lize carefully designed prompts to guide LLMs to
use external tools. They focus on devising proper
prompts or tools manipulation ways to select and
use tools when necessary (Liang et al., 2023; Shen
et al., 2023; Yao et al., 2022). In this work, we
follow the second type of methods and propose a
tool-augmented chain-of-thought reasoning strat-
egy that can better solve complex reasoning tasks.
Chain-of-Thought Reasoning. To further enhance
the reasoning capacity of LLMs, Chain-of-Thought
(CoT) prompting strategy (Wei et al., 2022; Kojima
et al., 2022) has been proposed to guide LLMs to
generate intermediate reasoning steps which can
boost the performance of LLMs. Through spe-
cial instructions ( e.g., ‚ÄúLet us think step by step‚Äù )
and in-context exemplars with detailed intermedi-
ate reasoning steps, LLMs can perform step-by-
step reasoning to reach the final answer. Based
on CoT, recent work has also proposed several
methods to further improve the performance, in-
cluding problem decomposition (Zhou et al., 2022;
Dua et al., 2022), appropriate exemplars selec-
tion (Ye et al., 2022; Shi et al., 2023), results post-
processing (Wang et al., 2022; Madaan et al., 2023;
Zheng et al., 2023), and changing the reasoning for-
mat (Yao et al., 2023; Wu et al., 2023). However, as
the generation process of CoT is one-pass, the uti-
lization of tools in intermediate steps would have to
interpret it, hurting the continuity of the generation
process. In this work, we propose a unified way
to integrate CoT reasoning and tool manipulation,
which utilizes the excellent multi-turn chatting ca-
pacity of LLMs to perform CoT reasoning across
multi-turn conversations.3 Preliminary
In this section, we present the task setting, then
introduce the Chain-of-Though prompting strategy
and the tool manipulation in reasoning tasks.
Task Setting. In this work, we focus on improv-
ing the reasoning ability of large language mod-
els (LLMs) on complex tasks, e.g., solving math-
ematics competition problems. Unlike tasks that
can be solved by humans via straightforward skills
or tools, complex tasks require advanced knowl-
edge ( e.g., mathematical theorem) and multi-step
reasoning to reach the answer. Typically, a com-
plex problem includes three types of texts, namely
problem statement, solution text, and answer key,
denoted asùëÑ,ùëÜandùê¥, respectively. The prob-
lem statement ùëÑintroduces the background and
description of a complex problem, and the solution
text illustrates the detailed solving process to ob-
tain the answer key. All of them are composed of
a sequence of tokens, where each token is either
a text word or a mathematical symbol. Formally,
given the problem statement ùëÑ, we aim to utilize
LLMs to perform multi-step reasoning, to finally
generate its accurate answer ùê¥.
Chain-of-Thought Prompting. To elicit the pow-
erful reasoning ability of LLMs for complex tasks,
Chain-of-Thought (CoT) prompt strategy (Wei
et al., 2022) has been widely used to guide LLMs
for performing step-by-step reasoning. Generally,
the CoT prompt consists of few exemplars whose a
series of intermediate reasoning steps {ùêº1,¬∑¬∑¬∑,ùêºùëõ}
are also involved. Each exemplar can be denoted
asùê∏=‚ü®ùëÑ,{ùêº1,¬∑¬∑¬∑,ùêºùëõ},ùê¥‚ü©. Formally, given the
question and few exemplars, a CoT prompt is com-
posed by integrating them as a long input of the
LLM, which can prompt the LLM to generate a
similar thought chain that leads to the final answer.
Tool Manipulation. Previous work has revealed
that LLMs are struggling with basic functionality
(e.g., arithmetical calculation (Schick et al., 2023)),
which can be solved by using specific external tools
(e.g., calculator), denoted as {ùëá1,...,ùëá ùëõ}. To ma-
nipulate tools, existing work mostly relies on writ-
ing a detailed prompt for describing how to use
available tools for the LLM, then incorporates it to
guide the selection of useful tools and generate the
tool arguments, and finally calls the tool API to ob-
tain the result. Following this way, in this work, we
focus on three useful tools that have been widelyused by humans to solve complex problems:
‚Ä¢Calculator: Given a mathematical expression,
the calculator can compute the value of it or sim-
plify it according to arithmetic rules ( e.g., combin-
ing like terms and reduction of fractions).
‚Ä¢Equation Solver: Given the equations system
and unknown variables, the equation solver can
automatically calculate the value of the contained
unknown variables through relative algorithms.
‚Ä¢Retriever: Given a query, the retriever aims to
extract the most relevant information ( e.g., docu-
ments) from a number of candidates. According to
the types of the retrieved corpus, it can be imple-
mented by specialized models, e.g., dense retrieval
model.
We implement the first two tools by using dif-
ferent functions of SymPy (Meurer et al., 2017), a
Python library for mathematical symbolic calcu-
lation. For the retriever, we adopt SimCSE (Gao
et al., 2021), a sentence embedding model to mea-
sure the text semantic similarity. Note that when
the input expression or equation is ill-formed or
unsolved, the above tools would return an error.
4 Approach
In this section, we present our proposed ChatCoT, a
new chain-of-thought (CoT) prompting framework
based on multi-turn conversations, for improving
chat-based LLMs on complex reasoning tasks with
tools. The overall illustration of our proposed Chat-
CoT is shown in Figure 1.
4.1 Overview
For complex tasks ( e.g., advanced mathematical
problems), LLMs require to frequently manipulate
the tools when in need, to fulfill the intractable
intermediate issues. However, as tools are not in-
trinsically integrated with LLMs, previous work
mostly relies on the LLM to generate the plan of
manipulating tools and then execution (Gao et al.,
2022; Lu et al., 2023), or immediately call tools by
stopping the continuous generation of LLMs (Yao
et al., 2022). The above both ways are not suitable
for the frequent interactions between LLMs and
tools, due to the error accumulation in planning
and frequent interruptions in LLM generation.
In our approach, we decompose the chain-of-
thought reasoning process of LLMs into a multi-
round conversation. In each turn, LLMs just require
to concentrate on manipulating tools or accomplish-
ing reasoning in the current step, and the whole[Reasoning] Iterative Tool-augmented ReasoningProblem : A function ùëìhas the property that ùëì(3ùë•‚àí1)=ùë•^2+ùë•+1for all real numbers ùë•. What is ùëì(5)?
Solution : Let‚Äôs think step by step. Let ùë¢=3ùë•‚àí1. Then ùë•=ùë¢+1
3, and ùëìùë¢=ùë¢+1
32
+ùë¢+1
3+1=ùë¢2+2ùë¢+1
9+ùë¢+1
3+1=ùë¢2+5ùë¢+13
9. In particular, ùëì5=52+5√ó5+13
9=7.
Problem : Solve for ùëí, given that 2ùëëis 8less than 17ùëí, and 2ùëíis 9less than ùëë.
Solution: Let‚Äôs think step by step. We begin with a system of two equations 2d=17e‚àí8, 2e=d‚àí9.       Since the second equation can also be rewritten as d=2e+9, 
we can plug this expression for dback into the first equation and solve for e.      2d=17e‚àí8‚Üí22e+9=17e‚àí8‚Üí4e+18=17e‚àí8‚Üí‚àí13e=‚àí26‚Üíe=2.
Tool Knowledge
Reasoning Format
Problem: A function ùëì‚Ä¶‚Ä¶
First, we need to find out ‚Ä¶‚Ä¶
‚Ä¶‚Ä¶Task Knowledge
Problem: ‚Ä¶‚Ä¶
Solution:  ‚Ä¶‚Ä¶Test Problem: Solve 
Solve for ùëí‚Ä¶‚Ä¶
Retrieval 
SetCalculator can help you ‚Ä¶‚Ä¶
Equation Solver can help ‚Ä¶‚Ä¶
Do you understand?
Yes, I understand ‚Ä¶‚Ä¶
I give you some example.
Problem: The product ‚Ä¶‚Ä¶
You can use ‚Ä¶‚Ä¶
Yes, I understand. I will solve the 
problem step by step and use tool to 
help me.
Problem: A function ùëìhas the property 
that ùëì(3ùë•‚àí1)=ùë•^2+ùë•+1‚Ä¶‚Ä¶
First, we need to find out ‚Ä¶‚Ä¶
‚Ä¶ ‚Ä¶Problem: Solve for ùëí, given that 2ùëëis 8less than 
17ùëí, and 2ùëíis 9less than ùëë.
First, we need to express ùëëin terms of ùëí‚Ä¶‚Ä¶
To solve this sub -problem, which tool can we use? 
You can choose calculator, equation solver or ‚Ä¶‚Ä¶
Equation SolverLLM for Reasoning
LLM for Tools Selection
What is the argument ‚Ä¶‚Ä¶
2ùëë=17ùëí‚àí8, 2ùëí=ùëë‚àí9‚Ä¶‚Ä¶
Results: ùëë=13,ùëí=2‚Ä¶‚Ä¶Tools Execution
Calculator Retriever‚Ä¶ ‚Ä¶
[Prompting] Conversational Memory
Next Iteration StepChatCoT
ùë∏:
ùë∞ùüè:
ùë∞ùüê:
ùë∞ùüë:Vanilla CoT
[Prompting] Context
ùë∏:
ùë∞ùüè: ùë∞ùüê:
ùë∞ùüë: ùë®:[Reasoning] One -time Inference‚Ä¶ ‚Ä¶Figure 1: The comparison of vanilla CoT and ChatCoT, illustrated for a mathematical problem. For vanilla CoT, the
content underlined are generated by LLMs. For ChatCoT, the conversational knowledge memory is initialized to
provide tools, task and reasoning format knowledge. Then, the tool-augmented reasoning step is iterated multiple
times to perform step-by-step reasoning, until obtaining the answer.
reasoning process would keep on pushing without
premature planning and sudden interruption. In
this way, the whole reasoning process would be
converted to a conversation between LLMs and an
agent, which follows pre-defined rules to guide
LLMs and manipulate the tool. By designing
proper chatting strategies, the agent would auto-
matically elicit LLMs to perform reasoning and
select a tool, or invoke the tool for execution.
In our approach, we first initialize the multi-turn
conversation by feeding chat-based LLMs with
the background knowledge, i.e., the description
of tools, relevant task exemplars, and the demon-
stration of decomposed chain-of-thought in chat,
which are the conversational knowledge memory
for supporting the following reasoning. Then, we
propose the tool-augmented reasoning procedure
that leverages LLMs to perform reasoning with
tools in the current step and iterate it to fulfill
all sub-tasks in the whole reasoning process, until
reaching the answer. We introduce the details of
the two components in the following.4.2 Initializing Conversational Knowledge
Memory
To guide chat-based LLMs to follow our proposed
ChatCoT using external tools, it is essential to de-
sign proper prompts in context. In our approach,
as we reformulate the chain-of-thought reasoning
into a decomposed multi-turn conversation, we can
also feed the essential prompts into LLMs at early
turns as the context, to initialize the conversation
background knowledge. It can be seen as the in-
context knowledge memory in the format of dia-
logue that stores useful knowledge for helping chat-
based LLMs manipulate tools or perform reasoning.
Here, we consider three types of knowledge about
tools, task, and multi-turn reasoning format, respec-
tively. The details of prompts are in Appendix A.
Tools Knowledge. As LLMs have never seen tools
during pre-training, for each tool in Section 3, we
hand-craft its description in the following pattern:
‚Äú[ùëá] can help you [ ùëå]‚Äù, where [ùëá]is the tool name
and[ùëå]shows its detailed functionality. Then, we
merge all the descriptions and design the inputprompt to tell LLMs about the knowledge of all
tools. We also hand-craft the expected response
of the LLM. It will be also fed into the LLM, to
indicate the LLM that it has accepted our prompt
and should follow it.
Retrieval-Augmented Task Knowledge. Since
LLMs can learn the task knowledge from in-
context exemplars, we leverage a retriever to se-
lect the most relevant instance from the training
dataset, to provide more useful knowledge for the
given question. Concretely, we train SimCSE (Gao
et al., 2021), a sentence embedding method that
can measure the semantic similarity of texts, via
the unsupervised training strategy on the training
set. Then, we leverage it to retrieve the top- ùëòmost
semantically similar exemplars, and concatenate
their problem statement ùëÑand solution ùëÜto com-
pose the input prompt. Similarly, we also feed it
with our expected response into the LLM.
Multi-turn Reasoning Format. To elicit LLMs
following multi-turn reasoning format, we man-
ually annotate the whole multi-round dialogue
ùêº1,¬∑¬∑¬∑,ùêºùëõof randomly sampled five questions
from the training set, to create the exemplars. Then,
we feed the dialogues of all the exemplars into the
chat-based LLM round by round, as the context to
guide LLMs to follow it for performing reasoning.
Summary. The above three types of multi-turn
utterances are pre-defined with corresponding con-
tents and formats, which compose the conversa-
tional knowledge memory of our approach. It
would be leveraged to initialize the conversational
context, and support the following step-by-step rea-
soning for answering the question.
4.3 Iterative Tool-augmented Reasoning
Based on the above conversational knowledge
memory, we iterate the tool-augmented reasoning
step to perform step-by-step tool-augmented rea-
soning, until finally obtain the answer.
4.3.1 Tool-augmented Reasoning Step
The tool-augmented reasoning step can be iterated
in multiple times. In each iteration, based on the
current results, we first leverage LLMs to perform
reasoning, then select the proper tool by LLMs,
and finally execute the selected tool to obtain the
intermediate result in the current step.
LLM for Reasoning. Guided by the exemplars in
the conversation history, LLMs are able to decom-pose the whole reasoning process into multi-turn
chat. Specially, LLMs would be elicited by the
contextual exemplars to directly perform reason-
ing in natural language based on the current result,
without specialized prompts or instructions. Conse-
quently, LLMs can rely on the retrieval-augmented
task knowledge in context, to generate the natu-
ral language solution till the point that needs the
functionality of tools.
LLM for Tools Selection. After reasoning, we
utilize the LLM to select a useful tool ( e.g., cal-
culator), which will be employed to provide the
required functionality for the LLM. Here, the input
prompt of the LLM is ‚Äú To solve this sub-problem,
which tool can we use? ‚Äù After feeding it into the
LLM, if the LLM requires to utilize tools, it will
select a suitable one, and then we further ask the
LLM to formulate the input arguments of the tool,
e.g., mathematical expression. Otherwise, it will
answer ‚Äú Do not use tool ‚Äù, and the LLM will con-
tinue to perform reasoning.
Tools Execution. Given the selected tool and for-
mulated arguments by LLMs, we can execute the
tool with the arguments to obtain the result in the
current iteration. Here, we also consider that the re-
sults from the tool may be not satisfied by the LLM,
e.g., irrelevant retrieved documents. In this case,
we can also add several feedback rounds where the
LLM judges if the result is useful or expected, and
then reuse the tool to acquire a new result.
4.3.2 Iteration for Step-by-Step Reasoning
We iterate the above step based on the in-context
conversation knowledge memory, to perform step-
by-step reasoning on the given question ùëÑ. We
start the whole iteration process using the following
prompt: ‚Äú You should solve the problem step by step
and you should follow the react in the history [ ùëÑ]‚Äù.
Then, after reaching the answer key, the iteration
process will be stopped by LLMs. In practice, we
find that chat-based LLMs are prone to continue
chatting although the answer key has appeared in
the reasoning process. Thus, we set the maximum
chat turns, and devise the following prompt to force
LLMs to stop reasoning and conclude the answer:
‚ÄúBase on the context, what is the answer? ‚Äù.
As our proposed approach only decomposes the
one-pass chain-of-thought reasoning into multi-
turn chat and adds the utilization of tools, it is
agnostic to the task types and tools implementa-
tion. Therefore, it is a general framework that canDataset Category Train Dev /Test
MATHAlgebra 1744 1187
CP 771 474
Precalculus 746 546
Prealgebra 1205 871
Geometry 870 479
IA 1295 903
NT 869 540
HotpotQA Distractor 90477 7405
Table 1: Statistics of the two complex reasoning datasets.
CP, IA, and NT denote Counting and Probability ,Inter-
mediate Algebra , and Number Theory , respectively.
be applied to a variety of complex reasoning tasks
that require suitable tools. Besides, our approach
also supports the recently proposed improvement
strategies based on the chain-of-thought method,
e.g., self-consistency (Wang et al., 2022). We con-
duct corresponding experiments in Section 5.3 to
validate it.
5 Experiment
In this section, we conduct experiments to evaluate
the effectiveness of ChatCoT. The implementation
details can be found in Appendix B.
5.1 Experimental settings
Datasets. We consider two complex reasoning
datasets for evaluation, i.e., MATH (Hendrycks
et al., 2021) and HotpotQA (Yang et al., 2018). The
details of these two datasets are shown in Table 1.
We adopt accuracy as the evaluation metric.
‚Ä¢MATH is composed of challenging compe-
tition mathematical problems which require ad-
vanced mathematical knowledge. It is divided into
seven categories, i.e.,Algebra, Counting and Prob-
ability, Precalculus, Prealgebra, Geometry, Inter-
mediate Algebra, and Number Theory. We adopt
the calculator and an equation solver as external
tools to help LLMs.
‚Ä¢HotpotQA is a multi-hop question answering
dataset, where each question is associated with a
collection of paragraph candidates containing sev-
eral golden contents which are useful for reasoning.
We use the development set under the distractor set-
ting of HotpotQA for evaluation, where the anno-
tation of golden paragraphs is not aware to LLMs.
We employ the retriever as the external tool.
Baselines. We mainly compare our approach withthe following prompting strategies based on Chat-
GPT (OpenAI, 2022):
‚Ä¢Chain-of-Thought (CoT) (Wei et al., 2022)
is a prominent method to boost the performance
of LLMs in reasoning tasks. In CoT, LLMs are
prompted to generate the intermediate reasoning
path and reasoning step by step to reach the final an-
swer. Previous work has shown that the utilization
of external tools and similar exemplars improves
the performance of CoT. Therefore, we implement
external tools to help LLMs reason and retrieve to
help LLMs select exemplars, which are named CoT
w/ Tool , and CoT w/ Retri , respectively.
‚Ä¢Learning-to-Program (LP) (Guo et al., 2023)
guides LLMs to program in natural language by
learning solutions in the training set, and elicits
LLMs to solve tasks following the program.
‚Ä¢Progressive-Hint Prompting (PHP) (Zheng
et al., 2023) proposes to iteratively refine the solu-
tion based on the answer hints from previous trials.
The iterative method achieves SOTA on MATH.
To provide a more complete evaluation, we
also report the performance of various LLM back-
bones with the vanilla CoT prompting, including
PaLM (Chowdhery et al., 2022), PaLM 2 (Google,
2023), Minerva (Lewkowycz et al., 2022), Galac-
tica (Taylor et al., 2022), LLaMA (Touvron et al.,
2023) and GPT-3 (Brown et al., 2020).
5.2 Main Results
We present the evaluation results of our approach
on MATH and HotpotQA datasets in Table 2 and
Table 3 respectively.
First, for the comparison of backbones for CoT
prompting, ChatGPT achieves the best perfor-
mance, demonstrating its outstanding mathematical
reasoning ability. Our method elicits the reasoning
process by leveraging the strong multi-turn dia-
logue ability of ChatGPT, thus leading to a better
release of the reasoning ability from ChatGPT.
Second, retrieval-augmented methods ( e.g.,
ChatCoT, CoT w/ Retri) outperform other baselines.
The reason is that retrieved exemplars may contain
more relevant knowledge and reasoning steps that
are beneficial to solve the given problem. On Ge-
ometry tasks of MATH, CoT w/ Retri achieves the
largest improvement over vanilla CoT than other
sub-tasks. Another possible reason is that ChatGPT
is more unfamiliar to the knowledge and symbol of
geometry than others. Without similar exemplars,
it is difficult for LLMs to well understand them.ModelsPrompt
StrategyMATH
Algebra CP PC PA Geometry IA NT Avg.
GPT-3 CoT 6.0 4.7 4.0 7.7 3.1 4.4 4.4 5.2
PaLM CoT 9.7 8.4 4.4 19.2 7.3 3.5 6.0 8.8
LLaMA CoT - - - - - - - 10.6
Galactica CoT 29.0 13.9 12.8 27.2 12.3 9.6 11.7 20.4
Minerva CoT 51.3 28.0 18.0 55.0 26.8 13.7 21.2 33.6
PaLM 2 CoT - - - - - - - 34.3
ChatGPTCoT 48.1 31.4 21.1 56.6 22.3 18.3 29.1 35.1
CoT w/ Tool 35.9 22.6 9.3 40.5 13.6 9.4 19.4 23.8
CoT w/ Retri 52.7 32.7 18.9 58.4 29.2 19.9 31.7 37.7
LP 49.6 30.2 16.3 52.3 22.5 16.9 29.8 34.0
PHP 51.1 33.7 16.1 57.7 25.4 17.1 35.1 36.5
ChatCoT 56.1 34.2 23.8 59.2 29.9 19.5 32.6 39.4
Table 2: Experimental results on MATH dataset. PC and PA denote Precalculus andPrealgebra , respectively. Avg.
is the average value of all categories. The best are denoted in bold and the second-best are underlined.
Methods HotpotQA
CoT 38.0
CoT w/ Tool 31.4
ChatCoT w/o Feedback 53.8
ChatCoT 59.2
Table 3: The results on HotpotQA. We report the results
of the development set under the distractor setting.
Third, given the results of CoT and CoT w/ Tool
on MATH and HotpotQA, we can find that directly
utilizing external tools during reasoning is not a
suitable way, which may hurt the performance of
LLMs. The reason may be that injecting tool us-
age into the CoT reasoning process will hurt the
continuity of reasoning.
Finally, ChatCoT achieves state-of-the-art per-
formance on MATH dataset based on ChatGPT and
outperforms other baselines on HotpotQA. Com-
pared with the previous SOTA method PHP, Chat-
CoT outperforms six of seven sub-tasks on MATH
dataset and achieves 7.9% relative improvement on
average accuracy over the PHP method. The ex-
periment results have verified the effectiveness of
ChatCoT on complex reasoning tasks. By leverag-
ing conversational knowledge memory and multi-
round dialogue to reasoning, ChatCoT has the ad-
vantage to utilize plug-and-play tools. Moreover,
on the Number Theory tasks of MATH, we can find
that PHP achieves the best performance. The rea-
son may be that there are fewer equations that need
to be computed or simplified. Thus, the advantageMethods MATH
TK RATK MRF PC Geo NT
‚úî ‚úî ‚úî 23.8 29.9 32.6
‚úó ‚úî ‚úî 23.3 29.2 30.6
‚úî ‚úó ‚úî 20.0 27.4 31.0
‚úî ‚úî ‚úó 21.6 24.2 32.2
‚úó ‚úó ‚úî 16.7 21.1 29.3
Table 4: The results of ablation study. TK, RATK,
and MRF denote if using tool knowledge, retrieval-
augmented task knowledge, and multi-turn reasoning
format at early turns of the conversation, respectively.
Geo is the abbreviation of Geometry .
of the utilization of tools becomes less obvious.
5.3 Detailed Analysis
In order to further verify the effectiveness of each
component in ChatCoT, we conduct experiments
about ablation, adaption, tools utilization and ex-
pense. We present the case study in Appendix C.1.
Ablation Study. In the ablation study, we evaluate
the effectiveness of conversational memory, includ-
ing tool knowledge memory, retrieval-augmented
knowledge memory, and multi-turn reasoning for-
mat memory. As shown in Table 4, removing
any type of conversational memory will reduce
the performance of ChatCoT, which indicates the
effectiveness of these memories in complex reason-
ing. In particular, removing retrieval-augmented
knowledge memory or multi-turn reasoning format
memory will lead to a large drop, which showsMethods CP NT
CoT + SC 35.2+3.8% 34.4+5.3%
ChatCoT + SC 40.1+5.9% 38.3+5.7%
Table 5: The evaluated accuracy of combining our
approach with self-consistency. SC denotes self-
consistency. We also report the absolute improvement
compared with vanilla methods on subscripts.
Methods Frequency Success
CoT w/ Tool 3.0% 85.7%
ChatCoT w/o TK 56.0% 93.0%
ChatCoT w/o MRF 10.0% 64.2%
ChatCoT 70.0% 92.0%
Table 6: Frequency and success rate of tool manipu-
lation on Number Theory task of MATH. TK, MRF
denote tool knowledge, multi-turn reasoning format at
early turns of the conversation respectively.
that mathematical knowledge and reasoning for-
mat knowledge is important for LLMs in reasoning
tasks, while LLMs can learn the usage of external
tools from exemplars without descriptions.
Combination with Improvement Strategies.
ChatCoT is a general method to enhance the abil-
ity of tool manipulation of LLMs. It can be
integrated with improvement strategies and fur-
ther boost the performance of LLMs on reasoning
tasks. To evaluate the applicability of ChatCoT
to improvement strategies designed for CoT, we
compare ChatCoT with CoT on two subtasks of
MATH, where both methods are augmented with
self-consistency (Wang et al., 2022), a represen-
tative improvement strategy for CoT prompting.
Concretely, we sample 5 outputs for majority vot-
ing in self-consistency. As shown in Table 5, self-
consistency brings improvement in both CoT and
ChatCoT. In particular, the absolute improvement
of ChatCoT is slightly higher than CoT, showing
that ChatCoT can adapt to self-consistency well.
The reason is that, with the decomposing of reason-
ing procedures, the intermediate steps of ChatCoT
are more confident, and small mistakes will be
corrected easily. Moreover, we construct the case
study about the combination with ChatCoT and
Self-Refine (Madaan et al., 2023) in Appendix C.2.
Tools Utilization Analysis. As we mentioned
above, in complex reasoning tasks, infrequently
or incorrectly utilizing external tools might lead to
wrong answers. Thus, we conduct the experimentMethods Generated Tokens
CoT 224.6
CoT w/ Tool 296.2
Self-Consistency 1017.4
ChatCoT 355.2
Table 7: The comparison of the number of generated
tokens from LLMs among different prompt strategies.
about whether LLMs can frequently or correctly
leverage based on different methods. Table 6 ex-
presses the performance of tools utilization in the
Number Theory task of MATH of baseline and our
approach. ‚ÄúFrequency‚Äù denotes the ratio of prob-
lems where LLMs correctly leverage tools. ‚ÄúSuc-
cess‚Äù denotes the rate of LLMs utilizing tools suc-
cessfully among all the times of invoking tools. We
can observe that ChatCoT achieves a balance of fre-
quency and ratio of success. Tool knowledge pro-
vides the function of tools for LLMs and improves
the frequency that LLMs utilize the tools. LLMs
can learn how to leverage external tools through
the multi-turn reasoning format and boost the ratio
of successful utilization of tools. Without any of
them, the frequency and ratio of success will drop
which might not be conducive to reasoning.
Number of Generated Tokens Analysis. Despite
guiding LLMs to reason through multi-turn dia-
logue, the computation expense of ChatCoT is not
significantly larger than the CoT method. In Ta-
ble 7, we present the average number of generated
tokens from LLMs in several methods on MATH
dataset, which reflects the computation expense to a
certain degree. We can observe that ChatCoT is on
the same order of magnitude with other baselines
(e.g., CoT and CoT w/ Tool). Therefore, ChatCoT
does not bring significant expenses compared to
existing prompting methods.
6 Conclusion
In this paper, we have proposed ChatCoT, a new
framework to manipulate the tools for the CoT rea-
soning. It naturally integrates the reasoning process
and manipulating tools through a form of multi-turn
conversations. At each turn, LLMs can either in-
teract with tools or perform the reasoning by itself.
Our approach can effectively leverage the multi-
turn conversation ability of chat-based LLMs. Ex-
perimental results on two complex reasoning tasks
including MATH and HotpotQA have verified theeffectiveness of ChatCoT.
Currently, our experiments are mainly conducted
on mathematical reasoning tasks, and we will test
the effectiveness of the proposed approach to more
types of reasoning tasks. Besides, we will also
consider extending the number of available tools
for solving different tasks.
Limitations
In this section, we discuss the limitations of our
work. First, we do not utilize GPT-4 in our experi-
ment or evaluate the performance of GPT-4 in the
ChatCoT framework. That is because our appli-
cation for GPT-4 has not been accepted. Second,
ChatCoT is designed for chat-based LLMs and it
is hardly compatible with other LLMs. However,
most LLMs support multi-turn conversation cur-
rently and they perform well on reasoning tasks.
Besides, although LLMs have achieved strong abil-
ity in reasoning tasks, the requirement of compu-
tation expense and GPU resource is higher than
other pre-trained language models which have mil-
lions of parameters. The utilization of LLMs will
produce more carbon dioxide and pollute the envi-
ronment.
Acknowledgement
This work was partially supported by National Nat-
ural Science Foundation of China under Grant No.
62222215, Beijing Natural Science Foundation un-
der Grant No. L233008 and 4222027, and Beijing
Outstanding Young Scientist Program under Grant
No. BJJWZYJH012019100020098. And this work
is also partially supported by the Outstanding Inno-
vative Talents Cultivation Funded Programs 2021
of Renmin University of China. Xin Zhao is the
corresponding author.
References
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CoRR , abs/2204.02311.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Web-
son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
gun, Xinyun Chen, Aakanksha Chowdhery, Sharan
Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,
Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav
Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam
Roberts, Denny Zhou, Quoc V . Le, and Jason Wei.
2022. Scaling instruction-finetuned language models.
CoRR , abs/2210.11416.
Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt
Gardner. 2022. Successive prompting for decompos-
ing complex questions. In Proceedings of the 2022
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2022, Abu Dhabi, United
Arab Emirates, December 7-11, 2022 , pages 1251‚Äì
1265.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. PAL: program-aided language
models. CoRR , abs/2211.10435.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2023. Pal: Program-aided language
models.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Domini-
can Republic, 7-11 November, 2021 , pages 6894‚Äì
6910. Association for Computational Linguistics.Google. 2023. Palm 2 technical report. Google .
Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan
Wu, Dongyan Zhao, and Nan Duan. 2023. Learn-
ing to program with natural language. CoRR ,
abs/2304.10464.
Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting
Hu. 2023. Toolkengpt: Augmenting frozen lan-
guage models with massive tools via tool embeddings.
CoRR , abs/2305.11554.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-
cob Steinhardt. 2021. Measuring mathematical prob-
lem solving with the MATH dataset. In Proceedings
of the Neural Information Processing Systems Track
on Datasets and Benchmarks 1, NeurIPS Datasets
and Benchmarks 2021, December 2021, virtual .
Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,
Wayne Xin Zhao, and Ji-Rong Wen. 2023a. Struct-
gpt: A general framework for large language model
to reason over structured data. arXiv preprint
arXiv:2305.09645 .
Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,
and Ge Li. 2023b. Self-planning code generation
with large language model. CoRR , abs/2303.06689.
Omar Khattab, Keshav Santhanam, Xiang Lisa Li,
David Hall, Percy Liang, Christopher Potts, and
Matei Zaharia. 2022. Demonstrate-search-predict:
Composing retrieval and language models for
knowledge-intensive NLP. CoRR , abs/2212.14024.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In NeurIPS .
Aitor Lewkowycz, Anders Andreassen, David Dohan,
Ethan Dyer, Henryk Michalewski, Vinay V . Ra-
masesh, Ambrose Slone, Cem Anil, Imanol Schlag,
Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur,
Guy Gur-Ari, and Vedant Misra. 2022. Solving quan-
titative reasoning problems with language models. In
NeurIPS .
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,
Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, Yun Wang, Linjun Shou, Ming
Gong, and Nan Duan. 2023. Taskmatrix.ai: Com-
pleting tasks by connecting foundation models with
millions of apis. CoRR , abs/2303.16434.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining
approach. CoRR , abs/1907.11692.
Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-
Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-
feng Gao. 2023. Chameleon: Plug-and-play compo-
sitional reasoning with large language models. arXiv
preprint arXiv:2304.09842 .Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and
Kai-Wei Chang. 2022. A survey of deep learning for
mathematical reasoning. CoRR , abs/2212.10535.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-
ardmath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct.
CoRR , abs/2308.09583.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Sean Welleck, Bodhisattwa Prasad Majumder,
Shashank Gupta, Amir Yazdanbakhsh, and Peter
Clark. 2023. Self-refine: Iterative refinement with
self-feedback. CoRR , abs/2303.17651.
Aaron Meurer, Christopher P. Smith, Mateusz Pa-
procki, Ondrej Cert√≠k, Sergey B. Kirpichev, Matthew
Rocklin, Amit Kumar, Sergiu Ivanov, Jason Keith
Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig,
Brian E. Granger, Richard P. Muller, Francesco
Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johans-
son, Fabian Pedregosa, Matthew J. Curry, Andy R.
Terrel, Step√°n Roucka, Ashutosh Saboo, Isuru Fer-
nando, Sumith Kulal, Robert Cimrman, and An-
thony M. Scopatz. 2017. Sympy: symbolic com-
puting in python. PeerJ Comput. Sci. , 3:e103.
OpenAI. 2022. Introducing chatgpt. OpenAI Blog .
OpenAI. 2023. Gpt-4 technical report. OpenAI .
Bhargavi Paranjape, Scott Lundberg, Sameer Singh,
Hannaneh Hajishirzi, Luke Zettlemoyer, and
Marco Tulio Ribeiro. 2023. Art: Automatic multi-
step reasoning and tool-use for large language mod-
els.arXiv preprint arXiv:2303.09014 .
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022.
TALM: tool augmented language models. CoRR ,
abs/2205.12255.
Shishir G. Patil, Tianjun Zhang, Xin Wang, and
Joseph E. Gonzalez. 2023. Gorilla: Large lan-
guage model connected with massive apis. CoRR ,
abs/2305.15334.
Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and
Xifeng Yan. 2022. Limitations of language mod-
els in arithmetic and symbolic induction. CoRR ,
abs/2208.05051.
Shuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu
Zhang. 2023. Making language models better
tool learners with execution feedback. CoRR ,
abs/2305.13068.
Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
CoRR , abs/2302.04761.Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
hugging face.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H. Chi, Nathanael Sch√§rli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. CoRR ,
abs/2302.00093.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas
Scialom, Anthony Hartshorn, Elvis Saravia, An-
drew Poulton, Viktor Kerkez, and Robert Stojnic.
2022. Galactica: A large language model for science.
CoRR , abs/2211.09085.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal
Azhar, Aur√©lien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. CoRR .
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .
Le, Ed H. Chi, and Denny Zhou. 2022. Self-
consistency improves chain of thought reasoning in
language models. CoRR , abs/2203.11171.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
NeurIPS .
Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li,
Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng,
Qingyun Wu, and Chi Wang. 2023. An empirical
study on challenging math problem solving with GPT-
4.CoRR , abs/2306.01337.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
Brussels, Belgium, October 31 - November 4, 2018 ,
pages 2369‚Äì2380.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023. Tree of thoughts: Deliberate
problem solving with large language models. CoRR ,
abs/2305.10601.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. CoRR , abs/2210.03629.
Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoy-
anov, Greg Durrett, and Ramakanth Pasunuru. 2022.
Complementary explanations for effective in-context
learning. CoRR , abs/2211.13892.Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao,
Jing Sha, Shijin Wang, and Ji-Rong Wen. 2023. Eval-
uating and improving tool-augmented computation-
intensive math reasoning. CoRR , abs/2306.02408.
Wayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen
Zhang, Yuanhang Zhou, Jing Sha, Zhigang Chen,
Shijin Wang, Cong Liu, and Ji-Rong Wen. 2022. Ji-
uzhang: A chinese pre-trained language model for
mathematical problem understanding. In KDD ‚Äô22:
The 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, Washington, DC, USA,
August 14 - 18, 2022 , pages 4571‚Äì4581.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,
Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang
Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
2023a. A survey of large language models. CoRR ,
abs/2303.18223.
Xin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong,
Zhipeng Chen, Yuanhang Zhou, Ji-Rong Wen, Jing
Sha, Shijin Wang, Cong Liu, and Guoping Hu. 2023b.
Jiuzhang 2.0: A unified chinese pre-trained language
model for multi-task mathematical problem solving.
InProceedings of the 29th ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining,
KDD 2023, Long Beach, CA, USA, August 6-10,
2023 .
Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo
Li, and Yu Li. 2023. Progressive-hint prompting
improves reasoning in large language models. CoRR ,
abs/2304.09797.
Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Olivier Bousquet, Quoc Le, and Ed H. Chi. 2022.
Least-to-most prompting enables complex reasoning
in large language models. CoRR , abs/2205.10625.A Details of Conversation Memory
In this part, we present the details of the prompt in
conversation Memory.
Tool Knowledge. The two turns of utterances are:
User: ‚ÄúYou can use tool to help you solve the
problem and I give you the instruction of tools
usage. [ùëá1] can help you [ ùëå1]¬∑¬∑¬∑Do you under-
stand? ‚Äù
LLM: ‚ÄúYes, I understand. I will use tool to help
me solve the problem. ‚Äù.
Retrieval-Augmented Task Knowledge. The two-
turn utterances are:
User: ‚ÄúI give you some example. Problem: [ ùëÑ1]
Solution: [ùëÜ1]¬∑¬∑¬∑You can use the knowledge and
thoery in these problem. Do you understand? ‚Äù
LLM: ‚ÄúYes, I understand. I will solve the prob-
lem step by step and use tool to help me. ‚Äù.
Multi-turn Reasoning Format. The multi-turn
utterances are based on the following pattern:
User: ‚ÄúProblem: [ùëÑ‚Ä≤
1] Let‚Äôs think step by step
and use knowledge in similar problem to solve this
problem. ‚Äù
LLM: ‚Äú[ùêº1]‚Äù.
¬∑¬∑¬∑
LLM: ‚Äú[ùêºùëõ]‚Äù.
B Implementation Details
During the evaluation, we utilize ChatGPT (gpt-
3.5-turbo) (OpenAI, 2022) as our backbone model,
and fine-tune RoBERTa (Liu et al., 2019) following
SimCSE (Gao et al., 2021) on the training sets of
MATH and HotpotQA separately as the retriever in
corresponding tasks.
For MATH, we leverage 5-shot setting. The ex-
emplars of CoT and CoT w/ Tool are randomly
sampled, while exemplars of CoT w/ Retri are re-
trieved top-5 similar problems by the retriever. For
ChatCoT, 2 retrieval exemplars and 3 annotated
exemplars will be adopted. For HotpotQA, we
leverage 4-shot setting which is similar to MATH,
due to the length limitation of input. For the CoT
method, we retrieve the top-3 relevant paragraphs
from the paragraph collection as evidence of the
given question. In ChatCoT, as the retrieved para-
graphs might be not useful for LLMs, LLMs can
send feedback to the retriever to show other results
at most 5 times.C Case Study
C.1 Framework of ChatCoT
In order to better present the reasoning process of
ChatCoT, we conduct the case study of two prob-
lems in MATH dataset, which is shown in Figure 2.
The beginning prompt contains knowledge of
tools, tasks, and reasoning format. In the tool
knowledge, we introduce the usage and function
of external tools. For task knowledge, we retrieve
similar problems and corresponding solutions from
the training set as retrieval exemplars through se-
mantics similarity, which might contain relevant
knowledge. The reasoning format is used to guide
LLMs to reason through multi-round dialogue. The
beginning prompt and corresponding response are
pre-set to enhance the sensitivity of LLMs to in-
structions.
Instructed by the beginning prompt, LLMs it-
erate the tool-augmented reasoning step for step-
by-step reasoning in ChatCoT. We illustrate the
flow of ChatCoT with the example on the left in
Figure 2. Given the math problem, LLMs first rea-
son out that ùëëshould be expressed in terms of ùëí.
Then, LLMs communicate with the agent to se-
lect the appropriate external tool Equation Solver
and specify the corresponding arguments to invoke
the tool, including the unknown symbols andthe
system of equations . Based on the selection, The
assistant executes the tool invocation and returns
the solution of the system of equations to LLMs.
After execution, LLMs can perform the next rea-
soning step based on the given result. The iterative
tool-augmented reasoning process can be iterated
several times until LLMs find out the answer. In
this case, LLMs find that the original problem has
been solved, thus obtaining the final answer and
ending the reasoning process.
C.2 Combination with Self-Refine
To better show how LLMs correct mistakes through
ChatCoT, we present a case study of the combina-
tion of ChatCoT and Self-Refine in the following:
User : ‚ÄúProblem: What is the remainder when
1313+5is divided by 6?
Let‚Äôs think step by step ‚Äù
LLM :First, we need to find out the value of
1313+5using the properties of exponents.
......
LLM : ‚ÄúSecond, we need to find out the remain-
der when 302875106592258 is divided by 6.‚ÄùUser : ‚ÄúTo solve this sub-problem, which tool
can we use?‚Äù
LLM : ‚ÄúCalculator ‚Äù
User : ‚ÄúGive me the equation to calculate ‚Äù
LLM : ‚Äú302875106592258 mod 6 ‚Äù
User : ‚ÄúResults: 302875106592258 mod 6 =
1.81725063955355 ùëí+15‚àóùëöùëúùëë .
Continue reasoning ‚Äù
LLM : ‚ÄúFinally, we know that the remainder
when 1313+5is divided by 6is the same as the
remainder when 302875106592258 is divided by
6, which is 2‚Äù
User : ‚ÄúThe solution above might some mistake,
you should check the solution and get the final
answer. ‚Äù
LLM : ‚ÄúApologies for the mistake. Let‚Äôs correct
the solution.
To find the remainder when 1313+5is divided
by6, we can simplify the expression 1313first.
Using the property of remainders, we can ob-
serve that 13‚â°1(mod 6). Therefore, 1313‚â°
113‚â°1(mod 6).
Now, we can rewrite the expression as 1313+5‚â°
1+5‚â°6‚â°0(mod 6).
Hence, the remainder when 1313+5is divided
by6is 0.‚Äù
In this example, we can observe that LLMs try to
calculate the exact value of 1313+5at first, and then
get the remainder. However, the calculator does
not understand the expression ‚Äú 302875106592258
mod 6 ‚Äù and returns the wrong result. Through
refining, LLM changes the solution and gets the
correct answer.You can use tool to help you solve the problem and I give you the instruction ‚Ä¶‚Ä¶ 
Calculator can help you ‚Ä¶‚Ä¶ Equation solver can help you ‚Ä¶‚Ä¶ Do you understand?
Yes, I understand. I will use tool to help me solve the problem.
Retrieval Set
Problem 1 & Solution 1 ‚Ä¶‚Ä¶ Problem n & Solution n
I give you some example.
Problem: The product ‚Ä¶‚Ä¶
You can use ‚Ä¶‚Ä¶
Yes, I understand. I will solve the problem step by step and use tool to help me.
Problem: A function ùëìhas the property that ùëì(3ùë•‚àí1)=ùë•2+ùë•+1‚Ä¶‚Ä¶
Let's think step by step and use knowledge in similar problem to solve this problem
First, we need to find out the value of ùë•that corresponds to ùëì(5)‚Ä¶‚Ä¶
‚Ä¶ ‚Ä¶
You should solve the problem step by step and you should follow the react in the history ‚Ä¶‚Ä¶
Yes, I understand. I will follow my response in the conversation history ‚Ä¶‚Ä¶
Problem: Solve for ùëí, given that 2ùëëis 
8less than 17ùëí, and 2ùëíis 9less than ùëë. 
Let‚Äôs think step by step ‚Ä¶‚Ä¶Problem: What is the value of
345+45+45+45
Let's think step by step ‚Ä¶‚Ä¶
First, we need to simplify 
under the radical. ‚Ä¶‚Ä¶First, we need to express ùëë
in terms of ùëí‚Ä¶‚Ä¶
Give me the unknown variable 
Give me the equation systemùëë, ùëíEquation SolverTo solve this sub -problem, which tool 
can we use? ‚Ä¶‚Ä¶
Results: ùíÖ=ùüèùüë, ùíÜ=ùüê
Continue reasoning2ùëë=17ùëí‚àí8, 2ùëí=ùëë‚àí9
Finally, we can conclude 
that ùëë=13and ùëí=2. 
Therefore, the answer is 2.To solve this sub -problem, which 
tool can we use? ‚Ä¶‚Ä¶
346Give me the equation ‚Ä¶‚Ä¶
Results:ùüëùüíùüî=ùüèùüî
Continue reasoning
Therefore,‚Ä¶‚Ä¶346=16
Therefore, the answer is 
16.0.Tool Knowledge
Task Knowledge
Reasoning Format
LLM for Reasoning
LLM for Tools Selection
Tools Execution
‚Ä¶‚Ä¶CalculatorConversational 
Knowledge
MemoryI give you some example.
Problem: What is the ‚Ä¶‚Ä¶
You can use ‚Ä¶‚Ä¶
External Tools
Calculator, Equation Solver, ‚Ä¶‚Ä¶
Input Question
Tool-Augmented
ReasoningFigure 2: An illustration example for ChatCoT from MATH.