Challenges  and opportunities  for machine learning  in multiscale computational modeling  
Phong C.H. Nguyen1, Joseph B. Choi1, H.S. Udaykumar2,*, Stephen Baek1,3,* 
1University of Virginia, School of Data Science, Charlottesville, VA 22904, United States  
2University of Iowa, Department of Mechanical Engineering, Iowa City, IA 52242, United States  
3University of Virginia, Department of Mechanical and Aerospace Engineering, Charlottesville, VA 22903, United 
States  
*Corresponding Authors: hs-kumar@uiowa.edu , baek@virginia.edu   
Abstract:  
Many mechanical engineering applica tions call for multiscale computational modeling and simulation. 
However, solving for complex multiscale systems remains computationally onerous due to the high 
dimensionality of the solution space. Recently, machine learning (ML) has emerged as a promisin g solution 
that can either serve as a surrogate for, accelerate or augment traditional numerical methods. Pioneering 
work has demonstrated that ML provides solutions to governing systems of equations with comparable 
accuracy to those obtained using direct numerical methods, but with significantly faster computational 
speed. These high -speed, high -fidelity estimations can facilitate the solving of complex multiscale systems 
by providing a better initial solution to traditional solvers. This paper provides a perspective on the 
opportunities and challenges of using ML for complex multiscale modeling and simulation. We first outline 
the current state -of-the-art ML approaches for simulating multiscale systems and highlight some of the 
landmark developments. Next,  we discuss current challenges for ML in multiscale computational modeling , 
such as the data and discretization dependence, interpretability, and data sharing and collaborative platform 
development. Finally, we suggest several potential research directions for the future.  
Keywords : Machine learning , Artificial intelligence , Computational modeling, Multiscale modeling  
1 Introduction  
Multiscale computational modeling has emerged as  a central part of many mechanical engineering 
applications  in recent years . The need for multiscale models arises  from the limitations  of single -scale 
models in describ ing physic al laws and phenomena at different spatial and/or temporal  scales . Larger sca le 
models alone cannot capture the  fine-scale features or resolve key  details of complex mechanical  systems , 
while smaller scale models are computationally inefficient for simulating physic al phenomena  that manifest  
at a larger  system scale . Multiscale modeling  addresses this trade -off between physical fidelity  and 
efficiency  by using different numerical models to  represent physic al laws and phenomena  at different scales  
and bridging  scales  using closure models . Over the past decade, a growing number of studies have adopted 
multiscale modeling  and successfully demonstrated its ability to predict complex phenomena in areas  such 
as fluid mechanics  [1], [2] , material s modeling and  design  [3], [4] , manufacturing  and process modeling 
[5], [6] . 
Broadly speaking, there are two main strategies for linking models at different scales. Hierarchical  
multiscale modeling approaches  [5], [7] –[9], also known as sequential  or decoupled  multiscale modeling, 
precompute constitutive relations (e.g., elasticity constants, energy release rate) using fine -scale models. 
These fine -scale quantities of interest (QoIs) are typically computed ‘offline’ and then utilized during a 
system -scale compu tation s that seek to model  observable and measurable larger scale phenomena. On the 
contrary, concurrent  multiscale modeling approaches compute  the required QoIs for coarse -scale models ‘on-the-fly,’ as nee ed, by coupling with the fine -scale models  [10]–[13]. While hierarchical approaches 
require large  amounts of  memory and storage space to store precomputed QoIs, concurrent approaches face 
formidable computation cost s as the problem size increases. These challenges limit the  scalability and 
practicality of  multiscale comput ational modeling in real-world applications.  
Recent works suggest that machine learning ( ML) has the potential to overcome the  limitations  of 
traditional multiscale modeling methods . For  instance,  to address the issue of memory and storage 
requirements in hierarchical modeling, ML techniques can be applied  to learn a coarse , low fidelity , and 
low-cost representation , referred to as representation learning , of precomputed QoIs  [14]. This allow s for 
fast retrieval of information without the need for large memory and storage space s. Concurrent multiscale 
modeling  can also benefit from modern  deep learning  (DL) [15] models, which can replace traditional 
numerical s olvers  for ‘on -the-fly’ computation of QoIs. Previous studies have demonstrated that ML -based 
simulation can achieve comparable fidelity to direct numerical simulation while being considerably faster 
in computation time [16]. Moreover, ML can accelerate the solution of inverse problems, providing direct 
design solutions that meet design constraints without requiring the traditional iterative optimization process 
[17].  
Despite the promise of using ML in multiscale modeling, s everal critical challenges have emerged from 
pioneering works. One of the most significant obstacles is the data dependency  of ML, particularly with 
modern DL models . ML/DL has achieved remarkable success in computer vision by leveraging massive 
datasets w ith high -quality annotations, such as image labels and captions. However, this is not typically the 
case for multiscale modeling problems, which require intensive calculations  and laborious data collection. 
Therefore, alongside the development of fast and efficient data generation methods, the research community 
may benefit from ‘small data learning’ techniques, such as transfer learning and meta learning (Sections 
3.1.1 and 3.1.2), and collaborative/shared learning methods (Section 3.4).  
Additionally , the majority of ML models are dependent on the discretization  used during training, which 
can limit their scalability and practicality for multiscale problems where different discretization  settings  are 
Figure 1. Machine learning can be applied to accelerate  multiscale  computational modeling. Examples 
of applications include computational mechanics, design optimization, and manufacturing modeling.  
used. This issue can be addressed by developing discreti zation -independent ML models such as those that 
employ  continuous convolution or neural operators (Section 3.2) , which  are promising a nd warrant further 
investigation.  
Lastly , the ‘black -box’ nature of ML models poses a challenge to their  trustworthiness and interpretability 
in practic al applications . While  ML predictions can be accurate, the assimilated information during training 
that drive  predictions may not always  reflect the underlying physical phenomena . Hence, it is important to  
explain how predictions were derived and examine  the physics -awareness  of a model, i n addition to the 
evaluation of  their statistical performance. To achieve this , the development of techniques  for interpreting 
ML predictions, such as neural network visualiz ation (Section 3.3), is crucial and demands  additional effort s. 
This paper examines the promise of ML in accelerating the development of multiscale computational 
modeling and offers potential solutions to address the challenges. The paper is organized as f ollows. In 
Section 2, we present several examples of successful applications of ML in multiscale modeling. Section 3 
discusses the challenges that previous works have revealed, as well as our perspective on potential solutions. 
Finally, we provide conclusi ons and remarks in Section 4.  
2 Examples  of machine learning  in multiscale  computational modeling   
2.1 Computational mechanics  
2.1.1 Fluid modeling  
The intersection of  ML and fluid mechanics presents a  natural  synergy , owing to their similarity in dealing 
with large and complex datasets. In traditional fluid mechanics, one has to deal with a  large  amount of data 
coming from various source, including experiments, field measurements or large -scale simulation. The 
analysis of fluid mechanics data still relies large ly on domain expertise and traditional statistical analysis, 
which involve many simplif ying assumptions  [18]. Recently, the rise of ML, especially DL, has been a 
significant driving force  in fluid mechanics  research, rendering opportunities for tackling more difficult 
fluid mechanics problems such as turbulen t flows and  its modeling . By leveraging t he computational 
efficiency of ML, advancements in fluid flow modeling can be achieved , which can have a profound impact 
on a wide range of applications, such as biomedical engineering, aerospace engineering, and aerial/hydro -
robotics  and autonomous systems , among others.  
In fact , DL techniques such as  artificial neural networks (ANN) have been employed  to simulate fluid  
dynamics  very early, since  the 1990s  [19], [20]  and are still  being  developed [21]. Along with the 
breakthrough in DL architecture development, t he progress in fluid flow modeling  has been significantly 
accelerated . ML approaches  such as recurrent neural networks (RNNs) have been utilized  to model flows 
in extreme environments [22], [23]  Unlike traditional feedforward neural networks, RNNs have loops that 
allow information to be passed from one step of the network to the next, enabling them to maintain 
information about previous inputs a nd influence the current output. This characteristic makes RNNs capable 
of considering the inherent order of data and therefore well -suited to fluid mechanics modeling , where the 
data on the evolving flow  fields is  supplied to the machine learning algorithm  in the form of sequences of 
images, videos, or time -series.   
Another DL technique that has also demonstrated success in fluid modeling is G enerative adversarial 
networks (GAN) . The GAN  framework is composed of two neural networks, namely the gene rator and the 
discriminator, that are trained in adversarial manner to generate high -quality synthetic data. Motivated by 
the capability  of GAN  to capture and generate realistic complex geometric pattern s, Kim et al. [24] have applied GAN to model variou s complex fluid behavior s. The se authors demonstrated that GAN is capable 
of accurately capturing the velocity field   enabl ing many opportunities for further application including 
fast reconstruction of fluid flow, time -resampling, or fluid data compressi on. 
Although current DL-based approaches have achieved successes, they have a limitation that trained models 
are typically only effective for interpolating data points and may not be as reliable when extrapolating 
beyond the known data range  [18]. This reduces the generalizability and applicability of trained models and 
hinders their use in real -world applications. To address this limitation, it is crucial not only to expand the 
training dataset to ensure that most future prediction tasks fall withi n the interpolation of the training data 
but also to develop new modeling methods with better extrapolation capabilities.  
2.1.2 Materials microstructure modeling  
Materials microstructure modeling is another domain of application that has benefited  from  the 
advancement of ML. ML can greatly facilitate the materials microstructure modeling process by providing 
efficient and accurate methods for predicting materials p roperties and behavior  of given material 
compositions  [25] or microstructure  [26]. ML can achieve this modeling capability due to its ability to 
identify and learn from data complex patterns that m ay be difficult to capture using traditional statistical 
methods . This modeling capability allow s ML algorithms to learn complex relationships between a wide 
range of microstructure morphology parameters (e.g., grain size, grain orientation, or distribution of defects) 
and the output materials properties  or mechanical response . These rapid and accurate predictions then can 
be used to design new materials with specific properties or to optimize the currently available materials 
with improved performance  [27], [28]. 
Recent advances in DL, with the increase in approximation power of neural networks, has further 
accelerated progress in ML -based materials microstructure modeling. Modern DL methods such as 
convolutional neural networks (CNNs) can produce  high-fidelity estimation of materials microstructure 
response to loads with an accuracy comparable to traditional numerical methods but with multiple order 
faster computation time [16], [29] . Physics -informed ML [30]–[32] (PIML) is another branch of  DL method 
that has been widely adopted to simulate complex governing equations in multiscale materials modeling 
with increasing  success. In PI ML, prior physics knowledge is embedded in the design of DL algorithms, 
either via learning objectives  [30], or architecture design  [16], [31] . As reported in pioneer ing works  [16], 
the embedded prior physics knowledge helps reduce the need for large dataset s while  maintaining  the 
prediction accuracy and generalizability of the DL model. The strong predictive ca pabilities and 
computational efficiency of DL -based methods make them ideally suited as surrogates for design 
optimization  [33], multiscale SPP linkage modeling  [29], [34], [35] , and uncertainty quantification tasks  
[36]. 
2.1.3 Reducing the nonlinearity of  dynamical  systems  
Many complex  mechanical systems are high nonlinear, posing a significant challenge for  obtaining stable 
and reliable numerical solutions , which  often  requires an extremely fine grid resolution  to resolve physics 
at features like shocks, interfaces and reactive fronts. The presence of nonlinearities and the potential for 
emergence of features at spatial and temporal hierarchies of scales renders the computation of such 
problems time-consuming. To address these challenges , the Koopman operator theory  [37], [38] ,  has been 
drawing attention  recently ; it is based on the idea that any highly nonlinear dynamical system can be 
represented in terms of an infinite -dimensional linear operator, called the  Koopman operator. This linear 
representation of nonlinear dynamical systems facilitates the analysis, design, and control of complex system via well -established methods for linear system. Here , the challenge is in  approximating the 
Koopman operator  from data . This has led to  improved numerical methods such as dynamic mode 
decomposition (DMD)  [39], which utilized linear observables, and its extended version (eDMD)  [40], 
which augments the original DMD with nonlinear observables . These methods  have demonstrated 
success es in modeling nonlinear flows  [39], the Duffing oscillator, and the Kuramoto Sivashinsky pa rtial 
differential equation  [40], among many others . DMD extracts r ich and physically meaningful basis 
functions, which can  notably improve the prediction accuracy of numerous classification and forecasting 
tasks involving time -series data  [40], [41] . In addition to DMD  and its variants , DL with the representation 
power of neural networks has been applied exclusively to mode l the Koopman -invariant subspaces of 
observables [38], [42], [43] . The advantage of the Koopman -based methods  is that they do not require prior 
knowledge or an explicitly defined model, and therefore  can be scalable  to various applications  with 
unknown p hysics .  
2.2 Topology optimization  
Topology optimization (TO)  is another  a group of computational design problems  that benefi t greatly from 
ML. TO determines the optimal distribution of material in a given design domain to achieve  design 
objectives  (e.g., high strength)  while satisfying design constraints  (e.g., low weight)  [44]. With such 
capabilities, TO plays crucial role in many  materials and structural design applications [4], [37] –[39]. 
However, many TO methods face the challenge of  repeatedly comp uting an expensive objective function , 
which involves   finite element (FE) solver s at each iteration [17]. These computational processes can be 
time-consuming and demand significant computational resources, especially for problems that involve  
nonlinear analysis, such as plasticity, viscoelasticity, and dynamics. To this end , ML models can act as 
surrogate s to replace computationally expensive FE calculations in the objective function , thereby  
accelerat ing design optimization processes [45]–[47]. Moreover , ML -based models are  computationally 
inexpensive and  differentiable, making them suitable for gradient -based  TO methods , which can further 
facilitate  the optimization processes  [48].  
In addition to being used as surrogate models to replace expensive numerical models, ML can also be used 
to directly model the topology optimization process. In thi s approach, ML models take the design objective 
and constraints as input and give the final topology design as output. Various topology optimization 
problems, ranging from structures under static load  [49] to conductive heat transfer  [50], have been 
effectively tackled using CNN -based approaches . Deep generative models , such as GAN,  can also be used 
to predict the optimal topology design, given design objectives and constraints . As reported in the literatures, 
GAN -based methods  have demonstrated great s uccesses  in the optimization of  material microstructures  for 
desired properties  [17], [51], [52] . Finally,  reinforcement learning (RL)  has also shown  several  success es 
for recurrent optimization problems, as documented in prior research  [28], [53] .  
2.3 Design for manufacturing  
ML-based methods are also advantageous in design for manufacturing (DFM) due to their strong modeling 
capability and computational efficiency. Assimilating process -structure -property (PSP) linkages is a crucial 
task in computational modeling for DFM but remains a challenging task . Advanced  manufacturing  
techniques , such as metallic additive manufacturing, involve various  physical and chemical proce sses that 
occur across  multiple scales during the transformation of raw materials. As these processes are complex , 
traditional  numerical approaches  such as finite element analysis (FEA) or cellular automata can be  
expensive and inadequate  for process optimization , monitoring , and control . In such cases, ML can be used to simplify the computational models by serving as surrogates  or even replace the expensive numerical 
models entirely  [54].  
For instance, Tapia et al. [55] employed  Gaus sian Process (GP)  to predict the depth of the melt pool in laser 
powder bed fusion of 316L stainless steel. The GP model was trained using data from high -fidelity physics -
based numerical simulations and was able to  accurately predict  the melt pool depth of  single -track prints 
based on inputs including laser power, velocity, and spot size. Furthermore , the use of GP allows for the 
assessment of  uncertainty in prediction s, which  facilitat es the making  of robust and reliable design and 
control decisions  [56]. In the same vein, Wang et al. [57] proposed a physics -informed data -driven 
modeling framework that can connect multi ple data-driven surrogate models at different levels, allowing  
for the effective estimation of manufacturing uncertainty.  
In brief, applying ML to DFM has great potential for improving the optimization and control of complex 
manufacturing processes. However, this research area is currently overlooked, and more effort  is needed to 
advance the field further.  
3 Challenges in machine learning for  multiscale  computational modeling and potential solutions  
As we have seen so far, there have been reported in the literature notable successes of applying ML to 
multiscale computa tional modeling. However , there are still several critical  obstacles that need to  be 
surmounted before ML can be used in real -world applications . In this section, we discuss these challenges 
and offer potential solutions.  
3.1 Data dependency  
The use of ML for multiscale computational modeling presents several challenges that require attention . 
Although modern ML techniques  like DL can achiev e high predictive performance, they necessitate  a 
considerable  amount of training data . However, collecting such training data  is often an expensive and 
tedious process , especially when dealing with experimental data . Synthetic data generated from numerical 
simulation may offer  an alternative, but produc ing them  can also be computationally demanding  due to the 
complexity of multiscale problems [16], [58] . Insufficient  training data can result in  erroneous  predictions 
or overfitting, reducing the generalizability and trustworthiness of ML algorithms . 
Besides  the amount of training data, the quality of data is also crucial for the success of ML algorithms. 
Training data with missing values or many outliers /noise  can cause ML models to learn incorrect patterns, 
resulting in poor prediction performance, preventi ng the application of ML for real -world practice.   
In the subsections below,  we discuss several potential directions in this regard, including transfer learning 
and meta -learning . 
3.1.1 Transfer learning : utilizing prior knowledge from previous training  
Training  ML models for computational modeling requires a large amount of data, which can take an along 
time and effort to collect. Unfortunately, when a new problem arises, the same data collection process must 
be repeated, and the ML model needs to be retrained w ith the newly collected dataset. However, repeating 
the long and tedious data collection process is not desirable because collecting data for ML models used in multiscale computational modeling is  often  expensive. Moreover, retraining an ML model from scratch 
(Fig. 2(a)) may not always yield simil ar statistical performance compared to previous ly trained results . In 
such cases , one may consider  utiliz ing prior knowledge gained from previous training sessions to reduce 
the need for obtaining a large training dataset  for a new problem .  
In this vein, transfer learning  [59] (Fig. 2(b))  has  proven  to be  successful in many generic ML applications, 
such as image recognition  [60] and language  modeling  [61]. In transfer learning, model parameters of a 
pre-trained  ML model , trained on a similar but different dataset,  are used as the starting point for the current 
training session.  In the new training session, relatively minor changes are applied to a subset of  model 
parameters, while a big portion of  model parameters can be  "frozen " and not updated . This allows the ML 
model to fine -tune its model parameters to learn the new problem with only a small number of newly added 
data. Transfer learning can reduce the need for a large dataset while maintaining the prediction performance 
of the ML models. Previous studies have shown that transfer learning is effective in training ML models to 
predict the deformation of structures under sta tic loads in new loading scenarios [62]. Transfer learning has 
also been found to be successful in predicting the response of energetic material microstructures when 
subjected to new shock loading conditions  [34]. 
3.1.2 Meta -learning : learning generalized kn owledge for quick adaptations  
A step further from the idea of transfer learning is  the more generalizable  concept of 'meta -learning' (Fig . 
2(c)). Meta -learning  allows ML algorithms to quickly learn and adapt to new tasks by leveraging experience 
gained from solving similar tasks.  In this context, transfer learning can be  considered as a specific case of 
meta learning.   
Conceptually , in meta -learning  a meta -learner is cons tructed to learn generalized knowledge (meta -
knowledge) across different learning tasks and utilize it as a prior to learn specific knowledge for each task, 
thereby maximizing the likelihood : 
Figure 2. Three types of training ML models. (a) Training from the scratch for each task. (b) Transfer 
learning. (c) Meta -learning.   𝑃(𝜃𝑖|𝐷𝑖,𝐷0)=∫𝑃(𝜃𝑖|𝜙,𝐷𝑖)𝑃(𝜙|𝐷0)𝑑𝜙
𝜙≈𝑃(𝜃𝑖|𝜙∗,𝐷𝑖)𝑃(𝜙∗|𝐷0) (1) 
In Eq. (1) the meta -learner  is represented by meta -learning parameters 𝜙 and t he knowledge on a specific 
task 𝜎𝑖 can be represented as a combination of the meta -knowledge  𝜙 and the specific knowledge  𝜃𝑖|𝜙. 
The meta -knowledge is acquired via training on the reference dataset  𝐷0 which maximizes the posteriori  
𝜙∗=argmax
𝜙𝑃(𝜙|𝐷0). In more detail, given the refe rence dataset  𝐷0, the algorithm will first learn 
generalizable knowledge by leveraging the richness of the reference dataset  𝐷0, and then use it as a basis 
to learn the specific knowledge 𝜃𝑖∗ for the specific task 𝜎𝑖 using only a small data set  𝐷𝑖. 
Previous works showed that meta -learning approaches could provide good prediction performance on 
unseen tasks despite being trained with relatively small datasets . For instance, Liu et al. [63] initialized 
physics -informed neural networks (PINN) with the Reptile algorithm, a meta -learning initialization method. 
The authors demonstrated that PINN can be trained more quickly with initialization guided by meta -
learning algorithms than with the conv entional random ized initialization  methods . This new PINN training 
paradigm was successfully validated with a variety of forward and inverse test problems. In another study, 
Li et al. [64] proposed a physics -informed meta -learning (PIML) model for tool w ear prediction problems 
in milling process modeling. The authors incorporated a physics -informed loss term that ensured the meta -
learner learned the robust relationship between tool wear rate and cutting forces, thereby facilitating 
parameter estimation an d enhancing the interpretability of the PIML model.  
Previous examples have demonstrated that m eta-learning is well -suited for the small -data context of ML in 
computational modeling of multiscale problems. Meta-learning allows for rapid adaptation to new op erating 
conditions, such as new materials, geometric domains, or boundary conditions, without requiring a large 
training dataset. In addition, t he "learning -to-learn" procedure can also aid to bridge the knowledge gap 
between simulations and real -world exp eriments. Currently, ML models are commonly trained with 
simulation data, as observational data from physical experiments is typically limited and expensive. 
However, there is a substantial difference between the ideal ized environment of numerical simulati ons and 
the real -world environment of physical experiments, which may cause ML models to perform poorly in 
real-world settings. In such situations, meta -learning can be used to rapidly adapt pre -trained ML models 
from numerical simulations to real -world en vironments. Specifically, a meta -learner can be trained to learn 
both meta -knowledge representing underlying physics laws and specific knowledge representing the 
operating environment of datasets, i.e., ideal assumptions of numerical simulation or real -world 
environments . At the deployment stage, meta -learning can then be utilized to quickly adapt ML models on 
ideal environment assumptions to real -world environments, using just a small amount of real -world 
experiment data. With this approach, ML models excl usively trained with numerical simulations can be 
quickly adapted to operate in real -world environments.  
3.2 Discretization  matter s: the need for  discretization -independent  models  
Applying ML for multiscale computational modeling also presents a nother challenge  that is  the dependence  
of ML algorithms  on the discretization of the analyzed domain.  Once trained, m ost of the common ML 
techniques, such as CNNs, are limited  to a specific discretization setting (Fig. 3(a)) and do not generalize 
well for settin gs that differ from the discretization  of the training data.  This issue  of discretization 
dependence poses a significant challenge for training ML models for multiscale computational modeling.  The dependency of ML algorithms on the discretization requires efforts to determine the optimal 
discretization setting that ensures convergence properties for all data samples in  the dataset. However, the 
convergence property of a domain under examination heavily depends on its geometric characteristics, 
which notably vary across all data samples in the training dataset. Therefore, finding the optimal 
discretization setting is ext remely costly and time -consuming.  For this reason, developing discretization -
independent (Fig. 3(b)) ML models is preferable  as these models can make predictions regardless of the 
discretization of the examined domain . 
Initial efforts have been devoted toward developing discretization -independent ML models for 
computational modeling of physical problems. For instance, Wandel et al.  [65] transform ed the discretized 
representation of state variable fields into continuous representations by interpolation with Hermite spline 
kernels. The Hermite spline constants were  stored for each discrete grid point and could be handled by a 
CNN. Th is metho d successf ully model ed several complex flow phenomena, such as Karman vortex streets, 
the Magnus effect, the Doppler effect, interference patterns, and wave reflections. Quantitative assessment 
also show ed that using CNN on a continuous representation of state varia bles can improve the accuracy of 
ML models while still maintaining their computational efficiency advantage . In another work, Sun et al. 
[66] proposed the use of a continuous convolution kernel based on the Zernike polynomial for scalar field 
regression.  This method exhibited  an increase in prediction performance compared to the conventional 
Figure 3: The difference between discretization variant (a) and invariant models  (b). The discretization 
variant models are not generalized beyond the discretization setting of training data. Meanwhile, 
discretization invariant model can be trained on multiple dicretization settin gs and its prediction is not 
dependent on the discretization of input fields.  approach with discrete graph convolutions, as well as the capability of making predictions on arbitrary 
geometric domains with different discretization settings . The utilization of neural operators is another 
approach that can feature discretization -independent  characteristics  [67]–[71]. In this approach , neural 
networks are trained to learn operators, namely neural operators , that map between infinite dimensional 
input and output function  spaces . Since neural operators are trained to operate on function spaces, their 
predictions are not affected by discretization settings of examined domain  while the accuracy of prediction 
is still assured  [71].  
3.3 Interpretability  of machine learning models   
ML has demonstrated great potential in multiscale computational modeling, as discussed in previous 
sections. However, there is still a significant concern about the trustworthiness of these predictions, whi ch 
prevents their use in real -world applications. Th is concern arises from the "black -box" nature of ML 
predictions . ML models are often trained to optimize predefined objective functions that may not fully 
describe the real -world physics, leading to the p ossibility of ML models merely "memorizing" observed 
data but not being aware of  or representing the underlying physical phenomena. As reported by Nguyen et 
al. [16], the physics -naï ve architecture of ML models  developed  for generic purpose s, despite having 
relatively high prediction performance, often fail to capture the underlying physical laws . Due to these 
reasons , it is crucial to ensure the trustworthiness and physics -awareness  of ML predictions before utilizing 
these models for practical computational modeling.  
The need to justify the trustworthiness of  ML predictions necessitates the development of methods to 
interpret ML model predictions , i.e., understanding of the rationale behind ML predictions . Several i nitial  
efforts  have been made  to develop method for interpret ing ML predictions  in physical science . For instance,  
Nguyen et al. [16], used saliency map visualization to highlight "critical" microstructure locations that have 
a high impact on the creation of energy localization in energetic materials modeling. The interpretation 
results were compared to previous findings, and a high lev el of agreement between the saliency map 
identification and the literature on the characteristics of "critical" energetic materials microstructure was 
demonstrated . In another work, Lellep et al. [72] utilized Shapley Additive  Explanation (SHAP) algorith ms 
to investigate the  main factors that affect the ML prediction in fluid dynamic s. The authors demonstrated 
the applicability of the method in various  fluid dynamic datasets, including  the evaluation of plane Couette 
flow.  
3.4 Centralized platform for data sharing and collaborations  
Large databases, such as ImageNet [73], MNIST [74], and others, play a crucial role in the success of ML 
in computer vision and language modeling. Similarly , centralized databases offer a potential solution to the 
challenge of a shortage of high -quality training data for ML algorithms in computational modeling. 
Moreover, w ith the advancement of cloud computing technology, these platforms can be expanded for 
shared and collaborative learning, bringing together computational analysts, experimentalists, and ML 
researchers to facilitate effective collaboration. Unfortunately, such a platform is currently lacking.  
Figure 4 depicts our proposed centralized platform for data sharing and collaboration in ML for 
computational modeling, which is inspired by OpenML [75], an example of a successful sharing platform 
in the ML community. The proposed platform consists of three main components. Similar to several 
available platforms for scientific data sharing [76], [77] , our proposed platform includes an open scientific 
database for sharing and retrieving data. Additionally, ML model librar ies are included , allowing for the 
sharing of previously successful ML models. Fina lly, the proposed platform includes an interactive environment that allows scientists to collaborate on a particular project.  With the above features, the 
proposed centralized data sharing platform can create a common format and protocol to facilitate data and 
model sharing . Because engineering data are generally heterogeneous and can come from multiple sources , 
having such a sharing platform will enrich the dataset that can be used to train ML model s, thus helping to 
overcome the shortage of training data.  
Another important feature of the proposed sharing platform is its ability to facilitate collaborations via an 
interactive platform. In this collaborative platform, several most suitable datasets an d ML models can be 
suggested to researchers for specific problems. Researchers can then decide whether to utilize the suggested 
datasets and models or work with their own. Additionally, this platform allows ML developers to request 
or recommend additional data samples for simulation or experimentation based on the performance of the 
model on the current dataset. Consequently, experimentalists or computational analysts can confirm the 
validity or suggest alternate data samples based on their expertise.  
Compared to the conventional method of data sharing, this collaborative mechanism offers several 
advantages. First, this mechanism can reduce the time to collect and preprocess training data since the 
quality of training datasets suggested by the collabora tive platform has been justified via previous training 
sessions. Additionally, through this mechanism, ML developers can gain access to meaningful and valuable 
data, thereby improving the resulting ML model ’s predictive performance. Finally, this mechanism  aids to 
justify the robustness of ML algorithms. In computational modeling, ML models are typically trained on a 
single dataset, which can cause ML predictions to be biased toward the training dataset. With this 
collaborative environment, an ML model can be trained and validated on multiple  different  datasets,  
enhancing  its robustness and generalizability.  
4 Conclusion  
Multiscale computational modeling  plays a  critical role  in many mechanical engineering applications, such 
as computational mechanics, mechani cal design and optimization, and manufacturing /process  modeling. 
Despite the promise  of captur ing complex physics phenomena, the  progress in multiscale computational 
modeling  research  is hampered by the high computation cost. To this end, ML  can be  a viabl e option that 
Figure 4: A centralized platform is necessary  for data and model sharing as well as supporting collaboration 
between data providers and model developers.  can aid in the analysis of multiscale physical systems by either augmenting or serving as a surrogate for 
multiscale  computational models.  
However, several obstacles must be surmounted to successfully  apply ML to the computational modeling 
of multiscale physical systems. The first pressing issue is big data dependency , which requires immediate 
attention. Unlike traditional ML problems, such as image or text classifications, collecting scientific data i s 
difficult and expensive, particularly for multiscale physical systems. The lack of large, high -quality training 
datasets can significantly reduce the generalizability and robustness of ML models. Investigating the use of 
prior knowledge through physics -informed ML, transfer learning, and meta -learning are all viable options 
to overcome this challenge.  Another issue is dealing with the dependency  of ML algorithms on the 
discretization setting of the examined domain. To overcome this issue , developing  ML on  continuous 
representation or applying  neural  operators are potential  future directions. In addition to improving the 
statistical performance of ML -based computational models, it is equally important to validate the physics -
awareness and trustworthiness of  ML predictions . Finally , the development of a centralized platform for 
data and model  sharing is necessary  to facilitate the development of algorithms and foster collaboration in 
the field of multiscale computational modeling . 
Aside from  a few  pioneering work s, the above -mentioned issues are still understudied. Therefore, there is 
an opportunity for future research to explore better solutions to these challenges. Once these obstacles are 
overcome, ML has the potential to significantly accelerat e the development of multiscale computational 
modeling and facilitate the discovery of novel and more effective mechanical systems.  
Acknowledgement  
This paper is based upon work  support ed by the U.S. Air Force Office of Scientific Research (AFOSR) 
Multidisciplinary University Research Initiative (MURI) program under Grant No. FA9550 -19-1-0318  and 
by the National Science Foundation (NSF) Designing Materials to Revolutionize and Engineer our  Future 
(DMREF) program under Grant No. 2203580 . 
References  
[1] N. S. Martys and J. G. Hagedorn, “Multiscale modeling of fluid transport in heterogeneous materials 
using discrete Boltzmann methods,” Mater Struct , vol. 35, no. 10, pp. 650 –658, 2002, doi: 
10.1007/BF02480358.  
[2] S. Chen, M. Wang, and Z. Xia, “Multiscale Fluid Mechanics and Modeling,” Procedia IUTAM , vol. 
10, pp. 100 –114, 2014, doi: https://doi.org/10.1016/j.piutam.2014.01.012.  
[3] J. Fish, G. J. Wagner, and S. Keten, “Mesoscopic and multiscal e modelling in materials,” Nat Mater , 
vol. 20, no. 6, pp. 774 –786, 2021, doi: 10.1038/s41563 -020-00913 -0. 
[4] R. Sivapuram, P. D. Dunning, and H. A. Kim, “Simultaneous material and structural optimization 
by multiscale topology optimization,” Structural an d Multidisciplinary Optimization , vol. 54, no. 5, 
pp. 1267 –1281, 2016, doi: https://doi.org/10.1007/s00158 -016-1519 -x. 
[5] J. Gawad, A. van Bael, P. Eyckens, G. Samaey, P. van Houtte, and D. Roose, “Hierarchical multi -
scale modeling of texture induced plas tic anisotropy in sheet forming,” Comput Mater Sci , vol. 66, 
pp. 65 –83, 2013, doi: https://doi.org/10.1016/j.commatsci.2012.05.056.  [6] M. Markl and C. Körner, “Multiscale Modeling of Powder Bed –Based Additive Manufacturing,” 
Annu Rev Mater Res , vol. 46, n o. 1, pp. 93 –123, 2016, doi: 10.1146/annurev -matsci -070115 -032158.  
[7] A. Sridhar, V. G. Kouznetsova, and M. G. D. Geers, “Homogenization of locally resonant acoustic 
metamaterials towards an emergent enriched continuum,” Comput Mech , vol. 57, no. 3, pp. 4 23–
435, 2016, doi: 10.1007/s00466 -015-1254 -y. 
[8] C. Oskay and J. Fish, “Eigendeformation -based reduced order homogenization for failure analysis 
of heterogeneous materials,” Comput Methods Appl Mech Eng , vol. 196, no. 7, pp. 1216 –1243, 2007, 
doi: https://doi.org/10.1016/j.cma.2006.08.015.  
[9] Q. Yu and J. Fish, “Multiscale asymptotic homogeni zation for multiphysics problems with multiple 
spatial and temporal scales: a coupled thermo -viscoelastic example problem,” Int J Solids Struct , 
vol. 39, no. 26, pp. 6429 –6452, 2002, doi: https://doi.org/10.1016/S0020 -7683(02)00255 -X. 
[10] Krishna Muralidh aran, P A Deymier, and J H Simmons, “A Concurrent multiscale finite difference 
time domain/molecular dynamics method for bridging an elastic continuum to an atomic system,” 
Model Simul Mat Sci Eng , vol. 11, no. 4, p. 487, 2003, doi: 10.1088/0965 -0393/11/4/ 306. 
[11] J. Fish and W. Chen, “Discrete -to-continuum bridging based on multigrid principles,” Comput 
Methods Appl Mech Eng , vol. 193, no. 17, pp. 1693 –1711, 2004, doi: 
https://doi.org/10.1016/j.cma.2003.12.022.  
[12] G. S. Smith, E. B. Tadmor, N. Bernstein , and E. Kaxiras, “Multiscale simulations of silicon 
nanoindentation,” Acta Mater , vol. 49, no. 19, pp. 4089 –4101, 2001, doi: 
https://doi.org/10.1016/S1359 -6454(01)00267 -1. 
[13] A. K. Nair, D. H. Warner, R. G. Hennig, and W. A. Curtin, “Coupling quantum an d continuum 
scales to predict crack tip dislocation nucleation,” Scr Mater , vol. 63, no. 12, pp. 1212 –1215, 2010, 
doi: https://doi.org/10.1016/j.scriptamat.2010.08.038.  
[14] Y.-C. Chan, F. Ahmed, L. Wang, and W. Chen, “METASET: Exploring Shape and Property  Spaces 
for Data -Driven Metamaterials Design,” Journal of Mechanical Design , vol. 143, no. 3, Nov. 2020, 
doi: 10.1115/1.4048629.  
[15] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning . The MIT Press, 2016.  
[16] P. C. H. Nguyen, Y. -T. Nguyen, J. B. Choi, P. K. Seshadri, H. S. Udaykumar, and S. Baek, “PARC: 
Physics -Aware Recurrent Convolutional Neural Networks to Assimilate Meso -scale Reactive 
Mechanics of Energetic Materials,” ArXiv , vol. arXiv:2204.07234, Apr. 2022, doi: 
10.48550/arxiv.2204.07234.  
[17] H. T. Kollmann, D. W. Abueidda, S. Koric, E. Guleryuz, and N. A. Sobh, “Deep learning for 
topology optimization of 2D metamaterials,” Mater Des , vol. 196, p. 109098, 2020, doi: 
https://doi.org/10.1016/j.matdes.2020.109098.  
[18] S. L. Brunton, B. R. No ack, and P. Koumoutsakos, “Machine Learning for Fluid Mechanics,” Annu 
Rev Fluid Mech , vol. 52, no. 1, pp. 477 –508, Jan. 2020, doi: 10.1146/annurev -fluid -010719 -060214.  
[19] I. Grant and X. Pan, “An investigation of the performance of multi layer, neural networks applied 
to the analysis of PIV images,” Exp Fluids , vol. 19, no. 3, pp. 159 –166, 1995, doi: 
10.1007/BF00189704.  [20] C. L. Teo, K. B. Lim, G. S. Hong, and M. H. T. Ye o, “A neural net approach in analyzing photograph 
in PIV,” in Conference Proceedings 1991 IEEE International Conference on Systems, Man, and 
Cybernetics , 1991, pp. 1535 –1538 vol.3. doi: 10.1109/ICSMC.1991.169906.  
[21] S. L. Brunton, B. R. Noack, and P. Kou moutsakos, “Machine Learning for Fluid Mechanics,” Annu 
Rev Fluid Mech , vol. 52, no. 1, pp. 477 –508, Jan. 2020, doi: 10.1146/annurev -fluid -010719 -060214.  
[22] Z. Y. Wan, P. Vlachas, P. Koumoutsakos, and T. Sapsis, “Data -assisted reduced -order modeling of 
extreme events in complex dynamical systems,” PLoS One , vol. 13, no. 5, pp. e0197704 -, May 2018, 
[Online]. Available: https://doi.org/10.1371/journal.pone.0197704  
[23] P. R. Vlachas, W. Byeon, Z. Y. Wan, T. P. Sapsis, and P. Koumoutsakos, “Data -driven forec asting 
of high -dimensional chaotic systems with long short -term memory networks,” Proceedings of the 
Royal Society A: Mathematical, Physical and Engineering Sciences , vol. 474, no. 2213, p. 20170844, 
2018, doi: 10.1098/rspa.2017.0844.  
[24] B. Kim, V. C. Az evedo, N. Thuerey, T. Kim, M. Gross, and B. Solenthaler, “Deep Fluids: A 
Generative Network for Parameterized Fluid Simulations,” Jun. 2018, doi: 10.1111/cgf.13619.  
[25] B. Meredig et al. , “Combinatorial screening for new materials in unconstrained composi tion space 
with machine learning,” Phys Rev B , vol. 89, no. 9, p. 94104, Mar. 2014, doi: 
10.1103/PhysRevB.89.094104.  
[26] A. Bhaduri , A. Gupta, and L. Graham -Brady, “Stress field prediction in fiber -reinforced composite 
materials using a deep learning approach,” Compos B Eng , vol. 238, p. 109879, 2022, doi: 
https://doi.org/10.1016/j.compositesb.2022.109879.  
[27] Z. Rao et al. , “Machine  learning –enabled high -entropy alloy discovery,” Science (1979) , vol. 378, 
no. 6615, pp. 78 –85, Oct. 2022, doi: 10.1126/science.abo4940.  
[28] P. C. H. Nguyen, N. N. Vlassis, B. Bahmani, W. Sun, H. S. Udaykumar, and S. S. Baek, 
“Synthesizing controlled micr ostructures of porous media using generative adversarial networks and 
reinforcement learning,” Sci Rep , vol. 12, no. 1, p. 9034, 2022, doi: 10.1038/s41598 -022-12845 -7. 
[29] A. Gupta, A. Bhaduri, and L. Graham -Brady, “Accelerated multiscale mechanics modeli ng in a deep 
learning framework,” Dec. 2022, doi: 10.48550/arxiv.2212.14601.  
[30] M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics -informed neural networks: A deep learning 
framework for solving forward and inverse problems involving nonlinear par tial differential 
equations,” J Comput Phys , vol. 378, pp. 686 –707, 2019, doi: 
https://doi.org/10.1016/j.jcp.2018.10.045.  
[31] R. Pokharel, A. Pandey, and A. Scheinker, “Physics -Informed Data -Driven Surrogate Modeling for 
Full-Field 3D Microstructure and M icromechanical Field Evolution of Polycrystalline Materials,” 
JOM , vol. 73, no. 11, pp. 3371 –3382, 2021, doi: 10.1007/s11837 -021-04889 -3. 
[32] E. Haghighat, M. Raissi, A. Moure, H. Gomez, and R. Juanes, “A physics -informed deep learning 
framework for inver sion and surrogate modeling in solid mechanics,” Comput Methods Appl Mech 
Eng, vol. 379, p. 113741, 2021, doi: https://doi.org/10.1016/j.cma.2021.113741.  [33] X. Zhang, F. Xie, T. Ji, Z. Zhu, and Y. Zheng, “Multi -fidelity deep neural network surrogate mode l 
for aerodynamic shape optimization,” Comput Methods Appl Mech Eng , vol. 373, p. 113485, 2021, 
doi: https://doi.org/10.1016/j.cma.2020.113485.  
[34] P. C. H. Nguyen, Y. -T. Nguyen, P. K. Seshadri, J. B. Choi, H. S. Udaykumar, and S. Baek, “A 
physics -aware d eep learning model for energy localization in multiscale shock -to-detonation 
simulations of heterogeneous energetic materials,” Nov. 2022, doi: 10.48550/arxiv.2211.04561.  
[35] W. Yan et al. , “Data -driven multi -scale multi -physics models to derive process –structure –property 
relationships for additive manufacturing,” Comput Mech , vol. 61, no. 5, pp. 521 –541, 2018, doi: 
10.1007/s00466 -018-1539 -z. 
[36] A. Olivier, M. D. Shields, and L. Graham -Brady, “Bayesian neural networks for uncertainty 
quantification in data -driven materials modeling,” Comput Methods Appl Mech Eng , vol. 386, p. 
114079, 2021, doi: https://doi.org/10.1016/j.cma.2021.114079.  
[37] B. O. Koopman, “Hamiltonian Systems and Transfor mation in Hilbert Space.,” Proc Natl Acad Sci 
U S A , vol. 17 5, pp. 315 –8, 1931.  
[38] B. Lusch, J. N. Kutz, and S. L. Brunton, “Deep learning for universal linear embeddings of nonlinear 
dynamics,” Nat Commun , vol. 9, no. 1, p. 4950, 2018, doi: 10.1038/s41 467-018-07210 -0. 
[39] C. W. Rowley, I. Mezic, S. Bagheri, P. Schlatter, and D. S. Henningson, “Spectral analysis of 
nonlinear flows,” J Fluid Mech , vol. 641, pp. 115 –127, 2009, doi: DOI: 
10.1017/S0022112009992059.  
[40] Q. Li, F. Dietrich, E. M. Bollt, and I. G. Kevrekidis, “Extended dynamic mode decomposition with 
dictionary learning: A data -driven adaptive spectral decomposition of the Koopman operator,” 
Chaos: An Interdisciplinary Journal of Nonlinear Science , vol. 27, no. 10, p. 103111, Oct. 2017, 
doi: 1 0.1063/1.4993854.  
[41] A. Surana, “Koopman Operator Framework for Time Series Modeling and Analysis,” J Nonlinear 
Sci, vol. 30, no. 5, pp. 1973 –2006, 2020, doi: 10.1007/s00332 -017-9441 -y. 
[42] E. Yeung, S. Kundu, and N. O. Hodas, “Learning Deep Neural Netw ork Representations for 
Koopman Operators of Nonlinear Dynamical Systems,” 2019 American Control Conference (ACC) , 
pp. 4832 –4839, 2017.  
[43] N. Takeishi, Y. Kawahara, and T. Yairi, “Learning Koopman Invariant Subspaces for Dynamic 
Mode Decomposition,” in Proceedings of the 31st International Conference on Neural Information 
Processing Systems , 2017, pp. 1130 –1140.  
[44] M. P. Bendsøe and O. Sigmund, Topology Optimization . 2004. doi: https://doi.org/10.1007/978 -3-
662-05086 -6. 
[45] R. v Woldseth, N. Aage, J. A. Bærentzen, and O. Sigmund, “On the use of artificial neural networks 
in topology optimisation,” Structural and Multidisciplinary Optimization , vol. 65, no. 10, p. 294, 
2022, doi:  10.1007/s00158 -022-03347 -1. 
[46] D. A. White, W. J. Arrighi, J. Kudo, and S. E. Watts, “Multiscale topology optimization using neural 
network surrogate models,” Comput Methods Appl Mech Eng , vol. 346, pp. 1118 –1135, 2019, doi: 
10.1016/j.cma.2018.09.007.  [47] F. Ghavamian and A. Simone, “Accelerating multiscale finite element simulations of history -
dependent materials using a recurrent neural network,” Comput Methods Appl Mech Eng , vol. 357, 
2019, doi: 10.1016/j.cma.2019.112594.  
[48] L. Wang, Y. -C. Chan, F.  Ahmed, Z. Liu, P. Zhu, and W. Chen, “Deep generative modeling for 
mechanistic -based learning and design of metamaterial systems,” Comput Methods Appl Mech Eng , 
vol. 372, p. 113377, 2020, doi: https://doi.org/10.1016/j.cma.2020.113377.  
[49] D. W. Abueidda,  S. Koric, and N. A. Sobh, “Topology optimization of 2D structures with 
nonlinearities using deep learning,” Comput Struct , vol. 237, p. 106283, 2020, doi: 
https://doi.org/10.1016/j.compstruc.2020.106283.  
[50] Q. Lin, J. Hong, Z. Liu, B. Li, and J. Wang, “ Investigation into the topology optimization for 
conductive heat transfer based on deep learning approach,” International Communications in Heat 
and Mass Transfer , vol. 97, pp. 103 –109, 2018, doi: 
https://doi.org/10.1016/j.icheatmasstransfer.2018.07.001.  
[51] M. M. Behzadi and H. T. Ilieş, “GANTL: Toward Practical and Real -Time Topology Optimization 
With Conditional Generative Adversarial Networks and Transfer Learning,” Journal of Mechanical 
Design , vol. 144, no. 2, Dec. 2021, doi: 10.1115/1.4052757.  
[52] Z. Yang, X. Li, L. Catherine Brinson, A. N. Choudhary, W. Chen, and A. Agrawal, “Microstructural 
Materials Design Via Deep Adversarial Learning Methodology,” Journal of Mechanical Design , 
vol. 140, no. 11, Oct. 2018, doi: 10.1115/1.4041371.  
[53] F. Sui, R. Guo, Z. Zhang, G. X. Gu, and L. Lin, “Deep Reinforcement Learning for Digital Materials 
Design,” ACS Mater Lett , vol. 3, no. 10, pp. 1433 –1439, Oct. 2021, doi: 
10.1021/acsmaterialslett.1c00390.  
[54] N. S. Johnson et al. , “Invited review: Machine  learning for materials developments in metals 
additive manufacturing,” Addit Manuf , vol. 36, p. 101641, 2020, doi: 
https://doi.org/10.1016/j.addma.2020.101641.  
[55] G. Tapia, S. Khairallah, M. Matthews, W. E. King, and A. Elwany, “Gaussian process -based 
surrogate modeling framework for process planning in laser powder -bed fusion additive 
manufacturing of 316L stainless steel,” The International Journal of Advanced Manufacturing 
Technology , vol. 94, no. 9, pp. 3591 –3603, 2018, doi: 10.1007/s00170 -017-1045 -z. 
[56] M. A. Bessa et al. , “A framework for data -driven analysis of materials under uncertainty: Countering 
the curse of dimensionality,” Comput Methods Appl Mech Eng , vol. 320, pp. 633 –667, 2017, doi: 
https://doi.org/10.1016/j.cma.2017.03.037.  
[57] Z. Wan g et al. , “Uncertainty Quantification in Metallic Additive Manufacturing Through Physics -
Informed Data -Driven Modeling,” JOM , vol. 71, no. 8, pp. 2625 –2634, 2019, doi: 10.1007/s11837 -
019-03555 -z. 
[58] S. Roy, N. K. Rai, O. Sen, and H. S. Udaykumar, “Struct ure–property linkage in shocked multi -
material flows using a level -set-based Eulerian image -to-computation framework,” Shock Waves , 
vol. 30, no. 5, pp. 443 –472, 2020, doi: 10.1007/s00193 -020-00947 -y. 
[59] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A surve y of transfer learning,” J Big Data , vol. 3, no. 
1, p. 9, 2016, doi: 10.1186/s40537 -016-0043 -6. [60] J. Plested and T. Gedeon, “Deep transfer learning for image classification: a survey,” May 2022, 
doi: 10.48550/arxiv.2205.09904.  
[61] C. B. Do and A. Y. Ng, “Transfer learning for text classification,” in Advances in Neural Information 
Processing Systems , 2005, vol. 18. [Online]. Available: 
https://proceedings.neurips.cc/paper/2005/file/bf2fb7d1825a1df3ca308ad0bf48591e -Paper.pdf  
[62] C. Xu, B. T. Cao, Y. Yuan, and G. Meschke, “Transfer learning based  physics -informed neural 
networks for solving inverse problems in engineering structures under different loading scenarios,” 
Comput Methods Appl Mech Eng , vol. 405, p. 115852, 2023, doi: 
https://doi.org/10.1016/j.cma.2022.115852.  
[63] X. Liu, X. Zhang, W. Peng, W. Zhou, and W. Yao, “A novel meta -learning initialization method for 
physics -informed neural networks,” Neural Comput Appl , vol. 34, pp. 14511 –14534, 2021.  
[64] Y. Li, J. Wang, Z. Huang, and R. X. Gao, “Physics -informed meta learning for machining t ool wear 
prediction,” J Manuf Syst , vol. 62, pp. 17 –27, 2022, doi: https://doi.org/10.1016/j.jmsy.2021.10.013.  
[65] N. Wandel, M. Weinmann, M. Neidlin, and R. Klein, “Spline -PINN: Approaching PDEs without 
Data using Fast, Physics -Informed Hermite -Spline CN Ns,” ArXiv , vol. abs/2109.07143, 2021.  
[66] Z. Sun, E. Rooke, J. Charton, Y. He, J. Lu, and S. Baek, “ZerNet: Convolutional Neural Networks 
on Arbitrary Surfaces Via Zernike Local Tangent Space Estimation,” Computer Graphics Forum , 
vol. 39, 2020, doi: 10.1 111/cgf.14012.  
[67] N. Kovachki et al. , “Neural Operator: Learning Maps Between Function Spaces,” Aug. 2021, doi: 
10.48550/arxiv.2108.08481.  
[68] M. A. Rahman, M. A. Florez, A. Anandkumar, Z. E. Ross, and K. Azizzadenesheli, “Generative 
Adversarial Neural Operators,” May 2022, doi: 10.48550/arxiv.2205.03017.  
[69] M. A. Rahman, Z. E. Ross, and K. Azizzadenesheli, “U -NO: U -shaped Neural Operators,” Apr. 
2022, doi: 10.48550/arxiv.2204.11127.  
[70] Z. Li et al. , “Physics -Informed Neural Operator for Learning Par tial Differential Equations,” Nov. 
2021, doi: 10.48550/arxiv.2111.03794.  
[71] Z. Li et al. , “Fourier Neural Operator for Parametric Partial Differential Equations,” Oct. 2020, doi: 
10.48550/arxiv.2010.08895.  
[72] M. Lellep, J. Prexl, B. Eckhardt, and M. Linkmann, “Interpreted machine learning in fluid dynamics: 
explaining relaminarisation event s in wall -bounded shear flows,” J Fluid Mech , vol. 942, p. A2, Jul. 
2022, doi: 10.1017/jfm.2022.307.  
[73] J. Deng, W. Dong, R. Socher, L. -J. Li, K. Li, and L. Fei -Fei, “ImageNet: A large -scale hierarchical 
image database,” in 2009 IEEE Conference on Comput er Vision and Pattern Recognition , 2009, pp. 
248–255. doi: 10.1109/CVPR.2009.5206848.  
[74] L. Deng, “The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best 
of the Web],” IEEE Signal Process Mag , vol. 29, no. 6, pp. 141 –142, 2012 , doi: 
10.1109/MSP.2012.2211477.  [75] J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo, “OpenML: Networked Science in Machine 
Learning,” SIGKDD Explor. Newsl. , vol. 15, no. 2, pp. 49 –60, Jun. 2014, doi: 
10.1145/2641190.2641198.  
[76] H. Zhao, X. Li, Y . Zhang, L. S. Schadler, W. Chen, and L. C. Brinson, “Perspective: NanoMine: A 
material genome approach for polymer nanocomposites analysis and design,” APL Mater , vol. 4, 
no. 5, p. 53204, 2016, doi: 10.1063/1.4943679.  
[77] J. Hu et al. , “MaterialsAtlas.or g: a materials informatics web app platform for materials discovery 
and survey of state -of-the-art,” NPJ Comput Mater , vol. 8, no. 1, p. 65, 2022, doi: 10.1038/s41524 -
022-00750 -6. 
  