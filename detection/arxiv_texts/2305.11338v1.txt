1
Coordinated Transformer with Position &
Sample-aware Central Loss for Anatomical
Landmark Detection
Qikui Zhu, Yihui Bi, Danxin Wang, Xiangpeng Chu, Jie Chen, Yanqing Wang
Abstract —Heatmap-based anatomical landmark detection is
still facing two unresolved challenges: 1) inability to accurately
evaluate the distribution of heatmap; 2) inability to effectively
exploit global spatial structure information. To address the
computational inability challenge, we propose a novel position-
aware and sample-aware central loss. Speciﬁcally, our central loss
can absorb position information, enabling accurate evaluation of
the heatmap distribution. More advanced is that our central loss
is sample-aware, which can adaptively distinguish easy and hard
samples and make the model more focused on hard samples while
solving the challenge of extreme imbalance between landmarks
and non-landmarks. To address the challenge of ignoring struc-
ture information, a Coordinated Transformer, called CoorTrans-
former, is proposed, which establishes long-range dependen-
cies under the guidance of landmark coordination information,
making the attention more focused on the sparse landmarks
while taking advantage of global spatial structure. Furthermore,
CoorTransformer can speed up convergence, effectively avoiding
the defect that Transformers have difﬁculty converging in sparse
representation learning. Using the advanced CoorTransformer
and central loss, we propose a generalized detection model
that can handle various scenarios, inherently exploiting the
underlying relationship between landmarks and incorporating
rich structural knowledge around the target landmarks. We
analyzed and evaluated CoorTransformer and central loss on
three challenging landmark detection tasks. The experimental
results show that our CoorTransformer outperforms state-of-
the-art methods, and the central loss signiﬁcantly improves the
performance of the model with p-values <0:05.
Index Terms —central loss, Coordinated Transformer, structure
information, landmark detection.
I. I NTRODUCTION
ACCURATE and automatic detection of anatomical land-
marks plays an essential role in the ﬁeld of biomedical
The source code of this work is available at the GitHub repository.
Q. Zhu is with Department of Biomedical Engineering, Case Western
Reserve University, OH, USA. (e-mail: QikuiZhu@163.com).
Y . Bi is with Department of Orthopaedics, The Second Afﬁliated Hos-
pital of Anhui Medical University, Hefei 230601, China. Institute of Or-
thopaedics, Research Center for Translational Medicine, The Second Afﬁl-
iated Hospital of Anhui Medical University, Hefei 230601, China (e-mail:
docbyh1992@163.com).
D. Wang is with the College of Computer Science and Technology, China
University of Petroleum, Qingdao, China (e-mail: wangdx@upc.edu.cn).
X. Chu is with Guangzhou Twelfth people’s Hospital, Guangzhou
Occupational Disease Prevention and Treatment Hospital, Guangzhou
Otolaryngology-head and Neck Surgery Hospital, Guangzhou, China. (e-mail:
13155182545@163.com).
J. Chen is with the Department of Pathology and Institute of Clinical Pathol-
ogy, West China Hospital, Chengdu, China.(e-mail: jzcjedu@foxmail.com).
Y . Wang is with Department of Gynecology, Renmin Hospital of Wuhan
University, Wuhan, China. (e-mail: yanqingwang543@gmail.com).research, as it has contributed to the development of various
medical ﬁelds [1], [2]. For example, to plan cardiac interven-
tions, it is essential to identify standardized planes of the heart;
correspondences between anatomical landmarks in medical
images can offer supplementary guidance information for
deformable medical image registration [3]. In clinical settings,
landmarks are typically annotated manually. Nevertheless,
manual annotation of landmarks is often a tedious and time-
consuming task that requires signiﬁcant expertise in anatomy
and is susceptible to inter- and intra-observer errors. Hence,
there is an urgent need for an accurate and automatic method
for anatomical landmarks detection.
Although existing approaches [4]–[8], including regression-
based methods and heatmap-based methods, have signiﬁcantly
improved landmark detection, they tend to focus more on local
texture information and disregard global spatial structure infor-
mation (Challenge I), as depicted in Figure. 1(a). Speciﬁcally,
regression-based methods try to map the input image to the
landmark coordination via fully connected layers. However,
the fully connected layers would destroy the spatial structure
of local features and further make the localization performance
deteriorates greatly. Although heatmap-based models can pre-
serve the local spatial structure of image features and achieve
better performance than coordination regression-based models
generally, they are more concerned with local texture informa-
tion and neglect the global sensing of the shape of the image,
particularly for the underlying relationship between landmarks,
which makes them vulnerable to large appearance variations.
Furthermore, another unsolved challenge in heatmap-based
detection is inability to accurately evaluate the distribution
of heatmap (Challenge II). As the label of landmarks has
been transformed into probability distributions, existing loss
functions, such as cross-entropy loss and focal loss [9], can not
provide an accurate evaluation for each pixel (Figure. 1(b)).
Therefore, several issues in landmark detection require urgent
attention and resolution.
To address the computational inability challenge for
heatmap-based landmark detection, we propose a novel
position-aware and sample-aware central loss (Figure. 1(2)).
Speciﬁcally, central loss addresses three challenges in
heatmap-based landmark detection: 1) Inability to accurately
evaluate the distribution of heatmap: As the landmark point
is converted to a Gaussian probability, existing losses cannot
accurately evaluate the distribution. 2) Inability to distinguish
easy and hard samples: differences in location and surrounding
tissue make landmarks speciﬁc, and it’s necessary to dis-
Copyright © 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to
servers or lists, or reuse of any copyrighted component of this work in other works.arXiv:2305.11338v1  [cs.CV]  18 May 20232
1)Existing automatic anatomical landmark detection methods arestillfacing twounresolved
challenges .
regression -based 
methodsheatmap -based 
methodXYcoordinates heatmap
➢Destroying thespatial structure oflocal features .
➢Deteriorating localization performance greatly .➢Focusing onlocal texture information, neglecting the
global sensing oftheshape oftheimage .
3)Our novel CoorTransformer addresses thechallenge that inability toeffectively exploit
global spatial structure information .
CoorTransformerStructure -aware attention Coordinate 
information2)Our novel position -aware and sample -aware central loss addresses thecomputational
inability challenge .
loss lossCross -entropy Focal loss
Central lossLandmark label
Challenge II: Inability to accurately evaluate the distribution of heatmap
label valuePredicted label
Model can beguided bytheincorrect loss information, leading toanegative impact onitscoverage and
accuracy .label valuea)Challenges inexisting methodsChallenge I: Inability to effectively exploit global spatial structure information
b)Challenges inaccurately evaluating thedistribution ofheatmapGaussian distribution
label valuePredicted label
label valueLandmark label
loss
➢Position -aware :reasonable andaccurate distribution evaluation ;
➢Sample -aware :adaptively distinguish easy and hard samples,
enabling themodel tofocus more onthehard sample ;
➢Advantage I:absorbing coordination information toexploit
struct relationships between landmarks and incorporate rich
global structural knowledge ;step
➢Advantage II:speeding upconvergence
and effectively avoid the defects of
Transformers insparse representation learning ;speed up 
convergenceloss
CoorTransformerTransformerInnovations
Fig. 1. 1) Existing automatic anatomical landmark detection methods are still facing two unresolved challenges: a) inability to effectively exploit global
spatial structure information; b) inability to accurately evaluate the distribution of heatmap. 2) Our novel central loss can address the computational inability
challenge by a position-aware mechanism and a sample-aware mechanism, which can absorb position information for accurate evaluating distribution and
adaptively distinguish easy and hard samples, enabling the model to focus more on the hard samples. Additionally, 3) our CoorTransformer overcomes the
challenge of inability to effectively exploit global spatial structure information by novelty incorporating rich global structural knowledge.
tinguish between easy and hard landmarks. 3) Inability to
handle extreme imbalance: in the landmark, there is an extreme
imbalance between landmark pixels and background pixels. A
large number of negatives can be overwhelming and make
up the majority of the loss, resulting in the gradient being
dominated by large negative values and ignoring the land-
mark information. Even when adding weight to balance the
importance of positive/negative pixels, it does not differentiatebetween easy/hard pixels. Furthermore, and most importantly,
existing losses such as cross-entropy loss and focal loss are
not perfectly suitable for scenarios where the ground truth is in
the form of probability distributions. As shown in Figure 1(b),
these loss functions may produce incorrect loss information
about the model’s performance. This can result in the model
being guided in the wrong direction, ultimately affecting its
coverage and accuracy. Our position-aware and sample-aware3
central loss solves the above challenges by using two novel
terms: 1) self-weighting term; 2) difference-modulating term.
The self-weighting term is associated with the relative position
of each pixel. Pixels that are closer to the landmark are
assigned greater weights, which makes the central loss is
position-aware and advances the balancing of the inﬂuence of
different pixels for addressing the challenge of extreme imbal-
ance. The difference-modulating term formulates the distance
between the predicted value and the ground truth, providing
accurate distribution evaluation to guide the model toward the
optimal convergence. This term brings two contributions: 1)
it can accurate evaluate the labels with probability values,
overcoming the computational inability challenge, and 2) it’s
sample-aware, which can distinguish easy/hard samples and
pay more attention to the hard samples while solving the
extreme imbalance challenge. (More details about the rela-
tionship between cross-entropy loss and focal loss in Section
III.B.)
Considering inability to effectively exploit global spatial
structure information, we propose a Coordinated Transformer
(Figure. 1(3)), called CoorTransformer, for spatial structure
exploitation. Although Transformer’s [10] attention mech-
anism is capable of encoding long-range dependencies or
heterogeneous interactions [11]–[13], Transformer is known
to be effective only when large training datasets are avail-
able. What’s more, as the attention module assigns nearly
uniform attention weights to all pixels in the feature maps,
long training epochs and enough training samples are re-
quired for the Transformer to focus on sparse landmarks.
Due to the above-mentioned problems, existing Transformers
are not efﬁcient enough to extract meaningful information
from the limited and sparse landmarks and meet the defect
that difﬁcult to converge. Superior to existing Transformers,
CoorTransformer novelty uses coordination information when
establishing long-range dependencies between landmarks. The
coordination information makes the attention more concerned
with spares landmarks, meanwhile, incorporating rich global
structural knowledge. What’s more, under the guidance of the
coordination information, CoorTransformer can quickly and
accurately locate and identify sparse landmarks, which can
speed up convergence and effectively avoid the defects of
Transformers in sparse representation learning. Speciﬁcally,
our CoorTransformer improves the ability of Transformer to
extract the most relevant features from the feature maps in
the vicinity of the target landmark and exploit the underlying
relationship among landmarks for incorporating rich structure
knowledge and quicker converge.
A generalized detection model that can handle various sce-
narios is built upon the CoorTransformer and central loss. The
details of the detector are described in section IV .B. To better
evaluate the advancement of CoorTransformer and central loss,
three challenging landmark detection tasks [8], [14], [15] are
employed. Experimental results show that our central loss
can improve the accuracy of landmark detection and our
CoorTransformer-based detector outperforms the state-of-the-
art methods.
Our contributions include:
For the ﬁrst time, we address the issue of inability toaccurately evaluate the distribution of heatmap by our
novel designed position-aware and sample-aware central
loss.
Our novel central loss is both position-aware and sample-
aware, overcoming computational limitations and en-
abling adaptive differentiation between easy and hard
samples while solving the challenge of extreme imbal-
ance.
Our novel Coordinated Transformer makes signiﬁcant
advancements in exploiting the underlying relationship
between sparse landmarks and incorporating rich struc-
tural knowledge by introducing coordination information.
What’s more, it avoids the defect of Transformer strug-
gling to converge in sparse representation learning.
A generalized detection mode is proposed, which over-
comes the drawback of neglecting global sensing of
spatial structure present in existing methods. Extensive
experiments demonstrate that our CoorTransformer and
central loss contribute to landmark detection under vari-
ous medical images. It shows good generalization ability
in the multi-dataset evaluation and achieves state-of-the-
art accuracy on several landmark detection benchmarks.
II. R ELATED WORKS
A. Landmark detection
Accurate and automatic detection of anatomical landmark
is an essential step for many image analysis and interpretation
methods in the ﬁeld of biomedical research. It has been widely
used in various ﬁelds, including localizing tumors, detecting
organs or structures, and searching for regions of interest in
related medical images.
Recently, a number of automatic detection methods for
anatomical landmarks have been introduced. For example,
Ibragimov et al. [4] proposed a novel landmark-based shape
representation method, which is combined with the game-
theoretic framework for landmark detection. Speciﬁcally, the
connections among landmarks describing a 3-D object are
established according to their cost to the resulting shape
representation and by considering concepts from transportation
theory. Klinder et al. [16] ﬁrst reported an automatically
whole-spine vertebral bone identiﬁcation, detection, segmen-
tation method in CT images. ˇStern et al. [5] investigated
different RRF architectures and proposed a novel random
forest localization algorithm. The proposed algorithm can im-
plicitly model the global conﬁguration of multiple, potentially
ambiguous landmarks and distinguish locally similar structures
by automatically identifying and exploring the back-projection
of the response from accurate local RF predictions. Hanaoka
et al. [17] presented a novel Gibbs-sampling- and importance-
sampling-based combinatory optimization framework for 197
landmarks detection in computed tomography (CT) volumes.
Moreover, the proposed framework can stochastically handle
missing landmarks caused by detection failure. L ´opez-Linares
et al. [18] presented a fully automatic approach for robust and
reproducible thrombus region of interest detection and subse-
quent ﬁne thrombus segmentation based on fully convolutional
networks and holistically-nested edge detection network. Al-
ternatively, Ghesu et al. [19] introduced a new paradigm for4
accurately detecting anatomical landmarks in 3D-CT scans
with arbitrary ﬁeld-of-view by using the concept of multi-
scale DRL and robust statistical shape model. Nair et al. [20]
utilized multiple uncertainty estimates based on Monte Carlo
(MC) dropout and developed a 3D segmentation model for
lesion detection and segmentation in medical images. Vlontzos
et al. [21] formulated the problem of multiple anatomical
landmark detection as a multi-agent reinforcement learning
scenario and proposed a Collaborative DQN for landmark
detection in brain and cardiac MRI volumes and 3D US. Chen
et al. [22] proposed a model-agnostic shape-regulated self-
training framework for semi-supervised landmark detection by
fully utilizing the global shape constraint. Speciﬁcally, a PCA-
based shape model is designed to adjust pseudo labels and
eliminate abnormal ones for ensuring pseudo labels are reliable
and consistent. And a novel Region Attention loss is proposed
to make the network automatically focus on the structure-
consistent regions around pseudo labels. Lee et al. [23]
proposed a single-passing convolutional neural network for
accurate landmark detection. The proposed network ﬁrst re-
gresses the initial positions of all the landmarks for extracts of
global contexts. Subsequently, extracting local features from
landmark-centered patches through global regression. And a
novel patch-wise attention module is proposed to weigh the
relative importance of global and local features for better
feature fusion.
B. Transformer
Recently, Transformer [24] emerges as an active research
area in the computer vision community and has shown its po-
tential to be a viable alternative to CNNs in many task. For ex-
ample, Li et al. [25] proposed a facial landmark detection net-
work DTLD based on the cascaded transformer. The proposed
method can directly regress landmark coordinates and can be
trained end-to-end. Additionally, self-attention and deformable
attention used in DTLD enable structure relationship exploring
and more related image feature extracting. Zhu et al. [11]
introduced a multi-view coupled Transformer that helps over-
come the limitation of the Transformer, where the dimensional
relationship within the Transformer is not considered. Their
approach includes two types of self-attention mechanisms
that examine the relationship between spatial and dimensional
attention, and a view-complementary approach that models
both local and global spatial contextual information. Zhu et
al. [26] proposed a universal model for multi-domain landmark
detection by taking advantage of Transformer for modeling
long dependencies and develop a domain-adaptive transformer
model Wu et al. [27] proposed a pure Transformer based
model for facial landmark detection. Extensive experiments
demonstrate that the proposed Transformer outperforms the
most recent CNN-based methods. Wan et al. [28] utilized refer-
ence heatmap information and proposed a Reference Heatmap
Transformer (RHT) for precise facial landmark detection. The
proposed RHT consists of a Soft Transformation Module
(STM), a Hard Transformation Module (HTM), and a Multi-
Scale Feature Fusion Module (MSFFM). STM and HTM for
extracting the reference heatmap information and facial shape
Cross -entropy Focal loss Central lossLoss
Loss
probability of prediction ground truth = 0.9 ground truth = 0.1
well-classified
exampleswell-classified
examples
0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.03.5
0.00.20.40.60.8Loss is still highLoss is still highFig. 2. Comparison between cross-entropy, focal loss, and central loss
functions, particularly when analyzing positive and negative samples with a
distribution of 0.9 and 0.1, which demonstrates that our central loss function
is more computationally accuracy than cross-entropy and focal loss.
constraints. MSFFM fuses the transformed heatmap features
and the semantic features to enhance feature representations.
III. C ENTRAL LOSS
A. Deﬁnition of central loss
The central loss is speciﬁcally designed to tackle three
challenges in heatmap-based landmark detection tasks: 1)
inability to accurately evaluate the distribution of heatmap; 2)
inability to distinguish easy/hard pixels; 3) difﬁculty dealing
with extreme imbalance between landmark pixels and back-
ground pixels;
Formally, our central loss consists of two novel terms:
(1) self-weighting term; (2) difference-modulating term, as
described in Eq. 1.
CL(px;py) = py|{z}
(1)(2)z}| {
jpy pxjrlog(px) (1)
where,py2[0;1];px2[0;1]is ground truth and predication
results, respectively. r0is weighting parameter. Remark-
able, the ground truth pyis a probability value between 0 and
1.
The self-weighting term is designed to balance the inﬂuence
of positive and negative samples. Unlike the typical weighting
factor, which is determined by inverse class frequency or set
as a hyperparameter through cross-validation, the weighting
factor of the self-weighting term is associated with the relative
position of each pixel, particularly for those pixels surrounding
the landmark point. Speciﬁcally, pixels that are closer to the
landmark are assigned greater weights by py, while pixels that
are farther away are assigned smaller weights by py, which
advances the model in locating the position of the landmark.
The difference-modulating term is introduced for solving
two problems: 1) Inability to accurately evaluate the dis-
tribution of heatmap, as the value of ground truth pyis a
probability. Existing loss functions, such as cross-entropy loss,
cannot accurately evaluate the distribution of the heatmap.5
…Coordinate 
Module
Coordinate -index 
featuresMulti -head Attention
Coordinate Information
Sampled
K
Q
Vstructure -aware 
attention 
Conv 3x 3GELUConv 1x1
C H W
2 H W
Coordinate  Module
x
(a)(b)
p
px
pK
pQ
Voutput
➢Advantage III:Avoiding thedefect that Transformer isdifficult toconverge insparse
representation learning ;➢Advantage I:Making theattention more focused onthe
sparse landmarks ;
➢Advantage II:Taking advantage ofglobal
spatial structure ;
Coordinated Transformer
Fig. 3. Our CoorTransformer establishes long-range dependencies under the guidance of landmark coordination information, making the attention more
focused on the sparse landmarks ( Advantage I ) while taking advantage of global spatial structure ( Advantage II ). Furthermore, CoorTransformer can speed
up convergence, which effectively avoids the defect that Transformer is difﬁcult to converge in sparse representation learning ( Advantage III ). (a) A graphical
illustration of Coordinated Transformer. (b) The detail of the coordinate module.
2) Inability to distinguish easy/hard pixels. Speciﬁcally, the
difference-modulating term formulates the distance between
the predicted value and the ground truth to guide the model
towards converge, which has three contributions: 1) accurately
evaluating the distribution of heatmap; 2) able to distinguish
easy/hard pixels; 3) solving the extreme imbalance between
landmark pixels and back-ground pixels. The experimental
results show that our central loss signiﬁcantly (p-value <0:05)
improves the performance of the landmark detection.
B. Relationship with cross-entropy and focal loss
Intuitively, when r= 0, our central loss is equivalent to
cross-entropy loss. During model training, the cross-entropy
loss assigns equal weights to all pixels, which means that
the loss function treats all misclassiﬁed samples equally. A
large number of misclassiﬁed negative samples may occupy
most of the loss and dominate the gradient, causing landmark
information to be ignored. Therefore, cross-entropy is difﬁcult
to solve the challenge of extreme imbalance between landmark
pixels and background pixels. Our central loss can avoid
the problem by balancing the importance of positive/negative
pixels using the novel difference-modulating term. What’s
more, superior to focal loss, central loss can formulate the
distance between the predicted value and the ground truth,
which guides the model toward convergence. Additionally,
central loss eliminates the need for an -balanced variant and
allows for the adaptive adjustment of the proportion of various
pixels based on their position information.
A more intuitive comparison is shown in Figure. 2. For
well-classiﬁed pixels, where the corresponding ground truth
is 0.1 and 0.9, and the value of prediction is approximately
equal to 0.1 and 0.9, the central loss can accurately evaluate
and compute the loss (the loss value tends to 0). In contrast,
even with perfect predictions, cross-entropy, and focal loss
still provide a high loss for the model. These unconﬁdent loss
values can guide the model in the wrong direction, ultimately
affecting its coverage and accuracy.Discussion of the Innovation :
1) Central loss can accurately evaluate the distribution
of heatmap, which addresses the challenge of inability to
accurately evaluate the distribution of heatmap for heatmap-
based landmark detection tasks.
2) Central loss can distinguish easy and hard pixels, which
enables the model to pay more attention to the hard negatives
and less attention to the easy samples, which addresses the
challenge of inability to distinguish easy/hard pixel.
3) Central loss is associated with the relative position
of each pixel, which addresses the challenge of extreme
imbalance between landmark pixels and back-ground pixels
by assigning greater weights to pixels that are closer to the
landmark, while pixels that are farther away are assigned
smaller weights.
IV. C OORDINATED TRANSFORMER
A. CoorTransformer
While Transformer has achieved remarkable results in var-
ious domains [12], [13], it still faces some challenges. For a
medical image, the key elements are usually equal to image
pixels (HW) and can be very large, which leads to the size
of attention being quadratic large ( HWHW ), and further
makes the convergence of attention tedious. Therefore, Trans-
former requires long training schedules before convergence is
achieved, and the attention weights learned by Transformer
are also almost uniformly distributed across all the pixels
in the feature maps, especially for sparse landmarks. Due to
the above mentioned problems, existing transformers are not
efﬁcient enough to extract meaningful information from the
limited and sparse landmarks.
To overcome the above challenges, we introduce the co-
ordination information into Transformer and propose a novel
Coordinated Transformer (CoorTransformer). Speciﬁcally, as
shown in Figure. 3, CoorTransformer models the relations
among landmarks effectively under the guidance of the loca-
tion information in the feature maps. These focused locations6
Conv
Bock
Conv
Bock
Conv
Bock
Conv
BockU
p
U
pU
p
concatCoorTransformerCoorTransformer
CoorTransformerconcat
concat ConvBockConvBockConvBock
Multi -scale features fusion
Fig. 4. A graphical illustration of Coordinated Transformer based detector,
which overcomes the drawbacks of neglecting global sensing of spatial
structure present in existing methods.
are determined by sampling points which are learned from
the queries by a coordinate module. Formally, given the input
feature map x2RCHW, the uniform grid of points
p2R2HWare computed as the coordination information
through the coordinate module. The values of coordinate
points are linearly spaced 2D coordinates and are normalized
to the range [ 1; +1] , in which ( 1; 1)indicates the top-left
corner and (+1; +1) indicates the bottom-right corner. After-
ward, coordinate-index features xp2RCHWare sampled
at each coordinate based on the coordinate information.
Different from the existing Transformer, the Query sequence
Qp2RcHWand Key sequences Kp2RcHWinside
CoorTransformer are computed through coordinate-index fea-
turesxp2RCHW, By utilizing the input Query, Key, Value
sequenceQ,KandV. the attention is computed as
Structure-aware Attention (Qp;Kp;V) =softmax (QpKpT
p
d)V;
(2)
whereCis the number of channels, HandWrepresent the
spatial dimension. And inside the attention layer, for each
Query element, its similarity to all Key elements is calculated
and then normalized via softmax, which is used to multiply
the Value elements to achieve aggregated output and further
establish long-range relations among each pixel.
Our CoorTransformer has three major advantages: 1) under
the guidance of landmark coordination information, our atten-
tion can focus more on the sparse landmarks; 2) the computed
attention is structure-aware, which can learn the underlying
relationship between landmarks and incorporate rich structure
knowledge. 3) With the information of coordinate index, the
convergence of attention weights is faster, avoiding the defect
that Transformer is difﬁcult to converge in sparse representa-
tion learning.
B. CoorTransformer-based detector
With the advanced CoorTransformer, we propose a
CoorTransformer-based detector for landmark detection,
which can inherently extract the landmark features and exploit
the underlying relationship between landmarks for incorporat-
ing rich structural knowledge. The architecture of the detector
is shown in Figure. 4, which consists of two stages and skip
connections. The encoder stage is constructed by a series
of convolutional blocks, each convolutional block consists
of a convolutional layer, batch normalization, and a Relu
layer for feature extraction and down-sampling. The decoderstage is constructed by a series of up-sampling layers and
CoorTransformer modules with different numbers of attention
heads. Meanwhile, spatial and channel attention is used in
the convolution block of the decoder stage for feature fusion.
Behind the decoder stage, a classiﬁcation head is attached. The
classiﬁcation head consists of a convolutional layer for getting
the ﬁnal detection result.
Discussion of the Innovation :
1) For the ﬁrst time, coordinate information has been incor-
porated into the Transformer for landmark detection, enabling
the exploitation of underlying relationships between sparse
landmarks and the incorporation of rich structural knowledge.
2) CoorTransformer has an advantage in focusing more
on sparse landmarks. With the guidance of coordinate infor-
mation, CoorTransformer overcomes the challenge of Trans-
former struggling to converge in sparse representation learn-
ing.
3) CoorTransformer is structure-aware, allowing it to learn
the underlying relationships between landmarks and incorpo-
rate rich structural knowledge.
4) Our detector, based on CoorTransformer, addresses the
challenge of neglecting global spatial structure information in
existing anatomical landmark detection methods.
V. E XPERIMENTS
A. Datasets and implementation details
Cephalometric Landmark Dataset [14]: The dataset con-
sists of 400 cephalometric radiographs with 19 manually
labeled landmarks by two doctors in each image, and the
ground truth is the average of annotations of the two doctors.
Each image is of size 19352400 with a resolution of
0:1mm0:1mm We randomly choose 150 images for training
and 250 images for testing. During training, we resize the
original image to the size of 416512 to keep the ratio of
width and height.
Digital Hand Atlas Dataset [8]: The hand dataset is also a
public dataset that contains 909 X-ray images of various sizes.
And each image with 19 manually labeled landmarks. During
training, we resize images to the shape of 512 384. And
609 images are used for training and the other 300 images are
used for testing.
Pulmonary Chest X-Ray Dataset [15]: This data is a
public chest dataset that contains two subsets. We select the
China set and exclude cases that are labeled as abnormal
lungs(diseased lungs) to form our experimental dataset. Fi-
nally, the chest dataset has 279 X-ray images. Each image
with six landmarks. During training, we randomly select 229
images used for training and the last 50 images for testing. And
all of the input images are resized to the shape of 512 512.
Training details: The number of CoorTransformer blocks
is [1, 1, 1, 1], and the attention heads number is [8, 4, 2, 1].
The channel expansion factor is = 4. The network is trained
under the supervision of central loss (weighting parameter r
is set to 2). Our model is implemented using PyTorch [30]
and trained end-to-end with Adam [31] optimization. In the
training phase, the learning rate is initially set to 0.0001
and a cyclic scheduler is used to decrease the learning rate7
TABLE I
QUALITY METRICS OF DIFFERENT MODELS ON HEAD ,HAND ,AND CHEST DATASETS SHOW THAT COORTRANSFORMER ACHIEVES THE BEST
PERFORMANCE . *REPRESENTS THE PERFORMANCES COPIED FROM THE ORIGINAL PAPER .
MethodHead Hand Chest
MRESDR(%)MRESDR(%)MRESDR(%)
2mm 2.5mm 3mm 4mm 2mm 4mm 10mm 3px 6px 9px
Ibragimov et al. [4]* 1.84 68.13 74.63 79.77 86.87 – – – – – – – –
ˇStern et al. [5]* – – – – – 0.80 92.20 98.45 99.83 – – – –
Lindner et al. [6]* 1.67 70.65 76.93 82.17 89.85 0.85 93.68 98.95 99.94 – – – –
Urschler et al. [7]* – 70.21 76.95 82.08 89.01 0.80 92.19 98.46 99.95 – – – –
Payer et al. [29]* – 73.33 78.76 83.24 89.75 0.66 94.99 99.27 99.99 – – – –
Zhu et al. [8]* 1.54 77.79 84.65 89.41 94.93 0.84 95.40 99.35 99.75 5.57 57.33 82.67 89.33
Our 1.43 79.28 85.43 90.32 95.33 0.76 95.48 99.35 99.86 4.58 63.82 89.84 93.90
TABLE II
QUALITY METRICS OF DIFFERENT LOSS ON HEAD DATASETS .
EXPERIMENTAL RESULTS DEMONSTRATE THAT CENTRAL LOSS ACHIEVES
THE HIGHEST PERFORMANCE .
Method MRESDR(%)
2mm 2.5mm 3mm 4mm
Cross-entropy loss 1.78 76.59 82.57 87.39 92.91
Weighted cross-entropy loss 4.02 63.24 72.06 80.00 89.09
Focal loss 1.68 78.21 83.75 87.96 93.24
Central loss 1.59 79.60 85.37 89.60 94.61
from 1e-3 to 1e-4 dynamically. For evaluation, the inference
model is chosen as the one with minimum validation loss. The
experiments were carried out on one NVIDIA RTX A4000
GPU with 16G memory and the batch size is set to 2. And
mean radial error (MRE) that measures the Euclidean distance
between prediction and ground truth and successful detection
rate (SDR) with various settings are used as evaluation metrics.
B. Comparison with state-of-the-art methods
CoorTransforme-based detector outperforms state-of-the-art
landmark detection methods including Ibragimov et al. [4],
Stern et al. [5], Lindner et al. [6], Urschler et al. [7], Payer et
al. [29], and Zhu et al. [8]. The results of the CoorTransformer
and comparison methods are shown in Table I. On the head
dataset, our CoorTransformer achieves the best accuracy (1.44
in MRE, and 79.28%, 85.43%, 90.32%, 95.33% in SDR with
2mm, 2.5mm, 3mm, 4mm thresholds). On the hand dataset,
our CoorTransformer achieves the best accuracy (95.48%,
99.35%) in SDR with 2mm, 4mm threshold. On the hand
dataset, our CoorTransformer achieves the best MRE accuracy
(4.58) and the best SDR accuracy (63.82%, 89.84%, 93.90%)
with 3px, 6px, and 9px thresholds, respectively. These im-
provements convincingly demonstrate the effectiveness of
CoorTransformer in exploiting the underlying relationship
among landmarks and incorporating rich structural knowledge.
C. Analysis of central loss
To analyze the effectiveness of our central loss for solv-
ing the challenges associated with heatmap-based landmark
detection, we conduct extensive studies on the Cephalometric
Landmark dataset using representative loss functions including
cross-entropy loss, weighted cross-entropy loss, and focal
Loss. Meanwhile, to ensure an accurate and fair evaluation of
Cross-entropy
Weighted Cross-entropy
Focal loss
Central loss0306090120150
MRE
Cross -entropy Focal loss Weighted 
cross -entropy Central loss***Fig. 5. Our central loss achieves better mean and standard deviation on the
Cephalometric landmark dataset. The distribution of MRE of various Losses.
Symbols ** and * represent signiﬁcant differences between the corresponding
method and our method, p <5e-5 and p <5e-2, respectively.
each loss function, we removed other interfering components,
such as CoorTransformer module, and exclusively employed
Unet as the detection model.
1) Effectiveness of central loss: Table II and Figure. 5
show that our central loss achieved the best quantitative results
on the Cephalometric Landmark dataset. Table II lists the
quantitative evaluation result of our central loss and other
loss functions from a holistic perspective. Our central loss
achieved the best MRE and SDR, which demonstrates that
central loss is more computationally conﬁdent for the heatmap-
based landmark detection task than other well-established
loss functions. Furthermore, from the distribution of MRE of
various losses in Figure. 5, our central loss also obtained a
better mean and standard deviation, which further conﬁrmed
that central loss can effectively discount the effect of easy
negatives, focusing all attention on the hard negative pixel
under extreme imbalance conditions. Meanwhile, as shown in
Table II, the paired t-tests between central loss and the other
three losses were performed on MRE value. The p-Values
(<0:05) of paired t-tests show that the difference between
central loss and the other two base losses is signiﬁcant,
which also proves the superiority of central loss in landmark
detection.
To gain a more comprehensive understanding of the cen-8
0.00.10.20.3
0.00.10.20.30.40.5GT Cross -entropyWeighted
cross -entropy Focal loss Central loss
Cross -entropy Weighted
cross -entropyFocal loss Central loss
Cross -entropy Weighted
cross -entropyFocal loss Central loss0.10±𝟎.𝟏𝟏 0.10±𝟎.𝟎𝟗
0.21±𝟎.𝟎𝟑 0.21±𝟎.𝟎𝟑
0.10±𝟎.𝟏𝟎
0.09±𝟎.𝟏𝟎0.10±𝟎.𝟎𝟖
0.08±𝟎.𝟎𝟔
b)𝐿2distance a) Heatmap results𝐿2
𝐿2
Fig. 6. Our novel central loss outperforms other losses and helps the baseline model achieve the highest accuracy. a) The predicated heatmap from various
loss functions; b) The statistical analysis (mean and standard deviation) of L2distance.
A B C D02461.62 1.59 1.66 1.68
0r=
1r=
2r=
3r=MRE
Fig. 7. Statistical analysis (mean and standard deviation) of MRE for central
loss across varying rvalues.
tral loss, an analysis was conducted on the distribution of
landmarks. Figure 6 presents the predicted results obtained
using different loss functions. It is evident that the weighted
cross-entropy resulted in the poorest predictions with the
highest error rate, even worse than that of the cross-entropy
loss. This observation suggests that simply adding weights
to landmarks is inadequate for addressing the challenges that
arise in landmark detection. In terms of quantitative evaluation
of the alignment between predicted results and ground truth,
the average L2distance between the distribution of predicted
results and ground truth are computed. Central loss exhibited
the most robust match with the distribution of true landmarks,
surpassing the performance of both cross-entropy loss and
focal loss, which demonstrates that cross-entropy and focal
loss cannot accurately evaluate the distribution of landmarkTABLE III
STATISTICAL ANALYSIS FOR CENTRAL LOSS ACROSS VARYING rVALUES .
EXPERIMENTAL RESULTS DEMONSTRATE THAT CENTRAL LOSS ACHIEVES
THE HIGHEST PERFORMANCE WHEN r= 2.
Value MRESDR(%)
2mm 2.5mm 3mm 4mm
r= 0 1.68 76.67 82.84 88.38 93.62
r= 1 1.62 79.26 85.18 89.83 94.88
r= 2 1.59 79.60 85.37 89.60 94.61
r= 3 1.66 77.28 83.05 88.29 93.60
and the feedback information mislead the coverage of model.
2) Impact of weighting parameter r:Table. III and Fig-
ure. 7 list the inﬂuence of ron central loss. When r= 0, the
central loss is identical to the cross-entropy loss. In this case,
Unet produces inferior results, indicating that cross-entropy
loss is inadequate in addressing the issue of extreme sample
imbalance and probability-based labels. However, according to
the survey, cross-entropy loss is the most frequently used loss
function in heatmap-based landmark detection tasks. As the
value ofrincreases, the difference-modulating term formu-
lates the distance between the predicted value and the ground
truth to guide the model convergence. Additionally, this term
can effectively distinguish the easy and hard pixels, making
the model focus all attention on the hard negative examples for
improving the performance of the model. Speciﬁcally, when
r= 1, Unet achieved improvements of 0.06, 2.59%, 2.34%,
1.45%, and 1.26% in MRE and SDR (2mm, 2.5mm, 3mm, and
4mm), respectively. When r= 2, the improvements of Unet
in MRE and SDR (2mm, 2.5mm, 3mm, and 4mm) are 0.09,
2.93%, 2.53%, 1.22%, and 0.99%, respectively. When r= 3,9
TABLE IV
QUALITY METRICS OF DIFFERENT TRANSFORMERS ON HEAD ,HAND ,AND CHEST DATASETS . EXPERIMENTAL RESULTS DEMONSTRATE THAT
COORTRANSFORMER ACHIEVES THE HIGHEST PERFORMANCE .
MethodHead Hand Chest
MRESDR(%)MRESDR(%)MRESDR(%)
2mm 2.5mm 3mm 4mm 2mm 4mm 10mm 3px 6px 9px
Baseline 1.78 76.59 82.57 87.39 92.91 0.82 95.69 99.11 99.64 9.54 52.03 80.08 91.46
Transformer 1.75 75.58 82.31 87.18 92.99 0.84 95.96 99.27 99.60 6.32 54.88 85.37 91.87
Swin Transformer 1.56 77.33 83.05 89.18 95.14 0.79 95.94 99.26 99.66 6.84 53.66 80.49 90.24
Deformable Transformer 1.55 76.32 83.18 88.69 94.88 0.82 96.03 99.14 99.60 5.55 50.00 82.21 92.68
CoorTransformer 1.43 79.28 85.43 90.32 95.33 0.76 95.48 99.35 99.86 4.58 63.82 89.84 93.90
Baseline Transformer SwinTransformer Deformabel Transformer CoorTransformer
Fig. 8. Experimental results demonstrate that our CoorTransformer outper-
forms other Transformers and helps the baseline model achieve the highest
accuracy. The red points are the predicted landmarks while the green points
are the ground truth labels. On the top left corner of the image is the MRE
value.
the improvements of Unet in MRE and SDR (2mm, 2.5mm)
are 0.02, 0.61%, and 0.21%, respectively. Two innovations are
responsible for the improved performance: 1) the difference-
modulating term provides a reasonable and computationally
conﬁdent solution to handling probability-based label values;
2) by combining the difference-modulating term with a self-
weighting term, the model is able to focus more on hard
negative examples and reduce attention on easier ones.
D. Comparison with state-of-the-art Transformers
We also compare CoorTransformer with state-of-the-
art Transformers, including Transformer [10], Swin Trans-
former [32], Deformable Attention Transformer [33], for
demonstrating the superiority of the CoorTransformer in parse
representations learning.
1) Effective of CoorTransformer: The results of the various
Transformers are shown in Table IV and Figure. 8. Coor-
Transformer beats all Transformers and helps the baseline
(without Transformer module) model achieve the highest ac-
curacy in head and hand datasets, which demonstrates that
CoorTransformer is more effective in exploiting structural
relationships between landmarks. Speciﬁcally, on the head
data, the improvements in MRE and SDR (2mm, 2.5mm,
3mm and 4mm) are 0.35, 2.69%, 2.86%, 2.93% and 2.42%,
0 10 20 30 40 50200040006000800010000
Training loss
StepBaseline
Transformer
Swin Transformer
Deformabl eTransformer
CoorTransformerFig. 9. Experimental results demonstrate that CoorTransformer outperforms
all other Transformers in terms of convergence.
respectively. On the hand data, the improvements in MRE
and SDR (2mm, 4mm, 10mm) are 0.06, 0.21%, 0.24%, and
0.22%, respectively. And on the chest data, the improvements
in MRE and SDR (3px, 6px, 9px) are 4.87, 11.79%, 9.76%,
and 2.44%, respectively. These improvements convincingly
demonstrate the effectiveness of CoorTransformer in modeling
content information among landmarks and capturing structural
information. Qualitative evaluation is also evaluated on three
datasets. As it can be seen from Figure. 8, CoorTransformer
also achieves a higher level of detection accuracy, which
demonstrates that the innovations in our framework bring
signiﬁcant enhancement of Transformer in landmark detection.
2) Advance of CoorTransformer: Figure. 9 illustrates the
loss convergence of several Transformers on the Head dataset.
We can observe that the Transformer has the slowest con-
vergence speed, even slower than the baseline. The major
reason is that the vanilla self-attention module cannot effec-
tively and reasonably model long-range dependencies among
sparse landmarks. Our CoorTransformer outperforms all other
Transformers in terms of convergence, which demonstrates
that our CoorTransformer is more resilient and efﬁcient when
dealing with learning sparse representations.
VI. C ONCLUSION
For the ﬁrst time, we have proposed a more effective
and computational accurate central loss for the heatmap-
based landmark detection, which addresses three challenges:
1) inability to accurately evaluate the distribution of heatmap;
2) inability to differentiate between easy and hard pixels; 3)10
difﬁculty dealing with extreme imbalance between landmark
and background pixels. Furthermore, we have addressed the
challenge of neglecting global sensing of spatial structure in
existing landmark detection methods by designing a novel
Coordinated Transformer. Our CoorTransformer incorporates
coordination information when establishing long-range depen-
dencies between pixels, making the attention more focused on
sparse landmarks while incorporating the global spatial struc-
ture information. What’s more, CoorTransformer effectively
avoids the defect that Transformer is difﬁcult to converge
in sparse representation learning. Experimental results from
three landmark detection tasks demonstrate that our Coor-
Transformer can effectively extract the underlying relationship
between landmarks for incorporating rich global structural
knowledge. And our central loss signiﬁcantly improves both
the qualitative and quantitative performance of the landmark
detection model.
REFERENCES
[1] B. Rahmatullah, A. T. Papageorghiou, and J. A. Noble, “Image anal-
ysis using machine learning: Anatomical landmarks detection in fetal
ultrasound images,” in 2012 IEEE 36th Annual Computer Software and
Applications Conference . IEEE, 2012, pp. 354–355.
[2] A. Gertych, A. Zhang, J. Sayre, S. Pospiech-Kurkowska, and H. Huang,
“Bone age assessment of children using a digital hand atlas,” Comput-
erized medical imaging and graphics , vol. 31, no. 4-5, pp. 322–331,
2007.
[3] M. Grewal, T. M. Deist, J. Wiersma, P. A. Bosman, and T. Alderli-
esten, “An end-to-end deep learning approach for landmark detection
and matching in medical images,” in Medical Imaging 2020: Image
Processing , vol. 11313. SPIE, 2020, pp. 548–557.
[4] B. Ibragimov, B. Likar, F. Pernu ˇs, and T. Vrtovec, “Shape representation
for efﬁcient landmark-based segmentation in 3-d,” IEEE transactions on
medical imaging , vol. 33, no. 4, pp. 861–874, 2014.
[5] D. ˇStern, T. Ebner, and M. Urschler, “From local to global random
regression forests: exploring anatomical landmark localization,” in Med-
ical Image Computing and Computer-Assisted Intervention–MICCAI
2016: 19th International Conference, Athens, Greece, October 17-21,
2016, Proceedings, Part II 19 . Springer, 2016, pp. 221–229.
[6] C. Lindner, P. A. Bromiley, M. C. Ionita, and T. F. Cootes, “Robust and
accurate shape model matching using random forest regression-voting,”
IEEE transactions on pattern analysis and machine intelligence , vol. 37,
no. 9, pp. 1862–1874, 2014.
[7] M. Urschler, T. Ebner, and D. ˇStern, “Integrating geometric conﬁguration
and appearance information into a uniﬁed framework for anatomical
landmark localization,” Medical image analysis , vol. 43, pp. 23–36,
2018.
[8] H. Zhu, Q. Yao, L. Xiao, and S. K. Zhou, “You only learn once:
Universal anatomical landmark detection,” in Medical Image Computing
and Computer Assisted Intervention–MICCAI 2021: 24th International
Conference, Strasbourg, France, September 27–October 1, 2021, Pro-
ceedings, Part V 24 . Springer, 2021, pp. 85–95.
[9] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss
for dense object detection,” in Proceedings of the IEEE international
conference on computer vision , 2017, pp. 2980–2988.
[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in Neural Information Processing Systems , 2017, pp. 5998–6008.
[11] Q. Zhu, Y . Wang, X. Chu, X. Yang, and W. Zhong, “Multi-view
coupled self-attention network for pulmonary nodules classiﬁcation,” in
Proceedings of the Asian Conference on Computer Vision , 2022, pp.
995–1009.
[12] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V . M. Patel, “Medical
transformer: Gated axial-attention for medical image segmentation,” in
Medical Image Computing and Computer Assisted Intervention–MICCAI
2021: 24th International Conference, Strasbourg, France, September
27–October 1, 2021, Proceedings, Part I 24 . Springer, 2021, pp. 36–46.
[13] A. Hatamizadeh, Y . Tang, V . Nath, D. Yang, A. Myronenko, B. Land-
man, H. R. Roth, and D. Xu, “Unetr: Transformers for 3d medical image
segmentation,” in Proceedings of the IEEE/CVF winter conference on
applications of computer vision , 2022, pp. 574–584.[14] C.-W. Wang, C.-T. Huang, J.-H. Lee, C.-H. Li, S.-W. Chang, M.-J.
Siao, T.-M. Lai, B. Ibragimov, T. Vrtovec, O. Ronneberger et al. , “A
benchmark for comparison of dental radiography analysis algorithms,”
Medical image analysis , vol. 31, pp. 63–76, 2016.
[15] S. Jaeger, A. Karargyris, S. Candemir, L. Folio, J. Siegelman,
F. Callaghan, Z. Xue, K. Palaniappan, R. K. Singh, S. Antani et al. ,
“Automatic tuberculosis screening using chest radiographs,” IEEE trans-
actions on medical imaging , vol. 33, no. 2, pp. 233–245, 2013.
[16] T. Klinder, J. Ostermann, M. Ehm, A. Franz, R. Kneser, and C. Lorenz,
“Automated model-based vertebra detection, identiﬁcation, and segmen-
tation in ct images,” Medical Image Analysis , vol. 13, no. 3, pp. 471–482,
2009.
[17] S. Hanaoka, A. Shimizu, M. Nemoto, Y . Nomura, S. Miki, T. Yoshikawa,
N. Hayashi, K. Ohtomo, and Y . Masutani, “Automatic detection of over
100 anatomical landmarks in medical ct images: A framework with
independent detectors and combinatorial optimization,” Medical image
analysis , vol. 35, pp. 192–214, 2017.
[18] K. L ´opez-Linares, N. Aranjuelo, L. Kabongo, G. Maclair, N. Lete,
M. Ceresa, A. Garc ´ıa-Familiar, I. Mac ´ıa, and M. A. G. Ballester, “Fully
automatic detection and segmentation of abdominal aortic thrombus in
post-operative cta images using deep convolutional neural networks,”
Medical image analysis , vol. 46, pp. 202–214, 2018.
[19] F. C. Ghesu, B. Georgescu, S. Grbic, A. Maier, J. Hornegger, and D. Co-
maniciu, “Towards intelligent robust detection of anatomical structures
in incomplete volumetric data,” Medical image analysis , vol. 48, pp.
203–213, 2018.
[20] T. Nair, D. Precup, D. L. Arnold, and T. Arbel, “Exploring uncertainty
measures in deep networks for multiple sclerosis lesion detection and
segmentation,” Medical image analysis , vol. 59, p. 101557, 2020.
[21] A. Vlontzos, A. Alansary, K. Kamnitsas, D. Rueckert, and B. Kainz,
“Multiple landmark detection using multi-agent reinforcement learning,”
inMedical Image Computing and Computer Assisted Intervention–
MICCAI 2019: 22nd International Conference, Shenzhen, China, Oc-
tober 13–17, 2019, Proceedings, Part IV 22 . Springer, 2019, pp. 262–
270.
[22] R. Chen, Y . Ma, L. Liu, N. Chen, Z. Cui, G. Wei, and W. Wang,
“Semi-supervised anatomical landmark detection via shape-regulated
self-training,” Neurocomputing , vol. 471, pp. 335–345, 2022.
[23] M. Lee, M. Chung, and Y .-G. Shin, “Cephalometric landmark detection
via global and local encoders and patch-wise attentions,” Neurocomput-
ing, vol. 470, pp. 182–189, 2022.
[24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: Transformers for image recognition
at scale,” arXiv preprint arXiv:2010.11929 , 2020.
[25] H. Li, Z. Guo, S.-M. Rhee, S. Han, and J.-J. Han, “Towards accurate
facial landmark detection via cascaded transformers,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 4176–4185.
[26] H. Zhu, Q. Yao, and S. K. Zhou, “Datr: Domain-adaptive transformer
for multi-domain landmark detection,” arXiv preprint arXiv:2203.06433 ,
2022.
[27] W. Wu, Y . Cai, and Q. Zhou, “Transmarker: A pure vision transformer
for facial landmark detection,” in 2022 26th International Conference
on Pattern Recognition (ICPR) . IEEE, 2022, pp. 3580–3587.
[28] J. Wan, J. Liu, J. Zhou, Z. Lai, L. Shen, H. Sun, P. Xiong, and W. Min,
“Precise facial landmark detection by reference heatmap transformer,”
arXiv preprint arXiv:2303.07840 , 2023.
[29] C. Payer, D. ˇStern, H. Bischof, and M. Urschler, “Integrating spatial
conﬁguration into heatmap regression based cnns for landmark localiza-
tion,” Medical image analysis , vol. 54, pp. 207–219, 2019.
[30] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An
imperative style, high-performance deep learning library,” in Advances
in Neural Information Processing Systems , 2019, pp. 8024–8035.
[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.
[32] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin transformer: Hierarchical vision transformer using shifted
windows,” in Proceedings of the IEEE/CVF International Conference on
Computer Vision , 2021, pp. 10 012–10 022.
[33] Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang, “Vision transformer
with deformable attention,” in Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , 2022, pp. 4794–4803.