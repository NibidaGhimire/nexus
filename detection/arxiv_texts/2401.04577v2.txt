Published as a conference paper at ICLR 2024
MASKED AUDIO GENERATION USING A SINGLE NON-
AUTOREGRESSIVE TRANSFORMER
∗Alon Ziv1,3, Itai Gat1, Gael Le Lan1, Tal Remez1, Felix Kreuk1, Alexandre D ´efossez2
Jade Copet1, Gabriel Synnaeve1, Yossi Adi1,3
1FAIR Team, Meta
2Kyutai
3The Hebrew University of Jerusalem
alonzi@cs.huji.ac.il
ABSTRACT
We introduce MAGN ET, a masked generative sequence modeling method that
operates directly over several streams of audio tokens. Unlike prior work, MAG-
NET is comprised of a single-stage, non-autoregressive transformer. During
training, we predict spans of masked tokens obtained from a masking sched-
uler, while during inference we gradually construct the output sequence using
several decoding steps. To further enhance the quality of the generated audio,
we introduce a novel rescoring method in which, we leverage an external pre-
trained model to rescore and rank predictions from MAGN ET, which will be
then used for later decoding steps. Lastly, we explore a hybrid version of MAG-
NET, in which we fuse between autoregressive and non-autoregressive models
to generate the first few seconds in an autoregressive manner while the rest of
the sequence is being decoded in parallel. We demonstrate the efficiency of
MAGN ET for the task of text-to-music and text-to-audio generation and con-
duct an extensive empirical evaluation, considering both objective metrics and
human studies. The proposed approach is comparable to the evaluated base-
lines, while being significantly faster (x 7faster than the autoregressive base-
line). Through ablation studies and analysis, we shed light on the importance
of each of the components comprising MAGN ET, together with pointing to the
trade-offs between autoregressive and non-autoregressive modeling, considering
latency, throughput, and generation quality. Samples are available on our demo
pagehttps://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT
1 I NTRODUCTION
Recent developments in self-supervised representation learning (Hsu et al., 2021; D ´efossez et al.,
2022), sequence modeling (Touvron et al., 2023; Rozi `ere et al., 2023), and audio synthesis (Lee
et al., 2022; Polyak et al., 2021) allow a great leap in performance when considering high quality
conditional audio generation. The prominent approach, in recent years, is to represent the audio
signals as a compressed representation, either discrete or continuous, and apply a generative model
on top of it (Lakhotia et al., 2021; Kharitonov et al., 2022; Borsos et al., 2023a; Kreuk et al., 2022a;
Copet et al., 2023; Lam et al., 2023; Agostinelli et al., 2023; Gat et al., 2023; Sheffer & Adi, 2023;
Maimon & Adi, 2022; Schneider et al., 2023; Huang et al., 2023b; Liu et al., 2023a; Li et al., 2023;
Liu et al., 2023b). Recently, D ´efossez et al. (2022); Zeghidour et al. (2021) proposed to apply a
VQ-V AE directly on the raw waveform using residual vector quantization to obtain a multi-stream
discrete representation of the audio signal. Later on, Kreuk et al. (2022a); Wang et al. (2023); Zhang
et al. (2023); Copet et al. (2023); Kreuk et al. (2022b) presented a conditional language modeling
on such audio signals representations. In parallel, Schneider et al. (2023); Huang et al. (2023b); Liu
et al. (2023a) proposed training a conditional diffusion-based generative model operating on learned
continuous representations of the audio signal obtained from a pre-trained auto-encoder model.
∗Work was done as part of Alon’s internship at FAIR.
1arXiv:2401.04577v2  [cs.SD]  5 Mar 2024Published as a conference paper at ICLR 2024
Overall, the family of generative models explores in prior work can be roughly divided into two: (i)
autoregressive (AR) in the form of language models (LMs), usually operating on discrete audio rep-
resentations; and (ii) diffusion-based models usually operating on continuous latent representations
of the audio signal. Although providing impressive generation results, these approaches have several
main drawbacks. Due to its autoregressive nature, following the LM approach yields relatively high
inference time which turns into high latency values, hence making it less appalling for interactive
applications such as music generation and editing under Digital Audio Workstations (DAW). On the
other hand, diffusion models perform parallel decoding, however, to reach high-quality music sam-
ples recent studies report using a few hundred diffusion decoding steps (Huang et al., 2023a; Liu
et al., 2023b). Moreover, diffusion models struggle with generating long-form sequences. Recent
studies present results for either 10-second generations (Liu et al., 2023b; Li et al., 2023; Yang et al.,
2022) or models that operate in low resolution followed by a cascade of super-resolution models to
reach 30-second segments (Huang et al., 2023a).
In this work, we present MAGN ET, a short for Masked Audio Generation using Non-autoregressive
Transformers. MAGN ET is a novel masked generative sequence modeling operating on a multi-
stream representation of an audio signal. The proposed approach comprised of a single transformer
model, working in a non-autoregressive fashion. During training, we first sample a masking rate
from the masking scheduler, then, we mask and predict spans of input tokens conditioned on un-
masked ones. During inference, we gradually build the output audio sequence using several de-
coding steps. We start from a fully masked sequence and at each iteration step, we fix the most
probable token spans, i.e., the spans that got the top confidence score. To further enhance the quality
of the generated audio, we introduce a novel rescoring method. In which, we leverage an external
pre-trained model to rescore and rank predictions from MAGN ET. Lastly, we explore a Hybrid
version of MAGN ET, in which we fuse autoregressive and non-autoregressive models. The hybrid-
MAGN ET generates the beginning of the tokens sequence in an autoregressive manner while the
rest of the sequence is being decoded in parallel, similarly to the original MAGN ET. A visual de-
scription of the inference of the proposed method can be seen in Fig. 1.
Similar non-autoregressive modeling was previously proposed by Ghazvininejad et al. (2019) for
machine translation, Chang et al. (2022) for class-conditional image generation and editing, and
Chang et al. (2023) for image generation guided by rich textual description followed by a super-
resolution component. Borsos et al. (2023b) recently proposed SoundStorm, a non-autoregressive
method for the task of text-to-speech and dialogue synthesis. SoundStorm is conditioned on “seman-
tic” tokens obtained from an autoregressive model. Unlike, SoundStorm, MAGN ET is composed of
a single non-autoregressive model and was evaluated on music and audio generation which, unlike
speech, leverages the full frequency spectrum of the signal.
We evaluate the proposed approach considering both text-to-music and text-to-audio generation. We
report objective metrics together with a human study and show the proposed approach achieves com-
parable results to the evaluated baselines while having significantly reduced latency (x 7faster than
the autoregressive-based method). We further present an analysis of the proposed method consid-
ering latency, throughput, and generation quality. We present the trade-offs between the two when
considering autoregressive and non-autoregressive models. Lastly, we provide an ablation study that
sheds light on the contribution of each component of the proposed approach to the performance.
Our contributions: (i) We present a novel non-autoregressive model for the task of audio modeling
and generation, denoted as MAGN ET. The proposed method is able to generate relatively long
sequences ( 30seconds long), using a single model. The proposed approach has a significantly
faster inference time while reaching comparable results to the autoregressive alternative; (ii)
We leverage an external pre-trained model during inference to improve generation quality via a
rescoring method; and (iii) We show how the proposed method can be combined with autoregressive
modeling to reach a single model that performs joint optimization, denoted as Hybrid-MAGN ET.
2 B ACKGROUND
Audio representation. Modern audio generative models mostly operate on a latent representation
of the audio, commonly obtained from a compression model (Borsos et al., 2023a; Kreuk et al.,
2022a; Yang et al., 2022). Compression models such as Zeghidour et al. (2021) employ Residual
Vector Quantization (RVQ) which results in several parallel streams. Under this setting, each stream
2Published as a conference paper at ICLR 2024
is comprised of discrete tokens originating from different learned codebooks. Prior work, proposed
several modeling strategies to handle this issue (Kharitonov et al., 2022; Wang et al., 2023).
Specifically, D ´efossez et al. (2022) introduced EnCodec , a convolutional auto-encoder with a latent
space quantized using Residual Vector Quantization (RVQ) (Zeghidour et al., 2021), and an adver-
sarial reconstruction loss. Given a reference audio signal x∈Rd·fswithdthe audio duration and
fsthe sample rate, EnCodec first encodes it into a continuous tensor with a frame rate fr≪fs.
Then, this representation is quantized into z∈ {1, . . . , N }K×d·fr, with Kbeing the number of
codebooks used in RVQ and Nbeing the codebook size. Notice, after quantization we are left with
Kdiscrete token sequences, each of length T=d·fr, representing the audio signal. In RVQ, each
quantizer encodes the quantization error left by the previous quantizer, thus quantized values for
different codebooks are in general dependent, where the first codebook is the most important one.
Audio generative modeling. Given a discrete representation of the audio signal, z, our goal is to
model the conditional joint probability distribution pθ(z|y), where yis a semantic representation of
the condition. Under the autoregressive setup we usually follow the chain rule of probability, thus
the joint probability of a sequence can be computed as a product of its conditional probabilities:
pθ(z1, . . . , z n|y) =nY
i=1pθ(zi|zi−1, . . . , z 1, y). (1)
The above probability chain rule can be thought of as a masking strategy, where, in each time step i,
we predict the probability of the i-th token, given its past tokens, while we mask future tokens. For
that, we define a masking function m(i), that mask out all tokens larger than i, which results in:
pθ(z1, . . . , z n|y) =nY
i=1pθ(zi|(1−m(i))⊙z, y), (2)
where each element in m(i) = [m1(i), . . . , m T(i)]is defined as mj(i) = 1[j≥i].Notice, Eq. (2)
does not hold for any masking strategy. One should pick a masking strategy that satisfies the proba-
bility chain rule.
Extending Eq. (2) to the non-autoregressive setup can be done by modifying the masking strategy
and the decomposition of the joint probability to predict an arbitrary subset of tokens given the
unmasked ones using several decoding steps. Let us formally define the masking strategy as follows,
mj(i)∼ 1[j∈ M i]where Mi∼ U 
{A ⊆ M i−1:|A|=γ(i;s)·T}
, (3)
andγis the masking scheduler, with sdecoding steps, defined as γ(i;s) = cos(π(i−1)
2s)andM0=
{1, . . . , T }. In other words, at each time step iwe mask a subset of γ(i;s)·Ttokens sampled from
the masked set at the previous time step. Thus the modified version of Eq. (2) is,
pθ(z1, . . . , z n|y) =sY
i=1pθ(m(i)⊙z|(1−m(i))⊙z, y). (4)
In practice, during training, a decoding time step i∈[1, s]and the tokens to be masked from M0
are randomly sampled. The tokens at indices t∈ M iare then replaced by a special mask token,
and the model is trained to predict the target tokens at the masked positions Migiven the unmasked
tokens. This modeling paradigm was previously explored by Ghazvininejad et al. (2019); Chang
et al. (2022; 2023); Borsos et al. (2023b).
Recall, the audio representation is composed of multi-stream sequences created by RVQ. In which,
the first codebook encodes the coarse information of the signal while later codebooks encode the
quantization error to refine the generation quality. To handle that, Borsos et al. (2023b) proposed to
predict tokens from codebook kgiven its preceding codebooks. During training, a codebook level
k, is being uniformly sampled from {1, . . . , K }. Then, we mask and predict the tokens of the k-th
codebook given previous levels via teacher forcing. At inference, we sequentially generate the token
streams, where each codebook is being generated conditioned on previously generated codebooks.
3 M ETHOD
Following the approach presented in the previous section solely does not lead to high-quality audio
generation. We hypothesize this is due to three factors: (i) The masking strategy operates over
3Published as a conference paper at ICLR 2024
13 993 2 2 422 1
Restricted
TransformerCross Attention
״Classic reggae track
with an electronic 
guitar solo ״Text EmbeddingsT5Candidate
RVQ Tokens
Masked
RVQ TokensRescoring
13 993 129 41 1 422 83
13 993 422Rescoring
......
Decoding Iteration 1 Decoding Iteration 2 Decoding Iteration sEnCodec
251 888 12 33
1009 788 22 76 892 37
32 55 20 249 591 992
13 993 129 323 422 83251 888 12 732 222 33
1009 788 22 76 892 37
32 55 20 249 591 992
13 993 129 323 422 83
Span
MaskingRestricted
TransformerRestricted
Transformer
Span
Masking13 993 129 323 422 83
13 993 422Rescoring
Restricted
Transformer
Span
Masking
13 993 422Rescoring
Restricted
Transformer
Span
Masking
13 993 129 323 422 83Rescoring
Restricted
Transformer
Span
Masking32 55 16 1 19 591 10
13 993 129 323 422 83
Decoding Iteration 3
Figure 1: Inference of MAGN ET model. During each iteration, we mask a subset of token spans
(starting from a fully masked sequence). Next, we rescore the tokens based on an external pre-
trained model. Finally, we select the token spans to be re-masked for the next decoding iteration.
individual tokens that share information with adjacent tokens. Hence, allowing the model to “cheat”
during tokens prediction while being trained using teacher forcing; (ii) The temporal context of the
codebooks at levels greater than one, is generally local and influenced by a small set of neighboring
tokens. This affects model optimization; (iii) Sampling from the model at different decoding steps
requires different levels of diversity with respect to the condition. Also sampling can be combined
with external scoring models.
In this section, we present MAGN ET in details. MAGN ET consists of a non-autoregressive audio-
based generative masked language model, conditioned on a semantic representation of the condition,
operating on several streams of discrete audio tokens obtained from EnCodec . We follow a similar
modeling strategy as presented in Section 2 while introducing core modeling modifications consist-
ing of masking strategy, restricted context, sampling mechanism, and model rescoring.
3.1 M ASKING STRATEGY
Adjacent audio tokens often share information due to the receptive field of the audio encoder. Hence,
we use spans of tokens as the atomic building block of our masking scheme, rather than individual
ones as done in prior work. We evaluated various span lengths lbetween 20ms to 200ms and found
a 60ms span length to give the best overall performance (see Section 5.3 for detailed results). We
sample a masking rate γ(i), from the scheduler, and compute the average amount of spans to be
masked accordingly. As spans may overlap, this process requires a careful design. We select the
number of spans u, that satisfies 1− T−l
u
/ T
u
≈γ(i), where lis the span length. The above
expression is the expected masking rate over all possible placements of uspans of length lover
the sequence. Full derivation can be found in the Appendix C. During inference time, we follow a
similar strategy, in which we re-mask the least probable spans instead of individual tokens as done
in prior work. We consider the span’s probability as the token with the maximal probability. For
computational efficiency, we use non-overlapping spans.
3.2 R ESTRICTED CONTEXT
Recall, the used audio tokenizer is based on RVQ, where each quantizer encodes the quantization
error left by the previous quantizer. Thus quantized codebooks later than the first one heavily de-
pend on previous codebooks rather than surrounding tokens. To leverage that we analyze the used
EnCodec and restrict the context of the codebooks accordingly.
Specifically, the audio encoder consists of a multi-layer convolutional network and a final LSTM
block. Analyzing the receptive field for the used EnCodec shows that the receptive field of the con-
volutional network is ∼160ms, while the effective receptive field when including the LSTM block
is∼180ms. We empirically estimate the receptive field of the model, using a shifted impulse func-
tion over time while measuring the magnitude of the encoded vector in the middle of the sequence.
4Published as a conference paper at ICLR 2024
Fig. 3 in Appendix G depicts such process. Notice, although theoretically, the LSTM has an infinite
memory, practically we observe it is bounded.
We utilize this observation to improve model optimization, by restricting the self-attention of code-
books greater than 1, to attend only on tokens at a temporal distance smaller than ∼200ms. Similar
ideas were proposed in the context of language modeling by Rae & Razavi (2020); Roy et al. (2021).
We depict the used attention map for the restricted context in Fig. 8.
3.3 M ODEL INFERENCE
Sampling as described in Eq. (3) uses a uniform sampling to choose spans from the previously set
of masked spans. In practice, we use the model confidence at the i-th iteration as a scoring function
to rank all possible spans and choose the least probable spans to be masked accordingly. However,
the scoring function does not have to be part of the generative model.
A common practice in Automatic Speech Recognition (ASR) decoding, is to generate a set of dif-
ferent hypotheses from one model and rescore them using another model (Benesty et al., 2008;
Likhomanenko et al., 2020). Inspired by the ASR rescoring method, we propose a novel strategy in
which at iteration iwe generate a candidate token sequence using MAGN ET. Then, we feed it to
an external model and get a new set of probabilities for each of the token spans. Lastly, we use a
convex combination of both probabilities (the one emitted by MAGN ET and the one obtained from
the rescorer model), to sample from:
p(z|y) =w·pθ(z|y) + (1 −w)·prescorer (z|y). (5)
In this work, we use M USIC GENand AudioGen as our rescorering models (in a non-autoregressive
manner). The proposed rescoring method is generic and is not tied to any specific rescoring model.
Following the proposed approach improves the generated audio quality and stabilizes inference. A
pseudo-code of our entire decoding algorithm is described in Fig. 4, Appendix D.
Classifier-free guidance annealing. Token prediction is done using a Classifier-Free Guidance
(CFG) (Ho & Salimans, 2022). During training, we optimize the model both conditionally and
unconditionally, while at inference time we sample from a distribution obtained by a linear combi-
nation of the conditional and unconditional probabilities.
While prior work (Copet et al., 2023; Kreuk et al., 2022a) used a fixed guidance coefficient, λ >1,
we instead use a CFG annealing mechanism controlled by the masking schedule γ. As the masking
rateγ(i)decreases, the guidance coefficient is annealed during the iterative decoding process. The
motivation behind this approach is to gradually reduce text adherence and guide the generation
process toward the already fixed tokens. Intuitively, this transforms the sampling process from
textually guided to contextual infilling. Formally, we use a CFG coefficient of
λ(i) =γ(i)·λ0+ (1−γ(i))·λ1, (6)
where λ0andλ1are the initial and final guidance coefficients respectively. This approach was also
found to be beneficial in 3D shape generation (Sanghi et al., 2023).
4 E XPERIMENTAL SETUP
Implementation details. We evaluate MAGN ET on the task of text-to-music generation and text-
to-audio generation. We use the exact same training data as using by Copet et al. (2023) for music
generation and by Kreuk et al. (2022a) for audio generation. A detailed description of the used
dataset can be found on Appendix A.2. We additionally provide a detailed description about the
datasets used to train the evaluated baselines in Table 4.
Under all setups, we use the official EnCodec model as was published by Copet et al. (2023); Kreuk
et al. (2022a)1. The model gets as input an audio segment and outputs a 50Hz discrete represen-
tation. We use four codebooks, where each has a codebook size of 2048 . We perform the same
text preprocessing as proposed by Copet et al. (2023); Kreuk et al. (2022a). We use a pre-trained
T5 Raffel et al. (2020) model to extract semantic representation from the text description and use it
as model conditioning.
1https://github.com/facebookresearch/audiocraft
5Published as a conference paper at ICLR 2024
Table 1: Comparison to Text-to-Music Baselines. The Mousai and MusicGen models were retrained
on the same dataset, while for MusicLM we use the public API for human studies. We report the
original FAD for AudioLDM2, and MusicLM. For human studies, we report mean and CI95.
MODEL FADvgg↓KL↓CLAP scr↑ OVL.↑ REL.↑ # STEPS LATENCY (S)
Reference - - - 92.69±0.89 93.97±0.82 - -
Mousai 7.5 1.59 0 .23 73.97±1.93 74.12±1.43 200 44.0
MusicLM 4.0 - - 84.03±1.28 85.57±1.12 - -
AudioLDM 2 3.1 1.20 0 .31 77.69±1.93 82.41±1.36 208 18.1
MUSIC GEN-small 3.1 1.29 0 .31 84.68±1.45 83.89±1.01 1500 17.6
MUSIC GEN-large 3.4 1.23 0 .32 85.65±1.51 84.12±1.12 1500 41.3
MAGN ET-small 3.3 1.12 0 .31 81.67±1.72 83.21±1.17 180 4.0
MAGN ET-large 4.0 1.15 0 .29 84.26±1.43 84.21±1.34 180 12.6
We train non-autoregressive transformer models using 300M (MAGN ET-small) and 1.5B
(MAGN ET-large) parameters. We train models using 30-second audio crops sampled at random
from the full track. We train the models for 1M steps with the AdamW optimizer (Loshchilov &
Hutter, 2017), a batch size of 192examples, β1= 0.9,β2= 0.95, a decoupled weight decay of 0.1
and gradient clipping of 1.0. We further rely on D-Adaptation-based automatic step-sizes (Defazio
& Mishchenko, 2023). We use a cosine learning rate schedule with a warmup of 4K steps. Addi-
tionally, we use an exponential moving average with a decay of 0.99. We train the models using
respectively 32GPUs for small and 64GPUs for large models, with float16 precision. For compu-
tational efficiency, we train 10-second generation models with a batch size of 576examples for all
ablation studies. Finally, for inference, we employ nucleus sampling (Holtzman et al., 2020) with
top-p 0.9, and a temperature of 3.0that is linearly annealed to zero during decoding iterations. We
use CFG with a condition dropout of 0.3at training, and a guidance coefficient 10.0annealed to 1.0.
Evaluation metrics. We evaluate the proposed method using the same setup as proposed by Copet
et al. (2023); Kreuk et al. (2022a), which consists of both objective and subjective metrics. For the
objective metrics, we use: the Fr ´echet Audio Distance (FAD), the Kullback-Leiber Divergence (KL),
and the CLAP score (CLAP). We report the FAD (Kilgour et al., 2018) using the official implemen-
tation in Tensorflow with the VGGish model2. Following Kreuk et al. (2022a), we use a state-of-
the-art audio classifier (Koutini et al., 2021) to compute the KL-divergence over the probabilities
of the labels between the original and the generated audio. We also report the CLAP score (Wu
et al., 2023; Huang et al., 2023b) between the track description and the generated audio to quantify
audio-text alignment, using the official CLAP model3.
For the human studies, we follow the same setup as in Kreuk et al. (2022a). We ask human raters
to evaluate two aspects of the audio samples (i) overall quality (O VL), and (ii) relevance to the text
input (R EL). For the overall quality test, raters were asked to rate the perceptual quality of the pro-
vided samples in a range of 1to100. For the text relevance test, raters were asked to rate the match
between audio and text on a scale of 1to100. Raters were recruited using the Amazon Mechanical
Turk platform. We evaluate randomly sampled files, where each sample was evaluated by at least
5raters. We use the CrowdMOS package4to filter noisy annotations and outliers. We remove
annotators who did not listen to the full recordings, annotators who rate the reference recordings
less than 85, and the rest of the recommended recipes from CrowdMOS (Ribeiro et al., 2011).
5 R ESULTS
5.1 T EXT-TO-MUSIC GENERATION
We compare MAGN ET to Mousai (Schneider et al., 2023), MusicGen Copet et al. (2023), Audi-
oLDM2 Liu et al. (2023b)5, and MusicLM (Agostinelli et al., 2023). We train Mousai using our
dataset using the open source implementation provided by the authors6.
2github.com/google-research/google-research/tree/master/frechet audio distance
3https://github.com/LAION-AI/CLAP
4http://www.crowdmos.org/download/
5huggingface.co/spaces/haoheliu/audioldm2-text2audio-text2music (September 2023)
6Implementation from github.com/archinetai/audio-diffusion-pytorch (March 2023)
6Published as a conference paper at ICLR 2024
2 4 8 16 32 64 128 256
Batch size010203040506070Latency (s)
6.18
0.624.9246.11Autoregressive
Non-autoregressive
(a) Latency
2 4 8 16 32 64 128 256
Batch size012345Throughput (samples/s)5.14
2.78Autoregressive
Non-autoregressive (b) Throughput
2 4 6 810 12 14 16 18
Latency (s)0.600.650.700.750.800.850.900.951.00FAD0
1
2
3
45AR prompt
AR (c) Latency/FAD trade-off
Figure 2: Latency and throughput analysis: MAGN ET is particularly suited to small batch sizes (up
to 10 times lower latency than M USIC GEN), while M USIC GENbenefits from a higher throughput for
bigger batch sizes. MAGN ET offers flexibility regarding the latency/quality trade off by allowing a
customizable decoding schedule or following the Hybrid-MAGN ET variant.
Table 1 presents the results of MAGN ET on the task of text-to-music generation compared to var-
ious baselines. Results are reported on the MusicCaps benchmark. As can be seen, MAGN ET
reaches comparable performance to MusicGen, which performs autoregressive modeling, while be-
ing significantly faster both in terms of latency and decoding steps. When comparing to Audi-
oLDM2, which is based on latent diffusion, MAGN ET gets worse FAD and CLAP scores, while
reaching better KL subjective scores. Notice, AudioLDM2 was trained on 10-seconds generation
at16kHz while MAGN ET was trained on 30-seconds generation at 32kHz. When we reduce the
sequence length to 10-second generations our FAD reaches to 2.9and CLAP score of 0.31. We
additionally evaluate MAGN ET on the task of text-to-audio generation (environmental sound gen-
eration). Results and details regarding the baselines can be found in Appendix G. Results show
similar trends as on text-to-music of MAGN ET providing comparable performance to the autore-
gressive baseline while being significantly faster.
5.2 A NALYSIS
Latency vs. Throughput. We analyze the trade-offs between latency and throughput as a function
of the batch size, as illustrated in Fig. 2a and Fig. 2b. Latency and throughput are reported for
generated samples of 10-second duration on an A100 GPU with 40GB of RAM. Due to CFG,
the batch size value is typically twice the generated sample count. Indeed the model outputs two
distributions in parallel: one conditioned on the text prompt and the other unconditioned.
Compared with the baseline autoregressive model (red curve), the non-autoregressive model (dashed
blue curve) especially excels on small batch sizes due to parallel decoding, with a latency as low
as600ms for a single generated sample (batch size of two in the 2a), more than 10times faster
than the autoregressive baseline. This is especially interesting in interactive applications that require
low-latency. The non-autoregressive model is faster than the baseline up to a batch size of 64.
However, in scenarios where throughput is a priority (e.g. generate as many samples as possible,
irrespective of the latency), we show that the autoregressive model is favorable. While the non-
autoregressive model throughput is bounded to ∼2.8samples/second for batch sizes bigger than
64, the autoregressive model throughput is linear in batch size, only limited by the GPU memory.
Hybrid-MAGN ET.Next, we demonstrate how a hybrid version can also be combined. We boot-
strap the non-autoregressive generation with an autoregressive-generated audio prompt. We train
a single model that incorporates both decoding strategies. During training, we sample a time step
t∈ {1, . . . , T }and compute the autoregressive training loss for all positions that precede t. The
rest of the sequence is being optimized using the MAGN ET objective. This is done by designing a
custom attention mask that simulates the inference behavior during training (causal attention before
t, parallel attention after t). During inference, the model can be used autoregressively to generate a
short audio prompt and switch to non-autoregressive decoding to complete the generation faster. A
detailed description of the hybrid training can be found on Appendix E.
We analyze the effect of the chosen tin Fig. 2c using a 30-second generations without rescor-
ing. Starting from a fully non-autoregressive generation decoding, we ablate on the autoregressive-
generated prompt duration. The results indicate that the longer the prompt, the lower the FAD.
The Hybrid-MAGN ET is even able to outperform the full autoregressive baseline (when consid-
7Published as a conference paper at ICLR 2024
Table 2: Span length and restricted context ablation. We report FAD scores for MAGN ET using an
In-domain test set considering different span lengths, with and without temporally restricted context.
Span-length 1 2 3 4 5 10
Restricted context ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✗ ✓
FAD 3.07 2 .130.74 0 .660.82 0 .610.97 0 .631.13 0 .841.66 1 .05
Table 3: We evaluate the effect of the rescorer on model performance. We report mean and CI95.
MODEL RE-SCORE FADvgg↓KL↓CLAP scr↑ OVL.↑ REL.↑ LATENCY (S)
MAGN ET-small ✗ 3.7 1.18 0 .30 80.65±1.48 81.06±1.19 2.5
MAGN ET-small ✓ 3.3 1.12 0 .31 83.31±1.11 84.53±1.11 4.0
MAGN ET-large ✗ 4.2 1.19 0 .30 82.19±1.23 82.96±0.91 8.2
MAGN ET-large ✓ 4.0 1.15 0 .29 84.43±1.24 85.57±1.04 12.6
ering FAD), starting from a 1-second prompt while still being significantly faster ( 3.2s of latency
down from 17.6s). This Hybrid strategy is another way to control quality/latency trade-offs when
performance of the non-autoregressive model does not quite match its autoregressive counterpart.
5.3 A BLATION
The effect of modeling choices. To validate our findings regarding the necessity of span masking
for audio modeling, as well as the necessity of temporal context restriction for efficient optimization,
we train different model configurations and report the resulting FAD in Table 2. Results suggest that
using restricted context consistently improves model performance across all settings. Moreover,
using a span-length of 3, which corresponds to spans of 60ms yields the best performance.
The effect of CFG annealing. Table 6 in the Appendix presents results computed over in-domain
samples using several CFG coefficient configurations. We evaluate both constant CFG schedules,
e.g. by setting λ0=λ1= 3, and annealing CFG. Results suggest that using λ0= 10 ,λ1= 1yields
the best FAD score over all evaluated setups. This finding aligns with our hypothesis that during
the first decoding steps a stronger text adherence is required, while at later decoding steps we would
like the model to focus on previously decoded tokens.
The effect model rescorer. Next, we evaluate the effect of model rescorering on the overall perfor-
mance. Results are presented in Table 3. Results suggest that applying model rescoring improves
performance for almost all metrics. However, this comes at the expense of slower inference time.
The effect of decoding steps. We explore the effect of less decoding steps on the overall latency
and performance, see Fig. 7. It seems that reducing the decoding steps for higher levels does not
impact quality as much as for the first level. For scenarios where minimizing the latency is the top
priority, one should consider only 1step per higher codebook level: in such case, latency drops to
370ms, at the expense of a 8% increase of FAD compared to 10steps per higher levels.
Decoding visualization. We visualize the masking dynamics of MAGN ET’s iterative decoding
process. In specific, we plot the mask m(i)chosen by MAGN ET during the generation of a
10-second audio sample, for each iteration i∈ {1, . . . , 20}. As can be seen, MAGN ET decodes
the audio sequence in a non-causal manner, choosing first a sparse set of token spans at various
disconnected temporal locations, and gradually “inpaint” the gaps until convergence to a full token
sequence. Visualization and full details can be found in Appendix F.
6 R ELATED WORK
Autoregressive audio generation. Recent studies considering text-to-audio generation can be
roughly divided into two: (i) environmental sounds generation; and (ii) music generation. As for
environmental sound generation, Kreuk et al. (2022a) proposed applying a transformer language
model over discrete audio representation, obtained by quantizing directly time-domain signals using
EnCodec D ´efossez et al. (2022). Sheffer & Adi (2023) followed a similar approach to Kreuk et al.
(2022a) for image-to-audio generation. Dhariwal et al. (2020) proposed representing music samples
8Published as a conference paper at ICLR 2024
in multiple streams of discrete representations using a hierarchical VQ-V AE. Next, two sparse trans-
formers applied over the sequences to generate music. Gan et al. (2020) proposed generating music
for a given video, while predicting its midi notes. Recently, Agostinelli et al. (2023) proposed a
similar approach to AudioLM (Borsos et al., 2023a), which represents music using multiple streams
of “semantic tokens” and “acoustic tokens”. Then, they applied a cascade of transformer decoders
conditioned on a textual-music joint representation (Huang et al., 2022). Donahue et al. (2023) fol-
lowed a similar modeling approach, but for the task of singing-to-accompaniment generation. Copet
et al. (2023) proposed a single stage Transformer-based autoregressive model for music generation,
conditioned on either text or melodic features, based on EnCodec .
Non-autoregressive audio generation. The most common approach for non-autoregressive gener-
ation is diffusion models. These models naturally apply over continuous representations however
can also operate over discrete representations. Yang et al. (2022) proposed representing audio spec-
trograms using a VQ-V AE, then applying a discrete diffusion model conditioned on textual CLIP
embeddings for the generation part Radford et al. (2021). Huang et al. (2023b); Liu et al. (2023a;b)
proposed using latent diffusion models for the task of text-to-audio, while extending it to various
other tasks such as inpainting, image-to-audio, etc. Schneider et al. (2023); Huang et al. (2023a);
Maina (2023); Forsgren & Martiros (2022); Liu et al. (2023b) proposed using a latent diffusion
model for the task of text-to-music. Schneider et al. (2023) proposed using diffusion models for
both audio encoder-decoder and latent generation. Huang et al. (2023a) proposed a cascade of diffu-
sion model to generate audio and gradually increase its sampling rate. Forsgren & Martiros (2022)
proposed fine-tuning Stable Diffusion (Rombach et al., 2022) using spectrograms to generate five-
second segments, then, using image-to-image mapping and latent interpolation to generate long
sequences. Li et al. (2023) present impressive generation results using a latent diffusion model with
a multi-task training objective, however for 10-second generation only.
The most relevant prior work to ours involves masked generative modeling. Ghazvininejad et al.
(2019) first proposed the Mask-Predict method, a masked language modeling with parallel decoding
for the task of machine translation. Later on, Chang et al. (2022) followed a similar modeling strat-
egy, denoted as MaskGIT, for the task of class-conditioned image synthesis and image editing, while
Chang et al. (2023) extended this approach to high-quality textually guided image generation over
low-resolution images followed by a super-resolution module. Lezama et al. (2022) further proposed
the TokenCritic approach, which improves the sampling from the joint distribution of visual tokens
over MaskGIT. Recently, Borsos et al. (2023b) proposed the SoundStorm model, which has a similar
modeling strategy as MaskGIT but for text-to-speech and dialogue synthesis. Unlike MaskGIT, the
SoundStorm model is conditioned on semantic tokens obtained from an autoregressive model. The
proposed work differs from this model as we propose a single non-autoregressive model, with a
novel audio-tokens modeling approach for the task of text-to-audio. Another concurrent work, is
VampNet (Garcia et al., 2023), a non-autoregressive music generation model. Unlike MAGN ET,
VampNet is based on two different models (one to model the “coarse” tokens and one to model the
“fine” tokens), and do not explore text-to-music generation without audio-prompting.
7 D ISCUSSION
Limitations. As discussed in section 5.2, the proposed non-autoregressive architecture targets low-
latency scenarios. By design, the model re-encodes the whole sequence at each decoding step, even
for time steps that have not changed between two consecutive decoding steps. This is a fundamental
difference with autoregressive architectures that can benefit from caching past keys and values and
only encode one-time step per decoding step, which efficiently scales when the batch size increases.
Such a caching strategy could also be adopted for non-autoregressive architectures, for time steps
that do not change between consecutive decoding steps, however, this requires further research.
Conclusion. In this work, we presented MAGN ET which, to the best of our knowledge, is the first
pure non-autoregressive method for text-conditioned audio generation. By using a single-stage en-
coder during training and a rescorer model, we achieve competitive performance with autoregressive
methods while being approximately 7times faster. We also explore a hybrid approach that combines
autoregressive and non-autoregressive models. Our extensive evaluation, including objective metrics
and human studies, highlights MAGN ET’s promise for real-time audio generation with comparable
or minor quality degradation. For future work, we intend to extend the research work on the model
rescoring and advanced inference methods. We believe this research direction holds great potential
in incorporating external scoring models which will allow better non-left-to-right model decoding.
9Published as a conference paper at ICLR 2024
ACKNOWLEDGEMENTS .
The authors would like to thank Or Tal, Michael Hassid and Nitay Arcusin for the useful theoret-
ical discussions. The authors would additionally like to thank Kamila Benzina and Nisha Deo for
supporting this project. This research work was supported in part by ISF grant 2049/22.
REFERENCES
Andrea Agostinelli, Timo I Denk, Zal ´an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,
Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating
music from text. arXiv preprint arXiv:2301.11325 , 2023.
J Benesty, J Chen, and Y Huang. Automatic speech recognition: A deep learning approach, 2008.
Zal´an Borsos, Rapha ¨el Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Shar-
ifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a
language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech,
and Language Processing , 2023a.
Zal´an Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco
Tagliasacchi. Soundstorm: Efficient parallel audio generation. arXiv preprint arXiv:2305.09636 ,
2023b.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative
image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022.
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan
Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-
eration via masked generative transformers. arXiv preprint arXiv:2301.00704 , 2023.
Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexan-
dre D ´efossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284 , 2023.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and
memory-efficient exact attention with IO-awareness. In Advances in Neural Information Process-
ing Systems , 2022.
Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. arXiv
preprint arXiv:2301.07733 , 2023.
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.
Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341 , 2020.
Chris Donahue, Antoine Caillon, Adam Roberts, Ethan Manilow, Philippe Esling, Andrea
Agostinelli, Mauro Verzetti, Ian Simon, Olivier Pietquin, Neil Zeghidour, et al. Singsong: Gen-
erating musical accompaniments from singing. arXiv preprint arXiv:2301.12662 , 2023.
Alexandre D ´efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio
compression. arXiv preprint arXiv:2210.13438 , 2022.
S Forsgren and H Martiros. Riffusion-stable diffusion for real-time music generation. 2022. URL
https://riffusion. com/about , 2022.
Chuang Gan, Deng Huang, Peihao Chen, Joshua B Tenenbaum, and Antonio Torralba. Foley mu-
sic: Learning to generate music from videos. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16 . Springer, 2020.
Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo. Vampnet: Music gener-
ation via masked acoustic token modeling. arXiv preprint arXiv:2307.04686 , 2023.
Itai Gat, Felix Kreuk, Tu Anh Nguyen, Ann Lee, Jade Copet, Gabriel Synnaeve, Emmanuel Dupoux,
and Yossi Adi. Augmentation invariant discrete representation for generative spoken language
modeling. In IWSLT , 2023.
10Published as a conference paper at ICLR 2024
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel
decoding of conditional masked language models. arXiv preprint arXiv:1904.09324 , 2019.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 , 2022.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration, 2020.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,
and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked
prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
2021.
Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel PW Ellis. Mu-
lan: A joint embedding of music audio and natural language. arXiv preprint arXiv:2208.12415 ,
2022.
Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong
Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: Text-conditioned music
generation with diffusion models. arXiv preprint arXiv:2302.03917 , 2023a.
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin
Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced
diffusion models. arXiv preprint arXiv:2301.12661 , 2023b.
Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh
Nguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, et al. Text-free prosody-
aware generative spoken language modeling. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) , 2022.
Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fr ´echet audio distance:
A metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466 , 2018.
Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating
captions for audios in the wild. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers) , 2019.
Khaled Koutini, Jan Schl ¨uter, Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio
transformers with patchout. arXiv preprint arXiv:2110.05069 , 2021.
Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D ´efossez, Jade Copet, Devi
Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv
preprint arXiv:2209.15352 , 2022a.
Felix Kreuk, Yaniv Taigman, Adam Polyak, Jade Copet, Gabriel Synnaeve, Alexandre D ´efossez, and
Yossi Adi. Audio language modeling using perceptually-guided discrete representations. arXiv
preprint arXiv:2211.01223 , 2022b.
Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte,
Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative
spoken language modeling from raw audio. Transactions of the Association for Computational
Linguistics , 9, 2021.
Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo
Ma, Xuchen Song, et al. Efficient neural music generation. arXiv preprint arXiv:2305.15719 ,
2023.
Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: A universal
neural vocoder with large-scale training. arXiv preprint arXiv:2206.04658 , 2022.
11Published as a conference paper at ICLR 2024
Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean
Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.
xformers: A modular and hackable transformer modelling library. https://github.com/
facebookresearch/xformers , 2022.
Jos´e Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with
token-critic. In European Conference on Computer Vision . Springer, 2022.
Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex Wang. Jen-1: Text-guided uni-
versal music generation with omnidirectional diffusion models. arXiv preprint arXiv:2308.04729 ,
2023.
Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello, Jacob Kahn, Gilad Avidov,
Ronan Collobert, and Gabriel Synnaeve. Rethinking evaluation in asr: Are our models robust
enough? arXiv preprint arXiv:2010.11745 , 2020.
Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and
Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv
preprint arXiv:2301.12503 , 2023a.
Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu
Wang, Yuxuan Wang, and Mark D Plumbley. Audioldm 2: Learning holistic audio generation
with self-supervised pretraining. arXiv preprint arXiv:2308.05734 , 2023b.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 , 2017.
Gallil Maimon and Yossi Adi. Speaking style conversion with discrete self-supervised units. arXiv
preprint arXiv:2212.09730 , 2022.
Kinyugo Maina. Msanii: High fidelity music synthesis on a shoestring budget. arXiv preprint
arXiv:2301.06468 , 2023.
Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-
delrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled
self-supervised representations. arXiv preprint arXiv:2104.00355 , 2021.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning .
PMLR, 2021.
Jack W Rae and Ali Razavi. Do transformers need deep long-range memory. arXiv preprint
arXiv:2007.03356 , 2020.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research , 2020.
Fl´avio Ribeiro, Dinei Flor ˆencio, Cha Zhang, and Michael Seltzer. Crowdmos: An approach for
crowdsourcing mean opinion score studies. In IEEE international conference on acoustics, speech
and signal processing (ICASSP) . IEEE, 2011.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2022.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguis-
tics, 2021.
Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, et al. Code llama: Open foundation models for code.
arXiv preprint arXiv:2308.12950 , 2023.
12Published as a conference paper at ICLR 2024
Aditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis, Hooman Shayani, Amir H Khasahmadi, Srinath
Sridhar, and Daniel Ritchie. Clip-sculptor: Zero-shot generation of high-fidelity and diverse
shapes from natural language. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 2023.
Flavio Schneider, Zhijing Jin, and Bernhard Sch ¨olkopf. Mo \ˆ usai: Text-to-music generation with
long-context latent diffusion. arXiv preprint arXiv:2301.11757 , 2023.
Roy Sheffer and Yossi Adi. I hear your true colors: Image guided audio generation. In ICASSP 2023-
2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .
IEEE, 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing
Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech
synthesizers. arXiv preprint arXiv:2301.02111 , 2023.
Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui*, Taylor Berg-Kirkpatrick, and Shlomo Dubnov.
Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption
augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing,
ICASSP , 2023.
Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diff-
sound: Discrete diffusion model for text-to-sound generation. arXiv preprint arXiv:2207.09983 ,
2022.
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-
stream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and
Language Processing , 2021.
Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing
Liu, Huaming Wang, Jinyu Li, et al. Speak foreign languages with your own voice: Cross-lingual
neural codec language modeling. arXiv preprint arXiv:2303.03926 , 2023.
13Published as a conference paper at ICLR 2024
Table 4: Details about the training sets used to train the proposed method and the evaluated baselines.
METHOD NO.OFHOURS SAMPLING RATE DETAILS
MusicGen 20,000 32kHz ShutterStock, Pond5, and proprietary data
MusicLM 280,000 24 kHz Proprietary data
Mousai 2,500 48kHz ShutterStock, Pond5, and proprietary data
‘AudioLDM2 29,510 16kHzAudioSet, WavCaps, AudioCaps, VGGSound, Free
Music Archive, Million Song Dataset,
LJSpeech, and GigaSpeech
MAGN ET 20,000 32kHz ShutterStock, Pond5, and proprietary data
A E XPERIMENTAL SETUP
A.1 I MPLEMENTATION DETAILS
Under all setups, we use the official EnCodec model as was published by Copet et al. (2023)7. The
model gets as input an audio segment and outputs a 50Hz discrete representation. We use four
codebooks where each has a codebook size of 2048 . We perform the same text preprocessing as
proposed by Copet et al. (2023); Kreuk et al. (2022a).
We train non-autoregressive transformer models using 300M (MAGN ET-small) and 1.5B
(MAGN ET-large) parameters. We use a memory efficient Flash attention (Dao et al., 2022) from
the xFormers package (Lefaudeux et al., 2022) to improve both speed and memory usage. We train
models using 30-second audio crops sampled at random from the full track. We train the models for
1M steps with the AdamW optimizer (Loshchilov & Hutter, 2017), a batch size of 192examples,
β1= 0.9,β2= 0.95, a decoupled weight decay of 0.1and gradient clipping of 1.0. We further
rely on D-Adaptation based automatic step-sizes (Defazio & Mishchenko, 2023). We use a cosine
learning rate schedule with a warmup of 4K steps. Additionally, we use an exponential moving
average with a decay of 0.99. We train the models using respectively 32GPUs for the small model
and64GPUs for the large ones with float16 precision.
Finally, for inference, we employ nucleus sampling (Holtzman et al., 2020) with top-p 0.9, and a
temperature of 3.0that is linearly annealed to zero during decoding iterations. We use CFG with
condition dropout rate of 0.3during training, and a guidance coefficient 10.0that is annealed to 1.0
during iterative decoding.
A.2 D ATASETS
We follow the same setup as in Copet et al. (2023) and use 20K hours of licensed music to train
MAGN ET. Specifically, we rely on the same 10K high-quality music tracks, the ShutterStock,
and Pond5 music data collections as used in Copet et al. (2023)8with respectively 25K and 365K
instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with
metadata composed of a textual description and additional information such as the genre, BPM, and
tags.
For the main results and comparison with prior work, we evaluate the proposed method on the
MusicCaps benchmark (Agostinelli et al., 2023). MusicCaps is composed of 5.5K samples (ten-
second long) prepared by expert musicians and a 1K subset balanced across genres. We report
objective metrics on the unbalanced set, while we sample examples from the genre-balanced set for
qualitative evaluations. We additionally evaluate the proposed method using the same in-domain
test set as proposed by Copet et al. (2023). All ablation studies were conducted on the in-domain
test set.
7https://github.com/facebookresearch/audiocraft
8www.shutterstock.com/music and www.pond5.com
14Published as a conference paper at ICLR 2024
A.3 E VALUATION
Baselines. For music generation we compare MAGN ET Mousai (Schneider et al., 2023), Music-
Gen Copet et al. (2023), AudioLDM2 Liu et al. (2023b), and MusicLM (Agostinelli et al., 2023).
For Mousai, we train a model using our dataset for a fair comparison, using the open source imple-
mentation provided by the authors9.
Evaluation metrics. We evaluate the proposed method using the same setup as proposed in Copet
et al. (2023); Kreuk et al. (2022b), which consists of both objective and subjective metrics. For the
objective methods, we use three metrics: the Fr ´echet Audio Distance (FAD), the Kullback-Leiber
Divergence (KL) and the CLAP score (CLAP). We report the FAD (Kilgour et al., 2018) using the
official implementation in Tensorflow with the VGGish model10. A low FAD score indicates the
generated audio is plausible. Following Kreuk et al. (2022a), we use a state-of-the-art audio classifier
trained for classification on AudioSet (Koutini et al., 2021) to compute the KL-divergence over the
probabilities of the labels between the original and the generated audio. For the music generation
experiments only we additionally report the CLAP score (Wu et al., 2023; Huang et al., 2023b)
between the track description and the generated audio to quantify audio-text alignment, using the
official pretrained CLAP model11.
For the human studies, we follow the same setup as in Kreuk et al. (2022a). We ask human raters
to evaluate two aspects of the audio samples (i) overall quality (O VL), and (ii) relevance to the
text input (R EL). For the overall quality test, raters were asked to rate the perceptual quality of
the provided samples in a range of 1to100. For the text relevance test, raters were asked to rate
the match between audio and text on a scale of 1to100. Raters were recruited using the Amazon
Mechanical Turk platform. We evaluate randomly sampled files, where each sample was evaluated
by at least 5raters. We use the CrowdMOS package12to filter noisy annotations and outliers.
We remove annotators who did not listen to the full recordings, annotators who rate the reference
recordings less than 85, and the rest of the recommended recipes from CrowdMOS (Ribeiro et al.,
2011). For fairness, we include the same normalization scheme as proposed in Copet et al. (2023)
of normalizing all samples at −14dB LUFS.
B R ECEPTIVE FIELD ANALYSIS
We present the receptive field analysis of the EnCodec model in Fig. 3. We slide an impulse function,
in the form of a one-hot input vector, and measure the norm of the encoded latent vector in the middle
of the sequence, as function of the temporal distance from the impulse. We perform the process
twice: (i) For the full encoder (left) and (ii) while omitting the LSTM block and remaining only
with the convolutional network (right). Fig. 3 shows that the effective receptive field of EnCodec is
upper bounded by 100ms in each direction, supporting our choice to design MAGN ET’s restricted
transformer s.t. codebooks greater than one attend only tokens in a neighborhood of 100ms in each
direction.
C S PAN MASKING
Sampling a placement of utoken spans can be implemented by first sampling a subset of uindices
from{1, . . . , T }, serving as the span starts, and then extending each index to a span. Formally, we
sample I(u)∼ U({A ⊆ { 1, . . . , T }:|A|=u}), and then extend each index t∈I(u)to the span of
indices t, . . . , t +l−1. The total set of masked indices would be
Mspans(I(u);l)≜[
t∈I(u){t, . . . , t +l−1}. (7)
9Implementation from github.com/archinetai/audio-diffusion-pytorch (March 2023)
10github.com/google-research/google-research/tree/master/frechet audio distance
11https://github.com/LAION-AI/CLAP
12http://www.crowdmos.org/download/
15Published as a conference paper at ICLR 2024
200
 150
 100
 50
 0 50 100 150 200
Impulse dt (ms)3040506070zmid
(a) EnCodec’s middle latent vector’s impulse re-
sponse.
200
 150
 100
 50
 0 50 100 150 200
Impulse dt (ms)3040506070zmid
(b) The impulse response of the same vector when
omitting the LSTM block from the encoder.
Figure 3: A visualization of the receptive field analysis.
Proposition C.1. Given a random placement of uspans of size lover a sequence of length T, the
expected masking rate is as follows,
EI(u)∼U({A⊆{ 1,...,T}:|A|=u})1
TMspans
I(u);l
= 1− T−l
u
 T
u. (8)
Derivation: First, note that for a given token zt, the probability that ztwould remain unmasked, is
the probability to choose uspan starts only from the indices:
At≜{1, . . . , T } \ {t−l+ 1, . . . , t}. (9)
The total number of placements is T
u
, i.e., the number of possibilities to choose uspan starts out
of a set of Tindices without replacement. Similarly, the total amount of placements for which all
span starts are in At, is T−l
u
. Thus,
Ph
t∈ Mspans(I(u);l)i
= 1− T−l
u
 T
u. (10)
Consequently, the masking probability for each token is 1− T−l
u
/ T
u
.Finally, we define the
indicator random variable 1t∈Mspans(I(u);l)for each t∈ {1. . . T}, and conclude the derivation by
EI(u)hMspans
I(u);li
=EI(u)"TX
t=11t∈Mspans(I(u);l)#
(11)
=TX
t=1EI(u)
1t∈Mspans(I(u);l)
(12)
=T· 
1− T−l
u
 T
u!
. (13)
D M ODEL INFERENCE
Fig. 4 presents the inference process of MAGN ET. For clarity, we omit CFG and nucleus sampling,
and assume Tis a multiple of the span length l. To further ease the reading, we present the inference
algorithm for a single codebook, while in practice, we run Fig. 4 for every codebook k∈ {1. . . K}.
16Published as a conference paper at ICLR 2024
def MAGNeT_generate(B: int, T: int, text: List, s: int, model: nn.Module,
rescorer: nn.Module, mask_id: int, tempr: float, w: float):
# Start from a fully masked sequence
gen_seq = torch.full((B, T), mask_id, dtype=torch.long)
n_spans = T // span_len
spans_shape = (B, n_spans)
span_scores = torch.zeros(spans_shape, dtype=torch.float32)
# Run MAGNeT iterative decoding for 's' iterations
for iinrange(s):
mask_p = torch.cos((math.pi *i) / (2 *s))
n_masked_spans = max(int(mask_p *n_spans), 1)
# Masking
masked_spans = span_scores.topk(n_masked_spans, dim=-1).indices
mask = get_mask(spans_shape, masked_spans)
gen_seq[mask] = mask_id
# Forward pass
logits, probs = model.compute_predictions(gen_sequence, text, cfg= True , temperature=tempr)
# Classifier free guidance with annealing
cfg_logits = cfg(mask_p, logits, annealing= True )
# Sampling
sampled_tokens = sample_top_p(probs, p=top_p)
# Place the sampled tokens in the masked positions
mask = gen_seq == mask_id
gen_seq = place_sampled_tokens(mask, sampled_tokens[..., 0], gen_seq)
# Probs of sampled tokens
sampled_probs = get_sampled_probs(probs, sampled_tokens)
ifrescorer:
# Rescoring
rescorer_logits, rescorer_probs = rescorer.compute_predictions(gen_seq, text)
rescorer_sampled_probs = get_sampled_probs(rescorer_probs, sampled_tokens)
# Final probs are the convex combination of probs and rescorer_probs
sampled_probs = w *rescorer_sampled_probs + (1 - w) *sampled_probs
# Span scoring - max
span_scores = get_spans_scores(sampled_probs)
# Prevent remasking by placing -inf scores for unmasked
span_scores = span_scores.masked_fill(˜spans_mask, -1e5)
return gen_seq
Figure 4: MAGN ET’s text-to-audio inference. MAGN ET performs iterative decoding of ssteps.
In each step, the least probable non-overlapping spans are being masked, where the probability is a
convex combination of the restricted-transformer confidence and the probability obtained by the pre-
trained rescorer. Finally, the span probabilities are re-updated, while assigning ∞to the unmasked
spans, to prevent its re-masking and fix it as anchors for future iterations.
E H YBRID -MAGN ETTRAINING
The aim of Hybrid-MAGN ET is to switch from autoregressive generation to non-autoregressive
during inference, so as to generate an audio prompt with the same quality as M USIC GENthat can be
completed fast using MAGN ET inference. The goal is to find a compromise between M USIC GEN
quality and MAGN ET speed. To give Hybrid-MAGN ET the ability to switch between decoding
strategies, it requires a few adaptations from MAGN ET training recipe. One of them is to train
jointly on two different objectives as illustrated by Figure 5. Similarly to Borsos et al. (2023b) a
time step tis uniformly sampled from {1, . . . , T }that simulates an audio prompt for MAGN ET to
complete from. For all positions that precede tand for all codebook levels we propose to compute
the autoregressive training objective, using causal attention masking. For all succeeding positions
we keep the MAGN ET training objective: the model can attend to tokens from the audio prompt.
Moreover the codebook pattern is adapted for the autoregressive generation to work well, to that
17Published as a conference paper at ICLR 2024
222
76892
20249 591
993 129 323 4221009 432 439
32 55 53 23 31921
13993 2612 429 831
82133
37
992
83AR loss
NAR loss
AR-generated prompt NAR completion
Figure 5: Training of Hybrid-MAGN ET. During training a random timestep tis sampled. For
timesteps preceding ta causal attention mask is applied and cross entropy loss is computed for
all levels (blue highlighted squares). For timesteps succeeding tthe standard MAGN ET training
strategy is applied. Codebook levels are shifted following the delay pattern from Copet et al. (2023).
1 50 100 150 200 250 300 350 400 450
T emporal Timestep1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20Decoding Timestep
1 50 100 150 200 250 300 350 400 450
T emporal Timestep1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20Decoding Timestep
1 50 100 150 200 250 300 350 400 450
T emporal Timestep1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20Decoding Timestep
1 50 100 150 200 250 300 350 400 450
T emporal Timestep1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20Decoding Timestep
Figure 6: Decoding visualization of the chosen anchor tokens as a function of decoding steps, for
an iterative decoding process with s= 20 . We plot the mask m(i)chosen by MAGN ET, for each
i∈ {1, . . . , s }, during the generation of a 10-second audio sample for the text prompt ’A dynamic
blend of hip-hop and orchestral elements, with sweeping strings and brass’. The x-axis represents
time while the y-axis represents the decoding steps.
end we use the delay pattern from Copet et al. (2023). Thus the temporally restricted context from
MAGN ET is adapted to take into account the codebook level-dependent shifts.
18Published as a conference paper at ICLR 2024
Table 5: Text-to-Audio generation results. We report FAD and KL scores for all methods.
PARAMETERS TEXT CONDITIONING FAD↓KL↓
DiffSound 400M CLIP 7.39 2.57
AudioGen-base 285M T5-base 3.13 2.09
AudioGen-large 1500M T5-large 1.77 1.58
AudioLDM2-small 346M T5-large, CLAP, ImageBind, PE 1.67 1.01
AudioLDM2-large 712M T5-large, CLAP, ImageBind, PE 1.42 0.98
Make-an-Audio 332M CLAP 4.61 2.79
MAGN ET-small 300M T5-large 3.22 1 .42
MAGN ET-large 1500M T5-large 2.36 1 .64
0.4 0.6 0.8 1.0 1.2 1.4
Latency (s)0.60.70.80.91.01.11.2FAD5 steps
10 steps
20 steps
50 steps100 steps1 step
5 steps
10 steps20 stepsFirst codebook
Codebook >1
Figure 7: Effect of the decoding schedules on the quality/latency trade off. We vary the number of
decoding steps for the first codebook level (dashed red curve) and the higher codebook levels (dotted
blue curve) around a (20,10,10,10)decoding schedule.
F I TERATIVE DECODING DYNAMICS
Fig. 6 presents the masking dynamics of MAGN ET’s iterative decoding process with s= 20 . In
specific, we plot the mask m(i)chosen by MAGN ET, for each i∈ {1. . . s}, during the genera-
tion of a 10-second audio sample for the text prompt ’A dynamic blend of hip-hop and orchestral
elements, with sweeping strings and brass’ . To demonstrate MAGN ET’s stochasticity, we repeat
the process several times. As can be seen, MAGN ET decodes the audio sequence in a non-causal
manner, choosing first a sparse set of token-spans at various disconnected temporal locations, and
gradually “inpaint” the gaps until convergence to a full token sequence.
G A DDITIONAL RESULTS
Text-to-audio generation We follow Kreuk et al. (2022a) and use the exact same training sets
to optimize MAGN ET. We train MAGN ET in two model scales, of 300M and 1.5B parameters
respectively, and compare it to AudioGen (Kreuk et al., 2022a), DiffSound (Yang et al., 2022),
AudioLDM2 (Liu et al., 2023b), and Make-an-Audio Huang et al. (2023b). Results are reported
in Table 5. Results are reported on the AudioCaps testset (Kim et al., 2019). All audio files were
sampled at 16kHz. As can be see MAGN ET results are comparable or slightly worse than the
autoregressive alternative (AudioGen) while having significantly lower latency (the latency values
are the same as in Table 1 for MAGNeT, while AudioGen has the same latency as MusicGen). For
inference, different than the MAGN ET models trained for music generation, we use top-p 0.8, an
initial temperature of 3.5, and an initial CFG guidance coefficient of 20.0.
The effect of decoding steps. The latency of the non-autoregressive model can be controlled by
configuring the appropriate decoding steps, at the expense of quality degradation. In Fig. 7, we
report the in-domain FAD as a function of latency for different decoding steps. We ablate on the
first codebook level step count (dashed red curve) and the higher codebook levels step count (dotted
19Published as a conference paper at ICLR 2024
Figure 8: We restrict the attention maps to focus on local context for codebooks levels greater than
1. In this figure we consider 2time-steps restrictions for each side, in practice we use 5time-steps
for each side, resulting in 11tokens.
Table 6: CFG annealing ablation. We report FAD scores for different λ0, λ1configurations, as well
as KL and CLAP scores.
λ0→λ11→13→310→1010→120→2020→1
FADvgg↓ 3.95 0.99 0.63 0.61 0.80 0.68
KL↓ 0.79 0.60 0.56 0.56 0.57 0.56
CLAP scr↑0.13 0.28 0.28 0.31 0.31 0.31
blue curve), starting from the (20,10,10,10)decoding schedule. The red curve illustrates that a
good compromise between latency and quality can be obtained around 20steps for the first level,
after which decoding further will only marginally lower the FAD, while adding latency. Regarding
the higher codebook levels, we can observe an inflection point happening around 5steps after which
FAD remains stable. It is interesting to note that reducing the decoding steps for higher levels does
not impact quality as much as for the first level. For example the (10,10,10,10)decoding schedule
achieves 0.65FAD at a latency close to that of the (20,5,5,5)schedule, which achieves a lower
FAD of 0.61despite a smaller total step count.
The effect of CFG guidance annealing. We evaluate both constant CFG schedules, e.g. by setting
λ0=λ1= 3, and annealing CFG. Results are presented in Table 6. Results suggest that using
λ0= 10 ,λ1= 1 yields the best FAD score over all evaluated setups. This finding aligns with our
hypothesis that during the first decoding steps a stronger text adherence is required, while at later
decoding steps we would like the model to put more focus on previously decoded tokens.
20