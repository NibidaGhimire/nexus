arXiv:2302.03619v2  [cs.GR]  13 Feb 2023EUROGRAPHICS 2023 / K. Myszkowski and M. Nießner
(Guest Editors)Volume 42 (2023 ),Number 2
In-the-wild Material Appearance Editing using Perceptual
Attributes
J. Daniel Subias1and M. Lagunas2
1Universidad de Zaragoza, I3A, Spain
2Amazon, Spain
Edited Input 
Metallic  -      Metallic -       Metallic +      Metallic  +       Glossy  -         Glossy -        Glossy  +        Glossy +  
Figure 1: Our approach enables intuitive appearance editing of high- level perceptual attributes. Our framework takes as an inpu t a single
image of an object (top) and produces high-quality edits of m aterial attributes such as glossy or metallic, while preser ving the geometrical
structure and details (bottom). The“+” and “-” indicate whe ther the target high-level perceptual attribute is increas ed or decreased.
Abstract
Intuitively editing the appearance of materials from a sing le image is a challenging task given the complexity of the int eractions
between light and matter, and the ambivalence of human perce ption. This problem has been traditionally addressed by est i-
mating additional factors of the scene like geometry or illu mination, thus solving an inverse rendering problem and sub duing
the ﬁnal quality of the results to the quality of these estima tions. We present a single-image appearance editing framew ork
that allows us to intuitively modify the material appearanc e of an object by increasing or decreasing high-level percep tual
attributes describing such appearance (e.g., glossy or met allic). Our framework takes as input an in-the-wild image of a single
object, where geometry, material, and illumination are not controlled, and inverse rendering is not required. We rely o n gener-
ative models and devise a novel architecture with Selective Transfer Unit (STU) cells that allow to preserve the high-fr equency
details from the input image in the edited one. To train our fr amework we leverage a dataset with pairs of synthetic images ren-
dered with physically-based algorithms, and the correspon ding crowd-sourced ratings of high-level perceptual attri butes. We
show that our material editing framework outperforms the st ate of the art, and showcase its applicability on synthetic i mages,
in-the-wild real-world photographs, and video sequences.
CCS Concepts
•Computing methodologies →Machine learning; Computer graphics; Image manipulation;
1. Introduction
Humans are visual creatures, visual data play a key role in th e way
we perceive and understand the world around us. We are able of
recognizing materials, understanding their appearance, a nd reason-
ing about other physical properties effortlessly, just bri eﬂy lookingat them. The visual appearance of a material — whether it appe ars
glossy ,metallic , orrough — is one of the key properties that deter-
mine how we manipulate and interact with objects. However, s uch
visual appearance is formed by a complex multidimensional i nter-
action involving the material properties themselves, but a lso other
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John
Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
confounding factors like geometry or illumination, that ca n affect
our perception. Unfortunately, such underlying perceptua l process
is not yet completely understood [ And11 ,Fle14 ,MB10 ].
Given the large amount of dimensions that our perception in-
volves, many works have focused on editing material appeara nce
by estimating the known parameters of the scene (material, g e-
ometry, and illumination), thus solving an inverse renderi ng prob-
lem where the material is later modiﬁed [ BBP21 ,YX16 ]. How-
ever, this approach faces several problems: noise in the est ima-
tion of scene parameters can yield uncanny editing results. More-
over, the user should understand the intricacies of the mate rial
formulation to be able to edit. Techniques and hardware to ac -
quire material appearance are gaining accuracy, speed, and ef-
ﬁciency [ NJR15 ,AWL15 ,DJ18 ]; creating a data-driven shift for
appearance editing techniques [ SGM∗16,ZZW∗21,ZFWW20 ] .
These methods still present similar problems as using inver se ren-
dering. There exists a disconnection between the mathemati cal for-
mulation of material appearance and human-friendly parame ters.
The captured data is machine friendly but not human friendly . Edit-
ing material appearance given a single in-the-wild photogr aph, and
using intuitive perceptual attributes is therefore a chall enging task
that remains to be solved.
In this work, we present an image-based framework that does n ot
rely on any physically-based rendering but instead modiﬁes directly
the material appearance in the image space. It takes a certai n image
of an object as input and modiﬁes the appearance based on vary ing
the value of the desired high-level perceptual attribute de scribing
appearance (see Figure 1). Recently, the popularization of genera-
tive deep learning models had allow us to design data-driven frame-
works for image editing [ LDX∗19,HZK∗19,LZU∗17]. Since the
image cues that drive the perception of such attributes can n ot be
captured in a few image statistics, we rely on such generativ e neu-
ral networks to learn their relationship with material appe arance
and generate novel edited images [ SAF21 ,LSGM21 ].
We devise a generative architecture that allows to intuitiv ely
edit material appearance just from a single, in-the-wild, i mage and
given the value of the target perceptual attribute that we wa nt to
manipulate. To train our framework, a ﬁrst approach might be col-
lecting pairs of images the originals and the edited ones, wh ere
the edited examples were produced given a target high-level per-
ceptual attribute value. This approach is not only tedious b ut also
hinders the training process. We follow the work from Delano y et
al. [DLC∗22], which is based on training a two-step generative
framework using a wide dataset composed of a large variety of
images, paired with high-level perceptual ratings obtaine d through
user studies. However, this method needs additional geomet ry in-
formation (as a normal map) of the target object. We thus devi se
an encoder-decoder architecture that allows us to send the r elevant
high-frequency information of the object’s shape from the e ncoder
to the decoder thanks to the STU cell [ LDX∗19] and a novel loss
function. Thus, removing the need for the normal map as the in put
and not subduing our results in in-the-wild photographs to h aving
good estimations of the normal map.
We trained two version of our framework focusing on two high-
level perceptual attributes that are both common and easy to un-
derstand: metallic andglossy . We evaluate the consistency of ouredits on a wide variety of scenes with different illuminatio ns, ma-
terials, and geometries; and compare our results with the wo rk from
Delanoy et al. [ DLC∗22]. We observe that our method, despite re-
sorting to simpler input and having a simpler architecture, obtains
more realistic material appearance edits of the input image . We also
assess the temporal consistency by editing video sequences com-
posed of frames rendered using unseen illumination, geomet ry, and
materials. We observe that our framework is capable of produ cing
coherent outputs even when the additional temporal dimensi on is
included.
2. Related Work
2.1. Visual Perception
Understanding how our visual system interprets our world
is a longstanding goal in ﬁelds like computer graphics
or applied optics [ NOGRR21 ], that is yet to be under-
stood [ FDA01 ,FDA03 ]. Our visual perception of an ob-
ject is guided by its material properties; but, also involve s
factors such as geometry [ VLD07 ,HFM16 ], light condi-
tions [ HLM06 ,KFB10 ,CK15 ] or motion [ DFY∗11,MLMG19 ].
To reduce the dimensionality of the problem, previous work
have focused on understanding single, high-level appear-
ance properties like glossiness [PFG00 ,WAKB09 ,CK15 ],
translucency [ GXZ∗13,XZG∗20,GWA∗15] and soft-
ness [ SFV20 ,CDD21 ]; or draw inspiration from artists’ implicit
understanding of the key visual cues that guide visual perce p-
tion [ DCWP19 ,DSMG21 ]. Recent works suggest that material
perception may be driven by complex non-linear statistics b etter
approximated by highly non-linear models such as neural net -
works [ FS19 ,SAF21 ,DLG∗20,LMS∗19]. Inspired by this, we
propose a deep-learning-based framework for material appe arance
editing that relies on images paired with human judgments ab out
high-level perceptual attributes to be trained.
2.2. BRDF-based Material Editing
Editing material appearance is a complex task since there is a dis-
connection between our perception and materials’ physical proper-
ties [ FWG13 ,TFCRS11 ,CK15 ]. Non-parametric models such as
SVBRDFs are hard to edit. Different approaches have been pro -
posed, such as inverse shading trees [ LBAD∗06], procedural mod-
els [HHD∗22], or using deep-learning techniques [ DDB20 ]. Sev-
eral perceptually-based frameworks have been proposed to p ro-
vide intuitive controls over parametric [ FPG01 ,PFG00 ] and non-
parametric appearance models [ SGM∗16,MGZ∗17,HGC∗20].
Unfortunately, these models only capture material propert ies, while
leaving out other scene parameters that also drive our perce p-
tion of material appearances, such as geometry and illumina -
tion [ LSGM21 ]. Another line of work proposes an intrinsic de-
composition of the scene to manipulate materials [ BM15 ,YS19 ].
More recently, NeRF [ MST∗20] based approaches for such
decomposition are allowing for unprecedented levels of rea l-
ism [ BBJ∗21,ZSD∗21,SDZ∗21]. However, these methods only
provide a new material deﬁnition that can later be used in a sp eciﬁc
3D scene but do not allow to edit it. Our work presents an intui tive
framework that directly uses in-the-wild images and is capa ble of
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
editing the appearance of materials using high-level attri butes such
asglossiness .
2.3. Image-based Material Editing
Image-based material editing attempts to manipulate the pi xel
values directly into the image space. Several interactive f rame-
works have been proposed to provide users more intuitive con -
trols over the target editing regions of the image, selectin g them
just from a few strokes [ PL07 ,AP08 ,DTPG11 ]. The work from
Khan et al. [ KRFB06 ] proposes a single-image editing frame-
work exploiting our tolerance to certain physical inaccura cies.
Such approach was extended later to relight paintings [ LMJH∗10],
consider global illumination and caustics [ GSLM∗08], or weath-
ering effects [ XWT∗08]. More recent work, supported by the
success of deep-learning-based methods, introduces editi ng ap-
proaches by factoring the image into shape, material, and re -
ﬂectance [ LCY∗17,RRF∗16,MLTFR19 ]. Splitting an image into
separate components like frequency bands [ BBPA15 ] or shad-
ing and reﬂectance [ GMLMG12 ] has been a standard prac-
tice for image manipulation. Instead, we propose a generati ve-
based editing framework without the need to decompose the in -
put image, learning to edit directly the visual cues that dri ve
our perception of high-level attributes. Generative Adver sar-
ial Networks (GANs) [ GPAM∗14] have been proposed to edit
face attributes (i.e., hair color or gender) through the lat ent
space [ LZU∗17,HZK∗19,LDX∗19]. Our framework works in-
stead in the complex problem of material appearance, where c on-
founding factors like geometry or illumination have a direc t im-
pact in our perception. Closer to ours, Delanoy et al. [ DLC∗22]
introduce a generative framework for intuitive appearance editing
using high-level perceptual attributes. However, their me thod re-
quires two inputs: the image, and its normal map which yields non-
photorealistic results for in in-the-wild images where the normal
map is estimated. We devise a novel generative architecture that al-
lows to keep the high-frequency information from the geomet ry of
the input images, thus removing the need for the normal map as the
input while obtaining superior performance.
3. Our Framework
This section describes our proposed framework for single-i mage
appearance editing. We introduce our goal (Section 3.1), describe
the mathematical model of the Selective Transfer Units (STU ) cells
(Section 3.2), and, explain the different modules that make our
model architecture is built upon (Section 3.3).
3.1. Goal and Overview
Our goal is to generate an image ywhose material appearance we
want to edit from an input image xand a value attt∈[0,1]of the tar-
get high-level perceptual attribute to edit (e.g., glossy ormetallic ).
The target edited image ydepicts the same object as xand elicits a
visual appearance according to the value of the high-level p ercep-
tual attribute attt. For instance, more glossy ifatttis closer to 1 and
less if closer to 0. To achieve it, we introduce a novel framew ork
that relies on an encoder-decoder architecture Gthat encodes the
image x, and manipulates the latent space ztogether with the targetattribute atttto generate the edited image y. A high-level overview
of our framework is shown in Figure 2.
3.2. Selective Transfer Units
When using encoder-decoder architectures, it is common pra c-
tice to send information between the encoder and decoder by
using skip-connections [ RFB15 ]. This allows us to keep high-
frequency details in the generated image that are lost other -
wise [ KW13 ,HMP∗17]. However, in image editing tasks that ma-
nipulate the latent space, adding skip connections hampers the
editability of the model, where only the input image can be re -
constructed [ CCK∗18,HZK∗19,DLC∗22]. To bridge this gap, we
send information from the encoder to the decoder by selectiv ely
removing unnecessary data using Selective Transfer Units ( STU)
memory cells [ LDX∗19].
The STU architecture, illustrated in Figure 2, is a variant of the
GRU [ CvMG∗14,CGCB14 ] and allows encoder-decoder architec-
tures to keep the relevant information of the input image in t he
edited output when manipulating the latent space z. Given the fea-
ture map of the lthencoder layer denoted by flenc, the STU cell out-
puts an edited feature map flt, as is shown in Figure 2. Each STU
cell receives information from the previous cell via a featu re map
ˆsl+1(also called hidden state), which also contains informatio n of
the target attribute attt. The STU updates its internal hidden state
denoted as sland sends this to the next STU cell. For further infor-
mation about the mathematical formulation of STU cells refe r to
Appendix A.
3.3. Network Architecture
Our framework is a GAN-like model composed of a generator G
and a discriminator Dthat is only used during training. Our goal is
to leverage Gto edit an in-the-wild input image xaccording to a
target high-level perceptual attribute describing appear ance atttto
generate an edited image ywhere
y=G(x,attt). (1)
The generator Gis composed of an encoder module Gencthat en-
codes the input image to a latent vector zand a decoder module
Gdecthat generates the edited image, both with the same number n
of layers.
The encoder Genccompresses the image xin a latent code z,
while storing the set of features maps f={f1enc,f2enc,...,fn−1enc}, gen-
erated by its convolutional layers. The latent code zcorresponds to
the feature map generated by the last convolutional layer su ch that
z=fnenc. The decoder Gdecreconstructs the edited image yfrom
the latent code zconcatenated with the target high-level perceptual
attribute attt. To keep the high frequencies from the input in the
edited image y, we send the feature maps fvia skip-connections.
However, performing this process without additionally pro cessing
the information in the features affects the editing ability of the gen-
erator G. Thus, we introduce an STU cell in each skip-connection
where Gstdenotes the set of STU cells of the framework, and Glst
corresponds to the STU cell of the lthlayer. The STU cells edit the
set of feature maps ffrom the input, generating a set of edited ones
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
ft={f1t,f2t,...,fn−1
t}. Since the STU cell of the deepest skip con-
nection Gn−1
stdoes not receive a hidden state ˆsfrom another STU
cell, Gn−1
sttakes as input the latent code zconcatenated with the
target attribute atttto edit the feature map fn−1enc, as is shown in Fig-
ure2. For further details of our architecture refer to Appendix B.
4. Learning to Edit Material Appearance
Training GAN-like models is a complex task that requires car eful
tuning of the hyperparameters. We ﬁrst describe the dataset of im-
ages, with paired crowd-sourced ratings of high-level perc eptual
attributes, that we used to train our framework (Section 4.1). Then
we introduce the loss function that allows us to faithfully e dit ma-
terial appearance according to the target perceptual attri bute (e.g.,
glossy ) (Section 4.2), and last, we describe the technical details of
the training process (Section 4.3).
4.1. Training Dataset
Generating ground-truth data from analytical BRDF models i s not
a robust approach to training our framework, since their par ame-
ters are not aligned with our perception [ NDM06 ,WAKB09 ]. In
addition, our framework would learn to edit based on variati ons
in a physical parameter (e.g., roughness) and not on variati ons
in our perception. Thus, we leverage the dataset of Delanoy e t.
al [DLC∗22], designed for material appearance perception tasks.
This dataset based on the Lagunas et al. dataset [ LMS∗19] con-
tains renderings of 13 different geometries, illuminated b y 7 cap-
tured real-world illuminations [ Deb]. Renders have been made with
the physically-based renderer Mitsuba [ Jak10 ] using 100 differ-
ent BRDFs from the Merl dataset [ MPBM03 ]. For each combina-
tion of material ×shape×illumination, 5 different images with
slight variations in the viewpoint (randomly sampled withi n a 45
degrees cone around the original viewpoint) have been rende red, as
is shown in Figure 3. The dataset has 45,500 images (13 geometries
×100 materials ×7 illuminations ×5 views).
Each scene is described by a set of high-level perceptual at-
tributes (i.e., plastic ,rubber ,metallic ,glossy ,bright ,rough , and the
strength and sharpness of reﬂections ) rated on a normalized Likert
scale, from 0 to 1. A total of 2,600 paid subjects participate d in
the study, each of them rating 15 different random images. Th e
ﬁnal dataset gathers a perceptual rating per viewpoint. To r educe
the presence of noise and outliers, we use the median value po oled
over the 5 viewpoints and the shape. Then the attribute value s are
different for each material and illumination.
Data Augmentation To have more diversity in the images and
help our model generalize better, we add a data augmentation
pipeline. First, we resize the images to 512 ×512 px; then we ﬂip
and rotate them 90 degrees randomly, and we ﬁnally crop them t o
480×480 px. In addition, to reduce the bias on the colors BRDF
we represent images on the HSV color space and make a shift in
the Hue and Saturation channels. Finally, the input is resiz ed to a
size of 256 ×256 px. to remove the background. Within each input
image, the background is removed by applying a mask of the ob-
ject’s silhouette. Each image in the training dataset is pai red with
a high-level perceptual attribute atts∈[0,1]; however, with ourframework, we want to edit the input images with a different t ar-
get attribute attt. During training, we sample atttrandomly using a
uniform distribution U([0,1])allowing the generator Gto produce
edited images different from the input to trick the discrimi nator D.
4.2. Loss Functions
We adopt the adversarial training proposed by He et al. [ HZK∗19]
and introduce a GAN model where the discriminator Dhas two
branches DadvandDatt.Dadvconsists of ﬁve convolution layers
to predict whether an image is fake (edited) or real. Dattshares
the convolution layers with Dadvand, instead, predicts the high-
level attribute value attt. Figure 4shows a high-level scheme of
our framework during training.
Adversarial Losses Since we do not know what are the ground-
truth edited results, we use an adversarial loss [ GPAM∗14] aiming
to generate edited results indistinguishable from real ima ges. GANs
are complex to train, partially due to the instability of the loss func-
tion proposed in the original formulation [ ACB17 ]. Thus, we rely
on the WGAN-GP [ GAA∗17] loss to alleviate such a problem. The
discriminator Dis trained to give a high score to real images and
a low score to generated ones, aiming to disambiguate them, a nd
tries to minimize the adversarial loss deﬁned as:
LDadv=Ex[Dadv(x)]−Ey[Dadv(y)]+LGP. (2)
Formally, Dshould be a 1-Lipschitz continuous function. To
keep this constraint, WGANs [ ACB17 ] introduces a gradient
penalty term deﬁned as follows:
LGP=λ1Eˆ x/bracketleftBig
(||∇ ˆ xDadv(ˆ x)||2−1)2/bracketrightBig
. (3)
Where λ1is the gradient penalty weight. The generator Gis
trained such that the discriminator Dbelieves that generated im-
ages are real, giving them a high score. Therefore the genera tor’s
adversarial loss is given by:
LGadv=Ey[Dadv(y)]. (4)
Attribute Manipulation Losses Even though the ground truth is
missing, the editing result has to elicit a visual impressio n accord-
ing to the target perceptual attribute attt. Therefore, we introduce
an attribute classiﬁer Dattwhich learns to predict the attribute val-
uesattsfrom the images that belong to the training dataset. Since
we have to compare the distance from the predicted attribute s by
Datttoatts, the following attribute manipulation loss is computed
and optimized by Dattduring training:
LDatt=||atts−Datt(x)||1. (5)
The generator Gshould produce images with similar looks to
real ones to trick D, so its edits must be consistent with the target
perceptual attributes attt, minimizing the following distance:
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
Figure 2: (a) High-level overview of our framework. Our generator G is composed of an encoder G encand a decoder G dec. It is capable of
editing the input image xaccording to the target attribute atttto generate the edited image y. (b) The architecture of a single STU cell. As
an input, it takes the feature map of the current layer flencand the hidden state of the previous cell ˆsl+1. It outputs the updated hidden state sl
and feature map flt.
Illuminations GeometriesPeter
Sphere       Statue      Teapot    Waterpot   Havran-1   Havran-2 
Zenith        Blob        Bunny      Dragon        Lucy      Suzanne 
Einstein Viewpoints
Grace Doge Glacier Pisa Ennis Uﬃzi
Figure 3: Seven illuminations present in the training dataset and
corresponding rendered spheres with the brass material (to p). Thir-
teen scenes of the geometries present in the training datase t, and
ﬁve viewpoints of the bunny geometry rendered using the nylo n ma-
terial and Ennis illumination (bottom).
LGatt=||attt−Datt(y)||1. (6)
Reconstruction Loss Each image in the training set has crowd-
sourced ratings of high-level perceptual attributes atts, describing
their appearance. Thus, if we ask our generator Gto edit xaccord-
ing to the attribute atts, it should generate an edited image ˆxsimilar
to the input. We minimize the following L1-norm during train ing:
Lrec=||x−ˆx||1. (7)
Figure 4: Training scheme of our framework. The gray block rep-
resents our generator G, and the discriminator branches D advand
Dattare illustrated by the purple and yellow blocks, respective ly.
Pointed arrows denote the parameters used as input for the tr ain-
ing losses (Section 4.2).
Final Loss Taking the above losses into account, the objectives to
train the discriminator Dand generator Gcan be formulated as:
min
DL=−L Dadv+λ2LDatt, (8)
min
GL=−L Gadv+λ3LGatt+λ4Lrec. (9)
Where λ2,λ3, and λ4are the model trade-off parameters, that
are tunned to yield values of the individual losses in a simil ar order
of magnitude. Both GandDwill try to minimize their objective
function, however, once Ghas learned to trick D, the loss function
of the latter will tend to increase until both models reach a s table
equilibrium.
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
4.3. Training Details
We train the model using the ADAM optimizer [ KB14 ] with β1=
0.5 and β2=0.999. The learning rate ηis 2×10−4for both Gand
D. The trade-off parameters are λ2=50,λ3=100 and λ4=1000
for the Equations 8and9, and λ1=10 for Equation 3(see Ap-
pendix Bfor further details). At each training step, the discrimina -
tor iterates 7 times while the generator updates its paramet ers once.
The memory module Gstis composed of 4 skip connections with
STU cells between the encoder and decoder. All the experimen ts
are computed using the PyTorch [ PGM∗19] library with cuDNN
7.1, running on an Nvidia GeForce RTX 3090 GPU. In total, trai n-
ing the whole framework takes two days.
5. Evaluation and Results
We evaluate our framework on a set of synthetic images and rea l
photographs that have not been seen by the model during train -
ing. We start by describing the evaluation dataset(Section 5.1); val-
idate the design choices of our framework with a series of abl ation
studies (Section 5.2); demonstrate the robustness of the proposed
method by comparing the obtained results with varying geome -
try, illumination, or material (Section 5.3); and ﬁnally compare our
method to the state of the art, obtaining better performance (Sec-
tion5.4).
5.1. Evaluation Dataset
To evaluate our framework we leverage the synthetic dataset used
by Delanoy et al. [ DLC∗22], containing scenes with shapes and
materials never seen during training. They have been render ed us-
ing eight shapes collected from online sources, four illumi nations
obtained from HDRI-Haven [ Hdr], and eight materials coming
from Dupuy and Jakob’s database [ DJ18 ]. Figure 5shows a rep-
resentative subset of the synthetic scenes. We also test our frame-
work’s performance with real-world photographs downloade d from
online catalogs of decorative items and with in-the-wild mo bile
photos taken by us. We masked the object of interest using the on-
line API Kalideo [ Kal]. Figure 6shows material appearance editing
results using our framework sampling different values for t he tar-
get attribute attt. We can see the consistency of our edits when the
attribute varies for in-the-wild photographs.
5.2. Ablation Studies
We evaluate our design choices via a series of ablation studi es. Our
framework is based on a generative architecture that uses a d iscrim-
inator Dduring training and STU cells [ LDX∗19] to obtain more
realistic edited images while keeping the relevant high-fr equency
details of the input image. The model without the discrimina torD
or without STU cells are capable of learning to better recons truct
the input image. However, without D(w/o D) we are not able to
properly edit the material appearance since no feedback abo ut such
edits exists. On the other hand, concatenating skip-connec tions di-
rectly, without including information from the attribute, hampers
the decoder’s ability to generate the edited image from the l atent
code z(w/o STUs). Figure 7, depicts how not using a discrimina-
torDgenerates an image almost equal to the input, not allowing
Figure 5: Synthetic dataset used to evaluate our framework. The
images show eight geometries rendered with illumination an d ma-
terials never seen in the training process.
InputMetallic Glossy
Figure 6: Editing results by varying the metallic or glossy at-
tributes. The “+” and “-” indicate whether the target high-l evel
perceptual attribute is increased or decreased.
for editing. Besides, not including the STU cell does not con vey
the desired appearance according to the target high-level a ttribute
in the ﬁnal image.
5.3. Consistency of the Edits
We test the robustness of our framework exploring its editin g abil-
ity through samples from the evaluation dataset. Since we ha ve a
wide collection of rendered images with diverse materials, illumi-
nations, and geometries; we can ﬁx a scene parameter (i.e., i llumi-
nation, geometry, or material) and vary the other two. When w ork-
ing with in-the-wild photographs, scene parameters are unk nown.
We can not modify the material or geometry of the objects but w e
are able to change their illumination conditions by placing them
elsewhere. Figure 8(a), (b), (c) shows images edited by our frame-
work for the metallic attribute while varying scene parameters for
synthetic images. On the other hand, in (d) we vary the illumi nation
conditions for in-the-wild photographs of the same object ( for fur-
ther results see the Supplementary Material). We can see how our
framework produces consistent results in the different con ditions,
and with synthetic and in-the-wild photographs.
We also test the temporal consistency of our framework by edi t-
ing two video sequences frame by frame for both metallic and
glossy attributes.
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
Input w/o D w/o STUs OursGlossy +
Figure 7: Ablation studies where we analyze the impact of the
different components of our framework. From left to right: i nput
image, edited image without the discriminator D during trai ning,
edited image without using the STU cell, and our framework. W e
can see how by not using the discriminator D the framework is n ot
able to edit and only reconstructs the input image, and using only
skip connections, without the STU, hampers the ability to ed it. The
“+” indicates an increase of the target attribute.
Table 1: Average PSNR, SSIM, MSE and MAE reconstruct-
ing the input image on the synthetic dataset. We can see how
our framework outperforms the method proposed by Delanoy et
al. [DLC∗22].
Method PSNR↑SSIM↑MSE↓MAE↓
DLC*22 15.989 0.761 0.095 0.031
Ours 27.388 0.967 0.002 0.023
5.4. Comparison with the State of the Art
We compare our results against the method of Delanoy et
al. [DLC∗22]. We show results reconstructing the input image us-
ing the crowd-sourced perceptual attribute as the input, an d edit-
ing the input using a different target perceptual attribute . Since the
method of Delanoy et al. also needs the normal map as the input , we
evaluate in-the-wild photographs with their estimated nor mal map
(using Delanoy et al. estimator), and synthetic images with a per-
fectly normal map. Our method does not need the normal map as
the input. We rely on qualitative and quantitative comparis ons em-
ploying four metrics: Pixel-to-Signal Noise Ratio (PSNR), Struc-
tural Similarity Index (SSIM), Mean Squared Error (MSE), an d
Mean Absolute Error (MAE).
Quantitative Evaluation To evaluate the reconstruction ability we
rely on the synthetic images with their crowd-sourced perce ptual
attributes. Table 1shows we outperform the state of the art as a
result of introducing STU cells in each skip-connection. As illus-
trated in Figure 9, our method keeps high-frequency details from
the input image without the need of a normal map of the object’ s
surface as the input. We can see that specular reﬂections are present
on the reconstructed image while the previous method blurs t hem,
keeping only low frequencies.
Image Editing In Figure 10we can see a comparison between
the edited images by our method and the one by Delanoy et
al. [DLC∗22] (for further comparisons see the Supplementary Ma-
terial). Our approach learns to edit perceptual cues proper ly while
objects’ shape remains unchanged. The material appearance edits
from Delanoy et al. [ DLC∗22] strongly depend on the shape of their
estimated normal map [ LAD∗21]. This causes geometry details thatare not present in the normal map not to be present in the edite d im-
age. Also, an inaccurate estimation of the normal map may def orm
the original shape, especially in in-the-wild photographs where ge-
ometries are usually highly complex (see Figure 11).
5.5. Comparison with Physically-based Rendering
We compare our results with physically-based rendering by v ary-
ing the roughness parameter of the Principled BSDF [ BS12 ,Bur15 ]
in the interval [0,1]and rendering different versions of the same
scene (using geometry and illumination not present in our tr ain-
ing dataset). The other parameters and the albedo constant a nd we
rely on the physically-based renderer Mitsuba 3 [ JSR∗22] to gen-
erate the images. Then, with our method, we increase and decr ease
the gloss attribute using the rendered scene with a roughnes s value
of 0.5 as the input. As we can see in Figure 12, our edits convey
the overall appearance of the rendered images. However, we a re
less accurate when the gloss attribute is increased. This ma y be ex-
plained since removing existing information (highlights) is easier
than introducing missing information without providing th e envi-
ronment map.
6. Limitations and Future Work
We have presented and validated a framework for intuitive ma terial
appearance editing using single, in-the-wild images. We re lied on a
large set of images paired with crowd-sourced ratings of hig h-level
perceptual attributes to train our framework. We use a gener ative
neural network and devise a loss function that allows us to le arn
how to edit material appearance based on such high-level att ributes,
without any pairs of original and edited images. Our results show
that the presented method can achieve realistic results, al most on
par with real photographs, on a wide variety of different inp uts.
However, our method is not free of limitations. As we can see i n
Figure 13, when using input photographs that feature highly spec-
ular highlights, while able to convey the appearance of the t arget
high-level perceptual attribute, our framework may strugg le to edit
them. Instead of considering the original albedo to perform edit the
highlights when glossiness is decreased, the resulting edits feature
a dimmed region.
Material appearance perception and editing pose many chal-
lenges that are not fully investigated in this work. We pro-
pose a data-driven approach for intuitive material editing where
we have relied on an existing dataset that was rendered using
MERL [ MPBM03 ] material measurements. MERL just contains
100 real-world isotropic BRDFs, we have tried to increase th e
variety of such data using different data augmentation stra te-
gies during training time. This has allowed us to obtain plau si-
ble results, nevertheless, exploring more complex materia l repre-
sentations, or including other newly measured material dat asets
[DJ18 ,FV14 ,SCW∗21] could help obtain more universal models
for material editing. Also, the high-level perceptual rati ngs in the
dataset come from online surveys. While we may identify metallic
andglossy as different properties, there is a certain degree of cor-
relation in user answers between those attributes. In Figur e14we
show a real photograph edited by our framework. There, we ob-
serve that while yielding different results, the increased glossy and
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual AttributesMetallic + Metallic - Input
(a) (b) (d) (c)
Figure 8: Examples of edited images by our framework. (a) Renders of th e Among Us geometry under three different illuminations, an d
two constant materials. (b) Renders of Mario, Boomerang Flo wer, and Buddha geometries with the same material and light c onditions. (c)
Renders of the Boomerang Flower geometry with the same illum ination but different materials. (d) Photographs of a rubbe r duck taken in
different places under three light conditions. Our framewo rk is capable of producing compelling and consistent edits i n all cases. The “+”
corresponds to an increase in the target metallic attribute value attt, while the “-” corresponds to a decrease.
PSNR/SSIM
MSE/MAE  13.20/0.654
0.048/0.14227.39/0.967
0.002/0.023Input  
 DLC ∗22 Ours
Figure 9: A demonstration of the reconstruction quality for Buddha
geometry. Yellow insets show regions of the object’s surfac e with
specular reﬂections.
metallic edits share a certain resemblance. Investigating other at-
tributes, or different perceptual data collection strateg ies may yield
improved performance and more intuitive tools. We also obse rve
that the collected perceptual ratings are not distributed u niformly.
This result may come from both, a bias in the dataset, and hu-
mans’ perceptual behavior. As a result, our framework may no t
learn to uniformly edit material appearance when varying th e tar-
get attribute (see Supplementary Material). Another poten tial ap-
proach to generate the dataset would be to rely on a BRDF model
to generate images, and input a parameter of this material mo del
(e.g., roughness) as the target attribute. However, while t he frame-
work may be trained using this data, in this work we are addres sing
the more complex problem of editing appearance from high-le vel
perceptual attributes where the users have factored in all p otential
confounding factors of perception (including the material model) in
their ratings [ LSGM21 ]. Last, the generative neural network used
in this work has been trained on the limit of our hardware. To u se
higher-resolution images, one could use the proposed metho d (and
its weights) as a backbone, add additional layers, and ﬁne-t une the
model while increasing the resolution size of the input. Thi s could
be repeated in an iterative process until we get the desired r esolu-
Ours DLC ∗22
 DLC ∗22 Ours Input 
Figure 10: Comparison editing the glossy attribute using the
method of Delanoy et al. [ DLC∗22] and our framework for two in-
the-wild photographs. Our framework only requires the phot ograph
as the input while the work of Delanoy et al. needs to estimate the
normal map. We can see how our method better recovers the glos sy
appearance of the object when edited. Besides, it is able to r ecover
better high-frequency details. The “+” and “-” indicate whe ther
the target high-level perceptual attribute increased or de creased.
tion size [ KALL17 ]. However, although this is a possible approach,
its effectiveness requires further investigation. We have created a
framework that is trained for each attribute. Developing a n ovel
methodology that would allow to manipulate an appearance vector
can help to have a more comprehensive description of materia l ap-
pearance, and more intuitive edits. In addition to the resul ts we have
shown, we hope that our work can inspire additional research and
different applications around material appearance. We wil l make
our code available for further experimentation, to facilit ate the ex-
ploration of these possibilities.
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
Metallic +
 Glossy +Est. 
Norms. DLC ∗22 Input Ours
Figure 11: Delanoy et al. [ DLC∗22] output is highly dependent to
the estimated normal map of the input image. A low-frequency esti-
mation of the normal map yields dull edits of material appear ance.
The “+” indicates an increase of the target attribute.
Input
Render Ours
Figure 12: Results varying the roughness parameter of the Prin-
cipled BSDF [ BS12 ,Bur15 ] (top row) and using our framework
(bottom row) to edit the gloss high-level perceptual attrib ute. The
“+” and “-” indicate whether the roughness parameter and the
target high-level perceptual attribute increased or decre ased.
Input Glossy - Glossy - Input
Figure 13: Example of two failure cases of our framework for the
glossy attribute on photographs. The “-” indicates a value o f the
target attribute set to 0.
7. Acknowledgements
This project has received funding from the Government of
Aragon’s Departamento de Ciencia, Universidad y Sociedad d el
Conocimiento through the Reference Research Group "Graphi cs
and Imaging Lab", the European Union’s Horizon 2020 researc h
and innovation program under the Marie Skłodowska-Curie gr ant
agreement No 956585 (PRIME), the CHAMELEON project (Eu-
ropean Union’s Horizon 2020, European Research Council, gr ant
agreement No. 682080), and MCIN/AEI 10.13039/50110001103 3
Input Glossy - Glossy + Metallic - Metallic+
Figure 14: Comparison between the metallic and glossy attribute.
The “+” indicates that atttincreases, while “-” indicates that it
decreases. It is interesting to observe that if the metallic attribute
is decreased, the gloss is not removed from the image.
through Project PID2019-105004GB-I00. We want to thank the In-
stitute for Pure and Applied Mathematics, Universidad Poli tecnica
de Valencia, for the provided computational resources. Als o, we
would like to thank Daniel Martin and Ana Serrano for proofre ad-
ing, and Adolfo Munoz, Julio Marco, and Diego Gutierrez for t he
discussions about the project. Manuel Lagunas’ work was don e as
part of a collaboration with Universidad de Zaragoza and it d oes
not interfere with his position at Amazon.
References
[ACB17] A RJOVSKY M., C HINTALA S., B OTTOU L.: Wasserstein gen-
erative adversarial networks. In Proc. International Conference on Ma-
chine Learning (ICML) (06–11 Aug 2017), Precup D., Teh Y . W., (Eds.),
vol. 70 of Proceedings of Machine Learning Research , PMLR, pp. 214–
223. 4
[And11] A NDERSON B. L.: Visual perception of materials and surfaces.
Current Biology 21 , 24 (2011), R978–R983. 2
[AP08] A NX., P ELLACINI F.: Appprop: All-pairs appearance-space edit
propagation. In ACM SIGGRAPH 2008 Papers (New York, NY , USA,
2008), SIGGRAPH ’08, Association for Computing Machinery. 3
[AWL15] A ITTALA M., W EYRICH T., L EHTINEN J.: Two-shot svbrdf
capture for stationary materials. ACM Trans. on Graphics 34 , 4 (jul
2015). 2
[BBJ∗21] B OSS M., B RAUN R., J AMPANI V., B ARRON J. T., L IUC.,
LENSCH H. P.: Nerd: Neural reﬂectance decomposition from image col -
lections. In Proc. International Conference on Computer Vision (ICCV)
(October 2021), pp. 12684–12694. 2
[BBP21] B ATIM., B ARLA P., P ACANOWSKI R.: An inverse method for
the exploration of layered material appearance. ACM Trans. on Graphics
40, 4 (jul 2021). 2
[BBPA15] B OYADZHIEV I., B ALA K., P ARIS S., A DELSON E.: Band-
sifting decomposition for image-based material editing. ACM Trans. on
Graphics 34 , 5 (nov 2015). 3
[BM15] B ARRON J. T., M ALIK J.: Shape, illumination, and reﬂectance
from shading. IEEE Transactions on Pattern Analysis and Machine In-
telligence 37 , 8 (2015), 1670–1687. 2
[BS12] B URLEY B., S TUDIOS W. D. A.: Physically-based shading at
disney. In ACM SIGGRAPH (2012), vol. 2012, vol. 2012, pp. 1–7. 7,9
[Bur15] B URLEY B.: Extending the disney brdf to a bsdf with integrated
subsurface scattering. SIGGRAPH Course: Physically Based Shading in
Theory and Practice. ACM, New York, NY 19 (2015). 7,9
[CCK∗18] C HOI Y., C HOI M., K IMM., H AJ.-W., K IMS., C HOO
J.: Stargan: Uniﬁed generative adversarial networks for mu lti-domain
image-to-image translation. In Proc. Computer Vision and Pattern
Recognition (CVPR) (June 2018). 3
[CDD21] C AVDAN M., D REWING K., D OERSCHNER K.: Materials in
action: The look and feel of soft. bioRxiv (2021). 2
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
[CGCB14] C HUNG J., G ÜLÇEHRE Ç., C HOK., B ENGIO Y.: Empiri-
cal evaluation of gated recurrent neural networks on sequen ce modeling.
CoRR abs/1412.3555 (2014). 3,12
[CK15] C HADWICK A., K ENTRIDGE R.: The perception of gloss: A
review. Vision Research 109 (2015), 221–235. Perception of Material
Properties (Part I). 2
[CvMG∗14] C HOK., VAN MERRIËNBOER B., G ULCEHRE C., B AH-
DANAU D., B OUGARES F., S CHWENK H., B ENGIO Y.: Learning
phrase representations using RNN encoder–decoder for stat istical ma-
chine translation. In Proc. Empirical Methods in Natural Language Pro-
cessing (EMNLP) (Doha, Qatar, Oct. 2014), Association for Computa-
tional Linguistics, pp. 1724–1734. 3,12
[DCWP19] D ICICCO F., W IJNTJES M. W. A., P ONT S. C.: Under-
standing gloss perception through the lens of art: Combinin g percep-
tion, image analysis, and painting recipes of 17th century p ainted grapes.
Journal of Vision 19 , 3 (03 2019), 7–7. 2
[DDB20] D ESCHAINTRE V., D RETTAKIS G., B OUSSEAU A.: Guided
ﬁne-tuning for large-scale material transfer. Computer Graphics Forum
39, 4 (2020), 91–105. 2
[Deb] D EBEVEC P.:. URL:https://vgl.ict.usc.edu/Data/HighResProbes/ .
4
[DFY∗11] D OERSCHNER K., F LEMING R., Y ILMAZ O., S CHRATER P.,
HARTUNG B., K ERSTEN D.: Visual motion and the perception of sur-
face material. Current Biology 21 , 23 (2011), 2010–2016. 2
[DJ18] D UPUY J., J AKOB W.: An adaptive parameterization for efﬁcient
material acquisition and rendering. ACM Trans. on Graphics 37 , 6 (dec
2018). 2,6,7
[DLC∗22] D ELANOY J., L AGUNAS M., C ONDOR J., G UTIERREZ D.,
MASIA B.: A generative framework for image-based editing of mater ial
appearance using perceptual attributes. Computer Graphics Forum 41 , 1
(2022), 453–464. 2,3,4,6,7,8,9,13
[DLG∗20] D ELANOY J., L AGUNAS M., G ALVE I., G UTIERREZ D.,
SERRANO A., F LEMING R., M ASIA B.: The role of objective and sub-
jective measures in material similarity learning. In ACM SIGGRAPH
2020 Posters (New York, NY , USA, 2020), SIGGRAPH ’20, Associa-
tion for Computing Machinery. 2
[DSMG21] D ELANOY J., S ERRANO A., M ASIA B., G UTIERREZ D.:
Perception of material appearance: A comparison between pa inted and
rendered images. Journal of Vision 21 , 5 (05 2021), 16–16. 2
[DTPG11] D ONG Y., T ONG X., P ELLACINI F., G UOB.: Appgen: In-
teractive material modeling from a single image. In Proceedings of the
2011 SIGGRAPH Asia Conference (New York, NY , USA, 2011), SA ’11,
Association for Computing Machinery. 3
[FDA01] F LEMING R. W., D ROR R. O., A DELSON E. H.: How do
humans determine reﬂectance properties under unknown illu mination? 2
[FDA03] F LEMING R. W., D ROR R. O., A DELSON E. H.: Real-world
illumination and the perception of surface reﬂectance prop erties. Journal
of Vision 3 , 5 (07 2003), 3–3. 2
[Fle14] F LEMING R. W.: Visual perception of materials and their prop-
erties. Vision Research 94 (2014), 62–75. 2
[FPG01] F ERWERDA J. A., P ELLACINI F., G REENBERG D. P.: Psy-
chophysically based model of surface gloss perception. In Human Vision
and Electronic Imaging VI (2001), Rogowitz B. E., Pappas T. N., (Eds.),
vol. 4299, International Society for Optics and Photonics, SPIE, pp. 291
– 301. 2
[FS19] F LEMING R. W., S TORRS K. R.: Learning to see stuff. Current
Opinion in Behavioral Sciences 30 (2019), 100–108. Visual perception.
2
[FV14] F ILIP J., V ÁVRA R.: Template-based sampling of anisotropic
BRDFs. Computer Graphics Forum 33 , 7 (October 2014), 91–99. 7
[FWG13] F LEMING R. W., W IEBEL C., G EGENFURTNER K.: Percep-
tual qualities and material classes. Journal of Vision 13 , 8 (07 2013),
9–9. 2[GAA∗17] G ULRAJANI I., A HMED F., A RJOVSKY M., D UMOULIN V.,
COURVILLE A. C.: Improved training of wasserstein gans. In Advances
in Neural Information Processing Systems (2017), Guyon I., Luxburg
U. V ., Bengio S., Wallach H., Fergus R., Vishwanathan S., Gar nett R.,
(Eds.), vol. 30, Curran Associates, Inc. 4
[GMLMG12] G ARCES E., M UNOZ A., L OPEZ -MORENO J., G UTIER -
REZ D.: Intrinsic images by clustering. Computer Graphics Forum 31 ,
4 (2012). 3
[GPAM∗14] G OODFELLOW I., P OUGET -ABADIE J., M IRZA M., X U
B., W ARDE -FARLEY D., O ZAIR S., C OURVILLE A., B ENGIO Y.: Gen-
erative adversarial nets. In Advances in Neural Information Processing
Systems (2014), Ghahramani Z., Welling M., Cortes C., Lawrence N.,
Weinberger K., (Eds.), vol. 27, Curran Associates, Inc. 3,4
[GSLM∗08] G UTIERREZ D., S ERON F. J., L OPEZ -MORENO J.,
SANCHEZ M. P., F ANDOS J., R EINHARD E.: Depicting procedural
caustics in single images. ACM Trans. on Graphics 27 , 5 (dec 2008).
3
[GWA∗15] G KIOULEKAS I., W ALTER B., A DELSON E. H., B ALA K.,
ZICKLER T.: On the appearance of translucent edges. In Proc. Computer
Vision and Pattern Recognition (CVPR) (June 2015). 2
[GXZ∗13] G KIOULEKAS I., X IAO B., Z HAO S., A DELSON E. H.,
ZICKLER T., B ALA K.: Understanding the role of phase function in
translucent appearance. ACM Trans. on Graphics 32 , 5 (oct 2013). 2
[Hdr] H DRIHAVEN : Hdrihaven. URL: https://www.hdrihaven.com/ .
6
[HFM16] H AVRAN V., F ILIP J., M YSZKOWSKI K.: Perceptually moti-
vated brdf comparison using single image. Computer Graphics Forum
35, 4 (2016), 1–12. 2
[HGC∗20] H UB., G UOJ., C HEN Y., L IM., G UOY.: Deepbrdf: A
deep representation for manipulating measured brdf. Computer Graphics
Forum 39 , 2 (2020), 157–166. 2
[HHD∗22] H UY., H EC., D ESCHAINTRE V., D ORSEY J., R USHMEIER
H.: An inverse procedural modeling pipeline for svbrdf maps .ACM
Trans. on Graphics 41 , 2 (jan 2022). 2
[HLM06] H OY.-X., L ANDY M. S., M ALONEY L. T.: How direction
of illumination affects visually perceived surface roughn ess. Journal of
Vision 6 , 5 (04 2006), 8–8. 2
[HMP∗17] H IGGINS I., M ATTHEY L., P ALA., B URGESS C., G LOROT
X., B OTVINICK M., M OHAMED S., L ERCHNER A.: beta-vae: Learning
basic visual concepts with a constrained variational frame work. In ICLR
(Poster) (2017). 3
[HZK∗19] H EZ., Z UOW., K ANM., S HAN S., C HEN X.: Attgan: Facial
attribute editing by only changing what you want. IEEE Transactions on
Image Processing 28 , 11 (Nov 2019), 5464–5478. 2,3,4
[Jak10] J AKOB W.: Mitsuba renderer, 2010. URL:
http://www.mitsuba-renderer.org .4
[JSR∗22] J AKOB W., S PEIERER S., R OUSSEL N., N IMIER -
DAVID M., V ICINI D., Z ELTNER T., N ICOLET B., C RESPO
M., L EROY V., Z HANG Z.: Mitsuba 3 renderer, 2022. URL:
https://mitsuba-renderer.org .7
[Kal] K ALEIDO : Removebg. URL: https://www.remove.bg .6
[KALL17] K ARRAS T., A ILAT., L AINE S., L EHTINEN J.: Progressive
growing of gans for improved quality, stability, and variat ion, 2017. 8
[KB14] K INGMA D. P., B AJ.: Adam: A method for stochastic optimiza-
tion. arXiv preprint arXiv:1412.6980 (2014). 6
[KFB10] K ˇRIVÁNEK J., F ERWERDA J. A., B ALA K.: Effects of global
illumination approximations on material appearance. ACM Trans. on
Graphics 29 , 4 (jul 2010). 2
[KRFB06] K HAN E. A., R EINHARD E., F LEMING R. W., B ÜLTHOFF
H. H.: Image-based material editing. ACM Trans. on Graphics 25 , 3 (jul
2006), 654–663. 3
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
[KW13] K INGMA D. P., W ELLING M.: Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114 (2013). 3
[LAD∗21] L ACAMBRA J. C., A RTO M. L., D ELANOY J., C ORCOY
B. M., G UTIÉRREZ D., ET AL .: Normal map estimation in the wild.
Jornada de Jóvenes Investigadores del I3A 9 (2021). 7
[LBAD∗06] L AWRENCE J., B EN-ARTZI A., D ECORO C., M ATUSIK
W., P FISTER H., R AMAMOORTHI R., R USINKIEWICZ S.: Inverse shade
trees for non-parametric material representation and edit ing.ACM Trans.
on Graphics 25 , 3 (jul 2006), 735–745. 2
[LCY∗17] L IUG., C EYLAN D., Y UMER E., Y ANG J., L IENJ.-M.: Ma-
terial editing using a physically based rendering network. InProc. Inter-
national Conference on Computer Vision (ICCV) (2017), pp. 2280–2288.
3
[LDX∗19] L IUM., D ING Y., X IAM., L IUX., D ING E., Z UOW.,
WENS.: Stgan: A uniﬁed selective transfer network for arbitrar y im-
age attribute editing. In Proc. Computer Vision and Pattern Recognition
(CVPR) (June 2019). 2,3,6
[LMJH∗10] L OPEZ -MORENO J., J IMENEZ J., H ADAP S., R EINHARD
E., A NJYO K., G UTIERREZ D.: Stylized depiction of images based
on depth perception. In Proc. International Symposium on Non-
Photorealistic Animation and Rendering (NPAR) (New York, NY , USA,
2010), NPAR ’10, Association for Computing Machinery, p. 10 9–118. 3
[LMS∗19] L AGUNAS M., M ALPICA S., S ERRANO A., G ARCES E.,
GUTIERREZ D., M ASIA B.: A similarity measure for material appear-
ance. ACM Trans. Graph. 38 , 4 (jul 2019). 2,4
[LSGM21] L AGUNAS M., S ERRANO A., G UTIERREZ D., M ASIA B.:
The joint role of geometry and illumination on material reco gnition.
Journal of Vision 21 , 2 (02 2021), 2–2. 2,8
[LZU∗17] L AMPLE G., Z EGHIDOUR N., U SUNIER N., B ORDES A.,
DENOYER L., R ANZATO M. A.: Fader networks:manipulating im-
ages by sliding attributes. In Advances in Neural Information Processing
Systems (2017), Guyon I., Luxburg U. V ., Bengio S., Wallach H., Fergu s
R., Vishwanathan S., Garnett R., (Eds.), vol. 30, Curran Ass ociates, Inc.
2,3,13
[MB10] M ALONEY L. T., B RAINARD D. H.: Color and material percep-
tion: Achievements and challenges. Journal of Vision 10 , 9 (12 2010),
19–19. 2
[MGZ∗17] M YLO M., G IESEL M., Z AIDI Q., H ULLIN M., K LEIN R.:
Appearance bending: A perceptual editing paradigm for data -driven ma-
terial models. In Proceedings of the Conference on Vision, Modeling and
Visualization (Goslar, DEU, 2017), VMV ’17, Eurographics Association,
p. 9–16. 2
[MLMG19] M AOR., L AGUNAS M., M ASIA B., G UTIERREZ D.: The
effect of motion on the perception of material appearance. I nACM Sym-
posium on Applied Perception (New York, NY , USA, 2019), SAP ’19,
Association for Computing Machinery. 2
[MLTFR19] M AXIMOV M., L EAL-TAIXE L., F RITZ M., R ITSCHEL T.:
Deep appearance maps. In Proc. International Conference on Computer
Vision (ICCV) (October 2019). 3
[MPBM03] M ATUSIK W., P FISTER H., B RAND M., M CMILLAN L.: A
data-driven reﬂectance model. ACM Trans. on Graphics 22 , 3 (jul 2003),
759–769. 4,7
[MST∗20] M ILDENHALL B., S RINIVASAN P. P., T ANCIK M., B ARRON
J. T., R AMAMOORTHI R., N GR.: Nerf: Representing scenes as neural
radiance ﬁelds for view synthesis. In Computer Vision – ECCV 2020
(Cham, 2020), Vedaldi A., Bischof H., Brox T., Frahm J.-M., ( Eds.),
Springer International Publishing, pp. 405–421. 2
[NDM06] N GAN A., D URAND F., M ATUSIK W.: Image-driven Naviga-
tion of Analytical BRDF Models. In Symposium on Rendering (2006),
Akenine-Moeller T., Heidrich W., (Eds.), The Eurographics Association.
4
[NJR15] N IELSEN J. B., J ENSEN H. W., R AMAMOORTHI R.: On opti-
mal, minimal brdf sampling for reﬂectance acquisition. ACM Trans. on
Graphics 34 , 6 (oct 2015). 2[NOGRR21] N IEVES J. L., O JEDA J., G ÓMEZ -ROBLEDO L., R OMERO
J.: Psychophysical determination of the relevant colours t hat describe
the colour palette of paintings. Journal of Imaging 7 , 4 (Apr 2021), 72.
2
[PFG00] P ELLACINI F., F ERWERDA J. A., G REENBERG D. P.: To-
ward a psychophysically-based light reﬂection model for im age synthe-
sis. In Proceedings of the 27th Annual Conference on Computer Graph -
ics and Interactive Techniques (USA, 2000), SIGGRAPH ’00, ACM
Press/Addison-Wesley Publishing Co., p. 55–64. 2
[PGM∗19] P ASZKE A., G ROSS S., M ASSA F., L ERER A., B RADBURY
J., C HANAN G., K ILLEEN T., L INZ., G IMELSHEIN N., A NTIGA L.,
DESMAISON A., K OPF A., Y ANG E., D EVITOZ., R AISON M., T E-
JANI A., C HILAMKURTHY S., S TEINER B., F ANG L., B AIJ., C HIN-
TALA S.: Pytorch: An imperative style, high-performance deep le arning
library. In Advances in Neural Information Processing Systems (2019),
Wallach H., Larochelle H., Beygelzimer A., d'Alché-Buc F., Fox E., Gar-
nett R., (Eds.), vol. 32, Curran Associates, Inc. 6
[PL07] P ELLACINI F., L AWRENCE J.: Appwand: Editing measured ma-
terials using appearance-driven optimization. In ACM SIGGRAPH 2007
Papers (New York, NY , USA, 2007), SIGGRAPH ’07, Association for
Computing Machinery, p. 54–es. 3
[RFB15] R ONNEBERGER O., F ISCHER P., B ROX T.: U-net: Convolu-
tional networks for biomedical image segmentation. In International
Conference on Medical image computing and computer-assist ed inter-
vention (2015), Springer, pp. 234–241. 3
[RRF∗16] R EMATAS K., R ITSCHEL T., F RITZ M., G AVVES E.,
TUYTELAARS T.: Deep reﬂectance maps. In Proc. Computer Vision
and Pattern Recognition (CVPR) (2016). 3
[SAF21] S TORRS K. R., A NDERSON B. L., F LEMING R. W.: Unsu-
pervised learning predicts human perception and mispercep tion of gloss.
Nature Human Behaviour 5 , 10 (Oct 2021), 1402–1417. 2
[SCW∗21] S ERRANO A., C HEN B., W ANG C., P IOVARCI M., S EIDEL
H.-P., D IDYK P., M YSZKOWSKI K.: The effect of shape and illumi-
nation on material perception: model and applications. ACM Trans. on
Graph. (2021). 7
[SDZ∗21] S RINIVASAN P. P., D ENG B., Z HANG X., T ANCIK M.,
MILDENHALL B., B ARRON J. T.: Nerv: Neural reﬂectance and visi-
bility ﬁelds for relighting and view synthesis. In Proc. Computer Vision
and Pattern Recognition (CVPR) (June 2021), pp. 7495–7504. 2
[SFV20] S CHMIDT F., F LEMING R. W., V ALSECCHI M.: Softness and
weight from shape: Material properties inferred from local shape fea-
tures. Journal of Vision 20 , 6 (06 2020), 2–2. 2
[SGM∗16] S ERRANO A., G UTIERREZ D., M YSZKOWSKI K., S EIDEL
H.-P., M ASIA B.: An intuitive control space for material appearance.
ACM Trans. on Graphics 35 , 6 (nov 2016). 2
[TFCRS11] T HOMPSON W., F LEMING R., C REEM -REGEHR S., S TE-
FANUCCI J. K.: Visual perception from a computer graphics perspective .
CRC press, 2011. 2
[VLD07] V ANGORP P., L AURIJSSEN J., D UTRÉ P.: The inﬂuence of
shape on the perception of material reﬂectance. In ACM SIGGRAPH
2007 Papers (New York, NY , USA, 2007), SIGGRAPH ’07, Association
for Computing Machinery, p. 77–es. 2
[WAKB09] W ILLS J., A GARWAL S., K RIEGMAN D., B ELONGIE S.:
Toward a perceptual space for gloss. ACM Trans. on Graphics 28 , 4
(sep 2009). 2,4
[XWT∗08] X UEY S., W ANG J., T ONG X., D AIQ., G UOB.: Image-
based Material Weathering. Computer Graphics Forum (2008). 3
[XZG∗20] X IAOB., Z HAO S., G KIOULEKAS I., B IW., B ALA K.: Ef-
fect of geometric sharpness on translucent material percep tion. Journal
of Vision 20 , 7 (07 2020), 10–10. 2
[YS19] Y UY., S MITH W. A. P.: Inverserendernet: Learning single image
inverse rendering. In Proc. Computer Vision and Pattern Recognition
(CVPR) (June 2019). 2
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
[YX16] Y ANG J., X IAOS.: An inverse rendering approach for heteroge-
neous translucent materials. In Proceedings of the 15th ACM SIGGRAPH
Conference on Virtual-Reality Continuum and Its Applicati ons in Indus-
try - Volume 1 (New York, NY , USA, 2016), VRCAI ’16, Association for
Computing Machinery, p. 79–88. 2
[ZFWW20] Z SOLNAI -FEHÉR K., W ONKA P., W IMMER M.: Photore-
alistic material editing through direct image manipulatio n.Computer
Graphics Forum 39 , 4 (2020), 107–120. 2
[ZSD∗21] Z HANG X., S RINIVASAN P. P., D ENG B., D EBEVEC P.,
FREEMAN W. T., B ARRON J. T.: Nerfactor: Neural factorization of
shape and reﬂectance under an unknown illumination. ACM Trans. on
Graphics 40 , 6 (dec 2021). 2
[ZZW∗21] Z HENG C., Z HENG R., W ANG R., Z HAO S., B AOH.: A
compact representation of measured brdfs using neural proc esses. ACM
Trans. on Graphics 41 , 2 (nov 2021). 2
Appendix A: STU Details
The STU cells are a variant of the GRU [ CvMG∗14,CGCB14 ]
model and allow encoder-decoder Convolutional Neural Netw orks
(CNNs) to retain the relevant information over a long period . Let’s
say we want to send the feature map of the lthencoder layer de-
noted by flencto the lthlayer of the decoder. Given a single STU,
as is shown in Figure 2, the hidden state ˆsl+1holds the information
from the previous STU cell of the layer l+1. This input is used to
remove information from flencan generate the output feature map
fltas is shown in Figure 2. The hidden state slof the cell is calcu-
lated and sent to the next layer l−1. The hidden state ˆsl+1also has
information of the target attribute value:
ˆsl+1=Wt∗U/bracketleftBig
sl+1,attt/bracketrightBig
. (10)
Where∗Uand[·,·]denote the upsampling operation followed
by a convolution operation, and the concatenation operator respec-
tively. The update gate uhelps the cell to determine how much of
the past information (from the previous cell) needs to be pas sed
along to the future, while reset gate ris used to decide how much
of the past information to forget. These tensors are compute d as:
ul=σ/parenleftBig
Wu∗/bracketleftBig
fenc,ˆsl+1/bracketrightBig/parenrightBig
, (11)
rl=σ/parenleftBig
Wr∗/bracketleftBig
fenc,ˆsl+1/bracketrightBig/parenrightBig
. (12)
The sigmoid activation function σ(·)is applied to normalize the
result between 0 and 1. WuandWrare the weights matrices up-
dated during training. The hidden state sluses rlto store the rele-
vant information from the past and is calculated as follows:
sl=rl◦ˆsl+1. (13)
Where◦expresses the Hadamard product. This operation be-
tween rlandˆsl+1, determines what to remove from the previous
cell. To compute flt; ﬁrst, nonlinearity is introduced in the form of
tanh to ensure that the values in the candidate feature map ˆfltremain
in the interval [−1,1]:
Figure 15: Architecture of our generator G and discriminator D.
BN, IN and FC denote Batch Normalization, Instance Normaliz a-
tion and Fully Connected layer respectively.
ˆfl
t=tanh/parenleftBig
Wh∗/bracketleftBig
fenc,sl/bracketrightBig/parenrightBig
. (14)
Finally, the update gate uis needed to determine what to col-
lect from the candidate feature map ˆfltandˆsl+1, so we compute the
Hadamard product as with Equations 11and12, last, the result is
convolved by the weights Wh:
fl
t=/parenleftBig
1−ul/parenrightBig
◦ˆsl+1+ul◦ˆfl
t. (15)
Appendix B: Additional Details of Our Architecture
Our framework is composed of an encoder-decoder network Gand
the auxiliary attribute predictor and image discriminator Donly
used during training. The generator Gis composed of an encoder
Gencmade of 5 convolutional layers that reduce the spatial dimen -
sions of the input image by a factor of two, and a decoder Gdec
composed of 5 convolutional layers, scaling the input featu re of
each layer by an upsampling operation. The architecture of t he dis-
criminator Dis similar to Genc(5 convolutional layers), but both
DadvandDattapply full connected layers to output their predic-
tions, as is shown in Figure 15.
Trade-off Parameters Generative models are highly unstable dur-
ing training, selecting the correct trade-off parameters i s crucial.
We have observed that the decoder’s attribute manipulation loss
LDattdecreases rapidly compared to its adversarial loss functio n
LDadv. As we see in Figure 16increasing λ2improves the editing
ability of our framework. On the other hand, the decoder appl ies
several transpose convolutions operations to reconstruct the target
image. Unfortunately, this architecture may introduce som e arti-
facts in the edited image. We avoid using transpose convolut ions in
our framework since they produce artifacts on the ﬁnal image as is
shown in Figure 16.
Comparison with the State of the Art The number of trainable
parameters is an important factor when a deep learning model is be-
ing designed because a large number of parameters involves u sing
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.Subias & Lagunas / In-the-wild Material Appearance Editing using Perceptual Attributes
Trans Conv 
 US + Conv US + Conv Trans Conv Input
Glossy +
Figure 16: Ablation studies where we train four versions of our
framework. Arrow (pointing up) indicates we set the target a t-
tribute glossy atttto 1. From left to right: edited synthetic image
by our framework using transposed convolutions and trainin g with
λ2=10, upsampling followed by a convolutional layer training
with λ2=10, transposed convolutions, and training with λ2=50
and upsampling followed by a convolutional layer training w ith
λ2=50.
a lot of memory to train the models and, often, resources are s carce.
The work from Delanoy et al. [ DLC∗22] propose a framework
composed of two generators Giwith i∈ {1,2}based on a Fader
Network [ LZU∗17] architecture. G1compress the low-resolution
input image of the single object in a latent code z1.G2replicates
this behavior but takes high-resolution images as input to g enerate
its latent code z2. Since both latent codes, zimust not contain per-
ceptual information of the high-level perceptual attribut ea, during
training both generators Giplay an adversarial game with a latent
discriminator LDi. Also, the authors introduce an image discrim-
inator Dthat plays an adversarial game with G2to enhance the
quality of edited images. Table 2shows the trainable parameters of
the whole framework.
Our framework is composed of one generator Gand one dis-
criminator D, while the previous approach needs two generators Gi
helped by other two latent discriminators LD iand one image dis-
criminator C/Dto improve the editing ability. In Table 3we give
the number of total trainable parameters of our framework. S ince
our framework is simpler than the previous one, it has fewer t rain-
able parameters, so we reduce notably memory usage by reduci ng
the trainable parameters from 58 920 489 parameters to 33 326 314
and thus saving 43% of memory.
Table 2: Number of trainable parameters per module for the pre-
vious method.
Module Trainable Parameters
G1 20 356 515
LD1 4 326 401
G2 4 610 179
LD2 14 813 697
C/D 14 813 697
Total Parameters 58 920 489Table 3: Number of trainable parameters per module for our
framework.
Module Trainable Parameters
G(Genc+Gdec+Gst) 13 758 280
D(both DattandDadv) 19 568 034
Total Parameters 33 326 314
© 2023 The Author(s)
Computer Graphics Forum © 2023 The Eurographics Associatio n and John Wiley & Sons Ltd.