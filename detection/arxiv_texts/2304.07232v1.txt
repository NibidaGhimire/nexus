1
Technical Report: Evaluation of ChatGPT Model
for Vulnerability Detection
Anton Cheshkov, Pavel Zadorozhny, Rodion Levichev
Abstract ‚ÄîIn this technical report, we evaluated the perfor-
mance of the ChatGPT and GPT-3 models for the task of
vulnerability detection in code. Our evaluation was conducted on
our real-world dataset, using binary and multi-label classiÔ¨Åcation
tasks on CWE vulnerabilities. We decided to evaluate the model
because it has shown good performance on other code-based
tasks, such as solving programming challenges and understand-
ing code at a high level. However, we found that the ChatGPT
model performed no better than a dummy classiÔ¨Åer for both
binary and multi-label classiÔ¨Åcation tasks for code vulnerability
detection.
Keywords: GPT models, ChatGPT, Vulnerability Detection,
Large Language Models, OpenAI, GPT-4
I. I NTRODUCTION
This technical report evaluates the performance of the Chat-
GPT and GPT-3 models for the task of Java code vulnerability
detection. The report presents a preliminary study based on
our dataset from GitHub of patch-level Java code containing
both vulnerable and patched Ô¨Åles. We used the API provided
by OpenAI to send requests to GPT models for vulnerability
detection. The purpose of this report is to answer the question
of whether large language models such as ChatGPT and
GPT-3 can solve the problem of vulnerability detection. The
report includes a detailed description of the dataset collection,
data preprocessing, and evaluation methodology. Finally, we
provide an analysis of the results and recommendations for
future research.
II. P ROBLEM DEFINITION
In previous studies, GPT has been demonstrated to possess
the ability to understand and generate code. The model‚Äôs
proÔ¨Åciency in coding has been tested on various coding chal-
lenges, such as HumanEval and LeetCode, where it achieved
remarkable results, outperforming other LLMs (Large Lan-
guage Models) and being comparable to human performance
(Bubeck et al. (2023)). Given these Ô¨Åndings, we aim to
investigate whether the same model can also be employed for
other tasks, such as vulnerability detection.
The purpose of this report is to answer the question: can
large language models such as ChatGPT solve the problem
of vulnerability detection?
III. R ELATED WORK
In recent years, there have been several studies conducted
on the evaluation and capabilities of large language models for
code generation and analysis. Some of the studies that have
informed our research include:In the technical report "Sparks of ArtiÔ¨Åcial General Intelli-
gence: Early experiments with GPT-4" Bubeck et al. (2023),
the authors compared the capabilities of GPT-4, GPT-3, and
GPT-3-5 in understanding and generating code. The report
demonstrated the proÔ¨Åciency of these models in various coding
challenges, such as HumanEval and LeetCode, where they
achieved remarkable results that were comparable to human
performance. The models have also shown their ability to write
code from high-level instructions, as demonstrated by the zero-
shot creation of a 3D game using HTML and JavaScript, as
well as the implementation of a custom optimizer module for
deep learning applications.
In addition to code generation, the models exhibited a high
level of understanding of existing code, written by others.
This includes the ability to reason about code execution, as
evidenced by their correct explanation of the output of a C
program involving the size of two structures. Furthermore,
the models demonstrated the ability to execute non-trivial
Python code, simulating the code in natural language while
keeping track of several variables, nested loops, dictionaries,
and recursion.
The GPT series has also been evaluated on various tasks
such as summarization, sentiment analysis, question answer-
ing, among others, and has shown outstanding performance on
these tasks (Brown et al. (2020); Yoo et al. (2021)).
In the study "Evaluating Large Language Models Trained on
Code" (Chen et al. (2021)), the authors investigated the perfor-
mance of large language models, such as GPT-3, when trained
on code. They explored the model‚Äôs ability to understand and
generate code, offering valuable insights into the potential of
using such models for code-related tasks. According to their
experiments, Codex demonstrates proÔ¨Åciency in generating
certain types of code components but struggles with others,
such as SQL and shell injection payloads. The study Ô¨Ånds
that Codex underperforms compared to rudimentary Static
Application Security Testing (SAST) tools in vulnerability
discovery. As model capabilities improve, further research
is necessary to understand the potential for large language
models to excel at discovering high-dimension vulnerabilities,
such as "business logic" vulnerabilities.
In "A Comprehensive Capability Analysis of GPT-3 and
GPT-3.5 Series Models" (Ye et al. (2023)), the authors pro-
vided a detailed analysis of the capabilities of GPT-3 and
GPT-3.5 series models. They covered various tasks, including
summarization, sentiment analysis, and question-answering,
presenting an extensive understanding of the models‚Äô strengths
and weaknesses.arXiv:2304.07232v1  [cs.CR]  12 Apr 20232
IV. D ATASET COLLECTION
We collected a dataset of Java Ô¨Åles containing both vulner-
able and patched code from open GitHub repositories, which
were sourced based on their commit history. The vulnerable
code was selected manually by domain experts and was known
to contain vulnerabilities. The patched code was obtained by
applying a patch to the vulnerable code. The distribution
of CWE types of vulnerabilities is given in the Table A
of appendix. The table below represents only a portion of
the repositories used in our dataset, and a sample of the
projects and the number of vulnerabilities is provided in the
appendix Table B. The appendix Figure 1 shows function size
distribution of the collected dataset.
V. D ATA PREPROCESSING
A. Binary ClassiÔ¨Åcation
We extracted a subset of Ô¨Åles from the dataset, speciÔ¨Åcally
those in which a single function was modiÔ¨Åed. We then
introduced a function containing a known vulnerability and
its associated patched version into the subset, resulting in a
dataset consisting of a total of 308 samples.
B. Multi-label ClassiÔ¨Åcation
The dataset consists of a certain number of Common Weak-
ness Enumeration (CWE) vulnerability types, which are used
for multi-label classiÔ¨Åcation. From this dataset, the top Ô¨Åve
vulnerabilities (CWE-20, CWE-200, CWE-502, CWE-611,
CWE-79) were selected based on their frequency, resulting
in a total of 60 samples with vulnerabilities and 60 patched
samples of these functions (Negative). We used the list of
prompts presented in Table C of appendix.
VI. U SING MODELS API
In the study, two language models were used: the Davinci
model, which is considered one of the best and most advanced
models within the GPT-3 family, and the GPT-3.5-turbo model,
which is a speciÔ¨Åc implementation of the GPT-3 architecture
designed for chatbot applications. We used the API provided
by OpenAI to send requests to GPT models for each Java
function extracted during the data preprocessing step. These
prompts were designed to ask whether the Java code contained
a vulnerability, with the goal of generating a "Yes" or "No"
response from both GPT-3 and ChatGPT. It should be noted
that the ChatGPT API may differ from the model provided in
the web version.
VII. E VALUATING METHODOLOGY
In our evaluation methodology, we performed the follow-
ing actions: For binary classiÔ¨Åcation, we collected a dataset
containing 308 samples, including vulnerable and patched
Java functions. For multi-label classiÔ¨Åcation, we compiled a
dataset of 120 samples, with 60 vulnerable functions and 60
patched samples, focusing on the top Ô¨Åve Common Weakness
Enumeration (CWE) vulnerabilities. We then queried the mod-
els using the API with appropriate prompts. After obtaining
the results, we calculated relevant metrics and compared the
models‚Äô performance to a baseline model.Binary
Model Precision Recall F1-Score AUC
text-davinci-003 0.50 0.99 0.67 0.51
gpt-3.5-turbo 0.51 0.80 0.62 0.51
baseline 0.50 0.50 0.50 0.50
TABLE I
COMPARING ‚ÄôTEXT -DAVINCI -003‚Äô, ‚Äô GPT-3.5- TURBO ‚ÄôAND BASELINE
MODELS ON BINARY CLASSIFICATION TASK WITH 308 SAMPLES .
A. Metrics
To evaluate the performance of both GPT-3 and ChatGPT
models for vulnerability detection, we used the following
metrics:
Accuracy: The percentage of correctly classiÔ¨Åed samples
out of the total samples.
Precision: The percentage of true positives among the
predicted positive samples.
Recall: The percentage of true positives among the actual
positive samples.
F1 Score: The harmonic mean of precision and recall,
providing a balanced metric to evaluate the model‚Äôs
performance.
AUC: A measure of the performance of a binary classiÔ¨Å-
cation model, represented by the area under the Receiver
Operating Characteristic (ROC) curve.
B. Adjusting Model Parameters
In the evaluation process, we modiÔ¨Åed the temperature
parameter in GPT-3 and ChatGPT models to decrease output
randomness, while keeping other parameters at their default
values. Although the temperature had a minimal effect on
the responses, we set it to 0 to ensure more focused and
deterministic outputs.
C. Model Comparison
We compared three models, namely GPT-3, ChatGPT, and
a baseline model, across binary and multi-label classiÔ¨Åcation
tasks to evaluate their performance in detecting vulnerabilities
in Java code.
D. Baseline Model
In order to provide context for the results, we compared
GPT-3 and ChatGPT‚Äôs performance to a baseline dummy
classiÔ¨Åer, which draws labels according to its distribution in
the dataset.
We averaged the results of 1,000 runs of the dummy
classiÔ¨Åer. We do this to mitigate random Ô¨Çuctuations due to the
relatevely small size of the dataset and obtain a more accurate
performance estimate.
VIII. E VALUATION RESULTS
A. Binary classiÔ¨Åcation
Initially, we evaluated the model‚Äôs performance on a binary
classiÔ¨Åcation task, the dataset consisting of a total of 3083
Multi
CWE type Precision Recall F1-Score support
CWE-20 0.333 0.071 0.118 14
CWE-200 0.000 0.000 0.000 9
CWE-502 0.200 0.143 0.167 7
CWE-611 0.083 0.077 0.080 13
CWE-79 0.333 0.176 0.231 17
Negative 0.440 0.667 0.530 60
accuracy 0.383 120
macro avg 0.232 0.189 0.187 120
weighted avg 0.327 0.383 0.330 120
TABLE II
PERFORMANCE EVALUATION OF THE ‚ÄôGPT-3.5- TURBO ‚Äô M ODEL ON 5
CWE T YPES USING 120 S AMPLES
samples. The Table I compares the performance of three
models on a binary classiÔ¨Åcation task using 308 samples. The
models evaluated are ‚Äôtext-davinci-003‚Äô, ‚Äôgpt-3.5-turbo‚Äô, and
baseline. The evaluation metrics used are precision, recall, F1-
score, and AUC.
Despite GPT models have higher recall and F1 scores
compared to the baseline model1, the AUC score is not better
than random guess model.
This observation indicates that GPT models are not able to
distinct vulnerable code and its Ô¨Åxed non-vulnerable version.
This models tend to label examples as positive that leads to a
high false positive rate.
Also we used different prompts to ensure that our results
were not inÔ¨Çuenced by a speciÔ¨Åc prompt. List of Vulnerability
Detection Prompts for binary classiÔ¨Åcation task:
write whether the vulnerability is contained in the code
in the Yes/no format:
does the following function contain a vulnerability write
in Yes/no format and why?:
tell whether this function is vulnerable in the Yes/no
format:
write whether this function is vulnerable:
We conducted an experiment using different prompts, and
our Ô¨Åndings indicate that the scores remain consistent across
the various prompts, showing no signiÔ¨Åcant difference in
performance.
B. Multi-label classiÔ¨Åcation
To further evaluate the model‚Äôs capabilities, we selected the
top Ô¨Åve CWE vulnerability types based on their frequency
in the dataset (CWE-20, CWE-200, CWE-502, CWE-611,
CWE-79). We used this subset of the data to perform multi
classiÔ¨Åcation task. The results show (Table II, III, IV) that
the ChatGPT and GPT-3 models achieved an accuracy of 38%.
The tables include precision, recall, F1-score, and support for
each of the Ô¨Åve CWE types as well as accuracy, macro average,
and weighted average metrics.
In the sample, the negative class (representing patched
non-vulnerable functions) was predominant, and the models
exhibited a higher accuracy by predicting this class. However,
1The constant dummy classifer shows F1 score about 0:67for the balanced
sampleMulti
CWE type Precision Recall F1-Score support
CWE-20 0.333 0.071 0.118 14
CWE-200 0.000 0.000 0.000 9
CWE-502 0.333 0.143 0.200 7
CWE-611 0.118 0.154 0.133 13
CWE-79 0.273 0.176 0.214 17
Negative 0.442 0.633 0.521 60
accuracy 0.375 120
macro avg 0.250 0.196 0.198 120
weighted avg 0.331 0.375 0.330 120
TABLE III
PERFORMANCE EVALUATION OF THE ‚ÄôTEXT -DAVINCI -003‚Äô M ODEL ON 5
CWE T YPES USING 120 S AMPLES
Multi
CWE type Precision Recall F1-Score support
CWE-20 0.118 0.118 0.116 14
CWE-200 0.081 0.080 0.079 9
CWE-502 0.057 0.056 0.055 7
CWE-611 0.108 0.107 0.105 13
CWE-79 0.138 0.140 0.137 17
Negative 0.500 0.500 0.499 60
accuracy 0.305 120
macro avg 0.167 0.167 0.165 120
weighted avg 0.304 0.305 0.303 120
TABLE IV
PERFORMANCE EVALUATION OF THE BASELINE MODEL ON 5 CWE
TYPES USING 120 S AMPLES
for a more objective assessment of model‚Äôs ability to detect
vulnerable functions, we excluded negative examples from
the sample (Table V, VI, VII). Surprisingly, the models‚Äô
accuracy was lower after excluding the Negative class, indi-
cating that they struggled with correctly predicting vulnerable
functions.
For the multi-label classiÔ¨Åcation task, we used the following
prompt: "Which of these types of vulnerabilities is contained in
the code? Response in json format: "type": "CWE-502" Where
instead of "CWE-502" there can be any of the options: CWE-
502, CWE-79, CWE-20, CWE-611, CWE-200, Negative". We
requested the response to be in JSON format for ease of
parsing and to prevent unclear model responses. The appendix
Table C shows other variants of prompts that we used for this
task.
We observed an interesting anomaly: the ChatGPT predicts
a vulnerability type more accurately for Ô¨Åxed functions. An-
other words, if we assigned the same labels to Ô¨Åxed functions
as it was for its vulnerable countepart, the ChatGPT would
predict these labels about two times more accurate. Table
VIII shows accuracy for two groups across experiments with
different prompts.
The accuracy difference could be explained if the model
being trained on code with already Ô¨Åxed functions, or it could
be a random effect due to the small sample size.4
Multi
CWE type Precision Recall F1-Score support
CWE-20 1.000 0.071 0.133 14
CWE-200 0.000 0.000 0.000 9
CWE-502 1.000 0.143 0.250 7
CWE-611 0.250 0.077 0.118 13
CWE-79 1.000 0.176 0.300 17
accuracy 0.100 60
macro avg 0.542 0.078 0.133 60
weighted avg 0.688 0.100 0.171 60
TABLE V
PERFORMANCE EVALUATION OF THE ‚ÄôGPT-3.5- TURBO ‚Äô M ODEL ON 5
CWE T YPES USING 60 S AMPLES
Multi
CWE type Precision Recall F1-Score support
CWE-20 1.000 0.071 0.133 14
CWE-200 0.000 0.000 0.000 9
CWE-502 1.000 0.143 0.250 7
CWE-611 0.286 0.154 0.200 13
CWE-79 1.000 0.176 0.300 17
accuracy 0.117 60
macro avg 0.548 0.091 0.147 60
weighted avg 0.695 0.117 0.189 60
TABLE VI
PERFORMANCE EVALUATION OF THE ‚ÄôTEXT -DAVINCI -003‚Äô M ODEL ON 5
CWE T YPES USING 60 S AMPLES
Multi
CWE type Precision Recall F1-Score support
CWE-20 0.238 0.239 0.236 14
CWE-200 0.150 0.147 0.145 9
CWE-502 0.112 0.112 0.109 7
CWE-611 0.215 0.214 0.211 13
CWE-79 0.282 0.283 0.280 17
accuracy 0.218 60
macro avg 0.199 0.199 0.196 60
weighted avg 0.218 0.218 0.214 60
TABLE VII
PERFORMANCE EVALUATION OF THE DUMMY CLASSIFIER ON 5 CWE
TYPES USING 60 S AMPLESgpt-3.5(0)
gpt-3.5(1)
gpt-3.5(2)
gpt-3.5(3)
g1 7 6 7 6
g2 14 14 12 12
TABLE VIII
ACCURACY DIFFERENCE FOR VULNERABLE AND NON -VULNERABLE
GROUPS .
WHEREg1- ACCURACY ACROSS VULNERABLE FUNCTIONS (60
EXAMPLES ),g2- ACCURACY ACROSS NON -VULNERABLE CONTERPARTS
(60 EXAMPLES )IX. T HREATS TO VALIDITY
A. Conclusion validity
The conclusion validity of the study may be threatened by
the small sample size used in the experiment, which may limit
the statistical power of the analysis.
B. Construct validity
The construct validity of the study may be threatened by
the use of a single GPT model to detect vulnerabilities in Java
code. The model may have inherent limitations and biases
that may affect its ability to accurately detect vulnerabilities,
and the results may not be generalizable to other models or
approaches to vulnerability detection.
One more of the factors that may affect the validity of
the study is the choice of prompt for binary and multi-
label classiÔ¨Åcation. For example, in our case, for multi-label
classiÔ¨Åcation, we used prompt that asked the model about a
speciÔ¨Åc type of vulnerability. However, there are other ways to
classify vulnerabilities. The model may not be well integrated
into these types of vulnerabilities. Perhaps a more careful
selection of prompts could give different results.
Another factor that may inÔ¨Çuence the construct validity is
the model‚Äôs ability to generate a chain of thought (Wei et al.
(2022)), which refers to a series of intermediate reasoning
steps. Generating a chain of thought has been shown to sig-
niÔ¨Åcantly improve the reasoning capabilities of large language
models. In our study, we did not explicitly explore the chain-
of-thought prompting technique, which involves providing a
few chain of thought demonstrations as exemplars in prompt-
ing. This approach might have improved the model‚Äôs ability
to detect vulnerabilities more accurately.
C. Internal validity
This study‚Äôs internal validity may be threatened by the
possibility that the GPT model was trained on examples that
contained instances of CWE vulnerabilities but not on data that
demonstrated how vulnerabilities were remediated. Further
training may be necessary to improve the model‚Äôs ability
to accurately distinguish between vulnerable and remediated
functions.
Another factor that may threaten the internal validity of this
study is the manual selection of vulnerable code by domain
experts. Although domain experts can identify known vul-
nerabilities, there may be instances where vulnerabilities are
missed or incorrectly labeled. This could affect the accuracy of
the dataset and the subsequent evaluation of the GPT model‚Äôs
performance. Furthermore, as the dataset is compiled based on
domain expert opinions, it may introduce some bias, which
could also impact the results. To address this threat, future
studies could consider using multiple domain experts to review
and label the data, employ automated tools for vulnerability
identiÔ¨Åcation, or use larger and more diverse datasets.
The subset of Ô¨Åles extracted from the dataset for our experi-
ments was chosen based on a speciÔ¨Åc criterion: only Ô¨Åles with
a single modiÔ¨Åed function were included. This may introduce
a selection bias, as the distribution of vulnerabilities in this5
subset may not reÔ¨Çect the actual distribution of vulnerabilities
in the real world. Therefore, the generalizability of our results
to other datasets and scenarios may be limited. This represents
a potential internal validity threat to our study.
D. External validity
The use of a single dataset may limit the generalizability of
the Ô¨Åndings to other datasets.
X. C ONCLUSION
In conclusion, our experiments with GPT-3 and ChatGPT
models indicate that their current capabilities for effectively
detecting vulnerabilities in code are limited. Although they
have been trained on vast amounts of data, these models did
not achieve high performance on our test data. While natural
language processing models have demonstrated impressive
results in numerous areas, it is evident that their application
in vulnerability detection tasks requires further reÔ¨Ånement and
investigation.
As a possible direction for future research, it may be worth
exploring the impact of the chain-of-thought prompting tech-
nique on the performance of the ChatGPT and GPT-3 models
in vulnerability detection tasks. This technique has been shown
to improve the reasoning abilities of large language models
(Wei et al. (2022)), and our study did not explicitly investigate
it. By providing a few chain-of-thought demonstrations as
exemplars in prompting, we might improve the models‚Äô ability
to detect vulnerabilities more accurately.
We believe there is potential for these models to improve
and contribute to vulnerability detection in the future, with
more targeted research and advancements in the Ô¨Åeld. It would
be worthwhile to explore the efÔ¨Åcacy of upcoming largelanguage models, such as GPT-4, in addressing this task and
to identify ways to enhance their performance in this speciÔ¨Åc
domain.
REFERENCES
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.
Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877‚Äì1901.
S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Jo-
hannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat
Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of
artiÔ¨Åcial general intelligence: Early experiments with gpt-4.
arXiv preprint arXiv:2303.12712 (2023).
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-
rique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021.
Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374 (2021).
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought
prompting elicits reasoning in large language models. arXiv
preprint arXiv:2201.11903 (2022).
Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao,
Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang
Shen, et al. 2023. A Comprehensive Capability Analysis
of GPT-3 and GPT-3.5 Series Models. arXiv preprint
arXiv:2303.10420 (2023).
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee,
and Woomyeong Park. 2021. Gpt3mix: Leveraging large-
scale language models for text augmentation. arXiv preprint
arXiv:2104.08826 (2021).6
Prompt
"Which of these types of vulnerabilities is contained in the code?
1. CWE-502: Deserialization of Untrusted Data
2. CWE-79: Cross-site Scripting
3. CWE-20: Improper Input Validation
4. CWE-611: Improper Restriction of XML External Entity Reference
5. CWE-200: Exposure of Sensitive Information to an Unauthorized Actor
6. There is no vulnerability in the code"
"What type of vulnerability is in the code. Choose only one from the
following options.
1. CWE-502: Deserialization of Untrusted Data
2. CWE-79: Cross-site Scripting
3. CWE-20: Improper Input Validation
4. CWE-611: Improper Restriction of XML External Entity Reference
5. CWE-200: Exposure of Sensitive Information to an Unauthorized Actor
6. There is no vulnerability in the code"
"Which of these types of vulnerabilities is contained in the code?
1. CWE-502: Deserialization of Untrusted Data
2. CWE-79: Cross-site Scripting
3. CWE-20: Improper Input Validation
4. CWE-611: Improper Restriction of XML External Entity Reference
5. CWE-200: Exposure of Sensitive Information to an Unauthorized Actor"
"Which of these types of vulnerabilities is contained in the code?
Response in json format: "type": "CWE-502"
Where instead of "CWE-502" there can be any of the options:
CWE-502, CWE-79, CWE-20, CWE-611, CWE-200, Negative"
TABLE C
MULTI -LABEL PROMPTS
Fig. 1. Function size distribution in the dataset.APPENDIX
CWE Count CWE Count
CWE-538 1 CWE-400 4
CWE-79 17 CWE-862 5
CWE-552 1 CWE-352 2
NVD-CWE-Other 10 CWE-787 2
CWE-74 2 CWE-476 1
NVD-CWE-noinfo 9 CWE-428 1
CWE-264 11 CWE-200 9
CWE-345 2 CWE-863 3
CWE-20 14 CWE-915 1
CWE-22 11 CWE-388 1
CWE-611 13 CWE-203 1
CWE-732 1 CWE-404 2
CWE-502 7 CWE-284 1
CWE-269 1 CWE-384 1
CWE-362 1 CWE-668 2
CWE-601 1 CWE-240 1
CWE-287 4 CWE-399 1
CWE-401 1 CWE-335 1
CWE-835 5 CWE-754 1
CWE-310 1 CWE-755 1
TABLE A
NUMBER OF DIFFERENT CWE
ProjectNumber of
Vulnerabilities
apache‚Äìtomcat 32
jenkinsci‚Äìjenkins 21
apache‚Äìtomcat80 5
FasterXML‚Äìjackson-databind 4
apache‚Äìactivemq 4
theonedev‚Äìonedev 2
apache‚Äìlucene-solr 2
javamelody‚Äìjavamelody 2
opennetworkinglab‚Äìonos 2
pippo-java‚Äìpippo 2
apache‚Äìcordova-android 2
apache‚Äìtomcat55 2
traccar‚Äìtraccar 2
undertow-io‚Äìundertow 2
apache‚Äìstruts 2
apache‚Äìcxf 2
apache‚ÄìÔ¨Çink 1
apache‚Äìpdfbox 1
OWASP‚Äìjson-sanitizer 1
...
TABLE B
PROJECTS AND NUMBER OF VULNERABILITIES USED IN THE DATASET