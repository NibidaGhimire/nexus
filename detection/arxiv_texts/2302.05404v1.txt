arXiv:2302.05404v1  [stat.ML]  10 Feb 2023Minimax Instrumental Variable Regression and L2Convergence
Guarantees without Identiﬁcation or Closedness
Andrew Bennett*1, Nathan Kallus1, Xiaojie Mao2, Whitney Newey3,
Vasilis Syrgkanis4, and Masatoshi Uehara†1
1Cornell University
2Tsinghua University
3Massachusetts Institute of Technology
4Stanford University
February 13, 2023
Abstract
In this paper, we study nonparametric estimation of instrum ental variable (IV) regressions. Recently, many
ﬂexible machine learning methods have been developed for in strumental variable estimation. However, these
methods have at least one of the following limitations: (1) r estricting the IV regression to be uniquely identiﬁed;
(2) only obtaining estimation error rates in terms of pseudo metrics ( e.g.,projected norm) rather than valid metrics
(e.g.,L2norm); or (3) imposing the so-called closedness condition t hat requires a certain conditional expectation
operator to be sufﬁciently smooth. In this paper, we present the ﬁrst method and analysis that can avoid all three
limitations, while still permitting general function appr oximation. Speciﬁcally, we propose a new penalized
minimax estimator that can converge to a ﬁxed IV solution eve n when there are multiple solutions, and we
derive a strong L2error rate for our estimator under lax conditions. Notably, this guarantee only needs a widely-
used source condition and realizability assumptions, but n ot the so-called closedness condition. We argue that
the source condition and the closedness condition are inher ently conﬂicting, so relaxing the latter signiﬁcantly
improves upon the existing literature that requires both co nditions. Our estimator can achieve this improvement
because it builds on a novel formulation of the IV estimation problem as a constrained optimization problem.
1 Introduction
Instrumental variable (IV) estimation is an important prob lem in many applications. Examples include causal infer-
ence ( Angrist and Imbens ,1995 ;Newey and Powell ,2003 ;Deaner ,2018 ;Cui et al. ,2020 ), missing data problems
(Wang et al. ,2014 ;Miao et al. ,2015 ), asset pricing models ( Chen et al. ,2014 ;Christensen ,2017 ;Escanciano et al. ,
2020 ), dynamic discrete choice models ( Kalouptsidi et al. ,2021 ), and reinforcement learning ( Liao et al. ,2021 ;
Uehara et al. ,2021 ).
In this paper, we focus on the estimation of nonparametric IV (NPIV) regression ( Newey and Powell ,2003 ).
This problem involves three sets of variables X,Y, andZthat take values in compact Euclidean sets DX,DY,
andDZ, respectively. In the original IV estimation problem, Xstands for endogenous variables, Ystands for an
outcome variable, and Zstands for exogenous IVs. We deﬁne L2(X),L2(Z)as theL2spaces of functions of X,Z
*Alphabetical order
†Corresponding: mu223@cornell.edu
1respectively, deﬁned in terms of their distributions. We ar e interested in solving the following equation with respect
toh∈L2(Z):
E[Y−h(X)|Z] = 0.
This equation can be alternatively written as Th=r0, wherer0(Z) =E[Y|Z], andT:L2(X)→L2(Z)
is a bounded linear operator that maps every h∈L2(X)toE[h(X)|Z]∈L2(Z). Here both the function r0
and the operator Tare unknown. Instead, we only have access to a set of independ ent and identically distributed
observations D:={Xi,Yi,Zi}n
i=1.
There has been a surge in interest in NPIV regressions. A numb er of classical works have proposed sieve or
kernel-based estimators (e.g., Carrasco et al. ,2007 ;Horowitz ,2011 ;Newey ,2013 ;Newey and Powell ,2003 ;Chen ,
2007 ). However, NPIV estimation is notoriously difﬁcult becaus e it is an ill-posed inverse problem. In particular,
the solution to the NPIV equation Th=r0may not be unique, and even if it is unique, the solution may de pend
on the underlying data distribution discontinuously ( Carrasco et al. ,2007 ). Therefore, existing works typically
assume that the NPIV solution is unique ( Andrews ,2017 ;Newey and Powell ,2003 ). Even if it is not the case, they
restrict the linear operator Tand the NPIV solution ( Florens et al. ,2011 ;Chen ,2021 ). A widely used restriction
is the source condition, which assumes that the IV solution b elongs to a subspace deﬁned by the operator T(e.g.,
Carrasco et al. ,2007 ;Cavalier ,2011 ;Chen and Reiss ,2011 ). Under these conditions, the estimators proposed in
these classic literature can have strong theoretical guara ntees. However, these traditional nonparametric estimato rs
do not allow for the integration of modern, ﬂexible general f unction approximation methods such as neural networks
or tree-based methods.
To overcome this limitation, recent works have proposed var ious algorithms that can accommodate general
function approximation. These algorithms typically emplo y two function classes, HandG. In particular, the
function class His the hypothesis class for the solution to the NPIV equation Th=r0. The function class G, often
referred to as a witness function class or discriminator cla ss, is introduced to witness how much each given function
hviolates the NPIV equation. Then, NPIV estimators are deﬁne d as solutions to a minimax optimization problem
(Lewis and Syrgkanis ,2018 ;Bennett et al. ,2019 ;Dikkala et al. ,2020 ;Liao et al. ,2020 ;Muandet et al. ,2020 ):
argmin
h∈Hmax
g∈GL(h,g)
whereL(h,g)is an objective function mapping from H×G toR.
Although highly ﬂexible, these minimax approaches have sev eral limitations. First, they typically assume that
the solution to the NPIV equation Th=r0is unique. However, this assumption can be easily violated i f the
instrumental variables are not very strong ( Andrews and Stock ,2005 ;Andrews et al. ,2019 ), and they usually do
not hold in proximal causal inference ( Kallus et al. ,2021 ). Secondly, the minimax estimators may not give strong
L2error rate guarantees, and instead only have error rate guar antees in terms of a weaker projected mean squared
error (MSE) ( Dikkala et al. ,2020 ). However, even when the projected MSE vanishes to zero, the minimax estimator
may not converge to any ﬁxed IV solution since the projected M SE is a pseudometric unlike the L2metric. Thirdly,
current minimax estimators typically need some form of clos edness condition, such as Th∈ G for anyh∈ H
(Dikkala et al. ,2020 ;Liao et al. ,2020 ) or other close variant ( Bennett et al. ,2022 ). However, this assumption may
impose stringent restrictions on the operator T, noting that Gmust be a restricted class to ensure bounded statistical
complexity. In particular, the closedness assumption is at odds with the widely used source condition, since we
will show that the closedness assumption is more plausible w hen the spectrum of Tdecays more slowly while the
source condition is more plausible when the spectrum decays more rapidly.
To the best of our knowledge, all current approaches incorpo rating general function approximation for IV prob-
lems suffer from at least one of the three limitations listed above. In this paper, we propose the ﬁrst method that
avoids all three of these limitations. Speciﬁcally, we do no t assume that the NPIV solution is unique, and instead
we target the least norm solution h0. This is a standard approach for inverse problems with non-u nique solutions
2Table 1: Summary of current literature of minimax estimatio n with general function approximation. Our goal is
to solveTh=r0with respect to hwith unknown Tandr0. We denote its set of solutions by H0and the least
norm solution by h0. Estimators are deﬁned as solutions to certain minimax opti mizations minh∈Hmaxg∈GL(h,g)
whereHandGare hypothesis classes. For simplicity, we focus on compari son for VC classes HandG(while the
results in both our and these papers deal with general functi on classes) and for source condition with exponent 1. We
letT∗be the adjoint of T,¯G0={¯g0:T∗¯g0=h0}(nonempty under source condition), h0,α= argminh/bardblTh−
r0/bardbl2
2+α/bardblh/bardbl2
2, andα0>0a positive number. Note our condition is strictly weaker tha n that of Bennett et al.
(2022 ).
Primary assumptions Guarantee Rate
Dikkala et al. (2020 ) realizability H0∩H /ne}ationslash=∅, closedness T H ⊂ G +r0 Projected MSEs n−1/2
Liao et al. (2020 )sourceh0∈ R(T∗T), uniqueness of h0,
realizability h0,α∈ H ∀α≤α0, closedness TH ⊂ G +r0L2rates n−1/6
Bennett et al. (2022 )sourceh0∈ R(T∗T),
realizability h0∈ H,¯G0∩G /ne}ationslash=∅and closedness T∗G ⊂ HL2rates n−1/4
This worksourceh0∈ R(T∗T)
realizability h0∈ H,¯G0∩G /ne}ationslash=∅L2rates n−1/4
(Florens et al. ,2011 ;Babii and Florens ,2017 ;Chen ,2021 ;Bennett et al. ,2022 ). We show that our proposed esti-
mator can converge to the least norm IV solution and derive it sL2error rate guarantee. These theoretical guarantees
only need the fairly standard source condition and realizab ility assumptions ( i.e., well-speciﬁcation of HandQ).
Table 1summarizes the assumptions and guarantees in our paper and r elated ones.
Our proposed estimator and its theory are grounded in the nov el insight that ﬁnding the least norm solution
h0toTh=r0can be viewed as a constrained optimization problem. In part icular, we show that the least norm
solution can be uniquely identiﬁed as a saddle point of the mi nimax optimization of the Lagrangian. Although
previous minimax estimators also leverage minimax optimiz ation, their inner maximization is used to approximate
the projected MSE E[([Th](Z)−r0(Z))2], which necessitates the closedness assumption. In contras t, the inner
maximization in our methods results from the method of Lagra nge multipliers, and it does not need the closedness
assumption. Interestingly, we prove that the source condit ion is the sufﬁcient and necessary condition for the
existence of stationary Lagrange multipliers and thus the s addle point to our minimax optimization problem. This
also reveals a new role of the source condition widely used in inverse problems.
Our paper is organized as follows. In Section 2 , we present our setup of IV estimation and the limitations
of current works in this setting. In Section 3 , we introduce our minimax estimator by framing the problem a s a
constrained optimization problem. In Section 4 , we demonstrate that the minimax optimization identiﬁes th e least
norm solution given inﬁnite data. In Section 5 , we present the ﬁnite-sample error guarantee, i.e., L2convergence
rate. In Section 6 , we compare our estimator and theory to those in closely rela ted works. Finally, we conclude our
paper in Section 7 .
31.1 Related Works
Instrumental variable estimation has received considerab le attention as a subclass of inverse problems, as detailed
in the works of Carrasco et al. (2007 );Cavalier (2011 );Newey (2013 );Ito and Jin (2014 ).
Even when the operator Tand response r0are known, nonparametric instrumental variable estimatio n poses
signiﬁcant difﬁculties due to its ill-posed nature. The ill -posedness often refers to the presence of one or more
of the following characteristics: (1) the absence of soluti ons, (2) the existence of multiple solutions, and (3) the
discontinuity of the inverse of T. To address these challenges, various regularization tech niques have been pro-
posed, such as compactness of the solution space ( Newey and Powell ,2003 ), Tikhonov regularization, and Landwe-
ber–Fridman regularization ( Carrasco et al. ,2007 ;Cavalier ,2011 ). In practical settings where Tandr0are un-
known, a range of estimators have been proposed in the litera ture, including series-based estimators ( Ai and Chen ,
2003 ;Hall and Horowitz ,2005 ;Blundell et al. ,2007 ;Chen and Reiss ,2011 ;Darolles et al. ,2011 ;Chen and Pouzo ,
2012 ;Florens et al. ,2011 ;Chen ,2021 ), kernel-based estimators ( Hall and Horowitz ,2005 ;Horowitz ,2007 ), and
RKHS-based estimators ( Singh et al. ,2019 ;Muandet et al. ,2020 ).
Recently, there has been growing interest in the applicatio n of general function approximation techniques, such
as deep neural networks and random forests, to instrumental variable problems in a uniﬁed manner ( Dikkala et al. ,
2020 ;Lewis and Syrgkanis ,2018 ;Bennett et al. ,2019 ;Zhang et al. ,2020 ). Among these approaches, Dikkala et al.
(2020 );Liao et al. (2020 );Bennett et al. (2022 ) provide ﬁnite-sample convergence rate guarantees. Speci ﬁcally,
Liao et al. (2020 ) establishes L2convergence by linking minimax optimization with Tikhonov regularization un-
der the assumption of the source condition. Bennett et al. (2022 ) establishes an L2convergence guarantee un-
der the source condition from a distinct perspective. Notab ly, the assumptions we need are strictly weaker than
those of Bennett et al. (2022 ).Dikkala et al. (2020 ) guarantees convergence in terms of projected mean squared
error without the source condition; however, this guarante e is insufﬁcient to identify a speciﬁc element when the
solution is not unique. These works ( Dikkala et al. ,2020 ;Liao et al. ,2020 ;Bennett et al. ,2022 ) rely on the so-
called closedness assumption, which imposes restrictions on the smoothness of the operator Tvia the witness
class. This assumption has been the subject of considerable discussion in the context of ofﬂine reinforcement
learning, with researchers exploring ways to relax it ( Chen and Jiang ,2019 ;Uehara et al. ,2020 ;Foster et al. ,2021 ;
Huang and Jiang ,2022 ). In this paper, we examine the relaxation of this assumptio n in a more general IV setting.
This is of importance since the source condition and closedn ess are inherently conﬂicting.
We note that there are a number of alternative approaches for integrating machine learning into instrumen-
tal variable estimation ( Hartford et al. ,2017 ;Yu et al. ,2018 ;Xu et al. ,2020 ;Liu et al. ,2020 ;Kato et al. ,2021 ;
Lu et al. ,2021 ). However, to the best of our knowledge, these approaches do not offer an L2convergence rate
guarantee in the absence of the assumption of uniqueness.
2 Problem Setup
We aim to solve the following equation with respect to h:
Th=r0 (1)
wherer0(Z) =E[Y|Z]∈L2(Z)is an unknown function and T:L2(X)→L2(Z)is an unknown
conditional expectation operator that maps any h∈L2(X)toE[h(X)|Z]. Note that Tis a bounded operator
since its norm is upper-bounded by 1 via Jensen’s inequality . Moreover, we use T⋆:L2(Z)→L2(X)to denote
the adjoint operator of T,i.e.,/an}bracketle{tg,Th/an}bracketri}htL2(Z)=/an}bracketle{tT⋆g,h/an}bracketri}htL2(X)for anyh∈L2(X),g∈L2(Z)where/an}bracketle{t·,·/an}bracketri}htL2(X)and
/an}bracketle{t·,·/an}bracketri}htL2(Z)are inner products over L2(X)andL2(Z), respectively. It is known that T∗is given by [T∗g](X) =
E[g(Z)|X]for anyg∈L2(Z)(Carrasco et al. ,2007 ). Importantly, here we do not assume compactness of T,
because compactness is violated whenever X,Z include common variables, as is the case in many application s
4(Deaner ,2018 ;Cui et al. ,2020 ). Moreover, we denote the range space of TbyR(T), i.e.,R(T) ={Th:h∈
L2(X)}.
Throughout this work, we assume that there exists a solution to Equation ( 1).
Assumption 1 (Existence of solutions) .We haver0∈ R(T), i.e.,Nr0(T):={h∈ H:Th=r0} /ne}ationslash=∅.
Most of the existing literature further assumes that Tis injective and the solution to Equation ( 1) is unique.
However, even in this case, Equation ( 1) still corresponds to an ill-posed inverse problem, since t he inverse operator
T−1is generally unbounded, so the NIPV solution can be very sens itive to even slight perturbations to the data
distributions. Without further restrictions, we can only o btain an estimator ˆhwith convergence guarantee in terms
of the projected MSE E[{Tˆh−r0}2(Z)] =E[{T(ˆh−h)}2(Z)]forh∈ Nr0(T). However, the projected MSE
is only a pseudometric. Hence, even if E[{T(ˆh−h)}2(Z)]vanishes to zero, the estimator ˆhmay not converge
to a ﬁxed point. Furthermore, the projected MSE is weaker tha n the valid metric such as the L2metric. Indeed,
according to Jensen’s inequality, we have E[{ˆh−h)}2(X)]≥E[{T(ˆh−h)}2(Z)].However, the other direction
generally does not hold. Thus E[{ˆh−h)}2(X)]may not vanish even when E[{T(ˆh−h)}2(Z)]does.
In many problems, L2rate guarantees are preferable or even necessary ( Hall and Horowitz ,2005 ;Chen and Reiss ,
2011 ;Kallus et al. ,2021 ;Uehara et al. ,2021 ). In order to achieve L2convergence, we need to further restrict the
ill-posedness of the NPIV problem. One common way is to restr ict the magnitude of the ill-posedness measure
suph∈HE[{h−h′)}2(X)]
E[{T(h−h′)}2(Z)]for any solution h′∈ Nr0(T), whereHis the function class used to obtain the estima-
tor (e.g.,Dikkala et al. ,2020 ;Chen and Pouzo ,2012 ). This allows us to translate projected MSE guarantees to
corresponding L2error rates under the uniqueness of Equation ( 1).
However, in this paper, we do not assume a unique solution to E quation ( 1), because it may not hold in many
practical settings. In particular, uniqueness is violated when instrumental variables are weak ( Andrews and Stock ,
2005 ;Andrews et al. ,2019 ). For instance, when the spaces DXandDZare discrete and the cardinality of DZex-
ceeds that of DX, uniqueness generally does not hold. Moreover, uniqueness is usually violated in proximal causal
inference, as Kallus et al. (2021 ) demonstrates in various examples. When solutions are non- unique, Equation ( 1)
becomes even more ill-posed. In this case, existing estimat ors may still have projected MSE guarantees, but obtain-
ingL2rate guarantees becomes much more difﬁcult. In particular, the ill-posedness measure is generally inﬁnity
and thus uninformative. Most of the existing estimators do n ot necessarily converge to any particular solution in
Nr0(T)in terms of the L2metric.
Given that there may be (inﬁnitely) many solutions in Nr0(T), we propose to target a particular solution that
achieves the least norm, that is,
h0= argmin
h∈Nr0(T)0.5/an}bracketle{th,h/an}bracketri}htL2(X). (2)
This least norm solution is well-deﬁned as it is the projecti on of the origin in L2(X)onto a closed afﬁne space
Nr0(T)⊆L2(X). We formalize this in the following lemma.
Lemma 1. Suppose Assumption 1holds. Then the least norm solution h0∈ Nr0(T)uniquely exists, and {h0}=
R(T⋆)∩Nr0(T), whereR(T⋆)is the closure of the range space R(T⋆).
We note that some of the existing literature also targets the least norm solution when the IV equation admits
non-unique solutions ( Florens et al. ,2011 ;Santos ,2011 ;Chen ,2021 ), but they all focus on classic sieve or kernel-
based estimators. The only exception is Bennett et al. (2022 ) as they employ general function approximation while
allowing for non-unique solutions. But as we discuss in Sect ion6.3, their method requires a closedness assumption
that puts strong restrictions on the operator T. In this paper, we propose a new estimator for the least norm
solutionh0with a strong L2convergence guarantee. Importantly, our estimator accomm odates general function
approximation but does not need the closedness assumption, thereby improving upon the existing literature.
53 Penalized Minimax Instrumental Variable Regression
In this section, we propose our estimator for the least norm s olutionh0in Equation ( 2). To this end, we ﬁrst provide
a reformulation of the solution h0. Note that
h0= argmin
h∈L2(X)0.5/an}bracketle{th,h/an}bracketri}htL2(X),subject to Th=r0.
This is a constrained optimization problem over the Hilbert spaceL2(X). Following the method of Lagrange
multipliers, we can consider an alternative minimax optimi zation:
h0= argmin
h∈L2(X)sup
g∈L2(Z)L(h,g), L(h,g):= 0.5/an}bracketle{th,h/an}bracketri}htL2(X)+/an}bracketle{tr0−Th,g/an}bracketri}htL2(Z), (3)
wheregcorresponds to a Lagrange multiplier.
In Equation ( 3), the objective function L(h,q)is unknown since the two inner products involve the unknown
functionr0, the unknown operator T, and the unknown distribution of XandZ. To construct an estimator based
on Equation ( 3), we ﬁrst rewrite the inner products into expectations with respect to the distribution of XandZ:
/an}bracketle{th,h/an}bracketri}htL2(X)=E/bracketleftbig
h2(X)/bracketrightbig
,/an}bracketle{tr0−Th,g/an}bracketri}htL2(Z)=E[(Y−h(X))g(Z)].
Then we can replace the unknown expectations with empirical averages, and restrict the functions handgto some
classesH ⊂[DX→R]andG ⊂[DZ→R]. This leads to the following estimator:
ˆhmn∈argmin
h∈Hmax
g∈GLn(h,g), Ln(h,g):= 0.5En[h2(X)]+En[(Y−h(X))g(Z)], (4)
whereEn[·]stands for the empirical average operator based on sample da taD={Xi,Yi,Zi}. For example, we
haveEn[h2(X)] =1
n/summationtextn
i=1h2(Xi). Notably, the term En[h2(X)]in Equation ( 4) can be viewed as a penalization
term, so we call our estimator a penalized minimax estimator . The role of this penalization term is later discussed
inTheorem 1 .
The estimator ˆhmnin Equation ( 4) has a minimax optimization formulation. The computationa l perspective will
be discussed in Section C . This is in line with many recent machine learning IV estimat ors with general function
approximation (see a review in Section 1.1). However, our minimax optimization in Equation ( 4) is motivated
by the method of Lagrange multipliers, while existing minim ax estimators are based on fundamentally different
principles. As a result, our objective function Ln(h,g)differs from those used in existing minimax estimators. In
particular, our minimax estimator requires quite differen t conditions, as we will discuss in Section 6.
To justify the objective function in (4), we need to further guarantee that
h0= argmin
h∈Hmax
g∈GL(h,g). (5)
In Section 4, we establish Equation ( 5) under fairly mild conditions. Based on this, we then furthe r derive the L2
convergence rate of our proposed estimator ˆhmn.
4 Identiﬁcation of the Least Norm Solution
In this section, we establish that our proposed minimax form ulation can indeed identify the least norm solution
h0as shown in Equation ( 5). We start with introducing a key assumption for our result, and then present our
identiﬁcation result under this assumption.
64.1 Source Condition
Our identiﬁcation crucially depends on the following sourc e condition.
Assumption 2 (Source condition) .The function r0satisﬁes that r0∈ R(T T⋆).
Assumption 2further strengthens Assumption 1in that it restricts r0to a smaller subspace R(T T⋆)⊆ R(T).
In particular, we have R(T) =T(R(T⋆))1andR(T T⋆) =T(R(T⋆)), soR(T T⋆)is generally a strict subset
ofR(T), unlessR(T⋆)is a closed set. It is well known that for ill-posed inverse pr oblems, the operator T⋆
generally does not have a closed range space ( Carrasco et al. ,2007 ), thus in general Assumption 2imposes non-
trivial restrictions on the ill-posedness of the inverse pr oblem. In Section 6.4, we provide a more concrete example
to illustrate these restrictions.
Source conditions are common assumptions used to derive str ong convergence rate guarantees in the inverse
problem literature. They have been widely used for both inve rse problems with known operators ( e.g.,Engl et al. ,
1996 ;Ito and Jin ,2014 ) and IV problems with unknown operators ( e.g.,Florens et al. ,2011 ;Carrasco et al. ,2007 ;
Liao et al. ,2021 ). A standard source condition in the literature is that the s olutionh0satisﬁesh0∈ R((T T⋆)β/2)
for a positive exponent β >0. Our source condition in Assumption 2can be shown to be equivalent to h0∈
R((T T⋆)1/2)via the spectral theory of linear operators ( Cavalier ,2011 ). Thus, our Assumption 2is a source
condition of this kind with source exponent β= 1.
Assumption 2implies that there exists ¯g0∈L2(Z)such that
r0=T T⋆¯g0. (6)
In fact, any ¯g0satisfying Equation ( 6) is closely related to the least norm solution h0.
Lemma 2. If Assumption 2holds, then ¯g0satisﬁes (6)if and only if T⋆¯g0=h0.
In particular, given Lemma 2, the functions ¯g0that satisfy Equation ( 6) are given by:
Nh0(T⋆):={g∈L2(Z) :T⋆g=h0}. (7)
In the next subsection, we will show the importance of the sou rce condition given by Assumption 2. In particu-
lar, this condition ensures that we can obtain h0from the saddle points of L(h,g).
4.2 Saddle Points of the Minimax Optimization
Here, we characterize the saddle points of L(h,g)under Assumption 2, as follows:
Lemma 3. Suppose Assumption 2holds and let h0be the least norm solution in Equation (2)andNh0(T⋆)be the
set of functions given in Equation (7). Then, the set of saddle points of L(h,g)overh∈L2(X),g∈L2(Z), i.e.,
the points (h′,g′)that satisfy
L(h,g′)≥L(h′,g′)≥L(h′,g),∀h∈L2(X),∀g∈L2(Z),
is given by the set {h0}×Nh0(T⋆) ={(h0,¯g) : ¯g∈ Nh0(T⋆)}.
It is well-known that (h′,g′)is a saddle point if and only if we have the “strong duality” co ndition
infh∈L2(X)supg∈L2(Z)L(h,g) = supg∈L2(Z)infh∈L2(X)L(h,g)and
h′∈argmin
h∈L2(X)sup
g∈L2(Z)L(h,g), g′∈argmax
g∈L2(Z)inf
h∈L2(X)L(h,g).
We provide formal proof for this in Section I . Given this equivalent characterization of the saddle poin t, we can
obtain the following corollary from Lemma 3 .
1To see this note that L2(X) =R(T⋆)⊕R(T⋆)⊥, andR(T⋆)⊥=N(T), soT(L2(X)) =T(R(T⋆)).
7Corollary 1. If Assumption 2holds, then we have
h0= argmin
h∈L2(X)sup
g∈L2(Z)L(h,g),Nh0(T⋆) = argmax
g∈L2(Z)inf
h∈L2(X)L(h,g). (8)
It is worth noting that the equality for h0in Equation ( 8) holds even without the source condition. Moreover,
the strong duality infh∈L2(X)supg∈L2(Z)L(h,g) = supg∈L2(Z)infh∈L2(X)L(h,g)also holds in the absence of this
source condition. However, the source condition is importa nt to establish the existence of argmaxg∈L2(Z)infh∈L2(X)L(h,g)
and the second statement in ( 8). Equivalently, this shows that the source condition guara ntees the existence of sta-
tionary Lagrangian multipliers for the problem in Equation (2), and the set of stationary Lagrangian multipliers is
given byNh0(T⋆).
So far we have demonstrated that Assumption 2is a sufﬁcient condition for the existence of saddle points.
Interestingly, it is also a necessary condition for their ex istence.
Lemma 4. Suppose Assumption 1thatr0∈ R(T)holds. Then, there exists a saddle point of L(h,g)if and only if
Assumption 2holds.
The above lemma is proved by ﬁrst showing that the saddle poin t exists if and only if there exists a solution to
argming∈L2(Z)/bardblT∗g−h0/bardbl2
2. We then demonstrate that the existence of this optimizatio n problem is equivalent
to the source condition ( 2). Our Lemma 3 andLemma 4 show that the source condition is closely related to
the existence of stationary Lagrangian multipliers for the constrained optimization formulation of h0. To our
knowledge, this relation is novel in the literature.
Lemma 3 characterizes the saddle points over the unrestricted L2(X)andL2(Z)spaces. However, in practical
estimation, we can only use some function classes H ⊂L2(X),G ⊂L2(Z)with limited statistical complexity.
For these two restricted classes to capture some saddle poin ts, we need them to satisfy the following realizability
assumptions.
Assumption 3 (Realizability of the least norm solution) .We haveh0∈ H.
Assumption 4 (Realizability of the stationary Lagrange multiplier) .We haveNh0(T⋆)∩G /ne}ationslash= 0.
The realizability assumptions above require that the funct ion classes HandGare well-speciﬁed, in that they
contain at least some true saddle points. In particular, Ass umption 4is equivalent to h0∈ T∗G. In the following
theorem, we further extend the saddle point characterizati on ofh0in Corollary 1to these restricted classes under
these realizability conditions.
Theorem 1 (Key identiﬁcation theorem) .Suppose Assumption 2,3,4hold. Then
h0= argmin
h∈Hmax
g∈GL(h,g).
Theorem 1shows that under the source condition and the realizability assumptions, the min-max optimization
of our proposed objective over the function classes H,Gcan recover the saddle points in the classes. At a high level,
the proof of this theorem works by showing: (1) saddle points over the original class remain saddle points over the
restricted classes; (2) any additional saddle points over t he restricted classes are best-responses to saddle points
over the original class; and (3) h0is a unique best response to any ¯g∈ Nh0(T⋆)as a result of strong convexity of
L(h,g)inhinduced by /an}bracketle{th,h/an}bracketri}htL2(X). See Section B for details.
5 Finite Sample Guarantees
As discussed in Section 4 , our proposed minimax optimization formulation can identi fy the target least norm solu-
tionh0when the population distribution is known. In this section, we further show that our ﬁnite-sample estimator
ˆhmnin Equation ( 4) converges to h0, and we derive its L2error rate.
8Theorem 2 (L2convergence rates) .Suppose Assumption 2,3,4hold. Then, we have
/bardblˆhmn−h0/bardbl2≤/radicalBigg
2 sup
h∈H,g∈G/vextendsingle/vextendsingle/vextendsingle(En−E)[(Y−h(X))g(Z)+0.5h(X)2]/vextendsingle/vextendsingle/vextendsingle.
Notably, the assumptions required in Theorem 2 are identical to those in Theorem 1. In particular, both theo-
rems only require that the function classes HandQsatisfy the realizability conditions Assumption 3,4. Realizabil-
ity is a fundamental assumption in statistical learning the ory. For instance, realizability is a standard assumption
in least squares regression problems ( e.g.,Wainwright ,2019 ). To the best of our knowledge, existing minimax
IV regression estimators additionally require much strong er conditions such as T H ⊂ G orT∗G ⊂ H . These
conditions are often referred to as the closedness conditio n, and they impose additional restrictions on the operator
T. See Section 6for a detailed discussion.
It then remains to bound the right-hand side term in Theorem 2 . This is an empirical process term, which can
be easily upper-bounded by invoking standard statistical l earning theory for any reasonable function classes H,G
with bounded statistical complexities. In particular, we c an use standard symmetrization arguments to bound the
right-hand side of Theorem 2 with the Rademacher complexities of H,G. The Rademacher complexity Rn(H)
of classHis deﬁned as Rn(H) =n−1E[suph∈H/summationtextn
i=1σih(Xi)]where{σ1,...,σ n}are independent random
variables drawn from the Rademacher distribution. The Rade macher complexity Rn(G)of classGcan be deﬁned
analogously.
Corollary 2. Suppose Assumption 2,3,4hold. Let /bardblY/bardbl ≤CY,/bardblh/bardbl∞≤CHfor anyh∈ H and/bardblg/bardbl∞≤CGfor
g∈ G. Then, there exists a universal positive constant csuch that with probability at least 1−δ, we have
/bardblˆhmn−h0/bardbl2≤c/radicalBig
(CH+CG)(Rn(G)+Rn(H))+(CG+CH)CH/radicalbig
ln(1/δ)/n
Furthermore, for given function classes H,Q, we can obtain ﬁnal L2convergence rates by plugging in off-
the-shelf results of Rademacher complexities. For example , the following corollary is obtained by instantiating
Theorem 2 to ﬁnite classes.
Corollary 3. WhenH,Gare ﬁnite classes, with probability at least 1−δ, we have /bardblˆhmn−h0/bardbl2= Poly(CH,CG)/parenleftBig
ln(|H||G|/δ)
n/parenrightBig1/4
wherePoly(CH,CG)is a polynomial term in CHandCG.
As another example, we instantiate Theorem 2for more general nonparametric classes whose complexity ar e
characterized by their covering numbers.
Corollary 4. LetM(ǫ,H,/bardbl · /bardbl∞)andM(ǫ,G,/bardbl · /bardbl∞)be covering numbers of H,Gwith respect to L∞-norm.
Supposeln(M(ǫ,H,/bardbl·/bardbl∞)) =O(ǫ−β)andln(M(ǫ,G,/bardbl·/bardbl∞)) =O(ǫ−β)for someβ >0, and the conditions of
in Corollary 2hold. Then with probability at least 1−δ, we have
/bardblˆhmn−h0/bardbl2=

Poly(CH,CG){n−1/4+(ln(1/δ)/n)1/4},(β <2)
Poly(CH,CG){n−1/4ln(n)+(ln(1/δ)/n)1/4},(β= 2)
Poly(CH,CG){n−1/(2β)+(ln(1/δ)/n)1/4}(β >2).
If we specialize Corollary 4to Sobolev balls H,Gwith smoothness parameter αand input dimension d, we
haveβ=d/α, so the rates become O(n−1/4)whenα/d >2andO(n−α/(2d))whenα/d≤2. It is an interesting
question whether this rate is optimal. Although Chen and Reiss (2011 ) derives a minimax rate for NPIV regression
estimation, their result requires the NIPV equation to have a unique solution and they impose stronger conditions
on the function classes, so it is not directly comparable to o ur rate. A thorough investigation of the rate optimality
is left for future work.
9Finally, we also consider the case where the function classe sH,Gare misspeciﬁed so they may not satisfy the
realizability assumptions. This result is useful when we us e sieve estimators based on sample-dependent function
classesHandG, that approximate certain function spaces. For example, H,Gcan be linear models with polynomial
basis functions or neural networks with growing dimensions , which can gradually approach H¨ older or Sobolev balls
(Chen ,2007 ).
Theorem 3 (Finite sample result under misspeciﬁcation) .Suppose Assumption 2holds, and there exists h†∈ H
andg†∈ Gsuch that /bardblh†−h0/bardbl2≤ǫhandinf¯g0∈Nh0(T⋆)/bardblg†−¯g0/bardbl2≤ǫg. Then
/bardblˆhmn−h0/bardbl2≤/radicalbigg
{2CH+CG}ǫh+CHǫg+2 sup
h∈H,g∈G|(En−E)[(Y−h(X))g(Z)+0.5h(X)2]|.
Compared to Theorem 2, the upper bound in Theorem 3involves additional misspeciﬁcation errors ǫh,ǫgdue to
misspeciﬁed H,G. The empirical process term in Theorem 3can be again bounded by Rademacher complexities.
6 Discussions
In this section, we compare our method with existing minimax NPIV estimators in Dikkala et al. (2020 );Liao et al.
(2020 );Bennett et al. (2022 ) as they are most relevant. Other existing minimax estimato rs are similar so we only
brieﬂy review them in Section 1.1.
6.1 Comparisons to Dikkala et al. (2020 )
Dikkala et al. (2020 ) considers the following minimax estimator:
ˆhpro= argmin
h∈Hmax
g∈G˜Ln(h,g),˜Ln(h,g):=−0.5En[g2(Z)]+En[(Y−h(X))g(Z)].
Here for simplicity, we omit possible additional regulariz ers forhandg.
Dikkala et al. (2020 ) assumes the closedness condition that T(H −h⋄
0)⊂ G whereh⋄
0can be an arbitrary
solution to Th=r0(note this condition is invariant to the choice of h⋄
0). Under this condition, letting L(h,g)
be the population analog of ˜Ln(h,g), it can be shown that maxg∈GE[˜L(h,g)] = 0.5E[(T(h⋄
0−h)[Z])2] =
0.5E[([Th](Z)−r0(Z))2]. In other words, the minimax objective in Dikkala et al. (2020 ) is used to approxi-
mate the projected MSE objective under the closedness condi tion. In contrast, our proposed minimax objective is
motivated by the method of Lagrange multipliers, and it does not need the closedness condition.
To compare the theory in Dikkala et al. (2020 ) with our theory, we consider ﬁnite classes H,Gfor simplicity.
Then the theory in Dikkala et al. (2020 ) implies that if Nr0(T)∩H /ne}ationslash= 0 andT(H −h⋄
0)⊂ G forh⋄
0∈ Nr0(T),
then we have E[{T(ˆhpro−h⋄
0)}2(Z)] =O/parenleftbigg/parenleftBig
ln(|H||G|/δ)
n/parenrightBig1/2/parenrightbigg
with probability 1−δ.
Note that the rate O((ln(|H||G|)/n)1/2)above is faster than our rate O((ln(|H||G|)/n)1/4)in Corollary 3.
However, the rate above is for the weak projected MSE, while o ur rate in Corollary 3is for the stronger L2
error, so they are not comparable. In particular, the projec ted MSE rate cannot translate into an L2rate without
further restrictions. Dikkala et al. (2020 ) consider restrictiting the ill-posedness measure suph∈HE[{ˆh−h)}2(X)]
E[{T(ˆh−h)}2(Z)].
However, this ill-posedness measure may generally be inﬁni te, and in fact is guaranteed to be inﬁnite when the
solutions to the NPIV problem are nonunique, so using it to ge tL2convergence rates is often problematic.
Remark 1 (Enjoy the best of both worlds) .Here we observe that the estimator in Dikkala et al. (2020 ) can achieve
a fast projected MSE rate while our estimator achieves a slow L2rate. One may wonder whether it is possible to
achieve both guarantees at the same time. We explore this que stion in Section A and ﬁnd this is possible if we put
aside computational considerations.
106.2 Comparison to Liao et al. (2020 )
Liao et al. (2020 ) builds on Dikkala et al. (2020 ) and incorporates additional Tikhonov regularization int o the min-
imax optimization:
min
h∈Hmax
g∈G−0.5En[g2(Z)]+En[(Y−h(X))g(Z)]+αEn[h2(X)]. (9)
Liao et al. (2020 ) also needs the closedness assumption in Dikkala et al. (2020 ) and a realizability assumption that
the Tikhonov regularized solution h0,αis contained in Hfor small α. In addition, they assume that the NPIV
solution is unique and satisﬁes a source condition with expo nentβ∈(0,1], and the regularization strength α
vanishes to 0at an appropriate rate as n→ ∞ . Under these conditions, they can derive an L2convergence rate. In
particular, their L2rate has the order O(n−1/6)when the function classes are e.g.ﬁnite or VC, and β= 1.
Our proposed estimator and theory signiﬁcantly differ from Liao et al. (2020 ). Speciﬁcally, our estimator does
not involve the En[g2(Z)]term and our regularized term En[h2(X)]has a constant coefﬁcient 0.5but Equation ( 9)
needs a vanishing α. Moreover, our theory accommodates non-unique solutions, and uses different realizability
assumptions. Notably, under our source condition β= 1, our convergence rate O(n−1/4)is faster than the rate
O(n−1/6)inLiao et al. (2020 ).
6.3 Comparison to Bennett et al. (2022 )
Under the same source condition, Bennett et al. (2022 )2formulate the L2error ofh0as projected MSEs: E[{h0−
h}2(X)] =E[(T∗{¯g0−g})2(X)]whereT∗g=handT∗¯g0=h0. First, note that for any ﬁxed ¯g0such that
T⋆¯g0=h0, we have
Nh0(T∗) = argmin
g∈GE[(T∗{¯g0−g})2(X)] = argmin
g∈G0.5E[(T∗g)2(X)]−E[Yg(Z)].
Then, under the closedness assumption T∗G ⊂ H , we have
Nh0(T∗) = argmax
g∈Gmin
h∈H0.5E[h2(X)]+E[{Y−h(X)}g(Z)].
Then, noting that the inner minimizer hsatisﬁesT∗g=hfor any given g, and recalling the original goal is to ﬁnd
h0such that T∗¯g0=h0, we can deduce that3
{¯g0,h0}= argmax
g∈Gargmin
h∈H0.5E[h2(X)]+E[{Y−h(X)}g(Z)].
Finally, their proposed estimator ˆhﬂiis given by replacing expectations with empirical averages .
In comparison to our proposed estimator ˆhmn, the difference lies in the ﬂip of argmax andargmin . SinceG,H
could be non-convex, the two estimators are generally diffe rent. Indeed, this results in a signiﬁcant difference in
terms of the required assumptions. In ˆhﬂi, the primary assumptions are the source condition, ¯g0∈ G, andT∗G ⊂ H
(note that h0∈ H is implicit from the latter two conditions). Conversely, in our proposed estimator ˆhmn, the
primary assumptions are the source condition and ¯g0∈ G,h0∈ H. This condition is strictly weaker as we dispense
with the requirement of closedness. This improvement is sig niﬁcant due to the inherent conﬂict between the source
condition and closedness, as elucidated next.
2Note the main focus of Bennett et al. (2022 ) is to estimate the Riesz representator (in their notation, q†) withL2error rates. However,
their argument is easily adapted to our scenario.
3Here, letting a loss function to be L⋆(h,g), the equation (¯g0,h0) = argmingargmaxhL(h,g)means¯g0= argmingmaxhL(h,g)
andh0= argmaxhL(h,¯g0).
116.4 Tension between Source Condition and Closedness
In Sections 6.1to6.3, the existing estimators all require certain closedness as sumption, either T(H−h⋄
0)⊂ G for
an arbitrary solution h⋄
0toTh=r0, orT∗G ⊂ H . In contrast, our proposed estimator does not need any close dness
assumption. In this subsection, we show that the closedness conditions are inherently in tension with the source
condition. This illustrates the beneﬁt of getting rid of the source condition. For simplicity, we consider a compact
linear operator Tthat admits a singular value decomposition (SVD) {σi,ui,vi}∞
i=1, where{ui}∞
i=1,{vi}∞
i=1are
orthonormal bases in the Hilbert spaces L2(Z),L2(X), respectively, and σ1≥σ2≥ ··· are the singular values.
It follows that Tvi=σiui,T∗ui=σivi, andT T⋆has the SVD {σ2
i,ui,ui}∞
i=1. Here we assume a compact
operator merely for a simple countable SVD. Non-compact ope rators can be handled similarly, but involve more
cumbersome notations ( Cavalier ,2011 ).
To understand the source condition in Assumption 2, we write the function r0asr0=/summationtext∞
i=1γiuiwith/summationtext∞
i=1γ2
i<∞. The source condition r0∈ R(T T⋆)means that there exists ¯g0=/summationtext∞
i=1βiuiwith/summationtext∞
i=1β2
i<∞
such that h0=T∗T¯g0. It follows from the SVD of T T⋆thatγi=σ2
iβi. Therefore, the source condition requires/summationtext∞
i=1γ2
i/σ4
i<∞. This means that the function r0needs to be sufﬁciently smooth relative to the spectrum of T.
Obviously, the source condition is more readily satisﬁed wh en the decaying rate of {σi}∞
i=1is slower, i.e., when
the operators TandT⋆are less smooth. In contrast, the closedness conditions are generally more easily satisﬁed
when{σi}∞
i=1decays faster and the operators TandT⋆are more smooth.
Hence, we observe that the source condition and closedness i mply opposing restrictions on the smoothness of
the operators TandT⋆.
7 Conclusion
In this paper, we study NPIV regression with general functio n approximation. We propose a penalized minimax
estimator based on a novel constrained optimization formul ation of the least norm IV solution. We prove that
our estimator converges to this least norm solution, and der ive itsL2convergence rate under a source condition
and realizability assumptions on both function classes for the minimax estimator. Notably, our estimator does not
require uniqueness of the NPIV solution, and it avoids a clos edness condition commonly assumed for existing
minimax estimators. There are many interesting future dire ctions of research. One direction is extending our work
to more general inverse problems, including nonlinear inve rse problems ( Ito and Jin ,2014 ). Another direction is
extending our work to IV quantile regression ( Chernozhukov et al. ,2017 ).
References
Ai, C. and X. Chen (2003). Efﬁcient estimation of models with conditional moment restrictions containing un-
known functions. Econometrica 71 (6), 1795–1843.
Andrews, D. and J. H. Stock (2005). Inference with weak instr uments.
Andrews, D. W. (2017). Examples of l2-complete and boundedl y-complete distributions. Journal of economet-
rics 199 (2), 213–220.
Andrews, I., J. H. Stock, and L. Sun (2019). Weak instruments in instrumental variables regression: Theory and
practice. Annual Review of Economics 11 (1), 727–753.
Angrist, J. and G. Imbens (1995). Identiﬁcation and estimat ion of local average treatment effects.
Babii, A. and J.-P. Florens (2017). Is completeness necessa ry? estimation in nonidentiﬁed linear models. arXiv
preprint arXiv:1709.03473 .
12Bennett, A., N. Kallus, X. Mao, W. Newey, V . Syrgkanis, and M. Uehara (2022). Inference on strongly identiﬁed
functionals of weakly identiﬁed functions. arXiv e-prints , arXiv–2208.
Bennett, A., N. Kallus, and T. Schnabel (2019). Deep general ized method of moments for instrumental variable
analysis. Advances in neural information processing systems 32 .
Blundell, R., X. Chen, and D. Kristensen (2007). Semi-nonpa rametric iv estimation of shape-invariant engel curves.
Econometrica 75 (6), 1613–1669.
Carrasco, M., J.-P. Florens, and E. Renault (2007). Linear i nverse problems in structural econometrics estimation
based on spectral decomposition and regularization. Handbook of econometrics 6 , 5633–5751.
Cavalier, L. (2011). Inverse problems in statistics. In Inverse problems and high-dimensional estimation , pp. 3–96.
Springer.
Chen, J. and N. Jiang (2019). Information-theoretic consid erations in batch reinforcement learning. In International
Conference on Machine Learning , pp. 1042–1051. PMLR.
Chen, Q. (2021). Robust and optimal estimation for partiall y linear instrumental variables models with partial
identiﬁcation. Journal of Econometrics 221 (2), 368–380.
Chen, X. (2007). Large sample sieve estimation of semi-nonp arametric models. Handbook of econometrics 6 ,
5549–5632.
Chen, X., V . Chernozhukov, S. Lee, and W. K. Newey (2014). Loc al identiﬁcation of nonparametric and semipara-
metric models. Econometrica 82 (2), 785–809.
Chen, X. and D. Pouzo (2012). Estimation of nonparametric co nditional moment models with possibly nonsmooth
generalized residuals. Econometrica 80 (1), 277–321.
Chen, X. and M. Reiss (2011). On rate optimality for ill-pose d inverse problems in econometrics. Econometric
Theory 27 (3), 497–521.
Chernozhukov, V ., C. Hansen, and K. W¨ uthrich (2017). Instrumental variable quantile regression . Chapman and
Hall/CRC.
Christensen, T. M. (2017). Nonparametric stochastic disco unt factor decomposition. Econometrica 85 (5), 1501–
1536.
Cui, Y ., H. Pu, X. Shi, W. Miao, and E. T. Tchetgen (2020). Semi parametric proximal causal inference. arXiv
preprint arXiv:2011.08411 .
Darolles, S., Y . Fan, J.-P. Florens, and E. Renault (2011). N onparametric instrumental regression. Economet-
rica 79 (5), 1541–1565.
Daskalakis, C., A. Ilyas, V . Syrgkanis, and H. Zeng (2017). T raining gans with optimism. arXiv preprint
arXiv:1711.00141 .
Deaner, B. (2018). Proxy controls and panel data. arXiv preprint arXiv:1810.00283 .
Dikkala, N., G. Lewis, L. Mackey, and V . Syrgkanis (2020). Mi nimax estimation of conditional moment models.
Advances in Neural Information Processing Systems 33 , 12248–12262.
13Engl, H. W., M. Hanke, and A. Neubauer (1996). Regularization of inverse problems , V olume 375. Springer
Science & Business Media.
Escanciano, J. C., S. Hoderlein, A. Lewbel, O. Linton, and S. Srisuma (2020). Nonparametric euler equation
identiﬁcation and estimation. Econometric Theory .
Florens, J.-P., J. Johannes, and S. Van Bellegem (2011). Ide ntiﬁcation and estimation by penalization in nonpara-
metric instrumental regression. Econometric Theory 27 (3), 472–496.
Foster, D. J., A. Krishnamurthy, D. Simchi-Levi, and Y . Xu (2 021). Ofﬂine reinforcement learning: Fundamental
barriers for value function approximation. arXiv preprint arXiv:2111.10919 .
Hall, P. and J. L. Horowitz (2005). Nonparametric methods fo r inference in the presence of instrumental variables.
The Annals of Statistics 33 (6), 2904–2929.
Hartford, J., G. Lewis, K. Leyton-Brown, and M. Taddy (2017) . Deep iv: A ﬂexible approach for counterfactual
prediction. In International Conference on Machine Learning , pp. 1414–1423. PMLR.
Horowitz, J. L. (2007). Asymptotic normality of a nonparame tric instrumental variables estimator. International
Economic Review 48 (4), 1329–1349.
Horowitz, J. L. (2011). Applied nonparametric instrumenta l variables estimation. Econometrica 79 (2), 347–394.
Huang, A. and N. Jiang (2022). Beyond the return: Off-policy function estimation under user-speciﬁed error-
measuring distributions. In Neurips .
Ito, K. and B. Jin (2014). Inverse problems: Tikhonov theory and algorithms , V olume 22. World Scientiﬁc.
Kallus, N., X. Mao, and M. Uehara (2021). Causal inference un der unmeasured confounding with negative controls:
A minimax learning approach. arXiv preprint arXiv:2103.14029 .
Kalouptsidi, M., P. T. Scott, and E. Souza-Rodrigues (2021) . Linear iv regression estimators for structural dynamic
discrete choice models. Journal of Econometrics 222 (1), 778–804.
Kato, M., M. Imaizumi, K. McAlinn, S. Yasui, and H. Kakehi (20 21). Learning causal models from conditional
moment restrictions by importance weighting. In International Conference on Learning Representations .
Lewis, G. and V . Syrgkanis (2018). Adversarial generalized method of moments. arXiv preprint arXiv:1803.07164 .
Liao, L., Y .-L. Chen, Z. Yang, B. Dai, M. Kolar, and Z. Wang (20 20). Provably efﬁcient neural estimation of
structural equation models: An adversarial approach. In Advances in Neural Information Processing Systems ,
V olume 33, pp. 8947–8958.
Liao, L., Z. Fu, Z. Yang, Y . Wang, M. Kolar, and Z. Wang (2021). Instrumental variable value iteration for causal
ofﬂine reinforcement learning. arXiv preprint arXiv:2102.09907 .
Liu, R., Z. Shang, and G. Cheng (2020). On deep instrumental v ariables estimate. arXiv preprint arXiv:2004.14954 .
Lu, Y ., H. Chen, J. Lu, L. Ying, and J. Blanchet (2021). Machin e learning for elliptic pdes: fast rate generalization
bound, neural scaling law and minimax optimality. arXiv preprint arXiv:2110.06897 .
Miao, W., L. Liu, E. T. Tchetgen, and Z. Geng (2015). Identiﬁc ation, doubly robust estimation, and semiparametric
efﬁciency theory of nonignorable missing data with a shadow variable. arXiv preprint arXiv:1509.02556 .
14Muandet, K., A. Mehrjou, S. K. Lee, and A. Raj (2020). Dual ins trumental variable regression. Advances in Neural
Information Processing Systems 33 , 2710–2721.
Newey, W. K. (2013). Nonparametric instrumental variables estimation. American Economic Review 103 (3), 550–
56.
Newey, W. K. and J. L. Powell (2003). Instrumental variable e stimation of nonparametric models. Economet-
rica 71 (5), 1565–1578.
Santos, A. (2011). Instrumental variable methods for recov ering continuous linear functionals. Journal of Econo-
metrics 161 (2), 129–146.
Singh, R., M. Sahani, and A. Gretton (2019). Kernel instrume ntal variable regression. Advances in Neural Infor-
mation Processing Systems 32 .
Uehara, M., J. Huang, and N. Jiang (2020). Minimax weight and q-function learning for off-policy evaluation. In
International Conference on Machine Learning , pp. 9659–9668. PMLR.
Uehara, M., M. Imaizumi, N. Jiang, N. Kallus, W. Sun, and T. Xi e (2021). Finite sample analysis of minimax ofﬂine
reinforcement learning: Completeness, fast rates and ﬁrst -order efﬁciency. arXiv preprint arXiv:2102.02981 .
Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint , V olume 48. Cambridge
University Press.
Wang, S., J. Shao, and J. K. Kim (2014). An instrumental varia ble approach for identiﬁcation and estimation with
nonignorable nonresponse. Statistica Sinica , 1097–1116.
Xu, L., Y . Chen, S. Srinivasan, N. de Freitas, A. Doucet, and A . Gretton (2020). Learning deep features in
instrumental variable regression. arXiv preprint arXiv:2010.07154 .
Yu, B. et al. (2018). The deep ritz method: a deep learning-ba sed numerical algorithm for solving variational
problems. Communications in Mathematics and Statistics 6 (1), 1–12.
Zhang, R., M. Imaizumi, B. Sch¨ olkopf, and K. Muandet (2020) . Maximum moment restriction for instrumental
variable regression. arXiv preprint arXiv:2010.07684 .
15A Enjoy the Best of Both Worlds
Thus far, we have encountered two types of guarantees: slow L2rates and fast projected MSEs. The next step is
to obtain guarantees that possess both properties. If we put aside issues of computational efﬁciency, then this is
actually achievable. The estimator is deﬁned as follows:
ˆhboth= argmin
h∈Hnmax
g∈G˜Ln(h,g)
where
Hn={h∈ H;max
g∈GLn(h,g)−min
h∈Hmax
g∈GLn(h,g)≤µn}.
Here,µnis some hyperparameter. The set Hnis deﬁned so that each of its element has the L2convergence
guarantee under the source condition.
Theorem 4 (fast projected MSEs + slow L2errors) .SupposeH,Gare ﬁnite for simplicity. Suppose h0∈ H,T(H−
h0)⊂ G,Nh0(T⋆)∩G /ne}ationslash=∅. Then, when we take µn= (CH+CG)2/radicalbig
ln(|H||G|/δ)/n, with probability 1−δ, we
have
/bardblT(ˆhboth−h0)/bardbl2≤c(CH+CG)/radicalbigg
ln(|H||G|/δ)
n,/bardblˆhboth−h0/bardbl2≤c(CH+CG)/parenleftbiggln(|H||G|/δ)
n/parenrightbigg1/4
.
B General Characterization of Saddle Points
First, notice
{h0}= min
h∈L2(X)L(h,¯g0) (10)
for any¯g0∈ Nh0(T∗). In other words, the optimal response to L(h,¯g0)is uniquely h0. It follows from two
observations: (1) h0is a best response for any element ¯g0inNh0(T∗), since(h0,¯g0)is a saddle point by Lemma 3 ;
and (2) the best response for each ¯g0is unique, since L(h,¯g0)is strictly convex in h, due to the /an}bracketle{th,h/an}bracketri}htL2(X)term.
Next, we invoke the following general characterization of s addle points. Here, (˜x,˜y)∈argminx∈X′argmaxy′∈Y′f(x,y)
means˜x∈argminx∈X′maxy∈Y′f(x,y)and˜y∈argmaxy∈Y′f(˜x,y).
Lemma 5 (Characterization of saddle points over constrained sets) .LetZbe a set of saddle points for f(x,y)
overX,Y. LetZX= argminx∈Xmaxy∈Yf(x,y),(·,˜ZX) = argmaxy∈Yargminx∈Xf(x,y).Then, for X′⊂
X,Y′⊂ Y, ifZ ∩(X′,Y′)is non-empty, we have
ZX∩X′⊂argmin
x∈X′max
y∈Y′f(x,y) (11)
and
argmin
x∈X′max
y∈Y′f(x,y)⊂˜ZX∩X′. (12)
InLemma 5 , the primary assumption Z ∩(X′,Y′)/ne}ationslash=∅means that some saddle point (with respect to X,Y) is
included in X′,Y′. The equation ( 11) states that any saddle points ( ZX∩X′)over unconstrained function classes
(X,Y) are still saddle points over constrained function classes (X′,Y′). The equation ( 12) states that any saddle
point over constrained function classes ( X′,Y′) is included in ˜ZX.
We combine the above characterization of saddle points with Lemma 3 by setting (X′,Y′) = (H,G),(X,Y) =
(L2(X),L2(Z)),f(x,y) =L(h,g). As an immediate consequence, when h0∈ H,Nh0(T⋆)∩G /ne}ationslash=∅(i.e., saddle
points are included in (H,G)), using (11), we have {h0} ⊂argminh∈Hmaxg∈GL(h,g).Next, using (12), we
haveargminh∈Hmaxg∈GL(h,g)⊂ {h0}since(·,h0) = argmaxg∈Gargminh∈HL(h,g)by (10).
16C Computational Perspective
To solve the optimization problem in Equation ( 4), we can leverage the recent advances in minimax optimizati on
algorithms, even when the function classes HandGare neither convex nor concave, such as neural network class es
(Daskalakis et al. ,2017 ). In particular, using a Reproducing kernel Hilbert space ( RKHS) ball as Gis particularly
convenient, since then the inner maximization problem in Eq uation ( 4) has a closed form solution. Speciﬁcally,
whenG={g:/bardblg/bardblK≤1}for a positive deﬁnite kernel K:DZ×DZ→Rand its associated RKHS norm /bardbl·/bardblK,
Equation ( 4) reduces to
argmin
h∈H0.5En[h2(X)]+
1
n2n/summationdisplay
i=1n/summationdisplay
j=1(Yi−h(Xi))K(Zi,Zj)(Yj−h(Xj))
1/2
.
D Proof in Section 2
D.1 Proof of Lemma 1
Here, we have Nr0(T) =h0+N(T). The least norm solution h0amongNr0(T)is the projection of any element
inNr0(T)onto (the closed subspace) N(T)⊥. Hence,{h0}=N(T)⊥∩Nr0(T) =R(T⋆)∩Nr0(T). Here, we
useN(T)⊥=R(T⋆).
E Proof in Section 4
E.1 Proof of Lemma 2
It is clear from Lemma 1 .
E.2 Proof of Lemma 3
The proof is as follows. From Section I , a point (h′,g′)∈(L2(X),L2(Z))is a saddle point if and only if the
strong duality holds and h′∈argminh∈L2(X)supg∈L2(Z)L(h,g)andg′∈argmaxg∈L2(Z)infh∈L2(X)L(h,g).
We check this condition.
Hence, we ﬁrst show
{h0}= argmin
h∈L2(X)sup
g∈L2(Z)L(h,g),0.5/bardblh0/bardbl2
2= min
h∈L2(X)sup
g∈L2(Z)L(h,g) (13)
First, for any h/ne}ationslash=Nr0(T), we have supg∈L2(Z)L(h,g) =∞. Hence, the solution needs to belong to Nr0(T).
Sincesupg∈L2(Z)L(h,g) = 0.5E[h2(X)]for anyh∈ Nr0(T), using Lemma 2 , thus, from the deﬁnition of h0, the
solution is h0.
Next, we show
Nh0(T⋆) = argmax
g∈L2(Z)inf
h∈L2(X)L(h,g),0.5/bardblh0/bardbl2
2= max
g∈L2(Z)inf
h∈L2(X)L(h,g). (14)
We solve the inner minimization problem ﬁrst. Then,
inf
h∈L2(X)L(h,g) = inf
h∈L2(X)0.5/bardblh−T⋆g/bardbl2
2+/an}bracketle{tr0,g/an}bracketri}htL2(Z)−0.5/an}bracketle{tT⋆g,T⋆g/an}bracketri}htL2(X)
=/an}bracketle{tr0,g/an}bracketri}htL2(Z)−0.5/an}bracketle{tT⋆g,T⋆g/an}bracketri}htL2(X)
=/an}bracketle{tTh0,g/an}bracketri}htL2(Z)−0.5/an}bracketle{tT⋆g,T⋆g/an}bracketri}htL2(X) (User0=Th0)
=−0.5/bardblT⋆g−h0/bardbl2
2+0.5/bardblh0/bardbl2
2.
17By using Assumption 2, sinceNh0(T∗)is not empty, we have
Nh0(T∗) = argmax
g∈L2(Z)inf
h∈L2(X)L(h,g).
Finally, since the strong duality holds from (13) and(14), the set of saddle points is (h0,Nh0(T⋆)).
E.3 Proof of Lemma 4
Recall the saddle point exists if and only if argminh∈L2(X)supg∈L2(Z)L(h,g)andargmaxg∈L2(Z)infh∈L2(X)L(h,g)
exist and the strong duality holds. We already show that Assu mption 2is sufﬁcient to ensure the existence of the
saddle point. In this proof, we show Assumption 2is necessary to ensure the existence of the saddle point.
To ensure the existence of saddle point, we need to ensure the existence of argmaxg∈L2(Z)infh∈L2(X)L(h,g).
This optimization problem is equivalent to
argmin
g∈L2(Z)/bardblT⋆g−h0/bardbl2
2 (15)
as we see in the proof of Lemma 3 . This solution exists if and only if h0∈ R(T⋆)+R(T⋆)⊥. To prove this, we
deﬁne a projection operator onto R(T⋆)asPR(T⋆). Then, the solution of ( 15) exists if and only if PR(T⋆)h0∈
R(T⋆). Here,PR(T⋆)h0∈ R(T⋆)implies
h0=PR(T⋆)h0+(I−PR(T⋆))h0∈ R(T⋆)+R(T⋆)⊥.
Besides,h0∈ R(T⋆) +R(T⋆)⊥impliesPR(T⋆)h0∈ R(T⋆)recalling H=R(T⋆)/circleplustextR(T⋆)⊥. This ﬁnishes
proving that the solution of ( 15) exists if and only if h0∈ R(T⋆)+R(T⋆)⊥.
Finally, recall h0∈R(T⋆)using Lemma 1 . Thus,h0∈ R(T⋆) +R(T⋆)⊥impliesh0∈ R(T⋆)since if
h0=h0,2+h0,3,h0,2∈ R(T⋆),h0,3∈ R(T⋆)⊥, we have h0,3=h0−h0,2∈R(T⋆)∩R(T⋆)⊥={0}.
The statement is concluded by the fact h0∈ R(T⋆)impliesr0∈ R(T T⋆).
E.4 Proof of Lemma 5
Clearly, each element in Z ∩(X,Y)is a saddle point over X′,Y′since this is a saddle point over X,Y. Therefore,
ZX∩X′⊂argmin
x∈X′max
y∈Y′f(x,y).
Now, we prove the second statement. Let (x0,y0)be an element in Z ∩(X,Y)(this exists and this is a saddle
point). Then, take:
˜x∈argmin
x∈X′sup
y∈Y′f(x,y),˜y∈argmax
y∈Y′inf
x∈X′f(x,y).
Since(˜x,˜y)is a saddle point over X′,Y′, we have
f(x0,y0)≥f(x0,˜y)≥f(˜x,˜y)≥f(˜x,y0)≥f(x0,y0).
Then, the above inequalities are equalities. Hence, we have
f(x0,˜y) =f(˜x,y0), f(x0,y0) =f(x0,˜y)
This means that
˜x∈ Z′
X⊂ X′.
recalling y0∈argmaxy∈Yminx∈Xf(x,y).
18E.5 Proof of Theorem 1
We show two proofs.
First Proof. We use Lemma 5 . First, using ( 11),
{h0} ⊂argmin
h∈Hmax
g∈GL(h,g).
Second, we use ( 12). Here, recalling the proof of Lemma 3 , we have
(Nh0(T∗),h0))∈argmax
g∈Gargmin
h∈HL(h,g).
Therefore, using Lemma 3 , we have
argmin
h∈Hmax
g∈GL(h,g)⊂ {h0}.
Hence,
argmin
h∈Hmax
g∈GL(h,g) ={h0}.
Second Proof. We give more direct proof to show the ﬁnite sample result late r.
We take some element ¯g0fromNh0(T⋆)∩G. This satisﬁes T∗¯g0=h0. We deﬁne
L(h,g):= 0.5E[h2(X)]+E[g(Z){Y−h(X)}],
ˆg(h):= argmax
g∈GL(h,g),ˆh:= argmin
h∈Hsup
g∈GL(h,g),
andˆg:= ˆg(ˆh). Hence, for any h∈ H,
L(h,¯g0)−L(h0,¯g0)
= 0.5E[h2(X)]+E[¯g0(Z){Y−h(X)}]−0.5E[{h0}2(X)]0E[¯g0(Z){Y−h0(X)}]
= 0.5E[h2(X)]+E[¯g0(Z){h0(X)−h(X)}]−0.5E[{h0}2(X)]
= 0.5E[h2(X)]+E[h0(X){h0(X)−h(X)}]−0.5E[{h0}2(X)] (We useE[h0(X)|Z] = ¯g0(Z))
= 0.5E[{h(X)−h0(X)}2].
Therefore, for any h∈ H,
E[{h(X)−h0(X)}2] =L(h,¯g0)−L(h0,¯g0). (16)
Furthermore,
L(ˆh,ˆg)≥L(ˆh,¯g0) (Construction of estimators)
≥L(h0,¯g0) (Saddle point property)
≥L(h0,ˆg(h0)). (Saddle point property)
19Since we have L(ˆh,ˆg)≤L(h0,ˆg(h0))from the deﬁnition, all of the above inequalities are equali ties. Then, we
have
L(ˆh,¯g0)−L(h0,¯g0) = 0. (17)
In conclusion, combining ( 16) with ( 17), we have
E[{ˆh(X)−h0(X)}2]≤L(ˆh,¯g0)−L(h0,¯g0) = 0.
Hence,ˆh(X) =h0(X).
F Proof of Section 5
F.1 Proof of Theorem 2
We take some element ¯g0fromNh0(T⋆)∩G. This satisﬁes T∗¯g0=h0. We deﬁne
L(h,g):= 0.5E[h2(X)]+E[g(Z){Y−h(X)}],
Ln(h,g):= 0.5En[h2(X)]+En[g(Z){Y−h(X)}],
ˆg(h):= argmax
g∈G0.5En[h2(X)]+En[g(Z){Y−h(X)}],
M(H,G):= sup
h∈H,g∈G|(En−E)[{Y−h(X)}g(Z) +0.5h(X)2]|.
Using ( 16), recall for any h∈ H, we have
E[{h(X)−h0(X)}2] =L(h,¯g0)−L(h0,¯g0).
Here, we have
Ln(ˆh,ˆg(ˆh))≥Ln(ˆh,¯g0) (Construction of estimators)
≥L(ˆh,¯g0)−M(H,G)
≥L(h0,¯g0)−M(H,G) (Saddle point property)
≥L(h0,ˆg(h0))−M(H,G) (Saddle point property)
≥Ln(h0,ˆg(h0))−2M(H,G)
≥Ln(ˆh,ˆg(ˆh)−2M(H,G). (Construction of estimators.)
Therefore, we have
L(ˆh,¯g0)−L(h0,¯g0)≤2M(H,G).
Finally, we have
E[{ˆh(X)−h0(X)}2]≤L(ˆh,¯g0)−L(h0,¯g0)≤2M(H,G).
20F.2 Proof of Corollary 2
We calculate the following empirical process term:
sup
h∈H,g∈G|(En−E)[{Y−h(X)}g(Z) +0.5h(X)2]|.
Then, from Wainwright (2019 , Theorem 4.10), this is upper-bounded by
c/braceleftBig
Rn(A1)+Rn(A2)+Rn(A3)+(CG+CH)CH/radicalbig
ln(1/δ)/n/bracerightBig
where
A1={yg(z);g∈ G},A2={h(x)g(z);h∈ H,g∈ G},A3={0.5h(x)2;h∈ H}.
First, we have
Rn(A1)/lessorsimilarCGRn(G).
Secondly, we have
Rn(A2)/lessorsimilar(CH+CG)(Rn(G)+Rn(H)).
Here, we use the proof of Kallus et al. (2021 , Proof of Corollary 3). Thirdly, we have
Rn(A3)/lessorsimilar2CHRn(H).
Combining all results together, the empirical process term is upper-bounded by
c/braceleftBig
(CH+CG)(Rn(G)+Rn(H))+(CG+CH)CH/radicalbig
ln(1/δ)/n/bracerightBig
.
F.3 Proof of Corollary 4
We combine the Dudley integral Theorem 5 with Corollary 2.
F.4 Proof of Theorem 3
We take some element ¯g0fromNh0(T⋆)∩G. This satisﬁes T∗¯g0=h0.
L(h,g):= 0.5E[h2(X)]+E[g(Z){Y−h(X)}],
Ln(h,g):= 0.5En[h2(X)]+En[g(Z){Y−h(X)}],
ˆg(h):= argmax
g∈G0.5En[h2(X)]+En[g(Z){Y−h(X)}],
M(H,G):= sup
h∈H,g∈G|(En−E)[{Y−h(X)}g(Z) +0.5h(X)2]|.
andˆg= ˆg(ˆh). Recall for any h∈ H,
E[{h(X)−h0(X)}2]≤L(h,¯g0)−L(h0,¯g0).
21Furthermore,
L(ˆh,¯g0)−L(h0,¯g0)
=−L(h0,¯g0)+L(h†,ˆg(h†))/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(a)−L(h†,ˆg(h†))+L(ˆh,g†)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(c)−L(ˆh,g†)+L(ˆh,¯g0)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
(f).
Term (a) is upper-bounded as follows:
L(h†,ˆg(h†))−L(h0,¯g0)≤0.5E[{h†}2(X)]+/bardblˆg(h†)/bardbl2/bardblh0−h†/bardbl2−0.5E[h2
0(X)]
≤0.5E[{h†}2(X)]+sup
g/bardblg/bardbl2/bardblh0−h†/bardbl2−0.5E[h2
0(X)]
≤0.5/bardblh†+h0/bardbl2/bardblh†−h0/bardbl2+{sup
g/bardblg/bardbl2}/bardblh0−h†/bardbl2
≤ {2CH+CG}/bardblh0−h†/bardbl2.
Term (c) is upper-bounded as follows:
−L(h†,ˆg(h†))+L(ˆh,g†)≤ −L(h†,ˆg(h†))+Ln(h†,ˆg(h†))−Ln(h†,ˆg(h†))+Ln(ˆh,g†)−Ln(ˆh,g†)+L(ˆh,g†)
≤M(H,G)+(−Ln(h†,ˆg(h†))+Ln(ˆh,ˆg))+M(H,G)
≤2M(H,G).
The term (f) is upper-bounded as follows:
L(ˆh,¯g0)−L(ˆh,g†)≤ /bardblˆh/bardbl2/bardblg0−g†/bardbl2≤CH/bardblg0−g†/bardbl2
In conclusion, we have
E[{ˆh(X)−h0(X)}2]≤L(ˆh,¯g0)−L(h0,¯g0)≤ {2CH+CG}/bardblh0−h†/bardbl2+CH/bardblg0−g†/bardbl2+2M(H,G).
G Proof of Section 6
G.1 Proof of Rate in Section 6.1
Recall
ˆhpro= argmin
h∈Hmax
g∈G˜Ln(h,g),˜Ln(h,g):=−0.5En[g2(Z)]+En[{Y−h(X)}g(Z)].
Let
ˆgh:= argmax
g∈G˜Ln(h,g), gh:=E[Y−h(X)|Z=·],
Γ(h,g):=−0.5g2(Z)+{Y−h(X)}g(Z), κ(h,g):= Γ(h,g)−Γ(h,gh).
First Step. Our goal is to show
∀h∈ H;|En[κ(h,ˆgh)]|/lessorsimilar(C2
H+C2
G)ln(|G|/δ)
n. (18)
We ﬁxhhereafter.
22Here, ﬁrst, we have
E[κ(h,ˆgh)] = 0.5E[(ˆgh−gh)2(Z)].
Then,
E[κ(h,ˆgh)]≤En[κ(h,ˆgh)]+|(E−En)[κ(h,ˆgh)]|
≤ |(E−En)[κ(h,ˆgh)]|.
From the ﬁrst line to the second line, we use the deﬁnition of t he estimator and gh∈ G.
Now, we use Bernstein’s inequality. With probability 1−δ, we have
∀g∈ G,∀h∈ H;(E−En)[κ(h,g)]≤/radicalbigg
var[κ(h,g)]ln(|G||H|/δ)
n+(C2
H+C2
G)ln(|G||H|/δ)
n.
In the following, we condition on this event. Then, we have
E[κ(h,ˆgh)]/lessorsimilar/radicalbigg
var[κ(h,ˆgh)]ln(|G||H|/δ)
n+(C2
H+C2
G)ln(|G||H|/δ)
n. (19)
Here, we have
var[κ(h,ˆgh)] =E[{Γ(h,gh)−Γ(h,ˆgh)}2]
=E[0.25{ˆgh(Z)−gh(Z)}2{ˆgh(Z)+gh(Z)}2]
≤C2
GE[{ˆgh(Z)−gh(Z)}2].
Therefore, combining the above with (19), we obtain
E[{ˆgh(Z)−gh(Z)}2]/lessorsimilar(C2
H+C2
G)ln(|G||H|/δ)
n.
Hence,
|En[κ(h,ˆgh)]| ≤ |E[κ(h,ˆgh)]|+|(En−E)[κ(h,ˆgh)]|
= 0.5E[{ˆgh(Z)−gh(Z)}2]+|(En−E)[κ(h,ˆgh)]|
/lessorsimilar(C2
H+C2
G)ln(|G||H|/δ)
n+|(En−E)[κ(h,ˆgh)]|
/lessorsimilar(C2
H+C2
G)ln(|G||H|/δ)
n. (Use (19))
Second Step. We deﬁne
Ξ(h) := Γ(h,gh)−Γ(h0,gh0).
NoteΓ(h0,gh0) = 0 sinceh0. Furthermore,
E[Ξ(h)] =E[g2
h(Z)],E[Ξ2(h)]≤(C2
H+C2
G)E[g2
h(Z)]. (20)
Then,
E[Ξ(ˆh)]≤En[Ξ(ˆh)]+|(E−En)[Ξ(ˆh)]|
23Here, using the ﬁrst conclusion (18), we get
En[Ξ(ˆh)] =En[Γ(ˆh,gˆh)−Γ(h0,0)]
≤En[Γ(ˆh,ˆgˆh)−Γ(h0,ˆgh0)]+c(C2
H+C2
G)ln(|G||H|/δ)
n
≤c(C2
H+C2
G)ln(|G||H|/δ)
n.
From the ﬁrst line to the second line, we use h0∈ H and(18). From the second line to the third line, we use the
construction of the estimator.
Therefore,
E[Ξ(ˆh)]≤c(C2
H+C2
G)ln(|G||H|/δ)
n+|(E−En)[Ξ(ˆh)]|.
Here, we use Bernstein’s inequality. With probability 1−δ, we have
∀h∈ H;|(E−En)[Ξ(h)]| ≤/radicalbigg
var[Ξ(h)]ln(|H|/δ)
n+c(C2
H+C2
G)ln(|H|/δ)
n.
Hereafter, we condition on this event. Thus, using ( 20), we have
E[g2
ˆh(Z)]≤/radicalBigg
g2
ˆh(Z)ln(|H|/δ)
n+c(C2
H+C2
G)ln(|H||G|/δ)
n.
Therefore, by some algebra, we obtain
E[g2
ˆh(Z)]/lessorsimilar(C2
H+C2
G)ln(|H||G|/δ)
n.
H Proof of Section A
H.1 Proof of Theorem 4
We use the notation in Theorem 2 . Takeµnsuch that 2M(H,G)≤µnholds with probabiltiy 1−δ. We condition
on this event.
The guarantee in terms of projected MSEs is straightforward as long as h0is included in the conﬁdence ball
Hnwith probability 1−δby following the proof in Section 6 . In fact, we have
Ln(h0,ˆg(h0))−min
hLn(h,ˆg(h)) =Ln(h0,ˆg(h0))−Ln(ˆh,ˆg(ˆh))
≤Ln(h0,ˆg(h0))−Ln(ˆh,g(ˆh))
=Ln(h0,ˆg(h0))−L(h0,ˆg(h0)))+L(h0,ˆg(h0)))−L(ˆh,g(ˆh))+L(ˆh,g(ˆh))−Ln(ˆh,g(ˆh))
≤Ln(h0,ˆg(h0))−L(h0,ˆg(h0)))+L(h0,g(h0)))−L(ˆh,g(ˆh))+L(ˆh,g(ˆh))−Ln(ˆh,g(ˆh))
≤2M(H,G)≤µn.
Hence,h0∈ Hn.
24Next, we prove the L2convergence guarantee. Here, for any ˆhin the conﬁdence ball Hn, we have
Ln(ˆh,ˆg(ˆh))≥Ln(ˆh,¯g0) (Construction of estimators)
≥L(ˆh,¯g0)−M(H,G)
≥L(h0,¯g0)−M(H,G) (Saddle point property)
≥L(h0,ˆg(h0))−M(H,G) (Saddle point property)
≥Ln(h0,ˆg(h0))−2M(H,G)
≥min
hLn(h,ˆg(h))−2M(H,G)
≥Ln(ˆh,ˆg(ˆh))−2M(H,G)−µn.
Therefore,
Ln(h0,ˆg(h0))−min
hLn(h,ˆg(h))≤2M(H,G)+µn.
Hence, the L2rate guarantee is ensured since
E[{ˆh(X)−h0(X)}2]≤L(ˆh,¯g0)−L(h0,¯g0)≤2M(H,G)+µn.
I Auxiliary Lemmas
Lemma 6. (x∗,y∗)is a saddle point of f(x,y)over(X,Y)if and only if the strong duality holds and
x∗∈argmin
x∈Xmax
y∈Yf(x,y), y∗∈argmax
y∈Ymin
x∈Xf(x,y).
Proof. Suppose(x∗,y∗)is a saddle point of f(x,y)overX,Y. Then,
inf
x∈Xsup
y∈Yf(x,y)≤sup
y∈Yf(x∗,y)≤f(x∗,y∗)≤inf
x∈Xf(x,y∗)≤sup
y∈Yinf
x∈Xf(x,y).
Hence, the strong duality holds. The above inequalities are actually equalities. Therefore,
inf
x∈Xsup
y∈Yf(x,y) = sup
y∈Yf(x∗,y) =f(x∗,y∗) = inf
x∈Xf(x,y∗) = sup
y∈Yinf
x∈Xf(x,y).
Hence, we have
x∗∈argmin
x∈Xsup
y∈Yf(x,y), y∗∈argmax
y∈Yinf
x∈Xf(x,y).
Next, suppose the strong duality holds, and
x∗∈argmin
x∈Xmax
y∈Yf(x,y), y∗∈argmax
y∈Yinf
x∈Xf(x,y).
Then, we have
max
y∈Yinf
x∈Xf(x,y) = inf
x∈Xf(x,y∗)≤f(x∗,y∗)≤sup
y∈Yf(x∗,y) = min
x∈Xsup
y∈Yf(x,y).
25Finally, using the strong duality, the above is actually equ ality. Hence,
inf
x∈Xf(x,y∗) =f(x∗,y∗),sup
y∈Yf(x∗,y) =f(x∗,y∗).
This implies (x∗,y∗)is a saddle point since
∀x∈ X,∀y∈ Y;f(x,y)≥f(x∗,y∗)≥f(x∗,y).
Theorem 5 (Dudley integral) .Consider a function class Fcontaining f:X →R. Then, we have
Rn(F)≤inf
ǫ≥0/braceleftBigg
4ǫ+12/integraldisplay/bardblF/bardbl∞
ǫ/radicalbigg
lnN(τ,F,/bardbl·/bardbl∞)
ndτ/bracerightBigg
.
26