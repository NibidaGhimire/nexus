Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition
via approximate matrix multiplication
Maolin Che1, Yimin Wei2and Hong Yan3
1School of Mathematics, Southwestern University of Finance and
Economics, Chengdu, 611130, P. R. of China.
2School of Mathematical Sciences and Key Laboratory of
Mathematics for Nonlinear Sciences, Fudan University, Shanghai,
200433, P. R. of China.
3Department of Electrical Engineering and Center for Intelligent
Multidimensional Data Analysis, City University of Hong Kong,
83 Tat Chee Avenue, Kowloon, Hong Kong.
Contributing authors: chncml@outlook.com and
cheml@swufe.edu.cn; ymwei@fudan.edu.cn; h.yan@cityu.edu.hk;
Abstract
This paper develops fast and ecient algorithms for computing Tucker
decomposition with a given multilinear rank. By combining random
projection and the power scheme, we propose two ecient ran-
domized versions for the truncated high-order singular value decom-
position (T-HOSVD) and the sequentially T-HOSVD (ST-HOSVD),
which are two common algorithms for approximating Tucker decom-
position. To reduce the complexities of these two algorithms, fast
and ecient algorithms are designed by combining two algorithms
and approximate matrix multiplication. The theoretical results are
also achieved based on the bounds of singular values of stan-
dard Gaussian matrices and the theoretical results for approximate
matrix multiplication. Finally, the eciency of these algorithms are
illustrated via some test tensors from synthetic and real datasets.
Keywords: Tucker decomposition, low rank approximations of tensors,
randomized T-HOSVD, randomized ST-HOSVD, power scheme, standard
Gaussian matrices, tensor contraction, random projection, nearly optimal
probabilities, uniform probabilities, approximate matrix multiplication
1arXiv:2303.11612v1  [math.NA]  21 Mar 2023Springer Nature 2021 L ATEX template
2 Ecient algorithms for Tucker decomposition
1 Introduction
It is natural to use higher-order tensors, multidimensional matrices, or mul-
tiway arrays to represent the data from a wide range of applications, such as
machine learning, signal processing, higher-order statistics, computer vision,
pattern recognition and recommendation system [1{5]. A tensor is denoted by
A2RI1I2INwith entries given by ai1i2:::iN2R, wherein= 1;2;:::;In
andn= 1;2;:::;N . In particular, when N= 1, a rst-order tensor Ais a
vector of size I1, and when N= 2, a second-order tensor Ais a matrix of size
I1I2.
We rst introduce some notations. We use lower case letters (e.g. x;u;v )
for scalars, lower case bold letters (e.g. x;u;v) for vectors, bold capital letters
(e.g.A;B;C) for matrices, and calligraphic letters A;B;C;:::for tensors. This
notation is consistently used for the lower-order parts of a given structure. For
example, the entry with row index iand column index jin a matrix A, i.e.,
(A)ij, is represented as aij(also ( x)i=xiand (A)i1i2:::iN=ai1i2:::iN).
The symbols A>,Ay,kAkFandkAk2are, respectively, denoted by the
transpose, the Moore-Penrose pseudoinverse, the Frobenius norm and the spec-
tral norm of A2RIJ. We use IJto denote the identity matrix in RJJ.
The Kronecker product of matrices A2RIJandB2RKLis represented
asA
B. An orthogonal projection P2RIIis dened as P2=Pand
P>=P. A matrix Q2RIK(I > K ) is orthonormal if Q>Q=IK. We
useSNto denote the Nth-order symmetric group on the set f1;2;:::;Ng. We
use the interpretation of O() to refer to the class of functions the growth of
which is bounded above and below up to a constant. The term \for each n"
meansn= 1;2;:::;N . The term \for a given n" means the integer nsatises
1nN. For an event X, we use P(X) and E(X) to denote its probability
and expectation, respectively.
AsNor someIn's increase, it is impossible to deal with tensors directly.
Hence, it is necessary to use fewer parameters to eectively represent these
large-scale tensors. Tensor decomposition is one of the most eective meth-
ods to represent large-scale data. Unlike low rank matrix approximations,
there exist several methods for low rank approximations of tensors, such
as CANDECOMP/PARAFAC (CP) decomposition [6], Tucker decomposition
[7], Hierarchical Tucker decomposition [8, 9], tensor train decomposition [10],
t-SVD [11], tensor ring decomposition [12, 13] and fully connected tensor net-
work decomposition [14]. In this paper, we focus on the computation of low
multilinear rank approximations, which can be viewed as a special case of
Tucker decomposition.
With the desired multilinear rank, we consider the computation of Tucker
decomposition with all the mode- nfactor matrices being orthonormal, which
is also called the low rank approximation of a tensor in the Tucker format.
Problem 1. Suppose thatA2RI1I2IN. For given Npositive integers
f1;2;:::;NgwithnIn, the goal is to nd Northonormal matricesSpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 3
Qn2RInnsuch that
ai1i2:::iNI1;I2;:::;INX
j1;j2;:::;jN=1aj1j2:::jN(P1)i1j1(P2)i2j2:::(PN)iNjN;
where Pn=QnQ>
n2RInInis an orthogonal projection.
Assume thatfQ1;Q2;:::;QNgis a solution of Problem 1. Then the tensor
A 1(Q1Q>
1)2(Q2Q>
2)N(QNQ>
N) is called a low rank approximation in
the Tucker format to A. The developed algorithms for solving Problem 1 can
be divided into iterative algorithms and direct algorithms. Iterative algorithms
include the higher-order orthogonal iteration [15], the Newton-Grassmann
method [16], the Riemannian trust-region method [17], the Quasi-Newton
method [18], the semi-denite programming (SDP) [19], and the Lanczos-type
iteration [20, 21]. The truncated higher-order singular value decomposition (T-
HOSVD) [22] and the sequentially truncated HOSVD (ST-HOSVD) [23] are
two direct algorithms to nd an approximate solution of Problem 1.
Recently, many researchers established the randomized variants of both T-
HOSVD and ST-HOSVD. By using the randomized range nder algorithm [24]
to nd the factor matrix from the corresponding mode unfolding, Zhou et al.
[25] developed a distributed randomized Tucker decomposition approach for
arbitrarily big tensors with relatively low multilinear ranks, and Minster et al.
[26] presented randomized versions of T-HOSVD and ST-HOSVD, and gave a
detailed probabilistic analysis of the error in using both algorithms. As shown
in [26{28], the adaptive randomized versions of T-HOSVD and ST-HOSVD
are obtained by using the adaptive randomized range nder [24] for computing
each mode factor matrix from the corresponding mode unfolding. By applying
a randomized linear map to a tensor, Sun et al. [29] obtained a sketch that
captures the important directions within each mode, and then described a
new algorithm for computing a low rank approximation of the tensor in the
Tucker format. The main dierence between the work in [27] and that of [26]
is that the standard Gaussian vector is applied in [26] and the Kronecker
product of standard Gaussian vectors is exploited in [27]. After obtaining all
the factor matrices, Sun et al. [29] further considered an ecient algorithm
for computing the core tensor. Ahmadias et al. [30] provided more ways to
approximate the rst nleft singular vectors of the mode- nunfolding. Che et
al.[31] investigated the randomized versions of T-HOSVD and ST-HOSVD,
where the Kronecker product of the standard Gaussian matrices are employed
in the randomized range nder algorithm. When the singular values of each
unfolding of the original tensor decay slowly, an ecient randomized algorithm
combining the result of [31] and the power scheme is presented in [32]. The
storage of the Kronecker product of standard Gaussian matrices to project
each mode unfolding is less than that by the standard Gaussian matrix and the
Khatri-Rao product of standard Gaussian matrices to project the unfolding.Springer Nature 2021 L ATEX template
4 Ecient algorithms for Tucker decomposition
As shown in [32, 33], when the randomized algorithms are designed by com-
bining random projection and power scheme to obtain all the factor matrices,
the accuracy of these algorithms are competitive with that of some deter-
ministic algorithms, such as HOOI, T-HOSVD and ST-HOSVD. However,
these algorithms are slower than T-HOSVD and ST-HOSVD. Meanwhile, the
randomized algorithms without the power scheme are faster than HOOI, T-
HOSVD and ST-HOSVD, but may be worse than these algorithms for some
test tensors. Hence, the main research is to design fast and ecient algorithms
for computing the low multilinear rank approximation of a tensor with a known
multilinear rank. Numerical examples illustrate that the proposed algorithms
are faster than HOOI, T-HOSVD and ST-HOSVD, and their accuracy are
competitive with that of these deterministic algorithms.
Without loss of generality, the processing order used in ST-HOSVD is set
tof1;2;:::;Ng. We now give some dierences between our algorithms and the
existing randomized algorithms:
(a) In our algorithms, to generate Nstandard Gaussian matrices needsPN
n=1O(In(n+K)) operations; in the randomized variant of T-
HOSVD [26], to generate Nstandard Gaussian matrices requiresPN
n=1O(I1:::In 1In+1:::IN(n+K)) operations; and in the random-
ized variant of ST-HOSVD [26], to generate Nstandard Gaussian matrices
requiresPN
n=1O(1:::n 1In+1:::IN(n+K)) operations. For each n,
the random matrix used in [27] is the Khatri-Rao product of standard
Gaussian matrices and to generate this random matrix needs O((I1++
In 1+In+1++IN)(n+K)) operations for the T-HOSVD framework
andO((1++n 1+In+1++IN)(n+K)) operations for the
ST-HOSVD framework. Meanwhile, the random matrix used in [31, 32] is
the Kronecker product of standard Gaussian matrices and to generate this
matrix needs O(PN
m=1;m6=n(ImTm)) operations for the T-HOSVD frame-
work andO(1Tn;1++n 1In;n 1+In+1In;n+1++INTn;N) operations
for the ST-HOSVD framework, where Tn;1:::Tn;n 1Tn;n+1:::Tn;Nis the
smallest integer greater than n+K.
(b) For each n, in our algorithms, each factor matrix Qnis obtained by apply-
ing the thin singular value decomposition (SVD) to the matrix Cn, which is
the same as the way to obtain the factor matrix Qnin [31{33], and in ran-
domized variants of T-HOSVD and ST-HOSVD, the matrix Qnis obtained
by this process: 1) to compute the thin QR decomposition: QR=Cn, 2)
to compute the thin SVD: UV>=Q>A(n), and 3) to obtain the factor
matrix Qn=QU(:;1 :n).
(c) For each n, in Algorithms 1 and 4, the matrix Cnis computed as Cn=
(A(n)A>
(n))qGn;1, and when power iterations are involved in randomized
variants of T-HOSVD and ST-HOSVD, the matrix Cnis computed as Cn=
(A(n)A>
(n))qA(n)Gn;2, which is the same as in [32], except for the way to
form the matrix Gn;2. Sometimes, the cost to generate Gn;1is far less than
that to generate Gn;2.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 5
(d) By combining the randomized algorithms for low rank approximations [34,
35] in the framework of T-HOSVD and ST-HOSVD, we obtain the upper
bounds for the accuracy of the proposed algorithms. The theoretical bounds
for the accuracy of randomized variants of T-HOSVD and ST-HOSVD are
based on that for randomized SVD in [36].
The rest of the paper is organized as follows. In Section 2, we overview
the basic denitions of tensors and some results for Tucker decomposition.
In Sections 3 and 4, we respectively describe randomized versions of T-
HOSVD and ST-HOSVD by combining random projection, the power scheme
and approximate matrix multiplication. The upper bounds of kA bAk2
Fare
obtained in Section 5, where bAis a low rank approximation in the Tucker
format obtained by the proposed algorithms. We illustrate the eciency and
accuracy of the proposed algorithms via some test tensors from synthetic and
real datasets in Section 6 and conclude this paper in Section 7.
2 Preliminaries
We recommend [1{3, 28, 37] for a thorough survey of basic denitions of ten-
sors. For a given n, the mode- nproduct ofA2RI1I2INbyB2RJnIn,
denoted byAnB, isC2RI1In 1JnIn+1IN, where its entries are
given by
ci1:::in 1jin+1:::im=InX
in=1ai1:::in 1inin+1:::iNbjin:
Suppose thatA2RI1I2IN,F2RJnIn,G2RJmImandH2RJ0
nJn.
For eachmandnwithm6=n, we have
(AnF)mG= (AmG)nF=AnFmG;(AnF)nH=An(HF):
For two tensorsA;B2RI1I2IN, the Frobenius norm of a tensorAis
given bykAkF=p
hA;Aiand the scalar product hA;Biis dened as
hA;Bi=I1;I2;:::;INX
i1;i2;:::;iN=1ai1i2:::iNbi1i2:::iN:
For a given m2f1;2;:::;Mgandn, the mode-( n;m) product [10] (called
tensor contraction ) of two tensorsA2RI1I2INandB2RJ1J2JM
with common modes In=Jmproduces an order ( M+N 2) tensor
C=Am
nB2RI1In 1In+1INJ1Jm 1Jm+1JM;Springer Nature 2021 L ATEX template
6 Ecient algorithms for Tucker decomposition
where its entries are given by
ci1:::in 1in+1:::iNj1:::jm 1im+1:::jN=InX
in=1ai1:::in 1inin+1:::iNbj1:::jm 1injm+1::::::j M:
For a given n, the mode- nunfolding matrix of A2RI1I2IN, denoted
byA(n), arranges the mode- nbers into the columns of a matrix. More
specically, a tensor element ( i1;i2;:::;iN) maps on a matrix element ( in;j),
where
j=i1+ (i2 1)I1++ (in 1 1)I1:::In 2+ (in+1 1)I1:::In 1
++ (iN 1)I1:::In 1In+1:::IN 1:
2.1 Tucker decomposition: a review
For given Npositive integers fR1;R2;:::;RNgwithRn< In,Tucker
decomposition [7] of a tensorA2RI1I2INis dened as
AG 1U(1)2U(2)NU(N); (1)
where U(n)2RInRnare called the mode-nfactor matrices andG 2
RR1R2RNis called the core tensor . In general, the matrix Unis not be
restricted to be orthonormal. Hence, the low rank approximation in the Tucker
format from any solution of Problem 1 is a special case of Tucker decompo-
sition. Two other special cases are the nonnegative Tucker decomposition (cf.
[38]) and the higher-order interpolatory decomposition (cf. [39, 40]).
For eachn, Tucker decomposition is closely related to the mode- nunfolding
matrix A(n)2RInI1:::In 1In+1:::IN. In particular, the relation (1) implies
A(n)U(n)G(n)(U(N)

 U(n+1)
U(n 1)

 U(1))>;
where G(n)2RRnR1:::Rn 1Rn+1:::RN.
It follows that the rank of A(n)is less than or equal to Rn, as the mode- n
factor U(n)2RInRnat most has rank Rn. This motivates us to dene the
multilinear rank of Aas the tuple
fR1;R2;:::;RNg;where the rank of A(n)is equal toRn:
A classical algorithm for computing Tucker decomposition with all the
factor matrices being orthonormal is named after the T-HOSVD [37]. This
algorithm is not optimal in terms of giving the best tting as measured by
the norm of the dierence, but it is a good starting point for the HOOI
[15] to compute the Tucker decomposition with orthonormal factor matrices.
Vannieuwenhoven et al. [23] presented an alternative strategy for truncating
the HOSVD, which is denoted by ST-HOSVD. In contrast to the T-HOSVDSpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 7
algorithm that processes the modes independently, the ST-HOSVD algorithm
processes the modes sequentially with a given processing order in SN.
3 Randomized T-HOSVD with power scheme
By using the Frobenius norm of a tensor, we can rewrite Problem 1 as follows,
(
maxA 1Q>
12Q>
2NQ>
N2
F;
subject to Qn2RInnis orthonormal with n= 1;2;:::;N:(2)
According to [15], researching a solution for (2) is equivalent to nd a ten-
sorbA2RI1I2INthat minimizes the least-squares cost function f(bA) =
kA bAk2
F, with the constraints rank( bA(n)) =n, wherebA(n)is the mode- n
unfolding ofbA.
LetfQ1;Q2;:::;QNgbe a solution for (2). As shown in [23, 40], we have
A A 1 
Q1Q>
1
2 
Q2Q>
2
N 
QNQ>
N2
F
NX
n=1A An 
QnQ>
n2
F:(3)
Hence, an approximate solution fQ1;Q2;:::;QNgfor (2) can be obtained
by nding an orthonormal matrix Qn2RInnsuch that
Qn= argminA An 
QQ>2
F= argminA(n)  
QQ>
A(n)2
F;
subjectto Q2RInnis orthonormal ;n= 1;2;:::;N:
3.1 The proposed algorithm
According to the above descriptions, for each n, the columns of the matrix Qn
are the rst nleft singular vectors of A(n). Note that computing the rst n
left singular vectors is prohibitive for high dimensions. For each n, through
Algorithm 1, we obtain the approximation for the rst nleft singular vectors
ofA(n). In detail, rstly, we use a standard Gaussian matrix Gto project the
matrix ( A(n)A>
(n))q, whereq1 is a given integer; secondly, we obtain an
orthonormal matrix Qnfrom this projection matrix; and nally, we compute
the core tensorG=A 1Q>
12Q>
2NQ>
N.
Remark 1 In Algorithm 1, for each n, the matrix Cncan be computed by one of the
following two ways:
Strategy A: We rst compute C0=A>
(n)GnandCn=A(n)C0, and then
compute C0=A>
(n)CnandCn=A(n)C0with (q 1) times, which requires
4qI1:::In 1In:::IN(n+K) operations.Springer Nature 2021 L ATEX template
8 Ecient algorithms for Tucker decomposition
Algorithm 1 Solving Problem 1 via random projection and power scheme
Input : A tensorA 2 RI1I2IN, the desired multilinear rank
f1;2;:::;Ng, the oversampling parameter Kand an integer q1.
Output :Northonormal matrices Qnand a core tensor Gsuch thatA
G 1Q12Q2NQN.
1:Forn= 1;2;:::;N
2:Generate a standard Gaussian matrix G2RIn(n+K).
3:Arrange the mode- nunfolding A(n)ofA.
4:Compute Cn= (A(n)A>
(n))qGn.
5:Find an orthonormal matrix Qn2RInnsuch that there exists Sn2
Rn(n+K)for which
kQnSn CnkFn+1(Cn) =0
@n+KX
k=n+1k(Cn)21
A1=2
;
wherek(Cn) is thekth singular value of Cn.
6:End for
7:Compute the core tensor G=A 1Q>
12Q>
2NQ>
N.
Strategy B: We rst compute C0=A(n)B>
(A)and let Cn=Gn, and then
compute Cn=C0Cnwithqtimes, which requires 2 I1:::In 1I2
n:::IN+
2qI2
n(n+K) operations.
In Problem 1, for each n, we assume that nIn. Hence, for some small qandK,
we can ensure that 2 q(n+K)<In. Numerical examples illustrate that the case of
q= 1 andK= 10 is suitable for Algorithm 1. Hence, in the rest, we use Strategy A
to compute the matrix Cn.
We now study the computational complexity of Algorithm 1. For clarity,
we assumeI1I2INIand12Nin complexity
estimates [20, Page A2], where nis the number of the columns of Qn, and
InImeansIn=nIfor some constant n. To count the oating-point
operations of Algorithm 1, for each n, we evaluate the complexity for each step:
(a) to form a standard Gaussian matrix GnrequiresI(+K) operations;
(b) it needs O(IN) operations to form the matrix A(n);
(c) it requires 4 qIN(+K) operations to compute the matrix Cn; and
(d) computing QndemendsO(I(+K)) operations.
Meanwhile, it costs 2 I(IN 1+IN 2++IN 2+N 1) operations to
obtain the core tensor G. Then, by summing up the computational complexitySpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 9
for eachn, Algorithm 1 requires
N 
I(+K) +O(IN) + 4qIN(+K) +O(I2+IK)
+ 2I(IN 1+IN 2++IN 2+N 1)
operations for the tensor A.
3.2 Accelerating Algorithm 1 with approximate matrix
multiplication
For eachn, the matrix Cnin Algorithm 1 is computed by ( B(n)B>
(n))qG,
which requires 4 qI1:::In 1In:::IN(n+K) operations. These operations are
enormous when all the Inare large. In this section, we introduce a fast sampling
algorithm to approximate the matrix Cn.
Assume that 0 p1;p2;:::;pI1 andPI
i=1pi= 1, then we say
p= (p1;p2;:::;pI)2[0;1]Iis a probability distribution. For a given ran-
dom variable 2f1;2;:::;Ig, we sayMULTINOMIAL( p) ifp2[0;1]I
is a probability distribution and P(=i) =pi. We also say S2RIL
RANDSAMPLE( L;p) ifLis a positive integer, p2[0;1]Iis a probability
distribution, and the entries of Sare dened as follows,
sij=8
<
:1pLpi;ifj=i;
0;otherwise;
wherejMULTINOMIAL( p),i= 1;2;:::;I andj= 1;2;:::;L .
Drineas et al. [41] investigated a famous randomized algorithm for approxi-
mating matrix multiplication, called the BasicMatrixMultiplication algorithm
(Algorithm 2).
Algorithm 2 BasicMatrixMultiplication algorithm [41, Fig. 2]
Input : Two matrices A2RI1I2andB2RI2I3, the number of sampling
Ksuch that 1KI2, and a probability distribution fpigI2
i=1.
Output : Two matrices C2RI1KandR2RKI3such that ABCR.
1:Form S2RI2KRANDSAMPLE( K;p).
2:Compute C=ASandR=S>B.
Remark 2 An important issue for Algorithm 2 is the choice of the probabilities
fpigI2
i=1:Springer Nature 2021 L ATEX template
10 Ecient algorithms for Tucker decomposition
(a) The probabilities fpigI2
i=1are the optimal probabilities if
pi=kA(i;:)k2kB(:;i)k2PI2
j=1kA(j;:)k2kB(:;j)k2;
(b) The probabilities fpigI2
i=1are the nearly optimal probabilities if for some
positive constant 1,
pikA(i;:)k2kB(:;i)k2PI2
j=1kA(j;:)k2kB(:;j)k2;
(c) The probabilities fpigI2
i=1are the uniform probabilities if pi= 1=I2.
There exist two situations when we count the computational complexity
of Algorithm 2: for the (nearly) optimal probabilities, to construct CandR
requiresO(K(I1+I2+I3)) operations, and for the uniform probabilities, it
needsO(K(I1+I3)) operations to obtain CandR.
Remark 3 In the above remark, we give one type of the nearly optimal probabilities.
Furthermore, if all the pisatisfy one of the following conditions
pikA(i;:)k2PI2
j=1kA(j;:)k2; pikB(:;i)k2PI2
j=1kB(:;j)k2;
pikA(i;:)k2
2
kAk2
F; pikB(:;i)k2
2
kBk2
F;orpikA(i;:)k2kB(:;i)k2
kAkFBkF;
then the probabilities fpigI2
i=1are the nearly optimal probabilities.
In Algorithm 2, the strategy used to generate Sis a random sampling.
There exist other types of random sampling: including subspace sampling [42]
and adaptive sampling [43, 44]. Recently, many researchers study various block
versions of Algorithm 2, and the interested readers can refer to [45{48] for their
details. Meanwhile, there exists some randomized algorithms for computing
the approximate matrix multiplication based on random projection [49{51],
including standard Gaussian, SpEmb (sparse subspace embedding) and SRFT
(subsampled randomized Fourier transform) matrices.
By using Algorithm 2 to compute an approximation of Cnin Algorithm
1, we obtain another ecient algorithm to compute a low multilinear rank
approximation of a tensor A2RI1I2IN, denoted by Algorithms 3.
To count the complexity of Algorithm 3, we assume that T1T2
TNT. Then, for each n, we have
(a) it needs O(IN) operations to form the matrix A(n);
(b) for the (nearly) optimal probabilities, to construct C0
nrequiresO((I+
IN 1)T) operations, and for the uniform probabilities, it needs O(IT)
operations to obtain C0
n;Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 11
Algorithm 3 Accelerating Algorithm 1 with approximating matrix multipli-
cation
Input : A tensorA 2 RI1I2IN, the desired multilinear rank
f1;2;:::;Ng, the oversampling parameters K,Npositive integers
fT1;T2;:::;TNg, an integer q1, andNprobability distributions
fpn;igI1:::In 1In+1:::IN
i=1 .
Output :Northonormal matrices Qnand a core tensor Gsuch thatA
G 1Q12Q2NQN.
1:Forn= 1;2;:::;N
2:Generate Sn2RI1:::In 1In+1:::INTnRANDSAMPLE( Tn;pn).
3:Arrange A(n)as the mode- nunfolding ofA.
4:Obtain C0
n=A(n)Sn.
5:Form a standard Gaussian matrix Gn2RIn(n+K).
6:Compute Cn= (C0
nC0>
n)qGn.
7:Find an orthonormal matrix Qn2RInnsuch that there exists Sn2
Rn(n+K)for which
kQnSn CnkFn+1(Cn):
8:End for
9:Compute the core tensor G=A 1Q>
12Q>
2NQ>
N.
(c) to generate the matrix Gn2RI(+K)requiresI(+K) operations;
(d) it requires 4 IT(+K) operations to compute the matrix Cn; and
(e) computing QnrequiresO(I(+K)) operations.
Meanwhile, it costs 2 I(IN 1+IN 2++IN 2+N 1) operations to
obtain the core tensor G. Then, Algorithm 3 requires
N(O(IN) + 4IT(+K) +I(+K) +O(I2+IK))
+NO(IT) + 2I(IN 1+IN 2++IN 2+N 1)
operations for the uniform probabilities, and
N(O(IN) + 4IT(+K) +I(+K) +O(I2+IK))
+NO((I+IN 1)T) + 2I(IN 1+IN 2++IN 2+N 1)
operations for the (nearly) optimal probabilities.Springer Nature 2021 L ATEX template
12 Ecient algorithms for Tucker decomposition
4 Randomized ST-HOSVD with power scheme
LetfQ1;Q2;:::;QNgbe a solution for (2). As shown in [23, 40], for a
processing orderfp1;p2;:::;pNg2SN, we have
A A 1 
Q1Q>
1
2 
Q2Q>
2
N 
QNQ>
N
=A Ap1 
Qp1Q>
p1
+Ap1 
Qp1Q>
p1
 Ap1 
Qp1Q>
p1
p2 
Qp2Q>
p2
++Ap1 
Qp1Q>
p1
p2 
Qp2Q>
p2
pN 1
QpN 1Q>
pN 1
 Ap1 
Qp1Q>
p1
p2 
Qp2Q>
p2
pN 
QpNQ>
pN
:
(4)
In particular, when N= 3 andA2CI1I2I3, the equality (4) appears in
[33]; and whenfp1;p2;:::;pNg=f1;2;:::;Ng, the equality (4) can be found
in [27]. For each n2, we have
Ap1 
Qp1Q>
p1
pn 1
Qpn 1Q>
pn 1
 Ap1 
Qp1Q>
p1
pn 1
Qpn 1Q>
pn 1
n 
QpnQ>
pn
=Ap1 
Qp1Q>
p1
pn 1
Qpn 1Q>
pn 1
n 
IIpn QpnQ>
pn
=
Ap1Q>
p1pn 1Q>
pn 1n 
IIpn QpnQ>
pn
p1Qp1pn 1Qpn 1;
which implies that
Ap1 
Qp1Q>
p1
pn 1
Qpn 1Q>
pn 1
n 
IIpn QpnQ>
pn
F
=
Ap1Q>
p1pn 1Q>
pn 1n 
IIpn QpnQ>
pn
p1Qp1pn 1Qpn 1
F

Ap1Q>
p1pn 1Q>
pn 1n 
IIpn QpnQ>
pn
F:
Here the inequality holds by the fact kAQkFkAkFforA2RIJand any
orthonormal matrix Q2RJKwithKJ. Hence, we have
A A 1 
Q1Q>
1
2 
Q2Q>
2
N 
QNQ>
N2
F
Ap1 
IIp1 Qp1Q>
p12
F+ 
Ap1Q>
p1
p2 
IIp2 Qp2Q>
p22
F
++
Ap1Q>
p1p2Q>
p2pN 1Q>
pN 1
pN
IIpN QpNQ>
pN2
F:
(5)
Another way to obtain an approximate solution fQ1;Q2;:::;QNgfor
(2) is to solve the following Nsubproblems with a given processing order
fp1;p2;:::;pNg2SN:Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 13
(a) For givenA2RI1I2INandp1<Ip1, to nd an orthonormal matrix
Qp12RIp1p1such that
Qp1= argminA Apn 
QQ>2
F;
s:t:Q2RIp1p1is orthonormal;
(b) For each n2, to nd an orthonormal matrix Qpn2CIpnpnwith
pn<Ipnsuch that
Qn= argminAp1Q>
p1pn 1Q>
pn 1pn 
IIpnQQ>2
F;
s:t:Q2RIpnpnis orthonormal :
It is worthly noting that choosing the best processing order is is a very
challenging problem for solving (2) eciently, which is beyond the scrope of
this paper. Here, we assume that the processing order is f1;2;:::;Ngand then
rewrite (5) as follows,
A A 1 
Q1Q>
1
2 
Q2Q>
2
N 
QNQ>
N2
F
A 1 
II1 Q1Q>
12
F+ 
A 1Q>
1
2 
II2 Q2Q>
22
F
++ 
A 1Q>
12Q>
2N 1Q>
N 1
N 
IIN QNQ>
N2
F:(6)
4.1 Framework of the proposed algorithm
According to the above description, we obtain Algorithm 4 for estimating
the rstnleft singular vectors of A(n). Comparison with Algorithm 1, in
Algorithm 4, after obtaining Qn, we updateAasA=AnQ>
n. Hence, when
all the matrices Qnare obtained by Algorithm 1, we need to compute the core
tensorGasG=A 1Q>
12Q>
2NQ>
N; and when all the matrices Qnare
obtained by Algorithm 4, the core tensor Gis also obtained. Another dierence
between Algorithms 1 and 4 is that Algorithm 1 can be easily implemented in
a parallel computing environment.
To count the oating-point operations of Algorithm 4, we evaluate the
complexity for each n:
(a) to form a standard Gaussian matrix GnrequiresI(+K) operations;
(b) it needs O(n 1IN n+1) operations to form the matrix A(n);
(c) it demands 4 I(+K+T)(+K) operations to compute the matrix Cn;
(d) computing QnrequiresO(I(+K)) operations;
(e) updating the tensor Arequires 2IN n+1noperations.Springer Nature 2021 L ATEX template
14 Ecient algorithms for Tucker decomposition
Algorithm 4 Another algorithm for solving Problem 1 via random projection
and power scheme
Input : A tensorA 2 RI1I2IN, the desired multilinear rank
f1;2;:::;Ng, the oversampling parameter Kand an integer q1.
Output :Northonormal matrices Qnand a core tensor Gsuch thatA
G 1Q12Q2NQN.
1:Forn= 1;2;:::;N
2:Generate a standard Gaussian matrix G2RIn(n+K).
3:Form the mode- nunfolding A(n)ofA.
4:Compute Cn= (A(n)A>
(n))qGn.
5:Find an orthonormal matrix Qn2RInnsuch that there exists Sn2
Rn(n+K)for which
kQnSn CnkFn+1(Cn):
6:ComputeA=AnQ>
n.
7:End for
8:Set the core tensor GasG=A.
Then Algorithm 4 requires
N 
I(+K) +O(I2+IK)
+NX
n=1 
O(n 1IN n+1) + 4qIN n+1n 1(+K) + 2IN n+1n 1
operations for the tensor A.
4.2 Accelerating Algorithm 4 with approximate matrix
multiplication
Similar to the relationship between Algorithms 1 and 3, Algorithm 5 is
obtained by using Algorithm 2 to compute an approximation of Cnin
Algorithm 4.
Remark 4 In Algorithms 1, 3, 4 and 5, for each n, the matrix Qn2RInncan
be obtained by this process: we rst compute the SVD of CnasCn=UnnV>n,
and we then set Qn=Un(:;1 :n), where Un2RIn(n+K)is orthonormal,
Vn2R(n+K)(n+K)is orthogonal, and n2R(n+K)(n+K)is diagonal and
its diagonal entries are the singular values of Cn.
To count the complexity of Algorithm 5, we assume that T1T2
TNT. For eachn, we haveSpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 15
Algorithm 5 Accelerating Algorithm 4 with approximating matrix multipli-
cation
Input : A tensorA 2 RI1I2IN, the desired multilinear rank
f1;2;:::;Ng,Npositive integers fT1;T2;:::;TNg, the oversampling
parameters K, an integer q1, andNprobability distributions
fpn;ig1:::n 1In+1:::IN
i=1 .
Output :Northonormal matrices Qnand a core tensor Gsuch thatA
G 1Q12Q2NQN.
1:Forn= 1;2;:::;N
2:Generate Sn2R1:::n 1In+1:::INTnRANDSAMPLE( Tn;pn).
3:Arrange A(n)as the mode- nunfolding ofA.
4:Obtain C0
n=A(n)Sn.
5:Form a standard Gaussian matrix Gn2RIn(n+K).
6:Compute Cn= (C0
nC0>
n)qGn.
7:Find an orthonormal matrix Qn2RInnsuch that there exists Sn2
Rn(n+K)for which
kQnSn CnkFn+1(Cn):
8:ComputeA=AnQ>
n.
9:End for
10:Set the core tensor GasG=A.
(a) for the (nearly) optimal probabilities, to construct C0
nrequiresO((I+
IN nn 1)T) operations, and for the uniform probabilities, it needs O(IT)
operations to obtain C0
n;
(b) to generate the matrix Gn2RI(+K)requiresI(+K) operations;
(c) it requires 4 I(+K+T)(+K) operations to compute the matrix Cn;
(d) computing QnrequiresO(I2+IK) operations and updating the tensor
Arequires 2IN n+1noperations.
Then, Algorithm 5 requires
N(O(IL)+I(+K)+2IT+2qI2(+K)+O(I2+IK))+2NX
n=1IN n+1n
operations with the uniform probabilities, and
N(I(+K) + 2IT+ 2qI2(+K) +O(I2+IK)
+NX
n=1 
2IN n+1n+O(LI+LIN nn 1)
operations with the (nearly) optimal probabilities.Springer Nature 2021 L ATEX template
16 Ecient algorithms for Tucker decomposition
Remark 5 In both Algorithms 3 and 5, choosing the right sampling distribution
requires insight and ingenuity, which is out of scope of this work. Here, the sam-
pling distribution used in these two algorithms is chosen as the uniform sampling
distribution.
Fig. 1 FixingK= 10, results of applying Algorithms 1 and 4 with q= 1;2;3;4 toA: top
for Algorithm 1 and bottom for Algorithm 4.
4.3 Parameters selection
In this section, we consider to choose all the parameters K,qandTnin these
proposed algorithms. We dene the relative error (RE) is dened as
RE =kA bAkF=kAkF; (7)
withbA=A1(Q1Q>
1)2(Q2Q>
2)N(QNQ>
N), where for Ngiven positive
integersn< In, an optimal solution fQ1;Q2;:::;QNgfor (2) is obtained
from by applying the numerical algorithms to A2RI1I2IN.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 17
We now illustrate the eciencies of Algorithms 1, 3, 4 and 5 via a synthetic
tensorA2R600600600[29], which is dened as
A=B+kBkFp
4003E (8)
withB=C1A12A23A3, where the entries of C2R100100100are drawn
i.i.d. from the uniform distribution U(0;1), the matrix Anis an orthonormal
basis for the column space of a standard Gaussian matrix Bn2R600100, and
the entries ofE2R600600600are an independent family of standard normal
random variables. Here we set = 0:001 and the desired multilinear rank
f;;gas= 5;10;:::; 100.
When we x K= 10, by applying Algorithms 1 and 4 with dierent qto
the tensorA, the values of RE and CPU time are shown in Figure 1, and when
we xq= 1, by applying Algorithms 1 and 4 with dierent Kto the tensor
A, the results are shown in Figure 2. It implies from Figures 1 and 2 that in
terms of the relative error, for K= 10, Algorithms 1 and 4 with dierent qare
comparable, and for q= 1, Algorithms 1 and 4 with dierent Kare similar.
Fig. 2 Fixingq= 1, results of applying applying Algorithms 1 and 4 with K=
5;10;15;20;25;30 toA: top for Algorithm 1 and bottom for Algorithm 4.Springer Nature 2021 L ATEX template
18 Ecient algorithms for Tucker decomposition
In practice, for each n, we setTn= ceil(I1:::In 1In+1:::IN) for Algo-
rithm 3 and Tn= ceil(1:::n 1In+1:::IN) for Algorithm 5, where 0 <<
1 is a given number and in MAMLAB, ceil( x) rounds the elements of x2R
to the nearest integers towards innity. When we x q= 1 andK= 10, by
applying Algorithms 3 and 5 with dierent to the tensorA, the results are
shown in Figure 3. In terms of the relative error, Algorithm 3 with dierent 
are similar, and Algorithm 5 with 0:15 are better than Algorithm 5 with
 < 0:15. Meanwhile, for each , asincreases, the values of CPU time for
Algorithms 3 and 5 increases.
Fig. 3 FixingK= 10 and q= 1, results of applying Algorithms 3 and 5 with =
0:01;0:08;:::; 0:5 toA: top for Algorithm 3 and bottom for Algorithm 5.
Finally, when we x q= 1,K= 10 and= 0:2, Figure 4 illustrates the
results by applying Algorithms 1, 3, 2 and 5 to A. In terms of the relative
error, these algorithms are comparable, and Algorithm 4 is slight worse than
other algorithms; in terms of CPU time, Algorithm 5 is the fastest one, and
Algorithms 4 and 5 are faster than Algorithms 1 and 3.
In the rest, we set K= 10 andq= 1 in Algorithms 1, 3, 4 and 5. We also
set= 0:2 in Algorithms 3 and 5. In Section 5, we compare Algorithms 4 and
5 with the exisiting deterministic and randomized algorithms via several test
tensors from both synthetic and real datasets.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 19
Fig. 4 Fixingq= 1,K= 10 and= 0:2, results of applying Algorithms 1, 3, 4 and 5 to A.
5 Theoretical analysis
In this section, we will estimate the upper bounds of kA bAkF, with
bA=A 1(Q1Q>
1)2(Q2Q>
2)N(QNQ>
N), wherefQ1;Q2;:::;QNgis
obtained by applying the proposed algorithms to A2RI1I2INwith a
given multilinear rank f1;2;:::;Ng. Without loss of generality, for each
n, we letI0
n= minfIn;I1:::In 1In+1:::INg,I00
n= minfIn;Tng,I000
n=
minfIn;1:::n 1In+1:::INgandI0000
n= minf1:::In 1In+1:::IN;Tng.
For the cases of Algorithms 1 and 3, it follows from (3) that
kA bAkFNX
n=1kA A n(QnQ>
n)kF: (9)
Then, for each n, we need to give an upper bound for kA A n(QnQ>
n)kF.
Meanwhile, for the cases of Algorithms 4 and 5, it follows from (6) that
kA bAkFNX
n=1k(A 1Q>
12Q>
2n 1Q>
n 1)n(IIn QnQ>
n)kF:(10)
Then for each n, we will consider the upper bound for k(A1Q>
12Q>
2n 1
Q>
n 1)n(IIn QnQ>
n)kF.
5.1 Some necessary lemmas
A matrix G2RIJis a standard Gaussian matrix [52] if the entries form an
independent family of standard normal random variables, that is, independent
and identically distributed (i.i.d.) Gaussian random variables of zero mean and
unit variance.Springer Nature 2021 L ATEX template
20 Ecient algorithms for Tucker decomposition
Lemma 1 ([53]) LetG2RLIbe a standard Gaussian matrix with L < I . For
 >1, if
1 1
4(2 1)p
I222
e2 12
(11)
is nonnegative, then, the largest singular value of Gis at mostp
2Iwith probability
not less than the amount in (11).
Lemma 2 ([54]) LetIandLbe positive integers with LIand
1 1p
2(I L+ 1)e
(I L+ 1)I L+1
(12)
being nonnegative, where  > 1. Suppose that G2RLJis a standard Gaussian
matrix. Then, the smallest singular value of Gis at least 1 =(p
I) with probability
not less than the amount in (12).
A standard Gaussian matrix is a special case of sub-Gaussian matrices
[55, 56], whose entries are sub-Gaussian variables. A sub-Gaussian variable is
an important class of random variables that have strong tail decay properties.
Denition 1 ([57]) A real valued random variable Xis called a sub-Gaussian
random variable if there exist b>0 such that for all t>0 we have E(etX)eb2t2=2.
A random variable Xis centered if E(X) = 0.
Denition 2 Assume that s1,a1>0 anda2>0. The set A(s;a1;a2;K;I ) con-
sists of all KIrandom matrices Gwhose entries are the centered independent
identically distributed real valued random variables satisfying the following condi-
tions: (a) moments: E(jgijj3)s3; (b) norm: P(kGk2>a1p
I)e a2I; (c) variance:
E(jgijj2)1.
It is proven in [55] that if Ais sub-Gaussian, then G2A(s;a1;a2;I;J). For
a Gaussian matrix with zero mean and unit variance, we have s= (4=p
2)1=3.
Theorem 1 ([55]) Suppose that G2RKIis sub-Gaussian with KI,s1and
a2>0. Then P(kGk2>a1p
I)e a2I, wherea1= 6spa2+ 4.
Theorem 2 ([55]) Lets1,a1>0anda2>0. Suppose that G2A(s;a1;a2;K;I )
withI >(1 + 1=ln(K))K. Then, there exist positive constants c1andc2such that
P(K(G)c1p
I)e I+e c00I=(2s6)+e a2Ie c2I:
Lemma 3 ([31]) LetK,IandJbe positive integers with K < JI. For a given
matrix A2RIJ, there exist an orthonormal matrix Q2RIKandS2RKJSpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 21
such that
kQS AkFK+1(A) =0
@JX
i=K+1i(A)21
A1=2
;
wherei(A) is theith singular value of Aandj= 1;2;:::;J .
Lemma 4 ([31]) LetA2RIJandB2RJSwithSminfI;Jg. Then for all
k= 1;2;:::; minfI;J;Sg 1;minfI;J;Sg, we have
SX
i=ki(AB)2kBk2
2minfI;JgX
i=ki(A)2:
Lemma 5 Suppose that A2RI1I2,B2RI2I3,Kis a positive integer such that
1KI2, andfpigI2
i=1are such that pi0,PI2
i=1pi= 1, and for some positive
constant1,
pikA(i;:)k2kB(:;i)k2PI2
j=1kA(j;:)k2kB(:;j)k2:
Construct CandRwith Algorithm 2 and let CRbe an approximation to AB. Let
2(0;1) and= 1 +p
(8=) log(1=), then with probability at least 1  ,
kAB CRkFpKkAkFkBkF:
Lemma 6 Suppose that A2RI1I2,B2RI2I3,Kis a positive integer such
that 1KI2, andfpigI2
i=1are such that pi= 1=I2. Construct CandRwith
Algorithm 2 and let CR be an approximation to AB. Let2(0;1) and=
1 +I2p
Kp
8 log(1=) maxikA(:;i)k2kB(i;:)k2, then with probability at least 1  ,
kAB CRkFr
I2
K I2X
i=1kA(:;i)k2
2kB(i;:)k2
2!1=2
+:
5.2 Analyzing Algorithm 1
When the matrix Qnis obtained by Algorithm 1 with given n, the matrix
QnQ>
nA(n)is a good approximation to the matrix A(n), provided that there
exist matrices Gn2RIn(n+K)andRn2Rn(n+K)such that: (a) Qnis
orthonormal; (b) QnRnis a good approximation of ( A(n)A>
(n))qGn; and (c)
there exists a matrix F2R(n+K)1:::n 1In+1:::INsuch thatkFk2is not too
large and ( A(n)A>
(n))qGnFnis a good approximation of A(n).
Lemma 7 For a given n, suppose that A(n)2RInI1:::In 1In+1:::IN,Qn2RInn
is orthonormal, Rn2Rn(n+K),F2R(n+K)I1:::In 1In+1:::INandGn2
RIn(n+K)withn<n+K <I0n. For a given positive integer q>0, we have
kA(n) QnQ>
nA(n)k2
F2k(A(n)A>
(n))qGnFn A(n)k2
F
+ 2kFk2
2kQnRn (A(n)A>
(n))qGnk2
F:(13)Springer Nature 2021 L ATEX template
22 Ecient algorithms for Tucker decomposition
Remark 6 When the Frobenius norm is replaced by the spectral norm, Lemma 7 is
similar to Lemma 4.3 in [34], Lemma 4.4 in [57] and Lemma 3.1 in [35]. The proof is
similar to that of Lemma 4.5 in [32] and Lemma 5.4 in [31].
Proof The proof is straightforward, but tedious, as follows. By using the triangular
inequality, we have
kA(n) QnQ>
nA(n)k2
F
kA(n) (A(n)A>
(n))qGnFnk2
F
+k(A(n)A>
(n))qGnFn QnQ>
n(A(n)A>
(n))qGnFnk2
F
+kQnQ>
n(A(n)A>
(n))qGnFn QnQ>
nA(n)k2
F:(14)
Note that we have
kQnQ>
n(A(n)A>
(n))qGnFn QnQ>
nA(n)k2
F
k(A(n)A>
(n))qGnFn A(n)k2
FkQnQ>
nk2
2;
which implies that
kQnQ>
n(A(n)A>
(n))qGnFn QnQ>
nA(n)k2
Fk(A(n)A>
(n))qGnFn A(n)k2
F:(15)
For the second term in the right-hand side of (14), we have
k(A(n)A>
(n))qGnFn QnQ>
n(A(n)A>
(n))qGnFnk2
F
k(A(n)A>
(n))qGn QnQ>
n(A(n)A>
(n))qGnk2
FkFnk2
2:
From the triangular inequality, we have
k(A(n)A>
(n))qGn QnQ>
n(A(n)A>
(n))qGnk2
Fk(A(n)A>
(n))qGn QnRnk2
F
+kQnRn QnQ>
nQnRnk2
F+kQnQ>
nQnRn QnQ>
n(A(n)A>
(n))qGnk2
F:
By the fact that Q>nQn=In, we havekQnRn QnQ>nQnRnk2
F= 0 and
kQnQ>
nQnRn QnQ>
n(A(n)A>
(n))qGnk2
FkQnRn (A(n)A>
(n))qGnk2
F:
Therefore,
k(A(n)A>
(n))qGn QnQ>
n(A(n)A>
(n))qGnk2
F
2kFnk2
2kQnRn (A(n)A>
(n))qGnk2
F:(16)
Combining (14), (15) and (16) yields (13). 
The upper bound for kQnRn (A(n)A>
(n))qGnkFis obtained by combining
Lemmas 1, 3 and 4 together, summarized in the following theorem.
Theorem 3 For a given n, suppose that A(n)2RInI1:::In 1In+1:::INandGn2
RIn(n+K)is a standard Gaussian matrix with n<n+K <I0n. For >1and
 >1, let
1 1
4(2 1)p
In222
e2 12
(17)Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 23
be nonnegative. For a given positive integer q>0, there exist an orthonormal matrix
Qn2RInnandR2Rn(n+K)such that
kQnRn (A(n)A>
(n))qGnkFp
2Inn+1(A(n);q);
with a probability at least the amount in (17), where
n+1(A(n);q) =0
@I0
nX
i=n+1i(A(n))4q1
A1=2
:
According to the properties of the matrix Gn, for eachn, we estimate
k(A(n)A>
(n))qGnFn A(n)kFandkFnk2.
Theorem 4 For a given n, suppose that A(n)2RInI1:::In 1In+1:::INandGn2
RIn(n+K)is a randomly chosen standard Gaussian matrix with n<n+K <I0n.
For >1and >1, if
1 1
4(2 1)p
maxfI0n n;n+Kg222
e2 12
 1p
2(K+ 1)e
K+ 1K+1(18)
is nonnegative, then, there exists a matrix F2R(n+K)1:::n 1In+1:::INsuch that
k(A(n)A>
(n))qGnFn A(n)kF
p
2 maxfI0n n;n+Kg(n+K)
n(A(n))2q 1n+1(A(n);q) + n+1(A(n));
and
kFnk2pn+K
n(A(n))2q 1
with probability not less than the amount in (18).
Proof Suppose that A(n)=UnnV>n, where Un2RInI0
nandVn2
RI1:::In 1In+1:::INI0
nare orthonornal, and the diagonal entries of the diagonal
matrix n2RI0
nI0
nare nonnegative and arranged in descending order. Then, we
have
(A(n)A>
(n))q=Un2q
nU>
n:
Assume that given U>nandGn, suppose that
U>
nGn=H
R
;
where H2Rn(n+K)andR2R(I0
n n)(n+K). Since Gnis a standard
Gaussian matrix, and Vis an orthogonal matrix, then U>nGnis also a standardSpringer Nature 2021 L ATEX template
24 Ecient algorithms for Tucker decomposition
Gaussian matrix. Therefore, HandRare also standard Gaussian matrices. Let
P2R(n+K)Inbe dened by
P=
Hy 2q+1
n;10(n+K)(In n)
;n;1=n(1 :n;1 :n):
Dene Fn=PV>n. From Lemma 2, we estimate kFnk2as
kFnk2=kPV>
nk2=Hy 2q+1
n;1
21
n(A(n))2q 11
min(H)pn+K
n(A(n))2q 1
with a probability at least
1 1p
2(K+ 1)e
K+ 1K+1
;
where >1. Now, we can bound k(A(n)A>
(n))qGnFn A(n)kF. Note that we have
(A(n)A>
(n))qGnFn A(n)
=Unn
2q 1
nH
R
Hy 2q+1
n;10(n+K)(In n)
 IIn
V>
n:
Letn;2=n((n+ 1) :Jn;(n+ 1) :Jn). Then
n
2q 1
nH
R
Hy 2q+1
n;10(n+K)(In n)
 IIn
=n 
2q 1
n;10n(In n)
0(In n)n2q 1
n;2! 
0nn0n(In n)
RHy 2q+1
n;1 IIn n!
= 
0nn 0n(In n)
2q
n;2RHy 2q+1
n;1 n;2!
:
The norm of the last term is: 
0nn 0n(In n)
2q
n;2RHy 2q+1
n;1 n;2!
F2q
n;2RHy 2q+1
n;1
F+kn;2kF:
Moreover,2q
n;2RHy 2q+1
n;1
F 2q+1
n;1
2Hy
2kRk22q
n;2
F
1
n(A(n))2q 1Hy
2kRk2n+1(A(n);q);
with n+1(A(n);q) =k2q
n;2kF. We also know that
kRk2q
2 maxfI0n n;n+Kg
with a probability at least
1 1
4(2 1)p
maxfI0n n;n+Kg222
e2 12
:
Therefore,(A(n)A>
(n))qGF A(n)
F
1
n(A(n))2q 1Hy
2kRk2n+1(A(n);q) +kn;2kF
p
2 maxfI0n n;n+Kg(n+K)
n(A(n))2q 1n+1(A(n);q) + n+1(A(n)):
with a probability at least the amount in (18). Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 25
Theorem 5 For a given n, letn,InandKare positive integers such that n<
n+K <I0n. For >1and >1, suppose that
1 1
4(2 1)p
maxfI0n n;n+Kg222
e2 12
 1p
2(K+ 1)e
K+ 1K+1
 1
4(2 1)p
In222
e2 12(19)
is nonnegative. For A(n)2RInI1:::In 1In+1:::INand an integer q1, the
orthonormal matrix Qnare obtained by Algorithm 1. Then
kA(n) QnQ>
nA(n)kF2
nn+1(A(n);q) + n+1(A(n))
holds with probability at least the amount in (19), where
n=(p
maxfI0n n;n+Kg+pIn)p
2(n+K)
n(A(n))2q 1:
Proof By the fact that a2b2+c2implies that ab+c, wherea,bandcare
arbitrary positive numbers, it is deduced from Lemma 7 that
kA(n) QnQ>
nA(n)kF2k(A(n)A>
(n))qGnFn A(n)kF
+ 2kFk2kQnRn (A(n)A>
(n))qGnkF:
By combining Theorems 3 and 4 together, the result is obtained. 
ForNgiven positive integers nIn(n= 1;2;:::;N ), when
fQ1;Q2;:::;QNgis acquired from Algorithm 1, the upper bounds of kA bAkF
is given in the following theorem, which is derived from (9) and Theorem 5.
Theorem 6 For eachn, letn,InandKare positive integers such that n<
n+K <I0n. For >1and >1, suppose that
1 NX
n=1 
1
4(2 1)p
maxfI0n n;n+Kg222
e2 12
+1p
2(K+ 1)e
K+ 1K+1
+1
4(2 1)p
In222
e2 12! (20)
is nonnegative. For a given tensor A2RI1I2INand an integer q1,N
orthonormal matrices Qnare obtained by Algorithm 1. Then
kA bAkF2NX
n=1
nn+1(A(n);q) + n+1(A(n))
holds with probability at least the amount in (20), where
n=(p
maxfI0n n;n+Kg+pIn)p
2(n+K)
n(A(n))2q 1:Springer Nature 2021 L ATEX template
26 Ecient algorithms for Tucker decomposition
When the matrix Gnin Theorems 3 and 4 is a randomly chosen sub-
Gaussian matrix, these theorems should be rewritten as follows.
Remark 7 For a given n, suppose that A(n)2RInI1:::In 1In+1:::INandGn2
RIn(n+K)is a randomly chosen sub-Gaussian matrix with n< n+K < I0n.
For a given positive integer q>0, there exist an orthonormal matrix Qn2RInn
andR2Rn(n+K)such that
kQnRn (A(n)A>
(n))qGnkFan;1Inn+1(A(n);q);
with a probability at least 1  e an;2In, wherean;1= 6span;2+ 4 ands>1.
Remark 8 For a given n, suppose that A(n)2RInI1:::In 1In+1:::INandGn2
RIn(n+K)is a randomly chosen sub-Gaussian matrix with n<n+K <I0nand
n+K > (1 + 1=lnn)n. We dene an;1,an;2,cn;1andcn;2as in Theorems 1 and
2. Then, there exists a matrix F2R(n+K)1:::n 1In+1:::INsuch that
k(A(n)A>
(n))qGnFn A(n)kF
an;1cn;1p
maxfI0n n;n+Kg(n+K)
n(A(n))2q 1n+1(A(n);q) + n+1(A(n));
and
kFnk2cn;1pn+K
n(A(n))2q 1
with a probability at least 1  e cn;2(n+K) e an;2maxfI0
n n;n+Kg, wherean;1=
6span;2+ 4 ands>1.
The following result is obtained by combining Lemma 7, and the above two
remarks.
Remark 9 For eachn, letn,InandKare positive integers such that n<n+K <
I0nandn+K > (1+1=lnn)n. We dene an;1,an;2,cn;1andcn;2as in Theorems
1 and 2 with an;1= 6span;2+ 4 ands>1. For a given tensor A2RI1I2IN
and an integer q1,Northonormal matrices Qnare obtained by Algorithm 1. Then
kA bAkF2NX
n=1
nn+1(A(n);q) + n+1(A(n))
holds with a probability at least
1 NX
n=1
e cn;2(n+K)+e an;2maxfI0
n n;n+Kg
;
where
n=(p
maxfI0n n;n+Kg+In)pn+Kan;1cn;1
n(A(n))2q 1:Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 27
5.3 Analyzing Algorithm 3
For eachn, when Qnis obtained from Algorithm 3, we have
kA(n) QnQ>
nA(n)k2
F
rank(A(n))k(A(n) QnQ>
nA(n))(A(n) QnQ>
nA(n))>kF
= rank( A(n))kA(n)A>
(n) QnQ>
nA(n)A>
(n)kF
rank(A(n))
kA(n)A>
(n) CnC0>
nkF+kC0
nC0>
n QnQ>
nC0
nC0>
nkF
;
where Sn2RI1:::In 1In+1:::INTnRANDSAMPLE( Tn;pn) and C0
n=
A(n)Sn.
Corollary 1 For a given n, suppose that A(n)2RInI1:::In 1In+1:::INandC0n=
A(n)Sn, where Sn2RI1:::In 1In+1:::INTnRANDSAMPLE( Tn;pn). If the prob-
abilities pnare the nearly optimal probabilities, then for any n2(0;1), there exists
n= 1 +p
(8=n) log(1=n) such that
kA(n)A>
(n) C0
nC0>
nkFnpTnkA(n)k2
F
with a probability at least 1  n. If the probabilities pnare the uniform probabilities,
then for any n2(0;1), there exists n1 +I0
npTnp
8 log(1=n)kA(n)k2
Fsuch that
kA(n)A>
(n) C0
nC0>
nkF0
@1 +s
I0n
Tn+I0npTnp
8 log(1=n)1
AkA(n)k2
F
with a probability at least 1  n.
Proof The rst inequality is derived from Lemma 5. We now prove the second
inequality. As shown in Lemma 6, we have that for any n2(0;1), there exists
n= 1 +I0npInp
8 log(1=) max
ikA(n)(:;i)k2
2
such that
kA(n)A>
(n) C0
nC0>
nkFr
I0n
K0
@I0
nX
i=1kA(n)(:;i)k4
21
A1=2
+n
holds with a probability at least 1  n. It is obvious that max ikA(n)(:;i)k2
2
kA(n)k2
Fand
I0
nX
i=1kA(n)(:;i)k4
20
@I0
nX
i=1kA(n)(:;i)k2
21
A2
=kA(n)k4
F;
which implies the second inequality. Springer Nature 2021 L ATEX template
28 Ecient algorithms for Tucker decomposition
We now estimate the upper bound for kC0
nC0>
n QnQ>
nC0
nC0>
nkF. When
the matrix Qnis obtained by Algorithm 3 with a given n, the matrix
QnQ>
nC0
nC0>
nis a good approximation to the matrix C0
nC0>
n, provided that
there exist matrices Gn2RIn(n+K)andRn2Rn(n+K)such that: (a)
Qnis orthonormal; (b) QnRnis a good approximation of C0
nC0>
nGn; and (c)
there exists a matrix F2R(n+K)Insuch thatkFk2is not too large and
C0
nC0>
nGnFnis a good approximation of C0
nC0>
n.
Similar to Lemma 7, the following lemma can be easily proved.
Lemma 8 For a given n, suppose that C0n2RInTn,Qn2RInnis orthonormal,
Rn2Rn(n+K),F2R(n+K)InandGn2RIn(n+K)withn<n+K <
In. Then, we have
kC0
nC0>
n QnQ>
nC0
nC0>
nk2
F2kC0
nC0>
nGnFn C0
nC0>
nk2
F
+ 2kFk2
2kQnRn C0
nC0>
nGnk2
F:
According to the properties of the matrix Gn, for eachn, we estimate
kC0
nC0>
nGnFn C0
nC0>
nkFandkFnk2.
Theorem 7 For a given n, suppose that C0n2RInTnandGn2RIn(n+K)is a
randomly chosen standard Gaussian matrix with n<n+K <I00n. For >1and
 >1, if
1 1
4(2 1)p
maxfI00n n;n+Kg222
e2 12
 1p
2(K+ 1)e
K+ 1K+1(21)
is nonnegative, then, there exists a matrix F2R(n+K)1:::n 1In+1:::INsuch that
kC0
nC0>
nGnFn C0
nC0>
nkF
(q
2 maxfI00n n;n+Kg(n+K)+ 1)n+1(Cn;1);
and
kFnk2p
n+K
with probability not less than the amount in (21), where
n+1(Cn;1) =vuuutI00nX
i=n+1i(C0n)4:
Proof Suppose that C0n=UnnV>n, where Un2RInI00
nandVn2RInI00
n
are orthonornal, and the diagonal entries of the diagonal matrix n2RI00
nI00
nare
nonnegative and arranged in descending order. Then, we have
C0
nC0>
n=Un2
nU>
n:Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 29
Assume that given U>nandGn, suppose that
U>
nGn=H
R
;
where H2Rn(n+K)andR2R(I00
n n)(n+K). Since Gnis a standard
Gaussian matrix, and Vis an orthogonal matrix, then U>nGnis also a standard
Gaussian matrix. Therefore, HandRare also standard Gaussian matrices. Let
P2R(n+K)Inbe dened by
P=
Hy0(n+K)(In n)
;n;1=n(1 :n;1 :n):
Dene Fn=PU>n. From Lemma 2, we estimate kFnk2as
kFnk2=kPU>
nk2=Hy
21
min(H)p
n+K
with a probability at least
1 1p
2(K+ 1)e
K+ 1K+1
;
where >1. Now, we can bound kC0nC0>nGnFn C0nC0>nkF. Note that we have
C0
nC0>
nGnFn C0
nC0>
n=Un2
nH
R
Hy0(n+K)(In n)
 IIn
U>
n:
Letn;2=n((n+ 1) :Jn;(n+ 1) :Jn). Then
2
nH
R
Hy0(n+K)(In n)
 IIn
=0nn0n(In n)
2
n;2RHy 2
n;2
:
The norm of the last term is:0nn0n(In n)
2
n;2RHy 2
n;2
F2
n;2RHy
F+k2
n;2kF:
Moreover,
2
n;2RHy
FHy
2kRk22
n;2
F=Hy
2kRk2n+1(Cn;1);
with n+1(Cn;1) =k2
n;2kF. We also know that
kRk2q
2 maxfI00n n;n+Kg
with a probability at least
1 1
4(2 1)p
maxfI00n n;n+Kg222
e2 12
:
Therefore,
kC0
nC0>
nGnFn C0
nC0>
nkF(kHyk2kRk2+ 1)n+1(Cn;1):
with a probability at least the amount in (21). 
By combining Lemmas 3 and 8, and Theorem 7, we give an upper bound
forkC0
nC0>
n QnQ>
nCnC0>
nkF, as shown in the following corollary.Springer Nature 2021 L ATEX template
30 Ecient algorithms for Tucker decomposition
Corollary 2 For a given n, letn,In,TnandKare positive integers such that
n<n+K <I00n. For >1 and >1, suppose that
1 1
4(2 1)p
maxfI00n n;n+Kg222
e2 12
 1p
2(K+ 1)e
K+ 1K+1
 1
4(2 1)p
In222
e2 12(22)
is nonnegative. For a given tensor A2RI1I2INandq= 1, the orthonormal
matrix Qnis obtained by Algorithm 3. Then
kC0
nC0>
n QnQ>
nCnC0>
nkF2(n+ 1)n+1(Cn;1)
holds with a probability at least the amount in (22), where
n= (q
maxfI00n n;n+Kg+p
In)p
2(n+K):
Remark 10 Whenq>1, the constants 0nand00nin Corollary 2 will be given by
n=(p
maxfI00n n;n+Kg+pIn)p
2(n+K)
n(C0nC0>n)q 1;
which depends on the matrix C0n. From [58], when A>
(n)is an orthonormal matrix,
then with a probability at least 1  , the singular values of C0nC0>nbelong to
"
1 r
4Inlog(2In=)
Tn;1 +r
4Inlog(2In=)
Tn#
;
where 0< < 1 and 4Inlog(2In=)<TnI0n. However, the lower bounds for the
singular values of C0nC0>nis unclear for a general matrix A(n). Hence, how to analyze
Algorithm 3 with q>1 is an open issue, which will be considered in the future.
The following corollary states the upper bound of kA bAkF, where for
Ngiven positive integers nIn(n= 1;2;:::;N ), the optimal solution
fQ1;Q2;:::;QNgis obtained from Algorithm 3 with q= 1.
Corollary 3 For eachn, letn,In,TnandKare positive integers such that n
n+KI00n. For >1, >1 and 0<n<1, suppose that
1 NX
n=1 
1
4(2 1)p
maxfI00n n;n+Kg222
e2 12
+1p
2(K+ 1)e
K+ 1K+1
+1
4(2 1)p
In222
e2 12
+n! (23)
is nonnegative. For a given tensor A2RI1I2INandq= 1, the orthonormal
matrices Qn(n= 1;2;:::;N ) are obtained by Algorithm 3. Then
kA bAk2
FNX
n=1rank(A(n))
2(1 +n)Inn+1(A(n);1) +nkAk2
FSpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 31
holds with a probability at least the amount in (23), where
n= (q
maxfI00n n;n+Kg+p
In)p
2(n+K):
For the nearly optimal probabilities, there exists n= 1 +p
(8=n) log(1=n) such
that
n=npTn;
and for the uniform probabilities, there exists n1 +I0
npTnp
8 log(1=n)kA(n)k2
F
such that
n= 1 +s
I0n
Tn+I0npTnp
8 log(1=n):
Proof By combining (3), and Corollaries 1 and 2, we have that
kA bAkFNX
n=1rank(A(n))
2(1 +n)n+1(C0
n;1) +nkAk2
F
holds with a probability at least the amount in (23).
Since Sn2RI1:::In 1In+1:::INTnRANDSAMPLE( Tn;pn), it follows from
[59] thatkSnk2pIn. From Lemma 4, we have
n+1(C0
n;1)Inn+1(A(n);1);
which implies  n+1(C0n;1)Inn+1(A(n);1). Hence, this theorem is completely
proved. 
5.4 Analyzing Algorithm 4
For eachn, when Qnis obtained from Algorithm 4, we assume that B=
A 1Q>
1n 1Qn 1, which implies that B(n)=A(n)(Q1

 Qn 1
IIn 1

 IIN). Hence we can rewrite (10) as
A A 1 
Q1Q>
1
2 
Q2Q>
2
N 
QNQ>
N2
F
NX
n=1kB(n) QnQ>
nB(n)k2
F:
When we replace the matrix A(n)in Theorem 5 as B(n), the following
remark establishes the upper bound of kB(n) QnQ>
nB(n)kF.
Remark 11 For a given n, letn,InandKare positive integers such that n<
n+K <I000n. For >1 and >1, suppose that
1 1
4(2 1)p
maxfI000n n;n+Kg222
e2 12
 1p
2(K+ 1)e
K+ 1K+1
 1
4(2 1)p
In222
e2 12(24)Springer Nature 2021 L ATEX template
32 Ecient algorithms for Tucker decomposition
is nonnegative. For a given tensor A 2RI1I2INand a integer q1, the
orthonormal matrix Qnis obtained by Algorithm 4. Then
kB(n) QnQ>
nB(n)kF2
nn+1(B(n);q) + n+1(B(n))
holds with a probability at least the amount in (24), where
n=(p
maxfI000n n;n+Kg+pIn)p
2(n+K)
n(B(n))2q 1:
Since all the Qj(j= 1;2;:::;n 1) are orthonormal, the matrix Q1

 Qn 1
IIn 1

 IINis also orthonormal. According to Lemma 3.9 in [60] and Corol-
lary 5.1 in [31], we have that  n+1(B(n))n+1(A(n)) and n+1(B(n);q)
n+1(A(n);q). However, the lower bound of n(B(n)) is unclear. Hence, the
framework of analyzing Algorithm 1 is not suitable for considering Algorithm 4.
Note that
kB(n) QnQ>
nB(n)k2
F
rank(B(n))k(B(n) QnQ>
nB(n))(B(n) QnQ>
nB(n))>kF
rank(A(n))k(B(n) QnQ>
nB(n))(B(n) QnQ>
nB(n))>kF
= rank( A(n))k(B(n)B>
(n) QnQ>
nB(n)B>
(n))(IIn QnQ>
n)kF
rank(A(n))kB(n)B>
(n) QnQ>
nB(n)B>
(n)kF;
where the rst inequality holds for the fact that rank( B(n))rank(A(n)) and
the second inequality holds for the fact that kIIn QnQ>
nk21.
When we replace the matrix C0
nC0>
nin Corollary 2 as B(n)B>
(n), the follow-
ing corollary establishes the upper bound of kB(n)B>
(n) QnQ>
nB(n)B>
(n)kF.
Corollary 4 For a given n, letn,In, andKare positive integers such that n<
n+K <I000n. For >1 and >1, suppose that
1 1
4(2 1)p
maxfI000n n;n+Kg222
e2 12
 1p
2(K+ 1)e
K+ 1K+1
 1
4(2 1)p
In222
e2 12(25)
is nonnegative. For a given tensor A2RI1I2INandq= 1, the orthonormal
matrix Qnis obtained by Algorithm 4. Then
kB(n)B>
(n) QnQ>
nB(n)B>
(n)kF2(1 +n)n+1(B(n);1)
holds with a probability at least the amount in (25), where
n= (q
maxfI000n n;n+Kg+p
In)p
2(n+K):
We illustrate the relationship between the singular values of A(n)andB(n)
in the following lemma.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 33
Lemma 9 For eachn, assume that n<I000n. Then we have
I000
nX
k=n+1k(B(n))2I0
nX
k=n+1k(A(n))2:
Proof Note that B(n)=A(n)(Q1

 Qn 1
IIn 1

 IIN), then for each
k= 1;2;:::;I000n 1;I000n, we have
k(B(n))k(A(n)):
By combining Lemma 3.9 in [60] and the fact that
kQ1

 Qn 1
IIn 1

 IINk2= 1;
the proof is completed. 
Corollary 5 For eachn, letn,InandKare positive integers such that n
n+KI000n. For >1, >1 and 0<n<1, suppose that
1 NX
n=1 
1
4(2 1)p
maxfI000n n;n+Kg222
e2 12
+1p
2(K+ 1)e
K+ 1K+1
+1
4(2 1)p
In222
e2 12! (26)
is nonnegative. For a given tensor A2RI1I2INandq= 1, the orthonormal
matrices Qn(n= 1;2;:::;N ) are obtained by Algorithm 4. Then
kA bAk2
F2NX
n=1rank(A(n))(1 +n)Inn+1(A(n);1)
holds with probability at least the amount in (25), where
n= (q
maxfI000n n;n+Kg+p
In)p
2(n+K):
Proof This result is derived by combining (6), Corollary 4 and Lemma 9. 
5.5 Analyzing Algorithm 5
For eachn, when Qnis obtained from Algorithm 5, we have
kB(n) QnQ>
nB(n)k2
F
rank(B(n))k(B(n) QnQ>
nB(n))(B(n) QnQ>
nB(n))>kF
= rank( B(n))kB(n)B>
(n) QnQ>
nB(n)B>
(n)kF
rank(B(n))
kB(n)B>
(n) CnC0>
nkF+kC0
nC0>
n QnQ>
nC0
nC0>
nkF
;Springer Nature 2021 L ATEX template
34 Ecient algorithms for Tucker decomposition
withB(n)=A(n)(Q1

 Qn 1
IIn 1

 IIN) and C0
n=A(n)Sn,
whereB=A 1Q>
1n 1Qn 1andSn2R1:::n 1In+1:::INTn
RANDSAMPLE( Tn;pn).
Similar to Corollary 1, the upper bound for kB(n)B>
(n) CnC0>
nkFis given
in the following corollary.
Corollary 6 For a given n, let Sn2R1:::n 1In+1:::INTn
RANDSAMPLE( Tn;pn). Suppose that B(n)2Rn1:::In 1In+1:::INand
C0n=B(n)Sn. If the probabilities pnare the nearly optimal probabilities, then for
anyn2(0;1), there exists n= 1 +p
(8=n) log(1=n) such that
kB(n)B>
(n) C0
nC0>
nkFnpTnkB(n)k2
F
with a probability at least 1  n. If the probabilities pnare the uniform probabilities,
then for any n2(0;1), there exists n1 +I0000
npTnp
8 log(1=n)kB(n)k2
Fsuch that
kB(n)B>
(n) C0
nC0>
nkF0
@1 +s
I0000n
Tn+I0000npTnp
8 log(1=n)1
AkB(n)k2
F
with a probability at least 1  n.
ForNgiven positive integers f1;2;:::;Ngwithn<In, suppose that
the optimal solution fQ1;Q2;:::;QNgis obtained from Algorithm 5 with
q= 1. The upper bound for kA bAkFis obtained by combining Corollaries 2
and 6, and the fact that k(B(n))k(A(n)) withk= 1;2;:::;I000
n 1;I000
n.
Corollary 7 For eachn, letn,In,TnandKare positive integers such that n<
n+K < minfI00n;I0000ng. For >1, >1 and 0<n<1, suppose that
1 NX
n=1 
1
4(2 1)p
maxfI00n n;n+Kg222
e2 12
+1p
2(K+ 1)e
K+ 1K+1
+1
4(2 1)p
In222
e2 12
+n! (27)
is nonnegative. For a given tensor A2RI1I2INandq= 1, the orthonormal
matrices Qn(n= 1;2;:::;N ) are obtained by Algorithm 5. Then
kA bAk2
FNX
n=1rank(A(n))
2(1 +n)Inn+1(A(n);1) +nkAk2
F
holds with a probability at least the amount in (27), where
n= (q
maxfI00n n;n+Kg+p
In)p
2(n+K):
For the nearly optimal probabilities, there exists n= 1 +p
(8=n) log(1=n) such
that
n=npTn;Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 35
and for the uniform probabilities, there exists n1 +I0000
npTnp
8 log(1=n)kA(n)k2
F
such that
n= 1 +s
I0000n
Tn+I0000npTnp
8 log(1=n):
Remark 12 The theoretical analysis for Algorithms 3, 4 and 5 with q>1 is unclear
and will be studied in the future.
5.6 Related forms for Algorithms 1 and 4
One dierence between Algorithms 1 and 4 is that for each n, the tensor
Ain Algorithm 1 remains unchanged, but the tensor Ain Algorithm 4 is
updated byA=AnQ>
n. Another dierence is that for Algorithm 1, after
obtaining all the matrices Qn, we need to compute the core tensor GasG=
A 1Q>
12Q>
2NQ>
N, and for Algorithm 4, the core tensor Gis directly
obtained after nding all the matrices Qn.
Algorithms 6 and 7 are obtained by computing the matrix Qnin Algorithms
1 and 4 as this way: a) to obtain the thin QR factorization of CnasCn=PnR,
where Pn2RIn(n+K)is orthonormal and R2R(n+K)(n+K)is upper
triangular; b) to form C0
n=P>
nA(n); c) to compute an orthonormal matrix
Un2RInnsuch that its columns are the left singular vectors corresponding
to the topnsingular values of C0
n; and d) to form Qn=PnUn.
Algorithm 6 The related form for Algorithm 1
Input : A tensorA 2 RI1I2IN, the desired multilinear rank
f1;2;:::;Ng, the oversampling parameter Kand an integer q1.
Output :Northonormal matrices Qnand a core tensor Gsuch thatA
G 1Q12Q2NQN.
1:Forn= 1;2;:::;N
2:Generate a standard Gaussian matrix G2RIn(n+K).
3:Arrange the mode- nunfolding A(n)ofA.
4:Compute Cn= (A(n)A>
(n))qGn.
5:Compute the thin QR factorization of CnasCn=PnR.
6:Form C0
n=P>
nA(n)and let Unbe the left singular vectors corresponding
to topnsingular values of C0
n.
7:Compute Qn=PnUn.
8:End for
9:Compute the core tensor G=A 1Q>
12Q>
2NQ>
N.
For eachn, we setJ0
n=I1:::In 1In+1:::INandB=Ain Algorithm
6, and we set J0
n=1:::n 1In+1:::INandB=A 1Q>
1n 1Q>
n 1
in Algorithm 7. Suppose that B(n)=UnnV>
n, where Un2RInJnand
Vn2RJ0
nJnare orthonornal, and the diagonal entries of the diagonalSpringer Nature 2021 L ATEX template
36 Ecient algorithms for Tucker decomposition
Algorithm 7 The related form for Algorithm 4
Input : A tensorA 2 RI1I2IN, the desired multilinear rank
f1;2;:::;Ng, the oversampling parameter Kand an integer q1.
Output :Northonormal matrices Qnand a core tensor Gsuch thatA
G 1Q12Q2NQN.
1:Forn= 1;2;:::;N
2:Generate a standard Gaussian matrix G2RIn(n+K).
3:Arrange the mode- nunfolding A(n)ofA.
4:Compute Cn= (A(n)A>
(n))qGn.
5:Compute the thin QR factorization of CnasCn=PnR.
6:Form C0
n=P>
nA(n)and let Unbe the left singular vectors corresponding
to topnsingular values of C0
n.
7:Compute Qn=PnUn.
8:UpdateA=AnQ>
n.
9:End for
10:Set the core tensor GasG=A.
matrix n2RJnJnare nonnegative and arranged in descending order with
Jn= minfIn;J0
ng. Then ( B(n)B>
(n))q=Un2q
nU>
n. Let Un;1=Un(:;1 :n),
Un;2=Un(:;n+1 :Jn),n;1=n(1 :n;1 :n), and n;2=n((n+1) :
Jn;(n+ 1) :Jn). Dene Sn;1=U>
n;1GnandSn;2=U>
n;2Gn.
For eachn, when the matrix Qnis obtained by Algorithm 6. then, from
Theorem 3.5 in [61] and Theorem 9 in [36], we have
kB(n) QnQ>
nB(n)k2
Fkn;2k2
F+4q 1
nkn;2Sn;2Sy
n;1k2
F;
wheren=n+1(B(n))=n(B(n)). When the matrix Qnis obtained by
Algorithm 7, by using the same notations in Section 5.4, we have
kB(n) QnQ>
nB(n)k2
Fkn;2k2
F+4q 1
nkn;2Sn;2Sy
n;1k2
F;
which implies that
kB(n) QnQ>
nB(n)k2
Fkn;2k2
F+kn;2Sn;2Sy
n;1k2
F;
wheren=n+1(B(n))=n(B(n))1.
It is worth to note that: for Algorithm 6, the singular values of B(n)are
equivalent to that of A(n), and for Algorithm 7, the relationship between
the singular values of B(n)andA(n)are in Lemma 4. Since Gnis a stan-
dard Gaussian matrix, Sn;1=U>
n;1GnandSn;2=U>
n;2Gnare also standard
Gaussian matrices. Hence, the upper bounds of kSn;2k2andkSy
n;1k2can be
obtained from Lemmas 1 and 2. Then, the upper bound of of kA bAk2
Fcan
be sequentially obtained by combining (3), and Lemmas 1 and 2.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 37
On the other hand, suppose that for each n, the matrix Qnis obtained by
Algorithm 6 or 7. From Theorem 9.2 in [24], we have
kB(n) PnP>
nB(n)k2
Fk(B(n)B>
(n))q PnP>
n(B(n)B>
(n))qk1=2q
F
(k2q
n;2kF+k2q
n;2Sn;2Sy
n;1kF)1=2q
(k2q
n;2kF+k2q
n;2kFkSn;2k2kSy
n;1k2)1=2q:
Hence, we can obtain the upper bound for kA eAkFby combining (3), and
Lemmas 1 and 2, where eA=A 1(P1P>
1)2(P2P>
2)N(PNP>
N). By
the relationship between PnandQn, it is clear to see that each term of the
multilinear rank of eAmay be larger than that of bA.
Suppose that the parameters Kandqin Algorithms 1, 4, 6 and 7 are the
same in Section 4.3. By setting = 0:01 and= 5;10;:::; 50, when we apply
these algorithms to the tensor Ain (8), the results are shown in Figure 5. In
terms of RLNE, these algorithms are comparable; and in terms of CPU time,
Algorithm 4 is faster than Algorithm 7, Algorithm 1 is faster than Algorithm
6, and Algorithms 4 and 7 are much faster than Algorithms 1 and 6.
Fig. 5 Results of applying Algorithms 1, 4, 6 and 7 to Awith= 0:01 and= 5;10;:::; 50.
Remark 13 These two modications are also suitable for Algorithms 3 and 5. The the-
oretical results for Tucker-SRFT and Tucker-SRFT-power in [31] have been derived
by changing the way to obtain the unitary matrix Qn2CInnas the same as that
in Algorithm 6 or Algorithm 7.
6 Numerical examples
In this section, we use the numerical computation software MATLAB to
develop computer programs and implement the calculations on a laptop with
Intel Core i5-4200M CPU (2.50GHz) and 8GB RAM. We set MATLABSpringer Nature 2021 L ATEX template
38 Ecient algorithms for Tucker decomposition
maxNumCompThreads to 1 and use \tic" and \toc" to measure running time
when applying all of the algorithms to the test tensors. The unit of CPU time is
second. The tensor-matrix product and tensor contraction can be implemented
by the functions \ttm" and \ttt" in the MATLAB Tensor Toolbox [62, 63].
We compare the eciency and accuracy of Algorithms 4 and 5 with HOOI
[15], T-HOSVD [37], ST-HOSVD [23], randomized T-HOSVD [26], randomized
ST-HOSVD [26], Adap-Tucker [27], Tucker-SVD [31] and Tucker-SVD-power
[32]. Note that randomized T-HOSVD, Adap-Tucker, Tucker-SVD, Tucker-
SVD-power, and Algorithms 4 and 5 are implemented by MATLAB codes,
HOOI is implemented by the function tucker als [63], T-HOSVD and ST-
HOSVD are implemented by the function mlsvd [64] with dierent parameters,
and randomized ST-HOSVD is implemented by the mlsvd rsi [64]. We also
compare Algorithms 4 and 5 with randomized T-HOSVD and randomized ST-
HOSVD under subspace iterations. The number of subspace iterations is set to
q= 1. For all algorithms, the processing order is f1;2;:::;Ngand the factor
matrices are orthonormal. For Algorithms 4 and 5, we set K= 10 andq= 1,
and for Algorithm 5, we set Tn= ceil(1:::n 1In+1:::IN) with= 0:2.
LetQn2RInnbe derived using any numerical algorithm to A 2
RI1I2IN. For a given multilinear rank f1;2;:::;Ng, the relative error
(RE) is dened in (7), and bA=A 1(Q1Q>
1)2(Q2Q>
2)N(QNQ>
N) is
a low rank approximation in the Tucker format to A.
6.1 A sparse tensor
Consider a sparse tensor A2R600600600such that
A=50X
i=1
i2xiyizi+600X
i=511
i2xiyizi
where xi,yiandziare sparse vectors in R600with only 0.05 sparsity.
In this section, we set = 1000. For dierent multilinear ranks f;;g,
when we apply Algorithms 4 and 5, and the existing deterministic and ran-
domized algorithms for nding a low multilinear rank approximation of the
sparse tensorA, the values of RE and the CPU time are shown in Figure 6.
In terms of RE, Algorithms 4 and 5 are similar to HOOI, T-HOSVD,
ST-HOSVD, Tucker-SVD-power, randomized T-HOSVD with q= 1 and
randomized ST-HOSVD with q= 1, and are better than Adap-Tucker,
Tucker-SVD, randomized T-HOSVD and randomized ST-HOSVD; in terms
of CPU time, Algorithm 5 is the fastest one, and Algorithm 4 is slower than
Adap-Tucker and Tucker-SVD and faster than other algorithms.
6.2 A low multilinear rank tensor plus the white noise
LetB2R600600600be given in the Tucker form [65]: B=G1B12B23B3,
where the entries of G2R100100100andBn2R600100are i.i.d. Gaussian
variables with zero mean and unit variance. The form of this test tensor AisSpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 39
Fig. 6 Results of applying Algorithms 4 and 5, HOOI, T-HOSVD, ST-HOSVD, randomized
T-HOSVD, randomized ST-HOSVD, Adap-Tucker, Tucker-SVD and Tucker-SVD-power to
the sparse tensor Awith= 5;10;:::; 100.
given asA=B+N, whereN2R600600600is an unstructured perturbation
tensor with dierent noise level . The following signal-to-noise ratio (SNR)
measure will be used:
SNR [dB] = 10 logkAk2
F
kNk2
F
: (28)Springer Nature 2021 L ATEX template
40 Ecient algorithms for Tucker decomposition
Fig. 7 Results of applying Algorithms 4 and 5, HOOI, T-HOSVD, ST-HOSVD, randomized
T-HOSVD, randomized ST-HOSVD, Adap-Tucker, Tucker-SVD and Tucker-SVD-power to
the tensorAwith SNR = 30; 28;:::; 28;30.
The FIT value for approximating the tensor Ais dened by FIT = 1  RE,
where RE is given in (7).
Here we assume that the desired multilinear rank is given by
f100;100;100g. By applying Algorithms 4 and 5, and the existing determinis-
tic and randomized algorithms to the tensor Awith dierent values of SNR,
the related results are given in Figure 7.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 41
It follows from this gure that a) the values of CPU time obtained by
Algorithms 4 and 5 are less than that obtained by other algorithms, b) the
values of FIT of Algorithms 4 and 5 are similar to that obtained by HOOI, T-
HOSVD, ST-HOSVD, Tucker-SVD-power, randomized T-HOSVD with q= 1
and randomized ST-HOSVD with q= 1, and c) the values of FIT of Algo-
rithms 4 and 5 are larger than that obtained by Adap-Tucker, Tucker-SVD,
randomized T-HOSVD and randomized ST-HOSVD.
Fig. 8 Results of applying Algorithms 4 and 5, HOOI, T-HOSVD, ST-HOSVD, randomized
T-HOSVD, randomized ST-HOSVD, Adap-Tucker, Tucker-SVD and Tucker-SVD-power to
the tensorAwith= 5;10;:::; 100.Springer Nature 2021 L ATEX template
42 Ecient algorithms for Tucker decomposition
6.3 A more synthetic tensor
We construct the input tensor A2R600600600as
A= tendiag( v;[600;600;600])1A12A23A3;
where \tendiag" is a function in the MATLAB Tensor Toolbox ([63]), which
converts vto a diagonal tensor in R600600600, and Anis an orthogonal basis
for the column space of a standard Gaussian matrix Bn2R600600. The vector
v2R600is given by
vi=(
1; i= 1;2;:::; 50;
(i 49) 2; i= 51;52;:::; 600:
Figure 8 illustrates that for dierent multilinear rank f;;gwith=
5;10;:::; 100, in terms of RE, Algorithms 4 and 5 are similar to and slightly
worse than HOOI, T-HOSVD, ST-HOSVD, Tucker-SVD-power, randomized
T-HOSVD with q= 1 and randomized ST-HOSVD with q= 1, and better
than Adap-Tucker, Tucker-SVD, randomized T-HOSVD and randomized ST-
HOSVD; in terms of CPU time, Algorithm 5 is the fastest one and Algorithms
4 and 5 are faster than other deterministic and randomized algorithms.
6.4 Compression of tensors from real-world applications
In this section, we consider the Extended Yale Face Dataset B1[66] and the
Washington DC Mall dataset2. The Extended Yale Face Dataset B has 560
images that contain the rst 12 possible illuminations of 28 dierent people
under the rst poss, where each image has 480 640 pixels in a grayscale range.
After selecting 400 images and extracting a part of 400 400 pixels from each
image, there is collected into a 400 400400 tensor with real numbers.
The sensor system used in this case measured pixel response in 210 bands
in the 0.4 to 2.4 um region of the visible and infrared spectrum. Bands in the
0.9 and 1.4 um region where the atmosphere is opaque have been omitted from
the data set, leaving 191 bands. The data set contains 1280 scan lines with 307
pixels in each scan line. When we choose the rst 600 scan lines, this dataset
is collected as a tensor of size 600 307191.
For the rst dataset, we set 1=2=3== 20;40;:::; 200. We
also set1= 40;80;:::; 400,2= 20;40;:::; 200 and3= 10;20;:::; 100 for
the second dataset. The values of other parameters are the same as that in
the above two sections. By applying these algorithms to the tensors from two
datasets, the related results are shown in Figures 9 and 10, respectively.
For both the Extended Yale Face Dataset B and the Washington DC Mall
dataset, in terms of CPU time, Algorithm 5 is the fastest one and Algorithms
1The Yale Face Database is at http://vision.ucsd.edu/ iskwak/ExtYaleDatabase/ExtYaleB.
html.
2The Washington DC Mall dataset is at https://engineering.purdue.edu/biehl/MultiSpec/
hyperspectral.html.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 43
4 and 5 are faster than other algorithms; and in terms of RE, Algorithms
4 and 5 are slightly less than HOOI, T-HOSVD, ST-HOSVD, Tucker-SVD-
power, randomized T-HOSVD with q= 1 and randomized ST-HOSVD with
q= 1, and better than Adap-Tucker, Tucker-SVD, randomized T-HOSVD and
randomized ST-HOSVD. It is worthy noting that in terms of RE, for both the
Extended Yale Face Dataset B, Algorithm 5 is less than Algorithm 4, and for
the Washington DC Mall dataset, Algorithm 5 is similar to Algorithm 4.
Fig. 9 Results of applying Algorithms 4 and 5, HOOI, T-HOSVD, ST-HOSVD, randomized
T-HOSVD, randomized ST-HOSVD, Adap-Tucker, Tucker-SVD and Tucker-SVD-power to
the tensor from the Extended Yale Face Dataset B with = 20;40;:::; 200.Springer Nature 2021 L ATEX template
44 Ecient algorithms for Tucker decomposition
Fig. 10 Results of applying Algorithms 4 and 5, HOOI, T-HOSVD, ST-HOSVD, ran-
domized T-HOSVD, randomized ST-HOSVD, Adap-Tucker, Tucker-SVD and Tucker-SVD-
power to the tensor from the Washington DC Mall dataset with 1= 40;80;:::; 400,
2= 20;40;:::; 200 and3= 10;20;:::; 100.
6.5 Two examples for fourth-order tensors
In the above four sections, we compare Algorithms 4 and 5 with the existing
deterministic and randomized algorithms via some third-order tensors. We now
introduce two fourth-order tensor to further illustrate the proposed algorithms.Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 45
The rst tensorA2R200200200200is given by
A=G 1B12B23B34B4+N;
where the entries of G2R50505050andBn2R20050are i.i.d. Gaussian
variables with zero mean and unit variance, and N2R200200200200is an
unstructured perturbation tensor with dierent noise level . The signal-to-
noise ratio (SNR) measure is given in (28) and the denition of RLNE is similar
to (7) for the case of third-order tensors.
The second tensor B2R200200200200is a sparse tensor such that
B=50X
i=11000
i2xiyiziwi+200X
i=511
i2xiyiziwi
where xi,yi,ziandwiare sparse vectors in R200with only 0.05 sparsity.
For the tensorA, the desired multilinear rank is set to f50;50;50;50g.
The desired multilinear rank for the tensor Bis denoted byf;;;g. In
Algorithms 4 and 5, we assume that K= 10 andq= 1. The processing order
in Algorithm 5 is f1;2;3;4g. From the description in Section 4.3, in Algorithm
5, we setT1= ceil(I3),T2= ceil(I2),T3= ceil(2I) andT4= ceil(3)
with= 0:01.
By applying Algorithms 4 and 5, ST-HOSVD, HOOI, randomized ST-
HOSVD and randomized ST-HOSVD with q= 1 to the tensor Awith
SNR = 30; 26;:::; 26;30 and the tensor Bwith= 5;10;:::; 50, the
related results are shown in Figures 11 and 12, respectively. From these two
gures, in terms of CPU time, Algorithm 5 is the fastest one and Algorithms
4 and 5 are faster than ST-HOSVD, HOOI, randomized ST-HOSVD and
randomized ST-HOSVD with q= 1.
In terms of FIT, a) it follows from Figure 11 that randomized ST-HOSVD
is the worst one, Algorithm 5 is worse than Algorithm 4, ST-HOSVD, HOOI
and randomized ST-HOSVD with q= 1, and Algorithm 4 is similar to ST-
HOSVD, HOOI and randomized ST-HOSVD with q= 1; and b) it follows
from Figure 12 that Algorithms 4 and 5 are slightly worse than ST-HOSVD,
HOOI and randomized ST-HOSVD with q= 1.
7 Conclusions
For a given multilinear rank, two ecient randomized algorithms for com-
puting the low multilinear rank approximation were designed by combining
random projection and power scheme. To reduce the complexities of these
two algorithms, we derived two other fast algorithms by applying approximate
matrix multiplication into these two algorithms. We analyzed their theoretical
results by using the bounds of singular values of standard Gaussian matri-
ces and the theoretical results of approximate matrix multiplication. TheSpringer Nature 2021 L ATEX template
46 Ecient algorithms for Tucker decomposition
Fig. 11 Results of applying Algorithms 4 and 5, ST-HOSVD, HOOI, randomized ST-
HOSVD and randomized ST-HOSVD with q= 1 toAwith SNR = 30; 28;:::; 28;30.
Fig. 12 Results of applying Algorithms 4 and 5, ST-HOSVD, HOOI, randomized ST-
HOSVD and randomized ST-HOSVD with q= 1 toBwith= 5;10;:::; 50.
comparisons of the proposed algorithms with the existing deterministic and
randomized algorithms were displayed by high-dimensional image analysis.
We call Problem 1 as the xed multilinear rank problem for low multilinear
rank approximations. We now introduce the xed-precision problem for low
multilinear rank approximations, which summarized in the following problem.
Problem 2. Suppose thatA2RI1I2IN. For a given ">0, the goal is to
ndNpositive integers nand orthonormal matrices Qn2RInnsuch that
kA A 1(Q1Q>
1)2(Q2Q>
2)N(QNQ>
N)kF":
Che and Wei [27] proposed an adaptive randomized algorithm for solving
Problem 2. By this algorithm, we can derive a desired multilinear rank for A
with a given ">0. By using the ecient randomized algorithms for the xed-
precision problem of low rank approximations ([67]) to each mode unfolding
matrix, Minster et al. [26] studied adaptive randomized algorithms to computeSpringer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 47
low multilinear rank approximations for use in applications where the target
multilinear rank is not known beforehand. Hence, one issue is to design fast
and ecient algorithms for solving Problem 2.
Acknowledgements
This work is supported by the Hong Kong Innovation and Technology Com-
mission (InnoHK Project CIMDA), the Hong Kong Research Grants Council
(Project 11204821), City University of Hong Kong (Projects 9610034 and
9610460) and the National Natural Science Foundation of China under the
grant 12271108.
References
[1] Cichocki, A., Lee, N., Oseledets, I.V., Phan, A.H., Zhao, Q., Mandic, D.P.:
Tensor networks for dimensionality reduction and large-scale optimiza-
tion: Part 1 low-rank tensor decompositions. Foundations and Trends in
Machine Learning 9(4-5), 249{429 (2016)
[2] Cichocki, A., Lee, N., Oseledets, I.V., Phan, A.H., Zhao, Q., Mandic,
D.P.: Tensor networks for dimensionality reduction and large-scale opti-
mization: Part 2 applications and future perspectives. Foundations and
Trends in Machine Learning 9(6), 431{673 (2017)
[3] Cichocki, A., Mandic, D.P., De Lathauwer, L., Zhou, G., Zhao, Q., Caiafa,
C.F., Phan, H.A.: Tensor decompositions for signal processing appli-
cations: From two-way to multiway component analysis. IEEE Signal
Processing Magazine 32(2), 145{163 (2015)
[4] Grasedyck, L., Kressner, D., Tobler, C.: A literature survey of low-
rank tensor approximation techniques. GAMM-Mitteilungen 36(1), 53{78
(2013)
[5] Vervliet, N., Debals, O., Sorber, L., De Lathauwer, L.: Breaking the curse
of dimensionality using decompositions of incomplete tensors: Tensor-
based scientic computing in big data analysis. IEEE Signal Processing
Magazine 31(5), 71{79 (2014)
[6] Carroll, J.D., Chang, J.-J.: Analysis of individual dierences in mul-
tidimensional scaling via an n-way generalization of \Eckart-Young"
decomposition. Psychometrika 35(3), 283{319 (1970)
[7] Tucker, L.R.: Some mathematical notes on three-mode factor analysis.
Psychometrika 31(3), 279{311 (1966)
[8] Kressner, D., Tobler, C.: Algorithm 941: Htucker|a MATLAB tool-
box for tensors in hierarchical Tucker format. ACM Transactions onSpringer Nature 2021 L ATEX template
48 Ecient algorithms for Tucker decomposition
Mathematical Software (TOMS) 40(3), 1{22 (2014)
[9] Grasedyck, L.: Hierarchical singular value decomposition of tensors. SIAM
Journal on Matrix Analysis and Applications 31(4), 2029{2054 (2010)
[10] Oseledets, I.V.: Tensor-train decomposition. SIAM Journal on Scientic
Computing 33(5), 2295{2317 (2011)
[11] Kilmer, M.E., Martin, C.D.: Factorization strategies for third-order
tensors. Linear Algebra and its Applications 435(3), 641{658 (2011)
[12] Sedighin, F., Cichocki, A., Phan, A.-H.: Adaptive rank selection for tensor
ring decomposition. IEEE Journal of Selected Topics in Signal Processing
15(3), 454{463 (2021)
[13] Zhao, Q., Zhou, G., Xie, S., Zhang, L., Cichocki, A.: Tensor ring
decomposition. arXiv preprint arXiv:1606.05535 (2016)
[14] Zheng, W.-J., Zhao, X.-L., Zheng, Y.-B., Pang, Z.-F.: Nonlocal patch-
based fully connected tensor network decomposition for multispectral
image inpainting. IEEE Geoscience and Remote Sensing Letters 19, 1{5
(2021)
[15] De Lathauwer, L., De Moor, B., Vandewalle, J.: On the best rank-1 and
rank-(r1;r2;;rn) approximation of higher-order tensors. SIAM Journal
on Matrix Analysis and Applications 21(4), 1324{1342 (2000)
[16] Eld en, L., Savas, B.: A Newton-Grassmann method for computing the
best multilinear rank-( r1;r2;r3) approximation of a tensor. SIAM Journal
on Matrix Analysis and Applications 31(2), 248{271 (2009)
[17] Ishteva, M., Absil, P.-A., Van Huel, S., De Lathauwer, L.: Best low
multilinear rank approximation of higher-order tensors, based on the Rie-
mannian trust-region scheme. SIAM Journal on Matrix Analysis and
Applications 32(1), 115{135 (2011)
[18] Savas, B., Lim, L.-H.: Quasi-Newton methods on Grassmannians and mul-
tilinear approximations of tensors. SIAM Journal on Scientic Computing
32(6), 3352{3393 (2010)
[19] Navasca, C., De Lathauwer, L.: Low multilinear rank tensor approxi-
mation via semidenite programming. In: IEEE 17th European Signal
Processing Conference, pp. 520{524 (2009)
[20] Goreinov, S.A., Oseledets, I.V., Savostyanov, D.V.: Wedderburn rank
reduction and Krylov subspace method for tensor approximation. Part 1:
Tucker case. SIAM Journal on Scientic Computing 34(1), 1{27 (2012)Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 49
[21] Savas, B., Eld en, L.: Krylov-type methods for tensor computations I.
Linear Algebra and its Applications 438(2), 891{918 (2013)
[22] De Lathauwer, L., De Moor, B., Vandewalle, J.: A multilinear singular
value decomposition. SIAM Journal on Matrix Analysis and Applications
21(4), 1253{1278 (2000)
[23] Vannieuwenhoven, N., Vandebril, R., Meerbergen, K.: A new truncation
strategy for the higher-order singular value decomposition. SIAM Journal
on Scientic Computing 34(2), 1027{1052 (2012)
[24] Halko, N., Martinsson, P.-G., Tropp, J.A.: Finding structure with ran-
domness: Probabilistic algorithms for constructing approximate matrix
decompositions. SIAM Review 53(2), 217{288 (2011)
[25] Zhou, G., Cichocki, A., Xie, S.: Decomposition of big tensors with low
multilinear rank. arXiv preprint arXiv:1412.1885v1 (2014)
[26] Minster, R., Saibaba, A.K., Kilmer, M.E.: Randomized algorithms for
low-rank tensor decompositions in the Tucker format. SIAM Journal on
Mathematics of Data Science 2(1), 189{215 (2020)
[27] Che, M., Wei, Y.: Randomized algorithms for the approximations of
Tucker and the tensor train decompositions. Advances in Computational
Mathematics 45(1), 395{428 (2019)
[28] Che, M., Wei, Y.: Theory and Computation of Complex Tensors and Its
Applications. Springer, Singapore (2020)
[29] Sun, Y., Guo, Y., Luo, C., Tropp, J.A., Udell, M.: Low-rank Tucker
approximation of a tensor from streaming data. SIAM Journal on
Mathematics of Data Science 2(4), 1123{1150 (2019)
[30] Ahmadiasl, S., Cichocki, A., Phan, A., Oseledets, I., Abukhovich, S.,
Tanaka, T.: Randomized algorithms for computation of Tucker decom-
position and higher order SVD (HOSVD). IEEE Access 9, 28684{28706
(2021)
[31] Che, M., Wei, Y., Yan, H.: Randomized algorithms for the low multilinear
rank approximations of tensors. Journal of Computational and Applied
Mathematics 390(2021). article no. 113380
[32] Che, M., Wei, Y., Yan, H.: The computation of low multilinear rank
approximations of tensors via power scheme and random projection.
SIAM Journal on Matrix Analysis and Applications 41(2), 605{636
(2020)Springer Nature 2021 L ATEX template
50 Ecient algorithms for Tucker decomposition
[33] Che, M., Wei, Y., Yan, H.: An ecient randomized algorithm for com-
puting the approximate Tucker decomposition. Journal of Scientic
Computing 88(2021). article no. 32
[34] Martinsson, P.-G., Rokhlin, V., Tygert, M.: A randomized algorithm for
the decomposition of matrices. Applied and Computational Harmonic
Analysis 30(1), 47{68 (2011)
[35] Rokhlin, V., Szlam, A., Tygert, M.: A randomized algorithm for principal
component analysis. SIAM Journal on Matrix Analysis and Applications
31(3), 1100{1124 (2010)
[36] Zhang, J., Saibaba, A.K., Kilmer, M.E., Aeron, S.: A randomized tensor
singular value decomposition based on the t-product. Numerical Linear
Algebra with Applications 25(5), 2179 (2018)
[37] Kolda, T.G., Bader, B.W.: Tensor decompositions and applications. SIAM
Review 51(3), 455{500 (2009)
[38] Zhou, G., Cichocki, A., Xie, S.: Fast nonnegative matrix/tensor factor-
ization based on low-rank approximation. IEEE Transactions on Signal
Processing 60(6), 2928{2940 (2012)
[39] Drineas, P., Mahoney, M.W.: A randomized algorithm for a tensor-based
generalization of the singular value decomposition. Linear Algebra and its
Applications 420(2-3), 553{571 (2007)
[40] Saibaba, A.K.: HOID: higher order interpolatory decomposition for ten-
sors based on Tucker representation. SIAM Journal on Matrix Analysis
and Applications 37(3), 1223{1249 (2016)
[41] Drineas, P., Kanna, R., Mahoney, M.W.: Fast Monte Carlo algorithms
for matrices I: approximating matrix multiplication. SIAM Journal on
Computing 36, 132{157 (2006)
[42] Drineas, P., Mahoney, M.W., Muthukrishnan, S.: Subspace sampling and
relative-error matrix approximation: Column-based methods. In: Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms
and Techniques, pp. 316{326. Springer, Barcelona, Spain (2006)
[43] Deshpande, A., Rademacher, L., Vempala, S.S., Wang, G.: Matrix
approximation and projective clustering via volume sampling. Theory of
Computing 2(1), 225{247 (2006)
[44] Wang, S., Zhang, Z.: Improving CUR matrix decomposition and the
Nystr om approximation via adaptive sampling. The Journal of Machine
Learning Research 14(1), 2729{2769 (2013)Springer Nature 2021 L ATEX template
Ecient algorithms for Tucker decomposition 51
[45] Chang, W.-T., Tandon, R.: Random sampling for distributed coded
matrix multiplication. In: ICASSP 2019-2019 IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP), pp.
8187{8191 (2019)
[46] Eriksson-Bique, S., Solbrig, M., Stefanelli, M., Warkentin, S., Abbey, R.,
Ipsen, I.C.: Importance sampling for a Monte Carlo matrix multiplication
algorithm, with application to information retrieval. SIAM Journal on
Scientic Computing 33(4), 1689{1706 (2011)
[47] Niu, C., Li, H.: Optimal sampling algorithms for block matrix multiplica-
tion. Journal of Computational and Applied Mathematics (2023). article
no. 115063
[48] Wu, Y., Polydorides, N.: A multilevel Monte Carlo estimator for matrix
multiplication. SIAM Journal on Scientic Computing 42(5), 2731{2749
(2020)
[49] Cohen, M.B., Nelson, J., Woodru, D.P.: Optimal approximate matrix
product in terms of stable rank. arXiv preprint arXiv:1507.02268 (2015)
[50] Eftekhari, A., Yap, H.L., Rozell, C.J., Wakin, M.B.: The restricted
isometry property for random block diagonal matrices. Applied and
Computational Harmonic Analysis 38(1), 1{31 (2015)
[51] Srinivasa, R.S., Davenport, M.A., Romberg, J.: Localized sketch-
ing for matrix multiplication and ridge regression. arXiv preprint
arXiv:2003.09097 (2020)
[52] Tropp, J.A., Yurtsever, A., Udell, M., Cevher, V.: Practical sketching
algorithms for low-rank matrix approximation. SIAM Journal on Matrix
Analysis and Applications 38(4), 1454{1485 (2017)
[53] Coakley, E., Rokhlin, V., Tygert, M.: A fast randomized algorithm for
orthogonal projection. SIAM Journal on Scientic Computing 33(2), 849{
868 (2011)
[54] Chen, Z., Dongarra, J.J.: Condition numbers of Gaussian random matri-
ces. SIAM Journal on Matrix Analysis and Applications 27(3), 603{620
(2005)
[55] Litvak, A.E., Pajor, A., Rudelson, M., Tomczakjaegermann, N.: Smallest
singular value of random matrices and geometry of random polytopes.
Advances in Mathematics 195(2), 491{523 (2005)
[56] Rudelson, M., Vershynin, R.: Smallest singular value of a random rectan-
gular matrix. Communications on Pure and Applied Mathematics 62(12),Springer Nature 2021 L ATEX template
52 Ecient algorithms for Tucker decomposition
1707{1739 (2009)
[57] Shabat, G., Shmueli, Y., Aizenbud, Y., Averbuch, A.: Randomized LU
decomposition. Applied and Computational Harmonic Analysis 44, 246{
272 (2016)
[58] Boutsidis, C., Zouzias, A., Mahoney, M.W., Drineas, P.: Randomized
dimensionality reduction for k-means clustering. IEEE Transactions on
Information Theory 61(2), 1045{1062 (2014)
[59] Aizenbud, Y., Shabat, G., Averbuch, A.: Randomized LU decomposition
using sparse projections. Computers and Mathematics with Applications
72(9), 2525{2534 (2016)
[60] Woolfe, F., Liberty, E., Rokhlin, V., Tygert, M.: A fast randomized algo-
rithm for the approximation of matrices. Applied and Computational
Harmonic Analysis 25(3), 335{366 (2008)
[61] Gu, M.: Subspace iteration randomization and singular value problems.
SIAM Journal on Scientic Computing 37(3), 1139{1173 (2015)
[62] Bader, B.W., Kolda, T.G.: Algorithm 862: Matlab tensor classes for
fast algorithm prototyping. ACM Transactions on Mathematical Software
(TOMS) 32(4), 635{653 (2006)
[63] Bader, B.W., Kolda, T.G.: MATLAB Tensor Toolbox Version 3.2.1.
Available online. https://www.tensortoolbox.org (April 5, 2021)
[64] Vervliet, N., Debals, O., Sorber, L., Van Barel, M., De Lathauwer, L.:
Tensorlab 3.0. Available online. http://tensorlab.net (2016)
[65] Caiafa, C.F., Cichocki, A.: Generalizing the column-row matrix decom-
position to multi-way arrays. Linear Algebra and its Applications 433(3),
557{573 (2010)
[66] Georghiades, A.S., Belhumeur, P.N., Kriegman, D.: From few to many:
illumination cone models for face recognition under variable lighting and
pose. IEEE Transactions on Pattern Analysis and Machine Intelligence
23(6), 643{660 (2001)
[67] Yu, W., Gu, Y., Li, Y.: Ecient randomized algorithms for the xed-
precision low-rank matrix approximation. SIAM Journal on Matrix
Analysis and Applications 39(3), 1339{1359 (2018)