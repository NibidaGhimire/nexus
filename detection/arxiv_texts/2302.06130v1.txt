1
Learning to Scale Temperature in Masked
Self-Attention for Image Inpainting
Xiang Zhou, Yi Gong, and Yuan Zeng
Abstract ‚ÄîRecent advances in deep generative adversarial net-
works (GAN) and self-attention mechanism have led to signiÔ¨Åcant
improvements in the challenging task of inpainting large missing
regions in an image. These methods integrate self-attention
mechanism in neural networks to utilize surrounding neural
elements based on their correlation and help the networks
capture long-range dependencies. Temperature is a parameter
in the Softmax function used in the self-attention, and it enables
biasing the distribution of attention scores towards a handful
of similar patches. Most existing self-attention mechanisms in
image inpainting are convolution-based and set the temperature
as a constant, performing patch matching in a limited feature
space. In this work, we analyze the artifacts and training
problems in previous self-attention mechanisms, and redesign
the temperature learning network as well as the self-attention
mechanism to address them. We present an image inpainting
framework with a multi-head temperature masked self-attention
mechanism, which provides stable and efÔ¨Åcient temperature
learning and uses multiple distant contextual information for
high quality image inpainting. In addition to improving image
quality of inpainting results, we generalize the proposed model to
user-guided image editing by introducing a new sketch generation
method. Extensive experiments on various datasets such as Paris
StreetView, CelebA-HQ and Places2 clearly demonstrate that our
method not only generates more natural inpainting results than
previous works both in terms of perception image quality and
quantitative metrics, but also enables to help users to generate
more Ô¨Çexible results that are related to their sketch guidance.
Index Terms ‚ÄîImage inpainting, temperature scaling, self-
attention, free-form holes
I. I NTRODUCTION
Image inpainting refers to Ô¨Ålling holes or masked regions
with plausible content coherent with the neighborhood context
while making the entire image visually realistic. It allows
to remove undesired objects or restore damaged regions in
images and serves various computer vision applications, such
as object removal, image editing, image denoising and image-
based blending. Traditional methods [1], [2] infer the pixels
of missing regions by propagating from hole boundaries or
copying background patches into holes starting from low-
resolution to high-resolution. These methods work well for
synthesizing plausible stationary textures, but usually fail
in non-stationary cases where patterns are unique or non-
repetitive. Recent learning-based image inpainting methods
treat image inpainting as a conditional generation problem and
Xiang Zhou and Yi Gong are with Department of Electronic and Electrical
Engineering, Southern University of Science and Technology, Shenzhen,
China. E-mail: zhoux2020@mail.sustech.edu.cn; gongy@sustech.edu.cn
Yuan Zeng is with Academy for Advanced Interdisciplinary Studies,
Southern University of Science and Technology, Shenzhen, China. E-mail:
hi.zengyuan@gmail.comleverage the advancements in deep generative models, such as
Generative Adversarial Networks (GAN) [3] and Variational
Auto-encoders (V AE) [4], to infer semantic content. Such
methods can learn realistic semantics and textures from a
large collection of images and synthesize novel contents in
an end-to-end fashion. Deep learning-based image inpainting
models can be roughly divided into two categories: one-stage
models and two-stage models. One-stage models only include
one deep generative network for image completion. Two-stage
models consist of a content inference network for coarse image
or edge/semantic map completion and a reÔ¨Ånement network
for high quality appearance.
A key challenge in image inpainting is exploiting visible
information globally when holes are large, or the expected con-
tents inside holes have complicated semantic layouts, textures,
or depths. To address this challenge, more recent two-stage
image inpainting methods [5]‚Äì[7] exploited self-attention to
increase the receptive Ô¨Åeld with desired contextual information
in the second stage. Attention mechanisms in neural networks
explicitly model the relation between neural elements based
on their correlation, helping deep learning introduce long-
range dependencies. They share similar principles to non-local
processing, and serve as a crucial component in various natural
language processing and computer vision tasks. For image
inpainting, the self-attention mechanism is very closely linked
to the problem of nearest neighbor-based patch matching. It
performs non-local patch matching via replacing the Ô¨Åltering
of matched patches with a convolution layer, and uses Soft-
max function to transform similarity scores into probabilities.
Temperature is a hyperparameter used in Softmax function
for biasing the distribution of weights towards a handful of
similar patches. Although signiÔ¨Åcant progress has been made
in improving the quality of image inpainting, attention-based
image inpainting models commonly set the temperature to a
constant, making attention on limited spatial locations in fea-
ture space. A possible solution is to learn the temperature and
adaptively attend on similar patches at multiple distant spatial
locations. In [7], we proposed an attention mechanism, called
adaptive multi-temperature mask-guided attention (ATMA),
and showed that ATMA can help GAN model generate higher
quality inpainting results. However, ATMA is a convolution-
based self-attention mechanism which limits the size of patch
and requires iterative computation. Furthermore, its temper-
ature learning process is not stable during training, and the
quality of its results including color discrepancy and edge
responses surrounding holes, needs further improvement. This
work aims to propose a new attention mechanism, improve
the training stability of the temperature learning and furtherarXiv:2302.06130v1  [cs.CV]  13 Feb 20232
(a) Masked Input
 (b) Coarse
Result
Temperature(c) Temperature Scaling Trajectory
TemperatureHigh
Low (d) Temperature
Scaling Features
(e) Final Result
Fig. 1. Illustration of temperature scaling in our masked self-attention for image inpainting. In (c), we show the trajectory of a reÔ¨Åned query patch in mask
region under different temperatures, and the background blue dots represent key patches in known region. The corresponding reÔ¨Åned feature maps are shown
in (d). As temperature increases, the position of reÔ¨Åned patch moves from the most similar key patch position to the center, and the reÔ¨Åned feature map
becomes smooth. Our module adaptively tunes the temperature to get the Ô¨Ånal inpainting result.
reduce visual artifacts.
Similar to contextual attention in [5], ATMA performed
patch matching as convolution-based attention. It used key
patches as convolution kernels that can be properly normalized
for cosine similarity calculation, while query patches remained
not normalized due to limitation of convolution-based attention
calculation. For low constant temperature, such mechanisms
work well on patch matching. While for learning temperature,
query patches should be normalized to prevent length of
queries affect on temperature learning. In contrast to doing
convolution-based self-attention, this work proposes a masked
self-attention mechanism that does dot-product attention with
queries, keys and values. By normalizing queries and keys, the
similarity comparison can be implemented with simple matrix
multiplications in parallel. The proposed attention module
enables the use of patches of any size, and provides efÔ¨Åcient
temperature learning.
For the temperature training stability and visual artifacts
in ATMA [7], we identify the causes for these issues and
describe changes in architecture that eliminate them. First,
we investigate the origin of the temperature training stability
problem. We Ô¨Ånd that the LeakyReLU activation function used
in the last layer of the temperature learning network may
produce negative temperature, resulting in the largest weight
on the most dissimilar neural patch and unstable training
of temperature. In Section III-D, we redesign the activation
function, which provides training with stabilized temperature
learning network against negative temperatures. These changes
in architecture are simple, fast and stable in training, producing
high quality inpainting results for free-form holes.
In this work, we carefully intervene in each component
of the coarse-to-Ô¨Åne image inpainting framework in previous
work [7] to alleviate the problem and unlock the potential
of the adaptive self-attention for better image inpainting. We
propose a novel free-form image inpainting framework with a
multi-head temperature masked self-attention mechanism toimprove training stability and inpainting quality. The main
contributions of this work are summarized as follows:
A novel multi-head temperature masked self-attention
(MHTMA) mechanism that exploits context information
to ensure appearance consistency is introduced and inte-
grated into a coarse-to-Ô¨Åne image inpainting framework.
It addresses high quality image inpainting by optimizing
the feature space for matching, and it utilizes learnable
temperatures to attend on matched patches at multiple
distant spatial locations, as shown in Fig. 1. In addition,
it enables parallel non-local patch similarity comparison
and provides efÔ¨Åcient temperature learning with normal-
ized queries, keys and values.
We redesign a temperature learning network with a new
activation function for stable training. During training, the
temperature parameter which controls the weight distri-
bution in the masked self-attention is directly optimized.
We conduct an in-depth analysis of the effectiveness
of different temperature scaling strategies and network
branches on inpainting performance. Experimental results
show the proposed method can improve image generation
quality by learning to control the conÔ¨Ådence of using
similar neural patches for patch propagation.
We demonstrate the proposed inpainting framework
achieves higher quality free-form inpainting than previous
state-of-the-art on challenge inpainting datasets, including
CelebA-HQ and Places2.
II. R ELATED WORK
A. Image Inpainting
Traditional image inpainting methods include diffusion-
based and patch-based methods. Diffusion-based methods use
variational algorithms to propagate color information from the
known regions to the holes. These methods work well for
small or narrow missing regions, but they tend to result in3
over-blurring when the missing regions grow larger. Patch-
based methods perform texture synthesis techniques to copy-
paste neighboring patches from the background regions into
the missing regions [2], [8]. Compared to diffusion-based
methods, they can Ô¨Åll in large missing regions with stationary
texture and result in high quality texture. Criminisi et al.
[9] proposed to optimize patch search using multiple scales
and orientations. In [2], an approximate nearest neighbor
algorithm, called PatchMatch, was proposed and has shown
signiÔ¨Åcant practical values for image inpainting. To further
reduce computational complexity of patch matching and im-
prove search speed, a coherency sensitive hashing-based image
inpainting approach was presented in [10]. Although patch-
based methods can produce convincing continuations of the
background, they cannot generate semantic or novel content.
Recently, many learning-based single-stage image inpaint-
ing methods have been proposed to use deep learning or GAN
strategy to Ô¨Åll in missing regions with semantic content. Con-
text encoders [11] Ô¨Årstly handled 6464sized holes by train-
ing deep neural networks with both pixel-wise reconstruction
loss and generative adversarial loss. Iizuka et al. [12] proposed
local and global discriminators to improve local texture and
overall image layout. Zheng et al. [13] proposed a variational
auto-encoders [4] (V AE)-based pluralistic image inpainting
approach to generate diverse image inpainting results. To better
handle irregular holes, a partial convolution was introduced for
image inpainting in [14], where the convolution was masked
and re-normalized to use valid pixels only. Suvorov et al.
[15] proposed a resolution-robust large mask inpainting model
with fast Fourier convolutions. Wang et al. [16] presented a
dynamic selection network to avoid the interference of the
invalid information from holes and utilize valid information
adaptively for image inpainting.
Our work is more closely related to two-stage GAN-based
models that infer a coarse image in the Ô¨Årst stage and after-
wards reÔ¨Åne the coarse image with visually pleasant textures
in the second stage. Yang et al. [17] proposed to improve
the models in [11] using multi-scale neural patch synthesis in
the second stage. EdgeConnect model [18] predicted salient
edges and afterwards generated completion results guided by
the prior edges. Yu et al. [5] introduced contextual attention
that enables trainable patch matching in the second stage to
produce higher quality inpainting results, and further improved
the model using gated convolution and a patch-based GAN
loss for free-form image inpainting in [19]. Zeng et al. [20]
performed conventional patch matching in the second stage to
copy-past high-frequency missing information from training
exemplars and generated diverse high quality outputs. Zheng et
al. [6] proposed a two-stage image inpainting model that uses
a transformer-based architecture for coarse image inpainting
in the Ô¨Årst stage and an attention-aware layer for improving
appearance consistency between visible and generated regions
in the second stage. In [21], a transformer-based network was
designed to recover pluralistic coherent structures together
with coarse textures, while the latter convolutional neural net-
work (CNN) was conducted to enhance the local texture details
of coarse priors. Xu et al. [22] proposed a Texture Memory-
Augmented deep image inpainting framework, where texturegeneration is guided by a texture memory of patch samples
extracted from unmasked regions in the second stage. In [7],
we introduced multiple self-adaptive temperature parameters
to control the scale of the softness of the contextual attention
in the second stage. However, the training process of the model
in [7] is not stable and the inpainting results of the model still
need to be improved, such as color consistency and texture
details. In this work, we focus on analyzing the effectiveness
of temperature scaling on image inpainting and generating
more realistic inpainting results with an improved temperature
learning method.
B. Self-Attention
Self-attention mechanism has been successfully used in
machine translation , increasing the modeling capacity of
deep neural networks by concentrating on crucial features and
suppressing unimportant ones. Recently, many studies have
explored the effectiveness of self-attention on deep learning-
based image generation, proving that distant relationship
modeling via attention mechanisms enables learning high-
dimensional and complex image distribution to generate more
realistic outputs. Zhang et al. [23] proposed a self-attention
module to reconstruct each feature point using the weighted
sum of all feature points. Yu et al. [5] proposed a contextual
attention mechanism using a convolution-based self-attention
mechanism to attend on related features at distant spatial lo-
cations. Liu et al. [24] proposed a coherent semantic attention
layer to construct the correlation between neural features of
masked regions. Zheng et al. presented a self-attention layer
that exploits short-long term context information to improve
appearance consistency in [13] and modiÔ¨Åed the attention
layer to handle the attention to visible regions separately from
masked regions in [6]. In [7], we designed a mask-guided
attention layer similar to the attention mechanisms in [5]
and [25] that enable learnable non-local patch matching, and
proposed to learn temperatures to attend on neural features at
multiple spatial locations. Unlike existing methods, instead of
doing a convolution-based self-attention calculation for train-
able patch matching, this work designs a masked self-attention
to match patches via matrix multiplication of normalized query
and key patches, which provides an efÔ¨Åcient way to learn the
temperature.
C. Temperature Scaling
Given a vector x= (x1;;xN)2RN, it can be trans-
formed into a probability vector with the Softmax function
as
Softmax( x=t)i=exi=t
PN
n=1exn=t; (1)
where Softmax( x=t)i2(0;1), andPN
n=1Softmax( x=t)n=
1.t >0is a temperature parameter, which is able to adjust
the smoothness of the output distribution. The lower the
temperature, the sharper or harder the distribution gets, and
the larger the temperature, the Ô¨Çatter or softer the distribution
will be. Temperature scaling has been shown to be important
in supervised learning [26], model calibration [27], knowledge4
distillation [28], [29], contrastive learning [30], [31], image
generation [5], [25] and neutral language supervision [32],
[33]. Existing temperature scaling approaches can be catego-
rized into three groups: constant temperature adjusting, man-
ually designed temperature tuning and adaptive temperature
learning.
Constant temperature adjusting treats the temperature as
a constant and adjusts it before model training. Hinton et
al. [28] adopted a high temperature in Softmax function for
knowledge distillation. The temperature was applied to show
rich similarity structure of a cumbersome model. In [34], a
constant temperature was used in a dot-product attention to
avoid vanishing gradient. Yu et al. [5] proposed a contextual
attention for image inpainting, and a low constant temperature
was used in contextual attention to weight the similarity
between foreground and background patches. Caccia et al.
[35] established a quality-diversity evaluation procedure for
language generation using temperature tuning over local and
global sample metrics.
Manually designed temperature tuning approaches tune the
temperature parameter iteratively with pre-designed strategies.
Kirkpatrick et al. [36] introduced a temperature in Simulated
Annealing to balance the exploration and exploitation trade-
off in combinatorial optimization, and used exponentially
decreasing annealing strategy to tune the temperature. Sim-
ilar temperature tuning strategy was also used in Boltzmann
Machine learning algorithm [37], and Boltzmann exploration
[38] in reinforcement learning.
Recently, many approaches have been proposed to adap-
tively tune temperature during training deep learning models.
For example, Lin et al. [33] used a neural network to learn
temperature and adaptively control the softness of attention
for better neural machine translation performance. Radford
et al. [32] optimized the temperature parameter as a log-
parameterized multiplicative scalar for language supervision.
Zhang et al. [31] proposed a temperature learning-based ap-
proach to generate uncertainty scores for many contrastive
methods. Pl√∂tz et al. [25] introduced temperature as a re-
laxation indicator to the approximation of neural nearest
neighbor. In [7], we adopted multiple learnable temperatures
in a mask-guided attention to adaptively adjust the softness
of the attention, resulting in the attention module attending
on matched features with different concentration levels and
capturing long-term context for better image completion. In
this work, we further analyze the effectiveness of temperature
scaling on image inpainting, and improve the temperature
learning approach with a multi-head attention and a modiÔ¨Åed
activation function for better robustness during training.
III. A PPROACH
This section presents the proposed framework from top
to bottom. We Ô¨Årst introduce the overview of the proposed
framework. Then, we deÔ¨Åne the loss functions used to train the
framework. After that, details of the multi-head temperature
masked self-attention and learning-based self-adaptive control
of temperature are described.A. Coarse-to-Ô¨Åne Inpainting framework
The overview of the proposed image inpainting framework
is illustrated in Fig. 2. Given an input incomplete image Iin,
which is a degraded version of a complete image Igt. Image
inpainting aims at generating visually realistic result Ioutfrom
Iinwith information of Igt. The proposed inpainting frame-
work is a two-stage coarse-to-Ô¨Åne inpainting architecture. It
consists of three parts: a coarse completion network Gcfor
roughly estimating the missing regions of the input Iinand
producing a coarse inpainting result Ic; a reÔ¨Ånement network
Grfor generating the Ô¨Ånal output Ioutwith Ô¨Åner texture
and better appearance consistency; a global-local discriminator
Dglfor calculating adversarial losses. The three parts are
sequentially cascaded and trained in an end-to-end manner.
Coarse Completion Network The coarse completion net-
workGcis based on a simple encoder-decoder architecture,
where gated convolutions [19] are used at both ends and
dilated gated convolutions are used in the middle. Gated
convolution learns optimal mask automatically from training
data and assigns soft values to each spatial location in deep
layers. It performs dynamic feature selection between the
mask regions and existing regions for better image inpainting.
Dilated gated convolutional layers are used in the mid-layers
to compute each output pixel with a much larger input area.
The input of the network Gcis an RGB image with free-form
holes, and the output Icis an image with entire content. The
encoder extracts neural features of the input Iinand reduces
its spatial dimension, and the decoder is adopted to restore the
feature dimensions and generate Ic.
ReÔ¨Ånement Network The intermediate image Icis then
fed into the reÔ¨Ånement network Grfor high quality image
inpainting. Compared with Iin,Icis more semantically plau-
sible and coherent with known regions, which allows Gr
to learn better feature representation for better appearance
consistency. Grconsists of two parallel encoders and a single
decoder. The Ô¨Årst branch encoder tries to capture distant
context information using gated convolutions and an improved
self-attention module. The second encoder is built with gated
and dilated gated convolution layers, and tends to aggregate
local texture information. The joint usage of the two encoders
provides complementary information from each other. The
decoder consists of 5 gated convolution layers and a standard
convolution layer. It takes the features extracted from the two
encoders as input and generates the Ô¨Ånal inpainting output
Iout.
Global-local Discriminator The output Ioutis then dis-
criminated by a global-local discriminator Dgl. The discrim-
inator assesses both of the global and local consistency of
Ioutusing two parallel feature extraction branches, including
a global branch and a local branch. SpeciÔ¨Åcally, the global
branch is performed based on the entire output, guiding the
framework to generate a globally coherent inpainting result.
It uses 6 convolution layers and a fully-connected layer to
extract neural features of the output Iout. The local branch
consists of 5 convolution layers and a fully-connected layer,
and its input is a 128128sized patch that randomly cropped
from the output Iout. The local branch assesses small local5
ùêåùêàùëñùëõùêàùëê
ùêà
ùêàùëúùë¢ùë°
Real /
Fake?ùê∫ùëê
ùê∫ùëü
ùê∑ùëîùëôAttention
Branch
Conv.
Branch
Embedding Network
Attention Module
Stage 2 DecoderConv. Encoder
Attention EncoderCoarse Network
Global Local Critic
Fig. 2. Illustration of our coarse-to-Ô¨Åne image inpainting framework. Given an incomplete image, we use an encoder-decoder network in the Ô¨Årst stage to
roughly Ô¨Åll in the missing regions. Then, a reÔ¨Ånement network is designed in the second stage to capture distant contextual information and aggregate local
textual information, which generates Ô¨Ånal inpainting results with Ô¨Åner texture details and better appearance consistency.
areas centered at the completed regions in pursuit of local
consistency of the generated patches. After that, the features
extracted from the global and local branches are concatenated
into a single feature vector and fed into a fully-connected layer.
Finally, a sigmoid function is used to distinguish whether the
generated output is real or fake.
B. Loss Functions
We train our model with a decoupled joint loss function
to handle consistency of both high-level semantics and low-
level textures. The joint loss includes reconstruction loss LG
r,
perceptual lossLG
pand adversarial losses LD
adv,LG
adv. More
details are given as follows.
Reconstruction Loss We usel1distance to compute the
per-pixel reconstruction loss LG
r. The reconstruction loss
makes the constraints that the intermediate output Icand the
Ô¨Ånal output Ioutshould approximate the ground-truth Igt, that
is,
LG
r=1kIc Igtk1+2kIout Igtk1; (2)
where1and2are scales.
Perceptual Loss We adopt the perceptual loss [39] to
penalize the perceptual and semantic differences between the
intermediate output Icand the ground-truth Igt, which is given
by
LG
p=kVi(Ic) Vi(Igt)k1; (3)
whereViis the activation of the ith layer of the VGG-16 net-
workV. The networkVis pre-trained for image classiÔ¨Åcation
on ImageNet [40] and used as a Ô¨Åxed loss network. In our
experiments, we use ReLU1 _2for feature extraction.
Adversarial Loss Motivated by global and local GANs
[12] and spectral-normalization GANs [41], we adopt the fast
approximation algorithm of spectral normalization described
in [41] together with hinge loss [42] to discriminate if the inputis real or fake. The adversarial loss functions for discriminator
and generator are given by
LD
adv=EIpdata(I)[1 Dgl(I)]+
+EIoutpdata(Iout)[1 +Dgl(Iout)]+;
(4)
and
LG
adv= EIoutpdata(Iout)[Dgl(Iout)]; (5)
where []+is a ramp function, i.e., [x]+= max(0;x).LD
adv
andLG
advdenote the adversarial objective for discriminator
and generator, respectively.
Full objective With a weighted sum of the aforementioned
losses, our generator is optimized by minimizing the following
objectiveLG:
LG=LG
r+pLG
p+G
advLG
adv; (6)
wherepandadvare scales. We set 1= 1:2,2= 1,
p= 0:004andG
adv= 0:01in our experiments.
C. Multi-head Temperature Masked Self-attention
As illustrated in Fig. 3, the proposed multi-head temperature
masked self-attention (MHTMA) aims at converting the input
feature cube Fin2RHWCto a reÔ¨Åned feature cube Fout2
RHWC.
Single Temperature Masked Self-Attention The input
feature cube Finis Ô¨Årst transformed into an embedded feature
cubeFe2RHWC0with lower channel dimensions using
a channel-wise fully connected network Fdr().
Then, we use unrolling operation [43] to extract patches
with size of ssfrom Fe, and get queries Q=
(q1;:::;qNq)2RNq(ssC0), keys K= (k1;:::;kNk)2
RNk(ssC0)and values V = ( v1;:::;vNk)2
RNk(ssC0). Note that the number of extracted query
patchesNqis different from the number of extracted key
patchesNk, since zero-padding is performed on feature maps
Fewhen extracting query patches to ensure the size of Fout6
Extract 
Patches‚Ñ±ùëëùëü(‚ãÖ)
Cosine
Similarity
‚Ñ±ùëõ(ùêê)‚Ñ±ùëõ(ùêä)
ùêí
ùêí‚Ñ±ùëõ(‚ãÖ)
‚Ñ±ùëõ(‚ãÖ)Extract 
Patches ùêå
ùê¶ùê¶ùëñ ùêå‚Ä≤
Down
Sampling 
ùíé‚Ä≤
+ùúÜùëö‚àíùúÜùëöùêímSoftmax (¬∑ )
ùêï
[ùë°1,ùë°2]
ùêÖùëñùëõ ùêÖùëí ùêÖùëúùë¢ùë°
Element -wise product
Matrix product
Concat
‚Ñ±ùëö(‚ãÖ)
RollingHead 1Head 2
mùëñ
Fig. 3. Illustration of our attention module MHTMA.
is the same as those of Fin. After that, we normalize queries
Qand keys K, and calculate the attention score S2RNqNk
as
S=Fn(Q)Fn(K)T; (7)
whereFndenotes normalization.
LetMdenote a binary mask with values 1 for missing
regions and 0 for elsewhere. We Ô¨Årst down-sample the original
maskMinto the size of the input feature cube, which is H
W. For each pixel i, we extract a patch miwith sizess
centered on pixel ifrom the down-sampled mask and transfer
the patch miinto a binary value m0
i, which is given by
m0
i=Fm(mi) =(
1;if sum( mi) = 0
0;otherwise(8)
After updating mask values for all pixels in the down-sampled
mask, we obtain a mask vector m02RNk1. Note that the
number of mask patches is the same as the number of keys
inK. We construct a new mask M0=1(m0)Tusing an all-
ones column vector 12RNq1, and update the attention score
matrix Sas
Sm=M0(S=t+m) m; (9)
wheremis a hyperparameter, denotes element-wise pro-
duction,tdenotes the temperature parameter and Smdenotes
the masked attention score matrix. The masked attention score
Smis adopted to adjust the attention score to a lower constant
level m, when keys are extracted from missing regions.
After that, Softmax function is employed to transform the
masked attention score matrix Sminto attention coefÔ¨Åcients
W, which is given by
W= Softmax( Sm); (10)
Given the attention coefÔ¨Åcients W, we transform the values
Vinto reÔ¨Åned feature patches P2RNqssC0, which is
given by
P=WV; (11)and use rolling operation [43] to form the reÔ¨Åned feature
maps Fr. The temperature parameter tin Eq. (9) is adopted
to control the output distribution, which corresponds to the
attention coefÔ¨Åcients used for sum of different neural patches.
The lower the temperature t, the harder distribution will be,
and the higher the temperature t, the softer the distribution
gets. A harder distribution indicates that the model is more
conÔ¨Ådent to use the most similar neural feature patch, and the
model tends to use a greater number of neural feature patches
for restoration with a softer distribution.
MHTMA To utilize different degrees of softness and make
the model attend on features at multiple distant spatial loca-
tions, we design a multi-head masked self-attention (MHTMA)
module. Single temperature masked self-attention is believed
to attend on one position in each row, since the output of Soft-
max typically would have one dimension signiÔ¨Åcantly larger
than other dimensions in each row. MHTMA module employs
multiple temperature scaled masked self-attention in parallel,
which was proposed to jointly attend on multiple positions and
better capture long-range contextual dependencies. The Ô¨Ånal
reÔ¨Åned feature cube FoutofKheads is given by
Fout= Concat i2[K][Fi
r]; (12)
where Fi
rdenotes the reÔ¨Åned feature cube obtained from the i-
th head. As illustrated in Fig. 3, the Kgroups of reÔ¨Åned feature
cubes are then rolled and concatenated to generate Fout. In
this work, we design a learning network to adaptively control
the temperatures. The details of the temperature learning are
presented in the next subsection.
D. Learning-based Self-Adaptive Control of Temperature
We design an embedding network Femb to learn the
temperatures from the input feature maps Fin. Inspired by
recent CNN-based temperature learning models [7], [33], our
embedding network follows a CNN architecture similar to the
one used in [7]. As illustrated in Fig. 4, the network Femb7
√ó 4
-10 -5 0 5 10
Input0510OutputSoftPlus Activation FunctionConv
LeakyReLU
Global Average
PoolingGlobal Max
Pooling
Linear
SoftPlus
Fig. 4. Illustration of our embedding network for temperature learning.
Algorithm 1 Training of the proposed single temperature
masked self-attention
Input: Input feature map Fin, Mask M, Temperature t, Batch
sizeN.
Output: Output feature map Fr.
1:Fe F dr(Fin).
2:Q;K;V Unrolling( Fe).
3:S F n(Q)Fn(K)T// Self-attention based.
4:Compute M0based on M.
5:Sm M0(S=t+m) m.
6:W Softmax( Sm).
7:P VW .
8:Fr Rolling( P).
Ô¨Årst uses 4 convolution layers to encode Fin. Then, a global
average pooling (GAP) layer and a global max pooling (GMP)
layer are employed in parallel to extract global information.
After that, the outputs of GAP and GMP are concatenated and
transformed by a fully connected layer. In [7], LeakyReLU
was used as the last activation function, which may produce
negative temperatures and cause unstable training. To address
this issue, our network Fembuses Softplus as the last activation
function, which is given by
Softplus(x) = ln(1 +e x); (13)
and the overall learning of a temperature can be formulated
as
t= Softplus(Femb(Fin)): (14)
Learned temperature tis then used to scale the attention
scoreSmin Eq. (9). The training of our attention mechanism
using a single learned temperature is given in Algorithm
1. ForKheads, we use Kdifferent channel-wise fully
connected networks Fdrto calculate the similarity of patches
in projection on different hyperplanes.IV. E XPERIMENTS
A. Datasets
We evaluate the proposed image inpainting method on three
benchmark datasets including Paris StreetView [44], CelebA-
HQ [45] and Places2 [46].
Paris StreetView [44]: It contains 15,000 images with reso-
lution of 936537. The images are mainly composed of highly
structured buildings with regular patterns, supplemented by
urban natural landscapes. We follow the default split that
produces 14,900 training images and 100 validation images.
CelebA-HQ [45]: It contains 30,000 aligned face images
with a maximum resolution of 1024 1024. We randomly
select 1,000 male and 1,000 female faces to form a gender-
balanced test set, and use the remaining 28,000 for training.
Places2 [46]: We use the high-resolution version of
Places365-Standard dataset as our Places2 dataset. It contains
1,803,460 training samples of more than 300 scenes. We
randomly select 2000 images from the validation set as test
images in our experiments.
To generate incomplete images as inputs, we use a mask
generation method presented in [19]. SpeciÔ¨Åcally, a union of
a random free-form mask and a 96 96 sized square mask is
generated on-the-Ô¨Çy during training and inference.
B. Implementation details
The proposed model is trained with PyTorch v1.9.1, CUDA
v11.1, cuDNN v8.0.5 on a PC with an Intel Xeon CPU and
two 24G NVIDIA GeForce RTX 3090 GPUs. The resolution
of the input images is 256 256. Before feeding the images
into the model, we Ô¨Årst resize the images and augment them
using horizontal Ô¨Çip with a probability of 0.5. The missing
regions are initialized with a constant value, i.e., ImageNet
mean RGB value [0:485;0:456;0:406]. The proposed model
is trained with a batch size of 16 and optimized using Adam
optimizer [47] with 1= 0:5and2= 0:9. The learning
rates for the generator and discriminators are set to 1e-4 and
1e-4, respectively. The training process is terminated when
the validation loss is not decreased within 30 epochs, and the
model with the smallest validation loss is saved for test. It
takes around 6 days for training Paris StreetView model, 7
days for CelebA-HQ and 10 days for Places2. The inference
is performed on a single GPU, and our model takes 0.1 seconds
per 256256 image.
C. Experiment setup
We conduct three experiments to analyze and evaluate the
proposed image inpainting model. First, we do ablation studies
to analyze the impact of different temperature scaling meth-
ods, different generation branches in the reÔ¨Ånement network,
and different adversarial loss functions on inpainting quality.
Then, we do quantitative and qualitative comparisons between
the proposed model and Ô¨Åve existing state-of-the-art image
inpainting methods:
- PatchMatch [2]: It is known as one of the state-of-the-art
non-learning-based approaches.
- ParConv [14]: It is a partial convolution-based image
inpainting model for irregular holes.8
(a) Ground Truth
 (b) Input
 (c) W/ CA
 (d) W/ ATMA
 (e) W/ MHTMA
Fig. 5. Visual comparison between the proposed model with MHTMA and the two baseline models, which replace the MHTMA with contextual attention
and ATMA. The MHTMA-based model generates more natural results with fewer artifacts than the CA-based and ATMA-based baseline models.
- DeepFillv2 [19]: It is an improved version of the image
inpainting method DeepFill [5], by combining gated convolu-
tion generator along with spectral-normalized discriminator to
handle free-form image inpainting.
- WaveFill [48]: It is a wavelet-based image inpainting
method, which synthesizes missing regions at different fre-
quency bands explicitly and separately.
- ATMA [7]: It is a free-form image inpainting method,
which includes an adaptive multi-temperature mask-guided
attention mechanism in the reÔ¨Ånement network of DeepFill
[5].
TABLE I
QUANTITATIVE COMPARISON OF IMAGE INPAINTING USING DIFFERENT
ATTENTION MECHANISMS IN NETWORKS ON CELEB A-HQ
Methods MAE # PSNR " SSIM " FID#
ours 1.282% 28.82 0.9167 5.147
w/ ATMA 1.372% 28.44 0.9108 5.799
w/ CA 1.309% 28.71 0.9132 5.425
We use the Content-Aware Fill of Adobe Photoshop to
generate the inpainting results of PatchMatch, and retrain
all deep learning models including ParConv1, DeepFillv22,
WaveFill3and ATMA using exactly the same dataset setting
and mask generation method as our model. After that, we
evaluate the effectiveness of the proposed model in user-guided
inpainting.
D. Ablation studies
Effect of MHTMA To evaluate the effectiveness of the
proposed MHTMA module, we compare our model with two
baseline free-form inpainting models: ATMA-based model
and CA-based model. The two baseline models are modiÔ¨Åed
versions of the proposed model, where ATMA-based model
replaces MHTMA with ATMA proposed in [7] and the CA-
based model replaces MHTMA with the Contextual Attention
(CA) proposed in [19]. The contextual attention mechanism
is a convolution-based self-attention algorithm, which uses a
constant temperature to adjust the attention scores and generate
Ô¨Åner details. The training of the contextual attention module
is given in Algorithm 2.
1ParConv: https://github.com/naoto0804/pytorch-inpainting-with-partial-
conv
2DeepFillv2: https://github.com/JiahuiYu/generative_inpainting
3WaveFill: https://github.com/yingchen001/WaveFillAlgorithm 2 Training of contextual attention module [5]
Input: Input feature map Fin, Mask M, Temperature1
10,
Batch sizeN.
Output: Output feature map Fout.
1:forn= 1:::N do
2:pn Unrolling( Fin;n).
3:p0
n F n(pn). // Normalization.
4:Sn Conv2d( Fin;n;p0
n). // Convolution-based.
5: Compute M0
naccording to Mn.
6:Sm;n M0
nSn.
7:Wn Softmax( Sm;n=1
10)M0
n.
8:Fout;n TransposedConv2d( Wn;pn).
9:end for
10:Fout (Fout;1;Fout;2;:::;:::; Fout;N ).
The visual comparison results in Fig. 5 and the qualitative
comparison results in Table I show that the proposed model
with MHTMA generates inpainting results with better visual
and numerical performance than the two baseline models. This
is because that MHTMA can learn temperatures in a stable
way and uses suitable neural patches for high quality image
generation. In addition, since attention scores in ATMA are
adjusted using learned temperatures, the ATMA-based model
generates more realistic results with fewer artifacts than the
CA-based model.
0 2 4 6 8 10
Iteration num #10500.10.20.30.40.5Temperature
t1
t2
(a) Our model
0 2 4 6 8 10 12
Iteration num #10505101520Temperature#10-6
t1
t2 (b) ATMA-based model
Fig. 6. Temperature training curves of the proposed model and ATMA-
based model. Replacing activation function LeakyReLU with Softplus makes
the temperature learning network converge to a high temperature and a low
temperature.
Temperature training curves The temperature training
curves of the proposed model and ATMA-based model [7] are
illustrated in Fig. 6(a) and Fig. 6(b), respectively. We observe9
that the proposed model learns two different temperatures t1
andt2. The temperature learning process of our model can
be divided into three stages. At the beginning of the training,
temperatures cool rapidly to copy-and-paste the nearest neigh-
bor. This can be explained that the nearest neighbor patch
provides more meaningful information for image restoration
than learned features at the beginning of the training. Since
the learned features gradually contain meaningful information
after the model is trained for a period of time, temperatures
warm up to make use of more similar patches for image
restoration. The model converges to a high temperature and
a low temperature with enough training iterations. In addition,
although ATMA-based model learns two different tempera-
tures with enough training iterations, the learning process is
unstable and fails to converge. This is because that ATMA
uses LeakyReLU as the last activation function for temperature
learning, allowing negative temperature and causing unstable
training. Unlike ATMA, the proposed MHTMA uses Softplus
to restrict the learned temperatures to be positive and gets
stable temperature learning.
Computational efÔ¨Åciency of the proposed model We
compare the computational efÔ¨Åciency of our model with those
of our previous model ATMA [7]. We repeat the feedforward
pass of each model 500 times and calculate their average
consuming time. Fig. 7(a) and Fig. 7(b) show the comparison
results on single and two GPUs, respectively. The results in
Fig. 7 show that the time for ATMA to make a feedforward
pass is increased linearly with increasing the number of batch
size. Compared with ATMA, the proposed model consumes
less time under different batch sizes, especially with 16 batch
size per GPU.
24 8 16
Batch size00.050.10.150.20.250.3Time (s)
for-loop computing
parallel computing
(a) On single RTX 3090 GPU
4 8 16 32
Batch size00.050.10.150.20.250.3Time (s)
for-loop computing
parallel computing (b) On two RTX 3090 GPUs
Fig. 7. Comparison of computational efÔ¨Åciency between for-loop and parallel
implementation of our attention. We observe that the proposed model with
parallel computing consumes less time than a baseline model using existing
for-loop computing under different batch sizes.
Effects of different branches in the second stage of the
proposed model The second stage of our model consists of
two branches: a convolution branch and an attention branch,
for generating Ô¨Åner details based on the Ô¨Årst stage. Existing
two-stage image inpainting models [7], [19] have demonstrated
the effectiveness of the dual-branch-based second stage on
Ô¨Åner detail generation. In this subsection, we analyze how
each branch affects the feature extraction and image generation
of the proposed model, e.g., turning off the attention branch
during inference to visualize the image inpainting resultsof our model. Fig. 8 shows the inpainting results of the
proposed model with different branches in the second stage.
Fig. 8(c) and Fig. 8(d) show that both convolution branch
and attention branch can restore the missing regions with
reasonable semantic information, but with different texture
information. In addition, Fig. 8(e) and Fig. 8(f) show the
inpainting results of our model using high temperature and
low temperature in attention branch, respectively. We observe
that the inpainting results of the model with high temperature
in attention branch contain more texture details, but suffer from
uncertainty and inconsistency in color and texture generation.
The model with low temperature in attention branch generates
more realistic inpainting results with certain and consistent
colors and textures. Our full model with both convolution and
attention branches produces the Ô¨Ånal visually realistic results,
see Fig. 8(g).
WGAN-GP vs HAL-SN Wasserstein Generative Adversar-
ial Network (WGAN-GP) [49] and Hinge Adversarial Loss
with discriminator Spectral Normalization (HAL-SN) [41],
[42] are two general methods for stabilizing training in GAN-
based image generation. We do ablation studies on CelebA-
HQ to illustrate the effectiveness of WGAN-GP and HAL-
SN. Quantitative comparison results in Table II show that our
model with HAL-SN generates slightly better inpainting re-
sults than the one with WGAN-GP. In addition, the training of
the HAL-SN-based model is faster than those of the WGAN-
GP-based model, since the HAL-SN-based model processes
71.8 batches per minute and the WGAN-GP-based model
processes 59.0 batches per minute. Thus, we use HAL-SN
loss instead of WGAN-GP loss in our model.
TABLE II
QUANTITATIVE COMPARISON OF WGAN-GP AND HAL-SN ON
CELEB A-HQ.
MAE# PSNR(dB)" SSIM" FID#
WGAN-GP 1.603% 26.39 0.8981 5.559
HAL-SN 1.571% 26.67 0.9008 5.392
E. Comparison with existing work
Qualitative comparison Fig. 9 shows qualitative compari-
son results on CelebA-HQ. We observe that PatchMatch fails
to recover large missing regions with complex content by
copying similar patches from known regions. ParConv gen-
erates reasonable structures with blurry textures. DeepFillv2
makes improvements by gated convolution and contextual
attention, but still exhibits observable unpleasant boundaries
and blurry textures. WaveFill generates more semantically
reasonable results with less artifacts. However, some restored
regions still contain unnatural texture details, such as eyes
and mouths. ATMA produces results with more texture details
by capturing long-range contextual information with learnable
temperatures, but the generated details in teeth and mouths are
still not as delicate as the known ones. As a comparison, the
inpainting results by the proposed method are more natural
with Ô¨Åner textures and less color inconsistency.
Next, we report the visual comparison results on the more
challenging Places2 dataset by comparing to the previous10
(a) Ground Truth
 (b) Input
 (c) W/ convolution
 (d) W/ attention
 (e) W/o low
temperature attention
(f) W/o high
temperature attention
(g) Full Model
Fig. 8. Image inpainting results of different branches used in the second stage. The inpainting results in (c) and (d) show that either convolution or attention
branch can generate reasonable semantic information for missing regions, and generate different texture details. Results in (e) and (f) show that our model
with low temperature head in attention branch generates more visible acceptable results than the one with high temperature head. This indicates that the low
temperature branch helps to generate the certain and basic part of image, while high temperature branch helps the model to generate the uncertain and Ô¨Åne
part of image. Inpainting results of our full model in (g) show that two temperatures help the model capture more long-range contextual information for high
quality image inpainting.
(a) Ground Truth
 (b) Input
 (c) PatchMatch [2]
 (d) ParConv [14]
 (e) DeepFillv2 [19]
 (f) WaveFill [48]
 (g) ATMA [7]
 (h) Ours
Fig. 9. Qualitatively comparison results on CelebA-HQ dataset. Our model generates more visually pleasant results with Ô¨Åner details, such as details in eyes
and teeth. Zoom in for better visualization.
PatchMatch [2], DeepFillv2 [19], WaveFill [48] and ATMA
[7]. As shown in Fig. 10, PatchMatch works well on this
dataset with repetitive structures. DeepFillv2 and WaveFill
generate reasonable completion results but still contain dis-
torted structures and texture artifacts. ATMA uses more similar
features in attention-based Ô¨Åner inpainting stage and improves
inpainting performance with fewer texture artifacts, but it has
noticeable color inconsistency near edge regions when the
mask regions contain colorful contents. Since our model uses
improved network architecture and attention module, it obtains
more visually pleasing results with more semantic structuresTABLE III
QUANTITATIVE COMPARISON ON VALIDATION IMAGES OF CELEB A-HQ
WITH RANDOM GENERATED FREE -FORM MASKS .
MAE# PSNR(dB)" SSIM" FID#
ParConv [14] 1.554% 27.70 0.9000 7.606
DeepFillv2 [19] 1.680% 27.24 0.8973 7.864
WaveFill [48] 1.541% 27.82 0.9059 6.968
ATMA [7] 1.297% 28.71 0.9161 5.692
ours 1.282% 28.82 0.9167 5.147
and better texture details than the other inpainting approaches.11
(a) Ground Truth
 (b) Input
 (c) PatchMatch [2]
 (d) ParConv [14]
 (e) DeepFillv2 [19]
 (f) WaveFill [48]
 (g) ATMA [7]
 (h) Ours
Fig. 10. Qualitatively comparison results on Places2 dataset. Our model generates more reasonable object and scene structures with better color consistency
and fewer artifacts. Zoom in for better visualization.
TABLE IV
QUANTITATIVE COMPARISON ON VALIDATION IMAGES OF PLACES 2WITH
MASKS UNDER DIFFERENT MASK RATIOS .
Ratio ParConv DeepFillv2 WaveFill ATMA oursMAE#(0.0, 0.1] 0.7780% 0.7986% 0.8616% 0.6096% 0.5820%
(0.1, 0.2] 1.474% 1.557% 1.472% 1.277% 1.217%
(0.2, 0.3] 2.381% 2.500% 2.295% 2.118% 2.007%
(0.3, 0.4] 3.276% 3.395% 3.090% 2.917% 2.763%
(0.4, 0.5] 4.044% 4.185% 3.806% 3.612% 3.415%PSNR(dB)" (0.0, 0.1] 29.82 29.81 29.21 30.76 31.16
(0.1, 0.2] 26.23 25.72 26.47 26.83 27.14
(0.2, 0.3] 23.73 23.19 24.23 24.30 24.66
(0.3, 0.4] 22.19 21.53 22.77 22.66 23.03
(0.4, 0.5] 21.18 20.53 21.88 21.63 22.01SSIM"(0.0, 0.1] 0.9535 0.9591 0.9532 0.9633 0.9647
(0.1, 0.2] 0.9101 0.9179 0.9158 0.9248 0.9282
(0.2, 0.3] 0.8563 0.8682 0.8688 0.8783 0.8838
(0.3, 0.4] 0.8023 0.8204 0.8222 0.8327 0.8408
(0.4, 0.5] 0.7520 0.7740 0.7766 0.7892 0.7999FID#(0.0, 0.1] 19.70 14.65 18.25 14.14 13.47
(0.1, 0.2] 31.13 27.56 26.51 26.12 24.57
(0.2, 0.3] 49.17 42.49 41.60 41.06 39.41
(0.3, 0.4] 64.27 56.35 54.48 54.66 52.65
(0.4, 0.5] 80.68 71.17 69.25 68.68 67.25
Quantitative comparison For quantitative analysis, we
compare our model with current state-of-the-art methods on
CelebA-HQ and Places2 datasets. We test the models on masks
with random size and masks with certain mask ratios, and
evaluated the inpainting results in terms of mean absolute error
(MAE), peak signal-to-noise ratio (PSNR), structural similar-
ity index measure (SSIM) and Fr√©chet inception distance (FID)
[50]. We average the performance over 2000 and 36500 images
from validation sets of CelebA-HQ and Places2, respectively.
The results on CelebA-HQ with randomly generated masks are
reported in Table III. It shows that all learning-based models
perform better than PatchMatch, and the superiority of the
proposed model in the quantitative comparison. In addition,the results in Table IV show that our model achieves better
quantitative performance on Places2 under different mask
ratios.
F . User-Guided Inpainting
To extend the proposed network for user-guided image
inpainting, which provides interactive scenes to allow users to
control the inpainting results, we adopt sketch generation and
use the sketch-based binary mask as an additional input during
training. Our sketch mask generation method consists of three
steps: 1) generating gray edge maps; 2) converting the gray
maps to binary images; 3) improving the binary images with
clearer lines, e.g., thinner with less adhesion. We directly use
the edge detector PiDiNet in [51] to generate a gray edge map,
and produce a binary image by setting all values in the gray
map above a constant threshold (e.g., 0.65 for an image from
Paris StreetView dataset) to ones and the others to zeros. Next,
we perform area opening [52] on the binary image to remove
small objects and generate a new binary image, i.e., removing
all connected components/objects that have fewer than 100
pixels in 8-connectivity. Finally, we perform morphological
skeletonization [53] to further extract skeleton of Ô¨Ågures in the
binary image. Sketch examples generated from the different
steps are shown in Fig. 11.
To train our user-guided inpainting model, we generate a
sketch-based binary mask by multiplying the sketch mask with
the binary mask M, and use it as an additional channel input.
The network architecture of the user-guided inpainting model
is the same as those of the proposed model, but trained with
5-channel inputs, including R, G, B channels, binary mask
channel and sketch-based binary mask channel. During testing,
users draw lines as sketch or edge to guide the inpainting12
(a) Input
 (b) Step 1: Edge extraction
 (c) Step 2: Binarization
 (d) Step 3: Skeletonization
Fig. 11. Sketch examples generated from the different steps of our sketch mask generation method. Our method uses binalization and skeletonization in step
2 and 3 respectively to reÔ¨Åne the edge maps extracted using PiDiNet [51], and generates sketches with thiner lines and more clear object structures.
(a) Input w/o
guide
(b) Result w/o
guide
(c) Input w/ guide
 (d) Result w/
guide
Fig. 12. Results of our user-guided inpainting. Our model can generate more
desired inpainting results according to the user-provided sketches.
model to generate the desired inpainting results. The examples
of user-guided inpainting results on Paris StreetView in Fig.
12 show that our user-guided inpainting model can generate
visually appealing contents according to the user guidance.
V. C ONCLUSION
In this work, we proposed an enhanced free-form image
inpainting framework by identifying and addressing three
issues in our previous work [7], i.e., temperature training
stability, computational efÔ¨Åciency of the attention module,
and visual artifacts. For stabilizing temperature training, we
designed a temperature learning network with Softplus activa-
tion function. The new temperature learning network avoids
to generate negative temperatures that results in unstable
training, and it is able to learn multiple adaptive temperatures
that help the inpainting framework capture more long-range
contextual information and generate high quality inpainting
results. To improve computational efÔ¨Åciency of the attention
module with larger patch size and parallel computing scheme,
a masked self-attention mechanism that formulates features
as queries, keys and values and does dot-product attention
was introduced. In addition, we further improved realism andappearance consistency of the generated results by modifying
the network architecture, e.g., removing batch normalization,
and using Hinge Adversarial Loss with discriminator Spectral
Normalization (HAL-SN).
The proposed image inpainting framework was analyzed
and evaluated on three benchmark image inpainting datasets,
including Paris StreetView, CelebA-HQ and Places2. Experi-
mental results demonstrated the effectiveness of the proposed
framework on temperature learning and image inpainting.
Furthermore, experiments on quantitative and qualitative com-
parisons between the proposed method and the state-of-the-art
free-form image inpainting methods in referenced literature
demonstrated the superiority of the proposed method. More-
over, we presented a sketch generation method and extended
our inpainting framework for user-guided image inpainting.
Our experimental results on Paris StreetView showed that the
proposed framework with user sketches as an additional input
allows users to have Ô¨Åne control of the Ô¨Ånal results.
The proposed framework can generate high quality inpaint-
ing results and can be used for user-guided image inpaint-
ing, which has an important value for image inpainting and
interactive image editing. However, similar to conventional
nearest neighbor based patch matching methods, doing pixel-
wise nearest neighbor based patch-matching in deep image
generative models for high quality image generation is not
memory efÔ¨Åcient. Feature work will be interesting to prune
the network architecture to improve memory efÔ¨Åciency of the
patch-based attention algorithm. Furthermore, considering the
practical deployment of our inpainting model, we feel that it
will be important to Ô¨Ånd new ways to reduce the training data
requirements in our future studies.
REFERENCES
[1] C. Ballester, M. Bertalmio, V . Caselles, G. Sapiro, and J. Verdera,
‚ÄúFilling-in by joint interpolation of vector Ô¨Åelds and gray levels,‚Äù IEEE
Transactions on Image Processing , vol. 10, no. 8, pp. 1200‚Äì1211, 2001.
[2] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman, ‚ÄúPatch-
match: A randomized correspondence algorithm for structural image
editing,‚Äù ACM Transactions on Graphics , vol. 28, no. 3, 2009.
[3] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, ‚ÄúGenerative adversarial nets,‚Äù in
Advances in Neural Information Processing Systems , vol. 27, 2014.
[4] D. P. Kingma and M. Welling, ‚ÄúAn introduction to variational autoen-
coders,‚Äù Foundations and Trends in Machine Learning , vol. 12, no. 4,
pp. 307‚Äì392, 2019.13
[5] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, ‚ÄúGenerative
image inpainting with contextual attention,‚Äù in IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 5505‚Äì5514, 2018.
[6] C. Zheng, T.-J. Cham, J. Cai, and D. Phung, ‚ÄúBridging global context in-
teractions for high-Ô¨Ådelity image completion,‚Äù in IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2021.
[7] X. Zhou, Y . Zeng, and Y . Gong, ‚ÄúImage completion with adaptive
multi-temperature mask-guided attention,‚Äù in British Machine Vision
Conference , 2021.
[8] A. Efros and T. Leung, ‚ÄúTexture synthesis by non-parametric sampling,‚Äù
inIEEE International Conference on Computer Vision , vol. 2, pp. 1033‚Äì
1038 vol.2, 1999.
[9] A. Criminisi, P. Perez, and K. Toyama, ‚ÄúRegion Ô¨Ålling and object
removal by exemplar-based image inpainting,‚Äù IEEE Transactions on
Image Processing , vol. 13, no. 9, pp. 1200‚Äì1212, 2004.
[10] S. Korman and S. Avidan, ‚ÄúCoherency sensitive hashing,‚Äù in IEEE
International Conference on Computer Vision , pp. 1607‚Äì1614, 2011.
[11] D. Pathak, P. Kr√§henb√ºhl, J. Donahue, T. Darrell, and A. A. Efros, ‚ÄúCon-
text encoders: Feature learning by inpainting,‚Äù in IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 2536‚Äì2544, 2016.
[12] S. Iizuka, E. Simo-Serra, and H. Ishikawa, ‚ÄúGlobally and locally
consistent image completion,‚Äù ACM Transactions on Graphics , vol. 36,
no. 4, 2017.
[13] C. Zheng, T.-J. Cham, and J. Cai, ‚ÄúPluralistic image completion,‚Äù in
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 1438‚Äì1447, 2019.
[14] G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catanzaro,
‚ÄúImage inpainting for irregular holes using partial convolutions,‚Äù in
European Conference on Computer Vision , pp. 89‚Äì105, 2018.
[15] R. Suvorov, E. Logacheva, A. Mashikhin, A. Remizova, A. Ashukha,
A. Silvestrov, N. Kong, H. Goka, K. Park, and V . Lempitsky,
‚ÄúResolution-robust large mask inpainting with fourier convolutions,‚Äù
inIEEE/CVF Winter Conference on Applications of Computer Vision ,
pp. 3172‚Äì3182, 2022.
[16] N. Wang, Y . Zhang, and L. Zhang, ‚ÄúDynamic selection network for
image inpainting,‚Äù IEEE Transactions on Image Processing , vol. 30,
pp. 1784‚Äì1798, 2021.
[17] C. Yang, X. Lu, Z. Lin, E. Shechtman, O. Wang, and H. Li, ‚ÄúHigh-
resolution image inpainting using multi-scale neural patch synthesis,‚Äù
inIEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 4076‚Äì4084, 2017.
[18] K. Nazeri, E. Ng, T. Joseph, F. Qureshi, and M. Ebrahimi, ‚ÄúEdgeconnect:
Structure guided image inpainting using edge prediction,‚Äù in IEEE/CVF
International Conference on Computer Vision Workshop , pp. 3265‚Äì3274,
2019.
[19] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, ‚ÄúFree-form
image inpainting with gated convolution,‚Äù in IEEE/CVF International
Conference on Computer Vision , 2019.
[20] Y . Zeng, Y . Gong, and J. Zhang, ‚ÄúFeature learning and patch matching
for diverse image inpainting,‚Äù Pattern Recognition , vol. 119, p. 108036,
2021.
[21] Z. Wan, J. Zhang, D. Chen, and J. Liao, ‚ÄúHigh-Ô¨Ådelity pluralistic image
completion with transformers,‚Äù in IEEE/CVF International Conference
on Computer Vision , pp. 4672‚Äì4681, 2021.
[22] R. Xu, M. Guo, J. Wang, X. Li, B. Zhou, and C. C. Loy, ‚ÄúTexture
memory-augmented deep patch-based image inpainting,‚Äù IEEE Trans-
actions on Image Processing , vol. 30, pp. 9112‚Äì9124, 2021.
[23] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, ‚ÄúSelf-attention
generative adversarial networks,‚Äù in International Conference on Ma-
chine Learning , vol. 97, pp. 7354‚Äì7363, 2019.
[24] H. Liu, B. Jiang, Y . Xiao, and C. Yang, ‚ÄúCoherent semantic attention for
image inpainting,‚Äù in IEEE/CVF International Conference on Computer
Vision , pp. 4169‚Äì4178, 2019.
[25] T. Pl√∂tz and S. Roth, ‚ÄúNeural nearest neighbors networks,‚Äù in Advances
in Neural Information Processing Systems , vol. 31, 2018.
[26] A. Agarwala, J. Pennington, Y . Dauphin, and S. Schoenholz, ‚ÄúTempera-
ture check: theory and practice for training models with softmax-cross-
entropy losses,‚Äù arXiv:2010.07344 , 2020.
[27] M. Hein, M. Andriushchenko, and J. Bitterwolf, ‚ÄúWhy ReLU networks
yield high-conÔ¨Ådence predictions far away from the training data and
how to mitigate the problem,‚Äù in IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 41‚Äì50, 2019.
[28] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a
neural network,‚Äù in NIPS Deep Learning and Representation Learning
Workshop , 2015.[29] J. Rajasegaran, S. Khan, M. Hayat, F. S. Khan, and M. Shah, ‚ÄúSelf-
supervised knowledge distillation for few-shot learning,‚Äù in British
Machine Vision Conference , 2021.
[30] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚ÄúA simple framework
for contrastive learning of visual representations,‚Äù in International
Conference on Machine Learning , vol. 119, pp. 1597‚Äì1607, 2020.
[31] O. Zhang, M. Wu, J. Bayrooti, and N. Goodman, ‚ÄúTemperature as un-
certainty in contrastive learning,‚Äù in NeurIPS Self-Supervised Learning
Theory and Practice Workshop , 2021.
[32] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,
‚ÄúLearning transferable visual models from natural language supervi-
sion,‚Äù in International Conference on Machine Learning , vol. 139,
pp. 8748‚Äì8763, 2021.
[33] J. Lin, X. Sun, X. Ren, M. Li, and Q. Su, ‚ÄúLearning when to
concentrate or divert attention: Self-adaptive attention temperature for
neural machine translation,‚Äù in Conference on Empirical Methods in
Natural Language Processing , pp. 2985‚Äì2990, 2018.
[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. u. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Advances
in Neural Information Processing Systems , vol. 30, 2017.
[35] M. Caccia, L. Caccia, W. Fedus, H. Larochelle, J. Pineau, and L. Charlin,
‚ÄúLanguage GANs falling short,‚Äù in International Conference on Learn-
ing Representations , 2020.
[36] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, ‚ÄúOptimization by
simulated annealing,‚Äù SCIENCE , vol. 220, no. 4598, pp. 671‚Äì680, 1983.
[37] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, ‚ÄúA learning algorithm
for boltzmann machines,‚Äù Cognitive science , vol. 9, no. 1, pp. 147‚Äì169,
1985.
[38] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .
MIT press, 2018.
[39] J. Johnson, A. Alahi, and L. Fei-Fei, ‚ÄúPerceptual losses for real-
time style transfer and super-resolution,‚Äù in European Conference on
Computer Vision , 2016.
[40] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:
A large-scale hierarchical image database,‚Äù in IEEE Conference on
Computer Vision and Pattern Recognition , pp. 248‚Äì255, 2009.
[41] T. Miyato, T. Kataoka, M. Koyama, and Y . Yoshida, ‚ÄúSpectral normal-
ization for generative adversarial networks,‚Äù in International Conference
on Learning Representations , 2018.
[42] J. H. Lim and J. C. Ye, ‚ÄúGeometric GAN,‚Äù arXiv:1705.02894 , 2017.
[43] K. Chellapilla, S. Puri, and P. Simard, ‚ÄúHigh Performance Convolutional
Neural Networks for Document Processing,‚Äù in International Workshop
on Frontiers in Handwriting Recognition , Universit√© de Rennes 1,
Suvisoft, 2006.
[44] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros, ‚ÄúWhat makes
paris look like paris?,‚Äù ACM Transactions on Graphics , vol. 31, no. 4,
pp. 1‚Äì9, 2012.
[45] T. Karras, T. Aila, S. Laine, and J. Lehtinen, ‚ÄúProgressive growing of
GANs for improved quality, stability, and variation,‚Äù in International
Conference on Learning Representations , 2018.
[46] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, ‚ÄúPlaces:
A 10 million image database for scene recognition,‚Äù IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2017.
[47] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
inInternational Conference on Learning Representations , 2015.
[48] Y . Yu, F. Zhan, S. Lu, J. Pan, F. Ma, X. Xie, and C. Miao, ‚ÄúWaveÔ¨Åll: A
wavelet-based generation network for image inpainting,‚Äù in IEEE/CVF
International Conference on Computer Vision , 2021.
[49] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and A. C. Courville,
‚ÄúImproved training of wasserstein GANs,‚Äù in Advances in Neural
Information Processing Systems , vol. 30, 2017.
[50] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
‚ÄúGANs trained by a two time-scale update rule converge to a local nash
equilibrium,‚Äù in Advances in Neural Information Processing Systems ,
vol. 30, 2017.
[51] Z. Su, W. Liu, Z. Yu, D. Hu, Q. Liao, Q. Tian, M. Pietik√§inen, and L. Liu,
‚ÄúPixel difference networks for efÔ¨Åcient edge detection,‚Äù in IEEE/CVF
International Conference on Computer Vision , pp. 5117‚Äì5127, 2021.
[52] L. Vincent, ‚ÄúMorphological area openings and closings for grey-scale
images,‚Äù in Shape in Picture , pp. 197‚Äì208, 1994.
[53] P. Maragos and R. Schafer, ‚ÄúMorphological skeleton representation and
coding of binary images,‚Äù IEEE Transactions on Acoustics, Speech, and
Signal Processing , vol. 34, no. 5, pp. 1228‚Äì1244, 1986.