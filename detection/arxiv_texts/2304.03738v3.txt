arXiv:2304.03738v3  [cs.CY]  13 Nov 2023Should ChatGPT be Biased?
Challenges and Risks of Bias in Large Language Models
Emilio Ferraraa,b
aThomas Lord Department of Computer Science, University of S outhern California, Los Angeles, 90007, CA, USA
bUSC Information Sciences Institute, 4676 Admiralty Way #10 01, Marina del Rey, 90292, CA, USA
E-mail: emiliofe@usc.edu
Abstract
As generative language models, exempliﬁed by ChatGPT, cont inue to advance in their capa-
bilities, the spotlight on biases inherent in these models i ntensiﬁes. This article delves into the
distinctive challenges and risks associated with biases sp eciﬁcally in large-scale language models.
We explore the origins of biases, stemming from factors such as training data, model speciﬁcations,
algorithmic constraints, product design, and policy decis ions. Our examination extends to the
ethical implications arising from the unintended conseque nces of biased model outputs. In addi-
tion, we analyze the intricacies of mitigating biases, ackn owledging the inevitable persistence of
some biases, and consider the consequences of deploying the se models across diverse applications,
including virtual assistants, content generation, and cha tbots. Finally, we provide an overview of
current approaches for identifying, quantifying, and miti gating biases in language models, under-
scoring theneed for a collaborative, multidisciplinary eﬀo rt to craft AI systems that embody equity,
transparency, and responsibility. This article aims to cat alyze a thoughtful discourse within the
AI community, prompting researchers and developers to cons ider the unique role of biases in the
domain of generative language models and the ongoing quest f or ethical AI.
Keywords: Artiﬁcial Intelligence, Generative AI, Bias, Large Langua ge Models, ChatGPT,
GPT-4
1. Introduction
In recent years, there has been a remarkable surge in the real m of artiﬁcial intelligence (AI),
marked by the emergence of transformative technologies lik e ChatGPT and other generative lan-
guage models ([1, 2, 3, 4, 5, 6, 7, 8, 9]). These AI systems repr esent a class of sophisticated models
designed to excel in generating human-like text and compreh ending natural language. Using deep
learning techniques and vast datasets, they have the abilit y to discern intricate patterns, make
contextual inferences, and generate coherent and contextu ally relevant responses to a diverse range
of inputs ([10, 11, 12, 1, 2]). By mirroring human language ca pabilities, these models have unveiled
a multitude of applications, ranging from chatbots and virt ual assistants to translation services
and content generation tools ([13]).
Language models have ushered in a transformative era, under pinning the development of chat-
bots that emulate human interaction in conversations ([14] ). These chatbots have become vital
tools, simplifying customer service, technical support, a nd information queries with human-like
interaction. Furthermore, the integration of language mod els into virtual assistants has endowed
Preprint submitted to First Monday November 14, 2023Contributing Factor Description References
Training Data Biases in the source material or the selection process
for training data can be absorbed by the model and
reﬂected in its behavior.[28, 29, 30, 31]
Algorithms Biases can be introduced or ampliﬁed through algo-
rithms that place more importance on certain fea-
tures or data points.[32, 30, 33]
Labeling and Annotation In (semi)supervised learning scenarios, biases may
emerge from subjective judgments of human annota-
tors providing labels or annotations for the training
data.[34, 35, 36]
Product Design Decisions Biases can arise from prioritizing certain use cases or
designing user interfaces for speciﬁc demographics or
industries, inadvertently reinforcing existing biases
and excluding diﬀerent perspectives.[37, 38]
Policy Decisions Developers might implement policies that prevent (or
encourage) a given model behavior. For example,
guardrails that modulate the behavior of ChatGPT
and Bing-AI were designed to mitigate unintended
toxic model behaviors or prevent malicious abuse.[39, 40, 41, 42]
Table 1: Factors Contributing to Bias in AI Models
them with the ability to provide precise and contextually ap propriate responses to user inquiries
([15]). Virtual assistants, thus enhanced, become indispe nsable aides, capable of managing tasks
ranging from appointment scheduling to web searches and sma rt home device control ([16]).
Inthesphereoftranslation, harnessingtheprowessoflarg elanguagemodelsfacilitates markedly
improved and ﬂuent translations that span multiple languag es, including those with limited re-
sources ([17, 18, 19, 20, 21]). Such capabilities not only fo ster enhanced cross-linguistic commu-
nication but can also enable timely solutions during emerge ncies and crises, especially in regions
where low-resource languages or indigenous dialects are sp oken ([22]).
Moreover, the aptitude of language models to generate coher ent and contextually pertinent text
hasrenderedtheminvaluableintherealmofcontent creatio n. Acknowledgedfortheirproﬁciencyin
producing various types of content, spanning articles, soc ial media posts, and marketing materials,
they have established a profound impact ([23, 24]).
These applications, among many others, underscore the tran sformative prowess of generative
language models across an array of industries and sectors. H owever, as their adoption proliferates
and their inﬂuence extends into ever-diverse domains ([25, 26]), it is imperative to confront the
distinctive challenges posed by the potential biases that m ay be established within these models.
These biases can have profound implications for users and so ciety at large, highlighting the urgent
need for comprehensive examination and mitigation of these issues [27].
2. Deﬁning bias in generative language models
2.1. Factors contributing to bias in Large Language Models
Bias, in the context of large language models such as GPT-4 ([ 9, 43, 44]) and predecessors
([3, 4, 5]), or other state-of-the-art alternatives ([45, 4 6]; including multimodal variants, [47]),
can be deﬁned as the presence of systematic misrepresentati ons, attribution errors, or factual
distortions that result in favoring certain groups or ideas , perpetuating stereotypes, or making
2incorrect assumptions based on learned patterns. Biases in such models can arise due to several
factors (cf., Table 1).
One factor is the training data . If the data used to train a language model contain biases, ei ther
from the source material or through the selection process, t hese biases can be absorbed by the
model and subsequently reﬂected in its behavior ([28, 29, 30 , 31]). Biases can also be introduced
through the algorithms used to process and learn from the data. For example, if an alg orithm
places more importance on certain features or data points, i t may unintentionally introduce or
amplify biases present in the data ([32, 30, 33]). In (semi)s upervised learning scenarios, where
human annotators provide labels or annotations for the training data, biases may emerge from the
subjective judgments of the annotators themselves, inﬂuen cing the model’s understanding of the
data ([34, 35, 36]).
The choice of which use cases to prioritize or the design of user interfaces can also contr ibute to
biases in large language models. For example, if a language m odel is primarily designed to generate
content for a certain demographic or industry, it may inadve rtently reinforce existing biases and
excludediﬀerent perspectives ([37, 38]). Lastly, policy decisions can play arole inthe manifestation
of biases in language models. The developers of both commerc ial and openly available language
models might implement policies that prevent (or encourage ) a given model behavior. For example,
bothOpenAIandMicrosofthavedeliberateguardrailsthatm odulatethebehaviorofChatGPTand
Bing-AI to mitigate unintended toxic model behaviors or pre vent malicious abuse ([39, 40, 41, 42]).
2.2. Types of biases in Large Language Models
Large language models, which are commonly trained from vast amounts of text data present
on the Internet, inevitably absorb the biases present in suc h data sources. These biases can take
various forms ( cf., Table 2).
Demographic biases arise when the training data over-repre sents or under-represents certain
demographic groups, leading the model to exhibit biased beh avior towards speciﬁc genders, races,
ethnicities, or other social groups ([34, 28, 29, 35, 31, 48] ). Cultural biases occur when large
language models learn and perpetuate cultural stereotypes or biases, as they are often present
in the data used for training. This can result in the model pro ducing outputs that reinforce or
exacerbate existing cultural prejudices ([49, 50, 30]). Li nguistic biases emerge since the majority
of the internet’s content is in English or a few other dominan t languages, making large language
models more proﬁcient in these languages. This can lead to bi ased performance and a lack of
support for low-resource languages or minority dialects ([ 51, 52, 53, 54, 31]). Temporal biases
appear as the training data for these models are typically re stricted to limited time periods or have
temporal cutoﬀs. This may cause the model to be biased when rep orting on current events, trends,
and opinions. Similarly, the model’s understanding of hist orical contexts or outdated information
may belimited dueto a lack of temporally representative dat a ([3, 55, 56, 57]). Conﬁrmation biases
in the training data may result from individuals seeking out information that aligns with their pre-
existing beliefs. Consequently, large language models may inadvertently reinforce these biases by
providing outputs that conﬁrm or support speciﬁc viewpoint s ([28, 29, 2, 58]). Lastly, ideological
and political biases can be learned and propagated by large l anguage models due to the presence
of such biases in their training data. This can lead to the mod el generating outputs that favor
certain political perspectives or ideologies, thereby amp lifying existing biases ([59, 60, 56, 61]).
This paper aims to explore the question of whether language m odels like GPT-4 ([9, 43, 44]), its
prior versions ([3, 4, 5]), or other commercial or open-sour ce alternatives ([45, 46, 62]) that power
applications like ChatGPT ([8]) (or similar) should be bias ed or unbiased, taking into account
3Types of Bias Description References
Demographic Biases These biases arise when the training data over-
represents or under-represents certain demo-
graphic groups, leading the model to exhibit bi-
ased behavior towards speciﬁc genders, races, eth-
nicities, or other social groups.[34, 28, 29, 35, 31, 48]
Cultural Biases Large language models may learn and perpetuate
cultural stereotypes or biases, as they are often
present in the data used for training. This can
result in the model producing outputs that rein-
force or exacerbate existing cultural prejudices.[49, 50, 30]
Linguistic Biases Since the majority of the internet’s content is in
English or a few other dominant languages, large
language models tend to be more proﬁcient in
these languages. This can lead to biased perfor-
mance and a lack of support for low-resource lan-
guages or minority dialects.[51, 52, 53, 54, 31]
Temporal Biases The training data for these models are typically
restricted to limited time periods, or have tem-
poral cutoﬀs, which may cause the model to be
biased when reporting on current events, trends,
and opinions. Similarly, the model’s understand-
ing of historical contexts or outdated information
may be limited for lack of temporally representa-
tive data.[3, 55, 56, 57]
Conﬁrmation Biases The training data may contain biases that re-
sult from individuals seeking out information that
aligns with their pre-existing beliefs. Conse-
quently, large language models may inadvertently
reinforce these biases by providing outputs that
conﬁrm or support speciﬁc viewpoints.[28, 29, 2, 58]
Ideological & Political Biases Large language models can also learn and prop-
agate the political and ideological biases present
in their training data. This can lead to the model
generating outputs that favor certain political per-
spectives or ideologies, thereby amplifying exist-
ing biases.[59, 60, 56, 61]
Table 2: Types of Biases in Large Language Models
the implications and risks of both perspectives. By examini ng the ethical, practical, and societal
consequences of each viewpoint, we hope to contribute to the ongoing discussion surrounding
responsible language model development and use. Through th is exploration, our goal is to provide
insightsthatcan helpguidethefutureevolution ofGPT-sty le andothergenerative languagemodels
toward more ethical, fair, and beneﬁcial outcomes while min imizing potential harm.
3. Why are generative language models prone to bias?
3.1. Biases from the data
ChatGPT and other applications based on large language mode ls are trained using a process
that primarily relies on unsupervised learning, a machine l earning technique that enables models
to learn patterns and structures from vast amounts of unlabe lled data ([63, 64]). In most cases
4with these language models, the data consists of extensive t ext corpora available on the internet,
which includes websites, articles, books, and other forms o f written content ([2, 45, 46, 62]).
ChatGPT in particular is trained on a diverse range of intern et text datasets that encompass
various domains, genres, and languages. While the speciﬁcs of the dataset used for GPT-4 are
proprietary ([9, 43]), the data sources utilized for traini ng its predecessor, GPT-3, likely share
similarities. For GPT-3 and predecessors, the primary data set used was WebText ([3]), which is
an ever-growing large-scale collection of web pages ([4, 5] ). WebText was created by crawling the
internet and gathering text from web pages. The sources of da ta include, but are not limited to:
•Websites : Text is extracted from a wide array of websites, covering to pics such as news,
blogs, forums, and informational websites like Wikipedia. This enables the model to learn
from diverse sources and gain knowledge on various subjects .
•Books: Text from books available online, including both ﬁction an d non-ﬁction, contributes
to the training data. This helps the model to learn diﬀerent wr iting styles, narrative struc-
tures, and a wealth of knowledge from various ﬁelds.
•Social media platforms : Content from social media platforms, like Twitter, Facebo ok, and
Reddit, is incorporated to expose the model to colloquial la nguage, slang, and contemporary
topics of discussion.
•Conversational data : To improve the model’s conversational abilities, text fro m chat logs,
comment sections, and other conversational sources are als o included in the training dataset.
The developers of ChatGPT note that the WebText data is prepr ocessed and ﬁltered to remove
low-quality content, explicit material, web and social spa m [65, 66], and other undesirable text
before being fed into the model ([4, 5]). However, due to the v ast scale of the data and the
limitations of current ﬁltering techniques, someundesira bleor biased content may still seep into the
training dataset, aﬀecting the behavior of the resulting mod el. In addition to WebText, GPT-3 was
furthertrained usinga ﬁltered version of the CommonCrawl d ataset (https://commoncrawl.org ),
a publicly available, massive web-crawled dataset that con tains raw web page data, extracted
metadata, and text content from billions of web pages in mult iple languages ([5]).
Another commonly-used dataset for language model training isThe Pile ([67, 68]) an extensive
and diverse collection of 22 smaller datasets, combining va rious sources of scientiﬁc articles, books,
and web content. It is designed for training large-scale lan guage models, particularly in the domain
of scientiﬁc research and understanding.
3.2. Biases from the models
During the training process, generative language models ar e exposed to billions of sentences
and phrases, allowing them to learn the intricate relations hips between words, grammar, context,
and meaning ([63, 64]). As they process the text data, they gr adually acquire natural language
generation capabilities, enabling them to produce coheren t and contextually relevant responses to
various inputs. However, some capabilities of these models can lead to bias ( cf., Table 3):
Generalization. One crucial aspect of these models is their ability to genera lize, which allows them
to apply the knowledge gained from their training data to new and previously unseen inputs,
providing contextually relevant responses and prediction s even in unfamiliar situations. However,
5this ability also raises concerns about potential biases, a s models may inadvertently learn and
perpetuate biases present in their training data, even if th e data has been ﬁltered and cleaned to
the extent possible ([69, 29]).
Propagation. As these models learn from the patterns and structures prese nt in their training data,
they may inadvertently absorb and propagate biases they enc ounter, such as adopting stereotypes,
favoring certain groups or ideas, or making assumptions bas ed on learned patterns that do not
accurately represent the full spectrum of human experience . This propagation of biases during
training poses signiﬁcant challenges to the development of fair and equitable AI systems, as biased
models can lead to unfair treatment, reinforce stereotypes , and marginalize certain groups ([28, 70,
71]).
Emergence. In large language models, the phenomenon of emergence, whic h refers to the spon-
taneous appearance of unanticipated capabilities despite these functionalities not being explicitly
encoded within the model’s architecture or training data, c an also result in unexpected biases
due to the intricate interplay between model parameters and biased training data ([72, 73]). The
high-dimensional representations and non-linear interac tions in these models make it diﬃcult to
predict or control these emergent biases, which may manifes t in various ways, such as stereotyping,
oﬀensive language, or misinformation. To address this chall enge, researchers are exploring bias
mitigation strategies during training, ﬁne-tuning with cu rated datasets, and post-hoc emergent
bias analyses ([74, 75]).
Non-linearity. The non-linear relationships between biases in the system o r data and their real-
world impact imply that small biases may have massive negati ve eﬀects, and large biases might not
result in signiﬁcant consequences. This disproportionali ty arises due to the complex interdepen-
dencies between the model parameters and the high-dimensio nal representations learned during
training [71]. Randomized controlled trials could be used t o draw causal relationships between
the extent of each bias and their eﬀects. In the absence of that , due to ethical reasons, multi-
faceted approaches involving in-depth analysis of model be havior, rigorous evaluation with diverse
benchmarks, and the application of mitigation techniques t hat account for the nonlinear nature of
emergent biases are needed [76].
Alignment. To address these issues, a strategy known as Reinforcement L earning with Human
Feedback (RLHF) ([7]) was developed to ﬁne-tune large langu age models like ChatGPT to reduce
their biases and align them with human values. This approach involves collecting a dataset of
human demonstrations, comparisons, and preferences to cre ate a reward model that guides the
ﬁne-tuning process [77]. InstructGPT (ChatGPT’s default m odel) ([8]) is trained using RLHF and
then ﬁne-tuned using Proximal Policy Optimization (PPO), a policy optimization algorithm ([6]).
It is paramount to understand if the same principles could be exploited to deliberately misalign a
model.
3.3. Can bias be mitigated with human-in-the-loop approache s?
Bias in generative language models can be mitigated to some e xtent with human-in-the-loop
(HITL) approaches. These approaches involve incorporatin g human input, feedback, or oversight
throughout the development and deployment of the language m odel, which can help address is-
sues related to biases and other limitations. Here are some w ays to integrate human-in-the-loop
approaches to mitigate bias:
6Source Description References
Generalization Models generalize knowledge from training data to new input s,
potentially leading to biased behavior if the data contains biases.
This raises concerns about perpetuating biases, even if tra ining
data has been cleaned and ﬁltered.[69, 29]
Propagation Models may absorb and propagate biases in training data, ado pt-
ing stereotypes and favoring certain groups or ideas, or mak ing
assumptions on on non-representative learned patterns.[28, 70, 71]
Emergence Unanticipated capabilities and biases may emerge in large l an-
guage models due to complex interactions between model pa-
rameters and biased training data. It has been proven diﬃcul t
to predict or control these emergent biases.[72, 73, 74, 75]
Non-linearity Biases in AI systems may have non-linear real-world impact,
making it diﬃcult to predict their consequences: small mode l
biases may have massive negative eﬀects, whereas large mode l
biases might not cause signiﬁcant consequences.[76, 71]
Alignment Reinforcement Learning with Human Feedback (RLHF) ﬁne-
tunes large language models to reduce biases and align them
with human values. The same principles might be abused to
lead to unfair model behaviors.[7, 8, 6]
Table 3: Sources of model bias in large language models and th eir descriptions.
•Training data curation : Humans can be involved in curating and annotating high-qua lity
and diverse training data. This may include identifying and correcting biases, ensuring
a balance of perspectives, and reducing the inﬂuence of cont roversial or oﬀensive content
([78, 31, 33]).
•Model ﬁne-tuning : Subject matter experts can guide the model ﬁne-tuning proc ess by
providing feedback on the model’s outputs, helping the mode l generalize better and avoid
biased or incorrect responses ([79]).
•Evaluation and feedback : Human reviewers can evaluate the model’s performance and
provide feedback to developers, who can then iteratively im prove the model. This feedback
loop is essential for identifying and addressing bias-rela ted issues. ([58]).
•Real-time moderation : Human moderators can monitor and review the model’s output s
in real-time, intervening when necessary to correct biased or inappropriate responses. This
approach can be especially useful in high-stakes or sensiti ve applications ([80]).
•Customization and control : Users can be provided with options to customize the model’s
behavior, adjusting the output according to their preferen ces or requirements. This approach
can help users mitigate bias in the model’s responses by tail oring it to speciﬁc contexts or
domains ([3, 81]).
While human-in-the-loop approaches can help mitigate bias , it is essential to recognize that
they may not be able to eliminate it entirely. Bias can stem fr om various sources, such as the
training data, ﬁne-tuning process, or even the human review ers themselves. However, combining
machine learning techniques with human expertise can be a pr omising way to address some of the
challenges posed by biases in generative language models.
7Challenge Description References
Inherent biases in
languageHuman language is a reﬂection of society, containing var-
ious biases, stereotypes, and assumptions. Separating
useful patterns from these biases can be challenging as
they are deeply ingrained in language structures and ex-
pressions.[82, 83, 84, 85, 86, 87]
Ambiguity of cul-
tural normsCultural norms and values vary signiﬁcantly across com-
munities and regions. Determining which norms to en-
code in AI models is a complex task that requires a nu-
anced understanding of diverse cultural perspectives.[88, 89, 90, 91]
Subjectivity of fair-
nessFairness is a subjective concept with various interpreta-
tions. Eliminating bias from AI models requires deﬁning
“fair” in the context of applications, which is challenging
due to the diverse range of stakeholders and perspectives.[92, 93, 94]
Continuously evolv-
ing language and cul-
tureLanguage and culture constantly evolve, with new expres-
sions, norms, and biases emerging over time. Keeping
AI models up-to-date with these changes and ensuring
they remain unbiased requires continuous monitoring and
adaptation.[95, 96, 97]
Table 4: Challenges in addressing biases in Large Language M odels
4. The inevitability of some forms of bias
4.1. Are some biases inevitable?
Completely eliminating bias from large language models is a complex and challenging task due
to the inherent nature of language and cultural norms. Since these models learn from vast amounts
of text dataavailable onthe internet, they are exposedto th ebiases present withinhuman language
and culture. Addressing bias in these models involves tackl ing several key challenges ( cf., Table 4).
First, human language is a reﬂection of society and as such, i t contains various biases, stereo-
types, and assumptions. Separating useful patterns from th ese biases can be challenging, as they
are often deeply ingrained in the way people express themsel ves and the structures of language
itself ([82, 83, 84, 85, 86, 87]). Second, cultural norms and values can vary signiﬁcantly across
diﬀerent communities and regions. What is considered accept able or appropriate in one context
may be seen as biased or harmful in another. Determining whic h norms should be encoded in AI
models and which should be ﬁltered out is a complex task that r equires careful consideration and
a nuanced understanding of diverse cultural perspectives ( [88, 89, 90, 91]).
Furthermore, fairness is a subjective concept that can be in terpreted in various ways. Com-
pletely eliminating bias from AI models would require devel opers to deﬁne what “fair” means
in the context of their applications, which can be a challeng ing task, given the diverse range of
stakeholders and perspectives involved ([92, 93, 94]). Las tly, language and culture are constantly
evolving, with new expressions, norms, and biases emerging over time. Keeping AI models up-
to-date with these changes and ensuring that they remain unb iased is an ongoing challenge that
requires continuous monitoring and adaptation ([95, 96, 97 ]).
Despite these challenges, it is essential for developers, r esearchers, and stakeholders to continue
working towards reducing bias in large language models. By d eveloping strategies for identifying
and mitigating biases, collaborating with diverse communi ties, and engaging in ongoing evaluation
and improvement, we can strive to create AI systems that are m ore equitable, fair, and beneﬁcial
for all users.
84.2. Utility despite bias?
Biased AI models can still be useful in certain contexts or ap plications, as long as users are
aware of their limitations and take them into account when ma king decisions. In some cases, the
biases present in these models may even be representative of the real-world context in which they
are being used, providing valuable insights in surfacing so cietal inequalities that need to be tackled
at their root.
The key to leveraging biased AI models responsibly is to ensu re that users have a clear under-
standing of the potential biases and limitations associate d with these models, so they can make
informed decisions about whether and how to use them in diﬀere nt contexts. Some strategies for
addressing this issue include:
•Transparency : Developers should be transparent about the methodologies , data sources,
and potential biases of their AI models, providing users wit h the necessary information to
understand the factors that may inﬂuence the model’s predic tions and decisions. Best prac-
tices about the documentation of models and data have been ad vanced by the AI community
([58, 98]).
•Education and awareness : Providing resources, training, and support to help users b etter
understand the potential biases in AI models and how to accou nt for them when making
decisions. This may involve creating guidelines, best prac tices, or other educational materials
that explain the implications of bias in AI and how to navigat e it responsibly.
•Context-speciﬁc applications : In some limited cases, biased AI models may be viable
for speciﬁc applications or contexts where their biases ali gn with the relevant factors or
considerations. Experts should be employed to carefully ev aluate the appropriateness of
using biased models in these situations, taking into accoun t the potential risks and beneﬁts
associated with their use, and actionable plans to recogniz e, quantify, and mitigate biases.
•Continuous monitoring and evaluation : Regularly assessing the performance of AI mod-
els in real-world contexts, monitoring their impact on user s and aﬀected communities, and
making adjustments as needed to address any biases or uninte nded consequences that emerge
over time ([31]).
By acknowledging that biased models can still be useful in li mited contexts and taking steps
to ensure that users are aware of their limitations and quali ﬁed to recognize and mitigate them,
we can promote the responsible use of AI technologies and har ness their potential beneﬁts while
minimizing the risks associated with bias.
5. The broader risks of generative AI bias
5.1. Pillars of responsible generative AI development
Ethical considerations of fairness and equality play a cruc ial role in the development and de-
ployment of generative AI applications. As these models int egrate increasingly into various aspects
of our lives, their potential impact on individuals and soci ety as a whole becomes a matter of
signiﬁcant concern. The responsibility lies with develope rs, researchers, and stakeholders to ensure
that AI models treat all users and groups equitably, avoidin g the perpetuation of existing biases
or the creation of new ones ( cf., Table 5).
9One key ethical consideration is representation . It is essential to ensure that the training data
used to develop generative AI models are representative of t he diverse range of perspectives, ex-
periences, and backgrounds that exist within society ([99, 35, 94, 58, 100]). This helps to reduce
the risk that biases are absorbed and propagated by models, l eading to more equitable outcomes.
Transparency is another important aspect. Developers should be transpar ent about the method-
ologies, data sources, and potential limitations of their g enerative AI models ([101, 102]). This
enables users to better understand the factors that may inﬂu ence the model’s predictions and de-
cisions.Accountability is also crucial for responsible generative AI development. Developers and
stakeholders must establish a clear framework for accounta bility, which may includemonitoring the
performance of AI models, addressing biases and errors, and responding to users and communities
aﬀected ([103, 104, 105]). An unique aspect of generative AI, compared to traditional machine
learning, is its ability to possibly replace human artistic expression or to plagiarize the style and
uniqueness of human work: as such, preservation of intellec tual property, copyright protection and
prevention of plagiarism are paramount [27]. Inclusivity is another key ethical consideration. Gen-
erative AI applications should be designed to be inclusive a nd accessible to all users, taking into
account factors such as language, culture, and accessibili ty needs ([106, 107]). This ensures that
the beneﬁts of AI are shared equitably across society.
Lastly,continuous improvement is vital to achieve fairness and equality in generative AI ap -
plications. Developers must commit to an ongoing process of evaluating, reﬁning, and improving
their AI models to address biases and ensure fairness over ti me ([108, 109, 58, 110]). This may
involve collaborating with researchers, policymakers, an d aﬀected communities to gain insight and
feedback that can help guide the development of more equitab le AI systems.
By prioritizing ethical considerations of fairness and equ ality, AI developers can create appli-
cations that not only harness the power of advanced technolo gies such as large language models,
but also promote a more just and inclusive society, where the beneﬁts and opportunities of AI are
accessible to all.
5.2. The risks of exacerbating existing societal biases
Bias in widely-adopted AI models, including ChatGPT and oth er generative language models,
can have far-reaching consequences that extend beyond the i mmediate context of their applications.
Whenthesemodelsabsorbandpropagatebiases, includingth osepresentintheirtrainingdata, they
may inadvertently reinforce stereotypes, marginalize cer tain groups, and lead to unfair treatment
across various domains. Some examples of how biased AI model s can adversely impact diﬀerent
areas include:
•Hiring: AI-driven hiring tools that use biased models may exhibit u nfair treatment towards
applicants from underrepresented groups or those with non- traditional backgrounds. This
couldleadtotheperpetuationofexistinginequalities int hejobmarket, limitingopportunities
for aﬀected individuals and reducing diversity in the workfo rce ([111, 112]). Large language
models can be used to automate the screening of job applicant s, such as by analyzing resumes
and cover letters. Since these models are trained on vast amo unts of text data, they may
have internalized biases present in the data, such as gender or racial biases. As a result, they
could unintentionally favor certain applicants or disqual ify others based on factors unrelated
to their qualiﬁcations, reinforcing existing inequalitie s in the job market.
•Lending : Financial institutions increasingly rely on AI models for credit scoring and lending
decisions. Biased models may unfairly penalize certain gro ups, such as minority communities
10Pillar Description References
Representation Ensuring that the training data used to develop AI models is
representative of the diverse range of perspectives, exper iences,
and backgrounds that exist within society. This helps to red uce
the risk of biases being absorbed and propagated by the model s,
leading to more equitable outcomes[99, 35, 94, 58, 100]
Transparency Developers should be transparent about the methodologies, data
sources, and potential limitations of their AI models, enab ling
users to better understand the factors that may inﬂuence the
model’s predictions and decisions[101, 102]
Accountability It is essential for developers and stakeholders to establis h a clear
framework for accountability, which may include monitorin g the
performance of AI models, addressing biases and errors, and
responding to the concerns of users and aﬀected communities[103, 104, 105]
Inclusivity AI applications should be designed to be inclusive and acces sible
to all users, taking into account factors such as language, c ulture,
and accessibility needs, to ensure that the beneﬁts of AI are
shared equitably across society[106, 107]
Protection of
IP, human
work, and hu-
man artistic
expressionGenerative AI models have the remarkable capability to crea te
human-like text, artwork, music, and more. This creative as pect
presents unique challenges, including issues related to in tellec-
tual property and the protection of human-generated copyri ght
work to avoid AI plagiarism[27]
Continuous im-
provementDevelopers must commit to an ongoing process of evaluating,
reﬁning, and improving their AI models to address biases and
ensure fairness over time. This may involve working with re-
searchers, policy makers, and aﬀected communities to gain i n-
formation and feedback that can help guide the development o f
more equitable AI systems[108, 109, 58, 110]
Table 5: Pillars of responsible generative AI development
or individuals with lower socio-economic status, by assign ing them lower credit scores or
denying them access to loans and ﬁnancial services based on b iased assumptions ([113, 114,
115]). In lending, large language models can be used to asses s creditworthiness or predict
loan default risk, e.g., based on automated analysis of appl ication or support documents.
If the data used to train these models contain historical bia ses or discriminatory lending
practices, the models may learn to replicate these patterns . Consequently, they could deny
loans to certain demographics or oﬀer unfavorable terms base d on factors like race, gender,
or socioeconomic status, perpetuating ﬁnancial inequalit y ([116]).
•Content moderation : AI-powered content moderation systems help manage and ﬁlt er
user-generated content on social media platforms and other online communities. If these
systems are trained on biased data, they may disproportiona tely censor or suppress the
voices of certain groups, while allowing harmful content or misinformation from other sources
to proliferate ([117, 118, 119, 120]). Language models can b e employed to automatically
moderate and ﬁlter content on social media platforms or onli ne forums. However, these
models may struggle to understand the nuances of language, c ontext, and cultural diﬀerences.
They might over-moderate or under-moderate certain types o f content, disproportionately
aﬀecting certain groups or topics. This could lead to censors hip or the ampliﬁcation of
harmful content, perpetuating biases and misinformation ( [121, 122, 123, 124]).
11•Healthcare : AI models are increasingly used to support medical decisio n-making and re-
source allocation. Biased models may result in unfair treat ment for certain patient groups,
leading to disparities in healthcare access and quality, an d potentially exacerbating existing
health inequalities ([125, 126, 110, 105]). Large language models can be employed for tasks
such as diagnosing diseases, recommending treatments, or a nalyzing patient data ([127, 128]).
If the training data includes biased or unrepresentative in formation, the models may produce
biased outcomes. Data used to train these models might be pre dominantly collected from
speciﬁc populations, leading to less accurate predictions or recommendations for underrepre-
sentedgroups. Thiscanresultinmisdiagnoses, inadequate treatment plans, orunequalaccess
to care. Models might unintentionally learn to associate ce rtain diseases or conditions with
speciﬁc demographic factors, perpetuating stereotypes an d potentially inﬂuencing healthcare
professionals’ decision-making. Finally, biases in healt hcare data could lead to models that
prioritize certain types of treatments or interventions ov er others, disproportionately beneﬁt-
ing certain groups and disadvantaging others ([129, 130]).
•Education : AI-driven educational tools and platforms can help person alize learning experi-
ences and improve educational outcomes. However, biased mo dels may perpetuatedisparities
by favoring certain learning styles or cultural background s, disadvantaging students from un-
derrepresented or marginalized communities ([131, 132]). Large language models can be used
in education for tasks such as personalized learning, gradi ng, or content creation. If the mod-
els are trained on biased data, they may exacerbate existing biases in educational settings.
Furthermore, if used for grading or assessing student work, language models might internalize
biases in historical grading practices, leading to unfair e valuation of students based on factors
like race, gender, or socioeconomic status. Finally, acces s in itself to ChatGPT or other AI
tools for education can exacerbate preexisting inequaliti es.
5.3. Paths to AI transparency
Transparency and trust are essential components in the deve lopment and deployment of AI
systems. As these models become more integrated into variou s aspects of our lives, it is increasingly
importantforusersandregulators tounderstandhowtheyma kedecisionsandpredictions, ensuring
that they operate fairly, ethically, and responsibly.
Emphasizing transparency in AI systems can provide several beneﬁts:
•Informed decision-making : When users and regulators have a clear understanding of
how AI models make decisions and predictions, they can make m ore informed choices about
whether to use or rely on these systems in diﬀerent contexts. T ransparency can empower
users to evaluate the potential risks and beneﬁts of AI syste ms and make decisions that align
with their values and priorities ([133, 134]).
•Public trust : Fostering transparency can help build public trust in AI sy stems, as it demon-
strates a commitment to ethical development and responsibl e deployment. When users and
regulators can see that developers are taking steps to ensur e the fairness and equity of their
models, they may be more likely to trust and adopt these techn ologies ([101, 102]).
•Ethical compliance : By promoting transparency, developers can demonstrate th eir compli-
ance with ethical guidelines and regulations, showcasing t heir commitment to the responsible
development of AI systems. This can help to establish a stron g reputation and foster positive
relationships with users, regulators, and other stakehold ers ([135, 116]).
12•Collaborative improvement : Transparency can facilitate collaboration between devel op-
ers, researchers, policymakers, and aﬀected communities, e nabling them to share insights
and feedback that can help guide the development of more equi table and ethical AI systems
([108, 109, 58, 110]).
In summary, emphasizing transparency and trust in AI system s, including generative language
models, is crucial for ensuring that users and regulators ha ve a clear understanding of how these
models make decisions and predictions. By promoting transp arency, developers can demonstrate
their commitment to ethical development and responsible de ployment, fostering public trust and
paving the way for more equitable and beneﬁcial AI applicati ons.
5.4. Regulatory eﬀorts, industry standards, and ethical gu idelines
As concerns about bias in AI systems continue to grow, severa l ongoing regulatory eﬀorts
and industry standards have emerged to address these challe nges. AI ethics guidelines and fair-
ness frameworks aim to provide guidance and best practices f or developers, organizations, and
policymakers to reduce bias and ensure responsible develop ment and deployment of AI systems
([135, 136]).
Some notable eﬀorts are summarized in Table 6. These ongoing r egulatory eﬀorts and industry
standards represent important steps in addressing bias and promoting the responsible develop-
ment of AI systems. By adhering to these guidelines and frame works, developers can contribute
to the creation of AI technologies that are more equitable, f air, and beneﬁcial for all users and
communities.
6. The role of human oversight and intervention
6.1. How to identify and mitigate bias?
Identifying and mitigating bias in AI models is essential fo r ensuring their responsible and
equitable use. Various methods can be employed to address bi as in AI systems:
•Regular audits : Conducting regular audits of AI models can help identify po tential biases,
errors, or unintended consequences in their outputs. These audits involve evaluating the
model’s performance against a set of predeﬁned metrics and c riteria, which may include
fairness, accuracy, and representativeness. By monitorin g AI models on an ongoing basis,
developers can detect and address biases before they become problematic ([104]).
•Retraining with curated data : Retraining AI models with curated data can help reduce
biases in their predictions and decisions. By carefully sel ecting and preparing training data
that is more diverse, balanced, and representative of diﬀere nt perspectives, developers can
ensure that AI models learn from a broader range of inputs and experiences, which may help
mitigate the inﬂuence of biases present in the original trai ning data ([79, 8]).
•Applying fairness metrics : Fairness metrics can be used to evaluate the performance of
AI models with respect to diﬀerent user groups or populations ([145, 146]). By analyzing AI
model outputs based on these metrics, developers can identi fy potential disparities or biases
in the model’s treatment of diﬀerent users and take steps to ad dress them. Examples of
fairness metrics include demographic parity, equalized od ds, and equal opportunity ([147]).
13Table 6: Ongoing Regulatory Eﬀorts and Industry Standards t o Address Bias in AI
Eﬀort Description
European Union’s AI
Ethics GuidelinesThe EU’s High-Level Expert Group on Artiﬁcial Intelligence has developed a set of AI
Ethics Guidelines that outline key ethical principles and r equirements for trustworthy AI,
including fairness, transparency, accountability, and hu man agency ([137]).
IEEE’s Ethically
Aligned DesignThe Institute of Electrical and Electronics Engineers (IEE E) has published a comprehen-
sive document, ”Ethically Aligned Design,” which provides recommendations and guide-
lines for the ethical development of AI and autonomous syste ms, with a focus on human
rights, data agency, and technical robustness ([138]).
Partnership on AI A coalition of tech companies, research in stitutes, and civil society organizations, the
Partnership on AI aims to promote the responsible developme nt and use of AI technologies.
They work on various initiatives, including fairness, tran sparency, and accountability, to
ensure that AI beneﬁts all of humanity ([139]).
AI Fairness 360
(AIF360)Developed by IBM Research, AIF360 is an open-source toolkit that provides a compre-
hensive suite of metrics and algorithms to help developers d etect and mitigate bias in
their AI models. It assists developers in understanding and addressing fairness concerns
in their AI applications ([140]).
Google’s AI Princi-
plesGoogle has outlined a set of AI Principles that guide the ethi cal development and use of
AI technologies within the company. These principles empha size fairness, transparency,
accountability, and the importance of avoiding harmful or u njust impacts ([141]).
Algorithmic Impact
Assessment (AIA)Developed by the AI Now Institute, the AIA is a framework desi gned to help organizations
evaluate and mitigate the potential risks and harms of AI sys tems. The AIA guides
organizations through a structured process of identifying and addressing potential biases,
discrimination, and other negative consequences of AI depl oyment ([142]).
OECD Recommenda-
tion of the Council on
Artiﬁcial IntelligenceThe OECD Recommendation of the Council on Artiﬁcial Intelli gence is a set of guidelines
that provides a framework for the responsible development a nd deployment of AI, with
ﬁve principles focused on inclusive growth, human-centere d values, transparency and ex-
plainability, robustness, security and safety, and accoun tability ([143, 144]).
•Algorithmic debiasing techniques : Various algorithmic techniques have been developed
to mitigate bias in AI models during training or post-proces sing. Some of these techniques
include adversarial training, re-sampling, and re-weight ing, which aim to minimize the inﬂu-
ence of biased patterns and features on the model’s predicti ons and decisions ([28, 148, 36,
70, 49, 100, 114, 112]).
•Inclusion of diverse perspectives : Ensuring that AI development teams are diverse and
inclusive can help bring a wide range of perspectives and exp eriences to the table, which
can contribute to the identiﬁcation and mitigation of biase s in AI models. By involving
individuals from diﬀerent backgrounds, cultures, and disci plines, developers can create more
robust, fair, and representative AI systems ([67, 68]).
•Human-in-the-loop approaches : Incorporating human experts into the AI model devel-
opment and decision-making processes can help provide valu able contextual understanding
and ethical judgment that AI models may lack. Humans can serv e as an additional layer of
quality control, identifying biases, errors, or unintende d consequences in AI model outputs
and providing feedback to improve the model’s performance a nd fairness.
Next, we dive deeper into the role that human experts can play in the responsible design and
14development of AI systems, including large language models , and their continuous oversight.
6.2. The importance of humans in the AI loop
Emphasizingtheimportanceof involving human expertsin AI system development, monitoring,
anddecision-makingiscrucialtoensuringtheresponsible andethical deploymentofAItechnologies.
Humans possess the ability to provide context and ethical ju dgment that AI models may lack, and
their involvement can help address potential biases, error s, and unintended consequences that may
arise from the use of these systems. Some key beneﬁts of invol ving human experts in AI system
development include:
•Contextual understanding : Humanexpertscanprovidevaluableinsightsintothecultu ral,
social, and historical contexts that shape language and com munication, helping to guide AI
models in generating more appropriate and sensitive respon ses ([131, 149, 150]).
•Ethical judgment : Human experts possess the moral and ethical reasoning skil ls needed to
evaluate the potential impacts of AI systems on users and aﬀec ted communities. By involving
human experts in decision-making, we can ensurethat AI mode ls align with ethical principles
and values, such as fairness, transparency, and accountabi lity ([138, 135, 151, 137, 116]).
•Bias identiﬁcation and mitigation : Humanexpertscanhelpidentifyandaddressbiasesin
AImodels, workingalongside developers toimplement strat egies for mitigating or eliminating
harmful biases and promoting more equitable and representa tive AI systems ([36, 100, 112]).
•Quality assurance and validation : Human experts can serve as a vital layer of quality
control, evaluating AI model outputs for coherence, releva nce, and potential biases, and
providing feedback to improve the model’s performance, acc uracy, regulatory compliance,
and trustworthiness ([152]).
•Human override : Incorporating human experts into AI system workﬂows can he lp strike a
balancebetweenautomationandhumanjudgment, allowinghu manstointerveneandoverride
AI model decisions when necessary to ensure fairness, accou ntability, and ethical compliance
([153]).
By involving human experts in the development, monitoring, and decision-making processes of
AI systems, we can leverage their contextual understanding and ethical judgment to complement
the capabilities of AI models. This collaborative approach can help us create AI systems that are
more responsible, equitable, and beneﬁcial for all users, w hile also addressing the potential risks
and challenges associated with bias in AI.
6.3. Possible strategies and best practices to address bias in generative AI
Addressing and mitigating potential biases in generative A I models requires a collaborative
eﬀort between AI developers, users, and aﬀected communities. Fostering a more inclusive and fair
AI ecosystem involves engaging various stakeholders in the development, evaluation, and deploy-
ment of AI technologies. This collaboration can oversee tha t AI models are designed to be more
equitable, representative, and beneﬁcial to all users.
Some key aspects of fostering collaboration in the AI ecosys tem are shown in Table 7. By fos-
tering collaboration between AI developers, users, and aﬀec ted communities, we can work towards
15Strategy Description References
Engaging with af-
fected communitiesInvolving aﬀected communities in the development
and evaluation of AI models can lead to the creation
of generative AI systems that are more culturally sen-
sitive, contextually relevant, and fair to all users[154, 155, 156, 157, 158, 159,
160]
Multidisciplinary col-
laborationBringing together experts from diﬀerent ﬁelds, such
as computer science, social sciences, humanities, and
ethics, can help to develop more robust strategies for
addressing and mitigating bias in generative AI sys-
tems[109, 41, 156]
User feedback and
evaluationEncouraging users to provide feedback on AI model
outputs and performance can contribute to the on-
going improvement and reﬁnement of generative AI
models, ensuring that they remain fair, accurate, and
relevant to users’ needs[161, 162, 163, 164]
Openness and trans-
parencySharing information about the methodologies, data
sources, and potential biases of generative AI mod-
els can enable stakeholders to make more informed
decisions about whether and how to use these tech-
nologies in diﬀerent contexts[156, 165, 143, 135]
Establishing partner-
shipsForming partnerships between AI developers, re-
search institutions, non-proﬁt organizations, and
other stakeholders can facilitate the sharing of knowl-
edge, resources, and best practices, leading to the de-
velopment of more equitable and responsible AI tech-
nologies[135]
Table 7: Strategies for addressing bias in generative AI sys tems
creating a more inclusive and fair AI ecosystem that respect s and values diverse perspectives and
experiences. This collaborative approach can help ensure t hat AI technologies are developed and
deployed in a way that is equitable, responsible, and beneﬁc ial for all users, while also addressing
the potential risks and challenges associated with bias in A I.
7. Conclusions
This paper highlights the challenges and risks associated w ith biases in generative language
models like ChatGPT, emphasizing the need for a multi-disci plinary, collaborative eﬀort to develop
more equitable, transparent, and responsible AI systems th at enhance a wide array of applications
while minimizing unintended consequences.
Various methods for identifying and mitigating bias in AI mo dels were presented, including
regular audits, retraining with curated data, applying fai rness metrics, and incorporating human
experts in AI system development, monitoring, and decision -making. To achieve this goal, the
development and deployment of AI technologies should prior itize ethical principles, such as fairness
andequality, ensuringthatall usersandgroupsaretreated equitably. Humanoversight playsavital
role in providing context and ethical judgment that AI model s may lack, helping to identify and
address potential biases, errors, or unintended consequen ces. Collaboration between AI developers,
users, and aﬀected communities is essential for fostering a m ore inclusive and fair AI ecosystem,
ensuring that diverse perspectives and experiences are con sidered and valued.
Continued research into methods for identifying, addressi ng, and mitigating biases in AI models
will be critical to advancing the state of the art and promoti ng more equitable and inclusive AI
16Table 8: Future avenues for Large Language Models and genera tive AI research.
Research Area Description
Fairness, Bias, and Ethics Addressing and minimizing biase s in language models, as well as under-
standing their ethical implications, is a critical area of r esearch; developing
methods to detect, mitigate and prevent biases in AI models i s essential.
Interpretability and Explainability Understanding the in ternal workings of large language models is a signiﬁ-
cant challenge; researchers are developing methods to make models more
interpretable and explain their predictions.
Auditability and Accountability Large language models inc reasingly impact various sectors of society, inﬂu-
encing decision-making and shaping public discourse; ensu ring that models
are transparent, their actions can be traced, and those resp onsible for their
development and deployment can be held accountable for the c onsequences
of the AI’s actions is vital for fostering trust and maintain ing ethical stan-
dards in the AI community and beyond.
Controllable and Safe AI Ensuring that AI models can generat e outputs that align with human inten-
tions and values is an important research question; develop ing methods to
control AI behavior, reduce harmful outputs and improve saf ety measures
is vital.
Societal Eﬀects The societal eﬀects and implications of the deployment of AI systems encom-
pass a wide range of concerns, including labor markets, priv acy, bias, access
to technology, public discourse, security, ethics, and reg ulation; Observing,
characterizing, quantifying and understanding the broade r eﬀects that the
deployment of AI systems has on society warrant careful cons ideration and
continued research as these technologies continue to proli ferate.
systems. By bringing together experts from various discipl ines, including computer science, social
sciences, humanities, andethics, wecan fosteramorecompr ehensiveunderstandingof thepotential
biases and ethical challenges associated with AI applicati ons.
Fostering an open and ongoing dialogue between stakeholder s is crucial for sharing knowledge,
best practices, and lessons learned from the development an d use of AI applications. This dialogue
can help to raise awareness of the potential risks and challe nges associated with biases in AI models
and promote the development of strategies and guidelines fo r mitigating their negative impacts.
Future research avenues. As the development of large language models continues, seve ral essential
aspects of research are necessary to advance their understa nding and ensure their responsible
deployment ( cf., Table 8). Among these are understanding their inner workin gs, addressing ethical
concerns, ensuring controllability and safety, and develo ping more robust evaluation methods.
One critical area of research is fairness, bias, and ethics, which involves detecting, mitigating,
and preventing biases in AI models to minimize their impact o n various sectors of society. Another
important research question is interpretability and expla inability, as understanding the internal
workings of large language models remains a signiﬁcant chal lenge. Researchers are working to
make models more interpretable and explain their predictio ns. Additionally, large language models
can inﬂuence decision-making and shape public discourse, m aking auditability and accountability
essential for fostering trust and maintaining ethical stan dards in the AI community and beyond.
Controllable and safe AI is also important to ensure that AI m odels can generate outputs
that align with human intentions and values. Finally, obser ving, characterizing, quantifying, and
understandingthe broader eﬀects that deploying AI systems h as on society is critical for addressing
17societal eﬀects, which encompass a wide range of concerns, in cluding labor markets, privacy, bias,
access to technology, public discourse, security, ethics, and regulation. The need for a multi-
disciplinary, collaborative eﬀortto develop moreequitabl e, transparent, and responsibleAI systems
is clear.
This paper emphasized the challenges and risks associated w ith biases in generative language
models like ChatGPT and highlights the importance of contin ued research to develop responsible
AI systems that enhance a wide array of applications while mi nimizing unintended consequences.
Acknowledgements
The author is grateful to all current and past members of his l ab at USC, and the numerous
colleagues and students at USC Viterbi and Annenberg, who en gaged in stimulating discussions
and provided invaluable feedback about this study.
References
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones , A. N. Gomez, /suppress L. Kaiser, I. Polosukhin, Attention
is all you need, Advances in neural information processing s ystems 30 (2017).
[2] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-t raining of deep bidirectional transformers for
language understanding, arXiv preprint arXiv:1810.04805 (2018).
[3] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., Improving language understanding by generative
pre-training (2018).
[4] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskev er, et al., Language models are unsupervised
multitask learners, OpenAI blog 1 (8) (2019) 9.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dh ariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al., Language models are few-shot learners, Ad vances in neural information processing systems
33 (2020) 1877–1901.
[6] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimo v, Proximal policy optimization algorithms, arXiv
preprint arXiv:1707.06347 (2017).
[7] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, G. Irving, Fine-tuning
language models from human preferences, arXiv preprint arX iv:1909.08593 (2019).
[8] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P . Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,
et al., Training language models to follow instructions wit h human feedback, arXiv preprint arXiv:2203.02155
(2022).
[9] OpenAI, Gpt-4 technical report (2023).
[10] Y. Bengio, R. Ducharme, P. Vincent, A neural probabilis tic language model, Advances in neural information
processing systems 13 (2000).
[11] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean , Distributed representations of words and phrases
and their compositionality, Advances in neural informatio n processing systems 26 (2013).
[12] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learning with neural networks, Advances in neural
information processing systems 27 (2014).
[13] T. Young, D. Hazarika, S. Poria, E. Cambria, Recent tren ds in deep learning based natural language processing,
IEEE Computational intelligenCe magazine 13 (3) (2018) 55– 75.
[14] H. Chen, X. Liu, D. Yin, J. Tang, A survey on dialogue syst ems: Recent advances and new frontiers, Acm
SIGKDD Explorations Newsletter 19 (2) (2017) 25–35.
[15] S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, J. Wes ton, Personalizing dialogue agents: I have a dog,
do you have pets too?, arXiv preprint arXiv:1801.07243 (201 8).
[16] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-B arahona, P.-H. Su, S. Ultes, S. Young, A network-
based end-to-end trainable task-oriented dialogue system , arXiv preprint arXiv:1604.04562 (2016).
[17] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, L. S. Chao , Learning deep transformer models for machine
translation, arXiv preprint arXiv:1906.01787 (2019).
[18] A. Karakanta, J. Dehdari, J. van Genabith, Neural machi ne translation for low-resource languages without
parallel corpora, Machine Translation 32 (2018) 167–189.
18[19] N. Pourdamghani, K. Knight, Neighbors helping the poor : improving low-resource machine translation using
related languages, Machine Translation 33 (3) (2019) 239–2 58.
[20] M. R. Costa-juss` a, J. Cross, O. C ¸elebi, M. Elbayad, K. Heaﬁeld, K. Heﬀernan, E. Kalbassi, J. Lam, D. Licht,
J. Maillard, et al., No language left behind: Scaling human- centered machine translation, arXiv preprint
arXiv:2207.04672 (2022).
[21] S. Ranathunga, E.-S. A. Lee, M. Prifti Skenduli, R. Shek har, M. Alam, R. Kaur, Neural machine translation
for low-resource languages: A survey, ACM Computing Survey s 55 (11) (2023) 1–37.
[22] C. Christianson, J. Duncan, B. Onyshkevych, Overview o f the DARPA LORELEI program, Machine Transla-
tion 32 (2018) 3–9.
[23] K.-C. Yang, E. Ferrara, F. Menczer, Botometer 101: Soci al bot practicum for computational social scientists,
Journal of Computational Social Science 5 (2) (2022) 1511–1 528.
[24] E. Ferrara, Social bot detection in the age of ChatGPT: C hallenges and opportunities, First Monday 28 (6)
(2023).
[25] A. Gilson, C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor, D. Chartash, et al., How does
ChatGPT perform on the united states medical licensing exam ination? the implications of large language
models for medical education and knowledge assessment, JMI R Medical Education 9 (1) (2023) e45312.
[26] J. H. Choi, K. E. Hickman, A. Monahan, D. Schwarcz, ChatG PT goes to law school, Available at SSRN (2023).
[27] E. Ferrara, GenAI against humanity: Nefarious applica tions of generative artiﬁcial intelligence and large
language models, arXiv preprint arXiv:2310.00737 (2023).
[28] T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, A. T. Kalai, Man is to computer programmer as woman
is to homemaker? debiasing word embeddings, Advances in neu ral information processing systems 29 (2016).
[29] A. Caliskan, J. J. Bryson, A. Narayanan, Semantics deri ved automatically from language corpora contain
human-like biases, Science 356 (6334) (2017) 183–186.
[30] S. L. Blodgett, S. Barocas, H. Daum´ e III, H. Wallach, La nguage (technology) is power: A critical survey of
“bias” in NLP, arXiv preprint arXiv:2005.14050 (2020).
[31] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchel l, On the dangers of stochastic parrots: Can language
models be too big?, in: Proceedings of the 2021 ACM conferenc e on fairness, accountability, and transparency,
2021, pp. 610–623.
[32] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herber t-Voss, J. Wu, A. Radford, G. Krueger, J. W. Kim,
S. Kreps, et al., Release strategies and the social impacts o f language models, arXiv preprint arXiv:1908.09203
(2019).
[33] D. Hovy, S. Prabhumoye, Five sources of bias in natural l anguage processing, Language and Linguistics Com-
pass 15 (8) (2021) e12432.
[34] R. Munro, S. Bethard, V. Kuperman, V. T. Lai, R. Melnick, C. Potts, T. Schnoebelen, H. Tily, Crowdsourcing
and language studies: the new generation of linguistic data , in: NAACL Workshop on Creating Speech and
Language Data With Amazon’s Mechanical Turk, Association f or Computational Linguistics, 2010, pp. 122–
130.
[35] J. Buolamwini, T. Gebru, Gender shades: Intersectiona l accuracy disparities in commercial gender classiﬁcation ,
in: Conference on fairness, accountability and transparen cy, PMLR, 2018, pp. 77–91.
[36] E. M. Bender, B. Friedman, Data statements for natural l anguage processing: Toward mitigating system bias
and enabling better science, Transactions of the Associati on for Computational Linguistics 6 (2018) 587–604.
[37] J. Kleinberg, S. Mullainathan, M. Raghavan, Inherent t rade-oﬀs in the fair determination of risk scores, arXiv
preprint arXiv:1609.05807 (2016).
[38] R. Benjamin, Race after technology: Abolitionist tool s for the new jim code (2020).
[39] F. Doshi-Velez, B. Kim, Towards a rigorous science of in terpretable machine learning, arXiv preprint
arXiv:1702.08608 (2017).
[40] R. Binns, Fairness in machine learning: Lessons from po litical philosophy, in: Conference on fairness, account-
ability and transparency, PMLR, 2018, pp. 149–159.
[41] K. Crawford, R. Dobbe, T. Dryer, G. Fried, B. Green, E. Ka ziunas, A. Kak, V. Mathur, E. McElroy, A. N.
S´ anchez, et al., Ai now 2019 report, New York, NY: AI Now Inst itute (2019).
[42] M. O. Prates, P. H. Avelar, L. C. Lamb, Assessing gender b ias in machine translation: a case study with
Google translate, Neural Computing and Applications 32 (20 20) 6363–6381.
[43] OpenAI, GPT-4 System Card (2023).
[44] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Ho rvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,
S. Lundberg, et al., Sparks of artiﬁcial general intelligen ce: Early experiments with gpt-4, arXiv preprint
arXiv:2303.12712 (2023).
19[45] C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Ma tena, Y. Zhou, W. Li, P. J. Liu, Exploring the
limits of transfer learning with a uniﬁed text-to-text tran sformer, The Journal of Machine Learning Research
21 (1) (2020) 5485–5551.
[46] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. La chaux, T. Lacroix, B. Rozi` ere, N. Goyal, E. Hambro,
F. Azhar, et al., Llama: Open and eﬃcient foundation languag e models, arXiv preprint arXiv:2302.13971
(2023).
[47] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual Chat GPT: Talking, drawing and editing with visual
foundation models, arXiv preprint arXiv:2303.04671 (2023 ).
[48] H. R. Kirk, Y. Jun, F. Volpin, H. Iqbal, E. Benussi, F. Dre yer, A. Shtedritski, Y. Asano, Bias out-of-the-box:
An empirical analysis of intersectional occupational bias es in popular generative language models, Advances
in neural information processing systems 34 (2021) 2611–26 24.
[49] S. Bordia, S. R. Bowman, Identifying and reducing gende r bias in word-level language models, arXiv preprint
arXiv:1904.03035 (2019).
[50] M. T. Ribeiro, T. Wu, C. Guestrin, S. Singh, Beyond accur acy: Behavioral testing of NLP models with
checklist, arXiv preprint arXiv:2005.04118 (2020).
[51] A. Conneau, G. Lample, M. Ranzato, L. Denoyer, H. J´ egou , Word translation without parallel data, arXiv
preprint arXiv:1710.04087 (2017).
[52] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Che n, N. Thorat, F. Vi´ egas, M. Wattenberg,
G. Corrado, et al., Google’s multilingual neural machine tr anslation system: Enabling zero-shot translation,
Transactions of the Association for Computational Linguis tics 5 (2017) 339–351.
[53] T. Pires, E. Schlinger, D. Garrette, How multilingual i s multilingual BERT?, arXiv preprint arXiv:1906.01502
(2019).
[54] S. Ruder, I. Vuli´ c, A. Søgaard, A survey of cross-lingu al word embedding models, Journal of Artiﬁcial Intelli-
gence Research 65 (2019) 569–631.
[55] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi , F. Roesner, Y. Choi, Defending against neural fake
news, Advances in neural information processing systems 32 (2019).
[56] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong re asons: Diagnosing syntactic heuristics in natural
language inference, arXiv preprint arXiv:1902.01007 (201 9).
[57] N. A. Smith, Contextual word representations: putting words into computers, Communications of the ACM
63 (6) (2020) 66–74.
[58] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman , B. Hutchinson, E. Spitzer, I. D. Raji, T. Gebru,
Model cards for model reporting, in: Proceedings of the conf erence on fairness, accountability, and transparency,
2019, pp. 220–229.
[59] N. Garg, L. Schiebinger, D. Jurafsky, J. Zou, Word embed dings quantify 100 years of gender and ethnic
stereotypes, Proceedings of the National Academy of Scienc es 115 (16) (2018) E3635–E3644.
[60] L. Dixon, J. Li, J. Sorensen, N. Thain, L. Vasserman, Mea suring and mitigating unintended bias in text
classiﬁcation, in: Proceedings of the 2018 AAAI/ACM Confer ence on AI, Ethics, and Society, 2018, pp. 67–73.
[61] R. W. McGee, Is ChatGPT biased against conservatives? a n empirical study, An Empirical Study (February
15, 2023) (2023).
[62] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,
S. Gehrmann, et al., Palm: Scaling language modeling with pa thways, arXiv preprint arXiv:2204.02311 (2022).
[63] Z. Jiang, F. F. Xu, J. Araki, G. Neubig, How can we know wha t language models know?, Transactions of the
Association for Computational Linguistics 8 (2020) 423–43 8.
[64] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herb ert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song,
U. Erlingsson, et al., Extracting training data from large l anguage models., in: USENIX Security Symposium,
Vol. 6, 2021.
[65] E. Ferrara, The history of digital spam, Communication s of the ACM 62 (8) (2019) 82–91.
[66] E. Ferrara, Twitter spam and false accounts prevalence , detection, and characterization: A survey, First
Monday 27 (12) (2022).
[67] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Fo ster, J. Phang, H. He, A. Thite, N. Nabeshima,
et al., The pile: An 800gb dataset of diverse text for languag e modeling, arXiv preprint arXiv:2101.00027
(2020).
[68] S. Biderman, K. Bicheno, L. Gao, Datasheet for the pile, arXiv preprint arXiv:2201.07311 (2022).
[69] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, N. A. Smith, Annotation artifacts in
natural language inference data, arXiv preprint arXiv:180 3.02324 (2018).
[70] S. Dev, J. Phillips, Attenuating bias in word vectors, i n: The 22nd International Conference on Artiﬁcial
20Intelligence and Statistics, PMLR, 2019, pp. 879–887.
[71] E. Ferrara, The butterﬂy eﬀect in artiﬁcial intelligen ce systems: Implications for ai bias and fairness, arXiv
preprint arXiv:2307.05842 (2023).
[72] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, D. Zhou, Chain of thought prompting elicits
reasoning in large language models, arXiv preprint arXiv:2 201.11903 (2022).
[73] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM. int8 (): 8-bit matrix multiplication for transformers
at scale, Advances in Neural Information Processing System s 35 (2022) 30318–30332.
[74] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid , A. Fisch, A. R. Brown, A. Santoro, A. Gupta,
A. Garriga-Alonso, et al., Beyond the imitation game: Quant ifying and extrapolating the capabilities of lan-
guage models, arXiv preprint arXiv:2206.04615 (2022).
[75] J. Wei, Y. Tay, R. Bommasani, C. Raﬀel, B. Zoph, S. Borgea ud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler,
et al., Emergent abilities of large language models, arXiv p reprint arXiv:2206.07682 (2022).
[76] S. Chiappa, Path-speciﬁc counterfactual fairness, in : Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, Vol. 33, 2019, pp. 7801–7808.
[77] V. Kumar, H. Koorehdavoudi, M. Moshtaghi, A. Misra, A. C hadha, E. Ferrara, Controlled text generation
with hidden representation transformations, in: Findings of the Association for Computational Linguistics:
ACL 2023, Association for Computational Linguistics, Toro nto, Canada, 2023, pp. 9440–9455.
[78] D. Hovy, S. L. Spruit, The social impact of natural langu age processing, in: Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics ( Volume 2: Short Papers), 2016, pp. 591–598.
[79] S. Gururangan, A. Marasovi´ c, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, N. A. Smith, Don’t stop
pretraining: Adapt language models to domains and tasks, ar Xiv preprint arXiv:2004.10964 (2020).
[80] J. H. Park, P. Fung, One-step and two-step classiﬁcatio n for abusive language detection on twitter, arXiv
preprint arXiv:1706.01206 (2017).
[81] Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio , J. Chai, M. Lapata, A. Lazaridou, J. May,
A. Nisnevich, et al., Experience grounds language, arXiv pr eprint arXiv:2004.10151 (2020).
[82] P. Bourdieu, Language and symbolic power, Harvard Univ ersity Press, 1991.
[83] N. Fairclough, Language and power, Pearson Education, 2001.
[84] G. Lakoﬀ, M. Johnson, Metaphors we live by, University o f Chicago press, 2008.
[85] J. H. Hill, The everyday language of white racism, John W iley & Sons, 2009.
[86] B. L. Whorf, Language, thought, and reality: Selected w ritings, MIT press, 2012.
[87] M. Foucault, Archaeology of knowledge, Routledge, 201 3.
[88] C. Geertz, The interpretation of cultures, Vol. 5043, B asic books, 1973.
[89] G. Hofstede, Culture’s consequences: International d iﬀerences in work-related values, Vol. 5, sage, 1984.
[90] R. Inglehart, Christian Welzel Modernization, Cultur al Change, and Democracy The Human Development
Sequence, Cambridge: Cambridge university press, 2005.
[91] H. C. Triandis, Individualism and collectivism, Routl edge, 2018.
[92] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian , On the (im)possibility of fairness, arXiv preprint
arXiv:1609.07236 (2016).
[93] M. B. Zafar, I. Valera, M. Gomez Rodriguez, K. P. Gummadi , Fairness beyond disparate treatment & disparate
impact: Learning classiﬁcation without disparate mistrea tment, in: Proceedings of the 26th international
conference on world wide web, 2017, pp. 1171–1180.
[94] S. Barocas, M. Hardt, A. Narayanan, Fairness and machin e learning: Limitations and opportunities, Fairness
and Machine Learning: Limitation and Oppotunities (2019).
[95] S. S. Mufwene, The Ecology of Language Evolution. Cambr idge Approaches to Language Contact., ERIC,
2001.
[96] H. Jenkins, M. Deuze, Convergence culture (2008).
[97] M. Castells, The rise of the network society, John wiley & sons, 2011.
[98] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H . Wallach, H. D. Iii, K. Crawford, Datasheets for
datasets, Communications of the ACM 64 (12) (2021) 86–92.
[99] A. Torralba, A. A. Efros, Unbiased look at dataset bias, in: CVPR 2011, IEEE, 2011, pp. 1521–1528.
[100] T. Sun, A. Gaut, S. Tang, Y. Huang, M. ElSherief, J. Zhao , D. Mirza, E. Belding, K.-W. Chang, W. Y. Wang,
Mitigating gender bias in natural language processing: Lit erature review, arXiv preprint arXiv:1906.08976
(2019).
[101] S. Larsson, F. Heintz, Transparency in artiﬁcial inte lligence, Internet Policy Review 9 (2) (2020).
[102] U. Ehsan, Q. V. Liao, M. Muller, M. O. Riedl, J. D. Weisz, Expanding explainability: Towards social trans-
parency in AI systems, in: Proceedings of the 2021 CHI Confer ence on Human Factors in Computing Systems,
212021, pp. 1–19.
[103] S. Wachter, B. Mittelstadt, L. Floridi, Transparent, explainable, and accountable AI for robotics, Science
robotics 2 (6) (2017) eaan6080.
[104] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud, D. Theron, P. Barnes,
Closing the AI accountability gap: Deﬁning an end-to-end fr amework for internal algorithmic auditing, in:
Proceedings of the 2020 conference on fairness, accountabi lity, and transparency, 2020, pp. 33–44.
[105] H. Smith, Clinical AI: opacity, accountability, resp onsibility and liability, AI & SOCIETY 36 (2) (2021) 535–
545.
[106] M. R. Morris, Ai and accessibility, Communications of the ACM 63 (6) (2020) 35–37.
[107] R. Schwartz, J. Dodge, N. A. Smith, O. Etzioni, Green AI , Communications of the ACM 63 (12) (2020) 54–63.
[108] C. O’neil, Weapons of math destruction: How big data in creases inequality and threatens democracy, Crown,
2017.
[109] K. Holstein, J. Wortman Vaughan, H. Daum´ e III, M. Dudi k, H. Wallach, Improving fairness in machine learning
systems: What do industry practitioners need?, in: Proceed ings of the 2019 CHI conference on human factors
in computing systems, 2019, pp. 1–16.
[110] R. Challen, J. Denny, M. Pitt, L. Gompels, T. Edwards, K . Tsaneva-Atanasova, Artiﬁcial intelligence, bias
and clinical safety, BMJ Quality & Safety 28 (3) (2019) 231–2 37.
[111] M. Bogen, A. Rieke, Help wanted: An examination of hiri ng algorithms, equity, and bias (2018).
[112] M. Raghavan, S. Barocas, J. Kleinberg, K. Levy, Mitiga ting bias in algorithmic hiring: Evaluating claims
and practices, in: Proceedings of the 2020 conference on fai rness, accountability, and transparency, 2020, pp.
469–481.
[113] D. K. Citron, F. Pasquale, The scored society: Due proc ess for automated predictions, Wash. L. Rev. 89 (2014)
1.
[114] N. T. Lee, P. Resnick, G. Barton, Algorithmic bias dete ction and mitigation: Best practices and policies to
reduce consumer harms, Brookings Institute: Washington, D C, USA 2 (2019).
[115] B. Ustun, A. Spangher, Y. Liu, Actionable recourse in l inear classiﬁcation, in: Proceedings of the conference
on fairness, accountability, and transparency, 2019, pp. 1 0–19.
[116] L. Weidinger, J. Mellor, M. Rauh, C. Griﬃn, J. Uesato, P .-S. Huang, M. Cheng, M. Glaese, B. Balle,
A. Kasirzadeh, et al., Ethical and social risks of harm from l anguage models, arXiv preprint arXiv:2112.04359
(2021).
[117] T. Gillespie, Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape
social media, Yale University Press, 2018.
[118] S. T. Roberts, Behind the screen, Yale University Pres s, 2019.
[119] E. C. Choi, E. Ferrara, Automated claim matching with l arge language models: Empowering fact-checkers in
the ﬁght against misinformation, arXiv preprint arXiv:231 0.09223 (2023).
[120] I. Augenstein, T. Baldwin, M. Cha, T. Chakraborty, G. L . Ciampaglia, D. Corney, R. DiResta, E. Fer-
rara, S. Hale, A. Halevy, et al., Factuality challenges in th e era of large language models, arXiv preprint
arXiv:2310.05189 (2023).
[121] M. Sap, D. Card, S. Gabriel, Y. Choi, N. A. Smith, The ris k of racial bias in hate speech detection, in:
Proceedings of the 57th annual meeting of the association fo r computational linguistics, 2019, pp. 1668–1678.
[122] T. Davidson, D. Bhattacharya, I. Weber, Racial bias in hate speech and abusive language detection datasets,
arXiv preprint arXiv:1905.12516 (2019).
[123] I. V. Pasquetto, B. Swire-Thompson, M. A. Amazeen, F. B enevenuto, N. M. Brashier, R. M. Bond, L. C.
Bozarth, C. Budak, U. K. Ecker, L. K. Fazio, et al., Tackling m isinformation: What researchers could do with
social media data, The Harvard Kennedy School Misinformati on Review (2020).
[124] F. Ezzeddine, O. Ayoub, S. Giordano, G. Nogara, I. Sbei ty, E. Ferrara, L. Luceri, Exposing inﬂuence campaigns
in the age of llms: a behavioral-based ai approach to detecti ng state-sponsored trolls, EPJ Data Science 12 (1)
(2023) 46.
[125] M. A. Gianfrancesco, S. Tamang, J. Yazdany, G. Schmaju k, Potential biases in machine learning algorithms
using electronic health record data, JAMA internal medicin e 178 (11) (2018) 1544–1547.
[126] Z. Obermeyer, B. Powers, C. Vogeli, S. Mullainathan, D issecting racial bias in an algorithm used to manage
the health of populations, Science 366 (6464) (2019) 447–45 3.
[127] T. Davenport, R. Kalakota, The potential for artiﬁcia l intelligence in healthcare, Future healthcare journal
6 (2) (2019) 94.
[128] K. Y. Ngiam, W. Khor, Big data and machine learning algo rithms for health-care delivery, The Lancet Oncology
20 (5) (2019) e262–e273.
22[129] J. K. Paulus, D. M. Kent, Predictably unequal: underst anding and addressing concerns that algorithmic clinical
prediction may increase health disparities, NPJ digital me dicine 3 (1) (2020) 99.
[130] E. Ferrara, Fairness and bias in artiﬁcial intelligen ce: A brief survey of sources, impacts, and mitigation
strategies, arXiv preprint arXiv:2304.07683 (2023).
[131] M. C. Elish, D. Boyd, Situating methods in the magic of B ig Data and AI, Communication monographs 85 (1)
(2018) 57–80.
[132] N. Selwyn, Should robots replace teachers?: AI and the future of education, John Wiley & Sons, 2019.
[133] B. Goodman, S. Flaxman, European union regulations on algorithmic decision-making and a “right to expla-
nation”, AI magazine 38 (3) (2017) 50–57.
[134] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Gi annotti, D. Pedreschi, A survey of methods for explaining
black box models, ACM computing surveys (CSUR) 51 (5) (2018) 1–42.
[135] A. Jobin, M. Ienca, E. Vayena, The global landscape of A I ethics guidelines, Nature Machine Intelligence 1 (9)
(2019) 389–399.
[136] L. Floridi, Establishing the rules for building trust worthy AI, Nature Machine Intelligence 1 (6) (2019) 261–262 .
[137] Independent High-Level Expert Group on Artiﬁcial Int elligence, Ethics guidelines for trustworthy AI, Technica l
report, European Commission (2019).
[138] K. Shahriari, M. Shahriari, Ieee standard review—eth ically aligned design: A vision for prioritizing human well -
being with artiﬁcial intelligence and autonomous systems, in: 2017 IEEE Canada International Humanitarian
Technology Conference (IHTC), IEEE, 2017, pp. 197–201.
[139] J. Heer, The partnership on AI, AI Matters 4 (3) (2018) 2 5–26.
[140] R. K. Bellamy, K. Dey, M. Hind, S. C. Hoﬀman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta,
A. Mojsilovi´ c, et al., Ai fairness 360: An extensible toolk it for detecting and mitigating algorithmic bias, IBM
Journal of Research and Development 63 (4/5) (2019) 4–1.
[141] S. Pichai, Ai at Google: our principles, The Keyword 7 ( 2018) 1–3.
[142] D. Reisman, J. Schultz, K. Crawford, M. Whittaker, Alg orithmic impact assessments: A practical framework
for public agency, AI Now (2018).
[143] C. Cath, S. Wachter, B. Mittelstadt, M. Taddeo, L. Flor idi, Artiﬁcial intelligence and the ‘good society’: the
US, EU, and UK approach, Science and engineering ethics 24 (2 018) 505–528.
[144] K. Yeung, Recommendation of the council on artiﬁcial i ntelligence (OECD), International legal materials 59 (1)
(2020) 27–34.
[145] S. Yan, H.-t. Kao, E. Ferrara, Fair class balancing: En hancing model fairness without observing sensitive at-
tributes, in: Proceedings of the 29th ACM International Con ference on Information & Knowledge Management,
2020, pp. 1715–1724.
[146] Y. H. Ezzeldin, S. Yan, C. He, E. Ferrara, S. Avestimehr , Fairfed: Enabling group fairness in federated learning,
in: Proceedings of the 37th AAAI Conference on Artiﬁcial Int elligence, 2023.
[147] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, A. Gal styan, A survey on bias and fairness in machine
learning, ACM Computing Surveys (CSUR) 54 (6) (2021) 1–35.
[148] B. H. Zhang, B. Lemoine, M. Mitchell, Mitigating unwan ted biases with adversarial learning, in: Proceedings
of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 2 018, pp. 335–340.
[149] D. Leslie, Understanding artiﬁcial intelligence eth ics and safety, arXiv preprint arXiv:1906.05684 (2019).
[150] Y. K. Dwivedi, L. Hughes, E. Ismagilova, G. Aarts, C. Co ombs, T. Crick, Y. Duan, R. Dwivedi, J. Edwards,
A. Eirug, et al., Artiﬁcial intelligence (ai): Multidiscip linary perspectives on emerging challenges, opportunitie s,
and agenda for research, practice and policy, Internationa l Journal of Information Management 57 (2021)
101994.
[151] N. A. Smuha, The EU approach to ethics guidelines for tr ustworthy artiﬁcial intelligence, Computer Law
Review International 20 (4) (2019) 97–106.
[152] M. Felderer, R. Ramler, Quality assurance for ai-base d systems: Overview and challenges (introduction to
interactive session), in: Software Quality: Future Perspe ctives on Software Engineering Quality: 13th Inter-
national Conference, SWQD 2021, Vienna, Austria, January 1 9–21, 2021, Proceedings 13, Springer, 2021, pp.
33–42.
[153] A. Etzioni, O. Etzioni, Keeping AI legal, Vand. J. Ent. & Tech. L. 19 (2016) 133.
[154] W. Wallach, C. Allen, I. Smit, Machine morality: botto m-up and top-down approaches for modelling human
moral faculties, AI & Society 22 (2008) 565–582.
[155] L. Taylor, R. Schroeder, Is bigger better? the emergen ce of big data as a tool for international development
policy, GeoJournal 80 (2015) 503–518.
[156] B. D. Mittelstadt, P. Allo, M. Taddeo, S. Wachter, L. Fl oridi, The ethics of algorithms: Mapping the debate,
23Big Data & Society 3 (2) (2016) 2053951716679679.
[157] V. Eubanks, Automating inequality: How high-tech too ls proﬁle, police, and punish the poor, St. Martin’s
Press, 2018.
[158] M. L. Gray, S. Suri, Ghost work: How to stop Silicon Vall ey from building a new global underclass, Eamon
Dolan Books, 2019.
[159] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI Now (2019).
[160] S. Costanza-Chock, Design justice: Community-led pr actices to build the worlds we need, The MIT Press,
2020.
[161] H. Cramer, V. Evers, S. Ramlal, M. Van Someren, L. Rutle dge, N. Stash, L. Aroyo, B. Wielinga, The eﬀects of
transparency on trust in and acceptance of a content-based a rt recommender, User Modeling and User-adapted
interaction 18 (2008) 455–496.
[162] M. K. Lee, D. Kusbit, E. Metsky, L. Dabbish, Working wit h machines: The impact of algorithmic and data-
driven management on human workers, in: Proceedings of the 3 3rd annual ACM conference on human factors
in computing systems, 2015, pp. 1603–1612.
[163] S. Amershi, D. Weld, M. Vorvoreanu, A. Fourney, B. Nush i, P. Collisson, J. Suh, S. Iqbal, P. N. Bennett,
K. Inkpen, et al., Guidelines for human-AI interaction, in: Proceedings of the 2019 chi conference on human
factors in computing systems, 2019, pp. 1–13.
[164] J. Stoyanovich, B. Howe, H. Jagadish, Responsible dat a management, Proceedings of the VLDB Endowment
13 (12) (2020).
[165] M. Taddeo, L. Floridi, Regulate artiﬁcial intelligen ce to avert cyber arms race (2018).
24