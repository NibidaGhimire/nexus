1
Weakly-Supervised 3D Medical Image
Segmentation using Geometric Prior and
Contrastive Similarity
Hao Du, Qihua Dong, Yan Xu, Jing Liao
Abstract ‚ÄîMedical image segmentation is almost the most
important pre-processing procedure in computer-aided diagnosis
but is also a very challenging task due to the complex shapes
of segments and various artifacts caused by medical imaging,
(i.e., low-contrast tissues, and non-homogenous textures). In this
paper, we propose a simple yet effective segmentation framework
that incorporates the geometric prior and contrastive similarity
into the weakly-supervised segmentation framework in a loss-
based fashion. The proposed geometric prior built on point
cloud provides meticulous geometry to the weakly-supervised
segmentation proposal, which serves as better supervision than
the inherent property of the bounding-box annotation ( i.e.,
height and width). Furthermore, we propose the contrastive
similarity to encourage organ pixels to gather around in the
contrastive embedding space, which helps better distinguish low-
contrast tissues. The proposed contrastive embedding space can
make up for the poor representation of the conventionally-
used gray space. Extensive experiments are conducted to verify
the effectiveness and the robustness of the proposed weakly-
supervised segmentation framework. The proposed framework
are superior to state-of-the-art weakly-supervised methods on
the following publicly accessible datasets: LiTS 2017 Challenge,
KiTS 2021 Challenge and LPBA40. We also dissect our method
and evaluate the performance of each component.
Index Terms ‚ÄîWeakly-supervised Segmentation, Medical Im-
age Segmentation, Contrastive Similarity, Geometric Prior, Point
Cloud
I. I NTRODUCTION
SEGMENTATION is of fundamental importance for the
understanding and interpretation of medical images, as
it is essential for the diagnostic, treatment, and follow-up
rehabilitation of various diseases. This task has been widely
studied with the recent advent of deep convolutional neural
networks (CNNs) [1], [2]. Nevertheless, there exists the main
limitation that their methods require a large number of training
images with pixel-wise annotations. The extremely high cost
of collecting and annotating these training images largely
Hao Du, Qihua Dong and Jing Liao are with Department of
Computer Science, City University of Hong Kong, Hong Kong, China
(e-mail: haodu8-c@my.cityu.edu.hk,qihuadong2-c@my.cityu.edu.hk and
jingliao@cityu.edu.hk).
Yan Xu are with School of Biological Science and Medical Engineering,
Beihang University, Beijing, China (e-mail: xuyan04@gmail.com).
Hao Du and Qihua Dong contributed equally to this work. Cor-
responding authors: Jing Liao (jingliao@cityu.edu.hk) and Yan Xu
(xuyan04@gmail.com).
This work was supported by the HKSAR Innovation and Technology
Commission (ITC) under ITF Project MHP/109/19 and by the National
Natural Science Foundation in China under Grant 62022010, the 111 Project in
China under Grant B13003, the high performance computing (HPC) resources
at Beihang University.
Input Image
Bounding -Box
AnnotationTemplateGeometric
Prior
Contrastive
SimilarityPositive Queue
Negative Queue
Embedding
SpacePoint Cloud
Min. Chamfer 
Distance
‚Ä¶
‚Ä¶
EncoderDecoderFig. 1. The illustration of the proposed weakly-supervised segmen-
tation framework. As shown in the Ô¨Ågure, we propose geometric
prior and contrastive similarity for weakly-supervised segmentation.
The top row indicates the geometric prior of our method. We Ô¨Årst
convert the conventionally-used volume representation to point cloud
representation and register the template organ to the predicted organ.
Then we minimize their Chamfer Distance. The bottom row explains
the core idea of the proposed contrastive similarity. By dividing pixels
into positive and negative pixels, we encourage organ pixels to gather
around in the embedding space to better segment the low-contrast
organ.
hampers the performance and limits the scalability of deep
CNNs in the medical domain. A popular paradigm to alleviate
the need for pixel-wise annotations is the weakly-supervised
segmentation with bounding-box annotations [3]‚Äì[7]. They
employ bounding-box annotations to generate proposals,
which are fake labels and thereby mimic full supervision.
Nevertheless, despite the good performances achieved by
these works in certain practical scenarios, their applicability
might be limited for two reasons: 1) complex shapes : Some
organs have delicate structures , i.e., intra-kidney variabilities,
which are difÔ¨Åcult to be precisely segmented without pixel-
wise supervision; 2) imaging artifacts : as discussed in previous
works [8]‚Äì[10], various medical imaging artifacts caused by
technical or physical problems make low-contrast tissues and
non-homogenous textures hard to distinguish, especially in the
conventionally widely-used gray space. The complex shapes
and imaging artifacts largely limit the applicability of the
weakly-supervised segmentation models in many scenarios,
especially when segmenting complex structures.
To conquer the challenge of complex shapes we propose to
learn the geometric prior of the organ by a standard organ
template. Instead of using volume representation, we Ô¨Årst
leverage the gridding reverse [11] to convert the segmentationarXiv:2302.02125v1  [eess.IV]  4 Feb 20232
result from volume representation to point cloud representation
and then compare it with the template in the point cloud space.
The basic unit in point cloud representation is much more
Ô¨Åne-grained and Ô¨Çexible than the volume representation ( , i.e.,
Ô¨Çexible point v.s.uniform voxel grids), which helps better
describe delicate geometric structures. On the other hand,
unlike the conventionally-used gray space [8], we leverage
the contrastive learning [12] to encode the pixels to high-
dimensional embedding space and encourage pixels of the
same labels to gather around. This helps alleviate the imag-
ing artifacts for richer expressivity in the embedding space
compared to the gray space.
In this paper, we present a novel weakly-supervised seg-
mentation framework, which makes the earliest effort to
incorporate geometric prior and contrastive similarity. And
the framework is general as well that can be easily applied
to improve multiple weakly-supervised segmentation models
with bounding-box annotations, i.e., Ai+L [13], BoxInst [7].
By learning geometry prior from the given template and dis-
tinguishing low-contrast tissues by the contrastive similarity,
our method can generate high-quality results with bounding
box supervision only.
Our method consists of two major components. In the
geometric prior component, the shapes of proposals are con-
strained by a given template represented by point cloud. Both
the external boundaries and internal structures of the proposal
will be optimized by minimizing the distance according to a
given template. The second component is the contrastive sim-
ilarity, addressing the issues raised by medical imaging arti-
facts. By pre-training a contrastive head, we successfully learn
the difference between organ pixels and non-organ pixels.
This component can better distinguish low-contrast tissues and
non-homogenous texture than conventionally widely-used gray
space. Through extensive experiments, we demonstrate that
our method can generate a high-quality segments, along with
delicate internal details and accurate boundaries. We show
that our method outperforms other bounding-box weakly-
supervised methods [5], [7] under similar settings. We also
conduct extensive experiments to verify the effectiveness of
components in our method.
In summary, our major contributions are three folds.
We propose a simple yet effective weakly-supervised
segmentation framework with bounding-box annotations,
which can be easily applied to many weakly-supervised
segmentation models and improve their performances.
We propose the geometric prior in point cloud represen-
tation to better guide the learning of shapes, especially
for those organs with complex structures.
The proposed contrastive similarity makes up for the poor
representation of the conventional gray space and thus can
better distinguish tissues with medical imaging artifacts.
Our code and data will be made publicly available for further
research.
II. R ELATED WORK
IN this section, we Ô¨Årst review existing weakly-supervised
medical segmentation methods with bounding-box annota-
tions in both natural and medical image segmentation, then wediscuss recent works with geometric prior and Ô¨Ånally present
the trends in contrastive similarity.
A. Weakly-supervised medical semantic segmentation
Generally, methods in weakly-supervised segmentation are
classiÔ¨Åed into four categories by the type of their weak anno-
tations: scribbles [14], points [15], [16], image-level tags [17],
[18] and bounding-box annotations [5]. Scribbles and points
supervision at least label one scribble or point for each region,
and the annotated areas will be directly incorporated into the
calculation of segmentation loss. Wang et al. [19] propose to
leverage a random walker algorithm [20] to generate initial
proposals for the unlabeled regions and then supervise the
training of segmentation models by the initial segments. Qu
et al. [16] uses a similar training pipeline but a different
label generation method for label generation which combines
K-means clustering and V oronoi partition diagram. Xu et
al.[18] enrich the image-level labels to instance-level labels
by multiple instance learning (MIL) and segment images using
only volume-level labels.
Weakly-supervised segmentation with bounding box annota-
tions earns increasing interest in medical image segmentation
for its simplicity and low-annotation cost. We can deÔ¨Åne
the bounding boxes with two corner coordinates that are
easy to store in real scenarios. In addition, the bounding
box annotations are location-aware so that they provide the
spatial relationship of the target object, which is a popular
direction in recent researches [21]‚Äì[24]. In the early stages,
researchers [22], [25] propose to consider pixels within the
bounding box as foreground pixels and train the segmentation
framework by these noisy labels. Despite the good perfor-
mance achieved by such a scheme, they may accumulate
errors during the alternative generation process. Most recently,
researchers [7] tried to directly generate the segmentation
result instead of the error-prone alternative way. Generally,
they build a mask head to produce the segmentation result, and
the bounding box annotation is employed to train this mask
head. In this work, we follow this segmentation scheme where
the segmentation result is directly generated by the mask
head. Furthermore, to address the fore-mentioned complex
shapes and imaging artifacts we propose geometric prior and
contrastive similarity, respectively.
B. Geometric prior
Different from natural images, there exists obvious anatom-
ical prior ( , i.e., atlas prior) in medical images, speciÔ¨Åcally in
organs of human bodies ( i.e., shape and position). Existing
works incorporating such anatomical prior mostly fall into
two categories: loss-based methods and graph-based meth-
ods. Generally, graph-based methods [26]‚Äì[30] leverage the
probability maps of occurring anatomy chances to construct
graph models and estimate the foreground probability from
the input image gray space. An appearance model of basic
forms is employed to improve the segmentation accuracy [29].
Gao et al [30] proposes to apply an appearance ConvNet
to characterize the foreground. Despite the high accuracy
achieved by these methods, the graphical models bring heavy3
and expensive computational burdens to the segmentation
framework, which makes it infeasible in certain scenarios.
Another popular direction in combining the prior with
the segmentation framework is loss-based methods. Re-
searchers [31]‚Äì[36] mostly minimize the distance between the
segmentation network output and the pre-deÔ¨Åned anatomical
priors. Several works [31], [32] pose regularization terms on
the training objective ( , i.e., anatomical adjacency or boundary
conditions). Distance between predictions and atlas prior are
also calculated in latent feature space [35], [36].
Compared with graph-based methods, loss-based methods
provide a versatile fashion to incorporate anatomical priors
with a wider range of scales while maintaining the compu-
tational efÔ¨Åciency of the segmentation framework. However,
previous loss-based methods fail to address the aforementioned
two issues for two aspects: 1) Previous works generally slice
the volume into 2D or 3D patches, which are then processed
sequentially to save memory cost. However, such a partitioning
method breaks the global geometric relationships, resulting in
inferior segmentation performance. Different from them, we
learn the geometric prior in 3D embedding space to capture the
overall geometry and proposed completeness head to ensure
the shape completeness of the proposal. 2) Unlike previous
works using a volume representation, we leverage the Gridding
Reverse [11] to convert the volume representation to point
cloud representation. Compared to the volume representation
constrained by uniform voxel grids, point cloud without grids
is more Ô¨Çexible in representing delicate structures.
C. Contrastive learning
Contrastive Learning aims to attract the positive and reverse
the negative by dividing the feature space into positive and
negative data pairs. As for semantic segmentation, it has been
mainly used as pre-training [37]‚Äì[39]. Van et al. [40] apply
it to distinguish features from various salient masks, showing
its superiority in unsupervised set-ups. Wang et al. [41] have
shown advantages of contrastive learning by learning in both
pixel and region levels. Some researchers leverage contrastive
learning to address the time-consuming pixel-wise labeling in
medical image segmentation. Chaitanya et al. [12] propose
a two-stage self-supervised contrastive learning framework to
learn the feature matching both in global and local mechanisms
from unlabeled data in the pre-training stage. Hu et al. [42]
proposes a semi-supervised scheme to learn self-supervised
global contrast and supervised local contrast. In our work, we
observe that the conventionally-used gray space is not enough
to distinguish positive and negative pixels ( , i.e., organ pixels
and non-organ pixels), especially in Magnetic Resonance
Imaging. We thus leverage contrastive learning to calculate the
contrastive similarity between pixels. By encoding pixels to
high-dimensional features and encouraging pixels of the same
label to gather around in the embedding space, it enhances
the discriminability and thus alleviates the poor performance
of the gray space in handling medical imaging artifacts, i.e.,
artifacts in ultrasound imaging and similar surrounding tissues.III. M ETHOD
A. Overall framework
Given an input image I2RSHW(Sindicates the slice
number,Hindicates height and Wrepresents width) and
its corresponding bounding-box annotation B16(constrained
by its upper left coordinates and bottom right coordinates),
our weakly-supervised framework F()obtains the pixel-wise
segmentation mask M=F(I)and the training goal is to
minimize the loss function Lframe:
min
FLframe(I;B;F) (1)
Pipeline Following nnUNet [2], we randomly sample an
input patch p2RS0H0W0from the original input image
and encode the patches by a ConvNet encoder E. Similar
to [1], we adopt a multi-layer ConvNet as the decoder Gto
obtain the feature maps P2RS0H0W0, where the decoder
shares the same layer number as the encoder. As shown in
Fig. 2, the proposed geometric prior and contrastive learning
are incorporated in the training of the mask head to address the
aforementioned complex shapes and imaging artifacts issues.
The training loss Lframe is composed of two components: Lori
andLmask
Lframe=Lori+Lmask (2)
whereLoriindicates the original training loss of a standard
weakly-supervised framework ( i.e.,Lfcos in BoxInst [7]) and
Lmask stands for the training loss of the mask head. In the
following paragraphs, we mainly discuss the training of the
mask head. The training of the mask head can be formulated
as Eq.(3).
Lmask=Lgeo+Lcons (3)
The mask head produce binary segmentation masks which is
further optimized by our proposed geometric prior Lgeoand
contrastive similarity Lcons. More speciÔ¨Åcally, for geometric
prior loss, we build a completeness head to predict the com-
pleteness score for every proposal, indicating the conditional
probability that the object is complete inside the input. Each
complete proposal is converted into point cloud and registered
with the point cloud of the template organ. The Chamfer
Distance loss is applied to minimize their distance. In the
aspect of the contrastive similarity loss, we build a contrastive
head to obtain the contrastive similarity by the feature maps
that the contrastive head assigns positive and negative labels to
each position in the feature maps. In the following paragraphs,
we Ô¨Årst introduce the geometric prior. Then we elaborate on
the technical details of the proposed contrastive similarity.
B. Geometric Prior
The proposed geometric prior is applied in 3D point cloud
space for two reasons: 1) we observe that 2D slices cannot
well-preserve the geometric continuity of 3D organs. Thus,
we learn the geometric shape of the organ in 3D embed-
ding space. 2) The conventionally-used volume representation
cannot handle the segmentation of meticulous structures. The
expressivity of the volume representation is largely limited
by the uniform voxel grids. Instead, we leverage gridding
reverse [11] to process the segmentation of complex shapes
in point cloud embedding space.4
As shown in Fig. 2, we propose the geometric prior to
better weakly supervise the training of the mask head. The
geometric prior refers to the template organ‚Äôs boundary shape
and internal distribution. More speciÔ¨Åcally, we introduce grid-
ding reverse [11] to building a bridge between the volume
representation and point cloud representation conversion. This
helps us to get rid of the representation constraint in the
volume representation. After converting both template organ
Tand proposal Sinto the point cloud representation, we then
register the template organ to the proposal by a widely-used
ICP registration tool [43]. We Ô¨Ånally minimize the geometric
prior loss of the Chamfer Distance between the template organ
and the proposal in the point cloud embedding space. Below
we Ô¨Årst introduce the conversion and registration of point
cloud and then the deÔ¨Ånition of geometric prior loss.
Conversion & Registration To utilize the rich expressivity
in point cloud representation, we introduce the gridding re-
verse [11] to help the transition between the volume represen-
tation and the point cloud representation. For each voxel grid,
the gridding reverse calculates the weighted sum of the eight
vertices of the corresponding grid and assigns the weighted
sum to coordinates of a new point. Unlike uniform voxel
grids, the high Ô¨Çexibility of points‚Äô coordinates enable the
point cloud representation to describe meticulous and complex
architectures. This helps better learn the difÔ¨Åcult intra-organ
variabilities in the weakly-supervised segmentation. Further-
more, we propose the sparse registration, which is applied
before calculating the Chamfer Distance between SandT.
Tiny rotation of the template greatly impacts the calculation
of the Chamfer Distance, especially when the object structure
is much more complex. We thus conduct registration [43]
between the general shape of the proposal and the template.
SpeciÔ¨Åcally, we sample 20% points uniformly across the
interval for the template and the proposal respectively, and
then calculate the transform matrix between them by the ICP
registration tool [43]. The template point cloud is registered
according to the transform matrix.
Loss The geometric prior is then applied in the loss function
of the mask head training that we optimize the Chamfer
Distance between the proposal Sand the registered template
organ T. This can be formulated as follows:
Lgeo=1
jSjX
x2Smin
y2Tjjx yjj2+1
jTjX
y2Tmin
x2Sjjy xjj2:(4)
SpeciÔ¨Åcally, the mask head produces binary segmentation
masks for each proposal. The proposal is a probabilistic
segmentation mask consisting of the segmented instances.
This segmentation mask is further processed by the Gumbel-
Softmax [44] to obtain the binary voxel. Locations of low
probability are assigned to 0and vice versa. We then adopt
the gridding reverse [11] to obtain the point cloud proposal
Sfrom the binary voxel. Finally, we calculate the Chamfer
Distance between SandTas the geometric prior loss.
C. Contrastive Similarity
Gray space performs poorly for the artifacts in medical
imaging and similar surrounding tissues. It is not enough todistinguish between positive and negative pixels ( , i.e., organ
pixels and non-organ pixels). We thus leverage contrastive
learning to calculate the contrastive similarity between pixels.
Encoding pixels to high-dimensional features and encouraging
pixels of the same label to gather around in the embedding
space helps to increase the distinguishability.
To calculate the proposed contrastive similarity, we Ô¨Årst
build a ConvNet contrastive head after the decoder. Following
previous contrastive learning works [42], [45], we build a two-
layer point-wise convolution h()to extract distinct represen-
tations from feature maps P. More speciÔ¨Åcally, we Ô¨Årst pre-
train the proposed contrastive head in a coarse-to-Ô¨Åne fashion
where only bounding box annotations are included in the
pre-training stage. We encode pixels into embedding features
C=h(P)and encourage pixels of the same label to gather
around in the embedding space. The contrastive similarity
between two pixels is deÔ¨Åned as the distance in the embedding
space. To calculate the contrastive similarity loss for the
whole image, an undirected graph is constructed where the
vertices correspond to the pixels and edges are links between
neighboring pixels. The contrastive similarity associated with
each edge is then summarized for the calculation of the overall
contrastive similarity loss. Below we Ô¨Årst introduce the pre-
training of the contrastive head and then the deÔ¨Ånition of
contrastive similarity loss.
Pre-training The pre-training of the contrastive head is
conducted in a weakly-supervised fashion that only bounding
box annotations are included in the pre-training stage. To be
more speciÔ¨Åc, there are two sub-stages in the pre-training
stage: coarse and reÔ¨Åne . In the coarse stage, we Ô¨Årst take
pixels within the bounding box as positive labels and pixels
outside the bounding box as negative labels. Then we train the
contrastive head by such labeling. However, the performance
of the contrastive head is largely limited for the noisy labels.
Thus, we propose the reÔ¨Åne stage to further improve the
performance of the contrastive head. In the reÔ¨Åne stage, we
Ô¨Årst take random Knegative pixels as referring pixels . And for
each pixel within the bounding box, we calculate the distance
Dbetween all K referring pixels:
Du;v;z =KX
i=11ffCu;v;zCigg (5)
whereCu;v;z2RSHWindicates the feature at the (u;v;z )
of the embedding features, 1stands for 1 if the distance is
greater than and 0 if less, and is the threshold to decide
whether pixels are positive or negative. If Du;v;z is greater than
K=2, the pixel at location (u;v;z )is considered positive, and
vice versa. Then, we train the contrastive head using the same
training loss as in the coarse stage, which is formulated as:
loss= 1
j
jX
(u;v;z )2
1
jP(u;v;z )j
logP
(up;vp;zp)2P(u;v;z )exp(Cu;v;zCup;vp;zp=)
P
(un;vn;zn)2N(u;v;z )exp(Cu;v;zCun;pn;zn=)(6)
whereCu;v;z2RSHWindicates the feature at the (u;v;z )
of the feature map, and 
stands for all points inside input.5
P(u;v;z )denotes the set of points with the same label as the
pixel at (u;v;z )andNdenotes the set of points with different
labels.is the temperature constant.
Loss Considering an undirected graph G= (V;E)built on
the input image I, whereVcorresponds pixels and Eindicates
edges between neighboring pixels, the predicted segmentation
mask can be viewed as the probability of pixel (u;v;z )being
foreground. Then the probability of pixel (u1;v1;z1)and pixel
(u2;v2;z2)being the same label is:
Prob (ye= 1) = ~Mu1;v1;z1~Mu2;v2;z2
+ (1 ~Mu1;v1;z1)(1 ~Mu2;v2;z2);
(7)
where ~Mindicates the foreground probability mask and ye
represents the label of the edge.
Thus, we can deÔ¨Åne an indicator on each edge to indicate
whether they belongs to the same label. If the contrastive
similarity between two neighboring pixels is above the pre-
deÔ¨Åned threshold , the indicator on the edge linking them
is assigned to 1, and 0vice versa. We discard the edges
with 0and further summarize the contrastive similarity loss
of positive edges, which can be formulated as:
Lcons= 1
NX
e2Ein1fCestartCeendglogProb (ye= 1):(8)
This serves as the contrastive similarity loss of the whole
image.
IV. E XPERIMENTS
A. Datasets
LiTS: The public liver LiTS [49] dataset comprises 201
CT scans from various CT scanners and devices. The
resolution of images in this dataset is from 0:56mm to
1:0mm in axial and 0.45mm to 6.0mm in zdirection.
Slices inzrange from 42 to 1026. We split 131 cases
into training and evaluation sets by a ratio of 4:1.
KiTS21: The publicly accessible KiTS21 [50] dataset
consists of 300 cases during the period from 2010 to
2020. Each CT scan in the dataset is annotated by three
expert annotators for the following semantic classes:
Kidney, Tumor, and Cyst. We split the provided CT scans
into training and validation sets by a ratio of 4:1.
LPBA40: This dataset consists of 40 T1-weighted image
volumes from randomly selected cases (among 40 scans:
20 males, 20 females, and 29.2 6.3 years). The scans
were acquired with a spatial resolution of 0.86 1.5
0.86 mm3. Here, we conduct experiments on the subset
of the hippocampus in LPBA40. .
B. Implementation details and evaluation metrics
The proposed framework with geometric prior and con-
trastive similarity can be easily incorporated with any weakly-
supervised segmentation models with bounding box annota-
tions. To verify the robustness of our method, we conduct ex-
periments with two models: Ai+L [13] and BoxInst [7]. Here,
we take the latest BoxInst [7] as our baseline, and experiments
are conducted based on this model unless otherwise speciÔ¨Åed.The training setting is mostly based on BoxInst‚Äôs training
settings: The basic learning rate is 0.01 with weight decay 1e-
4, and a MultiStepLR scheduler with warmup is adopted. Our
framework is trained on GeForce RTX 3090 GPU. During the
inference stage, we set the threshold of the completeness head
to 0.6 and the threshold of the class head to 0.5 empirically.
We randomly select a training sample as the template for each
speciÔ¨Åc dataset unless otherwise speciÔ¨Åed. We evaluate our
results by two widely adopted metrics: the Dice score (DSC)
in percentages and the Hausdorff Distance (95%). 1) DSC
score is calculated as the overlap area of two masks divided
by their summation. A higher DSC score corresponds to better
overlap with GT. 2) The Hausdorff Distance mainly measures
the boundary distance between the segmentation result and the
pixel-wise segmentation masks. Better segmentation results are
of a smaller value than inferior results.
Pre-processing & Post-processing Following nnUNet [2],
the pre-processing includes downsampling, patching, and data
augmentation. Here, we downsample the data to reduce mem-
ory use and ensure the existence of complete instances inside
one patch. Then in the post-processing stage, the pipeline con-
tains resampling, patching, and patch-NMS (Non-Maximum
Suppression between patches). We Ô¨Årst adjust the spacing and
patch the data to the same size as training, with a certain step
size. Then we predict a segmentation mask for each patch and
return all the predictions to the original space. Since there
are overlapping areas between patches, we use patch-NMS in
these areas to eliminate the duplicates.
Bounding box annotation We conduct experiments on
LiTS, KiTS, and LPBA40 datasets. All these three datasets
have pixel-wise segmentation annotations. We utilize the cor-
responding pixel-wise annotations to obtain the correspond-
ing bounding-box annotations. In the training stage, only
bounding-box annotations are included. During the inference
stage, we evaluate the performance by pixel-wise annotations.
C. Results
Quantitative Results. We report the quantitative results
of three datasets (LiTS17, KiTS21, and LPBA40) in Tab. I.
Our method aims to integrate the geometric prior and con-
trastive similarity to give better supervision in the training
of the weakly-supervised segmentation framework. Compared
with BoxInst [7], which is the baseline of our method, the
segmentation performance of our method is largely improved
over all three datasets. This is because the geometric prior
can supervise the learning of both outer shape and the inner
structure, and the contrastive similarity better distinguishes
organ and non-organ pixels in the embedding space. Simi-
larly, ours outperforms the most recent two weakly-supervised
methods [42], [47] supervised by bounding-box annotations,
which are based on the Multiple Instance Learning ( i.e., MIL).
The reason behind this is that MIL mainly focuses on the
boundary regression of the proposal, but additionally, we learn
the internal details of organs from the geometric prior. We also
evaluate the upper bound of our method in which we supervise
the framework training with ground truth.
Qualitative Results. We present the qualitative compari-
son with two state-of-the-art weakly-supervised segmentation6
...
Mask 
Head
Encoder 
Feature 
MapsInputs
...
Contrastive
HeadContrastive Similarity
Feature 
MapsLconsSegmentation
Masks
Positive 
Queue
Negative 
QueueLgeo
Embedding
Features
ProposalTemplate
...
...Positive 
Queue
Negative 
Queue
Embedding
Features
Decoder 
Geometric Prior
Organ
TemplateSparse 
RegistrationGridding
Reverse
Chamfer 
Distance Loss
Segmentation 
Masks
Registered
Template
Point Cloud
Proposal
Pre-training StageInference Stage
...
Completeness 
Head
...
Contrastive 
Head
...
If Complete
Segmentation 
MasksForeground
Prob.0.930.91 0.79
0.21 0.07 0.110.490.89 ...
...Maximize
Fig. 2. The overall framework of our weakly-supervised segmentation framework with geometric prior and contrastive similarity. The training of our framework
is organized as follows: We Ô¨Årst pre-process the input images and obtain the embedding feature maps. Then we jointly supervise the mask head‚Äôs training by
the proposed geometric prior and contrastive similarity. In the aspect of geometric prior, we Ô¨Årst convert the segmentation result to point cloud if complete.
Then we calculate the Chamfer Distance between the result and the registered template. In the aspect of contrastive similarity, we Ô¨Årst pre-train a contrastive
head using only bounding box annotations. Then we minimize the distance between pixels of the same labels by the pre-trained contrastive head.
TABLE I
PERFORMANCE COMPARISON WITH OTHER METHODS . W E EVALUATE THE PERFORMANCE USING TWO METRICS : DICESCORE AND HD95. yWE OBTAIN
THEIR DICE SCORE BY THEIR OPEN SOURCE CODE .z:WE REBUILD THE 3D FRAMEWORK BASED ON ITS 2D OPEN SOURCE CODE .Ours w/o geometric
INDICATES WE REMOVE THE GEOMETRIC PRIOR COMPONENT .Ours w/o contrastive INDICATES WE REMOVE THE CONTRASTIVE SIMILARITY COMPONENT .
Method BackboneLiTS17 KITS21 LPBA40
"DSC(%) #HD95 "DSC(%) #HD95 "DSC(%) #HD95
Fully Supervised UNet [1] 95.5 5.3 96.0 3.2 83.7 2.1
DeepCut (2016) [25] - 37.1 15.2 36.2 14.7 - -
SDI (2017) [23] VGG-16 [46] 49.2 11.7 - - 38.7 9.0
MIL (2020) [47]yENet [48] 69.4 9.4 72.3 8.7 - -
GMIL (2021) [42]yENet [48] 71.1 8.8 71.7 5.9 58.2 4.7
BoxInst (2021) [7]zUNet [1] 47.1 11.3 48.4 11.6 37.9 8.9
Ours w/o geometric UNet [1] 52.9 10.9 54.2 10.7 44.9 7.1
Ours w/o contrastive UNet [1] 69.7 9.4 68.5 9.1 57.3 4.9
Ours UNet [1] 79.8 8.7 80.2 5.3 65.4 4.2
methods in Fig 3. Compared with BoxInst [7], which is the
baseline of our framework, we well preserve the proposal‚Äôs in-
ner structure detail and outer shape. By comparing Column.#4
and Column.#7, the segmentation accuracy is largely improved
compared to the baseline in Fig. 3. This is because the
proposed geometric prior and the contrastive similarity help
better segment organs. Additionally, the quality of our result
is much better than other state-of-the-art methods, especially in
terms of internal details. Their methods are based on Multiple
Instance Learning ( i.e., MIL, GMIL), which mainly focuses
on the outer shape of the proposal ignoring the hollows insidethe organs. In contrast to their methods, we learn the inner
structure and geometric details from the given template organ
along with the outer shape that boosts the training of the mask
head.
D. Ablation Study
In this subsection, we Ô¨Årst present the component-wise
analysis of our framework and then discuss the effectiveness
of the proposed geometric prior and contrastive similarity.
1) Robustness: In this subsection, we mainly discuss the
robustness of the proposed weakly-supervised segmentation7
TABLE II
ROBUSTNESS ANALYSIS . W E MEASURE THE CORRESPONDING
DICE SCORE ON THE KITS21 DATASET . W E IMPLEMENT THE 3D
VERSION OF BOXINST [7] BASED ON ITS 2D VERSION PUBLIC
CODE .
# Backbone ModelWithout With
Our Loss Our Loss
1 ResNet [51] BoxInst 3D [7] 47.8 77.1
2 UNet [1] BoxInst 3D [7] 49.1 80.2
3 ResNet [51] Ai+L [13] 67.2 72.1
framework. Our framework is built on BoxInst [7], which
is composed of Backbone and FPN. We analyze the im-
pact of the backbone on the weakly-supervised segmentation
performance. As shown in Tab. II, our method is robust to
other backbones ( i.e., ResNet [51] and U-Net [1]). Further-
more, we verify the generality of the proposed framework by
extending to other weakly-supervised segmentation models.
Following Chu et al. [13], we use the Ai+L [13] model with
a ResNet50 [51] backbone that is widely used in both non-
medical [52] and medical [53] segmentation tasks. We con-
catenate the geometric prior branch and contrastive similarity
branch after the decoder. As shown in Tab. II, the baseline
model obtains 67.2% in the KiTS dataset. The segmentation
performance is largely improved by 4.9% after adding the
proposed geometric prior and contrastive similarity losses.
2) Geometric prior: In the proposed geometric prior, we
minimize the Chamfer Distance between the template organ
and the proposal in the point cloud embedding space. The
point cloud representation is much more Ô¨Çexible than the vol-
ume representation. This is because the volume representation
is constrained by the uniform voxel grids. The representative
point of each grid is Ô¨Åxed to the center of each corresponding
grid. In contrast, there is no such constraint in the point
cloud representation. The coordinates of points are much
more precise and Ô¨Çexible elastic ( i.e.,f0.17, 0.18, ...gv.s.
f1.00, 2.00, ...g). This helps the point cloud representation
to better describe delicate and complex structures than the
conventionally-used volume representation. Thus, to validate
this point, we conduct an ablation of registration and optimiza-
tion with the volume representation. More speciÔ¨Åcally, we Ô¨Årst
register the organ by the widely-used SimpleITK [54]. Here,
the registration settings are set as follows: Mean Square metric,
random sampling strategy with a percentage of 0.01, shrink
factors [4;2;1], smoothing sigmas [2;1;0], and 100 iterations.
After the registration, we employ the Dice loss to optimize the
distance between the segmentation result and the geometric
prior. As shown in Fig. 4, the result shows that the point
cloud representation can handle more complex architectures
and precise geometric shapes, by improving 4.3% in the Dice
Score CoefÔ¨Åcient.
Internal details We further analyze the impact of the tem-
plate from the perspective of outer shape and inner structures,
respectively. The experiments in this subsection are organized
as follows: We Ô¨Årst fulÔ¨Åll the inner structure of the template
and only retain the outer shape. Then we train the mask head
with the deformed template. Normally, there exists intra-organvariabilities, i.e., as shown in Fig. 5, especially meticulous
architectures inside the kidney. And the segmentation per-
formance drastically degrades (3.9%) for the lack of internal
prior.
Analysis of the completeness head As discussed in
Sec. III-A, following nnUNet [2], for an input image, we
Ô¨Årst partition it into patches and sequentially process these
patches. However, during the partitioning, organs may not be
complete, which may cause the failure of the learning from
the template. Thus, to ensure the success of learning from
the template, the proposed completeness head detects whether
the proposal in the sampled patch is complete. Only complete
proposals are further processed. As shown in Tab. III, if we
remove the completeness head, the performance degrades by
1.1% compared with our framework.
3) Architectures of contrastive head: We conduct exper-
iments on the design of the proposed contrastive head in
this subsection. Following previous works [42], [45], the
contrastive head is composed of a two-layer point-wise con-
volution that helps distinguish organ tissues from non-organ
tissues. To further analyze the structure of the contrastive
head, we conduct ablation experiments on the dimension of
the embedding space. As shown in Tab. IV, we evaluate both
the performance and the efÔ¨Åciency of different embedding
dimension that by considering the trade-off between efÔ¨Åciency
and performance, we empirically set the embedding dimension
to 32.
Contrastive similarity We propose the contrastive simi-
larity in the embedding space. To compare the embedding
space with the commonly used gray space, we select the
following two similarity metrics: MSE andSSIM [55]. Both
MSE andSSIM are calculated on gray values since medical
images are in grayscale. As shown in Tab. IV, the MSE
metric fails to distinguish low-contrast tissues, and the SSIM
TABLE III
ANALYSIS OF GEOMETRIC PRIOR .yINDICATES THE INNER STRUCTURE OF
THE TEMPLATE .
# Ablation SettingDSC on
KiTS
1 w/o Completeness Head 79.1
2 w/o Internal Detailsy76.3
3 Baseline 80.2
TABLE IV
ANALYSIS OF THE CONTRASTIVE HEAD .y: C STANDS FOR THE
CONTRASTIVE SIMILARITY , SINDICATES THE SSIM SIMILARITY AND G
STANDS FOR THE GRAYSCALE SIMILARITY . THE INFERENCE SPEED
INDICATES THE TOTAL TIME OF 1000 RUNS .
# Component SettingEmbedding Embedding Inference DSC (%)
Dimension SpaceySpeed on KiTS
1
Contrastive Head8 C 3s 78.4
2 16 C 5s 79.1
3 32 C 8s 80.2
4 64 C 15s 80.4
5
Similarity Metrics32 C 8s 80.2
6 32 G 8s 73.4
7 32 S 12s 47.28
metric is inferior to the contrastive similarity in terms of both
segmentation performance and evaluation efÔ¨Åciency.
V. C ONCLUSION
IN this work, we propose a novel weakly-supervised seg-
mentation framework with bounding-box annotations. We
introduce the geometric prior and the contrastive similarity
to address the challenges of complex shapes and imaging
artifacts, enhancing the practicability and robustness of the
weakly-supervised segmentation framework. The geometric
prior enables the learning of delicate and complex structures,
and the contrastive similarity helps better distinguish organ
pixels from non-organ pixels.
Our framework is general, which can be easily applied
to many weakly-supervised segmentation models and thus
improve their performances. It will probably serve as a base
for possible future studies on weakly-supervised segmentation,
especially for medical image segmentation on organs that are
with a speciÔ¨Åc shape. Extensive experiments are conducted to
verify the effectiveness and the superiority of our proposed
geometric prior and contrastive similarity.
REFERENCES
[1] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convolutional networks
for biomedical image segmentation,‚Äù in International Conference on
Medical image computing and computer-assisted intervention . Springer,
2015, pp. 234‚Äì241.
[2] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,
‚Äúnnu-net: a self-conÔ¨Åguring method for deep learning-based biomedical
image segmentation,‚Äù Nature methods , vol. 18, no. 2, pp. 203‚Äì211, 2021.
[3] D. Pathak, P. Krahenbuhl, and T. Darrell, ‚ÄúConstrained convolutional
neural networks for weakly supervised segmentation,‚Äù in Proceedings of
the IEEE international conference on computer vision , 2015, pp. 1796‚Äì
1804.
[4] Z. Jia, X. Huang, I. Eric, C. Chang, and Y . Xu, ‚ÄúConstrained deep weak
supervision for histopathology image segmentation,‚Äù IEEE transactions
on medical imaging , vol. 36, no. 11, pp. 2376‚Äì2388, 2017.
[5] H. Kervadec, J. Dolz, M. Tang, E. Granger, Y . Boykov, and I. B. Ayed,
‚ÄúConstrained-cnn losses for weakly supervised segmentation,‚Äù Medical
image analysis , vol. 54, pp. 88‚Äì99, 2019.
[6] M. Bateson, H. Kervadec, J. Dolz, H. Lombaert, and I. B. Ayed,
‚ÄúConstrained domain adaptation for segmentation,‚Äù in International
Conference on Medical Image Computing and Computer-Assisted In-
tervention . Springer, 2019, pp. 326‚Äì334.
[7] Z. Tian, C. Shen, X. Wang, and H. Chen, ‚ÄúBoxinst: High-performance
instance segmentation with box annotations,‚Äù in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2021, pp. 5443‚Äì5452.
[8] T. Budrys, V . Veikutis, S. Lukosevicius, R. Gleizniene, E. Monastyreck-
iene, and I. Kulakiene, ‚ÄúArtifacts in magnetic resonance imaging:
how it can really affect diagnostic image quality and confuse clinical
diagnosis?‚Äù Journal of Vibroengineering , vol. 20, no. 2, pp. 1202‚Äì1213,
2018.
[9] F. E. Boas, D. Fleischmann et al. , ‚ÄúCt artifacts: causes and reduction
techniques,‚Äù Imaging Med , vol. 4, no. 2, pp. 229‚Äì240, 2012.
[10] S. N. Sarkar, D. B. Hackney, R. L. Greenman, B. A. Vachha, E. A. John-
son, S. Nagle, and G. Moonis, ‚ÄúA subjective and objective comparison
of tissue contrast and imaging artifacts present in routine spin echoes
and in iterative decomposition of asymmetric spin echoes for soft tissue
neck mri,‚Äù European journal of radiology , vol. 102, pp. 202‚Äì207, 2018.
[11] H. Xie, H. Yao, S. Zhou, J. Mao, S. Zhang, and W. Sun, ‚ÄúGrnet: Grid-
ding residual network for dense point cloud completion,‚Äù in European
Conference on Computer Vision . Springer, 2020, pp. 365‚Äì381.
[12] K. Chaitanya, E. Erdil, N. Karani, and E. Konukoglu, ‚ÄúContrastive
learning of global and local features for medical image segmentation
with limited annotations,‚Äù Advances in Neural Information Processing
Systems , vol. 33, pp. 12 546‚Äì12 558, 2020.[13] T. Chu, X. Li, H. V . V o, R. M. Summers, and E. Sizikova, ‚ÄúImproving
weakly supervised lesion segmentation using multi-task learning,‚Äù in
Medical Imaging with Deep Learning . PMLR, 2021, pp. 60‚Äì73.
[14] D. Lin, J. Dai, J. Jia, K. He, and J. Sun, ‚ÄúScribblesup: Scribble-
supervised convolutional networks for semantic segmentation,‚Äù in Pro-
ceedings of the IEEE conference on computer vision and pattern
recognition , 2016, pp. 3159‚Äì3167.
[15] A. Bearman, O. Russakovsky, V . Ferrari, and L. Fei-Fei, ‚ÄúWhat‚Äôs the
point: Semantic segmentation with point supervision,‚Äù in European
conference on computer vision . Springer, 2016, pp. 549‚Äì565.
[16] H. Qu et al. , ‚ÄúWeakly Supervised Deep Nuclei Segmentation Using
Partial Points Annotation in Histopathology Images,‚Äù IEEE Transactions
on Medical Imaging , pp. 1‚Äì1, 2020.
[17] G. Patel and J. Dolz, ‚ÄúWeakly supervised segmentation with cross-
modality equivariant constraints.‚Äù Medical Image Analysis , p. 102374,
2022.
[18] G. Xu et al. , ‚ÄúCAMEL: A Weakly Supervised Learning Framework for
Histopathology Image Segmentation,‚Äù in 2019 IEEE/CVF International
Conference on Computer Vision (ICCV) , 2019, pp. 10 681‚Äì10 690.
[19] X. Wang et al. , ‚ÄúWeakly Supervised Deep Learning for Whole Slide
Lung Cancer Image Analysis,‚Äù IEEE Transactions on Cybernetics , 2019.
[20] L. Grady, ‚ÄúRandom Walks for Image Segmentation,‚Äù IEEE Transactions
on Pattern Analysis and Machine Intelligence , vol. 28, pp. 1768‚Äì1783,
2006.
[21] J. Dai, K. He, and J. Sun, ‚ÄúBoxsup: Exploiting bounding boxes to super-
vise convolutional networks for semantic segmentation,‚Äù in Proceedings
of the IEEE international conference on computer vision , 2015, pp.
1635‚Äì1643.
[22] G. Papandreou, L.-C. Chen, K. P. Murphy, and A. L. Yuille, ‚ÄúWeakly-
and semi-supervised learning of a deep convolutional network for
semantic image segmentation,‚Äù in Proceedings of the IEEE international
conference on computer vision , 2015, pp. 1742‚Äì1750.
[23] A. Khoreva, R. Benenson, J. Hosang, M. Hein, and B. Schiele, ‚ÄúSimple
does it: Weakly supervised instance and semantic segmentation,‚Äù in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2017, pp. 876‚Äì885.
[24] M. Pu, Y . Huang, Q. Guan, and Q. Zou, ‚ÄúGraphnet: Learning image
pseudo annotations for weakly-supervised semantic segmentation,‚Äù in
Proceedings of the 26th ACM international conference on Multimedia ,
2018, pp. 483‚Äì491.
[25] M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach,
W. Bai, M. Damodaram, M. A. Rutherford, J. V . Hajnal, B. Kainz et al. ,
‚ÄúDeepcut: Object segmentation from bounding box annotations using
convolutional neural networks,‚Äù IEEE transactions on medical imaging ,
vol. 36, no. 2, pp. 674‚Äì683, 2016.
[26] B. Patenaude, S. M. Smith, D. N. Kennedy, and M. Jenkinson, ‚ÄúA
bayesian model of shape and appearance for subcortical brain segmen-
tation,‚Äù Neuroimage , vol. 56, no. 3, pp. 907‚Äì922, 2011.
[27] M. R. Sabuncu, B. T. Yeo, K. Van Leemput, B. Fischl, and P. Golland,
‚ÄúA generative model for image segmentation based on label fusion,‚Äù
IEEE transactions on medical imaging , vol. 29, no. 10, pp. 1714‚Äì1729,
2010.
[28] B. Fischl, D. H. Salat, E. Busa, M. Albert, M. Dieterich, C. Haselgrove,
A. Van Der Kouwe, R. Killiany, D. Kennedy, S. Klaveness et al. , ‚ÄúWhole
brain segmentation: automated labeling of neuroanatomical structures in
the human brain,‚Äù Neuron , vol. 33, no. 3, pp. 341‚Äì355, 2002.
[29] J. E. Iglesias and M. R. Sabuncu, ‚ÄúMulti-atlas segmentation of biomed-
ical images: a survey,‚Äù Medical image analysis , vol. 24, no. 1, pp. 205‚Äì
219, 2015.
[30] M. Gao, Z. Xu, L. Lu, A. Wu, I. Nogues, R. M. Summers, and
D. J. Mollura, ‚ÄúSegmentation label propagation using deep convolutional
neural networks and dense conditional random Ô¨Åeld,‚Äù in 2016 IEEE 13th
International Symposium on Biomedical Imaging (ISBI) . IEEE, 2016,
pp. 1265‚Äì1268.
[31] P.-A. Ganaye, M. Sdika, and H. Benoit-Cattin, ‚ÄúSemi-supervised learn-
ing for segmentation under semantic constraint,‚Äù in International Confer-
ence on Medical Image Computing and Computer-Assisted Intervention .
Springer, 2018, pp. 595‚Äì602.
[32] A. BenTaieb and G. Hamarneh, ‚ÄúTopology aware fully convolutional net-
works for histology gland segmentation,‚Äù in International conference on
medical image computing and computer-assisted intervention . Springer,
2016, pp. 460‚Äì468.
[33] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, and P.-A. Heng, ‚ÄúDcan: Deep
contour-aware networks for object instance segmentation from histology
images,‚Äù Medical image analysis , vol. 36, pp. 135‚Äì146, 2017.9
[34] Y . Zhou, Z. Li, S. Bai, C. Wang, X. Chen, M. Han, E. Fishman, and
A. L. Yuille, ‚ÄúPrior-aware neural network for partially-supervised multi-
organ segmentation,‚Äù in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2019, pp. 10 672‚Äì10 681.
[35] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero,
S. A. Cook, A. De Marvao, T. Dawes, D. P. O‚ÄòRegan et al. , ‚ÄúAnatomi-
cally constrained neural networks (acnns): application to cardiac image
enhancement and segmentation,‚Äù IEEE transactions on medical imaging ,
vol. 37, no. 2, pp. 384‚Äì395, 2017.
[36] A. V . Dalca, J. Guttag, and M. R. Sabuncu, ‚ÄúAnatomical priors in
convolutional networks for unsupervised biomedical segmentation,‚Äù in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2018, pp. 9290‚Äì9299.
[37] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, ‚ÄúPointcon-
trast: Unsupervised pre-training for 3d point cloud understanding,‚Äù in
European conference on computer vision . Springer, 2020, pp. 574‚Äì
591.
[38] Z. Xie, Y . Lin, Z. Zhang, Y . Cao, S. Lin, and H. Hu, ‚ÄúPropagate yourself:
Exploring pixel-level consistency for unsupervised visual representation
learning,‚Äù in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2021, pp. 16 684‚Äì16 693.
[39] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, ‚ÄúMomentum contrast
for unsupervised visual representation learning,‚Äù in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , 2020,
pp. 9729‚Äì9738.
[40] W. Van Gansbeke, S. Vandenhende, S. Georgoulis, and L. Van Gool,
‚ÄúUnsupervised semantic segmentation by contrasting object mask pro-
posals,‚Äù in Proceedings of the IEEE/CVF International Conference on
Computer Vision , 2021, pp. 10 052‚Äì10 062.
[41] W. Wang, T. Zhou, F. Yu, J. Dai, E. Konukoglu, and L. Van Gool,
‚ÄúExploring cross-image pixel contrast for semantic segmentation,‚Äù in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 7303‚Äì7313.
[42] X. Hu, D. Zeng, X. Xu, and Y . Shi, ‚ÄúSemi-supervised contrastive
learning for label-efÔ¨Åcient medical image segmentation,‚Äù in International
Conference on Medical Image Computing and Computer-Assisted Inter-
vention . Springer, 2021, pp. 481‚Äì490.
[43] Q.-Y . Zhou, J. Park, and V . Koltun, ‚ÄúOpen3D: A modern library for 3D
data processing,‚Äù arXiv:1801.09847 , 2018.
[44] E. Jang, S. Gu, and B. Poole, ‚ÄúCategorical reparameterization with
gumbel-softmax,‚Äù arXiv preprint arXiv:1611.01144 , 2016.
[45] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚ÄúA simple framework
for contrastive learning of visual representations,‚Äù in International
conference on machine learning . PMLR, 2020, pp. 1597‚Äì1607.
[46] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556 , 2014.
[47] H. Kervadec, J. Dolz, S. Wang, E. Granger, and I. B. Ayed, ‚ÄúBounding
Boxes for Weakly Supervised Segmentation: Global Constraints Get
Close to Full Supervision,‚Äù arXiv Preprint ArXiv:2004.06816 , 2020.
[48] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, ‚ÄúEnet: A deep
neural network architecture for real-time semantic segmentation,‚Äù arXiv
preprint arXiv:1606.02147 , 2016.
[49] P. Bilic, P. F. Christ, E. V orontsov, G. Chlebus, H. Chen, Q. Dou, C.-W.
Fu, X. Han, P.-A. Heng, J. Hesser et al. , ‚ÄúThe liver tumor segmentation
benchmark (lits),‚Äù arXiv preprint arXiv:1901.04056 , 2019.
[50] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y . Nan,
G. Mu, Z. Lin, M. Han et al. , ‚ÄúThe state of the art in kidney and
kidney tumor segmentation in contrast-enhanced ct imaging: Results of
the kits19 challenge,‚Äù Medical Image Analysis , p. 101821, 2020.
[51] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770‚Äì778.
[52] F. Lateef and Y . Ruichek, ‚ÄúSurvey on semantic segmentation using deep
learning techniques,‚Äù Neurocomputing , vol. 338, pp. 321‚Äì348, 2019.
[53] B. M. Anderson, E. Y . Lin, C. E. Cardenas, D. A. Gress, W. D. Erwin,
B. C. Odisio, E. J. Koay, and K. K. Brock, ‚ÄúAutomated contouring of
contrast and noncontrast computed tomography liver images with fully
convolutional networks,‚Äù Advances in radiation oncology , vol. 6, no. 1,
p. 100464, 2021.
[54] B. C. Lowekamp, D. T. Chen, L. Ib ¬¥aÀúnez, and D. Blezek, ‚ÄúThe design
of simpleitk,‚Äù Frontiers in neuroinformatics , vol. 7, p. 45, 2013.
[55] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage
quality assessment: from error visibility to structural similarity,‚Äù IEEE
transactions on image processing , vol. 13, no. 4, pp. 600‚Äì612, 2004.10
Input GT Fully BoxInst MIL GMIL OursKidney
Surface X
Kidney
Surface Y
Kidney
Surface Z
Liver
Surface X
Liver
Surface Y
Liver
Surface Z
Hippocampus
Surface X
Hippocampus
Surface Y
Hippocampus
Surface Z
Fig. 3. Qualitative results of our framework over three datasets. As shown in the Ô¨Ågure, four colors exist in the segmentation results. red
indicates ground-truth. green indicates correct predictions. blue indicates predictions where should have been predicted. yellow indicates
wrongly-predicted pixels. Our framework is built on BoxInst [7] which is the baseline method. We then compare several methods, including
fully-supervised results, the proposed method and demonstrate the high quality of the proposal.11Input
 volume
 point cloud
Fig. 4. V olume v.s.Point cloud. We optimize the learning of the
geometric shape in the form of volume representation by Dice Loss.
As shown in the Ô¨Ågure, the proposed point cloud representation
largely improves the segmentation result.
Input
 w/o internal
 w internal
Fig. 5. Internal details. To verify the effectiveness of the internal
details of the template, we erase the intra-organ variabilities by
fulÔ¨Ålling the template organ. As shown in the Ô¨Ågure, the segmentation
performance drastically degrades especially for organs with delicate
and complex structures. Figures from left to right are more and more
delicate and complex.