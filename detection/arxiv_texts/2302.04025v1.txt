WAT: Improve the Worst-class Robustness in Adversarial Training
Boqi Li1, Weiwei Liu1*
1School of Computer Science, Wuhan University, China
flbq988, liuweiwei863 g@gmail.com
Abstract
Deep Neural Networks (DNN) have been shown to be vulner-
able to adversarial examples. Adversarial training (AT) is a
popular and effective strategy to defend against adversarial
attacks. Recent works (Benz et al. 2020; Xu et al. 2021; Tian
et al. 2021) have shown that a robust model well-trained by AT
exhibits a remarkable robustness disparity among classes, and
propose various methods to obtain consistent robust accuracy
across classes. Unfortunately, these methods sacriÔ¨Åce a good
deal of the average robust accuracy. Accordingly, this paper
proposes a novel framework of worst-class adversarial training
and leverages no-regret dynamics to solve this problem. Our
goal is to obtain a classiÔ¨Åer with great performance on worst-
class and sacriÔ¨Åce just a little average robust accuracy at the
same time. We then rigorously analyze the theoretical prop-
erties of our proposed algorithm, and the generalization error
bound in terms of the worst-class robust risk. Furthermore,
we propose a measurement to evaluate the proposed method
in terms of both the average and worst-class accuracies. Ex-
periments on various datasets and networks show that our
proposed method outperforms the state-of-the-art approaches.
Introduction
Deep Neural Networks (DNNs) are known to be vulnerable
to adversarial examples (Szegedy et al. 2014; Goodfellow,
Shlens, and Szegedy 2015). An adversarial example in a
small perturbation from test data can easily fool the DNN
model, which remains a security issue and is unacceptable in
some applications of DNN, such as road sign classiÔ¨Åcation
(Eykholt et al. 2018) , text classiÔ¨Åcation (Ebrahimi et al.
2018), self-supervised learning (Wang and Liu 2022) and
object detection (Xu et al. 2020).
Numerous works (Raghunathan, Steinhardt, and Liang
2018; Madry et al. 2018; Li, Zou, and Liu 2022) have at-
tempted to improve the model robustness with various de-
fenses. Adversarial Training (AT) (Goodfellow, Shlens, and
Szegedy 2015; Madry et al. 2018) is one of the most widely
used and effective methods of defense. AT generates adver-
sarial examples from the training data in every mini-batch,
then uses these examples to replace training data or adds
them into the training data during the training phase.
*Corrsponding Author.
Copyright ¬©2023, Association for the Advancement of ArtiÔ¨Åcial
Intelligence (www.aaai.org). All rights reserved.Although AT obtains great average adversarial robustness
performance over classes, (Benz et al. 2020; Xu et al. 2021;
Tian et al. 2021) Ô¨Ånd that a robust model well-trained by
AT exhibits a large robustness disparity in different classes
on various balanced datasets, like the left classiÔ¨Åer in Fig-
ure 1. Thus, AT leaves some classes vulnerable and may not
perform well on some speciÔ¨Åc classes in certain real-world
secure systems. For example, in the autonomous driving con-
text, a classiÔ¨Åer that has been well trained by AT may perform
well on trafÔ¨Åc sign classiÔ¨Åcation and achieve great adversar-
ial robustness performance on average while still exhibiting
vulnerabilities on speciÔ¨Åc signs, which represents a potential
danger for users.
Recently, some works (Benz et al. 2020; Xu et al. 2021)
have attempted to solve this problem. Benz et al. (2020)
analyze this phenomenon and use cost-sensitive learning
to make the performance consistent over classes. Xu et al.
(2021) propose employing re-weight and re-margin strategies
to solve this problem. Both of these methods obtain consistent
robust accuracy over classes, but they sacriÔ¨Åce a good deal of
the average robust accuracy, like middle classiÔ¨Åer in Figure
1. To overcome the limitations of Benz et al. (2020); Xu
et al. (2021), this paper proposes a novel min-max learning
paradigm to optimize worst-class robust risk and leverages no-
regret dynamics to solve the proposed min-max problem, our
goal is to achieve a classiÔ¨Åer with great performance on worst-
class but sacriÔ¨Åce a little average robust accuracy like the
right classiÔ¨Åer in Figure 1. Moreover, we rigorously analyze
the theoretical properties of our proposed algorithm, and the
generalization error bound in terms of the worst-class robust
risk. Empirically, we Ô¨Ånd that a trade-off exists between
average and worst-class robust accuracies, and accordingly
propose a measurement to evaluate the method in terms of
both the average and worst-class accuracies.
The main contributions in this paper are as follows:
‚Ä¢We propose a novel framework of worst-class adversarial
training that leverages no-regret dynamics to solve the
problem.
‚Ä¢We analyze the theoretical properties of our proposed
algorithm, and the generalization error bound in terms of
the worst-class robust risk.
‚Ä¢A measurement is presented to evaluate the method in
terms of both the average and worst-class accuracies.arXiv:2302.04025v1  [cs.LG]  8 Feb 2023Classifier 
f
Average Robust 
Accuracy: 54%Worst -class Robust 
Accuracy: 10.5%Classifier 
f
Average Robust 
Accuracy: 30%Worst -class Robust 
Accuracy: 25%Classifier 
f
Average Robust 
Accuracy: 50%Worst -class Robust 
Accuracy : 25%
Previous works Our GoalFigure 1: A brief introduction of our main idea. Previous works only care about average or worst-class robust accuracy, while our
method considers both worst-class and average robust accuracy.
‚Ä¢Extensive experimental results on various datasets and
networks verify that our proposed method outperforms
state-of-the-art baselines.
Related Work
Adversarial Robustness :To improve adversarial robust-
ness of DNN, adversarial training (Goodfellow, Shlens, and
Szegedy 2015; Madry et al. 2018) is one of the most effec-
tive defenses. A large number of works (Zhang et al. 2019;
Tsipras et al. 2019; Yang et al. 2020) have explored the
trade-off between robustness and accuracy. Amongst them,
TRADES (Zhang et al. 2019) is one of the most popular meth-
ods due to its promising experimental results. Besides, Ma,
Wang, and Liu (2022) analyze the trade-off between robust-
ness and fairness. Montasser, Hanneke, and Srebro (2019);
Yin, Ramchandran, and Bartlett (2019); Xu and Liu (2022)
theoretically analyze the adversarial robust generalization
of a model while Simon-Gabriel et al. (2019) analyzes the
Ô¨Årst-order adversarial vulnerability of neural networks. Re-
cently, a few works have been developed to further improve
its performance, such as using unlabeled data (Carmon et al.
2019), feature alignments (Yan et al. 2021), wider networks
(Wu et al. 2021) and a few tricks (Pang et al. 2021).
Disparity of Class-wise Robustness :In natural training,
class-imbalance is a classical problem in long-tailed data. In
such problem, major class has more data than minor class.
Most of previous works to solve this problem can be con-
cluded as resampling (Zhou and Liu 2006) and cost-sensitive
learning (Zou et al. 2018). Recently, some works have opted
to focus on the class-wise robustness disparity in the adver-
sarial training. Benz et al. (2020) study this problem em-
pirically, and Ô¨Ånd that AT obtains a larger robust disparity
among classes than that of natural training even in balanced
data (e.g., CIFAR-10). Tian et al. (2021) also Ô¨Ånd the sim-
ilar experimental results on six different datasets. To solve
this problem, Benz et al. (2020) use a cost-sensitive learn-
ing fashion which is widely used in natural learning with
imbalanced datasets; Xu et al. (2021) propose a new method
to reduce the class-wise variance of robust accuracy over
classes. However their approaches both sacriÔ¨Åce a good deal
of the average robust accuracy because they aim to make the
performance consistent over classes. To address this issue,
this paper aims to improve the worst-class adversarial robust-
ness, while obtaining less average robust accuracy loss than
previous works.Preliminaries
This paper considers a K-class classiÔ¨Åcation problem over
input spaceXand output label space Y=f1;2;;Kg.
AssumeDis a distribution over Z=XY . We denote the
sample asS:fXYgn. LetFbe the hypothesis class, while
f(x;) :X !Y is a classiÔ¨Åer inF, where xis the input
variable and fis parametrized by . Let`:FZ! [0;B]
be the loss function. Throughout this paper, we assume that `
is bounded. The expected natural risk Rnat(f)and expected
robust riskRrob(f)over distributionDand classiÔ¨Åer f(x;)
can then be deÔ¨Åned with respect to loss function `as follows:
Rnat(f) = E
(x;y)D`(f(x;);y) (1)
Rrob(f) = E
(x;y)Dmax
x02B(x;)`(f(x0;);y) (2)
whereB(x;) =fx0:jjx0 xjjpgdenotes the`p-norm
(p1)ball centered at xwith radius.
Worst-class Adversarial Robustness
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013
/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000055/uni00000052/uni00000045/uni00000058/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000005a/uni00000052/uni00000055/uni00000056/uni00000057/uni00000010/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056
/uni00000044/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048
(a) PGD on CIFAR-10
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013
/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000055/uni00000052/uni00000045/uni00000058/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni0000005a/uni00000052/uni00000055/uni00000056/uni00000057/uni00000010/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056
/uni00000044/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048 (b) TRADES on CIFAR-10
Figure 2: Class-wise robustness disparity of different AT
using ResNet-18 on CIFAR-10. The robust accuracy (%) is
evaluated under PGD-20 attack.
Typically, one aims to use ERM to obtain a good classiÔ¨Åer
from a hypothesis class with low empirical risk. However, a
classiÔ¨Åer with low empirical risk may not perform well on
the worst class. To illustrate this phenomenon, we present the
results of different AT variants on the CIFAR-10 in Figure 2.
From results in Figure 2(b), we can see that TRADES (Zhang
et al. 2019) obtains a worst-class robust accuracy of 23%
under PGD-20 (Madry et al. 2018) attack, while the averagerobust accuracy of TRADES is 46%. A similar phenomenon
occurs when different variants of AT are used on different
datasets. This degree of robustness disparity among classes is
unacceptable in certain real-world secure systems. To study
this problem, we deÔ¨Åne class-wise risk and worst-class risk
as follows. We use Dkto denote the distribution of sample
belonging to class kclass, andSkto denote the sample drawn
fromDk.
Rnat
k(f) = E
(x;y)Dk[`(f(x;);y)] (3)
Rrob
k(f) = E
(x;y)Dk[ max
x02B(x;)`(f(x0;);y)] (4)
Similarly, we deÔ¨Åne the worst-class natural risk as
Rnat
wc(f) = maxk2[K]Rnat
k(f)and worst-class robust risk
asRrob
wc(f) = maxk2[K]Rrob
k(f), where [K]denotes the
set of all positive integers in [1;K]. It follows that we have
Rrob
wc(f)Rrob(f)Rnat(f).
Disparity of Adversarial Robustness
Figures 2(a) and 2(b) show that a large gap exists between the
worst-class robust accuracy and the average robust accuracy.
Therefore, a classiÔ¨Åer with low expected natural risk and
expected robust risk may have high robust risk on some
classes.
To solve this problem, recently, various strategies (Benz
et al. 2020; Xu et al. 2021) aimed at making the robust perfor-
mance of the model consistent over all classes have been pro-
posed. For example, (Xu et al. 2021) propose the re-weight
and re-margin strategies on TRADES. Empirically, these
works show that existing strategies typically sacriÔ¨Åce the av-
erage robust accuracy to improve worst-class robust accuracy.
It is hard to choose proper weight for each class.
/uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000014/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000015/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018
/uni0000005a/uni00000048/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000010/uni00000017/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni00000055/uni00000052/uni00000045/uni00000058/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000010/uni00000017/uni00000003/uni00000044/uni00000046/uni00000046
/uni0000005a/uni00000052/uni00000055/uni00000056/uni00000057/uni00000003/uni00000044/uni00000046/uni00000046
/uni00000044/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000044/uni00000046/uni00000046
Figure 3: Trade-off between average and worst-class robust
accuracy of ResNet-18 on CIFAR-10.
In Figure 3, we use TRADES to train a ResNet-18 (He et al.
2016) on CIFAR-10. We assign weight wkfor class-kand
use a weighted lossPK
k=1wk`trades (;), where`trades (;)
is the loss used in TRADES and is deÔ¨Åned as `trades :=
max x02B(x;)CE(h(x);y) +KL (h(x);h(x0)). We
change the weight of class-4 from 0.05 to 0.25 and set the
weights of the other classes to be (1 w4)=(K 1). In Fig-
ure 2(a), we Ô¨Ånd that the worst robust accuracy appears in
class-4, so we choose to change the weights of class-4.From the results in Figure 3, we can determine that when
the weight of class-4 is increased from 0.05 to 0.15, the worst-
class robust accuracy of TRADES grows by 23.1%, while
the average robust accuracy of TRADES drops by 0.09%.
Moreover, when the weight of class-4 is increased from 0.15
to 0.25, the worst-class and average robust accuracy drop
at the same time. It is therefore demonstrably difÔ¨Åcult to
Ô¨Ånd the optimal weight for each class, and it is imperative to
propose a measurement to simultaneously evaluate how much
a given strategy would boost worst-class robust accuracy and
decrease the average robust accuracy.
We useAto denote a vanilla adversarial training with-
out any strategy, and Ato denote adversarial training
with the strategy . We run the algorithm Aon hypoth-
esis classFand sample Strain , and obtain the classiÔ¨Åer
^f=A(F;Strain).
The average natural accuracy of a classiÔ¨Åer fwith respect
to distributionDis deÔ¨Åned as
Accnat(f;D) = 1 P(x;y)Dfy6=f(;x)g (5)
while average robust accuracy is deÔ¨Åned as
Accrob(f;D)=1 P(x;y)Df9x02B(x;);s.t.y6=f(;x0)g
(6)
Similarly, we denote the k-th class natural accuracy
asAccnat
k(f;D), the worst-class natural accuracy as
Accnat
wc(f;D), thek-th class robust accuracy as Accrob
k(f;D)
and the worst-class robust accuracy as Accrob
wc(f;D). Let the
average robust accuracy, the accuracy of the k-th class and
the worst-class accuracy of a classiÔ¨Åer fon a test setStest
beAccrob(f;Stest),Acck(f;Stest)andAccwc(f;Stest), re-
spectively. For simplicity, we here use dAcc(f)to denote
Acc(f;Stest). This paper proposes a novel measurement to
evaluate a method in terms of both the average and worst-
class accuracy.
^(F;;A;S) =dAccwc(A(F)) dAccwc(A(F))
dAccwc(A(F))
 dAcc(A(F)) dAcc(A(F))
dAcc(A(F))(7)
Clearly, the larger the value of ^is, the better a method
performs.
Proposed Method
In this section, we formulate a novel min-max problem and
then transform it into a two-player zero-sum game, and sub-
sequently proposes a no-regret dynamics algorithm to solve
the problem.
No-regret Dynamics
Consider a two-player zero-sum game, in which a decision-
maker repeatedly plays a game against an adversary. More
speciÔ¨Åcally, the decision-maker plays before the adversary
and does not know the action taken by the adversary in each
round. No-regret dynamics is one of the most efÔ¨Åcient meth-
ods of achieving an -coarse correlated equilibrium (Rough-
garden and Iwama 2017).Multiplicative Weight Updates Algorithm (Arora, Hazan,
and Kale 2012) is one of the most widely used no-regret
dynamic algorithms. Assume a game repeats for Trounds,
while the decision-maker has a choice of ndecisions. The
decision-maker needs to repeatedly make a decision from
the decision set and obtains an associated payoff from the
adversary, while the best decision may not be known as a
priori. Lett= 1;2;;Tdenote the current round. In each
roundt, the decision-maker produces a distribution ptover
the decision set and chooses an action from the set according
topt. At this time, the adversary chooses a cost vector Ct.
Letpt
kbe thek-th element of ptwhileCt
kdenotes thek-th
element of Ct. Hedge Algorithm (Freund and Schapire 1997)
is one of Multiplicative Weights Updates Algorithm that uses
an exponential function to adjust the weight of every decision
as follows.
pt
k=exp(Pt 1
i=1Ci
k)
PK
k=1exp(Pt 1
i=1Ci
k): (8)
Clearly, Hedge Algorithm produces the weights depending on
past performance. Intuitively, this scheme works well because
it tends to put heavy weights on high payoff decisions in the
long run.
Worst-class Adversarial Training
The loss of a classiÔ¨Åer fon training setStrcan be deÔ¨Åned as
Ltr
0(f) =Ltr(f) =1
jStrjX
(xi;yi)2Str`trades (f(xi;);yi);
(9)
wherejjdenotes the cardinality of a set. Let Ltr
k(f)be
the training loss on class k. Similarly, we use Lval
0(f)and
Lval
k(f)to denote the loss of a classiÔ¨Åer fon the validation
setSvaland validation loss on class k, respectively. `trades
is the loss used in TRADES.
We aim to minimize the following risk
min
fmax
k2[0;K]Rrob
k(f); (10)
whereRrob
0(f) =Rrob(f). We then formulate (10) as a
zero-sum game. In such a game, the learner has a decision
setf@Ltr
0(f)
@f;;@Ltr
K(f)
@fg,Ltr
0(f)is the excepted training
loss andLtr
k(f)is the training loss of class- kfor every 1
kK. The best decision is not known as a priori.
Remark. The reason that we add@Ltr
0(f)
@fto decision set is
the learner can directly choose@Ltr
0(f)
@fas a decision in such
a game.
The weight of each decision is initialized as 1=(K+ 1) .
In epocht, we use the validation set to evaluate the classiÔ¨Åer,
and use validation loss to denote the cost. The learning rate
is. In epocht, the learner updates the model according to
the following rule:
ft=ft 1 KX
k=0wt
k@Ltr
k(ft 1)
@f; (11)Algorithm 1: WAT: Worst-class Adversarial Training
Input: training dataStr, validation dataSval, learning
rate, training epochs T, number of classes Kand hyper-
parameter.
Initializef0;w0
k=1
K+1for everyk2[K].
for1tTdo
useStrto obtainLtr
0(ft 1);;Ltr
K(ft 1).
useSvalto obtainLval
0(ft 1);;Lval
K(ft 1).
ft=ft 1 PK
k=0wt
k@Ltr
k(ft 1)
@f
for0kKdo
wt+1
k=exp(Pt
i=1Lval
k(fi))PK
k=0exp(Pt
i=1Lval
k(fi)).
end for
end for
Output:f= arg max
k2[K]min
f2ff1;;fTgLval
k(f).
where
wt
k=exp(Pt 1
i=1Lval
k(fi))
PK
k=0exp(Pt 1
i=1Lval
k(fi)): (12)
After the learner updates the model, it obtains a loss
vector from the adversary. The algorithm is described in
more detail in Algorithm 1. Algorithm 1 outputs f=
arg max
k2[K]min
f2ff1;;fTgLval
k(f). The following theorem pro-
vides the guarantee of the worst-class loss.
Theorem 1. Assume the range of Lval(f)is[0;1], and
1=TPT
t=1Lval
k(ft)1=(1 ) mintLval
k(ft)for every
kand some1=2. We then have
max
kmin
tLval
k(ft)1
TTX
t=1KX
k=0wt
kLval
k(ft)+log(K+ 1)
T:
(13)
Proof. The proof of Theorem 1 can be found in the Appendix.
Remark. Theorem 1 shows that if we choose a proper ,
afterTrounds, the worst-class cost of the best classiÔ¨Åer
can be bounded by the average loss of previous rounds. Our
bound also depends on andT; a largerandTwill provide
a tighter bound.
Generalization Error Bound
This section provides the generalization error bound in terms
of the worst-class robust risk. The empirical natural risk and
robust risk are deÔ¨Åned as ^Rnat(f) =1
nPn
i=1`(f(xi;);yi)
and ^Rrob(f) =1
nPn
i=1max x02B(x;)`(f(x0;);yi), re-
spectively.
Rademacher complexity (Bartlett and Mendelson 2002)
is one of the classic measurements for generalization error.
LetS=fz1;z2;;zngbe an independent and identically
distributed (i.i.d.) sample with size nandibe a random
variable such that P[i= 1] = P[i= 1] = 1=2. The
Rademacher complexity of function class His deÔ¨Åned asRS(H) :=1
nE[suph2HPn
i=1ih(zi)]:We next analyze
the gap between the empirical risk and population risk of
the worst class. Let the training set Skbe drawn i.i.d. from
the distributionDk. The empirical k-th class robust risk is
deÔ¨Åned as
^Rrob
k(f) =1
jSkjX
(xi;yi)2Skmax
x02B(x;)`(f(x0
i;);yi):(14)
The empirical worst-class robust risk over S:=[k2[K]Sk
is^Rrob
wc(f) = maxk^Rrob
k(f). and ~`Fis deÔ¨Åned as `F=
f(x;y)!`(f(x);y) :f2Fg . We assumejSkj=jSj=K
holds for every k. We present the following Theorem.
Theorem 2. Suppose that the range of `(f(x);y)is[0;B].
Let~`(f(x);y) := max x02B(x;)`(f(x0);y). Then, for any
2(0;1), with probability at least 1 , the following holds
for allf2F,
Rrob
wc(f)^Rrob
wc(f) + 2Bmax
kRSk(~`F) + 3Bs
Klog2

2jSj:
Proof. The proof of Theorem 2 can be found in the Appendix.
Multi-class Linear ClassiÔ¨Åers
This section studies the generalization error of multi-class
linear classiÔ¨Åers. We here consider a K-class classiÔ¨Åcation
problem. LetFWbe a multi-class linear classiÔ¨Åer hypothesis,
andfW:X!RKinFWbe parameterized by a matrix W
with dimension Kd. Thek-th coordinate of fW(x)is the
score of the k-th class, and the prediction of fWis the class
with the highest score among the Kclasses. Let wk2Rdbe
thek-th column of W>and be upper bounded by Wunder
the`pnorm (p1):FW=ffW(x) :jjW>jjp;1Wg.
For multi-class classiÔ¨Åcation problems, we deÔ¨Åne the margin
operatorM(;y) :RK[K]!RasM(;y) =y 
maxy06=yy0, and a classiÔ¨Åer fpredicts correct if and only if
M(;y)>0. The ramp loss is deÔ¨Åned as follows:
(t) =8
><
>:1t0;
1 t
0<t<;
0t:(15)
Based on the margin operator and ramp loss, we have
`(fW(x);y) =(M(fW(x);y))and~`(fW(x);y) =
max
x02B(x;)(M(fW(x);y)). We use 1()to denote af0,1g-
valued indicator function. We then present the following
Theorem.
Theorem 3. Consider the multi-class linear classiÔ¨Åers in the
adversarial setting, and suppose that1
p+1
q= 1,p;q1.
For any Ô¨Åxed  >0andW > 0, we have with probability at
least 1 , for all Wsuch thatkW>kp;1W,
1 Accrob
wc(f;D)K
jSjX
(xi;yi)2SEi+2WK3
jSjU+c;
where
Ei= 1(hwyi;xii+maxy06=yi(hwy0;xii+kwy0 wyik1) );c=2WK2d1
q
p
jSj+ 3q
Klog2

2jSj;
U=maxy;kEP
(xi;yi)2Skixi 1(yi=y)
q
:
Proof. The proof of Theorem 3 can be found in the Appendix.
Remark. Only if we optimize worst-class robust risk, as
in our method, Theorem 2 and 3 hold. However, previous
works do not optimize this risk and Theorem 2 and 3 are not
applicable to them.
Experiments
In this section, we conduct experiments on various datasets
and models to evaluate the performance of our proposed
method. Code is available at https://github.com/boqili/WAT.
Datasets and Baselines
The datasets used in the experiments are CIFAR-10 and
CIFAR-100 (Krizhevsky, Hinton et al. 2009), which are de-
scribed in more detail in the Appendix.
Zhang et al. (2019), Xu et al. (2021) and Benz et al.
(2020) are used as our baselines. TRADES (Zhang et al.
2019) is one of the most popular adversarial training meth-
ods. FRL is presented in Xu et al. (2021). FRL has two
variants: FRL-RW is based on the re-weight strategy, and
FRL-RWRM is based on the re-weight and re-margin strat-
egy. Cost-sensitive Learning (CSL) (Benz et al. 2020) is a
classical approach to solving the class-imbalanced problem
on imbalanced datasets (Ting 2000; Khan et al. 2018). To
be fair, we use the same hyper-parameters and perform the
model selection for each method.
Evaluations
We use the following measures to evaluate the performance
of all methods.
Average and Worst-class accuracy . Following (Xu et al.
2021), we use average natural accuracy, average robust ac-
curacy, worst-class natural accuracy and worst-class robust
accuracy to evaluate the performance of all methods. We
use three strong adversarial attacks PGD-100, CW(Carlini
and Wagner 2017) attack and AutoAttack(Croce and Hein
2020) to evaluate robust accuracy. We set perturbation radius
= 8=255for CIFAR-10 and CIFAR-100. Other details can
be found in the Appendix.
Class-wise Variance ( CV). Class-wise variance is a com-
mon measure used in (Xu et al. 2021) and (Tian et al. 2021).
The deÔ¨Ånition of CVgiven in (Tian et al. 2021) is presented
below.
DeÔ¨Ånition 1. (Tian et al. 2021) Given one dataset containing
C classes, the accuracy of each class cisac, the average
accuracy over all class is a=1
CPC
c=1ac, and theCV is
deÔ¨Åned as:CV=1
cPC
c=1(ac a)2.
We useCVnatto denote the class-wise variance of natural
accuracy and CVrobto denote the class-wise variance of
robustness accuracy, We also use as deÔ¨Åned in Eq.(7) toTable 1: Comparison results of all methods using ResNet-18 on CIFAR-10 and CIFAR-100. We evaluate every method in terms
of both accuracy (%) and . We report the average natural accuracy, worst-class natural accuracy, average robust accuracy,
worst-class robust accuracy, nat,pgd,cwandAAfor every method. We use bold to denote the best value in every metric.
CIFAR-10 Natural PGD-100 CW AutoAttack
Method Avg. Wst. nat Avg. Wst. pgd Avg. Wst. cw Avg. Wst. AA
TRADES 82.11 64.6 0 51.69 25.2 0 50.38 24.1 0 48.64 21.7 0
FRL-RW 81.75 69.2 0.067 49.02 30.8 0.171 47.80 27.8 0.102 46.08 25.4 0.118
FRL-RWRM 80.69 71.4 0.088 49.16 32.0 0.221 47.45 28.1 0.108 45.94 26.1 0.147
CSL 76.29 67.1 -0.032 43.30 33.8 0.179 41.60 31.3 0.124 40.32 29.2 0.175
Ours 80.98 69.5 0.062 49.13 36.6 0.403 47.57 33.3 0.326 46.04 30.1 0.334
CIFAR-100 Natural PGD-100 CW AutoAttack
Method Avg. Wst. nat Avg. Wst. pgd Avg. Wst. cw Avg. Wst. AA
TRADES 54.57 19.00 0 27.39 3.00 0 24.87 1.00 0 23.57 1.00 0
FRL-RW 53.08 24.00 0.236 25.76 3.00 -0.060 22.39 2.00 0.900 21.09 1.00 -0.105
FRL-RWRM 52.55 22.00 0.121 26.04 4.00 0.284 22.33 2.00 0.898 21.11 2.00 0.896
CSL 53.83 21.00 0.092 26.19 4.00 0.290 22.35 2.00 0.899 22.25 2.00 0.944
Ours 53.99 19.00 -0.020 26.91 5.00 0.643 24.26 3.00 1.945 22.89 3.00 1.971
123 4
5
6
7
8 91010255075100
(a) Ours vs TRADES
123 4
5
6
7
8 91010255075100
 (b) Ours vs CSL
123 4
5
6
7
8 91010255075100
 (c) Ours vs FRL-RW
123 4
5
6
7
8 91010255075100
 (d) Ours vs FRL-RWRM
Figure 4: Class-wise robust accuracy disparity of all methods using ResNet-18 on CIFAR-10. We compare our method and
another method in terms of the class-wise robust accuracy evaluated under CW attack. We denote the results of our method with
a blue line, while the results of the comparison methods are represented by a purple line.
evaluate the method in terms of both the average and worst-
class accuracies.
Results
In Table 1, we report the performance of every method using
ResNet-18 on CIFAR-10 and CIFAR-100. We can clearly
observe that our method successfully outperforms other meth-
ods on both CIFAR-10 and CIFAR-100. More speciÔ¨Åcally,
under PGD-100 attack, our method improves the worst-class
robust accuracy of all compared methods by at least 2.8% on
the CIFAR-10 dataset and 1.0% on the CIFAR-100 dataset,
while improving the worst-class robust accuracy of all com-
pared methods for at least 2.0% on CIFAR-10 dataset and
1.0% on CIFAR-100 under CW attack. Under AutoAttack,
our method improves the worst-class robust accuracy of all
compared methods by at least 0.9% on the CIFAR-10 dataset
and 1.0% on the CIFAR-100 dataset as well. Moreover, com-
pared with TRADES, although all compared methods in-crease the robust accuracy, our method achieves the best
pgd,cwandAAvalue; in short, we sacriÔ¨Åce the least aver-
age robust accuracy to obtain the highest worst-class robust
accuracy.
Furthermore, to study the effectiveness of our method in
more detail, we conduct a comparison of the class-wise robust
accuracy evaluated under CW attack between our method and
all compared methods in Figure 4. As shown in Figure 4(a),
our method achieves higher robust accuracy of class-4 and
class-5 than TRADES, thus, our method obtains a good per-
formance on worst-class robust accuracy. In Figure 4(b), al-
though CSL achieves a great performance on the worst class,
it performs worse than our method on most other classes,
which leads to a low average robust accuracy. From Figures
4(c) and 4(d), we can see that our method achieves higher
robust accuracy on class-4 (the most vulnerable class) than
the other two baselines. Moreover, our proposed method sig-
niÔ¨Åcantly outperforms the other two baselines on class-5 andTable 2: Comparison results of all methods using WideResNet-34-10 on CIFAR-10.
CIFAR-10 Natural PGD-100 CW AutoAttack
Method Avg. Wst. nat Avg. Wst. pgd Avg. Wst. cw Avg. Wst. AA
TRADES 84.51 64.7 0 53.68 23.3 0 53.18 22.8 0 51.22 20.9 0
FRL-RW 83.93 74.5 0.145 50.59 30.0 0.230 50.58 29.1 0.227 48.36 27.1 0.241
FRL-RWRM 83.86 72.1 0.107 51.25 32.9 0.367 51.08 32.2 0.373 48.98 28.6 0.325
CSL 79.78 75.1 0.105 45.7 32.2 0.233 44.74 30.8 0.192 43.10 29.4 0.248
Ours 83.71 74.0 0.062 51.53 34.9 0.458 50.89 33.4 0.422 49.12 30.7 0.428
Table 3: Results of our method with different using ResNet-18 on CIFAR-10.
CIFAR-10 Natural PGD-100 CW AutoAttack
Method Avg. Wst. nat Avg. Wst. pgd Avg. Wst. cw Avg. Wst. AA
TRADES 82.11 64.6 0 51.69 25.2 0 50.38 24.1 0 48.64 21.7 0
Ours(=0.01) 81.54 68.0 0.046 50.50 26.6 0.033 49.86 25.0 0.027 47.65 22.6 0.021
Ours(=0.05) 81.76 69.3 0.068 50.06 34.2 0.326 49.53 31.7 0.298 47.05 28.1 0.262
Ours(=0.1) 80.98 69.5 0.062 49.13 36.6 0.403 47.57 33.3 0.326 46.04 30.1 0.334
Ours(=0.5) 79.30 67.3 0.008 48.09 37.5 0.418 45.42 32.5 0.250 43.98 31.1 0.337
class-8, which contributes to the highest cwof our method.
The results of class-wise robust accuracy disparity of all the
methods evaluated under PGD-100 attack and AutoAttack
on CIFAR-10 can be found in the Appendix.
We go on to evaluate the performance of all the methods on
WideResNet-34-10(Zagoruyko and Komodakis 2016). The
experimental results can be found in Table 2. From the re-
sults in Table 2, we can Ô¨Ånd that our method achieves the
highest worst-class robust accuracy evaluated under all three
attacks with at least 1.3% improvement. we also achieve the
highestpgd,cwandAAwhile we have comparable result
with compared methods in average robust accuracy evaluated
under all three attacks on CIFAR-10.
Parameter Analysis on 
We study the impact of hyper-parameter used in our method
on average and worst-class robust accuracy. We vary the
hyper-parameter fromf0.01,0.05,0.1,0.5g, and show the
results in Table 3. We Ô¨Ånd that a trade-off between the average
robust accuracy and the worst-class robust accuracy exists,
and if we improve the average robust accuracy, the worst-
class robust accuracy decreases at the same time. However,
a largerdoes not lead to a larger natandcw. In our
experiments, we Ô¨Ånd = 0:1yields the best natandcw
while= 0:5yields the best pgdandAA.
Comparison between CVand
From the results in Table 4, we can see that CSL obtains
the lowestCVcwvalue, while the average robust accuracy of
CSL is the worst. Notably, CVcwis not a good measurement
because it does not consider the trade-off between average
and worst-class robust accuracy. From the results in Table 4,
we can also see that our method achieves the best cw, hasTable 4: Comparison results between CVcwandcwusing
ResNet-18 on CIFAR-10.
CIFAR-10 CW Attack
Method Avg. Wst. CVcwcw
TRADES 50.38 24.1 0.0269 0
FRL-RW 47.80 27.8 0.0215 0.102
FRL-RWRM 47.45 28.1 0.0172 0.108
CSL 41.60 31.3 0.0027 0.124
Ours 47.57 33.3 0.0147 0.326
the highest worst-class robust accuracy, and is comparable
with FRL and CSL in average robust accuracy. Therefore,
cwis a more reasonable measurement than CVcwbecause
it considers average robust accuracy and worst-class robust
accuracy at the same time. The results evaluated under PGD-
100 attack and AutoAttack are shown in the Appendix.
Conclusion
To improve the worst-class robustness in adversarial training,
this paper proposes a novel framework of worst-class ad-
versarial training and leverages no-regret dynamics to solve
the problem. Theoretically, we provide the guarantee of the
worst-class loss and analyze the generalization error bound
in terms of the worst-class robust risk based on Rademacher
complexity. Moreover, we propose a measurement to evalu-
ate the method in terms of both the average and worst-class
accuracies. Empirical results verify the superiority of our
proposed approach.Acknowledgments
This work is supported by the National Natural Science Foun-
dation of China under Grant 61976161.
References
Arora, S.; Hazan, E.; and Kale, S. 2012. The Multiplicative
Weights Update Method: a Meta-Algorithm and Applications.
Theory of Computing , 8(1): 121‚Äì164.
Bartlett, P. L.; and Mendelson, S. 2002. Rademacher and
Gaussian Complexities: Risk Bounds and Structural Results.
Journal of Machine Learning Research , 3: 463‚Äì482.
Benz, P.; Zhang, C.; Karjauv, A.; and Kweon, I. S. 2020.
Robustness May Be at Odds with Fairness: An Empirical
Study on Class-wise Accuracy. CoRR , abs/2010.13365.
Carlini, N.; and Wagner, D. A. 2017. Towards Evaluating the
Robustness of Neural Networks. In S&P .
Carmon, Y .; Raghunathan, A.; Schmidt, L.; Duchi, J. C.;
and Liang, P. 2019. Unlabeled Data Improves Adversarial
Robustness. In NeurIPS .
Croce, F.; and Hein, M. 2020. Reliable evaluation of adver-
sarial robustness with an ensemble of diverse parameter-free
attacks. In ICML , volume 119, 2206‚Äì2216.
Ebrahimi, J.; Rao, A.; Lowd, D.; and Dou, D. 2018. HotFlip:
White-Box Adversarial Examples for Text ClassiÔ¨Åcation. In
Gurevych, I.; and Miyao, Y ., eds., ACL, 31‚Äì36.
Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati,
A.; Xiao, C.; Prakash, A.; Kohno, T.; and Song, D. 2018.
Robust Physical-World Attacks on Deep Learning Visual
ClassiÔ¨Åcation. In CVPR , 1625‚Äì1634.
Freund, Y .; and Schapire, R. E. 1997. A Decision-Theoretic
Generalization of On-Line Learning and an Application to
Boosting. Journal of Computer and System Sciences , 55(1):
119‚Äì139.
Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-
ing and Harnessing Adversarial Examples. In ICLR .
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual
Learning for Image Recognition. In CVPR , 770‚Äì778.
Khan, S. H.; Hayat, M.; Bennamoun, M.; Sohel, F. A.; and
Togneri, R. 2018. Cost-Sensitive Learning of Deep Feature
Representations From Imbalanced Data. IEEE Transactions
on Neural Networks and Learning Systems , 29(8): 3573‚Äì
3587.
Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple
layers of features from tiny images.
Li, X.; Zou, X.; and Liu, W. 2022. Defending Against Ad-
versarial Attacks via Neural Dynamic System. In NeurIPS .
Ma, X.; Wang, Z.; and Liu, W. 2022. On the Tradeoff Be-
tween Robustness and Fairness. In NeurIPS .
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu,
A. 2018. Towards Deep Learning Models Resistant to Adver-
sarial Attacks. In ICLR .
Montasser, O.; Hanneke, S.; and Srebro, N. 2019. VC Classes
are Adversarially Robustly Learnable, but Only Improperly.
InCOLT , volume 99, 2512‚Äì2530.Pang, T.; Yang, X.; Dong, Y .; Su, H.; and Zhu, J. 2021. Bag
of Tricks for Adversarial Training. In ICLR .
Raghunathan, A.; Steinhardt, J.; and Liang, P. 2018. CertiÔ¨Åed
Defenses against Adversarial Examples. In ICLR .
Roughgarden, T.; and Iwama, K. 2017. Twenty Lectures on
Algorithmic Game Theory. Bulletin of the EATCS , 122.
Simon-Gabriel, C.; Ollivier, Y .; Bottou, L.; Sch ¬®olkopf, B.;
and Lopez-Paz, D. 2019. First-Order Adversarial Vulnera-
bility of Neural Networks and Input Dimension. In ICML ,
volume 97, 5809‚Äì5817.
Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan, D.;
Goodfellow, I. J.; and Fergus, R. 2014. Intriguing properties
of neural networks. In ICLR .
Tian, Q.; Kuang, K.; Jiang, K.; Wu, F.; and Wang, Y . 2021.
Analysis and Applications of Class-wise Robustness in Ad-
versarial Training. In KDD , 1561‚Äì1570.
Ting, K. M. 2000. A Comparative Study of Cost-Sensitive
Boosting Algorithms. In ICML , 983‚Äì990.
Tsipras, D.; Santurkar, S.; Engstrom, L.; Turner, A.; and
Madry, A. 2019. Robustness May Be at Odds with Accuracy.
InICLR .
Wang, Z.; and Liu, W. 2022. Robustness VeriÔ¨Åcation for
Contrastive Learning. In ICML , volume 162, 22865‚Äì22883.
Wu, B.; Chen, J.; Cai, D.; He, X.; and Gu, Q. 2021. Do Wider
Neural Networks Really Help Adversarial Robustness? In
NeurIPS .
Xu, H.; Liu, X.; Li, Y .; Jain, A. K.; and Tang, J. 2021. To
be Robust or to be Fair: Towards Fairness in Adversarial
Training. In ICML , volume 139, 11492‚Äì11501.
Xu, J.; and Liu, W. 2022. On Robust Multiclass Learnability.
InNeurIPS .
Xu, K.; Zhang, G.; Liu, S.; Fan, Q.; Sun, M.; Chen, H.; Chen,
P.; Wang, Y .; and Lin, X. 2020. Adversarial T-Shirt! Evading
Person Detectors in a Physical World. In ECCV , volume
12350, 665‚Äì681.
Yan, H.; Zhang, J.; Niu, G.; Feng, J.; Tan, V . Y . F.; and
Sugiyama, M. 2021. CIFS: Improving Adversarial Robust-
ness of CNNs via Channel-wise Importance-based Feature
Selection. In ICML .
Yang, Y .; Rashtchian, C.; Zhang, H.; Salakhutdinov, R. R.;
and Chaudhuri, K. 2020. A Closer Look at Accuracy vs.
Robustness. In NeurIPS .
Yin, D.; Ramchandran, K.; and Bartlett, P. L. 2019.
Rademacher Complexity for Adversarially Robust Gener-
alization. In ICML , volume 97, 7085‚Äì7094.
Zagoruyko, S.; and Komodakis, N. 2016. Wide Residual
Networks. In BMVC .
Zhang, H.; Yu, Y .; Jiao, J.; Xing, E. P.; Ghaoui, L. E.; and Jor-
dan, M. I. 2019. Theoretically Principled Trade-off between
Robustness and Accuracy. In ICML , volume 97, 7472‚Äì7482.
Zhou, Z.; and Liu, X. 2006. Training Cost-Sensitive Neu-
ral Networks with Methods Addressing the Class Imbalance
Problem. IEEE Transactions on Knowledge and Data Engi-
neering , 18(1): 63‚Äì77.Zou, Y .; Yu, Z.; Kumar, B. V . K. V .; and Wang, J. 2018. Un-
supervised Domain Adaptation for Semantic Segmentation
via Class-Balanced Self-training. In ECCV , volume 11207,
297‚Äì313.Proof of Theorem
Proof of Theorem 1
Theorem 1. Assume the range of Lval(f)is[0;1], and 1=TPT
t=1Lval
k(ft)1=(1 ) mintLval
k(ft)for everykand some
1=2. We then have
max
kmin
tLval
k(ft)1
TTX
t=1KX
k=0wt
kLval
k(ft) +log(K+ 1)
T: (16)
We have this no-regret bound as a Lemma from (Arora, Hazan, and Kale 2012).
Lemma 1. Assume that all cost Ct
i2[ 1;1]and1=2. Then the Multiplicative Weights algorithm guarantees that after T
rounds, for any k, we have
TX
t=1KX
k=1Ct
kpt
kTX
t=1Ct
k TX
t=1jCt
kj logK
:
Now we prove Theorem 1.
Proof. From Lemma 1, for every k, we have
TX
t=1Lval
k(ft) TX
t=1jLval
k(ft)jTX
t=1KX
k=0wt
kLval
k(ft) +log(K+ 1)
: (17)
Use the assumption that the range of L(f)is[0;1], for everykwe can yield
(1 )TX
t=1Lval
k(ft)TX
t=1KX
k=0wt
kLval
k(ft) +log(K+ 1)
: (18)
Because inequation (18) holds for every k, with the assumption that 1=TPT
t=1Lval
k(ft)1=(1 ) mintLval
k(ft)holds
for everykand some1=2, we can yield
min
tLval
k(ft)1 
TTX
t=1Lval
k(ft)1
TTX
t=1KX
k=0wt
kLval
k(ft) +log(K+ 1)
T; (19)
for everyk. Thus we have
max
kmin
tLval
k(ft)1
TTX
t=1KX
k=0wt
kLval
k(ft) +log(K+ 1)
T: (20)
We conclude this proof.
Proof of Theorem 2
Theorem 2. Suppose that the range of `(f(x);y)is[0;B]. Let~`(f(x);y) := max x02B(x;)`(f(x0);y). Then, for any 2(0;1),
with probability at least 1 , the following holds for all f2F,
Rrob
wc(f)^Rrob
wc(f) + 2Bmax
kRSk(~`F) + 3Bs
Klog2

2jSj:
To prove Theorem 2, we need this following lemma.
Lemma 2. (Yin, Ramchandran, and Bartlett 2019) Suppose that the range of `(f(x);y)is[0;B]. Let ~`(f(x);y) :=
maxx02B(x;)`(f(x0);y). Then, for any 2(0;1), with probability at least 1 , the following holds for all f2F,
Rrob(f)^Rrob(f) + 2BRS(~`F) + 3Bs
log2

2jSj:
Now we prove Theorem 2.Proof. According to Lemma 2 , we have following results for every kunder the same assumption.
Rrob
k(f)^Rrob
k(f) + 2BRSk(~`F) + 3Bs
log2

2jSkj: (21)
Take the maximum values with kat left hand and right hand of the inequation respectively, we have
max
kRrob
k(f)max
k2
4^Rrob
k(f) + 2BRSk(~`F) + 3Bs
log2

2jSkj3
5: (22)
Use equation (23)
max
x(f(x) +g(x))max
xf(x) + max
xg(x); (23)
we have
Rrob
wc(f)^Rrob
wc(f) + 2Bmax
kRSk(~`F) + 3Bs
Klog2

2jSj: (24)
We conclude this proof.
Proof of Theorem 3
Theorem 3. Consider the multi-class linear classiÔ¨Åers in the adversarial setting, and suppose that1
p+1
q= 1,p;q1. For any
Ô¨Åxed >0andW > 0, we have with probability at least 1 , for all Wsuch thatkW>kp;1W,
1 Accrob
wc(f;D)K
jSjX
(xi;yi)2SEi+2WK3
jSjU+c;
where
Ei= 1
hwyi;xii+ max
y06=yi(hwy0;xii+kwy0 wyik1)
;
U=max
y;kE2
4X
(xi;yi)2Skixi 1(yi=y)
q3
5;
c=2WK2d1
q
p
jSj+ 3s
Klog2

2jSj:(25)
To prove Theorem 3, we need this following lemma.
Lemma 3. (Yin, Ramchandran, and Bartlett 2019) Consider the multi-class linear classiÔ¨Åers in the adversarial setting, and
suppose that1
p+1
q= 1,p;q1. For any Ô¨Åxed  >0andW > 0, we have with probability at least 1 , for all Wsuch that
jjW>jjp;1W,
P(x;y)D
9x02B(x;);s.t.y6= arg max
y02[K]hwy0;xi
1
nnX
i=11
hwyi;xii+ max
y06=yi 
hwy0;xii+kwy0 wyik1
+2WK
2
4p
Kd1
q
pn+1
nKX
y=1E2
4nX
i=1ixi 1(yi=y)
q3
53
5+ 3s
log2

2n:
Now we prove Theorem 3.
Proof. According to Lemma 3 , we have following results for every kunder the same assumption.P(x;y)Dk
9x02B(x;);s.t.y6= arg max
y02[K]hwy0;xi
1
jSkjX
(xi;yi)2Sk1
hwyi;xii+ max
y06=yi 
hwy0;xii+kwy0 wyik1
(26)
+2WK
2
4p
Kd1
q
p
jSkj+1
jSkjKX
y=1E2
4X
(xi;yi)2Skixi 1(yi=y)
q3
53
5+ 3s
log2

2jSkj:
Take the maximum values with kat left hand and right hand of (26) respectively and use (23), we can yield
max
kP(x;y)Dk
9x02B(x;);s.t.y6= arg max
y02[K]hwy0;xi
max
k2
41
jSkjX
(xi;yi)2Sk1
hwyi;xii+ max
y06=yi 
hwy0;xii+kwy0 wyik13
5 (27)
+ max
k2
42WK
2
4p
Kd1
q
p
jSkj+1
jSkjKX
y=1E2
4X
(xi;yi)2Skixi 1(yi=y)
q3
53
5+ 3s
log2

2jSkj3
5:
Because for every kwe have
X
(xi;yi)2Sk1
hwyi;xii+ max
y06=yi 
hwy0;xii+kwy0 wyik1
0: (28)
(28) implies that
max
k2
41
jSkjX
(xi;yi)2Sk1
hwyi;xii+ max
y06=yi 
hwy0;xii+kwy0 wyik13
5
max
k1
jSkjX
(xi;yi)2S1
hwyi;xii+ max
y06=yi 
hwy0;xii+kwy0 wyik1
=K
jSjX
(xi;yi)2S1
hwyi;xii+ max
y06=yi 
hwy0;xii+kwy0 wyik1
: (29)
Meanwhile, we can also yield
max
k2
42WK
2
4p
Kd1
q
p
jSkj+1
jSkjKX
y=1E2
4X
(xi;yi)2Skixi 1(yi=y)
q3
53
5+ 3s
log2

2jSkj3
5
=2WK
max
k1
jSkjKX
y=1E2
4X
(xi;yi)2Skixi 1(yi=y)
q3
5+2WK2d1
q
p
jSj+ 3s
Klog2

2jSj
2WK
max
y;kK
jSkjE2
4X
(xi;yi)2Skixi 1(yi=y)
q3
5+2WK2d1
q
p
jSj+ 3s
Klog2

2jSj
=2WK3
jSjmax
y;kE2
4X
(xi;yi)2Skixi1 (yi=y)
q3
5+2WK2d1
q
p
jSj+ 3s
Klog2

2jSj: (30)
Combine (27)(29)(30), we conclude this proof.Experiments Settings
Datasets and Networks
CIFAR-10:CIFAR-10 contains 60000 points of training data and 10000 of test data with 10 classes. There are 5000 training
images and 1000 test images in each class. We split 300 images in each class from the training set as the validation set. We train
ResNet-18 and WideResNet-34-10 for 100 epochs on CIFAR-10 and set the learning rate as 0.1.
CIFAR-100 :CIFAR-100 contains 60000 points of training data and 10000 of test data with 100 classes. There are 500 training
images and 100 test images in each class. We split 30 images in each class from the training set to form the validation set. We
train ResNet-18 for 100 epochs on CIFAR-100 and set the learning rate as 0.1.
Hyper-parameters used in every method
The model is trained under the perturbation radius train = 8=255. The batch size is 128, perturbation step size is 0.007 and the
number of iterations K= 10 . We use the SGD optimizer. The momentum is 0.9 and the weight decay is 2e-4. We evaluate the
model by PGD-100 and CW attack. For PGD-100, we set perturbation radius test= 8=255and step size is 0.003. For CW,
we set perturbation radius test= 8=255and step size is 0.003. For AutoAttack, we use the standard version of AA and set
perturbation radius test= 8=255.
Following (Xu et al. 2021), we set 1=2= 0:05,1=2= 0:05forFRL on CIFAR-10. The best andare chosen
fromf0:01;0:03;0:05;0:07;0:1gforFRL on CIFAR-100. Moreover, following Benz et al. (2020), we set = 0:05forCSL on
CIFAR-10. The best is chosen fromf0:01;0:03;0:05;0:07;0:1gforCSL on CIFAR-100. For our method, the best is chosen
fromf1e 3;5e 3;1e 2;5e 2;1e 1;5e 1gfor both CIFAR-10 and CIFAR-100.
Hardware SpeciÔ¨Åcation and Environment
Our experiments are conducted on a Ubuntu 64-Bit Linux workstation, having 10-core Intel Xeon Silver CPU (2.20 GHz) and 4
Nvidia GeForce RTX 2080 Ti GPUs with 11GB graphics memory.
Supplementary Experiments
123 4
5
6
7
8 91010255075100
(a) Ours V .S. TRADES
123 4
5
6
7
8 91010255075100
 (b) Ours V .S. CSL
123 4
5
6
7
8 91010255075100
 (c) Ours V .S. FRL-RW
123 4
5
6
7
8 91010255075100
 (d) Ours V .S. FRL-RWRM
Figure 5: Class-wise robust accuracy disparity of all methods using ResNet-18 on CIFAR-10. We compare our method and
another method in terms of the class-wise robust accuracy. We denote the results of our method with a blue line, while the results
of the comparison methods are represented by a purple line. We evaluate robust accuracy under PGD-100 Attack.
In Figure 5, we compare the class-wise robust accuracy evaluated by PGD-100 Attack between our method and all compared
methods on CIFAR-10. As shown in Figure 5(a), we Ô¨Ånd that our method achieves higher robust accuracy of class-4 and class-5
than TRADES, thus our method obtains a good performance on worst-class robust accuracy. In Figure 5(b), CSL performs worse
than our method in most of classes, which leads to a low average robust accuracy. From Figures 5(c) and 5(d), we can see that
our method achieves higher robust accuracy on class-4 than other two baselines, which is the most vulnerable class. Moreover,
our proposed method outperforms other two baselines on class-8 and class-10 signiÔ¨Åcantly, which contributes to the highest pgd
of our method.
In Figure 6, we compare the class-wise robust accuracy evaluated by AutoAttack between our method and all compared
methods on CIFAR-10. As shown in Figure 6(a), we Ô¨Ånd that our method achieves higher robust accuracy of class-4 and class-5
than TRADES, thus our method obtains a good performance on worst-class robust accuracy. In Figure 6(b), CSL performs worse
than our method in most of classes, which leads to a low average robust accuracy. From Figures 6(c) and 6(d), we can see that
our method achieves higher robust accuracy on class-4 than other two baselines, which is the most vulnerable class. Moreover,
our proposed method outperforms other two baselines on class-8 and class-10 signiÔ¨Åcantly, which contributes to the highest AA
of our method.123 4
5
6
7
8 91010255075100
(a) Ours V .S. TRADES
123 4
5
6
7
8 91010255075100
 (b) Ours V .S. CSL
123 4
5
6
7
8 91010255075100
 (c) Ours V .S. FRL-RW
123 4
5
6
7
8 91010255075100
 (d) Ours V .S. FRL-RWRM
Figure 6: Class-wise robust accuracy disparity of all methods using ResNet-18 on CIFAR-10. We compare our method and
another method in terms of the class-wise robust accuracy. We denote the results of our method with a blue line, while the results
of the comparison methods are represented by a purple line. We evaluate robust accuracy under AutoAttack.
More Results on CVand
Table 5: Comparison results between CVandusing ResNet-18 on CIFAR-10.
CIFAR-10 Natural PGD-100 Attack AutoAttack
Method Avg Wst CVnatnat Avg. Wst. CVpgdpgd Avg. Wst. CVAAAA
TRADES 82.11 64.6 0.0090 0 51.69 25.2 0.0250 0 48.64 21.7 0.0278 0
FRL-RW 81.75 69.2 0.0148 0.067 49.02 30.8 0.0186 0.171 46.08 25.4 0.0222 0.118
FRL-RWRM 80.69 71.4 0.0151 0.088 49.16 32.0 0.0150 0.221 45.94 26.1 0.0181 0.147
CSL 76.29 67.1 0.0018 -0.032 43.30 33.8 0.0024 0.179 40.32 29.2 0.0031 0.175
Ours 80.98 69.5 0.0037 0.062 49.13 36.6 0.0129 0.403 46.04 30.1 0.0155 0.334
From the results in Table 5, we can see that CSL obtains the lowest CVnatvalue, while the average natural accuracy of CSL
is the worst. CVnatis not a good measurement because it does not consider the trade-off between average natural accuracy and
worst-class natural accuracy while natis a more reasonable measurement than CVnatby considering average natural accuracy
and worst-class natural accuracy at the same time. Under both PGD-100 attack and AutoAttack, we Ô¨Ånd the similar result on 
andCV.pgdis a more reasonable measurement than CVpgdandAAis a more reasonable measurement than CVAAas well.