Meta-Learners for Few-Shot Weakly-Supervised Medical
Image Segmentation
Hugo Oliveira, Pedro H. T. Gama, Isabelle Bloch, Roberto Marcondes Cesar Jr
Abstract
Most uses of Meta-Learning in visual recognition are very often applied to image
classiﬁcation, with a relative lack of works in other tasks such as segmentation
and detection. We propose a generic Meta-Learning framework for few-shot
weakly-supervised segmentation in medical imaging domains. We conduct a
comparative analysis of meta-learners from distinct paradigms adapted to few-
shot image segmentation in diﬀerent sparsely annotated radiological tasks. The
imaging modalities include 2D chest, mammographic and dental X-rays, as well
as 2D slices of volumetric tomography and resonance images. Our experiments
consider a total of 9 meta-learners, 4 backbones and multiple target organ seg-
mentationtasks. Weexploresmall-datascenariosinradiologywithvaryingweak
annotation styles and densities. Our analysis shows that metric-based meta-
learning approaches achieve better segmentation results in tasks with smaller
domain shifts in comparison to the meta-training datasets, while some gradient-
and fusion-based meta-learners are more generalizable to larger domain shifts.
Keywords: Meta-Learning, Weakly-Supervised Segmentation, Few-Shot
Learning, Medical Images, Domain Generalization
1. Introduction
Despite the widespread use of deep learning in medical imaging, neural net-
works are still subject to a series of limitations that hamper their use in most
real-world medical settings. The main hurdle in implementing Deep Neural Net-
works (DNNs) [1] into clinical practice is the data-driven nature of these models,
Preprint submitted to Pattern Recognition May 12, 2023arXiv:2305.06912v1  [cs.CV]  11 May 2023which usually require hundreds, or even thousands of samples per class to ﬁt
properly, running the risk of suﬀering from underﬁtting or overﬁtting in small-
data scenarios. A compounding factor to this problem is the presence of domain
shift in real-world cases. In medical imaging, domain shift can be introduced
to a task due to changes in imaging equipment or settings, diﬀerences in the
cohort of training/test samples, imaging and label modalities, etc. Regarding
segmentation tasks, an additional complication in the use of deep learning is the
cost of producing the labels to be fed to the algorithms, as dense pixelwise labels
are known to be expensive and to require highly specialized anatomical knowl-
edge from the physician. In this case, sparse labels fed to weakly-supervised
segmentation algorithms can be a good compromise between annotation cost
and performance [2]. Still, the majority of works on one-/few-shot image seg-
mentation do not consider the case of sparsely labeled images, only focusing on
the case of fully labeled support sets [3].
In this paper, we address the task of domain generalization from Few-shot
Weakly-supervised Segmentation (FWS). In other words, we are interested in
mitigating the limitations of semantic image segmentation with DNNs in prob-
lems with small-data, weak supervision for segmentation, and completely dis-
tinct image and label spaces during training and testing.
One common key point in few-shot learning is to introduce some form of
prior knowledge into the models. A simple solution to this is to pretrain the
model on a larger dataset. In RGB image domains there are several datasets
with thousands or even millions of annotated samples (i.e. ImageNet [4], Insta-
gram 1B [5], Pascal VOC [6], Flickr30k [7], etc.), which can be used to produce
feature extractors to be leveraged in order to achieve good segmentation re-
sults with very few samples. Recently, Self-Supervised Learning (SSL) has been
proven to be a more robust initialization to such tasks [8] than supervised Ima-
geNet pretraining. However, such strategies are often limited to natural images,
not presenting considerable gains in comparison to random initializations for do-
mains such as medical imaging. Another approach to introduce prior knowledge
is to use meta-learning training. Already being successfully used in other pat-
2tern recognition tasks, it has been employed in few-shot image classiﬁcation [9],
with recent approaches for semantic segmentation gaining popularity [10]. Still,
the majority of works rely on ImageNet pretraining as feature extractors, while
also not conducting tests on weak labels. In this scenario, there is currently a
gap in reliable methods for FWS on non-RGB images.
We propose a framework capable of strong pretraining using Meta-Learning
onradiologicalimagesthatdoesnotassumetask-speciﬁcpriors, doesnotrequire
previous pretraining of the backbone prior to our meta-training phase, and,
therefore, can be generalized to any FWS task in radiology. An overview of
the proposed framework can be seen in Figure 1. During its meta-training
phase, it uses a meta-dataset D=fD1;D2;:::;Dng, then it is deployed on
an out-of-distribution (OOD) few-shot target task F, where the single highly
generalizable model , trained via a selective supervised loss function L, is
used as a predictor. As discussed in the following sections, can be trained
in several distinct ways, such as second-order optimization, metric learning,
and late fusion. Diﬀerently from Domain Adaptation (DA) tasks, the solutions
described in this work concern tasks wherein the target domain is related to
the meta-training data, but not available during training. Our tasks are closer
to Domain Generalization [11] scenarios, since our framework uses pixelwise
labeled data from multiple radiology domains (chest X-rays, mammographies,
dental X-rays, etc.) aiming to be generalizable to other medical imaging tasks
(i.e. computed tomography, magnetic resonance, positron emission tomography,
etc.). From the dense ground truth masks of the meta-dataset, we simulate
multiple weak (sparse) annotation styles and densities during the meta-training
in order to enforce agnosticism toward the target weak labeling.
We highlight the main contributions of this work as: 1) a generalizable
approach to porting meta-learners from multiple paradigms (see related work
in Section 2) for image classiﬁcation to be applied to FWS (Section 3); 2) a
detailed performance analysis of the multiple Meta-Learning algorithms being
adapted to multiple FWS in 2D radiology (Section 5.1); and 3) analysis of
multiple annotation styles and sparsity parameters (Section 5.2);
3...
image label weak mask
image label weak mask
image label weak mask
prediction
prediction
prediction
image weak mask predictionDeployMeta-trainingFigure 1: Overview of a meta-learner for FWS being pretrained in an episodic fashion on
multiplerelatedtasks fD1;D2;:::;Dngpertainingtoameta-dataset(top)andbeingdeployed
to a target few-shot task Fwith a sparsely annotated support set.
This work is organized as follows. Section 2 reviews the literature and dis-
cusses the taxonomy adopted in this paper to compare the diﬀerent approaches.
Section 3 describes the assessed meta-learning methods while the experimental
setup is explained in Section 4. The experimental results are presented and
discussed in Section 5, with conclusions presented in Section 6.
2. Related Work
2.1. Meta-Learning for Visual Recognition
Meta-Learning methods leverage multi-task learning in order to improve
generalization for OOD tasks in image classiﬁcation [9]. The idea is that, by
generalizing for multiple datasets and tasks in the meta-training phase, the
model is more well-equipped to deal with fully novel unseen tasks in the deploy-
4mentphase. WehighlightthreedistinctapproachesforachievingMeta-Learning
that are important to this paper, despite other paradigms (such as black-box or
Bayesian approaches) also being common: 1) gradient-based – or optimization-
based – methods, which acquire task-speciﬁc parameters via optimization of
ﬁrst- [12, 13] or second-order derivatives [12, 14, 15]; 2) metric learning [16, 17],
whereininsteadofdirectlypredictingclassprobabilities, methodsfocusonlearn-
ing distances across samples from similar and dissimilar classes; 3) fusion-based
approaches, which leverage the intermediary representations of the support set
to guide the predictions in the query set [18, 19, 20] via identity mapping (i.e.
concatenation, multiplication, addition, etc.).
Gradient-based methods yield – often through second-order optimization
– speciﬁc models for each task Tin a meta-batch. Each subtask Tcomputes its
own temporary parameters, optimized using the task loss LT. Finn et al.[12]
introduce the MAML algorithm, a second-order framework, which updates its
model parameters by taking averages of cost’s gradients of the speciﬁc task
models evaluated on new samples of the task T. Nichol et al.[13] propose the
Reptile algorithm, which only uses ﬁrst-order gradient information by updating
itsweightsinthedirectionofthediﬀerencebetweentaskspeciﬁcparametersand
global parameters. Also optimized via second-order derivatives, MetaSGD [14]
aims to automatically learn step sizes for the SGD optimizer in addition to the
model parameters. Another second-order algorithm similar to the MAML was
introduced by Raghu et al.[15]. Named ANIL, the method follows the MAML
algorithm, with the novelty that in the innerloop, instead of updating all the
parameters, this strategy only updates the ones related to the network output
head, i.e. the last classiﬁcation layers.
Metric-based approaches train a single model in multiple tasksT. The
objective of these approaches is to obtain an agnostic mapper to an embedding
space where similar samples are closer than dissimilar ones. Snell et al.[16]
propose the Prototypical Networks (ProtoNets), a model that tries to learn an
embedding function that computes prototypes to a class (e.g. a d-dimensional
vector that represents a class) and uses the distance to these prototypes for
5inference. For each class, features extracted from samples of their support –
the labeled set of images of a task – are averaged to create its prototype vector.
During inference, the feature extracted from a query image is compared with the
prototypes, and the class of the closest prototype, according to some distance
metric, is assigned to the query.
Fusion-based approaches , ontheotherhand, learnasinglemodel where
informationofsupportsetsisusedtoenhancethepredictionofthequeryimages.
Similarly to metric-based methods, fusion-based approaches for meta-learning
rely on an internal embedding from a neural network. However, instead of com-
puting cross-sample similarities through a distance function (e.g. Euclidean,
cosine, etc.) on the embeddings, such methods perform some form of late fu-
sion (i.e. concatenation, addition, multiplication, cross-attention, etc.) on the
support embeddings/labels and query samples. The Ridge Regression Diﬀer-
entiable Discriminator (R2D2) [19] uses the support embeddings and labels to
trainafullytensoriallogisticorridgeregressorusingleast-squares, whichadmits
closed-form solutions. Similarly, MetaOptNet [18] leverages a highly discrimina-
tive embedding generated from a neural network to train a diﬀerentiable SVM.
In both methods, the regressor obtained from the support data is then applied
to the query samples through a simple matrix-matrix product, resulting in a
few-shot classiﬁer guided by the few labeled support examples.
2.2. Meta-Learning for Image Segmentation
The most successful methods for FWS are Guided Nets [20] and PANets [17].
Guided Nets rely on pretrained backbones to extract features from both support
and query data, and apply late fusion in these embeddings to guide the predic-
tion over the query set from the support codes. By contrast, PANets rely on a
framework similar to ProtoNets [16] to compute prototypes for each class in the
embedding space of the support set instead of leveraging late fusion to guide the
prediction over the query. PANets also introduce Prototype Alignment Regu-
larization (PAR) in the training phase for better label eﬃciency, wherein the
query labels are also used to compute prototypes to predict the segmentations
6of support samples. Similarly to PANets, ProtoSeg [21] also use prototypes
for conducting FWS, but instead of repurposing pretrained backbones, this ap-
proach is trained directly on related tasks from scratch in order to allow for
inference over non-RGB images.
Hendryx et al.[22] adapt the Reptile [13] and First-Order-MAML (FO-
MAML) [12] to the problem of semantic segmentation. Their main contri-
bution, however, is the introduction of the EﬃcientLab architecture, which is
a convolutional network for semantic segmentation. Weakly-supervised Seg-
mentation Learning (WeaSeL) [23] applies the well-known MAML second-order
optimization-based framework [12] to FWS in medical imaging by training it
directly on tasks related to radiology, with the downside of being less eﬃcient
than ﬁrst-order approaches.
Even though multiple works on meta-learners speciﬁcally designed for seg-
mentation have appeared during the last years, there is still a gap in such
methods that can work with weakly-annotated images. Aiming at encourag-
ing a larger use of Meta-Learning for FWS tasks, in the remainder sections of
this text we propose generalizable pipelines for porting meta-learners designed
for image classiﬁcation to weakly-supervised segmentation tasks, and test these
approaches in real-world medical tasks.
3. Meta-Learners for Weakly Supervised Segmentation
We use most of the problem deﬁnitions from Gama et al.[23]. We con-
sider a training dataset Das a set of pairs (x;y)of images x2RHWBwith
dimensions HWandBbands/channels, and semantic labels y2RHW.
For each batch fed to an algorithm there are two partitions, named the support
set (Dsup) and the queryset (Dqry), such thatDsup\Dqry=;. We deﬁne a
segmentation task Tas a tupleT=fDsup;Dqry;tg(or,T=fD;tg), where
tis a target class or set of classes. In our setting, all segmentation tasks are
binary during both meta-training and deployment, so tis a single class that can
be referred to as positive/foreground in opposition to the negative/background
7class.Dqry=fxqry;yqrygis composed of a set of images ( xqry) and associ-
ated densely labeled segmentation ground truth ( yqry), while the support set
Dsup=fxsup;ysupgcontains another subset of images from the same dataset
DasDqry, but paired with a weakly-supervised mask ysup. We employ several
distinct strategies detailed in the supplementary materials of this manuscript to
procedurally acquire the weak labels ysupfrom the dense segmentation masks
of the meta-training datasets.
A few-shot semantic segmentation task Fis a speciﬁc type of segmentation
task. The diﬀerence is that the samples of Dsuphave their labels sparsely
annotated, andthelabelsin Dqryareabsentorunknownduringtraining/tuning.
Moreover, the number of samples k=jDsupjis small, e.g., 20,10or even less.
The problem is then deﬁned as follows. Given a few-shot task Fand a set of
segmentation tasks T=fT1;T2;:::;Tng, we want to segment the images from
Dqry
Fusing information from tasks in Tand information from Dsup
F. We also
require that no pair of image/labels of Fis present in T, in order to assure that
the only semantic information about Fis in its support annotations.
In order to teach the model through supervised inputs during the meta-
training phase, we employ supervised loss functions Lsup. As not all pixels
in an FWS task are labeled, we leverage the pixelwise Selective Cross-Entropy
(SCE) loss function [23, 21] in its binary form to conduct our supervised training
on the labeled pixels in a ground truth yand prediction logits ^y:
Lsce(y;^y) = 1
NNX
j=11j[yjlog^yj+ (1 yj) log(1 ^yj)]: (1)
In Equation 1, Nis the number of labeled pixels in y,jis an index iterating
over all pixels, ^yjis the probability predicted for each pixel jof being classi-
ﬁed as pertaining from the positive class, and 1j2f0;1gis a ﬂag indicating
whether a pixel jhas a valid annotation or not. Lsceis applicable to gradient-,
metric- and fusion-based methods, with only the form of computing the logits
^yjvarying across the diﬀerent paradigms. We employ the SCE loss function in
8our experiments for all methods, as we observed early on that other supervised
segmentation loss functions (i.e. Dice [24], Focal [25], etc.) did not achieve the
same performance as SCE.
Our proposed pipeline for FWS via Meta-Learning was conceived as a gen-
eral strategy to convert algorithms originally designed for image classiﬁcation to
semantic segmentation, in theory being applicable to any gradient-, metric- or
fusion-based method. We leverage knowledge gathered from previous works [23,
21] on FWS using MAML [12] and ProtoNets [16] to generalize the frameworks
to other state-of-the-art Meta-Learning algorithms [14, 19, 15, 18, 13]. Sec-
tions 3.1, 3.2 and 3.3 discuss how each Meta-Learning paradigm can be ported
to FWS tasks.
3.1. Gradient-based Meta-Learning for FWS
Inordertoadaptgradient-basedmeta-learnerstoFWSweemployFCNsand
Encoder-Decoder architectures, and we divide each network architecture into
two distinct parts: 1) a feature extraction component ; and 2) a segmentation
headh.receives support images xsupand outputs embedded representations
of the pixels in these images fsup=(xsup), whilehinputs fsupand outputs
segmentation predictions ^ysup. For both FCNs and Encoder-Decoders, his sim-
ply the last convolutional block responsible for the ﬁnal pixelwise classiﬁcation,
while the feature extractor comprises all previous layers in the architecture –
be it a sequence of Encoder/Decoder blocks or a CNN backbone.
The gradient-based pipeline can be observed in a graphical manner in Fig-
ure 2 for one single inner loop. As one can see, a support image xsupis initially
fed through , generating features fsup, which are then fed to h, yielding a
segmentation prediction ^ysup. The prediction is then compared to the sparsely
supervised ground truth ysup, from the support set, in the few labeled points
available through Lsup. The inner loop update function Uinoperates on the
gradients obtained through ﬁrst- [13] or second-order [12, 14] optimization, de-
pending on the meta-learning algorithm of choice. Uinreturns the task-speciﬁc
feature extractor ?and segmentation head h?, which are then fed with the
9query image xqry, resulting in a prediction ^yqryfor the query set. ^yqryand
yqryare then compared through Lsup, yielding gradients that can be backprop-
agated to meta-models andh.
Figure 2: Graphical illustration of one inner loop iteration for gradient-based FWS methods.
In a real-world implementation of this idea, this procedure is repeated for
all tasks randomly sampled in a meta-batch, each one with distinct ?andh?
parameter sets, and yielding diﬀerent gradients to be backpropagated to and
h. The gradients can then be merged – usually via averaging – to update 
andhto more generalist parameter sets, highly adaptable to multiple tasks at
once. This update to andhfrom the gradients obtained on the query set
from the task-speciﬁc models is the outer loop of the optimization-based meta-
learning algorithm. Additionally, it is highly desirable that the support image –
or, realistically, support batch of images – is fed multiple times through and
h, with the gradients being accumulated to generate ?andh?before feeding
the query set to the task-speciﬁc models. The number of iterations through the
support set used to compute ?andh?is a hyperparameter of gradient-based
meta-learners, often being limited by the amount of memory in the GPU and/or
10training time constraints.
3.2. Metric-based Meta-Learning for FWS
Figure 3: Graphical illustration of one inner loop iteration for metric-based FWS methods.
As discussed in Section 2, metric-learning can also be used to achieve meta-
learning through multi-task meta-training and some clever use of support set
annotations [16, 17, 21] that are leveraged distinctly from an optimization-based
approach. Instead of using the support set to tune task-speciﬁc models that
should perform well on the query, metric-based methods instead use the labels
fromthesupporttocomputeprototypesintheembeddingspace, whichcanthen
be used as pivots to compute distances to query samples. This process is similar
to a k-Nearest Neighbors or a Nearest Class Mean [26] algorithm operating on
the embedding space. This allows for extremely low-shot learning regimes (i.e.
one-shot) and even zero-shot learning, the latter being unfeasible with most
gradient-based methods.
We adapted metric-based meta-learners for FWS following the pipeline pre-
sented in Figure 3 for one single inner loop iteration. Diﬀerently from most
11gradient-based approaches, the feature extractor remains frozen during the
inner loops. The embedded spaces for the support fsupand query fqrysets,
both in RCHW, are fundamental to pixelwise classiﬁcation in this approach,
assumingCoutput channels to . The few support labels available are used to
compute pixel prototypes for each class, similarly to Snell et al.[16], which do
this at an image level. As all tasks are binary in our approach, two centroids
0;12RCare computed for the negative and positive classes, respectively; re-
sulting in an embedded representation esup. Prototypes only take into account
the labeled pixels from ysup, ignoring the unannotated ones from fsup.
The query set features fqry=(xqry)are then projected onto the space
esup, where the distances of each query pixelwise feature vector can be com-
puted in relation to 0and1according to some distance metric d(e.g. Eu-
clidean [16, 21], cosine [17], Mahalanobis, Manhattan, etc.). Logits can then
be computed for each query pixel according to their distances to the centroids
of the negative and positive classes, allowing them to be fed to a supervised
loss functionLsup. The gradients obtained from Lsup(yqry;^yqry)can then be
backpropagated through the pipeline, reaching the trainable parameters . Sim-
ilarly to the gradient-based methods, multiple inner loops such as the one shown
in Figure 3 are conducted in each meta-training iteration, with the gradients
rLsupbeing added or averaged before updating on the outer loop.
3.3. Fusion-based Meta-Learning for FWS
Fusion-based methods work by using the support set images and labels to
“guide” [27, 20] the classiﬁcation or segmentation of the desired classes on the
query set samples via late feature fusion between feature representations fsup
andfqry. Figure 4 exempliﬁes the pipeline of a fusion-based algorithm along
one inner loop, starting with the feature extraction from the support and query
images via – yielding fsupandfqry– and the extraction of features from the
sparse support mask ysup, resulting in fm. Feature representations fmandfsup
are then fused using a function 
and averaged into 1D vector f
, responsible
for guiding in segmenting query images according to the background and
12foregroundclassesin ysup. Theguidingvectoristhenfusedtothequeryfeatures
fqryusing another mapping and resulting in f. The fused feature maps f
canthenbepassedthroughasegmentation head h, resultinginpredictions ^yqry,
which is subsequently fed to Lsup.
avg
Figure 4: Graphical illustration of one inner loop iteration for fusion-based FWS methods.
As shown in Figure 4, some fusion-based approaches also extract features
from the sparsely labeled support segmentation mask ysupthrough a generic
model referred to as m. Guided Nets [20], for instance, repurpose the back-
bonetoshareparameterswith m, eventhoughwefoundthatkeepingdistinct
parameters sets for andmresulted in much more stable pretraining using
this strategy. Another important aspect of fusion-based methods is the choice
of the two functions: 
– that merges support image and label features; and –
responsible for fusing the support and query feature representations. Common
choices for these functions are concatenation, matrix multiplication, addition,
pixelwise multiplication or even trainable attention modules [3].
134. Experimental Setup
Our experiments compare both well-known FWS algorithms in the litera-
ture (PANets [17], Guided Nets [20], WeaSeL [23] – referred to as MAML and
ProtoSeg [21] – referred to as ProtoNets) and novel algorithms based on meta-
learners designed for few-shot classiﬁcation and ported to FWS (MetaSGD [14],
ANIL [15], Reptile [13], R2D2 [19] and MetaOptNet [18]).
We explored diﬀerent neural network architectures as backbones for the
meta-learners,including: U-Net( U)[21,28],EﬃcientLab-6-3( E)[22],DeepLabv3
(D) [29] and an FCN with a ResNet-12 ( R) [18] backbone.
Astheperformanceoffew-shotalgorithmsarenotoriouslyvariableaccording
to the chosen support set samples, all results shown in Section 5 were computed
according to a 5-fold cross-validation procedure, with paired samples for each
fold in all algorithms. As all of our tasks are binary by experimental design, the
simple Intersection over Union (IoU, also known as Jaccard index) between the
positiveandnegativeclasseswasourmainmeasureforassessingtheperformance
of FWS meta-learners.
Our implementation of meta-learners for FWS was coded using the Pytorch1
and learn2learn2libraries. Experiments were conducted in machines running
RTX 2070 GPUs with 8GB of memory, so many hyperparameters for the
meta-learning algorithms (i.e. meta batch size, number of inner loops, num-
ber of ﬁlters in the architectures, adaptation steps in 2ndorder gradient-based
methods, etc.) were chosen aiming to ﬁt this capacity. Whenever possible, we
set the default optimizer for our algorithms as Adam [30], unless in methods
that use quadratic program solvers, such as MetaOptNet [18] or R2D2 [19],
which rely on standard Stochastic Gradient Descent (SGD). As time limitation
was our main consideration in designing the experiments shown in this work,
we leveraged knowledge from previous works [23, 21] to set most of the other
hyperparameters that do not directly aﬀect the memory usage of meta-learners.
1https://pytorch.org/
2http://learn2learn.net/
14Readers can refer to our oﬃcial implementation3for details.
4.1. Datasets and Preprocessing
2D Data: As our methods were adapted solely to 2D data, we use multiple
2D radiological datasets with pixelwise annotations for the meta-training phase
of the meta-learners. We follow the experimental setup of Gama et al.[23,
21], borrowing their radiology datasets for the meta-training and test phases.
Sets of Chest X-Rays (CXRs), Mammographic X-Rays (MXRs), Dental X-Rays
(DXRs) and Digitally Reconstructed Radiographs (DRRs) are used as source
2D datasets. The CXR datasets contain lung, heart and clavicle annotations,
while MXRs are labeled for breast region and pectoral muscle. DXR images
contain reference segmentations for teeth and lower mandible, and DRRs are
labeled for ribs. Not all datasets in each imaging modality are labeled for all
organs, as we used solely public organ pixelwise labels for each dataset.
2D alices from 3D Data: Aiming to properly test the label eﬃciency of the
few-shot meta-learners on fully distinct domains in relation to the meta-training
ones, we also acquired 2D slices from volumetric datasets (e.g. Computed To-
mography – CT; and Magnetic Resonance Imaging – MRI) [31, 32, 33]. In order
to gather consistent 2D slices from the 3D datasets, we used the center slices of
each target organ of interest in the craniocaudal axis of the patients. These data
werefullyabsentfromanymeta-trainingconductedduringthiswork, beingonly
used as target domains/tasks.
Preprocessing: During Meta-Training we preprocess the support and query
images with Contrast Limited Adaptive Histogram Equalization (CLAHE) [34],
followed by random transformations (rotations, horizontal and vertical ﬂips,
etc.). At last, we resize the images to 140140pixels, ﬁnally performing a
random crop to 128128pixels. We noticed that the random data augmenta-
tion operations used in our experiments only worked when they were consistent
acrosswholebatches. Aimingtostandardizetheexperiments,duringthedeploy-
3https://github.com/hugo-oliveira/fsws_metalearning
15ment of our model on the target tasks we do not perform any random operation.
The preprocessing in the deployment stage consists simply in applying CLAHE
and resizing to 128128pixels.
4.2. Experiment Organization
In order to test the eﬃcacy of the meta-learners shown in Section 3 for
FWS, we designed an experimental setup aiming to assess the performance of
each algorithm in multiple radiological image segmentation tasks. We use the
pointsweak annotation style in order to assess the performance of algorithms
in very small data scenarios for two distinct few-shot tasks: Fid– the OpenIST
dataset4in the task of lung segmentation ( OpenIST-lungs ); andFood– the
Panoramic Dental X-Rays [35] in the task of inferior mandible segmentation
(Panoramic-mandible ).
TaskFidrepresentsarelatively“in-distribution” (ID)taskveryclosetomany
domains used during meta-training, as both the Shenzhen/Mongomery sets and
the Chest X-Ray 14 dataset are very similar to the OpenIST data and also con-
tain lung annotations. Task Food, however, is a very out-of-distribution task in
comparison to the meta-training domains, as no other dataset contains segmen-
tation for the inferior mandible and only one other DXR dataset (IVisionLab) is
used during the meta-training for the quite distinct task of teeth segmentation.
In order to avoid the meta-learners simply overﬁtting on the target dataset, we
hold out each target domain in an experiment from the meta-training phase,
assuring that the ﬁrst time the methods/networks have seen each target dataset
is during the test phase.
Additionally to the experiments on ID and OOD 2D images, we tested our
meta-learners pretrained on the 2D CXRs, MXRs and DXRs in four tasks from
three volumetric datasets: 1) StructSeg [31] on the Both Lungs andHearttasks
(StructSeg-lungs andStructSeg-heart ); 2) Medical Imaging Segmentation De-
cathlon (MSD) [32] slices on the Spleentask ( MSD-spleen ); and 3) private data
4https://github.com/pi-null-mezon/OpenIST
16from Oliveira et al.[33] in Cerebellum segmentation on pediatric MRI data
(STAP-cerebellum ). We selected these tasks as targets due to the relatively
large size of these organs in the images in comparison to the whole volumes.
This selection was conducted in order to avoid the inherent diﬃculties of com-
pensating for domain shift in datasets with imbalanced target classes [36].
While the meta-training is conducted with the largest variability possible
for the choice of support/query set samples and weak support masks, we ﬁxed
the seeds of all randomly chosen variables in the sparsiﬁcation algorithms for
the support set of the target task F, forcing them to be exactly the same on a
pixel-level for all samples.
At last, in order to evaluate the performance of the best meta-learners in
other weakly supervised segmentation styles, we conduct a series of experiments
with grid,scribbles, and skeleton annotations, as depicted in Figure 5. These
experiments are presented in Section 5.2 for Panoramic-mandible ,OpenIST-
lungs,StructSeg-lungs ,StructSeg-heart ,MSD-spleen andSTAP-cerebellum .
points grid scribbles skeleton
positive class negative class
image dense maskCXR DXR
Figure 5: Weak annotation styles (4 last columns) studied in this work for two distinct radi-
ological tasks: 1) OpenIST-lungs (top row); and 2) Panoramic-mandible (bottom row).
5. Results and Discussion
5.1. Results Intra Meta-Learning Paradigms
We present initially the results in the pointsannotation style for 1- and 5-
shot scenarios, each one with 4 distinct label conﬁgurations: 1) 1 annotated
17point (p=1) with a dilation radius of 1 (r=1); 2) p=1/r=3; 3) p=5/r=1; and
4) p=5/r=3. These experiments correspond to the Panoramic-mandible task,
where the domain shift is larger than OpenIST-lungs , as the focus of this work is
in Domain Generalization instead of simple cross-dataset transfer learning. This
results in 8 distinct annotation settings, which is the number of result columns
in Tables 1, 2, 3 and 4. Bold values in result tables represent the best overall
results for each label conﬁguration (column), while underlined values highlight
the backbones with the best performance for a meta-learner. All metrics within
0.01 of IoU diﬀerence from the best result are highlighted in those tables in
order to point not only to the best overall algorithm/backbone pairs but also
other strategies with comparable performance.
First, in order to assess the best from scratch baseline backbone, we show the
results for the 4 distinct segmentation architectures in Table 1. From these ini-
tial experiments, we observe that U-Nets and ResNet-12 achieve the best overall
results in the few-shot experiments, motivating us to present only ResNet-12 as
the main baseline with no pretraining for the remaining of our analysis.
Table 1: IoU results for the query set predicted by the from scratch baseline with four seg-
mentation backbones ( U,R,EandD) directly trained on the few-shot weakly-supervised
support samples with the pointsannotation style.
Method1-shot 5-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3p=1
r=1p=1
r=3p=5
r=1p=5
r=3
BaselineU .371 .386.444 .451 .459 .445.530 .545
R.312 .311 .478 .482 .443 .450 .556 .558
E.093 .098 .114 .128 .202 .234 .160 .167
D.273 .276 .231 .219 .224 .186 .243 .225
We present results for algorithms in each Meta-Learning paradigm sepa-
rately in order to sort the more label eﬃcient algorithms for comparisons cross-
paradigms. Tables 2, 3 and 4 show, respectively, the IoU metrics for the
gradient-, metric- and fusion-based methods for the 4 backbones analysed in
this work, as well as the baseline with ResNet-12.
Table 2 shows IoU results for pure 2ndorder [12, 14], pure 1storder [13], as
18well as optimization-based strategies that use both 2ndand1storder gradients
[15]. It is clear that ANIL- Dachieved the best overall results in gradient-based
methods, yielding the best performances in all FWS conﬁgurations. Hence, the
hybrid strategies of mixing 1storder gradients to train the backbone and 2nd
order gradients to train the segmentation head were the most label eﬃcient
gradient-based methods by quite some margin. Most of this strategy’s success
canbeattributedtothelargeramountofadaptationstepsthatcanbecomputed
to the segmentation head(10 adaptation steps inANIL)in comparison toapply-
ing2ndorder gradients to the whole backbone (2 adaptation steps for MAML
and MetaSGD). Thus, with additional GPU memory, one could theoretically
improve the performance of full 2ndorder approaches in theory. However, we
conducted early tests with up to 10 adaptation steps on MAML and MetaSGD
and these approaches tended to apparently overﬁt on the few annotated pixels
with more adaptation steps than 5, possibly due to the larger parameter ca-
pacity in relation to the amount of labeled data being ﬁtted by the 2ndorder
optimization in the whole backbone.
Metric-based methods shown in Table 3, in comparison to the baseline with
ResNet-12 backbone, highlight the superiority of PANets [17] in comparison
to ProtoNets [16] in all scenarios. The better performance of PANets can be
attributed to two factors: the use of the cosine distance instead of Euclidean
distance and the additional PAR regularization proposed by Wang et al.[17] –
further explained in Section 2.2. EﬃcientLab outperformed other backbones in
all but the two most sparsely labeled scenarios (1-shot/p=1), while the other
architecturesperformedquitewellin1-shot/p=1/r=1and1-shot/p=1/r=3, but
lacked the capacity of learning from more annotations as well as PANet- E.
Guided Nets, mainly with a ResNet-12 backbone, had a strong start in ex-
tremely low-data scenarios (i.e. 1-shot/p=1), but were unable to evolve to
learn from more annotations, maintaining their performance close to 0.45 of
IoU all throughout the other experiments. As for R2D2 and MetaOptNet,
their most promising backbones reach relatively similar performances in very
sparsely-annotated scenarios – from 0.36 to 0.39 in 1-shot/p=1 – with the con-
19Table 2: IoU results for ﬁve distinct gradient-based methods (MAML [12, 23], MetaSGD
[14], ANIL [15] and Reptile [13]) with the four segmentation backbones pretrained on our
meta-dataset and tuned on the few-shot weakly-supervised support samples with the points
annotation style.
Method1-shot 5-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3p=1
r=1p=1
r=3p=5
r=1p=5
r=3
Baseline R .312.311.478.482 .443.450.556.558
MAMLU.205 .239 .321 .360 .319 .336 .476 .473
R.260 .269 .387 .394 .373.358 .249 .243
E.179 .185 .253 .254 .206 .207 .301 .304
D.328.335.424.431 .322 .337 .488.501
MetaSGDU.306.351 .306 .304 .301 .300 .411 .435
R.268 .277 .269 .260 .340.390.441.473
E.244 .246 .376.388 .324 .320 .368 .366
D.222 .234 .173 .191 .256 .272 .421 .431
ANILU.327 .323 .369 .385 .361 .366 .475 .473
R.187 .203 .216 .260 .314 .341 .218 .190
E.361 .364 .504 .506 .484 .490 .507 .517
D .454 .451 .532 .541 .546 .552 .619 .627
ReptileU.334.341.328 .326 .349 .355 .365 .386
R.203 .205 .336.354 .394.414.524.536
E.206 .210 .281 .318 .367 .387 .314 .305
D.160 .161 .210 .205 .085 .080 .283 .317
siderable advantage that these architectures are able to continue learning from
the larger amount of annotated data in 1-shot/p=5 and 5-shot. We highlight
that R2D2- E, R2D2- Dand MetaOptNet- E, MetaOptNet- Dshow good perfor-
mances whenever larger amounts of annotated support pixels are provided.
From the results presented in Tables 1, 2, 3 and 4, we chose 7 distinct algo-
rithm/backbonepairsforfurtheranalysinginthefollowingsections: Baseline- R,
ANIL- E/D, PANets- R/E, R2D2- Dand MetaOptNet- D.
5.2. Weak Annotation Styles
While Section 5.1 focused on results for pointsannotations, the present
section shows results for three additional weakly-supervised mask styles: grid,
scribbles andskeleton.
20Table 3: IoU results for two distinct metric-based methods (ProtoNets [16, 21] and PANets
[17]) with the four segmentation backbones pretrained on our meta-dataset and tuned on the
few-shot weakly-supervised support samples with the pointsannotation style.
Method1-shot 5-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3p=1
r=1p=1
r=3p=5
r=1p=5
r=3
Baseline R .312.311.478.482 .443.450.556.558
ProtoNetU.371 .365 .364 .362 .454.462.558.563
R.363 .364 .397 .395 .460.463 .533 .540
E.319 .336 .445.453 .448 .449 .521 .525
D.382.381 .403 .399 .458.456.556.568
PANetU .416 .411.419 .432 .511 .509 .554 .561
R .423 .418.512 .510 .518 .522 .592 .600
E.295 .303 .570 .581 .530 .539 .615 .622
D .419 .421.462 .462 .481 .484 .578 .584
Table 4: IoU results for three distinct fusion-based methods (Guided Nets [20], R2D2 [19] and
MetaOptNet [18]) with the four segmentation backbones pretrained on our meta-dataset and
tuned on the few-shot weakly-supervised support samples with the pointsannotation style.
Method
and
Backbone1-shot 5-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3p=1
r=1p=1
r=3p=5
r=1p=5
r=3
Baseline R .312.311 .478 .482 .443.450.556.558
Guided NetU.015 .013 .020 .012 .000 .000 .000 .000
R .450 .449.447.443 .450.450.449.447
E.295 .298 .286 .306 .272 .272 .272 .291
D.150 .161 .140 .164 .254 .259 .260 .265
R2D2U.320 .324 .292 .341 .390 .419 .531 .550
R.365.365.425 .399 .487 .502 .596 .614
E.268 .272 .385 .422 .529 .526.677 .681
D.362.362.415.423 .436 .460 .706 .715
MetaOptNetU.383.379 .415 .408 .424 .423 .458 .458
R.390.382 .400 .382 .494 .489 .627 .640
E.257 .262 .399 .444 .513.515.659.670
D.358.377.426.448 .435 .486 .654.662
Figure 6 depicts some broader trends applied to all experiments. As ex-
pected, the OpenIST-lungs segmentation task – wherein the domain shift is
smaller both in the pixel- and label-space in comparison to the meta-dataset –
21p=1
r=1p=1
r=3p=5
r=1p=5
r=30.00.20.40.60.81.0
1-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3
5-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3
10-shot = {Panoramic Dental X-Rays, Inferior Mandible}
R2D2Baseline
MetaOptNetANIL P ANet
ResNet-12
Ef ficientLab
DeepLabv3(a)Panoramic-mandible /points .
p=1
r=1p=1
r=3p=5
r=1p=5
r=30.00.20.40.60.81.0
1-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3
5-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3
10-shot = {OpenIST, Both Lungs}
 (b)OpenIST-lungs /points .
s=20
r=1s=20
r=3s=12
r=1s=12
r=30.00.20.40.60.81.0
1-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
5-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
10-shot = {Panoramic Dental X-Rays, Inferior Mandible}
(c)Panoramic-mandible /grid.
s=20
r=1s=20
r=3s=12
r=1s=12
r=30.00.20.40.60.81.0
1-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
5-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
10-shot = {OpenIST, Both Lungs}
 (d)OpenIST-lungs /grid.
c=.1
r=1c=.25
r=1c=.5
r=1c=.1
r=4c=.25
r=4c=.5
r=40.00.20.40.60.81.0
1-shot
c=.1
r=1c=.25
r=1c=.5
r=1c=.1
r=4c=.25
r=4c=.5
r=4
5-shot
c=.1
r=1c=.25
r=1c=.5
r=1c=.1
r=4c=.25
r=4c=.5
r=4
10-shot = {Panoramic Dental X-Rays, Inferior Mandible}
(e)Panoramic-mandible /scribbles .
c=.1
r=1c=.25
r=1c=.5
r=1c=.1
r=4c=.25
r=4c=.5
r=40.00.20.40.60.81.0
1-shot
c=.1
r=1c=.25
r=1c=.5
r=1c=.1
r=4c=.25
r=4c=.5
r=4
5-shot
c=.1
r=1c=.25
r=1c=.5
r=1c=.1
r=4c=.25
r=4c=.5
r=4
10-shot = {OpenIST, Both Lungs}
 (f)OpenIST-lungs /scribbles .
r=2
d=2r=2
d=6r=4
d=2r=4
d=60.00.20.40.60.81.0
1-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
5-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
10-shot = {Panoramic Dental X-Rays, Inferior Mandible}
(g)Panoramic-mandible /skeleton .
r=2
d=2r=2
d=6r=4
d=2r=4
d=60.00.20.40.60.81.0
1-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
5-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
10-shot = {OpenIST, Both Lungs}
 (h)OpenIST-lungs /skeleton .
Figure 6: IoU results for multiple weakly-supervised annotation styles in 1-, 5- and 10-shots
for the best baseline backbone ( R) and the two overall best algorithm/backbone pairs in
each paradigm (ANIL- D, PANets- E, R2D2- Dand MetaOptNet- D). Rows reﬂect distinct
weakly-supervised annotation style, respectively: points,grid,scribbles,skeleton. The left
column represents results for the Panoramic-mandible task (large domain shift), while the
rightmost column depicts results for the OpenIST-lungs . Dotted lines reﬂect the performance
of segmentation algorithms on the weakly-supervised support sets, while the dashed horizontal
lines show the results tuned on the densely annotated support masks. Better viewed in color.
achieves considerably better segmentation performances than the Panoramic-
mandible segmentation. Even in very sparsely labeled support sets (i.e. points
in 1-shot/p=1), some algorithms reach 0.6 or more of IoU, while the 5-shot/p=1
scenario presents performances for PANets with IoU larger than 0.8. All other
annotation styles grid,scribbles andskeleton in their most sparsely annotated
22scenarios even reach 0.9 of IoU for OpenIST-lungs . Similarly to results reported
by Gama et al.[21], metric-based methods seem to be much more suitable to
predict tasks in target domains where the domain shift from the meta-dataset
is small. ANIL- Dfollows PANets quite closely in this task, while fusion-based
approaches reached a lower ceiling in performance on OpenIST-lungs .
One should notice that for OpenIST only annotations in randomly selected
pointsdid not reach around 0.8 of IoU, while all of grid,scribbles andskeleton
did. This quickly reaching ceiling in performance in all but one annotation style
implies that pointsare a less label-eﬃcient way of providing weak annotations.
Dependingonthemeta-learner, pointsinitsmoresparsesetting(1-shot/p=1)
achieves between around 0.45/0.70 of IoU for the Panoramic-mandible and
OpenIST-lungs tasks, respectively, while the highly eﬃcient skeleton annota-
tion style in its more sparse setting (1-shot/r=2) yields 0.60/0.90 for MetaOpt-
Net/PANets. In fact, annotations in skeleton andscribbles seem to me more
eﬃcient than points, mainly due to the fact that the time spent in labeling
single random points in an image is similar to the time spent in delineating a
scribble or a skeleton for a commonly-shaped organ, while also providing more
annotated points to tune the learner into the target task. Both draw-oriented
annotations also provide a relatively clear guide for the algorithm indicating the
organ borders, while randomly-sampled points do not provide this information.
Another clearly seen trend in Figure 6 is the very small performance gap
between sparse (dotted lines) and dense (dashed horizontal lines) annotations.
While the gap is considerably large in Panoramic-mandible for more sparsely
annotated scenarios in all annotation styles, for OpenIST-lungs in all scenarios
andPanoramic-mandible in more densely annotated scenarios it is negligible for
the best meta-learners.
We show in Figure 7 sample segmentation predictions from 4 meta-learners
and a baseline in 2D radiology data. Following the trends shown in Figure 6,
qualitative segmentation predictions in the Panoramic-mandible task are better
performed by the fusion-based methods (R2D2- Dand MetaOptNet- D). As for
the task with small domain shift, the metric-based PANets- Ework better in
23OpenIST-lungs , followed by the gradient-based ANIL- D. One can also derive
from the 4 top lines in Figure 7 that the pointsstyle is the least eﬀective weak
annotation modality for both Panoramic-mandible andOpenIST-lungs , while
scribbles andskeleton labels show overall superior performance for the best
meta-learners in each scenario.
5.3. Results for 2D Slices from Volumetric Data
This section presents our experiments on 2D slices from 3D data, repre-
senting fully OOD tasks in relation to the meta-training datasets. Quantita-
tive results for four tasks ( StructSeg-lungs ,StructSeg-heart ,MSD-spleen and
STAP-cerebellum ) are shown in Figure 8 for two annotation styles: gridand
skeleton. As all 2D slice tasks are considerably more OOD in comparison to
the meta-training datasets, we observe similar results to the ones presented for
thePanoramic-mandible task in Figure 6, with fusion-based methods achiev-
ing higher performance than their counterparts. These best results obtained by
R2D2 and MetaOptNet are followed by the optimization-based ANIL, with the
similarity-based PANets showing performance close to the baseline in the ma-
jority of tasks and annotation styles. Weak gridannotations seemed to perform
better than the skeleton labeling style in 2D slice tasks, possibly due to the
spatial uniformity of such annotations in tasks with large domain shifts, even
though it is often more time-demanding to label a grid of points rather than
simply draw a skeleton and outline of an organ.
Another interesting trend shown in Figures 8b, 8d, 8f and 8h is that the
baseline method was clearly outperformed by all meta-learners in the skele-
tonannotation style for all tasks, but PANets in the StructSeg-lungs task. In
contrast to that, the baseline had a very competitive performance for gridan-
notations, even outperforming all meta-learners in most sparsity conﬁgurations
in Figures 8c ( StructSeg-heart ) and 8e ( MSD-spleen ). Again, we attribute this
better performance of the baseline to the higher density of annotations in grid.
This implies that the baseline is less adaptable than meta-learners in highly
sparse and unorthodox annotation styles.
24pointsPanoramic X-Ray       Inferior Mandible
scribblesSupport Querygrid skeleton pointsOpenIST        Both Lungs
scribbles grid skeletonImage Dense Mask Sparse MaskBaseline
Image Ground T ruthANIL PANet MetaOptNet R2D2
FCN ResNet-12 DeepLabv3 EfficientLab DeepLabv3 DeepLabv3Figure 7: Qualitative results for FWS on two target 10-shot tasks: Panoramic-mandible (top
4 rows) and OpenIST-lungs (bottom 4 rows); for 4 distinct annotation styles: points(1stand
5throws), grid(2ndand 6throws), scribbles (3rdand 7throws) and skeleton (4thand 8th
rows). The positive class is represented in blue and the negative class is shown in red. For each
task/annotation style, we show one sample of the query set ( Fqry). Segmentation predictions
and sample IoU metrics for 4 of the best meta-learners (ANIL- D, PANet- E, MetaOptNet- D
and R2D2- D) and Baseline- Rare shown. Better viewed in color.
25p=1
r=1p=1
r=3p=5
r=1p=5
r=30.00.20.40.60.81.0
1-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3
5-shot
p=1
r=1p=1
r=3p=5
r=1p=5
r=3
10-shot = {Panoramic Dental X-Rays, Inferior Mandible}
R2D2Baseline
MetaOptNetANIL P ANet
ResNet-12
Ef ficientLab
DeepLabv3(a)StructSeg-lungs /grid.
r=2
d=2r=2
d=6r=4
d=2r=4
d=60.00.20.40.60.81.0
1-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
5-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
10-shot = {StructSeg Thorax, Both Lungs}
 (b)StructSeg-lungs /skeleton .
s=20
r=1s=20
r=3s=12
r=1s=12
r=30.00.20.40.60.81.0
1-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
5-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
10-shot = {StructSeg Thorax, Heart}
(c)StructSeg-heart /grid.
r=2
d=2r=2
d=6r=4
d=2r=4
d=60.00.20.40.60.81.0
1-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
5-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
10-shot = {StructSeg Thorax, Heart}
 (d)StructSeg-heart /skeleton .
s=20
r=1s=20
r=3s=12
r=1s=12
r=30.00.20.40.60.81.0
1-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
5-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
10-shot = {Decathlon (Spleen), Spleen}
(e)MSD-spleen /grid.
r=2
d=2r=2
d=6r=4
d=2r=4
d=60.00.20.40.60.81.0
1-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
5-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
10-shot = {Decathlon (Spleen), Spleen}
 (f)MSD-spleen /skeleton .
s=20
r=1s=20
r=3s=12
r=1s=12
r=30.00.20.40.60.81.0
1-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
5-shot
s=20
r=1s=20
r=3s=12
r=1s=12
r=3
10-shot = {STAP, Cerebellum}
(g)STAP-cerebellum /grid.
r=2
d=2r=2
d=6r=4
d=2r=4
d=60.00.20.40.60.81.0
1-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
5-shot
r=2
d=2r=2
d=6r=4
d=2r=4
d=6
10-shot = {STAP, Cerebellum}
 (h)STAP-cerebellum /skeleton .
Figure 8: IoU results for multiple weakly-supervised annotation styles in 1-, 5- and 10-shots
for the best baseline backbone ( R) and the two overall best algorithm/backbone pairs in each
paradigm (ANIL- D, PANets- E, R2D2- Dand MetaOptNet- D). Rows reﬂect distinct tasks, re-
spectively: StructSeg-lungs ,StructSeg-heart ,MSD-spleen andSTAP-cerebellum . Theleftmost
column represents results for gridannotations, while the right column depicts skeleton results.
Dotted lines reﬂect the performance of segmentation algorithms on the weakly-supervised sup-
port sets, while the dashed horizontal lines depict the results tuned on the densely annotated
support masks. Better viewed in color.
Finally, we show segmentation results for three of the 2D slice tasks in Fig-
ure 9. As the domain shift is larger for these tasks, similarly to the Panoramic-
mandible task, R2D2 and MetaOptNet achieved the overall best FWS results in
comparison to the baseline and gradient-/metric-based strategies. More specif-
ically, the metric-based PANet performed particularly poorly in these high
26Baseline
Image Ground TruthANIL PANet MetaOptNet R2D2
FCN ResNet-12 DeepLabv3 EfficientLab DeepLabv3 DeepLabv3scribbles grid skeletonFigure 9: Qualitative results for FWS on three target tasks: StructSeg-lungs ongridanno-
tations (top row), MSD-spleen onscribble annotations (middle row) and STAP-cerebellum
onskeleton annotations (bottom row). Meta-learners and annotation styles are analogous to
Figure 7. Better viewed in color.
domain-shifted tasks, segmenting other organs as the structure of interest in
the majority of cases. This implies that metric-based methods tend to be un-
able to correctly deal with the inherent ambiguity of FWS tasks, possibly due to
the diﬃculties in creating an embedding space that is discriminative to a wide
range of real-world unseen OOD tasks.
6. Conclusion
In this work we proposed a method for generalizing meta-learners from three
distinct paradigms (gradient-based [12, 14, 15, 13], metric-based [16, 17] and
fusion-based [19, 18, 20]) for FWS tasks without requiring ImageNet pretraining
or strong domain-dependent priors. We chose to apply our methodology to
radiology images, even though the same methods should also apply to other
non-RGB domains such as histopathology, remote sensing, seismic images, or
even 1D temporal signals.
For gradient-based methods, we observed that performance peaks when ap-
plying low-cost second-order gradient-based algorithms (i.e. ANIL [15]) in the
segmentation head. These methods are also more computationally eﬀective
27than second-order optimization-based methods applied to the whole network
(i.e. MAML [12] or MetaSGD [14]), which should allow them to be applied in
scenarios that require inference on higher spatial resolution images or even in
3D radiology. Fully ﬁrst-order methods (i.e. Reptile [13]), however, were not
able to reach the same performance as ANIL, even if they are relatively quicker
and more scalable to larger backbones. Future works in gradient-based methods
might include alternative supervised loss functions (i.e. Dice [24] or Focal [25])
and/or more label-eﬃcient segmentation heads [37].
In metric-based meta-learners, the cosine distance and prototype alignment
regularization of PANets [17] proved to be more powerful than the simple Eu-
clidean distance of ProtoNets [16, 21]. Similarity-based methods, however, are
only able to generalize to novel domains that are quite close to the meta-training
domains/tasks, possibly due to the lack of global context in such models. Early
experiments during this work tried to insert pixel location information in the
distance computation with no visible eﬀects for ProtoNets and PANets. Future
research directions might be more successful in integrating local information
with global context in such a way that beneﬁts segmentation tasks (i.e. via
CRFs [38] or visual attention [39]).
Previous works on FWS, such as Guided Nets [20] and PANets [17], proved
unable to adapt to novel domains with large domain shifts in relation to the
meta-training datasets. In such scenarios, fusion-based approaches [19, 18] were
considered the optimal choices, followed by gradient-based methods [15]. These
methodsarealsomorecomputationallyeﬀectivethansecond-orderoptimization-
basedmethodsappliedtothewholenetwork(i.e. MAML[12]orMetaSGD[14]),
which should allow them to be applied in scenarios that require inference on
higher spatial resolution images or even in 3D radiology.
Very challenging segmentation tasks that require context and texture anal-
ysis, with organs that do not have clearly deﬁned borders (such as STAP-
cerebellum [33]) are still quite hard to learn from in few-shot settings. In future
works, we intend to port the methods presented in this letter to fully 3D data
(CTs, MRIs, and PET scans) instead of reducing these volumes to 2D slices.
28We hope that the additional context from the 3rddimension will aid the algo-
rithms in learning these challenging 3D tasks, despite the higher computation
cost involved in learning 3D convolutional kernels.
At last, another major limitation of our pipeline is the need for annotated
data from related tasks, restricting the application of the Meta-Learning pre-
trainingtodomainswhereinlabeleddataisavailableformultipledatasets. Aim-
ing to mitigate this limitation, another promising direction might be to merge
Meta-Learning with SSL pseudolabels in order to eliminate the need of anno-
tated datasets from related domains.
Acknowledgements
TheauthorswouldliketothankFAPESP(grants#2015/22308-2,#2017/50236-
1and#2020/06744-5),SerrapilheiraInstitute(grant#R-2011-37776),andANR
(ANR-FAPESP project #ANR-17-CE23-0021) for their ﬁnancial support for
this research.
References
[1] A. Krizhevsky, I. Sutskever, G. E. Hinton, ImageNet Classiﬁcation with
Deep Convolutional Neural Networks, NIPS 25.
[2] J. Peng, Y. Wang, Medical Image Segmentation with Limited Supervision:
A Review of Deep Network Models, IEEE Access.
[3] H. Oliveira, R. M. Cesar, P. H. Gama, J. A. Dos Santos, Domain General-
ization in Medical Image Segmentation via Meta-Learners, in: Conference
on Graphics, Patterns and Images, Vol. 1, IEEE, 2022, pp. 288–293.
[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, ImageNet: A
Large-scale Hierarchical Image Database, in: CVPR, IEEE, 2009, pp. 248–
255.
29[5] I. Z. Yalniz, H. Jégou, K. Chen, M. Paluri, D. Mahajan, Billion-
scale semi-supervised learning for image classiﬁcation, arXiv preprint
arXiv:1905.00546.
[6] M.Everingham, S.Eslami, L.VanGool, C.K.Williams, J.Winn, A.Zisser-
man, The Pascal Visual Object Classes Challenge: A Retrospective, IJCV
111 (1) (2015) 98–136.
[7] P. Young, A. Lai, M. Hodosh, J. Hockenmaier, From image descriptions to
visualdenotations: Newsimilaritymetricsforsemanticinferenceoverevent
descriptions, Transactions of the Association for Computational Linguistics
2 (2014) 67–78.
[8] L.Jing, Y.Tian, Self-SupervisedVisualFeatureLearningwithDeepNeural
Networks: A Survey, IEEE TPAMI.
[9] T. Hospedales, A. Antoniou, P. Micaelli, A. Storkey, Meta-Learning in
Neural Networks: A Survey, IEEE TPAMI 44 (9) (2021) 5149–5169.
[10] S. Luo, Y. Li, P. Gao, Y. Wang, S. Serikawa, Meta-Seg: A Survey of Meta-
Learning for Image Segmentation, Pattern Recognition (2022) 108586.
[11] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng,
P. Yu, Generalizing to Unseen Domains: A Survey on Domain Generaliza-
tion, IEEE Transactions on Knowledge and Data Engineering.
[12] C. Finn, P. Abbeel, S. Levine, Model-Agnostic Meta-Learning for Fast
Adaptation of Deep Networks, in: ICML, PMLR, 2017, pp. 1126–1135.
[13] A.Nichol, J.Schulman, Reptile: AScalableMetalearningAlgorithm, arXiv
preprint arXiv:1803.02999 2 (3) (2018) 4.
[14] Z. Li, F. Zhou, F. Chen, H. Li, Meta-SGD: Learning to Learn Quickly for
Few-Shot Learning, arXiv preprint arXiv:1707.09835.
30[15] A. Raghu, M. Raghu, S. Bengio, O. Vinyals, Rapid Learning or Feature
Reuse? TowardsUnderstandingtheEﬀectivenessofMAML,arXivpreprint
arXiv:1909.09157.
[16] J. Snell, K. Swersky, R. Zemel, Prototypical Networks for Few-Shot Learn-
ing, NeurIPS 30.
[17] K. Wang, J. H. Liew, Y. Zou, D. Zhou, J. Feng, PANet: Few-shot Image
Semantic Segmentation with Prototype Alignment, in: CVPR, 2019, pp.
9197–9206.
[18] K. Lee, S. Maji, A. Ravichandran, S. Soatto, Meta-Learning with Diﬀeren-
tiable Convex Optimization, in: CVPR, 2019, pp. 10657–10665.
[19] L. Bertinetto, J. F. Henriques, P. H. Torr, A. Vedaldi, Meta-Learning with
Diﬀerentiable Closed-form Solvers, arXiv preprint arXiv:1805.08136.
[20] K. Rakelly, E. Shelhamer, T. Darrell, A. A. Efros, S. Levine, Few-
Shot Segmentation Propagation with Guided Networks, arXiv preprint
arXiv:1806.07373.
[21] P. H. T. Gama, H. N. Oliveira, J. Marcato, J. Dos Santos, Weakly Super-
vised Few-Shot Segmentation Via Meta-Learning, IEEE Transactions on
Multimedia.
[22] S. M. Hendryx, A. B. Leach, P. D. Hein, C. T. Morrison, Meta-Learning
Initializations for Image Segmentation, arXiv preprint arXiv:1912.06290.
[23] P. H. Gama, H. Oliveira, J. A. dos Santos, Learning to Segment Medical
Images from Few-Shot Sparse Labels, in: Conference on Graphics, Patterns
and Images, IEEE, 2021, pp. 89–96.
[24] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully Convolutional Neural
Networks for Volumetric Medical Image Segmentation, in: International
Conference on 3D Vision, IEEE, 2016, pp. 565–571.
31[25] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollár, Focal Loss for Dense
Object Detection, in: ICCV, 2017, pp. 2980–2988.
[26] T. Mensink, J. Verbeek, F. Perronnin, G. Csurka, Distance-based Im-
age Classiﬁcation: Generalizing to New Classes at Near-Zero Cost, IEEE
TPAMI 35 (11) (2013) 2624–2637.
[27] X. Zhang, Y. Wei, Y. Yang, T. S. Huang, SG-One: Similarity Guidance
Network for One-Shot Semantic Segmentation, IEEE Transactions on Cy-
bernetics 50 (9) (2020) 3855–3865.
[28] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional Networks for
BiomedicalImageSegmentation, in: MICCAI,Springer, 2015, pp.234–241.
[29] L.-C. Chen, G. Papandreou, F. Schroﬀ, H. Adam, Rethinking
Atrous Convolution for Semantic Image Segmentation, arXiv preprint
arXiv:1706.05587.
[30] D. P. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, arXiv
preprint arXiv:1412.6980.
[31] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. Van Gin-
neken, A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze, et al., A
Large Annotated Medical Image Dataset for the Development and Evalu-
ation of Segmentation Algorithms, arXiv preprint arXiv:1902.09063.
[32] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B. A.
Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers, et al.,
The Medical Segmentation Decathlon, Nature Communications 13 (1)
(2022) 1–13.
[33] H. Oliveira, L. Penteado, J. L. Maciel, S. F. Ferraciolli, M. S. Takahashi,
I. Bloch, R. C. Junior, Automatic Segmentation of Posterior Fossa Struc-
tures in Pediatric Brain MRIs, in: Conference on Graphics, Patterns and
Images, IEEE, 2021, pp. 121–128.
32[34] S. M. Pizer, E. P. Amburn, J. D. Austin, R. Cromartie, A. Geselowitz,
T. Greer, B. ter Haar Romeny, J. B. Zimmerman, K. Zuiderveld, Adaptive
Histogram Equalization and Its Variations, Computer Vision, Graphics,
and Image Processing 39 (3) (1987) 355–368.
[35] A. H. Abdi, S. Kasaei, M. Mehdizadeh, Automatic Segmentation of
Mandible in Panoramic X-Ray, Journal of Medical Imaging 2 (4) (2015)
044003.
[36] T. M. H. Hsu, W. Y. Chen, C.-A. Hou, Y.-H. H. Tsai, Y.-R. Yeh, Y.-C. F.
Wang, Unsupervised Domain Adaptation with Imbalanced Cross-Domain
Data, in: ICCV, 2015, pp. 4121–4129.
[37] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang, Y. Iwamoto, X. Han, Y.-W.
Chen, J. Wu, UNet 3+: A Full-Scale Connected UNet for Medical Image
Segmentation, in: ICASSP, IEEE, 2020, pp. 1055–1059.
[38] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du,
C. Huang, P. H. Torr, Conditional Random Fields as Recurrent Neural
Networks, in: ICCV, 2015, pp. 1529–1537.
[39] T. Hu, P. Yang, C. Zhang, G. Yu, Y. Mu, C. G. Snoek, Attention-based
Multi-Context Guiding for Few-Shot Semantic Segmentation, in: AAAI
Conference on Artiﬁcial Intelligence, Vol. 33, 2019, pp. 8441–8448.
33