Vision-based Target Pose Estimation with Multiple Markers
for the Perching of UA Vs
By Truong-Dong D O1;3), Nguyen X UAN-MUNG2)and Sung-Kyung H ONG1;2)
1)Department of Convergence Engineering for Intelligent Drone, Sejong University, Seoul, South Korea
2)Faculty of Mechanical and Aerospace Engineering, Sejong University, Seoul, South Korea
3)Department of Aerospace Engineering, Sejong University, Seoul, South Korea
Autonomous Nano Aerial Vehicles have been increasingly popular in surveillance and monitoring operations due to their e ciency
and maneuverability. Once a target location has been reached, drones do not have to remain active during the mission. It is possible
for the vehicle to perch and stop its motors in such situations to conserve energy, as well as maintain a static position in unfavorable
Ô¨Çying conditions. In the perching target estimation phase, the steady and accuracy of a visual camera with markers is a signiÔ¨Åcant
challenge. It is rapidly detectable from afar when using a large marker, but when the drone approaches, it quickly disappears as out
of camera view. In this paper, a vision-based target poses estimation method using multiple markers is proposed to deal with the
above-mentioned problems. First, a perching target with a small marker inside a larger one is designed to improve detection capability
at wide and close ranges. Second, the relative poses of the Ô¨Çying vehicle are calculated from detected markers using a monocular
camera. Next, a Kalman Ô¨Ålter is applied to provide a more stable and reliable pose estimation, especially when the measurement data
is missing due to unexpected reasons. Finally, we introduced an algorithm for merging the poses data from multi markers. The poses
are then sent to the position controller to align the drone and the marker‚Äôs center and steer it to perch on the target. The experimental
results demonstrated the e ectiveness and feasibility of the adopted approach. The drone can perch successfully onto the center of the
markers with the attached 25mm-diameter rounded magnet.
Key Words: vision-based perching, target pose estimation, multiple markers, nano unmanned vehicles, Kalman Ô¨Ålter
1. Introduction
Unmanned Aerial Vehicles (UA Vs), such as drones and mul-
ticopters, have become a popular research topic because of their
high autonomy and maneuverability.1‚Äì4)With the growing pop-
ularity of UA Vs, it is necessary to increase awareness of the en-
vironment while improving Ô¨Çight performance.The small size
and light weight of nano UA Vs make them ideal platforms.5‚Äì8)
However, they have a very limited Ô¨Çight time. Fortunately,
many missions do not require hovering for the entire duration.
Therefore, it is necessary to develop autonomous perching so-
lutions to conserve energy.
Perching refers to supporting the aerial robot‚Äôs weight from
within using grasping, attachment, or embedding techniques.9)
This capability could also be useful for tasks requiring the robot
to maintain a precise, static position, act as a radio relay in dis-
aster zones, or suspend operation during unfavorable weather.
Thus, perching is an appealing ability for aerial vehicles.
The advancement of embedded systems has resulted in a va-
riety of small cameras that can be mounted on UA Vs.10)These
camera modules provide simple, a ordable, and reliable solu-
tions that can greatly improve navigation systems.11, 12)In the
Ô¨Åeld of visual servoing, there is foundational literature cover-
ing control using monocular vision, which discusses the dif-
ferences between Position Based Visual Servoing (PBVS) and
Image-Based Visual Servoing (IBVS).13, 14)The visual servo-
ing approaches15, 16)have shown autonomous perching results
without the use of motion capture but are highly dependent on
objects‚Äô shapes and require the object to initially be in the Ô¨Åeld
of view. Cho et al.17)proposed an algorithm to estimate thepose of the robot based on a set of customized markers located
on the docking object.
ArUco18)marker is a binary Ô¨Åducial marker that composes
an internal binary matrix and a black boundary. Large mark-
ers are good for the distant detection of the camera. However,
when the sensor and the markers are close enough, the markers
can be out of the sensor‚Äôs Ô¨Åeld of view that causing the target
loss phenomenon and the perching failure. In contrast, a small
150 mm
25 mmXY
Z
Type of
markerDictionary ID Size (mm)
Large DICT_4x4_1000  997 150 x 150
Small DICT_ARUCO_ORIGINAL 5 25 x 25
Fig. 1. The designed multi-marker perching target.arXiv:2304.14838v1  [eess.SY]  25 Apr 2023Multi-ArUco markers
Markers 
DetectorPose 
Calculator  Kalman  
FilterPerching T arget Pose Estimator
Pose  
Merging  CameraFig. 2. Block diagram of Perching Target Pose Estimator using multiple markers
marker o ers the advantage of being detectable when the drone
approaches, but it is di cult to identify from a distance.
In this work, we use a camera to detect multiple markers
(ArUco) and then merge them to estimate poses. First, the
perching target is designed to enhance detection capability at
both a wide and close distance. Second, the pose of these mark-
ers was estimated using the image gathered by the vision cam-
era mounted on the vehicle. Third, a Kalman Ô¨Ålter is applied to
provide a more accurate and consistent pose, especially when
measurement data is missing due to transmission losses, cam-
era vibrations, or missing detection data. Next, we introduced
an algorithm for fusing the poses from calculated data. Finally,
we build a testbed for real experiments to validate the strategy
and allow the performance of the adopted techniques to be eval-
uated. The preliminary results indicate the viability of our pro-
posal for autonomous nano-UA Vs perching research.
The remainder of the paper is organized as follows. Sec-
tion 2 provides the methodologies used to accomplish this task.
The experimental results obtained are demonstrated in Section
3. Then, conclusions are presented in Section 4, along with
future research directions.
2. Methodology
The perching target, including a small marker located inside a
large one, is described in Fig. 1. A larger maker‚Äôs size will im-
prove detection accuracy but will also increase computational
complexity and processing time. Thus, we chose simple mark-
ers to detect in accordance with the requirement for quickness.
The larger square marker has a size of 150 mmand belongs to
theDICT 4x4100 dictionary with the ID=997. Meanwhile,
to prevent the detect confusion, the smaller marker is chosen
from DICT ARUCO ORIGINAL with ID=5. It is placed
at the center of the larger marker with 25 mmin the dimen-sion. Both have the same origin and coordinate; the inclusion
of the smaller marker may harm the detection of the larger one.
Nonetheless, in our tests, this problem did not occur.
Figure 2 demonstrates the block diagram of the proposed
multi-markers based perching target pose estimation algorithm.
Before we can begin using the ArUco library for marker de-
tection and pose estimation, the camera must be calibrated to
be able to recover information about the size of the markers
in real-world units and determine the pose of the camera in the
scene with good accuracy. The ArUco module provides this op-
tion through OpenCV19)from which we can obtain the camera
intrinsic parameters and the distortion coe cients.
Through the ArUco library from OpenCV , each obtained im-
age frame ( J) is processed in the computer and once the mark-
ers (Mcorners ) has been detected by the algorithm, the pose of the
quadcopter relative to each marker (cRM;ctM) is calculated by
Perspective-n-Point ( Pnp) solver algorithm20)and is fed to the
Kalman Ô¨Ålter.21)After merging them, a precise relative pose is
achieved so that CrazyÔ¨Çie can align itself towards the perching
target. The output of the proposed pose estimator is comprised
of the relative heading angle (  CF) and translation in x,y, and
z-axis ( xCF;yCF;zCF). All the estimated pose values between
the target and the drone are in the world frame centimeters.
2.1. Noise and incomplete data Ô¨Åltering
Because of the noise introduced by the limited capabilities of
the camera, transmission losses, or loss of detection, it requires
a method for estimating the relative pose of the drone given
the incomplete or noisy data from the images. The Kalman Ô¨Ål-
ter cleans up the measured data and projects the measurement
onto a state estimate. It addresses the problem of predicting
the state of a dynamical system at a discrete-time step k, given
measurements from the current state at the time step k 1 and
its uncertainty matrix. The data that we are looking to esti-
Time update (Pr ediction)Measur ement Update (Corr ection)
Initial estimates: 
Fig. 3. Operation of Kalman Filter.mate and Ô¨Ålter includes the relative yaw-angle (c M) and the
translationctMbetween the body-Ô¨Åxed frame of the quadro-
tor and the reference frame of the detected marker, meaning
that we need to deÔ¨Åne the state vector holding the elements:
cPM=n
 ;Àô ;tx;Àôtx;ty;Àôty;tz;Àôtz;o
, where c;Mis the camera and mark-
ers reference frame respectively. The operation of Kalman Ô¨Ålter
is illustrated in Fig. 3. Having a state vector containing eight
elements imply that we need a state transition vector initially
deÔ¨Åned as:
Fk=266666666666666666666666666666641t0 0 0 0 0 0
0 1 0 0 0 0 0 0
0 0 1 t0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 t0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 t
0 0 0 0 0 0 0 137777777777777777777777777777775(1)
where each non-zero element above the diagonal in each col-
umn of the matrix deÔ¨Ånes the time tbetween the states. The
measurement matrix, His then initiated as:
Hk=26666666666641 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 0 1 03777777777775(2)
where a non-zero value represents the elements of which we
want to measure and estimate. Thereafter, we deÔ¨Åne the state
uncertainty matrix, Rk=k1Inxn, where k1is the uncertainty fac-
tor and nis the number of parameters that we want to estimate,
in this case is four. Lastly, we deÔ¨Åne the process noise matrix,
Qk=k2I2nx2n, where k2is a constant determining the magnitude
of the process noise.
By deÔ¨Åning the Ô¨Ålter this way, we are assuming a constant
velocity, which means we have not accounted for acceleration.
This means that if the detected marker exhibits rapid movement
in the image frame and detection is suddenly lost for several
frames since the control signal to the drone relies on the esti-
mated data given by the Kalman Ô¨Ålter, it will follow a linear
motion proportional to the estimated velocity. As a result, we
reduced the speed for each state element exponentially when
the desired marker was not detected for a number of frames in
a row as bellow:
ÀÜx0
kn(vk)=ÀÜx0
kn(vk 1) (3)
where knnmax=8 is denote a time step with an undetected
marker,=0:85 denote the diminishing factor.2.2. Pose data merging
Three stages of pose estimation is indicate in Fig. 4. The
relative pose of large marker ( M1) and the small marker ( M2) to
the drone are deÔ¨Åned ascPM1=cxM1;cyM1;czM1;c M1Tand
cPM2=cxM2;cyM2;czM2;c M2T, respectively. Three stages
are needed to calculate the relative estimated pose values of the
drone to the target, PCF=xCF;yCF;zCF; CFT, is expressed
as below.
Stage 1: Only large marker ( M1) is detected (Fig. 4a):
S1=n
(z1;z2;zCF)2R3:z2zCFz1o
(4)
The pose of the drone to the perching target is calculated as
PCF=cPM1 (5)
Stage 2: Both large marker ( M1) and small marker ( M2) are
detected (Fig. 4b):
S2=n
(z2;z3;zCF)2R3:z3zCFz2o
(6)
Let deÔ¨Åne the pose vector of ( M1) and ( M2) as below:
cPM12=cxM1;cxM2;cyM1;cyM2;czM1;czM2;c M1;c M2T
(7)
We obtain the pose of the drone to the perching target as
PCF=AcPM12 (8)
where
A=2666666666664x1 x0 0 0 0 0 0
0 0 y1 y0 0 0 0
0 0 0 0 z1 z0 0
0 0 0 0 0 0  1  3777777777775
(9)
x;y;z;	is chosen by applying Least Mean Square
(LMS)22)Ô¨Åltering algorithm that minimizes cost function be-
tween the actual pose and the estimated pose of large marker
(M1) and small marker ( M2) from the camera:
C(n)=Efje(n)j2g (10)
where e(n) is the error at the current state nandEfgdenotes
the expected value.
Stage 3: Only small marker ( M2) is detected (Fig. 4c):
S3=n
(z3;zCF)2R3:zCFz3o
(11)
The pose of the drone to the perching target is determined as:
PCF=cPM2 (12)
a) b) c)
Fig. 4. Three stages of perching target pose estimation.Round Magnet
Steel W ire Rope
b)
c) a)Fig. 5. Experimental testbed.
3. Experiments
3.1. Testbed
Figure 5a) shows the testbed for evaluating the proposed es-
timator. It consists of 2 parts: a perching plate and a lifting
plate. Their four corners are connected by steel wire ropes. On
the perching plane, the designed marker is printed and placed
downward. A round magnet with a diameter of 25 mmis in-
stalled in the center of the perching target of multi-markers. The
lifting plate is plotted in vertical and horizontal lines along the
x and y-axis with one-centimeter spacing. Using this platform,
we can adjust and measure the actual values for reference in x,
y,zdirections.
The drone used in this research is a CrazyÔ¨Çie 2.1 nano UA V
as shown in Fig. 5c). The FPV camera is mounted on drone
and pointed upwards in eye-in-hand conÔ¨Åguration for detect-
ing visual markers and estimating relative pose. Image frame
collected from the camera have a size of 640 x 480 pixels. A
small and strong magnetic perching gear is held on the top of
the drone. The total weight of constructed CrazyÔ¨Çie is only 39
grams, including the battery.
Figure 5b) shows the drone‚Äôs camera view while it is perch-
ing. There are red, green, and blue lines illustrating the z,x,
andy-axis, respectively, with the green square enclosing the de-
tected marker. The left-down corner shows the estimated values
of the target pose in centimeters of x,y,z, and degrees of yaw-
angle.
3.2. Experimental results
Our experiments showed that the large ( M1) and small ( M2)
markers begin to be detected at a maximum distance z1=
115cm, z2=25cm, based on the experimental measurements
performed. In the absence of a large marker, the minimum rel-
ative distance is z3=15cm.
Figure 6 shows plots over a short tracking sequence where
the drone was moved in several di erent positions. This blue
line represents the estimated values obtained after using the
Kalman Ô¨Ålter, whereas the red line shows the raw data out-
put from the ArUco detection and pose estimation function of
a) b)
c) d)Fig. 6. The target pose data after utilizing the Kalman Ô¨Ålter.
OpenCV . In the real cases of perching target pose calculation,
the output data is not consecutive without the Kalman Ô¨Ålter es-
timation. Drone crashes will result from this. As can be seen
upon inspection of the Ô¨Ågure, during the measurement period,
the Kalman Ô¨Ålter is capable of making a good estimate of the
movement when the measurement input data is lost. Especially
att18s, no marker is detected for approximately one second,
and Kalman Ô¨Ålters predict marker movement using velocity at
this time, decreasing the velocity by the factor for each itera-
tion until the maximum number of frames nmaxis reached.
To determine the values of the merging coe cient, the esti-
mated values and actual values of them were collected at ten
random positions and directions. There are 100 consecutive
data values at each point. The LMS algorithm is then used to
Ô¨Ånd the optimal coe cient values with the least amount of er-
ror between actual and estimated data. We achieve x=0:275,
y=0:306,z=0:728, =0:469.
Table 1 shows relative position data ( xCF;yCF;zCF) including
the actual values, estimated values and the peak-to-peak mag-
nitudes of the error between them in centimeters. We measured
and estimated the position values at Ô¨Åve di erent point of the
drone, the peak-to-peak estimated position error magnitudes in
the range of 0.6cm and decreasing when the drone approaches
near and close to the center of the target.
Similarly, we also gathered the relative heading angle in de-
gree ( CF). There is no signiÔ¨Åcant di erence between actual
values and estimated values as shown in Table 2. When the
yaw-angle is close to the straight forward direction, the rounded
error is become zero.
Table 1. The estimation of relative position values in centimeter.
Actual
Position(0, 0, 24) (3, 5, 16) (5,8,11) (10,12,7) (6,-5,18)
Estimated
Position(0.3, -0.3, 24.4) (2.6, 5.3, 16.4) (5.4,8.4, 11.3) (10.5,12.6,7.2) (6.3,-5.4,18.5)
Est. Position
Error(0.3, -0.3, 0.4) (0.4, 0.3, 0.4) (0.4, 0.4, 0.3) (0.5, 0.6, 0.2) (0.3, -0.4, 0.5)
Table 2. The estimation of relative heading angle values in degree.
Actual
Heading Angle175 -160 135 90 30 10 5 0
Estimated
Heading Angle178 -157 137 92 31 9 5 1
Est. Heading Angle
Error3 -3 2 2 1 1 0 04. Conclusion
This research proposed a vision-based target pose estimation
method using multiple markers for high-precision nano drone
perching at both wide and close ranges. The multi-marker was
designed as a perching target, and the image frame was cap-
tured using a monocular camera mounted on CrazyÔ¨Çie. Af-
ter that, the markers will be detected and the pose calculated.
Kalman Ô¨Ålters were applied to Ô¨Åll in the empty gaps caused by
missing transmissions or detection failures. Then the pose from
multiple markers was combined with the optimal coe cients
achieved from the Least Mean Squared algorithm. Finally, a
testbed was created to validate the real-world measurement of
relative distances and heading angles between the vehicle and
target. Experimental results have conÔ¨Årmed the e ciency and
practicality of the presented approach. With this system, it is
possible to estimate the target pose for perching at the millime-
ter level of accuracy. This technique can be utilized for nano
UA Vs, multirotors, and space vehicles in the perching and dock-
ing tasks.
There are still directions for further development to achieve
even more remarkable results. In future works, the proposed
target pose estimator will be used for conducting the perching
of nano drones in disturbance environments.
Acknowledgments
This work was supported by Future Space Navigation &
Satellite Research Center through the National Research Foun-
dation funded by the Ministry of Science and ICT, the Republic
of Korea (2022M1A3C2074404).
This research was supported by the MSIT (Ministry of Sci-
ence and ICT), Korea, under the ITRC (Information Tech-
nology Research Center) support program (IITP-2022-2018-
0-01423) supervised by the IITP (Institute for Information &
Communications Technology Planning & Evaluation).
References
1) M. Silvagni, A. Tonoli, E. Zenerino, and M. Chiaberge: Multipur-
pose UA V for search and rescue operations in mountain avalanche
events, Geomatics, Natural Hazards and Risk, vol. 8, no. 1, pp.
18‚Äì33, Jul. 2016.
2) A. Sarabakha, C. Fu, E. Kayacan, and T. Kumbasar: Type-2 Fuzzy
Logic Controllers Made Even Simpler: From Design to Deployment
for UA Vs, IEEE Transactions on Industrial Electronics, vol. 65, no.
6, pp. 5069‚Äì5077, 2018.
3) Nguyen, N.P.; Xuan Mung, N.; Ha, L.N.N.T.; Hong, S.K. Fault-
Tolerant Control for Hexacopter UA V Using Adaptive Algorithm
with Severe Faults. Aerospace 2022, 9, 304.
4) Nguyen, N. P., Mung, N. X., Thanh, H. L. N. N., Huynh, T. T., Lam,
N. T., and Hong, S. K.: Adaptive sliding mode control for attitude
and altitude system of a quadcopter UA V via neural network. IEEEAccess, 9, 40076-40085, 2021.
5) W. Giernacki, M. Skwierczynski, W. Witwicki, P. Wronski, and P.
Kozierski: CrazyÔ¨Çie 2.0 quadrotor as a platform for research and ed-
ucation in robotics and control engineering, 2017 22nd International
Conference on Methods and Models in Automation and Robotics,
2017.
6) G. A. Garcia, A. R. Kim, E. Jackson, S. S. Kashmiri, and D. Shukla:
Modeling and Ô¨Çight control of a commercial nano quadrotor, 2017
International Conference on Unmanned Aircraft Systems, 2017.
7) Lee, J.-W., Xuan-Mung, N., Nguyen, N. P., and Hong, S. K.: Adap-
tive altitude Ô¨Çight control of quadcopter under ground e ect and
time-varying load: theory and experiments. Journal of Vibration and
Control, 2021.
8) ‚ÄúBitcraze Wiki,‚Äù projects crazyÔ¨Çie2: index [Bitcraze Wiki]. [On-
line]. Available: https: //wiki.bitcraze.io /projects:crazyÔ¨Çie2:index.
[Accessed: 22-Jul-2022].
9) Hang, K., Lyu, X., Song, H., Stork, J. A., Dollar, A. M., Kragic,
D., and Zhang, F.: Perching and resting‚ÄîA paradigm for UA V ma-
neuvering with modularized landing gears. Science Robotics, 4(28),
2019.
10) Xuan-Mung, N., Nguyen, N. P., Pham, D. B., Dao, N. N., and Hong,
S. K.: Synthesized Landing Strategy for Quadcopter to Land Pre-
cisely on a Vertically Moving Apron. Mathematics, 10(8), 1328,
2022.
11) Do, T. D., Duong, M. T., Dang, Q. V ., & Le, M. H.: Real-time self-
driving car navigation using deep neural network, 2018 4th Interna-
tional Conference on Green Technology and Sustainable Develop-
ment (GTSD) (pp. 7-12), IEEE, 2018.
12) Tran, L. A., Le, N. P., Do, T. D., & Le, M. H.: A Vision-based
Method for Autonomous Landing on a Target with a Quadcopter,
2018 4th International Conference on Green Technology and Sus-
tainable Development (GTSD) (pp. 601-606), IEEE, 2018.
13) F. Chaumette and S. Hutchinson: Visual servo control. I. Basic ap-
proaches, IEEE Robotics & Automation Magazine, vol. 13, no. 4,
pp.82‚Äì90, 2006.
14) S. Hutchinson, G. Hager, and P. Corke: A tutorial on visual servo
control, IEEE Transactions on Robotics & Automation, vol. 12, no.
5, pp. 651‚Äì670, 1996.
15) J. Thomas, G. Loianno, K. Daniilidis, and V . Kumar: Visual servo-
ing of quadrotors for perching by hanging from cylindrical objects,
IEEERobotics and Automation Letters, vol. 1, no. 1, pp. 57‚Äì64,
2016.
16) H. Zhang, B. Cheng, and J. Zhao: Optimal trajectory generation
for time-to-contact based aerial robotic perching, Bioinspiration &
Biomimetics, vol. 14, 2018.
17) S. Cho, S. Huh, and D. Shim: Visual detection and servoing for
automated docking of unmanned spacecraft, vol. 12, pp. a107‚Äìa116,
2014.
18) S. Garrido-Jurado, R. Mu Àúnoz-Salinas, F. J. Madrid-Cuevas, and M.
J. Mar ¬¥ƒ±n-Jim ¬¥enez: Automatic generation and detection of highly re-
liable Ô¨Åducial markers under occlusion, Pattern Recogn, 47-6, 2014.
19) Bradski G.: The OpenCV Library, Dr Dobb&#x27 s, Journal of
Software Tools, 2000;
20) Lepetit, V ., Moreno-Noguer, F., & Fua, P. : EPnP: An accurate O(n)
solution to the PnP problem, International Journal of Computer Vi-
sion, 81(2), 155, 2009.
21) Kalman, R.: A New Approach to Linear Filtering and Prediction
Problems, ASME Journal of Basic Engineering, 82, 35-45, 1960.
22) Haykin, S. S., & Widrow, B.: Least-mean-square adaptive Ô¨Ålters,
Hoboken, N.J: Wiley-Interscience, 2003.