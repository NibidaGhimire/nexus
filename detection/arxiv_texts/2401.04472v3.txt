A Survey on Efficient Federated Learning Methods for Foundation Model
Training
Herbert Woisetschl ¨ager1,Alexander Erben1,Shiqiang Wang2,
Ruben Mayer3and Hans-Arno Jacobsen4
1Technical University of Munich
2IBM Research
3University of Bayreuth
4University of Toronto
{herbert.woisetschlaeger, alex.erben }@tum.de, wangshiq@us.ibm.com, ruben.mayer@uni-bayreuth.de,
jacobsen@eecg.toronto.edu
Abstract
Federated Learning (FL) has become an established
technique to facilitate privacy-preserving collabo-
rative training across a multitude of clients. How-
ever, new approaches to FL often discuss their
contributions involving small deep-learning models
only and focus on training full models on clients. In
the wake of Foundation Models (FM), the reality is
different for many deep learning applications. Typ-
ically, FMs have already been pre-trained across a
wide variety of tasks and can be fine-tuned to spe-
cific downstream tasks over significantly smaller
datasets than required for full model training. How-
ever, access to such datasets is often challenging.
By its design, FL can help to open data silos. With
this survey, we introduce a novel taxonomy fo-
cused on computational and communication effi-
ciency, the vital elements to make use of FMs in
FL systems. We discuss the benefits and draw-
backs of parameter-efficient fine-tuning (PEFT) for
FL applications, elaborate on the readiness of FL
frameworks to work with FMs and provide future
research opportunities on how to evaluate genera-
tive models in FL as well as the interplay of privacy
and PEFT.
1 Introduction
Foundation Models (FMs) [Bommasani et al. , 2021 ]have
conquered the deep learning world with unprecedented speed,
enabling generative artificial intelligence for a broad audi-
ence. As FMs have been pre-trained on an extensive data ba-
sis and can be used in multi-modal applications, they perform
well over a wide range of tasks. To specialize these mod-
els on a downstream task, we use fine-tuning that can either
be done over the full model or with parameter-efficient fine-
tuning techniques (PEFT) [Huet al. , 2021; Lester et al. , 2021;
Zaken et al. , 2021 ]. A major advantage is that fine-tuning re-
quires orders of magnitude smaller datasets than pre-training
but benefits from access to a variety of samples pertaining to
a task.Access to a breadth of data has always been challenging
in deep learning, as data owners are typically reluctant to
share their data with service providers. To tackle the data
access challenge, McMahan et al. [2017] introduced Feder-
ated Learning (FL). FL enables privacy-preserving machine
learning over decentralized data without the necessity of shar-
ing input data and does not require high bandwidth client con-
nections. Rather, a set of clients collectively train a model and
send their local model updates to a server that subsequently
aggregates the updates to a global model. In FL applications
that involve small models with less than 1 million parame-
ters, we may spend as much time on communication as we
spend on computation [Yousefpour et al. , 2023 ]. However, it
is desirable to design systems in such a way that computation
takes up the majority of time. Luckily, the larger the models
become, the time spent on training is larger than on commu-
nication [Ryabinin et al. , 2023; Woisetschl ¨ager et al. , 2023;
Beutel et al. , 2020 ]. As such, scaling the model size in FL
systems can be desirable, and this introduces beneficial prop-
erties that can aid us in training large models with several
100 million parameters and beyond. These properties ren-
der FL the perfect choice for fine-tuning FMs for downstream
tasks. FL can provide access to a significantly larger and more
diverse data basis while benefiting from the increased time
spent on the computation of FMs over small models. Yet, the
costs of transmitting model updates remain significant even
with the increased computational load of FMs [Yousefpour et
al., 2023 ], making it a priority to jointly optimize the training
and communication efficiency.
Our survey is the first to study advances in computational
and communication-efficient methods for FM training in FL
applications. Our work contains three distinct contributions:
•A novel taxonomy on FL methods for FM training
focused on the core challenges in computation and
communication . We discover a gap between FL meth-
ods to increase computational efficiency and techniques
to improve communication efficiency. While we see
research on computational efficiency for FM training
and fine-tuning in FL applications, communication effi-
ciency methods are predominantly tailored to full-model
training. Our taxonomy aims to identify synergies be-Accepted for publication at IJCAI 2024. Please find the paper in the official proceedings: https://doi.org/10.24963/ijcai.2024/919arXiv:2401.04472v3  [cs.LG]  5 Sep 2024tween FL methods for FMs and efficient communication
methods.
•Holistic evaluation of existing FL computational ef-
ficiency methods for FMs and communication effi-
ciency techniques . We study how existing techniques
can help drive the adoption of FMs in FL applications
and what needs to be done to render FL frameworks
ready for large models.
•We thoroughly discuss future research directions . We
highlight future directions for research on computational
and communication efficiency as these domains grow
closer together. Also, we show what is necessary to
make FMs in FL applications a reality, especially with
regard to generative tasks and privacy considerations.
Efficient FL Systems for Foundation ModelsComputational EfficiencyCommunication EfficiencyParameter-Efficient Fine TuningPrompt TuningInstruction TuningFull Model (Pre-)Training
QuantizationSparsificationModel PruningGradient ProjectionFull Model CompressionTopicEfficiency LeverEfficiency Method
Figure 1: Our Taxonomy. Foundation Models, in conjunction with
Federated Learning, require efficient computational and communi-
cation methods.
2 Taxonomy
Our taxonomy introduces a novel perspective focused on the
current developments in the field of efficient computational
and communication methods for FL applications intended for
training and fine-tuning FMs. While communication effi-
ciency has been studied extensively in FL, computational ad-
vancements in conjunction with large models are currently
emerging. Our taxonomy is visualized in Figure 1.
2.1 Basics of Federated Learning
To understand the relevancy of our taxonomy, it is key to
briefly introduce the fundamentals of FL and the notations
of this paper.
Usually coordinated by a server, the overall goal of FL is to
collaboratively and iteratively train a DL model across a set
of clients Nand minimize the global loss, where fdenotes
an objective function with parameters w,
min
wf(w):=1
|N||N|X
n=1fn(w). (1)
With this, we create a model that generalizes across all
clients n∈N. Specifically, at the beginning, we commonly
initialize the model weights across clients. Then, clients trainthe model on their local data and return the updated model pa-
rameters to the server. At the same time, each client updates
its local parameters with fixed minibatch size mover one or
more epochs by applying gradient descent steps ∇l(wn
t;m)
to the model,
wn
t+1=wt−η∇l(wn
t;m),∀n∈N. (2)
Subsequently, only the model parameters wn
t+1are com-
municated back to the server.
2.2 Taxonomy Explanation
Our taxonomy is centered on two major challenges in FL: the
computational and communication efficiency levers. While
these challenges have been studied in FL extensively in the
past[Zhang et al. , 2021 ], existing approaches predominantly
focus on small models with <10M parameters. With the
emergence of FMs as the backbone for multi-modal applica-
tions, we need to combine computational and communication
efficiency in FL as these FMs typically come with >1 billion
parameters [Zhang et al. , 2023a ]; a growth factor of 100×
This introduces additional computational load for FL clients,
while their resources are often scarce already [Beutel et al. ,
2020 ]. At the same time, communication loads are also grow-
ing since many parameters need to be transmitted.
Computational Efficiency. We discuss computational effi-
ciency levers along four major categories. Full model training
is used to train large transformer models from the very begin-
ning (Section 3.1). Parameter-efficient fine-tuning techniques
can be utilized to improve a pre-trained FM for a specific
downstream task (Section 3.2). Prompt tuning enables per-
formance improvements of an FM without training the model
itself but by designing textual prompts that we prepend to an
input (Section 3.3). Instruction tuning enables fine-grained
control over the model training process and allows for a high
degree of model specialization for certain downstream tasks
(Section 3.4).
Communication Efficiency. While computationally effi-
cient methods for FM training in FL applications can already
reduce the number of parameters to communicate, there is
still a significant amount of data to be communicated be-
tween clients and servers. For instance, parameter-efficient
fine-tuning methods only require 1–2% of parameters to be
trainable. This still amounts to up to 14M parameters when
working with Alpaca-7B [Zhang et al. , 2023a ], larger than
the majority of models (with <1Mtrainable parameters)
currently being discussed in FL research [Heet al. , 2020;
Beutel et al. , 2020 ].
We discuss communication efficiency methods along two
major categories. (I) Model pruning is a method to commu-
nicate parts of a model between clients and the server, which
resemble the most important parameters for a client (Sec-
tion 4.1). (II) Full model compression is divided into three
sub-categories: First, quantization is a method to decrease the
numeric precision of model parameters. Second, sparsifica-
tion is used as a way to zero out less important model param-
eters. Third, gradient projection transforms high dimensional
parameter matrices in client updates into scalar vectors for
communication (Section 4.2).Training Underlying FL Max. Model Trainable Comms Savings Domain
Paper Regime Aggregation Strategy Params Parameters compared to FMT CV NLP Audio
Centralized Learning FMT, PEFT, PT, IT – ≥7B – –
FedBERT [Tian et al. , 2022 ] FMT Weighted Average 117M 110M 0%
FedCLIP [Luet al. , 2023 ] PEFT Weighted Average 85M 530K >99%
FedPEFT [Sunet al. , 2023 ] PEFT Weighted Average 85M 230K >99%
FedPETuning [Zhang et al. , 2023b ]PEFT Weighted Average 125M 1.25M 99%
SLoRA [Babakniya et al. , 2023 ] PEFT Weighted Average 67M 140K >99%
FedDPA [Yang et al. , 2024 ] PEFT Weighted Average 7B N/A >99%
FedPrompt [Zhao et al. , 2022 ] PT Weighted Average 223M – >99%
FedIT [Zhang et al. , 2023a ] IT Weighted Average 7B – >99%
Table 1: Computational Efficiency Methods for FL Systems and FM. FMs are generally multi-modal and provide a strong performance across
a variety of domains that are well explored in centralized learning but not in FL [Dosovitskiy et al. , 2020; OpenAI, 2023; Yang et al. , 2023 ].
3 Computational Efficiency
This section discusses recent methods to train FMs with FL
methods. We distinguish between full model training (FMT),
as it has been studied frequently in the FL domain, PEFT,
prompt tuning (PT), and instruction tuning (IT). Table 1 sum-
marizes existing computational efficiency methods for FM
training.
3.1 Full Model Training
Generally, full model training is referred to when training all
parameters of a neural network. In this approach, we locally
train a model on all clients with the objective of minimizing
the loss l.
The BERT model is one of the first models to use the
transformer layer architecture – the building blocks of FMs –
to achieve state-of-the-art performance at the time of its re-
lease. Tian et al. [2022] discuss federated pre-training of
BERT-family models with up to 117M parameters by apply-
ing masked language modeling (MLM). In MLM, the loss
is defined over the sum of probabilities Pof predicted to-
kensˆxover the representation function of a masked sentence
g x
M(x)
,
l(wt
n, m) =−X
ˆx∈M(x)logP
ˆxgx
M(x)
,∀n∈N.
(3)
In FL applications, supervised learning can be challeng-
ing as we cannot ensure a proper data labeling process for
supervised learning because we usually cannot access client
data. Here, MLM can be beneficial since it is a self-
supervised learning technique that masks parts of a sentence.
Those masked parts will be used as the prediction target
to automatically create an input and target sample. Fed-
erated pre-training provides a net benefit over training on
local datasets [Ding et al. , 2023; Babakniya et al. , 2023;
Heet al. , 2020 ]. However, the perplexity, an indicator
for quality in large models, yields four orders of magni-
tude worse results for large models (e.g., GPT-2) trained
in a federated fashion than centralized training, which is
a stark indicator of low model quality [Tian et al. , 2022;
Radford et al. , 2019 ]. Nonetheless, for use cases involving
sensitive data and strict privacy regulation, full model pre-
training allows the creation of foundation models based on
federated data.
While FL pre-training has shown some promise, it is brit-
tle and has shown to be worse at model sizes above 100Mparameters [Tian et al. , 2022 ]. the applicability of model
pre-training is currently limited due to the significantly lower
model quality than in centralized training.
3.2 Parameter Efficient Fine-Tuning
Generally, PEFT is used to improve the performance of large
models already trained on a large data corpus further and pro-
vide good performance across various tasks. This is espe-
cially effective since the data required for fine-tuning is or-
ders of magnitude smaller than for pre-training. When ap-
plying PEFT, additional fully connected layers are inserted
into the pre-trained model between the transformer blocks.
While the original model weights are frozen, only the newly
added layers are trained, typically resulting in ≥98% less
communication [Huet al. , 2021 ]. This renders PEFT tech-
niques well-suited for FL applications since they address
computation and communication alike. However, Babakniya
et al. [2023] show in their study that PEFT is more sensitive
towards non-IID data than FMT, but this sensitivity can be
mitigated. PEFT can be applied in FL applications as fol-
lows.
Sparse fine-tuning of pre-trained model parameters. As
communication is a key concern in FL, reducing the num-
ber of parameters to communicate between client and server
has become a priority. One of the most used approaches to
achieve this is BitFit [Zaken et al. , 2021 ]. The technique
freezes almost all model parameters wtand only trains the
bias term band the final classification layer wfinal
t over the
input features at, where the next-layer input features at+1=
at·wt+b. With this technique, it is only required to commu-
nicate bandwfinal
t. The communication load is reduced by
≥99%, i.e., instead of communicating 100 million parame-
ters, only 100 thousand parameters are sent over the network.
Sunet al. [2023] propose FedPEFT, a framework for fed-
erated transformer fine-tuning that freezes the model weights
to retain upstream knowledge within the model and adjust
the systematic error for the downstream task. Their experi-
mental results on vision transformers (ViT-B with 85M pa-
rameters) show on-par performance compared to full model
fine-tuning on non-IID data, all the while reducing communi-
cation by 99.8%.
Adapter-based fine-tuning of additionally added parame-
ters. When aiming to maintain a pre-trained model while
introducing task-specific knowledge, adapter-based fine-
tuning techniques provide strong performance and on-par ef-
ficiency compared to sparse fine-tuning techniques [Houlsbyet al. , 2019 ]. Here are two adapter layers introduced in each
transformer block of a foundation model. The adapter layer
aa
t+1is calculated based on a downstream projection wdown
t
of input feature aa
tinto a lower-dimensional space wdown
t∈
Rd×rfollowed by an upstream projection wup
t∈ Rr×u, re-
sulting in
aa
t+1=wup
t·h(wdown
t·aa
t). (4)
With this, we can fine-tune an FM over much fewer di-
mensions than when fully fine-tuning a model. This saves
both computational resources and communication costs in the
same range as FedPEFT.
An improvement with regard to computational and com-
munication efficiency over additional adapters is low-rank
adapters (LoRA) [Huet al. , 2021 ]. The technique uses a
lower dimensional representation A∈ Rr×u, where uis
the dimension of the next layer after the LoRA adapter and
B∈ Rd×r, where dis the dimension of the previous LoRA
adapter. The weight updates are calculated with wt+1=
wt+ ∆w=wt+BA.r≪min(d, u)casts the weight
update matrices into a much lower dimensionality than in the
original transformer module without the necessity of adding
additional adapters, i.e., LoRA builds an adapter for existing
parameters. However, as Ais initialized randomly to a Gaus-
sian distribution and Bas a zero matrix, this works well for
centralized settings with IID data [Huet al. , 2021 ]. For FL
settings, this initialization method bears the risk of slowing
down the fine-tuning process over non-IID data.
FedCLIP [Luet al. , 2023 ]introduces a PEFT method with
an adjusted FedAvg-based adapter aggregation technique.
Their approach yields significant performance improvements
over vanilla FedAvg and FedProx.
Zhang et al. [2023b] provide a systematic benchmark study
on adapter-based fine-tuning methods in privacy-preserving
FL systems. Their results show that fine-tuning with addi-
tional adapters and LoRA both yield the same benchmark
results regarding model accuracy. However, LoRA requires
66% less communication than additional adapters.
However, both FedCLIP and FedPETuning yield a worse
accuracy than full fine-tuning. SLoRA [Babakniya et al. ,
2023 ]and FedDPA [Yang et al. , 2024 ]are LoRA-based tech-
niques to fine-tune models in non-IID FL settings. Their ap-
proach parameterizes the weight update based on r,
wt+1=wt+β
rBA. (5)
Asβdepends on r, scaling the ratio helps control the
weight update impact of a single client. Subsequently, this
can be used to control inconsistent training updates caused
by non-IID data. To practically achieve this, Babakniya et
al.[2023] make use of a two-stage process: First, they used
singular vector decomposition on AandBto obtain a com-
mon initialization point for LoRA across all clients in an FL
system. Second, the training is facilitated with the commonly
initialized low-rank representations. Their approach achieves
on-par performance with full model fine-tuning. However,
they require a warmup time of approximately 100 FL rounds
for stage 1, which can be very expensive in FL settings as
clients are often unavailable consecutively for such a large
number of FL rounds [McMahan et al. , 2017 ].3.3 Prompt Tuning
Prompt tuning is another efficient method for tuning pre-
trained models to a downstream task. Here, we use bi-
nary sentiments subsequent to masked-language-modeling to
achieve high-quality results [Lester et al. , 2021 ]. As such,
the model remains entirely frozen, and we only tweak the
prompts (a very small number of tokens) that are being
prepended to each embedded input query to improve the out-
put quality. In contrast to fine-tuning, this method does not
interfere with the model architecture or parameters. Lester et
al.[2021] show that the effects of prompt tuning on model
performance in centralized settings become better with larger
models, i.e., for large FMs, prompt tuning bears significant
potential.
Specifically, in FL settings, for each client nthe likelihood
Pfor a desired output ˆxis calculated over prepending train-
able embedded prompts xpto each embedded input x. In the
interactive training process, the prompt is optimized in such a
way that it optimally resembles the local objective of a client,
max( Pn(ˆx|[xp;x])),∀n∈N. (6)
Zhao et al. [2022] introduce FedPrompt, a method to ef-
ficiently communicate federally generated prompts only and
aggregate them such that the global model performance of a
pre-trained model improves for a downstream task. Their ex-
perimental evaluation shows a general sensitivity of prompt
tuning towards data heterogeneity as the model performance
degrades by 5 – 10% for the 100M parameter BERT model
compared to a centrally trained baseline. However, with
RoBERTa Base (124M parameters), the sensitivity dimin-
ishes, and the FL results are on par with centralized training.
The larger T5 Base model (223M parameters) follows this
trend, showing that prompt tuning becomes more effective
with larger model sizes [Lester et al. , 2021 ].
3.4 Instruction Tuning
Some applications work with highly sensitive and protected
data or require a very high model performance. The previ-
ously mentioned fine-tuning techniques may not yield suffi-
cient results in these cases.
This is where instruction tuning comes into play as a tech-
nique that uses high-quality data. For instance, GPT-2 [Rad-
ford et al. , 2019 ]uses Reinforcement Learning with Human
Feedback (RLHF). RLHF is a multi-stage process where an
FM is initially trained on supervised data. In the second step
- reward model training - the FM generates outputs over a
given prompt, which a user then ranks based on their prefer-
ence. With this, the model learns human preferences. In the
third step - proximal policy optimization - the model trains
self-supervised for a maximum reward [Zheng et al. , 2023 ].
With FedIT, Zhang et al. [2023a] study instruction tuning
on LLaMA-7B in an FL application over heterogeneous client
tasks, e.g., learning brainstorming and text summarization on
different clients in a single system at the same time. Their
results show that the additional context on a downstream task
generated with federated instruction tuning provides net ben-
efits over central training only, even in heterogeneous set-
tings. However, these results were only produced over a sin-
gle dataset, Dollybricks-15k.3.5 Discussion
As more open-source FMs become available for unrestricted
use (e.g., Alpaca [Taori et al. , 2023 ], Falcon [Penedo et al. ,
2023 ]), it is unlikely that full model training will be a com-
mon use case since training an FM from scratch is very chal-
lenging, even in a centralized setting.
Therefore, we see a priority in improving upon PEFT for
downstream tasks, for instance, by introducing new and en-
hancing existing algorithms to remove the currently required
warm-up times to improve the performance of LoRA in non-
IID data environments. Computer Vision (CV) applications
may benefit from exploring prompt tuning for vision trans-
formers [Jiaet al. , 2022 ]. Also, we find significant challenges
for instruction tuning as data quality is a general issue in FL
[Longpre et al. , 2023 ], and access to human preferences, as
it is required for RLHF, is hardly available in a real-world
federated setting without incentive schemes.
Furthermore, PEFT also positively affects communication
efficiency by significantly reducing the number of trainable
(and thus communicable) parameters. As such, we now see
computational and communication efficiency growing closer
for FL and FMs, but not to a sufficient degree. As such, there
is still a stark need to develop new approaches to use PEFT
to improve computational and communication efficiency.
4 Communication Efficiency
With FMs, models have exponentially grown from a few mil-
lion to several billion parameters to be able to serve multi-
modal tasks [Bommasani et al. , 2021 ]. For FL, this specif-
ically means that the communication load between clients
and servers has grown significantly, even though with PEFT,
there is not necessarily the need to communicate an entire
model. However, when fine-tuning a billion-parameter FM
with adapter-based methods, we still need to facilitate com-
munication for millions of parameters. For cross-device sce-
narios involving more than 1,000 clients per training round,
the data traffic can quickly overstrain a server’s network ca-
pacity and potentially incur significant communication costs
for data transfer from the edge to a cloud [Xuet al. , 2023;
Erben et al. , 2023 ]. Therefore, efficient communication and
training design is vital for future FL systems. We distin-
guish two major efficiency methods: model pruning and full
model compression. A detailed overview of studies on effi-
cient communication in FL is provided in Table 2.
4.1 Model Pruning
The objective of model pruning (MP) is to retain and commu-
nicate only parts of a DL model that are relevant to a certain
task. The reduction of parameters with this technique reduces
the communication effort [Zhu and Gupta, 2017 ]. However,
the success of pruning highly depends on the underlying data.
Pruning client models without coordination may deny conver-
gence with heterogeneous non-IID data in FL systems. Jiang
et al. [2022] introduce PruneFL, a two-stage procedure to re-
alize model pruning in FL systems. The first stage is carried
out on a powerful client to find a common initialization for
the model and generate the importance-based pruning mask.This mask is then iteratively refined over multiple FL train-
ing rounds under the consideration of all clients. The exper-
imental results with PruneFL show two remarkable results:
(I) The models have a shorter time to accuracy over the same
task with PruneFL than with FedAvg, which is attributable
to the higher degree of model specialization. (II) Since the
model size is reduced, one would expect additional effects
on faster computation, but there is limited hardware support
for sparse matrix multiplications in training as they are re-
quired in PruneFL. Therefore, PruneFL has no computational
benefits to this point. However, this may change with new
hardware, such as sparse Tensor cores that support PruneFL’s
dynamic pruning approach [Zhuet al. , 2019 ]. With FedTiny,
Huang et al. [2022] present an approach that works identi-
cally to PruneFL, except for them swapping the common ini-
tialization procedure with using batch normalization values
of clients to choose a common initialization. Furthermore,
FjORD [Horv ´athet al. , 2021 ]and HeteroFL [Diao et al. ,
2020 ]provide similar approaches to pruning.
Isik et al. [2022] choose a similar approach for pruning
models based on the lottery ticket hypothesis, first introduced
to FL by Li et al. [2020]. Instead of commonly initializing
a pruned model for an FL system, they initialize a random
binary mask based on a shared seed on each client. This
reduces computational efforts in the ramp-up phase. After
an FL training round, each client communicates their binary
mask to the server, which creates a global model based on
the weighted average of those binary masks. With this, an
approximate weight estimate replaces the parameters on the
global model. From client to server, FedPM achieves signif-
icant communication efficiencies. However, the full model
still has to be communicated from the server to the clients,
lowering the net benefit.
Model pruning has also been discussed extensively for FM
fine-tuning outside of FL [Lagunas et al. , 2021; Sanh et al. ,
2020 ]. As existing pruning approaches have shown strong
benefits to delivering on-par model performance compared
to fine-tuning the full model, this is a promising direction to
combine federated PEFT with highly efficient pruning tech-
niques to further enhance communication. Along with prun-
ing, sparse tensor hardware can lower computational loads.
4.2 Full Model Compression
Model pruning is prone to omit segments of a DL model that
may become relevant at a later stage. This originates from
domain shifts [Peng et al. , 2020 ]and might require preserv-
ing the full model with all its parameters. For this, three
frequently discussed techniques for full model compression
in FL systems are Quantization, Sparsification, and Gradient
Projection.
Quantization (Q). The first work towards dynamic quanti-
zation is FedPAQ [Reisizadeh et al. , 2020 ], which combines
FedAvg with strong quantization guarantees, where Qrepre-
sents the quantization term for a local model update,
wt+1=wt+1
|N||N|X
n=1Q(wn
t+1−wt). (7)Enhancement Underlying FL Max. Model Communication Domain
Algorithm Method(s) Aggregation Strategy Parameters Savings vs. FedAvg CV NLP Audio
Centralized Learning MP, Q, S, GP – ≥7B –
FedKSeed [Qinet al. , 2023 ] GP Weighted Average 3B ≥99%
FedOBD [Chen et al. , 2023 ] Q Weighted Average 17M 89%
PruneFL [Jiang et al. , 2022 ] MP Weighted Average 132M 80%
FedPM [Isiket al. , 2022 ] MP Weighted Average 12M 98%
FedTiny [Huang et al. , 2022 ] MP Weighted Average 132M 97%
SoteriaFL [Liet al. , 2022 ] S FedSGD 0.05M N/A
FjORD [Horv ´athet al. , 2021 ] MP Weighted Average 11M 98%
FedPAQ [Reisizadeh et al. , 2020 ]Q FedSGD 0.2M 90%
HeteroFL [Diao et al. , 2020 ] MP Weighted Average 11M 98%
LotteryFL [Liet al. , 2020 ] MP Weighted Average 138M 50%
Table 2: Communication-efficient FL methods. Their centralized learning pendants are often tied to specific domains: CV [Habib et al. ,
2023 ], NLP [Heet al. , 2021 ], and Audio [Perez et al. , 2020 ].
While DL models often operate on full precision (32-bit),
this high degree of detail is not necessarily required [Zhou
et al. , 2018 ]. FedPAQ leverages this to reduce the commu-
nication intensity of FL applications. Qcalculates the opti-
mal float precision of a model update to preserve all required
information: Q(w) =∥w∥ ·sign(w)·ξ(w, s), as proposed
in QSGD by Alistarh et al. [2017]. ξformulates a stochastic
process to dynamically tune s, the level of precision. FedPAQ
has a significantly lower time to accuracy than QSGD, which
is attributable to dynamizing s. However, it must be noted
that dynamic quantization only yields benefits for commu-
nication. Depending on the infrastructure, the model updates
have to be cast back to full precision, creating additional com-
putational overhead on the client and server. Also, the method
has been only tested with small models ( <100K parameters).
FedOBD [Chen et al. , 2023 ]quantizes models with trans-
former block dropout, i.e., the random removal of entire
model blocks. The dropout mechanism is carried out during
training by each client and returns only the top-k most impor-
tant model blocks to communicate. Additionally, FedOBD
includes the ideas discussed in Alistarh et al. [2017] and Rei-
sizadeh et al. [2020] but proposes an optimization problem
out of the stochastic quantization where the trade-off origi-
nates from entropy and update size. The communication re-
quired for FL with a 17M parameter transformer model shows
FedOBD to cut communication cost by 2×vs. FedPAQ and
by8×compared to vanilla FedAvg [Chen et al. , 2023 ].
Sparsification (S). While model pruning and sparsification
technically have the same objective, pruned models do not
necessarily resemble sparse models. A model is sparse once
more than 50% of weights are set to 0[Frankle and Carbin,
2019 ]. However, pruning can also change the model archi-
tecture, i.e., not return the full model. Since it lends its
idea from [Frankle and Carbin, 2019 ], FedPM [Isik et al. ,
2022 ](see Section 4.1) can be considered as a model spar-
sification technique but does not necessarily lead to sparse
networks and may return partial networks. SoteriaFL [Liet
al., 2022 ]guarantees sparse networks while maintaining dif-
ferential privacy. Equation 7 is amended in such a way that
Q(wn
t+1−wt)is replaced by C(wn
t+1+N(0, σ2I))withCre-
sembling a sparse client update through shifted compression
that has proven to improve convergence of DL models in FL
settings compared to direct compression [Mitra et al. , 2021;Mishchenko et al. , 2019 ]. Overall, SoteriaFL mitigates the
trade-off between model utility and compression, i.e., the dif-
ferentially private models converge faster in stricter compres-
sion regimes than previously existing non-compressed differ-
entially private approaches.
Gradient projection (GP). Qin et al. [2023] introduce
FedKSeed tailored to efficiently train FMs in FL applications.
They do so by using seeds in the form of scalar vectors to cre-
ate gradient projections. As only the scalar vectors have to be
transmitted, the total amount of communication is reduced by
≥99% compared to applications that would send the original
multi-dimensional gradients. This is the first technique that
enables communication efficient FL applications with FMs,
regardless of the training regime (pre-training or fine-tuning).
4.3 Discussion
To date, advancements in communication-efficient methods
for FL systems have predominantly focused on training small,
full models. The communication paradigm shifts with the
emergence of FMs in FL applications. With fine-tuning tasks,
we only need to train a small fraction of model parameters,
and thus, only trainable parameters have to be communi-
cated. However, each trainable parameter usually contains
a high degree of information for a downstream task. As such,
the effectiveness of model pruning techniques is unclear as
they would cut away fine-tuned parameters. An unexplored
space in FL research is the pruning of FMs with subsequent
fine-tuning for downstream tasks. Pruning FMs can lead to
smaller transformer layers and, consequently, smaller PEFT
layers with fewer parameters. In turn, this could positively
affect computational and communication efficiency. Full-
model compression techniques do not alter the model struc-
ture but rather reduce the parameter precision. Thus, these
techniques can be used with FL applications and FMs in order
to further reduce the size of the communicated updates. Fur-
thermore, Qin et al. [2023] have shown that gradient projec-
tion is a promising direction to compress model updates with-
out sacrificing significant information. This can be beneficial
for PEFT applications as we have small specific adapters for
downstream tasks. However, the remaining key challenge is
the effect of non-IID data on PEFT. Potential compounding
effects of communication compression and lossy compres-
sion remain open for investigation.Secure Training Communication FM Training / Edge
Framework Aggregation Efficiency Efficiency Fine-Tuning Ready
FLARE DP, HEC
Currently, none of the
Frameworks implements
communication efficient
FL methods.FedML DP, HEC PEFT
FederatedScope PEFT
Flower DP, SMPC PEFT
FATE HEC, SMPC PEFT
Substra
PySyft DP
OpenFL
TFF DP
IBM FL HEC
Table 3: Current capabilities of state-of-the-art FL framework with
respect to our taxonomy and their ability to run in resource-limited
environments. Key: DP = Differential Privacy, HEC = Homomor-
phic Encryption, SMPC = Secure Multi-Party Computation.
5 Are FL Frameworks Ready for FMs?
The backbones for making FL applications available to a
broad audience are FL frameworks implementing recent ad-
vancements in FL research. We investigate widely used
frameworks for their FM readiness and progress in integrating
computational and communication efficiency (Table 3).
In theory, all FL frameworks could handle FMs with suf-
ficient hardware availability. Yet, only some implement ef-
ficient training methods. To further drive the adoption of
FL in times of FMs, the frameworks need to improve both
computationally- and communication-efficient methods for
training. FL frameworks that are characterized by their ac-
tive open-source community, FLARE [Roth et al. , 2022 ],
FATE [Fanet al. , 2023; Liu et al. , 2021 ], FedML [Heet al. ,
2020 ], TensorFlow Federated (TFF) [Google, 2019 ], Feder-
atedScope [Kuang et al. , 2023; Xie et al. , 2023 ]and Flower
[Beutel et al. , 2020 ], have adopted recent advancements in
FL research. The frameworks especially allow for PEFT of
FMs with LoRA. Substra [Galtier and Marini, 2019 ], PySyft
[Ziller et al. , 2021 ], OpenFL [Foley et al. , 2022 ], and IBM
FL[Ludwig et al. , 2020 ], in their versions as of 2023, focus
on training smaller FL tasks with 100K up to a 10M parame-
ters and, therefore, do not provide adapters for FM workloads
with more than 100M parameters. Yet, a consistent observa-
tion across all frameworks is their lack of efficient communi-
cation techniques (e.g., FedOBD). Workloads with FMs will
significantly increase communication costs, and the growing
use cases involving resource-constrained edge and IoT de-
vices require high efficiency for computation and communi-
cation. As such, only those FL frameworks enabling training
efficiency are viable choices for working with FMs.
6 Related Work
While there are ample surveys that provide a broad per-
spective on FL [Liet al. , 2023; Banabilah et al. , 2022;
Liuet al. , 2022; Nguyen et al. , 2021; Zhang et al. , 2021;
Aledhari et al. , 2020 ], there are two closely related surveys to
our work as they also focus on FMs.
In their survey, Zhuang et al. [2023] introduce a broad and
general perspective on FMs and FL. They extensively discuss
data modalities. This includes access to data across a large
number of highly distributed clients and the quality of data
that lives on these clients. Currently, FM training or fine-
tuning requires datasets with high data quality, i.e., the in-structions or texts used for MLM must be curated very care-
fully. Thus, their survey identifies a stark need for methods to
train or fine-tune FMs on a scattered data basis with (highly)
varying data quality. Further, Zhuang et al. [2023] discuss
approaches to integrate FL applications into the lifecycle of
FMs, i.e., how FMs can benefit from a continuously evolv-
ing system. While their survey briefly touches upon compu-
tational efficiency, our study provides an in-depth overview
of state-of-the-art training and fine-tuning techniques to ren-
der FMs in FL applications a reality. Furthermore, our study
includes a comprehensive overview of communication tech-
niques that can enhance the adoption of FMs in communica-
tion resource-constrained environments.
Yuet al. [2023] provide an overview of FMs and FL with a
special focus on privacy, an integral component of FL. Their
survey includes a comprehensive overview of different fields
of application for FMs, which they divide into once-off train-
ing and continual learning. The authors elaborate on tech-
nical challenges that may arise for specific use cases, such
as robustness towards unreliable clients, varying data qual-
ity, the degree of non-IID data, and scalability. In contrast,
our survey provides an application-agnostic, in-depth study
of existing methods suitable for FM training. Our focus is
to outline the technical challenges that currently hinder the
operationalization of FM for use cases in federated applica-
tions.
7 Conclusions & Future Directions
In this paper, we survey the current landscape of computa-
tional and communication efficiency methods in FL systems
and introduce a novel taxonomy based on the key techniques.
While efficient FL methods have been separate topics on their
own in the past, they become closely intertwined as we start
using FL systems to leverage FMs. Consequently, the follow-
ing three questions arise:
What are good and realistic evaluation strategies for gen-
erative downstream tasks in FL settings where we do not
have control of data? Fine-tuning generative FM requires
high-quality data. However, we do not have access to data on
the clients to monitor data quality before or during training.
As such, estimating data quality is of utmost importance.
How does hyperparameter optimization work for FMs in
continuously evolving FL systems? While hyperparame-
ter optimization in FL has been a key challenge, PEFT adds
additional complexity. As such FL systems must adapt to
the era of FMs by introducing adaptive parameterization for
PEFT techniques that can account for changing environmen-
tal conditions.
We must develop an understanding of the inter-
play between PEFT and privacy in FL systems.
Communication-efficient FL techniques have been studied
for their effect on privacy, but this is still an open topic for
PEFT, PT, and IT. While it is proven that PEFT is more sensi-
tive to data heterogeneity, the effects of perturbation through
differential privacy are still subject to further studies. The
same is true for PT and IT, as both techniques require precise
prompts and instructions, respectively. As such, noise may
have significantly negative effects here, as well.Acknowledgments
This work is partially supported by the Bavarian Ministry
of Economic Affairs, Regional Development and Energy
(Grant: DIK0446/01).
References
[Aledhari et al. , 2020 ]Mohammed Aledhari, Rehma Razzak,
Reza M. Parizi, and Fahad Saeed. Federated learning: A survey
on enabling technologies, protocols, and applications. IEEE
Access , 8:140699–140725, 2020.
[Alistarh et al. , 2017 ]Dan Alistarh, Demjan Grubic, et al. Qsgd:
Communication-efficient sgd via gradient quantization and en-
coding. In Advances in Neural Information Processing Systems ,
volume 30, 2017.
[Babakniya et al. , 2023 ]Sara Babakniya, Ahmed R. Elkordy, et al.
Slora: Federated parameter efficient fine-tuning of language
models, 2023.
[Banabilah et al. , 2022 ]Syreen Banabilah, Moayad Aloqaily, et al.
Federated learning review: Fundamentals, enabling technologies,
and future applications. Information Processing & Management ,
59(6), 2022.
[Beutel et al. , 2020 ]Daniel J. Beutel, Taner Topal, et al. Flower: A
friendly federated learning research framework, 2020.
[Bommasani et al. , 2021 ]Rishi Bommasani, Drew A. Hudson,
et al. On the opportunities and risks of foundation models, 2021.
[Chen et al. , 2023 ]Yuanyuan Chen, Zichen Chen, et al. Fedobd:
Opportunistic block dropout for efficiently training large-scale
neural networks through federated learning. In Proceedings of
the Thirty-Second International Joint Conference on Artificial In-
telligence, IJCAI-23 . IJCAI Org., 2023.
[Diao et al. , 2020 ]Enmao Diao, Jie Ding, et al. Heterofl: Compu-
tation and communication efficient federated learning for hetero-
geneous clients, 2020.
[Ding et al. , 2023 ]Ning Ding, Yujia Qin, et al. Parameter-efficient
fine-tuning of large-scale pre-trained language models. Nature
Machine Intelligence , 5(3):220–235, March 2023.
[Dosovitskiy et al. , 2020 ]Alexey Dosovitskiy, Lucas Beyer, et al.
An image is worth 16x16 words: Transformers for image recog-
nition at scale, 2020.
[Erben et al. , 2023 ]Alexander Erben, Ruben Mayer, and Hans-
Arno Jacobsen. How can we train deep learning models across
clouds and continents? an experimental study, 2023.
[Fanet al. , 2023 ]Tao Fan, Yan Kang, et al. Fate-llm: A industrial
grade federated learning framework for large language models,
2023.
[Foley et al. , 2022 ]Patrick Foley, Micah J Sheller, et al. Openfl: the
open federated learning library. Physics in Medicine & Biology ,
67(21), 2022.
[Frankle and Carbin, 2019 ]Jonathan Frankle and Michael Carbin.
The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR , 2019.
[Galtier and Marini, 2019 ]Mathieu N Galtier and Camille Marini.
Substra: a framework for privacy-preserving, traceable and col-
laborative machine learning, 2019.
[Google, 2019 ]Inc. Google. Tensorflow federated: Machine learn-
ing on decentralized data. https://www.tensorflow.org/federated,
2019. Accessed: 2023-12-01.[Habib et al. , 2023 ]Gousia Habib, Tausifa Jan Saleem, and Brejesh
Lall. Knowledge distillation in vision transformers: A critical
review, 2023.
[Heet al. , 2020 ]Chaoyang He, Songze Li, et al. Fedml: A research
library and benchmark for federated machine learning, 2020.
[Heet al. , 2021 ]Haoyu He, Xingjian Shi, et al. Distiller: A sys-
tematic study of model distillation methods in natural language
processing, 2021.
[Horv ´athet al. , 2021 ]Samuel Horv ´ath, Stefanos Laskaridis, et al.
FjORD: Fair and accurate federated learning under heteroge-
neous targets with ordered dropout. In Advances in Neural In-
formation Processing Systems , 2021.
[Houlsby et al. , 2019 ]Neil Houlsby, Andrei Giurgiu, et al.
Parameter-efficient transfer learning for nlp, 2019.
[Huet al. , 2021 ]Edward J. Hu, Yelong Shen, et al. Lora: Low-rank
adaptation of large language models, 2021.
[Huang et al. , 2022 ]Hong Huang, Lan Zhang, et al. Distributed
pruning towards tiny neural networks in federated learning, 2022.
[Isiket al. , 2022 ]Berivan Isik, Francesco Pase, et al. Sparse ran-
dom networks for communication-efficient federated learning,
2022.
[Jiaet al. , 2022 ]Menglin Jia, Luming Tang, et al. Visual prompt
tuning, 2022.
[Jiang et al. , 2022 ]Yuang Jiang, Shiqiang Wang, et al. Model prun-
ing enables efficient federated learning on edge devices. IEEE
Transactions on Neural Networks and Learning Systems , 2022.
[Kuang et al. , 2023 ]Weirui Kuang, Bingchen Qian, et al.
Federatedscope-llm: A comprehensive package for fine-tuning
large language models in federated learning, 2023.
[Lagunas et al. , 2021 ]Francois Lagunas, Ella Charlaix, et al. Block
pruning for faster transformers. In Proceedings of the 2021 Con-
ference on EMNLP , Online and Punta Cana, Dominican Repub-
lic, November 2021. ACL.
[Lester et al. , 2021 ]Brian Lester, Rami Al-Rfou, and Noah Con-
stant. The power of scale for parameter-efficient prompt tuning,
2021.
[Liet al. , 2020 ]Ang Li, Jingwei Sun, et al. Lotteryfl: Personal-
ized and communication-efficient federated learning with lottery
ticket hypothesis on non-iid datasets, 2020.
[Liet al. , 2022 ]Zhize Li, Haoyu Zhao, et al. Soteriafl: A uni-
fied framework for private federated learning with communica-
tion compression. In Advances in Neural Information Processing
Systems , volume 35, 2022.
[Liet al. , 2023 ]Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu,
Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey
on federated learning systems: Vision, hype and reality for data
privacy and protection. IEEE Transactions on Knowledge and
Data Engineering , 35(4):3347–3366, April 2023.
[Liuet al. , 2021 ]Yang Liu, Tao Fan, et al. Fate: An industrial grade
platform for collaborative learning with data protection. JMLR ,
22(1), 2021.
[Liuet al. , 2022 ]Ji Liu, Jizhou Huang, et al. From distributed ma-
chine learning to federated learning: a survey. Knowledge and
Information Systems , 64(4), 2022.
[Longpre et al. , 2023 ]Shayne Longpre, Le Hou, et al. The flan col-
lection: Designing data and methods for effective instruction tun-
ing, 2023.[Luet al. , 2023 ]Wang Lu, Xixu Hu, et al. Fedclip: Fast general-
ization and personalization for clip in federated learning, 2023.
[Ludwig et al. , 2020 ]Heiko Ludwig, Nathalie Baracaldo, et al. Ibm
federated learning: an enterprise framework white paper v0.1,
2020.
[McMahan et al. , 2017 ]Brendan McMahan, Eider Moore, et al.
Communication-Efficient Learning of Deep Networks from De-
centralized Data. In Proceedings of the 20th International Con-
ference on Artificial Intelligence and Statistics , volume 54 of Pro-
ceedings of Machine Learning Research . PMLR, 2017.
[Mishchenko et al. , 2019 ]Konstantin Mishchenko, Eduard Gor-
bunov, et al. Distributed learning with compressed gradient dif-
ferences, 2019.
[Mitra et al. , 2021 ]Aritra Mitra, Rayana Jaafar, George J. Pappas,
and Hamed Hassani. Linear convergence in federated learning:
Tackling client heterogeneity and sparse gradients. In Advances
in Neural Information Processing Systems , 2021.
[Nguyen et al. , 2021 ]Dinh C. Nguyen, Ming Ding, et al. Federated
learning for internet of things: A comprehensive survey. IEEE
Communications Surveys & Tutorials , 23(3), 2021.
[OpenAI, 2023 ]OpenAI. GPT-4 technical report, 2023.
[Penedo et al. , 2023 ]Guilherme Penedo, Quentin Malartic, et al.
The refinedweb dataset for falcon llm: Outperforming curated
corpora with web data, and web data only, 2023.
[Peng et al. , 2020 ]Xingchao Peng, Zijun Huang, et al. Federated
adversarial domain adaptation. In ICLR , 2020.
[Perez et al. , 2020 ]Andres Perez, Valentina Sanguineti, et al.
Audio-visual model distillation using acoustic images. In Pro-
ceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV) , 2020.
[Qinet al. , 2023 ]Zhen Qin, Daoyuan Chen, et al. Federated full-
parameter tuning of billion-sized language models with commu-
nication cost under 18 kilobytes, 2023.
[Radford et al. , 2019 ]Alec Radford, Jeff Wu, et al. Language mod-
els are unsupervised multitask learners. 2019.
[Reisizadeh et al. , 2020 ]Amirhossein Reisizadeh, Aryan
Mokhtari, et al. Fedpaq: A communication-efficient feder-
ated learning method with periodic averaging and quantization.
InProceedings of the Twenty Third International Conference on
Artificial Intelligence and Statistics , volume 108. PMLR, 2020.
[Roth et al. , 2022 ]Holger R. Roth, Yan Cheng, et al. Nvidia flare:
Federated learning from simulation to real-world. 2022.
[Ryabinin et al. , 2023 ]Max Ryabinin, Tim Dettmers, et al.
Swarm parallelism: Training large models can be surprisingly
communication-efficient, 2023.
[Sanh et al. , 2020 ]Victor Sanh, Thomas Wolf, and Alexander
Rush. Movement pruning: Adaptive sparsity by fine-tuning. In
Advances in Neural Information Processing Systems , volume 33.
Curran Associates, Inc., 2020.
[Sunet al. , 2023 ]Guangyu Sun, Matias Mendieta, et al. Explor-
ing parameter-efficient fine-tuning for improving communication
efficiency in federated learning, 2023.
[Taori et al. , 2023 ]Rohan Taori, Ishaan Gulrajani, et al. Stanford
alpaca: An instruction-following llama model. https://github.
com/tatsu-lab/stanford alpaca, 2023.
[Tian et al. , 2022 ]Yuanyishu Tian, Yao Wan, et al. Fedbert: When
federated learning meets pre-training. ACM Transactions on In-
telligent Systems and Technology , 13(4), 2022.[Woisetschl ¨ager et al. , 2023 ]Herbert Woisetschl ¨ager, Alexander
Erben, et al. Federated fine-tuning of llms on the very edge: The
good, the bad, the ugly, 2023.
[Xieet al. , 2023 ]Yuexiang Xie, Zhen Wang, et al. Federatedscope:
A flexible federated learning platform for heterogeneity. Pro-
ceedings of the VLDB Endowment , 16(5), 2023.
[Xuet al. , 2023 ]Zheng Xu, Yanxiang Zhang, et al. Federated
learning of gboard language models with differential privacy.
2023.
[Yang et al. , 2023 ]Dongchao Yang, Jinchuan Tian, et al. Uniaudio:
An audio foundation model toward universal audio generation,
2023.
[Yang et al. , 2024 ]Yiyuan Yang, Guodong Long, Taoshu Shen,
Jing Jiang, and Michael Blumenstein. Dual-personalizing adapter
for federated foundation models. ArXiv , abs/2403.19211, 2024.
[Yousefpour et al. , 2023 ]Ashkan Yousefpour, Shen Guo, et al.
Green federated learning, 2023.
[Yuet al. , 2023 ]Sixing Yu, J. Pablo Mu ˜noz, and Ali Jannesari.
Federated foundation models: Privacy-preserving and collabo-
rative learning for large models, 2023.
[Zaken et al. , 2021 ]Elad Ben Zaken, Shauli Ravfogel, and Yoav
Goldberg. Bitfit: Simple parameter-efficient fine-tuning for
transformer-based masked language-models, 2021.
[Zhang et al. , 2021 ]Chen Zhang, Yu Xie, et al. A survey on feder-
ated learning. Knowledge-Based Systems , 216, 2021.
[Zhang et al. , 2023a ]Jianyi Zhang, Saeed Vahidian, et al. Towards
building the federated gpt: Federated instruction tuning, 2023.
[Zhang et al. , 2023b ]Zhuo Zhang, Yuanhang Yang, et al. Fed-
PETuning: When federated learning meets the parameter-
efficient tuning methods of pre-trained language models. In
Findings of the Association for Computational Linguistics: ACL
2023 . ACL, 2023.
[Zhao et al. , 2022 ]Haodong Zhao, Wei Du, et al. Fedprompt:
Communication-efficient and privacy preserving prompt tuning
in federated learning, 2022.
[Zheng et al. , 2023 ]Rui Zheng, Shihan Dou, et al. Secrets of rlhf
in large language models part i: Ppo, 2023.
[Zhou et al. , 2018 ]Yiren Zhou, Seyed Moosavi-Dezfooli, et al.
Adaptive quantization for deep neural network. Proceedings of
the AAAI Conference on Artificial Intelligence , 32(1), 2018.
[Zhu and Gupta, 2017 ]Michael Zhu and Suyog Gupta. To prune,
or not to prune: exploring the efficacy of pruning for model com-
pression, 2017.
[Zhuet al. , 2019 ]Maohua Zhu, Tao Zhang, et al. Sparse tensor
core: Algorithm and hardware co-design for vector-wise sparse
neural networks on modern gpus. In Proceedings of the 52nd An-
nual IEEE/ACM International Symposium on Microarchitecture ,
MICRO ’52. ACM, 2019.
[Zhuang et al. , 2023 ]Weiming Zhuang, Chen Chen, and Lingjuan
Lyu. When foundation model meets federated learning: Motiva-
tions, challenges, and future directions, 2023.
[Ziller et al. , 2021 ]Alexander Ziller, Andrew Trask, et al. PySyft:
A Library for Easy Federated Learning . Springer International
Publishing, 2021.