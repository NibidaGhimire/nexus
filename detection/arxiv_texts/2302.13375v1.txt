Perceiving Unseen 3D Objects by Poking the Objects
Linghao Chen Yunzhou Song Hujun Bao Xiaowei Zhou
State Key Lab of CAD&CG, Zhejiang University
Abstract ‚Äî We present a novel approach to interactive 3D
object perception for robots. Unlike previous perception algo-
rithms that rely on known object models or a large amount of
annotated training data, we propose a poking-based approach
that automatically discovers and reconstructs 3D objects. The
poking process not only enables the robot to discover unseen
3D objects but also produces multi-view observations for 3D
reconstruction of the objects. The reconstructed objects are
then memorized by neural networks with regular supervised
learning and can be recognized in new test images. The
experiments on real-world data show that our approach could
unsupervisedly discover and reconstruct unseen 3D objects
with high quality, and facilitate real-world applications such
as robotic grasping. The code and supplementary materials
are available at the project page: https://zju3dv.github.
io/poking_perception/ .
I. INTRODUCTION
3D object perception plays a crucial role in computer
vision and robotics, with numerous real-world applications,
such as grasping, manipulation, and scene understanding.
Most existing methods for object perception either rely on
known object models or a large number of annotated data
for training. Since these approaches are costly and limited
to a single object instance or a few categories presented in
the training data, they are hardly applicable in real-world
scenarios, where many unseen objects may exist. Imagine
that a robot enters a new environment containing some
objects it has never seen before, how would it perceive the
3D objects for subsequent operations?
Typically, humans understand their surroundings through
interactive perception. By interacting with objects in the
scene, such as pushing, grasping, or poking, they can identify
the objects and build their 3D representations, which Ô¨Ånally
serve as a knowledge base to recognize them once presented
again. In this work, we present a novel system that imitates
this human behavior. As shown in Fig. 1, 3D object discovery
is achieved by poking, which enables the system to handle
unseen 3D objects regardless of their shapes, appearances,
categories, and poses. The poking process generates multi-
view observations for the 3D objects by motion, which are
used to reconstruct 3D object models. The reconstructed
models are then memorized through neural networks, which
are used for object recognition on new test images.
SpeciÔ¨Åcally, given a scene with several unseen objects, we
Ô¨Årst generate object proposals through point cloud clustering
based on geometric assumptions, which are then examined by
poking with a robot arm. The poking process prunes immov-
able object proposals and generates multi-view observations
Corresponding author: Xiaowei Zhou.
Discovery by pokingReconstruction
MemorizationFig. 1. The proposed system for 3D objects perception. The poking
process not only enables the system to discover unseen 3D objects but also
provides multi-view observations for object reconstruction. Based on the
reconstructed object models, the objects are memorized by neural networks
for recognizing them on new test images.
of 3D objects. We then use implicit neural representation
learning to reconstruct the objects based on these multi-view
observations, which optimizes geometry, appearance, and
poses simultaneously to yield high-quality object models.
Finally, the reconstructed models are memorized through
training a detector or object pose estimator with images
rendered from the models. The memorization process allows
us to recognize and perceive these objects with only one for-
ward pass on a new test image, enabling various downstream
tasks in real-world applications, such as robotic grasping,
manipulation, and scene understanding.
We evaluate our system through experiments in real-
world scenes. The results show that our method can effec-
tively discover unseen 3D objects and reconstruct them with
high quality in terms of geometry, appearance, and poses.
Additionally, the memorized object models enable precise
detection and pose estimation of the objects on new test
images.
II. RELATED WORK
Interactive perception. Currently, most 3D perception tasks
are passive, such as object detection [1]‚Äì[3], object pose
estimation [4, 5], object reconstruction [6, 7], etc. These
methods either rely on known object models or large amounts
of annotated data for training, which limits their applicability
in the real world. In contrast, several recent works in robotics
propose to learn from interaction with the environment [8].
[9] learns to map poking to object motion by random
poking and recording the change in the visual state of
the world. [10, 11] learn the object-centric representation
to build the mapping between physics actions and visual
observations. DensePhysNet [12] and DSR-Net [13] are most
relevant to ours. DensePhysNet [12] proposes to performarXiv:2302.13375v1  [cs.RO]  26 Feb 2023a few dynamic interactions with objects to learn a dense
object representation, and DSR-Net [13] proposes to use
interactive perception to discover, track, and reconstruct
objects simultaneously. However, relying on a set of pre-
deÔ¨Åned object categories or models for training limits their
abilities in generalizing to unseen objects. Recently, several
works in computer vision propose to discover and perceive
3D objects by motion. [14] and [15] propose unsupervised
training approaches to decompose the dynamic scene into the
background and several moving objects using motion cues.
However, all of them struggle with real-world scenes due to
the large gap between synthetic and real-world data in terms
of the visual complexity and diversity of object geometries
and appearances.
Robotic grasping. Traditionally, the simulator Graspit! [16]
generates a grasp through several analytical methods given
the object model. Recent works [17]‚Äì[24] propose learning-
based approaches to learn grasping from a large amount of
labeled data. Given a depth image as input, they predict the
grasp in an end-to-end manner to avoid the difÔ¨Åcult problem
of reconstructing the high-quality object model. However,
the lack of reasoning of object properties, such as geometry
and semantics, limits their applicability in downstream tasks.
To tackle this problem, some methods propose to perform
object reconstruction and grasping simultaneously. [25] uses
the structure of the reconstruction network to classify the
successful rate of grasping and use it as the objective function
for continuous grasp optimization. The reconstruction could
be used to further avoid undesired contact during grasping.
3D reconstruction. Traditionally, the seminal work Kinect-
Fusion [26] proposes to Ô¨Årst estimate the sensor pose using
a coarse-to-Ô¨Åne ICP algorithm and then perform TSDF fu-
sion [27] to obtain the object geometry. MaskFusion [28] and
MidFusion [29] perform instance segmentation before track-
ing and fusion to tackle the problem of reconstructing multi-
ple moving objects. Recently, implicit neural representation
learning has been widely used in the 3D reconstruction.
NeRF [30] is a pioneer work that proposes to use an MLP to
predict color and density for each 3D point, which is learned
by inverse volume rendering. V olSDF [31] and NeuS [32]
propose to predict Signed Distance Function (SDF) instead of
density to increase reconstruction quality. [33, 34] propose to
represent the scene with several neural radiance Ô¨Åelds, each
representing a foreground object or the background, to enable
scene decomposition and editing. BaRF [35], NeRF --[36],
and STaR [37] propose to jointly optimize the parameters
of neural radiance Ô¨Åelds and the relative poses between
the object and the camera to reduce reliance on accurate
camera/object poses in real-world applications.
III. METHOD
Given a 3D scene with several objects, our goal is to
enable a robot to perceive the existence and poses/geometries
of the objects which are never seen before. Our pipeline
consists of three stages: we Ô¨Årst discover the 3D objects bypoking (Sec. III-A), then reconstruct the 3D objects (Sec. III-
B), and Ô¨Ånally memorize them for recognition on new test
images (Sec. III-C).
A. Object discovery by poking
We start by describing the poking process that discovers
the objects in the scene and provides input to the reconstruc-
tion module.
The poking process consists of two stages. The Ô¨Årst stage
generates object proposals in the scene, which are then poked
and examined in the second stage.
Since there exist inÔ¨Ånite poking trajectories without any
prior of object locations, we propose to Ô¨Årst generate some
object proposals and then examine them to reduce the poking
search space which is analogous to the Region Proposal
Network (RPN) in object detection [2, 38]. SpeciÔ¨Åcally,
assuming that objects are always lying on a plane, we Ô¨Årst
perform plane segmentation and then cluster the point clouds
above the plane to obtain the object proposals. The object
proposals are then examined by poking and the ones which
cannot be moved will be treated as negative proposals and
pruned.
After generating object proposals, a robot arm pokes each
object and the process is recorded using an RGB-D camera.
The design of poking trajectory only needs to ensure the
objects to be viewed from an adequate number of viewpoints
and avoids occlusions from the robot arm, which is achieved
by performing multiple iterations of poking in a clockwise
direction. The details of the heuristic-based poking policy
are described in Algorithm 1 of the supplementary material.
Discussion. The utilization of learning-based grasp detection,
where a neural network is employed for grasp detection
followed by object grasping and scanning, is an intuitive
alternative for object discovery in robotics. However, this
approach is plagued by several limitations: 1) Learning-based
grasp detection is limited to the training domain and may fail
on unseen objects and even damage the fragile objects; 2)
Some objects may be too large to be grasped; 3) Grasping
may occlude the object and make the complete reconstruction
difÔ¨Åcult. In contrast, poking is neither limited by object
categories or sizes nor does it introduce severe occlusion.
Another alternative is to obtain multi-view observations by
moving a camera instead of moving the objects in the
scene. However, this approach has difÔ¨Åculty in segmenting
objects from the scenes with complex backgrounds or when
the objects are close to each other. Moreover, it cannot
eliminate the occlusion between objects. In contrast, our
method effectively reduces occlusion, prunes the negative
object proposals and ensures the correct number of objects
thanks to the poking process.
B. Object reconstruction
1) Decomposed neural radiance Ô¨Åelds: Given the RGB-D
video recorded in Sec. III-A, we devise an implicit neural
representation-based approach to reconstruct the objects.
NeRF [30] represents a scene with a neural radiance Ô¨Åeld.
Taking as input a 3D point xand a viewing direction d, amultilayer perceptron (MLP) is used to produce the density
and colorcof the point x. Then the pixel color along a
ray is computed using volume rendering:
^C(r) =NX
i=1Tiici; (1)
whereNis the number of 3D points along the ray r,
r(t) = o+tdis a ray with origin oand direction d,
i= 1 exp( ii),Ti= exp( Pi 1
j=1jj)is the
accumulated transmittance along the ray, and i=ti+1 ti
is the distance between neighboring samples along the ray.
As a single neural radiance Ô¨Åeld could only represent one
static scene, we propose to represent our dynamic scene with
a decomposed neural radiance Ô¨Åeld, in which each sub-Ô¨Åeld
represents a rigid part in the scene (the background or an
object) similar to [34, 37].
Meanwhile, since there is no surface constraint in the
NeRF representation, we follow V olSDF [31] to represent
the object neural radiance Ô¨Åeld as SDF and color for high-
quality reconstruction.
DenotingFb
as the background NeRF, Fk
as thek-th
object V olSDF ( k= 1;;K), andk
t2se(3)as the pose
of thek-th object at frame t, for a point xwith viewing
direction d, the color and density are computed as follows:
c(x)b;(x)b=Fb
(x;d); (2)
c(x)k;d(x)k=Fk
(xo;d); (3)
(x)k=8
<
:1

1 1
2exp
d(x)k

ifd(x)k<0
1
2exp
 d(x)k

ifd(x)k0;(4)
whered(x)kis the signed distance of point x,xo= (k
t) 1x
is the transformed point from the world coordinate to the
object coordinate, and is a learnable parameter.
Then, the pixel color ^C(r)and depth ^D(r)can be com-
puted as follows:
^C(r) =NX
i=1Ti(b
icb
i+KX
k=1k
ick
i); (5)
^D(r) =NX
i=1Tiidi; (6)
whereKis the number of neural radiance Ô¨Åelds, i=b
i+PK
k=1k
iis the composed density of all the neural radiance
Ô¨Åelds for point xi,i= 1 exp( ii),k
i=k
i
ii,b
i=
b
i
ii, and diis the depth of the point xi.
2) Optimizing neural radiance Ô¨Åelds and object motion:
During optimization, we jointly optimize the parameters of
the neural radiance Ô¨Åelds Fb
andFk
and the object poses
k
t.
Given the rendered pixel color ^C(r)and depth ^D(r), we
compute the color loss and depth loss as follows:
Lc=^C(r) C(r); (7)Ld=^D(r) D(r); (8)
wherekkis the 1-norm, C(r)andD(r)are the ground-truth
color and depth of ray r.
Moreover, we apply the Eikonal loss [39] to encourage
dto approximate a signed distance function as suggested
in [31].
Lsdf=Ez(krd(z)k 1)2; (9)
Since the object and the background are in contact, we
Ô¨Ånd it hard to decompose them especially with textureless
background due to its motion ambiguity. Inspired by [40], we
propose the following sparsity loss to solve this problem:
Lsp=wspj1 exp( i)j; (10)
wherewsp= exp( wmax(zm zi;0))is the loss weight
of the sparsity loss, iandziare the density and depth of a
pointxion a ray r,zm= max
tfDt
rgis the maximum depth
of the ray racross all the frames, and wis a weight decay
parameter.
The sparsity loss encourages the density of the object
V olSDF to be small, and wspassigns different weights for
points with different distances to the background surface.
Intuitively, the points on and farther than the background
surface are assigned a large loss weight, while the points
nearer than the background surface are assigned a small one.
This design eliminates the density of objects in unobserved
and ambiguous spaces and reduces the effect of the sparsity
loss on the spaces nearer than the background surface.
Combining the above terms, the total loss function is
L(b;o;o) =w1Lc+w2Ld+w3Lsdf+w4Lsp:(11)
Once the object neural radiance Ô¨Åeld is optimized, the
object mesh is extracted with the marching cubes [41]
operation, and the vertex colors are obtained by averaging the
radiance at the vertex positions under all view directions in
the input video. The segmentation mask could be rendered by
setting the radiance of the object V olSDF to 1 and the density
of the background NeRF to 0. This representation allows the
network to optimize the segmentation mask implicitly and
leads to a more accurate segmentation mask as demonstrated
in Sec. IV-B.
Sampling strategy. Since the region of the objects is rel-
atively small compared to the entire image, we design a
foreground sampling strategy for faster convergence. Repre-
sentingNras the number of pixels to sample over an image,
we propose to sample N=2pixels within the object mask and
the restN=2pixels over the entire image.
Moreover, we Ô¨Ånd it difÔ¨Åcult to decompose the robot
arm and objects since they are in contact during the poking
process. To restrict the impact of the robot, we propose not
to sample pixels within the robot mask, which is obtained by
rendering the robot arm model with its pose in each frame.
Training strategy. To avoid local optima when jointly
optimizing the object poses and the neural radiance Ô¨Åelds,
we initialize the object masks and poses and propose a
stage-wise training strategy. The object mask is initializedReconstructed‚Ä®object modelPVNet
RenderTraining‚Ä¶train pvnet
Synthesized images
Fig. 2. The training pipeline for PVNet based on the reconstructed
object model. The background of the synthesized images are randomly
chosen from the ScanNet dataset.
as the set of pixels whose optical Ô¨Çow norm is larger than a
threshold. The object poses are computed with scene Ô¨Çow
within the object mask and Least-Squares estimation fol-
lowed by Iterative Closest Points (ICP) for reÔ¨Ånement. The
optimization process is divided into 3 stages as follows. First,
the background NeRF is initialized by sampling outside the
robot arm mask and the object mask. Second, the foreground
object V olSDF is initialized by sampling only within the
object mask and the object poses are Ô¨Åxed. Finally, the neural
radiance Ô¨Åelds and the object poses are jointly optimized.
C. Memorizing the 3D objects
The next step following the reconstruction is to memorize
the 3D objects so that they can be rapidly recognized on
new test images. Here, we use the PVNet [4] to demonstrate
how to learn an object pose estimator based on the recon-
structed object model. Taking an RGB image as input, PVNet
predicts the 2D keypoint positions using pixel-wise voting
and computes the object pose with a Perspective-n-Point
(PnP) solver [42]. As shown in Fig. 2, the training images
for the PVNet are obtained by rendering the reconstructed
model at a large number of object poses. At inference time,
ICP is used to reÔ¨Åne the predicted object pose by aligning
the reconstructed object model and the point cloud back-
projected from the depth image to improve the object pose
accuracy.
D. Applications
The perception of objects can be applied to many down-
stream tasks. Here, we use robotic grasping as an example.
To grasp an object with a gripper, the relative pose between
the gripper and the base of the arm is computed as follows:
Tgb=TgoTocTcb; (12)
whereTgo,Toc, andTcbare the relative poses between the
gripper and the object, the object and the camera, and the
camera and the base of the arm, respectively. As shown in
Fig. 3, given the reconstructed object model, we use the
analytic method Graspit! [16] to compute Tgoand PVNet [4]
to estimateToc.Tcbis obtained via hand-eye calibration. The
details can be found in the supplementary material.
Reconstructed‚Ä®object modelGrasp poseObject pose
Real-world graspingGrasp‚Ä®generation
New test image
PVNetReal-world graspFig. 3. Real-world grasping pipeline based on the reconstructed object
model. Graspit! is used to generate a grasp pose given the reconstructed
object model in the object coordinates and the trained PVNet is used to
estimate the object pose on the new test image.
E. Implementation Details
Poking. We choose to perform 4 poking actions for each
object as we empirically Ô¨Ånd this number enough to observe
objects in sufÔ¨Åcient views to obtain a complete perception.
Other details of the poking process are in the supplementary
material.
Reconstruction. During reconstruction, we use a batch size
of 1024 rays, each sampled at 192 coordinates uniformly.
2 Adam optimizers with the learning rates decaying from
1e-3 and 5e-4 are used for the object poses and the neural
radiance Ô¨Åeld parameters, respectively. The 3 stages cost
10000, 10000, and 50000 iterations, respectively. The loss
weights are set to w1= 1,w2= 1,w3= 0:1,w4=2e-5, and
wis set to 200.
Memorization. We synthesize 10000 images to train the
PVNet. The object poses are sampled over 30 semi-spheres
with different distances to the object. The background images
are selected from the ScanNet dataset [43]. To increase the
generalization ability of the PVNet, both the synthesized
images and the images in the recorded video are used during
training.
Grasping. The grasp poses are generated by the Graspit! [16]
simulator and the one orienting downward is selected for
real-world grasping to avoid collision between the gripper
and the plane.
IV. EXPERIMENTS
A. Data collection
We capture a real-world RGB-D video to evaluate our
method, where a cat, a duck, and a coffee box are put
on a table. The video consists of 665 frames. To increase
efÔ¨Åciency, we drop the frames with no moving objects,
resulting in a 166-frame video. The image resolution is
1344648. The ground-truth models for the cat and the
duck are provided by the LINEMOD dataset [44], while the
coffee box is represented by a cube with manually-measured
sizes. The ground-truth object poses are obtained by aligning
the object models with the RGB-D point clouds. A meshrenderer is used to produce the ground-truth segmentation
masks with the ground-truth object poses and the object
models. We recommend watching the supplementary video
for the collected data.
B. Object reconstruction evaluation
We use MaskFusion [45] as the baseline for object recon-
struction. Since [45] cannot perform instance segmentation
for unseen objects, we use the initialized masks introduced
in Sec. III-B as the masks for them.
Tab. I compares our method with the baseline in terms
of object pose accuracy. We report the mean and maximum
of rotation and translation errors. Our method outperforms
the baseline by a large margin, particularly in the maximum
rotation errors for the cat and the duck, where we improved
by about 20 degrees. Our method jointly optimizes the object
poses and segmentation masks for all frames, eliminating
accumulated error even for textureless objects, which is not
possible with ICP used in [45].
Object Method Rotation (degree) Translation (cm)
catMF 11.914 / 30.074 1.676 / 4.684
Ours 4.391 /8.003 0.452 /1.168
boxMF 2.060 / 3.948 0.712 / 1.452
Ours 1.569 / 4.282 0.596 / 1.716
duckMF 14.144 / 31.871 3.728 / 8.212
Ours 4.070 /12.743 1.116 /3.388
TABLE I
OBJECT POSE COMPARISON BETWEEN MASKFUSION (MF) AND
OURS.WE REPORT MEAN ERROR /MAXIMUM ERROR OVER THE ENTIRE
VIDEO .
Object Method C.D. # F-score " N.C. " Mask IoU "
catMF 0.173 0.836 0.579 0.708
Ours 0.051 0.926 0.818 0.839
boxMF 0.705 0.783 0.657 0.762
Ours 0.051 0.937 0.823 0.790
duckMF 0.177 0.812 0.587 0.674
Ours 0.035 0.963 0.854 0.771
TABLE II
3DGEOMETRY COMPARISON BETWEEN MASKFUSION (MF) AND
OURS.C.D. IS CHAMFER DISTANCE . N.C. REPRESENTS NORMAL
CONSISTENCY .
Tab. II and Fig. 4 compare the results of object reconstruc-
tion and segmentation masks between our method and the
baseline. Our method outperforms the baseline in all metrics
and produces higher-quality segmentation masks, especially
for the cat and the duck. This improvement is due to the joint
Fig. 4. Qualitative comparison between MaskFusion and the proposed
method. The color indicates surface normal.
Fig. 5. Qualitative results of object pose estimation on new test
images. The estimated bounding boxes are shown in blue. Please refer to
the supplementary video for more visualization results.
optimization of object geometry and object pose, leading to
globally consistent results.
C. Object memorization evaluation
To evaluate object memorization, we perform object pose
estimation using the trained PVNet on new test images. Some
visualization results are shown in Fig. 5, where the PVNet
precisely estimates the object poses.
D. Real-world grasping
We perform a real-world robotic grasping task using a
parallel gripper to grasp objects placed on a plane. The
results, depicted in Fig. 6, show that the cat and the duck
are successfully grasped. Due to its size, the coffee box
could not be grasped from the top and is not included in
the demonstration.Fig. 6. Real-world grasping of the cat and the duck. Please refer to the
supplementary video for the entire grasping process.
E. Ablation
In this section, we conduct ablation experiments to analyze
the effectiveness of several designs in our method. The
results of the object pose evaluation and the visualization
results for the cat are shown in Tab. III and Fig. 7, respec-
tively.
The sparsity loss. To validate the beneÔ¨Åt of the sparsity loss,
we perform optimization without sparsity loss and extract the
object mesh. As visualized in Fig. 7 (b), our method cannot
decompose the object and the background correctly without
the sparsity loss due to the motion ambiguity of the texture-
poor background.
The foreground sampling strategy. To measure the ef-
fectiveness of the mask sampling strategy, we evaluate the
performance of the proposed method with a random sampling
strategy. As shown in the second line in Tab. III and Fig. 7
(c), the optimization could not focus on the object region
and thus produce very coarse results.
The stage-wise training strategy. To measure the effec-
tiveness of the stage-wise training strategy, we evaluate the
performance of the proposed method with stage 3 only.
Comparing the Ô¨Årst line and the third line in Tab. III shows
that the proposed method cannot decompose the foreground
objects and the background correctly without initializing the
radiance Ô¨Åelds in stage 1 and stage 2.
V. LIMITATION
There are several directions to improve our system. First,
the accuracy of depth scanning is a challenge, particularly
for glossy or transparent surfaces. This leads to errors in
object pose initialization and affects the accuracy of depthRotation (degree) Translation (cm)
full 4.390 / 8.003 0.452 / 1.168
w/o stage-wise training 18.421 / 44.382 3.820 / 9.000
w/o foreground sampling 9.417 / 30.385 1.056 / 4.160
TABLE III
ABLATION STUDY .WE REPORT MEAN ERROR /MAXIMUM ERROR FOR
THE CAT OVER THE ENTIRE VIDEO .
(a) (b)
(c) (d)
Fig. 7. Ablation study. Reconstructed models of the full version of the
proposed method (a), without applying sparsity loss (b), without foreground
sampling (c), and without stage-wise training strategy (d) are visualized.
supervision during optimization. Second, the current recon-
struction and memorization processes are time-consuming,
which can be potentially addressed with faster reconstruction
methods [46]‚Äì[48] and pose estimation methods that do not
require training [49]‚Äì[51].
VI. CONCLUSIONS
In this paper, we proposed a new system for unseen
3D object perception. The key idea is to perform poking
to discover 3D objects in the scene and then reconstruct
the 3D objects based on the multi-view observations from
object motion. The reconstructed models can be then utilized
to train neural networks for object recognition in new test
images. Our method achieved successful 3D object discovery
and high-quality reconstruction in real-world scenarios, as
demonstrated by experimental results. The learned neural
networks can be directly applied in downstream tasks like
robotic grasping, manipulation, and scene understanding.
We believe that our system presents a promising approach
towards the practical deployment of robots in real-world
environments.
Acknowledgement. The authors would like to acknowledge
the support from the National Key Research and Develop-
ment Program of China (No. 2020AAA0108901) and ZJU-
SenseTime Joint Lab of 3D Vision.REFERENCES
[1] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster r-cnn: Towards real-
time object detection with region proposal networks,‚Äù Advances in
neural information processing systems , vol. 28, 2015.
[2] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu,
and A. C. Berg, ‚ÄúSsd: Single shot multibox detector,‚Äù in European
conference on computer vision . Springer, 2016, pp. 21‚Äì37.
[3] J. Sun, L. Chen, Y . Xie, S. Zhang, Q. Jiang, X. Zhou, and H. Bao,
‚ÄúDisp r-cnn: Stereo 3d object detection via shape prior guided instance
disparity estimation,‚Äù in Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , 2020, pp. 10 548‚Äì10 557.
[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, ‚ÄúPvnet: Pixel-
wise voting network for 6dof pose estimation,‚Äù in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2019, pp. 4561‚Äì4570.
[5] H. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas,
‚ÄúNormalized object coordinate space for category-level 6d object pose
and size estimation,‚Äù in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2019, pp. 2642‚Äì2651.
[6] G. Gkioxari, J. Malik, and J. Johnson, ‚ÄúMesh r-cnn,‚Äù in Proceedings
of the IEEE/CVF International Conference on Computer Vision , 2019,
pp. 9785‚Äì9795.
[7] M. Runz, K. Li, M. Tang, L. Ma, C. Kong, T. Schmidt, I. Reid,
L. Agapito, J. Straub, S. Lovegrove et al. , ‚ÄúFrodo: From detections to
3d objects,‚Äù in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2020, pp. 14 720‚Äì14 729.
[8] J. Bohg, K. Hausman, B. Sankaran, O. Brock, D. Kragic, S. Schaal,
and G. S. Sukhatme, ‚ÄúInteractive perception: Leveraging action in
perception and perception in action,‚Äù IEEE Transactions on Robotics ,
vol. 33, no. 6, pp. 1273‚Äì1291, 2017.
[9] P. Agrawal, A. V . Nair, P. Abbeel, J. Malik, and S. Levine, ‚ÄúLearning to
poke by poking: Experiential learning of intuitive physics,‚Äù Advances
in neural information processing systems , vol. 29, 2016.
[10] Y . Ye, D. Gandhi, A. Gupta, and S. Tulsiani, ‚ÄúObject-centric forward
modeling for model predictive control,‚Äù in Conference on Robot
Learning . PMLR, 2020, pp. 100‚Äì109.
[11] M. Janner, S. Levine, W. T. Freeman, J. B. Tenenbaum, C. Finn, and
J. Wu, ‚ÄúReasoning about physical interactions with object-oriented
prediction and planning,‚Äù arXiv preprint arXiv:1812.10972 , 2018.
[12] Z. Xu, J. Wu, A. Zeng, J. B. Tenenbaum, and S. Song, ‚ÄúDensephysnet:
Learning dense physical object representations via multi-step dynamic
interactions,‚Äù arXiv preprint arXiv:1906.03853 , 2019.
[13] Z. Xu, Z. He, J. Wu, and S. Song, ‚ÄúLearning 3d dynamic scene repre-
sentations for robot manipulation,‚Äù arXiv preprint arXiv:2011.01968 ,
2020.
[14] Y . Du, K. Smith, T. Ulman, J. Tenenbaum, and J. Wu, ‚ÄúUnsuper-
vised discovery of 3d physical objects from video,‚Äù arXiv preprint
arXiv:2007.12348 , 2020.
[15] T. Kipf, G. F. Elsayed, A. Mahendran, A. Stone, S. Sabour, G. Heigold,
R. Jonschkowski, A. Dosovitskiy, and K. Greff, ‚ÄúConditional object-
centric learning from video,‚Äù arXiv preprint arXiv:2111.12594 , 2021.
[16] A. T. Miller and P. K. Allen, ‚ÄúGraspit! a versatile simulator for robotic
grasping,‚Äù IEEE Robotics & Automation Magazine , vol. 11, no. 4, pp.
110‚Äì122, 2004.
[17] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea,
and K. Goldberg, ‚ÄúDex-net 2.0: Deep learning to plan robust grasps
with synthetic point clouds and analytic grasp metrics,‚Äù arXiv preprint
arXiv:1703.09312 , 2017.
[18] D. Morrison, P. Corke, and J. Leitner, ‚ÄúClosing the loop for robotic
grasping: A real-time, generative grasp synthesis approach,‚Äù arXiv
preprint arXiv:1804.05172 , 2018.
[19] X. Zhou, X. Lan, H. Zhang, Z. Tian, Y . Zhang, and N. Zheng, ‚ÄúFully
convolutional grasp detection network with oriented anchor box,‚Äù in
2018 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS) . IEEE, 2018, pp. 7223‚Äì7230.
[20] F.-J. Chu, R. Xu, and P. A. Vela, ‚ÄúReal-world multiobject, multigrasp
detection,‚Äù IEEE Robotics and Automation Letters , vol. 3, no. 4, pp.
3355‚Äì3362, 2018.
[21] H. Liang, X. Ma, S. Li, M. G ¬®orner, S. Tang, B. Fang, F. Sun, and
J. Zhang, ‚ÄúPointnetgpd: Detecting grasp conÔ¨Ågurations from pointsets,‚Äù in 2019 International Conference on Robotics and Automation
(ICRA) . IEEE, 2019, pp. 3629‚Äì3635.
[22] Y . Qin, R. Chen, H. Zhu, M. Song, J. Xu, and H. Su, ‚ÄúS4g: Amodal
single-view single-shot se (3) grasp detection in cluttered scenes,‚Äù in
Conference on robot learning . PMLR, 2020, pp. 53‚Äì65.
[23] T. Patten, K. Park, and M. Vincze, ‚ÄúDgcm-net: dense geometrical
correspondence matching network for incremental experience-based
robotic grasping,‚Äù Frontiers in Robotics and AI , vol. 7, p. 120, 2020.
[24] H.-S. Fang, C. Wang, M. Gou, and C. Lu, ‚ÄúGraspnet-1billion: A large-
scale benchmark for general object grasping,‚Äù in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition ,
2020, pp. 11 444‚Äì11 453.
[25] M. Van der Merwe, Q. Lu, B. Sundaralingam, M. Matak, and T. Her-
mans, ‚ÄúLearning continuous 3d reconstructions for geometrically
aware grasping,‚Äù in 2020 IEEE International Conference on Robotics
and Automation (ICRA) . IEEE, 2020, pp. 11 516‚Äì11 522.
[26] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe, P. Kohli,
J. Shotton, S. Hodges, D. Freeman, A. Davison et al. , ‚ÄúKinectfusion:
real-time 3d reconstruction and interaction using a moving depth
camera,‚Äù in Proceedings of the 24th annual ACM symposium on User
interface software and technology , 2011, pp. 559‚Äì568.
[27] B. Curless and M. Levoy, ‚ÄúA volumetric method for building complex
models from range images,‚Äù in Proceedings of the 23rd annual
conference on Computer graphics and interactive techniques , 1996,
pp. 303‚Äì312.
[28] M. Runz, M. BufÔ¨Åer, and L. Agapito, ‚ÄúMaskfusion: Real-time recog-
nition, tracking and reconstruction of multiple moving objects,‚Äù in
2018 IEEE International Symposium on Mixed and Augmented Reality
(ISMAR) . IEEE, 2018, pp. 10‚Äì20.
[29] B. Xu, W. Li, D. Tzoumanikas, M. Bloesch, A. Davison, and
S. Leutenegger, ‚ÄúMid-fusion: Octree-based object-level multi-instance
dynamic slam,‚Äù in 2019 International Conference on Robotics and
Automation (ICRA) . IEEE, 2019, pp. 5231‚Äì5237.
[30] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor-
thi, and R. Ng, ‚ÄúNerf: Representing scenes as neural radiance Ô¨Åelds
for view synthesis,‚Äù Communications of the ACM , vol. 65, no. 1, pp.
99‚Äì106, 2021.
[31] L. Yariv, J. Gu, Y . Kasten, and Y . Lipman, ‚ÄúV olume rendering of
neural implicit surfaces,‚Äù Advances in Neural Information Processing
Systems , vol. 34, pp. 4805‚Äì4815, 2021.
[32] P. Wang, L. Liu, Y . Liu, C. Theobalt, T. Komura, and W. Wang, ‚ÄúNeus:
Learning neural implicit surfaces by volume rendering for multi-view
reconstruction,‚Äù arXiv preprint arXiv:2106.10689 , 2021.
[33] B. Yang, Y . Zhang, Y . Xu, Y . Li, H. Zhou, H. Bao, G. Zhang, and
Z. Cui, ‚ÄúLearning object-compositional neural radiance Ô¨Åeld for ed-
itable scene rendering,‚Äù in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2021, pp. 13 779‚Äì13 788.
[34] J. Ost, F. Mannan, N. Thuerey, J. Knodt, and F. Heide, ‚ÄúNeural
scene graphs for dynamic scenes,‚Äù in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2021, pp.
2856‚Äì2865.
[35] C.-H. Lin, W.-C. Ma, A. Torralba, and S. Lucey, ‚ÄúBarf: Bundle-
adjusting neural radiance Ô¨Åelds,‚Äù in Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2021, pp. 5741‚Äì5751.
[36] Z. Wang, S. Wu, W. Xie, M. Chen, and V . A. Prisacariu, ‚ÄúNerf‚Äì
: Neural radiance Ô¨Åelds without known camera parameters,‚Äù arXiv
preprint arXiv:2102.07064 , 2021.
[37] W. Yuan, Z. Lv, T. Schmidt, and S. Lovegrove, ‚ÄúStar: Self-supervised
tracking and reconstruction of rigid objects in motion with neural
rendering,‚Äù in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2021, pp. 13 144‚Äì13 152.
[38] R. Girshick, ‚ÄúFast r-cnn,‚Äù in Proceedings of the IEEE international
conference on computer vision , 2015, pp. 1440‚Äì1448.
[39] A. Gropp, L. Yariv, N. Haim, M. Atzmon, and Y . Lipman, ‚ÄúIm-
plicit geometric regularization for learning shapes,‚Äù arXiv preprint
arXiv:2002.10099 , 2020.
[40] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa, ‚ÄúPlenoctrees
for real-time rendering of neural radiance Ô¨Åelds,‚Äù in Proceedings of
the IEEE/CVF International Conference on Computer Vision , 2021,
pp. 5752‚Äì5761.
[41] W. E. Lorensen and H. E. Cline, ‚ÄúMarching cubes: A high resolution3d surface construction algorithm,‚Äù ACM siggraph computer graphics ,
vol. 21, no. 4, pp. 163‚Äì169, 1987.
[42] V . Lepetit, F. Moreno-Noguer, and P. Fua, ‚ÄúEpnp: An accurate o (n)
solution to the pnp problem,‚Äù International journal of computer vision ,
vol. 81, no. 2, pp. 155‚Äì166, 2009.
[43] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and
M. Nie√üner, ‚ÄúScannet: Richly-annotated 3d reconstructions of indoor
scenes,‚Äù in Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE , 2017.
[44] S. Hinterstoisser, V . Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige,
and N. Navab, ‚ÄúModel based training, detection and pose estimation of
texture-less 3d objects in heavily cluttered scenes,‚Äù in Asian conference
on computer vision . Springer, 2012, pp. 548‚Äì562.
[45] M. Runz, M. BufÔ¨Åer, and L. Agapito, ‚ÄúMaskfusion: Real-time recog-
nition, tracking and reconstruction of multiple moving objects,‚Äù in
2018 IEEE International Symposium on Mixed and Augmented Reality
(ISMAR) . IEEE, 2018, pp. 10‚Äì20.
[46] T. M ¬®uller, A. Evans, C. Schied, and A. Keller, ‚ÄúInstant neural graph-
ics primitives with a multiresolution hash encoding,‚Äù arXiv preprint
arXiv:2201.05989 , 2022.
[47] C. Sun, M. Sun, and H.-T. Chen, ‚ÄúDirect voxel grid optimization:
Super-fast convergence for radiance Ô¨Åelds reconstruction,‚Äù in Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022, pp. 5459‚Äì5469.
[48] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and
A. Kanazawa, ‚ÄúPlenoxels: Radiance Ô¨Åelds without neural networks,‚Äù
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 5501‚Äì5510.
[49] J. Sun, Z. Wang, S. Zhang, X. He, H. Zhao, G. Zhang, and X. Zhou,
‚ÄúOnepose: One-shot object pose estimation without cad models,‚Äù in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 6825‚Äì6834.
[50] X. He, J. Sun, Y . Wang, D. Huang, H. Bao, and X. Zhou, ‚ÄúOnepose++:
Keypoint-free one-shot object pose estimation without cad models,‚Äù
arXiv preprint arXiv:2301.07673 , 2023.
[51] I. Shugurov, F. Li, B. Busam, and S. Ilic, ‚ÄúOsop: A multi-stage
one shot object pose estimation framework,‚Äù in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 6835‚Äì6844.