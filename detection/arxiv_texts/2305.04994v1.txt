Crop identiÔ¨Åcation using deep learning on LUCAS crop cover photos
Momchil Yordanov1, Rapha ¬®el d‚ÄôAndrimont1, Laura Martinez-Sanchez1, Guido Lemoine1,
Dominique Fasbender1;2, Marijn van der Velde1
1European Commission, Joint Research Centre (JRC) ‚Äî Food Security Unit, Ispra , Italy
2Walloon Institute of Evaluation, Foresight and Statistics (IWEPS), Belgium
Abstract
Crop classiÔ¨Åcation via deep learning on ground imagery can deliver timely and accurate crop-speciÔ¨Åc information to various stake-
holders. Dedicated ground-based image acquisition exercises can help to collect data in data scarce regions, improve control on
timing of collection, or when study areas are to small to monitor via satellite. Automatic labelling is essential when collecting large
volumes of data. One such data collection is the EU‚Äôs Land Use Cover Area frame Survey (LUCAS), and in particular, the recently
published LUCAS Cover photos database. The aim of this paper is to select and publish a subset of LUCAS Cover photos for 12
mature major crops across the EU, to deploy, benchmark, and identify the best conÔ¨Åguration of Mobile-net for the classiÔ¨Åcation
task, to showcase the possibility of using entropy-based metrics for post-processing of results, and Ô¨Ånally to show the applications
and limitations of the model in a practical and policy relevant context. In particular, the usefulness of automatically identifying
crops on geo-tagged photos is illustrated in the context of the EU‚Äôs Common Agricultural Policy. The methodology makes use of
crop calendars from various sources to ascertain the mature stage of the crop; of the PyGeon library for labelling of image quality;
of an extensive paradigm for hyper-parameterization of Mobile-net from random parameter initialization; and of various techniques
from information theory in order to carry out more accurate post-processing Ô¨Åltering on results. The work has produced a dataset
of 169,460 images of mature crops for the 12 classes, out of which 15,876 were manually selected as representing a clean sample
without any foreign objects or unfavorable conditions. The best performing model achieved a Macro F1 (M-F1) of 0.75 on an
imbalanced test dataset of 8,642 photos. Using metrics from information theory, namely - the Equivalence Reference Probability,
resulted in achieving an increase of 6%. The most unfavorable conditions for taking such images, across all crop classes, were
found to be too early or late in the season. The proposed methodology shows the possibility for using minimal auxiliary data,
outside the images themselves, in order to achieve a M-F1 of 0.817 for labelling between 12 major European crops.
Keywords: Plant recognition, Agriculture, Computer vision, Deep Learning, Data valorization
1. Introduction
The Deep Learning (DL) paradigm is regarded as the Gold
Standard of the Machine Learning (ML) community (Alzubaidi
et al. [4]). While there is understandably a trade-o between
the better performance in the model and the amount of data
and resources necessary, it is clear that there is a signiÔ¨Åcant im-
provement with DL methods, especially so in the Ô¨Åeld of image
classiÔ¨Åcation. Recent advancements in Convolutional Neural
Networks (CNNs) have made popular classiÔ¨Åcation tasks ever
more a ordable in an operational context.
Related to this is the ability to perform on-device process-
ing in order to provide an option to anyone wanting to im-
plement the technology, while keeping computational overhead
low. A leading architecture in this regard is MobileNet (Howard
et al. [19]), of which there are third generation Ô¨Çavours (Howard
et al. [18]), which in turn are both signiÔ¨Åcantly smaller than and
equal in performance to the previous ones (Sandler et al. [30]).
MobileNets are convenient, as they perform on par with other
state of the art architectures such as Inception V3 on popular
benchmarking datasets, but have up to 20 million less parame-
ters.Another important point in making DL models operational
is the proper and appropriate use of post-processing techniques.
What is generally understood here are all manipulations done
to the data as output from the model. In an image classiÔ¨Åcation
setting this means everything done on the data after the output
of the softmax activation function, which is a probability vec-
tor with a value for each class, that sums to one (Salvi et al.
[29]). Popular post-processing approaches include combining
Random Forest or Support Vector Machine classiÔ¨Åers after the
CNN output, majority voting (d‚ÄôAndrimont et al. [14]), patch
aggregation (Matvienko et al. [25]), and thresholding. Thresh-
olding is one of the most popular choices, as it is usually simple
to implement - keep only the examples, for which the network
has a Maximum Probability (MP) for the winning class higher
than the threshold. An interesting development in the Ô¨Åeld is
the re-mapping of the base probabilistic output to a value of
higher versatility, taking notions from information theory such
as Shannon information and entropy (Bogaert et al. [7]).
In the agricultural sector, these technological developments
are reÔ¨Çected in projected increases of the use of ArtiÔ¨Åcial Intel-
ligence (AI) throughout the food production chain (Columbus
[9]). DL-aided Computer Vision (CV) in particular is crucial
Preprint submitted to ISPRS Journal of Photogrammetry and Remote Sensing May 10, 2023 arXiv:2305.04994v1  [cs.CV]  8 May 2023for automation and robotic tasks that rely on inspection, eval-
uation, and execution of management interventions (Tian et al.
[37]). Ultimately, these technical innovations should contribute
to decreasing costs, while increasing resource use e ciency and
precision, of food production systems. An important element of
the application of these technologies relates to the possibility of
new ways of information exchange among the various actors
in the food production chain. This may relate to certiÔ¨Åcation
of management practices (Santoso et al. [31]), traceability of
products (Kollia et al. [22]), as well as communication towards
consumers (Zhu et al. [44]), or indeed various activities in the
realm of citizen science and food related topics (Schiller et al.
[32]), including biodiversity (A ouard et al. [2]). In technical
terms, the possibilities have already been successfully tested for
weed management (Wu et al. [42]), crop disease recognition
and management (Mohanty et al. [26]), and harvesting opera-
tions (Kapach et al. [20]).
Activities also focus on training data collection and cura-
tion for increasingly speciÔ¨Åc applications. Lu and Young [23]
identiÔ¨Åed 34 public image datasets collected under Ô¨Åeld con-
ditions of relevance to precision agriculture. Zheng et al. [43]
presented a crop dataset for deep-learning-based classiÔ¨Åcation
and detection in precision agriculture, while Sudars et al. [36]
presented a dataset of annotated food crops and weed images
for robotic CV control. In the Earth Observation (EO) domain,
datasets such as CropHarvest (Tseng et al. [38]) with more than
90,000 worldwide geographically diverse samples with labels,
and the LUCAS 2018 Copernicus polygons (d‚ÄôAndrimont et al.
[11]) with almost 60,000 stratiÔ¨Åed samples in the EU, demon-
strate the push from the community to have open and free data
to facilitate ML-and-DL-driven research. In this manuscript,
we focus on recognizing crops and rely on a selection of legacy
close-up photos taken during Ô¨Åve tri-annual Land Use /Cover
Area frame Survey (LUCAS) surveys from 2006 to 2018 in the
EU (d‚ÄôAndrimont et al. [12]) for our training set.
A Ô¨Åtting application in this sense and speciÔ¨Åcally so in the
European context is the ability of technology to deliver to the
needs of regulating bodies that administer the technical regula-
tions of the European Union (EU)‚Äôs Common Agricultural Pol-
icy (CAP). The CAP is the single largest item on the EU budget,
amounting to a total of 58,38 billion euros in 2022, including
funds allocated for rural development, market measures, and in-
come support (Commission [10]). Thus developing technology
for automatizing the application process and the evidence pro-
vision for practices required under subsidy schemes is in great
demand by paying agencies of the respective Member States
(MS).
While Copernicus Sentinel based monitoring of the CAP
area subsidies is being developed and implemented (Devos et al.
[13]), ground based information in the form of geo-tagged pic-
tures (Sima et al. [33]) can support and complement the Checks
by Monitoring approach (CbM). CbM relies on Copernicus Sen-
tinel data-streams providing wall-to-wall coverage of EU terri-
tory and cloud-based processing on the Data and Information
Access Services (DIAS) platforms. By using Copernicus Ap-
plication Ready Data (CARD), in conjunction with geospatial
information from the Land Parcel IdentiÔ¨Åcation Systems (LPIS)and Geo-Spatial Aid Applications (GSAA), it is possible to ex-
tract parcel-level information of markers (see Devos et al. [13]).
These markers evidence speciÔ¨Åc practices (e.g. mowing, irriga-
tion, etc.) that can be related to compliance requirements. Nev-
ertheless, in situations that the Sentinel based checks do not
lead to conclusive results, geo-tagged pictures can be used to
support and complement checks. Such processing chains may
have to be developed for each speciÔ¨Åc agri-environmental prac-
tice for which evidence is needed. In the current CAP program-
ming period (2023-2027), this includes practices under GAEC
(Good Agricultural and Environmental Conditions) condition-
ality, as well as eco-schemes and agri-environmental and cli-
mate measures.
1.1. Objectives
The aim of the research is to benchmark and test computer
vision models to recognize Major and Mature European Crops
(MMEC) on close-up photos in a practical agricultural policy
relevant context. SpeciÔ¨Åc objectives are:
¬àTo select and publish a subset of LUCAS cover photos
representative for major and mature crops across the EU
for training purposes.
¬àTo deploy and benchmark a set of Mobile-net computer
vision models to recognize crops on close-up pictures and
identify the best performing model.
¬àTo explore the use of probability and entropy-based met-
rics to threshold and Ô¨Ålter correct and incorrect classiÔ¨Å-
cations.
¬àTo illustrate the applications and limitations of the model
for inference in a practical and agricultural policy rele-
vant context.
2. Materials and Methods
The methodological approach in the manuscript consists of
1) the procedure to select close-up LUCAS cover MMEC pho-
tos; 2) training, validating, and testing of a large set of Mobile-
net based computer vision models; 3) applying the best model
to inference photos across the EU; 4) evaluate model perfor-
mance using metrics, derived from information theory to Ô¨Ålter
and understand why photos are not classiÔ¨Åed well; 5) test model
performance against images exhibiting a series of unfavorable /out-
of-scope conditions; 6) illustrate practical implications for pro-
tocol development. More speciÔ¨Åcally the workÔ¨Çow is presented
in Figure 1.
2.1. Data
2.1.1. LUCAS cover photos
LUCAS Cover is a part of the core LUCAS survey since
its inception and accordingly data has been collected for all
Ô¨Åve campaigns form 2006 to 2018. A total of 875,661 LU-
CAS Cover photos have been collected and 874,646 of those
were published after anonymization and curation [12]. In con-
trast to other LUCAS core imagery (four N,S,W,E, photos in
2Figure 1: Conceptual diagram of the study. The used data is shown on the left. LUCAS attributes are fused with harmonized crop calendars for the selected crops,
after which the combined dataset undergoes a process of manual annotation using the pyGeon library. After annotating enough images of su ciently high quality,
a stratiÔ¨Åed sample across EU countries is done to select the training and inference sets, followed by the DL paradigm (described further in Figure 4). The DL
workÔ¨Çow produces a best parameterized model, that in turn is used to inference over a large imbalanced set, where post-processing and further operational-context
work takes place.
the cardinal directions, and the point photo P), the Cover (C)
photos, by protocol, must show the cover on the ground at the
GPS location where the survey is carried out in such a way that
the relevant crop, or plant can easily be identiÔ¨Åed during data
quality controls. An example of one photo per selected crop
is shown in Figure 2. The selection was done by reference to
the main crops that are monitored and forecast by the European
Commission‚Äôs Joint Research Centre crop forecasting activities
(AGRI4CAST, formerly MARS, see van der Velde et al. [40]).
Omitting some classes due to data insu ciency, and including
Temporary grassland, the number of crops arrives to 12. These
are: Common wheat (B11), Durum wheat (B12), Barley (B13),
Rye (B14), Oats (B15), Maize (B16), Potatoes (B21), Sugar
beet (B22), SunÔ¨Çower (B31), Rape and turnip rape (B32), Soya
(B33), Temporary grassland (B55). The LUCAS Cover dataset
is one of the two main inputs to the study, as shown on the left
in Figure 1.
2.1.2. Crop calendars and harmonization
One of the objectives of the study is the identiÔ¨Åcation of
mature crops on geo-tagged LUCAS imagery. The rationale
being that, from an operational standpoint, the mature stage of
the crop is the one in which it is most recognizable. The ma-
ture stages of the selected crops have to be Ô¨Årstly ascertained.
One way of doing so is by collecting all crop calendars from the
variety of sources available, harmonizing them into a common
format, extracting the harvest period for each crop, and Ô¨Ånally,
through the use of expert knowledge, derive the pre-harvest ma-
ture stage of the crop.
A Crop Calendar (CC) is a schedule that provides timely in-
formation about crops in their respective agro-ecological zone.
They are usually provided in tabular or gridded form and cover
the space of a calendar year by dividing it into the planting,
vegetative, and harvest stages of the respective crop. For the
present purposes, CCs were gathered from various sources (Ta-
ble 1) and harmonised to a common style (AGRI4CAST), as it
already hosts the data in tabular and numeric format, facilitat-
ing further processing. It must be noted that certain steps had
to be taken to account for instances where more than one vari-ety (spring /winter, or early /late ware varieties) of the same crop
is cultivated in a country. The decision was made to exclude
countries that cultivate both varieties, and use the CC informa-
tion for only those countries that cultivate the winter and early
ware variety, with the information for the excluded countries
being populated by expert knowledge.
Table 1: Crop calendar sources and references
CC source Crop Link
AGRI4CAST
[3]Corn, Winter Wheat,
Durum Wheat, RiceAGRI4CAST
USDA [39] SunÔ¨Çower, Barley,
Rye, Soybeans,
Spring Wheat,
Rapeseed, OatsUSDA
EUROPABIO
[15]Potato, Sugar Beet EUROPABIO
potato,
EU-
ROPABIO
sugarbeet
2.1.3. Expert knowledge gap Ô¨Ålling and mature pre-harvest stages
After harmonizing the CCs and extracting harvest stages at
national level, the study Ô¨Ålls the gaps and validates the result
by means of expert knowledge. One way of identifying gaps is
using the information from the JRC MARS bulletins (agr [1]).
These bulletins o er information on crop growth conditions and
yield forecast at EU level and neighbouring countries like the
UK, Ukraine, Black Sea area, and Maghreb. The rationale here
being that if there is information in the bulletin about the yield
of a certain crop for a certain country, then the crop is obviously
cultivated in the respective country, and ergo - CC informa-
tion about it should be present. After identifying the gaps, they
were Ô¨Ålled with all available information, comprising of inter-
polations from the COST 725 phenology network (Koch et al.
[21]), and expert knowledge. A breakdown of all the informa-
tion gathered and the sources it was collected from is available
3Figure 2: Example of one image per class for the selected 12 major European crops.
in the Supplementary Material Figure A.10. The Ô¨Ånal step is
acquiring the mature, pre-harvest conditions of the crops. This
was accomplished again with the use of expert knowledge and
was conducted in accordance with the following rules: for cere-
als, rapeseed, sunÔ¨Çower, and soya - remove the last half month
and then add 2 months at the beginning of the harvest stage;
for potatoes, sugarbeet, maize, and rice - remove the last half
month and add 3 months.
2.1.4. Manual photo pre-processing by visual assessment
The dataset was then visually assessed with the use of the
PyGeon jupyter library to remove examples not suited for the
study. The photos were selected on the basis of what one could
expect in farmer photos: artiÔ¨Åcial background (map, hand, leg,
pivot), and low quality photos (e.g. against the sun, shadowed,
etc.) were not allowed. Close ups showing individual leaves,
ears, grains were also removed. Overview photos where the
crop appears somewhere in the background, usually mixed with
other elements (a road, neighbouring Ô¨Åeld etc) were also re-
moved. Photos with seeds only on a bare soil background (in
cereals, soy, maize mostly), and other obviously wrong photos
were also eliminated, although this happened only a few times.
At this stage, photos Ô¨Çagged as not suitable to train on are
later manually classiÔ¨Åed into one of the six categories of unfa-
vorable conditions. These are out of season (too early or late in
the season), out of protocol (too close or too far away to imagethe plant matter adequately), or either being blurred or there be-
ing a foreign object in the photo. A total of 354 images were
selected in this way, while making sure that there is at least one
photo per year, per LUCAS land cover class, per unfavorable
condition. An example of unfavorable conditions for Common
wheat (B11) is shown on Figure 3.
The Ô¨Ånal clean dataset used in this study is available for
download https://jeodpp.jrc.ec.europa.eu/ftp/jrc-opendata/
DRLL/LUCASvision/ .
2.2. Method
The study makes use of a CNN for an image-classiÔ¨Åcation
CV exercise with a balanced training and inference set. There
are two rounds of training and parallelized inferencing that make
up the hyper-parameterization workÔ¨Çow (Figure 4) - one with-
out and one with data augmentations (Ô¨Çip, brightness, etc). Af-
ter the Ô¨Ånal augmented inference, the best model is identiÔ¨Åed
and it is fed with a much larger imbalanced inference set, sup-
posed to represent a quasi-operational scenario. SpeciÔ¨Åc and
innovative post-processing techniques are also explored.
2.2.1. Training and inference set(s) sample selection
The selected number of photos per class for training was
set to 400, following the current State of the Art [34]. In order
to select the set, a stratiÔ¨Åed sample across NUTS0 regions in
the EU is made from the MMEC dataset (from Section 3.1).
4Figure 3: The six classes of unfavorable conditions for Common wheat (B11).
Figure 4: Deep learning processing chain. The two rounds of training and inferencing are shown in black contour and labelled accordingly.
This is done with the idea of having equal representation across
EU countries, which allows for articulating conclusions on the
European scale.
In order to shorten processing time, instead of using the en-
tire leftover (post-training-set-selection) set of images for infer-
encing, the study makes use of a custom inference set, sampled
out of the leftover set. A total of 85 (the total number of exam-
ples of the least represented class (B12)) images per class were
selected with a geographic distribution that matches the one of
the training set. This ‚Äúbalanced‚Äù inference set was used during
the Ô¨Årst and second stage of inferencing (Figure 4).
The last set of images to be discussed is the ‚Äúimbalanced‚Äù
inference set, which includes all the photos left after the train-
ing set selection with all classes capped at 1000 examples per
class. This set includes the previous ‚Äúbalanced‚Äù set and it is the
one used on the identiÔ¨Åed best model in order to judge the pos-
sibility of using the model as an operational tool. It is also this
set that any further developments are tested on.
2.2.2. Hyper-parameter search and best model selection
The network in use is MobileNet V2. The images vary in
their native resolution (see Supplementary Material Table A.5),
but every image in the training and inference set is re-scaled to
the net input size of 224x224. The e ects of this re-scaling are
discussed in section 4.4. The V2 MobileNets are trained for
3000 epochs, with the following settable parameters - learning
rate, momentum, optimizer, batch size. These variables were
experimented within a random space [6] to generate values forinitializing the learning process. In this way 157 model con-
Ô¨Ågurations were tried in order to Ô¨Ånd the best approximation
for solving the problem. Model performance was then tested
by carrying out an independent inference exercise on the dedi-
cated balanced set. The models are then ranked based on their
Overall Accuracy (OA) to Ô¨Ånd the top Ô¨Åve performers. This
completes the Ô¨Årst round of training.
For the second round the top Ô¨Åve performers are run through
another cycle of training with the same conÔ¨Åguration, but adding
image augmentations, in this case random brightness and hor-
izontal image Ô¨Çips. The same inferencing on the balanced set
is done to rank the augmented models based on OA. The best
performing of these is then taken as the overall best model.
2.2.3. Operational use
After the best model is identiÔ¨Åed, it is used on the imbal-
anced inference set (see Section 2.2.1). Because of the class
imbalance, it was necessary to use a di erent metric - Macro-
F1 (M-F1) [27]. It is the results from this inference run that are
presented in Section 3. It is also on these results that the e ec-
tiveness of innovative post-processing techniques will be tested
and upon which all the discussion will be carried out.
2.2.4. Computational Infrastructure
All the code developed for this study is available openly
on the following repository: https://github.com/Momut1/
lucasVision . The working environment was carried in a docker
5image. The processing pipeline is fully reproducible and auto-
mated to work by calling shell scripts that respectively carry
out the hyper-tuning, inferencing, results derivation, and post-
processing and plotting. For more information consult the readMe
of the git repository. The processing was done on the JRC
BDAP, an in-house, cloud-based, versatile, petabyte-scale plat-
form for heavy-duty processing [35]. The o ered GPU services
work on a NVidia GeForce GTX 1080 Ti with 11GB mem-
ory, CUDA version 10.1, and CUDA driver version 418.67.
Pre-processing, launching, and post-processing are done in the
JEO-lab layer of the platform in a jupyter notebook docker con-
tainer, running TensorÔ¨Çow 1.3.0.
2.2.5. Equivalent Reference Probability Ô¨Ålter
Post-processing results from ML /DL exercises is an estab-
lished practice in practically all such workÔ¨Çows ([17], [8], [5]).
What it usually consists of is the selected removal, based on
some criteria, of a substantial enough number of the incorrectly
classiÔ¨Åed examples in order to increase model performance,
while simultaneously not falling into the trap of ‚Äùcherry-picking‚Äù
one‚Äôs results.
In classiÔ¨Åcation problems analysts can employ a Ô¨Ålter on
probability - keeping only examples for which the network has
output a MP of the winning class above a threshold. The ana-
lyst then decides where to put the threshold in order to control
the rigorousness of the Ô¨Ålter - higher for more stringent classi-
Ô¨Åcation, and lower for a more lenient one. The Ô¨Årst problem
with this is that it depends heavily on the user‚Äôs decision and
is thus, to a degree, arbitrary. The next problem is that the Ô¨Ål-
ter is one dimensional - one can only set a threshold along a
single axis. Introducing other, or indeed multiple, dimensions
to this process would allow for di erent spreads of the data in
the given space. The intuition is that given the chosen dimen-
sions, the data would neatly split between correct and incorrect
classiÔ¨Åcations and allow for more precise Ô¨Åltering. The desired
outcome from such Ô¨Åltering would be to remove the biggest
amount of incorrectly classiÔ¨Åed examples, without removing
too many correctly classiÔ¨Åed ones.
The proposed method works with a metric, based on infor-
mation theory - the Equivalent Reference Probability (ERP), as
described in [7]. In information theory Information is the mea-
sure of surprise from an event - rare or low probability events
are surprising and hence carry more information, and vice versa
(Equation 1). Entropy is the information for the probability dis-
tribution of the events of a given variable (Equation 2). A low
entropy means there is a more pronounced di erence between
the MP for a given class and the rest of the probabilities for
the remaining classes. In [7] the authors make use of the dif-
ference of information (Equation 3) between a reference class
(preferably the most probable class) and all the other classes in
the probability vector. Because the E[D(ijji)] is unrestricted
in terms of potential values, and because it needs an upper and
lower band in order to be interpreted, the authors suggest using
ERP (Equation 4). It is a single metric where ERP2f0; :::;1g;
values approaching 1 mean a very high conÔ¨Ådence in the most
probable class with the most equal distribution of the remainderto the other kclasses.
h(x)= log2(p(x)) (1)
H(X)= nX
i=1Pilog2Pi (2)
E[D(ijji)]=logp
i 1
1 p
iX
inipilogpi (3)
p=exp(E[D(ijji)])
exp(E[D(ijji)])+k 1(4)
The appropriate thresholds for ERP and probability are as-
certained with a custom function that iteratively moves the thresh-
old down the line. At each step it counts the number of dis-
qualiÔ¨Åed incorrect images, while trying to keep the number of
correctly classiÔ¨Åed ones below a certain percent. The settable
parameter to the function is thus the percentage of correctly
classiÔ¨Åed examples the analyst is willing to discard. After as-
certaining the thresholds the space within the scatter plot is di-
vided into four quadrants. Through the iterative exclusion of
one or combinations thereof of the examples in these quadrants,
the analyst can perform a more precise Ô¨Åltering on results.
3. Results
Results are divided into Ô¨Åve sections. Firstly we present
the MMEC dataset, secondly the best performing model is pre-
sented, third - the confusion matrix and M-F1 score for best per-
forming model is shown alongside the Producer (PA) and User
(UA) Accuracy, fourth the improvement generated from em-
ploying an ERP Ô¨Ålter, and lastly - we present the performance
of the model when faced with images from unfavorable condi-
tions for each class, simulating operational use of the model.
3.1. Mature Major European Crops
The processing chain from Sections 2.1.2 and 2.1.3 pro-
duces a dataset of 169,460 LUCAS photos of mature crops
across 25 EU Member States. Utilizing the manual labelling as
described in Section 2.1.4 the study also publishes 15,876 high
quality, ready-to-train-on photos. Each of which has been man-
ually checked and veriÔ¨Åed to exhibit a clear view to the crop in
its mature, pre-harvest stage with no visual obstructions, or for-
eign objects into the frame. Each class has more than 400 pho-
tos, allowing for considerable lee-way in training set selection.
A breakdown per country is visible on Table 2 and geographical
visualization of the same on Figure 5.
3.2. Best performing model
The models were ranked using OA on an independent infer-
ence set of 85 images per class (Table 3). The best model was
identiÔ¨Åed as number 78, achieving an OA of 79.4%. The rel-
evant parameter settings are - Learning Rate of 0.0035148759,
Batch size of 1024, Momentum of zero, as the Optimizer used
is Gradient Descent. The M-F1 and OA on the test set are iden-
tical, as we are dealing with a balanced inference set. The last
6B11 B12 B13 B14 B15 B16 B21 B22 B31 B32 B33 B55 Total Total MMEC
AT 136 32 139 103 29 69 48 67 18 85 122 124 972 3595
BE 59 2 71 4 3 39 93 49 0 23 0 62 405 2127
BG 179 5 129 19 12 148 11 0 110 90 4 3 710 2855
CY 15 1 29 0 2 0 6 0 0 0 0 0 53 207
CZ 72 4 28 32 20 47 30 45 4 194 12 55 543 6691
DE 156 40 157 176 133 114 177 152 17 220 6 212 1560 24055
DK 132 2 93 105 27 55 21 24 0 184 0 175 818 3226
EE 20 0 16 7 8 1 5 0 0 26 0 14 97 612
EL 62 81 88 0 25 62 6 7 22 9 0 4 366 1386
ES 68 58 85 105 80 22 59 58 34 50 0 178 797 19582
FR 121 97 116 130 118 82 139 143 75 186 142 193 1542 40989
HR 38 0 22 4 7 53 6 2 16 9 34 16 207 434
HU 74 34 103 66 30 53 17 8 49 135 49 8 626 7354
IT 136 54 175 17 130 54 50 98 17 21 378 166 1296 13387
LT 82 6 65 72 50 1 28 4 0 147 0 31 486 2313
LU 3 0 7 2 0 1 0 0 0 1 0 6 20 149
LV 66 3 72 44 33 9 12 1 0 110 0 42 392 1763
NL 150 0 53 23 2 134 200 115 0 3 0 54 734 1805
PL 66 12 40 105 61 98 89 93 1 173 7 67 812 20542
PT 28 7 24 40 50 22 19 0 1 0 0 68 259 877
RO 71 28 39 13 25 159 22 16 189 51 88 32 733 3649
SE 89 0 67 47 85 11 34 55 0 95 0 167 650 2742
SI 29 6 26 2 2 30 6 0 1 5 0 27 134 364
SK 89 13 95 27 16 51 18 25 42 164 55 18 613 3091
UK 155 0 134 8 84 78 126 105 0 182 0 179 1051 5665
Total 2096 485 1873 1151 1032 1393 1222 1067 596 2163 897 1901 15876 -
Total
MMEC47143 8062 31500 7296 6582 32175 4113 4414 6830 13958 1603 5784 - 169460
Table 2: All visually inspected MMEC photos labelled as good to train on from the manual annotation using PyGeon. The marginal rows labelled MMEC show the
total number of photos that show mature crops, which have not been visually inspected.
column shows the best model (78), applied over the imbalanced
inference set (see Section 2.2.1). The model is exposed to 7722
more examples, and the drop in M-F1 is 0.0369, meaning the
model is trained and generalizes very well over larger datasets.
Table 3: Output for the top three performing models with augmentations plus
the output from the best model ran on the imbalanced set. The applied augmen-
tation were left-right Ô¨Çip and random brightness. The table shows (in order)
the model number, along with the relevant conÔ¨Åguration (Learning rate, Batch
size, Momentum, Optimizer), the number of labelled images, the training and
validation accuracy, and the M-F1.
Ranking 1 2 3 Best
Model 78 88 4 78
Level Augm Augm Augm Best Model
LR 0.0035 0.0073 0.0096 0.0035
BS 1024 512 512 1024
Momentum 0 0 0 0
Optimizer GD GD GD GD
# of Images 1020 1020 1020 8642
Validation Accuracy 0.7768 0.7789 0.7747 0.7768
Training Accuracy 0.8945 0.8965 0.9238 0.8945
Test Accuracy 0.7941 0.7775 0.7755 0.7854
M-F1 0.7941 0.7775 0.7755 0.7572
3.3. Confusion Matrix
The confusion matrix for the best model run (78) over the
imbalanced operational inference set is presented in Figure 6.
It is clear that the majority of confusion happens between the
cereal classes (B11-B15) and with Grassland (B55). In fact,
the di erence between the average PA of all crops, excludingGrassland, and the average PA of the cereal classes is 27.9, and
for UA the di erence is 30.9. The class which gets most com-
monly miss-classiÔ¨Åed as a false positive is Durum wheat (B12)
with a UA of 10.8; the low score arguably has much to do with
the unequal representation of the class. The best performing
class is Maize (B16), with a PA of 95.5 and UA of 95, followed
closely by Rape and turnip rape (B32), showing the clear sepa-
ration of both from the other classes.
3.4. Equivalent reference probability Ô¨Ålter
The application of the quadrant Ô¨Åltering method using ERP
and MP is shown in Figure 7. The dotted lines represent the
thresholds identiÔ¨Åed by the functions described in Section 2.2.5.
The settable parameter is Ô¨Åxed at loosing no more than one per-
cent of the correctly classiÔ¨Åed images, meaning the identiÔ¨Åed
thresholds are the most conservative ones. They are 0.46 for MP
and 0.2 for ERP. The inscribed table shows the number of true
and false classiÔ¨Åcations in each quadrant as labelled by their
respective quadrant ID. Although similar, their is a notable dif-
ference in the distribution of the true and false classiÔ¨Åcations,
visible in the smooth Ô¨Åtted lines for each group.
The results achieved from employing such Ô¨Åltering are pre-
sented in Figure 7 in the table in the uppermost right corner.
There is an M-F1 increase of 0.6 from not using any Ô¨Ålter and
of 0.2 from using only the MP Ô¨Ålter.
7Figure 5: Geographical distribution of 15,876 LUCAS Cover photos across the EU, which have been manually screened and validated as ready-to-train-on. Map
projection EPSG:3035.
3.5. Unfavorable conditions
Best model 78 was applied over a stratiÔ¨Åed sample of 1
photo per year, per LUCAS LC1 class, and per unfavorable con-
dition, totalling at an inference set of 354, meaning 59 photos
per unfavorable condition (see the examples in Figure 3). A
boxplot of the Top1 probability for each unfavorable condition
is presented on Figure 8. The conditions are compared Ô¨Årstly to
a reference set of quality images that are randomly sampled to
have the same distribution as the sets of the conditions, and sec-
ondly to the entire imbalanced inference set. Model 78 is mostconfused about photos with foreign objects, landscape photos,
and photos showing the crop post its harvest period, with blurry,
early and especially close-up photos performing signiÔ¨Åcantly
closer to the reference in terms of Top1 probability.
The actual classiÔ¨Åcation results are presented on Table 4.
The worst results are achieved with photos exhibiting post-harvest
conditions with an OA of 20%, and early and examples with a
foreign object in the frame following with 31% and 37% re-
spectively. The unfavorable conditions that impact the perfor-
mance the least are blurry and overly close-up photos (54%).
8Figure 6: Confusion matrix for best model (78) over imbalanced inference set.
This illustrates that a clear protocol is needed when such auto-
mated procedures are used within operational workÔ¨Çows, such
as for the CAP [33]. In addition, models can progressively be
trained with a set of photos covering a wider range of conditions
to improve their generalization capacity.
False True OA
Blurry 27 32 0.54
Close 27 32 0.54
Early 41 18 0.31
Landscape 35 24 0.41
Object 37 22 0.37
Post-harvest 47 12 0.20
Table 4: Number of true and false classiÔ¨Åcations and overall accuracy for each
unfavorable condition.
4. Discussion
4.1. Context
Recently, several relevant studies were published. Zheng
et al. [43] present the CropDeep dataset, over which they test
state of the art classiÔ¨Åcation and detection DL algorithms. They
achieve an averaged accuracy of 99.81% over the CropDeep
datasets. These results are impressive, although not directly
comparable, as the images were collected from robots in a ster-
ile greenhouse environment, allowing for image conditions tobe identical between acquisitions. They furthermore used av-
erage accuracy as a metric over an imbalanced inference set,
which is not in accordance with the literature [24]. Gao et al.
[16] achieved an accuracy of 99.51% in di erentiating 30 wheat
cultivars at the Ô¨Çowering (most mature) stage. This is very im-
pressive, considering the present study su ered the most error
when trying to discriminate between the various cereal classes.
The di erence is again in the lab quality of the images taken,
whereby each image exhibits a single plant on a white back-
ground. d‚ÄôAndrimont et al. [14] achieved a M-F1 score of
62.3% for 10 classes using street level images. The current
study outperformed the cited work by 13.4%, though this can
be attributed to the lower presence of noise on the images fed
to the model.
This study presents the Ô¨Årst use of the LUCAS cover dataset
for automatic crop identiÔ¨Åcation. Indeed, it is the Ô¨Årst study
to apply DL for crop identiÔ¨Åcation on still images that are not
taken in a controlled environment and coming from a wide va-
riety of sensors, which truly mimics an operational scenario.
Secondly, the study produces an automated way to attach crop
life-cycle stage information to a database of photos. Third, the
introduction of quadrant Ô¨Åltering is a step towards a new State
of the Art for more precise post-processing Ô¨Åltering. Whether
using crop calendars to extract photos for speciÔ¨Åc crop life-
cycle stages, or using the dataset as a whole, the authors be-
lief that various lines of research may be developed using the
LUCAS cover photos.
9Figure 7: Scatter plot of ERP and probability quadrant Ô¨Åltering with marginal density plots for each variable. In red are the incorrect and in blue - the correct
classiÔ¨Åcations. Numbers within quadrants indicate the quadrant ID. The data is Ô¨Åtted with smooth lines for correct and incorrect classiÔ¨Åcations. The table in the
uppermost corner shows the results from the quadrant Ô¨Åltering. In order the columns represent - the quadrant method ID, the quadrants included in the method, the
number of images, and the M-F1 achieved through the inclusion of the respective Qs. In order the QMs represent - 1. MP only, 2. ERP only, 3. both above their
resp. thresholds, 4. at least one above its threshold.
4.2. ERP Ô¨Åltering
A main achievement of the study is the exploration of meth-
ods for Ô¨Åltering classiÔ¨Åcation results to achieve better perfor-
mance and to quantify uncertainty. The study made use of ERP
as a metric for assessing this uncertainty. According to the liter-
ature, ERP has been shown to be more robust than MP in clas-
sifying pixel-level thematic uncertainty [7]; more precise than
majority voting in post-processing speckle removal of classiÔ¨Åed
maps [41]; and more Ô¨Çexible than OA in terms of independence
of the distribution of the validation data [28].
In practice, MP and ERP are connected, which is clearly
visible in the distribution of both groups (correct and incorrect)
in the space where the joint probability reference distribution is
not null on Figure 7. From the marginal distribution plots we
can see that this connection is inverted - there is a high peak in
the low values of MP for the incorrectly classiÔ¨Åed points and a
high peak in the high values of ERP for the correctly classiÔ¨Åed
ones. Furthermore, as shown in Figure 9, ERP performs signiÔ¨Å-
cantly better than MP in post-processing Ô¨Åltering. Because ERPand MP are both probabilities that are in the range between zero
and one, their direct comparison in this regard is straightfor-
ward. Firstly in subplot A, where for an equal threshold value,
the M-F1 value is always higher when utilising ERP over MP.
This means that ERP is a much better estimator of uncertainty
and manages to capture to a Ô¨Åner degree the nuances that dis-
tinguish an incorrect from a correct classiÔ¨Åcation. It needs to be
mentioned that this is partly due to the fact that, while for MP
the smallest possible threshold value is relatively high (0.20),
with ERP it is found at the Ô¨Årst stage of Ô¨Åltering (0.01). As
seen on the secondary Y axis, which shows the number of im-
ages left in the set after performing the Ô¨Ålter, this process is not
without cost - the number is, for every threshold value, less for
ERP than for MP. Nevertheless, it is always preferable to have
a bigger spread of the data, over which to set thresholds, espe-
cially so when the analysis needs to be conservative regarding
the number of correct classiÔ¨Åcation it is willing to lose.
Furthermore, the histograms in subplots B and C show the
point at which the proportion of correct and incorrect classiÔ¨Åca-
10Figure 8: Top1 probability for all examples for a given unfavorable condition with reference to random sample of the same size of the balanced inference set and to
the entire imbalanced inference set. Number of examples in each box is given above the condition label.
tions for each threshold value, represented by the height of the
gray and yellow bars, relative to the red bar, changes in favour
of the correct ones. While with MP this point arrives at 0.54,
for ERP the change is present already at 0.24. Hence the rela-
tive cost in terms of number of examples disqualiÔ¨Åed due to the
threshold setting is proportionately lower with ERP in order to
achieve the same increase of M-F1.
4.3. Limitations
Although several novel aspects have been highlighted, some
limitations are present in our study. Firstly, there are issues with
the pre-processing of the data - in particular, the fact that CC in-
formation comes from a variety of sources. Albeit o cial CCs,
which have been harmonized, the fact that the study is treating
them as a-priori semantically harmonized could be problem-
atic. Because organizations, based on their goal, have di er-
ent data collection, processing, and publishing protocols, it is
conceivable that the data was intended for a di erent use. The
issue becomes even more apparent when considering the expert
knowledge and model output gap Ô¨Ålling. Indeed, the concern
for the latter introducing error into the results was such, that
the study went ahead and calculated the M-F1 for each country
(NUTS0 region), for which the crop calendar information was
derived from expert knowledge or model output, and then com-
pared to the reference M-F1. No clear drop in M-F1 based on
the origin of the mature crop information was registered from
this analysis.
Another data issue is that bias can be introduced during
manual selection by visual assessment. Other than errors due to
distraction during annotation, the annotator undoubtedly bases
their decision on which images to keep and discard based on
their own discretion. For example, the annotator had to consider
questions like - should there be any sky or abundance of soil
visible on the image; is the crop on this image to be consideredmature enough; and, especially so for the cereal classes - is this
the correct label. The matter is even more pronounced when
selecting examples for unfavorable conditions. During which,
for example, the distinction between ‚ÄùBlurry‚Äù and ‚ÄùClose‚Äù was
sometimes hard to make, the object in the ‚ÄùObject‚Äù class and
the visual appearance of the landscape in the ‚ÄùLandscape‚Äù one
were very varied, and that sometimes the image showed more
than a single unfavorable condition - the crop can be both early
in the season and blurred out, in which case one could have
used multi-tags. Such issues were considered prior to undertak-
ing each task, yet the possibility of bias has to be mentioned.
Secondly, there are issues related to the processing logic of
certain steps. One such is the identiÔ¨Åcation of threshold points
for MP and ERP to generate quadrants. The way the custom
function works is by peeking into the correct-incorrect clas-
siÔ¨Åcation results in order to iteratively arrive at the threshold
with the main consideration being keeping the number of dis-
qualiÔ¨Åed correct classiÔ¨Åcations below a certain percentage. In
a sense this means putting the proverbial data cart before the
horse, as instead of using simply the values of whichever cho-
sen metrics, the function also considers the result of the classi-
Ô¨Åcation.
4.4. Recommendations
There are several recommendations that would be a logi-
cal continuation of the work. In terms of class selection, the
major part of the confusion stems from the cereal classes (Sec-
tion 3). This makes sense, as to distinguish between them can
sometimes be troublesome even for a skilled professional. As
a grouped cereal class they are easily set apart from the rest of
the crops, but between them, the structure of the fruit, stem, and
leaf organs can look too similar. Indeed, the approach in Gao
et al. [16] yields such good results exactly because the model is
11Figure 9: Comparison between using MP and ERP for applying Ô¨Åltering on results. Plot A‚Äôs Ô¨Årst Y axis shows the evolution in M-F1 in connected points when
applying a threshold on the inference set; and it‚Äôs secondary Y axis - the diminishing number of photos in continuous planes when applying the same thresholds.
Plots B and C represent the distributions in logarithmic scale (base 10) of values present in the inference set in terms of ERP and MP respectively. The red bars in
both histograms are the incorrect classiÔ¨Åcations.
designed to pick up on the subtle di erences between the vari-
eties. In the present case grouping the cereals together would
produce a M-F1 of 88.2 without and 90.4 with quadrant Ô¨Ål-
tering, which is 12.5 and 14.7 points higher than the achieved
result. Ideally one would capture the cereal class Ô¨Årst at these
higher ranges of M-F1 and then have a separate model that deals
solely with classifying the type of cereal, variety, or cultivar.
Concerning the point of being more robust in identifying
thresholds, or more generally on the topic of splitting the space
in Figure 7, one could build a kind of Bayesian Discriminant
Rule in order to generalize the combination of the two 1-D
thresholds to a 2-D threshold. This can be done by taking into
consideration the joint distributions and would yield a single
curve that separates correctly and incorrectly classiÔ¨Åed exam-
ples.
An always current topic in DL for CV is the e ect of resolu-
tion on results. In this case, one can discuss both the input res-
olution of the source images, and the input resolution of the net
in use. Firstly, the range of values of images‚Äô resolutions in the
inference set vary between 480-3504 in height and 640-4672 in
width - a 7.3 times di erence in each dimension. Almost 65%
of the images are of resolution 1600x1200, with another 22%
being 2048x1563 (for full breakdown of available image reso-
lutions check Supplementary Table A.5). With such a spread
one can imagine that the level of detail visible on images from
either end of the range is quite di erent. When measuring the
correlation between image resolution and the proportion of cor-rectly classiÔ¨Åed examples for each resolution bin (Figure A.11),
the study found an R-squared value of 0.009, meaning the cor-
relation for this set of LUCAS photos is almost none. Secondly,
the net input size is 224x224, meaning each parallelogram im-
age of the training and inference set gets re-scaled to this square
size. Intuitively, one can say that larger images would lose more
information during re-scaling than smaller ones. In reality, the
re-scaling turns the problem into a detection of the major struc-
tural features of the crops (e.g. broad leaf vs cereals, colouring,
having recognisable Ô¨Çowers or not), where resolution does not
matter as much. This would also shed light as to the reason why
the network has trouble distinguishing between cereal classes.
The analysis still serves to illustrate that the method is devel-
oped to handle equally well images from di erent resolutions.
This further showcases the policy relevance of the work, as in
an operational context, a regulating body is expected to receive
evidence-images in a variety of image resolutions.
5. Conclusion
This study provides a subset of LUCAS Cover photos for 12
major crops across the EU, to deploy, benchmark, and identify
the best conÔ¨Åguration of Mobile-net for the classiÔ¨Åcation task,
to showcase the possibility of using entropy-based metrics for
post-processing of results, and Ô¨Ånally to show the applications
and limitations of the model in a practical and policy relevant
context. The work has produced a dataset of 169,460 images
12of mature crops for the 12 classes, out of which 15,876 were
manually selected as representing a clean sample without any
foreign objects or unfavorable conditions. The best performing
model to identify crop achieved a Macro F1 (M-F1) of 0.75
on an imbalanced test dataset of 8,642 photos. Using metrics
from information theory resulted in achieving an increase of
6%. The most unfavorable conditions for taking such images,
across all crop classes, were found to be to early or late in the
season. The proposed methodology shows the possibility for
using minimal auxiliary data, outside the images themselves,
in order to achieve a M-F1 of 0.817 for labelling between 12
major European crops.
References
[1] AGRI4CAST Bulletin Archive 2018. https://agri4cast.jrc.ec.
europa.eu/BulletinsArchive#2018 . Accessed: 2021-06-30.
[2] Antoine A ouard, Herv ¬¥e Go ¬®eau, Pierre Bonnet, Jean-Christophe Lom-
bardo, and Alexis Joly. Pl@ ntnet app in the era of deep learning. In
ICLR: International Conference on Learning Representations , 2017.
[3] AGRI4CAST. Joint research center agri4cast resource portal. URL
https://agri4cast.jrc.ec.europa.eu/DataPortal/Index.
aspx?o= .
[4] Laith Alzubaidi, Jinglan Zhang, Amjad J Humaidi, Ayad Al-Dujaili,
Ye Duan, Omran Al-Shamma, Jos ¬¥e Santamar ¬¥ƒ±a, Mohammed A Fadhel,
Muthana Al-Amidie, and Laith Farhan. Review of deep learning: Con-
cepts, cnn architectures, challenges, applications, future directions. Jour-
nal of big Data , 8(1):1‚Äì74, 2021.
[5] Subodh Bansal and Anuj Kumar. A post-processing fusion framework
for deep learning models for crop disease detection. In IOP Conference
Series: Materials Science and Engineering , volume 998, page 012065.
IOP Publishing, 2020.
[6] James Bergstra and Yoshua Bengio. Random search for hyper-parameter
optimization. Journal of machine learning research , 13(2), 2012.
[7] Patrick Bogaert, Franc ¬∏ois Waldner, and Pierre Defourny. An information-
based criterion to measure pixel-level thematic uncertainty in land cover
classiÔ¨Åcations. Stochastic Environmental Research and Risk Assessment ,
31(9):2297‚Äì2312, 2017.
[8] Ivan Bruha and A Famili. Postprocessing in machine learning and data
mining. ACM SIGKDD Explorations Newsletter , 2(2):110‚Äì114, 2000.
[9] Louis Columbus. 10 ways ai has the potential to
improve agriculture in 2021. URL https://www.
forbes.com/sites/louiscolumbus/2021/02/17/
10-ways-ai-has-the-potential-to-improve-agriculture-in-2021/
?sh=316edd747f3b .
[10] European Commission. The common agricultural policy at a glance.
URL https://ec.europa.eu/info/food-farming-fisheries/
key-policies/common-agricultural-policy/cap-glance_en .
[11] Rapha ¬®el d‚ÄôAndrimont, Astrid Verhegghen, Michele Meroni, Guido
Lemoine, Peter Strobl, Beatrice Eiselt, Momchil Yordanov, Laura
Martinez-Sanchez, and Marijn van der Velde. Lucas copernicus 2018:
Earth-observation-relevant in situ data on land cover and use throughout
the european union. Earth System Science Data , 13(3):1119‚Äì1133, 2021.
[12] Rapha ¬®el d‚ÄôAndrimont, Momchil Yordanov, Laura Martinez-Sanchez, Pe-
ter Haub, Oliver Buck, Carsten Haub, Beatrice Eiselt, and Marijn van der
Velde. Lucas cover photos 2006‚Äì2018 over the eu: 874,646 spatially
distributed geo-tagged close-up photos with land cover and plant species
label. Earth System Science Data Discussions , pages 1‚Äì14, 2022.
[13] Wim Devos, Aleksandra Sima, and Pavel Milenov. Conceptual basis of
checks by monitoring.
[14] Rapha ¬®el d‚ÄôAndrimont, Momchil Yordanov, Laura Martinez-Sanchez, and
Marijn van der Velde. Monitoring crop phenology with street-level im-
agery using computer vision. Computers and Electronics in Agriculture ,
196:106866, 2022.
[15] EUROPABIO. Europabio o cial website. URL https://www.
europabio.org/ .
[16] Jiameng Gao, Chengzhong Liu, Junying Han, Qinglin Lu, Hengxing
Wang, Jianhua Zhang, Xuguang Bai, and Jiake Luo. IdentiÔ¨Åcation methodof wheat cultivars by using a convolutional neural network combined with
images of multiple growth periods of wheat. Symmetry , 13(11):2012,
2021.
[17] Xin Gao, Sundaresh Ram, Rohit C Philip, Je rey J Rodr ¬¥ƒ±guez, Jeno Szep,
Sicong Shao, Pratik Satam, Jes ¬¥us Pacheco, and Salim Hariri. Selecting
post-processing schemes for accurate detection of small objects in low-
resolution wide-area aerial imagery. Remote Sensing , 14(2):255, 2022.
[18] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen,
Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasude-
van, et al. Searching for mobilenetv3. In Proceedings of the IEEE /CVF
international conference on computer vision , pages 1314‚Äì1324, 2019.
[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mobilenets: E cient convolutional neural networks for mobile vision ap-
plications. arXiv preprint arXiv:1704.04861 , 2017.
[20] Keren Kapach, Ehud Barnea, Rotem Mairon, Yael Edan, and Ohad Ben-
Shahar. Computer vision for fruit harvesting robots‚Äìstate of the art and
challenges ahead. International Journal of Computational Vision and
Robotics , 3(1/2):4‚Äì34, 2012.
[21] E Koch, E Dittmann, W Lipa, A Menzel, J Nekovar, and AJH van Vliet.
Cost action 725: Establishing a european phenological data platform for
climatological applications. In 17th International Congress of Biometeo-
rology (ICB 20050), O enbach am Main 2005 , pages 554‚Äì558, 2005.
[22] Ilianna Kollia, Jack Stevenson, and Stefanos Kollias. Ai-enabled e cient
and safe food supply chain. Electronics , 10(11):1223, 2021.
[23] Yuzhen Lu and Sierra Young. A survey of public datasets for computer
vision tasks in precision agriculture. Computers and Electronics in Agri-
culture , 178:105760, 2020.
[24] Yunqian Ma and Haibo He. Imbalanced learning: foundations, algo-
rithms, and applications. 2013.
[25] Ivan Matvienko, Mikhail Gasanov, Anna Petrovskaia, Raghavendra Belur
Jana, Maria Pukalchik, and Ivan Oseledets. Bayesian aggregation im-
proves traditional single image crop classiÔ¨Åcation approaches. arXiv
preprint arXiv:2004.03468 , 2020.
[26] Sharada P Mohanty, David P Hughes, and Marcel Salath ¬¥e. Using deep
learning for image-based plant disease detection. Frontiers in plant sci-
ence, 7:1419, 2016.
[27] Juri Opitz and Sebastian Burst. Macro f1 and macro f1. arXiv preprint
arXiv:1911.03347 , 2019.
[28] Maxwell Owusu, Monika Ku er, Mariana Belgiu, Tais Grippa, Moritz
Lennert, Stefanos Georganos, and Sabine Vanhuysse. Towards user-
driven earth observation-based slum mapping. Computers, environment
and urban systems , 89:101681, 2021.
[29] Massimo Salvi, U Rajendra Acharya, Filippo Molinari, and Kristen M
Meiburger. The impact of pre-and post-image processing techniques on
deep learning frameworks: A comprehensive review for digital pathology
image analysis. Computers in Biology and Medicine , 128:104129, 2021.
[30] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and
Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottle-
necks. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 4510‚Äì4520, 2018.
[31] I Santoso, M Purnomo, AA Sulianto, and A Choirun. Machine learning
application for sustainable agri-food supply chain performance: a review.
InIOP Conference Series: Earth and Environmental Science , volume
924, page 012059. IOP Publishing, 2021.
[32] Christopher Schiller, Sebastian Schmidtlein, Coline Boonman, Alvaro
Moreno-Mart ¬¥ƒ±nez, and Teja Kattenborn. Deep learning and citizen sci-
ence enable automated plant trait predictions from photographs. ScientiÔ¨Åc
Reports , 11(1):1‚Äì12, 2021.
[33] A Sima, P Loudjani, and W Devos. Use of geotagged photographs in the
frame of common agriculture policy checks. 2020.
[34] Deepak Soekhoe, Peter van der Putten, and Aske Plaat. On the impact
of data set size in transfer learning using deep neural networks. In Inter-
national symposium on intelligent data analysis , pages 50‚Äì60. Springer,
2016.
[35] Pierre Soille, A Burger, D De Marchi, Pieter Kempeneers, D Rodriguez,
Vassilis Syrris, and V Vasilev. A versatile data-intensive computing plat-
form for information retrieval from big geospatial data. Future Genera-
tion Computer Systems , 81:30‚Äì40, 2018.
[36] Kaspars Sudars, Janis Jasko, Ivars Namatevs, Liva Ozola, and Niks
Badaukis. Dataset of annotated food crops and weed images for robotic
13computer vision control. Data in brief , 31:105833, 2020.
[37] Hongkun Tian, Tianhai Wang, Yadong Liu, Xi Qiao, and Yanzhou Li.
Computer vision technology in agricultural automation‚Äîa review. Infor-
mation Processing in Agriculture , 7(1):1‚Äì19, 2020.
[38] Gabriel Tseng, Ivan Zvonkov, Catherine Lilian Nakalembe, and Han-
nah Kerner. Cropharvest: A global dataset for crop-type classiÔ¨Åca-
tion. In Thirty-Ô¨Åfth Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 2) , 2021.
[39] USDA. United states department of agriculture foreign agricul-
ture service. URL https://ipad.fas.usda.gov/rssiws/al/crop_
calendar/europe.aspx .
[40] M. van der Velde, C.A. van Diepen, and B. Baruth. The european
crop monitoring and yield forecasting system: Celebrating 25 years of
JRC MARS bulletins. Agricultural Systems , 168:56‚Äì57, January 2019.
doi: 10.1016 /j.agsy.2018.10.003. URL https://doi.org/10.1016/
j.agsy.2018.10.003 .
[41] Franc ¬∏ois Waldner, Matthew C Hansen, Peter V Potapov, Fabian L ¬®ow, Ter-
ence Newby, Stefanus Ferreira, and Pierre Defourny. National-scale crop-
land mapping based on spectral-temporal features and outdated land cover
information. PloS one , 12(8):e0181911, 2017.
[42] Zhangnan Wu, Yajun Chen, Bo Zhao, Xiaobing Kang, and Yuanyuan
Ding. Review of weed detection methods based on computer vision. Sen-
sors, 21(11):3647, 2021.
[43] Yang-Yang Zheng, Jian-Lei Kong, Xue-Bo Jin, Xiao-Yi Wang, Ting-Li
Su, and Min Zuo. Cropdeep: The crop vision dataset for deep-learning-
based classiÔ¨Åcation and detection in precision agriculture. Sensors , 19(5):
1058, 2019.
[44] Lili Zhu, Petros Spachos, Erica Pensini, and Konstantinos N Plataniotis.
Deep learning and machine vision for food processing: A survey. Current
Research in Food Science , 4:233‚Äì249, 2021.Appendix A. Appendix A
14Figure A.10: Extracted harvest conditions of each crop for each country in the EU after CC harmonization and expert knowledge gap Ô¨Ålling.Range of pixels WxH included % of
images
Less than 1 mil-
lion640x480, 1024x768,
800x6001.39
1-2 million 1600x1200, 1280x960,
1632x1224, 1200x1600,
1200x900, 1605x1204,
1728x1152, 1288x966,
1600x1198, 1612x1212,
1700x1130, 1600x963,
1261x817, 1600x900,
1397x1048, 1593x1200,
1319x989, 1280x1024,
1552x116464.73
2-3 million 1664x1248, 2048x1360,
1920x1080, 1824x1216,
1733x1300, 1792x1312,
1936x1288, 1656x1242,
1984x1488, 2048x1104,
2000x1333, 1824x1368,
1936x1296, 1936x1452,
1920x1440, 1662x1246,
2080x1368, 1360x2048,
2048x1376, 1800x1350,
1632x12326.68
3-4 million 2048x1536, 2304x1728,
2272x1704, 2288x1712,
1536x2048, 2352x1568,
2592x1458, 2200x1650,
2042x1532, 2133x1600,
2240x1680, 2080x154422.32
4-5 million 2560x1712, 2560x1920,
2400x1800, 2344x1758,
2464x1632, 1932x2580,
2576x19322.33
5-6 million 2592x1944, 2816x2112, 3.46
6-7 million 2848x2136, 3072x2048,
3456x1946, 2896x2172,0.95
7-8 million 3072x2304, 3264x2448,
3584x20163.08
Over 8 million 3968x2976, 3488x2616,
4320x2432, 3664x2748,
4672x3504, 3840x28800.81
Table A.5: Breakdown of the kinds of image sizes present in the operational
inference set (8642 images)
15Figure A.11: Scatter-plot of the e ects of image resolution, represented by the product of the image dimensions in numbers of pixels, on the validity of the
classiÔ¨Åcation, represented by the proportion of correctly classiÔ¨Åed examples in each resolution bin. The correlation between the two is given by the R-squared value
at the bottom of the plot.
16