AutoWS: Automated Weak Supervision Framework for Text
Classiﬁcation
Abhinav Bohra
Amazon.com LLC
abohra@amazon.comHuy Nguyen
Amazon.com LLC
nguynnq@amazon.comDevashish Khatwani
Amazon.com LLC
khatwad@amazon.com
Abstract
Creating large, good quality labeled data
has become one of the major bottlenecks
for developing machine learning applica-
tions. Multiple techniques have been devel-
oped to either decrease the dependence of
labeled data (zero/few-shot learning, weak
supervision) or to improve the eﬃciency of
labeling process (active learning). Among
those, Weak Supervision has been shown
to reduce labeling costs by employing hand
crafted labeling functions designed by do-
main experts. We propose AutoWS –
a novel framework for increasing the eﬃ-
ciency of weak supervision process while
decreasing the dependency on domain ex-
perts. Our method requires a small set of
labeled examples per label class and auto-
matically creates a set of labeling functions
to assign noisy labels to numerous unla-
beled data. Noisy labels can then be ag-
gregated into probabilistic labels used by a
downstream discriminative classiﬁer. Our
framework is fully automatic and requires
no hyper-parameter speciﬁcation by users.
We compare our approach with diﬀerent
state-of-the-art work on weak supervision
and noisy training. Experimental results
showthatourmethodoutperformscompet-
itive baselines.
1 Introduction
Text classiﬁcation is among the most popular
Natural Language Processing (NLP) tasks, and
has important applications in real-world, e.g.,
product categorization. The advent of Deep
Learning has brought the state-of-the-art to
a wide variety of text classiﬁcation tasks (Mi-
naee et al., 2021). However, this comes with
a price: deep learning based models usually
require large amounts of annotated training
data to achieve superior performance. In many
scenarios, manual data annotation is expensive
in terms of cost and eﬀort, especially whensubject matter experts (SME) must involve
and/or classiﬁcation tasks are scaled to mil-
lions of instances with hundreds to thousands
of classes. To reduce the manual annotation
eﬀort, machine learning research have explored
weak supervision approaches, i.e., possibilities
to build prediction models using limited, noisy
or imprecise labeled data. Weak supervision
concerns special conditions of supervised learn-
ing in which annotated training data may be
incomplete, inexact or inaccurate (Zhou, 2018).
In this paper, we study incomplete supervision,
i.e., data has ground truth labels but is too
small to train a performant model, and focus
on semi-supervised learning techniques to lever-
age numerous amount of in-domain unlabeled
data to improve prediction performance.
Prior studies have covered diﬀerent methods
to assign noisy training labels to unlabeled data
including crowdsourcing (Dalvi et al., 2013;
Joglekar et al., 2015; Yuen et al., 2011; Zhang
et al., 2016), distant supervision (Hoﬀmann
et al., 2011; Mintz et al., 2009; Smirnova and
Cudré-Mauroux, 2018; Takamatsu et al., 2012),
heuristic rules (Awasthi et al., 2020; Ratner
et al., 2017, 2016; Varma et al., 2017). Pre-
trained language models gain much attention
recently because they can be ﬁne-tuned with
little annotated data thanks to their great gen-
eralization capability (Perez et al., 2021; Rad-
ford et al., 2019). The above weak supervi-
sion sources are termed labeling functions (LF),
which may vary in terms of error rate, coverage
and probably generate conﬂict labels (Zhang
et al., 2022, 2021). Researchers have developed
label models that aggregate output of label-
ing functions to generate conﬁdence-weighted
or probabilistic labels, which are consequently
used to train an end model (e.g., ﬁnal text
classiﬁer).
In this study, we propose AutoWS – an au-arXiv:2302.03297v1  [cs.CL]  7 Feb 2023tomated end-to-end weak supervision frame-
work for text classiﬁcation. AutoWS provides
a wide range of machine-learning based label-
ing functions ranging from statistical models
to transformer-based language models, and dif-
ferent label models. Aiming to provide users
a fully automated framework, AutoWS imple-
ments a simple yet eﬀective evaluation process
even it is provided just a small labeled dataset.
With our proposed evaluation process, only top
labeling functions and the best label model
are selected to produce ﬁnal labels to unlabeled
data. Contributions of our study are followings:
•AutoWSisafullyautomatedframeworkso
that users neither need to provide any la-
beling heuristics/functions nor tune hyper-
parameters.
•Our experiments cover a wide variety
of data domains including news, users’
request, product titles, and emphasizes
datasets with a large number of classes.
While weak supervision has been studied
for many years, its application on many-
class classiﬁcation tasks was not compre-
hensively evaluated.
•With a capability of selecting top labeling
functions and best label models, AutoWS
outperforms prior studies on benchmark
data.
2 Related Work
In past few years, research in weak-supervision
explored automatic labeling function genera-
tion to remove the reliance on subject matter
experts. Snuba system used three learning algo-
rithms, i.e., Decision Tree, Logistic Regressions,
and K-Nearest Neighbor, to learn multiple clas-
siﬁers by varying feature sets (Varma and Ré,
2018). TaLLoR , GLaRA systems based on a
set of simple logical rules (i.e., predicates) to
induce compound rules for using as labeling
functions in a named entity tagging task (Li
et al., 2021; Zhao et al., 2021). Despite diﬀer-
ences among approaches, those studies aimed
to maintain a large and diverse pool of labeling
functions so that label aggregation models can
work more eﬀectively. In our study, we create a
pool of 15 labeling functions which are learned
automatically from labeled data. Therefore,our approach does not require human-created
seed functions or probing feature subsets as in
prior studies.
Pre-training then ﬁne-tuning framework
shows great eﬀectiveness in many classiﬁca-
tion tasks, especially weak supervision (Rad-
ford et al., 2019). Notable work in this di-
rection includes ELMo (Peters et al., 2018),
BERT (Devlin et al., 2019), ULMFiT (Howard
and Ruder, 2018) and VAMPIRE (Gururangan
et al., 2019). Self-training further enhances
this paradigm by incorporating both labeled
and unlabeled data in an uniﬁed iterative ﬁne-
tuning process. (Clark et al., 2018; Miyato
et al., 2016; Xie et al., 2020). In our study,
instead of improving ﬁne-tuning methods, we
make use of large pre-trained language mod-
els to create strong base classiﬁers as labeling
functions. As a result, our approach empowers
the labeling function pool by increasing its pre-
diction power rather than using a large number
of weak predictors (e.g. rules, heuristics).
To systematically resolve conﬂicts and over-
lapping among noisy labeling functions, ad-
vanced label aggregation techniques have been
proposed to replace majority voting. Represen-
tative work from weak supervision include Data
Programming (Ratner et al., 2016), MeTaL
(Ratner et al., 2019) and FlyingSquid (Fu et al.,
2020). In our study, we utilize and compare
diﬀerent label aggregation methods (i.e. Label
Model) to achieve the best ﬁnal classiﬁcation
model. An advantage of AutoWS is a simple
yet eﬀective evaluation procedure which allows
us select the best method among candidates.
3 AutoWS Architecture
AutoWS is a multi-module pipeline that has
four main components: (1) learn Labeling Func-
tionsfrom labeled datasets, (2) generate proba-
bilistic labels using Label Models , (3) greedy
search the best label model and top label-
ing functions, and (4) learn ﬁnal classiﬁcation
model. AutoWS architecture is illustrated in
Figure 1.
3.1 Problem Setup
We are given a set X=fx1;x2;:::;x LgofL
labeled samples with corresponding ground-
truth labels Y=fy1;y2;:::;y Lg, whereyiis
label of sample xiwithi= 1::L,yi2[0;C 1],Lablled DataMajority VotingLF 3LF 15LF 1
Unlabelled Datanoisy labelsnoisy labelsnoisy labelsnoisy labelsOptimization: Select Top Labelling FunctionsAdjust ConﬁdenceScore Threshold
Generate Noisy LabelsGreedy SearchAggregate weak labelsSelectLabellingAggregatorFlyingSquid
Majority VotingDavid Skene
LF 2FlyingSquidDawid-SkeneOptimizationheuristics:• Select BestLabel Aggregator• Select TopLabeling Functions• Adjust ConﬁdenceScore ThresholdAggregatedProbabilisticLabels
Final ClassiﬁerAggregate Noisy Labels
AggregatedProbabilistic LabelsFinal ClassiﬁerFigure 1: AutoWS architecture
Cis number of label classes. Let set U=
fu1;u2;:::;u Ngofnunlabeled samples. For
convenience, we also use Yxto indicate label
value of data point x. Weak supervision setup
assumes XandUare drawn from the same dis-
tribution and LN. Let F=ff1;f2;:::;f Mg
be a set of Mlabeling functions. Let a label
assignment to u2Ubefi(u)2[ 1;C 1]
wherefi(u) = 1indicatesfiabstains from
giving a label to u. For an input x, let xbe a
feature vector by a feature extraction function,
e.g., Tf-Idf vectorizer.
3.2 Labeling Functions
AutoWS makes use of total 15 labeling func-
tions which is a blend of heuristics, statistical
machine learning and deep learning.
Feature-based Models To reduce fea-
ture engineering, we only consider word-
vectorization methods to transform each raw
input into a feature vector. Three word-
vectorizers are utilized:
•Word-count vectorizer1: this transforms
each input text into a word count vector.
In current implementation, we only extract
unigrams given small labeled data.
•Tf-Idf vectorizer2: this converts input
textintoterm-frequencyinverse-document-
frequency vector. Similarly, we only ex-
tract unigrams.
•Sentence embedding: A pre-trained sen-
tencetransformermodelisexecutedtogen-
erate embeddings of input text (Reimers
1Scikit-learn’s CountVectorizer
2Scikit-learn’s TﬁdfVectorizerand Gurevych, 2019). We use pre-trained
model paraphrase-MiniLM-L6-v23
Once inputs are transformed into feature
space, we use three supervised learning algo-
rithms to train classiﬁcation models. For each
algorithm, we implement an Bayesian Opti-
mizertoﬁne-tunefollowinglearningalgorithm’s
hyper-parameters through a K-fold cross vali-
dation on labeled data.
•Adaboost: number of trees, maximum tree
depth, learning rate, minimum number
of samples in leaf node, and number of
features when searching best split during
branching.
•Random Forest: number of trees, and max-
imum tree depth.
•Support Vector Machine: regularization
parameterCand kernel coeﬃcient .
We utilize Scikit-learn library to train 9 la-
beling models of this group (Pedregosa et al.,
2011).
Similarity-based Models Labeling func-
tions in this group will assign label to an in-
put when it is found similar to a labeled sam-
ple. We build three heuristics using above
feature extraction: word count, Tf-IDF, and
sentence embeddings. For similarity-based la-
beling, each unlabeled sample is assigned the
label of most similar labeled sample. Sam-
ple similarity sim(u;x)is the cosine similar-
ity between two feature vectors u;x. Let
x= max x2X(sim(u;x)), we haveYu=Yx.
When there are multiple xthat are equally
most similar to ubut have diﬀerent labels, we
3www.sbert.net/docs/pretrained_models.htmlassignuthe label that is most popular in X.
After the above process, each unlabeled sample
uis associated with a label Yuand a similarity
score simu=sim(u;x). We observed that in
some cases, simuis low and similarity-based
labeling may not reliable. To handle such cases,
we ranku2Uby their similarity score simu
from high to low and assign label -1 to samples
below 10th quantile.
Transformer-based Models This group
consists of three transformer-based models:
BERT (Devlin et al., 2019), ELECTRA (Clark
et al., 2020), and DeBERTa (He et al., 2021).
AutoWS uses pre-trained BERT (base-uncased
version), ELECTRA (base) and DeBERTa
(base) and ﬁne-tune the networks on labeled
data. While prior work experimented with
weak, simple labeling heuristics, we think
strong labeling functions can contribute better
to yield higher quality probabilistic labels for
unlabeled data. We use AutoGluon’s text pre-
dictor to train these models (Shi et al., 2021).
3.2.1 Scoring Labeling Functions
We hypothesize that not all labeling functions
help a label model generate good quality proba-
bilisticlabels. Thus, weimplementanoptimiza-
tion module that selects top labeling functions
through an iterative process (see Section 3.4.1).
An exhaustive search over all combination of
15 labeling functions is too computationally
expensive, so we propose a greedy search by
ranking labeling functions by prediction score.
We split labeled set Xin toXtrainandXdev
of 80% and 20% of samples respectively. We
useXtrainto train labeling functions and re-
port prediction accuracy on Xdev. Once label-
ing functions are evaluated, they are retrained
with complete dataset X. Labeling functions’
accuracy scores are provided to optimization
module (Section 3.4.1)
3.3 Label Models
Unlabeled data is passed through newly trained
labeling functions to generate noisy labels (each
data point is assigned multiple noisy labels).
Then noisy labels are aggregated to generate
ﬁnal labels for unlabeled data. Current im-
plementation of AutoWS includes three label
aggregators: Majority Voting heuristic, Dawid-
Skene model (Dawid and Skene, 1979) and Fly-ingSquidmodel(Fuetal.,2020). Weimplement
unweighted majority voting so that it simply
counts how many LF vote for a label without
considering label prior or LF evaluation score.
Abstaining LF does not vote.
Dawid-Skene model (Dawid and Skene, 1979)
assumes an unobserved or latent ground truth
labelYuof sampleuand the probabilities that
LFfiprovides correct or incorrect label fi(u)
are only dependent on YuFlyingSquid method
models the joint probability distribution of hid-
den true labels and observed LF outputs as a
Binary Ising model when simplifying the clas-
siﬁcation task into binary version (Fu et al.,
2020). Each LF fis augmented by two binary
observed variables f0andf1which are depen-
dent to latent ground truth label. A Triplet
Method is used to solve the optimization prob-
lem through a closed form set of equations so
no learning is needed.
3.4 Final Classiﬁcation Model
Given a set of labeling functions (LF), a label
model and a numerical threshold t, we ﬁrst
run LF on unlabeled data Uto generate label
matrixLwhereLijkeeps LF output fi(uj).
Label matrix is fed into label model to gener-
ate probabilistic labels for every u2U. We
discard samples uwhose labels have aggregated
probabilistic score less than t,4and let the re-
maining set U0=fu:p(u)tg. Finally, to
train the ﬁnal classiﬁcation model, we ﬁne-tune
DeBERTa with the union set X[U0.
3.4.1 Optimization Module
With multiple labeling functions and label mod-
els, we propose a greedy search algorithm to
ﬁnd the best label model and top labeling func-
tions. As mentioned above, for each conﬁgura-
tion of a LF set, a label model and a threshold
t, we obtain a dataset U0with weak labels. We
evaluate the prediction power of those weak
labels by ﬁne-tuning DeBERTa using U0and
record prediction performance on X.5. We per-
form the following search heuristic:
1.With label model set Aand probability
threshold set T, we generatejAjjTj
4We utilize output score returned by label model
implementation in WRENCH
5This process is diﬀerent than training the ﬁnal
classiﬁer in that it uses Xas test data rather than
training dataTable 1: Dataset statistics
Dataset #Classes Train/Dev Test
AGNews 4 120,000 7,600
Yahoo! 10 1,400,000 60,000
Retail 21 41,586 4,642
conﬁgurations in which each conﬁguration
includes all LF, a label model A2Aand
a threshold t2T. By evaluating these
conﬁgurations, we obtain the best label
modelA, and threshold t.
2.With a set of quantiles Qwe generatejQj
conﬁgurations in which each has top qLF
(q2Q), label model A, and threshold
t. Evaluating these conﬁgurations give us
the bestqto select top LF.
Provided the best conﬁguration of top qLF,
label model Aand threshold t, we generate
probabilistic labels for unlabeled data and train
ﬁnal classiﬁcation model.
4 Data and Baseline Models
In order to evaluate AutoWS and compare with
diﬀerent baseline models, we experiment with
three many-class public datasets as shown in
Table 1. First two datasets are AGNews – news
articles (Zhang et al., 2015) and Yahoo!– user
questions/answers (Chang et al., 2008). We
use the data split by MixText and compare
our results with this study (Chen et al., 2020).
MixText achieved the best results among prior
semi-supervised learning methods on the data
by adapting the novel Mixup idea. MixText
relies on transformer-based models to encode
textual inputs into hidden vectors so that they
can interpolate two inputs to create a new train-
ing instance. Furthermore MixText uses pre-
dictions on unlabeled samples and their corre-
sponding back-translations to assign labels to
the unlabeled set.
Our third dataset is Retail (Elayanithottathil
and Keuper, 2021) comprising of 46,228 prod-
uct items We compare AutoWS with the best
noisy-label model reported in (Nguyen and
Khatwani, 2022). In the study, the authors
applied noise-resistance training to address a
challenge that training data has incorrect la-
bels. Weak labels generated by AutoWS areconsidered noisy. So this comparison gives us
insight of applying noise-resistance training in
weak supervision.
5 Experimental Results
5.1 AGNews and Yahoo! Datasets
Following MixText (Chen et al., 2020), we sub-
sample 200 inputs per class from original train-
ing data to create labeled dataset X, and com-
pileunlabeleddatawith5,000samplesperclass.
Test data is kept intact (see Table 1). Results
are shown in Table 2. As shown in row SOTA,
MixText achieved accuracy 89.2 for AGNews
and 71.3 for Yahoo! data. These were reported
signiﬁcantly higher than prior weak supervision
methods, i.e., VAMPIRE (Gururangan et al.,
2019) and UDA (Xie et al., 2020). Our Au-
toWS with optimization outperforms MixText
by yielding accuracy 89.4 and 72.3 for AGNews
and Yahoo! data respectively.6
To demonstrate the added-value by weak su-
pervision components of AutoWS, we report
performance of other settings: (1) use only la-
beled data to train the ﬁnal classiﬁer, (2) run
AutoWS without optimization (i.e., output of
all labeling functions are provided to Major-
ity Voting). In the ﬁrst setting, all DeBERTa,
ELECTRA and BERT performed signiﬁcantly
worse than AutoWS. This result is consistent
with prior studies, and further proves the es-
sential impact of leveraging noisy label data to
performance improvement. In the second set-
ting, by feeding output of all labeling function
outputs to Majority Voting, we observe signiﬁ-
cant accuracy decrease. This proves the critical
role of optimization module in AutoWS.
5.2 Retail Dataset
Retail data was used in (Nguyen and Khatwani,
2022) to study impact of noisy label (i.e., item
label is incorrect) to prediction performance.
The prior study experimented with training
data which was corrupted to have 20% and
40% samples have incorrect labels. The authors
trained an LSTM-CNNs model on noisy data,
and found that CoTeachPlus training method
(Yu et al., 2019) achieved the best performance
improvement. CoTeachPlus co-trains two mod-
6Our results are average over 3 runs with diﬀerent
random seeds and every run returns better performance
than MixText.Table 2: Prediction performance (accuracy 
100%) on AGNews, Yahoo! datasets.
AGNews Yahoo!
Train samples per class 200 200
Unlabeled data total 20,000 50,000
SOTA 89.2 71.3
DeBERTa 85.7 67.4
ELECTRA 81.2 60.9
BERT 88.3 69.6
AutoWS (wo/ Opt) 88.1 70.0
AutoWS (w/ Opt) 89.4 72.3
Table 3: Prediction performance (test macro F1 
100%) on Retail data
Condition Model F1
Clean label LSTM-CNNs 82.0
Noise rate 20% CoTeachPlus 78.5
Noise rate 40% CoTeachPlus 73.2
100 per class AutoWS 68.7
200 per class AutoWS 73.1
els simultaneously but let each model only pass
samples with small loss and prediction disagree-
ment to the other model for training.
AutoWS generates weak labels for unlabeled
dataset, thus the ﬁnal classiﬁer is trained on
noisy data. Performance on Retail data is
shown in Table 3. In our analysis, using 100
labeled samples per class, AutoWS generates
weak label training data which has noise rate
21%. The ﬁnal classiﬁer obtain F1 score 68.7
on test data. With 200 labeled samples per
class, the generated weak labels has noise rate
17% and ﬁnal accuracy is improved to 73.1.
This shows that increasing labeled data size
will lead to higher quality weak label data.
However, AutoWS performance is signiﬁcantly
lower than CoTeachPlus even though our weak
label data has lower noise rate than the prior
study. This reveals the limit of regular training
on noisy data. In future, we plan to apply noise-
resistance training methods, e.g., CoTechPlus,
to train the ﬁnal classiﬁer.
5.3 Labeling Function Ranking
Given a pool of 15 labeling functions, we con-
duct an analysis to reveal how labeling func-tions (LF) perform in diﬀerent settings. For
each dataset, we record LF scores (see Sec-
tion 3.2.1), and rank LF from high to low. We
observe that deep-learning based LF are not
always at top positions. For example, in AG-
News data, BERT has the highest prediction
score but the second and third places belong to
Support Vector Machine (SVM) and Random
Forest (RF) with sentence embeddings feature.
DeBERTa and ELECTRA takes 4th and 7th
positions respectively. More interestingly, in
Banking77 data, we see SVM, Similarity-based
Labeling and RF are top-3. Both BERT and
DeBERTa show up in top-3 LF in Yahoo! and
Product datasets. These ﬁndings support our
decision to keep a diverse set of LF with pres-
ence of heuristics, statistical machine learning
and deep-learning. In order to make AutoWS
more robust, we plan to pre-train transformer-
based language models for diﬀerent domains,
e.g., product, dialogue, to use as LF and ﬁnal
classiﬁer.
6 Conclusions
In this paper, we described AutoWS – a fully
automated weak supervision framework which
allows users to build competitive text classiﬁers
with a small amount of labeled data. AutoWS
implements a diverse set of labeling functions
and three diﬀerent label aggregation models
so that it can produce probabilistic labels for
unlabeled data. We proposed a simple yet eﬀec-
tive optimization heuristic to select top labeling
function and the best label model. This makes
AutoWS more robust than other weak super-
vision systems which require user to provide
labeling heuristics, choose label model or tune
hyper-parameters. Experiment results show
that AutoWS outperform prior weak supervi-
sion models on benchmark data.
In future, we will extend AutoWS with
domain-speciﬁc models, and more label aggre-
gation methods. Furthermore, we will enhance
optimization module and equip AutoWS with
noise-resistance training solutions to address
noisy labels generated by the system. Last but
not least, we plan to open-source AutoWS to
gain more attentions and interests from not
only NLP/ML experts but also developers and
researchers outside of the ﬁelds. Having Au-
toWS employed by end-users with little to zeroknowledge of NLP/ML to solve real-world text
classiﬁcation applications with limited data is
our ultimate goal.
References
Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal,
andSunitaSarawagi.2020. LearningfromRules
Generalizing Labeled Exemplars. In 8th In-
ternational Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020 . OpenReview.net.
Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth,
and Vivek Srikumar. 2008. Importance of Se-
mantic Representation: Dataless Classiﬁcation.
InAAAI.
Jiaao Chen, Zichao Yang, and Diyi Yang. 2020.
MixText: Linguistically-Informed Interpolation
of Hidden Space for Semi-Supervised Text Clas-
siﬁcation. In Proceedings of the 58th Annual
Meeting of the Association for Computational
Linguistics , pages 2147–2157, Online. Associa-
tion for Computational Linguistics.
Kevin Clark, Minh-Thang Luong, Quoc V. Le,
and Christopher D. Manning. 2020. ELECTRA:
Pre-training Text Encoders as Discriminators
Rather Than Generators. In ICLR.
Kevin Clark, Minh-Thang Luong, Christopher D.
Manning, and Quoc Le. 2018. Semi-Supervised
Sequence Modeling with Cross-View Training.
InProceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 1914–1925, Brussels, Belgium. Asso-
ciation for Computational Linguistics.
Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar,
and Vibhor Rastogi. 2013. Aggregating Crowd-
sourced Binary Ratings. In Proceedings of the
22nd International Conference on World Wide
Web, WWW ’13, pages 285–294, New York,
NY, USA. Association for Computing Machin-
ery. Event-place: Rio de Janeiro, Brazil.
A. Philip Dawid and Allan Skene. 1979. Maxi-
mum Likelihood Estimation of Observer Error-
Rates Using the EM Algorithm. Journal of The
Royal Statistical Society Series C-applied Statis-
tics, 28:20–28.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of Deep Bidirectional Transformers for Lan-
guage Understanding. In Proceedings of the
2019 Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 4171–4186, Min-
neapolis, Minnesota. Association for Computa-
tional Linguistics.Febin Sebastian Elayanithottathil and Janis Ke-
uper. 2021. A Retail Product Categorisation
Dataset. _eprint: 2103.13864.
Daniel Y. Fu, Mayee F. Chen, Frederic Sala,
Sarah M. Hooper, Kayvon Fatahalian, and
Christopher Ré. 2020. Fast and Three-rious:
Speeding Up Weak Supervision with Triplet
Methods. In Proceedings of the 37th Interna-
tional Conference on Machine Learning, ICML
2020, 13-18 July 2020, Virtual Event , volume
119 ofProceedings of Machine Learning Re-
search, pages 3280–3291. PMLR.
Suchin Gururangan, Tam Dang, Dallas Card, and
Noah A. Smith. 2019. Variational Pretraining
for Semi-supervised Text Classiﬁcation. In Pro-
ceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics , pages
5880–5894, Florence, Italy. Association for Com-
putational Linguistics.
Pengcheng He, Xiaodong Liu, Jianfeng Gao,
and Wei Chen. 2021. DeBERTa: Decoding-
Enhanced BERT with Disentangled Attention.
In2021 International Conference on Learning
Representations .
Raphael Hoﬀmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-Based Weak Supervision for Infor-
mation Extraction of Overlapping Relations.
InProceedings of the 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies , pages 541–550,
Portland, Oregon, USA. Association for Com-
putational Linguistics.
Jeremy Howard and Sebastian Ruder. 2018. Uni-
versal Language Model Fine-tuning for Text
Classiﬁcation. In Proceedings of the 56th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages328–
339, Melbourne, Australia. Association for Com-
putational Linguistics.
Manas R. Joglekar, Hector Garcia-Molina, and
Aditya G. Parameswaran. 2015. Comprehensive
and reliable crowd assessment algorithms. 2015
IEEE 31st International Conference on Data
Engineering , pages 195–206.
Jiacheng Li, Haibo Ding, Jingbo Shang, Julian
McAuley, and Zhe Feng. 2021. Weakly Su-
pervised Named Entity Tagging with Learnable
Logical Rules. In Proceedings of the 59th An-
nual Meeting of the Association for Computa-
tional Linguistics and the 11th International
Joint Conference on Natural Language Process-
ing (Volume 1: Long Papers) , pages 4568–4581,
Online. Association for Computational Linguis-
tics.
Shervin Minaee, Nal Kalchbrenner, Erik Cambria,
Narjes Nikzad, Meysam Chenaghlu, and Jian-
feng Gao. 2021. Deep Learning–Based TextClassiﬁcation: A Comprehensive Review. ACM
Comput. Surv. , 54(3). Place: New York, NY,
USA Publisher: Association for Computing Ma-
chinery.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language Process-
ing of the AFNLP ,pages1003–1011, Suntec, Sin-
gapore. Association for Computational Linguis-
tics.
Takeru Miyato, Andrew M. Dai, and Ian Good-
fellow. 2016. Adversarial Training Methods for
Semi-Supervised Text Classiﬁcation.
Huy Nguyen and Devashish Khatwani. 2022.
Robust Product Classiﬁcation with Instance-
Dependent Noise. In Proceedings of The Fifth
Workshop on e-Commerce and NLP (ECNLP 5) ,
pages 171–180, Dublin, Ireland. Association for
Computational Linguistics.
Fabian Pedregosa, Gaël Varoquaux, Alexandre
Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Pret-
tenhofer, Ron Weiss, Vincent Dubourg, Jake
Vanderplas, Alexandre Passos, David Courna-
peau, Matthieu Brucher, Matthieu Perrot, and
Édouard Duchesnay. 2011. Scikit-learn: Ma-
chine Learning in Python. Journal of Machine
Learning Research , 12(85):2825–2830.
Ethan Perez, Douwe Kiela, and Kyunghyun Cho.
2021. True Few-Shot Learning with Language
Models. In Advances in Neural Information Pro-
cessing Systems .
Matthew E. Peters, Mark Neumann, Mohit Iyyer,
Matt Gardner, Christopher Clark, Kenton Lee,
and Luke Zettlemoyer. 2018. Deep Contextu-
alized Word Representations. In Proceedings
of the 2018 Conference of the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers) , pages 2227–2237,
New Orleans, Louisiana. Association for Compu-
tational Linguistics.
Alec Radford, Jeﬀ Wu, Rewon Child, David
Luan, Dario Amodei, and Ilya Sutskever. 2019.
Language Models are Unsupervised Multitask
Learners.
Alexander J. Ratner, Stephen H. Bach, Henry R.
Ehrenberg, Jason Alan Fries, Sen Wu, and
C. Ré. 2017. Snorkel: Rapid Training Data Cre-
ation with Weak Supervision. Proceedings of the
VLDB Endowment. International Conference on
Very Large Data Bases , 11 3:269–282.Alexander J. Ratner, Braden Hancock, Jared A.
Dunnmon, Frederic Sala, Shreyash Pandey, and
C. Ré. 2019. Training Complex Models with
Multi-Task Weak Supervision. Proceedings of
the ... AAAI Conference on Artiﬁcial Intelli-
gence. AAAI Conference on Artiﬁcial Intelli-
gence, 33:4763–4771.
Alexander J. Ratner, Christopher De Sa, Sen Wu,
Daniel Selsam, and C. Ré. 2016. Data Program-
ming: Creating Large Training Sets, Quickly.
Advances in neural information processing sys-
tems, 29:3567–3575.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence Embeddings using Siamese
BERT-Networks. In Proceedings of the 2019
Conference on Empirical Methods in Natural
Language Processing . Association for Computa-
tional Linguistics.
Xingjian Shi, Jonas Mueller, Nick Erickson, Mu Li,
and Alex Smola. 2021. Multimodal AutoML
on Structured Tables with Text Fields. In 8th
ICML Workshop on Automated Machine Learn-
ing (AutoML) .
Alisa Smirnova and Philippe Cudré-Mauroux.
2018. Relation Extraction Using Distant Super-
vision: A Survey. ACM Comput. Surv. , 51(5).
Place: New York, NY, USA Publisher: Associa-
tion for Computing Machinery.
Shingo Takamatsu, Issei Sato, and Hiroshi Naka-
gawa. 2012. Reducing Wrong Labels in Distant
Supervision for Relation Extraction. In Proceed-
ings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1:
Long Papers) , pages721–729, JejuIsland, Korea.
Association for Computational Linguistics.
P. Varma, Bryan D. He, Payal Bajaj, Nishith
Khandwala, I. Banerjee, D. Rubin, and Christo-
pher Ré. 2017. Inferring Generative Model
Structure with Static Analysis. Advances in
neural information processing systems , 30:239–
249.
P. Varma and C. Ré. 2018. Snuba: Automating
Weak Supervision to Label Training Data. Pro-
ceedings of the VLDB Endowment. International
Conference on Very Large Data Bases , 12 3:223–
236.
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang
Luong, and Quoc V. Le. 2020. Unsupervised
Data Augmentation for Consistency Training.
InProceedings of the 34th International Con-
ference on Neural Information Processing Sys-
tems, NIPS’20, Red Hook, NY, USA. Curran
Associates Inc. Event-place: Vancouver, BC,
Canada.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu,
Ivor Tsang, and Masashi Sugiyama. 2019. Howdoesdisagreementhelpgeneralizationagainstla-
bel corruption? In International Conference on
Machine Learning , pages 7164–7173. PMLR.
Man-Ching Yuen, Irwin King, and Kwong-Sak Le-
ung. 2011. A Survey of Crowdsourcing Systems.
In2011 IEEE Third International Conference
on Privacy, Security, Risk and Trust and 2011
IEEE Third International Conference on Social
Computing , pages 766–773.
Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao
Zhang, and Alexander J. Ratner. 2022. A Sur-
vey on Programmatic Weak Supervision. ArXiv,
abs/2202.05433.
Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang,
YamingYang, MaoYang, andAlexanderRatner.
2021. WRENCH: A Comprehensive Benchmark
for Weak Supervision. In Thirty-ﬁfth Confer-
ence on Neural Information Processing Systems
Datasets and Benchmarks Track .
Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015. Character-Level Convolutional Networks
for Text Classiﬁcation. In Proceedings of the
28th International Conference on Neural Infor-
mation Processing Systems - Volume 1 , NIPS’15,
pages 649–657, Cambridge, MA, USA. MIT
Press. Event-place: Montreal, Canada.
Yuchen Zhang, Xi Chen, Dengyong Zhou, and
Michael I. Jordan. 2016. Spectral Methods
Meet EM: A Provably Optimal Algorithm for
Crowdsourcing. Journal of Machine Learning
Research , 17(102):1–44.
Xinyan Zhao, Haibo Ding, and Zhe Feng. 2021.
GLaRA: Graph-based Labeling Rule Augmen-
tation for Weakly Supervised Named Entity
Recognition. In Proceedings of the 16th Con-
ference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Vol-
ume, pages 3636–3649, Online. Association for
Computational Linguistics.
Zhi-Hua Zhou. 2018. A brief introduction to
weakly supervised learning. National Science
Review, 5:44–53.