‚ÄòTax-free‚Äô 3DMM Conditional Face Generation
Yiwen Huang Zhiqiu Yu Xinjie Yi Yue Wang James Tompkin
Brown University
Abstract
3DMM conditioned face generation has gained traction
due to its well-defined controllability; however, the trade-off
is lower sample quality: Previous works such as DiscoFace-
GAN and 3D-FM GAN show a significant FID gap compared
to the unconditional StyleGAN, suggesting that there is a
quality tax to pay for controllability. In this paper, we chal-
lenge the assumption that quality and controllability cannot
coexist. To pinpoint the previous issues, we mathematically
formalize the problem of 3DMM conditioned face generation.
Then, we devise simple solutions to the problem under our
proposed framework. This results in a new model that effec-
tively removes the quality tax between 3DMM conditioned
face GANs and the unconditional StyleGAN.
1. Introduction
Face image generation has wide application in com-
puter vision and graphics. Among different works in this
area, deep learning generative model approaches are espe-
cially good at generating high-quality photo-realistic face
images [12,14]. However, generative models provide limited
explicit control over their output due to their unsupervised
nature, relying instead on latent space manipulation [16]. On
the other hand, parametric models such as 3D Morphable
Models (3DMMs) embed facial attributes in a disentangled
parameter space, but their results lack photorealism [24].
In light of this, researchers have tried to build mod-
els that can synthesize high-resolution novel face images
with control by combining 3DMM with generative model-
ing [1, 3, 5, 18, 27]. Existing attempts can be roughly divided
into two categories: rigging and conditional generation. Rig-
based methods attempt to align the 3DMM parameter space
with the latent space of a pre-trained generative model [1,27].
Sample quality is not compromised by controllability; how-
ever, controllability is limited by the completeness and disen-
tanglement of the underlying latent space [28]. Conditional
generation methods use the 3DMM when training the gener-
ative model [3, 5, 18]. These offer improved controllability
but reduced sample quality since additional constraints are
imposed upon the generated samples for 3DMM consistency
and disentanglement.
Generated ImageIdentity
ExpressionIlluminationAngleSame 3DMM, random variation
Figure 1. 3DMM conditioned GANs show reduced image gen-
eration quality as a ‚Äòtax‚Äô for their added control. This tax is not
inevitable. Our approach produces images of almost equivalent
quality to unconditional generation while being at least as disentan-
gled for control.
We investigate the family of 3DMM conditional GAN
models. Deng et al. state that the quality drop in conditional
models is an inevitable tax that we pay for controllability [3].
What causes this tax? We hypothesize that it is caused by
overconstraint: that, to achieve consistency with the 3DMM
conditioning anddisentanglement among latent variables,
current methods have unnecessary side effects that compro-
mise quality. We challenge the claim of a ‚Äòquality tax‚Äô and
show that it can be largely eliminated if the overconstraints
can be identified and resolved. To this end, we formalize
3DMM conditioned face generation and identify minimal
solutions that satisfy controllability and disentanglement.
To summarize, our contributions are threefold:
‚Ä¢We propose a mathematical framework for 3DMM condi-
tioned face generation, and unify existing methods under
such formulation. This allows us to analyze the consis-
tency and the disentanglement behavior rigorously.
‚Ä¢We derive new methods to achieve consistency and disen-
tanglement from our mathematical framework. We show
that our methods are both theoretically justified and per-
form favorably against previous work in practice.
‚Ä¢We demonstrate a StyleGAN2-based model trained by our
methods that achieves state-of-the-art FID while maintain-
ing the full controllability of 3DMM.arXiv:2305.13460v2  [cs.CV]  26 May 20232. Method
2.1. Background and Problem Formulation
We define face images in a dataset ÀÜx‚àà X. We also define
a 3DMM code vector by p={zid, zexp, zillum, zangle, ztrans}, a
noise vector z, and a generator model G(p, z) :P √óZ ‚Üí X .
The goal of conditional generation is to create photorealistic
face images xaccording to pandz. For our goal, we can
form two related yet distinct objectives: consistency and
disentanglement .
Consistency. This objective requires that xis semantically
consistent with p,i.e.,pdictates the corresponding semantic
factors in x. We follow the formulation in InfoGAN [2]
and formalize the consistency objective as maximizing the
mutual information I(p;x)between pandx:
I(p;x) =H(p)‚àíH(p|x)
=Ex‚àºG(p,z)
Ep‚Ä≤‚àºP(p|x)[logP(p‚Ä≤|x)]
+H(p)
=Ep‚àºP(p),x‚àºG(p,z)[logP(p|x)] +H(p) (1)
For 3DMM conditioned face generation, the posterior
P(p|x)becomes tractable when the generative distribution
Pgbecomes sufficiently close to the distribution of real face
images. In such case, the posterior is exactly represented by
a pretrained face reconstruction model [4] that can accurately
predict pgiven x, allowing I(p;x)to be directly optimized.
Past works [3, 18] propose proxy objectives instead of
directly maximizing I(p;x). These objectives maximize
I(p;x)up to some deterministic transformation on p. We
show that directly optimizing the mutual information objec-
tive is better than optimizing proxy objectives. Further, as
the assumption that Pgis sufficiently close to the real image
distribution does not hold in general early in training, we
also introduce a progressive blending mechanism.
Disentanglement. Changing one semantic factor should
not interfere with other semantic factors. Let P ‚à™ Z =
{z0, z1, . . . , z n}where zidenotes the latent code for an in-
dependent semantic factor. We formally define disentangle-
ment following Peebles et al. [25]:
‚àÇ2G
‚àÇzj‚àÇzi= 0 ‚àÄiÃ∏=j (2)
Suppose we define a subset of latent factors that control
3DMM factors; zi‚àà P . For these, disentanglement is
achieved by construction via the consistency objective. The
remaining problem is to disentangle unsupervised factors
zj‚àà Z from zi‚àà P. Finally, as noted, the disentangling
of unsupervised factors zj‚àà Z from each other is an open
question [20, 23] and does not relate to 3DMM conditioning.
In the simplest case where Gis a scalar function and
each semantic factor ziis also a scalar, Eq. 2 indicates thatthe Hessian matrix HGis diagonal. In such case, disentan-
glement can be directly encouraged by a Hessian penalty.
However, it is observed that a Hessian penalty has a strong
negative impact on image quality (measured by FID [9]) [25]
and a solution to this problem is not yet clear.
As we found for consistency, disentanglement is also ap-
proximated by proxy objectives in previous work [3, 18].
We notice that all such approximations are restrictive; they
degrade image quality and rely on hand-designed rules that
only work for certain zi. To this end, we propose an alter-
native approach to the disentanglement problem. We show
in the following section that, in practice, disentanglement
can be achieved for free without any optimization via the
inductive bias of a carefully designed network.
2.2. Consistency via pRendering & Estimation
We maximize Eq. 1 to enforce semantic consistency be-
tween pandx. However, there remains a design space of
deterministic transformations on pto obtain a more amenable
representation for conditioning and optimizing G. To this
end, we use a differentiable renderer RDR [4] to derive a
3DMM representation that aligns with the image space per-
ceptually, and is independent of external factors such as the
PCA bases that pdepends on. Specifically, we let RDR
output the 3DMM rendered image rfromp, the Lambertian
albedo a, and the normal map n:
r, a, n =RDR (p) (3)
We define our 3DMM representation ‚Äò rep‚Äô as the Cartesian
product of r,aandn:rep(p) =r√óa√ón.Given the new
3DMM representation, we update Eq. 1:
I(rep(p);x) =Ep‚àºP(p),x‚àºG(rep(p),z)[logP(rep(p)|x)] +C(4)
where Cis the constant term H(rep(p)).
Consistency loss. Given a pretrained face reconstruction
model FR[4]:X ‚Üí P , we rewrite Eq. 4 as follows:
Lconsistency =Ep‚àºP(p),x‚àºG(rep(p),z)h
‚à•rep(FR(x))‚àírep(p)‚à•p
pi
.(5)
The choice of pdepends on our assumption about the func-
tional form of the posterior. We follow common assumptions
and assume Gaussian error, which leads to p = 2. [6]
Progressive blending. The posterior P(p|x)can only be
represented by FRwhen Pgis sufficiently close to the real
image distribution. In early training with Eq. 5, xis not a
realistic image and so FR(x)is nonsensical. To circumvent
this problem, we introduce a progressive blending variant of
Eq.5, following the intuition that ris always a close enough
approximation of the real face for FR:
L‚àó
consistency =Ep‚àºP(p),x‚àºG(rep(p),z)[d] (6)
d=‚à•rep(FR(Œ±x+ (1‚àíŒ±)r(p)))‚àírep(p)‚à•2
2where Œ±is a scalar that grows linearly from 0 to 1 in the first
ktraining images. We empirically find that this simple strat-
egy is sufficient to solve the intractable posterior problem
early in the training.
2.3. Structurally Disentangled Conditioning
Next, we discuss how we use rep(p)to condition G.
We generate per-layer conditioning feature maps c=
{c1, . . . , c l}using an encoder E, and inject each ciinto the
corresponding layer of the synthesis network as an auxiliary
input. We show that our conditioning method approximates
Eq. 2 without supervision [3,18], achieving disentanglement
for free as an inductive bias of the network architecture.
Feature injection. We extend each synthesis layer lito
take an auxiliary input cn‚àíiwhere nis the number of layers
in the synthesis network. The synthesis layer in [14] is
implemented by a stylized convolution where each channel
fjof the input feature maps fis scaled by sij. The per-
layer scaling vector si={sij‚àÄj}is computed from the
style vector wivia an affine transformation. We note that the
injected feature maps cn‚àíineed to be handled separately for
stylization. This is because cn‚àíiis essentially an embedding
ofPwhile wiis an embedding of Z. It is clear that Pis not
controlled by Zand therefore cn‚àíishould not be subject to
wi. To this end, we simply fix the scaling of each channel of
cn‚àíito 1 for stylization.
Disentanglement analysis. To simplify analysis, we omit
various details from the StyleGAN2 [14] generator (weight
demodulation, noise injection, equalized learning rate, etc.).
We formulate each layer liof the synthesis network as:
li(p, z) =Wi‚àó[cn‚àíi(p);si(z)‚äôœÉ(li‚àí1(p, z))] + Bi(7)
Wiis the weight tensor of li,Biis the bias tensor of li,
‚àódenotes convolution, ‚äôdenotes the Hadamard product,
andœÉis the activation function. There are two terms in
lithat depend on p:cn‚àíiandœÉ(li‚àí1). First, we analyze
disentanglement w.r.t. cn‚àíi:
‚àÇ2li
‚àÇz‚àÇcn‚àíi=‚àÇ
‚àÇz‚àÇ
‚àÇcn‚àíi(Wi‚àó[cn‚àíi;si‚äôœÉ(li‚àí1)] + Bi)
=‚àÇ
‚àÇz
Wi‚àó‚àÇ
‚àÇcn‚àíi[cn‚àíi;si‚äôœÉ(li‚àí1)]
=‚àÇ
‚àÇz(Wi‚àó[I; 0])
= 0 (8)
We see that variation in cn‚àíiis perfectly disentangled from
variation in z, therefore any non-zero‚àÇ2li
‚àÇz‚àÇpmust be the result
of variation in œÉ(li‚àí1):
idexpillumangleFigure 2. Finite difference approximation of the partial derivative of
the injected 3DMM render features w.r.t. the 3DMM parameters‚àÇc
‚àÇp.
We can see that the derivative maps are sparse, with the variation in
cdepicted in small white regions, indicating that disentanglement
is mostly successful.
‚àÇ2li
‚àÇz‚àÇp=‚àÇ2li
‚àÇz‚àÇœÉ (li‚àí1)‚àÇœÉ(li‚àí1)
‚àÇp
=
Wi‚àó
0;‚àÇsi
‚àÇz‚àÇœÉ(li‚àí1)
‚àÇp(9)
We examine the behavior of variation in p:
‚àÇœÉ(li‚àí1)
‚àÇp=‚àÇœÉ(li‚àí1)
‚àÇli‚àí1‚àÇli‚àí1
‚àÇp
=‚àÇœÉ(li‚àí1)
‚àÇli‚àí1
Wi‚àí1‚àó‚àÇcn‚àíi+1
‚àÇp;si‚àí1‚äô‚àÇœÉ(li‚àí2)
‚àÇp(10)
This analysis on‚àÇœÉ(li‚àí1)
‚àÇpapplies recursively to‚àÇœÉ(li‚àí2)
‚àÇp;
thus,‚àÇ2G
‚àÇz‚àÇp‚Üí0if‚àÄi.‚àÇci
‚àÇp‚Üí0.
In practice, we empirically find that small variation in p
does lead to little total variation in c. Variation in ctends
to be highly localized to small affected regions dictated by
p, with little variation otherwise (Fig. 2). This is likely the
combination effect of localized variation in repw.r.t. pand
the inductive bias of locality of a convolutional encoder. We
do not consider‚àÇ2G
‚àÇp‚àÇzas disentanglement in this direction is
automatically enforced by Lconsistency when pairing each p
with a set of different zs.
3. Experiments
Following previous works [3, 18], we evaluate our meth-
ods on 256√ó256 FFHQ [13]. We compare our model
against StyleGAN2 and two state-of-the-art 3DMM-based
generative models, DiscoFaceGAN (DFG) [3] and 3D-FM
GAN [18]. As the leading SOTA method 3D-FM GAN
does not have public code or models, comparison is difficult.
Where possible, we took results from their paper, but some
quantitative metrics could only be computed for our model
and for DiscoFaceGAN.
3.1. Qualitative Comparison
Our model achieves highly controllable generation while
preserving StyleGAN‚Äôs ability to generate highly photoreal-
istic images (Fig. 3). We can see that our model can produce
photorealistic faces with diverse races, genders, and ages.Identity Variation
 Expression Variation
Illumination Variation
 Pose Variation
Figure 3. Generated face samples with control as output from our model. While some unwanted variation remains, identity, expression,
illumination, and angle are controlled with high fidelity and no apparent visual artifacts.
ReferenceFacesGeneratedFrom the Same ùëùWhile Varying ùëß
Figure 4. Resampling the noise vector zwith the same set of
3DMM coefficients pshows high facial consistency, while other
unsupervised factors such as hair, hat, eyeglasses, and background
vary with z.
It also shows effective control over each of the 3DMM at-
tributes. Particularly, we use the same person as our base
image for all attribute edits; this verifies that our model can
perform robust generation with high quality. Fig. 4 com-
pares the images generated by our model conditioned on
the same pbut different z. The identity, expression, pose,
and illumination are preserved while all other attributes can
be modified. This means there is little overlap between at-
tributes controlled by pandz, and our model gains control
over target attributes.
3.2. Quantitative Comparison
We evaluate the performance of our model in terms of
quality and disentanglement.
Fr¬¥echet inception distance (FID) For image quality, we
compute the FID [9] against the entire FFHQ dataset as a
measure of the generation quality. Our model outperforms
the two state-of-the-art baselines, yielding an FID much
closer to the original StyleGAN trained on 256 √ó256 FFHQ
dataset (Tab. 1).
Disentanglement Score (DS) Introduced in DiscoFace-
GAN, this quantifies the disentanglement efficacy of each
of the four 3DMM-controlled attributes. For attribute vec-
torui‚àà {zid, zexp, zillum, zangle}, we first randomly sample
1K sets of the other three attribute vectors, denoted by
u{j}={uj:j= 1, ...,4, jÃ∏=i}. Then, for each set of
u{j}, we randomly sample 10 ui. In total, we have 10K
3DMM coefficients and hence generate 10K images. Then,Table 1. Our conditioning provides control and almost equivalent
quality to unconditioned baseline StyleGAN2. The baseline 3DMM
conditioning approaches do not produce comparable quality in
terms of FID and DS.
Method FID‚ÜìDS id‚ÜëDS exp‚ÜëDS illum‚ÜëDS angle‚Üë
StyleGAN2 3.78 - - - -
Ours 4.51 1.02 3.22 48.7 1245
DiscoFaceGAN 12.9 0.37 1.64 47.9 829
3D-FM GAN 12.2 - - - -
we re-estimate uiandu{j}using the 3D reconstruction net-
work [4]. For each attribute, we compute the L2 norm of the
difference between each uand the mean uvector and get the
mean L2 norm in each of the 1K sets. We then get œÉuiand
œÉuj‚Äôs by averaging the corresponding mean L2 norm over
the 1K sets and normalize them by the L2 norm of the mean
uvector computed on the entire FFHQ dataset. Finally, we
compute the disentanglement score:
DS(ui) =Y
j,jÃ∏=iœÉui
œÉuj(11)
A high DS indicates that when an attribute vector is
modified, only the corresponding attribute is changed on the
generated image while all other attributes remain unchanged.
Our model outperforms DiscoFaceGAN by large margins in
identity, expression, and pose (angle) control (Table 1).
4. Conclusion and Future Work
We present a simple conditional model derived from a
mathematical framework for 3DMM conditioned face gener-
ation. Our model shows strong performance in both quality
and controllability, reducing the need to choose between the
two and making control ‚Äòtax free‚Äô. Furthermore, our math-
ematical framework can be applied to future explorations
in conditional generation, allowing future investigators to
analyze other 3DMMs rigorously. However, our model does
not come without limitations. Unlike 3D-FM GAN [18],
our model is not specifically designed for image editing.
Thus, faces suffer the same inversion accuracy vs. editability
tradeoff as StyleGAN [12 ‚Äì14]. Future work might consider
applying the image editing techniques proposed by Liu et
al. [18] to our model for better face editing.References
[1]Rameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka.
Styleflow: Attribute-conditioned exploration of stylegan-
generated images using conditional continuous normalizing
flows. ACM Transactions on Graphics (ToG) , 40(3):1‚Äì21,
2021. 1
[2]Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable rep-
resentation learning by information maximizing generative
adversarial nets. CoRR , abs/1606.03657, 2016. 2
[3]Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin
Tong. Disentangled and controllable face image generation
via 3d imitative-contrastive learning, 2020. 1, 2, 3, 7
[4]Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia,
and Xin Tong. Accurate 3d face reconstruction with weakly-
supervised learning: From single image to image set. In
Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition workshops , pages 0‚Äì0, 2019. 2, 4, 7,
9
[5]Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ran-
jan, Michael J Black, and Timo Bolkart. Gif: Generative
interpretable faces. In 2020 International Conference on 3D
Vision (3DV) , pages 868‚Äì878. IEEE, 2020. 1
[6]Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning . MIT Press, 2016. http://www.
deeplearningbook.org . 2
[7]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial networks. Communi-
cations of the ACM , 63(11):139‚Äì144, 2020. 7
[8]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770‚Äì778, 2016. 7
[9]Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-
hard Nessler, and Sepp Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium.
Advances in neural information processing systems , 30, 2017.
2, 4
[10] IEEE. A 3D Face Model for Pose and Illumination Invariant
Face Recognition , Genova, Italy, 2009. 7
[11] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,
Jaakko Lehtinen, and Timo Aila. Training generative ad-
versarial networks with limited data. Advances in neural
information processing systems , 33:12104‚Äì12114, 2020. 7
[12] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¬®ark¬®onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. Advances in Neural Infor-
mation Processing Systems , 34:852‚Äì863, 2021. 1, 4, 7
[13] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 4401‚Äì4410, 2019. 3, 4, 7, 8
[14] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of theIEEE/CVF conference on computer vision and pattern recog-
nition , pages 8110‚Äì8119, 2020. 1, 3, 4, 7, 8
[15] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,
Jaakko Lehtinen, and Timo Aila. Modular primitives for high-
performance differentiable rendering. ACM Transactions on
Graphics , 39(6), 2020. 7
[16] Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine
Bordes, Ludovic Denoyer, and Marc‚ÄôAurelio Ranzato. Fader
networks: Manipulating images by sliding attributes. CoRR ,
abs/1706.00409, 2017. 1
[17] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceedings
of the IEEE , 86(11):2278‚Äì2324, 1998. 7
[18] Yuchen Liu, Zhixin Shu, Yijun Li, Zhe Lin, Richard Zhang,
and SY Kung. 3d-fm gan: Towards 3d-controllable face
manipulation. In European Conference on Computer Vision ,
pages 107‚Äì125. Springer, 2022. 1, 2, 3, 4
[19] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 11976‚Äì11986,
2022. 7
[20] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar
Raetsch, Sylvain Gelly, Bernhard Sch ¬®olkopf, and Olivier
Bachem. Challenging common assumptions in the unsu-
pervised learning of disentangled representations. In inter-
national conference on machine learning , pages 4114‚Äì4124.
PMLR, 2019. 2
[21] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rec-
tifier nonlinearities improve neural network acoustic models.
7
[22] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
Which training methods for gans do actually converge? In
International conference on machine learning , pages 3481‚Äì
3490. PMLR, 2018. 7
[23] Weili Nie, Tero Karras, Animesh Garg, Shoubhik Debnath,
Anjul Patney, Ankit B Patel, and Anima Anandkumar. Semi-
supervised stylegan for disentanglement learning. In Pro-
ceedings of the 37th International Conference on Machine
Learning , pages 7360‚Äì7369, 2020. 2
[24] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romd-
hani, and Thomas Vetter. A 3d face model for pose and
illumination invariant face recognition. In 2009 sixth IEEE
international conference on advanced video and signal based
surveillance , pages 296‚Äì301. Ieee, 2009. 1
[25] William Peebles, John Peebles, Jun-Yan Zhu, Alexei A. Efros,
and Antonio Torralba. The hessian penalty: A weak prior for
unsupervised disentanglement. In Proceedings of European
Conference on Computer Vision (ECCV) , 2020. 2
[26] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-
xl: Scaling stylegan to large diverse datasets. In ACM SIG-
GRAPH 2022 conference proceedings , pages 1‚Äì10, 2022. 7
[27] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian
Bernard, Hans-Peter Seidel, Patrick P ¬¥erez, Michael Zollh ¬®ofer,
and Christian Theobalt. Stylerig: Rigging stylegan for 3d
control over portrait images. CoRR , abs/2004.00121, 2020. 1
[28] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace
analysis: Disentangled controls for stylegan image generation.InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 12863‚Äì12872,
June 2021. 1
[29] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou,
Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer
is actually what you need for vision. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recog-
nition , pages 10819‚Äì10829, 2022. 7
[30] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup
initialization: Residual learning without normalization. arXiv
preprint arXiv:1901.09321 , 2019. 8Appendix
A. Implementation Details
We implement our model on top of the official Style-
GAN2 [14] and the PyTorch release of Deep3DRecon [4].
FRandRDR are both part of Deep3DRecon [4] and Gand
Dare part of StyleGAN2 [14]. We use the dataset tool pro-
vided in Deep3DRecon [4] to realign FFHQ [13] so that
image xaligns with 3DMM representation rep.
StyleGAN2 backbone. We follow the latest findings in
StyleGAN3 [12] and omit several insignificant details to
simplify StyleGAN2 [14]. We remove mixing regularization
and path length regularization. The depth of the mapping
network is decreased to 2, as recommended by Karras et
al. It is also noticed that decreasing the dimensionality of
zwhile maintaining the dimensions of wis beneficial [26].
Therefore, we reduce the dimensions of zto 64. All details
are otherwise unchanged, including the network architecture,
equalized learning rate, minibatch standard deviation, weight
(de)modulation, lazy regularization, bilinear resampling, and
exponential moving average of the generator weights.
Face reconstruction and differentiable renderer. We
use the pretrained checkpoint provided by Deng et al. [4]
forFR. This updated checkpoint was trained on an aug-
mented dataset that includes FFHQ [13] and shows slight
performance improvement over the TensorFlow release of
Deep3DRecon. We use the differentiable renderer RDR
that comes with the checkpoint for FRfrom the same code
repository. This renderer uses the Basel Face Model from
2009 [10] as the 3DMM parametric model for face modeling,
and nvdiffrast [15] for rasterization. We modify RDR so
it outputs aandnalong with r. The renderer is otherwise
unchanged.
Training procedure. Following the StyleGAN family [12 ‚Äì
14], we adopt the non-saturating loss [7] and R1 gradient
penalty [22] as the loss function for GAN training. We
additional append our Lconsistency , resulting in the following
objectives:
LD=‚àíEp,z[log(1 ‚àíD(G(rep(p), z)))]‚àí
Ex[log(D(x))] +Œ≥
2Exh
‚à•‚àáD(x)‚à•2
2i
(12)
LG=‚àíEp,z[log(D(G(rep(p), z)))] + ŒªLconsistency (13)
We closely follow the training configurations of the baseline
model in Karras et al. [11] and set Œ≥= 1. The batch size is
set to 64 and the group size of minibatch standard deviation
is set to 8. We empirically set Œª= 20 and the length of
Bilinear ‚ÜìConv 1 √ó1Conv 1 √ó1LReLUConv 3 √ó3LReLUConv 1 √ó1*+Transition LayerResBlock1
ResBlock2
LReLUConv 1 √ó1Bilinear ‚ÜìConv 1 √ó1LReLU
toFeat2toFeat1Figure 5. The detailed breakdown of a general stage of E.
progressive blending to k= 2√ó106. The learning rate of
bothGandDis set to 2.5√ó10‚àí3. We train our model until
Dsees 25M real images [12‚Äì14].
Instead of approximating the distribution P(p)using a
V AE [3], we simply use its empirical distribution when sam-
pling p‚àºP(p)and find this to be sufficient given our
3DMM representation.
B. Encoder Architecture
We follow the architecture design of StyleGAN2 and split
Einto different resolution stages. For each resolution stage
eiofE, we produce two sets of feature maps c2iandc2i+1
to condition the two synthesis layers of the corresponding
resolution stage of the synthesis network:
ei=(
E0(rep(p))i= 0
Ei(ei‚àí1)iÃ∏= 0
c2i=toFeat 2i(ei)
c2i+1=toFeat 2i+1(ei)(14)
We implement Eias a sequence of a transition layer and
two residual blocks (Fig. 5). ‚Äò toFeat ‚Äô is implemented by a
1√ó1convolution [17] with optional downsampling [13] and
leaky ReLU activation [21]. Following recent advances in
network architecture [19, 29], our ResNet [8] design of E
differs from the architecture of D[14] in several ways.
General stage. We notice that the two architectural
changes in [19] that lead to most performance boost are
separate downsampling layers and fewer activations. Thus,
we move the skip branch of the transition residual block up
to the stem as a transition layer, and remove all activations
in the residual block unless they are between two consec-
utive convolutional layers. We use leaky ReLU activation
withŒ±= 0.2, and bilinear downsampling instead of strided
convolution [13, 14]. We use the 1-3-1 bottleneck residual
block as it is more efficient than the 3-3 block [8]. The finalIdentityReference
Figure 6. Reference-based generation results. We extract the ex-
pression, illumination, and pose coefficients from the reference
images (first row) and apply them to randomly generated images
(first column).
convolutional layer (marked by *) in the residual block is
initialized to 0 [30], and this eliminates the need for nor-
malization or residual rescaling [14]. We apply equalized
learning rate to all convolutional layers.
Specialization. We remove bilinear downsampling from
the transition layer of the highest resolution stage; it is oth-
erwise identical to a general stage. Since the 4√ó4stage of
the synthesis network contains only one synthesis layer, we
place one toFeat layer without leaky ReLU in the 4√ó4stage
ofEaccordingly.
C. More Results
We show additional results in controlled generation that
display the robustness of our model and explain what control
exists in the non-conditioned zspace.
Reference-based generation In Fig. 6, we task our model
with reference-based generation where we keep the identity
of a generated image and swap its expression, illumination,
and pose with those of a real image. We can see that the
respective attributes from their source are all well preserved,
and the image quality does not degrade. This again demon-
strates the disentangled face generation from our model.
Feature granularity To inspect the impact of feature vari-
ability across the layers of the decoder, we inspect the im-
SourceBSourceACoarseMiddleFine
SourceBSourceACoarseMiddleFine
Figure 7. Style mixing results at different scales. Using the same
three images for Source A and Source B, we replace the style
vectors of images from Source A by the style vectors of images from
Source B at coarse resolutions (4 √ó4 - 8√ó8), middle resolutions
(16√ó16 - 32√ó32), and fine resolutions (64 √ó64 - 256 √ó256).
pact of swapping features across images with the same p. In
Fig. 7, we randomly pick a 3DMM coefficient vector pand
randomly sample z‚Äôs to generate three images (the same im-
ages for Source A and Source B). Following StyleGAN [13],
we replace some of the style vectors w+of images from
Source A by the corresponding style vectors of images from
Source B at coarse, middle, and fine scales. As pis the same,
the overall face region will not change significantly.GeneratedImageFacesGeneratedFromtheSameùëßWhileVaryingùëù
Figure 8. Resampling the 3DMM coefficient vector pwith the
same noise vector zshows high consistency in the background and
clothes while the face completely changes.
At coarse scale, there is no visible change to the images
from Source A. This is expected as the high-level attributes
of the image are supposed to be determined by the pvector.
At middle scale, the images from Source A remain mostly
unchanged except finer facial features such as the hair now
resemble those in the image from Source B. At fine scale,
the images from Source A undergo more significant changes
where the color scheme that affects the background, clothes,
hair color, and skin color now resembles those in the image
from Source B. This experiment indicates that each subset of
the style vectors w+controls a different set of features in the
generated image. We also notice that attributes controlled by
premain unchanged at any scale, which means our model‚Äôs
pspace and zspace are well separated.
3DMM vector resampling with fixed noise As opposed
to the experiment conducted in the main paper where we
vary the noise zwith fixed 3DMM vector p, we now vary
pwith fixed zas in Fig. 8. We can see that despite the
drastic change in the facial attributes from different p‚Äôs, the
background and clothes remain largely consistent with the
same z. This is another proof that the zvector has a good
control of the attributes not controlled by p.
Limitations. Due to the use of a pretrained FRandRDR ,
our model inevitably inherits the limitations of these models.
We find that Deep3DRecon [4] performs particularly poor
on darker skin tone, in that it tends to predict the skin tone
as the result of dim illumination. This leads to unexpected
skin tone change when editing the illumination (Figure.9).
Moreover, our model does not provide explicit control over
attributes not represented in Psuch as hair and eyeglasses.
We believe these restrictions can be resolved in the future by
an improved 3DMM.
AlbedoReference
Figure 9. Reference-based generation results that show unexpected
skin tone change. We see that the albedo predicted by FRdoes not
faithfully capture the darker skin tone.