Evaluating Verifiability in Generative Search Engines
Nelson F. Liu∗Tianyi Zhang Percy Liang
Computer Science Department
Stanford University
nfliu@cs.stanford.edu
Abstract
Generative search engines directly generate re-
sponses to user queries, along with in-line ci-
tations. A prerequisite trait of a trustworthy
generative search engine is verifiability , i.e.,
systems should cite comprehensively (high ci-
tation recall; all statements are fully supported
by citations) and accurately (high citation pre-
cision; every cite supports its associated state-
ment). We conduct human evaluation to audit
four popular generative search engines—Bing
Chat, NeevaAI, perplexity.ai, and YouChat—
across a diverse set of queries from a variety of
sources (e.g., historical Google user queries,
dynamically-collected open-ended questions
on Reddit, etc.). We find that responses from
existing generative search engines are fluent
andappear informative, but frequently con-
tain unsupported statements and inaccurate ci-
tations: on average, a mere 51.5% of generated
sentences are fully supported by citations and
only 74.5% of citations support their associated
sentence. We believe that these results are con-
cerningly low for systems that may serve as
a primary tool for information-seeking users,
especially given their facade of trustworthiness.
We hope that our results further motivate the
development of trustworthy generative search
engines and help researchers and users better
understand the shortcomings of existing com-
mercial systems.
1 Introduction
Generative search engines fulfill user information
needs by directly generating responses to input
queries, along with in-line citations (Figure 1).1
Existing generative search engines are rapidly gain-
ing users—in March 2023, Microsoft reported that
“roughly one third of daily preview users are using
*This work would not be possible without the 34 annotators
who performed human evaluation; we thank them for their
contributions.
1In contrast, conventional search engines are limited to
retrieving pre-existing webpages.
What are the latest discoveries from the James Webb 
Space Telescope? 
The James Webb Space Telescope is designed to peer  
into the dusty clouds of gas where stars and planetary  
systems are born.  Webb has captured the ﬁrst direct  
image of an exoplanet, and the Pillars of Creation in the  
Eagle Nebula [1][2] . Additionally, the telescope will be  
used to study the next interstellar interloper [3].
     nasa.gov (     citation does not support its associated statement) 
NASA's Webb Conﬁrms Its First Exoplanet 
... Researchers conﬁrmed an exoplanet, a planet that orbits another 
star, using NASA’s James Webb Space Telescope for the ﬁrst time. ... 
     cnn.com (     citation partially supports its associated statement) 
Pillars of Creation: James Webb Space Telescope ... 
... The Pillars of Creation, in the Eagle Nebula, is a star-forming region 
captured in a new image (right) by the James Webb Space Telescope 
that reveals more detail than a 2014 image (left) by Hubble ... 
     nasa.gov (     citation fully supports its associated statement) 
Studying the Next Interstellar Interloper with Webb 
...Scientists have had only limited ability to study these objects once 
discovered, but all of that is about to change with NASA's James 
Webb Space Telescope...The team will use Webb's spectroscopic 
capabilities in both the near-infrared and mid-infrared bands to study 
two different aspects of the interstellar object. Cited Webpages 
[1]: 
[2]: 
[3]: 
(*Some generated statements may not be fully supported  by 
citations, while others are fully supported. )
Figure 1: Generative search engines answer user queries
by generating a tailored response, along with in-line
citations. However, not all generated statements are
fully supported by citations (citation recall), and not
every citation supports its associated statement (citation
precision).
[Bing] Chat daily”, and that Bing Chat served 45
million chats in the first month of its public preview
(Mehdi, 2023). Generative search engines have the
potential to transform how people find informa-
tion online, but generated responses from existing
large language model-backed generative search en-
gines may not always be accurate (Maynez et al.,
2020). Given their potential and rapid mainstream
adoption, it is critical to evaluate these systems to
better understand their potential limitations (akin
to prior work in algorithmic auditing; Metaxas
and Pruksachatkun, 2017; Buolamwini and Gebru,
2018; Kiritchenko and Mohammad, 2018; Robert-
son et al., 2018; Metaxa et al., 2019; Green and
Chen, 2019; Birhane et al., 2022, inter alia ).arXiv:2304.09848v2  [cs.CL]  23 Oct 2023A prerequisite trait of a trustworthy generative
search engine is verifiability ,2that is, each gen-
erated statement about the external world should
be fully supported by a set of in-line citations, and
each provided citation should support its associated
statement. Verifiability enables readers to easily
check that any generated statement is supported by
its cited source.
We conduct a human evaluation to audit four
popular commercial generative search engines
(Bing Chat, NeevaAI, perplexity.ai, and YouChat)
across a diverse set of information-seeking queries
(e.g., various types of historical Google user
queries from NaturalQuestions (Kwiatkowski et al.,
2019), dynamically-collected open-ended ques-
tions from Reddit; see Appendix A for examples).
For each query-response pair, we use human
evaluation to measure a variety of dimensions:
1.fluency (whether the generated text is fluent
and cohesive; §2.2);
2.perceived utility (whether the generated an-
swer is helpful and informative; §2.2);
3.citation recall (the proportion of generated
statements about the external world that are
fully supported by their citations; §2.3); and
4.citation precision (the proportion of generated
citations that support their associated state-
ments; §2.4).
A trustworthy generative search engine should
achieve high citation recall and precision, indicat-
ing that its generated citations are comprehensive
(every generated statement is fully supported by
citation) and correct (every citation supports its
associated statement).
We find that existing generative search engine re-
sponses often have high fluency and perceived util-
ity (§4.1), but frequently contain unsupported state-
ments or inaccurate citations (low citation recall
and precision; §4.2). On average, merely 51.5% of
generated sentences are fully supported with cita-
tions (citation recall), and only 74.5% of citations
support their associated sentence (citation preci-
sion). Furthermore, citation precision is inversely
correlated with perceived utility ( r=−0.96); the
responses that seem more helpful are often those
with inaccurate citations (§4.3). This facade of
trustworthiness increases the potential for existing
generative search engines to mislead users. For
example, in Figure 1, a user with little background
2We adopt the term verifiability from the Wikipedia com-
munity. Verifiability is a core content policy of Wikipedia:
statements must be fully supported by provided sources.knowledge about the James Webb Space Telescope
(motivating a query about its recent discoveries)
will likely struggle to identify unsupported state-
ments in the generated response. We hypothesize
that citation precision is inversely correlated with
perceived utility because generative search engines
often copy or closely paraphrase from their cited
webpages (§4.4). This improves citation preci-
sion because copied text is often supported by the
cited webpage, but decreases perceived utility when
copied statements are irrelevant to the query or the
rest of the generated response.
We make the following contributions: first, we
define the citation recall and citation precision eval-
uation metrics, which aim to encourage the devel-
opment of systems that cite comprehensively and
correctly. Second, we conduct a human evaluation
of four popular generative search engines, finding
that responses are broadly fluent and appear useful,
but frequently contain unsupported statements and
inaccurate citations, increasing their potential to
mislead users. Third, we observe that perceived
utility is inversely correlated with citation precision
in existing generative search engines, and hypothe-
size that this inverse correlation occurs when some
systems copy or closely paraphrase from cited web-
pages. To facilitate further work on developing
trustworthy generative search engines, we have re-
leased our human evaluation annotations.3
2 Human Evaluation of Fluency,
Perceived Utility, and Verifiability
In this section, we formalize the inputs and outputs
of the generative search engines we study, describe
the evaluation of fluency and perceived utility, and
define and describe the evaluation of citation recall
and precision. Citation recall and precision are de-
signed to reward systems that cite comprehensively
(i.e., high recall; all statements are fully supported
by citations) and accurately (i.e., high precision; ev-
ery cite supports its associated statement). We also
define citation F1, a metric that combines citation
precision and citation recall.
2.1 Task Formulation
Given a user query qas input, a generative search
engine produces a text response r, which is a string
with embedded in-line citations. For the example in
Figure 1, the query qis “What are the latest discov-
3github.com/nelson-liu/evaluating-verifiability-in-
generative-search-engineseries from the James Webb Space Telescope?” and
the response ris the string paragraph “The James
Webb Space Telescope ... used to study the next in-
terstellar interloper [3].”, with embedded citations
“[1]”, “[2]”, and “[3]”.
To evaluate citation precision and recall, we
first segment the rinto a set of nstatements
S={s1, . . . , s n}. In this work, the segmenta-
tionSis set of sentences in the response r. For
each statement si∈ S, we construct a (possibly
empty) set Ci={ci,1, . . . , c i,k}ofkcitations as-
sociated with the statement si, where ci,jis the
jth citation associated with the ith response state-
ment. For each citation ci,j, we have a URL ui,j
and its contents pi,j. In this work, Ciis set of cita-
tions that occur in si(e.g., for si= “Blueberries[1],
cherries[2], and grapes[3] grow on trees.[4]”, Ci=
{[1],[2],[3],[4]}).
In practice, a sentence may contain multiple
independently-verifiable claims (e.g., conjuncts
such as “Cups can be made of glass[1] or plas-
tic[2].”), and a single in-line citation’s scope is
often ambiguous (e.g., a cite marker after two state-
ments could be interpreted as either supporting
both statements, or merely the final one); we leave
finer-grained evaluation to future work.
2.2 Measuring Fluency and Perceived Utility
To measure response fluency, annotators were
shown the user query, the generated response, and
the claim “The response is fluent and cohesive”.
We ask annotators to rate their level of agreement
with the claim on a five-point Likert scale from
Strongly Disagree toStrongly Agree . We use a
similar process to measure perceived utility, asking
annotators to rate their level of agreement with the
claim “The response is a helpful and informative
answer to the query”.
2.3 Measuring Citation Recall
Citation recall is the proportion of verification-
worthy statements that are fully supported by their
associated citations (see Figure 2 for several ex-
amples). Thus, computing citation recall requires
(i) identifying the verification-worthy statements
in a response and (ii) evaluating whether each
verification-worthy statement is fully supported by
its associated citations.
Identifying verification-worthy statements.
Given the statements Sin a response r, we first ask
annotators to remove statements in the response
First generated  statement [1 ✅
][2 ❌
][3 ⚠
].
Second generated statement [1 ✅
][2 ❌
][4 ❌
].
Third generated statement [4 ✅
][5 ⚠
].
Citation Recall : 3/3 = 100% 
Citation Precision : 3/8 = 37.5% 
First generated statement [1 ⚠
][2 ⚠
].
Second generated statement [2 ❌
].
Third generated statement. 
Citation Recall : 1/3 = 33% 
Citation Precision : 2/3 = 66% 
First generated statement [1 ✅
][2 ✅
][3 ❌
].
Second generated statement. 
Third generated statement. 
Citation Recall : 1/3 = 33% 
Citation Precision : 2/3 = 66% 
     : highlighted statement is fully supported by citations 
     : highlighted statement is not fully supported by citations. 
✅
: citation fully supports its associated statement. 
⚠
: citation partially supports its associated statement. 
❌
: citation does not support its associated statement. Figure 2: Examples of calculating citation recall and
precision. Citation recall measures the proportion of
generated statements that are supported by citations. Ci-
tation precision measures the proportion of citations that
support their associated statements. Partially-supporting
citations only improve citation precision when their as-
sociated statement is supported by the union of its cita-
tions and no other associated citation fully supports the
statement by itself (middle example).
that are not verification-worthy. We take the
position that every generated statement about
the external world is verification-worthy, even
those that might seem obvious, trivially true, or
“common sense”. Generated statements may be
incorrect, and statements that seem obvious to
some readers may be less than obvious to others
(e.g., “The Pope is Catholic”). We believe that
systems should aim to provide a source for all
generated statements about the external world,
enabling readers to easily verify anystatement in a
generated response.
In practice, almost all system-generated state-
ments are verification-worthy—notable exceptions
include statements about the speaker (the system)
itself (e.g., “As a language model, I do not have
the ability to ban books.”) and questions posed
to the user (e.g.,“Would you like to learn more?”,
generated by systems like Bing Chat and YouChat
that are deployed in conversational settings).
Evaluating whether a verification-worthy state-
ment is fully supported by its associated cita-
tions. Given the verification-worthy statements
in a response r, annotators evaluate whether each
statement is fully supported by its associated cita-tions (see the sentences of generated response in
Figure 1 for examples). To collect these binary
judgments, we use the attributable to identified
sources (AIS) evaluation framework of Rashkin
et al. (2022). In particular, a statement siis fully
supported by its associated citations Ciif a generic
hearer would affirm the statement “According to
cited webpages Ci,si”, within the context of the
query qand response r, and unsupported otherwise.
2.4 Measuring Citation Precision
Citation precision is the proportion of generated
citations that support their associated statements
(Figure 2). In contrast to citation recall, citation
precision rewards systems for citing accurately—a
response that cites every webpage on the Internet
for each generated statement would have high cita-
tion recall, but low citation precision (since many
articles are irrelevant and do not support their as-
sociated statement). To measure citation precision
for a response r, we first ask annotators to judge
whether each citation ci,kcontributes full, partial,
or no support for its associated statement si(see
cited webpages in Figure 1 for examples):
•Full support : all of the information in the
statement is supported by the citation.
•Partial support : some of the information in
the statement is supported by the citation, but
other parts are not supported (e.g., missing or
contradictory).
•No support : the citation does not support any
part of the statement (e.g., the cited webpage
is completely irrelevant or contradictory).
For statements that have multiple associated ci-
tations, we additionally ask annotators whether the
union of its associated cited webpages collectively
provides full support for the statement (a binary
judgment). Similar to citation recall, we use the
AIS evaluation framework of Rashkin et al. (2022)
to collect these binary judgments.
To calculate citation precision, let Tfsbe the
number of citations that fully support its associ-
ated statement, and let Tpsbe the number of cita-
tions that partially supports its associated statement,
where the associated statement is fully supported
by the union of its associated citations and no as-
sociated citation fully supports the statement by
itself.4LetNbe the total number of citations
4For an intuitive example of when partially-supporting
cites count toward improving precision (greater Tps), consider
the statement “Health benefits of cycling include improved car-
diovascular health[1] and lowered cholesterol levels[2].” within the response. Then, the citation precision is
(Tfs+Tps)/N.
2.5 Citation F1
Citation F1is a metric that combines citation pre-
cision and citation recall by taking their harmonic
mean:
F1= 2·citation precision ·citation recall
citation precision +citation recall
To achieve a high citation F1, systems must have
high citation precision andhigh citation recall.
3 Evaluation Setup
In this section, we describe the evaluated generative
search engines (§3.1), the diverse query distribu-
tions we use for evaluation (§3.2), and the details
of our human evaluation protocol (§3.3).
3.1 Evaluated Generative Search Engines
We evaluate four existing commercial generative
search engines: Bing Chat, NeevaAI, perplexity.ai,
and YouChat.5These systems pattern after prior
work (e.g., Nakano et al., 2021; Menick et al., 2022;
Glaese et al., 2022; Thoppilan et al., 2022, inter
alia) and generate responses by conditioning large
language models on the input query and retrieved
content (e.g., search results from a conventional
search engine). For each input, we save the sys-
tem’s first complete response (i.e., single-turn). Re-
sponses were scraped between late February and
late March 2023.
Note that evaluated generative search engines
have differing abstention rates (Table 1), which can
make direct comparison difficult—one might ex-
pect that systems with higher abstention rates might
also have higher evaluation performance, since they
can simply abstain from generating responses to
difficult queries (we do not find this to be the case
in practice). NeevaAI abstains from responding on
nearly 23% of evaluated queries, since its response
its associated citations [1] and [2]. Suppose that these citations
each contribute partial support for the entire statement—the
first citation [1] only states that “Health benefits of cycling
include improved cardiovascular health”, and second citation
[2] only states that “Health benefits of cycling include lowered
cholesterol levels”. Taken together, the citations offer full sup-
port for the statement. Although these citations do not fully
support the statement on their own, they still meaningfully
contribute to its verifiability—systems should not be penalized
for aggregating information from multiple citations.
5We do not evaluate OpenAI’s ChatGPT or Google’s Bard
because they do not provide in-line citations for their responses
(as of March 2023), and thus trivially have low verifiability.Abstention Rate ( ↓)
Bing Chat < 0.5%
NeevaAI 22.7%
perplexity.ai < 0.5%
YouChat < 0.5%
Table 1: Generative search engines may be designed
for deployment in different contexts. NeevaAI abstains
from responding to 22.7% of our 1450 queries, since its
response is designed for display within a conventional
search results page. In contrast, the conversational in-
terface of Bing Chat, and YouChat means that systems
must generate a response for nearly every input user
query (excepting, e.g., query character length limits).
is displayed within a conventional search engine
results page. In contrast, Bing Chat, perplexity.ai,
and YouChat respond to almost every user query.
3.2 Evaluated Query Distributions
To gain a broader understanding of the strengths
and weaknesses of existing commercial generative
search engines, we evaluate on a diverse set of
queries from a variety of sources (e.g., Google
user queries, open-ended Reddit questions, how-to
queries) requiring knowledge from several different
answer types (e.g., short textual spans, long-form
paragraph, lists, or tables). See Appendix A for
example queries from each distribution. Each sys-
tem is evaluated on 1450 queries—150 randomly-
sampled queries from each of AllSouls, davinci-
debate, ELI5 (KILT / Live), and WikiHowKey-
words, and 100 randomly-sampled queries for each
of the seven NaturalQuestions subdistributions.
AllSouls. We evaluate systems on open-ended
essay questions taken from the entrance exam (gen-
eral paper component) for All Souls College, Ox-
ford University. These questions cover topics in-
cluding the arts, science, politics, literature, current
events, and issues in education and sport.
davinci-debate. We evaluate systems on debate
topics generated from text-davinci-003 . To gen-
erate debate queries, we follow the procedure of
Bakker et al. (2022); see Appendix B.1 for details.
ELI5. We take queries from the “Explain Like
I’m Five” (ELI5) subreddit, where users provide
long-form layperson-accessible answers to submit-
ted questions. Submitted questions are required to
admit objective explanations, and answering them
often requires long-form textual responses.
We consider two subdistributions of ELI5
queries: ELI5 (KILT) and ELI5 (Live). ELI5(KILT) uses historical queries from the KILT ELI5
dataset (Fan et al., 2019; Petroni et al., 2021),
drawn from posts created before July 2018. A
retrieval-based system could hypothetically per-
form well on ELI5 (KILT) by simply identifying
the query’s source Reddit ELI5 post and copying
its content. As a result, we also evaluate generative
search engines on the ELI5 (Live) subdistribution,
which increases ecological validity by evaluating
systems on real user queries at their time of creation
and reducing the incidence of search results with
the query’s exact keywords.6We continuously
listen to the stream of new Reddit ELI5 posts and
immediately query generative search engines for
responses whenever a new post is created. This en-
sures that the source ELI5 post will not have been
indexed (and thus, cannot be retrieved) by conven-
tional search engines. minimizing the possibility
that the generative search engine has access to the
source ELI5 post.
WikiHowKeywords. We evaluate systems on
queries derived from WikiHow articles. We found
that directly querying generative search engines
with WikiHow article titles yields responses that
largely paraphrase or copy text directly from Wik-
iHow. As a result, we use text-davinci-003 to
paraphrase article titles (e.g., “How to Cut An Avo-
cado”) into keyword queries (e.g., “cut avocado”).
NaturalQuestions. We evaluate generative
search engines on NaturalQuestions (Kwiatkowski
et al., 2019) queries, stratified by their answer
type. NaturalQuestions contains historical queries
issued to the Google search engine coupled with
long and short answers extracted from Wikipedia.
We evaluate on queries from 7 NaturalQuestions
subdistributions: queries with paragraph-type long
answers (i) with and (ii) without short answers,
queries with list-type long answers (iii) with and
(iv) without short answer, queries with table-type
long answers (v) with and (vi) without short
answers, and finally (vii) queries with no long
answer (and thus no short answer either).
Summary. In total, we evaluate existing gener-
ative search engines on 12 total query distribu-
tions. Eight query distributions are taken from prior
work (ELI5 (KILT) and the seven NaturalQues-
tions query distributions), while four query distri-
butions were constructed for this work: AllSouls,
6The goal of the ELI5 (Live) subdistribution is not to iden-
tify queries with no relevant webpages on the Internet. We
recognize that many user queries express previously-seen in-
formation needs, even if the precise wording differs.davinci-debate, ELI5 (Live), and WikiHowKey-
words. These diverse settings provide broad cover-
age of several potential use cases and information
needs, helping us gain a comprehensive understand-
ing of systems’ strengths and weaknesses.
3.3 Human Evaluation Protocol
Annotation process. Evaluating a single query-
response pair requires human annotators to com-
plete a three-step The first step measures the re-
sponse’s fluency and perceived utility (§2.2), and
the second and third step provide the judgments
necessary to measure citation recall (§2.3) and pre-
cision (§2.4). See Appendix C for screenshots of
the annotation interface and Appendix D for the
annotation guidelines.
Annotator recruitment and training. Annota-
tion was performed on Amazon Mechanical Turk.
Annotators were pre-screened with a qualification
study, which required them to read an annota-
tion guidelines document and evaluate five repre-
sentative query-response pairs. We individually
reviewed submitted annotations for qualification
study and provided annotators with personalized
feedback to help correct any misconceptions or con-
fusion about the task. Annotators who performed
well on the qualification study and demonstrated
thorough understanding of the task and annotation
guidelines were permitted to participate in the main
round of human evaluation. We remained in con-
stant contact with annotators throughout the hu-
man evaluation process to answer questions about
corner-cases and clarify intended behavior. In total,
34 annotators participated in human evaluation.
Annotator compensation. Annotators were
compensated $1.00 per query-response pair for
responses with citations, and $0.38 per query-
response pair for responses without citations
($15.00 per hour, by conservative time estimates).
On average, annotators took approximately four
minutes to complete all three steps for a single
query-response pair for responses that contained at
least one citation.
Annotation agreement. Each query-response
pair is annotated once in the human evaluation
process. To measure inter-annotator agreement,
we collected three annotations for 250 randomly-
sampled query-response pairs, finding high agree-
ment rates (greater than 82.0% pairwise agreement
and 91.0 F1 for all judgments; see Appendix E).4 Results and Analysis
This section presents the results of our human eval-
uation study and discusses our main observations
and analyses. We see that fluency and perceived
utility are generally high across different generative
search engines (§4.1), while citation recall and pre-
cision are quite low (§4.2), though performance cer-
tainly varies by system and query distribution—the
low citation recall and precision, when combined
with the facade of trustworthiness from fluency and
high perceived utility, increase the potential for ex-
isting generative search engines to mislead users.
Our results also show that citation precision is in-
versely correlated with perceived utility in existing
generative search engines (§4.3). We hypothesize
that this is a byproduct of systems’ propensity to
copy or closely paraphrase text from cited web-
pages, which may increase citation precision and
decrease perceived utility (§4.4).
4.1 Fluency and Perceived Utility
See Appendix F for full fluency and perceived util-
ity results for every generative search engine on
each of our query distributions.
Generated responses are fluent and appear help-
ful. Averaging across all systems and responses
yields an average rating of 4.48 for fluency and
4.50 for perceived utility, indicating that annota-
tors generally found generated responses fluent and
helpful for answering the user’s input query.
Comparing fluency and perceived utility be-
tween generative search engines. Comparing
fluency and perceived utility ratings between the
generative search engines (aggregated over all re-
sponses), we see that Bing Chat receives the lowest
fluency / perceived utility ratings (4.40 / 4.34), fol-
lowed by NeevaAI (4.43 / 4.48), perplexity.ai (4.51
/ 4.56), and YouChat (4.59 / 4.62).
Comparing fluency across query distributions.
Comparing average fluency ratings across different
query distributions, we see similar ratings between
NaturalQuestions queries that have a long answer
(i.e., an extractive answer of some length exists on
Wikipedia) and non-NaturalQuestions distributions
(4.50 vs. 4.47, respectively). Comparing average
fluency ratings between NaturalQuestions subdistri-
butions, we see that generated responses to queries
that have a short extractive answer are generally
more fluent (4.55) than responses to queries with
only a long answer (4.46) or those without a longanswer (4.46), perhaps because responses to ques-
tions with short answers are generally shorter and
often only require factoid knowledge.
A notable outlier distribution is NaturalQues-
tions queries with table-type long answers and no
short answers, where system responses are dra-
matically less fluent (average of 4.36 across sys-
tems vs. average of 4.48 across all query distribu-
tions). These challenging queries often require ag-
gregating information across table cells or retrieved
sources, since the lack of a short answer implies
that no single Wikipedia table cell directly answers
the question (e.g., the query “how many grammys
does beyonce have without destiny’s child”). When
the retrieved webpages do not contain a clear ex-
tractive answer to the query, but contain facts that
seem relevant (e.g., information about Destiny’s
Child’s first Grammy, or Beyonce’s total number
of career Grammy awards), the generated response
is often a stilted agglomeration of statements from
various sources, reducing overall fluency.
Comparing perceived utility across query distri-
butions. In contrast to fluency, perceived utility
can differ substantially between different query dis-
tributions. Perceived utility is much higher for
NaturalQuestions queries containing a long answer
(4.59), as opposed to non-NaturalQuestions queries
(4.43). Comparing between different NaturalQues-
tions subdistributions, we see that perceived util-
ity is highest for queries that have a short answer
(4.62), followed by queries that have only a long an-
swer (4.55), and finally by queries that have no long
(or short) answer (4.52). Overall, perceived utility
decreases as queries require longer-form and less-
extractive answers (e.g., factoid NaturalQuestions
queries with short answers versus ELI5 queries).
4.2 Citation Recall and Precision
See Appendix G for full citation recall and preci-
sion results for every generative search engine on
each of our query distributions.
Existing generative search engines often do not
cite comprehensively or correctly. When aver-
aging across all systems, a mere 51.5% of gener-
ated statements are fully supported with citations
(recall), and only 74.5% of citations fully support
their associated statements (precision). We believe
these results are unacceptably low for systems that
are quickly becoming a popular tool for answer-
ing user queries and already have millions of users,
especially given that generated responses often ap-pear informative and useful.
Comparing citation recall and precision be-
tween generative search engines. Citation recall
and precision varies dramatically between different
generative search engines. perplexity.ai achieves
the highest average recall (68.7), compared to Nee-
vaAI (67.6), Bing Chat (58.7), and YouChat (11.1).
On the other hand, Bing Chat achieves the highest
average precision (89.5), followed by perplexity.ai
(72.7), NeevaAI (72.0), and YouChat (63.6). A gap
of nearly 58% separates the system with the high-
est and lowest recall (perplexity.ai vs. YouChat),
and the gap between the systems with the highest
and lowest precision is almost 25% (Bing Chat vs.
YouChat).
Comparing citation recall across query distri-
butions. Modifying the evaluation query distri-
bution appears to affect citation recall more than
citation precision. For example, the gap in cita-
tion recall between NaturalQuestions queries with
a long answer and non-NaturalQuestions queries
is nearly 11% (58.5 vs. 47.8, respectively). Simi-
larly, the difference in citation recall between Nat-
uralQuestions queries with and without short an-
swers is nearly 10% (63.4 for queries with a short
answer, 53.6 for queries with only a long answer,
and 53.4 for queries with no long or short answer).
We hypothesize that citation recall is driven by
the relevance of retrieved webpages. In the ab-
sence of retrieved evidence that directly answers
the input user query, systems generate statements
that are unsubstantiated by citations, resulting in
lower recall. For example, generative search en-
gines struggle with citation recall when evaluated
on the open-ended AllSouls essay questions (aver-
age recall of 44.3), because these queries generally
have no extractive answer on the Internet.
Comparing citation precision across query
distributions. Precision on NaturalQuestions
queries with long answers is higher than non-
NaturalQuestions distributions (76.1 vs. 72.3, re-
spectively). Precision is highest on NaturalQues-
tions queries with paragraph answer types (preci-
sion of 81.5 when a short answer exists and 78.7
when only a long answer exists). On the other hand,
citation precision is lowest when systems are evalu-
ated on AllSouls open-ended essay questions (67.8)
and davinci-debate queries (70.3). Comparing be-
tween NaturalQuestions subdistributions, average
system precision is higher on queries with short
answers (77.4) than those with only long answers20 40 60
Citation F14.44.54.6Perceived Utility
Bing ChatNeevaAIperplexity.aiYouChatFigure 3: Averaged perceived utility plotted against av-
eraged citation F1for each evaluated generative search
engine. Different systems make different trade-offs be-
tween perceived utility and citation F1. Note that these
systems are difficult to directly compare since they may
have different abstention rates (Table 1).
(74.8) or no long answer (73.5).
Summary. To summarize our human evaluation
results, Figure 3 plots average perceived utility
against average citation F1. Existing systems make
different trade-offs between citation recall, citation
precision, and perceived utility. See Appendix H
for full citation F1results for every generative
search engine on each of our query distributions.
4.3 Citation Precision is Inversely Related to
Perceived Utility
We find that citation precision is inversely corre-
lated with perceived utility in existing generative
search engines ( r=−0.96). For example, Bing
Chat achieves the highest precision, but has the low-
est perceived utility. In contrast, YouChat has the
lowest citation precision, but its responses attain
the highest perceived utility ratings.
This inverse relationship between citation pre-
cision and perceived utility is symptomatic of a
trade-off between faithfulness and abstractiveness
(Ladhak et al., 2022). In particular, we find that
system-generated statements often closely para-
phrase or directly copy from their associated ci-
tations (see §4.4 for further analysis). This results
in high citation precision (since extractively copied
text is almost always fully supported by the source
citation), but lower perceived utility (since the ex-
tractive snippets may not actually answer the user’s
input query). In contrast, systems that frequently
deviate from cited content (resulting in low citation
precision) may have greater freedom to generate
fluent responses that appear relevant and helpful
to the user’s input query.
There are different opinions on this topic.  Some researchers  
believe that cooperation, not struggle for survival, drives  
evolution  [1]. Others think that there are two driving forces  
of evolution: mutation (change) and competition  [2]. What 
do you think? Is cooperation or competition the driving force guiding 
the evolution of society? 
     phys.org 
(     citation fully supports its associated statement) 
Cooperation, not struggle for survival, drives evolution 
Researchers believe that competition tends to differentiate 
ecological requirements ... A new conceptual evolutionary 
model ... reviewed the debated mechanism of speciation, 
suggesting that competition and a struggle for existence are not 
the main drivers of evolution. 
     cooperation.cool 
(     citation fully supports its associated statement) 
Game Theory - Cooperation is cool 
Lots of biologists consider evolution as a continuous struggle 
for survival. They think that there are two driving forces of the 
evolution: mutation (change), and competition .Cited Webpages 
(*Some generated statements may not be fully supported  by 
citations, while others are fully supported. )
Bing Chat 
(higher citation precision, lower perceived utility) 
[2]: 
[1]: Figure 4: Citation precision is inversely correlated with
perceived utility in existing generative search engines.
Bing Chat often achieves high citation precision because
it closely paraphrases from cited webpages (bolded).
However, since these citations are largely irrelevant to
the user’s input query (biological evolution vs. societal
evolution), copying this contents results in lower per-
ceived utility.
This tradeoff is especially apparent on the All-
Souls query distribution, which contains open-
ended essay questions. AllSouls queries often can-
not be answered via extraction from a single web-
page on the Internet. For example, given the query
“Is cooperation or competition the driving force
guiding the evolution of society?”, conventional
search engine results focus on biological evolution,
rather than societal evolution. Bing Chat simply
copies irrelevant statements directly from the cited
sources, resulting in high citation precision but low
perceived utility (Figure 4).
4.4 Generative Search Engines Closely
Paraphrase From Cited Webpages
To better understand how generative search engines
use citations to support their responses, we analyze
the similarity between generated statements and
their supporting cited webpages. For citations that
provide full or partial support for their associated
statement, annotators were asked to provide evi-
dence by copy-pasting the minimal set of sentences
from the cited webpage that support their judg-
ment (if any such sentences exist). We compute
the BLEU (Papineni et al., 2002) and BERTScoreBLEU BERTScore (F1)
Bing Chat 44.1 78.8
NeevaAI 30.0 72.9
perplexity.ai 22.3 69.2
YouChat 28.6 72.0
Average 31.3 73.2
Table 2: Existing generative search engines closely para-
phrase from cited articles; generated statements have
high similarity with their cited webpages.
(Zhang et al., 2020) between each generated state-
ment and the annotator-provided evidence from the
associated citation. For statements with multiple as-
sociated citations, we take the maximum similarity
with any associated citation’s evidence.
Table 2 presents similarity metrics between gen-
erated statements and extracted evidence from sup-
porting webpages—when statements are fully or
partially supported by their citations, they often
copy or closely paraphrase from their cited arti-
cles. Furthermore, systems with higher similarity
between their generated statements and cited web-
pages also have higher average citation precision
(r= 0.80 between each of BLEU and BERTScore
with average citation precision), indicating that
their improved precision may largely be a byprod-
uct of their increased tendency to copy or para-
phrase from cited webpages.
5 Related Work
Existing work has proposed a variety of techniques
for building language models that provide refer-
ences to support generated text. Nakano et al.
(2021) use reinforcement learning from human
preferences to train language models to answer
questions and provide supporting evidence. Simi-
larly, Menick et al. (2022) also use reinforcement
learning from human preferences to train language
models to answer user questions, but their system
generates responses by conditioning on evidence
retrieved from a Google search for the given user
query. Finally, the LaMDA system of Thoppilan
et al. (2022) is trained to provide URLs that sup-
port its generated statements. In contrast to the
aforementioned line of work on training systems
to generate citations, Gao et al. (2022) propose a
method for post-editing generated output to reflect
and cite retrieved evidence.
Existing work has also proposed evaluation pro-
tocols and benchmarks for improving verifiabil-
ity in language generation systems. Rashkin et al.(2022) propose the attributed to identified sources
(AIS) evaluation framework to assess whether a par-
ticular statement is supported by provided evidence
and validate their guidelines on conversational
question answering, summarization, and table-to-
text systems. Bohnet et al. (2023) introduce the
task of attributed question answering, where sys-
tems are given an input question and must output
an answer string with a pointer to evidence text sup-
porting the answer, and propose a reproducible eval-
uation setup with NaturalQuestions queries (only
paragraph answer type containing long and short
answers) with Wikipedia as the evidence corpus.
In contemporaneous work, Peskoff and Stewart
(2023) have domain experts evaluate ChatGPT and
YouChat responses to 100 expert-written questions.
They find that generated responses are coherent and
concise, but frequently undersourced and inaccu-
rate; our results also show that YouChat responses
frequently lack citations for generated statements
(i.e., low citation recall).
6 Conclusion
In this work, we used human evaluation to audit the
verifiability of four popular commercial generative
search engines—Bing Chat, NeevaAI, perplexity.ai,
and YouChat. We find that responses from existing
generative search engines are generally fluent and
often appear informative, but frequently contain
unsupported statements and inaccurate citations
(low citation recall and precision)—a mere 51.5%
of generated statements are fully supported by cita-
tions (recall), and only 74.5% of citations support
their associated statements (precision). We believe
that existing systems’ citation recall and precision
are unacceptably low, given that they are quickly
becoming a popular tool for answering user queries
and already have millions of users. Moreover, we
find that citation precision is inversely correlated
with perceived utility in existing generative search
engines—the responses that seem more helpful are
often those with more unsupported statements or
inaccurate citations. Analysis suggests that this
inverse correlation occurs in existing systems be-
cause of their propensity to copy or closely para-
phrase from cited webpages, which inflates citation
precision at the cost of lower perceived utility. We
hope our results and insights further motivate the
development of trustworthy generative search en-
gines and help researchers and users better under-
stand their current shortcomings.Acknowledgements
We are grateful to the 34 annotators who partici-
pated in our human evaluation study—this work
would not have been possible without them. We
also thank Rishi Bommasani, Ge Gao, Natasha
Klein-Atlas, Vivian Lai, Kevin Lin, John Thickstun,
Eric Wallace, and Gerben Wierda for feedback and
discussions that helped improve this work. We
thank Amazon Web Services for providing Ama-
zon Mechanical Turk credits that helped support
this work. This work was supported in part by the
AI2050 program at Schmidt Futures (Grant G-22-
63429).
Limitations
The primary goal of this work was to assess verifia-
bility in generative search engine responses. How-
ever, note that verifiability is not factuality —rather
than arbitrating if a generated statement is true (dif-
ficult for all but the simplest claims; Rashkin et al.,
2022), verifiability enables users to easily check
any generated statement’s source, allowing them to
draw their own conclusions about whether to trust
the generated statement. Studying the factuality of
generative search engines (that may or may not pro-
vide citations) is an important direction for future
work—users may not necessarily bother to check
the sources, especially given that responses often
seem helpful and sound confident, and we’d thus
like responses to be as factual as possible.
In our evaluation of verifiability, we consider
sentence-level claims. However, sentences often
have multiple claims (e.g., “Cats[1] and dogs[2]
are common pets.”). However, there is currently
no clear linguistic definition on what constitutes a
claim. As a result, we use sentences for simplicity
and reproducibility. Proposing a concrete definition
of a “claim” and performing a finer-grained evalua-
tion is an interesting direction for future work.
References
Michiel Bakker, Martin Chadwick, Hannah Sheahan,
Michael Tessler, Lucy Campbell-Gillingham, Jan
Balaguer, Nat McAleese, Amelia Glaese, John
Aslanides, Matt Botvinick, and Christopher Sum-
merfield. 2022. Fine-tuning language models to find
agreement among humans with diverse preferences.
InProc. of NeurIPS .
Abeba Birhane, Vinay Uday Prabhu, and John Whaley.
2022. Auditing saliency cropping algorithms. In
Proc. of WACV .Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aha-
roni, Daniel Andor, Livio Baldini Soares, Massimil-
iano Ciaramita, Jacob Eisenstein, Kuzman Ganchev,
Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma,
Jianmo Ni, Lierni Sestorain Saralegui, Tal Schus-
ter, William W. Cohen, Michael Collins, Dipanjan
Das, Donald Metzler, Slav Petrov, and Kellie Webster.
2023. Attributed question answering: Evaluation
and modeling for attributed large language models.
ArXiv:2212.08037.
Joy Buolamwini and Timnit Gebru. 2018. Gender
shades: Intersectional accuracy disparities in com-
mercial gender classification. In Proc. of FAccT .
Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris
Callison-Burch, and Dan Roth. 2019. Seeing things
from a different angle: Discovering diverse perspec-
tives about claims. In Proc. of NAACL .
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proc. of ACL .
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-
cent Y . Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,
and Kelvin Guu. 2022. RARR: Researching and
revising what language models say, using language
models. ArXiv:2210.08726.
Amelia Glaese, Nat McAleese, Maja Tr˛ ebacz, John
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker,
Lucy Campbell-Gillingham, Jonathan Uesato, Po-
Sen Huang, Ramona Comanescu, Fan Yang, Abigail
See, Sumanth Dathathri, Rory Greig, Charlie Chen,
Doug Fritz, Jaume Sanchez Elias, Richard Green,
Soˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel
Foley, Susannah Young, Iason Gabriel, William Isaac,
John Mellor, Demis Hassabis, Koray Kavukcuoglu,
Lisa Anne Hendricks, and Geoffrey Irving. 2022.
Improving alignment of dialogue agents via targeted
human judgements. ArXiv:2209.14375.
Ben Green and Yiling Chen. 2019. Disparate interac-
tions: An algorithm-in-the-loop analysis of fairness
in risk assessments. In Proc. of FAccT .
Svetlana Kiritchenko and Saif Mohammad. 2018. Ex-
amining gender and race bias in two hundred senti-
ment analysis systems. In Proc. of *SEM .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: A benchmark for question answering
research. Transactions of the Association for Compu-
tational Linguistics , 7:452–466.
Faisal Ladhak, Esin Durmus, He He, Claire Cardie, and
Kathleen McKeown. 2022. Faithful or extractive? onmitigating the faithfulness-abstractiveness trade-off
in abstractive summarization. In Proc. of ACL .
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factual-
ity in abstractive summarization. In Proc. of ACL .
Yusuf Mehdi. 2023. The new Bing and Edge – progress
from our first month | Bing search blog. Accessed on
March 28, 2023.
Jacob Menick, Maja Trebacz, Vladimir Mikulik,
John Aslanides, Francis Song, Martin Chadwick,
Mia Glaese, Susannah Young, Lucy Campbell-
Gillingham, Geoffrey Irving, and Nat McAleese.
2022. Teaching language models to support answers
with verified quotes. ArXiv:2203.11147.
Danaë Metaxa, Joon Sung Park, James A. Landay, and
Jeff Hancock. 2019. Search media and elections: A
longitudinal investigation of political search results.
InProc. of CSCW .
Panagiotis Takis Metaxas and Yada Pruksachatkun.
2017. Manipulation of search engine results during
the 2016 us congressional elections.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff
Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William
Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,
Gretchen Krueger, Kevin Button, Matthew Knight,
Benjamin Chess, and John Schulman. 2021. We-
bGPT: Browser-assisted question-answering with hu-
man feedback. ArXiv:2112.09332.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL .
Denis Peskoff and Brandon Stewart. 2023. Credible
without credit: Domain experts assess generative
language models. In Proc. of ACL .
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
Vassilis Plachouras, Tim Rocktäschel, and Sebastian
Riedel. 2021. KILT: a benchmark for knowledge
intensive language tasks. In Proc. of NAACL .
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,
Lora Aroyo, Michael Collins, Dipanjan Das, Slav
Petrov, Gaurav Singh Tomar, Iulia Turc, and David
Reitter. 2022. Measuring attribution in natural lan-
guage generation models. ArXiv:2112.12870.
Ronald E. Robertson, David Lazer, and Christo Wilson.
2018. Auditing the personalization and composition
of politically-related search engine results pages. In
Proc. of WWW .
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,Amin Ghafouri, Marcelo Menegali, Yanping Huang,
Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,
Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-
Ching Chang, Igor Krivokon, Will Rusch, Marc
Pickett, Pranesh Srinivasan, Laichee Man, Kathleen
Meier-Hellstern, Meredith Ringel Morris, Tulsee
Doshi, Renelito Delos Santos, Toju Duke, Johnny So-
raker, Ben Zevenbergen, Vinodkumar Prabhakaran,
Mark Diaz, Ben Hutchinson, Kristen Olson, Ale-
jandra Molina, Erin Hoffman-John, Josh Lee, Lora
Aroyo, Ravi Rajakumar, Alena Butryna, Matthew
Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-
hen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-
Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc
Le. 2022. LaMDA: Language models for dialog ap-
plications. ArXiv:2201.08239.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. BERTScore:
Evaluating text generation with BERT. In Proc. of
ICLR .A Example queries from each evaluated query distribution
Source Example Queries
AllSoulsWhat are the functions of fashion?
Should wealth be inheritable?
davinci-debateShould private companies be allowed to manage public utilities?
Should controversial opinions be censored on social media?
ELI5 (KILT)Why is a circle 360 degrees and not 100 degrees?
Why can animals drink dirty water safely but humans can’t?
ELI5 (Live)Why jumping into water from great height feels like landing in concrete?
where does the deleted data go
WikiHowKeywordsage paper using tea
ways to stop stressing over exam results
NaturalQuestions
(paragraph long answer, has short answer)who wrote the song god your mama and me
what is the queen of spain’s name
NaturalQuestions
(paragraph long answer, no short answer)where did knock on wood superstition come from
what is the use of tap and die
NaturalQuestions
(list long answer, has short answer)what is the most nominated film for the oscars
who played guitar on i want you she’s so heavy
NaturalQuestions
(list long answer, no short answer)alicia keys if i ain’t got you awards
is all of florida in the same time zone
NaturalQuestions
(table long answer, has short answer)how many episodes are there in quantum leap
what kind of music is red hot chili peppers
NaturalQuestions
(table long answer, no short answer)where does copa airlines fly in the united states
michael jordan career high against every nba team
NaturalQuestions
(no long or short answer)what does the x card mean in uno
what changes were made when trinidad and tobago gained independence
Table 3: Example queries from each of the evaluated query distributions. Queries come from diverse sources
and require knowledge from a variety of answer types (e.g., short text span, long-form paragraph, list, or table).
Each system is evaluated on 1450 queries—150 randomly-sampled queries from each of AllSouls, davinci-debate,
ELI5 (KILT), ELI5 (Live), and WikiHowKeywords, and 100 randomly-sampled queries for each of the seven
NaturalQuestions subdistributions.
B Query Distribution Details
B.1 davinci-debate
We seed the data generation process with 100 debate questions, which are manually transformed propo-
sitions propositions taken from the Perspectrum dataset of Chen et al. (2019) (e.g., the proposition
“Vaccination must be made compulsory.” could be rewritten as the question “Should vaccines be manda-
tory?”).
To generate a debate question, we prompt text-davinci-003 with 10 randomly-sampled seed ques-
tions. We repeat this procedure until we have generated 150 unique debate questions that also do not
appear in our seed set. Finally, generated questions were manually filtered for inappropriate content.
B.2 ELI5
To transform ELI5 post titles into queries, we remove ELI5-specific prefixes (e.g., the post title “ELI5:
why can’t our brains recall every memory?” becomes the query “Why can’t our brains recall every
memory?”).
B.3 WikiHowKeywords
To paraphrase article titles into keyword queries, we prompt text-davinci-003 with “Given a question,
write a concise Google search query that would answer the question” and two in-context examples.C Annotation Interface
Figures 5-7 show the annotation interface used for human evaluation.
In the first step, annotators were shown the query and the generated response (without citations) and
asked to rate response fluency and perceived utility on a five-point Likert scale.
In the second step, annotators were shown the statements in the generated response (including any
generated citations) and asked to filter out statements are not verification-worthy.
Finally, in the third step, annotators were shown the statements that were previously judged to require
verification (in the prior step), as well as each statement’s associated system-generated citations. For
each statement and associated citation, annotators judged whether the citation fully supports, partially
supports, or does not support the statement, as interpreted within the broader context of the query and
system response. For statements with multiple associated citations, annotators are asked to judge whether
the citations, when taken together, fully support the statement; this captures cases where multiple citations
support disjoint parts of a statement (e.g., “Health benefits of cycling include improved cardiovascular
health[1] and lowered cholesterol levels[2].”).
Figure 5: First step of the annotation interface, where annotators judge response fluency and perceived utility.Figure 6: Second step of the annotation interface, where annotators uncheck statements that are not verification-
worthy. Statements that contain generated citations must be verification-worthy, so we automatically mark them as
such in the interface (greyed-out checkboxes next to the 2nd and 4th sentences above).
Figure 7: Third step of the annotation interface, where annotators provide judgments on whether each citation
supports its associated statement, and whether each statement is supported by the union of its citations (only
applicable when a statement has multiple associated citations).D Annotation Guidelines
Figures 8-12 show the annotation guidelines we used for the task. We ask crowd annotators to read these
guidelines as part of the qualification study. Only annotators that demonstrated a thorough understanding
of the guidelines and task were permitted to participate in the main round of human evaluation.
O v e r v i e w
H i !
W e
a r e
a
t e a m
o f
S t a n f o r d
r e s e a r c h e r s
i n t e r e s t e d
i n
e v a l u a t i n g
t h e
t r u s t w o r t h i n e s s
o f
A I
s y s t e m s .
I n
t h i s
t a s k ,
y o u
w i l l
e v a l u a t e
a n
A I
s y s t e m ' s
r e s p o n s e
t o
a
u s e r
q u e r y .
T h e
A I
s y s t e m
o u t p u t s
a
p a r a g r a p h
t h a t
c o n t a i n s
i n f o r m a t i o n
r e l e v a n t
t o
t h e
u s e r ' s
q u e r y ,
a n d
w e
w o u l d
l i k e
t o
e v a l u a t e
w h e t h e r
t h e
A I
s y s t e m
c a n
a c c u r a t e l y
c i t e
s o u r c e s
f o r
s t a t e m e n t s
i t
m a k e s
a b o u t
t h e
e x t e r n a l
w o r l d .
A t
a
h i g h
l e v e l ,
t h i s
t a s k
b r e a k s
d o w n
i n t o
t h r e e
s t e p s :
1 .
E v a l u a t i n g
r e s p o n s e
q u a l i t y
2 .
F i l t e r i n g
s e n t e n c e s
t h a t
d o
n o t
r e q u i r e
c i t a t i o n .
3 .
J u d g i n g
w h e t h e r
e a c h
s t a t e m e n t
i s
f u l l y
s u p p o r t e d
b y
i t s
c i t a t i o n ( s ) .
P l e a s e
c a r e f u l l y
r e a d
t h e
g u i d e l i n e s
b e l o w
b e f o r e
s t a r t i n g
o n
t h e
t a s k .
T h e
t a s k
c o m p e n s a t i o n
a c c o u n t s
f o r
t h e
t i m e
n e e d e d
t o
r e a d
t h e
g u i d e l i n e s .
P r e l i m i n a r i e s :
L o g g i n g
I n
W h e n
f i r s t
e n t e r i n g
t h e
s i t e ,
y o u
w i l l
b e
p r o m p t e d
t o
s e l e c t
a
u s e r n a m e .
P l e a s e
u s e
y o u r
w o r k e r
I D
a s
t h e
u s e r n a m e ,
s o
w e
c a n
k e e p
t r a c k
o f
t h e
e x a m p l e s
y o u ' v e
a n n o t a t e d .
T h e
t o p
o f
t h e
i n t e r f a c e
d i s p l a y s
y o u r
w o r k e r
I D ,
t h e
t o t a l
n u m b e r
o f
e x a m p l e s
s u b m i t t e d
f r o m
t h i s
u s e r n a m e ,
a n d
w i l l
s h o w
a
c o m p l e t i o n
c o d e
w h e n
y o u
h a v e
f i n i s h e d
t h e
t a s k .
I f
s o m e t h i n g
i s
w r o n g
w i t h
t h e
e x a m p l e ,
y o u
m a y
p r e s s
t h e
" F l a g
E x a m p l e "
b u t t o n
i n
t h e
t o p - r i g h t
c o r n e r
t o
r e p o r t
t h e
e r r o r .
P l e a s e
d o
n o t
s u b m i t
a n n o t a t i o n s
f o r
s u c h
e x a m p l e s .
Y o u r
t a s k
e n d s
a f t e r
y o u ' v e
c o m p l e t e d
5
r e s p o n s e s .
A
c o m p l e t i o n
c o d e
w i l l
a p p e a r
a t
t h e
t o p
o f
t h e
i n t e r f a c e - - - t h e r e
i s
n o
n e e d
t o
c o m p l e t e
m o r e
t h a n
5
r e s p o n s e s
t o
r e c e i v e
c r e d i t
f o r
t h e
s t u d y .
S t e p
1 :
E v a l u a t i n g
r e s p o n s e
q u a l i t y
Y o u
w i l l
b e
s h o w n
t h e
u s e r ' s
o r i g i n a l
q u e r y ,
a n d
t h e
s y s t e m ' s
r e s p o n s e
t o
t h e
q u e r y - - - p l e a s e
c a r e f u l l y
r e a d
b o t h
o f
t h e m .
T h e n ,
y o u
w i l l
b e
a s k e d
t o
r a t e
y o u r
l e v e l
o f
a g r e e m e n t
w i t h
t w o
q u e s t i o n s :
1 .
T h e
r e s p o n s e
i s
f l u e n t
a n d
c o h e s i v e .
2 .
T h e
r e s p o n s e
i s
a
h e l p f u l
a n d
i n f o r m a t i v e
a n s w e r
t o
t h e
q u e r y .
Figure 8: First page of the annotation guidelines.O n c e
y o u
h a v e
f i n i s h e d
s e l e c t i n g
a
r e s p o n s e
f o r
e a c h
o f
t h e
t w o
q u e s t i o n s ,
p r e s s
t h e
" N e x t
S t e p "
b u t t o n
i n
t h e
t o p - r i g h t
c o r n e r
t o
c o n t i n u e .
S t e p
2 :
F i l t e r i n g
s e n t e n c e s
t h a t
d o
n o t
r e q u i r e
c i t a t i o n .
T h e
g o a l
o f
t h i s
s t e p
i s
t o
f i l t e r
t h e
s e n t e n c e s
i n
t h e
s y s t e m
r e s p o n s e
b y
r e m o v i n g
s e n t e n c e s
t h a t
d o
n o t
r e q u i r e
c i t a t i o n
( u n c h e c k i n g
t h e m
i n
t h e
i n t e r f a c e ) .
W e
e x p e c t
t h e
m a j o r i t y
o f
s e n t e n c e s
p r o d u c e d
b y
t h e
s y s t e m
t o
r e q u i r e
c i t a t i o n ,
s o
d o n ' t
w o r r y
i f
y o u
f i n d
y o u r s e l f
r a r e l y
u n c h e c k i n g
s e n t e n c e s .
I n
g e n e r a l ,
w e
t a k e
t h e
p o s i t i o n
t h a t
a l l
s t a t e m e n t s
a b o u t
t h e
e x t e r n a l
w o r l d
r e q u i r e
c i t a t i o n
,
e v e n
i f
t h e y
a r e
t r i v i a l l y
t r u e
o r
" c o m m o n
s e n s e "
( s i n c e
u s e r s
m a y
d i f f e r
i n
t h e i r
b a c k g r o u n d ,
w h i c h
a f f e c t s
t h e i r
b a s i c
b e l i e f s ) .
F o r
e x a m p l e ,
t h e
f o l l o w i n g
s e n t e n c e s
r e q u i r e
c i t a t i o n :
-
( 1 a ) :
T h e
H o u s e
o f
L o r d s
i s
a
t o p i c
o f
o n g o i n g
d e b a t e
i n
t h e
U K .
-
( 1 b ) :
H o w e v e r ,
t h e r e
i s
s t i l l
n o
c o n s e n s u s
o n
w h a t
s h o u l d
r e p l a c e
t h e
E l e c t o r a l
C o l l e g e .
-
( 1 c ) :
T h e
s k y
i s
b l u e .
-
( 1 d ) :
T h e
m o o n
l a n d i n g
w a s
s t a g e d .
-
( 1 e ) :
I n
F e b r u a r y
2 0 2 3 ,
L e B r o n
J a m e s
t o o k
2 6 1 , 9 6 0
t o t a l
b r e a t h s .
-
( 1 f ) :
P a t r i c k
H e n r y
o n c e
s a i d
" G i v e
m e
l i b e r t y ,
o r
g i v e
m e
d e a t h " .
-
( 1 g ) :
T h a n k s g i v i n g
d i n n e r s
u s u a l l y
t a s t e
b a d .
-
( 1 h ) :
V o t i n g
r i g h t s
a r e
c o n t r o v e r s i a l
I n
p a r t i c u l a r ,
n o t e
t h a t
s e n t e n c e s
c a n
r e q u i r e
c i t a t i o n
d e s p i t e
b e i n g
n e a r l y
i m p o s s i b l e
t o
v e r i f y .
C o n s i d e r
e x a m p l e
( e )
a b o v e .
I t ' s
h i g h l y
u n l i k e l y
t h a t
a n y o n e
k n o w s
e x a c t l y
h o w
m a n y
b r e a t h s
L e B r o n
J a m e s
t o o k
i n
F e b r u a r y
2 0 2 3 ,
l e t
a l o n e
t h a t
s u c h
i n f o r m a t i o n
c o u l d
b e
l i n k e d
t o
i n
a
c i t a t i o n .
H o w e v e r ,
i t ' s
s t i l l
a
s t a t e m e n t
a b o u t
t h e
e x t e r n a l
w o r l d ,
a n d
i t ' s
s t i l l
p o s s i b l e
t o
f i n d
o u t
f o r
c e r t a i n
w h e t h e r
t h e
s t a t e m e n t
i s
t r u e
o r
f a l s e .
T h u s ,
t h e
s t a t e m e n t
r e q u i r e s
c i t a t i o n .
I n
c o n t r a s t ,
c o n s i d e r
t h e
f o l l o w i n g
e x a m p l e s
o f
s e n t e n c e s
t h a t
d o
n o t
r e q u i r e
c i t a t i o n :
-
( 2 a ) :
I
b e l i e v e
t h a t
t h e
m o o n
l a n d i n g
w a s
s t a g e d .
-
E x p l a n a t i o n
:
I n
g e n e r a l ,
a l l
s e n t e n c e s
p e r t a i n i n g
t o
" I "
d o
n o t
r e q u i r e
c i t a t i o n .
T h i s
s t a t e m e n t
e x p r e s s e s
a
b e l i e f
h e l d
b y
t h e
s p e a k e r .
T h e
s p e a k e r
i s
u n k n o w n ,
s o
t h i s
s t a t e m e n t
d o e s
n o t
r e q u i r e
c i t a t i o n .
N o t e
t h a t
t h e
s i m i l a r - l o o k i n g
s t a t e m e n t
" T h e
m o o n
l a n d i n g
w a s
s t a g e d "
( e x a m p l e
1 d )
r e q u i r e
c i t a t i o n
a n d
i s
v e r i f i a b l e .
-
( 2 b ) :
H a v e
y o u
l i s t e n e d
t o
t h a t
s o n g ?
-
E x p l a n a t i o n
:
Q u e s t i o n s
d o
n o t
h a v e
i n f o r m a t i o n
t o
v e r i f y .
-
( 2 c ) :
P i c k
u p
t h e
b a l l
o n
t h e
f l o o r .
-
E x p l a n a t i o n
:
C o m m a n d s
d o
n o t
h a v e
i n f o r m a t i o n
t o
v e r i f y .
-
( 2 d ) :
I t
i s
t h e
y e a r
2 3 0 0 .
R o b o t s
r u l e
t h e
e a r t h .Figure 9: Second page of the annotation guidelines.-
E x p l a n a t i o n
:
t h e
s e n t e n c e
" R o b o t s
r u l e
t h e
e a r t h . "
d o e s
n o t
r e q u i r e
c i t a t i o n ,
s i n c e
t h e
c o n t e x t
( " I t
i s
t h e
y e a r
2 3 0 0 " )
s p e c i f i e s
t h a t
t h i s
i s
a
h y p o t h e t i c a l
s i t u a t i o n
a n d
n o t
a
s t a t e m e n t
a b o u t
t h e
e x t e r n a l
w o r l d .
C a r e f u l l y
r e a d
e a c h
s e n t e n c e
a g a i n
a n d
d e c i d e
w h e t h e r
i t
r e q u i r e s
c i t a t i o n .
I f
i t
d o e s
n o t
r e q u i r e
c i t a t i o n ,
u n c h e c k
i t s
c o r r e s p o n d i n g
c h e c k b o x .
W h e n
y o u
h a v e
f i n i s h e d ,
p r e s s
t h e
" N e x t
S t e p "
b u t t o n
i n
t h e
t o p - r i g h t
c o r n e r
t o
p r o c e e d .
S t e p
3 :
J u d g i n g
w h e t h e r
e a c h
s t a t e m e n t
i s
f u l l y
s u p p o r t e d
b y
i t s
c i t a t i o n ( s ) .
I n
t h i s
s t e p ,
y o u
w i l l
e v a l u a t e
w h e t h e r
e a c h
s t a t e m e n t
i s
s u p p o r t e d
b y
i t s
c o r r e s p o n d i n g
c i t a t i o n s .
N o t e
t h a t
t h e
s y s t e m
r e s p o n s e s
m a y
a p p e a r
v e r y
f l u e n t
a n d
w e l l - f o r m e d ,
b u t
c o n t a i n
s l i g h t
i n a c c u r a c i e s
t h a t
a r e
n o t
e a s y
t o
d i s c e r n
a t
f i r s t
g l a n c e .
P a y
c l o s e
a t t e n t i o n
t o
t h e
t e x t .
R e a d
i t
c a r e f u l l y
a s
y o u
w o u l d
w h e n
p r o o f r e a d i n g .
C a r e f u l l y
r e a d
t h e
u s e r
q u e r y
a n d
t h e
s t a t e m e n t .
Y o u
m a y
a l s o
h a v e
t o
r e - r e a d
t h e
f u l l
s y s t e m
r e s p o n s e
t o
u n d e r s t a n d
t h e
s t a t e m e n t
i n
i t s
f u l l
c o n t e x t .
G i v e n
t h e
s t a t e m e n t ' s
a s s o c i a t e d
c i t a t i o n s ,
y o u r
t a s k
i s
t o
j u d g e
w h e t h e r
a l l
o f
t h e
i n f o r m a t i o n
p r o v i d e d
b y
t h e
s y s t e m
r e s p o n s e
i s
f u l l y
s u p p o r t e d
b y
t h e
s o u r c e
d o c u m e n t .
I n
p a r t i c u l a r ,
t h i s
q u e s t i o n
c a n
b e
a n s w e r e d
b y
c o n s i d e r i n g :
-
( A ) :
W h a t
i s
t h e
i n f o r m a t i o n
p r o v i d e d
b y
t h e
s t a t e m e n t ?
-
( B ) :
A c c o r d i n g
t o
t h e
c i t a t i o n ( s ) ,
i s
t h i s
s t a t e m e n t
t r u e ?
( A ) :
W h a t
i s
t h e
i n f o r m a t i o n
p r o v i d e d
b y
t h e
s t a t e m e n t ?
T o
d e t e r m i n e
t h e
i n f o r m a t i o n
p r o v i d e d
b y
t h e
s t a t e m e n t ,
y o u
m u s t
c o n s i d e r
t h e
q u e r y ,
t h e
s t a t e m e n t ,
a n d
t h e
c o n t e x t
o f
t h e
s t a t e m e n t
w i t h i n
t h e
f u l l
r e s p o n s e .
T h e
c i t a t i o n s
s h o u l d
b e
c o m p l e t e l y
i g n o r e d
w h e n
d e t e r m i n i n g
" t h e
i n f o r m a t i o n
p r o v i d e d
b y
t h e
s t a t e m e n t . "
C o n s i d e r
t h e
f o l l o w i n g
e x a m p l e :
Q u e r y :
W h y
d o
s o
m a n y
p e o p l e
w a n t
t o
g e t
m a r r i e d ?
R e s p o n s e
(
s t a t e m e n t
h i g h l i g h t e d ) :
P e o p l e
g e t
m a r r i e d
f o r
m a n y
r e a s o n s ,
i n c l u d i n g
l o v e ,
c o m p a n i o n s h i p ,
f i n a n c i a l
s e c u r i t y ,
a n d
t o
s h a r e
t h e i r
l i v e s
w i t h
a
p a r t n e r .
M a r r i a g e
c a n
a l s o
b e
s e e n
a s
a
w a y
t o
a f f i r m
m u t u a l
l o v e
o r
s t a r t
a
f a m i l y .
I n
t h i s
c a s e ,
t h e
s t a t e m e n t
i s
s t a n d - a l o n e ,
a n d
c a n
b e
i n t e r p r e t e d
w i t h o u t
l o o k i n g
a t
t h e
q u e r y
o r
t h e
r e s t
o f
t h e
r e s p o n s e .
H o w e v e r ,
t h i s
i s
n o t
a l w a y s
t h e
c a s e .
C o n s i d e r
t h e
e x a m p l e
b e l o w :
Q u e r y
:
I s
i t
w r o n g
t o
e x a g g e r a t e
i n
a
l e t t e r
o f
r e c o m m e n d a t i o n ?Figure 10: Third page of the annotation guidelines.R e s p o n s e
(
s t a t e m e n t
h i g h l i g h t e d ) :
Y e s ,
i t
i s
w r o n g .
L e t t e r s
o f
r e c o m m e n d a t i o n
s h o u l d
r e f l e c t
t h e
a u t h o r ' s
h o n e s t
p e r s p e c t i v e
o n
t h e
c a n d i d a t e .
T h e
r e s p o n s e
" Y e s ,
i t
i s
w r o n g "
i s
u n i n t e r p r e t a b l e
o n
i t s
o w n ,
b e c a u s e
i t
i s
n o t
c l e a r
w h a t
" i t "
r e f e r s
t o .
H o w e v e r ,
b y
u s i n g
t h e
c o n t e x t
o f
t h e
q u e r y ,
i t
b e c o m e s
c l e a r
t h a t
t h e
s t a t e m e n t
i s
e q u i v a l e n t
t o
" Y e s ,
[ e x a g g e r a t i n g
i n
a
l e t t e r
o f
r e c o m m e n d a t i o n ]
i s
w r o n g " .
F o r
a n o t h e r
e x a m p l e ,
c o n s i d e r :
Q u e r y :
h o w
m a n y
c h a r a c t e r s
a r e
i n
t h e
p r o l o g u e
o f
c a n t e r b u r y
t a l e s
R e s p o n s e
(
s t a t e m e n t
h i g h l i g h t e d ) :
I n
G e o f f r e y
C h a u c e r ' s
T h e
C a n t e r b u r y
T a l e s ,
3 2
c h a r a c t e r s
m a k e
t h e
j o u r n e y
t o
C a n t e r b u r y .
T h i s
i n c l u d e s
t h e
n a r r a t o r ,
t h e
h o s t ,
a n d
t h e
C a n o n ' s
y e o m a n ,
w h o
j o i n
t h e
g r o u p
l a t e r .
T h e
s t a t e m e n t
" T h i s
i n c l u d e s
t h e
n a r r a t o r ,
t h e
h o s t ,
a n d
t h e
C a n o n ' s
y e o m a n ,
w h o
j o i n
t h e
g r o u p
l a t e r . "
i s
u n i n t e r p r e t a b l e
o n
i t s
o w n ,
b e c a u s e
i t
i s
n o t
c l e a r
w h a t
" T h i s "
r e f e r s
t o ,
o r
w h a t
" g r o u p "
t h e y
j o i n .
T h e
p r e c e d i n g
s e n t e n c e
o f
t h e
r e s p o n s e
i s
e s s e n t i a l
f o r
r e a l i z i n g
t h a t
t h i s
s e n t e n c e
i s
e q u i v a l e n t
t o
" [ T h e
3 2
c h a r a c t e r s
t h a t
m a k e
t h e
j o u r n e y
t o
C a n t e r b u r y ]
i n c l u d e
t h e
n a r r a t o r ,
t h e
h o s t ,
a n d
t h e
C a n o n ' s
y e o m a n ,
w h o
j o i n
t h e
[ 3 2
c h a r a c t e r s ]
l a t e r " .
I n
g e n e r a l ,
u s e
y o u r
b e s t
j u d g m e n t
t o
d e t e r m i n e
t h e
i n f o r m a t i o n
p r o v i d e d
b y
t h e
s y s t e m
r e s p o n s e .
( B ) :
A c c o r d i n g
t o
t h e
c i t a t i o n ( s ) ,
i s
t h i s
s t a t e m e n t
t r u e ?
A g a i n ,
y o u
s h o u l d
u s e
y o u r
b e s t
j u d g m e n t
i n
d e t e r m i n i n g
w h e t h e r
a l l
o f
t h e
i n f o r m a t i o n
p r o v i d e d
b y
t h e
s t a t e m e n t
i s
s u p p o r t e d
b y
t h e
a s s o c i a t e d
c i t a t i o n ( s ) .
I t
m a y
b e
h e l p f u l
t o
a s k
y o u r s e l f
w h e t h e r
i t
i s
a c c u r a t e
t o
s a y
" a c c o r d i n g
t o
t h e
c i t a t i o n "
w i t h
a
s t a t e m e n t
f o l l o w i n g
t h i s
p h r a s e .
F o r
e x a m p l e ,
i s
i t
a c c u r a t e
t o
s a y
“ a c c o r d i n g
t o
t h e
c i t a t i o n ,
i n
G e o f f r e y
C h a u c e r ' s
T h e
C a n t e r b u r y
T a l e s ,
3 2
c h a r a c t e r s
m a k e
t h e
j o u r n e y
t o
C a n t e r b u r y " ?
B e
s u r e
t o
c h e c k
a l l
o f
t h e
i n f o r m a t i o n
i n
t h e
s t a t e m e n t .
Y o u
w i l l
b e
g i v e n
s i x
o p t i o n s :
-
"
F u l l
S u p p o r t "
:
A l l
o f
t h e
i n f o r m a t i o n
i n
t h e
s t a t e m e n t
i s
s u p p o r t e d
i n
t h e
d o c u m e n t .
-
"
P a r t i a l
S u p p o r t "
:
O n l y
s o m e
o f
t h e
i n f o r m a t i o n
i s
s u p p o r t e d
i n
t h e
d o c u m e n t ,
b u t
o t h e r
p a r t s
o f
t h e
i n f o r m a t i o n
a r e
m i s s i n g
f r o m
t h e
d o c u m e n t .
-
" N o
S u p p o r t "
:
T h i s
d o c u m e n t
d o e s
n o t
s u p p o r t
a n y
p a r t
o f
t h e
s t a t e m e n t .
-
" A r t i c l e
N o t
A c c e s s i b l e "
:
N o t
a b l e
t o
a c c e s s
t h e
d o c u m e n t
( e . g . ,
p a y w a l l
o r
t h e
l i n k
i s
d e a d )
-
"
C i t a t i o n
H a s
S u p p o r t
b u t
a l s o
R e f u t e s
S t a t e m e n t "
:
T h e
c i t a t i o n
h a s
i n f o r m a t i o n
t h a t
s u p p o r t s
t h e
s t a t e m e n t ,
b u t
a l s o
h a s
i n f o r m a t i o n
t h a t
r e f u t e s
t h e
s t a t e m e n t .
-
" S t a t e m e n t
i s
U n c l e a r ,
C a n ' t
M a k e
J u d g m e n t "
:
T h e
s t a t e m e n t
i s
s o
i n c o m p r e h e n s i b l e
t h a t
i t
c a n n o t
b e
d e t e r m i n e d
i f
t h e
c i t a t i o n
s u p p o r t s
t h e
s t a t e m e n t .Figure 11: Fourth page of the annotation guidelines.I f
t h e
c i t a t i o n
o f f e r s
" f u l l
s u p p o r t "
o r
" p a r t i a l
s u p p o r t "
o f
a
d o c u m e n t ,
y o u
w i l l
a l s o
b e
a s k e d
t o
c o p y
a n d
p a s t e
t h e
m i n i m a l
s e t
o f
s e n t e n c e s
f r o m
t h e
a r t i c l e
t h a t
s u p p o r t
y o u r
j u d g m e n t .
I n
c a s e s
w h e r e
y o u
c a n ' t
l o c a l i z e
t h e
j u d g m e n t
t o
p a r t i c u l a r
s e n t e n c e ( s )
( e . g . ,
t h e
e n t i r e
a r t i c l e
s u p p o r t s
t h e
s t a t e m e n t ,
o r
t h e
s u p p o r t
c o m e s
f r o m
a n
i m a g e
o r
g r a p h i c ) ,
f e e l
f r e e
t o
l e a v e
t h i s
i n p u t
b l a n k .
W h e n
a
s t a t e m e n t
h a s
m o r e
t h a n
o n e
a s s o c i a t e d
c i t a t i o n
,
y o u
w i l l
a l s o
j u d g e
w h e t h e r
t h e
c i t a t i o n s ,
w h e n
t a k e n
t o g e t h e r ,
f u l l y
s u p p o r t
t h e
s t a t e m e n t
(
Y e s
/
N o
) .
I n
o t h e r
w o r d s ,
i f
y o u
m e r g e d
a l l
o f
t h e s e
c i t a t i o n s
i n t o
o n e
b i g
w e b p a g e
( a n d
i t
b e c a m e
a
s i n g l e
c i t a t i o n ) ,
w o u l d
t h i s
c i t a t i o n
f u l l y
s u p p o r t
t h e
s t a t e m e n t ?
I f
t h e
c i t a t i o n s
c o n t r a d i c t
e a c h
o t h e r
( e . g . ,
o n e
f u l l y
s u p p o r t s
t h e
s t a t e m e n t ,
w h e r e a s
a n o t h e r
r e f u t e s
t h e
s t a t e m e n t ) ,
t h e n
s e l e c t
"
C i t a t i o n s
C o n t r a d i c t
E a c h
O t h e r
" .
Q u e s t i o n s
o r
f e e d b a c k ?
I f
y o u
h a v e
q u e s t i o n s
a b o u t
t h e
t a s k ,
o r
a n y
f e e d b a c k
a b o u t
h o w
w e
c o u l d
m a k e
i t
b e t t e r
o r
w h a t
y o u r
e x p e r i e n c e
w a s
l i k e
w i t h
i t ,
p l e a s e
e m a i l
n f l i u @ c s . s t a n f o r d . e d u
,
a n d
w e ' l l
g e t
b a c k
t o
y o u
p r o m p t l y .
T h a n k s !Figure 12: Fifth page of the annotation guidelines.E Annotation Quality
Table 4 presents inter-annotator agreement statistics, computed on a random sample of 250 query-response
pairs that received annotations each. We measure the pairwise agreement between individual pairs of
ratings and an F1 score comparing individual ratings to the majority consensus. We compute agreement on
judgments of (i) fluency and perceived utility, (ii) whether a statement is verification-worthy, (iii) whether
a citation supports its associated statement, and (iv) whether a statement is fully supported by the union
of its citations (in the case where multiple webpages are cited). When calculating agreement on fluency
and perceived utility judgments, we coarsen the 5-point Likert judgments into three options: “Disagree”,
“Neutral”, and “Agree”. Agreement rates between annotators are high (pairwise agreement greater than
82.0% and F1 greater than 91.0 for all judgments).
Inter-Annotator Agreement (↑)
Pairwise Agreement % F1
Fluency 88.5 94.1
Perceived Utility 86.4 93.1
Verifiability 94.6 97.3
Citation Supports 82.0 91.0
Statement Supported 82.2 91.1
Table 4: Inter-annotator agreement statistics. Pairwise Agreement % computes the proportion of individual
judgment pairs that agree, and F1 compares individual judgments to the majority consensus judgment. Inter-
annotator agreement is high (greater than 82.0% pairwise agreement % and 91.0 F1 for all judgments).F Fluency and Perceived Utility
Table 5 presents the fluency of generative search engine responses on each of our query distributions, and
Table 6 presents the perceived utility.
Fluency (↑)
Average Over
All Queries
Bing Chat 4.40
NeevaAI 4.43
perplexity.ai 4.51
YouChat 4.59
Average 4.48Fluency (↑)
AllSouls davinci-debateELI5
WikiHowKeywordsKILT Live
Bing Chat 4.31 4.37 4.36 4.30 4.41
NeevaAI 4.50 4.53 4.50 4.42 4.42
perplexity.ai 4.43 4.54 4.55 4.47 4.45
YouChat 4.58 4.65 4.56 4.53 4.52
Average 4.45 4.52 4.49 4.43 4.45
Fluency (↑)
NaturalQuestions
List Long Answer Table Long Answer Paragraph Long AnswerNo Answer
Has Short No Short Has Short No Short Has Short No Short
Bing Chat 4.49 4.52 4.46 4.30 4.54 4.41 4.39
NeevaAI 4.45 4.40 4.31 4.28 4.41 4.49 4.43
perplexity.ai 4.69 4.54 4.59 4.41 4.73 4.43 4.37
YouChat 4.65 4.56 4.60 4.45 4.66 4.69 4.64
Average 4.57 4.50 4.49 4.36 4.58 4.50 4.46
Table 5: Human evaluation results for generated response fluency (five-point Likert ratings). In general, existing
generative search engines produce fluent text. Performance is notably lower on NaturalQuestions queries with
table-type long answers and no short answers, which often require aggregating information within or across citations.Perceived Utility (↑)
Average Over
All Queries
Bing Chat 4.34
NeevaAI 4.48
perplexity.ai 4.56
YouChat 4.62
Average 4.50Perceived Utility (↑)
AllSouls davinci-debateELI5
WikiHowKeywordsKILT Live
Bing Chat 4.15 4.19 4.19 4.09 4.37
NeevaAI 4.44 4.39 4.54 4.46 4.42
perplexity.ai 4.39 4.60 4.54 4.50 4.51
YouChat 4.53 4.54 4.53 4.50 4.63
Average 4.38 4.43 4.45 4.39 4.48
Perceived Utility (↑)
NaturalQuestions
List Long Answer Table Long Answer Paragraph Long AnswerNo Answer
Has Short No Short Has Short No Short Has Short No Short
Bing Chat 4.63 4.49 4.49 4.47 4.53 4.40 4.38
NeevaAI 4.65 4.57 4.43 4.38 4.43 4.63 4.49
perplexity.ai 4.71 4.61 4.60 4.55 4.77 4.58 4.50
YouChat 4.72 4.64 4.70 4.54 4.77 4.77 4.70
Average 4.68 4.58 4.55 4.49 4.62 4.60 4.52
Table 6: Human evaluation results for perceived utility of generated responses (five-point Likert ratings). In general,
responses from existing generative search engines appear informative and useful.G Citation Recall and Precision
Table 7 presents generative search engine citation recall across the evaluated query distributions, and
Table 8 presents citation precision.
Citation Recall (%;↑)
Average Over
All Queries
Bing Chat 58.7
NeevaAI 67.6
perplexity.ai 68.7
YouChat 11.1
Average 51.5Citation Recall (%;↑)
AllSouls davinci-debateELI5
WikiHowKeywordsKILT Live
Bing Chat 55.6 57.1 59.8 59.9 50.7
NeevaAI 55.3 66.3 66.6 61.6 72.5
perplexity.ai 63.0 64.2 64.8 58.1 74.6
YouChat 3.2 3.9 3.0 4.6 12.1
Average 44.3 47.9 48.5 46.0 52.5
Citation Recall (%;↑)
NaturalQuestions
List Long Answer Table Long Answer Paragraph Long AnswerNo Answer
Has Short No Short Has Short No Short Has Short No Short
Bing Chat 74.1 60.6 63.5 49.2 72.1 66.3 61.9
NeevaAI 73.0 64.2 69.5 65.1 75.0 74.8 65.6
perplexity.ai 85.3 74.4 79.6 62.4 84.9 75.9 68.4
YouChat 21.6 16.6 30.6 11.5 31.6 21.8 17.8
Average 63.5 53.9 60.8 47.1 65.9 59.7 53.4
Table 7: Human evaluation results of citation recall in existing generative search engines. Citation recall is
concerningly low (many generated statements are not fully supported by citations), especially given that these
systems already have millions of users and may serve as a primary tool for fulfilling user information needs.Citation Precision (%;↑)
Average Over
All Queries
Bing Chat 89.5
NeevaAI 72.0
perplexity.ai 72.7
YouChat 63.6
Average 74.5Citation Precision (%;↑)
AllSouls davinci-debateELI5
WikiHowKeywordsKILT Live
Bing Chat 88.8 88.8 87.6 87.2 92.1
NeevaAI 69.8 74.1 75.7 73.8 74.0
perplexity.ai 61.7 68.4 64.9 66.3 77.4
YouChat 51.1 50.0 64.7 57.9 71.1
Average 67.8 70.3 73.2 71.3 78.7
Citation Precision (%;↑)
NaturalQuestions
List Long Answer Table Long Answer Paragraph Long AnswerNo Answer
Has Short No Short Has Short No Short Has Short No Short
Bing Chat 86.8 86.8 89.0 92.5 92.9 91.3 90.8
NeevaAI 73.2 67.6 67.1 64.2 73.4 76.5 70.8
perplexity.ai 82.1 81.0 76.0 71.7 83.8 79.7 74.0
YouChat 63.3 62.7 64.8 56.1 75.7 67.5 58.6
Average 76.4 74.5 74.2 71.1 81.5 78.7 73.5
Table 8: Human evaluation results of citation precision in existing generative search engines. Citation precision is
concerningly low (many generated citations do not support their associated statements), especially given that these
systems already have millions of users and may serve as a primary tool for fulfilling user information needs.H Citation F1
Table 9 presents the citation F1for every evaluated generative search engine on each query distribution.
Citation F1(↑)
Average Over
All Queries
Bing Chat 70.9
NeevaAI 69.8
perplexity.ai 70.6
YouChat 18.9
Average 57.6Citation F1(↑)
AllSouls davinci-debateELI5
WikiHowKeywordsKILT Live
Bing Chat 68.4 69.5 71.1 71.0 65.4
NeevaAI 61.7 70.0 70.8 67.1 73.2
perplexity.ai 62.3 66.2 64.8 62.0 76.0
YouChat 6.0 7.2 5.6 8.5 20.7
Average 49.6 53.2 53.1 52.2 58.8
Citation F1(↑)
NaturalQuestions
List Long Answer Table Long Answer Paragraph Long AnswerNo Answer
Has Short No Short Has Short No Short Has Short No Short
Bing Chat 79.9 71.4 74.1 64.2 81.2 76.8 73.6
NeevaAI 73.1 65.9 68.3 64.6 74.2 75.7 68.1
perplexity.ai 83.7 77.5 77.8 66.7 84.3 77.7 71.1
YouChat 32.2 26.2 41.5 19.2 44.6 32.9 27.4
Average 67.2 60.2 65.4 53.7 71.1 65.8 60.0
Table 9: Citation F1of generated responses.