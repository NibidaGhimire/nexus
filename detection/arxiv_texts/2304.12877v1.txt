Proximal Curriculum for Reinforcement Learning Agents
Georgios Tzannetos gtzannet@mpi-sws.org
Max Planck Institute for Software Systems
Bárbara Gomes Ribeiro bgomesr@mpi-sws.org
Max Planck Institute for Software Systems
Parameswaran Kamalaruban kparameswaran@turing.ac.uk
The Alan Turing Institute
Adish Singla adishs@mpi-sws.org
Max Planck Institute for Software Systems
Abstract
We consider the problem of curriculum design for reinforcement learning (RL) agents in
contextualmulti-tasksettings. Existingtechniquesonautomaticcurriculumdesigntypically
require domain-speciﬁc hyperparameter tuning or have limited theoretical underpinnings.
To tackle these limitations, we design our curriculum strategy, ProCuRL , inspired by
the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the
intuition that learning progress is maximized when picking tasks that are neither too hard
nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple
learning settings. We also present a practical variant of ProCuRL that can be directly
integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental
results on a variety of domains demonstrate the eﬀectiveness of our curriculum strategy over
state-of-the-art baselines in accelerating the training process of deep RL agents.
1 Introduction
Recent advances in deep reinforcement learning (RL) have demonstrated impressive performance in games,
continuous control, and robotics (Mnih et al., 2015; Lillicrap et al., 2015; Silver et al., 2017; Levine et al.,
2016). Despite these remarkable successes, a broader application of RL in real-world domains is often very
limited. For example, training RL agents in contextual multi-task settings and goal-based tasks with sparse
rewards still remains challenging (Hallak et al., 2015; Kirk et al., 2021; Andrychowicz et al., 2017; Florensa
et al., 2017; Riedmiller et al., 2018).
Inspired by the importance of curricula in pedagogical domains, there is a growing interest in leveraging
curriculum strategies when training machine learning models in challenging domains. In the supervised
learning setting, such as image classiﬁcation, the impact of the order of presented training examples has
been studied both theoretically and empirically (Weinshall et al., 2018; Weinshall & Amir, 2018; Zhou &
Bilmes, 2018; Zhou et al., 2021; Elman, 1993; Bengio et al., 2009; Zaremba & Sutskever, 2014). Recent works
have also studied curriculum strategies for learners in sequential-decision-making settings, such as imitation
learning (where the agent learns from demonstrations) and RL (where the agent learns from rewards). In
the imitation learning setting, recent works have proposed greedy curriculum strategies for picking the next
training demonstration according to the agent’s learning progress (Kamalaruban et al., 2019; Yengera et al.,
2021). In the RL setting, several curriculum strategies have been proposed to improve sample eﬃciency, e.g.,
by choosing an appropriate next starting state or goal state for the task to train on (Wöhlke et al., 2020;
Florensa et al., 2017; 2018; Racanière et al., 2020; Riedmiller et al., 2018; Klink et al., 2020a;b; Eimer et al.,
1arXiv:2304.12877v1  [cs.LG]  25 Apr 20232021). Despite extensive research on curriculum design for the RL setting, existing techniques typically have
limited theoretical underpinnings or require domain-speciﬁc hyperparameter tuning.
In this paper, we are interested in developing a principled curriculum strategy for the RL setting that is
broadly applicable to many domains with minimal tuning of hyperparameters. To this end, we rely on the
Zone of Proximal Development (ZPD) concept from the educational psychology literature (Vygotsky &
Cole, 1978; Chaiklin, 2003). The ZPD concept, when applied in terms of learning progress, suggests that
progress is maximized when the learner is presented with tasks that lie in the proximal zone , i.e., tasks that
are neither too hard nor too easy. This idea of proximal zone can be captured using a notion of probability
of success score PoS πt(s)w.r.t. the learner’s current policy πtfor any given task s. Building on this idea,
we mathematically derive an intuitive curriculum strategy by analyzing two simple learning settings. Our
main results and contributions are as follows:
I. We propose a curriculum strategy, ProCuRL , inspired by the ZPD concept. ProCuRL formalizes
the idea of picking tasks that are neither too hard nor too easy for the learner in the form of selection
strategy arg maxsPoSπt(s)·/parenleftbig
PoS∗(s)−PoSπt(s)/parenrightbig
, where PoS∗(s)corresponds to the probability of
success score w.r.t. an optimal policy (Section 3.1).
II. We derive ProCuRL under two speciﬁc learning settings where we analyze the eﬀect of picking a task
on the agent’s learning progress (Section 3.2).
III. We present a practical variant of ProCuRL , namely ProCuRL-val , that can be easily integrated
with deep RL frameworks with minimal hyperparameter tuning (Section 3.3).
IV. We empirically demonstrate the eﬀectiveness of ProCuRL-val over state-of-the-art baselines in
accelerating the training process of deep RL agents in a variety of environments (Section 4).1
1.1 Related Work
Curriculum strategies based on domain knowledge. Early works on curriculum design for the
supervised learning setting typically order the training examples in increasing diﬃculty (Elman, 1993;
Bengio et al., 2009; Schmidhuber, 2013; Zaremba & Sutskever, 2014). This easy-to-hard design principle
has been utilized in the hand-crafted curriculum approaches for the RL setting (Asada et al., 1996; Wu
& Tian, 2016). Moreover, there have been recent works on designing greedy curriculum strategies for the
imitation learning setting based on the iterative machine teaching framework (Liu et al., 2017; Yang et al.,
2018; Zhu et al., 2018; Kamalaruban et al., 2019; Yengera et al., 2021). However, these approaches require
domain-speciﬁc expert knowledge for designing diﬃculty measures.
Curriculum strategies based on ZPD concept. In the pedagogical setting, it has been realized that
eﬀective teaching provides tasks that are neither too hard nor too easy for the human learner. This intuition
of providing tasks from a particular range of diﬃculties is conceptualized in the ZPD concept (Vygotsky
& Cole, 1978; Chaiklin, 2003; Oudeyer et al., 2007; Baranes & Oudeyer, 2013; Zou et al., 2019). In
the RL setting, several curriculum strategies that have been proposed are inherently based on the ZPD
concept (Florensa et al., 2017; 2018; Wöhlke et al., 2020). A common underlying theme in both Florensa
et al. (2017) and Florensa et al. (2018) is that they choose the next task (starting or goal state) for the
learner uniformly at random from the set {s:rmin≤PoSπt(s)≤rmax}. Here, the threshold values rmin
andrmaxrequire tuning according to the learner’s progress and speciﬁc to the domain. Wöhlke et al.
(2020) propose a uniﬁed framework for the learner’s performance-based starting state curricula in RL. In
particular, the starting state selection policy of Wöhlke et al. (2020), P/bracketleftbig
s(0)
t=s/bracketrightbig
∝G(PoSπt(s))for some
functionG, accommodates existing curriculum generation methods like Florensa et al. (2017); Graves et al.
(2017). Despite promising empirical results, theoretical analysis of the impact of the chosen curriculum on
the RL agent’s learning progress is still missing in the aforementioned works.
Curriculum strategies based on self-paced learning (SPL). In the supervised learning setting, the
curriculum strategies using the SPL concept optimize the trade-oﬀ between exposing the learner to all
1Github repo: https://github.com/machine-teaching-group/tmlr2023_proximal-curriculum-rl .
2available training examples and selecting examples in which it currently performs well (Kumar et al., 2010;
Jiang et al., 2015). In SPDL(Klink et al., 2020b;a; 2021; 2022) and SPaCE (Eimer et al., 2021), the
authors have adapted the concept of SPL to the RL setting by controlling the intermediate task distribution
with respect to the learner’s current training progress. However, SPDLandSPaCE diﬀer in their mode
of operation and their objective. SPDLconsiders the procedural task generation framework where tasks
of appropriate diﬃcult levels can be synthesized, as also considered in Florensa et al. (2017; 2018). In
contrast, SPaCEconsiders a pool-based curriculum framework for picking suitable tasks, as popular in the
supervised learning setting. Further, SPDLconsiders the objective of a targeted performance w.r.t. a target
distribution (e.g., concentrated distribution on hard tasks); in contrast, SPaCE considers the objective
of uniform performance across a given pool of tasks. Similar to SPaCE, in our work, we consider the
pool-based setting with uniform performance objective. Both SPDLandSPaCE serve as state-of-the-art
baselines in our experimental evaluation. In terms of curriculum strategy, SPDL operates by solving an
optimization problem at each step to pick a task (Klink et al., 2021); SPaCE uses a ranking induced by the
magnitude of diﬀerences in current/previous critic values at each step to pick a task (Eimer et al., 2021). In
the appendix, we have also provided some additional information on hyperparameters for SPDL and SPaCE.
Other automatic curriculum strategies. There are other approaches for automatic curriculum gen-
eration, including: (i) by formulating the curriculum design problem with the use of a meta-level Markov
Decision Process (Narvekar et al., 2017; Narvekar & Stone, 2019); (ii) by learning how to generate training
tasks similar to a teacher (Dendorfer et al., 2020; Such et al., 2020; Matiisen et al., 2019; Turchetta et al.,
2020); (iii) by leveraging self-play as a form of curriculum generation (Sukhbaatar et al., 2018); (iv) by using
the disagreement between diﬀerent agents trained on the same tasks (Zhang et al., 2020); (v) by picking
the starting states based on a single demonstration (Salimans & Chen, 2018; Resnick et al., 2018); and
(vi) by providing agents with environment variations that are at the frontier of an agent’s capabilities, e.g.,
Unsupervised Environment Design methods (Dennis et al., 2020; Jiang et al., 2021a; Parker-Holder et al.,
2022). We refer the reader to recent surveys on curriculum design for the RL setting (Narvekar et al., 2020;
Portelas et al., 2021; Weng, 2020).
2 Formal Setup
In this section, we formalize our problem setting based on prior work on teacher-student curriculum learn-
ing (Matiisen et al., 2019).
MDP environment. We consider a learning environment deﬁned as a Markov Decision Process (MDP)
M:= (S,A,T,H,R,Sinit). Here,SandAdenote the state and action spaces, T:S×S×A→ [0,1]is the
transition dynamics, His the maximum length of the episode, and R:S×A→ Ris the reward function.
The set of initial states Sinit⊆Sspeciﬁes a ﬁxed pool of tasks, i.e., each starting state s∈Sinitcorresponds
to a unique task. Note that the above environment formalism is quite general enough to cover many practical
settings, including the contextual multi-task MDP setting (Hallak et al., 2015).2
RL agent and training process. We consider an RL agent acting in this environment via a policy
π:S×A → [0,1]that is a mapping from a state to a probability distribution over actions.3Given
a task with the corresponding starting state s∈ S init, the agent attempts the task via a trajectory
rollout obtained by executing its policy πfromsin the MDPM. The trajectory rollout is denoted
asξ={(s(τ),a(τ),R(s(τ),a(τ)))}τ=0,1,...,hwiths(0)=sand for some h≤H. The agent’s performance
on tasksis measured via the value function Vπ(s) :=E/bracketleftbig/summationtexth
τ=0R(s(τ),a(τ))/vextendsingle/vextendsingleπ,M,s(0)=s/bracketrightbig
. Then, the
uniform performance of the agent over the pool of tasks Sinitis given by Vπ:=Es∼Uniform (Sinit)[Vπ(s)].
The training process of the agent involves an interaction between two components: a student component
that is responsible for policy update and a teacher component that is responsible for task selection. The
interaction happens in discrete steps, indexed by t= 1,2,..., and is formally described in Algorithm 1.
2In this setting, for a given set of contexts C, the pool of tasks is given by {Mc= (S,A,Tc,H,Rc,Sinit) :c∈C}. Our
environment formalism (MDP M) covers this setting as follows: S=S×C;Sinit=Sinit×C;T((¯s/prime,c)|(¯s,c),a) =Tc(¯s/prime|¯s,a)
andR((¯s,c),a) =Rc(¯s,a),∀¯s,¯s/prime∈S,a∈A,c∈C.
3For general ﬁnite-horizon MDPs, including the time step as part of the state is important. However, to avoid complicating
the notation with additional indexing, we have assumed that the time step is implicitly included in the state.
3Letπenddenote the agent’s ﬁnal policy at the end of training. The training objective is to ensure that the
uniform performance of the policy πendis/epsilon1-near-optimal, i.e., (maxπVπ−Vπend)≤/epsilon1. In the following two
paragraphs, we discuss the student and teacher components in detail.
Student component. We consider a parametric representation for the RL agent, whose current knowledge
is parameterized by θ∈Θ⊆Rdand each parameter θis mapped to a policy πθ:S×A→ [0,1]. At stept, the
student component updates the knowledge parameter based on the following quantities: the current knowl-
edgeparameter θt,thetaskpickedbytheteachercomponent,andtherollout ξt={(s(τ)
t,a(τ)
t,R(s(τ)
t,a(τ)
t))}τ.
Then, the updated knowledge parameter θt+1is mapped to the agent’s policy given by πt+1:=πθt+1.
As a concrete example, the knowledge parameter of the Reinforce agent (Sutton et al., 1999) is up-
dated asθt+1←θt+ηt·/summationtexth−1
τ=0G(τ)
t·g(τ)
t, whereηtis the learning rate, G(τ)
t=/summationtexth
τ/prime=τR(s(τ/prime)
t,a(τ/prime)
t), and
g(τ)
t=/bracketleftbig
∇θlogπθ(a(τ)
t|s(τ)
t)/bracketrightbig
θ=θt.
Teacher component. At stept, the teacher component picks a task with the corresponding starting state
s(0)
tfor the student component to attempt via a trajectory rollout (see line 3 in Algorithm 1). The sequence
of tasks (curriculum) picked by the teacher component aﬀects the performance improvement of the policy
πt. The main focus of this work is to develop a teacher component to achieve the training objective in both
a computational and a sample-eﬃcient manner.
Algorithm 1 RL Agent Training as Interaction between Teacher-Student Components
1:Input:RL agent’s initial policy π1
2:fort= 1,2,...do
3:Teacher component picks a task with the corresponding starting state s(0)
t.
4:Student component attempts the task via a trajectory rollout ξtusing the policy πtfroms(0)
t.
5:Student component updates the policy to πt+1.
6:Output: RL agent’s ﬁnal policy πend←πt+1.
3 Proximal Curriculum Strategy
In Section 3.1, we propose a curriculum strategy for the goal-based setting. In Section 3.2, we show that
the proposed curriculum strategy can be mathematically derived by analyzing simple learning settings. In
Section 3.3, we present our ﬁnal curriculum strategy that is applicable in general settings.
3.1 Curriculum Strategy for the Goal-based Setting
Here, we introduce our curriculum strategy for the goal-based setting using the notion of probability of
success scores.
Goal-based setting. In this setting, the reward function Ris goal-based, i.e., the agent gets a reward of 1
only at the goal states and 0at other states; moreover, any action from a goal state also leads to termination.
For any task with the corresponding starting state s∈Sinit, we say that the attempted rollout ξsucceeds
in the task if the ﬁnal state of ξis a goal state. Formally, succ (ξ;s)is an indicator function whose value is
1when the rollout ξsucceeds in task s, and 0otherwise. Furthermore, for an agent with policy π, we have
thatVπ(s) :=E/bracketleftbig
succ(ξ;s)/vextendsingle/vextendsingleπ,M/bracketrightbig
is equal to the total probability of reaching a goal state by executing the
policyπstarting from s∈Sinit.
Probability of success. We begin by assigning a probability of success score for any task with the
corresponding starting state s∈Sinitw.r.t. any parameterized policy πθin the MDPM.
Deﬁnition 1. For any given knowledge parameter θ∈Θand any starting state s∈Sinit, we deﬁne the
probability of success score PoSθ(s)as the probability of successfully solving the task sby executing the policy
πθin the MDPM. For the goal-based setting, we have PoSθ(s) =Vπθ(s).
With the above deﬁnition, the probability of success score for any task s∈Sinitw.r.t. the agent’s current
policyπtis given by PoS t(s) :=PoSθt(s). Further, we deﬁne PoS∗(s) := maxθ∈ΘPoSθ(s).
4Curriculum strategy. Based on the notion of probability of success scores that we deﬁned above, we
propose the following curriculum strategy:
s(0)
t←arg max
s∈Sinit/parenleftBig
PoSt(s)·/parenleftbig
PoS∗(s)−PoSt(s)/parenrightbig/parenrightBig
, (1)
i.e., at step t, the teacher component picks a task associated with the starting state s(0)
taccording to Eq. 1.
The term PoS t(s)·(PoS∗(s)−PoSt(s))can be interpreted as the geometric mean of two quantities: the
learner’s probability of solving the task and the expected regret of the learner on this task. In the following
subsection, we show that the above curriculum strategy can be derived by considering simple learning
settings, such as contextual bandit problems with Reinforce agent; these derivations provide insights
about the design of the curriculum strategy.
3.2 Theoretical Justiﬁcations for the Curriculum Strategy
To derive our curriculum strategy for the goal-based setting, we additionally consider independent tasks
where any task s(0)
tpicked from the pool Sinitat steptonly aﬀects the agent’s knowledge component
corresponding to that task. Further, we assume that there exists a knowledge parameter θ∗∈Θsuch that
πθ∗∈arg maxπVπ, andπθ∗is referred to as the target policy. Then, based on the work of Weinshall et al.
(2018); Kamalaruban et al. (2019); Yengera et al. (2021), we investigate the eﬀect of picking a task s(0)
tat
stepton the convergence of the agent’s parameter θttowards the target parameter θ∗. Under a smoothness
condition on the value function of the form |Vπθ−Vπθ/prime|≤L·/bardblθ−θ/prime/bardbl1,∀θ,θ/prime∈Θfor someL>0, we can
translate the parameter convergence ( θt→θ∗) into the performance convergence ( Vπθt→Vπθ∗). Thus, we
deﬁne the improvement in the training objective at step tas
∆t(θt+1/vextendsingle/vextendsingleθt,s(0)
t,ξt) := [/bardblθ∗−θt/bardbl1−/bardblθ∗−θt+1/bardbl1]. (2)
In the above objective, we use the /lscript1-norm because our theoretical analysis considers the independent task
setting mentioned above. Further, we deﬁne the expected improvement in the training objective at step t
due to picking the task s(0)
tas follows:
Ct(s(0)
t) := Eξt|s(0)
t/bracketleftbig
∆t(θt+1|θt,s(0)
t,ξt)/bracketrightbig
. (3)
Note that the above quantity is an approximation of the expected learning progress measure as deﬁned
in Graves et al. (2017). In the following subsection, we justify our proposed curriculum strategy by analyzing
the above quantity for a speciﬁc agent model under the independent task setting. More concretely, for the
speciﬁc setting considered in Section 3.2.1, Theorem 1 implies that picking tasks based on the curriculum
strategygiveninEq.1maximizestheexpectedvalueoftheobjectiveinEq.2. Intheappendix, weprovidean
additional justiﬁcation by considering an abstract agent model with a direct performance parameterization.
3.2.1 Reinforce Agent with Softmax Policy Parameterization
We consider the Reinforce agent model with the following softmax policy parameterization: for any
θ∈R|S|·|A|, we parameterize the policy as πθ(a|s)∝exp(θ[s,a]),∀s∈S,a∈A. In the following, we
consider a problem instance involving a pool of contextual bandit tasks (a special case of independent task
setting). Consider an MDP Mwithg∈Sas the goal state for all tasks, Sinit=S\{g},A={a1,a2}, and
H= 1. We deﬁne the reward function as follows: R(s,a) = 0,∀s∈S\{g},a∈AandR(g,a) = 1,∀a∈A.
For a given probability mapping prand:S→ [0,1], we deﬁne the transition dynamics as follows: T(g|s,a1) =
prand(s),∀s∈S;T(s|s,a1) = 1−prand(s),∀s∈S; andT(s|s,a2) = 1,∀s∈S. Then, for the Reinforce
agent under the above setting, the following theorem quantiﬁes the expected improvement in the training
objective at step t:
Theorem 1. Consider the Reinforce agent with softmax policy parameterization under the independent
task setting as described above. Let s(0)
tbe the task picked at step twithPoSθt(s(0)
t) =pandPoSθ∗(s(0)
t) =p∗.
Then, we have: Ct(s(0)
t) = 2·ηt·p·/parenleftBig
1−p
p∗/parenrightBig
, whereηtis the learning of the Reinforce agent.
5For the above setting with prand(s) = 1,∀s∈S,maxs∈SinitCt(s)is equivalent to maxs∈SinitPoSt(s)·(1−
PoSt(s)). This means that for the case of PoS∗(s) = 1,∀s∈Sinit, the curriculum strategy given in Eq. 1 can
be seen as greedily optimizing the expected improvement in the training objective at step tgiven in Eq. 3.
3.3 Curriculum Strategy for General Settings
Next, we discuss various practical issues in directly applying the curriculum strategy in Eq. 1 for general
settings, and introduce several design choices to address these issues.
Softmax selection. When training deep RL agents, it is typically useful to allow some stochasticity in
the selected batch of tasks. Moreover, the arg max selection in Eq. 1 is brittle in the presence of any
approximation errors in computing PoS (·)values. To tackle this issue, we replace arg max selection in Eq. 1
with softmax selection and sample according to the following distribution:
P/bracketleftbig
s(0)
t=s/bracketrightbig
∝exp/parenleftBig
β·PoSt(s)·/parenleftbig
PoS∗(s)−PoSt(s)/parenrightbig/parenrightBig
, (4)
whereβis a hyperparameter. Here, PoS t(s)values are computed for each s∈Sinitusing rollouts obtained
via executing the policy πtinM; PoS∗(s)values are assumed to be provided as input.
PoS∗(·)is not known. Since the target policy πθ∗is unknown, it is not possible to compute the PoS∗(s)
values without additional domain knowledge. In our experiments, we resort to simply setting PoS∗(s) =
1,∀s∈Sinitin Eq. 4 – the rationale behind this choice is that we expect the ideal πθ∗to succeed in all the
tasks in the pool.4This brings us to the following curriculum strategy referred to as ProCuRL-env in our
experimental evaluation:
P/bracketleftbig
s(0)
t=s/bracketrightbig
∝exp/parenleftBig
β·PoSt(s)·/parenleftbig
1−PoSt(s)/parenrightbig/parenrightBig
. (5)
Computing PoSt(·)is expensive. It is expensive (sample ineﬃcient) to estimate PoS t(s)over the space
Sinitusing rollouts of the policy πt. To tackle this issue, we replace PoS t(s)with values Vt(s)obtained
from the critic network of the RL agent. This brings us to the following curriculum strategy referred to as
ProCuRL-val in our experimental evaluation:
P/bracketleftbig
s(0)
t=s/bracketrightbig
∝exp/parenleftBig
β·Vt(s)·/parenleftbig
1−Vt(s)/parenrightbig/parenrightBig
. (6)
Extension to non-binary or dense reward settings. The current forms of ProCuRL-val in Eq. 6 and
ProCuRL-env in Eq. 5 are not directly applicable for settings where the reward is non-binary or dense.
To deal with this issue in ProCuRL-val , we replace Vt(s)values from the critic in Eq. 6 with normalized
values given by Vt(s) =Vt(s)−Vmin
Vmax−Vminclipped to the range [0,1]. Here,VminandVmaxcould be provided
as input based on the environment’s reward function; alternatively we can dynamically set VminandVmax
during the training process by taking min-max values of the critic for states Sinitat stept. To deal with
this issue in ProCuRL-env , we replace PoS t(s)values from the rollouts in Eq. 5 with normalized values
Vt(s)as above. Algorithm 2 in the appendix provides a complete pseudo-code for the RL agent training
withProCuRL-val in this general setting.
4 Experimental Evaluation
In this section, we evaluate the eﬀectiveness of our curriculum strategies on a variety of domains w.r.t. the
uniform performance of the trained RL agent over the training pool of tasks. Additionally, we consider
the following two metrics in our evaluation: (i) total number of environment steps incurred jointly by the
teacher and the student components at the end of the training process; (ii) total clock time required for the
training process. Throughout all the experiments, we use the PPO method from Stable-Baselines3 library
for policy optimization (Schulman et al., 2017; Raﬃn et al., 2021).
4This simple choice leads to competitive performance in a variety of environments used in our experiments. However, the
above choice could lead to a suboptimal strategy for speciﬁc scenarios, e.g., when all PoS∗(s)are below 0.5. It would be
interesting to investigate alternative strategies to estimate PoS∗(s)during the training process, e.g., using top K%rollouts
obtained by executing the current policy πtstarting from s.
6Environment RewardContext StateActionPool size
PointMass-s binary R3R4R2100
PointMass-d non-binary R3R4R2100
BasicKarel binary 24000{0,1}886 24000
BallCatching non-binary R3R21R5100
AntGoal non-binary R2R29R850
(a) Complexity of the environments
 (b) Illustration of the environments
Figure 1: (a)shows complexity of the environments w.r.t. the reward signals, context variation, state space,
action space, and the pool size of the tasks used for training. (b)shows illustration of the environments
(from left to right): PointMass ,BasicKarel ,BallCatching , and AntGoal . Details are provided in
Section 4.1.
4.1 Environments
We consider 5diﬀerent environments in our evaluation, as described in the following paragraphs. Figure 1
provides a summary and illustration of these environments.
PointMass-s and PointMass-d. Based on the work of Klink et al. (2020b), we consider a contextual
PointMass environment where an agent navigates a point mass through a gate of a given size towards a
goal in a two-dimensional space. More concretely, we consider two settings: (i) PointMass-s environment
corresponds to a goal-based (i.e., binary and sparse) reward setting where the agent receives a reward of 1
only if it successfully moves the point mass to the goal position; (ii) PointMass-d environment corresponds
to a dense reward setting as used by Klink et al. (2020b) where the reward values decay in a squared
exponential manner with increasing distance to the goal. Here, the contextual variable c∈R3controls the
position of the gate ( C-GatePosition ), the width of the gate ( C-GateWidth ), and the friction coeﬃcient of
the ground ( C-Friction ). We construct the training pool of tasks by uniformly sampling 100tasks over the
space of possible tasks (here, each task corresponds to a diﬀerent contextual variable).
BasicKarel. This environment is inspired by the Karel program synthesis domain (Bunel et al.,
2018), where the goal of an agent is to transform an initial grid into a ﬁnal grid conﬁguration by
a sequence of commands. In our BasicKarel environment, we do not allow any programming
constructs such as conditionals or loops and limit the commands to the “basic” actions given by
A={move,turnLeft,turnRight,pickMarker ,putMarker,finish}. A task in this environment corre-
sponds to a pair of initial grid and ﬁnal grid conﬁgurations; the environment is episodic with goal-based (i.e.,
binary and sparse) reward setting where the agent receives a reward of 1only if it successfully transforms
the task’s initial grid into the task’s ﬁnal grid. Here, the contextual variable is discrete, where each task
can be considered as a discrete context. We construct the training pool of tasks by sampling 24000tasks;
additional details are provided in the appendix.
BallCatching. This environment is the same used in the work of Klink et al. (2020b); here, an agent needs
to direct a robot to catch a ball thrown towards it. The reward function is sparse and non-binary, only
rewarding the robot when it catches the ball and penalizing it for excessive movements. The contextual
vectorc∈R3captures the distance to the robot from which the ball is thrown and its goal position in a
plane that intersects the base of the robot. We construct the training pool of tasks by uniformly sampling
100tasks over the space of possible tasks.
AntGoal. This environment is adapted from the original MuJoCo Antenvironment (Todorov et al.,
2012). In our adaptation, we additionally have a goal on a ﬂat 2D surface, and an agent is rewarded for
moving an ant robot towards the goal location. This goal-based reward term replaces the original reward
term of making the ant move forward; also, this reward term increases exponentially when the ant moves
closer to the goal location. We keep the other reward terms, such as control and contact costs, similar
to the original MuJoCo Antenvironment. The environment is episodic with a length of 200steps. The
goal location essentially serves as a contextual variable in R2. We construct the training pool of tasks by
uniformly sampling 50goal locations from a circle around the ant.
7These environments are goal-based and have an implicit way of deﬁning a successful trajectory. Typically,
success is deﬁned as a reward signal to the agent for approaching the goal, as done by Klink et al. (2020b) for
PointMass ,BallCatching , and AntGoal . As future work, it would also be interesting to investigate
the eﬀect of our curriculum strategy on RL algorithms designed for the same goal-based setting but without
assuming that a goal proximity function is deﬁned in the environment (Ding et al., 2019; Eysenbach et al.,
2022; Lin et al., 2019).
4.2 Curriculum Strategies Evaluated
Variants of our curriculum strategy. We consider the curriculum strategies ProCuRL-val and
ProCuRL-env from Section 3.3. Since ProCuRL-env uses policy rollouts to estimate PoS t(s)in Eq. 5,
it requires environment steps for selecting tasks in addition to environment steps for training. To compare
ProCuRL-val andProCuRL-env in terms of trade-oﬀ between performance and sample eﬃciency, we
introduce a variant ProCuRL-envxwhere x controls the budget of the total number of steps used for
estimation and training. In Figure 3, variants with x ∈{2,4}refer to a total budget of about x million
environment steps when training comprises of 1million steps.
State-of-the-art baselines. SPDL(Klink et al., 2020b) and SPaCE(Eimer et al., 2021) are state-of-the-
art curriculum strategies for contextual RL. We adapt the implementation of an improved version of SPDL,
presented in Klink et al. (2021), to work with a discrete pool of tasks. We also introduce a variant of SPaCE,
namely SPaCE-alt , by adapting the implementation of Eimer et al. (2021) to sample the next training
task as P/bracketleftbig
s(0)
t=s/bracketrightbig
∝exp/parenleftbig
β·/parenleftbig
Vt(s)−Vt−1(s)/parenrightbig/parenrightbig
.PLR(Jiang et al., 2021b) is a state-of-the-art curriculum
strategy originally designed for procedurally generated content settings. We adapt the implementation of
PLRfor the contextual RL setting operating on a ﬁxed pool of tasks and include it as an additional baseline.
Prototypical baselines. IIDstrategy randomly samples the next task from the pool; note that IID
serves as a competitive baseline since we consider the uniform performance objective. We introduce two
additional variants of ProCuRL-env , namely EasyandHard, to understand the importance of the two
terms PoS t(s)and/parenleftbig
1−PoSt(s)/parenrightbig
in Eq. 5. Easysamples tasks as P/bracketleftbig
s(0)
t=s/bracketrightbig
∝exp/parenleftbig
β·PoSt(s)/parenrightbig
, and
Hardsamples tasks as P/bracketleftbig
s(0)
t=s/bracketrightbig
∝exp/parenleftbig
β·/parenleftbig
1−PoSt(s)/parenrightbig/parenrightbig
.
4.3 Results
Convergence behavior and curriculum plots. As shown in Figure 2, the RL agents trained using
the variants of our curriculum strategy, ProCuRL-env andProCuRL-val , either match or outperform
the agents trained with state-of-the-art and prototypical baselines in all the environments. Figures 4 and 5
visualizethecurriculumsgeneratedby ProCuRL-env ,ProCuRL-val , andIID;thetrendsfor ProCuRL-
valgenerally indicate a gradual shift towards harder tasks across diﬀerent contexts. The increasing trend
in Figure 4a corresponds to a preference shift towards tasks with the gate positioned closer to the edges;
the decreasing trend in Figure 4b corresponds to a preference shift towards tasks with narrower gates. For
BasicKarel , theincreasingtrendsinFigures5aand5bcorrespondtoapreferencetowardstaskswithlonger
solution trajectories and tasks requiring a marker to be picked or put, respectively. In Figures 5c and 5d,
tasks with a distractor marker ( C-DistractorMarker ) and tasks with more walls ( C-Walls) are increasingly
selected while training.
Metrics comparison. In Figure 3, we compare curriculum strategies considered in our experiments w.r.t.
diﬀerent metrics. ProCuRL-val has similar sample complexity as state-of-the-art baselines since it does
not require additional environment steps for the teacher component. ProCuRL-val performs better com-
pared to SPDL,SPaCEandPLRin terms of computational complexity. The eﬀect of that is more evident
as the pool size increases. The reason is that ProCuRL-val only requires forward-pass operation on the
critic-model to obtain value estimates for each task in the pool. SPDLandSPaCE not only require the
same forward-pass operations, but SPDLdoes an additional optimization step, and SPaCErequires a task
ordering step. As for PLR, it has an additional computational overhead for scoring the sampled tasks. In
terms of agent’s performance, our curriculum strategies exceed or match these baselines at diﬀerent training
segments. Even though ProCuRL-env consistently surpasses all the other variants in terms of perfor-
80.0 0.5 1.0
×1060.00.20.40.60.81.0Performance
POINT MASS-SPROCURL- ENV PROCURL- VAL IID SPDL PLR SPACE- ALT SPACE
0.0 0.5 1.0
×1063040506070
POINT MASS-D
0 1 2
Steps×1060.00.20.40.60.81.0
BASIC KAREL
0 1 2 3 4 5
×1063040506070
BALLCATCHING
0 1 2 3 4 5
×1060100200300400500600700
ANTGOALFigure 2: Performance comparison of RL agents trained using diﬀerent curriculum strategies described in
Section 4.2. The performance is measured as the mean reward ( ±1standard error) on the training pool
of tasks. The results are averaged over 20random seeds for PointMass-s andPointMass-d ,10random
seeds for BasicKarel andBallCatching , and 5random seeds for AntGoal . The plots are smoothed
across 5evaluation snapshots happening at over 25000training steps.
MethodEnvPointMass-s BasicKarel
Performance StepsTime Performance StepsTime
0.25M 0.5M 1M 1M 1M 0.25M 0.5M 1M 1M 1M
ProCuRL-env 0.60±0.16 0.79±0.13 0.84±0.1417.4±1.7 156 0.10±0.02 0.38±0.03 0.76±0.0434.2±0.9 191
ProCuRL-env40.50±0.15 0.64±0.15 0.71±0.15 4.0±0.0 43 0.10±0.03 0.38±0.04 0.75±0.04 4.6±0.1 53
ProCuRL-env20.36±0.17 0.53±0.16 0.60±0.17 2.0±0.0 25 0.10±0.03 0.32±0.05 0.73±0.04 2.4±0.1 44
ProCuRL-val 0.48±0.15 0.64±0.17 0.71±0.18 1.0±0.0 20 0.06±0.03 0.30±0.08 0.71±0.05 1.0±0.0 70
SPaCE 0.05±0.06 0.17±0.12 0.29±0.15 1.0±0.0 22 0.04±0.02 0.11±0.03 0.30±0.04 1.0±0.0 89
SPaCE-alt 0.22±0.12 0.37±0.15 0.46±0.17 1.0±0.0 21 0.05±0.03 0.18±0.06 0.50±0.08 1.0±0.0 69
SPDL 0.34±0.16 0.45±0.17 0.52±0.17 1.0±0.0 23 0.07±0.02 0.29±0.04 0.69±0.05 1.0±0.0 81
PLR 0.32±0.12 0.47±0.15 0.69±0.13 1.0±0.0 20 0.05±0.03 0.23±0.05 0.70±0.04 1.0±0.0 79
IID 0.27±0.15 0.34±0.17 0.36±0.19 1.0±0.0 20 0.03±0.02 0.15±0.06 0.64±0.08 1.0±0.0 34
Easy 0.37±0.13 0.44±0.12 0.50±0.1117.1±2.3 154 0.04±0.01 0.07±0.02 0.11±0.0322.6±0.9 126
Hard 0.01±0.01 0.00±0.00 0.01±0.0137.0±0.7 332 0.01±0.00 0.01±0.00 0.01±0.0035.2±3.9 197
Figure 3: Comparison of diﬀerent curriculum strategies described in Section 4.2 under the following metrics:
(i) performance (mean reward ±t×standard error, where tis the value from the t-distribution table for
95%conﬁdence (Beyer, 2019)) of the RL agent at 0.25,0.5, and 1million training steps; (ii) total number
of environment steps incurred at the end of 1million training steps (this captures the sample eﬃciency of
a curriculum strategy); (iii) total clock time in minutes at the end of 1million training steps (this captures
the computational eﬃciency of a curriculum strategy).
mance, its teacher component requires a lot of additional environment steps. Regarding the prototypical
baselines in Figure 3, we make the following observations: (a) IIDis a strong baseline in terms of sample
and computational eﬃciency; however, its performance tends to be unstable in PointMass-s environment
because of high randomness; (b) Easyperforms well in PointMass-s because of the presence of easy tasks
in the task space of this environment, but, performs quite poorly in BasicKarel ; (c)Hardconsistently
fails in both the environments.
Ablation and robustness experiments. We conduct additional experiments to evaluate the robustness of
ProCuRL-val w.r.t. diﬀerent values of βand diﬀerent /epsilon1-level noise in Vt(s)values. The results are reported
in the appendix. From the reported results, we note that picking a value for βin the range from 10to30leads
90.00.20.40.60.81.0
Steps×1061.82.02.22.4C-GatePositionPROCURL- ENV
PROCURL- VAL
IID(a)C-GatePosition
0.00.20.40.60.81.0
Steps×1063.53.84.14.44.75.05.3C-GateWidthPROCURL- ENV
PROCURL- VAL
IID (b)C-GateWidth
0.00.20.40.60.81.0
Steps×1061.21.41.61.82.02.2C-FrictionPROCURL- ENV
PROCURL- VAL
IID (c)C-Friction
(d) At 0.25M
 (e) At 0.5M
 (f) At 0.75M
Figure 4:(a-c)Curriculum visualization of ProCuRL-env ,ProCuRL-val , and IIDin the PointMass-s
environment; these plots show the moving average variation of the context variables of every 100tasks picked
by curriculum strategies during the training process (a picked task involves multiple training steps shown
on the x-axis of plots). The increasing trend in (a)corresponds to a preference shift towards tasks with the
gate positioned closer to the edges; the decreasing trend in (b)corresponds to a shift towards tasks with
narrower gates. (d-f)Illustrative tasks used during the training process for ProCuRL-val (M is 106).
0.0 0.5 1.0 1.5 2.0
Steps×1064.44.64.85.05.25.4C-TrajLengthPROCURL- ENV
PROCURL- VAL
IID
(a)C-TrajLength
0.0 0.5 1.0 1.5 2.0
Steps×1060.500.550.600.650.700.75C-MarkerActionPROCURL- ENV
PROCURL- VAL
IID (b)C-MarkerAction
0.0 0.5 1.0 1.5 2.0
Steps×1060.460.480.500.520.540.560.580.60C-DistractorMarkerPROCURL- ENV
PROCURL- VAL
IID (c)C-DistractorMarker
0.0 0.5 1.0 1.5 2.0
Steps×1065.905.956.006.056.106.156.206.256.30C-WallsPROCURL- ENV
PROCURL- VAL
IID (d)C-Walls
(e) At 0.1M
 (f) At 0.25M
 (g) At 0.5M
 (h) At 0.75M
 (i) At 1.5M
Figure 5: (a-d)Curriculum visualization of ProCuRL-env ,ProCuRL-val , and IIDin the BasicKarel
environment; these plots show the moving average variation of the context variables of every 500tasks picked.
The increasing trends in (a-d)correspond to a preference towards tasks: (a)with longer trajectories, (b)
requiring a marker action, (c)with more distractor markers, and (d)with more walls. (e-i)Illustrative
tasks used during the training process at diﬀerent steps for ProCuRL-val (M is 106).
tocompetitiveperformance, and ProCuRL-val isrobustevenfornoiselevelsupto /epsilon1= 0.2. Further, wecon-
duct an ablation study on the form of our curriculum objective presented in Eq. 1. More speciﬁcally, we con-
siderthefollowinggeneralizedvariantofEq.1withparameters γ1andγ2:s(0)
t←arg maxs∈Sinit/parenleftbig
PoSt(s)·(γ1·
PoS∗(s)−γ2·PoSt(s))/parenrightbig
. In our experiments, we consider the following range of γ2/γ1∈{0.6,0.8,1.0,1.2,1.4}.
The results are reported in the appendix. From the reported results, we note that our default curriculum
strategy in Eq. 1 (corresponding to γ2/γ1= 1.0) leads to competitive performance.
5 Concluding Discussions
We proposed a novel curriculum strategy for deep RL agents inspired by the ZPD concept. We mathemati-
callyderivedourstrategybyanalyzingsimplelearningsettingsandempiricallydemonstrateditseﬀectiveness
10in a variety of complex domains. Here, we discuss a few limitations of our work and outline a plan on how to
address them in future work. First, experimental results show that diﬀerent variants of our proposed curricu-
lum provide an inherent trade-oﬀ between runtime and performance; it would be interesting to systematically
study these variants to obtain a more eﬀective curriculum strategy across diﬀerent metrics. Second, it would
be interesting to extend our curriculum strategy to sparse reward environments with high-dimensional con-
text space; in particular, our curriculum strategy requires estimating the probability of success of all tasks
in the pool when sampling a new task which is challenging in these environments. Third, extending the
theoretical analysis of the curriculum strategy from independent task settings to correlated task settings
would be an interesting avenue to explore; this could involve developing a generalized version of ProCuRL
curriculum strategy using a distance metric over the context space (Klink et al., 2022; Huang et al., 2022).
Acknowledgments
Parameswaran Kamalaruban acknowledges support from The Alan Turing Institute. Funded/Co-funded
by the European Union (ERC, TOPS, 101039090). Views and opinions expressed are however those of the
author(s) only and do not necessarily reﬂect those of the European Union or the European Research Council.
Neither the European Union nor the granting authority can be held responsible for them.
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew,
Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight Experience Replay. In NeurIPS , 2017.
Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida, and Koh Hosoda. Purposive Behavior Acquisition for
a Real Robot by Vision-based Reinforcement Learning. Machine learning , 23(2-3):279–303, 1996.
Adrien Baranes and Pierre-Yves Oudeyer. Active Learning of Inverse Models with Intrinsically Motivated
Goal Exploration in Robots. Robotics and Autonomous Systems , 61(1):49–73, 2013.
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum Learning. In ICML,
2009.
William H Beyer. Handbook of Tables for Probability and Statistics . CRC Press, 2019.
Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging
Grammar and Reinforcement Learning for Neural Program Synthesis. In ICLR, 2018.
Seth Chaiklin. The Zone of Proximal Development in Vygotsky’s Analysis of Learning and Instruction.
Vygotsky’s Educational Theory in Cultural Context , pp. 39, 2003.
Patrick Dendorfer, Aljosa Osep, and Laura Leal-Taixé. Goal-GAN: Multimodal Trajectory Prediction based
on Goal Position Estimation. In ACCV, 2020.
Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and
Sergey Levine. Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design. In
NeurIPS , 2020.
Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned Imitation Learning.
InNeurIPS , 2019.
Theresa Eimer, André Biedenkapp, Frank Hutter, and Marius Lindauer. Self-Paced Context Evaluation for
Contextual Reinforcement Learning. In ICML, 2021.
Jeﬀrey L Elman. Learning and Development in Neural Networks: The Importance of Starting Small. Cog-
nition, 48(1):71–99, 1993.
Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Ruslan Salakhutdinov. Contrastive Learning as
Goal-conditioned Reinforcement Learning. In NeurIPS , 2022.
11Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse Curriculum
Generation for Reinforcement Learning. In CORL, 2017.
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic Goal Generation for Reinforce-
ment Learning Agents. In ICML, 2018.
Alex Graves, Marc G Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu. Automated Curricu-
lum Learning for Neural Networks. In ICML, 2017.
Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual Markov Decision Processes. CoRR,
abs/1502.02259, 2015.
Peide Huang, Mengdi Xu, Jiacheng Zhu, Laixi Shi, Fei Fang, and Ding Zhao. Curriculum Reinforcement
Learning using Optimal Transport via Gradual Domain Adaptation. In NeurIPS , 2022.
Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-Paced Curriculum
Learning. In AAAI, 2015.
Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward Grefenstette, and Tim Rock-
täschel. Replay-Guided Adversarial Environment Design. In NeurIPS , 2021a.
Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized Level Replay. In ICML, 2021b.
Parameswaran Kamalaruban, Rati Devidze, Volkan Cevher, and Adish Singla. Interactive Teaching Algo-
rithms for Inverse Reinforcement Learning. In IJCAI, 2019.
Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A Survey of Generalisation in Deep
Reinforcement Learning. CoRR, abs/2111.09794, 2021.
Pascal Klink, Hany Abdulsamad, Boris Belousov, and Jan Peters. Self-Paced Contextual Reinforcement
Learning. In CORL, 2020a.
Pascal Klink, Carlo D’Eramo, Jan R Peters, and Joni Pajarinen. Self-Paced Deep Reinforcement Learning.
InNeurIPS , 2020b.
Pascal Klink, Hany Abdulsamad, Boris Belousov, Carlo D’Eramo, Jan Peters, and Joni Pajarinen. A
Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning. Journal
of Machine Learning Research , 22:182–1, 2021.
Pascal Klink, Haoyi Yang, Carlo D’Eramo, Jan Peters, and Joni Pajarinen. Curriculum Reinforcement
Learning via Constrained Optimal Transport. In ICML, 2022.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-Paced Learning for Latent Variable Models.
InNeurIPS , 2010.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep Visuomotor
Policies. Journal of Machine Learning Research , 17(1):1334–1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Sil-
ver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR, abs/1509.02971,
2015.
X. Lin, H. Baweja, and D. Held. Reinforcement Learning without Ground-Truth State. ICML’19 Workshop
on Multi-Task and Lifelong Reinforcement Learning, 2019.
Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg, and
Le Song. Iterative Machine Teaching. In ICML, 2017.
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher–Student Curriculum Learning.
IEEE Transactions on Neural Networks and Learning Systems , 31(9):3732–3740, 2019.
12Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-Level Control Through
Deep Reinforcement Learning. Nature, 518(7540):529–533, 2015.
Sanmit Narvekar and Peter Stone. Learning Curriculum Policies for Reinforcement Learning. In AAMAS,
2019.
SanmitNarvekar, JivkoSinapov, andPeterStone. AutonomousTaskSequencingforCustomizedCurriculum
Design in Reinforcement Learning. In IJCAI, 2017.
SanmitNarvekar,BeiPeng,MatteoLeonetti,JivkoSinapov,MatthewETaylor,andPeterStone. Curriculum
Learning for Reinforcement Learning Domains: A Framework and Survey. Journal of Machine Learning
Research , 21:1–50, 2020.
Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic Motivation Systems for Autonomous
Mental Development. IEEE Transactions on Evolutionary Computation , 11(2):265–286, 2007.
Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette,
andTimRocktäschel. EvolvingCurriculawithRegret-BasedEnvironmentDesign. CoRR,abs/2203.01302,
2022.
RémyPortelas, CédricColas, LilianWeng, KatjaHofmann, andPierre-YvesOudeyer. AutomaticCurriculum
Learning for Deep RL: A Short Survey. In IJCAI, 2021.
Sébastien Racanière, Andrew K Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, and Timothy P
Lillicrap. Automated Curricula Through Setter-Solver Interactions. In ICLR, 2020.
Antonin Raﬃn, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
Stable-Baselines3: Reliable Reinforcement Learning Implementations. Journal of Machine Learning Re-
search, 22(268):1–8, 2021.
Cinjon Resnick, Roberta Raileanu, Sanyam Kapoor, Alexander Peysakhovich, Kyunghyun Cho, and Joan
Bruna. Backplay:“ Man muss immer umkehren”. CoRR, abs/1807.06919, 2018.
Martin A Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele,
Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by Playing Solving Sparse Reward
Tasks from Scratch. In ICML, 2018.
Tim Salimans and Richard Chen. Learning Montezuma’s Revenge from a Single Demonstration. CoRR,
abs/1812.03381, 2018.
Jürgen Schmidhuber. Powerplay: Training an Increasingly General Problem Solver by Continually Searching
for the Simplest Still Unsolvable Problem. Frontiers in Psychology , 4:313, 2013.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Opti-
mization Algorithms. CoRR, abs/1707.06347, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the Game of Go Without Human
Knowledge. Nature, 550(7676):354–359, 2017.
Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth Stanley, and Jeﬀrey Clune. Generative Teaching
Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data. In
ICML, 2020.
Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Robert Fergus.
Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. In ICLR, 2018.
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for
Reinforcement Learning with Function Approximation. In NeurIPS , 1999.
13Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-based Control. In
IROS, 2012.
Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal. Safe Reinforcement
Learning via Curriculum Induction. In NeurIPS , 2020.
Lev Semenovich Vygotsky and Michael Cole. Mind in Society: Development of Higher Psychological Pro-
cesses. Harvard University Press, 1978.
Daphna Weinshall and Dan Amir. Theory of Curriculum Learning with Convex Loss Functions. CoRR,
abs/1812.03472, 2018.
Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum Learning by Transfer Learning: Theory and
Experiments with Deep Networks. In ICML, 2018.
Lilian Weng. Curriculum for Reinforcement Learning. lilianweng.github.io , 2020. URL https://
lilianweng.github.io/posts/2020-01-29-curriculum-rl/ .
Jan Wöhlke, Felix Schmitt, and Herke van Hoof. A Performance-Based Start State Curriculum Framework
for Reinforcement Learning. In AAMAS, 2020.
Yuxin Wu and Yuandong Tian. Training Agent for First-Person Shooter Game with Actor-Critic Curriculum
Learning. In ICLR, 2016.
Scott Cheng-Hsin Yang, Yue Yu, arash Givchi, Pei Wang, Wai Keen Vong, and Patrick Shafto. Optimal
Cooperative Inference. In AISTATS , 2018.
Gaurav Raju Yengera, Rati Devidze, Parameswaran Kamalaruban, and Adish Singla. Curriculum Design
for Teaching via Demonstrations: Theory and Applications. In NeurIPS , 2021.
Wojciech Zaremba and Ilya Sutskever. Learning to Execute. CoRR, abs/1410.4615, 2014.
Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic Curriculum Learning Through Value Disagree-
ment. In NeurIPS , 2020.
Tianyi Zhou and Jeﬀ Bilmes. Minimax Curriculum Learning: Machine Teaching with Desirable Diﬃculties
and Scheduled Diversity. In ICLR, 2018.
Tianyi Zhou, Shengjie Wang, and Jeﬀ Bilmes. Curriculum Learning by Optimizing Learning Dynamics. In
AISTATS , 2021.
Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N. Raﬀerty. An Overview of Machine Teaching. CoRR,
abs/1801.05927, 2018.
Xiaotian Zou, Wei Ma, Zhenjun Ma, and Ryan S Baker. Towards Helping Teachers Select Optimal Content
for Students. In AIED, 2019.
14A Table of Contents
In this section, we give a brief description of the content provided in the appendices of the paper.
•Appendix B provides a proof for Theorem 1 and an additional theoretical justiﬁcation for our curriculum
strategy. (Section 3.2)
•Appendix C provides additional details and results for experimental evaluation. (Section 4)
B Theoretical Justiﬁcations for the Curriculum Strategy – Proof and Additional
Justiﬁcation (Section 3.2)
B.1 Proof of Theorem 1
Proof.For the contextual bandit setting described in Section 3.2.1, the Reinforce learner’s update rule
reduces to the following: θt+1←θt+ηt·1{s(1)
t=g}·/bracketleftbig
∇θlogπθ(a(0)
t|s(0)
t)/bracketrightbig
θ=θt. In particular, for s(0)
t=s
anda(0)
t=a1, we update:
θt+1[s,a1]←θt[s,a1] +ηt·1{s(1)
t=g}·(1−πθt(a1|s))
θt+1[s,a2]←θt[s,a2]−ηt·1{s(1)
t=g}·(1−πθt(a1|s))
and we set θt+1[s,·]←θt[s,·]whens(0)
t/negationslash=sora(0)
t/negationslash=a1. Lets(0)
t=s, and consider the following:
∆t(θt+1/vextendsingle/vextendsingleθt,s,ξt)
=/bardblθ∗−θt/bardbl1−/bardblθ∗−θt+1/bardbl1
=/bardblθ∗[s,·]−θt[s,·]/bardbl1−/bardblθ∗[s,·]−θt+1[s,·]/bardbl1
={θ∗[s,a1]−θt[s,a1] +θt[s,a2]−θ∗[s,a2]}−{θ∗[s,a1]−θt+1[s,a1] +θt+1[s,a2]−θ∗[s,a2]}
=θt+1[s,a1]−θt[s,a1] +θt[s,a2]−θt+1[s,a2]
= 2·ηt·1{a(0)
t=a1,s(1)
t=g}·(1−πθt(a1|s)).
For the contextual bandit setting, the probability of success is given by PoS θ(s) =Vπθ(s) =prand(s)·
πθ(a1|s),∀s∈S. We assume that ∃θ∗such thatπθ∗(a1|s)→1; here,πθ∗is the target policy. With the
above deﬁnition, the probability of success scores for any task associated with the starting state s∈Sinit
w.r.t. the target and agent’s current policies (at any step t) are respectively given by PoS∗(s) =PoSθ∗(s) =
prand(s) =p∗and PoSt(s) =PoSθt(s) =prand(s)·πθt(a1|s) =p. Now, we consider the following:
Ct(s) =Eξt|s/bracketleftbig
∆t(θt+1/vextendsingle/vextendsingleθt,s,ξt)/bracketrightbig
=Eξt|s/bracketleftBig
2·ηt·1{a(0)
t=a1,s(1)
t=g}·(1−πθt(a1|s))/bracketrightBig
= 2·ηt·prand(s)·πθt(a1|s)·(1−πθt(a1|s))
= 2·ηt·p·/parenleftbigg
1−p
p∗/parenrightbigg
.
B.2 Abstract Agent with Direct performance Parameterization
Weconsideranabstractagentmodelwiththefollowingdirectperformanceparameterization: forany θ∈Θ =
[0,1]|Sinit|, we have PoS θ(s) =θ[s],∀s∈Sinit.5Under this model, the agent’s current knowledge θtat steptis
5In this setting, we abstract out the policy πθand directly map the “parameter” θto a vector of “performance on tasks”
PoSθ. Then, we choose the parameter space as Θ = [0,1]Sinit(whered=Sinit) and deﬁne PoS θ=θ. Thus, an update in the
“parameter” θis equivalent to an update in the “performance on tasks” PoS θ.
15encodeddirectlybyitsprobabilityofsuccessscores {PoSθt(s)|s∈Sinit}. Thetargetknowledgeparameter θ∗
is given by{PoSθ∗(s)|s∈Sinit}. Under the independent task setting, we design an update rule for the agent
to reﬂect the characteristics of the policy gradient style update. In particular, for s=s(0)
t∈Sinit, we update
θt+1[s]←θt[s] +α·succ(ξt;s)·(θ∗[s]−θt[s]) +β·(1−succ(ξt;s))·(θ∗[s]−θt[s]),
whereα,β∈[0,1]andα>β. Fors∈Sinitands/negationslash=s(0)
t, we maintain θt+1[s]←θt[s]. Importantly, α>β
implies that the agent’s current knowledge for the picked task is updated more when the agent succeeds
in that task compared to the failure case. The update rule captures the following idea: when picking a
task that is “too easy”, the progress in θttowardsθ∗is minimal since (θ∗[s]−θt[s])is low; similarly, when
picking a task that is “too hard”, the progress in θttowardsθ∗is minimal since β·(θ∗[s]−θt[s])is low
forβ/lessmuch1. This idea aligns with the ZPD concept in terms of the learning progress (Vygotsky & Cole,
1978; Chaiklin, 2003). For the abstract agent under the above setting, the following theorem quantiﬁes the
expected improvement in the training objective at step t:
Theorem 2. Consider the abstract agent with direct performance parameterization under the independent
task setting as described above. Let s(0)
tbe the task picked at step twithPoSθt(s(0)
t) =pandPoSθ∗(s(0)
t) =p∗.
Then, we have: Ct(s(0)
t) =α·p·(p∗−p) +β·(1−p)·(p∗−p).
Proof.Lets(0)
t=s∈Sinit, and consider the following:
∆t(θt+1/vextendsingle/vextendsingleθt,s,ξt) =/bardblθ∗−θt/bardbl1−/bardblθ∗−θt+1/bardbl1
=θt+1[s]−θt[s]
=α·succ(ξt;s)·(θ∗[s]−θt[s]) +β·(1−succ(ξt;s))·(θ∗[s]−θt[s]).
For the abstract learner model deﬁned in Section B.2, we have PoS θ(s) =Vπθ(s) =θ[s], for anys∈Sinit.
Then, the probability of success scores for any task s∈Sinitw.r.t. the target and agent’s current policies (at
any stept) are respectively given by PoS∗(s) =PoSθ∗(s) =θ∗[s] =p∗and PoSt(s) =PoSθt(s) =θt[s] =p.
Now, we consider the following:
Ct(s) =Eξt|s/bracketleftbig
∆t(θt+1/vextendsingle/vextendsingleθt,s,ξt)/bracketrightbig
=Eξt|s[α·succ(ξt;s)·(θ∗[s]−θt[s]) +β·(1−succ(ξt;s))·(θ∗[s]−θt[s])]
=α·θt[s]·(θ∗[s]−θt[s]) +β·(1−θt[s])·(θ∗[s]−θt[s])
=α·p·(p∗−p) +β·(1−p)·(p∗−p).
For the above setting with α= 1andβ= 0,maxs∈SinitCt(s)is equivalent to maxs∈SinitPoSt(s)·(PoS∗(s)−
PoSt(s)). This, in turn, implies that the curriculum strategy given in Eq. 1 can be seen as greedily optimizing
the expected improvement in the training objective at step tgiven in Eq. 3.
C Experimental Evaluation – Additional Details (Section 4)
C.1 Environments
BasicKarel. This environment is inspired by the Karel program synthesis domain (Bunel et al., 2018),
where the goal of an agent is to transform an initial grid into a ﬁnal grid conﬁguration by a sequence
of commands. In the BasicKarel environment, we do not allow any programming constructs such
as conditionals or loops and limit the commands to the “basic” actions given by the action space
A={move,turnLeft,turnRight,pickMarker ,putMarker,finish}. A task in this environment corre-
sponds to a pair of initial grid and ﬁnal grid conﬁgurations. It consists of an avatar, walls, markers, and
empty grid cells, and each element has a speciﬁc location in the grid. The avatar is characterized by its
current location and orientation. Its orientation can be any direction {North,East,South,West}, and
its location can be any grid cell, except from grid cells where a wall is located. The state space Sof
16BasicKarel is any possible conﬁguration of the avatar, walls, and markers in a pair of grids. The avatar
can move around the grid and is directed via the basic Karel commands, i.e., the action space A. While the
avatar moves, if it hits a wall or the grid boundary, it “crashes” and the episode terminates. If pickMarker is
selected when no marker is present, the avatar “crashes” and the program ends. Likewise, if the putMarker
action is taken and a marker is already present, the avatar “crashes” and the program terminates. The
finishaction indicates the end of the sequence of actions, i.e., the episode ends after encountering this
action. To successfully solve a BasicKarel task, the sequence of actions must end with a finish, and
there should be no termination via “crashes”. Based on this environment, we created a multi-task dataset
that consists of 24000training tasks and 2400test tasks. All the generated tasks have a grid size of 4×4.
C.2 Evaluation Setup
Hyperparameters of PPO method. We use the PPO method from Stable-Baselines3 library with a
basic MLP policy for all the conducted experiments (Schulman et al., 2017; Raﬃn et al., 2021). For the
PointMass-S ,PointMass-D , and BallCatching environments, the MLP policy has a shared layer with
64units and a second layer with separate 64units for the policy and 64units for the value function. For
theBasicKarel environment, we use two separate layers of size [ 512,256] for the policy network and
two layers of size [ 256,128] for the value function network. For the AntGoal environment, we use two
separate layers of size [ 512,512] for the policy network and two layers of size [ 512,512] for the value function
network. For all the experiments, ReLU is the chosen activation function. In Figure 6, we report the PPO
hyperparameters used in the experiments. For each environment, all the hyperparameters are consistent
across all the diﬀerent curriculum strategies.
Hyperparameters PointMass-s PointMass-d BasicKarel BallCatching AntGoal
Nsteps 1024 1024 2048 5120 1024
γ 0.99 0.95 0.99 0.99 0.99
Nepochs 10 10 10 10 10
learning_rate 3e−4 3e−4 3e−4 3e−4 2e−5
batch_size 64 64 64 64 32
ent_coef 0 0 0 0 5e−7
clip_range 0.2 0.2 0.2 0.2 0.1
gae_lambda 0.95 0.95 0.95 0.95 0.8
max_grad_norm 0.5 0.5 0.5 0.5 0.6
vf_coef 0.5 0.5 0.5 0.5 0.7
Figure 6: Diﬀerent hyperparameters of the PPO method used in the experiments for each environment.
Compute resources. All the experiments were conducted on a cluster of machines with CPUs of model
Intel Xeon Gold 6134M CPU @ 3.20GHz.
C.3 Curriculum Strategies Evaluated
Variants of the curriculum strategy. Algorithm 2 provides a complete pseudo-code for the RL agent
using PPO method when trained with ProCuRL-val in the general setting of non-binary or dense rewards
(see Section 3.3). In Eq. 1 and Algorithm 1, we deﬁned tat an episodic level; however, in Algorithm 2,
tdenotes an environment step (in the context of the PPO method). For ProCuRL-env , in line 24 of
Algorithm 2, we estimate the probability of success for all the tasks using the additional rollouts obtained
by executing the current policy in M.
To achieve the constrained budget of evaluation steps in ProCuRL-envx(with x∈{2,4}), we reduce
the frequency of updating PoS tsince this is the most expensive operation for ProCuRL-env requiring
additional rollouts for each task. On the other hand, ProCuRL-val updates PoS tby using the values
17obtained from forward-pass on the critic model – this update happens whenever the critic model is updated
(every 2048 training steps for BasicKarel ). This higher frequency of updating PoS tinProCuRL-val is
why it is slower than ProCuRL-envx(with x∈{2,4}) forBasicKarel . Note that the relative frequency
of updates for PointMass is diﬀerent in comparison to BasicKarel because of very diﬀerent pool sizes.
Hence, the behavior in total clock times is diﬀerent.
Algorithm 2 RL agent using PPO method when trained with ProCuRL-val in the general setting
1:Input:RL algorithm PPO, rollout buﬀer D
2:Hyperparameters: policy update frequency Nsteps, number of epochs Nepochs, number of minibatches
Nbatch, parameter β,Vmin, andVmax
3:Initialization: randomly initialize policy π1and criticV1; set normalized probability of success scores
V1(s) = 0and PoS∗(s) = 1,∀s∈Sinit
4:fort= 1,...,Tdo
5:// add an environment step to the buﬀer
6:observe the state st, and select the action at∼πt(st)
7:execute the action atin the environment
8:observe reward rt, next state st+1, and done signal dt+1to indicate whether st+1is terminal
9:store (st,at,rt,st+1,dt+1)in the rollout buﬀer D
10:// choose new task when the current task/episode ends
11:ifdt+1=truethen
12: reset the environment state
13: sample next task st+1fromP/bracketleftbig
st+1=s/bracketrightbig
∝exp/parenleftbig
β·Vt(s)·(1−Vt(s))/parenrightbig
14:// policy and Vt(s)update
15:ift%Nsteps= 0then
16: setπ/prime←πtandV/prime←Vt
17: fore= 1,...,N epochsdo
18: forb= 1,...,N batchdo
19: sampleb-th minibatch of Nsteps/Nbatchtransitions B={(s,a,r,s/prime,d)}fromD
20: update policy and critic using PPO algorithm π/prime,V/prime←PPO (π/prime,V/prime,B)
21: setπt+1←π/primeandVt+1←V/prime
22: empty the rollout buﬀer D
23: // normalization for the environments with non-binary or dense rewards
24: updateVt+1(s)←Vt+1(s)−Vmin
Vmax−Vmin,∀s∈Sinitusing forward passes on critic
25:else
26: maintain the previous values πt+1←πt,Vt+1←Vt, andVt+1←Vt
27:Output: policyπT
Hyperparameters of curriculum strategies. In Figure 7, we report the hyperparameters of each
curriculum strategy used in the experiments (for each environment). Below, we provide a short description
of these hyperparameters:
1.βparameter controls the stochasticity of the softmax selection.
2.Nposparameter controls the frequency at which Vtis updated. For ProCuRL-env , we setNposhigher
thanNstepssince obtaining rollouts to update Vt(s)is expensive. For all the other curriculum strategies,
we setNpos=Nsteps. For SPaCE,Nposcontrols how frequently the current task dataset is updated based
on their curriculum. For SPDL,Nposcontrols how often we perform the optimization step to update the
distribution for selecting tasks.
3.crolloutsdetermines the number of additional rollouts required to compute the probability of success score
for each task (only for ProCuRL-env ).
4.{Vmin,Vmax}are used in the environments with non-binary or dense rewards to obtain the normalized
valuesV(s)(see Section 3.3). In Figure 7, { Vmin,t,Vmax,t} denote the min-max values of the critic for
statesSinitat stept.
185.ηandκparameters as used in SPaCE(Eimer et al., 2021).
6.VLBperformance threshold as used in SPDL(Klink et al., 2021).
7.ρstaleness coeﬃcient and βPLRtemperature parameter for score prioritization as used in PLR(Jiang
et al., 2021b).
Method Hyperparameters PointMass-s PointMass-d BasicKarel BallCatching AntGoal
ProCuRL-envβ 20 10 10 10 10
Npos 5120 5120 102400 20480 81920
crollouts 20 20 20 20 20
{Vmin,Vmax} n/a{Vmin,t,Vmax,t} n/a n/a{0, 300}
ProCuRL-valβ 20 10 10 10 10
Npos 1024 1024 2048 5120 1024
{Vmin,Vmax} n/a{Vmin,t,Vmax,t} n/a {0, 60} {0, 300}
SPaCEη 0.1 0.1 0.5 0.1 0.1
κ 1 1 64 1 1
Npos 1024 1024 2048 5120 1024
SPaCE-altβ 20 10 10 10 10
Npos 1024 1024 2048 5120 1024
SPDLVLB 0.5 3.5 0.5 30 100
Npos 1024 1024 2048 5120 1024
PLRρ 0.5 0.9 0.9 0.7 0.3
βPLR 0.1 0.3 0.1 0.3 0.1
Npos 1024 1024 2048 5120 1024
Figure 7: We present the hyperparameters of the diﬀerent curriculum strategies for all ﬁve environments.
ForSPDL, we choose the best performing VLBin the non-binary environments from the following sets: set
{1,3.5,10,20,30,40} forPointMass-D ; set { 20,25,30,35,42.5} forBallCatching ; set { 50,100,200,
300,400} forAntGoal . For PLR, we choose the best performing pair (βPLR,ρ)for each environment from
the set{0.1,0.3,0.5,0.7,0.9}×{ 0.1,0.3,0.5,0.7,0.9}.
C.4 Additional Results
Ablation and robustness experiments. We conduct additional experiments to evaluate the robustness of
ProCuRL-val w.r.t. diﬀerent values of βand diﬀerent /epsilon1-level noise in Vt(s)values. The results are reported
in Figure 8. Further, we conduct an ablation study on the form of our curriculum objective presented in
Eq. 1. More speciﬁcally, we consider the following generalized variant of Eq. 1 with parameters γ1andγ2:
s(0)
t←arg max
s∈Sinit/parenleftBig
PoSt(s)·/parenleftbig
γ1·PoS∗(s)−γ2·PoSt(s)/parenrightbig/parenrightBig
(7)
In our experiments, we consider the following range of γ2/γ1∈{0.6,0.8,1.0,1.2,1.4}. Our default curriculum
strategy in Eq. 1 essentially corresponds to γ2/γ1= 1.0. The results are reported in Figure 9.
Performance on test set. In Figure 10, we report the performance of the trained models in the training
set and a test set for comparison purposes. For PointMass-S , we constructed a separate test set of 100
tasks by uniformly picking tasks from the task space. For BasicKarel , we have a train and test dataset of
24000and2400tasks, respectively.
Pool of harder tasks. We sought to assess the eﬀectiveness of ProCuRL-val on tasks where IIDdoes
not perform well. To demonstrate this, we construct a more challenging set of tasks for the PointMass-
senvironment. We generate half of these tasks by uniformly sampling over the context space. The
remaining tasks are sampled from a bi-modal Gaussian distribution, where the means of the contexts
[C-GatePosition ,C-GateWidth ]are[−3,1]and[3,1]for the two modes, respectively. In Figure 11,
we present the results for ProCuRL-val andIID, and in Figure 12 the diﬀerent distributions.
19MethodEnvPointMass-s BasicKarel
ProCuRL-val Performance Performance
0.25M 0.5M 1M 0.25M 0.5M 1M
β= 10 0.48±0.17 0.58±0.19 0.70±0.18 0.06±0.03 0.30±0.08 0.71±0.05
β= 15 0.42±0.17 0.64±0.17 0.74±0.15 0.12±0.04 0.38±0.04 0.71±0.05
β= 20 0.48±0.15 0.64±0.17 0.71±0.18 0.18±0.06 0.42±0.06 0.75±0.06
β= 25 0.45±0.18 0.60±0.19 0.65±0.21 0.22±0.03 0.38±0.04 0.62±0.05
β= 30 0.54±0.18 0.64±0.20 0.74±0.19 0.20±0.06 0.36±0.07 0.67±0.07
/epsilon1= 0.00 0.48±0.15 0.64±0.17 0.71±0.18 0.06±0.03 0.30±0.08 0.71±0.05
/epsilon1= 0.01 0.53±0.18 0.62±0.19 0.71±0.20 0.06±0.02 0.30±0.06 0.69±0.04
/epsilon1= 0.05 0.39±0.16 0.60±0.17 0.70±0.20 0.06±0.02 0.31±0.06 0.72±0.04
/epsilon1= 0.1 0.47±0.17 0.59±0.16 0.67±0.18 0.06±0.03 0.30±0.07 0.69±0.07
/epsilon1= 0.2 0.49±0.16 0.61±0.18 0.68±0.18 0.04±0.02 0.26±0.08 0.74±0.03
Figure 8: Robustness of ProCuRL-val w.r.t. diﬀerent values of βand diﬀerent /epsilon1-level noise in Vt(s)values.
We present the results for the PointMass-s environment and BasicKarel environment. We report the
mean reward (±t×standard error, where tis the value from the t-distribution table for 95%conﬁdence) at
0.25,0.5, and 1million training steps averaged over 20and10random seeds, respectively.
MethodEnvPointMass-s BasicKarel
ProCuRL-val Performance Performance
0.25M 0.5M 1M 0.25M 0.5M 1M
γ2/γ1= 0.6 0.33±0.14 0.50±0.14 0.55±0.13 0.08±0.04 0.21±0.07 0.41±0.08
γ2/γ1= 0.8 0.26±0.15 0.43±0.17 0.55±0.20 0.11±0.03 0.36±0.05 0.64±0.07
γ2/γ1= 1.0 0.48±0.15 0.64±0.17 0.71±0.18 0.06±0.03 0.30±0.08 0.71±0.05
γ2/γ1= 1.2 0.42±0.19 0.55±0.19 0.65±0.17 0.07±0.04 0.30±0.11 0.72±0.08
γ2/γ1= 1.4 0.39±0.16 0.59±0.17 0.59±0.15 0.04±0.02 0.21±0.08 0.71±0.06
Figure 9: Performance comparison of the generalized form of our curriculum strategy presented in Eq. 7
w.r.t. diﬀerent values of γ2/γ1. We present the results for the PointMass-s environment and BasicKarel
environment. We report the mean reward ( ±t×standard error, where tis the value from the t-distribution
table for 95%conﬁdence) at 0.25,0.5, and 1million training steps averaged over 20and10random seeds,
respectively.
20MethodEnvPointMass-s BasicKarel
Performance (1M) Performance (2M)
Train Set Test Set Train Set Test Set
ProCuRL-env 0.84 0.78 0.92 0.90
ProCuRL-val 0.71 0.65 0.91 0.90
SPaCE 0.34 0.28 0.65 0.64
SPaCE-alt 0.47 0.40 0.82 0.81
SPDL 0.55 0.48 0.88 0.87
PLR 0.69 0.60 0.88 0.88
IID 0.39 0.32 0.90 0.89
Figure 10: Performance of the curriculum strategies, discussed in Section 4.2, in the training set and a test
set. We report the performance, i.e., expected mean reward, of the best model obtained during training for
all the methods. The training steps to achieve this performance is shown in parenthesis for each environment
(M is 106steps). We present the results for the PointMass-s environment and BasicKarel environment
and report the mean reward averaged over 20and10random seeds, respectively.
MethodEnvPointMass-s
Performance
0.25M 0.5M 1M 1.5M 2M
ProCuRL-val 0.07±0.07 0.19±0.11 0.40±0.15 0.46±0.16 0.49±0.16
IID 0.01±0.01 0.03±0.03 0.05±0.06 0.04±0.04 0.03±0.03
Figure 11: Performance comparison of our curriculum strategy, ProCuRL-val , and IIDin a pool of harder
tasks for the PointMass-s environment. We report the mean reward ( ±t×standard error, where tis
the value from the t-distribution table for 95%conﬁdence) at 0.25,0.5,1,1.5and2million training steps
averaged over 20random seeds.
-4 -2 0 2 4
C-GatePosition0.523.556.58C-GateWidth
0.0070.0080.0090.0100.0110.0120.0130.014
(a) Distribution for uniform pool of tasks
-4 -2 0 2 4
C-GatePosition0.523.556.58C-GateWidth
0.0000.0250.0500.0750.1000.1250.150
 (b) Distribution for harder pool of tasks
Figure 12: (a)shows the distribution of context values used to generate the uniform pool of tasks for the
main experimental; (b)the distribution of context values that is used to generate a harder pool of tasks.
21